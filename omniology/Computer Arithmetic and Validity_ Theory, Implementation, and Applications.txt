
de Gruyter Studies in Mathematics 33
Editors: Carsten Carstensen · Niels Jacob

de Gruyter Studies in Mathematics
1 Riemannian Geometry, 2nd rev. ed., Wilhelm P. A. Klingenberg
2 Semimartingales, Michel Me´tivier
3 Holomorphic Functions of Several Variables, Ludger Kaup and Burchard Kaup
4 Spaces of Measures, Corneliu Constantinescu
5 Knots, 2nd rev. and ext. ed., Gerhard Burde and Heiner Zieschang
6 Ergodic Theorems, Ulrich Krengel
7 Mathematical Theory of Statistics, Helmut Strasser
8 Transformation Groups, Tammo tom Dieck
9 Gibbs Measures and Phase Transitions, Hans-Otto Georgii
10 Analyticity in Infinite Dimensional Spaces, Michel Herve´
11 Elementary Geometry in Hyperbolic Space, Werner Fenchel
12 Transcendental Numbers, Andrei B. Shidlovskii
13 Ordinary Differential Equations, Herbert Amann
14 Dirichlet Forms and Analysis on Wiener Space, Nicolas Bouleau and
Francis Hirsch
15 Nevanlinna Theory and Complex Differential Equations, Ilpo Laine
16 Rational Iteration, Norbert Steinmetz
17 Korovkin-type Approximation Theory and its Applications, Francesco
Altomare and Michele Campiti
18 Quantum Invariants of Knots and 3-Manifolds, Vladimir G. Turaev
19 Dirichlet Forms and Symmetric Markov Processes, Masatoshi Fukushima,
Yoichi Oshima and Masayoshi Takeda
20 Harmonic Analysis of Probability Measures on Hypergroups, Walter R. Bloom
and Herbert Heyer
21 Potential Theory on Infinite-Dimensional Abelian Groups, Alexander Bendikov
22 Methods of Noncommutative Analysis, Vladimir E. Nazaikinskii,
Victor E. Shatalov and Boris Yu. Sternin
23 Probability Theory, Heinz Bauer
24 Variational Methods for Potential Operator Equations, Jan Chabrowski
25 The Structure of Compact Groups, 2nd rev. and aug. ed., Karl H. Hofmann
and Sidney A. Morris
26 Measure and Integration Theory, Heinz Bauer
27 Stochastic Finance, 2nd rev. and ext. ed., Hans Föllmer and Alexander Schied
28 Painleve´ Differential Equations in the Complex Plane, Valerii I. Gromak,
Ilpo Laine and Shun Shimomura
29 Discontinuous Groups of Isometries in the Hyperbolic Plane, Werner Fenchel
and Jakob Nielsen
30 The Reidemeister Torsion of 3-Manifolds, Liviu I. Nicolaescu
31 Elliptic Curves, Susanne Schmitt and Horst G. Zimmer
32 Circle-valued Morse Theory, Andrei V. Pajitnov

Ulrich Kulisch
Computer Arithmetic
and Validity
Theory, Implementation, and Applications
Walter de Gruyter
Berlin · New York
≥

Author
Ulrich Kulisch
Institute for Applied and Numerical Mathematics
Universität Karlsruhe
Englerstr. 2
76128 Karlsruhe
Germany
E-mail: ulrich.kulisch@math.uka.de
Series Editors
Carsten Carstensen
Department of Mathematics
Humboldt University of Berlin
Unter den Linden 6
10099 Berlin
Germany
E-Mail: cc@math.hu-berlin.de
Niels Jacob
Department of Mathematics
Swansea University
Singleton Park
Swansea SA2 8PP, Wales
United Kingdom
E-Mail: n.jacob@swansea.ac.uk
Mathematics Subject Classification 2000: 65-04, 65Gxx
Keywords: Computer arithmetic, interval arithmetic, floating-point arithmetic, verified com-
puting, interval Newton method

 Printed on acid-free paper which falls within the guidelines of the ANSI
to ensure permanence and durability.
Bibliographic information published by the Deutsche Nationalbibliothek
The Deutsche Nationalbibliothek lists this publication in the Deutsche Nationalbibliografie;
detailed bibliographic data are available in the Internet at http://dnb.d-nb.de.
ISBN 978-3-11-020318-9
 Copyright 2008 by Walter de Gruyter GmbH & Co. KG, 10785 Berlin, Germany.
All rights reserved, including those of translation into foreign languages. No part of this book may be
reproduced in any form or by any means, electronic or mechanical, including photocopy, recording,
or any information storage and retrieval system, without permission in writing from the publisher.
Printed in Germany.
Cover design: Martin Zech, Bremen.
Typeset using the author’s LATEX files: Kay Dimler, Müncheberg.
Printing and binding: Hubert & Co. GmbH & Co. KG, Göttingen.

This book is dedicated to my wife Ursula
and to my family,
to Brigitte and Joachim,
Johanna and Benedikt,
to Angelika and Rolf,
Florian and Niclas,
to
all former and present colleagues of my institute,
and to my students.

Lasset uns am Alten,
so es gut ist, halten
und dann auf dem alten Grund
Neues schaffen Stund um Stund.
House inscription in the Black Forest.

Contents
Preface
xi
Introduction
1
I
Theory of Computer Arithmetic
11
1
First Concepts
12
1.1
Ordered Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.2
Complete Lattices and Complete Subnets
. . . . . . . . . . . . . . .
17
1.3
Screens and Roundings . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.4
Arithmetic Operations and Roundings . . . . . . . . . . . . . . . . .
34
2
Ringoids and Vectoids
42
2.1
Ringoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.2
Vectoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
3
Deﬁnition of Computer Arithmetic
60
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
3.3
The Traditional Deﬁnition of Computer Arithmetic . . . . . . . . . .
67
3.4
Deﬁnition of Computer Arithmetic by Semimorphisms . . . . . . . .
69
3.5
A Remark About Roundings . . . . . . . . . . . . . . . . . . . . . .
75
3.6
Uniqueness of the Minus Operator . . . . . . . . . . . . . . . . . . .
77
3.7
Rounding Near Zero
. . . . . . . . . . . . . . . . . . . . . . . . . .
79
4
Interval Arithmetic
84
4.1
Interval Sets and Arithmetic
. . . . . . . . . . . . . . . . . . . . . .
84
4.2
Interval Arithmetic Over a Linearly Ordered Set . . . . . . . . . . . .
94
4.3
Interval Matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.4
Interval Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
4.5
Interval Arithmetic on a Screen . . . . . . . . . . . . . . . . . . . . . 106
4.6
Interval Matrices and Interval Vectors on a Screen . . . . . . . . . . . 114
4.7
Complex Interval Arithmetic . . . . . . . . . . . . . . . . . . . . . . 122
4.8
Complex Interval Matrices and Interval Vectors . . . . . . . . . . . . 129
4.9
Extended Interval Arithmetic . . . . . . . . . . . . . . . . . . . . . . 134
4.10 Exception-free Arithmetic for Extended Intervals . . . . . . . . . . . 140

viii
Contents
4.11 Extended Interval Arithmetic on the Computer . . . . . . . . . . . . . 145
4.12 Implementation of Extended Interval Arithmetic . . . . . . . . . . . . 149
4.13 Comparison Relations and Lattice Operations . . . . . . . . . . . . . 150
4.14 Algorithmic Implementation of Interval Multiplication and Division . 151
II
Implementation of Arithmetic on Computers
153
5
Floating-Point Arithmetic
154
5.1
Deﬁnition and Properties of the Real Numbers . . . . . . . . . . . . . 154
5.2
Floating-Point Numbers and Roundings . . . . . . . . . . . . . . . . 160
5.3
Floating-Point Operations . . . . . . . . . . . . . . . . . . . . . . . . 168
5.4
Subnormal Floating-Point Numbers
. . . . . . . . . . . . . . . . . . 177
5.5
On the IEEE Floating-Point Arithmetic Standard
. . . . . . . . . . . 178
6
Implementation of Floating-Point Arithmetic on a Computer
187
6.1
A Brief Review on the Realization of Integer Arithmetic
. . . . . . . 188
6.2
Introductory Remarks About the Level 1 Operations . . . . . . . . . . 196
6.3
Addition and Subtraction . . . . . . . . . . . . . . . . . . . . . . . . 201
6.4
Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
6.5
Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
6.6
Division . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
6.7
Rounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
6.8
A Universal Rounding Unit . . . . . . . . . . . . . . . . . . . . . . . 211
6.9
Overﬂow and Underﬂow Treatment
. . . . . . . . . . . . . . . . . . 213
6.10 Algorithms Using the Short Accumulator
. . . . . . . . . . . . . . . 215
6.11 The Level 2 Operations . . . . . . . . . . . . . . . . . . . . . . . . . 222
7
Hardware Support for Interval Arithmetic
233
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
7.2
An Instruction Set for Interval Arithmetic . . . . . . . . . . . . . . . 234
7.2.1
Algebraic Operations . . . . . . . . . . . . . . . . . . . . . . 234
7.2.2
Comments on the Algebraic Operations . . . . . . . . . . . . 235
7.2.3
Comparisons and Lattice Operations . . . . . . . . . . . . . . 236
7.2.4
Comments on Comparisons and Lattice Operations . . . . . . 236
7.3
General Circuitry for Interval Operations and Comparisons . . . . . . 236
7.3.1
Algebraic Operations . . . . . . . . . . . . . . . . . . . . . . 236
7.3.2
Comparisons and Result-Selection . . . . . . . . . . . . . . . 240
7.3.3
Alternative Circuitry for Interval Operations and Comparisons 241
7.3.4
Hardware Support for Interval Arithmetic on X86-Processors
243
7.3.5
Accurate Evaluation of Interval Scalar Products . . . . . . . . 244

Contents
ix
8
Scalar Products and Complete Arithmetic
245
8.1
Introduction and Motivation
. . . . . . . . . . . . . . . . . . . . . . 246
8.2
Historic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
8.3
The Ubiquity of the Scalar Product in Numerical Analysis
. . . . . . 252
8.4
Implementation Principles
. . . . . . . . . . . . . . . . . . . . . . . 256
8.4.1
Long Adder and Long Shift
. . . . . . . . . . . . . . . . . . 257
8.4.2
Short Adder with Local Memory on the Arithmetic Unit . . . 258
8.4.3
Remarks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
8.4.4
Fast Carry Resolution
. . . . . . . . . . . . . . . . . . . . . 261
8.5
Scalar Product Computation Units (SPUs) . . . . . . . . . . . . . . . 263
8.5.1
SPU for Computers with a 32 Bit Data Bus . . . . . . . . . . 263
8.5.2
A Coprocessor Chip for the Exact Scalar Product . . . . . . . 266
8.5.3
SPU for Computers with a 64 Bit Data Bus . . . . . . . . . . 270
8.6
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
8.6.1
Rounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
8.6.2
How Much Local Memory Should be Provided on an SPU?
. 274
8.7
The Data Format Complete and Complete Arithmetic . . . . . . . . . 275
8.7.1
Low Level Instructions for Complete Arithmetic
. . . . . . . 277
8.7.2
Complete Arithmetic in High Level Programming Languages
279
8.8
Top Speed Scalar Product Units
. . . . . . . . . . . . . . . . . . . . 282
8.8.1
SPU with Long Adder for 64 Bit Data Word . . . . . . . . . . 282
8.8.2
SPU with Long Adder for 32 Bit Data Word . . . . . . . . . . 287
8.8.3
A FPGA Coprocessor for the Exact Scalar Product . . . . . . 290
8.8.4
SPU with Short Adder and Complete Register . . . . . . . . . 291
8.8.5
Carry-Free Accumulation of Products in Redundant Arithmetic 297
8.9
Hardware Complete Register Window . . . . . . . . . . . . . . . . . 297
III
Principles of Veriﬁed Computing
301
9
Sample Applications
302
9.1
Basic Properties of Interval Mathematics . . . . . . . . . . . . . . . . 304
9.1.1
Interval Arithmetic, a Powerful Calculus to Deal with
Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
9.1.2
Interval Arithmetic as Executable Set Operations . . . . . . . 305
9.1.3
Enclosing the Range of Function Values . . . . . . . . . . . . 311
9.1.4
Nonzero Property of a Function, Global Optimization . . . . . 314
9.2
Differentiation Arithmetic, Enclosures of Derivatives . . . . . . . . . 316
9.3
The Interval Newton Method . . . . . . . . . . . . . . . . . . . . . . 324
9.4
The Extended Interval Newton Method . . . . . . . . . . . . . . . . . 327
9.5
Veriﬁed Solution of Systems of Linear Equations . . . . . . . . . . . 329
9.6
Accurate Evaluation of Arithmetic Expressions . . . . . . . . . . . . 336

x
Contents
9.6.1
Complete Expressions . . . . . . . . . . . . . . . . . . . . . 336
9.6.2
Accurate Evaluation of Polynomials . . . . . . . . . . . . . . 337
9.6.3
Arithmetic Expressions
. . . . . . . . . . . . . . . . . . . . 341
9.7
Multiple Precision Arithmetics . . . . . . . . . . . . . . . . . . . . . 343
9.7.1
Multiple Precision Floating-Point Arithmetic . . . . . . . . . 344
9.7.2
Multiple Precision Interval Arithmetic
. . . . . . . . . . . . 347
9.7.3
Applications
. . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.7.4
Adding an Exponent Part as a Scaling Factor to Complete
Arithmetic
. . . . . . . . . . . . . . . . . . . . . . . . . . . 354
A Frequently Used Symbols
356
B
On Homomorphism
358
Bibliography
359
List of Figures
395
List of Tables
399
Index
401

Preface
This book deals with computer arithmetic in a more general sense than usual, and
shows how the arithmetic and mathematical capability of the digital computer can be
enhanced in a quite natural way. The work is motivated by the desire and the need
to improve the accuracy of numerical computing and to control the quality of the
computed result.
As a ﬁrst step towards achieving this goal, the accuracy requirements for the ele-
mentary ﬂoating-point operations as deﬁned by the IEEE arithmetic standard [644],
for instance, are extended to the customary product spaces of computation: the com-
plex numbers, the real and complex intervals, the real and complex vectors and ma-
trices, and the real and complex interval vectors and interval matrices. All computer
approximations of arithmetic operations in these spaces should ideally deliver a re-
sult that differs from the correct result by at most one rounding. For all these product
spaces this accuracy requirement leads to operations which are distinctly different
from those traditionally available on computers. This expanded set of arithmetic op-
erations is taken as a deﬁnition of what is called basic computer arithmetic.
Central to this treatise is the concept of semimorphism. It provides a mapping
principle between the mathematical product spaces and their digitally representable
subsets. The properties of a semimorphism are designed to preserve as many of the
ordinary mathematical laws as possible. All computer operations of basic computer
arithmetic are deﬁned by semimorphism.
The book has three antecedents:
(I) Kulisch, U. W., Grundlagen des numerischen Rechnens – Mathematische Be-
gr¨undung der Rechnerarithmetik, Bibliographisches Institut, Mannheim, Wien,
Z¨urich, 1976, 467 pp., ISBN 3-411-015617-9.
(II) Kulisch, U. W. and Miranker W. L., Computer Arithmetic in Theory and Practice,
Academic Press, New York, 1981, 249 pp., ISBN 0-12-428650-X.
(III) Kulisch, U. W., Advanced Arithmetic for the Digital Computer – Design of Arith-
metic Units, Springer-Verlag, Wien, New York, 2002, 139 pp., ISBN 3-211-
83870-8.
The need to deﬁne all computer approximations of arithmetic operations by semi-
morphism goes back to the ﬁrst of these books. By the time the second book had been
written, early microprocessors were on the market. They were made with a few thou-
sand transistors, and ran at 1 or 2 MHz. Arithmetic was provided by an 8-bit adder.
Floating-point arithmetic could only be implemented in software. In 1985 the IEEE
binary ﬂoating-point arithmetic standard was internationally adopted. Floating-point
arithmetic became hardware supported on microprocessors, ﬁrst by coprocessors and

xii
Preface
later directly within the CPU. Of course, all operations of basic computer arithmetic
can be simulated using elementary ﬂoating-point arithmetic. This, however, is rather
complicated and results in unnecessarily slow performance. A consequence of this is
that for large problems the high quality operations of basic computer arithmetic are
hardly ever applied. Higher precision arithmetic suffers from the same problem if it
is simulated by software.
Dramatic advances in speed and memory size of computers have been made since
1985. Today a computer chip holds more than one billion transistors and runs at
3 GHz or more. Results of ﬂoating-point operations can be delivered in every cycle.
Arithmetic speed has gone from megaﬂops to gigaﬂops, to teraﬂops, and to petaﬂops.
This is not just a gain in speed. A qualitative difference goes with it. If the numbers
a petaﬂops computer produces in one hour were to be printed (500 on one page, 1000
on one sheet, 1000 sheets 10 cm high) they would form a pile that reaches from the
earth to the sun and back. With increasing speed, problems that are dealt with become
larger and larger. Extending the word size cannot keep up with the tremendous in-
crease in computer speed. Computing that is continually and greatly speeded up calls
conventional computing into question. Even with quadruple and extended precision
arithmetic the computer remains an experimental tool. The capability of a computer
should not just be judged by the number of operations it can perform in a certain
amount of time without asking whether the computed result is correct. It should also
be asked how fast a computer can compute correctly to 3, 5, 10 or 15 decimal places.
If the question were asked that way, it would very soon lead to better computers.
Mathematical methods that give an answer to this question are available. Computers,
however, are not built in a way that allows these methods to be used effectively.
Computer arithmetic must move strongly towards more reliability in computation.
Instead of the computer being merely a fast calculating tool it must be developed into
a scientiﬁc instrument of mathematics. Two simple steps in this direction would have
great effect. They are both simple and practical:
I. fast hardware support for (extended1) interval arithmetic and
II. a fast and exact multiply and accumulate operation or, what is equivalent to it, an
exact scalar product.
These two steps together with basic computer arithmetic comprise what is here
called advanced computer arithmetic. Fast hardware circutries for I. and II. are de-
veloped in Chapters 7 and 8, respectively. This additional computational capability is
gained at very modest hardware cost. Besides being more accurate the new computer
operations greatly speed up computation. I. and II., of course, can be used to execute
and speed up the operations of basic computer arithmetic. This would boost both the
speed of a computation and the accuracy of its result.
1including division by an interval that includes zero

Preface
xiii
Advanced computer arithmetic opens the door to very many additional applications.
All these applications are extremely fast. I. and II. in particular are basic ingredients
of what is called validated numerics or veriﬁed computing.
This book has three parts. Part 1, of four chapters, deals with the theory of computer
arithmetic, while Part 2, also of four chapters, treats the implementation of arithmetic
on computers. Part 3, of one chapter, illustrates by a few sample applications how
advanced computer arithmetic can be used to compute highly accurate and mathemat-
ically veriﬁed results.
Part 1: The implementation of semimorphic operations on computers requires the
establishment of various isomorphisms between different deﬁnitions of arithmetic op-
erations on the computer. These isomorphisms are to be established in the mathemat-
ical spaces in which the actual computer operations operate. This requires a careful
study of the structure of these spaces. Their properties are deﬁned as invariants with
respect to semimorphisms. These concepts are developed in Part 1 of the book. Part 1
is organized along the lines of its second antecedent. However it differs in many de-
tails from the earlier one, details that spring from advances in computer technology,
and many derivations and proofs have been reorganized and simpliﬁed.
Part 2: In Part 2 of the book, basic ideas for the implementation of advanced com-
puter arithmetic are discussed under the assumption that the data are ﬂoating-point
numbers. Algorithms and circuits are developed which realize the semimorphic oper-
ations in the various spaces mentioned above. The result is an arithmetic with many
desirable properties, such as high speed, optimal accuracy, theoretical describability,
closedness of the theory, and ease of use.
Chapters 5 and 6 consider the implementation of elementary ﬂoating-point arith-
metic on the computer for a large class of roundings. A particular section of Chapter 5
comments on the IEEE ﬂoating-point arithmetic standard. The ﬁnal section of Chap-
ter 6 contains a brief discussion of all arithmetic operations deﬁned in the product sets
mentioned above as well as between these sets. The objective here is to summarize
the deﬁnition of these operations and to point out that they all can be performed as
soon as an exact scalar product is available in addition to the operations that have
been discussed in Chapters 5 and 6.
Floating-point operations with directed roundings are basic ingredients of interval
arithmetic. But with their isolated use in software interval arithmetic is too slow to
be widely accepted in the scientiﬁc computing community. Chapter 7 shows, in par-
ticular, that with very simple circuitry interval arithmetic can be made practically as
fast as elementary ﬂoating-point arithmetic. To enable high speed, the case selections
for interval multiplication (9 cases) and division (14 cases including division by an
interval that includes zero) are done in hardware where they can be chosen without
any time penalty. The lower bound of the result is computed with rounding down-
wards and the upper bound with rounding upwards by parallel units simultaneously.
The rounding mode needs to be an integral part of the arithmetic operation. Also the
basic comparisons for intervals together with the corresponding lattice operations and

xiv
Preface
the result selection in more complicated cases of multiplication and division are done
in hardware. There they are executed by parallel units simultaneously. The circuits
described in this chapter show that with modest additional hardware costs interval
arithmetic can be made almost as fast as simple ﬂoating-point arithmetic. Such high
speed cannot be obtained just by running many elementary ﬂoating-point arithmetic
processors in parallel.
A basic requirement of basic computer arithmetic is that all computer approxima-
tions of arithmetic in the usual product spaces should deliver a result that differs from
the correct result by at most one rounding. This requires scalar products of ﬂoating-
point vectors to be computed with but a single rounding. The question of how a scalar
product with a single rounding can be computed just using elementary ﬂoating-point
arithmetic has been carefully studied in the literature. A good summary and what is
probably the fastest solution is given in [456] and [531]. However, we do not follow
this line here. No software simulation can compete with a simple and direct hardware
solution.
The most natural way to accumulate numbers is ﬁxed-point accumulation. It is
simple, error free and fast. In Chapter 8 circuitry for exact computation of the scalar
product of two ﬂoating-point vectors is developed for different kinds of computers. To
make the new capability conveniently available to the user a new data format called
complete is used together with a few simple arithmetic operations associated with each
ﬂoating-point format. Complete arithmetic computes all scalar products of ﬂoating-
point vectors exactly. The result of complete arithmetic is always exact; it is complete,
not truncated. Not a single bit is lost. A variable of type complete is a ﬁxed-point
word wide enough to allow exact accumulation (continued summation) of ﬂoating-
point numbers and of simple products of such numbers.
If register space for the complete format is available complete arithmetic is very
very fast. The arithmetic needed to perform complete arithmetic is not much different
from what is available in a conventional CPU. In the case of the IEEE double precision
format a complete register consists of about 1/2 K bytes. Straightforward pipelining
leads to very fast and simple circuits. The process is at least as fast as any conventional
way of accumulating the products including the so-called partial sum technique on
existing vector processors which alters the sequence of the summands and causes
errors beyond the usual ﬂoating-point errors.
Complete arithmetic opens a large ﬁeld of new applications. An exact scalar prod-
uct rounded into a ﬂoating-point number or a ﬂoating-point interval serves as build-
ing block for semimorphic operations in the product spaces mentioned above. Fast
multiple precision ﬂoating-point and multiple precision interval arithmetic are other
important applications. All these applications are very very fast. Complete arithmetic
is an instrumental addition to ﬂoating-point arithmetic. In many instances it allows
recovery of information that has been lost during a preceding pure ﬂoating-point com-
putation.

Preface
xv
Because of the many applications of the hardware support for interval arithmetic
developed in Chapter 7, and of the exact scalar product developed in Chapter 8, these
two modules of advanced computer arithmetic emerge as its central components.
Fast hardware support for all operations of advanced computer arithmetic is a
fundamental and overdue extension of elementary ﬂoating-point arithmetic. Arith-
metic operations which can be performed correctly with very high speed and at low
cost should never just be done approximately or simulated by slow software. The mi-
nor additional hardware cost allows their realization on every CPU. The arithmetic
operations of advanced computer arithmetic transform the computer from a fast cal-
culating tool into a mathematical instrument.
Part 3: Mathematical analysis has provided algorithms that deliver highly accurate
and completely veriﬁed results. Part 3 of the book goes over some examples. Such
algorithms are not widely used in the scientiﬁc computing community because they
are very slow when the underlying arithmetic has to be carried out on conventional
processors.
The ﬁrst section describes some basic properties of interval mathematics and shows
how these can be used to compute the range of a function’s values. Used with au-
tomatic differentiation, these techniques lead to powerful and rigorous methods for
global optimization. The following section then deals with differentiation arithmetic
or automatic differentiation. Values or enclosures of derivatives are computed directly
from numbers or intervals, avoiding the use of a formal expression for the derivative
of the function. Evaluation of a function for an interval X delivers a superset of the
function’s values over X. This overestimation tends to zero with the width of the
interval X. Thus for small intervals interval evaluation of a function practically de-
livers the range of the functions’s values. Many numerical methods proceed in small
steps. So this property together with differentiation arithmetic to compute enclosures
of derivatives is the key technique for validated numerical computation of integrals
and for solution of differential equations, and for many other applications.
Newton’s method is considered in two sections of Chapter 9. It attains its ultimate
elegance and power in the extended interval Newton method, which is globally con-
vergent and computes all zeros of a function in a given domain. The key to achieving
these fascinating properties is division by an interval that includes zero.
The basic ideas needed for veriﬁed solution of systems of linear equations are de-
veloped in Section 9.5. Highly accurate bounds for a solution can be computed in
a way that proves the existence and uniqueness of the solution within these bounds.
Mathematical ﬁxed-point theorems, interval arithmetic combined with defect correc-
tion or iterative reﬁnement techniques using complete arithmetic are basic tools for
achieving these results.
In Section 9.6 a method is developed that allows highly accurate and guaranteed
evaluation of polynomials and of other arithmetic expressions.

xvi
Preface
Section 9.7 ﬁnally shows how fast multiple precision arithmetic and multiple pre-
cision interval arithmetic can be provided using complete arithmetic and other tools
developed in the book.
Of course, the computer may often have to work harder to produce veriﬁed results,
but the mathematical certainty makes it worthwhile. After all, the step from assembler
to higher programming languages or the use of convenient operating systems also
consumes a lot of computing power and nobody complains about it since it greatly
enlarges the safety and reliability of the computation.
Computing is being continually and greatly speeded up. An avalanche of num-
bers is produced by a teraﬂops or petaﬂops computer (1 teraﬂops corresponds to 1012
ﬂoating-point operations per second). Fast computers are often used for safety critical
applications. Severe, expensive, and tragic accidents can occur if the eigenfrequencies
of a large electricity generator, for instance, are erroneously computed, or if a nuclear
explosion is incorrectly simulated. Floating-point operations are inherently inexact.
It is this inexactness at very high speed that calls conventional computing, just using
na¨ıve ﬂoating-point arithmetic, into question.
This book can, of course, be used as a textbook for lectures on the subject of com-
puter arithmetic. If one is interested only in the more practical aspects of implement-
ing arithmetic on computers, Part 2, with acceptance a priori of some results of Part 1,
is also suitable as a basis for lectures. Part 3 can be used as an introduction to veriﬁed
computing.
The second previous book was jointly written with Willard L. Miranker. On this
occasion Miranker was very busy with other studies and could not take part, so this
new book has been compiled solely by the other author and he takes full responsibility
for its text. However, there are contributions and formulations here which go back to
Miranker without being explicitly marked as such. I deeply thank Willard for his
collaboration on the earlier book as well as on other topics, and for a long friendship.
Contact with him was always very inspiring for me and for my Institute.
I would like to thank all former collaborators at my Institute. Many of them have
contributed to the contents of this book, have realized advanced computer arithmetic
in software on different platforms and in hardware in different technologies, have
embedded advanced computer arithmetic into programming languages and imple-
mented corresponding compilers, developed problem solving routines for standard
problems of numerical analysis, or applied the new arithmetic to critical problems in
the sciences. Among these colleagues are: Christian Ullrich, Edgar Kaucher, Rudi
Klatte, Gerd Bohlender, Dalcidio M. Claudio, Kurt Gr¨uner, J¨urgen Wolff von Guden-
berg, Reinhard Kirchner, Michael Neaga, Siegfried M. Rump, Harald B¨ohm, Thomas
Teufel, Klaus Braune, Walter Kr¨amer, Frithjof Blomquist, Michael Metzger, G¨unter
Schumacher, Rainer Kelch, Wolfram Klein, Wolfgang V. Walter, Hans-Christoph
Fischer, Rudolf Lohner, Andreas Kn¨ofel, Lutz Schmidt, Christian Lawo, Alexan-
der Davidenkoff, Dietmar Ratz, Rolf Hammer, Dimitri Shiriaev, Manfred Schlett,

Preface
xvii
Matthias Hocks, Peter Schramm, Ulrike Storck, Christian Baumhof, Andreas Wi-
ethoff, Peter Januschke, Chin Yun Chen, Axel Facius, Stefan Dietrich, and Norbert
Bierlox. For their contributions I refer to the bibliograhy.
I owe particular thanks to Axel Facius, to Gerd Bohlender, and to Klaus Braune.
Axel Facius keyed in and laid out the entire manuscript in LATEX and he did most of
the drawings. Drawings were also done by Gerd Bohlender. Klaus Braune helped to
prepare the ﬁnal version of the text. I thank Bo Einarsson for proofreading the book.
I also thank my colleagues at the institute G¨otz Alefeld and Willy D¨orﬂer for their
support of the book project.
I gratefully acknowledge the help of Neville Holmes who went carefully through
great parts of the manuscript, sending back corrections and suggestions that led to
many improvements. His help was indeed vital for the completion of the book.
The Karlsruher Universit¨atsgesellschaft supported typing the ﬁrst draft of the
manuscript.
Karlsruhe, November 2007
Ulrich W. Kulisch


Introduction
In general, a digital computer is understood to be an engine that deals with data, data
that are stored as binary digits or bits. The bits stored in a computer can be used to
represent all kinds of abstract or concrete objects, objects such as whole numbers,
letters of an alphabet, the books in a library, the accounts in a bank, the inhabitants
of a town, the text of a law or the features of a disease. A user can get the computer
to process the stored data, for example, by translation, searching or sorting. If the
computer is functioning correctly then its processing is free of error. A computer that
is programmed properly is therefore generally considered to be an infallible tool.
It is quite ironical that this view is often not justiﬁed when the digital computer is
used for its original purpose – numerical or scientiﬁc computation. This book treats
the arithmetic processing of real numbers, reviews their properties, and describes how
computational error can be reduced and avoided by special design of the computer’s
arithmetic.
Mathematics uses two methods to deﬁne the real numbers. The ﬁrst – occasionally
called the genetic method – begins by deﬁning the natural numbers, for instance using
the so-called Peano axioms. Then in turn the integral, the rational, the irrational (real)
and the complex numbers are deﬁned. The integral numbers (integers) for which
subtraction is always possible are deﬁned as pairs of natural numbers, the rational
numbers as pairs of integers. The complex numbers for which the square root always
exists are deﬁned as pairs of real numbers.
The step from the rational numbers to the real, however, differs essentially from
the other extensions. It can not be done by considering pairs of rational numbers.
Different methods are used to deﬁne the real numbers. They are logically equivalent.
The best-known genetic ways to describe the real numbers are as Dedekind cuts in the
set of rational numbers, and as fundamental sequences of rational numbers. In both
cases inﬁnitely many rational numbers are needed to deﬁne a real number.
The second method deﬁnes the real numbers as a conditionally complete, linearly
ordered ﬁeld, as described below. It is then shown that any two conditionally com-
plete, linearly ordered ﬁelds are isomorphic. This means that there can be at most one
such ﬁeld. The genetic method, sketched above, shows that one does exist and can be
constructed.
Thus the real numbers as deﬁned by the genetic method essentially represent the
only realization of a conditionally complete, linearly ordered ﬁeld. The axioms of this
ﬁeld form the basis of the huge body of analysis and its applications.
In axiomatic geometry the nature of a point or a straight line is irrelevant. All
that matters is the relation between points, straight lines and so on, as deﬁned by the

2
Introduction
axioms. Similarly, analysis is an abstraction in which the nature of a real number
is irrelevant. All that matters is the relation between real numbers as deﬁned by the
axioms.
Pure mathematics is pure largely because the huge corpus of analysis can be put
together in an abstract manner by using only the axioms and without using concrete
models. Numbers with special properties are represented by symbols like e or π. The
real numbers of the genetic method, or the decimal numbers of everyday life, only
serve as proof of existence of nontrivial models or as raw material for exercises.
The situation is very different in applied mathematics. Here, the value of a real
number has importance far beyond its symbolic representation or underlying abstract
theory. Any programmer knows that the name of a number (its address in storage) and
its value (the contents of the storage cell) are quite distinct, and that the value of any
number is represented in a speciﬁc numbering system.
Applied mathematics, therefore, though starting from axiomatic real analysis, also
has to consider the representation of real numbers, algorithms for deriving their rep-
resentation, and algorithms for computation with those representations. Of course,
the well-known algorithms for operations on decimally represented numbers, for in-
stance, encompass all the rules of a conditionally complete, linearly ordered ﬁeld.
However this does not mean that only these algorithms are needed for the successful
use of real analysis.
Abstract methods can greatly simplify developments and proofs. They allow work
to focus on the essence of a problem. Whenever possible concrete number represen-
tations should only be used when abstract methods have been exhausted.
What holds for the real numbers is very much the same for the so-called ﬂoating
point numbers used in computers, and which will be deﬁned later. Here also it is
extremely useful to contrast an abstract, axiomatic deﬁnition suitable for theoretical
studies, and particular representations suitable for calculation.
Real numbers are usually represented by “power” systems, most familiarly with
decimal powers. Here equality and the order relation are simple to deﬁne. The arith-
metic operations, however, cause difﬁculties.
A real number may only be exactly representable by an inﬁnite b -adic expansion2,
always if it is irrational, often even if it is rational. Such inﬁnite expansions cannot
be exactly added or multiplied. Operations for ﬁnite b -adic expansions, however,
can easily be carried out using simple algorithms based on single digit operations.
Increasing the number of digits of a ﬁnite expansion used to approximately represent
inﬁnite b -adic expansions can make the error in the result of arithmetic operations, in
principle, arbitrarily small.
Operations for inﬁnite b -adic expansions are deﬁned as the limit of the sequence
of results obtained by operating on increasingly large preﬁxes of those expansions.
In principle, a computer could approximate this limiting process. In practice, the
2for deﬁnition see Chapter 5

Introduction
3
obvious inefﬁciency of such an approach discourages its serious implementation even
on the fastest computers. (Note, however, that in the old days of Fortran people used
to run programs ﬁrst with single precision reals, then with double precision, then, if
the results disagreed, with extended precision.)
Numerical computing, as it is practised today, aims for reasonable and useful ap-
proximations, not for exact or arbitrarly precise results. On computers the real num-
bers are approximated by a subset on which all operations are simply and rapidly
carried out. The most common choice for this subset is ﬂoating-point numbers with
a ﬁxed number of digits in the mantissa. (Frequently the word signiﬁcand is used
instead of mantissa).
The task of numerical analysis is to develop and design algorithms which use
ﬂoating-point numbers to deliver a reasonably good approximation to the exact re-
sult. An essential part of this task is to quantify the error of the computed answer.
Managing this quite natural error is the crucial challenge of numerical or scientiﬁc
computing. In this respect, numerical analysis is completely irrelevant to everyday
applications of computers like those mentioned in the opening paragraph of this In-
troduction. For solving problems of this kind, integer arithmetic, which is exact, is
used, or should be, whenever arithmetic is needed.
When early ﬂoating-point arithmetic was being developed, its design was often
governed by objectives such as simpliﬁcation of its circuitry, ease of programming,
and the nature of the technology available. This often added costs and difﬁculties for
the user.
The computer, however, should be a mathematical tool, with its numerical proper-
ties and arithmetic operations precisely and mathematically deﬁned. These properties
and operations should be simple to describe and easy to understand so that both the
designers and the users of computers can work with a more complete knowledge of
the computational process.
The concept of semimorphism is central to the approach to computer arithmetic
described in this book. The various models described here lead directly to an under-
standing of the properties of semimorphism. These properties embody an ordering
principle that allows the entire treatment of arithmetic in this book to be understood
and developed in close analogy to geometry. The properties of a geometry can be
deﬁned and derived as invariants with respect to a certain group of transformations.
In much the same way, mathematical properties of a computer will here be deﬁned
as invariants with respect to semimorphisms. The algorithms and hardware circuits
for the arithmetic operations in various spaces describe the implementation of semi-
morphisms on computers. The result is an arithmetic with many desirable properties
– high speed, optimal accuracy, theoretical describability, closedness of the theory,
usability, applicability, and so on. This similarity to geometry allows computer arith-
metic to be presented here in a complete and well-rounded way.
Algorithms and circuits for computer arithmetic are implemented in a way that is

4
Introduction
natural for the nonspecialist. Doing this helps to avoid those misunderstandings of the
computer by its users that are caused by tricky implementations.
Directed roundings and interval operations are essential to computer arithmetic.
Intervals bring the continuum to the computer. The two ﬂoating-point bounds of an
interval represent the continuous set of real numbers between them.
Not only has the development of interval arithmetic had a great effect on the the-
oretical understanding of arithmetic on the computer, but the use of intervals is nec-
essary to the proper control of rounding errors. Interval arithmetic can guarantee the
result of a computation, and thus permit use of the computer for veriﬁcation and de-
cidability. Such questions cannot be answered by using arithmetic which rounds in
the traditional simple manner. And even with more sophisticated circuits, an interval
operation can be made as fast as its corresponding ﬂoating-point operation.
The spaces in which numerical algorithms are usually deﬁned and studied are the
basis of the computer arithmetic described here. All computation begins with integers
and real numbers. Thus numerical algorithms are usually deﬁned in the space R
of real numbers and the vectors V R or matrices MR over the real numbers. The
corresponding complex spaces C, V C, and MC are sometimes also used. All these
spaces are ordered by the relation ≤. In C and in the vector and matrix spaces ≤is
deﬁned componentwise.
For decidability and error control, numerical mathematics also considers algo-
rithms for intervals in these spaces. If we denote the set of intervals over an ordered
set {M, ≤} by IM, we obtain the spaces IR, IV R, IMR, and IC, IV C, and IMC.
In Figure 1, to which we shall repeatedly refer, a table of spaces is presented. The
second column of this ﬁgure lists the various spaces, introduced above, in which arith-
metic is deﬁned. Algorithms derived and deﬁned in these spaces, however, cannot be
used on a computer, in which only ﬁnite subsets can be represented. For the real
numbers R the so-called ﬂoating-point numbers with a ﬁxed number of digits in the
mantissa are commonly used as the subset S. If computation within S is not accurate
enough, a larger subset D of R is often used, where R ⊃D ⊃S.
Arithmetic must be applicable to the various vector, matrix and interval spaces, as
they extend S and D as well as R. This yields the ﬁnite spaces VS, MS, IS, IVS,
IMS, CS, V CS, MCS, ICS, IV CS, and IMCS, and the corresponding spaces over
D. These spaces are listed in the third and fourth columns of Figure 1. Thus CS is
the set of all pairs of S, V CS is the set of all n-tuples of such pairs, ICS is the set of
all intervals over the ordered set {CS, ≤}, and so forth.
In practice, ﬂoating-point numbers of single and double length are used for the sets
S and D. However, in Figure 1 and in the following, S and D are just symbols for
generic subsets of R with arithmetic properties to be deﬁned.
With the sets of the third and fourth columns of Figure 1 deﬁned, we turn now to the
question of deﬁning the operations on these sets. These operations need to approx-
imate those that are deﬁned on the corresponding sets listed in the second column.

Introduction
5
I
II
III
IV
R
⊃
D
⊃
S
V R
⊃
VD
⊃
VS
MR
⊃
MD
⊃
MS
PR
⊃
IR
⊃
ID
⊃
IS
PV R
⊃
IV R
⊃
IVD
⊃
IVS
PMR
⊃
IMR
⊃
IMD
⊃
IMS
C
⊃
CD
⊃
CS
V C
⊃
V CD
⊃
V CS
MC
⊃
MCD
⊃
MCS
PC
⊃
IC
⊃
ICD
⊃
ICS
PV C
⊃
IV C
⊃
IV CD
⊃
IV CS
PMC
⊃
IMC
⊃
IMCD
⊃
IMCS
Figure 1. Collection of spaces used in numerical computation.
Furthermore, the lines in Figure 1 are not arithmetically independent of each other.
For instance: a vector can be multiplied by a scalar as well as by a matrix; an interval
vector can by multiplied by an interval as well as by an interval matrix.
This leads to the following preliminary and informal deﬁnition of basic computer
arithmetic:
Basic computer arithmetic comprises all the needed operations deﬁned
on all the sets listed in the third and fourth columns of Figure 1 as well
as on all combinations of those sets.
The sets S and D may, for instance, be thought of as ﬂoating-point numbers of
single and double mantissa length. In a programming system data types which might
be called real, complex, interval, cinterval, rvector, cvector,
ivector, civector, and so on, would correspond to the spaces of the third and
fourth columns of the ﬁgure. In a good programming system, operations for these
spaces should be available for all combinations of data types within either column.
The ﬁrst part of this book develops deﬁnitions for these many operations and also
speciﬁes simple structures as settings for them. The second part then deals with the
implementation of these operations on computers. In particular, it is shown that the
operations can be built up modularly from relatively few fundamental algorithms and
routines. These can be implemented in fast hardware circuits.
Arithmetic operations can now be deﬁned for all the sets in the third and fourth and
possibly more such columns of Figure 1.

6
Introduction
Let M be one of these sets and let us assume that certain operations and relations
are deﬁned for its elements. Further, let M be the rules (axioms) given for the ele-
ments of M. The commutative law of addition might be one such rule. Then we call
the pair {M, M} a structure and refer to it as the structure of the set M. The structures
of the sets R, V R, MR, C, V C, and MC are well known. Now let M be one of these
sets and ◦one of the operations deﬁned on M. Let PM be the power set of M, the
set of all subsets of M. The operation ◦on M can be extended to the power set PM
by the following deﬁnition3:

A,B∈PM
A ◦B := {a ◦b | a ∈A ∧b ∈B}.
If we apply this deﬁnition to all operations ◦of M, the structure of the power set
{PM, PM} can be derived from the structure {M, M}. By this application, arith-
metic operations and a corresponding structure are deﬁned on the power sets of the
spaces R, V R, MR, C, V C, and MC, the sets of the ﬁrst column of Figure 1.
In brief, the operations and the structure {M, M} of the leftmost space of every
row in Figure 1 are always known. These operations and structures can be used to
deﬁne and derive the operations and structures of the thirty remaining spaces. This
need not be done for each subset space individually. Instead we do this for all the
rows of Figure 1 at once by using an abstract mapping principle. The same principle
would be applied if there would be more columns at the right in the table of Figure 1.
Approximating a given structure {M, M} by a structure {N, N} might be tried
in such a way that the mapping has properties like isomorphism or homomorphism.
However, N as a subset of M can be of different cardinality. There are inﬁnitely many
elements in the sets of the ﬁrst two columns of Figure 1 but a ﬁnite number in those of
the last two columns. There can be no one-to-one mapping, and thus no isomorphism,
between sets of different cardinality.
But even a homomorphism would preserve the structure of a group, a ring, or a
vector space. Simple examples can be used to show that, in the case of ﬂoating-point
numbers, no sensible homomorphism can be realized. It would be well, however, to
stay as close to homomorphism as possible. We shall see later that the following four
properties can be realized for all the set/subset pairs of Figure 1:
If M is a space in Figure 1 with N an adjacent subset to its right, then for every
arithmetic operation ◦in M an operation
◦in N is deﬁned by
(RG)

a,b∈N
a ◦b :=
(a ◦b),
where
: M →N is a mapping from M onto N which is called a rounding if it has
the following properties:
(R1)

a∈N
a = a,
3See Appendix A for the deﬁnition of the symbol V.

Introduction
7
(R2)

a,b∈M
(a ≤b ⇒
a ≤
b).
Many roundings have the additional property
(R4)

a∈M
(−a) = −
a.
These properties (RG), (R1), (R2), and (R4) can be shown to be necessary
conditions for a homomorphism between ordered algebraic structures {M, M} and
{N, N}. We therefore call a mapping with these properties a semimorphism. In a
semimorphism, (RG) deﬁnes the approximating operations
◦in the subset N for all
operations ◦in M. (R1) is a very natural property for a rounding.
(R2) imposes ordering on the semimorphism, so the rounding is monotone, and
(R4) makes it antisymmetric.
Although the properties (R1), (R2), and (R4) do not deﬁne the rounding
: M →
N uniquely, the semimorphism is responsible not only for the mapping of the elements
but also for the resulting structure {N, N}. This means that the set of rules N valid in
N depends essentially on the properties of semimorphism. Indeed, N can be deﬁned
as the set of rules that is invariant with respect to a semimorphism, that is, N ⊆
M, and the structure {N, N} is a generalization of {M, M}. If we consider the
mapping between the second and third columns in any row of Figure 1, we get a proper
generalization N ⊂M. Going from the third to the fourth column – and possibly
further – we always have N = M. We shall see later that the structures of ordered
or weakly ordered or isotonely ordered ringoids or vectoids represent the properties
that are invariant with respect to semimorphism. The ringoid is a generalization of the
mathematical structure of a ring and the vectoid of that of a vector space.
That semimorphisms preserve reasonable mathematical structures is another strong
reason for using semimorphisms to deﬁne all arithmetic operations in all subsets of
Figure 1.
For the power set and the interval spaces in Figure 1 the order relation in (R2) is
the subset relation ⊆. A rounding from any power set or interval set M onto its subset
N is deﬁned by properties (R1), (R2) (with ⊆replacing ≤), and also by
(R3)

a∈M
a ⊆
a.
These set and interval roundings are also antisymmetric, having the property (R4).
Additional important roundings from the real numbers onto the ﬂoating-point num-
bers are the monotone roundings directed downward ▽and upward △. These di-
rected roundings are uniquely deﬁned by (R1), (R2) and (R3). Arithmetic operations
are also deﬁned for these roundings by (RG).
With the ﬁve rules (RG) and (R1,2,3,4), many arithmetic operations are deﬁned
in the computer representable subsets of the twelve spaces in the second column of
Figure 1.
A deﬁnition of computer arithmetic is only practical if it can be implemented by
fast routines and circuitry. We shall later show special hardware circuits which prove

8
Introduction
not only that all the semimorphisms of Figure 1 are indeed practical, but also that all
the arithmetic operations can be realized with the highest possible accuracy. In the
product spaces many of these operations are not at all like those based on elementary
ﬂoating-point arithmetic.
For ﬂoating-point numbers it will later be shown that all the arithmetic operations
of the third and fourth columns of Figure 1 can be made very fast from a few modular
building blocks. These building blocks implement the ﬁve operations +, −, ×, /, ·,
each with the three roundings
, ▽, △, and with the case selections needed for in-
terval multiplication and division. Here · means the scalar product of two vectors,
is an antisymmetric monotone rounding like rounding to nearest, and ▽and △
are the downward and upward monotone roundings from the real numbers into the
ﬂoating-point numbers. All the ﬁfteen operations
◦, ▽
◦, △
◦with ◦∈{+, −, ×, /, ·}
of Figure 2 are deﬁned by (RG). In case of the scalar product, a and b are the vectors
a = (ai), b = (bi) with a ﬁnite number of elements.
+ ,
−,
× ,
/ ,
· ,
a · b =
n
i=1
ai · bi,
▽
+ ,
▽
−,
▽
× ,
▽
/ ,
▽· ,
a▽· b = ▽
n
i=1
ai · bi,
△
+ ,
△
−,
△
× ,
△
/ ,
△· ,
a△· b = △
n
i=1
ai · bi.
Figure 2. The ﬁfteen fundamental computer operations.
Fast hardware circuitry for all these operations is developed in Part 2.
The mapping principle of a semimorphism can also be derived directly from special
models of the sets in Figure 1. For instance, consider the mapping of the power set
of the complex numbers PC into the complex number intervals IC. An interval [a, b]
of two complex numbers a and b with a ≤b is a rectangle in the complex plane with
sides parallel to the axes. Performing an operation ◦for two complex intervals A and
B as power set elements (see Figure 3) does not typically yield an interval result, but
the more speciﬁc result A ◦B in the power set PC.
On a computer the result of an interval operation must be an interval. The power
set result A ◦B must therefore be mapped onto the least interval that contains it, a
rectangle as shown in Figure 3. This mapping
: PC →IC is a rounding having
the properties (R1,2,4) and (RG) of a semimorphism, and its order relation is the set
inclusion ⊆. If the set A ◦B is already an interval the rounding has no effect, so (R1)
holds. If we extend the set A ◦B this enlarges the least interval that includes it, so
(R2) holds. (R4) holds by symmetry. The result fulﬁlls (RG) A ◦B :=
(A ◦B),
by construction. Also, the rounding
: PC →IC has the property (R3) of being
upwardly directed which means that A ⊆
A for all A ∈PC. We shall see later that

Introduction
9
C
B
A = [a, b]
A ◦B
Figure 3. Operation for complex intervals.
the properties (R1,2,3) deﬁne the rounding
uniquely.
A is the least interval that
includes A.
Another example is the basic pair of spaces R and D of Figure 1. If we add two
ﬂoating-point numbers a and b (row 1), then the exact sum a + b is not in general a
ﬂoating-point number in D (Figure 4), and the result must be rounded into D. Fig-
ure 4 shows that the process of rounding conforms to (R1), (R2) and (R4). The order
relation is ≤. Floating-point addition is deﬁned by (RG): a + b :=
(a + b).
R
b
a
0
c = a + b
c = a + b
Figure 4. Floating-point addition.
The operations for other pairs of the spaces in Figure 1 are not easy to illustrate
simply, but semimorphism provides a mapping on which the implementation of very
fast and accurate algorithms and hardware circuits for all those operations can be
based.
Property (RG) of a semimorphism requires that every operation be performed in
such a way that it produces the same result as if the mathematically correct operation
were performed in the basic space M and the result rounded into the computer repre-
sentable subset N. Unlike conventional ﬂoating-point arithmetic’s approximation of
the arithmetic operations in the product spaces (complex numbers, vectors, matrices),
all operations deﬁned by semimorphism are optimal because no better computer rep-
resentable approximation to the true result (with respect to the prescribed rounding) is
possible. In other words, between the exact and the computed results of an operation

10
Introduction
there is no other element of the corresponding computer representable subset. This
can easily be shown. If a, b ∈N and α, β ∈N are the highest lower and lowest upper
bound respectively of the correct result a ◦b for any operation ◦in M so that
α ≤a ◦b ≤β
then
α =
(R1) α ≤
(R2)
(a ◦b) =
(RG) a ◦b ≤
(R2)
β =
(R1) β.
Thus, all semimorphic computer operations are accurate to within 1 ulp (unit in the
last place). Half ulp accuracy is achieved by rounding to nearest. In the product spaces
the rounding is deﬁned componentwise so this property holds for every component.
Thus the deﬁnition of arithmetic operations by semimorphisms in all subsets of
Figure 1 results in computer arithmetic with optimal accuracy. This in turn simpli-
ﬁes the error analysis of numerical algorithms. Further, the following compatibility
properties hold between the structures {M, M} and {N, N} for all operations ◦in
M:
(RG1)

a,b∈N
(a ◦b ∈N ⇒a ◦b = a ◦b),
(RG2)

a,b,c,d∈N
(a ◦b ≤c ◦d ⇒a ◦b ≤c ◦d),
(RG4)

a∈N
−a =
−a := (−e) · a.
For deﬁnition of e see Section 1.4. Also, if the rounding
: M →N is upwardly
directed then property (RG3) holds:
(RG3)

a,b∈N
a ◦b ≤a ◦b.
In summary we remark that
the formulas (RG), (R1), (R2), (R3), and (R4) for semimorphisms can
and should be used as an axiomatic deﬁnition of computer arithmetic
in the context of programming languages.
A programmer should know just what the result will be of any arithmetic operation
used in a program.
Basic computer arithmetic will be extended to what is called advanced computer
arithmetic in Chapters 7 and 8 of the book.

Part I
Theory of Computer Arithmetic

Chapter 1
First Concepts
In this chapter we give an axiomatic characterization of the essential prop-
erties of the sets and subsets shown in Figure 1. We then deﬁne the notion
of rounding from a set M onto a subset N and study the key properties of
certain special roundings and their interactions with simple arithmetic oper-
ations. To accomplish this, we employ several lattice theoretic concepts that
are developed at the beginning of this chapter.
1.1
Ordered Sets
As part of analysis the spaces listed in column two of Figure 1 carry three kinds
of mathematical structures: an algebraic structure, a topological or metric structure,
and an order structure. These are coupled by certain compatibility properties, for
instance: a ≤b ⇒a + c ≤b + c. When mapped on computer representable subsets
by a rounding these structures are considerably weakened. Since the rounding is a
monotone function the changes to the order structure are minimal. Thus the order
structure plays a key role and is of particular importance for the arguments in this
book. Therefore we begin this chapter by listing a few well-known concepts and
properties of ordered sets.
Deﬁnition 1.1. A relation ≤in a set M is called an order relation, and {M, ≤} is
called an ordered set if we have
(O1)

a∈M
a ≤a
(reﬂexivity),
(O2)

a,b,c∈M
(a ≤b ∧b ≤c ⇒a ≤c)
(transitivity),
(O3)

a,b∈M
(a ≤b ∧b ≤a ⇒a = b)
(antisymmetry).
An ordered set is called linearly or totally ordered if in addition
(O4)

a,b∈M
a ≤b ∨b ≤a.
■
{M, ≤} being an ordered set just means that there is an relation deﬁned in M. It
does not mean that for any two elements a, b ∈M either a ≤b or b ≤a holds. The
latter is valid in linearly ordered sets. If for two elements a, b ∈M neither a ≤b
nor b ≤a, then a and b are called incomparable, in notation a∥b. Ordered sets are
sometimes also called partially ordered sets. Since we shall consider many special

1.1 Ordered Sets
13
partially ordered sets, we will suppress the modiﬁer to avoid bulky expressions. This
practice is also quite common in the literature.
If {M, ≤} is an ordered (resp. a linearly ordered) set and T ⊆M, then {T, ≤} is
also an ordered (resp. a linearly ordered) set.
To each pair {a, b} of elements in an ordered set {M, ≤}, where a ≤b, the interval
[a, b] is deﬁned by
[a, b] := {x ∈M | a ≤x ≤b}.
A subset T ⊆M is called convex if with two elements a, b ∈T, the whole interval
[a, b] ∈T. The smallest convex superset of a set is called its convex hull.
Deﬁnition 1.2. A relation < in a set M is called antireﬂexive and the pair {M, <} an
antireﬂexively ordered set if we have
(AO1)

a∈M
¬(a < a)
(antireﬂexivity),
(AO2)

a,b,c∈M
(a < b ∧b < c ⇒a < c)
(transitivity).
■
The following lemma describes a well-known relation between the orderings of the
Deﬁnitions 1.1 and 1.2.
Lemma 1.3. (a) In an ordered set {M, ≤} an antireﬂexive ordering is deﬁned by
a < b
:⇔
(a ≤b ∧a ̸= b).
(b) In an antireﬂexively ordered set {M, <} an order relation is deﬁned by
a ≤b
:⇔
(a < b ∨a = b).
■
The proof of this lemma, being straightforward, is omitted. We note that throughout
this book we shall use the sign < in an ordered set {M, ≤} and the sign ≤in an
antireﬂexively ordered set {M, <} in the sense of Lemma 1.3. This is not always the
case in the literature.
We list a few well-known examples of ordered sets:
Examples. 1. The set {R, ≤} of real numbers is a linearly ordered set.
2. If M is a set and PM denotes the power set of M, which is deﬁned as the set of all
subsets of M, then with inclusion as an order relation {PM, ⊆} is an ordered set, and
we have the properties
(O1)

A∈PM
A ⊆A,
(O2)

A,B,C∈PM
(A ⊆B ∧B ⊆C ⇒A ⊆C),
(O3)

A,B∈PM
(A ⊆B ∧B ⊆A ⇒A = B).

14
1 First Concepts
3. If {M, ≤} is an ordered set and Mn denotes the product set, which is deﬁned as
the set of all n-tuples of elements of M and
x := (xi) :=
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠,
y := (yi) :=
⎛
⎜
⎜
⎜
⎝
y1
y2
...
yn
⎞
⎟
⎟
⎟
⎠∈Mn,
then by the deﬁnition
x ≤y
:⇔

i=1(1)n
xi ≤yi,
{Mn, ≤} becomes an ordered set.
In an ordered set {M, ≤} an element a is called the lower neighbor of an element
b if a < b and no other element of M lies between them; i.e., there exists no element
c ∈M such that a < c < b. The concept of lower neighbor can be used to draw a
ﬁgure of any ﬁnite ordered set. We just have to assign every element of M to a point
of the plane and place a lower than b whenever a < b. Then we connect every point
to each of its lower neighbors by a straight line segment. The resulting ﬁgure is called
the order diagram of the ordered set {M, ≤}.
Figure 1.1 shows the order diagram of ordered sets of 9 and 16 elements. The
set of Figure 1.1 (b) consists of two subsets, the respective elements of which are
incomparable.
(a)
(b)
Figure 1.1. Order diagrams.
An element x of an ordered set {M, ≤} is called a maximal (resp. minimal) element
if

a∈M
¬(x < a)

resp.

a∈M
¬(a < x)

.
Every ﬁnite ordered set has at least one maximal and one minimal element. To see
this, let M = {a1, a2, . . . , an} and set x1 = a1. Set x2 = a2 if a1 < a2, but set
x2 = x1 otherwise. In general, set xk = ak if xk−1 < ak and xk = xk−1 otherwise.

1.1 Ordered Sets
15
Then xn is a maximal element. An ordered set may have more than one maximal
or minimal element. See Figure 1.1. An inﬁnite set, however, need have neither a
maximal nor a minimal element.
Now we can show that every ﬁnite, nonvoid ordered set can be represented by an
order diagram. If M = {a}, this is clear. Let us assume now that the statement is true
for sets with n −1 elements, and let M be a set with n elements. Since M consists
of a ﬁnite number of elements, it has at least one maximal element. Call S ⊆M
the subset of M containing n −1 elements, which is obtained by removing one such
maximal element. S has an order diagram. Now we join the element a to the order
diagram of S and connect it with all its lower neighbors. The resulting ﬁgure is an
order diagram of M.
Properties of inﬁnite ordered sets may also be illustrated by order diagrams.
In the remainder of this section we deﬁne and discuss various constructs associated
with ordered sets.
Deﬁnition 1.4. An element of an ordered set {M, ≤} is called the least element o(M)
(resp. the greatest element i(M)) if

a∈M
o(M) ≤a

resp.

a∈M
a ≤i(M)

.
■
That both o(M) and i(M) are well deﬁned is the assertion of the following theo-
rem.
Theorem 1.5. An ordered set has at most one least and at most one greatest element.
Proof. Suppose a and b are least elements of M. Then a ≤b ∧b ≤a ⇒a = b
by (O3).
■
The concepts of least and greatest elements and minimal and maximal elements
are of course different. They can be illustrated by order diagrams. See, for instance,
Figure 1.1. A least element is always minimal and a greatest element always maximal.
The converse is not true. See Figure 1.1.
The power set {PM, ⊆} of a set M is an important example that contains a least
and a greatest element. In particular, o(PM) = ∅, and i(PM) = M.
Deﬁnition 1.6. Let {M, ≤} be a nonvoid ordered set and S ⊆M. An element a ∈M
is called a lower (resp. an upper) bound of S in M if

b∈S
a ≤b

resp.

b∈S
b ≤a

.
We denote the set of all lower (resp. upper) bounds of S in M by
L(S) :=

a ∈M |

b∈S
a ≤b


resp. U(S) :=

a ∈M |

b∈S
b ≤a

.

16
1 First Concepts
If S = {s}, we simply write L(s) resp. U(s) instead of L({s}) resp. U({s}). S is
called bounded in M if L(S) ̸= ∅and U(S) ̸= ∅. The greatest lower bound (resp.
the least upper bound) if it exists, is called the inﬁmum (resp. the supremum) of S.
Symbolically
inf S := i(L(S))
(resp. sup S := o(U(S))) .
■
Not every subset of an ordered set has an inﬁmum or a supremum or even lower
or upper bounds. For an illustration see, for instance, the ordered sets drawn in Fig-
ure 1.1.
According to Theorem 1.5, the inﬁmum (resp. the supremum) of a subset S ⊆M,
if it exists, is uniquely determined.
The case S = ∅is not excluded in Deﬁnition 1.6. Since in the case of the empty
set the deﬁnition of bounds requires nothing,

a∈M

b∈∅
a ≤b
∧

a∈M

b∈∅
b ≤a
every element of M is a lower and an upper bound of the empty subset ∅⊆M.
If an ordered set {M, ≤} has a least element o(M) and a greatest element i(M),
we have
inf M = o(M),
sup M = i(M),
and
inf ∅= i(M),
sup ∅= o(M).
If ∅̸= S ⊆M, it is always true that inf S ≤sup S.
If a subset S of an ordered set {M, ≤} has an inﬁmum (resp. a supremum) t, where
in particular t ∈S, then t is also least and therefore also a minimal element of S
(resp. greatest and therefore also a maximal element of S). Conversely, if a subset S
of an ordered set has a least (resp. greatest) element t, then t is also the inﬁmum (resp.
supremum) of S in M.
Now let {M, ≤} be an ordered set and S1, S2 nonvoid subsets of M each of which
is assumed to have an inﬁmum and a supremum in M. Then the following properties
hold:
(a) S1 ⊆S2 ⇒inf S2 ≤inf S1 ∧sup S1 ≤sup S2,
(b)

s1∈S1

s2∈S2
(s1 ≤s2 ⇒s1 ≤sup S1 ≤inf S2 ≤s2).
The proofs of these statements come directly from the preceding deﬁnitions and
properties. We indicate the one for (b):

s1∈S1

s2∈S2
(s1 ≤sup S1∧inf S2 ≤s2∧s1 ≤inf S2∧sup S1 ≤s2) ⇒sup S1 ≤inf S2.

1.2 Complete Lattices and Complete Subnets
17
1.2
Complete Lattices and Complete Subnets
We begin our discussion of lattices with the following deﬁnition.
Deﬁnition 1.7. Let {M, ≤} be an ordered set. Then
(O5) M is called a lattice if for any two elements a, b ∈M, the inf{a, b} and the
sup{a, b} exist;
(O6) M is called conditionally complete if for every nonempty, bounded subset S ⊆
M, the inf S and the sup S exist;
(O7) M is called completely ordered or a complete lattice if every subset S ⊆M
has an inﬁmum and a supremum.
■
Every ﬁnite subset S = {a1, a2, . . . , an} of a lattice has an inﬁmum and a supre-
mum. We prove this statement by induction. By deﬁnition any subset of two elements
has an inﬁmum and a supremum. Let us assume now that the assertion is correct for
subsets of n−1 elements. Then inf{a1, a2, . . . , an−1} exists. We consider the element
d := inf{inf{a1, a2, . . . , an−1}, an}. Then d is obviously a lower bound of S. Now let
c be any lower bound of S. Then c ≤inf{a1, a2, . . . , an−1} and c ≤an and therefore
c ≤d, i.e. d is the greatest lower bound of S, completing the induction. Moreover
we have also observed that inf{a1, a2, . . . , an} = inf{inf{a1, a2, . . . , an−1}, an}, i.e.,
that the inf (resp. sup) is associative.
Every ﬁnite lattice, therefore, has a least and a greatest element. Every ﬁnite lattice,
furthermore, is complete, i.e., is a complete lattice, since besides the empty set, inﬁma
and suprema are only to be considered for ﬁnite subsets, and we have already observed
that the inﬁmum and supremum of the empty subset equal the greatest and the least
element, respectively.
Since every ﬁnite lattice is an ordered set, every ﬁnite lattice can be represented by
an order diagram. The order diagram of a lattice is distinguished by the fact that any
two elements are downwardly and upwardly connected with other elements of the set.
Of course every completely ordered set is a lattice and every complete lattice is
conditionally complete.
Since the deﬁnitions of the inﬁmum and the supremum in an ordered set are com-
pletely dual, the following duality principle is valid throughout lattice theory:
If in any valid lattice theoretic statement or theorem the operations ≤, inf,
sup are replaced by ≥, sup, inf, respectively, a valid lattice theoretic state-
ment or theorem results.
In the deﬁnition of a complete lattice, the case S = M is included. Therefore,
inf M and sup M exist. Since they are elements of M, inf M is the least element and
sup M is the greatest element of M. Every complete lattice, therefore, has a least
element o(M) and a greatest element i(M).

18
1 First Concepts
To continue our development, it is convenient to have the following theorem estab-
lished.
Theorem 1.8. Let {M, ≤} be an ordered set with the property that every subset has
an inﬁmum (or a supremum). Then {M, ≤} is a complete lattice.
Proof.1 We are given that every subset S ⊆M has an inﬁmum, and we show that it
has a supremum also. Let U(S) be the set of upper bounds of S. (We may suppose
that U(S) ̸= ∅since inf ∅exists by hypothesis and inf ∅= i(M)). By assumption
the element u0 = inf U(S) exists. We show that u0 = sup S. We have

s∈S

u∈U(S)
s ≤u ⇒

s∈S
s ≤u0 = inf U(S),
i.e., u0 is upper bound of S. Now if u is any upper bound of S, then u ∈U(S) and
consequently u0 = inf U(S) ≤u. Therefore, u0 is the least upper bound of S, i.e.,
u0 = sup S = inf U(S).
■
According to Theorem 1.8, to verify that an ordered set is a complete lattice it is
sufﬁcient to show that every subset has a greatest lower bound or has a least upper
bound.
The concepts of a conditionally completely ordered set and a complete lattice are
closely related. If a conditionally completely ordered set has a least and a greatest
element it is a complete lattice.
Now let {M, ≤} be a conditionally completely ordered set without a least (resp.
greatest) element. We add to M an additional element y (resp. z) which by deﬁnition
is assumed to have the property

x∈M
y ≤x

resp.

x∈M
x ≤z

.
Then y is the least (resp. z greatest) element of {M ∪{y}, ≤} (resp. {M ∪{z}, ≤}),
and if M has neither a least nor a greatest element then {M ∪{y} ∪{z}, ≤} is a
complete lattice. Every ordered set that is conditionally complete thus can be made
into a complete lattice by adjoining a least and a greatest element.
This is a well-known procedure in the case of real numbers R. {R, ≤} is a con-
ditionally completely ordered set. In real analysis it is shown that every nonempty
bounded subset of real numbers has an inﬁmum and a supremum. If we adjoin −∞
and ∞to form {R ∪{−∞} ∪{∞}, ≤}, we obtain a complete lattice with the least
element −∞and the greatest element ∞. Similarly, we obtain a complete lattice by
adjoining the endpoints to an open interval of real numbers.
The following two theorems provide additional examples of complete lattices.
1We prove this theorem only for the case that an inﬁmum always exists. The proof in the case of the
supremum is dual. In the proofs of many following theorems, we omit the dual case without comment.

1.2 Complete Lattices and Complete Subnets
19
Theorem 1.9. Let {Mi, ≤i}, i = 1(1)n, be complete lattices. The product set M =
M1 × M2 × . . . × Mn is deﬁned as the set of all n-tuples a = (a1, a2, . . . , an) with
ai ∈Mi, i = 1(1)n. Let b = (b1, b2, . . . , bn) ∈M. If we deﬁne a relation ≤in M by
a ≤b
:⇔

i=1(1)n
ai ≤i bi,
then {M, ≤} is a complete lattice.
Proof. It is clear that {M, ≤} is an ordered set. Let S ⊆M and consider the set of
projections Sj := {sj | s ∈S}, where s = (s1, s2, . . . , sn). Then

i=1(1)n

si∈Si
inf Si ≤si
⇒

s∈S
(inf S1, inf S2, . . . , inf Sn) ≤s.
This means that (inf Si) := (inf S1, inf S2, . . . , inf Sn) is a lower bound of S. We
show that it is the greatest lower bound. Let c := (ci) := (c1, c2, . . . , cn) be any
lower bound of S. Then

i=1(1)n

si∈Si
ci ≤si
⇒

i=1(1)n
ci ≤inf Si
⇒
(ci) ≤(inf Si),
i.e., (inf Si) is the greatest lower bound of S. This means that inf S = (inf Si).
By duality we get sup S = (sup Si).
■
Theorem 1.10. The power set of a set M with inclusion ⊆as an order relation,
{PM, ⊆}, is a complete lattice. For each subset of PM, the inﬁmum is the inter-
section of the elements of the subset, and the supremum is the union.
Proof. {PM, ⊆} is an ordered set with the least element o(PM) = ∅and the greatest
element i(PM) = M. Let A ⊆PM and consider the sets of all lower bounds and
upper bounds of A:
L(A) :=

B ∈PM |

A∈A
B ⊆A

,
U(A) :=

B ∈PM |

A∈A
A ⊆B

.
Then inf A and sup A are, if they exist, deﬁned by
inf A := i(L(A)),
sup A := o(U(A)).
Now let X, Y ∈PM have the following properties:
X :=

a ∈M |

A∈A
a ∈A

,
Y :=

a ∈M |

A∈A
a ∈A

.
Then X is the intersection and Y is the union. We show that X = inf A and Y =
sup A. By the deﬁnition of a subset and the deﬁnition of X, we get

A∈A
X ⊆A,

20
1 First Concepts
i.e., X is lower bound of A, X ∈L(A). We still have to show that X is the greatest
lower bound. Let C ∈PM be any other lower bound of A. Then

A∈A
C ⊆A,
or by deﬁnition of a subset,

c∈C

A∈A
c ∈A.
Therefore, C ⊆X, i.e., X is the greatest lower bound, X = inf A. By duality we
obtain sup A = Y .
■
As an illustration, Figure 1.2 shows the order diagram of the power set PM of the
ﬁnite set M := {a, b, c}.
∅
M
{a}
{a, b}
{b}
{c}
{b, c}
{a, c}
Figure 1.2. Order diagram of the power set of the set M = {a, b, c}.
Now let {M, ≤} be a lattice and S ⊆M. Then {S, ≤} is an ordered set. {S, ≤}
may be a lattice. Let us consider the example represented in Figure 1.3.
{b}
{a}
{c}
{d}
{e}
{f}
{g}
i(M)
o(M)
(a)
o(M)
i(M)
{e}
{d}
{c}
(b)
Figure 1.3. Example for the concept of subnet.

1.2 Complete Lattices and Complete Subnets
21
The ordered set {M, ≤} of the nine points in Figure 1.3 (a) clearly represents a
lattice. The subset S ⊆M of the solid points in Figure 1.3 with respect to the
same order relation also represents a lattice, Figure 1.3 (b). In general, however, the
inﬁmum and supremum taken in {S, ≤} are different from those taken in {M, ≤}.
For example, in {M, ≤}, sup{c, d} = f, while in {S, ≤}, sup{c, d} = i(M). This
leads to the following deﬁnition.
Deﬁnition 1.11. Let {M, ≤} be an ordered set and S ⊆M. If {S, ≤} is a lattice, it is
called a subnet of {M, ≤}. A subnet is called an inf-subnet (resp. a sup-subnet) if

a,b∈S
inf
M {a, b} = inf
S {a, b}

resp.

a,b∈S
sup
M
{a, b} = sup
S
{a, b}

,
where the subscripts M and S indicate the set in which the inﬁmum (resp. supremum)
is taken. A subnet of a lattice is called a sublattice if it is both an inf-subnet and a
sup-subnet.
■
Similar properties hold in complete lattices. Therefore, we give the following deﬁ-
nition.
Deﬁnition 1.12. Let {M, ≤} be a complete lattice and S ⊆M. If {S, ≤} is also a
complete lattice, it is called a complete subnet of {M, ≤}. A complete subnet is called
a complete inf-subnet (resp. a complete sup-subnet) if

A⊆S
inf
M A = inf
S A

resp.

A⊆S
sup
M
A = sup
S
A

.
A complete subnet is called a complete sublattice if it is both a complete inf-subnet
and a complete sup-subnet. In this case the subscripts M and S can be omitted.
■
This deﬁnition leads directly to the following theorem and remark.
Theorem 1.13. Let {M, ≤} be a complete lattice and {S, ≤} a complete subnet. Then

A⊆S
inf
S A ≤inf
M A ∧sup
M
A ≤sup
S
A.
■
Remark 1.14. In a complete lattice {M, ≤}, the inﬁmum and supremum also exist
for the empty set ∅. Therefore we have:
(a) The greatest element of M is also the greatest element of every complete inf-
subnet, i.e., i(M) = inf ∅= i(S).
(b) The least element of M is also the least element of every complete sup-subnet,
i.e., o(M) = sup ∅= o(S).

22
1 First Concepts
(c) In a complete sublattice S of a complete lattice M, the least and the greatest
elements of M and S are identical: o(M) = o(S) = sup ∅und i(M) = i(S) =
inf ∅.
■
In the deﬁnition of a complete inf-subnet and a complete sup-subnet, it is presumed
that S is a complete lattice. The following theorem enables us to eliminate this re-
quirement.
Theorem 1.15. Let {M, ≤} be a complete lattice and S ⊆M. {S, ≤} is a complete
inf-subnet (resp. a complete sup-subnet) of {M, ≤} if and only if

A⊆S
inf
M A ∈S

resp.

A⊆S
sup
M
A ∈S

.
Proof. If {S, ≤} is a complete inf-subnet, we have infM A = infS A. Since infS A ∈
S, this demonstrates the necessity of the hypothesis. We now demonstrate the sufﬁ-
ciency. Let A ⊆S and I := infM A ∈S. Then for all a ∈A, I ≤a, i.e., I is lower
bound of A in S. Moreover,

x∈M
 
a∈A
x ≤a ⇒x ≤I

⇒

x∈S
 
a∈A
x ≤a ⇒x ≤I

,
i.e., I is greatest lower bound of A in {S, ≤}. This implies that infM A = infS A.
Since every subset A ⊆S has an inﬁmum in S, Theorem 1.8 implies that {S, ≤} is a
complete lattice.
■
The following corollary is a direct consequence of Theorem 1.15.
Corollary 1.16. Let {M, ≤} be a complete lattice and S ⊆M. {S, ≤} is a complete
sublattice of {M, ≤} if and only if

A⊆S
(inf
M A ∈S ∧sup
M
A ∈S).
■
We illustrate these concepts with two simple examples.
Examples. 1. Let {M, ≤} be an ordered set, a ∈M and S := {s ∈M | s ≤a}
(resp. S := {s ∈M | a ≤s}). Then we have:
(a) If {M, ≤} is a lattice, then {S, ≤} is a sublattice.
(b) If {M, ≤} is a complete lattice and S := S ∪{i(M)}, then {S, ≤} is a complete
sublattice.
Proof.
(a) Let x, y ∈S. Then x ≤a und y ≤a and therefore infM{x, y} ≤
supM{x, y} ≤a, i.e., infM{x, y}, supM{x, y} ∈S, which proves the assertion
by Corollary 1.16.

1.3 Screens and Roundings
23
(b) Let ∅̸= A ⊆S.
Then for all x ∈S, x ≤a, and therefore infM S ≤
supM S ≤a, i.e., infM S, supM S ∈S. But ∅⊆S also. Then in order to
apply Corollary 1.16, infM ∅= i(M) and supM ∅= o(M) must be elements
of S. Since o(M) ≤a, it is automatically an element of S and therefore of S.
That i(M) ∈S, however, has been assumed explicitly.
■
2. Let {M, ≤} be an ordered set and S := {s ∈M | s ∈[a, b]}, where [a, b] denotes
an interval in M. Then we have:
(a) If {M, ≤} is a lattice, then {S, ≤} is a sublattice.
(b) If {M, ≤} is a complete lattice, then {S ∪o(M) ∪i(M), ≤} is a complete sub-
lattice.
The proofs of these statements are analogous to the proofs for the previous example.
1.3
Screens and Roundings
We will now give an abstract characterization of the essential properties of the sets
and subsets displayed in Figure 1 which are essential for our description of computer
arithmetic. To motivate the next deﬁnition, let us consider two simple examples from
Figure 1. We recall that all sets listed in Figure 1 are ordered with respect to certain
order relations.
Consider the set IV2R of interval vectors of dimension 2. The elements of this set
are intervals of two dimensional real vectors. Such an element describes a rectangle
in the x, y plane with sides parallel to the axes. Such interval vectors are special
elements of the powerset PV2R of two dimensional real vectors. PV2R is deﬁned as
the set of all subsets of two dimensional real vectors. The following relations hold
between these two sets PV2R and IV2R:
(i) For each element A ∈PV2R, there exist upper bounds in IV2R with respect to
the order relation ⊆. See Figure 1.4.
(ii) For all A ∈PV2R, the set of upper bounds in the subset IV2R has a least element.
See Figure 1.4, where this least element is called C.
We shall see that these two properties describe the relationship between any set in
Figure 1 and its subsets as listed on its right.
In particular, consider the ﬁrst row of Figure 1. For R and the subset S taken to be
ﬂoating-point numbers, we obtain the above two properties once again:
(i) For each element a ∈R, there exist upper bounds in S with respect to the order
relation ≤. See Figure 1.5.
(ii) For all a ∈R, the set of upper bounds in the subset S has a least element. See
Figure 1.5.

24
1 First Concepts
In this example, corresponding properties also hold for the set of lower bounds.
These properties motivate the concept of a screen, which is formalized in the fol-
lowing deﬁnition.
Deﬁnition 1.17. Let {M, ≤} be an ordered set. For each element a ∈M, let L(a)
(resp. U(a)) be the set of lower (resp. upper) bounds of a. A subset S ⊆M is called
a screen of M if it fulﬁlls the properties
(S1)

a∈M
L(a) ∩S ̸= ∅
∧

a∈M
U(a) ∩S ̸= ∅,
(S2)

a∈M

x∈L(a)∩S

b∈L(a)∩S
b ≤x
∧

a∈M

y∈U(a)∩S

b∈U(a)∩S
y ≤b,
i.e., the set L(a) ∩S has a greatest element x = i(L(a) ∩S) and U(a) ∩S has a least
element y = o(U(a) ∩S).
If only the left-hand-side properties of (S1) and (S2) hold, then S is called a lower
semiscreen, and if only the right-hand-side properties hold, S is called an upper semis-
creen. Usually we shall write {S, ≤} to denote the screen or semiscreen to emphasize
the ordering.
■
Screens and upper semiscreens play a central role in the description of computer
arithmetic. For the sake of conciseness of expression, we shall, therefore, often speak
of an upper screen instead of an upper semiscreen.
Since in a screen of an ordered set the elements i(L(a)∩S) and o(U(a)∩S) always
exist, we can deﬁne mappings ϕ : M →S and ψ : M →S by
y
x
C
B
A
Figure 1.4. Illustration of the concept of a screen. A ∈PV2R, B, C ∈
IV2R; A ⊆B, A ⊆C, and for all D ∈IV2R: A ⊆D ⇒C ⊆D.
b′
c′
a
c
b
Figure 1.5. Illustration of the concept of a screen. a ∈R, b, c ∈S; a ≤b,
a ≤c, and for all d ∈S: a ≤d ⇒c ≤d.

1.3 Screens and Roundings
25
(R)

a∈M
ϕa := i(L(a) ∩S)
∧

a∈M
ψa := o(U(a) ∩S).
These mappings have the properties given in the following lemma.
Lemma 1.18. Let {M, ≤} be an ordered set and {S, ≤} a screen of M. The mappings
ϕ and ψ deﬁned by (R) have the following properties:
(R1)

a∈S
ϕa = a,

a∈S
ψa = a,
(R2)

a,b∈M
(a ≤b ⇒ϕa ≤ϕb),

a,b∈M
(a ≤b ⇒ψa ≤ψb),
(R3)

a∈M
ϕa ≤a,

a∈M
a ≤ψa.
Proof. (R1): If a ∈S, then a ∈U(a) ∩S, and therefore ψa = o(U(a) ∩S) = a.
(R2): a ≤b ⇒U(b) ∩S ⊆U(a) ∩S ⇒ψa = o(U(a) ∩S) ≤o(U(b) ∩S) = ψb.
(R3): ψa := o(U(a) ∩S) ∈U(a) ∩S. Therefore a ≤ψa.
The proof of the properties for ϕ is dual.
■
If S is a lower (resp. an upper) screen of M, then only the function ϕ (resp. ψ) can
be deﬁned. Then the properties (R1, 2, 3) can be demonstrated only for ϕ (resp. ψ).
The following theorem expresses a relationship between screens and subnets.
Theorem 1.19. A subset S of a complete lattice {M, ≤} is an upper (resp. lower)
screen of M if and only if it is a complete inf-subnet (resp. a complete sup-subnet).
Proof. (a) First we show that (S1) and (S2) ⇒{S, ≤} is a complete inf-subnet.
Choosing a = i(M) and using (S1), U(a) ∩S ̸= ∅, we have that i(M) = i(S).
Because of (S2) we can deﬁne the mapping ψ : M →S (which is the subject of
Lemma 1.18) with the property ψa = o(U(a) ∩S). Now let B ⊆S. Since B ⊆M
the hypothesis implies that the element x := infM B exists and we have

b∈B
(x ≤b ⇒
(R2) ψx ≤ψb =
(R1) b),
i.e., ψx is lower bound of B. Therefore we have ψx ≤infM B = x. But by (R3),
x ≤ψx. From both inequalities we get by (O3) x = ψx. Since ψ : M →S, then
x = infM B ∈S. Therefore by Theorem 1.15, {S, ≤} is a complete inf-subnet of
{M, ≤}, and for all B ⊆S, infM B = infS B.
(b) Now we show that (S1) and (S2) hold in any complete inf-subnet S ⊆M. In Re-
mark 1.14 we observed that in a complete inf-subnet i(M) = i(S) = inf ∅. Therefore
(S1) holds.
Further, for all A ⊆S we have by hypothesis infS A = infM A.
Therefore for all a ∈M,
inf(U(a) ∩S) ∈S.
(1.3.1)

26
1 First Concepts
Moreover, for all b ∈U(a) ∩S, a ≤b, and therefore also
a ≤inf(U(a) ∩S).
(1.3.2)
By (1.3.1) and (1.3.2) we get inf(U(a) ∩S) ∈U(a) ∩S, i.e., inf(U(a) ∩S) is the
least element of U(a) ∩S since it is an element of this set itself. We have, there-
fore, inf(U(a) ∩S) = o(U(a) ∩S) ∈U(a) ∩S, which completes the proof of the
theorem.
■
Corollary 1.20. A subset S of a complete lattice {M, ≤} is a screen of M if and only
if it is a complete sublattice.
■
In an upper screen of a complete lattice, (S2) requires that for any a ∈M the
set of upper bounds in S has a least element which itself is an upper bound of a.
The example given in Figure 1.6 shows that the corresponding property for the lower
bounds of a is not necessarily valid.
o(M)
d
b
f
e
c
a
i(M)
Figure 1.6. Illustration of an upper screen.
Let {S, ≤} be the subnet of solid points in Figure 1.6, i.e., S = {i, b, c, f}. {S, ≤}
is obviously an upper screen of {M, ≤} since i(M) = i(S) and for all A ⊆S,
infS A = infM A.
{S, ≤}, however, is not a lower screen of M. For instance o(M) ̸= o(S) = f
and supM{b, c} = a ̸= supS{b, c} = i. By Theorem 1.19, for all x ∈M, the
set of upper bounds U(x) ∩S has a least element inf(U(x) ∩S) with the property
x ≤inf(U(x) ∩S). On the contrary, in general the set L(x) ∩S has no greatest
element nor supS(L(x)∩S) ≤x∨supS(L(x)∩S) ∈L(x)∩S. In the example given
in Figure 1.6, we have for x = a, for instance,
L(a) ∩S = {b, c, f}
∧
a < sup
S
(L(a) ∩S) = i(M) /∈L(a) ∩S.

1.3 Screens and Roundings
27
This example shows furthermore that in an upper screen the least elements o(M) and
o(S) are not necessarily the same.
Now let S be a complete inf-subnet (resp. a complete sup-subnet) of a complete
lattice {M, ≤}. Then for all A ⊆S, infM A = infS A (resp. supM A = supS A),
while the suprema (resp. the inﬁma) may differ. See Theorem 1.13. The difference
is eliminated by employing the function ϕ (resp. ψ) introduced in (R) (see page 25).
This property is the subject of the following theorem.
Theorem 1.21. Let {M, ≤} be a complete lattice and {S, ≤} a lower (resp. an upper)
screen of M and ϕ : M →S (resp. ψ : M →S) deﬁned by
(R)

a∈M
ϕa := inf(U(a) ∩S)
(resp. 
a∈M
ψa := sup(L(a) ∩S)).
Then

A⊆S
inf
S A = ϕ(inf
M A)
(resp.

A⊆S
sup
S
A = ψ(sup
M
A)).
Proof.

a∈A⊆S
a ≤sup
M
A ⇒
(R2)

a∈A
ψa =
(R1) a ≤
(R2)
ψ(sup
M
A) ∈S.
Therefore
sup
S
A ≤ψ(sup
M
A).
(1.3.3)
By Theorem 1.13, however, in a complete subnet we have generally

A⊆S
sup
M
A ≤sup
S
A
⇒
(R2), (R1) ψ(sup
M
A) ≤sup
S
A.
(1.3.4)
Applying (O3) to (1.3.3) and (1.3.4) proves the theorem.
■
To illustrate the deﬁnition and the theorems given in this chapter, let us consider a
few examples.
Examples. 1. Let Z be a bounded set of complex numbers Z := {ζ := ξ + iη ∈C |
|ξ| ≤r ∧|η| ≤r}. The power set {PZ, ⊆} is a complete lattice. We consider the
set IZ of all rectangles of PZ with sides parallel to the axes. The elements of IZ are
intervals in the complex plane. Also let ∅∈IZ. We show that {IZ, ⊆} is an upper
screen of {PZ, ⊆}.
To see this, by Theorem 1.15 we have only to show that for every subset A ⊆IZ,
the intersection, which is the inﬁmum in {PZ, ⊆}, is an element of IZ. If A = ∅, we
have
inf
PZ ∅= inf
IZ ∅= i(PZ) = i(IZ) = Z.
If ∅̸= A ⊆IZ, it is easy to see by Figure 1.7(a) that the intersection of the ele-
ments of A is again an element of IZ. The properties (S1) and (S2) are illustrated in
Figure 1.7(b) and (c).

28
1 First Concepts
b
b = o(U(a) ∩IZ)
a ∈PZ
b ∈IZ
a ⊆b
a ∈PZ
a ⊆b
(b)
(c)
(a)
a
a
b
Figure 1.7. Illustration of an upper screen.
Further, we have o(PZ) = o(IZ) = ∅. Also the condition (S1) for a lower screen
holds. See Figure 1.8(a). The necessary and sufﬁcient criterion for a lower screen
given in Theorem 1.15, however is not valid. For example, in Figure 1.8(b) we have
supPZ{a, b} = a ∪b ⊂supIZ{a, b} = c. Therefore (S2) is not fulﬁlled either. In
Figure 1.8(c), for instance, supIZ(L(a) ∩IZ) = infIZ(U(a) ∩IZ) ⊇a. Figure 1.8(b)
further illustrates Theorem 1.21. We have supIZ{a, b} = ψ(supPZ{a, b}).
b ⊆a
b ∈IZ
a ∈PZ
a
b
a
b
a
c
(a)
(c)
(b)
Figure 1.8. IZ is not a lower screen of PZ.
2. Let X be a bounded set of real numbers X := {x ∈R | |x| ≤r}. The power set
{PX, ⊆} is a complete lattice. For all a1, a2 ∈X with a1 ≤a2 let IX := {[a1, a2] |
a1, a2 ∈X} be the set of intervals over X. Also let ∅∈IX. Then IX ⊂PX and
{IX, ⊆} is an upper screen of {PX, ⊆}. This example just represents the reduction
of example 1 to one dimension. All properties can be proved similarly.
3. Again let X := {x ∈R | |x| ≤r}. Then {X, ≤} is a linearly ordered, complete
lattice. If S is a ﬁnite subset of X with the property −r ∈S and r ∈S, then {S, ≤}
is a complete lattice. It is easy to see by Theorem 1.15 or by Deﬁnition 1.17 and

1.3 Screens and Roundings
29
Corollary 1.20 that {S, ≤} is a screen of {X, ≤}. See Figure 1.9. There, for all
A ⊆S, infX A = infS A = o(A) ∈S and supX A = supS A = i(A) ∈S.
r
−r
0
A ⊆S
Figure 1.9. Illustration of a screen.
4. As in example 2, let IX again denote the set of intervals over X augmented by the
empty set and S the subset of X, deﬁned in example 3. Now let IS denote the set of
intervals over X with endpoints in S and augmented by the empty set. In example 2
we saw that {IX, ⊆} is an upper screen of {PX, ⊆}. Now we show that {IS, ⊆} is a
screen of {IX, ⊆}. {IS, ⊆} is a complete lattice with the property o(IS) = o(IX) =
∅and i(IS) = i(IX) = X. The inﬁmum (resp. supremum) in IS is the intersection
(resp. the interval hull) as in IX. Therefore, for every subset of intervals in IS the
inﬁmum and supremum is the same as in IX.
5. Now let Z again be a bounded set of complex numbers as deﬁned in example 1.
We have seen there that the set of intervals {IZ, ≤} over Z is an upper screen of the
power set {PZ, ⊆}. As in example 3, let S again denote a ﬁnite subset of the set X.
With S we deﬁne a discrete subset N of complex numbers by
N := {ζ := ξ + iη | ξ, η ∈S ∧|ξ| ≤r ∧|η| ≤r}.
Now we consider the set of intervals IN over Z with bounds in N. Then {IN, ⊆}
is a screen of {IZ, ⊆}. The inﬁmum (resp. supremum) in IN is the intersection (resp.
the interval hull) as in IZ. For all subsets A ⊆IN we have infIZ A = infIN A ∈IN
and supIZ A = supIN A ∈IN. See Figure 1.10.
Figure 1.10. Illustration of a screen.
We shall see later that the concepts of a screen and of an upper screen encompass
all essential properties of the sets listed in Figure 1. Using these concepts, we now
specify the notion of rounding.

30
1 First Concepts
Deﬁnition 1.22. Let M be a set and S ⊆M. A mapping
: M →S is called a
rounding if it has the property
(R1)

a∈S
a = a
(projection).
A rounding of a complete lattice into a lower (resp. an upper) screen (resp. a screen)
is called monotone if
(R2)

a,b∈M
(a ≤b ⇒
a ≤
b)
(monotone).
It is called downwardly (resp. upwardly directed if
(R3)

a∈M
a ≤a

resp. 
a∈M
a ≤
a

(directed).
■
A general property of monotone mappings, applied to roundings, is given in the
following lemma.
Lemma 1.23. Let {M, ≤} be a complete lattice and {S, ≤} a lower (resp. an upper)
screen of M and
: M →S a monotone rounding. Then for all a ∈S, the set
−1a ⊆M is convex.
Proof. Let a1, a2 ∈
−1a ⊆M. Let b ∈M have the property a1 ≤b ≤a2. Then by
(R2), we get
a1 = a ≤
b ≤
a2 = a, and by (O3),
b = a, i.e., b ∈
−1a.
■
In Deﬁnition 1.22 three properties of roundings are enumerated, and we may ask if
there do indeed exist roundings with all three of these properties. A characterization
of such roundings, which supplies an afﬁrmative answer as well, is the subject of the
following theorem.
Theorem 1.24. Let {M, ≤} be a complete lattice and {S, ≤} a lower (resp. upper)
screen. A mapping
: M →S is a monotone downwardly (resp. upwardly) directed
rounding if and only if it has the property
(R)

a∈M
a := i(L(a) ∩S)

resp. 
a∈M
a := o(U(a) ∩S)

.
Proof. (a) It is shown in Lemma 1.18 that the mapping deﬁned by (R) has the prop-
erties (R1), (R2), and (R3).
(b) We still have to show that the mapping
: M →S with the properties (R1),
(R2) and (R3) also fulﬁlls (R). By (R3),
a ∈L(a) ∩S. Let b be any element of
L(a) ∩S. Then b ≤a and by (R1) and (R2),
b = b ≤
a, i.e.,
a is the greatest
element of L(a) ∩S.
■
Since in Theorem 1.24 {S, ≤} is a lower screen, it is a complete sup-subnet of
{M, ≤}. Then there exists the element sup(L(a) ∩S) ∈S ⊆M. Since the greatest

1.3 Screens and Roundings
31
element of a set is always its supremum we also have
(R)

a∈M
a := sup(L(a) ∩S)

resp. 
a∈M
a := inf(U(a) ∩S)

.
The following remark characterizes the uniqueness property expressed in Theo-
rem 1.24.
Remark 1.25. Since the inﬁmum and the supremum of a subset of a complete lattice
are unique, there exists only one monotone downwardly (resp. upwardly) directed
rounding of a complete lattice into a lower (resp. upper) screen. We shall therefore
often use the special symbols ▽(resp. △) to denote these mappings. They have the
property
(R)

a∈M
▽a = sup(L(a) ∩S)

resp. 
a∈M
△a = inf(U(a) ∩S)

.
■
These observations are collected into the statement of the following corollary.
Corollary 1.26. If {M, ≤} is a complete lattice and {S, ≤} a screen, then there exists
exactly one monotone downwardly directed rounding ▽: M →S and exactly one
monotone upwardly directed rounding △: M →S. These roundings can be deﬁned
by property (R).
■
The two directed roundings ▽and △are key elements in the set of all roundings.
The composition △2△1 of two such roundings, i.e., a mapping from M into a screen
D ⊂M followed by a mapping of D into a screen S ⊂D, is itself a monotone
upwardly directed rounding. In a linearly ordered complete lattice every monotone
rounding into a screen can be expressed in terms of the two monotone directed round-
ings ▽and △. These properties are made precise by the following two theorems.
Theorem 1.27. Let {M, ≤} be a complete lattice and let {S, ≤} and {D, ≤} both be
lower (resp. upper) screens of {M, ≤} with the property S ⊂D ⊆M. Further, let
▽: M →S, ▽1 : M →D, ▽2 : D →S (resp. △: M →S, △1 : M →D,
△2 : D →S) be the associated monotone downwardly (resp. upwardly) directed
roundings. Then

a∈M
▽a = ▽2(▽1a)

resp.

a∈M
△a = △2(△1a)

.
Proof. S ⊂D ⇒L(a)∩S ⊆L(a)∩D ⇒▽a = i(L(a)∩S) ≤▽1a = i(L(a)∩D).
If we apply the mapping ▽2 to this inequality, we obtain
▽2(▽a) =
(R1)
▽a ≤
(R2)
▽2(▽1a).
(1.3.5)

32
1 First Concepts
By (R3) we have ▽2(▽1a) ≤a, and therefore
▽(▽2(▽1a)) =
(R1)
▽2(▽1a) ≤
(R2)
▽a.
(1.3.6)
From (1.3.5) and (1.3.6) we get ▽a = ▽2(▽1a) by (O3).
■
In practical applications of this theorem, S may be a ﬂoating-point system and D
the set of ﬂoating-point numbers of double length on the same computer. We show
however, by means of a simple example, that the statement of Theorem 1.27 does
not hold generally for monotone but undirected roundings of a complete lattice into a
screen.
Example 1.28. Let M := [0, 8] ⊂R, S := {0, 4, 8}, and D := {0, 1, 2, 3, 4, 5, 6, 7, 8}.
Let the roundings
: M →S,
1 : M →D, and
2 : D →S be deﬁned as
mappings to the nearest element of the screen, where we additionally assume that the
midpoint of two neighboring screenpoints will be mapped onto the lower neighbor in
all cases. Then we get for a = 2.4 ∈M,
1a = 2 and
2(
1a) = 0 ̸=
a = 4.
In linearly ordered complete lattices we ﬁnd that in addition to the monotone di-
rected roundings, all monotone roundings play an important role. We discuss a few of
their properties beginning with the following lemma.
Lemma 1.29. Let {M, ≤} be a linearly ordered complete lattice and {S, ≤} be a
screen of {M, ≤}, and let ▽: M →S and △: M →S be the monotone directed
roundings. Then for all a ∈M, there exists no element b ∈S with the property
▽a < b < △a.
Proof. We have ▽a = i(L(a) ∩S) and △a = o(U(a) ∩S). Suppose that the
conclusion of the lemma is false. Then using (O4), we ﬁnd that for any a ∈M, there
exists an element b ∈S such that i(L(a) ∩S) < b ≤a ∨a ≤b < o(U(a) ∩S). This
is a contradiction of the deﬁnition of the greatest or of the least element.
■
We use this lemma to obtain the following theorem, which characterizes monotone
roundings in linearly ordered sets.
Theorem 1.30. Let {M, ≤} be a linearly ordered complete lattice, {S, ≤} a screen
of {M, ≤}, and ▽: M →S (resp. △: M →S) the monotone downwardly (resp.
upwardly) directed rounding. For each element a ∈M let I := [▽a, △a] and let I1
and I2 with I1 < I2 be subsets2 of M which partition I := I1 ∪I2. Then the mapping
: M →S is a monotone rounding if and only if

a∈S⊆M
a = a
∧

a∈M\S
a =

▽a
for all a ∈I1
△a
for all a ∈I2
.
2If U, V are subsets of {M, ≤}, U < V means u < v for all u ∈U and all v ∈V .

1.3 Screens and Roundings
33
Proof. (a) It is clear that every such mapping is a monotone rounding.
(b) We still have to show that every monotone rounding has the property stated in the
theorem. Now let I1 := {a ∈I |
a = ▽a} and let I2 := {a ∈I |
a = △a}.
Since
is a monotone rounding I1 and I2 are convex sets by Lemma 1.23, and since
for a /∈S: ▽a < △a, I1 < I2. Since for all a ∈I, ▽a ≤a ≤△a, then (R1) and
(R2) yield ▽a ≤
a ≤△a. Now Lemma 1.29 implies that I1 ∪I2 = I.
■
Theorem 1.30 asserts that every monotone rounding of a linearly ordered complete
lattice into a screen can be expressed by the monotone directed roundings △and
▽. Different monotone roundings are distinguished from each other in an interval
I := [a1, a2] between neighboring screenpoints a1 and a2 only by the way I is split
into associated subsets I1 < I2. The monotone directed roundings are extreme cases
with respect to this splitting:
▽: I1 = I \ {a2}, I2 = {a2}
and
△: I1 = {a1}, I2 = I \ {a1}.
Finally we show by simple examples that in general Lemma 1.29 and Theorem 1.30
are not valid in the case of a complete but not linearly ordered lattice. Let {M, ≤} be
the complete lattice that appears in Figure 1.11(a).
i(M)
(a)
(b)
o(M)
e
c
a
b
d
f
c
a
b
d
z
Figure 1.11. Roundings in non linearly ordered sets.
The subset {S, ≤} consisting of the solid points in Figure 1.11(a) obviously is a
screen of {M, ≤}. We deﬁne a mapping
: M →S by the following properties:
(i) All screenpoints are ﬁxed points of the mapping.
(ii)
a = b,
c = d,
e = f. See Figure 1.11(a).
Then
is a monotone rounding. However, neither Lemma 1.29 nor Theorem 1.30
holds. For instance, we obtain i(L(c) ∩S) = f <
c = d < o(U(c) ∩S) = i(M),
and consequently ▽c = f <
c = d < b < △c = i(M).

34
1 First Concepts
As another example we consider the set of complex numbers C with the order
relation deﬁned componentwise. {C, ≤} is a complete lattice. The subset S ⊂C
where the real and imaginary parts are restricted to integers is obviously a screen of
C. Figure 1.11(b) shows a small part of S. For all complex numbers z in the open
square shown in the ﬁgure it holds that: ▽z = a < c < △z = d and ▽z = a < b <
△z = d.
1.4
Arithmetic Operations and Roundings
If M1, M2, and M are nonempty sets, then a mapping ◦: M1×M2 →M of the prod-
uct set M1 × M2 into M is called an operation. The image ◦(a, b) of the operands
a ∈M1 and b ∈M2 is usually written in the dyadic form a ◦b. If, in particular,
M1 = M2 = M, one speaks of an inner operation in contrast to an outer operation,
where ◦: M1 ×M →M. In the latter case M1 is called the operator set. Examples of
an inner operation are the addition and multiplication of real numbers and of an outer
operation the multiplication of a vector by a scalar or the matrix-vector multiplica-
tion. A nonempty set with operations deﬁned for its elements is called an algebraic
structure.
The simplest algebraic structure is a set M with one inner operation ◦. It is called
a groupoid {M, ◦}. An element e ∈M is called a left neutral (resp. a right neutral)
element if

a∈M
e ◦a = a

resp.

a∈M
a ◦e = a

.
e is called a neutral element if it is a left as well as a right neutral element. A neutral
element is always unique since the presumption of two such elements e and e′ yields
e = e ◦e′ = e′.
{R, +} and {R, ·} are examples of groupoids with the neutral element 0 and 1
respectively. The groupoids {R, −} (resp. {R, /}) have only right neutral elements 0
(resp. 1).
On computers, subtraction is not always the inverse operation for addition, nor
division for multiplication. All operations have a more independent nature. It is,
however, essential that all operations occurring in the spaces of Figure 1 have a right
neutral element. See, for instance, Theorem 1.32 and the example following it.
An operation ◦: M1 × M2 →M or a ◦b is called ordered if for subsets N1 ⊆M1
and N2 ⊆M2 and ﬁxed (a′, b′) ∈N1 × N2 the mappings a ◦b′ and a′ ◦b are both
monotone. A nonempty set with ordered operations is called an ordered algebraic
structure.
{R, +, ≤} and {Rn, +, ≤} are examples of ordered groups. {R, +, ·, ≤} is an or-
dered ring.

1.4 Arithmetic Operations and Roundings
35
In particular, we shall call the triple {M, ◦, ≤} an ordered groupoid if {M, ◦} is a
groupoid, {M, ≤} is an ordered set, and the following condition (OA) holds:
(OA)

a,b,c∈M
(a ≤b ⇒a ◦c ≤b ◦c ∧c ◦a ≤c ◦b).
(OA) is referred to as the compatibility property between the algebraic and the order
structure. (OA) is equivalent to
(OA’)

a,b,c,d∈M
(a ≤b ∧c ≤d ⇒a ◦c ≤b ◦d).
To get (OA’) from (OA), (OA) has to be applied twice while (OA’) ⇒(OA) by
taking c = d.
A key objective of this treatise concerns the question of how operations that are
deﬁned in a certain set M can be approximated on a screen or on an upper screen S.
We shall see later that natural mapping properties such as isomorphism or homomor-
phism for characterizing this approximation cannot be achieved. Nevertheless, we
may develop reasonable and simple compatibility properties between the operations
in S and in M. Such properties are characterized by the following deﬁnition:
Deﬁnition 1.31. Let {M, ≤} be a complete lattice, {M, ◦} a groupoid, and {S, ≤}
a lower (resp. an upper) screen of {M, ≤}. A groupoid {S,
◦} is called a screen
groupoid (or rounded groupoid) if
(RG1)

a,b∈S
(a ◦b ∈S ⇒a ◦b = a ◦b).
A screen groupoid is called monotone if
(RG2)

a,b,c,d∈S
(a ◦b ≤c ◦d ⇒a ◦b ≤c ◦d).
A screen groupoid is called a lower (resp. upper) screen groupoid if
(RG3)

a,b∈S
a ◦b ≤a ◦b

resp.

a,b∈S
a ◦b ≤a ◦b

.
■
In Deﬁnition 1.31 three properties of screen groupoids are enumerated. We may
ask if there do indeed exist groupoids with all three of these properties and how such
groupoids can be produced. The following theorem supplies an answer to this ques-
tion.
Theorem 1.32. Let {M, ≤} be a complete lattice, {M, ◦} a groupoid, {S, ≤} a lower
(resp. upper) screen or a screen of {M, ≤}. Let
: M →S be a mapping and let
an operation
◦in S be deﬁned by
(RG)

a,b∈S
a ◦b :=
(a ◦b).
(a) If
has the property (Ri), i = 1, 2, 3 deﬁned for roundings, then {S,
◦} has
the property (RGi), i = 1, 2, 3, respectively.
(b) If the groupoid {M, ◦} has a right neutral element e and e ∈S, then (RG1),
(RG2), and (RG3) imply (RG).

36
1 First Concepts
Proof. (a) We omit the proof of this property since it is straightforward.
(b) We give the proof in the case of a lower screen. Let a, b ∈S and x := ▽(a ◦b).
Then by (R3) we obtain
x := x ◦e = ▽(a ◦b) ≤a ◦b.
(1.4.1)
Since e ∈S we obtain

a∈S
a ◦e = a ∈S ⇒
(RG1)

a∈S
(a ◦e = a ◦e = a),
i.e., e is also a right neutral element in {S,
◦}.
Applying (RG2) to (1.4.1), therefore, we get
x ◦e = x = ▽(a ◦b) ≤a ◦b.
(1.4.2)
(RG3) yields a ◦b ≤a ◦b. If we apply the rounding ▽to this inequality, we get by
(R1) and (R2)
▽(a ◦b) = a ◦b ≤▽(a ◦b).
(1.4.3)
From (1.4.2), (1.4.3) and (O3) we obtain a ◦b = ▽(a ◦b).
■
Remark 1.33. The monotone directed roundings ▽and △are unique. Thus under
the hypothesis of Theorem 1.32 there exists exactly one monotone lower (resp. upper)
screen groupoid on a lower (resp. upper) screen or a screen. Because of this fact, we
shall use the special signs ▽
◦(resp. △
◦) for the associated operations. Theorem 1.32
implies that ▽
◦(resp. △
◦) can be deﬁned by the property
(RG)

a,b∈S
a▽
◦b = ▽(a ◦b)

resp.

a,b∈S
a△
◦b = △(a ◦b)

.
■
This formula can now be used to construct the result of a computation in a lower
(resp. upper) screen groupoid. We illustrate this by an example.
Example 1.34. Let Z := {ζ := ξ + iη ∈C | |ξ| ≤r ∧|η| ≤r}, M := PZ and
S ⊆M the set of all rectangles of PZ with sides parallel to the axes. We already
know from example 1 in Section 1.3 that {S, ⊆} is an upper screen of {M, ⊆}. Now
we deﬁne a multiplication in M by

a,b∈M
a · b := {α · β | α, β ∈Z ∧α ∈a ∧β ∈b}.
In order not to leave the set Z while executing the product, we replace the real
and/or imaginary part of the product α · β by r whenever the former and/or the latter

1.4 Arithmetic Operations and Roundings
37
do in fact exceed r. Then {M, ·} is a groupoid with the neutral element {1}. Now set
r = 5 and consider two special elements a, b ∈M. See Figure 1.12(a).
a := {ζ := ξ + iη ∈C | ξ ∈[1, 2] ∧η ∈[0, 1]},
b := {ei π
4 } = {1
2
√
2(1 + i)}.
Here a, b are elements of S ⊂M. Figure 1.12(b) shows the product a · b as well as
the construction of the product a△· b in the upper screen groupoid using the formula
a△· b = △(a · b).
a△· b
a · b
(b)
η
45◦
1
1
2
ξ
(a)
45◦
b
1
1
2
ξ
η
a
Figure 1.12. Operation in an upper screen groupoid.
When the groupoids are ordered, additional properties can be derived for them. If
{M, ◦, ≤} is an ordered groupoid, then every monotone screen groupoid {S,
◦, ≤}
is also an ordered groupoid. This follows directly from (OA’) and (RG2):

a,b,c,d∈S
(a ≤b ∧c ≤d ⇒
(OA’) a ◦c ≤b ◦d ⇒
(RG2) a ◦c ≤b ◦d),
(1.4.4)
i.e., (OA) and (OA’) hold in {S,
◦, ≤} also.
We have already observed that {R, +, ≤} is an ordered groupoid. If S ⊂R, for
instance, denotes a set of ﬂoating-point numbers, we can conclude from (1.4.4) that
{S, ▽
+ , ≤} and {S, △
+ , ≤} are ordered groupoids.
The developments of this section already have many applications. They can be
used, for instance, to compute lower and upper bounds of a ﬁnite sum
s :=
n

i=1
si,
with si ∈S, i = 1(1)n,

38
1 First Concepts
on the computer. Here again, let S ⊂R denote a set of ﬂoating-point numbers. We
obtain successively:
s1▽
+ s2 :=
(RG)
▽(s1 + s2) ≤
(R3) s1 + s2,
(1.4.5)
s1▽
+ s2▽
+ s3 :=
(RG)
▽((s1▽
+ s2) + s3) ≤
(R3)
s1▽
+ s2 + s3
≤
(1.4.5), (OA)R
s1 + s2 + s3.
(1.4.6)
Continuing this way leads to:
s1▽
+ s2▽
+ s3▽
+ · · · ▽
+ sn ≤s =
n

i=1
si ≤s1△
+ s2△
+ s3△
+ · · · △
+ sn.
(1.4.7)
If only lower bounds s(l)
i
and upper bounds s(u)
i
of the summands are available:
s(l)
i
≤si ≤s(u)
i , these bounds can be used to obtain lower and upper bounds of the
sum s:
s(l)
1 ▽
+ s(l)
2 ▽
+ · · · ▽
+ s(l)
n
≤
(1.4.7)
n

i=1
s(l)
i
≤
(OA)R
n

i=1
si
≤
(OA)R
n

i=1
s(u)
i
≤
(1.4.7)
s(u)
1 △
+ s(u)
2 △
+ · · · △
+ s(u)
n .
(1.4.8)
Equation (1.4.8) can be used, for instance, to compute lower and upper bounds of
a scalar product
n

i=1
ai · bi,
ai, bi ∈S
on the computer. In this case the bounds s(l)
i
and s(u)
i
for the summands si := ai · bi
in (1.4.8) are obtained by
s(l)
i
:= ai▽· bi :=
(RG)
▽(ai · bi) ≤si ≤s(u)
i
:= ai△· bi :=
(RG) △(ai · bi).
We stress the fact that when computing bounds for a scalar product this way the lower
(resp. upper) bound of the sum is obtained by performing all arithmetic operations
with rounding downwards (resp. upwards). Since changing the rounding mode is
a slow process on many computers, the described methods are relatively fast. We
mention, however, that the computed bounds are not optimal. We shall see later in
this treatise that most accurate bounds for scalar products can be computed much
faster by ﬁxed-point accumulation and by making use of pipelining in a very natural
way.

1.4 Arithmetic Operations and Roundings
39
In a similar manner bounds for sums of other expressions can be computed by
(1.4.8) if bounds for the summands are available. Sums of quotients (Taylor series)
might be an example.
Additional properties of ordered groupoids are given by the following theorem:
Theorem 1.35. Let {M, ◦, ≤} be a completely ordered groupoid with a right neutral
element e ∈M and {S, ▽
◦, ≤} the monotone lower screen groupoid (resp. {S, △
◦, ≤}
the monotone upper screen groupoid) with e ∈S. Then for all a, b ∈M the following
inequalities hold:
(▽a)▽
◦(▽b) ≤▽(a ◦b) ≤a ◦b

resp. a ◦b ≤△(a ◦b) ≤(△a)△
◦(△b)

.
Proof. ▽a ≤a ∧▽b ≤b ⇒
(OA’) (▽a) ◦(▽b) ≤a ◦b ⇒
(R2)
▽((▽a) ◦(▽b)) =
(RG)
(▽a)▽
◦(▽b) ≤▽(a ◦b) ≤
(R3) a ◦b.
■
The inequalities (▽a)▽
◦(▽b) ≤▽(a ◦b) (resp. △(a ◦b) ≤(△a)△
◦(△b)) of
this theorem assert in general that the monotone lower (resp. upper) screen groupoid
is not a homomorphic image of the ordered groupoid {M, ◦, ≤}. In concrete cases
such as in interval arithmetic, it is easy to show by simple examples that the equality
sign is not valid in general.
The outer inequality of Theorem 1.35
(▽a)▽
◦(▽b) ≤a ◦b

resp. a ◦b ≤(△a)△
◦(△b)

asserts that the computation on the screen is always on one side of the computation in
the groupoid {M, ◦, ≤}. This property remains valid even in expressions containing
many operations.
Moreover, if several different operations are deﬁned in M and if they are all ap-
proximated in S by lower (resp. upper) screen groupoids, then the previous assertion
is also valid for expressions containing several different operations. In particular, this
is the case in interval arithmetic, where the calculations are done on an upper screen
with inclusion as an order relation.
Properties of comparison of computations in different lower (resp. upper) screen
groupoids are derived in the following theorem.
Theorem 1.36. Let {M, ◦, ≤} be a completely ordered groupoid with the right neutral
element e. Let S and D be lower (resp. upper) screens of {M, ≤} with the property
e ∈S ⊆D ⊆M and ▽: M →S, ▽1 : M →D, ▽2 : D →S the mono-
tone downwardly directed roundings (resp. △: M →S, △1 : M →D, △2 :
D →S the monotone upwardly directed roundings) and {S, ▽
◦, ≤}, {D, ▽
◦1, ≤},

40
1 First Concepts
{S, ▽
◦2, ≤} (resp. {S, △
◦, ≤}, {D, △
◦1, ≤}, {S, △
◦2, ≤}) the corresponding mono-
tone lower (resp. upper) screen groupoids. Then
(a) {S, ▽
◦, ≤} = {S, ▽
◦2, ≤}
(resp. {S, △
◦, ≤} = {S, △
◦2, ≤}),
(b)

a,b∈M
(▽a)▽
◦(▽b) ≤(▽1a)▽
◦1(▽1b) ≤a ◦b

resp.

a,b∈M
a ◦b ≤(△1a)△
◦1(△1b) ≤(△a)△
◦(△b)

,
i.e., operations in a ﬁner monotone lower (resp. upper) screen groupoid always lead
to nondegraded results.
Proof. (a) By Theorem 1.32 we have for all a, b ∈S : a▽
◦2b = ▽2(a▽
◦1b) and
therefore by Theorem 1.27:
a▽
◦b = ▽(a ◦b) = ▽2(▽1(a ◦b)) = ▽2(a▽
◦1b) = a▽
◦2b.
(b) We have
▽2(▽1a) ≤▽1a ∧▽2(▽1b) ≤▽1b
⇒
(OA’)
▽2(▽1a) ◦▽2(▽1b) ≤(▽1a) ◦(▽1b)
⇒
(RG2)
▽2(▽1a)▽
◦▽2(▽1b) ≤(▽1a)▽
◦(▽1b).
■
Some of the concepts and results of this section will also be needed in the case of
outer operations. To have them available, we state the following Deﬁnition 1.37 and
Theorem 1.38.
Deﬁnition 1.37. Let {M, ≤} be a complete lattice and {S, ≤} a lower (resp. an upper)
screen of {M, ≤}. Further, let N be an operator set of M with an outer operation
◦: N × M →N. Let T ⊆N. An operation
◦: T × S →S is called an outer
screen operation if
(RG1)

α∈T

a∈S
(α ◦a ∈S ⇒α ◦a = α ◦a).
An outer screen operation is called monotone if
(RG2)

α,β∈T

a,b∈S
(α ◦a ≤β ◦b ⇒α ◦a ≤β ◦b).
An outer screen operation is called lower (resp. upper screen operation if
(RG3)

α∈T

a∈S
α ◦a ≤α ◦a

resp. 
α∈T

a∈S
α ◦a ≤α ◦a

.
■

1.4 Arithmetic Operations and Roundings
41
Theorem 1.38. Let {M, ≤} be a complete lattice, {S, ≤} a lower (resp. upper) screen
or a screen of {M, ≤}, and
: M →S a mapping. Further, let N be an operator
set of M with an operation ◦: N × M →M. Let T ⊆N, and let an operation
◦: T × S →S be deﬁned by
(RG)

α∈T

a∈S
α ◦a :=
(α ◦a).
(a) If
has the property (Ri), i = 1, 2, 3, deﬁned for roundings, then the operation
◦has the property (RGi), i = 1, 2, 3, respectively.
(b) If there exists an identity operator ϵ ∈N with the property

a∈N
ϵ ◦a = a,
and if ϵ ∈T ⊆N, then (RG1), (RG2) and (RG3) imply (RG).
■
We omit the proof of this theorem since it is completely analogous to that of The-
orem 1.32. Since the monotone directed roundings are unique, there exists exactly
one monotone lower (resp. upper) screen operation, which we therefore denote by ▽
◦
(resp. △
◦). By Theorem 1.38 these operations can also be deﬁned by the property
(RG)

α∈T

a∈S
α▽
◦a := ▽(α ◦a)

resp. 
α∈T

a∈S
α△
◦a := △(α ◦a)

.

Chapter 2
Ringoids and Vectoids
In this chapter we shall develop the concepts of a ringoid and a vectoid as
well as those of weakly ordered or ordered or inclusion-isotonally ordered
ringoids and vectoids. The development will be self-contained and, in par-
ticular, independent of Chapter 1. Chapters 3 and 4 will bring the contents
of the ﬁrst two chapters together.
This chapter begins with the deﬁnition of a ringoid and the derivation of
its most important properties. Then we show that the power set of a ringoid
is a ringoid, that the matrices over a weakly ordered or an ordered ringoid
form a weakly ordered or an ordered ringoid, and that the complexiﬁcation
of a weakly ordered ringoid also leads to a weakly ordered ringoid. Then
we deﬁne the concept of a vectoid and derive its properties. The power set
of a vectoid turns out to be a vectoid. The n-tuples over a ringoid R form
a vectoid over R, as do the matrices over R. Finally we show that ringoids
and vectoids also occur in sets of mappings into a ringoid R.
2.1
Ringoids
We begin with the following deﬁnition of a ringoid.
Deﬁnition 2.1. A nonempty set R in which an addition (+) and a multiplication1 (·)
are deﬁned is called a ringoid if the following properties hold:
(D1)

a,b∈R
a + b = b + a,
(D2)

o∈R

a∈R
a + o = a,
(D3)

e∈R\{o}

a∈R
a · e = e · a = a,
(D4)

a∈R
a · o = o · a = o.
(D5) There exists an element x ∈R such that
(a) e + x = o,
1We often write ab instead of a · b. We adopt the convention that multiplication and division are to
be executed before addition and subtraction. However, the usual priorities dictated by parentheses are
assumed. Several operations of the same priority are to be executed from left to right, i.e., a1 ◦a2 ◦. . . ◦
an−1 ◦an := (a1 ◦a2 ◦. . . ◦an−1) ◦an, for n ≥3.

2.1 Ringoids
43
(b) x · x = e,
(c)

a,b∈R
x(a + b) = (xa) + (xb),
(d)

a,b∈R
x(ab) = (xa)b = a(xb).
(D6) x is unique.
A ringoid R is called division ringoid if a division / : R × R\N →R is deﬁned
with respect to which properties (D7,8,9) hold. Here N ⊆R is some subset of R
which contains o.
(D7)

a∈R
a/e = a,
(D8)

a∈R\N
o/a = o,
(D9)

a∈R

b∈R\N
x(a/b) = (xa)/b = a/(xb).
A ringoid or a division ringoid is called weakly ordered if {R, ≤} is an ordered set
and
(OD1)

a,b,c∈R
(a ≤b ⇒a + c ≤b + c),
(OD2)

a,b∈R
(a ≤b ⇒xb ≤xa).
A weakly ordered ringoid is called ordered if
(OD3)

a,b,c∈R
(o ≤a ≤b ∧c ≥o ⇒ac ≤bc ∧ca ≤cb).
A weakly ordered division ringoid is called ordered if
(OD4)
(a)

a,b∈R

c∈R\N
(o ≤a ≤b ∧c > o ⇒o ≤a/c ≤b/c),
(b)

a,b∈R\N

c∈R
(o < a ≤b ∧c ≥o ⇒c/a ≥c/b ≥o).
A ringoid is called inclusion-isotonally ordered with respect to an order relation
{R, ⊆} if for all operations ◦∈{+, ·}
(OD5)

a,b,c,d∈R
(a ⊆b ∧c ⊆d ⇒a ◦c ⊆b ◦d).
A division ringoid is called inclusion-isotonally ordered if
(OD6)

a,b∈R

c,d∈R\N
(a ⊆b ∧c ⊆d ⇒a/c ⊆b/d).
■
A ringoid as well as a division ringoid is just a set of elements in which three special
elements o, e, and x exist and for which the rules concerning the operations +, ·, /
are given by Deﬁnition 2.1. (D7) and (D9) imply that e /∈N and that for all a /∈N,
xa /∈N as well. A ringoid may be weakly ordered or ordered with respect to one
order relation as well as inclusion-isotonally ordered with respect to another order
relation. The third special element x in a ringoid has many properties that in the real
or complex number ﬁeld in particular distinguish the element −1. This motivates the
following deﬁnition.

44
2 Ringoids and Vectoids
Deﬁnition 2.2. In a ringoid {R, +, ·} we deﬁne a minus operator by

a∈R
−a := xa,
(2.1.1)
and a subtraction by

a,b∈R
a −b := a + (−b).
■
Setting a = e in (2.1.1) and using (D3) gives
x = −e.
(2.1.2)
Using (2.1.2), several of the rules of Deﬁnition 2.1 can be written in a simpler form:
(D5) There exists an element −e ∈R such that
(a) e + (−e) = o,
(b) (−e) · (−e) = e,
(c)

a,b∈R
−(a + b) = (−a) + (−b),
(d)

a,b∈R
−(ab) = (−a)b = a(−b).
(D9)

a∈R

b∈R\N
−(a/b) = (−a)/b = a/(−b).
(OD2)

a,b∈R
(a ≤b ⇒−b ≤−a),
Every ring with a unit element is also a ringoid. Thus a ringoid represents a certain
generalization of a ring with a unit element. In order to see this, we recall that a ring
with a unit element is an additive group with an associative multiplication with respect
to which a neutral element exists. The two distributive laws a(b + c) = ab + ac and
(a + b)c = ac + bc are also valid. From these properties the rules (D1), (D2), (D3),
(D5a), (D5c), (D5d) and (D6) of a ringoid follow immediately. The proofs of (D4)
and (D5b) run as follows:
(D4): a · o = a · ((−e) + e) = −a + a = o = ((−e) + e) · a = o · a,
(D5b): (−e) · o = (−e)((−e) + e) = (−e)(−e) + (−e) = o ⇒(−e)(−e) = e.
Here the last implication follows from the uniqueness of the additive inverse of −e
in the ring.
In a ringoid many familiar properties of a ring with a unit element still hold. Some
of them are summarized in the following theorem.
Theorem 2.3. Let {R, +, ·} be a ringoid with the neutral elements o and e. Then for
all a, b, c, d ∈R the following properties hold:
(a) o and e are unique, and −e ̸= o,

2.1 Ringoids
45
(b) o −a = −a = (−e) · a = a · (−e),
(c) −(−a) = a,
(d) −(a −b) = −a + b = b −a,
(e) (−a)(−b) = a · b.
(f) −e is the unique solution of the equation (−e) · z = e.
(g) a · e = o implies a = o and −a = o implies a = o.
(h) a −z = a ⇒z = o, i.e., o is the only right neutral element of subtraction.
In a division ringoid {R, N, +, ·, /} the following additional properties hold:
(i) (−a)/(−b) = a/b,
(j) (−e)/(−e) = e.
(k) If for all a, a/a = e, then e is the only right neutral element of division.
In a weakly ordered ringoid {R, +, ·, ≤} we have
(l) −e ̸= e,
(m) a ≤b ∧c ≤d ⇒a + c ≤b + d,
(n) a < b ⇒−b < −a.
In an ordered ringoid we have moreover
(o) o ≤a ≤b ∧o ≤c ≤d ⇒o ≤ac ≤bd ∧o ≤ca ≤db,
(p) a ≤b ≤o ∧c ≤d ≤o ⇒o ≤bd ≤ac ∧o ≤db ≤ca,
(q) a ≤b ≤o ∧o ≤c ≤d ⇒ad ≤bc ≤o ∧da ≤cb ≤o.
In a linearly ordered ringoid {R, +, ·, ≤} we have further
(r) a ≤b ∧c ≥o ⇒ac ≤bc ∧ca ≤cb,
(s) a ≤b ∧c ≤o ⇒ac ≥bc ∧ca ≥cb,
(t) a + (−a) = o, i.e., −a is an additive inverse of a,
(u) e > o ∧−e < o.
In an ordered division ringoid {R, N, +, ·, /, ≤} we have
(v) a > o ∧b > o ⇒a/b ≥o,
(w) a < o ∧b > o ⇒a/b ≤o ∧b/a ≤o,
(x) a < o ∧b < o ⇒a/b ≥o.
In a completely and weakly ordered ringoid we have
(y)

∅̸=A∈PR
(inf A = −sup(−A) ∧sup A = −inf(−A)).
In an inclusion-isotonally ordered ringoid {R, +, ·, ⊆}, (OD5) is also valid with
subtraction
(z)

a,b,c,d∈R
(a ⊆b ∧c ⊆d ⇒a −c ⊆b −d).

46
2 Ringoids and Vectoids
Proof. The proof of most of the properties is straightforward. Thus we give a concise
sketch of each such proof. This procedure will be typical in this chapter.
(a) A neutral element is unique. (−e)(−e) = e ⇒
(D4) −e ̸= o.
(b) o −a = o + (−a) = −a = (−e)a =
(D5d) a(−e).
(c) −(−a) = (−e)((−e)a) =
(D5d) ((−e)(−e))a =
(D5b) a.
(d) −(a −b) = (−e)(a + (−e)b) =
(D5c) (−e)a + (−e)((−e)b) =
(c) −a + b =
(D1) b −a.
(e) (−a)(−b) = ((−e)a)((−e)b) =
(D5d) (−e)((−e)a)b =
(c) ab.
(f) Assume that a solves (−e)z = e. Then −e = (−e) · e = (−e)((−e)a) =
(c) a.
(g) e · a = o ⇒a = o by (D3). −a = (−e)a = o ⇒
(D4) (−e)((−e)a) = o =
(c) a = o.
(h) a−o = a+(−o) =
(D4) a+o =
(D2) a, i.e., o is a right neutral element of subtraction.
Assume o′ is another one. Then for all a ∈R, a−o′ = a+(−o′) = a ⇒
(a) −o′ = o
⇒
(g) o′ = o.
(i) (−a)/(−b) = ((−e)a)/((−e)b) =
(D9) ((−e)((−e)a))/b =
(c) a/b.
(j) (−e)/(−e) =
(i) e/e =
(D7) e.
(k) Assume that e and e′ are right neutral elements. Then e = e′/e′ = e′.
(l) Assume that −e = e. Then a ≤b ⇒−a ≤−b in contradiction to (OD2).
(m) a ≤b ∧c ≤d ⇒a + c ≤b + c ≤b + d.
(n) a < b :⇔a ≤b ∧a ̸= b ⇒
(OD2) −b ≤−a ∧−b ̸= −a ⇔: −b < −a.
(o) o ≤a ≤b ∧o ≤c ≤d ⇒
(OD3) o ≤ac ≤bc ≤bd.
(p) a ≤b ≤o ∧c ≤d ≤o
⇒o ≤−b ≤−a ∧o ≤−d ≤−c ⇒
(o) o ≤(−b)(−d) ≤(−a)(−c) ⇒
(e) o ≤bd ≤
ac.
(q) a ≤b ≤o ∧o ≤c ≤d
⇒o ≤−b ≤−a ∧o ≤c ≤d ⇒
(o) o ≤(−b)c ≤(−a)d ⇒ad ≤bc ≤o.
(r)
(1) o ≤a ≤b ∧c ≥o ⇒
(OD3) o ≤ac ≤bc,
(2) a ≤b ≤o ∧c ≥o ⇒
(q) ac ≤bc ≤o,
(3) a ≤o ≤b ∧c ≥o ⇒ac ≤o ≤bc.
(s) a ≤b ∧c ≤o ⇒a ≤b ∧o ≤−c ⇒
(r) a(−c) ≤b(−c) ⇒
(OD2) bc ≤ac.

2.1 Ringoids
47
(t) b := a + (−a) ⇒−b = (−e)(a + (−a))
=
(D5c),(D2) a + (−a) = b.
(O4) ⇒b ≥o ∨b ≤o ⇒
(OD2) −b = b ≤o ∨−b = b ≥o ⇒
(O3) b = o.
(u) (D3) ⇒e ̸= o ⇒
(O4) e > o ∨e < o. Assume e < o ⇒
(p) e · e = e ≥o, i.e., e > o,
which is a contradiction ⇒e > o ⇒−e < o.
(v) (OD4).
(w) a < o ∧b > o ⇒
(n) −a > o ∧b > o ⇒
(v) (−a)/b ≥o ⇒
(OD2) (−a)/(−b) =
(i) a/b ≤o.
(x) a < o ∧b < o ⇒
(n) −a > o ∧−b > o ⇒
(v) (−a)/(−b) = a/b ≥o.
(y)
(1)

a∈A
(inf A ≤a ⇒
(OD2) −a ≤−inf A ⇒sup({−e} · A) ≤−inf A
⇒
(OD2) inf A ≤−sup(−A)).
(2)

a∈A
(−a ≤sup({−e} · A) ⇒
(OD2) −sup(−A) ≤a
⇒−sup(−A) ≤inf A).
(1) and (2) ⇒
(O3) inf A = −sup(−A).
(z) Employing (OD5) with multiplication we obtain

c,d∈R
(c ⊆d ⇒(−e) · c ⊆(−e) · d ⇒−c ⊆−d,
a ⊆b ∧c ⊆d ⇒a ⊆b ∧−c ⊆−d
⇒a −c = a + (−c) ⊆b + (−d) = b −d).
■
Since Theorem 2.3 is quite ramiﬁed, we offer the following comments by way of
summary. Theorem 2.3 shows that in a ringoid or division ringoid the minus operator
has the same properties as in the real or complex number ﬁeld. Compared to other
algebraic structures, a ringoid is distinguished by the fact that the existence of inverse
elements is not assumed. Nevertheless, subtraction is not an independent operation.
It can be deﬁned by means of the operations of multiplication and addition.
In a weakly ordered (resp. an ordered) ringoid for all elements that are comparable
with o with respect to ≤and ≥, the same rules for inequalities hold as for complex
(resp. real) numbers. Since in a linearly ordered ringoid, all elements are comparable
to o with respect to ≤and ≥, then for all elements the same rules for inequalities hold
as for inequalities among real numbers.
It is evident but important to note for further applications that {R, +, ·} and
{C, +, ·} are ringoids, while {R, {0}, +, ·, /} and {C, {0}, +, ·, /} are division
ringoids. {R, +, ·, ≤} is a linearly ordered ringoid. However, if the order relation
is deﬁned componentwise, {C, +, ·, ≤} is only a weakly ordered ringoid. The proper-
ties (OD1) and (OD2) are easily veriﬁed. We show that (OD3) is not valid by means
of a simple example. Let
0 ≤α := ei 3π
8 ≤β := 2ei 3π
8 ,
0 ≤γ := ei π
4 .

48
2 Ringoids and Vectoids
Then
α · γ = ei 5π
8
and
β · γ = 2ei 5π
8
and thus Re(β · γ) ≤Re(α · γ).
The following theorem plays a key role in subsequent chapters when we study the
spaces listed in Figure 1.
Theorem 2.4. In a linearly ordered ringoid {R, +, ·, ≤} the property (D6) is a con-
sequence of the remaining properties (D1,2,3,4,5), (O1,2,3,4), (OD1,2,3).
Proof. Setting b = e in (D5d) implies

a∈R
x · a = a · x.
(2.1.3)
Suppose that two elements x, y ∈R fulﬁll the properties (D5) and (OD2). Then,
without loss of generality, we can assume by (O4) that x ≤y. Then
x ≤y ⇒
(OD2) xy ≤xx =
(D5b) e ∧yy
=
(D5b) e ≤yx.
(2.1.4)
By (2.1.3) xy = yx, and therefore by (2.1.4) and (O3) xy = e. If we multiply the
equation by x, we obtain y = x.
■
Now let R be a nonempty set with elements a, b, c, . . ., and let PR be the power
set of R. For any operation ◦deﬁned for elements of R, we deﬁne corresponding
operations ◦in PR by

A,B∈PR
A ◦B := {a ◦b | a ∈A ∧b ∈B}.
(2.1.5)
The least element in PR with respect to set inclusion as an order relation is the
empty set ∅. The empty set is subset of any set. If in (2.1.5) A or B is the empty set,
then the result of the operation is deﬁned to be the empty set. By interpretation as a
limit process the latter case is formally included in (2.1.5).
If {R, ◦} is a group with neutral element e, then the power set {PR, ◦} is not a
group. To see this, we consider the equation A ◦X = {e} in PR:
A ◦X = {a ◦x | a ∈A ∧x ∈X} = {e}.
The only solutions of this equation occur for A = {a} and X = {a−1} with a ∈R.
From this observation we conclude that the power set of a ring, a ﬁeld, a vector
space, a matrix algebra, is not, respectively, a ring, a ﬁeld, a vector space, a matrix
algebra. However, the following theorem shows that ringoid properties are preserved
upon passage to the power set. Because of the property {o}·∅= ∅·{o} = ∅̸= {o},
however, the empty set has to be excluded.

2.1 Ringoids
49
Theorem 2.5. If {R, +, ·} is a ringoid with the special elements o, e, and x, then
{PR \{∅}, +, ·} is also a ringoid with the special elements {o}, {e}, and {x}. If
{R, N, +, ·, /} is a division ringoid, then {PR \{∅}, N∗, +, ·, /} is also a division
ringoid, where N∗:= {A ∈PR | A ∩N ̸= ∅}. Furthermore, {PR\{∅}, +, ·, ⊆}
(resp. {PR \{∅}, N∗, +, ·, /, ⊆}) is an inclusion-isotonally ordered ringoid (resp.
division ringoid).
Proof. The properties (D1,2,3,4,7,8), and (OD5) (resp. (OD6)) follow directly from
the deﬁnition of the operations in the power set and the corresponding properties in
R. The properties (D5) and (D9) in PR are easily veriﬁed for X = {−e}. It remains
to prove (D6):
Let S denote the subset of PR consisting of the elements {a}, {b}, {c}, . . . with
a, b, c, . . . ∈R. Then the mapping ϕ : R →S deﬁned by 
a∈R ϕa = {a} is
obviously an isomorphism.2
We have to show that X = {−e} is the only element in PR that has the properties
(D5a,b,c,d). Let X be any element with these properties. Then we have, in particular
(D5b) X · X = {x · y | x, y ∈X} = {e},
(D5d)

A,B∈PR
X(AB) = (XA)B = A(XB).
From (D5b) we obtain directly

x,y∈X
x · x = x · y = y · x = y · y = e.
(2.1.6)
With this we obtain from (D5d) with A = {x}, B = {y} for any x, y ∈X:
X · {e} = {e} · {y} = {x} · {e}.
(2.1.7)
This means that X contains only one element x ∈R. X = {x} still has to satisfy
(D5a,b,c,d) in PR for all A, B ∈PR, and in particular for A = {a} and B = {b} in
S. Since S is isomorphic to R we obtain x = −e. Therefore, X = {−e} is the only
element in PR that satisﬁes (D5).
■
Using Theorem 2.5, it is clear that {PR\{∅}, N, +, ·, /, ⊆} with N := {A ∈PR |
0 ∈A} and {PC\{∅}, N′, +, ·, /, ⊆} with N′ := {A ∈PC | 0 ∈A} are inclusion-
isotonally ordered division ringoids with the neutral elements {0} and {1} and with
X = {−1}.
2Let M and N be algebraic structures and let a one-to-one correspondence exist between the op-
erations ◦in M and ⋄in N.
A one-to-one mapping ϕ : M →N is called an isomorphism if
V
a,b∈M ϕa ⋄ϕb = ϕ(a ◦b). Then M and N are called isomorphic.

50
2 Ringoids and Vectoids
Now we consider matrices over a given ringoid R. Let P := {1, 2, . . . , m} and
Q := {1, 2, . . . , n}. A matrix is deﬁned as a mapping ϕ : P × Q →R. We denote
the set of all such matrices by MmnR. Then
MmnR =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
⎛
⎜
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn
⎞
⎟
⎟
⎟
⎠

aij ∈R, i = 1(1)m, j = 1(1)n
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
.
We shall often use the more concise notation A = (aij), B = (bij) ∈MmnR.
Equality of and addition and multiplication for matrices are deﬁned by
(aij) = (bij) :⇔aij = bij, for all i = 1(1)m and j = 1(1)n,

(aij),(bij)∈MmnR
(aij) + (bij) := (aij + bij),

(aij)∈MmnR

(bij)∈MnpR
(aij) · (bij) :=
⎛
⎝
n

j=1
aijbjk
⎞
⎠∈MmpR.
If {R, ≤} is an ordered set, an order relation ≤is deﬁned in MmnR by
(aij) ≤(bij) :⇔aij ≤bij, for all i = 1(1)m and j = 1(1)n.
If m = n, we simply write MnR.
The following theorem establishes ringoid properties of MnR.
Theorem 2.6. If {R, +, ·} is a ringoid with the neutral elements o and e, then
{MnR, +, ·} is also a ringoid, and its neutral elements are
O =
⎛
⎜
⎜
⎜
⎝
o
o
. . .
o
o
o
...
...
...
...
...
o
o
. . .
o
o
⎞
⎟
⎟
⎟
⎠,
E =
⎛
⎜
⎜
⎜
⎝
e
o
. . .
o
o
e
...
...
...
...
...
o
o
. . .
o
e
⎞
⎟
⎟
⎟
⎠,
X = −E =
⎛
⎜
⎜
⎜
⎝
−e
o
. . .
o
o
−e
...
...
...
...
...
o
o
. . .
o
−e
⎞
⎟
⎟
⎟
⎠.
If, moreover, {R, +, ·, ≤} is weakly ordered (resp. ordered), then {MnR, +, ·, ≤}
is a weakly ordered (resp. an ordered) ringoid.

2.1 Ringoids
51
Proof. The properties (D1,2,3,4) and (OD1,2) follow immediately from the deﬁnition
of the operations in MnR and the corresponding properties in R. The properties (D5)
are easily veriﬁed for X = −E. It remains to demonstrate (D6) and (OD3).
(D6): Let S denote the set of diagonal matrices3 with all diagonal elements the same.
Then the mapping ϕ : R →S deﬁned by
ϕa =
⎛
⎜
⎜
⎜
⎝
a
o
. . .
o
o
a
...
...
...
...
...
o
o
. . .
o
a
⎞
⎟
⎟
⎟
⎠
is obviously an isomorphism.
Now let X = (xij) be any element of MnR which fulﬁlls (D5). Then we have in
particular because of (D5a):
E + X = O.
This means that xij = o for all i ̸= j, i.e., X is a diagonal matrix with diagonal
entries xii, i = 1(1)n. From (D5d) we obtain for B = E:
X · A = A · X for all A = (aij) ∈MnR.
If we now choose aij = e for all i, j = 1(1)n, we obtain x11 = x22 = . . . = xnn,
and because of the isomorphism ϕ, xii = x = −e for all i = 1(1)n, i.e., X is a
diagonal matrix with the constant diagonal element x = −e. (D6) holds in MnR.
(OD3): O ≤A ≤B ∧C ≥O ⇒o ≤aij ≤bij ∧cij ≥o, for all i = 1(1)n
⇒
(OD3)R o ≤aircrj ≤bircrj, for all i, j, r = 1(1)n
⇒
Theorem 2.3(m)
n
r=1
aircrj ≤
n
r=1
bircrj, for all i, j = 1(1)n
⇒AC ≤BC.
■
Theorem 2.6 shows that {MnR, +, ·, ≤} is an ordered ringoid and that {MnC,
+, ·, ≤} is a weakly ordered ringoid. Furthermore, from Theorem 2.5 we conclude
that both {PMnR \ {∅}, +, ·, ⊆} and {PMnC\{∅}, +, ·, ⊆} are inclusion-isotonally
ordered ringoids.
Now we consider the process of complexiﬁcation of a given ringoid (resp. division
ringoid) R. We denote the set of all pairs over R by CR := {(a1, a2) | a1, a2 ∈R}.
Here a1 is called the real part of (a1, a2), while a2 is called the imaginary part. Equal-
ity, addition, multiplication, and division of elements α = (a1, a2), β = (b1, b2) ∈
3A matrix is called a diagonal matrix if aij = 0 for all i ̸= j. If the diagonal entries are all zero, we
call it the zero matrix; if the diagonal entries are all e, we call it the unit matrix.

52
2 Ringoids and Vectoids
CR are deﬁned by
α = β :⇔a1 = b1 ∧a2 = b2,
α + β := (a1 + b1, a2 + b2), for all α, β ∈CR,
α · β := (a1b1 −a2b2, a1b2 + a2b1), for all α, β ∈CR,
α/β := ((a1b1 + a2b2)/b, (a2b1 −a1b2)/b), with b = b1b1 + b2b2,
for all α, β ∈CR \ N and
N := {γ = (c1, c2) ∈CR | c1c1 + c2c2 ∈N}.
If {R, ≤} is an ordered set, an order relation ≤is deﬁned in CR by
(a1, a2) ≤(b1, b2) :⇔a1 ≤b1 ∧a2 ≤b2.
The following theorem shows that ringoid properties are preserved under complex-
iﬁcation.
Theorem 2.7. If {R, +, ·} is a ringoid with the neutral elements o and e, then
{CR, +, ·} is also a ringoid with the neutral elements ω = (o, o) and ϵ = (e, o).
Moreover −ϵ = (−e, o). If {R, N, +, ·, /} is a division ringoid, then {CR, N, +, ·, /}
is also a division ringoid. If {R, +, ·, ≤} is a weakly ordered ringoid, {CR, +, ·, ≤}
is likewise a weakly ordered ringoid.
Proof. The properties (D1,2,3,4,7,8) and (OD1,2) follow directly from the deﬁnition
of the operations and the corresponding properties in R. The properties (D5) and (D9)
can be shown to be valid for ξ = (−e, o) ∈CR: (D5a) and (D5b) are easily veriﬁed
for this ξ. (D5c), (D5d) and (D9) can be proved in a straightforward manner by using
the property
ξα = (−e, o)(a1, a2) = (−a1, −a2).
The only remaining property is (D6).
(D6): If we denote the set of all elements of CR with absent imaginary part by S,
then the mapping ϕ : R →S, deﬁned by ϕa = (a, o) for all a ∈R, is obviously an
isomorphism.
Now let us assume that η = (x, y) is any element of CR that satisﬁes (D5). η must
satisfy (D5a), which means
ϵ + η = ω ⇔e + x = o ∧y = o,
i.e., η is of the form η = (x, o). Since η satisﬁes (D5a,b,c,d) for all α, β ∈CR, it
does so in particular for α = (a, o) and β = (b, o) with a, b ∈R. Because of the
isomorphism R ↔S, the properties (D5a,b,c,d) in CR for elements (a, o) and (b, o)
are equivalent to (D5a,b,c,d) in R. Since R is a ringoid, there exists only one element
x = −e. Therefore ξ = (−e, o) is the only element in CR that satisﬁes (D5).
■

2.2 Vectoids
53
2.2
Vectoids
We begin our discussion of vectoids with the following deﬁnition.
Deﬁnition 2.8. Let {R, +, ·} be a ringoid with elements a, b, c, . . ., and with neutral
elements o and e. Let {V, +} be a groupoid with elements a, b, c, . . ., and following
properties:
(V1)

a,b∈V
a + b = b + a,
(V2)

o∈V

a∈V
a + o = a.
V is called an R-vectoid {V, R} if there is an outer multiplication · : R × V →V
deﬁned, which with the abbreviation

a∈V
−a = (−e) · a
has the following properties:
(VD1)

a∈R

a∈V
(a · o = o ∧o · a = o),
(VD2)

a∈V
e · a = a,
(VD3)

a,b∈V
−(a + b) = (−a) + (−b),
(VD4)

a∈R

a∈V
−(a · a) = (−a) · a = a · (−a).
An R-vectoid is called multiplicative if a multiplication in V with the following
properties is deﬁned:
(V3)

e∈V \{o}

a∈V
a · e = e · a = a,
(V4)

a∈V
a · o = o · a = o,
(V5) e −e = o,
(VD5)

a,b∈V
−(ab) = (−a)b = a(−b).
Now let {R, +, ·, ≤} be a weakly ordered ringoid. Then an R-vectoid or a multi-
plicative R-vectoid is called weakly ordered if {V, ≤} is an ordered set4 which has
the following two properties:
(OV1)

a,b,c∈V
(a ≤b ⇒a + c ≤b + c),
(OV2)

a,b∈V
(a ≤b ⇒−b ≤−a).
4Since it is always clear by the context which order relation is meant, we denote the order relation in
V and R by the same sign ≤.

54
2 Ringoids and Vectoids
A weakly ordered R-vectoid is called ordered if
(OV3)

a,b∈R

a,b∈V
(o ≤a ≤b ∧o ≤a ⇒a · a ≤b · a
∧
o ≤a ∧o ≤a ≤b ⇒
a · a ≤a · b).
A multiplicative vectoid is called ordered if it is an ordered vectoid and
(OV4)

a,b,c∈V
(o ≤a ≤b ∧o ≤c ⇒a · c ≤b · c ∧c · a ≤c · b).
An R-vectoid that may be multiplicative is called inclusion-isotonally ordered with
respect to order relations {R, ⊆} and {V, ⊆} if
(OV5)
(a)

a,b∈R

c,d∈V
(a ⊆b ∧c ⊆d ⇒a · c ⊆b · d),
(b)

a,b,c,d∈V
(a ⊆b ∧c ⊆d ⇒a ◦c ⊆b ◦d), ◦∈{+, ·}.
■
A vectoid (resp. a multiplicative vectoid) is just a set of elements in which one
(resp. two) special element o (and e) exists and which obeys rules concerning inner
and outer operations as enumerated in Deﬁnition 2.8. A vectoid may be weakly or-
dered or ordered with respect to one order relation and inclusion-isotonally ordered
with respect to another order relation. In the following deﬁnition, we use the minus
operator of a vectoid in order to deﬁne a subtraction.
Deﬁnition 2.9. In an R-vectoid {V, R} we deﬁne a subtraction by

a,b∈V
a −b = a + (−b).
■
Every vector space is a vectoid. This means that a vectoid represents a certain
substructure or a generalization of a vector space. To see this, we recall that the latter
is an additive group with an outer multiplication with the elements of a ﬁeld with the
following properties:
(1) a(a + b) = aa + ab,
(2) (a + b)a = aa + ba,
(3) a(ba) = (ab)a,
(4) ea = a.
These properties imply (V1) and (V2) directly. (VD2) is the same as (4). (VD4)
follows from (3) upon setting a = −e (resp. b = −e), and (VD3) follows from (1)
upon setting a = −e. To show (VD1), we argue as follows:
oa = (e −e)a = a −a = o,
ao = ao + ao −ao =
(1) a(o + o) −ao = ao −ao = o.
A vectoid possesses many of the familiar properties of a vector space. Some of
them are summarized by the following theorem.

2.2 Vectoids
55
Theorem 2.10. Let {V, R} be a vectoid with the neutral element o and with the neutral
element e if a multiplication exists. Then for all a, b ∈R, and for all a, b, c, d ∈V ,
the following properties hold:
(a) o and e are unique,
(b) o −a = −a,
(c) −(−a) = a,
(d) −(a −b) = −a + b = b −a,
(e) (−a)(−a) = aa,
(f) −a = o ⇔a = o,
(g) a −z = o ⇔z = o, i.e., o is the only right neutral element of subtraction.
In a multiplicative vectoid, we have further
(h) −a = (−e) · a = a · (−e),
(i) (−a) · (−b) = a · b,
(j) −e is the unique solution of the equation (−e) · z = e.
In a weakly ordered vectoid {V, R, ≤}, we have
(k) a ≤b ∧c ≤d ⇒a + c ≤b + d,
(l) a ≤b ⇒−b ≤−a.
In an ordered vectoid, we have additionally
(m) o ≤a ≤b ∧o ≤c ≤d ⇒o ≤ac ≤bd,
(n) a ≤b ≤o ∧o ≤c ≤d ⇒ad ≤bc ≤o,
(o) a ≤b ≤o ∧c ≤d ≤o ⇒o ≤bd ≤ac,
(p) o ≤a ≤b ∧c ≤d ≤o ⇒bc ≤ad ≤o.
In an ordered multiplicative vectoid, we have further
(q) o ≤a ≤b ∧o ≤c ≤d ⇒o ≤ac ≤bd ∧o ≤ca ≤db,
(r) a ≤b ≤o ∧o ≤c ≤d ⇒ad ≤bc ≤o ∧da ≤cb ≤o,
(s) a ≤b ≤o ∧c ≤d ≤o ⇒o ≤bd ≤ac ∧o ≤db ≤ca,
(t) o ≤a ≤b ∧c ≤d ≤o ⇒bc ≤ad ≤o ∧cb ≤da ≤o.
In a completely and weakly ordered vectoid, we have
(u)

A∈PV
inf A = −sup(−A) ∧sup A = −inf(−A).
In an inclusion-isotonally ordered vectoid {V, R, ⊆}, (OV5) holds for subtraction:
(v)

a,b,c,d∈V
(a ⊆b ∧c ⊆d ⇒a −c ⊆b −d).
Proof. We omit the explicit proof of this theorem since, using the corresponding prop-
erties of a vectoid, it is completely analogous to that of Theorem 2.3.
■

56
2 Ringoids and Vectoids
Theorem 2.10 allows us to assert that in a vectoid or a multiplicative vectoid the
same rules for the minus operator hold as in a vector space or a matrix algebra over
the real or complex number ﬁelds. Compared to other algebraic structures, a vectoid
is distinguished by the fact that the existence of inverse elements is not assumed.
Nevertheless, subtraction is not an independent operation. It can be deﬁned by means
of the outer multiplication and addition.
In an ordered vectoid (resp. multiplicative vectoid) for all elements that are com-
parable with o with respect to ≤and ≥, the same rules for inequalities hold as in the
vector space Rn over the real numbers (resp. in the algebra of matrices over the real
numbers). In a weakly ordered vectoid for all elements that are comparable with o
with respect to ≤and ≥, the same rules for inequalities hold as in the vector space
C n over the complex numbers.
In a multiplicative vectoid, the inner operations + and · always have the properties
(D1,2,3,4,5) of a ringoid. The properties (D1,2,3,4,5a) are identical with (V1,2,3,4,5),
while (D5b,c,d) can easily be proved:
(D5b) (−e)(−e)
=
Theorem 2.10(i) e · e = e,
(D5c) (−e)(a + b) =
(h) −(a + b)
=
(VD3) (−a) + (−b) =
(h) (−e)a + (−e)b,
(D5d) (−e)(a · b) =
(h) −(ab)
=
(VD5) (−a)b = a(−b) =
(h) ((−e)a)b = a((−e)b).
It can be shown by examples [132], however, that a multiplicative vectoid is not
necessarily a ringoid.
Powersets of vectoids are vectoids as well. This property is made precise by the
following theorem. As in the case of a ringoid, the empty set again has to be excluded.
(VD1) cannot be satisﬁed for the empty set. We have instead ∅· {o} = {o}∅= ∅̸=
{o}.
Theorem 2.11. If {V, R} is a vectoid with the neutral element o, then {PV \{∅}, PR\
{∅}, ⊆} is an inclusion-isotonally ordered vectoid with the neutral element {o}. If
{V, R} is multiplicative and e is the neutral element of multiplication, then {PV \
{∅}, PR \ {∅}, ⊆} is an inclusion-isotonally ordered multiplicative vectoid and {e}
is its neutral element of multiplication.
Proof. The proof is straightforward. All properties can be obtained from the corre-
sponding properties in {V, R} and the deﬁnition of the operations in the power set.
■
Once again let us consider matrices over a given ringoid R. In addition to the inner
operations and the order relation deﬁned above in MmnR, we now deﬁne an outer
multiplication by

a∈R

(aij)∈MmnR
a · (aij) := (a · aij).
Then the following theorem holds.

2.2 Vectoids
57
Theorem 2.12. Let {R, +, ·} be a ringoid with the neutral elements o and e. Then
{MmnR, R} is a vectoid.
The neutral element is the matrix all components of
which are o. If {R, +, ·, ≤} is a weakly ordered (resp. an ordered) ringoid, then
{MmnR, R, ≤} is a weakly ordered (resp. an ordered) vectoid.
Proof. We omit the proof of this theorem since it is straightforward.
■
For m = 1 we obtain matrices that consist only of one column. Such matrices are
called vectors. The product set VnR := R × R × . . . × R = M1n represents the set
of all vectors with n components. In this case the last theorem specializes into the
following form.
Theorem 2.13. Let {R, +, ·} be a ringoid with the neutral elements o and e. Then
{VnR, R} is a vectoid. The neutral element is the zero vector, i.e., the vector all
components of which are o. If {R, +, ·, ≤} is a weakly ordered (resp. an ordered)
ringoid, then {VnR, R, ≤} is a weakly ordered (resp. an ordered) vectoid.
■
Another important specialization of the m×n-matrices is the set of n×n-matrices
MnR. In this case we obtain the following specialization of Theorem 2.12.
Theorem 2.14. Let {R, +, ·} be a ringoid with neutral elements o and e.
Then
{MnR, R} is a multiplicative vectoid. The neutral elements are the zero matrix and
the unit matrix. If {R, +, ·, ≤} is a weakly ordered (resp. an ordered) ringoid, then
{MnR, R, ≤} is a weakly ordered (resp. an ordered) multiplicative vectoid.
Proof. (V1,2,3,4,5) follow immediately from the properties (D1,2,3,4,5a) of the
ringoid MnR. Likewise, (OV4) follows from (OD3). The properties (VD1,2,3,4)
and (OV1,2,3) were already veriﬁed in Theorem 2.12. The remaining property (VD5)
follows from (D5d) in MnR and the property (h) of Theorem 2.10.
■
Among the consequences of these theorems we may note again that {VnR, R, ≤} is
an ordered vectoid, while {VnC, C, ≤} is a weakly ordered vectoid, {MnR, R, ≤} is
an ordered multiplicative vectoid, and {MnC, C, ≤} is a weakly ordered multiplica-
tive vectoid. We shall use these theorems later for many other applications.
We have also deﬁned a multiplication of an m × n-matrix by an n × p-matrix. The
most important case of this multiplication is that of a matrix by a vector. The result
then is a vector, and the structure to be described in the following theorem matches
that of a vectoid.
Theorem 2.15. Let {R, +, ·} be a ringoid, {MnR, +, ·} the ringoid of n×n-matrices
over R, and VnR the set of n-tuples over R. Then {VnR, MnR} is a vectoid. The
neutral element is the zero vector. If {MnR, +, ·, ≤} is a weakly ordered (resp. an or-
dered) ringoid, then {VnR, MnR, ≤} is a weakly ordered (resp. an ordered) vectoid.

58
2 Ringoids and Vectoids
Proof. The properties (V1,2), (VD1,2,4), (OV1,2) follow immediately from the deﬁ-
nition of the operations. The properties (VD3) and (OV3) can be proved in complete
analogy of (D5c) and (OD3) in MnR (Theorem 2.6).
■
As a consequence of this theorem, of course, {VnR, MnR, ≤} is an ordered vec-
toid, while {VnC, MnC, ≤} is a weakly ordered vectoid.
We have now established in this chapter that the spaces listed in the leftmost ele-
ment of every row in Figure 1 are ringoids and vectoids with certain order properties
such as being weakly ordered or ordered or inclusion-isotonally ordered.
We conclude this section by pointing out that ringoids and vectoids also occur in
sets of mappings. Let {R, +, ·} be a ringoid with the neutral elements o and e and let
M be a set of mappings of a given set S into R, i.e., M := {x | x : S →R}. In M
we deﬁne an equality, addition, and outer and inner multiplication respectively by
x = y
:⇔
x(t) = y(t),
for all t ∈S,

x,y∈M
(x + y)(t)
:=
x(t) + y(t),
for all t ∈S,

a∈R

x∈M
(a · x)(t)
:=
a · x(t),
for all t ∈S,

x,y∈M
(x · y)(t)
:=
x(t) · y(t),
for all t ∈S.
If {R, ≤} is an ordered set, we deﬁne an order relation ≤in M by
x ≤y
:⇔
x(t) ≤y(t),
for all t ∈S.
Then the following theorem, which characterizes sets of mappings in terms of the
structures in question, is easily veriﬁed.
Theorem 2.16. Let {R, +, ·} be a ringoid with the neutral elements o and e. Then
{M, R} is a multiplicative vectoid. The neutral elements are o(t) = o and e(t) = e
for all t ∈S. If {R, +, ·, ≤} is a weakly ordered (resp. an ordered) ringoid, then
{M, R, ≤} is a weakly ordered (resp. an ordered) vectoid. With respect to the inner
operations {M, +, ·} is a ringoid.
■
In conclusion, a clarifying remark on the results that have been obtained may be
useful. At ﬁrst sight it may seem strange that for the power set the empty set has to
be excluded in order to obtain the ringoid and vectoid structures. This, however, is
only a notational problem. The power set of a set R is one of the most frequently used
complete lattices. Its least element is the empty set and the greatest element is the set
R itself. Thus, the concept of the power set is well established. That the least element
does not satisfy certain algebraic properties should not be surprising. The ordering

2.2 Vectoids
59
of algebraic structures is very often only conditionally complete and it is well known
that completion by adding a least and a greatest element is often impossible without
violating their algebraic properties.
The real numbers R, for instance, can be deﬁned as a conditionally complete lin-
early ordered ﬁeld. Derived spaces such as those in the second column of Figure 1
are also conditionally complete with respect to the order relation ≤. If the real num-
bers are completed in the customary manner, the least element −∞and the greatest
element +∞, which fail to satisfy the algebraic properties, need not be notationally
excluded.
Because of the central role of the real numbers computing is usually performed in
a conditionally completely ordered algebraic structure. We shall see in the following
chapters that on a computer, because of rounding or for other reasons, the least or the
greatest element of the order structure can nevertheless occur as result of an algebraic
operation.
In order not to interrupt the computation in such cases, attempts have been under-
taken to deﬁne algebraic operations also for the least and the greatest element. This
may lead to reasonable results in some cases; in others it does not. We do not follow
these lines in this treatise. We think that exceptional cases in a computation should be
a challenge for the user to think further about his problem and to deal with such cases
individually.
Remark 2.17. The discrepancy between a conditionally complete and a completely
ordered structure has a notational consequence for the following chapters. In Def-
inition 1.22 monotone and directed roundings have been deﬁned as mappings of a
complete lattice into a lower and upper screen, respectively. In this chapter we have
deﬁned and studied the concepts of a ringoid and a vectoid. We have seen that in a
weakly ordered or ordered ringoid or vectoid the least and the greatest element might
not satisfy the algebraic properties of the ringoid or vectoid. Henceforth, we therefore
assume that the ordering of a weakly ordered or ordered ringoid or vectoid is condi-
tionally complete. Then we have to distinguish between the symbol for the ringoid or
vectoid and its lattice theoretic completion. If the ringoid or vectoid is denoted by R
we shall denote its lattice theoretic completion by R. With this notation a rounding
then becomes a mapping
: R →S, where S is a lower (resp. upper) screen of
the complete lattice R.
■

Chapter 3
Deﬁnition of Computer Arithmetic
In all branches of mathematics one is interested in gaining insight into the
structures that can occur. We have established in the preceding chapter that
the spaces listed in the leftmost element of every row in Figure 1 are ringoids
and vectoids with certain order properties such as being weakly ordered or
ordered or inclusion-isotonally ordered. We are now going to show that
these structures recur in the subsets on the right-hand side of Figure 1 pro-
vided that the arithmetic and mapping properties are properly deﬁned. In
Section 3.1 of this chapter we give a heuristic approach to the mapping
concept of a semimorphism. In Section 3.2 we derive some fundamental
properties of semimorphisms and certain other mappings. In Section 3.3
we give the conventional deﬁnition of computer arithmetic and show that it
produces ordered and weakly ordered ringoids and vectoids in the subsets of
Figure 1. In Section 3.4 we introduce the deﬁnition of computer arithmetic
by means of semimorphism. This leads to the best possible arithmetic in all
spaces and in the product spaces in particular. We show expressly for all
rows of Figure 1 that do not include interval sets that weakly ordered and
ordered ringoids and vectoids are invariant with respect to semimorphisms.
The deﬁnition of arithmetic by semimorphism will also be used in Chapter 4
to derive similar results for the rows of Figure 1 that correspond to sets of
intervals.
3.1
Introduction
Let R be a set in which certain operations and relations are deﬁned, and let R′ be a set
of rules or axioms given for these operations and relations. By way of example, the
commutative law of addition might be one such rule. Then we call the pair {R, R′}
a structure. We shall also refer to {R, R′} as the structure of the set R. The real or
complex numbers, vectors, or matrices are well-known structures. We now assume
that certain operations in {R, R′} cannot be carried out by a computer. As an exam-
ple take the representation and addition of irrational numbers. We are then obliged to
replace {R, R′} by a structure {S, S′}, the operations of which can be done by ma-
chine and which lead to good approximations to the operations in {R, R′}. We shall
always take S ⊆R.

3.1 Introduction
61
In all signiﬁcant applications there exist at least two operations in R: an addition
+ and a multiplication ·, which have neutral elements or an identity operator in the
case of an outer multiplication. Let us denote these elements by o and e. In addition
we shall always have a minus operator. We assume therefore that the subset S has the
property
(S3) o, e ∈S
∧

a∈S
−a ∈S.
We further assume that the elements of R are mapped into the subset S by a round-
ing with the property
(R1)

a∈S
a = a.
In attempting to approximate a given structure {R, R′} by a structure {S, S′} with
S ⊆R, we are motivated to employ well-established mappings having useful prop-
erties such as isomorphism or homomorphism. For the sake of clarity, we give the
deﬁnition of these concepts for ordered algebraic structures.
Deﬁnition 3.1. Let {R, R′} and {S, S′} be ordered algebraic structures, and let a
one-to-one correspondence exist between the corresponding operations and order re-
lation(s). A mapping
: R →S is called a homomorphism if it is an algebraic
homomorphism, i.e., if the image of the operation is equal to the operation of the
images:

a,b∈R
(
a) ◦(
b) =
(a ◦b)
(3.1.1)
for all corresponding operations ◦in R and
◦in S and if it is an order homomor-
phism, i.e., if

a,b∈R
(a ≤b ⇒
a ≤
b).
(3.1.2)
A homomorphism is called an isomorphism if
: R →S is a one-to-one mapping
of R onto S and

a,b∈S
(a ≤b ⇒
−1a ≤
−1b).
■
Thus an isomorphism requires a one-to-one mapping. However, in our applications
S is in general a subset of R of different cardinality. There does not exist a one-to-one
mapping and certainly no isomorphism between sets of different cardinality.
A homomorphism preserves the structure of a group, a ring, or a vector space.
However, we show by a simple example that the approximation of {R, R′} by {S, S′}
by means of homomorphism cannot be realized in a sensible way. A simple theorem
which proves this statement is given in Appendix B.

62
3 Deﬁnition of Computer Arithmetic
Example 3.2. Let S be a ﬂoating-point system1 that is deﬁned as a set of numbers of
the form X = m · be. m is called the mantissa, b the base of the system and e the
exponent. Let b = 10, let the range of the mantissa be m = −0.9(0.1)0.9, and let
the exponent be e ∈{−1, 0, 1}. If
: R →S denotes the rounding to the nearest
number of S and x = 0.34, y = 0.54 ∈R, we get
x = 0.3,
y = 0.5 and thus
(
x) + (
y) =
((
x) + (
y)) =
(0.8) = 0.8 but
(x + y) =
(0.88) =
0.9, i.e., (
x) + (
y) ̸=
(x + y).
It seems desirable, however, to stay as close to a homomorphism as possible. We
therefore derive some necessary conditions for a homomorphism. If we restrict (3.1.1)
to elements of S, then because of (R1) we obtain
(RG)

a,b∈S
a ◦b =
(a ◦b).
We shall use this formula to deﬁne the operation
◦in S by means of the corre-
sponding operation ◦in R and the rounding
: R →S (see Remark 2.17). From
(3.1.2) we see that the rounding has to be monotone
(R2)

a,b∈R
(a ≤b ⇒
a ≤
b).
Now in the case of multiplication in (3.1.1), replace a by the negative multiplicative
unit −e. Then

b∈R
(−b) =
(−e) ·
b
=
(S3),(R1) (−e) ·
b =
(RG)
(−
b)
=
(S3),(R1) −
b.
Thus the rounding has to be antisymmetric:
(R4)

a∈R
(−a) = −
a.
Speciﬁcally (see Deﬁnition 3.3), we shall call a mapping a semimorphism if it has
the properties (R1,2,4), and (RG).
The conditions (R1,2,4) do not deﬁne the rounding uniquely. We shall see, however,
in this chapter and in Chapter 4 that in all cases of Figure 1, ordered or weakly ordered
ringoids and vectoids as well as inclusion-isotonally ordered ringoids and vectoids
are invariant with respect to semimorphisms. We shall see further in Part 2 of this
book that semimorphisms can be implemented on computers by fast algorithms and
hardware circuits in all cases of Figure 1. These properties give to the concept of
semimorphism a signiﬁcance that is quite independent of the present derivation of it
via the necessary conditions for a homomorphism.
3.2
Preliminaries
In ringoids and vectoids there is a minus operator available. We use it to extend the
concept of a screen and of a rounding. We do this in the following deﬁnition, where
we also formalize the notion of a semimorphism.
1See Chapter 5 for a general deﬁnition of a ﬂoating-point system.

3.2 Preliminaries
63
Deﬁnition 3.3. Let R be a ringoid or a vectoid and o the neutral element of addition
and e the neutral element of the multiplication if it exists. Further let {R, ≤} be a
complete lattice.
(a) A lower (resp. upper) screen {S, ≤} is called symmetric if
(S3) o, e ∈S
∧

a∈S
−a ∈S.
(b) A rounding of R into a symmetric lower (resp. upper) screen S,
: R →S is
called antisymmetric if
(R4)

a∈R
(−a) = −
a
(antisymmetric).
(c) A mapping of the ringoid or vectoid R into a symmetric lower (resp. upper)
screen is called a semimorphism if it is a monotone and antisymmetric rounding,
i.e., has the properties (R1,2,4), and if all inner and outer operations in S are
deﬁned by
(RG)

a,b∈S
a ◦b :=
(a ◦b).
■
With the minus operator in ringoids and vectoids an interdependence of the special
roundings ▽and △can now be proved. It is the subject of the following theorem.
Theorem 3.4. If {R, ≤} is a weakly ordered ringoid or vectoid, {R, ≤} a complete
lattice and {S, ≤} a symmetric screen of R, and ▽: R →S (resp. △: R →S) the
monotone downwardly (resp. upwardly) directed roundings, then

a∈R
(▽a = −△(−a) ∧△a = −▽(−a)).
Proof. We give the proof only for a ringoid. For a vectoid, the proof can be given
analogously by using corresponding properties of Theorem 2.10.
If {R, +, ·} is a ringoid, then {PR \ {∅}, +, ·} is a ringoid. With
L(a) := {b ∈R | b ≤a}
and
U(a) := {b ∈R | a ≤b}
we obtain because of (OD2) that
−(L(a) ∩S) := {b ∈S | −b ≤a} = {b ∈S | −a ≤b} = U(−a) ∩S
and
−(U(a) ∩S) := {b ∈S | a ≤−b} = {b ∈S | b ≤−a} = L(−a) ∩S.
Using this and Theorem 2.3(y), we obtain
sup(L(a) ∩S) = −inf(−(L(a) ∩S)) = −inf(U(−a) ∩S)

64
3 Deﬁnition of Computer Arithmetic
and
inf(U(a) ∩S) = −sup(−(U(a) ∩S)) = −sup(L(−a) ∩S).
This and the general property ▽a = sup(L(a) ∩S) ∧△a = inf(U(a) ∩S) prove the
theorem.
■
This theorem can easily be illustrated when R is a linearly ordered ringoid and S
is a ﬁnite subset of R. See Figure 3.1. It has been proved, independently, for much
more general cases such as complexiﬁcations and vector or matrix sets.
0
−▽(−a)
▽(−a)
−a
△a
a
Figure 3.1. Dependence of directed roundings in a linearly ordered ringoid.
Theorem 3.4 asserts that under its hypothesis the monotone directed roundings are
interdependent. Thus only one of the monotone directed roundings is necessary when
both are to be implemented on computers. The other one could then be obtained by
sign changes.
If S ̸= R and we take an a ∈R with a /∈S, then ▽a < a < △a. A comparison
of Deﬁnition 3.3 and Theorem 3.4 shows that the monotone directed roundings are
not antisymmetric under the hypothesis of the theorem. In contrast to this result,
we shall consider in Chapter 4 several monotone upwardly directed roundings that
are antisymmetric. In this case the screen and the monotone directed roundings are
deﬁned with respect to an order relation that is different from the one that orders the
ringoid or vectoid.
Crucial for the whole development and to applications in the next sections and in
Chapter 4 is the following theorem.
Theorem 3.5 (Rounding invariance of ringoid properties.). Let R be a ringoid or di-
vision ringoid, or a weakly ordered or ordered ringoid or division ringoid with the
special elements o, e, and −e. Further let {R, ≤} be a complete lattice and {S, ≤}
a symmetric (lower resp. upper) screen of R and
: R →S a semimorphism.
Then besides (D6), all properties of R also hold for the resulting structure in S. In
particular
(a) the property (Di) in S is a consequence of (Di) in R, for i = 1, 2, 3, 4, 5, 7, 8, 9
with the same special elements o, e, and −e in S,
(b) (ODi) in S is a consequence of (ODi) in R, for i = 1, 2, 3, 4, 5,

3.2 Preliminaries
65
(c) from (RG) and (Ri) follows (RGi) in S, for i = 1, 2, 3, 4 and for all ◦∈
{+, −, ·, /} with
(RG4)

a∈S
(−e) · a = (−e) · a,
(d) If {S,
+ , · } is a ringoid then (RG) and (RGi), i = 1, 2, 3 also hold for subtrac-
tion.
Proof. The proof of the theorem is obtained in a straightforward manner. In order to
gain familiarity with the theorem we recommend it be worked through in complete
detail. As sample here we just prove property (D5d). As a ﬁrst step we show the
validity of
(RG4):

a∈S
(−e) · a =
(RG)
(−a) =
(R4) −
a =
(R1) −a ∈
(S3) S.
(D5d): (−e) · (a · b) =
(RG)
((−e) ·
(ab)) =
(R4)
(
(−(ab)))
=
(R1)
(−(ab)) = =
(D5d)R
((−a)b)
=
(D5d)R
(a(−b))
=
(RG) (−a) · b =
(RG) a · (−b)
=
(RG4) ((−e) · a) · b
=
(RG4) a · ((−e) · b).
(d): If {S,
+ , · } is a ringoid, we have for all a, b ∈S that
a −b := a + ( −b)
=
(RG4) a + (−b)
=
(RG)+
(a −b),
which is (RG) and (RG1) for subtraction. (RG2) (resp. (RG3)) follow from (RG)
and (R2) (resp. (R3)).
■
All properties of a semimorphism (S3), (R1,2,4), and (RG) are repeatedly used
within the proof of Theorem 3.5. If we change these properties or if they are not
strictly implemented on a computer (even with respect to quantiﬁers), we get a differ-
ent structure or no describable structure at all in the subset S.
Theorem 3.5 asserts that if we generate S as hypothesized, we almost obtain for
S itself the structure of a ringoid (resp. a division ringoid). What is missing is the
establishment of the property (D6). A universal proof of this latter property is a more
difﬁcult task, and we are obliged to establish it individually for all cases in Figure 1.
For ease of reading we formally repeat the compatibility properties (RGi), i =
1(1)3, between the structure in R and in S that are mentioned in Theorem 3.5(c).
Under the hypothesis of Theorem 3.5 we have
(RG1)

a,b∈S
(a ◦b ∈S ⇒a ◦b = a ◦b),
(RG2)

a,b,c,d∈S
(a ◦b ≤c ◦d ⇒a ◦b ≤c ◦d),

66
3 Deﬁnition of Computer Arithmetic
(RG3)

a,b∈S
a ◦b ≤a ◦b
resp.

a,b∈S
a ◦b ≤a ◦b.
Here ◦denotes any operation of the set {+, −, ·, /}.
Since neither an isomorphism nor a homomorphism between the basic set R and
the screen S is available, we deﬁne arithmetic on the screen through semimorphisms.
As was done in the case of a groupoid, for brevity we characterize the compatibility
properties, which we then obtain between the operations in the basic set R and in the
screen S, by the following deﬁnition.
Deﬁnition 3.6. Let R be a ringoid (resp. vectoid) and S a symmetric screen of R. A
ringoid (resp. vectoid) in S is called
• a screen ringoid (resp. vectoid) if (RG1) holds,
• monotone if (RG2) holds,
• a lower or upper screen ringoid (resp. vectoid) if (RG3) holds,
always for all inner (resp. inner and outer) operations.
■
A screen ringoid always has the same special elements as the basic ringoid itself
since 
a∈S
(a + o ∈S ∧a · e ∈S ⇒
(RG1) a + o = a + o = a ∧a · e = a · e = a) and
−e := (−e) · e ∈S ⇒
(RG1) (−e) · e =
−e.
Similarly a screen vectoid always has the same neutral elements as the basic vectoid
itself, and, since in Deﬁnition 3.6 S is a symmetric screen, a screen vectoid always
has the same minus operator as the basic vectoid itself. For all a ∈S we have −a :=
(−e)a ∈
(S3) S ⇒
(RG1) (−e) · a =
−a.
In the case of a linearly ordered ringoid we can use Theorem 3.5 and Theorem 2.4
to obtain the following theorem.
Theorem 3.7. Let {R, +, ·, ≤} be a linearly ordered ringoid, {R, ≤} a complete lat-
tice, {S, ≤} a symmetric screen, and
: R →S a semimorphism. Then {S,
+ , · ,
≤} is a linearly ordered monotone screen ringoid.
■
Since the real numbers are a linearly ordered ringoid, Theorem 3.7 asserts, for ex-
ample, that the ﬂoating-point numbers are a linearly ordered monotone screen ringoid
if their arithmetic is deﬁned by a semimorphism. This process can be repeated each
time passing into a coarser subset and each time obtaining a linearly ordered mono-
tone screen ringoid. Theorem 3.7 enables us to deﬁne the arithmetic in the sets D and
S in Figure 1. These in turn can be used to deﬁne the arithmetic in the correspond-
ing columns of Figure 1. This latter process is the traditional deﬁnition of computer
arithmetic in the product sets in Figure 1. It will be discussed in the next section.

3.3 The Traditional Deﬁnition of Computer Arithmetic
67
3.3
The Traditional Deﬁnition of Computer Arithmetic
Let us now assume that {R, N, +, ·, /, ≤} is a linearly ordered division ringoid. We
already know by Theorem 2.7 that the process of complexiﬁcation leads to a weakly
ordered division ringoid {CR, N′, +, ·, /, ≤}. As for the matrices over R and CR,
we know by Theorem 2.6 that {MnR, +, ·, ≤} is an ordered ringoid and that
{MnCR, +, ·, ≤} is a weakly ordered ringoid. The deﬁnition of the operations in
CR, MnR, and MnCR by those of R as given preceding Theorems 2.6 and 2.7, we
call this the traditional or conventional deﬁnition of computer arithmetic.
If now D is a symmetric screen of R and S a symmetric screen of D and if we
deﬁne the arithmetic in D and S by semimorphism (stepwise), then by Theorem 3.7
we once more obtain linearly ordered ringoids. Therefore, we can again deﬁne an
arithmetic and structure in the derived sets MnD, CD, MnCD and MnS, CS, MnCS
by the traditional deﬁnition of the arithmetic and get the same structures as in the
corresponding sets over R. See Figure 3.2.
D
Theorem 3.7
MnD
Theorem 2.7
Theorem 2.6
CD
Theorem 2.7
Theorem 2.6
R
MnR
Theorem 2.7
Theorem 2.6
CR
S
MnS
CS
Theorem 2.6
Theorem 2.6
Theorem 2.6
MnCD
MnCR
MnCS
Theorem 3.7
Figure 3.2. The traditional deﬁnition of computer arithmetic leading to
ringoids.
As an example, R may be the linearly ordered ringoid of real numbers, D the
subset of double precision ﬂoating-point numbers, and S the subset of single precision
ﬂoating-point numbers of a given computer.
A similar process can be used to deﬁne vectoids over R, D, and S. Figure 3.3 lists
the resulting spaces and the theorems with which we proved the vectoid properties.
The spaces listed immediately under R, D, and S are ordered vectoids, and those
listed under CR, CD, and CS are weakly ordered vectoids.

68
3 Deﬁnition of Computer Arithmetic
{MmnCR, CR, ≤}
{VnCR, CR, ≤}
{MnCR, CR, ≤}
{VnCR, MnCR, ≤}
{MmnCD, CD, ≤}
{VnCD, CD, ≤}
{MnCD, CD, ≤}
{VnCD, MnCD, ≤}
Theorem 2.7
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
Theorem 2.7
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
Theorem 2.12
Theorem 2.13
Theorem 2.14
Theorem 2.15
{MmnCS, CS, ≤}
{VnCS, CS, ≤}
{MnCS, CS, ≤}
{VnCS, MnCS, ≤}
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
Theorem 2.7
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
R
D
Theorem 3.7
Theorem 3.7
S
CR
{MmnR, R, ≤}
{VnR, R, ≤}
{MnR, R, ≤}
{VnR, MnR, ≤}
CD
{MmnD, D, ≤}
{VnD, D, ≤}
{MnD, D, ≤}
{VnD, MnD, ≤}
CS
{MmnS, S, ≤}
{VnS, S, ≤}
{MnS, S, ≤}
{VnS, MnS, ≤}
Figure 3.3. The traditional deﬁnition of computer arithmetic leading to
vectoids.

3.4 Deﬁnition of Computer Arithmetic by Semimorphisms
69
The traditional method of deﬁning arithmetic is used on most existing computers
to specify the arithmetic of ﬂoating-point vectors, matrices, complexiﬁcations, and
so on. The operations in MnS, for instance, are deﬁned by the operations in S and
the usual formulas for addition and multiplication of real matrices. We saw that the
resulting structures can be described in terms of ordered or weakly ordered ringoids
and vectoids.
This will also be one of the main results of the deﬁnition of the arithmetic in the
subsets by means of semimorphisms, which we discuss in Section 3.4. The deﬁni-
tion of the arithmetic by semimorphisms goes further, leading to simple compatibility
properties such as (RG1,2,3,4) between the operations in the basic set and the screen.
It is the connection between these two classes of operations that is of central interest.
In Chapter 5 we derive error estimates for the operations deﬁned by both methods.
It will turn out that the error formulas for the deﬁnition of the operations by semi-
morphisms are much simpler and more accurate than those of the traditional method
and consequently allow a simpler and more accurate error analysis of numerical algo-
rithms. The resulting structures of both methods are the same: ringoids and vectoids.
But the arithmetic operations deﬁned by the two methods are very different.
3.4
Deﬁnition of Computer Arithmetic by Semimorphisms
The ﬁrst result of the deﬁnition of computer arithmetic by means of semimorphisms,
of course, is contained in Theorem 3.7.
In order to deﬁne the arithmetic in all the other spaces listed in Figures 3.4 and 3.5
by means of semimorphisms, we ﬁrst prove the following theorem.
Theorem 3.8. Let {R, +, ·} be a ringoid, {R, ≤} a complete lattice, {S, ≤} a sym-
metric lower (resp. upper) screen, and
: R →S a monotone and antisymmet-
ric rounding. Consider the product sets VnS := S × S × · · · × S ⊆VnR. Then
{VnR, ≤} is a complete lattice if the order relation is deﬁned componentwise and
{VnS, ≤} is a symmetric lower (resp. upper) screen of {VnR, ≤}. Further the map-
ping
: VnR →VnS deﬁned by

a=(ai)∈VnR
a :=
⎛
⎜
⎜
⎜
⎜
⎝
a1
a2
...
an
⎞
⎟
⎟
⎟
⎟
⎠
is a monotone and antisymmetric rounding.
Proof. Although the proof is straightforward, we give it in complete detail in order
to review the concepts dealt with. As a product set of complete lattices, {VnR, ≤}
is itself a complete lattice. The subset {VnS, ≤} is an ordered set and, as a product

70
3 Deﬁnition of Computer Arithmetic
set of complete lattices, also a complete lattice and therefore a complete subnet of
{VnR, ≤}. The supremum (resp. inﬁmum) in VnS equals the vector of suprema (resp.
inﬁma) taken in S. Since {S, ≤} is a lower (resp. upper) screen of R, the latter
equals the suprema (resp. inﬁma) taken in R. Consequently, the supremum (resp.
inﬁmum) taken in VnS equals the supremum (resp. inﬁmum) taken in VnR. Then
by Theorem 1.19, VnS is a lower (resp. upper) screen of {VnR, ≤}. Since S is a
symmetric screen of R, then the zero vector is an element of VnS, and for all a =
(ai) ∈VnS, −a = (−ai) ∈VnS, i.e., VnS is a symmetric lower (resp. upper) screen
of {VnR, ≤}. The mapping
: VnR →VnS is deﬁned by rounding componentwise.
This directly implies that the properties (R1), (R2), and (R4) of the rounding
: R →
S also hold for the rounding
: VnR →VnS.
■
A matrix of MmnR may be viewed as an element of the m×n-dimensional product
set. Therefore, Theorem 3.8 remains valid if VnR is replaced by MmnR or MnR and
VnS by MmnS or MnS. The rounding for matrices has to be deﬁned by rounding the
components. The following two theorems deﬁne the arithmetic in the sets listed in
Figure 3.2 by means of semimorphisms.
Theorem 3.9. Let {R, +, ·} be a ringoid, {R, ≤} a complete lattice, {S, ≤} a sym-
metric screen of {R, ≤}, and {S,
+ , · } a ringoid deﬁned by a semimorphism
:
R →S. Further, let {MnR, +, ·} be the ringoid of n×n-matrices over R. Operations
in MnS are speciﬁed by the semimorphism
: MnR →MnS deﬁned by

A=(aij)∈MnR
A := (
aij) and
(RG)

A,B∈MnS
A ◦B :=
(A ◦B), for ◦∈{+, ·}.
(a) Then {MnS,
+ , · } is a monotone screen ringoid of MnR.
(b) If the rounding
: R →S is upwardly (resp. downwardly) directed, then the
ringoid {MnS,
+ , · } is an upper (resp. lower) screen ringoid.
(c) If {MnR, +, ·, ≤} is a weakly ordered (resp. an ordered) ringoid, then {MnS,
+ , · , ≤} is a weakly ordered (resp. an ordered) screen ringoid of MnR.
Proof. (a) By Theorem 3.8 {MnS, ≤} is a symmetric screen of {MnR, ≤}, and the
rounding
: MnR →MnS is monotone and antisymmetric. Now Theorem 3.5
implies the validity of the properties (D1,2,3,4,5), (RG1,2), and (RG4). It remains
only to prove (D6).
(D6): Let O and E denote the neutral elements of {MnR, +, ·}. By Theorem 3.5 we
know that the element −E satisﬁes (D5) in MnS. We show that it is unique with
respect to this property. Let X = (xij) be any element of MnS that satisﬁes (D5).
Then by (D5a),
X + E = O,
(3.4.1)

3.4 Deﬁnition of Computer Arithmetic by Semimorphisms
71
and by the deﬁnition of the addition and the rounding, we get for i ̸= j:
xij + o = o ⇒
(D2)S xij = o,
i.e., X is a diagonal matrix. Specializing to the diagonal in (3.4.1), we get
xii + e = o
for all i = 1(1)n,
(3.4.2)
and by (D5b) for X:
xii · xii = e
for all i = 1(1)n.
(3.4.3)
From (D5c) and (D5d) in MnS we get for two diagonal matrices with the constant
diagonal entries a and b

a,b∈S
xii · (a + b) = xii · a + xii · b,
(3.4.4)
and

a,b∈S
xii · (a · b) = (xii · a) · b = a · (xii · b).
(3.4.5)
Relations (3.4.2)–(3.4.5) imply that xii has to satisfy the axiom (D5) in S. Since
{S,
+ , · } is a screen ringoid of {R, +, ·}, xii = −e for all i = 1(1)n, and therefore
X = −E ∈MnS.
(b) and (c) are simple consequences of Theorem 3.5.
■
Theorem 3.10. Let {R, +, ·} be a ringoid, {R, ≤} a complete lattice, {S, ≤} a sym-
metric screen of {R, ≤}, and {S,
+ , · } a ringoid deﬁned by a semimorphism
:
R →S. Further, let {CR, +, ·} be the ringoid of pairs (a, b) over R, and let opera-
tions in CS be speciﬁed by the semimorphism
: CR →CS deﬁned by

α=(a1,a2)∈CR
α = (
a1,
a2), and
(RG)

α,β∈CS
α ◦β :=
(α ◦β), ◦∈{+, ·}.
(a) Then {CS,
+ , · } is a monotone screen ringoid of CR.
(b) If
: R →S is upwardly (resp. downwardly) directed, then {CS,
+ , · } is an
upper (resp. lower) screen ringoid of CR.
(c) If {R, +, ·, ≤} is weakly ordered, then {CS,
+ , · , ≤} is a weakly ordered
screen ringoid of CR.
(d) Now let {R, N, +, ·, /} be a division ringoid, {CR, N ′, +, ·, /} with N′ =
{(c1, c2) ∈CR | c1c1 +c2c2 ∈N}, its complexiﬁcation, and {S, N ∩S,
+ , · ,
/ } a division ringoid deﬁned by the semimorphism
: R →S. If division by
an element β /∈N′ ∩CS is deﬁned by the semimorphism (RG), then {CS, N′ ∩
CS,
+ , · ,
/ } is a monotone screen division ringoid of CR.

72
3 Deﬁnition of Computer Arithmetic
Proof. (a) By Theorem 3.8 {CS, ≤} is a symmetric screen of {CR, ≤}, and
:
CR →CS is a monotone and antisymmetric rounding. Theorem 3.5 provides the
properties (D1,2,3,4,5), (RG1,2), and (RG4). It remains to prove (D6).
(D6): Let o and e denote the neutral elements of {R, +, ·}. Then ω = (o, o) and
ϵ = (e, o) are the neutral elements of {CR, +, ·}. By Theorem 3.5 we know that the
element (−e, o) satisﬁes (D5) in CS.
We show that it is unique with respect to this property. Let η = (x, y) be any element
of CS that satisﬁes (D5). Then by (D5a),
η + (e, o) = (o, o)
and by the deﬁnition of addition and the rounding, we get
y + o = o ⇒
(D2)S y = o,
i.e., η is of the form η = (x, o).
Let us now denote by T the set of all elements of CS with an imaginary part of
zero (second component). Then the mapping ϕ : S →T deﬁned by

a∈S
ϕ(a) = (a, o)
is a ringoid isomorphism.
The element η = (x, o) has to satisfy (D5) for all α, β ∈CS, and in particular
for α = (a, o) and β = (b, o) with a, b ∈S. Because of the isomorphism T ↔S,
the properties (D5) in CS for the elements (a, o) and (b, o) are equivalent to (D5) in
S. Since {S,
+ , · } is a screen ringoid, there exists only one such element x = −e.
Therefore, η = (−e, o) is the only element in CS that satisﬁes (D5).
(b), (c) and (d) are simple consequences of Theorem 3.5.
■
The operations deﬁned in MnS and CS by Theorems 3.9 and 3.10 are quite differ-
ent from those deﬁned in the same sets by the traditional deﬁnition of arithmetic in
these subsets in Theorems 2.6 and 2.7.
Theorems 3.9 and 3.10 already specify the deﬁnition of the arithmetic by semi-
morphisms for several of the sets displayed under D in Figure 3.2. As an example,
let R denote the linearly ordered set of real numbers, S and D the subsets of single
and double precision ﬂoating-point numbers of a given computer. Then the arithmetic
and structure are deﬁned in MnD by Theorem 3.9, in CD by Theorem 3.10, and in
MnCD once again by Theorem 3.9. See Figure 3.4.
These theorems, however, cannot be used directly to deﬁne the arithmetic and struc-
ture in MnS, CS, and MnCS. The reason for this is that in Theorems 3.9 and 3.10
the arithmetic in the spaces MnR, CR, and MnCR is deﬁned vertically in the tradi-
tional way. The arithmetics and structures of the spaces D, MnD, CD, and MnCD,
however, are no longer connected vertically in the traditional way.

3.4 Deﬁnition of Computer Arithmetic by Semimorphisms
73
Theorem 2.6
Theorem 3.9
Theorem 3.10
CR
MnCR
Theorem 3.5
MnCD
Theorem 3.5
Theorem 3.9
Theorem 3.10
CD
MnCS
MnCS
CS
CS
R
Theorem 3.7
D
Theorem 3.7
Theorem 2.7
Theorem 2.6
MnR
Theorem 3.9
Theorem 3.5
MnD
Theorem 3.9
MnS
MnS
S
Figure 3.4. The deﬁnition of computer arithmetic by semimorphism lead-
ing to ringoids.
Nevertheless, the following consideration leads to the desired results. If S is a
symmetric screen of D, then so is MnS of MnD. See Figure 3.4. For a monotone and
antisymmetric rounding ♦: D →S, we consider the semimorphism ♦: MnD →
MnS deﬁned by

A=(aij)∈MnD
♦A := (♦aij)
and
(RG)

A,B∈MnS
A♦
◦B := ♦(A ◦B), for ◦∈{+, ·}.
Then, apart from (D6), {MnS, ♦
+ , ♦· } has all desired properties as we know from
Theorem 3.5.
To see that {MnS, ♦
+ , ♦· , ≤} is a ringoid, we consider the composition of the map-
pings ♦
: R →S. This composition is also a monotone and antisymmetric round-
ing. By Theorem 3.9, therefore, the semimorphism ♦
: MnR →MnS with the

74
3 Deﬁnition of Computer Arithmetic
property
(RG)

A,B∈MnS
A♦
◦B := ♦(
(A ◦B)), for ◦∈{+, ·},
leads to a ringoid in MnS. This ringoid is identical to the one deﬁned by the semi-
morphism ♦: MnD →MnS.
{MnS, ♦
+ , ♦· , ≤}, therefore, is a weakly ordered (resp. an ordered) monotone
screen ringoid of MnD.
Using the composition of the mappings
: CR →CD and ♦: CD →CS,
it can be proved analogously by Theorem 3.5 and Theorem 3.10 that {CS, N′ ∩
CS, ♦
+ , ♦· , ♦
/ , ≤} is a weakly ordered monotone screen division ringoid of CD.
Similarly, Theorem 3.5 and Theorem 3.9 lead to the result that {MnCS, ♦
+ , ♦· ,
≤} is a weakly ordered monotone screen ringoid of {MnCD,
+ , · , ≤}. See Fig-
ure 3.4.
In Figure 3.4 the chain R ⊇D ⊇S can also be extended to the right by means
of coarser symmetric screens. Again it is clear by Theorems 3.5, 3.9, and 3.10 that
we obtain ringoids in all lines of Figure 3.4. In this sense the ringoid structures in all
lines of Figure 3.4 are invariant with respect to semimorphisms.
The following theorem enables us to deﬁne the arithmetic in the sets listed in Fig-
ure 3.3 by means of semimorphisms.
Theorem 3.11 (Rounding invariance of vectoid properties). Let {V, R} be a vectoid,
which may be multiplicative, {V , ≤} a complete lattice, {S, ≤} a symmetric lower
(resp. upper) screen of {V , ≤}, and {T,
+ , · } a screen ringoid of {R, +, ·}. Further,
let
: V →S be a semimorphism, i.e., a monotone and antisymmetric rounding with
which operations are deﬁned by
(RG)

a,b∈S
a ◦b :=
(a ◦b), ◦∈{+, ·},

a∈T

a∈S
a · a :=
(a · a).
Then all vectoid properties of {V, R} also hold for the resulting structure in {S, T}.
In particular
(a) the property (Vi) in S is a consequence of (Vi) in V , for i = 1, 2, 3, 4, 5,
(b) (VDi) in S is a consequence of (VDi) in V , for i = 1, 2, 3, 4, 5,
(c) (OVi) in S is a consequence of (OVi) in V , for i = 1, 2, 3, 4, 5,
(d) from (RG) and (Ri) follows (RGi) in S, for i = 1, 2, 3, 4 and for all inner and
outer operations.
Proof. The proof of this theorem is obtained in a straightforward manner. We omit
it here. In order to gain familiarity with the many properties we recommend it be
worked through in complete detail.
■
This theorem permits the deﬁnition of arithmetic by means of semimorphisms in all
rows of Figure 3.5. As an example, one may again interpret R as the linearly ordered
real numbers, S and D as the subsets of single and double precision ﬂoating-point

3.5 A Remark About Roundings
75
numbers of a given computer. The arithmetic and structure in the product sets listed
in Figure 3.5 are deﬁned by the theorems displayed on the connecting lines.
One may also extend the chain R ⊇D ⊇S by means of progressively coarser
symmetric screens. Theorem 3.11 then shows that one again obtains vectoids in all
rows of Figure 3.5. In this sense the vectoid structures in all rows of Figure 3.5 are
invariant with respect to semimorphisms.
We mention ﬁnally that the arithmetic deﬁned by semimorphisms in the sets listed
in Figure 3.5 is in principle different from that deﬁned in the same sets by the tradi-
tional method of deﬁning arithmetic in the product sets. In both cases the resulting
structures are vectoids although the arithmetic operations differ. Within the structures
in the rows beginning with {MmnR, R, ≤} and {VnR, R, ≤}, however, both methods
lead to the same operations.
3.5
A Remark About Roundings
The results of this chapter show the importance of monotone and antisymmetric
roundings. When composed into a semimorphism, they leave invariant the ringoid
and vectoid structure in all numerically interesting cases.
Besides the monotone and antisymmetric roundings the monotone directed round-
ings ▽and △are of particular importance. We recall Theorem 1.30, which asserts
that in a linearly ordered set every monotone rounding can be expressed by means of
the monotone downwardly and upwardly directed roundings ▽and △. Employing
Theorem 3.4 in the case of a linearly ordered ringoid, we see that even the rounding
△can be expressed by ▽and conversely. This leads to the result that in a linearly
ordered ringoid (e.g., the real numbers) every monotone rounding can be expressed
by the monotone downwardly directed rounding ▽alone. Apart from △, no other
monotone rounding has this property.
This implies that on a computer, in principle, every monotone rounding can be per-
formed if either the monotone downwardly or upwardly directed rounding ▽or △
is available. Since non-monotone roundings are of little interest, this points out the
polytone role of the monotone directed roundings ▽and △for all rounded computa-
tions.
Beyond this result, the monotone directed roundings ▽and △are needed for all
interval computations as we shall see in Chapter 4. Interval arithmetic is a funda-
mental extension to ﬂoating-point arithmetic. It brings mathematics and guarantees
into computing. Despite these basic facts, the monotone downwardly and upwardly
directed roundings ▽and △and the corresponding arithmetic operations deﬁned by
(RG) are not yet made conveniently available by hardware facilities and by program-
ming languages on computers.
We may interpolate the observation that the monotone downwardly directed round-
ing ▽, for instance, is not at all difﬁcult to implement. If negative numbers are rep-

76
3 Deﬁnition of Computer Arithmetic
{MmnCR, CR, ≤}
{VnCR, CR, ≤}
{MnCR, CR, ≤}
{VnCR, MnCR, ≤}
Theorem 2.7
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
Theorem 2.15
Theorem 2.14
Theorem 2.13
Theorem 2.12
{VnD, MnD, ≤}
{MmnD, D, ≤}
{VnD, D, ≤}
{MnD, D, ≤}
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
CR
{VnR, R, ≤}
{MnR, R, ≤}
{VnR, MnR, ≤}
R
{MmnR, R, ≤}
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
{MmnCD, CD, ≤}
{VnCD, CD, ≤}
{MnCD, CD, ≤}
{VnCD, MnCD, ≤}
D
Theorem 3.7
S
Theorem 3.7
{MmnS, S, ≤}
{VnS, S, ≤}
{MnS, S, ≤}
{VnS, MnS, ≤}
{MmnCS, CS, ≤}
{VnCS, CS, ≤}
{MnCS, CS, ≤}
{VnCS, MnCS, ≤}
Theorem 3.11
Theorem 3.11
Theorem 3.11
Theorem 3.11
Figure 3.5. The deﬁnition of computer arithmetic by semimorphisms lead-
ing to vectoids.

3.6 Uniqueness of the Minus Operator
77
resented by the b-complement within the rounding procedure, then for all x ∈R
the rounding ▽is identical with the truncation of the representation to ﬁnite length.
Truncation is the simplest and fastest rounding procedure.
3.6
Uniqueness of the Minus Operator
In Chapter 2 it was shown that passage to the power set, complexiﬁcation, and to the
matrices over a ringoid, leads straight back to ringoids. In Chapter 3 it was shown that
all these ringoids are invariant with respect to semimorphism. In Chapter 4 we shall
extend this statement to several interval structures.
The uniqueness of the minus operator is essential for the structure and properties
of ringoids and vectoids. Proof that the minus operator is unique was an essential part
in the establishment of all the derived ringoids mentioned in the last paragraph.
In a ringoid the minus operator is deﬁned by the four properties (D5a,b,c,d). It is
certainly an interesting question, whether the uniqueness of the minus operator can be
established by the property (D5a) (e + x = o) alone. We show by a simple example
that in general this is not the case. For that purpose we consider the mapping of the
real numbers R into a simple ﬂoating-point system.
A normalized ﬂoating-point number is a real number of the form
x = m · be.
Here m is the mantissa, b is the base of the number system in use and e is the exponent.
b is an integer greater than unity. The exponent is an integer between two ﬁxed integer
bounds e1, e2, and in general e1 < 0 < e2. The mantissa is of the form
m = ◦
r

i=1
di · b−i,
where ◦∈{+, −} is the sign of the number.
The di are the digits of the mantissa. They have the property di ∈{0, 1, . . . , b −1}
for all i = 1(1)r and d1 ̸= 0. Without the condition d1 ̸= 0, ﬂoating-point numbers
are said to be unnormalized. The set of normalized ﬂoating-point numbers does not
contain zero. So zero is adjoined to S. For a unique representation of zero it is often
assumed that m = 0.00 . . . 0 and e = 0. A ﬂoating-point system depends on the
constants b, r, e1, and e2. We denote it by S = S(b, r, e1, e2).
The ﬂoating-point numbers are not equally spaced between successive powers of
b and their negatives. This spacing changes at every power of b. In particular, there
are relatively large gaps around zero which contain no further ﬂoating-point number.
Figure 3.6 shows a simple ﬂoating-point system S = S(2, 3, −1, 2) consisting of 33
elements.

78
3 Deﬁnition of Computer Arithmetic
e
2e
m1
m2
m3
m4
−1
1
2
0.100
0.101
0.110
0.111
0
1
0.100
0.101
0.110
0.111
1
2
0.100
0.101
0.110
0.111
2
4
0.100
0.101
0.110
0.111
R
3
2
1
0
-1
-3
-2
of least absolute value
non zero elements
greatest element
least element
Figure 3.6. The characteristic spacing of a ﬂoating-point system.
In Figure 3.6 the mantissas are shown in binary representation.
If, for instance, the rounding towards zero is chosen, the entire interval (−1
4, 1
4) is
mapped onto zero. By Theorem 3.7 the semimorphism with this rounding generates a
ringoid in S. In this ringoid we have, of course,
e + x = 1 + (−1) =
(RG)
(1 + (−1)) =
o =
(R1) o,
i.e., (D5a) holds. But we also obtain
1 +

−7
8

:=
1
8

= 0,
i.e., (D5a) also holds for −7
8. In this case, however, (D5b) is not valid:

−7
8

·

−7
8

=
49
64

=
(0.765625) = 3
4 ̸= 1.
Since o ∈S, (R1) and (RG) yield immediately that for every a ∈S the element
−a is an additive inverse of a in S:
a + (−a) :=
(RG)
(a + (−a)) =
o =
(R1) o, for all a ∈S.
However, in a normalized ﬂoating-point system −a is not in general the only addi-
tive inverse of a in S. This was already shown by the example above. An afﬁrmative
second example is: 1
4
+ (−3
8) =
(−1
8) = 0.
So 1
4 and −3
8 form a pair of additive inverses. More generally, whenever the real
sum of two numbers in S falls into the interval (−1
4, 1
4) their sum a + b in S is zero.
We shall derive necessary and sufﬁcient conditions for the existence of unique ad-
ditive inverses in S under semimorphism in the next section. Under these conditions
the uniqueness of the minus operator in S is, of course, already a consequence of
(D5a).

3.7 Rounding Near Zero
79
3.7
Rounding Near Zero
Theorem 3.12. If S is a symmetric, discrete screen of R,
: R →S a semimorphism,
and ϵ > o the least distance between distinct elements of S, then for all a ∈S the
element b = −a is the unique additive inverse of a if and only if
−1(o) ⊆(−ϵ, ϵ).
(3.7.1)
Here
−1(o) denotes the inverse image of o and (−ϵ, ϵ) is the open interval be-
tween −ϵ and ϵ.
Proof. At ﬁrst we show that (3.7.1) is sufﬁcient: We assume that b ̸= −a is an additive
inverse of a in S. Then a + b = o and by (RG) a+b ∈
−1(o). This means by (3.7.1)
−ϵ < a + b < ϵ.
(3.7.2)
Since −b ̸= a we have by deﬁnition of ϵ : |a −(−b)| ≥ϵ. This contradicts (3.7.2), so
the assumption is false. Under the condition (3.7.1) there is no additive inverse of a in
S other than −a. In other words (3.7.1) is sufﬁcient for the uniqueness of the additive
inverse −a in S.
Now we show that (3.7.1) is necessary also: Since o ∈S by (R1)
(o) = o and
o ∈
−1(o). Since
is monotone,
−1(o) is convex. Since
is antisymmetric,
−1(o) is symmetric with respect to zero. Assuming that (3.7.1) is not true, then
−1(o) ⊃(−ϵ, ϵ). We take two elements a, b ∈S with distance ϵ. Then a ̸= b and
|a −b| = ϵ, i.e., a + (−b) ∈
−1(o) or a + (−b) = o. This means that −b ̸= −a
is inverse to a. Thus (3.7.1) is necessary because otherwise there would be more than
one additive inverse to a in S.
■
Equation (3.7.1) holds automatically if the number ϵ itself is an element of S. Then
because of (R1),
(ϵ) = ϵ and, because of the monotonicity of the rounding (R2),
−1(o) ⊆(−ϵ, ϵ). In other words: If the least distance ϵ of two elements of a discrete
screen S of R is an element of S, then for all a ∈S the element −a ∈S is the
unique additive inverse of a in S. This holds under the assumption that the mapping
: R →S is a semimorphism.
Such subsets of the real numbers do indeed occur. For instance, the integers or
the so-called ﬁxed-point numbers are subsets of R with this property. Sometimes the
normalized ﬂoating-point numbers are extended into a set with this property. Such a
set is obtained if in the case e = e1 unnormalized mantissas are permitted. Then ϵ
itself becomes an element of S and for all a ∈S the element −a is the unique additive
inverse of a. This is the case, for instance, if IEEE arithmetic with denormalized
numbers is implemented.
Figure 3.7 illustrates the behavior of typical roundings in the neighborhood of zero.
(R1) means that for ﬂoating-point numbers the rounding function coincides with the
identity mapping.

80
3 Deﬁnition of Computer Arithmetic
ε
ε
ε
ε
(a)
(a)
(a)
(a)
(a)
(a)
a
a
a
a
a
a
(b)
(c)
(f)
(d)
(e)
(a)
Figure 3.7. The behavior of frequently used roundings near zero.
(a) shows the conventional behavior of the rounding of normalized ﬂoating-point
numbers. In this case (3.7.1) does not hold and the additive inverse is not unique.
(b) shows the rounding away from zero of normalized ﬂoating-point numbers. In
this case (3.7.1) holds and we have unique additive inverses.
(c) here and in the following cases ϵ is an element of S and we have unique additive
inverses.
(d) shows the rounding towards zero near zero when denormalized numbers are per-
mitted for e = e1. In the IEEE arithmetic standard this is called gradual under-
ﬂow.
(e) shows the rounding to the nearest number in S in the neighborhood of zero. The
roundings (d) and (e) are provided by the IEEE arithmetic standard.
(f) shows the rounding away from zero in the neighborhood of zero with denormal-
ized numbers permitted for e = e1. This rounding has all the required properties.
It is
(a) = o if and only if a = o, a property which can be very important for
making a clear distinction between a number that is zero and a number that is
not, however small it might actually be. This rounding is not provided by the
IEEE arithmetic standard.
For the developments in the next chapter it is of interest whether Theorem 3.12
can be generalized in such a way that it becomes applicable to the product spaces
that occur in Figure 1 and in particular to those rows which contain intervals. In
all interval spaces of Figure 1 arithmetic will be deﬁned by semimorphism and the
resulting structures will be ringoids and vectoids again. The proof that the minus

3.7 Rounding Near Zero
81
operator is unique has been intricate in all cases of ringoids in Chapters 2 and 3. The
following theorem gives a criterion for the uniqueness of additive inverses. It contains
Theorem 3.12 as a special case.
Theorem 3.13. Let {R, +, ·} be a ringoid and {R, d} a metric space with a distance
function d : R × R →R which is translation invariant, i.e., which has the property

a,b,c∈R
d(a + c, b + c) = d(a, b)
(translation invariance).
(3.7.3)
Let {R, ≤} be a complete lattice, {S, ≤} a symmetric lower (resp. upper) screen
of R which is discrete, i.e., which has the property

a,b∈S
(a ̸= b ⇒d(a, b) ≥ϵ > o),
where ϵ > o is the least distance of distinct elements of S. Further, let
: R →S be
a semimorphism.
(a) If a is an element of S which has a unique additive inverse −a in R, then −a is
also the unique additive inverse of a in S if

x∈R
(
x = o ⇒d(x, o) < ϵ).
(3.7.4)
(b) If in addition {R, +} is a group, then (3.7.4) is necessary also.
Proof. From (RG) and (R1) follows that an element a ∈S which has an additive
inverse −a in R has the same additive inverse in S:
a + (−a) :=
(a + (−a)) =
o = o.
(3.7.5)
(a) We show that (3.7.4) is sufﬁcient for uniqueness of −a: We assume that b ̸= −a
is an additive inverse of a in S. Then we obtain by (RG) and (3.7.4):
a + b = o ⇒
(a + b) = o ⇒d(a + b, o) < ϵ.
(3.7.6)
On the other hand we get by the deﬁnition of ϵ and by the translation invariance (3.7.3):
d(b, −a) ≥ϵ ⇒d(a + b, a + (−a)) = d(a + b, o) ≥ϵ.
This contradicts (3.7.6). So the assumption that there is an additive inverse b of a in
S other than −a is false. In other words (3.7.4) is sufﬁcient for the uniqueness of the
additive inverse −a in S.
(b) We show that under the additional assumption that {R, +} is a group, (3.7.4) is
necessary also: By (R1) we obtain o ∈
−1(o). As a consequence of (R2)
−1(o)
is convex and by (R4)
−1(o) is symmetric, since for all a ∈R with
a ∈
−1(o) ⇒
a = o ⇒
(−a) =
(R4) −
(a) = o ⇒−a ∈
−1(o).

82
3 Deﬁnition of Computer Arithmetic
Thus
−1(o) is a symmetric and convex set. By (3.7.4) all elements of this set have
a distance to zero which is less than ϵ: d(x, o) < ϵ. Should (3.7.4) not hold, there
would be elements x ∈
−1(o) with d(x, o) ≥ϵ > o. ϵ is the least distance between
distinct elements of S. Now we chose two different elements a, b ∈S with distance
ϵ. Then
a ̸= b ∧d(a, b) = ϵ ⇒
(3.7.3) d(a −b, o) = ϵ ⇒a −b ∈
−1(o) ⇒a + (−b) = o.
This means −b is inverse to a and −b ̸= −a. In other words: If (3.7.4) does not hold
there are elements in S which have more than one additive inverse. This shows that
(3.7.4) is necessary.
■
By Theorem 3.13(a), (3.7.4) is a sufﬁcient criterion for the uniqueness of additive
inverses in columns 3 and 4 of Figure 1 for rows 1, 3, 4, 6, 7, 9, 10, and 12. We
mention explicitly that in these cases under condition (3.7.4) the resulting structure is
that of a ringoid and the uniqueness of the minus operator is a consequence of (D5a)
alone.
Theorem 3.13(b) shows that (3.7.4) is also necessary for the uniqueness of additive
inverses in columns 3 and 4 of Figure 1 for rows 1, 3, 7, and 9. In these cases {R, +}
is a group.
To fully establish the applicability of Theorem 3.13 we still have to demonstrate
that in all the basic spaces under consideration a metric does indeed exist which is
translation invariant (3.7.3). We just mention the appropriate metric and leave the
demonstration of (3.7.3) to the reader:
• If R is the set of real numbers R, then d(a, b) = |a −b|.
• If R is the set of complex numbers C, the distance of two complex numbers
a = a1 + ia2 and b = b1 + ib2 is deﬁned by d(a, b) := |a1 −b1| + |a2 −b2|.
• If R is the set of real intervals IR, the distance of two intervals a = [a1, a2] and
b = [b1, b2] is deﬁned by d(a, b) := max(|a1 −b1|, |a2 −b2|).
• If R is the set of complex intervals IC, the distance of two complex intervals
a = a1 + ia2 and b = b1 + ib2 with real intervals a1, a2, b1, and b2 is deﬁned by
d(a, b) := d(a1, b1) + d(a2, b2).
• For two matrices a = (aij) and b = (bij) with components aij, bij of R, C, IR,
or IC, the distance is deﬁned in each case as the maximum of the distances of
corresponding matrix components: d(a, b) := max(d(aij, bij)).
In all basic structures under consideration, which are the sets R, C, IR, IC, and the
matrices with components of these sets, (see Figure 1), the multiplicative unit e has a
unique additive inverse −e. Under the hypotheses of Theorem 3.12 and Theorem 3.13
(conditions (3.7.1) and (3.7.4)) respectively, therefore, −e is also the unique additive
inverse in any computer representable subset. This allows the deﬁnition of the minus

3.7 Rounding Near Zero
83
operator and of subtraction in the computer representable subsets in the same manner
as it was done in ringoids with all its consequences by (D5a) alone.
We did not follow this line in Chapter 3. The reason for this is the fact that there
are reasonable semimorphisms which are frequently used in practice and for which
the conditions (3.7.1) and/or (3.7.4) do not hold. An example of this is the rounding
towards zero from the real numbers into a ﬂoating-point screen S and we have seen in
Section 3.6 that elements of S may have more than one additive inverse. In particular
there may be more than one additive inverse for the multiplicative unit e in S.
At this stage of the development a look ahead at interval spaces is particularly
interesting. We shall deal with these spaces in the next chapter and we shall meet a
different situation there. Again we consider the sets R and C as well as the matrices
MR and MC with components of R and C, respectively. All these sets are ordered
with respect to the order relation ≤. If R denotes any one of these sets, an interval is
deﬁned by
A = [a1, a2] := {a ∈R | a1, a2 ∈R, a1 ≤a ≤a2}.
Arithmetic operations on the set IR of all such intervals will be deﬁned in the
next chapter. Every interval [a, a] ∈IR with a ∈R has a unique additive inverse
[−a, −a] in IR for all R ∈{R, C, MR, MC}. However, the elements of IR in general
are not computer representable and the arithmetic operations in IR are not computer
executable. Therefore, subsets S ⊆R of computer representable elements have to be
chosen. An interval in IS is deﬁned by2
A = [a1, a2] := {a ∈R | a1, a2 ∈S, a1 ≤a ≤a2}.
Arithmetic operations in IS are deﬁned by semimorphism, i.e., by (RG) with the
monotone and antisymmetric rounding ♦: IR →IS which is upwardly directed with
respect to set inclusion as an order relation. ♦is uniquely deﬁned by these properties
(R1,2,3,4). See Chapter 1. This process leads to computer executable operations in
the interval spaces IS where S is a computer representable subset of anyone of the
sets R, C, MR, and MC.
In both theorems of this section the inverse image of zero with respect to a rounding
plays a key role. Since the rounding ♦: IR →IS is upwardly directed with respect
to set inclusion as an order relation the inverse image of zero ♦−1(o) can only be zero
itself. Thus the criteria (3.7.4) for the existence of unique additive inverses evidently
holds in IS. This establishes the fact, with all its consequences, that the unit interval
[e, e] has a unique additive inverse [−e, −e] in IS. This holds for all interval sets IS
where S is a discrete subset of R ∈{R, C, MR, MC}. This will be the way we shall
prove the uniqueness of the minus operator in the next chapter.
2Note, that although the interval limits a1 and a2 are elements of S, the interior points are elements
of R.

Chapter 4
Interval Arithmetic
We start with the observation made in Chapter 2, that all the spaces listed
in the leftmost element in every row in Figure 1 are ringoids or vectoids.
In particular, the spaces that are power sets are inclusion-isotonally ordered
with respect to inclusion as an order relation. We shall deﬁne the opera-
tions in all interval spaces in Figure 1 by means of semimorphisms. With
these operations, each interval space becomes an inclusion-isotonally or-
dered monotone upper screen ringoid (resp. vectoid) of its left neighbor in
Figure 1. Furthermore, these ringoids and vectoids are ordered or weakly
ordered with respect to the order relation ≤, which we shall deﬁne below.
For all operations to be deﬁned in the interval sets listed in Figure 1,
we derive explicit formulas for the computation of the resulting interval in
terms of the bounds of the interval operands.
To do this, we show that the structures MnIR and IMnR as well as
{VnIR, IR, ≤} and {IVnR, IR, ≤} – if the operations in MnIR and VnIR
are deﬁned by the traditional method – are isomorphic with respect to all
algebraic operations and the order relation ≤. The same isomorphisms are
shown to exist between the corresponding spaces, wherein R is replaced by
its complexiﬁcation CR.
The operations in all interval spaces in the columns listed under D and S
in Figure 1 are also deﬁned via semimorphisms. The isomorphisms referred
to here are the main tool for obtaining optimal (i.e., the smallest appropri-
ate interval) and computer executable formulas for the operations in these
spaces.
4.1
Interval Sets and Arithmetic
Let {R, ≤} be an ordered set and PR the power set of R. The intervals
[a1, a2] := {x ∈R | a1, a2 ∈R, a1 ≤x ≤a2} with a1 ≤a2
are special elements contained in the power set PR. Thus, denoting the set of all
intervals over R by IR, we have IR ⊂PR. In IR we deﬁne equality and the order
relation ≤by
[a1, a2] = [b1, b2]
:⇔a1 = b1 ∧a2 = b2,
[a1, a2] ≤[b1, b2]
:⇔a1 ≤b1 ∧a2 ≤b2.

4.1 Interval Sets and Arithmetic
85
With these deﬁnitions, we formulate the following theorem.
Theorem 4.1. If {R, ≤} is a complete lattice, then {IR, ≤} is also a complete lattice.
Moreover, with A = [a1, a2]

S⊆{IR,≤}

inf S = [ inf
A∈S a1, inf
A∈S a2] ∧sup S = [sup
A∈S
a1, sup
A∈S
a2]

.
(4.1.1)
Proof. It is clear that {IR, ≤} is an ordered set. Since IR is a subset of the product
set R×R and the order relation is deﬁned componentwise, Theorem 1.10 implies that
{IR, ≤} is a complete lattice and that (4.1.1) holds.
■
We remark that within PR the corresponding deﬁnition
A ≤B :⇔inf
a∈A a ≤inf
b∈B b ∧sup
a∈A
a ≤sup
b∈B
b,
does not lead to an order relation because (O3) is obviously not valid.
If R := R ∪{−∞} ∪{+∞}, then the interval [−∞, −∞] is the least element and
the interval [+∞, +∞] is the greatest element of {IR, ≤}.
If {R, ≤} is a complete lattice, then R is the greatest element of the power set
{PR, ⊆}. Writing R in the form [o(R), i(R)], we see that it is also the greatest el-
ement of {IR, ⊆}. The empty set ∅is the least element of {PR, ⊆}. According to
the above deﬁnition, however, ∅is not an element of IR. To have a least element
available whenever necessary, we adjoin the empty set to IR and deﬁne
IR := IR ∪{∅}.
In the following theorem we show that IR serves as an upper screen of PR with
respect to inclusion as an order relation.
Theorem 4.2. If {R, ≤} is a complete lattice, then
(a) {IR, ⊆} is a conditionally completely ordered set. For all A ⊆IR, which are
bounded below, we have with B = [b1, b2] ∈IR
inf
{IR,⊆} A = [ sup
B∈A
b1, inf
B∈A b2] ∧
sup
{IR,⊆}
A = [ inf
B∈A b1, sup
B∈A
b2],
i.e., the inﬁmum is the intersection, and the supremum is the interval hull of the
elements of A.
(b) {IR, ⊆} is an upper screen of {PR, ⊆}.
Proof. (a) Since {R, ≤} is a complete lattice and the set A is bounded below (i.e., A
contains an interval), the intervals
I := [ sup
B∈A
b1, inf
B∈A b2], S := [ inf
B∈A b1, sup
B∈A
b2]

86
4 Interval Arithmetic
exist, and for B ∈A we have I ⊆B ∧B ⊆S, i.e., I is a lower and S an upper bound
of A. If I1 is any lower and S1 any upper bound of A, then I1 ⊆I and S ⊆S1, i.e., I
is the greatest lower and S the least upper bound of A.
(b) Using (a), we see that {IR, ⊆} is a complete lattice and therefore a complete
subnet of {PR, ⊆}. We still have to show that for every subset A ⊆IR, infIR A =
infPR A.
If A is bounded below in IR, then by (a) the inﬁmum in IR is the intersection as it
is in PR, and therefore infIR A = infIR A = infPR A.
If A is not bounded below in IR, we consider three cases:
(1) A = ∅. Then infIR A = infPR A = i(PR) = R.
(2) A ̸= ∅∧∅∈A. Then infIR A = ∅= infPR A.
(3) ∅/∈A. Since A is not bounded below in IR, we obtain once again infIR A =
∅= infPR A.
■
Now we consider the monotone upwardly directed rounding
: PR →IR, which
is deﬁned by the following properties:
(R1)

A∈IR
A = A,
(R2)

A,B∈PR
(A ⊆B ⇒
A ⊆
B) ,
(R3)

A∈PR
A ⊆
A.
Remark 4.3. At this stage we are facing a notational problem. The theory that is
being developed should be applicable to realistic models. These models can usually
be characterized as conditionally completely ordered algebraic structures. We have
seen in Section 1.1 of Chapter 1 that every conditionally completely ordered set R
can be made into a complete lattice by adjoining a least element o(R) and a greatest
element i(R). Then R := R∪{o(R)}∪{i(R)} is a complete lattice. This completion
will be necessary for the process of rounding. A rounding was deﬁned as a mapping of
a complete lattice into a lower (resp. upper) screen. However, the adjoined elements
o(R) and i(R) often do not satisfy all algebraic properties of the model.
■
As an example the real numbers R are deﬁned as a conditionally complete, linearly
ordered ﬁeld. Derived spaces such as those in the second column of Figure 1 are
also only conditionally complete with respect to the order relation ≤. If the real
numbers are completed in the customary manner by adjoining a least element −∞
and a greatest element +∞the resulting set R := R ∪{−∞} ∪{+∞} is a complete
lattice. Arithmetic operations are frequently deﬁned for the new elements, as for

4.1 Interval Sets and Arithmetic
87
instance:
∞+ x = ∞,
−∞+ x = −∞,
−∞+ (−∞) = (−∞) · ∞= −∞,
∞+ ∞= ∞· ∞= ∞,
∞· x = ∞for x > 0,
∞· x = −∞for x < 0,
x
∞=
x
−∞= 0,
together with variants obtained by applying the sign rules and the law of commutativ-
ity. However, the new elements −∞and +∞fail to satisfy several of the algebraic
properties of a ﬁeld. For example a + ∞= b + ∞even if a < b, so that the cancel-
lation law is not valid. Also, values like ∞−∞or 0 · ∞are not deﬁned for the new
elements.
A similar situation occurs in other models. We have seen that the power set PR
of a set R is a complete lattice with respect to set inclusion as an order relation. In
Theorem 2.5 it was shown that ringoid properties are preserved upon passage to the
power set. The empty set is the least element in PR. If operations for the empty set
are deﬁned the result can only be the empty set. Thus we have {0} · ∅= ∅· {0} =
∅̸= {0}. This is not in accordance with (D4), i.e., the empty set does not fulﬁl the
ringoid properties in PR.
In Theorem 4.2 it was shown that the completed set IR := IR ∪{∅} of intervals
serves as an upper screen of PR. However, to obtain a ringoid in Theorem 4.5, the
empty set has to be excluded again because of the property [0, 0] · ∅= ∅· [0, 0] =
∅̸= [0, 0].
A mathematically strict notation, therefore, would have to distinguish between a set
R and its lattice theoretic completion R. In derived sets this may lead to very com-
plicated notations. If, for instance, R is a conditionally completely ordered ringoid
and R its lattice theoretic completion, the monotone upwardly directed rounding from
the power set into the intervals would be a mapping
: PR →IR. In order to
avoid such unreadable notations we shall assume henceforth that the basic set is al-
ready completely ordered. If this set is a ringoid or a vectoid, the reader should be
aware, however, that the least and/or the greatest elements possibly do not satisfy all
algebraic properties of a ringoid or a vectoid. In particular for power set or interval
structures we shall in general not explicitly note that the empty set (the least element)
does not satisfy the algebraic properties of a ringoid or vectoid.
Following this line we now develop properties of the monotone upwardly directed
rounding
: PR →IR in the following theorem.
Theorem 4.4. Let {R, +, ·, ≤} be a completely and weakly ordered ringoid with the
neutral elements 0 and e. Then

88
4 Interval Arithmetic
(a) {IR, ⊆} is a symmetric upper screen of {PR, ⊆}, i.e., we have
(S3)
[0, 0], [e, e] ∈IR
∧

A=[a1,a2]∈IR
−A = [−a2, −a1] ∈IR.
(4.1.2)
(b) The monotone upwardly directed rounding
: PR →IR is antisymmetric, i.e.,
(R4)

∅̸=A∈PR
(−A) = −
A.
(c) We have
(R)

∅̸=A∈PR
A = inf(U(A) ∩IR) = o(U(A) ∩IR) =

inf
a∈A a, sup
a∈A
a
 
.
Proof. The assertions are shown in the order (a), (c), (b).
(a) With A := [a1, a2] := {x ∈R | a1 ≤x ≤a2}, we get −A := {−e} · A :=
{(−e) · x | x ∈A} = {−x ∈R | a1 ≤x ≤a2} = {x ∈R | a1 ≤−x ≤
a2}
=
(OD2)R {x ∈R | −a2 ≤x ≤−a1} = [−a2, −a1] ∈IR.
(c) Theorem 1.24 implies
A = inf(U(A) ∩IR). We show that
A = [inf A,
sup A]. For all A ∈PR such that A ̸= ∅we have A ⊆[inf A, sup A]. For
all B = [b1, b2] ∈IR with the property A ⊆B we obtain for all a ∈A:
b1 ≤a ≤b2 ⇒b1 ≤inf A ∧sup A ≤b2 ⇒[inf A, sup A] ⊆[b1, b2], i.e.,
[inf A, sup A] is the least upper bound of A in {IR, ⊆}, which proves (c).
(b) With
A = [inf A, sup A], we get
(−A) = [inf(−A), sup(−A)], and with
Theorem 2.3(y),
A = [−sup A, −inf A]. Now with (b) and (c) we obtain
(−A) = −
(A).
■
Now we make use of the monotone upwardly directed rounding
: PR →IR to
deﬁne operations
◦, ◦∈{+, −, ·, /}, in IR by the corresponding semimorphism that
has the following property:
(RG)

A,B∈IR
A ◦B :=
(A ◦B) =

inf
a∈A,b∈B(a ◦b),
sup
a∈A,b∈B
(a ◦b)
 
. (4.1.3)
Here the inﬁmum and supremum are taken over a ∈A, b ∈B, while the equality
on the right-hand side is a simple consequence of (R). These operations have the
following properties to which by Theorem 1.32 (RG) is equivalent:
(RG1)

A,B∈IR
(A ◦B ∈IR ⇒A ◦B = A ◦B),
(RG2)

A,B,C,D∈IR
(A ◦B ⊆C ◦D ⇒A ◦B ⊆C ◦D),
(RG3)

A,B∈IR
(A ◦B ⊆A ◦B).
When in these formulas the operator ◦represents division, we assume additionally
that B, D /∈!
N. For the deﬁnition of !
N, see Theorem 4.5, which we now state.

4.1 Interval Sets and Arithmetic
89
Theorem 4.5. (a) Let {R, +, ·, ≤} be a completely and weakly ordered ringoid with
neutral elements 0 and e. We assume additionally that x = −e is already unique in R
by (D5a) alone1. If operations are deﬁned in IR by the semimorphism
: PR →IR,
then {IR, +, · , ≤, ⊆} becomes a completely and weakly ordered ringoid with respect
to the order relation ≤. The special elements are [0, 0], [e, e], and [−e, −e]. With
respect to ⊆, IR is an inclusion-isotonally ordered monotone upper screen ringoid of
PR. Moreover, for all A = [a1, a2], B = [b1, b2] ∈IR, we have
(A) A + B = [a1 + b1, a2 + b2], A −B = A + (−B) = [a1 −b2, a2 −b1].
(b) If additionally {R, +, ·, ≤} is an ordered ringoid, then {IR,
+ , · , ≤, ⊆} is also
an ordered ringoid with respect to ≤and
(B) A ≥0 ∧B ≥0 ⇒A · B = [a1b1, a2b2],
(C) A ≤0 ∧B ≤0 ⇒A · B = [a2b2, a1b1],
(D) A ≤0 ∧B ≥0 ⇒A · B = [a1b2, a2b1],
A ≥0 ∧B ≤0 ⇒A · B = [a2b1, a1b2].
(c) If {R, N, +, ·, /, ≤} is an ordered division ringoid, then {IR, !
N,
+ , · ,
/ ,
≤} with !
N := {A ∈IR
|
A ∩N ̸= ∅} is also an ordered division ringoid.
Moreover, for all A = [a1, a2] ∈IR and B = [b1, b2] ∈IR \ !
N we have
(E) A ≥0 ∧0 < b1 ≤b2 ⇒A / B = [a1/b2, a2/b1],
(F) A ≥0 ∧b1 ≤b2 < 0 ⇒A / B = [a2/b2, a1/b1],
(G) A ≤0 ∧0 < b1 ≤b2 ⇒A / B = [a1/b1, a2/b2],
(H) A ≤0 ∧b1 ≤b2 < 0 ⇒A / B = [a2/b1, a1/b2].
Proof. (a) Theorem 3.5 directly implies the properties (D1,2,3,4,5,7,8,9) and (OD5).
It remains to show (A), (D6), (OD1), and (OD2). Let be A = [a1, a2], B = [b1, b2],
C = [c1, c2].
(A): We demonstrate the relation

A,B∈IR
(inf A + inf B = inf(A + B)
∧
sup A + sup B = sup(A + B)) (4.1.4)
1For instance, this is the case if {R, +} is a group.

90
4 Interval Arithmetic
which by (4.1.3) proves (A). We have

a∈A

b∈B
(inf A ≤a ∧inf B ≤b
⇒
(OD1)R inf A + inf B ≤a + b)
⇒inf A + inf B ≤inf(A + B).
On the other hand, inf(A + B) ≤a1 + b1 = inf A + inf B and therefore by (O3)
inf A + inf B = inf(A + B). The proof of the second property in (4.1.4) is dual.
(D6): [e, e] + [x, y] =
(A) [e + x, e + y] = [0, 0], i.e., e + x = 0 and e + y = 0. By
assumption there is only one element in R that solves (D5a). Thus x = y = −e.
(OD1):
A ≤B :⇔a1 ≤b1 ∧a2 ≤b2
⇒
(OD1)R a1 + c1 ≤b1 + c1 ∧a2 + c2 ≤b2 + c2
⇒inf A + inf C ≤inf B + inf C ∧sup A + sup C ≤sup B + sup C
⇒
(4.1.4) inf(A + C) ≤inf(B + C) ∧sup(A + C) ≤sup(B + C)
⇒
(4.1.3) (A + C) ≤(B + C).
(OD2):
A ≤B :⇔a1 ≤b1 ∧a2 ≤b2
⇒
(OD2)R −b1 ≤−a1 ∧−b2 ≤−a2
⇒[−b2, −b1] ≤[−a2, −a1] ⇒
(4.1.2) −B ≤−A ⇒
−B ≤
−A.
(b) The proof of (OD3) in IR is a direct consequence of (OD3) in R.
(B): A ≥0 ∧B ≥0 ⇒

a∈A

b∈B
0 ≤inf A ≤a ∧0 ≤inf B ≤b
⇒
(OD3)R
inf A · inf B ≤a · b ⇒inf A · inf B ≤inf(A · B).
On the other hand, inf(A·B) ≤a1·b1 = inf A·inf B ⇒
(O3) inf A·inf B = inf(A·B).
(C): A ≤[0, 0] ∧B ≤[0, 0] ⇒−A = [−a2, −a1] ≥[0, 0] ∧−B = [−b2, −b1] ≥
[0, 0] ⇒
(B) A · B = [(−a2)(−b2), (−a1)(−b1)] = [a2b2, a1b1].
(D): These cases can again be reduced to (B) by changing the sign of those intervals
whose bounds are less or equal to 0.
(c) (E): A ≥[0, 0]∧0 < b1 ≤b2 ⇒
a∈A

b∈B
0 ≤inf A ≤a ∧0 < b ≤sup B
⇒
(OD4)R
inf A/ sup B ≤a/ sup B ≤a/b ⇒inf A/ sup B ≤inf(A/B).
On the other hand, inf(A/B) ≤a1/b2 = inf A/ sup B ⇒inf A/ sup B =
inf(A/B).
The proof of the second property is dual.

4.1 Interval Sets and Arithmetic
91
Properties (F), (G) and (H) can be reduced to (E) by changing the sign of those
intervals whose bounds are less or equal to 0.
It remains to prove (OD4).
(OD4):
C ∈IR \ !
N ∧C > [0, 0] ⇒0 /∈C
⇒0 ≤a1 ≤b1 ∧0 ≤a2 ≤b2 ∧0 < c1 ≤c2
⇒
(OD4a)R 0 ≤a1/c2 ≤b1/c2 ∧0 ≤a2/c1 ≤b2/c1
⇒
(E) [0, 0] ≤A / C ≤B / C.
A, B ∈IR \ !
N ∧[0, 0] < A ≤B
⇒0 < a1 ≤b1 ∧0 < a2 ≤b2 ∧0 ≤c1 ≤c2
⇒
(OD4b)R c1/a2 ≥c1/b2 ≥0 ∧c2/a1 ≥c2/b1 ≥0
⇒
(E) C
/ A ≥C
/ B ≥[0, 0].
■
The rules (B)–(H) in Theorem 4.5 cover all cases where both of the operand inter-
vals are comparable with zero with respect to the order relation ≤. In all these cases
the result of an operation A ◦B, ◦∈{·, /}, can be expressed in terms of the bounds
of the operand intervals. The remaining cases are those in which one or both operands
are not comparable with [0, 0] with respect to the order relation ≤. We shall deal with
these cases in the next section using the additional hypothesis that the set {R, ≤} is
linearly ordered.
We already know that the real numbers {R, {0}, +, ·, /, ≤} are a completely or-
dered division ringoid if they are completed by the least and greatest elements −∞
and +∞. (These two elements, however, do not obey all the algebraic rules.) By
Theorem 4.5, therefore, {IR, N,
+ , · ,
/ , ≤, ⊆}, N := {A ∈IR | 0 ∈A} is a
completely ordered division ringoid with respect to ≤, while with respect to inclusion
⊆it is, moreover, an inclusion-isotonally ordered monotone upper screen ringoid of
the power set PR.
Since {C, {0}, +, ·, /, ≤} is a completely and weakly ordered division ringoid, we
obtain the same properties for the interval set {IC, N,
+ , · ,
/ , ≤, ⊆}, N := {A ∈
IC | 0 ∈A} except that the latter is weakly ordered with respect to ≤.
We also know that {MnR, +, ·, ≤} is a completely ordered ringoid and that
{MnC, +, ·, ≤} is a completely and weakly ordered ringoid. By Theorem 4.5 there-
fore {IMnR,
+ , · , ≤, ⊆} is a completely ordered ringoid and {IMnC,
+ , · , ≤, ⊆}
is a completely and weakly ordered ringoid with respect to ≤. With respect to ⊆,
both ringoids are inclusion-isotonally ordered and monotone upper screen ringoids of
PM nR (resp. PMnC).

92
4 Interval Arithmetic
We now consider vectoids. The corresponding concepts can be developed quite
similarly. Let {V, ≤} be a complete lattice, IV the set of intervals over V , and IV :=
IV ∪{∅}. Then {IV , ⊆} is an upper screen of {PV, ⊆}. The monotone upwardly
directed rounding is deﬁned by the following properties:
(R1)

A∈IV
A = A,
(R2)

A,B∈PV
(A ⊆B ⇒
A ⊆
B),
(R3)

A∈PV
A ⊆
A.
The following theorem characterizes the set IV and the rounding
: PV →IV
just introduced.
Theorem 4.6. Let {V, R, ≤} be a completely and weakly ordered vectoid with the
neutral element o. Then
(a) {IV , ⊆} is a symmetric upper screen of {PV, PR, ⊆}, i.e., we have
(S3) [o, o] (and [e, e])2 ∈IV ∧

A=[a1,a2]∈IV
−A = [−a2, −a1] ∈IV .
(b) The monotone upwardly directed rounding
: PV →IV is antisymmetric, i.e.,
(R4)

∅̸=A∈PV
(−A) = −
A.
(c) We have
(R)

∅̸=A∈PV
A = inf(U(A) ∩IV ) = o(U(A) ∩IV ) = [infa∈A a, supa∈A a].
Proof. By employing corresponding properties of the vectoid, the proof can be given
in a manner similar to the proof of Theorem 4.4.
■
Now we employ the monotone upwardly directed rounding
: PV →IV to
deﬁne inner and outer operations in IV by the corresponding semimorphism. This
semimorphism has the following properties:
(RG)

A,B∈IV
A ◦B :=
(A ◦B) = [infa∈A,b∈B(a ◦b), supa∈A,b∈B(a ◦b)].
The operations deﬁned by (RG) have the following properties (to which, by Theo-
rems 1.32 and 1.38, (RG) is equivalent):
(RG1)

A,B∈IV
(A ◦B ∈IV ⇒A ◦B = A ◦B),
(RG2)

A,B,C,D∈IV
(A ◦B ⊆C ◦D ⇒A ◦B ⊆C ◦D),
(RG3)

A,B∈IV
(A ◦B ⊆A ◦B).
2If V is multiplicative.

4.1 Interval Sets and Arithmetic
93
In the following theorem we characterize interval vectoids and derive explicit rep-
resentations for certain interval operations.
Theorem 4.7. (a) Let {V, R, ≤} be a completely and weakly ordered vectoid with the
neutral elements o and e (the latter if a multiplication exists), and let {IR,
+ , · ,
≤, ⊆} be the monotone upper screen ringoid of PR. If operations in IV are deﬁned
by the semimorphism
: PV →IV , then {IV, IR, ≤, ⊆} is a completely and weakly
ordered vectoid with respect to the order relation ≤. The neutral elements are [o, o]
and [e, e]. With respect to inclusion ⊆, IV is an inclusion-isotonally ordered mono-
tone upper screen vectoid of {PV, PR}. It is multiplicative if {V, R, ≤} is. Moreover,
for all A = [a1, a2], B = [b1, b2] ∈IV , we have
(A) A + B = [a1 + b1, a2 + b2], A −B = A + (−B) = [a1 −b2, a2 −b1].
(b) If in addition {V, R, ≤} is ordered, then {IV, IR, ≤, ⊆} is also ordered with re-
spect of ≤, and for all A = [a1, a2] ∈IR we have
(B) A ≥[0, 0] ∧B ≥[o, o] ⇒A · B = [a1b1, a2b2],
(C) A ≤[0, 0] ∧B ≤[o, o] ⇒A · B = [a2b2, a1b1],
(D) A ≤[0, 0] ∧B ≥[o, o] ⇒A · B = [a1b2, a2b1],
(E) A ≥[0, 0] ∧B ≤[o, o] ⇒A · B = [a2b1, a1b2].
If a multiplication exists in V , we also obtain for all A = [a1, a2]:
(F) A ≥[o, o] ∧B ≥[o, o] ⇒A · B = [a1b1, a2b2],
(G) A ≤[o, o] ∧B ≤[o, o] ⇒A · B = [a2b2, a1b1],
(H) A ≥[o, o] ∧B ≤[o, o] ⇒A · B = [a2b1, a1b2],
(I) A ≤[o, o] ∧B ≥[o, o] ⇒A · B = [a1b2, a2b1],
Proof. (a) Theorem 3.11 implies that {IV, IR, ≤, ⊆} is a monotone upper screen vec-
toid of {PV, PR}, which is multiplicative if {V, R, ≤} is. The inclusion-isotony is a
simple consequence of (OV5) in PV , of the monotonicity of the rounding
: PV →
IV , and of (RG). The proofs of the properties (OV1,2,3,4) and (A)–(I), which we
omit, are analogous to the proofs of the corresponding properties of Theorem 4.5.
■
As before, the rules (A)–(I) in Theorem 4.7 cover all cases where both operand
intervals are comparable with zero with respect to ≤. In all these cases the results of
the operation A · B or A · B can be expressed in terms of the bounds of the operand
intervals. The remaining cases are those in which one or both operands are not compa-
rable with the zero interval. In Section 4.3 of this chapter we shall describe a method
that furnishes explicit formulas for the resulting interval in these cases.

94
4 Interval Arithmetic
We know that {R, +, ·, ≤} is a completely ordered ringoid and that {C, +, ·, ≤}
is a completely and weakly ordered ringoid. By Theorem 4.7, therefore, {IVnR,
IR, ≤, ⊆} and {IVnR, IMnR, ≤, ⊆} are completely ordered vectoids with respect
to ≤. {IMnR, IR, ≤, ⊆} is a completely ordered multiplicative vectoid. Further,
{IVnC, IC, ≤, ⊆} and {IVnC, IMnC, ≤, ⊆} are completely and weakly ordered
vectoids with respect to ≤, while {IMnC, IC, ≤, ⊆} is a completely and weakly or-
dered multiplicative vectoid. With respect to the inclusion ⊆, all these vectoids are
inclusion-isotonally ordered monotone upper screen vectoids of the vectoids in the
corresponding power sets.
4.2
Interval Arithmetic Over a Linearly Ordered Set
We begin with a characterization of incomparable intervals.
Lemma 4.8. In a linearly ordered set {R, ≤}, two intervals A = [a1, a2], B =
[b1, b2] ∈IR are incomparable, A ∥B, with respect to ≤if and only if
a1 < b1 ≤b2 < a2
∨
b1 < a1 ≤a2 < b2.
Proof. It is clear that intervals with this property are incomparable. If a1 = b1, then
A and B are comparable. Therefore, A ∥B ⇒a1 < b1 ∨b1 < a1. If in the ﬁrst
case a2 ≤b2, then A and B are comparable. Therefore b2 < a2. The second case is
dual.
■
Thus far we have deﬁned all interval operations by means of the formula (RG).
This deﬁnition, however, is not directly usable on computers. In certain cases in
Theorem 4.5, we have expressed the result of an operation A ◦B in terms of the
bounds of the interval operands. For multiplication and division this was only possible
whenever both of the operands were comparable with [o, o] with respect to ≤. The
remaining cases are those in which one or both operands are incomparable with [o, o].
By Lemma 4.8 we see that in a linearly ordered set an interval A is incomparable
with [o, o] with respect to ≤if and only if o is an interior point of A, i.e., o ∈
◦A :=
{x ∈R
|
a1 < x < a2}. Therefore, the remaining cases are those in which
one or both interval operands have the element o ∈R as an interior point. Thus for
multiplication A · B, we still have to consider the cases
(a) A ≥[o, o] ∧o ∈
◦B,
(b) A ≤[o, o] ∧o ∈
◦B,
(c) o ∈
◦A ∧B ≥[o, o],
(d) o ∈
◦A ∧B ≤[o, o],
(e) o ∈
◦A ∧o ∈
◦B.

4.2 Interval Arithmetic Over a Linearly Ordered Set
95
Further, for division A / B, with A ∈IR and B ∈IR \ !
N, the cases
(a) o ∈
◦A ∧o < b1 ≤b2,
(b) o ∈
◦A ∧b1 ≤b2 < o
need to be considered.
All these cases will be covered by Theorem 4.10 to follow. To prove Theorem 4.10,
we require the following lemma:
Lemma 4.9. Let {R, ≤} be a linearly ordered complete lattice, and let Ai ⊆R,
i = 1(1)n. If A := "n
i=1 Ai, then
inf A = min
i=1(1)n{inf Ai}
∧
sup A = max
i=1(1)n{sup Ai}.
Proof. Suppose A consists of two sets A = A1 ∪A2. Then either inf A1 = inf A2 or
by relabeling if necessary inf A1 < inf A2. In the ﬁrst case inf A = inf A1 = inf A2,
and in the second case inf A = inf A1 < inf A2. Thus the assertion of the lemma is
correct in either case. The balance of the proof is a consequence of the fact that the
union of sets is associative: A = (· · · (A1 ∪A2) ∪· · · ∪An−1) ∪An.
■
Theorem 4.10. Let {R, N, +, ·, /, ≤}, (o ∈N), be a completely and linearly ordered
division ringoid with the neutral elements o and e, and let {IR, !
N,
+ , · ,
/ , ≤},
!
N := {A ∈IR | A ∩N ̸= ∅} be the ordered division ringoid of intervals over R.
Then for A = [a1, a2], B = [b1, b2] ∈IR the following properties hold:
(a) A ≥[o, o] ∧o ∈
◦B ⇒A · B = [a2b1, a2b2],
(b) A ≤[o, o] ∧o ∈
◦B ⇒A · B = [a1b2, a1b1],
(c) o ∈
◦A ∧B ≥[o, o] ⇒A · B = [a1b2, a2b2],
(d) o ∈
◦A ∧B ≤[o, o] ⇒A · B = [a2b1, a1b1],
(e) o ∈
◦A ∧o ∈
◦B ⇒A · B = [min{a1b2, a2b1}, max{a1b1, a2b2}].
Moreover, for A = [a1, a2] ∈IR and B = [b1, b2] ∈IR \ !
N:
(f) o ∈
◦A ∧o < b1 ≤b2 ⇒A / B = [a1/b1, a2/b1],
(g) o ∈
◦A ∧b1 ≤b2 < o ⇒A / B = [a2/b2, a1/b2].
Proof. The operations are deﬁned by (RG)
A ◦B :=
(A ◦B) = [inf(A ◦B), sup(A ◦B)].
If o ∈B = [b1, b2] so that b1 < o < b2 and if B1 := [b1, o] and B2 := [o, b2] so that
B = B1 ∪B2, then A · B = {a · b | a ∈A ∧b ∈B} = {a · b | a ∈A ∧b ∈
B1} ∪{a · b | a ∈A ∧b ∈B2} = A · B1 ∪A · B2.

96
4 Interval Arithmetic
By Lemma 4.9 we get
inf(A · B) = min{inf(A · B1), inf(A · B2)},
sup(A · B) = max{sup(A · B1), sup(A · B2)}.
With this relation we use Theorem 4.5 to obtain
(a) A ≥[o, o] ∧o ∈B
⇒A · B = [min{a2b1, o}, max{o, a2b2}] = [a2b1, a2b2].
(b), (c), (d) are proved in the same manner.
(e) o ∈A ∧o ∈B
⇒A · B = [min{o, o, a1b2, a2, b1}, max{a1b1, a2, b2, o, o}]
= [min{a1b2, a2, b1}, max{a1b1, a2, b2}].
(f) o ∈A ∧o < b1 ≤b2
⇒A / B = [min{a1/b1, o}, max{o, a2/b1}] = [a1/b1, a2/b1].
(g) is proved in the same manner.
■
As the result of Theorems 4.5 and 4.10, we can state that in a linearly ordered set
even for multiplication and division, the result of an interval operation A ◦B can
be expressed in terms of the bounds of the interval operands. To get each of these
bounds, typically only one multiplication or division is necessary. Only in the case
of Theorem 4.10(e), o ∈A and o ∈B, do two products have to be calculated and
compared. All these formulas are easily implemented on a computer.
For multiplication and division we summarize these results in Tables 4.1 and 4.2.
The formulas therein hold, in particular, for computations with real intervals.
The results of Theorems 4.5 and 4.10 are summarized by the following corollary.
Corollary 4.11. Let {R, {o}, +, ·, /, ≤} be a completely and linearly ordered division
ringoid. For intervals A = [a1, a2] and B = [b1, b2] of IR, the following formula
holds:
A ◦B :=
(A ◦B) = [ min
i,j=1,2{ai ◦bj}, max
i,j=1,2{ai ◦bj}]
for operations ◦∈{+, −, ·, /}, 0 /∈B in case of division.
■
Whenever in the Tables 4.1 and 4.2 both operands are comparable with the interval
[o, o] with respect to ≤, ≥, <, or >, the result of the interval operation A · B or A/B
contains both bounds of A and B. If one or both of the operands A or B, however,
contains zero as an interior point, then the result A · B and A/B is expressed by

4.2 Interval Arithmetic Over a Linearly Ordered Set
97
A = [a1, a2]
B = [b1, b2]
A · B
1
A ≥[o, o]
B ≥[o, o]
[a1b1, a2b2]
2
A ≥[o, o]
B ≤[o, o]
[a2b1, a1b2]
3
A ≥[o, o]
b1 < o < b2
[a2b1, a2b2]
4
A ≤[o, o]
B ≥[o, o]
[a1b2, a2b1]
5
A ≤[o, o]
B ≤[o, o]
[a2b2, a1b1]
6
A ≤[o, o]
b1 < o < b2
[a1b2, a1b1]
7
a1 < o < a2
B ≥[o, o]
[a1b2, a2b2]
8
a1 < o < a2
B ≤[o, o]
[a2b1, a1b1]
9
a1 < o < a2
b1 < o < b2
[min{a1b2, a2b1}, max{a1b1, a2b2}]
Table 4.1. Execution of multiplication.
A = [a1, a2]
B = [b1, b2]
A/B
1
A ≥[o, o]
0 < b1 ≤b2
[a1/b2, a2/b1]
2
A ≥[o, o]
b1 ≤b2 < 0
[a2/b2, a1/b1]
3
A ≤[o, o]
0 < b1 ≤b2
[a1/b1, a2/b2]
4
A ≤[o, o]
b1 ≤b2 < 0
[a2/b1, a1/b2]
5
a1 < o < a2
0 < b1 ≤b2
[a1/b1, a2/b1]
6
a1 < o < a2
b1 ≤b2 < 0
[a2/b2, a1/b2]
Table 4.2. Execution of division with B not containing 0.
only three of the four bounds of A and B. In all these cases (3,6,7,8,9) in Table 4.1,
the bound which is missing in the expression for the result can be shifted towards
zero without changing the result of the operation A · B. Similarly, in cases 5 and
6 in Table 4.2, the bound of B, which is missing in the expression for the resulting
interval, can be shifted toward ∞(resp. −∞) without changing the result of the oper-
ation A/B. This shows a certain lack of sensitivity of interval arithmetic whenever in
multiplication and division one of the operands contains zero as an interior point.
In all these cases – 3,6,7,8,9 of Table 4.1 and 5,6 of Table 4.2 – the result of A·B or
A/B also contains zero, and the formulas show that the result tends toward the zero
interval if the operands that contain zero do likewise. In the limit when the operand
that contains zero has become the zero interval, no such imprecision is left. This
suggests that within arithmetic expressions interval operands that contain zero as an
interior point should be made as small in diameter as possible.
We conclude this section with the following observation concerning IR.
Remark 4.12. It is an interesting fact, but not essential for the development of this
theory, that the intervals IR over the real numbers R form an algebraically closed
subset of the power set PR, i.e.,

A,B∈IR
A ◦B :=
(A ◦B) = A ◦B, for all ◦∈{+, −, ·, /}.
Here for division we assume that 0 /∈B.

98
4 Interval Arithmetic
To see this, we recall that A ◦B :=
(A ◦B) is the least interval that contains
A ◦B := {a ◦b | a ∈A, b ∈B}. It is well known from real analysis that for
all ◦∈{+, −, ·, /}, a ◦b is a continuous function of both variables. A ◦B is the
range of this function over the product set A×B. Since A and B are closed intervals,
A × B is a simply connected, bounded and closed subset of R2. In such a region the
continuous function a ◦b takes a maximum and a minimum as well as all values in
between. Therefore,
A ◦B :=
#
min
a∈A,b∈B{a ◦b},
max
a∈A,b∈B{a ◦b}
$
=
(A ◦B) = A ◦B.
■
4.3
Interval Matrices
Let {R, +, ·, ≤} be a completely ordered (resp. weakly ordered) ringoid with the neu-
tral elements o and e, and {MnR, +, ·, ≤} the ordered (resp. weakly ordered) ringoid
of matrices over R with the neutral elements
O =
⎛
⎜
⎜
⎜
⎝
o
o
. . .
o
o
o
...
...
...
...
...
o
o
. . .
o
o
⎞
⎟
⎟
⎟
⎠,
E =
⎛
⎜
⎜
⎜
⎝
e
o
. . .
o
o
e
...
...
...
...
...
o
o
. . .
o
e
⎞
⎟
⎟
⎟
⎠.
The power set {PM nR \ {∅}, +, ·, ⊆} is also a ringoid. If IMnR denotes the set
of intervals over {MnR, ≤}, then according to Theorem 4.4, IMnR := IMnR ∪∅
is a symmetric upper screen of {PM nR, ⊆}, and the monotone upwardly directed
rounding
: PMnR →IMnR is antisymmetric. We consider its semimorphism,
which in IMnR deﬁnes operations
◦, with ◦∈{+, ·}, by
(RG)

A,B∈IMnR
A ◦B :=
(A ◦B) = [inf(A ◦B), sup(A ◦B)].
Then according to Theorem 4.5 and under its hypotheses, {IMnR,
+ , · , ≤, ⊆}
is also a completely ordered (resp. weakly ordered) ringoid with respect to ≤and an
inclusion-isotonally ordered monotone upper screen ringoid of PMnR with respect
to ⊆.
By Theorem 4.5(a), the result of an addition or subtraction in IMnR can always
be expressed in terms of the bounds of the operands. For a multiplication this is only
possible if {R, +, ·, ≤} is an ordered ringoid and the operands are comparable with
the interval [O, O] ∈IMnR with respect to ≤. We are now going to derive explicit
formulas that are simple to implement for all products in IMnR.
To do this, we consider the set of n × n matrices MnIR. The elements of this set
have components that are intervals over R. If {R, +, ·, ≤} is a completely ordered
(resp. weakly ordered) ringoid, then by Theorem 4.5, {IR,
+ , · , ≤} is such a struc-
ture also. With the operations and order relation of the latter, we deﬁne operations
◦,

4.3 Interval Matrices
99
◦∈{+, ·}, and an order relation ≤in MnIR by employing the conventional deﬁnition
of operations for matrices:

A=(Aij),B=(Bij)∈MnIR
A + B := (Aij + Bij),

A=(Aij),B=(Bij)∈MnIR
A · B :=
 n
ν=1
Aiν · Bνj

,
(Aij) ≤(Bij) :⇔

i=1(1)n

j=1(1)n
Aij ≤Bij.
(4.3.1)
Here  denotes the repeated summation in IR.
By Theorem 2.6, we deduce directly that {MnIR,
+ , · , ≤} is a completely or-
dered (resp. weakly ordered) ringoid. We shall see that under certain further assump-
tions the ringoids {MnIR,
+ , · , ≤} and {IMnR,
+ , · , ≤} are isomorphic.
To this end, we deﬁne a mapping
χ : MnIR →IMnR
which for matrices A = (Aij) ∈MnIR with Aij = [a(1)
ij , a(2)
ij ] ∈IR, i, j = 1(1)n,
has the property
χA = χ(Aij) = χ([a(1)
ij , a(2)
ij ]) := [(a(1)
ij ), (a(2)
ij )].
(4.3.2)
Obviously χ is a one-to-one mapping of MnIR onto IMnR and an order isomor-
phism with respect to ≤. To characterize the algebraic operations, we prove Lem-
mas 4.13 and 4.14. In certain cases we ﬁnd that χ is also an algebraic isomorphism.
Lemma 4.13. Let {R, +, ·, ≤} be a completely and weakly ordered ringoid and con-
sider the semimorphism deﬁned by the monotone upwardly directed rounding
:
PR →IR. If the formula

Aν,Bν∈IR
 n

ν=1
Aν · Bν ⊆
n

ν=1
Aν · Bν

(4.3.3)
between the operations in {PR, +, ·} and {IR,
+ , · , ≤} holds, then the mapping
χ establishes an isomorphism between the completely and weakly ordered ringoids
{MnIR,
+ , · , ≤} and {IMnR,
+ , · , ≤} with respect to the algebraic operations
and the order relation.
Proof. The isomorphism of the order structures was already noted. Let A, B ∈MnIR,
where
A := ([a(1)
ij , a(2)
ij ]),
B := ([b(1)
ij , b(2)
ij ]).

100
4 Interval Arithmetic
Then for the addition of the images,
χA = [(a(1)
ij ), (a(2)
ij )], χB = [(b(1)
ij ), (b(2)
ij )] ∈IMnR
we obtain by Theorem 4.5(a) that
χA + χB =
(χA + χB) = [(a(1)
ij ) + (b(1)
ij ), (a(2)
ij ) + (b(2)
ij )]
= [(a(1)
ij + b(1)
ij ), (a(2)
ij + b(2)
ij )].
(4.3.4)
In MnIR we obtain also by Theorem 4.5(a) that
A + B := (Aij + Bij) = ([a(1)
ij + b(1)
ij , a(2)
ij + b(2)
ij ]).
(4.3.5)
Equations (4.3.4) and (4.3.5) imply
χA + χB = χ(A + B),
which proves the isomorphism of addition. For subtraction the proof can be given
analogously.
As a next step we prove the isomorphism for multiplication:
χA · χB = χ(A · B).
Let be Ai ∈IR, i = 1(1)n and Ai = [ai1, ai2] = [inf Ai, sup Ai]. Then we have

ai∈Ai

inf Ai ≤ai
⇒
(OD1)R
n

i=1
inf Ai ≤
n

i=1
ai

⇒
n

i=1
inf Ai ≤inf
n

i=1
Ai. (4.3.6)
On the other hand we have
inf
n

i=1
Ai ≤
n

i=1
ai1 =
n

i=1
inf Ai.
(4.3.7)
From (4.3.6), (4.3.7) and (O3) follows
n

i=1
inf Ai = inf
n

i=1
Ai ∧
n

i=1
sup Ai = sup
n

i=1
Ai
⇒
n
i=1
Ai =
% n

i=1
inf Ai,
n

i=1
sup Ai
&
(4.3.8)
=
%
inf
n

i=1
Ai, sup
n

i=1
Ai
&
=
n

i=1
Ai.

4.3 Interval Matrices
101
We have Ai ·Bi ⊆
(R3)
(Ai ·Bi) =
(RG) Ai · Bi
⇒
(OD5)R
n
i=1 Ai ·Bi ⊆n
i=1 Ai · Bi.
Using (R2) we get
n

i=1
Ai · Bi ⊆
n

i=1
Ai · Bi,
(4.3.9)
and from (4.3.3) we get by applying (R1) and (R2)
n

i=1
Ai · Bi ⊆
n

i=1
Ai · Bi.
(4.3.10)
Combining (4.3.9) and (4.3.10) yields
n

i=1
Ai · Bi =
n

i=1
Ai · Bi.
(4.3.11)
With A = (Aij) and B = (Bij) ∈MnIR we obtain:
χ(A · B) := χ
 n
ν=1
Aiν · Bνj

=
(4.3.8) χ

n

ν=1
Aiν · Bνj

=
(4.3.11) χ

n

ν=1
Aiν · Bνj

= χ
%
inf
n

ν=1
AiνBνj, sup
n

ν=1
AiνBνj
&
=
%
inf
n

ν=1
AiνBνj

,

sup
n

ν=1
AiνBνj
&
=
%
inf
 n

ν=1
AiνBνj

, sup
 n

ν=1
AiνBνj
&
=
%
inf
 n

ν=1
aiνbνj

, sup
 n

ν=1
aiνbνj
&
with (aij) ∈χA, (bij) ∈χB
=
(χA · χB) = χA · χB.
■
Lemma 4.14. Let {R, +, ·, ≤} be a completely and linearly ordered ringoid and
:
PR →IR the monotone upwardly directed rounding. Then

Aν,Bν∈IR
 n

ν=1
Aν · Bν ⊆
n

ν=1
Aν · Bν

.
Proof. By Corollary 4.11, we have for all A = [a1, a2] and B = [b1, b2] ∈IR
A · B = [ min
i,j=1,2(ai · bj), max
i,j=1,2(ai · bj)].
(4.3.12)

102
4 Interval Arithmetic
Let Ai = [ai1, ai2] and Bi = [bi1, bi2], i = 1(1)n. Then by the deﬁnition of
addition in PR we know that every x ∈n
i=1 Ai · Bi is of the form x = n
i=1 xi
with xi ∈Ai · Bi. From (4.3.12) we get
min
μ,ν=1,2(aiμ · biν) ≤xi ≤max
μ,ν=1,2(aiμ · biν)
⇒
OD1)R
n

i=1
min
μ,ν=1,2(aiμ · biν) ≤x ≤
n

i=1
max
μ,ν=1,2(aiμ · biν).
inf
n

i=1
AiBi ≤
n

i=1
min
μ,ν=1,2(aiμ · biν) ≤x ≤
n

i=1
max
μ,ν=1,2(aiμ · biν) ≤sup
n

i=1
AiBi,
i.e., every x ∈n
i=1 Ai · Bi is also an element of
n
i=1 Ai · Bi.
■
Lemmas 4.13 and 4.14 imply the following theorem, which establishes the isomor-
phic character of χ for a linearly ordered ringoid.
Theorem 4.15. Let {R, +, ·, ≤} be a completely and linearly ordered ringoid. Then
the mapping χ establishes an isomorphism between the completely ordered ringoids
{MnIR,
+ , · , ≤} and {IMnR,
+ , · , ≤} with respect to the algebraic and the or-
der structure.
■
IR
MnIR
χ
IMnR
PM nR
PR
R
MnR
Figure 4.1. Illustration of Theorem 4.15.
Whenever two structures {MnIR,
+ , · , ≤} and {IMnR,
+ , · , ≤} are isomor-
phic, corresponding elements can be identiﬁed with each other. This allows us to
deﬁne an inclusion relation even for elements A = (Aij), B = (Bij) ∈MnIR by
A ⊆B :⇔

i,j=1(1)n
Aij ⊆Bij,
and
(aij) ∈A = (Aij) :⇔

i,j=1(1)n
aij ∈Aij.

4.4 Interval Vectors
103
This convenient deﬁnition allows for the interpretation that a matrix A = (Aij) ∈
MnIR also represents a set of matrices as demonstrated by the following identity:
A = (Aij) ≡{(aij) | aij ∈Aij, i, j = 1(1)n}.
Both matrices contain the same elements.
For a linearly ordered ringoid {R, +, ·, ≤} for all operations in the ringoid {IR,
+,
· , ≤}, we have derived explicit and easily performable formulas in Section 4.2. The
formulas (4.3.1) for the operations
◦, ◦∈{+, ·}, in {MnIR,
+ , · , ≤}, therefore,
are also easily performable. This is not so for the operations · originally deﬁned in
IMnR. By means of the isomorphism, the operation
· can now be used to perform
the multiplication · .
The isomorphism shows that both the conventional deﬁnition of the operations in
MnIR in terms of those in IR and the deﬁnition of the operations in IMnR by means
of the semimorphism
: PM nR →IMnR lead to the same result. Figure 4.1
illustrates the connection.
4.4
Interval Vectors
Similar to Section 4.3, we are now going to derive easily implementable formulas for
the interval operations occurring in vectoids.
Again let {R, +, ·, ≤} be a completely ordered or weakly ordered ringoid. Then
{VnR, R, ≤}, {MnR, R, ≤}, and {VnR, MnR, ≤}, where the operations and the or-
der relation are deﬁned by the usual formulas, are completely ordered (resp. weakly
ordered) vectoids. {MnR, R, ≤} is in particular multiplicative.
Moreover, by Theorem 2.11 the three power sets {PVnR, PR, ⊆}, {PM nR, PR, ⊆
}, and {PVnR, PMnR, ⊆} are inclusion-isotonally ordered vectoids (with the empty
set excluded from the algebraic operations).
Let us now consider the semimorphism
: PVnR →IVnR := IVnR ∪∅.
With this mapping and by Theorem 4.7, {IVnR, IR, ≤, ⊆}, {IMnR, IR, ≤, ⊆}, and
{IVnR, IMnR, ≤, ⊆} become completely ordered (resp. weakly ordered) vectoids
with respect to ≤and inclusion-isotonally ordered monotone upper screen vectoids of
the corresponding power sets with respect to ⊆. Moreover, IMnR is multiplicative.
By Theorem 4.7, the result of an addition or subtraction in IVnR and IMnR can
always be expressed in terms of the bounds of the interval operands. For an outer
multiplication, this is possible only if the vectoid is ordered and if the operands are
comparable with the corresponding zero element with respect to ≤. Inner multipli-
cation in IMnR has already been considered in Section 4.3. We are now going to
derive explicit and easily implementable formulas for all outer multiplications in the
vectoids {IVnR, IR}, {IMnR, IR}, and {IVnR, IMnR}.
To do this, we consider the sets VnIR and MnIR. The elements of these sets have
components that are intervals over R. If {R, +, ·, ≤} is a completely ordered (resp.

104
4 Interval Arithmetic
weakly ordered) ringoid, then by Theorem 4.5, {IR,
+ , · , ≤} also has these prop-
erties. Employing the operations and order relation of the latter, we deﬁne operations
and an order relation ≤in VnIR and MnIR by means of the conventional method:

a=(Ai),b=(Bi)∈VnIR
a + b := (Ai + Bi),

A∈IR

a=(Ai)∈VnIR
A · a := (A · Ai),

A∈IR

A=(Aij)∈MnIR
A · A := (A · Aij),

A=(Aij)∈MnIR

a=(Ai)∈VnIR
A · a :=
 n
ν=1
Aiν · Aν

,
(Ai) ≤(Bi) :=

i=1(1)n
Ai ≤Bi.
(4.4.1)
Here as before 
denotes the repeated summation in IR.
For MnIR, we take the deﬁnition of the inner operations
+ ,
· , and the order
relation ≤as given in (4.3.1) in Section 4.3.
Using Theorems 2.13, 2.14 and 2.15, we obtain directly that {VnIR, IR, ≤},
{MnIR, IR, ≤}, and {VnIR, MnIR, ≤} are completely ordered (resp. weakly or-
dered) vectoids. Moreover, {MnIR, IR, ≤} is multiplicative.
Under certain further assumptions, we shall see that isomorphisms exist between
the following pairs of vectoids:
{VnIR, IR, ≤} ↔{IVnR, IR, ≤},
{MnIR, IR, ≤} ↔{IMnR, IR, ≤},
{VnIR, MnIR, ≤} ↔{IVnR, IMnR, ≤}.
To see this for vectors and matrices
a = (Ai) ∈VnIR
with
Ai = [a(1)
i , a(2)
i ] ∈IR,
A = (Aij) ∈MnIR
with
Aij = [a(1)
ij , a(2)
ij ] ∈IR,
we deﬁne the mappings ψ and χ:
ψ : VnIR →IVnR,
(4.4.2)
where ψa = ψ(Ai) = ψ([a(1)
i , a(2)
i ]) = [(a(1)
i ), (a(2)
i )], and
χ : MnIR →IMnR,
(4.4.3)
where χA = χ(Aij) = χ([a(1)
ij , a(2)
ij ]) = [(a(1)
ij ), (a(2)
ij )].

4.4 Interval Vectors
105
Obviously, the mappings ψ and χ are one-to-one and order isomorphisms with
respect to ≤. The following theorem characterizes properties of ψ and χ with respect
to the algebraic operations.
Theorem 4.16. Let {R, +, ·, ≤} be a completely and weakly ordered ringoid and
{IR,
+ , · , ≤} be the completely and weakly ordered ringoid of intervals over R.
Then we have
(a) The mapping ψ establishes an isomorphism between {VnIR, IR, ≤} and
{IVnR, IR, ≤} with respect to the algebraic and the order structure.
(b) If
: PR →IR denotes the monotone upwardly directed rounding and if for
the operations in PR and IR the formula

Aν,Bν∈IR
 n

ν=1
Aν · Bν ⊆
n

ν=1
Aν · Bν

(4.4.4)
holds, then the mapping χ establishes an isomorphism between the two com-
pletely and weakly ordered multiplicative vectoids {MnIR, IR, ≤} and
{IMnR, IR, ≤} with respect to the algebraic and the order structure.
(c) Under the assumption (4.4.4), the mappings establish an isomorphism between
the completely and weakly ordered vectoids {VnIR, MnIR, ≤} and {IVnR,
IMnR, ≤} with respect to the algebraic and the order structure.
Proof. The isomorphism of the order structure has already been noted immediately
preceding the statement of this theorem.
(a) The isomorphism of addition can be proved as in Lemma 4.13. For outer multi-
plication, we get with A ∈IR and a = (Ai) ∈VnIR, Ai ∈IR, i = 1(1)n:
ψ(A · a) := ψ(A · Ai) = ψ(
(A · Ai))
= ψ(inf(A · Ai), sup(A · Ai)) = [(inf(A · Ai)), (sup(A · Ai))]
= [inf(A · Ai), sup(A · Ai)] =
(A · ψa) = A · ψa.
(b) The isomorphism of the inner operations is proved in Lemma 4.13 above. The
proof for outer multiplications is analogous to (a).
(c) Lemma 4.13 demonstrated the isomorphism of the ringoids {MnIR,
+ ,
· , ≤}
and {IMnR,
+ , · , ≤}. The isomorphism of addition in VnIR and IVnR was
proved under (a). Therefore only the isomorphism of outer multiplication re-
mains to be shown. Using (4.4.4) it can be shown as in the proof of Lemma 4.13
that
ψ(A · a) = χA · ψa.
■

106
4 Interval Arithmetic
Combining this theorem with Lemma 4.14, we obtain the following theorem which
further characterizes the isomorphisms induced by the mappings ψ and χ.
Theorem 4.17. Let {R, +, ·, ≤} be a completely and linearly ordered ringoid. Then:
(a) The mapping χ establishes an isomorphism between the completely ordered vec-
toids {MnIR, IR, ≤} and {IMnR, IR, ≤} with respect to the algebraic and the
order structure.
(b) The mappings ψ and χ establish an isomorphism between the two completely
ordered vectoids {VnIR, MnIR, ≤} and {IVnR, IMnR, ≤} with respect to the
algebraic and the order structure.
■
Whenever the structures {VnIR, IR, ≤} and {IVnR, IR, ≤} (resp. {VnIR,
MnIR, ≤} and {IVnR, IMnR, ≤}) are isomorphic, corresponding elements can be
identiﬁed with one another. This allows us to deﬁne an inclusion relation even for
elements a = (Ai), b = (Bi) ∈VnIR by
a ⊆b :⇔

i=1(1)n
Ai ⊆Bi.
(ai) ∈a = (Ai) :⇔

i=1(1)n
ai ∈Ai.
By means of the following identity, this deﬁnition allows for the interpretation that
an interval vector a = (Ai) ∈VnIR also represents a set of vectors
a = (Ai) ≡{(ai) | ai ∈Ai, i = 1(1)n}.
In the case of a linearly ordered ringoid {R, +, ·, ≤} for all operations in the ringoid
{IR,
+ , · , ≤}, we have derived explicit and easily performable formulas in Sec-
tion 4.2 of this chapter. Formulas (4.4.1) for the operations
◦, therefore, are also
easily performable. This is not the case for the operations
◦originally deﬁned, for
instance, in {IVnR, IMnR, ≤}. Appealing to the isomorphism, we see that the oper-
ations
◦can be used in order to execute the operations
◦.
4.5
Interval Arithmetic on a Screen
Let {R, ≤} be a complete lattice and {S, ≤} a screen of {R, ≤}. Now we consider
intervals over R with endpoints in S:
[a1, a2] := {x ∈R | a1, a2 ∈S, a1 ≤x ≤a2}
with a1 ≤a2.
We denote the set of all such intervals by IS. Then IS ⊆IR and IS := IS ∪{∅} ⊆
IR := IR ∪{∅}.
Several properties of IS are described in the following theorem.

4.5 Interval Arithmetic on a Screen
107
Theorem 4.18. Let {R, ≤} be a complete lattice and {S, ≤} a screen of {R, ≤}.
Then:
(a) {IS, ⊆} is a conditionally completely ordered set. For all nonempty subsets A of
IS, which are bounded below, we have with B = [b1, b2] ∈IS:
inf
IS A = [ sup
B∈A
b1, inf
B∈A b2]
∧
sup
IS
A = [ inf
B∈A b1, sup
B∈A
b2],
i.e., the inﬁmum is the intersection, and the supremum is the interval hull of the
elements of A.
(b) {IS, ⊆} is a screen of {IR, ⊆}.
Proof. (a) Since {S, ≤} is a complete lattice and A is bounded below (i.e., A contains
an interval), the intervals
I = [ sup
B∈A
b1, inf
B∈A b2]
∧
S = [ inf
B∈A b1, sup
B∈A
b2]
exist, and for all B ⊆A, we have I ⊆B ∧B ⊆S, i.e., I is a lower bound and S an
upper bound of A. If I1 is any lower and S1 any upper bound of A, then I1 ⊆I and
S ⊆S1, i.e., I is the greatest lower and S the least upper bound of A.
(b) {IS, ⊆} is a complete lattice and therefore a complete subnet of {IR, ⊆}. We still
have to show that for every subset A ⊆IS, infIS A = infIR A and supIS A = supIR A.
If A is bounded from below in IS, then by (a), the inﬁmum is the intersection
and the supremum is the interval hull as in IR. If A is not bounded below in IS we
consider the cases
1. A = ∅. Then infIS A = infIR A = i(IS) = i(IR) = R and supIS A = supIR A =
o(IS) = o(IR) = ∅.
2. A ̸= ∅. Then supIS A = supIR A = [infB∈A b1, supB∈A b2] by (a).
2.1 ∅∈A. Then infIS A = ∅= infIR A.
2.2 ∅/∈A. Since A is not bounded below in IS, we get infIS A = ∅= infIR A.
■
In general, interval calculations are employed to determine sets that include the
solution of a given problem. If {R, ≤} is an ordered set and the operations for the
intervals of IR cannot be performed precisely on a computer, one has to approximate
them on a screen {S, ≤} of {R, ≤}. This approximation will be required to have the
following general properties, which we shall make precise later:
(a) The result of any computation in the subset IS always has to include the result of
the corresponding calculation in IR.
(b) The result of the computation in IS should be as close as possible to the result of
the corresponding calculation in IR.

108
4 Interval Arithmetic
We may seek to achieve these qualitative requirements by deﬁning the operations
in IS by means of the semimorphism associated with the monotone upwardly directed
rounding ♦: IR →IS. We are now going to describe this process.
The monotone upwardly directed rounding ♦: IR →IS is deﬁned by the proper-
ties
(R1)

A∈IS
♦A = A,
(R2)

A,B∈IR
(A ⊆B ⇒♦A ⊆♦B),
(R3)

A∈IR
A ⊆♦A.
In the following theorem we develop some properties of this rounding.
Theorem 4.19. Let {R, +, ·, ≤} be a completely and weakly ordered ringoid with the
neutral elements o and e, and {S, ≤} a symmetric screen of {R, +, ·, ≤}. Further, let
{IR,
+ , · } be a ringoid deﬁned by the semimorphism
: PR →IR. Then:
(a) {IS, ⊆} is a symmetric screen of {IR,
+ , · }, i.e., we have
(S3)
[o, o], [e, e] ∈IS
∧

A=[a1,a2]∈IS
−A = [−a2, −a1] ∈IS.
(4.5.1)
(b) The monotone upwardly directed rounding ♦: IR →IS is antisymmetric, i.e.,
(R4)

A∈IR
♦( −A) =
−(♦A).
(c) We have
(R)

A=[a1,a2]∈IR
♦A = inf(U(A) ∩IS) = [▽a1, △a2].
(4.5.2)
Proof. (a) With A = [a1, a2] ∈IS we obtain
−A := [−e, −e] · A :=
({−e} · A) =
(−A)
=
(S3)IR, (R1) −A
=
Theorem 4.4(a) [−a2, −a1]
∈
(S3)S
IS.
In the next step we prove (c).
(c) With B = [b1, b2] ∈IS, we get for all A = [a1, a2] ∈IR:
inf
IR(U(A) ∩IS)
=
Theorem 4.18(b) inf
IS (U(A) ∩IS)
=
Theorem 4.18(a) [
sup
B∈U(A)∩IS
b1,
inf
B∈U(A)∩IS b2].

4.5 Interval Arithmetic on a Screen
109
In general we have
[a1, a2] ⊆[b1, b2] ⇔b1 ≤a1 ∧a2 ≤b2.
With this and Theorem 1.24, we get:
♦A = inf
IS (U(A) ∩IS) = [
sup
b1∈S∧b1≤a1
b1,
inf
b2∈S∧a2≤b2 b2]
= [sup(L(a1) ∩S), inf(U(a2) ∩S)] = [▽a1, △a2].
(b) For all A = [a1, a2] ∈IR we have
♦( −A)
=
(4.5.1) ♦[−a2, −a1]
=
(4.5.2) [▽(−a2), △(−a1)]
=
Theorem 3.4 [−△a2, −▽a1]
=
(4.5.1)
−[▽a1, △a2]
=
(4.5.2)
−(♦A).
■
The monotone upwardly directed rounding ♦: IR →IS can be employed as a
semimorphism to deﬁne operations ♦
◦, ◦∈{+, ·, /} in IS as
(RG)

A,B∈IS
A♦
◦B := ♦(A ◦B) := ♦(
(A ◦B))
= ♦[inf(A ◦B), sup(A ◦B)]
=
(4.5.2) [▽inf(A ◦B), △sup(A ◦B)].
These operations have the following properties which by Theorem 1.32 also deﬁne
them:
(RG1)

A,B∈IS
(A ◦B ∈IS ⇒A♦
◦B = A ◦B),
(RG2)

A,B,C,D∈IS
(A ◦B ⊆C ◦D ⇒A♦
◦B ⊆C ♦
◦D),
(RG3)

A,B∈IS
(A ◦B ⊆A♦
◦B).
In these formulas we assume additionally in the case of division, that B, D /∈N.
For the deﬁnition of N see Theorem 4.20, to which we now turn.
Theorem 4.20. Let {R, +, ·, ≤} be a completely and weakly ordered ringoid with the
neutral elements o and e. We assume additionally that x = −e is already unique in R
by (D5a) alone3. Further, let {S, ≤} be a symmetric screen of {R, +, ·, ≤}. Consider
3This is the case, for instance, if {R, +} is a group.

110
4 Interval Arithmetic
the semimorphisms
: PR →IR and ♦: IR →IS. Then:
(a) {IS, ♦
+ , ♦· , ≤} is a weakly ordered ringoid.
(b) If {R, +, ·, ≤} is an ordered ringoid, then {IS, ♦
+ , ♦· , ≤} is also an ordered
ringoid.
(c) If {R, N, +, ·, /, ≤} is an ordered (resp. weakly ordered) division ringoid, then
{IS, N, ♦
+ , ♦· , ♦
/ , ≤} with N := {A ∈IS | A ∩N ̸= ∅} is also an ordered
(resp. weakly ordered) division ringoid.
(d) {IS, N, ♦
+ , ♦· , ♦
/ , ⊆} is inclusion isotonally ordered.
Proof. By Theorem 4.5 {IR,
+ , · } is a ringoid. Theorem 3.5 directly implies the
properties (D1,2,3,4,5,7,8,9). (OD5) and (OD6) in IS follow immediately from (OD5)
and (OD6) in IR and the monotonicity of the rounding ♦. It remains to show (D6).
(D6): Let X = [x, y] be any element of IS that fulﬁls (D5). Then
[e, e]♦
+ [x, y] = ♦([e, e] + [x, y]) = ♦([e + x, e + y]) = [o, o].
(4.5.3)
♦: IR →IS is the upwardly directed rounding with respect to ⊆. If the image of an
interval only consists of the single point o ∈R, then this interval itself must already
be this point, i.e., by (4.5.3) we obtain
e + x = o ∧e + y = o.
By assumption, there is exactly one element in R that solves (D5a). Thus x = y = −e
and X = [−e, −e].
It remains to show (OD1,2,3,4). By Theorem 3.5 it sufﬁces to show that the rounding
♦: IR →IS is also monotone with respect to ≤.
(R2): With A = [a1, a2] and B = [b1, b2] ∈IR, we have
A ≤B :⇔a1 ≤b1 ∧a2 ≤b2 ⇒
(R2)
▽a1 ≤▽b1 ∧△a2 ≤△b2
⇒[▽a1, △a2] ≤[▽b1, △b2] ⇒
(4.5.2) ♦A ≤♦B.
■
We shall apply Theorem 4.10 to various interval sets. The ﬁrst application, given in
the following theorem, deals with intervals on a screen of the linearly ordered division
ringoid of the real numbers R.
Theorem 4.21. Let {R, {0}, +, ·, /, ≤} be the completely and linearly ordered ringoid
of real numbers, and {S, ≤} a symmetric screen of R. Consider the semimorphisms
: PR →IR and ♦: IR →IS. Then {IS, N, ♦
+ , ♦· , ♦
/ , ≤, ⊆} with N :=
{A ∈IS | 0 ∈A} is an ordered division ringoid with respect to ≤and an inclusion-

4.5 Interval Arithmetic on a Screen
111
isotonally ordered monotone upper screen division ringoid of IR with respect to ⊆.
The neutral elements are [0, 0] and [1, 1].
■
Because of the great importance of the intervals on a screen of the real numbers R,
we derive explicit formulas for the operations in IS.
Let {R, {0}, +, ·, /, ≤} be the linearly ordered division ringoid of real numbers
and A = [a1, a2], B = [b1, b2] ∈IR. Corollary 4.11 summarizes the result of all
operations
◦, ◦∈{+, −, ·, /} in IR by means of the formula
A ◦B :=
(A ◦B) =
'
min
i,j=1,2(ai ◦bj), max
i,j=1,2(ai ◦bj)
(
.
By the deﬁnition of the operations in IS and by (4.5.2), we obtain for all A =
[a1, a2], B = [b1, b2] ∈IS that
A♦
◦B := ♦(A ◦B) =
'
▽min
i,j=1,2(ai ◦bj), △max
i,j=1,2(ai ◦bj)
(
.
Since ▽: R →S and △: R →S are monotone mappings, we obtain
A♦
◦B := ♦(A ◦B) =
'
min
i,j=1,2(ai▽
◦bj), max
i,j=1,2(ai△
◦bj)
(
.
For execution of the operations ♦
◦, ◦∈{+, −, ·, /}, in IS on a computer, we dis-
play the following formulas and Tables 4.3 and 4.4. These are obtained by employing
the preceding equation, Theorems 4.5 and 4.10, and especially Tables 4.1 and 4.2.
A = [a1, a2]
B = [b1, b2]
A♦· B = [mini,j=1,2(ai▽· bj), maxi,j=1,2(ai△· bj)]
1
A ≥[0, 0]
B ≥[0, 0]
[a1▽· b1, a2△· b2]
2
A ≥[0, 0]
B ≤[0, 0]
[a2▽· b1, a1△· b2]
3
A ≥[0, 0]
b1 < 0 < b2
[a2▽· b1, a2△· b2]
4
A ≤[0, 0]
B ≥[0, 0]
[a1▽· b2, a2△· b1]
5
A ≤[0, 0]
B ≤[0, 0]
[a2▽· b2, a1△· b1]
6
A ≤[0, 0]
b1 < 0 < b2
[a1▽· b2, a1△· b1]
7
a1 < 0 < a2
B ≥[0, 0]
[a1▽· b2, a2△· b2]
8
a1 < 0 < a2
B ≤[0, 0]
[a2▽· b1, a1△· b1]
9
a1 < 0 < a2
b1 < 0 < b2
[min(a1▽· b2, a2▽· b1), max(a1△· b1, a2△· b2)]
Table 4.3. Execution of the multiplication in IS.
The formulas in the Tables 4.3 and 4.4 show, in particular, that the operations ♦
◦,
◦∈{+, −, ·, /}, in IS are executable on a computer if the operations ▽
◦and △
◦,

112
4 Interval Arithmetic
A = [a1, a2]
B = [b1, b2]
A♦
/ B = [mini,j=1,2(ai▽
/ bj), maxi,j=1,2(ai△
/ bj)]
1
A ≥[0, 0]
0 < b1 ≤b2
[a1▽
/ b2, a2△
/ b1]
2
A ≥[0, 0]
b1 ≤b2 < 0
[a2▽
/ b2, a1△
/ b1]
4
A ≤[0, 0]
0 < b1 ≤b2
[a1▽
/ b1, a2△
/ b2]
5
A ≤[0, 0]
b1 ≤b2 < 0
[a2▽
/ b1, a1△
/ b2]
7
a1 < 0 < a2
0 < b1 ≤b2
[a1▽
/ b1, a2△
/ b1]
8
a1 < 0 < a2
b1 ≤b2 < 0
[a2▽
/ b2, a1△
/ b2]
Table 4.4. Execution of division in IS where B does not contain 0.
◦∈{+, −, ·, /}, for elements of S are available. These latter operations are deﬁned
by the following formulas:

a,b∈S
a▽
◦b := ▽(a ◦b)
∧

a,b∈S
a△
◦b := △(a ◦b).
Here for division we additionally assume that b ̸= 0.
With regard to dependency and economy we mention the following: In order to
perform the eight operations ▽
◦and △
◦, ◦∈{+, −, ·, /}, on a computer, it is sufﬁcient
if three of them, for instance, ▽
+ , ▽· , ▽
/ , or an equivalent triple, are available. This is
a consequence of the fact that subtraction can be expressed by addition and the result
of Theorem 3.4, which asserts that

a∈R
▽a = −△(−a)
∧
△a = −▽(−a).
For instance, we obtain the following list of formulas:
a▽
+ b = ▽(a + b),
a▽
−b = ▽(a + (−b)) = a▽
+ (−b),
a▽· b = ▽(a · b),
a▽
/ b = ▽(a/b),
a△
+ b = −▽(−(a + b)) = −((−a)▽
+ (−b)),
a△
−b = −▽(−(a + (−b))) = −((−a)▽
+ b),
a△· b = −▽(−(a · b)) = −((−a)▽· b) = −(a▽· (−b)),
a△
/ b = −▽(−(a/b)) = −((−a)▽
/ b) = −(a▽
/ (−b)).
We have already discussed the meaning of the monotone directed roundings ▽and
△in Section 3.5. Theorems 1.30 and 3.4 show, in particular, that in a linearly ordered

4.5 Interval Arithmetic on a Screen
113
ringoid every monotone rounding can be expressed in terms of either ▽or △. The
formula system that we have derived above shows additionally that to perform all
interval operations in IS either ▽or △sufﬁces.
Addition and multiplication in R are associative. Consequently, addition and multi-
plication in PR and, referring to Remark 4.12, those in IR are also associative. These
associativity properties, however, are no longer valid in IS. We show this by means
of a simple example for addition.
Without loss of generality we may assume that S is a ﬂoating-point system with
the number representation m·be with b = 10, m = −0.9(0.1)0.9 and e ∈{−1, 0, 1}.
Throughout the example, A, B ∈IS.
Let A := [0.3, 0.6], B := [0.3, 0.5], C := [0.3, 0.4]. Then
A♦
+ (B ♦
+ C) = [0.3, 0.6]♦
+ [0.6, 0.9] = [0.9, 0.2 · 10],
(A♦
+ B)♦
+ C = (♦[0.6, 1.1])♦
+ [0.3, 0.4] = [0.6, 0.2 · 10]♦
+ [0.3, 0.4]
= ♦[0.9, 2.4] = [0.9, 0.3 · 10].
That is
A♦
+ (B ♦
+ C) ̸= (A♦
+ B)♦
+ C.
These characteristics of the operations in IS are simple consequences of the fact
that the associative laws fail in {S, ▽
◦} and {S, △
◦}, ◦∈{+, ·}.
All the properties that we demonstrated in Theorem 1.35 hold for the rounding
♦: IR →IS and the operations in {IS, N, ♦
+ , ♦· , ♦
/ , ≤, ⊆}. In particular, the
inequality
♦(A ◦B) ⊆(♦A)♦
◦(♦B)
is valid for all A, B ∈IR. We show by means of a simple example that the strict
inclusion sign can occur. Thus there exists no homomorphism between the operations
◦in IR and ♦
◦in IS for all ◦∈{+, −, ·, /}. For example we use the ﬂoating-point
system deﬁned above.
Example 4.22. Let A := [0.36, 0.54], B := [0.35, 0.45] ∈IR. Then A + B =
[0.71, 0, 99]. By (4.5.2) we get ♦A = [0.3, 0.6], ♦B = [0.3, 0.5], and ♦(A + B) =
[0.7, 0.1 · 10]. Therefore, (♦A)♦
+ (♦B) = ♦((♦A) + (♦B)) = ♦[0.6, 1.1] =
[0.6, 0.2 · 10] and ♦(A + B) = [0.7, 0.1 · 10] ⊂[0.6, 0.2 · 10] = (♦A)♦
+ (♦B).
The different cases of interval multiplication and division represented by the Ta-
bles 4.3 and 4.4 are redundant, i.e., they are not distinct and partly overlap each other.
This is inconvenient for a computer realization of these operations. Tables 4.5 and 4.6
give a representation of interval multiplication and division where the redundancy is
eliminated.

114
4 Interval Arithmetic
0 ≤b1
b1 < 0 ≤b2
b2 < 0
0 ≤a1
[a1▽· b1, a2△· b2]
[a2▽· b1, a2△· b2]
[a2▽· b1, a1△· b2]
a1 < 0 ≤a2
[a1▽· b2, a2△· b2]
[min(a1▽· b2, a2▽· b1),
[a2▽· b1, a1△· b1]
max(a1△· b1, a2△· b2)]
a2 < 0
[a1▽· b2, a2△· b1]
[a1▽· b2, a1△· b1]
[a2▽· b2, a1△· b1]
Table 4.5. The nine cases of interval multiplication with A, B ∈IS.
0 < b1
b2 < 0
0 ≤a1
[a1▽
/ b2, a2△
/ b1]
[a2▽
/ b2, a1△
/ b1]
a1 < 0 ≤a2
[a1▽
/ b1, a2△
/ b1]
[a2▽
/ b2, a1△
/ b2]
a2 < 0
[a1▽
/ b1, a2△
/ b2]
[a2▽
/ b1, a1△
/ b2]
Table 4.6. The six cases of interval division A♦
/ B with A, B ∈IS and
0 ̸∈B.
4.6
Interval Matrices and Interval Vectors on a Screen
We begin with the following characterization of IMnS.
Theorem 4.23. Let {R, +, ·, ≤} be the completely and linearly ordered ringoid of
real numbers and {S, ≤} a symmetric screen of R. Consider the completely ordered
ringoid of matrices {MnR, +, ·, ≤} with the neutral elements O and E and the semi-
morphisms
: PMnR →IMnR and ♦: IMnR →IMnS. Then {IMnS, ♦
+ , ♦· ,
≤, ⊆} is a completely ordered ringoid with respect to ≤. The neutral elements are
[O, O] and [E, E]. With respect to ⊆, IMnS is an inclusion-isotonally ordered mono-
tone upper screen ringoid of IMnR.
Proof. Theorem 3.8 implies that MnS is a symmetric screen of MnR. By Theo-
rem 4.5, {IMnR,
+ , · , ≤} is a ringoid, and by Theorem 4.20. {IMnS, ♦
+ , ♦· , ≤, ⊆}
is an ordered ringoid with respect to ≤. By Theorem 3.5 it is an inclusion-isotonally
ordered monotone upper screen ringoid of IMnR.
■
The operations ♦
◦in IMnS are not executable on a computer. Therefore we are
now going to express these operations in terms of computer executable formulas. To
do this, let us once more consider the ringoid {MnIR,
+ , · , ≤, ⊆}. According to
Theorem 4.15, this ringoid is isomorphic to {IMnR,
+ , · , ≤, ⊆}. The isomorphism
is expressed by the equality

A,B∈MnIR
χA ◦χB = χ(A ◦B), ◦∈{+, ·}.
(4.6.1)

4.6 Interval Matrices and Interval Vectors on a Screen
115
Here the mapping χ: MnIR →IMnR is deﬁned by (4.3.1) in Section 4.3.
Using the rounding ♦: IR →IS and appealing to Theorem 3.9, a rounding
♦: MnIR →MnIS and operations in MnIS are deﬁned by

A=(Aij)∈MnIR
♦A := (♦Aij),

A,B∈MnIS
A♦
◦B := ♦(A ◦B), ◦∈{+, ·}.
For further purposes, let us denote the matrix A ◦B ∈MnIR by
(Cij) := A ◦B with Cij = [c1
ij, c2
ij] and c1
ij, c2
ij ∈R.
(4.6.2)
We are interested in the expression χ(A♦
◦B) and obtain for it
χ(A♦
◦B) = χ(♦Cij)
=
Theorem 4.19(c) χ( inf U(Cij) ∩IS).
If we exchange the componentwise inﬁma with the inﬁmum of a matrix, we obtain
χ(A♦
◦B) = χ(inf (U(Cij) ∩IS)).
Since the inclusion and the intersection in MnIR are deﬁned componentwise, we
may exchange the matrix parenthesis with the upper bounds and obtain
χ(A♦
◦B) = χ(inf U{([c1
ij, c2
ij]) ∩MnIS}),
or
χ(A♦
◦B) = χ(inf U{A ◦B ∩MnIS}).
If we now apply the mapping χ, we obtain
χ(A♦
◦B) = inf U{χ(A ◦B) ∩IMnS}
=
(4.6.1) inf U{(χA ◦χB) ∩IMnS}.
Since the rounding ♦: IMnR →IMnS has the property

A∈IMnR
♦A = inf(U(A) ∩IMnS),
we obtain
χ(A♦
◦B) = ♦(χA ◦χB).
By the deﬁnition of the operation ♦
◦in IMnS this ﬁnally leads to
χ(A♦
◦B) = χA♦
◦χB.
(4.6.3)

116
4 Interval Arithmetic
This states the fact that the mapping χ also represents an isomorphism between the
ringoid {MnIS, ♦
+ , ♦· , ≤, ⊆} deﬁned by Theorem 3.9 and the ringoid {IMnS, ♦
+ ,
♦· , ≤, ⊆} studied in Theorem 4.23.
The isomorphism (4.6.3) reduces the operations in IMnS, which are not computer
executable, to the operations in MnIS. We analyse these operations more closely.
By equation (4.3.1) in Section 4.3, operations in MnIR have been deﬁned by

A=(Aij), B=(Bij)∈MnIR
A + B := (Aij + Bij) ∧A · B :=
 n
ν=1
(Aiν · Bνj)

.
In this section operations in MnIS are deﬁned by

A=(Aij),B=(Bij)∈MnIS
A♦
+ B := ♦(A + B)
∧
A♦· B := ♦(A · B)
with the rounding ♦A := (♦Aij). This leads to the following formulas for the
operations in MnIS:
A♦
+ B = (♦(Aij + Bij)) = (Aij ♦
+ Bij),
(4.6.4)
A♦· B =

♦
n
ν=1
(Aiν · Bνj)

.
(4.6.5)
These operations are executable on a computer. The componentwise addition in
(4.6.4) can be performed by means of the addition in IS. The multiplications in (4.6.5)
are to be executed using Table 4.1. Then the lower bounds and the upper bounds are
to be added in R. Finally the rounding ♦: IR →IS has to be executed.
With Aij = [a1
ij, a2
ij], Bij = [b1
ij, b2
ij] ∈IS, (4.6.5) can be written in a more explicit
form:
A♦· B =
%
▽
n

ν=1
min
r,s=1,2(ar
iνbs
νj), △
n

ν=1
max
r,s=1,2(ar
iνbs
νj)
&
.
Here the products ar
iνbs
νj are elements of R (and in general not of S). The sum-
mands (products of double length) are to be correctly accumulated in R by the exact
scalar product. See Chapter 8. Finally the sum of products is rounded only once by
▽(resp. △) from R into S.
Since a semimorphism of a semimorphism is a semimorphism, this result can be
used not only to describe the structure on a screen but also on one or more coarser
screens. Accordingly, the passage that we made from IMnR to IMnS can also be per-
formed from IMnR to IMnD or from IMnD to IMnS. This results in isomorphisms
between the sets IMnR, IMnD, IMnS and MnIR, MnID, MnIS, respectively.
We are now going to illustrate these results. Let R again denote the linearly ordered
set of real numbers and S the subset of single precision ﬂoating-point numbers of a

4.6 Interval Matrices and Interval Vectors on a Screen
117
given computer. Figure 4.2 shows the spaces that were shown to be ringoids and
lists the theorems that proved the required properties. In particular, we display the
isomorphism between the ringoids IMnR, IMnD, IMnS and MnIR, MnID, MnIS,
respectively. We have already noted that the operations in the latter ringoids deﬁned
by Theorem 3.9 are executable on a computer.
MnIS
MnID
MnID
3.9
3.9
MnIS
IMnD
MnS
IMnS
4.5
3.9
2.5
χ
χ
χ
4.5
2.5
4.20
3.7
D
ID
2.6
2.6
3.7
4.20
S
IS
2.6
4.15
3.9
4.23
4.23
MnD
R
IR
MnIR
IMnR
MnR
PMnR
PR
Figure 4.2. Ringoids with interval matrices.
Within the sets MnID and MnIS, operations and a ringoid can also be deﬁned by
the conventional method deﬁning operations for matrices using the operations in ID
(resp. IS). We show that addition in these ringoids is identical to the addition deﬁned
by Theorem 3.9. The multiplication deﬁned by Theorem 2.6 delivers upper bounds of
the result of the multiplication deﬁned by Theorem 3.9.
Using (4.6.4) and (4.6.5), these observations can be directly veriﬁed by means of
the following formulas, wherein A = (Aij), B = (Bij) ∈MnIS:
A♦
+ B = (♦(Aij + Bij)) = (Aij ♦
+ Bij),
A♦· B =

♦
n
ν=1
Aiν · Bνj

⊆

n

ν=1
Aiν ♦· Bνj

.
(4.6.6)
The last inequality is a consequence of (R3) applied to the rounding ♦: IR →IS
and the inclusion-isotony of all operations in IS:
Aiν · Bνj ⊆♦(Aiν · Bνj) = Aiν ♦· Bνj, A + B ⊆♦(A + B) = A♦
+ B
⇒
n
ν=1
Aiν · Bνj ⊆
n

ν=1
Aiν ♦· Bνj.
The expressions on the right hand side of (4.6.6) represent the sum (resp. the prod-
uct) deﬁned in MnIS by the conventional method of the deﬁnition of operations for

118
4 Interval Arithmetic
matrices (Theorem 2.6). On computers practical calculations in MnID and MnIS
are, for simplicity, often done by use of the operations deﬁned by this method. The
multiplication deﬁned by semimorphism (4.6.5), however, provides optimal accuracy
for each component. It delivers the least upper bound of the matrix A · B in MnIS.
In Figure 4.2 double-headed arrows indicate isomorphisms, horizontal arrows de-
note semimorphisms, and vertical arrows indicate a conventional deﬁnition of the
arithmetic operations. A slanting arrow shows where the power set operations come
from. The sign
used in Figure 4.2 is intended to illustrate the inequality (4.6.6).
Interval sets, the operations of which are executable on a computer, are underlined in
Figure 4.2. The numbers on the arrows drawn in the ﬁgure indicate the theorems that
provide the corresponding properties.
We are now going to consider vectoids. Let {V, ≤} be a complete lattice, {S, ≤}
a screen of {V, ≤}, and {IS, ⊆} the set of intervals over V with bounds in S of the
form
A = [a1, a2] = {x ∈V | a1, a2 ∈S, a1 ≤x ≤a2} with a1 ≤a2.
Then IS ⊆IV and IS := IS ∪{∅} ⊆IV := IV ∪{∅}. By Theorem 4.18,
{IS, ⊆} is a screen of {IV , ⊆}. We consider the monotone upwardly directed round-
ing ♦: IV →IS, which is deﬁned by the properties
(R1)

A∈IS
♦A = A,
(R2)

A,B∈IV
(A ⊆B ⇒♦A ⊆♦B),
(R3)

A∈IV
A ⊆♦A.
The following theorem holds.
Theorem 4.24. Let {V, R, ≤} be a completely and weakly ordered vectoid, and let
{S, ≤} be a symmetric screen of {V, R, ≤}. Further, let {IV, IR, ≤, ⊆} be the vectoid
deﬁned by the semimorphism
: PV →IV . Then setting A = [a1, a2], we have:
(a) {IS, ⊆} is a symmetric screen of {IV , ⊆}, i.e., we have
(S3) [o, o], ([e, e]) ∈IS ∧

A=[a1,a2]∈IS
−A = [−a2, −a1] ∈IS.
(b) The monotone upwardly directed rounding ♦: IV →IS is antisymmetric, i.e.,
(R4)

A∈IV
♦( −A) =
−(♦A).
(c) We have

A∈IV
♦A = inf(U(A) ∩IS) = [▽a1, △a2].
(4.6.7)
(R)

4.6 Interval Matrices and Interval Vectors on a Screen
119
Proof. The proof of this theorem is completely analogous to that of Theorem 4.19.
■
Now we use the monotone upwardly directed rounding ♦: IV →IS to deﬁne
inner and outer operations ♦
◦in IS in terms of its associated semimorphism, which
by Theorem 4.6 has the following properties:
(RG)

A,B∈IS
A♦
◦B := ♦(A ◦B) = ♦(
(A ◦B))
=
(4.6.7) [▽inf(A ◦B), △sup(A ◦B)], for ◦∈{+, ·},
and with IT ⊆IR,

A∈IT

A∈IS
A♦· A := ♦(A · A) = ♦(
(A · A))
=
(4.6.7) [▽inf(A · A), △sup(A · A)].
The following theorem gives a characterization of {IS, IT, ≤, ⊆}.
Theorem 4.25. Let {V, R, ≤} be a completely and weakly ordered vectoid with the
neutral elements o and e if a multiplication exists, and let {S, ≤} be a symmetric
screen of {V, R, ≤}. Further, let {IT, ♦
+ , ♦· } be a monotone upper screen ringoid
of {IR,
+ , · } with respect to ⊆. Consider the two semimorphisms
: PV →IV
and ♦: IV →IS. Then {IS, IT, ≤, ⊆} is a weakly ordered vectoid with respect
to ≤. It is multiplicative if {V, R, ≤} is. The neutral elements are [o, o] and [e, e].
With respect to ⊆, {IS, IT, ≤, ⊆} is an inclusion-isotonally ordered monotone upper
screen vectoid of {IV, IR, ≤, ⊆}.
If {V, R, ≤} is an ordered vectoid, then {IS, IT, ≤, ⊆} is also an ordered vectoid
with respect to ≤.
Proof. The proof is an immediate consequence of Theorem 3.11.
■
A special application of this theorem is the set of interval vectors on a screen of
the linearly ordered ringoid of real numbers with outer multiplication by elements
of the ringoids {IS, ♦
+ , ♦· , ≤, ⊆} or {IMnS, ♦
+ , ♦· , ≤, ⊆}. The following corollary
describes this application.
Corollary 4.26. Let {R, +, ·, ≤} be the linearly ordered ringoid of real numbers, and
{S, ≤} a symmetric screen of R. Consider the ordered vectoids {VnR, R, ≤}, {MnR,
R, ≤}, and {VnR, MnR, ≤} as well as the semimorphisms
: PVnR →IVnR,
♦: IVnR →IVnS and the semimorphisms
: PM nR →IMnR, ♦: IMnR →
IMnS. Then {IVnS, IS, ≤, ⊆}, {IMnS, IS, ≤, ⊆}, and {IVnS, IMnS, ≤, ⊆} are or-
dered vectoids with respect to ≤. IMnS is multiplicative. With respect to ⊆, all
of these vectoids are inclusion-isotonally ordered monotone upper screen vectoids of
IVnR resp. IMnR.
■

120
4 Interval Arithmetic
The situation is similar to that which arose for matrix structures. The operations
within the vectoids {IVnS, IS}, {IMnS, IS}, {IVnS, IMnS} cannot in general be
executed on a computer since they are based on the corresponding powerset opera-
tions.
Therefore, we are now going to express these operations in terms of computer ex-
ecutable formulas. By Theorem 4.17 we saw that under the assumption that R is a
completely and linearly ordered ringoid, there exist isomorphisms between the fol-
lowing pairs of vectoids:
{VnIR, IR, ≤} ↔{IVnR, IR, ≤},
{MnIR, IR, ≤} ↔{IMnR, IR, ≤},
{VnIR, MnIR, ≤} ↔{IVnR, IMnR, ≤}.
The algebraic isomorphisms are expressed by the relations
ψ(A · a) = A · ψa,
ψ(a + b) = ψa + ψb,
χ(A · A) = A · χA,
χ(A ◦B) = χA ◦χB, ◦∈{+, ·},
ψ(A · a) = χA · ψa,
ψ(a + b) = ψa + ψb.
Here the mapping ψ : VnIR →IVnR and χ : MnIR →IMnR are deﬁned by (4.4.2)
and (4.4.3) in Section 4.4, and A ∈IR, a, b ∈VnIR, A, B ∈MnIR.
We observed above that the inner and outer operations ♦
◦in IVnS, which are de-
ﬁned by the semimorphism ♦: IVnR →IVnS, are not executable on the computer.
We now express these operations in terms of executable formulas. Using the mono-
tone upwardly directed rounding ♦: IR →IS, a rounding ♦: VnIR →VnIS (resp.
♦: MnIR →MnIS) and operations in VnIS (resp. MnIS) can be deﬁned by the
formulas

a=(Ai)∈VnIR
♦a := (♦Ai),

A=(Aij)∈MnIR
♦A := (♦Aij),

a, b∈VnIS
a♦
+ b := ♦(a + b),

A∈IS, a∈VnIS
A♦· a := ♦(A · a),

A∈IS, A∈MnIS
A♦· A := ♦(A · A),

A∈MnIS, a∈VnIS
A♦· a := ♦(A · a).

4.6 Interval Matrices and Interval Vectors on a Screen
121
As for the matrix case treated above, it can be shown that the mappings ψ and χ
establish isomorphisms between the ordered algebraic structures
{VnIS, IS, ≤} ↔{IVnS, IS, ≤},
{MnIS, IS, ≤} ↔{IMnS, IS, ≤},
{VnIS, MnIS, ≤} ↔{IVnS, IMnS, ≤}.
With a = (Ai), b = (Bi), and A = (Aij), we obtain the following relations for the
operations deﬁned above:
a♦
+ b = ♦(a + b) = ♦(Ai + Bi) = (Ai ♦
+ Bi),
A♦· a = ♦(A · a) = ♦(A · Ai) = (A♦· Ai),
A♦· A = ♦(A · A) = ♦(A · Aij) = (A♦· Aij),
A♦· a = ♦(A · a) = ♦
 n
ν=1
Aiν · Aν

=

♦
n
ν=1
Aiν · Aν

.
All these operations furnish the smallest appropriate interval, i.e., they are of opti-
mal accuracy. Moreover, they all can be executed on a computer. The formulas also
show that the operations a♦
+ b, A♦· a, and A♦· A are identical, whether deﬁned by the
conventional method or by semimorphism. However, in the case of {VnIS, MnIS, ≤}
for the outer multiplication the two deﬁnitions lead to the inequality
A♦· a =

♦
n
ν=1
Aiν · Aν

⊆

n

ν=1
Aiν ♦· Aν

.
(4.6.8)
Both sides of the last inequality are computer-executable formulas. The expression
on the left hand side is of optimal accuracy. It can be executed using Table 4.1 and the
exact scalar product. See Chapter 8. In practice however, calculations on computers
are for simplicity often performed using the operations deﬁned by the conventional
deﬁnition of the operations, which occurs on the right-hand side of (4.6.8).
Since a semimorphism of a semimorphism is a semimorphism, these results can be
used not only to describe the structure on a screen but also on one or more coarser
screens as well.
Figure 4.3 shows a diagram displaying the interval vectoids discussed in this chap-
ter. In Figure 4.3 arrows are used as they are in the diagram of Figure 4.2. Double
arrows indicate isomorphisms. Horizontal arrows denote semimorphisms. Vertical
arrows indicate a conventional deﬁnition of the arithmetic operations. The numbers
on the arrows drawn in the ﬁgure indicate the theorems that provide the corresponding
properties. The sign
used in Figure 4.3 indicates the inequality (4.6.8). All inter-
val structures, the operations of which are executable on a computer, are underlined
in Figure 4.3.

122
4 Interval Arithmetic
{VnIR, IR}
3.11
3.11
{VnD, D}
{VnR, R}
3.11
3.11
{VnS, S}
2.11
{PVnR, PR}
4.7
{IVnD, ID}
{IVnR, IR}
4.25
4.25
{IVnS, IS}
IR
4.20
4.20
PR
4.5
{VnD, MnD}
{VnR, MnR}
{VnS, MnS}
2.11
{PVnR, PMnR}
{IVnD, IMnD}
{IVnR, IMnR}
{IVnS, IMnS}
4.7
4.25
4.25
3.11
3.11
{VnID, MnID}
2.15
{VnIR, MnIR} 3.11
3.11
{VnIS, MnIS}
4.15
4.15
4.15
2.15
2.13
2.13
4.15
4.15
4.15
{VnID, MnID}
2.15
{VnIS, MnIS}
2.13
ID
IS
{VnID, ID}
{VnIS, IS}
Figure 4.3. Interval vectoids.
4.7
Complex Interval Arithmetic
To complete our discussion of all spaces displayed in Figure 1, it remains to consider
the complex interval spaces, which we now proceed to do.
Let {R, N, +, ·, /, ≤} be a completely and linearly ordered division ringoid with
the neutral elements o and e, and let CR := {(x, y)
|
x, y ∈R} be the set of
pairs over R. If in CR we deﬁne equality, the order relation ≤, and the operations
◦∈{+, −, ·, /} by means of the usual formulas, we know by Theorem 2.7 that
{CR, N, +, ·, /, ≤} with N := {γ ∈CR | γ = (c1, c2) ∧c1c1 + c2c2 ∈N} is
a completely and weakly ordered division ringoid with the neutral elements (o, o) and
(e, o). Then by Theorem 2.5, the power set {PCR\{∅}, !
N, +, ·, /} with !
N := {Φ ∈
PCR | Φ ∩N ̸= ∅} is also a division ringoid.
Now let ICR denote the set of intervals over CR of the form
Φ = [ϕ1, ϕ2] = {ϕ ∈CR | ϕ1, ϕ2 ∈CR, ϕ1 ≤ϕ ≤ϕ2} with ϕ1 ≤ϕ2.
Then by Theorems 4.2 and 4.4, ICR := ICR ∪{∅} is a symmetric upper screen
of PCR and the monotone upwardly directed rounding
: PCR →ICR is antisym-
metric. In ICR we deﬁne operations by employing the upwardly directed rounding

4.7 Complex Interval Arithmetic
123
as a semimorphism:
(RG)

Φ,Ψ∈ICR
Φ ◦Ψ :=
(Φ ◦Ψ) =
{ϕ ◦ψ | ϕ ∈Φ ∧ψ ∈Ψ},
with ◦∈{+, ·, /},
where for division, we assume that Ψ ∩!
N = ∅. For intervals Φ = [ϕ1, ϕ2] and
Ψ = [ψ1, ψ2] ∈ICR, we further deﬁne an order relation
Φ ≤Ψ :⇔(ϕ1 ≤ψ1 ∧ϕ2 ≤ψ2).
Then by Theorem 4.5 and under its hypothesis, {ICR, N∗,
+ , · ,
/ , ≤, ⊆} with
N∗:= {Φ ∈ICR | Φ∩!
N ̸= ∅} is a completely and weakly ordered division ringoid
with respect to ≤. With respect to ⊆it is an inclusion-isotonally ordered monotone
upper screen division ringoid of PCR.
The operations in ICR are deﬁned by those in PCR, and are therefore not exe-
cutable in practice. To obtain executable formulas, we are going to study the connec-
tion between these operations and those in the set CIR.
We start once more with the completely and linearly ordered division ringoid
{R, N, +, ·, /, ≤}. We assume additionally that x = −e is already unique in R by
(D5a) alone. Then by Theorem 4.5, {IR, N ′,
+ , · ,
/ , ≤, ⊆} is a completely or-
dered division ringoid. Theorems 4.5 and 4.10 prescribe explicit and performable
formulas for all operations in IR. If A = [a1, a2] and B = [b1, b2], these formulas are
summarized in Corollary 4.11 by the relation

A,B∈IR
A ◦B :=
(A ◦B) = [ min
i,j=1,2(ai ◦bj), max
i,j=1,2(ai ◦bj)],
◦∈{+, −, ·, /}.
Now consider the set CIR := {(X, Y ) | X, Y ∈IR}. For elements Φ = (X1, X2),
Ψ = (Y1, Y2) we deﬁne equality =, the order relation ≤, and the arithmetic operations
◦, ◦∈{+, ·, /}, as in Theorem 2.7 by
Φ = Ψ :⇔X1 = Y1 ∧X2 = Y2,
Φ ≤Ψ :⇔X1 ≤Y1 ∧X2 ≤Y2,
Φ + Ψ := (X1 + Y1, X2 + Y2),
Φ · Ψ := (X1 · Y1 −X2 · Y2, X1 · Y2 + X2 · Y1),
Φ / Ψ :=

X1 · Y1 + X2 · Y2
Y1 · Y1 + Y2 · Y2
, X2 · Y1 −X1 · Y2
Y1 · Y1 + Y2 · Y2

for Ψ ∈CIR \ N′′,
with N′′ := {Φ ∈CIR | Φ = (X1, X2) ∧X1 · X1 + X2 · X2 ∈N′}. Division here
means division in IR.

124
4 Interval Arithmetic
Thus all of these operations
◦are explicitly representable in terms of the opera-
tions
◦in IR. By Theorem 2.7, {CIR, N′′,
+ , · ,
/ , ≤} is a completely and weakly
ordered division ringoid.
We are interested in relating the operations in ICR and CIR. Thus we consider the
mapping τ : CIR →ICR with τ([x1, x2], [y1, y2]) = [(x1, y1), (x2, y2)].
Here τ obviously is a one-to-one mapping of CIR onto ICR and an order isomor-
phism.
We show that τ also is a ringoid isomorphism.
To do this, we prove that ad-
dition and multiplication in ICR and CIR are isomorphic.
Let Φ = (X1, X2),
Ψ = (Y1, Y2) ∈CIR. Then
τΦ + τΨ :=
(τΦ + τΨ)
=
%
inf
ϕ∈τΦ,ψ∈τΨ(ϕ + ψ),
sup
ϕ∈τΦ,ψ∈τΨ
(ϕ + ψ)
&
= [inf(x1 + y1, x2 + y2), sup(x1 + y1, x2 + y2)]
= [(inf(x1 + y1), inf(x2 + y2)), (sup(x1 + y1), sup(x2 + y2))],
where in the last two lines displayed here, the inﬁma and the suprema are taken over
xi ∈Xi, yi ∈Yi, i = 1, 2. On the other hand, we have
Φ + Ψ := (X1 + Y1, X2 + Y2)
= (
(X1 + Y1),
(X2 + Y2))
= ([inf(x1 + y1), sup(x1 + y1)], [inf(x2 + y2), sup(x2 + y2)]),
where in the last line displayed here, the inﬁma and the suprema are taken over x1 ∈
X1, y1 ∈Y1 or over x2 ∈X2, y2 ∈Y2, as the case may be. These formulas show that
τΦ + τΨ = τ(Φ + Ψ),
which demonstrates the isomorphism for addition.
For multiplication we have

ϕ=(x1,x2)∈τΦ

ψ=(y1,y2)∈τΨ
ϕ · ψ = (x1y1 −x2y2, x1y2 + x2y1)
∈τ(X1 · Y1 −X2 · Y2, X1 · Y2 + X2 · Y1)
= τ(Φ · Ψ),
and therefore τΦ · τΨ ⊆τ(Φ · Ψ). If we apply the monotone upwardly directed
rounding
: PCR →ICR to this inequality, we obtain
τΦ · τΨ ⊆
(R3)
(τΦ · τΨ) =
(RG) τΦ · τΨ
⊆
(R1,2) τ(Φ · Ψ).

4.7 Complex Interval Arithmetic
125
On the other hand, by employing the explicit formulas for the operations in the
ringoid IR, we obtain with Xi = [x1
i, x2
i ], Yi = [y1
i , y2
i ], i = 1, 2:
X1 · Y1 −X2 · Y2 =
'
min
i,j=1,2(xi
1yj
1), max
i,j=1,2(xi
1yj
1)
(
−
'
min
i,j=1,2(xi
2yj
2), max
i,j=1,2(xi
2yj
2)
(
=
'
min
i,j=1,2(xi
1yj
1) −max
i,j=1,2(xi
2yj
2), max
i,j=1,2(xi
1yj
1) −min
i,j=1,2(xi
2yj
2)
(
,
X1 · Y2 + X2 · Y1 =
'
min
i,j=1,2(xi
1yj
2), max
i,j=1,2(xi
1yj
2)
(
+
'
min
i,j=1,2(xi
2yj
1), max
i,j=1,2(xi
2yj
1)
(
=
'
min
i,j=1,2(xi
1yj
2) + min
i,j=1,2(xi
2yj
1), max
i,j=1,2(xi
1yj
2) + max
i,j=1,2(xi
2yj
1)
(
,
i.e., the bounds of τ(Φ · Ψ) consist of inner points of
(τΦ · τΨ) = τΦ · τΨ
or
τ(Φ · Ψ) ⊆
(τΦ · τΨ) = τΦ · τΨ.
If we take both inequalities together, we obtain
τ(Φ · Ψ) = τΦ · τΨ,
which demonstrates the ringoid isomorphism.
For division, we similarly obtain
τΦ/τΨ ⊆
(τΦ/τΨ) = τΦ / τΨ ⊆τ(Φ / Ψ).
Equality, however, is not provable. We show this by means of a simple example.
Let {R, {0}, +, ·, /, ≤} be the division ringoid of real numbers. Then with Φ =
(X1, X2) = ([0, 0], [2, 4]) and Ψ = (Y1, Y2) = ([1, 2], [0, 0]) we obtain with xi ∈Xi,
yi ∈Yi, i = 1, 2,
τΦ / τΨ =
(τΦ/τΨ)
=
'
inf x1y1 + x2y2
y1y1 + y2y2
, inf x2y1 −x1y2
y1y1 + y2y2

,

sup x1y1 + x2y2
y1y1 + y2y2
, sup x2y1 −x1y2
y1y1 + y2y2
(
= [(0, 1
2), (0, 4)],
Φ / Ψ :=

X1 · Y1 + X2 · Y2
Y1 · Y1 + Y2 · Y2
, X2 · Y1 −X1 · Y2
Y1 · Y1 + Y2 · Y2

= ([0, 0] / [1, 4], [2, 8] / [1, 4]) = ([0, 0], [1/2, 8]).

126
4 Interval Arithmetic
This result is not at all surprising. It is well known from interval analysis. In
the complex quotient, the quantities y1 and y2 occur more than once. The expres-
sion τ(Φ / Ψ), therefore, only delivers an overestimate of the range of the complex
function ϕ/ψ. In interval analysis, therefore, other formulas are occasionally used to
compute the complex quotient [152].
We illustrate the ringoid isomorphism derived above in a diagram. See Figure 4.4.
IR
PR
R
CR
PCR
ICR
CIR
τ
Figure 4.4. Illustration of the ringoid isomorphism τ.
Since the ringoids CIR and ICR are isomorphic, we may identify corresponding
elements with each other. This allows us to deﬁne an inclusion for elements Φ =
(X1, X2), Ψ = (Y1, Y2) ∈CIR as
Φ ⊆Ψ :⇔X1 ⊆Y1 ∧X2 ⊆Y2
and
ϕ = (x1, x2) ∈Φ = (X1, X2) :⇔x1 ∈X1 ∧x2 ∈X2.
These deﬁnitions permit the interpretation that an element Φ = (X1, X2) ∈CIR
represents a set of pairs of elements of CR by means of the identity
Φ = (X1, X2) ≡{(x1, x2) | x1 ∈X1, x2 ∈X2}.
Both sides contain the same elements.
We now consider the complex interval spaces on a screen. The following theorem
describes the structure of such a screen.
Theorem 4.27. Let R be the linearly ordered ringoid of real numbers with the neutral
elements 0 and 1, and let {S, ≤} be a symmetric screen of R. Consider the complex
division ringoid {C, {(0, 0)}, +, ·, /, ≤} as well as the semimorphisms
: PC →IC
and ♦: IC →ICS. Then {ICS, N, ♦
+ , ♦· , ♦
/ , ≤, ⊆} with N := {A ∈ICS |
(0, 0) ∈A} is a weakly ordered division ringoid with respect to ≤. The neutral
elements are [(0, 0), (0, 0)] and [(1, 0), (1, 0)]. With respect to ⊆, ICS is an inclusion-
isotonally ordered monotone upper screen division ringoid of IC.
Proof. We know by Theorem 3.8 that CS is a symmetric screen of C. By Theo-
rem 4.5 {IC,
+ , · } is a ringoid, and by Theorem 4.20 {ICS, N, ♦
+ , ♦· , ♦
/ , ≤, ⊆}
is a weakly ordered as well as an inclusion-isotonally ordered division ringoid.
■

4.7 Complex Interval Arithmetic
127
The operations ♦
◦in ICS are not directly implementable on a computer. There-
fore, we express them in terms of implementable formulas. Consider once more the
isomorphism τ : CIR →IC. It is expressed by the formula

Φ,Ψ∈CIR
τΦ ◦τΨ = τ(Φ ◦Ψ), ◦∈{+, ·}.
(4.7.1)
Using the rounding ♦: IR →IS and using Theorem 3.10, a rounding ♦: CIR →
CIS and operations in CIS are deﬁned by the semimorphism

Φ=(X1,X2)∈CIR
♦Φ := (♦X1, ♦X2),

Φ,Ψ∈CIS
Φ♦
◦Ψ := ♦(Φ ◦Ψ), ◦∈{+, ·}.
Now we show in a complete analogy to the matrix case, which we considered
above, that the operations in ICS and CIS are isomorphic. Denoting (Z1, Z2) :=
Φ ◦Ψ, we consider the expression τ(Φ♦
◦Ψ), and we obtain
τ(Φ♦
◦Ψ) = τ(♦(Z1, Z2)) = τ(♦Z1, ♦Z2)
= τ(inf(U(Z1) ∩IS), inf(U(Z2) ∩IS))
= τ(inf(U(Z1) ∩IS, U(Z2) ∩IS)).
Since the inclusion and intersection in CIS are deﬁned componentwise, we may
exchange the upper bound and the pair brackets to obtain
τ(Φ♦
◦Ψ) = τ(inf(U((Z1, Z2) ∩CIS))).
If we now effect the mapping τ, we obtain
τ(Φ♦
◦Ψ) = inf(U(τ(Φ ◦Ψ) ∩ICS))
=
(4.7.1) inf(U((τΦ ◦τΨ) ∩ICS))
=
Theorem 4.19(c) ♦(τΦ ◦τΨ).
By the deﬁnition of the operations ♦
◦in ICS, this ﬁnally leads to
τ(Φ♦
◦Ψ) = τΦ♦
◦τΨ.
(4.7.2)
Thus the mapping τ represents an isomorphism between the ringoids {CIS, ♦
+ ,
♦· , ≤, ⊆} deﬁned by Theorem 3.10 and {ICS,♦
+,♦· , ≤, ⊆} studied in Theorem 4.27.

128
4 Interval Arithmetic
The isomorphism reduces the operations in ICS, which are not executable in prac-
tice, to the operations in CIS that have the properties
Φ♦
+ Ψ = (X1 ♦
+ Y1, X2 ♦
+ Y2),
Φ♦· Ψ = (♦(X1 · Y1 −X2 · Y2), ♦(X1 · Y2 + X2 · Y1)).
These operations are implementable on a computer. Addition can be performed
componentwise in terms of addition in IS. Multiplication can be performed using
Table 4.1 and the exact scalar product. See Chapter 8.
The result of these considerations concerning complex intervals is illustrated in
Figure 4.5. The latter is completely analogous to Figure 4.2. We emphasize, however,
that the isomorphism shown in Figure 4.5 does not include division.
CIS
CID
CIS
ICD
CS
ICS
4.5
2.5
τ
4.5
2.5
4.20
3.7
D
ID
2.7
2.7
3.7
4.20
S
IS
2.7
CD
τ
τ
CID
3.10
3.10
4.27
4.27
3.10
3.10
CR
ICR
PCR
CIR
IR
PR
R
Figure 4.5. Complex interval arithmetic.
For reasons of simplicity, calculations on a computer are often performed by the
conventional method of the deﬁnition of the arithmetic in the ringoids CID and CIS
(Theorem 2.7). Multiplication in these ringoids leads to upper bounds for multiplica-
tion deﬁned in the same sets by means of Theorem 3.10. This can be seen by means
of the formula
Φ♦· Ψ = (♦(X1 · Y1 −X2 · Y2), ♦(X1 · Y2 + X2 · Y1))
⊆((X1♦· Y1 ♦
−X2 ♦· Y2), (X1 ♦· Y2 ♦
+ X2 ♦· Y1)).
The last inequality is a consequence of (R3) for the rounding ♦: IR →IS and the
inclusion-isotonality of addition and subtraction in IS.

4.8 Complex Interval Matrices and Interval Vectors
129
4.8
Complex Interval Matrices and Interval Vectors
We now consider complex interval matrices. Again, let R be the linearly ordered
ringoid of real numbers, C its complexiﬁcation, and {IC,
+ , · , ≤} the completely
and weakly ordered ringoid of intervals over C (Theorem 4.5). By Theorem 2.6, the
set of n × n-matrices {MnC, +, ·, ≤} also forms a completely and weakly ordered
ringoid.
We consider the ringoid {PMnC, +, ·} corresponding to the power set of MnC
(Theorem 2.5). In the subset of intervals IMnC ⊂PMnC we deﬁne operations
◦,
◦∈{+, ·}, by employing the semimorphism
: PMnC →IMnC := IMnC ∪{∅},

A,B∈IMnC
A ◦B :=
(A ◦B) = [inf(A ◦B), sup(A ◦B)].
Then using Theorem 4.5, we see that {IMnC,
+ , · , ≤} is also a completely and
weakly ordered ringoid.
Independently of the set IMnC, we now consider the set MnIC, i.e., the set of n ×
n-matrices the components of which are intervals over C. Employing the operations
in the ringoid {IC,
+ , · , ≤, ⊆}, we deﬁne operations and an order relation in MnIC
by means of Theorem 2.6. Then {MnIC,
+ , · , ≤} is seen to be a completely and
weakly ordered ringoid.
We consider the question of whether there exists any relation between the com-
pletely and weakly ordered ringoids {MnIC,
+ , · , ≤} and {IMnC,
+ , · , ≤}. The
assertion of Lemma 4.13 is that both of these ringoids are isomorphic with respect to
the algebraic and the order structure if the following formula relating the operations
in PC and IC holds:

Aν,Bν∈IC
 n

ν=1
Aν · Bν ⊆
 n

ν=1
Aν · Bν

.
(4.8.1)
Here
: PC →IC := IC ∪{∅} denotes the monotone upwardly directed
rounding. We are now going to show that this formula is valid in the present setting.
Because of the isomorphism between the ringoids CIR and IC, which we noted
above (see Figure 4.5), we simply identify certain elements of CIR and IC within the
following proof. These elements are the ones that correspond to each other through
the mapping
τ : CIR →IC with τ([x1, x2], [y1, y2]) = [(x1, y1), (x2, y2)].
Let Ai, Bi ∈IC. Using the isomorphism just referred to, Ai ≡(Ai1, Ai2) and
Bi ≡(Bi1, Bi2), where Ai1, Ai2, Bi1, Bi2 ∈IR. Then by the deﬁnition of the addi-
tion in PC we know that every x ∈n
i=1 Ai · Bi is of the form x = n
i=1 xi with

130
4 Interval Arithmetic
xi ∈Ai · Bi and
Ai · Bi ≡(Ai1 · Bi1 −Ai2 · Bi2, Ai1 · Bi2 + Ai2 · Bi1).
If we now also separate xi = (xi1, xi2) into real and imaginary parts, we obtain

i=1(1)n
(xi1 ∈Ai1 · Bi1 −Ai2 · Bi2
∧
xi2 ∈Ai1 · Bi2 + Ai2 · Bi1)
and
x =
 n

i=1
xi1,
n

i=1
xi2

.
Because of Corollary 4.11, we now get with Aij = [a1
ij, a2
ij] and Bij = [b1
ij, b2
ij],
i = 1(1)n, j = 1, 2, and using the following abbreviations αi, βi, γi, δi for all i =
1(1)n , that
αi := min
r,s=1,2(ar
i1 · bs
i1) −max
r,s=1,2(ar
i2 · bs
i2)
≤xi1 ≤
max
r,s=1,2(ar
i1 · bs
i1) −min
r,s=1,2(ar
i2 · bs
i2) =: βi,
γi := min
r,s=1,2(ar
i1 · bs
i2) + min
r,s=1,2(ar
i2 · bs
i1)
≤xi2 ≤
max
r,s=1,2(ar
i1 · bs
i2) + max
r,s=1,2(ar
i2 · bs
i1) =: δi
⇒
(OD1)R
n

i=1
αi ≤
n

i=1
xi1 ≤
n

i=1
βi
∧
n

i=1
γi ≤
n

i=1
xi2 ≤
n

i=1
δi,
and
x =
 n

i=1
xi1,
n

i=1
xi2

,
i.e., the bounds that we have just established for x consist of points in the set repre-
sented by the sum appearing in the expression
 n

i=1
Ai · Bi

.
This proves (4.8.1).
Then by Lemma 4.13, the two completely and weakly ordered ringoids {IMnC,
+ ,
· , ≤} and {MnIC,
+ , · , ≤} are isomorphic with respect to the algebraic and the
order structure. Figure 4.6 illustrates this isomorphism by means of a scheme. By this

4.8 Complex Interval Matrices and Interval Vectors
131
isomorphism the operations
◦, ◦∈{+, −, ·}, in IMnC, which are not implementable
in terms of their deﬁnition, are reduced to the operations in MnIC. Because of the
isomorphism τ : IC →CIR, these latter operations are identical to those in the
ringoid MnCIR. These operations in turn are deﬁned and can easily be implemented
in terms of those in CIR.
IC
PC
C
MnC
PMnC
IMnC
MnIC
Figure 4.6. Illustration of the isomorphism MnIC ↔IMnC.
We now consider complex interval matrices on a screen, and we begin with the
following theorem that characterizes such a structure.
Theorem 4.28. Let R be the linearly ordered ringoid of real numbers and let {S, ≤}
be a symmetric screen of R. Consider the complex ringoids {C, +, ·, ≤} and {MnC,
+, ·, ≤} and the semimorphisms
: PM nC →IMnC and ♦:IMnC →IMnCS.
Then {IMnCS, ♦
+ , ♦· , ≤, ⊆} is a completely and weakly ordered ringoid with respect
to ≤. With respect to ⊆, IMnCS is an inclusion-isotonally ordered monotone upper
screen ringoid of IMnC.
Proof. The proof of this theorem is completely analogous to the proofs of Theo-
rems 4.23 and 4.27.
■
Now in a manner similar to the discussion following Theorems 4.23 and 4.27, the
usual isomorphisms can be studied. Figure 4.7 gives the resulting diagram.
The case of complex vectoids can be presented as the following corollary of Theo-
rem 4.25.
Corollary 4.29. Let R be the linearly ordered ringoid of real numbers and {S, ≤} a
symmetric screen of R. Consider the complex ringoids {C, +, ·, ≤}, {MnC, +, ·, ≤}
and the weakly ordered vectoids {VnC, C, ≤}, {MnC, C, ≤}, and {VnC, MnC, ≤}
as well as the semimorphisms
: PVnC →IVnC, ♦: IVnC →IVnCS, and
: PM nC →IMnC, ♦: IMnC →IMnCS. Then the structures {IVnCS, ICS,
≤, ⊆}, {IMnCS, ICS, ≤, ⊆}, and {IVnCS, IMnCS, ≤, ⊆} are completely and
weakly ordered vectoids with respect to ≤. {IMnCS, ICS} is multiplicative. With
respect to ⊆, all of these vectoids are inclusion-isotonally ordered and monotone

132
4 Interval Arithmetic
2.6
MnCIR
CID
τ
CIR
τ
CIS
τ
3.10
2.6
MnCIS
MnCIS
2.6
MnCID
MnCID
3.9
3.10
3.9
MnIC
MnICD
3.9
MnICS
3.9
MnCD
2.5
PMnC
IMnC
MnC
MnCS
3.9
IMnCD
IMnCS
4.5
3.9
CD
2.5
PC
4.5
IC
C
3.10
3.10
CS
ICD
4.27
4.27
ICS
4.28
4.28
Figure 4.7. Complex interval matrices.
upper screen vectoids of {IVnC, IC}, {IMnC, IC}, and {IVnC, IMnC}, respec-
tively.
■
The operations in the interval vectoids occurring in this corollary are not imple-
mentable by using their deﬁnition. Under our assumptions, however, we may apply
the isomorphisms expressed by Theorem 4.16, and in addition, we make use of the
isomorphisms IC ↔CIR and IMnC ↔MnCIR. Summarizing, we obtain the
following isomorphisms
{IVnC, IC, ≤, ⊆} ↔{VnIC, IC, ≤, ⊆}
↔{VnCIR, CIR, ≤, ⊆},
{IMnC, IC, ≤, ⊆} ↔{MnIC, IC, ≤, ⊆}
↔{MnCIR, CIR, ≤, ⊆},
{IVnC, IMnC, ≤, ⊆} ↔{VnIC, MnIC, ≤, ⊆}
↔{VnCIR, MnCIR, ≤, ⊆}.
Now it can be shown in complete analogy to the considerations following Theo-
rems 4.23 and 4.27 that these isomorphisms also induce isomorphisms between the

4.8 Complex Interval Matrices and Interval Vectors
133
corresponding spaces over D and S, for instance
{IVnCS, ICS, ≤, ⊆} ↔{VnICS, ICS, ≤, ⊆}
↔{VnCIS, CIS, ≤, ⊆},
{IMnCS, ICS, ≤, ⊆} ↔{MnICS, ICS, ≤, ⊆}
↔{MnCIS, CIS, ≤, ⊆},
{IVnCS, IMnCS, ≤, ⊆} ↔{VnICS, MnICS, ≤, ⊆}
↔{VnCIS, MnCIS, ≤, ⊆}.
Figure 4.8 summarizes all these results. As in the similar ﬁgures displayed before,
double arrows indicate isomorphisms, horizontal arrows denote semimorphisms, and
vertical arrows indicate a conventional deﬁnition of the arithmetic operations. All
operations deﬁned by semimorphisms within vectoids ending with CID or CIS can
be implemented on a computer by the exact scalar product (Chapter 8). Upper bounds
for these operations can be obtained if within the same sets operations are also deﬁned
by the conventional method (Figure 4.8).
{VnIC, IC}
{VnC, C}
2.7
4.5
{IVnC, IC}
4.8
{VnCIR, CIR}
3.7
3.7
4.19
IC
PC
4.3
2.11
3.6
CI
2.9
2.9
{VnCIR, MnCIR}
2.11
{VnICD, ICD}
{VnCD, CD}
3.7
{IVnCD, ICD}
4.19
3.7
{VnCID, CID}
2.11
2.9
3.6
{VnCIS, CIS}
{VnCIS, MnCIS}
{VnICS, ICS}
{VnCS, CS}
{IVnCS, ICS}
2.11
2.9
{VnCID, CID}
{VnCIS, CIS}
{VnC, MnC}
2.7
{IVnC, IMnC}
4.5
4.9
{VnCD, MnCD}
{IVnCD, IMnCD}
{IVnCS, IMnCS}
{VnCS, MnCS}
3.7
{VnICD, MnICD}
{VnICS, MnICS}
4.19
{VnIC, MnIC}
4.19
3.7
{VnCID, MnCID}
3.7
3.7
{VnCID, MnCID}
{VnCIS, MnCIS}
{PVnC, PC}
{PVnC, PMnC}
CID
CIS
Figure 4.8. Complex interval vectoids.

134
4 Interval Arithmetic
4.9
Extended Interval Arithmetic
In an ordered or weakly ordered division ringoid R interval operations have been
deﬁned by (RG) (compare Section 4.1):
(RG)

A,B∈IR
A ◦B :=
(A ◦B) = [infa∈A,b∈B(a ◦b), supa∈A,b∈B(a ◦b)].
Here
: PR →IR is the monotone upwardly directed rounding from the power
set into the subset of intervals IR. In the case of division it was explicitly assumed
that 0 /∈B. Under this assumption the result is always an interval of IR. In particular,
for real numbers R the rounding has no effect. Remark 4.12 states that for intervals
A, B ∈IR the result of the power set operation A ◦B is already an interval of IR.
We now retract the assumption 0 /∈B for division. This is motivated by fundamen-
tal applications of interval mathematics. Zero ﬁnding is a central task of mathematics.
In conventional numerical analysis Newton’s method is the key method for computing
zeros of nonlinear functions. For a brief sketch of the method see Chapter 9. It is well
known that under certain natural assumptions on the function the method converges
quadratically to the solution if the initial value of the iteration is already close enough
to the zero. However, Newton’s method may well fail to ﬁnd the solution in ﬁnite as
well as in inﬁnite precision arithmetic. This may happen, for instance, if the initial
value of the iteration is not close enough to the solution.
In contrast to this the interval version of Newton’s method never fails, not even in
rounded arithmetic. It is globally convergent.
It is one of the main achievements of interval mathematics that the interval version
of Newton’s method has been extended so that it can be used to enclose all (single)
zeros of a function in a given domain. Newton’s method reaches its ﬁnal elegance and
strength in the form of the extended interval Newton method (Chapter 9). The method
is locally quadratically convergent and it never fails, not even in rounded arithmetic.
The key operation to achieve these fascinating properties of Newton’s method is
division by an interval which contains zero. We will now develop this operation,
which the extended interval Newton method uses to separate different solutions.
The power set PR over the real numbers R is deﬁned as the set of all subsets
of real numbers. With respect to set inclusion as an order relation {PR, ⊆} is an
ordered set and a complete lattice. The least element of {PR, ⊆} is the empty set.
The greatest element of {PR, ⊆} is the set R. The inﬁmum of two elements of PR is
the intersection and the supremum is the union.
The operations of the real numbers R have been extended to the power set PR by

A,B∈PR
A ◦B := {a ◦b | a ∈A ∧b ∈B}, for all ◦∈{+, −, ·, /}.
(4.9.1)

4.9 Extended Interval Arithmetic
135
The following properties are obvious and immediate consequences of this deﬁni-
tion:
A ⊆B ∧C ⊆D ⇒A ◦C ⊆B ◦D, for all A, B, C, D ∈PR,
(4.9.2)
and in particular
a ∈A ∧b ∈B ⇒a ◦b ∈A ◦B, for all A, B ∈PR.
(4.9.3)
Property (4.9.2) is called inclusion-isotony (or inclusion-monotonicity).
Property
(4.9.3) is called the inclusion property.
By use of parentheses these rules can immediately be extended to expressions with
more than one arithmetic operation, e.g.,
A ⊆B ∧C ⊆D ∧E ⊆F ⇒A ◦C ⊆B ◦D ⇒(A ◦C) ◦E ⊆(B ◦D) ◦F,
and so on. Moreover, if more than one operation is deﬁned then this chain of con-
clusions also remains valid for expressions containing several different operations. In
particular all these properties, i.e., the inclusion-monotonicity and inclusion proper-
ties, also hold if in (4.9.1) PR is restricted to operands of IR.
The set IR of closed and bounded intervals over R is a particular subset of PR.
With respect to set inclusion as an order relation IR again is an ordered set. Lattice
operations are also available in the set IR := IR ∪{∅}. The inﬁmum of two ele-
ments of IR is the intersection and the supremum is the interval (convex) hull. In IR
arithmetic operations are deﬁned by (4.9.1). If division by an interval that includes
zero is excluded the result is again an element of IR. It can be computed using sim-
ple formulas. Under the assumption 0 ̸∈B for division, the intervals of IR are an
algebraically closed subset4 of the power set PR, i.e., an operation for intervals of IR
performed in PR always delivers an interval of IR.
In accordance with (4.9.1) division in IR is deﬁned by

A,B∈IR
A/B := {a/b | a ∈A ∧b ∈B}.
(4.9.4)
The quotient a/b is deﬁned as the inverse operation of multiplication, i.e., as the
solution of the equation b · x = a. Thus (4.9.4) can be written in the form

A,B∈IR
A/B := {x | bx = a ∧a ∈A ∧b ∈B}.
(4.9.5)
For 0 /∈B (4.9.4) and (4.9.5) are equivalent. While in R division by zero is not
deﬁned the representation of A/B by (4.9.5) allows deﬁnition of the operation and
interpretation of the result for 0 ∈B also.
4as the integers are of the real numbers

136
4 Interval Arithmetic
By way of interpreting (4.9.5) for A = [a1, a2] and B = [b1, b2] ∈IR with 0 ∈B
the following eight distinct cases can be set out:
case
A = [a1, a2]
B = [b1, b2]
1
0 ∈A,
0 ∈B.
2
0 /∈A,
B = [0, 0].
3
a1 ≤a2 < 0,
b1 < b2 = 0.
4
a1 ≤a2 < 0,
b1 < 0 < b2.
5
a1 ≤a2 < 0,
0 = b1 < b2.
6
0 < a1 ≤a2,
b1 < b2 = 0.
7
0 < a1 ≤a2,
b1 < 0 < b2.
8
0 < a1 ≤a2,
0 = b1 < b2.
Table 4.7. The eight cases of interval division A/B, with A, B ∈IR, and
0 ∈B.
The list distinguishes the cases 0 ∈A (case 1) and 0 /∈A (cases 2 to 8). Since it is
generally assumed that 0 ∈B, these eight cases indeed cover all possibilities.
We now derive simple formulas for the result of the interval division A/B for these
eight cases:
(a) 0 ∈A ∧0 ∈B. Since every x ∈R fulﬁls the equation 0 · x = 0, we have
A/B = (−∞, +∞). Here (−∞, +∞) denotes the open interval between −∞
and +∞which just consists of all real numbers R, i.e., A/B = R.
(b) In the case 0 /∈A ∧B = [0, 0] the set deﬁned by (4.9.5) consists of all elements
which fulﬁl the equation 0 · x = a for a ∈A. Since 0 /∈A, there is no real
number which fulﬁls this equation. Thus A/B is the empty set, i.e., A/B = ∅.
case
A = [a1, a2]
B = [b1, b2]
B′
A/B′
A/B
1
0 ∈A
0 ∈B
(−∞, +∞)
2
0 /∈A
B = [0, 0]
∅
3
a2 < 0
b1 < b2 = 0
[b1, (−ϵ)]
[a2/b1, a1/(−ϵ)]
[a2/b1, +∞)
4
a2 < 0
b1 < 0 < b2
[b1, (−ϵ)]
[a2/b1, a1/(−ϵ)]
(−∞, a2/b2]
∪[ϵ, b2]
∪[a1/ϵ, a2/b2]
∪[a2/b1, +∞)
5
a2 < 0
0 = b1 < b2
[ϵ, b2]
[a1/ϵ, a2/b2]
(−∞, a2/b2]
6
a1 > 0
b1 < b2 = 0
[b1, (−ϵ)]
[a2/(−ϵ), a1/b1]
(−∞, a1/b1]
7
a1 > 0
b1 < 0 < b2
[b1, (−ϵ)]
[a2/(−ϵ), a1/b1]
(−∞, a1/b1]
∪[ϵ, b2]
∪[a1/b2, a2/ϵ]
∪[a1/b2, +∞)
8
a1 > 0
0 = b1 < b2
[ϵ, b2]
[a1/b2, a2/ϵ]
[a1/b2, +∞)
Table 4.8. The eight cases of interval division A/B, with A, B ∈IR, and
0 ∈B.

4.9 Extended Interval Arithmetic
137
In all other cases 0 /∈A also. We have already observed under (b) that the element
0 in B does not contribute to the solution set. So it can be excluded without changing
the set A/B.
So the general rule for computing the set A/B by (4.9.5) is to remove its zero from
the interval B and replace it by a small positive or negative number ϵ as the case may
be. The resulting set is denoted by B′ and represented in column 4 of Table 4.8. With
this B′ the solution set A/B′ can now easily be computed by applying the rules of
Table 4.2. The results are shown in column 5 of Table 4.8. Now the desired result
A/B as deﬁned by (4.9.5) is obtained if in column 5 ϵ tends to zero.
Thus in the cases 3 to 8 the results are obtained by the limit process A/B =
limϵ→0 A/B′. The solution set A/B is shown in the last column of Table 4.8 for
all the eight cases. There, as usual in mathematics round brackets indicate that the
bound is not included in the set. In contrast to this square brackets denote closed
interval ends, i.e., the bound is included.
The operands A and B of the division A/B in Table 4.8 are intervals of IR. The
results of the division shown in the last column, however, are no longer intervals of
IR. The result is now an element of the power set PR. With the exception of case 2
the result is now a set which stretches continuously to −∞or +∞or both.
In two cases (rows 4 and 7 in Table 4.8) the result consists of the union of two dis-
tinct sets of the form (−∞, c2]∪[c1, +∞). These cases can easily be identiﬁed by the
signs of the bounds of the divisor. Within the given framework of existing processors
only one interval can be delivered as the result of an interval operation. In the cases
4 and 7 of Table 4.8 the result, yet, can be returned as an improper interval [c1, c2]
where the left hand bound is higher than the right hand bound. Motivated by the ex-
tended interval Newton method5 it is reasonable to separate these results into the two
distinct sets: (−∞, c2] and [c1, +∞). The fact that an arithmetic operation delivers
two distinct results seems to be a totally new situation in computing. Evaluation of
the square root, however, also delivers two results and we have learned to live with it.
Computing certainly is able to deal with this situation.
A principle solution of the problem would be for the computer to provide a ﬂag for
distinct intervals. The situation occurs if the divisor is an interval that contains zero
as an interior point. In cases 4 and 7 of Table 4.8 the ﬂag would be raised and signaled
to the user. The user may then apply a routine of his choice to deal with the situation
as is appropriate for his application.6
5Newton’s method reaches its ultimate elegance and strength in the extended interval Newton
method. If division by an interval that contains zero delivers two distinct sets the computation is contin-
ued along two separate paths, one for each interval. This is how the extended interval Newton method
separates different zeros from each other and ﬁnally computes all zeros in a given domain. If the interval
Newton method delivers the empty set, the method has proved that there is no zero in the initial interval.
6This routine could be: modify the operands and recompute, or continue the computation with one
of the sets and ignore the other one, or put one of the sets on a list and continue the computation with the
other one, or return the entire set of real numbers (−∞, +∞) as result and continue the computation, or

138
4 Interval Arithmetic
If during a computation in the real number ﬁeld zero appears as a divisor the com-
putation should be stopped immediately. In ﬂoating-point arithmetic the situation is
different. Zero may be the result of an underﬂow. In such a case a corresponding in-
terval computation would not deliver zero but a small interval with zero as one bound
and a tiny positive or negative number as the other bound. In this case division is well
deﬁned by Table 4.8. The result is a closed interval which stretches continuously to
−∞or +∞as the case may be. In the real number ﬁeld zero as a divisor is an acci-
dent. So in interval arithmetic division by an interval that contains zero as an interior
point certainly will be a rare appearance. An exception is the interval Newton method.
Here, however, it is clear how the situation has to be handled. See Chapter 9 of this
book.
In the literature an improper interval [c1, c2] with c1 > c2 occasionally is called
an ‘exterior interval’. On the number circle an ‘exterior interval’ is interpreted as an
interval with inﬁnity as an interior point. We do not follow this line here. Interval
arithmetic is deﬁned as an arithmetic for sets of real numbers. Operations for real
numbers which deliver ∞(or −∞or +∞) as their result do not exist. Here and in the
following the symbols −∞and +∞are only used to describe sets of real numbers.
After the splitting of improper intervals into two distinct sets only four kinds of
result come from division by an interval of IR which contains zero:
∅,
(−∞, a],
[b, +∞),
and
(−∞, +∞).
(4.9.6)
We call such elements extended intervals. The union of the set of intervals of IR
with the set of extended intervals is denoted by (IR). The elements of the set (IR)
are themselves simply called intervals. (IR) is the set of closed intervals of R.7 IR
is the set of closed and bounded intervals of R. Intervals of IR and of (IR) are sets
of real numbers. −∞and +∞are not elements of these intervals. It is fascinating
that arithmetic operations can be introduced for all elements of the set (IR) in an
exception-free manner. This will be shown in the next section.
Under the assumption that the result is split into two distinct intervals after division
by an interval that contains zero as an interior point, the set (IR) can be seen as
another algebraically closed subset of the power set PR.
On a computer only subsets of the real numbers are representable. We assume now
that S is the set of ﬂoating-point numbers of a given computer. An interval between
two ﬂoating-point bounds represents the continuous set of real numbers between these
bounds. Similarly, except for the empty set, also extended intervals represent contin-
uous sets of real numbers.
To transform the eight cases of division by an interval which contains zero into
computer executable operations we assume now that the operands A and B are
stop computing, or ignore the ﬂag, or any other action.
7A subset of R is called closed if the complement is open.

4.9 Extended Interval Arithmetic
139
ﬂoating-point intervals of IS. To obtain a computer representable result we round
the result shown in the last column of Table 4.8 into the least computer representable
superset. That is, the lower bound of the result has to be computed with rounding
downwards and the upper bound with rounding upwards. Thus on the computer the
eight cases of division by an interval which contains zero have to be performed as
shown in Table 4.9.
case
A = [a1, a2]
B = [b1, b2]
A♦
/ B
1
0 ∈A
0 ∈B
(−∞, +∞)
2
0 /∈A
B = [0, 0]
∅
3
a2 < 0
b1 < b2 = 0
[a2▽
/ b1, +∞)
4
a2 < 0
b1 < 0 < b2
(−∞, a2△
/ b2] ∪[a2▽
/ b1, +∞)
5
a2 < 0
0 = b1 < b2
(−∞, a2△
/ b2]
6
a1 > 0
b1 < b2 = 0
(−∞, a1△
/ b1]
7
a1 > 0
b1 < 0 < b2
(−∞, a1△
/ b1] ∪[a1▽
/ b2, +∞)
8
a1 > 0
0 = b1 < b2
[a1▽
/ b2, +∞)
Table 4.9. The eight cases of interval division with A, B ∈IS, and 0 ∈B.
Table 4.10 shows the same cases as Table 4.9 in another layout.
B = [0, 0]
b1 < b2 = 0
b1 < 0 < b2
0 = b1 < b2
a2 < 0
∅
[a2▽
/ b1, +∞)
(−∞, a2△
/ b2]
(−∞, a2△
/ b2]
∪[a2▽
/ b1, +∞)
a1 ≤0 ≤a2
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
0 < a1
∅
(−∞, a1△
/ b1]
(−∞, a1△
/ b1]
[a1▽
/ b2, +∞)
∪[a1▽
/ b2, +∞)
Table 4.10. The result of the interval division with A, B ∈IS, and 0 ∈B.
Table 4.9 and Table 4.10 display the eight distinct cases of interval division A♦
/ B
with A, B ∈IS and 0 ∈B. On the computer the empty interval ∅needs a particular
encoding. (+NaN, -NaN) may be such an encoding. We explicitly stress that the sym-
bols −∞and +∞are used here only to represent the resulting sets. These symbols
are not elements of these sets.

140
4 Interval Arithmetic
4.10
Exception-free Arithmetic for Extended Intervals
Extending the remark of the last paragraph we begin this section with a basic state-
ment. The formulas for the result of interval operations derived in this and the previous
section will ultimately be executed by computer. There the bounds are ﬂoating-point
numbers. Floating-point numbers and ﬂoating-point intervals are objects of different
quality. A ﬂoating-point number is an approximate representation of a real number,
while an interval is a precisely deﬁned object. An operation mixing the two, which
ought to be an interval, may not be precisely speciﬁed. It is thus not reasonable to
deﬁne operations between ﬂoating-point numbers and intervals. If a user does indeed
need to perform an operation between a ﬂoating-point number and a ﬂoating-point
interval, he may do so by employing a transfer function – which may be predeﬁned
– which transforms its ﬂoating-point operand into a ﬂoating-point interval. In do-
ing so, the user is made aware of the possible loss of meaning of the interval as a
precise object. Prohibiting an automatic type transfer from ﬂoating-point numbers
to ﬂoating-point intervals prevents exceptions of the IEEE ﬂoating-point arithmetic
standard from being introduced into interval arithmetic. Here the symbols −∞and
+∞are only used for the description of particular sets of real numbers.
Table 4.11 and Table 4.12 show the results of addition and subtraction of intervals
of (IR). Any operation with the empty set always delivers the empty set.
Addition
(−∞, b2]
[b1, b2]
[b1, +∞)
(−∞, +∞)
(−∞, a2]
(−∞, a2 + b2]
(−∞, a2 + b2]
(−∞, +∞)
(−∞, +∞)
[a1, a2]
(−∞, a2 + b2]
[a1 + b1, a2 + b2]
[a1 + b1, +∞)
(−∞, +∞)
[a1, +∞)
(−∞, +∞)
[a1 + b1, +∞)
[a1 + b1, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.11. Addition for intervals of (IR).
Subtraction
(−∞, b2]
[b1, b2]
[b1, +∞)
(−∞, +∞)
(−∞, a2]
(−∞, +∞)
(−∞, a2 −b1]
(−∞, a2 −b1]
(−∞, +∞)
[a1, a2]
[a1 −b2, +∞)
[a1 −b2, a2 −b1]
(−∞, a2 −b1]
(−∞, +∞)
[a1, +∞)
[a1 −b2, +∞)
[a1 −b2, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.12. Subtraction for intervals of (IR).
To obtain these results in the operands A and B bounds like −∞and +∞are
replaced by a very large negative and a very large positive number respectively. Then
the basic rules for addition and subtraction for regular intervals [a1, a2] + [b1, b2] =

4.10 Exception-free Arithmetic for Extended Intervals
141
[a1 +b1, a2 +b2] and [a1, a2]−[b1, b2] = [a1 −b2, a2 −b1] are applied. In the resulting
formulas then the very large negative number is shifted to −∞and the very large
positive number to +∞. Finally the rules ∞+x = ∞, −∞+x = −∞, ∞+∞= ∞,
and −∞+ (−∞) = −∞with x ∈R are applied. These rules are well established in
real analysis.
The rules for division by an interval which contains zero developed in the last
section together with the rules for addition and subtraction for intervals of (IR) shown
in Table 4.11 and Table 4.12 allow an exception-free execution of the extended interval
Newton method of interval mathematics.
The following three tables (Table 4.13, Table 4.14, and Table 4.15) show the results
of multiplication and division for intervals of (IR). In the case of division again the
two cases 0 /∈B and 0 ∈B are studied separately.
The general procedure to obtain these results is very similar to the cases of addition
and subtraction. Bounds like −∞and +∞in the operands for A and B are replaced
by a very large negative and a very large positive number respectively. Then the basic
rules for multiplication and for division with 0 /∈B for regular intervals of IR are
applied. In Table 4.13 and Table 4.14 these rules are repeated in the rows and columns
in the left upper corner. In the resulting formulas the very large negative number is
then shifted to −∞and the very large positive number to +∞. Finally, very simple
and well-established rules of real analysis like ∞∗x = ∞for x > 0, ∞∗x = −∞
for x < 0, x/∞= x/−∞= 0, ∞∗∞= ∞, (−∞)∗∞= −∞are applied together
with variants obtained by applying the sign rules and the law of commutativity.
Table 4.15 shows the results of division for intervals of (IR) with 0 ∈B. The basic
procedure to obtain the results of the operations is similar to those in the former cases.
However, two situations have to be treated separately. These are the cases shown in
rows 1 and 2 of Table 4.8. Zero is now always an element of the divisor, i.e., 0 ∈B.
If 0 ∈A and 0 ∈B (row 1 of Table 4.8), the result consists of all real numbers,
i.e., A/B = (−∞, +∞). This applies to rows 2, 5, 6, and 8 of Table 4.15.
If 0 /∈A and B = [0, 0] (row 2 of Table 4.8), the result of the division is the empty
set, i.e., A/B = ∅. This applies to rows 1, 3, 4, and 7 of column 1 of Table 4.15.
The remaining cases of Table 4.15 can be treated very similarly to the former cases.
Bounds −∞and +∞in the extended interval operands are ﬁrst replaced by a very
large negative number and a very large positive number respectively. Then the rules
for division of regular intervals with 0 ∈B are applied. These rules are shown in
Table 4.8. In Table 4.15 these rules are repeated in rows and columns in the left
upper corner. After shifting the very large negative number to −∞and the very large
positive number to +∞and application of well-established rules of real analysis the
rules shown in Table 4.15 are easily obtained.
In summary it can be said that after a possible splitting of an improper interval into
two separate intervals the result of arithmetic operations for intervals of (IR) always
leads to intervals of (IR) again. No exceptions do occur performing these operations.

142
4 Interval Arithmetic
[b1, b2]
[b1, b2]
[b1, b2]
(−∞, b2]
(−∞, b2]
[b1, +∞)
[b1, +∞)
Multiplication
b2 ≤0
b1 < 0 < b2
b1 ≥0
[0,0]
b2 ≤0
b2 ≥0
b1 ≤0
b1 ≥0
(−∞, +∞)
[a1, a2], a2 ≤0
[a2b2, a1b1]
[a1b2, a1b1]
[a1b2, a2b1]
[0,0]
[a2b2, +∞)
[a1b2, +∞)
(−∞, a1b1]
(−∞, a2b1]
(−∞, +∞)
a1 < 0 < a2
[a2b1, a1b1]
[min(a1b2, a2b1),
[a1b2, a2b2]
[0,0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
max(a1b1, a2b2)]
[a1, a2], a1 ≥0
[a2b1, a1b2]
[a2b1, a2b2]
[a1b1, a2b2]
[0,0]
(−∞, a1b2]
(−∞, a2b2]
[a2b1, +∞)
[a1b1, +∞)
(−∞, +∞)
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0,0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
(−∞, a2], a2 ≤0
[a2b2, +∞)
(−∞, +∞)
(−∞, a2b1]
[0,0]
[a2b2, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, a2b1]
(−∞, +∞)
(−∞, a2], a2 ≥0
[a2b1, +∞)
(−∞, +∞)
(−∞, a2b2]
[0,0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
[a1, +∞), a1 ≤0
(−∞, a1b1]
(−∞, +∞)
[a1b2, +∞)
[0,0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
[a1, +∞), a1 ≥0
(−∞, a1b2]
(−∞, +∞)
[a1b1, +∞)
[0,0]
(−∞, a1b2]
(−∞, +∞)
(−∞, +∞)
[a1b1, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
[0,0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.13. Multiplication for intervals of (IR).

4.10 Exception-free Arithmetic for Extended Intervals
143
Division
[b1, b2]
[b1, b2]
(−∞, b2]
[b1, +∞)
0 /∈B
b2 < 0
b1 > 0
b2 < 0
b1 > 0
[a1, a2], a2 ≤0
[a2/b1, a1/b2]
[a1/b1, a2/b2]
[0, a1/b2]
[a1/b1, 0]
[a1, a2], a1 < 0 < a2
[a2/b2, a1/b2]
[a1/b1, a2/b1]
[a2/b2, a1/b2]
[a1/b1, a2/b1]
[a1, a2], a1 ≥0
[a2/b2, a1/b1]
[a1/b2, a2/b1]
[a2/b2, 0]
[0, a2/b1]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
(−∞, a2], a2 ≤0
[a2/b1, +∞)
(−∞, a2/b2]
[0, +∞)
(−∞, 0]
(−∞, a2], a2 ≥0
[a2/b2, +∞)
(−∞, a2/b1]
[a2/b2, +∞)
(−∞, a2/b1]
[a1, +∞), a1 ≤0
(−∞, a1/b2]
[a1/b1, +∞)
(−∞, a1/b2]
[a1/b1, +∞)
[a1, +∞), a1 ≥0
(−∞, a1/b1]
[a1/b2, +∞)
(−∞, 0]
[0, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.14. Division for intervals of (IR) with 0 ̸∈B.

144
4 Interval Arithmetic
Division
B =
[b1, b2]
[b1, b2]
[b1, b2]
(−∞, b2]
(−∞, b2]
[b1, +∞)
[b1, +∞)
0 ∈B
[0, 0]
b1 < b2 = 0
b1 < 0 < b2
0 = b1 < b2
b2 = 0
b2 > 0
b1 < 0
b1 = 0
(−∞, +∞)
[a1, a2],
∅
[a2/b1, +∞)
(−∞, a2/b2]
(−∞, a2/b2]
[0, +∞)
(−∞, a2/b2]
(−∞, 0]
(−∞, 0]
(−∞, +∞)
a2 < 0
∪[a2/b1, +∞)
∪[0, +∞)
∪[a2/b1, +∞)
[a1, a2],
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
a1 ≤0 ≤a2
[a1, a2],
∅
(−∞, a1/b1]
(−∞, a1/b1]
[a1/b2, +∞)
(−∞, 0]
(−∞, 0]
(−∞, a1/b1]
[0, +∞)
(−∞, +∞)
a1 > 0
∪[a1/b2, +∞)
∪[a1/b2, +∞)
∪[0, +∞)
(−∞, a2],
∅
[a2/b1, +∞)
(−∞, a2/b2]
(−∞, a2/b2]
[0, +∞)
(−∞, a2/b2]
(−∞, 0]
(−∞, 0]
(−∞, +∞)
a2 < 0
∪[a2/b1, +∞)
∪[0, +∞)
∪[a2/b1, +∞)
(−∞, a2],
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
a2 > 0
[a1, +∞),
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
a1 < 0
[a1, +∞),
∅
(−∞, a1/b1]
(−∞, a1/b1]
[a1/b2, +∞)
(−∞, 0]
(−∞, 0]
(−∞, a1/b1]
[0, +∞)
(−∞, +∞)
a1 > 0
∪[a1/b2, +∞)
∪[a1/b2, +∞)
∪[0, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.15. Division for intervals of (IR) with 0 ∈B.

4.11 Extended Interval Arithmetic on the Computer
145
For the development in the preceding sections it was essential to distinguish be-
tween open and closed interval bounds. Thus besides of the empty set intervals of
(IR) are enclosed in square and/or round brackets. If the bracket adjacent to a bound
is round, the bound is not included in the interval; if it is square, the bound is included
in the interval.
On the computer, of course, the lower bound of the result has to be rounded down-
wards and the upper bound rounded upwards. The bounds −∞and +∞are not
changed by these roundings.
We summarize the complete set of arithmetic operations for interval arithmetic in
(IS) that should be provided on the computer in the next section. (IS) is the set of
closed real intervals with bounds of S.
4.11
Extended Interval Arithmetic on the Computer
Addition
(−∞, b2]
[b1, b2]
[b1, +∞)
(−∞, +∞)
(−∞, a2]
(−∞, a2△
+ b2]
(−∞, a2△
+ b2]
(−∞, +∞)
(−∞, +∞)
[a1, a2]
(−∞, a2△
+ b2]
[a1▽
+ b1, a2△
+ b2]
[a1▽
+ b1, +∞)
(−∞, +∞)
[a1, +∞)
(−∞, +∞)
[a1▽
+ b1, +∞)
[a1▽
+ b1, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.16. Addition of extended intervals on the computer.
Subtraction
(−∞, b2]
[b1, b2]
[b1, +∞)
(−∞, +∞)
(−∞, a2]
(−∞, +∞)
(−∞, a2△
−b1]
(−∞, a2△
−b1]
(−∞, +∞)
[a1, a2]
[a1▽
−b2, +∞)
[a1▽
−b2, a2△
−b1]
(−∞, a2△
−b1]
(−∞, +∞)
[a1, +∞)
[a1▽
−b2, +∞)
[a1▽
−b2, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.17. Subtraction of extended intervals on the computer.

146
4 Interval Arithmetic
[b1, b2]
[b1, b2]
[b1, b2]
(−∞, b2]
(−∞, b2]
[b1, +∞)
[b1, +∞)
Multiplication
b2 ≤0
b1 < 0 < b2
b1 ≥0
[0, 0]
b2 ≤0
b2 ≥0
b1 ≤0
b1 ≥0
(−∞, +∞)
[a1, a2], a2 ≤0
[a2▽· b2, a1△· b1]
[a1▽· b2, a1△· b1]
[a1▽· b2, a2△· b1] [0, 0] [a2▽· b2, +∞) [a1▽· b2, +∞) (−∞, a1△· b1] (−∞, a2△· b1] (−∞, +∞)
a1 < 0 < a2
[a2▽· b1, a1△· b1] [min(a1▽· b2, a2▽· b1), [a1▽· b2, a2△· b2] [0, 0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
max(a1△· b1, a2△· b2)]
[a1, a2], a1 ≥0
[a2▽· b1, a1△· b2]
[a2▽· b1, a2△· b2]
[a1▽· b1, a2△· b2] [0, 0] (−∞, a1△· b2] (−∞, a2△· b2] [a2▽· b1, +∞) [a1▽· b1, +∞) (−∞, +∞)
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
(−∞, a2], a2 ≤0
[a2▽· b2, +∞)
(−∞, +∞)
(−∞, a2△· b1]
[0, 0] [a2▽· b2, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, a2△· b1] (−∞, +∞)
(−∞, a2], a2 ≥0
[a2▽· b1, +∞)
(−∞, +∞)
(−∞, a2△· b2]
[0, 0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
[a1, +∞), a1 ≤0
(−∞, a1△· b1]
(−∞, +∞)
[a1▽· b2, +∞)
[0, 0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
[a1, +∞), a1 ≥0
(−∞, a1△· b2]
(−∞, +∞)
[a1▽· b1, +∞)
[0, 0] (−∞, a1△· b2]
(−∞, +∞)
(−∞, +∞)
[a1▽· b1, +∞) (−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
[0, 0]
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.18. Multiplication of extended intervals on the computer.

4.11 Extended Interval Arithmetic on the Computer
147
Division
[b1, b2]
[b1, b2]
(−∞, b2]
[b1, +∞)
0 /∈B
b2 < 0
b1 > 0
b2 < 0
b1 > 0
[a1, a2], a2 ≤0
[a2▽
/ b1, a1△
/ b2]
[a1▽
/ b1, a2△
/ b2]
[0, a1△
/ b2]
[a1▽
/ b1, 0]
[a1, a2], a1 < 0 < a2
[a2▽
/ b2, a1△
/ b2]
[a1▽
/ b1, a2△
/ b1]
[a2▽
/ b2, a1△
/ b2]
[a1▽
/ b1, a2△
/ b1]
[a1, a2], a1 ≥0
[a2▽
/ b2, a1△
/ b1]
[a1▽
/ b2, a2△
/ b1]
[a2▽
/ b2, 0]
[0, a2△
/ b1]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
[0, 0]
(−∞, a2], a2 ≤0
[a2▽
/ b1, +∞)
(−∞, a2△
/ b2]
[0, +∞)
(−∞, 0]
(−∞, a2], a2 ≥0
[a2▽
/ b2, +∞)
(−∞, a2△
/ b1]
[a2▽
/ b2, +∞)
(−∞, a2△
/ b1]
[a1, +∞), a1 ≤0
(−∞, a1△
/ b2]
[a1▽
/ b1, +∞)
(−∞, a1△
/ b2]
[a1▽
/ b1, +∞)
[a1, +∞), a1 ≥0
(−∞, a1△
/ b1]
[a1▽
/ b2, +∞)
(−∞, 0]
[0, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
Table 4.19. Division of extended intervals with 0 ̸∈B on the computer.

148
4 Interval Arithmetic
Division
B =
[b1, b2]
[b1, b2]
[b1, b2]
(−∞, b2]
(−∞, b2]
[b1, +∞)
[b1, +∞)
0 ∈B
[0, 0]
b1 < b2 = 0
b1 < 0 < b2
0 = b1 < b2
b2 = 0
b2 > 0
b1 < 0
b1 = 0
(−∞, +∞)
[a1, a2], a2 < 0
∅
[a2▽
/ b1, +∞)
(−∞, a2△
/ b2]
(−∞, a2△
/ b2]
[0, +∞)
(−∞, a2△
/ b2]
(−∞, 0]
(−∞, 0]
(−∞, +∞)
∪[a2▽
/ b1, +∞)
∪[0, +∞)
∪[a2▽
/ b1, +∞)
[a1, a2], a1 ≤0 ≤a2
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞) (−∞, +∞)
[a1, a2], a1 > 0
∅
(−∞, a1△
/ b1]
(−∞, a1△
/ b1]
[a1▽
/ b2, +∞)
(−∞, 0]
(−∞, 0]
(−∞, a1△
/ b1]
[0, +∞)
(−∞, +∞)
∪[a1▽
/ b2, +∞)
∪[a1▽
/ b2, +∞)
∪[0, +∞)
(−∞, a2], a2 < 0
∅
[a2▽
/ b1, +∞)
(−∞, a2△
/ b2]
(−∞, a2△
/ b2]
[0, +∞)
(−∞, a2△
/ b2]
(−∞, 0]
(−∞, 0]
(−∞, +∞)
∪[a2▽
/ b1, +∞)
∪[0, +∞)
∪[a2▽
/ b1, +∞)
(−∞, a2], a2 > 0
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞) (−∞, +∞)
[a1, +∞), a1 < 0
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞) (−∞, +∞)
[a1, +∞), a1 > 0
∅
(−∞, a1△
/ b1]
(−∞, a1△
/ b1]
[a1▽
/ b2, +∞)
(−∞, 0]
(−∞, 0]
(−∞, a1△
/ b1]
[0, +∞)
(−∞, +∞)
∪[a1▽
/ b2, +∞)
∪[a1▽
/ b2, +∞)
∪[0, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞) (−∞, +∞)
Table 4.20. Division of extended intervals with 0 ∈B on the computer.

4.12 Implementation of Extended Interval Arithmetic
149
4.12
Implementation of Extended Interval Arithmetic
The rules for the operations of extended intervals on the computer in Tables 4.16–4.20
look rather complicated. Their implementation seems to require a major number of
case distinctions. The situation, however, can be greatly simpliﬁed by the following
hints.
We ﬁrst consider the operations addition, subtraction, multiplication, and division
by an interval which does not contain zero. On the computer actually only the basic
rules for addition
[a1, a2] + [b1, b2] = [a1▽
+ b1, a2△
+ b2],
(4.12.1)
for subtraction
[a1, a2] −[b1, b2] = [a1▽
−b2, a2△
−b1],
(4.12.2)
for multiplication the rules of Table 4.1, and for division those of Table 4.2 are to be
provided. In the Tables 4.16–4.18 these rules are printed in bold letters.
The remaining rules shown in the Tables 4.16–4.18 can automatically be produced
out of these basic rules by the computer itself if a few well-established rules for com-
puting with −∞and +∞are formally applied. With x ∈R these rules are
∞+ x = ∞,
−∞+ x = −∞,
−∞+ (−∞) = (−∞) · ∞= −∞,
∞+ ∞= ∞· ∞= ∞,
∞· x = ∞for x > 0,
∞· x = −∞for x < 0,
x
∞=
x
−∞= 0,
together with variants obtained by applying the sign rules and the law of commuta-
tivity. If in an interval operand a bound is −∞or +∞the multiplication with 0 is
performed as if the following rules would hold
0 · (−∞) = 0 · (+∞) = (−∞) · 0 = (+∞) · 0 = 0.
These rules have no meaning otherwise.
The case of division of an extended interval by an interval that contains zero, Ta-
ble 4.20 is a little more complicated. Basically it can be treated very similarly than
the other cases. However, two situations have to be dealt with separately. Zero can
now be an element of A and in one case we have B = [0, 0].
If 0 ∈A and 0 ∈B then by row 1 of Table 4.9 the result consists of the set of
all real numbers, i.e., A/B = (−∞, +∞). This applies to rows 2, 5, 6, and 8 of
Table 4.20.
If 0 /∈A and B = [0, 0] then by row 2 of Table 4.9 the result is the empty set, i.e.,
A/B = ∅. This applies to rows 1, 3, 4, and 7 of column 1 of Table 4.20.
Realization of the remaining cases of Table 4.20 then can automatically be pro-
duced by the computer itself out of the few rules printed in bold letters in Table 4.20
by applying the established rules for −∞and +∞shown above.

150
4 Interval Arithmetic
4.13
Comparison Relations and Lattice Operations
Three comparison relations are important for intervals of IR and (IR):
equality,
less than or equal,
and
set inclusion.
(4.13.1)
Since bounds for intervals of (IR) may be −∞or +∞these comparison relations
are deﬁned as if performed in the complete lattice {R∗, ≤} with R∗:= R ∪{−∞} ∪
{+∞}.
Let A and B be intervals of (IR) with bounds a1 ≤a2 and b1 ≤b2 respectively.
Then the relations equality and less than or equal in (IR) are deﬁned by:
A = B
:⇔
a1 = b1 ∧a2 = b2,
A ≤B
:⇔
a1 ≤b1 ∧a2 ≤b2.
With the order relation ≤, {(IR), ≤} is a lattice. If either A or B is the empty interval
the result is false. The greatest lower bound (glb) and the least upper bound (lub) of
A, B ∈(IR) are the intervals
glb(A, B)
:=
[min(a1, b1), min(a2, b2)],
lub(A, B)
:=
[max(a1, b1), max(a2, b2)].
The inclusion relation in (IR) is deﬁned by
A ⊆B :⇔b1 ≤a1 ∧a2 ≤b2.
(4.13.2)
With the relation ⊆, {(IR), ⊆} is also a lattice. If A is the empty interval the result
is true. If B is the empty interval the result is false. The least element in {(IR), ⊆}
is the empty set ∅and the greatest element is the set R, i.e., the interval (−∞, +∞).
The inﬁmum of two elements A, B ∈(IR) is the intersection and the supremem is
the interval hull (convex hull):
inf(A, B)
:=
[max(a1, b1), min(a2, b2)]
or the empty set ∅,
sup(A, B)
:=
[min(a1, b1), max(a2, b2)].
The intersection of an interval with the empty set is the empty set. The interval hull
with the empty set is the other operand.
If in the formulas for glb(A, B), lub(A, B), inf(A, B), sup(A, B), a bound is −∞
or +∞a round bracket should be used at this interval bound to denote the resulting
interval. This bound is not an element of the interval.
If in any of the comparison relations deﬁned here both operands are the empty set,
the result is true. If in (4.13.2) A is the empty set the result is true. Otherwise the result
is false if in any of the three comparison relations only one operand is the empty set.

4.14 Algorithmic Implementation of Interval Multiplication and Division
151
A particular case of inclusion is the relation element of. It is deﬁned by
a ∈B :⇔b1 ≤a ∧a ≤b2.
(4.13.3)
Another useful operation is to check whether an interval [a1, a2] is a proper interval.
It computes the result a1 ≤a2. It is used to test whether the result of an interval
division consists of a proper or an improper interval.
4.14
Algorithmic Implementation of Interval Multiplication
and Division
In this section we give an algorithmic description of interval multiplication and in-
terval division. In the case of interval division by an interval that contains zero we
restrict the algorithm to the original solution discussed in Section 4.9. We leave the
development of an algorithm for the implementation of the general case developed in
Sections 4.10 and 4.11 as an exercise to the user. An algorithm for interval multi-
plication has to realize Table 4.5 and an algorithm for interval division has to realize
Tables 4.10 and 4.6. In the following algorithms lb denotes the lower bound and ub
the upper bound of the result interval.
We begin with the multiplication.
if (a1 < 0 ∧a2 > 0 ∧b1 < 0 ∧b2 > 0) then
p := a1▽· b2; q := a2▽· b1
lb := min(p, q)
r := a1△· b1; s := a2△· b2
ub := max(r, s)
else
if (b2 ≤0 ∨(b1 < 0 ∧a1 ≥0)) then p := a2 else p := a1
if (a2 ≤0 ∨(a1 < 0 ∧b2 ≥0)) then q := b2 else q := b1
lb := p▽· q
if (b1 ≥0 ∨(a1 ≥0 ∧b2 > 0)) then r := a2 else r := a1
if (a1 ≥0 ∨(a2 > 0 ∧b1 ≥0)) then s := b2 else s := b1
ub := r△· s
The 14 cases of interval division are realized by the following algorithm.
if (b2 < 0 ∨b1 > 0) then
if (b2 < 0)) then p := a2 else p := a1
if (a1 ≥0 ∨(a2 > 0 ∧b1 < 0)) then q := b2 else q := b1
lb := p▽
/ q
if (b1 > 0)) then r := a2 else r := a1
if (a2 ≤0 ∨(a1 < 0 ∧b1 < 0)) then s := b2 else s := b1

152
4 Interval Arithmetic
ub := r△
/ s
else if (a1 ≤0 ∧a2 ≥0 ∧b1 ≤0 ∧b2 ≥0) then [lb, ub] := [−∞, +∞]
else if ((a2 < 0 ∨a1 > 0) ∧b1 = 0 ∧b2 = 0) then [lb, ub] := [, ]
else if (a2 < 0 ∧b2 = 0) then [lb, ub] := [a2▽
/ b1, +∞]
else if (a2 < 0 ∧b1 = 0) then [lb, ub] := [−∞, a2△
/ b2]
else if (a1 > 0 ∧b2 = 0) then [lb, ub] := [−∞, a1△
/ b1]
else if (a1 > 0 ∧b1 = 0) then [lb, ub] := [a1▽
/ b2, +∞]
else if (a2 < 0 ∧b1 < 0 ∧b2 > 0) then [lb, ub] := [a2▽
/ b1, a2△
/ b2]
else if (a1 > 0 ∧b1 < 0 ∧b2 > 0) then [lb, ub] := [a1▽
/ b2, a1△
/ b1]
In this algorithm [, ] denotes the empty set. The algorithm is organized in such a way
that the more complicated cases, where the result consists of two separate intervals,
appear at the end. Even in these cases the results are represented as a single interval
which overlaps the point inﬁnity (i.e., lb > ub). Thus the result of an interval division
always just consists of two bounds.
Hardware support for interval arithmetic will be discussed in Chapter 7.

Part II
Implementation of Arithmetic on Computers

Chapter 5
Floating-Point Arithmetic
Thus far our considerations have proceeded under the general assumption
that the set R in Figure 1 is a linearly ordered ringoid. We are now going
to be more speciﬁc and assume that R is the linearly ordered ﬁeld of real
numbers and that the subsets D and S are special screens, which are called
ﬂoating-point systems. In Section 5.1 we brieﬂy review the deﬁnition of
the real numbers and their representation by b-adic expansions. Section 5.2
deals with ﬂoating-point numbers and systems. We also discuss several
basic roundings of the real numbers into a ﬂoating-point system, and we
derive error bounds for these roundings.
In Section 5.3 we consider ﬂoating-point operations deﬁned by semi-
morphisms, and we derive error bounds for these operations.
Then we
consider the operations deﬁned in the product sets listed under S and D
in Figure 1, and we derive error formulas and bounds for these operations
also. All this is done under the assumption that the operations are deﬁned
by semimorphisms.
For scalar product and matrix multiplication, we also derive error for-
mulas and bounds for the conventional deﬁnition of the operations. The
error formulas are simpler if the operations are deﬁned by semimorphisms,
and the error bounds are smaller by a factor of at least n compared to the
bounds obtained by the conventional deﬁnition of the operations. These two
properties are reproduced in the error analysis of many algorithms.
Finally we review the so-called IEEE ﬂoating-point arithmetic standard.
5.1
Deﬁnition and Properties of the Real Numbers
Two methods of deﬁning the real numbers are most commonly used. These may be
called the constructive method and the axiomatic method. We have mentioned these
concepts already in the introduction of this book. For clarity we brieﬂy consider the
axiomatic method here.
The real numbers {R, +, ·, ≤} can be deﬁned as a conditionally complete linearly
ordered ﬁeld, i.e.,
I {R, +, ·} is a ﬁeld.
II {R, ≤} is a conditionally complete, linearly ordered set.

5.1 Deﬁnition and Properties of the Real Numbers
155
III The following compatibility properties hold between the algebraic and the order
structure in R:
(a)

a,b,c∈R
(a ≤b ⇒a + c ≤b + c).
(b)

a,b,c∈R
(a ≤b ∧c ≥0 ⇒a · c ≤b · c).
In the ﬁeld {R, +, ·} both, addition and multiplication have the structure of a com-
mutative group. 0 and 1 are the neutral elements of addition and multiplication in
R respectively, with 0 ̸= 1.
Both groups are connected by the distributive law:
a · (b + c) = a · b + a · c. The order structure in {R, +, ·, ≤} fulﬁlls our axioms
(O1), (O2), (O3), (O4), and (O6).
These few properties are the basis for the huge construct that is real analysis. All
further properties of the real numbers are derived from them. In this connection the
following theorem is of fundamental interest.
Theorem 5.1. Two conditionally complete, linearly ordered ﬁelds R and R′ are iso-
morphic.
■
A proof of this theorem can be found in many places in the literature, for instance,
in [416].
Since corresponding elements of isomorphic structures can be identiﬁed with each
other, Theorem 5.1 asserts that there exists at most one conditionally complete, lin-
early ordered ﬁeld. Such a structure does in fact exist since the constructive method
mentioned in the introduction of this book actually produces one. In this sense the
real numbers deﬁned by that method represent the only realization of a conditionally
complete, linearly ordered ﬁeld.
Theorem 5.1 also implies that the real numbers cannot be extended by additional
elements without giving up or weakening one of the above properties. By partly
abandoning the order structure, the complex numbers may be obtained. Sometimes
the two elements −∞and +∞with the property −∞< a < +∞, for all a ∈R,
are adjoined to the real numbers. With these elements {R ∪{−∞} ∪{+∞}, ≤} is
a complete lattice. However, it is no longer a linearly ordered ﬁeld. For instance,
a + ∞= b + ∞even if a < b.
For real numbers the absolute value is deﬁned as a mapping |·| : R →R+ := {x ∈
R | x ≥0} with the property

a∈R
|a| := sup(a, −a) =

a
for a ≥0,
−a
for a ≤0.
Using the absolute value, a distance ρ(a, b) := |a −b| can be deﬁned which has all
the properties of a metric. With the latter, the concept of the limit of a sequence and
the limit of a series can be deﬁned:

156
5 Floating-Point Arithmetic
A sequence1 {an} ∈RN := {α | α : N →R} of elements an ∈R is called
convergent to a number a ∈R, in which event we write lim an = a if

ϵ>0

N(ϵ)∈N

n>N(ϵ)
|an −a| < ϵ.
Otherwise the sequence is called divergent.
Using a sequence {an}, an associated sequence {sn} can be deﬁned, where
sn :=
n

ν=1
aν.
If {sn} is convergent to a limit s, we say that the inﬁnite series
∞

ν=1
aν = a1 + a2 + a3 + . . .
is convergent and write
s = lim
n→∞sn =
∞

ν=1
aν.
Here s is called the sum of the series. The numbers sn are called the partial sums.
If {sn} is divergent, the series is called divergent.
Beyond the great usefulness of an abstract theory it must be remembered that anal-
ysis has been developed for its application in nature. In applied mathematics, apart
from the abstract theory and symbolic representation of real numbers, their value is
of great importance. The value of a real number is represented in special number sys-
tems. Applied mathematics, therefore, also has to consider particular representations
of real numbers, algorithms for their computation, and algorithms for computing the
result of operations on real numbers.
We begin our study of the representation of real numbers with the following well-
known theorem on b-adic systems. This theorem may be shown to follow from the
axioms I–III, see [463].
Theorem 5.2. Every real number x is uniquely represented by a b-adic expansion of
the form
x = ◦dndn−1 · · · d1d0.d−1d−2d−3 · · · = ◦
−∞

ν=n
dνbν
(5.1.1)
with ◦∈{+, −} and b ∈N, b > 1. Here the di, i = n(−1) −∞, are integers that
satisfy the inequalities
0 ≤di ≤b −1 for all i = n(−1) −∞,
(5.1.2)
di ≤b −2 for inﬁnitely many i.
(5.1.3)
1N denotes the set of natural numbers.

5.1 Deﬁnition and Properties of the Real Numbers
157
Every b-adic expansion (5.1.1)–(5.1.3) represents exactly one real number.
If we approximate (5.1.1) by the ﬁnite sum
sm := ◦
−m

ν=n
dνbν,
the following error bound holds:
0 ≤|x −sm| < b−m.
(5.1.4)
■
In (5.1.1) b is called the base of the number system. The di, i = n(−1) −∞, are
called the digits.
Theorem 5.2 asserts that there exists a one-to-one correspondence between the real
numbers and the b-adic expansions of the form (5.1.1)–(5.1.3). Condition (5.1.3)
needs some interpretation. Without it the representation of a real number by a b-
adic expansion (5.1.1) need not be unique. Consider, for instance, the two decimal
expansions with the partial sums
sn = 3 · 10−1 + 0 · 10−2 + 0 · 10−3 + · · · + 0 · 10−n
and
s′
n = 2 · 10−1 + 9 · 10−2 + 9 · 10−3 + · · · + 9 · 10−n.
Both sequences {sn} and {s′
n} converge2 to and therefore represent the same real
number s = 0.3.
Although b-adic expansions are familiar objects, and especially so within the deci-
mal system, it will be useful to review several additional facts concerning them. If the
digits of a b-adic expansion are all zero for i < −m, it is called ﬁnite. Such zeros can
be omitted,
x = ◦dndn−1 · · · d1d0.d−1d−2 · · · d−m = ◦
−m

ν=n
dνbν.
(5.1.5)
The signiﬁcance of b-adic expansions lies in the fact that all real numbers, in partic-
ular the irrational numbers, can be approximated by ﬁnite expansions. If we terminate
a b-adic expansion with the digit d−m, then according to (5.1.4) the resulting error is
less than b−m. By taking m large enough, the error can be made arbitrarily small.
Multiplication by bm transforms each ﬁnite b-adic expansion (5.1.5) into a whole
number. Thus each such expansion represents a rational number. On the other hand,
not every rational number has a ﬁnite b-adic expansion. For instance, in the decimal
system, we have 1/3 = 0.333 . . . . We say that the decimal expansion of 1/3 has a
period of length 1. In general in a b-adic system the rational numbers are characterized
by the fact that they have periodic b-adic expansions.
2A monotone increasing sequence of real numbers that is bounded above converges to its supremum.

158
5 Floating-Point Arithmetic
If we deﬁne a b-adic expansion by selecting its digits randomly out of the set
{0, 1, 2, . . . , b −1}, the chance of obtaining a periodic expansion is negligible. This
suggests that most of these expansions represent irrational numbers. In what follows
we approximate real numbers by ﬁnite b-adic expansions, i.e., by a subset of the ra-
tional numbers. In the decimal system, for instance, the number 1/3 is not a member
of this approximating subset.
It is possible that the one-to-one correspondence between the numbers and the b-
adic expansions permits the deﬁnition of the real numbers through their b-adic expan-
sions. Two b-adic expansions would then be called equal if they are identical. We can
also easily decide which of two numbers is the smaller. Consider the two expansions
c = ◦cpcp−1 · · · c1c0.c−1c−2c−3 · · ·
(5.1.6)
and
d = ◦dqdq−1 · · · d1d0.d−1d−2d−3 · · · ,
(5.1.7)
and let n be the largest index for which cn ̸= dn. Without loss of generality we may
assume cn < dn, i.e., cn + 1 ≤dn. Then c < d since
c =
−∞

ν=p
cνbν =
n

ν=p
cνbν +
−∞

ν=n−1
cνbν <
n

ν=p
cνbν + bn ≤
n

ν=p
dνbν ≤d.
Negative b-adic expansions are treated correspondingly.
The deﬁnition of the operations for b-adic expansions, however, is an involved one.
An inﬁnite b-adic expansion cannot easily be added or multiplied in a simple way.
The operations for inﬁnite b-adic expansions must be deﬁned in terms of a sequence
of approximations.
For ﬁnite b-adic expansions of the form (5.1.5), the operations of addition, sub-
traction, multiplication, and division can easily be executed by well-known rules (at
least for b = 10). These algorithms can easily be derived by using the representation
(5.1.5). They reduce the operations with real numbers to operations with single digits.
To prescribe these algorithms for addition, subtraction, and multiplication, tables for
the addition, subtraction, and multiplication of all possible combinations of the digits
0, 1, 2, 3, . . . , b −1 sufﬁce. Division can be executed as a repeated subtraction. Since
these algorithms are very well known from computation with decimal numbers, we
refrain from deriving them here.
However, direct operation with inﬁnite b-adic expansions is not possible. Oper-
ations are deﬁned by means of successive approximations. Since the error of such
approximations can, in principle, be made arbitrarily small, they can be used as legit-
imate replacements for the operations for real numbers. We describe this process in
some further detail. Once again let
c = ◦cpcp−1 · · · c1c0.c−1c−2c−3 · · ·
(5.1.8)

5.1 Deﬁnition and Properties of the Real Numbers
159
and
d = ◦dqdq−1 · · · d1d0.d−1d−2d−3 · · ·
(5.1.9)
be the b-adic expansions of two real numbers. We denote the nth partial sums by
γn = ◦cpcp−1 · · · c1c0.c−1c−2 · · · c−n
(5.1.10)
and
δn = ◦dqdq−1 · · · d1d0.d−1d−2 · · · d−n.
(5.1.11)
To deﬁne s := c ◦d, ◦∈{+, −, ·, /}, in terms of the approximations γn and δn,
we form the sequence {σn}, with
σn := γn ◦δn, ◦∈{+, −, ·, /}.
If for division limn→∞δn ̸= 0, the sequence {σn} exists for sufﬁciently large n. The
quantity σn can be calculated by the well-known algorithms for ﬁnite b-adic expan-
sions. Appealing to a well-known theorem of analysis, we have
lim
n→∞σn = lim
n→∞(γn ◦δn) = lim
n→∞γn ◦lim
n→∞δn = c ◦d, ◦∈{+, −, ·, /},
and therefore s = limn→∞σn.
However, the sequence {σn} is not identical to the sequence {sn} of partial sums
of s. We illustrate this with two examples.
Examples. 1. 1
29 + 2
29 = 3
29 or 0.03448 . . . + 0.06896 . . . = 0.10344 . . .
n
σn
sn
−1
0.0
0.1
−2
0.09
0.10
−3
0.102
0.103
−4
0.1033
0.1034
...
...
...
2. 1
3 · 1
3 = 1
9 or 0.33333 . . . · 0.33333 . . . = 0.11111 . . .
n
σn
sn
−1
0.09
0.1
−2
0.1089
0.11
−3
0.110889
0.111
−4
0.11108889
0.1111
...
...
...

160
5 Floating-Point Arithmetic
Note that this example also shows that using double precision arithmetic does not
guarantee single precision accuracy. Nevertheless, the real number s is well deﬁned
as the limit of the sequence {σn}. The irrational numbers can be deﬁned as Cauchy
sequences of rational numbers. Two sequences are equivalent if they have the same
limit. In this sense {σn} and {sn} both belong to the equivalence class that deﬁnes s.
The ﬁrst part of this book is intended to provide an abstract setting for certain
subsystems of real numbers. Their realization and implementation on computers is
studied in the second part of the book. Floating-point systems are of particular inter-
est.
5.2
Floating-Point Numbers and Roundings
We begin our discussion by recalling that, according to Theorem 5.2, every real num-
ber x can be uniquely represented by a b-adic expansion of the form
x = ◦dndn−1 · · · d1d0.d−1d−2 · · · = ◦
−∞

ν=n
dνbν
(5.2.1)
with ◦∈{+, −}, b ∈N, b > 1, and
0 ≤di ≤b −1
for all i = n(−1) −∞,
(5.2.2)
di ≤b −2
for inﬁnitely many i.
(5.2.3)
We may further assume that dn ̸= 0.
In (5.2.1) the (b-ary) point may be shifted to any other position if we compensate
for this shifting by a multiplication with a corresponding power of b. If the point is
shifted immediately to the left of the ﬁrst nonzero digit dn, the resulting expression is
referred to as the normalized b-adic representation of the number x. Zero is the only
real number that has no such representation. Conditions (5.2.2) and (5.2.3) assure
the uniqueness of the normalized b-adic representation of all other real numbers. We
employ the symbol Rb to denote all these numbers, including zero:
Rb := {0} ∪

x = ◦m · be | ◦∈{+, −}, b ∈N, b > 1, e ∈Z,
(5.2.4)
m =
∞

i=1
xib−i,
xi ∈{0, 1, . . . , b −1}, i = 1(1)∞, x1 ̸= 0,
xi ≤b −2 for inﬁnitely many i

.
Here Z denotes the set of integers, b is called the base of the representation, ◦the
sign of x (sgn(x)), m the mantissa of x (mant(x)), and e the exponent of x (exp(x)).

5.2 Floating-Point Numbers and Roundings
161
In the literature m sometimes is also called the fraction part or the signiﬁcand.
Often the sign ◦∈{+, −} is considered to be part of the mantissa.
In general the elements of Rb cannot be represented on a computer. Only truncated
versions of these elements can be so represented. The following deﬁnition distin-
guishes a special subset of Rb, which we use in the development to follow.
Deﬁnition 5.3. A real number is called a normalized ﬂoating-point number or simply
a ﬂoating-point number if it is an element of the set S = S(b, r, e1, e2):
S = S(b, r, e1, e2) :=

x ∈Rb | m =
r

i=1
xib−i, e1 ≤e ≤e2, e1, e2 ∈Z

.
In general e1 < 0 and e2 > 0. To have a unique representation of zero available in
S, we assume additionally that sgn(0) = +, mant(0) = 0.00 . . . 0, (r zeros after the
(b-ary) point), and exp(0) = 0.
The set S = S(b, r, e1, e2) is called a ﬂoating-point system.
■
In a ﬂoating-point system S the mantissa is restricted to a ﬁnite length r and the
exponent is bounded by e1 and e2.
For a ﬂoating-point system S = S(b, r, e1, e2) and its elements we have
(S3) 0, 1 ∈S ∧
x∈S
−x ∈S
and
b−1 ≤|m| < 1.3
(5.2.5)
Every element of S represents a rational number. The representation is unique. The
number of elements in S(b, r, e1, e2) is 2(b −1)br−1(e2 −e1 + 1) + 1.
The greatest ﬂoating-point number in S(b, r, e1, e2) is
B := +0. (b −1)(b −1) · · · (b −1)
)
*+
,
r−digits
be2.
The least number in S is −B. The nonzero ﬂoating-point numbers in S that are of
least absolute value are
−0.100 · · · 0 · be1,
+0.100 · · · 0 · be1.
(5.2.6)
The ﬂoating-point numbers in S are not uniformly distributed between the numbers
in (5.2.6) and −B and +B. The density decreases with increasing exponent. To
illustrate this, we use an example of a ﬂoating-point system of 33 elements. This
example is from [176] and corresponds to b = 2, r = 3, e1 = −1, and e2 = 2.
3In the binary number system the leading digit of a normalized ﬂoating-point number is always 1.
Thus, in this particular case, the binary point is frequently placed directly after the leading 1 and the
exponent is adjusted appropriately. In this case we have 1 ≤|m| < 2.

162
5 Floating-Point Arithmetic
Figure 3.6 depicts the ﬂoating-point system S(2, 3, −1, 2). A ﬂoating-point system
S(b, r, e1, e2) is a discrete subset of R.
To continue, we introduce the following notation:
R := R ∪{−∞} ∪{+∞},
S := S(b, r, e1, e2) := S(b, r, e1, e2) ∪{−∞} ∪{+∞},
S′ := [−B, +B] ⊂R.
Then S(b, r, e1, e2) is a screen of R. Further S(b, r, e1, e2) is a screen of S′. All of
these screens are symmetric, i.e., they have the property (S3).
We are now going to consider roundings from R into S(b, r, e1, e2). Similar round-
ings could also be deﬁned from S′ into S(b, r, e1, e2).
A rounding
: R →S is deﬁned by the property
(R1)

x∈S
x = x.
Other important properties of roundings are:
(R2)

x,y∈R
(x ≤y ⇒
x ≤
y)
(monotonicity),
(R3)

x∈R
x ≤x
or

x∈R
x ≤
x
(directed),
(R4)

x∈R
(−x) = −
x
(antisymmetry).
In the following development we are especially concerned with the monotone di-
rected roundings ▽and △as well as the monotone and antisymmetric roundings.
We shall consider the following roundings:

x∈R
▽x := max{y ∈S | y ≤x},
monotone downwardly directed rounding,

x∈R
△x := min{y ∈S | y ≥x},
monotone upwardly directed rounding,

x∈R
bx :=

▽x
if x ≥0
△x
if x < 0 ,
monotone rounding towards zero,

x∈R
ox :=

△x
if x ≥0
▽x
if x < 0 ,
monotone rounding away from zero.
Using the notation

x∈R,x≥0
sμ(x) := ▽x + (△x −▽x) · μ/b,
μ = 1(1)b −1,

5.2 Floating-Point Numbers and Roundings
163
we deﬁne the roundings
μ : R →S, μ = 1(1)b −1, which are in common use:

x∈[0,be1−1)
μx := 0,

x≥be1−1
μx :=

▽x
for x ∈[▽x, sμ(x))
△x
for x ∈[sμ(x), △x] ,

x<0
μx := −
μ(−x).
If b is an even number,
:=
b/2 denotes a rounding to the nearest ﬂoating-point
number. Figure 5.1 contains a diagram of this rounding for the ﬂoating-point system
S(2, 3, −1, 2) illustrated in Figure 3.6.
0
1/2
1
2
3
0
1/2
1
2
3
4
5
S
R
Figure 5.1. Rounding to the nearest ﬂoating-point number.
The roundings listed above have many familiar properties. For instance, we have
▽x = −△(−x) ∧△x = −▽(−x),
(5.2.7)
bx = sgn(x) · ▽|x| ∧
0x = sgn(x) · △|x|.
(5.2.8)
All of these roundings are monotone (R2). The roundings
μ, μ = 0(1)b are also
antisymmetric (R4).
We recall that in a linearly ordered set every monotone rounding and therefore,
in particular, all the roundings
μ, μ = 0(1)b, can be expressed in terms of the
monotone downwardly or upwardly directed rounding ▽or △(Section 3.5). Since

164
5 Floating-Point Arithmetic
△can be expressed in terms of ▽and vice versa, we give an explicit description of the
rounding ▽: R →S only. In the following description of ▽we use the abbreviation
B = 0.(b −1)(b −1) . . . (b −1) · be2 for the greatest positive ﬂoating-point number.
Then we obtain for ▽x:
▽x =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
+∞
for x = +∞,
+B
for + B ≤x < +∞,
+0.x1x2 . . . xr · be
for be1−1 ≤x < +B,
+0.000 . . . 0 · be1
for 0 ≤x < be1−1,
−0.100 . . . 0 · be1
for −be1−1 ≤x < 0,
−0.x1x2 . . . xr · be
for −B ≤x < −be1−1
∧xr+i = 0 for all i ≥1,
−0.100 . . . 0 · be+1
for −B ≤x < −be1−1
∧xi = b −1 for all i = 1(1)r
∧xr+i ̸= 0 for any i ≥1,
−(0.x1x2 . . . xr + b−r) · be
for −B ≤x < −be1−1
∧xi ̸= b −1 for any i ∈{1, 2, . . . , r}
∧xr+i ̸= 0 for any i ≥1,
−∞
for −∞≤x < −B.
Using the function [x] (the greatest integer less than or equal to x) the description
of ▽x can be shortened:
▽x =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
+∞
for x = +∞,
+B
for + B ≤x < +∞,
[m · br] · be−r
for be1−1 ≤|x| ≤+B,
+0.000 . . . 0 · be1
for 0 ≤x < be1−1,
−0.100 . . . 0 · be1
for −be1−1 ≤x < 0,
−∞
for −∞≤x < −B.
The more detailed description of ▽x above shows that a normalization may still
be necessary.
A few additional but very similar cases occur if for e < e1 the exponent e is set to
e1 and unnormalized mantissas are permitted.
Thus the implementation of the rounding ▽on a computer is simpliﬁed if the func-
tion [x] is available. In the algorithms for the implementation of computer arithmetic,
which we describe in Chapter 6, we shall use this prescription of ▽x.
In these representations for ▽x we have assumed that the mantissa of a ﬂoating-
point number is represented by the so-called signed-magnitude representation. For

5.2 Floating-Point Numbers and Roundings
165
real numbers x ≥0 the rounded value ▽x then is obtained by truncation of x after
the rth digit of the normalized mantissa m of x. If we denote this process by t(x)
(truncation), we have
▽x = t(x) for x ≥0.
This is very easy to implement. Truncation can also be used to perform the round-
ing ▽x of negative numbers x < 0 if negative numbers are represented by their
b-complement. Then the rounded value ▽x can be obtained by truncation of the
b-complement x + a of x via the process:
▽x = t(x + a) −a for x < 0
(5.2.9)
with a suitable a. See Figure 5.2.
0
x
▽x
t(x + a)
t(x + a) −a
Figure 5.2. Execution of the rounding ▽x for b-complement representa-
tion of negative numbers x < 0.
Example 5.4. We assume that the decimal number system is used, and that the man-
tissa has three decimal digits. Then we obtain for the positive real number x =
0.354672 · 103 ∈R:
▽x = t(x) = 0.354 · 103.
For the negative number x = −0.354672 · 103 we obtain obviously
▽x = −0.355 · 103.
This value is obtained by application of (5.2.9) with a = 1.00 . . . 0 · 103:
x + a = 0.645328 · 103,
t(x + a) = 0.645 · 103,
▽x = t(x + a) −a = −0.355 · 103.
Here the simple b-complement has been taken twice. In between the function t(x)
was applied which also is simple. These three steps are particularly simple if the
binary number system is used.
It is interesting that in the case of the (b−1)-complement representation of negative
numbers the monotone downwardly directed rounding ▽x cannot be executed by the
function t(x). This representation is isomorphic to the sign-magnitude representation.

166
5 Floating-Point Arithmetic
The roundings of R →S(b, r, e1, e2), which we have been discussing so far, are not
the only ones that are used in computer arithmetic or in numerical analysis. Typically
for error analyses of numerical processes, it is usually assumed that the rounding has
the property
x = x(1 −ϵ) with |ϵ| ≤ϵ∗= const.
(5.2.10)
Figure 5.3 illustrates a rounding with this property. The graph corresponding to
has to stay within a cone around the identity mapping. It is not necessarily a monotone
nor even an antisymmetric function.
S
R
Figure 5.3. A special rounding.
Because of (R1) every rounding corresponds to a step function that crosses the
graph of the identity mapping for all values in S(b, r, e1, e2). This implies, in partic-
ular, that within an interval around zero the rounding cannot be described by (5.2.10).
Compare the Figures 5.1 and 5.3.
Although equation (5.2.10) does not necessarily describe a monotone mapping,
we show in the following theorem that within the interval be1−1 ≤|x| ≤B, every
monotone rounding has this property (5.2.10).
Theorem 5.5. Let S = S(b, r, e1, e2) be a ﬂoating-point system and
: R →S
any monotone rounding. Further, let δ(x) := x −
x be the rounding error and
ϵ1 := δ(x)/x and ϵ2 := δ(x)/
x the relative rounding errors. Then

x∈R
(be1−1 ≤|x| ≤B ⇒
x = x(1 −ϵ1) with |ϵ1| ≤ϵ∗
∧x =
x(1 −ϵ2) with |ϵ2| ≤ϵ∗
∧|x −
x| < ϵ∗|x| ∧|x −
x| < ϵ∗|
x|).

5.2 Floating-Point Numbers and Roundings
167
Here ϵ* is independent of x, and
ϵ∗=

1
2b1−r
for
=
b1−r
for
̸=
.
Here
denotes the rounding to the nearest ﬂoating-point number of S, and B =
0.(b −1)(b −1) . . . (b −1) · be2.
Proof. We consider the formula that includes ϵ1. The formula with ϵ2 can be dealt
with analogously. We have
x =
x + δ(x) =
x + ϵ1x ⇒
x = x(1 −ϵ1).
With x = ◦m · be we obtain using (5.2.5) that b−1 · be ≤|x| < be.
Since |δ(x)| ≤b−r · be, we get
|ϵ1| = |δ(x)|/|x| ≤(b−r · be)/(b−1 · be) = b1−r.
It is easy to see that actually |ϵ1| < b1−r: If x = b−1 · be, then x is a screen point,
and because of (R1), δ(x) = 0. If δ(x) = b−r · be, then |x| > b−1 · be.
The case
=
can be proved analogously.
■
Theorem 5.5 furnishes error bounds for a rounding only for |x| ∈[be1−1, B]. For
|x| < be1−1, the relative error ϵ1 can – depending on the deﬁnition of the rounding
function – be identically unity or even tend to inﬁnity. For |x| > B, the relative error
ϵ1 tends to unity as x goes to inﬁnity.
Figure 5.4 shows the relative error of the rounding illustrated in Figure 5.1. For
be1−1 ≤|x| ≤B, we have |ϵ1| < ϵ∗= 1
2 · b1−r. Within this region the relative
error can only be made smaller by enlarging the number r of digits in the mantissa.
Figure 5.4 also shows that it is difﬁcult (resp. impossible) to approximate real numbers
well on the computer when they lie to the left (resp. right) of this region. For |x| <
be1−1, the relative error is unity. For |x| ≥B, it tends to unity as x goes to inﬁnity. The
only way to decrease the size of these outer regions is to decrease e1 and to increase
e2. This requires enlarging the range used for the representation of the exponent.
We summarize these results in the following two rules:
(a) To approximate better within the region be1−1 ≤|x| ≤B, more digits must be
used to represent the mantissa.
(b) To enlarge the region be1−1 ≤|x| ≤B of good approximation, more digits must
be used to represent the exponent.
If a number |x| ∈(0, be1−1) occurs in a virtual sense on a computer one speaks
of exponent underﬂow or simply of underﬂow. Correspondingly, one speaks of expo-
nent overﬂow or simply of overﬂow if |x| > B. With these concepts, the condition
be1−1 ≤|x| ≤B, which appears in Theorem 5.5, is usually expressed by saying: If
no underﬂow and no overﬂow occurs, then . . . .

168
5 Floating-Point Arithmetic
0
1
2
3
4
5
0.0
−0.2
0.2
0.4
0.6
0.8
1.0
−ϵ∗
ϵ∗
R
asymptotically against 1
Figure 5.4. Relative rounding error.
5.3
Floating-Point Operations
We now turn to consideration of arithmetic operations in ﬂoating-point systems, fol-
lowed by treatment of errors of such operations.
In Chapters 3 and 4 we established that the arithmetic in the subsets under D and
S and in all rows of Figure 1 can be deﬁned by semimorphisms. We recall that if M
is any ringoid or vectoid and N ⊆M a symmetric lower or upper screen (or both) of
M, then a semimorphism
is deﬁned by the following formulas:
(R1)

x∈N
x = x,
(R2)

x,y∈M
(x ≤y ⇒
x ≤
y),
(R4)

x∈M
(−x) = −
x,
(RG)

x,y∈N
(x ◦y :=
(x ◦y)), for ◦∈{+, −, ·, /} with y ̸= 0 for ◦= /.
In general the operations deﬁned by (RG) are not associative, nor are addition and
multiplication distributive. We show this by means of a few simple examples.
Examples. Consider the ﬂoating-point system S = S(10, 1, −1, 1). Let x, y, z ∈S.
1. Consider the operation x△
+ y := △(x + y). For x := 0.6, y := 0.5, z = 0.4, we
obtain:
(x△
+ y)△
+ z = (△1.1)△
+ 0.4 = 0.2 · 10△
+ 0.4 = △(0.24 · 10) = 0.3 · 10,
x△
+ (y△
+ z) = 0.6△
+ 0.9 = △1.5 = 0.2 · 10,
i.e., (x△
+ y)△
+ z ̸= x△
+ (y△
+ z).

5.3 Floating-Point Operations
169
2. Consider the operation x▽· y := ▽(x · y). For x := 0.3, y := 0.4, z := 0.4, we
obtain:
(x▽· y)▽· z = (▽0.12)▽· 0.4 = 0.1▽· 0.4 = 0.4 · 10−1,
x▽· (y▽· z) = 0.3▽· (▽0.16) = 0.3▽· 0.1 = 0.3 · 10−1,
i.e., (x▽· y)▽· z ̸= x▽· (y▽· z).
The same effect can be shown for nondirected roundings. For instance, let
:
R →S be the rounding to the nearest number of the screen with the property that the
midpoint between two neighboring screen numbers is rounded upwardly. Then:
3. With x := 0.7, y := 0.7, z := 0.9, we obtain for the operation x + y :=
(x + y)
(x + y) + z = (
1.4) + 0.9 = 0.1 · 10 + 0.9 =
0.19 · 10 = 0.2 · 10,
x + (y + z) = 0.7 + (
1.6) = 0.7 + 0.2 · 10 =
0.27 · 10 = 0.3 · 10,
i.e., (x + y) + z ̸= x + (y + z).
4. With x := 0.3, y := 0.4, z := 0.4, we obtain for the operation x · y :=
(x · y)
(x · y) · z = (
0.12) · 0.4 = 0.1 · 0.4 =
0.04 = 0.4 · 10−1,
x · (y · z) = 0.3 · (
0.16) = 0.3 · 0.2 =
0.06 = 0.6 · 10−1,
i.e., (x · y) · z ̸= x · (y · z).
The distributive law doesn’t hold either:
5. For x := 0.3, y := 0.7, z := 0.9, we obtain
x · (y + z) = 0.3 · (
1.6) = 0.3 · 0.2 · 10 = 0.6,
x · y + x · z = (
0.21) + (
0.27) = 0.2 + 0.3 = 0.5,
i.e., x · (y + z) ̸= x · y + x · z.
If the operations in the subset S of R are deﬁned by (RG), employing a mono-
tone rounding, then error bounds may be obtained. These are given in the following
theorem whose proof follows directly from Theorem 5.5.
Theorem 5.6. Let S = S(b, r, e1, e2) be a ﬂoating-point system,
: R →S be a
monotone rounding, and let operations in S be deﬁned by
(RG)

x,y∈S
x ◦y :=
(x ◦y) for all ◦∈{+, −, ·, /} with y ̸= 0 for ◦= /.

170
5 Floating-Point Arithmetic
If δ := x ◦y −x ◦y denotes the absolute error and ϵ1 := δ/(x ◦y) and ϵ2 :=
δ/(x ◦y) relative errors, then for all operations ◦∈{+, −, ·, /}, the following error
relations hold:

x,y∈S
(be1−1 ≤|x ◦y| ≤B ⇒x ◦y = (x ◦y)(1 −ϵ1) with |ϵ1| < ϵ∗
∧x ◦y = (x ◦y)(1 −ϵ2) with |ϵ2| < ϵ∗
∧|(x ◦y) −(x ◦y)| < ϵ∗|x ◦y|
∧|(x ◦y) −(x ◦y)| < ϵ∗|x ◦y|).
Here ϵ∗and B are the constants deﬁned in Theorem 5.5. Division is not deﬁned if
y = 0.
■
In Theorems 5.5 and 5.6, as well as in several of the following theorems, the con-
dition on the left-hand side of the ⇒sign means that neither underﬂow nor overﬂow
occurs. The error estimates in these theorems are valid in this case.
Note that the relations on the right-hand side of the ⇒sign in Theorem 5.6 are
precise quantitative statements about the errors in arithmetic. Contrast this with the
usual error estimates in numerical analysis. Those estimates have the same form as
the bounds here, but are only qualitative statements.
Theorem 5.6 is valid for all roundings
∈{▽, △,
μ, μ = 0(1)b} and in partic-
ular for all semimorphisms.
Then, as a result of the deﬁnition of the arithmetic by semimorphism in rows 2, 3,
7, 8 and 9 of Figure 1, similar error relations can also be derived. As an example, we
consider the matrix operations in row 3. The results are summarized in the following
theorem. (See also Theorem 3.9).
Theorem 5.7. Let S = S(b, r, e1, e2) be a ﬂoating-point system,
: R →S a
monotone rounding, and MnR the set of n×n matrices over R. We deﬁne a rounding
: MnR →MnS by

X=(xij)∈MnR
X := (
xij)
and operations
◦by
(RG)

X,Y ∈MnS
X ◦Y :=
(X ◦Y ), ◦∈{+, ·}.
Then

X=(xij)∈MnR
(be1−1 ≤|xij| ≤B ⇒
X =

xij(1 −ϵ1
ij)

with |ϵ1
ij| < ϵ∗
∧X = (
xij(1 −ϵ2
ij)) with |ϵ2
ij| < ϵ∗
∧|X −(
X)| < ϵ∗|X|
∧|X −(
X)| < ϵ∗|
X|).

5.3 Floating-Point Operations
171
With Z := (zij) := X ◦Y , ◦∈{+, ·}, we obtain the following error relations for
the operations:

X,Y ∈MnS
(be1−1 ≤|zij| ≤B ⇒X ◦Y = (zij(1 −ϵ1
ij)) with |ϵ1
ij| < ϵ∗
∧X ◦Y = (X ◦Y )(1 −ϵ2
ij) with |ϵ2
ij| < ϵ∗
∧|X ◦Y −(X ◦Y )| < ϵ∗|X ◦Y |
∧|X ◦Y −(X ◦Y )| < ϵ∗|X ◦Y |).
Here all absolute values are to be taken componentwise. ϵ∗and B are deﬁned as
in Theorem 5.5.
■
As before, we may note now that Theorem 5.7 holds for all roundings
∈{▽, △,
μ, μ = 0(1)b} and in particular for all semimorphisms. It is clear that corresponding
formulas hold for the matrix-vector multiplication. Corresponding theorems can also
be derived for all complex operations and for the complex matrix and vector opera-
tions. All these results lead to error relations that are especially simple by comparison
with error estimates derived on the basis of the conventional deﬁnition of arithmetic.
To illustrate this, we derive the corresponding error relations for matrix multiplication
when the latter is deﬁned by the conventional method.
The essence of the point to be made is illustrated by a comparison of the error
relations for the scalar product deﬁned by the conventional method and by semimor-
phism. Actually, the error relations for matrix multiplication derived in Theorem 5.7
deal with such scalar products if the former are written component-wise. If x = (xi)
and y = (yi) with xi, yi ∈S, i = 1(1)n, the deﬁnition of the scalar product by
semimorphism simply leads to the error relations
x · y :=
 n

i=1
xiyi

= (1 −ϵ1)
n

i=1
xiyi = (x · y)(1 −ϵ1)
with |ϵ1| < ϵ∗,
(5.3.1)
x · y =
n

i=1
xiyi = (1 −ϵ2) ·
 n

i=1
xiyi

= (x · y)(1 −ϵ2)
with |ϵ2| < ϵ∗,
(5.3.2)
and to the error bounds

n

i=1
xiyi −
 n

i=1
xiyi
 < ϵ∗

n

i=1
xiyi
 ,
(5.3.3)

n

i=1
xiyi −
 n

i=1
xiyi
 < ϵ∗

 n

i=1
xiyi
 ,
(5.3.4)

172
5 Floating-Point Arithmetic
provided that neither underﬂow nor overﬂow occurs. The relations hold for all mono-
tone roundings and in particular for all
∈{▽, △,
μ, μ = 0(1)b}. ϵ∗is deﬁned
as in Theorem 5.5. The error estimates (5.3.3) and (5.3.4) also can be written in the
simpler form |x · y −x · y| < ϵ∗|x · y| and |x · y −x · y| < ϵ∗|x · y|.
If the ﬂoating-point matrix product is deﬁned by the conventional method, the
scalar products are deﬁned by the formula
n
i=1
xi · yi = (x1 · y1) + (x2 · y2) + . . .
+ (xn · yn).
If we apply Theorem 5.6 to this relation, we obtain the following expression when
n = 4:
(x1 · y1) + (x2 · y2) + (x3 · y3) + (x4 · y4)
=
---
x1y1(1 −ϵ1) + x2y2(1 −ϵ2)
.
(1 −ϵ5) + x3y3(1 −ϵ3)
.
(1 −ϵ6)
+ x4y4(1 −ϵ4)
.
(1 −ϵ7)
= x1y1(1 −ϵ1)(1 −ϵ5)(1 −ϵ6)(1 −ϵ7)
+ x2y2(1 −ϵ2)(1 −ϵ5)(1 −ϵ6)(1 −ϵ7)
+ x3y3(1 −ϵ3)(1 −ϵ6)(1 −ϵ7)
+ x4y4(1 −ϵ4)(1 −ϵ7).
(5.3.5)
Here |ϵi| < ϵ∗for all i = 1(1)7.
The many epsilons occurring in this expression make it more complicated than the
simple formulas (5.3.1) and (5.3.2), which occur in Theorem 5.7. We may expect
that an error analysis of numerical algorithms is more troublesome if it is based on
(5.3.5) instead of (5.3.1) and (5.3.2). Moreover, in general (5.3.1) and (5.3.2) are more
accurate than (5.3.5).
We now estimate the error in (5.3.5) as a means of simplifying it and generalizing
it simultaneously. We shall use Bernoulli’s inequality
1 + nx ≤(1 + x)n for all n ∈N and all x ≥−1,
(5.3.6)
and Bernoulli’s formula
n
k

=
n!
k!(n −k)! = n · (n −1) · . . . · (n −k + 1)
1 · 2 · 3 · · · k
.
(5.3.7)
We have to estimate the expression:

n

i=1
xiyi −
n
i=1
xi · yi
 .
We prove the following simple lemma.

5.3 Floating-Point Operations
173
Lemma 5.8. If ϵ∗∈R, n ∈N and 0 ≤nϵ∗< 1 then

i=1(1)n
|ϵi| <ϵ∗
⇒
n
/
i=1
(1 −ϵi) ∈
'
1 −nϵ∗,
1
1 −nϵ∗
(
⊆
'
1 −
nϵ∗
1 −nϵ∗, 1 +
nϵ∗
1 −nϵ∗
(
.
Proof. By Bernoulli’s inequality (5.3.6) we have
1 −
nϵ∗
1 −nϵ∗≤1 −nϵ∗≤(1 −ϵ∗)n.
On the other hand we obtain
(1 −ϵ∗)n ≤
n
/
i=1
(1 −ϵi) ≤(1 + ϵ∗)n =
n

k=0
n
k
 -
ϵ∗.k
≤
(5.3.7)
n

k=0
nk-
ϵ∗.k ≤
∞

k=0
-
nϵ∗.k =
1
1 −nϵ∗
=
1 +
nϵ∗
1 −nϵ∗.
■
We now use the abbreviation
η :=
nϵ∗
1 −nϵ∗,
and distinguish two index sets:
I+ := {i | xiyi ≥0},
I−:= {i | xiyi < 0}.
Generalizing (5.3.5) we obtain
n
i=1
xi · yi =
n
i=1
xiyi(1 −ϵi1) =
n

i=1
xiyi
n
/
k=1
(1 −ϵik).
(5.3.8)
Here the formula on the right-hand side has been completed by factors (1 −ϵik) to
obtain n factors for every summand where possibly ϵik = 0 for some k.
We are now going to derive upper and lower bounds for (5.3.8).
Upper bound:
n
i=1
xi · yi =

i∈I+
xiyi
n
/
k=1
(1 −ϵik) −

i∈I−
|xiyi|
n
/
k=1
(1 −ϵik).

174
5 Floating-Point Arithmetic
Applying Lemma 5.8 and the abbreviation η we now obtain
n
i=1
xi · yi ≤

i∈I+
xiyi(1 + η) −

i∈I−
|xiyi|(1 −η)
=
n

i=1
xiyi + η

i∈I+
xiyi + η

i∈I−
|xiyi|
=
n

i=1
xiyi + η
n

i=1
|xiyi|.
Lower bound:
n
i=1
xi · yi ≥

i∈I+
xiyi(1 −η) −

i∈I−
|xiyi|(1 + η)
=
n

i=1
xiyi −η

i∈I+
xiyi −η

i∈I−
|xiyi|
=
n

i=1
xiyi −η
n

i=1
|xiyi|.
Thus we obtain for the absolute error of the conventional computation of the scalar
product the estimate:

n

i=1
xiyi −
n
i=1
xi · yi
 ≤η
n

i=1
|xiyi| =
nϵ∗
1 −nϵ∗
n

i=1
|xiyi|.
(5.3.9)
This error estimate holds as long as 0 ≤nϵ∗< 1, i.e., ϵ∗< 1/n. For the double
precision binary data format of the IEEE arithmetic standard 754, ϵ∗= 2−53. Thus
n < 9 · 1015. This are very large vector length indeed.
Since nϵ∗< 1 the factor nϵ∗/(1 −nϵ∗) on the right-hand side of (5.3.9) is greater
than nϵ∗. This, of course, is larger than the factor ϵ∗in (5.3.3) obtained for the com-
putation of the scalar product deﬁned by semimorphism.
By another error estimate the absolute error can be shown to be bounded by

n

i=1
xiyi −
n
i=1
xi · yi
 ≤(n + 0.1)ϵ∗
n

i=1
|xiyi|.
(5.3.10)
In any case these bounds for the relative error are at least n times as large as those
we obtained in (5.3.3) for the computation of the scalar product deﬁned by semimor-
phism.

5.3 Floating-Point Operations
175
For matrix multiplication deﬁned by the conventional method, error bounds may
also be obtained by the process discussed above. Letting X := (xij), Y := (yij),
Z := (zij) := X · Y , these bounds are

X,Y ∈MnS

i,k
be1−1 ≤|zik| ≤B ⇒|X · Y −X · Y | ≤
nϵ∗
1 −nϵ∗

n

j=1
|xijyjk|

if nϵ∗< 1. As before, these bounds are more than n times as large as those obtained
if the matrix multiplication is deﬁned by semimorphism. Above all the error formulas
obtained for the semimorphism are simpler. The two properties of being more accu-
rate and simpler are reproduced in the error analysis of many algorithms in numerical
analysis.
In the next chapter we will study hardware circuits for ﬂoating-point arithmetic. In
Chapter 8 hardware circuits for the computation of scalar products of ﬂoating-point
vectors will be developed. These circuits compute the exact scalar product extremely
fast, which has consequences for an error analysis of many numerical algorithms.
The error relations established for arithmetic operations so far can also be extended
to the interval rows of Figure 1. Of course, such an extension would be mainly of
theoretical interest since a basic principle of interval mathematics is that the com-
puter controls the rounding error and other errors automatically. Nevertheless, as an
example of interval arithmetic error relations, give Theorem 5.9 below. Since for all
interval rows in Figure 1, arithmetic is deﬁned by semimorphism, the validity of this
theorem follows easily by employing Theorems 5.5 and 5.6.
In order to state Theorem 5.9, we require a few concepts of interval mathematics.
See Chapter 9. The distance between the two intervals A = [a1, a2], B = [b1, b2] ∈
IR is deﬁned by
q(A, B) := max{|a1 −b1|, |a2 −b2|},
while the absolute value of the interval A is given by
|A| := q(A, [0, 0]) = max(|a1|, |a2|) = max
a∈A |a|.
If A = (Aij), B = (Bij) ∈MnIR are matrices with interval components, the distance
matrix and the absolute value matrix are deﬁned by
q(A, B) := (q(Aij, Bij)),
|A| := (|Aij|).
Theorem 5.9. Let S = S(b, r, e1, e2) be a ﬂoating-point system and let ♦: IR →IS

176
5 Floating-Point Arithmetic
be the monotone upwardly directed rounding. Then

X=[x1,x2]∈IR
-
be1−1 ≤|x1|, |x2| ≤B
⇒♦X = [▽x1, △x2] = [x1(1 −ϵ1), x2(1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧X = [x1, x2] = [▽x1(1 −ϵ1), △x2(1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧q(X, ♦X) < ϵ∗|X| ∧q(X, ♦X) < ϵ∗|♦X|
.
.
If X, Y ∈IS and Z := [z1, z2] := X ◦Y , ◦∈{+, ·, /} with 0 /∈Y for o = /, then

X,Y ∈IS
-
be1−1 ≤|z1|, |z2| ≤B
⇒X ♦
◦Y = [▽z1, △z2] = [z1(1 −ϵ1), z2(1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧X ◦Y = [z1, z2] = [▽z1(1 −ϵ1), △z2(1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧q(X ◦Y, X ♦
◦Y ) < ϵ∗|X ◦Y |
∧q(X ◦Y, X ♦
◦Y ) < ϵ∗|X ♦
◦Y |
.
.
If, moreover, a rounding ♦: MnIR →MnIS and operations ♦
◦in MnIS are deﬁned
by

X=(Xij)∈MnIR
♦X := (♦Xij),
(RG)

X,Y∈MnIS
X♦
◦Y := ♦(X ◦Y),
◦∈{+, ·},
then

X=[x(1)
ij ,x(2)
ij ]∈MnIR

ij
be1−1 ≤|x(1)
ij |, |x(2)
ij | ≤B
⇒♦X = [▽x(1)
ij , △x(2)
ij ] = [x(1)
ij (1 −ϵ1), x(2)
ij (1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧X = [x(1)
ij , x(2)
ij ] = [▽x(1)
ij (1 −ϵ1), △x(2)
ij (1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧q(X, ♦X) < ϵ∗|X| ∧q(X, ♦X) < ϵ∗|♦X|

.

5.4 Subnormal Floating-Point Numbers
177
If X, Y ∈MnIS and Z := [z(1)
ij , z(2)
ij ] := X ◦Y, ◦∈{+, ·, /}, then

X,Y∈IS

ij
be1−1 ≤|z(1)
ij |, |z(2)
ij | ≤B
⇒X♦
◦Y = [▽z(1)
ij , △z(2)
ij ] = [z(1)
ij (1 −ϵ1), z(2)
ij (1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧X ◦Y = [z(1)
ij , z(2)
ij ] = [▽z(1)
ij (1 −ϵ1), △z(2)
ij (1 −ϵ2)]
with |ϵ1|, |ϵ2| < ϵ∗= b1−r
∧q(X ◦Y, X♦
◦Y) < ϵ∗|X ◦Y|
∧q(X ◦Y, X♦
◦Y) < ϵ∗|X♦
◦Y|

.
Proof. The proof is simple. We only indicate it for IR. The matrix properties then
follow componentwise,
q(X, ♦X) = max{|x1 −▽x1|, |x2 −△x2|}
= max{|ϵ1x1|, |ϵ2x2|} < ϵ∗|X|,
q(X, ♦X) = max{|▽x1(1 −ϵ1) −▽x1|, |△x2(1 −ϵ2) −△x2|}
= max{|ϵ1▽x1|, |ϵ2△x2|} < ϵ∗|♦X|.
■
Using (RG), (R1) and (R2), we observe even more importantly than these error
bounds, that semimorphisms provide maximal accuracy in the sense that there is no
element of the screen N between the results of the operation ◦and of its approxima-
tion
◦.
5.4
Subnormal Floating-Point Numbers
In a normalized ﬂoating-point system S = S(b, r, e1, e2) an element a ∈S may have
more than one additive inverse. Examples for this have been given in Section 3.6. In
particular it was shown that the multiplicative unit 1 may have more than one additive
inverse.
Theorem 3.12 in Section 3.7 establishes a necessary and sufﬁcient condition for
the uniqueness of an additive inverse in a discrete subset of the real numbers R. In
Theorem 3.13 this result is extended to more general structures.
In the case of a ﬂoating-point system S(b, r, e1, e2) it was shown that this condition
is met if for e = e1 unnormalized mantissas are admitted in addition to the normal-
ized ﬂoating-point numbers of S(b, r, e1, e2) (Deﬁnition 5.3). This extended set of
ﬂoating-point numbers is speciﬁed by the following deﬁnition.

178
5 Floating-Point Arithmetic
Deﬁnition 5.10. Let S(b, r, e1, e2) be a ﬂoating-point system, and let
D = D(b, r, e1)
:=

x = ◦mbe1 | ◦∈{+, −}, m =
r

i=1
xib−i, xi ∈{0, 1, . . . , b −1}
0
.
The elements of D(b, r, e1) are called denormalized or subnormal numbers.
The union of S(b, r, e1, e2) and D(b, r, e1)
F = F(b, r, e1, e2) := S(b, r, e1, e2) ∪D(b, r, e1)
then forms the full ﬂoating-point system.
■
Subnormal numbers provide representations for values smaller than the smallest
normalized number. They lower the probability of an exponent underﬂow. Subnormal
numbers reduce the gap between the smallest normalized ﬂoating-point number and
zero. The addition of subnormal numbers to the normalized ﬂoating-point numbers
has been termed gradual underﬂow or graceful underﬂow. Denormalized numbers
are not included in all the designs of arithmetic units that follow the IEEE arithmetic
standard. This is mainly due to the high cost associated with their implementation.
Compared to normalized ﬂoating-point numbers, the representation and the opera-
tions for denormalized numbers require a more complex design and possibly a longer
overall execution time.
5.5
On the IEEE Floating-Point Arithmetic Standard
Early computers designed and built by Konrad Zuse, the Z3 (1941) and the Z4 (1945),
are among the ﬁrst computers that used the binary number system and ﬂoating-point
for number representation [43, 63, 500, 501, 533, 640, 641, 642].
Floating-point numbers were normalized. The mantissa was of the form “1. · · · ”
and had a value between 1 and 2. In the memory the leading 1 was not stored. It was
used as what today is called a hidden bit. Both machines carried out the four basic
arithmetic operations of addition, subtraction, multiplication, division, and the square
root by hardware. In the Z4 ﬂoating-point numbers were represented by 32 bits. 8 bits
were used for the exponent, one bit for the sign and 24 bits for the mantissa. During
execution of the arithmetic operations two additional bits were used to improve the
accuracy and to allow a correct rounding. Special representations and corresponding
wirings were available to handle the three special values: 0, ∞, and indeﬁnite (for 0/0
or ∞−∞, and others). An additional bit was used to distinguish between numbers and
these special values. If it was 0, the word represented a normal ﬂoating-point number.
If it was 1, the word represented a special value and the coding of the word decided
whether it was 0, ∞, or indeﬁnite. The least exponent was used to represent zero and

5.5 On the IEEE Floating-Point Arithmetic Standard
179
the greatest exponent to represent ∞. These early computers built by Konrad Zuse
possessed all the essential capabilities now required by the so-called IEEE ﬂoating-
point arithmetic standard.
Today’s advanced technology allows extra features such as additional word sizes
and differences in the coding and number of special cases.
There are two different IEEE standards for ﬂoating-point arithmetic: IEEE 754
(1985) [644] and IEEE 854 (1987) [645].
IEEE 754 speciﬁes formats and arithmetic for binary ﬂoating-point numbers. It
deﬁnes exceptional conditions and speciﬁes default handling of these conditions. One
purpose of the standard is to greatly simplify porting of programs. For operations
speciﬁed by the standard, numerical results and exceptions are uniquely determined
by the value of the input data, the sequence of operations, and the destination formats.
Thus, when a program is moved from one machine to another, the results of the basic
operations will be the same in every bit if both machines support the standard.
IEEE 754 is a standard with base b = 2. It deﬁnes four formats for ﬂoating-point
numbers, a 32-bit single precision and a 64-bit double precision format and extended
formats for both of these which are used for intermediate results. The single extended
format should have at least 44 bits, and the double extended format should have at
least 80 bits. The single precision format uses 24 bits for the mantissa and 8 bits for
the exponent, and the double precision format uses 53 bits for the mantissa and 11 bits
for the exponent. The formats are designed to squeeze the maximum information into
32 and 64 bits respectively. Since in the binary number system the leading digit of a
normalized ﬂoating-point number is always 1, this bit is not carried along when the
numbers are stored. It is kept as a hidden bit. This gives room to use one bit for the
sign of the mantissa in both the single and double precision format. The two extended
formats use more bits for the mantissa and the exponent than the corresponding 32-
and 64-bit formats.
The IEEE 854 standard allows either base b = 2 or base b = 10. Unlike IEEE
754 it does not specify how ﬂoating-point numbers are encoded into bits. It speciﬁes
constraints on the allowable number of bits for the representation of the mantissa for
single and double precision formats.
To avoid a sign digit for the representation of the exponent e the IEEE binary stan-
dard uses a so-called characteristic or bias c. c is chosen in such a way that the biased
exponent E is an unsigned integer,
E := c + e.
If the biased exponent is E, then the exponent of the ﬂoating-point number is e =
E −c. In the IEEE binary standard the bias is 127 for single precision and it is 1023
for double precision. The two data formats are coded as shown in Figure 5.5 where
M stands for the mantissa or signiﬁcand.

180
5 Floating-Point Arithmetic
S
E
M −1
Figure 5.5. Coding of ﬂoating-point numbers in the IEEE 754 standard.
The value of the ﬂoating-point number f is
f = (−1)S · M · 2E−c.
If S is 1 the sign of the number is negative and it is positive if S is 0.
An advantage of the particular coding of ﬂoating-point numbers with a biased ex-
ponent is that nonnegative ﬂoating-point numbers can be treated as integers for com-
parison purposes.
Table 5.1 shows the parameters of the IEEE 754 formats.
Single
Single
Double
Double
extended
extended
Word length in bits
32
≥44
64
≥80
Signiﬁcand in bits
1 + 23
≥32
1 + 52
≥64
Exponent width in bits
8
≥11
11
≥15
Bias
127
1023
Approximate range
2128 ≈3.8 · 1038
21024 ≈9 · 10307
Approximate precision
2−24 ≈1.6 · 10−7
2−53 ≈10−16
Table 5.1. The formats of the IEEE 754 standard.
The IEEE 754 standard requires that the four basic arithmetic operations of addi-
tion, subtraction, multiplication, and division are provided with four different round-
ings: rounding downwards, rounding upwards, rounding toward 0, and rounding to
the nearest ﬂoating-point number. In the latter case it is additionally required that
the midpoint between two adjacent ﬂoating-point numbers is rounded in a way that
the result has a least signiﬁcant digit which is even. This rounding scheme is called
round-to-nearest-even.
The IEEE 754 standard uses denormalized numbers when the exponent is emin,
i.e., denormalized mantissas are permitted. One reason for this may be that denormal-
ized numbers guarantee the uniqueness of additive inverses (Theorem 3.12 in Sec-
tion 3.7), i.e.,
x −y = 0 ⇔x = y.
Denormalized numbers are called subnormal in 854 and denormal in 754. One
also speaks of gradual underﬂow or graceful underﬂow. Providing for denormalized
numbers in the hardware arithmetic unit makes the unit more complex, more costly,
and possibly slower. Many arithmetic units that follow the IEEE arithmetic standard,
therefore, do not support denormalized numbers directly but leave their treatment to

5.5 On the IEEE Floating-Point Arithmetic Standard
181
software. Even hardware designs that implement denormalized numbers allow the
programmer to avoid their use if faster execution is desired.
During a computation exceptional events like overﬂow or division by zero may
occur. For such events the IEEE standard reserves some bit patterns to represent
special quantities. It speciﬁes special representations for +∞, −∞, +0, −0, and
for NaN (not a number). Traditionally, an overﬂow or division by zero would cause
a computation to be interrupted. There are, however, examples for which it makes
sense for a computation to continue. In IEEE arithmetic the general strategy upon an
exceptional event is to deliver a result and continue the computation. This requires
the result of operations on or resulting in special values to be deﬁned. Examples are:
4/0 = ∞, −4/0 = −∞, 0/0 = NaN, ∞−∞= NaN, 0 · ∞= NaN, ∞/∞= NaN,
1/(−∞) = −0, 3/(+∞) = −0, log 0 = −∞, log x = NaN when x < 0, 4 −∞=
−∞. When an NaN participates in a ﬂoating-point operation, the result is always an
NaN.
The purpose of these special operations and results is to allow programmers to
postpone some tests and decisions to a later time in the program when it is more
convenient. When an exceptional event like division by zero or overﬂow occurs during
execution of a program and the computation is continued the user must be informed
about this circumstance. Status ﬂags serve this purpose. The IEEE standard makes a
distinction between ﬁve classes of exceptions: overﬂow, underﬂow, division by zero,
invalid operation (like 0 · ∞or ∞−∞), and inexact. There is one status ﬂag for each
of the ﬁve exceptions. When any exception occurs the corresponding status ﬂag is
set. To help users to analyse exceptions in a program the IEEE standard recommends
so-called trap handlers be installed. For more details see [187, 217, 309, 461].
The IEEE ﬂoating-point arithmetic standards 754 and 854 have been under revision
for some time. It can be expected that in the future there will be only one Standard for
Floating-Point Arithmetic P 754. The present draft speciﬁes formats and methods for
binary and decimal ﬂoating-point arithmetic in computer programming environments.
Exception conditions are deﬁned and default handling of these conditions is speciﬁed.
The new standard IEEE P 754 speciﬁes 4 binary and 3 decimal ﬂoating-point for-
mats. Table 5.2 shows the parameters of the IEEE P 754 formats.
b = 2
b = 2
b = 2
b = 2
b = 10
b = 10
b = 10
Word length
16
32
64
128
32
64
128
Signiﬁcand
1 + 10
1 + 23
1 + 52
1 + 112
7
16
34
Exponent width
5
8
11
15
11
13
17
emax
+15
+127
+1023
+16383
+96
+384
+6144
emin
−14
−126
−1022
−16382
−95
−383
−6143
Bias
15
127
1023
16383
101
398
6176
Table 5.2. The binary and decimal formats of the IEEE P 754 standard.

182
5 Floating-Point Arithmetic
For the binary 32 and 64 bit format and for the decimal 64 bit format also extended
(so-called non-interchange) formats are speciﬁed.
The current Draft Standard for Floating-Point Arithmetic P 754 does not explicitly
require that each of the operations with directed roundings ▽
+ , ▽
−, ▽
× , ▽
/
and △
+ ,
△
−, △
× , △
/
is provided by a distinct operation code on future processors. With respect
to rounded operations the current draft explicitely states: “Every operation shall be
performed as if it ﬁrst produced an intermediate result correct to inﬁnite precision and
with unbounded range, and then rounded that result according to one of the (rounding)
modes.” The mode is determined by the value of a mode variable which assumingly
has to be set before the operation is executed. This allows the conclusion that again
the rounding will be separated from the arithmetic operation on future processors that
conﬁrm with the IEEE P 754 ﬂoating-point arithmetic standard. This raises the fear
that interval arithmetic will again be slow and not in balance with the fast ﬂoating-
point arithmetic on future processors.
An exact multiply and accumulate instruction or equivalently an exact scalar prod-
uct is not required in the present draft of the IEEE P 754 standard.
Comments
For elementary binary ﬂoating-point computation the IEEE ﬂoating-point arithmetic
standard, adopted in 1985, is undoubtedly thorough, consistent, and well deﬁned
[644]. It has been widely accepted and has been used in almost every processor devel-
oped since 1985. This has greatly improved the portability of ﬂoating-point programs.
However, computer technology has been dramatically improved since 1985. Arith-
metic speed has gone from megaﬂops to gigaﬂops to teraﬂops, and it is already ap-
proaching the petaﬂops range. This is not just a gain in speed. A qualitative difference
goes with it. At the time of the megaﬂops computer a conventional error analysis was
recommended in every numerical analysis textbook. Today the PC is a gigaﬂops com-
puter. For the teraﬂops or petaﬂops computer conventional error analysis is no longer
practical. An avalanche of numbers is produced when a petaﬂops computer runs. If
the numbers processed in one hour were to be printed (500 on one page, 1000 on one
sheet, 1000 sheets 10 cm high) they would need a pile of paper that reaches from
the earth to the sun and back. Computing indeed has already reached astronomical
dimensions!
This brings to the fore the question of whether the computed result really solves the
given problem. The only way to answer this question is by using the computer itself.
Every ﬂoating-point operation is potentially in error. The capability of a computer
should not be judged by the number of operations it can perform in a certain amount
of time without asking whether the computed result is correct. It would be much
more reasonable to ask how fast a computer can compute correctly to 3, 5, 10 or 15
decimal places for certain problems. If the question were asked that way, it would
very soon lead to better computers. Mathematical methods that give an answer to this

5.5 On the IEEE Floating-Point Arithmetic Standard
183
question are available for very many problems. Computers, however, are at present
not designed in a way that allows these methods to be used effectively.
During the last several decades, methods for very many problems have been devel-
oped which allow the computer itself to validate or verify its computed results. See
the literature list and Chapter 9 of this book. These methods compute unconditional
bounds for a solution and can iteratively improve the accuracy of the computed re-
sult. Very few of these veriﬁcation techniques need higher precision ﬂoating-point
arithmetic. Fast double precision ﬂoating-point arithmetic is the basic arithmetical
tool.
Two additional arithmetical features are fundamental and necessary:
I. fast hardware support for extended4 interval arithmetic and
II. a fast and exact multiply and accumulate operation or,
what is equivalent to it, an exact scalar product.5
The IEEE standards seem to support interval arithmetic. They require the basic
four arithmetic operations to have rounding to nearest, towards zero, and downwards
and upwards. The latter two are essential for interval arithmetic. But almost all pro-
cessors that provide IEEE arithmetic separate the rounding from the basic operation,
which proves to be a severe drawback. In a conventional ﬂoating-point computation
this does not cause any difﬁculties. The rounding mode is set only once. Then a
large number of operations is performed with this rounding mode, one in every cycle.
However, when interval arithmetic is performed the rounding mode has to be switched
very frequently. The lower bound of the result of every interval operation has to be
rounded downwards and the upper bound rounded upwards. Thus, the rounding mode
has to be reset for every arithmetic operation. If setting the rounding mode and the
arithmetic operation are equally fast this slows down the computation of each bound
unnecessarily by a factor of two in comparison to conventional ﬂoating-point arith-
metic. On almost all existing commercial processors, however, setting the rounding
mode takes a multiple (three, ﬁve, ten) of the time that is needed for the arithmetic
operation. Thus an interval operation is unnecessarily at least eight (or twenty and
even more) times slower than the corresponding ﬂoating-point operation not counting
the necessary case distinctions for interval multiplication and interval division. This is
fatal for interval arithmetic. The rounding should be an integral part of the arithmetic
4including division by an interval that includes zero
5To achieve high speed all conventional vector processors provide a ‘multiply and accumulate’ in-
struction. It is, however, not accurate. By pipelining, the accumulation (continued summation) is ex-
ecuted very swiftly. The accumulation is done in ﬂoating-point arithmetic. The pipeline usually has
four or ﬁve stages. What comes out of the pipeline is fed back to become the second input into the
pipeline. Typically, four or ﬁve sums are built up independently before being added together. This
so-called partial sum technique alters the sequence of the summands and causes errors in addition to
the usual ﬂoating-point errors. A vectorizing compiler uses this ’multiply and accumulate’ operation
within a user’s program as often as possible, since it greatly speeds up the execution. Thus the user loses
complete control of his computation.

184
5 Floating-Point Arithmetic
operation. Every one of the rounded arithmetic operations with rounding to nearest,
downwards or upwards should be equally fast.
For interval arithmetic we need to be able to call each of the operations ▽
+ , ▽
−,
▽
× , ▽
/
and △
+ , △
−, △
× , △
/
as one single instruction, that is, the rounding must
be inherent to each. Therefore in programming languages notations for arithmetic
operations with different roundings should be provided. They could be:
+,
-,
*,
/
for operations with rounding to the nearest ﬂoating-point
number,
+>,
->,
*>,
/>
for operations with rounding upwards,
+<,
-<,
*<,
/<
for operations with rounding downwards, and
+|,
-|,
*|,
/|
for operations with rounding towards zero (chopping).
New operation codes are necessary! Implementation of fast interval arithmetic is
discussed in Chapter 7 of this book.
Frequently used programming languages regrettably do not provide four plus, mi-
nus, multiply, and divide operators for ﬂoating-point numbers. This, however, does
not justify generally separating the rounding from the arithmetic!
Instead, a future ﬂoating-point arithmetic standard should require that every future
processor shall provide the 16 operations listed above. A future standard even can and
should dictate names for the corresponding assembler instructions as for instance:
addp, subp, mulp, divp, addn, subn, muln, divn, addz, subz, mulz, and divz. Here p
stands for rounding toward positive, n for rounding toward negative, and z for round-
ing toward zero. With these operators interval operations can easily be programmed.
They would be very fast and fully transferable from one processor to another.
This is exactly the way interval arithmetic is provided in the C++ class library C-
XSC which has been successfully used for more than 15 years [206, 288]. Since C++
only allows operator overloading, the operations with the directed roundings cannot
and are not provided in C-XSC. These operations are rarely needed in applications.
If they are needed, interval operations are performed instead and the upper or lower
bound of the result then gives the desired answer.
There is a need for this older mechanism where ﬁrst the rounding mode has to be
set and then the arithmetic operation is carried out, in particular for those applications
where the rounding mode is to be selected randomly. In this case the status register
should provide a new mode for choosing the rounding mode randomly (by a hard-
ware random number generator). Then all standard rounding operations would be
performed with roundings that change at random. This would not have any effect on
the arithmetic operations which have their rounding speciﬁed.
To obtain close bounds for a solution, interval arithmetic has to be combined with
defect correction or iterative reﬁnement techniques. To be effective these techniques
require an exact multiply and accumulate instruction or, equivalent to this, an exact
scalar product. It is realized by accumulating products of the full double length into
a wide ﬁxed-point register. This ﬁxed-point accumulation is completely free of trun-

5.5 On the IEEE Floating-Point Arithmetic Standard
185
cation error. Fast hardware circuitry for an exact multiply and accumulate instruction
for all kinds of computers is discussed in Chapter 8 of this book.
A very natural pipelining of the multiply and accumulate instruction leads to very
fast and simple circuits. The stages: loading the data, computing, shifting, and accu-
mulation of the products are performed in one pipeline. Furthermore ﬁxed-point ac-
cumulation of the products is simpler than accumulation in ﬂoating-point arithmetic.
Many intermediate steps that are executed in a ﬂoating-point accumulation such as
normalization and rounding of the products and of the intermediate sum, composition
into a ﬂoating-point number and decomposition into mantissa and exponent for the
next operation, do not occur in the ﬁxed-point accumulation. It is simply performed
by a shift and addition of the products of full double length into a wide ﬁxed-point
register. This brings a considerable speed increase compared to a possibly wrong ac-
cumulation of the scalar product using conventional ﬂoating-point arithmetic. The
hardware expenditure for it is comparable to that for a fast multiplier with an adder
tree, accepted years ago. All that is actually needed is a tiny local memory on the
arithmetic unit of about 1K bytes. We really can and we should afford this at a time
where computer memory is measured in gigabytes. The arithmetic itself is not much
different to what is available on a conventional CPU. Fixed-point accumulation is
error free!
With a fast and exact multiply and accumulate instruction, fast quadruple or multi-
ple precision arithmetic can also be easily provided. A multiple precision number is
represented as an array of ﬂoating-point numbers. The value of this number is the sum
of its components. It can be represented in the wide ﬁxed-point register. Addition and
subtraction of multiple precision variables or numbers can easily be performed in this
register. Multiplication of two such numbers is simply a sum of products of ﬂoating-
point numbers. Details are developed in Chapter 9 of this book.
Interval arithmetic can bring guarantees into computation while an exact multiply
and accumulate instruction can bring high accuracy via defect correction methods and
at high speed. It also is the key operation for fast multiple precision arithmetic for real
and interval data. See Chapter 9 of this book.
Fast and accurate hardware support for I. and II. must be added to conventional
ﬂoating-point arithmetic. Both are necessary extensions. Instead of the computer
being merely a fast calculating tool, they would turn it into a scientiﬁc instrument for
mathematics. Computing that is continually and greatly speeded up makes this step
necessary and it is that very speed that calls conventional computing into question.
Of course, the computer would often have to do more work to obtain veriﬁed re-
sults. But the mathematical safety should be worth it. The step from assembler to
higher programming languages or the use of convenient operating systems also con-
sumes a lot of computing power and nobody complains about it. Fast computers in
particular are often used for safety critical applications. Severe, expensive, and tragic
accidents can occur if the eigenfrequencies of a heavy electricity generator, for in-

186
5 Floating-Point Arithmetic
stance, are erroneously computed, or if a nuclear explosion is incorrectly simulated.
The IEEE standards 754 and 854 and also the expected future standard P 754 are
very conservative.
Multiply and add fused is the only operation which goes be-
yond Zuse’s elementary ﬂoating-point arithmetic. Otherwise, the standards merely
extend the word size. This, however, cannot keep up with the dramatic gain in com-
puter speed. Computations will soon need 1020 arithmetic operations and more. The
tremendous progress in computer technology and the great increase in computer speed
needs to be accompanied by extension of the mathematical capacity of the computer.
Advanced computer arithmetic as developed in this book takes a big stride for-
ward. It provides a large number of additional high quality arithmetic operations and
it supplies high speed tools for checking the quality of intermediate results and for
dynamically extending the precision in critical situations. Basic tools to verify com-
puted results ought to be standard equipment on every computer in the 21st century.
Computer arithmetic should go far beyond elementary ﬂoating-point arithmetic for a
few ﬁxed precisions.
Conclusion
In simplicity lies truth. The following requirements should be included in a future
computer arithmetic standard:
(i) A well-deﬁned double precision ﬂoating-point arithmetic.
(ii) Fast and direct hardware support for double precision interval arithmetic. See
Chapter 7 of this book.
(iii) A fast and exact multiply and accumulate (i.e., continued addition) operation or,
what is equivalent to it, an exact scalar product for the double precision format.
It is the basic tool to achieve high speed dynamic (multiple) precision arithmetic
for real and for interval data. Pipelining gives it high speed and exactitude brings
very high accuracy into computation. See Chapters 8 and 9 of this book.
Of course, elementary functions with proven and reliable a priori error bounds are
necessary as part of computer arithmetic. In C-XSC elementary functions are avail-
able for multiple precision arithmetics with large exponent ranges for real, interval,
complex and complex interval data, see [331].
For problems which require an extremely large exponent range the 128-bit arith-
metic of the proposed IEEE P 754 ﬂoating-point arithmetic standard may be useful.
However, more ﬂexibility is highly desirable for future computing. Fast pipelined
hardware implementation of an exact scalar product for the double precision format is
no more complex than implementing a full 128-bit ﬂoating-point arithmetic. Multiple
precision arithmetic as developed in Section 9.7 fully beneﬁts from a high speed exact
scalar product. The exponent range of the double precision format is not a limitation
for multiple precision arithmetics. An exponent part can easily be added as a scaling
factor, see Section 9.7.4 and [331].

Chapter 6
Implementation of Floating-Point Arithmetic on a
Computer
In this chapter we deal with the implementation of arithmetic on a computer,
and in particular by means of a ﬂoating-point screen S. The implementation
will be described for all operations and for all rows displayed in Figure 1.
Floating-point arithmetic is based on integer arithmetic. Integer arith-
metic is not a focus of this book. Nevertheless we review some basic aspects
of integer arithmetic on computers in Section 6.1 of this chapter.
In subsequent sections we consider ﬂoating-point arithmetic. We split
the implementation into two stages or levels with the details of level 2 based
on level 1. The level 1 routines include different ﬂoating-point operations
for the ﬁrst row of Figure 1. These operations are deﬁned by formula (RG)
for all roundings of the set {▽, △,
μ, μ = 0(1)b}. The level 2 routines
then describe the operations deﬁned in all the other rows of Figure 1.
With the very powerful computer technology that is available today,
arithmetic on a computer, including ﬂoating-point arithmetic, will in general
be supplied by the computer hardware. Vendors have developed sophisti-
cated ideas and circuits to implement arithmetic on a computer. Discussion
of these techniques is beyond the scope of this book.
Instead we give a detailed algorithmic description of the implementa-
tion of ﬂoating-point arithmetic. We show that this process can be split up
into several independent routines, which include routines for approxima-
tion of the arithmetic operations, for normalization, and for the different
roundings. We discuss most of these routines for two different kinds of ac-
cumulator, which we call the long and the short. The latter represents the
minimum requirement. Chapter 8 of this book is devoted to the computa-
tion of the scalar product. There very fast hardware circuits are developed
which compute the scalar product of two vectors with ﬂoating-point com-
ponents exactly or with only a single rounding. This operation is used in
the arithmetic of many rows of Figure 1. We treat this method as a level 1
operation.
The last section of this chapter contains a brief discussion of the level 2
routines. We simply summarize the deﬁnition of the operations in the dif-
ferent rows of Figure 1 and point out that they all can be performed by using
the level 1 operations. These ideas have already been extensively studied in

188
6 Implementation of Floating-Point Arithmetic on a Computer
preceding chapters. We don’t derive algorithms for these operations because
they are simple and offer no difﬁculties in principle provided the level 1 rou-
tines are available and the operations are clearly deﬁned.
6.1
A Brief Review on the Realization of Integer Arithmetic
This part of the book deals with ﬂoating-point arithmetic. Floating-point arithmetic is
based on integer arithmetic. The implementation of integer arithmetic on computers
is only of secondary signiﬁcance here. Good books on the topic, for instance [309],
are available. However, some basic appreciation of integer arithmetic on computers
is needed to fully understand this and the following two chapters. We provide it in
this section. We are not aiming for completeness. Nor are we discussing the most
advanced technologies.
A few basic logical operators are the building blocks for integer arithmetic. Elec-
trical circuits that realize these operators are called gates. The three operators and,
or, and not form a complete operator system. These operators are deﬁned by the
following table.
a ∧b
a ∨b
¬a
a
b
(and)
(or)
(not)
0
0
0
0
1
0
1
0
1
1
1
0
0
1
0
1
1
1
1
0
Table 6.1. Deﬁnition of the logical operators and, or, and not.
We shall represent these operators by the symbols shown in Figure 6.1.
a ∨b
a
b
or
a
¬a
not
a
b
a ∧b
and
Figure 6.1. Symbols for the logical operators and, or, and not.
Occasionally generalizations of these operators with more input entities will be
used, see Figure 6.2.
When a switch is used as basic bi-stable element with the state 0 if it is open, and
1 if it is closed, the logical operation or can be obtained by putting two switches in
parallel and the operation and can be obtained by putting two switches in series. See
Figure 6.3.

6.1 A Brief Review on the Realization of Integer Arithmetic
189
a1
a2...
an
a,
a = 0 ⇔
n
i=1
(ai = 0),
i.e., a = 1, if at least for one i, ai = 1.
...
a,
a = 1 ⇔
n
i=1
(ai = 1),
a1
a2
an
and
or
Figure 6.2. Symbols for and and or operators.
open
closed
logical or
logical and
Figure 6.3. Realization of an and and an or gate by switches.
The British mathematician George Boole proved in 1854 in his Treatise on the Laws
of Human Thought that every logical function f of n logical variables v1, v2, . . . , vn
can be fully described by an expression consisting of the logical values true and false
and the three logical operators and, or, and not. Thus these three logical operators are
called a complete operator system.
Instead of the logical operator system and, or and not, other logical operators can be
used as a complete operator system. It is interesting that among the various complete
operator systems, either of the two functions nand (¬(x ∧y) and nor ( ¬(x ∨y)) by
itself form a complete operator system. We leave it to the reader to prove that the
logical operators and, or, and not can be expressed by nand and nor alone.
For binary numbers arithmetic operators can be built with the logical operators and,
or, and not as well as with any other complete operator system. Elementary building
blocks for arithmetic operators are the half adder (HA) and the full adder (FA). The
half adder takes two binary inputs and produces two binary outputs, the sum digit (s),
and the carry digit (c). A symbol for the half adder, a simple circuit and an analysis
are given in Figure 6.4.
The full adder also produces two binary outputs, the sum digit (s) and the carry
digit (c) from three binary inputs a1, a2, a3. Its symbol, realization and analysis are
shown in Figure 6.5.

190
6 Implementation of Floating-Point Arithmetic on a Computer
HA
c
s
a
b
a
0
1
0
1
b
0
0
1
1
c = a ∧b
0
0
0
1
¬c
1
1
1
0
a ∨b
0
1
1
1
s
0
1
1
0
a
b
c
s
Figure 6.4. Half adder.
a1 a2 a3
a1
0
1
0
1
0
1
0
1
a2
0
0
1
1
0
0
1
1
a3
0
0
0
0
1
1
1
1
s′
0
1
1
0
0
1
1
0
c′
0
0
0
1
0
0
0
1
c′′
0
0
0
0
0
1
1
0
s
0
1
1
0
1
0
0
1
c
0
0
0
1
0
1
1
1
a1
a2
a3
c
s
c′
HA
HA
s′
c′′
FA
s
c
Figure 6.5. Full adder.
The half and full adders are themselves building blocks for the addition of n-bit-
numbers (binary words). The simplest such adder is the serial adder. It computes the
sum s of two summands
a =
n−1

i=0
ai · 2i
and
b =
n−1

i=0
bi · 2i,
ai, bi ∈{0, 1}, i = 0(1)n −1,
using only one full adder. The summands a and b are placed into two registers as
shown in Figure 6.6. The a-register is called the accumulator (AC). The addition
begins with the least signiﬁcant digits a0 and b0. Then the summands a and b are
shifted one unit to the right and the sum digit of a0 + b0 is written into the empty
space at the left end of the a-register. A possible carry is written into the carry register
which is shown below the full adder in Figure 6.6. Continuing in this way, in each
cycle a pair of digits ai and bi of a and b are shifted into the adder. There they are
added together with the carry ci−1 from the addition of the less signiﬁcant digits ai−1
and bi−1. Thus the sum s = a + b is built up in AC. The ﬁnal sum s may have one

6.1 A Brief Review on the Realization of Integer Arithmetic
191
more digit than the summands a and b. It is the carry digit of the addition an−1+bn−1.
n cycles are needed to compute the ﬁnal sum by the serial adder.
a
ci−1
FA
ci−1
ai
bi
b
AC
Figure 6.6. The serial adder.
The addition of two n-bit binary words can be done much faster by a parallel adder.
Here instead of only one, n full adders are used as shown in Figure 6.7.
an−1 bn−1
FA
sn
sn−1
an−2 bn−2
FA
sn−2
cn−2
a1
b1
FA
s1
c1
a0
b0
HA
s0
c0
cn−3
Figure 6.7. Parallel adder.
In the parallel adder, the adder for the least signiﬁcant bits may be a half adder. sn
is the possible carry of the addition of the most signiﬁcant bits an−1 and bn−1 with
the carry cn−2.
For large n the parallel adder just described is not optimal with respect to comput-
ing time. Each gate needs its own run time and the many carries can make the addition
quite slow.
Many ways to speed up the addition have been developed. We consider only one of
these, the carry-select-adder. The idea is to subdivide the adder into several sections.
The addition time for each of these sections then is much shorter. But carries may
occur between adjacent sections. These carries are not known when the addition is
started. Therefore for all but one adder section duplicate adders are provided. One
of them adds the input data with an assumed input carry of zero, the other with an
assumed input carry of one. Now all the adder sections can perform their additions
simultaneously. Then the result is selected by the output carries of each adder section.
See Figure 6.8.
The duplication within the adder sections in a carry-select-adder does not double
the number of gates needed. The two halves of an adder section use nearly the same
input data. Their data differ only in the input carry. Thus a look at the circuit for the
full adder shows that most of the gates can be used for both halves of an adder section.

192
6 Implementation of Floating-Point Arithmetic on a Computer
0
L
0
L
0
L
Figure 6.8. Carry-select-adder.
An even simpler parallel adder is the John von Neumann adder. It is the oldest
parallel adder. Instead of full adders only half adders are used. See Figure 6.9.
HA
s
an−1
bn−1
HA
s
an−2
c
bn−2
HA
c
b2
a2
s
a0
c
HA
s
c
b0
HA
s
c
b1
a1
c−1
Figure 6.9. The John von Neumann adder.
The addition is performed in several cycles. After each cycle the sum exit of the half
adders is returned into the a-register (the accumulator) while the carry is forwarded
to the next more signiﬁcant position of the b-register. After the ﬁrst cycle b0 is set to
zero and a0 is the least signiﬁcant digit of the sum. After the second cycle b1 is zero
and a1 is the next more signiﬁcant digit of the sum, and so on. The addition is ﬁnished
if all digits bi, i = 0(1)n −1, are zero. After at most n cycles this is the case. Then
the a-register (accumulator) contains the sum. A possible carry cn−1 is the nth digit
of the sum.
Another frequently used adder is the carry-save-adder, the CSA for short. Basi-
cally it consists of an array of full adders. See Figure 6.10.
FA
FA
FA
FA
FA
FA
FA
Figure 6.10. Arrangement of full adders in a carry save adder.
A full adder has three binary inputs and two binary outputs, the sum and the carry
digit. Thus a CSA accepts three binary words as input and it produces two binary
words as output. Since no carry propagation is performed between adjacent full adders
the response time is very short. For brevity we shall use the following symbol for a
CSA:
CSA

6.1 A Brief Review on the Realization of Integer Arithmetic
193
If the output of a CSA is fed back into the input registers a and b (as in the John von
Neumann adder), the unit can be used to add in a new summand in each cycle.
CSA
d
b
a
Figure 6.11. Continued addition by a carry save adder.
The runtime to add m summands s1, s2, . . . , sm still can be shortened if several
CSAs are used which are arranged in a chain. This saves the feedback time of the
result into the a- and b-registers. The resulting adder circuit consists of m −2 CSAs,
each one consisting of n full adders. The output of every CSA together with a new
summand is the input for the next CSA in the chain. For the addition of the two
remaining summands (the output of the last CSA in the chain) a fast parallel adder
has to be used.
s1
s3
s2
s4
s5
sm
CSA
CSA
CSA
CSA
fast parallel adder
Figure 6.12. Addition of m summands by a carry save adder chain.

194
6 Implementation of Floating-Point Arithmetic on a Computer
A further speed up could be obtained if several of the adders would work in par-
allel. The result is a tree structure of CSAs, an adder tree or Wallace tree ([604]).
This structure is frequently used for fast multiplication where several multiples of one
factor have to be added.
Figure 6.13 shows an adder tree for m = 19 summands. m −2 = 17 CSAs are
needed as for a chain. But the number of stages is reduced from 17 to 6.
While the number of CSAs and the number of stages in an adder tree are ﬁxed,
the topology is not. No carries are propagated in an adder tree. The response time,
therefore, is very short. An adder tree is just an electrical network. Often the response
time of the tree is shorter than the time needed for the ﬁnal addition by a parallel
adder.
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
CSA
fast parallel adder
Figure 6.13. Adder tree for the addition of 19 binary words.
For fast multiplication many sophisticated methods have been developed. One
method ﬁrst computes multiples of the multiplicand (possibly in parallel) and then
accumulates these in parallel.
Here we are going to discuss a very simple but straightforward multiplier and on
the way mention techniques to speed up the multiplication. We assume that two n-
digit binary numbers are to be multiplied. The product then has 2n digits. The simple
multiplier is shown in Figure 6.14.
The two factors to be multiplied are placed in the registers MD for the multiplicand
and MR for the multiplier. The multiplication begins with the least signiﬁcant digit
a0 of MR.
The content of MD is multiplied by a0 and the product is added to the accumulator
AC. Then the combined register (AC,MR) is shifted one unit to the right. Now a1 is

6.1 A Brief Review on the Realization of Integer Arithmetic
195
bn−1
. . .
b1 b0
fast parallel adder
MD
AC
MR
an−1
an−1
a1
a2
a0
a1
an−1
...
...
Figure 6.14. A simple multiplier.
in the position of the least signiﬁcant digit and the procedure is repeated. After n
such steps the product of double length appears in the combined register (AC,MR). A
slashed line in Figure 6.14 indicates parallel data transfer.
The run time of this multiplication method can be shortened if an adder is provided
for each digit of the multiplier. The adders are arranged in a chain. Each adder
produces an n-digit sum and a carry bit. The least signiﬁcant digit is a digit of the
product. The upper n −1 digits and the carry bit of the sum are passed along to the
next adder. Compared to the method shown in Figure 6.14, this arrangement saves the
shift and the feedback time of the intermediate sum Figure 6.15.
Figure 6.15. Fast multiplication.
If in any multiplication method the multiplier contains a block of k zeros, a shift by
k digits can speed up the multiplication. In the case of a block of several ones in MR

196
6 Implementation of Floating-Point Arithmetic on a Computer
one can proceed as follows:
00 · · · 001
v1 · · · 11
u0 · · · 0 =
v

i=u
2i = 2v+1 −2u.
Thus the v −u + 1 additions of the multiplicand can be replaced by a subtraction
in the position u and an addition in the position v + 1.
Large multipliers can also be built by making use of very fast smaller ones.
A very fast method of computing a product certainly is obtained if small multiples
(by factors 2 or 3) of the multiplicand are added by an adder tree.
For division, various methods have also been developed and are in use.
One
method, of course, is the conventional approach which uses addition/subtraction and
shifting. If proper positioning is assumed the standard method of computing x/y
works as follows: Subtract y from x until what remains becomes negative. The num-
ber of subtractions minus one is the ﬁrst digit of the quotient. Then shift y by one
position to the right (or what remained by one position to the left). Now add y until
what remains becomes positive. The second digit of the quotient is base b minus the
number of these additions. Now again shift y by one position to the right (or what
remains by one position to the left), and so on.
Another frequently applied method is to compute the reciprocal of the divisor by
Newton’s method and then the quotient by a fast multiplication. See, for instance,
[309].
6.2
Introductory Remarks About the Level 1 Operations
The level 1 operations are designed as building blocks for the operations in the product
sets and interval sets displayed in Figure 1. These operations include, in particular, all
the operations of row 1 of Figure 1 for different roundings, including those operations
which, in a narrower interpretation, are often called ﬂoating-point arithmetic. We
develop algorithms for these operations in the following sections. To some extent the
details of these algorithms depend on the technology being used.
Tremendous progress in computer technology has been made since the invention
of the microprocessor in the early 1970s, and further advances can be expected for
the future. The progress is expressed and becomes evident in measures like the clock
frequency, the number of transistors on a chip, the memory size and the memory
hierarchy, but also in the power of the design tools.
An early microprocessor was built on a chip with a few thousand transistors. It
was running at 1 or 2 MHz and it provided an 8 bit adder. If at all, ﬂoating-point
arithmetic could only be implemented in software by loops. As a simple consequence
multiplication and division took signiﬁcantly more computing time than ﬂoating-point
addition and subtraction.

6.2 Introductory Remarks About the Level 1 Operations
197
Today a microprocessor chip can carry 100 million transistors and more. It runs
at 4 GHz. Floating-point operations are implemented in hardware for word sizes of
64, 80, and even more bits. Operations such as addition, subtraction, multiplication,
division, and also certain compound operations like multiply and add fused can be
delivered every cycle. High computing speed is supported by a memory hierarchy
consisting of register memory, cache memory, main memory, and external memory.
Under these circumstances vendors have developed sophisticated ideas and circuits
to implement ﬂoating-point arithmetic. Discussion of these techniques is beyond the
scope of this book.
It is, however, important to understand the basic principles, that semimorphic op-
erations can be implemented for all rows of Figure 1, and what the minimum require-
ments for a given word size are. Computer hardware should more and more support
arithmetic operations in the product spaces. The mathematical formulas for their re-
alization are developed and listed in this chapter. Details of operations such as those
for intervals and vectors and matrices are considered in the next two chapters.
We now turn to a detailed description of the implementation of what is convention-
ally called ﬂoating-point arithmetic on computers. Although nowadays on practically
all computers ﬂoating-point operations are made available in hardware, we do not dis-
cuss particular hardware circuits here. Nor do we assume a particular representation
of ﬂoating-point numbers. We conﬁne the description to the logic ﬂow using detailed
ﬂow diagrams.
We assume that ﬂoating-point numbers are elements of the ﬂoating-point system
S = S(b, r, e1, e2) and that they are given in the so-called signed-magnitude repre-
sentation:
x = mant(x) · bexp(x)
with mant(x) = ◦r
i=1 xib−i. We assume here, that the mantissa carries the sign of
the number, ◦∈{+, −}, x1 ̸= 0, and e1 ≤exp(x) ≤e2 with e1 < 0 and e2 > 0.
Zero is assumed to be represented by sign(0) = +, exp(0) = 0, and all digits
of the mantissa are 0. Thus, a ﬂoating-point number x is represented by the pair
(mant(x), exp(x)). The base b is implicit in the representation.
The signed-magnitude representation is employed in common practice, and it ap-
pears to be the most natural representation as well. The following algorithms are
described for a ﬁxed but arbitrary base b > 1.
However, the base 10 would be the most natural base at least for the representa-
tion of the mantissas. The base 10 is used in common practice, and it avoids many
unnecessary anomalies and confusion that may occur when other bases are used in
computers. In particular, errors arising from conversion of numbers to and from the
decimal system are avoided. b = 2 or b = 16 in particular are other frequently used
bases.
The architecture of the computer that we use to describe the algorithms is that of the
classical computer, circa 1960. The ﬂoating-point numbers (mantissa and exponent

198
6 Implementation of Floating-Point Arithmetic on a Computer
together) are stored in words. The accumulator is able to accommodate twice as many
digits as are in the ﬂoating-point mantissa and a few extra. This computer is a very
convenient tool with which to express the essential steps of the algorithms.
For a different computer, for instance one in which the storage is organized byte-
wise or in which the accumulator is only 8 or 16 bits long, a few additional consid-
erations of minor difﬁculty are necessary. A convenient way of implementing the
arithmetic that we describe is then a simulation of our ideal computer on an actual
computer.
Now we derive the algorithms for the implementation of arithmetic for the ﬁrst row
of Figure 1. According to our theory, the only way to deﬁne arithmetic for this row is
by means of the formula
(RG)

x,y∈S
x ◦y :=
(x ◦y), ◦∈{+, −, ·, /} with y ̸= 0 for ◦= /.
We implement (RG) for all roundings
∈{▽, △,
μ, μ = 0(1)b}. Division is
not deﬁned if y = 0.
At ﬁrst sight it seems to be doubtful that formula (RG) can be implemented on
computers at all. To determine the approximation x ◦y, the exact but unknown result
x◦y seems to be required in (RG). If, for instance, in the case of addition in a decimal
ﬂoating-point system, x is of the magnitude 1050 and y of the magnitude 10−50, more
than 100 decimal digits in the accumulator would be necessary to represent x + y.
Even if the largest computers had such long accumulators, it would hardly be neces-
sary to employ so many digits. We will show by means of the following algorithms for
all ◦∈{+, −, ·, /} that whenever x◦y is not representable on the computer, it is sufﬁ-
cient to replace it by an appropriate and representable value x!◦y. The latter will have
the property
(x ◦y) =
(x !◦y) for all roundings
∈{▽, △,
μ, μ = 0(1)b}.
Then x !◦y can be used to deﬁne x ◦y by means of the relations

x,y∈S
x ◦y =
(x ◦y) =
(x !◦y),
∈{▽, △,
μ, μ = 0(1)b}.
The algorithms that implement this relation can, in principle, be separated into the
following six steps.
1. Decomposition of x and y, i.e., separation of x and y into mantissa and exponent.
If these parts of ﬂoating-point numbers are stored in separate words, this step is
vacuous.
2. Determination of x !◦y. It may be that x !◦y = x ◦y.
3. Normalization of x !◦y. If the result of ii is already normalized, this step can be
skipped.
4. Treatment of exponent overﬂow and underﬂow.
5. Rounding of x !◦y to determine x ◦y =
(x !◦y) =
(x ◦y).

6.2 Introductory Remarks About the Level 1 Operations
199
6. Composition, i.e., assembling the mantissa and exponent of the result into a
ﬂoating-point number. If these parts of ﬂoating-point numbers are stored in sep-
arate words, this step is vacuous.
true
true
false
false
DC
A,S
M
DV
η2
η1
N1
N2
ϵ
OF,UF
E
ρ
C
γ
PN
R
Figure 6.16. Flow diagram for the arithmetic operations. DC: Decompo-
sition; A,S: Addition and Subtraction; M: Multiplication; DV: Division;
N1,N2: Normalization; OF,UF: Overﬂow or Underﬂow; E: Exception; R:
Rounding; PN: Postnormalization; C: Composition.
Figure 6.16 shows these six steps in the form of a ﬂow diagram. Labels are in-
troduced between single steps to label segments of the detailed diagrams, which we
consider below.
We shall see that division can be executed in a manner that eliminates the need for
normalization.

200
6 Implementation of Floating-Point Arithmetic on a Computer
Since we deal with monotone roundings only, the normalization has to be per-
formed before the rounding since otherwise the monotonicity is destroyed. After
rounding a post normalization may become necessary.
In the following sections we brieﬂy discuss the main features of the steps of the
algorithms that are enumerated in Figure 6.16. We summarize the results by ﬂow
diagrams.
First we develop algorithms for the ﬂoating-point operations using a so-called long
accumulator. By this we understand a computer register with one digit, which may
be a binary digit, in front of the point and 2r + 1 digits of base b after the point.
See Figure 6.17(a). Here by point we mean the b-ary point, i.e., the decimal point
for b = 10. A long accumulator is a convenient tool to implement the ﬂoating-point
operations.
We shall discuss alternative algorithms for the execution of ﬂoating-point opera-
tions using a so-called short accumulator in Section 6.9. See Figure 6.17(b). By this
we understand a computer register with one digit, which can be a binary digit, in front
of the point and r + 2 digits of base b plus one binary digit after the point. The algo-
rithms show that further reduction of the length of the accumulator is not possible if
the operations are deﬁned by (RG) for all roundings
∈{▽, △,
μ, μ = 0(1)b}.
1 bit
r
1 bit
r
r
(a)
2r + 1 digits of base b
1 bit
r + 2 digits of base b
(b)
Figure 6.17. (a) long accumulator; (b) short accumulator.
The question of whether the algorithms using the short or the long accumulator are
satisfactory has no simple answer. In the discussion of this question we assume that
an adder of the same length as the accumulator is used.
If the accumulator and the algorithms are implemented in hardware, both meth-
ods may be nearly equal in speed. If the accumulator is available in hardware while
the algorithms are to be implemented in software, the algorithms that use the long
accumulator are likely to be faster because they are simpler. If the computer has a rel-
atively short accumulator and storage word (for instance, 8 or 16 bits) and if the longer

6.3 Addition and Subtraction
201
accumulator has to be simulated, then the algorithms using the short accumulator are
probably faster.
In principle the following can be said: Extending the length of the short accumu-
lator simpliﬁes the algorithmic ﬂow. So an ideal length of the accumulator might
consist of the largest number of digits over which a parallel addition can be delivered
in every addition cycle. This length is technology dependent. For the binary number
system accumulators of 170 bits are in use, but this width can probably be extended
further.
The key to the whole implementation is to take care that the formula (RG), as well
as the rounding properties (R1)–(R4), which are operative, are strictly realized. This
means that these formulas have to be valid for all x, y ∈S and not only for some or
most of such x, y. Even an interval around zero, however small, may not be excluded
from this requirement.
With these provisos, a principal result of the following sections is that the whole
implementation can be separated into six independent steps as indicated in Figure 6.16
and its context. This means, in particular, that the provisional result, x !◦y for all
◦∈{+, −, ·, /}, may be determined independently of the rounding function. The
latter is to be applied so that
(x◦y) =
(x!◦y) for all x, y ∈S, and for all roundings
∈{▽, △,
μ, μ = 0(1)b}. Consequently, in the ﬂow diagram of Figure 6.16, the
rounding R may be any of the roundings
∈{▽, △,
μ, μ = 0(1)b}, and the
entire algorithm delivers the result deﬁned by (RG) and this particular rounding.
In the following ﬂow diagrams, we use the usual conventions: rectangles denote
statements; circles, labels; and hexagons, conditions. The ﬂow diagrams are otherwise
self-explanatory. The operands are always called x and y. The result is called z.
In the following algorithms, all operator symbols +, −, ·, /, ≤, ≥, <, > are deﬁned
and used in the sense of integer arithmetic. A left (resp. right) shift is expressed
by a multiplication (resp. division) by powers of the base b. As a (rather awkward)
device to describe certain digit manipulations, we occasionally employ the function
[x] := entire(x). It determines the greatest integer less than or equal to x.
6.3
Addition and Subtraction
We assume that the long accumulator is being used. We denote the operands by
x = (mx, ex), y = (my, ey), and the result by z = (mz, ez). Without loss of
generality, we assume that ex ≥ey. We set ez := ex. When my ̸= 0 we distinguish
two cases:
(i) ex −ey ≥r + 2. Here y is too small in absolute value to inﬂuence the ﬁrst r
digits of the sum x + y. In the case of the roundings
μ, μ = 1(1)b −1, we
therefore simply obtain mz = mx. However, an arbitrarily small y can change
the mantissa of x in the case of the roundings ▽, △,
0, and
b. To handle

202
6 Implementation of Floating-Point Arithmetic on a Computer
these cases correctly, we set
my := sign(my) · b−(r+3).
(ii) ex −ey ≤r + 1. Here we divide my by bex−ey, i.e., we shift my to the right by
ex −ey digits of base b, and we set my := my · b−(ex−ey).
Then we compute the intermediate sum mz := mx+my. Upon completion of this
addition algorithm, the result is still to be normalized and rounded.
Figure 6.18 displays a ﬂow diagram for the addition algorithm that we have just
sketched. The ﬁrst statement in this diagram represents the decomposition of x and
y. The second statement selects the greater (and lesser) of the operands x and y. If
realized in hardware this is just a comparison plus a selection.
2r + 1 digits of base b sufﬁce for the representation of this sum in all cases. The
binary digit in front of the point, which comes into play upon mantissa overﬂow, is
not strictly necessary. It is convenient to use it to avoid complicated shiftings, which
slow down the addition.
By way of comment on the condition ex −ey ≥r + 2 displayed in the ﬂow chart
of Figure 6.18, we interpolate an example in which ex −ey = r +1 and which shows
that for the rounding
μ, addition of a digit in the (r + 2)th place can change all
digits of mx.
In the following four examples let r = 3. We assume that the operands x and y
are exact and are already appropriately positioned as indicated. The letter η indicates
normalization.
x
0.
1
0
. . .
0
0
0
·b3
y
−0.
0
0
. . .
0
0
b −μ + 1
·b3
x + y
0.
0
b −1
. . .
b −1
b −1
μ −1
·b3
η(x + y)
0.
b −1
b −1
. . .
b −1
μ −1
·b2
μ(x + y)
0.
b −1
b −1
. . .
b −1
·b2
The effect does not occur for μ = b, i.e., we assume here μ ≤b −1. The following
example shows that a corresponding addition in the (r+3)th place, however, does not
change the mantissa of x.
x
0.
1
0
. . .
0
0
0
0
·b3
y
−0.
0
0
. . .
0
0
0
b −μ + 1
·b3
x + y
0.
0
b −1
. . .
b −1
b −1
b −1
μ −1
·b3
η(x + y)
0.
b −1
b −1
. . .
b −1
b −1
μ −1
·b2
μ(x + y)
0.
1
0
. . .
0
·b2

6.3 Addition and Subtraction
203
ex := exp(x); mx := mant(x)
my := mant(y)
ey := exp(y);
ez := ey; mz := my
ey := ex; my := mx
ex := ez; mx := mz
ex < ey;
true
false
my := my · b−(ex−ey)
true
false
true
true
false
false
my := sign(my) · b−(r+3)
ez := ex
my = 0
ex −ey ≥r + 2
mz := mx + my
mz = 0
η1
ez := 0
γ
mz := mx
γ
Figure 6.18. Execution of the addition x !+ y.

204
6 Implementation of Floating-Point Arithmetic on a Computer
These examples make sense only if b −μ + 1 is a nonzero digit of the number
system that is used, i.e., if 1 ≤b −μ + 1 ≤b −1. From this inequality we obtain
2 ≤μ. On the other hand it was assumed μ ≤b −1. From the two inequalities one
may conclude b ≥3. We comment on the binary case b = 2 below. If b ≥3, the
rounding
1 (μ = 1) is certainly of only minor interest.
Now we consider the case b = 2, for which the two examples studied above are
irrelevant. The rounding
1 corresponds to the standard rounding to the nearest num-
ber of the screen. We show again by a simple example that an addition can change all
digits of mx if the exponents differ by r + 1:
x
0.
1
0
. . .
0
0
0
0
0
·b3
y
−0.
0
0
. . .
0
0
1
0
1
·b3
x + y
0.
0
1
. . .
1
1
0
1
1
·b3
η(x + y)
0.
1
1
. . .
1
0
1
1
·b2
1(x + y)
0.
1
1
. . .
1
·b2
An addition, however, does not change the mantissa of mx if the difference in the
exponents is r + 2. We illustrate this by a further example:
x
0.
1
0
. . .
0
0
0
0
0
·b3
y
−0.
0
0
. . .
0
0
0
1
1
·b3
x + y
0.
0
1
. . .
1
1
1
0
1
·b3
η(x + y)
0.
1
1
. . .
1
1
0
1
·b2
1(x + y)
0.
1
0
. . .
0
·b3
These examples show that if b ≥3 and μ ≥2, as well as if b = 2 and μ = 1,
the circumstance ex −ey = r + 1 must be treated under case 2 where nondegenerate
operations are performed on the mantissa. This state of affairs occurs in practice, for
instance, when b = 2, 10, or 16 and for rounding to the nearest number of the screen
(μ = b/2).
We now give several examples to illustrate the addition algorithm. Let r = 4.
Examples. (a) We consider the case ex −ey ≥r + 2.
(1) x = +0.d1d2d3d4 · b3, d4 ̸= b −1, and y > 0. Then
x !+ y = 0.d1d2d3d4001 · b3,
▽(x + y) = ▽(x !+ y) = 0.d1d2d3d4 · b3,
△(x + y) = △(x !+ y) = 0.d1d2d3(d4 + 1) · b3,
μ(x + y) =
μ(x !+ y) = 0.d1d2d3d4 · b3 for 1 ≤μ ≤b −1.

6.3 Addition and Subtraction
205
(2) x = −0.d1d2d3d4 · b3, d4 ̸= b −1, and y < 0. Then
x !+ y = −0.d1d2d3d4001 · b3,
▽(x + y) = ▽(x !+ y) = −0.d1d2d3(d4 + 1) · b3,
△(x + y) = △(x !+ y) = −0.d1d2d3d4 · b3,
μ(x + y) =
μ(x !+ y) = −0.d1d2d3d4 · b3 for 1 ≤μ ≤b −1.
(3) x = +0.1000 · b3 and y < 0. Then
x !+ y = 0.0(b −1)(b −1)(b −1)
(b −1)(b −1)(b −1) · b3,
η(x !+ y) = 0.(b −1)(b −1)(b −1)(b −1)
(b −1)(b −1) · b2,
▽(x + y) = ▽(x !+ y) = 0.(b −1)(b −1)(b −1)(b −1) · b2,
△(x + y) = △(x !+ y) = 0.1000 · b3,
μ(x + y) =
μ(x !+ y) = 0.1000 · b3 for 1 ≤μ ≤b −1.
(4) x = −0.1000 · b3 and y > 0. Then
x !+ y = −0.0(b −1)(b −1)(b −1)
(b −1)(b −1)(b −1) · b3,
η(x !+ y) = −0.(b −1)(b −1)(b −1)(b −1)
(b −1)(b −1) · b2,
▽(x + y) = ▽(x !+ y) = −0.1000 · b3,
△(x + y) = △(x !+ y) = −0.(b −1)(b −1)(b −1)(b −1) · b2,
μ(x + y) =
μ(x !+ y) = −0.1000 · b3 for 1 ≤μ ≤b −1.
(b) Now we consider the case ex −ey ≤r + 1 and in particular for b = 10.
(1) x = 0.1000 · 106 and y = −0.5001 · 101. Then
x + y = 0.0999
9499
9 · 106,
η(x + y) = 0.9999
4999 · 105,
▽(x + y) = 0.9999 · 105,
△(x + y) = 0.1000 · 106,
μ(x + y) = 0.9999 · 105 for 5 ≤μ ≤9.

206
6 Implementation of Floating-Point Arithmetic on a Computer
(2) x = 0.1000 · 106 and y = −0.5000 · 101. Then
x + y = 0.0999
9500
0 · 106,
η(x + y) = 0.9999
5000 · 105,
▽(x + y) = 0.9999 · 105,
△(x + y) = 0.1000 · 106,
μ(x + y) = 0.1000 · 106 for μ = 5.
In the last two examples the operands are identical except for the last digit of y. If
the (2r+1)th digit of the accumulator was in fact not present, the corresponding sums
x + y would be identical. Since these sums differ, these two examples show that it is
really necessary to have 2r + 1 digits available in the accumulator if the addition is to
be executed as prescribed and the result is required to be correct for all the roundings
▽, △, and
5.
However, we shall see in Section 6.9 that the desired correct answer can also be
obtained with the short accumulator.
6.4
Normalization
A normalization consists of appropriate right or left shifts of the mantissa and a cor-
responding adjustment of the exponent.
The addition algorithm described in Figure 6.18 can result in a mantissa with the
property |mz| ≥1. The inequality
|mx| + |my| · b−(ex−ey) < 1 + 1,
however, shows that a shift of at most one digit to the right may be required. It also
shows that a single bit is sufﬁcient to record such an overﬂow of the mantissa for an
addition since the leading digit can only be unity or zero. If this bit is one after an
addition, then the 2rth as well as the (2r +1)th digit in the accumulator is necessarily
zero. Thus the result after the right shift is still represented correctly, having all the
digits.
After an addition, however, a left shift of more than one digit may be necessary.
Figure 6.19 shows the ﬂow diagram for the execution of the normalization after an
addition or subtraction. We assume that the hardware is able to compute the number
of leading zeros in the intermediate sum. Otherwise a loop would have to be provided
to do the left shift digit by digit. We denote the number of leading zeros by l. In
general the shift of the mantissa and the corresponding correction of the exponent are
executed in different parts of the hardware of the computer.

6.5 Multiplication
207
false
true
true
false
η1
|mz| ≥1
mz := mz · bl; ez := ez −l
|mz| < b−1
ϵ
mz := mz · b−1; ez := ez + 1
Figure 6.19. Execution of the normalization after the addition.
The algorithm for multiplication will be discussed in the next section. However,
we foreshadow here that the normalization procedure for multiplication is simpler.
To see this, note that since b−1 ≤|mx| < 1 and b−1 ≤|my| < 1, we obtain
b−2 ≤|mx||my| < 1. Then no right shift and at most one left shift is necessary
after multiplication. Figure 6.20 shows the ﬂow diagram for the execution of the
normalization after a multiplication.
true
false
ϵ
|mz| < b−1
η2
mz := mz · b; ez := ez −1
Figure 6.20. Execution of the normalization after a multiplication.
6.5
Multiplication
If x = 0 or y = 0 then x · y = 0 and
(x · y) = 0. Otherwise we determine
ez := ex + ey (the possibility that ez lies outside the range [e1, e2] is considered in
Section 6.9) and mz := mx·my. This multiplication can be executed correctly within

208
6 Implementation of Floating-Point Arithmetic on a Computer
the long accumulator. Because b−2 ≤|mx| · |my| < 1, a normalization consisting
of at most one left shift may be necessary. Figure 6.21 displays the ﬂow diagram
for multiplication. The relevant normalization procedure is described in the previous
section.
ex := exp(x); mx := mant(x)
my := mant(y)
ey := exp(y);
false
true
γ
mx = 0 ∨my = 0
ez := ex + ey; mz := mx · my
η2
mz := 0; ez := 0
Figure 6.21. Execution of multiplication.
Every digital processor should also be able to deliver the double length unrounded
product as it is needed for many applications.
6.6
Division
We determine the quotient x !/ y. If y = 0, an error message is to be given. An
exception handling routine may be started, which if x ̸= 0 may set the quotient to
±∞. If x = 0 and y ̸= 0, then x/y = 0 and
(x/y) = 0. If x ̸= 0 and y ̸= 0
we determine ez := ex −ey (the possibility that ez lies outside the range [e1, e2] is
dealt with in Section 6.9). We read mx into the ﬁrst r digits of the accumulator and
set all the remaining digits of the accumulator to zero. If |mx| ≥|my|, we shift the
contents of the accumulator to the right by one digit and increase ez by unity. Now
we divide the contents of the accumulator by my. It is sufﬁcient to determine the ﬁrst
r + 1 digits of the quotient. We denote this quotient by (mx/my)r+1. It is already
normalized. The (r + 1)th digit is only needed for the execution of the roundings
μ,
1 ≤μ ≤b −1. However for one of the roundings ▽, △,
0 and
b, even if the
(r + 1)th digit is zero, an arbitrarily small remainder can inﬂuence the mantissa (even
its leading digit). The contents of the accumulator after the ﬁrst r + 1 digits is the
remainder. We denote it by rmd.
For proper treatment of all these cases, we proceed as follows:
(i) mz := (mx/my)r+1, i.e., compute the ﬁrst r + 1 digits of mz.
(ii) If rmd = 0, there is no further treatment of mz.
(iii) If rmd ̸= 0, we write a 1 in the (r + 3)th digit of mz.

6.7 Rounding
209
Figure 6.22 shows a ﬂow chart for the execution of division.
ex := exp(x); mx := mant(x)
my := mant(y)
ey := exp(y);
false
false
true
true
true
digits of the quotient)
false
(denotes the ﬁrst r + 1
γ
ϵ
division by zero treatment
division by zero message
my = 0
mx = 0
mz := 0; ez := 0
ez := ex −ey
|mx| ≥|my|
mx := mx/b; ez := ez + 1
mz := (mx/my)r+1
ϵ
remainder = 0
mz := sign(mz) · (|mz| + b−(r+3))
Figure 6.22. Execution of the division x !/ y.
6.7
Rounding
After normalization the mantissa, which in general has a length of 2r + 1 digits, is
to be rounded to r digits. All roundings of the set
∈{▽, △,
μ, μ = 0(1)b} are
monotone functions. Rounding a number either truncates the number or enlarges its
absolute value. The latter case requires another add operation. So rounding is a rel-
atively complicated process. The add operation may cause a carry which propagates
from the least signiﬁcant bit across all digits of the number. Thus a rounding can alter
the mantissa so that |mz| = 1 which causes another normalization. See Figure 6.16.
For the roundings
μ, μ = 1(1)b −1 we give a symbolic formal description of the
rounding part of Figure 6.16 in the following Figure 6.23. The description employs

210
6 Implementation of Floating-Point Arithmetic on a Computer
the function [x] := entire(x) which determines the greatest integer less than or equal
to x.
true
false
mz := sign(mz) · b−r[|mz| · br + (b −μ) · b−1]
|mz| ≥1
z := mz · bez
η1
ρ
γ
Figure 6.23. Execution of the roundings
μ, μ = 1(1)b −1.
The last statement in Figure 6.23 denotes the composition of mz and ez to form
the result z. See Figure 6.16.
Figure 6.24 gives a symbolic description in the form of a ﬂow diagram for the
rounding ▽.
true
false
η1
mz := b−r[mz · br]
|mz| ≥1
ρ
γ
Figure 6.24. Execution of the rounding ▽.
Here the description does not explicitly indicate that an addition is involved. The
add operation is implicit in the function [mz · br] if it operates on negative values.
Similarly the execution of the rounding △requires an add operation for positive
arguments. The rounding △also can be produced in terms of ▽by using the relation
△x = −▽(−x).
The simplest of all the roundings under consideration, in a certain sense, is the
rounding towards zero,
b in our notation. It is also known as truncation or chop-
ping. Here no additional add operation is involved for the rounding and as a conse-

6.8 A Universal Rounding Unit
211
quence no additional normalization after the rounding can occur. Figure 6.25 shows
the corresponding ﬂow chart.
ρ
γ
mz := sign(mz) · b−r[|mz| · br]
Figure 6.25. The rounding towards zero
b, truncation or chopping.
Base 16 and rounding by truncation was used on IBM/360 computers for many
years, and is still used in their /370 and /390 architectures. Although these ma-
chines performed pretty well, their numerical accuracy has frequently been judged
as very poor in the literature. For rounding by truncation the relative rounding error is
bounded by one ulp (unit in the last place), while for rounding to nearest it is bounded
by 1/2 ulp. Such arguments are of value as long as the computer technology is rela-
tively limited. But their value drops as computing technology improves, which it has
been doing very rapidly. With increasing word length the fact that truncation requires
no additional add operation for the rounding process may outweigh the rounding error
problem and speed up the operation.
We mention here again that the rounding ▽can also be performed by truncation if
negative numbers are represented by their b-complement. See (5.2.9) and its context
in Section 5.2. This may require taking the b-complement twice.
Finally, we remark once more that the algorithms for all operations ◦∈{+, −, ·, /}
show that the intermediate result x !◦y can be chosen so that for all roundings
∈
{▽, △,
μ, μ = 0(1)b}, the property

x,y∈S
(x ◦y) =
(x !◦y)
holds. This assures us that for the rounding step R in the execution of arithmetic op-
erations in Figure 6.16, any of the roundings ▽, △,
μ, μ = 0(1)b can be employed.
The result obtained is the one deﬁned by (RG) and that rounding. No other part of the
entire algorithm in Figure 6.16 need be changed.
6.8
A Universal Rounding Unit
The remark of the last paragraph allows the construction of a universal rounding unit.
It can be used to perform all roundings of the set {▽, △,
μ, μ = 0(1)b}. Any
rounding can only produce one out of two different answers. It either truncates the

212
6 Implementation of Floating-Point Arithmetic on a Computer
mantissa or it enlarges it by one unit in the last place. If these two values are available
the results of all roundings of the set {▽, △,
μ, μ = 0(1)b} can easily be performed
by case selection through a simple multiplexer.
true
false
ρ
PN
γ
η1
z := (−1)S▽|z|
z := (−1)S△|z|
cv
0
1
Figure 6.26. A universal rounding unit.
Figure 6.26 gives a brief sketch of such a rounding unit. There the labels ϱ, η1,
and γ are used as deﬁned in Figure 6.16. The intermediate unrounded result of the
arithmetic operation that reaches the rounding unit is assumed to be of the form
z = (−1)S · 0.z1z2 · · · znzn+1zn+2 · · · · be.
Thus the sign of the number is negative if S = 1 and it is positive if S = 0. The
multiplexer is controlled by a logical control variable cv. We assume that the value
true is represented by 0 and the value false by 1. The two assignments in Figure 6.26
perform the roundings
0z = (−1)S · △|z| and
bz = (−1)S · ▽|z|. See (5.2.8).
The different roundings of the set {▽, △,
μ, μ = 0(1)b} can be obtained by very
simple values of the control variable cv. The following table displays values of cv for
different roundings.
rounding
▽z
△z
0z
bz
μ, μ = 1(1)b −1
cv
S
S
0
1
zn+1 ≥μ
Table 6.2. A universal rounding unit.
If b is an even number then
b/2 is the rounding to the nearest ﬂoating-point num-
ber. In case of the binary number system also the rounding to nearest even can be
obtained by a simple expression for the control variable cv.

6.9 Overﬂow and Underﬂow Treatment
213
The simplest and fastest of all roundings under consideration is truncation, i.e., the
rounding towards zero. It never produces an overﬂow of the mantissa. The left hand
pass in the sketch of Figure 6.26 on the other hand is certainly slower. It enlarges the
absolute value of the mantissa and may cause another normalization pass.
Thus a ﬁrst glance at Figure 6.26 seems to reveal that the universality of the round-
ing unit slows down the execution of the rounding truncation. This, however, needs
not to be so. If the required rounding is truncation the left hand pass in Figure 6.26
needs not to be executed at all.
In Figure 6.26 the control variable cv selects the rounding direction. There are
important and very useful applications in numerical analysis where the direction of
the rounding is to be selected randomly. In this case the control variable cv is chosen
by a random number generator possibly by hardare.
6.9
Overﬂow and Underﬂow Treatment
For traditional ﬁxed-point computations it is well known that all computed values
must be less than unity and the problem data themselves must be preprocessed, typ-
ically by scaling, so that the computation proceeds coherently. There are analogous
requirements for the use of all classical calculating devices such as analog computers,
planimeters, and so forth. It is easy to forget that the same requirement exists for
modern digital computers, even though they employ ﬂoating-point operations and an
enormous range of representable numbers.
Indeed, a situation can occur in a ﬂoating-point computation where this repre-
sentable range is exceeded. The terms underﬂow and overﬂow characterize these
occurrences, and we now consider them.
We saw in Chapter 5 that real numbers x with the property
be1−1 ≤|x| ≤G := 0.(b −1)(b −1) . . . (b −1) · be2
are conveniently mapped into a ﬂoating-point system. Any associated error need not
be larger than the distance between two neighboring ﬂoating-point numbers of x.
Whenever in a ﬂoating-point computation a number x occurs with
0 < |x| < be1−1 respectively |x| > G
we speak of an exponent underﬂow respectively of an exponent overﬂow. Since in
a ﬂoating-point computation the mantissa is always normalized, any underﬂow or
overﬂow will be evident in the exponent. An attempt to represent a real number in
the event of underﬂow or overﬂow in the ﬂoating-point system results in the loss
of nearly all information about the number. Therefore, whenever in a ﬂoating-point
computation an exponent underﬂow or overﬂow occurs this must be made known to
the user. This indication makes it possible to decide what to do about it.

214
6 Implementation of Floating-Point Arithmetic on a Computer
The algorithms discussed in this chapter show that underﬂow and overﬂow can
occur in the performance of all arithmetic operations. Although such occurrences
may be rather rare, they must be tested for at all times. See Figure 6.16.
A frequent source of underﬂow and overﬂow in conventional ﬂoating-point com-
putations is multiplication, in particular in scalar products. This is absolutely unnec-
essary. Scalar products can always be computed exactly and without any underﬂow
or overﬂow. This problem will be dealt with and ultimately solved in Chapter 8 where
we shall see that the exact computation of scalar products is faster than a computation
in conventional ﬂoating-point arithmetic.
While complete recovery from underﬂow and overﬂow is not possible in con-
ventional ﬂoating-point arithmetic, several computer architectures offer special treat-
ments to handle these situations. Two different procedures are in common use.
The ﬁrst is a trial and error approach. During the preparation and programming of
the problem, care is to be taken to ensure that the entire computation can be performed
within the available range of numbers. The special difﬁculty is that it is hard to judge
in advance that all intermediate results will stay within this range. It is a convenient
and perhaps the most customary practice, therefore, simply to go ahead with the com-
putation without too much analysis concerning the range of numbers that will occur.
If an underﬂow or overﬂow occurs, the computer stops the computation with an error
message. The user than tries a little harder and preprocesses the problem somewhat
further, typically by rescaling. The computation is then rerun. Several such rescaling
steps are often needed.
The second method exploits the observation that some underﬂows and overﬂows
are benign with respect to further computation. As an example let us consider the
case of underﬂow. Suppose that the sum
S :=
n

i=1
ui(x)
is to be calculated, leading to a result of magnitude 1030. If the computation of one of
the summands ui causes an underﬂow, viz., |ui| < 10e1−1, then ui does not inﬂuence
the ﬂoating-point sum at all and can be ignored. On the premise that it is sometimes
sensible to continue a computation which has underﬂowed or overﬂowed, a well-
deﬁned underﬂow or overﬂow procedure is to be invoked. It often consists of mapping
the numbers x for which |x| < be1−1 to zero, and those for which |x| > G to −G or
−∞, or to +G or +∞depending on the sign of the mantissa, and continue with the
computation. Additionally, a detailed underﬂow or overﬂow message is given to the
user. At the end of the computation the user can then decide whether the underﬂow or
overﬂow that occurred during the calculation was harmless or not. In the former case
the computation is ﬁnished. In the latter, a better scaling of the problem is required,
followed by recomputation.
Each of these two recovery processes has advantages and disadvantages. The ﬁrst
may cause too many interruptions, especially when the interruptions are unnecessary

6.10 Algorithms Using the Short Accumulator
215
from the point of view of the second method. The second method may waste computer
time persisting with a computation that is afterwards rejected as incorrect.
For completeness we ﬁnish by giving the ﬂow diagram for the part of Figure 6.16
between the labels denoted by ϵ and ρ.
false
true
true
false
ez < e1
exponent underﬂow message
exponent underﬂow treatment
ρ
ϵ
ez > e2
exponent overﬂow message
exponent overﬂow treatment
Figure 6.27. Exponent underﬂow and exponent overﬂow.
6.10
Algorithms Using the Short Accumulator
In the case ex −ey ≤r + 1 of the algorithm for the addition shown in Figure 6.18,
the mantissa my may be shifted to the right as many as r + 1 digits. Thus the ensu-
ing addition, as described above, requires an accumulator of 2r + 1 digits after the
point. The multiplication algorithm commences with a correct calculation of the prod-
uct of the mantissas mx · my. As with addition, this operation can be conveniently
accommodated within an accumulator of 2r + 1 digits after the point. However, we
mentioned at the beginning of this chapter that all ﬂoating-point operations can also
be performed within shorter accumulators. In this case extra care must be taken so
that the operations are executed appropriately, i.e., so that formula (RG) holds.
Figure 6.28 displays a ﬂow chart for the execution of addition using the short accu-
mulator. See Figure 6.17. This accumulator consists of r +2 digits of base b followed
by an additional bit as well as an additional bit in front of the point. This latter bit is
used for treatment of mantissa overﬂow during addition. The contents of the accumu-
lator which follow the point are denoted by
.d1d2 . . . drdr+1dr+2d.

216
6 Implementation of Floating-Point Arithmetic on a Computer
Here the di are digits of base b, while d is a binary digit. To permit a simple
description of the algorithms corresponding to the ﬂow charts in Figures 6.28 and
6.29, we take the (l + 3)th digit of the accumulator to be a full digit of base b. It
could, however, be replaced by a single binary digit as indicated here.
The branches ex −ey ≥r + 2 in Figures 6.18 and 6.28 are the same. The comple-
mentary branch ex −ey ≤r + 1 in Figure 6.28 is split by the condition ex −ey > 2.
If ex−ey ≤2, addition with the short accumulator proceeds as before. The condition
ex −ey > 2 ∧ex −ey ≤r + 1 speciﬁes that branch of the algorithm of Figure 6.28
in which addition is executed differently than in the algorithm of Figure 6.18. In this
case, |mz| = |mx + my| ≥||mx| −|my|| > b−1 −b−2 = (b −1)b−2 ≥b−2, i.e., the
ﬁrst nonzero digit of mz occurs within the ﬁrst or second place after the point. Then
for the roundings
μ, 1 ≤μ ≤b −1, the digit of mz, which keys the rounding, is no
further than the (r + 2)th place to the right of the point. However, for the roundings
▽, △,
0 or
b, a digit beyond the (r + 2)th can inﬂuence the result. Then during
any shifting of my to the right, note must be taken whether digits shifted beyond the
(r + 2)th place are zero or not. This information is recorded in the (r + 3)th digit d.
Thus d need only be a binary digit. The long condition in Figure 6.28 can be realized
by an or-gate.
As before, the addition mz := mx + my may cause an overﬂow in the mantissa.
This would require a right shift within the normalization algorithm. The (r + 3)th
digit may be altered as a result of this right shift. Figure 6.29 displays the algorithm
for the execution of the normalization using the short accumulator.
To simplify the description of the algorithms in Figures 6.28 and 6.29, we assume
that the (r + 3)th digit after the point is a full digit of base b. In fact as we have
already remarked, it sufﬁces to treat this digit as a binary digit. As in Figure 6.19, in
Figure 6.29 the number of leading zeros if a left shift is necessary is denoted by l.
We illustrate the algorithms of Figures 6.28 and 6.29 with several examples. We
take r = 4 and use the decimal system b = 10. The provisional and possibly un-
normalized and unrounded result is denoted by x !+ y. We begin with the following
case.
Examples. (a) sign(x) = sign(y).
(1) x = 0.9999 · b3, y = 0.1001 · b0,
x + y = 1.0000
001 · b3,
x !+ y = 0.1000
001 · b4,
▽(x + y) = ▽(x !+ y) = 0.1000 · b4,
△(x + y) = △(x !+ y) = 0.1001 · b4,
μ(x + y) =
μ(x !+ y) = 0.1000 · b4, for 1 ≤μ ≤9.

6.10 Algorithms Using the Short Accumulator
217
ey := exp(y);
ex := exp(x);
ez := ey; mz := my
ey := ex; my := mx
ex := ez; mx := mz
my := mant(y)
mx := mant(x)
ex < ey;
true
false
my := my · b−(ex−ey)
my := sign(my) · b−(r+3)([|my| · br+2] · b + 1)
true
true
my := my · b−(ex−ey)
false
my := sign(my) · b−(r+3)
false
true
true
false
true
false
false
|my| · b−(ex−ey)+r+2 −[|my| · b−(ex−ey)+r+2] ̸= 0
mz := mx + my
mz = 0
ez := 0
γ
ez := ex
ex −ey ≥r + 2
my = 0
γ
η3
mz := mx
ex −ey > 2
Figure 6.28. Execution of addition with the short accumulator.

218
6 Implementation of Floating-Point Arithmetic on a Computer
true
false
true
false
true
false
mz := sign(mz) · ([|mz| · br+1] · b + 1) · b−(r+3))
ϵ
ez := ez + 1
[br+3|mz|] −[br+1|mz|] · b2 ̸= 0
η3
|mz| < b−1
mz := mz · b−1
mz := mz · bl
ez := ez −l
|mz| ≥1
Figure 6.29. Execution of normalization with the short accumulator.

6.10 Algorithms Using the Short Accumulator
219
(2) x = 0.9998 · b3, y = 0.2070 · b0,
x + y = 1.0000
070 · b3,
x !+ y = 0.1000
001 · b4,
▽(x + y) = ▽(x !+ y) = 0.1000 · b4,
△(x + y) = △(x !+ y) = 0.1001 · b4,
μ(x + y) =
μ(x !+ y) = 0.1000 · b4, for 1 ≤μ ≤9.
(3) x = 0.8234 · b5, y = 0.5012 · b1,
x + y = 0.8234
5012 · b5,
x !+ y = 0.8234
501 · b5,
▽(x + y) = ▽(x !+ y) = 0.8234 · b5,
△(x + y) = △(x !+ y) = 0.8235 · b5,
μ(x + y) =
μ(x !+ y) = 0.8235 · b5, for 1 ≤μ ≤5,
μ(x + y) =
μ(x !+ y) = 0.8234 · b5, for 6 ≤μ ≤9.
(4) x = −0.8234 · b5, y = −0.5021 · b1,
x + y = −0.8234
5021 · b5,
x !+ y = −0.8234
501 · b5,
▽(x + y) = ▽(x !+ y) = −0.8235 · b5,
△(x + y) = △(x !+ y) = −0.8234 · b5,
μ(x + y) =
μ(x !+ y) = −0.8235 · b5, for 1 ≤μ ≤5,
μ(x + y) =
μ(x !+ y) = −0.8234 · b5, for 6 ≤μ ≤9.
(5) x = −0.9998 · b3, y = −0.2070 · b0,
x + y = −1.0000
070 · b3,
x !+ y = −0.1000
001 · b4,
▽(x + y) = ▽(x !+ y) = −0.1001 · b4,
△(x + y) = △(x !+ y) = −0.1000 · b4,
μ(x + y) =
μ(x !+ y) = −0.1000 · b4, for 1 ≤μ ≤9.
(b) Now let sign(x) ̸= sign(y).

220
6 Implementation of Floating-Point Arithmetic on a Computer
(6) x = 0.1000 · b4, y = −0.1001 · b−1,
x + y = 0.0999
98999 · b4,
x !+ y = 0.0999
989 · b4,
▽(x + y) = ▽(x !+ y) = 0.9999 · b3,
△(x + y) = △(x !+ y) = 0.1000 · b4,
μ(x + y) =
μ(x !+ y) = 0.9999 · b3, for μ = 9,
μ(x + y) =
μ(x !+ y) = 0.1000 · b4, for 1 ≤μ ≤8.
(7) x = 0.1000 · b6, y = −0.5001 · b1,
x + y = 0.0999
94999 · b6,
x !+ y = 0.0999
949 · b6,
▽(x + y) = ▽(x !+ y) = 0.9999 · b5,
△(x + y) = △(x !+ y) = 0.1000 · b6,
μ(x + y) =
μ(x !+ y) = 0.9999 · b5, for 5 ≤μ ≤9,
μ(x + y) =
μ(x !+ y) = 0.1000 · b6, for 1 ≤μ ≤4.
(8) x = 0.1000 · b6, y = −0.5000 · b1,
x + y = 0.0999
95000 · b6,
x !+ y = 0.0999
950 · b6,
▽(x + y) = ▽(x !+ y) = 0.9999 · b5,
△(x + y) = △(x !+ y) = 0.1000 · b6,
μ(x + y) =
μ(x !+ y) = 0.1000 · b6, for 1 ≤μ ≤5,
μ(x + y) =
μ(x !+ y) = 0.9999 · b6, for 6 ≤μ ≤9.
(9) x = 0.1000 · b4, y = −0.5412 · b0,
x + y = 0.0999
4588 · b4,
x !+ y = 0.0999
459 · b4,
▽(x + y) = ▽(x !+ y) = 0.9994 · b3,
△(x + y) = △(x !+ y) = 0.9995 · b3,
μ(x + y) =
μ(x !+ y) = 0.9995 · b3, for 1 ≤μ ≤5,
μ(x + y) =
μ(x !+ y) = 0.9994 · b3, for 6 ≤μ ≤9.

6.10 Algorithms Using the Short Accumulator
221
(10) x = −0.1000 · b6, y = 0.5001 · b1,
x + y = −0.0999
94999 · b6,
x !+ y = −0.0999
949 · b6,
▽(x + y) = ▽(x !+ y) = −0.1000 · b6,
△(x + y) = △(x !+ y) = −0.9999 · b5,
μ(x + y) =
μ(x !+ y) = −0.1000 · b6, for 1 ≤μ ≤4,
μ(x + y) =
μ(x !+ y) = −0.9999 · b5, for 5 ≤μ ≤9.
(11) x = −0.8234 · b4, y = 0.5021 · b0,
x + y = −0.8233
4979 · b4,
x !+ y = −0.8233
499 · b4,
▽(x + y) = ▽(x !+ y) = −0.8234 · b4,
△(x + y) = △(x !+ y) = −0.8233 · b4,
μ(x + y) =
μ(x !+ y) = −0.8234 · b4, for 1 ≤μ ≤4,
μ(x + y) =
μ(x !+ y) = −0.8233 · b4, for 5 ≤μ ≤9.
The next example conﬁrms that an accumulator of r + 1 digits followed by an addi-
tional binary digit d after the point cannot deliver correct results as deﬁned by (RG)
in all cases.
(12) x = 0.1000 · b6, y = −0.5001 · b1,
x + y = 0.0999
94999 · b6,
x !+ y = 0.0999
99 · b6,
μ(x + y) = 0.9999 · b5 ̸= 0.1000 · b6 =
μ(x !+ y), for 5 ≤μ ≤9.
The preceding study shows that for addition and subtraction, the length of the short
accumulator as shown in Figure 6.17(b) is both necessary and sufﬁcient to realize
formula (RG) for all roundings in the set {▽, △,
μ, μ = 0(1)b}.
The short accumulator is also sufﬁcient to perform the multiplication and division
deﬁned by (RG) for all of these roundings. We give a simple numerical example for
multiplication:

222
6 Implementation of Floating-Point Arithmetic on a Computer
0. 9 9 9 9 · 0. 9 9 9 9
8 9 9 9 1
8 9 9 9 1
9 8 9 9 0| 1
r + 2 digits
8 9 9 9 1
|
9 9 8 9 0| 0
r + 2 digits
8 9 9 9 1
|
9 9 9 8 0 0
r + 2 digits
0. 9 9 9 8 0 0 0 1
Proceeding in a straightforward manner, we can show that the short accumulator
is sufﬁcient to perform the division (deﬁned by (RG)) of two ﬂoating-point numbers
and with any one of the roundings {▽, △,
μ, μ = 0(1)b}.
At the end of Section 6.5 where multiplication was considered, we mentioned that
every digital processor should also be able to deliver the double length unrounded
product. Such double length products cannot be efﬁciently produced by the short
accumulator. This consideration gives very high priority to the choice of at least the
long accumulator for ﬂoating-point operations.
Nevertheless, our study of the short accumulator is useful. In numerical analysis
computation of the sum of two products a · b + c · d of ﬂoating-point numbers is a
frequent task which is often very critical numerically. It appears, for instance, as the
determinant of a two by two matrix, or in the computation of the real and imaginary
parts of the product of two complex numbers, or in the absolute value of a two dimen-
sional vector. The results of this section show that such problems can be solved with
very high accuracy.
If the processor delivers the products to the full double length an expression of the
form a · b + c · d can be computed with just one rounding
(a · b + c · d),
∈
{▽, △,
μ, μ = 0(1)b}, with an error less than or equal to b−2r, by an accumulator
as shown in Figure 6.30. Here the products a · b and c · d are of length 2r. The
accumulator shown in Figure 6.30 is the short accumulator for ﬂoating-point numbers
with a mantissa of 2r digits.
r
r
Figure 6.30. Accumulator for highly accurate computation of
(a·b+c·d).
6.11
The Level 2 Operations
This section contains a brief discussion of all arithmetic operations deﬁned in the sets
of Figure 1 as well as between these sets. The basic ideas of these operations have
been extensively studied in Chapters 3 and 4. Thus our objective is simply to summa-

6.11 The Level 2 Operations
223
rize the deﬁnition of these operations and to point out that they can be performed by
using the operations that have been discussed earlier in this chapter. We do not derive
algorithms for these operations because they are simple.
We consider the ﬁve basic data sets (types) Z, S, CS, IS, ICS. Here Z denotes the
bounded set of integers representable on the computer. In an appropriate program-
ming language they may be called integer, real, complex, real interval,
complex interval.
We assume that integer arithmetic in Z is available and
brieﬂy repeat the deﬁnition of the arithmetic operations in the remaining basic data
sets, beginning with S.
(a) The arithmetic in S (real). We assumed throughout the preceding chapters that
ﬂoating-point arithmetic in S is deﬁned by semimorphism, i.e., by means of the for-
mula
(RG)

a,b∈S
a ◦b :=
(a ◦b), for ◦∈{+, −, ·, /} with b ̸= 0 for ◦= /,
where
denotes one of the monotone and antisymmetric roundings
μ, μ = 0(1)b.
All of these roundings, as well as the operations (RG) have been discussed earlier in
this chapter. Division is not deﬁned for b = 0.
(b) The arithmetic in CS is also deﬁned by semimorphism,
(RG)

(a,b),(c,d)∈CS
(a, b) ◦(c, d) :=
((a, b) ◦(c, d)), for ◦∈{+, −, ·, /},
where the rounding
: C →CS is deﬁned by

(a,b)∈C
(a, b) := (
a,
b).
Once again the rounding on the right-hand side denotes one of the roundings
μ,
μ = 0(1)b. This leads to the following explicit representation of the operations for all
couples of elements (a, b), (c, d) ∈CS:
(a, b) + (c, d) = (a + c, b + d),
(a, b) −(c, d) = (a −c, b −d),
(a, b) · (c, d) = (
(ac −bd),
(ad + bc)),
(a, b) / (c, d) =

ac + bd
c2 + d2

,
bc −ad
c2 + d2

.
Division is not deﬁned if c = d = 0.
Thus addition and subtraction can be executed in terms of the corresponding opera-
tions in S. The products occurring on the right-hand side of the multiplication formula
lead to ﬂoating-point numbers of length 2r. The summation and the rounding appear-
ing in the formula for multiplication can, for instance, be performed by routines for
the computation of exact scalar products which will be discussed in Chapter 8. An al-

224
6 Implementation of Floating-Point Arithmetic on a Computer
ternative and simpler method has already been indicated at the end of the last section.
See Figure 6.30.
For the computation of the quotient several methods are available. One method is
simply to apply an algorithm for evaluation of arithmetic expressions with maximum
accuracy. See Section 9.6. Another method has been developed by [151, 152]. A third
method could proceed in the following way. In the quotient formula, expressions of
the form
q := (x1y1 + x2y2)/(u1v1 + u2v2)
occur with xi, yi, ui, vi ∈S. First compute the products xiyi and uivi to the full
length of 2r digits. Then compute r+8 digit approximations ˜n and ˜d for the numerator
and denominator of the quotient q. With these expressions an approximation ˜q := ˜n/ ˜d
of r + 5 digits for the exact quotient can be obtained. Then in most cases
˜q =
q
for all roundings
∈{▽, △,
μ, μ = 0(1)b}. If
˜q ̸=
q, the correct result
q
can be determined from ˜q, and the residual
R := (u1v1 · ˜q + u2v2 · ˜q −x1y1 −x2y2).
Since cancellation occurs in the computation of this expression, the exact scalar
product (see Chapter 8) has to be applied for its evaluation.
The order relation ≤in CS is deﬁned in terms of the order relation in S:
(a, b) ≤(c, d) :⇔a ≤b ∧c ≤d.
(c) The arithmetic in IS is deﬁned by the semimorphism
(RG)

A,B∈IS
A♦
◦B := ♦(A ◦B), for ◦∈{+, −, ·, /},
with the monotone upwardly directed rounding ♦: IR →IS.
In Chapter 4 we derived the following formulas for the execution of (RG) for all
couples of intervals [a, b], [c, d] ∈IS:
[a, b]♦
+ [c, d] = [a▽
+ c, b△
+ d],
[a, b]♦
−[c, d] = [a▽
−d, b△
−c],
[a, b]♦· [c, d] = [min{a▽· c, a▽· d, b▽· c, b▽· d},
max{a△· c, a△· d, b△· c, b△· d}],
[a, b]♦
/ [c, d] = [min{a▽
/ c, a▽
/ d, b▽
/ c, b▽
/ d},
max{a△
/ c, a△
/ d, b△
/ c, b△
/ d}].
Here division is not deﬁned if 0 ∈[c, d]. In the multiplication and division formulas,
the minimum and maximum can be determined by using the Tables 4.5 and 4.6 in
Section 4.6. Division by an interval that includes zero is described in the Tables 4.9
and 4.10 of Section 4.9.

6.11 The Level 2 Operations
225
The order relations ≤and ⊆in IS are deﬁned by
[a, b] ≤[c, d] :⇔a ≤c ∧b ≤d,
[a, b] ⊆[c, d] :⇔c ≤a ∧b ≤d.
Here the relation ≤on the right-hand side denotes the order relation in S.
In addition to the arithmetic operations in IS, the intersection ∩and the interval
hull ¯∪have to be made available:
[a, b] ∩[c, d] :⇔[max{a, c}, min{b, d}],
[a, b] ¯∪[c, d] :⇔[min{a, c}, max{b, d}].
If b < c or d < a the intersection is the empty set.
(d) The arithmetic in ICS is also deﬁned by semimorphism
(RG)

Φ,Ψ∈ICS
Φ♦
◦Ψ := ♦(Φ ◦Ψ), for ◦∈{+, −, ·, /},
where ♦: IC →ICS denotes the upwardly directed rounding. We showed in Chap-
ter 4 that these operations are isomorphic to the operations in CIS as far as addition,
subtraction, and multiplication are concerned. See also Figure 4.5 in Chapter 4. The
division in CIS delivers upper bounds for the quotient in ICS.
Thus for the execution of the above semimorphism, we obtain the following formu-
las for all couples of elements (A, B), (C, D) ∈CIS with A = [a1, a2], B = [b1, b2],
C = [c1, c2], D = [d1, d2] ∈IS:
(A, B)♦
+ (C, D) = (A♦
+ C, B ♦
+ D)
=
-
[a1▽
+ c1, a2△
+ c2], [b1▽
+ d1, b2△
+ d2]
.
,
(A, B)♦
−(C, D) = (A♦
−C, B ♦
−D)
=
-
[a1▽
−c2, a2△
−c1], [b1▽
−d2, b2△
−d1]
.
,
(A, B)♦· (C, D) = (♦(A · C −B · D), ♦(A · D + B · C))
=
▽( min
i,j=1,2{aicj} −max
i,j=1,2{bidj}),
△( max
i,j=1,2{aicj} −min
i,j=1,2{bidj})
 
,
▽( min
i,j=1,2{aidj} + min
i,j=1,2{bicj}),
△( max
i,j=1,2{aidj} + max
i,j=1,2{bicj})
 
,
(A, B)♦
/ (C, D) =
-
♦((A · C + B · D) / (C · C + D · D)),
♦((B · C −A · D) / (C · C + D · D))
.
.

226
6 Implementation of Floating-Point Arithmetic on a Computer
The division is deﬁned only if 0 /∈C · C + D · D. The operation
◦, ◦∈{+, −,
·, /}, on the right-hand side of the last two formulas denote operations in IR. For the
execution of · and
/ , see Chapter 4, Tables 4.1 and 4.2, and Corollary 4.11.
The order relations ≤and ⊆in CIS are deﬁned by
(A, B) ≤(C, D) :⇔A ≤C ∧B ≤D,
(A, B) ⊆(C, D) :⇔A ⊆C ∧B ⊆D.
Here the relations ≤and ⊆on the right-hand side are those in IS.
The intersection ∩and the interval hull ¯∪in CIS are deﬁned componentwise:
(A, B) ∩(C, D) :⇔(A ∩C, B ∩D),
(A, B) ¯∪(C, D) :⇔(A ¯∪C, B ¯∪D).
Here the operations ∩and ¯∪on the right-hand side are those in IS.
We have now deﬁned inner arithmetic operations and order relations for all the ﬁve
basic data sets (types) Z, S, CS, IS, CIS. We still have to deﬁne operations and
relations between elements of different sets.
For addition, subtraction and multiplication, the types of the result of the operation
are given in Table 6.3. The Figure also describes the result of division with the one
exception that the quotient of two integers is deﬁned to be of type S.
b
a
Z
S
CS
IS
CIS
Z
Z
S
CS
IS
CIS
S
S
S
CS
−
−
CS
CS
CS
CS
−
−
IS
IS
−
−
IS
CIS
CIS
CIS
−
−
CIS
CIS
Table 6.3. Resulting type of operations among the basic data sets.
The general rule for performing the operations upon operands of different types is
to lift the operand of simpler type into the type of the other operand by means of a
type transfer. Then the operation can be executed as one of the inner operations, all
of which we have already dealt with. If, for instance, a multiplication a · b has to
be performed, where a ∈S and B ∈CS, we transfer a into an element of CS by
adjoining an imaginary part that is zero. Then we multiply the two elements of CS.
Or if, for instance, an addition a + b has to be performed with a ∈Z and b ∈IS, we
ﬁrst transfer a into a ﬂoating-point number of S and then this number into the interval
[a, a] ∈IS. Then the addition in IS can be used to execute the operation.

6.11 The Level 2 Operations
227
A dash in the Table 6.3 means that it is not reasonable to deﬁne the corresponding
operation a ◦b a priori. From our point of view, a ﬂoating-point number of S, for
instance, is an approximate representation of a real number, while an interval is a
precisely deﬁned object. The product of the two, which ought to be an interval, may
then not be precisely speciﬁed. However, if the user does indeed need, for instance, to
multiply a ﬂoating-point number of S and a ﬂoating-point interval of IS, he may do so
by employing a transfer function – which may be predeﬁned – which transforms the
ﬂoating-point operand of S into a ﬂoating-point interval of IS. However, in doing so,
he should be aware of the possible loss of meaning of the interval as a precise bound.
This requirement on the part of the user to invoke explicitly the transfer function in
these cases is intended to alert him to questions of accuracy in the arithmetic.
Table 6.4 shows how to perform the intersection and the interval hull between
operands of different basic data types whenever this is reasonable. If necessary, type
lifting is also performed by automatic type transfer.
¯∪
∩
IS
CIS
IS
IS
CIS
CIS
CIS
CIS
Table 6.4. Table of intersections and interval hulls between the basic inter-
val types.
We have now completed our deﬁnition and discussion of the inner and mixed oper-
ations for the ﬁve basic data sets. We are now going to deﬁne operations for matrices
and vectors of these sets. All arithmetic operations are again deﬁned by semimor-
phism.
Let T be one of the sets (types) Z, S, CS, IS, CIS. We denote the set of m × n
matrices with components of T by
MmnT := {(aij) | aij ∈T for i = 1(1)m ∧j = 1(1)n}.
In MmnT we deﬁne operations ◦: MmnT × MmnT →MmnT by

(aij)(bij)∈MmnT
(aij) ◦(bij) := (aij ◦bij).
Here the operation sign ◦on the right-hand side denotes certain operations, according
to the following three cases. In
Case T = Z one of the integer operations +, −;
Case T = S or CS one of the rounded operations
+ ,
−;
Case T = IS or ICS one of the operations ♦
+ , ♦
−, ∩, ¯∪.

228
6 Implementation of Floating-Point Arithmetic on a Computer
In addition to these operations, we deﬁne outer multiplications · : T × MmnT →
MmnT by

a∈T

(bij)∈MmnT
a · (bij) := (a · bij).
Here the multiplication sign · on the right-hand side also has three realizations. In
Case T = Z integer multiplication;
Case T = S or CS rounded multiplication · ;
Case T = IS or ICS multiplication ♦· .
Finally, we consider the product of matrices. It is a mapping · : MmnT ×MnpT →
MmpT. There are ﬁve cases for deﬁnition of this multiplication:
Case T = Z

(aij)∈MmnZ

(bjk)∈MnpZ
(aij) · (bjk) :=
⎛
⎝
n

j=1
aij · bjk
⎞
⎠.
Case T = S

(aij)∈MmnS

(bjk)∈MnpS
(aij) · (bjk) :=
⎛
⎝
n

j=1
aij · bjk
⎞
⎠.
Here
denotes one of the roundings
∈{▽, △,
μ, μ = 0(1)b}. If aij and bij
are ﬂoating-point numbers of length r, the products aij ·bjk are of length 2r. They are
to be accumulated with a single rounding. For computation of the exact scalar product
see Chapter 8.
Case T = CS

((aij,bij))∈MmnCS

((cjk,djk))∈MnpCS
((aij, bij)) · ((cjk, djk))
:=
⎛
⎝
⎛
⎝
n

j=1
(aijcjk −bijdjk),
n

j=1
(aijdjk + bijcjk)
⎞
⎠
⎞
⎠.
Here
denotes one of the roundings
∈{▽, △,
μ, μ = 0(1)b}. If aij, bij,
cjk, and djk are ﬂoating-point numbers of length r, the components of the product
matrix can be calculated by one of the methods for the computation of the exact scalar
product. See Chapter 8.

6.11 The Level 2 Operations
229
Case T = IS

([aij,bij])∈MmnIS

([cjk,djk])∈MnpIS
([aij, bij])♦· ([cjk, djk])
:=
⎛
⎝
⎡
⎣▽
n

j=1
min{aijcjk, aijdjk, bijcjk, bijdjk},
△
n

j=1
max{aijcjk, aijdjk, bijcjk, bijdjk}
⎤
⎦
⎞
⎠.
If the aij, bij, cjk, and djk are ﬂoating-point numbers of length r, all the products in
the braces are of length 2r and can be determined exactly. However, it is not necessary
to calculate all these products, since to determine their minimum (resp. maximum),
Table 4.1 in Section 4.2 is to be used. The sum can then be evaluated by one of the
methods for the computation of the exact scalar product. See Chapter 8.
Case T = CIS
For all (([a1
ij, a2
ij], [b1
ij, b2
ij])) ∈MmnCIS and (([c1
jk, c2
jk], [d1
jk, d2
jk])) ∈MnpCIS, we
have
(([a1
ij, a2
ij],[b1
ij, b2
ij]))♦· (([c1
jk, c2
jk], [d1
jk, d2
jk]))
:=

♦
n
j=1

[a1
ij, a2
ij] · [c1
jk, c2
jk] −[b1
ij, b2
ij] · [d1
jk, d2
jk]

,
♦
n
j=1

[a1
ij, a2
ij] · [d1
jk, d2
jk] + [b1
ij, b2
ij] · [c1
jk, c2
jk]
 
=
#
▽
n

j=1
-
min
s,t=1,2{as
ijct
jk} −max
s,t=1,2{bs
ijdt
jk}
.
,
△
n

j=1
-
max
s,t=1,2{as
ijct
jk} −min
s,t=1,2{bs
ijdt
jk}
.$
,
#
▽
n

j=1
-
min
s,t=1,2{as
ijdt
jk} + min
s,t=1,2{bs
ijct
jk}
.
,
△
n

j=1
-
max
s,t=1,2{as
ijdt
jk} + max
s,t=1,2{bs
ijct
jk}
.$
.

230
6 Implementation of Floating-Point Arithmetic on a Computer
As before, the products occurring in this expression are of length 2r. To determine
their minimum and maximum, Table 4.1 in Section 4.2 can be used. The sums and
differences can then be evaluated by applying one of the methods for the computation
of the exact scalar product. See Chapter 8.
If T denotes one of the sets (types) Z, S, CS, IS, CIS, we denote the set of vectors
with components in T by
VnT := {(ai) | ai ∈T for i = 1(1)n}
and the set of transposed vectors with components in T by
V ′
nT := {(ai)′ | ai ∈T for i = 1(1)n}.
The operations deﬁned above for matrices are directly extended to vectors if we
identify the sets
VnT ≡Mn1T,
V ′
nT ≡M1nT,
T ≡M11T.
The relevant operators, as well as the types of the results, are shown in Table 6.5.
◦
T
MmnT
VnT
V ′
nT
T
T
−
−
−
MmnT
−
MmnT
−
−
VnT
−
−
VnT
−
V ′
nT
−
−
−
V ′
nT
◦=
⎧
⎪
⎨
⎪
⎩
+, −
T = Z
+ , −
T = S or CS
♦
+ , ♦
−
T = IS or CIS
◦
T
MnpT
VnT
V ′
nT
T
T
MnpT
VnT
V ′
nT
MmnT
−
MmpT
VmT
−
VnT
−
−
−
MnnT
V ′
nT
−
V ′
pT
T
−
◦=
⎧
⎪
⎨
⎪
⎩
·
T = Z
·
T = S or CS
♦·
T = IS or CIS
Table 6.5. Type of results of matrix and vector operations.
Transposed vectors are used to perform scalar or dyadic products. The left table
also holds for the intersection and interval hull of matrices and vectors with compo-
nents of IS or CIS.
The order relation ≤for matrices of MmnT with T ∈{Z, S, CS, IS, CIS} is de-
ﬁned by

(aij),(bij)∈MmnT
(aij) ≤(bij) :⇔

i=1(1)m

j=1(1)n
aij ≤bij
and

a,b∈MmnT
a < b :⇔a ≤b ∧a ̸= b.

6.11 The Level 2 Operations
231
In MmnIS, inclusion ⊆is deﬁned by

(aij),(bij)∈MmnIS
(aij) ⊆(bij) :⇔

i=1(1)m

j=1(1)n
aij ⊆bij
and

a,b∈MmnIS
a ⊂b :⇔a ⊆b ∧a ̸= b.
In MmnCIS, inclusion is deﬁned by

((aij,bij)),((cij,dij))∈MmnCIS
((aij, bij)) ⊆((cij, dij))
:⇔

i=1(1)m

j=1(1)n
aij ⊆cij ∧bij ⊆dij
and

a,b∈MmnCIS
a ⊂b :⇔a ⊆b ∧a ̸= b.
We have now completed speciﬁcation of matrix and vector operations when the
components of both operands are of the same type for all the basic data sets (types) Z,
S, CS, IS, CIS. We now consider the operations for which the components of both
operands are of different types.
As before, the general rule for performing the matrix and vector operations for
operands of different component types is to lift the operand with the simpler compo-
nent type into the type of the other operand by means of a type transfer. Then the
operation can be executed as one of the same component type, all of which we have
already dealt with. If, for instance, a matrix a ∈MnnS has to be multiplied by a
vector b ∈VnCS, we transfer a into an element of MnnCS by adding zero imaginary
parts to all of the components of a. Then we multiply this matrix of MnnCS by the
vector b ∈VnCS. Or if, for instance, a matrix a ∈MmnZ has to be multiplied by
a vector b ∈VnIS, we ﬁrst transfer a into a ﬂoating-point matrix of MmnS and then
into an interval matrix of MmnIS whose components are all point intervals. Then we
multiply the matrix a ∈MmnIS by the vector b ∈VnIS.
Now let T1, T2, and T3 each denote one of the basic data sets (types) Z, S, CS, IS,
CIS. We consider the set of matrices MmnTi, vectors VnTi, and transposed vectors
V ′
nTi, i = 1(1)3, whose components are chosen from the basic data types. Among
other operations, elements of these sets can be multiplied. Table 6.6 (bottom) displays
the types of such products. In this table, T3 is to be replaced by the result type of
Table 6.3 if the components of the operands are of the type T1 and T2 respectively.
A corresponding table for matrix and vector addition, subtraction, intersection, and
interval hull is also given in Table 6.6.

232
6 Implementation of Floating-Point Arithmetic on a Computer
+, −, ∩, ¯∪
T2
MmnT2
VnT2
V ′
nT2
T1
T3
−
−
−
MmnT1
−
MmnT3
−
−
VnT1
−
−
VnT3
−
V ′
nT1
−
−
−
V ′
nT3
·
T2
MnpT2
VnT2
V ′
nT2
T1
T3
MnpT3
VnT3
V ′
nT3
MmnT1
−
MmpT3
VmT3
−
VnT1
−
−
−
MnnT3
V ′
nT1
−
V ′
pT3
T3
−
Table 6.6. Type of the result of matrix and vector operations.
To conclude we emphasize that the formulas and the discussion in this section show
that all of the level 2 operations and relations can be performed on a computer if the
level 1 operations are available. The level 1 operations include an exact scalar product.

Chapter 7
Hardware Support for Interval Arithmetic
A hardware unit for interval arithmetic (including division by an interval
that contains zero) is described in this chapter. After a brief introduction
an instruction set for interval arithmetic is deﬁned which is attractive from
the mathematical point of view. The set consists of the basic arithmetic
operations and comparisons for intervals including the relevant lattice oper-
ations. To enable high speed, the case selections for interval multiplication
(9 cases) and interval division (14 cases) are done in hardware. The lower
bound of the result is computed with rounding downwards and the upper
bound with rounding upwards by parallel units simultaneously. The round-
ing mode must be inherent to all the arithmetic operations. Also the basic
comparisons for intervals together with the corresponding lattice operations
and the result selection in more complicated cases of multiplication and
division are done in hardware by parallel units. The circuits described in
this chapter show that with modest additional hardware costs interval arith-
metic can be made almost as fast as simple ﬂoating-point arithmetic. Fast
hardware support of interval arithmetic is an essential engredient of what is
called advanced computer arithmetic in this book.
7.1
Introduction
Interval mathematics has been developed to a high standard over the last few decades.
It provides methods which deliver results with guarantees. However, the arithmetic on
existing processors makes these methods slow. This chapter deals with the question of
how interval arithmetic can be provided effectively on computers. This is an essential
prerequisite for the superior and fascinating properties of interval mathematics to be
more widely used in the scientiﬁc computing community. With more suitable proces-
sors, rigorous methods based on interval arithmetic could be comparable in speed to
today’s approximate methods. Interval arithmetic is a natural extension to ﬂoating-
point arithmetic, not a replacement for it. As computers speed up, from gigaﬂops to
teraﬂops to petaﬂops, interval arithmetic becomes a principal and necessary tool for
controlling the precision of a computation as well as the accuracy of the computed
and guaranteed result.
In conventional numerical analysis Newton’s method is the key algorithm for non-
linear problems. The method converges quadratically to the solution if the initial value

234
7 Hardware Support for Interval Arithmetic
of the iteration is already close enough to it. However, it may fail in ﬁnite or inﬁnite
precision arithmetic even if there is only one solution in the given interval. In con-
trast to this, the interval version of Newton’s method is globally convergent. It never
fails, not even in rounded arithmetic. Newton’s method reaches its ultimate elegance
and power in the extended interval Newton method. It yields enclosures of all single
zeros in a given domain. It is quadratically convergent. The key operation to achieve
these fascinating properties is division by an interval which contains zero. It separates
different zeros from each other. A method which provides for computation of all the
zeros of a system of equations in a given domain is very frequently used. This jus-
tiﬁes taking division by an interval which contains zero into the basic set of interval
operations, and implementing it by the hardware of the computer.
In this chapter the consideration is restricted to operands or IR. These are closed
and bounded intervals of real numbers. The result, however, may be an element of
(IR).
To handle critical situations it is generally agreed that interval arithmetic must be
supplemented by some measure to increase the precision within a computation. An
easy way to achieve this is an exact multiply and accumulate instruction or, what
is equivalent to it, an exact scalar product. With it quadruple or multiple precision
arithmetic can easily be provided. If the multiply and accumulate instruction is im-
plemented in hardware it is very fast. The data can be stored and moved as double
precision ﬂoating-point numbers. Generally speaking, interval arithmetic brings guar-
antees and mathematics into computation, while the exact multiply and accumulate
instruction brings higher (dynamic) precision and accuracy. Hardware support for the
exact multiply and accumulate instruction is discussed in Chapter 8 of the book. Fast
quadruple or multiple precision arithmetic and multiple precision interval arithmetic
are discussed in Chapter 9.
7.2
An Instruction Set for Interval Arithmetic
From the mathematical point of view, the following instructions for interval operations
and comparisons are desirable. In the following A = [a1, a2] and B = [b1, b2] denote
interval operands of IR. C = [c1, c2] denotes the result of an interval operation. If
in C a bound is −∞or +∞the bound is not included in the interval. In this case a
round bracket is used for the representation of the interval at this interval bound. For
the operations ◦∈{+, −, ·, /} with rounding downwards or upwards, the symbols
▽
◦, △
◦, ◦∈{+, −, ·, /} are used respectively.
7.2.1
Algebraic Operations
Addition. C := [a1▽
+ b1, a2△
+ b2].
Subtraction. C := [a1▽
−b2, a2△
−b1].

7.2 An Instruction Set for Interval Arithmetic
235
Multiplication.
b1 ≥0
b1 < 0 ≤b2
b2 < 0
a1 ≥0
[a1▽· b1, a2△· b2]
[a2▽· b1, a2△· b2]
[a2▽· b1, a1△· b2]
a1 < 0 ≤a2
[a1▽· b2, a2△· b2]
[min(a1▽· b2, a2▽· b1),
max(a1△· b1, a2△· b2)]
[a2▽· b1, a1△· b1]
a2 < 0
[a1▽· b2, a2△· b1]
[a1▽· b2, a1△· b1]
[a2▽· b2, a1△· b1]
Division.
0 /∈B
b1 > 0
b2 < 0
a1 ≥0
[a1▽
/ b2, a2△
/ b1]
[a2▽
/ b2, a1△
/ b1]
a1 < 0 ≤a2
[a1▽
/ b1, a2△
/ b1]
[a2▽
/ b2, a1△
/ b2]
a2 < 0
[a1▽
/ b1, a2△
/ b2]
[a2▽
/ b1, a1△
/ b2]
0 ∈B
b1 = b2 = 0
b1 < b2 = 0
b1 < 0 < b2
0 = b1 < b2
a2 < 0
∅
[a2▽
/ b1, +∞) [a2▽
/ b1, a2△
/ b2]1 (−∞, a2△
/ b2]
a1 ≤0 ≤a2
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
(−∞, +∞)
a1 > 0
∅
(−∞, a1△
/ b1] [a1▽
/ b2, a1△
/ b1]1 [a1▽
/ b2, +∞)
In the table for division by an interval which contains zero, ∅denotes the empty
interval. Since the result of an interval operation is supposed to always be a single
interval, the results which consist of the union of two intervals are delivered and rep-
resented as improper intervals [a2▽
/ b1, a2△
/ b2] and [a1▽
/ b2, a1△
/ b1]. In these special
cases the left hand bound is higher than the right hand bound.
The empty set ∅needs a particular encoding. (+NaN, -NaN) may be an appropriate
encoding for the empty interval.
7.2.2
Comments on the Algebraic Operations
Except for a few cases in the table for division by an interval which contains zero, the
lower bound of the result is always obtained with an operation rounded downwards
and the upper bound with an operation rounded upwards. Multiplication and division
need all combinations of lower and upper bounds of input intervals depending on
the signs of the bounds. Thus an operand selection has to be performed before the
1special encoding of result (−∞, c2] ∪[c1, +∞)

236
7 Hardware Support for Interval Arithmetic
operation can be executed. However, in all cases of computing the bounds of the
result interval the left hand operand is a bound of the interval A = [a1, a2] and the
right hand operand is always a bound of the operand B = [b1, b2]. Thus the operand
selection can be executed for the bounds of A and B separately.
In one special case, 0 ∈A and 0 ∈B, multiplication requires the computation of
two result pairs. Then the interval hull of these is taken.
Otherwise, if 0 ∈B then division may produce various special results like +∞,
−∞or the empty interval.
7.2.3
Comparisons and Lattice Operations
c is a value of type boolean.
equality c := (a1 = b1 ∧a2 = b2),
less than or equal c := (a1 ≤b1 ∧a2 ≤b2),
greatest lower bound C := [min(a1, b1), min(a2, b2)],
least upper bound C := [max(a1, b1), max(a2, b2)],
inclusion c := (b1 ≤a1 ∧a2 ≤b2),
element of c := (b1 ≤a ∧a ≤b2), special case of inclusion,
interval hull C := [min(a1, b1), max(a2, b2)],
intersection C := [max(a1, b1), min(a2, b2)] or the empty interval ∅,
check interval branch on a1 > a2 (checking for proper interval).
7.2.4
Comments on Comparisons and Lattice Operations
For comparisons and lattice operations no shufﬂing of bounds is needed. All combi-
nations of minimum and maximum of lower and upper bounds do occur.
In IEEE arithmetic the bit string of nonnegative ﬂoating-point numbers can be
treated as an integer for comparison purposes. Computation of minimum or maxi-
mum is done by comparison and selection.
Intersection and checking for an improper interval needs a comparison of the lower
and the upper bound of the result interval. In all other cases the lower and the upper
bound of the result interval can be computed independently.
7.3
General Circuitry for Interval Operations
and Comparisons
7.3.1
Algebraic Operations
We assume that the data are read from and written into a register ﬁle or through a
memory-access-unit. Each memory cell holds 128 bits for pairs of double precision

7.3 General Circuitry for Interval Operations and Comparisons
237
ﬂoating-point numbers holding the lower and upper bound of an interval. Intervals
are moved within the unit via busses of 128 bits.
There are three such busses in the interval arithmetic unit, one for the operand
A = [a1, a2], a second one for the operand B = [b1, b2], and a third one for the result
C = [c1, c2]. The interval arithmetic unit has two parts, one to perform the operand
selection and the operations, the other to perform comparisons and result selection.
See Figure 7.1.
..
A-Bypass
Operand A = [a1, a2]
Operand B = [b1,b2]
Register-File (or Memory-Access-Unit)
Result
Operand-Selection
Result-Selection
64-bit
0
1
n
.
Lower
Upper
for Intervals
(pairs of reals)
B-Bypass
C = [c1,c2]
a2
a1
b2
b1
a1
b1
a2
b2
c1
c2
and
Operations
and
Comparisons
a1
a2
b1
b2
Figure 7.1. General circuitry for interval operations and comparisions.
An operation is performed as follows: The operands A = [a1, a2] and B = [b1, b2]
are read from the memory onto the A-bus and B-bus and forwarded to the Operand-
Selection and Operations Unit. Here from the lower and upper bounds of the operands,
multiplexers select various combinations of values depending on the operation, and
on signs and zeros of all four values. Finally a pair of operations ▽
◦and △
◦, ◦∈
{+, −, ·, /}, is performed on the selected values, one with rounding downwards and
one with rounding upwards. See Figure 7.2. In many cases the computed values
are already the desired result C = [c1, c2]. In these cases the result is forwarded to
the memory via the C-bus. But there are exceptions, for instance multiplication where
both operands contain zero, or division by an interval that contains zero. See the tables
in Section 7.2. In these cases the computed values are forwarded to the Comparison
and Result-Selection Unit for further processing.
The selector signals oa1, oa2, ob1, and ob2 control the multiplexers. The Operand-
Selection and Operations Unit performs all arithmetic operations.
In addition the lower bounds of A and B are simply added with rounding down-
wards and the upper bounds with rounding upwards [a1▽
+ b1, a2△
+ b2]. The selector
signals are set to oa1 = 0, oa2 = 1, ob1 = 0, and ob2 = 1.

238
7 Hardware Support for Interval Arithmetic
In subtraction the bounds of B are exchanged by the operand selection. Then the
subtraction is performed, the lower bound being computed with rounding downwards
and the upper bound with rounding upwards [a1▽
−b2, a2△
−b1]. The selector signals
are set to oa1 = 0, oa2 = 1, ob1 = 1, and ob2 = 0.
Zero
Zero
Zero
Zero
Operand A
Operand B
a1
sa1
oa1
za2
za1
sa2
zb2
zb1
sb1
oa2
sb2
ob2
ob1
0
1
0
1
0
1
0
1
a2
b1
b2
Control-Signals:
s:
Sign
z:
Zero
o:
Operand-Select
a1| a2
b1| b2
b1| b2
a1| a2
a1 ∇ b1
a2 Δ b2
°
°
(Intermediate)
Result
a2
b1
a1
b2
Figure 7.2. Operand-Selection and Operations Unit.
Multiplication is a little more complicated. The various multiplications are per-
formed in the following way:
If neither operand A nor B contains zero (sa1 · sa2 · sb1 · sb2 = 0)
then the bounds are shufﬂed, the result [a1▽· b1, a2△· b2] is computed with the
selected bounds, and delivered to the target via the C-bus.
else
(i) The bounds are shufﬂed for the ﬁrst multiplication by operand selec-
tion, the ﬁrst partial result [a1▽· b2, a1△· b1] is computed, and it is for-
warded to the Comparison and Result-Selection Unit via the A-bus.
(ii) The bounds are shufﬂed for the second variation by operand selection,
the second partial result [a2▽· b1, a2△· b2] is computed, and it is for-
warded to the Comparison and Result-Selection Unit via the B-bus.
(iii) In the Comparison and Result-Selection Unit the hull of the two mul-
tiplications is selected and delivered as the ﬁnal result to the target via
the C-bus.

7.3 General Circuitry for Interval Operations and Comparisons
239
For multiplication, the multiplexers are controlled by the following selector sig-
nals:1
oa1 = sb2 + sa1 · sb1 + ms,
oa2 = sb1 + sa1 · sb2 + ms,
ob1 = ms(sa2 + sa1 · sb2),
ob2 = sa1 + sa2 · sb1 + ms.
These signals are computed by the signs of the bounds of the interval operands
A = [a1, a2] and B = [b1, b2]. In the expressions the signal ms is zero in case 1, and
it is one in case 2, of the conditional statement above.
For multiplication, every operand selector signal can be realized by two or three
gates!
Division is a little simpler than multiplication, but it is organized in a similar pat-
tern:
If B does not contain zero (sb1 · zb1 + sb2 = 1)
then the bounds are shufﬂed, the result [a1▽
/ b1, a2△
/ b2] is computed with the
selected bounds, and delivered to the target via the C-bus
else
(i) the bounds are shufﬂed, the arithmetic operation a1▽
/ b1 and/or a2△
/ b2
is computed with the selected bounds, and it is forwarded to the Com-
parison and Result-Selection Unit together with the selection code for
special values
(ii) in the Comparison and Result-Selection Unit the result is generated
from arithmetic values and/or special values (−∞|∅1| + ∞|∅2), and
it is forwarded to the target via the C-bus. ∅1 and ∅2 are assumed
here to represent particular encodings of the empty set ∅= (∅1, ∅2).
(+NaN, -NaN) may be such an encoding.
The operand selection is controlled by the following selector signals:
oa1 = sb2 + sa1 · sb1,
oa2 = sb1 + sa2 · sb2,
ob1 = sa1 + sa2 · sb1,
ob2 = sa2 + sa1 · sb1.
For division, every operand selector signal can be realized by two gates!
For division by an interval which contains zero, the result is an extended interval
where at most one bound is a real number or it is an improper interval where the left
hand bound is higher than the right hand bound. The instruction check interval sup-
plies a test for improper intervals.2 In Newton’s method, for instance, the following
operation is then an intersection with a regular interval. It delivers one or two regular
intervals.
1Note: A negative sign is a 1. A bar upon a logical value means inversion. In the expressions a dot
stands for a logical and, and a plus for a logical or.
2Within the given framework of existing processors only one interval can be delivered as the result
of an operation. Other solutions which use special registers or ﬂags or new exceptions would require an
adaption of the operating system.

240
7 Hardware Support for Interval Arithmetic
7.3.2
Comparisons and Result-Selection
In this unit, comparisons and minima and maxima are to be computed. We mention
again that in IEEE arithmetic the bit string of nonnegative ﬂoating-point numbers can
be interpreted as an integer for comparison purposes. Computation of a minimum or
maximum comprises comparison and then selection. Thus the arithmetic that has to
be done in this unit is relatively simple. Again the computation of the lower bound
and the upper bound of the result is done in parallel and simultaneously. No bound
shufﬂing is necessary in this unit. See Figure 7.3.
The comparisons for equality, less than or equal and set inclusion are done by
comparing the bounds, combining the results and setting a ﬂag in the state register.
For the operation element of an interval, [a, a] is built, then a test for inclusion is
applied.
The minimum and maximum computations for the operations higher lower bound,
lower upper bound, interval hull, and result selection for multiplication where 0 ∈
A and 0 ∈B are executed by comparing the bounds and selecting the higher and the
lower, respectively. Then the result is delivered to the target.
Lower Bounds
Upper Bounds
a1 
a1 ∇ b2
Result
a2 ≤ b2
a1 ≤ b1
max ≤ min
+NaN
-NaN
-∞
+∞
≡
≡
v
v
v
x1
x2
q1
q2
0
1
0
1
00
10
01
01
10
00
m1
m2
n
i1
n
i2
n‘
i‘1
n‘
i‘2
Remark:
IEEE-Floats may 
be compared like 
integers
is
is
n‘ i‘
n‘ i‘
Control Signals:
q
equality
l
less-than
x
sel. Maximum
m
mux-contr.
is
Intersect-Inst.
v
Valid Interval
n
gen. NaN
i
gen. Infinity
c1
c2
*
b1 
a2 ∇ b1
*
a2 
a1 Δ b1
*
b2 
a2 Δ b2
*
l2
l1
max(a1,b1)
min(a2,b2)
Figure 7.3. Comparisons and Result-Selection Unit.
The computation of the intersection is slightly more complicated. First the bounds
of the operands are compared and the higher lower bound and the lower upper bound
are selected. If max(a1, b1) ≤min(a2, b2), the intersection, which is the interval
[max(a1, b1), min(a2, b2)] is delivered to the target, otherwise the empty interval
is delivered.
In Figure 7.3 it is assumed that the empty set is encoded as ∅=
(+NaN, −NaN).

7.3 General Circuitry for Interval Operations and Comparisons
241
For division by an interval which contains zero, an interval with bounds like +NaN,
−∞, -NaN, +∞has to be delivered. These alternatives are selected in the Comparison
and Result-Selection Unit.
Now we deﬁne the various selector signals that appear in Figure 7.3.
n = n1 = n2 = (za1 · sa1 · sa2 + sa2) · zb2 · sb1,
3 Gates,
i1 = za1 · zb1 + za1 · sb1 · sb2 + sa1 · sa2 · sb1 · sb2
+ sa2 · zb1 · zb2 + sa1 · zb1 · zb2 + sa1 · sa2 · zb1,
7 Gates,
i2 = za1 · zb1 + za1 · sb1 · sb2 + sa1 · sa2 · sb1 · sb2
+ sa1 · sa2 · zb1 + sa2 · zb1 · zb2 + sa2 · zb1 · za2,
4 Gates,
i′
1 = is · i1,
i′
2 = is · i2,
n′ = n1 = n2 = is · n + n · v.
The expressions for i1 and i2 contain common subexpressions.
The logical expressions given in this section were developed from function tables
of up to several hundred entries. Then minimization was performed which leads to
the equations given. We do not repeat the tables and the minimization here because all
this is a standard procedure in circuit design. The function tables contain very many
“don’t care” entries which allows realization with a very small number of gates. Since
the “don’t care” entries may be used during minimization in different ways, various
designs may end up with different equations of equal complexity.
7.3.3
Alternative Circuitry for Interval Operations and Comparisons
In Figure 7.2 the operand selection and the execution of the interval operations to-
gether form the Operand-Selection and Operations Unit. This is justiﬁed since the
time needed for the operand selection is negligible in comparison to the time needed
to perform the arithmetic operations.
It may, however, be useful to separate the operand selection from the operations
themselves. Separation into two units would make it easier to use these for other
purposes. Examples are ordinary arithmetic or shufﬂing of parts of the operands.
Many processors are being built which already provide several independent arithmetic
units. These may then be easily used as part of the interval operations unit. Figure 7.4
shows a circuit for such interval operations and comparisons.
The Operand-Selection Unit and the Operations Unit would then look as shown in
Figure 7.5 and Figure 7.6. We will not discuss these circuits in further detail. Their
functionality should be clear from what has been said already.
In summary it can be said that a hardware unit for fast interval arithmetic has a very
regular structure. Interval arithmetic is just regular arithmetic for pairs of reals with
particular roundings, plus operand selection, plus clever control.

242
7 Hardware Support for Interval Arithmetic
o ∈ {+,-,*,/}
..
A-Bypass
Operand A = [a1, a2]
Operand B=[b1,b2]
Register-File (or Memory-Access-Unit)
Result
Operand-Selection
a1∇b1
a2Δb2
Result-Selection
64-bit
0
1
n
.
Lower
Upper
for Intervals
(pairs of reals)
B-Bypass
C = [c1,c2]
a2
a1
b2
b1
a1
a1
b2
b1
a2
b2
a2
b1
a1
b1
a2
b2
c1
c2
a1
a2
b1
b2
°
°
Figure 7.4. General circuitry for interval operations and comparisons.
Zero
Zero
Zero
Zero
Operand A
Operand B
a1
sa1
oa1
za2
za1
sa2
zb2
zb1
sb1
oa2
sb2
ob2
ob1
0
1
0
1
0
1
0
1
Intermediate Operand A
Intermediate Operand B
a2
b1
b2
Control-Signals:
s:
Sign
z:
Zero
o:
Operand-Select
b1
a2
a1
b2
a1| a2
b1| b2
b1| b2
a1| a2
Figure 7.5. Operand Selection Unit.
Lower Bounds
Upper Bounds
a1
b1
a2
b2
a1 ∇ b1
a2 Δ b2
°
°
(Intermediate)
Result
Figure 7.6. Arithmetic Operations Unit.

7.3 General Circuitry for Interval Operations and Comparisons
243
7.3.4
Hardware Support for Interval Arithmetic on X86-Processors
It is interesting to note that most of what is needed for fast hardware support for
interval arithmetic is already available on current x86-processors. Figure 7.7 shows
ﬁgures from various publications by Intel.
Figure 6. Packed double precision ﬂoating-point data type
Figure 11-3. Packed Double-Precision Floating-Point Operation
Figure 11-5. SHUFFD Instruction Packed Shufﬂe Operation
Figure 7.7. Figures from various Intel publications.
On an Intel Pentium 4, for instance, eight registers are available for words of 128
bits (xmm0, xmm1, . . ., xmm7). The x86-64 processors even provide 16 such regis-
ters. These registers can hold pairs of double precision ﬂoating-point numbers. They
can be viewed as bounds of intervals. Parallel operations like +, −, ·, /, min, max, and
compare can be performed on these pairs of numbers. What is not available and would

244
7 Hardware Support for Interval Arithmetic
be needed is for one of the two operations to be rounded downwards and the other one
rounded upwards. The last picture in Figure 7.7 shows that even shufﬂing of bounds
is possible under certain conditions. This is half of operand selection needed for in-
terval arithmetic. So an interval operation would need two such units or to pass this
unit twice. Also nearly all of the data paths are available on current x86-processors.
Thus full hardware support of interval arithmetic would probably add less than 1%,
more likely less than 0.1% to a current Intel or AMD x86 processor chip.
Full hardware support of fast interval arithmetic on RISC processors may cost a lit-
tle more as these lack pairwise processing. But most of them have two arithmetic units
and use them for super scalar processing. What has to be added is some sophisticated
control.
7.3.5
Accurate Evaluation of Interval Scalar Products
Chapter 8 deals with exact computation of scalar products of two vectors with ﬂoating-
point components. This requires computation of the products of corresponding vector
components to the full double length and exact accumulation of these products. In
Chapter 9 use will often be made of exact scalar products of two vectors with inter-
val components. These can be computed by hardware which is very similar to that
developed in this chapter.
If A = (Aν) and B = (Bν) with Aν = [aν1, aν2] and Bν = [bν1, bν2] ∈IS are two
such vectors this requires evaluation of the formulas
A♦· B = ♦
 n
ν=1
Aν · Bν

=

♦
n
ν=1
Aν · Bν

(7.3.1)
=
%
▽
n

ν=1
min
i,j=1,2(aνibνj), △
n

ν=1
max
i,j=1,2(aνibνj)
&
.
(7.3.2)
Here the products of the bounds of the vector components aνibνj are elements
of R but in general not of S. They are to be computed to the full double length.
Then the minima and maxima have to be selected. These selections can be done
by distinguishing the nine cases shown in Table 4.1. The selected products are then
accumulated in R as an exact scalar product. Finally the sum of products is rounded
only once by ▽(resp. △) from R into S. The two sums can be computed into two
long ﬁxed-point registers by one of the techniques shown in Chapter 8.
The case distinctions (the minimum and maximum selection in (7.3.2)) can be hard-
ware supported as shown in Figure 7.2 or 7.5. However, the succeeding multiplica-
tions of these ﬁgures have here to be done without rounding. Clearly, forwarding the
products to the result selection unit or to the target in Figures 7.1 and 7.4 then requires
wider busses. Design of a scalar product unit for vectors with interval components is
an interesting task. See Section 8.6.2.

Chapter 8
Scalar Products and Complete Arithmetic
This chapter deals with the implementation of the scalar or dot product of
two ﬂoating-point vectors, an operation which is occasionally called multi-
ply and accumulate. Basic computer arithmetic and the mapping principle
of semimorphism require that the computed result of the scalar product dif-
fers from the exact result by at most one rounding. The circuits developed
in this chapter, however, go beyond the concept of semimorphism. Scalar
products are computed exactly. The gain is simplicity, speed and accuracy.
The circuits described here for the computation of such scalar products can
be used in all kinds of computers: personal computers, workstations, main-
frames, supercomputers, and digital signal processors.
The most natural way to compute a scalar product exactly is to add the
products of the vector components into a long ﬁxed-point register on the
arithmetic unit. This method has the advantage of being quite simple and
very fast. A special resolution technique is used to absorb the carry as soon
as it arises. Since ﬁxed-point accumulation is error free it always provides
the desired exact answer. So we have the seeming paradox and striking out-
come that scalar products of vectors with millions of components can be
computed exactly using a relatively small ﬁnite register in the arithmetic
unit. Fixed-point accumulation is simpler and faster than accumulation in
ﬂoating-point arithmetic. The circuits developed in this chapter show that
there is no way to compute an approximation of a scalar product faster than
the exact result. In a very natural pipeline the arithmetic, consisting of mul-
tiplication and accumulation, can be performed in the time which is needed
to read the data into the arithmetic unit. Pipelining is a basic ﬁrst step to-
wards parallel computing and high speed. In numerical analysis, the scalar
product is ubiquitous.
To make the new capability conveniently available to the user a new data
format called complete is used together with a few simple arithmetic opera-
tions associated with each ﬂoating-point format. Complete arithmetic com-
putes all scalar products of ﬂoating-point vectors exactly. The result of com-
plete arithmetic is always exact; it is complete, not truncated. Not a single
bit is lost. A variable of type complete is a ﬁxed-point word wide enough to
allow exact accumulation (continued summation) of ﬂoating-point numbers
and of simple products of such numbers. If register space for the complete
format is available complete arithmetic is very very fast. The arithmetic

246
8 Scalar Products and Complete Arithmetic
needed to perform complete arithmetic is not much different from that avail-
able in a conventional CPU. In the case of the IEEE double precision format
a complete register consists of about 1/2 K bytes. Instructions for complete
arithmetic for low and high level programming languages are discussed.
8.1
Introduction and Motivation
The number one requirement for computer arithmetic has always been speed. It is
the main need that drives the technology. With increased speed larger problems can
be attempted. Pipelining of arithmetic operations and vector processing are impor-
tant techniques to speed up a computation. Particularly suited to pipelining are so-
called elementary compound operations like accumulate or multiply and accumulate.
The ﬁrst operation is a particular case of the second one which computes a sum of
products, the dot product or scalar product of two vectors. Advanced processors and
programming languages offer these operations. Pipelining makes them really fast,
especially if there are many summands.
In conventional vector processors the accumulation is done in ﬂoating-point arith-
metic by the so-called partial sum technique. The pipeline for the accumulation con-
sists of p steps. What comes out of the pipeline is wrapped back into the input of the
pipeline as a second summand. Thus p sums are built up in ﬂoating-point arithmetic.
Finally these sums are added together to obtain the scalar product. This partial sum
technique changes the sequence of the summands. This is an additional source of er-
ror. The conventional error analysis for a ﬂoating-point algorithm does not necessarily
hold for this kind of evaluation. Difﬁculties concerning the accuracy of conventional
vector processors have been addressed in the literature [202, 489, 656, 657]. This is an
unsatisfactory situation. The user should not be obliged to perform an error analysis
every time a compound arithmetic operation, implemented by the manufacturer or in
the programming language, is employed. If such operations are automatically inserted
into a user’s program by a vectorizing compiler the user loses complete control of his
computation.
Computer users, vendors, and even mathematicians often argue that a more accurate
arithmetic is not needed since the underlying model is imprecise, or discretization
errors are much larger than rounding errors.
This kind of justiﬁcation is rather dubious. One source of error does not justify
adding another one. Even an imprecise mathematical model or imprecise data will
suffer from the use of an imprecise or sloppy arithmetic. The systematic development
of a mathematical model requires that the error resulting from the computation can
largely be excluded. This requires the best possible arithmetic. What happens outside
the computer is the responsibility of the user. As soon as the data are in the computer
treating them as exact is a must. The vendor is responsible for the arithmetic that
is used in the computer. To be as accurate as possible is also a must. Internal and

8.2 Historic Remarks
247
external error sources must be separately identiﬁable.
There is another side of the computational coin apart from speed: the accuracy
and reliability of the computed result. Progress on this side is also very important,
if not essential. Basic computer arithmetic requires that all computer approximations
of arithmetic operations – in particular those in the usual vector and matrix spaces –
differ from the exact result by at most one rounding.
The concept of semimorphism requires that scalar products be computed with but
a single rounding. Extensive studies of the topic and a look into computing history
(see the next section), however, show that accumulations of products can easily be
performed exactly on the computer with no rounding whatsoever. These concepts
are discussed in this chapter. This goes beyond the concept of semimorphism. The
methods lead to a considerable gain in computing speed. Exception handling is greatly
simpliﬁed.
In this chapter, scalar product units are developed for different kinds of computer:
personal computers, workstations, mainframes, supercomputers and digital signal
processors. All these units compute the scalar product exactly. Not a single bit is lost.
The products are added into a ﬁxed-point register in the arithmetic unit. Fixed-point
addition is error free. It always provides the desired exact answer. Pipelining makes
these operations really fast. The differences in the circuits for the different processors
are dictated by the speed with which the processor delivers the data into the arithmetic
or scalar product unit. The data are the vector components. In all cases the extended
computational capability is gained at modest cost. The cost increase is comparable
to that from a simple to a fast multiplier, for instance by a Wallace tree, accepted
years ago. It is a central result of the development that for all processors mentioned
above, circuits can be given for the computation of the exact scalar product with vir-
tually no unoverlapped computing time needed for the execution of the arithmetic. In
a pipeline, the arithmetic can be executed in the time the processor needs to read the
data into the arithmetic unit. This means that no other method of computing a scalar
product can be faster, in particular not a conventional approximate computation of the
scalar product in double or quadruple precision ﬂoating-point arithmetic. Fixed-point
accumulation of the products is simpler than accumulation in ﬂoating-point. Many
intermediate steps that are executed in a ﬂoating-point accumulation, such as nor-
malization and rounding of the products and the intermediate sum, composition into
a ﬂoating-point number and decomposition into mantissa and exponent for the next
operation, do not occur in the ﬁxed-point accumulation of the exact scalar product.
Since the result is always exact, no exception handling is needed.
8.2
Historic Remarks
Floating-point arithmetic has been used since the early forties and ﬁfties (Zuse Z3,
1941) [63, 500, 501, 533]. Technology in those days was poor (electromechanical

248
8 Scalar Products and Complete Arithmetic
relays, electron tubes). It was complex and expensive. The word size of the Z3 was
24 bits. The storage provided 64 words. The four elementary ﬂoating-point operations
were all that could be provided. For more complicated calculations error analysis was
left to the user.
Before that time, highly sophisticated mechanical computing devices were used.
Several very interesting techniques provided the four elementary operations addition,
subtraction, multiplication and division. Many of these calculators were able to per-
form an additional ﬁfth operation which was called Auﬂaufenlassen or the running
total. The input register of such a machine had perhaps 10 or 12 decimal digits. The
result register was much wider and had perhaps 30 digits. It was a ﬁxed-point register
which could be shifted back and forth relative to the input register. This allowed a
continuous accumulation of numbers and of products of numbers into different po-
sitions of the result register. Fixed-point accumulation was thus error free. See Fig-
ures 8.1(a)–(d).
On all these computers the result register is much wider than the input data register.
The two calculators displayed in the Figures 8.1(c) and (d) show more than one long
result register. This allowed the simultaneous accumulation of different long sums.
This ﬁfth arithmetic operation was the fastest way to use the computer. It was
applied as often as possible. No intermediate results needed to be written down and
typed in again for the next operation. No intermediate roundings or normalizations
had to be performed. No error analysis was necessary. As long as no underﬂow or
overﬂow occurred, which would be obvious and visible, the result was always exact.
It was independent of the order in which the summands were added. Rounding was
only done, if required, at the very end of the accumulation.
This extremely useful and fast ﬁfth arithmetic operation was not built into the early
ﬂoating-point computers. It was too expensive for the technologies of those days.
Later its superior properties had been forgotten. Thus ﬂoating-point arithmetic is still
comparatively incomplete.
After Zuse, the electronic computers of the late forties and early ﬁfties represented
their data as ﬁxed-point numbers. Fixed-point arithmetic was used because of its supe-
rior properties. Fixed-point addition and subtraction are exact. Fixed-point arithmetic
with a rather limited word size, however, imposed a scaling requirement. Problems
had to be preprocessed by the user so that they could be accommodated by this ﬁxed-
point number representation. With increasing speed of computers, the problems that
could be solved became larger and larger. The necessary preprocessing soon became
an enormous burden.
Thus automatic scaling became generally accepted as ﬂoating-point arithmetic.
It largely eliminated this burden. A scaling factor is appended to each number in
ﬂoating-point representation. The arithmetic hardware takes care of the scaling. Ex-
ponents are added (subtracted) during multiplication (division). It may result in a big
change in the value of the exponent. But multiplication and division are relatively sta-

8.2 Historic Remarks
249
(a) Burkhardt Arithmometer Step
drum calculating machine by Arthur
Burkhardt & Cie, Glash¨utte, Germany,
1878.
(b) Brunsviga Pin wheel calculat-
ing machine BRUNSVIGA, system
Trinks,
by
Brunsviga
Maschinen-
werke Grimme Natalis & Co., Braun-
schweig, Germany, 1917.
(c) MADAS, by H.W. Egli, Z¨urich,
Switzerland (1936)
(Multiplication, Automatic Division,
Addition, Subtraction).
(d)
MONROE,
model
MONRO-
MATIC ASMD (1956),
by
Monroe
Calculating
Machine
Company, Inc., Orange, New Jersey,
USA.
Figure 8.1. Some mechanical computing devices developed between 1878
and 1956.

250
8 Scalar Products and Complete Arithmetic
ble operations in ﬂoating-point arithmetic. Addition and subtraction, on the contrary,
are troublesome in ﬂoating-point. Early ﬂoating point arithmetic was done on early
ﬁxed-point machines by programming.
The quality of ﬂoating-point arithmetic has been improved over the years. The
data format was extended to 64 and even more bits and the IEEE arithmetic standard
has ﬁnally taken the bugs out of various realizations. Floating-point arithmetic has
been used very successfully in the past. Very sophisticated and versatile algorithms
and libraries have been developed for particular problems. However, in a general
application the result of a ﬂoating-point computation is often hard to judge. It can be
satisfactory, inaccurate or even completely wrong. Neither the computation itself nor
the computed result indicate which one of the three cases has occurred. We illustrate
the typical shortcomings by three very simple examples. All data in these examples
are IEEE double precision ﬂoating-point numbers! For these and other examples see
[507]:
Examples. 1. Compute the following, theoretically equivalent expressions:
1020
+
17
−
10
+
130
−
1020,
1020
−
10
+
130
−
1020
+
17,
1020
+
17
−
1020
−
10
+
130,
1020
−
10
−
1020
+
130
+
17,
1020
−
1020
+
17
−
10
+
130,
1020
+
17
+
130
−
1020
−
10.
A conventional computer using the double-precision data format of the IEEE ﬂoating-
point arithmetic standard returns the values 0, 17, 120, 147, 137, −10. These errors
come about because the ﬂoating-point arithmetic is unable to cope with the digit range
required with this calculation. Notice that the data cover less than 4% of the digit
range of the double precision data format!
2. Compute the solution of a system of two linear equations Ax = b, with
A =
 64919121
−159018721
41869520.5
−102558961

,
b =
1
0

.
The solution can be expressed by the formulas
x1 =
a22
a11a22 −a12a21
and
x2 =
−a21
a11a22 −a12a21
.
A workstation using IEEE double precision ﬂoating-point arithmetic returns the ap-
proximate solution
˜x1 = 102558961
and
˜x2 = 41869520.5,

8.2 Historic Remarks
251
while the correct solution is
x1 = 205117922
and
x2 = 83739041.
After only 4 ﬂoating-point operations all digits of the computed solution are wrong.
A closer look into the problem reveals that the error happens during the computation
of the denominator. This is just the kind of expression that will always be computed
exactly by the missing ﬁfth operation.
3. Compute the scalar product of the two vectors a and b with ﬁve components each:
a1 = 2.718281828 · 1010,
b1 = 1486.2497 · 109,
a2 = −3.141592654 · 1010,
b2 = 878366.9879 · 109,
a3 = 1.414213562 · 1010,
b3 = −22.37492 · 109,
a4 = 0.5772156649 · 1010,
b4 = 4773714.647 · 109,
a5 = 0.3010299957 · 1010,
b5 = 0.000185049 · 109.
The correct value of the scalar product is −1.00657107 · 108. IEEE-double precision
arithmetic delivers +4.328386285·109 so even the sign is incorrect. Note that no vec-
tor element has more than 10 decimal digits. IEEE arithmetic computes with a word
size that corresponds to about 16 decimal digits. Only 9 ﬂoating-point operations are
used.
Problems that can be solved by computers become larger and larger. Today fast
computers are able to execute several billion ﬂoating-point operations every second.
This number exceeds the imagination of any user. Traditional error analysis of nu-
merical algorithms is based on estimates of the error of each individual arithmetic
operation and on the propagation of these errors through a complicated algorithm. It
is simply no longer possible to expect that the error of such computations can be con-
trolled by the user. There remains no alternative to further developing the computer’s
arithmetic to enable it to control and validate the computational process.
Computer technology is extremely powerful today. It allows solutions which even
an experienced computer user may be totally unaware of. Floating-point arithmetic
which may fail in simple calculations, as illustrated above, is no longer adequate to
be used exclusively in computers of such gigantic speed for huge problems. The rein-
troduction into computers of the ﬁfth arithmetic operation, the exact scalar product, is
a step which is long overdue. A central and fundamental operation of numerical anal-
ysis which can be executed exactly with only modest technical effort should indeed
always be executed exactly and no longer only approximately. With the exact scalar
product all the valuable properties which have been listed in connection with the old
mechanical calculators return to the modern digital computer.
The exact scalar product is the fastest way to use the computer. It should be
applied as often as possible. No intermediate results need to be stored and

252
8 Scalar Products and Complete Arithmetic
read in again for the next operation. No intermediate roundings and nor-
malizations have to be performed. No intermediate overﬂow or underﬂow
can occur. No error analysis is necessary. The result is always exact. It is
independent of the order in which the summands are added. Rounding is
only done, if required, at the very end of the accumulation.
This chapter pleads for extending ﬂoating-point arithmetic by the exact scalar prod-
uct as the ﬁfth elementary operation. This combines the advantages of ﬂoating-point
arithmetic (no scaling requirement) with those of ﬁxed-point arithmetic (fast and ex-
act accumulation of numbers and of single products of numbers even for very long
sums). The exact scalar product reintegrates the advantages of ﬁxed-point arithmetic
into digital computing and ﬂoating-point arithmetic. It is obtained by putting a ca-
pability into modern computer hardware which was already available on calculators
before the electronic computer came on stage.
This chapter claims that, and explains how, scalar products, the source data of
which are ﬂoating-point numbers, can always be exactly computed. In the old days
of computing (1950–1980) computers often provided sloppy arithmetic in order to be
fast. This was “justiﬁed” by explaining that the last bit was incorrect in many cases,
due to rounding errors. So why should the arithmetic be slowed down or more hard-
ware be invested by computing the best possible answer of the operations under the
assumption that the last bit is correct? Today it is often asked: why do we need an
exact scalar product? The last bit of the data is often incorrect and a ﬂoating-point
computation of the scalar product delivers the best possible answer for problems with
perturbed data. With the IEEE arithmetic standard this kind of “justiﬁcation” has
been overcome. This allows problems to be handled exactly where the data are ex-
act and the best possible result of an operation is needed. In mathematics it makes a
big difference whether a computation is exact for many or most data or for all data!
For problems with perturbed (inexact) data, interval arithmetic is the appropriate tool.
The exact scalar product extends the accuracy requirements of the IEEE arithmetic
standard to all operations in the usual product spaces of computation, to complex
numbers, to vectors, matrices, and their interval extensions. If implemented in hard-
ware it brings an essential speed up of these operations, and it allows an easy and very
fast realization of multiple or variable precision arithmetic.
In short: Interval arithmetic brings guarantees and mathematics into computing,
while the exact scalar product brings speed, dynamic precision, and accuracy. A
combination of both is what is needed from a modern computer.
8.3
The Ubiquity of the Scalar Product in Numerical
Analysis
In numerical analysis the scalar or dot product is ubiquitous. It is not merely a funda-
mental operation in all the product spaces mentioned above. The process of residual

8.3 The Ubiquity of the Scalar Product in Numerical Analysis
253
or defect correction, or of iterative reﬁnement, is composed of scalar products. There
are well-known limitations to these processes in ﬂoating-point arithmetic. The ques-
tion of how many digits of a defect can be guaranteed in single, double or extended
precision arithmetic has been carefully investigated. With an exact scalar product the
defect can always be computed to full accuracy. It is the exact scalar product which
makes residual correction effective. It has a direct and positive inﬂuence on all itera-
tive solvers of systems of linear equations.
A simple example may illustrate the advantages of what has been said: Solving a
system of linear equations Ax = b is the central task of numerical computing. Large
linear systems can only be solved iteratively. Iterative system solvers repeatedly com-
pute the defect d (sometimes called the residual) d := b −A˜x of an approximation
˜x. It is well known that the error e of the approximation ˜x is a solution of the same
system with the defect as the right hand side: Ae = d. If ˜x is already a good ap-
proximation of the solution, the computation of the defect suffers from cancellation in
ﬂoating-point arithmetic, and if the defect is not computed correctly the computation
of the error does not make sense. In the computation of the defect it is essential that,
although ˜x is just an approximation of the solution x∗, ˜x is assumed to be an exact
input and that the entire expression for the defect b−A˜x is computed as a single exact
scalar product. This procedure delivers the defect to full accuracy and by that also to
multiple precision accuracy. Thus the defect can be read to two or three or four fold
precision as necessary in the form d = d1 + d2 + d3 + d4 as a long real variable.
The computation can then be continued with this quantity. This often has positive
inﬂuence on the convergence speed of the linear system solver [165, 166, 167, 189].
It is essential to understand that apart from the exact scalar product, all operations are
performed in double precision arithmetic and thus are very fast. If the exact scalar
product is supported by hardware it is faster than a conventional scalar product in
ﬂoating-point arithmetic1.
But also direct solvers of systems of linear equations proﬁt from computing the de-
fect to full accuracy. The step of verifying the correctness of an approximate solution
is based on an accurate computation of the defect. If a ﬁrst veriﬁcation attempt fails,
a good enough approximation can be computed with the exact scalar product. See
Section 9.5.
The so-called Krawczyk-operator which is used to verify the correctness of an ap-
proximate solution is able to solve the problem only in stable situations. Matrices
from the so-called Matrix Market usually are very ill conditioned. In such cases the
Krawczyk-operator almost never ﬁnds a veriﬁed answer. Similarly, for instance, in
the case of a Hilbert matrix of dimension greater than eleven the Krawczyk-operator
always fails to ﬁnd a veriﬁed solution. In all these cases, however, the Krawczyk-
operator recognizes that it cannot solve the problem and then automatically calls a
1An iterative method which converges to the solution in inﬁnite precision arithmetic often converges
more slowly or even diverges in ﬁnite precision arithmetic.

254
8 Scalar Products and Complete Arithmetic
more powerful operator, the so-called Rump-operator which then in almost all cases
solves the problem satisfactorily.
The Krawczyk-operator ﬁrst computes an approximate inverse R of the matrix A
and then iterates with the matrix I −RA, where I is the identity matrix. The Rump-
operator inverts the product RA in ﬂoating-point arithmetic again and then multiplies
its approximate inverse by RA. This product is computed with the exact scalar prod-
uct and from that it can be read to two or three or four fold precision as necessary as
a long-real matrix. The iteration then is continued with the matrix I −(RA)−1RA.
It is essential to understand that even in the Rump-algorithm all arithmetic operations
except for the (very fast) exact scalar product are performed in double precision arith-
metic and thus are very fast. This linear system solver is an essential ingredient of
many other problem solving routines with automatic result veriﬁcation. It is essential
to understand how it works.
To be successful interval arithmetic has to be complemented by some easy way to
use multiple or variable precision arithmetic. The fast and exact scalar product is the
tool to provide this very easily.
With the exact scalar product quadruple or other multiple precision arithmetic can
easily be provided on the computer. This enables the use of higher precision oper-
ations in numerically critical parts of a computation. It helps to increase software
reliability. A multiple precision number is represented as an array of ﬂoating-point
numbers. The value of this number is the sum of its components. The number can
be represented in the long register in the arithmetic unit. Addition and subtraction of
multiple precision numbers can easily be performed in this register. Multiplication of
two such numbers is simply a sum of products. It can be computed easily and fast by
means of the exact scalar product. For instance, using fourfold precision the product
of two such numbers a = (a1 + a2 + a3 + a4) and b = (b1 + b2 + b3 + b4) is obtained
by
a · b = (a1 + a2 + a3 + a4) · (b1 + b2 + b3 + b4)
= a1b1 + a1b2 + a1b3 + a1b4 + a2b1 + . . . + a4b3 + a4b4
=
4

i=1
4

j=1
aibj.
The result is independent of the sequence in which the summands are added.
Several of the XSC-languages, see for instance [288], provide intrinsic data types
l-real, l-interval, etc., which are implemented by the exact scalar product.
For details see Section 9.7 of this book and [392]. The value of a variable of type long
is the sum of its components. Addition and subtraction of such multiple precision
data can easily be performed in the long register. Multiplication of two variables
of this type can be computed easily and fast by the exact scalar product. Division is
performed iteratively. The multiple precision data type long is controlled by a global

8.3 The Ubiquity of the Scalar Product in Numerical Analysis
255
variable called stagprec (staggered precision). If stagprec is 1, the long type is
identical to its component type. If, for instance, stagprec is 4 each variable of this
type consists of an array of four variables of its component type. Again its value is the
sum of its components. The global variable stagprec can be increased or decreased
at any place in the program. Thus a loop can be executed starting with precision one
and the precision can be increased (or decreased) in each repetition as required by the
user. Thus no program changes are necessary to beneﬁt from increased precision. The
user or the computer itself can choose the precision which optimally ﬁts the problem.
The elementary functions are also available for the types l-real and l-interval
[114, 115, 311, 312, 313]. If stagprec is 2, a data type is encountered which is
occasionally denoted as double-double or quadruple precision.
If one runs out of precision in a certain problem class, one often runs out of quadru-
ple precision very soon as well. It is preferable and simpler, therefore, to provide a
high speed basis for enlarging the precision rather than to provide any ﬁxed higher
precision or to simulate higher precision in software. A hardware implementation of
a full quadruple precision arithmetic is more costly than an implementation of the ex-
act scalar product. The latter only requires ﬁxed-point accumulation of the products.
On the computer, there is only one standardised ﬂoating-point format that is double
precision.
For many applications it is necessary to compute the value of the derivative of
a function. Newton’s method in one or several variables is a typical example of this.
Modern numerical analysis solves this problem by automatic or algorithmic differenti-
ation. The so-called reverse mode is a very fast method of automatic differentiation. It
computes the gradient, for instance, with at most ﬁve times the number of operations
needed to compute the function value. The memory overhead and the spatial com-
plexity of the reverse mode can be signiﬁcantly reduced by the exact scalar product if
this is considered as a single, always correct, basic arithmetic operation in the vector
spaces [555, 556]. The very powerful methods of global optimization [492, 494] are
impressive applications of these techniques.
Many other applications require that rigorous mathematics can be done with the
computer using ﬂoating-point arithmetic. As an example, this is essential in simula-
tion runs (eigenfrequencies of a large generator, fusion reactor, simulation of nuclear
explosions) or mathematical modelling where the user has to distinguish between
computational artefacts and genuine reactions of the model. The model can only
be developed systematically if errors resulting from the computation can be excluded.
Nowadays computer applications are of immense variety. Any discussion of where
a dot product computed in quadruple or extended precision arithmetic can be used
to substitute for the exact scalar product is superﬂuous. Since the former can fail
to produce a correct answer an error analysis is needed for all applications. This
can be left to the computer. As the scalar product can always be executed exactly
with moderate technical effort it should indeed always be executed exactly. An error
analysis thus becomes irrelevant. Furthermore, the same result is always obtained

256
8 Scalar Products and Complete Arithmetic
on different computer platforms. An exact scalar product eliminates many rounding
errors in numerical computations. It stabilises these computations and speeds them
up as well. It is the necessary complement to ﬂoating-point arithmetic.
8.4
Implementation Principles
We begin with a brief review of the deﬁnition and the notation of ﬂoating-point num-
bers.
A normalized ﬂoating-point number x (in signed-magnitude representation) is a
real number of the form x = ◦m be. Here ◦∈{+, −} is the sign of the number, b is
the base of the number system in use and e is the exponent. The base b is an integer
greater than unity. The exponent e is an integer between two ﬁxed integer bounds e1
and e2, and in general e1 < 0 < e2. The mantissa (or signiﬁcand) m is of the form
m =
l

i=1
di · b−i.
The di are the digits of the mantissa. They have the property di ∈{0, 1, . . . , b −1}
for all i = 1(1)l and d1 ̸= 0. Without this last condition ﬂoating-point numbers
are said to be unnormalized. The set of normalized ﬂoating-point numbers does not
contain zero. For a unique representation of zero we assume the mantissa and the
exponent to be zero. Thus a ﬂoating-point system depends on the four constants
b, l, e1 and e2. We denote it by R = R(b, l, e1, e2). On occasion we shall use the
abbreviations sign(x), mant(x) and exp(x) to denote the sign, mantissa and exponent
of x respectively.
Now we turn to our task. Let a = (ai) and b = (bi), i = 1(1)n, be two vectors
with n components which are ﬂoating-point numbers, i.e.,
ai, bi ∈R(b, l, e1, e2), for i = 1(1)n.
We are going to compute the two results (scalar products):
s :=
n

i=1
ai · bi = a1 · b1 + a2 · b2 + . . . + an · bn,
and
c :=
n

i=1
ai · bi =
(a1 · b1 + a2 · b2 + . . . + an · bn) =
s,
where all additions and multiplications are the operations for real numbers and
is
a rounding symbol representing rounding to nearest, towards zero, upwards or down-
wards.

8.4 Implementation Principles
257
We shall discuss circuits for the hardware realization of these operations for proces-
sors like personal computers, workstations, mainframes, supercomputers and digital
signal processors. The differences in the circuitry for these various processors are
dictated by the speed with which the processor delivers the vector components ai and
bi, i = 1, 2, . . . , n, to the arithmetic or scalar product unit.
Since ai and bi are ﬂoating-point numbers with a mantissa of l digits, the products
ai · bi in the sums for s and c are ﬂoating-point numbers with a mantissa of 2l digits.
The exponent range of these numbers doubles also, i.e., ai · bi ∈R(b, 2l, 2e1, 2e2).
All these summands can be taken into a ﬁxed-point register of length 2e2+2l +2|e1|
without loss of information, see Figure 8.2: If one of the summands has an exponent
0, its mantissa can be taken into in a register of length 2l. If another summand has
exponent 1, it can be treated as having an exponent 0 if the register provides further
digits on the left and the mantissa is shifted one place to the left. An exponent −1 in
one of the summands requires a corresponding shift to the right. The largest exponents
in magnitude that may occur in the summands are 2e2 and 2|e1|. So any summand can
be treated as having an exponent 0 and be taken into a ﬁxed-point register of length
2e2 + 2l + 2|e1| without loss of information.
In the following two subsections we shall detail two principal solutions to the prob-
lem. The ﬁrst solution uses a long adder and a long shift. The second solution uses
a short adder and some local memory or register space in the arithmetic unit. At
ﬁrst sight both of these principal solutions seem to lead to relatively slow hardware
circuits. However more reﬁned studies will later show that very fast circuits can be
devised for both methods and for the various processors mentioned above. A ﬁrst step
in this direction is the provision of the very fast carry resolution scheme described in
Section 8.4.4.
Actually it is a central result of this study that, for all processors under consid-
eration, circuits for the computation of the exact scalar product are available for
which virtually no unoverlapped computing time for the execution of the arithmetic is
needed. In a pipeline, the arithmetic can be done within the time the processor needs
to read the data into the arithmetic unit. This means that no other method of comput-
ing the scalar product can be faster, in particular, not even a conventional computation
of scalar products in ﬂoating-point arithmetic which may lead to an incorrect answer.
Once more we emphasize the fact that the methods to be discussed here compute the
scalar product of two ﬂoating-point vectors of arbitrary ﬁnite length without loss of
information or with only one rounding at the very end of the computation.
8.4.1
Long Adder and Long Shift
If the register shown in Figure 8.2 is built as an accumulator with an adder, all sum-
mands could be added in without loss of information. To accommodate possible over-
ﬂows, it is convenient to provide a few, say k, more digits of base b on the left. With
such an accumulator, every such sum or scalar product can be added in without loss

258
8 Scalar Products and Complete Arithmetic
of information. As many as bk overﬂows may occur and be accommodated without
loss of information. In the worst case, presuming every sum causes an overﬂow, we
can accommodate sums with n ≤bk summands.
A teraﬂops computer would perform about 1020 operations in 10 years. So 20
decimal or about 65 binary digits certainly are a safe and reasonable upper bound for
k. Thus, the long accumulator and the long adder consist of L = k + 2e2 + 2l + 2|e1|
digits of base b. The summands are shifted to the proper position and added in. See
Figure 8.2. Fast carry resolution techniques will be discussed later. The ﬁnal sums s
and c are supposed to be in the single exponent range e1 ≤e ≤e2, otherwise c is not
representable as a ﬂoating-point number and the problem has to be scaled.
2l
2l
|e1|
2|e1|
e2
k
2e2
Figure 8.2. Long register with long shift for exact scalar product accumu-
lation.
8.4.2
Short Adder with Local Memory on the Arithmetic Unit
In a scalar product computation the summands are all of length 2l. So actually the
long adder and long accumulator may be replaced by a short adder and a local store
(register space) of size L on the arithmetic unit. The local store is organized in words
of length l or l′, where l′ is a power of 2 and slightly larger than l (for instance l = 53
bits and l′ = 64 bits). Since the summands are of length 2l, they ﬁt into a part of
the local store of length 3l′. This part of the store is determined by the exponent of
the summand. We load this part of the store into an accumulator of length 3l′. The
summand mantissa is placed in a shift register and is shifted to the correct position as
determined by the exponent. Then the shift register contents are added to the contents
of the accumulator. Figure 8.3.
An addition into the accumulator may produce a carry. As a simple method to
accommodate carries, we enlarge the accumulator on its left end by a few more digit
positions. These positions are ﬁlled with the corresponding digits of the local store.
If not all of these digits equal b −1 for addition (or zero for subtraction), they will
accommodate a possible carry of the addition (or borrow in the case of subtraction).
Of course, it is possible that all these additional digits are b −1 (or zero). Then a loop
can be provided that takes care of the carry and adds it to (subtracts it from) the next
digits of the local store. This loop may need to be traversed several times. Other carry

8.4 Implementation Principles
259
(borrow) handling processes are possible and will be dealt with later. This completes
our sketch of the second method for an exact computation of scalar products using a
short adder and some local store on the arithmetic unit. See Figure 8.3.
k
l’
local store of L digits
adder
l’
2l
shifted summand
Figure 8.3. Short adder and local store on the arithmetic unit for exact
scalar product accumulation.
8.4.3
Remarks
The scalar product is very frequent in scientiﬁc computing. The two solutions de-
scribed in the last two subsections are both simple, straightforward and mature.
Remark 8.1. The purpose of the k digits on the left end of the register in Figure 8.2
and Figure 8.3 is to accommodate possible overﬂows. The only numbers that are
added to this part of the register are plus or minus unity. So this part of the register
can just be treated as a counter by an incrementer/decrementer.
■
Remark 8.2. The ﬁnal result of a scalar product computation is assumed to be a
ﬂoating-point number with an exponent in the range e1 ≤e ≤e2. During the com-
putation, however, summands with an exponent outside of this range may well occur.
The remaining computation then has to cancel all the extra digits. However, in a nor-
mal scalar product computation, the register space outside the range e1 ≤e ≤e2
will be little used. The conclusion should not be drawn from this consideration that
the register size can be restricted to the single exponent range in order to save some
silicon area. This would require the implementation of complicated exception han-
dling routines which ﬁnally require as much silicon but do not solve the problem in
principle.
■
Remark 8.3. We emphasize once more that the number of digits, L, needed for
the register to compute scalar products of two vectors exactly only depends on the
ﬂoating-point data format. In particular it is independent of the number n of compo-
nents of the two vectors to be multiplied.

260
8 Scalar Products and Complete Arithmetic
As samples we calculate the register width L for a few typical and frequently used
ﬂoating-point data formats:
(a) IEEE-arithmetic single precision:
b = 2; word length: 32 bits; sign: 1 bit; exponent: 8 bits; mantissa: l = 24 bits;
exponent range: e1 = −126, e2 = 127, binary.
L = k + 2e2 + 2l + 2|e1| = k + 554 bits.
With k = 86 bits we obtain L = 640 bits. This register can be represented by 10
words of 64 bits.
(b) /370 architecture, long data format:
b = 16; word length: 64 bits; sign: 1 bit; mantissa: l = 14 hex digits; exponent
range: e1 = −64, e2 = 63, hexadecimal.
L = k + 2e2 + 2l + 2|e1| = k + 282 digits of base 16.
With k = 88 bits we obtain L = 88 + 4 · 282 = 1216 bits. This register can be
represented by 19 words of 64 bits.
(c) IEEE-arithmetic double precision:
b = 2; word length: 64 bits; sign: 1 bit; exponent: 11 bits; mantissa: l = 53 bits;
exponent range: e1 = −1022, e2 = 1023, binary.
L = k + 2e2 + 2l + 2|e1| = k + 4196 bits.
With k = 92 bits we obtain L = 4288 bits. This register can be represented by
67 words of 64 bits.
These samples show that the register size (at a time where memory space is mea-
sured in gigabits and gigabytes) is modest in all cases. It grows with the exponent
range of the data format. If this range should be extremely large, as for instance in the
case of an extended precision ﬂoating-point format, only an inner part of the register
would be supported by hardware. The outer parts which are used very rarely could be
simulated in software. The long data format of the /370 architecture covers a range of
about 10−75 to 1075, which is very modest. This architecture dominated the market for
more than 25 years and most problems could conveniently be solved with machines
of this architecture within this range of numbers.
■
Remark 8.4. Multiplication is often considered to be more complex than addition. In
modern computer technology this is no longer the case. Very fast circuits for multipli-
cation using carry-save-adders (Wallace tree) are available and common. They nearly
equalize the time to compute a sum and a product of two ﬂoating-point numbers. In
a scalar product computation a large number of products is usually to be computed.
The multiplier is able to produce these products very quickly. In a balanced scalar
product unit the accumulation should be able to absorb a product in about the same
time the multiplier takes to produce it. Therefore, measures have to be taken to equal-
ize the speed of both operations. Because of a possible long carry propagation the
accumulation seems to be the more complicated process.
■

8.4 Implementation Principles
261
Remark 8.5. Techniques to implement the exact scalar product on machines which
do not provide enough register space on the arithmetic logical unit will be discussed
later in this chapter.
■
8.4.4
Fast Carry Resolution
Both the solutions for our problem which have been sketched above seem at ﬁrst
glance to be slow. The ﬁrst solution requires a long shift which is necessarily slow.
The addition over perhaps 4000 bits is slow also, in particular if a long carry propa-
gation is necessary. For the second solution, ﬁve steps have to be carried out: 1. read
from the local store, 2. perform the shift, 3. add the summand, 4. resolve the carry,
possibly by loops, and 5. write the result back into the local store. Again the carry
resolution may be very time consuming.
As a ﬁrst step to speed up both solutions, we discuss a technique which allows a
very fast carry resolution. Actually a possible carry can be accommodated while the
product, the addition of which might produce a carry, is still being computed.
Both solutions require a long register in which the ﬁnal sum in a scalar product
computation is built up. Henceforth we shall call this register a complete register
and CR for short.2 It consists of L digits of base b. The CR is a ﬁxed-point register
wherein any sum of ﬂoating-point numbers and of simple products of ﬂoating-point
numbers can be represented without error.
To be more speciﬁc we now assume that we are using the double precision data
format of the IEEE-arithmetic standard 754. See case (c) of Remark 8.3. As soon
as the principles are clear, the technique can easily be applied to other data formats.
The mantissa here consists of l = 53 bits. We assume additionally that the CR that
appears in both solutions is subdivided into words of l′ = 64 bits. The mantissa of
the product ai · bi is then 106 bits wide. It touches at most three consecutive 64-bit
words of the CR which are determined by the exponent of the product. A shifter then
aligns the 106 bit product into the correct position for the subsequent addition into the
three consecutive words of the CR. This addition may produce a carry (or a borrow
in the case of subtraction). The carry is absorbed by the next more signiﬁcant 64 bit
word of the CR in which not all digits are 1 (or 0 for subtraction). Figure 8.4(a). For
fast identiﬁcation of this word two information bits or ﬂags are appended to each CR
word. Figure 8.4(b). One of these bits, the all bits 1 ﬂag, is set to 1 if all 64 bits of the
register word are 1. This means that a carry will propagate through the entire word.
The other bit, the all bits 0 ﬂag, is set to 0, if all 64 bits of the register word are 0.
This means that in the case of subtraction a borrow will propagate through the entire
word.
During the addition of a product into three consecutive words of the CR, a search is
started for the next more signiﬁcant word of the CR where the all bits 1 ﬂag is not set.
2For this use of complete see Section 8.7.

262
8 Scalar Products and Complete Arithmetic
This is the word which will absorb a possible carry. If the addition generates a carry,
this word must be incremented by one and all intermediate words must be changed
from all bits 1 to all bits 0. The easiest way to do this is simply to switch the ﬂag
bits from all bits 1 to all bits 0 with the additional semantics that if a ﬂag bit is set,
the appropriate constant (all bits 0 or all bits 1) must be generated instead of reading
the CR word contents when fetching a word from the CR, Figure 8.4(b). Borrows are
handled in an analogous way.
k
carry
start
address
carry
resolution
address
k
000000
110001 111111 111111 111111
XXXXXX
XXXXXX
XXXXXX
XXXXXX
XXXXXX 000000
carry skip area
carry generation
local fixed-point addition
a)
b)
000000
110001 111111 111111 111111
XXXXXX
XXXXXX
XXXXXX 000000
0
1
1
1
0
0
0
0
0
0
0
1
1
1
1
0
0
0
0
0
0
0
0
+1
extracted carry flag word
carry flag logic
carry dissolution indicator
Figure 8.4. Fast carry resolution.
This carry handling scheme allows a very fast carry resolution. The generation of
the carry resolution address is independent of the addition of the product, so it can be
performed in parallel. At the same time, a second set of ﬂags is set up for the case that
a carry is generated. In this case the carry is added into the appropriate word and the
second set of ﬂags is copied into the former ﬂag word.
Simultaneously with the multiplication of the mantissas of ai and bi their exponents
are added. This is just an eleven bit addition. The result is available very quickly. It
delivers the exponent of the product and the address for its addition. By using the
ﬂags, the carry resolution address can be determined and the carry word can be incre-
mented/decremented as soon as the exponent of the product is available. It could be
available before the multiplication of the mantissas is ﬁnished. If the accumulation of
the product then produces a carry, the incremented/decremented carry word is written
back into the CR, otherwise nothing is changed.

8.5 Scalar Product Computation Units (SPUs)
263
This very fast carry resolution technique could be used in particular for the compu-
tation of short scalar products which occur, for instance, in the computation of the real
and imaginary parts of the product of two complex ﬂoating-point numbers. A long
scalar product, however, is usually performed in a pipeline. Then, during the multipli-
cation, the previous product is added in. It seems to be reasonable, then, to wait with
the carry resolution until the addition of the previous product is actually ﬁnished.
8.5
Scalar Product Computation Units (SPUs)
Having discussed the two principal solutions for exact scalar product computation as
well as a very fast carry handling scheme, we now turn to a more detailed design of
scalar product computation units for various processors. These units will be called
SPUs, which stands for Scalar Product Units. If not otherwise mentioned we assume
throughout this section that the data are stored in the double precision format of the
IEEE-arithmetic standard 754. There the ﬂoating-point word has 64 bits and the man-
tissa 53 bits. A central building block for the SPU is the complete register, the CR.
It is a ﬁxed-point register wherein any sum of ﬂoating-point numbers and of simple
products of ﬂoating-point numbers can be represented without error. The SPU allows
the computation of scalar products of two vectors with any ﬁnite number of ﬂoating-
point components exactly or with a single rounding at the very end of the computation.
As shown in Remark 8.3(c) of Section 8.4.3, the CR consists of 4288 bits. It can be
represented by 67 words of 64 bits.
The scalar product is frequently used in scientiﬁc computation so its execution
needs to be fast. All circuits to be discussed in this section perform the scalar product
in a pipeline which simultaneously executes the following steps:
(a) read the two factors ai and bi to perform a product,
(b) compute the product ai · bi to the full double length, and
(c) add the product ai · bi to the CR.
Step (a) turns out to be the bottleneck of this pipeline. Therefore, we shall develop
different circuits for computers which are able to read the two factors ai and bi into
the SPU in four or two or one portion. The latter case will be discussed in Section 8.8.
Step (b) produces a product of 106 bits. It maps onto at most three consecutive words
of the CR. The address of these words is determined by the product’s exponent. In
step (c) the 106 bit product is added to the three consecutive words of the CR.
8.5.1
SPU for Computers with a 32 Bit Data Bus
Here we consider a computer which is able to read the data into the arithmetic logical
unit and/or the SPU in portions of 32 bits. An older personal computer (until ca. 2003)
is typical of this kind of computer.

264
8 Scalar Products and Complete Arithmetic
The ﬁrst solution with an adder and a shifter for the full CR of 4288 bits would
be expensive. So the SPU for these computers is built upon the second solution (see
Figure 8.5).
CR
CR1CR
Figure 8.5. Accumulation of a product to the CR by a 64 bit adder.
For the computation of the product ai · bi the two factors ai and bi are to be read.
Both consist of 64 bits. Since the data can only be read in 32 bit portions, the unit has
to read four times. We assume that with the necessary decoding this can be done in
eight cycles. See Figure 8.6.
This is rather slow and turns out to be the bottleneck for the whole pipeline. In a
balanced SPU the multiplier should be able to produce a product and the adder should
be able to accumulate the product in about the same time the unit needs to read the
data. Therefore, it sufﬁces to provide a 27 × 27 bit multiplier. It computes the 106 bit
product of the two 53 bit mantissas of ai and bi by 4 partial products. The subsequent
addition of the product into the three consecutive words of the CR is performed by an
adder of 64 bits. The appropriate three words of the CR are loaded into the adder one

8.5 Scalar Product Computation Units (SPUs)
265
cycle
read
mult/shift
accumulate
read a1
i−1
read a2
i−1
read b1
i−1
read b2
i−1
read a1
i
read a2
i
ci−1 := ai−1 · bi−1
read b1
i
ci−1 := shift (ci−1)
read b2
i
load1
read a1
i+1
add/sub
load2
store1
add/sub
load3
read a2
i+1
ci := ai · bi
store2
add/sub
load carry
store3
inc/dec
read b1
i+1
ci := shift (ci)
store carry
store ﬂags
read b2
i+1
load1
read a1
i+2
add/sub
load2
store1
add/sub
load3
read a2
i+2
ci+1 := ai+1 · bi+1
store2
add/sub
load carry
store3
inc/dec
read b1
i+2
ci+1 := shift (ci+1)
store carry
store ﬂags
read b2
i+2
load1
read a1
i+3
add/sub
load2
store1
add/sub
load3
read a2
i+3
ci+2 := ai+2 · bi+2
store2
add/sub
load carry
store3
inc/dec
read b1
i+3
ci+2 := shift (ci+2)
store carry
store ﬂags
read b2
i+3
Figure 8.6. Pipeline for the accumulation of scalar products on computers
with 32 bit data bus.

266
8 Scalar Products and Complete Arithmetic
after the other and the appropriate portion of the product is added. The sum is written
back into the same word of the CR that the portion has been read from. A 64 out of
106 bit shifter must be used to align the product onto the relevant word boundaries.
See Figure 8.5. The addition of the three portions of the product into the CR may
cause a carry. The carry is absorbed by incrementing (or decrementing in the case
of a borrow) a more signiﬁcant word of the CR as determined by the carry handling
scheme.
The pipeline is sketched in Figure 8.6. There, we assume that a dual port RAM
is available on the SPU to store the CR. This is usual for register memory. It allows
reading from the CR and writing into it simultaneously. Eight machine cycles are
needed to read the two 64 bit factors ai and bi for a product, including the necessary
address decoding. This is also about the time in which the multiplication and the shift
can be performed in the second step of the pipeline. The three successive additions
and the carry resolution in the third step of the pipeline again can be done in about the
same time. See Figure 8.6. Figure 8.7 shows a block diagram for an SPU with 32 bit
data bus.
The sum of the exponents of ai and bi delivers the exponent of the product ai · bi.
It consists of twelve bits. The six low order (less signiﬁcant) bits of this sum are used
to perform the shift. The more signiﬁcant bits of the sum deliver the CR address to
which the product ai · bi has to be added. So the originally very long shift is split into
a short shift and an addressing operation. The shifter performs a relatively short shift
operation. The addressing selects the three words of the CR for the addition of the
product.
The CR RAM needs only one address decoder to ﬁnd the starting address for an
addition. The two more signiﬁcant parts of the product are added to the contents of the
two CR words with the next two addresses. The carry logic determines the word that
absorbs the carry. All these address decodings can be hard wired. The result of each
of the four additions is written back into the same CR words to which the addition has
been executed. The two carry ﬂags appended to each accumulator word are indicated
in Figure 8.7. In practice the ﬂags are kept in separate registers.
We stress the fact that in the circuit just discussed virtually no unoverlapped com-
puting time is needed for the arithmetic. In the pipeline the arithmetic is performed in
the time that is needed to read the data into the SPU. Here, we assumed this requires
eight cycles, allowing both the multiplication and the accumulation to be performed
very economically and sequentially by a 27 × 27 bit multiplier and a 64 bit adder.
Both the multiplication and the addition are themselves performed in a pipeline. The
arithmetic overlaps with the loading of the data into the SPU.
8.5.2
A Coprocessor Chip for the Exact Scalar Product
A vector arithmetic coprocessor chip XPA 3233 for the PC was developed in a CMOS
0.8 μm VLSI gate array technology at the author’s Institute in 1993/94 in collaboration

8.5 Scalar Product Computation Units (SPUs)
267
53 x 53 bit
multiplication
by 27 x 27 bit
multiplier
53
11
53
32
64
12
6
6
11
64
64
64
shifter
106
mant (ai)
exp (ai)
mant (bi)
exp (bi)
adder
dual port RAM
64 x 67
complete register
& flag registers
adder
address
decoder
flag
control
exception
interface
data bus
Figure 8.7. Block diagram for an SPU with 32 bit data supply and sequen-
tial addition into the CR.

268
8 Scalar Products and Complete Arithmetic
with the Institute for Microelectronics at the Universit¨at Stuttgart and the Institute for
Informatics of the Technische Universit¨at Hamburg-Harburg. VHDL and COMPASS
design tools were used. For design details see [232, 281, 373] and in particular [61].
The chip is connected with the PC via the PCI-bus. The PCI- and EMC-interfaces
are integrated on the chip. With the coprocessor the PC became a vector computer.
In its time the chip computed the exact scalar product between two and four times
faster than the PC could compute an approximation in ﬂoating-point arithmetic. With
increasing clock rate of the PC the PCI-bus turned out to be a severe bottleneck.
To keep up with the increased speed the SPU must be integrated into the arithmetic
logical unit of the processor and interconnected by an internal bus system.
The chip, see Figure 8.8, realizes the SPU that has been discussed in Section 8.5.1
using 207,000 transistors. About 30% of the transistors and the silicon area are used
for the CR and the ﬂag registers with the carry resolution logic. The remaining 70%
of the silicon area is needed for the PCI/EMC interface and the chip’s own multiplier,
shifter, adder and rounding unit. All these units would be superﬂuous if the SPU
were integrated into the arithmetic unit of the processor. A multiplier, shifter, adder
and rounding unit are already there. Everything just needs to be arranged a little
differently. Ultimately the SPU requires fewer transistors and less silicon area than
is needed for the exception handling of the IEEE-arithmetic standard. The SPU is
logically much more regular and simple. With it a large number of exceptions that
can occur in a conventional ﬂoating-point computation are avoided. The silicon area
would shrink considerably if full custom design were used.
Testing of the coprocessor XPA 3233 was easy. XSC-languages had been available
and used since 1980. An identical software simulation of the exact scalar product
had also been implemented. Additionally a large number of problem solving routines
had been developed and collected in the toolbox volumes [205, 207, 289, 290, 332].
All that had to be done was to change the PASCAL-XSC compiler a little to call the
hardware chip instead of its software simulation. Surprisingly, 40% of the chips on the
ﬁrst wafer were correct and, probably due to the high standard of the implementors
and their familiarity with the theoretical background, with PASCAL-XSC and the
toolbox routines no redesign was necessary. The chips produced results identical to
those of the software simulation.
Modern computer technology can provide millions of transistors on a single chip.
This allows capabilities to be put into the computer hardware which even an experi-
enced computer user is totally unaware of. Through ignorance of the technology and
the design tools and implementation techniques, obvious and easy capabilities are not
demanded by mathematicians. The engineer on the other hand, who is familiar with
these techniques, is not aware of the consequences for mathematics [232].
There are processors available where the data supply to the arithmetic unit or the
SPU is much faster. We discuss the design of a SPU for such processors in the next
section and in Section 8.8.

8.5 Scalar Product Computation Units (SPUs)
269
Register
File
Interface
Status
Reg.
Data path
control
Flag registers
carry resolution logic
Accumulator Memory
Add/Subtract
Multiplier
Wallace tree
Booth logic
Rounding
Shifter
PCI/EMC
decode
Instruct.
Figure 8.8. Functional units, chip and board of the vector arithmetic co-
processor XPA 3233.

270
8 Scalar Products and Complete Arithmetic
8.5.3
SPU for Computers with a 64 Bit Data Bus
Now we consider a computer which is able to read data into the arithmetic logical unit
and/or the SPU in portions of 64 bits. Fast workstations or mainframes are typical of
this kind of computer.
Now the time to perform the multiplication and the accumulation overlapped in
pipelines as before is no longer available. To keep the execution time for the arithmetic
within the time the SPU needs to read its data, we have to invest in more circuitry. For
multiplication a 53 × 53 bit multiplier must now be used. The result is still 106 bits
wide and so will go into two or three 64 bit words of the CR. But the addition of the
product and the carry resolution now have to be performed in parallel.
If the 106 bit summand spans three consecutive 64 bit words of the CR, a closer
look shows that the 22 least signiﬁcant bits of those three words are never changed by
addition of the summand. Thus the adder needs to be 170 bits wide only. Figure 8.9
shows a sketch for the parallel accumulation of a product.
In the circuit a 106 to 170 bit shifter is used. The four additions are to be performed
in parallel. So four read/write ports are to be provided for the CR RAM. Sophisticated
logic must be used for the generation of the carry resolution address, since this address
must be generated very quickly. Again the CR RAM needs only one address decoder
to ﬁnd the starting address for an addition. The more signiﬁcant parts of the product
are added to the contents of the two CR words with the next two addresses. A tree
structured carry logic now determines the CR word which absorbs the carry. A very
fast hardwired multi-port driver can be designed which allows all 4 CR words to be
read into the adder in one cycle.
Figure 8.10 shows the pipeline for this kind of addition. In the ﬁgure we assume
that two machine cycles are needed to decode and read one 64 bit word into the SPU.
Figure 8.11 shows a block diagram for an SPU with a 64 bit data bus and parallel
addition.
We emphasize that again virtually no unoverlapped computing time is needed for
the execution of the arithmetic. In a pipeline the arithmetic is performed in the time
which is needed to read the data into the SPU. Here, we assume that with the necessary
address decoding, this requires four cycles for the two 64 bit factors ai and bi for a
product. To match the shorter time required to read the data, more circuitry has to be
used in the multiplier and the adder.
If the technology is fast enough it may be reasonable to provide a 256 bit adder
instead of the 170 bit adder. An adder width of a power of 2 may simplify shifting as
well as address decoding. The lower bits of the exponent of the product control the
shift operation while the higher bits are directly used as the starting address for the
accumulation of the product into the CR.
The two ﬂag registers appended to each CR word are indicated in Figure 8.11 again.
In practice the ﬂags are kept in separate registers.

8.5 Scalar Product Computation Units (SPUs)
271
CR
Figure 8.9. Parallel accumulation of a product into the CR.

272
8 Scalar Products and Complete Arithmetic
cycle
read
mult/shift
accumulate
read ai−1
read bi−1
read ai
ci−1 := ai−1 · bi−1
read bi
ci−1 := shift (ci−1)
address decoding
read ai+1
ci := ai · bi
load
add/sub ci−1
read bi+1
ci := shift (ci)
store & store ﬂags
address decoding
read ai+2
ci+1 := ai+1 · bi+1
load
add/sub ci
read bi+2
ci+1 := shift (ci+1)
store & store ﬂags
address decoding
read ai+3
ci+2 := ai+2 · bi+2
load
add/sub ci+1
read bi+3
ci+2 := shift (ci+2)
store & store ﬂags
Figure 8.10. Pipeline for the accumulation of scalar products.
8.6
Comments
8.6.1
Rounding
If the result of an exact scalar product is needed later in a program, the contents of the
CR must be put into user memory. How this can be done will be discussed later in
this section.
If not processed any further the exact result of a scalar product computation usually
has to be rounded into a ﬂoating-point number or a ﬂoating-point interval. The ﬂag
bits that are used for the fast carry resolution can be used for the rounding of the CR
contents also. By looking at the ﬂag bits, the leading result word in the CR can easily
be identiﬁed. This and the next CR word are needed to compose the mantissa of the
result. This 128 bit quantity must then be shifted to form a normalized mantissa of
an IEEE-arithmetic double precision number. The shift length can be extracted by
looking at the leading result word in the CR with the same procedure which identiﬁed
it by looking at the ﬂag bit word.
For the correct rounding downwards (or upwards) it is necessary to check whether
any of the discarded bits is a one. This is done by testing the remaining bits of the
128 bit quantity in the shifter and by looking at the all bits 0 ﬂags of the following CR
words. This information is then used to control the rounding.
Rounding is only needed at the very end of a scalar product computation. If a large
number of products has been accumulated the contribution of the rounding to the

8.6 Comments
273
53 x 53 bit
multiplier
53
11
53
64
64
12
11
170
64
64
42
64
shifter
106
mant (ai)
exp (ai)
mant (bi)
exp (bi)
adder & carry-inc.
adder
address
decoder
flag
control
exception
interface
data bus
four port RAM
64 x 67
complete register
&
flag registers
carry
resolve
address
start address
Figure 8.11. Block diagram for an SPU with 64 bit data bus and parallel
addition into the CR.

274
8 Scalar Products and Complete Arithmetic
computing time is negligible. However, if a short scalar product or a single ﬂoating-
point addition or subtraction has to be carried out by the SPU, a very fast rounding
procedure is essential for the speed of the overall application.
The rounding speed depends heavily on the speed with which the leading one digit
of the CR can be detected. A pointer to this digit, carried along with the computation,
would immediately identify this digit. The pointer logic requires additional hardware
and its beneﬁt decreases if lengthy scalar products are to be computed.
For short scalar products or single ﬂoating-point operations leading zero anticipa-
tion (LZA) would be more useful. The ﬁnal result of a scalar product computation
is supposed to lie in the exponent range between e1 and e2 of the CR. Otherwise the
problem has to be scaled. So hardware support for LZA is only needed for this part of
the CR. A comparison of the exponents of the summands identiﬁes the CR word for
which the LZA should be activated. LZA involves fast computation of a provisional
sum which differs from the correct sum by at most one leading zero. With this infor-
mation the leading zeros and the shift width for the two CR words in question can be
detected easily and quickly [577].
8.6.2
How Much Local Memory Should be Provided on an SPU?
There are applications which make it desirable to provide more than one CR on the
SPU. If, for instance, the components of the two vectors a = (ai) and b = (bi) are
complex ﬂoating-point numbers, the scalar product a · b is also a complex ﬂoating-
point number. It is obtained by accumulating the real and imaginary parts of the
product of two complex ﬂoating-point numbers. The formula for the product of two
complex ﬂoating-point numbers
-
x = x1 + ix2, y = y1 + iy2
⇒x · y = (x1 · y1 −x2 · y2) + i (x1 · y2 + x2 · y1)
.
shows that the real and imaginary parts of ai and bi are both needed for the computa-
tion of both the real part of the product ai · bi as well as the imaginary part.
Access to user memory is usually slower than access to register memory. To obtain
high computing speed it is desirable, therefore, to bring the real and imaginary parts
of the vector components in only once and to compute the real and imaginary parts of
the products simultaneously in two CRs on the SPU instead of reading the data twice
and performing the two accumulations sequentially. Interestingly, the old calculators
shown in Figures 8.1(c) and (d) on page 249 had two long registers.
Very similar considerations show that a high speed computation of the scalar prod-
uct of two vectors with interval components makes two CRs desirable as well.
We brieﬂy sketch here a scalar product unit for two vectors with interval compo-
nents. If A = (Aν) and B = (Bν) with Aν = [aν1, aν2] and Bν = [bν1, bν2] ∈IS are

8.7 The Data Format Complete and Complete Arithmetic
275
two such vectors this requires evaluation of the formulas
A♦· B = ([aν1, aν2])♦· ([bν1, bν2])
(8.6.1)
=
%
▽
n

ν=1
min
i,j=1,2(aνibνj), △
n

ν=1
max
i,j=1,2(aνibνj)
&
.
(8.6.2)
Here the products of the bounds of the vector components aνibνj are to be computed
to the full double length. Then the minima and maxima have to be selected. These
selections can be done by distinguishing the nine cases shown in Table 4.1. This can
be hardware supported as shown in Figure 7.2 or 7.5. The selected products are then
accumulated in R as an exact scalar product. Clearly, forwarding the products then
requires wider busses. The two sums are computed in two complete registers by one
of the techniques shown in this chapter. Finally the sum of products is rounded only
once by ▽(resp. △) from R into S. See Figure 8.12. After multiplication in the ﬁgure
the path through minimum and maximum selection respectively is only activated, if
both intervals [aν1, aν2] and [bν1, bν2] contain zero.
The unit shown in Figure 8.12 can also be used to compute the scalar product of
two vectors the components of which are complex ﬂoating-point numbers.
For vectors with complex interval components even four CRs would be useful.
There might be other reasons to provide local memory space for more than one
CR on the SPU. A program with higher priority may interrupt the computation of a
scalar product and require a CR. The easiest way to solve this problem is to open
a new CR for the program with higher priority. Of course, this can happen several
times which raises the question how much local memory for how many CRs should
be provided on an SPU. Three might be a good number to solve this problem. If a
further interrupt requires another CR, the CR with the lowest priority could be mapped
into the main memory by some kind of stack mechanism. This technique would not
limit the number of interrupts that may occur during a scalar product computation.
These problems and questions must be addressed as part of the operating system’s
responsibility.
For a time sharing environment memory space for more than one CR on the SPU
may also be useful.
However the contents of the last two paragraphs are of a somewhat hypothetical
nature. The author is of the opinion that the scalar product is a fundamental and basic
operation which should not be, and never needs to be, interrupted.
8.7
The Data Format Complete and Complete Arithmetic
We have seen in the previous sections that scalar or dot products can be computed
exactly and without any exceptions in a register of L = k + 2e2 + 2l + 2|e1| digits of
base b. In numerical analysis the scalar product is ubiquitous. Most of the advanced

276
8 Scalar Products and Complete Arithmetic
i1
a
i1
b
a 2
i
b 2
i
i1
a
i1
b
a 2
i
b 2
i
lower bound
upper bound
operand selection
min select
multiplier
multiplier
max select
shifter
adder
register
shifter
adder
register
register file (or memory access unit)
for intervals (pairs of reals)
complete
complete
Figure 8.12. SPU for vectors with interval components.

8.7 The Data Format Complete and Complete Arithmetic
277
applications discussed in Chapter 9 of this book are based on applications of the exact
scalar product. With it advanced computer arithmetic surpasses elementary ﬂoating-
point arithmetic as well as basic computer arithmetic. The basic components for the
computation of the exact scalar product, therefore, should be given into the hands of
the users.
We do this by a new data format which is called complete and a limited set of arith-
metic operations deﬁned for it. Complete arithmetic is performed in what is called a
complete register, CR for short. Complete arithmetic sufﬁces to compute all scalar
products of ﬂoating-point vectors exactly. The result of complete arithmetic is always
exact, it is complete, and no intermediate roundings or truncations are performed. Not
a single bit is lost.
A datum of the type complete is a ﬁxed-point word of the size of the complete
register. It consists of L = k + 2e2 + 2l + 2|e1| digits of base b. See Figure 8.13.
It covers the full range of the ﬂoating-point system R = R(b, 2l, 2e2, 2e1) and a few
additional digits. It sufﬁces to allow exact accumulation (continued summation) of
products of ﬂoating-point numbers. In the complete register the b-ary point is sitting
immediately to the right of the (k + 2e2)th digit of base b.
k
2e2
2l
2|e1|
Figure 8.13. Complete Register CR.
Complete arithmetic can be provided for every speciﬁc ﬂoating-point format.
The following two sections give programming instructions for complete arithmetic
for low and high level programming languages respectively.
8.7.1
Low Level Instructions for Complete Arithmetic
For complete arithmetic the following ten low level instructions are recommended.
They are the most natural and are necessary for application of complete arithmetic.
These low level capabilities support the high level instructions developed in the next
section, and are based on experience with these in the XSC-languages since 1980.
Very similar instructions were provided by the processors developed in [585], and
[61]. Practically identical instructions were used in [650] to support ACRITH and
ACRITH-XSC [649, 651, 653]. These IBM program products were developed at the
author’s institute in collaboration with IBM.
The ten low level instructions for complete arithmetic are:
1. clear the CR,
2. add a product to the CR,
3. add a ﬂoating-point number to the CR,

278
8 Scalar Products and Complete Arithmetic
4. subtract a product from the CR,
5. subtract a ﬂoating-point number from the CR,
6. read the CR and round its contents to the destination format,
7. store the contents of the CR in memory,
8. load the CR from memory,
9. add stored CR to CR,
10. subtract stored CR from CR.
The clear instruction can be performed by setting all all bits 0 ﬂags to 0. Also
the all bits 1 ﬂags should be set to 0. The load and store instructions are performed
by using the load/store instructions of the processor. For the add, subtract and round
instructions the following denotations are used here. The preﬁx sp identiﬁes SPU
instructions. ln denotes the ﬂoating-point format that is used and will be db for IEEE
double. In all these instructions, the CR is an implicit source and destination operand.
The numbering of the instructions above is repeated in the following.
2. spadd ln src1, src2
multiply the numbers in the given registers and add the product to the CR.
3. spadd ln src
add the number in the given register to the CR.
4. spsub ln src1, src2
multiply the numbers in the given registers and subtract the product from the CR.
5. spsub ln src
subtract the number in the given register from the CR.
6. spstore ln.rd dest
get CR contents and put the rounded value into the destination register.
Here rd controls the rounding mode that is used when the CR contents are stored
in a ﬂoating-point register. It is one of the following:
rn
rounding to nearest,
rz
rounding toward zero,
rp
rounding upwards, i.e., toward positive ∞,
rn
rounding downwards, i.e., toward negative ∞.
7. spstore dest
get CR contents and put them into the destination memory operand.
8. spload src
load the complete number from the given memory operand into the CR.

8.7 The Data Format Complete and Complete Arithmetic
279
9. spadd src
the complete number at the location src is added to the contents of the CR in the
processor.
10. spsub src
the complete number at the location src is subtracted from the contents of the CR
in the processor.
If multiple CRs are provided on the SPU additional instructions for exchanging CR
contents may be useful.
8.7.2
Complete Arithmetic in High Level Programming Languages
Advances in computer technology allow the quality and high accuracy of the basic
ﬂoating-point operations of addition, subtraction, multiplication and division to be
extended to the arithmetic operations in the linear spaces and their interval extensions
which are most commonly used in computation. The scalar product serves to provide
these operations. It can be produced by an instruction multiply and accumulate. The
products are accumulated in a complete register CR which has enough digit positions
to contain the exact sum in its entirety. Only a single rounding error of at most one unit
in the last place is introduced when the completed scalar product (often also called dot
product) is returned to one of the ﬂoating-point registers.
By operator overloading in modern programming languages matrix and vector op-
erations can be provided with highest accuracy and in a simple notation, if the exact
scalar product is available. However, many scalar products that occur in a computation
do not appear as vector or matrix operations in the program. A vectorizing compiler
is certainly a good tool for detecting such masked scalar products in a program. Since
the hardware-supported exact scalar product is very fast this would increase both the
accuracy and the speed of the computation.
In the computer, the scalar product is produced by the elementary computer instruc-
tions shown in the last section. Programming and the detection of scalar products in a
program can be simpliﬁed a great deal if some of these computer instructions are put
into the hands of the user and incorporated into high level programming languages.
This has been done with great success since 1980 in the so-called XSC-languages
(eXtended Scientiﬁc Computation) [79, 257, 288, 289, 290, 291, 333, 355, 356, 647,
648, 649, 653] that have been developed at the author’s institute. All these languages
provide an exact scalar product implemented in software based on integer arithmetic.
If a computer is equipped with the hardware unit XPA 3233 (see Section 8.5.2) the
hardware unit is called instead. A large number of problem solving routines with au-
tomatic result validation has been implemented in the XSC-languages for practically
all standard problems of numerical analysis [206, 207, 332, 368, 651, 653]. These
routines have been very successfully applied in the sciences.

280
8 Scalar Products and Complete Arithmetic
We mention a few of these constructs and demonstrate their usefulness. Central to
this is the idea of allowing variables of the size of the CR to be deﬁned in a user’s pro-
gram. For this purpose a new data type called complete is introduced. A variable of
the type complete is a ﬁxed-point variable with L = k + 2e2 + 2l + 2|e1| digits of
base b. See Figure 8.13. As has been shown earlier, every ﬁnite sum of ﬂoating-point
products n
i=1 ai · bi can be represented as a variable of type complete. Moreover,
every such sum can be computed in a complete register (CR) without loss of informa-
tion. Along with the type complete the following constructs serve as primitives for
developing expressions in a program which can easily be evaluated with the low level
instructions introduced in the last section:
complete
new data type
:=
assignment from complete
to complete or
to real with rounding to nearest or
to interval with roundings downwards and upwards
depending on the type on the left hand side of the
:= operator.
For variables of type complete so-called complete expressions are permitted.
As every expression a complete expression is composed of operands and operations.
Only three kinds of operands are permitted:
(a) a constant or variable of type real,
(b) an exact product of two such objects, and
(c) another datum of type complete.
REAL
REAL
COMPLETE
REAL
Figure 8.14. Syntax diagram for COMPLETE EXPRESSION.
Such operands can be added or subtracted in an arbitrary order. All operations
(multiplication, addition, and subtraction) are to be exact. The result is a datum of
type complete. It is always exact. No truncation or rounding is performed during
execution of a complete expression. The result of complete arithmetic reﬂects the
complete information as given by the input data and the operations in the expression.

8.7 The Data Format Complete and Complete Arithmetic
281
Figure 8.14 shows a syntax diagram for COMPLETE EXPRESSION. In the diagram
solid lines are to be traversed from left to right and from top to bottom. Dotted lines
are to be traversed oppositely.
Compete arithmetic is a necessary complement to ﬂoating-point arithmetic. Al-
though it looks very simple it is a very powerful tool. A central task of numerical
analysis is the solution of systems of linear equations. The so-called veriﬁcation step
for a system of linear equations is solely performed by complete arithmetic. It com-
putes close bounds for the solution. Complete arithmetic also is a fundamental tool
for many other applications discussed in Chapter 9.
We now illustrate its use by a few simple algorithms. For instance, let x be a vari-
able of type complete and y and z variables of type real. Then in the assignment
x := x + y * z
the double length product of y and z is added to the variable x of type complete
and its new value is assigned to x.
The scalar product of two vectors a=(a[i]) and b=(b[i]) is now easily im-
plemented with a variable x of type complete as follows:
x := 0;
for i := 1 to n do x := x + a[i] * b[i];
y := x;
The last statement y := x rounds the value of the variable x of type complete
into the variable y of type real by applying the standard rounding of the computer.
Variable y then has the value of the scalar product a · b which is within a single
rounding error of the exact scalar product a·b.
For example, the method of defect correction or iterative reﬁnement requires highly
accurate computation of expressions of the form
a * b −c * d
with vectors a, b, c, and d. Employing a variable x of type complete, this expres-
sion can now be programmed as follows:
x := 0;
for i := 1 to n do x := x + a[i] * b[i];
for i := 1 to n do x := x −c[i] * d[i];
y := x;
or by
x := 0;
for i := 1 to n do x := x + a[i] * b[i] −c[i] * d[i];
y := x;
The result y, involving 2n exact multiplications and 2n −1 exact additions, is
produced with a single rounding operation.

282
8 Scalar Products and Complete Arithmetic
In the last two examples y could have been deﬁned to be of type interval. Then
the last statement y := x would produce an interval with a lower bound which is
obtained by rounding the complete value of x downwards and an upper bound by
rounding it upwards. Thus, the bounds of y will be either the same or two adjacent
ﬂoating-point numbers.
In the XSC-languages the functionality of the complete type and expression is
available also for complex data as well as for interval and complex interval data. Cor-
responding types would be called ccomplete, icomplete, and cicomplete
respectively. High speed dot products for these types would require two complete
registers in the ﬁrst two cases and four in the latter.
Complete arithmetic and expressions can also be deﬁned for higher dimensional
data types like vectors and matrices of the four basic types mentioned in the last para-
graph. In the corresponding syntax diagrams the product would be the vector matrix
product and the matrix product respectively. On scalar processors these expressions
would be executed componentwise using the basic complete arithmetic. Since the ex-
ecution for the different components is independent of each other, parallel execution
of several components is possible if the processor allows this.
The data type complete is the appropriate generalization of the long result regis-
ters that have been used in the old calculators shown in the Figures 8.1(c) and (d).
8.8
Top Speed Scalar Product Units
A top-performance computer is able to read two data x and y to produce the product
x · y into the arithmetic logical unit and/or the SPU simultaneously in one cycle.
Supercomputers and vector processors are typical of this kind of computer. Usually
the ﬂoating-point word has 64 bits and the data bus is 128 or even more bits wide.
However, digital signal processors with a word size of 32 bits can also belong in this
class if two 32 bit words are read into the ALU and/or SPU in one cycle. For such
computers both the solutions sketched in Sections 8.4.1 and 8.4.2 make sense and
will be considered in the following. The higher the speed of the system the more
circuitry has to be used for the accumulation of the products. The more complex and
expensive solution seems to be best suited to reveal the basic ideas. So we begin with
the solution which uses a long ﬁxed-point adder and a 64-bit data format.
8.8.1
SPU with Long Adder for 64 Bit Data Word
In [365] the basic ideas for a general data format have been developed. However, to
be very speciﬁc we discuss here a circuit for the double precision format of the IEEE-
arithmetic standard 754. The word size is 64 bits. The mantissa has 53 bits and the
exponent 11 bits. The exponent covers a range from −1022 to +1023. The CR has

8.8 Top Speed Scalar Product Units
283
4288 bits. We assume again that the scalar product computation can be divided into
the independent steps:
(a) read ai and bi,
(b) compute the product ai · bi,
(c) add the product to the CR.
Now by assumption the SPU can read the two factors ai and bi entirely and simul-
taneously. We call the time that is needed for this a cycle. Then, in a balanced design,
steps b) and c) should take about the same time. Using well-known fast multiplication
techniques like Booth-Recoding and Wallace-tree this certainly is possible for step b).
Here, the two 53 bit mantissas are multiplied. The product has 106 bits. The main
difﬁculty seems to appear in step c). There, we have to add a summand of 106 bits to
the CR in every cycle.
Following the idea that has been discussed in Section 8.4.1 the addition is per-
formed by a long adder and a long shift, both of L = 4288 bits. An adder and a shift
of this size are necessarily slow, certainly too slow to process one summand of 106
bits in a single cycle. Therefore, measures have to be taken to speed up the addition
as well as the shift. As a ﬁrst step we subdivide the long adder into shorter segments.
The width of these segments is chosen in such a way that a parallel addition can be
performed in a single cycle. We assume here that the segments consist of 64 bits.3 A
64 bit adder certainly is faster than a 4288 bit adder. Now each of the 64 bit adders
may produce a carry. We write these carries into carry registers between adjacent
adders. See Figure 8.15.
Figure 8.15. Parallel and segmented parallel adder.
3Other segment sizes are possible; see [284, 285].

284
8 Scalar Products and Complete Arithmetic
If a single addition has to be performed these carries have to be propagated straight
away. In a scalar product computation, however, this is not necessary. We assume
many summands have to be added up. In the next machine cycle the carries are added
to the next more signiﬁcant adder, possibly together with another summand. Only at
the very end of the accumulation, when no more summands are coming, carries may
have to be eliminated. However, every summand is relatively short. It consists of 106
bits only. So during the addition of a summand, carries are only produced in a small
part of the 4288 bit adder. The carry elimination, on the other hand, takes place during
each step of the addition wherever a carry is left. So in an average case there will only
be very few carries left at the end of the accumulation and a few additional cycles will
sufﬁce to absorb the remaining carries. Thus, segmenting the adder enables it to keep
up with steps a) and b) and to read and process a summand in each cycle.
The long shift of the 106 bit summand is slow also. It is speeded up by a matrix
shaped arrangement of the adders. Only a few, let us assume here four, of the partial
adders are placed in a row. We begin with the four least signiﬁcant adders. The four
next more signiﬁcant adders are placed directly beneath of them and so on. The most
signiﬁcant adders form the last row. The rows are connected as shown in Figure 8.16.
In our example, where we have 67 adders of 64 bits, 17 rows sufﬁce to arrange the
entire summing matrix. Now the long shift is performed as follows: The summand of
106 bits carries an exponent. In a fast shifter of 106 to 256 bits the summand is shifted
into a position where its most signiﬁcant digit is placed directly above the position in
the long adder which carries the same exponent identiﬁcation E. The remaining digits
of the summand are placed immediately to its right. Now the summing matrix reads
this summand into the S-registers (summand registers) of every row. The addition is
done in the row where the exponent identiﬁcation coincides with that of the summand.
It may happen that the most signiﬁcant digit of the summand has to be shifted so
far to the right that the remaining digits would hang over at the right end of the shifter.
These digits then are reinserted at the left end of the shifter by a ring shift. If now the
more signiﬁcant part of the summand is added in row r, its less signiﬁcant part will
be added in row r −1.
By this matrix shaped arrangement of the adders, the unit can perform both a shift
and an addition in a single cycle. The long shift is reduced to a short shift of 106 to 256
bits, which is fast. The remaining shift happens automatically by the row selection for
the addition in the summing matrix.
Every summand carries an exponent which in our example consists of 12 bits. The
lower part of the exponent, i.e., the 8 least signiﬁcant digits, determine the shift width
and with it the selection of the columns in the summing matrix. The row selection is
speciﬁed by the 4 most signiﬁcant bits of the exponent. This corresponds roughly to
the selection of the adding position in two steps by the process of Figure 8.3. The shift
width and the row selection for the addition of a product ai · bi to the CR are known
as soon as the exponent of the product has been computed. Since the exponents of ai

8.8 Top Speed Scalar Product Units
285
and bi consist of 11 bits only, the result of their addition is available very quickly. So
while the mantissas are being multiplied the shifter can be switched and the addresses
of the CR words for the accumulation of the product ai · bi can be selected.
The summing matrix just described must be able to process positive and negative
summands. In the latter case borrows may occur. They also have to be processed,
possibly over several cycles. The 106 bit summand maps onto at most three consec-
utive words of the CR. The summand is added by these three partial adders. Each
of these adders can produce a carry. The carry of the leftmost of these partial adders
can with high probability be absorbed, if the addition always is executed over four
adders and the fourth adder then is the next more signiﬁcant one. This can reduce the
number of carries that have to be resolved during future steps of the accumulation and
in particular at the end.
In each step of the accumulation an addition only has to be activated in the selected
row of adders and in those adders where a non zero carry is waiting to be absorbed.
This adder selection can reduce the power consumption for the accumulation step
signiﬁcantly.
The carry resolution method that has been discussed so far is quite natural. It is
simple and does not require special hardware support. If long scalar products are be-
ing computed it works very well. At the end of the accumulation, only if no more
summands are coming, a few additional cycles may be required to absorb the remain-
ing carries. Then a rounding can be done. However, the additional cycles for the carry
resolution at the end of the accumulation, although few in general, depend on the data
and are unpredictable. For short scalar products the time needed for these additional
cycles may be disproportionately high and indeed exceed the addition time.
With the fast carry resolution mechanism that has been discussed in Section 8.4.4
these difﬁculties can be overcome. At the cost of some additional hardware all car-
ries can be absorbed immediately at each step of the accumulation. The method is
shown in Figure 8.16 also. Two ﬂag registers for the all bits 0 and the all bits 1 ﬂags
are shown at the left end of each partial accumulator word in the ﬁgure. The 106 bit
products are added by three consecutive partial adders. Each of these adders can pro-
duce a carry. The carries between adjacent adders can be avoided, if all partial adders
are built as carry-select-adders. This increases the hardware costs only moderately.
The carry registers between adjacent adders are then no longer necessary.4 The ﬂags
indicate which one of the more signiﬁcant CR words will absorb the leftmost carry.
During the addition of a product only these 4 CR words are changed and only these
4 adders need to be activated. The addresses of these 4 words are available as soon
as the exponent of the summand ai · bi has been computed. During the addition step
the carry word can be incremented (decremented) while the product is being added.
If the addition produces a carry the incremented word will be written back into this
4This is the case in Figure 8.17 where a similar situation is discussed. There all adders are supposed
to be carry-select-adders.

286
8 Scalar Products and Complete Arithmetic
Figure 8.16. Block diagram of an SPU with long adder for a 64 bit data
word and 128 bit data bus.

8.8 Top Speed Scalar Product Units
287
CR. If the addition does not produce a carry, the CR word remains unchanged. Since
we have assumed that all partial adders are built as carry-select-adders this ﬁnal carry
resolution scheme requires no additional hardware. Simultaneously with the incre-
menting/decrementing of the carry word a second set of ﬂags is set up for the case
that a carry is generated. In this case the second set of ﬂags is copied into the former
ﬂag word.
The accumulators that belong to partial adders in Figure 8.16 are denoted by AC.
Beneath them a small memory is indicated in the ﬁgure. It can be used to save the CR
contents very quickly should a program with higher priority interrupt the computation
of a scalar product and require the unit for itself. However, the author is of the opinion
that the scalar product is a fundamental and basic arithmetic operation which should
never be interrupted.
In Section 8.6.2 we have discussed applications like complex arithmetic or interval
arithmetic which make it desirable to provide more than one CR on the SPU. The
local memory on the SPU shown in Figure 8.16 can be used for fast execution of
scalar products in complex and interval arithmetic.
In Figure 8.16 the registers for the summands carry an exponent identiﬁcation de-
noted by E. This is very useful for the ﬁnal rounding. The usefulness of the ﬂags for
the ﬁnal rounding has already been discussed. They also serve for fast clearing of the
accumulator.
The SPU which has been discussed in this section seems to be costly. However,
it consists of a large number of identical parts and it is very regular. This allows a
highly compact design. Furthermore the entire unit is simple. No particular exception
handling techniques are to be dealt with by the hardware. The result is exact. Vector
computers are the most expensive. A compact and simple solution, though expensive,
is justiﬁed for these systems.
8.8.2
SPU with Long Adder for 32 Bit Data Word
In this section we consider a computer which uses a 32 bit ﬂoating-point word and
which is able to read two such words into the ALU and/or SPU simultaneously in one
portion. Digital signal processors are like this. Real time computing requires very
high computing speed and high accuracy in the result. As in the last section we call
the time that is needed to read the two 32 bit ﬂoating-point words a cycle.
We ﬁrst develop circuitry for the SPU using a long adder and a long shift. To be very
speciﬁc we assume that the data are given as single precision ﬂoating-point numbers
conforming to the IEEE-arithmetic standard 754. There the mantissa consists of 24
bits and the exponent has 8 bits. The exponent covers a range from −126 to +127
(in binary). As discussed in Remark 8.3(a) of Section 8.4.3, 640 bits is a reasonable
choice for the CR. It can be represented by ten words of 64 bits.
Again the scalar product is computed by independent steps like
(a) read ai and bi,

288
8 Scalar Products and Complete Arithmetic
(b) compute the product ai · bi,
(c) add the product to the CR.
Each of the mantissas of ai and bi has 24 bits. Their product has 48 bits. It can be
computed very fast by a 24 × 24 bit multiplier using standard techniques like Booth-
Recoding and Wallace-tree. The addition of the two 8 bit exponents of ai and bi
delivers the exponent of the product consisting of 9 bits.
The CR consists of 10 words of 64 bits. The 48 bit mantissa of the product maps
onto at most two of these words. The product is added by the corresponding two
consecutive partial adders. Each of these two adders can produce a carry. The carry
between the two adjacent adders can immediately be absorbed if all partial adders
are built as carry-select-adders again. The carry of the more signiﬁcant of the two
adders will be absorbed by one of the more signiﬁcant 64 bit words of the CR. The
ﬂag mechanism (see Section 8.4.4) indicates which one of the CR words will absorb
a possible carry. So during an addition of a summand the contents of at most 3 CR
words are changed and only these three partial adders need to be activated. The ad-
dresses of these words are available as soon as the exponent of the summand ai · bi
has been computed. The carry word can be incremented (decremented) while the
product is being added in. If the addition produces a carry the incremented word will
be written back into the CR. If the addition does not produce a carry, this CR word
remains unchanged. Since all partial adders are built as carry select adders no ad-
ditional hardware is needed for the carry resolution. While the carry word is being
incremented/decremented, a second set of ﬂags is set up in case a carry is generated.
In this case the second set of ﬂags is copied into the former ﬂag word.
Details of the circuitry just discussed are summarized in Figure 8.17. The ﬁgure is
highly similar to Figure 8.16 of the previous section. To avoid the long shift, the long
adder is designed as a summing matrix consisting of two adders of 64 bits in each row.
For simplicity in the ﬁgure only three rows of the ﬁve needed to represent the full CR
are shown.
In a fast shifter of 48 to 128 bits the 48 bit product is shifted into a position where
its most signiﬁcant digit is placed directly above the position in the long adder which
carries the same exponent identiﬁcation E. The remaining digits of the summand are
placed immediately to its right. If they hang over at the right end of the shifter, they
are reinserted at the left end by a ring shift. Above the summing matrix in Figure 8.17
two possible positions of summands after the shift are indicated.
The summing matrix now reads the summand into its S-registers. The addition
is done by those adders where the exponent identiﬁcation coincides with that of the
summand. The exponent of the summand has nine bits. The lower part, i.e., the seven
least signiﬁcant bits, determines the shift width. The selection of the two adders which
perform the addition is determined by the two most signiﬁcant bits of the exponent.
In Figure 8.17 some local memory is indicated for each part of the CR. It can
be used to save the CR contents very quickly should a program with higher priority

8.8 Top Speed Scalar Product Units
289
Figure 8.17. Block diagram of an SPU with long adder for a 32 bit data
word and 64 bit data bus.

290
8 Scalar Products and Complete Arithmetic
interrupt the computation of a scalar product and require the unit for itself. The local
memory on the SPU also can be used for fast execution of scalar products in complex
and interval arithmetic.
Compared to Figure 8.16, Figure 8.17 shows an additional 32 bit data path directly
from the input register to the fast shifter. This data path is used to allow very fast
execution of the operation multiply and add fused, rnd(a·b+c), which is provided by
some conventional ﬂoating-point processors. While the product a·b is being computed
by the multiplier, the summand c is added to the CR.
The SPU which has been discussed in this section seems to be costly at ﬁrst glance.
While a single ﬂoating-point addition can be done conveniently with one 64 bit adder,
here 640 full adders (ten 64-bit adders) have been used in carry-select-adder mode.
However, the advantages of this design are tremendous. While a conventional ﬂoa-
ting-point addition can produce a completely wrong result with only two or three
additions, the new unit never delivers a wrong answer, even if millions of ﬂoating-
point numbers or single products of such numbers are added. An error analysis is
never necessary for these operations. The result is exact. The unit consists of a large
number of identical parts and it is very regular. This allows a very compact design.
No particular hardware has to be included to deal with rare exceptions. Although an
increase in adder equipment by a factor of 10, compared with a conventional ﬂoating-
point adder, might seem to be high, the number of full adders used for the circuitry is
not extraordinary. We stress the fact that for a Wallace tree in the case of a standard
53 × 53 bit multiplier about the same number of full adders is used. For fast conven-
tional computers this has been the state of the art multiplication for many years and
nobody complains about high cost.
8.8.3
A FPGA Coprocessor for the Exact Scalar Product
A hardware unit which realizes the SPU as discussed in Section 8.8.2 has been built
in Field Programmable Gate Array (FPGA) technology at the author’s institute in
2001/2002 [73]. The XILINX XCV800 processor was used. The unit has been de-
signed in such a way that it can also be used to compute the exact scalar product for
double precision ﬂoating-point numbers within the single exponent range, i.e., if no
under- and no overﬂow occurs.
The design of an SPU in FPGA technology, of course, is simpler than that of the
unit discussed in Section 8.5.2 where a gate array technology or full custom design
was used. No expensive hardware equipment and reproduction facilities are needed in
the FPGA technology. Everything can be done in a mathematics institute. However,
the board that ﬁnally carries the FPGA chip is more complicated than the one shown
in Figure 8.8. Several additional support chips are needed to reach the full function-
ality. The main disadvantage of this solution however, was the speed. Fixed-point
accumulation of the exact scalar product is supposed to be about four times as fast as
a computation of the scalar product in ﬂoating-point arithmetic. This theoretical gain

8.8 Top Speed Scalar Product Units
291
in speed could not keep up with the loss of speed caused by the much lower clock rate
of the FPGA chip in comparison with the clock rate of the most advanced processors.
The situation may change in the future if the clock rate of an FPGA processor comes
closer to that of its main processor. Nevertheless a coprocessor for the exact scalar
product can only be a temporary solution. Eventually the exact scalar product must
be incorporated into the ALU of the processor itself.
8.8.4
SPU with Short Adder and Complete Register
In the circuits discussed in Sections 8.8.1 and 8.8.2 adder circuitry was provided for
the full width of the CR. The long adder was segmented into partial adders of 64 bits.
In Section 8.8.1 67 and in Section 8.8.2 10 such units were used. During the addition
of a summand, however, in Section 8.8.1 only four, and in Section 8.8.2 only three, of
these units are activated. This raises the question of whether adder circuitry is really
needed for the full width of the CR and whether the accumulation can be done with
only three or four adders in accordance with the solution of Section 8.4.2. There the
CR is kept as local memory on the arithmetic unit.
In this section we develop such a solution for the 64-bit data format. A solution in
principle using a short adder and local memory on the arithmetic unit was discussed
in Section 8.5.3. There the data ai and bi to perform a product ai · bi are read into
the SPU successively in two portions of 64 bits. This leaves four machine cycles to
perform the accumulation in the pipeline.
Now we assume that the two data ai and bi for a product ai · bi are read into the
SPU simultaneously in one portion of 128 bits. Again we call the time that is needed
for this a cycle. In accordance with the solution shown in Figure 8.16 and Section
8.8.1 we assume that the multiplication and the shift can also be done in one such
read cycle. In a balanced pipeline, then, the circuit for the accumulation must be able
to read and process one summand in each (read) cycle also. The circuit in Figure 8.18
displays a solution. Closely following the summing matrix in Figure 8.16 we assume
there that the local memory is organized in 17 rows of four 64 bit words.
In each cycle the multiplier supplies a product (summand) to be added in the ac-
cumulation unit. Every such summand carries an exponent which in our example is
12 bits long. The eight lower (least signiﬁcant) bits of the exponent determine the
shift width. The row selection of the CR is given by the four most signiﬁcant bits of
the exponent. This roughly corresponds to the selection of the adding position in two
steps by the process described in the context of Figure 8.3. The shift width and the
row selection for the addition of the product to the local memory are known as soon
as the exponent of the product has been computed. Since the exponents of ai and bi
consist of 11 bits only, the result of their addition is available very quickly. So while
the mantissas are still being multiplied the shifter can be switched and the addresses
for the CR words for the accumulation of the product ai · bi can be selected.

292
8 Scalar Products and Complete Arithmetic
Figure 8.18. Block diagram of an SPU with short adder and local store for
a 64 bit data word and 128 bit data bus.

8.8 Top Speed Scalar Product Units
293
After being shifted the summand reaches the accumulation unit. It is read into the
input register IR of this unit. The shifted summand now consists of an exponent e, a
sign s, and a mantissa m. The mantissa maps onto three consecutive words of the CR,
while the exponent is reduced to the four most signiﬁcant bits of the original exponent
of the product.
Now the addition of the summand is performed in the accumulation unit by the
following three steps:
(i) The local memory is addressed by the exponent e. The contents of the addressed
part of the CR including the word which resolves the carry are transferred to the
register before summation (RBS). This transfer moves four words of 64 bits. The
summand is also transferred from IR to the corresponding section of RBS. In Figure
8.18 this part of the RBS is denoted by e′, s′ and m′ respectively.
(ii) In the next cycle the addition or subtraction is executed in the add/subtract unit
according to the sign. The result is transferred to the register after summation (RAS).
The adder/subtracter consists of four parallel adders of 64 bits which are working in
carry select mode. The summand maps onto three of these adders. Each of these three
adders can produce a carry. The carries between adjacent adders are absorbed by the
carry select addition. The fourth word is the carry word. It is selected by the ﬂag
mechanism. During the addition step a one is added to or subtracted from this word.
If the addition produces a carry the incremented/decremented word will be selected.
If the addition does not produce a carry this word remains unchanged. While the carry
word is being incremented/decremented, a second set of ﬂags is set up which is copied
into the ﬂag word if a carry is generated. In Figure 8.18 two possible locations of the
summand after the shift are indicated. The carry word is always the most signiﬁcant
word. Incrementing or decrementing this word never produces a carry. Thus the
adder/subtracter in Figure 8.18 can simply be built as a parallel carry-select-adder.
(iii) In the next cycle the computed sum is written back into the same four memory
cells of the CR to which the addition has been executed. Thus only one address
decoding is necessary for the read and write step. A different bus called write data in
Figure 8.18 is used for this purpose.
Figure 8.20 shows a more detailed block diagram for an SPU with short adder and
local store.
In summary the addition consists of the typical three steps: 1. read the summand,
2. perform the addition, and 3. write the sum back into the (local) memory. Since
a summand is delivered from the multiplier in each cycle, all three phases must be
active simultaneously, i.e., the addition itself must be performed in a pipeline. This
means that it must be possible to read from the memory and to write into the memory
in each cycle simultaneously. So two different data paths have to be provided. This,
however, is usual for register memory.
The pipeline for the addition has three steps. Pipeline conﬂicts are quite possible.
A pipeline conﬂict occurs if an incoming summand needs to be added to a partner

294
8 Scalar Products and Complete Arithmetic
from the CR which is still being computed and not yet available in the local memory.
These situations can be detected by comparing the exponents e, e′ and e′′ of three
successively incoming summands. In principle all pipeline conﬂicts can be resolved
by the hardware. Here we discuss the resolution of the two pipeline conﬂicts which
are by far the most likely.
One conﬂict situation occurs if two consecutive products carry the same exponent
e. In this case the two summands map onto the same three words of the CR. Then the
second summand is unable to read its partner for the addition from the local memory
because it is not yet available. This situation is checked by the hardware where the ex-
ponents e and e′ of two consecutive summands are compared. If they are identical, the
multiplexer blocks off the process of reading from the local memory. Instead the sum
which is just being computed is written back directly into the register before summa-
tion (RBS) via the multiplexer so that the second summand can be added immediately
without memory involvement.
Another possible pipeline conﬂict occurs if from three successively incoming sum-
mands the ﬁrst one and the third one carry the same exponent. Since the pipeline
consists of three steps, the partner for the addition of the third one is not yet in the lo-
cal memory but is still in the register after summation (RAS). This situation is checked
by the hardware also, see Figure 8.18. There the two exponents e and e′′ of the two
summands are compared. In the case of coincidence the multiplexer again suppresses
the reading from the local memory. Instead, the result of the former addition, which
is still in the RAS, is directly written back into the RBS via the multiplexer. So this
pipeline conﬂict can also be resolved by the hardware without memory involvement.
The case e = e′ = e′′ is also possible. It would cause a reading conﬂict in the
multiplexer. The situation can be avoided by writing a dummy exponent into e′′ or by
reading from the add/subtract unit with higher priority.
The product that arrives at the accumulation unit maps onto three consecutive words
of the CR. A more signiﬁcant fourth word absorbs the possible carry. The solution
for the two pipeline conﬂicts just described works well if this fourth word is the next
more signiﬁcant word. A carry is not absorbed by the fourth word if all its bits are
one, or are all zero in case of a subtraction. The probability that this is the case is
1 : 264 < 10−19. In the vast majority of instances this will not be the case.
If it is the case the word which absorbs the carry is selected by the ﬂag mechanism
and read into the most signiﬁcant word of the RBS. The addition step then again works
well including the carry resolution. But difﬁculties occur in both cases of pipeline
conﬂict. Figure 8.19 displays a certain part of the CR. The three words to which the
addition is executed are denoted by 1, 2 and 3. The next more signiﬁcant word is
denoted by 4 and the word which absorbs the carry by 5.
For a pipeline conﬂict with e = e′ or e = e′′ the following addition again touches
the words 1, 2 and 3. Now the carry is absorbed either by word 4 or by word 5. Word
4 absorbs the carry if an addition is followed by an addition or a subtraction by a

8.8 Top Speed Scalar Product Units
295
Figure 8.19. Carry propagation for pipeline conﬂict.
subtraction. Word 5 absorbs the carry if an addition is followed by a subtraction or
vice versa. So the hardware has to take care that either word 4 or 5 is read into the
most signiﬁcant word of the RBS depending on the operation which follows. The
case that word 5 is the carry word again needs no particular care. Word 5 is already
in the most signiﬁcant position of the RBS. It is simply treated the same way as the
words 1, 2 and 3. In the other case word 4 has to be read from the CR into the RBS,
simultaneously with the words 1, 2 and 3 from the add/subtract unit or from the RAS
into the RBS. In this case word 5 is written into the local memory via the normal write
path.
So far certain solutions for the possible pipeline conﬂicts e = e′ and e = e′′ have
been discussed. These are the most frequent but not the only conﬂicts that may occur.
Similar difﬁculties appear if two or three successive incoming summands overlap only
partially. In this case the exponents e and e′ and/or e′′ differ by 1 or 2 so that these
situations can be also detected by comparison of the exponents. Another pipeline
conﬂict appears if one of the two following summands overlaps with a carry word. In
these cases summands have to be built up in parts from the adder/subtracter or from the
RAS and the CR. Thus hardware solutions for these situations are more complicated
and costly. We leave a detailed study of these situations to the reader/designer and
offer the following alternative. The accumulation pipeline consists of three steps only.
Instead of investing in a lot of hardware logic for rare cases of pipeline conﬂicts it may
be simpler and less expensive to stall the pipeline and delay the accumulation by one
or two cycles as needed. It should be mentioned that other details, for instance the
width of the adder that is used, can also greatly change the design aspects. A 128
bit instead of the 64 bit adder width which was assumed here could simplify several
details.
It was already mentioned that the probability for the carry to run further than the
fourth word is less than 10−19. However, a particular situation where this happens
occurs if the sum changes its sign. This can happen frequently. To avoid a complicated
carry handling procedure in this case a small carry counter of perhaps three bits could
be appended to each 64 bit word of the CR. If these counters are not zero at the end
of the accumulation their contents have to be added to the CR. For further details see
[285, 286].
As was pointed out in connection with the unit discussed in Section 8.5.3, the ad-
dition of the summand actually can be carried out over 170 bits only. Thus the shifter
that is shown in Figure 8.18 can be reduced to a 106 to 170 bits shifter and the data
path from the shifter to the input register IR as well as the one to the RBS also need
to be 170 bits wide only.

296
8 Scalar Products and Complete Arithmetic
s’
e
IR
RBS
RAS
IR   = Input Register
RBS = Register Before Summation
RAS = Register After Summation
e’
e’’
s
m
read data
write data
read address
write
address
add / subtract
r’’
=
=
m’
rounding unit
r’
Figure 8.20. Block diagram for an SPU with short adder and local store
for a 32 bit data word and 64 bit data bus.

8.9 Hardware Complete Register Window
297
8.8.5
Carry-Free Accumulation of Products in Redundant Arithmetic
So far in this chapter various scalar product units have been studied. The ﬂag mech-
anism discussed in Section 8.4.4 was a basic technique for fast carry resolution. Its
implementation requires logic and hardware which uses some silicon area on the chip.
See, for instance, the chip displayed in Figure 8.8. Extra logic and circuitry is needed
to deal with carry resolution for the pipeline conﬂicts of the scalar product unit dis-
cussed in the previous subsection.
Instead of using more and more logic and hardware to deal with complicated sit-
uations, an alternative approach to the problem may be simpler and more attractive.
Redundant number representation and arithmetic, binary signed-digit arithmetic in
particular, would allow a carry-free accumulation of the products into the complete
register.
Two bits are needed to represent a binary digit in binary signed-digit number repre-
sentation. So the complete register would double in size. On the other hand, the carry
resolution logic would disappear.
Binary signed-digit arithmetic is a little more complicated than conventional binary
arithmetic. Nevertheless, eliminating the carry propagation makes it very fast. The
addition time is independent of the word length of the operands in binary signed-digit
arithmetic.
Addition of a product to the CR in signed-digit arithmetic would affect the CR only
locally. Thus a short adder would sufﬁce to accumulate the products into the CR. At
the end of the accumulation a conventional binary subtraction of two CR contents may
be necessary to obtain a conventional binary number. This long subtraction is about
of the same complexity as a single multiplication.
Very fast multipliers using binary signed-digit number representation and arith-
metic have been designed and used successfully. See [579, 580]. The subject is well
studied. See, for instance, [309] and the literature cited there.
Thus a general use of binary signed-digit number representation and arithmetic in
the SPU seems to be very attractive.
8.9
Hardware Complete Register Window
So far it has been assumed in this chapter that the SPU is incorporated as an integral
part of the arithmetic unit of the processor. Now we discuss the question of what can
be done if this is not the case and if not enough register space for the CR is available
on the processor.
The ﬁnal result of a scalar product computation is assumed to be a ﬂoating-point
number with an exponent in the range e1 ≤e ≤e2. If this is not the case, the problem
has to be scaled. During the computation of the scalar product, however, summands
with an exponent outside of this range may occur. The remaining computation then

298
8 Scalar Products and Complete Arithmetic
has to cancel all the digits outside of the range e1 ≤e ≤e2. So in a normal scalar
product computation, the register space outside this range will be used less frequently.
It was mentioned earlier in this book that the conclusion should not be drawn from
this consideration that the register size can be restricted to the single exponent range in
order to save some silicon area. This would require the use of complicated exception
handling routines in software or in hardware. The latter may ﬁnally require as much
silicon. A software solution certainly is much slower. The hardware requirement
for the CR with standard arithmetic is modest and the necessary register space really
should be invested.
However, the memory space for the CR on the arithmetic unit grows with the ex-
ponent range of the data format. If this range is extremely large, as for instance for
an extended precision ﬂoating-point format, then only an inner part of the CR can
be supported by hardware. We call this part of the CR a Hardware Complete Regis-
ter Window (HCRW). See Figure 8.21. The outer parts of this window must then be
handled in software. Probably they would seldom be used.
k
2e2
2l
2|e1|
2l
CRW
software CR
Figure 8.21. Hardware Complete Register Window (HCRW).
There are still other reasons that support the development of techniques for the
computation of the exact scalar product using an HCRW. Many conventional comput-
ers on the market do not provide enough register space to represent the full CR on the
CPU. Then an HCRW is one choice which allows a fast and exact computation of the
scalar product in many cases.
Another possibility is to place the CR in the user memory, i.e., in the data cache.
In this case only the starting address of the CR and the ﬂag bits are put into (ﬁxed)
registers of the general purpose register set of the computer. This solution has the
advantage that only a few registers are needed and that a longer CR window or even
the full CR can be provided. This reduces the need to handle exceptions. The disad-
vantage of this solution is that for each accumulation step, four memory words must
be read and written in addition to the two operand loads. So the scalar product com-
putation speed is limited by the data cache to processor transfer bandwidth and speed.
If the full CR is provided this is a very natural solution. It has been realized on sev-
eral IBM, SIEMENS and HITACHI computers of the /370 architecture in the 1980s
[650, 651, 653, 663].
A faster solution certainly is obtained for many applications with an HCRW in the
general purpose register set of the processor. Here only a part of the CR is present in

8.9 Hardware Complete Register Window
299
hardware. Overﬂows and underﬂows of this window have to be handled by software.
A full CR for the double precision data format of the IEEE arithmetic standard 754
requires 4288 bits or 67 words of 64 bits. We assume here that only ten of these words
are located in the general purpose register set.
Such a window covers the full CR that is needed for a scalar product computation
in case of the single precision data format of the IEEE arithmetic standard 754. It also
allows exact computation of scalar products in the case of the long data format of the
/370 architecture as long as no under- or overﬂows occur. In this case 64 + 28 + 63 =
155 hexadecimal digits or 620 bits are required. With an HCRW of 640 bits all scalar
products that do not cause an under- or overﬂow could have been correctly computed
on these machines. This architecture was successfully used and even dominated the
market for more than 20 years. This example shows that even if an HCRW of only 640
bits is available, the vast majority of scalar products will execute on fast hardware.
Of course, even if only an HCRW is available, all scalar products should be com-
puted exactly. Any operation that over- or underﬂows the HCRW must be completed
in software. This requires a complete software implementation of the CR, i.e., a vari-
able of type complete. All additions that do not ﬁt into the HCRW must be executed
in software into this complete variable.
There are three situations where the HCRW can not accumulate the product exactly:
• The exponent of the product is so high that the product does not (completely)
ﬁt into the HCRW. Then the product is added in software to the complete
variable.
• The exponent of the product is so low that the product does not (completely)
ﬁt into the HCRW. Then the product is added in software to the complete
variable.
• The product ﬁts into the HCRW, but its accumulation causes a carry to be prop-
agated outside the range of the HCRW. In this case the product is added into the
HCRW. The carry must be added in software to the complete variable.
If at the end of the accumulation the contents of the software CR are not zero, the
contents of the HCRW must be added to the software CR to obtain the correct value
of the scalar product. Then a rounding can be performed if required. If at the end of
the accumulation the contents of the software CR are zero, the HCRW contains the
exact value of the scalar product and a rounded value can be obtained from it.
Thus, in general, a software controlled full CR supplements an HCRW. The soft-
ware routines must be able to perform the following functions:
• clear the software CR. This routine must be called during the initialization of the
HCRW. Ideally, this routine only sets a ﬂag. The actual clearing is only done if
the software CR is needed.
• add or subtract a product to/from the software CR.

300
8 Scalar Products and Complete Arithmetic
• add or subtract a carry or borrow to/from the software CR at the appropriate digit
position.
• add the HCRW to the software CR. This is required to produce the ﬁnal result
when both the HCRW and the software CR were used. Then a rounding can be
performed.
• round the software CR to a ﬂoating-point number.
With this software support scalar products can be computed exactly using an
HCRW at the cost of a substantial software overhead and a considerable time penalty
for products that fall outside the range of the HCRW. The software overhead caused
by the reduction of the full width of the CR to an HCRW represents the trade off
between hardware expenditure and runtime.
A much weaker alternative solution to the HCRW-software environment just de-
scribed is to discard the products that underﬂow the HCRW. A counter variable is
used to count the number of discarded products. If a number of products were dis-
carded, the last bits of the HCRW must be considered invalid. A valid rounded result
can be generated by hardware if these bits are not needed. If this procedure fails to
produce a useful answer the whole accumulation is repeated in software using a full
CR.
A 640 bit HCRW seems to be the shortest satisfactory hardware window. If this
much register space is not available, a software implementation probably is the best
solution.
If a shorter HCRW must be implemented, then it should be a movable window. This
can be represented by an exponent register associated with the hardware window. At
the beginning of an accumulation, the exponent register is set so that the window
covers the least signiﬁcant portion of the CR. Whenever a product would cause the
window to overﬂow, its exponent tag is adjusted, i.e., the window moves to the left,
so that the product ﬁts into the window. Products that would cause an underﬂow
are counted and otherwise ignored. The rounding instruction checks whether enough
signiﬁcant digits are left to produce a correctly rounded result or whether too much
cancellation did occur. In the latter case it is up to the user to accept the inexact result
or to repeat the whole accumulation in software using a full CR.
Using this technique an HCRW as short as 256 bits could be used to perform
rounded scalar product computation and quadruple precision arithmetic. However,
it would not be possible to perform many other nice and useful applications of the
exact scalar product with this type of scalar product hardware such as, for instance,
complete arithmetic, a long real arithmetic or a long interval arithmetic.
To allow fast execution of a number of multiple precision arithmetics the HCRW
should not be too small.

Part III
Principles of Veriﬁed Computing

Chapter 9
Sample Applications
Floating-point arithmetic is the fast way to perform scientiﬁc and engineer-
ing calculations. Today, individual ﬂoating-point operations are maximally
accurate in general. However, after as few as two operations, the computed
result might be completely wrong. Computers can now carry out up to 1014
or 1015 ﬂoating-point operations in a second. Therefore particular attention
must be paid to the reliability of the computed results. Conventional error
analysis is impossible for computation at such speed.
Numerical analysts have devised algorithms and techniques which make
it possible for the computer itself to verify the correctness of computed re-
sults, and even to establish the existence and uniqueness of a solution within
computed close bounds, for numerous problems and applications. Such
techniques are termed automatic result veriﬁcation.
Advanced computer arithmetic as treated in this book provides the basis
for results to be easily and automatically veriﬁed on the computer. Interval
arithmetic brings the continuum to the computer. Fixed-point accumula-
tion of products makes it easy and quite natural to ensure the accuracy of a
computation. A veriﬁed solution of an ordinary differential equation, for in-
stance, is exact just as is a solution obtained by a computer algebra system,
which as a rule still requires a valid formula evaluation.
In this chapter we go over a few sample applications showing the use of
advanced computer arithmetic. We sketch what automatic result veriﬁcation
means and how it works in the case of these examples.
In the ﬁrst section we develop some basic aspects of interval mathe-
matics. Interval arithmetic is introduced as a shorthand notation and an
automatic calculus to deal with inequalities. Interval operations are also
interpreted as special power set operations. The inclusion isotony and the
inclusion property are central and important consequences of this property.
They can be used to enclose the range of a function’s values. Advanced
techniques for such enclosure by centered forms or by subdivision are also
discussed. These techniques are used, for instance, for global optimization.
In combination with automatic differentiation, interval arithmetic allows
computation of enclosures of derivatives, of Taylor coefﬁcients, of gradi-
ents, of Jacobian or Hessian matrices.
Evaluation of a function for an interval X delivers a superset of the func-
tion’s values over X. This overestimation tends to zero with the width of the

303
interval X. Thus for small intervals interval evaluation of a function prac-
tically delivers the function’s values. Many numerical methods proceed in
small steps. So this property together with differentiation arithmetic to com-
pute enclosures of derivatives is the key technique for validated numerical
computation of integrals, for solution of ordinary differential equations, of
reﬁned techniques for global optimization and for many other applications.
Solving systems of linear equations is basic to numerical computation.
We show how highly accurate guaranteed bounds for the solution can be ob-
tained, based on an approximate solution, and how the existence and unique-
ness of the solution within these bounds can be proved by the computer. It
may well happen that an attempt to verify the correctness of a computed re-
sult fails to produce a correct answer. This is detected by the computer, and
in this case the computer itself can choose an alternative algorithm or repeat
the computation using higher precision. We show that by using the exact
scalar product a highly accurate enclosure of the solution can be obtained
even in very ill conditioned cases without using higher precision arithmetic.
Newton’s method is considered in two sections of this chapter. In con-
ventional numerical analysis it is the crucial method for nonlinear problems.
Traditionally Newton’s method is used to compute an approximate solution
of a nonlinear real function. It is well known that the method converges
quadratically to the solution if the function is twice continuously differen-
tiable and the starting value is sufﬁciently close to the solution. If these
conditions do not hold, and in other cases, the method may well fail in ﬁnite
as well as in inﬁnite precision arithmetic even if there is only a single solu-
tion in a given interval. The method is only locally convergent. In contrast to
this, the interval version of Newton’s method is globally convergent. Begin-
ning with a rough enclosure of the solution it computes a nested sequence of
enclosures. The bounds ﬁnally converge quadratically to the solution. The
interval version of Newton’s method never diverges or fails otherwise, not
even in rounded arithmetic.
Newton’s method reaches its ultimate elegance and strength in the ex-
tended interval Newton method. It separates different solutions, and it en-
closes all single zeros in a given domain. It is globally convergent and it
never fails. The method is locally quadratically convergent.
Arithmetic expressions are basic ingredients of all numerical computing.
In Section 9.6 a method is developed which evaluates an arithmetic expres-
sion with guaranteed high accuracy. The method uses fast interval arithmetic
and a fast and exact scalar product. If these operations are hardware sup-
ported the computing time is of the same order as that for a conventional
evaluation of the expression in ﬂoating-point arithmetic. If the ﬂoating-
point evaluation fails completely additional computing time and storage are
needed.

304
9 Sample Applications
With a fast and exact multiply and accumulate operation or an exact
scalar product fast quadruple or multiple precision arithmetics can easily
be provided on the computer for real as well as for interval data. These
operations are developed in the last section of this chapter.
9.1
Basic Properties of Interval Mathematics
9.1.1
Interval Arithmetic, a Powerful Calculus to Deal with Inequalities
Problems in technology and science are often described by an equation or by a system
of equations. Mathematics is used to manipulate these equations in order to obtain a
solution. The Gauss algorithm, for instance, is used to compute the solution of a sys-
tem of linear equations by adding, subtracting, multiplying and dividing equations in
a systematic manner. Newton’s method is used to compute approximately the location
of a zero of a nonlinear function or of a system of such functions.
Data are often given by bounds rather than by simple numbers. Bounds are ex-
pressed by inequalities. To compute bounds for the solution to a problem requires a
systematic calculus to deal with inequalities. Interval arithmetic provides this calcu-
lus. It supplies the basic rules for how to add, subtract, multiply, divide, and otherwise
manipulate inequalities in a systematic manner: Let bounds for two real numbers a
and b be given by the inequalities a1 ≤a ≤a2 and b1 ≤b ≤b2. Addition of these
inequalities leads to bounds for the sum a + b:
a1 + b1 ≤a + b ≤a2 + b2.
The inequality for b is reversed by multiplication with −1: −b2 ≤−b ≤−b1. Ad-
dition to the inequality for a then delivers the rule for the subtraction of one inequality
from another:
a1 −b2 ≤a −b ≤a2 −b1.
Interval arithmetic provides a shorthand notation for these rules by suppressing the
≤symbols. We simply identify the inequality a1 ≤a ≤a2 with the closed and
bounded real interval [a1, a2]. The rules for addition and subtraction for two such
intervals now read:
[a1, a2] + [b1, b2] = [a1 + b1, a2 + b2],
(9.1.1)
[a1, a2] −[b1, b2] = [a1 −b2, a2 −b1].
(9.1.2)
The rule for multiplication of two intervals is more complicated. Nine cases are to
be distinguished depending on whether a1, a2, b1, b2, are less or greater than zero. For
division the situation is similar. In conventional interval arithmetic division A/B is
not deﬁned if 0 ∈B.

9.1 Basic Properties of Interval Mathematics
305
As a result of these rules it can be stated that for real intervals the result of an
interval operation A ◦B, for all ◦∈{+, −, ·, /}, can be expressed in terms of the
bounds of the interval operands (with the A/B exception above). In order to get each
of these bounds, typically only one real operation is necessary. Only in one case of
multiplication, 0 ∈
◦A and 0 ∈
◦B, two products have to be calculated and compared.
Here
◦A denotes the interior of A, i.e., c ∈
◦A means a1 < c < a2.
We illustrate the efﬁciency of this calculus for inequalities by a simple example.
See [13]. Let x = Ax + b be a system of linear equations in ﬁxed-point form with
a contracting real matrix A and a real vector b, and let the interval vector X be a
rough initial enclosure of the solution x∗∈X. For this interval vector X we can
now formally write down the Jacobi method, the Gauss-Seidel method, a relaxation
method or some other iterative scheme for the solution of the linear system. Doing so
we obtain a number of iterative methods for the computation of enclosures of linear
systems of equations. Further iterative schemes then can be obtained by taking the
intersection of two successive approximations. If we now decompose all these meth-
ods in formulas for the bounds of the intervals we obtain a major number of methods
for the computation of bounds for the solution of linear systems which have been de-
rived by well-known mathematicians painstakingly about 40 years ago, see [28, 135].
The calculus of interval arithmetic reproduces these and other methods in the simplest
way. The user does not have to take care of the many case distinctions occurring in
the matrix vector multiplications. The computer executes them automatically by the
preprogrammed calculus. Also the rounding errors are enclosed. The calculus evolves
its own dynamics.
9.1.2
Interval Arithmetic as Executable Set Operations
The rules for interval operations can also be interpreted as arithmetic operations for
sets. As such they are special cases of general set operations. Further important
properties of interval arithmetic can immediately be obtained via set operations. Let
M be any set with a dyadic operation ◦: M × M →M deﬁned for its elements. The
power set PM of M is deﬁned as the set of all subsets of M. The operation ◦in M
can be extended to the power set PM by the following deﬁnition
A ◦B := {a ◦b | a ∈A ∧b ∈B} for all A, B ∈PM.
(9.1.3)
The least element in PM with respect to set inclusion as an order relation is the
empty set ∅. The greatest element is the set M. The empty set is a subset of any set.
Any arithmetic operation on the empty set produces the empty set.
The following properties are obvious and immediate consequences of (9.1.3):
A ⊆B ∧C ⊆D ⇒A ◦C ⊆B ◦D for all A, B, C, D ∈PM,
(9.1.4)
and in particular
a ∈A ∧b ∈B ⇒a ◦b ∈A ◦B for all A, B ∈PM.
(9.1.5)

306
9 Sample Applications
Property (9.1.4) is called the inclusion isotony (or inclusion monotony). (9.1.5) is
called the inclusion property.
By use of parentheses these rules can immediately be extended to expressions with
more than one arithmetic operation, e.g.,
A ⊆B ∧C ⊆D ∧E ⊆F ⇒A ◦C ⊆B ◦D ⇒(A ◦C) ◦E ⊆(B ◦D) ◦F,
and so on. Moreover, if more than one operation is deﬁned in M this chain of conclu-
sions also remains valid for expressions containing several different operations.
If we now replace the general set M by the set of real numbers, (9.1.3), (9.1.4), and
(9.1.5) hold in particular for the power set PR of the real numbers R. This is the case
for all operations ◦∈{+, −, ·, /}, if we assume that in case of division 0 is not an
element of the divisor, for instance, 0 /∈B in (9.1.3).
The set IR of closed and bounded intervals over R is a subset of PR. Thus (9.1.3),
(9.1.4), and (9.1.5) are also valid for elements of IR. The set IR with the operations
(9.1.3), ◦∈{+, −, ·, /}, is an algebraically closed1 subset within PR. That is, if
(9.1.3) is performed for two intervals A, B ∈IR the result is always an interval
again. This holds for all operations ◦∈{+, −, ·, /} with 0 /∈B in the case of
division. This property is a simple consequence of the fact that for all arithmetic
operations ◦∈{+, −, ·, /}, a ◦b is a continuous function of both variables. A ◦B
is the range of this function over the product set A × B. Since A and B are closed
intervals, A × B is a simply connected, bounded and closed subset of R2. In such a
region the continuous function a ◦b takes a maximum and a minimum as well as all
values in between. Therefore
A ◦B = [
min
a∈A,b∈B(a ◦b),
max
a∈A,b∈B(a ◦b)], for all ◦∈{+, −, ·, /},
provided that 0 /∈B in the case of division.
Consideration of (9.1.3), (9.1.4) and (9.1.5) for intervals of IR leads to the cru-
cial properties of all applications of interval arithmetic. Because of the great im-
portance of these properties we repeat them here. Thus we obtain for all operations
◦∈{+, −, ·, /}:
The set deﬁnition of interval arithmetic:
A ◦B := {a ◦b | a ∈A ∧b ∈B}
for all A, B ∈IR,
0 /∈B in case of division,
(9.1.6)
the inclusion isotony (or inclusion monotony):
A ⊆B ∧C ⊆D ⇒A ◦C ⊆B ◦D
for all A, B, C, D ∈IR,
0 /∈C, D in case of division,
(9.1.7)
1As the integers are within the reals for ◦∈{+, −, ·}.

9.1 Basic Properties of Interval Mathematics
307
and in particular the inclusion property:
a ∈A ∧b ∈B ⇒a ◦b ∈A ◦B
for all A, B ∈IR,
0 /∈B in case of division.
(9.1.8)
If for M = R in (9.1.3) the number of elements in A or B is inﬁnite, the operations
are effectively not executable because inﬁnitely many real operations would have to
be performed. If A and B are intervals of IR, however, the situation is different. In
general A or B or both will again contain inﬁnitely many real numbers. The result of
the operation (9.1.6), however, can now be performed by a ﬁnite number of operations
with real numbers, with the bounds of A and B. For all operations ◦∈{+, −, ·, /}
the result is obtained by the explicit formulas derived in Chapter 4 of this book or at
[353, 367].
For intervals A = [a1, a2] and B = [b1, b2] the formulas for the interval operations
can be summarized by
A ◦B = [ min
i,j=1,2(ai ◦bj), max
i,j=1,2(ai ◦bj)] for all ◦∈{+, −, ·, /},
(9.1.9)
with 0 /∈B in the case of division. (9.1.9), however, should not be interpreted as
recommendation for the computation of the result of interval operations.
Since interval operations are particular power set operations, the inclusion isotony
and the inclusion property also hold for expressions with more than one arithmetic
operation.
In programming languages the concept of an arithmetic expression is usually de-
ﬁned to be a little more general. Besides constants and variables, elementary functions
(sometimes called standard functions) like sqr, sqrt, sin, cos, exp, log, tan may also
be elementary ingredients. All these are put together with arithmetic operators and
parentheses into the general concept of an arithmetic expression. This construct is
illustrated by the syntax diagram of Figure 9.1. Therein solid lines are to be traversed
from left to right and from top to bottom. Dotted lines are to be traversed oppositely,
i.e., from right to left and from bottom to top. In Figure 9.1 the syntax variable REAL
FUNCTION merely represents a real arithmetic expression hidden in a subroutine.
Now we deﬁne the general concept of an arithmetic expression for the data type
interval by exchanging the data type real in Figure 9.1 for the data type
interval. This results in the syntax diagram for INTERVAL EXPRESSION shown
in Figure 9.2. In Figure 9.2 the syntax variable INTERVAL FUNCTION represents an
interval expression hidden in a subroutine.
In the syntax diagram for INTERVAL EXPRESSION in Figure 9.2 the concept of an
interval elementary function is not yet deﬁned. We simply deﬁne it as the range of the
function’s values taken over an interval (within the domain of deﬁnition D(f) of the
function). In case of a real function f we denote the range of values over the interval
[a1, a2] by
f([a1, a2]) := {f(a) | a ∈[a1, a2]}, with [a1, a2] ∈D(f).

308
9 Sample Applications
Figure 9.1. Syntax diagram for REAL EXPRESSION.
Figure 9.2. Syntax diagram for INTERVAL EXPRESSION.

9.1 Basic Properties of Interval Mathematics
309
For instance:
e[a1,a2] = [ea1, ea2],
[a1, a2]2n =

[min(a2n
1 , a2n
2 ), max(a2n
1 , a2n
2 )]
for 0 /∈[a1, a2],
[0, max(a2n
1 , a2n
2 )]
for 0 ∈[a1, a2],
sin[−π/4, π/4] = [−1
2
√
2, 1
2
√
2],
cos[0, π/2] = [0, 1].
For non-monotonic functions the computation of the range of values over an in-
terval [a1, a2] requires the determination of the global minimum and maximum of the
function in the interval [a1, a2]. For the usual elementary functions, however, these are
known. With this deﬁnition of elementary functions for intervals, the key properties of
interval arithmetic, the inclusion monotony (9.1.5) and the inclusion property (9.1.6)
extend immediately to elementary functions and with this to interval expressions as
deﬁned in Figure 9.2:
A ⊆B ⇒f(A) ⊆f(B), with A, B ∈IR
(inclusion isotone),
and in particular for a ∈R and A ∈IR:
a ∈A ⇒f(a) ∈f(A)
(inclusion property).
We summarize the development so far by stating that interval arithmetic expres-
sions are generally inclusion isotone and that the inclusion property holds. These
are the key properties of interval arithmetic. They give interval arithmetic its raison
d’ˆetre. To start with, they provide the possibility of enclosing imprecise data within
bounds and then continuing the computation with these bounds. This always results
in guaranteed enclosures.
As the next step we deﬁne a (computable) real function simply by a real arithmetic
expression. We need the concept of an interval evaluation of a real function. It
is deﬁned as follows: In the arithmetic expression for the function all operands are
replaced by intervals and all operations by interval operations (where all intervals
must be within the domain of deﬁnition of the real operands). This is just the step
from Figure 9.1 to Figure 9.2. What is obtained is an interval expression. Then all
arithmetic operations are performed in interval arithmetic. For a real function f(a)
we denote the interval evaluation over the interval A by F(A).
With this deﬁnition we can immediately conclude that interval evaluations of (com-
putable) real functions are inclusion isotone and that the inclusion property holds in
particular:
A ⊆B ⇒F(A) ⊆F(B)
(inclusion isotone),
(9.1.10)
a ∈A ⇒f(a) ∈F(A)
(inclusion property).
(9.1.11)

310
9 Sample Applications
These concepts immediately extend in a natural way to functions of several real
variables. In this case in (9.1.11) a is an n-tuple, a = (a1, a2, . . . , an), and A and B
are higher dimensional intervals, e.g., A = (A1, A2, . . . , An), with Ai ∈IR for all
i = 1(1)n.
Remark 9.1. Two different real arithmetic expressions can deﬁne equivalent real
functions, for instance:
f(x) = x(x −1)
and
g(x) = x2 −x.
Evaluation of the two expressions for a real number always leads to the same real
function value. In contrast to this, interval evaluation of the two expressions may lead
to different intervals. In the example we obtain for the interval A = [1, 2]:
F(A) = [1, 2]([1, 2] + [−1, −1])
G(A) = [1, 2][1, 2] −[1, 2]
= [1, 2][0, 1] = [0, 2],
= [1, 4] −[1, 2] = [−1, 3].
■
A look at the syntax diagram for INTERVAL EXPRESSION, Figure 9.2, shows that
operator overloading in a programming language does not sufﬁce to support interval
arithmetic effectively. Also, the elementary functions must be provided for interval
arguments and the concept of a function subroutine must not be restricted to the data
types integer and real. It must be extended to the data type interval.
Implementing the elementary functions for the data type interval is a great
challenge for the mathematician. For an elementary function of the data type real
the computer provides a result, the accuracy of which cannot easily be judged by the
user. This is no longer the case when the elementary functions are provided for inter-
val arguments. Then, if called for a point interval (where the lower and upper bound
coincide), a comparison of the lower and upper bound of the result of the interval eval-
uation of the function reveals immediately the accuracy with which the elementary
function has been implemented. Extremely careful implementation of the elementary
functions is needed to get such results, and since interval versions of the elementary
functions have been provided on a large scale [288, 289, 290, 291, 355, 356, 653] the
conventional real elementary functions on computers also had to be and have been
improved step by step by the manufacturers. A highly advanced programming envi-
ronment in this respect is a decimal version of PASCAL-XSC [79, 80] where, besides
the usual 24 elementary functions, about the same number of special functions are
provided for real and interval arguments with highest accuracy.
Ease of programming is essential for any sophisticated use of interval arithmetic. If
it is not so, coding difﬁculties absorb all the attention and capacity of users and prevent
them from developing deeper mathematical ideas and insight. It is a matter of fact that
a great many of the existing and established interval methods and algorithms have
originally been developed in PASCAL-XSC even if they have been coded afterwards
in other languages. Programming ease is essential indeed.

9.1 Basic Properties of Interval Mathematics
311
We summarize this discussion by stating that it does not sufﬁce for an adequate
use of interval arithmetic on computers that only the four basic arithmetic operations
+, −, · and / for intervals are somehow supported by the computer hardware. An ap-
propriate language support is absolutely necessary. Two things seem to be necessary
for a major breakthrough in interval mathematics. A leading vendor has to provide
the necessary hardware and software support and the body of numerical analysts must
acquire a broader insight and skills to use this support.
9.1.3
Enclosing the Range of Function Values
The interval evaluation of a real function f over the interval A was denoted by F(A).
We now compare it with the range of function values over the interval A which was
denoted by
f(A) := {f(a) | a ∈A}.
(9.1.12)
We have observed that interval evaluation of an arithmetic expression and of real
functions is inclusion isotone (9.1.7), (9.1.10) and that the inclusion property (9.1.8),
(9.1.11) holds. Since (9.1.8) and (9.1.11) hold for all a ∈A we can immediately state
that
f(A) ⊆F(A),
(9.1.13)
i.e., that the interval evaluation of a real function over an interval delivers a superset
of the range of function values over that interval. If A is a point interval [a, a] this
reduces to:
f(a) ∈F([a, a]).
(9.1.14)
These are basic properties of interval arithmetic. Computing with inequalities al-
ways aims for bounds for function values, or for bounds for the range of function
values. Interval arithmetic provides for this computation in principle.
Many applications need the range of function values over an interval. Its computa-
tion is a very difﬁcult task. It is equivalent to the computation of the global minimum
and maximum of the function in that interval. On the other hand, interval evaluation
of an arithmetic expression is very easy to perform. It requires about twice as many
real arithmetic operations as an evaluation of the expression in real arithmetic. Thus
interval arithmetic provides an easy means to compute upper and lower bounds for
the range of values of an arithmetic expression.
In essence, any complicated algorithm performs a sequence of arithmetic expres-
sions. Thus interval evaluation of such an algorithm would compute bounds for the
result from given bounds for the data. In practice however, the sizes of the intervals
grow very fast and for lengthy algorithms the bounds quickly become meaningless,
especially if the bounds for the starting data are already wide. This raises the question
of whether measures can be taken to keep the sizes of the intervals from growing too
much. Interval mathematics has developed such measures and we are going to sketch
these now.

312
9 Sample Applications
If an enclosure for a function value is computed by (9.1.14), the quality of the com-
puted result F([a, a]) can be judged by the diameter of the interval F([a, a]). This
ability to easily judge the quality of a computed result is not available with (9.1.13).
Even if F(A) is a large interval, it can be a good approximation for the range of func-
tion values f(A) if the latter is large also. So some means to measure the deviation
between f(A) and F(A) in (9.1.13) is desirable.
It is well known that the set IR of real intervals becomes a metric space with the
so-called Hausdorff metric, where the distance q between two intervals A = [a1, a2]
and B = [b1, b2] is deﬁned by
q(A, B) := max{|a1 −b1|, |a2 −b2|}.
(9.1.15)
See, for instance, [28, 29].
With this distance function q the following relation can be proved to hold under
natural assumptions about f:
q(f(A), F(A)) ≤α · d(A), with a constant α ≥0.
(9.1.16)
Here d(A) denotes the diameter of the interval A:
d(A) := |a2 −a1|.
(9.1.17)
In case of functions of several real variables the maximum of the diameters d(Ai)
appears on the right hand side of (9.1.16).
The relation (9.1.16) shows that the distance between the range of values of the
function f over the interval A and the interval evaluation of the expression for f
tends to zero linearly with the diameter of the interval A. So the overestimation of
f(A) by F(A) decreases with the diameter of A and in the limit d(A) = 0 and the
overestimation vanishes.
Because of this result subdivision of the interval A into subintervals Ai, i = 1(1)n,
with A = "n
i=1 Ai is a frequently applied technique to obtain better approximations
for the range of function values. Then (9.1.16) holds for each subinterval:
q(f(Ai), F(Ai)) ≤αi · d(Ai), with αi ≥0 and i = 1(1)n,
and, in general, the union of the interval evaluations over all subintervals
n5
i=1
F(Ai)
is a much better approximation for the range f(A) than is F(A).
There are yet other methods for better enclosing the range of function values f(A).
We have already observed that the interval evaluation F(A) of a function f depends

9.1 Basic Properties of Interval Mathematics
313
on the expression used for the representation of f. So by choosing appropriate repre-
sentations for f the overestimation of f(A) by the interval evaluation F(A) can often
be reduced. Indeed, if f allows a representation of the form
f(x) = f(c) + (x −c) · h(x), with c ∈A,
(9.1.18)
then under natural assumptions on h
q(f(A), F(A)) ≤β · (d(A))2, with a constant β ≥0.
(9.1.19)
(9.1.18) is called a centered form of f. In (9.1.18) c is not necessarily the center of A
although it is often chosen as the center. (9.1.19) shows that the distance between the
range of values of the function f over the interval A and the interval evaluation of a
centered form of f tends toward zero quadratically with the diameter of the interval A.
In practice, this means that for small intervals the interval evaluation of the centered
form leads to a very good approximation of the range of function values over an
interval A. Again, subdivision is a method that can be applied in the case of a large
interval A. It should be clear, however, that in general the bound in (9.1.19) is only
better than in (9.1.16) for small intervals.
The reduced overestimation of the range of function values by the interval evalua-
tion of the function with the diameter of the interval A, and the method of subdivision,
are reasons why interval arithmetic can successfully be used in many applications.
Numerical methods often proceed in small steps. This is the case, for instance, with
numerical quadrature or cubature, or with numerical integration of ordinary differ-
ential equations. In all these cases an interval evaluation of the remainder term of
the integration formula (using differentiation arithmetic) controls the step size of the
integration, and anyhow because of the small steps, overestimation is practically neg-
ligible.
We now mention brieﬂy how centered forms can be obtained. Usually a centered
form is derived via the mean-value theorem. If f is differentiable in its domain D,
then f(x) = f(c) + f′(ξ)(x −c) for ﬁxed c ∈D and some ξ between x and c. If x
and c are elements within the interval A ⊆D, then also ξ ∈A. Therefore
f(x) ∈F(A) := f(c) + F ′(A)(A −c), for all x ∈A.
Here F ′(A) is an interval evaluation of f′(x) in A.
In (9.1.18) the slope
h(x) = f(x) −f(c)
x −c
can be used instead of the derivative for the representation of f(x). Slopes often lead
to better enclosures for f(A) than do derivatives. For details see [37, 337, 492].
Derivatives and enclosures of derivatives can be computed by a process which is
called automatic differentiation or differentiation arithmetic. Slopes and enclosures

314
9 Sample Applications
of slopes can be computed by another process which is very similar to automatic dif-
ferentiation. In both cases the computation of the derivative or slope or enclosures of
these is done together with the computation of the function value. For these processes
only the expression or algorithm for the function is required. No explicit formulas for
the derivative or slope are needed. The computer interprets the arithmetic operations
in the expression by differentiation or slope arithmetic. The arithmetic is hidden in the
runtime system of the compiler. It is activated by type speciﬁcation of the operands.
For details see [37, 39, 205, 206, 337, 492, 494], and Section 9.2 on differentiation
arithmetic. Thus the computer is able to produce and enclose the centered form via
the derivative or slope automatically.
Without going into further details, we mention once more that none of these con-
siderations is restricted to functions of a single real variable. Subdivision in higher
dimensions, however, is a difﬁcult task which requires additional tools and strategies.
Typical of such problems are the computation of the bounds of the solution of a system
of nonlinear equations, and global optimization or numerical integration of functions
of more than one real variable. In all these and other cases, zero ﬁnding is a central
task. Here the extended interval Newton method plays an extraordinary role so we
review this method in Section 9.3.
9.1.4
Nonzero Property of a Function, Global Optimization
We ﬁrst consider the question of whether a given function, deﬁned by an arithmetic
expression, has zeros in a given interval X = [a, b], see Figure 9.3. This question
cannot be answered with mathematical certainty if only ﬂoating-point arithmetic is
available. All one can do is to evaluate the function at say 1000 points in the interval
X. If all computed function values are positive, it is very likely that the function does
not have any zeros in X. However, this conclusion is certainly not reliable. Because of
rounding errors, a positive result could be computed for a negative function value. The
function could also descend to a negative value between adjacent evaluation points
with positive values (see the lower part of Figure 9.3). The question can be answered
much more simply if interval arithmetic is available. A single interval evaluation of
the function may sufﬁce to solve the problem with complete mathematical certainty.
We evaluate the function only once in interval arithmetic for the interval X. This
delivers a superset Y = F(X) of the range f(X) of all function values over X. If
this superset does not contain zero, as in our example, the range, which is a subset,
does not contain zero. As a consequence, the function does not have any zeros in the
interval X,
0 /∈Y = F(X) ⇒0 /∈f(X) ⊆F(X) = Y.
Even this simple example shows that interval arithmetic is not necessarily more
costly than ﬂoating-point arithmetic. It is a useful extension of ﬂoating-point arith-
metic. The interval evaluation of the function is only twice as costly as a single

9.1 Basic Properties of Interval Mathematics
315
ﬂoating-point evaluation, not to mention the 1000 ﬂoating-point evaluations cited
above.
a
b
f(X)
Y
y = f(x)
x
a
b
y = f(x)
x
Figure 9.3. Non zero property of a function.
As the next example we consider the task of determining the global minimum of a
function in a given domain. Again we restrict the discussion to the most simple, the
one dimensional case, see Figure 9.4. The given domain is divided into a number of
subintervals. In each subinterval, we now evaluate the function in interval arithmetic.
This delivers a superset of the range of values of the function in each subinterval. Now
we select the subinterval with the lowest lower bound for the range of function values.
For an inner point of this subinterval we evaluate the function in interval arithmetic.
This delivers a guaranteed upper bound of a function value. Those subintervals, the
lower bound of which is greater than this function value can now be eliminated from

316
9 Sample Applications
further treatment, because they cannot contain the global minimum. The remaining
subintervals (two in the example in Figure 9.4) now are further subdivided and treated
by the same technique.
The method also works and succeeds very well for higher dimensions up to a cer-
tain limit, since whole continua can quickly be excluded from the search. In the
literature [211, 212, 213, 491, 492, 494] more reﬁned methods are used. Frequently,
safe bounds for the global minimum can be computed faster than an approximation
delivered by conventional techniques, the quality of which is still uncertain.
These methods, of course, can also be used for a fast and highly accurate computa-
tion of the range of values of a function in a given domain.
x
f(x)
Figure 9.4. Global optimization.
9.2
Differentiation Arithmetic, Enclosures of Derivatives
For many applications in scientiﬁc computing the value of the derivative of a function
is needed. The interval Newton method requires the computation of an enclosure
of the ﬁrst derivative of the function over an interval. The typical “school method”
ﬁrst computes a formal expression for the derivative of the function by applying well-
known rules of differentiation. Then this expression is evaluated for a point or an
interval. Differentiation arithmetic avoids the computation of a formal expression for
the derivative. It computes derivatives or enclosures of derivatives just by computing
with numbers or intervals. We are now going to sketch this method for the simplest
case where the value of the ﬁrst derivative is to be computed. If u(x) and v(x) are
differentiable functions then the following rules for the computation of the derivative

9.2 Differentiation Arithmetic, Enclosures of Derivatives
317
of the sum, difference, product, and quotient of the functions are well known:
(u(x)
+
v(x))′
=
u′(x) + v′(x),
(u(x)
−
v(x))′
=
u′(x) −v′(x),
(u(x)
·
v(x))′
=
u′(x) · v(x) + u(x) · v′(x),
(u(x)
/
v(x))′
=
1
v2(x)(u′(x)v(x) −u(x)v′(x))
=
1
v(x)(u′(x) −u(x)
v(x)v′(x)).
(9.2.1)
These rules can be used to deﬁne an arithmetic for ordered pairs of numbers, similar
to complex arithmetic or interval arithmetic. The ﬁrst component of the pair consists
of a function value u(x0) at a point x0. The second component consists of the value
of the derivative u′(x0) of the function at the point x0. For brevity we simply write
(u, u′) for the pair of numbers. Then the arithmetic for pairs follows immediately
from (9.2.1):
(u, u′)
+
(v, v′)
=
(u + v, u′ + v′),
(u, u′)
−
(v, v′)
=
(u −v, u′ −v′),
(u, u′)
·
(v, v′)
=
(u · v, u′v + uv′),
(u, u′)
/
(v, v′)
=
(u/v, 1
v(u′ −( u
v)v′)), v ̸= 0.
(9.2.2)
The set of rules (9.2.2) is called differentiation arithmetic. It is an arithmetic which
deals just with numbers. The rules (9.2.2) are easily programmable and are executable
by a computer. These rules are now used to compute simultaneously the value of a
real function and of its derivative at a point x0. For brevity we call these values the
function-derivative-value-pair. Why and how can this computation be done?
A computable real function can be deﬁned by an arithmetic expression in the man-
ner that arithmetic expressions are usually deﬁned in a programming language. Apart
from the arithmetic operators +, −, ·, and /, arithmetic expressions contain only three
kinds of operands as basic components. These are constants, variables and certain
differentiable elementary functions such as exp, log, sin, cos, or sqr. The derivatives
of these functions are well known.
If for a function f(x) a function-derivative-value-pair is to be computed at a point
x0, all basic components of the arithmetic expression of the function are replaced by
their particular function-derivative-value-pair by the following rules:
a constant:
c
−→
(c, 0),
the variable:
x0
−→
(x0, 1),
the elementary functions:
exp(x0)
−→
(exp(x0), exp(x0)),
log(x0)
−→
(log(x0), 1/x0),
sin(x0)
−→
(sin(x0), cos(x0)),
cos(x0)
−→
(cos(x0), −sin(x0)),
sqr(x0)
−→
(sqr(x0), 2x0),
and so on.
(9.2.3)

318
9 Sample Applications
Now the operations in the expression are applied following the rules (9.2.2) of
differentiation arithmetic.
The result is the function-derivative-value-pair (f(x0),
f′(x0)) of the function f at the point x0.
Example 9.2. For the function f(x) = 25(x −1)/(x2 + 1) the function value and
the value of the ﬁrst derivative are to be computed at the point x0 = 2. Applying the
substitutions (9.2.3) and the rules (9.2.2) we obtain
(f(2), f′(2)) = (25, 0)((2, 1) −(1, 0))
(2, 1)(2, 1) + (1, 0)
= (25, 0)(1, 1)
(4, 4) + (1, 0) = (25, 25)
(5, 4)
= (5, 1).
Thus f(2) = 5 and f′(2) = 1.
If in the arithmetic expression for the function f(x) elementary functions occur in
composed form, the chain rule has to be applied, for instance
exp(u(x0))
−→
(exp(u(x0)), exp(u(x0)) · u′(x0)) = (exp u, u′ exp u),
sin(u(x0))
−→
(sin(u(x0)), cos(u(x0)) · u′(x0)) = (sin u, u′ cos u),
and so on.
Example 9.3. For the function f(x) = exp(sin(x)) the value and the value of the ﬁrst
derivative are to be computed for x0 = π. Applying the above rules we obtain
(f(π), f′(π)) = (exp(sin(π)), exp(sin(π)) · cos(π))
= (exp(0), −exp(0)) = (1, −1).
Thus f(π) = 1 and f′(π) = −1.
Differentiation arithmetic is often called automatic differentiation or algorithmic
differentiation. All operations are performed on numbers. A computer can easily and
safely execute these operations though people cannot.
Automatic differentiation is not restricted to real functions which are deﬁned by
an arithmetic expression. Any real algorithm in essence evaluates a real expression
or the value of one or several real functions. Substituting for all constants, vari-
ables and elementary functions their function-derivative-value-pair, and performing
all arithmetic operations by differentiation arithmetic, will compute simultaneously
the function-derivative-value-pair of the result. Large program packages have been
developed which do just this, in particular for problems in higher dimensions.
Automatic differentiation or differentiation arithmetic simply uses the arithmetic
expression or the algorithm for the function. A formal arithmetic expression or algo-
rithm for the derivative does not explicitly occur. Of course an arithmetic expression
or algorithm for the derivative is evaluated indirectly. However, this expression re-
mains hidden. It is evaluated by the rules of differentiation arithmetic. Similarly if
differentiation arithmetic is performed for a ﬁxed interval X0 instead of for a real

9.2 Differentiation Arithmetic, Enclosures of Derivatives
319
point x0, an enclosure of the range of function values and an enclosure of the range of
values of the derivative over that interval X0 are computed simultaneously. Thus, for
instance, neither the Newton method nor the interval Newton method requires that the
user provides a formal expression for the derivatives. The derivative or an enclosure
for it is computed just by use of the expression for the function itself.
Automatic differentiation allows many generalizations which altogether would ﬁll
a thick book. We mention only a few of these.
If the value or an enclosure of the second derivative is needed one would use triples
instead of pairs and extend the rules (9.2.2) for the third component by corresponding
rules: u′′ + v′′, u′′ −v′′, uv′′ + 2u′v′ + vu′′, and so on. In the arithmetic expression
a constant c would now have to be replaced by the triple (c, 0, 0), the variable x by
(x, 1, 0) and the elementary functions also by a triple with the second derivative as the
third component.
Another generalization is Taylor arithmetic. It works with tuples where the ﬁrst
component represents the function value and the following components represent the
successive Taylor coefﬁcients. The remainder term of an integration routine for a
deﬁnite integral or for an initial value problem of an ordinary differential equation
usually contains a derivative of higher order. Interval Taylor arithmetic can be used to
compute a safe enclosure of the remainder term over an interval. This enclosure can
serve as an indicator for automatic step size control.
The following formulas describe the trapezoidal rule process if a constant step size
is used:
6 xi+1
xi
f(x)dx = h
2 (f(xi) + f(xi+1)) −h3
12f′′(ξi),
ξi ∈[xi, xi+1].
6 b
a
f(x)dx = h
n

i=0
′f(xi) −h3
12
n−1

i=0
f′′(ξi),
ξi ∈[xi, xi+1].
6 b
a
f(x)dx ∈h
n

i=0
′f(xi) −h3
12
n−1

i=0
f′′([xi, xi+1]).
In these formulas a dash after the sigma symbol indicates as usual that the ﬁrst and
the last summand have to be halved.
The following two deﬁnite integrals have been computed with Romberg’s extrapo-
lation method:
f1(x) = 2xex2 sin(ex2),
I1 =
6 2
0
f1(x)dx,

320
9 Sample Applications
and
f2(x) =
1
0.12 + (3x −1)2 −
1
0.12 + (3x −4)2
+
1
0.12 + (3x −7)2 −
1
0.12 + (3x −10)2 ,
I2 =
6 4
0
f2(x)dx.
In this case the error term is expressed by the Euler–MacLaurin sum formula. An
adaptive step size control by the error term concentrates the work to those areas where
the function is steep or where it oscillates severely. This reduces the execution time.
The following bounds are obtained in double precision ﬂoating-point arithmetic:
I1 ∈0.9109640392659329
8,
I2 ∈−0.1519639422329306
5.
See [5, 277, 332, 573, 651, 653].
Both integrals are difﬁcult to compute with conventional approximate methods.
Estimation of the error term for automatic step size control is difﬁcult with traditional
ﬂoating-point arithmetic. More details can be found in the literature [5, 170, 171, 172,
173, 277, 278, 279, 280, 476, 477, 478, 479, 480, 482, 570, 571, 572, 573].
In [388] R. Lohner has developed a program package AWA which computes con-
tinuous upper and lower bounds for the solution of initial value problems of nonlinear
systems of differential equations. The package carries its own step size control. It can
be applied to boundary and eigenvalue problems of ordinary differential equations as
well. In these cases the existence and uniqueness of the solution are not known a
priori. These properties are automatically veriﬁed by the enclosure algorithm, i.e., by
the computer. We brieﬂy sketch the method:
It considers nonlinear systems of ordinary differential equations of the ﬁrst order.
The right hand side of the differential equation is developed into a Taylor polynomial
with remainder term by automatic differentiation. Now the remainder term is evalu-
ated for an appropriate step size. Since an enclosure of the solution has to be com-
puted, the computation has to be done in interval arithmetic. In contrast to numerical
quadrature, for a differential equation, the remainder term contains the unknown func-
tion as well as the independent variable. To compute an enclosure of the remainder
term in interval arithmetic, therefore, one ﬁrst needs a safe enclosure of the unknown
function for the particular step of integration. This seems to be a vicious circle. Yet
the circle can be opened. First a rough enclosure of the unknown function in the inte-
gration interval is computed. For that purpose the differential equation is transformed
into an equivalent integral equation. Now the integrand contains the unknown func-
tion. In order to enclose the integral within safe bounds by interval arithmetic one

9.2 Differentiation Arithmetic, Enclosures of Derivatives
321
-200
-100
0
100
200
0
0.5
1
1.5
2
f(x) = 2xex2 sin(ex2)
-100
-50
0
50
100
0
0.5
1
1.5
2
2.5
3
3.5
4
f(x) =
1
a2+(3x−1)2 −
1
a2+(3x−4)2 +
1
a2+(3x−7)2 −
1
a2+(3x−10)2,
a = 0.1
Figure 9.5. Veriﬁed Romberg integration.

322
9 Sample Applications
again needs an enclosure of the unknown function in the whole integration interval.
In the simplest case such an enclosure is obtained by estimation. Since the differential
equation fulﬁls a Lipschitz condition there is always such an enclosure. To keep this
estimated enclosure small it may be that the step size has to be reduced. This ﬁrst, still
estimated initial enclosure is now made safe by Banach’s ﬁxed-point principle. This
means that with this ﬁrst initial enclosure the right hand side of the integral equation
is evaluated in interval arithmetic and it is checked for the expected enclosure. If the
check fails the step size has to be reduced and/or the estimated initial enclosure has to
be changed. As soon as an enclosure is obtained one has a safe initial enclosure of the
unknown solution in the integration interval. With it the remainder term of the Taylor
polynomial can be evaluated. The remainder term is now small of higher order. So a
much better enclosure of the unknown solution in the integration interval is obtained
by a polynomial with interval coefﬁcients. Continuation of this method over several
such steps causes the well-known wrapping effect. It is controlled by an intricate use
of local coordinates.
Boundary and eigenvalue problems of ordinary differential equations are trans-
formed into initial value problems by shooting methods. Then the algorithm also
proves existence and uniqueness of the solution.
These methods for validated computation of the solution of ordinary differential
equations are, in general, more expensive in computing time than ordinary approxi-
mating methods. However, the additional cost is justiﬁed and often more than com-
pensated for overall. A single run on the computer sufﬁces to ﬁnd a safe solution.
Typical trial and error runs to verify the solution are not needed. In particular, critical
situations can more easily be detected and analysed. The method has successfully
been used to detect and enclose periodic solutions of differential equations, and to
prove the existence and uniqueness of the solution. It even has been successfully
applied to chaotic solutions of differential equations.
Lohner’s AWA program package can be obtained from:
http://webserver.iam.uni-karlsruhe.de/awa/.
See also [158, 332, 387, 388, 391, 437].
Another problem class where automatic differentiation has been very successfully
applied is global optimization. Global optimization has already been considered in
Section 9.1.4. The basic strategy there was subdivision of the given domain into
subintervals and elimination of those subintervals which cannot contain the global
minimum. Interval evaluation of the expression for the function for an interval delivers
a superset of the range of function values in that interval. Those subintervals for which
the lower bound of the interval evaluation of the function is greater than a safe function
value can be ignored as they cannot contain the global minimum.
If the given function is continuously twice differentiable, interval evaluations of
derivatives also can be used to eliminate subintervals from further treatment. Again,
we consider here only the one dimensional case.

9.2 Differentiation Arithmetic, Enclosures of Derivatives
323
If the ﬁrst derivative of the function is positive or negative across the entire interval,
the function is monotone in that interval and it cannot have a global minimum within
that interval. Automatic differentiation of the function for any subinterval delivers an
enclosure (an overestimation) of the range of values of the derivative of the function
in that subinterval. If this enclosure does not contain zero, the function is necessarily
monotone in that subinterval, which can therefore be ignored.
Also enclosures of the second derivative of the function in a subinterval can be used
to exclude subintervals from further treatment. An enclosure of the second derivative
of the function in an interval also can easily be obtained by automatic differentia-
tion. If the second derivative of the function is negative across the entire interval, the
function cannot have a minimum in that interval.
So if the interval evaluation of the second derivative of the function in a subinterval
delivers a negative interval, the function cannot have a minimum in that subinterval,
which can therefore be ignored.
Without going into further detail, we simply aver that Newton’s method can also
be used to exclude subintervals from futher treatment. Corresponding methods, algo-
rithms and programs are also available for problems of more than one dimension.
Basic to all these methods is the fundamental property of interval arithmetic: Eval-
uation of a function or derivative over an interval delivers an enclosure of its values
over a continuum of points, even of points which are not representable on the com-
puter.
In complex, rational, matrix or vector, interval, differentiation and Taylor arith-
metic and the like, the arithmetic itself is predeﬁned and can be hidden in the runtime
system of the compiler. The user calls the arithmetic operations by the usual operator
symbols. The desired arithmetic is activated by type speciﬁcation of the operands.
As an example the PASCAL-XSC program shown in Figure 9.6 computes and
prints enclosures of the 36th and the 40th Taylor coefﬁcient of the function
f(x) = exp

5000
sin (11 + (x/100)2) + 30

over the interval a = [1.001, 1.005].
First the interval a is read. Then it is expanded into the 41-tuple of its Taylor
coefﬁcients (a, 1, 0, 0, . . . , 0) which is kept in b. Then the expression for f(x) is
evaluated in interval Taylor arithmetic and enclosures of the 36th and the 40th Taylor
coefﬁcient over the interval a are printed.
Automatic differentiation develops its full power in differentiable functions of sev-
eral real variables. For instance, values or enclosures of the gradient
gradf = ( ∂f
∂x1
, ∂f
∂x2
, . . . , ∂f
∂xn
)
of a function f : Rn →R or the Jacobian or Hessian matrix can be computed directly
from the expression for the function f. No formal expressions for the derivatives are

324
9 Sample Applications
program sample;
use itaylor;
function f(x:
itaylor):
itaylor[lb(x)..ub(x)];
begin f := exp(5000/(sin(11+sqr(x/100))+30));
end;
var a:
interval; b, fb:
itaylor[0..40];
begin
read(a);
expand(a,b);
fb := f(b);
writeln (’36th Taylor coefficient:
’, fb[36]);
writeln (’40th Taylor coefficient:
’, fb[40]);
end.
Test results for a = [1.001, 1.005]
36th Taylor coefficient: [-2.4139E+002, -2.4137E+002]
40th Taylor coefficient: [ 1.0759E-006, 1.0760E-006]
Figure 9.6. Computation of enclosures of Taylor coefﬁcients.
needed. A special mode, the so-called reverse mode, allows a considerable acceler-
ation for many algorithms of automatic differentiation. In the particular case of the
computation of the gradient the following inequality can be shown to hold:
A(f, ▽f) ≤5A(f).
Here A(f, ▽f) denotes the number of operations for the computation of the gra-
dient including the function evaluation, and A(f) the number of operations for the
function evaluation, i.e., the number A(f, ▽f) of operations for the computation of
the gradient and the function differs from the number A(f) of operations for the func-
tion evaluation only by a constant factor 5. It is independent of n! For more details
see [170, 171, 172, 173, 174, 476, 477, 478, 479, 480, 481].
9.3
The Interval Newton Method
Traditionally Newton’s method is used to compute an approximation of a zero of a
nonlinear real function f(x), i.e., to compute a solution of the equation
f(x) = 0.
(9.3.1)
The method approximates the function f(x) in the neighborhood of an initial value
x0 by the linear function (the tangent)
t(x) = f(x0) + f′(x0)(x −x0)
(9.3.2)

9.3 The Interval Newton Method
325
the zero of which can easily be calculated by
x1 := x0 −f(x0)
f′(x0).
(9.3.3)
x1 is used as new approximation for the zero of (9.3.1). Continuation of this method
leads to the general iteration scheme:
xν+1 := xν −f(xν)
f′(xν), ν = 0, 1, 2, . . . .
(9.3.4)
It is well known that if f(x) has a single zero x∗in an interval X and f(x) is twice
continuously differentiable, then the sequence
x0, x1, x2, . . . , xν, . . .
converges quadratically towards x∗if x0 is sufﬁciently close to x∗. If the latter condi-
tion does not hold the method may well fail.
The interval version of Newton’s method computes an enclosure of the zero x∗of a
continuously differentiable function f(x) in the interval X by the following iteration
scheme:
Xν+1 := (m(Xν) −f(m(Xν))
F ′(Xν) ) ∩Xν,
ν = 0, 1, 2, . . . ,
(9.3.5)
with X0 = X. Here F ′(Xν) is the interval evaluation of the ﬁrst derivative f′(x)
of f over the interval Xν and m(Xν) is the midpoint of the interval Xν. Instead of
m(Xν) another point within Xν could be chosen. In conventional interval arithmetic,
where division A/B is only deﬁned if 0 /∈B, the interval Newton method can only
be applied if 0 /∈F ′(X0). This guarantees that f(x) has only a single zero in X0.
In contrast to (9.3.4), the method (9.3.5) must always converge. Because of the
intersection with Xν the sequence
X0 ⊇X1 ⊇X2 ⊇. . .
(9.3.6)
is bounded. It can be shown that under natural conditions on the function f the se-
quence converges quadratically to x∗[29, 444].
The operator
N(X) := x −f(x)
F ′(X),
x ∈X ∈IR,
(9.3.7)
is called the interval Newton operator. It has the following properties:
I. If N(X) ⊆X, then f(x) has exactly one zero x∗in X.
II. If N(X) ∩X = ∅, then f(x) has no zero in X.

326
9 Sample Applications
X1 = N(X) ∩X
= [x1, n2]
x1
x2
n2
n1
x
f(x)
x
X
N(X)
f(x)
Figure 9.7. Geometric interpretation of the interval Newton method.
Thus, N(X) can be used to prove the existence or absence of a zero x∗of f(x) in
X. Since if there is a zero x∗in X the sequence (9.3.5), (9.3.6) converges, if there is
no such zero N(X) ∩X = ∅must occur in (9.3.6).
The interval version of Newton’s method (9.3.5) can also be derived via the mean
value theorem. If f(x) is continuously differentiable and has a single zero x∗in the
interval X, and f′(x) ̸= 0 for all x ∈X, then
f(x) = f(x∗) + f′(ξ)(x −x∗) for all x ∈X and some ξ between x and x∗.
Since f(x∗) = 0 and f′(ξ) ̸= 0 this leads to
x∗= x −f(x)
f′(ξ).
If F ′(X) denotes the interval evaluation of f′(x) over the interval X, we have f′(ξ) ∈
F ′(X) and therefore
x∗= x −f(x)
f′(ξ) ∈x −f(x)
F ′(X) = N(X) for all x ∈X,
i.e., x∗∈N(X) and thus
x∗∈(x −f(x)
F ′(X)) ∩X = N(X) ∩X.
Now we obtain by setting X0 := X and x = m(X0)
X1 := (m(X0) −f(m(X0))
F ′(X0) ) ∩X0,
and by continuation (9.3.5).

9.4 The Extended Interval Newton Method
327
In close similarity to the conventional Newton method the interval Newton method
also allows some geometric interpretation. For that purpose let be X = [x1, x2] and
N(X) = [n1, n2]. F ′(X) is the interval evaluation of f′(x) over the interval X. As
such it is a superset of all the slopes of tangents that can occur in X. The conventional
Newton method (9.3.3) computes the zero of the tangent of f(x) in (x0, f(x0)). Sim-
ilarly N(X) is the interval of all zeros of straight lines through (x, f(x)) with slopes
within F ′(X), see Figure 9.7. Of course, f′(x) ∈F ′(X).
The straight line through f(x) with the least slope within F ′(X) cuts the real axis
at n1, and the one with the greatest slope at n2. Thus the Interval Newton Operator
N(X) computes the interval [n1, n2] which in the sketch of Figure 9.7 is situated
on the left hand side of x. The intersection of N(X) with X then delivers the new
interval X1. In the example in Figure 9.7, X1 = [x1, n2].
Newton’s method allows some visual interpretation. From the point (x, f(x)) the
conventional Newton method sends a ray of light along the tangent. The search is
continued at the intersection of this ray with the x-axis. The interval Newton method
sends a set of rays like a ﬂoodlight from the point (x, f(x)) to the x-axis. This set
includes the directions of all tangents that occur in the entire interval X. The interval
N(X) comprises all cuts of these rays with the x-axis.
It is a fascinating discovery that the interval Newton method can be extended so
that it can be used to compute all zeros of a real function in a given interval. The
basic idea of this extension is quite old [12]. Many scientists have worked on details
of how to use this method, of how to deﬁne the necessary arithmetic operations, and
of how to bring them to the computer. But inconsistencies have occurred again and
again. However, understanding has now reached a point which allows a consistent
realization of the method and of the necessary arithmetic. Newton’s method reaches
its ultimate elegance and power in the extended interval Newton method which we are
now going to discuss.
9.4
The Extended Interval Newton Method
The extended interval Newton method can be used to compute enclosures of all the ze-
ros of a continuously differentiable function f(x) in a given interval X. The iteration
scheme is identical to the one deﬁned by (9.3.5) in Section 9.3:
Xν+1 := (m(Xν) −f(m(Xν))
F ′(Xν) ) ∩Xν = N(Xν) ∩Xν,
ν = 0, 1, 2, . . . ,
with X0 := X. Here again F ′(Xν) is the interval evaluation of the ﬁrst derivative
f′(x) of the function f over the interval Xν, and m(Xν) is any point within Xν,
the midpoint for example. If f(x) has more than one zero in X, then the derivative
f′(x) has at least one zero (horizontal tangent of f(x)) in X also, and the interval

328
9 Sample Applications
evaluation F ′(X) of f′(x) contains zero. Thus extended interval arithmetic has to be
used to execute the Newton operator
N(X) = x −f(x)
F ′(X), with x ∈X.
As shown by Tables 4.8 and 4.9 the result is no longer an interval of IR. It is an
element of the power set PR which, in general, stretches continuously to −∞or +∞
or both. The intersection N(X)∩X with the ﬁnite interval X then produces a ﬁnite set
again. It may consist of a ﬁnite interval of IR, or of two separate such intervals, or of
the empty set. These sets are now the starting values for the next iteration. This means
that where two separate intervals have occurred, the iteration has to be continued with
two different starting values. This situation can occur repeatedly. On a sequential
computer where only one iteration can be performed at a time all intervals which are
not yet dealt with are collected in a list. This list then is treated sequentially. If more
than one processor is available different subintervals can be dealt with in parallel.
Again, we illustrate this process by a simple example. The starting interval is de-
noted by X = [x1, x2] and the result of the Newton operator by N = [n1, n2]. See
Figure 9.8.
x
x2
x1
n2
n1
x
f(x)
f(x)
X
N(X)
N(X)
= [x1, n2] ∪[n1, x2]
= N(X) ∩X
X1
Figure 9.8.
Geometric interpretation of the extended interval Newton
method.
Now F ′(X) is again a superset of all slopes of tangents of f(x) in the interval
X = [x1, x2]. But now 0 ∈F ′(X). N(X) again is the set of zeros of straight
lines through (x, f(x)) with slopes within F ′(X). Let be F ′(X) = [s1, s2]. Since
0 ∈F ′(X) we have s1 ≤0 and s2 ≥0. The straight lines through (x, f(x)) with the
slopes s1 and s2 cut the real axis at n1 and n2. Thus the Newton operator produces
the set
N(X) = (−∞, n2] ∪[n1, +∞).

9.5 Veriﬁed Solution of Systems of Linear Equations
329
The ﬂoodlight is now shining in two directions. Intersection with the original set X
(the former iterate) delivers the set
X1 = N(X) ∩X = [x1, n2] ∪[n1, x2]
consisting of two ﬁnite intervals of IR. From this point the iteration has to be contin-
ued with the two starting intervals [x1, n2] and [n1, x2].
9.5
Veriﬁed Solution of Systems of Linear Equations
Systems of linear equations play a central role in numerical analysis. Such systems
are often very large. An avalanche of numbers is produced if a linear system of equa-
tions with one million unknowns is “solved” on a computer by a direct method. Every
ﬂoating-point operation is potentially in error. This brings to the fore the question of
whether the computed result really solves the problem or of how many of the com-
puted digits are correct. Based on rigorous mathematics a validated inclusion of the
solution can be computed which answers the question.
Various methods of so-called automatic result veriﬁcation or validation by the com-
puter for systems of linear equations have been developed over recent years. Here we
only discuss a few basic ideas and techniques. Interval arithmetic and mathematical
ﬁxed-point theorems are applied. Basic is the use of the Brouwer ﬁxed-point theorem
which is now stated.
Theorem 9.4. A continuous mapping g : Rn →Rn which maps a non-empty, convex,
closed and bounded set X ⊆Rn into itself, i.e.,

x∈X
g(x) ∈X,
has at least one ﬁxed-point x∗∈X.
■
A few concepts of interval analysis are needed. For an interval X = [x1, x2] ∈IR
the diameter d(X) and the absolute value |X| are deﬁned by
d(X) := x2 −x1
(diameter),
|X| := max{|x| | x ∈X}
(absolute value).
The distance between two intervals X = [x1, x2], Y = [y1, y2] ∈IR is deﬁned by
the Hausdorff metric
q(X, Y ) := max{|x1 −y1|, |x2 −y2|}
(Hausdorff metric).

330
9 Sample Applications
Convergence of a sequence of intervals Xi = [xi1, xi2] ∈IR, i = 1, 2, 3, . . . , is
deﬁned as usual in a metric space:
lim
i→∞Xi = X = [x1, x2] :⇔lim
i→∞q(Xi, X) = 0
:⇔

lim
i→∞xi1 = x1 ∧lim
i→∞xi2 = x2

.
For vectors and matrices the diameter, absolute value and distance are deﬁned com-
ponentwise. An interval vector X ∈VnIR is an n-dimensional box, an interval in
each coordinate direction.
Now we consider a system of linear equations
Ax = b with A ∈Rn×n, b ∈Rn.
(9.5.1)
A solution x∗∈Rn is a zero of the function
g(x) := Ax −b.
(9.5.2)
Linear systems of equations are often solved iteratively. The following theorem
gives a necessary and sufﬁcient criterion for convergence of an iterative scheme using
intervals. The proof is given in [28, 29].
Theorem 9.5. An iterative method
Xi+1 := BXi + c, B ∈Rn×n, c ∈Rn, Xi ∈VnIR, i = 0, 1, 2, . . . ,
converges to the unique solution of the equation
x = Bx + c
for every X0 ∈VnIR if and only if ρ(|B|) < 1. Here ρ(|B|) denotes the spectral
radius of the matrix |B|.
■
Now we derive an iterative method for the solution of (9.5.1). Newton’s method
applied to (9.5.2) would lead to
xi+1 := xi −A−1(Axi −b),
where A−1 denotes the inverse of the matrix A. Since A−1 is unknown we replace it
by a suitably chosen matrix R, an approximate inverse R ≈A−1 of A, for instance.
This leads to
xi+1 := (I −RA)xi + Rb.
In the relevant literature the operator
g(X) := (I −RA)X + Rb,
X ∈VnIR,
(9.5.3)
is called the Krawczyk-operator, see [336].

9.5 Veriﬁed Solution of Systems of Linear Equations
331
In the expression of the right hand side of the assignment in (9.5.3) all operations
are evaluated in interval arithmetic. The variable X occurs only once in the expres-
sion. Thus no overestimation appears if it is evaluated for an interval vector X.
By Theorem 9.5 an iterative method with the Krawczyk-operator
Xi+1 := (I −RA)Xi + Rb,
X0 ∈VnIR,
(9.5.4)
converges for every initial interval vector X0 to the unique solution of (9.5.1) if and
only if ρ(|I −RA|) < 1.
The necessary and sufﬁcient criterion for convergence of (9.5.4) is hard to check.
Computation of the eigenvalues of a matrix is of complexity similar to that of the
solution of the linear system. The following theorem gives criteria for a convergence
of (9.5.4) which can be checked very easily. In this theorem a comparison relation
x < y for vectors x, y ∈Rn is used. It is deﬁned componentwise.
Theorem 9.6. (a) The iterative method (9.5.4) converges to a unique solution x∗of
(9.5.1) if for an initial interval vector X = X0 ∈VnIR the Krawczyk-operator
strictly reduces the diameter
d(X1) := d(g(X)) < d(X).
(9.5.5)
Then the matrices A and R in (9.5.3) are not singular, i.e., the linear system (9.5.1)
has a unique solution.
(b) If in addition to (9.5.5) the Krawczyk-operator maps the interval X0 into itself,
i.e., if
X1 := g(X0) ⊆X0
(9.5.6)
then x∗∈X0 and the iteration (9.5.4) converges to x∗by a nested sequence
X0 ⊇X1 ⊇X2 ⊇. . .
with x∗∈Xi for all i = 0, 1, 2, . . . .
Proof. (a) d(g(X)) = d((I −RA)X + Rb) = d((I −RA)X) = |I −RA|d(X)
⇒
(9.5.5) |I −RA|d(X) < d(X).
(9.5.7)
X is an interval vector of dimension n, i.e., an n-dimensional box. By (9.5.5) the
components di of the diameter vector d(X) are all positive, di > 0, i = 1(1)n.
Multiplication of d(X) by the diagonal matrix
D := diag(1/di) :=
⎛
⎜
⎜
⎜
⎜
⎝
1/d1
0
· · ·
0
0
1/d2
...
...
...
...
...
0
0
· · ·
0
1/dn
⎞
⎟
⎟
⎟
⎟
⎠

332
9 Sample Applications
leads to the vector u := D · d(X), all components of which are 1. By expansion of
the left hand side of (9.5.7) we now obtain the inequality
D · |I −RA| · D−1u < u.
(9.5.8)
The components of the vector on the left hand side of the inequality (9.5.8) are the
row sums of the matrix D|I −RA|D−1. Their maximum is a norm, i.e., we have
||D · |I −RA| · D−1|| < 1,
and therefore
ρ(D · |I −RA| · D−1) < 1.
Since similar matrices have the same eigenvalues we obtain
ρ(|I −RA|) < 1
and by the theorem of Perron and Frobenius [597, 598]
ρ(I −RA) ≤ρ(|I −RA|) < 1.
Therefore the matrix I −(I −RA) = RA is not singular, nor are A and R. The
iteration (9.5.4) converges to the unique solution of (9.5.1).
(b) By the Brouwer ﬁxed-point theorem the mapping g(x) has a ﬁxed-point x∗in X0.
The ﬁxed-point is unique by (a) and the iteration (9.5.4) converges to it. By (9.5.4)
x∗∈Xi, for all i = 0, 1, 2, . . . : With (9.5.6) we obtain by induction and the inclusion
isotonicity of interval arithmetic
Xi ⊆Xi−1 ⇒Xi+1 := g(Xi) ⊆g(Xi−1) = Xi.
■
The condition (9.5.5) of Theorem 9.6(a) can easily be checked by the computer.
However, under this condition the solution x∗of the linear system can lie outside the
iterates Xi produced by the iteration (9.5.4). In this case both bounds of the iterates
Xi converge from outside to the solution x∗. An enclosure x∗∈Xi is not obtained.
From the practical point of view it is desirable that the iteration delivers bounds,
i.e., an enclosure of the solution x∗. This is achieved by the additional condition
(9.5.6) of Theorem 9.6(b).
In the relevant literature instead of (9.5.5), d(g(X)) < d(X), and (9.5.6), g(X) ⊆
X, the criterion
g(X) ⊆
◦X
(9.5.9)
is often given and used. In (9.5.9)
◦X denotes the interior of X (the bounds are ex-
cluded), [503]. (9.5.5) and (9.5.6) hold under the condition (9.5.9). The opposite,
however, is not true.

9.5 Veriﬁed Solution of Systems of Linear Equations
333
(9.5.9) guarantees convergence of the iteration and it delivers an enclosure of the
solution x∗. (9.5.9) can easily be checked on the computer. For two interval vectors
X = [x1, x2] and Y = [y1, y2] we have
X ⊆
◦Y
⇔
y1 < x1 ∧x2 < y2.
The criteria (9.5.6) and (9.5.9) can only apply if x∗∈X0. This property can be
achieved by a process which is known as ϵ-inﬂation.
First an approximation ˜x for the solution x∗is computed by some ordinary method,
for example by Gaussian elimination. This approximation is expanded by offsetting it
by a small value ϵ in each component direction. This results in an n-dimensional cube
which has the computed approximation as its midpoint. This cube is now chosen as
the starting value X0 for the iteration (9.5.4). If R is a good approximation of A−1
then the criterion for convergence ρ(|I −RA|) < 1 usually holds. Then in most cases
in practice the condition (9.5.6) or (9.5.9) holds already after one iteration. If it does
not occur another ϵ-inﬂation is applied. Only rarely are more iterations needed to
verify (9.5.6) or (9.5.9), and then only two or three. Rump has shown in [515] that the
iteration with ϵ-inﬂation reaches the condition (9.5.9) if and only if ρ(|I −RA|) < 1.
The methods that just have been described can be further improved. Let us assume
that an approximate solution ˜x of the linear system has already been computed by a
favorite algorithm. Then an interval inclusion E of the error e = x∗−˜x usually leads
to a very good inclusion of the solution x∗:
e = x∗−˜x ∈E ⇒x∗∈˜x + E.
(9.5.10)
It is well known that the error e is a solution of a linear system with the same matrix
and the defect or residual r of the approximation ˜x as the right hand side:
Ae = r.
(9.5.11)
Here the residual r is deﬁned by
r := b −A˜x.
(9.5.12)
Thus an inclusion of the error can be obtained by the interval iteration scheme:
Ei+1 := (I −RA)Ei + Rr.
(9.5.13)
See [503].
When used on the computer (9.5.12) and (9.5.13) are very sensitive to rounding.
If ˜x in (9.5.12) is already a good approximation of the solution x∗, then A˜x is close
to b and the subtraction b −A˜x causes cancellation in conventional ﬂoating-point
arithmetic. For an imprecisely known r, determination of the error e from (9.5.11)
could be worthless.

334
9 Sample Applications
Similarly, if R in (9.5.13) and in (9.5.4) is a good approximation of A−1, then RA
is close to I and the subtraction I−RA in (9.5.13) causes cancellation in conventional
ﬂoating-point arithmetic. In such a case the iteration matrix in (9.5.13) is known only
approximately so that the computed result of the iteration (9.5.13) and (9.5.4) could
be worthless.
The situation is essentially improved if the expressions b −A˜x in the residual
(9.5.12) and I −RA in the iteration (9.5.13) and (9.5.4) are computed to full ac-
curacy by the exact scalar product. The result is then rounded to the least including
interval. This makes the residual correction process work very well.
On the computer the iteration (9.5.13) is then carried out by the following algo-
rithm:
(i) Compute an approximate inverse R of A using your favorite algorithm.
(ii)
˜X := ♦(R · b)
B := ♦(I −R · A)
r := ♦(b −A · ˜X)
E := ♦(R · r)
Z := E
i
:= 0
(iii) repeat Y := E ♦· [1 −ϵ, 1 + ϵ]
i := i + 1
E := ♦(B · Y + Z)
until (E ⊆
◦Y ) or i = 10
(iv) if (E ⊆
◦Y ) then {It has been veriﬁed that a unique solution
x∗of Ax = b exists and x∗∈˜X + E}
else {Veriﬁcation failed, A is probably ill-conditioned}.
In the else case a more powerful algorithm could be called (see Rump-operator). In
the steps 2 and 3 of the algorithm above all operations +, −, · are to be executed
exactly as real number operations.
What has been described so far is known as result veriﬁcation. For an approxima-
tion ˜x of the solution x∗of a linear system an enclosure of the solution is computed
and existence and uniqueness of the solution within the computed bounds are veriﬁed.
In the algorithm all operations and expressions are to be executed exactly in complete
arithmetic.
It is possible that the veriﬁcation step fails to produce an enclosure after let’s say 10
iterations with ϵ-inﬂation. This can happen in the case of an extremely ill conditioned
linear system. Then the matrix R in (9.5.4) and in (9.5.13) may not be good enough
to allow a successful iteration. This situation is recognized by the computer in step
4 of the algorithm above. The algorithm then automatically calls a more powerful
operator which in almost all cases leads to a successful inclusion. This procedure has

9.5 Veriﬁed Solution of Systems of Linear Equations
335
been proposed, implemented, and successfully applied by S. M. Rump, [503]. We
brieﬂy sketch it here.
For the Krawczyk-operator the matrix RA has already been computed. Now its
inverse (RA)−1 is computed in ﬂoating-point arithmetic. Then the product (RA)−1·R
is computed with the exact scalar product. Theoretically, if real arithmetic could be
used, the multiplication (RA)−1 · R would eliminate the matrix R and lead to the
ideal situation A−1. But it can be expected that the product (RA)−1 · R is closer to
A−1 than the original matrix R in the Krawczyk-operator. The exact scalar product
computes the product (RA)−1 · R to full accuracy. From that a portion can be read to
two or three or more fold precision as a long real matrix, for instance, (RA)−1 · R ≈
R1+R2+R3+R4. In many cases the ﬁrst two approximations (RA)−1·R ≈R1+R2
sufﬁce. This leads to the iteration scheme
Xi+1 := (I −R1A −R2A)Xi + R1b + R2b.
This scheme is called the Rump method and the operator
h(X) := (I −R1A −R2A)X + R1b + R2b
or its alternative for the error E is called the Rump-operator.
If executed on the computer each component in the expression I −R1A −R2A
is evaluated as a single exact scalar product. The result then is rounded to the least
including interval. The expression on the right hand side of the Rump-operator is then
evaluated in interval arithmetic.
It is essential to understand that even in the Rump-operator all operations, apart
from the exact scalar product and in particular the computation of the inverse (RA)−1,
are performed in conventional ﬂoating-point arithmetic and thus are very fast. If the
exact scalar product is implemented in hardware it is faster than a conventional scalar
product in ﬂoating-point arithmetic.
Rump has applied the method to the Hilbert matrix of dimension 20. Its condition
number is of the magnitude 1028. To present the matrix on the computer with no
rounding errors the components are multiplied by a common factor to obtain integer
entries. Rump has shown in [508] that with an approximation of (RA)−1 · R by a two
fold sum the inverse of the Hilbert matrix can be computed to least bit accuracy.
In a recent paper by Oishi, Tanabe, Ogita, and Rump [459] the authors prove the
convergence of variants of this method even for extremely ill-conditioned problems.
They show that matrices with a condition number of about 10100 can successfully be
inverted if the product (RA)−1 is computed to k fold precision (with k ≈10).
The linear system solver discussed in this section including the extension by the
Rump-operator is an essential ingredient of many problem solving routines with au-
tomatic result veriﬁcation in numerical analysis. It can be extended to problems with
interval coefﬁcients and complex coefﬁcients, [508]. Also, generalizations are avail-
able for systems of nonlinear equations, [39, 522].

336
9 Sample Applications
The whole process described in this section delivers highly accurate bounds for
the solution and simultaneously proves the existence and uniqueness of the solution
within the computed bounds. Thus the computer is no longer merely a fast calculating
tool, but a scientiﬁc mathematical instrument. This makes the computer much more
user friendly. Of course, often the computer has to do more work to obtain veriﬁed
results. But the mathematical certainty should be worth it. Computing that is contin-
ually and greatly speeded up makes this step necessary. It is the very speed that calls
conventional ﬂoating-point computing into question.
9.6
Accurate Evaluation of Arithmetic Expressions
Arithmetic expressions are basic ingredients of all numerical software. The need to
have mathematical models which approximate the real world leads to more and more
complicated formulas. Many of these formulas used in modern engineering are not
created by hand but by symbolic manipulations on a computer. Numerical evaluation
of these formulas is usually done in ﬂoating-point arithmetic. This is a speedy process
since ﬂoating-point arithmetic is supported by fast hardware. However, the computed
result may differ quite a bit from the correct result of the expression without the user
being informed of it. It is important, therefore, to have methods which deliver highly
accurate bounds for the value of the expression.
In this section we develop a method which evaluates an arithmetic expression with
guaranteed high (maximum) accuracy. The method uses fast ﬂoating-point arithmetic
with directed roundings (interval arithmetic) and a fast and exact scalar product. If
all these operations are reasonably supported by the computer’s hardware, the com-
puting time of the method is of the same order as that for a conventional evaluation
of the expression in ﬂoating-point arithmetic. Should the conventional evaluation fail
completely additional computing time and storage is needed.
Occasionally a simple method of evaluating an arithmetic expression to high ac-
curacy is recommended. It uses multiple precision interval arithmetic (see the next
section). In the ﬁrst step the expression is evaluated in any fast hardware supported
interval arithmetic. If the width of the result is larger than the desired accuracy, the
evaluation is repeated in doubled precision, in tripled precision, and so on, until the
desired accuracy is obtained.
The method has two disadvantages: Evaluation in a higher precision does not make
use of the work that was already done in lower precision, and the speed of the arith-
metic decreases drastically with increasing precision multiples.
9.6.1
Complete Expressions
There is a class of arithmetic expressions which can always be evaluated exactly.
These expressions have been called complete expressions in Section 8.7. Examples

9.6 Accurate Evaluation of Arithmetic Expressions
337
of such expressions are given in the previous section where in the case of a system
of linear equations the residual b −A · ˜x or the iteration matrix I −R · A had to be
computed to full accuracy. Disregarding the usual priority for the operations, every
component in these expressions is computed as a single exact scalar product. Thus
cancellation of digits is avoided. In Section 8.7 complete expressions are deﬁned on
the scalar, vector or matrix level.
We give two other examples for complete expressions. If A and B are matrices,
and a, b and c are vectors, then
a + A · b + B · c
is a complete expression. Or, if Ai and Bi, i = 1(1)4, are vectors or matrices, then
A1 · B1 + A2 · B2 + A3 · B3 + A4 · B4
is a complete expression. Sharp or accurate evaluation of a complete expression
means that the whole expression, i.e., each component of it, is evaluated as a single
exact scalar product.
Sharp evaluation of complete expressions is a basic feature in the programming
languages PASCAL-XSC, ACRITH-XSC, and Fortran-XSC. In these languages com-
plete expressions are enclosed in parentheses and preﬁxed by a #, the sharp symbol.
A rounding symbol can follow the sharp symbol. Complete expressions are evaluated
exactly by complete arithmetic. The full result is stored if no rounding is speciﬁed. It
is rounded to real, if the rounding symbol is < (down), > (up), or ∗(to nearest), and
it is rounded to an interval, if the rounding symbol is #.
Accurate evaluation of general arithmetic expressions is performed in three steps.
The ﬁrst step transforms the expression into a simple system of equations. The second
step evaluates this system in ﬂoating-point arithmetic. The third step then computes
sharp bounds for the residual and with this an enclosure of the error. The key operation
in the third step is an exact scalar product. If a desired accuracy of the result is not
obtained the third step is repeated once or several times.
9.6.2
Accurate Evaluation of Polynomials
The basic ideas for an accurate evaluation of expressions can be extended to polyno-
mials. Evaluating a polynomial can be very delicate, especially in the neighborhood
of a zero. Accurate evaluation of a polynomial is thus a crucial task for all zero ﬁnders.
We consider the polynomial
p(z) =
n

i=0
aizi,
where the ai, i = 1(1)n, are given ﬂoating-point numbers. The evaluation is done for
a ﬂoating-point number z = t. Horner’s scheme leads to:
p(t) = (. . . (ant + an−1)t + . . . + a1)t + a0.

338
9 Sample Applications
Now for each step in the Horner scheme we introduce an auxiliary variable xi,
i = n(−1)0 (starting with xn = an):
xn := an,
xn−1 := xn · t + an−1,
...
xi := xi+1 · t + ai,
...
x0 := x1 · t + a0.
This is a system of linear equations
A · x = b,
for the unknowns xi, i = 0(1)n, with a simple matrix A and the right hand side
b = (ai):
A =
⎛
⎜
⎜
⎜
⎝
1
0
0
−t
1
...
...
0
−t
1
⎞
⎟
⎟
⎟
⎠,
x =
⎛
⎜
⎜
⎜
⎝
xn
xn−1
...
x0
⎞
⎟
⎟
⎟
⎠,
b =
⎛
⎜
⎜
⎜
⎝
an
an−1
...
a0
⎞
⎟
⎟
⎟
⎠.
x0 is the value of the polynomial p(t).
This system could be solved with high accuracy and veriﬁcation of the result with
one of the methods for linear systems discussed in Section 9.5. However, these meth-
ods are designed for general matrices and do not take advantage of the simple structure
of the present problem. Furthermore, high accuracy is not needed for all components,
but only for the last one. Thus it is worthwhile to develop a special method for the
present problem.
For systems of linear equations the method of defect correction or iterative reﬁne-
ment works well if a fast and exact multiply and accumulate operation is used to
compute critical terms like the defect b −A · ˜x of an approximate solution ˜x. So we
use this method here also.
As a next step we compute an approximation x(0)
i
for each of the auxiliary variables
xi in ﬂoating-point arithmetic by forward substitution
x(0)
n := an,
x(0)
i
:= x(0)
i+1 · t + ai, i = n −1(−1)0.
This is just a ﬂoating-point evaluation of the polynomial by Horner’s scheme. Then
x(0)
0
≈p(t).

9.6 Accurate Evaluation of Arithmetic Expressions
339
If x∗denotes the exact solution and x(0) = (x(0)
i ) the approximate solution of the
linear system Ax = b, the error is
e := x∗−x(0).
The defect or the residual r of x(0) is deﬁned by
r := b −A · x(0).
It is well known that the error e is the solution of a system of linear equations with
the same matrix A and the residual r as the right hand side:
A · e = r.
If we compute an enclosure E of the error e, we have an enclosure of the solution
x∗:
e = x∗−x(0) ∈E ⇒x∗∈x(0) + E.
With the exact scalar product a sharp enclosure R(1) for the residual is obtained by
the complete expression
R(1) := ♦(b −Ax(0)).
Here an accurate evaluation of the residual by the exact scalar product is essential
to avoid catastrophic cancellation and overestimation of the residual.
Now a ﬁrst enclosure E(1) of the error e is obtained as solution of the equation
A · E(1) = R(1).
Because of the special form of the matrix A, E(1) can be computed by forward
substitution (Horner’s scheme) using interval arithmetic. If the components of R(1)
and E(1) are denoted by R(1)
i , E(1)
i , i = 0(1)n, respectively: R(1) = (R(1)
i ), E(1) =
(E(1)
i ), we obtain
E(1)
n
:= R(1)
n ,
E(1)
i
:= E(1)
i+1 · t + R(1)
i , i = n −1(−1)0.
By construction (and using the properties of interval arithmetic) we have
p(t) ∈x(0)
0
+ E(1)
0 .
If the diameter of the interval on the right hand side is not sufﬁciently small, the
residual iteration is continued:
Let x(1) := m(E(1)) be the midpoint of the error vector E(1). We use the vector
x(0) + x(1) as a new approximation of the solution of the linear system.

340
9 Sample Applications
At the beginning of the (k+1)th iteration we have k+1 vectors x(0), x(1), . . . , x(k),
the sum
k

j=0
x(j)
of which is an approximate solution of the linear system Ax = b. A sharp enclo-
sure R(k+1) of the residual of this approximation is now obtained by the complete
expression
R(k+1) := ♦
⎛
⎝b −A ·
k

j=1
x(j)
⎞
⎠∈IRn+1.
With this enclosure R(k+1) of the residual a corresponding new enclosure of the
error is now deﬁned by the linear system
A · E(k+1) = R(k+1).
The solution of this system is now computed by forward substitution in interval
arithmetic (Horner’s scheme). Thus we obtain by construction:
p(t) = x∗
0 ∈
k

j=0
x(j)
0
+ E(k+1)
0
.
The iteration is stopped when the desired accuracy is reached. It is shown in [86, 87,
368] that the sequence of interval vectors E(k) converges toward the zero vector as
k →∞, as k
j=0 x(j) approximates x∗. With this we have
p(t) = x∗
0 = lim
k→∞
⎛
⎝
k

j=0
x(j)
0
⎞
⎠
as an exact evaluation of the polynomial p(z) at the point z = t.
The rate of convergence of the method is linear. It is inversely proportional to the
condition number of the matrix A. In fact, the number of residual iterations necessary
to achieve maximum accuracy is a rough indicator for the condition number of A.
The method is also discussed and executable programs are given in [205, 206, 207].
See also [319]. We cite a few examples which are given in the literature:
Examples. (i) Evaluation of the polynomial
p(t) = (t −2)4 = t4 −8t3 + 24t2 −32t + 16
for t = 2.0001 delivers the veriﬁed inclusion of p(t):
[1.000000000008441E −016, 1.000000000008442E −016]
after two iterations. Horner scheme evaluation in ﬂoating-point arithmetic yields
−3.552713678800501E −015.

9.6 Accurate Evaluation of Arithmetic Expressions
341
(ii) Evaluation of the polynomial
p(t) = (1 −t)3 = −t3 + 3t2 −3t + 1
for t = 1.000005 delivers the veriﬁed inclusion of p(t):
[−1.250000000024568E −016, −1.250000000024566E −016]
after two iterations. Horner scheme evaluation in ﬂoating-point arithmetic yields
1.110223024625157E −016.
(iii) It is well known that evaluation of the power series is not a suitable method for
computing values of the exponential function for negative arguments. With the new
method evaluation is simple. With
p(t) =
90

k=0
tk
k!
we obtain for t = −20 the inclusion of p(t):
[2.06115362243E −9, 2.06115362244E −9]
whereas Horner scheme evaluation in ﬂoating-point arithmetic yields
1.188534E −4.
Here an arithmetic with 13 decimal digits was used.
Note: The coefﬁcients tk/k! are not exactly representable. Therefore, the compu-
tation was done for 90!p(t). To obtain an enclosure of exp(−20) an enclosure of the
91st summand would still have to be computed and added to the received interval.
Throughout this subsection it has been assumed that the coefﬁcients of the poly-
nomial are ﬂoating-point numbers. In [389] R. Lohner has extended the method to
polynomials with long interval coefﬁcients.
9.6.3
Arithmetic Expressions
For brevity in this section we only sketch the methods. They follow the scheme used
to obtain highly accurate enclosures for values of polynomials. Again iterative reﬁne-
ment using an exact scalar product is the basic mathematical tool.
We begin with the example
(a + b) · c −d/e,

342
9 Sample Applications
where a, b, c, d, e are ﬂoating-point numbers. Evaluation of the expression can be
performed by the following steps:
x1 := a,
x2 := x1 + b,
x3 := x2 · c,
x4 := d,
x5 := x4/e,
x6 := x3 −x5.
This is again a system of linear equations with a lower triangular matrix. An en-
closure of the value of the expression can be computed very similarly to the case of
polynomials.
But there are arithmetic expressions which lead to a system of nonlinear equations.
For example, the expression
(a + b)(c + d)
with ﬂoating-point numbers a, b, c, d leads to the system of nonlinear equations:
x1 := a,
x2 := x1 + b,
x3 := c,
x4 := x3 + d,
x5 := x2 · x4.
All such systems are of a special lower triangular form. Whenever a new auxiliary
variable is introduced, only those with lower indices appear on the right hand side.
If the resulting system is nonlinear it can be transformed into a linear system by
an automatic algebraic transformation process. Then iterative reﬁnement techniques
with an exact scalar product can be applied in a very similar way to the polynomial
case. See [86, 87].
Other methods directly use the system of nonlinear equations. These methods again
proceed in two steps:
(a) an approximation x(0) for the correct solution x∗is computed in ﬂoating-point
arithmetic by forward substitution.
(b) an enclosure E of the error e = x∗−x(0) is computed.
This leads to an enclosure for the solution:
x∗∈x(0) + E.

9.7 Multiple Precision Arithmetics
343
Formulas for E are obtained by use of the mean value theorem. They are evaluated
by the exact scalar product.
If the quality of the enclosure x∗∈x(0) + E is not sufﬁcient, the midpoint x(1) :=
m(E) is used to improve the approximation x(0) to x(0) + x(1). Then step (b) is
repeated and so on.
A detailed description and executable algorithms and programs can be found in
Chapter 8 of the books [205, 206, 207]. See also [175].
The methods have also been extended to matrix expressions and complex expres-
sions.
Example 9.7. The method which uses the nonlinear system of equations described in
this subsection works well even for extremely ill conditioned problems, for instance
in the example:
333.75 · b6 + a2 · (11 · a2 · b2 −b6 −121 · b4 −2) + 5.5 · b8 + a/(2 · b).
(9.6.1)
Evaluation of the expression in IEEE double precision ﬂoating-point arithmetic for
a = 77617.0 and b = 33096.0 delivers the value
1.172604.
The same value is obtained in double and extended precision on a /370 mainframe
in IBM ﬂoating-point arithmetic. The method sketched here delivers the enclosure
[−0.8273960599469, −0.8273960599467]
after three iterations.
Due to its developer the polynomial (9.6.1) is known as the Rump polynomial in
the literature.
9.7
Multiple Precision Arithmetics
With a fast and exact multiply and accumulate operation or scalar product, fast quadru-
ple and multiple precision arithmetics can easily be provided on the computer for real
as well as for interval data. A multiple precision number, for instance, is represented
as an array of ﬂoating-point numbers. The value of this number is the sum of its
components. It can be represented in the complete register as a long ﬁxed-point vari-
able. In this section we brieﬂy sketch the deﬁnition of the operations +, −, ·, /,
and the square root for multiple precision numbers and for multiple precision inter-
vals. Further information and PASCAL-XSC programs can be found in the literature
[332, 392].
On the basis of these operations for these multiple precision numbers and intervals,
algorithms for the elementary functions also can easily be provided. They have been

344
9 Sample Applications
provided and are available in the programming languages PASCAL-XSC, ACRITH-
XSC, and Fortran-XSC. The concept of function and operator overloading in these
languages makes application of multiple precision real and interval arithmetic very
simple.
We assume that multiple precision numbers and intervals are elements of special
data types which we call long real and long interval, respectively.
9.7.1
Multiple Precision Floating-Point Arithmetic
A multiple precision number x of type long real is represented as an array of
ﬂoating-point numbers. The value of the number is the sum of its components:
x =
n

i=1
xi,
(9.7.1)
and n is called the length of the representation (9.7.1).
Sometimes we will refer to xi as the ith component of x. Of course, the representa-
tion of a multiple precision number in the form (9.7.1) is not unique. The components
xi are ﬂoating-point numbers and as such they may have different signs. Also there
may be representations of x of different lengths. However, this will not present any
difﬁculties. All calculations will be carried out in a complete register which is a long
ﬁxed-point register and therefore provides a means for the unique representation of
numbers and intermediate results.
It is desirable that the absolute values of the components in (9.7.1) are ordered as
|x1| > |x2| > . . . > |xn| and that the exponents of two successive summands xi,
xi+1 differ at least by l where l is the mantissa length. We then say that the mantissas
do not overlap. In this case x is represented with an optimal precision of about n · l
mantissa digits. However, overlapping mantissas are not excluded in (9.7.1) but their
use means loss of precision.
We assume that the number of components of a multiple precision number is con-
trolled by a global variable called stagprec, (staggered precision). If stagprec
is 1, the long real data type is identical to the type real.
If, for instance,
stagprec is 4, each variable of this type consists of an array of four variables of
type real. It is desirable that the variable stagprec can be increased or decreased
at any point in a program. This enables the use of higher precision data and operations
in numerically critical parts of a computation. It helps to increase software reliability.
Let now x, y and z be multiple precision numbers:
x =
nx

i=1
xi,
y =
ny

i=1
yi,
z =
nz

i=1
zi.
We suppose that z is the result of an arithmetic operation, where nz is the current
run time value of the variable stagprec, i.e., stagprec= nz. We consider arith-
metic operations for multiple precision numbers:

9.7 Multiple Precision Arithmetics
345
Negation. Negation of a multiple precision number x is obtained by changing the sign
of all components:
−x = −
nx

i=1
xi =
nx

i=1
(−xi).
Arithmetic operations for multiple precision numbers are evaluated in a complete
variable. In the following algorithms for addition, subtraction, and multiplication this
complete register is called cv. After execution of any sequence of operations the
content of the complete register still has to be converted into the multiple precision
number z = nz
i=1 zi. For this conversion it is reasonable to use the rounding towards
zero. It is denoted by chop. The operator symbols +, −, and · in the algorithms denote
the operations for real numbers.
Addition and Subtraction.
cv :=
nx

i=1
xi ±
ny

i=1
yi,
for i := 1 to nz do
zi := chop(cv)
cv := cv −zi
Multiplication.
cv :=
nx

i=1
ny

j=1
xi · yj,
(9.7.2)
for i := 1 to nz do
zi := chop(cv)
cv := cv −zi
Here the products xi ·yi of the ﬂoating-point numbers xi and yi are to be computed
to the full double length and accumulated in the complete register.
By the conversion into the multiple precision number z used after addition, sub-
traction, and multiplication we obtain non-overlapping components zi, i = 1(1)nz
which all have the same sign.
Division. For division we use an iterative algorithm which computes the nz compo-
nents zi of the quotient z = x/y successively. We start with computing
z1 := (
x) / (
y).
Here
denotes the rounding to a ﬂoating-point number by the rounding towards
zero and
/ denotes the standard ﬂoating-point division. With this approximation we

346
9 Sample Applications
compute the next component z2 of z by
z2 :=
 nx

i=1
xi −
ny

i=1
yi · z1

/ (
y).
Here the expression in parentheses is computed correctly in the complete register as
a single exact scalar product. The result then is rounded into a standard ﬂoating-point
number by rounding toward zero. A ﬂoating-point division ﬁnally delivers the second
component z2 of z.
In general, if we have an approximation k
i=1 zi, the next component zk+1 is com-
puted inductively by
zk+1 :=
⎛
⎝
nx

i=1
xi −
ny

i=1
k

j=1
yi · zj
⎞
⎠
/ (
y).
(9.7.3)
Here again the numerator is computed correctly in the complete register as a single
exact scalar product and then rounded into a standard ﬂoating-point number. Finally
a ﬂoating-point division is performed.
Since in (9.7.3) the residual of the approximation k
j=1 zj is computed with only
a single rounding, the components zi, i = 1(1)nz of the quotient z = x/y do not
overlap. However, for division the quotient
z =
nz

i=1
zi
may have positive as well as negative components.
Scalar Product. Let X = (xk) and Y = (yk) be vectors with multiple precision
components xk and yk respectively:
xk =
nx

i=1
xk
i ,
yk =
ny

i=1
yk
i .
By (9.7.2) the product xk · yk of the two vector components is obtained by
xk · yk =
nx

i=1
ny

j=1
xk
i · yk
j .
(9.7.4)
Computation of the scalar product of the two vectors X = (xk) and Y = (yk) is
just the accumulation of all the products (9.7.4) for k from 1 to n:
X · Y =
n

k=1
xk · yk =
n

k=1
nx

i=1
ny

j=1
xk
i · yk
j .

9.7 Multiple Precision Arithmetics
347
This shows that the scalar product of the two multiple precision vectors X and
Y can be computed by accumulating products of ﬂoating-point numbers in a single
complete variable cv. The result is a multiple precision number z = nz
i=1 zi. It is
obtained by conversion of the complete register contents into the multiple precision
number z by:
for i := 1 to nz do
zi := chop(cv)
cv := cv −zi
Similar considerations hold for the computation of two multiple precision matrices
or for the computation of the defect of a system of linear equations with multiple
precision data. Of course, the formulas for these computations are getting more and
more complicated. But the user does not have to be concerned with these. By operator
overloading the work is done by the computer automatically.
The key operation for all these processes is a fast and exact scalar product. Quadru-
ple precision arithmetic is not a substitute for it.
At several occasions in this section rounding towards zero is applied where round-
ing to nearest could have been used instead. The reason for this is that rounding
towards zero is simpler and faster in general than rounding to nearest.
9.7.2
Multiple Precision Interval Arithmetic
Deﬁnition 9.8. Let xi, i = 1(1)n, n ≥0, be ﬂoating-point numbers and X =
[xlow, xhigh] an interval with ﬂoating-point bounds xlow and xhigh. Then an element
of the form
x =
n

i=1
xi + X
(9.7.5)
is called a long interval of length n. The xi are called the components of x and X is
called the interval component.
■
In (9.7.5) n is permitted to be zero. Then the sum in (9.7.5) is empty and x = X is
just an interval with standard ﬂoating-point bounds.
In the representation (9.7.5) of a long interval it is desirable that the components
do not overlap. The following operations for long intervals are written so that they
produce results with this property.
Arithmetic operations for long intervals are deﬁned as usual in interval arithmetic:
Deﬁnition 9.9. Let x and y be long intervals, then
x ◦y := {ξ ◦η|ξ ∈x ∧η ∈y}, for ◦∈{+, −, ·, /},
with 0 /∈y for ◦= /.
■

348
9 Sample Applications
Of course, in general, this theoretical result is not representable on the computer.
Here the result must be a long interval again. We do not, however, require that it is the
least enclosing long interval of some prescribed length. But we must require that the
computed long interval z is a superset of the result deﬁned in Deﬁnition 9.9: x◦y ⊆z.
Not to require optimality of the result gives room for a compromise between tightness
of the enclosure and the efﬁciency of the implementation.
Negation.
−x =
nx

i=1
(−xi) + [−xhigh, −xlow].
Let now x, y, and z be three long intervals:
x =
nx

i=1
xi + [xlow, xhigh],
y =
ny

i=1
yi + [ylow, yhigh],
z =
nz

i=1
zi + [zlow, zhigh].
Addition and subtraction of two long intervals x and y simply add and subtract
the lower and higher bounds of x and y into two complete registers which we call
lo and hi. Their contents ﬁnally have to be converted into a long interval z. This
conversion routine will be discussed after the description of the operations of addition
and subtraction. In the following algorithms the symbols +, −, and · again denote the
operations for real numbers.
Addition.
hi :=
nx

i=1
xi +
ny

i=1
yi
lo := hi + xlow + ylow
hi := hi + xhigh + yhigh
z := convert(lo, hi, nz).
Subtraction.
hi :=
nx

i=1
xi −
ny

i=1
yi
lo := hi + xlow −yhigh
hi := hi + xhigh −ylow
z := convert(lo, hi, nz).

9.7 Multiple Precision Arithmetics
349
Conversion.
convert(lo, hi, nz)
stop := false
i := 0
repeat i := i + 1
zlow := chop(lo)
zhigh := chop(hi)
if zlow = zhigh then zi := zlow
lo := lo −zi
hi := hi −zi
else zi := 0
stop := true
until stop or i = nz
for i := i + 1 to nz do zi := 0
zlow := ▽lo
zhigh := △hi
This routine reads successive numbers zlow and zhigh from the complete registers
and, as long as they are equal and the length nz has not yet been reached, they are
assigned to zi and subtracted from the complete registers. If zlow and zhigh are different
or the length nz for the result z is reached, then the remaining values in the complete
registers are converted to the interval component Z of z by appropriate rounding. If
some of the zi are not yet deﬁned, they are set to zero.
The conversion routine has the property that the real components of z do not over-
lap.
Multiplication. Multiplication can be implemented in various ways yielding different
results because of the subdistributivity law of interval arithmetic. Thus we have:
x · y =
 nx

i=1
xi + X
 ⎛
⎝
ny

j=1
yj + Y
⎞
⎠
⊆
nx

i=1
ny

j=1
xi · yj + X
ny

j=1
yj + Y
nx

i=1
xi + XY
(9.7.6)
⊆
nx

i=1
ny

j=1
xi · yj +
ny

j=1
Xyj +
nx

i=1
Y xi + XY.
(9.7.7)
This seems to suggest that better results can be obtained from using the second line
(9.7.6) than from the third line. However, to compute the products X ny
i=1 yi and
Y nx
i=1 xi we ﬁrst have to round the sums to machine intervals. As a consequence of
these additional roundings, the second line (9.7.6) yields coarser enclosures than the
third line. Therefore, we use line three for the multiplication algorithm.

350
9 Sample Applications
Again, lo and hi are two complete registers, xi, yi are reals, X, Y are intervals, and
z is the resulting long interval. The multiplication routine is as follows:
lo :=
nx

i=1
ny

j=1
xi · yj
hi := lo
[lo, hi] := [lo, hi] +
ny

j=1
Xyj +
nx

i=1
Y xi + X · Y
z := convert(lo, hi, nz).
Here again all accumulations in lo and hi are to be done without any intermediate
roundings.
Division. For division, again an iterative algorithm is applied. It computes the nz real
components zi of the quotient x/y successively.
To compute the approximation nz
i=1 zi, we start with
z1 :=
m(x) /
m(y).
Here m(x) and m(y) represent points selected within x and y respectively, the
midpoints for instance.
denotes the rounding to a ﬂoating-point number by the
rounding towards zero and
/ means ﬂoating-point division.
Now the components zi, i = 1(1)nz of z are computed by the same formula as for
a long real arithmetic:
zk+1 :=
⎛
⎝
nx

i=1
xi −
ny

i=1
k

j=1
yizj
⎞
⎠
/ (
m(y)).
(9.7.8)
Here the numerator is computed exactly in a complete register and then rounded
towards zero into a ﬂoating-point number. Finally a ﬂoating-point division is per-
formed.
This iteration again guarantees that the zi do not overlap.
Now the interval component Z of the result z is computed as
Z := ♦
⎛
⎝
nx

i=1
xi −
ny

i=1
nz

j=1
yizj + X −
nz

j=1
Y zj
⎞
⎠♦
/ (♦y),
(9.7.9)
where ♦denotes the rounding to a ﬂoating-point interval and ♦
/ denotes the division
of two ﬂoating-point intervals.

9.7 Multiple Precision Arithmetics
351
It is not difﬁcult to see that z = nz
i=1 zi + Z is a superset of the exact range
{ξ ◦η | ξ ∈x ∧η ∈y}. For α ∈X and β ∈Y we have the identity
nx
i
xi + α
ny
i
yi + β =
nz

j
zj +
nx
i
xi + α −ny
i
nz
j yizj −nz
j zjβ
ny
i
yi + β
.
An interval evaluation of this expression for α ∈X and β ∈Y shows immediately
that the exact range x/y is contained in nz
i=1 zi + Z as computed by (9.7.8) and
(9.7.9).
This leads to the following algorithm. Therein xi, yi, zi, and ym are ﬂoating-point
reals, X, Y , and Z are ﬂoating-point intervals.
denotes rounding towards zero into
a ﬂoating-point number and ♦denotes the rounding to a ﬂoating-point interval.
lo := nx
i=1 xi
my :=
m(y)
z1 := (
lo) / my
for k := 2 to nz do
lo := lo −ny
i=1 yizk−1
zk := (
lo) / my
lo := lo −ny
i=1 yiznz
hi := lo
Z := ♦([lo, hi] + X −nz
i=1 Y zi)♦
/ (♦y).
In this algorithm the double sum in (9.7.8) and (9.7.9) is accumulated in the com-
plete register lo as long as the zk are computed. The ﬁnal value in lo is then used
in the computation of the interval part Z. Thus the amount of work is reduced to a
minimum.
Scalar Product. We leave it to the reader to derive formulas for the computation of
the scalar product of two vectors with long interval components. We mention, how-
ever, that this is not necessary at all. By operator overloading the computer solves
this problem automatically. What is needed are two complete registers and a fast and
exact scalar product for ﬂoating-point numbers.
Square Root. An algorithm for the square root can be obtained analogously as in the
case of division. It computes the zi, i = 1(1)nz of the approximation part iteratively:
z1 :=
7
x,
zk+1 :=
⎛
⎝
nx

i=1
xi −
k

i,j=1
zizj
⎞
⎠/(2z1).
(9.7.10)
This guarantees that the zi do not overlap since in the numerator of (9.7.10) the
defect of the approximation nz
i=1 zi is computed with one rounding only. Now the

352
9 Sample Applications
interval part Z is computed as
Z :=
♦
nx
i=1 xi −nz
i,j=1 zizj + X

7
♦x + ♦nz
i=1 zi
.
(9.7.11)
As in the case of division, it is easy to see that nz
i=1 zi+Z as computed by (9.7.10)
and (9.7.11) is a superset of the exact range {√ξ | ξ ∈x}; in fact, for all γ ∈X we
have the identity:
8
9
9
:
nx

i=1
xi + γ =
nz

j=1
zj +
nx
i=1 xi + γ −nz
i,j=1 zizj
7nx
i=1 xi + γ + nz
j=1 zj
.
(9.7.12)
This leads to the following algorithm for the computation of the square root:
lo := nx
i=1 xi
z1 :=
7
lo
for k := 2 to nz do
lo := lo −2 k−2
j=1 zjzk−1 −zk−1zk−1
zk := (
lo)/(2z1)
lo := lo −2 nz−1
j=1 zjznz −znzznz
hi := lo
Z := ♦([lo, hi] + X)/(
7
♦x + ♦nz
j=1 zj).
To allow easy application of the long interval arithmetic just described a few addi-
tional operations should be supplied such as computation of the inﬁmum, the supre-
mum, the diameter, and the midpoint. Also elementary functions can be and have
been implemented for long intervals. They may already make use of the arithmetic
for long intervals.
9.7.3
Applications
We now brieﬂy sketch a few applications of multiple precision interval arithmetic. We
restrict the discussion to problem classes which have already been dealt with in this
chapter. We assume that the ﬂoating-point inputs to an algorithm are exact. Imprecise
data should be brought into the computer as intervals as accurately as possible, possi-
bly as long intervals. It should be clear that in such a case the result is a set, and if the
algorithm is unstable, this set may well be large. Even the best arithmetic can only
compute bounds for this set. These bounds may not look very accurate even if they
may be so.
Among the ﬁrst problems that have been solved to very high and guaranteed ac-
curacy was systems of linear equations by S. M. Rump [503]. The method was then

9.7 Multiple Precision Arithmetics
353
continually extended to other problem classes. We consider a system of linear equa-
tions A · x = b. Let x1 be an approximate solution and e1 := x∗−x1 be the error to
the exact solution x∗. Then e1 is the solution of the system
A · e1 = b −Ax1.
(9.7.13)
If we compute an interval enclosure X1 of e1, we have an enclosure of x∗by a long
interval: x∗∈x1 + X1. This method can now be iterated. With a new approximate
solution x2 := m(x1 + X1), where m denotes the midpoint of the interval x1 + X1,
a second error e2 can be computed by
A · e2 = b −Ax2.
An enclosure X2 of e2 leads to a new enclosure of x∗:
x∗∈x2 + X2.
Essential for success of this method is the fact that the defect b −A · xi of the
approximate solution xi can be computed to full accuracy by the exact scalar product.
This method of iterated defect correction can also be applied to compute with very
high accuracy enclosures of arithmetic expressions or of polynomials. An enclosure
of the solution is obtained as a long interval.
The methods just discussed can also be applied to problems where the initial data
themselves are long intervals. See [389].
The method for the evaluation of polynomials with long interval coefﬁcients allows
additional applications. It can be applied, for instance, to evaluate higher dimensional
polynomials and to represent the result as a long interval. To avoid too many indices
we sketch the method for the two-dimensional case. The independent variables are
denoted by x and y. Let the polynomial of degree n in x and m in y be
p(x, y) =
m

j=0
n

i=0
aijxiyj =
m

j=0
 n

i=0
aijxi

yj.
Its value can be obtained by successively computing the values of the m + 2 one-
dimensional polynomials
bj := bj(x) :=
n

i=0
aijxi, j = 0(1)m,
(9.7.14)
and
p(x, y) :=
m

j=0
bjyj.
(9.7.15)

354
9 Sample Applications
The results of the computation (9.7.14) are long intervals. The ﬁnal result (9.7.15)
is also a long interval. It may be rounded into a ﬂoating-point interval if desired.
Long interval arithmetic has also been very successfully applied to the computation
of orbits of discrete dynamic systems. It is well known that such computations are
highly unstable if the system exhibits chaotic behavior. In this case even for very sim-
ple systems ordinary ﬂoating-point arithmetic delivers results which are completely
wrong. Also ordinary interval arithmetic (i.e., intervals of ﬂoating-point numbers)
yields very poor enclosures after a few iterations. By use of a long interval arithmetic
highly accurate enclosures of orbits can be computed for a considerably longer time.
Of course, similar results also can be obtained by means of multiple precision arith-
metic simulated in software by integer arithmetic. However, if the exact multiply and
accumulate operation is available in hardware, the long interval arithmetic fully ben-
eﬁts from the speed of this ﬂoating-point hardware. Algorithms and programs are
available in [389, 392].
9.7.4
Adding an Exponent Part as a Scaling Factor to Complete
Arithmetic
The components of a multiple precision variable and the bounds of a multiple preci-
sion interval variable are ﬂoating-point numbers. If the basic ﬂoating-point format is
double precision this seems to be a severe limitation for the multiple precision arith-
metics considered in this section. If the largest component of a multiple precision
datum has a small exponent it may well be that one or several of the other components
underﬂow the exponent range of the basic ﬂoating-point format. This is particularly
troublesome in case of multiple precision interval arithmetic. In applications of inter-
val arithmetic it is essential that the width of the intervals be kept as tiny as possible.
If components of a multiple precision interval variable lie in the underﬂow region of
the basic ﬂoating-point format a rounding into a ﬂoating-point interval means a great
loss of precision and accuracy.
As an example let us assume that a and b are multiple precision real numbers, both
of 144 decimal digits and a small exponent
a = 1.098 · · · 981 · 10−154,
b = 1.234 · · · 789 · 10−154.
Then the exact product a · b = 1.343 · · · · 10−308 has 288 decimal digits and its
exponent nearly underﬂows the IEEE 754 double precision exponent range. Thus an
enclosure of the exact product a · b can only be computed to about 16 decimal digits.
This is a great loss of accuracy compared with the correct product.
This loss of accuracy can be avoided by scaling a and b appropriately, for instance,
as := 2+1022 · a,
bs := 2+1022 · b.

9.7 Multiple Precision Arithmetics
355
Then an enclosure of as · bs can be computed to least bit accuracy, i.e., to 288
correct digits and we have
a · b = 2−2044 · (as · bs).
In conventional ﬂoating-point arithmetic the exponent part accomplishes an auto-
matic scaling of the computation. In a very similar manner now an exponent part
can be added to a multiple precision real or interval variable x with an appropriately
chosen exponent range Emin ≤ex ≤Emax. Here ex is an integer. Thus a scaled mul-
tiple precision real or interval variable is a pair (ex, x). Based on the developments in
Sections 9.7.1 and 9.7.2, arithmetic for these pairs can now be deﬁned. The result is a
long real or long interval arithmetic with an exponent part as a scaling factor.
Hardware implementation of an exact scalar product (complete arithmetic) is not at
all more complicated than implementation of a full quadruple precision arithmetic as
deﬁned in IEEE P 754. By pipelining an exact scalar product can be computed with
extreme speed. This allows a very fast multiple precision real and interval arithmetic.
By adding an exponent part as a scaling factor a very ﬂexible and fast computing tool
is obtained. It allows highly accurate solution of even very ill-conditioned problems.
Multiple precision data types with an exponent part as scaling factor for the ba-
sic data types real, interval, complex, and complex interval have been made avail-
able in C-XSC by Blomquist, Kr¨amer and Hofschuster [288, 331]. The number of
components used for the fraction part is controlled by a global variable stagprec
(staggered precision) which can be increased or decreased during program execution
depending on the desired accuracy. With reasonably chosen Emin and Emax underﬂow
and overﬂow need not occur.
To make these scaled multiple precision arithmetics generally applicable, elemen-
tary and possibly also special functions have to be provided. Here care has to be taken
that the functions are evaluated with high accuracy in short computing times over
large deﬁnition ranges.
The programming environment C-XSC provides a large class of elementary and
special functions for multiple precision data with an exponent part as scaling factor
for the basic data types real, interval, complex, and complex interval, see [331].
What has been discussed in this section can be seen as advanced ﬂoating-point
arithmetic, ﬂoating-point arithmetic on a higher level, adapted to the needs of high
speed scientiﬁc computing.
Via complete arithmetic this advanced ﬂoating-point
arithmetic is based ultimately on the standardized double precision ﬂoating-point
arithmetic. If complete arithmetic is hardware supported by pipelining the advanced
ﬂoating-point arithmetics for real and interval data are also very fast. Very ill-condi-
tioned problems have been solved with these arithmetics to several hundred correct
digits.
An exact scalar product could also be adapted to implement greatly extended pre-
cision integer arithmetic.

Appendix A
Frequently Used Symbols
A logical statement has a value that is either true or false, t or f for short. Such
statements can be combined in logical expressions by logical operators. In this treatise
the logical operators and, or and not are occasionally used. They are denoted by the
symbols ∧, ∨, and ¬ respectively and are deﬁned by the following table:
a
b
¬a
a ∧b
a ∨b
t
t
f
t
t
t
f
f
f
t
f
t
t
f
t
f
f
t
f
f
In words, a ∧b is true if both operands are true, and a ∨b is true if at least one
operand is true.
Besides the logical operators ∧and ∨, the for all quantor , and the existence
quantor  are frequently used in the text. If M is a set, a ∈M and P(a) is a logical
statement, then
(i)

a∈M
P(a)
means: P(a) is true for all a ∈M.
(ii)

a∈M
P(a)
means: there exists an a ∈M such that P(a) is true.
Thus the for all quantor , and the existence quantor  in a certain sense can be
understood as generalizations of the logical operators ∧and ∨respectively. That is, if
M is a ﬁnite set with elements a1, a2, . . . , an, then
(i)

a∈M
P(a)
is equivalent to P(a1) ∧P(a2) ∧. . . ∧P(an), and
(ii)

a∈M
P(a)
is equivalent to P(a1) ∨P(a2) ∨. . . ∨P(an).

357
Other frequently used symbols:
▽
symbol for rounding downward.
△
symbol for rounding upward.
▽
+ , ▽
−, ▽· , ▽
/
ﬂoating-point operations with rounding downward.
△
+ , △
−, △· , △
/
ﬂoating-point operations with rounding upward.
a∥b
the elements a and b of an ordered set are incomparable.
◦A
denotes the interior of the interval A = [a1, a2],
i.e., c ∈
◦A means a1 < c < a2.
IR
set of closed and bounded real intervals.
(IR)
set of extended real intervals together with
closed and bounded real intervals.

Appendix B
On Homomorphism
Theorem B.1. Let {R, +} be the additive group of real numbers, S a ﬁnite subset of
R with n elements, and 0 ∈S. If
: R →S is a mapping with
(0) = 0 and
+ an
operation in S with the property (homomorphism)

a,b∈R
(a + b) = (
a) + (
b),
(B.0.1)
then

a,b∈R,a<b

x∈[a,b]
x = 0.
Proof. Let c1, c2, . . . , cn+1 ∈[a, b] be distinct elements of [a, b], ci ̸= cj, for i ̸= j,
then by a possible renumbering
c1 =
c2.
(B.0.2)
We obtain
(c2 + (−c1)) =
(c2 −c1)
=
(B.0.1)
(c2) + (
(−c1))
=
(B.0.2),(B.0.1)
(c1 −c1) =
(0) = 0.
Thus with c = c2 −c1 we have
c =
(0) = 0. Moreover
(2c) =
(c + c)
=
(B.0.1)
(c) +
(c)
= 0 + 0
=
(B.0.1)
(0 + 0) =
(0) = 0,
and by induction
(kc) = 0 for all k ∈N.
Since c = c2 −c1 ≤b −a, there exists an r ∈N with rc ∈[a, b], and with x := rc
we have
x = 0 with x ∈[a, b].
■
A minimum requirement of a rounding is the property
(R1)
a = a for all a ∈S,
i.e., all elements of S are ﬁxed points of the mapping. By Theorem B.1 in case of
a homomorphism every interval [a, b] ⊂R, whatever small it might be, contains
an element x which is mapped onto zero. A mapping with this property is not a
reasonable rounding. For further details see [503].

Bibliography
[1] O. Aberth, Precise Numerical Analysis, Wm. C. Brown Publishers, Dubuque, Iowa,
1988.
[2] M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions, Nat. Bur.
Standards, Appl. Math. Series 55, U.S. Government Printing Ofﬁce, Washington, D.C.,
1964.
[3] E. Adams, Enclosure methods and scientiﬁc computation, in: [48], pp. 3–31, 1989.
[4] E. Adams and U. Kulisch, On scientiﬁc computing with automatic result veriﬁcation,
in: [5], pp. 1–12, 1993.
[5] E. Adams and U. Kulisch (eds.), Scientiﬁc Computing with Automatic Result Veriﬁca-
tion. I. Language and Programming Support for Veriﬁed Scientiﬁc Computation, II. En-
closure Methods and Algorithms with Automatic Result Veriﬁcation, III. Applications
in the Engineering Sciences, Academic Press, San Diego, 1993.
[6] E. Adams and R. Lohner, Error bounds and sensitivity analysis, in: Scientiﬁc Comput-
ing, edited by R. S. Stepleman, pp. 213–222, North Holland, Amsterdam, 1983.
[7] R. C. Agarwal, J. W. Cooley, F. G. Gustavson, J. B. Shearer, G. Slishman and B. Tuck-
erman, New Scalar and Vector Elementary Functions for the IBM System/370, IBM
Journal of Research and Development 30:2 (1986), 123–144.
[8] A. Akkas, Instruction Set Enhancements for Reliable Computations, Ph.D. Thesis,
Lehigh University, January 2002.
[9] A. Akkas, A combined interval and ﬂoating-point comparator/selector, in: IEEE 13th
International Conference on Application-speciﬁc Systems, Architectures and Proces-
sors, San Jose, USA, July, 2002, pp. 208–217, 2003.
[10] R. Albrecht, G. Alefeld and H. J. Stetter (eds.), Validation Numerics – Theory and
Applications, Computing Supplementum 9, Springer, Wien New York, 1993.
[11] R. Albrecht and U. Kulisch (eds.), Grundlagen der Computerarithmetik, Computing
Supplementum 1, Springer, Wien New York, 1977.
[12] G. Alefeld, Intervallrechnung ¨uber den komplexen Zahlen und einige Anwendungen,
Dissertation, Universit¨at Karlsruhe, 1968.
[13] G. Alefeld, ¨Uber die aus monoton zerlegbaren Operatoren gebildeten Iterationsver-
fahren, Computing 6 (1970), 161–172.
[14] G. Alefeld, ¨Uber die Durchf¨uhrbarkeit des Gaußschen Algorithmus bei Gleichungen
mit Intervallen als Koefﬁzienten, in: [11], pp. 15–19, 1977.
[15] G. Alefeld, Intervallanalytische Methoden bei nichtlinearen Gleichungen, in: Jahrbuch
¨Uberblicke Mathematik 1979, pp. 63–78, 1979.
[16] G. Alefeld, Bounding the slope of polynomials and some applications, Computing 26
(1981), 227–237.

360
Bibliography
[17] G. Alefeld, On the convergence of some interval-arithmetic modiﬁcations of Newton’s
method, in: Scientiﬁc Computing, edited by R. Stepleman, pp. 223–230, IMACS/North
Holland Publishing Company, Amsterdam, 1983.
[18] G. Alefeld, Berechenbare Fehlerschranken f¨ur ein Eigenpaar unter Einschluss von Run-
dungsfehlern bei Verwendung des genauen Skalarproduktes, Z. Angew. Math. Mech. 67
(1987), 145–152.
[19] G. Alefeld, Rigorous error bounds for singular values of a matrix using the precise
scalar product, in: [270], pp. 9–30, 1987.
[20] G. Alefeld, ¨Uber die Konvergenz des Intervall-Newton-Verfahrens, Computing 39
(1987), 363–369.
[21] G. Alefeld, Errorbounds for quadratic systems of nonlinear equations using the precise
scalar product, in: [372], pp. 59–67, 1988.
[22] G. Alefeld, Existence of solutions and iterations for nonlinear equations, in: [426],
pp. 207–227, 1988.
[23] G. Alefeld, Enclosure methods, in: [591], pp. 55–72, 1990.
[24] G. Alefeld, ¨Uber das Divergenzverhalten des Interval-Newton-Verfahrens, Computing
46 (1991), 289–294.
[25] G. Alefeld, Inclusion methods for systems of nonlinear equations – the interval Newton
method and modiﬁcations, in: [224], pp. 7–26, 1994.
[26] G. Alefeld, A. Frommer and B. Lang (eds.), Scientiﬁc Computing and Validated Nu-
merics, Proceedings of SCAN-95, Akademie Verlag, Berlin, 1996.
[27] G. Alefeld and R. D. Grigorieff (eds.), Fundamentals of Numerical Computation
(Computer-Oriented Numerical Analysis), Computing Supplementum 2, Springer,
Wien New York, 1980.
[28] G. Alefeld and J. Herzberger, Einf¨uhrung in die Intervallrechnung, Informatik 12,
Bibliographisches Institut, Mannheim Wien Z¨urich, 1974.
[29] G. Alefeld and J. Herzberger, Introduction to Interval Computations, Academic Press,
New York, 1983.
[30] G. Alefeld and J. Herzberger (eds.), Numerical Methods and Error Bounds, Mathemat-
ical Research 89, Akademie Verlag, Berlin, 1996.
[31] G. Alefeld, B. Illg and F. Potra, On a class of enclosure methods for systems of equa-
tions with higher order of convergence, in: [591], pp. 151–159, 1990.
[32] G. Alefeld, G. Kreinovich and G. Mayer, On the shape of the symmetric, and skew-
symmetric solution set, SIAM J. Matrix Anal. Appl. 18 (1997), 693–705.
[33] G. Alefeld, G. Kreinovich and G. Mayer, The shape of the solution set of linear interval
equations with dependent coefﬁcients, Math. Nachr. 192 (1998), 23–36.
[34] G. Alefeld and R. Lohner, On higher order centered forms, Computing 35 (1985), 177–
184.
[35] G. Alefeld and G. Mayer, The Cholesky Method for Interval Data, Presentation at the
International Conference on “Numerical Analysis with Automatic Result Veriﬁcation”,
Lafayette, Louisiana, USA, February 25 – March 1, 1993.
[36] G. Alefeld and G. Mayer, A computer-aided existence and uniqueness proof for an
inverse matrix eigenvalue problem, Int. J. Interval Computations 1 (1994), 4–27.

Bibliography
361
[37] G. Alefeld and G. Mayer, Einschließungsverfahren, in: [226], pp. 155–186, 1995.
[38] G. Alefeld and G. Mayer, On the symmetric and unsymmetric solution set of interval
systems, SIAM J. Matrix Anal. Appl. 16 (1995), 1223–1240.
[39] G. Alefeld and G. Mayer, Interval analysis: theory and applicastions, J. Comput. Appl.
Math. 121 (2000), 421–464.
[40] G. Alefeld and G. Mayer, On singular interval systems, in: [47], pp. 191–197, 2004.
[41] G. Alefeld, J. Rohn, S. M. Rump and T. Yamamoto (eds.), Symbolic Algebraic Methods
and Veriﬁcation Methods, Springer, Wien New York, 2001.
[42] G. Alefeld and U. Sch¨afer, Iterative methods for linear complementary problems with
interval data, Computing 70 (2003), 235–259.
[43] J. Alex, Wege und Irrwege des Konrad Zuse, Spektrum der Wissenschaft, Januar 1997,
78–90.
[44] U. Allend¨orfer and D. Shiriaev, PASCAL-XSC to C – a portable PASCAL-XSC com-
piler, in: [271], pp. 91–104, 1991.
[45] U. Allend¨orfer and D. Shiriaev, PASCAL-XSC. A Portable Development System, Pro-
ceedings of 13th World Congress on Computation and Applied Mathematics, IMACS
’91, Dublin, 1991.
[46] U. Allend¨orfer and D. Shiriaev, PASCAL-XSC. A portable development system, in:
[125], pp. 11–20, 1992.
[47] R. Alt, A. Frommer, R. B. Kearfott and W. Luther (eds.), Numerical Software with
Result Veriﬁcation, Lecture Notes in Computer Science, Springer, Berlin, 2004.
[48] W. F. Ames (ed.), Numerical and Applied Mathematics, J. C. Baltzer Scientiﬁc Publish-
ing, Basel, 1989.
[49] A. S. Andreev, I. T. Dimov, S. M. Markov and Ch. Ullrich (eds.), Mathematical Mod-
elling and Scientiﬁc Computations, Bulgarian Academy of Sciences, Soﬁa, 1991.
[50] N. Apostolatos, U. Kulisch, R. Krawczyk, B. Lortz, K. Nickel and H.-W. Wippermann,
The algorithmic language Triplex-ALGOL 60, Numerische Mathematik 11 (1968),
175–180.
[51] H.-R. Arndt and G. Mayer, On the semi-convergence of interval matrices, Linear Alge-
bra and its Applications 393 (2004), 15–37.
[52] L. Atanassova and J. Herzberger (eds.), Computer Arithmetic and Enclosure Methods,
Proceedings of SCAN 91, North-Holland, Elsevier Science Publishers B. V., Amster-
dam, 1992.
[53] W. Auzinger and H. J. Stetter, Accurate arithmetic results for decimal data on non-
decimal computers, Computing 35 (1985), 141–151.
[54] D. H. Bailey, A Portable High Performance Multiprecision Package, RNA Technical
Report RNR-90-022, 1993.
[55] D. H. Bailey, A Fortran-90 based multiprecision system, ACM Trans. Math. Software
21 (1995), 379–387.
[56] B. Barth, Eine veriﬁzierte Einschließung von Werten der Weierstraßschen ℘-Funktion,
Diplomarbeit am Institut f¨ur Angewandte Mathematik, Universit¨at Karlsruhe, 1991.
[57] W. Barth and E. Nuding, Optimale L¨osungen von Intervallgleichungssystemen, Com-
puting 12 (1974), 117–125.

362
Bibliography
[58] H. Bauch, K.-U. Jahn, D. Oelschl¨agel, H. S¨usse and V. Wiebigke, Intervallmathematik.
Theorie und Anwendungen, BSB B. G. Teubner Verlagsgesellschaft, Leipzig, 1987.
[59] Ch. Baumhof, Behavioural Description of a Scalar Product Unit, Universit¨at Karl-
sruhe, ESPRIT Project OMI/HORN, Deliverable Report D1.2/2, December 1992.
[60] Ch. Baumhof, A new VLSI vector arithmetic coprocessor for the PC, in: [654], Vol. 12,
pp. 210–215, 1995.
[61] Ch. Baumhof, Ein Vektorarithmetik-Koprozessor in VLSI-Technik zur Unterst¨utzung
des Wissenschaftlichen Rechnens, Dissertation, Universit¨at Karlsruhe, 1996.
[62] Ch. Baumhof and G. Bohlender, A VLSI Vector Arithmetic Coprocessor for the PC,
Proceedings of WAI’96 in Recife/Brasil, RITA (Revista de Inform´atica Te´orica e Apli-
cada), Extra Edition, October 1996.
[63] W. De Beauclair, Rechnen mit Maschinen, Vieweg, Braunschweig, 1968.
[64] H. Beek, ¨Uber die Struktur und Absch¨atzungen der L¨osungsmenge von linearen Glei-
chungssystemen mit Intervallkoefﬁzienten, Computing 10 (1972), 231–244.
[65] H. Behnke, Inclusion of eigenvalues of general eigenvalue problems of matrices, in:
[372], pp. 69–78, 1988.
[66] H. Behnke, The determination of guaranteed bounds to eigenvalues with the use of
variational methods II, in: [591], pp. 155–170, 1990 (Part I see [186]).
[67] H. Behnke, The calculation of guaranteed bounds for eigenvalues using complementary
variational principles, Computing 47 (1992), 11–27.
[68] H. Behnke and U. Mertins, Bounds for eigenvalues with the use of ﬁnite elements, in:
[366], pp. 119–131, 2001.
[69] H. Behnke, U. Mertins, M. Plum and Ch. Wieners, Eigenvalue inclusions via domain
decomposition, Proc. R. Soc. London 456 (2000), 2717–2730.
[70] M. Berz, Automatic differentiation as nonarchimedean analysis, in: [52], pp. 439–450,
1992.
[71] M. Berz, C. Bischof, G. Corliss and A. Griewank (eds.), Computational Differentiation,
Techniques, Applications, and Tools, SIAM, 1996.
[72] D. Bethke and J. Herzberger, ¨Uber Eigenschaften von zwei Methoden zur Ein-
schließung der Inversen einer Intervallmatrix, in: [245], pp. 409–413, 1991.
[73] N. Bierlox, Ein VHDL Koprozessor f¨ur das exakte Skalarprodukt, Dissertation, Univer-
sit¨at Karlsruhe, 2002.
[74] C. Bischof, A. Carle, G. Corliss, A. Griewank and P. Hovland, ADIFOR: Fortran
Source Translation for Efﬁcient Derivatives, ADIFOR Working Note No. 4, Argonne
National Laboratory, address: see [147], Report MCS-P278-1291, February 1992.
[75] Ch. M. Black, R. B. Burton and T. H. Miller, The need for an industry standard of ac-
curacy for elementary-function programs, ACM Trans. on Math. Software 10:4 (1984),
361–366.
[76] J. H. Bleher, S. M. Rump, U. Kulisch, M. Metzger, Ch. Ullrich and W. Walter,
FORTRAN-SC: A study of a FORTRAN extension for engineering/scientiﬁc compu-
tation with access to ACRITH, Computing 39 (1987), 93–110 (also in [372], pp. 227–
244, 1988).

Bibliography
363
[77] F. Blomquist, Implementierung und Fehlerabsch¨atzungen von PASCAL-XSC Standard-
funktionen f¨ur ein dezimales Datenformat, Universit¨at Karlsruhe, 1992.
[78] F. Blomquist, Implementierung einiger spezieller mathematischer Funktionen in
PASCAL-XSC, private communication, 1992.
[79] F. Blomquist, PASCAL-XSC, BCD-Version 1.0, Benutzerhandbuch f¨ur das dezimale
Laufzeitsystem, Universit¨at Karlsruhe, Institut f¨ur Angewandte Mathematik, 1997.
[80] F. Blomquist, Verifzierende Numerik mit PASCAL-XSC, BCD-Version. 1-713, Institut
f¨ur Angewandte Mathematik, Universi¨at Karlsruhe, 2000, available at www.math.
uni-wuppertal.de/org/WRST/blom.html.
[81] F. Blomquist, W. Hofschuster and W. Kr¨amer, Complex Interval Functions in C-XSC,
preprint BUW-WRSWT 2005/2, Universit¨at Wuppertal, 2005.
[82] F. Blomquist, W. Hofschuster and W. Kr¨amer, Real and Complex Taylor Arithmetic in
C-XSC, preprint BUW-WRSWT 2005/4, Universit¨at Wuppertal, 2005.
[83] F. Blomquist, W. Hofschuster and W. Kr¨amer, Vermeidung von ¨Uber- und Unter-
lauf und Verbesserung der Genauigkeit bei reeller und komplexer staggered Intervall-
Arithmetik, preprint BUW-WRSWT 2007, Universit¨at Wuppertal, 2007.
[84] P. Bochev and S. Markov, A self-validating numerical method for the matrix exponen-
tial, Computing 43 (1989), 59–72.
[85] P. Bochev and S. Markov, Simultaneous self-veriﬁed computation of exp(A) and
; 1
0 exp(As)ds, Computing 45 (1990), 183–191.
[86] H. Boehm, Berechnung von Polynomnullstellen und Auswertung arithmetischer Aus-
dr¨ucke mit garantierter maximaler Genauigkeit, Dissertation, Universit¨at Karlsruhe
1983.
[87] H. Boehm, Evaluation of arithmetic expressions with maximum accuracy, in: [368],
pp. 121–137, 1983.
[88] G. Bohlender, Genaue Summation von Gleitkommazahlen, in: [11], pp. 21–32, 1977.
[89] G. Bohlender, Floating-Point Computation of Functions with Maximum Accuracy,
IEEE Transactions on Computers, Vol. C-26, no. 7, July 1977.
[90] G. Bohlender, Genaue Berechnung mehrfacher Summen, Produkte und Wurzeln von
Gleitkommazahlen und allgemeine Arithmetik in h¨oheren Programmiersprachen, Dis-
sertation, Universit¨at Karlsruhe, 1978.
[91] G. Bohlender, What do we need beyond IEEE arithmetic?, in: [593], pp. 1–32, 1990.
[92] G. Bohlender, A vector extension of the IEEE standard for ﬂoating-point arithmetic,
in: [271], pp. 3–12, 1991.
[93] G. Bohlender, Bibliography on enclosure methods and related topics, in: [5], pp. 571–
608, 1993.
[94] G. Bohlender, Literature List on Enclosure Methods and Related Topics, Institut f¨ur
Angewandte Mathematik, Universit¨at Karlsruhe, Report, 1998.
[95] G. Bohlender, D. Cordes, A. Kn¨ofel, U. Kulisch, R. Lohner and W. V. Walter, Proposal
for accurate ﬂoating-point vector arithmetic, in: [5], pp. 87–102, 1993.
[96] G. Bohlender, K. Gr¨uner, E. Kaucher, R. Klatte, W. Kr¨amer, U. Kulisch, W. L. Mi-
ranker, S. Rump, Ch. Ullrich and J. Wolff von Gudenberg, PASCAL-SC: A PASCAL

364
Bibliography
for Contemporary Scientiﬁc Computation, Research Report RC 9009, IBM Thomas J.
Watson Research Center, Yorktown Heights, New York, 1981.
[97] G. Bohlender, E. Kaucher, R. Klatte, U. Kulisch, W. L. Miranker, Ch. Ullrich and J.
Wolff von Gudenberg, FORTRAN for contemporary numerical computation, IBM Re-
search Report RC 8348, Computing 26 (1981), 277–314.
[98] G. Bohlender and A. Kn¨ofel, A survey of pipelined hardware support for accurate scalar
products, in: [271], pp. 29–43, 1991.
[99] G. Bohlender, P. Kornerup, D. W. Matula and W. V. Walter, Semantics for exact
ﬂoating-point operations, in: Proceedings of the 10th IEEE Symposium on Computer
Arithmetic, pp. 22–26, Grenoble France, IEEE Comp. Soc., 1991.
[100] G. Bohlender, W. Kr¨amer and W. L. Miranker, Grading of Basic Arithmetical Opera-
tions and Functions, IBM Rsearch Report, RC 19593(86059)6-1-94, 1994.
[101] G. Bohlender, W. L. Miranker and J. Wolff von Gudenberg, Floating-point systems for
theorem proving, in: Computer Aided Proofs in Analysis, edited by K. R. Meyer and
D. S. Schmidt, Springer, 1990.
[102] G. Bohlender, L. Rall, Ch. Ullrich and J. Wolff von Gudenberg, PASCAL-SC
– Wirkungsvoll programmieren, kontrolliert rechnen, Bibliographisches Institut,
Mannheim, 1986.
[103] G. Bohlender, L. Rall, Ch. Ullrich and J. Wolff von Gudenberg, PASCAL-SC: A Com-
puter Language for Scientiﬁc Computation, Academic Press, New York, 1987.
[104] G. Bohlender and T. Teufel, Demonstration of the bit-slice processor unit BAP–SC in
a 68000 environment, in: [603], Vol. 1, pp. 155–158, 1985, or in: [532], pp. 331–336,
1986.
[105] G. Bohlender and T. Teufel, BAP–SC: A decimal ﬂoating-point processor for optimal
arithmetic, in: [270], pp. 31–58, 1987.
[106] G. Bohlender, W. Walter, P. Kornerup and D. W. Matula, Semantics for exact ﬂoating-
point operations, in: Proceedings of the 10th IEEE Symposium on Computer Arith-
metic, pp. 22–26, Grenoble France, IEEE Press, 1991.
[107] H. B¨ohm, Auswertung arithmetischer Ausdr¨ucke mit maximaler Genauigkeit, in:
[374], pp. 175–184, 1982.
[108] H. B¨ohm, Berechnung von Polynomnullstellen und Auswertung arithmetischer
Ausdr¨ucke mit garantierter maximaler Genauigkeit, Dissertation, Universit¨at Karl-
sruhe, 1983.
[109] H. B¨ohm, Evaluation of arithmetic expressions with maximum accuracy, in: [368],
pp. 121–137, 1983.
[110] J. M. Borwein and P. B. Borwein, The Arithmetic-Geometric Mean and Fast Computa-
tion of Elementary Functions, SIAM Review, Vol. 26, No. 3, July 1984.
[111] J. M. Borwein and P. B. Borwein, Pi and the AGM, John Wiley & Sons, 1987.
[112] J. M. Borwein, P. B. Borwein and D. H. Bailey, Ramanujan, modular equations, and
approximations to Pi, Amer. Math. Monthly 96 (1989), 201–219, 1989.
[113] G. Brassard, S. Monet and D. Zuffellato, Algorithmes pour l’arithm´etique des tr`es
grands entiers, TSI Technique et Science Informatiques, Vol. 5, No. 2, 1986.

Bibliography
365
[114] K. Braune, Hochgenaue Standardfunktionen f¨ur reelle und komplexe Punkte und Inter-
valle in beliebigen Gleitpunktrastern, Dissertation, Universit¨at Karlsruhe, 1987.
[115] K. Braune, Standard functions for real and complex point and interval arguments with
dynamic accuracy, Computing Supplementum 6 (1988), 159–184.
[116] K. Braune, A-posteriori Fehlerschranken bei der Berechnung inverser Standardfunk-
tionen mit Hilfe des Newton-Verfahrens, ZAMM 70 (1990), T579–T581.
[117] K. Braune and W. Kr¨amer, Standard functions for intervals with maximum accuracy,
in: Proceedings of the 11th IMACS World Congress, Vol. 1, pp. 167–170, Oslo, 1985.
[118] K. Braune and W. Kr¨amer, High-accuracy standard functions for intervals, in: Com-
puter Systems: Performance and Simulation, edited by M. Ruschitzka, Elsevier Science
Publishers, 1985.
[119] K. Braune and W. Kr¨amer, High-accuracy standard functions for real and complex
intervals, in: E. Kaucher, U. Kulisch and Ch. Ullrich, Computerarithmetic: Scien-
tiﬁc Computation and Programming Languages, pp. 81–114, B. G. Teubner, Stuttgart,
1987.
[120] P. B. Brent, Fast multiple-precision evaluation of elementary functions, J. of the Asso-
ciation for Computing Machinery 23:2 (1976), 242–251.
[121] R. P. Brent, A FORTRAN multiple precision arithmetic package, ACM Trans. Math.
Software 4 (1978), 57–70.
[122] R. P. Brent, MP User’s Guide, fourth edition, Technical Report TR–CS–81–08, Depart-
ment of Computer Science, Australian National University, Canberra, 1981.
[123] R. P. Brent, J. A. Hooper and J. M. Yohe, An Augment interface for Brent’s multiple-
precision arithmetic package, ACM Trans. Math. Software 6 (1980), 146–149.
[124] C. Brezinski (ed.), SCAN’2002 International Conference (Guest Editors: Rene Alt and
Jean-Luc Lamotte), Numerical Algorithms, Vol. 37, 1–4, Kluwer Academic Publishers,
2004.
[125] C. Brezinski and U. Kulisch (eds.), Computational and Applied Mathematics I – Algo-
rithms and Theory, Proceedings of the 13th IMACS World Congress, Dublin, Ireland,
Elsevier Science Publishers B.V., 1992.
[126] K. B¨uhler and W. Barth, A new intersection algorithm for parametric surfaces based on
LIEs, in: [329], pp. 179–190, 2001.
[127] R. Bulirsch and J. Stoer, Asymptotic upper and lower bounds for results of extrapola-
tion methods, Numerische Mathematik 8 (1966), 93–104.
[128] P. R. Cappello and W. L. Miranker, Systolic super summation, IEEE Transactions on
Computers 37:6 (1988), 657–677.
[129] P. R. Cappello and W. L. Miranker, Systolic Super Summation with Reduced Hardware,
IBM Research Report RC 14259 (#63831), IBM Research Division, Yorktown Heights,
New York, November 30, 1988.
[130] C. Y. Chen, Adaptive numerische Quadratur und Kubatur mit Genauigkeitsgarantie,
Dissertation, Universit¨at Karlsruhe, 1998.
[131] X. Chen, A veriﬁcation method for solutions of nonsmooth equations, Computing 58
(1997), 281–294.

366
Bibliography
[132] D. M. Claudio, Beitr¨age zur Struktur der Rechnerarithmetik, Dissertation, Universit¨at
Karlsruhe, 1979.
[133] D. M. Claudio, Contribution to the structure of computer arithmetic, Computing 24
(1980), 115–118.
[134] D. M. Claudio, An algorithm for solving nonlinear equations based on the regula falsi
and Newton methods, ZAMM 64 (1984), T407–T408.
[135] L. Collatz, Funktionalanalysis und numerische Mathematik, Springer, Berlin Heidel-
berg New York, 1968.
[136] D. Cordes, Veriﬁzierter Stabilit¨atsnachweis f¨ur L¨osungen periodischer Differential-
gleichungen auf dem Rechner mit Anwendungen, Dissertation, Universit¨at Karlsruhe,
1987.
[137] D. Cordes, Sp¨arlich besetzte Matrizen, in: [357], pp. 129–136, 1989.
[138] D. Cordes, Runtime system for a PASCAL-XSC compiler, in: [271], pp. 151–160,
1991.
[139] D. Cordes and E. Kaucher, Self-validating computation for sparse matrix problems, in:
[270], pp. 133–149, 1987.
[140] D. Cordes and W. Kr¨amer, Vom Problem zum Einschließungsalgorithmus, in: Wis-
senschaftliches Rechnen mit Ergebnisveriﬁkation, edited by U. Kulisch, pp. 167–181,
Vieweg, Braunschweig, 1989.
[141] D. Cordes and W. Kr¨amer, PASCAL-XSC Modules for Multiple-Precision Operations
and Functions, Universit¨at Karlsruhe, 1991.
[142] G. F. Corliss, Computing narrow inclusions for deﬁnite integrals, in: [270], pp. 150–
169, 1987.
[143] G. F. Corliss, Applications of differentiation arithmetic, in: [426], pp. 127–148, 1988.
[144] G. F. Corliss, Industrial applications of interval techniques, in: [591], pp. 91–113, 1990.
[145] G. F. Corliss, Automatic differentiation bibliography, in: [194], pp. 331–353, 1991 (see
also [147]).
[146] G. F. Corliss, Validated anti-derivatives, in: Computer Aided Proofs in Analysis, edited
by R. K. Meyer and D. S. Schmidt, IMA Volumes in Mathematics and Its Applications
28, Springer, New York, 1991.
[147] G. F. Corliss (ed.), Automatic Differentiation Bibliography, Argonne National Labora-
tory, Mathematics and Computer Science Division, 9700 South Cass Avenue, Argonne,
Illinois, Report ANL/MCS-TM-167 (30 pages), July 1992.
[148] G. F. Corliss, Intorduction to Validated ODE Solving, Technical Report No. 416, Mar-
quette University, Milwaukee, Wisconsin, March 1995.
[149] G. F. Corliss and L. B. Rall, Adaptive, Self-Validating Numerical Quadrature, MRC
Technical Summary Report # 2815, University of Wisconsin, Madison, 1985.
[150] G. F. Corliss and L. B. Rall, Computing the range of derivatives, in: [271], pp. 195–212,
1991.
[151] H. Cornelius and R. Lohner, Computing the range of values of real functions with
accuracy higher than second order, Computing 33 (1984), 331–347.
[152] H. Cornelius and R. Lohner, Enclosing the range of values of real functions, ZAMM 64
(1984), T408–T410.

Bibliography
367
[153] T. Csendes (ed.), Developments in Reliable Computing, Kluwer Academic Publishers,
1999.
[154] L. Dadda, Some schemes for parallel multipliers, Alta Frequenza 34 (1965), 346–356.
[155] Ph. J. Davis and Ph. Rabinowitz, Methods of Numerical Integration, Academic Press,
San Diego, 1984.
[156] T. J. Dekker, A ﬂoating-point technique for extending the available precision, Numeri-
cal Mathematics 18 (1971), 224–242.
[157] J. Demmel and Y. Hida, A ﬂoating-point technique for extending the available preci-
sion, SIAM J. Sci. Comput. 25 (2003), 1214–1248.
[158] St. Dietrich, Adaptive veriﬁzierte L¨osung gew¨ohnlicher Differentialgleichungen, Dis-
sertation, Universit¨at Karlsruhe, 2002.
[159] H.-J. Dobner, Einschließungsalgorithmen f¨ur hyperbolische Differentialgleichungen,
Dissertation, Universit¨at Karlsruhe, 1986.
[160] W. E. Egbert, Personal Calculator Algorithms I, II, III and IV, Hewlett-Packard Journal,
Mai 1977, Juni 1977, November 1977 und April 1978.
[161] P. Eijgenraam, The Solution of Initial Value Problems Using Interval Arithmetic. For-
mulation and Analysis of an Algorithm, Dissertation, Mathematisch Centrum, Amster-
dam, 1981.
[162] B. Einarsson (ed.), Accuracy and Reliability in Scientiﬁc Computing, SIAM, 2005.
[163] H. Engels, Numerical Quadrature and Cubature, Academic Press, New York, 1980.
[164] H. Erb, Ein Gleitpunkt-Arithmetikprozessor mit mehrfacher Pr¨azision zur veriﬁzierten
L¨osung linearer Gleichungssysteme, Dissertation, Universit¨at Karlsruhe, 1992.
[165] A. Facius, Inﬂuence of rounding errors in solving large sparse linear systems, in: [153],
pp. 17–30, Kluwer Academic Publishers, 1999.
[166] A. Facius, Iterative Solution of Linear Systems with Improved Arithmetic and Result
Veriﬁcation, Dissertation, Universit¨at Karlsruhe, 2000.
[167] A. Facius, Highly accurate veriﬁed error bounds for Krylov type linear system solvers,
in: [366], pp. 76–98, 2001.
[168] K. V. Fernando and M. W. Pont, Computing accurate eigenvalues of a hermitian matrix,
in: [595], pp. 104–115, 1989.
[169] H. Fischer, Fast method to compute the scalar product of gradient and given vector,
Computing 41 (1986), 261–265.
[170] H.-C. Fischer, Schnelle automatische Differentiation, Einschliessungsmethoden und
Anwendungen, Dissertation, Universit¨at Karlsruhe, 1990.
[171] H.-C. Fischer, Range computation and applications, in: [591], pp. 197–211, 1990.
[172] H.-C. Fischer, Efﬁziente Berechnung von Ableitungswerten, Gradienten und Tay-
lorkoefﬁzienten, in: Jahrbuch ¨Uberblicke Mathematik 1992, edited by S. D. Chatterji,
B. Fuchssteiner, U. Kulisch, R. Liedl and W. Purkert, pp. 59–73, Vieweg, Braun-
schweig, 1992.
[173] H.-C. Fischer, Automatic differentiation and applications, in: [5], pp. 105–142, 1993.
[174] H.-C. Fischer, Automatisches Differenzieren, in: [226], pp. 53–104, 1995.

368
Bibliography
[175] H.-C. Fischer, G. Schumacher and R. Haggenm¨uller, Evaluation of arithmetic expres-
sions with guaranteed high accuracy, Computing Supplementum 6 (1988), 149–158.
[176] G. E. Forsythe, Pitfalls in computation, or why a math book isn’t enough, Amer. Math.
Monthly 9 (1970), 931.
[177] A. Frommer, L¨osung linearer Gleichungssysteme auf Parallelrechnern, Vieweg, Braun-
schweig, 1990.
[178] A. Frommer, Asynchronous iterations for enclosing solutions of ﬁxed point problems,
in: [52], pp. 243–252, 1992.
[179] A. Frommer, Proving conjectures by use of interval arithmetic, in: [366], pp. 1–13,
2001.
[180] P. Gaffney and E. Houstis (eds.), Programming Environments for High Level Scientiﬁc
Problem Solving, Proceedings of IFIP WG 2.5 conference, Karlsruhe, September 23–
27, 1991, Elsevier Science Publishers, 1993.
[181] S. Gal, Computing Elementary Functions: A New Approach for Achieving High Accu-
racy and Good Performance, IBM Technical Report 88.153, 1985.
[182] S. Gal and B. Bachelis, An Accurate Elementary Mathematical Library for the IEEE
Floating Point Standard, IBM Technical Report 88.223, IBM Israel, Technion City,
Haifa, Israel, 1988.
[183] J. Garloff, Interval mathematics. A bibliography, Freib. Int.-Ber. 6 (1985), 1–222.
[184] J. Garloff, Bibliography on interval mathematics. Continuation, Freiburger Intervall-
Berichte 87:2 (1987), 1–50.
[185] A. Gienger, Zur L¨osungsveriﬁkation bei Fredholmschen Integralgleichungen zweiter
Art, Dissertation, Universit¨at Karlsruhe, 1997.
[186] F. Goerisch and Z. He, The determination of guaranteed bounds to eigenvalues with the
use of variational methods I, in: [591], pp. 137–153, 1990 (Part II see [66]).
[187] D. Goldberg,
What Every Computer Scientist Should Know About Floating-Point
Arithmetic, ACM Computing Surveys 23, No. 1, March 1991.
[188] G. H. Golub and C. F. van Loan, Matrix Computations, third edition, John Hopkins,
Baltimore, 1995.
[189] A. Greenbaum, Iterative Methods for Solving Linear Systems, SIAM, Philadelphia,
1997.
[190] R. T. Gregory and D. L. Karney, A Collection of Matrices for Testing Computational
Algorithms, John Wiley & Sons, New York, 1969.
[191] A. Griewank, On automatic differentiation, in: Mathematical Programming: Recent
Developments and Applications, edited by M. Iri and K. Tanabe, pp. 83–108, Kluwer
Academic Publishers, 1989.
[192] A. Griewank, Achieving Logarithmic Growth of Temporal and Spatial Complexity in
Reverse Automatic Differentiation, Preprint MCS-P228-0491, Argonne National Lab-
oratory, Argonne, 1991.
[193] A. Griewank, Evaluating Derivatives: Principles and Techniques of Algorithmic Dif-
ferentiation, Frontiers Appl. Math., 19. SIAM, Philadelphia, 2000.

Bibliography
369
[194] A. Griewank and G. Corliss (eds.), Automatic Differentiation of Algorithms: Theory,
Implementation, and Applications, Proceedings of Workshop on Automatic Differenti-
ation at Breckenridge, SIAM, Philadelphia, 1991.
[195] K. Gr¨uner, Fehlerschranken f¨ur lineare Gleichungssysteme, in: [11], pp. 47–55, 1977.
[196] K. Gr¨uner, Allgemeine Rechnerarithmetik und deren Implementierung, Dissertation,
Universit¨at Karlsruhe, 1979.
[197] K. Gr¨uner, Solving complex problems for polynomials and linear systems with veriﬁed
high accuracy, in: [270], pp. 199–220, 1987.
[198] K. Gr¨uner, Solving the Complex Eigenvalue Problem with Veriﬁed High Accuracy, ES-
PRIT project DIAMOND, Doc. No. 03/3-2/1/Kl.p, 1987.
[199] K. Gr¨uner, Solving the complex algebraic eigenvalue problem with veriﬁed high accu-
racy, in: [595], pp. 59–78, 1989.
[200] W. Hahn and K. Mohr, APL/PCXA. Erweiterung der IEEE Arithmetik f¨ur technisch
wissenschaftliches Rechnen, Hanser Verlag, M¨unchen, 1989.
[201] H. Hamada, A new real number representation and its operations, in: [654], Vol. 8,
pp. 153–157, 1987.
[202] R. Hammer, How reliable is the arithmetic of vector computers?, in: [592], pp. 467–
482, 1990.
[203] R. Hammer, Maximal genaue Berechnung von Skalarproduktausdr¨ucken und hochge-
naue Auswertung von Programmteilen, Dissertation, Universit¨at Karlsruhe, 1992.
[204] R. Hammer, PASCAL-XSC: From accurate expressions to the accurate evaluation of
program parts, in: [52], pp. 119–128, 1992.
[205] R. Hammer, M. Hocks, U. Kulisch and D. Ratz, Numerical Toolbox for Veriﬁed Com-
puting I: Basic Numerical Problems, Springer, Berlin Heidelberg New York, 1993.
[206] R. Hammer, M. Hocks, U. Kulisch and D. Ratz, C++ Toolbox for Veriﬁed Computing:
Basic Numerical Problems. Springer, Berlin Heidelberg New York, 1995.
[207] R. Hammer, M. Hocks, U. Kulisch and D. Ratz, Numerical Toolbox for Veriﬁed Com-
puting I: Basic Numerical Problems, MIR, Moskau, 2005 (in Russian).
[208] R. Hammer, M. Neaga and D. Ratz, PASCAL-XSC. New concepts for scientiﬁc com-
putation and numerical data processing, in: [5], pp. 15–44, 1993.
[209] E. R. Hansen, Topics in Interval Analysis, Clarendon Press, Oxford, 1969.
[210] E. R. Hansen, The centered form, in: [209], pp. 102–106, 1969.
[211] E. R. Hansen, An overview of global optimization using interval analysis, in: [426],
pp. 289–307, 1988.
[212] E. R. Hansen, Global Optimization Using Interval Analysis, Marcel Dekker Inc., New
York Basel Hong Kong, 1992.
[213] E. R. Hansen and G. W. Walster, Global Optimization Using Interval Analysis, second
edition, Chapman and Hall, London, 2004.
[214] J. F. Hart, E. W. Cheney, C. L. Lawson, H. J. Maehly, C. K. Mesztenyi, J. R. Rice, H. C.
Thacher Jr. and C. Witzgall,
Computer Approximations, Wiley, New York London
Sydney, 1968.

370
Bibliography
[215] G. Heindl, Inclusions of the range of functions and their derivatives, in: [271], pp. 229–
238, 1991.
[216] G. Heindl, An improved algorithm for computing the product of two machine intervals,
Report IAGMPI - 9314, Universit¨at Wuppertal, 1983.
[217] J. L. Hennessy and D. A. Patterson, Computer Architecture: A Quantitative Approach,
third edition, Elsevier Science, San Francisco, CA, 2003.
[218] A. Hergenhan, Speziﬁkation und Entwurf einer hochleistungsf¨ahigen Gleitkomma-
Architektur, Diplomarbeit, Technische Universit¨at Dresden, 1994.
[219] P. Hertling, A Lower Bound for Range Enclosure in Interval Arithmetic, Centre for
Discrete Mathematics and Theoretical Computer Science Research Report Series, De-
partment of Computger Science, University of Auckland, January 1998.
[220] J. Herzberger, Metrische Eigenschaften von Mengensystemen und einige Anwendun-
gen, Dissertation, Universit¨at Karlsruhe, 1969.
[221] J. Herzberger, Intervallm¨aßige Auswertung von Standardfunktionen in ALGOL-60,
Computing 5 (1970), 377–384.
[222] J. Herzberger, On Schulz’s method in circular complex arithmetic, in: [592], pp. 93–
102, 1990.
[223] J. Herzberger, Iterative inclusion of the inverse matrix, in: [49], pp. 14–26, 1991.
[224] J. Herzberger (ed.):
Topics in Validated Computations, Proceedings of IMACS–
GAMM International Workshop on Validated Numerics, Oldenburg 1993, Elsevier Sci-
ence Publishers (North Holland), Amsterdam, 1994.
[225] J. Herzberger, Basic deﬁnitions and properties of interval arithmetic, in: [224], pp. 1–6,
1994.
[226] J. Herzberger, Wissenschaftliches Rechnen. Eine Einf¨uhrung in das Scientiﬁc Comput-
ing, Akademie Verlag, Berlin, 1995.
[227] J. Herzberger and D. Bethke, Interval Schulz’s method: On the case of an interval
matrix, in: [52], pp. 199–203, 1992.
[228] J. Herzberger and Lj. Petkovi´c, Efﬁcient iterative algorithms for bounding the inverse
of a matrix, Computing 44 (1990), 237–244.
[229] N. J. Higham, The accuracy of ﬂoating-point summation, SIAM J. Sci. Comput. 14
(1993), 783–799.
[230] N. J. Higham, Accuracy and Stability of Numerical Algorithms, second edition, SIAM,
Philadelphia, 2002.
[231] M. Hocks, Innere-Punkt-Methoden und automatische Ergebnisveriﬁkation in der Lin-
earen Optimierung, Dissertation, Universit¨at Karlsruhe, 1995.
[232] B. Hoefﬂinger, Next-generation ﬂoating-point arithmetic for top-performance PCs, in:
Conference Proceedings of the 1995 Silicon Valley Personal Computer Design Confer-
ence and Exposition, pp. 319–325, 1995.
[233] T. Hoff, How children accumulate numbers or why we need a ﬁfth ﬂoating-point op-
eration, in: Jahrbuch ¨Uberblicke Mathematik, pp. 219–222, Vieweg, Braunschweig
Wiesbaden, 1993.
[234] W. Hofschuster, Zur Berechnung von Funktionseinschließungen bei speziellen Funk-
tionen der mathematischen Physik, Dissertation, Universit¨at Karlsruhe, 2000.

Bibliography
371
[235] W. Hofschuster, F. Blomquist and W. Kr¨amer, Vermeidung von ¨Uber- und Unter-
lauf und Verbesserung der Genauigkeit bei reeller und komplexer staggered Intervall-
Arithmetik, preprint BUW-WRSWT 2007, Universit¨at Wuppertal, 2007.
[236] W. Hofschuster and W. Kr¨amer, Rechnergest¨utztes Fehlerkalk¨ul mit Anwendung auf ein
genaues Tabellenverfahren, preprint des IWRMM, Universit¨at Karlsruhe, 1996.
[237] W. Hofschuster and W. Kr¨amer, A computer oriented approach to get sharp reliable
error bounds, Reliable Computing 3 (1997), 239–248.
[238] W. Hofschuster and W. Kr¨amer, C-XSC 2.0 – A C++ class library for extended
scientiﬁc computing, in: Numerical Software with Result Veriﬁcation, edited by R.
Alt, A. Frommer, B. Kearfott and W. Luther, pp. 15–35, Springer Lecture Notes in
Computer Science 2991, 2004. For more information see: http://www.math.
uni-wuppertal.de/∼xsc/ or http://www.xsc.de/.
[239] W. Hofschuster, W. Kr¨amer, M. Lerch, G. Tischler and J. Wolff von Gudenberg, filib+
+ a Fast Interval Library Supporting Containment Computations, TOMS, to appear.
[240] W. N. Holmes, Computers and People, Wiley & Sons, Hoboken, New Jersey, 2006.
[241] W. N. Holmes, Binary arithmetic, in: Computer, Vol. 40, pp. 1–4, IEEE Computer
Society, 2007.
[242] T. E. Hull and A. Abrham, Properly rounded variable precision square root, ACM Trans.
on Math. Software 11:3 (1985), 229–237.
[243] T. E. Hull, A. Abrham, M. S. Cohen, A. F. X. Curley, D. A. Penny and J. T. M. Sawchuk,
Numerical turing, ACM SIGNUM Newsletter 20:3 (1985), 26–34.
[244] M. Iri, Simultaneous computation of functions, partial derivatives and estimates for
rounding errors – complexity and practicality, Japan Journal of Applied Mathematics
1 (1984), 223–252.
[245] K.-U. Jahn (ed.), Computernumerik mit Ergebnisveriﬁkation, Problemseminar, Tech-
nische Hochschule Leipzig, 13.–15. M¨arz 1991. Proceedings in Wissenschaftliche
Zeitschrift der Technischen Hochschule Leipzig, Jahrgang 15, Heft 6, 1991.
[246] C. Jansson, Zur linearen Optimierung mit unscharfen Daten, Dissertation, Universit¨at
Kaiserslautern, 1985.
[247] C. Jansson, Guaranteed error bounds for the solution of linear systems, in: [592],
pp. 103–110, 1990.
[248] C. Jansson, A Fast Direct Method for Computing Veriﬁed Inclusions, Berichte
des Forschungsschwerpunktes Informations- und Kommunikationstechnik, Technische
Universit¨at Hamburg-Harburg, Bericht 90.4, 1990.
[249] C. Jansson, Interval linear systems with symmetric matrices, skew-symmetric matrices,
and dependencies in the right-hand side, Computing 46 (1991), 265–274.
[250] C. Jansson, A geometric approach for computing a posteriori error bounds for the so-
lution of a linear system, Computing 47 (1991), 1–9.
[251] C. Jansson, On self-validating methods for optimization problems, in: [224], pp. 381–
438, 1994.
[252] C. Jansson, A branch-and-bound algorithm for bound constrained optimization prob-
lems without derivatives, J. Glob. Optim. 7:3 (1995), 297–331.
[253] C. Jansson, Convex-concave extensions, BIT 402 (2000), 291–313.

372
Bibliography
[254] C. Jansson, Quasiconvex relaxations based on interval arithmetic, Lin. Alg. Appl. 324
(2001), 27–53.
[255] C. Jansson and O. Kn¨uppel, A branch-and-bound algorithm for bound constrained op-
timization problems without derivatives, J. Glob. Optim. 7:3 (1995), 297–331.
[256] C. Jansson and S. M. Rump, Rigorous sensitivity analysis for real symmetric matrices
with uncertain data, in: [271], pp. 293–316, 1991.
[257] P. Januschke, Oberon-XSC, Eine Programmiersprache f¨ur das wissenschaftliche Rech-
nen, Dissertation, Universit¨at Karlsruhe, 1998.
[258] K. Jensen and N. Wirth, Pascal User Manual and Report, ISO Pascal Standard, third
edition, Springer, Berlin Heidelberg New York, 1985.
[259] D. W. Juedes, A taxonomy of automatic differentiation tools, in: Automatic Differenti-
ation of Algorithms: Theory, Implementation and Applications, edited by A. Griewank
and G. F. Corliss, pp. 315–329, SIAM, Philadelphia, Penn., 1991.
[260] W. Kahan, Further remarks on reducing truncation errors, Comm. ACM 8:1 (1965), 40.
[261] W. Kahan, A More Complete Interval Arithmetic, Lecture Notes prepared for a summer
course at the University of Michigan, June 17–21, 1968.
[262] W. Kahan, A survey on error analysis, in: Proceedings of the IFIP Congress, pp. 1214–
1239, Information Processing 71, North-Holland, Amsterdam, 1972.
[263] W. Kahan, Branch cuts for complex elementary functions, in: The State of the Art in
Numerical Analysis, edited by A. Iserles and M. J. D. Powell, pp. 165–211, Clarendon
Press, Oxford, 1987.
[264] W. Kahan, Doubled Precision IEEE Standard 754 Floating-Point Arithmetic, Mini-
Course on “The Regrettable Failure of Automated Error Analysis”, Conference on
Computers and Mathematics, MIT, June 13, 1989.
[265] S. A. Kalmykov, J. I. Shokin and S. C. Juldashev, Methods of Interval Analysis, Novosi-
birsk, 1986 (in Russian).
[266] A. Karatsuba and Y. Ofman, Multiplication of multidigit numbers on automata, Soviet
Physics Dokl. 7 (1963), 595–596.
[267] E. Kaucher, ¨Uber metrische und algebraische Eigenschaften einiger beim numerischen
Rechnen auftretender R¨aume, Dissertation, Universit¨at Karlsruhe, 1973.
[268] E. Kaucher, Algebraische Erweiterungen der Intervallrechnung unter Erhaltung der
Ordnungs- und Verbandsstrukturen, in: Grundlagen der Computerarithmetik, edited
by R. Albrecht and U. Kulisch, pp. 65–79, Computing Supplementum 1, Springer,
Wien New York, 1977.
[269] E. Kaucher, ¨Uber Eigenschaften und Anwendungsm¨oglichkeiten der erweiterten Inter-
vallrechnung und des hyperbolischen Fastk¨orpers ¨uber R, in: Grundlagen der Comput-
erarithmetik, edited by R. Albrecht and U. Kulisch, pp. 81–94, Computing Supplemen-
tum 1, Springer, Wien New York, 1977.
[270] E. Kaucher, U. Kulisch and Ch. Ullrich (eds.), Computerarithmetic: Scientiﬁc Compu-
tation and Programming Languages, B. G. Teubner, Stuttgart, 1987.
[271] E. Kaucher, G. Mayer and S. M. Markov (eds.), Computer Arithmetic, Scientiﬁc Com-
putation and Modelling, Proceedings of SCAN-90, IMACS Annals on Computing and
Applied Mathematics 12, 1992, J. C. Baltzer AG, Basel, 1991.

Bibliography
373
[272] E. Kaucher and W. L. Miranker, Self-Validating Numerics for Function Space Prob-
lems, Computations with Guarantees for Differential and Integral Equations, Aca-
demic Press, Orlando, 1984.
[273] R. B. Kearfott, A review of techniques in the veriﬁed solution of constrained global
optimization problems, in: [276], pp. 23–59, 1996.
[274] R. B. Kearfott, A speciﬁc proposal for interval arithmetic in Fortran, March 1996, avail-
able from http://interval.usl.edu/F90/f96-pro.asc.
[275] R. B. Kearfott, K. D. Dawande and C. Hu, Algorithm 737: INTLIB: A portable Fortran
77 interval standard function library, ACM Transactions on Mathematical Software 20
(1994), 447–459.
[276] R. B. Kearfott and V. Kreinovich (eds.), Applications of Interval Computations,
Kluwer, Dordrecht, 1996.
[277] R. Kelch, Ein adaptives Verfahren zur numerischen Quadratur mit automatischer
Ergebnisveriﬁkation, Dissertation, Universit¨at Karlsruhe, 1989.
[278] R. Kelch, Self-validating numerical quadrature, in: [595], pp. 162–202, 1989.
[279] R. Kelch, An adaptive procedure for numerical quadrature with automatic result veriﬁ-
cation, in: [592], pp. 301–317, 1990.
[280] R. Kelch, Numerical quadrature by extrapolation with automatic result veriﬁcation, in:
[5], pp. 143–185, 1993.
[281] J. Kernhof, Ch. Baumhof, B. H¨ofﬂinger, U. Kulisch, S. Kwee, P. Schramm, M. Selzer
and Th. Teufel, A CMOS Floating-Point Processing Chip for Veriﬁed Exact Vector
Arithmetic, ESSIRC 94, Ulm, Sept. 1994.
[282] I. Kießling, M. Lowes and A. Paulik, Genaue Rechnerarithmetik – Initervallrechnung
und Programmieren mit PASCAL-SC, B. G. Teubner, Stuttgart, 1988.
[283] R. Kirchner and U. Kulisch, Schaltungsanordnung und Verfahren zur schnellen
Berechnung von Summen und Skalarprodukten von Gleitkommazahlen mit maximaler
Genauigkeit mittels Pipelinetechnik, in: Beitr¨age zur Angewandten Mathematik und
Statistik, edited by M. J. Beckmann, K. W. Gaede and K. Ritter, pp. 139–177, Hansa,
M¨unchen Wien, 1987.
[284] R. Kirchner and U. Kulisch, Arithmetic for vector processors, in: [654], Vol. 8,
pp. 256–269, 1987.
[285] R. Kirchner and U. Kulisch, Accurate arithmetic for vector processors, Journal of Par-
allel and Distributed Computing 5 (1988), 250–270.
[286] R. Kirchner and U. Kulisch, Fast and accurate computation of sums and inner products,
in: Computational Techniques and Applications: CTAC–87, edited by J. Noye and C.
Fletcher, pp. 3–28, Proceedings of CTAC–87 in Sydney (Australia), North-Holland,
1988.
[287] R. Kirchner and U. Kulisch, Hardware support for interval arithmetic, Reliable Com-
puting 12:3 (2006), 225–237.
[288] R. Klatte, U. Kulisch, C. Lawo, M. Rauch and A. Wiethoff, C-XSC – A C++ Class
Library for Extended Scientiﬁc Computing, Springer, Berlin Heidelberg New York,
1993. See also http://www.math.uni-wuppertal.de/∼xsc/ ord http:
//www.xsc.de/.

374
Bibliography
[289] R. Klatte, U. Kulisch, M. Neaga, D. Ratz and Ch. Ullrich, PASCAL-XSC –
Sprachbeschreibung mit Beispielen, Springer, Berlin Heidelberg New York, 1991.
See also http://www.math.uni-wuppertal.de/∼xsc/ or http://www.
xsc.de/.
[290] R. Klatte, U. Kulisch, M. Neaga, D. Ratz and Ch. Ullrich, PASCAL-XSC – Lan-
guage Reference with Examples, Springer, Berlin Heidelberg New York, 1992.
See also http://www.math.uni-wuppertal.de/∼xsc/ or http://www.
xsc.de/.
[291] R. Klatte, U. Kulisch, M. Neaga, D. Ratz and Ch. Ullrich, PASCAL-XSC – Lan-
guage Reference with Examples, MIR, Moskau, 1995, third edition 2006 (in Russian).
See also http://www.math.uni-wuppertal.de/∼xsc/ or http://www.
xsc.de/.
[292] W. Klein, Veriﬁed Solution of Linear Systems with Band-Shaped Matrices, DIAMOND
Workpaper, Doc. No. 03/3-3/3, January 1987.
[293] W. Klein, Data Structure and Symbolic LU-Factorization for General Sparse Matrices,
DIAMOND Workpaper, Doc. No. 03/3-7/3, March 1987.
[294] W. Klein, Veriﬁed results for linear systems with sparse matrices, in: [595], pp. 137–
161, 1989.
[295] W. Klein, Zur Einschließung der L¨osung von linearen und nichtlinearen Fredholmschen
Integralgleichungssystemen zweiter Art, Dissertation, Universit¨at Karlsruhe, 1990.
[296] G. Klotz, Faktorisierung von Matrizen mit maximaler Genauigkeit, Dissertation, Uni-
versit¨at Karlsruhe, 1987.
[297] U. Klug, Veriﬁed inclusions for eigenvalues and eigenvectors of real symmetric matri-
ces, in: [592], pp. 111–125, 1990.
[298] A. Kn¨ofel, Hardwareentwurf eines Rechenwerks f¨ur semimorphe Skalar- und Vektor-
operationen unter Ber¨ucksichtigung der Anforderungen veriﬁzierender Algorithmen,
Dissertation, Universit¨at Karlsruhe, 1991.
[299] A. Kn¨ofel, Advanced circuits for the computation of accurate scalar products, in: [271],
pp. 63–67, 1991.
[300] A. Kn¨ofel, Fast hardware units for the computation of accurate dot products, in: [654],
Vol. 10, pp. 70–74, 1991.
[301] A. Kn¨ofel, A hardware kernel for scientiﬁc/engineering computations, in: [5], pp. 549–
570, 1993.
[302] O. Kn¨uppel, PROFIL/BIAS – a fast interval library, Computing 53 (1994), 277–287.
[303] D. E. Knuth, Euler’s constant to 1271 places, Math. Comp. 16 (1962), pp. 275–281.
[304] D. E. Knuth, The Art of Computer Programming, Vol. 2: Seminumerical Algorithms,
second edition, Addison-Wesley, Reading, Massachusetts, 1981.
[305] M. Koeber, L¨osungseinschließung bei Anfangswertproblemen f¨ur quasilieare hyper-
bolische Differentialgleichungen, Dissertation, Universit¨at Karlsruhe, 1997.
[306] R. Kolla, A. Vodopivec and J. Wolff von Gudenberg, The IAX Architecture – Interval
Arithmetic Extension, Report No. 225, Institut f¨ur Informatik, Universit¨at W¨urzburg,
1999.

Bibliography
375
[307] R. Kolla, A. Vodopivec and J. Wolff von Gudenberg, Splitting double precision FPUs
for single precision interval arithmetic, in: ARCS’99 Workshops zur Architektur von
Rechensystemen, edited by W. Erhard, K.-E. Großpietsch, W. Koch, E. Maehle and E.
Zehendner, pp. 5–16, Universit¨at Jena, 1999.
[308] S. K¨onig, On the inﬂation parameter used in self-validating methods, in: [592],
pp. 127–132, 1990.
[309] I. Koren, Computer Arithmetic Algorithms, Prentice Hall, Englewood Cliffs, New Jer-
sey, 1993.
[310] M Koshelev and V. Kreinovich, Interval Computations, 1997, available from http:
//cs.utep.edu/interval-comp.html.
[311] W. Kr¨amer, Inverse Standardfunktionen f¨ur reelle und komplexe Intervallargumente
mit a priori Fehlerabsch¨atzungen f¨ur beliebige Datenformate, Dissertation, Universit¨at
Karlsruhe, 1987.
[312] W. Kr¨amer, Inverse standard functions for real and complex point and interval argu-
ments with dynamic accuracy, Computing Supplementum 6 (1988), 185–211.
[313] W. Kr¨amer, Mehrfachgenaue reelle und intervallm¨aßige Staggered-Correction Arith-
metik mit zugeh¨origen Standardfunktionen, in: Bericht des Instituts f¨ur Angewandte
Mathematik, pp. 1–80, Universit¨at Karlsruhe, 1988.
[314] W. Kr¨amer, Fehlerschranken f¨ur h¨auﬁg auftretende Approximationsausdr¨ucke, ZAMM
69 (1989), T44–T47.
[315] W. Kr¨amer, Genaue Auswertung von Polynomen in mehreren Variablen, DFG-Bericht
zum Forschungsvorhaben Ku 155/12-1, 1989.
[316] W. Kr¨amer, Berechnung der Gammafunktion Γ(X) f¨ur reelle Punkt- und Intervallar-
gumente, ZAMM 70 (1990), T581–T584.
[317] W. Kr¨amer, Highly accurate evaluations of program parts with applications, in: Contri-
butions to Computer Arithmetic and Self-Validating Numerical Methods, edited by C.
Ullrich, pp. 397–409, J. C. Baltzer AG, Scientiﬁc Publishing Co., IMACS, 1990.
[318] W. Kr¨amer, Computation of veriﬁed bounds for elliptic integrals, in: Proceedings of the
International Symposium on Computer Arithmetic and Scientiﬁc Computation, Olden-
burg 1991 (SCAN91), edited by J. Herzberger and L. Atanassova, Elsevier Science
Publishers (North-Holland), Amsterdam, 1992.
[319] W. Kr¨amer, Evaluation of polynomials in several variables with high accuracy, in: Com-
puter Arithmetic, Scientiﬁc Computation and Mathematical Modelling, edited by E.
Kaucher, S. Markov and G. Mayer, pp. 239–249, I. C. Baltzer AG, Scientiﬁc Publish-
ing Co., IMACS, 1991.
[320] W. Kr¨amer, Einschluß eines Paares konjugiert komplexer Nullstellen eines reellen
Polynoms, ZAMM 71 (1991), T820–T824.
[321] W. Kr¨amer, Genaue Berechnung von komplexen Polynomen in mehreren Variablen, in:
[245], pp. 401–407, 1991.
[322] W. Kr¨amer, Veriﬁed solution of eigenvalue problems with sparse matrices, in: [600],
pp. 32–33, 1991.
[323] W. Kr¨amer, PASCAL-XSC Module for Multiple-Precision Interval Operations and
Functions, Universit¨at Karlsruhe, 1991.

376
Bibliography
[324] W. Kr¨amer, Die Berechnung von Standardfunktionen in Rechenanlagen, in: Jahrbuch
¨Uberblicke Mathematik, edited by S. D. Chatterji, B. Fuchssteiner, U. Kulisch, R. Liedl
and W. Purkert, pp. 97–115, Vieweg, Braunschweig, 1992.
[325] W. Kr¨amer, Eine portable Langzahl- und Langzahlintervallarithmetik mit Anwendun-
gen, ZAMM 73 (1992), T849–T853.
[326] W. Kr¨amer, Veriﬁed solution of eigenvalue problems with sparse matrices, in: Compu-
tational and Applied Mathematics I. Algorithms and Theory, edited by C. Brezinsky,
pp. 1–11, Proceedings of the 13th IMACS World Congress in Dublin (Ireland), Elsevier
Science Publishers B. V., Amsterdam, 1992.
[327] W. Kr¨amer, Multiple-precision computations with result veriﬁcation, in: [5], pp. 325–
356, 1993.
[328] W. Kr¨amer, Constructive error analysis, Journal of Universal Computer Science 4:2
(1998), 147–163.
[329] W. Kr¨amer and A. Bantle, Automatic forward error analysis for ﬂoating-point algo-
rithms, Reliable Computing 7:4 (2001), 321–340.
[330] W. Kr¨amer and B. Barth, Computation of interval bounds for Weierstrass’ elliptic func-
tion, in: [10], pp. 147–159, 1993.
[331] W. Kr¨amer, F. Blomquist and W. Hofschuster, Vermeidung von ¨Uber- und Unter-
lauf und Verbesserung der Genauigkeit bei reeller und komplexer staggered Intervall-
Arithmetik, preprint BUW-WRSWT 2007, Universit¨at Wuppertal, 2007.
[332] W. Kr¨amer, U. Kulisch and R. Lohner, Numerical Toolbox for Veriﬁed Computing II:
Theory, Algorithms and Pascal-XSC Programs, electronically available via Rudolf.
Lohner@math.uka.de (Vol. I see [205, 206]).
[333] W. Kr¨amer and W. Walter, FORTRAN-SC: A FORTRAN Extension for Engineer-
ing/Scientiﬁc Computation with Access to ACRITH, General Information Notes and
Sample Programs, 1–51, IBM Deutschland GmbH, Stuttgart, 1989.
[334] W. Kr¨amer and J. Wolff von Gudenberg (eds.), Scientiﬁc Computing, Validated Nu-
merics, Interval Methods, Kluwer Acadamic Publishers, New York Boston Dordrecht
London Moscow, 2001.
[335] W. Kr¨amer and J. Wolff von Gudenberg, Extended Interval Power Function, Proceed-
ings of Validated Computing, Reliable Computing, 2003.
[336] R. Krawczyk, Newton-Algorithmen zur Bestimmung von Nullstellen mit Fehler-
schranken, Computing 4 (1969), 187–201.
[337] R. Krawczyk and A. Neumaier, Interval slopes for rational functions and associated
centered forms, SIAM Journal on Numerical Analysis 22 (1985), 604–616.
[338] V. Kreinovich, A. Lakeyev, J. Rohn and P. Kahl, Computaional Complexity and Feasi-
bility of Data Processing and Interval Computations, Kluwer, Dordrecht, 1998.
[339] V. Kreinovich and J. Wolff von Gudenberg, An optimality criterion for arithmetic on
complex sets, Geombinatorics X:1 (2000), 31–37.
[340] V. Kreinovich and J. Wolff von Gudenberg, A full function-based calculus of directed
and undirected intervals: Markov’s interval arithmetic revisited, Numerical Algorithms
37:1–4 (2004), 417–428.
[341] F. Kr¨uckeberg, Ordinary differential equations, in: [209], 1969.

Bibliography
377
[342] F. Kr¨uckeberg, Arbitrary accuracy with variable precision arithmetic, in: [453], pp. 95–
101, 1985.
[343] K. Kubota and M. Iri, PADRE2 – a FORTRAN precompiler yielding error estimates
and second derivatives, in: [194], pp. 251–262, 1991.
[344] O. K¨uhn, Rigorously computed orbits of dynamical systems without the wrapping ef-
fect, Computing 61 (1998), 47–67.
[345] O. K¨uhn, Towards an optimal control of the wrapping effect, in: [153], pp. 43–51,
1999.
[346] U. Kulisch, Grundz¨uge der Intervallrechnung, in: Jahrbuch ¨Uberblicke Mathematik 2,
pp. 51–98, Bibliographisches Institut, Mannheim, 1969.
[347] U. Kulisch, An axiomatic approach to rounded computations, TS Report No. 1020,
Mathematics Research Center, University of Wisconsin, Madison, Wisconsin, 1969,
and Numerische Mathematik 19 (1971), 1–17.
[348] U. Kulisch, On the concept of a screen, Mathematics Research Center, The University
of Wisconsin, Madison, Wisconsin, Technical summary Report Nr. 1084, 1–12, 1970,
and ZAMM 53 (1973), 115–119.
[349] U. Kulisch, Rounding invariant structures, Mathematics Research Center, The Univer-
sity of Wisconsin, Madison, Wisconsin, Technical Summary Report Nr. 1103, 1970,
1–47.
[350] U. Kulisch, Interval arithmetic over completely ordered ringoids, Mathematics Re-
search Center, The University of Wisconsin, Madison, Wisconsin, Technical Summary
Report Nr. 1105, 1–56, 1970.
[351] U. Kulisch, Implementation and formalization of ﬂoating-point arithmetics, IBM
T. J. Watson-Research Center, Report Nr. RC 4608, 1–50, 1973. Invited talk at the
Carath`eodory Symposium, September 1973 in Athens, published in: The Greek Math-
ematical Society, C. Caratheodory Symposium, 328–369, 1973, and in Computing 14
(1975), 323–348.
[352] U. Kulisch, ¨Uber die Arithmetik von Rechenanlagen, in: Jahrbuch ¨Uberblicke Math-
ematik 1975, pp. 69–198, Wissenschaftsverlag, Bibliographisches Institut, Mannheim
Wien Z¨urich, 1975.
[353] U. Kulisch, Grundlagen des Numerischen Rechnens – Mathematische Begr¨undung der
Rechnerarithmetik, Informatik 19, Bibliographisches Institut, Mannheim Wien Z¨urich,
1976.
[354] U. Kulisch, Mathematical Foundation of Computer Arithmetic, 3rd Symposium on
Computer Arithmetic of the IEEE Computer Society in Dallas/Texas, 1–13, published
in Transactions on Computers, Vol. C-26, 610–621, 1977.
[355] U. Kulisch (ed.), PASCAL-SC: A PASCAL Extension for Scientiﬁc Computation, In-
formation Manual and Floppy Disks, Version IBM PC/AT, Operating System DOS,
Wiley-Teubner series in computer science, B. G. Teubner Verlag, Stuttgart, 1987.
[356] U. Kulisch (ed.), PASCAL-SC: A Pascal extension for Scientiﬁc Computation, Informa-
tion Manual and Floppy Disks. Version ATARI ST., B. G. Teubner, Stuttgart, 1987.
[357] U. Kulisch (ed.), Wissenschaftliches Rechnen mit Ergebnisveriﬁkation – Eine Einf¨uh-
rung, Vortr¨age einer Tagung in Karlsruhe 1988, ausgearbeitet von S. Ge¨org, R. Ham-

378
Bibliography
mer und D. Ratz, Vol. 58, Akademie Verlag, Berlin, und Vieweg Verlagsgesellschaft,
Wiesbaden, 1989.
[358] U. Kulisch, Numerik mit automatischer Ergebnisveriﬁkation, Jahrbuch ¨Uberblicke
Mathematik 1993, pp. 199–218, Vieweg Verlag, 199, and GAMM-Mitteilungen 1
(1994), 39–58.
[359] U. Kulisch, How children accumulate numbers – or: Why we need a ﬁfth ﬂoating-point
operation, in: Jahrbuch ¨Uberblicke Mathematik 1993, pp. 219–222, Vieweg, 1993.
[360] U. Kulisch, Memorandum ¨uber Computer, Arithmetik und Numerik. Anl¨aßlich des 85.
Geburtstages von K. Zuse, in: Jahrbuch ¨Uberblicke Mathematik, pp. 1–59, Vieweg,
Braunschweig Wiesbaden, 1995.
[361] U. Kulisch, Numerical algorithms with automatic result veriﬁcation, in: The Mathemat-
ics of Numerical Algorithms, edited by J. Renegar, M. Shub and S. Smale, pp. 471–502,
Lectures in Applied Mathematics 32, AMS, 1996.
[362] U. Kulisch, Advanced arithmetic for the digital computer – design of arithmetic units,
in: Electronic Notes in Theoretical Computer Science, pp. 1–72, Elsevier, Amsterdam,
1999. http://www.elsevier.nl/locate/entcs/volume24.html.
[363] U. Kulisch, Advanced arithmetic for the digital computer – interval arithmetic revisited,
in: [366], pp. 15–75, 2001.
[364] U. Kulisch, Advanced Arithmetic for the Digital Computer – Design of Arithmetic
Units, Springer, Wien New York, 2002.
[365] U. Kulisch and R. Kirchner, Arithmetic for vector processors, in: Proceedings of the
8th Symposium on Computer Arithmetic of the IEEE Computer Society, Como, Italy,
pp. 256–269, IEEE Computer Society Press, Washington, D.C., 1987.
[366] U. Kulisch, R. Lohner and A. Facius (eds.), Perspectives on Enclosure Methods,
Springer, Wien New York, 2001.
[367] U. Kulisch and W. L. Miranker, Computer Arithmetic in Theory and Practice, Aca-
demic Press, New York, 1981.
[368] U. Kulisch and W. L. Miranker (eds.), A New Approach to Scientiﬁc Computation, Pro-
ceedings of Symposium held at IBM Research Center, Yorktown Heights, N.Y., 1982,
Academic Press, New York, 1983.
[369] U. Kulisch and W. L. Miranker, The arithmetic of the digital computer: A new ap-
proach, IBM Research Center RC 10580, 1–62, 1984, SIAM Review 28:1 (1986), 1–40.
[370] U. Kulisch and L. B. Rall, Numerics with automatic result veriﬁcation, Mathematics
and Computers in Simulation 35 (1993), 435–450.
[371] U. Kulisch, S. M. Rump and J. Wolff von Gudenberg, Accuracy of the IBM /370
ﬂoating-point arithmetic and possible improvements, Technical Report, IBM, 1982.
[372] U. Kulisch and H. J. Stetter (eds.), Scientiﬁc Computation with Automatic Result Veri-
ﬁcation, Computing Supplementum 6, Springer, Wien New York, 1988.
[373] U. Kulisch, T. Teufel and B. Hoefﬂinger, Genauer und trotzdem schneller: Ein neuer
Coprozessor f¨ur hochgenaue Matrix- und Vektoroperationen. Titelgeschichte, Elek-
tronik 26 (1994), 52–56.
[374] U. Kulisch and Ch. Ullrich (eds.), Wissenschaftliches Rechnen und Programmierspra-
chen, Berichte des German Chapter of the ACM 10, B. G. Teubner, Stuttgart, 1982.

Bibliography
379
[375] J.-R. Lahmann,
Eine Methode zur Einschließung von Eigenpaaren nichtselb-
stadjungierter Eigenwertprobleme und ihre Anwendung auf die Orr–Sommerfeld-
Gleichung, Dissertation, Universit¨at Karlsruhe, 1999.
[376] J.-R. Lahmann and M. Plum, A computer-assisted instability proof for the Orr–
Sommerfeld equation with Blasius proﬁle, ZAMM 84:3 (2004), 188–204.
[377] Ch. Lawo, C-XSC – A programming environment for extended scientiﬁc computation,
in: [600], p. 34, 1991.
[378] Ch. Lawo, C-XSC. Eine Programmierumgebung f¨ur veriﬁziertes Rechnen in C++, Talk
on SCAN’91 conference, Oldenburg, 1991.
[379] Ch. Lawo, C-XSC. Eine objektorientierte Programmierumgebung f¨ur veriﬁziertes wis-
senschaftliches Rechnen, Dissertation, Universit¨at Karlsruhe, 1992.
[380] Ch. Lawo, C-XSC. A programming environment for veriﬁed scientiﬁc computing and
numerical data processing, in: [5], pp. 71–86, 1993.
[381] M. Lerch and J. Wolff von Gudenberg, Expression templates for dotproduct expres-
sions, Interval 98, Reliable Computing 5:1 (1999), 69–80.
[382] M. Lerch and J. Wolff von Gudenberg, Implementation and test of a library for ex-
tended interval arithmetic, in: RNC4 Fourth Real Numbers and Computers, edited by
P. Kornerup, pp. 111–124, Online Proceedings RNC4, 2000.
[383] H. Leuprecht and W. Oberaigner, Parallel algorithms for the rounding exact summation
of ﬂoating-point numbers, Computing 28 (1982), 89–104.
[384] P. Lichter, Realisierung eines VLSI-Chips f¨ur das Gleitkomma-Skalarprodukt der
Kulisch-Arithmetik, Diplomarbeit, Fachbereich 10, Angewandte Mathematik und In-
formatik, Universit¨at des Saarlandes, 1988.
[385] S. Linnainmaa, Analysis of some known methods of improving the accuracy of
ﬂoating-point sums, BIT 14 (1974), 167–202.
[386] S. Linnainmaa, Software for double-precision ﬂoating-point computations, ACM Trans.
on Math. Software 7:3 (1981), 272–283.
[387] R. Lohner, Enclosing the solutions of ordinary initial and boundary value problems, in:
[270], pp. 255–286, 1987.
[388] R. Lohner, Einschließung der L¨osung gew¨ohnlicher Anfangs- und Randwertaufgaben
und Anwendungen, Dissertation, Universit¨at Karlsruhe, 1988.
[389] R. Lohner, Precise evaluation of polynomials in several variables, in: [372], pp. 139–
148, 1988.
[390] R. Lohner, Enclosing all eigenvalues of symmetric matrices, in: [595], pp. 87–103,
1989.
[391] R. Lohner, Computation of guaranteed enclosures for the solutions of ordinary ini-
tial and boundary value problems, in: Computational Ordinary Differential Equations,
edited by J. R. Cash and I. Gladwell, pp. 425–435, Clarendon Press, Oxford, 1992.
[392] R. Lohner, Interval arithmetic in staggered correction format, in: [5], pp. 301–321,
1993.
[393] R. Lohner and J. Wolff von Gudenberg, Complex interval division with maximum ac-
curacy, in: Proceedings of the 7th IEEE Symposium on Computer Arithmetic in Urbana
(Illinois), pp. 332–336, IEEE Comp. Soc., 1985.

380
Bibliography
[394] B. Lortz, Eine Langzahlarithmetik mit optimaler einseitiger Rundung, Dissertation,
Universit¨at Karlsruhe, 1971.
[395] G. Ludyk, CAE von dynamischen Systemen:
Analyse, Simulation, Entwurf von
Regelungssystemen, Springer, Berlin, 1990.
[396] M. Malcolm, On accurate ﬂoating-point summation, Comm. ACM 14 (1971), 731–736.
[397] S. M. Markov, Scientiﬁc Computation and Mathematical Modeeling, DATECS Pub-
lishing, Soﬁa, 1993.
[398] P. W. Markstein, Computation of elementary functions on the IBM RISC system/6000
processor, IBM Journal of Research and Development 34:1 (1990), 111–119.
[399] G. Mayer, On the convergence of powers of interval matrices, Linear Algebra Appl. 58
(1984), 201–216.
[400] G. Mayer, On the vonvergence of powers of interval matrices (2), Numer. Math. 46
(1985), 69–83.
[401] G. Mayer, Regul¨are Zerlegungen und der Satz von Stein und Rosenberg f¨ur Intervall-
matrizen, Habilitationsschrift, Universit¨at Karlsruhe, 1986.
[402] G. Mayer, Enclosing the solutions of linear equations by interval iterative processes,
in: [372], pp. 47–58, 1988.
[403] G. Mayer, Grundbegriffe der Intervallrechnung, in: [357], pp. 101–117, 1989.
[404] G. Mayer, Old and new Aspects for the interval Gaussian algorithm, in: [271], pp. 329–
349, 1991.
[405] G. Mayer, Some remarks on two interval-arithmetic modiﬁcations of the Newton
method, Computing 48, pp. 125–128, 1992.
[406] G. Mayer, Enclosures for eigenvalues and eigenvectors, in: [52], pp. 49–67, 1992.
[407] G. Mayer, Result veriﬁcation for eigenvectors and eigenvalues, in: [224], pp. 209–276,
1994.
[408] G. Mayer, Epsilon-inﬂation in veriﬁcation algorithms, J. Com. Appl. Math. 43 (1995),
147–169.
[409] G. Mayer, Epsilon-inﬂation with contractive interval functions, Appl. Math. 43 (1998),
241–254.
[410] G. Mayer, Beitr¨age zur Intervallrechnung, in: Lexikon der Mathematik, Spektrum,
Mannheim, 2000.
[411] G. Mayer and A. Frommer, A multisplitting method for veriﬁcation and enclosure on a
parallel computer, in: [592], pp. 483–497, 1990.
[412] G. Mayer and J. Rohn, On the applicability of the interval Gaussian algorithm, Reliable
Computing 4 (1998), 205–222.
[413] G. Mayer and J. Wolff von Gudenberg, Stichw¨orter zur Intervallrechnung, in: Lexikon
der Mathematik, Spektrum, Mannheim, 2000.
[414] J. Mayer, A Crout ILU preconditioner with pivoting and row permutation, preprint Nr.
05/04, Universit¨at Karlsruhe, Institut f¨ur Wissenschaftliches Rechnen, 2005.
[415] O. Mayer, ¨Uber die in der Intervallrechnung auftretenden R¨aume und einige Anwen-
dungen, Dissertation, Universit¨at Karlsruhe, 1968.

Bibliography
381
[416] E. J. McShane and T. A. Botts, Real Analysis, Van Nostrand-Reinhold, Princeton, New
Jersey, 1959.
[417] T. Meis, Brauchen wir eine Hochgenauigkeitsarithmetik?, in: Elektronische Rechenan-
lagen, pp. 19–23, Carl Hanser, M¨unchen, 1987.
[418] R. K. Meyer and D. S. Schmidt (eds.), Computer Aided Proofs in Analysis, IMA Vol-
umes in Mathematics and Its Applications 28, Springer, New York, 1991.
[419] C. Miranda, Un’osservazione su un teorema di Brouwer, Bol. Un. Mat. Ital. Ser. II 3
(1941), 5–7.
[420] W. L. Miranker and R. A. Toupin, Accurate Scientiﬁc Computations, Lecture Notes in
Computer Science 235, Springer, Berlin, 1986 (Symposium Bad Neuenahr, Germany,
1985).
[421] O. M¨oller, Quasi double precision in ﬂoating-point addition, BIT 5 (1965), 37–50.
[422] R. E. Moore, Interval Arithmetic and Automatic Error analysis in Digital Computing,
Thesis, Stanford University, October 1962.
[423] R. E. Moore, Interval Analysis, Prentice Hall Inc., Englewood Cliffs, New Jersey, 1966.
[424] R. E. Moore, Methods and Applications of Interval Analysis, SIAM, Philadelphia,
Pennsylvania, 1979.
[425] R. E. Moore, Computational Functional Analysis, Ellis Horwood, Chichester, 1985.
[426] R. E. Moore (ed.), Reliability in Computing: The Role of Interval Methods in Scientiﬁc
Computing, Perspectives in Computing 19, Proceedings of the Conference at Columbus
in Ohio, September 8–11, 1987, Academic Press, San Diego, 1988.
[427] R. E. Moore, Variable precision computing. Strategies for improving efﬁciency, in:
[600], pp. 73–74, 1991.
[428] V. Moynihan, Techniques for generating accurate eigensolutions in ADA, in: [595],
pp. 79–86, 1989.
[429] F. Mr´az, Nonnegative solutions of interval linear systems, in: [52], pp. 299–308, 1992.
[430] J.-M. Muller, Discrete Basis and Computation of Elementary Functions, IEEE Trans.
on Computers, Vol. C-34, No. 9, 1985.
[431] M. M¨uller, Entwicklung eines Chips f¨ur ausl¨oschungsfreie Summation von Gleitkom-
mazahlen, Dissertation, Universit¨at des Saarlandes, Saarbr¨ucken, 1993.
[432] M. M¨uller, Ch. R¨ub and W. R¨ulling, Exact Addition of Floating Point Numbers, Son-
derforschungsbericht 124, FB 14, Informatik, Universit¨at des Saarlandes, Saarbr¨ucken,
1990.
[433] M. R. Nakao, A numerical approach to the proof of existence of solutions for elliptic
problems, Japan J. Appl. Math. 5:2 (1988), 313–332.
[434] M. R. Nakao, State of the art for numerical computations with guaranteed accuracy,
Math. Japanese 48 (1998), 323–338.
[435] M. R. Nakao, Y. Watanabe, N. Yamamoto and T. Nishida, Some computer assisted
proofs for solutions of the heat convection problems, Reliable Computing 9 (2003),
359–372.
[436] M. Neaga, Erweiterungen von Programmiersprachen f¨ur wissenschaftliches Rechnen
und Er¨orterung einer Implementierung, Dissertation, Universit¨at Kaiserslautern, 1984.

382
Bibliography
[437] N. S. Nedialkov, Computing Rigorous Bounds on the Solution of Initial Value Problem
for an Ordinary Differential Equation, Thesis, University of Toronto, Toronto, 1999.
[438] M. Neher, An enclosure method of the solution of linear ODEs with polynomial coef-
ﬁcients, Numer. Funct. Anal. Optim. 20 (1999), 779–803.
[439] M. Neher, Geometric series bounds for the local errors of Taylor methods for linear
n-th order ODEs, in: [41], pp. 183–193, 2001.
[440] A. Neumaier, Rundungsfehleranalyseeiniger Verfahren zur Summation endlicher Sum-
men, Z. Angew. Math. Mech. 54 (1974), 39–51.
[441] A. Neumaier, Overestimation in linear interval equations, SIAM J. Numer. Anal. 24:1
(1987), 207–214.
[442] A. Neumaier, The enclosure of solutions of parameter-dependent systems of equations,
in: [426], pp. 269–286, 1988.
[443] A. Neumaier, Rigorous sensitivity analysis for parameter-dependent systems of equa-
tions, J. Math. Anal. Appl. 144 (1989), 16–25.
[444] A. Neumaier, Interval Methods for Systems of Equations, Cambridge University Press,
Cambridge, 1990.
[445] A. Neumaier, The wrapping effect, ellipsoid arithmetic, stability and conﬁdence re-
gions, in: [10], pp. 175–190, 1993.
[446] A. Neumaier and T. Rage, Rigorous chaos veriﬁcation in discrete dynamical systems,
Physica D 67 (1993), 327–346.
[447] J. von Neumann and H. H. Goldstine, Numerical inverting of matrices of high order,
Bulletin of the American Mathematical Society 53:11 (1947), 1021–1099.
[448] K. C. Ng, Argument Reduction for Huge Arguments: Good to the Last Bit, Sun Pro,
Sun Microsystems Inc., 1992.
[449] H. T. Nguyen, V. Kreinovich, S. Nesterov and M. Nakumura, On hardware support for
interval computations and for soft computing: Theorems, IEEE Transactions on Fuzzy
Systems 5:1 (1997), 108–127.
[450] K. Nickel, Das Kahan–Babuskasche Summierungsverfahren in Triplex-ALGOL 60, Z.
Angew. Math. Mech. 50 (1970), 369–373.
[451] K. Nickel (ed.), Interval Mathematics, Proceedings of the International Symposium in
Karlsruhe 1975, Springer, Wien, 1975.
[452] K. Nickel, Interval Mathematics 1980, Proceedings of the International Symposium in
Freiburg 1980, Academic Press, New York, 1980.
[453] K. Nickel, Interval Mathematics 1985, Proceedings of the International Symposium in
Freiburg 1985, Springer, Wien, 1986.
[454] W. Oettli and W. Prager, Computability of approximate solution of linear equations
with given error bounds for coefﬁcients and right-hand sides, Numer. Math. 6 (1964),
405–409.
[455] T. Ogita, S. M. Rump and S. Oishi, Accurate sum and dot product with applications, in:
Proceedings of the IEEE International Symposium on Computer Aided Control Systems
Design, Taipei, 2004, 152–155, 2004.
[456] T. Ogita, S. M. Rump and S. Oishi, Accurate sum and dot product, SIAM Journal on
Scientiﬁc Computing 26:6 (2005), 1955–1988.

Bibliography
383
[457] T. Ohta, T. Ogita, S. M. Rump and S. Oishi, Numerical veriﬁcation method for arbitrar-
ily ill-conditioned linear systems, Trans. JSIAM 15:3 (2005), 269–287.
[458] S. Oishi and S. M. Rump, Fast veriﬁcation of solutions of matrix equations, Num. Math.
90:4 (2002), 755–773.
[459] S. Oishi, K. Tanabe, T. Ogita and S. M. Rump, Convergence of Rump’s method for
inverting arbitrarily ill-conditioned matrices, Journal of Computational and Applied
Mathematics 205 (2007), 533–544.
[460] J. M. Ortega and W. C. Rheinbold, Iterative Solution of Nonlinear Equations in Several
Variables, Academic Press, New York London, 1970.
[461] D. A. Patterson and J. L. Hennessy, Computer Organization and Design, Elsevier Sci-
ence, San Francisco, 2005.
[462] M. Payne and R. Hanek, Radian reduction for trigonometric functions, Signum 18
(1983), 19–24.
[463] O. Perron, Irrationalzahlen, de Gruyter, Berlin, 1960.
[464] M. Petkovic, Iterative Methods for Simultaneous Inclusion of Polynomial Zeros, Lec-
ture Notes in Mathematics 1387, Springer, Berlin, 1989.
[465] M. Petkovic and L. D. Petkovic, Complex Interval Arithmetic and its Applications, Wi-
ley, New York, 1998.
[466] M. Pichat, Correction d’une somme en arithm´etique `a virgule ﬂottante, Numerische
Mathematik 19 (1972), 400–406.
[467] M. Plum, Computer-assisted existence proofs for two point boundary value problems,
Computing 46 (1991), 19–34.
[468] M. Plum, Inclusion methods for elliptic boundary value problems, in: [224], pp. 323–
379, 1994.
[469] M. Plum, Computer-assisted enclosure methods for elliptic differential equations, Lin-
ear Algebra and its Applications 324 (2001), 147–187.
[470] M. Plum, Safe numerical error bounds for solutions of nonlinear elliptic boundary value
problems, in: [41], pp. 195–207, 2001.
[471] M. Plum and Ch. Wieners, New solutions for the Gelfand problem, J. Math. Anal. Appl.
269 (2002), 588–606.
[472] M. Plum and Ch. Wieners, Optimal a priori estimates for interface problems, Num.
Math. 95 (2003), 735–759.
[473] D. M. Priest, Algorithms for arbitrary precision ﬂoating-point arithmetic, in: [654],
Vol. 10, pp. 132–143, 1991.
[474] L. B. Rall, Error in Digital Computation, J. Wiley, New York, 1965.
[475] L. B. Rall, Computaional Solution of Nonlinear Operator Equations, Wiley, New York,
1969.
[476] L. B. Rall, Applications of software for automatic differentiation in numerical compu-
tation, in: [27], pp. 141–156, 1980.
[477] L. B. Rall, Automatic Differentiation: Techniques and Applications, Lecture Notes in
Computer Science 120, Springer, Berlin, 1981.

384
Bibliography
[478] L. B. Rall, Differentiation and generation of Taylor coefﬁcients in PASCAL-SC, in:
[368], pp. 291–309, 1983.
[479] L. B. Rall, Differentiation in PASCAL-SC: Type GRADIENT, ACM TOMS 10 (1984),
161–184.
[480] L. B. Rall, Optimal implementation of differentiation arithmetic, in: [270], pp. 287–
295, 1987.
[481] L. B. Rall, Differentiation arithmetics, in: [591], pp. 73–90, 1990.
[482] L. B. Rall, Tools for mathematical computation, in: Computer Aided Proofs in Analysis,
edited by R. K. Meyer and D. S. Schmidt, pp. 217–228, IMA Volumes in Mathematics
and Its Applications 28, Springer, New York, 1991.
[483] L. B. Rall and T. W. Reps, Algorithmic differencing, in: [366], pp. 133–147, 2001.
[484] H. Ratschek, Teilbarkeitskriterien der Intervallarithmetik, J. Reine Ange. Math. 252
(1971), 128–137.
[485] H. Ratschek, Centered forms, SIAM J. Numer. Anal. 17 (1980), 656–662.
[486] H. Ratschek and J. Rokne, Computer Methods for the Range of Functions, Ellis Hor-
wood Limited, Chichester, 1984.
[487] H. Ratschek and J. Rokne, New Computer Methods for Global Optimizaation, Ellis
Horwood, Chichester, 1988.
[488] H. Ratschek and J. Rokne, Nonuniform variable precision computing, in: [600], pp. 71–
72, 1991.
[489] D. Ratz, The effects of the arithmetic of vector computers on basic numerical methods,
in: [591], pp. 499–514, 1990.
[490] D. Ratz, Programmierpraktikum mit PASCAL-SC, in: Computer Theoretikum und
Praktikum f¨ur Physiker 5, edited by G. H¨ohler and H. M. Staudenmaier, pp. 43–67,
Fachinformationszentrum Karlsruhe, 1990.
[491] D. Ratz, Globale Optimierung mit automatischer Ergebnisveriﬁkation, Dissertation,
Universit¨at Karlsruhe, 1992.
[492] D. Ratz, Automatic Slope Computation and its Application in Nonsmooth Global Opti-
mization, Shaker, Aachen, 1998.
[493] D. Ratz, On Extended Interval Arithmetic and Inclusion Isotony, preprint, Institut f¨ur
Angewandte Mathematik, Universit¨at Karlsruhe, 1999.
[494] D. Ratz, Nonsmooth global optimization, in: [366], pp. 277–337, 2001.
[495] G. Reitwiesner, An ENIAC determination of π and e to more than 2000 decimal places,
MTAC 4 (1950), 11–15.
[496] H.-G. Rex, Zur L¨osungseinschließung linearer Gleichungssysteme, in: [245], pp. 441–
447, 1991.
[497] R. Rihm, ¨Uber Einschließungsverfahren f¨ur gew¨ohnliche Anfangswertprobleme und
ihre Anwendung auf Differentialgleichungen mit unstetiger rechter Seite, Dissertation,
Universit¨at Karlsruhe, 1993.
[498] R. Rihm, Interval methods for initial value problems in ODEs, in: [224], pp. 173–207,
1994.

Bibliography
385
[499] J. Rohn and A. Deif, On the range of eigenvalues of an interval matrix, Computing 47
(1992), 373–377.
[500] R. Rojas, Die Architektur der Rechenmaschinen Z1 und Z3 von Konrad Zuse, Infor-
matik Spektrum 19:6 (1996), 303–315.
[501] R. Rojas, Konrad Zuses Rechenmaschinen: sechzig Jahre Computergeschichte, in:
Spektrum der Wissenschaft, pp. 54–62, Spektrum Verlag, Heidelberg, 1997.
[502] D. R. Ross, Reducing trancation errors using cascading accumulators, Comm. ACM 8
(1965), 32–33.
[503] S. M. Rump, Kleine Fehlerschranken bei Matrixproblemen, Dissertation, Universit¨at
Karlsruhe, 1980.
[504] S. M. Rump, Rechnervorf¨uhrung, Pakete f¨ur Standardprobleme der Numerik, in: [374],
pp. 29–50, 1982.
[505] S. M. Rump, L¨osung linearer und nichtlinearer Gleichungssysteme mit maximaler
Genauigkeit, in: [374], pp. 147–174, 1982.
[506] S. M. Rump, Solving non-linear systems with least signiﬁcant bit accuracy, Computing
29 (1982), 183–200.
[507] S. M. Rump, How reliable are results of computers? / Wie zuverl¨assig sind die Ergeb-
nisse unserer Rechenanlagen?, in: Jahrbuch ¨Uberblicke Mathematik, pp. 163–168,
Bibliographisches Institut, Mannheim, 1983.
[508] S. M. Rump, Solving algebraic problems with high accuracy, in: [368], pp. 51–120,
1983.
[509] S. M. Rump, Solving of linear and nonlinear algebraic problems with sharp guaranteed
bounds, Computing Suppl. 5 (1984), 147–168.
[510] S. M. Rump, Algorithms for Veriﬁed Inclusions: Theory and Practice, Reliability in
Computing, Academic Press, San Diego, 1988.
[511] S. M. Rump, Lineare Probleme, in: [357], pp. 119–127, 1989.
[512] S. M. Rump, Rigorous sensitivity analysis for systems of linear and nonlinear equa-
tions, Math. of Comp. 54:190 (1990), 724–736.
[513] S. M. Rump, Estimation of the sensitivity of linear and nonlinear algebraic problems,
Linear Algebra and its Applications 153 (1991), 1–34.
[514] S. M. Rump, Convergence properties of iterations using sets, in: [245], pp. 427–431,
1991.
[515] S. M. Rump, On the solution of interval linear systems, Computing 47 (1992), 337–353.
[516] S. M. Rump, Inclusion of the solution of large linear systems with positive deﬁnite
symmetric M-matrix, in: [52], pp. 339–347, 1992.
[517] S. M. Rump, Validated solution of large linear systems, in: [10], pp. 191–212, 1993.
[518] S. M. Rump, Veriﬁcation methods for dense and sparse systems of equations, in: [224],
pp. 63–135, 1994.
[519] S. M. Rump, A note on epsilon-inﬂation, Reliable Computing 4 (1998), 371–375.
[520] S. M. Rump, INTLAB – Interval Laboratory, TU Hamburg-Harburg, 1998.
[521] S. M. Rump, INTLAB–INTerval LABoratory, in: [153], pp. 77–104, 1999.
[522] S. M. Rump, Self-validating methods, Linear Algebra Appl. 324 (2001), 3–13.

386
Bibliography
[523] S. M. Rump, Computational error bounds for multiple or nearly multiple eigenvalues,
Linear Algebra Appl. 324 (2001), 209–226.
[524] S. M. Rump, Fast veriﬁcation algorithms in MATLAB, in: [41], pp. 209–226, 2001.
[525] S. M. Rump, Ten methods to bound multiple roots of polynomials, J. Comput. Appl.
Math. 156 (2003), 403–432.
[526] S. M. Rump, Perron–Frobenius theory for complex matrices, Linear Algebra and its
Applications 363 (2003), 251–273.
[527] S. M. Rump, INTLAB – Interval Laboratory. A Matlab toolbox for veriﬁed com-
putations, Version 5.1, 2005, available at http://www.ti3.tu-harburg.de/
rump/intlab/index.html.
[528] S. M. Rump, Computer-assisted proofs and self-validating methods, in: [162], pp. 195–
239, 2005.
[529] S. M. Rump and H. B¨ohm, Least signiﬁcant bit evaluation of arithmetic expressions,
Computing 30 (1983), 189–199.
[530] S. M. Rump and E. Kaucher, Small bounds for the solution of linear systems, in: [27],
pp. 157–164, 1980.
[531] S. M. Rump, T. Ogita and S. Oishi, Accurate Floating-point Summation, Technical Re-
port 05.12, Faculty for Information and Communication Sciences, Hamburg University
of Technology, November 13, 2005.
[532] M. Ruschitzka (ed.), Computer Systems: Performance and Simulation, in collaboration
with R. Vichnevetsky, Proceedings of the 11th IMACS World Congress on System
Simulation and Scientiﬁc Computation, August 5–9, 1985, Oslo. Preprints see [603];
additional papers in [270]. Elsevier Science Publishers B.V. (North–Holland), 1986.
[533] H. Rutishauser, A. Speiser and E. Stiefel, Programmgesteuerte digitale Rechenger¨ate,
Zeitschrift f¨ur Angewandte Mathematik und Physik 1 (1950), 277–297.
[534] E. Salamin, Computation of π using arithmetic-geometric mean, Mathematics of Com-
putation 30:135 (1976), 565–570.
[535] U. Schauer and R. A. Toupin, Solving large sparse linear systems with guaranteed ac-
curacy, in: [420], pp. 142–167, 1986.
[536] L. Schmidt, Semimorphe Arithmetik zur automatischen Ergebnisveriﬁkation auf Vek-
torrechnern, Dissertation, Universit¨at Karlsruhe, 1992.
[537] P. Schramm, Sichere Verschneidung von Kurven und Fl¨achen im CAGD, Dissertation,
Universit¨at Karlsruhe, 1995.
[538] A. Schrijver, Theory of Linear and Integer Programming, Wiley, New York, 1986.
[539] M. J. Schulte, A Variable Precision, Interval Arithmetic Processor, Ph.D. Thesis, Uni-
versity of Texas at Austin, 1996, available at http//www.eecs.lehigh.edu/
∼mschulte/papers.
[540] M. J. Schulte, E. Bickerstaff and E. E. Swartzlander Jr., Hardware interval multipliers,
Journal of Theoretical and Applied Informatics 3:2 (1996), 73–90.
[541] M. J. Schulte and E. E. Swartzlander Jr., A hardware design and arithmetic algorithms
for a vatiable precision, interval arithmetic coprocessor, in: Proceedings of the 12th
Symposium on Computer Arithmetic, pp. 163–171, IEEE Computer Society Press,
1995.

Bibliography
387
[542] M. J. Schulte and E. E. Swartzlander Jr., A processor for staggered interval arithmetic,
in: Proceedings of the International Conference on Application Speciﬁc Array Proc-
ssors, pp. 104–112, IEEE Computer Society Press, 1995.
[543] M. J. Schulte and E. E. Swartzlander Jr., A software interface and hardware design for
variable-precision interval arithmetic, Reliable Computing 1 (1995), 325–342 (software
student prize).
[544] M. J. Schulte and E. E. Swartzlander Jr., Variable precision, interval arithmetic copro-
cessor, Reliable Computing 2 (1996), 47–62.
[545] M. J. Schulte and E. E. Swartzlander Jr., A family of variable-precision, interval arith-
metic processor, IEEE Transactions on Computers 5:49 (2000), 387–398.
[546] M. J. Schulte, V. Zelov, A. Akkas and J. C. Burley, Adding interval support to the GNU
Fortran Compiler, January 19, 1998, available at http//www.eecs.lehigh.
edu/∼mschulte/compiler/work-notes.
[547] M. J. Schulte, V. Zelov, A. Akkas and J. C. Burley, The interval-enhanced GNU Fortran
compiler, in: [153], pp. 311–322, 1999.
[548] G. Schumacher, Einschließung der L¨osung von linearen Gleichungssystemen auf Vek-
torrechnern, in: [357], pp. 239–249, 1989.
[549] G. Schumacher, Genauigkeitsfragen bei algebraisch-numerischen Algorithmen auf
Skalar- und Vektorrechnern, Dissertation, Universit¨at Karlsruhe, 1989.
[550] H. Schwandt, Schnelle fast global konvergente Verfahren f¨ur die F¨unf-Punkte-
Diskretisierung der Poissongleichung mit Dirichletschen Randbedingungen auf
Rechteckgebieten, Dissertation, TU Berlin, Berlin 1981.
[551] D. Shanks and W. Wrench, Calculation of π to 100,000 decimals, Math. Computing 16
(1962), 76–79.
[552] P. S. Shary, Solving the linear interval tolerance problem, Mathematics and Computers
in Simulation 39 (1995), 53–85.
[553] J. R. Shewchuk, Adaptive precision ﬂoating-point arithmetic and fast robust geometric
predicates, Disccrete Comput. Geom. 18 (1997), 305–363.
[554] D. V. Shiriaev, On the memory efﬁcient differentiation, ZAMM 72 (1992), T632–T634.
[555] D. V. Shiriaev, Reduction of spatial complexity in reverse automatic differentiation by
means of inverted code, in: [52], pp. 475–484, 1992.
[556] D. V. Shiriaev, Fast Automatic Differentiation for Vector Processors and Reduction of
the Spatial Complexity in a Source Translation Environment, Dissertation, Universit¨at
Kalrsuhe, 1993.
[557] D. V. Shiriaev and G. W. Walster, Interval Arithmetic Speciﬁcation, 1998, available at
http://www.mscs.mu.edu/∼globsol/readings.html.
[558] R. D. Skeel, Iterative reﬁnement implies numerical stability for Gaussian elimination,
Math. Comp. 35 (1980), 817–832.
[559] D. M. Smith, Algorithm 693: A Fortran package for ﬂoating-point multiple-precision
arithmetic, ACM Trans. on Math. Software 17:2 (1991), 273–283.
[560] J. Spanier and K. B. Oldham, An Atlas of Functions, Hemisphere publishing corpora-
tion, 1987.

388
Bibliography
[561] O. Spaniol, Arithmetik in Rechenanlagen – Logik und Entwurf, B. G. Teubner, Stuttgart,
1976.
[562] A. P. Speiser, 50 years of the seminar for applied mathematics, Swiss Federal Institute
of Technology, November 18–20, 1998, The Early Years of the Institute: Acquisition
and Operation of the Z4, Planning of the ERMETH, Mitteilungen der Gesellschaft f¨ur
Angewandte Mathematik und Mechanik 22:2 (1999), 159–168.
[563] H. J. Stetter, Sequential defect correction for high-accuracy ﬂoating-point algorithms,
in: Numerical Analysis, pp. 186–202, Lecture Notes in Mathematics 1066, Springer,
Berlin Heidelberg New York, 1984.
[564] H. J. Stetter, Staggered correction representation, a feasible approach to dynamic pre-
cision, in: Proceedings of the Symposium on Scientiﬁc Software, edited by D. Y. Cai,
L. D. Fosdick and H. C. Huang, China University of Science and Technology Press,
Beijing, China, 1989.
[565] G. H. Stewart, Introduction to Matrix Computations, Academic Press, New York, 1973.
[566] J. E. Stine, Design Issues for Accurate and Reliable Arithmetic, Ph.D. Thesis, Lehigh
University, January 2001.
[567] J. E. Stine and M. J. Schulte, A combined interval and ﬂoating point multiplier, in:
Proceedings of the 8th Great Lakes Symposium on VLSI, Lafayette, LA, February, 1998,
pp. 208–213, IEEE Computer Society Press, Wahington, D.C., 1998.
[568] J. E. Stine and M. J. Schulte, A case for interval hardware on superscalar processors,
in: [334], pp. 53–68, 2001.
[569] J. Stoer and R. Bulirsch, Introduction to Numerical Analysis, Springer, New York,
1980.
[570] U. Storck, Zweidimensionale Integration mit automatischer Ergebnisveriﬁkation,
ZAMM 73 (7/8) (1993), T897–T899.
[571] U. Storck, Veriﬁed Calculation of the Nodes and Weights for Gaussian Quadrature
Formulas, Interval Computation, St. Petersburg, 1993.
[572] U. Storck, Numerical integration in two dimensions with automatic result veriﬁcation,
in: [5], pp. 187–224, 1993.
[573] U. Storck, Veriﬁzierte Berechnung mehrfach geschachtelter singul¨arer Integrale der
Gaskinetik, Dissertation, Universit¨at Karlsruhe, 1995.
[574] U. Storck, R. Lohner and U. Schnabel, An Algorithm for Inclusion of Multiple and
Clusters of Eigenvalues, Universit¨at Karlsruhe, Fakult¨at f¨ur Mathematik, preprint Nr.
01/22, 2001, submitted to LAA for publication.
[575] A. Sch¨onhage and V. Strassen, Schnelle Multiplikation großer Zahlen, Computing 7
(1971), 281–292.
[576] T. Sunaga, Theory of an interval algebra and its application to numerical analysis,
RAAG Memoires 2 (1958), 547–564.
[577] H. Suzuki, H. Morinaka, H. Makino, Y. Nakase, K. Mashiko and T. Sumi, Leading-zero
anticipatory logic for high-speed ﬂoating-point addition, IEEE Journal of Solid-State
Circuits 31:8 (1996), 1157–1169.
[578] D. W. Sweeny, On the computation of Euler’s constant, Math. Comp. 17 (1963), 170–
178.

Bibliography
389
[579] N. Takagi, Studies on hardware algorithms for arithmetic operations with redundant
binary representation, Dissertation, Department of Information Science, Faculty of En-
gineering, Kyoto University, 1987.
[580] N. Takagi, H. Yasuura and S. Yajima, High speed VLSI multiplication algorithm with
a redundant binary adder tree, IEEE Trans. on Computers 34 (1985), 789–796.
[581] P. T. P. Tang, Table-driven implementation of the exponential function in IEEE ﬂoating-
point arithmetic, ACM Trans. on Math. Software 15:2 (1989), 144–157.
[582] P. T. P. Tang, Table-driven implementation of the logarithm function in IEEE ﬂoating-
point arithmetic, ACM Trans. on Math. Software 16:4 (1990), 378–400.
[583] P. T. P. Tang, Table-driven implementation of the expm1 function in IEEE ﬂoating-point
arithmetic, ACM Trans. on Math. Software 18:2 (1992), 211–222.
[584] R. J. W. T. Tangelder, The Design of Chip Architectures for Accurate Inner Product
Computation, Dissertation, Technical University Eindhoven, 1992.
[585] T. Teufel, Ein optimaler Gleitkommaprozessor, Dissertation, Universit¨at Karlsruhe,
1984.
[586] T. Teufel, Implementation of a ﬂoating-point arithmetic with an accurate scalar product
for digital signal processing, in: [52], pp. 147–156, 1992.
[587] T. Teufel, Genauer und trotzdem schneller – Ein neuer Coprozessor f¨ur hochgeneue
Matrix- und Vectoroperationen, Elektronik 26 (1994), 52–56.
[588] T. Teufel, A Novel VLSI Vector Arithmetic Coprocessor for Advanced DSP Applica-
tions, Proceedings of ICSPAT’96, Vol. 2, pp. 1894–1898, Boston, 1996.
[589] T. Teufel and G. Bohlender, A bit-slice processor unit for optimal arithmetic, in: [603],
Vol. 1, pp. 151–154, and [532], pp. 325–329, 1986.
[590] Ch. Ullrich, Rundungsinvariante Strukturen mit ¨außeren Verkn¨upfungen, Dissertation,
Universit¨at Karlsruhe, 1972.
[591] Ch. Ullrich, Computer Arithmetic and Self-Validating Numerical Methods, Proceedings
of SCAN-89, invited papers, Academic Press, San Diego, 1990.
[592] Ch. Ullrich, Programming languages for enclosure methods, in: [591], pp. 115–136,
1990.
[593] Ch. Ullrich (ed.), Contributions to Computer Arithmetic and Self-Validating Numerical
Methods, J. C. Baltzer AG, Scientiﬁc Publishing Co., Basel, 1990.
[594] Ch. Ullrich, Interval arithmetic on computers, in: [224], pp. 473–497, 1994.
[595] Ch. Ullrich and J. Wolff von Gudenberg (eds.), Accurate Numerical Algorithms. A
Collection of DIAMOND Research Papers, Springer, Berlin, 1989.
[596] D. K. Unkauf, A. T. Gerlicher, S. M. Rump and J. H. Bleher, Verfahren und Schal-
tungsanordnung zur Addition von Gleitkommazahlen, Patentanmeldung, Technical Re-
port, IBM, 1986.
[597] R. S. Varga, Matrix Iterative Analysis, Prentice Hall, Englewood Cliffs, New Jersey,
1962.
[598] R. S. Varga, Matrix Iterative Analysis, second edition, Springer, Berlin, 2000.
[599] R. S. Varga, Scientiﬁc Computation on Mathematical Problems and Conjectures,
SIAM, Philadelphia, 1990.

390
Bibliography
[600] R. Vichnevetsky and J. J. H. Miller (eds.), IMACS’91, 13th World Congress on Compu-
tation and Applied Mathematics. Proceedings in 4 Volumes, July 22–26, 1991, Trinity
College, Dublin, Ireland, 1991 (see also [125]).
[601] J. Vignes, Discrete stochastic arithmetic for validating results of numerical software,
in: [124], pp. 377–390, 2004.
[602] J. E. Volder, The CORDIC trigonometric computing technique, IRE Transactions on
Electronic Computing EC-8:3 (1959), 330–334.
[603] B. Wahlstr¨om, R. Henriksen and N. P. Sundby (eds.), Proceedings of 11th IMACS World
Congress on System Simulation and Scientiﬁc Computation, Vol. 5, August 5–9, 1985,
Oslo, published in [532], additional papers in [270].
[604] C. S. Wallace, A suggestion for a fast multiplier, IEEE Trans. on Computers EC-13
(1964), 14–17.
[605] P. J. L. Wallis (ed.), Improving Floating-Point Programming, J. Wiley, Chichester, 1990
(ISBN 0 471 92437 7).
[606] G. W. Walster, The use and implementation of interval data types, in: [162], pp. 173–
194, 2005.
[607] G. W. Walster, FORTRAN-SC. A FORTRAN extension for engineering/scientiﬁc com-
putation with access to ACRITH: Language description with examples, in: [426],
pp. 43–62 1988.
[608] G. W. Walster, Einf¨uhrung in die wissenschaftlich-technische Programmiersprache
FORTRAN-SC, ZAMM 69:4 (1989), T52–T54.
[609] G. W. Walster, Flexible Precision Control and Dynamic Data Structures for Program-
ming Mathematical and Numerical Algorithms, Dissertation, Universit¨at Karlsruhe,
1990.
[610] G. W. Walster, FORTRAN-XSC: A portable FORTRAN 90 module library for accurate
and reliable scientiﬁc computing, in [10], pp. 265–285, 1993.
[611] W. Walter, FORTRAN-SC: A FORTRAN Extension for Engineering/Scientiﬁc Compu-
tation with Access to ACRITH, Language Reference and User’s Guide, second edition,
IBM Deutschland GmbH, Stuttgart, January 1989.
[612] W. V. Walter, Mathematical Foundations of Fully Reliable and Portable Software for
Scientiﬁc Computing, Universit¨at Karlsruhe, 1995.
[613] J. S. Walther, A Uniﬁed Algorithm for Elementary Functions, Spring Joint Computer
Conference Proc., Vol. 38, 1971.
[614] J. Weissinger, Numerische Mathematik auf Personal Computern, Bibliographisches In-
stitut, Mannheim, 1984.
[615] J. Weissinger, Sp¨arlich besetzte Gleichungssysteme. Eine Einf¨uhrung mit Basic- und
Pascal-Programmen, Bibliographisches Institut, Mannheim, 1990.
[616] A. Wiethoff, Globale Optimierung auf Parallelrechnern, Dissertation, Universit¨at Karl-
sruhe, 1998.
[617] J. Wilkinson, Rounding Errors in Algebraic Processes, Prentice-Hall, Englewood
Cliffs, New Jersey, 1964.
[618] J. Wilkinson, Rundungsfehler, Springer, Berlin, 1969.

Bibliography
391
[619] J. Wilkinson and C. Reinsch, Handbook for Automatic Computation, Vol. 2: Linear
Algebra, Springer, Berlin Heidelberg New York, 1971.
[620] D. T. Winter, Automatic identiﬁcation of scalar products, in: [605], 1990.
[621] Th. Winter, Ein VLSI-Chip f¨ur Gleitkomma-Skalarprodukt mit maximaler Genauigkeit,
Diplomarbeit, Fachbereich 10, Angewandte Mathematik und Informatik, Universit¨at
des Saarlandes, 1985.
[622] H.-W. Wippermann, Realisierung einer Intervallarithmetik in einem ALGOL-60 Sys-
tem, Elektronische Rechenanlagen 9 (1967), 224–233.
[623] H.-W. Wippermann, Implementierung eines ALGOL-60 Systems mit Schranken-
zahlen, Elektronische Datenverarbeitung 10 (1968), 189–194.
[624] J. Wolff von Gudenberg, Einbettung allgemeiner Rechnerarithmetik in PASCAL mittels
eines Operatorkonzepts und Implementierung der Standardfunktionen mit optimaler
Genauigkeit, Dissertation, Universit¨at Karlsruhe, 1980.
[625] J. Wolff von Gudenberg, Computing zy with maximum accuracy, Computing 31
(1983), 185–189.
[626] J. Wolff von Gudenberg, Reliable expression evaluation in PASCAL-SC, in: Reliability
in Computing, edited by R. E. Moore, pp. 81–98, Academic Press, Boston New York
London Sydney, 1988.
[627] J. Wolff von Gudenberg, Modelling SIMD-type parallel arithmetic operations in Ada,
in: Ada. The Choice for ’92, edited by D. Christodoulakis, LNCS 499, Springer, Berlin
1991.
[628] J. Wolff von Gudenberg, Parallel accurate linear algebra subroutines, Computing 1:2
(1995), 189–199.
[629] J. Wolff von Gudenberg, Hardware support for interval arithmetic, in: [334], pp. 32–38,
2001.
[630] J. Wolff von Gudenberg, Hardware Support for Interval Arithmetic, Extended Ver-
sion, Report No. 125, Institut f¨ur Informatik, Universit¨at W¨urzburg, 1995, and in [26],
pp. 32–27, 1996.
[631] J. Wolff von Gudenberg, Proceedings of Interval’96, International Conference on In-
terval Methods and Computer Aided Proofs in Science and Engineering, W¨urzburg,
Germany, September 30–October 2, 1996. Special issue 3/97 of the journal Reliable
Computing, 1997.
[632] J. Wolff von Gudenberg, Java for scientiﬁc computation. Pros and cons, J. Universal
Computer Science 4:1 (1998), 11–15.
[633] J. Wolff von Gudenberg, OOP and interval arithmetic – language support and libraries,
in: Numerical Software with Result Veriﬁcation, edited by R. Alt, A. Frommer, B.
Kearfott and W. Luther, Lecture Notes in Computer Science, Springer, Berlin Heidel-
berg New York, 2000.
[634] J. Wolff von Gudenberg, Interval arithmetic on multimedia architectures, Reliable
Computing 8:4 (2002), 307–312.
[635] T. Yilmaz, J. F. M. Theeuwen, R. J. W. T. Tangelder and J. A. G. Jess, The design of a
chip for scientiﬁc computation, Eindhoven University of Technology, 1989, and in:
Proceedings of the Euro-Asic Symposium, Grenoble, Jan. 25–27, 1989, pp. 335–346.

392
Bibliography
[636] J. M. Yohe, Roundings in ﬂoating-point arithmetic, IEEE Trans. on Computers C-22:6
(1973), 577–586.
[637] G. Zielke and V. Drygalla, Genaue L¨osung linearer Gleichungssysteme, GAMM Mitt.,
Ges. Angew. Math. Mech. 26 (2003), 7–107.
[638] A. Ziv, Fast evaluation of elementary mathematical functions with correctly rounded
last bit, ACM Trans. on Math. Software 17:3 (1991), 410–423.
[639] Y.-K. Zhu, J.-H. Yong and G.-Q. Zheng, A new distillation algorithm for ﬂoating-point
summation, SIAM J. Sci. Comput. 26:6 (2005), 2066–2078.
[640] K. Zuse, Der Plankalk¨ul, GMD Berichte, Bonn, 1972.
[641] K. Zuse, Der Computer – Mein Lebenswerk, Springer, Berlin, 1984.
[642] K. Zuse, The Plankalk¨ul, Oldenbourg, M¨unchen Wien, 1989.
[643] American National Standards Institute/Institute of Electrical and Electronics Engi-
neers, IEEE Standard Pascal Computer Programming Language, ANSI/IEEE Std.
770 X3.97-1983, New York, 1983; J. Wiley & Sons Inc., 1983.
[644] American National Standards Institute/Institute of Electrical and Electronics Engineers,
A Standard for Binary Floating-Point Arithmetic, ANSI/IEEE Std. 754-1985, New
York, 1985 (reprinted in SIGPLAN 22:2 (1987), 9–25).
[645] American National Standards Institute/Institute of Electrical and Electronics Engineers,
A Standard for Radix-Independent Floating-Point Arithmetic, ANSI/IEEE Std. 854-
1987, New York, 1987.
[646] Cyrix, FasMath CX-83D87 User’s Manual, Cyrix Corporation, P.O. Box 850118,
Richardson, TX 75085-0118, 1990.
[647] IAM, PASCAL-XR: PASCAL for eXtended Real arithmetic, Joint research project with
Nixdorf Computer AG, Institute of Applied Mathematics, Universit¨at Karlsruhe, Post-
fach 6980, D-76128 Karlsruhe, Germany, 1980.
[648] IAM, FORTRAN-SC: A FORTRAN Extension for Engineering/Scientiﬁc Computation
with Access to ACRITH, Institute of Applied Mathematics, Universit¨at Karlsruhe, Post-
fach 6980, D-76128 Karlsruhe, Germany, January 1989.
1. Language Reference and User’s Guide, second edition.
2. General Information Notes and Sample Programs.
[649] IAM, ACRITH–XSC, A Programming Language for Scientiﬁc Computation, Syntax
Diagrams, Institute of Applied Mathematics, Universit¨at Karlsruhe, Postfach 6980, D-
76128 Karlsruhe, Germany, 1990.
[650] IBM, IBM System/370 RPQ. High Accuracy Arithmetic, SA 22-7093-0, IBM Deutsch-
land GmbH (Department 3282, Sch¨onaicher Strasse 220, D-71032 B¨oblingen), 1984.
[651] IBM, IBM High-Accuracy Arithmetic Subroutine Library (ACRITH), IBM Deutschland
GmbH (Department 3282, Sch¨onaicher Strasse 220, D-71032 B¨oblingen), third edition,
1986.
1. General Information Manual, GC 33-6163-02.
2. Program Description and User’s Guide, SC 33-6164-02.
3. Reference Summary, GX 33-9009-02.
[652] IBM, Verfahren und Schaltungsanordnung zur Addition von Gleitkommazahlen, Eu-
rop¨aische Patentanmeldung, EP 0 265 555 A1, 1986.

Bibliography
393
[653] IBM, ACRITH–XSC: IBM High Accuracy Arithmetic – Extended Scientiﬁc Computa-
tion. Version 1, Release 1, IBM Deutschland GmbH (Department 3282, Sch¨onaicher
Strasse 220, D-71032 B¨oblingen), 1990.
1. General Information, GC33-6461-01.
2. Reference, SC33-6462-00.
3. Sample Programs, SC33-6463-00.
4. How To Use, SC33-6464-00.
5. Syntax Diagrams, SC33-6466-00.
[654] Institute of Electrical and Electronics Engineers (IEEE), Proceedings of x-th Sympo-
sium on Computer Arithmetic ARITH, IEEE Computer Society Press, IEEE Service
Center, 445 Hoes Lane, P.O. Box 1331, Piscataway, NJ 08855-1331, USA.
Editors of proceedings, place of conference, date of conference.
1. R. R. Shively, Minneapolis, June 16, 1969.
2. H. L. Garner and D. E. Atkins, Univ Maryland, College Park, May 15–16, 1972.
3. T. R. N. Rao and D. W. Matula, SMU, Dallas, Nov. 19–20, 1975.
4. A. Avizienis and M. D. Ercegovac, UCLA, Los Angeles, October 25–27, 1978.
5. K. S. Trivedi and D. E. Atkins, Univ Michigan, Ann Arbor, May 18–19, 1981.
6. T. R. N. Rao and P. Kornerup, Univ Aarhus, Denmark, June 20–22, 1983.
7. K. Hwang, Univ Illinois, Urbana, June 4–6, 1985.
8. M. J. Irwin and R. Stefanelli, Como, Italy, May 19–21, 1987.
9. M. Ercegovac and E. E. Swartzlander Jr., Santa Monica, September 6–8, 1989.
10. P. Kornerup and D. Matula, Grenoble, France, June 26–28, 1991.
11. E. E. Swartzlander Jr., M. J. Irwin and G. Jullien, Windsor, Ontario, June 29–July
2, 1993.
12. S. Knowles and W. H. Mc Allister, Bath, England, July 19–21, 1995.
13. Th. Lang, J.-M. Muller and N. Takagi, Asilomar, California, July 6–9, 1997.
[655] IEEE, A Proposed Standard for Binary Floating-Point Arithmetic, IEEE Computer,
March 1981.
[656] IMACS and GAMM, IMACS-GAMM resolution on computer arithmetic, Mathematics
and Computers in Simulation 31 (1989), 297–298, or in Zeitschrift f¨ur Angewandte
Mathematik und Mechanik 70:4 (1990), T5, or in [591], pp. 301–302, 1990, or in [592],
pp. 523–524, 1990, or in [271], pp. 477–478, 1991.
[657] IMACS and GAMM, IMACS-GAMM proposal for accurate ﬂoating-point vector arith-
metic, GAMM Rundbrief 2 (1993), 9–16. Mathematics and Computers in Simulation,
Vol. 35, IMACS, North Holland, 1993. News of IMACS, Vol. 35, No. 4, 375–382,
Oct. 1993.
[658] ISO/IEC, ISO/IEC 7185:1990 – Information Technology – Programming Languages –
Pascal, second edition, 1990.
[659] ISO/IEC, ISO/IEC 10206:1991 – Information Technology – Programming Languages
– Extended Pascal, 1991.

394
Bibliography
[660] ISO, Language Independent Arithmetic Standard (LIA-1), Second Committee Draft
Standard (Version 4.0), ISO/IEC CD 10967-1, 1992.
[661] MAPLE, Reference Manual, Symbolic Computation Group, University of Waterloo,
Ontario, Canada, 1988.
[662] Numerik Software GmbH, PASCAL-XSC: A PASCAL Extension for Scientiﬁc Com-
putation. User’s Guide, Numerik Software GmbH, Haid-und-Neu-Straße 7, D-76131
Karlsruhe, Germany / Postfach 2232, D-76492 Baden-Baden, Germany, 1991.
[663] SIEMENS, ARITHMOS (BS 2000) Unterprogrammbibliothek f¨ur Hochpr¨azisionsa-
rithmetik. Kurzbeschreibung, Tabellenheft, Benutzerhandbuch, SIEMENS AG, Bereich
Datentechnik, Postfach 83 09 51, D-8000 M¨unchen 83, Bestellnummer U2900-J-Z87-
1, September 1986.
[664] Sun Microsystems, Interval Arithmetic Programming Reference, Fortran 95, Sun Mi-
crosystems, Inc., 901 San Antonio Road, Palo Alto, CA 94303, USA, 2000.

List of Figures
1
Collection of spaces used in numerical computation.
. . . . . . . . . . .
5
2
The ﬁfteen fundamental computer operations. . . . . . . . . . . . . . . .
8
3
Operation for complex intervals. . . . . . . . . . . . . . . . . . . . . . .
9
4
Floating-point addition. . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.1
Order diagrams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.2
An order diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.3
Example for the concept of subnet. . . . . . . . . . . . . . . . . . . . . .
20
1.4
Illustration of the concept of a screen in IV2R. . . . . . . . . . . . . . . .
24
1.5
Illustration of the concept of a screen in R. . . . . . . . . . . . . . . . . .
24
1.6
Illustration of an upper screen. . . . . . . . . . . . . . . . . . . . . . . .
26
1.7
Illustration of an upper screen. . . . . . . . . . . . . . . . . . . . . . . .
28
1.8
IZ is not a lower screen of PZ. . . . . . . . . . . . . . . . . . . . . . . .
28
1.9
Illustration of a screen. . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.10 Illustration of a screen. . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
1.11 Roundings in non linearly ordered sets.
. . . . . . . . . . . . . . . . . .
33
1.12 Operation in an upper screen groupoid. . . . . . . . . . . . . . . . . . . .
37
3.1
Dependence of directed roundings in a linearly ordered ringoid.
. . . . .
64
3.2
The traditional deﬁnition of computer arithmetic leading to ringoids. . . .
67
3.3
The traditional deﬁnition of computer arithmetic leading to vectoids. . . .
68
3.4
The deﬁnition of computer arithmetic by semimorphism leading to ringoids. 73
3.5
The deﬁnition of computer arithmetic by semimorphisms leading to vec-
toids. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.6
The characteristic spacing of a ﬂoating-point system. . . . . . . . . . . .
78
3.7
The behavior of frequently used roundings near zero. . . . . . . . . . . .
80
4.1
Illustration of Theorem 4.15. . . . . . . . . . . . . . . . . . . . . . . . . 102
4.2
Ringoids with interval matrices.
. . . . . . . . . . . . . . . . . . . . . . 117
4.3
Interval vectoids. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
4.4
Illustration of the ringoid isomorphism τ.
. . . . . . . . . . . . . . . . . 126
4.5
Complex interval arithmetic. . . . . . . . . . . . . . . . . . . . . . . . . 128
4.6
Illustration of an isomorphism. . . . . . . . . . . . . . . . . . . . . . . . 131
4.7
Complex interval matrices. . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.8
Complex interval vectoids. . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.1
Rounding to the nearest ﬂoating-point number.
. . . . . . . . . . . . . . 163

396
List of Figures
5.2
Execution of rounding for b-complement representation of negative num-
bers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
5.3
A special rounding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
5.4
Relative rounding error. . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
5.5
Coding of ﬂoating-point numbers in the IEEE 754 standard.
. . . . . . . 180
6.1
Symbols for the logical operators and, or, and not. . . . . . . . . . . . . . 188
6.2
Symbols for and and or operators. . . . . . . . . . . . . . . . . . . . . . 189
6.3
Realization of an and and an or gate by switches. . . . . . . . . . . . . . 189
6.4
Half adder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
6.5
Full adder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
6.6
The serial adder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.7
Parallel adder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
6.8
Carry-select-adder. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
6.9
The John von Neumann adder. . . . . . . . . . . . . . . . . . . . . . . . 192
6.10 Arrangement of full adders in a carry save adder.
. . . . . . . . . . . . . 192
6.11 Continued addition by a carry save adder.
. . . . . . . . . . . . . . . . . 193
6.12 Addition of m summands by a carry save adder chain. . . . . . . . . . . . 193
6.13 Adder tree for the addition of 19 binary words.
. . . . . . . . . . . . . . 194
6.14 A simple multiplier. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
6.15 Fast multiplication. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
6.16 Flow diagram for the arithmetic operations. . . . . . . . . . . . . . . . . 199
6.17 (a) long accumulator; (b) short accumulator. . . . . . . . . . . . . . . . . 200
6.18 Execution of the addition x !+ y. . . . . . . . . . . . . . . . . . . . . . . 203
6.19 Execution of the normalization after the addition. . . . . . . . . . . . . . 207
6.20 Execution of the normalization after a multiplication. . . . . . . . . . . . 207
6.21 Execution of multiplication.
. . . . . . . . . . . . . . . . . . . . . . . . 208
6.22 Execution of the division x !/ y. . . . . . . . . . . . . . . . . . . . . . . . 209
6.23 Execution of some roundings.
. . . . . . . . . . . . . . . . . . . . . . . 210
6.24 Execution of the downwardly directed rounding. . . . . . . . . . . . . . . 210
6.25 The rounding towards zero, truncation or chopping. . . . . . . . . . . . . 211
6.26 A universal rounding unit.
. . . . . . . . . . . . . . . . . . . . . . . . . 212
6.27 Exponent underﬂow and exponent overﬂow. . . . . . . . . . . . . . . . . 215
6.28 Execution of addition with the short accumulator. . . . . . . . . . . . . . 217
6.29 Execution of normalization with the short accumulator. . . . . . . . . . . 218
6.30 Accumulator for short scalar products. . . . . . . . . . . . . . . . . . . . 222
7.1
General circuitry for interval operations and comparisions. . . . . . . . . 237
7.2
Operand-Selection and Operations Unit. . . . . . . . . . . . . . . . . . . 238
7.3
Comparisons and Result-Selection Unit. . . . . . . . . . . . . . . . . . . 240
7.4
General circuitry for interval operations and comparisons. . . . . . . . . . 242
7.5
Operand Selection Unit. . . . . . . . . . . . . . . . . . . . . . . . . . . . 242

List of Figures
397
7.6
Arithmetic Operations Unit.
. . . . . . . . . . . . . . . . . . . . . . . . 242
7.7
Figures from various Intel publications.
. . . . . . . . . . . . . . . . . . 243
8.1
Some mechanical computing devices developed between 1878 and 1956. . 249
8.2
Long accumulator with long shift for exact scalar products. . . . . . . . . 258
8.3
Short adder and local store on the arithmetic unit for exact scalar product
accumulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
8.4
Fast carry resolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
8.5
Accumulation of a product to the CR by a 64 bit adder. . . . . . . . . . . 264
8.6
Pipeline for the accumulation of scalar products on computers with 32 bit
data bus. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
8.7
Block diagram for an SPU with 32 bit data supply and sequential addition
into the CR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
8.8
Functional units, chip and board of the vector arithmetic coprocessor
XPA 3233. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
8.9
Parallel accumulation of a product into the CR.
. . . . . . . . . . . . . . 271
8.10 Pipeline for the accumulation of scalar products. . . . . . . . . . . . . . . 272
8.11 Block diagram for an SPU with 64 bit data bus and parallel addition into
the CR.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
8.12 SPU for vectors with interval components. . . . . . . . . . . . . . . . . . 276
8.13 Complete Register CR. . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
8.14 Syntax diagram for COMPLETE EXPRESSION. . . . . . . . . . . . . . 280
8.15 Parallel and segmented parallel adder. . . . . . . . . . . . . . . . . . . . 283
8.16 Block diagram of an SPU with long adder for a 64 bit data word and 128
bit data bus. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
8.17 Block diagram of an SPU with long adder for a 32 bit data word and 64
bit data bus. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
8.18 Block diagram of an SPU with short adder and local store for a 64 bit data
word and 128 bit data bus.
. . . . . . . . . . . . . . . . . . . . . . . . . 292
8.19 Carry propagation for pipeline conﬂict. . . . . . . . . . . . . . . . . . . . 295
8.20 Block diagram for an SPU with short adder and local store for a 32 bit
data word and 64 bit data bus.
. . . . . . . . . . . . . . . . . . . . . . . 296
8.21 Hardware Complete Register Window (HCRW). . . . . . . . . . . . . . . 298
9.1
Syntax diagram for REAL EXPRESSION. . . . . . . . . . . . . . . . . . . 308
9.2
Syntax diagram for INTERVAL EXPRESSION.
. . . . . . . . . . . . . . . 308
9.3
Non zero property of a function. . . . . . . . . . . . . . . . . . . . . . . 315
9.4
Global optimization.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
9.5
Veriﬁed Romberg integration. . . . . . . . . . . . . . . . . . . . . . . . . 321
9.6
Computation of enclosures of Taylor coefﬁcients. . . . . . . . . . . . . . 324
9.7
Geometric interpretation of the interval Newton method.
. . . . . . . . . 326
9.8
Geometric interpretation of the extended interval Newton method. . . . . 328


List of Tables
4.1
Execution of multiplication.
. . . . . . . . . . . . . . . . . . . . . . . .
97
4.2
Execution of division with B not containing 0. . . . . . . . . . . . . . . .
97
4.3
Execution of the multiplication in IS.
. . . . . . . . . . . . . . . . . . . 111
4.4
Execution of division in IS where B does not contain 0. . . . . . . . . . . 112
4.5
The nine cases of interval multiplication in IS. . . . . . . . . . . . . . . . 114
4.6
The six cases of interval division in IS. . . . . . . . . . . . . . . . . . . . 114
4.7
The eight cases of interval division in IR. . . . . . . . . . . . . . . . . . 136
4.8
The eight cases of interval division in IR. . . . . . . . . . . . . . . . . . 136
4.9
The eight cases of interval division in IS.
. . . . . . . . . . . . . . . . . 139
4.10 The eight cases of interval division in IS.
. . . . . . . . . . . . . . . . . 139
4.11 Addition for intervals of (IR). . . . . . . . . . . . . . . . . . . . . . . . 140
4.12 Subtraction for intervals of (IR). . . . . . . . . . . . . . . . . . . . . . . 140
4.13 Multiplication for intervals of (IR). . . . . . . . . . . . . . . . . . . . . 142
4.14 Division for intervals of (IR) with 0 ̸∈B. . . . . . . . . . . . . . . . . . 143
4.15 Division for intervals of (IR) with 0 ∈B. . . . . . . . . . . . . . . . . . 144
4.16 Addition of extended intervals on the computer. . . . . . . . . . . . . . . 145
4.17 Subtraction of extended intervals on the computer. . . . . . . . . . . . . . 145
4.18 Multiplication of extended intervals on the computer. . . . . . . . . . . . 146
4.19 Division of extended intervals with 0 ̸∈B on the computer. . . . . . . . . 147
4.20 Division of extended intervals with 0 ∈B on the computer. . . . . . . . . 148
5.1
The formats of the IEEE 754 standard. . . . . . . . . . . . . . . . . . . . 180
5.2
The binary and decimal formats of the IEEE P 754 standard. . . . . . . . 181
6.1
Deﬁnition of the logical operators and, or, and not. . . . . . . . . . . . . 188
6.2
A universal rounding unit.
. . . . . . . . . . . . . . . . . . . . . . . . . 212
6.3
Resulting type of operations among the basic data sets. . . . . . . . . . . 226
6.4
Table of intersections and interval hulls between the basic interval types. . 227
6.5
Type of results of matrix and vector operations. . . . . . . . . . . . . . . 230
6.6
Type of the result of matrix and vector operations. . . . . . . . . . . . . . 232


Index
(AO1), 13
(AO2), 13
(D1), 42
(D2), 42
(D3), 42
(D4), 42
(D5), 42
(D6), 43
(D7), 43
(D8), 43
(D9), 43
(O1), 12
(O2), 12
(O3), 12
(O4), 12
(O5), 17
(O6), 17
(O7), 17
(OA’), 35
(OA), 35
(OD1), 43
(OD2), 43
(OD3), 43
(OD4), 43
(OD5), 43
(OD6), 43
(OV1), 53
(OV2), 53
(OV3), 54
(OV4), 54
(OV5), 54
(R), 25
(R1), 6, 30
(R2), 7, 30
(R3), 30
(R4), 7, 62
(RG), 6, 35, 62
(RG1), 35, 40
(RG2), 35, 40
(RG3), 35, 40
(RG4), 65
(S1), 24
(S2), 24
(S3), 63
(V1), 53
(V2), 53
(V3), 53
(V4), 53
(V5), 53
(VD1), 53
(VD2), 53
(VD3), 53
(VD5), 53
A
absolute error, 170, 174
absolute value, 155, 175, 329
absolute value matrix, 175
AC, 190
accumulate, 246
accumulator, 190, 192, 198, 206
ACRITH, 277
ACRITH-XSC, 277
add, 278
adder tree, 194
addition, 197, 201, 202, 215, 221, 237,
345, 348
additive group, 358
additive inverse, 177
advanced computer arithmetic, xii, xiii,
xvi, 10, 302
algebraic homomorphism, 61
algebraic structure, 12, 34, 155
algorithmic differentiation, 255, 318
and, 188
antireﬂexively ordered set, 13

402
Index
antireﬂexivity, 13
antisymmetric, 7, 63, 64, 122
antisymmetric rounding, 7, 62, 69, 75,
83, 88, 92, 98, 108, 118, 162,
223
antisymmetry, 12, 162
applied mathematics, 2
approximate solution, 353
arithmetic expression, xv, 307, 311, 314,
317, 336
arithmetic operation, 222, 233
arithmetic unit, 247
associative, 168
Auﬂaufenlassen, 248
automatic result veriﬁcation, 302
automatic differentiation, xv, 255, 302,
313, 318, 320, 322, 323
axiomatic method, 154
B
b-adic expansion, 156, 158, 160
b-adic system, 156
b-complement, 165, 211
base, 157, 160, 256
basic computer arithmetic, xi, xii, xiv,
5, 10, 247
biased exponent, 179
binary digit, 1
binary number system, 165
bit, 1
borrow, 261
boundary value problem, 322
bounded, 16
bounds for a scalar product, 38
bounds of a ﬁnite sum, 37
Brouwer ﬁxed-point theorem, 329, 332
C
cancellation, 253
carry, 190, 245, 258, 261, 283
carry propagation, 260, 261
carry register, 190
carry resolution, 257, 261, 262
carry-save-adder, 192, 260
carry-select-adder, 191, 285
case selection, 233
centered form, 302, 313, 314
chaotic solution, 322
clear instruction, 278
commutative group, 155
comparison, 233, 237, 240
compatibility property, 35, 65, 66, 69
compiler, 323
complete, xiv, 245, 261, 277, 299
complete sublattice, 21
complete arithmetic, xiv, xv, 245, 277,
300, 334
complete expression, 336
complete inf-subnet, 21, 22, 25
complete lattice, 17, 19, 30, 63, 85,
106, 155
complete operator system, 188, 189
complete register, xiv, 246, 261, 263,
277, 343, 345, 347–349
complete sublattice, 22, 26
complete subnet, 21, 107
complete sup-subnet, 21, 22, 25
complete variable, 345, 347
completely ordered, 17
complex ﬂoating-point number, 274
complex interval, 82, 122, 126
complex interval matrix, 129, 131
complex numbers, 1, 82, 155
complex ringoid, 131
composition, 199
computer arithmetic, 60
computer speed, xii
conditionally complete, 17
conditionally complete, linearly ordered
ﬁeld, 1
conditionally completely ordered, 107
continuum, 323
convergent, 156

Index
403
convex, 13, 30, 79, 81
convex hull, 13
coprocessor, 266
CR, 263, 277
CSA, 192
cubature, 313
cycle, 283
D
data format, 277
decidability, 4
decomposition, 198
defect, 333, 338, 351, 353
defect correction, xv, 253, 338
deﬁnite integral, 319
denormal, 180
denormalized number, 178, 180
derivative, 316, 323
diameter, 312, 329, 352
differential equation, 320
differentiation arithmetic, xv, 313, 316
digit, 157
directed, 162
directed rounding, 7, 31, 32, 162
distance, 81, 155, 175, 312, 329
distance matrix, 175
distributive, 168
distributive law, 155, 169
divergent, 156
division, 196, 197, 208, 239, 345, 350
division by zero, 181
division ringoid, 43, 49, 52
dot product, 245
double precision, 174, 179, 250, 253,
260, 261, 263, 282
downwardly directed, 30
downwardly directed rounding, 30, 63,
162, 163
dual port RAM, 266
duality principle, 17
dynamic precision, 252
dynamic system, 354
E
eigenvalue problem, 322
elementary ﬂoating-point arithmetic, xiii
elementary functions, 307, 310, 318,
352
empty interval, 235, 240
enclosures of derivatives, 316
epsilon-inﬂation, 333, 334
error estimate, 69
error relation, 171
Euler–MacLaurin sum formula, 320
exact, 245
exact scalar product, xii, xiii, 116, 121,
128, 133, 175, 223, 224, 228–
230, 232, 251, 257, 279, 303,
336, 341, 351, 353
exception handling, 247
exception-free, 138, 141
exception-free interval arithmetic, 140
existence, 303
existence quantor, 356
exponent, 160, 256
exponent overﬂow, 167, 213
exponent underﬂow, 167, 213
extended format, 179
extended interval, 138, 141
extended interval arithmetic, xii, 134,
328
extended interval Newton method, xv,
134, 234, 303, 314, 327
extrapolation method, 319
F
FA, 189
false, 356
ﬁeld, 48, 154
Field Programmable Gate Array, 290
ﬁxed-point accumulation, xiv, 248
ﬁxed-point arithmetic, 248, 252
ﬁxed-point register, 247, 257
ﬁxed-point theorem, xv, 329
ﬂag, 261

404
Index
ﬂag register, 270, 285
ﬂoating-point arithmetic, 187, 196, 197,
223, 233, 248, 251, 302, 314,
336
ﬂoating-point number, 3, 161, 197, 222,
227
ﬂoating-point operation, 154, 213, 215
ﬂoating-point system, 154, 161, 169,
197
ﬂow chart, 202
ﬂow diagram, 197, 201
for all quantor, 356
FPGA, 290
fraction part, 161
full adder, 189, 191, 192
future processor, 184
G
gate, 188
gigaﬂops, xii
global maximum, 309, 311
global minimum, 309, 311, 315, 322
global optimization, xv, 302, 314, 322
globally convergent, 234
graceful underﬂow, 178, 180
gradient, 323
gradual underﬂow, 178, 180
greatest element, 15, 17, 85
group, 6
groupoid, 34, 35, 53
H
HA, 189
half adder, 189, 192
Hardware Complete Register Window,
298
Hausdorff metric, 312, 329
HCRW, 298
homomorphism, 6, 35, 61, 358
Horner scheme, 337, 338, 340
I
identity operator, 41
IEEE 754 standard, 179
IEEE 854 standard, 179
IEEE arithmetic standard, 79, 80, 252
IEEE binary ﬂoating-point arithmetic
standard, xi
IEEE ﬂoating-point arithmetic standard,
xiii, 154, 178, 250
IEEE P 754, 186, 355
ill conditioned, 343
imaginary part, 51
improper interval, 141, 236, 239
inclusion isotonally ordered, 110
inclusion isotone, 309, 311, 332
inclusion isotony, 302, 306
inclusion monotony, 306
inclusion property, 302, 306, 307, 309,
311
inclusion-isotonally ordered, 111, 114,
119, 123, 131
inclusion-isotonally ordered division ringoid,
43, 126
inclusion-isotonally ordered ringoid, 42,
43, 45, 51
inclusion-isotonally ordered upper screen
ringoid, 89, 91, 98
inclusion-isotonally ordered upper screen
vectoid, 93, 94, 103
inclusion-isotonally ordered vectoid, 42,
54–56, 103
inclusion-isotony, 93
incomparable, 12, 14, 94
indeﬁnite, 178
inequalities, 304
inf-subnet, 21
inﬁmum, 16, 352
inﬁnite series, 156
information bit, 261
initial value problem, 320
inner operation, 34
integer, 1
integer arithmetic, 3, 187

Index
405
intersection, 85, 107, 240
interval, 13, 83, 84, 106
interval analysis, 329
interval arithmetic, xiii, xv, 152, 233,
302, 304, 306, 309, 314, 320,
332
interval division, 152, 233
interval evaluation of a real function,
309
interval expression, 307, 309
interval hull, 85, 107, 240
interval mathematics, 175, 233, 302,
311
interval multiplication, 233
interval Newton method, 319, 324, 325,
327
interval Newton operator, 325
interval operation, 305, 307
interval spaces, 80
interval structure, 77
interval Taylor arithmetic, 319
interval vectoid, 93
interval vector, 330, 331
invariant, 60, 74, 75, 77
invariant with respect to semimorphism,
62
irrational number, 158
isomorphic, 1, 49, 102, 106, 124, 129
isomorphism, 6, 35, 49, 51, 52, 61, 99,
100, 103, 105, 106, 116, 121,
127, 130, 132
isotonely ordered, 7
iterative reﬁnement, xv, 253, 338, 341
J
John von Neumann adder, 192
K
Krawczyk-operator, 253, 254, 331, 335
L
lattice, 17
lattice operation, 233
leading zero anticipation, 274
least element, 15, 17
left neutral element, 34
linearly ordered, 12
linearly ordered division ringoid, 95,
96
linearly ordered ringoid, 45, 47, 48, 66,
101, 102, 106, 110, 114
linearly ordered set, 96, 154
local memory, 257, 291
logical expression, 356
logical operator, 188, 356
logical statement, 356
long accumulator, 200, 201, 208, 258
long adder, 257, 258, 283, 288
long interval, 341, 344, 347, 351–353
long interval arithmetic, 300, 354
long real, 253, 344
long real arithmetic, 300
long register, 261
long shift, 257, 261, 283, 284, 288
lower semiscreen, 24
lower bound, 15
lower neighbor, 14
lower outer screen operation, 40
lower screen, 25
lower screen groupoid, 35
M
mainframe, 270
mantissa, 3, 160, 197, 204, 206, 209,
222, 256, 344
matrix, 50, 82, 98, 115, 330
matrix algebra, 48
Matrix Market, 253
matrix operation, 231
matrix product, 172
maximal element, 14
MD, 194
mean-value theorem, 313
megaﬂops, xii

406
Index
metric, 82, 155
metric space, 81, 312
metric structure, 12
midpoint, 352
minimal element, 14
minus operator, 44, 66, 77, 81
monotone mapping, 111
monotone outer screen operation, 40
monotone rounding, 7, 30, 32, 33, 62,
166, 169, 172
monotone screen groupoid, 35
monotone upwardly directed rounding,
64
monotonicity, 162
MR, 194
multiple precision arithmetic, xvi, 254,
304, 343, 354
multiple precision interval arithmetic,
xvi
multiple precision number, 343–345
multiplicand, 194
multiplication, 194, 197, 207, 221, 238,
345, 349
multiplicative, 53
multiplicative vectoid, 55, 57, 58, 105
multiplier, 194
multiply, 278
multiply and accumulate, xii, 234, 245,
246
multiply and add fused, 197, 290
N
nand, 189
natural numbers, 1
necessary condition, 78
negation, 345
neutral element, 34, 44, 50, 63, 66, 155
Newton method, xv, 134, 196, 233, 303,
319, 324, 327, 330
Newton operator, 328
nonlinear problem, 303
nor, 189
normalization, 198, 199, 202, 206, 207,
209, 216
normalized, 160
normalized ﬂoating-point number, 77,
161, 256
normalized ﬂoating-point system, 177
not, 188
numerical integration, 313, 314
numerical quadrature, 320
O
operand selection, 235, 237
operation, 34
operator overloading, 347, 351
or, 188
order diagram, 14, 15, 17, 20
order homomorphism, 61
order isomorphism, 99, 105, 124
order relation, 12, 50, 85
order structure, 12, 155
ordered, 7
ordered algebraic structure, 34, 61, 121
ordered division ringoid, 43, 45, 89, 91
ordered groupoid, 35
ordered multiplicative vectoid, 55
ordered operation, 34
ordered ringoid, 42, 43, 45, 50, 51, 69,
89, 94, 98, 99, 103, 110, 114
ordered set, 12, 50, 84
ordered vectoid, 42, 54, 55, 57, 69, 94,
103
ordering, 7
ordinary differential equation, 322
outer multiplication, 53, 103
outer operation, 34, 40, 119
outer screen operation, 40
overestimation, 312, 313
overﬂow, 167, 172, 181, 206, 213, 216,
248, 252, 257
P
parallel adder, 191, 193

Index
407
partial sum technique, xiv, 246
partially ordered, 12
PASCAL-XSC, 268, 310, 323
periodic solution, 322
Perron and Frobenius theorem, 332
personal computer, 263
petaﬂops, xii, xvi
pipeline, 245, 257, 263, 293
pipeline conﬂict, 294, 295
pipelining, 246
point interval, 310
polynomial, xv, 337, 340, 353
power set, 6, 8, 13, 19, 48, 77, 84, 305
precision, 344
problem solving routine, xvi
product set, 14, 19, 69
product space, 9
programming language, 10
pure mathematics, 2
Q
quadrature, 313
quadruple precision, 255
quadruple precision arithmetic, 304, 343
R
rational number, 157
rational numbers, 1
real algorithm, 318
real analysis, 155
real intervals, 82
real numbers, 1, 82, 154
real part, 51
reciprocal, 196
redundant number representation, 297
reﬂexivity, 12
register space, 257, 258
relative error, 170
relative rounding error, 166
remainder term, 320
residual, 333, 339
residual correction, 334
result selection, 237
reverse mode, 255, 324
right neutral element, 34, 55
ring, 6, 7, 44, 48
ringoid, 7, 42, 44, 49, 50, 52, 53, 58,
63, 69, 77, 80
ringoid isomorphism, 72, 124, 125
round-to-nearest-even, 180
rounded groupoid, 35
rounding, 6, 30, 198, 209, 256, 272
rounding downwards, 233
rounding error, 166
rounding invariance, 64, 74
rounding to nearest, 8
rounding to the nearest ﬂoating-point
number, 163
rounding to the nearest number, 169
rounding towards zero, 162, 210, 345
rounding upwards, 233
Rump method, 335
Rump-algorithm, 254
Rump-operator, 254, 334, 335
running total, 248
runtime system, 323
S
scalar product, 171, 245, 279, 346, 351
scalar product unit, 257, 263
scaling, 213, 248
screen, 24–26, 69, 106, 118, 131
screen division ringoid, 71
screen gruppoid, 35
screen ringoid, 66, 70, 71, 74
screen vectoid, 66
semimorphic operation, xiii, 197
semimorphism, xi, xiii, 3, 7, 9, 60, 62,
63, 66, 69, 71, 74, 77, 79, 80,
83, 88, 98, 99, 103, 108–110,
114, 116, 119, 123, 129, 154,
168, 171, 223–225, 227, 247
sequence, 155
serial adder, 190

408
Index
series, 155
set operations, 305
sharp symbol, 337
short accumulator, 200, 206, 215, 221,
222
short adder, 257–259, 291
sign, 160, 256
signed-magnitude representation, 164,
197, 256
signiﬁcand, 3, 161, 256
single precision, 179, 260
slope, 313
slope arithmetic, 314
special element, 49
special elements, 66
special functions, 310
spectral radius, 330
SPU, 263
square root, 351
staggered precision, 255, 344
standard functions, 307
step function, 166
step size control, 319
structure, 6, 60
subdistributivity, 349
subdivision, 302, 312, 322
subinterval, 315
sublattice, 21
subnet, 21
subnormal, 180
subnormal number, 178
subtract, 278
subtraction, 44, 54, 197, 201, 221, 238,
348
sufﬁcient condition, 78
summing matrix, 284, 288
sup-subnet, 21
supercomputer, 282
supremum, 16, 352
switch, 188
symmetric, 81
symmetric screen, 63, 69, 108, 110, 114,
118
symmetric upper screen, 88, 92, 122
syntax diagram, 307
system of equations, 304
system of linear equations, xv, 303, 305,
329, 338, 342, 353
system of nonlinear equations, 314, 342
T
Taylor arithmetic, 319, 323
Taylor coefﬁcient, 323
Taylor polynomial, 320, 322
teraﬂops, xii, xvi
topological structure, 12
totally ordered, 12
transfer function, 227
transitivity, 12, 13
translation invariant, 81, 82
trapezoidal rule, 319
true, 356
truncation, 165, 210, 211
type transfer, 227, 231
U
ulp, 10
underﬂow, 167, 172, 213, 248, 252
unique additive inverse, 78, 81, 82
uniqueness, 77, 81, 303
unnormalized, 77, 256
unnormalized mantissa, 79, 164, 177
upper semiscreen, 24
upper bound, 15
upper outer screen operation, 40
upper screen, 24, 25, 85, 92
upper screen division ringoid, 111, 123
upper screen groupoid, 35, 37
upper screen ringoid, 114
upper screen vectoid, 93, 119, 132
upwardly directed, 8, 30, 83

Index
409
upwardly directed rounding, 30, 63, 86,
88, 92, 98, 99, 101, 108, 109,
118, 122, 129, 134, 162, 163,
176, 224, 225
V
validated numerical computation, 303
validated numerics, xiii
vectoid, 7, 42, 53, 55, 57, 63, 80, 118
vector arithmetic coprocessor, 266
vector operation, 231
vector processing, 246
vector processor, 282
vector space, 6, 7, 48
vectorizing compiler, 246
veriﬁcation, 4
veriﬁed computing, xiii, xvi
W
Wallace tree, 194, 247, 260
weakly ordered, 7, 126
weakly ordered division ringoid, 43, 123,
124
weakly ordered ringoid, 42, 43, 45, 47,
50, 51, 53, 63, 69, 87, 89, 91,
94, 98, 99, 103, 105, 110, 129,
130
weakly ordered vectoid, 42, 53, 55, 57,
63, 69, 92, 93, 103, 105, 118,
119, 131
word, 198
workstation, 270
X
XSC-languages, 254, 268, 277, 279
Z
zero, 314
Zuse, 178, 247



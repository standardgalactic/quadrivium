 
 
 
Nonlinear System Theory 
 
The Volterra/Wiener Approach 
 
by 
 
Wilson J. Rugh 
 
 
 
 
 
 
Originally published by The Johns Hopkins 
University Press, 1981 (ISBN O-8018-2549-0). Web 
version prepared in 2002. 
 

Contents 
 
 
 
 
PREFACE 
CHAPTER 1  Input/Output Representations in the Time Domain     1 
1.1  Linear Systems     1 
1.2  Homogeneous Nonlinear Systems     3 
1.3  Polynomial and Volterra Systems     18 
1.4  Interconnections of Nonlinear Systems     21 
1.5  Heuristic and Mathematical Aspects     34 
1.6  Remarks and References     37 
1.7  Problems     42 
Appendix 1.1  Convergence Conditions for Interconnections 
of Volterra Systems     44 
Appendix 1.2  The Volterra Representation for Functionals     49 
CHAPTER 2  Input/Output Representations in the Transform Domain     54 
2.1  The Laplace Transform     54 
2.2  Laplace Transform Representation of Homogeneous Systems     60 
2.3  Response Computation and the Associated Transform     68 
2.4  The Growing Exponential Approach     75 
2.5  Polynomial and Volterra Systems     81 
2.6  Remarks and References     85 
2.7  Problems     87 
CHAPTER 3  Obtaining Input/Output Representations from 
Differential-Equation Descriptions     93 
3.1  Introduction      94 
3.2  A Digression on Notation     103 
3.3  The Carleman Linearization Approach     105 
3.4  The Variational Equation Approach     116 
3.5  The Growing Exponential Approach     124 
3.6  Systems Described by 
th
N
−Order Differential Equations     127 
3.7  Remarks and References     131 

3.8  Problems     135 
Appendix 3.1  Convergence of the Volterra Series Representation 
for Linear-Analytic State Equations     137 
CHAPTER 4  Realization Theory     142 
4.1  Linear Realization Theory     142 
4.2  Realization of Stationary Homogeneous Systems     152 
4.3  Realization of Stationary Polynomial and Volterra Systems     163 
4.4  Properties of Bilinear State Equations     173 
4.5  The Nonstationary Case     180 
4.6  Remarks and References     183 
4.7  Problems     191 
Appendix 4.1  Interconnection Rules for the Regular Transfer Function     194 
CHAPTER 5  Response Characteristics of Stationary Systems     199 
5.1  Response to Impulse Inputs     199 
5.2  Steady-State Response to Sinusoidal Inputs     201 
5.3  Steady-State Response to Multi-Tone Inputs     208 
5.4  Response to Random Inputs     214 
5.5  The Wiener Orthogonal Representation     233 
5.6  Remarks and References     246 
5.7  Problems     250 
CHAPTER 6  Discrete-Time Systems     253 
6.1  Input/Output Representations in the Time Domain     253 
6.2  Input/Output Representations in the Transform Domain     256 
6.3  Obtaining Input/Output Representations from State Equations     263 
6.4  State-Affine Realization Theory     269 
6.5  Response Characteristics of Discrete-Time Systems     277 
6.6  Bilinear Input/Output Systems     287 
6.7  Two-Dimensional Linear Systems     292 
6.8  Remarks and References     298 
6.9  Problems     301 
CHAPTER 7  Identification     303 
7.1  Introduction     303 
7.2  Identification Using Impulse Inputs     305 

7.3  Identification Based on Steady-State Frequency Response     308 
7.4  Identification Using Gaussian White Noise Inputs     313 
7.5  Orthogonal Expansion of the Wiener Kernels     322 
7.6  Remarks and References     326 
7.7  Problems     329 

PREFACE
When confronted with a nonlinear systems engineering problem, the ﬁrst approach
usually is to linearize; in other words, to try to avoid the nonlinear aspects of the problem.
It is indeed a happy circumstance when a solution can be obtained in this way. When it
cannot, the tendency is to try to avoid the situation altogether, presumably in the hope that
the problem will go away. Those engineers who forge ahead are often viewed as foolish,
or worse. Nonlinear systems engineering is regarded not just as a difﬁcult and confusing
endeavor; it is widely viewed as dangerous to those who think about it for too long.
This skepticism is to an extent justiﬁable. When compared with the variety of
techniques available in linear system theory, the tools for analysis and design of nonlinear
systems are limited to some very special categories. First, there are the relatively simple
techniques, such as phase-plane analysis, which are graphical in nature and thus of limited
generality. Then, there are the rather general (and subtle) techniques based on the theory
of differential equations, functional analysis, and operator theory. These provide a
language, a framework, and existence/uniqueness proofs, but often little problem-speciﬁc
information beyond these basics. Finally, there is simulation, sometimes ad nauseam, on
the digital computer.
I do not mean to say that these techniques or approaches are useless. Certainly
phase-plane analysis describes nonlinear phenomena such as limit cycles and multiple
equilibria of second-order systems in an efﬁcient manner. The theory of differential
equations has led to a highly developed stability theory for some classes of nonlinear
systems. (Though, of course, an engineer cannot live by stability alone.) Functional
analysis and operator theoretic viewpoints are philosophically appealing, and undoubtedly
will become more applicable in the future. Finally, everyone is aware of the occasional
success story emanating from the local computer center.
What I do mean to say is that a theory is needed that occupies the middle ground in
generality and applicability. Such a theory can be of great importance for it can serve as a
starting point, both for more esoteric mathematical studies and for the development of
engineering techniques. Indeed, it can serve as a bridge or communication link between
these two activities.
In the early 1970s it became clear that the time was ripe for a middle-of-the-road
formulation for nonlinear system theory. It seemed that such a formulation should use
some aspects of differential- (or difference-) equation descriptions, and transform
representations, as well as some aspects of operator-theoretic descriptions. The question
was whether, by making structural assumptions and ruling out pathologies, a reasonably
1

simple, reasonably general, nonlinear system theory could be developed. Hand in hand
with this viewpoint was the feeling that many of the approaches useful for linear systems
ought to be extensible to the nonlinear theory. This is a key point if the theory is to be
used by practitioners as well as by researchers.
These considerations led me into what has come to be called the Volterra/Wiener
representation for nonlinear systems.
Articles on this topic had been appearing
sporadically in the engineering literature since about 1950, but it seemed to be time for an
investigation that incorporated viewpoints that in recent years proved so successful in
linear system theory. The ﬁrst problem was to specialize the topic, both to avoid the
vagueness that characterized some of the literature, and to facilitate the extension of linear
system techniques. My approach was to consider those systems that are composed of
feedback-free interconnections of linear dynamic systems and simple static nonlinear
elements.
Of course, a number of people recognized the needs outlined above. About the same
time that I began working with Volterra/Wiener representations, others achieved a notable
success in specializing the structure of nonlinear differential equations in a proﬁtable way.
It was shown that bilinear state equations were amenable to analysis using many of the
tools associated with linear state equations. In addition, the Volterra/Wiener representation
corresponding to bilinear state equations turned out to be remarkably simple.
These
topics,
interconnection-structured
systems,
bilinear
state
equations,
Volterra/Wiener representations, and their various interleavings form recurring themes in
this book. I believe that from these themes will be forged many useful engineering tools
for dealing with nonlinear systems in the future. But a note of caution is appropriate.
Nonlinear systems do not yield easily to analysis, especially in the sense that for a given
analytical method it is not hard to ﬁnd an inscrutable system. Worse, it is not always easy
to ascertain beforehand when methods based on the Volterra/Wiener representation are
appropriate.
The
folk
wisdom
is
that
if
the
nonlinearities are
mild, then
the
Volterra/Wiener methods should be tried. Unfortunately, more detailed characterization
tends to destroy this notion before capturing it, at least in a practical sense.
So, in these matters I ask some charity from the reader. My only recommendation is
the merely obvious one to keep all sorts of methods in mind. Stability questions often will
call for application of methods based on the theory of differential equations. Do not forget
the phase plane or the computer center, for they are sure to be useful in their share of
situations. At the same time I urge the reader to question and reﬂect upon the possibilities
for application
of the Volterra/Wiener methods discussed herein.
The theory is
incomplete, and likely to remain so for some time. But I hope to convince that, though the
sailing won’t be always smooth, the wind is up and the tide fair for this particular passage
into nonlinear system theory - and that the engineering tools to be found will make the trip
worthwhile.
This text represents my ﬁrst attempt to write down in an organized fashion the
nonlinear system theory alluded to above.
As such, the effort has been somewhat
frustrating since the temptation always is to view gaps in the development as gaps, and not
as research opportunities. In particular the numerous research opportunities have forced
2

certain decisions concerning style and content. Included are topics that appear to be a
good bet to have direct and wide applicability to engineering problems. Others for which
the odds seem longer are mentioned and referenced only. As to style I eschew the
trappings of rigor and adopt a more melliﬂuous tone. The material is presented informally,
but in such a way that the reader probably can formalize the treatment relatively easily
once the main features are grasped. As an aid to this process each chapter contains a
Remarks and References section that points the way to the research literature. (Historical
comments that unavoidably have crept into these sections are general indications, not the
result of serious historical scholarship.)
The search for simple physical examples has proven more enobling than productive.
As a result, the majority of examples in the text illustrate calculations or technical features
rather than applications. The same can be said about the problems included in each
chapter. The problems are intended to illuminate and breed familiarity with the subject
matter. Although the concepts involved in the Volterra/Wiener approach are not difﬁcult,
the formulas become quite lengthy and tend to have hidden features. Therefore, I
recommend that consideration of the problems be an integral part of reading the book. For
the most part the problems do not involve extending the presented material in signiﬁcant
ways. Nor are they designed to be overly difﬁcult or open-ended. My view is that the
diligent reader will be able to pose these kinds of problems with alacrity.
The background required for the material in this book is relatively light if some
discretion is exercised. For the stationary system case, the presumed knowledge of linear
system theory is not much beyond the typical third- or fourth-year undergraduate course
that covers both state-equation and transfer-function concepts. However, a dose of the
oft-prescribed mathematical maturity will help, particularly in the more abstract material
concerning realization theory.
As background for some of the material concerning
nonstationary systems, I recommend that the more-or-less typical material in a ﬁrst-year
graduate course in linear system theory be studied, at least concurrently. Finally, some
familiarity with the elements of stochastic processes is needed to appreciate fully the
material on random process inputs.
I would be remiss indeed if several people have who worked with me in the
nonlinear systems area were not mentioned. Winthrop W. Smith, Stephen L. Baumgartner,
Thurman R. Harper, Edward M. Wysocki, Glenn E. Mitzel, and Steven J. Clancy all
worked on various aspects of the material as graduate students at The Johns Hopkins
University. Elmer G. Gilbert of the University of Michigan contributed much to my
understanding of the theory during his sabbatical visit to The Hopkins, and in numerous
subsequent discussions. Arthur E. Frazho of Purdue University has been most helpful in
clarifying my presentation of his realization theory. William H. Huggins at Johns Hopkins
introduced me to the computer text processor, and guided me through a sometimes stormy
author-computer relationship. It is a pleasure to express my gratitude to these colleagues
for their contributions.
3

PREFACE TO WEB VERSION
Due to continuing demand, small but persistent, for the material, I have created this
Web version in .pdf format. The only change to content is the correction of errors as listed
on on the errata sheet (available on my Web site). However, the pagination is different, for
I had to re-process the source ﬁles on aged, cantankerous typesetting software. Also, the
ﬁgures are redrawn, only in part because they were of such low quality in the original. I
have not created new subject and author indexes corresponding to the new pagination.
Permission is granted to access this material for personal or non-proﬁt educational
use only. Use of this material for business or commercial purposes is prohibited.
Wilson J. Rugh
Baltimore, Maryland, 2002
4

CHAPTER 1
INPUT/OUTPUT REPRESENTATIONS
IN THE TIME DOMAIN
The Volterra/Wiener representation for nonlinear systems is based on the Volterra
series functional representation from mathematics. Though it is a mathematical tool, the
application to system input/output representation can be discussed without ﬁrst going
through the mathematical development. I will take this ad hoc approach, with motivation
from familiar linear system representations, and from simple examples of nonlinear
systems. In what will become a familiar pattern, linear systems will be reviewed ﬁrst.
Then homogeneous nonlinear systems (one-term Volterra series), polynomial systems
(ﬁnite Volterra series), and ﬁnally Volterra systems (inﬁnite series) will be discussed in
order.
This chapter is devoted largely to terminology, introduction of notation, and basic
manipulations concerning nonlinear system representations. A number of different ways of
writing the Volterra/Wiener representation will be reviewed, and interrelationships
between them will be established. In particular, there are three special forms for the
representation that will be treated in detail: the symmetric, triangular, and regular forms.
Each of these has advantages and disadvantages, but all will be used in later portions of
the book. Near the end of the chapter I will discuss the origin and justiﬁcation of the
Volterra series as applied to system representation. Both the intuitive and the more
mathematical aspects will be reviewed.
1.1 Linear Systems
Consider the input/output behavior of a system that can be described as single-input,
single-output, linear, stationary, and causal. I presume that the reader is familiar with the
convolution representation
y (t) =
−∞∫
∞
h (σ)u (t −σ) dσ
(1)
where u (t) is the input signal, and y (t) is the output signal. The impulse response h (t),
1

herein called the kernel, is assumed to satisfy h (t) = 0 for t < 0.
There are several technical assumptions that should go along with (1). Usually it is
assumed that h (t) is a real-valued function deﬁned for t ε (−∞,∞), and piecewise
continuous except possibly at t = 0 where an impulse (generalized) function can occur.
Also the input signal is a real-valued function deﬁned for t ε (−∞,∞); usually assumed to
be piecewise continuous, although it also can contain impulses. Finally, the matter of
impulses aside, these conditions imply that the output signal is a continuous, real-valued
function deﬁned for t ε (−∞,∞).
More general settings can be adopted, but they are unnecessary for the purposes
here. In fact, it would be boring beyond the call of duty to repeat these technical
assumptions throughout the sequel. Therefore, I will be casual and leave these issues
understood, except when a particularly cautious note should be sounded.
It probably is worthwhile for the reader to verify that the system descriptors used
above are valid for the representation (1). Of course linearity is obvious from the
properties of the integral. It is only slightly less easy to see that the one-sided assumption
on h (t) corresponds to causality; the property that the system output at a given time cannot
depend on future values of the input. Finally, simple inspection shows that the response to
a delayed version of the input u (t) is the delayed version of the response to u (t), and thus
that the system represented by (1) is stationary. Stated more precisely, if the response to
u (t) is y (t), then the response to u (t −T) is y (t −T), for any T ≥ 0, and hence the system is
stationary.
The one-sided assumption on h (t) implies that the inﬁnite lower limit in (1) can be
replaced by 0. Considering only input signals that are zero prior to t = 0, and often this
will be the case, allows the upper limit in (1) to be replaced by t. The advantage in
keeping inﬁnite limits is that in the many changes of integration variables that will be
performed on such expressions, there seldom is a need to change the limits. One of the
disadvantages is that some manipulations are made to appear more subtle than they are.
For example, when the order of multiple integrations is interchanged, I need only remind
that the limits actually are ﬁnite to proceed with impunity.
A change of the integration variable shows that (1) can be rewritten as
y (t) =
−∞∫
∞
h (t −σ)u (σ) dσ
(2)
In this form the one-sided assumption on h (t) implies that the upper limit can be lowered
to t, while a one-sided assumption on u (t) would allow the lower limit to be raised to 0.
The representation (1) will be favored for stationary systems - largely because the kernel is
displayed with unmolested argument, contrary to the form in (2).
To diagram a linear system from the input/output point of view, the labeling shown
in Figure 1.1 will be used. In this block diagram the system is denoted by its kernel. If the
kernel is unknown, then Figure 1.1 is equivalent to the famous linear black box.
2

Figure 1.1. A stationary linear system.
 
If the assumption that the system is stationary is removed, then the following
input/output representation is appropriate.
Corresponding to the real-valued function
h (t,σ) deﬁned for t ε (−∞,∞), σ ε (−∞,∞), with h (t,σ) = 0 if σ > t, write
y (t) =
−∞∫
∞
h (t,σ)u (σ) dσ
(3)
As before, it is easy to check that this represents a linear system, and that the special
assumption on h (t,σ) corresponds to causality. Only the delay-invariance property that
corresponds to stationarity has been dropped. Typically h (t,σ) is allowed to contain
impulses for σ = t, but otherwise is piecewise continuous for t ≥ σ ≥ 0. Of course, the
range of integration in (3) can be narrowed as discussed before.
Comparison of (2) and (3) makes clear the fact that a stationary linear system can be
regarded as a special case of a nonstationary linear system. Therefore, it is convenient to
call the kernel h (t,σ) in (3) stationary if there exists a kernel g (t) such that
g (t −σ) = h (t,σ)
(4)
An
easy
way
to
check
for
stationarity
of
h (t,σ)
is
to
check
the
condition
h (0,σ−t) = h (t,σ).
If this is satisﬁed, then setting g (t) = h (0,−t) veriﬁes (4) since
g (t −σ) = h (0,σ−t) = h (t,σ).
A (possibly) nonstationary linear system will be diagramed using the representation
(3) as shown in Figure 1.2.
Figure 1.2. A nonstationary linear system.
1.2 Homogeneous Nonlinear Systems
The approach to be taken to the input/output representation of nonlinear systems
involves a simple generalization of the representations discussed in Section 1.1. The more
difﬁcult, somewhat unsettled, and in a sense philosophical questions about the generality
and usefulness of the representation will be postponed. For the moment I will write down
the representation, discuss some of its properties, and give enough examples to permit the
claim that it is interesting.
Corresponding to the real-valued function of n variables hn(t 1, . . . ,tn) deﬁned for
ti ε (−∞,∞), i = 1, . . . ,n, and such that hn(t 1, . . . ,tn) = 0 if any ti < 0, consider the
3
u
h(t)
y
u
h(t,σ)
y

input/output relation
y (t) =
−∞∫
∞
. . .
−∞∫
∞
hn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(5)
The resemblance to the linear system representations of the previous section is clear.
Furthermore the same kinds of technical assumptions that are appropriate for the
convolution representation for linear systems in (1) are appropriate here. Indeed, (5) often
is called generalized convolution, although I won’t be using that term.
Probably the ﬁrst question to be asked is concerned with the descriptors that can be
associated to a system represented by (5).
It is obvious that the assumption that
hn(t 1, . . . ,tn) is one-sided in each variable corresponds to causality. The system is not
linear, but it is a stationary system as a check of the delay invariance property readily
shows.
A system represented by (5) will be called a degree-n homogeneous system. The
terminology arises because application of the input αu (t), where α is a scalar, yields the
output αny (t), where y (t) is the response to u (t). Note that this terminology includes the
case of a linear system as a degree-1 homogeneous system. Just as in the linear case,
hn(t 1, . . . ,tn) will be called the kernel associated with the system.
For simplicity of notation I will collapse the multiple integration and, when no
confusion is likely to arise, drop the subscript on the kernel to write (5) as
y (t) =
−∞∫
∞
h (σ1, . . . ,σn)u (t−σ1) . . . u (t−σn) dσ1 . . . dσn
(6)
Just as in the linear case, the lower limit(s) can be replaced by 0 because of the one-sided
assumption on the kernel. If it is assumed also that the input signal is one-sided, then all
the upper limit(s) can be replaced by t. Finally, a change of each variable of integration
shows that (6) can be rewritten in the form
y (t) =
−∞∫
∞
h (t −σ1, . . . ,t−σn)u (σ1) . . . u (σn) dσ1 . . . dσn
(7)
At this point it should be no surprise that a stationary degree-n homogeneous system
will be diagramed as shown in Figure 1.3. Again the system box is labeled with the kernel.
 
Figure 1.3. A stationary degree-n homogeneous system.
There are at least two generic ways in which homogeneous systems can arise in
engineering applications. The ﬁrst involves physical systems that naturally are structured
in terms of interconnections of linear subsystems and simple nonlinearities. In particular I
will consider situations that involve stationary linear subsystems, and nonlinearities that
4
u
y
h(t1,…,tn)

can be represented in terms of multipliers.
For so-called interconnection structured
systems such as this, it is often easy to derive the overall system kernel from the subsystem
kernels simply by tracing the input signal through the system diagram. (In this case
subscripts will be used to denote different subsystems since all kernels are single variable.)
Example 1.1
Consider the multiplicative connection
 
 
of three linear subsystems, shown in Figure 1.4. The linear subsystems can be described
by
yi(t) =
−∞∫
∞
hi(σ)u (t −σ) dσ ,
i = 1,2,3
and thus the overall system is described by
y (t) = y 1(t)y 2(t)y 3(t)
=
−∞∫
∞
h 1(σ)u (t −σ) dσ
−∞∫
∞
h 2(σ)u (t −σ) dσ
−∞∫
∞
h 3(σ)u (t−σ) dσ
=
−∞∫
∞
h 1(σ1)h 2(σ2)h 3(σ3)u (t −σ1)u (t −σ2)u (t −σ3) dσ1dσ2dσ3
Clearly, a kernel for this degree-3 homogeneous system is
h (t 1,t 2,t 3) = h 1(t 1)h 2(t 2)h 3(t 3)
A second way in which homogeneous systems can arise begins with a state equation
description of a nonlinear system. To illustrate, consider a compartmental model wherein
each variable xi(t) represents a population, chemical concentration, or other quantity of
interest. If the rate of change of xi(t) depends linearly on other xj(t)’s, but with a scalar
parametric control signal, then x.
i(t) will contain terms of the form du (t)xj(t). Nonlinear
compartmental models of this type lead to the study of so-called bilinear state equations
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t), t ≥0, x (0) = x 0
where x (t) is the n x 1 state vector, and u (t) and y (t) are the scalar input and output
signals. Such state equations will be discussed in detail later on, so for now a very simple
5
Figure 1.4.   An interconnection structured system.
h1(t)
u
h2(t)
y
h3(t)
Π

case will be used to indicate the connection to homogeneous systems.
Example 1.2
Consider a nonlinear system described by the differential equation
x.(t) = Dx (t)u (t) + bu (t)
y (t) = cx (t) , t ≥0, x (0) = 0
where x (t) is a 2 x 1 vector, u (t) and y (t) are scalars, and
D = H
I 1
0
0
0J
K , b = H
I 0
1J
K , c = [0 1]
It can be shown that a differential equation of this general form has a unique solution for
all t ≥ 0 for a piecewise continuous input signal. I leave it as an exercise to verify that this
solution can be written in the form
x (t) =
0∫
t
e
D
σ2∫
t
u (σ1)dσ1
bu (σ2) dσ2
where, of course, the matrix exponential is given by
e
D
σ2∫
t
u (σ1)dσ1
= I + D
σ2∫
t
u (σ1) dσ1 + 2!
1
___D2[
σ2∫
t
u (σ1) dσ1]2 + . . .
For the particular case at hand, D2 = 0 so that
e
D
σ2∫
t
u (σ1)dσ1
=
H
A
A
A
I σ2∫
t
u (σ1)dσ1
1
1
0
J
A
A
A
K
Thus the input/output relation can be written in the form
y (t) =
0∫
t
ce
D
σ2∫
t
u (σ1)dσ1
bu (σ2) dσ2
=
0∫
t
σ2∫
t
u (σ1)u (σ2) dσ1dσ2
From this expression it is clear that the system is homogeneous and of degree 2. To put the
input/output representation into a more familiar form, the unit step function
δ−1(t) =
B
C
D 1, t ≥0
0, t < 0
can be introduced to write
6

h2(t)
v
Π
h1(t)
w
y
y (t) =
0∫
t
0∫
t
δ−1(σ1−σ2)u (σ1)u (σ2) dσ1dσ2
Thus, a kernel for the system is
h (t 1,t 2) = δ−1(t 1−t 2)
There will be occasion in later chapters to consider homogeneous systems that may
not be stationary. Such a system is represented by the input/output expression
y (t) =
−∞∫
∞
h (t,σ1, . . . ,σn)u (σ1) . . . u (σn) dσ1 . . . dσn
(8)
It is assumed that the kernel satisﬁes h (t,σ1, . . . ,σn) = 0 when any σi > t so that the
system is causal. Of course, this permits all the upper limits to be replaced by t. If one-
sided inputs are considered, then the lower limits can be raised to 0.
As a simple example of a nonstationary homogeneous system, the reader can rework
Example 1.1 under the assumption that the linear subsystems are nonstationary. But I will
consider here a case where the nonstationary representation quite naturally arises from a
stationary interconnection structured system.
Example 1.3
The interconnection shown in Figure 1.5 is somewhat more complicated
than that treated in Example 1.1. As suggested earlier, a good way to ﬁnd a kernel is to
begin with the input signal and ﬁnd expressions for each labeled signal, working toward
the output. The signal v (t) can be written as
 
 
 
v (t) =
−∞∫
t
h 3(t −σ3)u (σ3) dσ3 u (t)
Similarly
7
Π
h3(t)
u
Figure 1.5.  The system considered in Example 1.3.

w (t) =
−∞∫
t
h 2(t −σ2)v (σ2) dσ2 u (t)
=
−∞∫
t
h 2(t −σ2)
−∞∫
σ2
h 3(σ2−σ3)u (σ3) dσ3 u (σ2) dσ2 u (t)
=
−∞∫
t
−∞∫
σ2
h 2(t −σ2)h 3(σ2−σ3)u (σ3)u (σ2) dσ3dσ2 u (t)
The output signal is given by
y (t) =
−∞∫
t
h 1(t−σ1)w (σ1) dσ1
=
−∞∫
t
−∞∫
σ1
−∞∫
σ2
h 1(t −σ1)h 2(σ1−σ2)h 3(σ2−σ3)u (σ1)u (σ2)u (σ3) dσ3dσ2dσ1
Thus a kernel for this degree-3 system can be written in the form
h(t,σ1,σ2,σ3) = h 1(t −σ1)h 2(σ1−σ2)h 3(σ2−σ3)δ−1(σ2−σ3)δ−1(σ1−σ2)
Because of the usual one-sided assumptions on the linear subsystem kernels, the step
functions might be regarded as superﬂuous. More importantly, a comparison of Examples
1.1 and 1.3 indicates that different forms of the kernel are more natural for different system
structures.
Comparing the representation (8) for nonstationary systems to the representation (7)
for stationary systems leads to the deﬁnition that a kernel h (t,σ1, . . . ,σn) is stationary if
there exists a kernel g (t 1, . . . ,tn) such that the relationship
g (t −σ1, . . . ,t −σn) = h (t,σ1, . . . ,σn)
(9)
holds for all t,σ1, . . . ,σn. Usually it is convenient to check for stationarity by checking
the functional relationship
h (0,σ1−t, . . . ,σn−t) = h (t,σ1, . . . ,σn)
(10)
for if this is satisﬁed, then (9) is obtained by setting
g (t 1, . . . ,tn) = h (0,−t 1, . . . ,−tn)
(11)
Therefore, when (10) is satisﬁed I can write, in place of (8),
y (t) =
−∞∫
∞
g (t −σ1, . . . ,t −σn)u (σ1) . . . u (σn) dσ1 . . . dσn
(12)
Performing this calculation for Example 1.3 gives a stationary kernel for the system in
Figure 1.5:
g (t 1,t 2,t 3) = h 1(t 1)h 2(t 2−t 1)h 3(t 3−t 2)δ−1(t 3−t 2)δ−1(t 2−t 1)
8

As mentioned in Section 1.1, in the theory of linear systems it is common to allow
impulse
(generalized)
functions
in
the
kernel.
For
example,
in
(1)
suppose
h (t) = g (t) + g 0δ0(t), where g (t) is a piecewise continuous function and δ0(t) is a unit
impulse at t = 0. Then the response to an input u (t) is
y (t) =
−∞∫
∞
h (t −σ)u (σ) dσ
=
−∞∫
∞
g (t −σ)u (σ) dσ +
−∞∫
∞
g 0δ0(t−σ)u (σ) dσ
=
−∞∫
∞
g (t −σ)u (σ) dσ + g 0u (t)
(13)
That is, the impulse in the kernel corresponds to what might be called a direct
transmission term in the input/output relation. Even taking the input u (t) = δ0(t) causes
no problems in this set-up. The resulting impulse response is
y (t) =
−∞∫
∞
g (t −σ)δ0(σ) dσ +
−∞∫
∞
g 0δ0(t −σ)δ0(σ) dσ
= g (t) + g 0δ0(t)
(14)
Unfortunately these issues are much more devious for homogeneous systems of
degree n > 1. For such systems, impulse inputs cause tremendous problems when a direct
transmission term is present. To see why, notice that such a term must be of degree n, and
so it leads to undeﬁned objects of the form δ0
n(t) in the response. Since impulsive inputs
must be ruled out when direct transmission terms are present, it seems prudent to display
such terms explicitly. However, there are a number of different kinds of terms that share
similar difﬁculties in the higher degree cases, and the equations I am presenting are
sufﬁciently long already. For example, consider a degree-2 system with input/output
relation
y (t) =
−∞∫
∞
−∞∫
∞
g (t −σ1,t −σ2)u (σ1)u (σ2) dσ1dσ2
+
−∞∫
∞
g 1(t −σ1)u 2(σ1) dσ1 + g 0u 2(t)
(15)
Adopting a loose terminology, I will call both of the latter two terms direct transmission
terms. Allowing impulses in the kernel means that the representation
y (t) =
−∞∫
∞
h (t −σ1,t−σ2)u (σ1)u (σ2) dσ1dσ2
sufﬁces with
9

h (t 1,t 2) = g (t 1,t 2) + g 1(t 1)δ0(t 1−t 2) + g 0δ0(t 1)δ0(t 2)
(16)
The dangers not withstanding, impulses will be allowed in the kernel to account for
the various direct transmission terms. But as a matter of convention, a kernel is assumed to
be impulse free unless stated otherwise. I should point out that, as indicated by the
degree-2 case, the impulses needed for this purpose occur only for values of the kernel’s
arguments satisfying certain patterns of equalities.
Example 1.4
A simple system for computing the integral-square value of a signal is
shown in Figure 1.6.
 
Figure 1.6. An integral-square computer.
 
This system is described by
y (t) =
−∞∫
∞
δ−1(t −σ)u 2(σ) dσ
so that a standard-form degree-2 homogeneous representation is
y (t) =
−∞∫
∞
−∞∫
∞
δ−1(t −σ1)δ0(σ1−σ2)u (σ1)u (σ2) dσ2dσ1
If the input signal is one-sided, then the representation can be simpliﬁed to
y (t) =
0∫
t
0∫
t
δ0(σ1−σ2)u (σ1)u (σ2) dσ2dσ1
On the other hand, a simple system for computing the square-integral of a signal is shown
in Figure 1.7.
Figure 1.7. A square-integral computer.
 
This system is described by
10
Π
δ–1(t)
u
y
Π
δ–1(t)
u
y

y (t) = [
−∞∫
∞
δ−1(t −σ)u (σ) dσ]2
=
−∞∫
∞
−∞∫
∞
δ−1(t −σ1)δ−1(t −σ2)u (σ1)u (σ2) dσ1dσ2
If the input signal is one-sided, then the representation simpliﬁes to
y (t) =
0∫
t
0∫
t
u (σ1)u (σ2) dσ1dσ2
Comparison of these two systems indicates that direct transmission terms (impulsive
kernels) arise from unintegrated input signals in the nonlinear part of the system.
A kernel describing a degree-n homogeneous system will be called separable if it
can be expressed in the form
h (t 1, . . . ,tn) =
i =1Σ
m
v 1i(t 1)v 2i(t 2) . . . vni(tn)
(17)
or
h (t,σ1, . . . ,σn) =
i =1Σ
m
v 0i(t)v 1i(σ1) . . . vni(σn)
(18)
where each vji(.) is a continuous function. It will be called differentiably separable if each
vji(.) is differentiable. Almost all of the kernels of interest herein will be differentiably
separable. Although explicit use of this terminology will not occur until much later, it will
become clear from examples and problems that separability is a routinely occurring
property of kernels.
The reader probably has noticed from the examples that more than one kernel can be
used to describe a given system. For instance, the kernel derived in Example 1.1 can be
rewritten in several ways simply by reordering the variables of integration. This feature
not only is disconcerting at ﬁrst glance, it also leads to serious difﬁculties when system
properties are described in terms of properties of the kernel. Therefore, it becomes
important in many situations to impose uniqueness by working with special, restricted
forms for the kernel. Three such special forms will be used in the sequel: the symmetric
kernel, the triangular kernel, and the regular kernel. I now turn to the introduction of these
forms.
A symmetric kernel in the stationary case satisﬁes
hsym(t 1, . . . ,tn) = hsym(t π(1), . . . ,t π(n))
(19)
or, in the nonstationary case,
hsym(t,σ1, . . . ,σn) = hsym(t,σπ(1), . . . ,σπ(n))
(20)
where π(.) denotes any permutation of the integers 1, . . . ,n. It is easy to show that without
loss of generality the kernel of a homogeneous system can be assumed to be symmetric. In
11

fact any given kernel, say h (t 1, . . . ,tn) in (6), can be replaced by a symmetric kernel
simply by setting
hsym(t 1, . . . ,tn) = n !
1
___
π(.)Σ h (t π(1), . . . ,t π(n))
(21)
where the indicated summation is over all n ! permutations of the integers 1 through n. To
see that this replacement does not affect the input/output relation, consider the expression
−∞∫
∞
hsym(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn =
n !
1
___
π(.)Σ
−∞∫
∞
h (σπ(1), . . . ,σπ(n))u (t −σπ(1)) . . . u (t−σπ(n)) dσπ(1) . . . dσπ(n)
(22)
Introducing the change of variables (actually, just a relabeling) τi = σπ(i), i = 1, . . . ,n, in
every term of the summation in (22) shows that all terms are identical. Thus summing the
n ! identical terms on the right side shows that the two kernels yield the same input/output
behavior.
Often a kernel of interest is partially symmetric in the sense that not all terms of the
summation in (21) are distinct. In this situation the symmetric version of the kernel can be
obtained by summing over those permutations that give distinct summands, and replacing
the n ! by the number of such permutations. A signiﬁcant reduction in the number of terms
is often the result.
Example 1.5
Consider a degree-3 kernel that has the form
h (t 1,t 2,t 3) = g (t 1)g (t 2)g (t 3)f (t 1+t 2)
Incidently, note that this is not a separable kernel unless f (t 1+t 2) can be written as a sum
of terms of the form f 1(t 1)f 2(t 2). To symmetrize this kernel, (21) indicates that six terms
must be added. However, the ﬁrst three factors in this particular case are symmetric, and
there are only three permutations that will yield distinct forms of the last factor; namely
f (t 1+t 2), f (t 1+t 3), and f (t 2+t 3). Thus, the symmetric form of the given kernel is
hsym(t 1,t 2,t 3) = 3
1__g (t 1)g (t 2)g (t 3)[f (t 1+t 2)+f (t 1+t 3)+f (t 2+t 3)]
Again I emphasize that although the symmetric version of a kernel usually contains
more terms than an asymmetric version, it does offer a standard form for the kernel. In
many cases system properties can be related more simply to properties of the symmetric
kernel than to properties of an asymmetric kernel.
The second special form of interest is the triangular kernel. The kernel in (8),
h (t,σ1, . . . ,σn), is triangular if it satisﬁes the additional property that h (t,σ1, . . . ,σn) = 0
when σi +j > σj for i,j positive integers. A triangular kernel will be indicated by the
subscript "tri" when convenient. For such a kernel the representation (8) can be written in
the form
12

y (t) =
−∞∫
t
−∞∫
σ1
. . .
−∞∫
σn −1
htri(t,σ1, . . . ,σn)u (σ1) . . . u (σn) dσn . . . dσ1
(23)
Sometimes this special form of the input/output relation will be maintained for triangular
kernels, but often I will raise all the upper limits to ∞or t and leave triangularity
understood. On some occasions the triangularity of the kernel will be emphasized by
appending unit step functions. In this manner (23) becomes
y (t) =
−∞∫
∞
htri(t,σ1, . . . ,σn)δ−1(σ1−σ2)δ−1(σ2−σ3)
. . . δ−1(σn −1−σn)u (σ1) . . . u (σn) dσn . . . dσ1
(24)
Notice that there is no need to use precisely this deﬁnition of triangularity. For
example,
if
htri(t,σ1, . . . ,σn) = 0
when
σj > σi +j,
then
the
suitable
triangular
representation is
y (t) =
−∞∫
t
−∞∫
σn −1
. . .
−∞∫
σ2
htri(t,σ1, . . . ,σn)u (σ1) . . . u (σn) dσ1 . . . dσn
(25)
Stated another way, a triangular kernel
htri(t,σ1, . . . ,σn) = htri(t,σ1, . . . ,σn)δ−1(σ1−σ2) . . . δ−1(σn −1−σn)
(26)
remains triangular for any permutation of the arguments σ1, . . . ,σn. A permutation of
arguments simply requires that the integration be performed over the appropriate triangular
domain, and this domain can be made clear by the appended step functions. However, I
will stick to the ordering of variables indicated in (23) and (26) most of the time.
Now assume that the triangular kernel in (26) in fact is stationary . Then let
gtri(σ1, . . . ,σn) = htri(0,−σ1, . . . ,−σn)δ−1(σ2−σ1) . . . δ−1(σn−σn −1)
(27)
so that
gtri(t −σ1, . . . ,t −σn) = htri(t,σ1, . . . ,σn)δ−1(σ1−σ2) . . . δ−1(σn −1−σn)
(28)
and the input/output relation in (23) becomes
y (t) =
−∞∫
t
−∞∫
σ1
. . .
−∞∫
σn −1
gtri(t −σ1, . . . ,t −σn)u (σ1) . . . u (σn) dσn . . . dσ1
(29)
Or, performing the usual variable change,
y (t) =
−∞∫
t
−∞∫
σn
. . .
−∞∫
σ2
gtri(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(30)
an expression that emphasizes that in (27) triangularity implies gtri(t 1, . . . ,tn) = 0 if
ti > ti +j. But, again, this is not the only choice of triangular domain. In fact, for a degree-n
13

kernel there are n !
choices for the triangular domain, corresponding to the n !
permutations of variables in the inequality t π(1) ≥ t π(2) ≥  . . .  ≥ t π(n) ≥ 0. So there is
ﬂexibility here: pick the domain you like, or like the domain you pick.
To present examples of triangular kernels, I need only review some of the earlier
examples. Notice that the nonstationary kernel obtained in Example 1.3 actually is in the
triangular form (24). Also the input/output representation obtained in Example 1.2 can be
written in the form
y (t) =
0∫
t
0∫
σ1
u (σ1)u (σ2) dσ2dσ1
This corresponds to the triangular kernel htri(t,σ1,σ2) =  δ−1(σ1−σ2) in (24), or to the
triangular kernel gtri(t 1,t 2)  = δ−1(t 2−t 1) in (29).
The relationship between symmetric and triangular kernels should clarify the
features of both. Assume for the moment that only impulse-free inputs are allowed. To
symmetrize a triangular kernel it is clear that the procedure of summing over all
permutations of the indices applies. However, in this case the summation is merely a
patching process since no two of the terms in the sum will be nonzero at the same point,
except along lines of equal arguments such as σi = σj, σi = σj = σk, and so on. And since
the integrations are not affected by changes in integrand values along a line, this aspect
can be ignored. On the other hand, for the symmetric kernel hsym(t,σ1, . . . ,σn) I can write
the input/output relation as a sum of n ! n-fold integrations over the n ! triangular domains
in the ﬁrst orthant. Since each of these integrations is identical, the triangular form is
given by
htri(t,σ1, . . . ,σn)=n !hsym(t,σ1, . . . ,σn)δ−1(σ1−σ2)δ−1(σ2−σ3) . . . δ−1(σn −1−σn)
(31)
In the stationary case the symmetric kernel hsym(t 1, . . . ,tn) yields the triangular kernel
corresponding to (30) as
gtri(t 1, . . . ,tn) = n !hsym(t 1, . . . ,tn)δ−1(t 2−t 1) . . . δ−1(tn−tn −1)
(32)
Of course, these formulas imply that either of these special forms is (essentially) uniquely
speciﬁed by the other.
Example 1.6
For the stationary, symmetric degree-2 kernel
hsym(t 1,t 2) = e t 1+t 2 e min [t 1,t 2]
a corresponding triangular kernel is
htri(t 1,t 2) = 2e 2t 1+t 2δ−1(t 2−t 1)
It is instructive to recompute the symmetric form. Following (21),
14

hsym(t 1,t 2) = 2
1__[2e 2t 1+t 2δ−1(t 2−t 1) + 2e 2t 2+t 1δ−1(t 1−t 2)]
= e t 1+t 2[e t 1δ−1(t 2−t 1) + e t 2δ−1(t 1−t 2)]
Now this is almost the symmetric kernel I started with. Almost, because for t 1 = t 2 the
original symmetric kernel is e 3t 1, while the symmetrized triangular kernel is 2e 3t 1. This is
precisely the point of my earlier remark. To wit, values of the kernel along equal argument
lines can be changed without changing the input/output representation. In fact they must
be changed to make circular calculations yield consistent answers.
Now consider what happens when impulse inputs are allowed, say u (t) = δ0(t). In
terms of the (nonstationary) symmetric kernel, the response is y (t) = hsym(t, 0, . . . ,0), and
in terms of the triangular kernel, y (t) = htri(t, 0, . . . ,0). Thus, it is clear that in this
situation (31) is not consistent. Of course, the difﬁculty is that when impulse inputs are
allowed, the value of a kernel along lines of equal arguments can affect the input/output
behavior. For a speciﬁc example, reconsider the stationary kernels in Example 1.6 with an
impulse input.
Again, the problem here is that the value of the triangular kernel along equal
argument lines is deﬁned to be equal to the value of the symmetric kernel. This can be
ﬁxed by more careful deﬁnition of the triangular kernel. Speciﬁcally, what must be done is
to adjust the deﬁnition so that the triangular kernel gets precisely its fair share of the value
of the symmetric kernel along equal-argument lines. A rather fancy "step function" can be
deﬁned to do this, but at considerable expense in simplicity. My vote is cast for simplicity,
so impulse inputs henceforth are disallowed in the presence of these issues, and kernel
values along lines will be freely adjusted when necessary. (This luxury is not available in
the discrete-time case discussed in Chapter 6, and a careful deﬁnition of the triangular
kernel which involves a fancy step function is used there.
The reader inclined to
explicitness is invited to transcribe those deﬁnitions to the continuous-time case at hand.)
The third special form for the kernel actually involves a special form for the entire
input/output representation. This new form is most easily based on the triangular kernel.
Intuitively speaking, it shifts the discontinuity of the triangular kernel out of the picture
and yields a smooth kernel over all of the ﬁrst orthant. This so-called regular kernel will
be used only in the stationary system case, and only for one-sided input signals.
Suppose htri(t 1, . . . ,tn) is a triangular kernel that is zero outside of the domain
t 1 ≥ t 2 ≥ . . . ≥ tn ≥ 0. Then the corresponding input/output representation can be written in
the form
y (t) =
−∞∫
∞
htri(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
where the unit step functions are dropped and the inﬁnite limits are retained just to make
the bookkeeping simpler. Now make the variable change from σ1 to τ1 = σ1−σ2. Then
the input/output representation is
15

y (t) =
−∞∫
∞
htri(τ1+σ2,σ2, . . . ,σn)u (t−τ1−σ2)u (t −σ2) . . . u (t −σn) dτ1dσ2 . . . dσn
Now replace σ2 by τ2 = σ2−σ3 to obtain
y (t) =
−∞∫
∞
htri(τ1+τ2+σ3,τ2+σ3,σ3, . . . ,σn)
u (t −τ1−τ2−σ3)u (t −τ2−σ3)u (t−σ4) . . . u (t −σn) dτ1dτ2dσ3 . . . dσn
Continuing this process gives
y (t) =
−∞∫
∞
htri(τ1+ . . . +τn,τ2+ . . . +τn, . . . ,τn −1+τn,τn)
u (t −τ1−. . . −τn)u (t −τ2−. . . −τn) . . . u (t −τn) dτ1 . . . dτn
(In continuing the process, each variable change can be viewed as a change of variable in
one of the iterated integrals. Thus the Jacobian of the overall change of variables is unity,
as is easily veriﬁed. This is a general feature of variable changes in the sequel.) Letting
hreg(t 1, . . . ,tn) = htri(t 1+ . . . +tn,t 2+ . . . +tn, . . . ,tn)
(33)
be the regular kernel, I can write
y (t)=
−∞∫
∞
hreg(τ1, . . . ,τn)u (t −τ1−. . . −τn)u (t −τ2−. . . −τn) . . . u (t −τn)dτ1 . . . dτn
(34)
where hreg(t 1, . . . ,tn) is zero outside of the ﬁrst orthant, t 1, . . . ,tn ≥ 0. As mentioned
above, the usual discontinuities encountered along the lines tj −1 = tj, and so on, in the
triangular kernel occur along the edges tj = 0 of the domain of the regular kernel.
It should be clear from (33) that the triangular kernel corresponding to a given
regular kernel is
htri(t 1, . . . ,tn) = hreg(t 1−t 2,t 2−t 3, . . . ,tn −1−tn,tn)
δ−1(t 1−t 2)δ−1(t 2−t 3) . . . δ−1(tn −1−tn) , t 1, . . . ,tn ≥0
(35)
Thus (33) and (35), in conjunction with the earlier discussion of the relationship between
the triangular and symmetric kernels, show how to obtain the symmetric kernel from the
regular kernel, and vice versa.
I noted earlier that particular forms for the kernel often are natural for particular
system structures. Since the regular kernel is closely tied to the triangular kernel, it is not
surprising that when one is convenient, the other probably is also (restricting attention, of
course, to the case of stationary systems with one-sided inputs). This can be illustrated by
16

reworking Example 1.3 in a slightly different way.
Example 1.7
Using an alternative form of the linear system convolution representation,
the calculations in Example 1.3 proceed as follows. Clearly the system is stationary, and
one-sided input signals are assumed implicitly. First the signal v (t) can be written in the
form
v (t) =
−∞∫
∞
h 3(σ3)u (t −σ3) dσ3 u (t)
Then
w (t) =
−∞∫
∞
h 2(σ2)v (t −σ2) dσ2 u (t)
=
−∞∫
∞
−∞∫
∞
h 2(σ2)h 3(σ3)u (t −σ2−σ3)u (t −σ2) dσ2dσ3 u (t)
and
y (t) =
−∞∫
∞
h 1(σ1)w (t −σ1) dσ1
=
−∞∫
∞
−∞∫
∞
−∞∫
∞
h 1(σ1)h 2(σ2)h 3(σ3)u (t −σ1−σ2−σ3)u (t −σ1−σ2)u (t −σ1) dσ1dσ2dσ3
Now a simple interchange of the integration variables σ1 and σ3 gives
y (t) =
−∞∫
∞
h 3(σ1)h 2(σ2)h 1(σ3)u (t −σ1−σ2−σ3)u (t −σ2−σ3)u (t −σ3) dσ1dσ2dσ3
which is in the regular form (34).
It is worthwhile to run through the triangular and regular forms for a very speciﬁc
case. This will show some of the bookkeeping that so far has been hidden by the often
implicit causality and one-sided input assumptions, and the inﬁnite limits. Also, it will
emphasize the special starting point for the derivation of the regular kernel representation.
Example 1.8
A triangular kernel representation for the input/output behavior of the
bilinear state equation in Example 1.2 has been found to be
y (t) =
0∫
t
0∫
σ1
u (σ1)u (σ2) dσ2dσ1
Explicitly incorporating the one-sidedness of the input signal, causality, and triangularity
into the kernel permits rewriting this in the form
17

y (t) =
−∞∫
∞
−∞∫
∞
δ−1(t −σ1)δ−1(t −σ2)δ−1(σ1−σ2)δ−1(σ1)δ−1(σ2)u (σ1)u (σ2) dσ2dσ1
This expression can be simpliﬁed by removing the redundant step functions. Then replace
the variables of integration σ1 and σ2 by t −σ1 and t −σ2, respectively, to obtain
y (t) =
−∞∫
∞
−∞∫
∞
δ−1(σ1)δ−1(σ2−σ1)δ−1(t −σ1)δ−1(t −σ2)u (t −σ1)u (t −σ2) dσ2dσ1
Now the kernel clearly
is triangular, and nonzero on the
domain σ2 ≥ σ1 ≥ 0.
Interchanging the two integration variables gives an input/output expression in terms of a
triangular kernel with domain σ1 ≥ σ2 ≥ 0:
y (t) =
−∞∫
∞
−∞∫
∞
δ−1(σ2)δ−1(σ1−σ2)δ−1(t −σ1)δ−1(t −σ2)u (t −σ1)u (t −σ2) dσ2dσ1
This is the starting point for computing the regular kernel representation. Replace σ1 with
τ1 = σ1−σ2, and then σ2 with τ2 to obtain
y (t) =
−∞∫
∞
−∞∫
∞
δ−1(τ1)δ−1(τ2)δ−1(t −τ1−τ2)δ−1(t −τ2)u (t −τ1−τ2)u (t −τ2) dτ2dτ1
This input/output representation is in regular form, and if the one-sidedness of the input is
left understood, the regular kernel is
hreg(t 1,t 2) = δ−1(t 1)δ−1(t 2)
Furthermore, putting together this result and a slight variation of Example 1.7 shows that
the bilinear state equation in Example 1.2 can be represented by the interconnection of
integrators and multipliers shown in Figure 1.8.
Figure 1.8. Interconnection representation for Example 1.2.
 
Incidently, it is obvious that the triangular, symmetric, and regular forms all collapse
to the same thing for homogeneous systems of degree 1. Therefore, when compared to
linear system problems, it should be expected that a little more foresight and artistic
judgement are needed to pose nonlinear systems problems in a convenient way. This is
less an inherited ability than a matter of experience, and by the time you reach the back
cover such judgements will be second-nature.
18
y
u
δ–1(t)
δ–1(t)
Π

1.3 Polynomial and Volterra Systems
A system described by a ﬁnite sum of homogeneous terms of the form
y (t) =
n =1Σ
N
−∞∫
∞
hn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(36)
will be called a polynomial system of degree N, assuming hN(t 1, . . . ,tN) ≠ 0. If a system is
described by an inﬁnite sum of homogeneous terms, then it will be called a Volterra
system.
Of course, the same terminology is used if the homogeneous terms are
nonstationary. By adding a degree-0 term, say y 0(t), systems that have nonzero responses
to identically zero inputs can be represented.
Note that, as special cases, static nonlinear systems described by a polynomial or
power series in the input:
y (t) = a 1u (t) + . . . + aNu N(t)
y (t) =
n =1Σ
∞
anu n(t)
(37)
are included. Simply take hn(t 1, . . . ,tn)  = anδ0(t 1) . . . δ0(tn) in (36). Further examples of
polynomial systems are easy to generate from interconnection structured systems. The
simplest case is a cascade connection of a linear system followed by a polynomial
nonlinearity. If the nonlinearity is described by an inﬁnite power series, a Volterra system
is the result.
Since the Volterra system representation is an inﬁnite series, there must be
associated convergence conditions to guarantee that the representation is meaningful.
Usually these conditions involve a bound on the time interval and a bound for u (t) on this
interval. These bounds typically depend upon each other in a roughly inverse way. That
is, as the time interval is made larger, the input bound must be made smaller, and vice
versa. The calculations required to ﬁnd suitable bounds often are difﬁcult.
Example 1.9
The following is possibly the simplest type of convergence argument for a
Volterra system of the form (36) with N = ∞. Suppose that for all t
|u (t)| ≤K
and
−∞∫
∞
|hn(σ1, . . . ,σn)| dσ1 . . . dσn ≤an
Then since the absolute value of a sum is bounded by the sum of the absolute values,
19

|y (t)| ≤
n =1Σ
∞

−∞∫
∞
hn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn 
≤
n =1Σ
∞
−∞∫
∞
|hn(σ1, . . . ,σn)| |u (t −σ1)| . . . |u (t −σn)| dσ1 . . . dσn
≤
n =1Σ
∞
anK n
Convergence of the power series on the right side implies convergence of the series
deﬁning the Volterra system. In this case the time interval is inﬁnite, but of course the
convergence condition is quite restrictive.
In the sequel I will be concerned for the most part with polynomial- or
homogeneous-system representations, thereby leaping over convergence in a single bound.
Of course, convergence is a background issue in that a polynomial system that is a
truncation of a Volterra system may be a good approximation only if the Volterra system
representation converges. When Volterra systems are considered, the inﬁnite series will be
treated informally in that the convergence question will be ignored. All this is not to slight
the importance of the issue. Indeed, convergence is crucial when the Volterra series
representation is to be used for computation. The view adopted here is more a
consequence of the fact that convergence properties often must be established using
particular features of the problem at hand. A simple example will illustrate the point.
Example 1.10
Consider the Volterra system
y (t) =
0∫
t
cbu (σ) dσ +
0∫
t
0∫
σ1
cDbu (σ1)u (σ2) dσ2dσ1
+
0∫
t
0∫
σ1
0∫
σ2
cD 2bu (σ1)u (σ2)u (σ3) dσ3dσ2dσ1 + . . .
where c, b, and D are 1 x n, n x 1, and n x n matrices, respectively. Factoring out the c and
b, and using a simple identity to rewrite the triangular integrations gives
y (t) = c[
0∫
t
u (σ)dσ + 2!
1
___D[
0∫
t
u (σ)dσ]2 + 3!
1
___D2[
0∫
t
u (σ)dσ]3 + . . . ]b
Now arguments similar to those used to investigate convergence of the matrix exponential
can be applied. The result is that this Volterra system converges uniformly on any ﬁnite
time interval as long as the input is piecewise continuous - a much, in fact inﬁnitely, better
result than would be obtained using the approach in Example 1.9. Incidentally, this
Volterra system representation corresponds to the bilinear state equation
x.(t) = Dx (t)u (t) + bu (t)
y (t) = cx (t)
a particular case of which was discussed in Example 1.2. I suggest that the reader discover
20

this by differentiating the vector Volterra system representation for x (t):
x (t) =
0∫
t
bu (σ) dσ +
0∫
t
0∫
σ1
Dbu (σ1)u (σ2) dσ2dσ1 + . . .
1.4 Interconnections of Nonlinear Systems
Three basic interconnections of nonlinear systems will be considered: additive and
multiplicative parallel connections, and the cascade connection. Of course, additive
parallel and cascade connections are familiar from linear system theory, since linearity is
preserved. The multiplicative parallel connection probably is unfamiliar, but it should
seem to be a natural thing to do in a nonlinear context. The results will be described in
terms of stationary systems. I leave to the reader the light task of showing that little is
changed when the nonstationary case is considered.
Interconnections of homogeneous systems will be discussed ﬁrst. No special form
assumptions are made for the kernels because the triangular or symmetric forms are not
preserved under all the interconnections. Furthermore, the regular kernel representation
will be ignored for the moment. To describe interconnections of polynomial or Volterra
systems, a general operator notation will be introduced later in the section. This operator
notation always can be converted back to the usual kernel expressions, but often much ink
is saved by postponing this conversion as long as possible.
 
The basic additive connection of two homogeneous systems is shown in Figure 1.9.
The overall system is described by
y (t) =
−∞∫
∞
hn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
+
−∞∫
∞
gm(σ1, . . . ,σm)u (t −σ1) . . . u (t −σm) dσ1 . . . dσm
(38)
When m = n it is clear that this is a degree-n homogeneous system with kernel
fn(t 1, . . . ,tn) = hn(t 1, . . . ,tn) + gn(t 1, . . . ,tn)
(39)
And if both kernels hn(t 1, . . . ,tn) and gn(t 1, . . . ,tn) are symmetric (triangular), then the
kernel fn(t 1, . . . ,tn) will be symmetric (triangular). When m ≠ n the overall system is a
21
u
y
hn(t1,…,tn)
gm(t1,…,tm)
Σ
Figure 1.9. An additive parallel connection.

polynomial system of degree N = max[n, m ].
The second connection of interest is the parallel multiplicative connection shown in
Figure 1.10. The mathematical description of the overall system is
y (t) = [
−∞∫
∞
hn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn]
[
−∞∫
∞
gm(σ1, . . . ,σm)u (t −σ1) . . . u (t −σm) dσ1 . . . dσm]
=
−∞∫
∞
[hn(σ1, . . . ,σn)gm(σn +1, . . . ,σn +m)]u (t −σ1)
. . . u (t −σn +m) dσ1 . . . dσn +m
(40)
Thus the multiplicative connection yields a homogeneous system of degree n + m with
kernel
fn +m(t 1, . . . ,tn +m) = hn(t 1, . . . ,tn)gm(tn +1, . . . ,tn +m)
(41)
 
In general, neither symmetry nor triangularity is preserved in this case.
Note the
relabeling of variables in this development for it is quite common. Distinct labels for the
variables must be maintained to avoid confusion and preserve sanity.
The cascade connection of two systems is shown in Figure 1.11. The customary,
though usually unstated, assumption is made that the two systems do not interact with each
other. That is, there is no loading effect. To obtain a description for this connection, write
y (t) =
−∞∫
∞
gm(σ1, . . . ,σm)v (t −σ1) . . . v (t −σm) dσ1 . . . dσm
(42)
where, for j = 1, . . . ,m,
22
u
y
hn(t1,…,tn)
gm(t1,…,tm)
Π
Figure 1.10. A multiplicative parallel connection.

v (t−σj) =
−∞∫
∞
hn(σm+(j −1)n +1, . . . ,σm+jn)u (t −σj−σm+(j −1)n +1)
. . . u (t −σj−σm+jn) dσm+(j −1)n +1 . . . dσm+jn
(43)
Figure 1.11. Cascade connection of two systems.
 
Of course, I have chosen the labeling of variables in (43) to make the end result look nice.
Substituting (43) into (42) gives
y (t) =
−∞∫
∞
gm(σ1, . . . ,σm)[
−∞∫
∞
hn(σm+1, . . . ,σm+n)u (t −σ1−σm+1)
. . . u (t −σ1−σm+n) dσm+1 . . . dσm+n]
. . . [
−∞∫
∞
hn(σm+(m−1)n +1, . . . ,σm+mn)u (t −σm−σm+(m−1)n +1)
. . . u (t −σm−σm+mn) dσm+(m−1)n +1 . . . dσm+mn] dσ1 . . . dσm
(44)
Now, in the bracketed terms replace each variable of integration σm+(j −1)n +i by the
variable τ(j −1)n +i = σm+(j −1)n +i + σj, i = 1, . . . ,n, j = 1, . . . ,m. Then moving the outer
m-fold integration to the inside gives
y (t) =
−∞∫
∞
[
−∞∫
∞
gm(σ1, . . . ,σm)hn(τ1−σ1, . . . ,τn−σ1)
. . . hn(τ(m−1)n +1−σm, . . . ,τmn−σm) dσ1 . . . dσm ]u (t −τ1)
. . . u (t −τmn) dτ1 . . . dτmn
(45)
Thus, the cascade connection yields a homogeneous system of dgree mn with kernel
fmn(t 1, . . . ,tmn) =
−∞∫
∞
gm(σ1, . . . ,σm)hn(t 1−σ1, . . . ,tn−σ1)
. . . hn(t (m−1)n +1−σm, . . . ,tmn−σm) dσ1 . . . dσm
(46)
23
u
hn(t1,…,tn)
gm(t1,…,tm)
y
v

It almost is needless to say that symmetry or triangularity usually is lost in this connection.
This means that fmn(t 1, . . . ,tmn) must be symmetrized or triangularized as a separate
operation.
I should pause at this point to comment that double-subscripted integration variables
can be used in derivations such as the above. However, it is usually more convenient in the
long run to work with single-subscripted variables, and the results look better.
When applying the cascade-connection formula, and
other
convolution-like
formulas to speciﬁc systems, some caution must be exercised in order to account properly
for causality. The use of inﬁnite limits and implicit causality assumptions is an invitation
to disaster for the careless. I invite the reader to work the following example in a cavalier
manner just to see what can happen.
Example 1.11    Consider the cascade connection shown in Figure 1.11 with
h 1(t 1) = e −t 1 , g 2(t 1,t 2) = δ0(t 1−t 2)
The kernels can be rewritten in the form
h 1(t 1) = e −t 1δ−1(t 1) , g 2(t 1,t 2) = δ0(t 1−t 2)δ−1(t 1)δ−1(t 2)
to incorporate explicitly the causality conditions. Then for t 1,t 2 ≥ 0, (46) gives the kernel
of the overall system as
f(t 1t 2) =
−∞∫
∞
−∞∫
∞
δ0(σ1−σ2)δ−1(σ1)δ−1(σ2) e −(t 1−σ1)
δ−1(t 1−σ1) e −(t 2−σ2)δ−1(t 2−σ2) dσ1dσ2
=
−∞∫
∞
δ−1(σ1)δ−1(σ1)e −(t 1−σ1)δ−1(t 1−σ1)e −(t 2−σ1)δ−1(t 2−σ1) dσ1
This expression can be simpliﬁed by using the integration limits to account for the
constraints imposed by the unit step functions. Then, for t 1, t 2 ≥ 0,
f (t 1,t 2) =
0∫
min[t 1,t 2]
e −(t 1−σ1)e −(t 2−σ1) dσ1
= e −t 1−t 2
0∫
min [t 1,tw]
e 2σ1 dσ1
= 2
1__ e −t 1−t 2[e 2min[t 1,t 2]−1]
Another way to write this result is
f (t 1,t 2) = 2
1__e −t 1−t 2−e −t 1−t 2 , t 1,t 2 ≥0
24

Of course, these same subtleties can arise in linear system theory. They may have
escaped notice in part because only single integrals are involved, and there is little need
for the notational simplicity of inﬁnite limits, and in part because the use of the Laplace
transform takes care of convolution in a neat way. The title of Chapter 2 should be
reassuring in this regard.
When the regular kernel representation is used, the interconnection rules are more
difﬁcult to derive. Of course, the analysis of the additive parallel connection is the
exception. If the two regular representations are of the same degree, then the regular kernel
for the additive connection is simply the sum of the subsystem regular kernels. If they are
not of the same degree, a polynomial system is the result, with the two homogeneous
subsystems given in regular representation. For cascade and multiplicative-parallel
connections, I suggest the following simple but tedious procedure. For each subsystem
compute the triangular kernel from the regular kernel. Then use the rules just derived to
ﬁnd a kernel for the overall system. Finally, symmetrize this kernel and use the result of
Problem 1.15 to compute the corresponding regular kernel.
For interconnections of polynomial or Volterra systems, a general operator notation
will be used to avoid carrying a plethora of kernels, integration variables, and so forth,
through the calculations. At the end of the calculation, the operator notation can be
replaced by the underlying description in terms of subsystem kernels. However, for some
types of problems this last step need not be performed. For example, to determine if two
block diagrams represent the same input/output behavior it simply must be checked that
the two diagrams are described by the same overall operator.
I should note that
convergence issues in the Volterra-system case are discussed in Appendix 1.1, and will be
ignored completely in the following development.
The notation
y (t) = H [u (t)]
(47)
denotes a system H with input u (t) and output y (t). Often the time argument will be
dropped, and (47) will be written simply as
y = H [u ]
(48)
(Though nonstationary systems are being ignored here, notice that for such a system the
time argument probably should be displayed, for example, y (t) = H [t,u (t)].)
It is
convenient to have a special notation for a degree-n homogeneous system, and so a
subscript will be used for this purpose:
y = Hn[u ]
(49)
Then a polynomial system can be written in the form
y = H [u ] =
n =1Σ
N
Hn[u ]
(50)
with a similar notation for Volterra systems. The convenience of this slightly more explicit
notation is that conversion to the kernel notation is most easily accomplished in a
25

homogeneous-term-by-homogeneous-term fashion.
Considering system interconnections at this level of notation is a simple matter, at
least in the beginning. The additive parallel connection of two systems, H and G, gives the
system H + G, which is described by
y = H [u ] + G [u ] = (H + G)[u ]
(51)
As usual, the addition of mathematical operators (systems) is deﬁned via the addition in
the range space (addition of output signals). In a similar manner, the multiplicative parallel
connection of the systems H and G gvies the system HG described by
y = H [u ]G [u ] = (HG)[u ]
(52)
Notice that both of these operations are commutative and associative. That is,
GH = HG , (FG)H = F (GH)
G + H = H + G , (F + G) + H = F + (G + H)
(53)
Furthermore, the multiplication of systems is distributive with respect to addition:
F(G + H) = FG + FH ,
(G + H)F = GF + HF
(54)
In terms of the notation in (50), it is perfectly clear that
H + G = (H1 + G1) + (H2 + G2) + . . .
(55)
Using (54) the multiplication of two systems is described by
HG = (H1 + H2 + . . . )(G1 + G2 + . . . )
= (H1 + H2 + . . . )G1 + (H1 + H2 + . . . )G2 + . . .
= H1G1 + (H2G1 + H1G2) + (H3G1 + H2G2 + H1G3) + . . .
(56)
The terms in (55) and (56) have been grouped according to degree since
degree (Hm + Gm) = m
degree (HmGn) = m + n
(57)
Now it is a simple matter to replace the expressions in (55) and (56) by the corresponding
kernel representations.
So far it has been good clean fun, but the cascade connection is a less easy topic. A
system H followed in cascade by a system G yields the overall system G*H, where the *
notation is deﬁned by
y = G [H [u ]] = (G*H)[u ]
(58)
But a little more technical caution should be exercised at this point. In particular I have not
mentioned the domain and range spaces of the operator representations. For the
26

multiplicative and additive parallel connections, these can be chosen both for convenience
and for ﬁdelity to the actual system setting. However, for the composition of operators in
(58) it must be guaranteed that the range space of H is contained in the domain of G.
Having been duly mentioned, this condition and others like it will be assumed.
The cascade operation is not commutative except in special cases — one being the
case where only degree-1 systems are involved:
G1*H1 = H1*G1
(59)
The cascade operation is distributive with respect to addition and multiplication only in
the particular orderings
(G + H)*F = G*F + H*F
(GH)*F = (G*F)(H*F)
(60)
and in the special case of the alternative ordering:
F1* (G + H) = F1*G+ +F 1*H
(61)
These results can be established easily be resorting to the corresponding kernel
representations, and the rules derived earlier. Also it is obvious from earlier results that
degree (Gn*Hm) = degree (Hm*Gn) = mn
(62)
To consider cascade connections of polynomial or Volterra systems in terms of the
notation in (50) requires further development. Using the notations
w =
n =1Σ
∞
wn =
n =1Σ
∞
Hn[u ]
y =
n =1Σ
∞
yn =
n =1Σ
∞
Gn[w ]
(63)
the objective is to ﬁnd, for the cascade system G*H in Figure 1.11, an operator expression
of the form
y =
n =1Σ
∞
Fn[u ]
(64)
where each homogeneous operator Fn is speciﬁed in terms of the Hn’s and Gn’s. It is
convenient in this regard to consider the input signal αu (t), where α is an arbitrary real
number. Then Hn[αu ] = αnwn, and
w =
n =1Σ
∞
αnwn
(65)
so that
27

y =
m=1
Σ
∞
Gm[
n =1Σ
∞
αnwn]
(66)
The general term of interest is
Gm[
n =1Σ
∞
αnwn]
(67)
and to analyze this further it is necessary to bring in the kernel representation for Gm.
Letting gsym(t 1, . . . ,tm) be the symmetric kernel corresponding to Gm, a simple
computation gives
Gm[
n =1Σ
∞
αnwn] =
n 1=1
Σ
∞
. . .
nm=1
Σ
∞
αn 1+ . . . +nm Gˆ m[(wn 1, . . . ,wnm)]
(68)
where the new operator is deﬁned by
Gˆ m[(wn 1, . . . ,wnm)] =
−∞∫
∞
gsym(σ1, . . . ,σm)wn 1(t−σ1)
. . . wnm(t−σm) dσ1 . . . dσm
(69)
Note
that
Gˆ m[(wn, . . . ,wn)]
is
the
usual
degree-m
operator
Gm[wm],
and
that
Gˆ m[(wn 1, . . . ,wnm)] is symmetric in its arguments by the symmetry of the kernel. These
properties will be used shortly.
Substituting (68) into (66) gives
y =
m=1
Σ
∞
n 1=1
Σ
∞. . .
nm=1
Σ
∞
αn 1+ . . . +nm Gˆ m[(wn 1, . . . ,wnm)]
(70)
Thus, to determine the operators Fn in (64), coefﬁcients of like powers of α in (70) and in
the expression
y =
n =1Σ
∞
Fn[αu ] =
n =1Σ
∞
αnFn[u ]
(71)
must be equated. Then the various terms involving the notation Gˆ n[(wn 1, . . . ,wnm)] must
be broken into their component parts involving the operators Gm and Hn. This is a
complicated process in general, so I will do just the ﬁrst few terms. Equating coefﬁcients of
α in (70) and (71) gives
F1[u ] = G1[w 1] = G1[H1[u ]]
Thus the degree-1 portion of the overall cascade connection is described by the operator
F1 = G1*H1
(72)
Equating coefﬁcients of α2 gives
28

F2[u ] = G1[w 2] + Gˆ 2[(w 1,w 1)]
= G1[w 2] + G2[w 1]
=G1[H2[u ]] + G2[H1[u ]]
Thus,
F2 = G1*H2 + G2*H1
(73)
Kernels for F1 and F2 are easily calculated from kernels for H1, H2, G1, and G2 by using
(72) and (73) in conjunction with (46).
More interesting things begin to happen when F3 is sought. Equating coefﬁcients of
α3 gives
F3[u ] = G1[w 3] + Gˆ 2[(w 1,w 2)] + Gˆ 2[(w 2,w 1)] + Gˆ 3[(w 1,w 1,w 1)]
=G1[w 3] + 2Gˆ 2[(w 1,w 2)] + G3[w 1]
But now a simple calculation involving the kernel representation shows that
G2[w 1+w 2] = G2[w 1] + G2[w 2] + 2Gˆ 2[(w 1,w 2)]
(74)
Thus
F3[u ] = G1[w 3] + G3[w 1] −G2[w 1] + G2[w 2] + G2[w 1+w 2]
(75)
and the degree-3 operator for the overall system is
F3 = G1*H3 + G3*H1 −G2*H1 −G2*H2 + G2* (H1+H2)
(76)
On the face of it, it might not be clear that (76) yields a degree-3 operator. Obviously
degree-2 and degree-4 terms are present, but it happens that these add out in the end. See
Problem 1.16.
Though the way to proceed probably is clear by now, I will do one more just for the
experience. Equating coefﬁcients of α4 in (70) and (71) gives
F4[u ] = G1[w 4] + G2[w 2] + 2Gˆ 2[(w 1,w 3)] + 3Gˆ 3[(w 1,w 1,w 2)] + G4[w 1]
(77)
Using an expression of the form (74), the term 2Gˆ 2[(w 1,w 3)] can be replaced. Also it is a
simple calculation using the kernel representation to show that
3!Gˆ 3[(w 1,w 1,w 2)] = G3[2w 1+w 2] −2G3[w 1+w 2] −6G3[w 1]+G3[w 2]
Thus the degree-4 operator for the overall system is
F4 = G1*H4 + G2*H2 + G4*H1 + G2* (H1+H3) −G2*H1 −G2*H3
+ 2
1__G3*(2H1+H2) −G3* (H1+H2) −3G3*H1 + 2
1__G3*H2
(78)
29

Just as in (76), the use of (78) to compute a kernel for F4 is straightforward for the Gi*Hj
terms, but a bit more complicated for the Gi* (Hj+Hk) terms.
So far in this section the feedback connection has been studiously avoided. The
time-domain analysis of nonlinear feedback systems in term of kernel representations is
quite unenlightening when compared to transform-domain techniques to be discussed later
on. (This is similar to the linear case. Who ever analyzes linear feedback systems in terms
of the impulse response?) However, the situation is far from simple regardless of the
representation used. Even a cursory look at a feedback system from the operator viewpoint
will point up some of the difﬁculties; in fact, it will raise some rather deep issues that the
reader may wish to pursue.
The feedback interconnection of nonlinear systems is diagramed in operator
notation in Figure 1.12.
Figure 1.12. A nonlinear feedback system.
 
The equations describing this system are
y = G [e ]
(79)
e = u −H [y ]
(80)
It is of interest to determine ﬁrst if these equations specify an "error system" operator
e = E [u ]. From (79) and (80) it is clear that such an operator must satisfy the equation
E [u ] = u −H [G [E [u ]]]
(81)
or, in operator form
E = I −H*G*E
(82)
where I is the identity operator, I[u ] = u. Equation (82) can be rewritten in the form
E + H*G*E = (I + H*G)*E = I
(83)
Thus, a sufﬁcient condition for the existence of a solution E is that (I + H*G)−1 exist, in
which case
E = (I + H*G)−1
(84)
(If the inverse does not exist, then it can be shown that (79) and (80) do not have a solution
for e, or that there are multiple solutions for e. Thus the sufﬁcient condition is necessary as
well.)
30
u
y
Σ
H
G
e
–

Of coure, the reason the operator E is of interest is that (79) then gives an overall
(closed-loop) operator representation of the form y = F [u ] for the system according to
y = G [e ] = G [E [u ]]
Thus
F = G*E = G* (I + H*G)−1
(85)
an expression that should have a familiar appearance. It should be noted that for these
developments to be of system-theoretic interest, the indicated inverse must not only exist,
it must also represent a causal system.
To complete the discussion, it rremains to give methods for computing the
homogeneous terms and corresponding kernels for F. The general approach is to combine
the ﬁrst equality in (85) with (82) to write
F = G* (I −H*F)
(86)
But from this point on, a simple example might be more enlightening than a general
calculation.
Example 1.12
Consider the feedback system in Figure 1.12 where
G = G1 + G3 , H = H1
In this case (86) becomes
F = (G1 + G3)* (I −H1*F)
Writing F as a sum of homogeneous operators and using distributive properties gives the
equation
F1 + F2 + F3 + . . . = G1* (I −H1*F 1 −H2*F 2 −. . . )
+ G3*(I −H1*F 1 −H1*F 2 −H1*F 3 −. . . )
To ﬁnd an equation for F1, the degree-1 terms on both sides are equated:
F1 = G1 −G1*H1*F 1
Thus
(I + G1*H1)*F 1 = G1
and, granting the existence of the operator inverse, F1 is given by
F1 = (I + G1*H1)−1*G1
The terms in this development can be rearranged using the commutativity properties of
degree-1 operators to give the equivalent expression
31

F1 = G1*(I + G1*H1)−1
But either way, the result is of interest only if (I + G1*H1)−1 represents a causal system.
Now, equating degree-2 terms on both sides of the original equation gives
F2 = −G1*H1*F 2
Since invertibility of I + G1*H1 has been assumed, this equation implies
F2 = 0
Equating the degree-3 terms in the original equation gives
F3 = −G1*H1*F 3 + G3* (I −H1*F 1)
The
reader
might
well
wonder
how
to
conclude
that
the
degree-3
terms
in
G3* (I − H1*F 1 − H1*F 2 −  . . . ) are those indicated, since the tempting distributive law
does not hold.
In fact, the justiﬁcation involves retreating to the time-domain
representations using symmetric kernels, and showing that the omitted terms are of degree
greater than 3. Leaving this veriﬁcation to the reader, the degree-3 terms can be rearranged
to give
(I + G1*H1)*F 3 = G3* (I −H1*F 1)
Solving yields, again with the operator inverse treated casually,
F3 = (I + G1*H1)−1*G3*(I −H1*F 1)
This can be rewritten in different ways using commutativity properties, and of course
substitution can be made for F1 if desired. The higher-degree terms can be calculated in a
similar fashion.
Inspection of the homogeneous terms computed in this example indicates an
interesting feature of the operator inverse in (85). Namely, only a linear operator inverse
is required to compute the homogeneous terms in the closed-loop operator representation.
This is a general feature, as the following brief development will show.
Suppose H is an operator representation for a nonlinear system. Then G(p) is called
a p th-degree postinverse of H if
F = G(p)*H = I + Fp +1 + Fp +2 + . . .
(87)
In other words, G(p) can be viewed as a polynomial truncation of H−1, assuming of course
that H−1 exists. The expression (87) can be used to determine G(p) in a homogeneous-
term-by-homogeneous-term fashion by using the cascade formulas developed earlier. That
is, (87) can be written, through degree 3, in the form
32

G(p)*H = (G1
(p) + G2
(p) + G3
(p) + . . . )* (H1 + H2 + H3+ + . . . )
= (G1
(p)*H1) + (G2
(p)*H1 + G1
(p)*H2)
+ [G3
(p)*H1G1
(p)*H3 −G2
(p)*H1 −G2
(p)*H2 + G2
(p)* (H1 + H2)] + . . .
where the terms have been grouped according to degree. Assuming p ≥ 3, the ﬁrst
condition to be satisﬁed is
G1
(p)*H1 = I
To solve this operator equation, H1 must be invertible, and furthermore H1
−1 must
correspond to a causal system for the result to be of interest in system theory. Often these
restrictions are not satisﬁed, but it is hard to be explicit about conditions in terms of
operator representations. (If H1 can be described in terms of a proper rational transfer
function representation H1(s), the reader is no doubt aware that for causal invertibility it is
necessary and sufﬁcient that H1(∞) ≠ 0.) At any rate, I will assume the restricions are
satisﬁed, and write
G1
(p) = H1
−1
(88)
with the remark that for feedback applications inverses of the form (I + H1)−1 are
required, and invertibility is less of a problem.
The second condition to be satisﬁed by the p th-degree postinverse of H is
G2
(p)*H1 + G1
(p)*H2 = 0
Solving for G2
(p) is a simple matter, giving
G2
(p) = −H1
−1*H2*H1
−1
(89)
Notice that no further assumptions were required to solve for G2
(p). The ﬁnal condition that
will be treated explicitly is
G3
(p)*H1 + G1
(p)*H3 −G2
(p)*H1 −G2
(p)*H2 + G2
(p)* (H1 + H2) = 0
Regarding G1
(p) and G2
(p) as known gives
G3
(p) = [−G1
(p)*H3 + G2
(p)*H1 + G2
(p)*H2 −G2
(p)* (H1 + H2)]*H1
−1
= −G1
(p)*H3*H1
−1 + G2
(p) + G2
(p)*H2*H1
−1 −G2
(p)* (I + H2*H1
−1)
(90)
and, again, the only inverse required is H1
−1. The higher-degree homogeneous terms in a
p th-degree postinverse can be calculated similarly. Furthermore, a p th-degree preinverse
can be deﬁned, and it can be shown that the p th-degree postinverse also acts as a p th-
degree preinverse. This is left to Problem 1.19. If the inverse of an operator exists, and the
Volterra series representation is convergent, then the p th-degree inverses are polynomial
truncations of the inverse that will be accurate for inputs sufﬁciently small.
33

To conclude this discussion, three comments are pertinent to the topic of feedback
connections. The ﬁrst is that even a simple feedback connection yields a Volterra system,
with the complexity of the higher-degree terms increasing at a rapid rate. The second is
that the operator inverses will sink of their own weight unless buoyed by an appropriate
amount of rigor. Finally, efﬁcient methods for computing the kernels corresponding to
operator inverses have not been discussed, and in that sense the development here needs to
be completed. I will return to this problem in Chapter 2, and in the meantime the
references given in Remark 1.4 can be consulted for further discussion.
1.5 Heuristic and Mathematical Aspects
One
justiﬁcation
or,
loosely
speaking,
derivation
of
the
Volterra
series
representation is based on a very intuitive approach to nonlinear system description. It is
natural to view the output y (t) of a nonlinear system at a particular time t as depending (in
a nonlinear way) on all values of the input at times prior to t. That is, y (t) depends on
u (t −σ) for all σ ≥ 0. It is convenient, though not necessary, to regard t as the present
instant, and then restate this as: the present output depends on all past input values. At any
rate, this viewpoint leads to the following idea.
If u (t −σ) for all σ ≥ 0 can be
characterized by a set of quantities u 1(t),u 2(t),..., then the output y (t) can be represented
as a nonlinear function of these quantities,
y (t) = f (u 1(t),u 2(t), . . . )
(91)
The ﬁrst step in pursuing this line of thought is to ﬁnd a characterization for the past
of an input signal. So suppose that t is ﬁxed and the input u (t −σ), 0 ≤ σ < ∞, is an element
of the Hilbert space of square-integrable functions L2(0,∞). That is,
0∫
∞
u 2(t −σ) dσ < ∞
Furthermore, suppose that φ1(σ),φ2(σ), . . . , is an orthonormal basis for this space:
0∫
∞
φi(σ)φj(σ) dσ =
B
C
D 0, i ≠j
1, i = j
Then the value of the input signal at any time in the past can be written in the form
u (t −σ) =
i =1Σ
∞
ui(t)φi(σ)
(92)
where
ui(t) =
0∫
∞
u (t −σ)φi(σ) dσ
(93)
Although t is considered to be ﬁxed, this development yields a characterization of the past
of u (t) in terms of u 1(t),u 2(t), . . . , regardless of t.
34

With this characterization in hand, expand the function f (u 1(t),u 2(t), . . . ) into a
power series so that the output at any time t is
y (t) = a +
i =1Σ
∞
aiui(t) +
i 1=1Σ
∞
i 2=1Σ
∞
ai 1i 2ui 1(t)ui 2(t) + . . .
(94)
(Of course, all the inﬁnite sums are truncated in practice to obtain an approximate
representation.) To see what all this has to do with the Volterra/Wiener representation,
simply substitute (93) into (94) to obtain
y (t) = a +
0∫
∞
i =1Σ
∞
aiφ(σ1)u (t −σ1) dσ1
+
0∫
∞
0∫
∞
i 1=1Σ
∞
i 2=1Σ
∞
ai 1i 2φi 1(σ1)φi 2(σ2)u (t −σ1)u (t −σ2) dσ1dσ2 + . . .
(95)
With the obvious deﬁnition of the kernels in terms of the orthonormal functions φi(σ), this
is precisely the kind of representation that has been discussed.
Though this demonstration is amusing, and somewhat enlightening, it seems
appropriate at least to outline a more rigorous mathematical justiﬁcation of the Volterra
series representation. The development to be reviewed follows the style of the Weierstrass
Theorem and deals with approximation of stationary nonlinear systems by stationary
polynomial systems. It is assumed at the outset that the input signal space U and the
output signal space Y are contained in normed linear function spaces so that a norm .
is available.
Then the input/output behavior of a system is viewed as an operator
F :U → Y, and the object is to ﬁnd a polynomial approximation to F. (The reader
uninterested in mathematics, or unfamiliar with some of the terms just used, can skip
directly to Section 1.6.)
The Weierstrass Theorem states that if f (t) is a continuous, real-valued function on
the closed interval [t 1,t 2], then given any ε > 0 there exists a real polynomial p (t) such
that | f (t) − p (t)| < ε for all t ε [t 1,t 2]. A generalization of this result known as the
Stone-Weierstrass Theorem can be stated as follows. Suppose X is a compact space and Φ
is an algebra of continuous, real-valued functions on X that separates points of X and that
contains the constant functions. Then for any continuous, real-valued function f on X and
any ε > 0 there exists a function p ε Φ such that | f (x) − p (x)| < ε for all x ε X. (The
algebra Φ separates points if for any two distinct elements x 1,x 2 ε X there exists a p ε Φ
such that p (x 1) − p (x 2) ≠ 0.) It is this generalization that leads rather easily to the
representation of interest in the stationary-system case.
The set-up is as follows. One choice for the input space is the set U ⊂ L2(0,T) of
functions satisfying: (a) there exists a constant K > 0 such that for all u (t) ε U,
0∫
T
|u (t)| 2 dt ≤K
(b) for every ε > 0 there exists a δ > 0 such that for all u (t) ε U and all τ < δ,
35

0∫
T
|u (t +τ)−u (t)| 2 dt < ε
It can be shown that this is a compact space (see Remark 1.5). A property that follows
easily using these conditions is that if u (t) ε U, then for any t 1 ε [0,T], u 1(t) ε U, where
u 1(t) =
B
C
D u (t −t 1), t 1 ≤t ≤T
0, 0 ≤t ≤t 1
This property is important, for the Stone-Weierstrass Theorem then gives results when the
output space is C [0,T]; the space of continuous, real-valued functions on [0,T] with the
maximum absolute value norm. To see how this works, suppose that F is a continuous
operator
that
is
stationary
and
causal,
and
F :U → C [0,T].
Suppose
also
that
P :U → C [0,T] is a continuous, stationary, causal operator such that for all u (t) ε U,
|F [u ] −P [u ]| t =T < ε
Then for any u (t) there is a t 1 ε [0,T] such that
F [u ] −P [u ]=
tε[0,T]
max |F [u ] −P [u ]|
= |F [u ] −P [u ]| t =T−t 1 = |F [u 1] −P [u 1]| t =T < ε
Therefore, F and P can be viewed as real-valued functions by looking at F [u ] and P [u ]
only at t = T. But the bounds to be obtained will apply for all t ε [0,T], that is, will apply
for F [u ] and P [u ] as elements of C [0,T].
Now the last step is to choose appropriately the algebra Φ of stationary, causal, and
continuous operators from U into C [0,T].
For this take the algebra generated by
P 1[u ] = 1, and all operators of the form
P2[u ] =
0∫
t
h (σ)u (t −σ) dσ
via repeated addition, scalar multiplication, and multiplication. Stationarity and causality
of the operators in Φ is obvious. It is assumed that each h (t) is such that
0∫
T
|h (t)| 2 dt < ∞
for continuity of the operators. That Φ separates points in U is a technical calculation that
will be omitted. The algebra Φ therefore consists of operators of the form
36

y (t) = P [u (t)] = h 0 +
i =1Σ
N1
0∫
t
h 1,i(σ)u (t −σ) dσ
+
i =1Σ
N2
j =1Σ
N3
0∫
t
0∫
t
h 2,i(σ1)h 3,j(σ2)u (t −σ1)u (t −σ2) dσ1dσ2 + . . .
which, with the obvious kernel deﬁnitions, is the set of stationary, causal, polynomial
systems.
Everything is now in a form suitable for the Stone-Weierstrass Theorem. Thus, if a
system input/output behavior can be represented by a continuous, stationary, causal
operator, F :U → C [0,T], then given any ε > 0 there is a continuous, stationary and causal
operator P such that for all u (t) ε U
|F [u ] −P [u ]| t =T < ε
or,
F [u ] −P [u ]< ε
That is, there is a polynomial system that approximates F to within ε. Furthermore, it is
clear from the construction of Φ that the kernels of the polynomial system will be
separable.
While this is a powerful result, I should point out that the main drawback is in the
restrictive input space U. The compactness requirement rules out many of the more
natural choices of U. For example, the unit ball in L2(0,T), the set of all u (t) such that
u (t) ≤ 1, is not compact. At any rate, further discussion of the mathematical aspects
of the Volterra representation can be found in Appendix 1.2, and the references cited in
Section 1.6.
1.6 Remarks and References
Remark 1.1
The system representations discussed herein were introduced in
mathematics by V. Volterra around the turn of the century in the very beginning of
functional analysis. Volterra used the graphic terminology "function of lines," and the
notation
F |[
a
u (t)
b
]|
(sometimes with the interval [a,b ] left understood) to describe what later came to be called
a functional. He deﬁned the notion of derivatives of a function of lines, and developed
multiple integral representations. Then Volterra extended Taylor’s Theorem to obtain
expressions of the form
37

F |[f (t) + u (t)]| = F |[f (t)]| +
a∫
b
F ´|[f (t),σ1]|u (σ1) dσ1
+
a∫
b
a∫
b
F ´´|[f (t),σ1,σ2]|u (σ1)u (σ2) dσ1dσ2 + . . .
the terms of which were called homogeneous of degree n. An overview of this work can
be found in
V. Volterra, Theory of Functionals and of Integral and Integro-differential Equations,
Dover, New York, 1958.
This is an English translation of a book ﬁrst published in Spanish in 1927. More detailed
accounts can be found in various volumes by Volterra, Frechet, and others in the
Collection of Monographs on the Theory of Functions, E. Borel editor, published by
Gauthier-Villars, Paris. Volterra’s earliest discussion of these ideas apparently resides in
several notes in R. C. Accademia dei Lincei, in 1887.
Remark 1.2
The ﬁrst use of Volterra’s representation in nonlinear system theory occurs
in the work of N. Wiener in the early 1940s. This work, which deals with the response of a
nonlinear system to white noise inputs, will be discussed in Chapter 5. However, the
heuristic justiﬁcation of the Volterra series representation given at the beginning of
Section 1.5 follows Wiener’s viewpoint. Several technical reports from the Research
Laboratory of Electronics (RLE) at the Massachusetts Institute of Technology (MIT)
contain subsequent work on the Volterra functional representation applied to nonlinear
systems. For the material introduced in this chapter, the most appropriate of these reports
are the two listed below (with National Technical Information Service order numbers
shown parenthetically).
M. Brilliant, "Theory of the Analysis of Nonlinear Systems," MIT RLE Technical Report
No. 345, 1958 (AD216-209).
D. George, "Continuous Nonlinear Systems," MIT RLE Technical Report No. 355, 1959
(AD246-281).
Another early report of interest is from Cambridge University, and is reprinted in
J. Barrett, "The Use of Functionals in the Analysis of Nonlinear Physical Systems,"
Journal of Electronics and Control, Vol. 15, pp. 567-615, 1963.
Scattered through the literature in the 1960s are a number of articles that introduce the
Volterra series representation for nonlinear systems. It is safe to say that many of these
articles redevelop material essentially contained in the reports listed above, undoubtedly
because these early reports were not published in the widely available literature. In recent
38

years, two books have appeared dealing with the Volterra/Wiener representation for
nonlinear systems. These are
P. Marmarelis, V. Marmarelis, Analysis of Physiological Systems, Plenum, New York,
1978.
M. Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, John Wiley, New
York, 1980.
The latter book is based on the early MIT work, while the former concentrates on
applications of the Wiener theory in biomedical engineering. Both books contain
introductory material on the Volterra series.
Remark 1.3
The symmetric form for kernels has been used since the beginning of work
in this area. It is very natural from a mathematical point of view, and symmetric
representations were used by Volterra. However, the use of triangular kernels is much
more recent, beginning with the paper
A. Isidori, A. Ruberti, "Realization Theory of Bilinear Systems," in Geometric Methods in
System Theory, D. Mayne and R. Brockett, eds., D. Reidel, Dordrecht, Holland, pp. 83-
130, 1973.
These authors also have used the regular kernel (in an implicit manner) to solve the
realization problem for bilinear state equations, a topic that will be discussed in Chapter 4.
The ﬁrst explicit discussion of the regular kernel representation appears in
G. Mitzel, S. Clancy, W. Rugh, "On Transfer Function Representations for Homogeneous
Nonlinear Systems," IEEE Transactions on Automatic Control, Vol. AC-24, pp. 242-249,
1979.
While the various forms in which kernels and input/output representations can be written
might seem needlessly confusing, each of them has important properties that will be
encountered in due course. I feel that it is important to become familiar with these forms
early in the program, for the right choice of representation can make a particular topic
much easier.
Remark 1.4
Many, if not most, of the early reports and papers on Volterra series
methods discuss the interconnection of systems. See for example the reports by George
and Brilliant mentioned in Remark 1.1. Discussions of operator representations,
interconnection rules, and the feedback connection in particular can be found in
G. Zames, "Functional Analysis Applied to Nonlinear Feedback Systems," IEEE
Transactions on Circuit Theory, Vol. CT-10, pp. 392-404, 1963.
39

G. Zames, "Realizability Conditions for Nonlinear Feedback Systems," IEEE Transactions
on Circuit Theory, Vol. CT-11, pp. 186-194, 1964.
and in
J. Willems, The Analysis of Feedback Systems, MIT Press, Cambridge, Massachusetts,
1971.
C. Desoer, M. Vidyasagar, Feedback Systems: Input/Output Properties, Academic Press,
New York, 1975.
A recent review of the difﬁcult and subtle issues surrounding operator inverses is given in
W. Porter, "An Overview of Polynomic System Theory," Proceedings of the IEEE, Vol. 64,
pp. 18-23, 1976.
The closely related question of inverses for polynomial or Volterra systems is discussed in
M. Schetzen, "Theory of p th-Order Inverses of Nonlinear Systems," IEEE Transactions on
Circuits and Systems, Vol. CAS-23, pp. 285-291, 1976.
A. Halme, J. Orava, "Generalized Polynomial Operators for Nonlinear Systems Analysis,"
IEEE Transactions on Automatic Control, Vol. AC-17, pp. 226-228, 1972.
All these more complicated issues aside, a warning is appropriate for even the simple
problems involving parallel and cascade connections. The experience that everybody
probably has in linear block diagram manipulation tends to encourage an overexuberant
approach to problems involving interconnections of nonlinear systems. This easily can
lead to wrong answers that are seductive in their simplicity. Sober and serious attention to
the speciﬁc commutative and distributive properties of nonlinear interconnections is the
only way to avoid being caught in a compromising position.
Remark 1.5
The Stone-Weierstrass approach to the question of approximate polynomial
system representations dates back at least to the report by Brilliant mentioned in Remark
1.1. The presentation in Section 1.5 follows
P. Gallman, "Representation of Nonlinear Systems via the Stone-Weierstrass Theorem,"
Automatica, Vol. 12, pp. 619-622, 1976.
A proof that the set of inputs U used in that development is a compact space can be found
in
L. Liusternik, V. Sobolev, Elements of Functional Analysis, Unger, New York, 1961.
40

A closely related line of work on Weierstrass approaches begins with the paper
P. Prenter, "A Weierstrass Theorem for Real Separable Hilbert Spaces," Journal of
Approximation Theory, Vol. 3, pp.341-351, 1970.
and acquires the causality results so important for system theory in
W. Porter, T. Clark, "Causality Structure and the Weierstrass Theorem," Journal of
Mathematical Analysis and Applications, Vol. 52, pp.351-363, 1975.
A constructive approach to the Weierstrass-type system approximation theorems is
developed in
W. Porter, "Approximation by Bernstein Systems," Mathematical Systems Theory, Vol. 11,
pp. 259-274, 1978.
The approximating systems are linear systems followed by polynomial nonlinearities, and
therefore this paper can be viewed as a conﬁrmation of the heuristic discussions in Section
1.5 and Problem 1.18.
Another viewpoint toward the approximation properties of
polynomial systems is developed in
W. Root, "On the Modeling of Systems for Identiﬁcation, Part I: ε-Representations of
Classes of Systems," SIAM Journal on Control, Vol. 13, pp. 927-944, 1975.
Finally, references on approximation by systems describable by bilinear state equations
are given in Remark 4.6 of Section 4.6.
Remark 1.6
A somewhat different integral representation for nonlinear systems can be
written in the form
y (t) =
−∞∫
∞
h [t −σ,u (σ)] dσ
for stationary systems. Various properties are discussed in
L. Zadeh, "A Contribution to the Theory of Nonlinear Systems," Journal of the Franklin
Institute, Vol. 255, pp. 387-408, 1953.
A. Gersho, "Nonlinear Systems with a Restricted Additivity Property," IEEE Transactions
on Circuit Theory, Vol CT-16, pp.150-154, 1969.
The basic approximation properties for such a representation and the relationship to the
Volterra type representation are presented in the paper by Gallman cited in Remark 1.5.
Sufﬁce it to say here that the core of the matter is another application of the Stone-
41

Weierstrass Theorem.
1.7 Problems
1.1. A system that has the property that the response to αu (t) is αy (t) is a degree-1
homogeneous system. Does it follow that the system is linear? In other words, is the
response to α1u 1(t) + α2u 2(t) given by α1y 1(t) + α2y 2(t)?
1.2. Find two degree-2, homogeneous, interconnection structured systems that have the
same response to the input u (t) = δ0(t), but different responses to the input u (t) = δ−1(t).
1.3. Find a kernel for the system shown below.
 
1.4. Symmetrize the kernel
h (t 1,t 2,t 3) = h 1(t 1+t 2+t 3)h 2(t 1+t 2)h 3(t 1)
1.5. Write the kernels derived in Examples 1.1 and 1.3 in symmetric form.
with symmetric kernel followed by a linear system is "automatically" symmetric.
1.9. Show that the symmetric form of a kernel is separable if and only if the triangular
form is separable (neglecting the unit step functions, of course).
42
h1(t)
u
h2(t)
Π
h3(t)
y
u
y
e
–t  –t
δ–1(t)
1
2
u
y
δ–1(t)
e
–t
(.)2
1.7. Show that the symmetric kernel for the interconnection structured system shown
below is
1.6. Compute an overall kernel for the cascade connection shown below, and compare
your result with Example 1.11.

1.13. Suppose that in Example 1.3 each hi(t) can be written as
hi(t) =
ji=1Σ
ni
ki=1Σ
σi
ajikit ki−1e
λjit, t ≥0
Show that the overall system kernel is separable.
1.14. Consider a Volterra system of the form
y (t) =
n =1Σ
∞
0∫
t
hn(t −σ1, . . . ,t −σn)u (σ1) . . . u (σn) dσn . . . dσ1
where h 1(t) = δ−1(t), and for n > 1,
hn(t 1, . . . ,tn) = anδ0(t 1−t 2)δ0(t 2−t 3) . . . δ0(tn −1−tn)
Give conditions
under which this series converges.
Can
you devise
a simple
interconnection diagram for this system?
1.15. Given the symmetric kernel hsym(t 1, . . . ,tn), show that the regular kernel on the ﬁrst
orthant is given by
hreg(t 1, . . . ,tn) = n !hsym(t 1+ . . . +tn,t 2+ . . . +tn, . . . ,tn)
1.16. Show that
43
1.12. Find kernels for the polynomial system representation of the interconnection shown
below.
h1(t)
u
h2(t1,t2)
Π
g1(t)
y
Σ
g2(t1,t2)
1.10. List the possible "direct transmission" terms for the degree-3 case. Then show how
these can be represented by introducing impulses into the kernel.
1.11. Compute kernels for the two systems shown below.
δ–1(t)
Π
Π
Σ
u
y
y
u
δ0(t) + 2δ–1(t) 
δ–1(t)
Π

G2* (H1 + H2) −G2*H1 −G2*H2
represents a degree-3 homogeneous system.
1.17. Analyze the feedback system diagramed below, and show that awful things happen
for a unit step input.
1.18. Use the heuristic justiﬁcation of the Volterra series representation given in Section
1.5 to substantiate the following claim. Any (suitably smooth) nonlinear system can be
approximated by a linear system followed by a polynomial nonlinearity.
1.19.
Deﬁne a p th-degree preinverse for a nonlinear system and calculate
the
homogeneous terms through degree 3. Show that this system also acts as a 3rd-degree
postinverse. Are the degree-3 pre and post inverses identical?
APPENDIX 1.1 Convergence Conditions for Interconnections of
Volterra Systems
When Volterra systems are interconnected in the manners discussed in Section 1.4,
some care should be exercised with regard to convergence issues. An analysis of the
situation is most conveniently performed in terms of the convergence argument in
Example 1.9. Although the various bounds that are obtained are very conservative, the
bounds are less important than the fact of convergence.
Suppose y = G [u ] and y = H [u ] are two stationary Volterra systems, where the
respective kernels are bounded according to
−∞∫
∞
|gn(t 1, . . . ,tn)| dt 1 . . . dtn = gn
−∞∫
∞
|hn(t 1, . . . ,tn)| dt 1 . . . dtn = hn
where gn and hn are ﬁnite nonnegative numbers, and n = 1,2,.... (I might as well be
conservative in the matter of technicalities, and assume that the kernels are continuous for
nonnegative arguments, and that the input and output signals are continuous.) Permitting
degree-0 terms with absolute values g 0 and h 0 to be included in the Volterra systems,
suppose that the power series
bG(x) =
n =0Σ
∞
gnx n , bH(x) =
n =0Σ
∞
hnx n
have positive radii of convergence rG and rH, respectively. Then the Volterra system
44
u
y
Σ
–δ0(t1)δ0(t2)
e
–

representation for y = G [u ] converges if the input satisﬁes
|u (t)| < rG for all t.
Furthermore, if | u (t)| ≤ r < rG for all t, then as discussed in Example 1.9, |y (t)| ≤ bG(r)
for all t. For this reason bG(x) is called the bound function for the system, and the values
of bG(x) for x ≥ 0 are of interest.
From
this
formulation
a
number
of
conclusions
about
convergence
of
interconnections of Volterra systems and bounds on responses follow from elementary
facts about power series. For the Volterra system representation for F = H + G in (55), the
kernels satisfy
0∫
∞
| fn(t 1, . . . ,tn)| dt 1 . . . dtn =
0∫
∞
|hn(t 1, . . . ,tn) + gn(t 1, . . . ,tn)| dt 1 . . . dtn
≤hn + gn
so that the bound function for H + G satisﬁes
bH+G(x) ≤bH(x) + bG(x), x ≥0
and the radius of convergence satisﬁes rH+G ≥ min [rH,rG]. Also, the kernels for the
Volterra system representation for F = HG in (56) satisfy
0∫
∞
| fn(t 1, . . . ,tn)| dt 1 . . . dtn =
0∫
∞

j =0Σ
n
hj(t 1, . . . ,tj)gn −j(tj +1, . . . ,tn)| dt 1 . . . dtn
≤
j =0Σ
n
hjgn −j
Thus,
bHG(x) ≤bH(x)bG(x), x ≥0
and rHG ≥ min [rH,rG]. Stating these results informally: if H and G are Volterra systems
that converge for sufﬁciently small inputs, then H + G and HG are Volterra systems that
converge for sufﬁciently small inputs.
The cascade connection F = G*H is less easy to handle in an explicit manner
because the formula for the kernels of G*H in terms of the kernels for G and H is quite
messy. However, an indirect approach can be used to conclude convergence. Suppose
|u (t)| ≤ r < rH, and r > 0 is such that
n =0Σ
∞
hnr n < rG
Then the Volterra system representation for H converges, and the output signal from H has
a bound that is within the radius of convergence of the system G. Thus the Volterra system
representation for G*H will have a positive radius of convergence, speciﬁcally,
45

rG*H ≥r
Notice that if the degree-0 term h 0 is too large, it can be impossible to conclude
convergence of the Volterra system representation for G*H from this argument. On the
other hand, if h 0 = 0, then the argument insures that the Volterra system representation of
the cascade connection G*H will converge for sufﬁciently small inputs.
For use in the sequel, it is important to consider the bound function for the cascade
system. Assuming that no degree-0 terms are present for simplicity, the development that
leads to (70) in Section 1.4 shows that the degree-k term in the cascade representation is
given by
m=1
Σ
∞
n 1+ . . . +nm=k
n 1=1
Σ
∞. . .
nm=1
Σ
∞
−∞∫
∞
gsym(σ1, . . . ,σm)wn 1(t −σ1) . . . wnm(t −σm) dσ1 . . . dσm
where
wn(t) =
−∞∫
∞
hsym(τ1, . . . ,τn)u (t −τ1) . . . u (t −τn) dτ1 . . . dτn
From this expression the kernel of the degree-k term in the cascade connection satisﬁes
−∞∫
∞
| fk(t 1, . . . ,tk)| dt 1 . . . dtk ≤
m=1
Σ
∞
n 1+ . . . +nm=k
n 1=1
Σ
∞. . .
nm=1
Σ
∞
gmhn 1 . . . hnm
and thus the bound function for the cascade satisﬁes
k =1Σ
∞
fkx k ≤
m=1
Σ
∞
n 1=1
Σ
∞. . .
nm=1
Σ
∞
gmhn 1 . . . hnmx n 1+ . . . +nm
≤
m=1
Σ
∞
gm(
n =1Σ
∞
hnx n), x ≥0
That is,
bG*H(x) ≤bG(bH(x)), x ≥0
When
the
feedback
connection
is
considered,
issues
of
well-posedness,
convergence, and stability intertwine in a nasty way. The following discussion of
convergence for feedback Volterra systems will illustrate the situation, and at least
partially unravel it.
The basic feedback connection is shown in Figure A1.1.
46

Figure A1.1. The basic feedback system.
If the Volterra system K is the cascade connection K = H*G, and if this feedback system is
followed in cascade by G, then the general feedback system shown in Figure 1.12 is
obtained. Since cascade systems have been considered already, I will concentrate on the
basic feedback system alone. From another point of view, the basic feedback system is
much like the "error system" described by (81) with K = H*G.
The ﬁrst assumption to be made is that K represents a Volterra system with no
degree-0 term, and with radius of convergence rK > 0. Writing K = K 1 + (K − K 1) to
exhibit the degree-1 term, the basic feedback system can be redrawn as shown in Figure
A1.2.
 
Figure A1.2. The basic feedback system with separated degree-1 terms.
 
Here the degree-1 term of the closed-loop representation y = F [u ] is given by
F 1 = (I + K 1)−1, and it is assumed that this is a well deﬁned, causal system so that the
procedure discussed in Section 1.4 can be used to compute the higher-degree terms in F.
(Thus, the kinds of problems indicated in Problem 1.17 are avoided.) Furthermore, it is
assumed that the kernel for F1 satisﬁes
−∞∫
∞
| f 1(t)| dt = f 1 < ∞
where f 1 > 0 is assumed (without loss of generality) to avoid trivialities in the sequel.
Notice that since f 1(t) is the kernel for a linear feedback system, this boundedness
assumption involves the stability properties of linear feedback systems in an essential way.
The remainder of the discussion is devoted to computing bounds on the higher-
degree kernels and establishing convergence of the feedback system under the foregoing
assumptions. Using (82) the feedback system in Figure A1.2 is described by the operator
equation
F = F1* [I −(K −K 1)*F ], F1 = (I + K 1)−1
47
u
y
Σ
K
–
u
y
Σ
(I + K1)–1
K – K1
–

Writing out the homogeneous terms for F and (K − K 1) gives
F1 + F2 + F3 + . . . = F1 −F1* (K 2 + K 3 + . . . )*(F1 + F2 + F3 + . . . )
= F1 + Q* (F1 + F2 + F3 + . . . )
where Q = −F1* (K 2 + K 3 +  . . . ) is a cascade Volterra system that contains no degree-0
or degree-1 terms, and that by previous results has a bound function
bQ(x) =
n =2Σ
∞
qnx n
with positive radius of convergence. Now suppose there is a bound function for the
feedback system,
bF(x) =
n =1Σ
∞
fnx n
where f 1 is known and fn ≥ 0, n ≥ 2. Then, using the argument in the cascade case, bF(x)
satisﬁes
bF(x) ≤f 1x + bQ(bF(x))
Writing this equation in power-series form gives
n =1Σ
∞
fnx n ≤f 1x +
n =2Σ
∞
qn(
m=1
Σ
∞
fmx m)n, x ≥0
Thus
n =2Σ
∞
fnx n ≤
n =2Σ
∞
qn(
m=1
Σ
∞
fmx m)n
= q 2(f1
2x 2 + 2f 1f 2x 3 + . . . ) + q 3(f1
3x 3 + . . . ) + . . . , x ≥0
and it is clear from this expression that bounds on the individual coefﬁcients in bF(x) can
be determined sequentially:
f 2 ≤q 2f1
2, f 3 ≤2q 2f 1f 2 + q 3f1
3, . . .
To ascertain the convergence properties of a power series that satisﬁes the
coefﬁcient bounds recursively constructed above, an indirect approach is needed. Suppose
y (x) is a solution of
y = f 1x + bQ(y)
Rearranging the equation, and substituting the power series representation for bQ gives the
convergent power series expression
48

x = f 1
1
___ y −f 1
1
___
n =2Σ
∞
qny n, |y | < rQ
Now the theory of inversion of power series can be applied since the linear term on the
right side has nonzero coefﬁcient. This gives y (x) as a power series in x that has a positive
radius of convergence. Furthermore, a recursive computation similar to that above shows
that the coefﬁcients in the power series for y (x) are given by
y (x) = f 1x + (q 2f1
2)x 2 + (2q 2f 1f 2+q 3f1
3)x 3 + . . .
Therefore bF(x) ≤ y (x) for x ≥ 0, and it follows that bF(x) has a positive radius of
convergence.
To summarize informally, the Volterra system representation for the feedback
system in Figure A1.1 converges for inputs that are sufﬁciently small if the Volterra system
representation for K converges for sufﬁciently small inputs. It should be emphasized again
that this result depends crucially on the existence and boundedness assumptions on the
degree-1 term in the closed-loop system.
APPENDIX 1.2 The Volterra Representation for Functionals
The Volterra series representation originated in the mathematics of functional
analysis. A review of the mathematics therefore is appropriate, and moreover will indicate
the rigorous approach that lies behind the purely symbolic treatment of generalized
functions as impulse functions in the main text. A complete and self-contained exposition
is beyond the scope of an appendix, so I will be more detailed than usual in citing
reference material.
Recalling that a (real) functional is a real-valued function deﬁned on a linear space,
it will be helpful to show ﬁrst how the representation of functionals is connected to the
representation of systems. Of course, in this case the linear space is assumed to be a linear
space of functions, namely, the input signals. At a particular time t, the output value y (t)
of a system can be viewed as depending on the past history of the input signal,
u (t −σ), σ ≥ 0. In other words, the output at a particular time is a real functional of the
input signal. If the system is stationary, then this functional will be the same regardless of
the particular choice of t, and so the system is completely characterized. If the system is
nonstationary, more care is required in the interpretation of the functional representation,
and so that case will be ignored for simplicity.
It is natural to consider real functionals on the space L2(0,∞) of real functions
deﬁned for 0 ≤ t < ∞that satisfy
f2 =
0∫
∞
f 2(σ) dσ < ∞
A functional will be denoted by F :L2(0,∞) → R, and it will be assumed that F is n + 1
times Frechet differentiable at the point of interest in L2(0,∞), here taken to be zero for
convenience. Let F(k)[w 1, . . . ,wk] denote the k th Frechet differential of F at 0 with
49

increments w 1, . . . ,wk, and recall that F(k)[w 1, . . . ,wk] is a symmetric, multilinear (k-
linear), continuous mapping from L2(0,∞) × . . . × L2(0,∞) (k-tuples of functions) into R.
Then Taylor’s formula can be applied to yield the following result. For all φ ε L2(0,∞) in
some neighborhood of 0, 1
F [φ] =
k =1Σ
n
k !
1
___F(k)[φ, . . . ,φ] + Rn[φ]
where Rn[φ] < Mφn +1, and F [0] = 0 is assumed for simplicity. (Alternatively,
stronger hypotheses give the existence of an inﬁnite series that converges in some
neighborhood of 0.)
Since the k th term in this representation is determined by a k-linear functional, the
Volterra representation involves the integral representation of such functionals. I will
concentrate on the ﬁrst two terms in the representation, namely, the integral representation
of linear and bilinear functionals. The remaining terms are handled in a manner similar to
the bilinear case. And before proceeding further, it is worthwhile to point out that some
casually apparent approaches are not sufﬁciently general, or appropriate, for the class of
functionals (systems) of interest.
Beginning with the ﬁrst term in the Taylor expansion, it is tempting to apply the
Riesz Theorem for linear functionals on L2(0,∞) and write
F(1)[φ] =
0∫
∞
ψ(σ)φ(σ) dσ
where ψ ε L2(0,∞) is ﬁxed. Unfortunately, this approach to obtaining an integral
representation excludes certain linear functionals that correspond to systems of interest. As
an example, consider the linear functional
F(1)[φ] = φ(0)
which corresponds to the identity system when φ(σ) is identiﬁed with u (t −σ). In this case
the Riesz Theorem cannot be applied, and moreover in the space L2(0,∞) the value of a
function at a point cannot be discussed.
Another example of an approach that fails to be sufﬁciently general can be
constructed by using a slightly different setting. Let L2(0,T) denote the Hilbert space of
square-integrable functions deﬁned for 0 ≤ t ≤ T. Consider a symmetric bilinear functional
F(2)[φ,ψ]:L2(0,T) → R such as the second term in the Taylor expansion given earlier.
Suppose that for all φ,ψ ε L2(0,T) for which the derivatives φ. and ψ. are also elements of
L2(0,T) the bilinear functional satisﬁes
________________
1
J. Dieudonne, Foundations of Modern Analysis, Academic Press, New York, 1960, Chapter 8.
50

|F(2)[φ.,ψ. ]| ≤Mφψ
|F(2)[φ.,ψ]| ≤Mφψ
Then there exists k (t 1,t 2) ε L2((0,T)X (0,T)) such that  2
F(2)[φ,ψ] =
0∫
T
0∫
T
k (σ1,σ2)φ(σ1)ψ(σ2) dσ1dσ2
Unfortunately, the hypotheses here are too restrictive to allow consideration of a bilinear
functional of the form
F(2)[φ,ψ] =
0∫
T
φ(σ)ψ(σ) dσ
which, upon taking φ(t) = ψ(t), corresponds to a system composed of a squarer followed
by an integrator:
F(2)[φ,φ] =
0∫
T
φ2(σ) dσ
Here u (t −σ) is identiﬁed with φ(σ), and only ﬁnite-length input signals are considered. Of
course, in the main text an integral representation for this kind of system involves an
impulse in the kernel,
F(2)[φ,φ] =
0∫
T
0∫
T
δ(σ1−σ2)φ(σ1)φ(σ2) dσ1dσ2
and a few simple rules are used to manipulate impulses.
From these considerations it is clear that some of the more readily available integral
representation theorems are not appropriate tools for investigating the Volterra series
representation for nonlinear systems, and that the issue of impulses arises in a basic way.
To consider the matter further involves using a more restricted input space than L2. Let K
be the set of all real functions that are inﬁnitely differentiable and that vanish outside some
ﬁnite interval in [0,∞). Then K is a linear space, K ⊂ L2(0,∞), and it can be shown that K
is dense in L2(0,∞). A sequence of functions in K, say φ1(t), φ2(t),  . . . is said to converge
to zero in K if (a) there is some ﬁnite interval such that all of the functions vanish outside
the interval, and (b) the sequence converges uniformly to zero on the interval, and for any
positive integer j the sequence of derivatives φ1
(j)(t), φ2
(j)(t),  . . . converges uniformly to
zero on the interval. A functional F :K → R is called a continuous linear functional on K
if F [
.
] is linear and if for every φ1(t), φ2(t),  . . . converging to zero on K, the sequence
________________
2
I. Gelfand, N. Vilenkin, Generalized Functions, Vol. 4, Academic Press, New York, 1964, pp. 11 - 13.
51

F [φ1], F [φ2],  . . . converges to zero in R.  3 It should be noted that it is a purely technical
exercise to show that a continuous linear functional on L2(0,∞) also is a continuous linear
functional on K. Thus a functional of the form
F [φ] =
0∫
∞
f (σ)φ(σ) dσ
where f ε L2(0,∞), is a continuous linear functional on K. But
F [φ] = φ(0)
also is a continuous linear functional on K, although there is no corresponding way to
write an integral representation.
The established, though very confusing, terminology is to call any continuous linear
functional on K a generalized function or distribution. It is standard practice, however, to
write generalized functions in integral form. Accordingly, the functional F [φ] = φ(0) is
written in the integral form
F [φ] =
0∫
∞
δ(σ)φ(σ) dσ = φ(0)
where δ(t) is the impulse function, though of course such a "function" doesn’t exist, and
the notation is purely symbolic. The fact remains that as long as a simple set of rules is
followed, such a symbolic integral representation is quite convenient. The cited references
show in detail the correspondence between the special rules for manipulating impulses,
and
the
corresponding rigorous interpretation in terms of
generalized
functions
(continuous linear functionals on K).
Now consider bilinear functionals on the space K × K of pairs of inﬁnitely
differentiable functions, each of which vanishes outside some ﬁnite interval. Such a
functional F(2)[φ,ψ] is called continuous in each argument if whenever either φ ε K or
ψ ε K is ﬁxed, F(2)[φ,ψ] is a continuous linear functional on K. It is also necessary to
consider the linear space K 2 of inﬁnitely differentiable functions of two variables, φ(t 1,t 2)
that vanish outside of a ﬁnite region in R × R. The notions of convergence to zero in K 2
and of continuity of a linear functional F(1):K 2 → R are the obvious extensions of the
corresponding notions in K. And just as for generalized functions on K, an integral
representation of the form
F(1)[φ(t 1,t 2)] =
0∫
∞
0∫
∞
k (σ1,σ2)φ(σ1,σ2) dσ1dσ2
________________
3
L. Zadeh, C. Desoer, Linear System Theory, McGraw-Hill, New York, 1963, Appendix A; or I. Gelfand, G.
Shilov, Generalized Functions, Vol. 1, Academic Press, New York, 1964, Chapter 1.
52

is used for generalized functions on K 2, although k (t 1,t 2) is often of a purely symbolic
nature. A celebrated result in the theory of generalized functions now can be stated.  4
The Kernel Theorem
Suppose F(2):K × K → R is a bilinear functional that is
continuous in each of its arguments. Then there exists a generalized function F(1):K 2 → R
such that
F(2)[φ,ψ] = F(1)[φ(t 1)ψ(t 2)]
Of course, this result immediately provides a symbolic integral representation for
bilinear functionals that are continuous in each argument. For ψ(t) = φ(t), the integral
representation becomes
F(2)[φ,φ] =
0∫
∞
0∫
∞
k (σ1,σ2)φ(σ1)φ(σ2) dσ1dσ2
and for φ(σ) = u (t −σ) this takes the form of the degree-2 homogeneous system
representation used in the main text.
The symbolic integral representation for higher-degree homogeneous systems
follows in a similar way using a more general version of the kernel theorem.  5 Finally, it is
a technical matter to extend the symbolic representation to L2(0,∞) using the fact that K is
dense in L2(0,∞), and the fact that the notions of continuity used in the development are
quite strong. Again, it should be emphasized that these symbolic representations are
convenient because they permit writing a large class of homogeneous systems in a
standard integral form.
________________
4
I. Gelfand, N. Vilenkin, Generalized Functions, Vol. 4, Academic Press, New York, 1964, p. 18.
5
Ibid., p. 20.
53

CHAPTER 2
INPUT/OUTPUT REPRESENTATIONS
IN THE TRANSFORM DOMAIN
The generalization of the Laplace transform to functions of several variables yields
a tool of considerable importance in stationary nonlinear system theory. Just as for linear
systems, the Laplace transform of a multivariable kernel is called a transfer function. This
representation is useful both for characterizing system properties, and for describing
system
input/output
behavior.
Furthermore,
many
of
the
rules
for
describing
interconnections of systems can be expressed most neatly in terms of transfer functions. A
basic reason for all these features is that certain multivariable convolutions can be
represented in terms of products of Laplace transforms, much as in the single-variable
case.
Corresponding to each of the special forms for the kernel of a homogeneous system,
there is a special form of the transfer function. Polynomial and Volterra systems can be
described by the collection of transfer functions corresponding to the homogeneous
subsystems. All of these representations will be used extensively in the sequel.
2.1 The Laplace Transform
I begin by reviewing for a moment the deﬁnition of the Laplace transform of a one-
sided, real-valued function f (t):
F (s) = L[f (t)] =
0∫
∞
f (t)e −st dt
(1)
Of course, a comment should be included about the region of convergence of F (s), that is,
the range of values of the complex variable s for which the integral converges. However,
the reader is probably accustomed to treating convergence considerations for the Laplace
transform in an off-handed manner. The reason is that for the typical functions arising in
linear system theory convergence regions always exist.
54

Usually, the functions encountered are exponential forms, in other words, ﬁnite
linear combinations of terms of the form t me λt. (Of course λ may be complex, but the
terms appear in conjugate pairs so the function is real.) It is easily veriﬁed that for
functions of this type the integral in (1) converges for all s in some half-plane of the
complex plane, and the resulting transform is a real-coefﬁcient rational function of s that is
strictly proper (numerator-polynomial degree less than denominator-polynomial degree).
Also, the typical operations on these functions of time (addition, integration, convolution)
yield functions of the same type. One result of these observations is that a strictly
algebraic viewpoint is valid for the Laplace transform in the setting of the limited class of
functions described above. But, for the purposes here, a more relaxed, informal treatment
based on the integral deﬁnition will do. (What is called informal here, a mathematician
would call formal!)
For the inverse Laplace transform, the computation of f (t) from F (s), the familiar
line integration formula is
f (t) = L−1[F (s)] = 2πi
1
____
σ−i∞∫
σ+i∞
F (s)e st ds
(2)
where σ is chosen within the convergence region of F (s). For rational Laplace transforms,
of course, the partial fraction expansion method is used, and the calculations are worry-
free as far as convergence issues are concerned.
Given a function of n variables f (t 1, . . . ,tn) that is one-sided in each variable, the
Laplace transform is deﬁned in a fashion analogous to (1):
F (s 1, . . . ,sn) = L[f (t 1, . . . ,tn)]
=
0∫
∞
f (t 1, . . . ,tn)e −s1t 1 . . . e −sntn dt 1 . . . dtn
(3)
Of course, this deﬁnition also is subject to convergence considerations. However, for
reasons similar to those given above, I will proceed in an informal way.
The (perhaps justiﬁably) nervous reader should investigate the situation further. In
particular, it is easy to show that if f (t 1, . . . ,tn) is a linear combination of terms of the
form
t1
m1e λ1t 1t2
m2e λ2t 2 . . . tn
mne λntn ,
t 1, . . . ,tn ≥0
that is, an exponential form on the ﬁrst orthant, then the integral in (2) can be written as a
sum of products of integrals of the form in (1). This indicates that convergence regions
always exist. Carrying out the integrations shows that Laplace transforms are obtained
that are rational functions in more than one variable (ratios of multivariable polynomials).
Similar investigations for exponential forms on a triangular domain, or for symmetric
exponential forms, lead to similar conclusions, though the convergence regions have more
complicated geometry in general. (See Problem 2.3.)
55

Example 2.1
To compute the Laplace transform of
f (t 1,t 2) = t 1 −t 1e −t 2, t 1,t 2 ≥0
the deﬁnition in (3) gives
F (s 1,s 2) =
0∫
∞
0∫
∞
(t 1−t 1e −t 2)e −s1t 1e −s2t 2 dt 1dt 2
=
0∫
∞
0∫
∞
t 1e −s1t 1e −s2t 2 dt 1dt 2 −
0∫
∞
0∫
∞
t 1e −t 2e −s1t 1e −s2t 2 dt 1dt 2
=
s1
2
1
___
0∫
∞
e −s2t 2 dt 2 −
s1
2
1
___
0∫
∞
e −t 2e −s2t 2 dt 2
=
s1
2s 2(s 2+1)
1
__________
The properties of the multivariable Laplace transform that will be used in the sequel
are rather simple to state, and straightforward to prove. In fact, the proofs of the properties
are left to the reader with the hint that they are quite similar to the corresponding proofs in
the single-variable case. This is not meant to imply that the calculations involved in using
the multivariable transform are as simple as in the single-variable transform. To the
contrary, it is easy to think of examples where the frequency-domain convolution in
Theorem 2.6 is at best exhausting to carry out, and at worst frightening even to
contemplate.
In the following list of theorems, and throughout the sequel, one-sidedness is
assumed, and the capital letter notation is used for transforms.
Theorem 2.1
The Laplace transform operation is linear:
L[f (t 1, . . . ,tn) + g (t 1, . . . ,tn)] = F (s 1, . . . ,sn) + G (s 1, . . . ,sn)
L[αf (t 1, . . . ,tn)] = αF (s 1, . . . ,sn), for scalar α
(4)
Theorem 2.2
If f(t 1, . . . ,tn) can be written as a product of two factors,
f (t 1, . . . ,tn) = h (t 1, . . . ,tk)g (tk +1, . . . ,tn)
(5)
then
F (s 1, . . . ,sn) = H (s 1, . . . ,sk)G (sk +1, . . . ,sn)
(6)
Theorem 2.3
If f (t 1, . . . ,tn) can be written as a convolution of the form
f (t 1, . . . ,tn) =
0∫
∞
h (σ)g (t 1−σ, . . . ,tn−σ) dσ
(7)
56

then
F (s 1, . . . ,sn) = H (s 1+ . . . +sn)G (s 1, . . . ,sn)
(8)
Theorem 2.4
If f (t 1, . . . ,tn) can be written as an n-fold convolution of the form
f (t 1, . . . ,tn) =
0∫
∞
h (t 1−σ1, . . . ,tn−σn)g (σ1, . . . ,σn) dσ1 . . . dσn
(9)
then
F (s 1, . . . ,sn) = H (s 1, . . . ,sn)G (s 1, . . . ,sn)
(10)
Theorem 2.5
If T1, . . . ,Tn are nonnegative constants, then
L[f (t 1−T1, . . . ,tn−Tn)] = F (s 1, . . . ,sn)e −s1T1−. . . −snTn
(11)
Theorem 2.6
If f (t 1, . . . ,tn) is given by the product
f (t 1, . . . ,tn) = h (t 1, . . . ,tn)g (t 1, . . . ,tn)
(12)
then
F (s 1, . . . ,sn) =
(2πi)n
1
______
σ−i∞∫
σ+i∞
H (s 1−w 1, . . . ,sn−wn)G (w 1, . . . ,wn) dw 1 . . . dwn (13)
Example 2.2
For the function
f (t 1,t 2) = e −t 1−2t 2 −e −t 1−3t 2, t 1, t 2 ≥0
the deﬁnition of the Laplace transform in (3) can be applied, or I can write
f (t 1,t 2) = e −t 1(e −2t 2 −e −3t 2)
and apply Theorem 2.2. Choosing the latter approach, results from the single-variable
case imply that
F (s 1,s 2) = s 1+1
1
_____ [ s 2+2
1
_____ −s 2+3
1
_____ ] =
s 1s2
2+s2
2+5s 1s 2+6s 1+5s 2+6
1
_________________________
It is natural to call a rational, multivariable Laplace transform strictly proper if the
degree of the numerator polynomial in sj is less than the denominator-polynomial degree
in sj for each j. The discussion so far might give the impression that in the multivariable
case Laplace transforms that are strictly proper rational functions correspond to
exponential forms. Unfortunately, such a degree of similarity to the single-variable case is
too much for which to hope. I will give two examples to show this. The ﬁrst involves the
unit step function, and the second involves the unit impulse function. Note that the second
57

example indicates that the treatment of generalized functions in the multivariable Laplace
transform follows in a natural way from the single-variable case using the sifting property
of the impulse.
Example 2.3
Consider the function
f (t 1,t 2) = δ−1(t 2−2t 1), t 1, t 2 ≥0
which clearly is discontinuous along 2t 1 = t 2. The corresponding Laplace transform is
F (s 1,s 2) =
0∫
∞
0∫
∞
δ−1(t 2−2t 1)e −s1t 1e −s2t 2 dt 1dt 2
=
0∫
∞
0∫
t 2/2
e −s1t 1e −s2t 2 dt 1dt 2
= s 1
1
___
0∫
∞
e −s2t 2 dt 2 −s 1
1
___
0∫
∞
e −[(1/2)s1+s2]t 2 dt 2
= s 2(s 1+2s 2)
1
__________
Example 2.4
For the impulse function
f (t 1,t 2) = δ0(t 1−t 2)
the Laplace transform can be computed directly from the deﬁnition.
F (s 1,s 2) =
0∫
∞
0∫
∞
δ0(t 1−t 2)e −s1t 1e −s2t 2 dt 1dt 2
=
0∫
∞
[
0∫
∞
δ0(t 1−t 2)e −s1t 1dt 1]e −s2t 2 dt 2
=
0∫
∞
e −s1t 2e −s2t 2 dt 2
= s 1+s 2
1
______
The basic relationship used to determine the one-sided function f (t 1, . . . ,tn)
corresponding to a given F (s 1, . . . ,sn) is a multiple line integration of the form
f (t 1, . . . ,tn) = L−1[F (s 1, . . . ,sn)]
=
(2πi)n
1
______
σ−i∞∫
σ+i∞
F (s 1, . . . ,sn)e s1t 1 . . . e sntn ds 1 . . . dsn
(14)
The value of σ is different for each integral, in general, and must be suitably chosen to
58

avoid convergence difﬁculties. Under appropriate technical hypotheses, the line integrals
can be replaced by Bromwich contour integrals, and the calculus of residues applied.
Often this will be done, but without explicit mention of the technicalities.
Again, for n = 1 this is precisely the inverse-transform formula mentioned just
before the method of partial fraction expansion is discussed in detail. That is to say, for
rational, single-variable Laplace transforms, the line integration need never be performed.
Unfortunately, such a nice alternative inversion procedure for multivariable Laplace
transforms is not available. Naive generalization of partial fraction expansion is doomed
since a multivariable polynomial in general cannot be written as a product of simple
factors, each in a single variable. Thus, except for certain special cases to be discussed
later, the line integrals indicated in (14) must be evaluated. In effect, the inverse transform
must be found one variable at a time. But even a simple example should indicate why (14)
can be a lot more fun to talk about than to use.
Example 2.5
To compute the inverse Laplace transform of
F (s 1,s 2) = s 1s 2(s 1+s 2)
1
___________
write
f (t 1,t 2) =
(2πi)2
1
______
σ−i∞∫
σ+i∞
s 1s 2(s 1+s 2)
1
___________ e s1t 1e s2t 2 ds 1ds 2
= 2πi
1
____
σ−i∞∫
σ+i∞
s 2
1
___ e s2t 2 [ 2πi
1
____
σ−i∞∫
σ+i∞
s 1(s 1+s 2)
1
_________ e s1t 1 ds 1] ds 2
The term in brackets can be regarded as a single-variable inverse Laplace transform in s 1
with s 2 a constant. This gives
f (t 1,t 2) = 2πi
1
____
σ−i∞∫
σ+i∞
s 2
1
___ e s2t 2( s 2
1
___δ−1(t 1) −s 2
1
___ e −s2t 1δ−1(t 1) ) ds 2
= 2πi
1
____
σ−i∞∫
σ+i∞
s2
2
1
___ e s2t 2 ds 2 δ−1(t 1) −2πi
1
____
σ−i∞∫
σ+i∞
s2
2
1
___ e −s2t 1e s2t 2 ds 2 δ−1(t 1)
The ﬁrst term is the single-variable inverse Laplace transform of 1/(s2
2), namely, t 2, while
the second term is similar with a time delay of t 1 units indicated. Thus, inserting step
functions to make the one-sided nature of the function explicit,
f (t 1,t 2) = t 2δ−1(t 1)δ−1(t 2) −(t 2−t 1)δ−1(t 2−t 1)δ−1(t 1)
or, being a bit more clever,
f (t 1,t 2) = min[t 1,t 2] ,
t 1, t 2 ≥0
In fact I could have been clever at the beginning of this example by noting that Theorem
59

2.3 can be applied. Taking
H (s 1+s 2) = s 1+s 2
1
______
G (s 1,s 2) = s 1s 2
1
_____
corresponding to
h (t) = δ−1(t)
g (t 1,t 2) = δ−1(t 1)δ−1(t 2)
gives
f (t 1,t 2) =
0∫
∞
h (σ)g (t 1−σ,t 2−σ) dσ
=
0∫
∞
δ−1(σ)δ−1(t 1−σ)δ−1(t 2−σ) dσ
=
0∫
∞
δ−1(t 1−σ)δ−1(t 2−σ)dσ =
0∫
min [t 1,t 2]
dσ
= min[t 1,t 2]
The reader might reﬂect on just how simple this example is. The main feature is that
the denominator of F (s 1,s 2) is given in terms of a product of simple factors. Thus, the line
integrals are easily evaluated by residue calculations, or partial fraction expansions.
Without this factored form, it generally is impossible to perform the sequence of single-
variable inverse transforms that leads to the multivariable inverse transform. Apparently,
there is no easy way around this dilemma. The good news is that the inverse-transform
operation is required only rarely in the sequel. Even so, the factoring problem will arise in
other contexts, and in the next section I will discuss it further.
2.2 Laplace-Transform Representation of Homogeneous Systems
For a stationary linear system with kernel h (t), the transfer function of the system is
the Laplace transform of h (t):
H (s) =
0∫
∞
h (t)e −st dt
(15)
Restricting attention to one-sided input signals, and using the convolution property of the
Laplace transform, the input/output relation
60

y (t) =
−∞∫
∞
h(σ)u(t−σ) dσ =
0∫
t
h (σ)u (t −σ) dσ
(16)
can be written in the form
Y(s) = H (s)U (s)
(17)
where Y(s) and U (s) are the transforms of y (t) and u (t).
If a system transfer function is known, and the input signal of interest has a simple
transform U (s), then the utility of this representation for computing the corresponding
output signal is clear. Another reason for the importance of the transfer function is that
many system properties can be expressed rather simply as properties of H (s). Also the
transfer function of a "linear" interconnection of linear systems is easily computed from
the
subsystem
transfer
functions.
In
developing
a
transform
representation
for
homogeneous systems, these features of the degree-1 case give a preview of the goals.
A degree-n homogeneous system with one-sided input signals can be represented by
y (t) =
−∞∫
∞
h (σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
=
0∫
t
h (σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(18)
Inspection of the list of properties of the multivariable Laplace transform yields no direct
way to write this in a form similar to (17). Therefore, an indirect approach is adopted by
writing (18) as the pair of equations
yn(t 1, . . . ,tn) =
0∫
t 1
. . .
0∫
tn
h (σ1, . . . ,σn)u (t 1−σ1) . . . u (tn−σn) dσ1 . . . dσn
y (t) = yn(t 1, . . . ,tn) t 1= . . . =tn=t = yn(t, . . . ,t)
(19)
Now, the ﬁrst equation in (19) can be written as a relationship between Laplace transforms
by using Theorems 2.4 and 2.2:
Yn(s 1, . . . ,sn) = H (s 1, . . . ,sn)U(s 1) . . . U (sn)
(20)
I call
H (s 1, . . . ,sn) = L[h (t 1, . . . ,tn)]
(21)
a (multivariable) transfer function of the homogeneous system.
At this point the utility of the multivariable transfer function for response
computation is far from clear. Given H (s 1, . . . ,sn) and U (s), it is easy to compute
Yn(s 1, . . . ,sn). However, the inverse Laplace transform must be computed before y (t) can
be found from the second equation in (19), and often this is not easy.
Before proceeding to a further investigation of response computation, I will discuss
some simple properties of the multivariable transfer function representation with regard to
61

the interconnection of systems. In doing so, the transform-domain system diagram shown
in Figure 2.1 will be used.
Figure 2.1. A degree-n homogeneous system.
 
Perhaps
the
most
obvious
feature
of
the
multivariable
transfer
function
representation involves the additive parallel connection of homogeneous systems of the
same degree. The overall transfer function is the sum of the subsystem transfer functions
by the linearity property of the Laplace transform. On the other hand, the kernel of a
multiplicative parallel connection of homogeneous systems is factorable in the sense of
Theorem 2.2. Therefore, the overall transfer function of this connection also can be
written by inspection of the subsystem transfer functions.
When the cascade connection of two homogeneous systems is considered, the going
gets rougher. There is a reasonably neat way to write the multivariable Laplace transform
of the overall cascade-system kernel derived in Section 1.4, and this is left as an
exercise.We will focus on two special cases that correspond precisely to Theorems 2.3 and
2.4.
Consider the cascade connection of a degree-n homogeneous system followed by a
linear system as shown in Figure 2.2. From the analysis in Section 1.4, a kernel for the
overall system is
fn(t 1, . . . ,tn) =
−∞∫
∞
g 1(σ)hn(t 1−σ, . . . ,tn−σ) dσ
(22)
Thus, from Theorem 2.3, a system transfer function is
Fn(s 1, . . . ,sn) = Hn(s 1, . . . ,sn)G1(s 1+ . . . +sn)
(23)
Figure 2.2. A cascade connection.
 
Shown in Figure 2.3 is a cascade connection of a linear system followed by a
degree-n homogeneous system. From Section 1.4, an overall system kernel is given by
fn(t 1, . . . ,tn) =
−∞∫
∞
gn(σ1, . . . ,σn)h 1(t 1−σ1) . . . h 1(tn−σn) dσ1 . . . dσn
Application of Theorem 2.4 to this expression shows that an overall system transfer
function is
62
u
Hn(s1,…,sn)
y
u
Hn(s1,…,sn)
G1(s)
y

Fn(s 1, . . . ,sn) = H1(s 1) . . . H1(sn)Gn(s 1, . . . ,sn)
(25)
Figure 2.3. A cascade connection.
Example 2.6
Application of these results to the interconnection structured system shown
in Figure 2.4 gives the transfer function
H(s 1,s 2) = H1(s 1)H2(s 2)H3(s 1+s 2)
(In system diagrams such as that in Figure 2.4, I retain the meaning of the Π symbol as a
time-domain multiplication, even though the subsystems are represented in the transform
domain. Also, the notational collision involved in using subscripts to denote different
single-variable transfer functions, rather than the number of variables, will be ignored.)
Example 2.7
Computing an overall transfer function of the system shown in Figure 2.5
is a bit more subtle because it must be remembered that a distinct variable should be
reserved for the unity transfer functions as well as the others. The relationship between
the intermediate signal and the input signal can be written from Example 2.4 as
W (s 1,s 2) = H3(s 1)H2(s 1+s 2)U (s 1)U(s 2)
Then
Y(s 1,s 2,s 3) = H3(s 1)H2(s 1+s 2)H1(s 1+s 2+s 3)U (s 1)U (s 2)U(s 3)
and an overall transfer function is
H (s 1,s 2,s 3) = H1(s 1+s 2+s 3)H2(s 1+s 2)H3(s 1)
63
Gn(s1,…,sn)
y
u
H1(s)
H1(s)
u
H2(s)
H3(s)
y
Π
Figure 2.4. The system discussed in Example 2.6.
Figure 2.5. The system discussed in Example 2.7.
Π
H1(s)
w
H2(s)
Π
H3(s)
u
y

 
Consideration of less elementary interconnections will be postponed until another
tool for working with transfer functions is developed in Section 2.4. For the moment, I will
return brieﬂy to the problem of response computation using the multivariable transfer
function. The point to be made is that, the general formulas notwithstanding, sometimes it
is quite easy to compute the response of a nonlinear system.
Example 2.8
Suppose that for the system shown in Figure 2.4 the subsystem transfer
functions are
H1(s) = s
1__, H2(s) = s +1
1
_____, H3(s) = 1
Then compute the system response to the input signal u (t) = δ−1(t) as follows. First write
Y(s 1,s 2) = H(s 1,s 2)U(s 1)U(s 2) =
s1
2s 2(s 2+1)
1
__________
Taking the inverse Laplace transform, easy in this case, gives
y(t 1,t 2) = t 1(1 −e −t 2)
Thus, the output signal is
y (t) = t(1 −e −t)
Of course, in a simple case such as this it is just as easy to trace the input signal through
the system. The response of the ﬁrst subsystem is y 1(t) = L−1[1/s 2] = t, and the response
of the second is y 2(t) = L−1[1/s (s +1)] = 1 − e −t. Multiplying these together gives y (t).
The transform representation and the rules for describing interconnections and
computing the response to given inputs are valid regardless of any special form that the
kernel might take. That is, it doesn’t matter whether the transform of the symmetric
kernel, triangular kernel, or just any everyday kernel is used (leaving aside the regular
kernel representation). However, the symmetric transfer function and triangular transfer
function, corresponding, respectively, to the symmetric and triangular kernels, will have
particular applications in the sequel. Thus, it is useful to derive relationships between
these special forms.
It is clear from the deﬁnition of the Laplace transform that the symmetric transfer
function is a symmetric function in the variables s 1, . . . ,sn. The triangular transfer
function, on the other hand, possesses no particular feature that by inspection distinguishes
it from any other asymmetric transfer function.
Computing the symmetric transfer function corresponding to a given triangular
transfer function is straightforward, in principle. Choosing the obvious notation, the time-
domain formula
hsym(t 1, . . . ,tn) = n !
1
___
π(.)Σ htri(t π(1), . . . ,t π(n))
(26)
64

gives, by linearity of the Laplace transform,
Hsym(s 1, . . . ,sn) = n !
1
___
π(.)Σ Htri(s π(1), . . . ,s π(n))
(27)
Obtaining Htri(s 1, . . . ,sn) from a given Hsym(s 1, . . . ,sn) is more difﬁcult because
the time-domain formula
htri(t 1, . . . ,tn) = n ! hsym(t 1, . . . ,tn)δ−1(t 1−t 2)δ−1(t 2−t 3) . . . δ−1(tn −1−tn)
(28)
must be expressed in the transform domain. I should pause to emphasize that
the
triangular domain t 1 ≥ t 2 ≥  . . .  ≥ tn ≥ 0 is being used here. This particular choice is
nonessential, but a different choice of triangular domain will give a different triangular
transfer function.
Using Theorem 2.6 and the Laplace transform
L[δ−1(t 1−t 2)δ−1(t 2−t 3) . . . δ−1(tn −1−tn)] =
s 1(s 1+s 2)(s 1+s 2+s 3) . . . (s 1+s 2+ . . . +sn)
1
____________________________________
(29)
gives
Htri(s 1, . . . ,sn) =
(2πi)n
n !
______
σ−i∞∫
σ+i∞
w 1(w 1+w 2) . . . (w 1+ . . . +wn)
Hsym(s 1−w 1, . . . ,sn−wn)
__________________________ dw 1 . . . dwn
(30)
 
Figure 2.6. The system discussed in Example 2.9.
Example 2.9
For the system shown in Figure 2.6, it is clear from the interconnection
rules that
Hsym(s 1,s 2) = s 1s 2(s 1+s 2+1)
1
______________
To compute the triangular transfer function representation, (30) gives
65
(.)2
___
s+1
1
u
y
—s
1

Htri(s 1,s 2) =
(2πi)2
2
______
σ−i∞∫
σ+i∞
σ−i∞∫
σ+i∞
w 1(w 1+w 2)
Hsym(s 1−w 1,s 2−w 2)
__________________ dw 2dw 1
=
(2πi)2
2
______
σ−i∞∫
σ+i∞
σ−i∞∫
σ+i∞
(s 1−w 1)(s 2−w 2)(s 1+s 2−w 1−w 2+1)(w 1)(w 1+w 2)
1
__________________________________________ dw 2 dw 1
It is convenient to rearrange this expression in the following way to apply the residue
calculus:
Htri(s 1,s 2) =
2πi
2
____
σ−i∞∫
σ+i∞
(s 1−w 1)w 1
1
__________
2πi
1
____
σ−i∞∫
σ+i∞
(s 2−w 2)(s 2−w 2+s 1−w 1+1)(w 2+w 1)
1
_______________________________ dw 2 dw 1
Now the inner integral can be evaluated by calculating the residue of the integrand
corresponding to the pole w 2 = −w 1. This residue is
(s 2−w 2)(s 2−w 2+s 1−w 1+1)
1
________________________ w2=−w1 = (s 2+w 1)(s 1+s 2+1)
1
_________________
so that
Htri(s 1,s 2) = s 1+s 2+1
2
_________
2πi
1
____
σ−i∞∫
σ+i∞
(s 1−w 1)w 1(w 1+s 2)
1
_________________ dw 1
To evaluate the integral using the residue calculus, the sum of the residues of the integrand
at the poles w 1 = 0 and w 1 = −s 2 must be calculated. This gives
(s 1−w 1)(w 1+s 2)
1
_______________ w1=0 + (s 1−w 1)w 1
1
__________ w1=−s2 = s 1(s 1+s 2)
1
_________
and thus
Htri(s 1,s 2) = s 1(s 1+s 2)(s 1+s 2+1)
2
__________________
The alert reader probably has noted that these residue calculations depend upon
knowing the denominator polynomial of the integrand in a factored form. This begs again
the question of factoring multivariable polynomials, and the question of just what the term
"factor" means in this context. These involve nontrivial problems in mathematics, and a
general discussion would be at once lengthy, and not very beneﬁcial for the purposes here.
However, there are a few special classes of polynomials that can be handled more or less
effectively, and these classes sufﬁce at least to provide examples of typical calculations.
Thus, I will avoid generalities and concentrate on two of these classes, both of which
involve symmetric polynomials.
First, consider the situation where the polynomial P (s 1, . . . ,sn) is known to have
the form
66

P (s 1, . . . ,sn) = P1(s 1) . . . P1(sn)P2(s 1+ . . . +sn)
where P1(s) and P2(s) are (single-variable) polynomials. Furthermore, for simplicity,
assume that P1(s) and P2(s) each have distinct (multiplicity-one) roots. Clearly these are
severe restrictions on the form of P (s 1, . . . ,sn) when compared with all possible n-
variable polynomials. Some measure of the system-theoretic importance of this class of
polynomials can be obtained by reviewing the examples so far presented.
These
polynomials arise in the symmetric transfer function of a cascade connection of a linear
system followed by an n th-power nonlinearity followed by another linear system.
The nice feature of the special form of P (s 1, . . . ,sn) is that the factoring problem
can be solved by factoring the single-variable polynomial P (s, . . . ,s) = P1
n(s)P2(ns). If
s = α is a root of P (s, . . . ,s) of multiplicity one, then (s 1+ . . . +sn−nα) is a factor of
P2(s 1+ . . . +sn), and thus of P (s 1, . . . ,sn). If s = α is a root of P (s, . . . ,s) of multiplicity
n, then (s −α) is a factor of P1(s), and thus (s 1−α) . . . (sn−α) is a factor of P (s 1, . . . ,sn). If
s = α is a root of P (s, . . . ,s) of multiplicity n +1, then (s 1−α) . . . (sn−α) and
(s 1+ . . . +sn−nα) are factors of P (s 1, . . . ,sn). That these are the only possibilities should
be clear because of the restricted form of the polynomial. A similar procedure can be
developed for the case where P1(s) and P2(s) are permitted to have multiple roots, but this
is left to Section 2.6.
I will also discuss brieﬂy a factoring approach for somewhat more general
symmetric polynomials. However, for the degree-2 case, this form agrees precisely with
that considered above: P (s 1,s 2) = P1(s 1)P1(s 2)P2(s 1+s 2). For various numerical values
of s 2, compute the roots of P (s,s 2). A root s = λ(s 2) that is ﬁxed with respect to different
values of s 2 gives a candidate for a factor (s 1−λ)(s 2−λ) of P (s 1,s 2). This candidate is
readily checked by performing long division. A root s = λ(s 2) for which λ(s 2) + s 2 is
constant for various values of s 2 gives a candidate factor (s 1+s 2−λ) for P (s 1,s 2). Again,
long division can be used to check and remove such a factor. In this way the factors are
found one at a time.
Now consider 3-variable, symmetric polynomials of the general form
P (s 1,s 2,s 3) =
P1(s 1)P1(s 2)P1(s 3)P2(s 1+s 2)P2(s 1+s 3)P2(s 2+s 3)P3(s 1+s 2+s 3)
For various numerical values of s 2 (= s 3), compute the roots of the single-variable
polynomial
P (s,s 2,s 2) = P1(s)P1
2(s 2)P2
2(s +s 2)P2(2s 2)P3(s +2s 2)
A root s = λ(s 2) that is ﬁxed with respect to s 2 gives a candidate factor of the form
(s 1−λ)(s 2−λ)(s 3−λ) for P (s 1,s 2,s 3). A root s = λ(s 2) for which λ(s 2) + s 2 is constant
gives a candidate factor of the form (s 1+s 2−λ)(s 1+s 3−λ)(s 2+s 3−λ). Finally, a root for
which λ(s 2) + 2s 2 is constant gives a candidate factor (s 1+s 2+s 3−λ). In all cases, the
factor candidates can be checked and removed by long division.
67

In the general case, symmetric polynomials of the form
P (s 1, . . . ,sn) =
j =1
Π
n
P1(sj)
πΠ P2(s π(1)+s π(2))
πΠ P3(s π(1)+s π(2)+s π(3)) . . . Pn(s 1+ . . . +sn)
where the products are over all permutations π of the integers 1,...,n, can be attacked using
this procedure. However, the complications rapidly increase, and the attack involves a
great deal of artful guessing and single-variable factoring.
Of course, there are many other forms for symmetric polynomials. Even in the 2-
variable case, factors of the form (s 1s 2+1) or (s 1−s 2)(s 2−s 1) have not been considered.
There are a couple of reasons for ignoring these situations: procedures are unknown, and,
more importantly, for the system theory of interest here such factors do not arise. Further
explanation must be postponed until Chapter 4.
2.3 Response Computation and the Associated Transform
Because of the need to perform the multivariable inverse Laplace transform, the
response computation procedure used in Example 2.8 often is unwieldy. There is an
alternative method that is based on the idea of computing the Laplace transform of the
output signal, Y(s), directly from Yn(s 1, . . . ,sn). Then only a single-variable inverse
Laplace transform is required to ﬁnd y (t). The procedure for computing Y(s) from
Yn(s 1, . . . ,sn) is called association of variables, and Y(s) is called the associated
transform. The notation
Y(s) = An[Yn(s 1, . . . ,sn)]
(31)
is used to denote the association of variables operation.
I begin with the n = 2 case, for then the general case is an easy extension.
Theorem
2.7
For the
Laplace
transform Y2(s 1,s 2), the
associated
transform
Y(s) = A2[Y2(s 1,s 2)] is given by the line integral
Y(s) = 2πi
1
____
σ−i∞∫
σ+i∞
Y2(s −s 2,s 2) ds 2
(32)
Proof
Writing the inverse Laplace transform
y 2(t 1,t 2) =
(2πi)2
1
______
σ−i∞∫
σ+i∞
Y2(s 1,s 2)e s1t 1e s2t 2 ds 1ds 2
and setting t 1 = t 2 = t yields
y (t) = 2πi
1
____
σ−i∞∫
σ+i∞
[ 2πi
1
____
σ−i∞∫
σ+i∞
Y2(s 1,s 2)e s1t ds 1]e s2t ds 2
68

Changing the variable of integration s 1 to s = s 1+s 2 gives
y (t) = 2πi
1
____
σ−i∞∫
σ+i∞
[ 2πi
1
____
σ−i∞∫
σ+i∞
Y2(s −s 2,s 2)e (s −s2)t ds ]e s2t ds 2
Finally, interchanging the order of integration to obtain
y (t) = 2πi
1
____
σ−i∞∫
σ+i∞
[ 2πi
1
____
σ−i∞∫
σ+i∞
Y2(s −s 2,s 2) ds 2]e st ds
shows that the bracketed term must be Y(s) = L[y (t)], and the proof is complete.
Note that by reordering the integrations in this proof, another formula for the
association operation is obtained. Namely
Y(s) = 2πi
1
____
σ−i∞∫
σ+i∞
Y2(s 1,s−s 1) ds 1
(33)
In the degree-n case, computations similar to those in the proof of Theorem 2.7 lead
to a number of different formulas for association of variables. These different formulas
arise from different orderings of the integrations when manipulating the inverse-Laplace-
transform expression for yn(t,...,t), (not to mention different variable labelings). Two forms
are shown below, with veriﬁcation left to Problem 2.12:
Y(s) =
(2πi)n −1
1
________
σ−i∞∫
σ+i∞
Yn(s −s 2,s 2−s 3, . . . ,sn −1−sn,sn) dsn . . . ds 2
(34)
Y(s) =
(2πi)n −1
1
________
σ−i∞∫
σ+i∞
Yn(s −s 1−s 2−. . . −sn −1,s 1,s 2, . . . ,sn −1)ds 1 . . . dsn −1
(35)
But the details of the calculations that establish (34) and (35) also show that the
association operation can be regarded as a sequence of pairwise associations. Such a
sequence can be written as shown below, where semicolons are used to set off the two
variables to be associated.
Yn −1(s,s 3, . . . ,sn) = A2[Yn(s 1,s 2;s 3, . . . ,sn)]
Yn−2(s,s 4, . . . ,sn) = A2[Yn −1(s,s 3;s 4, . . . ,sn)]
...
Y(s) = A2[Y2(s,sn)]
(36)
It is readily demonstrated that this particular sequence of computing the pairwise
associations corresponds to the formula (35) if (32) is used at each step.
Before going further, I should point out one situation in which the associated
transform has a clear system-theoretic interpretation. If Hn(s 1, . . . ,sn) is the multivariable
transfer function of a degree-n homogeneous system, and if u (t) = δ0(t), then from (10),
69

Yn(s 1, . . . ,sn) = Hn(s 1, . . . ,sn). Therefore, the Laplace transform of the impulse response
of the system is Y(s) = H(s) = An[Hn(s 1, . . . ,sn)].
Of course, some caution should be exercised because, as noted in Chapter 1, the
impulse response of a homogeneous system may not exist. This implies that difﬁculties
can arise with the association operation. As an example, consider the system shown in
Figure 2.4 with H1(s) = H2(s) = 1. In this case, the input signal u (t) = δ0(t) results in the
undeﬁned signal δ0
2(t) at the input to H3(s). The reader should set up the association
operation for the transfer function of this system, namely H3(s 1+s 2), and attempt to
evaluate the line integral to see how this difﬁculty appears.
To perform the association operation in (32), the line integral usually is replaced by
a Bromwich contour integration and the residue calculus is applied (assuming the requisite
technical hypotheses). However, simpler approaches should not be overlooked.
For
instance, there are cases where the time-domain route yields the answer easily.
Example 2.10
If
F2(s 1,s 2) =
(s 1−α)k(s 2−β)j
1
_____________
where α and β are real numbers, and k and j are positive integers, then
f 2(t 1,t 2) = (k −1)!
1
______t1
k −1e αt 1
(j −1)!
1
______t2
j −1e βt 2
Therefore,
f (t) = f 2(t,t) = (k −1)!(j −1)!
1
___________t k +j −2 e (α+β)t
and from the single-variable Laplace transform,
F (s) = A2[F (s 1,s 2)] = (k −1)!(j −1)!
1
___________(k +j −2)!
(s −α−β)k +j −1
1
____________
= B
D
k −1
k +j −2E
G (s −α−β)k +j −1
1
____________
For many commonly occurring types of rational functions, the association operation
can be accomplished by the application of a few simple properties to a small group of
tabulated associations. The proofs of these involve familiar integral manipulations, so I
leave them to the reader. Note that the notation indicated in (31) is used throughout. Two
tables of simple associated transforms are given at the end of this section. These are not
extensive; just enough transforms are given to facilitate computations in simple examples.
Theorem 2.8
If F (s 1, . . . ,sn) can be written in the factored form
F (s 1, . . . ,sn) = H (s 1, . . . ,sk)G (sk +1, . . . ,sn)
(37)
then
70

F (s) = An[F (s 1, . . . ,sn)] = 2πi
1
____
σ−i∞∫
σ+i∞
H (s −s 1)G (s 1) ds 1
(38)
where H (s) = Ak[H (s 1, . . . ,sk)] and G (s) = An −k[G (sk +1, . . . ,sn)].
For a number of special cases, (38) can be worked out explicitly, as the following
results indicate.
Corollary 2.1
If F (s 1, . . . ,sn) can be written in the factored form
F (s 1, . . . ,sn) =
(sk+α)q +1
1
_________ G(s 1, . . . ,sk −1,sk +1, . . . ,sn)
(39)
where α is a scalar, then
F (s) =
q !
(−1)q
_____
ds q
d q
____ G (s +α)
(40)
Corollary 2.2
If F (s 1, . . . ,sn) can be written in the form
F (s 1, . . . ,sn) =
sk
2+α2
α
______ G (s 1, . . . ,sk −1,sk +1, . . . ,sn)
(41)
where α is a scalar, then
F (s) = 2i
1
___ [G (s −iα) −G (s +iα)]
(42)
Corollary 2.3
If F (s 1, . . . ,sn) can be written in the form
F (s 1, . . . ,sn) =
sk
2+α2
sk
______ G (s 1, . . . ,sk −1,sk +1, . . . ,sn)
(43)
then
F (s) = 2
1__[G (s −iα) + G (s +iα)]
(44)
Theorem 2.9
If F (s 1, . . . ,sn) can be written in the form
F (s 1, . . . ,sn) = H (s 1+ . . . +sn) G (s 1, . . . ,sn)
(45)
then
F (s) = H (s)G (s)
(46)
Example 2.11
The impulse response of the degree-3 system described by
71

H (s 1,s 2,s 3) =
(s 1+1)(s2
2+3s 2+2)(s 3+2)
1
______________________
can be computed as follows. Using Corollary 2.1 to associate the variables s 1 and s 2 gives
A2[
(s 1+1)(s2
2+3s 2+2)
1
________________] =
(s +1)2+3(s +1)+2
1
________________ =
s 2+5s+6
1
_________
The second step of the procedure again involves Corollary 2.1:
A3[H (s 1,s 2,s 3)] = A2[
(s 2+5s+6)(s 3+2)
1
_______________]
=
(s +2)2+5(s +2)+6
1
________________ =
s 2+9s+20
1
__________
Thus, by partial fraction expansion, the impulse response of the system is
y (t) = e −4t −e −5t
A considerably different input/output representation can be obtained in the transform
domain when the regular kernel representation is the starting point in the time domain. If a
system is described by
y (t) =
0∫
∞
hreg(σ1, . . . ,σn)u (t −σ1−. . . −σn)u (t −σ2−. . . −σn)
. . . u (t −σn) dσ1 . . . dσn
(47)
I will call
Hreg(s 1, . . . ,sn) = L[hreg(t 1, . . . ,tn)]
(48)
the regular transfer function for the system. It is a simple matter to relate the regular
transfer function to the triangular transfer function since
hreg(t 1, . . . ,tn) = htri(t 1+ . . . +tn,t 2+ . . . +tn, . . . ,tn)
for t 1, . . . ,tn ≥ 0. A change of variables in the deﬁnition of the Laplace transform of
hreg(t 1,...,tn) gives
Hreg(s 1, . . . ,sn) = Htri(s 1,s 2−s 1,s 3−s 2, . . . ,sn−sn −1)
(49)
And it is clear from this expression that
Htri(s 1, . . . ,sn) = Hreg(s 1,s 1+s 2,s 1+s 2+s 3, . . . ,s 1+ . . . +sn)
(50)
It is unfortunate, although perhaps no surprise, that the relationships between the
regular and symmetric transfer functions are much more difﬁcult. In fact, the connection is
made through the triangular transfer function by using (27) and (30) in conjunction with
(49) and (50).
To derive interconnection rules for the regular transfer function is a quite tedious
process using the theory so far developed. Therefore, I will postpone this topic until
72

Appendix 4.1, at which point a different approach to the problem is available. But the
computation of input/output behavior in terms of Hreg(s 1, . . . ,sn) need not be delayed.
The transform-domain input/output representation in terms of the regular transfer
function is most easily derived from the association of variables formula involving the
triangular transfer function. Namely, from (35),
Y(s) =
(2πi)n −1
1
________
σ−i∞∫
σ+i∞
Htri(s −s 1−. . . −sn −1,s 1, . . . ,sn −1)
U (s−s 1−. . . −sn −1)U (s 1) . . . U (sn −1) dsn −1 . . . ds 1
(51)
Using (50) gives
Y(s)=
(2πi)n −1
1
________
σ−i∞∫
σ+i∞
Hreg(s −s 1−. . . −sn −1,s −s 2−. . . −sn −1, . . . ,s −sn −1,s)
U (s −s 1−. . . −sn −1)U (s 1) . . . U (sn −1) dsn −1 . . . ds 1
(52)
The important fact about this formula is that there is one situation of wide interest wherein
the line integrations can be evaluated in a general fashion. The restrictions on the forms of
Hreg(s 1, . . . ,sn) and U (s) in the following result are to permit application of the residue
calculus. The form of the regular transfer function may seem a bit strange, but in Chapter 4
it will become very familiar.
Theorem 2.10
Suppose that a degree-n homogeneous system is described by a strictly
proper, rational regular transfer function of the form
Hreg(s 1, . . . ,sn) = Q1(s 1) . . . Qn(sn)
P (s 1, . . . ,sn)
_______________
(53)
where each Qj(sj) is a single-variable polynomial. If the input signal is described by
U (s) =
i =1Σ
r
s +γi
ai
_____
(54)
where γ1, . . . ,γr are distinct, then the response of the system is given by
Y(s) =
i 1=1Σ
r
. . .
in −1=1
Σ
r
ai 1 . . . ain −1 Hreg(s +γi 1+ . . . +γin −1,
s +γi 2+ . . . +γin −1, . . . ,s+γin −1,s)U (s +γi 1+ . . . +γin −1)
(55)
Proof
This result is proved by evaluating the integrals in (52) one at a time using the
residue calculus. The ﬁrst integration to be performed is
73

2πi
1
____
σ−i∞∫
σ+i∞
Hreg(s −s 1−. . . −sn −1,s −s 2−. . . −sn −1, . . . ,s −sn −1,s)
U (s −s 1−. . . −sn −1)U (sn −1) dsn −1
= 2πi
1
____
σ−i∞∫
σ+i∞
Q1(s −s 1−. . . −sn −1) . . . Qn −1(s −sn −1)Qn(s)
P (s −s 1−. . . −sn −1, . . . ,s −sn −1,s)
_____________________________________
U (s −s 1−. . . −sn −1)U (sn −1) dsn −1
A key point here is that 1/Qn(s) can be factored out of the integral, leaving the
denominator of the integrand in the form F (s −sn −1)G (sn −1). Thus the residue calculus can
be applied, and the sum of the residues of the integrand at the poles of U (sn −1), namely,
the poles −γ1,  . . . ,−γr, gives the result
in −1=1
Σ
r
ain −1Hreg(s −s 1−. . . −sn −2+γin −1,s−s 2−. . . −sn −2+γin −1,
. . . ,s +γin −1,s)U (s −s 1−. . . −sn −2+γin −1)
Now (52) can be written in the form
Y(s) =
in −1=1
Σ
r
ain −1 (2πi)n −2
1
________
σ−i∞∫
σ+i∞
Hreg(s−s 1−. . . −sn −2+γin −1,s−s 2−. . . −sn −2+γin −1,
. . . ,s+γin −1,s)U (s −s 1−. . . −sn −2+γin −1)U (s 1) . . . U (sn −2)dsn −2 . . . ds 1
Performing the integrations with respect to sn −2 using the residue calculus just as before,
Y(s) =
in −2=1
Σ
r
in −1=1
Σ
r
ain −2ain −1 (2πi)n −3
1
________
σ−i∞∫
σ+i∞
Hreg(s −s 1−. . . +γin −2+γin −1,
. . . ,s+γin −1,s)U (s −s 1−. . . −sn −3+γin −2+γin −1)U (s 1) . . . U (sn −3)dsn −3 . . . ds 1
Continuing to work through the integrations will lead directly to the input/output relation
in (54).
Example 2.12
The triangular transfer function corresponding to the system in Figure 2.6
is
Htri(s 1,s 2) = s 1(s 1+s 2)(s 1+s 2+1)
2
__________________
Applying (50) gives the regular transfer function for the system:
Hreg(s 1,s 2) = Htri(s 1,s 2−s 1) = s 1s 2(s 2+1)
2
__________
74

Now it is a straightforward matter to compute the unit-step response of the system via (55).
In this case
Y(s) = Hreg(s,s)U (s) =
s 3(s +1)
2
________
and a simple partial fraction expansion yields
y (t) = 2 −2t + t 2 −2e −t, t ≥0
Of course, it is easy to verify this answer by tracing the input signal through the system
diagram.
2.4 The Growing Exponential Approach
Another viewpoint towards the transform representation of homogeneous systems is
based upon the following property of linear systems. Consider a system described by
y (t) =
−∞∫
∞
h (σ)u (t −σ) dσ
(56)
with the growing exponential input signal u (t) = e λt, λ > 0, deﬁned for all t. The response
is
y (t) =
−∞∫
∞
h (σ) e λ(t−σ) dσ
=
0∫
∞
h (σ)e −λσ dσ e λt
(57)
where the lower limit has been raised to 0 in view of the one-sidedness of h (t). Thus, if
H (s) is the system transfer function, and if λ is in the region of convergence of the
transform,
y (t) = H (λ)e λt ,
t ε (−∞,∞)
(58)
In particular, if the linear system is stable, then λ will be in the region of convergence
since λ > 0.
The fact that a growing exponential input signal is simply scaled by a linear system
to produce the output signal is sometimes called the eigenfunction property of linear
systems. In addition, the response of the linear system to a linear combination of growing
exponentials
u (t) =
i =1Σ
p
αi e λit ,
λ1, . . . ,λp > 0
(59)
is given by
75

y (t) =
i =1Σ
p
αiH (λi) e λit
(60)
This is sometimes useful since arbitrary two-sided input signals can be approximated by
linear combinations of growing exponentials. In the present context the transfer function
H (s) of a linear system is regarded as that function that characterizes the response to
growing exponentials via (58) or (60).
For a homogeneous system of degree n > 1, I will proceed in an analogous fashion.
Using the representation
y (t) =
−∞∫
∞
hn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(61)
the reader can see readily that the response to u (t) = e λt, λ > 0, t ε (−∞,∞), is
y (t) = Hn(λ, . . . ,λ)e nλt
(62)
where Hn(s 1, . . . ,sn) is a system transfer function. But it should come as no surprise that
the response to a single growing exponential indicates little about the behavior of the
system for other inputs. In other words, it is clear that a complete characterization of the
multivariable function Hn(s 1, . . . ,sn) is not obtained from the single-variable function
Hn(λ, . . . ,λ).
Before treating input signals that are arbitrary linear combinations of growing
exponentials, consider the case of "double exponential" inputs
u (t) = α1e λ1t + α2e λ2t, λ1,λ2 > 0, t ε (−∞,∞)
(63)
These will be of particular importance in the sequel. From (63) and (61), write
y (t) =
−∞∫
∞
hn(σ1, . . . ,σn)
j =1
Π
n
[α1e λ1(t −σj) + α2e λ2(t −σj)] dσ1 . . . dσn
=
−∞∫
∞
hn(σ1, . . . ,σn)
k 1=1
Σ
2
. . .
kn=1
Σ
2
(
j =1
Π
n
αkj) exp[
j =1Σ
n
λkj(t −σj)] dσ1 . . . dσn
=
k 1=1
Σ
2
. . .
kn=1
Σ
2
(
j =1
Π
n
αkj )
0∫
∞
hn(σ1, . . . ,σn) exp(−
j =1Σ
n
λkjσj) dσ1 . . . dσn exp(
j =1Σ
n
λkjt)
=
k 1=1
Σ
2
. . .
kn=1
Σ
2
(
j =1
Π
n
αkj) Hn(λk 1, . . . ,λkn)exp(
j =1Σ
n
λkjt)
(64)
Note that many of the terms in this output expression have identical exponents (λk 1+ . . . +
λknt). I can write all terms with exponent kλ1 + (n −k)λ2)t as the single term
α1
k α2
n −k Gk,n −k(λ1,λ2)exp[(kλ1 + (n −k)λ2)t ]
(65)
76

where
Gk,n −k(λ1,λ2) =
k 1+ . . . +kn=2n −k
k 1=1
Σ
2
. . .
kn=1
Σ
2
Hn(λk 1, . . . ,λkn)
(66)
Thus, the response of the system to the input (63) can be written in the form
y (t) =
k =0Σ
n
α1
k α2
n −k Gk,n −k(λ1,λ2) e [kλ1+(n −k)λ2]t
(67)
There are two observations to be made in the context of the double-exponential
case. First, note that if Hn(s 1, . . . ,sn) is the symmetric transfer function of the system,
then (66) can be written as
Gk,n −k(λ1,λ2) = B
D k
nE
G Hnsym(
k
λ1, . . . ,λ1;
n −k
λ2, . . . ,λ2)
(68)
Second, if the system is of degree n = 2, then (68) implies that
H2sym(λ1,λ2) = 2
1__ G1,1(λ1,λ2)
(69)
That is, for a double-exponential input to a degree-2 system, the symmetric transfer
function is determined by the coefﬁcient of the e (λ1 + λ2)t term in the output.
Example 2.12
To determine the multivariable transfer function of the system shown in
Figure 2.4, the double exponential input method can be applied since the system is of
degree 2. Denoting the input to H3(s) by v (t), and choosing the coefﬁcients α1 = α2 = 1
in (63),
v (t) = [H1(λ1)e λ1t + H1(λ2)e λ2t][H2(λ1)e λ1t + H2(λ2)e λ2t]
= H1(λ1)H2(λ1)e 2λ1t + [H1(λ1)H2(λ2) + H1(λ2)H2(λ1)]e (λ1+λ2)t
+ H1(λ2)H2(λ2)e 2λ2t
Thus,
y (t) = H1(λ1)H2(λ1)H3(2λ1)e 2λ1t
+ [H1(λ1)H2(λ2) + H1(λ2)H2(λ1)]H3(λ1+λ2)e (λ1+λ2)t
+ H1(λ2)H2(λ2)H3(2λ2)e 2λ2t
and the symmetric transfer function of the system is
Hsym(s 1,s 2) = 2
1__ [H1(s 1)H2(s 2) + H1(s 2)H2(s 1)]H3(s 1+s 2)
77

It is clear that this is the symmetric version of the transfer function derived in Example 2.6.
Now consider the response of a degree-n homogeneous system to an arbitrary linear
combination of growing exponentials such as in (59). My purpose is again twofold: to
present a method for determining the symmetric transfer function of such a system, and to
develop a representation for the system response to such an input signal.
Substituting (59) into (61) gives
y (t) =
−∞∫
∞
hn(σ1, . . . ,σn)
j =1
Π
n
[α1e λ1(t−σj)+ . . . +αpe λp(t−σj)] dσ1 . . . dσn
=
−∞∫
∞
hn(σ1, . . . ,σn)
k 1=1
Σ
p
. . .
kn=1
Σ
p
[
j =1
Π
n
αkj]
exp[
j =1Σ
n
λkj(t −σj)] dσ1 . . . dσn
=
k 1=1
Σ
p
. . .
kn=1
Σ
p
[
j =1
Π
n
αkj]
−∞∫
∞
hn(σ1, . . . ,σn) exp(−
j =1Σ
n
λkjσj) dσ1 . . . dσn exp(
j =1Σ
n
λkjt)
=
k 1=1
Σ
p
. . .
kn=1
Σ
p
[
j =1
Π
n
αkj]Hn(λk 1, . . . ,λkn) exp(
j =1Σ
n
λkjt)
(70)
where, as before, certain assumptions have been made concerning the regions of
convergence. Of course, many of the terms in this expression contain identical exponents.
By collecting all those terms with like exponents, a simpliﬁed expression can be obtained.
In particular, consider all those terms in (70) with the exponent (m1λ1+m2λ2
+ . . . +mpλp)t, where each mi is an integer satisfying 0 ≤ mi ≤ n with
i =1Σ
p
mi = n. I can
write this collection of terms as
α1
m1 . . . αp
mp Gm1, . . . ,mp (λ1, . . . ,λp) exp[(m1λ1+ . . . +mpλp)t ]
(71)
where, using an implicit summation in which mj of the n indices take the value j,
j = 1,...,p,
Gm1, . . . ,mp(λ1, . . . ,λp) =
mj indices = j
k 1=1
Σ
p
. . .
kn=1
Σ
p
Hn(λk 1, . . . ,λkn)
(72)
Thus, the output signal is expressed in the form
y (t) =
mΣ α1
m1 . . . αp
mp Gm1, . . . ,mp(λ1, . . . ,λp) exp[(m1λ1+ . . . +mpλp)t ]
(73)
where
mΣ indicates a p-fold sum over all integer indices m1, . . . , mp such that 0 ≤ mi ≤ n,
78

and m1+ . . . +mp = n.
If the transfer function Hn(s 1, . . . ,sn) is symmetric, then the implicit sum in (72) can
be made somewhat more explicit. Since there will be
B
A
D m1
n E
A
G
identical terms with m1 of the
indices 1, then
B
A
D
m2
n −m1E
A
G
identical terms with m2 of the indices 2, and so forth; and since
B
A
D m1
n E
A
G
B
A
D
m2
n −m1E
A
G
B
A
D
m3
n −m1−m2E
A
G
. . . B
A
D
mp −1
n −m1−. . . −mp −2E
A
G
= m1!m2! . . . mp!
n !
______________
(74)
(72) can be written in the form
Gm1, . . . ,mp(λ1, . . . ,λp) = m1!m2! . . . mp!
n !
______________Hnsym(
m1
λ1, . . . ,λ1; . . . ;
mp
λp, . . . ,λp)
(75)
Aside from being a neater way to write the exponential coefﬁcients, when p = n the
relationship
G1, . . . ,1(λ1, . . . ,λn) = n !Hnsym(λ1, . . . ,λn)
(76)
is obtained. To rephrase: when a linear combination of n growing exponentials is applied
to a degree-n homogeneous system, the symmetric transfer function of the system is given
by 1/n ! times the coefﬁcient of exp[(λ1+ . . . +λn)t ]. The utility of this approach will be
demonstrated by the following not quite trivial example. I invite the reader to compute the
transfer function by using the interconnection rules, just to compare the two approaches.
Figure 2.7. A cascade connection of linear
systems and integer-power nonlinearities.
 
Example 2.13
The system shown in Figure 2.7 is assumed to have stable linear
subsystems (for convergence purposes) and integer-power nonlinearities, with m1m2 = m.
To ﬁnd the transfer function, consider the input
u (t) =
i =1Σ
m
e λit ,
λ1, . . . ,λm > 0
Tracing this signal through the system diagram gives
79
u
H0(s)
H1(s)
(.)m
(.)m
y
1
2
v

v (t) = [
i =1Σ
m
H0(λi)e λit]m1
=
i 1=1Σ
m . . .
im1=1
Σ
m
[
j =1
Π
m1
H0(λij)] exp(
k =1Σ
m1
λikt)
Then
y (t) = [
i 1=1Σ
m . . .
im1=1
Σ
m
[
j =1
Π
m1
H0(λij)]H1(
k =1Σ
m1
λik) exp(
k =1Σ
m1
λikt) ]m2
= [
i 1=1Σ
m . . .
im1=1
Σ
m
[
j =1
Π
m1
H0(λij)]H1(
k =1Σ
m1
λik ) exp(
k =1Σ
m1
λikt) ]
[
im1+1=1
Σ
m
. . .
i 2m1=1
Σ
m
[
j =m1+1
Π
2m1
H0(λij)]H1(
k =m1+1
Σ
2m1
λik) exp(
k =m1+1
Σ
2m1
λikt) ]
. . .
=
i 1=1Σ
m . . .
im=1Σ
m
[
j =1
Π
m
H0(λij)][
j =1
Π
m2
H1(
k =1Σ
m1
λik +(j −1)m1)] exp(
k =1Σ
m
λikt)
where in the middle equality only the ﬁrst two factors have been written for simplicity.
The important point is that distinct variables must be maintained in each factor.
Now collect all those terms that correspond to the exponent (λ1+ . . . +λm)t in order
to obtain the symmetric transfer function. There are m ! such terms, m choices of which
index is 1, m −1 choices of which index is 2, and so on. However, each of these m ! terms
is itself an asymmetric transfer function; the various choices of indices correspond to the
various permutations of the variables. Taking, for instance, i 1=1, i 2=2, . . . ,im=m, one
system transfer function is obtained:
Hm(s 1, . . . ,sm) =
j =1
Π
m
H0(sj)
j =1
Π
m2
H1(
k =1Σ
m1
sk +(j −1)m1)
The properties of growing exponentials just discussed provide another viewpoint
from which the transfer function can be considered. Recall that in the linear case the
transfer function H (s) can be viewed as that function that characterizes the response to a
linear combination of growing exponentials (59), as shown in (60). For a degree-n
homogeneous system, a transfer function Hn(s 1, . . . ,sn) can be viewed as a function that
characterizes the response to (59) via the expression
y (t) =
k 1=1
Σ
p
. . .
km=1
Σ
p
(
j =1
Π
n
αkj)Hn(λk 1, . . . ,λkn) exp(
j =1Σ
n
λkjt)
(77)
I emphasize that this characterization of the transfer function is appropriate for both
symmetric and asymmetric forms of the transfer function, although the special features of
(77) in the case of symmetric transfer functions will prove most useful.
80

2.5 Polynomial and Volterra Systems
The transform representation of stationary polynomial or Volterra systems basically
involves the collection of homogeneous subsystem transfer functions. Thus, for example,
response calculations are performed by summing the responses of the subsystems as
calculated individually by association of variables or from Theorem 2.10. For Volterra
systems this summation requires consideration of the convergence properties of an inﬁnite
series of time functions. Often, convergence is crucially dependent on properties of the
input, for example, bounds on the amplitude of the input signal.
The growing exponential approach to transfer-function determination can be a very
convenient tool for stationary polynomial and Volterra systems. To determine the ﬁrst N
symmetric transfer functions, assume that the input signal is a sum of N distinct growing
exponentials. Then if the output is a sum of growing exponentials, either by calculation or
by assumption, the coefﬁcient of e (λ1+ . . . +λn)t is n !Hnsym(λ1, . . . ,λn), n = 1,2, . . . ,N. For
interconnections of homogeneous systems, this approach obviates the need to explicitly
unravel the various homogeneous subsystems from the structure.
Example 2.14
A quick trace of the input αu through the system in Figure 2.8 shows that
it is polynomial of degree 2. Application of the input signal
u (t) = e λ1t + e λ2t, λ1,λ2 > 0
yields the output signal
y (t) = H1(λ1)H2(λ1)e λ1t + H1(λ2)H2(λ2)e λ2t + H1(λ1)H3(λ1)e 2λ1t
+ [H1(λ1)H3(λ2)+H1(λ2)H3(λ1)]e (λ1+λ2)t + H1(λ2)H3(λ2)e 2λ2t
+ H4(λ1)e λ1t + H4(λ2)e λ2t
= [H1(λ1)H2(λ1)+H4(λ1)]e λ1t + [H1(λ1)H3(λ2)+H1(λ2)H3(λ1)]e (λ1+λ2)t + . . .
Thus, the transfer function of the degree-1 subsystem is
H (s 1) = H1(s 1)H2(s 1) + H4(s 1)
and the symmetric transfer function of the degree-2 subsystem is
H2sym(s 1,s 2) = 2
1__[H1(s 1)H3(s 2) + H1(s 2)H3(s 1)]
Of course, in this simple case the transfer-function interconnection rules produce the same
result more efﬁciently.
81

Figure 2.8. A polynomial system of degree 2.
 
When a linear or nonlinear feedback loop is closed around a homogeneous,
polynomial, or Volterra system, the result is a Volterra system, in general. This situation
was discussed in Chapter 1 only in terms of a general operator notation, principally
because time-domain analysis of the feedback connection is prohibitively complex. Now
that transform-domain tools are available, I will illustrate the feedback computations for
the  three terms for the general feedback system shown in Figure 2.9.
Figure 2.9. A feedback system in operator notation.
 
Considerable simpliﬁcation is achieved if the system is redrawn as in Figure 2.10. This
conﬁguration shows that the problem of computing the closed-loop representation can be
viewed as two cascade-connection problems, and a feedback connection of the relatively
simple form shown in Figure 2.11.
Figure 2.10. The system in Figure 2.9, redrawn.
 
The cascade connections are left to the reader (Problem 2.16), while the feedback
connection in Figure 2.11 now will be discussed in detail.
82
u
y
Σ
F
G
–
u
y
Σ
H
–
Figure 2.11. The basic feedback system.
H1(s)
u
H3(s)
H4(s)
Π
H2(s)
Σ
y
u
Σ
F
–
G
y
G

 
To compute the ﬁrst three closed-loop symmetric transfer functions, the feedback
system in Figure 2.11 is redrawn using transform-domain notation as shown in Figure 2.12.
It is assumed that the feedback transfer functions are symmetric, and only the ﬁrst three are
shown, as the higher-degree transfer functions will not enter the calculation.
Figure 2.12. Symmetric transfer function representations
for the basic feedback system.
 
The procedure for determining the ﬁrst three symmetric transfer functions is to
assume an input of the form
u (t) = e λ1t + e λ2t + e λ3t
and a response in terms of the unknown closed-loop symmetric transfer functions of the
form
y (t) = F1(λ1)e λ1t + F1(λ2)e λ2t + F1(λ3)e λ3t + 2!F2sym(λ1,λ2)e (λ1+λ2)t
+ 2!F2sym(λ1,λ3)e (λ1+λ3)t + 2!F2sym(λ2,λ3)e (λ2+λ3)t
+ 3!F3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t + . . .
The economy of notation obtained by hiding many terms behind the ellipsis in this
expression is required to avoid writer’s cramp. At the same time extreme care is required,
for the right answer can be avoided if the economical notation causes contributing terms to
be neglected.
Notice also that here is where the assumption that the system can be
described by a Volterra series expression enters the calculation.
Now the strategy is to trace the signals through the system diagram in Figure 2.11 to
through the homogeneous systems in the feedback path involves the previously derived
formulas for the response of homogeneous systems to sums of growing exponentials. Of
course, the majority of terms in these formulas are discarded because they do not
contribute to the end result. Finally, equating coefﬁcients of like exponentials in the two
H1(s)
H2sym(s1, s2)
Σ
H3sym(s1, s2, s3)
–
–
–
u
y
83

output expressions yields a set of equations for the closed-loop symmetric transfer
functions. Proceeding in this manner gives
y (t) = e λ1t + e λ2t + e λ3t −H1(λ1)F1(λ1)e λ1t −H1(λ2)F1(λ2)e λ2t
−H1(λ3)F1(λ3)e λ3t −2H1(λ1+λ2)F2sym(λ1,λ2)e (λ1+λ2)t −. . .
−6H1(λ1+λ2+λ3)F3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t −. . .
−2H2sym(λ1,λ2)F1(λ1)F1(λ2)e (λ1+λ2)t −. . .
−4[F1(λ1)F2sym(λ2,λ3)H2sym(λ1,λ2+λ3) + F1(λ2)F2sym(λ1,λ3)
H2sym(λ2,λ1+λ3) + F1(λ3)F2(λ1,λ2)H2sym(λ3,λ1+λ2)]e (λ1+λ2+λ3)t
−6F1(λ1)F1(λ2)F1(λ3)H3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t + . . .
Equating the coefﬁcients of e λ1t in the two expressions for y (t) gives
F1(λ1) = 1 −H1(λ1)F1(λ1)
Solving yields the degree-1 closed-loop transfer function
F1(s) = 1 + H1(s)
1
_________
Equating coefﬁcients of e (λ1+λ2)t gives the equation
2F2sym(λ1,λ2) = −2H1(λ1+λ2)F2sym(λ1,λ2) −2H2sym(λ1,λ2)F1(λ1)F1(λ2)
the solution of which gives
F2sym(s 1,s 2) = [1+H1(s 1+s 2)][1+H1(s 1)][1+H1(s 2)]
−H2sym(s 1,s 2)
________________________________
Equating coefﬁcients of e (λ1+λ2+λ3)t gives
6F3sym(λ1,λ2,λ3) = −6 H1(λ1+λ2+λ3)F3sym(λ1,λ2,λ3)
−6H3sym(λ1,λ2,λ3)F1(λ1)F1(λ2)F1(λ3) −4[F1(λ1)F2sym(λ2,λ3)
H2sym(λ1,λ2+λ3) + F1(λ2)F2sym(λ1,λ3)H2sym(λ2,λ1+λ3)
+ F1(λ3)F2sym(λ1,λ2)H2sym(λ3,λ1+λ2)]
Thus,
84

F3sym(s 1,s 2,s 3) = [1+H1(s 1+s 2+s 3)][1+H1(s 1)][1+H1(s 2)][1+H1(s 3)]
1
_____________________________________________
[ −H3sym(s 1,s 2,s 3) +
1+H1(s 2+s 3)
(2/3)H2sym(s 1,s 2+s 3)H2sym(s 2,s 3)
_____________________________
+
1+H1(s 1+s 3)
(2/3)H2sym(s 2,s 1+s 3)H2sym(s 1,s 3)
_____________________________
+
1+H1(s 1+s 2)
(2/3)H2sym(s 3,s 1+s 2)H2sym(s 1,s 2)
_____________________________ ]
2.6 Remarks and References
Remark 2.1
An introduction to the multivariable Laplace transform as a mathematical
tool can be found in
V. Ditkin, A. Prudnikov, Operational Calculus in Two Variables and Its Applications,
Pergamon Press, New York, 1962.
This is a translation of the original volume in Russian published by Fizmatgiz, in Moscow,
in 1958. A very readable treatment of convergence issues, numerous examples, properties,
and extensive tables of the 2-variable transform are provided. These go well beyond the
introduction in Section 2.1. The multivariable Fourier transform is closely related to the
multivariable Laplace transform, and expositions of the Fourier transform that include the
multivariable case are much more numerous. For example, see
D. Champeney, Fourier Transforms and Their Physical Applications, Academic Press,
New York, 1973.
(The multivariable Fourier transform will be used extensively in chapters 5 and 7.)
Remark 2.2
The use of the multivariable Laplace transform in system theory, the
association operation, and interconnection rules are discussed in
D. George, "Continuous Nonlinear Systems," MIT RLE Technical Report No. 355, 1959
(AD246-281).
More readily available expositions include
Y. Ku, A. Wolff, "Volterra-Wiener Functionals for the Analysis of Nonlinear Systems,"
Journal of The Franklin Institute, Vol. 281, pp. 9-26, 1966.
85

R. Parente, "Nonlinear Differential Equations and Analytic System Theory," SIAM Journal
on Applied Mathematics, Vol. 18, pp. 41-66, 1970.
L. Chua, C. Ng, "Frequency-Domain Analysis of Nonlinear Systems: General Theory,
Formulation of Transfer Functions," IEE Journal on Electronic Circuits and Systems, Vol.
3, pp. 165-185, 257-269, 1979.
Remark 2.3
Simple properties of the association operation and a table of associated
transforms are given in
C. Chen, R. Chiu, "New Theorems of Association of Variables in Multidimensional
Laplace Transforms," International Journal of Systems Science, Vol. 4, pp. 647-664, 1973.
General formulas for performing the association operation in a wide class of Laplace
transforms (with factored denominators) are derived in
L. Crum, J. Heinen, "Simultaneous Reduction and Expansion of Multidimensional Laplace
Transform Kernels," SIAM Journal on Applied Mathematics, Vol. 26, pp. 753-771, 1974.
A basic review of the residue calculations that I have used several times to evaluate
complex convolution integrals can be found in
R. Schwartz, B. Friedland, Linear System Theory, McGraw-Hill, New York, 1965.
The proof of Theorem 2.10 touches upon some relatively subtle issues in the residue
method. In particular, the need for the factored form for the denominator of the regular
transfer function can be appreciated more fully by analyzing the residue calculation in
detail for the example
Hreg(s 1,s 2) = s 1s 2 + 1
1
________
Remark 2.4
Many authors have discussed the properties of the response of a
homogeneous system to sums of exponentials. Usually the exponents are considered to be
purely imaginary so there are close ties to the frequency response of the system - a topic to
be considered in Chapter 5. However, the growing exponential viewpoint adopted in
Section 2.4 seems to lead more economically to a characterization of the symmetric
transfer function. A more general version of Example 2.13 is considered in
W. Smith, W. Rugh, "On the Structure of a Class of Nonlinear Systems," IEEE
Transactions on Automatic Control, Vol. AC-19, pp. 701-706, 1974.
In particular, it is shown that the response to two growing exponentials sufﬁces to
86

characterize homogeneous systems that are cascades of power nonlinearities and linear
systems.
Remark 2.5
The regular transfer function representation, its relationship to the
triangular and symmetric transfer functions, and the input/output formula in Theorem 2.10
were introduced in
G. Mitzel, S. Clancy, W. Rugh, "On Transfer Function Representations for Homogeneous
Nonlinear Systems," IEEE Transactions on Automatic Control, Vol. AC-24, pp. 242-249,
1979.
Remark 2.6
Further discussion of the transform-domain analysis of the feedback
connection for nonlinear systems can be found in the report by George cited in Remark
2.1. See also
M. Brilliant, "Theory of the Analysis of Nonlinear Systems," MIT RLE Technical Report
No. 345, 1958 (AD216-209).
and
J. Barrett, "The Use of Functionals in the Analysis of Nonlinear Physical Systems,"
Journal of Electronics and Control, Vol. 15, pp. 567-615, 1963.
Recurrence relations for the closed-loop symmetric transfer function of certain feedback
systems are derived in
E. Bedrosian, S. Rice, "The Output Properties of Volterra Systems (Nonlinear Systems
with Memory) Driven by Harmonic and Gaussian Inputs," Proceedings of the IEEE, Vol.
59, pp. 1688-1707, 1971.
Remark 2.7
The suggested factoring procedure for symmetric polynomials of the form
P 1(s 1) . . . P1(sn)P2(s 1+ . . . +sn) is discussed in
K. Shanmugam, M. Lal, "Analysis and Synthesis of a Class of Nonlinear Systems," IEEE
Transactions on Circuits and Systems, Vol. CAS-23, pp. 17-25, 1976.
The factoring procedure I have outlined for more general symmetric polynomials was
suggested by E. G. Gilbert.
2.7 Problems
2.1. Compute the Laplace transforms of
87

f (t 1,t 2) = δ0(t 1)δ0(t 2)
f (t 1,t 2) = δ0(t 1)δ0(t 1−t 2)
2.2. Compute the Laplace transforms of the symmetric functions
f (t 1,t 2) = 2
1__t 1 + 2
1__t 2 −2
1__t 1e −t 2 −2
1__t 2e −t 1, t 1,t 2 ≥0
g (t 1,t 2) =
B
C
D t 2 −t 2e −t 1, t 2 > t 1 > 0
t 1 −t 1e −t 2, t 1 ≥t 2 ≥0
2.3. Find convergence regions for the Laplace transforms of the one-sided functions
f (t 1,t 2) = e min [t 1,t 2], f (t 1,t 2) =
B
C
D 0, otherwise
e t 2, t 1 ≥t 2 ≥0
2.4. Find the inverse Laplace transform of
F (s 1,s 2) = (s 1+1)(s 2+1)(s 1+s 2+2)
1
_____________________
using the line integration formula. Then check your answer using the cascade formula and
cleverness.
2.5. Prove Theorem 2.3.
2.6. Prove Theorem 2.4.
2.7. Prove Theorem 2.5.
2.8. Show that if
L[f (t 1, . . . ,tn)] = F (s 1, . . . ,sn)
then
L[e −a 1t 1−. . . −antn f (t 1, . . . ,tn)] = F (s 1+a 1, . . . ,sn+an)
2.9. State and prove a ﬁnal-value theorem for multivariable Laplace transforms.
2.10. Suppose L[f (t)] = F (s). Find a formula for the 2-variable Laplace transforms
L{f (min[t 1,t 2])} and L{f (max[t 1,t 2])}.
88

2.11. If L[f (t)] = F (s), ﬁnd the 2-variable transform L[f (t 1+t 2)].
2.12. Show that the association formulas (34) and (35) are equivalent.
2.13. Compute a transfer function for the cascade connection shown below.
2.14. Verify the symmetric transfer function given in Example 2.9 by computing the
Laplace transform of the symmetric kernel given in Problem 1.7.
2.15. Using the kernels found in Example 1.4, compute the symmetric transfer functions
for the integral-square and square-integral computers. Then calculate these transfer
functions using the interconnection rules in Section 2.2.
2.16. Consider the system y (t) = h (u (t)), where h (u) is an analytic function given by
h (u) =
j =0Σ
∞
hju j
Show that the degree-n symmetric transfer function for the system is given by
Hnsym(s 1, . . . ,sn) = hn, n = 1,2, . . .
2.17. Find transfer functions through degree 3 for the polynomial system shown below.
2.18. Compute the symmetric transfer function for the system shown below.
89
u
y
e
–t –t
δ–1(t)
1
2
H1(s)
H2sym(s1, s2)
H3sym(s1, s2, s3)
Σ
G1(s)
G2sym(s1, s2)
G3sym(s1, s2, s3)
Σ
Π
Π
Σ
—s
1

2.19. Show that the two systems below have identical responses to all single growing
exponentials. Is this so for double growing exponential inputs?
2.20. If
F (s 1,s 2) =
(s 1+s 2)2+2(s 1+s 2)+3
1
___________________
compute F (s) = A2[F (s 1,s 2)].
(Be careful!) Can you compute the association for
F (s 1,s 2)/s 1s 2?
2.21. Consider the system described by
y (t) = u (t) + ε[u. (t)]2 u..(t)
where the dot notation is used for differentiation. Show that the ﬁrst three symmetric
transfer functions for this system are
H1(s) = 1, H2(s 1,s 2) = 0, H3(s 1,s 2,s 3) = 2εs 1s 2s 3(s 1+s 2+s 3)
2.22. Show that
L[δ−1(t 1−t 2)δ−1(t 2−t 3) . . . δ−1(tn −1−tn)] = s 1(s 1+s 2) . . . (s 1+s 2+ . . . +sn)
1
___________________________
2.23. Given a polynomial P (s 1, . . . ,sn), devise a simple test to determine if it can be
written as a product of single-variable polynomials
P (s 1, . . . ,sn) = P1(s 1) . . . Pn(sn)
How would you determine the Pj(sj)’s?
2.24. For the feedback system shown below, show that the ﬁrst three closed-loop
symmetric transfer functions are
90
(.)2
___
s + b
K
(.)2
__________
s2+4bs+4b2
4K2

F1(s) =G (s)
F2sym(s 1,s 2) = −h 2G (s 1)G (s 2)G (s 1+s 2)
F3sym(s 1,s 2,s 3) = [ 3
2h2
2
____ [G (s 1+s 2) + G (s 1+s 3) + G (s 2+s 3)]
−h 3]G (s 1)G (s 2)G (s 3)G (s 1+s 2+s 3)
2.25. For the feedback system shown below, ﬁnd the ﬁrst three closed-loop transfer
functions.
91
u
Σ
h2(.)2 + h3(.)3
y
G(s)
–
Σ
G1(s)
G3sym(s1, s2, s3)
Σ
H1(s)
u
y
–

Table 2.1
Simple 2-Variable Associated Transforms
A2[ (s 1+a)(s 1+b)
K
____________]
=
s +a +b
K
_______
A2[ (s 1+a)(s 1+b)(s 2+a)(s 2+b)
K
________________________]
=
(s +2a)(s +2b)(s +a +b)
2K
____________________
A 2[
(s 1+a)(s 1+b)(s 2+a)(s 2+b)[c (s 1+s 2)2+d (s 1+s 2)+e ]
K
_____________________________________________]
=
(cs 2+ds +e)(s +a +b)(s +2a)(s +2b)
2K
______________________________
A 2[ (s 1+a)(s 1+b)(s 2+a)(s 2+b)
K (s 1+c)(s 2+c)
________________________]
=
(b −a)2
K
_______ [ s +2a
(c −a)2
_______ − 
s +a +b
2(c −a)(c −b)
____________ +  s +2b
(c −b)2
_______]
A2[
(s 1+a)(s 1+b)(s 2+a)(s 2+b)[d (s 1+s 2)2+e (s 1+s 2)+f ]
K (s 1+c)(s 2+c)
_____________________________________________]
=
ds 2+es +f
K /(b −a)2
_________ [ s +2a
(c −a)2
_______ − 
s +a +b
2(c −a)(c −b)
____________ +  s +2b
(c −b)2
_______]
Table 2.2
Simple 3-Variable Associated Transforms
A3[ (s 1+a)(s 2+b)(s 3+c)
K
__________________]
=
s +a +b +c
K
__________
A3[ (s 1+a)(s 1+b)(s 2+a)(s 2+b)(s 3+a)(s 3+b)
K
___________________________________]
=
(a −b)3
K
_______ [ s +3b
1
______ − s +2a +b
3
________ +  s +a +2b
3
________ − s +3a
1
______]
A 3[ (s 1+a)(s 1+b)(s 2+a)(s 2+b)(s 3+a)(s 3+b)
K (s 1+c)(s 2+c)(s 3+c)
___________________________________]
=
(a −b)3
K
_______ [ s +3a
(a −c)3
_______ − 
s +2a +b
3(a −c)2(b −c)
____________ +  
s +a +2b
3(a −c)(b −c)2
____________ − s +3b
(c −b)3
_______]
A3[
(s 1+a)2(s 2+a)2(s 3+a)2
K
_____________________]
=
(s +a)4
6K
_______

CHAPTER 3
OBTAINING INPUT/OUTPUT REPRESENTATIONS
FROM DIFFERENTIAL-EQUATION DESCRIPTIONS
Systems often are described in terms of a vector, ﬁrst-order differential equation
called the state equation. When the input/output behavior of a system described in this
way is of interest, a representation for the solution of the state equation is needed. In this
chapter, several procedures for determining the kernels or transfer functions in a
Volterra/Wiener representation corresponding to a given state equation will be discussed.
In general, an inﬁnite Volterra series is required, and this raises again the issue of
convergence. Although general convergence results will be mentioned, most of the
discussion will be phrased in terms of ﬁnding degree-N polynomial-system truncations of
the full Volterra system. (A proof of one general convergence result is given in Appendix
3.1.)
A major difﬁculty in dealing with nonlinear differential equations is that existence
and/or uniqueness of solutions, even in a local sense, cannot be taken for granted. The
nasty things that sometimes occur can be demonstrated with very simple, innocent-
appearing examples, and I presume the reader is well aware of the situation. To avoid all
this, it will be assumed that the differential equations under study all have unique solutions
on the time interval of interest, regardless of the particular initial state or (nominally
assumed to be piecewise-continuous) input signal. This means that well known conditions
on the growth and smoothness properties of the nonlinear functions in a given differential
equation should be checked before methods based on the Volterra/Wiener representation
are used. In fact, they should be checked before any methods are used.
Much of the development in the following pages is in terms of differential equations
with time-variable parameters, that is, the nonstationary case. The reader uninterested in
this case can specialize the development readily. Indeed, not much more is required than
to drop arguments in the right places and replace Φ(t,τ) by e A(t −τ).
93

3.1 Introduction
To ease into the subject, I begin with a review of one technique for determining an
input/output representation corresponding to the linear state equation
x.(t) = A (t)x (t) + b (t)u (t) , t ≥0
y (t) = c (t)x (t) , x (0) = x 0
(1)
In this expression x (t) is the n-dimensional state vector, u (t) is the scalar input, and y (t) is
the scalar output. Typical assumptions would be that on some ﬁnite time interval [0,T],
A (t), b (t), and c (t) are continuous, and the input signal is bounded and piecewise
continuous.
Such assumptions are sufﬁcient to guarantee the existence of a unique
solution of (1) for all t ε [0,T]. This standard result usually is derived from successive
approximations, though that will not be demonstrated here. My principal interest is to get
the form of the solution to (1) in a suggestive way with respect to an approach to nonlinear
state equations.
First consider the solution of (1) with u (t) = 0 for all t ≥ 0. In that case, both sides of
the differential equation can be integrated to obtain
x (t) = x 0 +
0∫
t
A (σ1)x (σ1) dσ1
(2)
Based upon this expression, repeated substitutions can be performed. More speciﬁcally,
write
x (σ1) = x 0 +
0∫
σ1
A (σ2)x (σ2) dσ2
(3)
and substitute into (2) to obtain
x (t) = x 0 +
0∫
t
A (σ1) dσ1x 0 +
0∫
t
A (σ1)
0∫
σ1
A (σ2)x (σ2) dσ2dσ1
(4)
Continuing by substituting for x(σ2) in (4) using an expression of the form (3), gives
x (t) = [I +
0∫
t
A (σ1) dσ1 +
0∫
t
A (σ1)
0∫
σ1
A (σ2) dσ2dσ1] x 0
+
0∫
t
A (σ1)
0∫
σ1
A (σ2)
0∫
σ2
A (σ3)x (σ3) dσ3dσ2dσ1
Repeating this process indeﬁnitely, and showing that the last term approaches 0 (in
norm) in a uniform way, gives a solution in the form
x (t) = Φ(t,0) x 0
(5)
94

where the transition matrix Φ(t,τ) is deﬁned on any ﬁnite square [0,T] × [0,T] by the
uniformly convergent series
Φ(t,τ) = I +
τ∫
t
A (σ1) dσ1 +
τ∫
t
A (σ1)
τ∫
σ1
A (σ2) dσ2dσ1
+ . . . +
τ∫
t
A (σ1)
τ∫
σ1
A (σ2) . . .
τ∫
σk −1
A (σk) dσk . . . dσ1 + . . .
(6)
known as the Peano-Baker series.
An important property of the transition matrix that will be used in the sequel without
beneﬁt of derivation is the multiplication formula
Φ(t,σ)Φ(σ,τ) = Φ(t,τ)
(7)
This formula in conjunction with the fact that Φ(t,τ) is invertible at each t and τ gives
Φ−1(t,τ) = Φ(τ,t). Finally, when A (t) is actually a constant matrix A, it is not difﬁcult to
show that Φ(t,τ) is precisely the matrix exponential e A(t −τ).
The solution of (1) for zero input can be used to obtain a representation for the
solution of (1) with an arbitrary input signal u (t). Since Φ(t,τ) is invertible for all t and τ,
change variables to z (t) = Φ−1(t, 0)x (t) and rewrite (1) as
z.(t) = bˆ(t)u (t) ,
t ≥0
y (t) = cˆ(t)z (t) ,
z (0) = x 0
(8)
where
bˆ(t) = Φ−1(t, 0)b (t)
cˆ(t) = c (t)Φ(t, 0)
(9)
In the state equation (8), there is no term of the form A (t)z (t), which was the objective of
the variable change. Integrating both sides of the differential equation in (8) gives
z (t) = x 0 +
0∫
t
bˆ(σ)u (σ) dσ
(10)
which becomes, in terms of the original variables,
x (t) = Φ(t, 0)x 0 +
0∫
t
Φ(t,σ)b (σ)u (σ) dσ
(11)
Thus,
95

y (t) = c (t)Φ(t, 0)x 0 +
0∫
t
c (t)Φ(t,σ)b (σ)u (σ) dσ
(12)
For the case where x 0 = 0, the degree-1 homogeneous input/output representation
y (t) =
0∫
t
h (t,σ)u (σ) dσ
(13)
with kernel
h (t,σ) = c (t)Φ(t,σ)b (σ)
(14)
has been obtained. Furthermore, if A (t), b (t), and c (t) actually are constant matrices, then
Φ(t,σ) = e A(t −σ), and (13) becomes a convolution integral with
h (t,σ) = h (t −σ) = ce A(t −σ)b
(15)
I will commence consideration of the nonlinear case by taking this same
resubstitution approach to bilinear state equations. This starting point is appropriate in
part because the class of bilinear state equations was the ﬁrst wide class of nonlinear
equations for which a general form for the kernels was obtained - and in part because the
general form is such a splendid example of mathematical pulchritude. Moreover, it will
become clear in later sections that the treatment of the bilinear case is a precursor to more
general developments.
A bilinear state equation is a vector differential equation of the form
x.(t) = A (t)x (t) + D (t)x (t)u (t) + b (t)u (t)
y (t) = c (t)x (t) , t ≥0 , x (0) = x 0
(16)
where, as before, x (t) is n x 1, while u (t) and y (t) are scalars. Typical assumptions for
(16) are the same as in the linear case. In Problem 3.9, the reader is invited to mimic a
standard successive approximation proof to show that these assumptions guarantee
existence of a unique solution on any ﬁnite time interval.
Using the variable change z (t) = Φ−1(t, 0)x (t), where Φ(t,τ) is the transition matrix
corresponding to A (t), yields a simpliﬁed form of (16):
z.(t) = Dˆ (t)z (t)u (t) + bˆ(t)u (t)
y (t) = cˆ(t)z (t) , t ≥0 , z (0) = z 0
(17)
where
96

bˆ(t) = Φ−1(t, 0)b (t)
Dˆ (t) = Φ−1(t, 0)D (t)Φ(t, 0)
cˆ(t) = c (t)Φ(t, 0)
(18)
Just as in the linear case, the technique to ﬁnd the form of the input/output representation
is to integrate both sides of the differential equation in (17), and then resubstitute for z (t).
The ﬁrst step of the procedure gives
z (t) = z 0 +
0∫
t
Dˆ (σ1)z (σ1)u (σ1) dσ1 +
0∫
t
bˆ(σ1)u (σ1) dσ1
(19)
Substituting for z (σ1) using an expression of this same form,
z (t) = z 0 +
0∫
t
Dˆ (σ1)z 0u (σ1) dσ1
+
0∫
t
0∫
σ1
Dˆ (σ1)Dˆ (σ2)z (σ2)u (σ1)u (σ2) dσ2dσ1
+
0∫
t
0∫
σ1
Dˆ (σ1)bˆ(σ2)u (σ1)u (σ2) dσ2dσ1 +
0∫
t
bˆ(σ1)u (σ1) dσ1
(20)
Substituting for z (σ2) in (20) using an expression of the form (19), and continuing in this
manner yields, after N − 1 steps,
z (t) = z 0 +
k =1Σ
N
0∫
t
0∫
σ1
. . .
0∫
σk −1
Dˆ (σ1) . . . Dˆ (σk)z 0u (σ1) . . . u (σk) dσk . . . dσ1
+
k =1Σ
N
0∫
t
0∫
σ1
. . .
0∫
σk −1
Dˆ (σ1) . . . Dˆ (σk −1)bˆ(σk)u (σ1) . . . u (σk) dσk . . . dσ1
+
0∫
t
0∫
σ1
. . .
0∫
σN−1
Dˆ (σ1) . . . Dˆ (σN)z (σN)u (σ1) . . . u (σN) dσN . . . dσ1
(21)
Actually the notation in (21) is rather poor for the k = 1 terms in the summations. A clearer
expression would be
97

z (t) = z 0 +
0∫
t
Dˆ (σ1)z 0u (σ1) dσ1
+
k =2Σ
N
0∫
t
0∫
σ1
. . .
0∫
σk −1
Dˆ (σ1) . . . Dˆ (σk)z 0u (σ1) . . . u (σk) dσk . . . dσ1
+
0∫
t
bˆ(σ1)u (σ1) dσ1
+
k =2Σ
N
0∫
t
0∫
σ1
. . .
0∫
σk −1
Dˆ (σ1) . . . Dˆ (σk −1)bˆ(σk)u (σ1) . . . u (σk) dσk . . . dσ1
+
0∫
t
0∫
σ1
. . .
0∫
σN−1
Dˆ (σ1) . . . Dˆ (σN)z (σN)u (σ1) . . . u (σN) dσN . . . dσ1
However, for reasons of economy I will continue to use the collapsed version in (21).
Equation (21) is in many ways analogous to (5) in the linear case, and it can be
shown that the last term in (21) approaches 0 in a uniform way on any ﬁnite time interval.
Therefore on any ﬁnite time interval the solution of the bilinear state equation can be
represented by the uniformly convergent (vector) Volterra series:
z (t) = z 0 +
k =1Σ
∞
0∫
t
0∫
σ1
. . .
0∫
σk −1
Dˆ (σ1) . . . Dˆ (σk)z 0u (σ1) . . . u (σk) dσk . . . dσ1
+
k =1Σ
∞
0∫
t
0∫
σ1
. . .
0∫
σk −1
Dˆ (σ1) . . . Dˆ (σk −1)bˆ(σk)u (σ1) . . . u (σk) dσk . . . dσ1
(22)
(Problems 3.12 and 3.13 show cleverly the convergence property of (22).)
Incorporating the output equation and changing back to the original variables gives
the Volterra system representation
y (t) = c (t)Φ(t, 0)x 0 +
k =1Σ
∞
0∫
t
0∫
σ1
. . .
0∫
σk −1
c (t)Φ(t,σ1)D (σ1)Φ(σ1,σ2)D (σ2)
. . . D (σk)Φ(σk,0)x 0u (σ1) . . . u (σk) dσk . . . dσ1
+
k =1Σ
∞
0∫
t
0∫
σ1
. . .
0∫
σk −1
c (t)Φ(t,σ1)D (σ1)Φ(σ1,σ2)D (σ2)
. . . D (σk −1)Φ(σk −1,σk)b (σk)u (σ1) . . . u (σk) dσk . . . dσ1
(23)
which also converges uniformly on any ﬁnite time interval.
98

There are three kinds of terms in (23): those that depend on the initial state alone,
those that depend on the input alone, and those that depend on both. That is, unlike the
linear case, the response is not simply the sum of the forced and unforced responses. If
u (t) = 0 for all t ≥ 0, the bilinear state equation looks like a linear state equation, and the
response has the corresponding familiar form. If x 0 = 0, the input/output behavior is
described by a reasonably simple Volterra system. Finally, if x 0 ≠ 0 is ﬁxed, the
input/output behavior is again described by a Volterra system, but the kernels depend on
the speciﬁc value of x 0.
It should not be too surprising that the input/output behavior of a nonlinear system
depends in a somewhat complicated way on the initial state of the system. With ﬁxed
initial state, (23) is a Volterra system representation with a degree-0 term that is a speciﬁed
time function, and with the kernels of the higher-degree terms completely speciﬁed.
However, it usually is most convenient to introduce a variable change in the bilinear state
equation to allow the choice of zero initial state in the new variables. This will be
discussed more generally in due course, but for now simple examples will show how
variable-change ideas can be implemented.
Example 3.1
The direct method for generating frequency modulated (FM) signals is to
use a voltage controlled oscillator. That is, the frequency of a harmonic oscillator is
changed in accordance with a message signal u (t). The basic differential equation model is
y..(t) + [ω2 + u (t)]y (t) = 0 , t ≥0 , y (0) = 0 , y.(0) = 1
where y (t) is the generated FM signal. This model can be written in the state equation
form by setting
z (t) =
H
A
I y.(t)
y (t) J
A
K
to obtain
z.(t) =
H
A
I −ω2
0
0
1 J
A
K
z (t) + H
I −1
0
0
0 J
K z (t)u (t)
y (t) = [1
0] z (t) ,
z (0) = H
I 1
0 J
K
Now introduce a new state equation description for which the initial state is 0 by
subtracting the zero input response. For u (t) = 0,
99

z (t) = e Atz 0 =
H
A
A
I
−ω sin(ωt)
cos(ωt)
cos(ωt)
ω
1__ sin(ωt) J
A
A
K
H
A
I 1
0 J
A
K
=
H
A
A
I
cos(ωt)
ω
1__ sin(ωt) J
A
A
K
so let
x (t) = z (t) −
H
A
A
I
cos(ωt)
ω
1__ sin(ωt) J
A
A
K
Writing the differential equation in terms of x (t) gives
x.(t) =
H
A
I −ω2
0
0
1 J
A
K
x (t) + H
I −1
0
0
0 J
K x (t)u (t) +
H
A
A
I
ω
−1
___sin (ωt)
0
J
A
A
K
u (t)
y (t) = [ 1
0 ] x (t) + ω
1__ sin(ωt) , x (0) = 0
Applying the result in (23) to this bilinear state equation yields
y (t) = ω
1__ sin(ωt) +
0∫
t
h (t,σ1)u (σ1) dσ1
+
0∫
t
0∫
σ1
h (t,σ1,σ2)u (σ1)u (σ2) dσ2dσ1 + . . .
where the ﬁrst two triangular kernels are
h (t,σ1) =
ω2
−1
___ sin [ω(t−σ1)] sin(ωσ1)δ−1(t −σ1)
h (t,σ1,σ2) =
ω3
1
___ sin [ω(t−σ1)] sin[ω(σ1−σ2)] sin(ωσ2)δ−1(t −σ1)δ−1(σ1−σ2)
Example 3.2
As another illustration of the formulation of bilinear state equation models
and the calculation of kernels, consider the ideal, separately excited, direct-current motor
diagramed in Figure 3.1.
100

va (t)
Rf
Lf
Inductor
Resistor
+
vf (t)
if (t)
Inductor
Resistor
La
Ra
ia (t)
+
T
TL
ω
+ea (t)
 
 
Figure 3.1. An ideal DC motor.
The differential equation description of the ﬁeld circuit is
dt
d
___ if(t) = −Lf
Rf
___ if(t) + Lf
1
___ vf(t)
The basic characteristics of the armature circuit require further explanation. The so-called
generated voltage ea(t) is proportional to the product of the ﬁeld current and the motor
speed:
ea(t) = K if(t) ω(t)
The magnetic torque generated by the motor is similarly proportional to the product of the
ﬁeld and armature currents:
T(t) = K if(t) ia(t)
Thus, the armature circuit is described by
dt
d
___ia(t) = −La
Ra
___ ia(t) −La
K
___ if(t) ω(t) + La
1
___ va(t)
and the mechanical load system is described by
dt
d
___ω(t) = J
K
__ if(t) ia(t) −J
1__ TL
where TL is the mechanical load torque, and J is the moment of inertia.
A simple method for speed control in a DC motor is to keep the armature voltage
constant, va(t) = Va, and control the ﬁeld current if(t) by means of a variable resistor in the
ﬁeld circuit. To represent this scheme in a particular case, suppose the motor load acts as a
damping device. That is, suppose TL = Bω(t), where B is the viscous damping coefﬁcient.
(For example, the motor might be stirring a ﬂuid.) Then with the input u (t) = if(t), output
y (t) = ω(t), and state vector
x (t) =
H
A
I ω(t)
ia(t) J
A
K
the system is described by
101

x.(t) =
H
A
I
0
−Ra/La
−B /J
0
J
A
K
x (t) +
H
A
I K /J
0
0
−K /La J
A
K
x (t)u (t) +
H
A
I
0
Va/La J
A
K
y (t) = [0
1] x (t) , x (0) =
H
A
I ω(0)
ia(0) J
A
K
This bilinear state equation is not quite in the form of (16) because of the constant term on
the right side. To remove this term, let xc(t) be the solution of the differential equation
with x (0) = 0 and u (t) = 0. Then it is readily veriﬁed that
xc(t) =
H
A
A
A
I
0
Ra
Va
___(1 −e
−La
Ra
___t
) J
A
A
A
K
Now let z (t) = x (t) − xc(t), and compute a differential equation for z (t):
z.(t) = x.(t) −x.
c(t)
=
H
A
I
0
−Ra/La
−B /J
0
J
A
K
z (t) +
H
A
I K /J
0
0
−K /La J
A
K
z (t)u (t)
+
H
A
I (KVa/JRa)(1−e −(Ra/La)t)
0
J
A
K
u (t)
y (t) = [0
1] z (t) , z (0) =
H
A
I ω(0)
ia(0) J
A
K
This is a bilinear state equation description in the standard form (16), and the calculation
of the solution via (23) is straightforward. For example, if the initial conditions are 0, then
the ﬁrst three triangular kernels are
102

h 1(t,σ1) = JRa
KVa
_____ e
−J
B
___(t −σ1)
(1 −e
−La
Ra
___σ1
)δ−1(t −σ1)
h 2(t,σ1,σ2) = 0
h 3(t,σ1,σ2,σ3) = −
J 2LaRa
K 3Va
_______ e
−J
B
___(t −σ1)
e
−La
Ra
___(σ1−σ2)
e
−J
B
___(σ2−σ3)
(1 −e
−La
Ra
___σ3
)δ−1(t −σ1)δ−1(σ1−σ2)δ−1(σ2−σ3)
3.2 A Digression on Notation
As more general nonlinear differential equations are considered,
notational
complexities begin to appear. These have to do with functions of several variables and
their power series expansions. The difﬁculties probably are not unfamiliar, but their
resolution here in terms of Kronecker (tensor) products is somewhat uncommon, hence
this digression.
For matrices A = (aij) and B = (bij), of dimension na x ma and nb x mb respectively,
the Kronecker product is deﬁned by
A ⊗B =
H
A
A
I ana1B
...
a 11B
. . .
...
. . .
anamaB
...
a 1maB J
A
A
K
(24)
It is clear that A ⊗B
has dimension nanb x mamb, and that any two matrices are
conformable with respect to this product. The Kronecker product is associative so that
A ⊗B ⊗C is written without ambiguity. The following relationships are easily proved,
assuming conformability with respect to ordinary matrix addition and multiplication.
(A + B)⊗(C + D) = (A ⊗C) + (A ⊗D) + (B ⊗C) + (B ⊗D)
(25)
(AB)⊗(CD) = (A ⊗C)(B ⊗D)
(26)
In fact, these properties can be written in simpler forms since the Kronecker product is
given a higher precedence than matrix addition and multiplication:
(A + B)⊗(C + D) = A ⊗C + A ⊗D + B ⊗C + B ⊗D
(27)
(AB)⊗(CD) = A ⊗C B ⊗D
(28)
Additional properties that are not hard to prove are listed below.
Property 1 The product A ⊗B=0 if and only if A = 0 or B = 0.
103

Property
2
If
A
and
B
are
invertible,
then
A ⊗B
is
invertible
and
(A ⊗B)−1 = A−1 ⊗B−1.
Property 3 If rank A = ra and rank B = rb, then rank A ⊗B = rarb.
The Kronecker product notation will be used for polynomials or power series in
several variables. For example, if f :Rn → Rm, then the power-series expansion of f (x)
about x = 0 is written
f (x) = F0 + F1 x + F2 x ⊗x + F3 x ⊗x ⊗x + . . .
(29)
where each Fi is a coefﬁcient matrix of appropriate dimension, to be speciﬁc, m x n i.
Usually I will simplify the notation somewhat by setting  x (i) = x ⊗ . . . ⊗x (i terms) and
writing
f (x) =
i =0Σ
∞
Fix (i)
(30)
Taking a closer look reveals that there are redundancies hidden in this notation. In
particular x (i) is an n i x 1 vector, but only B
D
i
n +i −1E
G entries are distinct. For example,
writing transposes to save space, if
x = [ x 1 x 2 x 3 ]
(31)
then
x (2) = [ x1
2
x 1x 2 x 1x 3 x 2x 1 x2
2
x 2x 3 x 3x 1 x 3x 2 x3
2 ]
(32)
The redundancy could be eliminated by deleting the repeated entries and using, say, a
lexicographic ordering for the rest. Adopting a square-bracket notation for the result, this
procedure gives
x [2] = [ x1
2
x 1x 2 x 1x 3 x2
2
x 2x 3 x3
2 ]
(33)
For many purposes, this reduced Kronecker product is preferable because the
dimensions are smaller. However, some explicitness is sacriﬁced for the economy of
dimension when general calculations are performed. For example, suppose A is n x n and
y = Ax. Then
y (2) = y ⊗y = (Ax)⊗(Ax)
= A ⊗A x ⊗x
= A(2)x (2)
(34)
While it is apparent that there exists a (smaller dimension) matrix A[2] such that
y [2] = A[2] x [2]
(35)
104

it is difﬁcult to write A[2] in explicit terms of A.
As another example, consider the linear differential equation
x.(t) = Ax (t), x (0) = x 0
(36)
again with A an n x n matrix. After verifying the product rule
dt
d
___ [x (2)(t)] = x.(t)⊗x (t) + x (t)⊗x.(t)
(37)
a differential equation for x (2)(t)can be written in the form
dt
d
___ x (2)(t) = [A ⊗In + In ⊗A ]x (2)(t), x (2)(0) = x0
(2)
(38)
where In is the n x n identity. Although it can be shown that x [2] also satisﬁes a linear
differential equation, and one of lower dimension, there is no apparent way to write the
coefﬁcient matrix explicitly in terms of A. (Incidently, the notation x. (2)(t) is being avoided
for good reason. Notice that (d /dt)[x (2)(t)] is much different from [(d /dt)x (t)](2), and thus
the dot notation tends to ambiguity.)
This differential equation example is of interest for more than just notational
reasons. What has been shown is that if x (t) satisﬁes a linear differential equation, then so
does x (2)(t). Clearly, this argument can be continued to show that x (k)(t) satisﬁes a linear
differential equation, k = 3,4, . . . . A very similar observation provides the key for the
methods to be discussed in Section 3.3.
The result of these considerations is that I will use the Kronecker product notation
for the general developments in this chapter.
However, it is clear that the more
economical notation can be substituted with a concomitant loss in explicitness. Going
further, in simple examples it probably is proﬁtable to abandon both these special
notations and work freestyle.
3.3 The Carleman Linearization Approach
The Carleman linearization method for computing kernels will be considered ﬁrst in
the context of state equations of the form
x.(t) = a (x (t),t) + b (x (t),t)u (t) , t ≥0
y (t) = c (x (t),t) , x (0) = x 0
(40)
where x (t) is the n x 1 state vector and the input u (t) and output y (t) are scalar signals.
One reason for starting with this particular form is that the corresponding kernels do not
contain impulses. This is a direct result of the fact that the input in (40) appears linearly.
Toward the end of the section I will remove this restriction and brieﬂy discuss a more
general case.
Another reason for the form in (40) is that the existence of a convergent Volterra
system representation can be guaranteed under general hypotheses. Suppose the functions
a (x,t), b (x,t), and c (x,t) are analytic in x and continuous in t, in which case (40) is called a
105

linear-analytic state equation.
Then various methods can be used to establish the
following, somewhat loosely stated, result. (A proof using the techniques discussed in
Section 3.4 is given in Appendix 3.1.)
Theorem 3.1
Suppose a solution to the unforced linear-analytic state equation exists for
t ε [0,T]. Then there exists an ε > 0 such that for all inputs satisfying |u (t)| < ε there is a
Volterra system representation for the state equation that converges on [0,T].
It is interesting to compare this with the corresponding result for bilinear state
equations. For linear-analytic state equations, the existence of a convergent Volterra
system representation is guaranteed only for sufﬁciently small input signals, while for
bilinear state equations the input signals need only be bounded.
The ﬁrst step in actually computing the kernels will be to perform some variable
changes to put the state equation into a simpler form. These are not necessary, but they do
make the subsequent derivation less fussy. Incidently, it is not clear that such variable
changes are always a great idea. When dealing with particular problems or examples,
signiﬁcant features can be obscured. But I yield to maintaining simplicity of derivations,
with the remark that the form of the Volterra system representation can be derived without
the variable changes.
The ﬁrst simpliﬁcation is that the function c (x,t) in (40) can be taken to be linear in
x with little loss of generality. Differentiating the output equation under the assumption
that c (x,t) is continuously differentiable in t gives a differential equation for y (t),
y.(t) = [ ∂x
∂
___ c (x,t)]x.(t) + ∂t
∂___ c (x,t)
= [ ∂x
∂
___ c (x,t)][a (x,t) + b (x,t)u (t)] + ∂t
∂___ c (x,t)
(41)
with y (0) = c (x 0,0). Since the right side of (41) has the linear-analytic form, y (t) can be
adjoined to the bottom of x (t) to form a new vector xˆ(t). Then the state equation can be
written in the form
xˆ
.
(t) = aˆ(xˆ(t),t) + bˆ(xˆ(t),t)u (t) , xˆ(0) = xˆ 0
y (t) = cˆ(t)xˆ(t) , t ≥0
(42)
where xˆ(t) is an (n +1) x 1 vector. In this case cˆ(t) = [ 0  . . .  0 1 ].
I also will assume that the solution of the differential equation in (42) with u (t)=0 is
xˆ(t) = 0. To show this entails no loss of generality, suppose that the response for u (t) = 0
is x 0(t). Then setting x
_
(t) = xˆ(t) − x 0(t), (42) can be written in the form
106

x
_.
(t) = xˆ
.
(t) −x.
0(t)
= aˆ(xˆ(t),t) + bˆ(xˆ(t),t)u (t) −aˆ(x 0(t),t)
= aˆ(x
_
(t)+x 0(t),t) + bˆ(x
_
(t)+x 0(t),t)u (t) −aˆ(x 0(t),t)
= a
_
(x
_
(t),t) + b
_
(x
_
(t),t)u (t)
y (t) = cˆ(t)x
_
(t) + cˆ(t)x 0(t), x
_
(0) = 0, t ≥0
with the appropriate deﬁnitions of a
_
(x
_
,t) and b
_
(x
_
,t). Thus, simplifying the notation, state
equations of the form
x.(t) = a (x (t),t) + b (x (t),t)u (t) , t ≥0
y (t) = c (t)x (t) + y 0(t) , x (0) = 0
(43)
will be considered. Here x (t) is an n x 1 state vector, u (t) = 0 implies x (t) = 0 and
y (t) = y 0(t), and a (0,t) = 0 because of the variable change employed.
It should be noted that there is a price to pay for this last variable change. Namely,
the unforced solution x 0(t) must be computed in order to obtain the right side of the
simpliﬁed differential equation in (43). While this might not be a severe problem when the
unforced system is linear in x (t), clearly the computation of x 0(t) in a more general
situation can be arbitrarily difﬁcult.
The goal now is to determine the terms through degree N of a polynomial
input/output expression for (43). That is, to determine an input/output representation of
the form
y (t) = y 0(t) +
k =1Σ
N
−∞∫
∞
h (t,σ1, . . . ,σk) u (σ1) . . . u (σk) dσ1 . . . dσk
(44)
Of course, in general there will be terms of higher degree that have been ignored in (44).
Since the state equation (43) can be represented as a convergent Volterra system (under the
conditions stated earlier), a polynomial truncation of the series will be an accurate
approximation for inputs that are sufﬁciently small.
Actually, the method to be considered for determining the polynomial system
representation generates a polynomial representation for x (t). That is, a set of vector
kernels is determined for an expression of the form
x (t) =
k =1Σ
N
−∞∫
∞
w (t,σ1, . . . ,σk)u (σ1) . . . u (σk) dσ1 . . . dσk
Then, since y (t) is a linear function of x (t), the kernels for the input/output representation
are readily computed.
107

The Carleman linearization method begins with the replacement of the right side of
the state equation (43) by power series representations. Adopting the Kronecker-product
notation, write
a (x,t) = A1(t)x + A2(t)x (2) + . . . + AN(t)x (N) + . . .
b (x,t) = B0(t) + B1(t)x + . . . + BN−1(t)x (N−1) + . . .
(45)
where the terms not shown are of higher degree in x than the terms that are shown. Thus,
(43) is written in the form
x.(t) =
k =1Σ
N
Ak(t)x (k)(t) +
k =0Σ
N−1
Bk(t)x (k)(t)u (t) + . . .
y (t) = c (t)x(t) + y 0(t),
x(0) = 0, t ≥0
(46)
where I have explicitly retained terms through degree N in the expansion of a (x,t), and
terms through degree N −1 in the expansion of b (x,t). That higher-degree terms in these
expansions will not contribute to the ﬁrst N kernels will be seen in due course.
The representation in (46) is an appropriate ﬁrst step because it can be shown that
the output of (46) with the higher-degree terms deleted, call it yˆ(t), for any input u (t),
when compared to the response y (t) of (43) to this same input, satisﬁes
yˆ(t) −y (t)| ≤KβN+1, t ≥0
(47)
where K is a constant, and
β =
t≥0
max |u (t)|
Now consider the responses yˆ(t) and y (t) for inputs of the form αu (t), where α is any real
number. In this situation,
|y^ (t) −y (t)| ≤K |αN+1βN+1
so that the polynomial representations (truncations) for y (t) and yˆ(t) must be identical
through degree N.
To determine the ﬁrst N kernels corresponding to (46), a differential equation is
developed for x (2)(t), dropping from explicit consideration terms of degree greater than N
along the way.
108

dt
d
___[x (2)(t)] = dt
d
___[x (1)(t)⊗x (1)(t)] = x. (1)(t)⊗x (1)(t)+x (1)(t)⊗x. (1)(t)
= [
k =1Σ
N
Ak(t)x (k)(t) +
k =0Σ
N−1
Bk(t)x (k)(t)u (t)]⊗x (1)(t)
+ x (1)(t)⊗[
k =1Σ
N
Ak(t)x (k)(t) +
k =0Σ
N−1
Bk(t)x (k)(t)u (t)] + . . .
=
k =1Σ
N−1
[Ak(t)⊗In + In ⊗Ak(t)] x (k +1)(t)
+
k =0Σ
N−2
[Bk(t)⊗In + In ⊗Bk(t)] x (k +1)(t)u (t) + . . . , x (2)(0) = 0
(48)
Thus x (2)(t) satisﬁes a differential equation that has the same general form as the
differential equation for x (1)(t) in (46).
Continuing in this fashion yields a differential equation for x (j)(t) to degree N of the
form
dt
d
___ [x (j)(t)] =
k =1Σ
N−j +1
Aj,k(t)x (k +j −1)(t) +
k =0Σ
N−j
Bj,k(t)x (k +j −1)(t)u (t) + . . . ,
x (j)(0) = 0, j = 1, . . . ,N
(49)
with the notation deﬁned by A1,k = Ak, and for j > 1,
Aj,k(t) = Ak(t)⊗In ⊗. . . ⊗In + In ⊗Ak(t)⊗In ⊗. . . ⊗In
+ . . . + In ⊗. . . ⊗In ⊗Ak(t)
(There are j −1 Kronecker products in each term, and j terms.) A similar notation is used
for Bj,k(t). Now, the crucial observation is that by setting
x ⊗(t) =
H
A
A
A
I x (N)(t)
...
x (2)(t)
x (1)(t) J
A
A
A
K
I can write the collection of differential equations in (49) as the big bilinear state equation
(dropping some arguments) plus higher-degree terms:
109

dt
d
___x ⊗=
H
A
A
A
A
I
0
...
0
0
A11
0
...
0
A21
A12
. . .
...
. . .
. . .
. . .
AN1
...
A3,N−2
A2,N−1
A1N
J
A
A
A
A
K
x ⊗
+
H
A
A
A
A
I
0
...
0
B20
B11
0
...
B30
B21
B12
. . .
...
. . .
. . .
. . .
BN0
B3,N−3
B2,N−2
B1,N−1
0
...
0
0
0 J
A
A
A
A
K
x ⊗u +
H
A
A
A
A
I
0
...
0
0
B10 J
A
A
A
A
K
u + . . .
y (t) = [c (t) 0 . . . 0] x ⊗(t) + y 0(t) + . . . , x ⊗(0) = 0
(50)
Upon deleting all the higher-degree terms represented by the ellipses, this state equation is
called a truncated Carleman linearization of the linear-analytic state equation in (43). (It
might also be appropriate to call (50) a bilinearization of (43).)
It is straightforward in principle to ﬁnd the degree-N polynomial representation for
the input/output behavior of the bilinear state equation (50). Since the input/output
behavior of (50) agrees with that of (43) through terms of degree N, the polynomial
representation of degree N for (50) is precisely the same as that for (43). Note that this
approach gives all N kernels in triangular form via (22) from Section 3.1.
Example 3.3
A phase-locked loop for the demodulation of FM signals is diagramed in
Figure 3.2. The input is an FM signal
r (t) = sin [ωt + φ1(t)]
where
φ1(t) =
0∫
t
u (σ)dσ
and u (t) is the message (modulating) signal.
Figure 3.2. A phase-locked loop.
The loop ﬁlter is described by the transfer function G (s), and the voltage-controlled
oscillator produces the signal
110
g(t),
Π
r
y
filter
Voltage–controlled
oscillator
v
G(s)

v (t) = 2Kcos [ωt + φ2(t)]
where
φ2(t) =
0∫
t
y (σ)dσ
and y (t) is the output of the phase-locked loop. The output of the multiplier then consists
of two terms: a high-frequency term
Ksin [2ωt + φ1(t) + φ2(t)]
and a low-frequency term
Ksin [φ1(t) −φ2(t)]
Assuming that the loop ﬁlter removes the high-frequency term, the signal e (t) can be
considered to contain only the low-frequency term. That is,
e (t) = Ksin[φ1(t) −φ2(t)] = Ksin [x (t)]
where the phase error signal x (t) is given by
x (t) = φ1(t) −φ2(t) =
0∫
t
u (σ) dσ −
0∫
t
y (σ) dσ
Then the output of the loop ﬁlter, which also is the output of the phase-locked loop, is
y (t) =
0∫
t
g (t −τ)e (τ) dτ
From these relationships a differential-integral equation that describes the phase error is
x.(t) = φ.
1(t) −φ.
2(t) = u (t) −y (t)
= u (t) −
0∫
t
g (t −τ)Ksin [x (τ)] dτ
This equation suggests the model shown in Figure 3.3. When x (t) is zero the loop is said
to be locked, and in this situation φ1(t) = φ2(t), or y (t) = u (t).
Figure 3.3. A nonlinear model for the phase-locked loop.
The difﬁculty in analyzing the model depends chieﬂy on the nature of the loop-ﬁlter
transfer function G (s). For simplicity, I will consider only the so-called ﬁrst-order phase-
111
φ1
Σ
sin(.)
G(s)
g(t),
K
__s
φ2
x
y
–

locked loop, wherein G (s) = 1 (or g (t) = δ0(t)). Then the differential equation description
for the phase error simpliﬁes to
x.(t) = −K sin [x (t)] + u (t)
and if the loop is locked, x (0) = 0. To compute the kernels in this simple case, there is no
need to use the general notation. The differential equation can be replaced by the equation
x.(t) = −Kx (t) + 6
K
__ x 3(t) + u (t) + . . .
where only those terms that contribute to the ﬁrst three kernels have been retained
explicitly. Since x (j)(t) = x j(t) for scalar x (t), let
x ⊗(t) =
H
A
A
I x 3(t)
x 2(t)
x(t) J
A
A
K
Then (50) becomes
dt
d
___ [x ⊗(t)] =
H
A
I
0
0
−K
0
−2K
0
−3K
0
K /6 J
A
K
x ⊗(t)
+
H
A
I 0
2
0
3
0
0
0
0
0 J
A
K
x ⊗(t)u (t) +
H
A
I 0
0
1 J
A
K
u (t) + . . .
x (t) = [ 1 0 0 ] x ⊗(t)
where the phase-error signal is taken to be the output of the state equation. A short
calculation yields
e At =
H
A
A
I
0
0
e −Kt
0
e −2Kt
0
e −3Kt
0
(e −Kt−e −3Kt)/12 J
A
A
K
and from (23) the ﬁrst three triangular kernels are
h (t,σ1) = e −K(t −σ1)δ−1(t−σ1)
h (t,σ1,σ2) = 0
h (t,σ1,σ2,σ3) = 2
1__[e −Kte −Kσ1e Kσ2e Kσ3 −e −3Kte Kσ1e Kσ2e Kσ3]
δ−1(t −σ1)δ−1(σ1−σ2)δ−1(σ2−σ3)
(The unit step functions are there just to emphasize that these are triangular kernels.)
112

Extending the approach of this section to state equations of the form
x.(t) = f (x (t),u (t),t), x (0) = 0
y (t) = h (x (t),t) , t ≥0
(51)
where u (t) and y (t) are scalar signals is not hard. It is messy, to be sure, especially when
worked out in detail, but the mechanics are familiar. A power series form of the equation
is obtained, and then a set of vector kernels describing x (t) is calculated in much the same
way as was done for bilinear state equations. But now the nonlinear dependence of
f (x,u,t) upon u means that the kernels must contain impulses. A transparent case will
show why.
Example 3.4
For the scalar state equation
x.(t) = u 2(t), x (0) = 0
y (t) = x 3(t)
integration directly yields
x (t) =
0∫
t
u 2(σ) dσ
Writing this as a degree-2 homogeneous term
x (t) =
0∫
t
0∫
t
h (t,σ1,σ2)u (σ1)u (σ2) dσ2dσ1
requires the impulsive kernel
h (t,σ1,σ2) = δ0(σ1−σ2)
Thus, the output is given by
y (t) =
0∫
t
δ0(σ1−σ2)δ0(σ3−σ4)δ0(σ5−σ6)u (σ1) . . . u (σ6) dσ6 . . . dσ1
which clearly shows that the system is homogeneous of degree 6.
Returning to the differential equation in (51), I assume that f (0,0,t) = 0, and, as
usual, that f (x,u,t) has properties that sufﬁce to remove worries about existence and
uniqueness of solutions for t ≥ 0. Differentiability sufﬁcient to carry out the following
development will be assumed, and the argument t will be dropped in part because the
calculations are essentially the same for the stationary and nonstationary cases. Using the
Kronecker product notation, the expansion of f (x,u) about x = 0, u = 0 through degree N
can be written in the form
113

f (x,u) = F01u + F02u 2 + F03u 3 + F10x (1) + F11x (1)u + F12x (1)u 2
+ F20x (2) + F21x (2)u + F22x (2)u 2 + . . .
This provides a differential equation for x of the form
x. =
i =0Σ
N
j =0Σ
N
Fijx (i)u j + . . .
(52)
where F00 = 0. The procedure for developing differential equations for x (2), . . . ,x (N) is
just as before. Now, however, the equation for
x ⊗=
H
A
A
A
I x (N)
...
x (2)
x
J
A
A
A
K
will have a number of additional terms:
dt
d
___x ⊗= Fx ⊗+ G1x ⊗u + G2x ⊗u 2 + . . . + GNx ⊗u N−1 + g 1u + . . . + gNu N + . . .(53)
From this point, the idea is to mimic the development in the bilinear case. Using a change
of variables involving the transition matrix for F, and then integrating both sides of the
resulting state equation sets up the iterative resubstitution procedure. Of course, there are
many more terms here, but, at this level of notation, applying the procedure and inserting
impulses to write the homogeneous terms in the right form is straightforward in principle.
Once this has been done, expanding the output equation to degree N,
h (x) = y 0(t) + h 1x + h 2x (2) + . . . + hNx (N) + . . .
= y 0(t) + hx ⊗+ . . .
(54)
leads to a polynomial input/output representation upon deletion of the ellipses. Notice that
this last step requires nothing more complicated than using properties of the Kronecker
product. In particular no additional impulses need be inserted.
Example 3.5
The nonlinear feedback system shown in Figure 3.4 is described by
x.(t) = Ax (t) + bψ[u (t)−y (t)]
y (t) = cx (t) , t ≥0 , x (0) = 0
where the scalar nonlinearity is a polynomial (or power series)
ψ(α) = α + ψ2α2 + ψ3α3 + . . .
114

Figure 3.4. Nonlinear system for Example 3.4
Using the approach just outlined, I will compute the ﬁrst- and second-degree kernels for
the closed-loop system. Corresponding to (53), the terms needed for the ﬁrst two kernels
involve setting
x ⊗=
H
A
A
I x (2)
x J
A
A
K
and replacing the given state equation by a state equation of the form
dt
d
___x ⊗= Fx ⊗+ G1x ⊗u + g 1u + g 2u 2 + . . .
y = hx ⊗
It should be clear that higher-degree terms in x ⊗and u will not be needed. Moreover, it
will turn out that the general notation being employed in the differential equation for x ⊗
carries along terms that are superﬂuous as far as the ﬁrst two kernels are concerned. In
particular, the equation for x ⊗contains terms involving x, x (2), xu, x (2)u, u, and u 2. The
x (2)u terms are not needed and arbitrarily setting the coefﬁcients to 0 can simplify matters.
The differential equation for x can be written out in the form
x. = Ax + b [(u −cx) + ψ2(u −cx)2 + ψ3(u −cx)3 + . . . ]
= (A −bc)x + bu + ψ2bu 2 −2ψ2bcxu + ψ2b (cx)2 + 3ψ3b (cx)2u + . . .
Using the Kronecker product notation to write
(cx)2 = (cx)⊗(cx) = c ⊗cx (2)
and dropping into the dots terms that do not enter into the x ⊗equation, gives
x. = (A −bc)x + bu + ψ2bu 2 −2ψ2bcxu + ψ2bc ⊗cx (2) + 3ψ3bc ⊗cx (2)u + . . .
To develop a differential equation for x (2), the product rule gives
dt
d
___[x (2)] = x. ⊗x + x ⊗x.
= [(A −bc)⊗I + I ⊗(A−bc)]x (2) + [b ⊗I + I ⊗b ]xu
+ ψ2[b ⊗I + I ⊗b ]xu 2 −2ψ2[bc ⊗I + I ⊗bc ]x (2)u + . . .
Again terms that will not contribute to the ﬁnal result have been dropped. Thus the state
equation (53) in terms of x ⊗is
115
Σ
ψ (.)
y
u
ce At b
–

dt
d
___[x ⊗] =
H
A
I
0
A −bc
[(A −bc)⊗I + I ⊗(A −bc)]
ψ2bc ⊗c
J
A
K
x ⊗
+
H
A
I [b ⊗I + I ⊗b ]
−2ψ2bc
−2ψ2[(bc)⊗I + I ⊗(bc)]
3ψ3bc ⊗c
J
A
K
x ⊗u + H
I 0
b J
K u +
H
A
I
0
ψ2b J
A
K
u 2
y = [ c
0 ] x ⊗,
x ⊗(0) = 0
Now the resubstitution procedure can be applied just as was done for bilinear state
equations, with the exception that impulses must be inserted to obtain terms of the correct
form. It is easy to show that in the general notation the ﬁrst two triangular kernels are
given by
h (t,σ) = he F(t −σ)g 1
h (t,σ1,σ2) = he F(t −σ1)G1e F(σ1−σ2)g 1 + he F(t −σ1)g 2δ0(σ1−σ2)
To complete the calculation, these kernels can be expressed in terms of the given
state equation by showing that
e Ft =
H
A
A
A
I
0
e (A−bc)t
e [(A−bc)⊗I+I ⊗(A−bc)]t
0∫
t
e (A−bc)(t −σ)ψ2bc ⊗ce [(A−bc)⊗I+I ⊗(A−bc)]σdσ J
A
A
A
K
Then the ﬁrst two triangular kernels are
h (t,σ) = ce (A−bc)(t −σ)bδ−1(t −σ)
h (t,σ1,σ2) = [−2ψ2e (A−bc)(t −σ1)bce (A−bc)(σ1−σ2)b
+ ψ2
0∫
t −σ1
ce (A−bc)(t −σ1−γ)bc ⊗ce [(A−bc)⊗I+I ⊗(A−bc)]γ dγ (b ⊗I+I ⊗b)e (A−bc)(σ1−σ2)b
+ ψ2ce (A−bc)(t −σ1)bδ0(σ1−σ2)]δ−1(t −σ1)δ−1(σ1−σ2)
As mentioned at the outset, the terms involving x (2)u, in other words, the terms in the
second block column of G1, do not enter the result, and could have been set to zero for
simplicity.
3.4 The Variational Equation Approach
In the variational equation approach, a state-equation description is obtained for
each degree-k homogeneous subsystem in the input/output representation. It turns out that,
although the equation for the degree-k subsystem is coupled nonlinearly to the equations
for the lower-degree subsystems, each of the equations has identical ﬁrst-degree (linear)
116

terms. Thus the various kernels can be computed using the linear-state-equation solution
reviewed in Section 3.1.
As in the previous section, I begin by considering the linear-analytic state equation
x.(t) = a (x (t),t) + b (x (t),t)u (t) , t ≥0
y (t) = c (t)x (t) + y 0(t) , x (0) = 0
(55)
where a (0,t) = 0 so that the response to u (t) = 0 is x (t) = 0, y (t) = y 0(t). The analyticity
assumption can be weakened since only a ﬁnite number of kernels will be computed, but it
is retained here for simplicity.
More general state equations without the special
assumptions on the unforced response are discussed later in the section.
The homogeneous-subsystem state equations are derived by considering the
response of the differential equation in (55) to inputs of the form αu (t), where α is an
arbitrary scalar. The response can be written as an expansion in the parameter α of the
form (In the present context, subscripts do not indicate components of a vector.)
x (t) = αx 1(t) + α2x 2(t) + . . . + αNxN(t) + . . .
(56)
where the dots contain terms of degree greater than N in α. Viewing the analytic functions
a (x,t) and b (x,t) in terms of power series, substituting (56) into (55), and equating
coefﬁcients of like powers of α leads to a differential equation for each xk(t), the degree-k
component of x (t).
Just as in the Carleman linearization approach, the ﬁrst step is to replace the terms in
(55) by power series representations. For ease of exposition, only the calculation of the
ﬁrst three kernels will be treated. Thus the state equation
x.(t) = A1(t)x (1)(t) + A2(t)x (2)(t) + A3(t)x (3)(t)
+ B0(t)u (t) + B1(t)x (1)(t)u (t) + B2(t)x (2)(t)u (t) + . . .
y (t) = c (t)x (1)(t) + y 0(t) , x (1)(0) = 0 , t ≥0
(57)
will be considered. Both the assumed input αu (t) and the assumed response in (56) are
substituted into (57). Note that from the rules of calculation for Kronecker products,
x (2)(t) = [αx 1(t) + α2x 2(t) + . . . ]⊗[αx 1(t) + α2 x 2(t) + . . . ]
= α2x1
(2)(t) + α3[x 1(t)⊗x 2(t) + x 2(t)⊗x 1(t)] + . . .
x (3)(t) = α3x1
(3)(t) + . . .
(58)
where, again, only terms of degree 3 or less are explicitly retained. The terms of higher
degree in x that have been dropped from (58) would not contribute lower-degree terms in
α. That is, substituting (56) into a degree-k function of x yields terms of degree k and
higher in α. Now, (57) can be written in the form
117

αx.
1(t) + α2x.
2(t) + α3x.
3(t) + . . .
= αA1(t)x 1(t) + α2[A1(t)x 2(t)+A2(t)x1
(2)(t)]
+ α3[A1(t)x 3(t)+A2(t)(x 1(t)⊗x 2(t)+x 2(t)⊗x 1(t))+A3(t)x1
(3)(t)]
+ αB0(t)u (t) + α2B1(t)x 1(t)u (t) + α3[B1(t)x 2(t)+B2(t)x1
(2)(t)]u (t) + . . . ,
αx 1(0) + α2x 2(0) + α3x 3(0) + . . . = 0
(59)
Since this differential equation and the equation for the initial state must hold for all α,
coefﬁcients of like powers of α can be equated. This gives the ﬁrst three variational
equations:
x.
1(t) = A1(t)x 1(t) + B0(t)u (t), x 1(0) = 0
x.
2(t) = A1(t)x 2(t) + A2(t)x1
(2)(t) + B1(t)x 1(t)u (t), x 2(0) = 0
x.
3(t) = A1(t)x 3(t) + A2(t)[x 1(t)⊗x 2(t) + x 2(t)⊗x 1(t)]
+ A3(t)x1
(3)(t) + B1(t)x 2(t)u (t) + B2(t)x1
(2)(t)u (t) ,
x 3(0) = 0
(60)
The ﬁrst equation in (60) is the linearized version of the differential equation in (55).
Deﬁning the vector kernel
w (t,σ) = Φ(t,σ)B0(σ)δ−1(t −σ)
(61)
where Φ(t,τ) is the transition matrix for A1(t), yields the representation
x 1(t) =
0∫
t
w(t,σ) u (σ) dσ
(62)
Proceeding to the second equation in (60), the term x1
(2)(t) can be written in the form
x1
(2)(t) = [
0∫
t
w (t,σ)u (σ) dσ]⊗[
0∫
t
w(t,σ)u (σ) dσ]
=
0∫
t
0∫
t
w(t,σ1)⊗w(t,σ2)u (σ1)u (σ2) dσ2dσ1
(63)
Substituting (62) and (63) into the second equation in (60), it is found that this is a linear
differential equation in x 2(t). (It should be clear that this linearity feature is the key to the
method.) Thus
118

x 2(t) =
0∫
t
Φ(t,σ)[A 2(σ)
0∫
σ
0∫
σ
w(σ,σ1)⊗w (σ,σ2)u (σ1)u (σ2) dσ2dσ1
+ B1(σ)
0∫
σ
w(σ,σ1)u(σ1)dσ1u (σ)] dσ
(64)
Using the fact that w (t,σ) = 0 if σ > t, (64) can be written in the form
x 2(t) =
0∫
t
0∫
t
0∫
t
Φ(t,σ)A2(σ)w (σ,σ1)⊗w (σ,σ2)u (σ1)u (σ2) dσdσ2dσ1
+
0∫
t
0∫
t
Φ(t,σ)B1(σ)w(σ,σ1)u(σ1)u (σ) dσdσ1
(65)
Thus the degree-2 component of x (t) is given by
x 2(t) =
0∫
t
0∫
t
w(t,σ1,σ2)u (σ1)u (σ2) dσ2dσ1
(66)
where
w(t,σ1,σ2) =
0∫
t
Φ(t,σ)A2(σ)w (σ,σ1)⊗w (σ,σ2) dσ
+ Φ(t,σ2)B1(σ2)w (σ2,σ1)
=
max [σ1,σ2]
∫
t
Φ(t,σ)A2(σ)Φ(σ,σ1)⊗Φ(σ,σ2) dσB0(σ1)⊗B0(σ2)
+ Φ(t,σ2)B1(σ2)Φ(σ2,σ1)B0(σ1)δ−1(σ2−σ1), 0 ≤σ1,σ2 ≤t
(67)
Of course, the same procedure is used to derive a degree-3 vector kernel that
describes x 3(t). This straightforward but messy calculation is left to the reader. To
determine the degree-3 polynomial representation for the input/output behavior, it is clear
that each of the vector kernels should be multiplied by c (t).
Example 3.6
Revisiting the ﬁrst-order phase-locked loop in Example 3.3 using the
variational equation approach will indicate the mechanics in a more detailed fashion, as
well as contrast the two methods discussed so far. To compute the ﬁrst three kernels, the
starting point is the state equation for the phase error in power series form:
x.(t) = −Kx (t) + 6
K
__x 3(t) + u (t) + . . . ,
x (0) = 0
Substituting the expansion
119

x (t) = αx 1(t) + α2x 2(t) + α3x 3(t) + . . .
into the state equation with the assumed input αu (t) gives the ﬁrst three variational
equations:
x.
1(t) = −Kx 1(t) + u (t) , x 1(0) = 0
~
x.
2(t) = −Kx 2(t) , x 2(0) = 0
x.
3(t) = −Kx 3(t) + 6
K
__x1
3(t) , x 3(0) = 0
Solving the ﬁrst variational equation is a simple matter:
x 1(t) =
0∫
t
e −K(t −σ)u (σ) dσ
Thus, the degree-1 kernel for the system is
h (t,σ) = e −K(t −σ)δ−1(t−σ)
The second variational equation is even simpler, giving x 2(t) = 0 for all t ≥ 0. Thus the
degree-2 kernel is identically 0. The third variational equation gives
x 3(t) =
0∫
t
e −K(t −σ)
6
K
__ x1
3(σ) dσ
Writing this in the standard degree-3 homogeneous form is going to take a little more
work. The ﬁrst step is to write
x1
3(σ) = [
0∫
σ
e −K(σ−σ1)u (σ1) dσ1 ]3
=
0∫
σ
e −K(σ−σ1)e −K(σ−σ2)e −K(σ−σ3)u (σ1)u (σ2)u (σ3) dσ1dσ2dσ3
=
0∫
t
e −K(σ−σ1)e −K(σ−σ2)e −K(σ−σ3)δ−1(σ−σ1)δ−1(σ−σ2)δ−1(σ−σ3)
u (σ1)u (σ2)u (σ3) dσ1dσ2dσ3
Substituting this expression into the expression for x 3(t), and rearranging the order of
integration, gives
120

x 3(t) =
0∫
t
6
K
__e −Kt [
0∫
t
e −2Kσδ−1(σ−σ1)δ−1(σ−σ2)δ−1(σ−σ3) dσ]e K(σ1+σ2+σ3)
u (σ1)u (σ2)u (σ3) dσ1dσ2dσ3
=
0∫
t
12
−1
___[e −3Kte K(σ1+σ2+σ3) −e −Kte −2Kmax (σ1,σ2,σ3)e K(σ1+σ2+σ3)]
u (σ1)u (σ2)u (σ3) dσ1dσ2dσ3
Now a degree-3 kernel for the system is apparent.
Notice, however, that it is not
immediately apparent that this result agrees with that in Example 3.3.
The mechanics of the variational equation approach change little when the most
general state equations are considered. In fact, the special assumptions on the linearity of
the output and on the zero-input response can be relaxed without causing distress. To
illustrate this in some detail, consider the general state equation
x.(t) = f (x (t),u (t),t) , x (0) = x 0
y (t) = h (x (t),u (t),t) , t ≥0
(68)
where u (t) and y (t) are scalars. Suppose that with the ﬁxed initial state and the input uˆ(t),
the response is xˆ(t), yˆ(t). In this setting it is of interest to ﬁnd a polynomial input/output
representation that describes the deviation of the output from yˆ(t), y δ(t) = y (t) − yˆ(t), in
terms of the deviation of the input from uˆ(t), u δ(t) = u (t) − uˆ(t). This means I am
abandoning all the changes of variables previously used to clean up notations. Through
degree N, the right side of the differential equation in (68) can be replaced by (dropping
most t’s)
f (x,u,t) = f (xˆ+x δ,u^+u δ,t)
= f (xˆ,uˆ,t) +
i =0Σ
N
j =0Σ
N
Fij(t)xδ
(i)uδ
j + . . . , F00 = 0
via a Taylor series about xˆ, uˆ. Now consider deviation inputs of the form αu δ(t), where α
is an arbitrary scalar, and assume that the resulting deviation response is expanded in
terms of α:
x δ = αx 1δ + α2x 2δ + . . .
(Note that the α0 term is missing since α = 0 implies the input is uˆ, which implies the
response is xˆ.)
Substituting into the differential equation gives, through degree 3 (again)
121

αx.
1δ + α2x.
2δ + α3x.
3δ + . . . = F10(t)[αx 1δ + α2x 2δ + α3x 3δ]
+ F20(t)[αx 1δ + α2x 2δ](2) + F30(t)[αx 1δ](3)+ αF01(t)u δ
+ α2F02(t)uδ
2 + α3F03(t)uδ
3 + F11(t)[αx 1δ + α2x 2δ]αu δ
+ F12(t)[αx 1δ]α2uδ
2 + F21(t)[αx 1δ](2)αu δ + . . .
Equating coefﬁcients of like powers of α gives the ﬁrst three variational equations listed
below.
x.
1δ = F10(t)x 1δ + F01(t)u δ , x 1δ(0) = 0
x.
2δ = F10(t)x 2δ + F20(t)x1δ
(2) + F02(t)uδ
2 + F11(t)x 1δu δ , x 2δ(0) = 0
x.
3δ = F10(t)x 3δ + F20(t)[x 1δ ⊗x 2δ + x 2δ ⊗x 1δ]
+ F30(t)x1δ
(3) + F03(t)uδ
3 + F11(t)x 2δu δ + F12(t)x 1δuδ
2
+ F21(t)x1δ
(2)u δ , x 3δ(0) = 0
But now the computation of vector kernels for each variation proceeds just as before,
except that an occasional impulse must be inserted to obtain the standard form of a
homogeneous term. Then the process is completed by expanding the output equation,
substituting into that expansion, and regathering terms of like degree with perhaps some
insertion of more impulses since the output map in (68) is permitted to depend on the
input.
Example 3.7
The variational equation approach will be applied to the nonlinear
feedback system of Example 3.5:
x.(t) = Ax (t) + bψ[u (t)−y (t)] , x (0) = 0
y (t) = cx (t) , t ≥0
where
ψ(α) = α + ψ2α2 + ψ3α3 + . . .
and where uˆ(t) = 0, xˆ(t) = 0, and yˆ(t) = 0 for all t ≥ 0. To compute the kernels through
degree 2, the system is written as
x.(t) = Ax (t) + b [u (t)−y (t)] + ψ2b [u (t)−y (t)]2 + . . .
It is appropriate to drop the general notation for this example since it offers no particular
advantage. Assuming the input αu (t) and response
122

x (t) = αx 1(t) + α2x 2(t) + . . .
substituting, and equating coefﬁcients of like powers of α gives the variational equations
x.
1(t) = [A −bc ]x 1(t) + bu (t) , x 1(0) = 0
x.
2(t) = [A −bc ]x 2(t) + ψ2b [u (t)−cx 1(t)]2 , x 2(0) = 0
From the ﬁrst equation,
x 1(t) =
0∫
t
e [A−bc ](t −σ1)bu (σ1) dσ1
and since the output equation is linear, the degree-1 kernel in the polynomial input/output
map is
h(t−σ1) = ce [A−bc ](t−σ1)bδ−1(t −σ1)
The second equation is solved in a similar fashion, although the terms involved are more
complicated.
x 2(t) =
0∫
t
e [A−bc ](t−σ1)ψ2b [u (σ1) −
0∫
σ1
h(σ1−σ2)u (σ2) dσ2]2 dσ1
=
0∫
t
0∫
σ1
ψ2e [A−bc ](t −σ1)bδ0(σ1−σ2)u (σ1)u (σ2) dσ2dσ1
+
0∫
t
0∫
σ1
−2ψ2e [A−bc ](t−σ1)bh(σ1−σ2)u (σ1)u (σ2) dσ2dσ1
+
0∫
t
0∫
σ1
0∫
σ1
ψ2e [A−bc ](t −σ1)bh (σ1−σ2)h (σ1−σ3)u (σ2)u (σ3) dσ2dσ3dσ1
Thus the degree-2 term of the input/output map is
0∫
t
0∫
t
h(t,σ1,σ2)u (σ1)u (σ2) dσ2dσ1
where, with a little relabeling of variables in the last term,
h (t,σ1,σ2) = ψ2h(t −σ1)δ0(σ1−σ2)δ−1(σ1−σ2) −2ψ2h(t −σ1)h (σ1−σ2)
+
0∫
t
ψ2h (t −σ3)h (σ3−σ2)h (σ3−σ1) dσ3
This degree-2 kernel can be written in terms of the given system parameters in the obvious
123

way.
3.5 The Growing Exponential Approach
The properties of growing exponentials discussed in Chapter 2 can be adapted
readily to the problem of ﬁnding transfer function descriptions from constant-parameter
(stationary) state equations. Consider the general form
x.(t) = a (x (t),u (t)) , x (0) = 0
y (t) = cx (t) + y 0(t) , t ≥0
(69)
where a (0,0) = 0 and a (x,u) is analytic in x and u. Brieﬂy stated, the ﬁrst N symmetric
transfer functions corresponding to (69) are computed as follows. First replace a (x,u) by a
power series in x and u. Then assume an input of the form
u (t) = e λ1t + . . . + e λNt
(70)
and assume a solution of the form
x (t) =
mΣ Gm1, . . . ,mN(λ1, . . . ,λN)e (m1λ1 + . . . + mNλN)t
(71)
where the notation is precisely that of Chapter 2, and the vector coefﬁcients are
undetermined. Substituting into the differential equation, solve for
G1,0, . . . ,0(λ1), G1,1,0, . . . ,0(λ1,λ2), . . . , G1, . . . ,1(λ1, . . . ,λN)
by equating coefﬁcients of like exponentials. Then since the output is a linear function of
x,
H1(s) = cG 1,0, . . . ,0(s)
H2sym(s 1,s 2) = 2!
1
___ cG 1,1,0, . . . ,0(s 1,s 2)
...
HNsym(s 1, . . . ,sN) = N !
1
___ cG 1, . . . ,1(s 1, . . . ,sN)
(72)
I should note that considerable savings in labor is realized if exponentials that obviously
will not contribute to the terms of interest are dropped at each stage of the calculation. For
example, no term in (71) with mj > 1 for at least one j need be carried.
Example 3.8
To ﬁnd the ﬁrst three symmetric transfer functions corresponding to the
now familiar state equation description
x.(t) = −Kx (t) + 6
K
__x 3(t) + u (t) + . . .
assume an input of the form
124

u (t) = e λ1t + e λ2t + e λ3t
and, dropping arguments for simplicity, a solution of the form
x (t) = G100 e λ1t + G010e λ2t + G001e λ3t + G200e 2λ1t
+ G020e 2λ2t + G002e 2λ3t + G110e (λ1+λ2)t
+ G101e (λ1+λ3)t + G011e (λ2+λ3)t + G111e (λ1+λ2+λ3)t + . . .
Of course, in this scalar case with the output identical to the state, the G notation could be
replaced by symmetric transfer function notation. Also note that the G200, G020, G002
terms are included just to show in the context of an example that they are superﬂuous. At
any rate, an easy calculation gives
x 3(t) = 6G100G010G001e (λ1+λ2+λ3)t + . . .
Substituting into the differential equation, and equating the coefﬁcients of
e λ1t, e (λ1+λ2)t, e (λ1+λ2+λ3)t
respectively, yields the equations
λ1 G100 + KG 100 = 1
(λ1+λ2) G110 + KG 110 = 0
(λ1+λ2+λ3) G111 + KG 111 = KG 100 G010 G001
Solving these in turn gives
G100(λ1) = λ1+K
1
______
G110(λ1,λ2) = 0
G111(λ1,λ2,λ3) =
λ1+λ2+λ3+K
KG 100G010G001
______________
Using the obvious facts:
G010(λ2) = λ2+K
1
______ , G001(λ3) = λ3+K
1
______
the ﬁrst three symmetric transfer functions are
125

H1(s) = s +K
1
_____
H2sym(s 1,s 2) = 0
H3sym(s 1,s 2,s 3) = (s 1+s 2+s 3+K)(s 1+K)(s 2+K)(s 3+K)
K /6
________________________________
Example 3.9
Consider again the simplest general nonlinear equation of the form (69);
the bilinear state equation
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t) , x (0) = 0
To ﬁnd the ﬁrst two symmetric transfer functions, let
u (t) = e λ1t + e λ2t, λ1,λ2 > 0
and assume that
x (t) = G1,0e λ1t + G0,1e λ2t + G1,1e (λ1+λ2)t + . . .
Substituting into the differential equation and equating the coefﬁcients if e λ1t gives
λ1G1,0 = A G1,0 + b
Solving this linear equation yields
G1,0 = (λ1I−A)−1b
so that the degree-1 transfer function is
H (s) = c (sI −A)−1b
The coefﬁcients of e (λ1+λ2)t are equated in a similar fashion to yield the equation
(λ1+λ2) G1,1 = A G1,1 + D G1,0 + D G0,1
Substituting G1,0 = (λ1I−A)−1b and G0,1 = (λ2I−A)−1b and solving gives
G1,1 = [(λ1+λ2)I−A ]−1 D [(λ1I−A)−1b + (λ2I−A)−1b ]
Thus the degree-2 symmetric transfer function is
H2sym(s 1,s 2) = 2
1__ c[(s 1+s 2)I−A ]−1 D [(s 1I−A)−1b + (s 2I−A)−1b]
Note that a simpler asymmetric version can be written by inspection, namely
H2(s 1,s 2) = c [(s 1+s 2)I−A ]−1 D (s 1I−A)−1b
I leave it to the reader to show that a degree-3 asymmetric transfer function can be written
126

as
H3(s 1,s 2,s 3) = c [(s 1+s 2+s 3)I−A]−1 D [(s 1+s 2)I−A ]−1 D (s 1I−A)−1b
From this, a pattern for the higher-degree transfer functions should be clear.
3.6 Systems Described by N th-Order Differential Equations
Various versions of the methods that have been discussed have appeared in the
literature from time to time. Mostly these have been set up for N th-order, nonlinear,
differential equations in the older literature. Since some problems are described quite
naturally in these terms, I will review the variational expansion method for the equation
y (N)(t) + aN−1(t)y (N−1)(t) + . . . + a 0(t)y (t) +
k =2Σ
K
bk(t)y k(t) = u (t)
(73)
where
y (0) = y (1)(0) = . . . = y (N−1)(0) = 0
(74)
so that the solution for u (t) = 0 is y (t) = 0. Of course, this is a special case, but the ideas
generalize in a transparent fashion.
Consider the response to the input αu (t), where α is a scalar, and write
y (t) =
m=1
Σ
M
αmym(t) + . . .
(75)
where only the terms of degree M or less have been explicitly retained. Substituting into
the differential equation gives
m=1
Σ
M
αm
n =0Σ
N
an(t)ym
(n)(t) +
k =2Σ
K
bk(t)[
m=1
Σ
M
αmym(t)]k + . . . = αu (t)
(76)
where aN(t) = 1, and the initial conditions are
ym
(n)(0) = 0 , n = 0,1, . . . ,N −1 , m = 1,2, . . . ,M
Equating coefﬁcients of α on both sides gives
n =0Σ
N
an(t)y1
(n)(t) = u (t) , y1
(n)(0) = 0 , n = 0, . . . ,N −1
(77)
and the solution of this linear differential equation can be written in the form
y 1(t) =
0∫
t
h 1(t,σ1)u (σ1) dσ1
(78)
Equating coefﬁcients of α2 gives
127

n =0Σ
N
an(t)y2
(n)(t) + b 2(t)y1
2(t) = 0, y1
(n)(0) = 0, n = 0, . . . ,N −1
(79)
The solution of this differential equation can be written in the form
y 2(t) =
0∫
t
h 1(t,σ1)b 2(σ1)y1
2(σ1) dσ1
(80)
To write this in the usual degree-2 homogeneous form requires a substitution for y1
2(σ1):
y 2(t) =
0∫
t
h 1(t,σ1)b 2(σ1)
0∫
σ1
h 1(σ1,σ2)u (σ2) dσ2
0∫
σ1
h 1(σ1,σ3)u (σ3) dσ3dσ1
=
0∫
t
0∫
σ1
0∫
σ1
h 1(t,σ1)b 2(σ2)h 1(σ1,σ2)h 1(σ1,σ3)u(σ2)u (σ3) dσ2dσ3dσ1
Inserting unit step functions so the limits of integration can be raised to t, and relabeling
variables, gives
y 2(t) =
0∫
t
0∫
t
h 2(t,σ1,σ2)u (σ1)u (σ2) dσ1dσ2
(81)
where
h 2(t,σ1,σ2) =
0∫
t
h 1(t,σ)b 2(σ)h 1(σ,σ1)h 1(σ,σ2)δ−1(σ−σ1)δ−1(σ−σ2) dσ
(82)
I can proceed in a similar way to compute the higher-degree kernels in the
(truncated) polynomial input/output representation.
The only obstacle to a general
formulation lies in the nonlinear term in (76). This can be handled by writing
[
m=1
Σ
M
αmym(t)]k =
j =kΣ
Mk
αjyj,k(t)
(83)
and deriving a recursion for the terms yj,k(t), j ≥ k.
Let
f (α) =
m=1
Σ
M
αmym(t)
g (α) = [
m=1
Σ
M
αmym(t)]k −1
(84)
Then, g (α) can be written as
128

g (α) =
j =k −1
Σ
Mk −1
αjyj,k −1(t)
(85)
and
f (α)g (α) =
j =kΣ
Mk
αjyj,k(t)
(86)
To isolate the αj, j ≥ k, terms on both sides of this equation, differentiate both sides j times
with respect to α and set α = 0. Using the product rule
dαj
d j
____[f (α)g (α)] =
i =0Σ
j
(i
j) [
dαi
d i
____ f (α)][
dαj −1
d j −i
______g (α)]
(87)
gives
i =0Σ
j
yi(t)yj −i,k −1(t) = yj,k(t)
(88)
But the lower limit on the sum can be raised to 1 since y 0(t) = 0. Since j ≥ k, and since
nonzero summands correspond to j −i ≥ k −1, the upper limit on the sum can be replaced by
j-k+1. Thus,
yj,k(t) =
i =1Σ
j −k +1
yi(t)yj −i,k −1(t)
(89)
where, for k = 1, yj, 1(t) = yj(t).
Returning now to the problem at hand, equate the coefﬁcients of α3 on both sides of
the equation
m=1
Σ
M
αm
n =0Σ
N
an(t)ym
(n)(t) +
k =2Σ
K
bk(t)
j =kΣ
Mk
αjyj,k(t) + . . . = αu (t) , ym
(n)(0) = 0
(90)
This gives
n =0Σ
N
an(t)y3
(n)(t) + b 2(t)y 3,2(t) + b 3(t)y 3,3(t) = 0
(91)
where all initial conditions are zero. The solution can be written in the form
y 3(t) =
0∫
t
h 1(t,σ1)[b 2(σ1)y 3,2(σ1) + b 3(σ1)y 3,3(σ1)] dσ1
(92)
The recursions just developed yield
y 3,2(σ1) = y 1(σ1)y 2,1(σ1) + y 2(σ1)y 1,1(σ1) = 2y 1(σ1)y 2(σ1)
y 3,3(σ1) = y 1(σ1)y 2,2(σ1) = y1
3(σ1)
(93)
so that
129

y 3(t) =
0∫
t
h 1(t,σ1)[2b 2(σ1)y 1(σ1)y 2(σ1) + b 3(σ1)y1
3(σ1)] dσ1
(94)
Now it is just a matter of substitution for y 1(σ1), y 2(σ2) from (78), (80), and some
manipulation of integrals to put this into the form of a degree-3 homogeneous subsystem.
I should also point out that the growing exponential method can be adapted quite
easily to N th-order differential equations, as the following example shows.
Example 3.10
The simple pendulum consists of a mass m suspended on a massless rod
of length L. The input torque at the pivot is u (t), the damping coefﬁcient at the pivot is a,
and the output y (t) is the angle from the vertical. The well known differential equation
describing this system is
y..(t) +
mL2
a
_____y.(t) + L
g__sin [y (t)] =
mL2
1
_____u (t)
and it is assumed that the initial conditions are zero. To compute the ﬁrst three symmetric
transfer functions by the growing exponential method, the ﬁrst step is to replace sin [y (t)]
by its power series expansion. Of course, only terms through order three need be retained
explicitly, so the differential equation of interest is
y..(t) +
mL2
a
_____y.(t) + L
g__y (t) −3!L
g
____y 3(t) + . . . =
mL2
1
_____u (t)
The growing exponential method can be simpliﬁed in this case by arguing, either
from the physics of the situation or from the differential equation, that y (t) will contain no
homogeneous terms of even degree. That is, if the input signal u (t) produces the output
signal y (t), then the input signal −u (t) produces −y (t), and it follows that only odd-degree
terms can be present.
To calculate the symmetric transfer functions through degree three, assume an input
signal of the form
u (t) = e λ1t + e λ2t + e λ3t , λ1,λ2,λ3 > 0 , t ε (−∞,∞)
Since all degree-2 terms are known to be zero, assume the response
y (t) = H1(λ1)e λ1t + H1(λ2)e λ2t + H1(λ3)e λ3t
+ 3!H3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t + . . .
where, as usual, only terms contributing to the ﬁnal result have been retained. (Notice that
the symmetric transfer function notation, rather than the G-notation, has been used, since
the calculations involve the output directly.) Substituting into the differential equation
gives, again with many terms dropped,
130

λ1
2H1(λ1)e λ1t + (λ1+λ2+λ3)23!H3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t +
mL2
a
_____λ1H1(λ1)e λ1t
+
mL2
a
_____(λ1+λ2+λ3)3!H3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t + L
g__H1(λ1)e λ1t
+ L
g__3!H3sym(λ1,λ2,λ3)e (λ1+λ2+λ3)t
−L
g__H1(λ1)H1(λ2)H1(λ3)e (λ1+λ2+λ3)t + . . .
=
mL2
1
_____e λ1t +
mL2
1
_____e λ2t +
mL2
1
_____e λ3t
Equating coefﬁcients of e λ1t gives
H1(λ1) =
λ1
2 + a /(mL2)λ1 + g /L
1/(mL2)
___________________
Thus, the degree-1 transfer function is
H1(s) =
s 2 + a /(mL2)s + g /L
1/(mL2)
__________________
Equating coefﬁcients of e (λ1+λ2+λ3)t yields
3!H3sym(λ1,λ2,λ3) =
(λ1+λ2+λ3)2+a /(mL2)(λ1+λ2+λ3)+g /L
g /L
__________________________________ H1(λ1)H1(λ2)H1(λ3)
or, in more compact form,
H3sym(s 1,s 2,s 3) =
3!
mgL
_____H1(s 1+s 2+s 3)H1(s 1)H1(s 2)H1(s 3)
3.7 Remarks and References
Remark 3.1
The idea of using resubstitution (sometimes called Peano-Baker) techniques
or successive approximations (often called Picard iterations) is well known in the theory of
differential equations. Perhaps the ﬁrst suggestion that successive approximations be used
to compute kernels was made by J. Barrett in a published discussion appended to the paper
R. Flake, "Volterra Series Representations of Time-Varying Nonlinear
Systems,"
Proceedings of the Second International Congress of IFAC, Butterworths, London, pp.
91-97, 1963.
The possibilities for the successive approximation approach in a more general setting were
131

ﬁrst realized much later in
C. Bruni, G. DiPillo, G. Koch, "On the Mathematical Models of Bilinear Systems,"
Ricerche di Automatica, Vol. 2, pp. 11-26, 1971.
where the general form for the kernels corresponding to a bilinear state vector equation
was ﬁrst derived. The linear-analytic system case was ﬁrst considered in
R. Brockett, "Volterra Series and Geometric Control Theory," Automatica Vol. 12, pp.
167-176, l976 (addendum with E. Gilbert, p. 635).
The conditions for existence of a uniformly convergent Volterra system representation are
derived by combining the successive approximation technique with power series
expansion of the analytic functions in the state equation. A crucial step in computing the
form of the kernels is the use of the Carleman linearization idea to obtain a bilinear state
vector equation that approximates the linear-analytic differential equation. This technique
is discussed in
A. Krener, "Linearization and Bilinearization of Control Systems," Proceedings of the
1974 Allerton Conference, Electrical Engineering Department, University of Illinois,
Urbana-Champaign, Illinois, pp. 834-843, 1974.
This paper contains arguments that justify the bound indicated in (47). For differential
equations that have nonlinear terms in the input, the calculation of the kernels is discussed
in
R. Brockett, "Functional Expansions and Higher Order Necessary Conditions in Optimal
Control," in Mathematical System Theory, G. Marchesini, S. Mitter eds., Lecture Notes in
Economics and Mathematical Systems, Vol. 131, Springer-Verlag, New York, pp. 111-121,
1976.
In these latter three references, the reduced Kronecker product representation is used. The
relationship between successive approximation and the computation of Volterra series is
discussed further in
B. Leon, D. Schaefer, "Volterra Series and Picard Iteration for Nonlinear Circuits and
Systems," IEEE Transactions on Circuits and Systems, Vol. CAS-25, pp. 789-793, 1978.
Remark 3.2
An extensive compilation of properties of the Kronecker product along with
some applications in system theory can be found in
J. Brewer, "Kronecker Products and Matrix Calculus in System Theory," IEEE
Transactions on Circuits and Systems, Vol. CAS-25, pp. 772-781, 1978.
132

Remark 3.3
The variational expansion method has a long and gloried history in the
mathematics of differential equations that are analytic in a parameter. The method was
used by Euler, clariﬁed by Cauchy, and made completely rigorous by the convergence
proofs of Poincare. A detailed treatment of this theory with convergence proofs for results
much like Theorem 3.1 is given in
F. Moulton, Differential Equations, Macmillan, New York, 1930.
Another approach to the variational expansion method for nonlinear systems is discussed
in
E. Gilbert, "Functional Expansions for the Response of Nonlinear Differential Systems,"
IEEE Transactions on Automatic Control, Vol. AC-22, pp. 909-921, 1977.
Therein functional expansions involving homogeneous terms are discussed in general and
compared with functional expansions involving the Frechet differential.
Then the
homogeneous
functional
expansion
is
used
to
obtain
the
variational
equations
corresponding to a given state equation, and these variational equations are solved to
obtain the kernels much as I have done. My use of the Kronecker product notation is an
attempt to make the mechanics of the approach more explicit.
But for rigor and
completeness, consult these references.
Remark 3.4
A number of methods for computing kernels or transfer functions
corresponding to given differential equations have been omitted from this chapter. One
example is the method for computing kernels given in the paper by R. Flake mentioned in
Remark 3.1. Another example is the procedure for computing triangular kernels given in
C. Lesiak, A. Krener, "The Existence and Uniqueness of Volterra Series for Nonlinear
Systems," IEEE Transactions on Automatic Control, Vol. AC-23, pp. 1090-1095, 1978.
A method for calculating transfer functions which is much different from the growing
exponential approach is discussed in
R. Parente, "Nonlinear Differential Equations and Analytic System Theory," SIAM Journal
on Applied Mathematics, Vol. 18, pp. 41-66, 1970.
Remark 3.5
While the methods that have been discussed all solve more or less the same
problem, there are important differences. A notable feature of the Carleman linearization
approach for linear-analytic systems is that the kernels are prescribed in terms of a simple
general form. That this general form involves quantities of very high dimension is clear,
and it seems fair to say that the Carleman linearization method trades dimensionality for
simplicity. Another feature is that the kernels are found in triangular form. This means, for
example, that the Carleman linearization method can be used to ﬁnd the regular kernels via
133

a simple variable change.
An appealing feature of the variational equation method is that the various degree
subsystems are displayed in terms of interlocking differential equations. Also the
dimensions of the quantities involved are much lower than in the Carleman linearization
method. The biggest difﬁculty is the lack of a general form for the kernels. Not only can
the kernels be difﬁcult to compute, they are not triangular, symmetric, or regular.
The growing exponential approach is somewhat different from the others in that the
symmetric transfer functions are obtained. Whether this is an advantage or disadvantage
depends largely on the purpose of computing the input/output representation. The main
advantage of the method seems to be that it is subtlety-free. The computations become
lengthy, perhaps unwieldy, but they are of an extremely simple nature.
Remark 3.6
Elementary discussions of frequency-modulation techniques and the
phase-locked loop demodulation method can be found in many books on communications.
See for example
S. Haykin, Communication Systems, John Wiley, New York, 1978.
Volterra series analysis of the phase-locked loop using, incidentally, the variational
equation method, is discussed in
H. Van Trees, "Functional Techniques for the Analysis of the Nonlinear Behavior of
Phase-Locked Loops," Proceedings of the IEEE, Vol. 52, pp. 894-911, 1964.
The pendulum example is discussed in
R. Parente, "Functional Analysis of Systems Characterized by Nonlinear Differential
Equations," MIT RLE Technical Report No. 444, 1966.
and in Chapter 8 of
M. Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, John Wiley, New
York, 1980.
The basic theory of ideal DC machines is developed in many texts. See for example A.
Fitzgerald, C. Kingsly, Electric Machinery, 2nd ed., McGraw-Hill, New York, 1961.
The report by Parente mentioned above also contains Volterra series analyses of series-
and shunt-wound DC motors. Nonlinear Differential Equations," MIT RLE Technical
Report No. 444, 1966.
134

3.8 Problems
3.1. If A is a 3 x 3 matrix, show how to compute A[2] from A(2).
3.2. Find the ﬁrst three kernels corresponding to the scalar state equation
x.(t) = cos [x (t)] + u (t)
y (t) = x (t), x (0) = 2
π__, t ≥0
by both the Carleman linearization method, and the variational equation method. Find the
ﬁrst three symmetric transfer functions using the growing exponential method.
3.3. Suppose x (t) satisﬁes the differential equation
x.(t) =
H
A
I −a 0
0
−a 1
1 J
A
K
x (t) ,
x (0) = x 0
Find linear differential equations that are satisﬁed by x (2)(t) and x [2](t).
3.4. Write the system described by
x..(t) = u (t), y (t) = x 2(t)
as a bilinear state equation of the form (16).
3.5. Use the growing exponential method to ﬁnd the ﬁrst three symmetric transfer
functions corresponding to the N th-order differential equation:
y (N)(t) + aN−1y (N−1)(t) + . . . + a 0y (t) +
k =2Σ
∞
bky k(t) = u (t)
3.6. Show that for a stationary state equation, the variational equation method implies that
the system can be represented as an interconnection structured system.
3.7. Derive an expression for the kernels of the bilinear state equation using the
variational equation method.
3.8. Consider the bilinear state equation with polynomial output,
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = c 1x (t) + c 2x (2)(t) + . . . + cNx (N)(t)
Show how to rewrite this as a bilinear state equation in the form (16).
135

3.9. Locate a proof of the existence of solutions for linear state equations using the
method of successive approximations. Using the successive approximations deﬁned for
the bilinear state equation,
x.
0(t) = A (t)x 0(t) + b (t)u (t), x 0(0) = 0
x.
j(t) = A (t)xj(t) + D (t)xj −1(t)u (t) + b (t)u (t), xj(0) = 0, j > 0
rewrite the proof to show existence of solutions for the bilinear case.
3.10.
Verify the
Volterra system
representation for bilinear
state
equations
by
differentiating the expression for z (t) in (22) and substituting into the differential equation
in (17).
3.11. Show uniform convergence of the resubstitution procedure in the linear case (see
(5)) by ﬁlling in the following outline. Assume A (t) is continuous on [0,T], and assume
the existence of a unique, continuous solution x (t) on [0,T]. Conclude that A (t) ≤ K 1,
x (t) ≤ K 2 for t ε [0,T]. Show that

0∫
t
A (σ1)
0∫
σ1
A (σ2) . . .
0∫
σn −1
A (σn)x (σn) dσn . . . dσ1≤
n !
K1
nK 2Tn
________
Conclude uniform convergence of the Peano-Baker series for Φ(t, 0) on [0,T].
3.12. Show that the bilinear state equation (16) can be written in the form
x.
1(t) = A1(t)x 1(t) + D1(t)x 1(t)u (t)
y (t) = c 1(t)x 1(t) , x 1(0) = x 10
by deﬁning the state vector according to
x 1(t) = H
I x (t)
1
J
K
Then show that (16) also can be written in the form
z.(t) = D2(t)z (t)u (t)
y (t) = c 2(t)z (t) , z (0) = z 0
3.13. Use Problem 3.12 to establish the form of the Volterra system representation, and the
relevant convergence conditions, for a bilinear state equation using the following device.
Let Φ(t,τ) be the transition matrix for u (t)D2(t) and then write the solution of the system
in terms of the Peano-Baker series for Φ(t,τ).
3.14. Compute the degree-5 symmetric transfer function for the pendulum system in
136

Example 3.8 using the growing exponential method.
3.15. Show that a degree-5 transfer function for the ﬁrst-order phase-locked loop is
H (s 1, . . . ,s 5) = H1(s 1+ . . . +s 5)[ 2
K
__H1(s 1)H1(s 2)H3sym(s 3,s 4,s 5)
−5!
K
___H1(s 1) . . . H1(s 5)]
3.16. From the general expression for the Volterra system representation of a bilinear state
equation with constant coefﬁcient matrices, derive a closed-form expression for the
solution when A and D commute.
APPENDIX
3.1
Convergence
of
the
Volterra
Series
Representation for Linear-Analytic State Equations
A sufﬁcient
condition
for the
existence
of a
convergent
Volterra
system
representation for a system described by a linear-analytic state equation is stated in
Theorem 3.1 at the beginning of Section 3.3. The purpose of this appendix is to give a
detailed sketch of a proof of that theorem, and to point out that the proof also yields an
interesting alternative statement of the convergence conditions. The proof uses the
variational equation approach given in Section 3.4, although this is known in mathematics
as the Poincare expansion.  1
I will begin by considering an analytic differential equation containing a real
parameter α
x.(t) = f (x (t),t) + αg (x (t),t) , t ≥0 , x (0) = x 0
(1)
The assumptions are that f (x,t) and g (x,t) are n x 1 analytic functions of x and continuous
functions of t on Rn × [0,∞). (Local versions of these assumptions can be used with no
essential complications, but with some loss in simplicity of exposition. Speciﬁcally, it can
be assumed that f (x,t) and g (x,t) are analytic for x in some neighborhood of the solution
of (1) when α = 0.) Assume that for α = 0 the differential equation has a solution deﬁned
for t ε [0,T]. Then the variable-change argument at the beginning of Section 3.3 can be
applied, and it sufﬁces to consider the case where x (0) = 0, and where the solution for
α = 0 is x (t) = 0 for all t ε [0,T]. Thus it can be assumed that
________________
1
A very detailed, rigorous, and complete treatment of the Poincare expansion is given in Chapter 3 of the
book:
F. Moulton, Differential Equations, Macmillan, New York, 1930. A somewhat less detailed
exposition is given in Chapter 5 of the book: T. Davies, E. James, Nonlinear Differential Equations,
Addison-Wesley, New York, 1966.
137

f (0,t) = 0 , t ε [0,T]
(2)
Following the method in Section 3.4, an expansion in terms of the parameter α (the
parenthetical subscripts are used here to distinguish terms in the expansion from
components of a vector),
x (t) = αx (1)(t) + α2x (2)(t) + . . .
(3)
can be computed for the solution of the differential equation. The major part of this
appendix is devoted to establishing the convergence of such an expansion. From this
convergence result, the existence of a convergent Volterra series representation for x (t)
will follow in a straightforward fashion.
Suppose that the n component functions fj(x,t) of f (x,t) and gj(x,t) of g (x,t) are
represented by their power series expansions in x about x = 0.
Then each scalar
component of the right side of the vector differential equation (1) can be viewed as a
power series in x and α, although only terms in α0 and α occur, that converges for
α ≤ K, |xj ≤ r, j = 1,2, . . . ,n.
Here K is any positive number, and r is some
sufﬁciently small positive number that is taken to be independent of j without loss of
generality.
Furthermore, a number M can be found such that whenever
|xj ≤ r,
j = 1, . . . ,n, and t ε [0,T],
| fj(x,t)| ≤M, |gj(x,t)| ≤K
M
___ ,
j = 1, . . . ,n
(4)
This implies, by the Cauchy bounds for analytic functions, that the various partial
derivatives of fj(x,t) and gj(x,t) at x = 0 are bounded as follows. Using the superscript
(i 1, . . . ,in) to denote
∂x1
i 1 . . . ∂xn
in
∂i 1+ . . . +in
___________
(5)
then, for t ε [0,T],
| fj
(i 1, . . . ,in)(0,t)| ≤
r i 1+ . . . +in
i 1! . . . in!M
___________ ,
i 1+ . . . +in = 0,1,2, . . .
|gj
(i 1, . . . ,in)(0,t)| ≤
r i 1+ . . . +in
i 1! . . . in!M /K
_____________ ,
i 1+ . . . +in = 0,1,2, . . .
(6)
where each ij is a nonnegative integer.
The bounds (6) can be used to select a dominating function as follows. Consider the
real-valued function
φ(x,α) = K 1r 1
M
_____
(r 1K 1−K 1x 1−. . . −K 1xn−r 1α)
(K 1x 1+ . . . +K 1xn+r 1α) (r 1K 1+K 1x 1+ . . . +K 1xn+r 1α)
________________________________________________
(7)
where r 1 < r and K 1 < K are positive numbers. This is an analytic function of x and α that
can be represented by its Taylor series expansion about x = 0, α = 0. The series will
138

converge for, say, |xj < r 1/2n, j = 1, . . . ,n, and α < K 1.
Moreover, a simple
calculation using (6) shows
φ(i 1, . . . ,in)(0,0) ≥| fj
(i 1, . . . ,in)(0,t)| , t ε [0,T]
[ ∂α
∂
___φ](i 1, . . . ,in)(0,0) ≥|gj
(i 1, . . . ,in)(0,t)| , t ε [0,T]
(8)
Thus every coefﬁcient in the power series expansion of φ(x,α) about x = 0, α = 0 is no less
than the absolute value of the corresponding coefﬁcient in the power series expansion of
fj(x,t) + αgj(x,t), for t ε [0,T].
Now consider the n x 1 vector differential equation
z.(t) =
H
A
I φ(z (t),α)
...
φ(z (t),α) J
A
K
,
z (0) = 0
(9)
and suppose the solution can be expressed as a convergent Taylor series in α. Then, going
through the procedure in Section 3.4 gives a method for calculating the terms in the
expansion
z (t) = αz (1)(t) + α2z (2)(t) + . . .
(10)
The coefﬁcients in the differential equations for each component of z (j)(t) in (10) are given
by the coefﬁcients in the power series expansion of φ(z,α), while the coefﬁcients in the
differential equations for each component of x (j)(t) in (3) are given by the coefﬁcients in
the power series expansions of fj(x,t) and gj(x,t). It is straightforward to show from a
comparison of these differential equations using (8) that each component of x (j)(t) is
bounded in absolute value by the corresponding component of z (j)(t) for t ε [0,T]. 2 Thus,
convergence of (10) for some range of α implies convergence of (3) for the same range of
α.
To ﬁnd the radius of convergence of (10), I will carry through the details concerning
the solution of (9) and then consider the expansion of this solution in a power series in α.
The solution z (t) will have all components identical so that the change of variables
z 1(t) = . . . = zn(t) = n
r 1
___ [w (t) −K 1
α
____]
(11)
can be performed. That is,
w (t) = r 1
n
___ zj(t) + K 1
α
____ ,
j = 1, . . . ,n
(12)
Then the new variable w (t) satisﬁes the scalar differential equation
________________
2
This is shown in detail for the case n = 2 in Davies and James’s Nonlinear Differential Equations.
139

w. (t) = r 1
nM
____ w (t)[1 + w (t)][1 −w (t)]−1, w (0) = K 1
α
____
(13)
The scalar equation can be solved by separation of variables, and the solution is given by
w (t) =
2α/K 1
1 −α/K 1
________ e
r1
−nM
_____t
[1 + K 1
α
____ −[(1+ K 1
α
____)2 −K 1
4α
____ e r1
nM
____t
]1/2]
(14)
This solution can be expanded in a power series in α about α = 0, and the radius of
convergence is determined by the requirement that
(1+ K 1
α
____)2 −K 1
4α
____ e r1
nM
____t
≥0
(15)
Adding
4e
r1
2nM
_____t
−4e r1
nM
____t
to both sides gives
[1 + K 1
α
____ −2e r1
nM
____t
]2 ≥4e
r1
2nM
_____t
−4e r1
nM
____t
from which
K 1
α
____ ≤−1 + 2e r1
nM
____t
−2(e
r1
2nM
_____t
−e r1
nM
____t
)1/2
Further manipulation yields
K 1
α
____ ≤
1 + (1 −e
r1
−nM
_____t
)1/2
1 −(1 −e
r1
−nM
_____t
)1/2
________________
(16)
This condition gives that the solution of the scalar differential equation (13), and thus the
solution of the vector differential equation (9), can be expanded in a power series in α that
converges for
α< K 1
1 + [1−e
r1
−nM
_____t
]
1 −[1−e
r1
−nM
_____t
]
_____________
(17)
Of course, since the series expansion of the solution is unique, (17) gives a condition for
convergence of (10).
The condition (17) for convergence of the variational expansion can be viewed in
two ways. For the speciﬁed time interval [0,T], (17) gives that the expansion (3) will
140

converge if α is sufﬁciently small. On the other hand, for a speciﬁed value of α, (17) gives
that the expansion will converge for T sufﬁciently small. These two interpretations can be
carried through the following application to linear-analytic state equations, although I
shall explicitly deal only with the ﬁrst.
Now consider a linear-analytic state equation
x.(t) = a (x (t),t) + b (x (t),t)u (t) , x (0) = x 0
(18)
Suppose that the class of input signals is composed of continuous functions u (t) for
t ε [0,T], that satisfy |u (t)| ≤ α. For any such input, setting
f (x,t) = a (x,t) ,   αg (x,t) = b (x,t)u (t) 
(19)
permits application of the convergence result to conclude that the variational expansion
for (18) converges for t ε [0,T] so long as α is sufﬁciently small. Following the process
given in Section 3.4 for replacing the k th term in the variational expansion by a degree-k
homogeneous integral representation completes the argument needed to prove:
Theorem A3.1
Suppose a solution to the unforced linear-analytic state equation (18)
exists for  t ε [0,T]. Then there exists an α > 0 such that for all continuous input signals
satisfying |u (t)| < α,  t ε [0,T], there is a Volterra series representation for the solution of
the state equation
141

CHAPTER 4
REALIZATION THEORY
The realization problem for a given input/output representation can be viewed as the
reverse of the problem considered in Chapter 3. That is, realization theory deals with
computing and characterizing the properties of state-equation representations that
correspond to a speciﬁed homogeneous, polynomial, or Volterra system. Of course, the
speciﬁed system is assumed to be described in terms of a set of kernels or transfer
functions. In particular, most of the discussion here will be for stationary systems
described by the regular kernel or regular transfer function representation.
After a review of linear realization theory, realizability conditions and procedures
for computing bilinear state equation realizations will be discussed for stationary
homogeneous systems.
Then stationary polynomial and Volterra systems will be
addressed. Following a discussion of structural properties of bilinear state equations,
realizability conditions for nonstationary systems in terms of (nonstationary) bilinear state
equations
are
considered.
Throughout
the
development,
only
ﬁnite-dimensional
realizations are of interest - inﬁnite-dimensional realizations are ruled out of bounds.
Furthermore, emphasis is placed on the construction and properties of minimal-dimension
bilinear realizations.
4.1 Linear Realization Theory
The basic realization problem in linear system theory can be stated as follows.
Given a linear-system transfer function H (s), ﬁnd a ﬁnite-dimensional linear state
equation, called a linear realization in this context, that has H (s) as its transfer function.
The linear state equations of interest take the form
x.(t) = Ax (t) + bu (t) ,
t ≥0
y (t) = cx (t) ,
x (0) = 0
(1)
142

where x (t) is the m × 1 state vector, for each t an element of the state space Rm, and u (t)
and y (t) are scalars. A direct transmission term, du (t), can be added to the output equation
without changing the basic development, but that will not be done here. For economy, the
linear state equation (1) will be denoted by (A,b,c,Rm).
It is natural to consider the realization problem in two parts. First, ﬁnd necessary and
sufﬁcient conditions on H (s) for linear realizability. That is, ﬁnd conditions such that a
linear realization (ﬁnite dimension) exists for the given system. Second, for a linear
realizable system ﬁnd a method for computing A, b, and c. Usually it is of interest to ﬁnd a
minimal linear realization; a realization with dimension m as small as possible.
The linear-realizability question is very simple, as the reader is no doubt aware. It is
clear that strictly-proper rationality of the transfer function H (s) is a necessary condition
for linear realizability of the system, since the transfer function for (1) is the strictly proper
rational function c (sI − A)−1b. This condition also is sufﬁcient, as can be shown by using
well known forms of the state equation (1), which can be written by inspection from the
coefﬁcients of H (s). While this familiar development could be pursued to the construction
of minimal linear realizations, I will review a different approach, one that extends more
easily to the nonlinear case. In fact, because of the similarity of ideas in the linear and
nonlinear realization theories, the review of the linear case will be more detailed than
usual.
Using the well known series expansion
(sI −A)−1 = Is−1 + As −2 + A2s −3 + . . .
(2)
the transfer function of the linear state equation (1) can be written as a negative power
series
c (sI −A)−1b = cbs −1 + cAbs −2 + cA 2bs −3 + . . .
(3)
(For simplicity of notation, I leave the dimension of identity matrices to be ﬁxed by
conformability requirements.) This makes clear the fact that for linear realizability it
sufﬁces to consider only those transfer functions H (s) that can be represented by a
negative power series of the form
H (s) = h 0s −1 + h 1s −2 + h 2s −3 + . . .
(4)
In other words, only transfer functions that are analytic at inﬁnity and that have a zero at
inﬁnity need be considered.
Comparison of (3) and (4) shows that, from the series
viewpoint, the basic mathematical problem in linear realization theory involves ﬁnding
matrices A, b, and c, of dimensions m × m, m × 1, and 1 × m such that
cA jb = hj ,
j = 0,1,2, . . .
(5)
The ﬁrst step in solving this basic problem will be to construct a particularly simple
abstract realization. That is, a realization wherein A, b, and c are speciﬁed as linear
operators involving a specially chosen linear space as the state space. Then matrix
representations can be computed for these linear operators when the state space is replaced
by Rm.
143

Suppose V (s) is any negative power series of the form
V (s) = v 0s −1 + v 1s −2 + v 2s −3 + . . .
(6)
A shift operator S is deﬁned according to
SV (s) = v 1s −1 + v 2s −2 + v 3s −3 + . . .
(7)
In words, the action of the shift operator is to slide the coefﬁcients of the series one
position to the left while dropping the original left-most coefﬁcient. Clearly SV (s) is a
negative power series so that the shift operator can be applied repeatedly. The usual
notation S jV (s) is used to denote j applications of S.
Using the shift operator and a given transfer function H (s), a linear space of
negative power series over the real ﬁeld, with the usual deﬁnitions of addition and scalar
multiplication, can be speciﬁed as follows. Let
U = span { H (s), SH (s), S 2H (s), . . . }
(8)
Clearly the shift operator is a linear operator on U, S :U → U.
Now deﬁne the
initialization operator L:R → U to be the linear operator speciﬁed by the given transfer
function (viewed as a series), so that for any real number r,
Lr = H (s)r
(9)
Finally, deﬁne the evaluation operator E :U → R by
EV (s) = E (v 0s −1 + v 1s −2 + . . . ) = v 0
(10)
where V (s) is any element of U.
I should point out that when H (s) is viewed as the function given by the sum of the
negative power series, the shift operator and the space U can be reinterpreted. Indeed,
when V (s) is a function corresponding to a negative power series in s,
SV (s) = sV (s) −[sV (s)]s =∞
EV (s) = [sV (s)]s =∞
(11)
and U becomes a linear space of functions of s. Although the negative power series
representation is often most convenient to demonstrate properties and prove results, the
interpretation in (11) is usually better for the examples and problems in the sequel.
It is very simple to demonstrate that the linear operators S, L, and E form an abstract
realization on the linear space U. This is called the shift realization, and it is written as
(S,L,E,U). The veriﬁcation of the realization involves nothing more than the calculations:
144

ES 0L = EH (s) = E (h 0s −1 + h 1s −2 + . . . ) = h 0
ESL = ESH (s) = E (h 1s −1 + h 2s −2 + . . . ) = h 1
ES 2L = ES (h 1s −1 + h 2s −2 + . . . )
= E (h 2s −1 + h 3s −2 + . . . ) = h 2
(12)
and so on. The appropriate interpretation here is that each constant hj represents a linear
operator, hj:R → R, which is given by the composition of linear operators E, S j, and L.
To ﬁnd a concrete (unabstract) realization of the form (1), it remains to replace U by
a linear space Rm, and to ﬁnd matrix representations for the operators S, L, and E with
respect to this replacement. There are many ways to do this, each of which gives a
different matrix structure for the realization. But central to the replacement is the following
result, the proof of which exhibits one particular construction of a realization.
Theorem 4.1
A linear system described by the transfer function H (s) is linear realizable
if and only if U is ﬁnite dimensional. Furthermore, if the system is linear realizable, then
(S,L,E,U) is a minimal linear realization.
Proof
Suppose H (s) is linear realizable, and that (A,b,c,Rm) is any linear realization
of H (s). Letting W be the linear space of all negative power series, deﬁne a linear operator
Φ:R m → W according to
Φ(x) = cxs −1 + cAxs −2 + cA 2xs −3 + . . .
Clearly H (s) ε R [Φ] since
Φ(b) = cbs −1 + cAbs −2 + cA 2bs −3 + . . .
Also, from
Φ(A jb) = cA jbs −1 + cA j +1bs −2 + cA j +2bs −3 + . . .
= S jH (s)
it is clear that S jH (s) ε R [Φ]. Thus U ⊂ R [Φ], and it follows that dimension U ≤ m since
Φ is a linear operator on an m-dimensional space. Moreover, this argument shows that the
dimension of U is less than or equal to the state-space dimension of any linear realization
of H (s). Thus (S,L,E,U) is a minimal-dimension linear realization.
Now suppose that U has ﬁnite dimension m. Then Problem 4.3 shows that
H (s), SH (s), S 2H (s) , . . . , S m−1H (s)
is a basis for U, and thus U can be replaced by Rm by choosing the standard ordered basis
vectors ej ε Rm according to
145

e 1 = H (s), e 2 = SH (s), . . . , em = S m−1H (s)
Writing S mH (s) as a linear combination of H (s), SH (s),  . . . , S m−1H (s), say
S mH (s) =
j =0Σ
m−1
rm−1−jS jH (s)
the shift operator can be represented on Rm by the m × m matrix
A =
H
A
A
A
A
I 0
...
0
1
0
0
...
1
0
0
. . .
...
. . .
. . .
. . .
1
...
0
0
0
r 0
...
rm−3
rm−2
rm−1 J
A
A
A
A
K
Viewing the initialization operator as L:R → Rm, it is clear that L can be represented by
the m × 1 vector corresponding to H (s):
b = e 1 =
H
A
A
I 0
...
0
1 J
A
A
K
Finally, the evaluation operator can be viewed as E :R m → R, and a matrix representation
can be computed as follows. By the correspondence between U and R m, it is clear that
Eej +1 = hj for j = 0,1,2, . . . ,m −1, and thus E is represented by
c = [ h 0 h 1 . . . hm−1 ]
This construction of a realization completes the proof.
Before presenting an example, it is appropriate to use the shift realization
formulation to recover the well known result on rationality mentioned at the beginning of
this section. There are more direct proofs, but the one given here should clarify the nature
of the linear space U.
Theorem 4.2
A linear system described by the transfer function H (s) is linear realizable
if and only if H (s) is a strictly proper rational function.
Proof
If H (s) is the strictly proper rational function
H (s) =
s m + am−1s m−1 + . . . + a 0
bm−1s m−1 + bm−2s m−2 + . . . + b 0
_____________________________
then, from (11),
146

SH (s) =
s m + am−1s m−1 + . . . + a 0
bm−1s m + bm−2s m−1 + . . . + b 0s
____________________________ −bm−1
=
s m + am−1s m−1 + . . . + a 0
(bm−2−bm−1am−1)s m−1 + (bm−3−bm−1am−2)s m−2 + . . . + (−bm−1a 0)
________________________________________________________
Similarly,
S 2H (s) =
s m + am−1s m−1 + . . . + a 0
(bm−3−bm−1am−2−bm−2am−1+bm−1am−1
2
)s m−1 + . . . + (−bm−2a 0+bm−1am−1a 0)
________________________________________________________________
It should be clear from just these two calculations that for any j ≥ 0, S jH (s) is a strictly
proper rational function with the same denominator as H (s). Only the numerator
polynomial changes with application of the shift operator. Thus every element in U can be
viewed as a rational function which is strictly proper with the same denominator. Only the
numerator polynomial can differ from element to element. Since polynomials of degree at
most m − 1 form a linear space of dimension at most m, it follows that dimension U ≤ m,
and thus that H (s) is linear realizable. The proof of the converse, as mentioned at the
beginning of the section, follows by calculation of the transfer function for a linear state
equation.
Example 4.1
For the strictly proper rational transfer function
H (s) =
s 3+4s 2+5s+2
4s 2+7s +3
_____________
simple calculations give
SH (s) =
s 3+4s 2+5s+2
4s 3+7s 2+3s
_____________ −4 =
s 3+4s 2+5s +2
−9s 2−17s −8
_____________
and
S 2H (s) =
s 3+4s 2+5s +2
−9s 3−17s 2−8s
_____________ + 9 =
s 3+4s 2+5s+2
19s 2+37s +18
_____________
It is clear that H (s) and SH (s) are linearly independent in U, but one more calculation
shows that
S 2H (s) = −3SH (s) −2H (s)
Thus U can be replaced by R2 by choosing the standard ordered basis elements according
to
H
I 0
1 J
K = H (s),
H
I 1
0 J
K = SH (s)
A matrix representation for the initialization operator on this basis clearly is
147

b = H
I 0
1 J
K
Also, since the matrix representation for the shift operator must satisfy
A H
I 0
1 J
K = H
I 1
0 J
K ,
A H
I 1
0 J
K = H
I −3
−2 J
K
it follows that
A = H
I 1
0
−3
−2 J
K
Finally, since EH (s) = 4 and ESH (s) = −9, a matrix representation for the evaluation
operator is
c = [ 4 −9 ]
That a dimension-2 realization has been obtained for a degree-3 transfer function can be
explained by factoring the numerator and denominator of H(s) to write
H (s) =
(s +2)(s +1)2
(4s+3)(s +1)
___________
The linear independence calculations involved in constructing the shift realization
"automatically" canceled the common factor in the numerator and denominator.
The realization theory just presented can be rephrased to yield a well known rank
condition test for realizability. Viewing U as a linear space of negative power series, each
element
S jH (s) = hjs −1 + hj +1s −2 + hj +2s −3 + . . . ,
j = 0,1,2, . . .
(13)
can be replaced by the corresponding sequence of coefﬁcients
( hj, hj +1, hj +2, . . . ) ,
j = 0,1,2, . . .
(14)
Then it is clear that U is ﬁnite dimensional if and only if only a ﬁnite number of these
sequences are linearly independent.
Arranging this idea in an orderly fashion, and
including the fact that the dimension of U is the dimension of the minimal linear
realizations of H (s), gives a familiar result in linear system theory.
Theorem 4.3
A linear system described by the transfer function H (s) in (4) is linear
realizable if and only if the Behavior matrix
BH =
H
A
A
A
I
...
h 2
h 1
h 0
...
h 3
h 2
h 1
...
h 4
h 3
h 2
...
. . .
. . .
. . . J
A
A
A
K
(15)
has ﬁnite rank. Furthermore, for a linear-realizable system the rank of BH is the dimension
of the minimal linear realizations of H (s).
148

I could go on from here to outline the construction of minimal realizations directly
from the Behavior matrix. But this is intended to be a brief review, so all of that will be
left to the references. I will also omit reviewing the equivalence properties of minimal
linear realizations of a given H (s), and the connections with the concepts of reachability
and observability. These topics will arise in Section 4.4 in conjunction with bilinear
realizations, and taking D = 0 in that material captures most of the linear theory here being
skipped.
However, before leaving the topic of stationary linear realization theory, I should
point out that little changes if the starting point is a given kernel, instead of a given
transfer function. Since a strictly proper rational transfer function corresponds precisely to
a kernel of the exponential form
h (t) =
i =1Σ
m
j =1Σ
σi
aijt j −1e λit ,
t ≥0
(16)
it is clear that h (t) is realizable by a linear state equation if and only if it has the form (16).
To proceed via the shift realization approach, note that a given kernel can be assumed to
be analytic for t ≥ 0, for otherwise it is clear that it cannot be realizable by a linear state
equation. Expanding h (t) in a power series about t = 0, the Laplace transform of h (t) can
be written in the form
L[h (t)] =
0∫
∞
h (t)e −st dt
= h (0)
0∫
∞
e −st dt + h (1)(0)
0∫
∞
1!
t___e −st dt + h (2)(0)
0∫
∞
2!
t 2
___e −st dt + . . .
= h (0)s −1 + h (1)(0)s −2 + h (2)(0)s −3 + . . .
(17)
where
h (j)(0) =
dt j
d j
____ h (t)t =0
Thus, the entries in (4) are speciﬁed by the derivatives of the kernel evaluated at 0,
hj = h (j)(0). From this point, construction of the shift realization proceeds just as before.
For multi-input, multi-output linear systems, the theory of realization becomes more
subtle. Although only single-input, single-output systems are under consideration in this
book, the basic linear realizability result for the multivariable case will arise as a technical
consideration in Section 4.2. Therefore, a brief comment is appropriate.
Consider the linear state equation
x.(t) = Ax (t) + Bu (t) ,
t ≥0
y (t) = Cx (t) ,
x (0) = 0
(18)
where x (t) is m × 1, the input u (t) is an r × 1 vector, and y (t) is a q × 1 vector. The
149

corresponding transfer function is the q × r matrix
H (s) = C (sI −A)−1B
(19)
Theorem 4.4
Suppose a linear system is described by the q × r transfer function matrix
H (s). Then the system is realizable by a ﬁnite-dimensional linear state equation of the
form (18) if and only if H (s) is a strictly proper rational matrix. That is, if and only if each
element Hij(s) of H (s) is a strictly proper rational function.
The necessity portion of Theorem 4.4 is clear upon writing (sI − A)−1 in the classical
adjoint over determinant form. Sufﬁciency is equally easy: each strictly proper, rational
Hij(s) can be realized by a state equation of the form (1), and then all these state equations
can be combined to give an r-input, q-output state equation of the form (18). It is in the
question of minimal-dimension realizations that things get more difﬁcult, but these issues
will not arise in the sequel and so they will be ignored here.
The question of realizability is of interest for nonstationary systems also. Recalling
the deﬁnitions of stationarity and separability in Chapter 1, the results will be stated in
terms of the input/output representation
y (t) =
−∞∫
∞
h (t,σ)u (σ) dσ
(20)
In the nonstationary case, a linear state equation realization with time-variable coefﬁcients
will be of interest,
x.(t) = A (t)x (t) + b (t)u (t)
y (t) = c (t)x (t)
(21)
It is convenient for technical reasons to require that A (t), b (t), and c (t) be continuous
matrix functions. That is, each entry in these coefﬁcient matrices is a continuous function.
Theorem 4.5
The kernel h (t,σ) is realizable by a ﬁnite-dimensional, time-variable linear
state equation if and only if it is separable.
Proof
If the kernel is linear realizable, and (21) is a realization of h (t,σ), then
h (t,σ) = c (t)Φ(t,σ)b (σ)
Writing
c (t)Φ(t, 0) = [v 01(t) . . . v 0n(t)] , Φ(0,σ)b (σ) =
H
A
A
I v 1n(σ)
...
v 11(σ) J
A
A
K
shows that h (t,σ) is of the separable form
150

h (t,σ) =
i =1Σ
n
v 0i(t)v 1i(σ)
(22)
The continuity required by separability is furnished by the continuity assumptions on the
linear state equation.
Now suppose h (t,σ) is separable and, in fact, is given by (22). (Since h (t,σ) is
real-valued, it can be assumed that each vji(.) is real.) Then setting
A (t) = 0 , b (t) =
H
A
A
I v 1n(t)
...
v 11(t) J
A
A
K
, c (t) = [v 01(t) . . . v 0n(t)]
in (21) gives a realization for h (t,σ). Notice that this realization has continuous coefﬁcient
matrices since separability implies that v 0i(t) and v 1i(σ) in (22) are continuous.
An obvious question, and the one of most interest here, deals with when a kernel
h (t,σ) can be realized by a constant-parameter linear state equation. In other words, when
is an input/output representation written in nonstationary form actually realizable by a
stationary system?
Theorem 4.6
The kernel h (t,σ) is realizable by a ﬁnite-dimensional, constant-parameter
linear state equation if and only if it is stationary and differentiably separable.
Proof
Necessity of the conditions follows directly from the form of h (t,σ) given by a
constant-parameter realization. The sufﬁciency proof is more subtle, and so I will begin by
considering the special case where the kernel is stationary, differentiably separable, and of
the form
h (t,σ) = v 0(t)v 1(σ)
where v 0(t) and v 1(σ) are (necessarily) real, differentiable functions. The ﬁrst step is to
pick T > 0 so that
q 1 =
−T∫
T
v0
2(t) dt > 0
Of course, it can be assumed that such T exists, for otherwise h (t,σ) = 0 and the theorem is
uninteresting. Now, by stationarity, h (t,σ) = h (0,σ−t) so that
dσ
d
___ h (t,σ) + dt
d
___ h (t,σ) = 0
or
v 0(t)v.
1(σ) + v.
0(t)v 1(σ) = 0
Multiplying this equation by v 0(t) and integrating with respect to t from −T to T gives
151

q 1v.
1(σ) + r 1v 1(σ) = 0
where
r 1 =
−T∫
T
v 0(t)v.
0(t) dt
But q 1 > 0 so that the differential equation is nontrivial, and thus v 1(σ) is the exponential
v 1(σ) = v 1(0)e −(r1/q 1)σ
Then the stationarity condition gives
v 0(t) = v 0(0)e (r1/q 1)t
from which it follows that
h (t,σ) = v 0(0)v 1(0)e (r1/q 1)(t −σ)
In other words, if a kernel is stationary, differentiably separable, and single-term, then it
must be a simple exponential. Clearly this kernel is realizable by a linear state equation.
To complete the proof, the case where the kernel takes the more general form in (22) must
be considered. If each v 0i(t) and v 1i(t) is real-valued, this is easy since an additive
parallel connection of linear state equations can be represented by a linear state equation.
If some of the functions in (22) are complex-valued, then it is left to the reader to show
that, since conjugates must be included, a linear state equation realization with real
coefﬁcient matrices can be found.
4.2 Realization of Stationary Homogeneous Systems
For a speciﬁed homogeneous nonlinear system, the problem that will be discussed
here is the problem of ﬁnding realizations in the form of bilinear state equations. That is,
state equations of the form
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t) , t ≥0 , x (0) = 0
(23)
where x (t) is the m × 1 state vector, for each t an element of the state space Rm, and the
input and output are scalars. (Again, much of the theory generalizes neatly to the multi-
input, multi-output case.) The choice of zero initial state reﬂects an interest in the simplest
kind of input/output behavior, although the x (0) ≠ 0 case can be developed in a similar
manner if x (0) is an equilibrium state.
Of course, a bilinear state equation in general does not have a homogeneous
input/output representation. Thus, the results in this section involve rather specialized
bilinear state equations. Also, the bilinear realization problem for homogeneous systems
is subsumed by the theory of Section 4.3 for polynomial and Volterra systems. The intent
of the discussion here is to provide a leisurely introduction to the ideas, and to establish
152

notation.
The input/output representation to be used in conjunction with (23) is derived in
Chapter 3. There it is shown that the bilinear state equation (23) can be described by a
Volterra system representation in which the degree-n subsystem can be written in the
(nonstationary) triangular form
yn(t) =
0∫
t
0∫
σ1
. . .
0∫
σn −1
h (t,σ1, . . . ,σn)u (σ1) . . . u (σn) dσn . . . dσ1
(24)
where the kernel is given by
h (t,σ1, . . . ,σn) = ce A(t−σ1)De A(σ1−σ2)D . . . De A(σn −1−σn)b ,
t ≥σ1 ≥. . . ≥σn ≥0
(25)
For the purposes of developing a bilinear realization theory, the main emphasis will be on
the regular kernel and regular transfer function. To obtain the regular kernel from (25), the
ﬁrst step is to impose stationarity as prescribed in Section 1.2. This gives a stationary
triangular kernel:
gtri(σ1, . . . ,σn) = h (0,−σ1, . . . ,−σn)
= ce Aσ1De A(σ2−σ1)D . . . De A(σn−σn −1)b ,
σn ≥σn −1 ≥. . . ≥σ1 ≥0
(26)
Rewriting (26) as a triangular kernel over the "ﬁrst" triangular domain gives
htri(t 1, . . . ,tn) = ce AtnDe A(tn −1−tn)D . . . De A(t 1−t 2)b ,
t 1 ≥t 2 ≥. . . ≥tn ≥0
(27)
Thus, the regular kernel for the degree-n homogeneous subsystem corresponding to the
bilinear state equation (23) is of the form
hreg(t 1, . . . ,tn) = ce AtnDe Atn −1D . . . De At 1b
(28)
Of course, when written out in scalar terms the form of hreg(t 1, . . . ,tn) is much
messier. Indeed, taking into account the kinds of terms that can appear in a matrix
exponential shows that the regular kernel corresponding to a bilinear state equation can be
written in the form
hreg(t 1, . . . ,tn) =
i 1=1Σ
m1
j 1=1
Σ
µ1 . . .
in=1Σ
mn
jn=1Σ
µn
ai 1 . . . in
j 1 . . . jn
(j 1−1)! . . . (jn−1)!
t1
j 1−1 . . . tn
jn−1
________________ e
−λi 1t 1 . . . e
−λintn
(29)
The various coefﬁcients and exponents in this expression can be complex, but since the
regular kernel is real, well known conjugacy conditions must be satisﬁed.
153

Clearly the regular kernels for a bilinear state equation are of a particularly simple
form. And taking the Laplace transform of (28) shows that the regular transfer functions
also have a simple form, namely
Hreg(s 1, . . . ,sn) = c (snI−A)−1D (sn −1I−A)−1D . . . D (s 1I−A)−1b
(30)
Writing each (sjI−A)−1 as the classical adjoint over the determinant shows that each
regular transfer function for a bilinear state equation is a strictly proper rational function
in that the numerator polynomial degree in each variable is (strictly) less than the
denominator polynomial degree in that variable. Furthermore, Hreg(s 1, . . . ,sn) has the
very special property that the denominator polynomial can be expressed as a product of
(real-coefﬁcient) single-variable polynomials, so that the regular transfer functions for a
bilinear state equation can be written in the form
Hreg(s 1, . . . ,sn) = Q1(s 1) . . . Qn(sn)
P (s 1, . . . ,sn)
_______________
(31)
A rational function of this form will be called a recognizable function, following the
terminology of automata theory. (Of course, a recognizable function can be made to
appear unrecognizable by the insertion of a common factor, for example (s 1 + s 2), in the
numerator and denominator polynomials. However, I will use the terminology in a manner
that implicitly assumes such silliness is removed.)
The bilinear realization problem will be discussed here in terms of a given degree-n
homogeneous system described by the regular transfer function. It is left understood that
all the regular transfer functions of degree ≠n are zero.
What has been shown to this point is that for a stationary, degree-n homogeneous
system to be bilinear realizable, it is necessary that the regular transfer function be a
strictly proper, recognizable function. That is, it is necessary that the regular kernel have
the exponential form (28) or, equivalently, (29). The following argument shows that the
condition on the regular transfer function also is sufﬁcient for bilinear realizability of the
system. In terms of the time-domain representation, this means that the exponential form
of the regular kernel is necessary and sufﬁcient for bilinear realizability of the system.
So, suppose that a degree-n homogeneous system is described by a strictly proper,
recognizable, regular transfer function of the form (31), where
P (s 1, . . . ,sn) =
i 1=0Σ
m1−1
. . .
in=0Σ
mn−1
pi 1 . . . ins1
i 1 . . . sn
in
(32)
and where Qj(sj) is a monic polynomial of degree mj, j = 1, . . . ,n. In order to construct a
corresponding bilinear realization, it is convenient to write the numerator in a matrix
factored form
P (s 1, . . . ,sn) = Sn Sn −1 . . . S 1 P
(33)
where each Sj is a matrix with entries that involve only the variable sj, and P is a vector of
coefﬁcients from P (s 1, . . . ,sn).
This corresponds in the n = 1 case to writing a
polynomial as a product of a row vector of variables [1 s s 2 . . . s m1−1] and a column vector
154

of coefﬁcients [p 0 p 1 . . . pm1−1]´. Before giving the somewhat messy prescription for (33)
in the general case, an example will show just how simple the construction is.
Example 4.2
For the polynomial
P (s 1,s 2,s 3) = s 1s 2s 3 + s 1s 2 + s 1s 3 + s 2s 3 + s 2 + 1
it is a simple matter to factor out the dependence on s 3 by writing
P (s 1,s 2,s 3) = [1
s 3]
H
A
I s 1s 2 + s 1 + s 2
s 1s 2 + s 2 + 1 J
A
K
Now, the s 2 dependence in each polynomial on the right side of this expression can be
factored out in a similar way:
P (s 1,s 2,s 3) = [1
s 3]
H
A
I 0
1
0
s 2
1
0
s 2
0 J
A
K
H
A
A
A
I s 1+1
s 1
s 1+1
1
J
A
A
A
K
The last step should be obvious, yielding
P (s 1,s 2,s 3) = [1
s 3]
H
A
I 0
1
0
s 2
1
0
s 2
0 J
A
K
H
A
A
A
I 0
0
0
1
0
0
0
s 1
0
0
1
0
0
0
s 1
0
0
1
0
0
0
s 1
0
0
1
0
0
0
s 1
0
0
0 J
A
A
A
K
H
A
A
A
A
A
A
I 1
1
1
0
1
1
0
1 J
A
A
A
A
A
A
K
The general prescription for (33) in terms of (32) goes as follows. Let
Sn = [1 sn . . . sn
mn−1]
(34)
and for j = 1, . . . ,n −1, deﬁne Sj to be the (mn . . . mj +1) × (mn . . . mj) matrix with i th row
[ 01 × (imj−mj) 1 sj . . . sj
mn−1 01 × (mn . . . mj−imj) ]
(35)
Then P is the column vector speciﬁed by
P´ = [p 0 . . . 0 p 10 . . . 0 . . . pm1−1,0 . . . 0 p 010 . . . 0 p 110 . . . 0
. . . pm1−1,10 . . . 0 . . . p 0m2−1,...,mn−1 . . . pm1−1,...,mn−1] (36)
The result of this numerator factorization procedure is that the regular transfer
function can be written in the factored form
Hreg(s 1, . . . ,sn) = Gn(sn) . . . G1(s 1)
(37)
where
155

G1(s 1) = Q1(s 1)
S 1P
_______ , Gj(sj) = Qj(sj)
Sj
______ ,
j = 2, . . . ,n
(38)
are strictly proper, matrix rational functions. Thus each Gj(sj) has a linear realization, and
can be written in the form
Gj(sj) = Cj(sjI −Aj)−1Bj
(39)
Now consider the bilinear state equation speciﬁed by
A =
H
A
A
A
I
0
...
0
A1
0
...
A2
0
. . .
...
. . .
. . .
An
...
0
0 J
A
A
A
K
,
b =
H
A
A
A
I
0
...
0
B1 J
A
A
A
K
D =
H
A
A
A
A
I
0
...
0
B2C 1
0
0
...
B3C 2
0
0
. . .
...
. . .
. . .
. . .
BnCn −1
...
0
0
0
0
...
0
0
0 J
A
A
A
A
K
,
c = [ 0 . . . 0 Cn ]
(40)
The regular transfer functions for this bilinear state equation can be computed via (30).
Due to the block-diagonal form of A,
(sI −A)−1 =
H
A
A
A
I
0
...
0
(sI −A1)−1
. . .
...
. . .
. . .
(sI −An)−1
...
0
0
J
A
A
A
K
so a straightforward computation gives
Hreg(s 1, . . . ,sk) = 0 , k = 1, . . . ,n −1,n +1,n +2, . . .
Hreg(s 1, . . . ,sn) = Cn(snI−An)−1BnCn −1(sn −1I−An −1)−1Bn −1Cn −2
. . . B2C 1(s 1I−A1)−1B1
= Gn(sn) . . . G1(s 1)
(41)
Thus (40) is a degree-n homogeneous bilinear realization for the given regular transfer
function. This development can be summarized as follows.
Theorem 4.7
A degree-n homogeneous system described by the regular transfer function
Hreg(s 1, . . . ,sn) is bilinear realizable if and only if Hreg(s 1, . . . ,sn) is a strictly proper,
recognizable function.
156

In addition to the realizability condition in Theorem 4.7, the development above
indicates that the bilinear realization problem for a degree-n homogeneous system
essentially involves a sequence of n linear realization problems.
But the simple
factorization procedure used to obtain (37) usually leads to a bilinear realization of quite
high dimension, even if minimal linear realizations of each Gj(sj) are used. To construct a
minimal-dimension bilinear realization, a more sophisticated factorization procedure can
be used, but I will not pursue that approach further. (See Remark 4.5.)
An alternative approach to bilinear realization theory for a given regular transfer
function of the form (31) involves the notion of an abstract shift realization similar to that
in the linear case. I will present this approach in detail since it directly provides minimal-
dimension bilinear realizations, and since it will be the main tool for polynomial and
Volterra systems. The shift realization approach is most easily introduced in terms of a
negative power series representation of Hreg(s 1, . . . ,sn) of the form
Hreg(s 1, . . . ,sn) =
i 1=0Σ
∞. . .
in=0Σ
∞
hi 1 . . . ins1
−(i 1+1) . . . sn
−(in+1)
(42)
For strictly proper, recognizable transfer functions of the form (30), the validity of this
series representation is clear upon repeated use of the expansion (2). The general setting
for the shift realization approach can be taken to be the class of regular transfer functions
that are analytic at inﬁnity, and have zeros at inﬁnity in each variable; though it is clear
from Theorem 4.7 that this generality will not be needed here. At any rate, comparing (42)
with the series form for (30) shows that the basic mathematical problem can be stated as
follows. Find matrices A, D, b, and c, of dimensions m × m, m × m, m × 1, and 1 × m, such
that for all nonnegative integers j 1, j 2, j 3, . . . ,
cA jkDA jk −1D . . . DA j 1b =
B
A
C
A
D
0 , k ≠n
hj 1 . . . jn , k = n
(43)
Similar to the linear case, it is convenient to use the notation (A,D,b,c,Rm) to indicate an
m-dimensional bilinear realization corresponding to the given regular transfer function.
For any negative power series in k variables,
V (s 1, . . . ,sk) =
i 1=0Σ
∞. . .
ik=0Σ
∞
vi 1 . . . iks1
−(i 1+1) . . . sk
−(ik+1)
(44)
deﬁne the shift operator S by
SV (s 1, . . . ,sk) =
i 1=0Σ
∞. . .
ik=0Σ
∞
vi 1+1,i 2 . . . iks1
−(i 1+1) . . . sk
−(ik+1)
(45)
Notice that the shift involves only the s 1 variable, and that for k = 1 it reduces to the shift
operator deﬁned in Section 4.1. Clearly S is a linear operator, and SV (s) is a negative
power series in k variables so that S jV (s) is well deﬁned.
157

Also needed is an index operator T that is deﬁned on V (s 1, . . . ,sk) in (44) by
TV(s 1, . . . ,sk) =
i 1=0Σ
∞. . .
ik −1=0
Σ
∞
v 0i 1 . . . ik −1s1
−(i 1+1) . . . sk −1
−(ik −1+1)
(46)
for the case k > 1, and by TV(s 1) = 0 for the k = 1 case. Note that T is a linear operator
and TV(s 1, . . . ,sk) is a negative power series in k − 1 variables. Thus T can be repeatedly
applied, though 0 will be obtained after at most k steps. Throughout the following
development the symbols S and T will be used regardless of the particular domain of
negative power series, in particular, regardless of the number of variables.
Now suppose a given degree-n homogeneous system is described by a regular
transfer function Hreg(s 1, . . . ,sn) in the negative power series form. Deﬁne a linear space
of negative power series according to
U1 = span { Hreg(s 1, . . . ,sn), SHreg(s 1, . . . ,sn), S 2Hreg(s 1, . . . ,sn), . . . }
(47)
Using the notation TU1 for the image of U1 under T, let
U2 = span { TU1, STU1, S 2TU1, . . . }
Un = span { TUn −1, STUn −1, S 2TUn −1, . . . }
(48)
Then Uj is a linear space of negative power series in n +1−j variables, j = 1, . . . ,n, and
Ui ∩ Uj = 0. Furthermore Uj is invariant with respect to S, that is, SUj ⊂ Uj, and
TUj ⊂ Uj +1.
Now consider the linear space
U = span { U1, . . . , Un }
(49)
The elements of U are negative power series in n variables or less, and both S and T then
can be viewed as linear operators from U into U. Deﬁne the initialization operator
L:R → U in terms of the given regular transfer function by
Lr = Hreg(s 1, . . . ,sn)r
(50)
and deﬁne the evaluation operator E :U → R by
EV (s 1, . . . ,sk) =
B
A
C
A
D
EV (s 1), k = 1
0, k > 1
(51)
where EV (s 1) is the evaluation operator deﬁned in (10) for the linear case.
The spiritual similarity of this setup to that for linear systems should be apparent.
Also, since regular transfer functions that are not necessarily in power series form will be
of most interest, it is convenient to interpret the linear operators deﬁned above directly in
terms of functions of k variables corresponding to negative power series. Indeed, the
158

easily derived formulas are:
SV (s 1, . . . ,sk) = s 1V (s 1, . . . ,sk) −[s 1V (s 1, . . . ,sk)]s1=∞
TV(s 1, . . . ,sk) = [s 1V (s 1, . . . ,sk)]
sk=sk −1
...
s2=s1
s1=∞
EV (s 1, . . . ,sk) =
B
A
C
A
D
[s 1V (s 1)]s1=∞, k = 1
0, k > 1
(52)
These interpretations are very important for calculations since the negative power series
representations are not at all pleasant to actually manipulate.
To show that (S,T,L,E,U) is an abstract realization for the given regular transfer
function is remarkably simple. The identity
ES jnTS jn −1T . . . TS j 1L = hj 1 . . . jn
(53)
is veriﬁed in the following sequence of calculations.
S j 1L = S j 1Hreg(s 1, . . . ,sn)
=
i 1=0Σ
∞
i 2=0Σ
∞. . .
in=0Σ
∞
hi 1+j 1,i 2 . . . ins1
−(i 1+1)s2
−(i 2+1) . . . sn
−(in+1)
TS j 1L =
i 1=0Σ
∞. . .
in −1=0
Σ
∞
hj 1i 1 . . . in −1s1
−(i 1+1) . . . sn −1
−(in −1+1)
S j 2TS j 1L =
i 1=0Σ
∞
i 2=0Σ
∞
in −1=0
Σ
∞
hj 1,i 1+j 2,i 2 . . . in −1s1
−(i 1+1)s2
−(i 2+1) . . . sn −1
−(in −1+1)
TS j 2TS j 1L =
i 1=0Σ
∞. . .
in −2=0
Σ
∞
hj 1j 2i 1 . . . in −2s1
−(i 1+1) . . . sn −2
−(in −2+1)
S jnTS jn −1T . . . TS j 1L =
i 1=0Σ
∞
hj 1 . . . jn −1i 1+jns1
−(i 1+1)
ES jnTS jn −1T . . . TS j 1L = hj 1 . . . jn
It is easy to show that the remaining terms in (43) indeed are 0. If k < n, then the E
operator does the job since its argument will have more than one variable. If k > n, then
159

the T’s will give 0. Now the realization procedure involves determining if U is ﬁnite
dimensional, and if so ﬁnding matrix representations for the linear operators S, T, E, and L
when U is replaced by Rm. The following result and its proof are reminiscent of Theorem
4.1, though the proof is postponed until Section 4.3.
Theorem 4.8
A degree-n homogeneous system described by the regular transfer function
Hreg(s 1, . . . ,sn) is bilinear realizable if and only if U is ﬁnite dimensional. Furthermore, if
the system is bilinear realizable, then (S,T,L,E,U) is a minimal bilinear realization.
I should remark that it is not hard to show that U is ﬁnite dimensional if
Hreg(s 1, . . . ,sn) is a strictly proper, recognizable function. It might be worthwhile for the
reader to work this out following the spirit of the proof of Theorem 4.2, just to gain some
familiarity with the linear operators S and T when applied to strictly proper, recognizable
functions.
To ﬁnd a matrix realization when U is ﬁnite dimensional, it is convenient to replace
U in the following way. If dimension U = m, choose the standard ordered basis for Rm so
that e 1, . . . ,em1 represents the linearly independent elements of U1, em1+1, . . . ,em2
represents the linearly independent elements of U2, and so on. Then from the fact that
U1, . . . ,Un, are disjoint, and from the invariance properties mentioned earlier, matrix
representations for S and T will have the form
A =
H
A
A
A
I
0
...
0
A11
0
...
A22
0
. . .
...
. . .
. . .
Ann
...
0
0 J
A
A
A
K
D =
H
A
A
A
A
I
0
...
0
D21
0
0
...
D32
0
0
. . .
...
. . .
. . .
. . .
Dn,n −1
...
0
0
0
0
...
0
0
0 J
A
A
A
A
K
(54)
Also, from the special form of the image of L and the null space of E, the respective matrix
representations will have the form
b =
H
A
A
A
I 0
...
0
b 1 J
A
A
A
K
, c = [0 0 . . . cn]
(55)
(Actually, the casual basis picker invariably ends up with b = e 1.) The dimension of each
Ajj is mj × mj, and D, b, and c are partitioned accordingly. Note that this is precisely the
type of block-form realization used to derive Theorem 4.7.
Example 4.3
Given the bilinear-realizable regular transfer function
160

Hreg(s 1,s 2) = (s 1+2)(s 2+3)
1
____________ = s 1s 2+3s 1+2s 2+6
1
________________
the ﬁrst step in constructing a realization is to compute the spaces U1 and U2. Since
SHreg(s 1,s 2) = s 1s 2+3s 1+2s 2+6
s 1
________________ −s 2+3
1
_____
= s 1s 2+3s 1+2s 2+6
−2
________________ = −2Hreg(s 1,s 2)
it is clear that
U1 = span
B
C
D s 1s 2+3s 1+2s 2+6
1
________________ E
F
G
To compute U2, note that
THreg(s 1,s 2) = [ s 1s 2+3s 1+2s 2+6
s 1
________________]s2=s1
s1=∞= s 1+3
1
_____
STHreg(s 1,s 2) = s 1+3
s 1
_____ −1 = s 1+3
−3
_____
Thus,
U2 = span
B
C
D s 1+3
1
_____ E
F
G
and making the replacement
U1 = span
B
C
D
H
I 0
1 J
K
E
F
G
, U2 = span
B
C
D
H
I 1
0 J
K
E
F
G
the matrix representations for S, T, L, and E can be obtained as follows. If A is the matrix
representation for S, then
A H
I 0
1 J
K = H
I 0
−2 J
K , A H
I 1
0 J
K = H
I −3
0 J
K
Thus,
A = H
I 0
−2
−3
0 J
K
If D is the matrix representation of T, then
D H
I 0
1 J
K = H
I 1
0 J
K , D H
I 1
0 J
K = H
I 0
0 J
K
so that
161

D = H
I 1
0
0
0 J
K
It is clear that the matrix representation of L is
b = H
I 0
1 J
K
and, ﬁnally, since
E s 1s 2+3s 1+2s 2+6
1
________________ = 0, E s 1+3
1
_____ = 1
the matrix representation for E is
c = [0
1]
It should be clear that the two approaches to the realization problem for
homogeneous systems that have been discussed are analogous to the two main approaches
to linear realization theory. The shift realization approach is based mainly on series
representations of rational functions, while the other approach is based more on direct
manipulations of the polynomials in the rational transfer function. In addition, the shift
realization approach for nonlinear systems can be rephrased in terms of a Behavior matrix
not too unlike that in the linear case. This formulation will be demonstrated in Section 4.3.
There is a particular form of interconnection structured realization that corresponds
to the block-form bilinear state equation speciﬁed by (54) and (55), or by (40).
Partitioning the state vector x (t) in the form
x (t) =
H
A
A
I xn(t)
...
x 1(t) J
A
A
K
(56)
where xj(t) is mj × 1, the block-form realization can be described by the set of state
equations:
x.
1(t) = A11x 1(t) + b 1u (t)
x.
2(t) = A22x 2(t) + D21x 1(t)u (t)
x.
n(t) = Annxn(t) + Dn,n −1xn −1(t)u (t)
y (t) = cnxn(t)
(57)
Then the realization corresponds to the cascade connection of multi-input, multi-output
linear systems, and vector multipliers shown in Figure 4.1. (A vector quantity, in general,
deserves a double line, while scalar quantities get a single line.)
162

Figure 4.1. An interconnection structured realization.
4.3 Realization of Stationary Polynomial and Volterra Systems
The polynomial system case will be discussed ﬁrst, and in most detail, though the
notation will be chosen in a way that will facilitate consideration of Volterra systems later.
Again, ﬁnite-dimensional bilinear state equation realizations are of interest, particularly
those of minimal dimension. It will be seen that the shift approach extends to this setting
in a very simple fashion.
Suppose a degree-N polynomial system of is described by the sequence of regular
transfer functions
( H (s 1), Hreg(s 1,s 2), . . . , Hreg(s 1, . . . ,sN), 0, . . . )
(58)
where the transfer functions of degree greater than N are all indicated as zeros. The ﬁrst
result shows that the basic realizability condition for polynomial systems follows directly
Theorem 4.9
The polynomial system speciﬁed in (58) is bilinear realizable if and only if
each regular transfer function is a strictly proper, recognizable function.
Proof
Suppose, ﬁrst, that each transfer function Hreg(s 1, . . . ,sj) is strictly proper and
recognizable. Then from Section 4.2 it is clear that each can be realized by a degree-j
homogeneous bilinear state equation
x.
j(t) = Ajxj(t) + Djxj(t)u (t) + bju (t)
yj(t) = cjxj(t)
(59)
where j = 1,...,N, and D1 = 0. (The degree-1 realization, of course, is a linear state
equation.) Now, consider the additive parallel connection of these state equations. Such a
connection can be described by the "block diagonal" bilinear state equation (A,D,b,c,Rm)
given by
x.(t) =
H
A
A
A
I
0
...
0
A1
0
...
A2
0
. . .
...
. . .
. . .
AN
...
0
0 J
A
A
A
K
x (t) +
H
A
A
A
I
0
...
0
D1
0
...
D2
0
. . .
...
. . .
. . .
DN
...
0
0 J
A
A
A
K
x (t)u (t) +
H
A
A
A
I bN
...
b 2
b 1 J
A
A
A
K
u (t)
y (t) = [ c 1 . . . cN ] x (t)
Using the block diagonal form for A and D, it is straightforward to compute the degree-k
163
u
…
I(sI – A11)–1 b1
y
Π
Π
I(sI – A22)–1 D21
x1
cn(sI – Ann)–1 Dn,n–1
x2
Π
xn –1
…

transfer function for this realization:
c (skI−A)−1D . . . D (s 1I−A)−1b =
j =1Σ
N
cj(skI−Aj)−1Dj . . . Dj(s 1I−Aj)−1bj
But the fact that the j th bilinear state equation is homogeneous of degree j implies that all
the summands on the right side are 0 except for when j = k. Thus
c (skI−A)−1D . . . D (s 1I−A)−1b = ck(skI−Ak)−1Dk . . . Dk(s 1I−Ak)−1bk
=
B
A
C
A
D
0, k > N
Hreg(s 1, . . . ,sk), k = 1, . . . ,N
and (A,D,b,c,Rm) is a bilinear realization for the given polynomial system.
Now suppose the polynomial system is bilinear realizable, and furthermore that
(A,D,b,c,Rm) is such a realization. Then by calculation each degree-n regular transfer
function is the strictly proper, recognizable function
c (snI −A)−1D . . . D (s 1I −A)−1b , n = 1, . . . ,N
Thus the proof is complete.
The analog of Theorem 4.9 in the time domain should be obvious. A degree-N
polynomial system is bilinear realizable if and only if each regular kernel has the
exponential form given in (28) or (29).
The basic bilinear realizability result for polynomial systems also can be developed
via a shift realization approach, based on the assumption that each regular transfer
function in (58) can be written as a negative power series of the form
Hreg(s 1, . . . ,sk) =
i 1=0Σ
∞. . .
ik=0Σ
∞
hi 1 . . . iks1
−(i 1+1) . . . sk
−(ik+1) ,
k = 1, . . . ,N
(60)
Of course, this assumption can be made with no loss of generality as far as bilinear
realizations are concerned. It is clear from the formulation in Section 4.2 that constructing
a bilinear realization involves ﬁnding matrices A, D, b, and c such that for all nonnegative
integers j 1,j 2, . . . ,
cA jkDA jk −1D . . . DA j 1b =
B
A
C
A
D
0, k > N
hj 1 . . . jk, k = 1, . . . ,N
(61)
As has become usual in this chapter, the ﬁrst step is to construct an abstract shift
realization.
Given any ﬁnite sequence of negative power series
164

( V1(s 1), V2(s 1,s 2), V3(s 1,s 2,s 3), . . . )
(62)
deﬁne the shift operator S by
S ( V1(s 1), V2(s 1,s 2), V3(s 1,s 2,s 3), . . . )
= ( SV 1(s 1), SV 2(s 1,s 2), SV 3(s 1,s 2,s 3), . . . )
(63)
where SVk(s 1, . . . ,sk) is the shift operator deﬁned in (44) and (45). Similarly, deﬁne the
index operator T by
T( V1(s 1), V2(s 1,s 2), V3(s 1,s 2,s 3), . . . )
= ( TV2(s 1,s 2), TV3(s 1,s 2,s 3), . . . )
(64)
where TVk(s 1,...,sk) is the index operator deﬁned in (46). (Of course, if (62) is viewed as a
sequence of functions deﬁned by negative power series, then the shift and index operators
can be interpreted as per (52).)
In order to proceed further, it is convenient to use the notation
Hˆ (s 1, . . . ,sN) = ( H (s 1), Hreg(s 1,s 2), . . . , Hreg(s 1, . . . ,sN),0, . . . )
(65)
to indicate the given degree-N polynomial system. Then deﬁne the following linear spaces
of ﬁnite sequences of negative power series.
U1 = span {Hˆ (s 1, . . . ,sN), SHˆ (s 1, . . . ,sN), S 2Hˆ (s 1, . . . ,sN), . . . }
U2 = span {TU 1, STU1, S 2TU1, . . . }
UN = span {TUN−1, STUN−1, S 2TUN−1, . . . }
(66)
Letting U = span {U 1, . . . ,UN }, S and T can be viewed as operators from U into U.
Deﬁne the initialization operator L:R → U in terms of the given Hˆ (s 1, . . . ,sN) by
Lr = Hˆ (s 1, . . . ,sN)r
(67)
and the evaluation operator E :U → R by
E ( V1(s 1), V2(s 1,s 2), V3(s 1,s 2,s 3), . . . ) = EV1(s 1)
(68)
where EV1(s 1) is deﬁned as in the linear case.
Now the calculations to show that (S,T,L,E,U) is an abstract bilinear realization for
the given Hˆ (s 1, . . . ,sN) follow directly from the calculations in the homogeneous case.
For instance,
165

ES jL = E (S jH (s 1), S jHreg(s 1,s 2), . . . , S jHreg(s 1, . . . ,sN), 0, . . . )
= ES jH (s 1) = hj ,
j = 0,1,2, . . .
ES j 2TS j 1L = E (S j 2TS j 1H (s 1,s 2), . . . , S j 2TS j 1Hreg(s 1, . . . ,sN), 0, . . . )
= ES j 2TS j 1Hreg(s 1,s 2) = hj 1j 2 ,
j 1, j 2 = 0,1,2, . . .
(69)
Theorem 4.10
A degree-N polynomial system described by the sequence of regular
transfer functions Hˆ (s 1, . . . ,sN) is bilinear realizable if and only if U is ﬁnite dimensional.
Furthermore, if the system is bilinear realizable, then (S,T,L,E,U) is a minimal bilinear
realization.
Proof Suppose that the polynomial system described by Hˆ (s 1, . . . ,sN) is bilinear
realizable, that (A,D,b,c,Rm) is any bilinear realization of the system, and that (S,T,L,E,U)
is the shift realization of the system. Let W be the linear space of all sequences of negative
power series such as (62). Then deﬁne a linear operator Φ:Rm → W by
Φ(x) = ( c (s 1I−A)−1x, c (s 2I−A)−1D (s 1I−A)−1x,
c (s 3I−A)−1D (s 2I−A)−1D (s 1I−A)−1x, . . . )
where for brevity I have written the right side as a sequence of strictly proper,
recognizable functions instead of the corresponding negative power series. Notice that
Φ(b) = Hˆ (s 1, . . . ,sN). Furthermore, using the deﬁnition of the shift operator in the
homogeneous case,
c (skI −A)−1D . . . D(s 1I −A)−1Ab
=
i 1=0Σ
∞. . .
ik=0Σ
∞
cA ikD . . . DAi 1+1bs1
−(i 1+1) . . . sk
−(ik+1)
= S
i 1=0Σ
∞. . .
ik=0Σ
∞
cA ikD . . . DAi 1bs1
−(i 1+1) . . . sk
−(ik+1)
= SHreg(s 1, . . . ,sk) , k = 1, . . . ,N
Extending this calculation to the sequence of regular transfer functions Hˆ (s 1, . . . ,sN)
shows that
Φ(Ab) = SHˆ (s 1, . . . ,sN)
Using the deﬁnition of T in a similar way,
166

c (sk −1I −A)−1D . . . D (s 1I −A)−1Db
=
i 1=0Σ
∞. . .
ik −1=0
Σ
∞
cA ik −1D . . . DAi 1Dbs1
−(i 1+1) . . . sk −1
−(ik −1+1)
= T
i 1=0Σ
∞. . .
ik=0Σ
∞
cA ikD . . . DAi 1bs1
−(i 1+1) . . . sk
−(ik+1)
= THreg(s 1, . . . ,sk) , k = 2, . . . ,N
Again, this calculation, when extended to Hˆ (s 1, . . . ,sN), implies that
Φ(Db) = THˆ (s 1, . . . ,sN)
Combining these results gives
Φ(Ain −1D . . . DAi 1b) = S in −1T . . . TSi 1Hˆ (s 1, . . . ,sN) , n = 1, . . . ,N
which shows that U ⊂ R [Φ]. Since Φ is a linear map on an m-dimensional space, it
follows that dimension U ≤ m. Thus U is ﬁnite dimensional, and furthermore the abstract
shift realization is minimal since the dimension of U is no greater than the state space
dimension of any other bilinear realization of Hˆ (s 1, . . . ,sN).
Assuming now that U has ﬁnite dimension m, the following construction yields a
minimal bilinear realization (A,D,b,c,Rm) of Hˆ (s 1, . . . ,sN). Replacing the space U by Rm
with the standard ordered basis choices e 1, . . . ,em1 for the linearly independent elements
of U1, em1+1, . . . ,em2 for the additional linearly independent elements of U2, and so on,
gives a realization as follows. Since SUj ⊂ span {U 1, . . . ,Uj }, it is clear that the matrix
representation for S will have the block-triangular form
A =
H
A
A
A
I
0
...
0
A11
0
...
A22
A12
. . .
...
. . .
. . .
AMM
...
A2M
A1M J
A
A
A
K
(70)
where Ajj is mj × mj. Also TUj ⊂ span {U 1, . . . ,Uj +1 }, which implies that the matrix
representation for T will have the block (almost triangular) form
D =
H
A
A
A
A
I
0
...
0
D21
D11
0
...
D32
D22
D12
. . .
...
. . .
. . .
. . .
DM,M −1
...
D3,M −1
D2,M −1
D1,M −1
DMM
...
D3M
D2M
D1M J
A
A
A
A
K
(71)
where the blocks are partitioned according to those in A. (Notice that M (≤ N) blocks are
indicated, rather than N. The reason is that a particular Uj may be contained in
span {U 1, . . . ,Uj −1 }.) The matrix representation for L clearly will have the block form
167

b =
H
A
A
A
I 0
...
0
b 1 J
A
A
A
K
(72)
and a matrix representation for E is found by computing the action of E on each Uj to
obtain
c = [ c 11 . . . c 1M ]
(73)
where each c 1j is 1 × mj.
Example 4.6
Consider the degree-2 polynomial system described by the regular transfer
functions
Hˆ (s 1,s 2) = ( s 1+1
1
_____ , (s 1+2)(s 2+3)
1
____________ , 0, . . . )
To ﬁnd U1, compute
SHˆ (s 1,s 2) = ( s 1+1
−1
_____ , (s 1+2)(s 2+3)
−2
____________ , 0, . . . )
S 2Hˆ (s 1,s 2) = ( s 1+1
1
_____ , (s 1+2)(s 2+3)
4
____________ , 0, . . . )
= −2Hˆ (s 1,s 2) −3SHˆ (s 1,s 2)
Thus,
U1 = span
B
C
D
( s 1+1
1
_____ , (s 1+2)(s 2+3)
1
____________ , 0, . . . ) , ( s 1+1
−1
_____ , (s 1+2)(s 2+3)
−2
____________ , 0, . . . )
E
F
G
To ﬁnd U2 the image of these basis elements for U1 under T must be computed, and then
the subsequent image under repeated shifts must be computed.
THˆ (s 1,s 2) = T( s 1+1
1
_____ , (s 1+2)(s 2+3)
1
____________ , 0, . . . ) = ( s 1+3
1
_____ , 0, . . . )
TSHˆ (s 1,s 2) = T( s 1+1
−1
_____ , (s 1+2)(s 2+3)
−2
____________ , 0, . . . ) = ( s 1+3
−2
_____ , 0, . . . )
= −2THˆ (s 1,s 2)
STHˆ (s 1,s 2) = S ( s 1+3
1
_____ , 0, . . . ) = ( s 1+3
−3
_____ , 0, . . . )
= −3THˆ (s 1,s 2)
168

Thus,
U2 = span
B
C
D
( s 1+3
1
_____ , 0, . . . )
E
F
G
Now replace U = span {U 1,U2 } by R3 and choose the standard ordered basis elements
according to
H
A
I 0
0
1 J
A
K
= ( s 1+1
1
_____ , (s 1+2)(s 2+3)
1
____________ , 0, . . . )
H
A
I 0
1
0 J
A
K
= ( s 1+1
−1
_____ , (s 1+2)(s 2+3)
−2
____________ , 0, . . . )
H
A
I 1
0
0 J
A
K
= ( s 1+3
1
_____ , 0, . . . )
This yields the matrix representations
A =
H
A
I 0
1
0
0
−3
−2
−3
0
0 J
A
K
, D =
H
A
I 1
0
0
−2
0
0
0
0
0 J
A
K
The calculations
EHˆ (s 1,s 2) = 1, ESHˆ (s 1,s 2) = −1, ETHˆ (s 1,s 2) = 1
give
c = [1 −1 1]
and, ﬁnally,
b =
H
A
I 0
0
1 J
A
K
Now consider the bilinear realization problem for a given Volterra system. It is
assumed that the system is speciﬁed in terms of a sequence of regular transfer functions
written in the notation
Hˆ (s 1, . . . ,s ∞) = ( H (s 1), Hreg(s 1,s 2), Hreg(s 1,s 2,s 3), . . . )
(74)
As usual, Hˆ (s 1, . . . ,s ∞) will be viewed as a sequence of negative power series, each of
which takes the form in (60). From this perspective, it is clear that the bilinear realization
problem for a Volterra system involves ﬁnding matrices A, D, b, and c such that for all
k = 1,2, . . . , and all nonnegative integers j 1, . . . ,jk,
cA jkD . . . DA j 1b = hj 1 . . . jk
(75)
169

The construction of an abstract shift realization for Hˆ (s 1, . . . ,s ∞) proceeds along
the same lines as in the polynomial system case, so only a brief review of the mechanics is
needed. The shift and index operators are deﬁned, as in the polynomial case, according to
SVˆ(s 1, . . . ,s ∞) = (SV 1(s 1), SV 2(s 1,s 2), . . . )
TVˆ(s 1, . . . ,s ∞) = (TV2(s 1,s 2), TV3(s 1,s 2,s 3), . . . )
(76)
In terms of these operators and the given Volterra system, a set of linear spaces is deﬁned
by
U1 = span {Hˆ (s 1, . . . ,s ∞), SHˆ (s 1, . . . ,s ∞), S 2Hˆ (s 1, . . . ,s ∞), . . . }
U2 = span {TU 1, STU1, S 2TU1, . . . }
U3 = span {TU 2, STU2, S 2TU2, . . . }
(77)
...
and, ﬁnally,
U = span {U 1, U2, U3, . . . }
(78)
It is clear that S and T are linear operators from U into U. Deﬁne the initialization operator
L:R → U in terms of the given system by
Lr = Hˆ (s 1, . . . ,s ∞)r
(79)
and the evaluation operator E :U → R by
E (V 1(s 1), V2(s 1,s 2), . . . ) = EV1(s 1)
(80)
The demonstration that (S,T,L,E,U) is an abstract bilinear realization for the given
Volterra system follows from by now standard calculations. Also, one answer to the
bilinear realizability and minimality questions is easily obtained.
If U has ﬁnite
dimension, then it is clear by the replacement construction that the given system is bilinear
realizable. On the other hand, a simple argument using a Φ operator similar to that in the
proof of Theorem 4.10 shows that if the given system is bilinear realizable, then U has
ﬁnite dimension. (And in this case the shift realization is a minimal bilinear realization for
the system.)
Thus bilinear realizability of a Volterra system is equivalent to ﬁnite
dimensionality of the linear space U. The search for a more direct characterization begins
in the direction of Theorem 4.9.
Theorem 4.11
If the Volterra system speciﬁed by Hˆ (s 1, . . . ,s ∞) in (74) is bilinear
realizable, then each regular transfer function Hreg(s 1, . . . ,sk) is a strictly proper,
recognizable function.
A proof of Theorem 4.11 consists of nothing more than taking a bilinear realization
of Hˆ (s 1, . . . ,s ∞) and observing that by calculation each Hreg(s 1, . . . ,sk) is a strictly
proper, recognizable function. This observation, together with Theorem 4.9, also yields the
170

following interesting fact.
Corollary 4.1
If a Volterra system is bilinear realizable, then any polynomial system
formed by truncation of the Volterra system is also bilinear realizable.
Unfortunately, the search for a more direct characterization of bilinear realizability
for Volterra systems appears to end with the failure of the converse of Theorem 4.11.
Example 4.5
Consider the Volterra system
Hˆ (s 1, . . . ,s ∞) = ( s 1+1
1
_____ , (s 1+1)(s 2+1)
1/2!
____________ , . . . , (s 1+1) . . . (sn+1)
1/n !
_______________ , . . . )
Applying the index operator repeatedly gives
T jHˆ (s 1, . . . ,s ∞) = (
s 1+1
1/(j +1)!
________ , (s 1+1)(s 2+1)
1/(j +2)!
____________ , . . . , (s 1+1) . . . (sn+1)
1/(n +j +1)!
_______________ , . . . )
for j = 1,2, . . . . The denominators of the subsystem transfer functions all behave in the
expected way. But since the collection of sequences (of numerators)
( (j +1)!
1
______ , (j +2)!
1
______ , . . . , (j +n +1)!
1
_________ , . . . ) ,
j = 0,1, . . .
is inﬁnite dimensional, it is clear without even calculating the action of the shift operator
that U = span {U 1, U2, . . .  } will be inﬁnite dimensional. Thus Hˆ (s 1, . . . ,s ∞) is not
bilinear realizable.
Suppose a Volterra system is given wherein every subsystem regular transfer
function is strictly proper and recognizable. To check for bilinear realizability, Example
4.7 indicates that there is no choice but to work through the calculation of the dimension of
U. Of course, this raises the issue of having a general form for the sequence of regular
transfer functions. When such a general form is available, the calculation of a bilinear
realization can be easy if the dimensions are small.
Example 4.6
For the Volterra system
Hˆ (s 1, . . . ,s ∞) = ( s 1+1
1
_____ , (s 1+1)(s 2+1)
1
____________ , . . . , (s 1+1) . . . (sn+1)
1
_______________ , . . . )
a quick calculation shows that
S jHˆ (s 1, . . . ,s ∞) = (−1)jHˆ (s 1, . . . ,s ∞) ,
j = 1,2, . . .
and
T jHˆ (s 1, . . . ,s ∞) = Hˆ (s 1, . . . ,s ∞) ,
j = 1,2, . . .
Therefore dimension U = 1, and another easy calculation shows that a minimal bilinear
realization is
171

x.(t) = −x (t) + x (t)u (t) + u (t)
y (t) = x (t)
In addition to illustrating a sort of easiest-possible Volterra system realization
problem, this example in conjunction with Example 4.5 shows that the interrelationships of
the subsystem "gains" plays a crucial role in ﬁnite-dimensional realizability. That is,
bilinear realizability can be created or destroyed simply by changing constants in the
numerators of the regular transfer functions in a Volterra system. Another interesting
observation can be made by considering the degree-2 polynomial truncation of the system
in Example 4.6, namely,
Hˆ (s 1,s 2) = ( s 1+1
1
_____ , (s 1+1)(s 2+1)
1
____________ , 0, . . . )
The minimal-dimension bilinear realization for this polynomial system has dimension 2.
Thus truncation can increase the dimension of the minimal bilinear realization. See
Problem 4.7.
A perhaps cleaner statement of the condition for bilinear realizability of a given
Volterra system (or, for that matter, polynomial or homogeneous system) can be developed
from the shift-realization viewpoint. The approach involves replacing negative power
series with sequences so that U is viewed as a linear space of sequences of sequences,
arranging these sequences into a matrix, and then noting that rank ﬁniteness of the matrix
is equivalent to ﬁnite dimensionality of U. Again, this is all very similar in style to the
linear case.
Viewing a sequence of regular transfer functions Hˆ (s 1, . . . ,s ∞) as a sequence of
negative power series
Hˆ (s 1, . . . ,s ∞) = (
i 1=0Σ
∞
hi 1s1
−(i 1+1),
i 1=0Σ
∞
i 2=0Σ
∞
hi 1i 2s1
−(i 1+1)s2
−(i 2+1), . . . )
(81)
any expression of the form
S jkTS jk −1T . . . TS j 1Hˆ (s 1, . . . ,s ∞)
(82)
can be viewed in the same way. For example,
SHˆ (s 1, . . . ,s ∞) = (
i 1=0Σ
∞
hi 1+1s1
−(i 1+1),
i 1=0Σ
∞
i 2=0Σ
∞
hi 1+1,i 2s1
−(i 1+1)s2
−(i 2+1), . . . )
THˆ (s 1, . . . ,s ∞) = (
i 1=0Σ
∞
h 0i 1s1
−(i 1+1),
i 1=0Σ
∞
i 2=0Σ
∞
h 0i 1i 2s1
−(i 1+1)s2
−(i 2+1), . . . )
(83)
Each of these sequences of negative power series can be viewed as a sequence of
sequences. For example,
172

Hˆ (s 1, . . . ,s ∞) = ( (h 0,h 1,h 2, . . . ), (h 00,h 01,h 02, . . . ,h 10,h 11,h 12, . . . ), . . . )
SHˆ (s 1, . . . ,s ∞) = ( (h 1,h 2,h 3, . . . ), (h 10,h 11,h 12, . . . ,h 20,h 21,h 22, . . . ), . . . )
THˆ (s 1, . . . ,s ∞) = ( (h 00,h 01,h 02, . . . ), (h 000,h 001,h 002, . . . ,h 010,h 011,h 012, . . . ), . . . )
Of course, there are many ways to systematically list the multi-index sequences, but the
particular arrangement is immaterial as long as all are listed in the same way.
From this viewpoint, each Uj in (77) and U in (78) can be considered as a linear
space of sequences of sequences. The shift and index operators are interpreted as above,
and the operators L and E are similarly modiﬁed for the sequence interpretation. Then a
Behavior matrix for the given system is deﬁned in terms of the sequence interpretation by
BHˆ =
H
A
A
A
A
A
A
A
A
A
A
A
A
I
...
S jkT . . . TS j 1Hˆ (s 1, . . . ,s ∞)
...
S 2THˆ (s 1, . . . ,s ∞)
STHˆ (s 1, . . . ,s ∞)
THˆ (s 1, . . . ,s ∞)
...
S 2Hˆ (s 1, . . . ,s ∞)
SHˆ (s 1, . . . ,s ∞)
Hˆ (s 1, . . . ,s ∞)
J
A
A
A
A
A
A
A
A
A
A
A
A
K
=
H
A
A
A
A
I
...
h 00
...
h 1
h 0
...
h 01
...
h 2
h 1
...
. . .
...
. . .
. . .
...
h 000
...
h 10
h 00
...
h 001
...
h 11
h 01
...
. . .
...
. . .
. . . J
A
A
A
A
K
(84)
And now the following realizability condition should be an obvious restatement of the
ﬁnite-dimensionality condition on the linear space U.
Theorem 4.12
The Volterra system described by Hˆ (s 1, . . . ,s ∞) is bilinear realizable if
and only if the corresponding Behavior matrix BHˆ has ﬁnite rank. Furthermore, for a
bilinear-realizable system, the rank of BHˆ is the dimension of the minimal bilinear
realizations.
4.4 Properties of Bilinear State Equations
Having focused attention on the bilinear realization question, it is appropriate to
discuss some of the features of such state equations. As I have mentioned previously,
bilinear state equations have many structural features that are strikingly similar to well
known features of linear state equations. These features will be demonstrated in the
general situation where the bilinear state equation represents a Volterra system. There is
173

no reason to consider separately the special cases of homogeneous or polynomial systems.
A question that often arises is whether a given bilinear state equation is minimal.
That is, whether the state equation is a minimal bilinear realization of its input/output
description. A convenient way to address this question is through the appropriate concepts
of reachability and observability. These concepts will be developed and related to
minimality. Also, certain equivalence properties of minimal bilinear realizations will be
discussed.
The appropriate deﬁnition of reachability for the bilinear state equation
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t) , t ≥0 , x (0) = 0
(85)
begins with the notion of a reachable state. As usual, x (t) ε Rm, and u (t) and y (t) are
scalars.
Deﬁnition 4.1
A state x 1 of the bilinear state equation (85) is called reachable (from
x (0) = 0) if there exists a piecewise continuous input signal such that for some t 1 < ∞ ,
x (t 1) = x 1.
I should note that the speciﬁcation of piecewise continuity for the input signal is
more or less a matter of convenience. Both more general and more restrictive classes of
inputs can be chosen without changing the results. (But not to specify the class of
admissible inputs would be in poor taste.)
It would be nice if the set of reachable states for a bilinear state equation formed a
linear subspace of the state space Rm.
Unfortunately this is not the case; linear
combinations of reachable states may not be reachable. Thus, a somewhat weaker notion
of reachability is used so that the techniques of linear algebra can be applied.
Deﬁnition 4.2
The bilinear state equation (A,D,b,c,Rm) is called span reachable if the
set of reachable states spans Rm.
The ﬁrst step in establishing a criterion for span reachability of a given system is to
characterize the span of the reachable states. To this end, let LA,D(b) denote the least
dimension subspace of Rm containing b and invariant under A and D.
Lemma 4.1
The subspace Xsr ⊂ Rm spanned by the reachable states of (A,D,b,c,Rm) is
given by Xsr = LA,D(b).
Proof
Suppose x 1 is a reachable state, so that for some input u (t) and some t 1 < ∞,
x (t 1) = x 1. Then x 1 can be written using the expression derived in Chapter 3 for the
solution x (t) of a bilinear state equation. For the case of x (0) = 0 and constant coefﬁcient
matrices, the ﬁrst few terms are
174

x 1 =
0∫
t 1
e A(t 1−σ)bu (σ) dσ
+
0∫
t 1
0∫
σ1
e A(t 1−σ1)De A(σ1−σ2)bu (σ1)u (σ2) dσ2dσ1 + . . .
(86)
Expressing the matrix exponentials as power series, and using the uniform convergence of
these series to interchange summation and integration, a messier expression is obtained,
the ﬁrst few terms of which are
x 1 = b
0∫
t 1
u (σ) dσ + Ab
0∫
t 1
(t −σ)u (σ) dσ + Db
0∫
t 1
0∫
σ1
u (σ1)u (σ2) dσ2dσ1 + . . .
(87)
This expression shows that x 1 is a linear combination of products of A and D times b, so
that x 1 ε LA,D(b). Thus Xsr ⊂ LA,D(b) since there is a set of reachable states that forms a
basis for Xsr.
To obtain the reverse containment, it is not hard to show that if x (t) is contained in a
subspace for all t ≥ 0, then x.(t) is contained in the same subspace for all t ≥ 0. Thus, for
any constant input u (t) = u, and any reachable state x 1,
(A + Du)x 1 + bu ε Xsr
In particular, x 1 = 0 is reachable, and thus b ε Xsr. Therefore, if u is any real number, and
x 1 is any reachable state,
(A + Du)x 1 ε Xsr
Since there is a set of reachable states that spans Xsr, for any u the image of Xsr under
(A + Du) satisﬁes
(A + Du)Xsr ⊂Xsr
It is left to Problem 4.14 to show that this implies that Xsr is invariant under both A and D.
Since Xsr contains b and is invariant under A and D, LA,D(b) ⊂ Xsr. This completes the
proof.
A characterization of LA,D(b) can be obtained by recursively deﬁning
p 1 = b, pi = [Api −1 Dpi −1] , i = 2,3, . . .
(88)
and letting
Pi = [p 1 p 2 . . . pi]
(89)
Lemma 4.2
The linear subspaces LA,D(b) and R [Pm] are identical.
Proof
The linear subspace R [Pk] is the subspace spanned by the columns of Pk. The
columns of Pk contain those of Pk −1, and the additional columns are generated by
multiplication by A and D. Therefore,
175

R [P1] ⊂R [P2] ⊂. . . ⊂Rm
In particular, there exists a k−1 ≤ m such that
R [P1] ⊂R [P2] ⊂. . . ⊂R [Pk −1] = R [Pk] = . . . = R [Pm] ⊂Rm
and therefore R [Pk −1] = R [Pm] is invariant under A and D and contains b. It remains to
show that R [Pm] is the least-dimension such subspace.
So suppose X ⊂ Rm is any
subspace that contains b and that is invariant under A and D. But X must contain b, Ab,
Db, . . . , that is X ⊂ R [Pm]. Consequently R [Pm] is of least dimension.
This result leads directly to a criterion for span reachability because rank Pm is
precisely the dimension of R [Pm].
Theorem 4.13
The m-dimensional bilinear state equation (85) is span reachable if and
only if rank Pm = m.
I now turn to the problem of developing a suitable observability property for bilinear
state equations. Again, the concept to be used will be deﬁned in a somewhat weaker
fashion than observability for linear state equations.
Deﬁnition 4.3
A state
x 0 ≠ 0 of the
bilinear
state
equation
(85) is
called
indistinguishable (from 0) if the response y (t) with x (0) = x 0 is identical to the response
with x (0) = 0 for every piecewise continuous input signal.
Here, as before, piecewise continuity is speciﬁed just for deﬁniteness. Notice that
the deﬁnition implies nothing about the ability to compute a distinguishable initial state
from knowledge of the response y (t). This issue will be regarded as extraneous to the
structure theory under discussion.
Deﬁnition 4.4
The bilinear state equation (85) is called observable if there are no
indistinguishable states.
To characterize the concept of observability for the bilinear state equation
(A,D,b,c,Rm), it is convenient to let GA,D(c) ⊂ Rm be the largest subspace contained in
N [c ] that is invariant under A and D.
Lemma 4.3
The subset of all indistinguishable states of (A,D,b,c,Rm) is a linear
subspace that is given by Xi = GA,D(c).
Proof
(Sketch) Using the representation derived in Chapter 3, the response of the
bilinear system to arbitrary initial state x 0 and input u (t) can be written as a series, the ﬁrst
few terms of which are
176

y (t) = ce Atx 0 +
0∫
t
ce A(t−σ)De Aσx 0u (σ) dσ +
0∫
t
ce A(t −σ)bu (σ) dσ + . . .
Expanding the matrix exponentials yields terms of the form
y (t) = cx 0 + cAx 0t + cDx 0
0∫
t
u (σ) dσ
+ cb
0∫
t
u (σ) dσ + cAb
0∫
t
(t −σ)u (σ) dσ + . . .
(90)
It should be clear from this expression that the set of indistinguishable states forms a linear
subspace. Also it is easy to see that if x 0 ε GA,D(c), then x 0 is indistinguishable. In other
words, GA,D(c) ⊂ Xi. The reverse containment is obtained by showing that for all real
numbers u,
(A + Du)Xi ⊂Xi
and that
Xi ⊂N [c ]
The details are not hard to ﬁll in, and thus are omitted.
To characterize Xi now involves characterizing GA,D(c). Let
q 1 = c, qi =
H
A
I qi −1D
qi −1A J
A
K
, i = 2,3, . . .
(91)
and
Qi =
H
A
A
A
I qi
...
q 2
q 1 J
A
A
A
K
,
i = 1, 2, . . .
(92)
Then the following result is proved in a manner similar to Lemma 4.2.
Lemma 4.4
The linear subspaces Xi and N [Qm] are identical.
Now the obvious application of linear algebra gives an observability criterion.
Theorem 4.14
The m-dimensional bilinear state equation (85) is observable if and only
if rank Qm = m.
While these concepts are of interest in themselves, the intent here is to use them in
conjunction with the theory of minimal bilinear realizations. To this end there is one more
fact about the matrices Pi and Qi that is crucial.
177

Lemma 4.5
For any j,k = 1,2, . . . , the product QjPk is the same for all bilinear
realizations of a given system.
Proof
Suppose that (A,D,b,c,Rm) and (Aˆ,Dˆ,bˆ,cˆ,Rmˆ ) both are bilinear realizations of a
given system. Then for n = 1,2, . . . , the regular kernels of the two systems give
ce AσnDe Aσn −1D . . . De Aσ1b = cˆe AˆσnDˆe Aˆσn −1Dˆ . . . Dˆe Aˆσ1bˆ
for all σ1, . . . ,σn ≥ 0. Replacing every matrix exponential by its power series expansion
and equating coefﬁcients of like arguments shows that
cA inD . . . DAi 1b = cˆAˆ inDˆ . . . DˆAˆ i 1bˆ
for every n = 1,2, . . . , and every ij ≥ 0. This completes the proof since every element of
the product QjPk has precisely this form.
At this point, almost all the tools needed to characterize minimality for bilinear
realizations are at hand.
The one remaining calculation involves showing that if
(A,D,b,c,Rm) is a realization for a given system, then for any invertible, m x m matrix T,
(TAT−1,TDT−1,Tb,cT−1,Rm) also is a realization for the system. This is left as an easy
exercise.
Theorem 4.15
A bilinear realization of a speciﬁed Volterra system is minimal if and
only if it is span reachable and observable.
Proof
Suppose (85) is a bilinear realization of dimension m for the given Volterra
system, but that it is not span reachable. I will show how to construct another bilinear
realization of dimension < m. Since (A,D,b,c,Rm) is not span reachable, R [Pm] ⊂ Rm and
I can write Rm = R [Pm] ⊕ V, where ⊕denotes direct sum, and V is a linear subspace of
dimension at least 1. Pick a basis for Rm that is the union of a basis w 1, . . . ,wr for R [Pm]
and a basis wr +1, . . . ,wm for V. Letting T−1 be the m × m matrix with i th column wi, then
(TAT−1,TDT−1,Tb,cT−1,Rm) also is an m-dimensional bilinear realization of the given
Volterra system. Furthermore, since R [Pm] contains b and is invariant under A and D, this
new realization is in the partitioned form
TAT−1 =
H
A
I
0
A11
A22
A12 J
A
K
, TDT−1 =
H
A
I
0
D11
D22
D12 J
A
K
Tb =
H
A
I 0
b 1 J
A
K
, cT−1 = [c 1
c 2]
(93)
The 0 blocks in TAT−1 and TDT−1 are (m −r) × r, the 0 block in Tb is (m −r) × 1, and c 1 is
r × 1. Now it is an easy calculation to show that for n = 1,2, . . . and σ1, . . . ,σn ≥ 0,
ce AσnDe Aσn −1D . . . De Aσ1b = c 1e A11σnD11e A11σn −1D11 . . . De A11σ1b 1
Thus (A11,D11,b 1,c 1,Rr) is a bilinear realization of dimension r < m. In a very similar
178

fashion it can be shown that if a bilinear realization is not observable, then it is not
minimal.
Now suppose (A,D,b,c,Rm) and (Aˆ,Dˆ,bˆ,cˆ,Rmˆ ) are span-reachable and observable
bilinear realizations of dimension m and mˆ , respectively, for the given Volterra system.
Letting M = max [m,mˆ ], Lemma 4.4 gives
QMPM = Qˆ MPˆM
(94)
But the m rows of PM and m columns of QM are linearly independent, and the mˆ rows of
PˆM and mˆ columns of Qˆ M are linearly independent. Thus, leaving the details to the reader,
(94) implies m = mˆ . That is, all span-reachable and observable realizations of a given
Volterra system have the same dimension. In the ﬁrst part of the proof it was shown that a
minimal bilinear realization is span reachable and observable. Thus all span-reachable
and observable realizations of a given Volterra system are minimal.
The last step in the characterization of minimal bilinear realizations will be to show
that all such realizations of a given Volterra system are related by a change of variables.
Theorem 4.16
Suppose (A,D,b,c,Rm) is a minimal bilinear realization of a given system.
Then (Aˆ,Dˆ,bˆ,cˆ,Rm) also is a minimal bilinear realization of the system if and only if there
is an invertible matrix T such that A = TAˆT−1, D = TDˆT−1, b = Tbˆ, c = cˆT−1.
Proof
If such a T exists, then sufﬁciency follows from an easy exercise suggested
earlier. For necessity, suppose that both state equations are minimal bilinear realizations
of the given system. Then by Lemma 4.4,
QkPj = Qˆ kPˆ j , k,j = 1, 2, . . .
(95)
and, by Theorem 4.15, Qm, Qˆ m, Pm, and Pˆm all have rank m. In particular, this implies that
(Qm´Qm) is invertible, so if
T = (Qm´Qm)−1Qm´Qˆ m
then
Qm´QmT = Qm´Qˆ m
and
Qm´QmTPˆmPm´ = Qm´Qˆ mPˆmPm´ = Qm´QmPmPm´
Since PmPm´ is invertible, this gives that T is invertible and
T−1 = PˆmPm´(PmPm´)−1
Now (95) with k = 1, j = m implies cPm = cˆPˆm, which implies
cPmPm´ = cˆPˆmPm´
or, c = cˆT−1. Similarly, with k = m, j = 1, (95) becomes Qmb = Qˆ mbˆ, which gives b = Tbˆ.
179

Now note that the columns of APm are contained in the columns of Pm+1, and the columns
of DPm are contained in the columns of Pm+1. Thus (95) implies
QmAPm = Qˆ mAˆPˆm, QmDPm = Qˆ mDˆPˆm
Taking, for example, the ﬁrst of these equalities,
Qm´QmAPmPm´ = Qm´Qˆ mAˆPˆmPm´
or
A = (Qm´Qm)−1Qm´Qˆ mAˆPˆmPm´(PmPm´)−1 = TAˆT−1
The similar calculation for the second equality completes the proof.
4.5 The Nonstationary Case
The transform-domain tools that have been used so extensively in the preceding
sections cannot be used fruitfully for nonstationary systems. Also, the regular kernel has
been developed only for stationary systems, so this leaves the choice of using either
triangular or symmetric kernels in the input/output representation of nonstationary
systems. Since bilinear realizations are of interest, the triangular kernel developed in
Chapter 3 for such state equations will be used, though the results could be rephrased
rather easily in terms of symmetric kernels.
A nonstationary bilinear state equation takes the form
x.(t) = A (t)x (t) + D (t)x (t)u (t) + b (t)u (t)
y (t) = c (t)x (t)
(96)
where all the dimensions are as usual, and the coefﬁcient matrices are nominally assumed
to be continuous functions of t. In Chapter 3 it was shown that such a state equation with
x (0) = 0 yields a Volterra system representation
y (t) =
n =1Σ
∞
0∫
t
0∫
σ1
. . .
0∫
σn −1
h (t,σ1, . . . ,σn)u (σ1) . . . u (σn) dσn . . . dσ1
(97)
where the n th triangular kernel is given by
h (t,σ1, . . . ,σn) = c (t)Φ(t,σ1)D (σ1)Φ(σ1,σ2)D (σ2)Φ(σ2,σ3)
. . . D (σn −1)Φ(σn −1,σn)b (σn)
(98)
and where Φ(t,σ) is the transition matrix for A (t).
To consider the bilinear realization problem for a general Volterra system of the
form (97) is a difﬁcult task. About all that can be said at present is that the Volterra system
is bilinear realizable if and only if there exist appropriately dimensioned, continuous
matrix functions A (t), D (t), b (t), and c (t) such that the kernels can be written in the form
(98); rather like saying it is bilinear realizable if and only if it is bilinear realizable. The
difﬁculty is similar in nature to the difﬁculties that arise in the stationary case. Bilinear
180

realizability of a Volterra system depends both on properties of the individual kernels, and
on the way the kernels interrelate. However, the outlook is brighter for homogeneous and
polynomial systems, and I will concentrate on these cases.
Theorem 4.17
A degree-n homogeneous system described by
y (t) =
0∫
t
0∫
σ1
. . .
0∫
σn −1
h (t,σ1, . . . ,σn)u (σ1) . . . u (σn) dσn . . . dσ1
(99)
is bilinear realizable if and only if the kernel h (t,σ1, . . . ,σn) is separable.
Proof
If the system is bilinear realizable, then the kernel can be written in the form
(98). From properties of the transition matrix, it follows that the kernel is separable. (Just
as in the linear case, the continuity required by separability is furnished by the continuity
assumptions on the bilinear state equation.)
Now suppose that the kernel is separable,
h (t,σ1, . . . ,σn) =
i =1Σ
m
v 0i(t)v 1i(σ1) . . . vni(σn)
For the case of m = 1, it is easy to show that the bilinear state equation
x.(t) =
H
A
A
A
A
I 0
0
...
0
0
0
0
...
0
v 1m(t)
0
0
...
v 2m(t)
0
. . .
. . .
...
. . .
. . .
0
vn −1,m(t)
...
0
0
J
A
A
A
A
K
x (t)u (t) +
H
A
A
A
A
I vnm(t)
0
...
0
0
J
A
A
A
A
K
u (t)
y (t) = [v 0m(t) 0 . . . 0] x (t)
is a degree-n homogeneous system with kernel
h (t,σ1, . . . ,σn) = v 0m(t)v 1m(σ1) . . . vnm(σn)
The proof is now almost complete since an additive parallel connection of these simple
bilinear state equations can be used in the general case. The reason the proof is not
complete is that when m = 1 the vji(.) must be real functions, but for m > 1 they might be
complex. Consideration of these details is left to the reader.
It also is interesting to characterize those homogeneous systems that, although
represented in terms of a nonstationary triangular kernel, actually are realizable by a
constant-parameter bilinear state equation. Once again, the results are similar to the
linear-system results.
Theorem 4.18
A degree-n homogeneous system of the form (99) is realizable by a
constant-parameter bilinear state equation if and only if the kernel h (t,σ1, . . . ,σn) is
stationary and differentiably separable.
181

Proof
If the degree-n homogeneous system has a constant-parameter bilinear
realization, then stationarity and differentiable separability follow easily from the familiar
general form of the kernel.
Now suppose the triangular kernel is stationary and differentiably separable. For
simplicity I will consider the special case
h (t,σ1, . . . ,σn) = v 0(t)v 1(σ1) . . . vn(σn)
where each vj(.) is a real function. (Just as in the linear case, the generalization of the
proof is easy except when complex-valued functions are involved. Then more fussy
arguments are required to show that a real-coefﬁcient realization can be obtained.) The
main part of the proof will be devoted to showing that the kernel can be written in the form
h (t,σ1, . . . ,σn) = c 1e a 1(t−σ1)e a 2(σ1−σ2) . . . e an(σn −1−σn)
for real numbers c 1,a 1, . . . ,an. Once this is established, a bilinear realization is given by
x.(t) =
H
A
A
A
A
I 0
0
...
0
a 1
0
0
...
a 2
0
. . .
. . .
...
. . .
. . .
an
0
...
0
0 J
A
A
A
A
K
x (t) +
H
A
A
A
I 0
0
...
0
0
0
0
...
0
1
0
0
...
1
0
. . .
. . .
...
. . .
. . .
0
1
...
0
0 J
A
A
A
K
x (t)u (t) +
H
A
A
A
I 1
0
...
0
0 J
A
A
A
K
u (t)
y (t) = [ c 1 0 . . . 0 ] x (t)
as is readily veriﬁed by the usual calculation. The basic approach involves proving that
each vj(.) satisﬁes a constant-coefﬁcient linear differential equation of ﬁrst order. To show
this for, say, v 1(σ1), let
q 1 =
−T∫
T
. . .
−T∫
T
v0
2(t)v2
2(σ2) . . . vn
2(σn) dt dσ2 . . . dσn
where T has been chosen so that q 1 > 0. Note that if no such T exists, then the kernel is
identically 0, a trivial case. By stationarity
h (t,σ1, . . . ,σn) = h (0,σ1−t, . . . , σn−t)
so that
k =1Σ
n
∂σk
∂
____ h (t,σ1, . . . ,σn) + ∂t
∂___ h (t,σ1, . . . ,σn) = 0
Computing the derivatives using the separable form gives
v 0(t)v.
1(σ1)v 2(σ2) . . . vn(σn) + v 0(t)v 1(σ1)v.
2(σ2)v 3(σ3) . . . vn(σn)
+ . . . + v 0(t)v 1(σ1) . . . vn −1(σ1)v.
n(σn) + v.
0(t)v 1(σ1) . . . vn(σn) = 0
Multiplying this equation by v 0(t)v 2(σ2) . . . vn(σn) and rearranging gives
182

[v0
2(t)v2
2(σ2) . . . vn
2(σn)]v.
1(σ1) + [v0
2(t)v 2(σ2)v.
2(σ2)v3
2(σ3) . . . vn
2(σn)
+ . . . +v0
2(t)v2
2(σ2) . . . vn(σn)v.
n(σn) + v 0(t)v.
0(t)v2
2(σ2) . . . vn
2(σn)]v 1(σ1) = 0
Both sides of this expression can be integrated with respect to t,σ2, . . . ,σn to obtain
q 1v.
1(σ1) + r 1v 1(σ1) = 0
with the obvious deﬁnition of r 1. Thus v 1(σ1) satisﬁes a constant-parameter linear
differential equation (nontrivial since q 1 ≠ 0). This means that
v 1(σ1) = c 1e a 1σ1
for suitable a 1 and c 1.
A similar development can be carried out to show that
vj(σj) = cje ajσj for j = 2,3, . . . ,n. Now the stationarity condition can be written as
v 0(t)c 1e a 1σ1 . . . cne anσn = v 0(0)c 1e a 1(σ1−t) . . . cne an(σn−t)
= v 0(0)e (a 1+ . . . +an)(−t)c 1e a 1σ1 . . . cne anσn
which gives
v 0(t) = v 0(0)e −(a 1+ . . . +an)t
Thus, with the appropriate redeﬁnition of c 1,
h (t,σ1, . . . ,σn) = c 1e −(a 1+ . . . +an)te a 1σ1 . . . e anσn
= c 1e −(a 1+ . . . +an)(t −σ1)e −(a 2+ . . . +an)(σ1−σ2) . . . e −an(σn −1−σn)
and the proof is complete.
These results for homogeneous systems directly provide bilinear realizability results
for polynomial systems. That is, bilinear realizability for a polynomial system depends on
bilinear realizability of each and every homogeneous subsystem. The easy proof of the
following formalization is left to Section 4.7, with the hint that the proof of Theorem 4.9
merits imitation.
Theorem 4.19
A degree-N polynomial system has a (constant parameter) bilinear
realization if and only if each of the N triangular kernels is (stationary, and differentiably)
separable.
4.6 Remarks and References
Remark 4.1
There is an abundance of material on the linear realization problem, and
only a few references will be listed here. An elementary review for stationary systems,
including the multi-input, multi-output case, can be found in
183

C. Chen, Introduction to Linear System Theory, Holt, Rinehart, and Winston, New York,
1970.
An elementary treatment that emphasizes Hankel (Behavior) matrices and connections
with algebraic properties of rational functions is given in a book that modesty almost
prevents me from mentioning:
W. Rugh, Mathematical Description of Linear Systems, Marcel Dekker, New York, 1975.
A more research-oriented review of the Hankel matrix approach, along with an interesting
discussion of perspectives and open problems is given in
R. Kalman, "Realization Theory of Linear Dynamical Systems," in Control Theory and
Topics in Functional Analysis, Vol. 2, International Atomic Energy Agency, Vienna, pp.
235-256, 1976.
The abstract shift realization used in Section 4.1 is developed from the approach in
E. Gilbert, "Realization Algorithms for Linear Systems and the Role of the Restricted
Backward Shift Realization," Proceedings of the 1978 Conference on Information
Sciences and Systems, Electrical Engineering Department, The Johns Hopkins University,
Baltimore, pp. 145-151, 1978.
Finally, the realization problem for nonstationary linear systems is discussed in
L. Silverman, "Realization of Linear Dynamical Systems," IEEE Transactions on
Automatic Control, Vol. AC-16, pp. 554-568, 1971.
R. Brockett, Finite Dimensional Linear Systems, John Wiley, New York, 1970.
Remark 4.2
An early treatment of the nonlinear realization problem in terms of
interconnections of linear systems and multipliers is given in
M. Schetzen, "Synthesis of a Class of Nonlinear Systems," International Journal of
Control, Vol. 1, pp. 401-414, 1965.
In the degree-2 case, the basic interconnection structure is a cascade connection of a linear
system following a multiplicative parallel connection of two linear systems. Additive
parallel connections of these basic structures also are used. Realizability tests and
realization procedures are developed based on the structural form for the transfer function,
H1(s 1)H2(s 2)H3(s 1+s 2), which arises naturally from interconnection results. The issue of
realizability in terms of a standard form for the transfer function (say, the symmetric
transfer function) is not discussed.
184

Further development of realization ideas based on structural features of symmetric
transfer functions of particular kinds of interconnections of linear systems and multipliers
can be found in the following papers.
W. Smith, W. Rugh, "On the Structure of a Class of Nonlinear Systems," IEEE
Transactions on Automatic Control, Vol. AC-19, pp. 701-706, 1974.
K. Shanmugam, M. Lal, "Analysis and Synthesis of a Class of Nonlinear Systems," IEEE
Transactions on Circuits and Systems, Vol. CAS-23, pp. 17-25, 1976.
T. Harper, W. Rugh, "Structural Features of Factorable Volterra Systems," IEEE
Transactions on Automatic Control, Vol. AC-21, pp. 822-832, 1976.
Treatments of the interconnection realization problem that are not based on particular
interconnection structures are given for degree-2 homogeneous systems in
G. Mitzel, W. Rugh, "On a Multidimensional S-Transform and the Realization Problem for
Homogeneous Nonlinear Systems," IEEE Transactions on Automatic Control, Vol. AC-22,
pp. 825-830, 1977.
E. Gilbert, "Bilinear and 2-Power Input-Output Maps: Finite Dimensional Realizations and
the Role of Functional Series," IEEE Transactions on Automatic Control, Vol. AC-23, pp.
418-425, 1978.
In the ﬁrst of these papers, an algebraic approach to the Laplace transform is developed
based
on
formal
series
representations.
Using
the
recognizability
property,
interconnection realizations are developed from a partial fraction expansion of the given
transfer function. The second paper uses a specialization of an interconnection structure
for so-called bilinear input/output maps (to be discussed in Chapter 6) to arrive at
realizations in the homogeneous case.
Remark 4.3
There are many names in the literature for what are called here bilinear
state equations, including "regular systems," "internally bilinear systems," and "internally
bi-afﬁne systems." There are good reasons for any of these, and the reader is urged to
switch rather than ﬁght. On more substantive matters, an early treatment of the bilinear
realization problem for a given Volterra system appeared in
A. Isidori, A. Ruberti, "Realization Theory of Bilinear Systems," in Geometric Methods in
System Theory, D. Mayne, R. Brockett eds., D. Reidel, Dordrecht, Holland, pp. 81-130,
1973.
Two approaches to the problem are presented. The ﬁrst is a (nonconstructive) factorization
approach for the sequence of triangular kernels, while the second involves a so-called
185

generalized Hankel matrix essentially the same as the Behavior matrix in Section 4.3. It is
interesting to note the implicit use of the regular kernel in this development. The concepts
of span reachability and observability are introduced, and are essential tools in the
realization theory. Much of the basic content of this paper also can be found in the papers
P. D’Alessandro, A. Isidori, A. Ruberti, "Realization and Structure Theory of Bilinear
Dynamical Systems," SIAM Journal on Control, Vol. 12, pp. 517-535, 1974.
A. Isidori, "Direct Construction of Minimal Bilinear Realizations," IEEE Transactions on
Automatic Control, Vol. AC-18, pp. 626-631, 1973.
Another early paper dealing with bilinear realization is
R. Brockett, "On the Algebraic Structure of Bilinear Systems," in Theory and Application
of Variable Structure Systems, R. Mohler, A. Ruberti eds., Academic Press, New York, pp.
153-168, 1972. Equivalences for various forms of bilinear state equations and the notions
of span reachability and observability are emphasized in this paper.
Remark 4.4
A much different approach to the bilinear realization problem is given in
M. Fliess, "Sur la Realization des Systemes Dynamiques Bilineaires," C. R. Academie
Science, Paris, Series A, Vol. 277, pp. 923-926, 1973. though I recommend the less terse
account in
M. Fliess, "Un Outil Algebrique: les Series Formelles Noncommutatives," in Mathematical
System Theory, G. Marchesini, S. Mitter, eds., Lecture Notes in Economics and
Mathematical Systems, Vol. 131, Springer-Verlag, New-York, pp. 122-148, 1976.
This approach involves representing input/output behavior in terms of formal series in
noncommuting variables. To indicate in simple terms the nature of the formulation,
consider a Volterra system representation in triangular form:
y (t) = h 0(t) +
0∫
t
h 1(t,σ1)u (σ1) dσ1 +
0∫
t
0∫
σ2
h 2(t,σ1,σ2)u (σ1)u (σ2) dσ1dσ2 + . . .
Suppose that h 0(t) is analytic for t ≥ 0, and that each of the kernels is analytic on its
respective domain t ≥ σn ≥  . . .  ≥ σ1 ≥ 0. Then power series representations of the form
186

h 0(t) =
j =0Σ
∞
hj j !
t j
___
h 1(t,σ1) =
j 0=0
Σ
∞
j 1=0
Σ
∞
hj 0j 1
j 0!j 1!
(t −σ1)j 1σ1
j 0
__________
h 2(t,σ1,σ2) =
j 0=0
Σ
∞
j 1=0
Σ
∞
j 2=0
Σ
∞
hj 0j 1j 2
j 0!j 1!j 2!
(t −σ2)j 2(σ2−σ1)j 1σ1
j 0
__________________
...
can be used. These kernel representations provide a means of associating to the system a
noncommutative formal series in two variables (or, a formal series in two noncommuting
variables),
W =
j =0Σ
∞
hjw0
j +
j 0=0
Σ
∞
j 1=0
Σ
∞
hj 0j 1w0
j 1w 1w0
j 0 +
j 0=0
Σ
∞
j 1=0
Σ
∞
j 2=0
Σ
∞
hj 0j 1j 2w0
j 2w 1w0
j 1w 1w0
j 0 + . . .
The correspondence between the Volterra system representation and the noncommutative
series representation should be clear from just these ﬁrst "few" terms. Notice that the
noncommutativity is crucial, for if w 0 and w 1 commute, then it is impossible to distinguish
between terms. For example, commutativity would imply
w 0w 1w0
2w 1w0
3 = w0
6w1
2 = w0
2w 1w0
2w 1w0
2
Now, input/output properties of the system can be interpreted as properties of the series.
For example, W represents linear input/output behavior if and only if each nonzero term
contains precisely one occurrence of the variable w 1. Also, simple manipulations show
that the system represented by W is stationary if and only if each nonzero term in W ends
with the variable w 1, except for the constant term. In other words, if and only if W has the
form
W = h 0 +
j 1=0
Σ
∞
h 0j 1w0
j 1w 1 +
j 1=0
Σ
∞
j 2=0
Σ
∞
h 0j 1j 2 w0
j 2w 1w0
j 1w 1 + . . .
The bilinear realization problem for a system described by W is set up most naturally in
terms of bilinear state equations of the form
x.(t) = Ax (t) + Dx (t)u (t)
y (t) = cx (t) , x (0) = x 0
(Recall Problem 3.12.) Applying the resubstitution method to this state equation yields a
series expression that can be written in the form
187

y (t) = c [I + A
0∫
t
dσ + D
0∫
t
u (σ) dσ + A2
0∫
t
0∫
σ1
dσ2dσ1 + AD
0∫
t
0∫
σ1
u (σ2) dσ2dσ1
+ DA
0∫
t
u (σ1)
0∫
σ1
dσ2dσ1 + D2
0∫
t
u (σ1)
0∫
σ1
u (σ2) dσ2dσ1
+ A3
0∫
t
0∫
σ1
0∫
σ2
dσ3dσ2dσ1 + . . . ]x 0
Notice that the coefﬁcient matrix products correspond in a natural way to the order of the
iterated integrals of either 1 or u (t). These iterated integrals can be denoted by a monomial
in two variables, w 0 and w 1, to yield a noncommutative series representation for the
response of the bilinear system,
y = cx 0 + cAx 0w 0 + cDx 0w 1 + cA 2x 0w0
2 + cADx 0w 0w 1 + cDAx 0w 1w 0
+ cD 2x 0w1
2 + cA 3x 0w0
3 + . . .
Of course, this is a noncommutative series because
0∫
t
0∫
σ1
u (σ2) dσ2dσ1 ≠
0∫
t
u (σ1)
0∫
σ1
dσ2dσ1
that is, w 0w 1 ≠ w 1w 0.
Now a bilinear-realizability result can be stated immediately. A system represented
by W is bilinear realizable if and only if there exist two m × m matrices A and D, an m × 1
vector x 0, and a 1 × m vector c such that the coefﬁcient of w0
jkw 1w0
jk −1  . . . w 1w0
j 0 in W is
given by cA jkDA jk −1 . . . DA j 0x 0. This condition is equivalent to a rationality condition in
the algebraic theory of noncommutative series, and a quick glance at the references will
show that this is only the beginning of the story. The concepts of minimality, span
reachability, observability, and even a Behavior matrix, all can be formulated from the
theory. In fact the regular transfer function representation for stationary Volterra systems
can be deﬁned as a commutative series that can be obtained by associating the k-variable
commutative monomial s1
−(j 1+1) . . . sk
−(jk+1) to the 2-variable noncommutative monomial
w0
jkw 1 . . . w0
j 1w 1. This connection is discussed in
M. Fliess, "A Remark on Transfer Functions and the Realization of Homogeneous
Continuous-Time Systems," IEEE Transactions on Automatic Control, Vol. AC-24, pp.
507-508, 1979.
Remark 4.5
The shift realization approach to bilinear realization theory that I have used
so extensively is based on
A. Frazho, "A Shift Operator Approach to Bilinear System Theory," SIAM Journal on
188

Control and Optimization, Vol. 18, pp. 640-658, 1980.
The polynomial factorization approach for homogeneous systems is taken from
G. Mitzel, S. Clancy, W. Rugh, "On Transfer Function Representations for Homogeneous
Nonlinear Systems," IEEE Transactions on Automatic Control, Vol. AC-24, pp. 242-249,
1979.
Remark 4.6
The importance of bilinear systems can be further substantiated by
considering the approximation result established in
H. Sussman, "Semigroup Representations, Bilinear Approximation of Input-Output Maps,
and Generalized Inputs," in Mathematical System Theory, G. Marchesini, S. Mitter, eds.,
Lecture Notes in Economics and Mathematical Systems, Vol. 131, Springer-Verlag, New
York, 1976.
For single-input, single-output systems, the result can be outlined as follows. The input
space U consists of all measurable functions u (t) deﬁned on [0,T] and satisfying
|u (t)| ≤ M for all t ε [0,T], where T and M are ﬁxed. The output signal is given in
operator notation by y = F [u ]. It is assumed that F is causal, and continuous in the sense
that the sequence of output signals F [uk], k = 0,1, . . . , converges uniformly to F [u ]
whenever the sequence of input signals converges weakly to the input u. Then for every
ε > 0 there is a bilinear realization whose operator representation y = B [u ] satisﬁes
|F [u ] − B [u ]| < ε for all t ε [0,T] and all u ε U.
Similar results have been obtained using the noncommutative series representations
discussed in Remark 4.4. See
M. Fliess, "Series de Volterra et Series Formelles Non Commutatives," C. R. Academie
Science, Paris, Series A, Vol. 280, pp. 965-967, 1975.
M. Fliess, "Topologies pour Certaines Functions de Lignes Non Lineaires; Application aux
Asservissements," C. R. Academie Science, Paris, Series A, Vol. 282, pp. 321-324, 1976.
Remark 4.7
Of course, other kinds of realizations can be discussed in addition to
bilinear realizations. Linear-analytic state equations have been studied in this regard,
though not nearly to the extent of bilinear state equations. See
R. Brockett, "Volterra Series and Geometric Control Theory," Automatica, Vol. 12, pp.
167-176, 1976 (addendum with E. Gilbert, Vol. 12, p. 635).
It is not hard to show that a homogeneous or polynomial system is linear-analytic
realizable if and only if it is bilinear realizable. The point is that a minimal linear-analytic
realization can be of lower dimension than a minimal bilinear realization. For the
189

homogeneous case, a procedure for computing a minimal linear-analytic realization for a
bilinear-realizable system is given in
M. Evans, "Minimal Realizations of k-Powers," Proceedings of the 1980 Conference on
Information Sciences and Systems, Department of Electrical Engineering and Computer
Science, Princeton University, Princeton, New Jersey, pp. 241-245, 1980.
For polynomial systems, the minimal linear-analytic realization problem is discussed in
P. Crouch, "Dynamical Realizations of Finite Volterra Series," SIAM Journal on Control
and Optimization, Vol. 19, pp. 177-202, 1981.
In the Volterra system case, much remains to be done. The kinds of things that happen
when considering linear-analytic realizations are indicated by a rather simple example.
Consider the system with input/output behavior
y (t) = tanh [
0∫
t
u (σ) dσ]
Using the power series expansion of the hyperbolic tangent about 0 gives a Volterra
system representation of the form
y (t) =
n =1Σ
∞
0∫
t
. . .
0∫
t
n !
αn
___ u (σ1) . . . u (σn) dσ1 . . . dσn
where alphas are used because the actual coefﬁcients are rather complicated. In triangular
form, the Volterra system can be reresented by
y (t) =
n =1Σ
∞
0∫
t
0∫
σ1
. . .
0∫
σn −1
αn u (σ1) . . . u (σn) dσn . . . dσ1
This Volterra system has a scalar linear-analytic realization, namely,
x.(t) = [1 −x 2(t)]u (t)
y (t) = x (t) , x (0) = 0
but no ﬁnite-dimensional bilinear realization. In addition to showing that linear-analytic
realizability and bilinear realizability for Volterra systems are not equivalent, this example
shows why inﬁnite-dimensional bilinear realizations might be of interest. For a simple
calculation of triangular kernels shows that the Volterra system has a realization of the
form
190

dt
d
___
H
A
A
A
I
...
x 3(t)
x 2(t)
x 1(t) J
A
A
A
K
=
H
A
A
I ...
0
1
0
...
1
0
0
...
0
0
0
...
. . .
. . .
. . . J
A
A
K
H
A
A
A
I
...
x 3(t)
x 2(t)
x 1(t) J
A
A
A
K
u (t) +
H
A
A
I ...
0
0
1 J
A
A
K
u (t)
y (t) = [α1 α2 α3 . . . ]
H
A
A
A
I
...
x 3(t)
x 2(t)
x 1(t) J
A
A
A
K
Inﬁnite-dimensional bilinear realizations are discussed in the paper by Frazho mentioned
in Remark 4.5, and in
G. Koch, "A Realization Theorem for Inﬁnite Dimensional Bilinear Systems," Ricerche di
Automatica, Vol. 3, 1972.
R. Brockett, "Finite and Inﬁnite Dimensional Bilinear Realization," Journal of the
Franklin Institute, Vol. 301, pp. 509-520, 1976.
W. Wong, "Volterra Series, Universal Bilinear Systems, and Fock Representations,"
Proceedings of the 1979 Conference on Information Sciences and Systems, Electrical
Engineering Department, The Johns Hopkins University, Baltimore, pp. 207-213, 1979.
Of course, realizations in terms of state equations more general than linear-analytic also
can be considered. A transform-domain characterization of realizability and minimality for
degree-2 homogeneous systems in terms of very general state equations is given in
E. Gilbert, "Minimal Realizations for Nonlinear I-O Maps:The Continuous-Time 2-Power
Case," Proceedings of the 1978 Conference on Information Sciences and Systems,
Electrical Engineering Department, The Johns Hopkins University, Baltimore, pp. 308-
316, 1978.
Further results, including a canonical form for minimal realizations and the fact that the
state spaces of minimal realizations are related by a particular type of homeomorphism,
are to appear in
E. Gilbert, "Minimal Realizations for Continuous-Time 2-Power Input-Output Maps,"
IEEE Transactions on Automatic Control, Vol. AC-26, 1981.
4.7 Problems
4.1. Suppose the Behavior matrix BH in (15) for a given linear system has rank n. Show
that the ﬁrst n columns of BH are linearly independent.
191

4.2. If the Behavior matrix BH in (15) for a given linear system has rank n, let
A1 =
H
A
A
A
I hn −1
...
h 1
h 0
hn
...
h 2
h 1
. . .
...
. . .
. . .
h 2n −2
...
hn
hn −1 J
A
A
A
K
, A2 =
H
A
A
A
I hn
...
h 2
h 1
hn +1
...
h 3
h 2
. . .
...
. . .
. . .
h 2n −1
...
hn +1
hn
J
A
A
A
K
Show that
A = A2A1
−1 , b =
H
A
A
A
I hn −1
...
h 1
h 0 J
A
A
A
K
, c = [1 0 . . . 0]
is a minimal realization of the system. (Note that A1
−1 exists by Problem 4.1.)
4.3.
If
H (s)
is
a
strictly
proper
rational
function,
and
the
linear
space
U = span {H (s), SH (s), S 2H (s),  . . .  }
has
dimension
m, show that
H (s), SH (s),
 . . . , S m−1H (s) is a basis for U.
4.4. Suppose a degree-2 homogeneous system is described by the strictly proper regular
transfer function
Hreg(s 1,s 2) = s 1s 2 + 1
1
________
Compute the dimension of the linear space U deﬁned in (48) and (49).
4.5. Find a minimal bilinear realization for the square-integral computer discussed
in Example 1.4. Give another state-equation realization that has lower dimension than the
minimal bilinear realization.
4.6. Compute a minimal bilinear realization for the degree-2 polynomial system
Hˆ (s 1,s 2) = ( s 1+1
1
_____ , (s 1+2)(s 2+1)
1
____________ , 0, . . . )
4.7. Show that a degree-N truncation of the Volterra system in Example 4.6 has a minimal
bilinear realization of dimension N.
4.8. For the Volterra system
Hˆ (s 1, . . . ,s ∞) = ( s 1+1
k 1
_____ , (s 1+1)(s 2+1)
k 2
____________ , (s 1+1)(s 2+1)(s 3+1)
k 3
__________________ , . . . )
suppose the numerator coefﬁcients are such that
192

k 1s −1 + k 2s −2 + k 3s −3 + . . .
corresponds to a strictly proper rational function. Show that the system is bilinear
realizable.
4.9. Does the shift realization approach yield a simple, block-partitioned structure for
bilinear realizations in the Volterra system case?
4.10. Show that the system described by
Hreg(s 1, . . . ,sn) = Q1(s 1)Q2(s 2) . . . Qn(sn)
1
_____________________
is realized by the interconnection structured system shown below.
4.11. For the case where the numerator of the regular transfer function in (31) is a
constant, show that the bilinear realization in (40) is minimal if and only if every linear
realization in (39) is minimal.
4.12. For the bilinear state equation
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t)
suppose that the state vector is changed according to z (t) = Tx (t), where T is an n × n,
invertible matrix. Find the state equation in terms of z (t).
4.13. Show that the bilinear state equation (A,D,b,c,Rm) is span reachable if and only if
(TAT−1,TDT−1,Tb,cT−1,Rm) is span reachable.
4.14. Suppose A and D are m × m matrices, and X is a linear space that is invariant under
(A + Du) for all real numbers u. Show that X is invariant under both A and D.
4.15 Prove Theorem 4.19.
4.16.
Suppose a degree-n homogeneous system is described by a strictly proper,
recognizable
regular transfer function wherein all the roots of the denominator
polynomials have negative real parts. Show that if (A,D,b,c,Rm) is a minimal bilinear
realization of the system, then all the eigenvalues of A will have negative real parts. Show
also that the system is bounded-input, bounded-output stable.
193
…
Π
Π
…
_____
1
Q2(s)
u
Π
_____
1
Q1(s)
_____
y
Qn(s)
1

4.17. Suppose a bilinear-realizable system is connected in cascade with a linear-realizable
system. Show that the overall system is bilinear realizable, regardless of the ordering of
the two systems in the cascade. (Do not peek at Appendix 4.1.)
4.18 Polynomial systems of certain types can be represented by the sum of the regular
transfer functions of the homogeneous subsystems. Show how this works by redoing
Example 4.6 beginning with
Hˆ (s 1,s 2) = s 1+1
1
_____ + (s 1+2)(s 2+3)
1
____________ = (s 1+1)(s 1+2)(s 2+3)
s 1s 2+4s 1+2s 2+7
__________________
and slightly modifying the realization procedure.
4.19 This problem considers further the representation suggested in Problem 4.18. Show
that a degree-N polynomial system is bilinear realizable if and only if the sum of the
subsystem regular transfer functions is a recognizable function which is strictly proper in
s 1 and proper in s 2, . . . , sN.
APPENDIX 4.1 Interconnection Rules for the Regular Transfer
Function
The
derivation
of
interconnection
rules
for
the
regular
transfer
function
representation apparently best proceeds in a manner closely tied to the structure of bilinear
state equations. This approach is in contrast to the development of interconnection rules
for the other transfer function representations . At any rate, Chapters 3 and 4 have
provided the theory needed to generate a table of regular transfer functions, and it is the
purpose of this appendix to present such a table, and to show how it is derived.
The types of systems to be considered are interconnections of linear systems and
homogeneous bilinear systems (all ﬁnite dimensional).
The linear systems will be
described in terms of a state equation
z.(t) = Fz (t) + gu (t)
y (t) = hz (t)
or a (strictly proper, rational) transfer function
H (s) = h (sI −F)−1g
The homogeneous bilinear systems will be described in terms of a state equation
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t)
or a (strictly proper, recognizable) regular transfer function as given in (30) of Section 4.2:
194

Hreg(s 1, . . . ,sn) = c (snI−A)−1D(sn −1I−A)−1D . . . (s 1I−A)−1b
(As usual, the dimension of identity matrices will be ﬁxed by conformability requirements,
and will not be indicated by notation.)
The fact that a degree-n homogeneous bilinear state equation can be assumed to be
in the block partitioned form (40) will be important. This block form is repeated below for
convenience.
x (t) =
H
A
A
A
I xn(t)
...
x 2(t)
x 1(t) J
A
A
A
K
, A =
H
A
A
A
I
0
...
0
A1
0
...
A2
0
. . .
...
. . .
. . .
An
...
0
0 J
A
A
A
K
, D =
H
A
A
A
A
I
0
...
0
D1
0
0
...
D2
0
0
. . .
...
. . .
. . .
. . .
Dn −1
...
0
0
0
0
...
0
0
0 J
A
A
A
A
K
c = [0 . . . 0 cn] , b =
H
A
A
A
I 0
...
0
b 1 J
A
A
A
K
In terms of this block form, the regular transfer function can be written as
Hreg(s 1, . . . ,sn) = cn(snI−An)−1Dn −1(sn −1I−An −1)−1Dn −2 . . . (s 1I−A1)−1b 1
To generate a table of interconnection formulas, the basic idea is similar to
Carleman linearization. The ﬁrst step is to write a composite state equation for the overall
system in terms of subsystem state equations. The second step is to derive a differential
equation for the new "Kronecker product variables" that appear in the composite state
equation. Finally, if all these equations can be written as a big bilinear state equation, then
the regular transfer function of the overall system is easy to compute, in principle.
To illustrate this procedure, consider the next-to-last entry in Table 4.1; the
multiplicative parallel connection of a linear system and a degree-2 homogeneous bilinear
system. Making use of the block form of the homogeneous bilinear state equation, the
overall system can be described by:
z.(t) = Fz (t) + gu (t)
x.
1(t) = A1x 1(t) + b 1u (t)
x.
2(t) = A2x 2(t) + D1x 1(t)u (t)
y (t) = hz (t)c 2x 2(t)
Using the Kronecker product, the (scalar) output equation can be written in the form
y (t) = [hz (t)]⊗[c 2x 2(t)] = [h ⊗c 2][z (t)⊗x 2(t)]
Now, a bilinear equation for z (t)⊗x 2(t) is easily computed:
195

dt
d
___ [z (t)⊗x 2(t)] = z.(t)⊗x 2(t) + z (t)⊗x.
2(t)
= [F ⊗I + I ⊗A2][z (t)⊗x 2(t)] + [g ⊗I]x 2(t)u (t)
+ [I ⊗D1][z (t)⊗x 1(t)]u (t)
For the new term z (t)⊗x 1(t), a similar calculation gives a bilinear equation
dt
d
___ [z (t)⊗x 1(t)] = [F ⊗I + I ⊗A1][z (t)⊗x 1(t)] + [g ⊗I]x 1(t)u (t)
+ [I ⊗b 1]z (t)u (t)
Collecting all the state equations, and letting
xˆ(t) =
H
A
A
A
A
I z (t)⊗x 1(t)
z (t)⊗x 2(t)
x 2(t)
x 1(t)
z (t)
J
A
A
A
A
K
gives a bilinear state equation description of the multiplicative connection in block
partitioned form:
dt
d
___ xˆ(t) =
H
A
A
A
A
I 0
0
0
0
F
0
0
0
A1
0
0
0
A2
0
0
0
[F ⊗I+I ⊗A1]
0
0
0
[F ⊗I+I ⊗A2]
0
0
0
0
J
A
A
A
A
K
xˆ(t)
+
H
A
A
A
A
I
0
I ⊗b 1
0
0
0
0
g ⊗I
D1
0
0
g ⊗I
0
0
0
0
I ⊗D1
0
0
0
0
0
0
0
0
0 J
A
A
A
A
K
xˆ(t)u (t) +
H
A
A
A
A
I 0
0
0
b 1
g J
A
A
A
A
K
u (t)
Finally, a simple calculation gives the regular transfer function corresponding to this
block-partitioned bilinear state equation, as shown in Table 4.1.
The remaining entries in Table 4.1 are derived in the same way. In each case, a
bilinear state equation is derived for the overall system, and then the regular transfer
function is computed from the state equation. Of course, the reader will recall from
Chapter 3 that the reduced Kronecker product can be used to obtain interconnection
formulas that are more economical of dimension, but less explicit.
196

Table 4.1
Interconnection Table for Regular Transfer Functions*
Hreg(s 1, . . . ,sn) = c 1(s 1I −A1)−1b 1c 2(s 2I −A2)−1b 2 . . . cn(snI −An)−1bn
Hreg(s 1,s 2,s 3) = h (s 3I −F)−1gc 2(s 2I −A2)−1D1(s 1I −A1)−1b 1
Hreg(s 1, . . . ,s 4) = c 2(s 4I −A2)−1(D1 ⊗h)(s 3I −A1 ⊗I −I ⊗F)−1
[(I ⊗g)(s 2I −A1)−1b 1h + ((b 1h)⊗I)(s 2I −F ⊗I −I ⊗F)−1
(g ⊗I + I ⊗g)](s 1I −F)−1g
Hreg(s 1, . . . ,sn) = h (snI − F)−1gcn(snI − An)−1Dn −1(sn −1I − An −1)−1Dn −2
. . . D1(s 1 −A1)−1b 1
Hreg(s 1,s 2) = c 2(s 2I −A2)−1(D1 ⊗h)(s 2I −A1 ⊗I −I ⊗F)−1
[(I ⊗g)(s 1I −A1)−1b 1h + ((b 1h)⊗I)(s 2I −F ⊗I −I ⊗F)−1
(g ⊗I + I ⊗g)](s 1I −F)−1g
197
c2(s2I – A2)–1 D1 (s1I – A1)–1 b1
Π
h(sI – F)–1 g
Π
h(sI – F)–1 g
c2(s2I – A2)–1 D1 (s1I – A1)–1 b1
h(sI – F)–1 g
c2(s2I – A2)–1 D1 (s1I – A1)–1 b1
u
c1(sI – A1)–1 b1
…
y
Π
Π
cn(sI – An)–1 bn
Π
…
c2(sI – A2)–1 b2
cn(snI – An)–1 Dn–1 (sn–1I – An–1 )–1 Dn–2 …(s1I – A1)–1 b1
h(sI – F)–1 g

Hreg(s 1,s 2) = (c ⊗h)(s 2I −A ⊗I −I ⊗F)−1[(I ⊗g)(s 1I −A)−1b
+ (b ⊗I)(s 1I −F)−1g]
Hreg(s 1,s 2,s 3) = (h ⊗c 2)(s 3I −F ⊗I −I ⊗A2)−1{(g ⊗I)(s 2I −A2)−1D1(s 1I −A1)−1b 1
+ (I ⊗D)(s 2I −F ⊗I −I ⊗A1)−1[(I ⊗b 1)(s 1I −F)−1g
+ (g ⊗I)(s 1I −A1)−1b 1]}
Hreg(s 1, . . . ,sn) = h (snI −F)−1gcˆ(snI −Aˆn)−1bˆn(sn −1I −Aˆn −1)−1bˆn −1 . . . (s 1I −Aˆ 1)−1bˆ 1
where
cˆ = c ⊗c ⊗ . . .  ⊗c (n factors)
Aˆ j = I ⊗. . . ⊗I ⊗A + I ⊗. . . ⊗I ⊗A ⊗I
+ . . . + A ⊗I ⊗. . . ⊗I (j terms)
bˆj = I ⊗. . . ⊗I ⊗b + I ⊗. . . ⊗I ⊗b ⊗I
+ . . . + b ⊗I ⊗. . . ⊗I (j terms)
* Π denotes time-domain multiplication, ⊗denotes Kronecker product.
198
Π
h(sI – F)–1 g
c(sI – A)–1 b
Π
h(sI – F)–1 g
c2(s2I – A2)–1 D1 (s1I – A1)–1 b1
(.)n
c(sI – A)–1 b
h(sI – F)–1 g

CHAPTER 5
RESPONSE CHARACTERISTICS OF STATIONARY SYSTEMS
Methods for computing the response of a homogeneous system to a speciﬁed input
signal have been discussed in previous chapters. The integrations can be carried out in the
time-domain representation, or the association of variables method can be used in the
transform domain. In terms of the regular transfer function, a more explicit approach can
be used when the input is a sum of exponentials. Response computation for a polynomial
system is simply a matter of adding the homogeneous-subsystem responses, though a
convenient notation can be hard to ﬁnd. The same is true of Volterra systems, with the
additional complication of convergence issues.
For speciﬁc types of input signals, the response of a homogeneous system has
special features that generalize well known properties of linear systems. This is especially
true in the stationary system case, and thus I will discuss only that situation. The response
to impulse inputs, the steady-state response to sinusoidal inputs, and properties of the
response to stochastic inputs will be considered. Most of the discussion will be in terms of
the symmetric kernel or symmetric transfer function. This is both a matter of tradition, and
a result of the fact that the formulas usually appear in a simple form when expressed in
terms of symmetric representations. The material in this chapter will be useful in
connection with the identiﬁcation problem to be discussed in Chapter 7.
5.1 Response to Impulse Inputs
In this section the response of homogeneous systems to inputs composed of impulse
functions will be computed. For the polynomial or Volterra system cases, not much can be
done other than to add up the homogeneous-subsystem responses. The symmetric kernel
representation will be used throughout this section. Of course, these kernels are assumed
to be impulse free, so that the impulse response is guaranteed to be deﬁned.
Surely I will bore the reader by reminding that for the linear system
y (t) =
0∫
t
h (t −σ)u (σ) dσ
(1)
the input u (t) = δ0(t) yields the response y (t) = h (t), t ≥ 0. That is, the impulse response
199

of a linear system traces out the kernel. For a degree-n (> 1) homogeneous system
y (t) =
0∫
t
hsym(t −σ1, . . . ,t −σn)u (σ1) . . . u (σn) dσ1 . . . dσn
(2)
the input δ0(t) yields the response hsym(t, . . . ,t), t ≥ 0.
More interesting calculations arise when inputs composed of sums of impulses are
considered. For example, suppose the input to (2) is
u (t) = δ0(t) + δ0(t −T), T > 0
(3)
One way to compute the response is to multiply out the expression
u (σ1) . . . u (σn) = [δ0(σ1) + δ0(σ1−T)] . . . [δ0(σn) + δ0(σn−T)]
(4)
and integrate over each term. This is not difﬁcult because symmetry and some simple
combinatorics come to the rescue. The indices can be permuted so that the general term
arising in the product in (4) takes the form
δ0(σ1) . . . δ0(σm)δ0(σm+1−T) . . . δ0(σn−T)
without changing the outcome of the integrations. In fact, there will be B
D m
nE
G terms from
(4) that can be written in this particular form. Thus, the response is
y (t) =
m=0
Σ
n
B
D m
nE
G hsym(
m
t, . . . ,t;
n −m
t −T, . . . ,t −T)
(5)
Now consider the general case where the input to the degree-n system (2) is
u (t) = δ0(t) + δ0(t −T1) + . . . + δ0(t −Tp −1)
(6)
where T1, . . . ,Tp −1 is a set of distinct positive numbers. (Portions of the following
analysis should be reminiscent of Section 2.4.) Again, the procedure is to expand the
product
u (σ1) . . . u (σn) = [δ0(σ1)+ . . . +δ0(σ1−Tp −1)] . . . [δ0(σn)+ . . . +δ0(σn−Tp −1)]
and then perform the integration over each term. But permutation of indices does not
affect these integrations, and so the general term in the product can be written in the form
δ0(σ1) . . . δ0(σm1)δ0(σm1+1−T1) . . . δ0(σm1+m2−T1) . . . δ0(σn −mp+1−Tp −1) . . . δ0(σn−Tp −1)
Counting the number of terms that can be written in this way for a particular m1, . . . ,mp
yields multinomial coefﬁcients, and the response is given by
y (t) =
mΣ m1! . . . mp!
n !
___________ hsym(
m1
t, . . . ,t ; . . . ;
mp
t −Tp −1, . . . ,t −Tp −1)
(7)
where
mΣ is a p-fold summation over all integer indices m1, . . . ,mp such that 0 ≤ mi ≤ n
and m1+ . . . +mp = n.
200

5.2 Steady-State Response to Sinusoidal Inputs
For the remainder of this chapter, steady-state response properties will be the subject
of principal interest. Thus consideration of input/output stability properties is needed to
insure that the steady-state response is bounded. In the time domain, it is apparent from
bounding calculations in Section 1.3 that a sufﬁcient condition for bounded input, bounded
output stability of a degree-n homogeneous system is
−∞∫
∞
|hsym(t 1, . . . ,tn)| dt 1 . . . dtn < ∞
But in terms of transform representations, conditions are more difﬁcult to ﬁnd. A well
known condition for linear systems described by reduced rational transfer functions is that
a system is bounded input, bounded output stable if and only if all the poles of the transfer
function have negative real parts. In the degree-n case (n > 1), a sufﬁcient condition of
similar type can be given for systems described by strictly proper, recognizable, regular
transfer functions (Problem 4.16). Unfortunately, this result is much less simple to state in
terms of the symmetric transfer function. Furthermore, the difﬁculty in factoring general
symmetric polynomials makes conditions on the factors very hard to check. Thus the
stability properties needed for a valid steady-state analysis will simply be assumed.
For a stationary linear system described by
y (t) =
0∫
t
h (σ)u (t −σ) dσ
(8)
consider the response to the one-sided input signal
u (t) = 2Acos(ωt), t ≥0
(9)
It is more convenient to write this input in the complex exponential form
u (t) = Ae iωt + Ae −iωt
(10)
for then
y (t) = A
0∫
t
h (σ)e iω(t −σ) dσ + A
0∫
t
h (σ)e −iω(t −σ) dσ
(11)
or
y (t) = A[
0∫
t
h (σ)e −iωσ dσ]e iωt + A[
0∫
t
h (σ)e iωσ dσ]e −iωt
(12)
Assuming the system is stable, as t → ∞the integrals converge to H (iω) and
H (−iω), respectively, where
201

H (s) =
0∫
∞
h (σ)e −sσ dσ
(13)
is the system transfer function. Thus, by picking T large enough, it can be guaranteed that
for all t ≥ T the system response is within a speciﬁed tolerance of the so-called steady-state
response
yss(t) = AH(iω)e iωt + AH (−iω)e −iωt
(14)
Of course using standard identities this steady-state response can be rewritten in the forms
yss(t) = 2A Re [H (iω)] cos(ωt) −2A Im[H (iω)] sin(ωt)
(15)
or
yss(t) = 2A |H (iω)|cos[ωt + ∠H (iω)]
(16)
where standard notations have been used for real part, imaginary part, magnitude, and
angle. These calculations simply make explicit the well known fact that the steady-state
response of a linear system to a sinusoidal input of frequency ω is a sinusoid of the same
frequency, with amplitude and phase determined by the magnitude and angle of the
transfer function evaluated at s = ω. (I should point out that there is another way to view
the steady state. The input can be considered to begin at t = −∞, and then the response at
any ﬁnite t is the steady-state response.)
Now consider the generalization of these results to homogeneous systems described
by
y (t) =
0∫
t
hsym(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(17)
But before I begin, it seems wise to point out a common pitfall in discussing the response
of a nonlinear system to sinusoidal inputs. When working with linear systems, it is
common to consider the input (9) as the real part of the phasor 2Ae iωt. Then the response
of the system to this complex input is calculated, and the response of the system to (9) is
found simply by taking the real part of the response to the phasor. However, this shortcut
depends crucially on the assumption of linearity, as the following example shows.
Example 5.1
To compute the response of the system y (t) = u 2(t) to the input (9),
application of the input u 1(t) = 2Ae iωt gives the response y 1(t) = 4A2e i 2ωt. Then an
erroneous conclusion is that the system response to (9) is y (t) = 4A2cos(2ωt). It is
erroneous
because
direct
application
of
(9)
gives
y (t) = 4A2cos2(ωt)
=
2A2 + 2A2cos(2ωt).
With the one-sided input signal (10) the response of (17) can be computed from
202

y (t) =
0∫
t
hsym(σ1, . . . ,σn)
j =1
Π
n
[Ae iω(t −σj)+Ae −iω(t −σj)] dσ1 . . . dσn
(18)
To put this expression in more useful form, I will mimic the double-exponential-input
development of Section 2.4. Letting λ1 = iω and λ2 = −iω facilitates this development, for
then
y (t) = An
0∫
t
hsym(σ1, . . . ,σn)
k 1=1
Σ
2
. . .
kn=1
Σ
2
exp [
j =1Σ
n
λkj(t −σj)] dσ1 . . . dσn
= An
k 1=1
Σ
2
. . .
kn=1
Σ
2
H
A
A
I 0∫
t
hsym(σ1, . . . ,σn) exp(−
j =1Σ
n
λkjσj) dσ1 . . . dσn
J
A
A
K
exp(
j =1Σ
n
λkjt)
(19)
In a manner similar to the linear case, consider the response for large values of t.
Assuming stability of the system, the bracketed term in (19) approaches Hsym(λk 1, . . . ,λkn)
as t → ∞. Thus y (t) in (19) becomes arbitrarily close to the steady-state response deﬁned
by
yss(t) = An
k 1=1
Σ
2
. . .
kn=1
Σ
2
Hsym(λk 1, . . . ,λkn) exp(
j =1Σ
n
λkjt)
(20)
an expression that clearly is analogous to (14). Collecting together those terms with
identical exponents [kλ1 + (n −k)λ2], and recalling the deﬁnitions of λ1 and λ2, (20) can
be written as
yss(t) = An
k =0Σ
n
Gk,n −k(iω,−iω)e i (2k −n)ωt
(21)
where
Gk,n −k(λ1,λ2) =
k 1+ . . . +kn=2n −k
k 1=1
Σ
2
. . .
kn=1
Σ
2
Hsym(λk 1, . . . ,λkn)
= B
D k
nE
G Hsym(
k
λ1, . . . ,λ1;
n −k
λ2, . . . ,λ2)
(22)
One useful identity that follows from (22) is
Gk,n −k(iω,−iω) = Gn −k,k(−iω,iω)
It is convenient to rearrange the terms in (21) as follows. First write
203

yss(t) = An[Gn, 0(iω,−iω)e inωt + G0,n(iω,−iω)e −inωt]
+ An[Gn −1,1(iω,−iω)e i (n −2)ωt + G1,n −1(iω,−iω)e −i (n −2)ωt]
+ . . . +
B
A
C
A
D
An[G
2
n +1
_____,
2
n −1
_____(iω,−iω)e iωt+G
2
n −1
_____,
2
n +1
_____(iω,−iω)e −iωt] , n odd
AnGn /2,n /2(iω,−iω) , n even
= An[Gn, 0(iω,−iω)e inωt + Gn, 0(−iω,iω)e −inωt]
+ An[Gn −1,1(iω,−iω)e i (n −2)ωt + Gn −1,1(−iω,iω)e −i (n −2)ωt]
+ . . . +
B
A
C
A
D
An[G
2
n +1
_____,
2
n −1
_____(iω,−iω)e iωt+G
2
n +1
_____,
2
n −1
_____(−iω,iω)e −iωt] , n odd
AnGn /2,n /2(iω,−iω) , n even
(23)
Now, using standard identities,
yss(t) = 2An |Gn, 0(iω,−iω)|cos[nωt + ∠Gn, 0(iω,−iω)]
+ 2AnGn −1,1(iω,−iω)|cos[(n −2)ωt + ∠Gn −1,1(iω,iω)]
+ . . . +
B
A
C
A
D
2An |G
2
n +1
_____,
2
n −1
_____(iω,−iω)|cos[ωt + ∠G
2
n +1
_____,
2
n −1
_____(iω,−iω)] , n odd
AnGn /2,n /2(iω,−iω) , n even
(24)
Thus the steady-state response of a degree-n homogeneous system to a cosinusoidal input
of frequency ω is composed of cosinusoidal components at frequencies nω, (n −2)ω, . . . ,0
(n even) or ω (n odd).
Now consider a degree-N polynomial system
y (t) =
n =1Σ
N
−∞∫
∞
hnsym(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(25)
with the input signal u (t) = 2Acos(ωt). The steady-state response is obtained by adding
the contributions of each homogeneous subsystem. Each degree-n subsystem where n is
odd contributes terms at frequencies ω, 3ω, . . . , nω. Each degree-n subsystem where n is
even contributes a constant term and terms at frequencies 2ω, 4ω, . . . , nω. From (23) the
contribution of the degree-n subsystem to frequency kω, assuming k ≤ n and k and n have
the same parity, is
204

An G
2
n +k
_____,
2
n −k
_____(iω,−iω)e ikωt + An G
2
n +k
_____,
2
n −k
_____ (−iω,iω) e −ikωt
(26)
(It is useful to observe that the sum of the subscripts on G indicates the degree of the
subsystem, and the difference of the subscripts the harmonic.) Thus letting Nk be the
greatest integer ≤ N with the same parity as k, the steady-state response of (25) can be
written as
yss(t) = f 0(A,iω) +
k =1Σ
N
[ fk(A,iω) e ikωt + fk(A, −iω) e −ikωt ]
(27)
where
f 0(A,iω) = A2 G1,1(iω,−iω) + A4 G2,2(iω,−iω)
+ . . . + AN2 G
2
N2
____, 2
N2
____(iω,−iω)
(28)
f 1(A,iω) = A G1,0(iω,−iω) + A3 G2,1(iω,−iω)
+ . . . + AN1 G
2
N1+1
______,
2
N1−1
______(iω,−iω)
(29)
f 2 (A,iω) = A2 G2,0(iω,−iω) + A4 G3,1(iω,−iω)
+ . . . + AN2 G
2
N2+2
______,
2
N2−2
______(iω,−iω)
(30)
and so on. The general terms can be written in the forms
f 0(A,iω) =
j =1Σ
N2/2
A2jGjj(iω,−iω)
fk(A,iω) =
j =0Σ
(Nk−k)/2
Ak +2jGk +j,j(iω,−iω) , k =1,2, . . . ,N
(31)
As is by now usual, (27) can be written as
yss(t) = f 0(A,iω) + 2
k =1Σ
N
| fk(A,iω)|cos[kωt + ∠fk(A,iω)]
(32)
which makes explicit in real terms the fact that the steady-state response of a polynomial
system to sinusoidal inputs can be expressed as a ﬁnite Fourier series. Furthermore, the
Fourier coefﬁcients are polynomials in the input amplitude A with coefﬁcients that are
functions of the input frequency ω.
Example 5.2
Consider the pendulum system in Example 3.8. Using the symmetric
transfer functions through degree 3 calculated there, the steady-state response to
u (t) = 2Acos(ωt) is given by
205

yss(t) = 2 |AH (iω) + 3A3H3sym(iω,iω,−iω) + . . . cos [ωt + φ1(ω)]
+ 2 |A3H3sym(iω,iω,iω) + . . . cos [3ωt + φ3(ω)] + . . .
It is convenient to let
W (s) =
s 2 + (a /mL2)s + g /L
g /L
__________________
and write the transfer functions computed in Example 3.8 in the form
H (s) = mgL
1
_____ W (s)
H3sym(s 1,s 2,s 3) =
3!(mgL)3
1
_________ W (s 1)W (s 2)W (s 3)W (s 1+s 2+s 3)
Then
yss(t) = 2 | mgL
A
_____ W (iω) +
2(mgL)3
A3
________ W 3(iω)W (−iω) + . . . cos [ωt + φ1(ω)]
+ 2 |
3(mgL)3
A3
________ W 3(iω)W (i 3ω) + . . . cos [3ωt + φ3(ω)] + . . .
A simple analysis of this formula can be used to show the possibility of resonance
phenomena in the pendulum system at frequencies higher than the input frequency ω. This
phenomenon can occur even for very small input amplitudes A, but it is not predicted by
the usual linearized model of the pendulum. To be speciﬁc, suppose that the damping
coefﬁcient a is very small in relation to (g /L)1/2. Then the poles of W (s) are very close to
the undamped natural frequency ωo = (g /L)1/2. In this situation, if ω = ωo/3, then
|W (i 3ω)| can be very large in comparison to |W (iω)| so that the dominant term in yss(t)
is the third harmonic. Of course, both the third harmonic term and the fundamental terms
in the output depend on higher-degree transfer functions that have been ignored. But it can
be shown that these missing terms do not eliminate the possibility of resonance. In fact,
the higher-degree terms indicate the possibility of harmonic resonance at many other
choices of input frequency.
Although the discussion so far has been in terms of the symmetric transfer function,
similar results can be derived for the triangular and regular transfer functions. One way to
do this is to use the relationships between the various transfer functions that were derived
in Chapter 2. However, it is interesting to take a direct approach in the case of bilinear-
realizable regular transfer functions because the required stability property can be
explicitly stated.
Suppose
206

Hreg(s 1, . . . ,sn) = Q1(s 1) . . . Qn(sn)
P (s 1, . . . ,sn)
_______________
(33)
is a strictly proper, recognizable, regular transfer function.
With the input signal
u (t) = 2Acos (ωt), Theorem 2.10 in Section 2.3 with γ1 = iω, γ2 = −iω gives the response
formula
Y(s) = An
i 1=1Σ
2 . . .
in −1=1
Σ
2
Hreg(s+γi 1+ . . . +γin −1, . . . ,s+γin −1,s)
[ s +γi 1+ . . . +γin −1+iω
1
__________________ + s +γi 1+ . . . +γin −1−iω
1
__________________ ]
(34)
Since each term in (34) is a strictly proper, rational function in s, the steady-state response
can be computed via partial fraction expansion.
If it is assumed that all poles of
Hreg(s 1, . . . ,sn) have negative real parts, that is, all roots of each Qj(sj) have negative real
parts, then the pole factors contributed by the transfer function can be ignored as far as
steady-state response is concerned. Furthermore, since the poles contributed by the input
terms in (34) occur at
s = ± inω, ± i (n −2)ω , . . . ,
B
C
D
0 , n even
± iω , n odd
it is clear that the steady-state response is bounded.
To compute the steady-state response, let AnKk(iω) be the partial fraction expansion
coefﬁcient corresponding to the factor (s −ikω) on the right side of (34). Then, discarding
all the terms that will yield zero,
AnKk(iω) = (s −ikω)Y(s) s =ikω
= An
γi 1+ . . . +γin −1=−i (k +1)ω
i 1=1Σ
2 . . .
in −1=1
Σ
2
Hreg(s +γi 1+ . . . +γin −1, . . . ,s +γin −1,s) s =ikω
+ An
γi 1+ . . . +γin −1=−i (k +1)ω
i 1=1Σ
2 . . .
in −1=1
Σ
2
Hreg(s +γi 1+ . . . +γin −1, . . . ,s +γin −1,s) s =ikω (35)
This expression can be simpliﬁed by combining the two constrained, multiple summations
into one implicit sum, and then replacing s by ikω. This gives
Kk(iω) =
γ1+ . . . +γn −1=−i (k± 1)ω
γ1, . . . ,γn −1=± iω
Σ
Hreg(ikω+γ1+ . . . +γn −1 , . . . , ikω+γn −1,ikω)
(36)
While the general term in (36) is messy, note that for small n it is not hard to write out.
And, in general,
207

Kn(iω) = Hreg(iω,i 2ω, . . . ,inω)
The last step is to take the inverse Laplace transform of each term
(s −ikω)
Kk(iω)
________
in the partial fraction expansion. Using standard trigonometric identities, the steady-state
response is given by
yss(t) = 2An |Kn(iω)cos[nωt + ∠Kn(iω)]
+ 2An |Kn −2(iω)cos[(n −2)ωt + ∠Kn −2(iω)]
+ . . . +
B
A
C
A
D
2An |K 1(iω)cos[ωt + ∠K 1(iω)] , n odd
AnK 0(iω) , n even
(37)
For polynomial systems, the contributions of the various homogeneous subsystems can be
added together just as discussed earlier.
5.3 Steady-State Response to Multi-Tone Inputs
When a sum of sinusoids is applied to a homogeneous system of degree greater than
1, the response is complicated by the nonlinear interactions between terms of different
frequencies. To introduce this topic, I will begin with the so-called two-tone input:
u (t) = 2A1cos(ω1t) + 2A2cos(ω2t), t ≥0
= A1e iω1t + A1e −iω1t + A2e iω2t + A2e −iω2t
(38)
Again, the growing exponential development in Chapter 2 can be used, this time for the
case of four exponentials:
λ1 = iω1, λ2 = −iω1, λ3 = iω2, λ4 = −iω2
For a degree-n system with symmetric transfer function Hnsym(s 1, . . . ,sn), copying (73) of
Section 2.4 with the appropriate changes gives
yss(t) =
mΣ A1
m1+m2A2
m3+m4Gm1m2m3m4(λ1,λ2,λ3,λ4) e (m1λ1+ . . . +m4λ4)t
(39)
where
208

Gm1m2m3m4(λ1,λ2,λ3,λ4) = m1!m2!m3!m4!
n !
______________ Hnsym(
m1
λ1, . . . ,λ1; . . . ;
m4
λ4, . . . ,λ4) (40)
and
mΣ
is
a
four-fold
summation
over
m1, . . . ,m4
such
that
0 ≤ mi ≤ n
and
m1+ . . . +m4 = n. Substituting for the λ’s gives
yss(t) =
mΣ A1
m1+m2A2
m3+m4Gm1m2m3m4(iω1,−iω1,iω2,−iω2)e i [(m1−m2)ω1+(m3−m4)ω2]t (41)
Example 5.3
It is perhaps instructive to catalog the terms in (41) for the case n = 2.
There are ten terms in the summation, and these are shown in Table 5.1. To write the
output in terms of real quantities, properties of Gm1m2m3m4 with regard to complex
conjugation can be used. For example,
G0110(iω1,−iω1,iω2,−iω2) = 2H2sym(−iω1,iω2)
and
G1001(iω1,−iω1,iω2,−iω2) = 2H2sym(iω1,−iω2)
so it is clear that (dropping arguments) G0110 = G
__
1001, where the overbar indicates
complex conjugate. Similarly,
G1010 = G
__
0101, G2000 = G
__
0200, G0020 = G
__
0002
Thus standard trigonometric identities yield the expression
yss(t) = A1
2G1100 + A2
2G0011 + 2A1A2 |G0110cos[(ω2−ω1)t + ∠G0110]
+ 2A1A2 |G1010cos[(ω1+ω2)t + ∠G1010] + 2A1
2 |G2000cos[2ω1t + ∠G2000]
+ 2A2
2 |G0020cos[2ω2t + ∠G0020]
Note that all these frequency components need not occur at distinct frequencies. For
example, consider the case ω2 = 3ω1.
When higher-degree homogeneous systems are considered, the number of terms in
the steady-state response increases dramatically. Therefore, it seems more useful to derive
an expression that gives the coefﬁcient of a particular complex exponential term in the
output. As many or as few terms as desired then can be considered, and conjugate
exponential terms can be combined easily if the real form is wanted.
209

______________________________________
Table 5.1
Frequency-Response Terms for Example 5.3
Summation Indices
Summand
1
1
0
0
A1
2G1100
0
1
1
0
A1A2G0110e i (ω2−ω1)t
0
0
1
1
A2
2G0011
1
0
0
1
A1A2G1001e i (ω1−ω2)t
1
0
1
0
A1A2G1010e i (ω1+ω2)t
0
1
0
1
A1A2G0101e −i (ω1+ω2)t
2
0
0
0
A1
2G2000e i 2ω1t
0
2
0
0
A1
2G0200e −i 2ω1t
0
0
2
0
A2
2G0020e i 2ω2t
0
0
0
2
A2
2G0002e −i 2ω2t
______________________________________
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
The terms in (41) corresponding to the exponential e i [Mω1+Nω2]t, M ≥ 0, N ≥ 0, can
be written as follows:
m1=m2+M, m3=m4+N
m1+m2+m3+m4=n
m1=0
Σ
n
m2=0
Σ
n
m3=0
Σ
n
m4=0
Σ
n
A1
2m2+MA2
2m4+NGm2+M,m2,m4+N,m4(iω1,−iω1,iω2,−iω2)
But now the four-fold summation can be simpliﬁed by replacing m1 and m3 using the
indicated constraints to obtain
m2+m4=
2
n −M −N
________
m2=0
Σ
n
m4=0
Σ
n
A1
2m2+MA2
2m4+N Gm2+M,m2,m4+N,m4(iω1,−iω1,iω2,−iω2)
(42)
With this notation there are several relationships in the subscripts of G that are
convenient for checking. The sum of the subscripts is the degree of the system and the
difference of the ﬁrst two (last two) is the associated harmonic of ω1 (ω2). Although I
have assumed M, N ≥ 0, to obtain the coefﬁcient of the term e −i [Mω1+Nω2]t simply change
the sign of every frequency argument in every G. To obtain the coefﬁcient of, say,
e i [Mω1−Nω2]t, change the sign of every argument ω2 in every G. Note that changing the sign
of the frequency does not change the input signal, so that (42) remains valid.
Of course, the coefﬁcient of e i [Mω1+Nω2]t can be expressed directly in terms of the
symmetric transfer function Hnsym(s 1, . . . ,sn) using (40). This gives, using a collapsed
notation for the arguments of the transfer function,
210

m2+m4=
2
n −M −N
________
m2=0
Σ
n
m4=0
Σ
n
(m2+M)!m2!(m4+N)!m4!
n !A1
2m2+MA2
2m4+N
______________________ Hnsym(
m2+M
iω1 ;
m2
−iω1;
m4+N
iω2 ;
m4
−iω2)
(43)
The same rule is used in (43) as in (42) to obtain the coefﬁcient when M and/or N is
negative. I should emphasize that the exponential frequency terms e i [Mω1+Nω2]t may not be
distinct. For example, if ω1 = 2ω2, then [ω1+2ω2] = 2ω1 so that the coefﬁcients of these
two terms can be combined.
Example 5.4
The contribution of a degree-5 homogeneous system to the (assumed
distinct) frequency component e i [ω1+2ω2]t will be computed. In this case (43) specializes
to
m2+m4=1
m2=0
Σ
5
m4=0
Σ
5
(m2+1)!m2!(m4+2)!m4!
5!A1
2m2+1A2
2m4+2
_____________________ H5sym(
m2+1
iω1 ;
m2
−iω1;
m4+2
iω2 ;
m4
−iω2)
There are two terms in the summation, corresponding to the index pairs 0,1 and 1,0. Thus
the summation gives
3!
5!A1A2
4
_______ H5sym(iω1,iω2,iω2,iω2,−iω2) +
2!2!
5!A1
3A2
2
_______ H5sym(iω1,iω1,−iω1,iω2,iω2)
It is instructive also to compute the coefﬁcient of the frequency component e i [ω1−2ω2]t, for
there are two ways to proceed. The easiest is that mentioned above: take the coefﬁcient
just derived and replace every ω2 by −ω2 to obtain
3!
5!A1A2
4
_______ H5sym(iω1,−iω2,−iω2,−iω2,iω2) +
2!2!
5!A1
3A2
2
_______ H5sym(iω1,iω1,−iω1,−iω2,−iω2)
A straightforward application of (43) also works, although terms with negative factorials,
negative powers, and negative subscripts, which arise because of the implicit nature of the
formula, must be deleted. Speciﬁcally, (43) becomes, with M = 1, N = −2,
m2+m4=3
m2=0
Σ
5
m4=0
Σ
5
(m2+1)!m2!(m4−2)!m4!
5!A1
2m2+1A2
2m4−2
_____________________ H5sym(
m2+1
iω1 ;
m2
−iω1;
m4−2
iω2;
m4
−iω2)
The index pairs contributing to the summation are: 0,3; 1,2; 2,1; and 3,0. But the last two
pairs can be dropped as extraneous so that the coefﬁcient of e i [ω1−2ω2]t is
3!
5!A1A2
4
_______ H5sym(iω1,iω2,−iω2,−iω2,−iω2) +
2!2!
5!A1
3A2
2
_______ H5sym(iω1,iω1,−iω1,−iω2,−iω2)
which agrees with the earlier result.
211

For polynomial or Volterra systems, it should be clear that the analysis just
completed can be applied readily. To obtain the coefﬁcient of e i [Mω1+Nω2]t in the steady-
state response, the coefﬁcients in (43) must be added together for n = 1,2, . . . . Thus, the
coefﬁcient can be written for a Volterra system in terms of the symmetric transfer functions
as
m2=0
Σ
∞
m4=0
Σ
∞
(m2+M)!m2!(m4+N)!m4!
(2m2+2m4+M +N)!A1
2m2+MA2
2m4+N
______________________________
H(2m2+2m4+M +N)sym(
m2+M
iω1 ;
m2
−iω1;
m4+N
iω2 ;
m4
−iω2)
(44)
where n has been replaced by the appropriate sum of subscripts, and the constraints on the
summations have been removed.
Example 5.5
As an illustration of the use of (44), I will list the terms in the response of a
degree-3 polynomial system to the input (38). The complex conjugate terms will be
omitted since they add no information. The contribution of the degree-1 subsystem is
found by imposing the restriction 2m2+2m4+M +N = 1 in (44). In this case, there are no
negative-frequency terms other than complex-conjugate terms, so the list with nonnegative
M and N is complete as shown in Table 5.2.
___________________________________________
Table 5.2
Frequency-Response Terms: Degree-1 Subsystem*
Summation Indices
Frequency
m2
m4
M
N
Term
0
0
1
0
A1H1(iω1)e iω1t
0
0
0
1
A2H1(iω2)e iω2t
___________________________________________
LL
L
L
L
L
L
L
LL
L
L
L
L
L
L
* (plus complex −conjugate frequency terms)
 
The contribution of the degree-2 subsystem involves essentially repeating Table 5.1. But
the notation is different in the present context, so I will go ahead. Imposing the constraint
2m2+2m4+M +N = 2 in (44) gives the list in Table 5.3. Notice in this case there is only
one distinct frequency component generated by allowing M and/or N to become negative
(ignoring complex conjugates). Such a term will be called a sign switch to indicate how it
is obtained from previously computed terms.
212

__________________________________________________
Table 5.3
Frequency-Response Terms: Degree-2 Subsystem*
Summation Indices
Frequency
m2
m4
M
N
Term
1
0
0
0
2!A1
2H2sym(iω1,−iω1)
0
1
0
0
2!A2
2H2sym(iω2,−iω2)
0
0
2
0
A1
2H2sym(iω1,iω1)e i 2ω1t
0
0
0
2
A2
2H2sym(iω2,iω2)e i 2ω2t
0
0
1
1
2!A1A2H2sym(iω1,iω2)e i (ω1+ω2)t
sign switch
2!A1A2H2sym(iω1,−iω2)e i (ω1−ω2)t
__________________________________________________
LL
L
L
L
L
L
L
L
L
L
L
L
LL
L
L
L
L
L
L
L
L
L
L
L
* (plus complex −conjugate frequency terms)
 
In a similar manner, setting 2m2+2m4+M +N = 3 gives the contribution of the degree-3
subsystem as shown in Table 5.4.
________________________________________________________
Table 5.4
Frequency-Response Terms: Degree-3 Subsystem*
Summation Indices
Frequency
m2
m4
M
N
Term
1
0
1
0
2!
3!
___A1
3H3sym(iω1,iω1,−iω1)e iω1t
1
0
0
1
3!A1
2A2H3sym(iω1,−iω1,iω2)e iω1t
0
1
1
0
3!A1A2
2H3sym(iω1,iω2,−iω2)e iω1t
0
1
0
1
2!
3!
___A2
3H3sym(iω2,iω2,−iω2)e iω2t
0
0
2
1
2!
3!
___A1
2A2H3sym(iω1,iω1,iω2)e i (2ω1+ω2)t
0
0
1
2
2!
3!
___A1A2
2H3sym(iω1,iω2,iω2)e i (ω1+2ω2)t
0
0
3
0
A1
3H3sym(iω1,iω1,iω1)e i 3ω1t
0
0
0
3
A2
3H3sym(iω2,iω2,iω2)e i 3ω2t
sign switch
2!
3!
___A1
2A2H3sym(iω1,iω1,−iω2)e i (2ω1−ω2)t
sign switch
2!
3!
___A1A2
2H3sym(iω1,−iω2,−iω2)e i (ω1−2ω2)t
________________________________________________________
LL
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
LL
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
* (plus complex −conjugate frequency terms)
 
Of course, to complete this example, all these terms should be combined - a task I leave to
the reader.
To consider inputs that are sums of more than two sinusoidal terms, the same
approach is followed. For example, it is straightforward although tedious to verify the
213

following fact. For the input
u (t) = 2A1cos(ω1t) + 2A2cos(ω2t) + 2A3cos(ω3t)
(45)
to a Volterra system, the coefﬁcient of the exponential e i [Lω1+Mω2+Nω3]t, L, M, N ≥ 0, in the
steady-state response is
m2=0
Σ
∞
m4=0
Σ
∞
m6=0
Σ
∞
(m2+L)!m2!(m4+M)!m4!(m6+N)!m6!
(2m2+2m4+2m6+L+M +N)!A1
2m2+LA2
2m4+MA3
2m6+N
___________________________________________
H(2m2+2m4+2m6+L+M +N)sym(iω1,−iω1,iω2,−iω2,iω3,−iω3)
(46)
where the various numbers of arguments are entered into the transfer function the obvious
number of times - to be pedantic, m2+L, m2, m4+M, m4, m6+N, m6, respectively. When
L, M, or N are negative, the coefﬁcient is found by changing the sign of the corresponding
frequency arguments, just as before. Also just as before, the frequency components may
not be distinct, depending on the relative values of ω1, ω2, and ω3. I should emphasize
that (46) gives the coefﬁcient of just one complex exponential. So, what can be said about
the total steady-state response? Not much more than that it is a jungle into which the
prudent venture only with inkwell full.
5.4 Response to Random Inputs
Just as in the preceding sections, the linear theory to be generalized will be reviewed
ﬁrst. Suppose the input to the system
y (t) =
−∞∫
∞
h (σ)u (t −σ) dσ
(47)
is a sample function from a real stochastic process with expected value E [u (t)] and
autocorrelation
Ruu(t 1,t 2) = E [u (t 1)u (t 2)]
(48)
Then the output is a sample function from a real stochastic process, and it is of interest to
ﬁnd the expected value of the output, E [y (t)], the input/output cross-correlation
Ryu(t 1,t 2) = E [y (t 1)u (t 2)]
(49)
and the output autocorrelation
Ryy(t 1,t 2) = E [y (t 1)y (t 2)]
(50)
Proceeding by direct calculation, it is clear that since expectation can be
interchanged with integration,
214

E [y (t)] =
−∞∫
∞
h (σ)E [u (t −σ)] dσ
(51)
Furthermore,
y (t 1)u (t 2) =
−∞∫
∞
h (σ)u (t 1−σ)u (t 2) dσ
so that taking expected values on both sides gives
Ryu(t 1,t 2) =
−∞∫
∞
h (σ)Ruu(t 1−σ,t 2) dσ
(52)
Similarly,
y (t 1)y (t 2) =
−∞∫
∞
−∞∫
∞
h (σ1)h (σ2)u (t 1−σ1)u (t 2−σ2) dσ1dσ2
and thus
Ryy(t 1,t 2) =
−∞∫
∞
−∞∫
∞
h (σ1)h (σ2)Ruu(t 1−σ1,t 2−σ2) dσ1dσ2
(53)
Notice that a number of technical matters again are being ignored. For example, it is
assumed implicitly that E [u (t)] and Ruu(t 1,t 2) are sufﬁciently well behaved to permit the
integrations indicated above. Such considerations are not too difﬁcult to ﬁll in, and that
task is left to the reader, as usual.
The correlation relationships often are expressed in terms of a multivariable Fourier
transform. In strict analogy to the usual single-variable Fourier transform
F (ω) = F[f (t)] =
−∞∫
∞
f (t)e −iωt dt
(54)
the multivariable Fourier transform of a function f (t 1, . . . ,tn) is deﬁned by
F (ω1, . . . ,ωn) =
−∞∫
∞
f (t 1, . . . ,tn)e −iω1t 1 . . . e −iωntn dt 1 . . . dtn
(55)
Of course, this is no surprise, given the discussion of the multivariable Laplace transform
in Chapter 2. Furthermore, the multivariable Fourier transform exhibits all the properties
that might reasonably be expected after a review of the properties of the Laplace transform
in Chapter 2. The inverse Fourier transform is given by
215

f (t 1, . . . ,tn) =
(2π)n
1
_____
−∞∫
∞
F (ω1, . . . ,ωn)e iω1t 1 . . . e iωntn dω1 . . . dωn
(56)
For the purposes of this chapter, the Fourier transform of h (t) is called the system
function, and it is written as H (ω). In this context, perhaps I should remind the reader of
the common notational collision between Laplace and Fourier transforms. If a (Laplace)
transfer function H (s) exists for Re [s ] = 0, then the system function is given by
H (s) s =iω = H (iω). However, Laplace aside, it is more convenient to use the notation
H (ω) for the system function. Since the Laplace transform will be set aside for the
material dealing with random input signals, I will use the H (ω) notation for the system
function, and for all single- or multi-variable Fourier transforms.
Incidentally, the
hypotheses needed to insure the existence of Fourier transforms will be assumed. For
example, the system stability property corresponding to
−∞∫
∞
|h (t)| dt < ∞
can be assumed to guarantee the existence of the system function H (ω).
Letting
Suu(ω1,ω2) = F[Ruu(t 1,t 2)]
(57)
with similar deﬁnitions for the transforms of the other correlation functions, a
straightforward calculation shows that (52) and (53) can be represented by
Syu(ω1,ω2) = H (ω1)Suu(ω1,ω2)
Syy(ω1,ω2) = H (ω1)H (ω2)Suu(ω1,ω2)
(58)
These concepts are of most interest in the case where the real random process u (t) is
(strict-sense) stationary. For then, assuming the input signal was applied at t = −∞, the
output also is a real, stationary random process. In other words, the steady-state output is a
real, stationary random process. Of course, there is an implicit stability assumption here.
(The astute reader will notice that the stationarity condition is stronger than necessary for
the linear case, and that only wide-sense stationarity is needed. However, in the nonlinear
case strict-sense stationarity is required.)
In the case of stationary input, E [u (t)] is a constant, so that
E [y (t)] =
−∞∫
∞
h (σ) dσ E [u (t)]
(59)
Also, the autocorrelation function Ruu(t 1,t 2) depends only on the difference t 1 − t 2.
Following the usual notation by changing to the variables t 2 = t, t 1 = t + τ, the
autocorrelation Ruu(t +τ,t) is a function of τ only, and thus is written as Ruu(τ). To
determine the input/output cross-correlation in terms of the new variables, (52) can be
written as
216

Ryu(t +τ,t) =
−∞∫
∞
h (σ)Ruu(t +τ−σ,t) dσ
and since the right side is independent of t, this is written in the form
Ryu(τ) =
−∞∫
∞
h (σ)Ruu(τ−σ) dσ
(60)
Similarly, the output autocorrelation can be written as
Ryy(τ) =
−∞∫
∞
−∞∫
∞
h (σ1)h (σ2)Ruu(τ−σ1+σ2) dσ1dσ2
(61)
These relationships can be expressed in the frequency domain using the single-
variable Fourier transform.
This can be accomplished directly in an easy fashion.
However, to warm up for later developments, I will derive the expressions from the 2-
variable Fourier transform formulas in (58). Using the new variables introduced above,
Suu(ω1,ω2) =
−∞∫
∞
−∞∫
∞
Ruu(t 1,t 2)e −iω1t 1e −iω2t 2 dt 1dt 2
=
−∞∫
∞
−∞∫
∞
Ruu(t +τ,t)e −iω1(t +τ)e −iω2t dτ dt
=
−∞∫
∞
−∞∫
∞
Ruu(τ)e −iω1τe −i(ω1+ω2)t dτ dt
Integrating with respect to τ gives the Fourier transform Suu(ω1) = F [Ruu(τ)], which is the
power spectral density of the stationary random process. Then using the well known
transform
−∞∫
∞
e −iωt dt = 2πδ0(ω)
leads to
Suu(ω1,ω2) = 2πSuu(ω1)δ0(ω1+ω2)
Integrating both sides with respect to ω2 gives
Suu(ω1) = 2π
1
___
−∞∫
∞
Suu(ω1,ω2) dω2
(62)
This formula expresses the power spectral density of a stationary random process in
terms of the 2-variable Fourier transform of the general autocorrelation function of that
process. Of course a similar relationship is obtained for the cross-spectral density Syu(ω1)
217

in terms of Syu(ω1,ω2) given in (58). Thus the ﬁrst equation in (58) becomes
Syu(ω1,ω2) = 2πH (ω1)Suu(ω1)δ0(ω1+ω2)
so that the input/output cross-spectral density is given in terms of the input power spectral
density by
Syu(ω1) = 2π
1
___
−∞∫
∞
2πH (ω1)Suu(ω1)δ0(ω1+ω2) dω2
= H (ω1)Suu(ω1)
(63)
Proceeding in a similar fashion for the second relation in (58) gives the output power
spectral density in terms of the input power spectral density as
Syy(ω1) = H (ω1)H (−ω1)Suu(ω1)
= |H (ω1)| 2Suu(ω1)
(64)
I should note at this point that under appropriate ergodicity assumptions, the various
correlations and spectral densities in the stationary case can be expressed as time
averages. This fact will be crucial in Chapter 7, when identiﬁcation techniques are
discussed. Also note that, in terms of the system function, the expected value of the output
given in (59) can be written in the form
E [y (t)] = H (0)E [u (t)]
(65)
Now consider the generalization of the ideas just reviewed to nonlinear systems
described by
y (t) =
−∞∫
∞
h (σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
(66)
The discussion of polynomial or Volterra systems will be postponed until this
homogeneous case is treated, as usual.
When u (t) is a real random process, direct calculation gives
E [y (t)] =
−∞∫
∞
h (σ1, . . . ,σn)E [u (t −σ1) . . . u (t −σn)] dσ1 . . . dσn
=
−∞∫
∞
h (σ1, . . . ,σn)Ruu
(n)(t −σ1, . . . ,t −σn) dσ1 . . . dσn
(67)
where the n th-order autocorrelation function of the input is deﬁned by
Ruu
(n)(t 1, . . . ,tn) = E [u (t 1) . . . u (tn)]
In a similar fashion the input/output cross-correlation, and the output autocorrelation can
be written in the forms
218

Ryu(t 1,t 2) =
−∞∫
∞
h (σ1, . . . ,σn)Ruu
(n +1)(t 1−σ1, . . . ,t 1−σn,t 2) dσ1 . . . dσn
(68)
Ryy(t 1,t 2) =
−∞∫
∞
h (σ1, . . . ,σn)h (σn +1, . . . ,σ2n)
Ruu
(2n)(t 1−σ1, . . . ,t 1−σn,t 2−σn +1, . . . ,t 2−σ2n) dσ1 . . . σ2n
(69)
For n = 1 these expressions are just those discussed previously.
But for n > 1 the
expected value of the output and the (order-2) output correlations depend on higher-order
input autocorrelations. In other words, as n increases more statistical information about the
input is needed to characterize, for example, the output autocorrelation.
The expressions (67), (68), and (69) can be written in the form of convolutions
followed by variable associations, a form that is reminiscent of the convolutions and
variable associations that arise in considering the input/output representation of a
homogeneous system using the multivariable Laplace transform. Since it is of interest to
express (68) and (69) in terms of Fourier transforms, it is convenient to separate the
convolution aspect from the association aspect, as was done in Chapter 2. To do this,
deﬁne the multivariable input/output cross-correlation by
Rˆyu(t 1, . . . ,tn +1) =
−∞∫
∞
h (σ1, . . . ,σn)Ruu
(n +1)(t 1−σ1, . . . ,tn−σn,tn +1) dσ1 . . . dσn (70)
so that
Ryu(t 1,t 2) = Rˆyu(t 1, . . . ,tn +1) tn +1=t 2
t 1= . . . =tn=t 1
(71)
In a similar manner, the multivariable output autocorrelation is deﬁned by
Rˆyy(t 1, . . . ,t 2n) =
−∞∫
∞
h (σ1, . . . ,σn)h (σn +1, . . . ,σ2n)Ruu
(2n)(t 1−σ1, . . . ,t 2n−σ2n) dσ1 . . . dσ2n
(72)
so that
Ryy(t 1,t 2) = Rˆyy(t 1, . . . ,t 2n) tn +1= . . . =t 2n=t 2
t 1= . . . =tn=t 1
(73)
These intermediate multivariable quantities have no signiﬁcance other than to
facilitate the representation via Fourier transforms. Let the Fourier transform of the order-
n autocorrelation function of the input be
Suu
(n)(ω1, . . . ,ωn) = F[Ruu
(n)(t 1, . . . ,tn)]
(74)
and the Fourier transforms of the multivariable cross- and autocorrelations of the output be
219

Sˆyu(ω1, . . . ,ωn +1) = F[Rˆyu(t 1, . . . ,tn +1)]
Sˆyy(ω1, . . . ,ω2n) = F[Rˆyy(t 1, . . . ,t 2n)]
(75)
These will be called multivariable spectral densities, though they have little or nothing to
do with spectral density. It follows from the readily established convolution property of
Fourier transforms that in terms of the system function,
Sˆyu(ω1, . . . ,ωn +1) = H (ω1, . . . ,ωn)Suu
(n +1)(ω1, . . . ,ωn +1)
(76)
Sˆyy(ω1, . . . ,ω2n) = H (ω1, . . . ,ωn)H (ωn +1, . . . ,ω2n)Suu
(2n)(ω1, . . . ,ω2n)
(77)
For n = 1 the circumﬂexes can be removed from the left side, and then these expressions
agree with those in (58). The problem of interest now is to express Syy(ω1,ω2) and
Syu(ω1,ω2) in terms of the multivariable spectral densities for n > 1. That is, to express
the variable associations in (71) and (73) in terms of Fourier transforms. It takes a little bit
of maneuvering to accomplish this, but the maneuvers should be familiar from the proof of
the association-of-variables formula in Chapter 2.
The inverse Fourier transform relationship can be written for the multivariable
cross-correlation as
Rˆyu(t 1, . . . ,tn +1) =
(2π)n +1
1
_______
−∞∫
∞
Sˆyu(γ1, . . . ,γn +1)e iγ1t 1 . . . e iγn +1tn +1 dγ1 . . . dγn +1
from which
Ryu(t 1,t 2) =
(2π)n +1
1
_______
−∞∫
∞
Sˆyu(γ1, . . . ,γn +1)e i (γ1+ . . . +γn)t 1e iγn +1t 2 dγ1 . . . dγn +1
Taking the Fourier transform of both sides gives
Syu(ω1,ω2) =
(2π)n +1
1
_______
−∞∫
∞
Sˆyu(γ1, . . . ,γn +1)e −i (ω1−γ1−. . . −γn)t 1e −i (ω2−γn +1)t 2
dγ1 . . . dγn +1 dt 1dt 2
and integrating with respect to t 1 and t 2,
Syu(ω1,ω2) =
(2π)n −1
1
_______
−∞∫
∞
Sˆyu(γ1, . . . ,γn +1)δ0(ω1−γ1−. . . −γn)
δ0(ω2−γn +1) dγ1 . . . dγn +1
=
(2π)n −1
1
_______
−∞∫
∞
H (γ1, . . . ,γn)Suu
(n +1)(γ1, . . . ,γn +1)δ0(ω1−γ1−. . . −γn)
δ0(ω2−γn +1)dγ1 . . . dγn +1
(78)
Repeating this procedure for the output autocorrelation gives
220

Syy(ω1,ω2) =
(2π)2n −2
1
________
−∞∫
∞
Sˆyy(γ1, . . . ,γ2n)δ0(ω1−γ1−. . . −γn)
δ0(ω2−γn +1−. . . −γ2n)dγ1 . . . dγ2n
=
(2π)2n −2
1
________
−∞∫
∞
H (γ1, . . . ,γn)H (γn +1, . . . ,γ2n)Suu
(2n)(γ1, . . . ,γ2n)
δ0(ω1−γ1−. . . −γn)δ0(ω2−γn +1−. . . −γ2n)dγ1 . . . dγ2n
(79)
The similarities here with the association-of-variables formulas in Chapter 2 may not be
apparent yet, but I will discuss that shortly.
There is no question that these expressions for the output spectral density and
cross-spectral density are formidable when actual computations or applications are
contemplated. But they can be simpliﬁed somewhat by the process of imposing further
assumptions on the input random process. Just as in the linear case, the ﬁrst of these is
(strict-sense) stationarity. When a stationary input is applied at t = −∞to a stationary
homogeneous system, the usual and rather simple time-shift argument shows that the
output random process is stationary. Thus, the output autocorrelation (and power spectral
density) and the input/output cross-correlation (and cross-spectral density) can be
expressed as functions of a single variable using techniques reviewed earlier. I will do the
calculations for the spectral densities and leave the correlations to the Problems.
For the cross-spectral density, a relationship of the form (62) can be written, giving
Syu(ω1) = 2π
1
___
−∞∫
∞
Syu(ω1,ω2) dω2
=
(2π)n
1
_____
−∞∫
∞
Sˆyu(γ1, . . . ,γn +1)δ0(ω1−γ1−. . . −γn)δ0(ω2−γn +1) dγ1 . . . dγn +1dω2 (80)
Integrating ﬁrst with respect to ω2 yields
Syu(ω1) =
(2π)n
1
_____
−∞∫
∞
Sˆyu(γ1, . . . ,γn +1)δ0(ω1−γ1−. . . −γn) dγ1 . . . dγn +1
(81)
or, in terms of the system function and input spectral density of order n +1,
Syu(ω1) =
(2π)n
1
_____
−∞∫
∞
H (γ1, . . . ,γn)Suu
(n +1)(γ1, . . . ,γn +1)
δ0(ω1−γ1−. . . −γn) dγ1 . . . dγn +1
(82)
Notice that integrating with respect to γ1 in (81) gives
Syu(ω1) =
(2π)n
1
_____
−∞∫
∞
Sˆyu(ω1−γ2−. . . −γn,γ2, . . . ,γn +1) dγ2 . . . dγn +1
(83)
an expression that is very much like an association-of-variables formula in Section 2.3.
However, the unintegrated form in (82) will be more efﬁcient for further developments.
221

A similar calculation for the output power spectral density gives
Syy(ω1) =
(2π)2n −1
1
________
−∞∫
∞
Sˆyy(γ1, . . . ,γ2n)δ0(ω1−γ1−. . . −γn)dγ1 . . . dγ2n
=
(2π)2n −1
1
________
−∞∫
∞
H (γ1, . . . ,γn)H (γn +1, . . . ,γ2n)Suu
(2n)(γ1, . . . ,γ2n)
δ0(ω1−γ1−. . . −γn) dγ1 . . . dγ2n
(84)
Again, this can be interpreted as an association-of-variables formula.
To achieve further simpliﬁcation, it is assumed that the real, stationary, random-
process
input
is
zero-mean
and
Gaussian.
For
in
this
case
the
higher-order
autocorrelations (spectral densities) of the input process can be expressed in terms of the
order-2 autocorrelation (power spectral density). The derivation of this fact will not be
given, rather, I simply will present the formulas.
The order-n autocorrelation function of a stationary, zero-mean, Gaussian random
process u (t) can be written as
Ruu
(n)(t 1, . . . ,tn) =
B
A
C
A
D
0 , n odd
pΣ j,kΠ
n
Ruu(tj−tk) , n even
(85)
where
j,kΠ
n
is a product over a set of n /2 (unordered) pairs of integers from 1,2, . . . ,n, and
pΣ
is a sum over all
(n −1)(n −3)(n −5) . . . (1) =
(n /2)!2n /2
n !
_________
such products. While a more explicit notation can be adopted, it is so complicated that I
will use (85) and further explain with examples.
Example 5.6
For n = 2 there is only one pair, namely (1,2). Thus
Ruu
(2)(t 1,t 2) = Ruu(t 1−t 2)
that is, the usual order-2 autocorrelation. For n = 4 there are three sets of two pairs,
namely (1,2),(3,4); (1,3),(2,4); and (1,4),(2,3). Thus
Ruu
(4)(t 1,t 2,t 3,t 4) = Ruu(t 1−t 2)Ruu(t 3−t 4)
+ Ruu(t 1−t 3)Ruu(t 2−t 4) + Ruu(t 1−t 4)Ruu(t 2−t 3)
(86)
In a similar fashion the higher-order spectral densities can be expressed in terms of
the order-2 power spectral density. Taking the n-variable Fourier transform of (85) gives
222

Suu
(n)(ω1, . . . ,ωn) =
B
A
C
A
D
0 , n odd
(2π)n /2
pΣ j,kΠ
n
Suu(ωj)δ0(ωj+ωk) , n even
(87)
Example 5.7
For n = 2 this formula gives
Suu
(2)(ω1,ω2) = 2πSuu(ω1)δ0(ω1+ω2)
an expression that was derived at the beginning of this section. For n = 4 I leave the
calculation as an exercise and provide the result:
Suu
(4)(ω1,ω2,ω3,ω4) = (2π)2Suu(ω1)Suu(ω3)δ0(ω1+ω2)δ0(ω3+ω4)
+ (2π)2Suu(ω1)Suu(ω2)δ0(ω1+ω3)δ0(ω2+ω4)
+ (2π)2Suu(ω1)Suu(ω2)δ0(ω1+ω4)δ0(ω2+ω3)
(88)
Example 5.8
To illustrate the use of these formulas, the expected value of the output will
be computed for the case where the input random process is real, stationary, zero-mean,
Gaussian and white with unit intensity. That is, Ruu(τ) = δ0(τ). Also, it will be assumed
that the system is described in terms of the symmetric kernel or symmetric system
function. In this case substitution of (85) into (67) gives, for n ≥ 1, E [y (t)] = 0 when n is
odd, and
E [y (t)] =
−∞∫
∞
hsym(σ1, . . . ,σn)
pΣ j,kΠ
n
δ0(σk−σj) dσ1 . . . dσn
=
pΣ
−∞∫
∞
hsym(σ1, . . . ,σn)
j,kΠ
n
δ0(σk−σj) dσ1 . . . dσn , n even
Now in each term of the sum, the n /2 impulses can be integrated out, and this will leave
the kernel with only n /2 distinct arguments.
By symmetry of the kernel, the like
arguments can be arranged in pairs, and since they are just variables of integration, they
can be labeled in the form hsym(σ1,σ1, . . . ,σn /2,σn /2). There will be (n −1)(n −3) . . . (1)
terms of this type, so the result is
E [y (t)] =
(n /2)!2n /2
n !
_________
−∞∫
∞
hsym(σ1,σ1, . . . ,σn /2,σn /2) dσ1 . . . dσn /2 , n even
Using (82), the cross-spectral density and output power spectral density will now be
computed for a degree-n system described by the symmetric transfer function with a real,
stationary, zero-mean, Gaussian-random-process input. For the cross-spectral density, it is
clear from (87) that Syu(ω1) = 0 for n + 1 odd, that is, for a homogeneous system of even
degree. When n + 1 is even, a simple substitution gives
223

Syu(ω1) =
(2π)(n −1)/2
1
_________
−∞∫
∞
Hsym(γ1, . . . ,γn)δ0(ω1−γ1−. . . −γn)
pΣ j,kΠ
n +1
Suu(γj)δ0(γj+γk) dγ1 . . . dγn +1
=
(2π)(n −1)/2
1
_________
pΣ
−∞∫
∞
Hsym(γ1, . . . ,γn)δ0(ω1−γ1−. . . −γn)
j,kΠ
n +1
Suu(γj)δ0(γj+γk) dγ1 . . . dγn +1
(89)
Before working on this expression in the general case, an example is instructive. And, of
course, the n +1 = 2 case is too simple, giving just what was derived for linear systems.
Example 5.9
For n +1 = 4, (89) yields
Syu(ω1) =
2π
1
___
−∞∫
∞
Hsym(γ1,γ2,γ3)δ0(ω1−γ1−γ2−γ3)Suu(γ1)Suu(γ3)δ0(γ1+γ2)δ0(γ3+γ4) dγ1dγ2dγ3dγ4
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2,γ3)δ0(ω1−γ1−γ2−γ3)Suu(γ1)Suu(γ2)δ0(γ1+γ3)δ0(γ2+γ4) dγ1dγ2dγ3dγ4
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2,γ3)δ0(ω1−γ1−γ2−γ3)Suu(γ1)Suu(γ2)δ0(γ1+γ4)δ0(γ2+γ3) dγ1dγ2dγ3dγ4
Integrating with respect to γ4 in each of these terms gives
Syu(ω1) = 2π
1
___
−∞∫
∞
Hsym(γ1,γ2,γ3)δ0(ω1−γ1−γ2−γ3)Suu(γ1)Suu(γ3)δ0(γ1+γ2) dγ1dγ2dγ3
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2,γ3)δ0(ω1−γ1−γ2−γ3)Suu(γ1)Suu(γ2)δ0(γ1+γ3) dγ1dγ2dγ3
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2,γ3)δ0(ω1−γ1−γ2−γ3)γ2)δ0(γ2+γ3) dγ1dγ2dγ3
Now integrate the ﬁrst term with respect to γ3, the second term with respect to γ2, and the
third term with respect to γ1 to obtain
Syu(ω1) = 2π
1
___
−∞∫
∞
Hsym(γ1,γ2,ω1−γ1−γ2)Suu(γ1)Suu(ω1−γ1−γ2)δ0(γ1+γ2) dγ1dγ2
+ 2π
1
___
−∞∫
∞
Hsym(γ1,ω1−γ1−γ3,γ3)Suu(γ1)Suu(ω1−γ1−γ3)δ0(γ1+γ3) dγ1dγ3
+ 2π
1
___
−∞∫
∞
Hsym(ω1−γ2−γ3,γ2,γ3)Suu(ω1−γ2−γ3)Suu(γ2)δ0(γ2+γ3) dγ2dγ3
224

Finally, integrating with respect to γ2 in the ﬁrst term and γ3 in the remaining two terms
gives
Syu(ω1) = 2π
1
___
−∞∫
∞
Hsym(γ1,−γ1,ω1)Suu(γ1)Suu(ω1) dγ1
+ 2π
1
___
−∞∫
∞
Hsym(γ1,ω1,−γ1))Suu(γ1)Suu(ω1) dγ1
+ 2π
1
___
−∞∫
∞
Hsym(ω1,γ2,−γ2)Suu(ω1)Suu(γ2) dγ2
But from this expression it is clear that since Hsym(ω1,ω2,ω3) is symmetric, the
input/output cross-spectral density is
Syu(ω) = 2π
3
___ Suu(ω)
−∞∫
∞
Hsym(ω,γ,−γ)Suu(γ) dγ
(90)
This example illustrates the fact that all terms in the summation in (89) are identical
since Hsym(ω1, . . . ,ωn) is symmetric. Thus to get the general expression for the cross-
spectral density when n +1 is even, it is necessary to work only with a single term and
multiply the result by the number of terms in the summation.
Choosing the term
corresponding to the set of pairs (1,2),(3,4), . . . ,(n,n +1) gives
Syu(ω1) =
(2π)(n −1)/2
n (n −2) . . . (1)
_____________
−∞∫
∞
Hsym(γ1, . . . ,γn)δ0(ω1−γ1−. . . −γn)Suu(γ1)
Suu(γ3) . . . Suu(γn)δ0(γ1+γ2)δ0(γ3+γ4) . . . δ0(γn+γn +1) dγ1 . . . dγn +1
Integrating with respect to γ2, then γ4, and so on, gives, with a relabeling of variables,
Syu(ω) =
(2π)(n −1)/2
n (n −2)(n −4) . . . (1)
_________________ Suu(ω)
−∞∫
∞
Hsym(ω,γ1,−γ1,γ2,−γ2, . . . ,γ
2
n −1
_____,−γ
2
n −1
_____)
Suu(γ1)Suu(γ2) . . . Suu(γ
2
n −1
_____) dγ1 . . . dγ
2
n −1
_____ , n +1 even
(91)
while Syu(ω) = 0, for n +1 odd.
Now I begin what starts out appearing to be a similar calculation for the output
power spectral density for a homogeneous system with real, stationary, zero-mean,
Gaussian input. Again the symmetric kernel and transfer function representations are used
for the system. Substituting (87) into (84) gives
225

Syy(ω) =
(2π)n −1
1
_______
pΣ
−∞∫
∞
Hsym(γ1, . . . ,γn)Hsym(γn +1, . . . ,γ2n)
δ0(ω−γ1−. . . −γn)
j,kΠ
2n
Suu(γj)δ0(γj+γk) dγ1 . . . dγ2n
(92)
But this situation is considerably more complex than the cross-spectral density case
because Hsym(γ1, . . . ,γn)Hsym(γn +1, . . . ,γ2n) is in general not symmetric for a symmetric
system function. Thus, different types of terms will arise in the summation. Indeed, the
general form for Syy(ω) is extremely complicated. I will derive the result for n = 2 and
state the result for n = 3, leaving further considerations to the assiduous reader, or to the
literature.
Example 5.10
For n = 2, (92) becomes
Syy(ω) = 2π
1
___
−∞∫
∞
Hsym(γ1,γ2)Hsym(γ3,γ4)δ0(ω−γ1−γ2)Suu(γ1)Suu(γ3)
δ0(γ1+γ2)δ0(γ3+γ4) dγ1dγ2dγ3dγ4
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2)Hsym(γ3,γ4)δ0(ω−γ1−γ2)Suu(γ1)Suu(γ2)
δ0(γ1+γ3)δ0(γ2+γ4) dγ1dγ2dγ3dγ4
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2)Hsym(γ3,γ4)δ0(ω−γ1−γ2)Suu(γ1)Suu(γ2)
δ0(γ1+γ4)δ0(γ2+γ3) dγ1dγ2dγ3dγ4
Integrating each term with respect to γ4 gives
Syy(ω) = 2π
1
___
−∞∫
∞
Hsym(γ1,γ2)Hsym(γ3,−γ3)δ0(ω−γ1−γ2)Suu(γ1)Suu(γ3)
δ0(γ1+γ2) dγ1dγ2dγ3
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2)Hsym(γ3,−γ2)δ0(ω−γ1−γ2)Suu(γ1)Suu(γ2)
δ0(γ1+γ3) dγ1dγ2dγ3
+ 2π
1
___
−∞∫
∞
Hsym(γ1,γ2)Hsym(γ3,−γ1)δ0(ω−γ1−γ2)Suu(γ1)Suu(γ2)
δ0(γ2+γ3) dγ1dγ2dγ3
It should be fairly clear how to proceed. Integrating the ﬁrst term with respect to γ2 and
the last two terms with respect to both γ2 and γ3 gives
226

Syy(ω) = 2π
1
___ δ0(ω)
−∞∫
∞
Hsym(γ1,−γ1)Hsym(γ3,−γ3)Suu(γ1)Suu(γ3) dγ1dγ3
+ 2π
1
___
−∞∫
∞
Hsym(γ1,ω−γ1)Hsym(−γ1,−ω+γ1)Suu(γ1)Suu(ω−γ1) dγ1
+ 2π
1
___
−∞∫
∞
Hsym(γ1,ω−γ1)Hsym(−ω+γ1,−γ1)Suu(γ1)Suu(ω−γ1) dγ1
Using the symmetry of the system function to combine the last two terms allows the output
power spectral density for the n = 2 case to be written in the form
Syy(ω) = 2π
1
___ δ0(ω)
−∞∫
∞
Hsym(γ1,−γ1)Hsym(γ2,−γ2)Suu(γ1)Suu(γ2) dγ1 dγ2
+ π
1__
−∞∫
∞
Hsym(ω−γ,γ)Hsym(−ω+γ,−γ)Suu(γ)Suu(ω−γ) dγ
(93)
This example illustrates the different types of terms that can arise in the general
formula. For the record, I also list the result for degree-3 homogeneous systems.
Syy(ω) =
(2π)2
6
_____
−∞∫
∞
Hsym(ω−γ1−γ2,γ1,γ2)Hsym(−ω+γ1+γ2,−γ1,−γ2)
Suu(ω−γ1−γ2)Suu(γ1)Suu(γ2) dγ1 dγ2
+
(2π)2
9
_____ Suu(ω)
−∞∫
∞
Hsym(ω,γ1,−γ1)Hsym(−ω,γ2,−γ2)Suu(γ1)Suu(γ2) dγ1 dγ2 (94)
Example 5.11
Suppose that the input to the system shown in Figure 5.1 is a real,
stationary, zero-mean, Gaussian random process with power spectral density
Suu(ω) =
ω2+α2
A2
_______
Figure 5.1. A degree-2 homogeneous system.
 
To ﬁnd the power spectral density of the output, ﬁrst note that the symmetric system
function in this case is
Hsym(ω1,ω2) = iω1 + iω2 + β
B
_____________
Thus
227
(.)2
y
____
s + β
B
u

Syy(ω) = 2π
1
___ δ0(ω)
−∞∫
∞
−∞∫
∞
β2
B2
___
(γ1
2+α2)(γ2
2+α2)
A4
______________ dγ1dγ2
+ π
1__
−∞∫
∞
(iω+β)(−iω+β)
B2
______________
(γ2+α2)[(ω−γ)2+α2]
A4
__________________ dγ
=
2πβ2
A4B2
______ δ0(ω)[
−∞∫
∞
γ2+α2
1
______ dγ]2 +
ω2+β2
B2A4/π
_______
−∞∫
∞
(γ2+α2)[(ω−γ)2+α2]
1
__________________ dγ
Performing the integrations (tables are allowed) gives
−∞∫
∞
γ2+α2
1
______ dγ = α
π__
−∞∫
∞
(γ2+α2)[(ω−γ)2+α2]
1
__________________ dγ =
ω2+4α2
4π2
________
Thus the output power spectral density is
Syy(ω) =
2α2β2
A4B2π
_______ δ0(ω) +
(ω2+β2)(ω2+4α2)
4πA4B2
________________
Now consider the case of Volterra and polynomial systems with random inputs.
Since the expressions are formidable, the results will be given only for the ﬁrst few terms -
at least in the output power spectral density calculation.
It will be convenient to use the notation
y (t) =
n =1Σ
∞
yn(t)
(95)
where yn(t) is the output of the degree-n homogeneous term. And to be absolutely speciﬁc,
a subscript will be added to the kernels and transfer functions to indicate the degree. When
the input is a sample function from an arbitrary, real random process, the input/output
cross-correlation can be written as
Ryu(t 1,t 2) = E [y (t 1)u (t 2)] =
n =1Σ
∞
E [yn(t 1)u (t 2)] =
n =1Σ
∞
Rynu(t 1,t 2)
(96)
where Rynu(t 1,t 2) denotes the cross-correlation for the degree-n homogeneous case just
considered.
Thus the cross-correlation and cross-spectral density expressions for
polynomial or Volterra systems are found simply by summing the previously derived
expressions over n, the subsystem degree. For example, when the input is real, stationary,
zero-mean, and Gaussian, the input/output cross-spectral density is given by
228

Syu(ω) =Suu(ω)
odd n ≥1
Σ
(2π)(n −1)/2
(n)(n −2) . . . (1)
______________
−∞∫
∞
Hnsym(ω,γ1,−γ1, . . . ,γ
2
n −1
_____,−γ
2
n −1
_____)
Suu(γ1) . . . Suu(γ
2
n −1
_____) dγ1 . . . dγ
2
n −1
_____
(97)
Returning to the case of an arbitrary real random input, the calculation of the output
autocorrelation or power spectral density is considerably more complex. To see why, note
that
Ryy(t 1,t 2) = E [y (t 1)y (t 2)] =
n =1Σ
∞
m=1
Σ
∞
E [yn(t 1)ym(t 2)]
(98)
This expression can be written in the notation
Ryy(t 1,t 2) =
n =1Σ
∞
m=1
Σ
∞
Rynym(t 1,t 2)
(99)
where Rynym(t 1,t 2) = E [yn(t 1)ym(t 2)] is called the partial output autocorrelation. The
computation of this term is only slightly different from the computations considered
earlier, and the tools are quite familiar by now.
Performing the obvious calculations gives an expression for the partial output
autocorrelation,
Rynym(t 1,t 2) =
−∞∫
∞
hnsym(σ1, . . . ,σn)hmsym(σn +1, . . . ,σn +m)
Ruu
(n +m)(t 1−σ1, . . . ,t 1−σn,t 2−σn +1, . . . ,t 2−σn +m) dσ1 . . . dσn +m (100)
Just as before when faced with expressions of this form, it is convenient to deﬁne a
multivariable partial output autocorrelation by
Rˆynym(t 1, . . . ,tn +m) =
−∞∫
∞
hnsym(σ1, . . . ,σn)hmsym(σn +1, . . . ,σn +m)
Ruu
(n +m)(t 1−σ1, . . . ,tn +m−σn +m) dσ1 . . . dσn +m (101)
so that
Rynym(t 1,t 2) = Rˆynym(t 1, . . . ,tn +m) tn +1= . . . =tn +m=t 2
t 1= . . . =tn=t 1
(102)
Again, the advantage of this notation is that the convolution properties of the Fourier
transform can be applied in a direct fashion, and the variable associations can be handled
separately. Letting
229

Synym(ω1,ω2) = F[Rynym(t 1,t 2)]
Sˆynym(ω1, . . . ,ωn +m) = F[Rˆynym(t 1, . . . ,tn +m)]
Suu
(n +m)(ω1, . . . ,ωn +m) = F[Ruu
(n +m)(t 1, . . . ,tn +m)]
(103)
and using the system function deﬁned previously gives
Sˆynym(ω1, . . . ,ωn +m) = Hnsym(ω1, . . . ,ωn)Hmsym(ωn +1, . . . ,ωn +m)Suu
(n +m)(ω1, . . . ,ωn +m)
(104)
Repeating the derivation leading to (79) gives, in the present setting
Synym(ω1,ω2) =
(2π)n +m−2
1
_________
−∞∫
∞
Sˆynym(γ1, . . . ,γn +m)δ0(ω1−γ1−. . . −γn)
δ0(ω2−γn +1−. . . −γn +m) dγ1 . . . dγn +m(105)
Furthermore, for stationary random process inputs, the partial output power spectral
density is given by
Synym(ω) =
(2π)n +m−1
1
_________
−∞∫
∞
Sˆynym(γ1, . . . ,γn +m)δ0(ω−γ1−. . . −γn) dγ1 . . . dγn +m (106)
Of course, this formula checks with (84) for the case m = n.
Now assume that the input is real, stationary, zero-mean, Gaussian, and with power
spectral density Suu(ω). Substituting (87) and (104) into (106) gives
Synym(ω) = 0 , n +m odd
(107)
and
Synym(ω) =
(2π)(n +m−2)/2
1
___________
pΣ
−∞∫
∞
Hnsym(γ1, . . . ,γn)Hmsym(γn +1, . . . ,γn +m)
δ0(ω−γ1−. . . −γn)
j,kΠ
n +m
Suu(γj)δ0(γj+γk) dγ1 . . . dγn +m , n +m even (108)
The reduction of this expression to more explicit form is a combinatorial problem of some
complexity because the integrand lacks symmetry. I will be content to work out the terms
that give the output power spectral density for polynomial systems of degree 3.
230

Example 5.12
To compute Syy(ω) for the case of a degree-3 polynomial system, the
terms Synym(ω) must be computed for n,m = 1,2,3. But it is evident that
Sy 1y 2(ω) = Sy 2y 1(ω) = Sy 2y 3(ω) = Sy 3y 2(ω) = 0
For n = m = 1,2,3 the partial output power spectral densities have been calculated
previously, and are given in (64), (93), and (94). For n = 1 and m = 3, (108) gives
Sy 1y 3(ω) = 2π
1
___
pΣ
−∞∫
∞
H1(γ1)H3sym(γ2,γ3,γ4)δ0(ω−γ1)
j,kΠ
4
Suu(γj)δ0(γj+γk) dγ1 . . . dγ4
= 2π
1
___
−∞∫
∞
H1(γ1)H3sym(γ2,γ3,γ4)δ0(ω−γ1)Suu(γ1)Suu(γ3)
δ0(γ1+γ2)δ0(γ3+γ4) dγ1 . . . dγ4
+ 2π
1
___
−∞∫
∞
H1(γ1)H3sym(γ2,γ3,γ4)δ0(ω−γ1)Suu(γ1)Suu(γ2)
δ0(γ1+γ3)δ0(γ2+γ4) dγ1 . . . dγ4
+ 2π
1
___
−∞∫
∞
H1(γ1)H3sym(γ2,γ3,γ4)δ0(ω−γ1)Suu(γ1)Suu(γ2)
δ0(γ1+γ4)δ0(γ2+γ3) dγ1 . . . dγ4
Performing the integrations gives
Sy 1y 3(ω) = 2π
3
___ H1(ω)Suu(ω)
−∞∫
∞
H3sym(−ω,γ,−γ)Suu(γ) dγ
In a similar fashion Sy 3y 1(ω) can be computed. Alternatively, the easily proved fact that
Sy 3y 1(ω) = Sy 1y 3(−ω) can be used to obtain
Sy 3y 1(ω) = 2π
3
___ H1(−ω)Suu(ω)
−∞∫
∞
H3sym(ω,γ,−γ)Suu(γ) dγ
Now, collecting together all the terms gives the expression
231

Syy(ω) = H1(ω)H1(−ω)Suu(ω) + 2π
3
___ H1(ω)Suu(ω)
−∞∫
∞
H3sym(−ω,γ,−γ)Suu(γ) dγ
+ 2π
3
___ H1(−ω)Suu(ω)
−∞∫
∞
H3sym(ω,γ,−γ)Suu(γ) dγ
+ 2π
1
___ δ0(ω)
−∞∫
∞
−∞∫
∞
H2sym(γ1,−γ1)H2sym(γ2,−γ2)Suu(γ1)Suu(γ2) dγ1dγ2
+ π
1__
−∞∫
∞
H2sym(ω−γ,γ)H2sym(−ω+γ,−γ)Suu(γ)Suu(ω−γ) dγ
+
(2π)2
6
_____
−∞∫
∞
−∞∫
∞
H3sym(ω−γ1−γ2,γ1,γ2)H3sym(−ω+γ1+γ2,−γ1,−γ2)
Suu(ω−γ1−γ2)Suu(γ1)Suu(γ2) dγ1dγ2
+
(2π)2
9
_____ Suu(ω)
−∞∫
∞
−∞∫
∞
H3sym(ω,γ1,−γ1)H3sym(−ω,γ2,−γ2)
Suu(γ1)Suu(γ2) dγ1dγ2
(109)
Example 5.13
For the phase-locked loop introduced in Example 3.3, the ﬁrst three
symmetric transfer functions are shown in Example 3.8 to be
H (s) = s + K
1
______ , H2sym(s 1,s 2) = 0,
H3sym(s 1,s 2,s 3) = (s 1+s 2+s 3+K)(s 1+K)(s 2+K)(s 3+K)
K /6
________________________________
Suppose the message signal is real, stationary, zero-mean, Gaussian white noise with
intensity A. Thus,
Ruu(τ) = Aδ0(τ) , Suu(ω) = A
Then it is straightforward to verify that, through degree 3, the loop error signal x (t) also is
zero mean. In particular, from (65) the degree-1 component of x (t) is zero mean, and from
Example 5.8 the degree-2 and degree-3 components are also.
To illustrate the calculation of the power spectral density for x (t), I will evaluate the
terms in (109) that are of degree ≤2 in the noise intensity A. These terms give
Sxx(ω) = AH (ω)H (−ω) + 2π
3A2
____H (ω)
−∞∫
∞
H3sym(−ω,γ,−γ) dγ
+ 2π
3A2
____ H (−ω)
−∞∫
∞
H3sym(ω,γ,−γ) dγ
232

Clearly,
AH (ω)H (−ω) =
ω2 + K 2
A
________
and a simple calculation gives
2π
3A2
____ H (ω)
−∞∫
∞
H3sym(−ω,γ,−γ) dγ =
(ω2+K 2)(−iω+K)
KA 2/4π
________________
−∞∫
∞
γ2+K 2
1
_______ dγ
=
(ω2+K 2)(−iω+K)
A2/4
________________
Since the third term can be obtained form the second by replacing ω by −ω, the power
spectral density of the loop error is, through degree 2 in A,
Sxx(ω) =
ω2 + K 2
A
________ +
(ω2 + K 2)2
KA 2/2
__________
5.5 The Wiener Orthogonal Representation
A
major
difﬁculty
in
computing
the
output
power
spectral
density,
or
autocorrelation, for a polynomial or Volterra system is the profusion of partial output
spectral densities or autocorrelations (cross-terms). For this reason, and for other reasons
that will be discussed in Chapter 7, I will now consider a series representation that has
certain orthogonality properties with respect to the statistical characterization of the
response. Under appropriate convergence conditions, this Wiener series representation
can be viewed as a rearrangement of the terms in a Volterra series representation.
However, this viewpoint can be confusing, and it probably is best to regard the Wiener
representation as a separate topic, at least at the outset.
Throughout this section it is assumed that the input signal is a sample function from
a real, stationary, zero-mean, white Gaussian random process with intensity A. The
exposition will be in terms of inﬁnite series, with the usual avoidance of convergence
issues. Actually, the convergence properties of the Wiener representation are naturally
addressed in the mean square sense, and it can be shown that the resulting conditions are
less restrictive than those for the Volterra series. These issues will be left to the literature
cited in Section 5.6.
The Wiener representation for a system takes the form
y (t) =
n =0Σ
∞
Gn[kn,u (t)]
(110)
where each Wiener operator Gn[kn,u (t)] is a degree-n polynomial operator that is
speciﬁed (in a yet to be determined, and at this point nonobvious, manner) by a symmetric
Wiener kernel kn(t 1, . . . ,tn). Notice that the operator notation from Chapter 1 has been
changed slightly in that the subscript indicates now the polynomial degree of the operator,
and the dependence on kn is displayed. Also there is no subscript "sym" on the Wiener
233

kernel, even though it is symmetric. This conforms with the traditional notation, and helps
to
distinguish
the
Wiener
kernel
from
the
symmetric
Volterra
series
kernel
hnsym(t 1, . . . ,tn).
The important condition to be imposed is that what might be called the partial output
autocorrelations in this new representation satisfy
E [Gn[kn,u (t +τ)]Gm[km,u (t)]] = 0 , for all τ, m ≠n
(111)
Of course, when this condition is satisﬁed, the output autocorrelation is given by
Ryy(τ) =
n =0Σ
∞
E [Gn[kn,u (t +τ)]Gn[kn,u (t)]]
(112)
Although the Wiener representation can be determined through an elegant, general
argument, it is instructive to begin in an elementary fashion. (A more elegant derivation
will be used for the discrete-time case in Chapter 6.) The approach is to ﬁnd Gn[kn,u (t)]
by requiring that it be a degree-n polynomial operator that satisﬁes
E [Gn[kn,u (t +τ)]Fj[u (t)]] = 0 , for all τ, j = 0,1, . . . ,n −1
(113)
where Fj[u (t)] is any homogeneous operator of degree j. Of course this condition
guarantees that Gn[kn,u (t)] is orthogonal to any polynomial operator of degree n −1 or
less. In the following development, the symmetric kernel corresponding to Fj[u (t)] will be
denoted by fjsym(t 1, . . . ,tj), except that the "sym" is superﬂuous when j = 0,1.
The degree-0 Wiener operator is deﬁned to be simply G0[k 0,u (t)] = k 0. The
degree-1 Wiener operator is assumed to take the general form
G1[k 1,u (t)] =
−∞∫
∞
k 1(σ)u (t −σ) dσ + k 1,0
(114)
where k 1(t) is the degree-1 Wiener kernel, and k 1,0 is a constant that remains to be chosen.
This operator must be orthogonal to any degree-0 homogeneous operator F0[u (t)] = f 0,
that is
0 = E [G1[k 1,u (t +τ)]F0[u (t)]]
=
−∞∫
∞
f 0k 1(σ)E [u (t +τ−σ)] dσ + f 0k 1,0, for all τ
for any f 0. And since the expected value in the ﬁrst term is 0, this condition can be
satisﬁed by taking k 1,0 = 0. Thus the degree-1 Wiener operator takes the (familiar) form
G1[k 1,u (t)] =
−∞∫
∞
k 1(σ)u (t −σ) dσ
(115)
So far the Wiener representation looks just like a Volterra series representation, except that
the kernels may be different.
234

Now I proceed to degree 2, where more interesting things begin to happen. The
general form of G2[k 2,u (t)] is
G2[k 2,u (t)] =
−∞∫
∞
k 2(σ1,σ2)u (t −σ1)u (t −σ2) dσ1dσ2
+
−∞∫
∞
k 2,1(σ1)u (t −σ1) dσ1 + k 2,0
(116)
where k 2(t 1,t 2) is symmetric, and where the conditions to be satisﬁed are
E [G2[k 2,u (t +τ)]F1[u (t)]] = 0
E [G2[k 2,u (t +τ)]F0[u (t)]] = 0
(117)
for all τ. The ﬁrst condition gives
0 =
−∞∫
∞
k 2(σ1,σ2)f 1(σ)E [u (t +τ−σ1)u (t +τ−σ2)u (t −σ)] dσ1dσ2dσ
+
−∞∫
∞
k 2,1(σ1)f 1(σ)E [u (t +τ−σ1)u (t −σ)] dσdσ1
+
−∞∫
∞
k 2,0f 1(σ)E [u (t −σ)] dσ
= A
−∞∫
∞
k 2,1(σ+τ)f 1(σ) dσ
To guarantee that this is satisﬁed regardless of f 1(t), take k 2,1(t) = 0. Now the second
condition in (117) gives
0 =
−∞∫
∞
k 2(σ1,σ2)f 0E [u (t +τ−σ1)u (t +τ−σ2)] dσ1dσ2 + k 2,0f 0
= A
−∞∫
∞
k 2(σ,σ)f 0 dσ + k 2,0f 0
This can be satisﬁed by taking
k 2,0 = −A
−∞∫
∞
k 2(σ,σ) dσ
so that the degree-2 Wiener operator is
G2[k 2,u (t)] =
−∞∫
∞
k 2(σ1,σ2)u (t −σ1)u (t −σ2)dσ1dσ2 −A
−∞∫
∞
k 2(σ,σ)dσ
(118)
This is the ﬁrst illustration of how the Wiener polynomial operators are speciﬁed by a
single kernel. Also, notice that there is an implicit technical assumption here, namely, that
235

the integral of k 2(t,t) is ﬁnite.
I will work out one more just to show that nothing surprising happens. The degree-3
Wiener operator will take the general form
G3[k 3,u (t)] =
−∞∫
∞
k 3(σ1,σ2,σ3)u (t −σ1)u (t −σ2)u (t −σ3) dσ1dσ2dσ3
+
−∞∫
∞
k 3,2(σ1,σ2)u (t −σ1)u (t −σ2) dσ1dσ2
+
−∞∫
∞
k 3,1(σ1)u (t −σ1) dσ1 + k 3,0
(119)
where the degree-3 Wiener kernel k 3(t 1,t 2,t 3) is symmetric. Imposing the condition of
orthogonality to degree-0 homogeneous operators gives that
A
−∞∫
∞
k 3,2(σ,σ)f 0 dσ + k 3,0f 0 = 0
must hold for all f 0. To satisfy this condition, set
k 3,0 = −A
−∞∫
∞
k 3,2(σ,σ) dσ
Orthogonality with respect to degree-1 homogeneous operators gives the condition
0 = 3A2
−∞∫
∞
k 3(σ1,σ1,σ+τ)f 1(σ) dσ1dσ + A
−∞∫
∞
k 3,1(σ+τ)f 1(σ) dσ
for all f 1(t). To satisfy this, set
k 3,1(t) = −3A
−∞∫
∞
k 3(σ,σ,t) dσ
Up to this point the degree-3 Wiener operator has been specialized to the form
G3[k 3,u (t)] =
−∞∫
∞
k 3(σ1,σ2,σ3)u (t −σ1)u (t −σ2)u (t −σ3) dσ1dσ2dσ3
+
−∞∫
∞
k 3,2(σ1,σ2)u (t −σ1)u (t −σ2) dσ1dσ2
−3A
−∞∫
∞
k 3(σ1,σ1,σ)u (t −σ) dσ1dσ −A
−∞∫
∞
k 3,2(σ,σ) dσ
(120)
Imposing the (ﬁnal) condition that (120) be orthogonal to all degree-2 homogeneous
operators leads, after some calculation, to the choice k 3,2(t 1,t 2) = 0. Thus the degree-3
Wiener operator in (110) is
236

G3[k 3,u (t)] =
−∞∫
∞
k 3(σ1,σ2,σ3)u (t −σ1)u (t −σ2)u (t −σ3) dσ1dσ2dσ3
−3A
−∞∫
∞
k 3(σ1,σ1,σ)u (t −σ) dσ1dσ
(121)
The general result can be stated as follows.
Theorem 5.1
The degree-n Wiener operator is given by
Gn[kn,u (t)] =
i =0Σ
[n /2]
2i(n −2i)!i !
(−1)in !Ai
__________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi)
dτ1 . . . dτiu (t −σ1) . . . u (t −σn −2i) dσ1 . . . dσn −2i
(122)
where [n /2] indicates the greatest integer ≤ n /2, the Wiener kernel kn(t 1, . . . ,tn) is
symmetric, and where A is the intensity of the real, stationary, zero-mean, Gaussian-
white-noise input.
Proof
(Sketch) Suppose n is an even integer for deﬁniteness. (The proof for odd n is
similar.) Then the square brackets can be erased from the upper limit of the summation
sign. Also, retaining the notation Fj[u (t)] for an arbitrary degree-j homogeneous operator
with symmetric kernel fjsym(t 1, . . . ,tj),
E [Gn[kn,u (t +τ)]Fj[u (t)]] = 0 , odd j < n
This is because all terms will involve the expected value of a product of an odd number of
zero-mean Gaussian random variables. Thus, it remains to show that
E [Gn[kn,u (t +τ)]F2j[u (t)]] = 0 ,
j = 0,1, . . . ,
2
n −2
____
For j = 0 this condition reduces to showing that E [Gn[kn,u (t)]] = 0 for n > 0. Direct
calculation using (85) gives
E [Gn[kn,u (t)]] =
i =0Σ
n /2
2i(n −2i)!i !
(−1)in !Ai
__________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi)
dτ1 . . . dτi A(n −2i)/2
pΣ
j,kΠ
n −2i
δ0(σj−σk) dσ1 . . . dσn −2i
For each ﬁxed i, integrating out the (n −2i)/2 impulses in each product will yield identical
results because of the symmetry of the Wiener kernel. Furthermore, from (85) there are
(
2
n −2i
_____)!2(n −2i)/2
(n −2i)!
______________ =
( 2
n__ −i)!2n /22−i
(n −2i)!
_____________
products in the summation in the i th term. Therefore, with considerable relabeling of
237

variables, I can write
E [Gn[kn,u (t)]] =
2n /2
n !An /2
_______
i =0Σ
n /2
( 2
n__ −i)!i !
(−1)i
_________
−∞∫
∞
kn(σ1,σ1, . . . ,σn /2,σn /2) dσ1 . . . dσn /2
But
i =0Σ
n /2
( 2
n__ −i)!i !
(−1)i
_________ = (n /2)!
1
______
i =0Σ
n /2
(−1)i B
D
i
n /2E
G = 0
so the result is
E [Gn[kn,u (t)]] = 0 , n = 1,2, . . .
For j = 1 it must be shown that, assuming n > 2,
E [Gn[kn,u (t +τ)]F2[u (t)]] = 0
where the degree-2 operator F2[u (t)] is arbitrary. Again, proceeding by direct calculation
gives
E [Gn[kn,u (t +τ)]F2[u (t)]] =
i =0Σ
n /2
2i(n −2i)!i !
(−1)in !Ai
__________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi)
dτ1 . . . dτi f 2sym(σn −2i +1,σn −2i +2) E [u (t +τ−σ1)
. . . u (t +τ−σn −2i)u (t −σn −2i +1)u (t −σn −2i +2)] dσ1 . . . dσn −2i +2
=
i =0Σ
n /2
(n −2i)!i !2i
(−1)in !Ai
__________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi) dτ1 . . . dτi
f 2sym(σn −2i +1,σn −2i +2) A(n −2i +2)/2
pΣ
j,kΠ
n −2i +2
δ0(σj−σk) dσ1 . . . dσn −2i +2
First, for each ﬁxed i consider all the product terms in
pΣ that contain a factor of the form
δ0(σn −2i +1−σn −2i +2). Integrating out this impulse gives an identical result in each term,
and there are
(
2
n −2i
_____)!2(n −2i)/2
(n −2i)!
______________ =
( 2
n__ −i)!2n /22−i
(n −2i)!
_____________
such terms for each i. The remaining products of (n −2i)/2 impulses will contain all
possible pairs of arguments from σ1, . . . ,σn −2i. Thus, these terms give
238

i =0Σ
n /2
( 2
n__ −i)!i !2n /2
(−1)in !A(n +2)/2
_____________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi) dτ1 . . . dτi
f 2sym(σn −2i +1,σn −2i +1)
pΣ
j,kΠ
n −2i
δ0(σj−σk) dσ1 . . . dσn −2i +1
=
2n /2
n !A(n +2)/2
_________
i =0Σ
n /2
( 2
n__ −i)!i !
(−1)i
_________
−∞∫
∞
f 2sym(σn −2i +1,σn −2i +1) dσn −2i +1
−∞∫
∞
kn(σ1,σ1, . . . ,σn /2,σn /2) dσ1 . . . dσn /2
= 0
For emphasis, I will restate the key fact that has just been used. The set of all those terms
in
pΣ
j,kΠ
n −2i +2
δ0(σj −σk)
that contain an impulse of the form δ0(σn −2i +1−σn −2i +2) can be written as
δ0(σn −2i +1 −σn −2i +2)
pΣ
j,kΠ
n −2i
δ0(σj −σk)
This result will be used in the sequel. Now, for each ﬁxed i consider the remaining terms,
all of which contain factors of the form δ0(σj−σn −2i +1)δ0(σk−σn −2i +2) for j,k ≤ n −2i. Of
course, these terms occur only for i < n /2, and there are
(
2
n −2i +2
________)!2(n −2i +2)/2
(n −2i +2)!
___________________ −
( 2
n__ −i)!2(n −2i)/2
(n −2i)!
______________ =
( 2
n__ −i)!2n /22−i
(n −2i)!
_____________ (n −2i)
such terms for each i. Because of symmetry, all these terms will be identical after the
impulses are integrated out. Thus, these terms give
i =0Σ
n /2−1
( 2
n__−i)!i !2n /2
(−1)in !A(n +2)/2(n −2i)
___________________
−∞∫
∞
kn(σ1,σ1, . . . ,σn /2,σn /2)
f 2sym(σ1,σ2) dσ1 . . . dσn /2 = 0
since
i =0Σ
n /2−1
(−1)i B
D
i
n /2E
G (n −2i) = n
i =0Σ
n /2−1
(−1)i B
D
i
n /2−1E
G = 0
It now should be clear that to verify the orthogonality condition in general requires just
this type of calculation for larger values of j. This tedious exercise is omitted.
239

There are some general features to notice about Gn[kn,u (t)]. It is a degree-n
polynomial
operator
that
contains
homogeneous
terms
of
degree
n, n −2, . . . ,1 (n odd) or 0 (n even). However, all the homogeneous terms are speciﬁed by
the degree-n, symmetric, Wiener kernel kn(t 1, . . . ,tn) and by the input noise intensity A.
Lest the reader be puzzled over the notational abuse in (122) when i = 0, that term is
precisely
−∞∫
∞
kn(σ1, . . . ,σn)u (t −σ1) . . . u (t −σn) dσ1 . . . dσn
Finally, it should be clear that certain integrability conditions on the Wiener kernel must
hold if (122) is to make sense technically.
Now suppose a system is described by the Wiener representation in (111). Then, by
the orthogonality property, the output autocorrelation is given by
Ryy(τ) = E [y (t +τ)y (t)] =
n =0Σ
∞
E [Gn[kn,u (t +τ)]Gn[kn,u (t)]]
(123)
Before computing the general term, it is instructive to work out the ﬁrst few. For n = 0, it
is clear that
E [G0[k 0,u (t +τ)]G0[k 0,u (t)] = E [k0
2] = k0
2
(124)
For n = 1 the calculation is only slightly less trivial since this is the usual linear case:
E [G1[k 1,u (t +τ)]G1[k 1,u (t)]] =
−∞∫
∞
k 1(σ1)k 1(σ2)E [u (t +τ−σ1)u (t −σ2)] dσ1dσ2
= A
−∞∫
∞
k 1(σ+τ)k 1(σ) dσ
(125)
For n = 2 the calculation is a bit more involved, though all the steps have been done
at least once before in this chapter.
E [G2[k 2,u (t +τ)]G2[k 2,u (t)]]
=
−∞∫
∞
k 2(σ1,σ2)k 2(σ3,σ4)E [u (t +τ−σ1)u (t +τ−σ2)u (t −σ3)u (t −σ4)] dσ1 . . . dσ4
−A
−∞∫
∞
k 2(σ1,σ2)E [u (t +τ−σ1)u (t +τ−σ2)] dσ1dσ2
−∞∫
∞
k 2(σ,σ) dσ
−A
−∞∫
∞
k 2(σ1,σ2)E [u (t −σ1)u (t −σ2)]dσ1dσ2
−∞∫
∞
k 2(σ,σ) dσ
+ A2 [
−∞∫
∞
k 2(σ,σ) dσ]2
(126)
Upon expansion of the expectations, many of the terms add out, leaving the easily veriﬁed
result:
240

E [G2[k 2,u (t +τ)]G2[k 2,u (t)]] = 2A2
−∞∫
∞
k 2(σ1+τ,σ2+τ)k 2(σ1,σ2) dσ1dσ2
(127)
The general result will be presented more formally.
Theorem 5.2
For the Wiener polynomial operator Gn[kn,u (t)], where u (t) is real,
stationary, zero-mean, Gaussian white noise with intensity A,
E [Gn[kn,u (t +τ)]Gn[kn,u (t)]] = n !An
−∞∫
∞
kn(σ1+τ, . . . ,σn+τ)kn(σ1, . . . ,σn) dσ1 . . . dσn
(128)
Proof
(Sketch) To simplify the notation, the degree-n Wiener operator will be written
in the general polynomial form shown below. (Recall that only homogeneous terms whose
degree has the same parity as n occur in Gn[kn,u (t)].)
Gn[kn,u (t)] =
p (k)=p (n)
k =0Σ
n
gk(σ1, . . . ,σk)u (t −σ1) . . . u (t −σk) dσ1 . . . dσk
Then using the orthogonality property,
E [Gn[kn,u (t +τ)]Gn[kn,u (t)]]
= E [
−∞∫
∞
gn(σ1, . . . ,σn)u (t +τ−σ1) . . . u (t +τ−σn) dσ1 . . . dσn Gn[kn,u (t)]]
=
p (k)=p (n)
k =0Σ
n
−∞∫
∞
gn(σ1, . . . ,σn)gk(τ1, . . . ,τk) E [u (t +τ−σ1)
. . . u (t +τ−σn)u (t −τ1) . . . u (t −τk)] dσ1 . . . dσndτ1 . . . dτk
(129)
In the k th summand the expected value will contain a sum of products of (n +k)/2
impulses, and the argument of each impulse will be a difference of a pair of arguments
chosen from (t +τ−σ1), . . . ,(t +τ−σn), (t −τ1), . . . ,(t −τk).
First consider the k = n summand, and speciﬁcally those terms of the expected value
wherein every impulse in the product has one of the σi variables and one of the τi
variables in its argument. There will be n ! such products in the expected value, n choices
of the τi variable to pair with the ﬁrst σi variable, n −1 choices of the τi variable to pair
with the second σi variable, and so on. (Since the product is over unordered pairs, the
ordering of the σi´s is immaterial.) Since gn(t 1, . . . ,tn) is symmetrical, when the impulses
are integrated out, each of the resulting terms will be identical. Thus this portion of the
contribution of the k = n term can be written as
241

n !An
−∞∫
∞
gn(σ1+τ, . . . ,σn+τ)gn(σ1, . . . ,σn) dσ1 . . . dσn
Of course, in the original notation this is precisely (128), and so the remainder of the
argument is devoted to showing that all other terms in (129) are zero. The remaining
products of impulses in the k = n summand will contain at least one impulse with an
argument that is a difference of two of the σi variables. This feature is shared by all the
k < n terms in (129) since there will be at least two more σi variables than τi variables.
This kind of term will be discussed now for the general k ≤ n summand.
First, for ﬁxed k in (129) consider each product-of-impulses term in the expected
value that contains δ0(σn −1−σn) as a factor. Using the key fact noted in the proof of
Theorem 5.1, this collection of terms can be written as
Aδ0(σn −1−σn) E [u (t +τ−σ1) . . . u (t +τ−σn −2)u (t −τ1) . . . u (t −τk)]
Collecting these terms in (129) for each value of k, and integrating with respect to σn,
gives that their contribution to (129) is
p (k)=p (n)
k =0Σ
n
A
−∞∫
∞
gn(σ1, . . . ,σn −2,σn −1,σn −1)gk(τ1, . . . ,τk) E [u (t +τ−σ1)
. . . u (t +τ−σn −2)u (t −τ1) . . . u (t −τk)] dσ1 . . . dσn −1dτ1 . . . dτk
= A E [
−∞∫
∞
gn(σ1, . . . ,σn −2,σn −1,σn −1)u (t +τ−σ1) . . . u (t +τ−σn −2) dσ1 . . . dσn −1
p (k)=p (n)
k =0Σ
n
gk(τ1, . . . ,τk)u (t −τ1) . . . u (t −τk) dτ1 . . . dτk]
= A E [
−∞∫
∞
gn(σ1, . . . ,σn −2,σn −1,σn −1)dσn −1u (t +τ−σ1)
. . . u (t +τ−σn −2) dσ1 . . . dσn −2 Gn[kn,u (t)]]
= 0
by the orthogonality property of Gn[kn,u (t)].
Now, for ﬁxed k in (129), consider each product-of-impulses term in the expected
value that contains δ0(σn −3−σn −2) as a factor, but that does not contain δ0(σn −1−σn) as a
factor. The collection of such terms can be written as the set of terms that contain a factor
of
the
form δ0(σn −3−σn −2), minus
the
set
of
terms
that
contain
a factor
of
δ0(σn −3−σn −2)δ0(σn −1−σn). That is,
Aδ0(σn −3−σn −2) E [u (t +τ−σ1) . . . u (t +τ−σn −4)u (t +τ−σn −1)u (t +τ−σn)
‘
u (t −τ1) . . . u (t −τk)] −A2δ0(σn −3−σn −2)δ0(σn −1−σn) E [u (t +τ−σ1)
. . . u (t +τ−σn −4)u (t −τ1) . . . u (t −τk)]
Collecting these terms in (129) for each value of k, and integrating out the common
242

impulse factors, gives zero as follows. All terms corresponding to
Aδ0(σn −3−σn −2)E [u (t +τ−σ1) . . . u (t +τ−σn −4)u (t +τ−σn −1)u (t +τ−σn)]
give zero by the argument in the previous case. In a very similar fashion, the remaining set
of terms gives
p (k)=p (n)
k =0Σ
n
−A2
−∞∫
∞
gn(σ1, . . . ,σn −4,σn −3,σn −3,σn −1,σn −1)gk(τ1, . . . ,τk)
E [u (t +τ−σ1) . . . u (t +τ−σn −4)u (t −τ1) . . . u (t −τk)] dσ1 . . . dσn −4dτ1 . . . dτk
= −A2 E [
−∞∫
∞
gn(σ1, . . . ,σn −4,σn −3,σn −3,σn −1,σn −1)u (t +τ−σ1)
. . . u (t +τ−σn −4) dσ1 . . . dσn −4 Gn[kn,u (t)]]
= 0
The remainder of the proof continues in just this way, with the next step being to consider
those terms in (129) that contain δ0(σn −5−σn −4) as a factor, but not δ0(σn −3−σn −2) or
δ0(σn −1−σn). It is left to the reader to complete the calculation of all the zeros.
Methods for determining the Wiener kernels kn(t 1, . . . ,tn) for an unknown system
will be a major topic in Chapter 7. However, one way to ﬁnd the Wiener kernels for a
known system is to establish the relationship between the Wiener kernels and the Volterra
kernels. Suppose a system is described by the Wiener orthogonal representation, and also
by the symmetric-kernel Volterra series representation
y (t) =
n =0Σ
∞
−∞∫
∞
hnsym(σ1, . . . ,σn)u (t −σ1) . . . u (t−σn) dσ1 . . . dσn
(130)
Of course, it should be noted that strong convergence properties of both representations
are required before the two can be related. Thus, the following developments are ﬁne for
the polynomial system case, but must be qualiﬁed by convergence hypotheses to be taken
as rigorous in the inﬁnite series case.
Theorem 5.3
Suppose a system is described by the Wiener orthogonal representation
(110), (122), and by the symmetric Volterra system representation (130). Then the degree-
N symmetric Volterra kernel is given by
hNsym(t 1, . . . ,tN) =
j =0Σ
∞
N !j !2j
(−1)j(N +2j)!A j
______________
−∞∫
∞
kN+2j(t 1, . . . ,tN,τ1, . . . ,τ1,τj,τj) dτ1 . . . dτj
(131)
Proof
From (110) and (122), the Wiener representation for the system is
243

y (t) =
n =0Σ
∞
m=0
Σ
[n /2]
(n −2m)!m !2m
(−1)mn !Am
_____________
−∞∫
∞
kn(σ1, . . . ,σn −2m,τ1,τ1, . . . ,τm,τm)
dτ1 . . . dτm u (t −σ1) . . . u (t −σn −2m) dσ1 . . . dσn −2m
To ﬁnd an expression for hNsym(t 1, . . . ,tN), all terms of degree N must be extracted. These
terms are precisely those with n −2m = N. Supposing ﬁrst that N is even, if n −2m = N
then it is clear that n must be even, and n ≥ N. Thus the degree-N terms in the Wiener
representation are given by
n even
n =NΣ
∞
N !((n −N)/2)!2(n −N)/2
(−1)(n −N)/2n !A(n −N)/2
__________________
−∞∫
∞
kn(σ1, . . . ,σN,τ1,τ1, . . . ,τ
2
n −N
_____,τ
2
n −N
_____)
dτ1 . . . dτ
2
n −N
_____ u (t −σ1) . . . u (t −σN) dσ1 . . . dσN
To put this in a neater form, change the summation index n to j = (n −N)/2. This gives
j =0Σ
∞
N !j !2j
(−1)j(N +2j)!A j
______________
−∞∫
∞
kN+2j(σ1, . . . ,σN,τ1,τ1, . . . ,τj,τj)
dτ1 . . . dτju (t −σ1) . . . u (t −σN) dσ1 . . . dσN
A very similar development for the case of N odd leads to exactly the same expression for
the degree-N terms in the Wiener representation. Thus, it is clear that (131) gives the
symmetric Volterra kernel for the system.
To express the Wiener kernels in terms of the symmetric Volterra kernels is a
messier task. The approach used in the following proof is to write out (131) for the
symmetric
Volterra
kernels
hNsym(t 1, . . . ,tN),
h (N+2)sym(t 1, . . . ,tN,σ1,σ1),
h (N+4)sym(t 1, . . . ,tN,σ1,σ1,σ2,σ2), and so on. Then by tedious inspection it becomes clear
that the Wiener kernel kN(t 1, . . . ,tN) can be isolated using these expressions.
Theorem 5.4
Suppose a system is described by the symmetric Volterra system
representation (130), and by the Wiener orthogonal representation (110), (122). Then the
degree-N Wiener kernel is given by
kN(t 1, . . . ,tN) =
j =0Σ
∞
N !j !2j
(N +2j)!A j
__________
−∞∫
∞
h (N+2j)sym(t 1, . . . ,tN,σ1,σ1, . . . ,σj,σj) dσ1 . . . dσj
(132)
Proof
For convenience, let
a (N,j) =
N !j !2j
(N +2j)!A j
__________
Then (131) can be written as
244

hNsym(t 1, . . . ,tN) = kN(t 1, . . . ,tN)
+
j =1Σ
∞
(−1)ja (N,j)
−∞∫
∞
kN+2j(t 1, . . . ,tN,τ1,τ1, . . . ,τj,τj) dτ1 . . . dτj
and for k ≥ 1,
h (N+2k)sym(t 1, . . . ,tN,σ1,σ1, . . . ,σk,σk) = kN+2k(t 1, . . . ,tN,σ1,σ1, . . . ,σk,σk)
+
j =1Σ
∞
(−1)ja (N +2k,j)
−∞∫
∞
kN+2k +2j(t 1, . . . ,tN,σ1,σ1, . . . ,σk,σk,τ1,τ1, . . . ,τj,τj) dτ1 . . . dτj
Using some elementary manipulations and variable relabelings gives
−∞∫
∞
h (N+2k)sym(t 1, . . . ,tN,τ1,τ1, . . . ,τk,τk) dτ1 . . . dτk
=
i =kΣ
∞
(−1)i −ka (N +2k,i −k)
−∞∫
∞
kN+2i(t 1, . . . ,tN,τ1,τ1, . . . ,τi,τi) dτ1 . . . dτi
Now, the right side of (132) can be written as
hNsym(t 1, . . . ,tN) +
k =1Σ
∞
a (N,k)
−∞∫
∞
h (N+2k)sym(t 1, . . . ,tN,τ1,τ1, . . . ,τk,τk) dτ1 . . . dτk
= kN(t 1, . . . ,tN) +
j =1Σ
∞
(−1)ja (N,j)
−∞∫
∞
kN+2j(t 1, . . . ,tN,τ1,τ1, . . . ,τj,τj) dτ1 . . . dτj
+
k =1Σ
∞
a (N,k)
i =kΣ
∞
(−1)i −ka (N +2k,i −k)
−∞∫
∞
kN+2i(t 1, . . . ,tN,τ1,τ1, . . . ,τi,τi) dτ1 . . . dτi
For the general term of the form
−∞∫
∞
kN+2q(t 1, . . . ,tN,τ1,τ1, . . . ,τq,τq) dτ1 . . . dτq
with q ≥ 1, the coefﬁcient is
(−1)qa (N,q) +
k =1Σ
q
(−1)q −ka (N,k)a (N +2k,q −k)
Substituting the deﬁnition of a (N,j) and using simple identities shows that this coefﬁcient
is 0. Thus the proof is complete.
Example 5.14
Consider the degree-3 polynomial system shown in Figure 5.2.
245

Figure 5.2. A degree-3 polynomial system.
The Wiener kernels are easily computed from the symmetric Volterra kernels
h 1(t) = e −tδ−1(t)
h 2sym(t 1,t 2) = 0
h 3sym(t 1,t 2,t 3) = e −t 1e −t 2e −t 3δ−1(t 1)δ−1(t 2)δ−1(t 3)
Using (132), there are only two nonzero Wiener kernels:
k 1(t) = (1 + 2
3A
___)e −tδ−1(t)
k 3(t 1,t 2,t 3) = e −t 1e −t 2e −t 3δ−1(t 1)δ−1(t 2)δ−1(t 3)
5.6 Remarks and References
Remark 5.1
An introductory discussion of the impulse response of a homogeneous
system can be found in
M. Brilliant, "Theory of the Analysis of Nonlinear Systems," MIT RLE Technical Report
No. 345, 1958 (AD 216-209).
One area in which impulse inputs are of general interest is in impulse sampler models for
sampled data systems. This subject is discussed in
A. Bush, "Some Techniques for the Synthesis of Nonlinear Systems," MIT RLE Technical
Report No. 441, 1966 (AD 634-122).
Sampled-data and discrete-time systems will be treated in Chapter 6.
Remark 5.2
The steady-state response of a homogeneous system to a sinusoidal input is
brieﬂy discussed in
D. George, "Continuous Nonlinear Systems," MIT RLE Technical Report No. 355, 1959
(AD 246-281).
and in
246
u
y
e–t δ–1(t) 
Σ
(.)3

J. Barrett, "The Use of Functionals in the Analysis of Nonlinear Physical Systems,"
Journal of Electronics and Control, Vol. 15, pp.567-615, 1963.
The steady-state response of a nonlinear system to single- and multi-tone sinusoidal
inputs, and the use of this response in electronic circuit analysis is the subject of
J. Bussgang, L. Ehrman, J. Graham, "Analysis of Nonlinear Systems with Multiple Inputs,"
Proceedings of the IEEE, Vol. 62, pp.1088-1119, 1974.
Many interesting examples and experimental results are included in this paper, and in
other referenced reports by the authors. General formulas similar to those of Section 5.3
for the response to multi-tone inputs are derived in
E. Bedrosian, S. Rice, "The Output Properties of Volterra Systems (Nonlinear Systems
with Memory) Driven by Harmonic and Gaussian Inputs," Proceedings of the IEEE, Vol.
59, pp.1688-1707, 1971.
For a recent book-length treatment at a more elementary level, see
D. Weiner, J. Spina, Sinusoidal Analysis and Modeling of Weakly Nonlinear Circuits, Van
Nostrand Reinhold, New York, 1980.
Remark 5.3
The well known Manley-Rowe formulas are closely related to the topic of
steady-state frequency response of a Volterra system. As originally developed in 1956,
these formulas describe the constraints on power ﬂow at various frequencies in a nonlinear
capacitor. Since that time the formulas have been generalized to apply to a large class of
nonlinear systems. See the following two books, the ﬁrst of which is the more elementary.
R. Clay, Nonlinear Networks and Systems, John Wiley, New York, 1971.
P. Penﬁeld, Frequency-Power Formulas, MIT Press, Cambridge, Massachusetts, 1960.
Remark 5.4
The response of a nonlinear system to random inputs was the topic in which
N. Wiener ﬁrst used the Volterra representation for nonlinear systems. Wiener’s approach
will be discussed further in Chapter 7 in the context of system identiﬁcation. Material
similar to that discussed in Section 5.4 is introduced in the report by George and the paper
by Barrett cited above. The approach I have used follows
M. Rudko, D. Weiner, "Volterra Systems with Random Inputs: A Formalized Approach,"
IEEE Transactions on Communications, Vol. COM-26, pp.217-227, 1978 (Addendum: Vol.
COM-27, pp. 636-638, 1979).
This paper presents a derivation of the general formula for the cross-spectral density given
247

in (91), and a general reduction of the partial output power spectral density formula in
(108).
A different approach can be found in the paper by Bedrosian and Rice mentioned in
Remark 5.2. More general inputs such as a sum of sinusoidal and Gaussian signals also
are considered by Bedrosian and Rice. Many related papers, and reprints of some papers
cited above can be found in the collection
A. Haddad, ed., Nonlinear Systems: Processing of Random Signals - Classical Analysis,
Benchmark Papers, Vol. 10, Dowden, Hutchinson, and Ross, Stroudsberg, Pennsylvania,
1975.
A derivation of the formula for the order-n autocorrelation of a Gaussian random process
can be found in
M. Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, John Wiley, New
York, 1980.
J. Laning, R. Battin, Random Processes in Automatic Control, McGraw-Hill, New York,
1956.
Remark 5.5
A very important kind of Volterra system is a feedback system involving
linear dynamic subsystems, and a static nonlinear element. When a Gaussian input
process is applied to such a system, the description of the output statistics in terms of the
given subsystems is a difﬁcult problem. One approach, the so-called quasi-functional
method, is discussed in
H. Smith, Approximate Analysis of Randomly Excited Nonlinear Controls, MIT Press,
Cambridge, Massachusetts, 1966.
Remark 5.6
The idea of using orthogonal representations for the response of a nonlinear
system to a random input has been investigated from a number of different, though closely
related, viewpoints. The work of Wiener on nonlinear systems dates back to a report that is
difﬁcult to obtain.
N. Wiener, "Response of a Nonlinear Device to Noise," MIT Radiation Laboratory Report
No. 165, 1942.
The most readily available account by Wiener is in a book of transcribed lectures:
N.
Wiener,
Nonlinear
Problems
in
Random
Theory,
MIT
Press,
Cambridge,
Massachusetts, 1958.
248

The basic mathematical paper
R. Cameron, W. Martin, "The Orthogonal Development of Nonlinear Functionals in Series
of Fourier-Hermite Functionals," Annals of Mathematics, Vol. 48, pp. 385-392, 1947.
develops the representation of a nonlinear functional acting on a white Gaussian process
by using the orthogonality properties of Hermite polynomials. A related representation,
with emphasis on nonlinear system theory, is discussed in the paper by Barrett cited in
Remark 5.2, and in
J. Barrett, "Hermite Functional Expansions and the Calculation of Output Autocorrelation
and Spectrum for any Time-Invariant Nonlinear System with Noise Input," Journal of
Electronics and Control, Vol. 16, pp. 107-113, 1964.
Informative reviews of many aspects of these early contributions can be found in
L. Zadeh, "On the Representation of Nonlinear Operators," IRE Wescon Convention
Record, Part 2, pp. 105-113, 1957.
W. Root, "On System Measurement and Identiﬁcation," in System Theory, Polytechnic
Institute of Brooklyn Symposium Proceedings, Vol. 15, Polytechnic Press, New York, pp.
133-157, 1965.
R. Deutsch, Nonlinear Transformations of Random Processes, Prentice-Hall, Englewood
Cliffs, New Jersey, 1962.
A more recent paper on the Wiener representation is
G. Palm, T. Poggio, "The Volterra Representation and the Wiener Expansion: Validity and
Pitfalls," SIAM Journal on Applied Mathematics, Vol. 33, pp. 195-216, 1977.
This paper contains analyses of a number of delicate technical issues, particularly
convergence properties. Finally, the Wiener representation is discussed in detail at an
introductory level in
M. Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, John Wiley, New
York, 1980.
Remark 5.7
Orthogonal representations have been developed for nonlinear functionals
of Poisson and other, more general, stochastic processes. See
H. Ogura, "Orthogonal Functionals of the Poisson Process," IEEE Transactions on
Information Theory, Vol. IT-18, pp. 473-480, 1972.
249

A. Segall, T. Kailath, "Orthogonal Functionals of Independent-Increment Processes," IEEE
Transactions on Information Theory, Vol. IT-22, pp. 287-298, 1976.
A general framework for orthogonal expansions is discussed in
S. Yasui, "Stochastic Functional Fourier Series, Volterra Series, and Nonlinear Systems
Analysis," IEEE Transactions on Automatic Control, Vol. AC-24, pp. 230-241, 1979.
The reader should be forewarned that further pursuit of the topics of Section 5.5 leads
quickly into the theory of stochastic integrals and other stratospheric mathematical tools.
The reference list of any of the papers cited above can serve as a launch pad.
5.7 Problems
5.1. For a degree-n homogeneous system described in terms of the regular kernel
y (t) =
0∫
∞
hreg(σ1, . . . ,σn)u (t −σ1−. . . −σn) . . . u (t−σn) dσ1 . . . dσn
ﬁnd expressions for the response to the inputs u (t) = δ0(t) and u (t) = δ0(t) + δ0(t −T),
T > 0.
5.2. For the system shown below, ﬁnd an expression for the steady-state response to
u (t) = 2Acos (ωt) in terms of the subsystem transfer functions.
5.3. Show that the steady-state response of a degree-n homogeneous system to the input
u (t) = B + 2Acos(ωt) can be written in the form
yss(t) =
k =0Σ
n
B
D k
nE
G Bn −kAk
j =0Σ
k
B
D j
kE
G Hnsym(
n −k
0, . . . ,0;
j
iω, . . . ,iω;
k −j
−iω, . . . ,−iω) e i (2j −k)ωt
Notice that all of the ﬁrst n harmonics of the input frequency are included, not just those of
the same parity as n.
5.4. Derive a formula for the steady-state response of a degree-n homogeneous system to a
unit step function in terms of Hsym(s 1, . . . ,sn) and in terms of Hreg(s 1, . . . ,sn).
5.5. Find a necessary and sufﬁcient condition that the frequencies in the two-tone steady-
state response formula for a Volterra system be distinct for distinct values of M and N.
250
u
y
H0(s)
(.)n
H1(s)

5.6. Suppose that the input to a degree-2 homogeneous system is a real, stationary, zero-
mean, white Gaussian random process with unit intensity. Show that
Ryy(τ) =
−∞∫
∞
−∞∫
∞
hsym(σ1,σ1)hsym(σ2,σ2) dσ1dσ2
+ 2
−∞∫
∞
−∞∫
∞
hsym(τ+σ1,τ+σ2)hsym(σ1,σ2) dσ1dσ2
5.7. Suppose that u (t) is a stationary random input to a stationary, degree-n, homogeneous
system. Show that if the input is applied at t = −∞, then the output random process is
stationary.
5.8. For a stationary, zero-mean, Gaussian random process u (t) with autocorrelation
Ruu(τ), give an expression for Ruu
(6)(t 1, . . . ,t 6).
5.9. Express the result of Example 5.8 in terms of the symmetric system function.
5.10. Consider the modulation system diagramed below for the case where the message
signal is u (t) = Amcos(ωmt), t ≥ 0.
Show that
y (t) = Accos(ωct) + 2ωm
AcAm
_____cos[(ωc+ωm)t ] −2ωm
AcAm
_____cos[(ωc−ωm)t ] −Ac
Neglecting the constant term, when Am/Ac << 1 this is called a narrow-band FM signal with
sinusoidal modulation. Show that the FM modulation system in Example 3.1 also can be
used to generate such a signal.
5.11. Show that (83) can be written in the form
Syu(ω) =
(2π)n
1
_____
−∞∫
∞
Sˆyu(ω1−α2,α2−α3, . . . ,αn−αn +1,αn +1) dα2 . . . dαn +1
and interpret this as an association-of-variables expression.
5.12. Suppose the input to a degree-n homogeneous system is real, stationary, zero-mean,
251
u
y
Σ
Π
—s
–1
___
s
–ωc
Acsin(ωct)

Gaussian noise with autocorrelation Ruu(τ). Find an expression for the cross-correlation
Ryu(τ) in terms of the symmetric kernel.
5.13. Suppose a two-tone input is applied to a degree-n homogeneous system. Show that
in the steady-state response the number of frequency terms e i [Mω1+Nω2]t, ignoring complex
conjugates, is given by the number of integers M and N satisfying
parity |M + N | = parity n, |M | + |N | ≤n
5.14. Use (132) to derive the following relationship between the Wiener system function
KN(ω1, . . . ,ωN) = F [kN(t 1, . . . ,tN)] and the symmetric (Volterra) system functions
Hnsym(ω1, . . . ,ωn).
KN(ω1, . . . ,ωN) =
j =0Σ
∞
N !j !2j(2π)j
(N +2j)!A j
___________
−∞∫
∞
H(N+2j)sym(ω1, . . . ,ωN,γ1,−γ1, . . . ,γj,−γj) dγ1 . . . dγj
(Hint: Follow the familiar stategy of inverse transform, manipulation, transform.)
5.15. For the bilinear state equation
x.(t) = Ax (t) + Dx (t)u (t) + bu (t)
y (t) = cx (t)
show that if D2 = 0 and Db = 0, then the Wiener kernels for the system are identical to the
symmetric Volterra kernels.
5.16 Prove that E [Gn[kn,u (t +τ)]Gn[kn,u (t)]] in (128) can be written in terms of the
Wiener system function (Problem 5.14) as
E [Gn[kn,u (t +τ)]Gn[kn,u (t)]] =
(2π)n
n !An
_____
−∞∫
∞
|Kn(ω1, . . . ,ωn)| 2e i (ω1+ . . . +ωn)τ dω1 . . . dωn
5.17 Suppose a Volterra system is described by the regular kernels hnreg(t 1, . . . ,tn),
n = 1,2, . . . . For a Gaussian white noise input with intensity P, show that the output
expectation is
E [y (t)] =
n =1Σ
∞
Pn
0∫
∞
h 2nreg(0,σ1,0,σ2, . . . ,0,σn) dσ1 . . . dσn
5.18 Suppose the bilinear state equation (A,D,b,c,Rm) is driven by Gaussian white noise
with intensity P. Use Problem 5.17 to show that
E [y (t)] = −Pc (A + PD2)−1Db
Discuss the conditions under which this result is meaningful.
252

CHAPTER 6
DISCRETE-TIME SYSTEMS
Most of the nonlinear system theory that has been discussed so far for continuous-
time systems can be developed for discrete-time systems. There are differences, of course,
but these mostly are differences in technical detail or interpretation of the results. The
situation is similar to the linear case where the continuous- and discrete-time theories look
much the same.
In this chapter I will discuss brieﬂy the salient features of Volterra series methods
for discrete-time nonlinear systems.
For simplicity only stationary systems will be
considered.
Special attention will be devoted to points where the discrete- and
continuous-time theories differ, and much of the rather routine transcription of results will
be left to the reader. In addition, two new classes of systems will be discussed: bilinear
input/output systems, and two-dimensional linear systems. While the general classes of
multilinear input/output systems and multidimensional linear systems are of interest in
their own right, the simplest cases of each are introduced here to demonstrate the
similarity in representations and analysis methods to the now familiar class of
homogeneous systems.
6.1 Input/Output Representations in the Time Domain
Consider a discrete-time system representation of the form
y (k) =
i 1=0Σ
∞. . .
in=0Σ
∞
h (i 1, . . . ,in)u (k −i 1) . . . u (k −in) , k = 0,1,2, . . .
(1)
The input signal u (k) and output signal y (k) are real sequences that are assumed to be zero
for k < 0. The kernel h (i 1, . . . ,in) is real, and equal to zero if any argument is negative. It
is a simple matter to verify that a system described by (1) is stationary, causal, and
degree-n homogeneous. The upper limits on the summations can be lowered to k, but
inﬁnite upper limits are retained for notational simplicity.
Since for any k, y (k) in (1) is given by a ﬁnite summation, there is no need even to
mention technical hypotheses. In other words, issues like continuity and integrability in
the continuous-time case do not arise in regard to the representation in (1). Also, notice
253

that direct transmission terms are explicitly displayed in (1), and there is no need to
consider impulsive kernels. For example, if
h (i 1, . . . ,in) =
B
A
C
A
D
0 , otherwise
1 , i 1= . . . =in=0
then the system can be written in the form
y (k) = u n(k) , k = 0,1,2, . . .
The familiar sum-over-permutations argument shows that the kernel in (1) can be
replaced by the symmetric kernel
hsym(i 1, . . . ,in) = n !
1
___
π(.)Σ h (i π(1), . . . ,i π(n))
(2)
without loss of generality. (Recall that the summation is over all n ! permutations of
1,2, . . . ,n.) From the symmetric kernel representation, a triangular kernel can be deﬁned.
However, some care is required because it cannot be argued that values of the kernel at
particular arguments do not contribute to the sum, as was done for the integral of
nonimpulsive kernels in the continuous-time case. That is, the values of the triangular
kernel at boundary points of the triangular domain must be adjusted appropriately. One
way to do this adjustment is to use the notation
htri(i 1, . . . ,in) = hsym(i 1, . . . ,in) δˆ−1(i 1−i 2,i 2−i 3, . . . ,in −1−in)
(3)
where the special multivariable step function is deﬁned by
δˆ−1(i 1, . . . ,in −1) =
B
A
A
A
C
A
A
A
D n ! , i 1, . . . ,in −1 > 0
...
m1! . . . mj!
n !
__________ ,
B
C
D ij +1= . . . =ij +mj−1=0
i 1= . . . =im1−1=0, . . . ,
...
1 , i 1= . . . =in −1 = 0
0 , if any ij < 0
(4)
It is easy to verify that when n = 2 this setup yields consistent results in going from the
symmetric kernel to the triangular kernel using (3), and then from the triangular kernel
back to the symmetric kernel using (2). The higher-degree cases are less easy, but still
straightforward. The uncircumﬂexed notation will be retained for the more traditional step
function:
δ−1(k) =
B
C
D 0 , k < 0
1 , k = 0,1,2, . . .
254

The third special form is the regular kernel representation. Starting with the
triangular kernel representation,
y (k) =
i 1=0Σ
∞. . .
in=0Σ
∞
htri(i 1, . . . ,in)u (k −i 1) . . . u (k −in)
(5)
a simple change of variables argument gives
y (k) =
i 1=0Σ
∞. . .
in=0Σ
∞
hreg(i 1, . . . ,in)u (k −i 1−. . . −in)u (k −i 2−. . . −in) . . . u (k −in)
(6)
where
hreg(i 1, . . . ,in) = htri(i 1+ . . . +in,i 2+ . . . +in, . . . ,in)
= hsym(i 1+ . . . +in,i 2+ . . . +in, . . . ,in)δˆ−1(i 1, . . . ,in −1)
(7)
Notice again that the upper limits of the summations in (5) and (6) can be replaced by
ﬁnite quantities.
But that makes the notation more complicated, so just as in the
continuous-time case the inﬁnities are used.
Although only stationary systems will be considered, general representations of the
form
y (k) =
i 1=0Σ
k . . .
in=0Σ
k
h (k,i 1, . . . ,in)u (i 1) . . . u (in)
(8)
will arise. It is natural to follow the continuous-time case and call a kernel h (k,i 1, . . . ,in)
stationary if
h (0,i 1−k, . . . ,in−k) = h (k,i 1, . . . ,in)
(9)
If this relationship holds, then setting
g (i 1, . . . ,in) = h (0,−i 1, . . . ,−in)
10
yields the representation
y (k) =
i 1=0Σ
k . . .
in=0Σ
k
g (k −i 1, . . . ,k −in)u (i 1) . . . u (in)
(11)
which is equivalent to (8) since
g (k −i 1, . . . ,k −in) = h (0,i 1−k, . . . ,in−k) = h (k,i 1, . . . ,in)
(12)
A simple change of variables permits rewriting (11) in the form
y (k) =
j 1=0
Σ
k
. . .
jn=0Σ
k
g (j 1, . . . , jn)u (k −j 1) . . . u (k −jn)
(13)
that is, in the form of (1).
255

With these basic representations in hand, the description of polynomial and Volterra
systems is simply a matter of ﬁnite and inﬁnite sums of homogeneous terms. Of course, the
convergence issue becomes important for Volterra systems, but the basic approaches to
convergence in the continuous-time case carry over directly. The topic of interconnections
of discrete-time homogeneous, polynomial, or Volterra systems will not be discussed since
the developments are easily transcribed from Section 1.4.
6.2 Input/Output Representations in the Transform Domain
For an n-variable function f (i 1, . . . ,in) that is zero if any of the integers i 1, . . . ,in is
negative, that is, a one-sided function, the n-variable z-transform is deﬁned by
F (z 1, . . . ,zn) = Z[f (i 1, . . . ,in)]
=
i 1=0Σ
∞. . .
in=0Σ
∞
f (i 1, . . . ,in) z1
−i 1 . . . zn
−in
(14)
This can be viewed as a nonpositive power series in the complex variables z 1, . . . ,zn, in
which case convergence conditions must be included. However, for the functions that will
be considered here (just as for the functions typically considered in discrete-time linear
system theory) convergence regions always exist. Therefore, I will be very casual in this
regard.
Actually, (14) can be viewed as an algebraic object (formal series) in n
indeterminates, in which case the question of convergence does not arise. While this,
perhaps more sophisticated, viewpoint can be used to establish most of the results to be
discussed, I will retain the more classical interpretation.
Example 6.1
Reminiscent of Example 2.1, consider the function
f (i 1,i 2) = i 1 −i 1λ−i 2,
i 1,i 2 ≥0
where λ is a constant. The z-transform of this function can be computed from the basic
deﬁnition by writing
F (z 1,z 2) =
i 1=0Σ
∞
i 2=0Σ
∞
(i 1−i 1λ−i 2) z1
−i 1z2
−i 2
=
i 1=0Σ
∞
i 2=0Σ
∞
i 1z1
−i 1z2
−i 2 −
i 1=0Σ
∞
i 2=0Σ
∞
i 1λ−i 2z1
−i 1z2
−i 2
= (
i 1=0Σ
∞
i 1z1
−i 1 )(
i 2=0Σ
∞
z2
−i 2 ) −(
i 1=0Σ
∞
i 1z1
−i 1 )(
i 2=0Σ
∞
λ−i 2z2
−i 2 )
Summing each inﬁnite series (or recalling single-variable z-transforms), it is clear that
256

F (z 1,z 2) =
(z 1−1)2
z 1
_______
z 2−1
z 2
______ −
(z 1−1)2
z 1
_______
z 2−λ
z 2
______
=
(z 1−1)2(z 2−1)(z 2−λ)
(1−λ)z 1z 2
__________________
A careful look at the deﬁnition (14) and the calculations in Example 6.1 indicates
immediately a couple of properties of the z-transform. These and other properties listed
below are very similar in nature to properties of the Laplace transform, and the general
proofs are easy. All functions are assumed to be one-sided, and the capital-letter notation
is retained for the z-transform.
Theorem 6.1
The z-transform is linear:
Z[f (i 1, . . . ,in) + g (i 1, . . . ,in)] = F (z 1, . . . ,zn) + G (z 1, . . . ,zn)
Z[αf (i 1, . . . ,in)] = αF (z 1, . . . ,zn), for scalar α
(15)
Theorem 6.2
If f (i 1, . . . ,in) can be written as a product of two factors
f (i 1, . . . ,in) = h (i 1, . . . ,ik)g (ik +1, . . . ,in)
(16)
then
F (z 1, . . . ,zn) = H (z 1, . . . ,zk)G (zk +1, . . . ,zn)
(17)
Theorem 6.3
If f (i 1, . . . ,in) is given by the single-variable convolution
f (i 1, . . . ,in) =
j =0Σ
∞
h (j)g (i 1−j, . . . ,in−j)
(18)
then
F (z 1, . . . ,zn) = H (z 1 . . . zn)G (z 1, . . . ,zn)
(19)
Theorem 6.4
If f (i 1, . . . ,in) is given by the n-fold convolution
f (i 1, . . . ,in) =
j 1=0
Σ
∞. . .
jn=0Σ
∞
h (j 1, . . . , jn)g (i 1−j 1, . . . ,in−jn)
(20)
then
F (z 1, . . . ,zn) = H (z 1, . . . ,zn)G (z 1, . . . ,zn)
(21)
Theorem 6.5
If I1, . . . ,In are nonnegative integers, then
257

Z[f (i 1−I1, . . . ,in−In)] = z1
−I1 . . . zn
−InF (z 1, . . . ,zn)
(22)
The basic formula for the inverse z-transform is a multivariable contour integration:
f (i 1, . . . ,in) =
(2πi)n
1
______
Γn∫. . .
Γ1∫F (z 1, . . . ,zn) z1
i 1−1 . . . zn
in−1 dz 1 . . . dzn
(23)
where each Γj is an appropriate contour in the zj complex plane. For reasons that should be
obvious, this formula is difﬁcult to use. An alternative approach is to obtain the values of
f (i 1, . . . ,in) as the coefﬁcients in the series expansion of F (z 1, . . . ,zn) into nonpositive
powers of z 1, . . . ,zn. If F (z 1, . . . ,zn) is a rational function, this series expansion can be
simply a matter of division of the numerator polynomial by the denominator polynomial.
But I should point out that some care must be exercised, because not every rational
function corresponds to a z-transform. The distinction is that a z-transform must
correspond to a nonpositive power series. Requiring a rational function to be proper or
strictly proper is not a remedy.
Example 6.2
The rational function
F (z 1,z 2) = z 1+z 2
1
______
is not a z-transform since division gives
z 1+z 2
1
______ = z1
−1 −z1
−2z 2 + z1
−3z2
2 −. . .
or
z 1+z 2
1
______ = z2
−1 −z 1z2
−2 −z1
2z2
−3 −. . .
neither of which can be written as a nonpositive power series. On the other hand,
F (z 1,z 2) = z 1z 2−1
z 1z 2
_______
is a z-transform since it corresponds to the nonpositive power series
F (z 1,z 2) = 1 + z1
−1z2
−1 + z1
−2z2
−2 + . . .
The corresponding function can be written as
f (i 1,i 2) =
B
A
C
A
D
0 , otherwise
1 , i 1 = i 2 = 0,1,2, . . .
The z-transform representation is used for degree-n homogeneous systems in just the
same way that the Laplace transform is used in the continuous-time case. A transfer
258

function for a degree-n homogeneous discrete-time system is deﬁned as the z-transform of
a kernel for the system. For example, the symmetric transfer function is
Hsym(z 1, . . . ,zn) = Z[hsym(i 1, . . . ,in)]
(24)
Unfortunately, though probably not unexpectedly, to represent the input/output relation (1)
directly in terms of U (z), Y(z), and Hsym(z 1, . . . ,zn) seems to be impossible. The usual
device is to write (1) as the pair of equations:
yn(k 1, . . . ,kn) =
i 1=0Σ
∞. . .
in=0Σ
∞
hsym(i 1, . . . ,in)u (k 1−i 1) . . . u (kn−in)
y (k) = yn(k 1, . . . ,kn)k 1= . . . =kn=k
(25)
Then Theorem 6.4 permits rewriting the ﬁrst equation in the form
Yn(z 1, . . . ,zn) = Hsym(z 1, . . . ,zn)U (z 1) . . . U (zn)
(26)
while the second equation is an association of variables that involves contour integrations
of the form
Y(z) =
(2πi)n −1
1
________
Γ1∫. . .
Γn −1∫
z 1 . . . zn −1
Yn(z 1,z 2/z 1, . . . ,z /zn −1)
_____________________ dz 1 . . . dzn −1
(27)
The representation of (1) in terms of the triangular transfer function takes the same form.
In the case of the regular transfer function, the formulas in (25) and (26) do not
directly apply. However, by suitably restricting the class of input signals, a much more
explicit formula can be derived. This result is similar to Theorem 2.10, although I will
present it in detail with a proof that is much different from that in the continuous-time case,
and that requires no hypotheses on the form of the regular transfer function.
The ﬁrst step is to establish a basic expression for the z-transform of the input/output
expression (6). Write the regular transfer function
Hreg(z 1, . . . ,zn) = Z[hreg(i 1, . . . ,in)]
(28)
in the form
Hreg(z 1, . . . ,zn) =
i 1=0Σ
∞. . .
in −1=0
Σ
∞
Hi 1 . . . in −1(zn)z1
−i 1 . . . zn −1
−in −1
(29)
where each Hi 1 . . . in −1(zn) is deﬁned according to
Hi 1 . . . in −1(zn) =
in=0Σ
∞
hreg(i 1, . . . ,in)zn
−in ,
i 1, . . . ,in −1 = 0,1,2, . . .
(30)
Lemma 6.1
The z-transform of the output of a degree-n homogeneous, discrete-time
system can be written in the form
259

Y(z) =
i 1=0Σ
∞. . .
in −1=0
Σ
∞
Hi 1 . . . in −1(z)
k =i 1+ . . . +in −1
Σ
∞
u (k −i 1−. . . −in −1)u (k −i 2−. . . −in −1) . . . u (k −in −1)u (k) z −k (31)
Proof
Taking the z-transform of y (k) as given in (6) yields
Y(z) =
k =0Σ
∞
i 1=0Σ
∞. . .
in=0Σ
∞
hreg(i 1, . . . ,in)
u (k −i 1−. . . −in)u (k −i 2−. . . −in) . . . u (k −in) z −k
Replacing the summation index k by j = k −in gives
Y(z) =
i 1=0Σ
∞. . .
in −1=0
Σ
∞
[
in=0Σ
∞
hreg(i 1, . . . ,in)z −in]
j =−in
Σ
∞
u (j −i 1−. . . −in −1)u (j −i 2−. . . −in −1) . . . u (j −in −1)u (j) z −j
Now the result is clear from (30) and the assumption that u (k) = 0 for k < 0.
This lemma provides an alternative to the association-of-variables method for
performing input/output calculations. Furthermore, a more direct expression for Y(z) can
be obtained for a ubiquitous class of inputs.
Theorem 6.6
Suppose a degree-n homogeneous, discrete-time system is described by
the regular transfer function Hreg(z 1, . . . ,zn), and the input is of the form
U (z) =
j =1Σ
m
z −λj
ajz
_____ , λj ≠0 ,
j = 1, . . . ,m
(32)
Then
Y(z) =
j 1=1
Σ
m
. . .
jn −1=1
Σ
m
aj 1 . . . ajm−1
Hreg( λj 1 . . . λjn −1
z
__________, λj 2 . . . λjn −1
z
__________ , . . . , λjn −1
z
_____, z) U ( λj 1 . . . λjn −1
z
__________) (33)
Proof
The z-transform in (32) clearly corresponds to the input signal
u (k) =
j =1Σ
m
ajλj
k ,
k = 0,1, . . .
Substituting into (31) gives
260

Y(z) =
i 1=0Σ
∞. . .
in −1=0
Σ
∞
Hi 1 . . . in −1(z)
k =i 1+ . . . +in −1
Σ
∞
[
j =1Σ
m
ajλj
k −i 1−. . . −in −1][
j 1=1
Σ
m
aj 1λj 1
k −i 2−. . . −in −1] . . . [
jn −1=1
Σ
m
ajn −1λjn −1
k
] z −k
=
j 1=1
Σ
m . . .
jn −1=1
Σ
m
aj 1 . . . ajn −1
i 1=0Σ
∞. . .
in −1=0
Σ
∞
Hi 1 . . . in −1(z)
j =1Σ
m
aj
k =i 1+ . . . +in −1
Σ
∞
λj
k −i 1−. . . −in −1λj 1
k −i 2−. . . −in −1 . . . λjn −1
k
z −k
Now replace the index k by r = k −i 1− . . . −in −1 to obtain
Y(z) =
j 1=1
Σ
m . . .
jn −1=1
Σ
m
aj 1 . . . ajn −1
i 1=0Σ
∞. . .
in −1=0
Σ
∞
Hi 1 . . . in −1(z)
( λj 1 . . . λjn −1
z
__________)−i 1( λj 2 . . . λjn −1
z
__________)−i 2 . . . ( λjn −1
z
_____)−in −1
j =1Σ
m
aj
r =0Σ
∞
λj
r( λj 1 . . . λjn −1
z
__________)−r
=
j 1=1
Σ
m . . .
jn −1=1
Σ
m
aj 1 . . . ajn −1 Hreg( λj 1 . . . λjn −1
z
__________, . . . , λjn −1
z
_____,z)U ( λj 1 . . . λjn −1
z
__________)
Theorem 6.6 is general enough to cover a wide range of situations, and, although
somewhat messy in appearance, the calculation of system responses is relatively
straightforward. For example, if Hreg(z 1, . . . ,zn) is proper rational, then Y(z) also will be
proper rational, and partial fraction expansion can be used to compute y (k).
Before leaving the topic of transform representations, I should point out a few
simple relationships between the various transfer functions. Using (7), and a simple
change of variables,
Hreg(z 1, . . . ,zn) =
i 1=0Σ
∞. . .
in=0Σ
∞
htri(i 1+ . . . +in,i 2+ . . . +in, . . . ,in) z1
−i 1 . . . zn
−in
=
j 1=0
Σ
∞. . .
jn=0Σ
∞
htri(j 1, . . . , jn) z1
−j 1 ( z 1
z 2
___)−j 2 . . . ( zn −1
zn
____)−jn
= Htri(z 1,z 2/z 1, . . . ,zn/zn −1)
This relationship is easily inverted to obtain
Htri(z 1, . . . ,zn) = Hreg(z 1,z 1z 2, . . . ,z 1 . . . zn)
It is much messier to consider the symmetric transfer function. A basic relationship
implied by (2) is
261

Hsym(z 1, . . . ,zn) = n !
1
___
π(.)Σ Htri(z π(1), . . . ,z π(n))
Therefore
Hsym(z 1, . . . ,zn) = n !
1
___
π(.)Σ Hreg(z π(1),z π(1)z π(2), . . . ,z 1 . . . zn)
To compute Hreg or Htri from Hsym, it seems that the best way to proceed is to ﬁnd
the symmetric kernel, use (3) or (7) to obtain the regular or triangular kernel, and then
compute the z-transform. This is an unpleasant prospect at best, but there are some tricks
that can be used in simple cases.
Example 6.3
For the n = 2 case,
Hsym(z 1,z 2) = 2
1__ Hreg(z 1,z 1z 2) + 2
1__ Hreg(z 2,z 1z 2)
= 2
1__
i 1=0Σ
∞
i 2=0Σ
∞
hreg(i 1,i 2)z1
−i 1 (z 1z 2)−i 2
+ 2
1__
i 1=0Σ
∞
i 2=0Σ
∞
hreg(i 1,i 2)z2
−i 1 (z 1z 2)−i 2
Thus, a simple change of variables gives
2Hsym(z 1,z 2/z 1) =
i 1=0Σ
∞
i 2=0Σ
∞
hreg(i 1,i 2) z1
−i 1z2
−i 2
+
i 1=0Σ
∞
i 2=0Σ
∞
hreg(i 1,i 2) z1
i 1z2
−(i 1+i 2)
Clearly, the ﬁrst term on the right side is Hreg(z 1,z 2), while the second term contains only
positive powers of z 1 plus 1/2 of each z1
0 term in 2Hsym(z 1,z 2/z 1). Thus, Hreg(z 1,z 2) can
be obtained by dividing out 2Hsym(z 1,z 2/z 1), deleting all terms involving positive powers
of z 1, and multiplying each z1
0 term by 1/2. For the particular case
Hsym(z 1,z 2) = z 1z 2−1
z 1z 2
_______
changing variables and dividing gives
2Hsym(z 1,z 2/z 1) = z 2−1
2z 2
_____ = 2(1 + z2
−1 + z2
−2 + . . . )
Then, since the complete series is composed of z1
0 terms,
262

Hreg(z 1,z 2) = 1 + z2
−1 + z2
−2 + . . .
= z 2−1
z 2
_____
6.3
Obtaining
Input/Output
Representations
from
State
Equations
All of the methods discussed in Chapter 3 can be adapted to the discrete-time case
with relatively little change. Rather than ﬁll several pages by doing this, I will concentrate
on a judicious combination of the variational equation method and the Carleman
linearization method, and consider a general class of state equations at the outset. As
mentioned before, a nice feature of the discrete-time case is that the issue of impulsive
kernels does not arise. That is, direct transmission terms are naturally included in the
discrete-time input/output representation. However, the reader will notice that these terms
do complicate considerably the general forms for the kernels.
State equations of the form
x (k +1) = f [x (k),u (k)] ,
k = 0,1, . . .
y (k) = h [x (k),u (k)]
(34)
will be treated, where x (k) is n x 1 and u (k) and y (k) are scalars. It is assumed that the
initial state is x (0) = 0, that f (0,0) = 0, and that h (0,0) = 0. This is done for simplicity,
though if x (0) = x 0 = ≠ 0, and f (x 0,0) = x 0, then x 0 is an equilibrium state and a simple
variable change can be used to obtain the zero-initial-state formulation. (If x 0 is not an
equilibrium state, then more subtle machinations are required to recast the problem into
the form considered here.)
The ﬁnal assumption on (34) is that the functions f (x,u) and h (x,u) are such that
they can be represented using a Taylor’s formula about x = 0, u = 0 of order sufﬁcient to
permit calculating the polynomial input/output representation to the degree desired. Then
the given state equation can be replaced by an approximating state equation of the form
x (k +1) =
i =0Σ
N
j =0Σ
N
Fijx (i)(k)u j(k) , F00 = 0
y (k) =
i =0Σ
N
j =0Σ
N
Hijx (i)(k)u j(k) , H00 = 0
(35)
where x (i)(0) = 0 ,  i = 1, . . . ,N, and the standard Kronecker product notation is used.
Just as in the continuous-time case, the crucial fact is that the kernels through degree N
corresponding to (35) will be identical to the kernels through degree N corresponding to
(34). (The reader well versed in Chapter 3 will notice that the upper limits on the sums in
(35) need not be taken too seriously. There are a number of terms in (35) which will not
contribute to the degree-N polynomial representation.)
263

The next step is to develop difference equations for x (2)(k), x (3)(k), and so forth,
corresponding to the difference equation for x (k) in (35). This is a simple matter in
principle, though the form of the equation is different from the continuous-time case
because no product rule is involved in expressing x (j)(k +1) in terms of x (j −1)(k +1). For
example, the difference equation for x (2)(k) is given by
x (2)(k +1) = x (k +1)⊗x (k +1)
= [
i =0Σ
N
j =0Σ
N
Fijx (i)(k)u j(k)]⊗[
i =0Σ
N
j =0Σ
N
Fijx (i)(k)u j(k)]
(36)
Using implicit summation, this will result in a difference equation of the form
x (2)(k +1) =
i,j≥0
Σ [
m+n =j
k +q =i
Σ
Fkm ⊗Fqn]x (i)(k)u j(k)
(37)
where the initial condition is x (2)(0) = 0. This equation has the same form as the
difference equation for x (k) in (35), and it should be clear that the equations for x (3)(k),
x (4)(k),  . . . will also. Now, set
x ⊗(k) =
H
A
A
I x (N)(k)
...
x (1)(k) J
A
A
K
(38)
This leads to an approximating equation through degree N in the so-called state-afﬁne form
x ⊗(k +1) =
i =0Σ
N−1
Aix ⊗(k)u i(k) +
i =1Σ
N
biu i(k) , x ⊗(0) = 0
y (k) =
i =0Σ
N−1
cix ⊗(k)u i(k) +
i =1Σ
N
diu i(k)
(39)
where the upper limits in the summations are chosen to include the terms needed to
compute kernels of degree ≤ N. Of course, the dimension of this state equation is quite
high, but for a general derivation this is less of a problem than the plethora of terms.
Notice that a bilinear discrete-time state equation is a pleasingly simple case.
To solve the state-afﬁne difference equation in (39), I will use the variational
equation method, and drop the now superﬂuous Kronecker symbol. The procedure is to
assume an input signal of the form αu (k), α an arbitrary real number, and a solution of the
form
x (k) = αx 1(k) + α2x 2(k) + α3x 3(k) + . . .
(40)
Substituting into the state equation and equating the coefﬁcients of like powers of α yields
the variational equations
264

x 1(k +1) = A0x 1(k) + b 1u (k) , x 1(0) = 0
x 2(k +1) = A0x 2(k) + A1x 1(k)u (k) + b 2u 2(k) , x 2(0) = 0
x 3(k +1) = A0x 3(k) + A1x 2(k)u (k) + A2x 1(k)u 2(k) + b 3u 3(k) , x 3(0) = 0
...
xN(k +1) =
i =0Σ
N−1
AixN−i(k)u i(k) + bNu N(k) , xN(0) = 0
(41)
These equations can be solved easily, and writing the solutions recursively gives (for
k > 0)
x 1(k) =
i =0Σ
k −1
A0
k −1−ib 1u (i)
x 2(k) =
i =0Σ
k −1
A0
k −1−i[A1x 1(i)u (i) + b 2u 2(i)]
x 3(k) =
i =0Σ
k −1
A0
k −1−i[A1x 2(i)u (i) + A2x 1(i)u 2(i) + b 3u 3(i)]
...
xN(k) =
i =0Σ
k −1
A0
k −1−i[
j =1Σ
N−1
AjxN−j(i)u j(i) + bNu N(i)]
(42)
Unraveling this recursive set yields rather complicated solution formulas for the
variational equations. The ﬁrst three expressions are listed below.
x 1(k) =
i 1=0Σ
k −1
A0
k −1−i 1b 1u (i 1)
x 2(k) =
i 1=0Σ
k −1
i 2=0Σ
i 1−1
A0
k −1−i 1A1A0
i 1−1−i 2b 1u (i 1)u (i 2) + A0
k −1−i 1b 2u 2(i 1)
x 3(k) =
i 1=0Σ
k −1
i 2=0Σ
i 1−1
i 3=0Σ
i 2−1
A0
k −1−i 1A1A0
i 1−1−i 2A1A0
i 2−1−i 3b 1u (i 1)u (i 2)u (i 3)
+ A0
k −1−i 1A1A0
i 1−1−i 2b 2u (i 1)u 2(i 2) + A0
k −1−i 1A2A0
i 1−1−i 2b 1u 2(i 1)u (i 2)
+ A0
k −1−i 1b 3u 3(i 1)
(43)
265

Before proceeding to a general result, it is convenient to convert the ﬁrst two
solution expressions in (43) to the regular form. Of course, a vector kernel for x 1(k) is
easy to describe in regular form. Write
x 1(k) =
i 1=1Σ
k
A0
i 1−1b 1u (k −i 1) =
i 1=0Σ
k
g (i 1)u (k −i 1)
(44)
where, using a step function to indicate g (0) = 0,
g (i 1) = A0
i 1−1b 1δ−1(i 1−1)
(45)
Now, x 2(k) can be written in the triangular, vector-kernel expression
x 2(k) =
i 1=0Σ
k
i 2=0Σ
i 1
wtri(k,i 1,i 2)u (i 1)u (i 2)
where
wtri(k,i 1,i 2) =
B
A
A
C
A
A
D
0 , otherwise
A0
k −1−i 1b 2 , k > i 1 = i 2 ≥0
A0
k −1−i 1A1A0
i 1−1−i 2b 1 , k > i 1 > i 2 ≥0
Or, using unit step and unit pulse functions,
wtri(k,i 1,i 2) = A0
k −1−i 1A1A0
i 1−1−i 2b 1δ−1(k −1−i 1)δ−1(i 1−1−i 2)
+ A0
k −1−i 1b 2δ−1(k −1−i 1)δ0(i 1−i 2) , i 1,i 2 ≥0
(46)
To check stationarity, note that
wtri(0,i 1−k,i 2−k) = wtri(k,i 1,i 2)
(47)
so a triangular kernel in stationary form is
gtri(i 1,i 2) = wtri(0,−i 1,−i 2)
= A0
i 1−1A1A0
i 2−1−i 1b 1δ−1(i 1−1)δ−1(i 2−1−i 1)
+ A0
i 1−1b 2δ−1(i 1−1)δ0(i 2−i 1) , i 1,i 2 ≥0
(48)
Over the ﬁrst triangular domain, the triangular kernel is
gtri(i 1,i 2) = A0
i 2−1A1A0
i 1−1−i 2b 1δ−1(i 2−1)δ−1(i 1−1−i 2)
+ A0
i 2−1b 2δ−1(i 2−1)δ0(i 1−i 2)
(49)
so that the regular kernel is given by
266

greg(i 1,i 2) = gtri(i 1+i 2,i 2)
= A0
i 2−1A1A0
i 1−1b 1δ−1(i 2−1)δ−1(i 1−1) + A0
i 2−1b 2δ−1(i 2−1)δ0(i 1)
(50)
for i 1,i 2 ≥ 0.
Taking into account the output equation in (39), it is clear that the degree-2 term,
y 2(k), in the output is given by
y 2(k) = c 0x 2(k) + c 1x 1(k)u (k) + d 2u 2(k)
=
i 1=0Σ
k
i 2=0Σ
k
c 0greg(i 1,i 2)u (k −i 1−i 2)u (k −i 2)
+
i 1=0Σ
k
c 1g (i 1)u (k −i 1)u (k) + d 2u 2(k)
=
i 1=0Σ
k
i 2=0Σ
k
hreg(i 1,i 2)u (k −i 1−i 2)u(k −i 2)
(51)
where, from (45) and (50),
hreg(i 1,i 2) =
B
A
A
C
A
A
D c 0A0
i 2−1A1A0
i 1−1b 1 , i 1,i 2 > 0
c 0A0
i 2−1b 2 , i 1 = 0 , i 2 > 0
c 1A0
i 1−1b 1 , i 1 > 0, i 2 = 0
d 2 , i 1 = i 2 = 0
(52)
To perform this calculation in general is a very messy exercise in the manipulation
of summations and indices. Therefore, I will omit the details and simply present the result.
The degree-n regular kernel corresponding to the state-afﬁne state equation (39) is given
by
hreg(i 1, . . . ,in) =
B
A
A
A
A
C
A
A
A
A
D c 0A0
in−1A1A0
in −1−1 . . . A1A0
i 1−1b 1 , i 1, . . . ,in > 0
...
cqA0
in −q−1ArA0
in −q −r−1 . . . A0
ij−1bj ,
B
C
D all others = 0
ij, . . . ,in −q −r,in −q > 0
...
cn −1A0
i 1−1b 1 , i 1 > 0, i 2 = . . . = in = 0
dn , i 1 = . . . = in = 0
(53)
Notice that the sum of the subscripts in each term of this expression is n, and that the
subscript on the coefﬁcient preceding each A0
ik−1 determines the index k. There will be a
total of 2n terms in a general degree-n kernel.
Example 6.4
Discrete-time state-afﬁne systems arise directly in the description of
267

bilinear continuous-time systems with sampled input signals. For simplicity only the
degree-2 homogeneous case will be discussed, and the impulse model will be used for the
sampled signal. That is, the system is described by
y (t) =
0∫
∞
0∫
∞
hreg(σ1,σ2)u (t −σ1−σ2)u (t −σ2) dσ2dσ1
where
hreg(t 1,t 2) = ce At 2De At 1b, t 1,t 2 ≥0
and the input signal is
u (t) =
k =0Σ
∞
u (kT)δ0(t −kT)
where T is the sampling period. Then the output at the mth sampling instant is given by
y (mT) =
0∫
∞
0∫
∞
hreg(σ1,σ2)
k 1=0
Σ
∞
u (k 1T)δ0(mT−σ1−σ2−k 1T)
k 2=0
Σ
∞
u (k 2T)δ0(mT−σ2−k 2T) dσ2dσ1
=
k 1=0
Σ
∞
k 2=0
Σ
∞
hreg(k 2T−k 1T,mT−k 2T)u (k 1T)u (k 2T)
Restoring this expression to regular form requires changes of variables of summation.
First replace k 2 by j 2 = m − k 2, and replace k 1 by j 1 = m − j 2 − k 1. Then using the fact
that the input signal and the regular kernel both are zero for negative arguments gives
y (mT) =
j 1=0
Σ
∞
j 2=0
Σ
∞
hreg(j 1T,j 2T)u (mT−j 1T−j 2T)u (mT−j 2T)
Thus it is clear that the regular kernel for this discrete-time representation is
h (j 1T,j 2T) = c (e AT)j 2D (e AT)j 1b , j 1,j 2 ≥0
Now with the deﬁnitions
A0 = e AT , A1 = D , b 1 = e ATb , b 2 = Db
c 0 = ce AT , c 1 = cD , d 2 = cDb
this kernel conforms to the state-afﬁne kernel speciﬁed in (53).
As a ﬁnal comment, observe that it is straightforward to compute the degree-n
regular transfer function from (53). Indeed,
268

Hreg(z 1, . . . ,zn) =
i 1=0Σ
∞. . .
in=0Σ
∞
hreg(i 1, . . . ,in) z1
−i 1 . . . zn
−in
= dn + cn −1(z 1I −A0)−1b 1 + . . .
+ cq(zn −qI −A0)−1Ar(zn −q −rI −A0)−1 . . . (zjI −A0)−1bj
+ . . . + c 0(znI −A0)−1A1(zn −1I −A0)−1A1 . . . A1(z 1I −A0)−1b 1 (54)
though this expression, like (53), is not very explicit, and a certain amount of digging is
needed to produce all 2n terms.
Example 6.5
The degree-3 regular transfer function for the state-afﬁne state equation
(39) is
Hreg(z 1,z 2,z 3) = d 3 + c 2(z 1I −A0)−1b 1 + c 1(z 2I −A0)−1b 2
+ c 0(z 3I −A0)−1b 3 + c 1(z 2I −A0)−1A1(z 1I −A0)−1b 1
+ c 0(z 3I −A0)−1A2(z 1I −A0)−1b 1 + c 0(z 3I −A0)−1A1(z 2I −A0)−1b 2
+ c 0(z 3I −A0)−1A1(z 2I −A0)−1A1(z 1I −A0)−1b 1
If the state equation actually is bilinear, then the only surviver among these terms is the
last one.
6.4 State-Afﬁne Realization Theory
The realization problem for discrete-time systems will be discussed mainly in terms
of the regular-transfer-function input/output representation, and state-afﬁne state equations
(realizations). Thus, the realization theory considered here is somewhat more general than
that in Chapter 4. In fact, the bilinear realization theory for discrete-time systems will
appear as a relatively uncomplicated special case. (The bilinear theory also can be
obtained from Chapter 4 by modiﬁcations only slightly more difﬁcult than wholesale
replacement of s’s by z’s.) I will concentrate on homogeneous and polynomial systems
here, and leave Volterra systems to the original research literature.
Recall that a transfer function is called rational if it can be written as a ratio of
polynomials:
H (z 1, . . . ,zn) = Q (z 1, . . . ,zn)
P (z 1, . . . ,zn)
____________
(55)
A rational transfer function is called proper (strictly proper) if degree P (z 1, . . . ,zn) in
each variable is no greater than (less than) degree Q (z 1, . . . ,zn) in the corresponding
variable.
A
rational
transfer
function
is
called
recognizable
if
269

Q (z 1, . . . ,zn) = Q1(z 1) . . . Qn(zn), where each Qj(zj) is a single variable polynomial. As
in Chapter 4, it is assumed that the numerator and denominator polynomials are relatively
prime to rule out trivial issues.
A state-afﬁne realization of a degree-n homogeneous or polynomial system takes the
form
x (k +1) =
i =0Σ
n −1
Aix (k)u i(k) +
i =1Σ
n
biu i(k)
y (k) =
i =0Σ
n −1
cix (k)u i(k) +
i =1Σ
n
diu i(k)
(56)
where the (ﬁnite) dimension of the state vector x (k) is called the dimension of the
realization. Notice that the upper limits on the summations in (56) have been set in
accordance with the degree of the system.
Using this formulation a basic result on realizability can be stated as follows.
Theorem 6.7
A degree-n homogeneous discrete-time system is state-afﬁne realizable if
and only if the regular transfer function of the system is a proper, recognizable function.
Proof
If the system has a state-afﬁne realization, then the regular transfer function can
be written as in (54). Writing each (zkI − A0)−1 in the classical-adjoint-over-determinant
form, and placing all the terms over a common denominator shows that Hreg(z 1, . . . ,zn) is
a proper, recognizable function.
If Hreg(z 1, . . . ,zn) is a proper, recognizable function, then it can be written in the
form
Hreg(z 1, . . . ,zn) = Q1(z 1) . . . Qn(zn)
P (z 1, . . . ,zn)
_______________
(57)
where
P (z 1, . . . ,zn) =
j 1=0
Σ
m1 . . .
jn=0Σ
m2
pj 1 . . . jnz1
j 1 . . . zn
jn
Qi(zi) = zi
mi +
ji=0Σ
mi−1
qi,jizi
ji , i = 1, . . . ,n
(58)
Just as in the continuous-time case, the numerator polynomial can be written in a matrix
factored form
P (z 1, . . . ,zn) = Zn . . . Z2Z1P
where Zj contains entries chosen from 0, 1, zj, . . . , zj
mj,  j = 1, . . . ,n, and P is a vector of
coefﬁcients. Thus the regular transfer function can be written in the factored form
270

Hreg(z 1, . . . ,zn) = Qn(zn)
Zn
______ . . .
Q2(z 2)
Z2
_______
Q1(z 1)
Z1P
_______
= Gn(zn) . . . G2(z 2)G1(z 1)
Each Gj(zj) is a matrix with proper rational entries, and thus linear-realization techniques
can be used to write
Gj(zj) = Cˆ j(zjI −Aˆ j)−1Bˆ j + Dˆ j, j = 1, . . . ,n
Now consider the state-afﬁne realization (56) speciﬁed as follows. Let A0 be block
diagonal, and Aj be zero except possibly on the j th block subdiagonal:
A0 =
H
A
A
A
A
I
0
...
0
Aˆ 1
0
...
Aˆ 2
0
. . .
...
. . .
. . .
Aˆn
...
0
0 J
A
A
A
A
K
A1 =
H
A
A
A
I
0
...
Bˆ 2Cˆ 1
0
. . .
...
. . .
. . .
BˆnCˆ n −1
...
0
0
0
...
0
0 J
A
A
A
K
Aj =
H
A
A
A
A
I
0
...
Bˆ j +1Dˆ j . . . Dˆ 2Cˆ 1
...
0
. . .
...
. . .
...
. . .
BˆnDˆ n −1 . . . Dˆ n −j +1Cˆ n −j
...
0
...
0
J
A
A
A
A
K
Let each bj have zero entries except possibly for the j th block entry according to
b 1 =
H
A
A
A
I
0
...
0
Bˆ 1J
A
A
A
K
, bj =
H
A
A
A
A
I
0
...
Bˆ jDˆ j −1 . . . Dˆ 1
...
0
J
A
A
A
A
K
, j = 2, . . . ,n
Let each cj have zero entries except possibly for the (n −j)th block entry according to
c 0 = [0 . . . 0 Cˆ n]
cj = [0 . . . (Dˆ n . . . Dˆ n −j +1Cˆ n −j) . . . 0] , j = 1, . . . ,n −1
and, ﬁnally, let d 1 = . . . = dn −1 = 0, dn = Dˆ nDˆ n −1 . . . Dˆ 1. The regular transfer functions
corresponding to this state-afﬁne realization can be computed from (54). Because of the
271

special block structure, it turns out that all the transfer functions of degree ≠ n are zero,
and the degree-n transfer function is given by
Hreg(z 1, . . . ,zn) = (Dˆ n + Cˆ n(znI−Aˆn)−1Bˆn) . . . (Dˆ 1 + Cˆ 1(z 1I−Aˆ 1)−1Bˆ 1)
= Gn(zn) . . . G1(z 1)
This calculation, which is left as an uninteresting exercise, completes the proof.
Corollary 6.1
A degree-n homogeneous, discrete-time system is bilinear realizable if
and only if the regular transfer function of the system is a strictly proper, recognizable
function.
Before proceeding to the construction of minimal state-afﬁne realizations, perhaps a
slight digression on realizability is in order. So far I have presented all the realization
results in terms of the regular transfer function. These results are easily transcribed to the
triangular transfer function representation since a simple change of variables relates the
two transfer functions. However, it is much more difﬁcult to discuss realizability in terms
of the symmetric transfer function. One way to approach this topic is to use relationships
between the regular and symmetric transfer functions. This topic was discussed brieﬂy in
Section 6.2 for the degree-2 case, and that discussion will be followed up here.
Example 6.6
From the relationship
Hsym(z 1,z 2) = 2
1__Hreg(z 1,z 1z 2) + 2
1__Hreg(z 2,z 1z 2)
it is clear that the symmetric transfer function for a state-afﬁne-realizable degree-2
homogeneous system must have the form
Hsym(z 1,z 2) = Q1(z 1)Q1(z 2)Q2(z 1z 2)
P (z 1,z 2)
____________________
where Q1(z 1) and Q2(z 2) are single-variable polynomials, and P (z 1,z 2) is a 2-variable
polynomial. But the numerator cannot be arbitrary; there also are constraints on the form
of P (z 1, . . . ,zn). These constraints are rather subtle to work out, so I will be content to
continue Example 6.3 by indirect methods. The symmetric transfer function
Hsym(z 1,z 2) = z 1z 2−1
z 1z 2
_______
is state-afﬁne realizable since the corresponding regular transfer function is the proper,
recognizable rational function
Hreg(z 1,z 2) = z 2−1
z 2
_____
The construction of a state-afﬁne realization as given in the proof of Theorem 6.7 is
exceedingly simple in this case, giving
272

Cˆ 1 = Aˆ 1 = Bˆ 1 = 0, Dˆ 1 = 1
Cˆ 2 = Aˆ 2 = Bˆ 2 = Dˆ 2 = 1
Thus a state-afﬁne realization is
x (k +1) = H
I 0
0
1
0 J
K x (k) + H
I 1
0 J
K u 2(k)
y (k) = [0
1] x (k) + u 2(k)
Note that this case is so simple that by deﬁning a new input, uˆ(t) = u 2(t), the realization is
linear. Also, it is clear that the ﬁrst component of the state vector can be discarded to
obtain a 1-dimensional realization.
For the polynomial-system case, the input/output representation that is natural to
consider is the sequence of regular transfer functions of the homogeneous subsystems.
Then the basic realizability result (and its proof) is simply a restatement of Theorem 4.9 in
Section 4.3.
Theorem 6.8
A polynomial, discrete-time system is state-afﬁne realizable if and only if
the regular transfer function of each homogeneous subsystem is a proper, recognizable
function.
To construct minimal-dimension state-afﬁne realizations for polynomial systems, a
shift operator approach much like that in Chapter 4 will be used. Of course, there are more
kinds of terms to be dealt with in the state-afﬁne case, and nonpositive power series rather
than negative power series are involved due to the deﬁnition of the z-transform. However,
the basic ideas are just the same.
For a given ﬁnite-length sequence of regular transfer functions
Hˆ (z 1, . . . ,zN) = ( H (z 1), Hreg(z 1,z 2), . . . , Hreg(z 1, . . . ,zN),0, . . . )
(59)
where
Hreg(z 1, . . . ,zk) =
i 1=0Σ
∞. . .
ik=0Σ
∞
hreg(i 1, . . . ,ik) z1
−i 1 . . . zk
−ik
(60)
the minimal realization problem can be stated as follows. Find matrices A0, . . . , AN−1 of
dimension m x m, b 1, . . . , bN of dimension m x 1, c 0, . . . , cN−1 of dimension 1 x m, and
scalars d 1, . . . , dN such that (53) is satisﬁed for n = 1, . . . , N, the right side of (53) is 0
for n > N, and such that m is as small as possible. These matrices specify a state-afﬁne
realization of the form (56) of dimension m, and the shorthand notation (Aj,bj,cj,dj,Rm)
will be used for such a realization.
Given any nonpositive power series
Vk(z 1, . . . ,zk) =
i 1=0Σ
∞. . .
ik=0Σ
∞
vi 1 . . . ik z1
−i 1 . . . zk
−ik
(61)
273

deﬁne a shift operator via
SVk(z 1, . . . ,zk) =
i 1=0Σ
∞. . .
ik=0Σ
∞
vi 1+1,i 2 . . . ik z1
−i 1 . . . zk
−ik
(62)
It is easy to verify that the shift is a linear operator that can be interpreted as
SVk(z 1, . . . ,zk) = z 1[Vk(z 1, . . . ,zk) −Vk(∞,z 2, . . . ,zk)]
(63)
and that SVk(z 1, . . . ,zk) is also a nonpositive power series. An index operator is deﬁned
by
TVk(z 1, . . . ,zk) =
B
A
C
A
D
i 1=0Σ
∞. . .
ik −1=0
Σ
∞
v 0i 1 . . . ik −1z1
−i 1 . . . zk −1
−ik −1 , k > 1
0 , k = 1
(64)
that is
TVk(z 1, . . . ,zk) = Vk(∞,z 1, . . . ,zk −1) , k > 1
(65)
Again, it is not hard to see that T is a linear operator and that TVk(z 1, . . . ,zk) is a
nonpositive power series. The same symbols S and T will be used regardless of the
number of variables in the series to which the operators are applied. Then these deﬁnitions
can be extended to ﬁnite-length sequences of nonpositive power series in the obvious way:
S (V (z 1),V2(z 1,z 2), V3(z 1,z 2,z 3), . . . ) = ( SV (z 1), SV 2(z 1,z 2), SV 3(z 1,z 2,z 3), . . . )(66)
T( V (z 1), V2(z 1,z 2), V (z 1,z 2,z 3), . . . ) = ( TV2(z 1,z 2), TV3(z 1,z 2,z 3), . . . )
(67)
Now suppose a degree-N polynomial system is speciﬁed as in (59). Then a
collection of linear spaces of ﬁnite-length sequences of nonpositive power series is deﬁned
as follows.
U1 = span { S iHˆ (z 1, . . . ,zN) i > 0 }
U2 = span { S iTS jHˆ (z 1, . . . ,zN) i > 0, j ≥0 }
U3 = span { S iTS jTSkHˆ (z 1, . . . ,zN) i > 0, j ≥0, k ≥0 }
...
U = span { U1, U2, . . . , UN }
(68)
Then S and T can be viewed as linear operators with U as the domain and range.
Deﬁne a set of initialization operators Lj:R → U according to
274

L1r = SHˆ (z 1, . . . ,zN) r
L2r = STHˆ (z 1, . . . ,zN) r
...
LNr = STN−1Hˆ (z 1, . . . ,zN) r
(69)
and deﬁne a set of evaluation operators Ej:U → R as follows. If
Vˆ(z 1, . . . ,zN) = ( V1(z 1), V2(z 1,z 2), . . . , VN(z 1, . . . ,zN), 0, . . . )
is an element of U, then
E0Vˆ(z 1, . . . ,zN) = V1(∞)
E1Vˆ(z 1, . . . ,zN) = E0TVˆ(z 1, . . . ,zN) = V2(∞,∞)
...
EN−1Vˆ(z 1, . . . ,zN) = E0TN−1Vˆ(z 1, . . . ,zN) = VN(∞, . . . ,∞)
(70)
Finally, let dj:R → R be speciﬁed by
dj = Hreg(z 1, . . . ,zj) z1= . . . =zj=∞,
j = 1, . . . ,N
(71)
To show that these all are linear operators on their respective domains is very easy.
Now it can be shown that if U is ﬁnite dimensional, then (ST j,Lj,Ej,dj,U) is an
abstract, ﬁnite-dimensional, state-afﬁne realization of the given polynomial system. Once
this is done, ﬁnding a concrete realization involves replacing U by Rm and ﬁnding the
matrix representations Aj = ST j, bj = Lj, cj = Ej, and interpreting the scalar operators dj as
constants. The proof that this process yields a minimal-dimension state-afﬁne realization
will be omitted since it is complicated. In fact, I will omit most of the demonstration that
(ST j,Lj,Ej,dj,U) is a realization. To indicate how the calculation goes, consider the case of
a degree-N polynomial system with N > 3,
Hˆ (z 1, . . . ,zN) = ( H(z 1), Hreg(z 1,z 2), Hreg(z 1,z 2,z 3), . . . )
= (
i 1=0Σ
∞
h (i 1)z1
−i 1,
i 1=0Σ
∞
i 2=0Σ
∞
hreg(i 1,i 2)z1
−i 1z2
−i 2,
i 1=0Σ
∞
i 2=0Σ
∞
i 3=0Σ
∞
hreg(i 1,i 2,i 3)z1
−i 1z2
−i 2z3
−i 3, . . . )
Then a selection of degree-3 terms in (53) can be veriﬁed as follows. The constant term is
d 3 = Hreg(∞,∞,∞) = hreg(0,0,0)
The term c 2A0
j 1−1b 1 in (53) corresponds to
275

E2S j 1−1L1 = E0T2S j 1Hˆ (z 1, . . . ,zN)
= E0T2(
i 1=0Σ
∞
h (i 1+j 1) z1
−i 1,
i 1=0Σ
∞
i 2=0Σ
∞
hreg(i 1+j 1,i 2) z1
−i 1z2
−i 2,
i 1=0Σ
∞
i 2=0Σ
∞
i 3=0Σ
∞
hreg(i 1+j 1,i 2,i 3) z1
−i 1z2
−i 2z3
−i 3, . . . )
= E0(
i 1=0Σ
∞
hreg(j 1,0,i 1) z1
−i 1, . . . ) = hreg(j 1,0,0)
As a ﬁnal illustration, c 0A0
j 3−1A2A0
j 1−1b 1 corresponds to
E0S j 3−1(ST2)S j 1−1L1 = E0S j 3T2S j 1Hˆ (z 1, . . . ,zN)
= E0S j 3(
i 1=0Σ
∞
hreg(j 1,0,i 1) z1
−i 1, . . . )
= E0(
i 1=0Σ
∞
hreg(j 1,0,i 1+j 3) z1
−i 1, . . . )
= hreg(j 1,0,j 3)
All this shifting, indexing, and evaluating rapidly becomes a lot of fun and I urge the
reader to do a few more terms. However, the investment in notation and calculation
needed to verify the realization in general is probably unproﬁtable.
Example 6.7
Just to ﬁx the nature of the calculations, consider a simple polynomial
system described by
Hˆ (z 1,z 2) = ( z 1−1
z 1
_____ , z 2−1
z 2
_____ , 0, . . . )
(Here I will not work with the power-series form of the regular transfer functions, for
simplicity.) Application of the shift operator gives
SHˆ (z 1,z 2) = ( z 1−1
z 1
_____ , 0, . . . )
S 2Hˆ (z 1,z 2) = SHˆ (z 1,z 2)
so that
U1 = span
B
C
D
( z 1−1
z 1
_____ , 0, . . . )
E
F
G
276

Application of the index operator gives THˆ (z 1,z 2) = SHˆ (z 1,z 2), and an easy calculation
shows that TSHˆ (z 1,z 2) = 0. Thus U = U1 is a one-dimensional linear space, and it can be
replaced by R1 with the basis element 1. In terms of this basis, the shift and index
operators are represented by S = 1 and T = 0. Thus A0 = 1, and A1 = 0. The initialization
operators are represented by L1 = SHˆ (z 1,z 2) = 1, and L2 = STHˆ (z 1,z 2) = 1, so that
b 1 = b 2 = 1. The evaluation operators give
E0SHˆ (z 1,z 2) = 1 ,
E1SHˆ (z 1,z 2) = 0
from which c 0 = 1 and c 1 = 0. Finally, it is clear that d 1 = d 2 = 1. Thus, a minimal state-
afﬁne realization of the given system is
x (k +1) = x (k) + u (k) + u 2(k)
y (k) = x (k) + u (k) + u 2(k)
The extension of this approach to the Volterra system case should be evident in
broad outline. The kinds of difﬁculties that arise are indicated in Section 4.4, and the
general theory is discussed in detail in the research literature cited in Section 6.8.
6.5 Response Characteristics of Discrete-Time Systems
The response of both homogeneous and polynomial discrete-time systems to various
classes of input signals can be analyzed using much the same approach as in Chapter 5.
To substantiate this claim, I will outline how some of the analysis goes for unit pulse and
sinusoidal inputs. For random input signals, some of the results paralleling the
continuous-time case will be derived from a less informal viewpoint than was adopted in
Section 5.5.
Consider ﬁrst the response of a degree-n homogeneous system to inputs composed
of sums of delayed unit pulses, where the unit pulse is deﬁned by
δ0(k) =
B
C
D 0, otherwise
1, k = 0
(72)
In terms of the symmetric kernel representation, the calculations in Section 5.1 carry over
directly, and so they will not be repeated here. However, I will go through some simple
calculations in terms of the regular kernel representation in order to point out one perhaps
surprising feature.
For the homogeneous system
y (k) =
i 1=0Σ
∞. . .
in=0Σ
∞
hnreg(i 1, . . . ,in)u (k −i 1−. . . −in) . . . u (k −in)
(73)
with the input u (k) = δ0(k), simple inspection yields the response
y (k) = hnreg(0, . . . ,0,k) , k = 0,1, . . .
(74)
277

A more interesting situation occurs when the input is composed of two unit pulses,
u (k) = δ0(k) + aδ0(k −K)
(75)
where a is a real number, and K is a positive integer. The calculation of the corresponding
response as outlined below is simple, though a bit lengthy. The response formula (73)
gives
y (k) =
i 1=0Σ
∞. . .
in=0Σ
∞
hnreg(i 1, . . . ,in)[δ0(k −i 1−. . . −in)+aδ0(k −K −i 1−. . . −in)]
. . . [δ0(k −in −1−in)+aδ0(k −K −in −1−in)][δ0(k −in)+aδ0(k −K −in)]
(76)
Inspection of the last bracketed term on the right side shows that the summand will be
nonzero only for two values of in, namely, in = k, and in = k − K. Thus,
y (k) =
i 1=0Σ
∞. . .
in −1=0
Σ
∞
hnreg(i 1, . . . ,in −1,k)[δ0(−i 1−. . . −in −1)
+ aδ0(−K −i 1−. . . −in −1)] . . . [δ0(−in −1) + aδ0(−K −in −1)]
+
i 1=0Σ
∞. . .
in −1=0
Σ
∞
ahnreg(i 1, . . . ,in −1,k −K)[δ0(K −i 1−. . . −in −1)
+ aδ0(−i 1−. . . −in −1)] . . . [δ0(K −in −1) + aδ0(−in −1)]
(77)
In the ﬁrst term on the right side of (77), a little thought shows that the only nonzero
summand occurs when i 1 = . . . = in −1 = 0. The second term is somewhat less easy to
penetrate, so I will carry the calculation one step further. The summand will be nonzero
only for the values in −1 = K, and in −1 = 0. Thus,
y (k) = hnreg(0, . . . ,0,k)
+
i 1=0Σ
∞. . .
in −2=0
Σ
∞
ahnreg(i 1, . . . ,in −2,K,k −K)[δ0(−i 1−. . . −in −2)
+ aδ0(−i 1−. . . −in −2−K)] . . . [δ0(−in −2) + aδ0(−in −2−K)]
+
i 1=0Σ
∞. . .
in −2=0
Σ
∞
a 2hnreg(i 1, . . . ,in −2,0,k −K)[δ0(K −i 1−. . . −in −2)
+ aδ0(−i 1−. . . −in −2)] . . . [δ0(K −in −2) + aδ0(−in −2)]
(78)
Again, from the ﬁrst summation, only the summand with i 1 = . . . = in −2 = 0 is nonzero.
The second summation should be reduced further, but a pattern rapidly emerges that yields
the response formula
278

y (k) = hnreg(0, . . . ,0,k) + ahnreg(0, . . . ,0,K,k −K)
+ a 2hnreg(0, . . . ,0,K, 0,k −K) + a 3hnreg(0, . . . ,0,K, 0,0,k −K)
+ . . . + a nhnreg(0, . . . ,0,k −K)
(79)
(Rather than insert unit step functions in this expression, it is left understood that the
regular kernel is zero if any argument is negative.)
The interesting thing about the response formula (79) is that if the system described
by hnreg(i 1, . . . ,in) is bilinear realizable, and n > 2, then y (k) is identically zero. This
follows from the fact that the regular kernel corresponding to a homogeneous bilinear state
equation is zero if any argument is zero. The general statement is that a degree-n
homogeneous bilinear state equation has zero response to an input that contains at most
n −1 nonzero values. The general proof in the style of the two-pulse case requires a very
messy calculation. A much shorter proof is suggested in Problem 6.4. At any rate, this
special property of discrete-time bilinear state equations indicates their somewhat
restricted input/output behavior. In contrast, state-afﬁne state equations are quite general,
as should be clear from Section 6.3.
Frequency-response properties of the type discussed in Chapter 5 carry over more or
less directly to the discrete-time case. To illustrate this I will brieﬂy consider the steady-
state response of a discrete-time, homogeneous system to the input signal
u (k) = 2Acos (ωk) = Ae iωk + Ae −iωk
(80)
The output can be written in terms of the symmetric kernel as
y (k) =
i 1=0Σ
k . . .
in=0Σ
k
hnsym(i 1, . . . ,in)
j =1
Π
n
[Ae iω(k −ij) + Ae −iω(k −ij)]
(81)
Letting λ1 = iω and λ2 = −iω, expanding the n-fold product, and rearranging the
summations gives
y (k) = An
k 1=1
Σ
2
. . .
kn=1
Σ
2
[
i 1=0Σ
k . . .
in=0Σ
k
hnsym(i 1, . . . ,in) exp(−
j =1Σ
n
λkjij)] exp(
j =1Σ
n
λkjk)
Assuming convergence of the bracketed summations as k becomes large, y (k) becomes
arbitrarily close to the steady-state response deﬁned by
yss(k) = An
k 1=1
Σ
2
. . .
kn=1
Σ
2
Hnsym(e
λk 1, . . . ,e
λkn) exp(
j =1Σ
n
λkjk)
(82)
There are many terms in (82) with identical exponents, and these can be collected
together using the symmetry of the transfer function. Let
Gm,n −m(e λ1,e λ2) = B
D m
nE
G Hnsym(
m
e λ1, . . . ,e λ1;
n −m
e λ2, . . . ,e λ2)
(83)
for m = 0,1, . . . ,n. Then, replacing λ1 by iω and λ2 by −iω,
279

yss(k) = An[Gn, 0(e iω,e −iω)e inωk + G0,n(e iω,e −iω)e −inωk]
+ An[Gn −1,1(e iω,e −iω)e i (n −2)ωk + G1,n −1(e iω,e −iω)e −i (n −2)ωk]
+ . . . +
B
A
C
A
D
An[G
2
n +1
_____,
2
n −1
_____(e iω,e −iω)e iωk + G
2
n −1
_____,
2
n +1
_____(e iω,e −iω)e −iωk] , n odd
AnGn /2,n /2(e iω,e −iω) , n even
(84)
Using standard identities and the fact that
Gm,n −m(e iω,e −iω) = Gn −m,m(e −iω,e iω)
gives the steady-state response expression
yss(k) = 2An |Gn, 0(e iω,e −iω)| cos[nωk + ∠Gn, 0(e iω,e −iω)]
+ 2An |Gn −1,1(e iω,e −iω)| cos[(n −2)ωk + ∠Gn −1,1(e iω,e −iω)]
+ . . . +
B
A
C
A
D
2An |G
2
n +1
_____,
2
n −1
_____(e iω,e −iω)cos[ωk + ∠G
2
n +1
_____,
2
n −1
_____(e iω,e −iω)] , n odd
AnGn /2,n /2(e iω,e −iω) , n even
(85)
This calculation should be enough to indicate that the results of Sections 5.2 and 5.3 can
be developed for the discrete-time case with ease.
Nonlinear systems with random inputs, the last major topic of Chapter 5, can be
developed for discrete-time systems in a simple, informal manner that parallels the
continuous-time case. Rather than do this, I will discuss orthogonal representations for
discrete-time nonlinear systems with random inputs from a more general and more
rigorous viewpoint. (While these more general ideas can be carried back to continuous-
time systems, it is much easier to approach rigor without mortis in the discrete-time case.)
The development of orthogonal representations for nonlinear systems with random
input signals will be based on the notion of orthogonalizing a random process. A discrete-
time random process will be written in the form
u = u (k) ; k = . . . ,−1,0,1,2, . . .
(86)
and it will be assumed throughout that u is real, (strict-sense) stationary, and such that
|E [u n(k)]| < ∞for all nonnegative integers n.
Furthermore, it will be assumed
throughout that the random process u is independent of order n for all nonnegative integers
n. That is, for distinct indices i 1, . . . ,in, and any polynomials p 1(x), . . . ,pn(x),
E [p 1(u (i 1)) . . . pn(u (in))] = E [p 1(u (i 1))] . . . E [pn(u (in))]
(87)
This is a restrictive assumption, but it plays a crucial role in the development. It can be
shown that a white Gaussian random process satisﬁes this assumption, and so the setting
280

here includes the discrete-time version of the case discussed in Section 5.5.
Deﬁnition 6.1
The random process u is called polynomial orthogonalizable if there exist
real, symmetric polynomial functions
Φn(i 1, . . . ,in,u) = Φn(u (i 1), . . . ,u (in)) , n = 0,1,2, . . .
(88)
such that for all integers i 1, . . . ,in,j 1, . . . ,jm,
E [Φn(i 1, . . . ,in,u)Φm(j 1, . . . , jm,u)] =
B
A
C
A
D
0 , n ≠m
E [Φn(i 1, . . . ,in,u)Φn(j 1, . . . , jn,u)] , n = m
(89)
Such a set will be called a polynomial orthogonal representation for u.
An approach to ﬁnding polynomial orthogonal representations for a random process
can be given as follows. The notation that often will be used for Φn involves collecting
together repeated arguments and showing the number of occurrences. From symmetry, it
is clear that this reordering is immaterial.
Lemma 6.2
Suppose that ψn(x), n = 0,1,2, . . . , is a set of single-variable polynomials
with ψ0(x) = 1 and such that for the random process u,
E [ψn(u (k))ψm(u (k))] =
B
A
C
A
D
0 , n ≠m
E [ψn
2(u (k))] < ∞, n = m
(90)
Then the random process u is polynomial orthogonalizable, and a polynomial orthogonal
representation is given by
Φn(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip,u) = ψn 1(u (i 1)) . . . ψnp(u (ip))
(91)
where i 1, . . . ,ip are distinct integers, and n 1+ . . . +np = n.
Proof
It is clear that each Φn deﬁned in (91) is a symmetric polynomial function.
Furthermore, with some abuse of notation that arises in collecting together repeated
arguments in the style of (91), E [Φn(i 1, . . . ,in,u)Φm(j 1, . . . ,jm,u)] can be written in the
form
E [Φn(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip,u)Φm(
m1
j 1, . . . , j 1; . . . ;
mq
jq, . . . , jq,u)]
= E [ψn 1(u (i 1)) . . . ψnp(u (ip))ψm1(u (j 1)) . . . ψmq(u (jq))]
(92)
where i 1, . . . ,ip are distinct with n 1+ . . . +np = n, and j 1, . . . ,jq are distinct with
m1+ . . . +mq = m. If m ≠ n either there is a distinct integer, say i 1, in the set
281

{i 1, . . . ,ip, j 1, . . . ,jq}, or there are two identical integers, say i 1 = j 1, such that n 1 ≠ m1.
Using the independence assumption to write (92) as a product of expected values, in the
former case one of the factors will be E [ψn 1(u (i 1))], which is zero since ψn 1(x) is
orthogonal
to
ψ0(x) = 1.
In
the
latter
case
one
of
the
factors
will
be
E [ψn 1(u (i 1))ψm1(u (i 1))], which also is zero. Thus (89) has been veriﬁed, although it is
convenient to further note here that when n = m, (92) gives zero unless {j 1, . . . ,jn} is a
permutation
of
{i 1, . . . ,in}.
If
the
permutation
condition
holds,
then
E [Φn(i 1, . . . ,in,u)Φn(j 1, . . . ,jn,u)] is given by
E [Φn
2(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip,u)] = E [ψn 1
2 (u (i 1))] . . . E [ψnp
2 (u (ip))]
(93)
where i 1, . . . ,ip are distinct, and n 1+ . . . +np = n.
The example that will be carried throughout this topic corresponds to the Wiener
orthogonal representation discussed for the continuous-time case in Section 5.5.
Example 6.8
Suppose the random process u is zero-mean, Gaussian, and white, with
intensity E [u 2(k)] = A. Then it can be veriﬁed that u satisﬁes the order n independence
condition. To construct a polynomial orthogonal representation, take ψn, n = 0,1,2, . . . to
be the Hermite polynomials given by
ψn(x) =
r =0Σ
[n /2]
r !2r(n −2r)!
(−1)rn !Ar
___________ x n −2r
(94)
where [n /2] is the largest integer ≤ n /2. The ﬁrst few Hermite polynomials are
ψ0(x) = 1, ψ1(x) = x, ψ2(x) = x 2 −A
It is left to the references to verify (90) in this case, and to obtain the identity
E [ψn
2(u (k))] = n !An
(95)
However, I should point out that arguments reminiscent of the proofs of Theorems 5.1 and
5.2 can be used in place of an appeal to the literature. At any rate, the Hermite polynomials
lead to a polynomial orthogonal representation for zero-mean, white Gaussian random
processes via the deﬁnition in (91).
The following mathematical framework will be convenient in developing the
representation for nonlinear systems with random inputs. Let F [u (k)] be a real-valued
functional of the sample function u (k) of the random process u.
Assume that
E [F 2[u (k)]] < ∞, and denote by L2(u) the Hilbert space of such functionals F and G with
inner product
<F,G > = E [F [u (k)]G [u (k)]]
(96)
Suppose  Φ0, Φ1,  . . .  is a polynomial orthogonal representation for u constructed as in
Lemma 6.2. Then for each n and i 1, . . . ,in, Φn(i 1, . . . ,in,u) is an element of L2(u). If
fn(i 1, . . . ,in) is a real-valued function that satisﬁes
282

i 1=−∞
Σ
∞
. . .
in=−∞
Σ
∞
fn
2(i 1, . . . ,in) < ∞
(97)
then
i 1=−∞
Σ
∞
. . .
in=−∞
Σ
∞
fn(i 1, . . . ,in)Φn(i 1, . . . ,in,u)
(98)
is an element of L2(u). (The demonstration of this fact will be omitted. In the sequel,
functions fn that are nonzero for only ﬁnitely many arguments will be considered, and in
this case the claim is clear.) It is left to the reader to show that, when considering
expressions of the form (98), the symmetry of Φn(i 1, . . . ,in,u) implies that without loss of
generality fn(i 1, . . . ,in) can be assumed to be symmetric.
Now consider a stationary, causal system y (k) = H [u (k)], where the input is a
sample function from the real, stationary, independent of order n, random process u. To
represent the system as an element of L2(u), assume that k is ﬁxed, E [y 2(k)] < ∞, and for
simplicity that the system is ﬁnite memory. That is, there exists a positive integer M such
that y (k) depends only on the values u (k), u (k −1), . . . ,u (k −M). Such a system will be
denoted by the functional notation
y (k) = H [u (k−j) , j = 0,1, . . . ,M]
(99)
All the machinery is now available to develop a representation for a system of the
form (99), which explicitly involves a polynomial orthogonal representation for u. In
particular, consider a representation of the form
yN(k) = HN[u (k−j) ,
j = 0,1, . . . ,M]
=
n =0Σ
N
i 1=0Σ
M . . .
in=0Σ
M
kn(i 1, . . . ,in)Φn(k −i 1, . . . ,k −in,u)
(100)
where each kn(i 1, . . . ,in) is symmetric. Clearly HN belongs to L2(u), and the system
representation is stationary, ﬁnite memory, and causal. The objective is to choose the
coefﬁcient functions kn(i 1, . . . ,in) so that (100) approximates (99) in the mean-square
sense. That is, choose k 0, k 1(i 1), . . . , kN(i 1, . . . ,iN) to minimize the error
y (k) −yN(k)2 = <y (k)−yN(k), y (k)−yN(k)> = E [(y (k) −yN(k))2]
(101)
Deﬁnition 6.2
The symmetric functions kn(i 1, . . . ,in) that minimize (101) are called
Fourier kernels (relative to the Φn), and the resulting functional (100) is called a
functional Fourier series representation of the system.
Theorem 6.9
Suppose a polynomial orthogonal representation for u is constructed as in
Lemma 6.2 using the polynomials ψ0(x), ψ1(x),  . . . . Then the n th Fourier kernel is given
by
283

kn(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip) =
n !E [ψn 1
2 (u (k))] . . . E [ψnp
2 (u (k))]
n 1! . . . np!
_____________________________ E [y (k)ψn 1(u (k−i 1)) . . . ψnp(u (k−ip))] (102)
where i 1, . . . ,ip are distinct, n 1+ . . . +np = n, and n = 0,1, . . . ,N.
Proof
Using an abbreviated notation for (100) with all arguments discarded, the error
criterion (101) can be written in the form
y −yN2 = <y −
n =0Σ
N
knXΦn, y −
n =0Σ
N
knXΦn>
= <y,y > −2<y,
n =0Σ
N
knXΦn> + <
n =0Σ
N
knXΦn,
n =0Σ
N
knXΦn>
Using the easily veriﬁed result
<knXΦn,kmXΦm> = 0, n ≠m
and writing the inner products as expectations gives
y −yN2 = E [y 2] −2
n =0Σ
N
E [y (knXΦn)] +
n =0Σ
N
E [(knXΦn)2]
Now, expand the notation of the terms on the right side. First
E [y (knXΦn)] = E [y (k)
i 1=0Σ
M . . .
in=0Σ
M
kn(i 1, . . . ,in)Φn(k−i 1, . . . ,k−in,u)]
=
i 1=0Σ
M . . .
in=0Σ
M
kn(i 1, . . . ,in)E [y (k)Φn(k−i 1, . . . ,k−in,u)]
Using symmetry properties and the construction for Φn in Lemma 6.2, a general term in
E [y (knXΦn)] can be isolated as follows. Suppose i 1, . . . ,ip, 1 ≤ p ≤ n, are distinct
nonnegative integers, and n 1, . . . ,np are positive integers with n 1+ . . . +np = n. Then all
those terms containing nj occurrences of the argument ij, j = 1, . . . ,p, are identical, and
the collection of these terms can be written as
n 1! . . . np!
n !
__________ kn(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip) E [y (k)ψn 1(u (k−i 1)) . . . ψnp(u (k−ip))]
In a similar manner,
284

E [(knXΦn)2] = E [
i 1=0Σ
M . . .
in=0Σ
M
kn(i 1, . . . ,in)Φn(k−i 1, . . . ,k−in,u)
j 1=0
Σ
M . . .
jn=0Σ
M
kn(j 1, . . . , jn)Φn(k−j 1, . . . ,k−jn,u)]
=
i 1=0Σ
M . . .
in=0Σ
M
j 1=0
Σ
M . . .
jn=0Σ
M
kn(i 1, . . . ,in)kn(j 1n)
E [Φn(k−i 1, . . . ,k−in)Φn(k−j 1, . . . ,k−jn,u)]
As discussed earlier, the expected value on the right side is zero unless j 1, . . . ,jn is a
permutation of i 1, . . . ,in. Using this fact, and symmetry properties, again a general term
can be isolated. Suppose i 1, . . . ,ip and n 1, . . . ,np are just as above. Then the collection
of terms containing nj occurences of the argument ij, j = 1, . . . ,p, can be written as
[ n 1! . . . np!
n !
__________]2kn
2(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip) E [ψn 1
2 (u (k−i 1))] . . . E [ψnp
2 (u (k−ip))]
Now, the error criterion can be expressed as a sum of terms of the general types given
above, with the sum ranging over all distinct nonnegative integers i 1, . . . ,ip, p = 1, . . . ,n,
and over all distributions of these integers as given by the positive integers n 1, . . . ,np,
with n 1+ . . . +np = n. That is, in a vague summation notation,
y (k) −yN(k)2 = E [y 2(k)]
+ Σ n 1! . . . np!
(−2)n !
__________ kn(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip)
E [y (k)ψn 1(u (k −i 1)) . . . ψnp(u (k −ip))]
+ ( n 1! . . . np!
n !
__________)2kn
2(
n 1
i 1, . . . ,i 1; . . . ;
np
ip, . . . ,ip)
E [ψn 1
2 (u (k −i 1))] . . . E [ψnp
2 (u (k −ip))]
(103)
Minimization of the quadratic criterion is straightforward, and the result is easily seen to
be the Fourier kernel speciﬁed in (102).
Example 6.9
Consider again the case where u is zero-mean, white, and Gaussian with
intensity A. Using the results of Example 6.8, it is straightforward to calculate the ﬁrst few
terms in the functional Fourier series representation explicitly in terms of u rather than the
orthogonal representation of u. In terms of the abbreviated notation for (100),
285

k 0XΦ0 = k 0
k 1XΦ1 =
i 1=0Σ
M
k 1(i 1)ψ1(u (k−i 1)) =
i 1=0Σ
M
k 1(i 1) u (k−i 1)
k 2XΦ2 =
i 1=0Σ
M
i 2=0Σ
M
k 2(i 1,i 2)Φ2(k−i 1,k−i 2,u)
=
i 1=0Σ
M
i 2=0Σ
M
k 2(i 1,i 2) u (k−i 1)u (k−i 2) −A
i =0Σ
M
k 2(i,i)
(104)
and so on. These terms should begin to look familiar from the continuous-time case in
Section 5.5. Indeed, a messy general argument shows that
knXΦn =
r =0Σ
[n /2]
r !2r(n −2r)!
(−1)rn !Ar
___________
i 1=0Σ
M . . .
in −2r=0
Σ
M
j 1=0
Σ
M . . .
jr=0Σ
M
kn(i 1, . . . ,in −2r,j 1,j 1, . . . , jr,jr) u (k−i 1) . . . u (k−in −2r)
(105)
Thus, the Wiener orthogonal representation can be viewed as a particular case (of the
general functional Fourier series) that displays explicitly the system input rather than an
orthogonal representation of the input.
A natural question to ask about the functional Fourier series representation concerns
the convergence properties as more terms are added. While this question will not be
analyzed
in
detail,
assuming
certain
completeness
properties
of
the
orthogonal
polynomials ψn(x), n = 0,1, . . . , it can be shown that
N →∞
lim y (k) −yN(k)2 = 0
(106)
That is, the given system can be approximated to desired accuracy by a ﬁnite functional
Fourier series of the form
y (k) =
n =0Σ
N
knXΦn
=
n =0Σ
N
i 1=0Σ
M . . .
in=0Σ
M
kn(i 1, . . . ,in)Φn(k−i 1, . . . ,k−in,u)
(107)
Finally, the orthogonality property of the functional Fourier series offers a simple
expression for the autocorrelation function of the system output, the derivation of which is
left as an exercise.
286

6.6 Bilinear Input/Output Systems
The theory of nonlinear systems with input/output behavior that can be described by
an n-linear operator is closely related to the theory of homogeneous nonlinear systems. To
illustrate, I will consider the special case of stationary systems that can be represented by a
bilinear (2-linear) operator. (The terminology is dangerous. Systems described by bilinear
operators should not be confused with systems described by bilinear state equations.) It is
a straightforward matter to carry the discussion back to the continuous-time case, though
some differences do appear. It is less straightforward to generalize the theory to the n-
linear case simply because of the morass of algebra, and the resulting need to develop a
more abstract and subtle notation.
Consider a stationary, causal, discrete-time system that has two scalar inputs, u 1(k)
and u 2(k), and a scalar output y (k). Such a system can be represented in operator notation
by
y = F [u 1,u 2]
(108)
where F is an operator with appropriately deﬁned, real, linear function spaces for the
domain and range. The system is called a bilinear input/output system if F is a bilinear
operator; that is, if F is linear in each argument. More precisely, F is a bilinear operator if
F [α1u 1+αˆ 1uˆ 1,α2u 2+αˆ 2uˆ 2] = α1α2F [u 1,u 2] + α1αˆ 2F [u 1,u^ 2]
+ αˆ 1α2F [uˆ 1,u 2] + αˆ 1αˆ 2F [uˆ 1,uˆ 2]
(109)
for all real α1, αˆ 1, α2, αˆ 2 and all input signals u 1(k), uˆ 1(k), u 2(k), and uˆ 2(k).
The main part of the discussion here will be concerned with developing a more
explicit form for the input/output representation. A simple way to accomplish this is to
consider bilinear input/output systems that are realizable by a general class of state
equations. Then special properties of these state equations can be used to obtain properties
of the corresponding input/output representation. I will pursue this approach for bilinear
input/output systems that are realizable by state equations of the form
x (k +1) = f [x (k),u 1(k),u 2(k)] , k = 0,1,2, . . .
y (k) = h (x (k)) , x (0) = 0
(110)
where x (k) is the n-dimensional state vector, and f and h are assumed to be analytic
functions satisfying f (0,0,0) = 0 and h (0) = 0. The choice of the equilibrium initial state
at zero and the analyticity requirements can be relaxed in various ways without changing
the essential features of the results. Also, more general output equations, namely those of
the form
y (k) = h [x (k),u 1(k),u 2(k)]
can be handled by the methods to be used, although the formulas and block diagrams
become more complicated.
287

The next step is to use power series expansions of f and h in (110) to rewrite the
state equation description, and then to consider input signals of the form indicated in
(109). By writing x (k) and y (k) as expansions in terms of α1, αˆ 1, α2, and αˆ 2, and
imposing the bilinear input/output condition (109), various terms in the resulting
variational equations can be eliminated to obtain a simpler state equation. I will simplify
this procedure somewhat by imposing the input/output condition (implied by (109))
F [α1u 1,α2u 2] = α1α2F [u 1,u 2]
(111)
Removing terms in the state equation that are incompatible with (111) will yield a simple
structural form for realizations of bilinear input/output systems. (It will become more or
less apparent that further simpliﬁcation is not obtained by imposing the more complicated
condition (109), although a proof of this fact will not be given.)
Using the familiar Kronecker product notation, the state equation (110) can be
written in the form
x (k +1) = A1x (k) + A2x (k)⊗x (k) + D1x (k)u 1(k) + D2x (k)u 2(k)
+ b 1u 1(k) + b 2u 2(k) + b 3u 1(k)u 2(k) + . . .
y (k) = c 1x (k) + c 2x (k)⊗x (k) + . . .
(112)
where only the terms that enter into the subsequent development are displayed. For input
signals α1u 1(k) and α2u 2(k), assume
x (k) = α1x 1(k) + α2x 2(k) + α1α2x 3(k) + . . .
(113)
Again, only those terms in α1 and α2 are displayed that will yield an output y (k)
consistent with (111). Substituting (113) into the state equation and equating coefﬁcients
of like terms in α1, α2, and α1α2, yields the following state equation description for the
bilinear input/output system represented by (110):
x 1(k +1) = A1x 1(k) + b 1u 1(k), x 1(0) = 0
x 2(k +1) = A1x 2(k) + b 2u 2(k), x 2(0) = 0
x 3(k +1) = A1x 3(k) + A2[x 1(k)⊗x 2(k) + x 2(k)⊗x 1(k)] + D1x 2(k)u 1(k)
+ D2x 1(k)u 2(k) + b 3u 1(k)u 2(k), x 3(0) = 0
y (k) = c 1x 3(k) + c 2[x 1(k)⊗x 2(k) + x 2(k)⊗x 1(k)]
(114)
This set of equations can be put into a simpler form, although at considerable expense in
dimension, by applying the Carleman linearization idea to the equation for x 3(k). Let
xˆ 3(k) = x 1(k)⊗x 2(k) + x 2(k)⊗x 1(k)
(115)
Then a straightforward computation shows that xˆ 3(k) satisﬁes
288

xˆ 3(k +1) = A1 ⊗A1xˆ 3(k) + [A1 ⊗b 1 + b 1 ⊗A1]x 2(k)u 1(k) + [A1 ⊗b 2
+ b 2 ⊗A1]x 1(k)u 2(k) + [b 1 ⊗b 2 + b 2 ⊗b 1]u 1(k)u 2(k)
(116)
where xˆ 3(0) = 0. Now let
z 3(k) =
H
A
I xˆ 3(k)
x 3(k) J
A
K
(117)
and combine the equations for x 3(k) and xˆ 3(k) to obtain
z 3(k +1) =
H
A
I
0
A1
A1 ⊗A2
A2
J
A
K
z 3(k) +
H
A
I A1 ⊗b 1 + b 1 ⊗A1
D1
J
A
K
x 2(k)u 1(k)
+
H
A
I A1 ⊗b 2 + b 2 ⊗A1
D2
J
A
K
x 1(k)u 2(k) +
H
A
I b 1 ⊗b 2 + b 2 ⊗b 1
b 3
J
A
K
u 1(k)u 2(k)(118)
Of course, the output equation can be written in the form
y (k) = [c 1
c 2] z 3(k)
(119)
Summarizing in a simpler notation, the bilinear input/output system (110) also can be
described by a state equation of the form
x 1(k +1) = A1x 1(k) + b 1u 1(k) , x 1(0) = 0
x 2(k +1) = A1x 2(k) + b 2u 2(k) , x 2(0) = 0
x 3(k +1) = A3x 3(k) + D1x 2(k)u 1(k) + D2x 1(k)u 2(k) + b 3u 1(k)u 2(k) , x 3(k) = 0
y (k) = c x 3(k) 
(120)
Figure 6.1. Interconnection realization of a bilinear input/output system.
The simple structural form of (120) is indicated by the interconnection diagram
shown in Figure 6.1, where vector quantities are denoted by double arrows. Of course, this
interconnection realization is usually far from minimal since the dimension of the state
289
b3
Π
Π
Π
Σ
y
D2(zI – A1)–1 b1
D1(zI – A1)–1 b2
u2
u1
c(zI – A3)–1
v

reduced Kronecker product, but the result still would be far from minimal.
A concrete form for the input/output representation of a bilinear input/output system
described by (110) can be derived from the interconnection structure shown in Figure 6.1.
The derivation involves the familiar procedure of tracing the various signals through the
diagram until the output signal is reached. Clearly, for k > 0,
D2x 1(k)u 2(k) =
j 2=0
Σ
k −1
D2A1
k −j 2−1b 1u 1(j 2)u 2(k)
D1x 2(k)u 1(k) =
j 2=0
Σ
k −1
D1A1
k −j 2−1b 2u 1(k)u 2(j 2)
v (k) = D2x 1(k)u 2(k) + D1x 2(k)u 1(k) + b 3u 1(k)u 2(k)
y (k) =
j 1=0
Σ
k −1
cA3
k −j 1−1v (j 1)
(121)
Putting these equations together gives the input/output formula
y (k) =
j 1=0
Σ
k −1
j 2=0
Σ
j 1−1
cA3
k −j 1−1D2A1
j 1−j 2−1b 1u 1(j 2)u 2(j 1)
+
j 1=0
Σ
k −1
j 2=0
Σ
j 1−1
cA3
k −j 1−1D1A1
j 1−j 2−1b 2u 1(j 1)u 2(j 2)
+
j 1=0
Σ
k −1
cA3
k −j 1−1b 3u 1(j 1)u 2(j 1)
Thus, a bilinear input/output system described by (110) can be represented in the form
y (k) =
j 1=0
Σ
k −1
j 2=0
Σ
k −1
h (k −j 1,k −j 2)u 1(j 1)u 2(j 2)
(123)
where, after some rearrangement of (122), the kernel is given by
h (i 1,i 2) =
B
A
A
C
A
A
D
0 , otherwise
cA3
i 1−1b 3 , 0 < i 1 = i 2
cA3
i 2−1D2A1
i 1−i 2−1b 1 , 0 < i 2 < i 1
cA3
i 1−1D1A1
i 2−i 1−1b 2 , 0 < i 1 < i 2
(124)
In addition to representing a bilinear input/output system, it is apparent that (123)
corresponds to a causal and stationary system.
290

Using the 2-variable z-transform, a transfer function representation can be deﬁned
for bilinear input/output systems:
H (z 1,z 2) =
i 1=0Σ
∞
i 2=0Σ
∞
h (i 1,i 2)z1
−i 1z2
−i 2
(125)
The special structure for the kernel as displayed in (124) also implies a special structure
for the transfer function. Substituting (124) into (125) gives
H (z 1,z 2) =
i 1=1Σ
∞
i 2=i 1+1
Σ
∞
cA3
i 1−1D1A1
i 2−i 1−1b 2z1
−i 1z2
−i 2
+
i 2=1Σ
∞
i 1=i 2+1
Σ
∞
cA3
i 2−1D2A1
i 1−i 2−1b 1z1
−i 1z2
−i 2
+
i 1=i 2=1
Σ
∞
cA3
i 1−1b 3z1
−i 1z2
−i 2
(126)
To illustrate the remainder of the calculation, I will work out the ﬁrst term on the
right side of (126) in detail. Replacing the summation index i 2 by j 2 = i 2−i 1−1 and using
the identity
i =0Σ
∞
Aiz −i = z (zI −A)−1
(127)
allows the ﬁrst term to be rewritten as follows:
i 1=1Σ
∞
j 2=0
Σ
∞
cA3
i 1−1D1A1
j 2b 2z1
−i 1z2
−(j 2+i 1+1) =
i 1=1Σ
∞
cA3
i 1−1D1(z 2I −A1)−1b 2(z 1z 2)−i 1
= c (z 1z 2I −A3)−1D1(z 2I −A1)−1b 2
Performing this kind of calculation on the remaining two terms in (126) yields
H (z 1,z 2) = c (z 1z 2I −A3)−1D1(z 2I −A1)−1b 2
+ c (z 1z 2I −A3)−1D2(z 1I −A1)−1b 1 + c (z 1z 2I −A3)−1b 3
(128)
Thus a general form has been obtained for the transfer function of a bilinear input/output
system that can be described by a state equation of the form (110).
I will leave further discussion of the theory of bilinear input/output systems to the
literature cited in Section 6.8. It should be clear at this point that such systems can be
studied using methods similar to those developed for Volterra/Wiener representations.
Input/output calculations in the transform domain involve the association-of-variables
technique, and for certain types of input signals explicit response formulas can be derived.
The structural form of the transfer function (or kernel) can be used to describe elementary
conditions for realizability in terms of an interconnection structure. Finally, the reader
surely has noticed that by setting u 1(k) = u 2(k) = u (k), the bilinear input/output system
291

reduces to a homogeneous system of degree 2. All of this indicates the symbiotic
relationship between research in multilinear input/output systems and in homogeneous
systems.
6.7 Two-Dimensional Linear Systems
The theory of multidimensional linear systems involves representations that
resemble the Volterra/Wiener representation for nonlinear systems. Two-dimensional,
stationary, discrete-time linear systems constitute the most widely studied case, and I will
discuss the basics of this theory in order to exhibit the connections to nonlinear system
theory.
Motivation for the study of two-dimensional discrete-time systems comes
principally from the processing (or ﬁltering) of two-dimensional signals, notably in image
or array processing, and geophysics.
The basic input/output representation for a two-dimensional, stationary, discrete-
time linear system can be written in the form
y (k 1,k 2) =
i 1=0Σ
k 1
i 2=0Σ
k 2
h (k 1−i 1,k 2−i 2)u (i 1,i 2) , k 1,k 2 = 0,1,2, . . .
(129)
The input u (k 1,k 2) and output y (k 1,k 2) are real two-dimensional (or, doubly indexed)
signals that are deﬁned for integer arguments, but that are assumed to be zero if either
argument is negative. Linearity is easily veriﬁed: in the obvious notation, the response to
αu 1(k 1,k 2) + βu 2(k 1,k 2) is αy 1(k 1,k 2) + βy 2(k 1,k 2) for any scalars α and β. Stationarity
corresponds to a delay invariance property, which, in the context of (129), can be stated as
follows. If u 2(k 1,k 2) = u 1(k 1−K 1,k 2−K 2), then y 2(k 1,k 2) = y 1(k 1−K 1,k 2−K 2) for all
nonnegative integer pairs K 1, K 2. Notice that the concept of causality is not mentioned,
though something vaguely like that is built into the representation. The reader might enjoy
consulting his muse on the types of operations on an array of data u (k 1,k 2) that might be
described by (129).
Using the 2-variable z-transform, and the convolution property in Theorem 6.4,
gives the input/output representation
Y(z 1,z 2) = H (z 1,z 2)U (z 1,z 2)
(130)
where H (z 1,z 2) = Z[h (k 1,k 2)] is called the transfer function of the system. It should be
immediately obvious from earlier chapters how to use (130) to investigate the response
properties of the system for various classes of input signals.
Example 6.10
The simplest (nonzero) input signal is the unit pulse input, which is
deﬁned in the two-dimensional setting by
u 0(k 1,k 2) =
B
A
C
A
D
0 , otherwise
1 , k 1 = k 2 = 0
292

From (129), the response clearly is
y (k 1,k 2) = h (k 1,k 2) , k 1,k 2 = 0,1,2, . . .
or, since U0(z 1,z 2) = 1,
Y(z 1,z 2) = H (z 1,z 2)
Of course, in a digital ﬁltering context it is the steady-state frequency response
properties of the system that are of prime importance. It is rather easy to work out these
properties, and therefore that task is left to Section 6.9.
There are several types of state equation representations that can be adopted for the
study of two-dimensional linear systems. I will work with the general form
x (k 1+1,k 2+1) = A1x (k 1+1,k 2) + A2x (k 1,k 2+1) + B1u (k 1+1,k 2) + B2u (k 1,k 2+1)
y (k 1,k 2) = cx (k 1,k 2) , k 1,k 2 = 0,1,2, . . .
(131)
where x (k 1,k 2) is an n x 1 vector. Iterating this equation for the ﬁrst few values of k 1 and
k 2 shows that the initial conditions, more appropriately called boundary conditions,
required for solution are the values x (k 1,0), k 1 = 0,1, . . . , and x (0,k 2), k 2 = 0,1, . . . .
This multiplicity of boundary conditions indicates that x (k 1,k 2) is not a state vector for the
system in any precise sense of the term. That is, knowledge of the value of x (k 1,k 2) and
the input signal does not sufﬁce to determine the value of x (k 1+K 1,k 2+K 2). Stated in the
context of array processing, a single value of x (k 1,k 2) does not specify the "state" of the
array. Rather, the equation for x (k 1,k 2) gives the recursion necessary to specify the array
in a pointwise fashion. Thus I will call x (k 1,k 2) a local state vector for the two-
dimensional system, and call n the local dimension of the system (there goes the
terminology, again). Further consideration of the nature of a "global state" will be left to
the literature since my interest here is more mechanical than philosophical.
There are intuitive ways to arrive at a choice for the form of the local state equation
for two-dimensional systems. This intuition is based on viewing the system as an array
processor, and imagining various methods by which the values in the array might be
generated. I will go through one of these just to provide motivation for the choice in (131).
Example 6.11
Suppose the values y (k 1,k 2) in a certain array can be generated by a
combination of a horizontal recursion and a vertical recursion. Let xh(k 1,k 2) be the local
horizontal state, and xv(k 1,k 2) be the local vertical state, and suppose the local states
propagate according to
xh(k 1+1,k 2) = A1xh(k 1,k 2) + A2xv(k 1,k 2) + B1u (k 1,k 2)
xv(k 1,k 2+1) = A3xh(k 1,k 2) + A4xv(k 1,k 2) + B2u (k 1,k 2)
y (k 1,k 2) = c 1xh(k 1,k 2) + c 2xv(k 1,k 2) , k 1,k 2 = 0,1, . . .
(132)
Of course, the input signal u (k 1,k 2) must be speciﬁed, and it is clear that the boundary
293

conditions that must be speciﬁed are the values of xh(0,k 2), and xv(k 1,0) (the left-hand and
bottom edges of the array). These local state equations can be put into the form (131) by
deﬁning x (k 1,k 2) as
x (k 1,k 2) =
H
A
I xv(k 1,k 2)
xh(k 1,k 2) J
A
K
Then a straightforward calculation gives
x (k 1+1,k 2+1) =
H
A
I A3
0
A4
0 J
A
K
x (k 1+1,k 2) +
H
A
I
0
A1
0
A2 J
A
K
x (k 1,k 2+1)
+
H
A
I
0
B1 J
A
K
u (k 1,k 2+1) +
H
A
I B2
0 J
A
K
u (k 1+1,k 2)
y (k 1,k 2) = [c 1
c 2] x (k 1,k 2)
which shows that (132) can be viewed as a special case of (131).
The transfer function corresponding to the local state equation in (131) is easy to
compute using the result of Problem 6.3. For zero boundary conditions, the state equation
can be written in the transform-domain form
z 1z 2X (z 1,z 2) = A1z 1X (z 1,z 2)+A2z 2X (z 1,z 2)+B1z 1U (z 1,z 2)+B2z 2U (z 1,z 2) (133)
Solving gives
X (z 1,z 2) = (z 1z 2I −A1z 1 −A2z 2)−1(B1z 1 + B2z 2)U (z 1,z 2)
(134)
so that the input/output relationship takes the form
Y(z 1,z 2) = c (z 1z 2I −A1z 1 −A2z 2)−1(B1z 1 + B2z 2)U (z 1,z 2)
(135)
Thus the transfer function corresponding to (131) can be written in the form
H (z 1,z 2) = c (z 1z 2I −A1z 1 −A2z 2)−1(B1z 1 + B2z 2)
(136)
From the transform-domain solution of the local state equation, an "array-domain"
solution can be derived as follows. Using an identity of the form (127) permits writing the
matrix inverse in (134) in the form
(z 1z 2I −A1z 1 −A2z 2)−1 =
i =0Σ
∞
(A1z 1 + A2z 2)i(z 1z 2)−(i +1)
=
i 1=0Σ
∞
i 2=0Σ
∞
Ai 1,i 2 z1
−i 1z2
−i 2
(137)
where Ai 1,i 2 might appropriately be called the two-dimensional transition matrix.
Equating coefﬁcients of like terms in (137) shows that the ﬁrst few values of Ai 1,i 2 are
294

A0,i = Ai, 0 = 0 , i = 0,1,2, . . .
A1,1 = I , A1,2 = A1 , A1,3 = A1
2
A2,1 = A2, A3,1 = A2
2 , A2,2 = A1A2 + A2A1
(138)
Now the convolution property, Theorem 6.4 in in Section 6.2, conjunction with (137) and
(134) can be used to obtain an expression for the local state. First note that
X (z 1,z 2) = (
i 1=0Σ
∞
i 2=0Σ
∞
Ai 1,i 2z1
−i 1z2
−i 2 )(B1z 1 + B2z 2)
j 1=0
Σ
∞
j 2=0
Σ
∞
u (j 1,j 2)z1
−j 1z2
−j 2
=
i 1=0Σ
∞
i 2=0Σ
∞
j 1=0
Σ
∞
j 2=0
Σ
∞
z 1Ai 1,i 2B1u (j 1,j 2)z1
−(i 1+j 1)z2
−(i 2+j 2)
+
i 1=0Σ
∞
i 2=0Σ
∞
j 1=0
Σ
∞
j 2=0
Σ
∞
z 2Ai 1,i 2B2u (j 1,j 2)z1
−(i 1+j 1)z2
−(i 2+j 2)
(139)
Replacing j 1 by k 1 = j 1+i 1 and j 2 by k 2 = j 2+i 2, and making use of the "one-sidedness"
of the input signal gives
X (z 1,z 2) =
k 1=0
Σ
∞
k 2=0
Σ
∞
i 1=0Σ
k 1
i 2=0Σ
k 2
z 1Ai 1,i 2B1u (k 1−i 1,k 2−i 2)z1
−k 1z2
−k 2
+
k 1=0
Σ
∞
k 2=0
Σ
∞
i 1=0Σ
k 1
i 2=0Σ
k 2
z 2Ai 1,i 2B2u (k 1−i 1,k 2−i 2)z1
−k 1z2
−k 2
(140)
It follows that the solution of the local state equation (131) is given by
x (k 1,k 2) =
i 1=0Σ
k 1
i 2=0Σ
k 2
(Ai 1+1,i 2B1 + Ai 1,i 2+1B2)u (k 1−i 1,k 2−i 2)
(141)
There are a number of structural features of local state equations of the form (131)
that are similar to familiar properties in one-dimensional linear system theory.
To
illustrate, I will brieﬂy discuss reachability and observability concepts for local state
equations.
Deﬁnition 6.3
A state x 1 of the local state equation (131) is called a reachable state
(from zero boundary conditions) if there exists an input signal such that for some
K 1,K 2 < ∞, x (K 1,K 2) = x 1. The local state equation is called reachable if every state is
reachable.
From (141) it is clear that a state x 1 is a reachable state if and only if
x 1 ε span { (Ai 1+1,i 2B1 + Ai 1,i 2+1B2) i 1,i 2 = 0,1, . . . }
(142)
Before restating this condition in the traditional form of a rank condition for reachability
295

of the state equation, it is necessary to establish the following result (which is related to a
two-dimensional version of the Cayley-Hamilton Theorem).
Lemma 6.3
For the two-dimensional state transition matrix deﬁned in (137),
span { A i 1,i 2 i 1,i 2 = 0,1, . . . } = span { A i 1,i 2 i 1,i 2 = 0,1, . . . ,n}
(143)
Proof
Expressing the matrix inverse in (137) in the classical adjoint-over-determinant
form gives
adj (z 1z 2I −A1z 1 −A2z 2) = det (z 1z 2I −A1z 1 −A2z 2)
i 1=0Σ
∞
i 2=0Σ
∞
Ai 1,i 2 z1
−i 1z2
−i 2
On the left side of this expression there are no terms with nonpositive powers of z 1 or z 2.
On the right side, det (z 1z 2I − A1z 1 − A2z 2) is a polynomial of degree n in z 1 and degree n
in z 2, while the nonzero terms in the double summation can occur only for i 1,i 2 ≥ 1. Thus
equating coefﬁcients of like terms of the form z1
−i 1z2
−i 2, i 1,i 2 ≥ 0, shows that when i 1 > n or
i 2 > n there is a nontrivial linear combination of the matrices Ai 1,i 2 that is zero. Clearly
this conclusion implies (143).
Theorem 6.10
The local state equation (131) is reachable if and only if the matrix
[B1B2A1,1B1A1,1B2A2,1B1+A1,2B2 . . . An +1,nB1+An,n +1B2]
(144)
has (full) rank n.
Proof
Although sparsely denoted, the matrix in (144) contains as columns all n x 1
vectors of the form (Ai 1+1,i 2B1 + Ai 1,i 2+1B2) with i 1 ≤ n, i 2 ≤ n. Thus, the result is an easy
consequence of Lemma 6.3 and the condition for state reachability in (142).
The appropriate deﬁnition of observability for the local state equation (131) is based
on the nonexistence of boundary conditions that at the output are indistinguishable from
the zero boundary conditions.
Deﬁnition 6.4
The local state equation (131) is called observable if there is no set of
nonzero boundary conditions such that with identically zero input, the output is identically
zero.
The development of conditions to characterize observability can be based on an
analysis of the response of (131) to zero inputs and nonzero boundary conditions. Such an
analysis followed by an application of Lemma 6.3 leads to Theorem 6.11, the proof of
which is left to the reader.
Theorem 6.11
The local state equation (131) is observable if and only if the matrix
296

H
A
A
A
A
I cA n,n
...
cA 2,1
cA 1,2
c
J
A
A
A
A
K
(145)
has (full) rank n.
At the time of this writing, a realization theory for two-dimensional linear systems in
terms of local state equations of the form (131) has not been completely worked out. It is
clear from (136) that proper rationality of a given transfer function H (z 1,z 2) is a necessary
condition for realizability. Further inspection reveals that another necessary condition is
that both the numerator and denominator polynomials of H (z 1,z 2) must be zero when
z 1 = z 2 = 0. In other words, these polynomials must not have nonzero constant terms.
These necessary conditions also are sufﬁcient, and a proof can be given by constructing a
realization for a general transfer function that satisﬁes the conditions. To write out such a
realization would be tiresome, so I will indicate vaguely what one looks like with an
example and leave the general form to the literature.
Example 6.12
Consider the two-dimensional linear system described by
H (z 1,z 2) = z 1z 2 + a 10z 1 + a 01z 2
b 10z 1 + b 01z 2
___________________
A simple calculation shows that a realization for this system is
x (k 1+1,k 2+1) =
H
A
I −a 01
0
−a 10
0
J
A
K
x (k 1+1,k 2) +
H
A
I
0
−a 01
0
−a 10 J
A
K
x(k 1,k 2+1)
+ H
I 1
0 u (k 1+1,k 2) + 0
1 J
K u (k 1,k 2+1)
y (k 1,k 2) = [b 01
b 10] x (k 1,k 2)
where all the initial conditions are zero.
Of course, the construction of minimal-dimension realizations for two-dimensional
linear systems is of great interest, and much remains to be done in this area. In the one-
dimensional case, the concepts of reachability and observability are useful tools in
developing a theory of minimal realizations. However, the following example shows that
in the two-dimensional case the situation is more complicated, and that perhaps the
reachability and observability deﬁnitions discussed earlier are not the best choices.
Example 6.13
For the transfer function
H (z 1,z 2) = z 1z 2 + z 1 + z 2
z 1 −z 2
_____________
297

the realization given in Example 6.12 becomes
x (k 1+1,k 2+1) = H
I −1
0
−1
0 J
K x (k 1+1,k 2) + H
I 0
−1
0
−1 J
K x (k 1,k 2+1)
+ H
I 1
0 J
K u (k 1+1,k 2) + H
I 0
1 J
K u (k 1,k 2+1)
y (k 1,k 2) = [1
−1] x (k 1,k 2)
A quick calculation shows that this local state equation is both reachable and observable.
But it is not minimal since another realization is given by
x (k 1+1,k 2+1) = −x (k 1+1,k 2) −x (k 1,k 2+1) + u (k 1+1,k 2) −u (k 1,k 2+1)
y (k 1,k 2) = x (k 1,k 2)
Finally, it is easy to show that a bilinear input/output system (or for that matter, a
degree-2 homogeneous system) can be modeled using a two-dimensional linear system.
This involves nothing more than comparing the transform-domain input/output equations
for the two classes of systems. Such a comparison shows that a bilinear input/output
system with transfer function H (z 1,z 2) can be viewed as follows. From the input signals
u 1(k) and u 2(k), form an array u (k 1,k 2) = u 1(k 1)u 2(k 2). Process this array with the two-
dimensional linear system with transfer function H (z 1,z 2) to obtain the array y (k 1,k 2).
Then set y (k) = y (k,k), that is, let y (k) be the diagonal of the array. Schematically this
implementation is shown in Figure 6.2.
 
Figure 6.2. Implementation of a bilinear input/output system
using a two-dimensional linear system.
6.8 Remarks and References
Remark 6.1
Early works dealing with Volterra series representations for discrete-time
systems include
P. Alper, "A Consideration of the Discrete Volterra Series," IEEE Transactions on
Automatic Control, Vol. AC-10, pp. 322-327, 1965.
A. Bush, "Some Techniques for the Synthesis of Nonlinear Systems," MIT RLE Technical
Report No. 441, 1966 (AD 634-122).
298
array
u1(k)
u2(k)
y(k)
former
diagonal
readout
u1(k1)u2(k2)
y(k1,k2)
H(z1,z2)

H. Barker, S. Ambati, "Nonlinear Sampled-Data System Analysis by Multidimensional Z-
Transforms," Proceedings of the IEE, Vol. 119, pp. 1407-1413, 1972.
All of these papers discuss the basic time-domain and transform-domain representations
for discrete-time systems.
Remark 6.2
The development of state-afﬁne realization theory in Section 6.4 is drawn
from
S. Clancy, W. Rugh, "On the Realization Problem for Stationary Homogeneous Discrete-
Time Systems," Automatica, Vol. 14, pp. 357-366, 1978.
S. Clancy, W. Rugh, "The Regular Transfer Function and Bilinear and State-Afﬁne
Realizations for Stationary, Homogeneous, Discrete-Time Systems," Proceedings of the
1978
Conference
on
Information
Sciences
and
Systems,
Electrical
Engineering
Department, The Johns Hopkins University, Baltimore, pp. 167-172, 1978.
A. Frazho, "Shift Operators and State-Afﬁne Realization Theory," Proceedings of the 19th
IEEE Conference on Decision and Control, Albuquerque, New Mexico, pp. 904-909,
1980.
The ﬁrst of these papers contains a further discussion of the division-deletion method
given in Example 6.3 for computing Hreg from Hsym. Another approach to the state-afﬁne
realization question is given in
E. Sontag, "Realization Theory of Discrete-Time Nonlinear Systems: I. The Bounded
Case," IEEE Transactions on Circuits and Systems, Vol. CAS-26, pp. 342-356, 1979.
It should be noted that the papers by Frazho and Sontag cover much more general systems
than those discussed in Section 6.4. Also, the noncommutative series approach (Remark
4.3) can be used in the discrete-time case. See
M. Fliess, "Un Codage Non Commutatif pour Certains Systemes Echantillonnes Non
Lineaires," Information and Control, Vol. 38, pp. 264-287, 1978.
Remark 6.3
Calculations of the output mean, auto- and cross-correlations, and spectral
densities for a discrete-time system with white noise input are given in
G. Cariolaro, G. Di Masi, "Second-Order Analysis of the Output of a Discrete-Time
Volterra System Driven by White Noise," IEEE Transactions on Information Theory, Vol.
IT-26, pp. 175-184, 1980.
The treatment of orthogonal representations in Section 6.5 is based on
299

S. Yasui, "Stochastic Functional Fourier Series, Volterra Series, and Nonlinear System
Analysis," IEEE Transactions on Automatic Control, Vol. AC-24, pp. 230-242, 1979.
This paper treats a number of additional topics, a few of which will be discussed in
Chapter 7. A detailed discussion of the properties of Hermite polynomials with regard to
nonlinear system theory can be found in
M. Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, John Wiley, New
York, 1980.
Remark 6.4
The theory of multilinear input/output systems, in particular bilinear
input/output systems, was spurred by
R. Kalman, "Pattern Recognition Properties of Multilinear Machines," IFAC International
Symposium on Technical and Biological Problems of Control, Yeravan, USSR, 1968 (AD
731-304).
The basic interconnection structure representation for such a system was presented in
M. Arbib, "A Characterization of Multilinear Systems," IEEE Transactions on Automatic
Control, Vol. AC-14, pp. 699-702, 1969.
Both of these papers use modern algebraic representations and the Nerode equivalence
concept in essential ways. More recent works that continue the development from abstract
viewpoints include
E. Fornasini, G. Marchesini, "Algebraic Realization Theory of Bilinear Discrete-Time
Input/Output Maps," Journal of The Franklin Institute, Vol. 301, pp. 143-161, 1976.
B. Anderson, M. Arbib, E. Manes, "Foundations of System Theory: Multidecomposable
Systems," Journal of The Franklin Institute, Vol. 301, pp. 497-508, 1976.
The methods I have used to introduce the theory of bilinear input/output systems follow
more closely those in
E. Gilbert, "Bilinear and 2-Power Input-Output Maps: Finite Dimensional Realizations and
the Role of Functional Series," IEEE Transactions on Automatic Control, Vol. AC-23, pp.
418-425, 1978.
Further developments regarding the structure shown in Figure 6.1 can be found in
J. Pearlman, "Canonical Forms for Bilinear Input/Output Maps," IEEE Transactions on
Automatic Control, Vol. AC-23, pp. 595-602, 1978.
300

and a discussion of the difﬁculties involved in the minimal realization problem for
multilinear input/output systems is given in
J. Pearlman, "Realizability of Multilinear Input/Output Maps," International Journal of
Control, Vol. 32, pp. 271-283, 1980.
Remark 6.5
There has been a rapid growth of interest in the theory of multidimensional
linear systems since the early 1970s. Several aspects of this theory are discussed in the
special issue on multidimensional systems of the Proceedings of the IEEE, Vol. 65, 1977.
The particular local state equation representation I have considered was introduced in
E. Fornasini, G. Marchesini, "Doubly Indexed Dynamical Systems: State-Space Models
and Structural Properties," Mathematical Systems Theory, Vol. 12, pp. 59-72, 1978.
The general form for the realization of a given transfer function given there can be adapted
to the setting in Section 6.7. The local state equation discussed in Example 6.9 is
introduced in
R. Roesser, "A Discrete State Space Model for Linear Image Processing," IEEE
Transactions on Automatic Control, Vol. AC-20, pp. 1-10, 1975.
The relation between bilinear input/output systems and linear two-dimensional systems
given in Section 6.7 is exploited to study stability and realization properties of bilinear
input/output systems in
E. Kamen, "On the Relationship between Bilinear Maps and Linear Two-Dimensional
Maps," Nonlinear Analysis, Vol. 3, pp. 467-481, 1979.
6.9 Problems
6.1.
Suppose two homogeneous, discrete-time, nonlinear systems are connected in
cascade. Derive an expression for a kernel for the overall system in terms of the subsystem
kernels.
6.2. If Z[f (k)] = F (z), ﬁnd the 2-variable z transform Z[f (k 1+k 2)].
6.3. If
Z[f (k 1, . . . ,kn)] = F (z 1, . . . ,zn)
show that
Z[f (k 1+1,k 2, . . . ,kn)] = z 1F (z 1, . . . ,zn) −f (0,k 2, . . . ,kn)
301

6.4. Using a block-form bilinear realization, show that the response of a degree-n (greater
than 1), homogeneous, discrete-time system to an input of the form
u (k) = δ0(k) + a 1δ0(k −K 1) + . . . + an −2δ0(k −Kn −2)
is identically zero.
6.5. Compute the symmetric transfer function of the state-afﬁne realization in Example 6.6
using a discrete-time version of the growing exponential method.
6.6. Write the state-afﬁne state equation (56) in the form
x (k +1) = A0x (k) + A [u (k)]x (k) + b [u (k)]
y (k) = c [u (k)]x (k) + d [u (k)]
with the obvious deﬁnitions of the functions A (u), b (u), c (u), and d (u). Then show the
input/output relationship for the system can be written in the form
y (k) =
n =1Σ
N
i 1=0Σ
k . . .
in=0Σ
k
c [u (k)]A0
in−1A [u (k −in)]A0
in −1−1A [u (k −in −1−in)]
. . . A0
i 2−1A [u (k −i 2−. . . −in)]A0
i 1−1b [u (k −i 1−. . . −in)]
6.7.
Suppose a discrete-time, homogeneous system is described
by the proper,
recognizable regular transfer function Hreg(z 1, . . . ,zn). Derive a formula for the steady-
state response of the system to the input u (k) = 2Acos(ωk).
6.8. Suppose u (k) is a real random signal deﬁned on k = 0,1, . . . ,K. Discuss the
possibilities of using the system diagramed below as an autocorrelation computer.
6.9. Complete the details in the proof of Theorem 6.9.
6.10. For a system described by (107), derive an expression for the output autocorrelation
Ryy(j).
6.11. Verify the realization in Example 6.12.
302
u
y
Π
—
zm
1
___
z – 1
Kz

CHAPTER 7
IDENTIFICATION
The term identiﬁcation will be used in a very broad sense to mean the obtaining of
information about the kernels or transfer functions in a Volterra/Wiener representation of
an unknown system from input/output experiments. This information usually will be in the
form of values of the kernels or transfer functions for particular numerical values of the
arguments. However, I also will discuss some simple parameter identiﬁcation problems
that arise when a particular structure is assumed for the unknown system, or when an
expansion of the kernels in terms of known functions is assumed. It will become clear in
the course of the discussion that much remains to be done.
Stationary polynomial systems will be considered, and usually the input/output
experiments will involve the application of input signals of one of the types considered in
Chapter 5 or Section 6.5. As a matter of convenience, sometimes the discussion will be in
terms of continuous-time systems, and sometimes in terms of discrete-time systems.
7.1 Introduction
The determination of kernel values for an unknown system from a general
input/output experiment is a linear problem. This is most easily demonstrated for the case
of a discrete-time, polynomial system where, for technical simplicity, it is assumed that the
system has ﬁnite memory M, and that the degree-0 term is zero. Assuming one-sided input
signals, such a system can be described by the triangular kernel representation
y (k) =
n =1Σ
N
i 1=0Σ
M
i 2=0Σ
i 1 . . .
in=0Σ
in −1
hntri(i 1, . . . ,in)u (k −i 1) . . . u (k −in)
(1)
Now suppose that for the input-signal values u (0), . . . ,u (K), the corresponding output-
signal values y (0), . . . ,y (K) are known. Then it is straightforward from (1) to write a
linear matrix equation in terms of the unknown kernel values:
Y = HU
(2)
where
303

Y = [y (0) . . . y (K)]
H = [h 1(0) h 1(1) . . . h 1(M) h 2tri(0,0) h 2tri(1,0) h 2tri(1,1) . . . hNtri(M, . . . ,M)]
and
U =
H
A
A
A
A
A
A
A
A
A
A
A
I
...
0
u N(0)
0
0
0
u 2(0)
0
0
u (0)
...
0
u 2(0)
u (0)u (1)
u 2(1)
0
u (0)
u (1)
. . .
. . .
. . .
. . . J
A
A
A
A
A
A
A
A
A
A
A
K
(3)
Now it is clear that if K is such that U is a square matrix, and if U is invertible, then the
kernel values are given by H = YU−1. If K is larger or smaller than this value, or if U is not
invertible, then least-squares techniques such as pseudo-inversion can be used to obtain
approximations to the kernel values.
While this development indicates the nature of the kernel-determination problem, it
should be clear that the dimensions involved are very large in most cases of interest. For
example, there are on the order of (M +1)n values of a degree n kernel with memory M. As
a result, the solution of the linear equation Y = HU can be quite difﬁcult. These
considerations lead naturally to the introduction of approximation techniques involving
expansions of the kernels in terms of known functions.
Suppose it is assumed that each of the triangular kernels in (1) can be represented as
a linear combination of products of known functions φ0(k), φ1(k), . . . ,φJ(k). In particular,
it is assumed that
hntri(k 1, . . . ,kn) =
j 1=0
Σ
J
j 2=0
Σ
J
. . .
jn=0Σ
J
αj 1 . . . jnφj 1(k 1) . . . φjn(kn)
(4)
Then (1) can be rewritten in the form
y (k) =
n =1Σ
N
j 1=0
Σ
J
. . .
jn=0Σ
J
αj 1 . . . jn
i 1=0Σ
M . . .
in=0Σ
in −1
φj 1(i 1) . . . φjn(in)u (k −i 1) . . . u (k −in)
(5)
or, in a simpler notation,
y (k) =
n =1Σ
N
j 1=0
Σ
J
. . .
jn=0Σ
J
αj 1 . . . jn Φj 1 . . . jn(k)
(6)
with the obvious deﬁnition of Φj 1 . . . jn(k). For a known input signal u (0),u (1), . . . ,u (K)
and known corresponding response y (0),y (1), . . . ,y (K), the Φj 1 . . . jn(k) are known, and
304

(6) yields a set of linear equations for the unknown coefﬁcients αj 1 . . . jn. If J is small, then
the dimension of the system of equations is much smaller than the dimension of (2). That
is, there can be many fewer expansion coefﬁcients than kernel values.
Further investigation of the details of these general approaches will be left to the
reader. For the remainder of the chapter, I will be concerned with identiﬁcation methods
based on particular types of input signals.
7.2 Identiﬁcation Using Impulse Inputs
Continuous-time linear system identiﬁcation based on the impulse response is
widely discussed, even on occasion used, and so it seems necessary to discuss the
corresponding situation for the nonlinear case. The reader should be forewarned, however,
that the theoretical discussion has only limited potential for application.
Suppose a
degree-n homogeneous system is described by
y (t) =
0∫
t
hnsym(t −σ1, . . . ,t −σn)u (σ1) . . . u (σn) dσ1 . . . dσn
(7)
Then from Section 5.1, the response to u 0(t) = δ0(t) is y 0(t) = hnsym(t, . . . ,t).
The
response to
up −1(t) = δ0(t) + δ0(t −T1) + . . . + δ0(t −Tp −1)
(8)
for p = 2,3, . . . ,n, where T1, . . . ,Tp −1 are distinct positive numbers, is
yp(t) =
mΣ m1! . . . mp!
n !
___________ hnsym(
m1
t, . . . ,t; . . . ;
mp
t −Tp −1, . . . ,t −Tp −1)
(9)
where
mΣ is a p-fold sum over all integers m1, . . . ,mp such that 0 ≤ mi ≤ n, and
m1+ . . . +mp = n.
Based on these response formulas, an identiﬁcation strategy for
homogeneous systems is easy to explain for the degree-2 case.
For a degree-2 system, the responses to u 0(t) and u 1(t) are, respectively,
y 0(t) = h 2sym(t,t)
y 1(t) = h 2sym(t,t) + 2h 2sym(t,t −T1) + h 2sym(t −T1,t −T1)
(10)
Thus, the values of the symmetric kernel at equal arguments are given directly by values
of y 0(t). To determine the value of the symmetric kernel at any two distinct arguments,
say t 1 and t 2, with t 1 > t 2, simply take T1 = t 1 − t 2 for then (10) easily gives
h 2sym(t 1,t 2) = 2
1__[y 1(t 1) −y 0(t 1) −y 0(t 2)]
(11)
This kind of analysis can be generalized to degree-n homogeneous systems. That is,
values of the system kernel at particular arguments can be found by properly combining
the set of system responses to a set of input signals of the form u 0(t), . . . ,un −1(t), as given
305

in (8). The precise details of the general calculation are messy, and so I will leave them to
the motivated reader and the literature.
The kind of calculation just covered can also be used in the polynomial system case.
Again the degree-2 case will illustrate the development. Consider a system described by
y (t) =
0∫
t
h 1(t −σ)u (σ) dσ +
0∫
t
h 2sym(t −σ1,t −σ2)u (σ1)u (σ2) dσ1dσ2
(12)
The responses to u 0(t) and u 1(t) from (8) are listed below:
y 0(t) = h 1(t) + h 2sym(t,t)
y 1(t) = h 1(t) + h 1(t −T1) + h 2sym(t,t) + 2h 2sym(t,t −T1) + h 2sym(t −T1,t −T1)
(13)
Now, to show how to determine the value of the degree-2 kernel h 2sym(t 1,t 2) for speciﬁed
t 1 > t 2, I can proceed just as in the degree-2 homogeneous case. Setting T1 = t 1 − t 2, an
easy calculation gives
h 2sym(t 1,t 2) = 2
1__[y 1(t 1) −y 0(t 1) −y 0(t 2)]
(14)
But what about the degree-1 kernel? It is clear from (13) that values of this kernel
must be separated from values of the degree-2 kernel at equal arguments. The issue of
interpolation arises here, and one approach is to notice that 2u 0(t) yields the response
y 2(t) = 2h 1(t) + 4h 2sym(t,t)
(15)
Then y 0(t) and y 2(t) yield a set of equations that can be written in vector form
H
A
I y 2(t)
y 0(t) J
A
K
=
H
A
I 2
1
4
1 J
A
K
H
A
I h 2sym(t,t)
h 1(t)
J
A
K
(16)
Solving yields
h 1(t) = 2y 0(t) −2
1__y 2(t)
h 2sym(t,t) = −y 0(t) + 2
1__y 2(t)
(17)
Thus, these types of kernel values can be obtained at any value of t ≥ 0.
For higher-degree polynomial systems, this analysis can be continued. But the
details become increasingly fussy, and the interpolation idea involving impulses of various
weights becomes increasingly barren from a feasibility viewpoint. Thus I drop the subject
here, although similar ideas will arise in conjunction with less drastic input signals.
The question of how these symmetric-kernel evaluations might be used depends
very much on the situation at hand. In most of the applications to date, sufﬁcient values
have been obtained to make plots of the kernel, and these have been analyzed to determine
characteristics of the system. Little can be said in general since the analysis depends so
306

much on the physical system being modeled.
From a general viewpoint, the ability to determine a mathematical model of a system
from kernel values depends critically on the assumptions that are made about the
(unknown) system. For example, a functional form for the kernels might be assumed, in
which case the determination of the parameters in the functional form is another step in the
system identiﬁcation process. This kind of assumption can be conveniently implemented
by assuming an interconnection structure for the unknown system, or by assuming the
system can be described by a particular type of state-equation realization. Since little can
be said about the general case at present, I will be content with a simple example which,
incidently, indicates that the symmetric kernel is not always the most convenient choice of
representation.
Example 7.1
Suppose it is known that a system can be described by a differential
equation of the form
y..(t) + a 1y.(t) + a 0y (t) = b 0u (t) + d 0y (t)u (t)
or, equivalently, the bilinear state equation (A,D,b,c,R2):
x.(t) =
H
A
I −a 0
0
−a 1
1 J
A
K
x (t) +
H
A
I d 0
0
0
0 J
A
K
x (t)u (t) +
H
A
I b 0
0 J
A
K
u (t)
y (t) = [ 1
0 ] x (t)
To avoid trivial cases, assume b 0,d 0 ≠ 0. The results of Problem 5.1, in conjunction with
the general form
hnreg(t 1, . . . ,tn) = ce AtnDe Atn −1D . . . e At 1b , n = 1,2, . . .
give the unit-impulse response of the system in the form
y 0(t) =
n =1Σ
∞
hnreg(0, . . . ,0,t) = h 1(t) = ce Atb
where the facts that D2 = 0 and Db = 0 have been used. Now, it can be assumed from
linear system theory that c, A, and B, equivalently, a 0, a 1, and b 0 can be calculated from
this unit-impulse response. To determine D, that is, d 0, the response of the system to
δ0(t) + δ0(t −T), T > 0 will be used. This response can be written in the form
y 1(t) =
n =1Σ
∞
[hnreg(0, . . . ,0,t) + hnreg(0, . . . ,0,T,t −T)
+ hnreg(0, . . . ,0,T, 0,t −T) + . . . + hnreg(0, . . . ,0,t −T)]
= ce Atb + ce A(t −T)bδ−1(t −T) + ce A(t −T)De ATbδ−1(t −T)
It is left as an easy exercise to show that since c, A, and b are known, d 0 can be computed
307

from the value of y 1(t) for any t > T.
7.3 Identiﬁcation Based on Steady-State Frequency Response
The steady-state response of homogeneous and polynomial systems to sinusoidal
inputs provides the basis for another approach to the identiﬁcation problem. The ideas are
similar to well known linear-system frequency response methods for ﬁnding values of the
transfer function. Speciﬁcally, suppose a stable linear system is described by the transfer
function H (s). Then, following the review in Section 5.2, the (complex) value of H (iω)
for ﬁxed, real ω can be determined by measuring the magnitude and phase of the steady-
state response to u (t) = 2Acos(ωt). Actually, two evaluations are determined since
H (−iω) is given by the complex conjugate of the measured complex number H (iω).
Again I will begin the discussion of nonlinear systems by considering a degree-2
homogeneous system described in terms of the symmetric transfer function. From Section
5.2, the steady-state response to u (t) = 2Acos(ωt) is
yss(t) = 2A2H2sym(iω,−iω) + 2A2 |H2sym(iω,iω)|cos[2ωt + ∠H2sym(iω,iω)]
(18)
Thus, the values of H2sym(iω,−iω) and H2sym(iω,iω) can be determined. But this does not
provide enough information in general to uniquely determine the system transfer function.
Example 7.2
Consider the degree-2 systems shown in Figure 7.1. Either by computing
the symmetric transfer functions and substituting into (18), or by tracing the input
2Acos(ωt) through the systems, it can be shown that the steady-state responses to single-
tone inputs are identical. Also it can be veriﬁed that the responses to other types of inputs
are not identical, although this should be clear. The calculations are as boring as the result
is unfortunate, and thus I omit the details.
308
Figure 7.1. The systems considered in Example 7.2.
u
y
Π
__________
(s+1)(s+2)
s–1
1
____
(s+1)3
u
y
Π
__________
(s+1)2(s+2)
1
(s+1)2
____
s–1

One way to circumvent this situation is to use a more complicated input signal. For
example, consider the response of a degree-2 system to a two-tone input:
u (t) = 2A1cos(ω1t) + 2A2cos(ω2t)
(19)
From Example 5.5, the steady-state response in terms of the symmetric transfer function is
given by
yss(t) = 2A1
2H2sym(iω1,−iω1) + 2A2
2H2sym(iω2,−iω2)
+ 4A1A2 |H2sym(−iω1,iω2)|cos[(ω2−ω1)t + ∠H2sym(−iω1,iω2)]
+ 4A1A2 |H2sym(iω1,iω2)|cos[(ω1+ω2)t + ∠H2sym(iω1,iω2)]
+ 2A1
2 |H2sym(iω1,iω1)|cos[2ω1t + ∠H2sym(iω1,iω1)]
+ 2A2
2 |H2sym(iω2,iω2)|cos[2ω2t + ∠H2sym(iω2,iω2)]
(20)
Now suppose that ω1 and ω2 are such that the frequency ω1 + ω2 is distinct from the other
frequencies appearing in (20). Then amplitude and phase measurement of this component
of the steady-state frequency response will give the (complex) value H2sym(iω1,iω2).
Postponing the discussion of what to do with this value, it should be clear how to
proceed for higher-degree homogeneous systems. To outline the degree-3 case, consider
the three-tone input
u (t) = 2A1cos(ω1t) + 2A2cos(ω2t) + 2A3cos(ω3t)
(21)
Specializing (46) of Section 5.3 to n = 3, L = M = N = 1, the coefﬁcient of e i (ω1+ω2+ω3)t is
3!A1A2A3H3sym(iω1,iω2,iω3)
and the coefﬁcient of e −i (ω1+ω2+ω3)t is
3!A1A2A3H3sym(−iω1,−iω2,−iω3)
These give the real frequency term
3!2A1A2A3 |H3sym(iω1,iω2,iω3)|cos[(ω1+ω2+ω3)t + ∠H3(iω1,iω2,iω3)]
(22)
If the frequencies ω1, ω2, and ω3 are incommensurable, this frequency term will be
distinct, and thus the amplitude and phase can be measured to obtain the value
H3sym(iω1,iω2,iω3). This result extends directly to the degree-n case, where the response
to an n-tone input can be used to determine the value of H (iω1, . . . ,iωn).
Finding these transfer function evaluations in the polynomial system case is greatly
complicated by the fact that higher-degree homogeneous subsystems contribute steady-
state response terms at the same frequencies as the lower-degree subsystems. As a simple
example, suppose a polynomial system is composed of just degree-1 and degree-3
homogeneous subsystems.
If the input 2Acos(ωt) is applied, then the steady-state
response is
309

yss(t) = 2A |H1(iω)|cos[ωt + ∠H1(iω)]
+ 2A3 |H3sym(iω,iω,−iω)|cos[ωt + ∠H3sym(iω,iω,−iω)]
+ 2A3 |H3sym(iω,iω,iω)|cos[3ωt + ∠H3sym(iω,iω,iω)]
(23)
Of course, the two terms at frequency ω can be combined into one term using standard
identities. But the point is that the degree-3 homogeneous subsystem contributes to the
frequency components needed to determine H1(iω).
It is instructive to pursue this example a little further. The response of the system to
the input
u (t) = 2A1cos(ω1t) + 2A2cos(ω2t) + 2A3cos(ω3t)
(24)
will contain a term at the frequency ω1+ω2+ω3. Furthermore, this component will be
distinct if the three input frequencies are incommensurable. This indicates that values of
the degree-3 subsystem transfer function H3sym(iω1,iω2,iω3) can be determined just as
before. However, the reader can easily verify that the difﬁculty in determining values of
H1(iω) remains. For instance, H3sym(iω1,iω2,−iω2), H3sym(iω1,iω3,−iω3), and H1(iω1)
all contribute to the frequency-ω1 term in the steady-state response.
This situation brings up the problem of determining a symmetric transfer function
from its evaluations. It is to be expected that certain assumptions will be needed on the
structure of the transfer function, although just what these should be is unclear. In the
linear case, it usually is assumed that the transfer function H (s) is a strictly proper rational
function, and sometimes H (s) is assumed to be of known degree n. Then there are many
methods for determining the transfer function from a set of evaluations of the form H (iω).
This approach is unrealistic when it is assumed that n is known, although it provides a
simple starting point for further study. Unfortunately, such a general starting point is
unavailable at present in the nonlinear case. Thus, I will abandon the general situation and
illustrate one approach with a simple class of polynomial systems. Suitably severe
restrictions will be imposed on the form of the homogeneous-subsystem transfer functions
so that they can be easily determined from evaluations of the type that arise from
frequency-response measurements.
Suppose an unknown nonlinear system is known to have the interconnection
structure shown in Figure 7.2,
Figure 7.2. A cascade interconnection structure.
 
where it is assumed that the linear subsystems are stable. Furthermore, since constant
multipliers can be distributed throughout the cascade in any number of ways, it is assumed
that G1(0) = G2(0) = 1. The interconnection structure is equivalent to assuming that the
310
u
y
G1(s)
aN(.)N + aN–1(.)N–1 + … + a1(.)
G2(s)

symmetric transfer functions for the system have the form
Hnsym(s 1, . . . ,sn) = anG1(s 1) . . . G1(sn)G2(s 1+ . . . +sn) , n = 1,2, . . . ,N
(25)
I hardly need repeat that this structural assumption is quite severe. However, it will
permit the determination of the subsystem transfer functions, at least in principle, from
simple measurements of the steady-state frequency response. In fact, only single-tone
inputs will be required, regardless of the value of N.
The results of Section 5.2 can be applied to easily calculate the steady-state
frequency response of a system of the form shown in Figure 7.2. For an input
u (t) = 2Acos(ωt)
the steady-state response can be written in the form
yss(t) = f 0(A,iω) + 2
n =1Σ
N
| fn(A,iω)|cos[nωt + ∠fn(A,iω)]
(26)
where
f 0(A,iω) =
k =1Σ
[N/2] B
D k
2kE
G A2ka 2kG1
k (iω)G1
k (−iω)
fn(A,iω) =
k =0Σ
[(N−n)/2] B
D n +k
n +2kE
G An +2kan +2kG1
n +k(iω)G1
k (−iω)G2(inω),
(27)
n = 1,2, . . . ,N
where [x ] indicates the greatest integer ≤ x.
There are several approaches that can be used to determine the linear-subsystem
transfer functions and the coefﬁcients in the polynomial nonlinearity. I will discuss a very
simple method that requires only single-tone inputs (including constant inputs), and that
does not require the measurement of relative phase. However, for reasons that will
become clear shortly, it must be assumed that G1(s) and G2(s) are minimum-phase
transfer functions.
The ﬁrst step is to determine the coefﬁcients a 1,a 2, . . . ,aN by measuring the
steady-state response to step-function inputs at various amplitudes. The steady-state
response of the system to u (t) = Aδ−1(t) is
yss(t) = a 1A + a 2A2 + . . . + aNAN
Therefore, measuring the (constant) value of yss(t) for N different input amplitudes gives
the coefﬁcient values by polynomial interpolation.
The determination of the linear-subsystem transfer functions G1(s) and G2(s) will
be accomplished from amplitude measurements on the fundamental frequency component
of the steady-state response to inputs of the form u (t) = 2Acos(ωt). In other words,
measurements of | f 1(A,iω)| for various values of A and ω will be used. For deﬁniteness it
is assumed that N is odd so that f 1(A,iω) can be written out in the form
311

f 1(A,iω) = G1(iω)G2(iω)[Aa 1 + (2
3)A3a 3 |G1(iω)| 2
+ . . . +
B
A
A
D
2
N +1
_____
N
E
A
A
G
ANaN |G1(iω)| N−1]
(28)
Since f 1(A,iω) is given in the form of a product of a simple complex function of ω and a
complicated real function of ω, it is a simple matter to compute the corresponding
squared-magnitude function:
| f 1(A,iω)| 2 = |G1(iω)| 2 |G2(iω)| 2[Aa 1 + B
D 1
3E
G A3a 3 |G1(iω)| 2
+ . . . + B
D 1
NE
G ANaN |G1(iω)| N−1]2
= A2a1
2 |G1(iω)| 2 |G2(iω)| 2 + 2 B
D 2
3E
G A4a 1a 3 |G1(iω)| 4 |G2(iω)| 2
+ . . . +
B
A
A
D
2
N +1
_____
N
E
A
A
G
2
A2N aN
2 |G1(iω)| 2N |G2(iω)| 2
(29)
Now, an identiﬁcation strategy can be outlined as follows, assuming a 1,a 3 ≠ 0 for
convenience. For ﬁxed frequency ω1, | f 1(A,iω1)| 2 is a polynomial in A2. Thus,
measuring the amplitude of the fundamental of the responses to a suitable number of
different amplitude inputs with frequency ω1 permits calculation of the coefﬁcients
P1(ω1) = a1
2 |G1(iω1)2 |G2(iω1)| 2
P2(ω1) = a 1a 3 |G1(iω1)| 4 |G2(iω1)| 2
by polynomial interpolation. Therefore,
|G1(iω1)| 2 = a 1a 3
a1
2
_____
P1(ω1)
P2(ω1)
_______
|G2(iω1)| 2 =
a1
4
a 1a 3
_____
P2(ω1)
P1
2(ω1)
_______
This process can be repeated for various values of ω1 so that the squared-magnitude
functions for the linear subsystems can be determined as functions of ω. Then using the
minimum-phase assumption, and the normalization G1(0) = G2(0) = 1, the transfer
functions G1(s) and G2(s) can be computed using well known methods in linear system
theory.
312

7.4 Identiﬁcation Using Gaussian White Noise Excitation
This technique is an extension of a well known cross-correlation technique for the
identiﬁcation of a stationary linear system. To review brieﬂy, suppose that the input to a
linear system described by
y (t) =
−∞∫
∞
h (σ)u (t −σ) dσ
(30)
is real, stationary Gaussian white noise with mean zero and intensity A. Then forming the
product
y (t)u (t −T1) =
−∞∫
∞
h (σ)u (t −σ)u (t −T1) dσ, T1 ≥0
(31)
and taking the expected value of both sides gives
E [y (t)u (t −T1)] =
−∞∫
∞
h (σ)E [u (t −σ)u (t −T1)] dσ
=
−∞∫
∞
h (σ)Aδ0(σ−T1) dσ
= Ah (T1)
(32)
Thus, values of the kernel can be obtained from the obvious kind of input/output
experiment based on (32). Of course, it is crucial from an implementation viewpoint that
the ergodicity assumption be satisﬁed. For then the expected value is given by a time
average, and (32) can be rewritten in the form
h (T1) = A
1__
T→∞
lim 2T
1
___
−T∫
T
y (t)u (t −T1) dt
(33)
An implementation of this identiﬁcation approach is diagramed in Figure 7.3.
Figure 7.3. Cross-correlation identiﬁcation of a linear system.
A very similar analysis leads to a very similar procedure for determining values of
the symmetric kernel of a degree-n homogeneous system. The salient features are made
313
u
unknown
Π
linear system
adjustable
delay, T1
time
average

apparent by the degree-2 case, so suppose the input to the system
y (t) =
−∞∫
∞
h 2sym(σ1,σ2)u (t −σ1)u (t −σ2) dσ1dσ2
(35)
is Gaussian white noise just as before. I assume that the kernel is symmetric for reasons
that will become apparent when terms are added up (below). Now, for T1,T2 ≥ 0, T1 ≠ T2,
E [y (t)u (t −T1)u (t −T2)]
=
−∞∫
∞
h 2sym(σ1,σ2)E [u (t −σ1)u (t −σ2)u (t −T1)u (t −T2)] dσ1dσ2
(35)
The expectation on the right side can be expanded to give
E [y (t)u (t −T1)u (t −T2)] = A2
−∞∫
∞
h 2sym(σ1,σ2)δ0(σ2−σ1)δ0(T2−T1) dσ1dσ2
+ A2
−∞∫
∞
h 2sym(σ1,σ2)δ0(T1−σ1)δ0(T2−σ2) dσ1dσ2
+ A2
−∞∫
∞
h 2sym(σ1,σ2)δ0(T2−σ1)δ0(T1−σ2) dσ1dσ2
= A2δ0(T2−T1)
−∞∫
∞
h 2sym(σ,σ) dσ + 2A2h 2sym(T1,T2)
(36)
Since T1 ≠ T2, (36) yields
h 2sym(T1,T2) =
2A2
1
____ E [y (t)u (t −T1)u (t −T2)]
(37)
Imposing the ergodicity assumption permits (37) to be written in the time-average form
h 2sym(T1,T2) =
2A2
1
____
T→∞
lim 2T
1
___
−T∫
T
y (t)u (t −T1)u (t −T2) dt, T1 ≠T2
(38)
An implementation of (38) is diagramed in Figure 7.4.
314

Figure 7.4. Cross-correlation identiﬁcation method for
a degree-2 system.
For T1 = T2 ≥ 0, this approach breaks down because for white noise E [u 2(t)] does
not exist.
Traditionally, this is sidestepped by either of the claims:
1)
in any
implementation of the method, u (t) is actually not white, 2) the values h 2sym(T,T) can be
obtained by continuous extension of values h 2sym(T1,T2) for T1 ≠ T2. Either claim can be
valid under appropriate circumstances, but it will be seen in due course that this "diagonal
value" issue can cause important difﬁculties.
For general degree-n homogeneous systems, the cross-correlation identiﬁcation
method is based on the relationship
hnsym(T1, . . . ,Tn) =
n !An
1
_____ E [y (t)u (t −T1) . . . u (t −Tn)]
(39)
where T1, . . . ,Tn are distinct, nonnegative numbers. The derivation of this formula is left
as an exercise, the solution to which is essentially contained in a calculation later in this
section.
To consider the application of the cross-correlation approach to polynomial systems,
a degree-3 polynomial system will be used:
y (t) =
−∞∫
∞
h 1(σ1)u (t −σ1) dσ1
+
−∞∫
∞
h 3sym(σ1,σ2,σ3)u (t −σ1)u (t −σ2)u (t −σ3) dσ1dσ2dσ3
Computing the input/output cross-correlation E [y (t)u (t −T1)u (t −T2)u (t −T3)] gives
E [y (t)u (t −T1)u (t −T2)u (t −T3)] = Ah 1(T1)δ0(T3−T2) + Ah 1(T2)δ0(T3−T1)
+ Ah 1(T3)δ0(T2−T1) + 3!A3h 3sym(T1,T2,T3)
Thus, for T1,T2,T3 distinct, the degree-3 polynomial case is just like the degree-3
homogeneous case in giving
315
unknown
Π
degree 2 system
adjustable
delay, T1
time
average
adjustable
delay, T2
Π

h 3sym(T1,T2,T3) =
3!A3
1
_____ E [y (t)u (t −T1)u (t −T2)u (t −T3)]
Computing the cross-correlation E [y (t)u (t −T1)] gives
E [y (t)u (t −T1)] = Ah 1(T1) + 3A2
−∞∫
∞
h 3sym(σ,σ,T1) dσ
(40)
Therefore, determining values of the degree-1 kernel involves the degree-3 kernel with not
all arguments distinct. Unless the integral term in (40) can be approximated accurately
using appropriate approximate values of h 3sym(σ,σ,T1), the degree-1 kernel values cannot
be isolated. Of course, there are hypotheses, usually quite restrictive, that can ameliorate
the situation. Often these hypotheses can be conveniently phrased in terms of an assumed
interconnection structure for the unknown system.
Example 7.2
Suppose a system is known to have the interconnection structure shown in
Figure 7.5.
Figure 7.5. A degree-2 polynomial system.
 
Then the input/output representation can be written in the form
y (t) =
−∞∫
∞
h (σ)u (t −σ) dσ +
−∞∫
∞
−∞∫
∞
a 2h (σ1)h (σ2)u (t −σ1)u (t −σ2) dσ1dσ2
With an input that is a sample function from a zero-mean, white Gaussian random process
with intensity A, the mean of the response is
E [y (t)] =
−∞∫
∞
h (σ)E [u (t −σ)] dσ +
−∞∫
∞
−∞∫
∞
a 2h (σ1)h (σ2)E [u (t −σ1)u (t −σ2)] dσ1dσ2
= a 2A
−∞∫
∞
h 2(σ) dσ
The input/output cross-correlation is given by
316
u
Σ
y
h(t)
a2(.)2

Ryu(τ) = E [y (t)u (t −τ)]
=
−∞∫
∞
h (σ)E [u (t −σ)u (t −τ)] dσ
+
−∞∫
∞
−∞∫
∞
a 2h (σ1)h (σ2)E [u (t −σ1)u (t −σ2)u (t −τ)] dσ1dσ2
= Ah (τ)
Thus, values of the kernel can be computed from input/output cross-correlations. And if a
sufﬁcient number of values are computed to approximate the integral, then the constant a 2
can be computed from the response mean.
The general difﬁculties encountered in the polynomial system case can be
circumvented by adopting the Wiener orthogonal representation. (Another important
reason for using the Wiener representation is suggested in Problem 7.5.) Suppose that a
system can be described by
y (t) =
n =0Σ
N
Gn[kn,u (t)]
(41)
where, as given in Section 5.5,
Gn[kn,u (t)] =
i =0Σ
[n /2]
2i(n −2i)!i !
(−1)in !Ai
__________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi)
dτ1 . . . dτi u (t −σ1) . . . u (t −σn −2i) dσ1 . . . dσn −2i
(42)
Following the notation in Section 5.5, the Wiener kernels are symmetric despite the
absence of the subscript "sym". Now the identiﬁcation problem can be viewed as the
problem of determining the symmetric function kn(t 1, . . . ,tn) which speciﬁes Gn[kn,u (t)],
n = 0,1, . . . ,N.
The procedure again involves products of delayed versions of the Gaussian white
noise input.
Such a product, u (t −T1) . . . u (t −Tn), can be viewed as a degree-n
homogeneous operator on the input, and this viewpoint allows use of the orthogonality
property
of
the
Wiener
operators.
(Recall
that
the
homogeneous
operator
u (t −T1) . . . u (t −Tn) can be written in integral form using impulsive kernels, but there
seems to be little reason to do so for the following calculation.)
First note that the expected value of the output is
E [y (t)] =
n =0Σ
N
E [Gn[kn,u (t)]]
(43)
and, using the result established in the proof of Theorem 5.1, the degree-0 Wiener kernel is
given by
317

k 0 = E [y (t)]
(44)
The value of the degree-1 Wiener kernel k 1(t), at t = T1 ≥ 0 is found as follows. First,
E [y (t)u (t −T1)] = E [
n =0Σ
N
Gn[kn,u (t)] u (t −T1)]
= E [G0[k 0,u (t)]u (t −T1)] + E [G1[k 1,u (t)] u (t −T1)]
(45)
where the fact that Wiener operators of degree > 1 are orthogonal to any degree-1 operator
has been used. In a more explicit notation,
E [y (t)u (t −T1)] = k 0E [u (t −T1)] +
−∞∫
∞
k 1(σ)E [u (t −σ)u (t −T1)] dσ
= Ak 1(T1)
so that
k 1(T1) = A
1__ E [y (t)u (t −T1)]
(46)
Of course, under an ergodicity hypothesis this calculation can be implemented as a time
average.
Now I press on to the determination of the degree-2 Wiener kernel. For distinct
nonnegative numbers T1 and T2, the evaluation k 2(T1,T2) can be found by noting that
E [y (t)u (t −T1)u (t −T2)] = E [
n =0Σ
N
Gn[kn,u (t)]u (t −T1)u (t −T2)]
= E [k 0u (t −T1)u (t −T2) +
−∞∫
∞
k 1(σ)u (t −σ)u (t −T1)u (t −T2) dσ
+
−∞∫
∞
k 2(σ1,σ2)u (t −σ1)u (t−σ2)u (t −T1)u (t −T2) dσ1dσ2
−A
−∞∫
∞
k 2(σ,σ) dσu (t −T1)u (t −T2)]
= Ak 0δ0(T1−T2) + 2A2k 2(T1,T2)
(47)
Thus, since T1 ≠ T2,
k 2(T1,T2) =
2A2
1
____ E [y (t)u (t −T1)u (t −T2)]
(48)
318

The degree-m (≤ N) Wiener kernel is evaluated in a similar fashion. For T1, . . . ,Tm
distinct nonnegative numbers, the calculation can be outlined as follows:
E [y (t)u (t −T1) . . . u (t −Tm)] = E [
n =0Σ
N
Gn[kn,u (t)]u (t −T1) . . . u (t −Tm)]
(49)
By the orthogonality property,
E [y (t)u (t −T1) . . . u (t −Tm)] =
n =0Σ
m
E [Gn[kn,u (t)]u (t −T1) . . . u (t −Tm)]
(50)
Changing to a more explicit notation and using (42) gives
E [y (t)u (t −T1) . . . u (t −Tm)] =
n =0Σ
m
−∞∫
∞
kn(σ1, . . . ,σn)E [u (t −σ1) . . . u (t −σn)
u (t −T1) . . . u (t −Tm)] dσ1 . . . dσn
+
n =0Σ
m
i =1Σ
[n /2]
2i(n −2i)!i !
(−1)in !Ai
__________
−∞∫
∞
kn(σ1, . . . ,σn −2i,τ1,τ1, . . . ,τi,τi) E [u (t −σ1) . . . u (t −σn −2i)
u (t −T1) . . . u (t −Tm)] dτ1 . . . dτi dσ1 . . . dσn −2i
(51)
In the ﬁrst summation in (51), the expected value can be rewritten as a sum of products of
impulses. When n = m further analysis of the integrations indicates that two types of
terms will arise: those that contain a factor δ0(Ti−Tj), and those that contain no impulse,
but rather an evaluation of the kernel for some permutation of arguments T1, . . . ,Tm.
Since the Tj´s are distinct, all those terms with impulse factors will be zero, and it can be
shown that the remaining terms give, by symmetry of the kernel, m !Amkm(T1, . . . ,Tm).
When n < m in the ﬁrst summation in (51), there are two cases. If n + m is odd, then the
expected value is zero. If n + m is even, then each term in the expected value will contain
a factor of the form δ0(Ti−Tj), and so again zero is obtained. For similar reasons, all the
terms in the second summation in (51) yield 0. Thus,
km(T1, . . . ,Tm) =
m !Am
1
______ E [y (t)u (t −T1) . . . u (t −Tm)]
(52)
under the hypothesis that the Tj´s are distinct.
The reader undoubtedly is convinced by now of the crucial nature of the distinct Tj
assumption. Unfortunately, this causes an important difﬁculty when it is the symmetric
Volterra kernel that is of interest. To convert the Wiener representation (41) into a Volterra
series representation, the various terms of like degree in (41) must be gathered together.
Recalling Theorem 5.3, the degree-n symmetric kernel in a Volterra series representation
of the system in (41) is given by
319

hnsym(t 1, . . . ,tn) =
m=0
Σ
(N−n)/2
n !m !2m
(−1)m(n +2m)!Am
_______________
0∫
∞
kn +2m(t 1, . . . ,tn,σ1,σ1, . . . ,σm,σm) dσ1 . . . dσm
(53)
It is clear that values of the symmetric Volterra kernel even for distinct arguments depend
on values of the Wiener kernels for indistinct arguments.
A way to avoid the diagonal difﬁculty is to use the residual
y (t) −
n =0Σ
m−1
Gn[kn,u (t)]
(54)
rather than just the response y (t) in the computation of kernel values. It can be shown (see
Problem 7.4) that for any nonnegative values T1, . . . ,Tm,
km(T1, . . . ,Tm) =
m !Am
1
______ E [(y (t) −
n =0Σ
m−1
Gn[kn,u (t)])u (t −T1) . . . u (t −Tm)]
(55)
Example 7.3
The difﬁculty in determining kernel values for nondistinct arguments does
not arise in the discrete-time case. When the input is a stationary, zero-mean, white
Gaussian random process with intensity A, Theorem 6.9 can be simpliﬁed using the results
of Example 6.8 to give the following relationships.
k 0 = E [y (k 0)]
k 1(i 1) = A
1__E [y (k 0)u (k 0−i 1)]
k 2(i 1,i 2) =
B
A
C
A
D 2A2
1
____E [y (k 0)(u 2(k 0−i 1) −A)] , i 1 = i 2
2A2
1
____ E [y (k 0)u (k 0−i 1)u (k 0−i 2)] , i 1 ≠i 2
The higher-degree kernels are given by similar formulas.
Just as in the case of Volterra kernels, the question of how to use the values of the
Wiener kernels is difﬁcult. Again I will indicate one approach by investigating further the
case where a particular interconnection structure is assumed. A side beneﬁt is that in the
course of the development there will be occasion to exercise a number of the tools that
have been developed for manipulating Volterra series and Wiener series representations.
Suppose an unknown system is known to have the interconnection structure shown
in Figure 7.6, where the two linear systems are assumed to be stable, minimum phase, and
such that G1(0) = G2(0) = 1. Notice that here the Fourier transform notation is being used
so that the linear subsystems are speciﬁed in terms of system functions.
320

Figure 7.6. A familiar interconnection structure.
 
Proceeding as in the case of transfer functions, it is easy to show that in terms of the
subsystem system functions, symmetric Volterra system functions are given by
Hnsym(ω1, . . . ,ωn) = anG1(ω1) . . . G1(ωn)G2(ω1+ . . . +ωn) , n = 1,2, . . . ,N (56)
Then from Problem 5.14, the Wiener system functions, the Fourier transforms of the
Wiener kernels, are given by
Kn(ω1, . . . ,ωn) =
j =0Σ
[(N−n)/2]
n !j !2j
(n +2j)!A jan +2j
______________ G1(ω1) . . . G1(ωn)
G2(ω1+ . . . +ωn)[ 2π
1
___
−∞∫
∞
G1(γ)G1(−γ)dγ]j
(57)
Using Parseval’s relation for single-variable Fourier transforms gives
Kn(ω1, . . . ,ωn) =
j =0Σ
[(N−n)/2]
n !j !2j
(n +2j)!A jan +2j
______________ [
−∞∫
∞
g1
2(τ) dτ]j
G1(ω1) . . . G1(ωn)G2(ω1+ . . . +ωn) , n = 1, . . . ,N
(58)
Now, from the results of the cross-correlation method it will be assumed that a
sufﬁcient number of values of the degree-1 Wiener kernel have been obtained to permit the
computation of K 1(ω). Then (58) gives
K 1(ω) =
j =0Σ
[(N−1)/2]
j !2j
(1+2j)!A ja 1+2j
______________ [
−∞∫
∞
g1
2(τ)dτ]jG1(ω)G2(ω)
(59)
That is, the product G1(ω)G2(ω) is determined up to an unknown constant.
Suppose also that a sufﬁcient number of values of the degree-2 Wiener kernel have
been computed to permit the calculation of K 2(ω1,ω2). Then (58) gives
K 2(ω1,ω2) =
j =0Σ
[(N−2)/2]
2!j !2j
(2+2j)!A ja 2+2j
______________ [
−∞∫
∞
g1
2(τ)dτ]jG1(ω1)G1(ω2)G2(ω1+ω2) (60)
That is, the product G1(ω1)G1(ω2)G2(ω1+ω2) is determined up to a constant.
In order to show how to obtain G1(ω) and G2(ω) from the ﬁrst two Wiener system
functions, it is convenient to write
321
u
y
G1(ω)
aN(.)N + aN–1(.)N–1 + … + a1(.)
G2(ω)

K 1(ω) = α1G1(ω)G2(ω)
K 2(ω1,ω2) = α2G1(ω1)G1(ω2)G2(ω1+ω2)
where α1 and α2 are unknown constants. Then it is easy to verify that for any ω,
K 1(ω/2)
K 2(−ω/2,ω)
___________ = α1
α2
___
G1(ω/2)
G1(−ω/2)G1(ω)
______________
so that the magnitude spectrum of G1(ω) is determined up to an unknown constant
according to
|G1(ω)| = α2
α1
______
|K 1(ω/2)|
|K 2(−ω/2,ω)|
_____________
Of course, this implies that the magnitude spectrum of G2(ω) is determined up to an
unknown constant according to
|G2(ω)| = α1
1
______
|G1(ω)|
|K 1(ω)|
________
Using the minimum-phase and normalization assumptions, the calculation of G1(ω) and
G2(ω) is a well known problem in linear system theory. Further consideration of the
identiﬁcation problem, in particular, the determination of the coefﬁcients in the
nonlinearity, is left to Problem 7.8. But it is important to notice how the linear subsystems
in the degree-N polynomial system can be determined from just two kinds of input/output
cross-correlations.
7.5 Orthogonal Expansion of the Wiener Kernels
Because of difﬁculties in the use of Wiener-kernel values, an orthogonal expansion
approach can be an important alternative. The basic idea, similar to that brieﬂy discussed
in Section 7.1, is to represent the Wiener kernels of the unknown system in terms of an
orthonormal basis for the Hilbert space L2(0,∞), and then determine the coefﬁcients in this
orthonormal expansion. Again, the input signal to be used is a real, stationary, zero-mean,
white Gaussian random process with intensity A.
Suppose the unknown system can be described in terms of the Wiener orthogonal
representation.
Furthermore, assume that each Wiener kernel kn(t 1, . . . ,tn) can be
represented in the following way. Let φ1(t),φ2(t), . . . be an orthonormal basis in L2(0,∞).
That is,
0∫
∞
φi(t)φj(t) dt =
B
C
D 1 , i = j
0 , i ≠j
(61)
Then in terms of this basis write each Wiener kernel in the form
322

kn(t 1, . . . ,tn) =
i 1=1Σ
∞. . .
in=1Σ
∞
ki 1 . . . inφi 1(t 1) . . . φin(tn)
(62)
where
ki 1 . . . in =
0∫
∞
kn(t 1, . . . ,tn)φi 1(t 1) . . . φin(tn) dt 1 . . . dtn
(63)
Notice that for any permutation π of i 1, . . . ,in,
ki 1 . . . in = k π(i 1) . . . π(in)
(64)
by the symmetry hypothesis implicit in the use of the Wiener operators. Of course, the
expansion (62) will be truncated to some ﬁnite number of terms in practice, thereby
yielding an approximate representation. The identiﬁcation problem now is posed in terms
of determining the expansion coefﬁcients ki 1 . . . in.
For the degree-0 Wiener kernel, there is nothing to discuss since k 0 = E [y (t)]. For
the degree-1 Wiener kernel,
k 1(t) =
i =1Σ
∞
kiφi(t)
(65)
the i th
coefﬁcient can be identiﬁed according to the following cross-correlation
calculation:
E [y (t)
0∫
∞
φi(σ)u (t −σ)dσ] = E [
n =0Σ
∞
Gn[kn,u (t)]
0∫
∞
φi(σ)u (t −σ) dσ]
= k 0
0∫
∞
φi(σ)E [u (t −σ)] dσ +
0∫
∞
0∫
∞
k 1(τ)φi(σ)E [u (t −τ)u (t −σ)] dτdσ
= A
0∫
∞
k 1(τ)φi(τ) dτ = Aki
(66)
In terms of the notation to be used for the higher-degree cases, (66) can be written as
ki = A
1__ E [y (t)G1[φi,u (t)]]
(67)
If ergodicity is assumed, the cross-correlation can be computed by time-averaging. Then
the identiﬁcation method can be diagramed in terms of a multiplicative connection of the
unknown system with the known system G1[φi,u (t)] as shown in Figure 7.6.
323

Figure 7.7. Coefﬁcient identiﬁcation method for k 1(t).
 
The determination of the coefﬁcient ki 1i 2 for the Wiener kernel k 2(t 1,t 2) can be
diagramed as shown in Figure 7.7, where the unknown system is connected in
multiplicative parallel with the known system described by the Wiener operator
G2[φi 1φi 2,u (t)] =
0∫
∞
2
1__[φi 1(τ1)φi 2(τ2)+φi 1(τ2)φi 2(τ1)]u (t −τ1)u (t −τ2) dτ1dτ2
−A
0∫
∞
φi 1(τ)φi 2(τ) dτ
(68)
(Notice that the Wiener-operator notation is being abused slightly to avoid writing out the
symmetric version of φi 1φi 2.)
Figure 7.8. Coefﬁcient identiﬁcation method for k 2(t 1,t 2).
 
Using the orthogonality properties of the Wiener operator,
E [y (t) G2[φi 1φi 2,u (t)]] = E [
n =0Σ
∞
Gn[kn,u (t)]G2[φi 1φi 2,u (t)]]
= E [G2[k 2,u (t)]G2[φi 1φi 2,u (t)]]
= E [G2[k 2,u (t)]
0∫
∞
2
1__[φi 1(τ1)φi 2(τ2)+φi 1(τ2)φi 2(τ1)]u (t −τ1)u (t −τ2) dτ1dτ2
324
u
y
Π
G1[φi ,u(t)]
time
average
Gn[kn ,u(t)]
n=0Σ
∞
u
y
Π
G2[φi ,φi ,u(t)]
time
average
1
2
Gn[kn ,u(t)]
n=0Σ
N

=
0∫
∞
2
1__ k 2(σ1,σ2)[φi 1(τ1)φi 2(τ2) + φi 1(τ2)φi 2(τ1)]
E [u (t −σ1)u (t −σ2)u (t −τ1)u (t −τ2)] dσ1dσ2dτ1dτ2
−A
0∫
∞
k 2(σ,σ)φi 1(τ1)φi 2(τ2)E [u (t −τ1)u (t −τ2)] dσdτ1dτ2
(69)
Computation of the expected values goes in the usual manner to yield
E [y (t) G2[φi 1φi 2,u (t)]] = 2A2
2
1__[ki 1i 2 + ki 2i 1] = 2A2ki 1i 2
(70)
That is,
ki 1i 2 =
2A2
1
____ E [y (t) G2[φi 1φi 2,u (t)]]
(71)
The identiﬁcation procedure for the expansion coefﬁcients for the degree-n Wiener
kernel kn(t 1, . . . ,tn) proceeds in just the same way. The calculations corresponding to
(69) are much more complicated, but these can be avoided by invoking earlier results. The
starting point is shown in Figure 7.9.
 
Figure 7.9. Coefﬁcient identiﬁcation method for kn(t 1, . . . ,tn).
 
Application of the orthogonality property immediately gives
E [y (t) Gn[φi 1 . . . φin,u (t)] = E [Gn[kn,u (t)]Gn[φi 1 . . . φin,u (t)]]
Now using a slight variant of the proof of Theorem 5.2, it is easy to show that
325
u
y
Π
Gn[φi 
…φi ,u(t)]
time
average
1
n
Gn[kn ,u(t)]
n=0Σ
∞

E [Gn[kn,u (t)]Gn[φi 1 . . . φin,u (t)]]
= n !An
0∫
∞
kn(t 1, . . . ,tn) n !
1
___
π(.)Σ φi 1(t π(1)) . . . φin(t π(n)) dt 1 . . . dtn
= n !An
0∫
∞
kn(t 1, . . . ,tn)φi 1(t 1) . . . φin(tn) dt 1 . . . dtn
= n !Anki 1 . . . in
(73)
This gives the general formula
ki 1 . . . in =
n !An
1
_____ E [y (t) Gn[φi 1 . . . φin,u (t)]]
(74)
7.6 Remarks and References
Remark 7.1
The fact that the determination of kernel values is a linear problem has been
discussed by many authors working from several different viewpoints. For a treatment in a
general continuous-time setting, see
W. Root, "On the Modeling of Systems for Identiﬁcation Part I: ε-Representations of
Classes of Systems," SIAM Journal on Control, Vol. 13, pp. 927-975, 1975.
The polynomial-system identiﬁcation problem can be viewed as ﬁtting a polynomial
system to a given set of input/output pairs. An operator-theoretic study of this formulation
is given in
W. Porter, "Synthesis of Polynomic Systems," SIAM Journal on Mathematical Analysis,
Vol. 11, pp. 308-315, 1980.
An elementary discussion of the material of Section 7.1 along with an interesting
application can be found in
J. Amarocho, A. Bandstetter, "Determination of Nonlinear Rainfall-Runoff Processes,"
Water Resources Research, Vol. 7, pp.1087-1101, 1971.
Remark 7.2
A more complete discussion of the use of impulse inputs for identiﬁcation
can be found in the paper
M. Schetzen, "Measurement of the Kernels of a Nonlinear System of Finite Order,"
International Journal of Control, Vol. 1, pp. 251-263, 1965.
326

For the discrete-time case, see
S. Clancy, W. Rugh, "A Note on the Identiﬁcation of Discrete-Time Polynomial Systems,"
IEEE Transactions on Automatic Control, Vol. AC-24, pp.975-978, 1979.
Remark 7.3
An elementary review of the structural aspects of linear-system
identiﬁcation using rational interpolation theory is given in
W. Rugh, Mathematical Description of Linear Systems, Marcel Dekker, New York, 1975.
This treatment includes the topic of identiﬁcation from steady-state frequency response.
The steady-state response to single-tone inputs also can be used for identiﬁcation in a class
of interconnection structured systems somewhat more general than the linear-polynomial-
linear sandwich. See
S. Baumgartner, W. Rugh, "Complete Identiﬁcation of a Class of Nonlinear Systems from
Steady-State Frequency Response," IEEE Transactions on Circuits and Systems, Vol.
CAS-22, pp. 753-759, 1975.
E. Wysocki, W. Rugh, "Further Results on the Identiﬁcation Problem for the Class of
Nonlinear Systems SM," IEEE Transactions on Circuits and Systems, Vol. CAS-23, pp.
664-670, 1976.
J. Sandor, D. Williamson, "Identiﬁcation and Analysis of Nonlinear Systems by Tensor
Techniques," International Journal of Control, Vol. 27, pp. 853-878, 1978.
Identiﬁcation based on the steady-state response to multi-tone inputs is discussed in
K. Shanmugam, M. Jong, "Identiﬁcation of Nonlinear Systems in Frequency Domain,"
IEEE Transactions on Aerospace and Electronics, Vol. AES-11, pp. 1218-1225, 1975. The
following paper on nonlinear system identiﬁcation does not use the Volterra or Wiener
representations, but it should be consulted by the serious reader.
L. Zadeh, "On the Identiﬁcation Problem," IRE Transactions on Circuit Theory, Vol. 3, pp.
277-281, 1956.
Remark 7.4
The method in Section 7.5 for obtaining Wiener-kernel orthogonal
expansion coefﬁcients using a Gaussian white noise input is the original identiﬁcation
procedure suggested by Wiener. Wiener’s results are presented in terms of a Laguerre
function expansion basically because the Laguerre functions can be realized using
electrical circuits. However, any orthogonal expansion can be used. For a detailed analysis
of the Wiener model, see
327

M. Schetzen, The Volterra and Wiener Theories of Nonlinear Systems, John Wiley, New
York, 1980.
Remark 7.5
The cross-correlation technique for determining Wiener kernel values was
proposed in
Y. Lee, M. Schetzen, "Measurement of the Wiener Kernels of a Nonlinear System by
Cross-correlation," International Journal on Control, Vol. 2, pp. 237-254, 1965.
Further discussion of the cross-correlation method from a more mathematical point of view
can be found in
S. Klein, S. Yasui, "Nonlinear Systems Analysis with Non-Gaussian White Stimuli:
General Basis Functionals and Kernels," IEEE Transactions on Information Theory, Vol.
IT-25, pp. 495-500, 1979.
G. Palm, T. Poggio, "The Volterra Representation and the Wiener Expansion: Validity and
Pitfalls," SIAM Journal on Applied Mathematics, Vol. 33, pp. 195-216, 1977.
S. Yasui, "Stochastic Functional Fourier Series, Volterra Series, and Nonlinear Systems
Analysis," IEEE Transactions on Automatic Control, Vol. AC-24, pp. 230-242, 1979.
The ﬁrst of these papers discusses the difﬁculties involved in ﬁnding equal-argument
kernel values by the cross-correlation method. This method has been used much more
widely than the original Wiener method, in large part because of the often great number of
expansion coefﬁcients that must be found in the Wiener method. Applications of the
cross-correlation method have been particularly numerous in the biological modeling ﬁeld.
See for example
P. Marmarelis, V. Marmarelis, Analysis of Physiological Systems, Plenum, New York,
1978.
This book contains discussions of a number of important issues that arise in applications.
These issues include the problem of approximating white noise, and computational
methods for the cross-correlation method. The identiﬁcation of cascade structured systems
using the cross-correlation method has been treated in
M. Korenberg, "Identiﬁcation of Biological Cascades of Linear and Static Nonlinear
Systems," Proceedings of the Sixteenth Midwest Symposium on Circuit Theory, pp. 1-9,
1973.
M. Korenberg, "Cross-correlation Analysis of Neural Cascades," Proceedings of the Tenth
Annual Rocky Mountain Bioengineering Symposium, pp. 47-52, 1973.
328

This work is also discussed in the book by Marmarelis and Marmarelis.
Remark 7.6
The identiﬁcation problem for polynomial systems using Gaussian inputs
can be formulated in terms of Fourier transforms. This leads to an expression for the
system function in terms of higher-order cumulant spectra of the response. This
formulation and methods for estimating cumulant spectra are discussed in
D. Brillinger, "Fourier Analysis of Stationary Processes," Proceedings of the IEEE, Vol.
62, pp. 1628-1643, 1974.
D. Brillinger, "The Identiﬁcation of Polynomial Systems by means of Higher Order
Spectra," Journal of Sound and Vibration, Vol. 12, pp. 301-313, 1970.
I should note that these papers require a deeper background in statistics than that presumed
in Section 7.4.
7.7 Problems
7.1. Suppose a discrete-time, degree-n, homogeneous, system is such that
hreg(i 1, . . . ,in) = 0 , if any ij = 0
Show that for the set of positive integers I1, . . . ,In, hreg(I1, . . . ,In) can be determined
from the system response to
u (k) = δ0(k) + δ0(k −I1) + δ0(k −I1−I2) + . . . + δ0(k −I1−. . . −In)
7.2. For a degree-n homogeneous system with the cascade structure shown below, analyze
the possibility of identiﬁcation using the steady-state response to single-tone inputs.
7.3. For the system shown in Figure 7.2, devise a single-tone identiﬁcation strategy that
does not require step function inputs.
7.4. Derive (55) for m = 0,1,2,3, and discuss the limitations of the corresponding modiﬁed
cross-correlation approach to identiﬁcation.
7.5. For identiﬁcation in the inﬁnite series case using Gaussian white noise, discuss the
advantages of the Wiener representation over the Volterra representation.
329
u
G1(s)
G2(s)
(.)m
(.)m
y
1
2

7.6. Develop a simple cross-correlation technique for the identiﬁcation of cascade
structured systems of the form shown below. Do not assume that G (ω) is minimum phase.
7.7. For the class of systems considered in Problem 7.6, develop an identiﬁcation
approach based on steady-state responses to single-tone inputs.
7.8. For the system shown in Figure 7.6, show how to determine the coefﬁcients in the
330
u
y
aN(.)N + aN–1(.)N–1 + … + a1(.)
G(ω)


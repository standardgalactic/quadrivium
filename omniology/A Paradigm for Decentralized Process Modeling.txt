A PARADIGM FOR 
DECENTRALIZED 
PROCESS MODELING 

THE KLUWER INTERNATIONAL SERIES 
IN ENGINEERING AND COMPUTER SCIENCE 

A P ARADIGM FOR 
DECENTRALIZED 
PROCESS MODELING 
by 
Israel Ben-Shaul 
Technion, Israel Institute of Technology 
and 
IBM Haifa Research Laboratory 
Haifa, ISRAEL 
Gail Kaiser 
Columbia University 
New York, NY, USA 
~ . . , 
SPRINGER-SCIENCE+BUSINESS MEDIA, B.V. 

ISBN 978-1-4613-5974-6 
ISBN 978-1-4615-2305-5 (eBook) 
DOI 10.1007/978-1-4615-2305-5 
Library of Congress CataIoging-in-Publication Data 
A C.I.P. Catalogue record for this book is available 
from the Library of Congress. 
Copyright c 1995 by Springer Science+Business Media Dordrecht 
Originally published by Kluwer Academic Publishers in 1995 
Softcover reprint ofthe hardcover Ist edition 1995 
Ali rights reserved. No part of this publication may be reproduced, stored in 
a retrieval system or transmitted in any form or by any means, mechanical, 
photo-copying, recording, or otherwise, without the prior written permission of 
the publisher, Springer Science+Business Media, B.Y. 
Printed on acid-free paper. 

CONTENTS 
LIST OF FIGURES 
IX 
LIST OF TABLES 
Xl 
PREFACE 
Xlll 
1 
INTRODUCTION 
1 
1.1 
Process Modeling 
2 
1.2 
Why Decentralization ? 
7 
1.3 
A Motivating Example 
7 
1.4 
Research Focus 
10 
1.5 
Requirements 
14 
2 
PREVIOUS AND RELATED WORK 
17 
2.1 
PCEs 
17 
2.2 
Marvel: The Predecessor PCE 
21 
2.3 
Other Domains 
46 
3 
THE DECENTRALIZED MODEL 
51 
3.1 
Definitions 
52 
3.2 
Defining Process Interoperability: the Treaty 
61 
3.3 
Enacting Process Interoperability: the Summit 
75 
3.4 
The Motivating Example Revisited 
81 
3.5 
Application of the Model 
83 
3.6 
Groupware Tools and Delegation in Summits 
95 
3.7 
Extensions and Alternatives to the Decentralized Model 
98 

VI 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
4 
REALIZATION OF THE DECENTRALIZED 
MODEL IN OZ 
101 
4.1 
Operational Overview of Oz 
102 
4.2 
Oz Objectbase 
106 
4.3 
Defining Process Interoperability in Oz 
109 
4.4 
Multi-Process Enactment in Oz 
135 
4.5 
Support for Delegation and Groupware in Oz 
157 
4.6 
Implementation Status 
167 
5 
ARCHITECTURAL SUPPORT FOR 
DECENTRALIZATION IN OZ 
169 
5.1 
Architectural Overview 
170 
5.2 
Communication Infrastructure 
179 
5.3 
A Process for Site Configuration 
191 
5.4 
Context Switching in Summit 
195 
5.5 
The Remote Object Cache 
198 
5.6 
Oz Over the Internet 
209 
5.7 
Implementation Status 
214 
6 
THE ISPW EXAMPLE: VALIDATION AND 
METHODOLOGY ISSUES 
217 
6.1 
Overview of the Scenario 
218 
6.2 
Solution in Oz 
219 
6.3 
Methodology Issues 
234 
7 
SUMMARY, EVALUATION, AND FUTURE 
WORK 
239 
7.1 
Evaluation 
242 
7.2 
Future Directions 
244 
A 
CONFIGURATION PROCESS SOURCES 
247 
A.l Registration Strategy 
247 
A.2 A Sample Oz Envelope 
251 
B 
THE ISPW-9 PROBLEM: DEFINITION AND 
SOLUTION IN OZ 
257 

Contents 
B.1 The ISPW-9 Example 
B.2 Solution in Oz 
REFERENCES 
INDEX 
VB 
257 
260 
283 
297 

LIST OF FIGURES 
Chapter 1 
1.1 
A Motivating Example 
Chapter 2 
2.1 Several Classes from C/Marvel 
2.2 
Example Rule from C/Marvel 
2.3 
Example Tool Declaration from C/Marvel 
2.4 Class Definitions for Access Control 
2.5 
Marvel Architecture 
Chapter 3 
3.1 
A Generic Single-Process Environment 
3.2 Change Sub-Process 
3.3 A Decentralized Environment 
3.4 Enactment of Motivating Example 
3.5 Another Enactment of Motivating Example 
3.6 Comparison of PMLs 
3.7 Example Multi-Process Petri-net 
3.8 Summit in Ada 
Chapter 4 
4.1 
A Sample Oz Environment 
4.1 
An Oz Environment with One Open Remote Site 
4.2 Load Interface in Oz 
4.3 Import and Export Interfaces in Oz 
4.4 Integration of Imported Rules 
4.5 The Import Algorithm in Oz 
8 
25 
29 
32 
42 
44 
57 
58 
60 
82 
83 
84 
90 
96 
104 
105 
106 
111 
114 
116 

x 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
4.6 The Treaty Interface in Oz 
119 
4.7 Evolution Timestamps: Export-Request Import-Accept 
124 
4.7 
Evolution Timestamps: Export-Accept Import-Request 
124 
4.8 
Run-time Treaty Validation Algorithm 
125 
4.9 
Compile Rule 
126 
4.10 Definition of class CFILE 
128 
4.10 Another Definition of class CFILE 
128 
4.11 Multi-edit rule 
129 
4.12 Execution Trace of Summit Example 
148 
4.13 Oz Animation of Summit Example 
149 
4.13 Oz Animation of Summit Example 
150 
4.14 Delegation Example 
161 
Chapter 5 
5.1 
Oz Architecture 
5.2 
Refresh Policy in Oz 
5.3 
The built-in class SUB_ENV 
5.4 Connection Database 
5.4 
Connection Database with A Remote Connection 
5.5 Server-to-server Communication Protocol 
5.6 
A Communication Deadlock Example 
5.7 The Extended Busy-wait-service Algorithm 
Chapter 6 
6.1 
Process Design for ISPW-9 Example Scenario 
6.2 
notify_bug Rule 
6.3 The QA Objectbase 
6.4 The QA Objectbase with New Test Runs 
6.5 
Rule Animation of the Testing Task 
6.6 The Analyze Summit Task 
6.7 
The Review Summit Task 
6.8 
Conference Rule 
171 
177 
182 
184 
185 
187 
196 
199 
220 
222 
223 
224 
225 
227 
228 
230 

LIST OF TABLES 
Chapter 4 
4.1 
Delegation Types 
Chapter 5 
5.1 
Context Switch Summary 
5.2 
Performance comparison with and without cache 
Chapter 6 
6.1 
Treaties in the ISPW Process 
6.2 Summary of Lines of Code for the ISPW problem 
158 
199 
208 
232 
233 

PREFACE 
A process (or workflow) is an organized set of steps for carrying out an endeavor, 
be it software development or maintenance of a software product, computer-
aided design and manufacturing of an engineered system, management and 
oversight of a business enterprise, or automating and monitoring managed care 
and delivery in healthcare organizations increasingly concerned with enforcing 
proper procedures and protecting themselves against liability lawsuits. Pro-
cesses are "situated", in the sense that different processes are suitable for differ-
ent projects, organizations, and application domains. Thus, a computer-based 
system intended to intelligently assist users in carrying out any of a wide range 
of processes should be knowledge-based, where the knowledge is some suitable 
encoding of the desired process. In particular, a process model represents the 
knowledge of a specific process, including the process steps, their prerequisites 
and consequences, and any synchronization among concurrent steps, defined in 
some process modeling language (PML). A process-centered environment (peE) 
includes an engine that interprets a given PML and assists the users in some 
way to carry out a specified process instance, where forms of assistance may 
include (but are not limited to) automation, enforcement of consistency con-
straints, guidance, analysis, simulation, monitoring and measurements, of the 
process. 
In parallel to the recent advances in process modeling technology, the world 
populace at large has experienced an explosive growth in telecommunication 
technologies and infrastructures that enable regional and even global commu-
nication (e.g., the Internet), which are also promoted by the U.S federal gov-
ernment and other national governments as the "Information Superhighway". 
This "globalization" provides opportunities for growth and collaboration among 
individual professionals and teams that are geographically dispersed and time 
shifted, but at the same time introduces heterogeneity, autonomy and security 
as key concerns to address. Thus, decentralization into autonomous and secure 
yet cooperating components is an inevitable trend in future organizations and 
systems. 

XIV 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
This book presents a novel approach that combines both trends and sug-
gests a paradigm for decentralized PCEs, supporting concerted efforts among 
geographically-dispersed teams -
each local individual or team with its own 
autonomous process -
with emphasis on flexible control over the degree of 
collaboration versus autonomy provided. A key guideline in this approach is to 
supply abstraction mechanisms whereby pre-existing processes (or workflows) 
can be encapsulated and retain security of their internal artifacts and status 
data, while agreeing with other processes on formal interfaces through which 
all their interactions are conducted on intentionally shared information. 
The book is a valuable resource for both researchers and practitioners, particu-
larly in software engineering, software development environments, and software 
process and workflow management, but also in electrical, mechanical, civil and 
other areas of engineering which have analogous needs for design processes, 
environmental support and concurrent engineering, and beyond to private and 
public sector workflow management and control, groupware support, and het-
erogeneous distributed systems in general. 
THE SCOPE OF THE BOOK 
The main body of the book addresses definition and execution of multiple 
interoperating processes (or workflows) at three levels of abstraction: 
1. A conceptual framework, given by a formal and generic (i.e., system- and 
language- independent) model for process-interoperability. 
2. A comprehensive realization of the model in the Oz PCE system (the 
successor to the earlier single-process Marvel PCE). 
3. An architectural framework and infrastructure that supports the decen-
tralized model. 
To understand the decentralized model, some background material is necessary. 
We begin with an introduction to the field of process modeling and PCEs and 
define basic terms that are used frequently throughout the book. The moti-
vation for this research is discussed next, and a motivating example scenario 
of distributed software development is given. The chapter concludes with a 
description of the focus of our work and a set of derived requirements which 
have guided this research. 

Preface 
xv 
Chapter 2 starts with a survey of state-of-the-art PCEs, particularly those 
that address distribution, interoperability, and heterogeneity. The main part 
of this chapter is devoted to a detailed description of Marvel, the predecessor 
to Oz. While the emphasis is on concepts and techniques which were carried 
over to Oz -
such as the rule-based process modeling notation and the as-
sociated rule-chaining semantics, transactional concurrency control and failure 
recovery support, and the components of the single-server architecture -
it 
is self-contained and somewhat external to the rest of the book and provides a 
unique opportunity to obtain conceptual and implementation details of Marvel 
never published before. The chapter concludes with relevant work in related 
fields, namely databases, heterogeneous processing, and computer-supported 
cooperative work (CSCW). 
Chapter 3 describes the generic decentralized model, which is the cornerstone 
of this research. It covers the two main abstractions used to define and ex-
ecute process interoperability, namely the Treaty and the Summit, and ap-
plies these metaphorical abstractions to three families of PMLs that represent 
the paradigms of choice in many existing PCEs, namely, rules, Petri nets and 
grammars, with brief discussion on its applicability to APPL/ A, probably the 
best-known imperative PML. 
The generic model, as a high-level abstraction, leaves many aspects undefined 
and unresolved, both technical and conceptual. Chapter 4 addresses these 
issues by discussing the realization of the model in the Oz multi-site PCE, 
covering: the algorithms that implement the Treaty abstraction for defining 
common sub-processes while preserving the autonomy and consistency of both 
local- and inter-process definitions; the realization of the Summit abstraction 
for multi-process execution focusing on the operational and transactional se-
mantics; and an effective (although preliminary) implementation of language 
and system mechanisms for integration of groupware technologies such as desk-
top conferencing. 
Chapter 5 is devoted to the architecture of Oz. It describes the means by 
which Oz provides on one hand operational independence and security for sub-
environments with their own autonomous processes collaborating in a multi-
site endeavor, and on the other hand process interconnectivity as a basis for 
the process interoperability explained in Chapter 4. The chapter is organized 
as a collection of loosely-coupled issues concerned with different aspects of 
the system's characteristics, including: the communication infrastructure that 
provides site interconnectivity and a dynamic site configuration mechanism; 
a context-switching mechanism that resolves potential communication dead-
locks and starvation during the concurrent execution of Summits; an efficient 

XVI 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
remote-object cache that enhances the performance of Summits; and architec-
tural extensions to support arbitrary geographical dispersion over the Internet. 
This chapter is particularly useful for both researchers and practitioners in dis-
tributed environment architecture in general and workflow system architectures 
in particular, as well as a rare opportunity to expose system builders to the 
various issues and possible solutions involved in constructing an Internet-wide 
environment. 
To validate and evaluate the decentralized model and its realization in Oz, 
Chapter 6 presents an actual multi-site software development environment in-
stance that was built to support an example "benchmark" process scenario 
defined by an international committee for the International Software Process 
Workshop series. In addition, the chapter discusses a methodology for defin-
ing decentralized processes according to our model, based in part on this case 
study. 
The final chapter evaluates this research and development mostly against the 
requirements set forth in Chapter 1, summarizes the contributions of, and con-
clusions from this work, and outlines directions for the future. 
THE USE OF THE BOOK 
The book is primarily intended to provide an in-depth discussion of decen-
tralized process modeling and emictment technology, covering both high-level 
concepts and a full-blown realization of these concepts in a concrete system. 
In the classroom, either the whole book or selected chapters (see below) could 
be used as one of the readings for a graduate course on software engineering, 
software process, or software development environments, or even for a course 
studying workflow systems outside Computer Science (e.g., in a classical engi-
neering department for engineering design, or in a business school for business 
practices or enterprise-wide management, or in the medical informatics depart-
ment of a health sciences institution concerned with computer-assistance for 
managed care). Selected portions of the book, such as section 2.2 on Marvel, 
could also be employed as a case study in an advanced undergraduate software 
engineering course. 
Chapter 1 is required for all other parts of the book. Section 2.2 can be studied 
in depth by itself, and is also recommended as a prerequisite for Chapter 4, 
but not necessarily for Chapter 3. Chapter 3, probably the most important 
chapter, is a pre-requisite to Chapter 4, the second most important chapter. 
Chapter 5 is somewhat independent from previous chapters, although a better 

Preface 
XVll 
understanding of the problems can be gained from reading them; it requires 
some technical background in distributed systems. Finally, Chapters 6 and 7 
require prior reading of at least Chapter 3 and probably also Chapter 4. 
ACKNOWLEDGEMENTS 
This book is based on the doctoral dissertation of Dr. Ben-Shaul, conducted at 
Columbia University from 1992-1994 and advised by Prof. Kaiser. The authors 
would like to thank the many people who participated directly in this research 
and to those who read portions of the thesis and provided valuable sugges-
tions. In particular we would like to thank John Arnold, Naser Barghouti, Will 
Chou, George Heineman, John Hinsdale, Simon Kaplan, John Kender, Yong 
Su Kim, Andrew Lih, Steve Linde, Hsin Liu, Will Marrero, Hideyuki Miki, 
K. Narayanaswamy, Lee Osterweil, Steve Popovich, Ken Ross, Peter Skopp, 
Mike Sokolsky, Andrew Tong, Shelley Tselepis, Giuseppe Valetto and Jack 
Yang. George Heineman in particular provided an enormous contribution to 
this work, and along with the authors is a primary architect of the Oz system. 
Naser Barghouti, working with Prof. Kaiser, was a primary architect of the 
predecessor Marvel system. Peter Skopp and Mike Sokolsky were also principal 
developers of the Oz and Marvel systems, respectively. 
Dr. Ben-Shaul would also like to thank his devoted parents, Rivka and Simcha 
Ben-Shaul, his wife Tamar, for her continous support and encouragement, and 
his daughter Mika, for providing wonderful moments of joy. 
Prof. Kaiser would also like to thank her parents, Beverly and Frank Kaiser, 
her husband Dan Duchamp, and her son Matthew Duchamp. Without their 
support, this research could never have been completed and this book would 
never have been written. 

1 
INTRODUCTION 
Software Engineering (SE) is aimed at constructing cost-effective and high-
quality large-scale software. As such, it is concerned with methodologies, tools, 
and frameworks that can assist groups of developers throughout the lifecycle 
of a software product. 
Software Development Environments (SDEs) is a sub-field within SE 
that is more specifically concerned with providing frameworks, or infrastruc-
tures, for supporting the development of software products. Thus, the SDE 
community is less concerned with the development of specific tools but rather 
with the integration and interoperability of these tools within an environment; 
it is not only concerned with support for an individual user but also with sup-
port for coordination and interaction among multiple users participating in the 
development of a product; and, finally, it is concerned with the management 
and integrity of the various artifacts involved in a software product (e.g., mod-
ules, libraries, design documents), which are manipulated by diverse users using 
diverse tools. 
Early research in SDEs, dating back to the mid seventies, focused on support for 
coding and debugging [13]. Called programming environments, these systems 
often included a set oflanguage-specific tools (e.g., language-based editors) that 
assisted an individual programmer to code programs. 
There was a transition in the eighties, when language-based editors were used 
as front-ends to integrate a set of language-based tools. Examples included 
Pecan [lOS], which emphasized visualization; and generator environments like 
Gandalf [4S], and the Synthesizer Generator [110]. 

2 
CHAPTER 1 
More advanced SDEs focused on general purpose tool-integration techniques. 
An example system was Field [109], in which tools interact with each other 
by sending and receiving messages to a centralized Broadcast Message Server, 
effectively implementing a "software bus" from which tools can be easily added 
or removed, thereby providing extensibility. Another direction in SDE re-
search was to provide comprehensive support to the entire lifecycle of a software 
project, not only programming. Termed Integrated Project Support Environ-
ments (IPSEs) [120], these systems had to also support multiple users (as op-
posed to earlier single-user systems), and often included management of the 
software artifacts. ISTAR [33] is a representative IPSE that emphasized inte-
gration of managerial as well as engineering tasks and tools!. Other systems 
from that era include DSEE [84] and NSE [62]. 
1.1 
PROCESS MODELING 
In order to provide comprehensive project support, SE researchers and de-
velopers had to observe and understand project development processes. This 
marked the beginning of the ''process'' era [85], pioneered by Humphrey [64] 
and Lehman [86]. The realization that the process of constructing and main-
taining software was crucial in determining the success of the project -
while 
at the same time widely different from project to project, depending on the 
nature of the product being developed, management policies, tools used and so 
forth -
has led the SDE community to shift its focus towards support for the 
process as a key factor in increasing productivity and improving the quality of 
software. 
The term "software process" can be defined as an orderly approach to 
applying methods and tools to software development. It includes: 
• 
The set of activities carried out during the development process. These 
activities can be low-level activities, such as invoking a compiler on source 
code, or they can be high-level and decomposable activities (also called 
tasks), such as the integration test phase of a large system. Process ac-
tivities can be further categorized by the degree of computer support they 
require, ranging from purely computer-oriented activities, such as compila-
tion, to activities with partial computer support, such as code inspection, 
to purely human-oriented activities, such as (physical) design meetings. 
1 A more detailed account of ISTAR will be given in Chapter 2. 

Introduction 
3 
• 
Local constraints on activities, typically in the form of prerequisites to, or 
consequences of, activities. This includes constraints imposed on the exe-
cution of activities and obligations that must be carried out as a result of 
the execution of activities. For example, a process might restrict deposit-
ing a modified source code to a repository that holds a stable version of 
the system, to only allow the deposit if the source code had been statically 
inspected. An example of an activity implication might be that after the 
release of a new version of a product, all current licensees must be notified 
of the upgraded version. 
• 
Global constraints on tasks. For example, a release deadline constraint 
might affect all activities related to preparing the release. 
• 
Partial ordering among tasks. 
For example, the well-known waterfall 
model [113] for software development implies sequential ordering among 
the various phases with feedback to previous phases. 
• 
Synchronization among concurrent tasks. It is common for software pro-
cesses to allow for concurrent execution of tasks. This implies that the 
process can also specify points where dependent tasks synchronize. 
Interest in the software process has consequently led SDE researchers to explore 
ways to represent the process with a formal notation in order to analyze and/or 
partially automate support for it. Pioneered by Balzer and Osterweil [97], this 
research direction has become to be known as process modeling2. The ad-
vantages of using a formal notation for the definition of software processes are: 
(1) Understanding -
clearly, by explicitly and rigorously defining processes 
one can clarify and gain a better understanding of the processes themselves. A 
written process also helps in explaining it to the personnel involved in the pro-
cess, thereby achieving a better understanding among the project participants. 
Moreover, by using programming language concepts such as data and control 
abstraction, modularity, and encapsulation, processes can be better defined and 
understood much in the same way that programming and design in general 
benefit from these techniques. (2) Analysis -
Static process analysis can be 
performed on processes modeled in a formal notation using well-understood 
techniques from the programming languages and other domains, e.g., concur-
rent computing techniques, which can ease process improvement. For example, 
program optimization techniques can be used to eliminate redundant activities, 
to point out potential for increased concurrency among independent activities, 
to identify "dead ends" from which the process cannot proceed, and so forth. 
2 Although Osterweil referred to it as "process programming" , this tenn is generally asso-
ciated with a specific approach to process modeling. 

4 
CHAPTER 1 
Furthermore, when processes are represented as state machines (e.g., as in 
StateMate [63]) formal automata techniques can be employed, for example for 
reach ability analysis. (3) Execution -
Perhaps the dominant aspect, at least 
within the SDE community, and the one that is emphasized in this research. 
Once there is a formalism that encodes a (software) process, there is a potential 
for the process to be interpreted by a "process machine" (or process engine) 
that is sensitive to the defined process, and can assist in its execution3 . 
However, while there is an obvious resemblance between software processes 
and ordinary software, there is also a fundamental distinction between them. 
Whereas the operational semantics of the latter are fully and completely de-
fined by the software program, the compiler, and the underlying machine (and 
operating system) on which the program runs, a software process model only 
defines a (possibly small) subset of the overall process. In particular, the soft-
ware process is not being executed completely on a physical machine, and it 
involves (unpredictable) humans carrying out significant portions of the pro-
cess. As a consequence, the process engine cannot (and should not) control 
all aspects of the process, and all we can hope for is to find ways in which 
it can assist users in carrying out the process. This view has implications on 
the choice of the modeling paradigm and on the types of support that can and 
should be provided. Thus, we will use the term process modeling to encompass 
definition of the model regardless of whether it is executable or not. 
The process community invented the term process enactment to distinguish 
it from the conventional notion of program execution and emphasize the role 
of humans who direct and perform portions of the process. However, the com-
munity is divided over the use of this term, so we will use the terms "process 
enactment" and "process execution" interchangeably throughout the book to 
denote what we refer to here as enactment. 
Enactment is also sometimes confused with the concept of simulation (although 
process simulation might be a useful option) in that while a simulation might 
also involve nondeterministic agents and behaviors that describe natural phe-
nomena, process enactment involves support of real execution of the process, 
involving real devices, tools, artifacts, and real users. There are several forms 
of enactment, or execution assistance, including but not limited to: 
1. Enforcement -
This refers to the capability of the process engine to ensure 
that constraints, obligations and general process invariants are maintained 
31n some cases, such as in APPL/A [123], process models are compiled and executed 
directly. 

Introduction 
5 
consistently. For example, a specific process might impose a constraint that 
no source file can be edited unless it has been properly checked out to a 
private workspace using the process' configuration management subsystem. 
In this case, the process engine will enforce this constraint and disallow 
violations, thereby maintaining process consistency as promised. More 
complicated consistency constraints might span a group of artifacts and 
a set of interrelated activities. For example, if a function signature has 
been modified, all callers (from different modules) should be outdated, to 
force recompilation. This constraint is likely to reduce errors from interface 
mismatch. 
It is, of course, not desirable to enforce all aspects of the process on all 
individuals. Indeed, there has been some criticism about process enact-
ment stemming from the impression that individuals are "controlled" by 
the process and must operate within strict rules that restrict their work 
and creativity. The goal is to enforce those constraints and invariants that 
constitute process consistency (the "law of the system" [94]), but relax 
or even leave undefined other activities in the process. While different 
environments provide different enforcement capabilities, the degree of en-
forcement also depends largely on the specific process model, as defined 
on a project-specific basis. One system known for its enforcement support 
is Darwin [94]. 
2. Automation -
This refers to the environment's capability to carry out 
some activities of the process automatically on behalf of users. Automa-
tion might be explicitly specified by the process model, or it can be inferred 
by the system, and it may be optional. Note that automation can be used 
for enforcement. On the other hand, enforcement can be supported inde-
pendently from automation. SDEs known for their automation support 
include eLF [106] and Marvel [72]. 
3. Guidance -
The environment builder might choose to guide users in per-
forming tasks in the process, without actually forcing them to do any of 
them. For example, the process might maintain a "to do" list of pending 
tasks. SDEs known for guidance support include ProcessWEAVER [38] 
and Merlin [128]. 
4. Monitoring -
This refers to the environment's capability to monitor the 
progress of the process, and accurately assess the state of the process at 
any particular point in time (other mechanisms can be used to actually 
extract the process state for various purposes). One extension of process 
monitoring is process diagnostics, i.e., when monitoring detects problems 
that can be diagnosed and later repaired. Note that process diagnostics 
in this context refers to inspecting active enactable processes in a manner 

6 
CHAPTER 1 
analogous to a debugger, and is different from the static analysis mentioned 
earlier in the context of modeling. Another extension of monitoring is 
process instrumentation, for measurement purposes. SDEs known for their 
monitoring support include SMART [41] and Provence [82]. 
Process Centered Environments (PCEs) are environments that provide 
a Process Modeling Language (PML) in which project-specific processes 
are defined by a process administrator (as opposed to environment end-user), 
and a corresponding process enactment engine that is sensitive to the defined 
process and supports its execution in some or all ways described above. 
Most, but not all, PCEs support some form of data modeling, both for the 
product artifacts which are being manipulated by the process (product data), 
and for the data used by the PCE itself to keep track of process state (process 
data). Moreover, some PCEs support data modeling on a per-project basis; 
in these cases, data modeling may be considered to be part of the process, 
although it could be specified by a separate Data Definition Language (DDL). 
Process modeling has increasingly attracted attention in the software engi-
neering community, as evidenced by the Ninth International Software Process 
Workshop [44] and the Third International Conference on the Software Pro-
cess [101]. Various PCEs have been constructed as research prototypes and 
non-commercial systems, and some have been recently released as commercial 
products. Examples of relatively well-known academic and other research PCEs 
include Arcadia [66], Common Lisp Framework (CLF) [106], Melmac [30], Mer-
lin [128], Spade [6] and TEMPO [15]. Examples of commercial products include 
ProcessWEAVER [38] and HP SynerVision [60]. 
Process modeling and enactment is not limited to the domain of software en-
gineering, however. It may be applied to any domain that requires and/or 
may benefit from automation, guidance, management, oversight, and enforce-
ment of procedures. Example domains include general engineering design (e.g., 
computer-aided design and manufacturing of an engineered system), business 
practices, health care management and delivery, etc. One example of a com-
mercial PCE that supports business processes is Lion [88]. 
The state-of-the-art in PCE technology (including all the systems mentioned 
above) has been supporting centralized and homogeneous processes, for moderate-
sized and often co-located groups. 

Introduction 
7 
1.2 
WHY DECENTRALIZATION? 
Large-scale product development typically requires the participation of multiple 
people, often divided into multiple heterogenous groups, each concerned with a 
different facet of the product. For example, one software product may require 
different teams for requirements elicitation, functional specification, design, 
coding, testing, documentation, and maintenance; another product may also 
involve multiple teams, in this case with each responsible for full development 
of a distinct component of the system. Each team uses its own selection of 
tools, its own private artifact database, and its own management policies and 
development workflow -
all parts of the process. At the same time, the teams 
need to cooperate in order to develop the product, and as studies in software 
engineering have shown [39], the interaction among team members accounts for 
a significant fraction of the total cost of the product being developed. 
The degrees ofteam autonomy and collaboration between teams both depend on 
the nature of the product being developed and on organizational policies (e.g., 
centralized vs. decentralized management). Sometimes multiple independent 
organizations with pre-existing processes need to collaborate on a product, in 
which case autonomy (privacy or security) is a "hard" constraint that cannot 
be compromised. 
In recent years, there has been an explosive growth in telecommunication tech-
nologies and infrastructures that enable global communication (most notably, 
the Internet). This "globalization" provides immense opportunities for growth 
and collaboration among teams that are geographically dispersed and time 
shifted. Indeed, as a result of these enabling technologies, the field of Com-
puter Support for Collaborative Work (CSCW) has gained popularity in recent 
years, providing tools and platforms for collaborations among multiple users 
(see [98] and Section 2.3.3). However, these technologies also introduce hetero-
geneity and autonomy as key concerns which will have to be taken in the near 
future as a given requirement, as opposed to a design by choice. 
1.3 
A MOTIVATING EXAMPLE 
The following is a sample process that illustrates the problems in modeling and 
enacting decentralized processes. Assume there are three development teams 
working in separate sub-environments (henceforth SubEnvs) SEl, SE2, and 
SE3, who are responsible for three disjoint components of a system S, labeled 

8 
1-----------
Tools 
~ 
66 
1-~~;:~=~~rQ 
[J-D "'0 
SE20 
Proc ... 
Change 
1. Review and approve (decentralized) 
2. Modify interface (centralized) 
CHAPTER 1 
SE3 o 
Proce .. 
1-----------
Tool. £6 6 
1-----------
D~6 
3. Modify affected components (decentralized) 
4. Unit-Test change (decentralized) 
5. Integration test (centralized) 
Figure 1.1 
A Motivating Example 

Introduction 
9 
S1, S2, and S3, respectively (see Figure 1.1). The teams operate in different 
sites4 and reside in different geographical areas. They each develop their own 
private artifacts (represented as rectangles in the figure), using their private 
tool-set (triangles) and their own methods and policies, i.e., process (clouds). 
Each component can be coded and unit-tested independently, and the compo-
nents are interconnected through published, well-defined, interfaces. Suppose 
S2's interface has to be modified in order to enhance some of its functionality, 
thereby requiring the other components to change. The following steps are then 
taken (corresponding to the numbers in figure 1.1): (1) the proposed change 
has to be reviewed and approved by all SubEnvs; (2) the interface of S2 is 
actually modified; (3) The affected components are modified to correspond to 
the new interface; (4) a local test of each component is performed; and (5) 
an integration test with all revised components is performed. For simplicity, 
only the "successful" path, i.e., assuming that all the steps were carried out 
successfully, is described. (One example of an unsuccessful path would be a 
failure of the local testing at one SubEnv, which might require reiteration to 
step 1.) 
While the global modification and integration test must be performed syn-
chronously (with respect to all sites) and at one site, the review, local modifi-
cation, and local test activities can be performed asynchronously in the local 
sites, and they can differ at different sites. For example, one site might employ 
"white box" local testing, while another site might use "black box" testing. 
Moreover, even identical operations might trigger different related operations 
when issued at different sites. 
At the modeling level, there should be a conceptual framework that allows for 
the definition of interoperability among the autonomous processes on a per 
SubEnv basis, in terms of interactions and information exchange among them. 
At the enactment level, a DEcentralized PCE (henceforth DEPCE) should en-
able and support the execution of a decentralized task that possibly involves 
data from multiple sites. At the architectural level, there must exist an in-
frastructure that is capable of providing mechanisms for consistent and reliable 
access to shared data, communication protocols and capabilities for accessing 
remote data in a proper manner, and a decentralized enactment engine that 
performs well, both in terms of functionality and efficiency. 
4 Although the tenn site usually refers to an administratively cohesive Internet domain 
sharing a single network file system, we will use it throughout the book more liberally to 
denote logical cohesiveness of a computing unit, and therefore will use at times site and 
SubEnv interchangeably. To distinguish between logical and physical sites, we will refer to 
the latter as an Internet domain, or simply domain. 

10 
CHAPTER 1 
For this particular example, a wrong and in some cases impossible solution 
would be to collect all the necessary data from the remote sites to the coordi-
nating site (i.e., SE2), and then carry out all the process steps (and all implied 
and triggered steps) on all data as defined in SE2's process. Besides the obvious 
performance limitations, this approach would be a clear violation of autonomy, 
since each site has its own sub-process for its local activities (e.g., local test-
ing), which may not even be known to the coordinating site. Another possible 
problem might be that some of the tools do not exist in all SubEnvs (e.g., 
because licensing binds a tool to a specific site or host), and other tools can 
be executed only in specific SubEnvs (e.g., special-purpose hardware), which 
necessarily binds the execution of a process step to a specific SubEnv. On the 
other hand, some of the operations involve data from multiple sites and must 
execute at a common location, and all sites have to agree on it. Therefore, the 
solution should enable handling of such work in a manner that retains maxi-
mum process autonomy and operational independence while still providing for 
collaboration and interaction among the processes as needed. 
1.4 
RESEARCH FOCUS 
This research focuses on the modeling (definition) and enactment (execution) of 
interoperability and collaboration among independent, autonomous, and possibly 
pre-existing processes. Within this focus, it is important to clearly identify what 
issues are addressed in this book, and equally important, what issues are beyond 
the scope of this book. 
1.4.1 
Decentralization vs. Distribution 
Broadly speaking, a distributed system is defined as one that provides a sin-
gle, homogeneous, logical perspective to its applications, but is physically dis-
tributed into multiple computing units, usually across machines of a single site. 
That is, a distributed system transparently shields the distribution from its 
applications [26]. In contrast, a decentralized system is comprised ofrelatively 
independent and heterogenous subsystems with some degree of correlation be-
tween them, perhaps (although not necessarily) spread among multiple sites. 
In particular, transparency is intentionally not supported, for several reasons. 
First, it inherently violates site autonomy since it implies unrestricted access to 
at least some remote resources and repositories. Second, transparency is simply 
irrelevant when the involved entities are heterogenous at the application level 

Introduction 
11 
(Le., they run different programs, see Section 1.4.2). Finally, transparency is 
undesirable when the entities are geographically dispersed, since it is often nec-
essary to distinguish between different access costs, given that they can vary 
widely depending on the available bandwidth, available computing resources, 
etc. For example, if a component has a timeout mechanism on fetching ob-
jects for fault tolerance purposes, then it should be aware of the "degree of 
remoteness" of the object being accessed in order to determine when to time 
out. 
Observing the evolution and scaling up of large systems, the natural order 
tends to be: (1) centralized control, (2) distributed control, (3) decentralized, 
or ''federated'' control (the best representative of this kind of evolution is the 
database field). The reader might wonder why skip over step 2 and jump 
directly into step 3, when the problems of step 2 are not yet resolved? The 
answer is, that if transparency shields users and applications from knowing 
where the data is, and retains a uniform view of the data and the process, 
then the main problem becomes to provide this transparency. From the PCE 
research aspect, it is much more interesting to look at loosely coupled and 
autonomous systems that allow for different processes to coexist. Furthermore, 
environment distribution is a form of "vertical" scale-up, in that it allows for 
more users to work, but under the same process and within some bounded 
physical distance (typically a local-area network). In this research we explore 
mainly "horizontal" scale-up, where the number of users per group sharing the 
same process may not grow much (and in fact may consist of a single user), but 
the number of groups may be arbitrarily large, each group with its own private 
process and data but collaborating in a concerted effort with the other groups. 
1.4.2 
Process, Language and System 
Heterogeneity 
Heterogeneity can be categorized into three levels: system, language, and ap-
plication. For example, in a heterogenous database system, the multiple local 
databases can differ from each other in their inner structure (system heterogene-
ity), they can differ in their front-end Data Definition Language (DDL) while 
still having similar inner structure (language heterogeneity), and they can sup-
port different schemas at the different databases, written using the same DDL 
(application heterogeneity). Similarly, DEPCEs can have system heterogeneity 
by allowing different product databases to be used in different sites. They can 
support language heterogeneity by allowing different sub-processes to be writ-

12 
CHAPTER 1 
ten in different PMLs, and they can allow different processes written with the 
same PML to interoperate (application heterogeneity). 
Each level can be further categorized based on the degree of heterogeneity. 
For example, system heterogeneity can vary from component difference (e.g., 
different databases) to more substantial architectural difference (e.g., message 
bus vs. client server). Also, there can be different combinations of the above. 
For example, there could be support for multiple languages to define different 
aspects of a single, centralized process, or there could be support for interop-
erability between different subsystems with different corresponding PMLs that 
are used for support of different aspects of the same process. Thus, while de-
centralization usually implies heterogeneity, the reverse is not necessarily true. 
Support for heterogeneity is, in general, an extremely difficult problem. This 
book explores a limited aspect of heterogeneity within the context of decentral-
ization. The system and language levels are fixed, that is, it assumes, for the 
most part, homogeneity at the system level and at the PML level, and focuses 
on heterogeneity at the process level. 
That is not to say that the other aspects of heterogeneity are totally ignored. 
For example, it is hoped that the language level can be addressed by translating 
various PMLs to an underlying "assembly" PML, similar to the approach taken 
in some Heterogenous Distributed Data Bases (HDDBs) (e.g., [81]). In fact, 
some evidence that this approach is feasible was given by the work described 
in [73]. Furthermore, in investigating architectural support for decentralization, 
some level of system heterogeneity, namely componentization, is considered 
and is one of its guidelines (see Chapter 5). Nevertheless, multi-PML support 
and componentization are, by-and-Iarge, guidelines (and constraints) on this 
research as opposed to subjects of the research, and are partially addressed 
in [104] and [54], respectively. 
Finally, while this research focuses on process heterogeneity and assumes ho-
mogeneity at the language level, it does not restrict itself to a specific PML, 
but instead attempts to provide a high-level abstract model that can be imple-
mented by a family of PMLs. 
1.4.3 
Bottom-up vs. Top-down 
As mentioned above, this research investigates interoperability between mul-
tiple, possibly pre-existing, processes. This implies a bottom-up view on the 

Introduction 
13 
construction of a multi-site environment, without necessarily having any a pri-
ori knowledge of the "neighboring" processes at the time of construction. This 
is in contrast to decomposing a single process in a top-down fashion into sub-
processes with predefined and coordinated interfaces. There are architectural 
implications which further distinguish between the two approaches, as will be 
seen in Chapter 3. 
1.4.4 
Inter-Process Collaboration vs. 
Intra-Process Coordination 
Intra-process coordination is concerned with coordinating concurrent activities 
that might violate the consistency of the project database, assuming that all 
participants use the same process, the same schema, and most importantly, 
share the same centralized, project database. In contrast, this research is 
about collaboration between users or teams with different processes, different 
schemas, and most importantly, different project databases. (Work on intra-
process coordination has been investigated in the Marvel project, primarily by 
Barghouti [8], and is discussed briefly in Section 2.2.) 
1.4.5 
Logical and Physical Decentralization 
Logical decentralization refers to multiple autonomous and heterogenous pro-
cesses which are enacted separately but are physically co-located (i.e., operate 
within the same local area network), and physical decentralization adds the 
dimension of physical separation between the SubEnvs, with arbitrary com-
munication bandwidth between them. Physical decentralization obviously has 
implications on the architecture (e.g., variable bandwidth, no shared file sys-
tem, security, see Section 5.6), but is likely to also affect the general model. 
Nevertheless, the problem of providing interoperability among heterogenous 
and autonomous processes can be examined independently of the additional 
constraints and problems associated with having those processes enacted in 
arbitrary physical separation. Thus, while we discuss in length the architec-
tural considerations and the actual design of a DEPCE that supports physical 
dispersion among the SubEnvs, the generic model is at the logical level and 
applies to both. 

14 
CHAPTER 1 
1.5 
REQUIREMENTS 
We now formulate the general problems and motivations discussed above into a 
set of well defined "research requirements" which guide the design of the model 
and its realization. This is a high-level overview of the requirements. More 
detailed refinements of these requirements are given in the relevant chapters, 
as the context to understand them builds up. 
1. Process Locality -
A basic requirement is that as far as local work is con-
cerned, a DEPCE should provide the same capabilities and same support 
as a PCE does. The underlying assumption is that most of the work done 
by a site is local to that site, and therefore each site should still be opti-
mized towards local work. Thus, a DEPCE subsumes the capabilities of a 
PCE. We address here mainly the additional requirements for a DEPCE 
that are not PCE-specific, and extensions to existing peE requirements. 
( [21] discusses general requirements for a PCE). 
2. Process Autonomy -
Each local SubEnv should have complete control 
over its process and data, while allowing access by remote SubEnvs under 
restrictions that are solely determined by the local SubEnv. Access to a 
process has two perspectives: (1) access to the local artifacts owned by 
the process through a process interface; and (2) access to, and use of, the 
process itself. Autonomy constraints imply that, by default, a site allows 
no sharing of process or data. Moreover, once defined, sharing should 
be minimized to the degree necessary for interoperation. Autonomy is a 
strong requirement with major influence on all aspects of this research. 
3. Process Collaboration and Interoperability -
Autonomy trivially exists 
when there is no possible interaction between the SubEnvs. However, we 
are interested in allowing interoperability between multiple independently-
defined heterogenous SubEnvs, each serving a group of users. Collabora-
tion in this context refers to not only enabling "read-only" access to remote 
process and data, but also enabling operations that might affect the pro-
cess state and the product data of remote processes. Such interoperability 
is particularly difficult in the context of processes, because of the richness 
of the semantics associated with process and its modeling power. 
4. Independent Operation and Self-Containment -
Related to autonomy, this 
means that a SubEnv should be able to behave as a complete environment 
by itself when not collaborating with any other SubEnvs, and SubEnvs 
must be able to operate concurrently and independently, except when their 
processes explicitly collaborate. The most fundamental implication of this 

Introduction 
15 
requirement is that it requires a "share-nothing" architecture. That is, 
no service, mechanism, or data, in the environment can be centralized 
or shared across SubEnvs, and all interaction should be based solely on 
message passing. An additional implication of this requirement is that 
SubEnvs must be prepared to be dynamically disconnected/reconnected 
from/to each other when interoperating, without disrupting the operation 
of local tasks in individual SubEnvs, and moreover, they cannot rely on 
having all sites always active. Besides the architectural implications, this 
requirement also effects the conceptual model in various ways, as will be 
seen in Chapter 3. 
5. SubEnv Interconnectivity -
To support process interoperability, there must 
be an infrastructure that supports connectivity between the participating 
SubEnvs. This includes a name service to identify and address SubEnvs 
and a communication service to exchange messages between them. Auton-
omy and independence constraints exclude a centralized name service and 
require a mechanism to (re)connect to, and automatically invoke, remote 
SubEnvs. 
6. Dynamic Reconfiguration -
A related issue to interconnectivity is that of 
site configuration. A DEPCE should have the capability to dynamically 
add new SubEnvs and remove inactive SubEnvs without disrupting the 
operation of the currently active SubEnvs. The concept of an inactive 
SubEnv is a direct implication of requirement 4. An inactive SubEnv 
is one that is temporarily dormant, although it may have recently been 
active, and could potentially be active in the future. In contrast, an active 
SubEnv is one that is currently under execution. 
7. Support for Pre-existing SubEnvs -
A DEPCE should enable a SubEnv 
with a pre-existing process to "join" a global environment with other 
pre-existing process(es), with minimal overhead. Similarly, a "split" of 
a SubEnv from its current global environment should be supported. This 
requirement is important when two or more organizations with established 
processes need to collaborate for a limited time. 
8. Data Sharing, Querying, and Presentation -
While remote data access 
should not be transparent in a DEPCE and governed by the process op-
erating on it as discussed earlier, there should still be a mechanism that 
enables sites to access, query, and browse through data residing at remote 
sites, provided that the access is granted by the owner process. Moreover, 
since PCEs often support complex and highly structured data models, it is 
especially desirable to be able to display graphically the types of data and 
the relationships among them. This represents a challenge both in user 

16 
CHAPTER 1 
interface design, and in the communication protocols that are responsible 
for updating the user's view(s). Chapter 5 elaborates on this issue. 
9. Transaction Support- Multi-user SDEs in general, and PCEs in particu-
lar, require sophisticated and flexible concurrency control (CC) and fail-
ure recovery mechanisms [11, 23]. DEPCEs add the dimension of remote 
vs. local access and potential heterogeneity of transaction management 
policies. This complicates CC and recovery because extended transaction 
models devised for centralized, and even distributed systems, might not 
be adequate. For example, if semantics-based CC is employed, then dif-
ferent processes impact their local CC policy differently, requiring some 
sort of negotiation between local CC engines. Furthermore, operational 
independence excludes any sort of centralized transaction service. 
10. Flexibility -
This is perhaps the most important (meta) requirement. It 
is concerned with the general approach to be taken to fulfill all the above 
requirements. That is, analogous to one of the most important characteris-
tics of PCE technology -
modeling a process on a per-project basis and its 
enactment by a process-sensitive engine -
a DEPCE should similarly pos-
sess such flexibility and be able to specify the degree of autonomy as well 
as the collaboration on a per-SubEnv basis, and not by a hard-wired pol-
icy. Flexibility, autonomy and independent-operation requirements have 
been most influential on this research, and are henceforth referred to as 
the "core research requirements". 
To summarize, this research attempts to provide an enabling technology for 
process-interoperability. The underlying theme in the requirements is to pro-
vide both autonomy and interoperability, which are often conflicting goals. 
Obviously, some compromise between these two is necessary. The idea is to 
apply suitable modeling and enactment facilities that will minimize the impact 
that SubEnvs can have on one another besides what they have explicitly agreed 
upon. 

2 
PREVIOUS AND RELATED WORK 
While decentralization, heterogeneity and interoperability have been active re-
search topics in several communities -
databases, engineering, and distributed 
systems in general -
these issues have been mostly unexplored in the PCE 
community until recently, mainly because the field is relatively young and the 
state-of-the-art in PCE technology was too immature. Recently, however, there 
has been a surge of activity in this area, and these issues have also been ac-
knowledged as one of the main future research directions [100] and seem to 
be a natural evolution of peE technology. Nevertheless, most of the work to 
date has been still on interoperability and heterogeneity under a single process 
and/or under centralized control with a centralized shared database. 
This chapter is divided into three major parts. The first part (Section 2.1) 
surveys SDEs and PCEs that address some levels of heterogeneity and/or de-
centralization. The second part (Section 2.2) gives a detailed account of Marvel, 
the predecessor to Oz, from which many concepts (and code) were inherited. 
This section is important mainly for the understanding of Chapters 4 and 5, 
where the Oz system is presented. Finally, the third part (Section 2.3) surveys 
work that has been done in related fields, namely databases, heterogeneous 
processing, and CSCW. 
2.1 
peES 
ISTAR [33], one of the earliest SDEs (or "Integrated Project Support Environ-
ments"), provided comprehensive support to the software development lifecycle, 
including both management and software engineering. The main idea in ISTAR 

18 
CHAPTER 2 
is the contractual approach, in which a "contractor" (e.g., a group of program-
mers) provides services to a client (e.g., manager). The contract must have 
well defined deliverables and acceptance criteria, and might include additional 
constraints imposed by the client. A contractor can further delegate some of 
the tasks to a sub-contractor, creating a "contract hierarchy" in a top-down 
fashion. In addition, the ISTAR architecture permits for sub-contracts (and all 
of their sub-contracts, recursively) to operate autonomously in different sites, 
since the contract databases are distinct and can be operated independently. 
Although ISTAR was not a PCE (it had a somewhat hard-coded process), its 
architecture is an important (and somewhat neglected) step towards decentral-
ization. 
Shy, Taylor, and Osterweil were among the first to explicitly identify decentral-
ization as a key environment technology [116]. Their theoretical work draws an 
analogy between software development and the business corporation, and they 
advocate a "federated decentralization" model for PCEs with global support 
for environment infrastructure capabilities and local management with means 
to mediate relations between local processes. Among the arguments made for 
this model (as opposed to "corporate autocracy" or "radical decentralization") 
are: (1) The level of global support is not rigid. (2) While the communication 
is established under guidelines determined by the global process, the actual 
communication is provided and maintained under the control of the local en-
tities. (3) Extensibility, because integration of processes and services can be 
implemented gradually. This preliminary model, while advocating decentral-
ization, still considers every sub-environment to be strongly affiliated with the 
corporation and necessarily abiding by some global rules. Thus, autonomy is 
necessarily restricted a priori. 
Heimbigner argues in [50] that just like databases, "environments will move 
to looser, federated, architectures ... address inter-operability between partial-
environments of varying degrees of openness". He also notes that part of the 
reason for not adopting this approach until recently was due to the inadequacy 
of existing software process technology. However, his focus is on support for 
multiple formalisms. His proposed ProcessWall [51] is an attempt to address 
heterogeneity at the language level. The main idea in the ProcessWall is the 
separation of process state from the programs that construct the state; in theory, 
multiple process formalisms (e.g., procedural and rule-based) can co-exist and 
be used for writing fragments of a process. However, decentralization as a 
concept is not addressed, and in particular, the process state server is inherently 
centralized. 

Previous and related work 
19 
Peuschel and Wolf explain why current client-server architectures are inade-
quate to support distributed software processes [103]. They identify four alter-
natives for distribution: (1) hierarchical process organization; (2) distributed 
process data; (3) distributed process engines over local area network; (4) dis-
tributed process engines over wide area network. They further propose four ar-
chitectural approaches to meet these requirements, concluding that distributed 
process engines with only partially distributed process data -
including a 
common process database that serves as a communication platform among the 
process engines -
is best. Once again, this proposal excludes the possibility 
of a "shared-nothing" architecture. 
Kernel/2r [61], from the Eureka Software Factory project, supports a special 
case of process formalism interoperability. The system identifies and divides 
the process into three distinguished kinds of process fragments, each with a 
separate process engine (and PML). The interworking process engine, MEL-
MAC [30], supports cooperation between teams or within a team. An instance 
of the interaction process engine, WHOW, supports a single user working with 
a variety of tools to create, manipulate and delete development materials. The 
interoperation support, through the MUSE software bus, behaves like a process 
engine in that it controls partially ordered sequences of tool invocations where 
human intervention is not required. Although Kernel/2r does not directly sup-
port collaboration among multiple independent processes, MEL MAC can, in 
principle, interface to teams who use another PCE (or who are not concerned 
with process at all). 
ProcessWEAVER, another spin-off from the Eureka Software Factory, is a com-
mercial product of Cap Gemini Innovation, with a Petri-net based PML. Fern-
strom describes " .. .in a process, which consists of a set of cooperating sub-
processes, every sub-process can be characterized by the set of 'services' it 
provides and requires from the other sub-processes" [38]. This sounds like 
a decentralized approach. However, in the ProcessWEAVER system, " ... pro-
cesses are recursively structured into sub-processes of finer and finer granular-
ity and detail." In other words, processes are defined top-down, and provide 
essentially for fine-grained decomposition of one global process, whereas our 
requirements imply that what is in effect the decentralized process of a global 
environment should be defined bottom-up from the (collaborating) processes 
of the constituent SubEnvs. Finally, autonomy concerns for local process and 
their artifacts, which is a fundamental requirement in our approach, is not 
considered. 
SMART [41] is an attempt to provide a methodology and a supporting technol-
ogy for the process (as opposed to product) lifecycle through multi-formalism 

20 
CHAPTER 2 
support, whereby different phases in the lifecycle are supported by different 
formalisms and corresponding (sub )systems. Specifically, SMART views the 
lifecycle of a process as consisting of a development phase; followed by analysis 
and possibly a simulation phase; followed by an embedding phase, in which 
a process model is instantiated with actual tools and product data bound to 
it; followed by an execution and monitoring phase, which feeds back to the 
development phase. Modeling, analysis, and simulation are performed with the 
Articulator system [92], process execution is performed by HP's SynerVision, 
and Matisse [42] (also from HP) is used to maintain a knowledge-base contain-
ing the artifacts that represent the process models developed in the Articula-
tor, and serves as an integration medium between Articulator and SynerVision. 
Thus, the emphasis is on multi-paradigm support for the process, and on bi-
directional translation: from process models to process (executable) programs, 
and from the process execution state back to the process model level. From a 
heterogeneity standpoint, SMART can be categorized as having some degree 
of system heterogeneity, since it integrates three different systems, and formal-
ism heterogeneity, although not for defining different aspects of the process 
(as in ProcessWall), but rather for supporting different phases of a predefined 
lifecycle. However, there is no support for multiple processes with distinct 
instantiated products. 
TEMPO [15] is a peE that is designed to support "programming-in-the-many" , 
i.e., projects that involve a large number of people, and therefore its emphasis is 
on modeling and mechanisms for supporting collaboration, coordination, and 
synchronization between project participants. TEMPO provides three main 
abstractions that facilitate modeling multi-user aspects of the process: (1) 
hierarchical decomposition of processes to sub-processes in a top-down fash-
ion, similar to ProcessWEAVER; (2) support for multiple private views of the 
process, through the role concept which allows to define private constraints 
and properties; and (3) active and programmable connections between role in-
stances, which are defined and controlled by rules with temporal constraints 
in addition to pre- and post-conditions. TEMPO is data-centered, and is built 
on top of Adele [14], an active object management system with data-driven 
triggering, which enables to realize rule processing in TEMPO. While TEMPO 
provides for definition of "personal" processes and supports coordination among 
them, it is still inherently centralized, in that it requires a single database as 
the coordination platform, and supports multiple views of essentially a single 
group process, defined in a top-down fashion. 

Previous and related work 
21 
2.2 
MARVEL: THE PREDECESSOR PCE 
This section gives a relatively detailed overview of Marvel, for three reasons: 
First, to make more concrete the general concepts referred to throughout the 
book, since Marvel is fairly representative as well as one of the best-known 
of the early 1990's generation of research projects in process-centered envi-
ronments [29]. Second, to introduce system-specific terminology, since large 
portions of Marvel were (re)used in Oz; from the end-user's perspective, a 
single-site Oz environment appears and behaves nearly the same as a Marvel 
environment -
although the support for delegation and computer-supported 
cooperative work is a major innovation (see Section 4.5). Third, to clearly dis-
tinguish the new research presented in this book from the work that was done 
earlier in the context of the Marvel project. 
It is useful to note that Marvel went through a series of significantly different 
revisions. Marvel version 0 [72] was implemented at the Software Engineering 
Institute in proof-of-concept form in 1986, by modifying a multi-user C pro-
gramming environment called Smile [71] that had been developed at Carnegie 
Mellon University around 1980, and bears only faint resemblance to the later 
versions developed at Columbia University (none of the version 0 code was 
reused). Version 1 [69] from 1987-88 employed Smile only as its back-end 
database and provided a similar textual command-line user interface, but did 
not have any particular bias towards supporting software development in C 
(or any other programming language). The version 2.x series [70] in 1989-90 
replaced Smile with a home-grown object management system and added a 
rudimentary X windows graphical user interface; only a single user (at a time) 
was supported. 
Finally, the version 3.x series [21, 59] of 1991-93 began with major conceptual 
and implementation changes to arrive at a client/server architecture that sup-
ported multiple users, and soon added a sophisticated XView user interface. 
Although Marvel's main target was always understood as software engineering, 
particularly the downstream phases of the lifecycle, we have also constructed 
Marvel environments for a variety of purposes -
ranging from document 
processing to course teaching assistant support to a (toy) blocks world prob-
lem [56]. Starting with 3.0, we used successive versions of Marvel to support all 
our own software development as well as for producing the manuals and other 
technical writing. The initial version of Oz was developed using a Marvel 3.1.1 
environment instance, but we migrated to Oz for the manuals in late 1993 and 
for software development in mid-1994. All versions of Marvel were written in 

22 
CHAPTER 2 
C; version 3.1.1 runs on SparcStation Sun OS 4.1.3, DecStation Ultrix 4.3 and 
RS6000 AIX 3.2, and consists of about 154,000 lines of code. 
Throughout this book, all references to Marvel should be understood as refer-
ring to the final Marvel release, version 3.1.1. The initial version of Oz de-
scribed in this book could have been deemed Marvel version 4.0, but we chose 
a new name to signify a substantial departure from our previous research. 
In a nutshell, Marvel is a highly tailorable PCE that supports project-specific 
definitions for the data model, process model, toolbase, and coordination model 
in special purpose languages. A Marvel environment instance cannot operate 
without these definitions, but they need not be of concern to the average end-
user. The project-specific definitions would normally be written, tested and 
installed (and evolved as the project needs change -
see Section 2.2.6), by a 
trained process designer (a process administrator in Marvel terminology). The 
runtime support for these notations (e.g., the process engine) is based on a 
client-server architecture that supports multiple users and executes a shared 
process on a centralized project database. We now discuss each major aspect 
of the system separately. 
2.2.1 
Data Model 
The data model defines an object-oriented schema for the product data (the 
software artifacts under development) and, possibly, the process data (state in-
formation used to track the ongoing process). The objects and their attributes 
are then available for reference in the process model, and access and manipu-
lation by the process engine. This contrasts with some PCEs, where only the 
process state is directly accessible to the process engine, and the real product 
data is external (e.g., in the native file system) and managed by the end-users 
or some other component of the environment (typically tool envelopes, see 
below). This alternative view has been taken in several experimental Marvel 
instances [40, 104, 2]: Marvel does not in any sense force the process designer to 
include product data schema in the data model (or the corresponding instances 
in the process engine's objectbase). However, since most of our experience in 
Oz as well as Marvel has been with environment instances where the prod-
uct and process data are indeed tightly coupled, we assume this methodology 
throughout the book. 
As in most object-oriented databases, an object in Marvel has a unique identity 
and an associated set of data attributes, declared by a statically defined class. 

Previous and related work 
23 
The equivalent of the "methods" of persistent object-oriented programming 
languages are represented as a set of process steps that take an instance of a 
specified class as a parameter, but these are defined separately in the process 
model (see Section 2.2.2). Process steps can also be formulated as "triggers" 
to be executed when an object is updated. Multiple-inheritance is supported 
in the conventional manner, i.e., the class lattice is a directed acyclic graph, 
and subclasses denote specialization and extension of their superclasses. Both 
attributes and process steps defined on a subclass override those with the same 
names declared for their superclasses. "Multi-methods", in the form of pro-
cess steps with multiple parameters typed by the same or different classes, 
are also supported -
with a distance-based metric for resolving inheritance 
conflicts [10]. 
Marvel objects are relatively coarse-grained, often representing one or more 
product files, as opposed to fine-grained objects representing, e.g., abstract 
syntax tree nodes. A Marvel class may declare any combination of four kinds 
of attributes: state, file, composite, and reference link. The first two kinds 
of attributes contain the "contents" of objects whereas the last two denote 
relationships with other objects. State attributes have primitive (integer, string, 
etc.) or enumerated types, and are used mainly for process data (although they 
can be used to hold product data). File attributes can be either text (ASCII) 
or binary, and are usually used to maintain product data in conventional files. 
File attributes are implemented as a file-system path pointing to the file in 
a "hidden" file system (HFS), which is constructed and managed by Marvel's 
object manager. The objectbase is intended to abstract the file system, and 
end-users are not intended to access the HFS directly. Paths to files outside 
Marvel's HFS can be represented by string attributes, to be interpreted by tool 
envelopes (see Section 2.2.3); however, this practice is not recommended since 
Marvel can then provide no guarantees regarding process enforcement or task 
atomicity (see Section 2.2.5). 
Composite attributes denote an "is-part-of" relationship between objects to 
form a composition hierarchy (i.e., when an object is deleted, copied or moved, 
all objects reachable transitively through its composite attributes are also 
deleted, copied or moved, respectively). Finally, reference link attributes (or 
simply links) represent any arbitrary semantic relationship between objects. 
Both composite and link attributes are typed, allowing an instance of any sub-
class of the given type class. (The type may be ENTITY, the built-in root class 
of the inheritance lattice, in effect permitting any object.) Both composite and 
link attributes allow the process designer to specify whether an arbitrary num-
ber of objects can be contained/linked (by the set_of construct) or whether 
only a single object is permitted. 

24 
CHAPTER 2 
The general structure of an instantiated Marvel object base can be viewed as 
a forest of trees, each of which represents a composite object, with additional 
links between objects across and within the trees. Each top-level object must 
be an instance of a class named GROUP (for historical reasons). 
The Marvel object-oriented database management system also includes a declar-
ative query language featuring both navigational and associative access, includ-
ing complex queries combining both modes, and supporting ad hoc queries from 
the user interface as well as queries embedded in process steps. The ad hoc 
notation is essentially the same as the embedded form, whose functionality is 
discussed briefly in Section 2.2.2. The Free Software Foundation gdbm byte 
server is used as a back-end storage manager. 
Figure 2.1 shows three representative classes, the first one of the six from the 
standard access control support (see Section 2.2.7) and the latter two from 
among the 34 classes defined specifically for C/Marvel, the process used for 
developing Marvel itself. The MINIPROJECT class, representing a private user 
workspace, inherits attributes from its superclasses BUILT (not shown here) 
and PROTECTED..ENTITY (part of the optional access control support, see Sec-
tion 2.2.7). Both BUILT and PROTECTED..ENTITY are examples of what are often 
termed "mix-ins" in the object-oriented programming language community: 
they are not intended to be instantiated, but serve only as superclasses to en-
capsulate common attribute sets and behaviors (i.e., the corresponding rules). 
MINIPROJECT defines the following attributes: config is a state attribute of 
type string that denotes general configuration in which the project under the 
local workspace has to be built; it is used to direct the chaining to configuration-
specific rules. options is another string attribute that is used to supply an 
argument list for tool envelopes. Example values include debugger flags, pre-
processor directives and platform on which the project is being built. exec is 
a single composite attribute that holds a single object of class EXEFILE (not 
shown here) that represents an executable program. As in conventional object-
oriented design, there are tradeoff's in using attributes vs. classes to represent 
entities. For example, an alternative schema design could represent the exe-
cutable program as a file attribute of MINIPROJECT, and augment MINIPROJECT 
with the state attributes of EXEFILE; this design would save space and reduce 
access costs but would be less modular and less reusable. In particular, since 
we had to use EXEFILE in various other parts of the schema we preferred the 
former approach. files is a file (set) attribute that contains a set of objects of 
class FILE. Note that any object that is instantiated from any of the sub-classes 
of FILE qualifies. For example, we use this attribute to contain CFILE (C source 
files), HFILE (C header files) and DOCFILE (documentation files) objects. The 
last three attributes in MINI PROJECT are link attributes: glb_exe is a single 

Previous and related work 
PROTECTEDJ&NTITY:: 
superclass ENTITY; 
owner: 
user; 
perm-Btring: 
string = "rwad rwa*"; 
end 
AFILE :: superclass ARCHIVABLE, RANDOMIZABLE, HISTORY, 
PROTECTEDJ&NTITY; 
end 
machines 
config 
MINI PROJECT 
config 
options 
exec 
files 
glb_exe 
includes 
afiles 
end 
set_of MACHINE; 
string = "MSL"; 
superclass BUILT, PROTECTED-ENTITY; 
string; 
string; 
EXEFILE; 
set_of FILE; 
link EXEFILE; 
set_of link INC; 
set_of link AFILE; 
Figure 2.1 
Several Classes from C/Marvel 
25 
link that points to a stable version of the executable program (in a "master" 
workspace); includes points to sets of header files that are included by source 
files in the local workspace; and afiles points to sets of archive libraries that 
are used to build the executable. 
2.2.2 
Process Model 
The process model is specified in a rule-based process modeling language, called 
the Marvel Strategy Language (MSL). (The data model notation above and 
tool declarations later on are also part of MSL, but for the purposes of this 
discussion we treat them separately.) The term "strategy" refers to a modular 
unit defining a portion of the data model, process model and toolbase of an 
environment. In practice, the data model has usually been relegated to a 
single strategy included (using syntax that effects the C header file sense of 
"include") by process model/tool base strategies, but need not be. Strategies 

26 
CHAPTER 2 
do not support true information hiding, since everything defined in an included 
strategy becomes available to the including strategy as well as of its other 
included strategies 
Each process step is encapsulated in a rule. Although various forms of rules 
(based on production systems, logic programming, and/or planning) are prob-
ably the most popular approach, competing process modeling languages de-
scribed in the literature employ Petri nets, grammars, task graphs, and other 
paradigms to define process models [79]. We have found it relatively easy to 
write translators from various other notations into MSL rules (see [74,55,105]), 
and sometimes think of MSL as a "process assembly language" [68] and Mar-
vel's process engine as a "process virtual machine" [4]. 
Unlike some other peEs, Marvel's process steps (MSL rules) are not repre-
sented as first-class objects in the data model and objectbase but instead in 
an internal form that supports efficient rule chaining (see Section 2.2.5) -
al-
though the user interface does provide means for inspecting the rule base in 
various ways. However, one experimental Marvel instance [40] as well as a re-
leased version of Marvel with an alternative process modeling language [74], 
both based from the process designer's perspective on "activity structures" 
(constrained expressions [111, 3]) rather than rules, reify the process model 
itself as a hierarchy of first-class objects representing individual activities and 
concurrent process segments, respectively. 
A Marvel rule has a (not necessarily unique) name, zero or more typed formal 
parameters (each type is either a class in the data model or LITERAL), and three 
optional constructs: condition, activity, and effects. A rule may be prefixed by 
the hide directive, which means it is intended to be triggered automatically 
during chaining or explicitly by a privileged process administrator, but cannot 
be invoked from the regular end-user interface (and does not appear in the user 
menu). 
The rule condition consists of two subparts: bindings and property list. A rule 
binding specifies a quantified local variable (also known as a derived parameter 
in Marvel terminology, to distinguish it from regular rule parameters) to which 
objects are bound, a class restriction on the allowed bindings (which may be 
the root ENTITY class, effectively allowing any object to be bound), and an 
objectbase query that determines the binding set (of zero or more objects). 
There may be an arbitrary number of bindings listed to be executed in the 
given sequence. A query may include a complex logical condition consisting of 
conjunctions and disjunctions on predicates over the actual parameters, vari-
ables bound earlier in the bindings sequence, literal values and the results of 

Previous and related work 
27 
various built-in functions such as CurrentUser and CurrentTime. Queries are 
explained further below. 
A property list is also a complex logical condition consisting of conjunctions 
and disjunctions on such predicates, but it returns a boolean value rather than 
binding a set of objects to a local variable. If the property list evaluates to 
true and every existentially quantified variable is bound to at least one object, 
whether or not it is referenced in the property list, the condition is said to be 
"satisfied". The specification of variable quantification (foraH and exists) 
in the bindings is a historical idiosyncrasy in the design of MSL, since this 
information is used only when evaluating the property list 1. If the quantifier is 
universal, then all objects in the binding set of that variable must satisfy the 
condition, and if it is existential then at least one object in the set must satisfy 
the condition for the whole condition to yield a true value. Marvel enforces 
that the property list must be satisfied prior to execution of the rule's activity; 
it is simply impossible to invoke the activity otherwise. 
The predicates in a query or a property list may be either structural or associa-
tive. Structural (or navigational) predicates navigate the objectbase to obtain 
ancestors or descendants (in the composition hierarchy) of specified types, con-
tainers or members of composite attributes, or objects linked to or from other 
objects. 
Associative predicates select from the objectbase all those objects 
satisfying a relation (equality, inequality, less than, etc.) specified between at-
tributes of two parameters and/or variables, or between such an attribute and 
a literal. A query can be composed of several nested sub-queries connected by 
AND or OR, each of which can be either navigational or associative (figure 2.2 
shows some composite binding queries). A declarative syntax is employed for 
both kinds of predicates; this is different from many object-oriented database 
systems, which require imperative code to effect navigational queries and often 
do not support associative queries at all. 
The formal parameter types and condition are used to distinguish among mul-
tiple rules with the same name. As mentioned previously, rules are treated 
as (multi-)methods and conventional inheritance mechanisms and (dynamic) 
rule overloading mechanisms are applied to select the rule to invoke. But there 
may be several rules with the identical signature, in which case only the first 
rule whose condition is satisfied (or can be satisfied by backward chaining, see 
Section 2.2.5) is actually "fired", meaning the activity and one of the rule's 
effects are executed. The rules are considered for execution in a rather opaque 
1 This has been corrected in a new more general process engine, called Amber [126], which 
like Oz is based ancestrally on Marvel. 

28 
CHAPTER 2 
implementation-specific manner, which in effect assumes that the conditions 
are mutually exclusive. 
An activity indicates invocation of an external tool to operate (usually) on 
product data. (The toolbase, comprising declarations of external tools and 
interfaces to those tools, is discussed in Section 2.2.3.) If there is no activity, 
then by definition there can be only one effect -
which is an immediate 
implication of the rule's condition. A rule with no activity is called an "inference 
rule" in Marvel terminology. 
If there is an activity, then in general the tool may have several possible termi-
nation results mapped one-to-one with the given effects. A non-empty activity 
specifies a tool declaration (through which the proper envelope is determined), 
its input arguments -
which may be literal values, state attributes and/or 
file attributes (of the parameters and bound variables), and output arguments 
-
which may be file attributes (in the form of subsets of input file attributes) 
and primitive values bound to special local variables intended to be used in the 
effect. In addition to output arguments and perhaps side effects on the contents 
of input file attributes, each envelope returns a code that uniquely selects one 
of the specified effects (0 for the first effect, 1 for the second, etc.). Return 
codes that do not map to some predefined effect (except for -1, which denotes 
exception in the tool invocation) are ignored. 
Finally, a rule's effects are mutually exclusive in the sense that only one effect 
is asserted at the end of a rule invocation, as determined by the return code 
from the activity. Each effect consists of a sequence of assertions. An effect 
assertion assigns the specified literal or activity output value to an attribute, 
or applies any of Marvel's built-in add, delete, move, copy, rename, link and 
unlink operations. An assertion always updates every object in the binding set 
of the indicated parameter or local variables, for both universal and existential 
quantification. 
A sample rule, one of the 184 in C/Marvel (overloaded or hidden such that 
only 35 unique rule names are visible in the user menu), is shown in Figure 2.2. 
This archive rule accepts one parameter of class MODULE. It has six composite 
binding expressions, a property list expression, an activity that takes three 
arguments, each of which can be possibly bound to a set of objects, and two 
effects. (We will return to this rule later in Section 2.2.5.) 
Rules are interrelated by means of automatically determined matchings between 
assertions in the effect of one rule and predicates in a condition (either the bind-
ings or property list) of another rule -
or even the same rule since cycles are 

Previous and related work 
archive [?m:MODULE]: 
### bindings 
(member [?m.cfiles ?c]) 
(and (forall CFILE ?c suchthat 
(and no_chain 
(or 
(?c.config = ?m.config) 
(?c.possible..config = '"'»» 
(forall YFILE ?y suchthat 
(and no_chain (member [?m.yfiles ?y]) 
(or 
(?y.config = ?m.config) 
(?y.possible..config = 1111»» 
(forall LFILE ?x suchthat 
(and no_chain (member [?m.lfiles ?x]) 
(or 
(?x.config = ?m.config) 
(?x.possible..config = 1111»» 
(forall MODULE ?child suchthat 
no_chain (member [?m.modules ?child]» 
(exists AFILE ?a suchthat 
(and no_chain(linkto [?m.afiless ?a]) 
(?a.config = ?m.config») 
(forall MACHINE ?mc suchthat (member [?a.machines ?mc]») 
### property-list 
(and no_chain 
(?m.archive~tatus = NotArchived) 
no~orward 
(?m.compile~tatus = Compiled) 
no~orward 
(?c.archive~tatus = Archived) 
no~orward 
(?y. archive~tatus = Archived) 
no~orward 
(?x.archive~tatus = Archived) 
no~orward (?chi1d.archive~tatus = Archived» 
ARCHIVER mass_update ?m.log ?a.file ?a.history # activity 
(and 
(?m.archive~tatus = Archived) 
# effect 0 
no_chain (?mc. time~tamp = CurrentTime) 
[?a.archive~tatus = Archived]); 
no_chain (?m.archive~tatus = NotArchived); 
# effect 1 
Figure 2.2 
Example Rule from C/Marvel 
29 

30 
CHAPTER 2 
permitted (they may be required to implement, for instance, the conventional 
edit-compile-debug loop in a software development process). Such matchings 
are compiled for efficiency into a static rule network (the rule network can be 
changed while the process is in progress but quiescent, i.e., no rule chain is 
currently pending; see Section 2.2.6). 
Thus, sequencing between steps in a process can be implicitly defined by match-
ing the condition of one rule and an effect of another rule, and the process exe-
cution engine enforces and, if allowed by chaining directives (see Section 2.2.5), 
automates this (partial) ordering. For example, forward chaining may automate 
a sequence of process steps when the asserted effect of one rule satisfies the con-
dition of another. However, the process is not in any sense limited to a sequence 
of steps. Rule conditions and effects can also be defined to specify alternatives, 
iteration and synchronization, although this may be relatively complex for an 
inexperienced process designer. [74] describes an alternative process modeling 
language for Marvel, called the Activity Structures Language, where such con-
trol flow is explicitly specified in a concurrent regular expression notation; this 
language is translated into rules for execution. 
In practice, only small groups of rules are chained together into "tasks" (most 
peEs have some task aggregation for process steps that are performed to-
gether), and the end-user determines the ordering among tasks via selection 
of one of the "entry points" into the task from the rule menu. Because both 
backward chaining (to do the earlier portion of the task, that is, earlier with 
respect to the chosen entry rule) and forward chaining (to do the later portion, 
again with respect to that rule), there are usually several entry points. It is un-
likely that the process designer would implement the entire process as a single 
task (i.e., rule chain), but it is possible to do. This may be useful for process 
simulation and user training. 
2.2.3 
Toolbase 
The Marvel toolbase consists of two separate but inter-related components: tool 
declarations and tool envelopes (although sometimes only the tool declarations 
are considered part of the tool base proper). A tool declaration specifies a tool 
name and defines a static interface to a list of tool operations, each much 
like a procedure header, from the viewpoint of the process model. A single 
tool declaration may encompass several utilities, perhaps a completely different 
program for each declared operation, but the intent is that the operations 
together constitute a single conceptual tool with respect to the process. A tool 

Previous and related work 
31 
envelope defines the static interface to a tool operation from the viewpoint of 
the executable program that effects that operation, and must be written by 
someone who is highly knowledgeable about how to use that application (but 
no knowledge of its internal implementation is required, i.e., the program is 
treated as a "black box"). The rule activity gives the dynamic interface, as in 
a procedure call; thus the same tool operation may be referenced in multiple 
rule activities. 
One of the 17 distinct tool declarations from C/Marvel is shown in Figure 2.3. 
The notation is somewhat baroque, originally intended to mimic Marvel's class 
definition syntax. For example, ARCHIVER is not a true "class", since there 
are no corresponding "instances" in the objectbase. The actual "instances" 
might be the run-time invocations of the various tool envelopes treated in the 
notation as attributes. Each such attribute consists of an operation name, e.g., 
update_cfile, an envelope name, e.g., update, and class and attribute names 
of the formal parameters to the envelope. In this case, Marvel would expect 
to find a file named update. env in the "hidden" file system containing the 
translated envelope shell script. 
Notice that the formal parameters are not typed in the usual sense, as a string 
or text that could come from any class. Instead, the formal parameters are 
tied directly to classes and their attributes, to reflect semantic meaning and 
appropriateness of the tool operation. Each parameter is optionally followed 
by a lock mode, in the figure either "X" for exclusive or "S" for shared, but 
lock modes and their conflict matrix (e.g., "S" conflicts with "X" but not with 
"S") are not built in and any mode names (e.g., "foo", "bar" and "mumble") 
and conflict matrix could be defined. Locking is used in Marvel for concurrency 
control and recovery (see Section 2.2.4). 
As in some other PCEs, envelopes provide a means for encapsulating tools in 
a "black box" manner -
which is necessary when neither source code nor 
an application programming interface (API) is available, a frequent case for 
commercial off-the-shelf (COTS) tools. A few PCEs assume that new tools are 
implemented specifically for that PCE, and many expect that existing tools are 
modified to match the PCE interface, but the Marvel project has always taken 
the position that users will desire to incorporate their old favorites as well as 
the latest greatest new tools available into their environments -
and thus no 
requirements of any sort may be placed on the tools. 
Marvel envelopes are written in the Shell Envelope Language (SEL) [45], which 
slightly extends the three main UNIX shells -
sh, csh and ksh. The additional 
syntax declares input and output arguments and indicates the return code to 

32 
CHAPTER 2 
ARCHIVER :: 
superclass TOOL; 
update_cfile 
CFILE. obj ectJ:ode S"; 
update_yfile 
YFILE. obj ectJ:ode S"; 
update~file 
LFILE. obj ectJ:ode S"; 
mass_update 
AFILE.history X"; 
mass..randomize 
X"; 
list....archive 
S"; 
randomize 
AFILE.history X"; 
archi ve_out_cf ile 
CFILE. obj ectJ:ode S"; 
archive..out~file 
LFILE.objectJ:ode S"; 
archi ve..out_yf ile 
YFILE. obj ectJ:ode S"; 
archi ve...in_cf ile 
CFILE. obj ectJ:ode S"; 
archi ve...in~f ile 
LFILE. obj ectJ:ode S"; 
archi ve...in_yfile 
YFILE. obj ectJ:ode S"; 
end 
string = "update MACHINE.afile X 
string = "update MACHINE.afile X 
string = "update MACHINE. afile X 
string = "massJlpdate MACHINE. afile X 
string = "mass..randomize MACHINE.afile 
string = "list....archive" MACHINE.afile 
string = "randomize MACHINE.afile X 
string 
"archi ve..out MACHINE.afile X 
string = "archive..out MACHINE.afile X 
string = "archive..out MACHINE.afile X 
string = "archive...in MACHINE.afile X 
string = "archive...in MACHINE.afile X 
string = "archive...in MACHINE.afile X 
Figure 2.3 
Example Tool Declaration from C/Marvel 

Previous and related work 
33 
the rule (to select one of its effects). This syntax is translated into the appro-
priate shell scripting language by a utility, so the actual envelopes executed are 
pure shell scripts. The main purpose of SEL is to interface between Marvel 
objects and the underlying files in the "hidden" file system (HFS) and to hide 
from the envelope writer the details of the HFS, among other reasons to encour-
age that only those files passed as attributes in the rule activity are actually 
accessed. In addition, envelopes enable to return attribute values and a subset 
of objects (from an input object-set) from the shell back to Marvel. A sample 
envelope, used for configuration purposes in Oz, is given in appendix A.2. (SEL 
syntax has been augmented in Oz, among other things to support groupware 
tools (Section 4.5), but is backward compatible. In particular, the envelope 
in the appendix contains only old syntax and can be viewed also as a Marvel 
envelope). 
2.2.4 
Coordination Modeling 
There was substantial research conducted adjunct to the Marvel project on 
extended transaction models and mechanisms supporting cooperative concur-
rency control and recovery policies (see [67, 75, 8, 9, 54, 56]). In fact, Marvel's 
architecture was heavily influenced by, and geared towards, supporting flexibil-
ity in the selection and application of transaction management subsystems [21]. 
For example, data and transaction management were separated rather than 
combined as usual in the object management system. Furthermore, conflict de-
tection (lock management) and conflict resolution (cooperation management) 
were separated within transaction management, to enable flexibility in tailoring 
process-specific and/or project-specific concurrency control policies. 
A Marvel environment instance's coordination model encompasses two distinct 
forms of specification: a set of lock management tables and a set of scenario-
resolution rules (which should not be confused with Marvel's process model-
ing rules). The former can be replaced by a standard set, and the latter is 
entirely optional since the built-in default (when no policies are specified) is 
conventional serializability and atomicity [24]. Coordination modeling and the 
corresponding cooperative transaction management are outside the scope of 
this book, and we address only the relatively conventional atomic rule chaining 
units discussed in Section 2.2.5 and their (less conventional) extension to Oz 
Summits in Section 4.4.5 (see also [58]). 
2.2.5 
Process Execution 

34 
CHAPTER 2 
Process execution is implemented in Marvel by rule chaining. Both forward 
and backward chaining over the rule network are used to automatically trigger 
process step executions in certain situations, as explained below, to guarantee 
atomicity of a process segment or to automate portions of a process fragment 
for the user's convenience. Automation and atomicity, along with enforcement, 
are the three main forms of process execution supported in Marvel. 
Most peEs enforce process step prerequisites, but very few provide any notion 
of atomicity across multiple process steps. Some other PCEs automate process 
sequences in a sense similar to Marvel, but many others guide the user through 
the process by posting newly enabled process steps in an agenda ("to do" list) 
and never initiate a step on their own 2 • 
Marvel's process execution is user-driven, with reactive control: no rule exe-
cution is attempted until a user selects either a particular rule or a data ma-
nipulation command (e.g., add an object, delete an object and its composite 
object hierarchy descendants, rename an object, link a designated attribute 
of one object to another object, etc.) corresponding to one of a set of built-in 
rules (or an overloaded project-specific rule with the same name as one of these 
rules). No backward rule chaining is attempted unless the condition of such a 
rule is unsatisfied, and no forward chaining is attempted unless the effect of an 
executed rule results in actually changing the objectbase. Due to the environ-
ment's refusal to execute activities and effects of process steps whose conditions 
are not satisfied, this is deemed "proscriptive" as opposed to the alternative 
"prescriptive" [49]. Most other PCEs that support process automation instead 
direct exactly what each user must do at each point while the process unfolds 
(if desired, this could be approximated in Marvel by writing the entire process 
as one long chain where the user selects only the start-off rule -
but then the 
process designer would have to take care that each effect in the chain updates 
the objectbase in some way rather than simply reasserting a previous value). 
A Marvel user enters a process step initiation command (either selected from 
the pull-down rule menu or entered as text) with arguments (some may be 
selected by mouse clicks on object icons displayed in the graphical user interface 
while others, such as literals, are entered as text). The process engine then 
2 A proof-of-concept fonn of "guidance chaining", with the process designer's choice of 
guidance versus automation forward chaining indicated by annotations on effect assertions, 
was included in an early version of Oz and later ported to Amber (126). Rules instantiated by 
forward chaining were placed in a designated user or group agenda, allowing for delegation, 
rather than immediately scheduled for execution, so the user or any member of the group, 
respectively, could select to perfonn them at his/her convenience -
assuming they had not 
been done by someone else or otherwise become unnecessary in the meantime. 

Previous and related work 
35 
applies its overloading mechanism to determine those rule(s) with the same 
name and "closest" signature to the provided actual parameters considering 
multiple-inheritance. Then it executes the bindings and evaluates the property 
list. 
If the condition of the rule is not satisfied, backward chaining is attempted: 
One unsatisfied predicate in the property list is selected in an implementation-
specific manner, and those rules with matching effect assertions are automati-
cally attempted in an implementation-specific order (based on the precompiled 
rule network). An attempted prerequisite rule's own condition may itself not 
be satisfied, so the backward chain unfolds recursively. After the selected pred-
icate in the original rule has become satisfied, another unsatisfied predicate (if 
any) is considered until the entire condition evaluates to true. If a particular 
predicate cannot be satisfied after all possibilities for backward chaining from 
it have been exhausted, either the condition is determined to be unsatisfiable 
(at this point in the process -
it may be satisfiable later on) or, in the case 
of a disjunction, another unsatisfied predicate is considered. This algorithm is 
restricted by chaining directives on individual predicates in the process model, 
which may specify no..backward, no..forward or no_chain. In some processes, 
more that half of all predicates are associated with such directives because little 
automation is desired for that particular project. 
If the condition as a whole is already satisfied or becomes satisfied during 
backward chaining, the activity is initiated. After the activity has completed, 
the appropriate effect is asserted. This triggers automatic forward chaining to 
any rules whose conditions become satisfied by the assertion(s) of this effect. 
The asserted effects of these rules may in turn satisfy the conditions of other 
rules, and so on. Eventually, no further conditions become satisfied and forward 
chaining terminates (actually, it is possible foy the process designer to write an 
infinite forward chaining loop, either intentionally or accidentally). Marvel then 
waits for the next user command. 
The appropriate actual parameters for rules invoked through either backward 
or forward chaining are not always immediately apparent (as they would be in 
many goal-driven or data-driven production systems, respectively). In particu-
lar, an object updated in one rule's effect and tested in another rule's property 
list need not be an argument to either rule, but instead could be bound to a 
derived parameter of one or both rules. The parameter passing between rules is 
implemented by a sophisticated algorithm that "inverts" the logic of the bind-
ings in a chained-to rule, to determine the set of possible actual parameters for 
the rule (the cross-product when there is more than one parameter), given the 
details of the query in which the relevant derived parameter was bound [59]. 

36 
CHAPTER 2 
We will refer to this parameter passing algorithm as the "inversion algorithm" . 
Further details of both this problem and our solution are outside the scope of 
this book. 
Assertions in effects of rules are annotated to indicate either atomicity or au-
tomation. By definition, all forward chaining from an atomicity assertion in the 
selected effect to rules with (now) satisfied conditions and empty activities (i.e., 
an inference rule) is mandatory. If it is not possible to complete such a chain, 
due to a concurrency control conflict or a system crash, then the entire chain 
is automatically rolled back and all its side-effects on process and product data 
undone to the state they were in before the chain began. Thus an atomicity 
chain is in essence an atomic and serializable transaction -
although serializ-
ability can be relaxed according to project-specific concurrency control policies 
specified in the coordination model (see Section 2.2.4). It is important to notice 
that atomicity chaining in this sense applies only to forward chaining into rules 
with satisfied conditions; if a rule is attempted during forward chaining but 
its condition is not (now) satisfied, the atomicity assertion is ignored and that 
rule is not considered part of the transaction (this implements the equivalent 
of "if-then" statements within an imperative transaction). 
In contrast, forward chaining from an automation assertion or into any rule 
with a non-empty activity is optional- and can be explicitly restricted by the 
chaining directives, mentioned above, on individual automation assertions. It 
is important to understand that only automation chaining is optional: users are 
still obliged to follow some legal process step sequence implied by the conditions 
and effects of rules, whether through manual selection of commands from the 
rule menu or automation chaining. The restriction of atomicity chaining to 
forward chaining, and only into inference rules (the first rule in the chain may 
have an activity), are due to limitations in the Marvel implementation and are 
not inherent to the process execution model. Some of these restrictions have 
been partially removed in Oz (see Section 4.4.5). 
In Marvel's process modeling notation, MSL, an automation predicate is en-
closed in parentheses "( ... )", and may optionally be preceded by a chaining 
directive. An atomicity predicate is enclosed in square brackets "[ ... ]". For 
example, the property list of the archive rule shown in Figure 2.2 consists 
solely of automation predicates. It first checks that the MODULE parameter 
(represented by the 1m symbol) has not already been archived, and then per-
mits backward chaining to attempt to compile the MODULE parameter and/or to 
archive any ofthe CFILE (1c symbol), YFILE (1y) or LFILE (1x) components, or 
nested MODULEs (1child). However, the no..forward directives prevent the pro-
cess engine from automatically chaining into this rule from other rules whose 

Previous and related work 
37 
asserted effects might otherwise satisfy these predicates. Thus, there is full 
control over the degree of automation. 
The first effect of this archive rule has two automation predicates and one 
atomicity predicate. The atomicity predicate guarantees that whenever this 
archive rule is successfully executed and its first effect selected, then any 
other rules (with empty activities) whose own conditions are satisfied by setting 
the status of a related AFILE to Archived will also be executed. If for some 
reason (such as a system crash) one of these rules -
or one of their own such 
implications -
could not be completed, then the whole recursive atomicity 
chain (including the original rule and the side-effects of its activity) would be 
rolled back as if none of its rules had ever been fired, that is, an atomicity chain 
is an "all-or-nothing" transaction. In contrast, no such atomicity requirements 
are imposed by the assertion of automation predicates, which can be halted 
midstream by any concurrency control or failure situations that come up. 
2.2.6 
Process and Schema Evolution 
Process evolution in a PCE is analogous to schema evolution in a database 
management system, but inherently more complex. An initial process model 
is developed based on a requirements analysis for the project, or is reused or 
adapted from a previous project or an organization-wide template, but changes 
are often needed later on for various reasons [89]. By this point, however, the 
process may be already instantiated with process state reflecting substantial 
progress through the installed process (note more than one change may be 
made during the lifetime of a project). To replace the process model, it is 
necessary to modify this state so that it is possible to continue work using the 
new process from (more-or-Iess) the point at which work using the old process 
left off. 
The modifications must ensure that the process state is both syntactically and 
semantically appropriate with respect to the new process while still reflecting 
the work already performed under the old process. In particular, it is usually 
undesirable to "start over" with a pristine state and then redo all the previous 
work, even if much of the redo can be automated [52]. It is also, in general, 
incorrect to continue work using the previous process state "as is": the current 
values of process data items may not even be meaningful with respect to the 
new process, or may not accurately represent whether or not newly added or 
modified process steps have been started or completed. Further, it is tedious 
and error-prone to modify a large process state manually, so a PCE should sup-

38 
CHAPTER 2 
port some kind of automated process evolution utility. Note we are concerned 
here with automatically upgrading the process state given a new process model 
provided by the process designer, not with automatically determining how the 
process itself should change (an important topic outside the scope of this book). 
Schema evolution is often a necessary adjunct to process evolution, because 
changes in the process model may mandate changes in the data model (or 
schema) specifying the types and composition of the process data and possibly 
also the product data' representation (e.g., new kinds of artifacts may be added 
or new cross-reference links may be needed among existing artifacts). The 
need for a schema evolution mechanism is clear: When the structure of a data 
type (e.g., a relation in a relational database, or a class in an object-oriented 
database) is modified, the pre-existing instances that were defined according 
to the previous version of the structure must be upgraded to conform with the 
new one in order to be accessed properly. Restricting old instances to access 
by old applications, as is sometimes done in persistent programming languages, 
is generally inadequate for PCEs where those instances represent the state of 
an in-progress process intended to be continued. The affected instances must 
all be modified, either all at once during a special evolution procedure (eager) 
or as they are retrieved during subsequence process enactment (lazy). Marvel 
follows an eager approach, which is more appropriate when associative queries 
are supported (e.g., if all objects with a particular attribute value are selected, 
it is complicated to handle unevolved objects that might have held that value 
had they already been evolved). 
Some structure modifications, like adding or removing primitive attributes 
to/from a class, can be handled relatively easily by adding those attributes 
to all instances with some predefined default values or simply deleting the at-
tributes, respectively. Renaming attributes or classes, but intending to keep the 
old values, is trivial if the schema designer indicates the renamed items explic-
itly (as opposed to expecting the system to recognize them as such). Changing 
the types or the allowable ranges of primitive attributes may be harder to im-
plement and/or may require detailed update specifications from the designer 
(for example, call this application-specific function to convert these to those). 
Finally, changes that affect the composition hierarchy (e.g., removing a com-
posite child attribute might imply disconnecting the hierarchy), or reference 
links among objects (e.g., adding a new link attribute may require for cor-
rectness, with respect to the application( s), that existing objects be linked to 
each other on some semantic basis -
not usually specified in the data model 
itself) are hardest to implement. The difficulties are compounded when the 
class hierarchy itself is changed (adding or removing superclasses from a class 
definition) . 

Previous and related work 
39 
In most cases, however, the differences between the old and new schemas (other 
than renames) can be syntactically analyzed to enable subsequent automatic 
update of the database. This is the gist of the schema evolution mechanism 
in Marvel, implemented as part of the Evolver utility. The Evolver consists of 
two components: The front-end component compares the old and new schemas 
and either produces a "delta" (which is displayed to the process administrator 
to allow retraction if the evolution is not as expected) or rejects the evolution 
if it contains changes that are not supported (the Evolver intentionally does 
not automate any changes that would delete file attributes, for safety reasons, 
although such changes can be made manually). The back-end actually up-
dates the objectbase to match the new schema. Schema evolution is mainly a 
database issue and is not elaborated further in this book. For more details on 
Marvel's schema evolution, see [83]. 
Process evolution is considerably more complicated than schema evolution. 
First, it is not obvious how to analyze the syntactic "delta" between the two 
process models and represent it in a form that can be used for evolution. Sec-
ond, it is far from clear how to identify the semantic differences between the old 
and the new models in order to determine how to mechanically upgrade the pro-
cess state. Third, even if a semantic "delta" is feasible, it is not clear whether 
to apply the changes on none, some, or all of the relevant process data items. 
An archetypal example is when the new process includes some non-trivial step, 
such as code inspection or program-based testing, that was not covered by the 
original process. Say some existing modules have already moved past the point 
in the process where such steps would normally be applied: should the state 
be backed up and the later work discarded, or should the steps be applied only 
during later maintenance activities? The former case is often economically in-
feasible and may have adverse effects on personnel morale, while the latter is 
difficult to handle technically since in the meantime the process state may be 
inconsistent. 
Process Consistency 
Process consistency refers to constraints that are defined in the process (either 
implicitly or explicitly, depending on the process modeling notation), and are 
assumed to always hold for any relevant process state in any instantiation of the 
process. The process state is deemed consistent if all constraints have indeed 
been enforced with respect to all past process steps, and inconsistent otherwise. 
Thus, an underlying premise here is that under normal circumstances, process 
constraints will be enforced by the process engine. It is important to recognize 
that constraints are not the same as process step prerequisites, and enforcement 

40 
CHAPTER 2 
of such prerequisites is only part of guaranteeing that constraints always hold. 
For instance, there may also be implications that sets of process steps (i.e., 
tasks) are performed as a unit (see Section 2.2.5). 
Adding process steps (or tasks) to an existing process might introduce new 
constraints, some of which could potentially result in a process model that is 
inconsistent with respect to the pre-existing process state. Returning to the 
previous example, suppose that the process designer adds a new code inspection 
step to a development process. The associated new constraint in the process 
model is that a source code module can be deposited into the stable repository 
only after it has successfully passed code inspection. Then, the modules already 
in the repository violate this new constraint and thus introduce inconsistency. 
However, it may not be reasonable to require immediate code inspection of 
all source code already in the stable repository before the new constraint was 
introduced, perhaps hundreds of thousands of lines of code, prior to resuming 
the normal process flow. [5] presents some ideas about how such inconsistencies 
might be "tolerated" (lazily) until an affected data item is next accessed. 
The gist of the (eager) approach taken in Marvel is to automatically generate 
a list consisting of every process step in any atomicity chain (see Section 2.2.5) 
affected by the new constraint(s), and then invoke all such steps (and their 
consequences within the atomicity chain) on all data items of the relevant types, 
the cross-product in the case of multiple parameters. The goal is to apply the 
new constraints retroactively to the existing process state, in order to make the 
state consistent with respect to the new constraints -
but not necessarily by 
performing all the steps that would be required under normal process execution. 
In particular, the generated list contains only process steps that update state 
but do not actually perform any activities (i.e., what are termed inference rules 
in Section 2.2.2). The result is to propagate changes in state attributes to 
represent a consistent state with respect to the new process model, without 
process participants actually doing any work. A detailed discussion can be 
found in [83]. 
2.2.7 
Access Control 
Access control is a mechanism that allows one to specify by whom data can be 
accessed, and in what (generic) manner -
independent of the particular ap-
plication that accesses them or the particular circumstances under which they 
are accessed. In other words, it is a persistent property of the data items them-
selves. An example of such a mechanism is the Unix file system permissions, 

Previous and related work 
41 
where the generic access modes are read, write, and execute, and who can ac-
cess a file in each mode are divided into simple owner, group, and universal 
categories. 
Marvel needed an analogous access control facility for individual objects. One 
approach would be to build a flexible mechanism that could be defined on a 
per-project basis to meet the demands of a particular project, much like other 
aspects of Marvel are tailored. An access control modeling notation could be 
integrated within MSL, like data modeling, or supported semi-independently, 
like coordination modeling. However, since such an access control mechanism 
would necessarily involve low-level operations on the objectbase -
including 
interaction with the "hidden" file system and dynamic checks on the object-
base each time an object is accessed by whatever means -
this approach 
would require exposing those operations to the modeling language. Further, 
the run-time support would be complex, since access control necessarily must 
override other behavior specifications, e.g., to be automatically initiated and 
hold priority over relevant process steps . 
Access control was treated in the Marvel project less as a research topic and 
more as a bare necessity, so a pragmatic compromise was reached. The repre-
sentation of access control information is defined in the same notation as data 
model classes and attributes, but the manipulation of permission data is done 
through a set of built-in operations as opposed to process model rules. The 
actual checking of permissions, whenever an object is accessed by a built-in 
operation (add, link, etc.) or a process step (rule), is also hardwired. Rep-
resenting the permissions data by normal class/attribute definitions enables 
one to potentially access and manipulate the permissions through MSL rules, 
analogous to the way structural operations in Marvel have both built-in and 
rule-based interfaces (see Section 2.2.2). 
In order to realize access control protection, two important concepts were in-
corporated into Marvel: the notion of a user object, representing an individual 
Marvel user and to which user-specific permission defaults (similar to the Unix 
umask) can be attached, and the notion of a group object, representing a spec-
ified set of users with common privileges similar to Unix permission groups. 
Both the USER and USER_GROUP classes are defined using MSL and their in-
stances appear in the object base as first-class objects. Even though their ma-
nipulation for access control purposes (only) is hard-wired as explained above, 
these built-in classes and their instances could potentially be used for project-
specific process model applications such as human resource management. In-
deed, several Marvel environment instances employ specialized subclasses of 
the USER class with additional information, e.g., to represent roles. 

42 
strategy protection 
imports none; 
exports all; 
objectbase 
PROTECTED-EITITY:: 
superclass EITITY; 
owner: 
user; 
perm....string: 
string = "rwad rwa*"; 
end 
Login-Register:: 
superclass PROTECTED..EITITY; 
group...l.ist: 
GROUP .LIST; 
user ...l.ist: 
USER.LIST; 
end 
GROUP .LIST:: 
superclass PROTECTED-EITITY; 
glist: 
set_of USER_GROUP; 
end 
USER.LIST:: 
superclass PROTECTED..EITITY; 
ulist: 
set..of USER; 
end 
USER-GROUP: : 
superclass PROTECTED-EITITY; 
grpJ1al\le: 
string; 
end 
USER:: 
superclass PROTECTED-EITITY; 
groups: 
set..of link USER..GROUP; 
mask....string: 
string = "rwad r*a*"; 
end 
end_obj ectbase 
Figure 2.4 
Class Definitions for Access Control 
CHAPTER 2 

Previous and related work 
43 
The MSL strategy defining permission-related classes is shown in Figure 2.4. 
(The "rwad" permissions string refers to read, write, add a composite child 
attribute, and delete privileges; a permission string contains two mandatory 
sub-strings, one for the owner and one for "others" (i.e., anybody), and op-
tional <group-name,permission-substring> pairs for affiliated groups.) This 
strategy must be included in any environment instance where access control is 
desired. In addition to defining users and groups, the access control mechanism 
must associate permissions with all those individual objects in the object base 
intended to be protected. This was implemented via a generic class called 
PROTECTED..ENTITY, such that access control is applied only to instances of 
classes defined as subclasses of PROTECTED..ENTITY. Therefore, access control 
is entirely optional. In particular, if there is no need for such a mechanism, 
such as in a single-user environment, then the protection strategy is simply 
omitted from the process model; in a multi-user environment where protection 
is required only for certain kinds of objects, no overhead from access control 
is incurred for other objects. Another benefit of this approach is that access 
control can be easily added to, or removed from, an environment instance by 
simply adding PROTECTED..EITITY as a superclass to all classes and performing 
schema evolution on the objectbase as explained in Section 2.2.6. 
The runtime behavior of the protection model is as follows: When a user logs in 
to a Marvel environment instance, the system associates the user with his/her 
appropriate USER object, if there is one. The matching between a user and the 
corresponding user object is done via the Unix userid; there is no analogous 
association with Unix groups, however -
Marvel groups are specified on a 
per-environment basis. If there is no USER object for that userid, he/she is 
associated with the anonymous instance of the USER class, with default guest 
permissions. If a user logs in as a process administrator (restricted through 
a separate access control list attached to each environment instance), he/she 
is associated with the special administrator object rather than his/her own 
USER object (and acquires maximal privileges, i.e., the protection model is 
not enforced). When a user attempts to access any object whose class de-
scends from PROTECTED..ENTITY (including a protection-model object), either 
directly through built-in operations or indirectly through rule chaining, the 
specified permissions are checked by matching the relevant USER object (and 
any USERJ;ROUP objects to which it is linked) with the accessed object's owner 
and perm...string attributes. For more details on Marvel's access control mech-
anism see [83]. 
2.2.8 
Marvel Architecture 

44 
Administrator 
Client 
LOADER 
Tool Envelopes 
(SELl 
CHAPTER 2 
CLIENT 
Unix 
Figure 2.5 
Marvel Architecture 
Marvel's architecture, illustrated in Figure 2.5, follows the client-server model. 
The server is centralized and implements task management (including the pro-
cess execution engine), multi-task synchronization, and data integration and 
repository services. Clients include the user interface, which supports object-
base navigation, and manage interactions with external tools. The rationale 
behind the Marvel architecture can be found in [21]. 
There are two basic kinds of clients: XView (Xll Windows) and tty (for dumb 
terminals and low-bandwidth dialups); there is also a batch interface for com-
mand scripts based on the tty client. We direct most of our attention here, and 
throughout the book regarding Oz's clients, to the XView rather than tty client 
since the former provides somewhat more functionality; for example, an XView 
client can support multiple threads of control, each corresponding to a different 
set of windows, which a user can switch among at will and provides an ad hoc 
query interface as well as graphical objectbase browsing. The server and all 
of its clients must reside in the same Internet domain and share a network file 
system. The server can be invoked manually, or be automatically brought up 
by a daemon when the first client is started (in which case it is automatically 
shut down when the last client disconnects). 

Previous and related work 
45 
A few user commands are handled entirely at the client, such as requests to 
graphically display the rule network or the class hierarchy (these are loaded 
into the client at connection). Most commands are transferred to the server, 
which validates and performs the requested action as explained in the sections 
above. In the case of a command that maps to a rule execution, the server 
carries out condition evaluation and possibly backward chaining, but it sends 
the activity back to the client for execution. At that point it switches context 
to handle the next client message in its FIFO queue (or remains idle until the 
next message arrives). Upon completion of an activity, the client forwards its 
results to the server. When that message reaches the front of the queue, the 
server asserts the selected effect and attempts to forward chain to other rules. 
Both backward and forward chaining may lead to additional interactions with 
the client to execute the activities of chained-to rules. 
Besides the client, server and daemon components, the system also includes a 
collection of utilities for various purposes. For example, the Loader translates 
a set of MSL strategies into the internal form of the class hierarchy and rule 
network; Marvelizer immigrates existing software artifacts from the file system, 
constructing an object base and corresponding hidden file system, according to 
an environment instance's data model and conversion specifications supplied 
by the process administrator [119]; MakeEnvelope translates an envelope file 
from SEL to bare shell . 
The Marvel system is adequate for a small to medium number of people par-
ticipating in the same process; we have experience using the system with up to 
20 concurrent clients (used by 10 concurrent users). As the number of clients 
grows, however, the server would become a bottleneck: the implementation is 
single-threaded, and the only context-switching during a rule chain is explicitly 
performed for activity execution as explained above. This has not been a prob-
lem in practice, since there are relatively few long inference chains (i.e., those 
containing only rules with empty activities) in the processes we have developed, 
but the server would need to be re-implemented in some multi-threaded form 
to support large numbers of users and there are likely to be scale limitations 
on a centralized server in any case. 
Perhaps more significantly, the system supports only a single process per en-
vironment instance, on a centralized project database; different environments 
can certainly provide different processes, but it is cumbersome to share data 
among separate environments (e.g., through a utility to manually prune and 
graft objectbase subtrees). There is no explicit notion of user roles or views, 
and the only way to specify that different process steps should apply to dif-
ferent users is to hardwire userids into the conditions of rules (e.g., there is a 

46 
CHAPTER 2 
built-in Current User function, used mostly for checking that the user on whose 
behalf a given rule is executing is the "owner" of the parameter data). The 
Marvel process design methodology basically assumes all members of the same 
project are intended to follow the same process. A view mechanism has been 
investigated [12], but the system's notion of process consistency inherently re-
quires that every view defines either identical or completely disjoint consistency 
models, so deviation would be possible mainly in the automation of process 
fragments (users can, however, employ different tools to perform the same pro-
cess step, e.g., choice of editor can be made through an environment variable 
accessed by shell code in the relevant envelopes). Thus, Marvel provides rela-
tively limited support for heterogeneity within a software development project, 
even though unrelated projects can enjoy arbitrarily diverse processes. 
2.3 
OTHER DOMAINS 
Autonomous decentralization, heterogeneity, and interoperability, are very ac-
tive research areas in several related fields, including the database commu-
nity (more specifically Heterogeneous Distributed Database Systems (HDDB)), 
Computer Support for Collaborative Work (CSCW), and Heterogeneous Pro-
cessing (HP). We briefly summarize each field, give some examples, and differ-
entiate them from the research on PCE decentralization. 
2.3.1 
Heterogeneous Distributed Data Bases 
(HDDBs) 
The relevance of work in HDDB to DEPCEs is analogous to the relevance of 
(centralized and distributed) database research to PCEs. Just as PCEs gen-
erally impose specific requirements on the representation, storage, and (con-
currency) control of the persistent artifacts being developed [23] (also known 
as software engineering databases) -
DEPCEs might similarly require special-
purpose modeling and mechanisms to support various degrees of heterogeneity 
and site autonomy with respect to the persistent artifacts which are manipu-
lated by the environment. The database community has defined this field as 
federated, or heterogeneous databases, which permit a high degree of site au-
tonomy [107]. The heterogeneity is usually with respect to one or both of two 
criteria, system and schema (the third "standard" criteria, namely language 
heterogeneity, is embedded in the system criteria since the data definition and 
manipulation languages are usually strongly associated with the underlying sys-

Previous and related work 
47 
tem): the sites may employ the identical system but devise their own schema 
independently (also known as a homogeneous federation); and/or they may se-
lect different database systems from among those supported by the federation 
"glue" (heterogeneous federation). 
UniSQL \M is an HDDB that assumes a common relational data model to which 
all component database systems convert their schemas. In [81], Kim describes 
a complete framework for classifying schematic and data heterogeneity as a 
basis for a later "homogenization" of the databases. The general approach to 
addressing heterogeneity is by providing an underlying common formalism into 
which the various formalisms translate. The main difficulty with this approach 
is that the common data model and the formalism (in this case, relational and 
SQL, respectively) must be expressive enough to support a wide variety of data 
models and languages -
which in many cases might not be feasible. 
Pegasus [35], from HP laboratories, is an HDDB that uses object-oriented tech-
nology to extend the schematic integration approach in an attempt to alleviate 
the difficulties with arbitrary mappings of data models and to increase local 
autonomy. First, local schemas need not be mapped completely, only imported 
(sub)schemas are integrated. This allows one to hide parts of the local schema 
(and the instantiated data) that are either hard to map or for privacy concerns. 
Second, the integrated schema is not necessarily global. Instead, Pegasus builds 
a hierarchy of integrated schemas that apply to a subset of the local databases 
and enables a more refined integration. The approach for integration is based on 
the notion of "upward-inheritance" , where types can be superimposed to gen-
eralize on local types in different schemas (and different databases). Finally, 
in addition to object-oriented data abstractions, Pegasus also exploits object-
oriented function abstractions to enable non-trivial integration of schemas by 
attaching optional functions to support the mapping. 
2.3.2 
Heterogeneous Processing 
Heterogeneous Processing is an emerging field concerned with architectures 
for distributed systems that support heterogeneity and interoperability of au-
tonomous entities. The main focus in this field is on (1) investigating system 
level heterogeneity, and (2) providing solutions to the general heterogeneity 
problems by exploring integration at the system level. The main relevance 
of this field to PCE research and technology is in the area of architectures 
for decentralized and heterogeneous PCEs. The analysis and identification of 

48 
CHAPTER 2 
requirements for DEPCE-specific architectures is another research issue that 
involves both communities. 
One of the prominent specifications for heterogeneous processing systems is 
OMG's Common Object Request Broker Architecture (CORBA) [96]. The key 
idea in CORBA is to insert a programmable intermediary level between clients 
that request to invoke operations on some (active) objects (which may be viewed 
as servers) and the object implementations, thereby providing an infrastructure 
that enables to "glue" heterogeneous components and mix and match between 
them. The heart of the intermediary mechanism is the Object Request Broker 
(ORB), which interconnects objects and clients (location, message transfer, 
etc.). Object implementations specify their interface in an Interface Definition 
Language (IDL) that is independent of the programming language in which the 
object is implemented, and is understood by the rest of the system. 
InterBase [25] is a system that addresses the (mostly data) heterogeneity prob-
lem by using integration at the system level. This is in contrast to most HDDBs 
that provide integration at the schema level. Moreover, it can be viewed as a 
control-oriented approach as opposed to data-oriented. The main idea in Inter-
Base is that each subsystem supplies a programmable Remote System Interface 
(RSI) that serves as an intermediary between the local sites and the ''federa-
tion" , and global transactions are supported through a distributed transaction 
manager that interacts with these RSIs and coordinates the concurrent execu-
tion of global transactions, thereby serving also as a coordination platform. 
An underlying theme in the field of heterogeneous processing is to use data and 
control abstractions to cope with system heterogeneity and interoperability, by 
trying to hide everything that is not pertinent to interoperability, minimize the 
"exposure" to global control, and determine the desired exposure at each site 
autonomously. 
2.3.3 
Computer Support for Collaborative 
Work (CSCW) 
CSCW focuses on support for human-human interactions, including social as-
pects of collaboration and tools to enhance collaboration and coordination, 
possibly among physically remote group members. As such it also borders with 
the user interfaces (or more generally Human Computer Interaction (HCI)) and 
Multimedia communities. CSCW is related to PCE technology because soft-
ware development is inherently a collaborative task. The interesting challenge 

Previous and related work 
49 
from the PCE perspective is in embracing CSCW technologies into the process 
framework. Some work in that direction is given in Sections 3.6, 4.5, Chapter 6, 
and [18]. 
Suite [31] is a system that provides an infrastructure for building multi-user 
and multimedia tools. Suite provides mechanisms to support flexible and fine-
grained concurrency control (needed to enable simultaneous sharing of data in 
multi-user applications), synchronous and asynchronous collaboration facilities, 
caching of user-interface state at local workstations (to reduce communication 
overhead), coupling of user-interface states of different collaborators, and au-
dio and teleconferencing annotations for multi-media collaboration. FLexible 
Environment for Collaborative Software Engineering (Flecse) [32] is a set of col-
laborative software engineering tools built on top of Suite, intended to support 
product development of a group of geographically-dispersed engineers, with 
focus on multi-user tools such as editing, debugging, and versioning. The tech-
nological idea is to provide the local user an interface front-end and a cached 
state of the tool (provided by Suite), while manipulating the actual tool's data 
and state in a central location. Flecse can be considered essentially as a toolset, 
without integration mechanisms or policies regarding their invocation. This, 
however may be beneficial when considering the integration of the tools within 
a PCE. 
An example of a full blown CSCW system is Conversation Builder (CB) [36]. 
The main concept in CB is that of a conversation, that serves as a context 
in which a user performs its actions ("utterances"), and can potentially affect 
other users participating in the same conversation through a shared conver-
sation space yet still protect their private conversation space. An interesting 
capability of CB that most CSCW systems lack, is the ability to specify activ-
ities and their interrelations using protocols, which are state-machine descrip-
tions of the flow of the conversations. Thus, a limited form of tailorability 
and collaboration modeling is provided. The architecture of CB is centralized, 
with a shared conversation engine (analogous to process engine in PCEs) with 
which all clients communicate. It is based on a multi-user FIELD-like message 
bus [109] that serves as the control-integration mechanism. CB has been used 
in several application domains, including configuration management and code 
inspection, and has some overlap with PCE concepts. However, the emphasis 
in CB is on user-user collaborations, not the software process in general, and 
it does not have any form of enactment. 

50 
CHAPTER 2 
2.3.4 
Summary 
What distinguishes research in DECPEs from the above domains, is the fact 
that heterogeneity, autonomy and interoperability have to be addressed not 
only in the context of architectural support (as in Heterogeneous Processing), 
data integration (HDDBs), and tools for human collaboration (CSCW), but 
in the context of the modeled process that oversees, integrates and assists in 
the invocation of multi-user tools on heterogeneous data and on behalf of, and 
with the participation of, multiple collaborating human users. Furthermore, 
the challenges in this research are to find suitable notations, mechanisms, and 
infrastructures that support all the above in a flexible and project-specific man-
ner. 

3 
THE DECENTRALIZED MODEL 
Chapter 1 provided the motivation for why decentralized environments should 
be investigated, and imposed some requirements on how to build them. In this 
chapter we will show how this can be done, by introducing a general formal 
model for process interoperability. The model is both language- and system-
independent and is in principle applicable to a wide range of Process Modeling 
Languages (PMLs) and Process Centered Environments (PCEs) (as will be seen 
in Section 3.5), although in a particular language/system only a subset of the 
model's capabilities might be realized. Chapters 4 and 5 will show a particular 
realization of the model in Oz. 
The high-level approach taken in this research to meet the challenges described 
in the introduction is to supply an abstraction mechanism whereby multiple 
possibly pre-existing processes can be encapsulated and retain security of their 
internal software artifacts, tools, and steps, while agreeing with other pro-
cesses on formal interfaces through which all their interactions are conducted 
on shared data. Thus, another perspective on process modeling is that process 
models, being encoded in formal notation, can be used as a sound basis for for-
mally modeling interoperability among processes. Furthermore, multi-process 
enactment engines can support the execution of collaborative inter-process ac-
tivities in the same way that single-server enactment engines support the exe-
cution of single processes. This thesis, then, builds on the premise that process 
modeling is in general a viable technology, and asserts that inter-process mod-
eling and enactment, as invented in this thesis, is a viable technology as well. 
Some intuition to the decentralized model may be gained by the "international 
alliance" metaphor which will be used occasionally in the book. In such an 
alliance, the default is for independent countries (processes) to operate au-

52 
CHAPTER 3 
tonomously and collaborate (interoperate) only in accordance with predefined 
treaties. The actual collaboration is modeled as a summit meeting, where prepa-
rations to the summit and consequences of the summit are performed indepen-
dently (according to private sub-processes) and the summit itself is performed 
cooperatively (according to a shared sub-process). 
We begin with definitions of terms and a formalization of concepts that are 
used in the rest of the model, followed by discussion of the model for definition 
and execution of decentralized processes, followed by application of the model 
to various types of PMLs, discussion of groupware support, and conclude with 
possible extensions to the model. 
3.1 
DEFINITIONS 
The purpose of this section is to establish a common terminology to be used 
consistently throughout the rest of the book. It is particularly important to 
clarify the meaning of the heavily overloaded term "environment", and to dis-
tinguish it from other concepts. 
3.1.1 
peEs, Process Models, and 
Environments 
As mentioned in the introduction, a Process Centered Environment (PCE) is 
a system in which processes are modeled and enacted, and a process model 
is an actual definition of a project-specific process, as specified in the Process 
Modeling Language (PML) supplied by the PCE. The PCE may supply "base" 
process models which are then tailored to project-specific needs, or processes 
can be written from scratch to support a specific project, but this distinction 
is irrelevant to the ensuing discussion. While a process model tailors a generic 
process (if any), it is a static entity which does not represent execution, only 
definition. 
A process model can be instantiated, by loading it and binding to it real ar-
tifacts, tools, users, and any other system bindings which are required by the 
PCE in order to initialize it for execution1 . An instantiated environment (or 
1 A process can also be initialized for simulation, in which case the bindings are to virtual 
or simulated artifacts, users, tools, and so forth. 

The Decentralized Model 
53 
environment instance) is an enact able process model. It can be viewed as the 
loaded "core image" of a process model. However, it usually maintains per-
sistent data and state that lasts across (operating system process) executions 
of the process model. For brevity, we shall call an instantiated environment 
simply an environment. This term should not be confused with the term 
PCE, which refers to the system on which (instantiated) environments run. 
Thus, a process' lifetime begins when it is initially defined as a process model. 
Afterwards it is loaded and instantiated for execution (perhaps with interme-
diate testing and analysis steps), at which point it turns into an instantiated 
environment. Note that the same process model can be instantiated in multiple 
environment instances. At some point during its execution, the process model 
might need to be refined, e.g., because of feedback from the environment and/or 
new requirements, in which case it is modified and then reloaded, although this 
time the persistent process state and product database have to be evolved to 
conform to the new process model. 
3.1.2 
A Generic Process Context Hierarchy 
The following is a generic definition of a three-level hierarchy of nested contexts 
within a single process. A particular PML might have more or fewer levels, but 
we assume that there is some mapping into these core levels: 
1. Activity -
This level is where the PCE interfaces to actual tools, including 
input/output data transfer with respect to the tools. This is sometimes 
done through wrappers, or envelopes. 
2. Process-step -
This level encapsulates an activity with local prerequisites 
and immediate consequences (if any) of the tool invocation, as imposed by 
the process. For example, in the FUNSOFT Petri-net based PML [46], a 
process step corresponds to a transition along with its (optionally) attached 
predicates; in the Articulator task graphs [93], this level corresponds to a 
node with its predecessor and successor edges; and in rule-based PMLs, a 
process step is represented by a rule with pre- and post-conditions. The 
process-step level may also supply the mechanism to interface among mul-
tiple activities in a process. For instance, in rule-based PMLs, a post-
condition of one rule is matched against a pre-condition of another rule to 
determine possible chaining; similarly, the firing of a Petri-net transition 
can enable another transition. 

54 
CHAPTER 3 
3. Task -
This level is defined as a set of logically related process steps that 
represent a coherent process fragment. Depending on the specific PML 
and peE: (1) there are typically some ordering constraints, or workflow, 
among the activities or process steps of a task; (2) parts of a task might 
possibly be inferred dynamically, emanating from an entry activity or pro-
cess step selected by the user; and (3) a task might be partially carried out 
automatically by the peE on behalf of the user, usually by triggering the 
inferred activities or steps. The task level may be explicitly defined in the 
PML through a special notation, or may be implicitly defined through the 
local prerequisites/consequences in the process-step level, or both. For ex-
ample, the Activity Structures Language [73] specifies "local constraints" 
using rules (the form of process steps), and "global control flow" using con-
strained expressions (explicit tasks). In a Petri-net PML, the task level 
typically corresponds to a subnet, if such a construct exists. Tasks may be 
further decomposed into subtasks. 
3.1.3 
A Multi-User, Single-Process 
Environment 
The following is a formal definition of a single-process environment. It is a 
minimal definition, in that it specifies only the ingredients which are necessary 
for our model. Thus, our goal here is to include as many peEs as possible, but 
at the same time identify (families of) peEs which cannot possibly fit in the 
model. 
An (instantiated) environment E is defined as a quintuple: 
E =< S,D,T,U,P > 
Where: 
• s -
A schema representing data types for modeling the product and 
process data manipulated by the environment. Note that this require-
ment excludes environments with no data modeling support (e.g., Syn-
ervision [60]). Furthermore, the schema must support the notion of an 
"object" as explained below. However, there are no further requirements 
on the expressiveness of the data definition language, and in particular, it 
can either be a sub-language of the PML, or a separate language. 

The Decentralized Model 
55 
• 
D -
A database storing a set of objects, each belonging to a certain type 
(or class) from S. This component requires persistent storage for at least 
the process data, and possibly for the corresponding product data (the 
latter could alternatively be maintained in a separate database or in the 
native file system, but should be identifiable from the process data). As 
noted above, the database should be object-based, in the sense that data 
elements are typed (or classified), they have unique identity, and they 
can be referenced and manipulated by the process. Requiring an object-
based project database does not seem to be a severe restriction, though, 
as most existing PCEs tend to use object-based databases (Adele-2 [14], 
Merlin [128], SPADE [6], Arcadia [66], Matisse [42], to name a few). 
• 
T -
A set of tools being used in the environment. The tools can be off-the-
shelf, or customized to work in the PCE, but in either case it is assumed 
that the PCE has means to invoke those tools from within the environment 
through process activities. 
• u -
A set of users using the environment. No built-in roles or hierarchies 
are assumed to be attached to users, except for the concept of environ-
ment administrator, who defines and can modify each of the elements in 
E (analogous to the role of a database administrator). 
Note that this component implies an important requirement on the under-
lying architecture: it must support multiple users sharing the instantiated 
process, possibly simultaneously. 
• 
P -
A set of activities/steps/tasks and their inter-relationships, which 
together comprise the process model. They can be invoked either manually 
by human end-users, or automatically by the process engine. Each activity 
encapsulates a tool from T, with formal parameters from S, and actual 
parameters from D. An activity is not required to be bound to specific 
users or roles from U, although such a requirement can be imposed by a 
specific implementation or a specific process definition. 
As can be seen, the definition above imposes some architectural requirements 
on the PCE. These will be fully discussed in Chapter 5. For the time being, 
it is sufficient to note that a PCE supporting this definition of an environment 
must include: 
• 
Data modeling, repository, and management (including concurrency) 
• 
Communication services 

56 
CHAPTER 3 
• 
Tool integration mechanisms 
• 
Task/Process management 
• 
User interface 
• 
Translators and/or interpreters for the process and data models 
Hence, with the exception of the last item, these are the same components as in 
the "toaster" reference model for general SDEs [34]. The key difference is that 
by using the translator and loader, some or all of the above components can be 
tailored, and are not hard wired into the system. Indeed, one of the architec-
tural challenges in building a DEPCE is to preserve the modifiability property, 
which, as evidenced in [78], is the most important property in determining the 
quality of the architecture. 
Based on the above definitions and requirements, a high-level view of an archi-
tecture of a single-process PCE with an instantiated environment is depicted in 
Figure 3.1. It consists of a database server managing the process schema and 
data, a tool server integrating the project's tools, a process server enacting the 
defined process, a client-user interface, and a communication layer connecting 
all components. A typical interaction with the PCE is as follows: an end-user 
from U initiates a task from P by invoking an activity that encapsulates tool(s) 
from T, on a set of data arguments from D that belong to classes from 5. The 
process server receives the request, and depending on the specific installed pro-
cess and other ongoing activities, determines what to do before, during and 
after the requested activity, involving the data and tool servers which can also 
interact directly with the client. 
A Sample Single-Process Environment 
The following is a specification of a sample environment, E1, that supports a 
code change sub-process (taken from an actual Marvel process model used to 
develop Oz). For simplicity, only that sub-process is presented here. E1 is 
defined as: 
where: 
51 ={ FILE, LIB, WORKSPACE, EXEFILE} 

The Decentralized Model 
Data 
Server 
<P,S,D, T,U> 
Environment 
Communication 
client 
client 
Tool 
Server 
client 
Figure 3.1 
A Generic Single-Process Environment 
57 

58 
~",,(FILE, WORKSPA§::> 
---.... .... " , , 
~--~ , 
",'" 
~~ 
--
--
Figure 3.2 Change Sub-Process 
/ 
I 
/ 
/ 
\ , 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
\ 
I 
I 
J 
J 
J 
J 
I 
I 
I 
f 
f 
I 
I 
I 
CHAPTER 3 
Dl ={ IssyWorkSpace, IssyExe, ServerLib, ClientLib, server.c, client.c, server.h, 
client.h, server.h.local } 
Tl = { rcs, emacs, cc, lint, gdb, codejnspect, tags, ar, find_dep, latex} 
Ul ={ israel, heineman, pds, popovich, tong} 
Pl is represented by the graph in Figure 3.2. 
A change sub-process consists of issuing a Reserve activity to check-out a 
source file from some master-area (maintained by another sub-process not men-
tioned here) to a local workspace; followed by the Outdate activity that out-
dates the local workspace, thereby invalidating any local binaries that were 
constructed in prior changes; followed by an Edit - LocalRef - Analyze -

The Decentralized Model 
59 
Compile - Build - Debug - Edit cycle (LocalRef is a utility that checks 
that relevant master-area artifacts have not been updated since the reserva-
tion took place.), followed by the unit-test task (marked in black-box in the 
figure to denote an entire sub-task which is not expanded in this example), a 
deposit activity, and a master-build task, also not given here. Forward edges 
represent the expected "successful" flow of the process, and backward (dashed) 
edges represent the flow of control when activities ''fail'' (e.g., unsuccessful 
compilation). 
3.1.4 
A Multi-Process Environment 
A multi-process decentralized environment is formally defined as: 
{E;} i = 1 ... n 
where each Ei is a single-process environment as defined in the previous sec-
tion. In addition, a multi-process environment has some (modeling and enact-
ment) facilities for environment and process inter-connection and interoperabil-
ity. This is the main subject of this chapter. 
Site autonomy and operational independence impose a strong architectural re-
quirement: it must be a "share-nothing" architecture. This means not only 
that processes are private, the data is also disjoint, and all inter-process com-
munication is performed through message passing. This is in sharp contrast 
to the "blackboard" or shared-memory approach adopted in the single-process 
environment with respect to the multiple clients, in which multiple entities op-
erate on shared data using a centralized process. While the data is disjoint, it 
must nonetheless be accessible by remote SubEnvs in order to enable process-
interoperability. Thus, we assume that the underlying PCE has the necessary 
mechanisms to reference and bind remote data objects to local activities, con-
sistent with the "global-browsing" requirement (see Chapter 5 for an actual 
implementation of these). Driven by autonomy requirements, however, the 
data in each SubEnv is private by default, and is said to be "owned" by its 
local process. Thus, access to both process and product data cannot be made 
from a remote process without prior "permission" from the owner process. 
The high-level architectural view of a generic decentralized PCE with a three-
site decentralized (instantiated) environment is depicted in Figure 3.3. 
Each local environment consists, in addition to the single-process components 
(as outlined in Figure 3.1), an inter-process server, a remote-data server, a 

60 
v 
I 
Remote 
Data 
Server 
Da .. 
Server 
ellent 
Wide Area Networl< 
client 
cUent 
Figure 3.3 
A Decentralized Environment 
cllent 
CHAPTER 3 
client 
Remote 
Tool 
Server 
Tool 
Server 
eDent 

The Decentralized Model 
61 
remote tool server, and a connectivity server (along with a possible connec-
tivity database) that enables SubEnvs to connect to, and communicate with, 
other SubEnvs participating in the same (global) environment. These ele-
ments together form the necessary infrastructure support needed for process-
interoperability. Notice the "no sharing" property, which enables full operation 
of some sites when some of the other sites are inactive or disconnected (e.g., 
the leftmost SubEnv in the figure is inactive). 
A multi-site activity is an activity that involves, when executed, data objects 
from remote SubEnvs. Note, however, that this is a dynamic property of an 
activity, in that a given activity mayor may not be considered a multi-site 
activity at different invocations, depending on whether the data bound to it 
includes remote objects. Multi-site activities are the building blocks of any 
process-interoperability in this model. 
Referring to the context-hierarchy described in the previous section, it is im-
portant to note that there is intentionally no fourth level that represents a local 
process as part of a global process. This reflects our concept of independent 
collaborating (local) processes. While this model of a DEPCE provides global 
infrastructure support to enable interoperability among local processes, it ex-
plicitly avoids the need for a global "super" process -
although such a process 
can be implicit. 
3.2 
DEFINING PROCESS 
INTEROPERABILITY: THE TREATY 
3.2.1 
Motivation and Requirements 
The following is a set of requirements specific to modeling interoperability, 
driven by the high-level requirements presented in Section 1.5. 
1. In order to enable invocation of multi-site activities, there must be a way to 
define and agree on a common sub-process that would become an integral 
part of each local process intended to collaborate during that sub-process 
(but not necessarily by all SubEnvs in a global environment). A common 
sub-process determines what actions can be taken in the multiple par-
ticipating SubEnvs. At the very least, the multi-site activities must be 
commonly specified so that they can be identified during execution. But 

62 
CHAPTER 3 
the "unit of commonality" might also be the process step, or even the task. 
In any case, this unit has to represent those process fragments that poten-
tially involve multiple local processes. The decision as to what level (in 
the context hierarchy) to choose as the unit of commonality depends on 
the modeling primitives of the specific PML. In a Petri-net formalism, for 
example, the transition (along with its input and output places) seems a 
natural choice, whereas in rule-based PMLs the rule (process step) is likely 
to be chosen. In PMLs that support task hierarchies and modularization 
(e.g., Articulator [93]), a subtask might be the right choice. 
It is important to recognize that the activity portion of a decentralized 
sub-process need not be executable in every participating SubEnv, e.g., 
since the encapsulated tool may not be physically available everywhere. 
Instead, the activity only needs to be executable in one of the SubEnvs 
intended to collaborate, which would hence always serve as the invoking, 
or coordinating process. This means that common sub-processes are not 
necessarily reciprocal, in the sense that not all participant SubEnvs have 
identical process "privileges" on multi-site activities. This issue has direct 
implications on the model, as will be seen shortly. 
2. In order to enable the definition (at least in strongly typed PMLs) and 
the execution (in all PMLs) of multi-site activities (as part of a multi-site 
common sub-process), the SubEnvs which are involved in that sub-process 
must have a common sub-schema, so that the types of the parameters 
specified in the activity are known at the SubEnvs. For example, if an 
activity Al is invoked from SubEnv El on remote data from E2, then E2 
must have the proper types in its schema and consequently the properly 
instantiated objects that are required by A1. 
Note, however, that a common sub-schema does not necessarily imply that 
the corresponding data instances are shared -
only their types (i.e., their 
schema) are shared. Defining common data schema and allowing access 
to data instances are separate concerns which should not be confused or 
coalesced. 
3. Following the above argument, there must be a way to define (and subse-
quently, control) which data instances are allowed to be accessed, in what 
way, and by which SubEnv. That is, local databases are by default private, 
consistent with the autonomy requirement, and parts of them can be made 
accessible to enable remote access by multi-site activities. 
4. It must be possible for a common sub-process (and the corresponding com-
mon sub-schema) to be shared among only some of the local processes 

The Decentralized Model 
63 
(SubEnvs) of a given global environment, not necessarily all of them. Fur-
ther, the same local processes must be able to participate in multiple com-
mon sub-processes, together with the same or different collections of remote 
processes. There is usually some portion of each local process that is not 
shared with any other process (a private sub-process). Similarly, it must 
be possible to specify access to subsets of the data instances to only some 
but not all participating SubEnvs, as opposed to allowing data to only be 
either totally private or universally public. 
5. Finally, the PML must allow for both dynamic inclusion and exclusion 
of common sub-processes, as well as independent evolution of private sub-
processes. The former is particularly important when independent pre-
existing processes decide to collaborate, perhaps only temporarily, while 
the latter is important for preserving the autonomy of local processes. One 
of the architectural implications of this requirement is incremental process 
compilation capabilities2 . 
3.2.2 
Alternatives, Design Choices, and 
Justifications 
In considering the possible alternatives to expressing common sub-processes 
within otherwise private and encapsulated processes, we can draw an analogy 
between our problem and similar problems in the "neighboring" domain of 
programming languages and distributed systems and investigate alternatives 
there: 
1. Process interface specified within the PML -
This approach includes pro-
gramming language abstraction mechanisms in which all control and data 
of a unit are by default private (or hidden) unless specified explicitly as 
public in the unit's interface. For example, the body/specification distinc-
tion in Ada could be used to expose only the common sub-processes (or 
sub-tasks in Ada terminology) in the specification and hide the private 
sub-process in the body. Another example is the export-import mechanism 
in Modula-2, in which a subset of the activities (functions) could be ex-
ported by one process (module in Modula-2 terminology) and imported 
by another, while the rest of the local process (module) is by default hid-
den. A third example is the object-oriented approach to encapsulation, 
2The meaning of "compilation" depends on the specific peE, but most translate their 
processes into some internal format rather than repeatedly reparsing and reinterpreting the 
source. 

64 
CHAPTER 3 
whereby a class denotes the public methods in its interface, and hides all 
other methods (which are part of its implementation). 
The main disadvantage with this language-based approach is that it is 
static in nature, conflicting with requirement 5 from the previous section. 
That is, the interface specifications cannot be changed while the program 
is executing, and all the bindings among the different modules are made 
at "compile" time. Another problem with this approach is that the under-
lying motivation for it is to provide abstraction for distinguishing between 
a unit's external (public) interface and its internal (private) implementa-
tion. While this might be the case in process inter-operability, more often 
the distinction is along the lines of shared versus private sub-processes, 
regardless of whether the private process is an "implementation" of the 
shared process. 
2. Process interconnection language, separate from a specific PML -
This is 
analogous to Module Interconnection Languages, in which a separate nota-
tion is used to denote how modules are inter-connected. For example, the 
Darwin [90] configuration language, the successor to Conic [91]3, enables 
(operating system) processes to interconnect independently of the specific 
language in which they are written, by means of typed ports through which 
data is exchanged between the processes. Ports are protected and made 
accessible through an import-export mechanism (the actual notation in 
Darwin is require and provide). 
The advantage of this approach over the previous one with respect to 
(software) process interoperability is that it can be made dynamic, as is 
the case with Darwin. That is, the nature and kinds of bindings between 
the processes can be changed dynamically. However, since this is still es-
sentially a language-based approach, dynamic changes impose a problem 
in terms of comprehensibility: either the changes do not correspond to 
the original source definitions, which is an obvious problem, or the inter-
connection is not explicitly declared, defeating in some sense the purpose 
of using a language-based approach to begin with. The latter approach 
is taken in Darwin, where the references to the services (or control con-
structs) are passed in messages, allowing to change their behavior, but as 
the authors point out, this feature is not recommended for long-term or 
semi-permanent bindings. 
3. Other distributed programming languages -
This community produced 
numerous languages that support some form of dynamic program config-
uration among relatively independent (operating system) processes. One 
3Not to be confused with the Darwin environment mentioned earlier. 

The Decentralized Model 
65 
representative is Hermes [122], another port-based language in which new 
ports can be added to an executing (operating system) process and ex-
isting port connections can also be changed, by statements executed from 
within the existing Hermes code. New processes can also be added using 
the create of statement, but only from within an existing process. Thus, 
it is not possible to add new facilities that were not anticipated in the 
original program. 
One aspect which is not addressed in either of these language-based approaches 
is the independent-operation requirement: as processes are defined (and later 
enacted) in separate SubEnvs, there must be facilities for enabling such dynamic 
cross-SubEnv bindings, which imply some degree of system support. 
Our solution then is system-based, not language-based. The idea is to take 
advantage of the underlying virtual machine specialized for supporting process 
modeling, and extend the available peE's enactment engine with mechanisms 
to support definition of the model. As such, this approach does not require the 
invention of a whole new PML intended for decentralization, nor does it make 
any assumptions about a particular PML, making it generically applicable. 
The formal model presented below attempts to address all of the requirements 
presented in Section 3.2.1. The central concept here is the Treaty. Some in-
tuition to the model can be gained from the "international alliance" metaphor 
mentioned in the beginning of this chapter. Multiple countries collaborate by 
signing "treaties" determining what kinds of artifacts are allowed to be ex-
changed and how to perform the exchange/collaboration. Once signed, treaties 
have to be ratified by the local parties, so that the full impact of the treaty is 
reflected in each country when enacted. 
The Treaty model addresses directly requirements 1,4 and 5 from Section 3.2.1. 
Requirements 2 and 3 are addressed in Sections 3.2.4 and 3.2.5, respectively. 
3.2.3 
The Treaty 
In the following discussion, the following notation is used: 
• 
Ei denotes an instantiated environment as defined earlier. 
• 
Ai is used to denote a set of process steps that form a common sub-process 
as explained above. Note that in terms of the definition of an environment, 

66 
CHAPTER 3 
Ai is a subset of P, i.e., it does not necessarily contain a subset of T, D, U, 
but it does imply a subset of S (schema) through the types of the formal 
parameters to the activities in Ai. Furthermore, Ai may consist of a set of 
unrelated steps, all of which are part of the common process, or they can 
be interrelated, for example representing a single common task. 
• 
Ai(Ej) denotes sub-process Ai of environment Ej, i.e., a fragment of Ej's 
process model. 
We define the following operations: 
1. export(Al(El), E2) -
Export Al from El to E2, enabling E2 to import 
A l . This operation executes locally at E l . 
2. import(Al (Ed, E 2) -
Get Al from El , and integrate it with E2's pro-
cess. This operation executes at E2 and involves also El. The successful 
outcome of this operation generates Al (E2 ), a local version of A 1 , fully 
integrated with the rest of E2 's process. The exact meaning of "full inte-
gration" is intentionally left out here, since it is PML-specific. Intuitively, 
the idea is that the newly imported sub-process gets interconnected with 
the local process and becomes an integral part of that process. For some 
concrete examples, see Section 3.5. 
These operations form the mechanism to implement common activities. How-
ever, as mentioned earlier, a separate concern is to determine execution priv-
ileges on the common activities, such as which SubEnv is entitled to execute 
a multi-site activity on remote data. In some cases, invocation of specific ac-
tivities cannot be made from some of the SubEnvs, for example, due to tool 
invocation restrictions (e.g., licenses, platforms, location of tool experts, etc.). 
It appears at first that such "execution privileges" semantics could be attached 
to the export and import operations in some fashion. However, early exper-
iments with our implementation revealed that these are indeed separate and 
orthogonal concerns. That it, we separate the issue of how to provide com-
mon multi-site activities from the concern of how to restrict or control their 
application. 
Therefore, we define the following two directives, each of which could be used 
in conjunction with either of the above operations: 

The Decentralized Model 
67 
1. request(AI' EI, E2) -
specify an intent of EI to use Al on data from E2. 
Note that Al can be either exported by EI or imported from some other 
SubEnv. 
2. accept(AI' EI, E2) -
allow Al to be used by EI on data from E2. Once 
again, Al could be originally defined at EI , in which case it was imported 
by E2, or it could be exported by E2 and imported by EI. The latter case 
resembles the concept of a process interface, where the process publishes 
the tasks that can be used by other processes to access its own data. 
To summarize, the four combinations and their intuitive meanings are: 
1. exporLrequest(AI(Ed, E2) -
I (Ed want to use my Al on your (E2) 
~L 
-
--
2. imporLaccept(AI(EI), E2) -
I (E2) allow you (EI) to use your Al on my 
data. 
--
--
3. exporLaccept(AI(EI), E2) -
I (Ed allow you (E2) to use my Al on my 
data. 
-
-
4. imporLrequest(AI(Ed, E2) -
I (E2) want to use your Al on your (EI) 
data. 
--
--
A (simple) Treaty (denoted as T) is a binary relationship between two sites, 
defined as either one of these two possibilities: 
In words, this Treaty allows users operating at EI to execute activities defined 
in Al on data from E2 • We shall refer to this Treaty as "a Treaty from EI to 
E2 , on AI". Both definitions lead to the same outcome, the difference being 

68 
CHAPTER 3 
the origin of At: in expression 3.1 At is initially defined in Et and is exported 
to E2, which imports it; whereas in expression 3.2 At is initially defined in E2 
and exported to ElJ which imports it. 
Thus, a Treaty between two SubEnvs consists of one requester and one acceptor, 
as well as one exporter and one importer. The export-import pair of operations 
establishes a common step (containing multi-site activities), and the request-
accept pair defines which site is eligible to invoke activities from the common 
step (the requester) and which one allows access to its data (the acceptor). 
The gist of the Treaty is that it requires both sides to actively participate (and 
perhaps negotiate) in the agreement that determines their inter-process inter-
actions. In particular, a request on an activity without a corresponding accept 
on the same activity has no effect on either SubEnv (regardless of whether the 
activity is properly imported-exported). As for the order of the operations in a 
Treaty, the main reason for them not being commutative is to protect the pri-
vacy of the exporting process. This means that any implementation of import 
should restrict its visibility only to activities which have been already exported 
by other SubEnvs. 
It is important to understand that the Treaty relationship is not symmetric. 
For example, the Treaty above does not imply that E2 can run activities from 
At on ElJ i.e., it is only uni-directional. This property of Treaties addresses 
the concerns raised in requirement 1 in Section 3.2.1. Furthermore, the Treaty 
is not transitive, and each Treaty between two sites must be formed explicitly. 
(Treaties can be considered reflexive, though, if self-export and self-import are 
defined as "no-ops".) 
The extension of a Treaty to multiple sites is defined as: 
n 
TAl (Et, (E2 ... En» = U TAl (Et, Ei) 
(3.3) 
;=2 
This multi-site Treaty allows users operating in Et to run activities defined in 
At on remote data from some or all of Ei , i > 1. 
To enable symmetric Treaties, we define a (binary) Full Treaty (denoted FT) 
as: 
(3.4) 

The Decentralized Model 
and similarly, a multi-site full Treaty is defined as: 
FTAl (El, E2 ... En) = U FTAl (Ej, Ej) 
i<j 
69 
(3.5) 
This consists of the union of all unordered pairs of binary full Treaties (or all 
ordered pairs of regular Treaties). While symmetric, full Treaties are still not 
transitive, to protect the privacy of sites. 
A Full Treaty allows any participating SubEnv to invoke a multi-site activity on 
data from any other SubEnv in the Treaty. Note that when multiple sites are 
involved, there are many combinations of possible Treaties between the sites 
on the same set of activities, not only simple or full. For example, the Treaties: 
TA (EI' (E2' E3)) 
TA(E2, (EI' E3 )) 
(3.6a) 
(3.6b) 
allow either EI or E2, but not E3 , to invoke multi-site activities from A on 
data from some or all of the three sites. 
As can be seen, this model provides maximum flexibility in expressing inter-
process collaboration, and each participant in a Treaty must explicitly "sign" 
it by invoking the proper operation that reflects its role in the Treaty. 
In order to retract from Treaties, the following operations are defined: 
1. unexport(AI (EI), E2) -
This operation executes in EI. It removes Al 
from further being available to E2 and invalidates possible previous Treaties. 
In addition, it revokes any privileges which were associated with the export 
(see below). 
2. unimport(AI(Ed, E2) -
This operation executes in E2 , effectively re-
moving Al from E2'S process. Like unexport, it invalidates any previous 
Treaties and privileges which were attached to the import. 
3. cancel(AI, EI, E2) -
has the opposite effect of request, i.e., it disallows to 
further use Al at EI on E2. It is issued at the requester end of a Treaty. 
4. deny(AI, EI, E2) -
The opposite of accept, it disallows EI to further 
access E2 's data through AI. It is issued at the acceptor end of the Treaty. 

70 
CHAPTER 3 
Since export and import are the mechanism for establishing shared common 
sub-processes, when unexport (unimport) is executed on a previously exported 
(imported) activity, the corresponding execution privileges property (either 
request or accept) is also revoked (by cancel or deny). The opposite is not 
true, though. A cancel/deny does not imply un export or unimport. For ex-
ample, a requester activity could be transformed to an acceptor activity by 
issuing a cancel followed by accept, regardless of whether it is an exported or 
imported activity. 
In order to enable unilateral withdrawal from the treaty -
which fits well both 
with the operational independence and the process autonomy principles -
all 
retracting operations are local, not involving remote interaction. However, this 
results in an overhead in execution time, as every invocation has to be validated 
at run time, because even if at some point at the past such an invocation was 
well formed in a Treaty, it might not be the case at the time of the invocation. 
This issue is discussed in Section 3.2.6. 
To summarize, Treaties are the abstraction mechanism used to define process 
interoperability. The only way by which a SubEnv can collaborate with other 
SubEnvs is through these pre-defined arrangements that determine how to col-
laborate, and on what artifacts. Consequently, the degree of collaboration (vs. 
autonomy) between each pair of SubEnvs is determined by the "size" of their 
common sub-process. This can range from total isolation (no common sub-
process is defined) -
where the SubEnvs have no means to access each other's 
data but are entirely autonomous -
to total collaboration (the entire process 
is common) -
where the SubEnvs lose any autonomy and logically share the 
same process and data and are perhaps only physically distributed. 
By splitting a Treaty into two independent operations and the Full Treaty 
into four operations (as opposed to bundling them to one global operation) we 
ensure that both ends agree on the Treaty and join it on their own terms. Not 
requiring synchronous execution of export and import enables Treaties to be 
formed incrementally and when each party wants to join them. In fact, of all 
the primitive operations, import is the only operation that requires both sides 
to be simultaneously active. This independent multi-step protocol also enables 
SubEnvs to retract from, and join to, a Treaty, independently and dynamically. 
Finally, it might appear that this approach suffers from being too low-level in 
that it makes it difficult and somewhat awkward to define multi-site Treaties by 
forming multiple pairwise Treaties. However, this formalism ensures maximum 
process autonomy in all involved sites. A particular implementation might use 
"macros" or "scripts" that perform all the necessary operations automatically 

The Decentralized Model 
71 
to form Treaties between ''friendly'' sites in cases that privacy can be compro-
mised for simplicity and convenience. Alternatively, an implementation might 
decide to bundle some of the operations. For example, it could always implic-
itly associate export with request and import with accept, or vice-versa, but 
not both. Or it could set defaults for the combinations but allow the expert 
process administrator to modify them. Finally, the PCE can make provisions 
for enabling a user to be an administrator on multiple SubEnvs, so that in en-
vironments that allow multi-site administrators (e.g., when the interoperability 
is between tightly-coupled SubEnvs), it is possible to bundle the Treaty as one 
operation, without violating autonomy. Several of these alternatives were in 
fact implemented in Oz (see Chapter 4 for details). 
3.2.4 
Defining Common Sub-Schemas 
There are several alternatives to defining common sub-schemas, with differ-
ing degrees of flexibility and complexity. The simplest and most restrictive 
approach is to require a global schema, i.e., all SubEnvs must have identical 
schema. This is obviously too restrictive and counter-autonomous, and in par-
ticular it prevents the bottom-up approach of forming Treaties over pre-existing 
environments. While some common sub-schema has to be formed ultimately, 
the goal is to minimize its extent. In addition, given the share-nothing archi-
tecture, it would be very hard to guarantee that the the complete schemas, 
which are maintained locally at the SubEnvs, are kept identical. 
A second alternative would be to require global product-data sub-schema, but 
to allow variation in the process-data sub-schema. (Recall that product data 
are the actual artifacts under development, e.g., source files, design documents, 
and so forth, and process data is used by the PCE to manage the project, e.g., 
a source file's version, its compilation status, etc.) The rationale behind this 
division is that in cases where the product data is heavily shared, it provides 
for more freedom in defining the process model while still requiring a common 
denominator for definition of the product. The opposite alternative would be 
to require global process data and local product data -
in cases where a global 
process needs to be defined but security and privacy of local data is important. 
While these approaches support a larger degree of autonomy (process and data, 
respectively), they are still too restrictive. In the former approach, some local 
processes might still need to retain additional private product data, and in the 
latter approach some SubEnvs might need private process data for extensions to 
the local process. In addition, data might not always be clearly classifiable into 
one of the two categories. Thus, while the distinction of process and product 

72 
CHAPTER 3 
data is conceptually important, it seems like the wrong criteria for determining 
what parts of the schema to make common. 
The third and most general approach would be to have both common and pri-
vate sub-schemas, regardless of the type of attributes/classes involved. Here 
again, a restricted approach would be to require the common sub-schema to 
be shared by all SubEnvs, and a more general approach would require only 
pairwise common sub-schemas that match the corresponding pairwise Treaties. 
This approach is the most flexible, and fits well with the overall research require-
ments, but it is also the hardest to realize. The problems associated with this 
approach fall in general under the domain of schematic heterogeneity, a topic 
that is investigated by the heterogeneous database community (e.g., see [81]) 
and is largely beyond the scope of this book as a research topic, although a 
practical solution is given in Section 4.3.3. 
3.2.5 
Sharing Data Instances 
Requirement 3 in Section 3.2.1 indicated the need to identify data instances 
which should be made accessible to Treaty subtasks. At the highest level, 
there are two main alternatives to address this issue: (1) tie the export of data 
instances with the export of common-sub tasks; or (2) treat export of data 
instances as orthogonal to defining common-processes. The first alternative 
implies that processes or tasks are defined on particular data instances. While 
this might be true in some cases, it is a narrow view that necessarily restricts 
the notion of a process and its scope. A more generic and realistic view of a 
process is that it is defined over classes of instances, and may over time be 
bound to, and execute on, different instance sets of the project database. In 
addition, it is quite possible that Treaties (as well as local processes) are defined 
before the creation of instances that are used in that Treaty, in which case the 
first alternative is impractical. 
Taking this view, our approach is to define an access-control mechanism that 
defines which SubEnv can access which data, through which operation. For-
mally, we define the operation 
to denote: make data instances Dl of the local SubEnv El accessible to remote 
SubEnv E2 in the specified access mode (e.g., read, write) under operations 

The Decentralized Model 
73 
from A2• Note that this operation is meaningless if A2 is not a part of a Treaty 
between Ei and E2 , and therefore could be checked at definition time. 
It is important to note that this operation, like the rest of the model, ignores 
issues that have to do with particular databases, PMLs, and PCEs, which 
might require prohibitively expensive implementation of these operations. In 
some cases it might be necessary to restrict the model in order to make it 
feasible. For example, the Oz implementation (given in Section 4.3.4) does 
not specify the last parameter, only allowing to distinguish between SubEnvs, 
not specific activities. Another PCE-specific issue is the granularity of access 
control. 
3.2.6 
Independent Local Evolutions 
We discuss now briefly how the Treaty model can support local evolutions, and 
some tradeoffs. A fully detailed account of this subject is given in Chapter 4. 
We assume that environments are evolved by the process administrator, who 
modifies in some way the process and/or the schema and reloads them into the 
environment, possibly upgrading the existing populated product and process 
database. 
The most important integrity constraint associated with the validity of a Treaty 
is what we refer to as the common sub-process invariant, which simply states 
that a common sub-process that was defined through a Treaty, must remain 
identical in all participating SubEnvs to retain its validity. Since there is no 
shared space in which Treaties are stored, this is the only way to guarantee that 
original Treaties have not been altered by the time they are invoked on remote 
data. Thus, if due to local evolutions a Treaty step is altered only in some 
but not all SubEnvs, the Treaty should be invalidated. Another important 
constraint that we impose is that evolutions of any sort cannot occur in a 
site while a Treaty activity is being executed in its SubEnv. In other words, 
Treaties cannot be invalidated in the midst oftheir execution. This constraint is 
essential to ensure that Treaties are executed "as agreed" (Section 3.3 discusses 
execution semantics of Treaties). 
Given a Treaty TAl (Ei' E2), several kinds of evolutions could make it invalid: 
1. Ei'S process is evolved, possibly modifying activities in Ai' Then the 
treaty might no longer be valid since Ai could differ arbitrarily from the 

74 
CHAPTER 3 
version agreed upon when the Treaty was signed. Therefore, whenever 
a multi-site activity is invoked from El on data from E2, E2 has to dy-
namically check whether its version of the imported activity is identical 
to the version invoked by El . A convenient mechanism is to associate a 
timestamp with each activity (see Section 4.3.2). 
2. E2's process is evolved, possibly modifying activities in its own version of 
A l . In this case the treaty is again no longer valid, and Al (E2) should be 
outdated and its validity checked at execution time. 
3. El cancels Al on E2, disallowing users in El to further invoke activities 
defined in A l , from El , on data from E2 • To cope with this evolution, 
each time a multi-site activity is attempted at El , El must check locally 
whether it is still allowed to invoke such an activity on data from E2 • The 
reader might wonder why to check locally for the validity of one's own 
operations. The answer is that SubEnvs in general involve multiple users, 
and even though a cancel was carried out by the local administrator, other 
users, perhaps not aware of the cancellation, might still try to invoke A l . 
4. E2 denies El to further execute Al on its own data. This evolution has 
the most significant implications with respect to Treaties, as it enables an 
accepting site to leave the Treaty unilaterally. Note that "leaving" the 
Treaty is not "breaking" it, since the Treaty is not misused, and is not left 
in the midst of its execution as indicated above. It is simply prohibited. To 
allow such one-sided deny operation, each time a multi-site activity issued 
at a coordinating site requests to access remote data, the remote SubEnvs 
must check dynamically whether the issued activity is still accepted, and 
reject it if it is not. 
The approach outlined above is consistent with our concerns for maximum lo-
cality and autonomy, both logically and physically. An alternative approach 
at the other end of the spectrum is to attempt to immediately notify all in-
volved SubEnvs when any of the above operations occur. For example, a deny 
operation would notify the denied SubEnv, which in turn would invalidate the 
Treaty at the "source" SubEnv. This approach has the advantage of reduc-
ing the number of invalid invocation requests, and more importantly, possibly 
eliminating the need for dynamic validation of Treaties. However, since it is 
possible that at a given time only some but not all of the SubEnvs are active or 
reachable on the network, it is in general impossible to eliminate altogether the 
dynamic validation mechanism. Moreover, this approach unnecessarily tightly 
binds the SubEnvs and incurs significant communication overhead, which is un-
acceptable when the SubEnvs are arbitrarily distant. For example, each local 

The Decentralized Model 
75 
evolution in a SubEnv would imply immediate broadcast to all other SubEnvs 
that have any Treaty arrangement with it. Finally, for this "immediate-update" 
approach to be effective, cross-site operations must be atomic (all or nothing), 
or otherwise all dynamic checks would still have to take place, defeating the 
purpose of the update in the first place. But preserving atomicity, particularly 
for distributed operations, involves significant overhead (for example, support-
ing context sensitive rollback) and require global control, two good reasons to 
avoid this approach. 
3.2.7 
Inter-process Consistency 
The final requirement in Section 3.2.1 regarding incremental and dynamic in-
clusion and exclusion of common sub-processes, introduces the problem of pre-
serving the local process consistency when a process is augmented with a new 
decentralized sub-process, or such an extension is removed. In particular, viola-
tions of process consistency that cannot be tolerated must somehow be rejected. 
The key to the solution of this problem lies in the observation that this is a 
subset of the more general problem of process evolution, where a local process 
is modified by adding (removing) new activities to (from) it. Since there is 
no global process, there are no global consistency considerations, reducing this 
problem to the same problem as in single-site peEs. Thus, this issue is not 
discussed here any further. The solution in Oz is to use the Evolver utility, 
which was covered earlier in Section 2.2.6. 
3.3 
ENACTING PROCESS 
INTEROPERA~ILITY: THE SUMMIT 
Given the Treaty model for defining process interoperability, the second major 
issue is to support the enactment of multiple interoperating processes. Once 
again, the major requirements that affect this model are autonomy, indepen-
dent operation, and flexible interoperability. Although a Treaty requires some 
mechanisms that enable its operation, inter-process enactment requires much 
more infrastructure support, since it implies extending significantly the process 
engine. 

76 
3.3.1 
CHAPTER 3 
Alternatives, Design Choices, and 
Justifications 
At first glance, there are two ways in which a multi-site task can be executed: 
(1) one SubEnv (call it the coordinating SubEnv) copies remote data into its 
own space and executes locally, or (2) the task leaves the data where it is, and 
requests that its activities be executed by the remote SubEnvs. This is similar 
to the two main approaches to distributed program execution: fetch the data 
and execute locally, or send a request for remote function execution. There are 
obvious tradeoffs between the two approaches, and the superiority of one over 
the other largely depends on the nature of the program and the volume of the 
data involved. 
However, since a multi-site task inherently involves more then one process, nei-
ther of these approaches is always feasible or desirable: (1) Process autonomy 
restricts application of the data fetching approach, since some of the remote 
data might not be accessible to the executing process, and even if it is, the 
prerequisites and consequences determined by the coordinating process might 
not maintain consistency with respect to the remote process(es). (2) The func-
tion sending approach does not address activities that manipulate data from 
multiple (local and remote) processes, but instead assumes that an activity's 
arguments all reside in the same SubEnv. In addition, as mentioned earlier, 
tools invoked by an activity may not be available at a remote SubEnv (in fact 
such a scenario might be the initial motivation for running the activity in the 
originating site), and even copying the tools might not work if the SubEnvs 
operate on heterogeneous platforms or if there are licensing restrictions. 
We devised a third hybrid approach, which combines the two approaches men-
tioned above in a manner that ameliorates their limitations. At the activity 
level, remote data is fetched and modified locally; but at the process-step level, 
any subtasks emanating from prerequisites and consequences are executed at 
the remote SubEnvs. This permits activities with arguments from multiple 
SubEnvs, executes tools at the same site as their process, and maintains con-
sistency in process and product data according to the local processes owning 
the data. 
3.3.2 
The Summit 
Following the "international alliance" metaphor mentioned in the introduc-
tion to this chapter, our decentralized enactment model can be described as a 

The Decentralized Mode! 
77 
"summit meeting". Before the meeting (multi-site activity), each party (pro-
cess) handles local constraints (prerequisites) that are necessary for the meeting 
to take place; then the meeting is held at one location (SubEnv), where the 
various parties send representatives (data) to collaborate; once the meeting is 
over and agreements were made (results of the activities), all parties return 
home (to their SubEnvs) and carry out the implications (consequences) of the 
meeting locally. Summits can lead to subsequent Summits, each involving a 
subset of the parties, possibly with different representatives (data arguments). 
Similarly, process interoperability takes place when an activity is invoked (either 
manually by an end-user or automatically by the process engine) on data from 
one or more remote SubEnvs. The case of only local data from the same 
SubEnv does not lead to inter-process collaboration, and is handled however 
it would normally be done by the underlying single-process peE. We call the 
process from which the multi-site activity is invoked the coordinating process. 
The Summit protocol consists of the following phases (not all of which must 
necessarily exist in all implementations): 
1. Summit Initialization and Validation -
First, the coordinating process in 
which the Summit request was issued, establishes a task context (neces-
sary to support interleaved execution of multiple activities) and allocates 
the necessary resources needed for the Summit. It then binds the actual 
parameter objects (at least one of which is remote, or otherwise this would 
not be considered a Summit) to the formal parameters of the activity. 
Initialization is followed by validation, checking whether the Summit is 
allowed to be executed. For example, a multi-site activity A1 is eligible for 
execution from site E1, on data from site E2 and E3 only if it conforms 
with the Treaty protocol, i.e.: 
(a) A1 was defined as a common activity by means of any valid combi-
nation of export and import operations. For example, it could have 
been exported by E1 and imported by E2 and E3, but it could also 
have been imported by E1 and exported from E2 or E3. 
(b) A1 has been requested in E1 and accepted in E2 and in E3. 
However, while a necessary condition, this static property is not a suffi-
cient condition for allowing the Summit, since by the time the Summit 
is invoked for execution, local evolutions might have violated the Treaty. 
Thus, an additional runtime Treaty validation phase is required. Vali-
dating whether the specific data instances are accessible to the remote 
processes is determined at the remote SubEnvs by checking their local 

78 
CHAPTER 3 
access-control mechanism to see if the instances are properly exported to 
the coordinating SubEnv, as outlined in Section 3.2.5. 
2. Pre-Summit -
The involved processes (i.e., those that own some of the 
data requested by the multi-site activity) are notified, and all of them 
(including the coordinating process) perform simultaneously and asyn-
chronously pre-Summit process actions, each according to its local process, 
with its local data, in the local SubEnv. Examples of pre-Summit actions 
include, not necessarily in this order: (1) Verification that prerequisites 
imposed by the process step enclosing the activity are satisfied (locally); 
(2) Verification that the activity can be executed with respect to the task 
workflow; (3) Active invocation of related activities, e.g., to satisfy (1) and 
(2); and (4) Deriving and binding data arguments that are required by the 
activity but were not specified as parameters (for example, a compile ac-
tivity on a C source file may require to bind all the #included header files 
for the activity). Pre-Summit requires that all involved SubEnvs identify 
the same requested activity, in order to know what to verify jsatisfy. This 
is guaranteed through the import mechanism of the Treaty. 
It might be possible in some cases (depending on the PML as well as the 
specific activity) for the coordinating process to determine locally whether 
or not launching remote pre-Summit is necessary for each participating 
SubEnv, in which case no "fan-out" to the local sites is required. In gen-
eral, however, the local SubEnvs need to be able to decide for themselves 
whether or not they need to undertake any work. The main point is the 
locality of the enactment, which is determined solely by each SubEnv on 
its local data, without "global" intervention. 
3. Summit -
If pre-Summit is successful in all involved processes, the re-
quested activity is invoked in the coordinating process, with all the neces-
sary local and remote data arguments. Note that for the time being, we 
restrict the invocation of a summit to occur not only at one time, but also 
at one location (i.e., the coordinating process). An extension of this phase 
that enables to perform summit activities across multiple sites simultane-
ously (e.g., groupware activities) is discussed separately in Section 3.6. 
4. Post-Summit -
When the Summit completes, all involved SubEnvs are 
notified, and all of them (including the coordinating SubEnv) perform 
simultaneously and asynchronously post-Summit process actions, again 
each according to its local process, with its local data, in the local SubEnv. 
Examples of post-Summit actions include, not necessarily in this order: (1) 
Assertions on the process and product data that reflect the fact that the 
various activities were executed (depending on the peE, it may not always 
be possible to directly modify such data within the activities themselves); 

The Decentralized Model 
79 
(2) Binding and assignment of data affected by the activities that were not 
supplied as arguments; (3) Verifying that consequences imposed by the 
steps in the Summit can be fulfilled (this is not always a logical implication 
of the pre-Summit verification); and (4) Triggering execution of further 
activities, e.g., as part of (3). 
5. Summit Completion -
When post-Summit completes in all local sites (in-
cluding the coordinating SubEnv operating in "local" mode) the coordi-
nating SubEnv checks whether further Summits are pending (see below 
on how composite Summits are formed). If any Summit is pending, the 
algorithm returns to step 1. If no Summits are pending, the Summit is 
completed by releasing all resources associated with the Summit. Note 
that this is a "tail-recursive" algorithm, in that its last step calls itself. 
Thus, all participating SubEnvs act as if the activities in the Summit took place 
in their local process with local data only -
in the sense that they all carry 
out the (pre- and post-) implications of the Summit activity -
although only 
the coordinating SubEnv really executes the activities. The interesting point 
here is that both pre- and post- Summit phases occur in each SubEnv only 
according to its local process, while execution of the Summit phase involves 
collaboration among the participating SubEnvs. This design minimizes the 
interference between the processes (and hence maximizes autonomy) while still 
allowing them to carry out the desired common activities as agreed upon in the 
Treaty. 
Composite Summits 
The Summit algorithm presented above indicated the possibility of multiple re-
lated Summits executing as one unit. Indeed, the capability to enact multiple 
Summits (or multi-activity Summits) is crucial, as can be evidenced even from 
the example given earlier in Chapter 1. While a complete answer necessar-
ily delves into the particulars of a specific PML (and indeed such a solution is 
given in Chapter 4) -
the general idea is that subsequent Summits are invoked 
through whatever enactment and binding mechanisms the PCE and its asso-
ciated PML provide. They might be specified explicitly in an imperative-like 
PML with static binding, and therefore invoked in a straight-forward manner, 
or inferred implicitly through the PCE's inference mechanism (e.g., rule chain-
ing, or Petri-net transition) with dynamic binding capability to bind remote 
objects to new Summits. The point is to be able to distinguish between deriva-
tion of local activities (which are therefore still part of post-Summit) versus 
new Summit activities. 

80 
CHAPTER 3 
Thus, a composite Summit (e.g., consisting of multiple Summit activities) can 
be viewed as alternating between "local" mode -
whereby each participating 
site (including the coordinating site) performs local operations -
and "global" 
mode in which the coordinating process carries out operations involving remote 
data, with the approach intended to minimize the "global" mode and maximize 
the "local" mode. 
Finally, a note on concurrency: the Summit protocol is primarily designed for 
increased autonomy. Nevertheless, this design also contributes to increased 
concurrency (and hence throughput) since the local (and thus independent) 
operations can potentially execute in parallel. 
Transactional Semantics 
The details of the operational and transactional semantics of the Summit are 
left undefined in this chapter since they are inherently PML- and peE-specific. 
A detailed solution is given in Section 4.4.5. Nevertheless, some of the issues 
that come up are generic, and are worth mentioning here. 
For one thing, it is mandatory that some transactional semantics be attached 
to the Summit. In particular, there must be mechanisms that preserve the 
atomicity of at least a single Summit, which means that distributed abort and 
rollback as well as distributed commit of Summits must be supported. While 
autonomy concerns should playa role in the design of distributed transaction 
management supporting our model, it is in general impossible to provide global 
consistency as well as complete local autonomy in transaction management (as 
shown in [121]). The approach to be taken, then, should be directed towards 
minimizing the global atomicity requirements of Summits. 
Another problem with supporting transactions in our model is that interleav-
ing among distributed tasks is inherent and unavoidable, making it harder to 
preserve atomicity in general and serializability in particular. (It is trivial to 
preserve the serializability of non-interleaving, and thus serial, execution.) The 
reason is that when a Summit task "fans-out", the coordinating SubEnv must 
switch contexts, or else the SubEnv would be blocked indefinitely (until all re-
mote SubEnvs finish their local execution), not able to serve other request, or 
even worse, deadlock. 
This effectively creates an interleaving among distributed transactions each 
time a Summit alternates between global and local modes of execution. 

The Decentralized Model 
81 
The transactional semantics of Summits in Oz are presented in Chapter 4, 
where they are tied to the notions of consistency and automation forms of en-
actment. However, the actual implementation of the decentralized transaction 
manager that supports these semantics is for the most part beyond the scope 
of this dissertation, and is part of Heineman's dissertation, covered in [58]. 
3.4 
THE MOTIVATING EXAMPLE 
REVISITED 
Recall the motivating example introduced in Section 1.3 (Figure 1.1). Fig-
ure 3.4 illustrates its enactment using the Summit protocol. Note that each 
box in the Figure does not necessarily represent a single activity, but rather 
a subtask that might be broken down into a fine-grained set of process steps. 
The change activity is initiated by the coordinating SubEnv SE2. Pre-Summit 
takes place in a decentralized manner, where each SubEnv performs the Review 
activity locally according to its own process. For example, SE3 requires an ad-
ditional analysis step before the review and both SEl and SE2 require a 
check-out phase using different configuration managers (RCS and SCCS, re-
spectively). Once reviewed by all sites, the Summit activity approve is exe-
cuted, determining whether to approve or disapprove the change based on the 
local reviews. If the approval step succeeds, the modify activity is executed, 
where the objects are modified. When finished, post-Summit begins, again in 
a decentralized manner. All SubEnvs are engaged in a unit-test step, but each 
one does it according to its own process. For example, SE3 employs a manual-
test procedure (e.g., for testing the user interface) which involves human users 
that actually perform the tests (devising the input sequences for the test suites 
can be also done manually or automatically for either manual or automatic 
testing), whereas the other SubEnvs perform automatic testing, but SE2 has 
an additional code-inspection step. Completion of the local testing leads to 
integration-test, another Summit activity in this composite Summit. 
It is important to understand that Figure 3.4 depicts a particular execution 
trace of the process, not the whole process. For example, figure 3.5 shows a 
different execution trace of the same process, where this time the review phase 
fails at SEl, requiring a revision in the proposed change, after which a second 
review succeeds and leads to the modify activity. This example illustrates how 
the Summit protocol can support some form of process negotiation, where the 
Summit activities represent the negotiation table, and the local implications 

82 
CHAPTER 3 
I 
I 
SEt 
SE2 
I 
SE3 
I 
I 
______________ L ______________ L ______________ . 
Chonae(SI, 81, S3) 
Inlr-SummH 
I 
I 
I 
I 
I 
I 
I 
I 
I 
---4--------------· 
I 
I 
•................... ................. . 
l--'-T',----I 
r·······································; 
l.~~.~~~I~.~~~S.~~ ..... j 
t······························· .. 
: 
: 
~~~'-: 
Pre-SummH 
-~--------------. 
SummH activities 
~ ............................... -\ 
. 
. 
. 
. 
i <Gde-IDlpecl(81) i 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
: Auto-ml(SI) 
L.. ........ ""[" ................ ..i 
; 
. 
, 
I 
i Mon .. l-teot(S3) i 
L. ......... ~ 
............... .! 
r-···:~~~·~:;~~~······1! 
l. ........................... .l 
Posr-SUmmH 
L. ............................. .: 
I 
--------------: ---- ----- -:-------------_. 
I 
I 
Summa activity 
I 
IDlearalion-leol(SI,Sl,S3,s) 
I 
I 
I 
--------------~--------------~--------------. 
I 
I 
! 
! 
Figure 3.4 Enactment of Motivating Example 

The Decentralized Model 
I 
SEI 
! 
SEl 
SF.1 
I 
I 
______________ L ______________ L ______________ . 
r·· .... ····_··.. 
"""'''''''''j 
\_1) 
i 
L. .. _ .. _ ...... .......::~.-...... -J 
........ 
fll ............ 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
---~--------------. 
I 
~-·-···-·-··-··-··-··-··-·1 
I 
r····~·-···l 
:0 ...... _ •• _ •••••••• _ • ...: 
i _2)! 
~~~~~~~~~1~~~~:~~~:; 
r·-" .. ··_·· ·_·· .. ··-i 
, 
...... (5.1) 
i 
I 
, 
1. .... _._ •• __ ..... oo_.J 
--+-------------_. 
r---:..u"-----,i 
L....:.:::::':":"''':'''=':'::'':'':-=-------.J! 
_51,52,53) 
II 
Figure 3.5 Another Enactment of Motivating Example 
83 
represent private consultations. A full example of such a negotiation-based 
process is given later in Chapter 6. 
3.5 
APPLICATION OF THE MODEL 
We describe now how the model can be applied to three families of PCEs 
categorized by the paradigm underlying their PMLs, namely rules, Petri-nets, 
and grammars, and outline how the model might be applied in the APPL/ A 
process programming language. These families were chosen to cover most of 
the known PCEs [79]. 

84 
CHAPTER 3 
Control Flow 
Task Graphs 
Grammars 
Decomposition 
Petri Nets 
Rules 
Figure 3.6 Comparison of PMLs 
Each PML style has its own strengths and weaknesses with respect to process 
modeling, and there is no "perfect" process modeling (some of these differences 
are overviewed in [73]). Further, a particular PML might arbitrarily divert 
from the properties that identify its family. Nevertheless, these PMLs can be 
generally characterized in terms of their support (or lack there of) for explicit 
(implicit) control-flow, local constraints, and modular decomposition (other 
important aspects of the PML are ignored in this discussion). In general, rules 
are best suited to defining local constraints and automatic inference of process 
steps, but multi-rule tasks are implicit and rules are inherently low-level with 
respect to supporting modularity and process decomposition. Petri-nets are 
good at explicitly defining the control flow of the process, but even though 
some versions enable sub nets to be defined as means of decomposition, they are 
still inherently low-level in that respect, like rules. Grammar-based peEs give 
implicit control-flow support, and modular decomposition support. Finally, 
Task Graphs provide both explicit control flow and modular decomposition. 
Figure 3.6 summarizes this characterization along two axes: implicit vs. explicit 
control flow, and low-level vs. high-level decomposition. 

The Decentralized Model 
85 
Since enactment in PCEs is heavily influenced by the type of PML used to 
model processes, we cover in the sequel Summits in full. As for modeling, 
though, since the Treaty approach is not language-oriented, most of the oper-
ations are not considered here. The main issues that are covered with respect 
to Treaties are: (1) identification of appropriate units of commonality and (2) 
implementation of the import operation. 
Finally, since we take the existing PMLs as given, the uninitiated reader should 
see the cited references for background and justification of each approach to 
process modeling. 
3.5.1 
Rule-Based PMLs 
In general, a rule represents a process step in our context hierarchy, consisting 
of an optional action (activity) with its pre-condition (prerequisites) and post-
condition (immediate consequences). Some rule formalisms consider the action 
to be optional, permitting "inference rules" where the pre-condition directly 
implies the post-condition, while other formalisms have only two parts, usually 
with the action and post-condition merged together. Parameters and variables 
are represented by symbols used in the rule, which are often but not necessarily 
typed (the restriction to data of the appropriate type is the simplest form 
of prerequisite). The process step corresponding to a rule is enacted by first 
evaluating the pre-condition; the action is initiated only if the pre-condition is 
true. Completion of the action leads to asserting the post-condition. 
Tasks are implicit in the possible rule chaining. Backward chaining involves 
matching the pre-condition of a rule with some rule whose post-condition might 
cause some subpart of the pre-condition to be satisfied. Then the firing of the 
second rule is considered recursively. Forward chaining arises when the action 
or post-condition of a rule fulfills the pre-conditions of some rules, which are 
then fired recursively. Rule-based PMLs can be roughly divided into backward-
chaining oriented such as Prolog-based Darwin [94], forward-chaining oriented 
such as AP5 [28], and those that incorporate both, like Merlin [128]. 
The rule is the natural unit of commonality for import-export, although in 
practice the import can be carried out on a set of logically related (but nev-
ertheless independent from each other) set of rules. Implementing import is 
relatively straightforward for rules, since the relationships to other rules is de-
termined implicitly through predicate matchings as outlined above. If the PML 
supports static compilation of the rule set into a rule network, are-compilation 

86 
CHAPTER 3 
is necessary. Otherwise, the new rule has to simply be added to the rule base 
and no further recompilation is needed. 
The Summit protocol applies to rules as follows: when a multi-site rule is fired, 
either directly by a user or indirectly through automatic chaining (we assume 
that the PCE has mechanisms to bind remote objects as rule parameters), the 
following takes place: 
1. Summit Initialization and Validation -
The participating SubEnvs supply 
data to be bound to the symbols of the rule, and the validity of the Summit 
request is performed as outlined in the generic model. The details of how 
this is accomplished depends on the PCE. 
2. Pre-Summit -
The condition of the rule is evaluated. Although in some 
cases (depending on the PML and the particular rule) the condition eval-
uation could be broken down to local sub-conditions which could be then 
evaluated in a distributed manner, single conditions that involve data from 
multiple sites must be evaluated centrally in the coordinating site. For ex-
ample, if a and b are symbols bound to objects al and bl, respectively, 
where al and bl belong to different SubEnvs, then a condition of the form: 
if( a.status = b.status) 
must be evaluated in the coordinating SubEnv. Thus, while part of the 
condition evaluation can be possibly distributed for optimization purposes, 
it cannot always be fully distributed. 
When the evaluation completes, if the condition is not satisfied (or at least 
is not already known to be satisfied), all SubEnvs with data that does not 
meet the condition are then notified. In backward-chaining PCEs, each 
SubEnv may then activate other rules in its local process, in an attempt to 
satisfy (or verify) the failed pre-condition on its own data -
possibly in a 
backtracking manner trying multiple alternatives. In any case, if the pre-
condition cannot ultimately be satisfied, then the rule execution is halted 
in the coordinating process. 
3. Summit -
The multi-site activity is executed in the coordinating SubEnv, 
involving both local and remote data. The activity may generate output 
to be bound to the symbols in the rule; again, the details are PCE-specific. 
4. Post-Summit -
On completion of the Summit, the coordinating process 
fans-out with the relevant output to the remote SubEnvs. All sites (includ-
ing the coordinating site in "local" mode) in turn assert the post-condition 

The Decentralized Model 
87 
of the rule on their own data. In forward-chaining systems, this leads to 
triggering of other rules in the local SubEnvs whose pre-conditions have 
become satisfied. However, while inferring rules for forward chaining, any 
discovered rules which are Summit rules (i.e., they fulfill the necessary 
Treaty requirements and have remote objects bound to them) are deferred 
until after the post-Summit phase completes (see below). Each SubEnv 
notifies the coordinating SubEnv when it completes its local post-Summit 
phase. 
5. Summit Completion -
At this phase the coordinating SubEnv checks 
whether there are further Summits pending, in which case it starts an-
other Summit, or if no more Summit rules are pending the (composite) 
Summit is completed. 
Since the Oz PML is rule-based, we defer further discussion of rules to Chap-
ter 4. 
3.5.2 
Petri-Nets 
The Petri-net [102] is a powerful formalism for modeling concurrent systems, 
and it has been widely applied to software process modeling. The application 
of our decentralized model to Petri-net-based PCEs is influenced primarily 
by SLANG [6] and FUNSOFT [46], and their corresponding PCEs SPADE 
and MELMAC, respectively. Each of these PMLs is based on extended Petri-
net formalisms (specifically, SLANG is based on ER nets, and FUNSOFT on 
predicate/transition nets), but we will stick for the most part with the general 
Petri-net formalism. 
Transitions usually represent our notion of activities (note that our activities 
are different from SLANG's notion of activities, which are more like our notion 
of a task). The equivalent of a process activity that involves (possibly external) 
tools is termed in SLANG a black transition, and in FUNSOFT it is called a 
regular agency. 
Places represent the activity's formal parameters. When places are typed, the 
input places can be viewed as prerequisites on the transitions, and the output 
places as immediate consequences on transitions. 
A predicate on the actual parameters (tokens, see below) can be attached to a 
transition and must be satisfied prior to firing the transition. The predicates 

88 
CHAPTER 3 
define local constraints on an activity, as opposed to the general control flow 
expressed by the topology of the net. Both languages support the notion of 
a predicate. In SLANG they are called guards, and in FUNSOFT simply 
predicates. 
Tokens (or the marking of the net) represent the current state of the process 
under execution and the product data used in the activities. A transition is 
said to be enabled when its input places contain the sufficient quota of tokens 
(with the right types) and the predicate on the transition is satisfied. 
Finally, a single net can be divided into several subnets, or they can be nested 
in a hierarchy in which case a subnet is represented as a transition of its su-
pernet, providing better abstraction and decomposition mechanisms. Such a 
subnet corresponds to our notion of a task (if the PML does not support subnet 
constructs, tasks are implicit, like in basic rule-based PMLs). 
A transition along with its attached predicates and input and output places cor-
respond to a process step, and is necessarily the minimal unit of commonality 
for Treaties, since it is impossible to alter the input or output places of a tran-
sition without having to modify the transition itself (this would be analogous 
to allowing to change the number or types of the parameters to rules). Also, 
the predicate is a local constraint on the transition and therefore conceptually 
part of it. 
The import operation in a Treaty is more complicated than in the case of 
rules, mainly due to the explicit topology of the net. That is, while in the case 
of rules the relationships between the imported and the existing rule sets can 
be inferred automatically, in Petri-nets there must be manual modification of 
the net to integrate the imported process steps. The integration of a process 
step into an existing net involves: (1) merging (or adding new) output places 
of local steps with input places of the imported step; and (2) merging output 
places of the imported step with (possibly newly created) input places of local 
steps. These operations effectively merge the imported (common) step with 
the local process (net). It is not mandatory, however, to connect an imported 
step to the net. There might not be opportunities to do so, just as it is possible 
that in rule-based PMLs an imported rule will not match with any local rule, 
leaving it isolated, in which case pre- and post-Summit become trivial. 
The Summit protocol starts when a common transition is attempted, and the 
input places contain some tokens representing remote objects (again, we assume 
remote binding capabilities which are provided by the underlying peE): 

The Decentralized Model 
89 
Summit initialization -
The unusual aspect of this phase is the binding pro-
cedure. While only the coordinating SubEnv actually binds data arguments 
to its input places, all involved SubEnvs mark their nets like the coordinating 
SubEnv, except the tokens in the non-coordinating SubEnvs are merely stubs. 
Pre-Summit -
The transition's predicate (if any) is evaluated at the coordinat-
ing site, and if not satisfied, the involved SubEnvs are notified. Since Petri-net 
based PMLs are usually not extended to support the equivalent of backward 
chaining in rules, pre-Summit might not exist or degenerate to condition eval-
uation if needed to be performed in a distributed manner. 
Summit -
The transition is fired in the coordinating SubEnv, invoking an ac-
tivity on the data arguments. When the activity finishes, all involved remote 
SubEnvs fire the transition without executing the activity. If there is a condi-
tional branching that depends on the result of applying the activity, then the 
same "return code" is used in all SubEnvs to properly direct the flow of tokens 
to the output places. 
Post-Summit -
All associated SubEnvs transfer the appropriate tokens from 
their input to their output places. This can lead to firing of local transitions 
depending on the local nets. When local firing of transitions that were trig-
gered by the Summit transition completes, the remote SubEnv notifies the 
coordinating SubEnv. 
Summit-Completion -
The coordinating SubEnv checks if new Summits can 
be derived from the previous Summit, based on further connections in the 
coordinating SubEnv's net. If none exist, the Summit is complete. 
To summarize, one way to look at a Treaty and a corresponding Summit in 
Petri-nets is as an "intersection" sub net which is shared by the participating 
local nets (although possibly with different usage privileges), whereby each local 
net has its own private connections to the subnet, and its own "role" in the 
shared subnet, in terms of sending the data that is necessary for enacting the 
Treaty subnet. 
An Example 
The following example, depicted in Figure 3.7, illustrates how Treaties and 
Summits can be applied in Petri-nets. This is a multi-process extension of an 
example which was originally given in [7] describing SLANG. 

90 
CODE 
module to 
be ediled 
edit 
edited 
module 
compile 
compiled 
module 
BocI-
e.dID, 
-----1 
CHAPTER 3 
1 
1 
1 
1 
1 
1 
1 
1 
1 : 
r··TESy·················-······························1 
I! 
old package 
! 
!-ii~~~i 
I 
1 
end 
\ 
~" .. IIIIIIIIIIIII .. I.II .... 1 
I 
cedllle 
1 
: 
itest 
1 
r 
J\···········~~;=;··········r······j 
! 
package 
! 
j 
~de 
I 
. § 
I 
... __ ~ 
.... .J 
li;;7~\.;~:~L;;;::;;;;;;;;;J 
1I:SI 
1 
ii 
code 
1 
! i 
1 
test 
1 
& 11-------I 
output 
1 
~ ii 
1--', 
1 
:: 
I..... 
mum 
1 
iii .,.. results 
1 
i I! 
! 
li:~ 
: 
.. 
~ 
C 
---t- J 
II 
~stfalled 
I 
L~:.'.'.·.~.'.'.·.·.·.·.·.'.'.'.'.'.·.'.,::.'.'.·.'.'.·.'.'.'.' .•.•.•.•. '.'.'.' ........ : •••.•• ' ••. ':.'.'.' ......... '.'.'.' ••••• '.':.'.'.' ••••••••••••••.•.•.•.•.• : .... .' .•.•.•• ~ ................... : 
Figure 3.7 Example Multi-Process Petri-net 

The Decentralized Model 
91 
In the example, there are two processes, CODE and TEST, used by two separate 
groups that are responsible for coding and testing the application, respectively. 
In order to increase productivity and consistency, the two teams, previously 
not connected in any way by their processes, decide to collaborate. The main 
collaborative step involves a joint evaluation of the test results by represen-
tatives from both groups that will lead to better understanding of the errors. 
In addition, implications of this step should provide local feedback to both 
groups. Finally, the necessary data transfer among the groups (e.g., object 
code, reports, etc.), previously done outside the process, should be modeled 
and handled through the inter-process modeling and binding mechanisms, re-
spectively, thereby enabling automatic and consistent transfer of the artifacts 
between the collaborating groups. 
The dashed sub-process within the TEST process is then identified as the fu-
ture shared sub-process. The main modifications made to that sub-process 
before turning it to a Treaty sub-process are in the addition of an interface 
input place (depicted by a circle with an inner-circle, representing in SLANG 
an end-user interacting with an activity) from the CODE group for purposes 
of the evaluation of the test results, and two new transitions with cross-process 
implications: (1) if the test fails, the CODE group is notified to fix the prob-
lems indicated by the test; (2) if the test is recognized as faulty, or insufficient, 
the TEST group is notified and modifies its package according to the recom-
mendations made in the evaluation. Finally, the input place holding the object 
code is now transferred by the CODE group through the Summit mechanism, 
whereas before it was implicitly supplied to the TEST group. This, however, 
does not require a change in the sub-process, since when the Treaty is estab-
lished, the object-code output place in the CODE process is merged with the 
corresponding input place in TEST. 
Once the Treaty is established, all coding and test package preparations are still 
done independently and autonomously as before, but the processes synchronize 
for the actual testing phase when both groups are ready, as indicated by the 
presence of their respective tokens in the input places of the shared activities. 
When the shared activities (Summit) are complete, a "fan-out" (or post-Summit) 
occurs, involving passing the relevant evaluation results to each team, possibly 
affecting their (local) state. At a later point, when both teams are ready for a 
second test, a second Summit activity is initiated. 
3.5.3 
Grammar-Based PMLs 

92 
CHAPTER 3 
The grammar hierarchy [27] and the corresponding automata provide another 
powerful formalism for modeling a wide variety of systems, although they may 
have been less frequently applied to software process modeling than the other 
paradigms mentioned. There is a spectrum of approaches to employing gram-
mars in process enactment, analogous to sentence generation at one end (what 
Heimbigner calls a prescriptive process [49]) to sentence recognition (parsing) at 
the other (proscriptive). The PDL project employed the former for context-free 
grammars [65], while the implementation of the Activity Structures Language 
on top of Marvel follows the latter approach [73]. One group experimented 
with both in the context of attribute grammars, for HFSP [76] and Objec-
tiveAttributeGrammars [115], respectively. 
Considering the grammar-based PMLs, a terminal symbol corresponds to an 
activity in our context hierarchy, a non-terminal symbol to a task, and a pro-
duction to a process step. Grammar-based PMLs usually associate some kind of 
condition with each production, or possibly with each symbol in a production, 
to specify when it could be selected. For example, in the PDL-based system 
these are called restriction conditions, in the Activity Structures Language they 
are rule skeletons, and in HFSP they are decomposition conditions. Symbols 
are associated with formal and actual parameters in some fashion specific to 
the PML and PCE. 
The symbol (along with its possible condition) seems the best candidate for 
the unit of commonality. But it doesn't have to be a terminal symbol. This 
reflects the hierarchical decomposition property of grammar-based PMLs, since 
it essentially allows to define any sub-process as common. However, any sub-
tree that can be possibly generated at execution from that symbol must be 
identical in both processes (otherwise it will not be common). Thus, the import 
of a symbol is necessarily recursive, i.e., when a symbol is imported, all of its 
possible productions are imported recursively. Of course, a cyclic import must 
be detected as part of the import procedure. As with Petri-nets, the importing 
site must also explicitly augment its grammar with the new symbol, and use it 
in its production(s). 
An issue that comes up in all PMLs but is particularly eminent here is the issue 
of (sub)task naming. The newly imported symbol must not conflict with the 
name of any other local symbol, and at the same time it (and in fact all the 
derived symbols in a Treaty) must be identified as the common symbol when 
the Summit is enacted, eliminating simple local renaming as an option. The 
general approach recommended here, and the one actually taken in Oz (see 
Chapter 5) consists of separation of logical and physical names combined with 

The Decentralized Model 
93 
uniqu~ physical name generation. This approach enables both private (logical) 
naming of subtasks, as well as a global name space for running Summits. 
The Summit protocol works as follows (we skip the first and last phases): 
1. Pre-Summit -
This phase begins when an activity represented by a com-
mon symbol is invoked in one process with data from multiple processes. 
The remote SubEnvs are notified, and any prerequisites of enacting that 
symbol are checked in each of the participating SubEnvs, each according 
to their own local process. In principle, a recognition-oriented peE might 
now recursively enact any symbols immediately preceding the common 
symbol in the current production in an attempt to fulfill the prerequi-
sites, analogous to backward-chaining for rule-based peEs. This could be 
regarded as a form of sentence generation. 
2. Summit -
Assuming all SubEnvs ultimately agree, the symbol is enacted 
in the coordinating SubEnv. If, however, this is a non-terminal symbol 
representing composite subtask, it is "parsed" recursively, possibly involv-
ing multiple multi-site activities. This is in fact a "natural" instance of 
composite Summits mentioned in the generic model. This is also why non-
terminal Treaty symbols are imported recursively: a common sub-task 
must be literally common so that all involved sites know (and trust) what 
exactly is taking place when their data is accessed. 
3. Post-Summit -
All the participating SubEnvs are notified by the coordi-
nator to complete the symbol. For example, in the case of a generation-
oriented peE, each local process might automate control flow through its 
local production within which the symbol was embedded. Once again, the 
productions including a common symbol might be completely different in 
different local processes, and enacted independently and autonomously. 
3.5.4 
APPL/A 
We conclude this section with an attempt to apply the decentralized model to 
APPL/ A [123]. APPL/ A is an imperative PML that extends Ada [47] with 
several constructs that support modeling and execution of processes, such as 
persistent data types (relations), triggers for reactive control, and predicates 
for specification of local constraints. APPL/ A takes an unusual approach to 
process modeling in that it models processes as actual programs that execute 
directly on the computer, unlike the more common approach whereby process 

94 
CHAPTER 3 
models are interpreted by and executed on a process engine. In that sense, it 
is lower-level than the declarative PMLs mentioned above. 
Hence, it is hard to imagine how to support the Treaty model in APPL/ A. 
For example, it is not clear how a common sub-process could be defined and 
integrated with local sub-processes, nor is it clear how Treaties can be formed 
incrementally and over possibly pre-existing processes (short of recoding and 
recompiling the programs). 
However, Ada (and hence APPL/A) has some built-in constructs for concur-
rent and distributed programming that might be used to model Summits. In 
particular, the Ada Rendezvous [16] seems suitable at first glance. Figure 3.8 
shows a partial implementation of the motivating example in pseudo-Ada us-
ing Rendezvous. The coordinating process encodes the composite Summit as 
an Ada task (Change..summit) and defines two synchronization points (repre-
sented as task entries) used by the local processes to signal that they have 
completed their work. The task entries are also the shared interfaces used for 
exchanging data between the tasks. The coordinator task first waits for the lo-
cal tasks to complete their local reviews (the local...review...done entry), then 
if the reviews are approved, the coordinator performs local code modifications. 
When the body of the accept completes, the results are transferred back to 
the waiting local processes, which in turn proceed with the unit testing. When 
done, they invoke the unit_test...done entry sending the information that is 
needed by the coordinator; the coordinator then performs integration test, and 
sends the new binaries to the local processes. For simplicity, we ignore various 
exceptions to the process. 
While powerful for certain kinds of concurrent tasks, the Rendezvous mecha-
nism does not seem satisfactory for modeling Summits. The main problem is 
with the alternating "activeness" of the local and coordinating entities. That 
is, Summits inherently require both that the coordinator will wait (i.e., accept) 
for the local processes, as well as that the local processes will wait for the co-
ordinator. For example, the initiation of a Summit from the coordinating site 
(missing form the solution, as the reader might have noticed) requires that the 
local processes will be waiting for a Rendezvous. However, if the local task 
executing the body of such an accept would invoke a task entry back to the 
coordinator, a deadlock would occur, since the coordinator is blocking on the 
entry. Although the accept blocks could be made smaller, it seems in general 
that Rendezvous is geared towards a single acceptor and multiple senders, as 
opposed to multiple tasks, each of which alternates between being a sender and 
an acceptor. 

The Decentralized Model 
95 
Another problem is that Rendezvous is "too synchronizing" for our purposes, 
and limits the potential concurrency. For example, when the local process 
completes its review, there is no reason for it to block. Instead it should be 
able to perform other tasks and be notified asynchronously (again, by its own 
accept block) when the coordinator has completed its task. Finally, another 
problem is support for multi-process (not only binary) Rendezvous. Here again, 
a solution might be to "collect" all the information from the individual binary 
Rendezvous and then call the "global" procedure, but the coding involved in 
implementing this would be complicated. In fact, this is the way the Oz process 
engine implements Summits. But this implementation is totally hidden from 
the process engineers; they use the Summit abstraction to model multi-site 
interoperability. Thus it is possible to realize Summits with Rendezvous, but 
perhaps would be easier for the process engineer if some enhancements would 
be made to APPL/ A. 
3.5.5 
Summary 
An interesting outcome of investigating the application of the generic model on 
different PMLs is that each PML illuminates different aspects of the model, 
due to its distinguished characteristics. 
For example, the explicit process 
structure of Petri-nets shows the Treaty as an intersection-net, and gram-
mars bring the naming issue to the forefront and also emphasize composite 
Summits due to their decomposition nature. Moreover, actual implementa-
tions of the model to different PMLs are likely to raise new requirements and 
subsequently modifications to the model. Yet, overall it seems that having a 
language-independent model enables on one hand to investigate problems in-
herent to process-interoperability regardless of the PML used to define it, and 
on the other hand it makes it potentially applicable to a wide range of for-
malisms. The particular model presented in this chapter seems to be capable 
of adjusting itself to the different concerns, abstractions, and granularities of 
the different PMLs. 
3.6 
GROUPWARE TOOLS AND 
DELEGATION IN SUMMITS 
The model presented so far in this chapter lacks consideration of two impor-
tant generic issues. The first has to do with integrating groupware technology, 
mainly integration of synchronous multi-user tools (e.g., multi-user editors) 

96 
CHAPTER 3 
task Change-Summit is 
entry locaLreview_done(sources: in files; reVIews: in docs; results: 
out res); 
entry unit_test_done(sources: in files; in..results: in res; out..results: 
out res; bins: out binaries) 
end Change-Summit; 
task body Change-Summit is 
begin 
loop 
accept local..review_done(sources: in files; reviews: in docs; re-
sults: out res) do 
if (reviews are OK) then 
modify _code( sources, results); 
end if; 
end local..review _done; 
accept unit_test_done(sources: 
in files; in..results: 
in res; 
out.J'esults: out res; bins: out binaries) do 
if (in.J'esults are OK) then 
integration_test(sources, bins, out.J'esults); 
end if; 
end unit_tesLdone; 
end loop; 
end Change_Summit; 
task type locaLproc; 
task body locaLproc is 
doJocal..review( doc); 
local..review _done( sources, local..results, global..results); 
if (global..results are OK) then 
doJocaUest( sources, test.J'esult); 
unit_test_done(sources, test_results, globaLtesLresults, binaries); 
end if; 
if (global..results are OK) then 
install...new _binaries(global..results); 
end if; 
end locaLproc; 
Figure 3.8 Summit in Ada 

The Decentralized Model 
97 
that enable multiple human users, possibly physically dispersed, to collaborate 
(support for enactment of asynchronous multi-user tools in PCEs has been in-
vestigated in [127]). While in some aspects the introduction of this technology 
obviously implies additional architectural support from the PCE (for example, 
connecting multiple human users from multiple sites to the same conceptual ac-
tivity), the extension of this aspect to our model seems both natural and simple. 
Recall that the Summit phase is always executed in the coordinating site, while 
the other sites behave "as if' the activity executed at their site, in order to 
carry out any implications of that activity locally. Integrating multi-user tools 
simply implies that when such tools are invoked as part of a Summit, all sites 
(not just the coordinating site) actually execute the Summit activity, instead 
of just "pretending" to execute it, so the Summit phase still executes in one 
point in time, but not necessarily in one point in space. Consider, for example, 
Petri-nets. Whereas in the standard model the remote sites would normally 
fire the transition without actually carrying out the associated activity, in the 
case of multi-user tools the transitions would really fire the activities, except 
they might involve binding new users, one per site, and thus must be defined 
(either as "instances" or as "roles", analogous to the distinction between classes 
and instances in object-oriented programming) somewhere in the body of the 
activity or elsewhere in the Summit. 
A second major aspect, which is related to but separate from groupware, is 
the issue of delegation. As soon as multiple users might be involved in the 
execution of related activities, the notion of delegating a task to a specific user 
(or a role instantiated by a human user) comes to the surface. Delegation can 
occur in our model in two places: within the Summit phase in a multi-user tool, 
as described above, and in either pre- or post-Summit phases. For example, a 
pre-Summit phase might involve firing an activity at a remote site by a user 
at that site, as opposed to the user that initiated the Summit, or in case of a 
non-interactive tool it could be fired "unmanned". The latter is considerably 
more efficient if the SubEnvs are physically distributed, since the execution 
occurs where the data resides. And the former might be more appropriate in 
many cases that involve interactive tools that have to be performed by specific 
remote users (see Section 4.5). 
Support for delegation, while extremely important for effective execution of 
Summits, is largely a PML/PCE-specific issue, since it requires language ex-
tensions to enable binding of activities to human roles, or providing a "user-
context" from which potential users can be selected for interactive execution 
of certain activities. Once such support exists, it can be extended to support 
multi-site activities. The main necessary extension is to enable binding of re-
mote users to activities, analogous to the mechanisms which are necessary to 

98 
CHAPTER 3 
enable binding of remote data to Summit activities. A particular approach to 
supporting delegation in the Oz framework is given in Section 4.5.1. 
Finally, note that support for both delegation and multi-user tools is in some 
sense orthogonal to SubEnv-interoperability, in that the same functionality 
could be used by multiple users of the same SubEnv. It is only more evident in 
the case of multiple SubEnvs. Indeed, the implementation of delegation in Oz 
supports both intra- and inter-process delegation and activation of multi-user 
tools. 
3.7 
EXTENSIONS AND ALTERNATIVES 
TO THE DECENTRALIZED MODEL 
There are several directions in which the Treaty and Summit models could be 
altered and/or extended. Some of the changes actually divert from the basic 
models and are not compatible with them, while others could be augmented to 
the basic model with varying degrees of implementation efforts. We consider 
here only the conceptual changes to the model, and defer implementation issues 
to the next chapter. 
3.7.1 
Summit Branching Policy 
The first alternative to consider is related to the execution order of local vs. 
global activities in composite Summits. Recall that in the Summit algorithm 
a Summit activity is followed by fan-out to local sites, and only when the 
fan-out completes, another Summit activity is enacted, and so forth. An al-
ternative approach would have been to execute all related Summit activities 
consecutively, preceded and proceeded by local operations (This, in fact, was 
the initial composite-Summit model as presented in [20]). That is, when a 
summit activity completes, the coordinating SubEnv enacts (recursively) any 
further Summit activities emanating from the previous one, and fan-out begins 
only when the global execution completes. The main advantage of the former 
(our Summit approach) is in the fact that it subsumes the functionality of the 
latter. 
A related problem at the implementation level has to do with carrying out the 
"branching policy" discussed above. The coordinating SubEnv has to be able 
to distinguish between Summit and local activities, and apply the right order of 

The Decentralized Model 
99 
execution as described above. This problem occurs particularly in cases where 
the choice of activities to run is not explicit in the PML code but rather inferred, 
such as in rule-based and grammar-based PMLs, where the branching occurs 
dynamically and is not known a priori. This issue is addressed in Section 4.4.3. 
3.7.2 
Consistent Treaty Withdrawal 
In some (but certainly not all) cases, it might be desirable to protect the in-
tegrity of a Treaty and disallow unilateral withdrawals, in cases where a Treaty 
represents some global consistency that must be enforced. Although this might 
be regarded as violation of the autonomy principle, it can be justified by the 
fact that the establishment of a Treaty in the first place expresses willingness of 
its members to relinquish some of their autonomy for collaboration purposes, 
and therefore they should be committed to conform with the Treaty. In this 
case, Treaty invalidation would be performed in a similar manner to Treaty 
definition, i.e., as a multi-site protocol involving all participating sites. In any 
case, such a "consistent" Treaty invalidation mechanism should be clearly an 
optional alternative to the ordinary Treaty mechanism that does allow unilat-
eral withdrawal, not a replacement for it. 
3.7.3 
Local Derivation of Summits 
The basic Summit algorithm restricts Summit activities to occur only at the 
coordinating site. Of course, different Summits can occur at different sites, 
but once a Summit begins at the coordinating site, all subsequent Summit 
activities in the same composite Summit must occur at the coordinating site. 
A reasonable extension to the model would be to allow derivation of Summit 
activities off local executions. Consider, for example, the case where an initial 
Summit between sites E1, E2, and E3 leads to another Summit between, say, 
E3 and E4, where E4 is a sub-contractor of E3 but is not concerned with the 
previous Summit. While this seems like a natural extension, there are several 
difficulties with it, both conceptual and technical. 
Conceptually, the main problem is that there is no way to define clear bound-
aries for a Summit and identify each activity with its associated Summit. Thus, 
the notions of a Summit context and a coordinating site become blurry and ill-
defined. 

100 
CHAPTER 3 
Technically, there are numerous problems to be resolved. The main one is 
concerned with concurrency: multiple "sub-Summits" can lead to circular de-
pendency which might lead to a deadlock, as will be explained in Section 5.4. 
Another problem is if atomicity semantics were desired for some kinds of com-
posite Summits (as is the case in Oz, see Section 4.4.5), maintaining atomicity 
across multiple coordinating SubEnvs requires complicated transaction facili-
ties. Finally, the issue of dynamic binding comes up. The problem is how can a 
site executing in "local mode" derive a Summit with other sites. One way is to 
"inherit" the context of the parent coordinating site and use it, but this is still 
limited to forming Summits off sites that participated in previous Summits. 
One restriction to the local derivation mechanism could make it more feasible: 
limit local derivation of Summits to involve new sites which did not participate 
in prior Summits, thereby forming acyclic hierarchical Summit structure that 
alleviates the concurrency problem, although it worsens the binding problem 
(a possible solution to the latter in Oz is discussed in Section 4.2). 
To summarize, while local derivation of Summits might be an attractive ex-
tension, there are several implications to implementing it, and they have to be 
weighed against the potential gained benefits before attempting any implemen-
tation. 
3.7.4 
Multiple Global Environments 
Another extension to consider is interaction between multiple (global) environ-
ments. One particularly attractive option would be to have a SubEnv that 
belongs to several environments, and executes in the context of one of the 
environments. Or it could even allow simultaneous execution with multiple en-
vironments (by the same or different users). The former approach is compatible 
with the general model since SubEnvs are self-contained and loosely coupled 
with each other. It might even have different Treaties with different SubEnvs 
in the different environments. The main architectural requirements are (1) to 
support global name space for global environments and globally unique SubEnv 
identification scheme, and (2) to have a mechanism that enables to dynamically 
bind (and unbind) a SubEnv to a particular global environment. The more am-
bitious approach to allow simultaneous interaction with multiple environments 
imposes several architectural problems, such as addressing and selecting ob-
jects in different SubEnvs, but is also in principle compatible with the general 
model, so long as the SubEnv has one instantiated process. 

4 
REALIZATION OF THE 
DECENTRALIZED MODEL IN Oz 
In this chapter we explore issues that are concerned with the realization of 
the decentralized model in a real PCE. Although most of the concepts intro-
duced here have been fully implemented 1, the dis~ussion in this chapter will 
try not to focus on the actual implementation. Thus, while detailed enough 
for understanding the wide range of issues concerned with such realization, the 
discussion will attempt to present the problems, ideas, and techniques that 
could be relevant to an implementation of the model in other PMLs/PCEs in 
general, and rule-uased PMLs/PCEs in particular. 
The chapter begins with an operational overview of Oz, followed by a spe-
cific discussion of its decentralized multi-site object base. Sections 4.3 and 4.4 
cover in detail the Treaty and Summit realizations, respectively. Section 4.5 
overviews mechanisms for groupware and delegation support in Oz, and Sec-
tion 4.6 summarizes the implementation status of Oz. 
Before launching into the realization of the decentralized multi-process model, 
we begin with an overview of the concepts that were "inherited" from the 
single-process Marvel PCE, so the reader should be familiar by now with the 
main concepts and functionalities of Marvel as presented earlier in Section 2.2. 
Oz is a multi-process PCE as defined in Section 3.1, supporting modeling and 
enactment of autonomous multiple sub-environments. As such, Oz subsumes 
the definition of a single-process PCE. In particular, an Oz environment con-
sisting of a single SubEnv behaves similarly to a Marvel environment (albeit 
an enhanced one, because of improvements that were made in Oz independent 
of support for multiple processes). 
1 The unimplemented features are summarized in Section 4.6. 

102 
CHAPTER 4 
As in Marvel, each local (sub )environment in Oz is tailored by a local adminis-
trator who provides the data model, process model, tool envelopes, and coordina-
tion model for its team. These definitions are translated into an internal format 
and then loaded into the environment by a special loader component. The data 
modeling capabilities of Oz are identical to those of Marvel -
object-oriented 
data definition including classification, multiple inheritance, composition hier-
archy, horizontal bi-directional links, and file and state attributes. Similarly, 
the process modeling language of Oz is based on the Marvel Strategy Lan-
guage (MSL), with some minor extensions (which will be pointed out as their 
functionalities are discussed). Most importantly, Oz extends the user-driven, 
rule-based paradigm to multi-process environments. Specifically, as far as lo-
cal processes are concerned, Oz processes are defined in terms of Marvel-like 
rules, which correspond to the notion of process-steps in the generic context 
hierarchy. Process tasks are implicitly defined by matchings between effects 
and conditions of rules. The constraints defined in the process (in terms of 
rule conditions) are enforced, and automatic enactment is supported through 
backward and forward chaining. Finally, the single-site transactional semantics 
of Oz are like those of Marvel, supporting atomicity and automation chaining 
(more details are given in Section 4.4.5). 
4.1 
OPERATIONAL OVERVIEW OF Oz 
In general, Oz has a two-level architecture: within a SubEnv, it has a client-
server architecture, with multiple clients communicating with a single central-
ized process-server. Across SubEnvs, Oz has a "share-nothing" architecture, 
as advocated in the formal model. This means that the processes, schemas, and 
instantiated objectbases are kept separately and disjointly in each SubEnv, and 
that there is no global repository or "shared memory" of any sort (details of 
the architecture will be given in Chapter 5). 
Human interaction with the environment is provided through a client that is 
connected primarily to its local server. Using the client's connection to its local 
server, users can operate with the local tools, on local data objects, and under 
the local process, much like in Marvel. In addition to the local server, however, 
Oz users can connect to remote servers. Each remote SubEnv is represented in 
each local object base by a "stub" object that is visible to the client. By issuing 
the built-in open-remote (close-remote) command with the appropriate stub 
object as parameter, a client can open (close) a connection to a remote SubEnv 
(again, implementation details are deferred until the next chapter). A remote 

Realization in Oz 
103 
connection provides limited access to the remote SubEnv. A remote client can 
browse through remote object bases and get information about remote objects 
(subject to access control permissions). However, a client has no access to 
remote processes (i.e., rules, tools) and access to remote data can be made only 
by binding remote objects as parameters to Treaty rules (their realization is 
the subject of Section 4.3.1). 
For example, figure 4.1a shows how the client for user israel (the user's name 
is shown in the upper left corner of the interface window) is connected to 
the local server of SubEnv IY, with a (default) view of the local objectbase2 
(parent-child relationships are depicted with straight lines and links by curved 
lines). Figure 4.1b shows israel's view after an open-remote on site CT has 
been made, making CT's remote objectbase available for browsing by israel. 
israel's client has not connected to SubEnvs HA and IJ, and they may, or may 
not, be currently active (i.e., executing). israel interacts with the environment 
by selecting commands from the rules menu, which contains all the process-
specific user-level commands (inference rules used internally do not appear in 
the menu), and he supplies arguments to the rules by clicking on objects from 
the objectbase. In particular, if a remote objectbase is open, he can initiate a 
Summit by selecting remote objects as arguments to Treaty rules. When the 
(local) server services the request to fire a rule, it checks its own process, and 
communicates with remote SubEnvs if the rule accesses remote data from their 
objectbases, and eventually determines whether an activity has to be executed. 
That activity could be either the one explicitly requested by the user, or another 
activity related to the requested one through a chained rule. The server then 
sends a message to the requesting client to execute the activity in its activity-
manager component (except in cases of delegation, where an activity can be 
sent to a delegated client, see Section 4.5). During a Summit activity, remote 
objects are temporarily copied to the local SubEnv and passed to the client 
prior to the activity execution. Note that since a client has no explicit access 
to remote processes, it cannot invoke "remote Summits" , thus all Summits are 
initiated by local clients. 
A special administrator client has, in addition to the normal client functionality, 
an interface for updating the actual process definition. An administrator client 
can evolve the process by adding, removing, or loading a whole new or revised 
set of rules into the current SubEnv. The interface to the load command is 
shown in figure 4.2, where the analyze strategy is about to be added to the local 
process. The administrator can also optionally specify a process configuration 
2 For simplicity, only a small object base is shown, but in reality Oz can maintain thousands 
of objects with adequate browsing support. 

104 
CHAPTER 4 
~ Oz 1.8: israBl @ bleecker .cs.columbla.edu 
Local Server: !.:.NY'--___ Current Object: !.:.NY'--______________ ---l 
~ 
~ 
(Construct Q) ~ 
(Query 17) ~ 
~ 
Left Print Information for Object 
Right: Change Current Object 
Figure 4.1a A Sample Oz Environment 
y 

Realization zn Oz 
~ Oz 1.0: Israel @bJeed<er.cs.cokmbla.edU 
Local SeNsr: ::..NY'--___ Current Object: "C.:..T _______________ 
_ 
(seSSion 9) ~~ 
~ 
(Query v) ~ 
~ 
Left: Print Information for Object 
Right: Change (utTent Object 
Figure 4.1b An Oz Environment with One Open Remote Site 
105 

106 
~ :: MSL load :: 
~ 
C§D ~ 
(Remove) (Files t» 
File: analyzEl., 
Figure 4.2 
Load Interface in Oz 
CHAPTER 4 
Active Ii! 
file that contains a list of strategies to load (several changes which were made 
to the strategy construct in Oz (with respect to Marvel), like the optional 
configuration file, are explained later in Section 4.3.1). 
4.2 
Oz OBJECTBASE 
Whereas an instantiated objectbase in Marvel is a forest data structure, a local 
object base in Oz is a rooted tree with a special SubEnv root object that contains 
information pertaining to the local SubEnv (the details are given in Chapter 5). 
Thus, a typical forest-like Marvel object base is mapped to an Oz object base 
by connecting all the forest's roots as children of the SubEnv object3 . 
Since each local objectbase is maintained by a different Oz server, a "global" 
object base is merely the union of all local disjoint object bases, although the 
schema may vary from site to site. This implies that composite objects cannot 
be partitioned across different objectbases, since this would violate the dis-
jointness property, because a composite object contains its sub-objects. For 
reference links, the situation is different. Links could conceptually cross an 
objectbase boundary since they do not impose a containment relationship. 
4.2.1 
Cross-Site Links 
There are tradeoffs in supporting cross-site links. The main advantage with 
having them is that they provide cross-site data modeling capabilities. Another 
advantage is that they enable remote objects to be bound as parameters to 
3 This provides an easy migration path from Marvel to Oz objectbases. Marvelobjectbases 
can be migrated to Oz local objectbases with a simple upgrade facility that essentially extends 
the original schema with the addition of the Oz built-in class that defines SubEnv objects, 
and connects all roots of the forest to the SubEnv object. 

Realization in Oz 
107 
multi-process rules in an on-going chain. This seemingly obscure property 
of cross-site links stems from the fact that automatic (as opposed to user-
invoked) derivation of the parameters of rules during chaining is based on their 
structural relationships to parameters in previously executed rules (for details 
of this "inversion" alIWrithm, see [59]). Thus, since cross-site links are the only 
way to structurally relate the otherwise disjoint objectbases, the lack of such a 
construct would eliminate the possibility of automatically deriving parameters 
from remote SubEnvs, unless those SubEnvs have already participated in earlier 
Summits in the on-going chain. 
If however, cross-site links are allowed, there are several conceptual and tech-
nical problems: 
1. The main conceptual problem is that a cross-site link permanently connects 
two local objectbases. With the navigational querying capabilities of MSL, 
cross-site links would allow a query to traverse an entire remote object-
bases through a remote link. This might violate both the autonomy and 
independent operation requirements. While autonomy might be relaxed 
in some cases in favor of close cooperation purposes, independent opera-
tion should not be compromised. For example, if a site with remote links 
from/to other sites is "down" or just unreachable, then any rule that uses 
links to the disconnected object base will either fail or produce different 
results depending on which sites are reachable. 
2. With cross-site links, it is no longer clear to users (as well as high-level 
modules in the system) what the origins of the involved data are, and how 
expensive it is to fetch, bind and execute an activity. This violates the 
non-transparency property. 
3. Since links in Oz are typed, cross-site links require implicit specification 
of common sub-schema. Thus, a remote object which is linked to a local 
object must correspond to the local schema type. 
4. Cross-site links create cross-site dependencies that might lead to commu-
nication deadlocks if not handled carefully. The gist of the problem is that 
when a remote query is requested from site A to site B (e.g., in the binding 
phase of a rule), site B might not be able to service the query without 
consulting other servers that are connected through cross-site links, pos-
sibly creating a circular dependency. The general issue of communication 
deadlocks is covered separately in Section 5.4. 
5. Implementation of cross-site links is both hard and expensive because these 
links would be "virtual", i.e., two objects linked by such a link do not 

108 
CHAPTER 4 
share an (operating system) process address space or file-system space. 
They would have to be implemented by "stubs" at both ends containing 
information that allows queries to "follow" the link, and a corresponding 
protocol between the servers that enables efficient access to those remote 
objects. 
To summarize, cross-site links could be viewed as an implicit, "data-oriented" 
approach to enabling access to remote data. Instead, the approach taken in 
this research favors explicit specification of the remote data to be accessed 
(through rule parameters), and in a more "process-oriented" fashion. Thus, 
regular cross-site links cannot be supported in Oz. 
However, preliminary experience with using Oz revealed that for some situa-
tions, not having any means to model inter-site data modeling led to an un-
intuitive and awkward modeling of inter-process modeling, particularly in the 
cases where the sites were more tightly coupled. This led to the contemplation 
of "soft" links4. 
Soft Links 
The main ideas in soft links are: (1) to distinguish cross site links from regular 
links, not only in the data definition language (different attribute types), but 
also in the process modeling language; and (2) to treat the invocation of rules 
with soft links as Summit rules, which implies that they must be "Treatified" 
before their use. 
This design addresses most of the problems which were raised above: (1) queries 
would not cross sites unless they specifically contain soft links in their definition, 
and site autonomy would be preserved by the fact that only Treaty rules can use 
soft links, and they are regarded as Summit rules. (2) Non-transparency would 
still be preserved, since the process must explicitly state when it accesses remote 
data, and normal links will only refer to local data. And any satisfactory solu-
tion to the sub-schema (3) and deadlock (4) problems for regular Treaties and 
Summits would also cover these soft link Treaties (these solutions are presented 
in Sections 4.3.3 and 5.4, respectively). Problem (5) is still valid, though, and 
is the main reason for not having at the time of this writing an implemented 
version of soft links. 
4 Borrowed from the Unix terminology for symbolic links. 

Reatization in Oz 
109 
Remote Derivation Without Soft Links 
Even if cross-site links do exist, there is still a need for a complementary mecha-
nism that addresses the automatic remote derivation problem mentioned earlier 
without using them (e.g., because of data privacy concerns). Recall the "sub-
contractor" example given in Section 3.7.3; with no cross-site links, there is no 
way that the sub-contractor site could be (automatically) derived as a candi-
date for a Summit with the sub-contracting site, because it was not part of the 
previous Summit. The proposed solution extends the parameter inversion al-
gorithm to allow logical inversions off associative (i.e., non-structural) queries. 
This capability, in conjunction with global associative queries, would enable 
one to model the example in the following manner: in the sub-contractor site 
E4 , a special attribute of the relevant object is set to denote that it is a sub-
contractor of E3. Then, upon completion ofthe first Summit, E3 issues a global 
query that locates the sub-contracting object in E4 and binds it to E3'S rule, 
thereby providing the necessary context to subsequently fire a Summit with E4 
as desired. Once again, this approach might be preferred over using cross-site 
links in cases where a "permanent" connection between the sites is not desired. 
Furthermore, the definition is more "process-oriented" as it is defined explicitly 
in the process and not "hidden" in the data modeling level. 
4.3 
DEFINING PROCESS 
INTEROPERABILITY IN Oz 
As discussed in Section 3.2.1, modeling interoperability involves the following 
aspects: 
1. Definition and evolution of common sub-processes. 
2. Definition and evolution of common sub-schemas. 
3. Definition of data instances to be used by common sub-processes. 
Given the core requirements of autonomy, independent operation, and flexi-
bility, effective modeling of interoperability imposes guidelines and constraints 
on how to implement modeling facilities for the above aspects, among them 
independent and dynamic evolutions of local processes, as well as non-global 
definition. 

110 
CHAPTER 4 
In the rest of this section we examine each aspect in detail. The first aspect, 
discussed at length in Sections 4.3.1 and 4.3.2, is an effective realization of the 
Treaty protocol. Definition of common sub-schema is addressed in Section 4.3.3, 
and definition of common data instances is given in Section 4.3.4 in a somewhat 
abbreviated form. 
4.3.1 
Defining Common Sub-Processes: the 
Treaty 
In general, Treaties are realized in Oz following the formal model outlined in 
Chapter 3. Treaties are formed by active participation of both sites of the 
Treaty, with one site being the requester and the other site the acceptor, as 
well as one importer and one exporter (all four combinations are possible). 
The basic unit of commonality in Oz is the rule. However, mainly due to 
granularity issues, the unit that is exported and imported is the strategy. 
A strategy is a bundling construct for rules, somewhat analogous to a module 
consisting of functions in modular programming languages. Although the intent 
of a strategy is to bundle rules that are conceptually related -
either by their 
functionality or by belonging to the same task- there is no enforcement of this 
policy. In particular, there are no chaining restrictions between rules in the 
same or in different strategies5 . 
A strategy consists of two sections: a tools section that declares tools and 
defines the interface to them (from rules), and a rules section in which rules 
are defined. In addition, the schema required by the rules is provided by special 
data definition strategies defined separately. The separation between schema 
and process definitions (which did not exist in Marvel) has several benefits: (1) 
it facilitates the realization of Treaties that are formed among rule strategies 
only; (2) it eases local evolution, avoiding the need to perform schema evolution 
if only rule strategies are modified; and (3) it increases componentization by 
separating data and process modeling, thereby allowing the potential to use 
different data definition languages (and different OMSs) with the same rule 
language, and vice versa. 
Oz provides the following five built-in commands for establishing Treaties: 
export, import, unexport, unimport, and treaty. Although there are no sep-
5 An early Marvel paper [72] described a different approach with respect to restrictions 
and chaining across strategies. 

Realization in Oz 
111 
III :: Impol'llunl!llport :: 
~ 
(S (Import) t Llrdm~,.~t) (Filas Do) 
Actl •• D 
Tvpe: I R •••• ,t I Accapt 15~Of.d I 
Remota Site: NV 
FlI.: test 
III :: lIIIPortlunllllPort:: 
~ 
(~) (EoPort) (Una.port) aE 
"""1 •• 0 
Tvp.: I RoqulSt I Actopt I 
S~","d I 
FIle: doc. 
Sit.: III 
Figure 4.3 Import and Export Interfaces in Oz 
arate commands for request, accept, deny and cancel, they are specified as 
parameters to each of the above commands, making it possible to generate all 
possible combinations that were discussed in the formal model. 
export 
The export operation is defined as: 
export(strategy(SrcSubEnv), DstSubEnv, [privileges]) 
This is an inexpensive operation in Oz. It executes locally at SrcSubEnv and 
merely involves adding an entry with the specified strategy and DstSubEnv 
to a persistent local export table. By default, Oz associates request privileges 
with export, Le., it assumes that in most cases the exporter wants to use the 
exported strategy on data from DstSubEnv. But the administrator can change 
the default by explicitly selecting accept privileges. In addition to accept and 
request, Oz provides a third option called shared. The semantics of the 
shared option are to export a strategy both as a requester and as an acceptor. 
The main use of this option is to facilitate convenient generation of full (i.e., 
hi-directional) Treaties: a shared export followed hy the proper shared import 
establishes a full Treaty. The actual Oz interface is shown in the lower window 
in figure 4.3, where strategy doc is about to he exported to SuhEnv IlJ as a 
requester. 

112 
CHAPTER 4 
import 
The import operation is defined as: 
import(strategy(SrcSubEnv), DstSubEnv, [privileges)) 
import is the main operation in modeling Treaties, and is quite complicated 
both in terms of user interface support and the internal implementation. As 
usual, we assume the existence of the necessary underlying infrastructure to 
communicate with the remote SubEnv (which is explained in detail in Chap-
ter 5). In particular, there must be an open connection to SrcSubEnv, since 
the operation is initiated at DstSubEnv but it involves both SubEnvs. 
The actual interface is shown in the upper window in figure 4.3. It shows that 
strategy test is to be imported from SubEnv NY as an acceptor, i.e., it will 
allow rules defined in the test strategy to be fired from users at remote site 
NY on local data (which happens to be site CT). As with export, it is possible 
to associate either of the three privileges with import, with accept being the 
default. 
1. The first issue to consider in realizing import is how to select the strategy to 
import, given that it resides in a remote SubEnv and is thus not usually visible 
to the local process. The import interface must supply the administrator at 
DstSubEnv a list of the available strategies at SrcSubEnv that were explicitly 
exported from it to DstSubEnv. Further, this information must be generated 
dynamically, since the list of exported strategies at SrcSubEnv can change at 
any time as a result of issuing local export or unexport operations. 
2. Once the importer at DstSubEnv selects the strategy to import, it has to be 
copied from SrcSubEnv, along with additional information needed for runtime 
validation (see Section 4.3.2). This is done in Oz by fetching a copy of the 
strategy, which is held for the duration of import and is removed immediately 
afterwards. (A physical copy of the strategy is required only when the SubEnvs 
do not share a file system. Otherwise, only a path to the strategy is sent, and 
the process translator at the importing site fetches the file from its original 
location.) There are two reasons for not keeping a local copy of the imported 
strategy: (1) only one physical copy of the strategy "source code" is maintained 
(and all importing SubEnvs point to that copy), thereby avoiding the need 
to keep multiple copies consistent; (2) this approach also facilitates dynamic 
validation of Treaties that might be violated by local evolutions, as will be 
explained shortly. 

Realization in Oz 
113 
Note that import fetches only the rules, without the envelopes (and tools called 
from them) associated with the rules. While this is not a problem with the 
default import-accept option (since in this case the activity is not executed at 
the importing SubEnv, only its data is accessed by the activity, which executes 
at another SubEnv), the import-request combination implicitly assumes that 
the activity exists at the importing SubEnv, so if this is not the case it must 
be copied outside the environment (e.g., cp, or ftp across domains). 
3. The third step is to verify that the strategy can be integrated with the local 
process at DstSubEnv, both syntactically and semantically. This includes 
sub-schema compatibility (discussed in Section 4.3.3) and process-consistency 
(discussed in Section 4.3.5). 
4. The fourth step involves connecting the rules in the imported strategy to 
the local rule-network. This is done by forward connecting each new rule to 
all other rules (both imported and local) whose conditions match the rule's 
effect, and backward connecting it to all rules whose effects matches the rule's 
condition6 . At the end of this procedure, the imported strategies are fully 
integrated with the local process. When executed as part of a Summit, local 
prerequisites and consequences (in addition to "global" Summit implications) 
of the imported rules would be automatically enacted. 
Figure 4.4 illustrates the integration phase (using rules that correspond to 
some of the steps in the motivating example). Suppose the modify rule is im-
ported by two different processes residing at SiteA and SiteB, respectively. 
In siteA, modify is backward connected to rule review through the match-
ing between modify's condition and review's effects, and it is forward con-
nected to rule manuaLtest through the matching between modify's effects 
and manuaLtest's condition. Similarly, in siteB, the rule modify is backward 
connected to analyze and forward connected to auto_test. Thus, modify be-
comes an integral part of both processes, and may trigger, or be triggered by, 
invocation of related rules during enactment. 
The ease with which process integration can be achieved reveals the strength of 
the declarative nature of the rule paradigm: process fragments can be incremen-
tally added (or incrementally removed) and automatically integrated without 
user intervention. The context-less rules, as well as the fine granularity of 
rules as process building blocks, also payoff handsomely. 
6 Actually it is more complicated because chaining directives filter some of the connections. 

114 
Icondition 
. 
( 1'f .Itatul "" NoU .... i ... 4 ) 
.. aathity 
( UVIIW r .... i .. pf .request ?f .revi •• ) 
, ;~~:!~u •. a.Yiewd ),oE--
SlteA 
SlteB 
CHAPTER 4 
analyseUflPILBJ I 
tfgon4itiOil 
. 
I Pf .It.tUI "" NotReviewed ) 
., activity 
( RZVIIN revi •• U.r.quest ?f.r .... t .. ) 
., atbata 
I Pf .Itatul ... vi .... ne4 lJ 
~---.j.. ( ?f .ltatUI .. bvi ... 4 ) I 
.. biD4ing 
lt04ify [?.PILE, ?bIPILB, ?c.PILB) I 
., conclition 
. 
(and 
(Pe.statul '" -.vi.wed. 
( ?b.atatuB = Reviewed ) 
( ?e.statul = bVi ... 4 J I 
.. activity 
{ MODIFY mod ?a.oontCltl i'b.content. ?a.contents 
" effect 
r-_-+_I_on_. __ ~ ;::::~~: : :::i~i:: ~ 
( ?C.ltatUI .. Modified) I, 
(fora1l IIlDULB ,. luchtha 
(malabar [?m.fU •• If)) 
( ?f .atatul = bVi •• raih4 ) I 
ttcondi tiOD. 
ttcondi tion 
( ?f .stat.uI '" JIOdifie4 )oE-
~i ?f .ltat1u = Ko4IU.d ) 
., activity 
.. activity 
( ns'l' man_t •• t ?III.uec I 
( HSIf auto_t •• t 'lD.exec ) 
.. effects 
., .Uacta 
( Pf •• tatul = unitT.,ted ) I 
( ?f .atatul "" tlniU •• tad ) I 
( i'f • statuI "" '!'eatraUed. ) I 
( ?f. status = 'l'.strail.d ) J 
Figure 4.4 Integration of hnported Rules 

Realization in Oz 
115 
Since accept/request operations can only be invoked by import/export, it is 
necessary for import to be idempotent with respect to the compilation men-
tioned above. This is particularly important for Treaties that involve more 
than two sites. For example, suppose site El imports strategy 52 from site E2 
and site E3 also imported 52 from E2. Now site El wants to grant accept priv-
ileges to E3, so it issues an import-accept command, but this time compilation 
of the process model is not necessary so only the execution-privileges flag is 
modified. When an import is requested on an already imported strategy (or 
alternatively, if it is a local strategy which was exported and is now imported, 
possibly to form a full Treaty), only the process privileges are updated, and 
the compilation part is ignored. We will refer to such import operation as a 
"faked" import. 
5. The last step of import involves sending an acknowledgement to 5rc5ubEnv. 
This acknowledgement is not critical, however, since runtime checks are per-
formed anyways to verify the validity of Treaties. Its sole purpose is to notify 
users at 5rc5ubEnv of those Treaties that are possibly available to them. 
Figure 4.5 illustrates the import algorithm and the interaction between the 
SubEnvs at the execution of the import, with numbers corresponding to the 
steps listed above. Notice that menu generation in the first step is done by 
direct communication between the remote client and the export server. Once 
the Oz client selects which strategy to import, it requests its local server to 
perform the import. From then on, the two servers communicate in a client-
server fashion -
with the import server acting as a "client" and the export 
server acting as a "server" -
until the service is completed, in which case both 
servers update the client: the import server sends to the client the revised set 
of rules and rule-network, and the export server notifies the client that a Treaty 
from the import server to the export server was formed. 
It is worth mentioning here a problem that manifests itself in this implementa-
tion of import, but is only a private case of the more general problem regarding 
server-to-server communication. Basically, the problem with this client-server-
like interaction between the import server and the export server is that as a 
"client", the import server needs to wait for the service to be provided. This 
means that if the export server is single threaded and non-context switchable at 
step (2), then the import server might block indefinitely, potentially "starving" 
its own clients, or even worse, deadlocking with other waiting servers acting 
as clients. In the specific case of import, since it involves changing the local 
process, the import server must execute in single-user mode. This alleviates 
the import problem since there are no potential starving clients, but it doesn't 
solve the deadlock problem. And in the general case multiple clients interact 

116 
CHAPTER 4 
import server 
import client 
export server 
(1) get strategy Ust _ 
r--. - send strategy Ust 
-
..,.. 
select strategy 
..----~ 
(2) request Import -
~ 
send strategy 
.-
(3, 4) INTEGRATE 
(5) send aeknowlegdement 
npdate client __ 
update (remote) eHent 
r---
-----
I-----
update cUsplay 
and make 
new strategy available 
Figure 4.5 The Import Algorithm in Oz 

Realization in Oz 
117 
concurrently with a server, adding the starvation problem as well. This problem 
is addressed separately in Section 5.4. 
There are two more properties that the import operation must have. One is 
atomicity; clearly, the import operation has several potential failure points, 
meaning that it must be accompanied by a context-sensitive rollback mech-
anism that preserves the integrity of the server in case of failures. However, 
there is no need to guarantee cross-site atomicity for import, which fits well 
with the general decentralized requirements; the acknowledgement is optional, 
as mentioned above. The atomicity of the operation has to be preserved only 
in the importing server. 
The second property is persistence. The imported strategy, along with the 
necessary information used for runtime validation, must be stored permanently 
with the local process since it outlives an execution of the server, and needs to 
be reloaded in subsequent evolutions. 
unexport 
The unexport operation is defined as: 
unezport(strategy(SrcSubEnv), DstSubEnv, [privileges]) 
Like ezport, this is a local operation that executes at SrcSubEnv. It re-
moves DstSubEnv from the list of SubEnvs that are entitled to further im-
port strategy. In addition, the execution privileges are undone based on the 
specified privileges argument -
when coupled with accept the effect is deny, 
coupled with request results in cancel, and coupled with shared revokes both. 
Note that if, for example, the exported strategy was previously shared (i.e., 
both requested and accepted), then unexporting with request (accept) retains 
the accept (request) privileges intact. 
unimport 
The unimport operation is defined as: 
unimport(strategy(SrcSubEnv), DstSubEnv, [privileges]) 

118 
CHAPTER 4 
Unlike its import counterpart, unimport is a local operation, as it should be 
according to the formal model. However, unlike unexport, it might involve 
some non-trivial amount of work at the server. The algorithm is as follows: if 
strategy is marked as imported from more than one SubEnvs, or if strategy 
is a local strategy (which was faked imported for full Treaty purposes), then 
unimport does not modify the process, and only updates the privileges similar 
to the way it is done in unexport. If, however, DstSubEnv is the only strategy 
from which strategy is marked as imported, then unimport removes strategy 
from SrcSubEnv's process. This requires "decremental" recompilation and re-
generation of the rule network. Such a physical unimport also revokes all priv-
ileges from all remote SubEnvs regardless of the parameters that were specified 
with the operation, since the strategy is removed from SrcSubEnv and cannot 
be used in any manner there. 
As can be seen, not having the four execution-privileges commands (request, 
accept, cancel, and deny) available separately from the four strategy-transfer 
commands (export, unexport, import, unimport) introduces some technical 
and conceptual difficulties. On the other hand, preliminary experiments showed 
that easing the procedure of forming Treaties is pragmatically important, and 
that most of the Treaties can be formed using the default privileges, while more 
proficient administrators can still select other options in order to get the desired 
behavior. In any case, this is mainly a user-interface issue; the important issue 
is that the equivalent semantics of the formal model are fully obtainable in Oz. 
Operational Overview of Forming Treaties 
Going back to the formal model, a simple binary Treaty between two SubEnvs 
is formed by an export operation at the source SubEnv, followed by a matching 
import operation at the target SubEnv. But these operations do not have to 
be synchronized, and in particular, the import can occur at anytime after the 
export, or never occur at all. From the system's standpoint, Treaties are formed 
implicitly, and perhaps even without explicit intention. That is, Treaties can 
be inferred automatically, when the right combination of export and import 
occurs at the SubEnvs. In some sense, this is a continuation of the context-less 
rule-based model that fits well with autonomy concerns. In particular, there is 
no need for a "global administrator" to form Treaties; they are formed by local 
administrators willing to collaborate in order to form the Treaties, and using 
the system to formalize their intentions as well as ensure that they are carried 
out as agreed. 

Realization in Oz 
119 
iii :: treatyAlntraaty :: 
(Done)Q§V(~~ 
Active !if 
Type: I Request I Accept I Shared I 
Remote Site: .c:.NY'--__________ ---i 
File: :..:rc •• _____________ 
---\ 
Figure 4.6 The Treaty Interface in Oz 
In cases where SubEnvs are more tightly coupled, however, there might be a 
need to support (simple and full) Treaties as one operation, to simplify their 
formation. Indeed, early experience with Oz revealed the need for such an 
operation in cases where, for example, each SubEnv represented a single-user 
process, in which case a global administrator (and a corresponding global Treaty 
operation) was essential. Therefore, Oz supports the explicit Treaty operation, 
which bundles an export and import, as explained below. 
Treaty as one operation 
In order to be eligible for executing the Treaty operation, a user has to have 
administrator privileges on both SubEnvs. Note, however, that in conformance 
with the "not-only-Iocal-or-global" principle, the user does not need universal 
administrator privileges, only on the two sites of a given Treaty. 
The treaty operation is defined as: 
Treaty(strategy(SrcSubEnv), DstSubEnv, (privileges]) 
and the actual Oz interface is shown in figure 4.6. 
The semantics of the operation are as follows: strategy is exported from 
SrcSubEnv and subsequently imported by DstSubEnv. Treaty is atomic, 
meaning that both SubEnvs have to rollback in case of a failure. In addi-
tion, DstSubEnv has to operate in single-user mode (i.e., only one client can 
be connected to it, although SrcSubEnv and other SubEnvs might have arbi-
trary number of active clients). To simplify matters, Treaty is always initiated 
by the exporter. However, the exporter can be either a requester (default) 
or an acceptor, implying acceptor or requester privileges on DstSubEnv, re-
spectively. Finally, as mentioned earlier, a shared privilege implements a full 
Treaty, i.e., either site can operate the rules in the strategy on the other site's 
data. 

120 
CHAPTER 4 
Rule Name Space 
Back in Section 3.5.3 we identified the naming problem of process units, which 
appears here too. Since Treaties effectively implement common sub-processes, 
once a set of rules is imported and integrated in a local process, there must be a 
way to identify the very same rule across the multiple "members" of the Treaty. 
Rule names are obviously not sufficient, since multiple (overloaded) rules with 
the same name can co-exist even within a single local SubEnv (see [59] regarding 
rule overloading). So, some sort of unique rule id scheme is needed. In Oz, 
this is done by using the unique SubEnv id (multiplied by a large constant) 
as a prefix to the normal id generation of the rule translator. (In fact, all 
the necessary unique identifiers are derived from the uniqueness property of 
the SubEnv id which is guaranteed at site registration time, as will be seen in 
Section 5.3.) Thus, rule-ids of all rules of an imported strategy are guaranteed 
to be distinct from the rule-ids of any local rules, or rules from other imported 
strategies. When a Summit rule is invoked, its rule id enables remote sites to 
uniquely identify the invoked rule with their own copy of the rule, and service 
the various Summit requests that refer to that rule (e.g., validation, remote 
backward and forward chaining, etc.). 
4.3.2 
Local Evolutions and Dynamic Treaty 
Validation 
In Chapter 3 we discussed the rationale for autonomous local evolution and 
the tradeoffs associated with it. The main point was that in order to comply 
with the autonomy principle, we want to allow independent modification of 
local processes, including operations that explicitly leave Treaties, as well as 
operations that might indirectly affect Treaties. To comply with the indepen-
dent operation principle, we do not want to depend on other SubEnvs when 
such evolutions occur, and we want to minimize the communication overhead. 
On the other hand, we still want to make sure that Treaties are valid during 
relevant Summits. Another aspect of the dynamic validation is to protect a 
site from remote "invasions" of privacy, by ensuring that rules that were never 
included in a Treaty are not allowed to execute across SubEnvs. 
Oz fully complies with this model of local evolution. With the exception of 
import, all operations that manipulate the local processes are local and involve 
no interaction with other SubEnvs. Therefore, some of the operations can po-
tentially invalidate prior Treaties. In order to be able to detect invalid Treaties, 
we have to revisit the conditions that constitute a valid Treaty, analyze all the 

Realization in Oz 
121 
cases that might cause some of those conditions to not hold, and ensure that 
the proper checks are made at run time to detect invalid Treaties. 
A (simple) Treaty from E1 to E2 on strategy 51 is valid only if all three con-
ditions below hold: 
1. Either: 
(a) 51 is marked at E1 as exported to E2, and is marked at E2 as imported 
from E1 ; 
(b) 51 is marked at E1 as imported from E2, and is marked at E2 as 
exported to E1. 
2. 51 is marked at E1 as a requester of E2, and is marked at E2 as an acceptor 
from E1• 
3. 51 is identically defined in both SubEnvs. This is the "common sub-
process" invariant discussed earlier in Section 3.2.6. 
The first condition can be invalidated whenever unexport at the exporting site, 
or un import at the importing site, is issued. unexport can be easily detected 
locally at the invoking site -
the invocation is rejected if the issued rule is not 
(anymore) exported. unimport is also easily detectable since when a Summit 
rule is requested on the remote site, if it is part of a strategy which has been 
unimported it will simply not be found (the case of faked import, i.e., when 
the rule was already defined locally, is covered in the second condition since 
the sole purpose for faked imports is to affect execution privileges). 
As for the second condition, both request and accept privileges have to be 
checked for their validity. 
1. request -
E1 can lose its request privileges on 51 if the equivalent of 
cancel(51 , E 1, E 2 ) was issued. This can occur in Oz in one of two ways, 
depending on the method by which the request privileges were originally 
obtained: (1) If through export-request, then an unexport-request on 51 
from E1 to E2 revokes request privileges. This can be validated at E1 
locally when the Summit rule is invoked, at the same time the export 
privileges are checked. (2) If through import-request, then an unimport 
on 51 at E1 invalidates condition 2. Thus, validity checking is similar to 
that for condition 1. In case of a faked import, the request privileges are 
checked locally. 

122 
CHAPTER 4 
2. accept -
E2 can revoke accept privileges from E1 on Sl whenever the 
equivalent of deny(Sl' E1, E2) occurs at E2. This can occur also in one of 
two ways, depending on the original commands issued to set up the privi-
leges: (I) In case of export-accept, an unexport-accept command revokes 
the accept privileges. To validate this case, E2 must explicitly check for 
proper accept privileges every time a rule in Sl is issued from E1 on data 
from E2; (2) In case of import-accept, an unimport at E2 invalidates the 
accept privileges. Again, in case of normal unimport, there is nothing to 
check, the rule will simply not be found. In case of a faked unimport, a 
check for accept privileges is required. 
The third condition implies that all copies of a Summit rule must be identical 
in all involved SubEnvs. This condition can become unsatisfied as a result of 
(local) process evolutions, and is more complicated to check for. The key to 
the solution is evolution timestamps, explained below. 
Evolution Timestamps 
A set of strategies can be added, removed, or modified, effectively evolving the 
on-going process at a local site. There are several considerations with respect to 
Treaty validation. First, it is important to retain the validity of prior Treaties 
which are not affected by the evolution. Specifically, a Treaty is not affected by 
a local evolution if the evolved site is (only) the importing site in that Treaty. 
Since sites have no access to the "source code" of imported rules (as explained 
earlier in Section 4.3), this evolution does not violate the common sub-process 
invariant. 
The more severe problem is when a strategy which was imported into a remote 
SubEnv{s) is being evolved at the exporting ("source") SubEnv. Regardless of 
the process privileges attached to the exported strategy, such evolution violates 
the common sub-process invariant. 
The idea is for the local SubEnv to assign a "timestamp" each time a strategy 
in its process is compiled and loaded locally. The term timestamp is a bit 
misleading, in that it is simply a local incrementing counter which does not 
depend on any real time or any other SubEnv's counter that would require 
"global time". When a strategy is imported, its timestamp is also shipped 
and stored at the importing SubEnv. At run-time, whenever a Summit rule is 
invoked for execution, the timestamp at the requesting SubEnv is compared to 
the one stored at the accepting SubEnv. If there is a mismatch, it means that 
local evolution took place at the exporting SubEnv, implying invalidation of 

Realization in Oz 
123 
the Treaty, and the execution is rejected. Re-activation of the Treaty can be 
made by either re-importing explicitly the (possibly modified) strategy, or by 
reloading the process, which also fetches the up-to-date versions of all imported 
strategies. 
Under the above circumstances, the mismatch is guaranteed regardless of whether 
the exporter is an acceptor or a requester. But care must be taken that the 
check is for exact comparison. That is, if the exporter is a requester, its times-
tamp will be greater than the importer's, but if the exporter is an acceptor, its 
timestamp will be smaller than the importer's. 
To illustrate this point and the timestamp mechanism in general, consider the 
two scenarios depicted in figures 4.7a and 4.7b. In 4.7a, strategy Sl is loaded 
in E1 with timestamp 7, then exported as a requester to E 2, and imported by 
E2 as an acceptor. A subsequent (local) load at E1 increases Sl 's timestamp to 
8, so when (a user operating in) E1 requests execution of a rule defined in Sl 
on data from E2, the timestamp comparison at E2 fails because the acceptor's 
(E2) timestamp on Sl is smaller than the requester's (Ed. In 4.7b, E1 is the 
exporter and E2 the importer as before, but the privileges are switched: E1 is 
the acceptor and E2 is the requester. When an attempt is made to execute from 
E2 a rule defined in Sl on data from E1 right after a (local) load that occurred 
at E1 (thereby increasing Sl 's timestamp), the timestamp comparison fails 
again, but this time because the acceptor (E1) has a larger timestamp number 
than the requester (E2). 
This dynamic approach to Treaty validation eliminates the need to notify all 
related SubEnvs when a local process change occurs (some of them might not 
even be active at that time), and moves the responsibility of upgrading the 
imported rules to each remote SubEnv when it actually needs to use them. This 
"lazy update" approach fits well with the general decentralized philosophy. 
Figure 4.8 summarizes this section by presenting the dynamic Treaty validation 
algorithm (in pseudo-code), which is executed in the acceptor SubEnv prior to 
invocation of each Summit rule. Notice the three distinct levels of checking 
which correspond to the three conditions discussed above, and the possible 
actions that are required in order to reactivate old Treaties or establish new 
ones. 

124 
CHAPTER 4 
El 
E2 
10ad(Sl,7) 
export-request(Sl, E2) 
import-accept(Sl, El, 7) 
10ad(Sl, 8) 
execute Sl on data from E2 
Treaty Invalid (7 < 8) 
time 
Figure 4.7a Evolution Timestamps: Export-Request hnport-Accept 
El 
E2 
10ad(Sl, 7) 
export-accept(SI, E2) 
lmport-request(SI, El, 7) 
10ad(SI, 8) 
execute SI on data from El 
Treaty Invalid (8 > 7) 
time 
Figure 4.7b Evolution Timestamps: Export-Accept hnport-Request 

Realization in Oz 
125 
verify-treaty (RuleId, SrcSubEnv, DstSubEnv): 
/* Executes at DstSubEnv * / 
if ( find rule with the given Ruleld) 
then 
1* CONDITION 1 * / 
else 
if ( DstSubEnv accepts the rule from DstSubEnv) 
then 
1* CONDITION 2 * / 
else 
if (rule's remote timestamp = rule's local timestamp) 
then 
1* CONDITION 3 * / 
else 
Treaty is valid, allow execution 
Treaty is invalid, reject execution 
Reason: local evolution at the exporting SubEnv 
Reactivation: re-import (or reload) at the 
importing SubEnv with proper privileges 
end if 
There is no Treaty on that rule, reject execution 
Reason: an equivalent of cancel occurred 
Reactivation: DstSubEnv needs to accept the strategy 
end if 
Requested rule does not exist in local SubEnv, cannot execute 
(Re)activation: DstSubEnv needs to (re)import the strategy 
end if 
Figure 4.8 Run-time Treaty Validation Algorithm 

126 
CHAPTER 4 
1) compile [?c:CFILE]: 
2) 
#. BIIDIIGS: collect header files #included in ?c 
3) 
(forall HFILE?h suchthat (linkto [?c.hfiles ?h]»: 
4) 
# COIDITIOI: 
6) 
# the C file must have been analyzed but 
6) 
# not compiled yet. 
7) 
(and 
( ?c. analyze..status = Analyzed ) 
8) 
no..backward ( ?c. compile..status = lotCompiled» 
9) 
# ACTIVITY: invoke with all necessary arguments 
10) 
COMPILER compile ?c.contents ?c.compile~og ?c.object_code 
11) 
?h.contents 
12) # EFFECTS: (depending on outcome of compilation) 
13) ( ?c.compile..status = Compiled); 
# 0: 
Compile succeeded 
14) [ ?c. compile..status = ErrorCompiled ]; # 1: 
Compile failed 
Figure 4.9 Compile Rule 
4.3.3 
Common Sub-Schema 
Since every data binding and reference in Oz rules is typed, rules implicitly 
require an underlying schema. For example, the compile rule in figure 4.9 
operates on a formal parameter of type CFILE (line 1), which must have a 
compile..status attribute of enumerated type with at least three of the possible 
values in that type being lotCompiled, Compiled, and ErrorCompiled, as 
seen in lines 8, 13, and 14, respectively. The bindings of the rule also impose 
structural requirements on the schema. In the example, line 3 implies that 
CFILE has a link attribute named hfiles to the class HFILE (representing 
header files that are included by the CFILE). 
When a rule is imported, the importing SubEnv must have the proper com-
patible sub-schema in order to be able to compile the rule and later execute 
it. Thus, the strong typing property rules out any hope for defining Treaties 
over totally disjoint and unknown schemas. The discussion of this issue in Sec-
tion 3.2.4 advocated a solution that allows to specify arbitrary common and 
private sub-schemas, as opposed to requiring global schema or restricting the 
common sub-schema by some criteria. We now turn to the solution in Oz. 

Realization in Oz 
127 
Solution in Oz 
There are two problems with realizing common sub-schemas: (1) Static -
how 
to test whether a remote sub-schema that is implied by a set of imported rules, 
is compatible with the local schema of the importing SubEnv. (2) Dynamic -
how to uniformly manipulate objects with only partial common-subschema. 
To illustrate the problems, consider the two definitions of the class CFILE at 
two SubEnvs, El and E 2 , given in figures 4.10a and 4.10b. There are several 
differences between the class definitions. For example, the first definition has a 
config attribute (line 7) that is missing from the second definition (which has 
instead a set attribute configs, in line 26); the enumerated analyze...status 
attribute at line 3 has a different set of values than in its counterpart at line 
17; and the compile..options attribute is of type string in the first definition 
(line 6) and an enumerated type in the second (line 20). Yet despite these 
differences, the compile rule that was shown earlier in figure 4.9 should be 
able to properly fire on objects from either definition, because the subset of 
attributes that is accessed by that rule is compatible. But in order to enable 
this (and disable non-compatible rules), the local process translator has to 
syntactically accept such rules at compile time, and the process engine must 
be able to accept objects from either class at runtime, even though they share 
only a sub-schema and are therefore not structurally identical. 
In some cases the same symbol might be bound to a set of objects defined 
by different classes, making the dynamic problem more severe. Consider, for 
example, the multi-edit rule in figure 4.11, in conjunction with the definitions 
of the CFILE class in figures 4.10a and 4.10b. Suppose that a user invokes 
the multi-edit rule with parameter objects Mi bound to 1mi, and M2 bound 
to 1m2 where Mi and M2 are from SubEnvs El and E2 , respectively. Then, 
the symbol 1c (line 5) will be bound to all CFILE objects which are children of 
either Mi or M2, meaning that they will be instantiated from different definitions 
of CFILE. Note that this rule should also be allowed to execute as far as sub-
schema compatibility is concerned, because the accessed attributes in this rule 
are common to both definitions. 
Addressing the Static Problem 
Oz's solution to the static problem is as follows. First, there are no provisions 
for allowing isomorphic compatibility, i.e., structurally identical sub-schema 
with different names. Although a possibly useful feature, it is beyond the 
scope ofthis research. But only by-name type checking is not sufficient either. 

128 
CHAPTER 4 
1) CFILE .. 
superclass FILE; # SubEnv E1 
2) # State Attributes 
3) 
analyze...status : 
(NotAnalyzed, Analyzed) = NotAnalyzed; 
4) 
compile...status : 
(NotCompiled, Compiled) = NotCompiled; 
5) 
obj ect_time...stamp 
time; 
6) 
compile-Options 
string; 
7) 
config 
string 
8) # File Attributes 
9) 
obj ect_code 
binary = ".0"; 
10) 
contents 
text = ".C ll ; 
11) # Composite Attributes 
12) # Reference Attributes 
13) 
hfiles 
set_of link HFILE; 
14) end 
Figure 4.10a Definition of class CFILE 
15) CFILE:: 
superclass FILE, PROTECTED~NTITY; # SubEnv E2 
16) # State Attributes 
17) 
18) 
19) 
20) 
21) 
22) 
23) 
24) 
25) 
26) 
27) 
28) 
29) 
analyze...status 
(NotAnalyzed, ErrorAnalyzed, Analyzed) 
= NotAnalyzed; 
compile...status 
(NotCompiled, Compiled) = NotCompiled; 
compile-Options 
(Debug, Optimize, Normal) = Normal; 
obj ect_time...stamp 
time; 
# File Attributes 
obj ect_code 
binary = It.o"; 
contents 
text 
= II c"· 
. , 
# Composite Attributes 
configs 
set_of CONFIGJ)RC_oz; 
# Reference Attributes 
hfiles 
set_of link HFILE; 
end 
Figure 4.10b Another Definition of class CFILE 

Realization in Oz 
1) multi-edit [?ml:MODULE, ?m2:MODULE]: 
2) 
(and 
3) 
# binding: 
129 
4) 
# collect all source files contained within either module 
6) 
(forall CFILE ?c suchthat (or 
6) 
(member [?ml.cfiles ?c]) 
7) 
(member [?m2.cfiles ?c]») 
8) 
# collect all the header files linked to the sources 
9) 
(forall HFILE 
?h suchthat (linkto [?c.ref ?h]»): 
10) # condition: check that sources are accessible to invoker 
11) 
(and 
12) 
(?c.reservation~tatus = CheckedOut) 
13) 
(?c.allowed-Bdit = CurrentUser» 
14) 
MULTIJ&DIT multi_editor ?c.contents ?h.contents 
16) 
16) 
17) 
#0. 
(and 
sources from both ml and m2 changed 
(?ml.status = lotCompiled) 
(?m2.status = lotCompiled)i 
18) 
#1. 
only sources from ml changed, 
19) 
(?ml.status = NotCompiled)i 
20) 
#2. 
only sources from m2 changed, 
21) 
(?m2.status = NotCompiled)i 
22) 
#3. 
no changes made, assert nothing 
23) 
no..assertioni 
Figure 4.11 
Multi-edit rule 

130 
CHAPTER 4 
For example, the compile_options attribute has the same name in both def-
initions but the types are different, so such incompatibility, which cannot be 
tolerated, will not be detected. So both by-name and by-structure checks are 
required. The process translator first checks whether the names of classes and 
attributes that are referenced in the imported rules exist in the local schema. 
This is identical to the by-name checking that is performed for local rule com-
pilation, so no special extensions are needed to obtain this functionality. This 
should be true for any strongly typed PML that must perform type checking 
to verify that the operands conform to the schema. The by-structure check is 
more complicated, however. Each attribute specified in the imported rules has 
to be checked in both schemas, to see that it corresponds to the same type. 
Note that there could be some complications due to inheritance, because an 
attribute of a class that is specified in a rule might actually be defined in one of 
its superclasses, implying that the process translator has to search through the 
class hierarchy. Moreover, checking for lattice compatibility (i.e., the composi-
tion hierarchies) might be required for the structural bindings in rules. Finally, 
a potential problem might be different ordering of the same attribute in dif-
ferent class definitions. This, however, is not a problem in Oz since attributes 
in Oz are accessed through their name, not through field offsets. In fact, this 
attribute-based access also facilitates the solution to the dynamic problem, as 
will be seen shortly. 
The general solution to the static problem is then to compare the subschema 
specified in the imported strategy with the local schema, generate the structural 
"delta" between them, and determine if the attributes accessed by the rules in 
the imported strategy overlap with the "delta". If there is such overlap, the 
imported strategy is not schema-compatible with the local process, and the 
import fails, otherwise it succeeds. 
An alternative solution would have been to merge schemas. That is, in case of, 
say, two CFILE definitions, both SubEnvs would end up with the same CFILE 
definition that is the union of the attributes, with some arbitration policy 
among conflicting attributes. However, this approach has several drawbacks, 
particularly with respect to the "core" requirements. The main problem with 
this approach is that it implies that the local instantiated objectbase has to 
be evolved to correspond to the new merged schema, and even the "source" 
SubEnv might possibly need to be evolved. But most of all, since objects are 
"mixed" only temporarily during execution of rules, but otherwise reside in 
their own private objectbase with their own schema, there is no justification 
to merge local schemas and evolve the objectbases just to satisfy the type 
restrictions. 

Realization in Oz 
131 
The solution in Oz is based on the "delta" analysis front-end component of 
the Evolver, discussed in Section 2.2.6. The idea is to apply this delta ana-
lyzer selectively to the classes that are referenced in the imported strategy, and 
generate a "delta" between the importing and the exporting SubEnv (which 
therefore must send those class definitions at import time). If the delta is un-
acceptable (for example, because of mismatched types) the import is rejected. 
It seems that the same approach could be applied to a wide range of implemen-
tations, so long as they have a schema evolution utility. The key observation is 
that the analysis step in schema evolution is similar to the one that is needed 
for import purposes. 
Addressing the Dynamic Problem 
The essence of the solution to the dynamic problem is attribute-based access 
mentioned above. That is, when an object is transferred from one site to an-
other, it is treated merely as a set of attribute-value pairs (and an object-id). 
These pairs also include all the attributes that were inherited from any super-
classes. Since all accesses to objects from the rule processor specify attributes, 
there is no need to carry the original class definition with the object. As long 
as the attributes that are accessed by the rule have been verified at Treaty 
definition time to be compatible with those defined in the remote schema, the 
corresponding remote objects can be accessed properly. Since all attributes 
that are accessed by the process are explicitly stated in the rules, there is no 
way to mistakenly access attributes that are not defined or have the wrong 
types. 
The simplicity by which the set of accessed attributes is specified in rules enables 
us to rigorously analyze which object fragments will be accessed, and therefore 
enable co-existence of different schemas with sufficient common-ground for exe-
cution of multi-site activities. The object-based paradigm in itself also helps to 
support co-existence of multiple schemas, because the object identity allows a 
rule to have a direct handle on an object, without necessarily requiring to know 
its full schema. It is sufficient to require only that the values of the attributes 
which are part of the subschema accessed by the "method" (rule) will be valid. 
4.3.4 
Exporting Data Instances 
In Section 3.2.5, it was realized that while in some cases the definition of a com-
mon sub-schema might imply that the instances of the classes in the common-

132 
CHAPTER 4 
subschema are also common, this is the exception, not the norm. In general, it 
would be impractical to assume that an accepting SubEnv implicitly "exports" 
all instances that belong to the common sub-schema. This observation certainly 
holds for Oz, where a typical project-database is built around a composition 
hierarchy that is orthogonal to the class hierarchy. Thus, the fact that a set of 
objects is instantiated from the same class is immaterial with respect to their 
semantic relationship; more often, a SubEnv would want to export a set of ob-
jects that are structurally related. Consider, for example, the case where CFILE, 
a class that abstracts C source files, is part of a Treaty from El to E2• Then 
the above approach would imply that users from El can access with Treaty 
rules any CFILE objects in E 2• Not only is this granularity too coarse, it is the 
wrong kind of association. Instead, the export of data should be specified by 
selecting objects which are related structurally, e.g., a sub-project containing 
some CFILE objects, as well as libraries, binaries etc. 
It is clear, then, that a separate exporLdata mechanism, as outlined in Sec-
tion 3.2.5, is required. The generic specification of exporLdata ignored several 
practical issues concerning specific PCE implementations, however. The first 
major issue to discuss is the various granularities for specifying the export: (1) 
Is it affordable to control the data export on a SubEnv-basis (as suggested in 
the generic specification) or should it be global; (2) Is it affordable to further 
control the export on a per-strategy basis (as suggested in the generic spec-
ification); (3) What should be the objectbase granularity for exporting data 
(the generic specification did not address this PCE-specific aspect at all); and 
(4) What should be the granularity for specifying access modes on exported 
data, if different from (3). The second major issue concerns the interface to 
such a mechanism. The problem is to abstract the operation so that it can be 
performed in a relatively high-level, not requiring to specify each data element 
separately. We now discuss our solution. 
Exporting Data in Oz 
Clearly, controlling the export on a per-SubEnv basis must be supported as it 
is essential for autonomy and security purposes, and is in-line with the "not-
only-Iocal-or-global" approach adopted throughout this work. Given that the 
number of SubEnvs is relatively small, the overhead should be small. As for 
the data granularity for specification of export, a single object seems like the 
ideal granularity, but the overhead is much higher since the number of objects 
is typically large, and this scheme implies that each object must have an addi-
tional information concerning export status. Moreover, in conjunction with the 
per-SubEn~ support, this information must be maintained as a list, as opposed 

Realization in Oz 
133 
to a binary flag. Similarly, specifications for access permissions and control on 
a per-SubEnv basis are desired but add both space and computation overhead. 
The design of ezporLdata in Oz is based on extensions to the basic access 
control mechanism inherited from Marvel (discussed earlier in Section 2.2.7). 
The idea is to utilize the already existing mechanism for specifying group per-
missions, and to associate SubEnv permissions with such groups. That is, an 
exported object has, in addition to its local user and group permissions, SubEnv 
permissions (that happen to be implemented as permission groups). When a 
SubEnv E1 receives a remote request from E2 to access a local object 0 1, the 
server at E1 checks whether 0 1 is accessible to E2 by inspecting 0 1 's group 
permissions for E2. If group permissions are not specified in 0 1 for E2, or if 
the permissions are not compatible with the request, the access is denied. 
Thus, using a straightforward extension of the general-purpose access control 
mechanism, we obtain: (1) object-level granularity for export; (2) per-SubEnv 
specification of export; and (3) object-level permission specification. Moreover, 
this extension does not seem to incur extra overhead beyond what is necessary 
by the local access control. However, just as with local access control, the 
number of groups attached to each object does affect performance, as each 
access to an object involves a larger search space. But given that the name 
space of SubEnvs is well known, a simple effective hashing algorithm can retain 
a constant search time. The only capability that cannot be supported in this 
approach is to be able to specify exports on a per-strategy basis. However, the 
problem with this feature is not with the actual specification, since it could be 
achieved by considering the Cartesian product of SubEnvs and strategies and 
creating a unique group for each pair. The main problem with this feature 
is that such a mechanism would imply that any evolution involving Treaty 
rules would require a global search in the database to update all objects that 
might have been affected by the evolution, making this feature intractable and 
unacceptable. 
The next issue to consider is how to abstract the ezporLdata operation so 
that it can be performed as a relatively high-level command. The idea here is 
to rely on the composition hierarchy as the abstraction for grouping objects. 
Thus, ezporLdata takes as arguments a remote SubEnv, a local composite-
object that is the root of the sub-tree to be exported, and a permission string. 
The operation then traverses all descendant objects of the root, and for each 
object it generates the proper SubEnv group (unless it already exists) with 
the specified permissions. This approach allows the administrator to apply the 
ezporLdata operation on arbitrary level of granularity. In particular, it can be 
applied to single "leaf" objects. 

134 
CHAPTER 4 
Finally, an additional possible extension to the access-control mechanism could 
be to extend remote permissions on a per-user basis (as opposed to on a 
SubEnv-basis), similar to local access control. The main problem with this 
approach is that the set of possible (remote) users is not known at any time, 
and obtaining knowledge at each SubEnv about users from remote SubEnvs 
contradicts decentralization. In addition, this model assumes that in most 
cases the group-level permissions associated with SubEnvs is sufficient, just as 
Treaties are formed between groups of users on a SubEnv basis (if the group 
becomes too large to treat it coherently, perhaps it should be split into separate 
SubEnvs). 
In the rare cases where remote user-level access control might still be highly 
desired, a possible approach might be to assign the remote user a (remote) 
user object, which entitles the permissions as specified in its mask, subject 
to the SubEnv permissions which receive first priority. One way to look at 
these permissions is that the remote user is treated as a friend user (borrowed 
from the concept of a friend function in C++), which, regardless of the origin 
SubEnv from which he/she operates, can access the remote data as a local user. 
In some cases, the "friend" user can be the same (mobile) person who logs in 
from several SubEnvs at different times. 
4.3.5 
Preserving Process Consistency 
The general process consistency problem was defined in Section 3.2.7. Because 
of the decentralized nature of our model, there is no notion of "global" consis-
tency. Just as sub-schema compatibility is a sub-problem of the more general 
schema evolution problem, sub-process "compatibility" (i.e., consistency) is a 
sub-problem of the more general process consistency problem. In fact, this 
is even more evident in the case of process than in the case of schema, since 
import implies only pure additions or deletion of whole rules. That is, it ex-
cludes the harder cases of allowing to modify existing rules. The reason is that 
Oz allows multiple rules with the same name (and even signature) to co-exist, 
and further, it does not have the notion of "merging" rules 7 . Thus, importing 
a strategy amounts to evolving a process by adding to it a set of new rules (or 
deleting rules in the case of unimport). This means that the process evolution 
algorithm employed in Marvel can be used "as-is" by the import operation to 
verify the consistency of the local process after a set of Treaty rules has been 
added to it. 
7This was supported in earlier versions of Marvel, but not in Marvel 3.x. and not in Oz. 

Realization in Oz 
135 
4.4 
MULTI-PROCESS ENACTMENT IN Oz 
Process enactment in Oz can be roughly divided into local and multi-SubEnv 
enactment. The former involves only a single SubEnv and all interactions 
between the server and any of its local clients (and their corresponding users). 
The latter includes all operations that involve interactions among servers and 
clients from multiple SubEnvs. 
Pure local enactment not related to multi-SubEnv enactment is largely the 
same as in Marvel, and is not discussed here any further. As for multi-SubEnv 
enactment, there are several types: 
1. Purely remote -
a client interacts directly with a remote SubEnv without 
involving its own local server. 
2. Built-in cross-site commands -
multi-SubEnv operations that are imple-
mented as part of the kernel, and are not process-specific. 
3. Enactment of Treaty rules, following the Summit model. 
The first two kinds of enactment are, for the most part, not process-specific 
and relatively minor, and are discussed briefly in Sections 4.4.1 and 4.4.2, re-
spectively. The third type is the major type of enactment, and is discussed in 
depth in Section 4.4.3. 
4.4.1 
Direct Remote Interaction 
First, we need to justify why a client would directly interact with a remote 
server without involving its local server (except for establishing the connection 
in the first place, explained later in Chapter 5). A simpler architecture would di-
rect any cross-SubEnv interaction through the local server, reducing the types 
of interactions across SubEnvs. However, given that there are some "core" 
built-in operations that have identical well-defined semantics in all servers and 
involve data from only one remote site, it makes sense to allow direct com-
munication between clients and remote servers for these operations, thereby 
eliminating unnecessary overhead at the local servers and reducing the number 
of message "hops". 
Several direct remote services are provided in Oz, mostly those that corre-
spond to the built-in object base access and manipulation operations, namely: 

136 
CHAPTER 4 
browse, print object, add, delete, link, unlink, and the single-server ver-
sions of move and copy. Thus, a client that is connected to a remote SubEnv 
can potentially issue these commands directly to the remote server without 
involving the local server. 
One potential negative implication of this approach might be due to overload-
ing. Since Oz allows built-in operations to be overloaded with process-specific 
rules that specialize these operations on certain classes (the default built-in 
operations work on all classes), a request for remote operation might have var-
ious unanticipated implications. This could have been a serious problem if the 
executed rule was remote to the data (i.e., local to the remote user's SubEnv). 
However, this is an impossible scenario since the operation is executed at the 
remote SubEnv using its own (perhaps overloaded) built-in operations, which 
are local to its data (and therefore "owned" by it). Thus, this does not incur 
any violation of autonomy or privacy. In the worst case, the (remote) user 
might be surprised by some unanticipated behavior resulting from invoking an 
overloaded built-in operation. 
4.4.2 
Built-in Multi-SubEnv Operations 
Oz extends Marvel's repertoire of built-in commands with: (1) cross-site copy 
and move objectbase operations; (2) import and treaty operations that support 
construction of Treaties; and (3) a set of built-in rules for SubEnv (de)registration. 
The import and treaty operations were covered earlier, and site registration is 
covered separately in Section 5.3, so we cover here only copy and move. 
The operational semantics of these operations are straightforward: they copy 
(move), a (possibly composite) object from one SubEnv to another. There 
are several technical problems, though: (1) The schemas at the two SubEnvs 
might differ in an incompatible manner, such that one or more of the objects at 
the source SubEnv belongs to classes that are either defined differently at the 
target SubEnv, or worse, not defined at all. A related problem, particularly 
with copying (moving) composite objects, is possible incompatibility in the 
composition hierarchies; (2) The overloading mechanism could be potentially 
dangerous here since, as described above, local processes might overload these 
operations, and unlike the single-SubEnv case, both SubEnvs are involved here. 
This means that it might be possible for one SubEnv to invoke an overloaded 
version of, say, the copy rule, on remote data, with the remote SubEnv (and 

Realization in Oz 
137 
its administrator) not knowing the contents of the rule (consider a worst case 
scenario where some malicious remote SubEnv overloads copy with delete). 
Before addressing these problems, it is worth mentioning a useful application 
of cross-site copy and move: it can be used to effectively implement object-
base split and merge operations (provided that the merged objectbases have 
compatible schemas). 
Addressing the Schema Compatibility Problem 
The main reason that the schema compatibility problem reappears here is that 
we want to support these operations without the need to specify them as part 
of a Treaty, similar to the way single-server built-in operations are supported. 
Further, this problem differs from the schema compatibility problem addressed 
in Section 4.9 where objects are temporarily transferred across SubEnvs. Here, 
the copied objects become part of the local persistent objectbase, thus they 
must fully correspond to the local schema definition. 
If we blindly allow copy (or move) to occur, we risk the possibility of acquiring 
objects which are completely or partially schema-incompatible, making them 
either inaccessible or even worse, corrupting the internal objectbase structure. 
An analogy to this situation is the well known "structure offset" problem in 
conventional programming languages, whereby a data structure with the same 
name is defined differently in two modules, and a pointer is passed from one 
module to another. The offset due to the different definition is likely to corrupt 
the program stack when the "receiving" module executes. 
There are two possible solutions to the schema problem. The first solution 
simply avoids the problem by requiring an identical sub-schema, and rejecting 
the operation in case of incompatibility. This is a simple but unnecessarily 
restrictive solution. The second solution, which was adopted in Oz, interprets 
an object copied to the target SubEnv according to the local schema's class 
definition. This in turn might result in some loss of data if some attributes 
in the class definition of the source SubEnv are missing from the definition in 
the target SubEnv. Alternatively, if the target class subsumes the source class, 
the default values (which can be determined optionally in the class definition) 
are assigned to the missing attributes. If attribute types conflict, the target 
SubEnv can coerce values if possible, or assign the local default values. 
As for the composition hierarchy, a similar approach is applied. Here, however, 
entire (composite) sub-objects might be lost if the expected composition at-

138 
CHAPTER 4 
tributes are missing in the target SubEnv. To avoid possibly undesirable loss 
of data due to schema incompatibility, which is especially important in the case 
of move, a warning message listing the lost data should be presented to the user 
with the option for a possible retraction. 
Addressing the Rule Overloading Problem 
A cross-site copy/move operation can be executed in one of two possible ways: 
it can be executed either at the source SubEnv (i.e., the SubEnv from which 
the objects are copied/moved) or at the target SubEnv. In either case, only one 
SubEnv fires the actual rule, and the other SubEnv performs the remote op-
eration derived from the actual built-in copy/move operation. (Note, however, 
that the low-level operations for copy and move as well as the other structural 
built-in operations cannot be overloaded in Oz -
only the rules containing 
them can be overloaded. This design choice was made deliberately to avoid the 
possibility of arbitrarily changing the semantics of primitive operations.) 
Thus, if the SubEnv that executes the copy (move) happens to have an over-
loaded rule which is unknown to the other SubEnv, such execution (without 
a Treaty) presents a clear violation of autonomy. Therefore, only the built-in 
(non-overloaded) versions of copy/move can be used for cross-site execution, 
and an overloaded version could be used only if it is part of a Treaty. 
4.4.3 
The Summit Model in Oz 
Implementation of Summits in Oz could be considered as the most important 
and most comprehensive aspect of the implementation efforts. Summits are the 
main means by which multiple SubEnvs interoperate, and as such, they encom-
pass all the support that is required to enable multi-process activities among 
the pre-defined common sub-processes while still preserving the autonomy and 
privacy of the private sub-processes. While Treaty support is also dynamic, 
it is conceptually static or "meta-enactment" since it deals with definitional 
aspects of the process. Consequently, infrastructure support for the realization 
of the Treaty protocol is much less complicated. Summit is the realization 
of "real" enactment of multi-user and multi-process activities, rules, and rule 
chains (i.e., automatic enactment of tasks). Thus, whereas Treaties refer to 
static properties of rules and data (e.g., formal parameters and types), Sum-
mits are concerned with dynamic properties of rules under execution, such as 
the runtime objects that are bound to an executing rule, the chaining context 
in which they execute, and so forth. 

Realization in Oz 
139 
To make our discussion more clear, we define a Summit rule to be a rule that 
contains actual parameters from at least one remote SubEnv. This is a dynamic 
property of rules. While it is true that every Summit rule must have been 
defined in some Treaty, the converse is not always true, since a Treaty rule can 
at times execute only with local data, in which case it is not acting as a Summit 
rule. A Summit task refers to an entire rule chain that involves at least one 
Summit rule. 
We now describe in detail the realization of the Summit protocol, covering 
all five phases of the formal model, with focus on inter-process aspects. To 
simplify the discussion, we defer the discussion of two important aspects: (1) 
transactional semantics of Summits, which are discussed in Section 4.4.5; and 
(2) Context switching requirements that enable concurrent execution of rules 
and Summits, discussed in Section 5.4. An efficient caching mechanism for 
accessing remote objects during Summits is covered in Section 5.5. 
To better illustrate how Summits work, the explanations below are accompa-
nied by an example, involving the multi-edit rule from figure 4.11, and the 
simple objectbases shown in figure 4.1b. 
Summit Initialization and Validation 
A Summit task is initialized as a result of an explicit request from a user. From 
the user's point of view, the only difference between invoking a Summit rule and 
a normal rule is that at least one of the parameter objects specified by the user is 
remote (recall from Section 2.2.5 that MSL supports late binding, allowing the 
user to select different objects at different times as parameters to rules, which 
is essential to the understanding of Summits). The local server from which the 
user invoked the rule is called the coordinating server. For example, assume 
that user israel operates in site NY and invokes the multi-edit rule (from 
figure 4.11) with one local MODULE object named ui, and one remote MODULE 
object from CT named db, corresponding to the objectbases of figure 4.1b. 
(To simplify the example, this rule operates with only one remote SubEnv, but 
recall that in general Summit rules can operate with multiple remote SubEnvs.) 
The first action taken by the coordinating server is to fetch copies of the remote 
objects from their original SubEnvs, and bind them to the parameters of the 
rule (in addition to the obvious binding of local objects, but as we focus on 
inter-site issues we will ignore from now on purely local aspects of the rule 
processor) . 

140 
CHAPTER 4 
This fetching is necessary because the client only holds an objectbase image 
that enables the user to select objects as parameters to rules. The client sends 
to the server object-ids, which are resolved by the server(s) to real objects. 
The reader might wonder at this point why is it necessary to fetch the remote 
objects before doing Treaty validation. The reason is somewhat pragmatic, 
and has to do with the rule-overloading mechanism. Recall that Oz allows 
multiple rules with the same name to co-exist, and determines which rule to 
execute based on the types and number of actual parameters supplied by the 
client [59]. Therefore, when the local server receives a request to execute a rule, 
it has to find the "closest" rule that matches the types of the parameters, so only 
after the remote objects (and their type information) are fetched, can the server 
determine which rule is intended for the Summit. (Actually, this could have 
been optimized, if the client maintained type information in its image and had 
sent it along with the remote object-ids -
but this is not implemented in Oz.) 
Overloading of rules appears to introduce another problem: when an object is 
fetched from the remote site, its class definition might differ from the one in the 
local schema, or might not even exist. However, if the class is not in the local 
schema, then overloading would never find a proper rule. Alternatively, if the 
class is identified in the local schema and a Treaty rule that matches the types 
of the parameter list is found, the corresponding sub-schema is guaranteed to 
be compatible, as described earlier in Section 4.3.3. 
Once the rule is identified, the second step involves Treaty validation. First, 
the coordinating server checks locally whether the rule could be invoked as a 
Summit rule, by checking that the rule has request privileges on the remote 
participating SubEnvs (i.e., those SubEnvs that have objects bound to param-
eters of the rule). If this is not the case, the rule cannot be executed in a 
Summit. But, as explained earlier, this is only a necessary condition, not a 
sufficient one, because the Treaty might have been invalidated unilaterally by 
one or more of the participating remote SubEnvs. So, after local validation, the 
coordinating server requests each participant SubEnv to execute the validation 
algorithm of figure 4.8 (covered in Section 4.3.2). 
After the rule has shown to be a valid Summit rule, the third step binds re-
mote derived parameters. In our example, the bindings of the multi-edit 
rule collect all objects of type CFILE that are children of either ui or db, 
and bind them to the symbol ?c (lines 5-7)8. This results in a binding set 
{ddt.c, query.c, tty.c, xv.c}. It also binds to the symbol ?h all the BFILE ob-
8The oddity with respect to universally quantifying ?c in line 5 was explained in Sec-
tion 2.2.2. 

Realization in Oz 
141 
jects which are linked to all ?c objects (line 9). This results in the binding set 
{db.h, shared.h}. As with regular parameters, sub-schema compatibility among 
derived parameters is assumed to have been checked at Treaty definition time. 
This concludes the initialization phase of a Summit in Oz. 
Pre-Summit 
This phase consists of two parts. First, the coordinating server evaluates the 
rule's condition. In our example, the condition is a simple conjunction of two 
predicates that evaluates to true if all objects bound to ?c have been properly 
checked out (line 12) and that the current user invoking the rule is allowed to 
edit those objects (line 13)9. 
The second part of pre-Summit is required if the condition is not satisfied (i.e., 
it evaluates to false). The coordinating server attempts to satisfy the con-
dition by fanning out to the participating sites and triggering local backward 
chaining at each site in an attempt to update the objects so that they satisfy 
the condition. Backward chaining is private, i.e., each process performs this 
step according to its autonomously defined sub-process. Some optimizations 
could be made here. For example, in cases where it is possible to satisfy a 
condition only based on backward chaining at the coordinating site, it should 
be attempted first, before any remote chaining is spawned. And in the cases 
where remote chaining is necessary, it should be spawned and performed simul-
taneously in all sites (including the coordinating site in "local" mode). Note 
this can be done only if there are no data dependencies across sites (another 
good reason to avoid cross-site links). 
The backward chaining algorithm is iterative in the following sense: After 
spawning the remote backward chains, the coordinating server "waits" (in prac-
tice it actually saves the context in a data structure called the "rule stack" 
and performs a context-switch to service other requests, but for the purposes 
of this discussion we can assume that it logically waits) for the remote servers 
to return with the results, which possibly contain some modified objects. The 
coordinating server then re-evaluates the condition. If backward chaining has 
not satisfied the condition, the rule is denied execution. However, even if a 
particular predicate became satisfied during backward chaining, the changes to 
the objects could have made other parts of the condition unsatisfied. Thus, 
the entire condition has to be re-evaluated each time, and backward chaining 
may be iteratively spawned several times during this phase until either the con-
9CurrentUser is a built-in value in Oz that denotes the user who fired the rule, or on 
whose behalf the server is firing the rule automatically. 

142 
CHAPTER 4 
dition is satisfied, or all possibilities have been exhausted and the rule is not 
satisfiable and cannot be executed. Note that the potential to enter an infinite 
loop while evaluating a condition exists, but it merely indicates a flaw in the 
process model. 
In our example, suppose that the object xv.c from NY and query.c from CT 
are not checked-out. NY's process backward chains to its local configuration 
manager, say ReS [125], and issues a check-out rule on the object. At the 
same time, CT backward chains locally to its private configuration manager, say 
sees [112], and issues its own check-out request. This rule could further have 
a condition that implies firing other rules recursively, independent of any other 
site's process knowledge or interest. To illustrate the need for re-evaluating the 
entire condition, it could be the case that some check-out rule satisfied the 
first predicate in multi -edi t 's conjunction (line 12), but at the same time it 
might have also unsatisfied the second predicate (line 13), which could have 
been satisfied prior to the backward chaining. Although perhaps not a likely 
situation in this particular case, we can see how this could happen in general. 
Execution of Remote Activities in pre-Summit 
One important aspect of remote backward (and also forward) chaining involves 
execution of (remote) activities. In Oz, both backward and forward chaining 
can lead to the execution of further activities. That is, chaining is not limited 
to inference rules, and can involve the same kinds of activities contained within 
user-invoked rules. In particular, some of those activities might be interactive, 
requiring input from a user. This presents both conceptual as well as technical 
problems that do not come up in local backward chaining: conceptually, the 
remote server must determine which user's client should execute the remote 
activities; technically, it should be able to redirect the activity to the specified 
user's client. 
The solution in Oz is to direct all activities to the initiating user, by default. 
An optimization could be to direct only interactive activities to the remote 
client and execute non-interactive activities with a local "proxy" client. To 
provide a full solution, however, Oz allows remote activities to be delegated to 
(remote) users by extending its modeling language to specify delegation, and 
by providing a delegation mechanism that redirects activities. This is explained 
separately in Section 4.5. 
In addition to directing activities to clients, an Oz server also sends process 
animation messages to the client that inform the user visually about the task 

Realization in Oz 
143 
being executed. In order to extend this useful capability to Summits, the server 
redirects all animation messages to the "coordinating-client" (the client that 
issued the Summit request), including messages that are executed on behalf 
of remote sites, and possibly by remote delegated users, thereby providing a 
"global" view of the process for monitoring purposes. 
Summit Activity 
If the condition of the rule is satisfied, the activity can execute at the coordi-
nating client. Activity execution usually involves dereferencing file attributes, 
which map to files in Oz's "hidden" file system (inherited from Marvel), and 
handing them to the file-based tools defined in the activities. If all servers share 
the same file system (e.g., via NFS), then as in the case of a single server, only 
path names of the associated files (both local and remote) need to be sent to 
the client, and the tool can access the files through the path names. If servers 
do not share a file system, however, the actual files have to be physically trans-
ferred to the coordinating server's file system before handing them to the client 
(clients and their local server must have a shared file system, by definition), 
and when the activity completes the files have to be transferred back to their 
original location in the file system. This implies a remote file transfer mecha-
nism separate from the object transfer mechanism. Note that while Oz objects 
are "light"- containing only state attributes, pointers to other objects, and 
"pointers" to files- files are arbitrarily large. In all the various cases mentioned 
previously, the remote objects being transferred contained no files. The remote 
file transfer mechanism, including prefetching and file caching considerations, 
is a separate mechanism outlined in Section 5.6.1. 
Back to our example, the bound CFILE objects ({ ddt.c, query.c, tty.c, xv.c}) and 
the related HFILE objects ({ db.h, shared.h}) are passed to the editor activity, 
which in turn invokes a multi-buffer editor tool with one buffer per file (line 
14 in figure 4.11). To complete the example, here is the tool definition for the 
multLedit activity, with similar access mode notations as explained earlier in 
Section 2.2.3: 
HULTI...EDIT 
multLedit 
end 
superclass TOOL; 
string = "multLedit CFILE. contents X 
HFILE.contents 5"; 

144 
CHAPTER 4 
This definition specifies that the mul t Ledi t activity requires to access the 
CFILE objects in eXclusive mode (denoted by the X lock request) and the HFILE 
objects in Shared mode (denoted by the S lock request). These specifications 
are propagated to, and enforced by, the remote transaction managers (covered 
partially in Section 4.4.5). 
Post-Summit 
The first step in post-Summit asserts the appropriate effects of the Summit rule, 
depending on the return code from the activity, including remote assertions. 
Since the executed rule is identical at all participating sites (because of the 
common sub-process invariant), this phase can be carried out in one of two 
ways: either the coordinating server sends a message to the remote servers 
to assert the effect of the rule on the (remote) objects, or the coordinating 
server itself asserts the effects on the replicated objects and sends the updated 
objects to the remote servers. Actually, a similar tradeoff exists with respect 
to the implementation of the binding phase: bindings could be either evaluated 
by requesting the remote server(s) to carry out entire binding queries, or by 
sending the remote servers primitive requests to fulfill parts of a composite 
binding query. The prototype rule-based approach to applying the general 
model (explained in Section 3.5.1) suggested the former approach, and seems to 
be the natural solution given that the definitions of both bindings and assertions 
are guaranteed to be identical in all participating SubEnvs. Moreover, this 
approach has the potential to perform better since the communication overhead 
is reduced. However, the latter approach simplifies rule processing in that the 
Summit rule executes as a whole at the coordinating server and there is no need 
to invoke remote rule processors to execute rule "fragments". In addition, the 
replicated remote objects must be updated in the coordinating server anyways 
as part of the cache management (explained in Section 5.5). Therefore, Oz 
employs the latter approach both in bindings and in effects. 
In the formal Summit protocol, the next step following the assertions is the 
"forward fan-out", in which each SubEnv (including the coordinating SubEnv) 
fires rules locally based on their local and private (sub )processes. Oz deviates 
slightly from this order, mainly due to low-level implementation details not 
discussed here iO . The coordinating server first derives both the local and the 
global (i.e., further Summit) rules to be executed in the forward chain. The 
inferred forward Summit rules are held in a separate Summit stack and are 
invoked in the Summit completion phase only after all local forward chains 
lOThey have to do with the notion of atomicity chaining, discussed in Section 4.4.5. 

Realization in Oz 
145 
complete in all sites. The inference of Summit rules is an important topic, 
explained separately below. 
Following the derivation phase, forward fan-out takes place. Each SubEnv then 
determines which rules to execute based on its local process, and carries out the 
chains locally until all possible forward chains have completed. At this point, 
they return to the coordinating server. 
Inference of (forward chaining) Summit Rules 
Multi-step (or composite) Summits are crucial for modeling and enactment of 
multi-process tasks, simply because a Summit task may consist of several steps. 
There are several approaches to modeling and enacting multi-step Summits. 
Technically, the coordinating server must distinguish chains which are part of 
the local fan-out from these which are "global" Summit rules. One alternative 
is to add modeling primitives (e.g., in the form of effect directives, similar to 
MSL chaining directives) that explicitly annotate effect predicates in rules as 
"Summit" predicates. These annotations could be used to determine which 
chains are local, and which are global. In fact, the initial implementation in 
Oz was done that way. However, this alternative both limits the power of the 
rule inference engine and proves to be unnecessary. 
Given that a Summit rule is syntactically a "normal" rule that just happens 
to have remote objects bound to it, then by extending the standard inversion 
mechanism to handle inversion of remote bindings in addition to local bindings, 
the basic rule-inference mechanism could infer Summit rules -
these are simply 
the rules that happen to have been instantiated with (some) remote objects as 
parameters. This, of course, could only happen in our Summit model if the 
triggering rule had some remote object parameters in the first place, or in other 
words, if it was a Summit rule. Thus, inference of Summits is done exactly in 
the same way that local inference is done. 
The main advantage of this approach is that, as a natural extension of the 
rule processor for handling derivation of Summits, it is no more (and no less) 
implicit that derivation of rules, and it has the potential for automatically 
inferring multi-step Summits which could not have been formed in the explicit 
notation unless they were pre-determined. Another advantage is that Summit 
rules are formed only as needed, whereas the annotation approach would force 
the administrator to consider Summits even when no remote data is involved. 
Finally, adding annotations would have added an (apparently unnecessary) 
burden on process administrators in forming Treaties. 

146 
CHAPTER 4 
Summit Completion 
Once local forward chaining completes in all involved SubEnvs, they notify 
the coordinating server, which in turn checks its Summit stack to see if there 
are any pending Summit rules. If there are no such rules, it completes the 
task, releasing resources that were allocated for the Summit, and commits the 
associated transactions (see Section 4.4.5). 
If there are pending Summit rules, the coordinating SubEnv essentially starts 
with the Summit initialization phase, except it bypasses the manual parameter 
binding phase, which is (automatically) performed by the inversion mechanism. 
Recall that binding must occur before the initiation of forward Summits, be-
cause it is the binding phase that actually recognizes which rules are Summit 
rules. 
Finally, there is one more important difference between the invocation of the 
first Summit and the invocation of subsequently derived Summit rules (in a 
composite Summit): The pre-Summit phase in derived Summits consists of 
only condition evaluation, without the fan-out for backward chaining. This 
stems from the fact that the rule processor in Oz (as in Marvel) is not capable 
of backward chaining during forward chaining. Thus, the realization is limited 
in that respect compared to the generic model. 
4.4.4 
A Composite Summit Example 
The following is an execution trace of a composite Summit example in Oz 
that realizes the motivating example (which was presented in Section 1.3 and 
revisited in Section 3.4). The example is best illustrated in figure 4.12, while 
figures 4.13a and 4.13b show two snapshots of the actual Oz animator that 
were taken as the Summit unfolded. 
The environment consists of three SubEnvs, siteA, siteB, and siteC. To 
simplify the example, each SubEnv has two objects which are relevant to this 
Summit, a parent object of class MODULE and a child object of class FILE (shown 
in the top of figure 4.12). 
The Change rule is fired at siteB (which then becomes the coordinating SubEnv) 
with FILE objects fA, fB and fC from siteA, siteB and SiteC, respectively, 
effectively initiating a three-site Summit. Pre-Summit is carried out by fanning-
out and performing local backward chaining. At siteB, the review rule is pre-

Realization in Oz 
147 
ceded by the SCCS_co rule that checks-out fB; at siteA, review is preceded by 
RCS_co on fA; and at siteC, review is preceded by an analyze activity. Then 
the actual Change activity (Summit rules are annotated in figure 4.12 with S, 
and their enactment relationships with bold lines; in the Oz animator, Summit 
rules are depicted by a special "mountain-summit" icon, to distinguish them 
from local rules) is carried out, followed by forward fan-out post-Summit, which 
in turn produces local chaining only at siteC, to the update~og rule. When 
post-Summit completes, siteB triggers approve, the next Summit rule. Note 
that approve takes MODULE objects as parameters, which are the respective 
parents of the FILE objects bound in Change, so remote derivation of parame-
ters is necessary here. approve produces no local chaining in any SubEnv and 
leads directly to the next Summit rule, modify. (Here again, remote deriva-
tion of parameters seamlessly takes place as modify operates on the children 
of approve's parameters.) When modify completes, the forward fan-out leads 
to the local testing phase, whereby siteB fires inspect followed by auto_test 
rules, siteA performs only auto_test, and siteC does manuaLtest. At the 
end of this local chaining, the final integration_test Summit rule is fired, to 
complete the composite-Summit. 
As presented in Section 3.4, it is possible that at any point during the enactment 
of such a process, some of the local operations do not succeed and a totally 
different execution trace is produced. For example, if any of the local reviews 
fails (i.e., the change is not approved at one site), then the Approve rule should 
lead to a revision session that leads to a second review (such a scenario is shown 
in Chapter 6). And it could also be the case that local or global unanticipated 
exceptions in the process lead to dead-ends from which the process cannot 
proceed. But this again indicates merely that there is a flaw in the process 
model. While system exceptions should be handled by Oz, process exceptions 
and mismatches between the different processes are the responsibility of the 
SubEnv administrators. 
4.4.5 
Transactional Semantics of Summit 
This is an important aspect of the Summit model, which was deferred until now 
mainly due to lack of detailed-enough context in which to discuss it. Clearly, 
some transactional properties are desirable in the execution of Summits. We 
are concerned here mainly with the atomicity property of transactions, which 
can be stated as a grouping of operations such that the outcome of their exe-
cution has all-or-nothing semantics. At the very least, the execution of a single 
Summit rule should be atomic. This by itself requires support for distributed 

148 
CHAPTER 4 
SheA 
SlteB 
SlteC 
mA 
mB 
mC 
fA 
fB 
r------------- r-------------r-------------
I I 
I 
r-;:::::::::::::::::::~I::t::::::~r:SSl_ Change(fB fA rq \ 
I 
I j 
1 
~ 
-ow(fA' ! I Dl,""..,IB' 
D RCS_co(fA)\ 
: 
D SCCS3o(fB) 
I I 
\ I 
: I 
I I 
I I 
\ : 
! I 
: 
I 
I 
\ 
\ 
\ 
I 
\ 
I 
I 
\ 
I 
I 
1 
D npdateJoe(tc) 
Figure 4.12 Execution Trace of Summit Example 

Realization in Oz 
149 
!iii "*"- ftrCIIIIIIIIIIIII: a&Iga fBc4;ii'A(3,o) fq4,Z) -.---------ii\ 
( D·)"~) [ p.~-.::.C"t9) ( s.v~ K9CHd) (Suspend) [ (.)111) Kaep 0 
I 
approva(mB mA mO 
d ~Change(fB fA fC) 
= . 
m 
review(fB) ~review(fA) 
review(fC) 
... 
analyze(fC) 
I 
RCS_co(fA) 
I 
SCCS _ co(fB) 
I 
-IE update _log(fC) 
I 
-IE update Jog(fC) 
I 
DIliE 
", 
F1nal apprOY81 I (YIn) 
~ 
·1 I 
I I 
I 
I 
II 
Figure 4.13a Oz Animation of Summit Example 

150 
CHAPTER 4 
til ~18rfaI:e for command: ClIMgB W(4,l) fA(3,o) fC(4,z) 
~ 
( Done) ( Re-eMecute) C Saye Record) ("";,,:,,,j) ( ()'It) Keep 0 
integratlon_test(mB mA me) 
t approve(mB rnA mC) 
~ 
~ modify(fB fA fC) 
~ manual_test(fC) 
~ auto _ test(fA) 
inspect(fB) 
auto_test(fB) 
integration_test(mB rnA mC) 
= 
[)crr:nJ 
0 
~1 nal approyal I (ytn) 
= 
.. 
• 
.J 
Figure 4.13b Oz Animation of Summit Example 

Realization in Oz 
151 
transactions. The situation becomes more complicated if some degree of atom-
icity is desired during a Summit task including pre- and post-Summits, where 
there is a need to handle simultaneous or overlapping execution of subparts of 
the atomic task at multiple sites. Finally, supporting atomicity of composite 
Summits, i.e., across Summit rules, might also be desired in some cases, and 
requires yet more transactional facilities. 
This section focuses on defining the desired transactional semantics of the Sum-
mit model. The general solution of how to build a transaction mechanism that 
addresses the needs dfs.:ribed here is beyond the scope of this book and is due 
to Heineman [58]. The transactional semantics in Oz are tied to the notion 
of atomicity vs. automation chaining which were inherited from Marvel and 
extended in Oz. Thus, we begin with a summary of this model in Marvel, and 
proceed with the extensions made in Oz. 
Atomicity vs. Automation in Marvel 
Both in Marvel and in Oz, a single rule is always an atomic unit. It is the 
smallest unit for which the atomicity property holds. This, however, does not 
mean that the rule actually executes atomically, only that the outcome of its 
execution is all-or-nothing. In fact, rules with activities never really execute 
atomically, because the server sends the activity for execution at the client and 
switches its context to service other clients (see Section 5.4). 
The interesting issues are with respect to the transactional semantics of chains 
of executing rules. What makes this form of execution interesting from a trans-
action management perspective is that the set of rules being executed is dis-
covered dynamically, and is not known a priori. 
atomicity chains support all-or-nothing execution of a chain of rulesll . In con-
trast, automation chains support tasks of activities that are logically related to 
each other, but do not require atomicity. That is, if a rule is aborted during 
automation chaining, only that rule is rolled-back, not the whole chain of previ-
ously executed rules. Atomicity chaining is usually associated with propagation 
of values that retain some complex consistency constraints in the process (as 
opposed to the simple constraint embodied in each rule's condition) whereas 
the automation chaining supports the execution of long-duration tasks, when 
it is not reasonable to rollback work that was done over large period of time 
(hours, or even days) just because a rule along the chain has aborted. The 
transaction support for this model in Marvel is described in [21, 8, 83]. 
111n some of Marvel publications they are called consistency chains, but this is a misnomer. 

152 
CHAPTER 4 
Recall that the implementation of this model in Marvel was based on annota-
tions made to predicates of rules. Atomicity chaining is realized by ensuring 
that chaining from an atomicity predicate in an asserted effect to rules with 
satisfied conditions and empty activities12 is mandatory; if it fails, the cor-
responding transaction rolls back. In contrast, chaining from an automation 
predicate (or into any rule with a non-empty activity) is optional and can be 
explicitly restricted through no...forward, no_backward or no_chain directives 
on individual automation predicates. If, during execution, automation chaining 
fails, only the updates of the failed rule are rolled-back, without affecting the 
outcome of rules that completed execution previously in that chain. However, 
since the type of chaining is based on attributes, it is possible that a certain 
rule in a chain will trigger both automation and atomicity chains from the 
same or from different attributes. This poses a problem because there could 
be some overlap in the data accessed by the rules in the chain. This means 
that if atomicity "wins", then in cases of roll-back it will erase automation ef-
fects, thus violating the semantics of automation chaining. And if automation 
"wins", atomicity is simply not preserved. 
The solution employed in Marvel is to execute all atomicity chains (also termed 
consistency implications) first, and any automation rules which are encountered 
during the atomicity chaining are queued (first-in-first-out). Once all imme-
diate atomicity chains complete and commit their work, the queued rules are 
inserted into the execution rule stack (initially in the same order in which they 
were queued), and automation chaining commences. Note that automation 
chaining can lead to further atomicity chains, in which case they are again 
executed atomically and queue all encountered automation rules, and so forth. 
One way to look at this form of execution is as a chain of automation rules, 
with occasional "bursts" of atomicity chaining. 
Support for Atomicity and Automation 'tn Oz 
The goal in the design of transaction support for Oz was to preserve the trans-
actional semantics of automation and atomicity as in Marvel, and to properly 
extend them to Summits. Moreover, just as Marvel supported the ability to 
define the granularity of atomicity on a per-task basis using annotations in 
rules, so does Oz allow to define either of the three possible granularities -
a 
single Summit rule, a Summit rule enclosed with pre- and post-Summit rules, 
and composite Summits -
using similar annotations. 
12The restriction to rules without activities is not inherent, it is only a limitation in the 
implementation. 

Realization in Oz 
153 
The execution of a single Summit rule is modeled as a distributed transac-
tion, preserving the atomicity of a rule. This includes support for two-phase 
commit protocol and distributed abort, and involves interaction between local 
transaction and lock managers (the details are in [58]). 
The Summit model introduces few new kinds of chains: 
1. Automation chain from a Summit rule to a local rule ("local" in this con-
text means non-Summit, it does not mean that it necessarily executes 
at the coordinating SubEnv). We will refer to this as Summit-to-Local-
AUtomation chain (SLAU). 
2. Automation chain from a Summit rule to another Summit rule. (Summit-
to-Summit-AUtomation, or SSAU) 
3. Atomicity chain from a Summit rule to a local rule (SLAT). 
4. Atomicity chain from a Summit rule to another Summit rule (SSAT). 
The semantics that are associated with the above new kinds of chains are as 
follows: The first two automation cases, SLAU and SSAU, are handled similarly 
to the local case, i.e., the chained rules are transactionally independent of the 
Summit rule that triggered them. The interesting cases are those that involve 
atomicity, namely SLAT and SSAT. 
Summit-Local-ATomicity (SLAT) 
One way to view SLAT chains is as a direct extension of local atomicity chains, 
and therefore to treat SLAT chains as part of a standard distributed transac-
tion. That is, the global transaction commits only if all local sub-transactions 
commit (using the 2 phase commit protocol), and any local abort leads to a roll-
back of sub-transaction at all sites and the global transaction. However, this 
"standard" approach has a serious flaw with respect to the semantics of Treaties 
and Summits. Since the local rules (to which the Summit rule chains) are not 
necessarily part of any Treaty, thus not explicitly "signed" or even known to 
exist in other sites, their effect on the Summit must be limited (consider, for 
example, a local "malicious" rule that always aborts and therefore causes the 
Summit rule to be rolled back). An alternative approach, therefore, is to make 
the local rules abort-independent from the global Summit transaction. That 
is, if a local rule aborts in the midst of SLAT chaining, the effects of the local 
chain are rolled back, but the state of the Summit transaction and other local 

154 
CHAPTER 4 
sub-transactions remain in general intact. An abort at the Summit transac-
tion, however, still entails local aborts. Moreover, local sub-transactions are 
still commit-dependent -
they cannot commit unless the Summit as a whole 
commits successfully. This model seems to fit well with the semantics of Sum-
mits, but it also introduces a problem: Since a Summit rule made an assertion 
on local data as part of its effect and the local atomicity chaining has aborted, 
the local SubEnv might be regarded as being in an inconsistent state. One solu-
tion to this approach is to follow the nested transaction model [95], and replace 
the failed local transaction by a sub-transaction that commits (e.g., retrying 
the local transaction) in order for the Summit transaction to commit. But this 
approach might hold up the Summit and is complicated to realize. Another 
solution is to roll-back all updates that were made on the local data of the 
aborting site, including the updates of the Summit rule (which are maintained 
locally anyway). This approach retains local process consistency, although it 
might produce global process "inconsistency", in that the effects of a Summit 
rule are completely undone in the aborting site and are completely done in all 
other sites. However, autonomy concerns outweigh global concerns in SLAT, 
and therefore global process consistency is not considered to be preserved under 
SLAT chaining. In order to obtain global process consistency, SSAT chaining 
should be used. 
Summit-Summit-ATomicity (SSAT) 
SSAT has the strongest notion of global atomicity. It indicates that several 
Summit rules, all of which have been Treatified and thus known at all par-
ticipating SubEnvs, are bonded to each other atomically. Note how this is 
different from the SLAT case, where the chained-to local rules are not part of 
a Treaty. The semantics of SSAT chains are that all operations made during 
SSAT chaining are fully atomic. Thus, we can distinguish "local" atomicity 
(this includes pure single-server execution that is unrelated to any Summit) in 
which local transactions are affected by the Summit transaction but cannot af-
fect it, and a stronger "global" atomicity that ties several SubEnvs and ensures 
true atomicity in all involves SubEnvs. 
Order of Execution 
The last issue concerns the order of execution. The ordering between local 
and Summit rules regardless of atomicity/automation concerns was already 
discussed in the context of the "Summit branching" policy in Section 3.7.1. As 
for automation vs. atomicity, arbitrary interleaving of automation and atom-

Realization in Oz 
155 
icity chains across SubEnvs would violate the corresponding semantics, similar 
to the problem in the single-server case which was covered in Section 4.4.5. 
Therefore, the order of rule execution combines both concerns, consisting of an 
atomicity phase followed by an automation phase, where each phase alternates 
between global and local modes following the standard Summit branching pol-
icy. More specifically, when a Summit rule completes, all local atomicity rules 
first execute in the participating SubEnvs, queueing (locally) any local automa-
tion rules encountered. When all SubEnvs complete their local atomicity, the 
next Summit atomicity rule (if any) is fired, followed by all local atomicity, and 
so forth. When the Summit atomicity phase completes, a global commit occurs. 
The next step is to fire all automation chains. Again, Summit automation rules 
fire first, followed by local automation chains, followed by the next Summit au-
tomation interval, and so forth. Just like in the single-server case, any local or 
Summit automation can trigger various local atomicity chains which are exe-
cuted as they are encountered, recursively, and Summit automation chains can 
also trigger atomicity Summits. 
The idea is the same, and applies to both the single-server and the extended 
multi-server models: While in automation mode, any encountered atomicity 
chains (and all of their atomicity implications) are executed immediately, and 
while in atomicity mode, any automation chains encountered are queued for 
later execution. 
As an example, reconsider the enactment of the motivating example from Fig-
ure 4.12. There, the update~og rule was actually spawned off an atomicity 
predicate in the Change rule, to enforce an invariant in siteC that states that 
every change to a source file must be logged (so, if there is no log record the file 
has not changed). This is an example of SLAT chaining. Thus, if update-1og 
aborts (e.g., due to a conflict), siteC rolls-back the updates which were made 
to fC by both update-1og and the Summit change rule. However, the other 
effects of change are not undone. This might lead to a different execution trace 
-
for example, if the condition of the Summit approve rule requires that all 
the relevant source files will be modified -
but it doesn't violate any process 
consistency. In particular, a similar execution trace could occur if the user that 
invoked the change rule simply didn't update fe. 
In the enactment of the motivating example there is no need for real SSAT 
chaining that would bind atomically a set of Summit rules (and all their im-
plications). Indeed, it seems that the decentralized nature of modeling and 
enactment does not lead to many occasions where such modeling is needed. 
One extension might have been to add an atomicity chain from approve to 

156 
CHAPTER 4 
a notify ~anagers Summit rule, so that failure in the latter rule would roll-
back the effects of approve and all other SLAT chains (if any) which fired 
off approve. In that case, all SSAT and SLAT chains would have fire before 
modify which is connected through SSAU chain. 
Local Tailoring of Annotations in Treaty Rules 
This final aspect of atomicity in conjunction with autonomy has to do with local 
control over rule annotations. Recall that MSL supports two kinds of annota-
tions on rules. One kind is the atomicity and automation annotations discussed 
above. The second kind is chaining directives (discussed in Section 2.2.5) -
the annotations that control backward/forward chaining to/from rules. While 
we discuss here only the first kind of annotations, similar arguments, problems, 
and solutions apply equally well to chaining directives. 
The general motivation should be to allow local tailoring of annotations of 
Treaty rules. Since they are an orthogonal dimension to the rules themselves, 
used for specifying chaining among rules, such tailoring does not conceptually 
violate the common sub-process invariant. Actually, such tailoring is important 
for autonomy concerns. The main reason is that a local process should control 
the local impact of a Summit. For example, if a local process imports a Summit 
rule that has an atomicity effect predicate, it might be desirable to "weaken" the 
atomicity of the predicate in the local version (i.e., to turn it into an automation 
predicate) to avoid local implications that are undesirable to that site. Similarly, 
it might be desirable at times to "strengthen" the local process consistency by 
replacing some automation predicates in the local version of a Summit rule with 
atomicity predicates. For example, this would allow to get the desired SLAT 
behavior from change to update~og even if change did not have originally an 
atomicity predicate. 
However, there are some obstacles to that approach: (1) A conceptual problem 
is with respect to SSAT chains. There are really two reasons for annotating 
a Summit rule with an atomicity predicate: to specify connections to other 
Summit rules (SSAT), and to specify connections to local rules (SLAT). While 
the latter should be controlled autonomously, the former is a global constraint 
that is inherently part of the common sub-process. The only solution to this 
problem is to extend the lexicon of the annotations to distinguish between 
global and local atomicity annotations. (2) Technically, the Treaty mechanism 
must be able to distinguish between alterations made to annotations and other 
alterations which constitute violation of the common sub-process invariant. (3) 
Finally, an implication of this requirement is that local SubEnvs would need 

Realization in Oz 
157 
to maintain their own (possibly slightly altered) copies of the original Treaty 
strategies, which would in turn require to address the associated problems that 
do not exist with the "single-source" approach (as explained in Section 4.3). 
Once these technical issues are resolved, the support could and should be added 
to enhance the autonomy of local processes. 
4.5 
SUPPORT FOR DELEGATION AND 
GROUPWARE IN Oz 
Delegation and groupware support were discussed at the generic level in Sec-
tion 3.6, focusing on how they could fit in the Summit model. We discuss here 
general issues regarding modeling and enactment of delegation, not necessarily 
across sites, and the particular realization in Oz. Note, however, that we have 
conducted only preliminary investigation of this subject, which is a major topic 
for future research (see Section 7.2). 
In general, our main interest is neither in inventing actual multi-user tools, nor 
in generic human-computer interaction support. It is specifically about process 
support for modeling and assisting in the interactions between multiple users 
of the PCE. Further, we restrict here the multi-user tools discussion to syn-
chronous tools, i.e., tools that require simultaneous participation of multiple 
users at the same time (or with bounded delay), such as multi-user editors, 
virtual white-boards, and so on. Without discounting the importance of in-
tegrating all kinds of groupware activities and user interactions discussed into 
PCEs, it seems that activities that are synchronous in their nature provide 
more opportunities for enactment support (mainly automation), and thus they 
are the subject of this section. For related work on infrastructure support for 
multi-user asynchronous tools (e.g., "large" tools that are themselves systems, 
like databases) see [127]. 
4.5.1 
Modeling and Enacting Delegation 
The need for delegation is clear: since delegation of tasks is commonplace 
in multi-person organizations, multi-user PCEs should enable users to dele-
gate certain tasks to other users and/or other machines. Moreover, in multi-
site PCEs, local activities that are executed as part of pre- or post-Summits 
must often be operated by local users and/or local machines to preserve auton-
omy /security of their process. Consider, for instance, part of the motivating 

158 
CHAPTER 4 
Local 
Remote 
Type 
1 
Machine 
"Proxy" delegation (non-interactive) 
2 
User 
Machine 
Machine delegation (export display) 
3 
Machine 
User 
User delegation (export display to remote user) 
4 
User, Machine 
Full delegation 
Table 4.1 
Delegation Types 
example: A multi-site change activity is preceded by a local review phase which 
must be performed at each site by the local person who is responsible for the 
document being reviewed. Clearly, the review task has to be delegated to the 
proper reviewer, and run in his/her local machine. At other times the combina-
tion of special resources (e.g., a special-purpose computer) and special users at 
remote sites might require the delegation of an activity. Another case is when 
non-interactive activities are "delegated" to a proxy client13 at a remote site 
that is nonetheless local to its data, thereby saving the overhead of transferring 
the data to the site where the original request was made. Finally, interactive 
activities could still be delegated to a remote machine for any of the above 
reasons with the display being exported to the user. The various user/machine 
delegation combinations are summarized in table 4.1 (Local vs. Remote is with 
respect to the Initiating User). We focus in this section on cases 3 and 4 in the 
table, i.e., on user and full delegation; cases 1 and two have been explored by 
Valetto in [127]. 
Delegation can be ad hoc or process-based. Ad hoc delegation assumes that 
a unit of work has been pre-assigned to a user (or a set of users) and that 
he can manually assign (parts of) it to other user(s). This approach is rela-
tively straightforward, and can be realized "outside the process" by built-in 
commands that allow to transfer tasks across users, provided that there exist 
a persistent mechanism in which to store these requests (e.g., a user agenda); 
such approach was taken by Tong et al. in [126]14, and we do not discuss it 
here any further. In contrast, process-based delegation assumes that delegation 
has been modeled in the process, and therefore the PCE with such instanti-
ated process model can assist in determining which tasks are delegated under 
specific circumstances, and who are the possible delegatees, depending on the 
13 A proxy client mechanism in Oz was implemented by Peter Skopp originally to support 
low-bandwidth clients, but was later generalized for other purposes including delegation; see 
[117] for details. 
14 Tong also describes support for hybrid form of process-based delegation, where the dele-
gation is modeled in the process but the delegated work is still entered into an agenda rather 
than executed immediately by the delegatee. 

Realization in Oz 
159 
current state of the project, the particular artifacts being manipulated, and the 
available users. Moreover, the process engine can assist in setting up the dele-
gation, recover from failures (e.g., refusal to accept the delegation, see below), 
and so on. The main point is that the delegation is captured in the process 
model, and consequently it can be supported in various ways. 
Process-based Support for delegation consists of two aspects: (1) modeling 
the work itself, delegatee(s), tools, and artifacts involved; and (2) supporting 
the enactment of the defined delegation. This includes locating delegatees, 
redirecting tasks to them, notification mechanisms (for both delegators and 
delegatees), and setting up the environment to enable invocation of tools on 
local or remote machines. 
Regarding modeling, there are several issues to explore: (1) what is the gran-
ularity of delegation that the process should support; (2) how to model users 
in general, and how to specify and determine the delegatees; and (3) what are 
the operational semantics of delegation. 
Interestingly, the same set of issues come up in supporting multi-user tools, 
except they require different solutions there, as will be seen in Section 4.5.2. 
Granularity of Delegation 
Granularity of delegation can range from a single activity, or rule, to a com-
plex rule-chain that consists of many related activities, some of which may 
themselves be delegated. Delegation of whole sub-chains may be particularly 
attractive for pre- and post-Summits, where whole local chains could be dele-
gated to local users. However, this subject is yet to be explored (for a partial 
treatment of task delegation see [126]); here we focus on the simpler case of 
activity delegation. 
In modeling delegation of individual activities, the specifications can be made 
within rules, and as such, all the modeling power of Oz rules can be used 
for delegation -
rule bindings can be used for dynamic binding of delegatees, 
provided that users are represented as objects in the objectbase; rule conditions 
could be used to check if delegation is possible; rule effects could be used to 
update information regarding the delegation; and backward chaining could be 
used to automatically search for delegatees by invoking other logically related 
rules that notify inactive delegatees. 

160 
CHAPTER 4 
Essentially, modeling delegation involved the addition of a new delegate opera-
tor that accepts representations of user(s) as operands. The delegate operator 
is defined and evaluated after the binding section and before the condition sec-
tion of a rule, for reasons that will be explained below. At run time, the rule 
processor tries to establish the delegation (explained below) and if the condi-
tion of the rule is satisfied, the (non-empty) activity is redirected to a qualified 
delegatee, provided that there exists one with an active client (otherwise, the 
activity might be stored in an agenda, as suggested earlier.). When the activity 
completes, the rule processor switches back to the delegator, and subsequent 
activities from chained rules are directed to him, unless further delegation op-
erations occur, and so on. 
Modeling and Binding Delegatees 
The main goal in the design of delegation in Oz was to not hard wire the 
delegatees in the instantiated process, but rather to bind them dynamically to 
rules. The main advantage of this approach is that different (sets ~f) users can 
be bound to a rule depending on the context in which the rule is invoked, and 
particularly depending on the specific set of objects that the rule manipulates. 
For example, a review rule could specify that its activity should be delegated 
to the owner of the document; then, depending on the document, each time the 
rule fires, it will be delegated to the appropriate owner. Moreover, if the owner 
of a document changes over time, subsequent invocations of the rule on the 
same document will automatically bind the rule to (one of) the new owner(s), 
because the delegation is specified to the owner(s) of the document, not to a 
specific user. 
Dynamic user binding was achieved in Oz relatively easily, using the normal 
data binding facilities. Delegatees are represented in binding predicates by an 
object. attribute pair, with the attribute restricted to being of type user -
a 
primitive Oz attribute type that accepts as correct values only valid (operating 
system) user-ids. Note that there is no restriction on the kinds of objects that 
the delegate operator accepts. An alternative approach would be to rely on 
some well known user repository. That is, users would be represented by objects 
in the object base and instantiated from a USER class, and the user repository 
could store information that could be used for various purposes, not only for 
delegation binding. This user repository approach is superior in terms of user 
modeling, and also solves the problem of relying on the operating-system's user-
id which might not be unique across domains. In fact, such a user repository 
has already been used (optionally) for access control purposes [83], but has 

Realization in Oz 
1) analyze_bug[?tr:TESTJlUN, ?c:CFILE]: 
2) (and 
3) 
(forall MODULE ?m suchthat (member [?m.cfiles ?c])) 
4) 
(exists WORKSPACE ?w suchthat (linkto [?w.module ?m]))) 
6) 
6) 
delegate[?w.owner]: 
7) (?c. bug..status = Suspected) 
8) 
# Prompt the user to determine whether the bug is here 
9) 
# and if so, produce a change request 
11) 
ANALYZE_TOOLS analyze_cfile_bug ?tr.report 
161 
12) 
?c.change~equest ?c.contents ?c.bug~eport 
13) (and no_chain (?c.bug..status = Defected) 
# bug discovered 
14) 
(?tr. report..status = Confirmed)); 
16) (?c.bug..status = Clean); 
# bug not here 
Figure 4.14 Delegation Example 
not been been applied to delegation yet and is a topic for future work (see 
Section 7.2.3). 
Regarding bindings, it is desirable to be able to specify a set of delegatees (not 
only one) for two reasons: to enable simultaneous delegation of an activity to 
multiple delegatees (which is exactly the mechanism used for multi-user tools), 
and to provide for multiple potential delegatees to select from. Binding to a 
set of users is also achieved "for free" when using the normal binding facilities, 
since they allow to bind a set of objects to a symbol. 
Figure 4.14 shows a rule with a delegation specification (taken from the "ISPW 
example" process, see Chapter 6). The activity involves analysis of a source file 
that is suspected of having a bug, so it is delegated to the person who "owns" 
that file (i.e., the author of the file). This is expressed in the rule by issuing 
a delegate operator (line 6) and binding to it the owner of the WORKSPACE 
(?w) that is linked to the MODULE (line 4) that contains the "suspected" CFILE 
object (line 3). Note how the same rule would be delegated to different users, 
depending on the CFILE object on which the rule was invoked. 

162 
CHAPTER 4 
Semantics of Delegation 
Having established the binding to users still leaves some aspects of the delega-
tion open: 
1. How to determine the single delegatee in the case of a set of candidates 
(the problem of simultaneous delegation to multiple users is addressed 
separately in Section 4.5.2) 
2. how to react to, and recover from, cases where delegation fails. 
The first issue comes up only if there is more than one delegatee. The case 
where there are no candidates at all (i.e., the symbol is bound to the empty 
set) is treated as a failure (see below); another special case is when one of the 
delegatees is also the delegator, in which case the delegation is void, and the 
rule is treated normally. In the general case we identified two methods to direct 
the selection: 
1. random -
choose an arbitrary candidate. A slight alternative is to choose 
the ''first'' candidate in an implementation-specific order; this still al-
lows process engineers to predict the order in which delegation will be 
attempted. 
2. interactive -
allow the user to choose the candidate from the (sub )set 
of users that are active in Oz. 
In either case, the process engine should filter out inactive candidates (Le., users 
with no active clients). In addition, the interactive mode which is designed to 
refine the control over delegation, must have an option to not choose any of the 
available active users in the binding set. Currently, Oz implements only the 
random method, and consequently does not support the explicit specification 
of method choice in the definition of delegation. 
The second issue is the failure semantics for delegation. A delegation operation 
is considered ''failed'' if either there is no active user among the candidate 
set, or none of the available users is willing to perform (that is, immediately) 
the delegated activity (the mechanism to determine the latter is described in 
Section 4.14). Failure is indicated by treating the delegate operator not only 
as a binding but also as a boolean predicate that returns true if delegation 
succeeded, and false if it failed. Thus, as far as the process is concerned, 

Realization in Oz 
163 
delegation failure is equivalent to a failure to satisfy the condition of a rule, 
and thus prevents the process engine from executing the activity of the rule. 
At first glance, one might be inclined to associate no further semantics with fail-
ure beyond the normal rule failure semantics. However, as we discovered, del-
egation failure may require further actions. We identified three, non-mutually 
exclusive actions: 
1. store the delegation in the delegated user's agenda. The delegatee(s) can 
be chosen using similar techniques as above. This action applies to both 
kinds of failure (i.e., no users vs. declining users). 
2. notify the delegated user(s), for instance bye-mail. 
3. delay the delegation and retry later. 
To distinguish between the various options, they should be supplied as either 
environment variables or as PML directives to delegation, and in any case 
optional. 
The "compensating" operations above are limited in the sense that they do not 
make the failed rule satisfiable. There are two ways in which the PCE could 
still attempt to proceed. The first and simple method involves a "program-
ming trick" -
the process engineer could write an alternative rule that would 
be triggered if the delegated rule failed. The idea is to match the conditions 
of both the delegated and the compensating rules and order them so that the 
delegated rule is evaluated first, and if it fails, the non-delegated rule is fired 
alternatively. (Oz allows for ordering multiple rules with different conditions 
whose signatures match the same user command.) Obviously, this option only 
makes sense in cases where there is an alternative path to follow; if the failed 
step is an "articulation point" in the possible "execution graph" (i.e., the pro-
cess must pass through this rule), then nothing can be done. An example where 
this technique is used is given in Chapter 6. 
The second and more natural approach to address delegation failures in a rule-
based PML like Oz is to attempt to satisfy them by backward chaining to 
other rules that could potentially satisfy the delegation operation, analogous 
to normal backward chaining. The fact that delegation is modeled as a boolean 
predicate facilitates this approach, since it could logically match with a "del-
egation" effect of another rule, thereby possibly chaining to it. For example, 
a generic "wakeup" rule could be chained off a rule with failed delegation and 

164 
CHAPTER 4 
activate inactive users by firing an activity that would notify the relevant users 
(if they're logged in at all, of course) of the requested delegation. 
Another useful extension to the PML builds on the fact that delegation is repre-
sented as a predicate in the PML. The delegation construct could be extended 
to support complex logical clauses. For example, a disjunction of delegation 
predicates would allow to bind a set of delegatees from several variables instead 
of one, and would ease the specification of delegation that is otherwise limited 
to a single variable; conjunction of delegation predicates could be used to bind 
users bound to all predicates, and so forth. This and the "chaining" exten-
sions to delegation have not been realized yet, and in general require further 
investigation. 
Infrastructure Support for Delegation 
The above discussion made some implicit assumptions about the capability of 
the system to support several operations: (1) how to locate users specified in 
delegation, and how to identify if they are "active"; (2) how to redirect activities 
across clients; (3) how to notify users (both delegators and delegatees) and in 
general support the interface to delegation. We briefly discuss these issues here. 
A user is considered active if he has at least one client associated with his 
user-id that is connected to the process server from which delegation is issued. 
That client can be either local or remote to the delegated server (see Chapter 5 
for more on the architecture), but it must be connected to it. In this scheme, 
checking whether a user is active is simply done by maintaining in the server 
an internal client-table, and searching clients with the specified user-id. Note 
that with the user attribute method (currently employed in Oz), Unix user 
ids may not be unique across sites, in which case site prefixes could be used to 
provide uniqueness. An extension to this scheme is to consider any client that 
is connected to any server in a multi-site environment as a potential delegatee. 
However, this entails significant overhead in discovering potential candidates, 
and has not been explored yet. 
As for activity redirection, this is mostly a low-level implementation-specific 
issue. There are two important points to mention here. First, the thread 
in the delegator's client that issued the delegation blocks until the delegatee 
completes the execution of the activity; this, however, does not prevent the 
delegator's client from issuing other activities from other threads. Second, when 
the delegatee completes execution of the activity, the return code of the activity 
is redirected as if it came from the delegator client. Thus, most components 

Realization in Oz 
165 
of the rule processor are shielded from this redirection; the rule processor is 
"fooled" and for the most part is not even aware of the delegation. 
The next issue is the general user interface support for delegation. Delegation 
is somewhat unusual in terms of user interface, in the sense that the delegatee is 
asynchronously (and perhaps might unexpectedly be) notified about the activ-
ity. Thus, the user interface must then have a way to (1) attract the delegatee's 
attention to the delegated activity; and (2) enable the delegatee to reject or at 
least defer the execution of the activity, with optional directions to store them 
for later execution, as discussed above. 
The last issue concerns the dynamic visualization of the process; since Oz sup-
ports dynamic animation ofthe executed process, the question is how to redirect 
animation messages. Our approach is to retain all process animation with the 
delegator, and redirect only the activity (and its enclosing user interface) to 
the delegatee(s). The rationale is that in activity delegation the "ownership" is 
only temporarily transferred to the delegatee for the execution of that activity, 
but in general control returns to the delegator when the delegation terminates. 
Note how in this case delegation of coarser granularity (e.g., task delegation) 
would require a different solution, probably to redirect the animation as well 
as the activities to the delegatees. 
4.5.2 
Modeling and Enactment of 
Synchronous Multi-User Tools 
Considering process support for synchronous multi-user tools, there are two 
main issues to explore: 
1. Specification and parameterization of multi-user activities besides user 
binding considerations; and 
2. Execution semantics, including selection of a user-subset. 
Basically, the invocation of a multi-user tool is initiated by the invocation of 
a rule (either manually by a client or through chaining) that encapsulates a 
multi-user activity. An activity is denoted as multi-user in the tool definition 
section of the strategy15(see an example in the change strategy, listed in Ap-
pendix B.2.2). That rule must also contain a user-binding specification for 
15 BOITowed from the language extensions which were made in [127]. 

166 
CHAPTER 4 
binding the participants. If proper binding is made, and the rule's condition 
is satisfied, the activity is invoked. When the activity finishes, the rule pro-
ceeds regularly and continues to be associated with the client that initiated the 
activity. We discuss now bindings and invocation in more detail. 
Semantics of User Binding 
As already mentioned, the same mechanism used for delegation can be also used 
for binding users to a multi-user activity. However, there are several differences 
regarding the policy of an acceptable binding set. 
First, the activity must be delegated to a set of at least two users and the 
initiator mayor may not be treated as an implicit participant (regardless of 
whether he is specified in the binding set). Second, both the random and the 
interacti ve options for user selection must be implemented in a different man-
ner here: the latter has to allow selection of a subset of participants (not just 
one), and the former has to choose a subset of participants. To further assist 
in both the automatic and the interactive binding procedures, the modeling 
of user binding should also be extended to allow the specification of minimum, 
exact, or maximum number of participants required for a certain multi-user 
activity, with the default being all the users in the binding set. 
Invocation of Multi-user Activities 
When a multi-user activity is about to execute (assuming the condition of the 
rule is satisfied and the proper users are bound to the activity), Oz conceptually 
replicates the activities in all participating clients (in practice, if all clients 
share the file system it only needs to send them a path of the envelope to 
execute), and uses a similar user interface as for delegation in order to notify 
the participants of the activity. Thereafter, each client invokes the activity in 
its own address space. However, Oz does not interfere or otherwise support 
the communication at the tool level, which is considered the responsibility of 
the (multi-user) tool itself. In order to "tie" the tools to the process(es), the 
standard tool envelopes [45] can be used to bind information from the process 
to the tools or the new enveloping mechanism for asynchronous tools [127] can 
be employed. 
When the activity completes, all return codes except the return code from 
the initiator clients are ignored. In that respect, multi-user tool support is 
simpler than delegation. An improved implementation should investigate how 

Realization in Oz 
167 
to incorporate return codes from all participants III order to form a single 
representative return code (e.g., majority vote). 
One of the important characteristics of process support for multi-user tools is its 
invocation point. In our approach, it was invoked from a single rule, associated 
with a single client; this is the simplest form. There are at least two alternatives 
which should be investigated: (1) simultaneous invocation ofa Summit activity 
in the participating sites (as opposed to one replicated activity), as outlined in 
Section 3.6; and (2) invocation from multiple independently executing rules -
for example by local concurrently executing chains in post-Summits. 
Finally, another open issue is support for roles. That is, if a tool has the 
notion of different roles (e.g., a multi-user inspection tool with a moderator vs. 
participant, see the change strategy listed in Appendix B.2.2 for an example), 
then the process should be able to model this, and subsequently support it by, 
for example, invoking different envelopes for different roles. Currently, this has 
to be manually coded within the envelopes . 
4.6 
IMPLEMENTATION STATUS 
As of this writing, most of the features which were discussed throughout this 
chapter are fully implemented and supported in version 1.0 of Oz( expected to 
be available for external licensing by 1996)16. To summarize, Oz supports, over 
multiple sites, each with a private process model and a private objectbase, the 
following: 
• 
Treaties -
all strategy-sharing operations (e.g., import) and their intersec-
tion with the execution privileges operations (e.g., request), global Treaty 
operation, local evolutions and dynamic treaty validation. 
• 
Summits -
Direct remote interaction, all built-in cross-site commands, in-
cluding support for sub-schema compatibility as described in Section 4.4.2, 
and most importantly, a full blown support for Summits as described in 
Section 4.4, including distributed transactions that support the transac-
tional semantics as described in Section 4.4.5. 
16We have been using Marvel 3.1.1 to produce Oz, employing a process based on code 
re-engineering and componentization (see [57]), but recently started to use Oz for its own 
further development. 

168 
CHAPTER 4 
• 
Delegation and groupware -
an effective (although still preliminary) im-
plementation of both modeling and enactment of delegation and multi-user 
tools. See Chapter 6 for actual examples of using these mechanisms. 
The aspects which were discussed in this chapter and were only designed or not 
fully implemented at the time of this writing include: global associative queries 
and "soft" cross-object base links are not implemented yet; the export..data 
mechanism discussed in Section 4.3.4 has not been implemented yet, although 
the underlying access-control on top of which this facility should be constructed 
is fully implemented and operational; the solution to the "static problem" in 
common sub-schemas, which was contemplated in Section 4.9 -
involving the 
Evolver's front-end to verify by-structure type equivalence among the common 
sub-schema -
has not been implemented yet, although the "dynamic" problem 
is implemented; checking for process consistency in Treaties using the Evolver 
as described in Section 2.2.6, is not implemented, mainly because some essential 
features in the Evolver itself are still incomplete; backward chaining during fan-
out in pre-Summits is carried out serially, and not in parallel, as suggested in 
Section 4.4.3 (forward chaining during post-Summits is carried out in parallel, 
though); the various language extensions to support "delegation-chaining" and 
the extended logic in evaluating delegation, as well as the delegation directives 
to control the assignment of users and the failure semantics, have not been 
implemented yet. 

5 
ARCHITECTURAL SUPPORT FOR 
DECENTRALIZATION IN Oz 
In the previous chapter we discussed the interpretation of the generic decentral-
ized model into a specific PML and peE; in this chapter we focus on the un-
derlying infrastructure that supports such a realization. One way to distinguish 
the material in this chapter from the previous one is that the previous chapter 
discussed interoperability at the process level, whereas here we discuss mainly 
interconnectivity at the system level, on which interoperability is founded. De-
spite being "low-level", the discussion in this chapter is, for the most part, 
conceptual, focusing on the research issues and ideas that are concerned with 
the design of a decentralized architecture that supports the interoperability 
model and meets the requirements set forth in Section 1.5. 
The chapter is organized as a collection of loosely-coupled issues which are 
concerned with different aspects of the system's characteristics. Section 5.1 
describes an overall architectural overview. Sections 5.2 and 5.3 are closely 
related: the former discusses the communication infrastructure, and the latter 
discusses the process for (dynamic) configuration of the database that maintains 
connectivity information. Sections 5.4 and 5.5 are also somewhat related: the 
former discusses the underlying support for Summits in Oz with emphasis on 
the context switching mechanism and handling communication deadlocks, and 
the latter discusses the remote object cache that enhances the performance of 
Summits. Section 5.6 discusses the extensions to the Oz architecture for oper-
ation over the Internet, supporting geographically dispersed SubEnvs. Finally, 
Section 5.7 summarizes the current state of the implementation. 

170 
CHAPTER 5 
5.1 
ARCHITECTURAL OVERVIEW 
The external view of the Oz architecture matches the generic description as 
seen in figure 3.3, and has already been outlined in Section 4.1. It is a multi-
client-server architecture, whereby each SubEnv follows a conventional client-
server architecture and is essentially self-sufficient for local work, and multi-
ple SubEnvs are inter-connected through a communication layer that enables 
process-interoperability, with no shared-memory. As many aspects ofthe single-
server architecture are similar to the Marvel architecture [21], they are therefore 
not discussed here any further. 
The internal architecture of Oz is illustrated in figure 5.1. We use the following 
graphical lexicon (partially adopted from [78]): squared boxes with the widest 
bold lines (e.g., the Server) represent operating-system processes, or indepen-
dent threads of control; squared boxes with lines with intermediate width (e.g., 
the Task component) represent top-level computational components which are 
part of an operating-system process but are relatively independent from other 
components; squared boxes with narrow solid lines are computational sub-
components; dashed-line separators within sub-components further modularize 
a (sub )component into its various functionalities; shaded ovals represent data 
repositories; and arrows represent data and/or control flow. 
Oz consists of three main runtime computational entities: the Environment 
Server (or simply, the Server), the Connection Server, and the Client!. In 
addition, there are several entities that convert the various project-specific def-
initions into an internal format which is understood and loaded by the server, 
and some objectbase utilities for checking, repairing, and converting (across 
platforms) Oz objectbases. 
There are three kinds of inter-connections: client-to-local-server, client-to-remote-
server, and server-to-server. The first connection is "permanent", in the sense 
that its existence is essential for the operation of the client. That is, a client 
is assumed to always be connected to its local server, and when such connec-
tion gets disconnected (either voluntarily or involuntarily due to some failure) 
the client ceases to exist and is removed from the local server's state. (An 
extension of this model, in which clients can be disconnected temporarily from 
their server, is investigated separately by Skopp [118].) In contrast, the two 
other connections can be regarded as "temporary" , since they are optional, and 
can be dynamically reconnected and disconnected over the course of a session, 
without disrupting the local operation of a SubEnv. This is a necessary feature 
1 There are actually three kinds of clients: XView, Motif, and a command-line client. 

"!I 
~. = 
.. 
to 
CI1 
;.. 
o 
IS 
> .. 
e: 
~ 
/D n [ 
In-memory 
L ____ ~~~~--
Object Mgmt.! 
Local 
I 
Remote 
------~--------I----
File 
I 
Storage 
I 
Remote 
Manager 
I 
Manager 
Legend: 
• 
- Data repository 
o -Compwtational 
component 
o 
~ 
I::::l 
(b 
("') 
(b 
;;:! 
.,.... 
~ -
~. 
(b 
~ 
~ 
~ 
;;:,--
'"". 
.,.... 
(b 
("') 
.,.... 
~ 
;i 
...... 
-:r 
...... 

172 
CHAPTER 5 
to fulfill the independent-operation requirement, particularly when the servers 
are spread arbitrarily over multiple domains. 
An Oz (multi-site) environment consists of a set of instantiated SubEnvs, and 
at any point in time none, some, or all SubEnvs may be active. A SubEnv 
is considered active if exactly one server is executing "on the environment", 
meaning that it has loaded the SubEnv's process, and the SubEnv's object base 
(containing the persistent product data and process state) is under the control 
of the server's object management system. Typically, an active environment has 
also at least one local active (i.e., executing) client connected to its local server, 
because the server automatically shuts itself down when there are no more 
active clients (and is automatically started up on demand by the Connection 
Server, as will be explained shortly). In the rest of this section we will interpret 
the architecture figure, so the reader is advised to refer to it throughout the 
section. 
5.1.1 
The Oz Environment Server 
The server is the "brain" of Oz. It consists of three main distinct compo-
nents: Task (or process), transaction, and data managers, each of which can 
be separately tailored externally. 
Task Manager 
The task manager is the main component in the server. Its front-end sub-
component is the scheduler. The scheduler receives requests for service from 
three entities that correspond to the previously mentioned inter-connections, 
namely local clients, remote clients, and remote servers. With few exceptions, 
these requests are served on a first-come-first-served basis (the exceptions are 
explained in Section 5.4). The server is non-preemptive, i.e., it relinquishes 
control and context-switches to other tasks only voluntarily. The next layer 
below the scheduler is the session, or context, layer. Each interaction with a 
server is enclosed within a context containing information that enables to switch 
and restore contexts (again, see Section 5.4). The most common case of a multi-
step task in Oz is a chain of rules. The context of a local rule-chain is kept in 
a data structure called the rule-stack and the context of composite Summits 
is maintained in a Summit-stack (mentioned earlier in Section 4.4.3). Most 
of the services provided by the server are handled either by the rule processor 
or by the built-in command processor. 

Oz Decentralized Architecture 
173 
The rule processor is the central element of the task manager. It contains 
the necessary functionality for processing both Summit and non-Summit rules, 
including parameter binding and rule overloading, activity execution prepa-
rations (including converting the object-based arguments to their file-based 
counterparts used by external tools), and backward and forward chaining with 
either direction in one of three possible modes: (pure) local, Summit, and local 
but spawned off a Summit rule. In addition, it controls the bindings, condition 
evaluation, and assertion in rules (all preformed by the query processor) and 
all access to remote objects in either of the above phases (performed by the 
data and transaction managers). 
The rule processor has very few "system" built-in rules (e.g., registration rules, 
see Section 5.3.1), so the behavior of a particular instantiated SubEnv is mostly 
determined by the external rule-base repository that it reads upon initialization. 
The rule-base contains the internal representation of the parsed administrator-
defined rules, their inter-connections (i.e., the rule-network), their interface to 
envelopes, and all the Treaty information -
imported, exported, requested, and 
accepted rules, as well as the corresponding sites with which those relationships 
hold. 
The built-in command processor handles all the hardwired kernel services 
which are available to every SubEnv. These include the primitive structural op-
erations on the objectbase (e.g., add and copy object), image refresh commands 
(explained later on), access-control, ad-hoc queries, and the various dynamic 
process loading and Treaty operations. 
Transaction Manager 
All access to data is mediated in Oz by the transaction manager. Due to 
the required decentralization, each transaction manager is inherently local, i.e., 
it is responsible only for its local database, and interacts with remote transac-
tion managers to manage access to remote objects. Thus, only local locks are 
maintained at each local transaction manager. The transaction manager can 
be configured by one or more of the following mechanisms: (I) an external lock 
table, containing compatibility matrix, power matrix, and inheritance tables; 
(2) a transaction table that associates lock modes (from the lock table) with 
operations which are carried out during process execution; and (3) a set of 
control-rules that tailor the default two phase locking protocol for concurrency 
control (see [21] for (1) and (2). (3) is due to Barghouti [8]). As with rules, 
an instance of the transaction manager without proper lock and transaction 

174 
CHAPTER 5 
tables is useless, but unlike rules, Oz provides default tables which are suitable 
in most cases (the control-rule base is entirely optional). 
As outlined in Chapter 1, the implementation of this component is in general 
outside the scope of this work and is treated separately by Heineman [54]. 
Data Manager 
This is the lowermost component in the server. Its main sub-component is 
the in-memory object manager that provides a uniform object-based access 
to data from any system component. Objects can be looked up in one of 
three ways: by structural navigation, by testing class membership, and by 
their object-id. Thus, three different data structures are superimposed on the 
objectbase: a directed graph that represents the structure of the object base 
with edges labeled as parent, child, or link; a linked-list that contains all objects 
of a given class; and a hash-table keyed by the object-id. Structural and by-
class searches are requested by the query processor to service navigational and 
associative queries, respectively, and by-id lookup is used for several purposes, 
among them to support direct user selection of objects as parameters to rules. 
The second major sub-component is the query processor. It has a language 
interface, and is called from both the rule processor and directly from the client 
for servicing ad-hoc queries. Queries on remote objects are handled at this level, 
by invoking a server-to-server service. 
The rest of data management consists of an untyped storage manager (imple-
mented on top of the gdbm package) that stores the objectbase contents; a file 
manager that manages access to file attributes (recall that file attributes in 
objects are merely paths to files which reside in the "hidden" file system); and 
an object cache that holds transient copies of remote objects when Summits 
take place -
discussed separately in Section 5.5. 
As far as modeling facilities, the data manager is defined by the project-specific 
schema which is tied to the instantiated objectbase, including both class- and 
composition-hierarchies. As in the case of rules, without a schema the data 
manager is useless since it cannot instantiate any objects (the built-in classes 
SUB_ENV, TOOL, and ENTITY are not suitable for general use). 

Oz Decentralized Architecture 
175 
Componentization 
Finally, one of the major research topics in the Oz project that is for the most 
part orthogonal to the work described in this book is concerned with compo-
nentizing the server's main three functionalities (task, transaction, and data 
management) into independent and replaceable components, as well as inte-
grating each component in other frameworks. Componentization, among other 
things, is important to support architectural design autonomy, where SubEnvs 
can be built with different components (e.g., different OMS, or different trans-
action management). While preliminary work towards componentization has 
been done by the author in the Marvel project (see [21]), it is in general out-
side the scope of this book and is addressed in part by Popovich [104] and 
Heineman [54]. 
5.1.2 
The Oz Client 
The client consists of four major sub-components: (1) interface to, and infor-
mation about, rules and built-in commands, (2) object base display, (3) activity 
execution module, and (4) an ad-hoc query interface. Oz clients are multi-
threaded, in that a single client supports multiple concurrent interactions with 
local or remote servers. This enables a user to run in parallel several (possibly 
long) activities from the same client. 
The command interface consists ofrule- and built-in menus, utilities for display-
ing rules and the (local) rule-network, all of which are stored at the client's ad-
dress space and can be dynamically refreshed when a new process is (re)loaded. 
Another informative utility is the Treaty information menu, which prints the 
state of the various active Treaties, import, export, request, and accept in-
formation, as it is known to the local site. But recall that Treaties can be 
invalidated unilaterally, which means that the Treaty information regarding 
remote sites represents only an approximation of their current state. 
In addition to the display of the static rule-network, the client has a dynamic 
rule animator that animates the enactment of rules, including backward and 
forward chaining. In Oz, the animator has been extended to support animation 
of Summits. The idea is that when local execution of rules at the remote sites 
takes place, the animation of these rules is still directed towards the client who 
initiated the Summit, so that a complete picture of the Summit is presented 
at the coordinating client. This occurs even when an activity is delegated to 
other clients. There are several architectural implications to this design: (1) 

176 
CHAPTER 5 
there must be a complete separation between control and animation messages 
which are sent from a server to client(s); (2) when a rule executes as part of 
remote backward or forward chaining, (i.e., during pre-Summit or post-Summit, 
respectively), it must carry with it the identification of the coordinating client 
(which operates in a different SubEnv) in order to direct the animation messages 
to it; and (3) the remote server on whose behalf pre- or post-Summit takes place 
must be able to communicate with the coordinating client to direct to it the 
proper animation messages. 
The objectbase display is the central component of the user interface, par-
ticularly with respect to multi-site interactions. In Oz, the client supports the 
display, browsing, and parameter selection from both local and remote object-
bases, as was seen earlier in figure 4.1b. (But recall that the client maintains 
an image of only the structural information for browsing and selection, not the 
full contents of the objectbase(s). Actually, this is also true for schema and 
rules: only their "names" are passed to clients for selection and display pur-
poses, not their contents.) This implies that the client has to maintain multiple 
simultaneous connections to the remote servers, and be able to direct different 
requests to different servers. In addition, decentralization concerns imply that 
the policy concerning the refresh of the various images should be determined 
on a per-SubEnv basis, and not be global, since the desired refresh policy for 
the objectbase image may vary depending on the degree of remoteness from, 
and frequency of interactions with, non-local servers. Thus, Oz supports a 
SubEnv-specific tailorable re/resh-policy2. That is, a user can determine for 
each object base the frequency for refreshing the local image, thereby control-
ling the communication overhead. The policy itself can be based on time, or 
on number of updates made to the objectbase (by other clients, since a client 
that affects the state of the object base receives always the updated image im-
mediately). The default policy, as with other aspects of communication in Oz, 
follows the "lazy" approach -
the updates are deferred until users actively 
request services from the server, after which the updates are piggy-backed to 
the reply. Figure 5.2 shows the client interface to the refresh policy, where the 
Periodic-Delta mode is selected on SubEnv siteA with value 5, meaning that 
after each 5 updates that are made to the structure of the objectbase of siteA 
(e.g., adding or deleting objects) the changes are propagated to the client. The 
threshold value denotes the number of changes after which instead of sending 
the "delta" the entire objectbase should be sent. This is usually a function of 
the size of the objectbase. 
2This feature did not exist in Marvel even for the single-SubEnv; instead, the refresh 
policy was hard-coded in the kernel. 

Oz Decentralized Architecture 
177 
II :: s.t:: 
~ 
,c oono) 
"dive !if 
I 
Server: slteA 
print _lbUllls: 
~ 
I 
print back links: 
~ 
print links: 
~ 
verbose: 
~ 
updatB policv: 
Oe1ta:5_ 1 -:-0, 
25 
, , , 
Threshokl ~ 
1 ~ 
100 
Figure 5.2 Refresh Policy in Oz 
The two remaining components of the client, namely activity execution and an 
ad-hoc query interface, are similar to their counterparts in Marvel, and even 
when remote objects are accessed in the activities, their transfer is transparent 
to these components. 
5.1.3 
Connection Server 
The Connection Server's main responsibility is to (re)establish connections to 
a local server from local clients, remote clients, and remote servers. However, it 
does not participate in the actual interactions between those entities; it serves 
only as a mediator for "hand-shaking" purposes. In some cases, the destination 
server to which a request for a connection is made, may not be active, in which 
case the Connection Server is capable of automatically (re)activating a dormant 
server. In other cases the desired server may be active but its address (host IP 
address and port number) might be unknown to the requesting entity, in which 
case the Connection Server sends that information to the requesting entity for 
further communication. 

178 
CHAPTER 5 
Unlike the environment Server, the Connection Server is always active3 . Thus, 
each configured host has its own (logical) Connection Server that supports all 
SubEnvs (of the same or different global environments) that reside in that host. 
The actual invocation and functionality of Connection Servers is discussed in 
Section 5.2.3. 
5.1.4 
Summit from the Architecture 
Standpoint 
We summarize the section by an overview of Summit execution, describing 
how the system's components interact during the course of a multi-SubEnv 
enactment, leaving out the details of two important topics which are discussed 
later separately, namely context-switching and caching of remote objects. 
A user interacts with the environment through a client. He/she initiates a 
Summit by selecting a rule from the rule-menu, and by selecting objects for 
the rule's parameters, with at least one object being remote. (recall that only 
object-ids are known to the client, the real objects are resolved at the server). 
Prerequisites to such firing are: (1) The rule must have gone through a Treaty 
definition, and must have request privileges on the sites from which remote 
objects were selected; (2) An open-remote command was issued by the client 
on all remote object bases from which objects were selected, or otherwise the 
remote objects would have been invisible to the invoking client. 
The request is then sent to the local server (the details of the actual com-
munication are deferred to Section 5.2) and enqueued for execution. At some 
point the scheduler dequeues the request for service and directs it to the session 
manager which creates a new context for the requested task. From there, the 
rule processor takes control. The first operation involves resolving the object-
ids to real objects. The local objects are resolved through search-by-id, and 
the remote objects are fetched by invoking a server-to-server request, and are 
stored in the local cache. Once the objects are resolved, the overloading mod-
ule determines which rule(s) should be invoked, followed by a server-to-server 
interaction to perform dynamic Treaty validation. The binding phase follows, 
calling the query processor, which in turn might submit queries to remote query 
processors. At this point, the rule processor calls the transaction manager to 
acquire locks on the binding set, and requests for locks on remote objects are di-
rected to the proper (remote) transaction managers. The condition evaluation 
3 Actually, it is implemented as a daemon invokable from the Unix inetd mechanism. 

Oz Decentralized Architecture 
179 
follows, and if it fails, backward chaining is invoked, involving server-to-server 
interaction to notify remote servers to perform local chaining. If/when the 
server determines that the client has to execute an activity, it sends to the 
client the necessary data and animation messages. Then, the client's activity 
manager spawns an operating system process that executes the activity; when 
finished, the client returns to the server with the return code and output from 
the activity. The server's rule processor enters the post-Summit phase, initiates 
remote forward chaining, and when all sites notify completion it proceeds with 
subsequent Summit rules, if any, and eventually completes the task, releasing 
all the resources associated with the task and removing it from the session 
manager. 
5.2 
COMMUNICATION 
INFRASTRUCTURE 
The communication infrastructure is the cornerstone of the inter-connectivity 
mechanism and is, therefore, important for the understanding of the decentral-
ized architecture. 
We address here two main issues: 
1. How to represent, store, identify and locate, computational entities (i.e., 
clients and servers) across SubEnvs. 
2. How to perform the actual transfer of data and control between those 
entities. 
The core research requirements impose several constraints on the design of the 
infrastructure: 
1. Decentralization and independent operation requirements (which in turn 
entail a "shared nothing" architecture) imply that the communication in-
formation cannot reside in a shared repository and must be therefore some-
how replicated. 
2. Independent operation coupled with the fact that SubEnvs mayor may not 
be active at certain points in time, imply that the architecture should be 

180 
CHAPTER 5 
designed to tolerate temporary disconnections between SubEnvs as a nor-
mal scenario, not only as an exception. Moreover, since the communication 
address of the entities might change dynamically (due to the "temporary" 
nature of these connections), the communication protocol should be able 
to dynamically (re)locate and (re)connect to remote sites, while carrying 
out other on-going tasks. 
3. The flexibility requirement suggests that there should be some degree of 
freedom in modeling the communication on a per-project basis. 
We begin with a high-level outline ofthe approach taken to address these issues, 
followed by the actual realization. 
5.2.1 
Approach 
The key to addressing the two major issues given the above requirements is in 
the proper design of: (1) a decentralized connection database and (2) a proper 
communication protocol that manipulates the database. 
The connection database is a persistent repository that contains the necessary 
information for cross-SubEnv communication. The shared-nothing requirement 
eliminates the possibility of a shared repository, so the obvious alternative 
is to replicate it in all sites. However, maintaining consistent replicas at all 
sites violates autonomy and independent operation, particularly due to the 
dynamic changes that occur frequently whenever sites are (de)activated. And 
with arbitrary geographical distribution of SubEnvs, this approach becomes 
simply impractical. On the other hand, despite the given lack of consistent 
replication, there must be a way to still ensure inter-SubEnv connectivity on 
demand. 
A hybrid approach that addresses both concerns is to maintain a semi-replicated 
database, whereby the database consists of two kinds of data: a static compo-
nent that contains connectivity information that changes rarely, is fully repli-
cated, and thus assumed to always be valid; and a dynamic component that con-
tains information that changes frequently, is not always replicated, and might 
be at times invalid. Corresponding to that division, there are two modes of 
communication: direct communication through the volatile dynamic informa-
tion, and indirect communication through the always valid static information. 
The former mode is faster, but will not work if the dynamic information is 

Oz Decentralized Architecture 
181 
invalid, and the latter is slower but the connectivity information is guaranteed 
to be accurate (this will be further clarified later in Section 5.2.3). 
As for flexibility concerns, the obvious direction to follow is to exploit the 
process-centered approach and provide facilities and notations for (1) modeling 
communication on a per-project basis, and for (2) the corresponding enactment 
mechanisms. However, communication modeling imposes problems that do not 
exist in software process modeling. First, since communication is primarily 
concerned with inter-SubEnv interactions, tailoring can be made only on a 
global environment basis (as opposed to within a single SubEnv), which means 
that communication modeling is at least partially a global modeling procedure. 
Second, communication involves low-level system calls and mechanisms that 
are hard to expose to the high-level modeling language. 
The solution here is a compromise: the connection database is modeled as 
a set of first-class instances of a class that is defined using the standard Data 
Definition Language, but the class is built-in. And manipulation ofthe database 
is performed in part by low-level components ofthe kernel, and in part by (built-
in) rules. The idea is to define a built-in structure of the database, but expose it 
and its contents to all levels of the system (and to users), and in particular make 
it modifiable via the PML, as well as from the kernel. Thus, manipulation of 
(parts) of the connection database is performed through a built-in process, and 
has the benefits that come with process modeling and enactment in general, 
although with some limitations (see Section 5.3). And even the class definition 
of the connection database can be augmented with additional attributes, so 
long as the default required attributes are intact. We now turn to the actual 
solution employed in Oz. 
5.2.2 
The Oz Connection Database 
The implementation of the connection database in Oz follows the rationale 
given above. Each SubEnv maintains a private connection database consisting 
of a set of objects of the built-in class SUB..EIV, each of which represents a 
distinct SubEnv in the global environment. The SubEnv objects are represented 
as the root objects of their respective object bases, and thus they are always 
part of the displayed image at all clients at all sites of the global environment. 
The actual definition of the SUB..EIV class is given in figure 5.3. The static 
attributes contain information which is determined at site configuration time, 
and is modified only by subsequent configurations (see Section 5.3). It contains 

182 
CHAPTER 5 
SUB..EIV :: superclass ENTITY; 
# Static Information 
env...name 
environments 
env.id 
environments 
subenv.id 
environment 
subenv...name 
name 
string; 
integer; 
integer; 
string; 
string; 
# unique across global 
# unique across global 
# unique within a global 
# site:pathname or logical 
# e.g.: 
cs.columbia.edu 
site...name 
site.ip...a.ddr 
has...nfs 
string; 
# e.g.: 
128.59.16.20 
boolean = false; # true if shares NFS with 
local server 
state 
(New, Initialized, Defunct) = New; # 
configuration state 
local 
stub object 
boolean; 
# Dynamic information 
activeJ10st 
string; 
bleecker. columbia. edu 
host.ip...addr 
port 
active 
guaranteed 
string; 
integer = 
boolean = 
# TRUE if local, FALSE if 
# e.g.: 
# e.g. 
128.59.24.34 
0; 
false; 
# port number, if active 
# TRUE if active, not 
subenv..ob 
connected here 
set..of ENTITY; 
# The local objectbase is 
# Project Specific 
end 
Figure 5.3 The built-in class SUKENV 

Oz Decentralized Architecture 
183 
values that enable to always locate the SubEnv and connect to it (through the 
Connection Server), like the subenv..name and subenv -id fields for identifying 
the SubEnv, and the site..name and site-ip....addr which specify the location 
of the Connection Server. Note that the value of the site..name attribute need 
not be identical to the value of active.host, due to the fact that a Connection 
Server can activate other hosts within its domain. This point is discussed later 
in Section 5.2.3. 
Unlike the static attributes, the dynamic attributes are frequently modified by 
the Oz kernel during normal (inter-) process enactment, and contain dynamic 
bindings of values (e.g., current Internet address of the host that executes on 
the SubEnv, its listening port, etc.). In each local connection database there 
is exactly one local SUB...EIV object (denoted by having a true value in its 
boolean local attribute), to which the local objectbase is connected (through 
the subenv...Db compositional attribute). The rest of the SUB...EIlV objects are 
"stubs" which are used to connect to other SubEnvs. For example, in an 
environment consisting of four SubEnvs, each SubEnv will have four distinct 
SubEnv objects (i.e., the total number of SubEnv objects in an environment is 
the square of the number of SubEnvs), one of which is the local "real" object 
and the other three are stubs "pointing" to the other SubEnvs. By definition, 
all stubs that point to the same object (one in each SubEnv) must contain 
identical static information -
this is guaranteed by the configuration process. 
In contrast, the dynamic information may vary in different stubs representing 
the same SubEnv object. The reason is that a stub in the server is updated only 
when the server (or one of its local clients) actively requests to communicate 
with other server represented by the stub. That is, the stub is not updated 
every time the corresponding real SubEnv object is modified (e.g., when it 
becomes inactive, or is reactivated on a different host). Thus, the dynamic 
information is always valid only in the real (i.e., non-stub) SubEnv object. 
As for the client, the situation is as follows. Upon initialization, it receives from 
its local server an image of the local object base, and an image of the connection 
database (see, for example, figure 4.1a). When the client issues the (built-in) 
open-remote command on a remote SubEnv stub (shown in figure 4.1b), the 
client switches the image of the stub with the image of the (remote) real object, 
along with its connected objectbase, and the local server's stub is updated 
with the proper dynamic information. This switch of images at the client is 
best illustrated in figures 5.4a and 5.4b: In 5.4a the client has no open remote 
connections so its image of the connection database is directly mapped to the 
local connection database; and 5.4b shows client's image after an open-remote 
was issued on SubEnv3 (ignore for the moment the Connection Server in the 

184 
CHAPTER 5 
alent 
SubEnvl 
SubEnv2 
SubEnv3 
DBI 
Figure 5.4a Connection Database 
figure). There, the image for SubEnv3 has switched from the local stub to the 
image of the real object (along with its connected objectbase). 
An alternative approach to maintaining the connection database at the client 
(which was in fact implemented in an earlier version of Oz) would not switch 
the image of the SubEnv objects upon opening a remote connection. Instead, 
a distinguished attribute of the stub would represent the sub-objectbase image 
stemming from the actual SubEnv object, and any requests to access remote 
objects would be directed to the local server, which would perform the request 
on behalf of its client, including possibly contacting the Connection Server. The 
main advantage of this approach over the former one is that it simplifies the 
client's operation and the communication protocols in general since the client 
communicates only with its local server and all cross site communication is 
done through the servers. However, this approach unnecessarily overloads the 
servers and overall increases significantly the performance overhead for remote 
communication, since every remote request must pass through the local server, 
including the built-in operations that do not require process-authorization such 
as parameter selection and remote browsing (note that access control can still 
protect sites from unauthorized remote access just as it does so for local clients). 

Oz Decentralized Architecture 
185 
alent 
lcduCCl 
--
----------
SubEnvl 
SubEnv2 
SubEnv3 
DB2 
Figure S.4h Connection Database with A Remote Connection 
5.2.3 
The Communication Protocol 
As mentioned earlier, Oz supports two modes of communication: the direct 
communication mode which uses the (possibly invalid) dynamic information at 
the connection database to connect directly to the desired server, and the indi-
rect mode which uses the static and always valid information in the connection 
database to connect through the Connection Server. Indirect communication 
is used either to establish a new connection for which there is no dynamic 
information available at the requesting server, or when the dynamic informa-
tion turns out to be out of date (e.g., due to the fact that the target server 
terminated its execution). 
In either case, indirect communication is followed by updating the correspond-
ing dynamic information in the stub, so that subsequent interactions with the 
same server can occur in direct mode. In some cases, there is no running server 
on a given SubEnv, which means that indirect communication must take place. 
In this case, the activation capabilities of the Connection Server are used to 
start up a new server. 

186 
CHAPTER 5 
Figure 5.4b illustrates the two modes in client-to-remote-server interactions (it 
is similarly handled in server-server communication). As long as the direct 
channel is valid, all interaction between the client and the remote SubEnv3 is 
done directly. If the SubEnv3's address is not known to the client, or has become 
invalid (e.g., the server has been deactivated), the indirect channel (shown as 
the dashed arrows) is used to establish the connection, after which the (new) 
direct channel is used again. Since the address of the Connection Server at 
SubEnv3 is always known (maintained by the static information) and it is al-
ways available (through the daemon mechanism), the likelihood of successfully 
(re)connecting is very high (assuming network connectivity). Finally, the in-
direct communication has an important role for fault tolerance: it is essential 
for handling inter- and intra- site failures independent of Oz. For example, 
if a specific host which used to run on a SubEnv crashes, subsequent commu-
nication with the Connection Server might lead to restarting a server on the 
SubEnv from the same or a different host in the domain. The communication 
protocol is summarized in figure 5.5. Note how all the necessary information 
can be obtained from the SubEnv objects in the local connection database. 
The design of the communication protocol meets the constraints imposed by in-
dependent operation and decentralization requirements, and is somewhat anal-
ogous to other aspects of the system that deal with interoperability. On one 
hand, the lazy approach to updating dynamically changing information avoids 
the need to broadcast the updates made in the (real) SubEnv objects to all 
the stubs in the remote SubEnvs. This is particularly important since the sites 
might be physically dispersed and thus incur large communication overhead. 
Moreover, the fact that not all sites are necessarily active at all times simply 
makes the "eager" approach impossible. And most of all, such updates are not 
always necessary (e.g., when some remote SubEnvs are not interacting with the 
updated SubEnv). On the other hand, it is still always possible to reach re-
mote SubEnvs (so long as they are reachable through the underlying network), 
with some overhead. The main point is that the "freshness" of the dynamic 
information is correlated with the frequency of communication, i.e., the more 
often a remote SubEnv is contacted, the more likely the dynamic information 
in the corresponding SubEnv stub will be accurate at the contacting SubEnv, 
thereby increasing the chances for successful direct communication. 
Remote Invocation of Environment Servers 
We hinted earlier on the possibility to enable a Connection Server invoked in 
one host to spawn an Environment Server on a different host within the same 

Oz Decentralized Architecture 
if (remote-server is marked as active) 
then 
try to connect directly using the dynamic information 
if (connection established) 
then 
communicate 
end if 
end if 
if ((remote-server is marked as NotActive) OR 
(dynamic information is invalid, communication failed)) 
then 
187 
contact the Connection Server through the static information 
if (connection established) 
then 
else 
1. get the (dynamic) information from the Connection 
Server and update the local stub SubEnv object 
2. communicate 
return error. Connection cannot be made at this point 
end if 
end if 
Figure 5.5 Server-to-server Communica.tion Protocol 

188 
CHAPTER 5 
domain4• This feature decouples the SubEnv's static "contact" host from the 
actual host in which the Environment Server executes, thereby allowing to de-
termine the execution node dynamically. There are several benefits to this 
design. First, the execution node can be determined based on various con-
siderations such as proximity to the SubEnv's data, load balancing, and fault 
tolerance issues. Second, if logical SubEnv names are supported (as explained 
below), SubEnvs can be migrated without affecting the Connection database, 
and in this case the Connection Server can choose the proper host based on its 
local site information. 
Thus, the Connection Server maintains information on, and can be configured 
to operate based on, local domain considerations, which are shielded from En-
vironment Servers and Clients that operate over a logical "site-based" name 
space (see below). 
Typically, the SubEnv's static address (as denoted in the site..name attribute 
of the SUB...EIV objects) should be that of the host in which the SubEnv's 
data physically resides, and that local host is then the default host for spawn-
ing the Environment Server. However, in some cases it is desired to have a 
static address of a "public" node in the domain, in which case the Connection 
Server should spawn an Environment Server on the "private" node in which the 
SubEnv physically resides. This scenario is typical in ''firewalled'' sites, where 
only a single node (the ''firewall'') can communicate with the outside world. 
To facilitate this feature, the Connection Server maintains a Domain SubEnv 
Table that contains invocation information for each SubEnv in its domain. 
This table is modified only at site configuration time (covered in 5.3) and is 
used only by the Connection Server. The main two pieces of information stored 
there are: (1) mapping of the logical name of a SubEnv to a physical host and 
path in which the SubEnv repositories reside; and A priority list of hosts in the 
domain in which to invoke Environment Servers on the SubEnv. 
5.2.4 
Decentralized Naming Schemes 
The obvious difficulty in decentralized naming is ensuring uniqueness and proper 
identification and location of elements without a centralized repository or global 
control. Further, autonomy considerations should lead us to strive to enable 
SubEnvs to handle their own naming without depending in any way on other 
"Recall that we use domain to explicitly denote a physical Internet domain and assume a 
shared files system, unlike our use of the term site, which mayor may not map to a domain. 

Oz Decentralized Architecture 
189 
SubEnvs. And as with other naming schemes, there might be a need to provide 
logical names which are mapped to internal physical ids. Oz employs separate 
name spaces for SubEnvs, object-ids, rule-ids, and client-ids. 
SubEnv Name Space 
SubEnv naming is the central naming scheme in Oz. It is used for both inter-
site communication purposes and as a basis for guaranteeing uniqueness in all 
other name spaces in Oz. Most of the necessary information resides in the 
connection database. 
One possible approach to naming, termed here site-based, is to bind each 
SubEnv with an Internet host as part of its identification. The main advantage 
of this approach is that it eliminates the need for an Oz-specific name space 
for SubEnvs, and in order to distinguish between multiple SubEnvs within the 
same site, the file system could be used. However, this approach implies a 
"hard" binding of a SubEnv to a specific site, which may not be desirable. Our 
motivation is to enable SubEnvs to migrate across hosts with minimum con-
figuration overhead, as well as to enable different hosts (in the same domain) 
to execute on a given SubEnv. regardless of the host that holds their data, as 
explained in Section 5.2.3. The approach in Oz is to maintain a logical nam-
ing scheme independent of the underlying physical hosts, thereby enabling to 
move SubEnvs across hosts/file system while retaining their same unique id, 
and invoke servers from different hosts on SubEnvs regardless of their physical 
location. Thus, in this SubEnv-based approach, the network address is only a 
regular attribute of the SubEnv object used for location purposes, but is not 
used as part of its identification. In particular, its value can change if the 
SubEnv migrates. 
However, we are still faced with the problem of ensuring uniqueness. One 
approach might be to reuse the native object-ids of the SubEnv objects as 
the SubEnv-ids. But this approach would require global control in assigning 
object-ids (or at least stub objects) to ensure uniqueness, which is far from 
desired for autonomy concerns, particularly given that assigning object-ids is a 
frequent operation (see Section 5.2.4). 
The preferred solution is then to base the naming on some sort of consensus. 
The actual naming scheme in Oz is as follows. When the registration pro-
cess (discussed in Section 5.3) initializes a new SubEnv in an environment, it 
queries all existing SubEnv objects (by looking at the connection database) and 
assigns a number that differs from all others. This SubEnv-id is guaranteed 

190 
CHAPTER 5 
to be unique within the (global) environment (although not across global envi-
ronments). And as part of the static attributes of the SubEnv object, all stubs 
pointing to the same object get the same SubEnv-ids although they mayor may 
not have the same object-ids, which are determined autonomously. Joining a 
pre-existing SubEnv (as opposed to registering a new one) might require then 
to reassign the SubEnv-id, but not the object-ids. 
In order to actually communicate across SubEnvs, their physical network ad-
dress is extracted from the proper SubEnv stub object, consisting of the file 
system path name of the environment directory that contains the SubEnv's per-
sistent repositories (Le., objectbase, rule-base, etc.), coupled with the Internet 
address of the host that controls that repository and the port number. 
Finally, in order to enable movement of SubEnvs across global environments, 
a global assignment is in general unavoidable. This functionality has not been 
realized in Oz yet, as evidenced by the first two unused fields in the SUB..EIV 
class. 
Object, Client, and Rule Ids 
Distributed object naming schemes have been thoroughly investigated in the 
distributed object-oriented database community (see, for example, Orion-2 [80]). 
This is in general outside the scope of this book as a research topic, and we 
only present here a simple solution. The main goal in the design of the object-
id management is to reconcile the conflict between allowing autonomy in id 
assignment and still providing uniqueness. The solution here is to identify an 
object by the pair (SubEnv...id, obj...id) where the latter is determined by the 
owner SubEnv with no global constraints, and the former relies on the unique 
SubEnv id as explained earlier. Note that this is not a "long" id split into two 
fields, but rather two different ids. In particular, unless a cross site operation 
takes place (e.g., Summit execution) only the obj...id field is used. This reflects 
the decentralized nature of the architecture. However, the client's image treats 
the pair effectively as a single id, since it might contain images from multiple 
object bases. This approach enables each local object management to employ 
its own id management without worrying about uniqueness across sites. Mov-
ing/ copying objects permanently across SubEnvs is treated as adding a new 
object to the target SubEnv with the specified values (and in case of move also 
deleting the source object), thereby assigning to it a new id locally. 
Two other entities that require global uniqueness are client- and rule-ids. As 
with object-ids, their uniqueness is derived from the uniqueness of the SubEnv-

Oz Decentralized Architecture 
191 
id. Rule-ids were already discussed in Section 4.6. The reason why client-
ids need to be unique across SubEnvs is that an open-remote operation is 
effectively treated by the remote server as a remote-login operation, meaning 
that the remote server makes an entry for the client, and it uses the client 
id in order to identify the client, so the ids of all clients originating from all 
sites must be distinct. The implementation of client-ids is similar to rule-id 
assignment, i.e., each local server maintains a private counter, to which it adds 
the SubEnv-id multiplied by large constant (the same constant is used in all 
SubEnvs, of course), thereby ensuring global uniqueness. 
5.3 
A PROCESS FOR SITE 
CONFIGURATION 
Recall that one of the goals in the design of the communication infrastructure 
was to enable some degree of modeling and tailorability. The first step towards 
achieving that goal was in the definition of the connection database as a set 
of first class objects instantiated from a designated class that could potentially 
be evolved on a per-project basis (Section 5.2.2). The second step towards 
achieving that goal is in the manipulation of the connection database. The 
idea is to exploit the concept of process modeling and apply it to configuration 
by defining a registration process specified in the normal PML, and to exploit 
the concept of process enactment by executing the configuration process using 
the enactment engine normally used to enact a software process. 
As with software processes, this approach grants the potential for tailorability 
of the configuration process. However, divergence from the standard process in 
this case is confined mostly to the global environment level, since configuration 
is inherently a global task. Nevertheless, some limited site-specific extensions 
to the global configuration process are also possible, in principle. In addition, 
since (re)configuration is performed using the normal process engine, it can be 
performed dynamically as it amounts to a normal process step invocation. This 
fulfills one of the base requirements set forth in Section 1.5, namely dynamic 
configuration. Further, the exploitation of process automation ensures that the 
addition/ deletion of sites is carried out consistently across all of a global envi-
ronment's sites, with minimal human (and error-prone) intervention. Finally, 
protection from accident is afforded through the object base's normal access 
control facilities. We now present the actual registration process. A detailed 
description of the configuration process is also given in [19]. 

192 
CHAPTER 5 
5.3.1 
Configuration Facilities 
The registration process presented here is mostly similar to any other Oz 
(sub )process. The differences are in that (1) it was written by the environ-
ment kernel implementors rather than by process engineers (although the lat-
ter might extend this process to some extent); and (2) it is a "global" process 
that requires the issuer to have administrator privileges on all remote SubEnvs, 
since it manipulates the connection database in all SubEnvs. 
The process consists of a set of rules and envelopes that wrap "configuration" 
tools, and operates over the connection database, i.e., over all SubEnv objects 
in all SubEnvs. The details of the registration process, just like those of soft-
ware development processes written by typical process engineers, can safely be 
ignored by most environment end-users. The process consists of three tasks: 
(1) Registering a new (perhaps pre-existing) SubEnv into an Environment; 
(2) Deregistering a SubEnv; and (3) Moving a SubEnv to a different location 
and/or host within the same global environment. All tasks are modeled as rules 
which are invoked interactively inside anyone of the existing SubEnvs, with 
the same user interface normally employed for regular process enactment. 
SubEnv Registration 
A multi-site environment is populated by means of a registration task that 
can be invoked from any other existing active SubEnv. The only exception 
is the creation of the first SubEnv, which is "hand crafted" using a special 
utility. The registration task consists of two steps: (1) adding a new stub 
object (representing the new SubEnv) to all existing SubEnvs (modeled by 
the register ..subenv rule); and (2) physically creating and initializing a new 
SubEnv or joining a pre-existing one (modeled by the send_connection..db 
rule). Both rules are listed in Appendix A.I. 
The register ..subenv rule may be evaluated from any site already partici-
pating in the relevant global environment. It binds the SubEnv objects of 
all existing SubEnvs ("real" object for the local SubEnv and stub objects for 
remote SubEnvs), and executes the register..subenv tool envelope (listed in 
Appendix A.2) with the SubEnv objects as the activity's parameters. 
This "tool" prompts the administrator for the new site's static information, and 
creates in all existing SubEnvs (including the local one) a replicated SubEnv 
stub object instantiated with the specified static information. If the envelope 
detects the occurrence of any of a set of common problems (e.g., cannot contact 

Oz Decentralized Architecture 
193 
a remote SubEnv), it returns an error code (which can in principle trigger the 
activation of an "exception handler" rule). 
The second step in the process creates and initializes the new SubEnv (or mod-
ifies the joining SubEnv if it pre-existed), by invoking a remote environment-
initialization utility at the new (joining) location, creating (modifying) the local 
SubEnv object there and adding all the stub objects -
one for each of the other 
SubEnvs in the environment. 
Notice that both steps require to contact remote SubEnvs and update their 
objectbases (adding SubEnv objects). This is possible due to a batch facility 
that enables recursive invocation of a new Oz client from within an envelope 
forked by an existing client. The new client performs the sequence of commands 
listed in a script and exits. This gives the ability for an envelope executing at a 
client in one SubEnv to generate a script of Oz commands and spawn another 
(batch) client that executes the generated script in a remote SubEnv. This 
technique provides for a simple registration mechanism that can be controlled 
from a single interactive client. For example, the registration envelope generates 
a script of commands that contain invocation of the init..remote...subenv rule 
(listed in Appendix A.I). This rule5 simply adds a new (SubEnv) object with 
some specified values. Then, it traverses the local connection database (i.e., 
the set of SubEnv objects) and for each remote stub object it spawns a batch 
client that operates on the proper remote SubEnv with the generated script as 
the input command batch file. 
SubEnv Deregistration 
This task is modeled by the deregister...subenv rule (shown in Appendix A.I). 
It removes a site from the global environment, by deleting the site's SubEnv 
objects from all other SubEnvs (again, using the batch facility), and by deleting 
the SubEnv objects representing these other SubEnvs in the site's own sub-
objectbase. The SubEnv itself is only split off from the global environment, 
but it is not destroyed; the former SubEnv can continue operation on its own 
as a single-site environment, and may be rejoined into this or another multi-
site environment later. For autonomy reasons, this step can only be performed 
locally, Le., at the site that is about to be de-registered. 
5The hide keyword preceding the definition of that rule prevents that rule from being 
displayed at the client's rule-menu, since it is not intended to be executed interactively. 

194 
CHAPTER 5 
SubEnv Migration 
The last supported step in the configuration process is migration of a SubEnv 
to another physical location, modeled as the changeJlubenv..location rule 
(shown in Appendix A.I). This rule prompts the user for the new location (host 
and file system path), physically moves the environment directory's contents 
to the new location, and updates all the stubs in the remote SubEnvs (again, 
using the remote batch facility). As with deregistration and for similar reasons, 
this rule can be fired only locally, i.e., at the SubEnv that is actually moved. 
5.3.2 
Summary 
The main research contribution with respect to configuration is that it is treated 
as a fully integrated process, benefiting from most of the advantages that come 
with process. In particular, it can be enacted by the process-centered envi-
ronment exactly like any other process that one undertakes during software 
development. Furthermore, the process can be partially modified and tailored 
for new and existing environment instances using the same process evolution 
capabilities, provided that the required parts of the data and rules are protected 
from modifications. 
There are several additional important advantages to this approach, as opposed 
to hard-wiring the (re)configuration mechanism internally in Oz: 
1. It is likely to prove substantially easier to modify, mainly for the parts that 
do not require kernel changes. In particular, it can be modified by system 
administrators on a per global environment basis. 
2. It was much easier to implement, reusing largely pre-existing facilities. For 
example, maintaining the configuration database as part of the process and 
product database took advantage of Oz's persistent object management 
system. 
3. Since the uniform mechanism is part and parcel with the rest of the system, 
many aspects of the (re)configuration process come nearly "for free". For 
instance, transactional (re)configuration can be supported immediately as 
a private case of the general decentralized transaction manager, eliminating 
the need for a special purpose transaction facility for configuration. 

Oz Decentralized Architecture 
195 
5.4 
CONTEXT SWITCHING IN SUMMIT 
5.4.1 
The Problem 
In a conventional client-server architecture, it is clear that if some requests take 
long time to service, and/or they consist of a series of interactions between the 
client and its server for which a context must be kept throughout the interaction 
-
then a context switching mechanism is necessary to avoid starvation of other 
waiting clients. For example, in a single-server enactment of a (local) rule-
chain, all activities execute at the client's address space. These activities might 
take arbitrarily long, and it is not reasonable to expect that the server will 
block while the activity executes at the client. Further, as a chain consists of 
several rules, it is even more unacceptable to assume that a whole chain (and 
its associated activities) will actually execute atomically, even if the "all or 
nothing" atomicity property is required for the execution. Thus, it is necessary 
for the server to keep a context for each chain, and switch among the contexts to 
service multiple clients concurrently. A mechanism for context-switching among 
multiple executing clients was part of Marvel (due to the author, see [17]) and 
was upgraded to Oz. 
Considering the multi-server architecture of Oz, there is an additional problem. 
In cases where a server has to communicate with other servers in order to 
service a client request, two things might happen: (1) the server might wait 
arbitrarily long until the remote servers complete to service the request, thereby 
reintroducing the starvation problem more vigorously than in the single server 
case; and (2) if a server has to wait for other servers, then the servers might 
deadlock. Moreover, since servers might wait arbitrarily long, the chances for 
getting into a deadlock situation in a naive implementation are pretty high. 
To illustrate the problem, consider the following example of a communication 
deadlock between two servers, illustrated in figure 5.6: Suppose a client C1 is 
requesting to fire a Summit rule at its local server, Sl, involving some remote 
objects managed by server S2. Sl requests from S2 the remote objects, and 
waits. The request is enqueued in S2's service queue, which already contains 
several requests. At some point after the request from Sl was made and before 
it was serviced, a client C2 requests from its local server S2 to perform another 
Summit rule, involving objects from S1. Sl's request is enqueued at S2, but 
since both servers are waiting for each other, we have a deadlock. Note that 
this deadlock problem is completely orthogonal to the notion of transaction 
deadlock. In particular, the objects or rules which were accessed by the servers 
can be totally unrelated to each other. 

196 
CHAPTER 5 
2or3 
1 
4 
2or3 
request 
request 
Figure 5.6 
A Communication Deadlock Example 
In order to realize the magnitude of this problem and the importance of solving 
it in Oz, we analyze the types of synchronous server-to-server and server-to-
client interactions which require the requesting server to (logically) wait for the 
response before continuing with the underlying task. 
The majority of these cases occur during the normal execution of an ordinary 
Summit rule: 
1. Binding of remote objects to the Summit rule's parameters 
2. Treaty validation -
all involved remote sites validate the eligibility of 
executing the Summit rule 
3. Binding of remote objects to the Summit rule's derived parameters 
4. Remote backward chaining 
5. Activity execution (server waits for client) 
6. Remote assertions 
7. Binding of remote objects as part of the inversion algorithm for new Sum-
mits (discussed earlier in Sections 2.2.5 and 4.4.3) 
8. Remote forward chaining 

Oz Decentralized Architecture 
197 
Other interactions that require the server to wait include the import and treaty 
commands, cross-site built-in operations (particularly move, where the local 
copy should not be removed unless the copy was successful) which were dis-
cussed earlier, remote file transfer, and transactions. 
The key problem in such interactions is that a server, acting as a client, can 
block indefinitely waiting to be serviced by another server due to circular wait-
ing. 
5.4.2 
The Solution 
One possible solution to the problem is to implement a fully context-switchable 
server, so that it never blocks. However, besides the difficulties with imple-
menting arbitrary context-switching, this might introduce inconsistency in the 
server's state if arbitrary interleaving is allowed. In particular, some critical 
sections (i.e., regions of code that have to execute atomically) might need to be 
defined in order to protect the integrity of the data in the servers, reintroducing 
the deadlock problem. 
The pragmatic solution in OZ6 consists of three different methods which are 
applied at different "break points" as defined above. The first one is full context 
switching, that is, the server sends the request to the remote server(s), saves the 
context of the operating task (which then enters a "sleep" state), and is ready 
to accept new requests for service. In addition to the context switch point for 
activity execution at the client (which already existed in Marvel), Oz employs 
two other context switch points in the major natural breaks of the Summit 
algorithm, namely remote backward chaining and remote forward chaining. 
The second method, which we shall refer to as "busy-service-wait" loop, is 
applied to services that are characterized by being simple and consisting of a 
single step, but are called from deep within a complex context that makes it 
highly undesirable to switch contexts there. Examples of such break points 
include binding of parameters, Treaty validation in Summits, and some other 
non-Summit services. The idea in busy-service-wait is that the requesting server 
is not blocking but does not leave its context either. That is, it is primarily 
waiting for the reply to its original request but while waiting, it checks to see 
if new incoming requests for service arrived, and services them immediately. 
The key observation that makes this method feasible, is that unlike servicing a 
client, an Oz server that services a request from another server, never needs to 
6 partiy due to Peter Skopp 

198 
CHAPTER 5 
communicate with other servers or clients7. That is, servicing remote servers is 
done locally. Under that premise, it is guaranteed that there will be no circular 
waiting because the kind of services that it handles while waiting for the reply 
do not depend on any other computational entity. 
The third and last method, that we shall refer to as "extended-busy-wait-
service", is a modification of the second method, and is applied to steps that 
are themselves composite and require multiple service requests to complete, 
but are still hard to fully context switch. In Oz these are the direct and 
inverse binding phases in Summits (steps 3 and 7 above), which might require 
several requests to multiple remote servers to complete the binding (recall from 
Chapter 4 that bindings in Oz are realized as a set of individual sub-binding 
requests). In these situations, care must be taken so that the partial bindings 
are not altered while servicing incoming requests in the service loop, or in other 
words, there has to be a way to protect the integrity of the data while bindings 
take place because they are not performed atomically. 
The solution here is to defer any service request that can potentially update 
objects and queue it for later execution. However, if not careful, the deadlock 
problem could reappear if two (or more) servers were in the same "binding" 
mode and were deferring each other's binding requests for later execution, in-
definitely. Fortunately, since the binding phase is read-only, its requests can be 
serviced immediately, so a server in a midst of a binding phase can still service 
remote requests for binding from other servers. When the binding phase com-
pletes, the server can context switch to service any queued update requests. 
Figure 5.7 summarizes the "extended busy-wait-service" algorithm that is exe-
cuted whenever a server requested a service from another server and is waiting 
for the reply, and table 5.1 summarizes the method applied in each of the break 
points presented earlier in the previous section. 
5.5 
THE REMOTE OBJECT CACHE 
5.5.1 
The Problems 
While a composite Summit is being executed, the same object might be accessed 
by the same remote server multiple times and for various purposes, including: 
7This would not be true if arbitrary cross-site links were implemented. Another exception 
to this is coordination rules, see [54]. 

Oz Decentralized Architecture 
while (waiting.ioLfeply) 
do 
if (incoming...message) 
then 
if (this is a new service request message) 
then 
if (a non-update request) 
then 
service_request( incoming...message) 
else 
queue_request (incoming...message ) 
end if 
199 
else 
/* this is the reply * / 
waiting.ioLfeply = FALSE 
end if 
end if 
end while 
Figure 5.7 The Extended Busy-wait-service Algorithm 
Remote Request 
Method 
1 
Parameter binding 
busy-wait-service 
2 
Treaty validation 
busy-wait-service 
3 
Derived parameter binding 
extended-busy-wait-service 
4 
Remote backward chaining 
full context switch 
5 
Activity execution 
full context switch 
6 
Remote assertions 
busy-wait-service 
7 
Inversion binding 
extended-busy-wait-service 
8 
Remote forward chaining 
full context switch 
Table 5.1 
Context Switch Summary 

200 
CHAPTER 5 
1. Binding of user-selected remote objects as parameters for Summit rules 
2. Binding of remote objects to derived parameters in Summit rules 
3. Automatic derivation of remote objects as parameters to Summit rules in 
an ongoing chain (using the inversion algorithm) 
4. Firing of different unrelated Summit rules, i.e., across different chains, 
whose respective durations and object "working set" overlap 
In a naive implementation of Summits (and the implementation that predated 
the current version of Oz), a fresh transient copy of the remote object had to 
be fetched for every request to access that object regardless of whether it had 
been already fetched. If a local copy of a remote object could be maintained 
consistently such that subsequent access to the object from the local server 
would only need to refer to that local copy instead of re-fetching it, and if the 
overhead for maintaining this consistency is small enough, a great improvement 
in performance would result. The idea of establishing remote object cache came 
up aiming at this purpose. 
There are several problems in implementing a cache in a system like OZ: 
1. Cache invalidation and update -
This is the most crucial aspect of the 
cache. Clearly, a copy of a remote object should be invalidated if the 
original object has been updated, or else the access to the (stale) copy is 
inconsistent. 
2. Structure Validity -
The arbitrary complex relationships among objects 
in an object-oriented database complicate cache management, since in ad-
dition to the validity ofthe objects, the validity ofthe links has to be main-
tained, particularly due to the various structural and associative querying 
capabilities which are used to access objects. 
3. Pre-fetching -
A related issue is to assess whether to pre-fetch related 
objects along with the requested one, and to what degree. For exam-
ple, pre-fetching could range from immediate "relatives" (children, parent, 
links) at one end of the spectrum, to the transitive closure at the other 
end. 
Cache and replica management as well as pre-fetching methods in distributed 
systems in general and in distributed database and file systems in particular, 
is a wide topic on its own and has been widely explored (e.g., Coda [114]). 

Oz Decentralized Architecture 
201 
However, special characteristics of Oz make it a "special case" worth discussing 
the solution to the first two problems mentioned above. Prefetching, while 
a promising direction, is beyond the scope of this book and is not further 
addressed here. 
5.5.2 
The Solution 
The cache in Oz is part of the data manager component (see figure 5.1), and 
can be viewed conceptually as a non-persistent extension of the in-core local 
objectbase. Each server maintains a single cache that is shared by all clients. 
An alternative to this design would have been to maintain a temporary cache 
on a per-task basis that is initialized upon fetching the first remote copy and 
is destroyed when the task is complete8 . The main advantage of this approach 
is that it simplifies the invalidation scheme since no other tasks can access the 
(private) copies, and at the end of the task all the copies are simply removed. 
However, the limited scope of such a cache and the large overhead associated 
with it, make this approach not worth pursuing. Thus, the cache can be ac-
cessed by any local task (from any client) that involves access to remote objects 
over arbitrary periods of time, as long as the cache objects are valid. 
The cache is implemented as a hash table that can be looked up by the ob-
ject's id similar to the hash table used in the main in-core objectbase, except 
the lookup is also by the SubEnv-id, not only the object-id. However, unlike 
the regular objectbase, it does not maintain the by-class list or the structural 
graph. Instead, each cached object is enclosed within a "cache entry" that 
contains information regarding its relationships to other objects, termed here 
the relationship lists (the cache entry contains also validity information which 
is discussed later). More specifically, the cache entry contains four lists that 
cover the two directions in the two kinds of relationships among Oz objects 
-
children, parent, forward links, and backward links. Each list is further de-
composed to sub-lists that correspond to the attributes within their category 
(recall that a class in Oz can have several attributes of the same type). The 
individual sub-lists, however, contain only object-ids, not pointers to objects. 
Thus, at any point in time, none, some, or all of the replicas of the objects that 
correspond to an id-list may reside in the cache. 
An important invariant in the design of the cache can be defined as follows: if 
a cache object is marked as valid, then its relationship lists are also valid, i.e., 
it is indeed connected in the original object base to all objects whose ids are 
8 Recall that a task corresponds to a single chain of rules. 

202 
CHAPTER 5 
stored in those lists. One implication of this validity invariant is that an object 
needs to be invalidated not only when its content changes, but also when its 
relationships with other objects change. More specifically: 
• 
When a new object is added, any cache copy of its parent should be inval-
idated. 
• 
When an object is deleted, any cache copy of itself, its parent, and objects 
that previously linked to the deleted object or were linked-to by the deleted 
object, become invalid. 
• 
When an object is linked to, or unlinked from, another object, any cache 
copy of both objects should become invalid. 
Other implications of this invariant will be discussed shortly. 
Finally, the cache in Oz is a "write-through" cache. That is, any updates to a 
remote copy (through remote assertions in the effects of rules) are immediately 
propagated to the original copy. However, such updates do not invalidate the 
remote site from which the update was made, since its copy is still up-to-date. 
Only remote copies of that objects from other sites are invalid. Technically, 
the cache should handle similarly structural changes to a remote object base 
such as addition and deletion of objects. However, at present only changes 
to the contents of individual objects (not to the object lattice) are supported 
in the cache; the structural operations are performed directly in the remote 
objectbase (and any relevant entries in the cache are invalidated). 
Maintaining Structure and Connectivity 
When a remote object is accessed directly by its id, the local object manager 
first looks in its cache, and if an entry is found and is marked as valid, the 
proper pointer to the object is returned to the caller. Otherwise, the object is 
fetched from the remote site, along with its relationship lists (but none of the 
related objects is actually accessed). 
The second way to access remote objects is through structural queries. (Asso-
ciative queries that appear in conjunction with structural queries are handled 
by evaluating the structural query first, and then filtering it through the asso-
ciative query. The infrequent purely associative queries are not supported in 
the cache and require remote fetch.) In Oz, exactly one of the two symbols in 

Oz Decentralized Architecture 
203 
a structural query is always already bound, and the second symbol is bound as 
a result of the query. For example, in the query: 
(forall CFILE 1c suchthat (member [1m.cfiles 1c])) 
the symbol 1m is already bound to a set of objects, and the query binds to the 
symbol 1c all objects that are members (i.e., children) of the cfiles attribute 
of the objects in the binding set of 1m. 
Thus, when a structural query on remote objects is made (e.g., as part of 
a Summit rule), all remote objects associated with the bound symbol (1m in 
the above example) must have already been fetched previously (either directly 
during the binding or through a previous query) and thus exist in the cache. 
To evaluate the query, first the bound objects must be valid, otherwise they are 
re-fetched. Thereafter, the proper relationship list is scanned (for example, in 
the above query the children. cfiles sub-list would be scanned) and each id 
is looked up in the cache. In the ideal case where all remote objects are found 
in the cache and are all valid, the query completes without accessing remote 
servers at all. Otherwise, the missing or invalid objects are {re)fetched from the 
remote servers (along with their relationship-lists). Thus, the relationship-lists 
playa major role in reducing the amount of remote fetching while incurring a 
relatively small overhead. Notice how the validity invariant mentioned above 
is necessary for the correctness of this scheme. 
Cache Invalidation Scheme 
The cache invalidation problem is clear: given that objects can be updated over 
time, there must be a way to know if the replica of an object in the cache is up-
to-date with respect to the original copy. If the replica is out-of-date, it must 
be invalidated so that subsequent access requests would result in re-fetching of 
a new copy. 
One solution to this problem is to maintain in the original object a list of sites 
that hold a replica in their cache, and whenever the primary copy is modified, 
all remote caches are asynchronously notified to invalidate their outdated copies 
of the object. The list of SubEnvs that hold a cache copy would have to be 
maintained in each object, adding an entry upon remote fetch and deleting it 
when the associated replica becomes invalid. Several minor variations can be 
made to this basic approach. For example, the server might send the modified 
object (or the changes from the original copy) along with the invalidation mes-

204 
CHAPTER 5 
sage. Another alternative to maintaining the per-object SubEnv list would be 
to simply broadcast the invalidation to all sites in the environment. 
However, this "instant notification" approach does not jibe well with autonomy 
and independent operation requirements, due to the overwhelming cross-site 
communication involved, and due to the fact that (possibly long) delays might 
easily lead to race conditions in which a cache object is accessed before it has 
been invalidated. 
There are other requirements that make the invalidation problem hard in Oz. 
The fact that object management is decoupled from transaction and task man-
agement implies that the cache and the invalidation scheme should also be 
decoupled from transactions and tasks, with transaction and tasks only invok-
ing invalidation requests and the cache implementing them. This is a clear 
example of the need to separate (invalidation) mechanisms and (invalidation) 
policies. Moreover, an instance of Oz might employ an advanced concurrency 
control method that facilitates collaboration by means of allowing, for exam-
ple, multiple simultaneous writers. Embedding a built-in invalidation policy 
in the cache is likely to conflict and effectively prevent such tolerance by, for 
example, repeatedly fetching new copies and restarting tasks due to the inter-
leaved updates. This reinforces the need to determine the invalidation policy 
independent of the mechanism. The cache in Oz takes into considerations all 
of the above arguments. 
The invalidation mechanism is relatively straightforward. Each cache entry 
contains a valid flag that indicates the validity of the object in its entry. 
When a cache object is invalidated, its valid flag it turned off, but its memory 
is not freed. The reason is that the same cache object might be pointed to by 
multiple (inactive) tasks, or even by multiple symbols in the same active task, 
so freeing the memory would result in stale pointers (unless the cache maintains 
a list of accessors and notifies them to unlink their pointer, but this is a costly 
procedure). Therefore, whenever a new copy is fetched, its contents are copied 
into the contents of the old object, so that all references continue to point to 
the right object. The only problem with this approach is that the memory 
occupied by a cache never shrinks (although it grows slowly because multiple 
fetches of the same object do not require additional memory allocation), but 
a simple garbage collection could be launched when the server is idle. Then, 
whenever a cache object is accessed, either directly or via a relationship list 
of another (valid) cache object, its valid flag is checked. If it is invalid, then 
a fresh copy is requested to be fetched. Note that following the analysis in 
Section 5.4, there is no need to abort the task to avoid deadlock. Once again, 

Oz Decentralized Architecture 
205 
the validity invariant mentioned above is necessary here for the correctness of 
queries involving cache objects. 
The invalidation policy in Oz is not as simple, however, as it is tied to the 
semantics of the operations that use the cache, in order to optimize it with 
respect to the requirements set above. Given that instant notification is im-
practical, we try to find ways to apply the lazy approach without sacrificing 
correctness. This is done by enforcing the following constraints: 
1. The first constraint made is that no two executing (i.e., active) tasks can 
update the same object (or its replica) simultaneously. Note that while this 
scenario cannot physically occur within a single-threaded server, it might 
very well occur if two tasks execute each at a different server and update 
different copies of the same object. This constraint can be enforced by the 
transaction manager, since we assume that each request for a remote fetch 
involves also a lock request. Although limiting, this scheme still allows 
for interleaved updates of an object by multiple tasks, as will seen shortly. 
Also, it is still adequate for the current support for groupware multi-user 
tools, because from the server's point of view, only the initiating task is 
updating the objects; multiple updates to parts ofthese objects (e.g. files) 
are handled by the multi-user tool and are shielded from the server. If, 
however, the support for multi-user tools is modified in Oz and requires 
to allow multiple decentralized tasks to explicitly update the same objects 
(for example, along the lines of [127]), this constraint might need to be 
relaxed. 
Thus, if we could find a way to properly update a ready-to-run task with 
the relevant update information just before activating it, its view of the 
cache would still be valid. 
2. The second constraint that we impose is that no two unrelated tasks execut-
ing in different servers can operate on an object with conflicting modes, 
that is, with at least one of them being a writer. While apparently re-
strictive, it still allows for related (sub )-tasks to conflict, and also allows 
multiple unrelated tasks executing in the same server to conflict. A typ-
ical example of the former case is remote backward chaining in Summit, 
whereby the object that "failed" to satisfy the condition must be updated 
in order to make the condition satisfiable, even-though the (non-active) 
Summit task at the coordinating site has been accessing the same object, 
at least for reading purposes. An example of the need for the latter case 
is for supporting advanced concurrency-control mechanisms such as in [8]. 

206 
CHAPTER 5 
The rationale behind this constraint is to keep the update privileges under 
the control of, and within the boundaries of the (global) Summit task (and 
its associated distributed transaction), so that all relevant update informa-
tion can be passed upon reactivation of a Summit task at the coordinating 
site. 
3. The third and last constraint states that an object in the cache is valid 
only while there is at least one Summit task that references it (note that 
objects reside in a cache of a server only if that server is the coordinating 
server of an executing Summit task). This implies that at the completion 
of a Summit, all the cache objects which were accessed by the Summit task 
are invalidated, unless some other Summit task is referencing them. This 
constraint appears to reduce the utilization of the cache most significantly, 
but is nevertheless necessary with lack of asynchronous instant notification, 
since we need to keep each cache object under the control of (at least one) 
active task that maintains its validity. 
Cache Operation During Summit 
The above constraints effectively define the invalidation policy. To illustrate 
invalidation, consider the operation of the cache manager at the coordinating 
site in a typical execution of a Summit, along its major phases: 
1. Summit Initialization and Validation -
This phase binds all objects that 
are accessed by this rule (but not necessarily all objects accessed by subse-
quent Summit rules in a composite Summit). Binding of the user-selected 
remote objects to the rule parameters and binding of remote objects to 
derived parameters through queries is done as explained earlier in Sec-
tion 5.5.2, and these objects are put in the cache. 
2. Pre-Summit -
If remote backward chaining takes place, the Summit task 
becomes inactive, and gets reactivated only when all remote sites complete 
their backward chain. When a remote backward chain completes, it sends 
along fresh copies of the modified objects to the coordinating site, and 
when the Summit task becomes active it updates the cache properly. The 
actual update can be implemented either by sending a "delta" (reducing 
the communication overhead) or by replacing whole objects (reducing the 
computation involved in calculating the delta). For small sized objects the 
latter approach was adopted in Oz, but for objects with file attributes that 
had to be transferred to remote domains a different approach was taken 
(see Section 5.6). 

Oz Decentralized Architecture 
207 
It is important to note that during backward chaining, the cached objects 
in the coordinating site cannot be updated by the coordinating process 
because it is effectively blocking, or by other subtasks in other SubEnvs 
because of the second constraint. 
3. Summit Activity -
This phase causes no problems in terms of invalidation 
since it executes only at the coordinating site. Still, any access to cached 
objects has to be validated as usual. 
4. Post-Summit -
This case is similar to pre-Summit in terms of the cache. 
While the coordinating task is not active, the remote sites might update 
their original copies of the objects and when they complete their work in 
"local" mode, they send to the coordinating server the updates so that it 
can update its cache. 
5. Inference of (forward) Summit Rules -
This case is similar to the binding 
step in phase 1. Note that this phase might extend the "working set" of 
objects accessed by this (composite) task, since it derives new parameter 
objects for subsequent Summit rules. 
6. Summit Completion -
The cache manager invalidates all objects accessed 
during the task that are not accessed by another on-going Summit in the 
same coordinator site. 
5.5.3 
Results and Summary 
In order to assess the improvement in performance, several experiments were 
conducted. The nature of the experiments was basically to run identical Sum-
mits on identical objectbase states twice, one time on a server with cache, and 
a second time on a server without cache, and compare their performance. Since 
the cache is not intended to add or remove functionality, the execution trace 
should be identical in both cases, the only difference being in the performance. 
Aside from the absolute execution times, the main basis for comparison was 
the number of messages which were exchanged between the servers during the 
Summit, and their type. Since the major delays are incurred by the commu-
nication overhead and their growth is inversely proportional to the available 
bandwidth, and given that all objects in the experiment were roughly of same 
size, it is a valid measure of performance improvement. 
We show here the results of one specific example on a three-site environment, 
involving a small number of objects and two remote derivations, one for par-
ents and one for children. The results are summarized in table 5.2. Obviously, 

208 
CHAPTER 5 
Message Type 
non-cache server 
cache-server 
purpose 
GET-REMOTE_OBJECT 
2 
2 
get remote obj 
GET _REMOTE..PARENTS 
36 
2 
get remote parents 
GET-REMOTE_CHILDREN 
18 
2 
get remote children 
CHECK_REMOTE_EXEC 
2 
2 
treaty validation 
BC_REMOTE 
2 
2 
remote backward chain 
ASSERT-REMOTE 
8 
8 
remote assertions 
FC-REMOTE 
6 
6 
remote forward chain 
Totals 
74 (29 sec) 
24 (9 sec) 
Table 5.2 
Performance comparison with and without cache 
the improvement is significant due to the reduction in the number of calls to 
get the remote parents and children, and there is no additional communica-
tion overhead due to the operation of the cache. It is interesting to note that 
the reason for the large number of requests for GET..1I.EMOTE.PAREITS and for 
GET ..REMOTE_CHILDREI stems from the fact that when the rule processor evalu-
ates which rules to forward-chain to, there are many possibilities to instantiate 
each such rule (i.e., to select objects as parameters), of which only a small 
fragment really gets executed, because the rule's condition is not satisfied on 
most of the instantiations9 . 
As for overall performance improvement, it was expected that the communi-
cation overhead will outweigh computation overhead across hosts, particularly 
as the available bandwidth between the hosts decreases. However, even across 
(operating-system) processes in the same machine the improvement was sig-
nificant. For example, a simulation of the above Summit with no interactive 
activities (thus mostly involving operation at the server host) with the three 
"sites" running as (operating-system) processes in the same physical host, took 
29 seconds on a non-cache server versus 9 seconds on a server with cache. As 
can be seen in table 5.2, the time ratio is very close to the ratio of the num-
ber of messages between the non-cache and the cache-server, confirming that 
the computation overhead is negligible relative to the communication overhead, 
and consequently clearly showing the performance improvement of the cache 
server. 
9 This spurious instantiation is typical of Oz rules with multiple parameters, since the rule 
processor generates a cross product of the sets of objects for each parameter in the chained-to 
rule. 

Oz Decentralized Architecture 
209 
Cache and Transaction Management 
There is one caveat to the above results: they were taken ignoring the over-
head of the locking-based decentralized transaction manager (and before it was 
activated in Oz). In principle, the transaction manager should not impact the 
ratio in performance between the cache- and the non-cache server, since when 
optimized, its locking overhead should be proportional to the number of objects 
obtained, and the non-locking-related overhead should be independent of the 
cache. In practice, however, if the locks are maintained separately from the 
objects, (as in the case of Oz), this requires implementation of cache locks in 
addition to the cache objects. With lack of such a cache, each request for a 
remote object would incur a locking request message to the remote transaction 
manager even if the server has a local replica of that object, thus decreasing 
the improvement of the cache-server. This is a subject of future work. 
5.6 
Oz OVER THE INTERNET 
Although the Oz model and its architecture are conceptually geared towards 
operation across sites, and although most of the implementation was done with 
geographical dispersion in mind, extending the system to operate over the In-
ternet introduces several new problems: 
1. No shared file system across sites 
2. Security, authorization, and access control issues 
3. Different administrative network domains 
4. Variable bandwidth and time shifting 
This section provides a brief summary of our preliminary exploration of only 
the first two problems, and solutions to them. The discussion in this section 
is by no means comprehensive or complete, and by-and-Iarge, it is a subject 
for further investigation (see 7.2). In particular, the last problem dealing with 
the general performance, optimization, and synchronization in the operation 
of the system as a function of the (perhaps dynamically changing) bandwidth, 
load, and frequency of interaction between the sites, is outside the scope of this 
book. 

210 
CHAPTER 5 
5.6.1 
No Shared File System 
The fact that SubEnvs have no means to share any data, either physically 
or through the underlying operating system (e.g., NFS), complicates several 
aspects of the implementation: 
1. The biggest problem is with respect to sharing bulk data, (e.g., files), due 
to the large volume of data involved. 
2. With lack of shared data, the file system cannot be used for communication 
purposes, for example to store and retrieve addresses of remote ports. 
Moreover, pathnames are no longer necessarily unique. 
File Transfer 
The main reasons for sharing files across sites in Oz are: 
• 
For execution of activities in Summit rules that involve remote objects 
(and their associated files). 
• 
In built-in cross-site copy and move operations 
• 
The import operation as part of Treaty involves receiving a list of available 
strategies from the exporting SubEnv and copying them (see Section 4.3). 
• 
The configuration process which was discussed earlier requires invocation 
of batch clients, meaning that at least the generated batch files must be 
transferred. Another problem with registration is the initialization of a 
new remote site (as opposed to joining an existing one, which causes no 
problems). 
Of the above, the file transfer during the activity execution on behalf of Summit 
rules is the most critical, and the only one discussed here. The other cases 
are simpler and can be realized on top of the general purpose file transfer 
mechanism (e.g., ftp). 
With a shared file system, when an Oz server transfers objects across sites for 
rule execution purposes, it physically transfers only the "light" status attributes 
(along with other information such as its id, class, etc.)10, but for the "heavy" 
lOThe size of an Oz object depends on the schema defined in its process, but on average it 
is about 100 bytes long. 

Oz Decentralized Architecture 
211 
file attributes only their pathname is sent, and when a remote client needs 
them (e.g., for executing a rule activity) they are accessed through the pro-
vided pathname (recall that we still assume that clients and their local servers 
share the file systemll ). Thus, with no shared file system there must be an un-
derlying file transfer mechanism that physically transfers the files. Moreover, 
this mechanism needs to transfer the files back if they were changed during 
their use. 
The main technical problem with implementing a file transfer mechanism is that 
it cannot be executed synchronously, otherwise it would effectively block both 
servers (the sender and the receiver) which, as seen earlier in Section 5.4, cannot 
be allowed. Thus, files must be sent asynchronously, "in the background" , and 
the receiver server notified when transfer is complete. Since the light objects 
arrive quickly at the receiving end, another optimization would be to start the 
activity and wait only when the files are really needed. Finally, care must be 
taken that no files are transferred unnecessarily (either back or forth). 
The general design of the file transfer mechanism12 is as follows. All file trans-
fers are initiated by the server that executes the (Summit) rule, i.e., the receiv-
ing server, and only when they are needed (as usual, this lazy approach seems 
most appropriate for this purpose). At this point, the server performs a context-
switch and the executing rule enters a sleep state. In order to not occupy the 
server for a long duration, files are transferred in small chunks (4kbytes) and 
only when the server is idle. An alternative and perhaps more appropriate 
approach would have been to send the files over a completely separate channel, 
but for practical considerations the former approach was chosen. 
When the transfer is complete, the sleeping rule is woken up and continues 
its execution (but recall that it has to wait for the next context-switching 
opportunity since the server cannot be preempted) by sending the necessary 
arguments to the client on whose behalf the file(s) were requested, for executing 
the rule activity. When the client finishes its activity, the rule enters a second 
context-switch point, this time for copying back files if they were updated by 
the activity. Both the receiver and the sender servers create and maintain 
information on the transfer. However, information regarding the halted rule is 
held only by the receiver. 
11 An extension of this model that deals with low-bandwidth clients where there is no shared 
file system with their server (mainly to address remote clients that are connected through a 
modem to the server) is dealt with separately by Skopp [117]. 
12This is largely due to Peter Skopp and Shelley Tselepis 

212 
CHAPTER 5 
In order to reduce unnecessary transfer, the following methods could be em-
ployed (in practice, only the first two have been implemented as ofthis writing): 
1. File caching -
When a server imports a remote file, the copy is placed in 
a special area of the SubEnv that identifies it uniquely. Then, upon com-
pletion of an activity involving a remote file, it is not destroyed. Instead, 
if the same file has to be sent to the same SubEnv for execution of another 
activity and it hasn't been changed since, the transfer is not necessary and 
the client can use the cached file. 
2. Checksum -
Each time a transfer is requested, the sending server first 
sends the receiving server a "magic" number that represents the file. If 
the receiving server happens to have that file, and their "magic" numbers 
are identical, there is no need to perform the transfer. There are two 
frequent situations in which no transfer is necessary: 1) in the copy-back 
stage, in case the executing activity accessed the files in "read-only" mode; 
and 2) if the file was recently updated by the same client or another client 
in the same SubEnv, and is sent again for another activity- this is possible 
due to file caching. 
3. Semantic prefetching -
Prefetching of files during idle time can lead to 
substantial improvement in performance. The idea is to anticipate future 
use of some files, and prefetch beforehand, so that when the user wants to 
use them they are already local. The key issue is to use the right criteria 
to determine what files are likely to be needed soon, and prefetch them. 
For example, one method might be to use history of access patterns (for 
more work in that area see [124]). The interesting aspect from the Oz 
perspective is that the process model contains semantic knowledge which 
might help in predicting future references. For example, by observing the 
rule network and anticipating a path in a chain of rules, files that are 
intended to be used in (forward-chained) rules in the near future can be 
prefetched. Some preliminary work in this area has been done by Skopp, 
see [117]. 
Extensions to Connection Server and Database 
The basic requirement here is to ensure that all the information which is neces-
sary for connectivity, without exceptions, can be obtained from the connection 
database or through the Connection Server. To achieve that, the following 
enhancements to the Connection Server were made: 

Oz Decentralized Architecture 
213 
• 
The SUB_ENV class has been extended with a boolean attribute called 
has..nfs, that indicates for each remote SubEnv, whether it is sharing its 
file system with the local SubEnv or not. This important information 
enables to determine, for example, whether to send pathnames and rely 
on the underlying shared file system, or actually send the files. Note that 
the attribute might be set to false even if there exists an underlying 
shared file system, in cases that the performance of the shared file system 
degrades, or is temporarily not operational. Even though it is part of the 
static information of SubEnv objects (and therefore its value is assigned at 
registration time), the has..nfs attribute's value is not replicated, since for 
example, the value of this attribute in each SubEnv object at its local site 
is always true, but the value of its stub in remote SubEnvs mayor may 
not be true depending on whether the two SubEnvs share a file system. 
Thus, the value is determined at each remote site individually. 
Incidentally, this example shows how the tailorability of the connection 
database has paid off: Modifying the SUB_ENV class was done simply by 
adding the attribute and evolving the object bases, with no code changes 
in the kernel. 
• 
To uniquely resolve pathnames as locations of environments, the simplest 
solution is to prepend them with the full host name. 
• 
The Connection Server has been extended with the capability to respond 
to queries about port numbers of active servers in its domain. (Previously, 
servers were "peeking" to each other's port-files which contained the port 
number.) 
• 
Finally, since intermittent disconnections and noise are more likely to inter-
fere with communication between geographically dispersed SubEnvs, the 
inter-pro cess-communication layer in clients and servers should be more 
fault tolerant and anticipate them. As indicated earlier, this is beyond the 
scope of this book. 
5.6.2 
Security Firewalls 
In order to address the desire of private corporations to be connected to some 
networked services but at the same time isolated (which is in a sense similar to 
the tension between autonomy and interoperability at the process level), some 
security mechanisms have been invented with an option to control the level 
and kind of "openness". One common security mechanism that is intended to 
isolate private networks from public networks (i.e., Internet) is the "firewall". A 

214 
CHAPTER 5 
firewall host is a dually homed machine, meaning that it contains two network 
interfaces -
one attached to the secure, private network and the other one 
is attached to the public, insecure network. The firewall machine can then be 
customized to allow or deny certain network packets to pass through, depending 
on source address, destination address, port number and in more advanced 
software, even by user identity. For example, it is common in companies to 
allow only incoming and outgoing mail and block any other service. 
In order to provide more flexibility and programmable control over the allowable 
communication through secured networks, the SOCKS package was chosen13. 
SOCKS is a public-domain package that allows hosts behind a firewall to gain 
full access to the Internet without requiring direct IP reachability. It works 
by redirecting requests to talk to Internet sites to a server, which authorizes 
connections and passes IP packets back and forth. The SOCKS package also 
allows external hosts to access a defined set of internal machines. The SOCKS 
daemon runs on a firewall machine and serves requests from other SOCKS 
clients. The SOCKS daemon receives packets on a designated SOCKS port, 
and decides whether to forward the packet to the other interface. By using the 
SOCKS library of replacement socket calls, Oz clients and servers can com-
municate with each other, thereby enabling interoperability through firewalled 
sites. For more details on the SOCKS integration inOz, see [87]. 
5.7 
IMPLEMENTATION STATUS 
As of this writing, most of the features discussed in this chapter have been fully 
implemented. The following is a summary of the aspects which were discussed 
in this chapter and are only designed or have not been fully implemented yet. 
Connection Server -
Automatically reverting from direct to indirect mode 
during a remote session when the dynamic information turns out to be out-
of-date, is not supported. Instead, a client has to manually close its remote 
connection and re-open it using the indirect mechanism. The domain SubEnv 
table and its associated feature -
to enable the Connection Server to invoke 
remote Environment Servers on remote hosts in the same domain as described 
in Section 5.2.3 -
is not operational yet (an earlier prototype was developed 
by but has not been upgraded yet). 
13This work was done by Andrew Lih. 

Oz Decentralized Architecture 
215 
Environment Server -
Global environment naming scheme is not supported, 
as explained earlier. Lock management is not optimized for operation with the 
object cache as explained in Section 5.2, and lock cache is not implemented 
yet. There is no garbage collection to clean up the cache. The file transfer 
mechanism has not been generalized yet. Therefore, the Treaty and registration 
processes are not operational over the Internet. Instead, they both have to be 
performed manually using external scripts. But once defined, Summits are fully 
operational across physical sites. 

6 
THE ISPW EXAMPLE: 
VALIDATION AND 
METHODOLOGY ISSUES 
The purpose of this chapter is twofold: (1) To validate the feasibility and 
effectiveness of the ideas and their implementations as presented in this book; 
and (2) to explore methodology issues regarding decentralized process modeling 
in a decentralized environment. Both objectives are attained by discussing the 
modeling and enactment of an instantiated Oz environment that supports the 
so-called ISPW-9 Example1. 
The ISPW example was first introduced at the 6th International Software Pro-
cess Workshop [77] in an attempt to provide a canonical "benchmark" process 
scenario, and " .. as a common framework for understanding and evaluating 
various approaches to software process modeling and enactment". Since then, 
the example has evolved several times ( [53, 37] and the latest version [99]), 
adding or revising (sub )scenarios that require more advanced modeling and 
enactment capabilities, and removing some of the rigidity of earlier versions. 
The advantages of using this example for validation purposes are not different 
than the case for using benchmarks in general, namely: they tend to be objec-
tive and not (suspected to be) contrived by the implementors of the solution; 
they are written by experts in the field and therefore expected to be compre-
hensive in their coverage of the issues that need to be addressed; therefore, they 
are well accepted within the community as a valid criteria for evaluating the 
technologies and their underlying concepts. 
The remainder of this section is organized as follows: Section 6.1 describes a 
brief overview of the Scenario (a full description copied from [99] is given in 
Appendix B). Section 6.2 discusses in detail the solution to the Scenario with 
1 This Oz environment was actually demonstrated at the 9th ISPW. 

218 
CHAPTER 6 
focus on design issues and rationale, as opposed to actual codification of the 
process; to give a flavor of the actual process code, selected modules are listed 
in Appendix B. Finally, Section 6.3 discusses methodology issues based on, but 
not only on, examples from the ISPW solution. 
6.1 
OVERVIEW OF THE SCENARIO 
The scenario involves a software system that is under development and is in a 
relatively advanced phase, at a point where at least some parts of the system 
can be tested outside the development team. The process involves test and 
change tasks. Briefly, the process2 is initiated at the testing phase, where a 
tester finds a problem, and reports it. The next step is the analysis of the 
problem, which produces a proposed solution, or a change request, identifying 
the source module(s) which might need to be modified in order to fix the prob-
lem. Thereafter, the change task starts, " .. according to pre-established change 
procedures (which entail assignment of resources, code and/or documentation 
modification, analysis/testing/review, approval/rejection .. )", followed by ac-
tual modification of the code and reiteration to the testing phase. In addition 
to the base-scenario, the example suggests additional optional sub-scenarios 
(e.g., problem reporting/analysis, approval/rejection procedures) and recom-
mends to demonstrate support for some specific capabilities (e.g., multi-user 
coordination, dynamic process changes). 
In order to demonstrate the full modeling and enactment capabilities of Oz, 
we extended the Scenario along two dimensions (which may also be regarded 
as an extended solution to the original problem): 
1. We discerned three teams, each responsible for a subset of the overall 
process and treated as a "site": 
(a) Quality Assurance (QA) -
In charge of testing the system 
(b) Coding -
The code development team 
(c) Design -
The design team, also supervising code development 
2. We added steps to the process that require multi-user and/or cross-team 
collaborations. 
2 In reality, the process described in the Scenario is really a small sub-process of the overall 
software process, but for the sake of brevity and clarity we will refer to it as a process, and 
will refer to smaller units within it as sub-processes. 
. 

The ISP W Example 
219 
6.2 
SOLUTION IN Oz 
The solution environment consists of three autonomous yet cooperating sub-
processes that correspond to the three teams specified above. That is, each 
process can perform some tasks locally and independent of the other processes, 
while some sub-tasks might be dependent on other processes, or require interop-
erability with other sub-processes. Some of those interoperabilities require both 
modeling and enactment support for multi-user collaborations via synchronous 
multi-user tools, which mayor may not cross sub-process boundaries. 
The solution process defines four major tasks: 
1. Test -
Performed locally at QA. 
2. Analyze -
Performed by QA and Coding teams. 
3. Review -
Performed by Design and Coding teams. 
4. Change -
Performed mostly locally at Coding, but with small extensions 
to both Design and QA. 
Thus, with the exception of the Review task -
which was defined in the prob-
lem specification as a sub-task of the Change task and is modeled in our solution 
as a distinct task -
all tasks map directly to the original problem specification. 
Figure 6.1 depicts the high-level design of the solution. The ovals represent 
tasks, bold links represent general control-flow dependencies, and arrows rep-
resent artifacts that get generated in one task and used in a subsequent task. 
The dashed "clouds" in the figure represent other tasks that are not relevant 
to the Scenario and might be executed independently and concurrently. 
We now turn to the description of the specific processes and their participation 
in the various multi-site tasks, which are modeled as Treaties and enacted as 
Summits. But first, we start with a description of the "product" we chose for 
the example. 

220 
CHAPTER 6 
Time 
---------,--------------T---------
I 
I 
QA 
I 
CODING 
I DESIGN 
_________ L ______________ L _______ _ 
I 
I 
I 
I 
I 
I 
~
BUg Report 
................. ---) 
: 
I 
i ..... _ .........••..• • 
I 
I 
I 
I 
I 
ChaDge Request 
I 
Analyze '---::::::J~ 
c·-···············\ 
.......................... : 
{ ••••••••• u ............. \ 
.......................... ! 
Change 
.................... :; 
.. 
.0 
. 
.' 
\ 
.' 
................... 
Test 
Figure 6.1 
Process Design for ISPW-9 Example Scenario 

The ISPW Example 
221 
6.2.1 
The Product 
It is important to outline the product being developed, since clearly, the charac-
teristics of the intended product impact the process. Our choice for the product 
was the query processor for the Darkover Object Management System3 . 
Briefly, the query processor receives as input a query in a format that is equiv-
alent to the condition sub-language of MSL, applies the query to an object-
oriented database with class definitions that are equivalent to MSL's Data 
Definition Language, and returns the result of the query. 
The reasons for choosing the query processor as our example product were: (1) 
It was still under (real) development, so readily available as a "real" example; 
(2) It was complex enough to justify a team of developers, but small enough to 
enable easy migration into OZ4; (3) It has a text-based user interface with well 
defined input and (expected) output. This characteristic enabled to construct 
an automatic testing sub-process. This is a typical example where the charac-
teristics of the product affect the process. Obviously, a testing sub-process for a 
product that involves an interactive user-interface could not be fully automatic, 
since the essence of the testing is in the human interaction with the product. 
6.2.2 
The QA Process 
The highly automatic testing (sub) task in our solution is black-box, i.e., we 
assume that the testers have neither access to, nor knowledge of, the source 
code. The schema defines two main kinds of artifacts: a TEST ..sUITE that repre-
sents test cases (both input and expected output), and can be grouped under a 
TESLSUITE....sET object; and a TESLRUN entity that represents the summary of 
the results of running a particular test (its instances can also be grouped under 
a TEST ..RUN....sET object). Each TEST ..RUN object is associated with a particular 
TEST ..sUITE..sET object that contains a set of TEST ..sUITE objects. TEST ..sUITE 
objects are generated manually, and are relatively static, whereas TEST..RUN 
objects are generated automatically, and are relatively dynamic. 
The process starts when a tester invokes the start_test.IUIl rule (see strat-
egy test in Appendix B), selecting the executable program to test, and a 
TEST..sUITE....sET object to use. This rule generates a TEST..RUN object, and 
3Darkover is being developed as part of the componenti1,ation of OZj one of near-term 
future goals is to make O1,'s OMS replaceable by an external system. 
411 header files with 1375 lines of code, 11 source files with 2884 lines of code. 

222 
# 
# 
notify_bug: 
# 
CHAPTER 6 
# 
chained off reportJbug. delegated to a user that is 
# notified of the problem 
# 
1) notifyJbug[?tr:TEST-RUN]: 
2) 
(exists PROGRAMMER ?p suchthat (and 
3) 
(?p.group = "CODING") 
4) 
(?p.role = "Manager"»): 
6) 
delegate [?p.user-id] : 
6) 
(?tr.report~tatus = Reported) 
7) 
# 
Generate a notification message 
8) 
TEST_TOOLS notifyJbug ?tr.report 
9) 
(?tr.report~tatus = Notified); 
Figure 6.2 
notify.bug Rule 
spawns a sequence of executions of the run_test rule, one for each individual 
test suite. This rule runs the tested program with the input specified in the 
test suite, and stores the output of the execution in the TEST -RUN object. If any 
of the individual tests fails (which is indicated by a mismatch between the ex-
pected and the actual outputs) the rule automatically chains to the reportJbug 
rule which forks a report generation tool that inserts all the relevant information 
such as the input and expected output of the test and the differences between 
them, and so forth. The reportJbug rule then chains to the notifyJbug rule 
which has an activity that is delegated to the manager of the Coding group. 
Figure 6.2 shows notify..bug. Note (lines 2-4) how the delegatee is determined 
dynamically by fulfilling the desired values for the role and group attributes, 
rather then hardwiring its value somewhere in the rule. If the delegatee is not 
logged-in at the moment of delegation, an asynchronous overloaded version of 
notifyJbug (listed in the Appendix) is triggered, which notifies the delegatee 
about the pending activity (this is an example of the "programming trick" 
described earlier in Section 4.14). This completes the mostly local testing task. 
Figure 6.3 shows a screen dump of the objectbase display at the testing site 
before any tests were performed (the client's objectbase display is zoomed at 

The ISPW Example 
iii Oz 1.0: i .. 
l @ bl8ecker ,C3.culumbia.8du 
Local Server: "'QA'----__ CutTent Object: "'QA'---____________ 
_ 
(session v) ~ 
(constru<t 9) (Local 9) (Quer\l 9) (print v) ~ 
compact 
left: Compact/uncDMPcat object sub-trse 
,-----------------------~~.-----------------------~~! 
Peter 
oz 
Figure 6.3 The QA Object base 
223 
the QA site, thus the other SubEnv objects are not seen in the figure, as they 
are seen in figure 6.4); and figure 6.4 shows the objectbase after the execution 
of two test runs with the newly generated TEST ...RUN objects (called run_l and 
run...2) properly linked to their respective TEST..5UITE object (suite_1) and 
to the tested executable (query_processor). Finally, figure 6.5 shows a trace 
of the execution of the automatic test task. Note how this execution invoked 
the asynchronous version of notify_bug, as evidenced by the message that was 
generated by the rule (shown in the bottom half of the window). 
6.2.3 
The Coding Process 
Coding is the central process in the global environment, since the entire Scenario 
revolves around code changes. The main artifact in this process is naturally 
the source code, along with its derivatives and relatives (e.g., object code, 
libraries, documentation, configuration management). They are organized in a 

224 
CHAPTER 6 
ISJ 011.0: i3l"all1 @ bl88ckar.ca.coHrDfa.adu 
Local Server: ,.QA"--___ Current Object: "'A"-___________ ---! 
(Session 9) ~ 
(construd v) (Lo.:al ';I) (QueI")' v) (Print v) ~ 
left: Print Information for Object 
Right: ChangR Cul"t"snt Objed:: 
GDING 
ESIGN 
Issy 
run 1 
test 1 
te 
Figure 6.4 The QA Objectbase with New Test Runs 

The ISP W Example 
!Iii! Interrace for command: start lest run test re ... I\$(4,O) suite 1(5,0) query.JllOcessor(Zl,O) ~, 
i ( Dono) (Re ..... ocute) (save Record) C;";";;:ij;;"r;;~') (c":;;;:) Keep 0 
I 
complet8_test(suit8_1l 
I start_test_run(test_results suite_l query_processor) 
run_test(test_3) 
run _ test(test_ 2) 
K run_test(test_l) 
report _ bug( run _2) 
notify_bug(run_2) 
OK complete_test(suite_l) 
, 
; 
~IJ~~~~~~~~~~~~~====~==~====~====~~i 
!OCilE 
0 : 
l"'En~t~.r~n~a~.e~ot"'t.~st~ru~n-------------------------------------' 
! run-2 
! 
Issy Ben-Shaul with user id oaryel is not logged in currently 
I The notif; cat; on will be 5ent to hi •• hen he 1 oggs-1 n 
! not1 f1 cat1 on 1 s sent to larvel 
I' 
Figure 6.5 
Rule Animation of the Testing Task 
225 

226 
CHAPTER 6 
project hierarchy, with the PROJECT class at top, containing MODULEs, BINaries, 
LIBraries, and so forth. The Schema further distinguishes between a shared 
stable "master" area, and private workspaces in which developers make their 
code development/modifications. 
The Coding group performs the Analysis and Change tasks, and participates 
in the Review task. The Analyze task, depicted in Figure 6.6, consists of two 
phases. The general analysis phase (carried out by the analyze..bug rule, listed 
in the test strategy) is performed by the manager of the Coding group (who 
was previously notified by QA), and he uses the bug report generated by the 
QA team along with its own source tree (thus, it is a Summit rule because it 
contains a parameter from a remote site). The manager tries to assess where the 
problem is (if there is any), ending up selecting a set of "suspected" source files. 
The second phase of the Analyze task consists of local forward implications at 
both the QA and the Coding groups. At the Coding group, each suspected 
file triggers an instance of a local overloaded version of the analyze_bug rule 
which is delegated to the file's "owner" (ownership is determined by the value 
of an attribute of the CFILE class). This rule provides the developer with all the 
past information that is relevant to the bug (e.g., reports from the QA group 
and from the manager), and the developer has to produce a change request 
report. At the QA group, the Summit triggers local chaining to the add_bonus 
rule which credits the person who found the bug. This is an example of a 
simple multi-site Summit rule that is integrated in both processes and triggers 
different local activities. 
Analyze is followed by the Review task, which is performed jointly by the Cod-
ing and the Design groups. This is the most interesting task from the process-
interoperability standpoint, and its discussion is deferred to Section 6.2.4. 
The Change task is most comprehensive in terms of activities involved, but 
is essentially a local one. It is a scaled-down version of the real Marvel pro-
cess which has been employed for the production of Oz, based on a check-out 
model, where developers check-out (check-in) artifacts from (to) the stable mas-
ter area to (from) their private workspaces. Although mostly local, this task 
contains a single multi-user multi-process step, namely code inspections. This 
step involves one participant from Design and one from Coding. Other steps 
in the Change task include: interaction with the configuration management 
5The actual synchronous multi-user inspection tool, developed in-house by Heineman and 
Skopp, defines one moderator and other participants, all of which are virtually sharing the 
same emacs buffer, in the sense that when the moderator moves its cursor it also moves it 
in the other participants. The tool also incorporates audio for verbal communication during 
the inspection process, and video stills to identify the participants. 

The ISPW Example 
QA 
Coding 
I 
..-, .-=-dd-::-:-bo-n-us-', I 
I 
I 
I 
I 
I 
I 
I 
I , 
• • • 
Delegation 
• 
Summit Rule 
Figure 6.6 The Analyze Summit Task 
227 

228 
CHAPTER 6 
Coding 
Design 
Delegation 
11111111111111 
Multi-user tools 
• 
Summit Rule 
Figure 6.7 The Review Summit Task 
sub-system, including sophisticated support for version branching and merg-
ing which was built on top of RCS [125]; tools for editing, compiling, building 
and debugging the code fix, automation support for parts of the process and 
consistency propagations (see Appendix B.2.2 for code listing). 
The final step in the Change process is the install...bin Summit step that 
involves the QA and Coding processes, providing the QA team with the newly 
created binaries for (re)testing. Thus, the overlap of the Change Task with 
Design and QA as shown in figure 6.1, is due to the "isolated" inspect jon and 
binary-installation steps, respectively. The task is, however, mostly local. 
6.2.4 
The Design Process 
The main artifacts in the Design process are of class DESIGN..DOCUMENT (which 
is a specialization of the FILE class), which can be grouped under objects of 
class DESIGN..DOC. 

The ISP W Example 
229 
The Review task (shown in figure 6.7) is the only Summit task in which the De-
sign process participates. It is initiated with the invocation of the setup..xeview 
Summit rule. This rule collects the artifacts which are necessary for the review, 
including the change proposal produced by the Analyze phase. Then, the Sum-
mit fans-out to both sites for local (and delegated) reviews which are performed 
in parallel. Each reviewer summarizes his/her results and recommends whether 
to approve the proposed change, revise it, or reject it altogether. Once both 
reviews complete, then if both succeeded, the approve Summit rule is invoked 
automatically and completes the task (see below). Otherwise, if at least one 
of the reviews failed, the multi-user, multi-process conference rule is invoked, 
setting up both reviewers with a groupware tool called white..board6 that al-
lows them to discuss and reconcile their conflicts with respect to the proposed 
change. At the end of this step, there are two possibilities: either a recommen-
dation for revision is made, or the change is rejected. In the latter case the a 
reject rule is called and the task terminates. In the former case, the conference 
rule leads to a local (delegated) revise step at the Coding process, in which 
the responsible developer(s) try to generate an improved change request. The 
conference rule is shown in Figure 6.8. Note how the rule binds the activity 
to two users based on objectbase information that relates them to the reviewed 
documents (lines 8-10); also note how the rule's condition "requires" at least 
one failed review (line 14-15); finally, note the two effects that correspond to 
the two possible outcomes mentioned above (lines 19-26). 
The Review task is iterative, in that following the local revise step, the 
setup..xeview Summit rule is automatically invoked to start a second round of 
reviews, and so forth. If the Review task completes successfully, the approve 
rule automatically triggers the Change task, by automatically checking-out 
the faulty modules that need repair to the workspaces of the respective de-
veloper(s). Thus, this is a case of an automatic transition between high-level 
tasks. This example of a composite Summit shows the versatility of the inter-
operability mechanism, and particularly how it can be used to model "process 
negotiations" , where the processes essentially interact with each other with the 
intention to reach an "agreement". 
6.2.5 
Treaty Definitions 
We now turn to discuss Treaty definitions for the ISPW example. In general, 
the decentralized model was followed and its associated mechanisms were used, 
6 This is a public-domain tool that enables multiple remote users to share a virtual white-
board on their screens. 

230 
CHAPTER 6 
# 
conference: 
# 
This Summit rule is invoked when some local review fails. 
# It forward chains to *local* revise at the CODING process, 
# 
and to the *Summi t* rules set_up..review and approve. 
# 
1) 
conference [?c: CFILE, ?design..doc :DESIGI.DOCUMEIT] : 
2) 
(and 
3) 
(forall GROUP 
?coding suchthat (ancestor [?coding ?c])) 
4) 
(forall GROUP 
?design suchthat (ancestor [?design 
?design..doc]) ) 
6) 
(forall MODULE ?m 
suchthat (member [?m.cfiles ?c])) 
6) 
?d] )) 
7) 
8) 
9) 
10) 
11) 
12) 
13) 
14) 
is) 
16) 
17) 
18) 
19) 
20) 
21) 
22) 
23) 
24) 
26) 
26) 
(forall DOCFILE ?d suchthat (member [?design..doc.docfiles 
# bind the relevant participants to ?p 
(forall WORKSPACE ?p suchthat (or (linkto [?p.module ?m]) 
(linkto [?p.doc ?design..doc])))): 
delegate[?p.owner]: 
(and 
no_backward (?d.review~tatus = ReviewRequested) 
no_backward (?c.review~tatus = ReviewRequested) 
(or no_backward (?d.review..rc = Failed) 
no_backward (?c.review..rc = Failed))) 
# invoke the multi-user whiteJboard tool 
MU_TOOLS confer ?c.contents ?d.contents 
?coding. site.ip_addr ?design. site.ip...addr 
# O. 
chain to revise and enable setup..review 
(and (?c.review~tatus 
= RevisionRequested) 
# double assertion to 
(?c. bug~tatus 
(?c. bug~tatus 
enable chaining to setup..review 
= Suspected) 
= Defected)); 
# 1. reject. need to start allover again. 
(and (?d.review~tatus = ChangeRejected) 
(?c.review~tatus = Rejected)); 
Figure 6.8 
Conference Rule 

The ISPW Example 
231 
both for the initial definition as well as for incremental evolution of Treaties. 
Following the model, then, it is clear that any Summit rule discussed above 
must have been defined by a Treaty before it could be properly executed across 
sites. However, it is not immediately clear where should the Treaty rules origi-
nally be defined, nor is it clear how to assign execution privileges (i.e., who can 
run Summits and on what data) to the Treaty rules. Finally, another classifi-
cation of Treaties could be made along the lines of the kind of interoperability 
obtained by the Treaty. By definition, any Treaty provides for interoperability 
at the process and data levels. However, some Treaties also provide for "user-
level" interoperability by means of multi-user tools. In fact, our Treaty and 
Summit mechanisms enabled us to implement sophisticated groupware tools 
with relatively small overhead, which could be regarded as one of the impor-
tant by-products of this research (see more in Chapter 7). 
The Analyze task should be executed only from the Coding process, since an-
alyzing the bug is the responsibility of the developers. In addition, most of the 
involved artifacts reside in the Coding process, so the multi-site analyze...bug 
rule should also be defined at the Coding site. Thus, the (simple) Treaty is 
defined from Coding to QA, with Coding as the requester and QA the acceptor. 
Analyze involves only "standard" interoperability, i.e., at the process and data 
levels. 
The Review task is more symmetric, and could potentially be both defined 
and executed from either the Coding or the Design process, with slight pref-
erence to Design on both accounts, due to the fact that Design has more im-
pact on the outcome of the Review. Thus, it was defined with Design as the 
exporter-requester and Coding as the importer-acceptor. Recall that not all the 
steps shown in the Review Summit were actually defined as Treaty rules, only 
the multi-site rules, namely setup...review, conference and approve. Finally, 
Review is an example of a task with a "user-level" interoperability, obtained 
through the conference rule. 
The other two Summit steps, namely code-inspection and binary installation, 
are isolated within the otherwise local Change task, and so should be defined 
in Coding as exporter and requester, with the other sites being importers and 
acceptors. 
Table 6.1 summarizes the multi-process steps in the solution, along with infor-
mation about the execution privileges set in the corresponding Treaties, and 
whether the interoperability involves process, data, and users (denoted by p, d 
and u in the interoperability column). The last column indicates whether 
the Summit is composite or not. 

232 
CHAPTER 6 
QA 
CODING 
DESIGN 
interoperability 
composite 
Analyze 
imp-ace 
exp-req 
p,d 
no 
Review 
imp-ace 
exp-req 
p,d,u 
yes 
Inspection 
exp-req 
imp-ace 
p,d,u 
no 
InstalLbin 
imp-ace 
exp-req 
p,d 
no 
Table 6.1 
Treaties in the ISPW Process 
Several observations can be made with respect to Treaties and Summits in the 
process: 
• 
All Treaties (and subsequently, Summits) in the process were binary (i.e., 
among two sites). However, while it seems that binary Treaties were indeed 
the natural solution for all inter-site interactions in this example, there is 
nothing in particular that prevents from forming Treaties among multiple 
collaborating sites. 
• 
With the exception of the Review task, all Treaties in the example were 
defined in the Coding process, and more importantly, the Coding site was 
the sole requester. This observation is a bit misleading, though, since 
it stems mainly from the lack of people who could code Treaties in a 
timely manner. While one might argue that this might be the case in a 
real-world example, this example would have been defined differently un-
der normal circumstances, with Treaty definitions and execution privileges 
spread more uniformly over the local processes. 
• 
With the exception of the Review task, there was no need for full Treaties. 
Moreover, there was actually a need to explicitly distinguish execution 
privileges (for example, in the case of the Analyze task). This validates 
our approach to enabling a refined definition of Treaties as opposed to 
allowing only definition of Full Treaties. A related observation is that all 
Treaties were defined with the import-accept and export-request combina-
tion, which verifies our intuition towards specifying the default modes for 
Treaty. 
• 
Some of the Treaties (e.g., the Analyze strategy) were used to model task 
transitions. This gives a new perspective on the role of Treaties, namely 
as an active mediation between (sub )processes, whereby processes "meet" 
each other with their interfaces to facilitate the transition from one process 
to another. Task transitions are not the only reason for Summits, however 
(as evidenced by the code-inspection tool, for example), so one should not 

The ISPW Example 
233 
QA 
CODING 
DESIGN 
BUILT-IN 
TOTAL 
SEL 
360 
1787 
163 
346 
2656 
MSL 
270 
1233 
58 
233 
1794 
SCHEMA 
46 
256 
30 
95 
427 
Table 6.2 
Sununary of Lines of Code for the ISPW problem 
draw the conclusion that interoperability is used only for task or process 
interface purposes. 
6.2.6 
Statistics and Summary of Solution 
The entire solution was designed and coded by four people7 over nine days. Al-
though it appears that the process has been outlined top-down, it was mainly 
because the Scenario was defined without consideration of autonomy; most of 
the subtasks were defined autonomously and in a decentralized manner, with 
Treaties defined in most cases after the local sub-processes have been estab-
lished. Each process was developed by a different person with the Treaties 
designed and implemented by the designers of the relevant SubEnvs. A sum-
mary of the total lines of code per process (including comments, which on 
the average account for about half of the lines of code in strategies) and their 
breakdown to schema, rules (MSL) and envelopes (SEL), is given in table 6.2. 
The amount of Treaty code (i.e., strategies and envelopes that were part of 
Treaties) was 375 lines of MSL code and 344 lines of SEL code, accounting for 
about 15% of the overall code. The actual execution time of Treaty rules was 
small relative to local executions, too. Thus, in general most of the work was 
performed locally. 
To make the process realistic, it was fully instantiated and enacted with a 
real product, which was "ozified" into an Oz environments. Similarly, all 
the test cases were valid, as well as the other artifacts that were maintained 
in the objectbases. All processes underwent numerous evolutions during the 
development cycle, both local and inter-site. 
7The author, George Heineman, Peter Skopp and Jack Yang. 
8 A tool that aids in migrating file-system-based environments into Oz is currently under 
development, based on the Marvelizer [119] tool which migrated artifacts from the file sys-
tem into a Marvel environment. We already employ a separate tool for upgrading Marvel 
environments into (single-site) Oz environments, and intend to combine both functionalities 
into the ozify tool. 

234 
CHAPTER 6 
The following is a summary of the highlights of the ISPW solution: 
• 
Three independent yet cooperating processes were constructed, assisting 
in the development of a multi-team software project, with control and data 
interleaving from being private for local work and shared for "global" work, 
as needed. 
• 
Enforcement and automation were the main forms of enactment support. 
For example, enforcement of the constraint that prevents from deposit-
ing code before undergoing code-inspection9 . An example for automation 
support was the automatic testing task. 
• 
Support for both engineering tasks (e.g., the local Change task) and man-
agerial tasks (e.g., assignment through delegation). 
• 
Support for modeling and executing user-delegation, with emphasis on 
dynamic user binding as explained in Section 4.5. 
• 
Support for modeling, executing, and integrating multi-user tools, includ-
ing both in-house and off-the-shelf tools. 
• 
Several additions, removal, and modifications to Treaties on the fly were 
made for evolution purposes (not described in this chapter). 
• 
The modeling itself was decentralized, which contributed significantly to 
the effectiveness of the process-modeling process. 
To conclude, the main validation was in the very fact that it was possible to 
fully implement in Oz an effective and comprehensive solution to the ISPW-
9 example, both in terms of modeling as well as enactment, and including 
the multi-process and groupware extensions. This should be regarded as an 
important acceptance criteria by itself (both conceptual and technical), given 
that there are very few (if any at all) other PCEs with similar capabilities. A 
detailed evaluation is given in Chapter 7. 
6.3 
METHODOLOGY ISSUES 
The main issue to explore here is, what is a recommended way to define de-
centralized processes. One of the objectives of this whole experiment was to 
9 This constraint was also implemented in our production Marvel process that supports 
the development of Oz, after it was realized that code was routinely deposited prematurely. 

The ISPW Example 
235 
observe not only the resultant environment, but also to observe the modeling 
meta-process and deduce from it a general methodology for defining processes. 
By-and-large, this is a topic for further investigation, as the experience we have 
had so far in modeling and enacting multi-site processes has been quite limited. 
Nevertheless, based on the conceptual framework of our approach, our rich 
experience with modeling single-site processes in Marvel, and the experiments 
we have conducted in modeling and enacting decentralized processes -
several 
observations can be made, as well as recommendations on how to approach 
modeling. 
The focus of this section is on methodology for the design and implementation 
phases. Although requirement specifications and analysis of the process are 
important phases (as in any other software engineering undertaking), method-
ology for these phases is beyond the scope of this work. More specifically, the 
emphasis here is on modeling interoperability in multi-site environments and 
the impact on the overall modeling, as opposed to modeling stand-alone pro-
cesses in a single-site environment. The following is a set of issues that have to 
be addressed when considering to build a multi-site environment instance: 
1. The first issue to consider is whether a multi-site environment is at all 
necessary. 
In cases where scale and heterogeneity are reasonably con-
tained, and physical and organizational boundaries do not exist (or can 
be somehow eliminated) there is probably no good reason for dividing an 
environment, in which case a single-site, single-process environment may 
suffice. 
2. The next issue to consider is how to divide an environment into sub-
processes. Several factors impact this division. As pointed out back in 
Chapter 1, in some cases the division is a given constraint, not a design 
consideration. For example, when some or all of the the SubEnvs already 
pre-exist, or when physical and/or organizational boundaries pre-define 
the division. 
However, in cases where the designer(s) have some control over the division, 
two main axes for division can be identified: 
• 
Project-based decomposition -
The scale of the project and its com-
plexity require to decompose it into a set of sub-projects and a cor-
responding set of groups, whereby each group is internally hetero-
geneous and is responsible for the complete development of a sub-
project. 

236 
CHAPTER 6 
• 
Task-based decomposition -
Here the project personnel is divided 
into groups such that each group is more homogeneous internally, 
has a designated role and is responsible for certain tasks within the 
overall global project. An example of such a division is the ISPW 
solution, where the processes corresponded to the QA, Coding, and 
Design tasks in the overall process. 
While both decompositions are motivated by scale and heterogeneity, the 
former is slightly more associated with scale and the latter with hetero-
geneity. In any case, the two approaches are not mutually exclusive and 
some environments might employ a combination of both 10. 
3. Once the division to SubEnvs exists (or if pre-existed), the next issue to 
address is how to actually model the processes. There are several non-
functional requirements that might constrain or otherwise impact inter-
process modeling: 
• 
A major constraint is bandwidth. The effective bandwidth between 
every two SubEnvs should be considered when designing Summits, 
and should be minimized when the bandwidth is low. 
• 
Another related consideration is whether the SubEnvs share a file 
system. In the case they do not, Summits should be defined so that 
the amount of transfer of bulk data (e.g., files) is minimized. 
• 
Time shifting (across geographically dispersed teams) should also be 
considered when modeling processes, particularly regarding process 
steps that depend on timing constraints. For example, synchronous 
multi-user activities should be restricted to times of day when there 
is an overlap in the working hours. 
4. Another dimension of the actual modeling is related to the external require-
ments which were partially mentioned above. For example, pre-existing 
SubEnvs necessarily imply a bottom-up approach to modeling, with inter-
site connections defined on top of the pre-existing (sub )processes, and with 
a high degree of process autonomy. Another consideration that might im-
pact modeling is the nature of the organization. For example, a centralized 
organization might require a top-down approach to modeling multi-site en-
vironments, and might also limit site autonomy. 
With all this in mind, we turn now to discussing a methodology for building 
an instance of what we consider a "mainstream environment", where the ex-
laThe natural extension to a hierarchy of SubEnvs was originally proposed in [17], but is 
beyond the scope of this book. 

The ISPW Example 
237 
ternal constraints are minimal and most of the major modeling aspects can be 
determined by the designer( s). 
6.3.1 
Approach to Modeling 
Our recommended approach to modeling multi-site processes can be classified 
as a hybrid between bottom-up and top-down. The approach is also consistent 
with the general decentralized philosophy, and resembles the enactment of a 
Summit. That is, modeling is carried out by alternating between local mode -
where the local processes are defined -
and global mode -
where Treaties are 
defined (and integrated within each local process), generally oriented towards 
maximizing locality. 
More specifically, an Oz environment should be built by alternating between 
two orthogonal and interleaved iterations: 
1. data-process -
First, an initial schema should be defined, mostly covering 
the definition of the product data. Next, the process definition comes, 
which in turn requires enhancements to the schema, mostly for adding 
state attributes. This iteration continues until the process stabilizes, and 
can also be made after the process is enacted, as part of process evolution. 
2. local-global- First, the private data and process should be defined (unless 
they pre-existed), followed by the definitions of the data and process to 
support site-interoperability, which in turn lead to enhancing the private 
schema and process to integrate these steps, and so on. Again, this itera-
tion continues until both the local processes and their interactions stabilize, 
and can also be refined later when the enacted process is evolved. 
The ordering of steps to perform across these two axes depends on several 
factors, such as whether some of the local processes were pre-defined, the degree 
of interactions between the process, and so forth. Under minimum constraints, 
though, the recommended order is: 
1. Define the local schema( s) (if not already pre-exist) 
2. Define the shared sub-schema(s) 
3. Define the local process( es) (if not already pre-exist) 

238 
CHAPTER 6 
4. Define the shared multi-site process(es) (Treaties) 
An important recommendation here that might be perceived as a divergence 
from the bottom-up decentralized approach is that if possible, the definition of 
shared sub-schema should come before the definition of local processes. Based 
on our experience, it proved much easier to lay the shared schema foundation 
before working on the local processes, as it facilitated a smoother composition 
of Treaties over the local processes. More discussion on the "globality" of data 
definition is given in Chapter 7. 
To summarize, this section attempted to provide some methodology for model-
ing multi-site processes. However, the importance of this section goes beyond 
providing implementation tips and guidelines for designing specific Oz pro-
cesses. The main research point was to assert that the approach for the (decen-
tralized) modeling process (or meta-process) is very much tied to the approach 
for (decentralized) process modeling, and that similar arguments, models, and 
techniques, are applicable to both. This assertion was verified through the 
ISPW example. 

7 
SUMMARY, EVALUATION, AND 
FUTURE WORK 
The main purpose of this research was to investigate the wide range of issues 
that are concerned with introducing decentralization (both inherent and by 
choice) into the process modeling and enactment research domain. To achieve 
this goal: 
• 
A conceptual framework was built in the form of a generic (i.e., language-
and system-independent) model that supports the definition, evolution, 
and execution of multiple autonomous and heterogeneous yet interoperat-
ing processes. 
• 
A technological framework was constructed to investigate the application 
of the model, as well as to validate, evaluate, and provide feedback to 
improve the conceptual model. 
Two key concerns guided this research: (1) maximization of local autonomy, 
both physically and logically, so as not to force a priori any global constraint 
on the definition, execution and operation of local sites, unless explicitly spec-
ified in a particular environment instance; and (2) flexibility and fine-grained 
control over the degree of interoperability. As would be expected, these two is-
sues are central in the domains of decentralized systems and process modeling, 
respectively. 
The essence of the approach to address decentralization was to extend the no-
tions of process modeling and process enactment to inter-process modeling and 
inter-process enactment, respectively. The former was achieved by the Treaty 
model. In essence, Treaties are abstraction mechanisms that allow to specify 
shared sub-processes for interoperability purposes while retaining the privacy 

240 
CHAPTER 7 
of the local sub-processes. Treaties have several unique characteristics: First, 
they require explicit and active participation of the involved entities to mutu-
ally agree on the nature of the interoperability, thereby balancing autonomy 
and global specification. Second, the definition of Treaties is fine-grained, in 
two respects: (1) they are defined pairwise, between every two sites that need 
to interoperate, as opposed to being global and known in all sites of a multi-
site environment; and (2) each Treaty is formed over a single and small sub-
process unit. Still, complex Treaties can be formed (and subsequently executed) 
between an arbitrary number of sites (not only two) and involve arbitrarily 
large sub-processes, by successive invocations of simple Treaties (which could 
be optimized from user interface perspective, as discussed earlier). The third 
characteristic of Treaties is that they are superimposed on top of pre-existing 
processes as opposed to being specified as part of each individual process; this 
enables gradual and incremental establishment of interoperability and supports 
the decentralized bottom-up approach. Fourth, they are designed to support 
local evolutions including unilateral retraction from Treaties (combined with 
dynamic Treaty validation), on demand. 
Inter-process enactment was achieved by the complementary Summit model. 
Summits are execution abstraction mechanisms for Treaty-defined sub-processes. 
They support "global" execution of shared sub-processes involving artifacts 
and/or users from multiple sites, while maximizing local execution of related 
private sub-processes. This is done by successively alternating between shared 
and private execution modes: the former is used for the synchronous execution 
of the (fine-grained) shared activities, involving artifacts and/or users from 
multiple sites, and the latter is used for the autonomous execution of any pri-
vate subtasks emanating from prerequisites and consequences of the shared 
activities. The generic decentralized model, which was theoretically applied to 
various families of PMLs, is the primary contribution of this research. 
The second major contribution of this research is the application of the model 
to Oz. This work filled gaps left by the generic model, by discussing solutions 
to various issues that are lower-level but are nevertheless still applicable to a 
wide range of PMLs and PCEs, such as specification of common sub-schemas 
in an object-oriented database, and associating transaction semantics with the 
model. In addition, the realization contributed significantly to the design of 
the model itself. Indeed, it would be naive to assume that the choice of the 
rule-based PML and the basic architecture of the PCE had no impact on the 
general model, and it is no coincidence that the rule paradigm fits comfort-
ably with this model. However, the rule-based modeling paradigm seems to be 
well-suited for decentralized modeling in its own right, regardless of our partic-
ular model, since rules do not require top-down definition and more often are 

Summary, Evaluation, and Future Work 
241 
modeled in bottom-up fashion; they are loosely and implicitly interrelated (like 
decentralized processes); and they are context-less and fine-grained, allowing 
to minimize the impact of any interoperability mechanism (e.g., Summit) on 
the local processes. Finally, the fact that the system was fully implemented 
enabled us to build real environment instances with real processes as a ba-
sis for validating the model, and allowed us to explore "meta" issues such as 
methodology for defining decentralized processes. 
The third major contribution of this research was in the investigation of infras-
tructure (or architectural) facilities to support a high degree of interconnectivity 
despite the "no-sharing" and non-transparency requirements that are essential 
for enabling physical decentralization. The general direction in most cases was 
to follow the "lazy-update" approach and tolerate a possibly temporary skewed 
view of the global state but add facilities that allow to both indicate this in-
consistency (with some additional overhead) and repair it, on demand. Such 
was the case, for example, with the semi-replicated connection database and 
the remote object cache management. 
There are two contributions which can be viewed as by-products of this re-
search, but are nevertheless important and form the basis for future research 
(see Section 7.2). The first is preliminary integration of groupware and pro-
cess technologies. The concept of formally defining (modeling) collaboration 
as process steps and subsequently assisting in the execution of collaborative 
tools can be investigated regardless of decentralization. However, this research 
brought groupware to the forefront as the need for its use in processes became 
evident (as in the ISPW example). And at the same time it was realized that 
groupware "as-is", not integrated in a framework, and without modeling and 
enactment support, is limited too. The fact that groupware is particularly 
attractive for supporting physically dispersed users, means that decentralized 
and heterogeneous environments are a realistic setting in which to consider such 
integration. Moreover, proper infrastructure facilities for environment decen-
tralization can be utilized to enable support for the integration of such tools 
into the environment framework. In other words, it seems that (system) inter-
connectivity enables (process) interoperability which in turn enables (human) 
collaboration. Thus, the marriage of process modeling and CSCW with the 
"blessing" of decentralization seems particularly attractive. 
The second by-product was the use of a process model for site configuration 
purposes. There were several interesting aspects to this approach. First, it was 
an example of modeling and enacting an other-than-software process. Second, it 
required to push the PML to its limits with respect to its capabilities to access 
and manipulate low-level system facilities such as communication protocols; 

242 
CHAPTER 7 
the investigation of the relationship between a PML and its underlying peE 
(the "process machine") is in itself an interesting research topic. And third, it 
gave an opportunity to explore the possible domain of modeling decentralized 
systems (see Section 7.2). 
7.1 
EVALUATION 
We now turn to specific evaluation of the results of this research by considering 
how the decentralized model fulfills the requirements from Section 1.51. Most 
of these issues have already come up in one way or another, so this is mainly 
a consolidation of them. We consider here both the formal model and the 
realization. 
1. Locality -
To a large degree, this requirement was met, both in the generic 
model and in Oz. The model was specifically designed to minimize the 
impact on local work. In particular, the approach of gradually superim-
posing interoperability on top of the underlying (possibly pre-existing and 
enact able) local processes, maximizes locality. As far as the impact of 
decentralization on the quality and performance of local work -
this is-
sue seems to have been successfully met, too. The overhead imposed by 
Oz on local work in a SubEnv compared with, say, work in an equivalent 
Marvel environment, is negligible. This is because the infrastructure over-
head is directly proportional to the degree of interoperability, so with no 
interoperability there is no overhead. 
Another aspect that promoted locality both in the model and in the real-
ization was the general evolutionary research approach, which, in analogy 
to decentralized systems, was built on top of pre-existing conceptual and 
technological foundations (i.e., the Marvel research and system, respec-
tively). Indeed, one of the big pay-offs of this approach was the ability to 
thoroughly investigate and evaluate decentralization relatively quickly. 
2. Autonomy and independent operation -
Throughout the book, we have 
seen numerous cases where autonomy played a major role in determining 
the design of the model and the system. Perhaps the major aspect that ful-
fills this requirement is that site autonomy is the default and is guaranteed 
unless explicit specification of interoperability is made. This is in contrast 
to (typically distributed) systems in which the shareability is the default, 
and some work has to be done to protect the privacy of individual sites. 
1 Some of the requirements are coalesced here. 

Summary, Evaluation, and Future Work 
243 
Autonomy-by-default is closely related to enabling independent operation, 
but includes also definitional and execution aspects. Regarding definition, 
the schema, process, and database are all by default autonomous. The 
fine-grained modeling of Treaties contributes also to autonomy since each 
site can control precisely what is shared and what is not. The loose com-
mitment to a Treaty that enables unilateral retraction further supports 
autonomy, even though it incurs some performance overhead in dynami-
cally verifying Treaties at runtime. Regarding execution, the general idea 
in supporting autonomy was to minimize the impact of interoperability 
beyond what was explicitly defined as shared, and to maximize local exe-
cution. Most of these arguments hold equally well to the generic model as 
well as to Oz. 
The tension between supporting autonomy and enabling facilities for in-
teroperability have led to some oversights regarding autonomy, however, 
mostly in the Oz realization. One ofthem is the global object base browsing 
facility. While powerful in its ability to visualize whole remote objectbases 
and control their refresh policies, this mechanism is provided by default 
and does not require explicit definition. Global browsing has two func-
tionalities -
the ability to view the contents of individual objects, and the 
ability to view and browse through the object base structure. The latter 
service cannot be disabled in Oz, and an improvement of this design might 
consider facilities for controlling the degree of browsing. The former can be 
controlled by specifying proper access-control permissions, but given that 
these permissions are optional, this too might be viewed as a violation of 
autonomy. Other built-in services provided by the kernel might also need 
to be improved in similar ways (e.g., cross site copy). 
3. Interoperability -
Given that autonomy was a crucial requirement, this 
"competing" requirement seems to also have been adequately addressed. 
Specifically, the execution semantics (and the corresponding infrastructure 
in the realization) support well the enactment of activities involving data 
from multiple sites. The additional facilities for modeling and enacting 
delegation and groupware tools facilitate collaboration, too. And finally, 
the infrastructure support and the built-in services enable interoperability. 
One area where some improvements might be needed is better support for 
global operations (for process and schema), especially in tightly-coupled 
environments, where autonomy can be compromised. The Treaty oper-
ation as a single command (with the issuer being administrator in both 
sites) was a step in that direction. Other possible improvements in process 
modeling include commands for defining multi-site Treaties, more selec-
tive Treaty invalidation procedures that do not invalidate Treaties unnec-
essarily, automatic updates of strategies without requiring to re-establish 

244 
CHAPTER 7 
Treaties, optional consistent Treaty withdrawals (along the lines of Sec-
tion 3.7.2, and so forth. 
Another issue to consider is alternatives to the Summit model of interop-
erability, in which interoperability is attained through shared activities. 
One alternative is message passing between multiple local activities that 
execute simultaneously. 
4. Support for pre-existing and heterogeneous processes -
This is an extension 
of the previous requirement, since the requirement is to support interop-
erability over such heterogeneous processes. As already outlined above, as 
far as process is concerned, both Summits and Treaties were designed with 
this requirement in mind, and proved to be quite effective. It is of course 
possible and even likely that two pre-existing and unrelated processes will 
have no common sub-process a priori. But "bridges" of interoperability 
can be incrementally added, with minimal distractions to local work. The 
situation with pre-existing schemas is less satisfactory, however, particu-
larly with respect to strongly typed PMLs. Such PMLs should provide 
facilities that enable to superimpose new shared sub-schemas on top of the 
pre-existing ones (perhaps along the lines of what is done in Pegasus [35]). 
Alternatively, PMLs might need to sacrifice some of their typing restric-
tions, at least for Summit rules, to accommodate heterogeneous schemas. 
5. Infrastructure support -
The comprehensive infrastructure that was built 
in Oz to support interconnectivity seems to have addressed the "conflict-
ing" independent operation requirement. For example, the Connection 
Server as an auxiliary entity for (re)establishing connections across sites 
and for (re)activating servers enabled sites to operate independently but 
acquire the necessary information when they needed to communicate with 
other sites. The main open issue here is proper modeling support for op-
eration over a wide-area network (see Section 7.2). 
7.2 
FUTURE DIRECTIONS 
7.2.1 
Modeling of Decentralized Systems 
This book explored how decentralization impacts process modeling. An inter-
esting topic to explore is the opposite direction, namely, how modeling and 
enactment can be applied to decentralized systems. It seems that the idea of 
describing the behavior of autonomous entities formally, as a basis for con-
structing consistent and trustworthy interoperability among them, and operat-

Summary, Evaluation, and Future Work 
245 
ing within an environment that supports their execution, goes beyond process 
modeling and can be applied to general distributed and decentralized system 
design. For example, this could be used to model and subsequently support 
interoperability among autonomous Internet repositories, making them more 
active and responsive to other objects on the network. 
Some related work in this direction has already been done in the area of inter-
connection languages (e.g., Conic [91]), and in the more general emerging field 
of software architecture [43], where researchers have been looking at formalisms 
to define the structure of complex (distributed) systems (e.g. [1]). 
7.2.2 
Wide Area Modeling 
This direction has been addressed in Oz only preliminarily at best (in Sec-
tion 5.6). The main research issue here is to explore constructs for modeling 
things like physical distance, (perhaps dynamically varying) bandwidth, time 
differences and other temporal constraints, security level, location in case of 
mobile sites, frequency of inter-site interactions and more -
in order to supo-
port the optimization, synchronization and in general the operation of sites 
collaborating over a wide area network. The key issue is to use the semantics 
of the explicit specifications to improve the the enactment support. For exam-
ple, a PCE might employ data prefetching when the bandwidth is low, and use 
further information in the process model to improve the hit/miss ratio. 
7.2.3 
User Modeling, Groupware and Process 
This topic has been already discussed above, and some preliminary work in 
this direction has been already done in this research (Sections 3.6, 4.5 and 
6.2), as well as by other members in our research group [18, 127, 126]. The 
research path to follow here is to continue to increase the modeling capabilities 
and abstractions that define various aspects of collaboration, use the semantics 
of the model to enhance the enactment and the integration of CSCW tools, 
and characterize the necessary infrastructure support for supporting CSCW in 
the process, such as advanced transaction facilities and object caching. One 
particularly neglected aspect of modeling is user modeling. This has several im-
plications such as unique identification of users across sites, aliasing of "users" 
from multiple sites to the same nomadic user, security issues, assignment of 
roles, end so on. 

246 
CHAPTER 7 
7.2.4 
System and Language Heterogeneity 
To re-quote Heimbigner [50], "environments will move to looser, federated, ar-
chitectures ... address inter-operability between partial-environments of varying 
degrees of openness". We certainly agree with this assessment. 
In this research we focused on heterogeneity at the process level, and for the 
most part assumed a homogeneous underlying framework and modelling lan-
guage. Multi-formalism and system interoperability, are still, by-and-large, 
open research issues, particularly when coupled with decentralization. One 
promising approach to follow is to construct a "virtual process machine" with 
low-level services (both centralized and distributed) that is decoupled from 
a particular formalism in which process models are specified, and thus sup-
ports any formalism that can be translated into the machine's "assembly" lan-
guage. Such process "machines" (or servers) should be capable of interoper-
ating with other heterogeneous process machines using underlying standards 
(e.g., CORBA [96]), and should be able to service encoded as well as ad-hoc 
processes. 

A 
CONFIGURATION PROCESS 
SOURCES 
A.l 
REGISTRATION STRATEGY 
# 
# 
# 
# 
# 
# 
# 
# 
Oz Process Centered Environment 
Copyright 1994 
The Trustees of Columbia University 
in the City of Nev York 
All Rights Reserved 
# Registration process. This is a system built-in strategy 
strategy register 
imports data_model; 
exports all; 
objectbase 
# 
# Tool Declarations 
# 
REGISTER :: superclass TOOL; 
register_subenv 
string - register_subenv; 
deregister_subenv 
string - deregister_subenv; 
send_connect ion_db 
string = send_connectio~db; 
change_subenv_name 
string· change_subenv_name; 

248 
end 
end_objectbase 
rules 
######### 
# 
# register_subenv: 
# 
# collect static information about the new subenv, 
# and replicate it in all existing subenvs 
# 
######### 
register_subenv [?new_name:LITERAL]: 
(and 
# collect all remote SubEnv objects 
(forall GROUP ?se suchthat (?se.local = false» 
# and the local one 
(exists GROUP ?lse suchthat (?lse.local = true») 
ApPENDIX A 
# this envelope actually does the replication in remote subenvs 
{ REGISTER register_subenv ?new_name ?se.subenv_name ?se.site_name 
?se.has_nfs ?lse.subenv_name 
return ?new_subenv_id 
?new_subenv_name 
?new_site_name 
?new_ip_addr} 
# now add the object locally with the information entered 
# by the administrator 
(and 
no_chain (?new:GROUP = add [NULL NULL ?new_name GROUP]) 
(?new.subenv_id 
= ?new_subenv_id) 
(?new.subenv_name 
?new_subenv_name) 
######### 
# 
(?new.site_name 
(?new.site_ip_addr = 
(?new.local 
(?new. state 
# init_remote_subenv: 
?new_site_name) 
?new_ip_addr) 
false) 
New»; 

Configuration Process Sources 
# 
# called from vithin the resister_subenv envelope (in batch mode), 
# from all remote SubEnvs, to assign the proper values to the 
# objects vhich vere just added by register_subenv 
# 
######### 
hide init_remote_subenv [?env_obj_name:LITERAL, ?env_name:LITERAL, 
?env_id:LITERAL, 
?subenv_id:LITERAL, 
?subenv _name: LITERAL, 
?si te_name: LITERAL, 
?site_ip_addr:LITERAL]: 
{} 
(and 
no_chain (?nev:GROUP = add 
(?nev.env_name 
(?nev.env_id 
(?nev.subenv_id 
(?nev.subenv_name 
[NULL 
NULL ?env_obj_name GROUP]) 
?env_name) 
?env_id) 
?subenv_id) 
= ?subenv_name) 
?site_name) 
?site_ip_addr) 
Hev) 
(?nev.site_name 
(?nev.site_ip_addr = 
(?nev. state 
######### 
# 
(?nev.local 
# send_connect ion_db: 
# 
= false» j 
# initialize the nevly created subenv, by sending to it the 
# connection database 
# 
######### 
send_connect ion_db [?nse:GROUP]: 
# collect all SUB_EHV objects except the nev one 
(forall GROUP ?s suchthat (?s.subenv_name <> ?nse.subenv_name» 
no_chain(?nse.state = Hev) 
249 
# this envelope actually copies the connection db to the nev SubEnv 
{ 
REGISTER send_connect ion_db 
# nev subenv's identification and location 

250 
?nse.has_nfs 
?nse.Name ?nse.env_name ?nse.env_id 
?nse.subenv_id ?nse.subenv_name 
?nse.site_name ?nse.site_ip_addr 
# all information of all subenvs. 
ApPENDIX A 
?s.Name ?s.env_name ?s.env_id ?s.subenv_id ?s.subenv_name 
?s.site_name ?s.site_ip_addr 
} 
(?nse.state = Initialized); 
######### 
# 
# init_subenv_map: 
# 
# 
# called from within the envelope (by invoking a batch client) 
# of send_connection_db. this assigns to all objects of the 
# connection db the proper values. 
# 
######### 
hide init_connection_db [?new:GROUP. ?env_name:LITERAL. ?env_id:LITERAL. 
?subenv_id:LITERAL. ?subenv_name:LITERAL. 
?site_name:LITERAL. ?site_ip_addr:LITERAL. ?local:LITERAL]: 
# no condition 
# no activity 
{ } 
(and (?new.env_name = ?env_name) 
(?new.env_id = ?env_id) 
(?new.subenv_id = ?subenv_id) 
(?new.subenv_name = ?subenv_name) 
(?new.site_name = ?site_name) 
(?new.site_ip_addr = ?site_ip_addr) 
(?new.local = ?local) 
######### 
# 
(?new.state = Initialized»; 
# deregister_subenv: 
# 
# Remove a subenv. by removing its root object from all remote 
# subenvs and removing all subenvs from the subenv that is removed. 

Configuration Process Sources 
# Leave the subenv "disconnected", but don't remove it. 
# 
######### 
deregister_subenv [?lse:GROUP]: 
# collect all remote SubEnv objects 
(forall GROUP ?se suchthat (?se.local = false» 
no_chain (?lse.local = true) 
{ REGISTER deregister_subenv ?se.Bame ?se.subenv_name ?lse.Bame} 
(delete [?se NULL] ); 
######### 
# 
# change_subenv_Iocation: 
# 
# Move the physical location of a subenv. 
# ASSUMES BFS 
# 
######### 
no_chain (?se.active = false) 
{ } 
change_subenv_Iocation [?se:GROUP]: 
no_chain (?se.active = false) 
A.2 
A SAMPLE Oz ENVELOPE 
251 

252 
# 
# 
# 
# 
ApPENDIX A 
Oz Process Centered Environment 
Copyright 1994 
# 
# 
# 
The Trustees of Columbia University 
in the City of New York 
All Rights Reserved 
# 
# Register a new SubEnv. 
ENVELOPE register_subenv; 
SHELL ksh; 
INPUT 
# the input matches the paramters specified in the register_subenv rule 
string 
new_name; 
set_of string 
subenv_name; 
set_of string 
site_name; 
set_of integer 
has_nfs; 
set_of string 
local_subenv_name; 
OUTPUT 
string 
string 
string 
string 
BEGIN 
?new_subenv_id; 
?new_subenv_name; 
?new_site_name; 
?new_ip_addr; 
# get the new subenv_name 
trap '/bin/rm -f /tmp/ping.out /tmp/oz_batch /tmp/host.out' 0 1 2 3 9 15 
echo "Enter New SubEnv Name (Currently path name)" 
read new_subenv_name 
if [ "x$new_subenv_name" = "x" ] 
then 
fi 
echo "Must enter new path name" 
RETURN "1":"", "" 
1111 
Itll. , 
echo "Enter SubEnv ID (increment the largest subenv_id in the environment)" 

Configuration Process Sources 
if [ IIX$neW_8Ubenv_id" = "XII] 
then 
fi 
echo "Must enter new subenv id" 
RETURI "1":"", 
1111 . 
till . 
1111; 
echo "Enter the noe of the pruary host (usually "here the SubEnv resides)" 
read new_site_name 
/usr/etc/ping $ne,,_site_name > /tmp/ping.out 2>11 
if [ $1 !- 0 ] 
then 
fi 
cat /tap/ping.out 
RETURl
lll":"II, 
1111 . 
1111 . 
1111; 
host $ne,,_site~aae > /tap/host.out 
if [ $1 !z 0 ] 
then 
cat /tmp!host.out 
RETURI "1":"11, lilt, 
1111 
1111. 
. . 
fi 
a"k '{ if (RR -
1) {print $4}}' /tap/host.out I read ne"_ip_addr 
if [ n:z:$ip_addr" .. IIX" ] 
then 
fi 
echo "Ip address could not be resolved." 
echo "Please enter the address or <return> to abort" 
read ne"_ip_addr 
if [ "x$ne"_ip_addr" = "x" ] 
then 
fi 
echo "Registration process aborted" 
RETURI"l":"II, 
1111, 
1111, 
IIlIi 
exit 
echo "I!oz script" > /tap/oz_batch 
253 
echo "init_reaote_subenv \"$ne,,_naJIe\" \"ozl\" \"0\" \"$nev_subanv_id\" \"$nev_Bubenv_name\" \"$ne'l 

254 
rm /tmp/subenv_name 
rm /tmp/site_name 
rm /tmp/has_nfs 
rm /tmp/combined 
> /dev/null 2>11 
> /dev/null 2>t1 
> /dev/null 2>11 
> /dev/null 2>11 
for i in $subenv_name 
do 
echo $i » /tmp/subenv_name 
done 
for i in $site_name 
do 
echo $i » /tmp/site_name 
done 
for i in $has_nfs 
do 
echo $i » /tmp/has_nfs 
done 
ApPENDIX A 
# if only the local site exists skip this, because shuffle will 
# bark on this 
if [ "x$subenv_name" != "x" ] 
then 
shuffle /tmp/subenv_name /tmp/site_name /tmp/has_nfs > /tmp/combined 
if [ $? != 0 ] 
then 
echo "register_subenv failed during shuffle" 
RETURI "1": 1111 t 
1111, 
1111, 
fi 
1111. , 
nr='awk 'END {print NR}' /tmp/combined' 
else 
nr=O 
fi 
ctr=l 
# add the new root object to all other SubEnvs 
while [ $ctr -Ie $nr ] 
do 
cur_subenv_name='awk '{if (NR == '$ctr') print $1} 
cur_site_name='awk '{if (NR == '$ctr') print $2} 
, /tmp/combined' 
, /tmp/combined' 

Configuration Process Sources 
cUI_has_nfs='awk '{if (NR == '$ctr') print $3} 
if [ $cUI_has_nfs = "1" ] 
then 
oz_tty -a -b /tmp/oz_batch $cUI_subenv_name 
else 
, /tmp/combined' 
oz_tty -a -r $cUI_site_name -b /tmp/oz_batch $cUI_subenv_name 
fi 
ctr='expr $ctr + l' 
done 
rm /tmp/ping.out /tmp/oz_batch /tmp/host.out 
255 
echo " •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• " 
echo " 
First step of registration succeeded!" 
echo" The registration process does not set the has_nfs attribute." 
echo" Set its value in all SubEnvs using the set_has_nfs rule." 
echo " Then proceed with the send_subenv_map rule" 
echo " •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• " 
RETURN "O":"$new_subenv_id", "$new_subenv_name", "$new_site_name","$new_ip_addl 
END 

B.l 
B 
THE ISPW-9 PROBLEM: 
DEFINITION AND SOLUTION IN 
Oz 
THE ISPW-9 EXAMPLE 
Life-cycle (Sub) Process Demonstration Scenario 
9th International Software Process Workshop (ISPW9) 
March 1994 
Maria H. Penedo (ISPW9 Example Chair) 
Base Scenario for Demonstration 
Problem Reporting and Change Process 
• 
A software project is on-going, with" parts" of the system already designed, 
codified, tested and baselined (i.e., under configuration management con-
trol). 
• 
A problem is reported by a tester on the testing of a piece of the system 
under development. The project's problem reporting and analysis proce-
dures are then followed and a person is assigned the task of the analysis 
of the problem. (Note: these procedures can be formal or informal, de-
pending on the type of project. Notification can be effected by mail, by 
forms, by a tool. The procedures may include rules or guidelines telling 
who assigns people resources to study which problems and what kind of 
steps need to be followed.) 
• 
A developer/analyst analyzes the problem and proposes a solution. Af-
ter the analysis (which can be illustrated via automated process support 
or assumed to have been done manually), the developer identifies that 
the problem affects one software module which has been coded, tested 
and baselined, and possibly also affects some documentation (e.g., de-

258 
ApPENDIX B 
sign and/or testing documents). (Note: the related documentation can be 
identified explicitly with help from the system, or implicitly via existing 
predefined rules in the system). 
• 
After some analysis, it is noted that the module to be fixed is currently 
being (re-)used by two separate users or teams (again how this is accom-
plished may vary, i.e., the system may flag this issue or this fact may be 
found explicitly by inspection by a configuration manager or the devel-
oper). Those users are notified of the problem and that the module will 
be changed. 
• 
The change process starts according to pre-established change procedures 
(which entail assignment of resources, code and/or documentation mod-
ification, analysis/testing/review, approval/rejection and new baseline of 
the module and associated documentation). 
• 
The module is checked out of the baseline according to the configuration 
management procedures for change but reuse of the old version continues. 
• 
The module is changed to fix the problem. (Optionally, the fix could be 
done by two or more separate developers and their cooperation may be 
illustrated via process support). 
• 
The module is tested (formally or informally). Once the problem is fixed, 
procedures for acceptance/rejection are followed. Once the module is ac-
cepted (i.e., the change does fix the problem and it does not violate any of 
the requirements), appropriate regression testing on the modules/systems 
which reuse a prior version of this module can be performed. 
• 
Once all is done, the change process is finalized. 
B.l.l 
Sub-scenarios 
1. Specify and demonstrate one or more specific procedures/policies to com-
plement the scenario (preferably performed with automated process sup-
port): 
• 
problem reporting and/or analysis 
• 
testing procedure/method 
• 
analysis of a problem using data in system 
• 
configuration control: retrieval, storage 
• 
code fix 

The ISPW-9 Problem: Definition and Solution in Oz 
259 
• 
problem approval/rejection 
• 
resource allocation 
2. User role support -
Demonstrate implicit/explicit support for project user 
roles, (multiple )user-to-( multiple) role assignment (static/dynamic), the 
impact of actions of one role upon another (i.e., automated cooperation 
among roles based on process definition), and how roles affect the interac-
tion styles and other aspects of the process. 
3. Individual Support -
Demonstrate how individuals are guided about what 
task to do next, how users are made aware of the state of the process, or 
how the system performs actions as a result of the users' actions. Demon-
strations should clearly illustrate how users are aware of the process, how 
the environment and individuals interact, and what variables control the 
different modes of interaction. 
4. Multi-user Coordination -
Demonstrate coordination of multiple people, 
including any support for resolution and tolerance of inconsistency. In 
particular, demonstrations can illustrate which aspects of these policies, if 
any, are hard-wired into their systems, and which can be altered by the 
particular model, and when the policy selections are made. 
5. Configuration Management -
Demonstrate how software and/or docu-
ments are controlled for the purpose of change, and how individuals using 
a module in their development are made aware of problems and/or changes 
to that module. 
6. Project/Central vs individual coordination -
Demonstrate how the exe-
cuting process supports both individual and project activities, and how 
the interactions of those activities are supported/mediated by the system. 
7. Process changes while in execution -
Dynamically demonstrate changing 
any of the process definitions supporting the scenario and points 1-5 above, 
and the effects of those changes. 

260 
ApPENDIX B 
B.2 
SOLUTION IN Oz 
The following section lists a representative subset of the source code of the 
processes that model the scenario, as given in Section 6.2. It is organized as 
follows: Section B.2.1Iist the schema strategy at the Coding process including 
the shared data definitions; Section B.2.2 lists the Treaty strategies in the 
Coding process; and Section B.2.3 lists sample local strategies from Coding, 
QA and Design processes. A longer listing containing all rules from the three 
processes along with some additional envelopes can be found in [22]. 
B.2.1 
The Schema 
# 
# 
# 
# 
# 
# 
# 
# 
Oz Software Development Environment 
Copyright 1994 
The Trustees of Columbia University 
in the City of New York 
strategy data_model 
imports protection; 
exports all; 
objectbase 
All Rights Reserved 
###### THE CODING TREE 
####### 
# PROJECT is an entity that defines much of the structure of a typical 
# software project. PROJECTs can contain libraries, binaries 
# documents header files and source code modules. 
PROJECT :: superclass ENTITY; 
archive_status : (Archived, NotArchived, Initialized) = Initialized; 
build_status: (Built, NotBuilt, Initialized) = Initialized; 
build_log: text; 
libs 
set_of LIBS; 
bins 
BINS; 
docs 
set_of DOC; 
incs 
set_of INC; 

The ISPW-9 Problem: Definition and Solution m Oz 
261 
srcs 
set_of MODULE; 
end 
LIBS :: superclass ENTITY; 
archive_status : (Archived, NotArchived, Initialized) 
libs : set_of LIB; 
Initialized; 
end 
# LIB is a shared archive type library. The ultimate representation of a 
# library is a .a file, that is, an archive format file. 
LIB :: superclass ENTITY; 
end 
archive_status : (Archived, Not Archived , Initialized) 
afile : binary = ". a" ; 
Initialized; 
# MODULE organizes CFILES based upon some higher order. Each module knows 
# which library (possibly more than one) it will be archived to. 
MODULES 
# can recursively contain other MODULES, and sets of CFILES. 
MODULE :: superclass ENTITY; 
end 
library 
set_of link LIB; 
archive_status 
(Archived, Not Archived , Initialized) = Initialized; 
modules 
cfiles 
external_doc 
set_of MODULE; 
set_of CFILE; 
link DOCFILE; 
# FILE is the generic class for anything that is represented as a unix 
# file. 
There are specializations (subtypes) for CFILE, HFILE and DOCFILE. 
# the reservation status is for configuration management purposes 
FILE :: superclass ENTITY; 
owner : user; 
timestamp: time; 
reservation_status 
(CheckedOutShared, CheckedOut, Available, 
end 
contents 
text; 
locker 
user; 
Initialized) = Initialized; 
# This is a specialization of FILE to C source files. 
# Several status attributes are added to record the status of 
# compilation, analysis, change, and review. 

262 
ApPENDIX B 
# And additional product-related artifacts are added to contain 
# object code, change requests, bug reports, etc. 
# finally a CFILE contains links to various HFILEs that it includes, 
# and to branching information. 
CFILE .. superclass FILE; 
compile_status 
(Compiled, NotCompiled, Initialized) 
Initialized; 
compile_log 
text; 
analyze_status 
(Analyzed, NotAnalyzed, Initialized) = Initialized; 
analyze_log 
text; 
change_status 
(Idle, St art Change , CompleteChange, Inspected) = Idle; 
change_request 
text = ".chg"; 
modified_change_request : text = ".mod_chg"; 
bug_status 
: (Clean, Suspected, Defected) 
Clean; 
bug_report: text = ".bug"; 
review_status 
(NotReviewed, ReviewRequested, Approved, Rejected, 
RevisionRequested, Revised) = NotReviewed; 
review_rc 
(Succeeded, Failed, None) = None; 
contents : text = ". c"; 
object_code : binary = ".0"; 
ref : set_of link HFILE; 
maintained_by : link PROGRAMMER; 
branches : set_of BRANCH; 
end 
# For different rcs versions 
BRANCH :: superclass ENTITY; 
locker : user; 
file : link CFILE; 
end 
# For HFILEs, we only want to know if they have been modified recently, 
# which will cause a global recompilation. 
HFILE :: superclass FILE; 
recompile_mod: boolean = false; 
contents : text = ".h"; 
end 
# DOCFILEs specialized FILEs with their artifacts that contain the 
# various files which are used by latex, and some status attributes for 
# monitoring the state of the change, review etc. are added. 
DOCFILE :: superclass FILE; 

The ISPW-9 Problem: Definition and Solution zn Oz 
end 
reformat_doc: boolean = false; 
plain 
text 
". txt II ; 
tex_file 
binary = ". tex"; 
dvi_file 
binary = ".dvi"; 
ps_file 
binary = II .pS"; 
review_status 
: (Idle, ReviewRequested, ChangeRejected, 
ChangeApproved) = Idle; 
review_rc : (Succeeded, Failed, None) = None; 
change_request: text = ". chg"; 
modified_change_request : text = II .mod_chg"; 
maintained_by : link PROGRAMMER; 
bug_report : text = ". bug II ; 
263 
# DOC is a class that represents an entire set of documents, typically for 
# a PROJECT or PROGRAM. 
A DOC can contain individual documents, and files 
# of it's own. 
DOC :: superclass ENTITY; 
documents : set_of DOCUMENT; 
files : set_of DOCFILE; 
end 
# DOCUMENT represents a complete individual document, such as a 
# user's manual or technical report. 
DOCUMENT :: superclass ENTITY; 
docfiles : set_of DOCFILE; 
end 
# INC represents a set of include (.h) files. 
INC :: superclass ENTITY; 
end 
archive_status : (Archived, NotArchived, Initialized) 
hfiles : set_of HFILE; 
BINS :: superclass ENTITY; 
Initialized; 
build_status: (Built, NotBuilt, Initialized) 
bins : set_of BIN; 
Initialized; 
end 

264 
ApPENDIX B 
# BIN represents a place where binaries for PROGRAMs (parts of a 
# PROJECT) are kept. 
BIN :: superc1ass ENTITY; 
build_status: (Built, NotBuilt, Initialized) 
Initialized; 
executable : binary; 
end 
###### THE TEST TREE 
####### 
TEST_PROJECT :: superclass ENTITY; 
test_status 
test_suites 
test_results 
end 
(Tested, NotTested, Initialized) 
set_of TEST_SUITE; 
set_of TEST_RUN_SET; 
TEST_SUITE .. superclass ENTITY; 
Initialized; 
test_status 
(Tested, NotTested, Initialized) 
Initialized; 
test_cases 
set_of TEST_CASE; 
shared_data 
binary; 
end 
TEST_CASE :: superclass FILE; 
test_input 
text = ". test_in" ; 
expected_output 
text = ".test_out"; 
maintained_by 
set_of link PROGRAMMER; 
end 
number 
run_status 
test_failed 
integer = 0; 
boolean = false; 
boolean = false; 
TEST_RUN_SET .. superclass ENTITY; 
test_runs 
set_of TEST_RUN; 
number 
: integer = 0; 
end 
TEST_RUN :: superclass ENTITY; 
performed_by 
user; 
timestamp 
time; 
test_output 
report 
report_status 
number 
text; 
text; 
(NotReported, Reported, Notified, Confirmed) 
= NotReported; 
integer; 

The ISPW-9 Problem: Definition and Solution in Oz 
end 
new 
test_suites 
bin 
integer; 
# 
0 
# 
1 
# 
2 
# 
3 
# 
4 
boolean = true; 
link TEST_SUITE; 
link BIN; 
###### THE USER TREE 
####### 
- successful 
- minor errors 
- moderate errors 
- quite serious errors 
- severe errors 
## modification of the default user tree 
PROGRAMMER .. 
fname 
lname 
role 
group 
at_office 
user_id 
bonus 
end 
superclass USER; 
string; 
string; 
string; 
string; 
boolean; 
user; 
integer; 
###### FROM THE DESIGN TREE 
####### 
DESIGN_PROJECT :: superclass ENTITY; 
documents : set_of DESIGN_DOCUMENT; 
end 
DESIGN_DOCUMENT :: superclass ENTITY; 
docfiles 
set_of DOCFILE; 
end 
##### THE WORKSPACE TREE ##### 
LOCAL_AREA :: superclass ENTITY; 
workspaces 
set_of WORKSPACE; 
project 
: link PROJECT; 
end 
WORKSPACE :: superclass BIN, PROGRAMMER; 
files : set_of FILE; 
module 
link MODULE; 
## link to its MODULE 
doc 
: link DESIGN_DOCUMENT; 
## link to its DESIGN 
265 

266 
ApPENDIX B 
owner 
user; 
end 
end_objectbase 
B.2.2 
Sam pIe Treaty Strategies 
# 
# 
# 
# 
# 
# 
# 
# 
Oz Software Development Environment 
Copyright 1994 
The Trustees of Columbia University 
in the City of New York 
All Rights Reserved 
strategy ANALYZE (Treaty) 
imports data_model; 
exports all; 
objectbase 
######### 
# 
# 
TOOL Definitions 
# 
######### 
ANALYZE_TOOLS 
:: superclass TOOL; 
analyze_bug 
string = analyze_bug; 
analyze_cfile_bug : string = analyze_cfile_bug; 
end 
end_objectbase 
rules 
######### 
# 
# 
analyze_bug: 

The ISPW-9 Problem: Definition and Solution zn Oz 
267 
# 
# 
This Summit rule starts up the analyze_task, done in CODING and QA. 
# It is performed at the CODING process, and is intended to find 
# 
"suspected" cfiles that might be the reason for the bug found at 
# 
the QA process 
# 
######### 
analyze_bug [?tr: TEST_RUN , ?p:PROJECT]: 
(and (forall TEST_SUITE ?ts suchthat (linkto [?tr.test_suites ?ts]» 
(forall CFILE?c suchthat (ancestor[?p ?c]»): 
(and 
no_chain (?tr.test_status > 0) 
no_chain (?tr.report_status = Notified» 
# This envelope returns a subset, which is the "bad" 
# sources need to send more ?tr attributes 
# ----------------------------------------
{ ANALYZE_TOOLS analyze_bug ?tr.report ?tr.test_status 
?tr.performed_by?tr.timestamp 
?c.contents return subset ?bad_c:CFILE} 
(?bad_c.bug_status = Suspected); 
######### 
# 
# 
analyze_bug (on a cfile) 
# 
# 
This Summit rule chains off the above rule, and is delegated to the 
# 
owner of the cfile. The owner determines whether the bug was really in 
# that CFILE, or not. In the former case, the bug_status becomes 
# 
defected, meaning that a change task should be performed on it. The 
# latter means this file is "clean". Also, generate a change request. 
######### 
analyze_bug [?tr: TEST_RUN , ?c:CFILE]: 
(and 
(forall MODULE ?m suchthat (member [?m.cfiles ?c]» 
(exists WORKSPACE ?w suchthat (linkto [?w.module ?m]») 
delegate[?w.owner]: 

268 
ApPENDIX B 
(?c.bug_status = Suspected) 
# Prompt the user whether the bug is here (return 0) or not (Return 1) 
# also, generate a change request in the CFILE. 
{ ANALYZE_TOOLS analyze_cfile_bug ?tr.report ?c.change_request ?c.contents 
?c.bug_report } 
(and no_chain (?c.bug_status = Defected) 
# chain locally at TEST site 
(?tr.report_status = Confirmed)); 
(?c.bug_status = Clean); 
######### 
# 
# reset_analyze_atts: 
# 
This rule is for debugging purposes only. 
# initialize attributes to start the analyze phase 
# 
######### 
hide reset_analyze_atts[?tr:TEST_RUN, ?c:CFlLE]: 
{ } 
(and 
no_chain (?c.bug_status = Clean) 
no_chain (?tr.report_status = Notified)); 
# 
# 
# 
# 
# 
# 
# 
# 
Oz Software Development Environment 
Copyright 1994 
The Trustees of Columbia University 
in the City of New York 
All Rights Reserved 
strategy REVIEW (Treaty) 
imports data_model; 
exports all; 
objectbase 

The ISPW-9 Problem: Definition and Solution in Oz 
######### 
# 
# 
TOOL Definitions 
# 
######### 
REVIEW_TOOLS 
init_review 
revise_doc 
end 
:: superclass TOOL; 
string = init_review; 
: string = revise_doc; 
MU_TOOLS 
:: superclass TOOL; 
[ protocol : SEL; 
multi_flag: MULTI_QUEUE; ] 
confer 
end 
: string = confer; 
end_objectbase 
rules 
######### 
# 
# 
setup_review: 
# 
269 
# 
This Summit rule starts up the review task, done in CODING and DESIGN. 
# It forward chains to local review at both CODING and DESIGN processes 
# 
######### 
setup_reviev[?p: PROJECT , ?design_doc:DESIGN_DOCUMENT]: 
(and (forall MODULE ?m suchthat (member [?p.srcs ?m])} 
(or 
(forall CFILE ?c suchthat (and (member [?m.cfiles ?c]) 
(?c.bug_status = Defected)}} 
(forall DOCFILE ?d suchthat (member [?design_doc.docfiles ?d])}} 
(?c.reviev_status = Revised) 
(?c.bug_status = Defected)} 
# this envelope simply copies the contents of the change request 
# and the bug_report to the same fields 

270 
I in the design doc object, and the bug_report 
I could possibly use the copy operation instead 
ApPENDIX B 
{ REVIEW_TOOLS init_review ?c.change_request ?c.bug_report 
?d.change_request ?d.bug_report } 
{and 
(?d.review_status = ReviewRequested) 
(?c.review_status = ReviewRequested»; 
IIIIIIIII 
I 
I approve: 
I 
I 
This Summit rule completes the Review Phase 
I 
IIIIIIIII 
approve[?c:CFILE, ?design_doc:DESIGN_DOCUMENT]: 
{forall DOCFILE ?d suchthat (member [?design_doc.docfiles ?d]» 
{and 
no_backward (?d.review_status = ReviewRequested) 
no_backward (?c.review_status = ReviewRequested) 
no_backward (?d.review_rc = Succeeded) 
no_backward (?c.review_rc = Succeeded» 
{ } 
{and 
(?d.reviev_status = ChangeApproved) 
(?c.reviev_status = Approved»; 
IIIIIIIII 
I 
I 
conference: 
I 
I 
This Summit rule is invoked when local reviews fail. 
I it is a multi-user rule that forward chains to local revise at 
I 
CODING 
I 
IIIIIIIII 
conference [?c: CFILE , ?design_doc:DESIGN_DOCUMENT]: 

The ISPW-9 Problem: Definition and Solution zn Oz 
# collect multiple users to same variable to get 
# delegation to multiple people for multi-user tool invocation 
(and 
(forall GROUP 
?coding suchthat (ancestor [?coding ?c]» 
(forall GROUP 
?design suchthat (ancestor [?design ?design_doc]» 
(forall MODULE?m 
suchthat (member 
[?m.cfiles ?c]» 
(forall DOCFILE ?d suchthat (member [?design_doc.docfiles ?d]» 
(forall WORKSPACE ?p suchthat (or 
delegate[?p.owner]: 
{and 
(linkto [?p.module ?m]) 
(linkto [?p.doc ?design_doc]»» 
no_backward (?d.review_status = ReviewRequested) 
no_backward (?c.review_status = ReviewRequested) 
{or 
no_backward (?d.review_rc = Failed) 
no_backward (?c.review_rc = Failed») 
# start white_board here to discuss design. prepare working files 
# this is a multi user activity 
{ 
MU_TOOLS confer ?c.contents ?d.contents 
?coding.site_ip_addr ?design.site_ip_addr} 
# O. ok, go to revise, and enable setup_review if revise succeeds 
# 
{and 
{and 
(?c.review_status 
= RevisionRequested) 
# this double assertion enables to chain back to setup_review 
(?c.bug_status 
Suspected) 
(?c.bug_status 
= Defected»; 
# 1. no hope, go to reject. needs to start allover again. 
{and 
(?d.review_status = ChangeRejected) 
(?c.review_status = Rejected»; 
######### 
# 
# reset_atts: 
# 
# 
This Summit rule is for debugging, initializing attributes 
271 
# 
for starting the review on a single cfile (from CODING) and a single 

272 
# docfile (from DESIGN) 
# 
######### 
reset_atts[?c:CFILE. ?d:DOCFILE]: 
{ } 
{and 
no_chain (?c.bug_status = Defected) 
no_chain (?c.review_rc = None) 
no_chain (?c.review_status = NotReviewed) 
no_chain (?d.review_status = Idle) 
no_chain (?d.review_rc = None»; 
# 
# 
# 
# 
# 
# 
# 
# 
Oz Software Development Environment 
Copyright 1994 
The Trustees of Columbia University 
in the City of New York 
All Rights Reserved 
strategy CHANGE (Treaty) 
imports data_model; 
exports all; 
objectbase 
######### 
# 
# 
TOOL Definitions 
# 
######### 
CHANGE_TOOLS .. superclass TOOL; 
install_bin 
string = install_bin; 
end 
# Synchronous Multi-user tool 
MU_INSPECTION :: superclass TOOL; 
[ protocol:SEL; multi_flag:MULTI_QUEUE 
] 
code_inspect 
: string = code_inspect; 
ApPENDIX B 

The ISPW-9 Problem: Definition and Solution zn Oz 
end 
end_objectbase 
rules 
######### 
# 
# 
code_inspect: 
# 
# 
This multi-user Summit rule is called manually, i.e., it is not 
# 
chained off any other rule. and at the moment doesn't chain 
# to any other rule either. 
# it has no condition, so it can be called at any time. 
# 
but deposit cannot occur unless the file has been inspected 
# It calls the multi-media inspection program developed in-house. 
# 
######### 
code_inspect[?c:CFILE, ?d:DOCFILE]: 
{and 
{forall GROUP 
?c_site suchthat (ancestor [?c_site ?c]» 
(fora11 GROUP 
?d_site suchthat (ancestor [?d_site ?d]» 
(forall MODULE 
?m 
suchthat (member 
[?m.cfiles ?c]» 
{forall DESIGN_DOCUMENT?dm suchthat (member 
[?dm.docfiles ?d]» 
(foral1 WORKSPACE ?w suchthat (or 
delegate[?w.owner]: 
(linkto [?w.module ?m]) 
(linkto [?w.doc ?dm]»» 
273 
{ MU_INSPECTION code_inspect ?c.contents ?c.change_request ?d.contents 
?c_site.site_ip_addr ?d_site.site_ip_addr 
?w.owner } 
(?c.change_status = Inspected); 
(?c.change_status = StartChange); 
######### 
# 
# 
install_bin: 
# 
# 
A simple rule that copies the binary from the CODING site to the 
# 
QA site. it does so by copying contents of object, as opposed to 

274 
# 
copying the whole object. 
# 
######### 
# Enforce to allow only a coding bin to be installed. 
# --------------------------------------------------
ApPENDIX B 
(exists PROJECT ?p suchthat no_chain{ancestor [?p ?coding_bin])) 
{ CHARGE_TOOLS install_bin ?test_bin.executable ?coding_bin.executable } 
(?test_bin.build_status = Built); 
B.2.3 
Sample Local Strategies 
In Coding Process 
# 
# 
# 
# 
# 
# 
# 
# 
Oz Process Centered Environment 
Copyright 1994 
The Trustees of Columbia University 
in the City of New York 
All Rights Reserved 
strategy build (LOCAL) 
# This strategy provides a rule to build a PROGRAM. 
It also provides two 
# inferences rules that will build an entire PROJECT or GROUP. 
imports data_modeli 
exports alli 
objectbase 
######### 
# 
# 
TOOL Definitions 
# 

The ISPW-9 Problem: Definition and Solution in Oz 
######### 
BUILD :: superclass TOOL; 
build_program : string = build; 
build_workspace : string = build_workspace; 
archive : string = archive; 
archive_module : string = archive_module; 
list_archive : string = list_archive; 
end 
end_objectbase 
rules 
# 
# Build the project when all BINS are built. 
# 
build [?proj:PROJECT]: 
# 
(exists BIBS 
?b suchthat (member 
[?proj.bins ?b])): 
(?b.build_status = Built) 
{ } 
(?proj.build_status = Built); 
# Force each BIN to be built. 
# 
build [?bs:BIBS]: 
# 
(exists BIB ?b suchthat (member [?bs.bins ?b])): 
(?b.build_status = Built) 
{ } 
(?bs.build_status = Built); 
# Build the BIB from libraries 
# 
build[?b:BIB] : 
(and (exists PROJECT?p suchthat (ancestor [?p ?b])) 
(forall LIB 
?l suchthat (ancestor [?p ?l]))): 
(?l.archive_status = Archived) 
275 

276 
ApPENDIX B 
{ BUILD build_program ?b.executable ?p.build_log ?l.afile } 
# 
(?b.build_status = Built); 
(?b.build_status = NotBuilt); 
# Build a WORKSPACE 
# 
build[?w:WORKSPACE]: 
{and 
{exists LOCAL_AREA ?la suchthat no_chain {member 
{exists PROJECT 
?p suchthat no_chain {linkto 
{forall CFILE 
?c suchthat no_chain {member 
{forall LIB 
?l suchthat no_chain (ancestor 
{and (?l.archive_status = Archived) 
(?c.compile_status = Compiled)) 
[?la.workspaces ?w])~ 
[?la.project ?p])) 
[?w. files ?c])) 
[?p ?1]))): 
{ BUILD build_workspace ?w.executable ?l.afile ?c.object_code } 
# 
(?w.build_status = Built); 
(?w.build_status = NotBuilt); 
# Archive a MODULE 
# 
archive [?m:MODULE]: 
# 
{and (forall CFILE ?c suchthat (member [?m.cfiles ?c])) 
{exists LIB 
?l suchthat (linkto [?m.library ?l]))): 
(?c.compile_status = Compiled) 
{ BUILD archive ?l.afile ?c.object_code } 
(?m.archive_status = Archived); 
# Archive a LIB 
# 
archive [?l:LIB]: 
{and {forall MODULE ?m suchthat (linkto [?m.library ?l])) 
{forall CFILE 
?c suchthat (ancestor [?m ?c]))): 

The ISPW-9 Problem: Definition and Solution in Oz 
(?m.archive_status = Archived) 
{ } 
[?l.archive_status = Archived]; 
# 
# Archive the LIBS 
# 
archive [?ls:LIBS]: 
(forall LIB ?l suchthat (member [?ls.libs ?l])): 
[?l.archive_status = Archived] 
{ } 
[?ls.archive_status = Archived]; 
view [?l: LIB] : 
{ BUILD list_archive 11.afile } 
# -------------------- ATOMICITY CHAINS --------------------
touch[?l:LIB]: 
(exists MODULE 1m suchthat (linkto [1m.library ?l])): 
[?m.archive_status = NotArchived] 
{ } 
[11.archive_status = NotArchived]; 
touch[?ls:LIBS]: 
(exists LIB 11 suchthat (member [?ls.libs ?l])): 
[11.archive_status = Not Archived] 
{ } 
[?ls.archive_status = NotArchived]; 
touch[?p:PROJECT]: 
(exists LIBS 11s suchthat (member [?p.libs ?ls])): 
[?ls.archive_status = NotArchived] 
{ } 
[?p.build_status = NotBuilt]; 
277 

278 
ApPENDIX B 
touch[?bs:BINS]: 
(exists PROJECT ?p suchthat (member [?p.bins ?bs]»: 
[?p.build_status = NotBuilt] 
{ } 
[?bs.build_status = NotBuilt]; 
touch[?b:BIN]: 
(exists BINS ?bs suchthat (member [?bs.bins ?b]»: 
[?bs.build_status = NotBuilt] 
{ } 
[?b.build_status = NotBuilt]; 
touch[?h:HFILE]: 
(exists INC ?i suchthat (member [?i.hfiles ?h]»: 
[?h.recompile_mod = true] 
{ } 
(and [?h.recompile_mod = false] 
[?i.archive_status = NotArchived]); 
touch[?i:INC]: 
(and (exists PROJECT ?p suchthat (member [?p.incs ?i]» 
(forall CFILE ?c suchthat (ancestor [?p ?c]»): 
[?i.archive_status = NotArchived] 
{ } 
(and [?c.compile_status = NotCompiled] 
no_chain (?i.archive_status = Archived»; 
touch[?m:MODULE]: 
(exists CFILE ?c suchthat (member [?m.cfiles ?c]»: 
[?c.compile_status = NotCompiled] 
{ } 
[?m.archive_status = NotArchived]; 
In QA Process 
# 
# 
# 
Oz Software Development Environment 

The ISPW-9 Problem: Definition and Solution m Oz 
# 
# 
# 
# 
# 
# 
strategy test 
imports data_model; 
exports all; 
objectbase 
# Tool definitions 
Copyright 1994 
The Trustees of Columbia University 
in the City of New York 
All Rights Reserved 
TEST_TOOLS :: superclass TOOL; 
create_test_run 
run_test 
notify_bug 
report_bug 
add_bonus 
edit_test 
string = create_test_run; 
string = run_test; 
string = notify_bug; 
string = report_bug; 
string = add_bonus; 
string = edit_test; 
view_result 
string = view_report; 
end 
generate_report 
string = generate_report; 
store_notify_bug : string = store_notify_bug; 
end_objectbase 
rules 
######### 
# 
# start_test_run: 
# 
# 
add a test run, link it to test_suite and the binary and trigger 
# 
chain of tests. 
# 
######### 
start_test_run[?tr:TEST_RUN_SET,?ts:TEST_SUITE,?b:BIN]: 
(forall TEST_CASE ?tc suchthat (member [?ts.test_cases ?tc]» 
279 

280 
ApPENDIX B 
# prompt the user for the name of the test run and return 
# it in the envelope 
{ TEST_TOOLS create_test_run return ?name } 
# the link effects trigger the run_test rules. 
(and 
(?nev:TEST_RUN = add [?tr test_runs ?name TEST_RUN]) 
# assign run number and increment the "global" counter at the parent 
(?nev.performed_by = CurrentUser) 
(?nev.number = ?tr.number) 
(?tc.test_failed = false) 
(link [?nev test_suites ?ts]) 
(link [?nev bin ?b]) 
(?tr.number += 1»; 
no_assertion; 
######### 
# 
# 
run_test: 
# 
# 
run automatically test on a test_case. 
# 
######### 
run_test[?tc:TEST_CASE]: 
(and 
(exists TEST_SUITE ?ts suchthat (member [?ts.test_cases ?tc]» 
(exists TEST_RUN ?tr suchthat (and (linkto [?tr.test_suites ?ts]) 
(?tr.nev = true») 
(exists BIN 
?b suchthat (linkto [?tr.bin 
?b]») 
{ TEST_TOOLS run_test ?tc.test_input ?tc.expected_output ?b.executable 
?ts.shared_data ?tr.test_output return ?stat} 
(and 
(?tr.timestamp = CurrentTime) 
(?tr.performed_by = CurrentUser) 
(?tc.run_status = true) 
(?tc.test_failed = false»; 

The ISPW-9 Problem: Definition and Solution zn Oz 
(and 
(?tr. timestamp = Current Time) 
(?tr.performed_by = CurrentUser) 
(?tc.run_status = true) 
(?tr.test_status += 1) 
(?tc.test_failed = true»; 
######### 
# 
# 
complete_test 
# 
# this rule is fired when all individual runs have finished, either 
# successfully or not 
# 
######### 
complete_test[?ts:TEST_SUITE]: 
(and 
(forall TEST_CASE ?tc suchthat (member [?ts.test_cases ?tc]» 
(forall TEST_RUN ?tr suchthat (and (linkto [?tr.test_suites ?ts]) 
(?tc.run_status = true) 
{ } 
(and 
(?tc.run_status = false) 
(?tr.new = false»; 
######### 
# 
# report_bug: 
(?tr.new = true»»: 
# 
when a test run fails, chain to this rule 
# 
######### 
(?tr.test_status > 0) 
{ TEST_TOOLS generate_report ?tr.performed_by ?tr.timestamp 
?tr.test_output ?tr.report} 
(?tr.report_status = Reported); 
281 

282 
######### 
# 
# 
notify_bug: 
ApPENDIX B 
# 
chained off report_bug, delegated to a user which is notified 
# 
of the problem 
# 
######### 
notify_bug[?tr:TEST_RUN]: 
(forall PROGRAMMER ?p suchthat (and 
:delegate[?p.user_id]: 
(?tr.report_status = Reported) 
(?p.group = "CODING") 
(?p.role = "Manager"») 
# this tool should generate a message with the input parameters as 
# "fill-in blanks" style 
{ TEST_TOOLS notify_bug ?tr.report } 
(?tr.report_status = Notified); 
######### 
# 
# notify_bug: 
# 
chained off report_bug, in case notification is impossible 
# 
######### 
notify_bug[?tr:TEST_RUN]: 
(forall PROGRAMMER ?p suchthat (and 
: (?tr.report_status = Reported) 
(?p.group = "CODING") 
(?p.role = "Manager"») 
# this tool should generate a message with the input parameters as 
# "fill-in blanks" style 
{ TEST_TOOLS store_notify_bug ?p.fname ?p.lname ?p.user_id } 
no_assertion; 

REFERENCES 
[1] Robert Allen and David Garlan. Formalizing architectural connections. 
In 16th International Conference on Software Engineering, pages 71-80, 
Sorrento, Italy, May 1994. IEEE Computer Society Press. Panel Intro-
duction. 
[2] John E. Arnold and Steven S. Popovich. Integrating, customizing and 
extending environments with a message-based architecture. Technical 
Report CUCS-008-95, Columbia University, Department of Computer 
Science, September 1994. The research described in this report was con-
ducted at Bull HN Information Systems, Inc. 
[3] George S. Avrunin, Laura K. Dillon, Jack C. Wileden, and William E. 
Riddle. Constrained Expressions: Adding analysis capabilities to design 
methods for concurrent software systems. IEEE Transactions on Software 
Engineering, SE-12(2):278-292, February 1986. 
[4] Robert Balzer. Process virtual machine. In Ian Thomas, editor, 7th Inter-
national Software Process Workshop: Communication and Coordination 
in the Software Process, pages 37-40, Yountville CA, October 1991. IEEE 
Computer Society Press. 
[5] Robert Balzer. Tolerating inconsistency. In 13th International Conference 
on Software Engineering, pages 158-165, Austin TX, May 1991. IEEE 
Computer Society Press. 
[6] Sergio Bandinelli and Alfonso Fuggetta. Computational reflection in soft-
ware process modeling: the SLANG approach. In 15th International 
Conference on Software Engineering, pages 144-154, Baltimore MD, May 
1993. IEEE Computer Society Press. 
[7] Sergio Bandinelli, Alfonso Fuggetta, Carlo Ghezzi, and Sandro Grigolli. 
Process enactment in SLANG. In J .C. Derniame, editor, Software Process 
Technology Second European Workshop, number 635 in Lecture Notes 
in Computer Science. Springer-Verlag, Trondheim, Norway, September 
1992. 

284 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
(8] Naser S. Barghouti. Concurrency Control in Rule-Based Software Devel-
opment Environments. PhD thesis, Columbia University, February 1992. 
CUCS-001-92. 
[9] Naser S. Barghouti. Supporting cooperation in the MARVEL process-
centered SDE. In Herbert Weber, editor, 5th ACM SIGSOFT Symposium 
on Software Development Environments, pages 21-31, Tyson's Corner 
VA, December 1992. Special issue of Software Engineering Notes, 17(5), 
December 1992. 
(10] Naser S. Barghouti and Gail E. Kaiser. Modeling concurrency in rule-
based development environments. IEEE Expert, 5(6):15-27, December 
1990. 
(11] Naser S. Barghouti and Gail E. Kaiser. Concurrency control in advanced 
database applications. A CM Computing Surveys, 23(3):269-317, Septem-
ber 1991. 
[12] Naser S. Barghouti and Gail E. Kaiser. Scaling up rule-based develop-
ment environments. International Journal on Software Engineering f3 
Knowledge Engineering, 2(1):59-78, March 1992. 
[13] David R. Barstow, Howard E. Shrobe, and Erik Sandewall (editors). In-
teractive Programming Environments. McGraw-Hill, New York, 1984. 
[14] Noureddine Belkhatir, Jacky Estublier, and Walcelio L. Melo. Adele 
2: A support to large software development process. In Mark Dowson, 
editor, 1st International Conference on the Software Process: Manufac-
turing Complex Systems, pages 159-170, Redondo Beach CA, October 
1991. IEEE Computer Society. 
(15] Noureddine Belkhatir, Jacky Estublier, and Walcelio L. Melo. Software 
process model and work space control in the Adele system. In 2nd In-
ternational Conference on the Software Process: Continuous Software 
Process Improvement, pages 2-11, Berlin Germany, February 1993. IEEE 
Computer Society Press. 
[16] M. Ben-Ari. The Ada Rendezvous, chapter 6, pages 93-105. Prentice Hall 
International, Englewood Cliffs, NJ, 1982. 
(17] Israel Z. Ben-Shaul. An object management system for multi-user pro-
gramming environments. Master's thesis, Columbia University, Depart-
ment of Computer Science, April 1991. CUCS-OI0-91. 

REFERENCES 
285 
[18] Israel Z. Ben-Shaul, George T. Heineman, Steve S. Popovich, Peter D. 
Skopp, Andrew Z. Tong, and Giuseppe Valetto. Integrating groupware 
and process technologies in the Oz environment. In Carlo Ghezzi, editor, 
9th International Software Process Workshop, pages 114-116, Airlie VA, 
October 1994. IEEE Computer Society Press. 
[19] Israel Z. Ben-Shaul and Gail E. Kaiser. A configuration process for a dis-
tributed software development environment. In 2nd International Work-
shop on Configurable Distributed Systems, pages 123-134, Pittsburgh PA, 
March 1994. IEEE Computer Society Press. 
[20] Israel Z. Ben-Shaul and Gail E. Kaiser. A paradigm for decentralized 
process modeling and its realization in the oz environment. In 16th Inter-
national Conference on Software Engineering, pages 179-188, Sorrento, 
Italy, May 1994. IEEE Computer Society Press. 
[21] Israel Z. Ben-Shaul, Gail E. Kaiser, and George T. Heineman. An ar-
chitecture for multi-user software development environments. Computing 
Systems, The Journal of the USENIX Association, 6(2):65-103, Spring 
1993. 
[22] Isreal Z. Ben-Shaul. A Paradigm for Decentralized Process Modeling and 
its Realization in the Oz Environment. PhD thesis, Columbia University, 
May 1995. CUCS-014-95. 
[23] Philip A. Bernstein. Database system support for software engineering. 
In 9th International Conference on Software Engineering, pages 166-178, 
Monterey CA, March 1987. IEEE Computer Society Press. 
[24] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman. Con-
currency Control and Recovery in Database Systems. Addison-Wesley, 
Reading MA, 1987. 
[25] Omran A. Bukhres, Jiansan Chen, Weimin Du, and Ahmed K. Elma-
garmid. Interbase: An execution environment for heterogeneous software 
systems. Computer, 26(8):57-69, August 1993. 
[26] Stefano Ceri and Giuseppe Pelagatti. Distributed Databases. McGraw 
Hill, 1985. 
[27] Noam Chomsky. On certain formal properties of grammars. Information 
and Control, 2(2), 1959. 
[28] Donald Cohen. Automatic compilation of logical specifications into ef-
ficient programs. In 5th National Conference on A rlificial Intelligence, 
volume Science, pages 20-25, Philadelphia, PA, August 1986. AAAI. 

286 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
[29] Bill Curtis, Marc I. Kellner, and Jim Over. Process modeling. Commu-
nications of the ACM, 35(9):75-90, September 1992. 
[30] Wolfgang Deiters and Volker Gruhn. Managing software processes in the 
environment MELMAC. In Richard N. Taylor, editor, 4th ACM SIG-
SOFT Symposium on Software Development Environments, pages 193-
205, Irvine CA, December 1990. Special issue of Software Engineering 
Notes, 15(6), December 1990. 
[31] Prasun Dewan and Rajiv Choudhary. A high-level and flexible frame-
work for implementing multiuser user interfaces. ACM Transactions on 
Information Systems, 10(4):345-380, October 1992. 
[32] Prasun Dewan and John Riedl. Towards computer-supported concurrent 
software engineering. Computer, 26(1):17-36, January 1993. 
[33] Mark Dowson. Istar -
an integrated project support environment. In Pe-
ter Henderson, editor, ACM SIGSOFTISIGPLAN Software Engineering 
Symposium on Practical Software Development Environments, pages 27-
33, Palo Alto, CA, December 1986. Special issue of SIGPLAN Notices, 
22(1), January 1987. 
[34] Anthony Earl. Principles of a reference model for computer aided soft-
ware engineering environments. In Fred Long, editor, Software Engineer-
ing Environments International Workshop on Environments, volume 467 
of Lecture Notes in Computer Science, pages 115-129, Chinon, France, 
September 1989. Springer-Verlag. 
[35] R. Ahmed et al. The Pegasus heterogenous multidatabase system. Com-
puter, 24(12):19-27, December 1991. 
[36] Simon M. Kaplan et al. Supporting collaborative software developement 
with conversation builder. In Herbert Weber, editor, 5th ACM SIG-
SOFT Symposium on Software Development Environments, pages 11-20, 
Tyson's Corner VA, December 1992. Special issue of Software Engineering 
Notes, 17(5), December 1992. 
[37] Marc I. Kellner et al. Software process modeling example problem. In 1st 
International Conference on the Software Process: Manufacturing Com-
plex Systems, pages 176-186, Redondo Beach CA, October 1991. 
[38] Christer Fernstrom. PROCESS WEAVER: Adding process support to 
UNIX. In 2nd International Conference on the Software Process: Con-
tinuous Software Process Improvement, pages 12-26, Berlin, Germany, 
February 1993. IEEE Computer Society Press. 

REFERENCES 
287 
[39] G. Forte and R.J. Norman. A self assessment by the software engineering 
community. Communications of the ACM, 35(4):29-32, April 1992. 
[40] Steven L. Gaede, Brian Nejmeh, and William E. Riddle. Interim report 
process management: Infrastructure exploration project. Technical Re-
port 7-48-5, Software Design & Analysis, Inc., March 1991. 
[41] Pankaj K. Garg, Peiwei Mi, Thuan Pham, Walt Scacchi, and Gary Thun-
quest. The SMART approach for software process engineering. In 16th 
International Conference on Software Engineering, pages 341-350, Sor-
rento, Italy, May 1994. IEEE Computer Society Press. 
[42] P.K. Garg, T. Pham, B. Beach, A. Deshpande, A. Ishizaki, K. Wentzel, 
and W. Fong. Matisse: A knowldge-based team programming environ-
ment. International Journal of Software Engineering and Knowledge En-
gineering, 4(1):15-59, 1994. 
[43] David Garlan and Dewayne Perry. Software architecture: Practice, po-
tential, and pitfalls. In 16th International Conference on Software En-
gineering, pages 363-364, Sorrento, Italy, May 1994. IEEE Computer 
Society Press. 
[44] Carlo Ghezzi, editor. 9th International Software Process Workshop, Airlie 
VA, October 1994. IEEE Computer Society Press. 
[45] Mark A. Gisi and Gail E. Kaiser. Extending a tool integration lan-
guage. In Mark Dowson, editor, 1st International Conference on the 
Software Process: Manufacturing Complex Systems, pages 218-227, Re-
dondo Beach CA, October 1991. IEEE Computer Society Press. 
[46} Volker Gruhn and Rudiger Jegelka. An evaluation of FUNSOFT nets. 
In J .C. Derniame, editor, Software Process Technology Second European 
Workshop, number 635 in Lecture Notes in Computer Science, pages 196-
214. Springer-Verlag, Trondheim, Norway, September 1992. 
[47] Nico Haberman and Dewayne Perry. Ada for Experienced Programmers. 
Addison-Wesley, Reading, MA, 1983. 
[48] Nico Habermann and David Notkin. 
Gandalf: 
Software develop-
ment environments. IEEE Transactions on Software Engineering, SE-
12(12):1117-1127, December 1986. 
[49] Dennis Heimbigner. Proscription versus Prescription in process-centered 
environments. In Takuya Katayama, editor, 6th International Software 
Process Workshop: Support for the Software Process, pages 99-102, Hako-
date, Japan, October 1990. IEEE Computer Society Press. 

288 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
[50] Dennis Heimbigner. A federated architecture for envrionments: Take 
II. In Preprints of the Process Sensitive SEE Architectures Workshop, 
Boulder CO, September 1992. 
[51] Dennis Heimbigner. The ProcessWall: A process state server approach 
to process programming. In Herbert Weber, editor, 5th ACM SIG-
SOFT Symposium on Software Development Environments, pages 159-
168, Tyson's Corner VA, December 1992. Special issue of Software Engi-
neering Notes, 17(5), December 1992. 
[52] Dennis Heimbigner. A revisionist approach to process change. In Wil-
helm Schafer, editor, 8th International Software Process Workshop: State 
of the Practice in Process Technology, pages 95-97, Wadern, Germany, 
March 1993. 
[53] Dennis Heimbigner and Marc Kellner. 
Software process example for 
ISPW-7, August 1991. 
[54] George T. Heineman. A transaction manager component for cooperative 
transaction models. In Ann Gawman, W. Morven Gentleman, Evelyn 
Kidd, Perke Larson, and Jacob Slonim, editors, 1993 CASCON Con-
ference, pages 910-918, Toronto Ontario, Canada, October 1993. IBM 
Canada Ltd. Laboratory and National Research Council Canada. 
[55] George T. Heineman. Automatic translation of process modeling for-
malisms. In 1994 Centre for Advanced Studies Conference, pages 110-120, 
Toronto ON, Canada, November 1994. IBM Canada Ltd. Laboratory. 
[56] George T. Heineman. Process modeling with cooperative agents. In Brian 
Warboys, editor, 3rd European Workshop on Software Process Technol-
ogy, volume 772 of Lecture Notes in Computer Science, pages 75-89, 
Villard de Lans (Grenoble), France, February 1994. Springer-Verlag. 
[57] George T. Heineman and Gail E. Kaiser. Incremental process support 
for code reengineering. In International Conference on Software Mainte-
nance, pages 282-290, Victoria BC, Canada, September 1994. 
[58] George T. Heineman and Gail E. Kaiser. An architecture for integrating 
concurrency control into environment frameworks. In 17th International 
Conference on Software Engineering, pages 305-313, Seattle WA, April 
1995. ACM Press. 
[59] George T. Heineman, Gail E. Kaiser, Naser S. Barghouti, and Israel Z. 
Ben-Shaul. Rule chaining in MARVEL: Dynamic binding of parameters. 
IEEE Expert, 7(6):26-32, December 1992. 

REFERENCES 
289 
[60] SynerVision for SoftBench: A Process Engine for Teams, 1992. Marketing 
literature. 
[61] Bernhard Holtkamp. 
Process engine interoperation in PSEEs. 
In 
Preprints of the Process Sensitive SEE Architectures Workshop, Boulder 
CO, September 1992. 
[62] M. Honda. Support for parallel development in the sun network soft-
ware environment. In 2nd International Workshop on Computer-Aided 
Software Engineering, pages 5-5 - 5-7, 1988. 
[63] Watts Humphrey and Marc I. Kellner. Software process modeling: Prin-
ciples of entity process models. In 11th International Conference on Soft-
ware Engineering, pages 331-342, Pittsburgh PA, May 1989. IEEE Com-
puter Society Press. 
[64] Watts S. Humphrey. Managing the Software Process. Addison-Wesley, 
Reading MA, 1989. 
[65] Hajimu Iida, Takeshi Ogihara, Katsuro Inoue, and Koji Torii. Generating 
a menu-oriented navigation system from formal description of software 
development activity sequence. In Mark Dowson, editor, 1st International 
Conference on the Software Process: Manufacturing Complex Systems, 
pages 45-57, Redondo Beach CA, October 1991. IEEE Computer Society 
Press. 
[66] R. Kadia. Issues encountered in building a flexible software development 
environment. In Herbert Weber, editor, 5th ACM SIGSOFT Symposium 
on Software Development Environments, pages 169-180, Tyson's Corner 
VA, December 1992. Special issue of Software Engineering Notes, 17(5), 
December 1992. 
[67] Gail E. Kaiser. Interfacing cooperative transactions to software develop-
ment environments. Office Knowledge Engineering, 4(1):56-78, February 
1991. Invited paper. 
[68) Gail E. Kaiser. A rule-based process server component for constructing 
rule-based development environments, October 1991. Position paper. 
[69) Gail E. Kaiser, Naser S. Barghouti, Peter H. Feiler, and Robert W. 
Schwanke. Database support for knowledge-based engineering environ-
ments. IEEE Expert, 3(2):18-32, Summer 1988. 
[70) Gail E. Kaiser, Naser S. Barghouti, and Michael H. Sokolsky. Experience 
with process modeling in the MARVEL software development environment 

290 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
kernel. In Bruce Shriver, editor, 23rd Annual Hawaii International Con-
ference on System Sciences, volume II, pages 131-140, Kona HI, January 
1990. 
[71] Gail E. Kaiser and Peter H. Feiler. Intelligent assistance without artificial 
intelligence. In 32nd IEEE Computer Society International Conference, 
pages 236-241, San Francisco CA, February 1987. IEEE Computer Soci-
ety Press. 
[72] Gail E. Kaiser, Peter H. Feiler, and Steven S. Popovich. Intelligent 
assistance for software development and maintenance. IEEE Software, 
5(3):40-49, May 1988. 
[73] Gail E. Kaiser, Steven S. Popovich, and Israel Z. Ben-Shaul. A bi-level 
language for software process modeling. In 15th International Conference 
on Software Engineering, pages 132-143, Baltimore MD, May 1993. IEEE 
Computer Society Press. 
[74] Gail E. Kaiser, Steven S. Popovich, and Israel Z. Ben-Shaul. A bi-level 
language for software process modeling. In Walter F. Tichy, editor, Con-
figuration Management, number 2 in Trends in Software, chapter 2, pages 
39-72. John Wiley & Sons, 1994. 
[75] Gail E. Kaiser and Calton Pu. Dynamic restructuring of transactions. 
In Ahmed K. Elmagarmid, editor, Database Transaction Models for Ad-
vanced Applications, San Mateo CA, 1992. Morgan Kaufmann. Available 
as Columbia University Department of Computer Science, CUCS-012-91, 
August 1991. 
[76] Takuya Katayama. A hierarchical and functional software process de-
scription and its enaction. In 11th International Conference on Software 
Engineering, pages 343-352, Pittsburgh PA, May 1989. IEEE Computer 
Science Press. 
[77] Takuya Katayama, editor. 6th International Software Process Workshop: 
Support for the Software Process, Hakodate, Japan, October 1990. IEEE 
Computer Society Press. 
[78] Rick Kazman, Len Bass, Gregory Abowd, and Mike Webb. SAAM: A 
method for analyzing the properties of software architectures. In 16th In-
ternational Conference on Software Engineering, pages 81-90, Sorrento, 
Italy, May 1994. IEEE Computer Society Press. 
[79] Marc 1. Kellner and H. Dieter Rombach. Session summary: Comparisons 
of software process descriptions. In Takuya Katayama, editor, 6th Inter-
national Software Process Workshop: Support for the Software Process, 

REFERENCES 
291 
pages 7-18, Hakodate, Japan, October 1990. IEEE Computer Society 
Press. 
[80] Won Kim, Nat Ballou, Jorge F. Garza, and Darrel Woelk. A distributed 
object-oriented database system suporting shared and private databases. 
ACM Transactions on Information Systems, 9(1):31-51, January 1991. 
[81] Won Kim and Jungyun Seo. Classifying schematic and data heterogeneity 
in multidatabase systems. Computer, 24(12):12-18, December 1991. 
[82] Balachander Krishnamurthy and Naser S. Barghouti. 
Provence: A 
process visualization and enactment environment. In Ian Sommerville 
and Manfred Paul, editors, 4th European Software Engineering Confer-
ence, number 717 in Lecture Notes in Computer Science, pages 451-465. 
Springer-Verlag, Garmisch-Partenkirchen, Germany, September 1993. 
[83] Programming Systems Laboratory. Marvel 3.1 Administrator's manual. 
Technical report, Columbia University Department of Computer Science, 
March 1993. 
[84] David B. Leblang and Robert P. Chase, Jr. Computer-aided software 
engineering in a distributed workstation environment. In Peter Hen-
derson, editor, ACM SIGSOFT/SIGPLAN Software Engineering Sympo-
sium on Practical Software Development Environments, pages 104-112. 
ACM Press, April 1984. Special issue of SIGPLAN Notices, 19(5), May 
1984. 
[85] Manny M. Lehman, editor. Software Process Workshop, Egham, Surrey, 
UK, February 1984. IEEE Computer Society Press. 
[86] Manny M. Lehman. Process models, process programs, programming 
support. In 9th International Conference on Software Engineering, pages 
14-16, Monterey, CA, March 1987. 
[87] Andrew Lih. Oz firewall support, February 1994. Project-course E6998y. 
[88] Lion Engineering Enviornment, 1994. Marketing literature. 
[89] Nazim H. Madhavji and Maria H. Penedo, editors. Special Section on the 
Evolution of Software Processes, volume 19:12 of IEEE Transactions on 
Software Engineering. December, 1993. 
[90] Jeff Magee, Naranker Dulay, and Jeff Kramer. Sturcturing parallel and 
distributed progrmas. Software Engineering Journal, 8(2):73-82, March 
1993. 

292 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
[91] Jeff Magee, Jeff Kramer, and Morris Sloman. 
Constructing dis-
tributed systems in Conic. IEEE Transactions on Software Engineering, 
15(6):663-675, June 1989. 
[92] Peiwei Mi and Walt Scacchi. Modeling articulation work in software engi-
neering processes. In Mark Dowson, editor, 1st International Conference 
on the Software Process: Manufacturing Complex Systems, pages 188-
201, Redondo Beach CA, October 1991. IEEE Computer Society Press. 
[93] Peiwei Mi and Walt Scacchi. Process integration in CASE environments. 
IEEE Software, 9(2):45-53, March 1992. 
[94] Naftaly H. Minsky and David Rozenshtein. A software development envi-
ronment for law-governed systems. In Peter Henderson, editor, A CM SIG-
SOFT/SIGPLAN Software Engineering Symposium on Practical Soft-
ware Development Environments, pages 65-75, Boston MA, November 
1988. ACM Press. Special issue of SIGPLAN Notices, 24(2), February 
1989 and of Software Engineering Notes, 13(5), November 1988. 
[95] J. Eliot B. Moss. Nested Transactions: An Approach to Reliable Dis-
tributed Computing. Information Systems. The MIT Press, Cambridge 
MA,1985. 
[96] John R. Nicol, C. Thomas Wilkes, and Frank A. Manola. Object ori-
entation in heterogeneous distributed computing systems. 
Computer, 
26(6):57-67, June 1993. 
[97] Leon Osterweil. Software processes are software too. In 9th International 
Conference on Software Engineering, pages 1-13, Monterey CA, March 
1987. IEEE Computer Society Press. 
[98] James D. Palmer and N. Ann Fields. Computer supported cooperative 
work. Computer, 27, May 1994. 
[99] Maria H. Penedo. Life-cycle (sub) process demonstration scenario, March 
1994. 9th International Software Process Workshop (ISPW9). 
[100] Maria H. Penedo and William Riddle. Process-sensitive SEE architecture 
(PSEA) workshop summary. In ACM SIGSOFTSoftware Engineering 
Notes, Boulder CO, April 1993. 
[101] Dewayne E. Perry, editor. 3rd International Conference on the Software 
Process: Applying Software Process, Reston VA, October 1994. IEEE 
Computer Society Press. 

REFERENCES 
293 
[102] James L. Peterson. 
Petri Net Theory and The Modeling of Systems. 
Prentice-Hall, Englewood Cliffs NJ, 1981. 
[103] Burkhard Peuschel and Stefan Wolf. Architectural support for distributed 
process centered software development environments. In Wilhelm Schafer, 
editor, 8th International Software Process Workshop, pages 126-128, 
Wadern, Germany, March 1993. IEEE Computer Society Press. 
[104] Steven S. Popovich. Rule-based process servers for software development 
environments. In 1992 Centre for Advanced Studies Conference, volume I, 
pages 477-497, Toronto ON, Canada, November 1992. IBM Canada Ltd. 
Laboratory. 
[105] Steven S. Popovich and Gail E. Kaiser. Modeling and replacing envi-
ronment process components with a rule-based process server. Technical 
Report CU CS-004-95, Columbia University Department of Computer Sci-
ence, April 1995. 
[106] CLF Project. CLF Manual. USC Information Sciences Institute, January 
1988. 
[107] Sudha Ram, editor. Special Issue on Heterogeneous Distributed Database 
Systems, volume 24: 12 of Computer. IEEE Computer Society Press, De-
cember 1991. 
[108] Steven P. Reiss. 
An approach to incremental compilation. In SIG-
PLAN '84 Symposium on Compiler Construction, pages 144-156, Mon-
treal, Canada, June 1984. Special issue of @i[SIGPLAN Notices], 19(6), 
June 1984. 
[109] Steven P. Reiss. Connecting tools using message passing in the field 
environment. IEEE Software, 7(4):57-66, July 1990. 
[110] Thomas W. Reps and Tim Teitelbaum. The Synthesizer Generator: A 
System for Constructing Language-Based Editors. Texts and Monographs 
in Computer Science. Springer-Verlag, New York, 1989. 
[111] William E. Riddle. Activity structure definitions. Technical Report 7-52-
3, Software Design & Analysis, March 1991. 
[112] M. J. Rochkind. The source code control system. IEEE Transactions on 
Software Engineering, SE-1:364-370, 1975. 
[113] W. W. Royce. Managing the development of large software systems. In 
WESTCON, California, 1970. 

294 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
[114] Mahadev Satyanarayanan, James J. Kistler, Puneet Kumar, Maria E. 
Okasaki, Ellen H. Siegel, and David C. Steere. Coda: A highly available 
file system for a distributed workstation environment. IEEE Transactions 
on Computers, 39(4):447-459, April 1990. 
[115] Yoichi Shinoda and Takuya Katayama. Towards formal description and 
automatic generation of programming environments. In Fred Long, edi-
tor, Software Engineering Environments International Workshop on En-
vironments, number 467 in Lecture Notes in Computer Science, pages 
261-270. Springer-Verlag, Chinon, France, September 1989. 
[116] Izhar Shy, Richard Taylor, and Leon Osterweil. A metaphor and a concep-
tual architecture for software development enviornments. In Fred Long, 
editor, Software Engineering Environments International Workshop on 
Environments, volume 467 of Lecture Notes in Computer Science, pages 
77-97. Springer-Verlag, Chinon, France, September 1989. 
[117] Peter D. Skopp. Process centered software development on mobile hosts. 
Technical Report CUCS-035-93, Columbia University Department of 
Computer Science, October 1993. MS Thesis Proposal. 
[118] Peter D. Skopp and Gail E. Kaiser. Disconnected operation in a multi-
user software development environment. In Bharat Bhargava, editor, 
IEEE Workshop on Advances in Parallel and Distributed Systems, pages 
146-151, Princeton NJ, October 1993. 
[119] Michael H. Sokolsky and Gail E. Kaiser. A framework for immigrating 
existing software into new software development environments. Software 
Engineering Journal, 6(6):435-453, November 1991. 
[120] Ian Sommerville. Software Engineering. Addison-Wesley, Reading, MA, 
1989. 
[121] Nandit Soparkar, Henry F. Korth, and Abraham Silberschatz. Failure-
resilient transaction management in multidatabases. 
Computer, 
24(12):28-36, December 1991. 
[122] Robert E. Strom, David F. Bacon, Arthur P. Goldberg, Andy Lowry, 
Daniel M. Yellin, and Shaula Alexander Yemini. Hermes A Language for 
Distributed Computing. Prentice-Hall, Englewood Cliffs NJ, 1991. 
[123] Stanley M. Sutton, Jr. APPL/A: A Prototype Language for Software-
Process Programming. PhD thesis, University of Colorado, 1990. 

REFERENCES 
295 
[124] Carl D. Tait and Dan Duchamp. Detection and exploitation of file work-
ing sets. In 11th International Conference on Distributed Computing 
Systems, pages 2-9, Arlington TX, May 1991. IEEE Computer Society 
Press. 
[125] Walter F. Tichy. RCS -
a system for version control. Software-
Practice fj Experience, 15(7):637-654, July 1985. 
[126] Andrew Z. Tong, Gail E. Kaiser, and Steven S. Popovich. 
A flexi-
ble rule-chaining engine for process-based software engineering. In 9th 
Knowledge-Based Software Engineering Conference, pages 79-88, Mon-
terey CA, September 1994. 
[127] Giuseppe Valetto. Expanding the repertoire of process-based tool inte-
gration. Master's thesis, Columbia University, Department of Computer 
Science, December 1994. CUCS-027-94. 
[128] Wilhelm Schafer, Burkhard Peuschel and Stefan Wolf. A knowledge-
based software development environment supporting cooperative work. 
International Journal on Software Engineering fj Knowledge Engineer-
ing, 2(1):79-106, March 1992. 

A 
accept,63 
in Oz, 107 
Access control, 40 
Ada, 59, 89 
Administrator, 99 
APPL/A,89 
Atomicity, 34, 36, 39, 147 
Automation, 26, 30, 34-37, 147 
B 
Backward chaining, 81 
C 
Cache, 195-203 
Cache 
invalidation, 199 
validity, 197 
cancel,65 
in Oz, 117 
Chaining, 28,34,36 
Class inheritance, 23 
Common sub-process invariant, 69, 
117 
Common sub-process, 57 
Common sub-schema, 58 
Composite Summit, 75 
Concurrency control, 33, 36 
Connection database, 176-177 
dynamic component, 176 
static component, 176 
Consistency, 39 
Context hierarchy, 49 
process activity, 49 
INDEX 
process step, 49 
process task, 50 
Context switch, 190, 192 
Cooperative transactions, 33, 36 
Coordinating process, 58, 73 
Coordinating server, 135 
Coordinating site, 70 
Coordination model, 33, 36 
CORBA,43 
Core requirements, 16, 105 
CSCW, 7, 44 
See also, Groupware 
D 
Data model, 38, 41 
Decentralized environment, 55 
Delegation, 93 
deny, 65 
in Oz, 118 
DEPCE, 9, 46 
Derived parameters, 136 
Direct communication, 176 
Domain SubEnv table, 184 
Domain, 9 
Dynamic Treaty validation, 116 
E 
Enactment, 4 
Enforcement, 27, 30, 34, 40 
Envelopes, 31 
Environment interconnectivity, 165 
Evolution timestamps, 118 
Evolution, 37 
export data, 68 
export, 62 

298 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
in Oz, 107 
F 
Forward chaining, 81 
Full Treaty, 64 
G 
Global environment, 57 
Grammar-based PMLs, 87 
Groupware, 91, 161 
H 
HDDB,42 
Heterogeneous processing, 43 
Hidden file system, 31, 41 
I 
import, 62 
in grammars, 88 
in Oz, 108 
in Petri-nets, 84 
Indirect communication, 176 
Inference rule, 28, 36 
Inheritance, 23, 27 
Instantiated environment, 48 
Inversion algorithm, 36 
Ispw-9 example, 211 
ISTAR,17 
L 
Local process consistency, 71 
Local process, 57 
M 
Marvel, 21-42 
Marvel 
access control, 24 
classes, 23 
data model, 22 
envelopes, 22 
hidden file system, 23 
object-oriented database, 22 
objectbase, 23-24 
objects, 22 
process engine, 33 
process model, 25 
query sublanguage, 24 
rule activity, 33 
schema, 22 
strategies, 25 
toolbase, 30 
transactions, 147 
Module interconnection languages, 
60 
MSL,25 
Multi-process environment, 55 
Multi-site activity, 57 
Multi-site Treaty, 64 
o 
Object-oriented database, 22 
Objectbase, 22, 39 
Oz, 71, 97-98 
accept, 107 
architecture, 166 
cache invalidation, 199 
cache, 170, 195 
relationship lists, 197 
validity invariant, 197 
cancel, 117 
client, 166 
command processor, 169 
connection database, 176 
Connection Server, 166, 173 
context switch, 190, 192 
data manager, 170 
delegation, 153-161 
deny, 118 
deregistration, 189 
direct communication, 176, 181 
domain SubEnv table, 184 
Environment Server, 166 
evolution timestamps, 118 
evolver,71 

Index 
export, 107 
groupware, 161-163 
import, 108, 113 
indirect communication, 176, 181 
objectbase, 102-105 
object base display, 172 
overview, 98-102 
post-Summit, 140 
pre-Summit, 137 
query processor, 170 
refresh policy, 172 
registration process, 186-187 
request, 107 
rule animator, 171 
rule processor, 169 
rule stack, 137 
p 
scheduler, 168 
session, 168 
strategy, 106 
Summit, 134-153-
Summit activity, 139 
Summit initialization, 135 
Summit stack, 168 
task manager, 168 
transaction manager, 169 
transactions, 147 
Treaty, 106-116 
unexport, 113 
unimport, 113 
peE, 6, 48, 79 
Pegasus, 43 
Petri-net, 83 
import, 84 
Summit, 84 
Treaty, 84 
PML, 6,79 
Post-Summit, 74 
example, 77 
in grammars, 89 
in Oz, 140 
in petri-nets, 85 
in rules, 82 
Pre-Summit, 74 
in Oz, 137 
example, 77 
in grammars, 89 
in petri-nets, 85 
in rules, 82 
299 
Process centered environment, 6 
Process data, 6 
Process model, 25, 37, 41, 48 
Process modeling language, 6 
Process, 2 
activities, 2 
administrator, 6 
animation, 138 
automation, 5 
autonomy, 14 
consistency, 39 
constraints, 3 
enactment, 4, 33 
enforcement, 4 
evolution, 37 
execution, 33 
guidance, 5 
instantiation, 48 
interoperability, 14 
locality, 14 
modeling, 3 
monitoring, 5 
ProcessWEAVER, 19 
Product data, 6 
Q 
Query sublanguage, 27 
R 
Recovery, 33, 36 
Registration process, 186-187 
Relationship lists, 197 
Rendezvous, 90 
request, 63 

300 
A PARADIGM FOR DECENTRALIZED PROCESS MODELING 
in Oz, 107 
Rule 
activity, 28 
assertions, 28 
bindings, 26, 36 
condition, 26, 35 
derived parameters, 26 
effects, 28, 35 
inference, 28 
network, 30 
overloading, 27 
parameter passing, 36 
parameters, 26 
property list, 27 
stack, 168 
post-Summit, 82 
pre-Summit, 82 
Summit, 82 
Rule based PMLs, 81 
s 
Schema evolution, 38 
Schema, 50 
SDE,1 
SEL, 31 
Semi-replicated database, 176 
Shared privileges, 107 
Simple Treaty, 63 
Single-process environment, 50 
Site deregistration, 189 
Site registration, 187 
Site, 9 
SLAT chains, 149 
SLAT transactions, 149 
SLAV chains, 149 
Smile, 21 
Software development 
environment, 1 
Software process, 2 
SSAT chains, 149 
SSAT transactions, 150 
SSAV chains, 149 
Strategy, 106 
Sub-environment, 7 
SubEnv,7,57 
interconnectivity, 15 
Summit, 73 
activity, 74 
T 
archi tect ure, 174 
completion, 75 
composite, 75 
example 
Petri-nets, 87 
in APPLj A, 89 
in grammars, 89 
in Petri-nets, 84 
in rules, 82 
initialization in Oz, 135 
initialization, 73 
metaphor, 73 
post-, 74 
pre-, 74 
rule, 135 
stack, 140, 168 
validation, 73 
Tasks, 30 
Toaster model, 52 
Tool 
declarations, 30 
envelopes, 31 
set, 51 
Toolbase, 30 
Transactions, 33, 36-37 
Treaty, 61, 64 
evolution, 69 
full, 64 
in grammars, 88 
in Petri-nets, 84 
invalidation, 69 
metaphor, 61 
multi-site, 64 
retraction, 65 

simple, 63 
symmetric, 64 
validation, 116, 119, 136 
withdrawal, 66 
u 
unexport,65 
in Oz, 113 
unimport, 65 
in Oz, 113 
301 


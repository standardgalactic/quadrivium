Marco Nehmeier
Jürgen Wolff von Gudenberg
Warwick Tucker (Eds.)
 123
LNCS 9553
16th International Symposium, SCAN 2014
Würzburg, Germany, September 21–26, 2014
Revised Selected Papers
Scientific Computing,
Computer Arithmetic,
and Validated Numerics

Lecture Notes in Computer Science
9553
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zürich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407

Marco Nehmeier
• Jürgen Wolff von Gudenberg
Warwick Tucker (Eds.)
Scientiﬁc Computing,
Computer Arithmetic,
and Validated Numerics
16th International Symposium, SCAN 2014
Würzburg, Germany, September 21–26, 2014
Revised Selected Papers
123

Editors
Marco Nehmeier
Institute of Computer Science
University of Würzburg
Würzburg
Germany
Jürgen Wolff von Gudenberg
Universität Würzburg
Würzburg
Germany
Warwick Tucker
Department of Mathematics
Uppsala University
Uppsala
Sweden
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-31768-7
ISBN 978-3-319-31769-4
(eBook)
DOI 10.1007/978-3-319-31769-4
Library of Congress Control Number: 2016934445
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
© Springer International Publishing Switzerland 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG Switzerland

In Memory of Walter Krämer
This book is dedicated to Walter Krämer (1952 – 2014), the longtime chair of the
SCAN Scientiﬁc Committee.
After having suffered from a long and heavy illness,
Walter Krämer passed away Monday, October 27, shortly
after this conference. His whole (scientiﬁc) life was ded-
icated to reliable computing. In the year 1979 he started
his career in reliable ﬂoating point computations with a
diploma (master) thesis on the evaluation of standard
functions for the IBM/370 hexadecimal ﬂoating point
architecture that became one of the cornerstones of the
IBM ACRITH project. In 1987 he earned his PhD from
the University of Karlsruhe. Six years later he advanced
his topic further by ﬁnishing his habilitation on the
calculation of functions and constants in computers. He
worked on several research and management positions at the University of Karlsruhe until
he became professor at the University of Wuppertal, holding the chair of scientiﬁc
computing and software engineering. He provided a great service to the community by
maintaining and further developing C-XSC, one of the most widely used interval arith-
metic packages. He was also active in representing and promoting our ﬁeld. He was the
chair of the GAMM Activity Group on Computer Arithmetic and Scientiﬁc Computing.
After the closing of this group he founded another one with similar content:
Computer-Assisted Proofs and Symbolic Computations. He also was a well-accepted
teacher at the University of Wuppertal and supervised more than 12 PhD theses.
According to his mission to spread the idea of reliable computing, it is not surprising that
several of his PhD students came from abroad, e.g., Brazil or Egypt.
All in all Walter Krämer was one of the most active promoters of our ﬁeld. He was
never afraid of difﬁcult and time-consuming work when the idea of reliable computing
was to be pushed. He traveled extensively and had contacts with researchers all around
the world. Even in the advanced state of his disease, he tried to keep in touch with his
research group.
The whole community and I in particular will always remember our colleague and
friend Walter Krämer.
March 2016
Jürgen Wolff von Gudenberg
A bibliography of Walter Krämer’s work can be found at the end of this book.
Picture: © Angelika Krämer-Maiss, consent for publication is granted.

Preface
SCAN2014,
the 16th
GAMM-IMACS International Symposium on Scientiﬁc
Computing, Computer Arithmetic, and Validated Numerics, was held in Würzburg,
Germany, September 21–26, 2014. This conference continued the series of interna-
tional SCAN symposia initiated in the late 1980s by the University of Karlsruhe,
Germany, and organized under the joint auspices of GAMM and IMACS. SCAN
symposia have been held in Germany, Europe, the USA, Russia, and Japan. The next
two SCAN conferences are scheduled to be held in Sweden and Japan.
The main concerns of research addressed by SCAN conferences are validation,
veriﬁcation, or reliable assertions of numerical computations. Interval arithmetic and
other treatments of uncertainty are developed as appropriate tools. In computer science
validation usually means ﬁnding the right model, whereas veriﬁcation means imple-
menting the model correctly. In our context, indeed, we use both validation and ver-
iﬁcation when solving real-life problems. It makes sense to consider the mathematical
modeling technology in more detail and to understand what is coupled with this or that
concept. We have:
Real-life
⇒Field oriented ⇒Mathematical ⇒Numerical
phenomenon
model
model
answer
“Validation” applies to the ﬁrst and second arrows in the scheme, while “veriﬁcation”
characterizes the third one. Numerical methods, algorithms, and computations are
responsible for the third arrow, too; they are not present in the ﬁrst and second stages
of the modeling process. This is why in connection with numerical methods “veriﬁ-
cation” is more appropriate. Worrying about whether the thickness of the input inter-
vals correctly captures uncertainty in the physical problem, however, is validation.1
One of the duties of the SCAN conferences is the awarding of the Moore Prize for
the application of interval arithmetic. This year Prof. Kenta Kobayashi was selected for
his research on the wave equation.
The symposium attracted more than 80 participants from 18 countries. The fol-
lowing seven invited lectures were given: Sylvie Boldo (Inria, France) gave a lecture
about formal veriﬁcation of tricky numerical computations. Ekaterina Auer (University
of Duisburg-Essen, Germany) talked about result veriﬁcation and uncertainty man-
agement in engineering applications. Bartlomiej Jacek Kubica (Warsaw University of
Technology, Poland) presented interval methods for solving of quantiﬁed nonlinear
problems. Interval arithmetic, one of the main subjects of the SCAN conferences, has
been standardized as IEEE Std 1788. Many participants of the symposium were active
in this working group. The technical editor John Price (Cardiff University, UK)
introduced the architecture of this standard in a plenary talk. Andrej Bauer (University
1 Discussion initiated by S. Shary.

of Ljubljana, Slovenia) reported on programming techniques for exact real arithmetic.
John Gustafson (Ceranovo Inc., USA) proposed an energy-efﬁcient and massively
parallel approach to valid numerics, which may replace ﬂoating point arithmetic in the
future. Algorithmic and software challenges at extreme scales were the topic of Jack
Dongarra’s (University of Tennessee and ORNL, USA) lecture.
More than 60 contributed talks were given. A strict and careful reviewing process
resulted in the 22 contributions that are collected in this volume. We take the oppor-
tunity to thank all the reviewers for their detailed comments presented in time. A great
help in organizing this review process was offered by Warwick Tucker (Uppsala
University, Sweden), the host of SCAN2016—many thanks.
I further want to thank the members of the Scientiﬁc Committee, whoever I asked
gave me immediate feedback, as well as the organizers of the last three meetings,
Vladik Kreinovich (El Paso), Nathalie Revol (Lyon), and Sergey Shary (Novosibirsk)
in particular.
I express a special thanks to all our sponsors, their donations made the conference
possible. Organizing such a conference involves a lot of work, but organizing this
conference was a real pleasure for me, thanks to the tremendous assistance I got from
the Organizing Committee comprising Alexander Dallmann, Fritz Kleemann, Marco
Nehmeier, Anika Schwind, and Susanne Stenglin.
December 2015
Jürgen Wolff von Gudenberg
Country
Participants
Country
Participants
Germany
18
UK
2
Japan
17
Austria
1
France
16
Brazil
1
USA
7
China
1
Russia
6
India
1
Czech Republic
3
The Netherlands
1
Bulgaria
2
Poland
1
Egypt
2
Slovenia
1
Hungary
2
Sweden
1
VIII
Preface

Organization
Program Committee
Jürgen Wolff von Gudenberg
Würzburg, Germany [Chair]
Vladik Kreinovich
El Paso, USA
Marco Nehmeier
Würzburg, Germany
Nathalie Revol
Lyon, France
Sergey P. Shary
Novosibirsk, Russia
Warwick Tucker
Uppsala, Sweden
Organizing Committee
Jürgen Wolff von Gudenberg
Würzburg, Germany [Chair]
Alexander Dallmann
Würzburg, Germany
Fritz Kleemann
Würzburg, Germany
Marco Nehmeier
Würzburg, Germany
Anika Schwind
Würzburg, Germany
Susanne Stenglin
Würzburg, Germany
Proceedings Coordinators
Marco Nehmeier
Würzburg, Germany
Anika Schwind
Würzburg, Germany
Scientiﬁc Committee
G. Alefeld
Karlsruhe, Germany
J.-M. Chesneaux
Paris, France
G.F. Corliss
Milwaukee, USA
T. Csendes
Szeged, Hungary
A. Frommer
Wuppertal, Germany
R.B. Kearfott
Lafayette, USA
W. Kraemer
Wuppertal, Germany
V. Kreinovich
El Paso, USA
U. Kulisch
Karlsruhe, Germany
W. Luther
Duisburg, Germany
G. Mayer
Rostock, Germany
S. Markov
Soﬁa, Bulgaria
J.-M. Muller
Lyon, France
M. Nakao
Fukuoka, Japan
M. Plum
Karlsruhe, Germany

N. Revol
Lyon, France
J. Rohn
Prague, Czech Republic
S. Rump
Hamburg, Germany
S. Shary
Novosibirsk, Russia
Yu. Shokin
Novosibirsk, Russia
W. Walter
Dresden, Germany
J. Wolff von Gudenberg
Würzburg, Germany
N. Yamamoto
Tokyo, Japan
X
Organization

Contents
Interval Arithmetic and Interval Functions
Hausdorff Continuous Interval Functions and Approximations . . . . . . . . . . .
3
Roumen Anguelov and Svetoslav Markov
Replacing Branches by Polynomials in Vectorizable Elementary Functions. . .
14
Olga Kupriianova and Christoph Lauter
The Forthcoming IEEE Standard 1788 for Interval Arithmetic . . . . . . . . . . .
23
John Pryce
Uncertainty
Numerical Probabilistic Approach for Optimization Problems . . . . . . . . . . . .
43
Boris Dobronets and Olga Popova
Towards the Possibility of Objective Interval Uncertainty. . . . . . . . . . . . . . .
54
Luc Longpré, Olga Kosheleva, and Vladik Kreinovich
How Much for an Interval? a Set? a Twin Set? a p-Box? A Kaucher
Interval? Towards an Economics-Motivated Approach to Decision
Making Under Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
Joe Lorkowski and Vladik Kreinovich
Sliding Mode Approaches Considering Uncertainty for Reliable Control
and Computation of Confidence Regions in State and Parameter Estimation . . .
77
Luise Senkel, Andreas Rauh, and Harald Aschemann
Linear Algebra
Efficiency of Reproducible Level 1 BLAS . . . . . . . . . . . . . . . . . . . . . . . . .
99
Chemseddine Chohra, Philippe Langlois, and David Parello
Tight Bounds on the Radius of Nonsingularity . . . . . . . . . . . . . . . . . . . . . .
109
David Hartman and Milan Hladík
Optimal Preconditioning for the Interval Parametric Gauss–Seidel Method . . .
116
Milan Hladík
Reproducible and Accurate Matrix Multiplication . . . . . . . . . . . . . . . . . . . .
126
Roman Iakymchuk, David Defour, Sylvain Collange, and Stef Graillat

Outer Bounds for the Parametric Controllable Solution Set
with Linear Shape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
Evgenija D. Popova
Reserve of Characteristic Inclusion as Recognizing Functional for Interval
Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
Irene A. Sharaya and Sergey P. Shary
Global Optimisation
Convergence and Inclusion Isotonicity of the Tensorial Rational
Bernstein Form. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
Jürgen Garloff and Tareq Hamadneh
The Bernstein Branch-and-Bound Unconstrained Global Optimization
Algorithm for MINLP Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
Bhagyesh V. Patil and P.S.V. Nataraj
Dynamical Systems
Interval Regularization Approach to the Firordt Method of the
Spectrophotometric Analysis of the Non-separated Mixtures . . . . . . . . . . . . .
201
Valentin Golodov
Computing Capture Tubes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
Luc Jaulin, Daniel Lopez, Vincent Le Doze, Stéphane Le Menec,
Jordan Ninin, Gilles Chabert, Mohamed Saad Ibnseddik,
and Alexandru Stancu
Some Remarks on the Rigorous Estimation of Inverse Linear
Elliptic Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
225
Takehiko Kinoshita, Yoshitaka Watanabe, and Mitsuhiro T. Nakao
Verified Parameter Identification for Dynamic Systems with Non-Smooth
Right-Hand Sides . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
Andreas Rauh, Luise Senkel, and Harald Aschemann
Exponential Enclosure Techniques for Initial Value Problems with Multiple
Conjugate Complex Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
Andreas Rauh, Ramona Westphal, Harald Aschemann,
and Ekaterina Auer
PDE
Curve Veering for the Parameter-dependent Clamped Plate . . . . . . . . . . . . .
259
Henning Behnke
XII
Contents

Numerical Verification for Elliptic Boundary Value Problem
with Nonconforming P1 Finite Elements . . . . . . . . . . . . . . . . . . . . . . . . . .
269
Tomoki Uda
Bibliography of Prof. Walter Krämer. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Werner Hofschuster, Evgenija D. Popova, and Ralph Baker Kearfott
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
291
Contents
XIII

Interval Arithmetic and Interval
Functions

HausdorﬀContinuous Interval Functions
and Approximations
Roumen Anguelov1,2 and Svetoslav Markov2(B)
1 Deptartment Mathematics and Applied Mathematics,
University of Pretoria, Pretoria, South Africa
roumen.anguelov@up.ac.za
2 Institute of Mathematics and Informatics,
Bulgarian Academy of Sciences, Soﬁa, Bulgaria
smarkov@bio.bas.bg
Abstract. The set of interval Hausdorﬀcontinuous functions constitutes
the largest space preserving basic algebraic and topological structural
properties of continuous functions, such as linearity, ring structure,
Dedekind order completeness, etc. Spaces of interval functions have impor-
tant applications not only in the construction of numerical methods and
algorithms, but to problems in abstract areas such as real analysis, set-
valued analysis, approximation theory and the analysis of PDEs. In this
work, we summarize some basic results about the family of interval Haus-
dorﬀcontinuous functions that make interval analysis a bridge between
numerical and real analysis. We focus on some approximation issues for-
mulating a new result on the Hausdorﬀapproximation of Hausdorﬀcon-
tinuous functions by interval step functions. The Hausdorﬀapproximation
of the Heaviside interval step function by sigmoid functions arising from
biological applications is also considered, and an estimate for the Hausdorﬀ
distance is obtained.
Keywords: Interval functions · Baire semi-continuous functions · Haus-
dorﬀcontinuous functions · Dilworth continuous functions · Sigmoid
functions
1
Introduction
Functions having discontinuities are encountered in many situations. A widely
used class of discontinuous functions is the class of Baire upper (lower) semi-
continuous functions [13]. Dilworth restricted the class of Baire semi-continuous
functions up to normal semi-continuous functions [15]. Both classes are con-
veniently reformulated in terms of interval-valued functions using graph com-
pletion operators [3]. For example, when a graph is completed, the Dilworth
normal semi-continuous functions are the Hausdorﬀcontinuous interval-valued
functions. Such a reformulation leads to interesting original results oriented to
practical applications.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 3–13, 2016.
DOI: 10.1007/978-3-319-31769-4 1

4
R. Anguelov and S. Markov
It has been shown that the space of Hausdorﬀcontinuous functions is the
largest linear space of interval functions [3–5,22]. This space has important appli-
cations in the theory of PDEs and real analysis [1,6–9,12]. Moreover, the space
of Hausdorﬀcontinuous functions has a special place in Interval Analysis as well,
more speciﬁcally in the analysis of interval-valued functions [2]. It has been also
shown that the practically relevant set, in terms of providing tight enclosures of
sets of real continuous functions, is the set of Dilworth continuous interval-valued
functions [5].
The relation between Baire semi-continuous functions and interval-valued
functions establishes a new paradigm for interval analysis as part of analysis
rather than (or in addition to) the ﬁeld of numerical methods, where it is cur-
rently classiﬁed, see e.g. [23]. This provides a new research direction of applying
interval analysis to abstract mathematical problems. It has been shown that
the spaces of interval-valued functions1 have important applications not only
in the construction of numerical methods but to problems in more abstract
areas like real analysis, set-valued analysis, approximation theory and analysis
of PDEs. Some of the more interesting results are: (i) A generalization of the
order convergence structure on the space of Hausdorﬀcontinuous functions to a
convergence structure on the space of minimal upper semi-continuous compact
set-valued (shortly: usco) maps [10]; (ii) All rational extensions and their metric
completions of C(X) are subspaces of the space of Hausdorﬀcontinuous func-
tions [11]; (iii) The solutions of large classes of nonlinear systems of PDEs can
be presented with Hausdorﬀcontinuous interval functions [6,7]; (iv) The theory
of continuous viscosity solutions of Hamilton-Jacobi equations can be recast in
the setting of Hausdorﬀcontinuous functions, where the discontinuous solutions
are accommodated in a natural way [8].
In the next Sect. 2 we summarize some basic results concerning the class
of interval Hausdorﬀcontinuous functions and the related classes of interval
functions. Section 3 contains a new result on the Hausdoﬀapproximation of
Hausdorﬀcontinuous functions by interval step functions. Section 4 is devoted
to a new result on the approximation of interval step functions by a class of
sigmoid functions arising from biological applications.
2
Classes of Interval Functions: Basic Results
The concept of Hausdorﬀcontinuity (H-continuity) generalizes the familiar con-
cept of continuity in such a way that many essential properties of the usual
continuous real functions are preserved. The set C(Ω) of all continuous real
functions deﬁned on a subset Ω ⊂Rn is a commutative ring with respect to the
point-wise deﬁned addition and multiplication of functions and a linear space
with respect to addition and multiplication by a scalar. Is it possible to extend
the algebraic operations on C(Ω) to the set H(Ω) of H-continuous functions in
a way that preserves these two basic algebraic structures, that is, the set H(Ω)
1 For brevity we shall further write “interval function” instead of “interval-valued
function”.

HausdorﬀContinuous Interval Functions and Approximations
5
to become a commutative ring and linear space with respect to the extended
operations? It turns out that the answer is aﬃrmative as brieﬂy shown in the
sequel.
2.1
Basic Notation and Deﬁnitions: Baire Continuous Functions
Deﬁnition 1. [13] A real-valued function f is upper (lower) semi-continuous at
a point x0 if the function values for arguments near x0 are either close to f(x0)
or less (greater) than f(x0).
Intervals on the real line R are denoted as a = [a, a] = {x : a ≤x ≤a}, and
the set of all intervals is denoted IR = {[a, a] : a, a ∈R, a ≤a}; denote also
w(a) = a −a, and |a| = max{|a|, |a|}.
Let Ω ⊆Rn be an open set. A real or interval function f on Ω is locally
bounded if for every x ∈Ω there exist δ > 0 and M ∈R such that |f(y)| <
M, y ∈Bδ(x), Bδ(x) = {y ∈Ω : ||x −y|| < δ}. Denote
A(Ω) = {f : Ω →I R, f locally bounded},
A(Ω) = {f : Ω →R, f locally bounded} ⊆
A(Ω).
Deﬁnition 2. D is a dense subset of Ω. The lower/upper Baire operators
I(D, Ω, ·), S(D, Ω, ·) :
A(D) →A(Ω) are deﬁned for f = [f, f] ∈A(D)
and x ∈Ω by
I(D, Ω, f)(x) = sup
δ>0
inf {f(y) : y ∈Bδ(x) ∩D},
S(D, Ω, f)(x) = inf
δ>0 sup {f(y) : y ∈Bδ(x) ∩D}.
Deﬁnition 3. The graph completion operator F : A(D) →A(Ω) for f ∈A(D)
is deﬁned as
F(D, Ω, f)(x) = [I(D, Ω, f)(x), S(D, Ω, f)(x)], x ∈Ω, f ∈A(D).
For D = Ω we write
I(f) = I(Ω, Ω, f), S(f) = S(Ω, Ω, f), F(f) = F(Ω, Ω, f).
Using end-point presentation of functions: f = [f, f] ∈A(Ω) we can write
I(D, Ω, f) = I(D, Ω, f),
S(D, Ω, f) = S(D, Ω, f),
F(D, Ω, f) = [I(D, Ω, f), S(D, Ω, f)].
Deﬁnition 4. A function f ∈A(Ω) is S-continuous, if F(f) = f.
Deﬁnition 5. A function f ∈A(Ω) is D-continuous, if for every dense subset
D of Ω, F(D, Ω, f) = f.

6
R. Anguelov and S. Markov
Deﬁnition 6. A function f ∈A(Ω) is H-continuous, if for every function g ∈
A(Ω) such that g(x) ⊆f(x),
x ∈Ω, F(g)(x) = f(x),
x ∈Ω.
Theorem 1. For every f ∈H(Ω) the set Wf = {x ∈Ω : w(f(x)) > 0} is of
ﬁrst Baire category (that is, H-continuous functions are “thin”).
H-continuous functions do not diﬀer much from the usual real-valued continuous
functions because they assume interval values only on a meagre set2.
2.2
Arithmetic Operations in H(R)
Interval arithmetic operations are denoted as usually: for a = [a, a], b = [b, b] ∈I R
we have a + b = {α + β : α ∈a,
β ∈b}, a × b = {αβ : α ∈a,
β ∈b};
or endpoint-wise: [a, a]+[b, b] = [a+b, a+b], [a, a]×[b, b] = [min{ab, ab, ab, ab},
max{ab, ab, ab, ab}].
For functions f, g ∈A(Ω), f = [f, f], g = [g, g], x ∈Ω, we have [2]
(f + g)(x) = f(x) + g(x) = [f(x) + g(x), f(x) + f(x)],
(f × g)(x) = f(x) × g(x) = [min M, max M],
M = {f(x)g(x), f(x)g(x), f(x)g(x), f(x)g(x)}.
Example 1. Denote by h ∈H(R) the (interval) Heaviside step function given by
h(x) =
⎧
⎨
⎩
0,
if x < 0,
[0, 1], if x = 0,
1,
if x > 0,
(1)
and g = (−1) × h ∈H(R). For the sum h + g we have
(h + g)(x) = h(x) + g(x) =

0,
if x < 0 or x > 0
[−1, 1], if x = 0,
showing that h + g /∈H(R).
Theorem 2. (a) There exists a unique function p ∈H(Ω) such that p(x) ⊆
(f + g)(x), x ∈Ω; (b) There exists a unique function q ∈H(Ω) such that
q(x) ⊆(f × g)(x), x ∈Ω.
We deﬁne H-addition and H-multiplication of H-continuous functions f, g ∈
H(Ω) via interval operations as follows.
Deﬁnition 7. (a) f ⊕g is the unique H-continuous function p(x) as deﬁned by
Theorem 2 (a), that is satisfying (f ⊕g)(x) ⊆(f + g)(x), x ∈Ω ; (b) f ⊗g
is the unique H-continuous function q(x) as deﬁned by Theorem 2 (b), that is
satisfying (f ⊗g)(x) ⊆(f × g)(x), x ∈Ω.
2 In topology, a meagre set, also called a set of ﬁrst Baire category, is a set that,
considered as a subset of a (usually larger) topological space, is small or negligible.

HausdorﬀContinuous Interval Functions and Approximations
7
Example 2. For the H-sum of the Heaviside step function h ∈H(R) given by
(1) and g = (−1) × h ∈H(R) we have (h ⊕g)(x) = 0, x ∈R.
Theorem 3. The set H(Ω) is a commutative ring with identity with respect to
the H-operations ⊕and ⊗.
Remark. Note that the H-operations ⊕and ⊗are not point-wise in general. At
a point where both operands have interval values the value of the H-sum ⊕or
the H-product ⊗are not determined by the values of the operands only at that
point but rather by the values of the operands in a neighborhood of the point.
In the special case when one of the operands is a real (point) valued function
the operations ⊕and ⊗coincide with the point-wise operations, namely we have:
(f ⊕g)(x) = (f + g)(x) if w(f(x)) = 0 or w(g(x)) = 0,
(f ⊗g)(x) = (f × g)(x) if w(f(x)) = 0 or w(g(x)) = 0.
More properties of the H-operations can be found in [4].
2.3
The Set of H-Continuous Functions as a Linear Space
Multiplication by a scalar is deﬁned as multiplication by a constant function.
Since the value of this function is a real number this multiplication coincides
with the point-wise multiplication
(α ∗f)(x) = α ∗f(x) =
[α f(x), αf(x)] if α ≥0,
[α f(x), αf(x)] if α < 0.
The set H(Ω) is a linear space with respect to “⊕” and “∗”. Moreover, it is
the largest space of interval functions as stated in the next theorem.
Theorem 4. [5] Let G(Ω) be the set of all D-continuous interval functions.
Assume that the set P ⊆G(Ω) is closed under inclusion in the sense that
f ∈P, g ∈G(Ω)
g(x) ⊆f(x), x ∈Ω.

=⇒g ∈P.
If P ⊆G(Ω) is a linear space, then P ⊆H(Ω).
Hence the H-operations “⊕”, “⊗” cannot be extended further than H(Ω) in
a way preserving the algebraic structure of C(Ω).
3
HausdorﬀApproximations Using Step Functions
3.1
HausdorﬀDistance and Modulus of H-Continuity
Let us recall that the Hausdorﬀdistance (H-distance) ρ(f, g) between two func-
tions f, g ∈A(Ω), Ω ⊆Rn, is deﬁned as the distance between their completed
graphs F(f) and F(g) considered as closed subsets of Ω × R [17], [21]. More
precisely,

8
R. Anguelov and S. Markov
ρ(f, g) = max{ sup
A∈F (f)
inf
B∈F (g) ||A −B||,
sup
B∈F (g)
inf
A∈F (f) ||A −B||},
(2)
wherein ||.|| is a norm in Rn+1. In technical proofs presented in the sequel
we assume that the norm in Rn+1 is the maximum norm, that is for A =
(a1, ..., an+1) we have ||A|| = max{|a1|, ..., |an+1|}. However, all statements
remain true for any norm due to the equivalence of the norms in Rn+1.
In the space of S-continuous functions on Ω, the H-distance satisﬁes the
axioms of a metric. There is a natural connection between the H-continuous func-
tions and the H-distance. For example, one can easily see that an S-continuous
function f is H-continuous if and only if ρ(f, f) = 0. Indeed, it follows from
the deﬁnition that f is H-continuous if and only if F(f) = F(f) or, equivalently,
ρ(F(f), F(f)) = 0. The link between the two concepts is further discussed below
in terms of the modulus of H-continuity.
For δ > 0, the operators Iδ and Sδ are deﬁned for f ∈A(Ω) as follows
Iδ(f)(x) = inf{f(y) : y ∈Bδ(x)}, x ∈Ω,
(3)
Sδ(f)(x) = sup{f(y) : y ∈Bδ(x)}, x ∈Ω.
(4)
It is easy to see that in terms of Deﬁnition 2 we have
I(f)(x) = sup
δ>0
Iδ(f)(x),
S(f)(x) = inf
δ>0 Sδ(f)(x), x ∈Ω.
Deﬁnition 8. The modulus of H-continuity τ(f; δ) for given f ∈A(Ω) and
δ > 0 is the H-distance between Iδ(f) and Sδ(f), that is
τ(f; δ) = ρ(Iδ(f), Sδ(f)) = ρ(F(Iδ(f)), F(Sδ(f))).
Theorem 5. [21]
An
S-continuous
function
f
is
H-continuous
iﬀ
limδ→0 τ(f; δ) = 0.
3.2
Interval Step Functions as an Approximation Tool
The usual concept of step-functions of a real argument can be extended to H(Ω)
as follows.
Deﬁnition 9. A function f ∈H(Ω) is called a step function if there exists a
collection {U1, U2, ..., Um} of open subsets of Ω with the following properties
(i) Ui ∩Uj = ∅for i ̸= j,
(ii) the set V =
k
i=1
Ui is dense in Ω,
(iii) for every i ∈{1, 2, ..., k}, f is a real constant on Ui.
It is easy to see that a step function is completely determined by its values
on the open set V . In fact we have f = F(V, Ω, f|V ). Similarly to the real step
functions, an interval step function f assumes ﬁnite number of values, namely the

HausdorﬀContinuous Interval Functions and Approximations
9
constant values on the sets Ui, i = 1, 2, ..., k, and some real intervals with end-
points equal to the constant functional values. Further, we note that the set of
step functions is a linear subspace of H(Ω). Indeed, the sum of step functions is
a step function and so is the product of a step function and a real number. In the
next theorem we establish some approximation properties of the step functions.
Theorem 6. Let f ∈H(Ω). For every ε > 0 there exists a step function ϕ such
that ρ(f, ϕ) < ε.
Proof. In view of Theorem 5, there exists δ > 0 such that τ(f; δ) < ε. Then
consider any collection of open sets {U1, U2, ..., Um} with the properties (i) and
(ii) as given in Deﬁnition 9 and such that the diameter of each set is smaller
than δ. These can be constructed for example by partitioning Ω via the planes
xl = iδ, i ∈Z, l = 1, 2, ..., n. Since f assumes interval values only on a meagre
subset of Ω, for every i ∈{1, 2, ..., k} there exists x(i) ∈Ui such that f(x(i)) ∈R.
Deﬁne ψ(x) = f(x(i)) for x ∈Ui, i = 1, 2, ..., k, and ϕ = F(V, Ω, ψ).
We show that ϕ is the required function. First let us note that ϕ is a step
function. Indeed, since ψ is continuous on any Ui, the operator F does not change
the values of ψ on each of these sets, so that ϕ(x) = ψ(x), x ∈V . This implies
both that ϕ is H-continuous and that it is a step function. Further, from the
deﬁnition of ϕ it follows that
Iδ(f) ≤ϕ ≤Sδ(f).
Therefore, we have
ρ(f, ϕ) ≤ρ(Iδ(f), Sδ(f)) = τ(f; δ) < ε,
which proves the theorem.
In the special case of a real argument, the interval step-functions have a
simple representation in terms of the Heaviside step function h given in (1).
Indeed, when Ω = R, the sets Ui, i = 1, ..., k, associated with an interval step
function f in terms of Deﬁnition 9 are open intervals of the form (di−1, di), where
d0 = −∞, dk = +∞, and d1, d2, ..., dk−1 is a ﬁnite increasing sequence of reals.
Let f(x) = ci for x ∈(di−1, di). A familiar rectangular pulse on the interval
[di−1, di], i = 1, ..., k −1, is represented as
h(x −di−1) −h(x −di)
Then the step function f is given by
f(x)=c1(1−h(x−d1)) ⊕c2(h(x−d1)−h(x−d2)) ⊕... ⊕ck−1(h(x−dk−2)−h(x−dk−1))
⊕ckh(x−dk−1) = c1 +
k−1

i=1
(ci+1 −ci)h(x−di).
Note that f is discontinuous only at the points d1, ..., dk−1 where it assumes
interval values. More precisely, we have

10
R. Anguelov and S. Markov
f(di) =
[ci, ci+1] if ci < ci+1
[ci+1, ci] if ci > ci+1 .
For other approximation properties using Hausdorﬀmetric adapted to reso-
lution analysis one may consult Sect. 6 of [3].
4
Approximation by Sigmoid Functions
Sigmoid functions ﬁnd multiple applications to neural networks and cell growth
population models [14,20]. A sigmoid function on R with a range [a, b] is deﬁned
as a monotone function s(t) : R →[a, b] such that limt→−∞s(t) = a, and
limt→∞s(t) = b. One usually considers continuous (or even smooth) sigmoid
functions. Within the class of H-continuous interval functions, the Heaviside
step function is a particular case of sigmoid function.
4.1
Approximation by Sigmoid Logistic Functions
An important class of smooth sigmoid functions arises from population growth
models. A classical example is the familiar Verhulst population growth model,
also known as logistic model. One can arrive to this model starting from the
reaction equation U + X
k
−→
X + X, where U is a nutrient substance, X
is a particular population and k is the speciﬁc growth rate of the population.
The biological interpretation ot this reaction equation is that the nutrient U
is utilized by the population X leading to the reproduction of the population.
Denoting the biomass of X by x and the mass (concentration) of U by u and
applying the mass action law, one obtains the dynamical system
du/dt = −kxu,
dx/dt = kxu,
u(0)
= u0, x(0) = x0.
Noticing that u′ + x′ = 0, hence u + x = x0 + u0 = const = a, we can substitute
u = a −x in the diﬀerential equation for x to obtain the Verhulst diﬀerential
equation x′ = kux = kx(a −x). The latter is usually written with a normalized
rate constant k := k/a as
dx
dt = k
ax(a −x) = kx

1 −x
a
	
, x(0) = x0.
(5)
The solution x to Eq. (5) passing through the point (0, x(0) = x0 = a/2) is the
(basic) logistic sigmoid function:
s0(t) =
a
1 + be−kt ; b = a −x0
x0
= 1.
(6)
In what follows we shall estimate the H-distance between a step function f and
a logistic sigmoid function g. Without loss of generality we can consider the Heav-
iside step function f = ah and the logistic sigmoid function (6): g = s0. Accord-
ing to (2) the H-distance ρ(f, g) between two functions f, g ∈A(Ω) for Ω ⊂R

HausdorﬀContinuous Interval Functions and Approximations
11
makes use of the maximum norm in R2 so that the distance between the points
A = (tA, xA), B = (tB, xB) in R2 is ||A−B|| = max(|tA −tB|, |xA −xB|). In that
case the H-distance d = ρ(h, s0) between the Heaviside step function ah and the
sigmoid function (6) satisﬁes the relations 0 < d < a/2 and a−s0(d) = d, that is
(a −d)/d = ekd, (0 < d < a/2).
(7)
Obviously d →0 implies k →∞(and vice versa). From (7), a straightforward
expression for the rate parameter k in terms of d follows:
k = 1
d ln a −d
d
= O(d−1 ln(d−1)).
(8)
4.2
Estimate for the H-Distance in Terms of the Rate Parameter
Relation (8) gives an estimate of the rate k in terms of the H-distance d. The
following theorem gives a relation for the H-distance d in terms of the rate
parameter k. For simplicity we assume a = 1, denoting thus in the sequel s0(t) =
(1 + e−kt)−1.
Theorem 7. The Hausdorﬀdistance d = ρ(h, s) between the Heaviside step
function h0 and the sigmoid Verhulst function s0 can be expressed in terms of
the reaction rate k as follows:
d = ln(k + 1)
k + 1

1 + O

ln ln(k + 1)
ln(k + 1)

.
(9)
Proof. Assuming a = 1 relation (7) becomes (1 −d)/d = ekd for 0 < d < 1/2.
This implies kd = ln(1/d) + ln(1 −d). In order to express d in terms of k, let us
examine the function
f(d) = kd −ln(1/d) −ln(1 −d), 0 < d < 1/2.
From lim f(d)d→0,d>0 = −∞, lim f(d)d→1/2,d<1/2 = k/2 > 0 we conclude that
f(d) = 0 possesses a solution in (0, 1/2). From f ′(d) = k+1/d+1/(1−d) > 0 we
conclude that function f is strictly monotonically increasing, hence f(d) = 0 has
an unique solution d(k) in (0, 1/2). For k →∞we have d(k) →0, hence ln(1 −
d(k)) = −d(k) + O(d(k)2). Consider then the function g(d) = (k + 1)d −ln(1/d)
which approximates function f with d →0 as O(d2); in addition g′(d) > 0. So
we can further denote by d(k) the (unique) zero of g and study g instead of f.
We look for two reals d−and d+ such that g(d−) < 0 and g(d+) > 0 (leading
to g(d−) < g(d(k)) < g(d+) and thus d−< d(k) < d+). Trying d−= 1/(k + 1)
and d+ = ln(k + 1)/(k + 1) we obtain g(1/(k + 1)) = 1 −ln(k + 1) < 0 and
g(ln(k + 1)/(k + 1)) = ln ln(k + 1) > 0 proving the estimates 1/(k + 1) < d(k) <
ln(k + 1)/(k + 1). To ﬁnd a better lower bound we compute
g

ln(k + 1)
k + 1

1 −ln ln(k + 1)
ln(k + 1)

= ln

1 −ln ln(k + 1)
ln(k + 1)

< 0.

12
R. Anguelov and S. Markov
We thus obtain
ln(k + 1)
k + 1
−ln ln(k + 1)
k + 1
< d(k) < ln(k + 1)
k + 1
,
which implies (9).
Remark. In the general case a ̸= 1 one should substitute in (9) k+1 by k+a−1.
5
Conclusions
We brieﬂy summarized some basic results (Theorems 1–5) about H-continuous
functions and their application to problems in abstract areas such as real analy-
sis, approximation theory, set-valued analysis and analysis of PDEs. We then for-
mulated and proved a new result (Theorem 6) on the Hausdorﬀapproximation
of H-continuous functions by interval step functions deﬁned on an open subset of
Rn. Finally we discussed some applications of H-continuous functions to biological
dynamic processes, in particular, we considered the remarkable phenomenon that
certain enzyme kinetic and population growth processes develop almost step-wise
[16,20]. Such processes are usually described or approximated by smooth sigmoid
functions (especially in the theory of artiﬁcial neural networks). However, it is
possible that H-continuous step-wise functions can be also conveniently used. To
substitute a sigmoid function by a step function, we need to know the approxi-
mation error (Theorem 7). Biological processes are often very sensitive and can
be eﬀectively studied within the framework of interval analysis [19]. In addition,
the input data coming from biological experiments are usually rather uncertain
and thus can be represented as interval data. If these interval data are guaran-
teed (that is they include the measurement errors), then numerical methods and
programming tools with automatic result veriﬁcation can be used [18].
The presented results suggest that interval analysis, apart from being cur-
rently associated with numerical analysis, can also be considered as belonging
to the ﬁeld of real analysis. We may thus consider interval analysis as a bridge
between real and numerical analysis, a bridge that extends both subjects and
uniﬁes them into a common scientiﬁc area.
Acknowledgments. RA acknowledges partial support of the National Research
Foundation of South Africa. RA and SM acknowledge partial support by the Institute
of Mathematics and Informatics at the Bulgarian Academy of Sciences. The authors
thank Prof. Kamen Ivanov for the analysis and derivation of formula (9). They are
grateful to the anonimous reviewer for his careful reading and many remarks.
References
1. Anguelov, R.: Dedekind order completion of C(X) by Hausdorﬀcontinuous func-
tions. Quaestiones Mathematicae 27, 153–170 (2004)

HausdorﬀContinuous Interval Functions and Approximations
13
2. Anguelov, R., Markov, S.: Extended segment analysis, Freiburger Intervall-
Berichte, Inst. Angew. Math, U. Freiburg i. Br. 10, pp. 1–63 (1981)
3. Anguelov, R., Markov, S., Sendov, B.: On the normed linear space of Hausdorﬀ
continuous functions. In: Lirkov, I., Margenov, S., Wa´sniewski, J. (eds.) LSSC 2005.
LNCS, vol. 3743, pp. 281–288. Springer, Heidelberg (2006)
4. Anguelov, R., Markov, S., Sendov, B.: Algebraic operations on the space of Haus-
dorﬀcontinuous interval functions. In: Bojanov, B. (ed.) Constructive Theory of
Functions, pp. 35–44. Marin Drinov Academic Publishing House, Soﬁa (2006)
5. Anguelov, R., Markov, S., Sendov, B.: The set of Hausdorﬀcontinuous functions–
the largest linear space of interval functions. Reliable Comput. 12, 337–363 (2006).
http://dx.doi.org/10.1007/s11155-006-9006-5
6. Anguelov, R., Rosinger, E.E.: Hausdorﬀcontinuous solutions of nonlinear PDEs
through the order completion method. Quaest. Math. 28, 271–285 (2005)
7. Anguelov, R., Rosinger, E.E.: Solving large classes of nonlinear systems of PDEs.
Comput. Math. Appl. 53, 491–507 (2007)
8. Anguelov, R., Markov, S., Minani, F.: Hausdorﬀcontinuous viscosity solutions
of Hamilton-Jacobi equations. In: Lirkov, I., Margenov, S., Wa´sniewski, J. (eds.)
LSSC 2009. LNCS, vol. 5910, pp. 231–238. Springer, Heidelberg (2010)
9. Anguelov, R., van der Walt, J.H.: Order Convergence on C(X). Quaestiones Math-
ematicae 28(4), 425–457 (2005)
10. Anguelov, R., Kalenda, O.F.K.: The convergence space of minimal USCO map-
pings. CZECHOSLOVAK MATH. J. 59(1), 101–128 (2009)
11. Anguelov, R.: Rational extensions of C(X) via Hausdorﬀcontinuous functions.
Thai J. Math. 5(2), 261–272 (2007)
12. Anguelov, R., van der Walt, J.H.: Algebraic and topological structure of some
spaces of set-valued maps. Comput. Math. Appl. 66, 1643–1654 (2013)
13. Baire, R.: Lecons sur les Fonctions Discontinues. Collection Borel, Paris (1905)
14. Costarelli, D., Spigler, R.: Approximation results for neural network operators
activated by sigmoidal functions. Neural Netw. 44, 101–106 (2013)
15. Dilworth, R.P.: The normal completion of the lattice of continuous functions. Trans.
Amer. Math. Soc. 68, 427–438 (1950)
16. Dimitrov, S., Markov, S.: Metabolic Rate Constants: some Computational Aspects,
Mathematics and Computers in Simulation (2015). doi:10.1016/j.matcom.2015.11.
003
17. Hausdorﬀ, F.: Set theory, 2nd edn. Chelsea Publ., New York (1962) [1957], ISBN
978-0821838358 (Republished by AMS-Chelsea 2005)
18. Kraemer, W., Gudenberg, J.W.v (eds.): Scientiﬁc Computing, Validated Numerics,
Interval Methods, Proc. SCAN-2000/Interval-2000, Kluwer/Plenum (2001)
19. Markov, S.: Biomathematics and interval analysis: A prosperous marriage. In:
Christov, C., Todorov, M.D. (eds.) Proceedings of the 2nd International Conference
on Application of Mathematics in Technical and Natural Sciences (AMiTaNS’10),
AIP Conference Proceedings 1301, 26–36 (2010)
20. Markov, S.: Cell Growth Models Using Reaction Schemes: Batch Cultivation, Bio-
math 2/2, 1312301 (2013). http://dx.doi.org/10.11145/j.biomath.2013.12.301
21. Sendov, B.: HausdorﬀApproximations. Kluwer, Boston (1990)
22. van der Walt, J. H.: The Linear Space of HausdorﬀContinuous Interval Functions.
Biomath 2, 1311261 (2013). http://dx.doi.org/10.11145/j.biomath.2013.11.261
23. Mathematics Subject Classiﬁcation (MSC2010), AMS (2010). http://www.ams.
org/mathscinet/msc/msc2010.html

Replacing Branches by Polynomials
in Vectorizable Elementary Functions
Olga Kupriianova(B) and Christoph Lauter
Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606, LIP6, 75005 Paris, France
{olga.kupriianova,christoph.lauter}@lip6.fr
Abstract. One of the goals for the mathematical function generator is to
produce vectorizable codes. Therefore, in the generated code there should
be no branching. As the most mathematical functions are implemented
with domain splitting procedure and piecewise-polynomial approxima-
tion, there are several if-else statements in the ﬁnal code to determine
the corresponding polynomial coeﬃcients. In this paper we propose a sim-
ple idea of replacing these if-else statements by the evaluation of a poly-
nomial function. This is a novel approach that may not work for all the
possible function implementation variants, and it needs to be improved
with the use of some more sophisticated methods.
Keywords: Mathematical functions · Branching · Vectorizable code ·
Interpolation · Reconstruction
1
Introduction
Collection of codes to evaluate mathematical function in some programming
language is called a libm. The standard libms provide a limited set of mathe-
matical functions in single and double precisions. Libms contain code usually for
elementary functions (sin, log, etc.) and several “special functions” like Gamma
or Bessel functions [1]. Hardware producers spend a lot of manpower on main-
tenance and optimization of their proprietary libms. The existing Open source
libraries are not ﬂexible enough and do not provide user speciﬁc implementations
of mathematical functions [2]. As one can deﬁne a huge number of implementa-
tion variants for each mathematical function (diﬀerent domain, accuracy, degree
of the approximation polynomial, etc.) it is not feasible to implement them all
manually. Thus, we have been developing Metalibm, an automatic code gen-
erator that produces ﬂexible implementations of mathematical functions [10].
Functions variants to be generated are deﬁned by a set of parameters like func-
tion name (or algebraic expression), implementation domain, ﬁnal accuracy and
maximum degree of its polynomial approximation. The generated code evaluates
the speciﬁed function the accuracy bounded by the ﬁnal accuracy parameter.
Since the prevalence of SIMD instructions on modern processors, the code
generation of vectorizable implementations is of big interest as well. To make
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 14–22, 2016.
DOI: 10.1007/978-3-319-31769-4 2

Replacing Branches by Polynomials in Vectorizable Elementary Functions
15
Metalibm produce vectorizable codes, the algorithms without (or almost with-
out) branching have to be used. For exponential and logarithmic functions vec-
torized loop calls reduce the computation time in 1.5–2 times.
Except for some rare cases, mathematical functions in Metalibm are approx-
imated with piecewise-polynomial functions. Thus, in the produced code there is
branching to determine the right polynomial coeﬃcients for each subdomain. We
propose here a method to avoid this branching. We are trying to ﬁnd a mapping
function (in mathematical sense of word function), that returns corresponding
subdomain indexes for the input values from the implementation domain. This
mapping function may be computed with one of the classical polynomial inter-
polation procedures. The proposed algorithm paves the way for research in gen-
eration of vectorizable functions implementations. We discuss here the weakness
of the proposed method as well as the way to improve it. However, this improve-
ment requires also research in interval arithmetic, basically in solving the linear
systems with bounded variables.
In the next Section we give a short overview on the implementation process
of mathematical functions, in Sect. 3 we explain in details the proposed method,
provide a pseudocode and discuss its future evolution. In Sect. 4 there are the
results of our method and the conclusion.
2
Mathematical Functions Implementation Workﬂow
The usual implementation workﬂow of a mathematical function is divided into
three steps: argument reduction, approximation and reconstruction [3]. In Met-
alibm we use a modiﬁed Remez algorithm for polynomial approximation [4].
The larger is the approximation domain, the larger will be polynomial degree
to approximate a function with the given accuracy. To save computation time
and to avoid accumulation of errors, the degree of the polynomial has to be low.
Therefore, the implementation domain for the function has to be reduced some-
how. It may be done in two ways: property-based argument reduction [5–7] or
domain splitting [8].
In the ﬁrst case mathematical properties of the function allow to establish
connection between the initial large domain [a, b] and a small one [α, β]. So, the
approximation is computed for some g(r), r ∈[α, β] instead of f(x), x ∈[a, b],
where r is reduced argument. Then on reconstruction step the inverse argument
transition has to be applied. As this approach is based only on function proper-
ties, it is limited and works only for several function families (e.g. exponential,
logarithmic, symmetrical). When the function properties do not allow to reduce
the domain or do not reduce it enough, piecewise-polynomial approximations are
used. In this case, the reconstruction step contains several if-else statements
to pick the right approximating polynomial for the function evaluation. To make
the code vectorizable, branching during the computations of the ﬁnite function
values has to be avoided.
Consider here that the domain splitting procedure returns a set of non-
overlapping intervals {Ik}N
k=0, such that Ik = [ak, ak+1], so the adjacent intervals

16
O. Kupriianova and C. Lauter
b
a
0
1
2
3
4
5
6
7
8
9
10
I0 I1
I2
I3
I4
I5
I6
I7
I8 I9
Fig. 1. Piecewise-constant mapping function M(x)
have the only point in the intersection Ik ∩Ik+1 = {ak+1}. The initial imple-
mentation domain is [a, b] = I = N
k=0 Ik. Then, to compute f(x) we execute
if-else statements to determine subdomain’s index k, where x ∈Ik.
The code may be written without branches with the use of a mapping function
that returns subdomain’s index for each input value from I:
M(x) = k, x ∈Ik, k = 0, 1, . . . , N.
The function M(x) is a piecewise-constant function as it is shown on Fig. 1.
For a naive domain splitting, when I is divided into N equal intervals we
may use a linear function m(x) =
N
b−a(x −a) and then the mapping function is
easily computed as M(x) = ⌊m(x)⌋. However, this splitting is not optimal and
a more sophisticated splitting algorithm is used instead [8].
3
Polynomial-Based Reconstruction Technique
3.1
How to Compute Polynomial Mapping
For a non-regular domain splitting as the one that is currently used in Metalibm
the mapping function may be computed with an interpolation polynomial p(x).
This polynomial passes through the points (ak, k), k = 0, . . . , N, where {ak}
are the splitpoints. Once the polynomial coeﬃcients are computed, the mapping
function can be computed as
M(x) = ⌊p(x)⌋, x ∈I.
Thereby, we obtain some conditions for this polynomial. Such a polynomial is
shown on Fig. 2.

Replacing Branches by Polynomials in Vectorizable Elementary Functions
17
b
a
p(x)
⌊p(x)⌋
0
1
2
3
4
5
6
7
8
9
10
I0I1
I2
I3
I4
I5
I6
I7
I8
I9
Fig. 2. Mapping function and a corresponding polynomial p(x)
3.2
Conditions for the Polynomial
As the mapping function stays constant on a subdomain Ik, the admissible range
for the polynomial values on this subdomain is [k, k + 1). Thus, the task is to
compute an interpolation polynomial p on the points (ak, k), k = 0, . . . , N for
which the following holds:
p(x) ∈[k, k + 1), x ∈[ak, ak+1].
(1)
A suitable polynomial p as well as the conditions (1) are shown on Fig. 3.
As the classical interpolation procedures guarantees only that p(ak) = k by
construction of the polynomial, conditions (1) have to be checked a posteriori.
This can be done in Sollya [9] with the evaluation of this polynomial p(x) over
an interval [ak, ak+1].
b
a
p(x)
⌊p(x)⌋
0
1
2
3
4
5
6
7
8
9
10
I0I1
I2
I3
I4 I5
I6
I7
I8 I9
Fig. 3. Admissible ranges for the polynomial values.

18
O. Kupriianova and C. Lauter
There is a certain ambiguity for the values of mapping function in the split-
points {ak}. In splitpoints the two polynomials corresponding to the adjacent
subdomains have the same value pk−1(ak) = pk(ak) = k. To get the index of
the approximating polynomial at the point ak we may admit M(ak) = k −1 or
M(ak) = k. Only in the “corner” splitpoints a0 and aN there is no ambiguity
for the values of mapping function.
As all the computations are performed in ﬂoating-point numbers, the interpo-
lation conditions p(ak) = k are no longer satisﬁed because of roundings. Taking
into account the ambiguity of the mapping function in the splitpoints, conditions
(1) have to be modiﬁed a little. As the set of ﬂoating-point numbers is discrete,
for a given ﬂoating-point number a it is possible to ﬁnd its predecessor pred(a)
and successor succ(a). This means that the admissible ranges for polynomial
values from (1) should be narrowed to the following:
p(x) ∈[k, k + 1), where x ∈[succ(ak), pred(ak+1)] ⊂Ik, 0 ≤k ≤N −1.
(2)
The conditions for the splitpoints should be added then.
p(x) ∈[k −1, k + 1), where x = ak, k = 1, . . . , N −1
(3)
The modiﬁed conditions for the polynomial ranges are shown on Fig. 4 with grey
rectangulars, the range of polynomial values in split points is illustrated with a
red line.
3.3
The Choice of the Interpolation Points
The interpolation points may be chosen in several diﬀerent ways. With the set
of splitpoints {ak}N
k=0 we compute four diﬀerent polynomials. First, we may use
“inner” polynomial with N −1 points {ak}N−1
k=1 . Then we can compute “left”
and “right” polynomial with N points {ak}N−1
k=0 or {ak}N
k=1. And the last variant
a1
a2
a3
succ(a1)
succ(a2)
pred(a2)
Fig. 4. Modiﬁed ﬂoating-point conditions for polynomial (Color ﬁgure online).

Replacing Branches by Polynomials in Vectorizable Elementary Functions
19
here is to compute a polynomial of degree N using all N +1 splitpoints. When a
posteriori conditions are not veriﬁed for all the four polynomials (2–3), we may
add some interpolation points. However, as the addition of new interpolation
points raises the degree of the polynomial according to Runge’s phenomenon
it will oscillate in the ends, which means that the conditions (2–3) are rarely
veriﬁed.
3.4
Towards a Priori Conditions
As the conditions for the polynomial values are checked only a posteriori, there
is no guarantee that the polynomial for mapping function exists for arbitrary
splitting. Contrariwise, our method ﬁnds it only for few splittings. When there
are some points where the polynomial exceeds the admissible range, we can add
them to the interpolation points, recompute the polynomial p(x) and recheck
the conditions (2)–(3). However, due to Runge’s phenomenon the polynomial
begins to oscillate [11] and the conditions are not veriﬁed. The choice of the
interpolation points for this polynomial remains an open problem.
However, the ranges for the polynomial values are still checked a posteriori
and there are some values out of the admissible range. These conditions may be
taken into account if we operate intervals instead of points. The classical inter-
polation problem is a system of linear equations with Vandermonde’s matrix:
⎛
⎜
⎜
⎜
⎝
1 x0 · · · xN
1
1 x1 · · · xN
1
...
...
...
...
1 xN · · · xN
N
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
c0
c1
...
cN
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
y0
y1
...
yN
⎞
⎟
⎟
⎟
⎠,
(4)
where (xi, yi), i = 0, . . . , N are the interpolation points and c0, . . . , cN are the
unknown polynomial coeﬃcients. Computing the unknown coeﬃcients means
solving the system (4). When we use intervals instead of points to compute
the interpolation polynomial, we take subdomains on abscissas and intervals
[k, pred(k +1)] on ordinates. Then, the task is almost the same: system of linear
equations with unknown coeﬃcients c0, . . . , cN. Except of the numbers xi, yi we
operate intervals in system (5).
⎛
⎜
⎜
⎜
⎝
1 x0 · · · xN
1
1 x1 · · · xN
1
...
...
...
...
1 xN · · · xN
N
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
c0
c1
...
cN
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
y0
y1
...
yN
⎞
⎟
⎟
⎟
⎠
(5)
The diﬀerence from the classical solution of the interval system is that we
need only one vector (c0, c1, . . . , cN), but not the set of all suitable coeﬃcients.
We may ﬁnd the tolerance solution set of the system (5) in polynomial time,
but it can be empty. In this case the united solution set may be found, but
this problem is NP-hard [12]. Anyway, we have connected coeﬃcients in the
system matrix, and the existing methods do not take into account this type of
connection. We leave this transition to a priori conditions for the future work.

20
O. Kupriianova and C. Lauter
3.5
Algorithm
To summarize the upper-mentioned, here is the pseudocode to obtain a recon-
struction polynomial. Procedure buildInterpolationPoly takes subdomains
from splitting and computes the interpolation with divided diﬀerences. We bound
the degree of the reconstruction polynomial with the parameter maxdegree.
Input: domain, maxdegree
N = length(domains);
if (N - 3 > maxdegree) {
print("Even the poly built by inner points has too large degree");
poly = -1;
} else {
innerpoints = extractOnlyInnerPoints(domains);
ypoints = [| 1, ..., N - 1 |];
poly = buildInterpolationPoly(innerpoints, ypoints);
if (!checkConditions(poly, domains)) {
xpoints = merge(inf(domains[0]), innerpoints);
ypoints = [| 0, ..., N - 1 |];
poly = buildInterpolationPoly(xpoints, ypoints);
if (!checkConditions(poly, domains)) {
xpoints = merge(innerpoints, sup(domains[N - 1]));
ypoints = [| 1, ..., N
|];
poly = buildInterpolationPoly(xpoints, ypoints);
if (!checkConditions(poly, domains)) {
xpoints = merge(inf(domains[0]), xpoints);
ypoints = [| 0, ..., N |];
poly = buildInterpolationPoly(xpoints, ypoints);
if (!checkConditions(poly, domains)) {
print("It was not possible to build such a polynomial.
Perhaps we need to add an interpolation point");
poly = -1;
}
}
}
}
}
return poly;
4
Conclusion
For the variation of the arcsin function on the domain [−0.6; 0.6] with the tar-
get accuracy ¯ε = 2−54 and maximum degree of the approximation Metalibm
splits the domain into six smaller subdomains. In this case Metalibm achieves
to compute the coeﬃcients for the mapping polynomial. It uses “left” inter-
polation polynomial computed on all the split points except the last one. The
proposed method allows to obtain performance gain in 1.5–3 times. Thus, this
algorithm paves a way for generation of vectorizable function implementations.

Replacing Branches by Polynomials in Vectorizable Elementary Functions
21
The main disadvantage of the proposed method is that it worked in ∼30 % of
tested cases. As it was mentioned, the main reason of failures of the algorithm
is the a posteriori condition checking.
We invented a general method that is not based on any speciﬁc instructions,
so can be used on a wide range of the machines. The use of the interval arithmetic
may allow to generate vectorizable code for larger range of function variations. As
we mentioned, there are two solutions for the interval system of linear equations
suitable for our task. When the tolerance solution set is empty, we may try
to ﬁnd the united solution. The second problem is NP-hard but does not have
to be avoided: we are interested not in the set of all the possible polynomial
coeﬃcients, but only in one combination of all the coeﬃcients that gives the
mapping function. Thus, this combination should be easier to ﬁnd than the whole
set. Furthermore, as this mapping function is computed during the function
generation, there are no strict requirements on complexity and performance of
this algorithm.
The proposed reconstruction technique with polynomial-based mapping func-
tion depends a lot on domain splitting. We tried to build an optimal domain split
in terms of the quantity of subdomains and the polynomial degrees of the approx-
imation on each of the subdomains. Mapping function for the regular splitting is
easy to ﬁnd but our splitting produces non-regular subdomains and computing
the suitable polynomial for mapping gets impossible in some cases. Besides that,
our splitting algorithm does not control the size (and the corresponding polyno-
mial degree) of the last subdomain. Though practically we have not noticed such
phenomenon, theoretically nothing prevents our splitting algorithm to get small
last subdomain with low corresponding polynomial degree (one or two). This
may be avoided with improving the splitting algorithm: instead of splitpoints
we should get intervals for the possible splitpoint. Then, on the reconstruction
step we ﬁx splitpoints from these “tolerance” intervals so, that our polynomial
for mapping function exists. Thus, the future research on this problem includes
also establishing the compromise between splitting optimality and existence of
polynomial for mapping.
References
1. Loosemore, R.M.S.e.a.S.: The GNU C Library Reference Manual. Free Software
Foundation Inc
2. Kupriianova, O., Lauter, C.: Metalibm: a mathematical functions codegenerator.
In: Proceedings of the 2014 Mathematical Software - ICMS 2014 - 4th International
Congress, Seoul, South Korea, 5–9 August 2014, pp.713–717. http://dx.doi.org/10.
1007/978-3-662-44199-2 106
3. Muller, J.-M.: Elementary Functions: Algorithms and Implementation. Birkhauser
Boston Inc., Secaucus (1997)
4. Brisebarre, N., Chevillard, S.: Eﬃcient polynomial l-approximations. In: 18th IEEE
Symposium on Computer Arithmetic (ARITH-18 2007), 25–27 June 2007, Mont-
pellier, France, pp. 169–176 (2007). http://doi.ieeecomputersociety.org/10.1109/
ARITH.2007.17

22
O. Kupriianova and C. Lauter
5. Tang, P.T.P.: Table-driven implementation of the exponential function in IEEE
ﬂoating-point arithmetic. ACM Trans. Math. Softw. 15(2), 144–157 (1989)
6. Tang, P.T.P.: Table-driven implementation of the logarithm function in IEEE
ﬂoating-point arithmetic. ACM Trans. Math. Softw. 16(4), 378–400 (1990).
http://acm.org/10.1145/98267.98294
7. Tang, P.T.P.: Table-driven implementation of the Expm1 function in IEEE
ﬂoating-point arithmetic. ACM Trans. Math. Softw. 18(2), 211–222 (1992).
http://doi.acm.org/10.1145/146847.146928
8. Kupriianova, O., Lauter, C.: A domain splitting algorithm for the mathematical
functions code generator. In: Asilomar Conference on Signals, Systems and Com-
puters, Paciﬁc Grove, CA, USA, 2–5 November 2014 (2014, to appear)
9. Chevillard, S., Jolde¸s, M., Lauter, C.: Sollya: an environment for the development
of numerical codes. In: Fukuda, K., Hoeven, J., Joswig, M., Takayama, N. (eds.)
ICMS 2010. LNCS, vol. 6327, pp. 28–31. Springer, Heidelberg (2010)
10. Brunie, N., de Dinechin, F., Kupriianova, O., Lauter, C.: Code generators for
mathematical functions (2014). <hal-01084726v2>
11. Cheney, E.W.: Introduction to Approximation Theory. AMS Chelsea Publishing,
New York (1982)
12. Shary, S.P.: Solving the linear interval tolerance problem. Math. Comput. Simul.
39, 53–85 (1995)

The Forthcoming IEEE Standard 1788
for Interval Arithmetic
John Pryce(B)
School of Mathematics, CardiﬀUniversity, Cardiﬀ, UK
smajdp1@cardiff.ac.uk
Abstract. This is a slightly expanded form of the author’s talk of the
same title at SCAN 2014, W¨urzburg. Angled towards people who use
interval numerical methods little or not at all, it brieﬂy describes how
interval arithmetic works, the mindset required to use it eﬀectively, why
an interval arithmetic standard was needed, the setting up of IEEE Work-
ing Group P1788 for the purpose, the structure of the standard it has
produced, some diﬃculties we encountered, and the current state of the
P1788 project. During production of these Proceedings the 1788 stan-
dard has been published, but the talk’s original title has been kept.
This article, slightly expanded from the author’s talk of the same title at
SCAN 2014, brieﬂy describes how interval arithmetic works, the mindset required
to use it eﬀectively, why an interval arithmetic standard was needed, the setting
up of IEEE Working Group P1788 for the purpose, the structure of the standard
it has produced, some diﬃculties we encountered, and the current state of the
P1788 project. During production of these Proceedings the 1788 standard has
been published, but the talk’s original title has been kept.
The references include a recent survey [14], a recent textbook [17], and a
current web site [8], that testify to the liveliness of this area.
1
What Intervals Are and Do
1.1
Basic Ideas
Interval Arithmetic (IA) implements “validated”, also called “veriﬁed”, numer-
ical calculation. That is, it can enclose solution components x of a problem in
an interval, i.e. between lower and upper bounds
x ∈x = [x, x] = { t ∈R | x ≤t ≤x }.
It does this even in ﬁnite-precision arithmetic, with roundoﬀerrors present.
E.g. it makes Brouwer’s ﬁxed point theorem:
If K ⊂Rn is compact convex, and function f is everywhere deﬁned
and continuous on K, and f(K) ⊆K, then f has a ﬁxpoint in K
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 23–39, 2016.
DOI: 10.1007/978-3-319-31769-4 3

24
J. Pryce
veriﬁable when K is a box (product of intervals) in the sense that during the
evaluation of f, suﬃcient conditions for “everywhere deﬁned and continuous”
can be found—with ease in favourable cases, but maybe requiring both brute
force and ﬁnesse in trickier cases..
The history of interval arithmetic might be traced back to Archimedes, in
the sense that he rigorously proved the bounds
3 10
71 < π < 3 1
7,
see Thomas Heath [3]. As a systematic discipline it seems to have begun in
the 20th century: Teruo Sunaga (Japan, 1958) [16]; Leonid Kantorovich (USSR,
1962) [4]. The most inﬂuential work of that time was the book by Ramon Moore
(USA, 1966) [11], describing for instance the ﬁrst implementation of a validated
ODE solver.
Currently signiﬁcant validated software exists for global optimisation, large
sparse linear systems, particle beam design for the Large Hadron Collider, and
many other applications. Rather than list extensive references we refer to those
in Siegfried Rump’s survey article [14] and in Vladik Kreinovich’s web site [8],
and also to the recent introductory book by Warwick Tucker [17].
1.2
Deﬁnition of Interval Operations
Interval operations take all combinations of points in the inputs, i.e.
x • y = { x • y | x ∈x and y ∈y },
where • is one of {+ −× ÷}
For ÷, disallow 0 ∈y for now. In ﬁnite precision round outward. With these
deﬁnitions one has the following fact, probably ﬁrst stated by Moore:
Theorem 1 (Fundamental Theorem of Interval Arithmetic). If a func-
tion f(x1, . . . , xn), deﬁned by an expression, is evaluated with interval operations
on interval inputs to get y = f(x1, . . . , xn) then
y contains the range of f over box x1 × · · · × xn in Rn.
Example 1. Let f(x1, x2) = x1+x2/x1, suppose 2-digit decimal arithmetic is
used, and let the input intervals be x 1 = [3, 4], x 2 = [3, 5]. We compute
x 1 + x 2
x 1
= [3, 4] + [3, 5]
[3, 4] = [3, 4] +
3
4, 5
3

round
−→[3, 4] + [.75, 1.7]
= [3.75, 5.7]
round
−→[3.7, 5.7] = f (x 1, x 2) = y.
y does contain the range of f over x 1 × x 2 = [3, 4] × [3, 5], which with a bit of
calculus is found to be [4, 5.25].
⊓⊔
2
Why Do Intervals Need New Algorithms?
2.1
Example: Interval Version of Newton’s Iteration
Consider Newton’s method for solving a 1-D nonlinear equation f(x) = 0.

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
25
A Wrong Approach. The usual formula is:
xk+1 = xk −f(xk)
f ′(xk)
A direct interval transcription of this would be
x k+1 = x k −f (x k)
f ′(x k)
where f and f ′ are interval versions of (the computer code for) f and f ′.
Unfortunately addition and subtraction of intervals—in inﬁnite precision—
just adds their widths. In symbols, w(a ± b) = w(a) + w(b). In ﬁnite precision
the result is even a little wider owing to roundoﬀ.
So the width of x k+1 equals the width of x k plus that of f (x k)/f ′(x k). The
latter width is usually strictly > 0, so each interval cannot be narrower than the
last, and usually is wider. Convergence of an interval algorithm to a root must
involve the interval becoming smaller. The above simple transcription of Newton
to intervals cannot possibly do that, and is bound to diverge!
A Right Approach. For a sensible solution to this problem, go back to basic
theory. Let f be a C1 function on a real interval I. Then by the Mean Value
Theorem MVT, for any root z and any x, both in the interval, there is some ξ
in the interval such that
f(x) = f(x) −f(z) = (x −z)f ′(ξ)
(1)
so provided f ′(ξ) ̸= 0, see later,
z = x −f(x)
f ′(ξ).
(2)
A pointer to the right algorithm is in the quantiﬁers: ∀root z, ∀x, ∃ξ. These
give a geometric interpretation to Eq. (2), shown in Fig. 1:
For any x ∈I, a searchlight shone from the point (x, f(x)) on the curve,
its rays bounded by the lowest and highest slopes of f on I, is certain to
illuminate any root z (identiﬁed with (z, 0) in the plane) in I.
To convert this to something computable note that in (2):
– f(x) must be computed as an interval, since f is program code, hence liable
to roundoﬀ.
– f ′(ξ) must also be an interval, for two reasons: (a) f ′ is program code; (b) ξ
is only known to exist—its exact position is unknown ( ∃).
– However x can be a point. It is an arbitrary programmer-selected point in I
( ∀)—typically the midpoint is used in practice.

26
J. Pryce
Fig. 1. Geometric view of Interval Newton: one-sided searchlight.
Hence, ( ∀) for any root z ∈I we also have
z ∈

x −[f(x)]
[f ′(ξ)]

using classical interval notation that [. . .] means “some interval containing”. In
more current notation, renaming the interval as x
z ∈

x −f([x])
f ′(x)

=

point −
interval function of point
interval function of interval

(3)
where [x] is 1-point interval {x} and f , f ′ are interval versions of f, f ′. This is
the start of a satisfactory algorithm.
More General Picture. Actually the searchlight shines in both directions, crucial
when the range of slopes includes both positive and negative slopes, see Fig. 2:
 
Fig. 2. Geometric view of Interval Newton: two-sided searchlight.

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
27
The same argument works as before, provided one interprets division as
reverse multiplication, write this as //:
c // b = { all solutions of bx = c}
1788’s mulRev(b,c).
So 0/0 is “whole real line” instead of “undeﬁned”. That is, (3) is replaced by
z ∈

x −

f ([x] // f ′(x)
	
.
(4)
This is just a restatement of formula (1), and its controlling quantiﬁers, two ∀
and one ∃. That is always valid even when (2) might divide by zero.
Now we enclose all roots even when many exist! However, the searchlight can
“split” I into two pieces, as Fig. 2 shows. 1788 provides a “two-output reverse
multiplication” operation mulRevToPair, adapted to this situation.
Interval Newton Iteration. We assumed the root z is in the interval x so it is
safe to intersect the interval given by (4) with x. This gives the 1983 method
of Hansen and Greenberg [2], later reﬁned by R.B. Kearfott [6], G. Mayer [10],
P. van Hentenryck et al. [18], and others.
We assume the function f is C1 on the initial interval (Fig. 3).
set x0 = initial interval I
for k = 0, 1, 2, . . .
xk = some chosen point in xk
Y k+1 = xk −f([xk]) // f ′(xk)
reverse multiplication
xk+1 = Y k+1 ∩xk
can split in two, see below
Fig. 3. One step of Interval Newton method (one-sided searchlight case).

28
J. Pryce
2.2
Lessons from the Example
Comments on the Algorithm. Here Y k+1, hence x k+1, is potentially a union of
two intervals that can be handled independently.
This structures the computation as a binary tree that progressively divides
root-clusters into smaller sets, trying (but not always succeeding) to isolate each
individual root. This can be done by various tree-traversal methods, exploiting
parallelism if available.
By construction it is clear that all roots in x k must be in Y k+1, hence in
(the possibly split in two) x k+1. However the algorithm has other remarkable
and less obvious properties:
– if x k+1 is empty, then no root exists in x k.
– If 0 /∈f ′(x k), then at most one root exists in x k (which must be in x k+1).
– If Y k+1 is nonempty, bounded, and contained in x k, then exactly one root
exists in x k.
Comments on the “Interval Mindset”. The analysis leading to the algorithm
wasn’t rocket science—just a careful look at how the quantiﬁers ∀, ∃appeared
in a use of the Mean Value Theorem.
In general however, to learn how to convert mathematics to eﬀective interval
algorithms takes time and practice.
3
Genesis of the Interval Standard Project
3.1
The Need
Over the years, dozens of interval software packages have been written, and
several, for instance PROFIL/BIAS, Filib++ and INTLAB, are widely used at
present [7,9,13]. However, they have not quite compatible mathematical foun-
dations, for instance diﬀerent answers to these questions:
– Should theory, and software, support unbounded intervals and the empty set?
Moore’s interval arithmetic did not.
– Is an interval x a set of numbers? In Kaucher interval arithmetic, an interval
is a set with a two-valued “mode”. [3, 4] is a proper interval, essentially the
normal set. [4, 3] is an improper interval; as a set it has the same value as
[3, 4] but its arithmetic rules are diﬀerent.
– If x is a set of numbers, are ±∞allowed to be members of x?
– How to handle operations that are not everywhere deﬁned on their input
intervals, such as the square root of [−2, 2], or division by an interval
containing 0?
In addition, they have diﬀerent software interfaces. Thus, currently one can-
not write algorithms that are portable at a mathematical level, let alone write
portable software.

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
29
3.2
Setting up a Working Group
In January 2008 at a conference in Dagstuhl, Germany, a project was started
with the aim of producing an IA standard. In July that year it was approved
by the IEEE as Working Group P1788 “A standard for interval arithmetic”. In
September, a conference in El Paso, USA, hosted its ﬁrst face to face meeting
at which the following oﬃcers were appointed.
Nathalie Revol, Chair
R. Baker Kearfott, Vice Chair and Acting Chair
William Edmonson, Secretary
Guillaume Melquiond, Archivist
J. Wolﬀvon Gudenberg, Web Master
George Corliss, Voting Tabulator
John Pryce, Senior Technical Editor
Also during the project Christian Keil acted as Deputy Technical Editor, and
Michel Hack, Vincent Lef`evre, Ian McIntosh, Dmitry Nadezhin, Ned Nedialkov
and J. Wolﬀvon Gudenberg were Assistant Technical Editors. About 140 people
registered on the mailing list, of whom around 45 were regular voting members.
The group approved a ﬁnal text in May 2014. Further revision, up to publica-
tion, then became the responsibility of the project’s sponsor ballot group (whose
membership overlaps with P1788’s) and of IEEE editorial staﬀ.
4
1788 Interval Principles
4.1
Deﬁnition of an Interval
There is a framework—so called ﬂavors—to support alternative mathematical
foundations. The standard currently has just one ﬂavor called Set-Based, in
which
– An interval x is a plain set, whose members are real numbers. This excludes
±∞as members, so intervals are subsets of the real line R.
– Open or half-open intervals are not allowed, but unbounded intervals are.
– The empty set is an interval.
This amounts to the mathematically simple deﬁnition:
Interval means topologically closed and connected subset of R.
A potential alternative ﬂavor is Kaucher (or very similar modal) interval
arithmetic [5]: an interval is not a plain set, but an ordered pair (x, x) of reals:
(x, x) “means”

set [x, x] ⊂R
if x ≤x (“proper” interval)
something other if x > x (“improper” interval).
Another potentially important ﬂavor is Siegfried Rump’s interval arithmetic
[15], which can support open or half-open intervals, and can handle ﬁnite
precision under- and over-ﬂow in a consistent and elegant way.

30
J. Pryce
4.2
The Levels Structure
As in the IEEE ﬂoating-point standard 754, the 1788 standard manages com-
plexity by distinguishing four speciﬁcation levels:
Level 1. Mathematical theory of the set IR of intervals and their operations.
Level 2. Finite precision intervals—datums—and operations, independently of
their representation.
Level 3. Representation of datums by objects, e.g. by a data structure compris-
ing two ﬂoating-point numbers.
Level 4. Encoding of Level 3 objects as bit strings.
Inter-level Maps. Maps between levels are crucial—especially those between
Level 1 and Level 2, or L1 ←→L2 for short. The P1788 group made the following
design decisions, which apply to all ﬂavors:
– Each datum is a mathematical interval, so the map from L2 to L1 is just
inclusion:
L2 datums
identity map
−→
L1 intervals
(∗)
(Not quite true: the datum also “knows what type it belongs to”, i.e. is
tagged with a unique name of its type. A programming language needs this
information, since diﬀerent types will be represented diﬀerently at L3.)
– Datums are organised into ﬁnite sets T called interval types. Thus each T
may be regarded as a ﬁnite subset of IR. The implementation has discretion
on what types to provide.
– A L1 interval x maps to an interval of type T—a T-interval or T-datum—by
the T-hull operation
hullT (x) = smallest T-interval that contains x,
where “contain” has a ﬂavor-deﬁned meaning (which for Set-Based intervals
is the usual one). This deﬁnes the map back from L1 to L2:
L1 intervals
T-hull
−→L2 datums of typeT
(∗∗)
– To do an operation x • y at L2 on T-datums, in any ﬂavor:
map x, y to L1 by (*);
do the operation at L1;
map back to L2 by (**).
This speciﬁcation of the relation between mathematics and ﬁnite precision looks
trivial but is not: it deﬁnes the whole character of the standard. Not all IA theories
are clear on this issue. Time will tell whether our choice was a wise one.
This choice aﬀects implementations. For instance an arbitrary-precision inter-
val package must be structured as a potentially inﬁnite set of types, each containing
ﬁnitely many intervals. It cannot comprise a single type containing potentially inﬁ-
nitely many intervals.

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
31
Maps for Levels 3 and 4. There are two fairly obvious rules:
– L2 ←→L3: Each L2 datum is represented by at least one L3 object; each L3
object represents at most one L2 datum.
– L3 ←→L4: Each L3 object is encoded by at least one L4 bitstring; each L4
bitstring encodes at most one L3 object.
5
Exception Handling
5.1
A Hypothetical Scenario
Less than 10 years hence in the Old Bailey, London, . . .
The case Crown versus Google concerns the Google Driverless Car, GDC.
One of them badly injured a pedestrian who stepped into the road in front of it.
The GDC’s emergency stop system is designed to act faster than a good
human driver (undisputed) but is it badly implemented (disputed)?
The software uses an interval algorithm, built on a 1788-conforming library,
which applies Brouwer’s ﬁxed point theorem. Could this have an error? E.g., it
may have thought it had enclosed a root of an equation when it had not.
Depending on what software bugs are found (if any), liability might lie with
the pedestrian’s negligence. Or with GDC’s software implementers. Or with the
1788 library implementers. Maybe even with the mathematicians who claimed
to have proved the design of 1788 is correct?
A lot of money rides on whether 1788-based code might be wrong, when
deciding that a function is deﬁned and continuous on a box.
5.2
Theoretical Context
Basic Problem. How (at Level 1) to treat operations that are not everywhere
deﬁned and/or continuous on their input box? For example
(real) square root √x
x
y
floor(x)

[−2, 2]
[2, 3]
[−1, 1]
floor([2.5, 4.5])
undeﬁned on −2 ≤x < 0
undeﬁned if y = 0
discontinuous at x = 3, 4
We decided the default is “evaluate where deﬁned, ignore where undeﬁned”,
called non-stop or loose evaluation. For instance

[−2, 2] = { √x | x ∈[−2, 2] and x ≥0 } = [0,
√
2]
with no error reported. This is similar to IEEE 754 ﬂoating-point, which responds
to an invalid operation such as 0/0 or ∞−∞by returning the result NaN and
continuing to compute.
This is a valid approach for, e.g., many global optimisation methods. It is not
valid when applying Brouwer’s theorem, which needs a guarantee that a function
is everywhere deﬁned and continuous on a box.
It also will not do for some graphics rendering algorithms, which need to
know a function is everywhere deﬁned on a box, but are not bothered about
continuity.

32
J. Pryce
Tracking Function Properties. One needs a mechanism to track whether a func-
tion has these desirable properties of deﬁnedness and/or continuity. This leads to
a powerful extension of the Fundamental Theorem of interval arithmetic based
on well-known theorems of set theory and analysis, which can be summarised:
If for function f given by an expression, each individual library operation
in f is everywhere deﬁned on its input set, then the same holds for f.
The same is true when deﬁned is replaced by deﬁned and continuous.
Example 2. Let f(x) = 1/√x, composed of library operations sqrt(t) =
√
t
followed by recip(t) = 1/t. Evaluate z = f (x) in exact (Level 1) interval
arithmetic. Abbreviate deﬁned and continuous to DAC.
Let the input to f be x = [1, 4]. We do y = sqrt(x) = [1, 2], followed by
z = recip([1, 2]) = [ 1
2, 1]. Each operation is (everywhere) DAC on its input:
sqrt on [1, 4] and recip on [1, 2]. We conclude f is DAC on this x.
If x is [0, 4] then sqrt is DAC on this x but recip is not DAC on the
resulting y = [0, 2], so we cannot say f is DAC on x.
Similarly, if x is [−2, 4] then sqrt fails to be DAC on this x and again we
cannot say f is DAC on x.
⊓⊔
In the two last cases of this example it is easy to prove f is deﬁnitely not DAC
on x, but for complicated functions in the presence of roundoﬀ, to prove such
a negative—deﬁnitely not everywhere DAC—is nearly impossible. Therefore the
1788 system only provides for a deﬁnite positive, which is cheap to compute. It
can also say deﬁnitely nowhere deﬁned on the input, which is cheap too.
5.3
Decorations
To provide a mechanism to track such properties of functions, we rejected the
IEEE 754 standard’s method of global ﬂags, as being obsolete for today’s mas-
sively parallel platforms. Instead, 1788 provides for decorated intervals. Such an
interval is a pair (y, dy), also written ydy when convenient:
– an ordinary interval y,
– a tag dy called a decoration, giving data about deﬁnedness, continuity, etc.1.
Formally, a decoration d is a label for an assertion (boolean-valued function)
pd(f, x) about a function f : Rn →R and a box x ⊆Rn, for arbitrary n. Five
decorations are deﬁned in order of “goodness”, ill < trv < def < dac < com:
ill Label for ill-formed intervals, formally “f is nowhere deﬁned”.
trv (trivial) Always true = “no information”.
def f is everywhere deﬁned on x.
dac As def, plus f is everywhere continuous on x.
com As dac, plus f is bounded on x at Level 2, meaning that no overﬂow occurred
while computing it2.
1 dy is just a mnemonic, “decoration for y”. It has nothing to do with diﬀerentials.
2 com means “common”, see Sect. 6, but also that code can verify it is common.

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
33
Let (y, dy) result from evaluating an arithmetic expression f(x1, . . . , xn)
– on correctly initialised decorated interval inputs (x 1, dx1), . . . , (x n, dxn) (the
programmer’s responsibility),
– using correctly written decorated interval library operations (the implemen-
tation’s responsibility).
Then Moore’s Fundamental Theorem says
y contains the range of f over x 1 × · · · × x n,
and in addition,
the decoration dy makes a true assertion about f over x.
For instance if dy is computed to be def then f has been proved to be
everywhere deﬁned on x.
As with a computed range enclosure, a computed decoration is often not
sharp. E.g. it may be trv (no information) or def (deﬁned) when actually dac
(deﬁned and continuous) is true. Much of the craft of IA is knowing how to
“sharpen” such information, e.g. by cutting an input box into smaller boxes
handled separately.
Example 3. Consider the ﬁxpoint problem, to solve g(x) = x where
g(x) = 2√x −1
2.
Roots are x = 3
2 ±
√
2 = 0.0858 . . . or 2.9142 . . .
We aim to use interval ﬁxpoint iteration
x 0 = initial guess;
x n+1 = g(x n) for n = 0, 1, . . .
First, use ordinary undecorated interval arithmetic.
Case A: x 0 = [2, 3]. Then
x 1 =

2
√
2 −1
2, 2
√
3 −1
2

= [2.3 . . . , 2.9 . . .] ⊂x 0.
This is genuine and (by Brouwer’s Theorem) it proves a ﬁxpoint exists in x 1.
Case B: x 0 = [−1, 1
16]. Then
x 1 = 2

[−1, 1
16] −1
2 = 2 [0, 1
4] −1
2 = [0, 1
2] −1
2 = [−1
2, 0], again ⊂x 0 !
But there is no root in x 0, let alone x 1! This is spurious, due to 1788’s (undeco-
rated) square root function discarding the negative part of x 0 without comment.
Now use decorated interval arithmetic. The rule for propagating decorations
is, roughly, that an operation outputs the worst decoration, in the “goodness”
order deﬁned on p. 10, out of the decorations on its operands and the decoration

34
J. Pryce
generated while performing the operation. Showing decorations as subscripts,
Case B gives
x 1 = [2]dac ×

[−1, 1
16]dac −[ 1
2]dac
= [2]dac × [0, 1
4]trv −[ 1
2]dac,
= [0, 1
2]trv −[ 1
2]dac
= [−1
2, 0]trv.
Recall trv = “no information”, so the calculation is, correctly, unable to verify
the conditions of Brouwer’s Theorem. But Case A produces
x 1 = [2.3 . . . , 2.9 . . .]dac,
proving g is DAC on x 0, as well as mapping x 0 into itself—the conditions for
applying Brouwer’s Theorem have been veriﬁed.
⊓⊔
The decoration system is the feature that most distinguishes 1788 from earlier
IA systems. An annex in the Standard contains a rigorous proof of correctness:
a Fundamental Theorem of Decorated Interval Arithmetic.
6
Diﬃculties the Group Encountered
Certain issues caused long and heated debate. We are grateful for the diplomatic
skills the Chair and Vice-chair sometimes needed to deploy, and the good sense
of IEEE procedural guidelines for “online democracy”. Here are a few examples.
The Choice of Foundational Mathematical Model. Most users of IA are in the
academic community, and most of these use some form of “interval is just a set of
numbers” theory and software. But Kaucher/modal theory—with intervals like
[4, 3]—has its proponents. One of them is Nate Hayes, whose company does high-
quality graphics rendering for the movie industry. For its specialised interpolation
algorithms, Kaucher methods are reported to give tighter enclosures and greater
speed.
The resulting tension between “intervals for knowledge” and “intervals for
proﬁt” was fruitful. Faced with two related kinds of object, it is natural to look
for a theory that supports both, in a tightly coupled sense that lets both exist
in a computer program and inter-operate.
We tried this over a period of many months with set-based and Kaucher
intervals, but failed. For instance, unbounded intervals within Kaucher theory
needed arbitrary restrictions or led to logical contradictions—brieﬂy, Kaucher
cannot handle [3, ∞) consistently, and set-based cannot handle [4, 3].
This was the main motivation for the ﬂavor concept. It allows diﬀerent
theories that are “recognisably 1788” in a loosely coupled sense. The main
requirement is that each ﬂavor’s intervals must include Moore’s original (closed,
bounded, nonempty) real intervals, called common intervals, and a library of

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
35
operations that at Level 1, when acting on common intervals, produce the same
results in all ﬂavors.
A Kaucher standard document was promised but has not materialised yet.
However at least one other theory that holds promise for eﬀective interval com-
putation ﬁts into the ﬂavor mould—that of Rump [15], especially if coupled with
the Gustafson universal numbers system [1] (and see elsewhere in this volume).
So I feel the eﬀort put into this part of the standard has not been wasted.
The Decoration Scheme. The group saw from the start that checking deﬁnedness
and continuity of a function can in principle be automated, and early on rejected
global ﬂags in favour of decorating individual intervals. The chosen scheme took
nearly two years, oﬀand on, to decide. Initially we used separate boolean ﬂags
for “deﬁned”, “continuous”, etc. Arnold Neumaier of Vienna ﬁrst proposed that
decorations should be a linear sequence. Of several such schemes, we nearly
adopted one with 6 decorations, till Guillaume Melquiond pointed out that one of
them gave no information not already available to a programmer, so we removed
it to give the current 5-decoration system.
(We are also indebted to Arnold for his earlier document, the Vienna Proposal
for Interval Standardization [12], from which many ideas in 1788 are drawn.)
Input/Output. I/O is important. A key reason why the Algol 60 language died
and Fortran, its arguably inferior contemporary, thrived is that the latter had a
language-deﬁned I/O scheme and the former did not.
The working group debated at length on what I/O should be required and
how prescriptive the standard should be. Eventually it agreed to specify an
external text representation of intervals, so called interval literals. Examples are
[empty], [1.23,4.56] and the uncertainty form 1.23?4 which means 1.23 ±
(4 units in the last place), i.e. [1.19, 1.27].
An implementation shall provide functions to read such literals in free for-
mat, and write them in either free (e.g., for interactive work) or ﬁxed (for tab-
ulation) format. However we did not standardise the conversion speciﬁers (such
as C’s %8.3f or Fortran’s F8.3 for output of ﬂoating-point numbers), leaving
this implementation-deﬁned, to be standardised at a future revision.
In addition to the above transformations, which generally incur roundoﬀ,
each ﬁnite-precision type T shall deﬁne an exact text representation, giving loss-
free conversion between T-intervals and text strings. For types based on IEEE
754 numbers, 1788 speciﬁes an interchange encoding, giving loss-free conversion
between T-intervals and bit strings. This last is 1788’s only Level 4 requirement.
What to say About Accuracy? The accuracy issue for intervals is diﬀerent from
that for ﬂoating-point. The result y of an interval library operation must enclose3
the mathematical result y. If not, it is wrong, period. If it does—even if it is the
useless result [−∞, +∞]—it is valid.
3 This has been called the “Thou Shalt Not Lie” principle.

36
J. Pryce
After much discussion we agreed that tightness—how close y is to the
enclosed y—is a quality-of-implementation issue, barring a few cases where
requiring optimal tightness is reasonable.
So the standard acts as a “regulatory authority” here—it does not specify
an accuracy, but it requires a conforming implementation to state the accuracy
of each of its library operations, in a veriﬁable way, using a format speciﬁed by
the standard. There is typically a trade-oﬀbetween accuracy and speed, and the
aim is to make it possible for users to make a fair comparison of the merits of
diﬀerent implementations.
7
Current State
The main text has around 70 pages, of which roughly 60 % are Level 1, 35 %
Level 2, 5 % Level 3, with a half-page of Level 4.
Following approval by a vote of the group in May 2014, the text was exten-
sively reworked with help from IEEE editorial staﬀto ﬁt their style guidelines.
It was signed oﬀin November 2014 to enter the Sponsor Ballot phase, and
examined by a selected group representative of academia, software develop-
ers, industry, etc., and of geographical regions. They approved it after vari-
ous changes, both editorial and technical. Finally, IEEE Std 1788TM-2015 was
approved by the IEEE Standards Board in June 2015, and published at the end
of that month.
In addition, a Basic Standard for Interval Arithmetic (BSIA) has been writ-
ten by Ned Nedialkov. At around 20 pages it is a cut down version, simpler to
implement and suitable for undergraduate teaching. A program that runs under
an implementation of the BSIA should run and give identical results up to round-
oﬀunder an implementation of the full standard. The IEEE have approved a
project, P1788.1, for the BSIA to become a separate but related standard.
A
Proof of Interval Newton Properties
This appendix proves the properties stated in Sect. 2.2. It may be of interest
because item (iv) of the Theorem does not seem to have appeared in the literature
before. An interval extension of a real function f of real variables means an inter-
val function f of corresponding interval variables such that y = f(x1, . . . , xn) is
in y = f (x 1, . . . , x n) whenever xi is in x i for each i = 1, . . . , n.
Theorem 2. Let f : R →R be C1 on an interval x, which may be unbounded.
Let f and f ′ be interval extensions of f and its derivative f ′, and let x be any
point of x. Deﬁne the set
Y = x −f([x]) // f ′(x)
where // denotes division in the sense of reverse multiplication. (Thus Y may be
empty, an interval, or the union of two disjoint unbounded intervals.)

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
37
Then
(i) Y contains all zeros of f in x.
(ii) If Y ∩x = ∅, there are no zeros of f in x.
(iii) If 0 /∈f ′(x), there is at most one zero of f in x.
(iv) If Y is nonempty, bounded and ⊆x, there is exactly one zero of f in x.
Proof. (i) Let z ∈x with f(z) = 0. By the Mean Value Theorem
f(x) = f(x) −f(z) = (x −z)f ′(ξ).
(5)
for some ξ ∈x. By deﬁnition of interval extension, f ′(ξ) ∈f ′(x) and f(x) ∈
f ([x]). Hence by the deﬁnition of reverse multiplication
x −z ∈f ([x]) // f ′(x),
that is
z ∈x −f ([x]) // f ′(x)
as required.
(ii) This is immediate from (i).
(iii) In (5) let both z and x be roots in x. Then we have 0 = f(x) −f(z) =
(x −z)f ′(ξ). By hypothesis 0 /∈f ′(x) which implies f ′(ξ) ̸= 0. Hence x −z = 0,
x = z, so there is at most one root.
(iv) Write b = f ′(x), c = f ([x]), both being nonempty by the deﬁnition of
interval extension. By hypothesis Y = x −c // b is nonempty and bounded, so
Z = c // b is nonempty and bounded.
I claim 0 /∈b. For suppose 0 ∈b. Then 0 /∈c, for if 0 ∈c then Z is the
unbounded set R, contrary to hypothesis. Now two subcases arise.
– Either b is singleton [0], making Z empty, contrary to hypothesis.
– Or, b contains 0 and another point, in which case it contains points arbitrarily
close to 0. Since 0 /∈c ̸= ∅, c contains a nonzero point. Together these imply
c // b is unbounded, again contrary to hypothesis.
Thus all the cases of 0 ∈b give a contradiction, proving 0 /∈b. Hence by
part (iii) there is at most one root in x and we must show there is at least one.
If f(x) = 0 there is nothing more to prove, so assume f(x) ̸= 0.
Let b∗be the bound of b nearest 0, so it is ﬁnite, ̸= 0 and in b. Let z∗be
the intercept on the x-axis of the line through (x, f(x)) with slope b∗, so
z∗= x −f(x)/b∗,
(6)
equivalently
f(x) = (x −z∗)b∗.
(7)
Since f(x) ∈c and b∗∈b, (6) shows z∗∈Y ⊆x. Also x ∈x so by the
Mean Value Theorem there is ξ ∈x with
f(x) −f(z∗) = (x −z∗)f ′(ξ).
(8)

38
J. Pryce
Subtracting this from (7) gives
f(z∗) = (x −z∗)(b∗−f ′(ξ))
(9)
Now f ′(ξ) is in b by the latter’s deﬁnition, so by the deﬁnition of b∗it has the
same sign as b∗and at least as large absolute value, i.e. f ′(ξ)/b∗≥1. Dividing
(9) by (7) (recalling f(x) ̸= 0) now gives
f(z∗)/f(x) = 1 −f ′(ξ)/b∗≤0,
so f(z∗) has opposite (in the weak sense) sign to f(x). By the Intermediate
Value Theorem f has a zero z between x and z∗. Since both the latter are in x
we have z ∈x, and the result is proved.
⊓⊔
References
1. Gustafson, J.L.: The End of Error: Unum Computing. Chapman and Hall/CRC.
Taylor & Francis, Upper Saddle (2015)
2. Hansen, E.R., Greenberg, R.I.: An interval Newton method. J. Appl. Math. Com-
put. 12, 89–98 (1983)
3. Heath, T.L.: A History of Greek Mathematics. Number v. 1 in A History of Greek
Mathematics. Clarendon Press, Oxford (1921)
4. Kantorovich, L.V.: On some new approaches to computational methods and to
processing of observations. Siberian Math. J. 3(5), 701–709 (1962). In Russian
5. Kaucher, E.W.: Interval analysis in the extended interval space IR. Comput. Suppl.
2, 33–49 (1980)
6. Baker Kearfott, R.: A Fortran 90 environment for research and prototyping of
enclosure algorithms for nonlinear equations and global optimization. ACM TOMS
21(1), 63–78 (1995)
7. Kn¨uppel, O.: PROFIL/BIAS – a fast interval library. Computing 53(3–4), 277–287
(1994)
8. Kreinovich, V.: Interval computations web site. http://cs.utep.edu/interval-comp,
August 28, 2014
9. Lerch, M., Tischler, G., von Gudenberg, J.W., Hofschuster, W., Kr¨amer, W.:
FILIB++, a fast interval library supporting containment computations. ACM
Trans. Math. Softw. 32(2), 299–324 (2006)
10. Mayer, G.: Epsilon-inﬂation in veriﬁcation algorithms. J. Comput. Appl. Math.
60, 147–169 (1995)
11. Moore, R.E.: Interval Analysis. Prentice-Hall, Englewood Cliﬀs, N.J. (1966)
12. Neumaier, A.: Vienna proposal for interval standardization. August 28, 2014.
December 2008. http://www.mat.univie.ac.at/∼neum/ms/1788.pdf
13. Rump, S.M.: INTLAB - INTerval LABoratory. In: Csendes, T. (ed.), Develop-
ments in Reliable Computing, pp. 77–104. Kluwer Academic Publishers, Dordrecht
(1999). http://www.ti3.tuhh.de/rump/
14. Rump, S.M.: Veriﬁcation methods: Rigorous results using ﬂoating-point arithmetic.
Acta Numer. 19, 287–449 (2010). Cambridge University Press
15. Rump, S.M.: Interval arithmetic over ﬁnitely many endpoints. BIT Numer. Math.
52(4), 1059–1075 (2012)

The Forthcoming IEEE Standard 1788 for Interval Arithmetic
39
16. Sunaga, T.: Theory of an interval algebra and its application to numerical analysis.
Res. Assoc. Appl. Geom. Memoirs 2, 29–46 (1958)
17. Tucker, W.: Validated Numerics: A Short Introduction to Rigorous Computations.
Princeton University Press, Princeton (2011)
18. van Hentenryck, P., Macallester, D., Kapur, D.: Solving polynomial systems using
a branch and prune approach. SIAM J. Numer. Anal. 34(2), 797–827 (1997)

Uncertainty

Numerical Probabilistic Approach
for Optimization Problems
Boris Dobronets(B) and Olga Popova
Siberian Federal University, Krasnoyarsk, Russia
{BDobronets,olgaarc}@yandex.ru
Abstract. In the paper a new approach to optimization problems with
random input parameters, which is deﬁned as random programming, is
discussed. This approach uses a numerical probability analysis and allows
us to construct the set of solutions of an optimization problem based on
the joint probability density function.
1
Introduction
The studies of many practical problems, including the problem of decision-
making, require the implementation of the optimization approach. The eﬀective-
ness of the solutions depends on several factors. Such factors primarily include
the data necessary for the description and the solution of the problem. One of
the important factors that should be considered when solving such problems is
uncertainty of input data.
We can distinguish three basic models of uncertainty: stochastic, fuzzy and
set-valued (in particular — interval-valued).
The nature of undeﬁned data may be random errors associated with mea-
surement, or the incompleteness of information. This is described as random,
inaccurate, incomplete data.
The paper deals with the numerical probabilistic approach to solving optimiza-
tion problems with random inputs. Using methods of mathematical programming
for these problems, we obtain optimal solutions that depend on these parameters.
In the cases where probability densities of input parameters are known, it is possi-
ble to construct a probability density function of the joint probability of the opti-
mal solutions on the basis of numerical probability analysis. In contrast to the
stochastic programming [6,8], where the optimal solution is a ﬁxed solution, this
approach allows us to obtain the whole set of solutions of an optimization problem
deﬁned by the constructed joint probability density function.
By random programming, we mean the methods for the construction of the
solution set for an optimization problem with random input parameters based
on the application of numerical probabilistic analysis.
It is important to note that after representation of the obtained uncertain-
ties we deal with the problem of choosing a method that allows us to perform
the subsequent calculations in such a way as to get real results without additional
uncertainties [5].
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 43–53, 2016.
DOI: 10.1007/978-3-319-31769-4 4

44
B. Dobronets and O. Popova
To this end, nowadays mathematical tools for uncertain programming are
developed. Uncertain programming is the theoretical basis for solving optimiza-
tion problems for various uncertainty conditions [6].
Since an interval number can be considered as a special case of an imprecise
quantity, interval analysis, interval arithmetic, and interval programming fall
into imprecise programming.
In most of stochastic programming algorithms, the operator of mathematical
expectation is used and averaging procedures are performed.
In this paper, we develop a technique that uses Numerical Probabilistic
Analysis to solve various problems with stochastic data uncertainty [2,4].
The basis of NPA consists of numerical operations on probability density
functions of random variables. They involve the operations “+”, “−”, “·”, “/”,
“↑”, “max”, “min”, as well as binary relations “≤”, “≥” and some others. The
numerical operations of the histogram arithmetic constitute the major compo-
nent of NPA [1].
Using the arithmetic of probability density functions and probabilistic exten-
sions, we can construct numerical methods that enable us to solve systems of
linear and nonlinear algebraic equations with stochastic parameters [2].
2
Formulation of the Problem and Background
Let us formulate the problem of random programming as follows:
min
x f(x, ξ),
(1)
subject to (s.t.)
gi(x, ξ) ≤0,
i = 1, ..., m,
(2)
where x is the solution vector, ξ is the vector of parameters, f(x, ξ) is the objec-
tive function, gi(x, ξ) are constraint functions.
The vector x∗is a solution of problem (1)–(2), if
f(x∗, ξ) = inf
x∈U f(x, ξ),
where
U = {x|gi(x, ξ) ≤0,
i = 1, ..., m}.
We will suppose that ξ have random components. As x∗is a function of the
vector ξ, then
x∗= x∗(ξ),
it is also a random vector and its joint probability distribution is what we are
interested in.
The solution set of (1)–(2) is deﬁned as follows
X = {x∗|f(x∗, ˜ξ) = inf
x∈U f(x, ˜ξ), gi(x, ˜ξ) ≤0,
i = 1, ..., m, ˜ξ ∈supp(ξ)}.

Numerical Probabilistic Approach for Optimization Problems
45
So in contrast to the deterministic problem, for x∗it is necessary to determine
the probability density function for each component of x∗
i as the joint probability
density.
When both objective function and constraint functions are linear functions,
the problem is called a problem of linear programming. Otherwise, the problem
is called a problem of nonlinear programming.
For example the problem of linear programming with random data is formu-
lated as follows:
min
x cT x,
(3)
s.t.
Ax = b, x ≥0,
(4)
where A is a matrix, b, c are vectors.
The vector x∗is the solution of problem (3)–(4) provided that
cT x∗= inf
x∈U cT x,
where
U = {x|Ax = b, x ≥0}.
Let A be a random matrix, b, c are random vectors. So x∗is a random
function of the variables in A, b and c.
The solution set of (3)–(4) is
X = {x∗|cT x∗= inf
x∈U cT x, Ax = b, x ≥0, A ∈supp(A), b ∈supp(b), c ∈supp(c)}.
3
Numerical Probabilistic Analysis
3.1
Operations on Probability Densities of Random Variables
Let us consider operations on histograms. Let p(x, y) be a joint probability den-
sity function of two random variables x and y. Let pz be a histogram approxi-
mating the probability density of the operations between two random variables
x ∗y, where ∗∈{+, −, ·, /, ↑}. Then the probability to ﬁnd the value z within
the interval [zi, zi+1] is determined by the formula [1,2]
P(zk < z < zk+1) =

Ωk
p(x, y) dx dy,
(5)
where Ωk = {(x, y)|zk ≤x ∗y ≤zk+1} and the value Pk of the histogram on the
interval [zk, zk+1] is deﬁned as
Pk =

Ωk
p(x, y) dx dy/(zk+1 −zk).

46
B. Dobronets and O. Popova
Then we extend the order relation ≻∈{<, ≤, ≥, >} to random variables:
x ≻y if and only if x ≻y for all x ∈x, y ∈y.
If the support of x, y are intersected, then we can talk about the probability of
x ≻y
P(x ≻y) =

Ω
p(x, y)dxdy,
where Ω = {(x, y)|x ≻y} is the set of points (x, y) ∈R2 such that x ≻y, p(x, y)
is the joint probability density of x, y.
For example, consider the operation max(x, y). The probability P(max(x, y)
< z) is determined by the formula
P(z) =

Ωz
p(x, y)dxdy,
where Ωz = {(x, y)|(x < z) and (y < z)} and the value Pi of the histogram on
the interval [zi, zi+1] is deﬁned as
Pi = (P(zi+1) −P(zi))/(zi+1 −zi).
3.2
Probabilistic Extensions
One of the most important problems that NPA deals with is to construct prob-
ability density functions of random variables. Let us start with the general case
where (x1, . . . , xn) is a system of continuous random variables with the joint
probability density function p(x1, . . . , xn) and the random variable z is a func-
tion f(x1, . . . , xn)
z = f(x1, . . . , xn).
(6)
By probabilistic extension of the function f, we mean the probability density
function of the random variable z.
Let us construct the histogram F approximating the probability density func-
tion of the variable z. Suppose the histogram F is deﬁned on a grid { zi | i =
0, . . . , n }. The domain is deﬁned as Ωi = {(x1, . . . , xn)|zi < f(x1, . . . , xn) <
zi+1}. Then the value Fi of the histogram on the interval [zi, zi+1] is deﬁned as
Fi =

Ωi
p(x1, x2, . . . , xn)dx1dx2 . . . dxn/(zi+1 −zi).
(7)
By histogram probabilistic extension of the function f, we mean the histogram
F constructed according to (7).
Let f(x1, . . . , xn) be a rational function. To construct the histogram of F, we
replaced the arithmetic operation by the histogram operation, while the variables
x1, x2, . . . , xn are replaced by the histogram of their possible values. It makes
sense to call the resulting histogram of F as natural histogram extension (similar
to “natural interval extension”).

Numerical Probabilistic Approach for Optimization Problems
47
Case 1 [3]. Let x1, . . . , xn be independent random variables. If f(x1, . . . , xn) is
a rational expression where each variable xi occurs no more than once, then the
natural histogram extension approximates a probabilistic extension.
Case 2. Let the function f(x1, . . . , xn) admit a change of variables, so that
f(z1, . . . , zk) is a rational function of the variables z1, . . . , zk satisfying the con-
ditions of Case 1. The variable zi is a function of xi, i ∈Indi and the Indi are
mutually disjoint. Suppose for each zi it is possible to construct the probabilistic
extension. Then the natural extension of f(z1, . . . , zk) is approximated by the
probabilistic extension of f(x1, . . . , xn).
Case 3. We have to ﬁnd the probabilistic extension for the function f(x1,
x2, . . . , xn), but the conditions of Case 2 are not fulﬁlled. Suppose for deﬁniteness
that only x1 occurs a few times.
If, instead of the random variable x1, we substitute a determinate value t,
then it is possible to construct the natural probabilistic extension for the function
f(t, x2, . . . , xn).
Suppose that t is discrete random value approximating x1 as follows. Let t
take the value ti with probability Pi and for each function f(ti, x2, . . . , xn) it is
possible to construct the natural probabilistic extension.
Then the probabilistic extension f of the function f(x1, . . . , xn) can be
approximated by the probability density ϕ as follows [3]
ϕ(ξ) =
n

i=1
Piϕi(ξ).
(8)
3.3
Systems of Linear Algebraic Equations
Consider a system of linear algebraic equations
Ax = b, i = 1...n,
(9)
where x ∈Rn is a random vector solution, A = (aij), b = (bi) are a ran-
dom matrix and a right-hand side vector, respectively. Suppose that the random
matrix aij and the vector bi have independent components with probability
densities paij, pbi respectively.
The support of the solution set can be represented as follows [2,7]
X = {x|Ax = b, A ∈supp(A), b ∈supp(b)}.
With each x ∈X we can associate the following subset of coeﬃcients Ax ⊂
supp(A), bx ⊂supp(b)
Ωx = {A, b|Ax = b, A ∈supp(A), b ∈supp(b)}.
Note that for ﬁxed x, the coeﬃcients of the matrix and the right-hand side vector
are related by
n

j=1
aijxj −bi = 0, i = 1, ..., n,

48
B. Dobronets and O. Popova
therefore
Ωx = {A, b|
n

j=1
aijxj −bi = 0, i = 1, ..., n}.
Suppose we want to ﬁnd the probability P(X0) that the solutions x falls in a
subset X0 ⊂X. With X0 we associate the set Ω0 = {Ωx|x ∈X0}.
Then
P(X0) =

Ω0
n

i=1
n

j=1
paij
n

i=1
pbidΩ.
Since P(X0) in many cases is proportional to the volume of Ω0, we can a
priori determine the areas with the lowest and highest probability.
4
Random Linear Programming
It is known that for the problem (3)–(4) the optimal solution x∗is achieved at
the corner of the set U.
Theorem 1 [9]. Let the set U be deﬁned the conditions (4). A point x =
(x1, ..., xn) ∈U is a corner point if and only if there exist numbers j1, ...jr:
Aj1xj1 + ... + Ajrxjr = b; xj = 0, j ̸= jl, l = 1, ..., r,
where the columns of the Aj1, ..., Ajr are linearly independent.
Example 1. Let U is deﬁned by a matrix A and vector b
A =

1
1 3 1
1 −1 1 2

, b =

3
1

,
then to the columns of the matrix A1, A2 there corresponds to a corner point
with coordinates (2, 1, 0, 0), to A1, A3 there corresponds (0, 0, 1, 0), and to A2, A4
there corresponds (0, 5/7, 0, 4/3).
Note that out of the n columns, we can choose r linearly independent columns
in no more than Cr
n ways. Hence, the number of corner points of the set U is
ﬁnite.
This means that we can try to solve the canonical problem (3)–(4) in the
following way:
(1) ﬁnd all corners points x of the set U,
(2) to calculate the value of the function (c, x) at each of the corner points
and to determine the smallest ane.
However, this approach is not necessarily valid, as even in problems of small
dimension the number of corner points can be very large.
Nevertheless, the idea of searching the corner points of a set is very fruitful
and served as basis for a number of methods for solving linear programming
problems. One of these methods is the so-called simplex method.
For the problem (3)–(4), construct the joint probability density of the
vector x∗. For this purpose, we use a method for the solution of deterministic
problems of linear programming, for example, the simplex method.

Numerical Probabilistic Approach for Optimization Problems
49
Consider the auxiliary problem.
min
x cT
t x,
(10)
s.t.
Atx = bt, x ≥0
(11)
and
At ∈supp(A), bt ∈supp(b), ct ∈supp(c),
(12)
ﬁnd a solution x∗
t and the corresponding corner point with numbers j1, ...jr.
We solve the random system of linear algebraic equations by numerical prob-
abilistic analysis [4]
(Aj1...Ajr)x = b.
The joint probability density of the obtained solution corresponds to x ∗
t . If
the supports of the input parameters are small enough, then due to continuity x ∗
t
coincides with x ∗. In the case of arbitrary supports of the input parameters the
search procedure for At, bt, ct, should be repeated, using the Monte Carlo method
or genetic algorithms. In the case that diﬀerent solutions x ∗
t are obtained, they
can be compared calculating the probabilistic extension f t = cT x ∗
t .
4.1
Numerical Example
As a numerical example, consider the following problem
min
x cT x,
(13)
s.t.
Ax = b, x ≥0
(14)
and
A ∈supp(A), b ∈supp(b), c ∈supp(c),
(15)
where A = (aij) is a uniform random matrix, each its element is a uniform
random variable with support [aij, aij], similarly, b, c are random vectors whose
elements are uniform random variables.
The supports are deﬁned as follows
A =
[1 −r, 1 + r]
[1 −r, 1 + r]
[1 −r, 1 + r] [−1 −r, −1 + r]
[3 −r, 3 + r] [1 −r, 1 + r]
[1 −r, 1 + r] [2 −r, 2 + r]

,
b =
[3 −r, 3 + r]
[1 −r, 1 + r]

,
c = (−1, −1, 0, 0).

50
B. Dobronets and O. Popova
Fig. 1. Joint density of the vector (x 1, x 2)
For r = 0, which corresponds to the deterministic case, the solution is x∗=
(2, 1, 0, 0), the columns of the matrix A1, A2 correspond to a corner point.
In Fig. 1 the joint density of the vector x 1, x 2 for r = 0.1 with components
x3 = 0, x4 = 0 is shown. The solid line is the boundary of the set of solutions
on the (x1, x2) plane. The set X of solutions is the quadrangle with the vertices
(2.0,0.636), (2.444,1.0), (2.0,1.444), (1.636,1.0). Value of the probability is repre-
sented by shades of gray. As can be seen from the Fig. 1 the probability density
is non-uniformly distributed, the highest density is achieved at the center, near
the point (2.0, 1.0).
The area of X strongly depends on the r, and it increases with increasing r
and becomes inﬁnite for r = 1. This is due to the fact that among the matrices

0 0
0 0

∈

[0, 2]
[0, 2]
[0, 2] [−2, 0]

,
there are linearly dependent columns.
5
Applications
As an illustration, we consider optimization of hydroelectric power generation.
Power generating electricity p can be expressed as
p = Chu,
where C is a constant; h is height of the water level, h ∈[hmin, hmax], u is water
passing through the turbine, u ∈[umin, umax].
Height h depends on the amount V of water in the reservoir:
h = h(V )

Numerical Probabilistic Approach for Optimization Problems
51
and
V (t) = V0 +
 t
0
q(ξ) −u(ξ) −ux(ξ)dξ.
where q(t) is inﬂow; ux(t) is water passing through the spillway (is known and
is determined by plant personnel); u is water passing through the turbine, u ∈
[umin, umax].
Suppose we want to maximize the generation of electricity in the time interval
[0, T]
max
u
P(u) =
 T
0
C h

V0 +
 T
0
q(t) −u(t) −ux(t)dt

u(t)dt.
Simplify the Problem
We represent volume of the reservoir as
V = V0 + S(h −h0),
and
h(t) = h0 + (V (t) −V0)/S = h0 + (
 t
0
q(ξ) −u(ξ) −ux(ξ)dξ)/S.
It is now clear that the problem to be solved is to maximize the objective
P(u) = C
 T
0

h0 + (
 t
0
q(ξ) −u(ξ) −ux(ξ)dξ)/S

u(t)dt,
where q(t) is inﬂow; ux(t) is water passing through the spillway; u is water
passing through the turbine, u ∈[umin, umax].
Let us consider a discrete model. Let ω = {t0 < t1 < . . . < tn} be a grid,
qi be a random input value of water inﬂow for a time [ti−1, ti], uxi be water
passing through the spillway for a time [ti−1, ti], ui be water passing through
the turbine for a time [ti−1, ti], U = (ui), i = 1, . . . , n,
max
U
P(U ) = C
n

i=1
⎛
⎝h0 + (
i

j=1
q j −uj −uxj)/S
⎞
⎠ui.
In some cases, the problem can be reduced to the solution of a random system
of linear algebraic equations. Here, only the right-hand side of the system is
random
2u1 + u2 + . . . + un = Sh0 + q 1,
u1 + 2u2 + . . . + ui + . . . + un = Sh0 + q 1 + q 2,
. . .

52
B. Dobronets and O. Popova
u1 + u2 + . . . + 2ui + . . . + un = Sh0 +
i

j=1
qj,
. . .
u1 + u2 + . . . + 2un = Sh0 +
n

j=1
q j.
Numerical Example. Note that ui, i = 1, . . . , n can be expressed as a linear
combination of q i, i = 1, . . . , n.
For n = 3 we get:
u1 = −q3 −2q2 + q1 + Sh0
4
,
u2 = −q3 + 2q2 + q1 + Sh0
4
,
u3 = 3q 3 + 2q 2 + q 1 + Sh0
4
.
Let q i ∈[qi, qi] be uniform random variables, S = 1, q1 = [0.1, 0.2], q2 =
[0.2, 0.3], q3 = [0.3, 0.4] be supports and h0 = 0.9.
Fig. 2. Histogram u1 and joint probability density (u1, u2), (u2, u3)
The supports are u1 = [0.0, 0.1], u2 = [0.25, 0.35], u3 = [0.575, 0.725]. In
Fig. 2 piecewise constant approximations of the joint probability density of the
vectors (u1, u2), (u2, u3) are shown. The value of the probability is represented
by shades of gray.
6
Conclusion
The considered methods for solving the above problems of linear optimization
enable one to represent random programming as an eﬀective method for solving
optimization problems with uncertain input parameters. Methods of random
programming allow one to build a joint probability density function on the set
of optimal solutions. This approach helps the decision maker to choose the best
solutions and enables risks assessment.

Numerical Probabilistic Approach for Optimization Problems
53
References
1. Dobronets, B.S., Krantsevich, A.M., Krantsevich, N.M.: Software implementation
of numerical operations on random variables. J. Siberian Federal Univ. Math. Phys.
6(2), 168–173 (2013)
2. Dobronets, B.S., Popova, O.A.: Numerical operations on random variables and
their application. J. Siberian Federal Univ. Math. Phys. 4(2), 229–239 (2011).
(Russian)
3. Dobronets, B.S., Popova, O.A.: Elements of numerical probability analysis. SibSAU
Vestnik 42(2), 19–23 (2012). (Russian)
4. Dobronets, B.S., Popova, O.A.: Numerical probabilistic analysis under aleatory
and epistemic uncertainty. Reliable Comput. 19, 274–289 (2014)
5. Schjaer-Jacobsen, H.: Representation and calculation of economic uncertainties:
Intervals, fuzzy numbers, and probabilities. Int. J. Prod. Econ. 78, 91–98 (2002)
6. Liu, B.: Theory and Practice of Uncertain Programming, 2nd edn. Springer,
Heidelberg (2009)
7. Popova, O.A.: Optimization problems with random data. J. Siberian Federal Univ.
Math. Phys. 6(4), 506–515 (2013)
8. Shapiro, A., Dentcheva, D., Ruszczynski, A.: Lectures on Stochastic Programming:
Modeling and Theory. SIAM, Philadelphia (2009)
9. Vasil’ev, F.P.: Numerical Methods for Solving Extremal Problems. Nauka, Moscow
(1988). (Russian)

Towards the Possibility of Objective Interval
Uncertainty
Luc Longpr´e, Olga Kosheleva, and Vladik Kreinovich(B)
University of Texas at El Paso, El Paso, TX 79968, USA
{longpre,olgak,vladik}@utep.edu
Abstract. Applications of interval computations usually assume that
while we only know an interval containing the actual (unknown) value
of a physical quantity, there is the exact value of this quantity, and
that in principle, we can get more and more accurate estimates of this
value. Physicists know, however, that, due to the uncertainty principle,
there are limitations on how accurately we can measure the values of
physical quantities. One of the important principles of modern physics
is operationalism – that a physical theory should only use observable
properties. This principle is behind most successes of the 20th century
physics, starting with relativity theory (vs. un-observable aether) and
quantum mechanics. From this viewpoint, it is desirable to avoid using
un-measurable exact values and to modify the mathematical formalisms
behind physical theories so that they explicitly only take objective uncer-
tainty into account. In this paper, we describe how this can be done for
objective interval uncertainty.
Keywords: Interval uncertainty · Algorithmic randomness · Physics
1
Formulation of the Problem
Is Interval Uncertainty Subjective? Applications of interval computations
usually assume that while we only know an interval [x, x] containing the actual
(unknown) value of a physical quantity x, there is the exact value x of this
quantity, and that in principle, we can get more and more accurate estimates of
this value.
This assumption is in line with the usual formulations of physical theo-
ries – as partial diﬀerential equations relating exact values of diﬀerent physical
quantities, ﬁelds, etc., at diﬀerent space-time locations and moments of time;
see, e.g., [2]. Physicists know, however, that, due to the uncertainty principle,
there are limitations on how accurately we can measure the values of physical
quantities [2,8].
This is not just a theoretical concern: for example, the International Union of
Pure and Applied Chemistry (IUPAC) has recently oﬃcially recognized that the
atomic weight of a chemical element is not an exact number; depending on where
the sample came from, the atomic weight may diﬀer within a certain interval;
see, e.g., [11].
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 54–65, 2016.
DOI: 10.1007/978-3-319-31769-4 5

Towards the Possibility of Objective Interval Uncertainty
55
It is Desirable to Take Objective Uncertainty into Account. One of the
important principles of modern physics is operationalism – that a physical theory
should only use observable properties. This principle is behind most successes of
20th century physics, starting with relativity theory (vs. un-observable aether)
and quantum mechanics. From this viewpoint, it is desirable to avoid using un-
measurable exact values and to modify the mathematical formalisms behind
physical theories so that they explicitly only take objective uncertainty into
account.
Objective Uncertainty is About Probabilities. According to quantum
physics, we can only predict probabilities of diﬀerent events. Thus, uncertainty
means that instead of exact values of these probabilities, we can only determine
intervals; see, e.g., [3,4].
Let us give a simple example. In the ideal world, atoms of the same element
have exactly the same atomic weight, and – if their are radioactive – exactly the
same probability p that they will decay within a given moment of time. In such
an ideal situation, by taking larger and larger samples and measuring the decay
frequency, we can get more and more accurate estimates of the desired decay
probability p.
In practice, as we have mentioned, diﬀerent objects made of the same ele-
ment have, in general, slightly diﬀerent average atomic weight – and similarly,
they have slightly diﬀerent average decay probabilities, probabilities that may
take diﬀerent values from the corresponding interval [p, p]. Depending on which
objects we take, we may get frequencies close to the lower bound p and we may
also get frequencies close to the upper bound p. As we increase the sample size,
we will get frequencies oscillating between p and p, without ever converging to
a single value.
Formulation of the Problem. What is the observational meaning of such
interval-valued probabilities?
2
Analysis of the Problem
What is the Observational Meaning of Probability? Probability refers
to repeated events: we repeat the same experiment (or perform many similar
observations) and record the results as a binary sequence ω1ω2 . . . For example,
when we talk about the probability of a coin falling heads, we mean that we
repeatedly ﬂip the coin and record the resulting sequence: for example, we can
take ωi = 1 if the coin falls heads in the i-th experiment, and ωi = 0 if this coin
falls tails.
In there terms, the fact that the probability of heads is 1/2 means that in the
limit, when n →∞, the ratio of 1 s in a sequence ω1 . . . ωn tends to 1/2. However,
this is only the part of this meaning: for example, for a sequence 0101. . . , the
ratio tends to 1/2, but we would not call it a random sequence corresponding to
probability 1/2.
From the practical viewpoint, when we say that a sequence ω1ω2 . . .
is random, we assume that this sequence satisﬁes all the probability laws

56
L. Longpr´e et al.
(such as the law of large numbers or the Central Limit Theorem); these proba-
bility laws are what practitioners use to check whether the sequence is random.
From this viewpoint, if a sequence satisﬁes all probability laws, then for all
practical purposes we can consider it random. Thus, we can formally deﬁne
a sequence to be random if it satisﬁes all probability laws. In precise terms,
a probability law is a property ℓwhich is true with probability 1: P(ℓ) = 1.
So, a sequence is random if it satisﬁes all the properties which are true with
probability 1.
Properties are in 1-1 correspondence with sets – to each property, we can
assign the set of all the sequences that satisfy this property and, vice versa, to
every set, we can assign a property of belonging to this set. When we talk about
probability laws, we mean only properties which can be described by ﬁnitely
many symbols from a certain formal language; the corresponding sets are known
as deﬁnable sets. Thus, we can say that a sequence is random if it belongs to all
deﬁnable sets of probability measure 1.
A sequence belongs to a set of measure 1 if and only if it does not belong
to its complement C = −S with P(C) = 0. So, we can equivalently say that a
sequence is random if it does not belong to any deﬁnable set of measure 0. This
is, in eﬀect, Kolmogorov-Martin-L¨of’s (KML) deﬁnition of a random sequence;
see, e.g., [7].
Each deﬁnable set is determined by a ﬁnite sequence of symbols. There are no
more than countably many ﬁnite sequences of symbols, thus, there are countably
many deﬁnable sets. So, the union of all such sets has measure 0. Therefore,
almost all sequences are KML-random.
Probability Interval: What Is Its Observational Meaning? We have
recalled what is an observational meaning of an exact probability p. What is
the observational meaning of a probability interval, when instead of a single
probability measure we have several possible probability measures?
This is not an easy question: in [1,6], we have shown that in seemingly rea-
sonable formalizations, every random sequence is actually random relative to
one of the possible probability measures. In such a formalization, every random
sequence – including the sequence of observations – corresponds to one speciﬁc
probability measure. In other words, there is a probability, we just do not know
it – this is exactly the subjective interval uncertainty that we are trying to avoid.
We Consider Independent Repeated Events. Probabilities have direct
observational meaning only for repeating events. In mathematical terms, inde-
pendent repeating events correspond to a product measure, when the probability
of two events A and B happening in two consequent tests is equal to the product
of the corresponding probabilities: P(A & B) = P(A) · P(B).
Traditional Case. The traditional case is when we know the exact probability p.
Then, observable sequences ω1ω2 . . . are KLM-random relative to a product of
p-measures.

Towards the Possibility of Objective Interval Uncertainty
57
It may be that in practice, we do not know the exact value of this probabil-
ity p, we only know the interval [p, p] containing this probability. In this case,
we have an interval uncertainty, but this interval uncertainty is subjective in
the following sense: there is the actual exact value of the probability, the value
which can be determined, e.g., by taking larger and larger samples; then the
corresponding frequencies will be closer and close to the actual probability.
What We are Trying to Describe. What we are trying to describe is the case
when there is no such objective probability: e.g., the case when the corresponding
frequencies do not have an exact limit: limit frequencies oscillate between p and p.
3
Objective Interval Uncertainty: Deﬁnitions
and the First Result
Deﬁnition 1. We say that a sequence is [p, p]-random if it is random for some
product measure with pi ∈[p, p].
Deﬁnition 2. We say that a sequence ω1ω2 . . . is objectively [p, p]-random if this
sequence is [p, p]-random, and it is not [q, q]-random for any proper subinterval
[q, q] ⊂[p, p].
Proposition 1. For every interval [p, p], there exist objectively [p, p]-random
sequences.
Proof. We will show that any sequence ω1ω2 . . . corresponding to pi for which
lim inf pi = p and lim sup pi = p is objectively [p, p]-random.
Since pi ∈[p, p], this sequence is [p, p]-random. Let us prove that this sequence
ω1ω2 . . . is not [q, q]-random for any proper subinterval [q, q] ⊂[p, p], i.e., that it
is not random w.r.t. any sequence qi ∈[q, q].
It is known that if two measures are mutually singular, then no sequence is
random w.r.t. both measures. For product measures, singularity is equivalent to
the following equality (see, e.g., [7,8]):
∞

i=1

(√pi −√qi)2 +

1 −pi −

1 −qi
2
= +∞.
For a proper subinterval, either p < q or q < p. Without loss of generality, let
us consider the case when p < q.
When lim inf pi = p then, for every ε > 0, there are inﬁnitely many i for which
√pi ≤√p+ε. For these i, we have qi ≥q, so √qi ≥√q. Thus, √qi−√pi ≥√q−
√p + ε

=
√q −√p

−ε. For ε = (√q −√p)/2, we have √qi −√pi > ε > 0
and therefore, the above sum is inﬁnite. So, a {pi}-random sequence ω1ω2 . . .
cannot be {qi}-random. The proposition is proven.

58
L. Longpr´e et al.
4
Objective Interval Uncertainty: A Stronger
Deﬁnition and the Second Result
Discussion. We want to describe the idea that all we know is an interval [p, p].
The above deﬁnition means, in eﬀect, that all the values pi from the sequence
pi are in between p and p and that, even if we dismiss ﬁnitely many probabilities,
no narrower interval contains all the remaining values of pi.
In general, however, such sequences may satisfy additional laws, in addition
to pi ∈[p, p]. For example, if we have p2i = p and p2i+1 = p, then we satisfy
the above condition – but we also satisfy the additional condition, that all even-
placed probabilities are equal to p and all odd-placed probabilities are equal
to p.
Is it possible to have a sequence of probabilities pi whose only meaningful
property is that all these values are from the interval [p, p]? In other words, is
it possible to ﬁnd a sequence pi which does not satisfy any other meaningful
property?
What Is a “Meaningful Property”. In order to answer the above question,
we need to formalize what is meant by a meaningful property.
In foundations of mathematics, the main object is a set. Properties naturally
correspond to sets: namely, to each property, we can put into correspondence
the set of all the sequences that satisfy this property. In these terms, describing
what we mean by a property is equivalent to describing the corresponding sets.
First, a meaningful property must be described by a ﬁnite sequence of sym-
bols in an appropriate mathematical language. Corresponding sets are known
as deﬁnable sets. It is important to realize that while every example of a set
that we can give is deﬁnable, not all sets are deﬁnable: for example, there are
more than countably many subsets of the set of all natural numbers, but since
there are only countably many ﬁnite sequences, there are only countably many
deﬁnable sets.
Second, it is reasonable to only consider observable properties, i.e., proper-
ties whose validity can be determined based on observations. Let us show that,
because of this requirement, it is reasonable to require that for each observable
property, the corresponding set of sequences is closed in the sense of component-
wise convergence: if for all k, the sequences p(k) = {p(k)
i
} belong to this set, then
their limit p = {pi}, with pi
def
= lim
k p(k)
i
, should also belong to the correspond-
ing set.
Indeed, in practice, we do not observe probabilities, we only observe fre-
quencies which are close to probabilities. If the actual probabilities are the limit
values (p1, p2, . . .), this means that for every ε > 0 and for a suﬃciently large
sample, we will observe frequencies fi which are ε-close to these limit value pi.
Since p is the limit of p(k), for suﬃciently large k, the values pi – and thus,
the frequencies fi – are close to the probabilities p(k)
i
, and therefore, consistent
with the assumption that the actual probabilities are p(k)
i
(and thus, with the
assumption that the actual probabilities satisfy the given property).

Towards the Possibility of Objective Interval Uncertainty
59
So, if the actual probabilities are equal to the limit, then no matter how
large a sample we take, the resulting observations will always be consistent with
the given property. Thus, it is reasonable to add the limit p to the set of all
probability sequences that satisfy the given property. Because of this argument,
in the following text, we will assume that for each observable property, the
corresponding set of sequences is closed.
The third property of the corresponding sets comes from the need to distin-
guish trivial unavoidable “properties” like p1 = p – properties that do not really
restrict any values beyond a few ﬁrst ones – from non-trivial properties that we
are trying to avoid. In other words, we need to formulate the idea that if we only
know approximate values of the ﬁrst n probabilities, then we cannot guarantee
that the corresponding property will be satisﬁed.
This requirement can be described in precise terms, if on the set of all the
sequences p we introduce a topology in which the basis is formed by “boxes”
(p1, p1)×. . .×(pn, pn) corresponding to diﬀerent n and diﬀerent bounds pi and pi.
(Convergence in this topology corresponds to the above point-wise convergence.)
In terms of this topology, the above requirement means that within every element
from the basis – and thus, within every open set – there should be a sequence
that does not belong to the corresponding set S. For closed sets, this requirement
means that the set S is nowhere dense.
Summarizing, we can formalize our requirements by saying that by a mean-
ingful property, we mean a closed nowhere dense deﬁnable set, and that the
actual sequence of probabilities p should not belong to any of such sets.
Deﬁnition 3. Let S be a set of all sequences pi ∈[p, p], with topology whose
basis is formed by the boxes (p1, p1) × . . . × (pn, pn).
– By a meaningful property, we mean a deﬁnable closed nowhere dense set
S ⊆S.
– We say that a sequence p satisﬁes a meaningful property S if p ∈S.
– We say that a sequence p ∈S has no other properties if p does not satisfy any
meaningful property S.
Proposition 2. For every two deﬁnable values p < p, there exists a sequence pi
for which pi ∈[p, p] for all i and which has no other properties.
Mathematical Comment. Not only there exist such sequences, but there are many
such sequences: as we can see from the proof, “almost all” sequences p ∈S
(almost all in some reasonable sense) have no other properties.
Practical Comment. The above description is, at this stage, very theoretical. We
do not have a full understanding of how to check whether an experimentally
observed sequence has no other properties. One consequence that we can check
is that the limit frequencies should ﬁll the whole interval [p, p] and not be bound
by any narrower subinterval. In other words, as we increase the sample size, the
frequencies should always ﬁll this whole interval.

60
L. Longpr´e et al.
However, we may recall that in statistics, in addition to observing frequencies,
there are other criteria that describe p-random sequences – e.g., according to the
Central Limit Theorem, deviations of frequencies from the probability must be,
asymptotically, normally distributed. It is therefore desirable to come up with
similar criteria for the case of sequences which are [p, p]-random for some non-
degenerate interval p < p.
Proof. By deﬁnition, a sequence p has no other properties if its does belong to
any property-related set S (in the sense of Deﬁnition 3), i.e., equivalently, if it
does not belong to the union U of all such sets S.
Each property-related set S is, by deﬁnition, a deﬁnable closed nowhere dense
set. As we have mentioned, there are no more than countably many deﬁnable
objects, so U is a union of countably many closed nowhere dense sets. Such
unions are known as meager sets, or sets of ﬁrst Baire category. It is known that
the set of all sequences is not meager; this is the main gist of the corresponding
Baire’s theorem; see, e.g., [9]. Thus, there are sequences p which do not belong
to U, i.e., which have no other properties. Moreover, “almost all” sequences p –
in the sense of all sequences except for a meager set – do not belong to U, i.e.,
have no other properties. The proposition is proven.
5
Why This Is Interesting: Objective Interval Uncertainty
Can Potentially Help in Solving NP-Hard Problems
Faster
What We Do in This Section. Objective interval uncertainty means that
the corresponding series of repeated experiments, the sequence of observations
ω1ω2 . . . is random with respect to some sequence of probabilities pi for which
pi ∈[p, p] and which has no other property.
In this section, we prove that by using such sequences ω, it is, in principle,
possible to drastically speed up the solution of NP-complete problems.
Practical Comment. It should be emphasized that our result only says that it
is theoretically possible to speed up the solution of NP-hard problem. At this
point, we do not know how to actually achieve such a speed-up – but we hope
that the proof of theoretically possibility of the speed-up will eventually lead to
practical algorithms.
Mathematical Comment. From the mathematical viewpoint, the result from this
section is a modiﬁcation of a similar result from [5].
What Is an NP Problem? Brief Reminder. In practice, we often need to
ﬁnd a solution that satisﬁes a given set of constraints or at least check that such
a solution is possible. Once we have a candidate for the solution, we can feasibly
check whether this candidate indeed satisﬁes all the constraints. In theoretical
computer science, “feasibly” is usually interpreted as computable in polynomial
time, i.e., in time bounded by a polynomial of the length of the input.
A problem of checking whether a given set of constraints has solution is
called a problem of the class NP if we can check, in polynomial time, whether

Towards the Possibility of Objective Interval Uncertainty
61
a given candidate is a solution; see, e.g., [10]. Examples of such problem includes
checking whether a given graph can be colored in 3 colors, checking whether a
given propositional formula – i.e., formula of the type
(v1 ∨¬v2 ∨v3) & (v4 ∨¬v2 ∨¬v5) & . . .
is satisﬁable, i.e., whether this formula is true by some combination of the propo-
sitional variables vi.
Each problem from the class NP can be algorithmically solved by trying
all possible candidates. For example, we can check whether a graph can be
colored by trying all possible assignments of colors to diﬀerent vertices of a
graph, and we can check whether a given propositional formula is satisﬁable
by trying all 2n possible combinations of true-or-false values v1, . . . , vn. Such
exhaustive search algorithms require computation time like 2n, time that grows
exponentially with n. For medium-size inputs, e.g., for n ≈300, the resulting
time is larger than the lifetime of the Universe. So, these exhaustive search
algorithms are not practically feasible.
It is not known whether problems from the class NP can be solved feasibly
(i.e., in polynomial time): this is a famous open problem P
?=NP. It is known,
however, there are problems in the class NP which are NP-complete in the sense
that every problem from the class NP can be reduced to this problem. Reduction
means, in particular, that if we can ﬁnd a way to eﬃciently solve one NP-
complete problem, then, by reducing other problems from the class NP to this
problem, we can thus eﬃciently solve all the problems from the class NP.
So, it is very important to be able to eﬃciently solve even one NP-complete
problem. (By the way, both above example of NP problems – checking whether
a graph can be colored in 3 colors and whether coloring a propositional formula
is satisﬁable – are NP-complete.)
How to Represent Instances of an NP-Complete Problem. For each NP-
complete problem P, its instances are sequences of symbols. In the computer,
each such sequence is represented as a sequence of 0 s and 1s. Thus, we can
append 1 in front of this sequence and interpret the resulting sequence as a
binary code of a natural number i (we need to add 1 in front, so that diﬀerent
sequences transform into diﬀerent numbers, otherwise 0 and 00 will lead to the
same number).
In principle, not all natural numbers i correspond to instances of a problem P;
we will denote the set of all natural numbers which correspond to such instances
by SP. For each i ∈SP, the correct answer (true or false) to the i-th instance
of the problem P will be denoted by sP,i.
Easier-to-Solve and Harder-to-Solve NP-Complete Problems. For some
easier-to-solve problems, there are feasible algorithms which solve “almost all”
instances, in the sense that for each n, the proportion of instance i ≤n for
which the problem is solved by this algorithm tends to 1. In this case, while the
worst-case complexity is still exponential, in practice, almost all problems can
be feasibly solved.

62
L. Longpr´e et al.
A more challenging case are harder-to-solve NP-complete problems, for which
no feasible algorithm is known that would solve almost all instances. In this
section, we show that our method works on all NP-complete problems, both
easier-to-solve and harder-to-solve ones.
What We Mean by Using Physical Observations in Computations. We
assume that the sequence ωi comes from observations. In addition to performing
computations, our computational device can, given a natural number i, use the
result ωi of the corresponding i-th observation in its computations. In other
words, given an integer i, we can produce ωi.
In precise theory-of-computation terms, this means computations that use
the sequence ω as an oracle; see, e.g., [10].
Comment. Since we are interested in feasible (= polynomial time) computations,
the code should be set up in such a way that the overall time of an experiment
does not exceed a polynomial of the length of the number i. This can be done,
e.g., if we explicitly add maximum waiting time into the description of the exper-
iment, by adding as many 0 s as the time that we plan to wait.
Deﬁnition 4. By a [p, p]-algorithm A, we mean an algorithm which uses, as
an oracle, a sequence ωi which is random with respect to a probability measure
determined by a sequence pi for which pi ∈[p, p] for all i and which has no other
properties.
Notation. The result of applying an algorithm A using ωi to an input i will be
denoted by A(ω, i).
Deﬁnition 5. Let P be an NP-complete problem. We say that a feasible [p, p]-
algorithm A solves almost all instances of P if for every ε > 0 and δ > 0 and for
every integer n, there exists an integer N ≥n for which, with probability ≥1−δ,
the proportion of the instances i ≤N of the problem P which are correctly solved
by A is greater than 1 −ε:
Prob
#i{i ≤N : i ∈SP & A(ω, i) = sP,i}
#i{i ≤N : i ∈SP}
> 1 −ε
	
≥1 −δ.
Comment. The restriction to suﬃciently long inputs N ≥n makes perfect sense:
for short inputs, NP-completeness is not an issue: we can perform exhaustive
search of all possible bit sequences of length 10, 20, and even 30. The challenge
starts when the length of the input is high.
Proposition 3. For every NP-complete problem P, there exists a feasible [p, p]-
algorithm A that solves almost all instances of P.
Comment. In other words, we show that if there is objective interval uncertainty,
then, theoretically, the use of the corresponding physical observations makes all
NP-complete problems easier-to-solve (in the above-described sense).

Towards the Possibility of Objective Interval Uncertainty
63
Of course, as we have mentioned earlier, this does not mean that we already
have an eﬃcient algorithm for solving NP-complete problems – but this theoreti-
cal possibility is encouraging, and we hope that it will eventually lead to eﬃcient
algorithms.
Proof. We know that for every i, the probability pi that ωi = 1 is in between
p and p. Thus, for every two numbers N ≫N ′, the proportion of values ωi
(i = N, N + 1, . . . , N ′ −1) which are equal to 1, should be either within the
interval [p, p] or at least close to this interval. Let us use this property to design
the desired algorithm A.
A value p from the interval [p, p] is:
– closer to p if it is larger than the midpoint 
p
def
= p + p
2
and
– closer to p if p is smaller than the midpoint.
The midpoint itself is equidistant from both endpoints p and p.
Let us therefore select an increasing sequence N1 < N2 < . . ., and take:
• A(ω, i) = 1 if the proportion of values ωi = 1 between Ni and Ni+1 is greater
than or equal to the midpoint 
p, and
• A(ω, i) = 0 if this proportion is smaller than 
p.
Let us prove that, for an appropriate sequence Ni, this algorithm indeed solves
almost all instances of the given problem P.
The proposition states that for very ε > 0, δ > 0, and n, there exists an
integer N ≥n for which the above inequality holds. To prove the existence of
such an N, let us consider the set T of all sequences p for which, for all N ≥n,
this inequality does not hold. We will show that this set T is deﬁnable, closed,
and nowhere dense. By deﬁnition of a sequence that has no other properties
(Deﬁnition 3), this would imply that the actual sequence p does not belong to
this set T – and thus, there exists the desired value N, which is exactly what
the proposition claims.
Deﬁnability is easy: we just had deﬁned this set. Closeness is also rather easy
to prove; it can be proven similarly to a similar closeness proof in [5].
The non-trivial part is nowhere density. To prove that the set T is nowhere
dense, it is suﬃcient, for each ﬁnite starting sequence p1, . . . , pn, to produce an
inﬁnite extension p for which the desired integer N ≥n exists (and which, thus,
does not belong to the set T).
We will take a sequence pi all whose elements are either equal to the lower
endpoint p or to the upper endpoint p. Speciﬁcally, for all the values between
Ni and Ni+1, we will take:
• pi = p if sP,i = 1,
• pi = p if sP,i = 0, and
• any of these two values if i ̸∈SP.

64
L. Longpr´e et al.
Let us show that for an appropriate choice of the sequence Ni, with proba-
bility ≥1 −δ, for all the values i from n to N = n
ε , we will have A(ω, i) = sP,i.
This will imply that the proportion of such i is indeed greater than 1 −ε with
probability ≥1 −δ.
For each i, we consider the arithmetic average of ki
def
= (Ni+1 −1) −Ni
independent 0-1 random values each of which is equal to 1 with some probability
p (namely, either with probability p or with probability p). It is known that this
arithmetic average is, in the limit ki →∞, normally distributed – this fact is a
particular case of the Central Limit Theorem. The mean value of this average is
equal to the corresponding probability p, and the standard deviation decreases,
with ki, as
1
√ki
. Let us use these facts to estimate the probability that with
p = 0 we will have A(ω, i) = 1 or vice versa. In other words, we are interested
in the probability that the average diﬀers from its expected values by at least
the half-width w
def
= p −p
2
. For a normal distribution with mean μ and standard
deviation σ, asymptotically, this probability is proportional to exp

−w2
2σ2
	
, i.e.,
to exp(−const · ki). If we select ki in such a way that exp(−const · ki) ≤1
i2 , i.e.,
ki = const · ln(i), then the probability that this happens for one of the values i
cannot exceed the sum of the probabilities corresponding to diﬀerent i, and is,
thus, smaller than the sum
∞

i=n
1
i2 . Thus, the sum tends to 0 and is, therefore,
smaller than δ for all suﬃciently large n.
So, we get the desired property if we ﬁnd Ni for which ki ≈Ni+1 −Ni ∼
const · ln(i). This approximate equality is true if we take Ni = i · ln(i).
For this choice of Ni, computing A(ω, i) requires Ni+1 −Ni ∼ln(i) calls to
the oracle – a number which is a linear function of the bit length of an integer
i. Thus, this algorithm is indeed feasible. The proposition is proven.
Acknowledgments. This work was supported in part by the National Science
Foundation grants HRD-0734825, HRD-1242122, and DUE-0926721. The authors are
thankful to all the participants of the 16th International Symposium on Scientiﬁc
Computing, Computer Arithmetic, and Validated Numerics SCAN’2014 (W¨urzburg,
German, September 21–26, 2014) for valuable discussions, and to the anonymous ref-
erees for valuable suggestions.
References
1. Cheu, D., Longpr´e, L.: Towards the possibility of objective interval uncertainty in
physics. Reliable Comput. 15(1), 43–49 (2011)
2. Feynman, R., Leighton, R., Sands, M.: The Feynman Lectures on Physics. Addison
Wesley, Boston (2005)
3. Gorban, I.I.: Theory of Hyper-Random Phenomena. Ukrainian National Academy
of Sciences Publ., Kyiv (2007). in Russian

Towards the Possibility of Objective Interval Uncertainty
65
4. Gorban, I.I.: Hyper-random phenomena: deﬁnition and description. Inf. Theor.
Appl. 15(3), 203–211 (2008)
5. Kosheleva, O., Zakharevich, M., Kreinovich, V.: If many physicists are right and
no physical theory is perfect, then by using physical observations, we can feasibly
solve almost all instances of each NP-complete problem. Math. Struct. Model. 31,
4–17 (2014)
6. Kreinovich, V., Longpr´e, L.: Pure quantum states are fundamental, mixtures (com-
posite states) are mathematical constructions: an argument using algorithmic infor-
mation theory. Int. J. Theoret. Phys. 36(1), 167–176 (1997)
7. Li, M., Vit´anyi, P.: An Introduction to Kolmogorov Complexity and Its Applica-
tions. Springer, Heidelberg (2008)
8. Longpr´e, L., Kreinovich, V.: When are two wave functions distinguishable: a new
answer to Pauli’s question, with potential application to quantum cosmology. Int.
J. Theoret. Phys. 47(3), 814–831 (2008)
9. Oxtoby, J.C.: Measure and Category: A Survey of the Analogies Between Topo-
logical and Measure Spaces. Springer, Heidelberg (1980)
10. Papadimitriou, C.: Computational Complexity. Addison Welsey, Reading (1994)
11. Wester, M.E., et al.: Atomic weights of the elements 2011 (IUPAC Technical
Report). Pure Appl. Chem. 85(5), 1047–1078 (2013)

How Much for an Interval? a Set? a Twin Set?
a p-Box? A Kaucher Interval?
Towards an Economics-Motivated Approach
to Decision Making Under Uncertainty
Joe Lorkowski and Vladik Kreinovich(B)
University of Texas at El Paso, El Paso, TX 79968, USA
lorkowski@computer.org, vladik@utep.edu
Abstract. A natural idea of decision making under uncertainty is to
assign a fair price to diﬀerent alternatives, and then to use these fair
prices to select the best alternative. In this paper, we show how to assign
a fair price under diﬀerent types of uncertainty.
Keywords: Decision making · Interval uncertainty · Set uncertainty ·
p-box
1
Decision Making Under Uncertainty: Formulation
of the Problem
In many practical situations, we have several alternatives, and we need to select
one of these alternatives. For example:
– a person saving for retirement needs to ﬁnd the best way to invest money;
– a company needs to select a location for its new plant;
– a designer must select one of several possible designs for a new airplane;
– a medical doctor needs to select a treatment for a patient, etc.
Decision making is the easiest if we know the exact consequences of selecting
each alternative. Often, however, we only have an incomplete information about
consequences of diﬀerent alternative, and we need to select an alternative under
this uncertainty.
Traditional decision theory (see, e.g., [8,12]) assumes that for each alternative
a, we know the probability pi(a) of diﬀerent outcomes i. It can be proven that
preferences of a rational decision maker can be described by utilities ui so that
an alternative a is better if its expected utility u(a)
def
= 
i
pi(a) · ui is larger.
Often, we do not know the probabilities pi(a). As a result, we do not know
the exact value of the gain u corresponding to each alternative. How can we then
make a decision?
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 66–76, 2016.
DOI: 10.1007/978-3-319-31769-4 6

How Much for an Interval? a Set? a Twin Set? a p-Box? A Kaucher Interval?
67
For the case when we only know the interval [u, u] containing the actual
(unknown) value of the gain u, a possible solution was proposed in the 1950 s by
a future Nobelist L. Hurwicz [5,8]: we should select an alternative that maximizes
the value αH · u(a) + (1 −αH) · u(a). Here, the parameter αH ∈[0, 1] described
the optimism level of a decision maker:
• αH = 1 means optimism;
• αH = 0 means pessimism;
• 0 < αH < 1 combines optimism and pessimism.
Hurwicz’s approach is widely used in decision making, but it is largely a
heuristic, and it is not clear how to extend it other types of uncertainty. It is
therefore desirable to develop more theoretically justiﬁed recommendations for
decision making under uncertainty, recommendations that would be applicable
to diﬀerent types of uncertainty.
In this paper, we propose such recommendations by explaining how to assign
a fair price to each alternative, so that we can select between several alternatives
by comparing their fair prices.
The structure of this paper is as follows: in Sect. 2, we recall how to describe
diﬀerent types of uncertainty; in Sect. 3, we describe the fair price approach; in
the following sections, we show how the fair price approach can be applied to
diﬀerent types of uncertainty.
Comment. Our result for the case of interval uncertainty has been previously
described in [9]; other results are new.
2
How to Describe Uncertainty
When we have a full information about a situation, then we can express our
desirability of each possible alternative by declaring a price that we are willing
to pay for this alternative. Once these prices are determined, we simply select
the alternative for which the corresponding price is the highest. In this full
information case, we know the exact gain u of selecting each alternative.
In practice, we usually only have partial information about the gain u: based
on the available information, there are several possible values of the gain u. In
other words, instead of the exact gain u, we only know a set S of possible values
of the gain.
We usually know lower and bounds for this set, so this set is bounded. It is
also reasonable to assume that the set S is closed: indeed, if we have a sequence
of possible values un ∈S that converges to a number u0, then, no matter how
accurately we measure the gain, we can never distinguish between the limit value
u0 and a suﬃciently close value un. Thus, we will never be able to conclude that
the limit value u0 is not possible – and thus, it is reasonable to consider it
possible, i.e., to include the limit point u0 into the set S of possible values.
In many practical situations, if two gain values u < u′ are possible, then all
intermediate values u′′ ∈(u, u′) are possible as well. In this case, the bounded
closed set S is simply an interval [u, u].

68
J. Lorkowski and V. Kreinovich
However, sometimes, some intermediate numbers u′′ cannot be possible values
of the gain. For example, if we buy an obscure lottery ticket for a simple prize-or-
no-prize lottery from a remote country, we either get the prize or lose the money.
In this case, the set of possible values of the gain consists of two values. To account
for such situations, we need to consider general bounded closed sets.
In addition to knowing which gain values are possible, we may also have an
information about which of these values are more probable and which values are
less probable. Sometimes, this information has a qualitative nature, in the sense
that, in addition to the set S of possible gain values, we also know a (closed)
subset s ⊆S of values which are more probable (so that all the values from the
diﬀerence S −s are less probable). In many cases, the set s also contains all its
intermediate values, so it is an interval; an important particular case is when
this interval s consists of a single point. In other cases, the set s may be diﬀerent
from an interval.
Often, we have a quantitative information about the probability (frequency)
of diﬀerent values u ∈S. A universal way to describe a probability distribution
on the real line is to describe its cumulative distribution function (cdf) F(u)
def
=
Prob(U ≤u). In the ideal case, we know the exact cdf F(u). In practice, we usually
only know the values of the cdf with uncertainty. Typically, for every u, we may
only know the bounds F(u) and F(u) on the actual (unknown) values F(u). The
corresponding interval-valued function [F(u), F(u)] is known as a p-box [2,3].
All this classiﬁcation relates to the usual passive uncertainty, uncertainty
over which we have no control. Sometimes, however, we have active uncertainty.
As an example, let us consider two situations in which we need to minimize the
amount of energy E used to heat the building. For simplicity, let us assume that
cooling by 1 degree requires 1 unit of energy.
In the ﬁrst situation, we simply know the interval [E, E] that contains the
actual (unknown) value of the energy E: for example, we know that E ∈[20, 25]
(and we do not control this energy). In the second situation, we know that the
outside temperature is between 50 F and 55 F, and we want to maintain the
temperature 75 F. In this case, we also conclude that E ∈[20, 25], but this time,
we ourselves (or, alternatively, the heating system programmed by us) set up
the appropriate amount of energy.
The distinction between the usual (passive) uncertainty and a diﬀerent
(active) type of uncertainty can be captured by considering improper intervals
ﬁrst introduced by Kaucher, i.e., intervals [u, u] in which we may have u > u see,
e.g., [7,13]. For example, in terms of these Kaucher intervals, our ﬁrst (passive)
situation is described by the interval [15, 20], while the second (active) situation
is described by an improper interval [20, 15].
In line with this classiﬁcation of diﬀerent types of uncertainty, in the following
text, we will ﬁrst consider the simplest (interval) uncertainty, then the general
set-valued uncertainty, then uncertainty described by a pair of embedded sets (in
particular, by a pair of embedded intervals). After that, we consider situations
with known probability distribution, situations with a known p-box, and ﬁnally,
situations described by Kaucher intervals.

How Much for an Interval? a Set? a Twin Set? a p-Box? A Kaucher Interval?
69
3
Fair Price Approach: Main Idea
When we have full information, we can express our desirability of each possible
situation by declaring a price that we are willing to pay to get involved in this
situation. To make decisions under uncertainty, it is therefore desirable to assign
a fair price to each uncertain situation: e.g., to assign a fair price to each interval
and/or to each set.
There are reasonable restrictions on the function that assigns the fair price to
each type of uncertainty. First, the fair price should be conservative: if we know
that the gain is always larger than or equal to u, then the fair price corresponding
to this situation should also be greater than or equal to u. Similarly, if we
know that the gain is always smaller than or equal to u, then the fair price
corresponding to this situation should also be smaller than or equal to u.
Another natural property is monotonicity: if one alternative is clearly better
than the other, then its fair price should be higher (or at least not lower).
Finally, the fair price should be additive in the following sense. Let us consider
the situation when we have two consequent independent decisions. In this case,
we can either consider two decision processes separately, or we can consider a
single decision process in which we select a pair of alternatives:
– the 1st alternative corresponding to the 1st decision, and
– the 2nd alternative corresponding to the 2nd decision.
If we are willing to pay the amount u to participate in the ﬁrst process, and we
are willing to pay the amount v to participate in the second decision process, then
it is reasonable to require that we should be willing to pay u + v to participate
in both decision processes.
On the examples of the above-mentioned types of uncertainty, let us describe
the formulas for the fair price that can be derived from these requirements.
4
Case of Interval Uncertainty
We want to assign, to each interval [u, u], a number P([u, u]) describing the fair
price of this interval. Conservativeness means that the fair price P([u, u]) should
be larger than or equal to u and smaller than or equal to u, i.e., that the fair
price of an interval should be located in this interval:
P([u, u]) ∈[u, u].
Let us now apply monotonicity. Suppose that we keep the lower endpoint u
intact but increase the upper bound. This means that we keep all the previous
possibilities, but we also add new possibilities, with a higher gain. In other words,
we are improving the situation. In this case, it is reasonable to require that after
this addition, the fair price should either increase or remain the same, but it
should deﬁnitely not decrease:
if u = v and u < v then P([u, u]) ≤P([v, v]).

70
J. Lorkowski and V. Kreinovich
Similarly, if we dismiss some low-gain alternatives, this should increase (or at
least not decrease) the fair price:
if u < v and u = v then P([u, u]) ≤P([v, v]).
Finally, let us apply additivity. In the case of interval uncertainty, about the
gain u from the ﬁrst alternative, we only know that this (unknown) gain is in
[u, u]. Similarly, about the gain v from the second alternative, we only know that
this gain belongs to the interval [v, v].
The overall gain u + v can thus take any value from the interval
[u, u] + [v, v]
def
= {u + v : u ∈[u, u], v ∈[v, v]}.
It is easy to check that (see, e.g., [6,10]):
[u, u] + [v, v] = [u + v, u + v].
Thus, for the case of interval uncertainty, the additivity requirement about the
fair prices takes the form
P([u + v, u + v]) = P([u, u]) + P([v, v]).
So, we arrive at the following deﬁnition:
Deﬁnition 1. By a fair price under interval uncertainty, we mean a function
P([u, u]) for which:
• u ≤P([u, u]) ≤u for all u and u (conservativeness);
• if u = v and u < v, then P([u, u]) ≤P([v, v]) (monotonicity);
• (additivity) for all u, u, v, and v, we have
P([u + v, u + v]) = P([u, u]) + P([v, v]).
Proposition 1. [9] Each fair price under interval uncertainty has the form
P([u, u]) = αH · u + (1 −αH) · u for some αH ∈[0, 1].
Comment. We thus get a new justiﬁcation of Hurwicz optimism-pessimism
criterion.
Proof.
1◦. Due to monotonicity, P([u, u]) = u.
2◦. Also, due to monotonicity, αH
def
= P([0, 1]) ∈[0, 1].
3◦. For [0, 1] = [0, 1/n] + . . . + [0, 1/n] (n times), additivity implies
αH = n · P([0, 1/n]), so P([0, 1/n]) = αH · (1/n).
4◦. For [0, m/n] = [0, 1/n] + . . . + [0, 1/n] (m times), additivity implies
P([0, m/n]) = αH · (m/n).

How Much for an Interval? a Set? a Twin Set? a p-Box? A Kaucher Interval?
71
5◦. For each real number r, for each n, there is an m such that m/n ≤r ≤
(m + 1)/n. Monotonicity implies
αH · (m/n) = P([0, m/n]) ≤P([0, r]) ≤P([0, (m + 1)/n]) = αH · ((m + 1)/n).
When n →∞, αH · (m/n) →αH · r and αH · ((m + 1)/n) →αH · r, hence
P([0, r]) = αH · r.
6◦. For [u, u] = [u, u] + [0, u −u], additivity implies P([u, u]) = u + αH · (u −u).
The proposition is proven.
5
Case of Set-Valued Uncertainty
Intervals are a speciﬁc case of bounded closed sets. We already know how to
assign fair price to intervals. So, we arrive at the following deﬁnition.
Deﬁnition 2. By a fair price under set-valued uncertainty, we mean a function
P that assigns, to every bounded closed set S, a real number P(S), for which:
• P([u, u]) = αH · u + (1 −αH) · u (conservativeness);
• P(S + S′) = P(S) + P(S′), where S + S′ def
= {s + s′ : s ∈S, s′ ∈S′}
(additivity).
Proposition 2. Each fair price under set uncertainty has the form P(S) =
αH · sup S + (1 −αH) · inf S.
Proof. It is easy to check that each bounded closed set S contains its inﬁmum
S
def
= inf S and supremum S
def
= sup S: {S, S} ⊆S ⊆[S, S]. Thus,
[2S, 2S] = {S, S} + [S, S] ⊆S + [S, S] ⊆[S, S] + [S, S] = [2S, 2S].
So, S + [S, S] = [2S, 2S]. By additivity, we conclude that P(S) + P([S, S]) =
P([2S, 2S]). Due to conservativeness, we know the fair prices P([S, S]) and
P([2S, 2S]). Thus, we can conclude that
P(S) = P([2S, 2S])−P([S, S]) = (αH·(2S)+(1−αH)·(2S))−(αH·S+(1−αH)·S),
hence indeed P(S) = αH · S + (1 −αH) · S. The proposition is proven.
6
Case of Embedded Sets
In addition to a set S of possible values of the gain u, we may also know a
subset s ⊆S of more probable values u. To describe a fair price assigned to such
a pair (S, s), let us start with the simplest case when the original set S is an
interval S = [u, u], and the subset s is a single “most probable”value u0 within
this interval. Such pairs are known as triples; see, e.g., [1] and references therein.
For triples, addition is deﬁned component-wise:
([u, u], u0) + ([v, v], v0) = ([u + v, u + v], u0 + v0).
Thus, the additivity requirement about the fair prices takes the form
P([u + v, u + v], u0 + v0) = P([u, u], u0) + P([v, v], v0).

72
J. Lorkowski and V. Kreinovich
Deﬁnition 3. By a fair price under triple uncertainty, we mean a function
P([u, u], u0) for which:
• u ≤P([u, u], u0) ≤u for all u ≤u ≤u (conservativeness);
• if u ≤v, u0 ≤v0, and u ≤v, then P([u, u], u0) ≤P([v, v], v0)
(monotonicity);
• (additivity) for all u, u, u0 v, v, and v0, we have
P([u + v, u + v], u0 + v0) = P([u, u], u0) + P([v, v], v0).
Proposition 3. Each fair price under triple uncertainty has the form
P([u, u], u0) = αL · u + (1 −αL −αU) · u0 + αU · u, where αL, αU ∈[0, 1].
Proof. In general, we have
([u, u], u0) = ([u0, u0], u0) + ([0, u −u], 0) + ([u −u, 0], 0).
So, due to additivity:
P([u, u], u0) = P([u0, u0], u0) + P([0, u −u0], 0) + P([u −u0, 0], 0).
Due to conservativeness, P([u0, u0], u0) = u0.
Similarly to the interval case, we can prove that P([0, r], 0) = αU · r for some
αU ∈[0, 1], and that P([r, 0], 0) = αL · r for some αL ∈[0, 1]. Thus,
P([u, u], u0) = αL · u + (1 −αL −αU) · u0 + αU · u.
The proposition is proven.
The next simplest case is when both sets S and s ⊆S are intervals, i.e., when,
inside the interval S = [u, u], instead of a “most probable” value u0, we have
a “most probable” subinterval [m, m] ⊆[u, u]. The resulting pair of intervals is
known as a “twin interval” (see, e.g., [4,11]).
For such twin intervals, addition is deﬁned component-wise:
([u, u], [m, m]) + ([v, v], [n, n]) = ([u + v, u + v], [m + n, m + n]).
Thus, the additivity requirement about the fair prices takes the form
P([u + v, u + v], [m + n, m + n]) = P([u, u], [m, m]) + P([v, v], [n, n]).
Deﬁnition 4. By a fair price under twin uncertainty, we mean a function
P([u, u], [m, m]) for which:
• u ≤P([u, u], [m, m]) ≤u for all u ≤m ≤m ≤u (conservativeness);
• if u ≤v, m ≤n, m ≤n, and u ≤v, then P([u, u], [m, m]) ≤P([v, v], [n, n])
(monotonicity);
• for all u ≤m ≤m ≤u and v ≤n ≤n ≤v, we have additivity:
P([u + v, u + v], [m + n, m + m]) = P([u, u], [m, m]) + P([v, v], [n, n]).

How Much for an Interval? a Set? a Twin Set? a p-Box? A Kaucher Interval?
73
Proposition 4. Each fair price under twin uncertainty has the following form,
for some αL, αu, αU ∈[0, 1]:
P([u, u], [m, m]) = m + αu · (m −m) + αU · (u −m) + αL · (u −m).
Proof. In general, we have
([u, u], [m, m]) = ([m, m], [m, m]) + ([0, m −m], [0, m −m])+
([0, u −m], [0, 0]) + ([u −m, 0], [0, 0)].
So, due to additivity:
P([u, u], [m, m]) = P([m, m], [m, m]) + P([0, m −m], [0, m −m])+
P([0, u −m], [0, 0]) + P([u −m, 0], [0, 0)].
Due to conservativeness, P([m, m], [m, m]) = m. Similarly to the interval case,
we can prove that:
• P([0, r], [0, r]) = αu · r for some αu ∈[0, 1],
• P([0, r], [0, 0]) = αU · r for some αU ∈[0, 1];
• P([r, 0], [0, 0]) = αL · r for some αL ∈[0, 1].
Thus,
P([u, u], [m, m]) = m + αu · (m −m) + αU · (u −m) + αL · (u −m).
The proposition is proven.
Finally, let us consider the general case.
Deﬁnition 5. By a fair price under embedded-set uncertainty, we mean a func-
tion P that assigns, to every pair of bounded closed sets (S, s) with s ⊆S, a real
number P(S, s), for which:
• P([u, u], [m, m]) = m + αu · (m −m) + αU · (U −m) + αL · (u −m)
(conservativeness);
• P(S + S′, s + s′) = P(S, s) + P(S′, s′) (additivity).
Proposition 5. Each fair price under embedded-set uncertainty has the form
P(S, s) = inf s + αu · (sup s −inf s) + αU · (sup S −sup s) + αL · (inf S −inf s).
Proof. Similarly to the proof of Proposition 2, we can conclude that
(S, s) + ([inf S, sup S], [inf s, sup s]) = ([2 · inf S, 2 · sup S], [2 · inf s, 2 · sup s]).
By additivity, we conclude that
P(S, s) + P([inf S, sup S], [inf s, sup s]) =

74
J. Lorkowski and V. Kreinovich
P([2 · inf S, 2 · sup S], [2 · inf s, 2 · sup s]),
hence
P(S, s) = P([2 · inf S, · sup S], [2 · inf s, 2 · sup s])−
P([inf S, sup S], [inf s, sup s]).
Due to conservativeness, we know the fair prices
P([2 · inf S, 2 · sup S], [2 · inf s, 2 · sup s]) and P([inf S, sup S], [inf s, sup s]).
Subtracting these expressions, we get the desired formula for P(S, s). The propo-
sition is proven.
7
Cases of Probabilistic and p-Box Uncertainty
Suppose that for some ﬁnancial instrument, we know the corresponding proba-
bility distribution F(u) on the set of possible gains u. What is the fair price P
for this instrument?
Due to additivity, the fair price for n copies of this instrument is n · P.
According to the Large Numbers Theorem, for large n, the average gain tends
to the mean value μ =

u dF(u).
Thus, the fair price for n copies of the instrument is close to n·μ: n·P ≈n·μ.
The larger n, the closer the averages. So, in the limit, we get P = μ.
So, the fair price under probabilistic uncertainty is equal to the average gain
μ =

u dF(u).
Let us now consider the case of a p-box [F(u), F(u)]. For diﬀerent functions
F(u) ∈[F(u), F(u)], values of the mean μ form an interval

μ, μ

, where μ =

u dF(u) and μ =

u dF(u). Thus, the price of a p-box is equal to the price of
an interval

μ, μ

.
We already know that the fair price of this interval is equal to
αH · μ + (1 −αH) · μ.
Thus, we conclude that the fair price of a p-box [F(u), F(u)] is αH·μ+(1−αH)·μ,
where μ =

u dF(u) and μ =

u dF(u).
8
Case of Kaucher (Improper) Intervals
For Kaucher intervals, addition is also deﬁned component-wise; in particular, for
all u < u, we have
[u, u] + [u, u] = [u + u, u + u].
Thus, additivity implies that
P([u, u]) + P([u, u]) = P([u + u, u + u]).
We know that P([u, u]) = αH · u + (1 −αH) · u and P([u + u, u + u]) = u + u.
Hence:
P([u, u]) = (u + u) −(αH · u + (1 −αH) · u).
Thus, the fair price P([u, u]) of an improper interval [u, u], with u > u, is
equal to P([u, u]) = αH · u + (1 −αH) · u.

How Much for an Interval? a Set? a Twin Set? a p-Box? A Kaucher Interval?
75
9
Summary and Conclusions
In this paper, for diﬀerent types of uncertainty, we derive the formulas for the
fair prices under reasonable conditions of conservativeness, monotonicity, and
additivity.
In the simplest case of interval uncertainty, when we only know the interval
[u, u] of possible values of the gain u, the fair price is equal to
P([u, u]) = αH · u + (1 −αH) · u,
for some parameter αH ∈[0, 1]. Thus, the fair price approach provides a justi-
ﬁcation for the formula originally proposed by a Nobelist L. Hurwicz, in which
αH describes the decision maker’s optimism degree: αH = 1 corresponds to pure
optimism, αH = 0 to pure pessimism, and intermediate values of αH correspond
to a realistic approach that takes into account both best-case (optimistic) and
worst-case (pessimistic) scenarios.
In a more general situation, when the set S of possible values of the gain u
is not necessarily an interval, the fair price is equal to
P(S) = αH · sup S + (1 −αH) · inf(S).
If, in addition to the set S of possible values of the gain u, we also know a
subset s ⊆S of “most probable” gain values, then the fair price takes the form
P(S, s) = inf s + αu · (sup s −inf s) + αU · (sup S −sup s) + αL · (inf S −inf s),
for some values αu, αL, and αU from the interval [0, 1]. In particular, when both
sets S and s are intervals, i.e., when S = [u, u] and s = [m, m], the fair price
takes the form
P([u, u], [m, m]) = m + αu · (m −m) + αU · (u −m) + αL · (u −m).
When the interval s consists of a single value u0, this formula turns into
P([u, u], u0) = αL · u + (1 −αL −αU) · u0 + αU · u.
When, in addition to the set S, we also know the cumulative distributive
function (cdf) F(u) that describes the probability distribution of diﬀerent pos-
sible values u, then the fair price is equal to the expected value of the gain
P(F) =

u dF(u).
In situations when for each u, we only know the interval [F(u), F(u)] of possible
values of the cdf F(u), then the fair price is equal to
P([F, F]) = αH ·

u dF(u) + (1 −αH) ·

u dF(u).
Finally, when uncertainty is described by an improper interval [u, u] with u > u,
the fair price is equal to
P([u, u]) = αH · u + (1 −αH) · u.

76
J. Lorkowski and V. Kreinovich
Acknowledgments. This work was supported in part by the National Science Foun-
dation grants HRD-0734825 and HRD-1242122 (Cyber-ShARE Center of Excellence)
and DUE-0926721, by Grant 1 T36 GM078000-01 and 1R43TR000173-01 from the
National Institutes of Health, and by grant N62909-12-1–7039 from the Oﬃce of Naval
Research.
The authors are thankful to all the participants of the 16th International Sym-
posium on Scientiﬁc Computing, Computer Arithmetic, and Validated Numerics
SCAN’2014 (W¨urzburg, German, September 21–26, 2014) for their interest, and to
the anonymous referees for valuable suggestions.
References
1. Cole, A.J., Morrison, R.: Triplex: a system for interval arithmetic. Softw. Pract.
Experience 12(4), 341–350 (1982)
2. Ferson, S.: Risk Assessment with Uncertainty Numbers: RiskCalc. CRC Press,
Boca Raton, Florida (2002)
3. Ferson, S., Kreinovich, V., Oberkampf, W., Ginzburg, L.: Experimental Uncer-
tainty Estimation and Statistics for Data Having Interval Uncertainty, Sandia
National Laboratories, Report SAND2007-0939 (2007)
4. Gardeﬁes, E., Trepat, A., Janer, J.M.: SIGLA-PL/1: development and applications.
In: Nickel, K.L.E. (ed.) Interval Mathematics 1980, pp. 301–315. Academic Press,
New York (1980)
5. Hurwicz, L.: Optimality criteria for decision making under ignorance, Cowles Com-
mission Discussion Paper, Statistics, No. 370 (1951)
6. Jaulin, L., Kieﬀer, M., Didrit, O., Walter, E.: Applied Interval Analysis, with
Examples in Parameter and State Estimation. Robust Control and Robotics.
Springer, London (2001)
7. Kaucher, E.: ¨Uber Eigenschaften und Anwendungsm¨oglichkeiten der erweiterten
lntervallrechnung und des hyperbolische Fastk¨opers ¨uber R. Comput. Suppl. 1,
81–94 (1977)
8. Luce, R.D., Raiﬀa, R.: Games and Decisions: Introduction and Critical Survey.
Dover, New York (1989)
9. McKee, J., Lorkowski, J., Ngamsantivong, T.: Note on fair price under interval
uncertainty. J. Uncertain Syst. 8(3), 186–189 (2014)
10. Moore, R.E., Kearfott, R.B., Cloud, M.J.: Introduction to Interval Analysis. SIAM
Press, Philadelphia, Pennsylviania (2009)
11. Nesterov, V.M.: Interval and twin arithmetics. Reliable Comput. 3(4), 369–380
(1997)
12. Raiﬀa, H.: Decision Analysis: Introductory Lectures on Choices Under Uncertainty.
Mcgraw-Hill, New York (1997)
13. Sainz, M.A., Armengol, J., Calm, R., Herrero, P., Jorba, L., Vehi, J.: Modal Interval
Analysis. Springer, Berlin, Heidelberg, New York (2014)

Sliding Mode Approaches Considering
Uncertainty for Reliable Control
and Computation of Conﬁdence Regions
in State and Parameter Estimation
Luise Senkel(B), Andreas Rauh, and Harald Aschemann
Chair of Mechatronics, University of Rostock, 18059 Rostock, Germany
{Luise.Senkel,Andreas.Rauh,Harald.Aschemann}@uni-rostock.de
Abstract. Robust control procedures are essential for a reliable func-
tionality of technical applications. Therefore, ﬁrstly, the mathematical
description of the system and, secondly, bounded as well as stochastic dis-
turbances play a major role in control engineering. Bounded uncertainty
occurs due to lack of knowledge about system parameters, manufacturing
tolerances and measurement inaccuracy. Stochastic disturbances, namely
process and measurement noise, play further a very important role in
system dynamics. Both classes of uncertainty are considered in the pre-
sented control and estimation purposes by using interval arithmetics,
where the estimator is necessary to reconstruct non-measurable system
states. Interval representations of uncertain variables provide the pos-
sibility to stabilize dynamic (nonlinear) systems in a robust way. This
is necessary because parameters and measured data are typically only
known within given tolerance bounds. Therefore, this paper combines
interval arithmetics with the advantages of sliding mode approaches for
control and estimation of states and parameters taking into account also
stochastic disturbances. The eﬃciency of these approaches is shown in
terms of an application describing the longitudinal dynamics of a vehicle.
Keywords: Sliding
mode
techniques
·
Uncertainty
·
Interval
arithmetics
1
Introduction
A challenging task in control theory is to ﬁnd a unique concept that can be
applied to a large number of systems, that provides suﬃciently accurate results,
stabilizes the system dynamics in a robust way, and can cope with uncertainty
(unknown parameters) as well as random eﬀects (e.g. non-modeled friction, mea-
surement inaccuracies). Unfortunately, the applicability of common numerical
calculations is limited in terms of rounding errors, truncation errors, and input
errors [4]. These problems can be overcome by calculations with intervals describ-
ing the range of a variable instead of its scalar value. Then, statements about the
dynamics of a system described by parameter ranges can be proposed without
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 77–96, 2016.
DOI: 10.1007/978-3-319-31769-4 7

78
L. Senkel et al.
the need of statistical methods (e.g. Monte-Carlo methods) [3]. Nevertheless,
interval arithmetic has to cope with the problem of overestimation due to the
dependency problem and the wrapping eﬀect [5]. In combination with robust
control and estimation purposes, a reliable possibility to deal with uncertainty
and to quantify their worst-case inﬂuence in the system dynamics can be pro-
vided while robustness and stability of the error dynamics are guaranteed.
Sliding mode techniques, so-called variable-structure approaches, are well
known for their robust performance and the possibility to handle bounded
uncertainties and disturbances in a more eﬃcient way than other approaches
as for example state-of-the-art back-stepping procedures [12]. Commonly, the
user deﬁnes a manifold (called sliding surface) that is assumed to be reached by
the system in a stable manner. Once this surface is reached, the system will not
diverge anymore and remains in the near surrounding area of this stable mode.
However, a large problem in sliding mode approaches is still the so-called chat-
tering that occurs inevitably due to noise that aﬀects the switching function due
to discretization in computer implementations [12]. In recent years, second-order
and other higher-order sliding modes have been developed to reduce chattering.
Nevertheless, common sliding modes are limited in their applicability due to
quite restrictive matching conditions [7]. For that reason, this paper presents
sliding mode techniques that make use of intervals to reduce chattering by tak-
ing into account bounded and stochastic uncertainty of parameters and states.
In general, the applicability of these approaches is not limited to a special class
of systems [7]. Therefore, the Itˆo diﬀerential operator is applied for an online cal-
culation of the variable-structure gain (called switching amplitude) in each time
step despite stochastic disturbances instead of the usual oﬄine computation.
This paper is structured as follows: Sect. 2 describes a scenario for which the
sliding mode approaches, ﬁrstly a control procedure (Sect. 3) and secondly an
observer for estimation of parameters and states (Sect. 4) has been applied. The
results are shown in Sect. 5 before this paper is concluded and ﬁnalized with an
outlook on further work in Sect. 6.
2
Application Scenario
The control and estimation procedures presented in the following are imple-
mented on a laboratory test rig available at the Chair of Mechatronics at the
University of Rostock which can be interpreted as the longitudinal dynamics
of a vehicle, cf. Fig. 1. The following assumptions have been made: all mass
moments of inertia of the motor, the brake as well as of the drive and load
side shafts are summarized into one mass moment of inertia J. Moreover,
a velocity-proportional friction coeﬃcient d (friction occurs inevitably in all bear-
ings, between the deﬂector rolls and the toothed belt) and the requirement that
static friction is compensated by an underlying motor control are considered.
Then, the system model is described by f(x (t) , p, u(t)) = ˙x(t) =
 ˙x1(t), ˙x2(t)T
with two system states x1(t) = ϕM(t) (angle on the drive side) and x2(t) =
˙x1(t) = ˙ϕM(t) = ωM(t) (corresp. angular velocity) according to

Sliding Mode Approaches Considering Uncertainty for Reliable Control
79
Fig. 1. Schematic visualization and photo of the available test rig.
˙x(t) = A · x(t) + b · u(t) =
0 1
0 α
 x1(t)
x2(t)

+
0
β

u(t) , y(t) = x1(t) .
(1)
In Eq. (1), two parameters α = −d
J ∈[α] and β = 1
J ∈[β] are included. Both
parameters inﬂuence the system dynamics in a signiﬁcant way due to the mul-
tiplicative coupling with the time-varying system states and the input (motor
control signal). Because of this and due to the fact that both parameters are
a-priori unknown, it is necessary to determine these and the system states simul-
taneously. Note that only the ﬁrst state y = x1 is measurable. However, for the
control strategy in the next section, knowledge about both states is necessary.
3
Sliding Mode Techniques for Control Purposes
In recent years, sliding mode approaches for control purposes have reached wide
attention due to their inherent robustness despite uncertainties and disturbances.
Both can be compensated in terms of the variable structure part in order to
stabilize the system’s tracking error eﬃciently. In the following, the classical
approach is described which is then extended to an interval-based formulation
and stochastic processes.
3.1
Classical Control Approach
The classical sliding mode controller for single-input single-output systems is
based on a Lyapunov function [9] with V (t) = 1
2s2(t) and its time derivative
˙V (t) = s(t) · ˙s(t) with the sliding variable chosen as a Hurwitz polynomial of
order n −1 (in the following, all time arguments are omitted)
s = κ0 · (x(0) −x(0)
d ) + κ1 · (x(1) −x(1)
d ) + ... + (x(n−1) −x(n−1)
d
) .
(2)
Here, x(0) = x1 denotes the ﬁrst state of a system which is given in nonlinear
controller canonical form, and x(i) with i = {1, ..., n−1} the derivatives up to the
order n −1. The terms xd, .., x(n−1)
d
denote the desired trajectory and its time
derivatives, resp. The condition for asymptotic stability ˙V < 0 for all s ̸= 0 can
be fulﬁlled by using the switching amplitude η and the deﬁnition of the absolute
value of the sliding variable |s| = s · sign(s) according to

80
L. Senkel et al.
˙V
!
≤−η · |s| ⇒s ˙s
!
≤−η · s · sign(s) ⇒˙s + η · sign(s)
!
≤−ϵ · sign(s)
⇒˙s + (η + ϵ)
  	
:=˜η>0
·sign(s)
!
≤0 .
(3)
In Eq. (3), ˜η can be interpreted as a desired convergence rate. In dependency of
the positive or negative sign of the sliding variable, the signum function yields
sign(s) =
⎧
⎨
⎩
1,
if
s > 0
−1,
if
s < 0
0,
else .
(4)
For the described application scenario, the sliding surface has the form s =
κ0 ·(x1 −x1,d)+(x2 −x2,d). The control law results after inserting the 2nd state
equation of (1) into the time derivative ˙s = κ0( ˙x1 −˙x1,d) + κ1( ˙x2 −˙x2,d) in
u = −κ0(x2 −x2,d) −(αx2 −˙x2,d) −˜η · sign(s)
β
.
(5)
In Eq. (5), κ0 > 0 and ˜η > 0 have to be chosen such that the condition for
asymptotic stability ˙V < 0 holds. The disadvantage of this approach is that the
results are often too conservative, especially in case of uncertain parameters so
that unnecessary large switching amplitudes deﬁned by the user are necessary.
Moreover, noise and actuator wear may occur [9]. Therefore, this approach is
extended in the following such that uncertainty as well as stochastic processes
are taken into consideration. Moreover, the scalar switching amplitude ˜η is gen-
eralized to a time-varying vector of the same dimension as the state vector x.
3.2
Extension to Uncertain and Stochastic Processes
In the following, an interval-based sliding mode controller (ISMC) is described
[10]. Consider an uncertain system of order n described by
dx = f(x, p, u)dt + Gp dwp and y = CC (p) · x + Gm dwm
(6)
for all p ∈[p] and x ∈[x] with the standard Brownian motion of the process
dwp ∈Rrp and of the measurement dwm ∈Rrm as well as the corresponding
matrices of standard deviations Gp ∈Rn×rp and Gm ∈Rny×rm, and the output
matrix CC. In the linear case1, it is assumed, that the function f(x, p, u) is
given in state-space notation with the system and input matrices AC and BC
according to f(x, p, u) = ACx + BCu for p ∈[p]. Requiring the existence
of a desired trajectory and its derivatives for all states, the vector of desired
trajectories is denoted by xd. Including intervals for system parameters, the
following control law2 can be deﬁned according to
1 Linearity in the state vector x which is assumed in this application scenario.
2 Here, only a system with a single input dim(u) = 1 is described. The approach
is also applicable for a system with more than one input. In this case, the vector
of switching amplitudes becomes a matrix. This also holds for the linear controller
gain K.

Sliding Mode Approaches Considering Uncertainty for Reliable Control
81
u = uF F −kT · x + ηT · sign(x −xd) .
(7)
In Eq. (7), a (static or a dynamic) feedforward control term uF F is included. An
underlying state feedback term is denoted by kT · x, where the controller gain
k is calculated by e.g. pole assignment for one parameter vector p ∈[p]. As
already mentioned, the scalar switching amplitude from the classical approach
becomes a vector η with dim(η) =dim(x) = n×1 (system order n). To calculate
the switching amplitude vector, the Itˆo diﬀerential operator [6] is applied while
further taking into consideration the uncertain system
[f] = ACx + BCu .
(8)
Note, that the terms AC, BC are interval evaluations according to AC ∈AC :=
AC(x, [p]) and BC ∈BC := BC(x, [p]). Consequently, the Itˆo diﬀerential oper-
ator according to the deﬁnition
L(VC) = ∂VC
∂t +
∂VC
∂˜x
T
· ([f] −˙xd) + 1
2trace

GT
p
∂2VC
∂˜x2 Gp

(9)
is used. It is evaluated taking into account a control error interval [Δxc] in x ∈
[x] = x + [Δxc] to consider also control errors as well as parameter uncertainty
speciﬁed as range bounds in terms of intervals p ∈[p]. A suitable candidate for a
Lyapunov function is given by VC = 1
2 ˜xT PC ˜x, with the deﬁnition of the vector-
valued sliding variable ˜x := x −xd = [x1 −x1,d , x2 −x2,d, ... , xn −xn,d]T . As
it can be seen, there is no explicit time dependency in VC so that ∂VC
∂t = 0. By
employing the condition L(VC)
!< −qT
Cabs (˜x) with a user-deﬁned element-wise
non-negative convergence rate vector qC, the components i ∈{1, ..., n} of the
switching amplitude vector are calculated by
ηi =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
sup

[M]+
i ·

−[ ˙Va,C] −qT
Cabs ([˜x]) −T

+ μ, if [M]+
i < 0
inf

[M]+
i ·

−[ ˙Va,C] −qT
Cabs ([˜x]) −T

−μ, if [M]+
i > 0
0,
else .
(10)
In Eq. (10), the abbreviation [M] := [BC]T PC |[˜x]| is used and the absolute val-
ues are given by abs ([˜x]) =
|[x1] −x1,d| . . . |[xn] −xn,d|
. Moreover, the matrix
of the absolute values |[˜x]| ∈Rn×n with [˜xi] = ([xi] −xi,d) (for all i ∈{1, ..., n})
and the sign function are deﬁned as
|[˜x]| =
⎡
⎢⎢⎢⎣
[˜x1] · sign([˜x1]) [˜x1] · sign([˜x2]) . . . [˜x1] · sign([˜xn])
[˜x2] · sign([˜x1]) [˜x2] · sign([˜x2]) . . . [˜x2] · sign([˜xn])
...
...
...
...
[˜xn] · sign([˜x1]) [˜xn] · sign([˜x2]) . . . [˜xn] · sign([˜xn])
⎤
⎥⎥⎥⎦and
(11)
sign([˜xi]) =
⎧
⎨
⎩
1,
if
inf([˜xi]) > 0
−1,
if
sup([˜xi]) < 0
0,
else .
(12)

82
L. Senkel et al.
Furthermore, [ ˙Va,C] = [˜x]T PC([AC] −[BC]K)[x] + [˜x]T PC[BC]uF F −[˜x]T PC ˙xd
and the trace of the stochastic processes T = 1
2trace

GT
p
∂2VC
∂˜x2 Gp

hold. The left
pseudo inverse3 of the matrix [M] is calculated by [M]+ =

[M]T · [M]
−1·[M]T .
In case that not all system states are measurable, it is necessary to reconstruct
them by an observer (see next section). Then, x in the control law is replaced
by the estimated states ˆx.
For this stability proof, the sign of L(VC) is relevant; if L(VC) < 0 the
system is stable. Note that the boundary L(VC) = 0 of the provable stability
domain is commonly shaped like an ellipsoid whose volume should be as small as
possible to reduce the non-stabilizable area [10]. Currently, the simulation of this
approach works ﬁne also in combination with the observer in Subsects. 4.2 and
4.3 (feedback of the estimated states). In future work, it will be implemented on
the described test rig in experiment and connected with the approach described
in Subsect. 4.4.
4
Sliding Mode Techniques for Estimation Purposes
Due to the fact that measurements in real applications are often limited, knowl-
edge of the non-measurable states is necessary to implement an eﬃcient con-
trol strategy. In the presented case, in addition to state estimation, uncertain
parameters will be identiﬁed simultaneously. Therefore, a classical sliding mode
observer approach is extended such that the real-time capability is ensured.
Often, parameter identiﬁcation procedures are evaluated oﬄine by using mea-
sured data which results in a set of parameters that are assumed to be constant
for the complete identiﬁcation horizon. In the following, it is assumed that para-
meters can change in each time step (index k) within a deﬁned range. Especially
for parameters containing uncertain or random eﬀects as static or sliding fric-
tion, this assumption is reasonable. Therefore, intervals deﬁning suitable range
bounds are considered in Subsects. 4.2, 4.3 and 4.4. Moreover, the presented
extension of a classical observer (Subsect. 4.1) is able to deal with process as
well as measurement noise but still guaranteeing the system’s stability in each
time step.
4.1
A Classical Observer Approach
As it has been shown in [9], assume a dynamic system ˙x = f (x, u) whose state
equations can be written in the form
˙x = f = A · x + B · u + S · w (x, u) ,
y = C · x .
(13)
In (13), x denotes the state vector, A as well as B denote the constant system
and input matrices, and u is a vector-valued control signal. The matrix S ∈Rn×q
includes inﬂuences of a-priori unknown terms on the system dynamics which have
3 For single-input systems, [M] turns out to be a column vector.

Sliding Mode Approaches Considering Uncertainty for Reliable Control
83
to fulﬁll the condition ∥w (x, u)∥≤ρ(u) with a ﬁxed upper bound for the vector
norm ρ(u) [1]. Consequently, the product S · w (x, u) contains all nonlinearities
included in the system model that do not ﬁt in the linear part of the set of state
equations. Moreover, the constant output matrix C links the system states in a
linear way to the output y. Now, a variable-structure observer described by a
set of ordinary diﬀerential equations (ODEs) [1]
˙ˆx = ˆA · ˆx + ˆB · u + ρ(u) · ˜S · ˜e + Hp · (ym −ˆy)
(14)
with the output equation ˆy = ˆCˆx can be deﬁned. In Eq. (14), ρ(u) is a factor
for the variable-structure part which has to be chosen such that the approxi-
mation S · w ≈ρ(u) · ˜S · ˜e holds. The observer gain matrix Hp is used for the
stabilization of the error dynamics of the linear part and is usually determined
by pole assignment. The vector ym includes all measured system outputs. All
remaining terms in (14) denoted by the symbol ˆ. characterize system, input and
output matrices as well as the estimated state vector of the observer parallel
model that replicates the original one. Moreover, the error vector is deﬁned as
˜e =
ST P(x−ˆx)
∥ST P(x−ˆx)∥and accounts for deviations between the true (original) and esti-
mated system states. There, the positive deﬁnite matrix P = PT results from
solving the Lyapunov equation ( ˆA −Hp · ˆC) · P + P · ( ˆA −Hp · ˆC)T + Q = 0
with a weighting matrix Q > 0. Note that the sliding mode observer (as well
as any other observer) is only applicable if the pair

ˆA, ˆC

is observable. As
soon as a change of sign occurs in the term em = ym −ˆy, the term w in (13) is
reproduced approximately by ˜e in the switching part of the observer. According
to the (matching) conditions
S · w ≈ρ(u) · ˜S · ˜e ≈ρ(u) · ˜S · sign (em)
(15)
the matrices S and ˜S have an identical structure. Considering this condition, the
observer diﬀerential equation is adapted to
˙ˆx = ˆA · ˆx + ˆB · u + Hp · em + hs · ˜S · sign (em)
(16)
with the sign function depending component-wise on the diﬀerence em. By this
reformulation of Eq. (16), it is now possible to handle uncertainty by means of the
variable structure term because the model does not only include a locally valid
model anymore. The stabilization of the error dynamics becomes now possible
in spite of nonlinearities in the system representation [9]. In the following, this
extended sliding mode observer will be modiﬁed further such that it is possible
to estimate multiplicatively coupled states and parameters simultaneously by
means of the modiﬁcation
˙ˆx = ˆA · ˆx + ˆB · u


	
ˆf(ˆx,u)
+Hp · em + P+ · ˆCT · Hs · sign (em)
(17)
with ˆA := ˆA (ˆx), ˆB := ˆB (ˆx). As it can be seen, the quite restrictive match-
ing condition (15) is removed. The variable structure gain, called switching

84
L. Senkel et al.
amplitude vector, is a diagonal matrix according to Hs = diag(hs) with
dim(hs) = ny × 1 and has to be deﬁned such that the Lyapunov function can-
didate V = 1
2(x −ˆx)T P(x −ˆx) > 0 is greater than zero and its time derivative
less than zero for asymptotic stability according to ˙V = (x −ˆx)T P( ˙x −˙ˆx) < 0.
In classical sliding mode approaches, the variable-structure gain is deﬁned as
a constant value. In Subsect. 4.2, a possibility is shown that calculates this gain
in each time step to reduce chattering and to avoid actuator saturations. More-
over, uncertainty of parameters and states will be included as well as stochastic
processes. The aim is to estimate states and to identify parameters simultane-
ously by a cascaded observer structure which will be shown in Subsect. 4.3.
4.2
Extended Approach Considering Uncertainty and Stochastic
Processes - Estimation of Point Values
In this section, the dual problem to control tasks — namely state estimation —
is solved by using sliding mode techniques taking into account parameter uncer-
tainty, sensor inaccuracies and noise processes as already described in [7] in
detail. Here, the calculated number of the switching amplitudes is equal to the
number of measurable states in order to guarantee stability and to reach the slid-
ing surface as fast as possible. Moreover, by a direct calculation of the switching
amplitude vector, its values can be adapted automatically in each time step
instead of predeﬁning a suitable constant gain by intelligent guessing.
As for control design, a suitable Lyapunov function candidate is necessary,
which is chosen as VO = 1
2eT · PO · e with the deviation e = x −ˆx between
the true and estimated system states. The matrix PO results from solving the
Lyapunov equation ˜AOPT
O + ˜AT
OPO +Q = 0 with ˜AO = ˆA−Hp ˆC that holds for
one working point (PO = PT
O > 0). Moreover, stochastic disturbances that aﬀect
the system dynamics as process or measurement noise can again be considered
by applying the Itˆo diﬀerential operator
L(VO) = ∂VO
∂t +
∂VO
∂e
T
· [¯f] + 1
2trace

GT ∂2VO
∂e2 G

(18)
with [¯f] = [f]−[ˆ˜f([ˆx], [p], u)] and the interval extension of the system ODEs (8).
The term ˆ˜f(ˆx, [p], u) describes an observer parallel model by
[ˆ˜f(ˆx, [p], u)] := ˆA(ˆx, [p]) · [ˆx] + ˆB(ˆx, [p]) · u + Hp · [em]


	
ˆf(x,[p],u)
+ P+
O ˆCT · Hs · sign(em + [Δym]),
[ˆy] := ˆC · [ˆx] .
(19)
In Eq. (19)4, em ∈[em] = ym −ˆy + [Δym] denotes the component-wise
deﬁned interval measurement error vector and accounts for bounded uncertainty
4 Note that time discretization errors are neglected and the procedure is not limited
to a special class of systems. Here, a SISO system is considered.

Sliding Mode Approaches Considering Uncertainty for Reliable Control
85
that becomes noticeable in deviations between the measured ym and estimated
system outputs ˆy ∈[ˆy] with the measurement error interval [Δym]. Moreover,
ˆA, ˆB as well as ˆC are the interval evaluations of the system, input and output
matrices ˆA(ˆx, [p]), ˆB(ˆx, [p]) and ˆC(ˆx, [p]). The matrix G contains the standard
deviation of both process Gp as well as measurement noise Gm according to
G = [Gp
−HpGm]; the matrix Hp stabilizes the linear part of the observer
denoted by ˆf(ˆx, p, u) for one working point ˆx ∈[ˆx] and p ∈[p]. This observer
gain Hp can be determined by pole assignment, minimizing a quadratic cost func-
tion (both for one special operating point) or by solving linear matrix inequalities
(valid for the whole deﬁned operating range) [10].
Reaching the sliding surface of all estimated states means that the diﬀerence
between the system states themselves and the estimated ones becomes small so
that switching around the sliding surface occurs. To prevent the observer from
unnecessary switchings in regions where the sign of em cannot be evaluated, the
interval error vector [e] = [x]−[ˆx] is introduced with [ˆx] = ˆx+[Δxe] (estimation
error interval [Δxe]). For measurable states, the estimation interval is replaced
by the measurement interval according to the sensor model [Δym] = ˆC · [Δxe] .
Due to the interval extension of e to [e], the sliding surface becomes an area
leading to smaller switching amplitudes Hs.
Applying the condition L(VO)
!< −qT
O |[em]| with a user-deﬁned convergence
rate qO, the expression [e]T PO ·[¯f]+ 1
2trace

GT ∂2VO
∂e2 G

< −qT
O |[em]| leads to
[e]T PO ·

A · [ˆx] + B · u −ˆA · [ˆx] −ˆB · u −Hp · [em]

−
−[e]T PO ·

P+
O ˆCT Hs · sign (em)

+ 1
2trace

GT ∂2VO
∂e2 G

< −qT
O |[em]| .
(20)
Note this inequality needs to be valid for all possible conﬁgurations inside the
speciﬁed intervals for states and parameters. Because of the positive deﬁniteness
of the matrix PO, the term POP+
O = In×n is equal to the identity matrix I
with PO = PT
O. With this simpliﬁcation, the components of the diagonal matrix
Hs = diag(hs) with dim(Hs) = ny × ny can be calculated component-wise
from Eq. (20). Therefore, the term [e]T PO ·

P+
O ˆCT Hs · sign ([em])

= [e]T ·

ˆCT Hs · sign ([em])

= hs,1·[em,1]·sign([em,1])+...+hs,ny ·[em,ny]·sign([em,ny])
can be reformulated using [em,i] · sign([em,i]) = |[em]| (i ∈1, ..., ny) according to
[e]T PO ·

P+
O ˆCT Hs · sign ([em])

= hT
s |[em]| .
(21)
Note that the output matrix ˆC := ˆC is independent of states or parameters in
this case. The absolute value of the interval measurement error |[em]| yields

86
L. Senkel et al.
|[em,i]| =

em,i; em,i
 =
⎧
⎪
⎨
⎪
⎩

−em,i ; −em,i

,
for em,i ≤0 ,

em,i ; em,i

,
for em,i ≥0 ,

0 ; max{|em,i|, |em,i|}

else .
(22)
From Eq. (18) with (21), the switching amplitude vector can be calculated by
hs =

0,
if
[δO] ⊆[em]T [em]
sup

|[em]|+ ·

[ ˙Va,O] + 1
2 · trace

GT ∂2VO
∂e2 G

+ qT
O

, else .
(23)
In Eq. (23), [ ˙Va,O] = [e]T PO·(AC ·[x]+BC ·u−ˆA·[ˆx]−ˆB·u−Hp·[em]) holds with
the interval error vector [e]. To prevent a division by zero, a small interval [δO]
around zero is included in the calculation of the switching amplitude to reduce
on the one hand unnecessary chattering and to reduce the value of the switching
amplitude on the other hand [10]. The interval pseudo inverse in Eq. (23) is
deﬁned as |[em]|+ =

|[em]|T |[em]|
−1
· |[em]|T . A stability proof can be done
by evaluating Eq. (18) in each time step.
4.3
Cascaded Structure of the Observer
In the following, the approach described in the previous section is applied to
the simpliﬁed model for the longitudinal dynamics of a vehicle (see Sect. 2) in
terms of a cascaded structure. To estimate all system states and to identify the
system parameters, two subsystems are necessary due to multiplicative couplings
of states and parameters in the system model. The states are estimated by the
ﬁrst subsystem and serve as virtual measurements for the second subsystem
that identiﬁes the parameters in each time step by using integrator disturbance
models. The physical background of those is, that the parameters are assumed to
be located within speciﬁed intervals in which they are allowed to vary between
two time steps. Figure 2 shows the deﬁnitions of matrices, the models of both
subsystems as well as the corresponding observer parallel models. From the latter
ones, the linear structure of the interval-based sliding mode observer (ISMO) can
be derived by factorization of ˆf (i) for both subsystems i ∈{S1, S2}. Note the
representation by the system matrix ˆA(S2) and the input vector ˆb(S2) of the
second subsystem is evaluated for one operating point and consequently this
also holds for H(S2)
p
and P(S2)
O
. However, the stability proof is still valid because
the variable-structure part stabilizes the error dynamics of the complete system.
4.4
Extended Approach Considering Uncertainty and Stochastic
Processes - Estimation of Conﬁdence Intervals
In the previous subsection, point values for system states and parameters were
estimated such that the system’s stability can be guaranteed. Due to calcula-
tion with intervals, these point values are just one solution of others. Therefore,
the following strategies provide the possibility to estimate all possible solutions

Sliding Mode Approaches Considering Uncertainty for Reliable Control
87
Fig. 2. Structure of the cascaded observer, deﬁnitions of the nominal system models f (i), observer parallel models ˆf (i), (both necessary for
the calculation of the switching amplitude) and the interval-based sliding mode observers ˙ˆx
(i) for the estimation of states and parameters
that both are guaranteed to be located in speciﬁed range bounds by adapting the switching amplitude in each time step, i ∈{S1, S2},
cf. [7].

88
L. Senkel et al.
which results in the calculation of conﬁdence regions (enclosures) for parameters
and states. Note that stability is guaranteed for all solutions summarized by
the conﬁdence regions. The calculation of conﬁdence regions can be realized by
using (1) M¨uller’s theorem, (2) cooperativity, (3) a quasi-linear system repre-
sentation (with real or complex eigenvalues) in combination with a state-space
transformation, and (4) an aﬃne system representation. These four points will be
described in the following and, if possible, applied to the scenario (see Sect. 2) in
simulation. Note that the inﬂuence of time discretization errors can be neglected,
because the considered uncertainty represented by intervals is larger by multiple
orders of magnitudes. Before the four strategies can be implemented, the two
subsystems need to be reformulated such that no direct interval dependencies
in the right-hand side of the ODEs are included. Therefore, the state vector is
extended in the following by integrator disturbance models for these intervals
(in the presented scenario, only measurement intervals occur on the right-hand
sides of the ODEs, where the interval in sign([Δem]) is no problem due to the
deﬁnition of the sign function according to Eq. (12)). This reformulation leads
to the extended subsystems
⎡
⎢⎢⎢⎢⎢⎢⎣
˙ˆx1
˙ˆx2
ˆx3
˙ˆx4
˙ˆz1
[Δ ˙ym]
⎤
⎥⎥⎥⎥⎥⎥⎦

	

˙ˆx(S1)
ext
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝

ˆA(S1), H(S1)
p
01×6

−

H(S1)
p
0


	

H(S1)
p,ext
·
 ˆC(S1) 0


	

ˆ
C(S1)
ext
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠

	

A(S1)
O,ext
⎡
⎢⎢⎢⎢⎢⎢⎣
ˆx1
ˆx2
ˆx3
ˆx4
ˆz1
[Δym]
⎤
⎥⎥⎥⎥⎥⎥⎦

	

ˆx(S1)
ext
+H(S1)
p,ext · C(S1)
ext · x(S1)
ext

	

y(S1)
m,ext
+ b(S1)
ext · u + (P(S1)
O,ext)+ · ˆC(S1)
ext · H(S1)
s
· sign(ym −ˆy + [Δym])

	

[e(S1)
m
]

	

scalar point-value
and
(24)
⎡
⎢⎢⎣
˙ˆx(S2)
[Δ ˙ym,1]
[Δ ˙ym,2]
[Δ ˙ym,3]
⎤
⎥⎥⎦

	

˙ˆx(S2)
ext
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝

ˆA(S2), H(S2)
p
03×8

−

H(S2)
p
03×3


	

H(S2)
p,ext
·
 ˆC(S2) 03×3

	

ˆ
C(S2)
ext
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠

	

A(S2)
O,ext
⎡
⎢⎢⎣
ˆx(S2)
[Δym,1]
[Δym,2]
[Δym,3]
⎤
⎥⎥⎦

	

ˆx(S2)
ext
+
⎡
⎢⎢⎣
b(S2)
0
0
0
⎤
⎥⎥⎦

	

b(S2)
ext
·u
+ H(S2)
p,ext · ˆC(S2)T
ext
· x(S2)
ext

	

y(S2)
m,ext
+(P(S2)
O,ext)+ · ˆC(S2)
ext · H(S2)
s
· sign(ym −ˆy + [Δym])

	

[e(S2)
m
]
.
(25)
with the vector and matrix deﬁnitions x(S1)
ext
= [x1
x2
x3
x4
z1
[Δym]]T ,
b(S1)
ext = [0 0 0 0 0 0]T , C(S1)
ext = [C(S1)
ext
0]T , P(S1)
O,ext = [P(S1)
O
05×1 ; 01×6],
ˆx(S2) = [ˆx2
ˆx3
ˆα
ˆβ
ˆz2]T , ˙ˆx(S2) = [ ˙ˆx2
˙ˆx3
ˆα
˙ˆβ
˙ˆz2]T , and P(S2)
O,ext =
[P(S2)
O
05×3 ; 03×6] (see also Fig. 2).

Sliding Mode Approaches Considering Uncertainty for Reliable Control
89
M¨uller’s Theorem. M¨uller’s Theorem can always be taken into account, even
if the proof of cooperativity of a system fails. In this case, interval enclosures of
a system described by ODEs can be calculated as follows. According to M¨uller
[2], the enclosures of all time-varying system states can be computed, if lower
and upper functions can be found that bound the right-hand side of the system
ODEs ˙x = f(z, p, u) for all uncertain states z ∈[z] and parameters p ∈[p].
These functions are denoted by fχ (lower function) and fψ (upper function).
Then, the bounding system can be deﬁned by the worst-case enclosures
χi ≤xi ≤ψi
(26)
for all i = {1, ..., n} component-wise (system order n). Analogously, the system
equations can be determined according to the diﬀerential inequalities
fχ,i ≤˙xi ≤fψ,i
(27)
for all possible states, parameters and inputs. Then, a system of order 2n has to
be solved to ﬁnd the lower and upper enclosures for all states in a numerical way
by veriﬁed solution algorithms. Therefore, the lower and upper bounds result
from minimization and maximization
fχ,i ≤min{fi(z, p, u)|χj ≤zj ≤ψj with zi ≡χi and pk ≤pk ≤pk} ,
(28)
fψ,i ≥max{fi(z, p, u)|χj ≤zj ≤ψj with zi ≡ψi and pk ≤pk ≤pk}
(29)
for j ∈{1, ..., i −1, i + 1, ..., n} and k ∈{1, ..., np}. For the application example
in Sect. 2, this procedure leads do unstable solutions that are too conservative
and include unphysical combinations due to overestimation.
Enclosures for Cooperative Systems. A given system f(x, p, u) is called
cooperative system if it is monotone concerning its initial values. The domain for
all possible states can be calculated by evaluating the system for all corner points
of intervals containing the initial values. In general, the calculation of conﬁdence
regions by taking into account the cooperativity of a system is a special case of
M¨uller’s theorem. Suﬃcient conditions for a cooperative system are that
– all
states
are
non-negative
for
all
times
according
to
˙xi
=
fi(x1, ..., xi−1, 0, xi+1,...,xnx ) ≥0 (parameters and inputs are not denoted
explicitly),
– all entries of the Jacobian fulﬁll the condition ∂fi
∂xj ≥0 for all i, j ∈{1, ..., n}
(i ̸= j).
If these conditions are fulﬁlled, guaranteed lower and upper bounds can be cal-
culated by evaluating the system ODEs at all corner points instead of using
the whole range of xi as it is done with M¨uller’s theorem. This decreases the
computational eﬀort signiﬁcantly. All corner points are used to ﬁnd the inﬁmum
(lower bound) and supremum (upper bound) of the enclosures in each time step.
For this initial value problem, veriﬁed numerical solution algorithms are used

90
L. Senkel et al.
to avoid errors due to computational accuracy, rounding, and time discretiza-
tion [8,13]. For the given application scenario, enclosures for the system states
and the two system parameters are of interest. Therefore, this algorithm needs to
be implemented for both subsystems. This leads for the ﬁrst subsystem to 26 · 6
and for the second subsystem to 28·8 state equations containing unphysical para-
meter combinations. Unfortunately, overestimation due to this large number of
conﬁgurations leads to unstable solutions for both subsystems.
Quasi-Linear System Representation with State-Space Transformation.
Another possibility to calculate enclosures for states and parameters can be
found by reformulating the system into a quasi-linear representation. In order
to reduce overestimation caused by the dependency problem, the system states
need to be decoupled. This generates a state-space transformation into new coor-
dinates (by using Jordan or Metzler matrices) and allows a recursive evaluation
of the new system equations. Afterwards, a backward transformation into orig-
inal coordinates is necessary. Here, the Jordan matrix is taken into account, in
which the (real or complex) eigenvalues of the system are located on the diag-
onal of the matrix and zeros on all other entries. The algorithm presented in
the following needs to be implemented for both subsystems in order to get the
enclosures of parameters and states. The state-space transformation follows with
ˆxext = V · z and ˙ˆxext = V · ˙z for the single-input single-output system (S1) and
the single-input multi-output system (S2) to
[˙z] = inv(V) · (AO,ext[ˆxext] + bextu + Hp,extym + (PO,ext)+CT
ext · Hssign([em]))
= inv(V) · (AO,extV[z] + bextu + Hp,extym + (PO,ext)+CT
ext · Hssign([em]))
= J[z] + inv(V) · (bextu + Hp,extym + (PO,ext)+CT
ext · Hssign([em])) .
(30)
The relation between the Jordan canonical form and the matrix of eigenvectors
V is given by J = diag(λi) = inv(V) · AO,ext · V where λi are real or complex
eigenvalues according to λi = σi±j·ωi with multiplicity one. Both matrices result
directly from the Matlab command [V, J] = eig(AO,ext) for both subsystems
separately. The state-space transformation is the prerequisite for the algorithm
in Fig. 3 that is based on the Euler method. Due to the fact that the system
matrix of subsystem S2 signiﬁcantly depends on the positive or negative sign of
the time derivative of the input signal ˙u (see Fig. 2), the algorithm needs to be
divided into 4 cases where for S1 only a single case is necessary. This is justiﬁed
by the fact that this system matrix represents an integrator chain state estimator
which provides virtual measurements for subsystem S2. The following algorithm
is executed in each time step, where two subsequent time steps tk (index k) and
tk+1 (index k + 1) are taken into account.
Additionally, in Fig. 3 the terms
γ+ = b · u + Hp,ext,+ · ym + (PO,ext,+)+ · ˆCT
ext · sign(ym −ˆy + [Δym])
(31)
γ−= b · u + Hp,ext,−· ym + (PO,ext,−)+ · ˆCT
ext · sign(ym −ˆy + [Δym])
(32)

Sliding Mode Approaches Considering Uncertainty for Reliable Control
91
Fig. 3. Structure diagram: quasi-linear system representation, T = tk+1 −tk.
with the pseudo inverses (.)+ are used. In dependency of the possible change of
sign from ˙uk to ˙uk+1, diagonal matrices of eigenvalues J+ and J−, matrices of
eigenvectors V+ and V−, linear observer gains Hp,ext,+ or Hp,ext,−, and matrices
PO,ext,+ or PO,ext,−are used. These are evaluated oﬄine for two ﬁxed operating
points of ˙u in the system matrix ˆA(S2)
ext
(index + if ˙uk+1 > 0 and index −if
˙uk+1 < 0). Then, a backward transformation from [zk+1] = [zk+1, zk+1] follows
with [xext,k+1] = V+ ·[zk+1] or [xext,k+1] = V−·[zk+1] in dependency of the sign
of ˙u. The goal of this procedure is to get intervals for parameters and states that
comprise all possible conﬁgurations where stability can be guaranteed. Figure 4
shows this in a graphical way: Starting with a large initial interval [x0], at the end
of the simulation a smaller interval [xf] results. Unfortunately, this algorithm
provides only good results for subsystem S1. The results of subsystem S2 are
not feasible due to overestimation and rotation of interval boxes (leading to non-
negligible wrapping eﬀect) caused by the importance of the positive or negative
sign of ˙u.
Aﬃne System Representation. In the previous subsection, only overestima-
tion due to the dependency problem could be reduced but not overestimation
due to the wrapping eﬀect so that the algorithm failed for subsystem S2. There-
fore, a new system representation is shown that makes it possible to directly map
states and parameters to their initial intervals in each time step. This reduces
computational eﬀort, neither state-space nor backward transformation are nec-
essary (because there are no dependencies of the states due to constant initial
intervals) and no interval box rotations occur. The algorithm is depicted in
Fig. 5 and based on the Euler forward discretization (sample time T = tk+1 −tk,

92
L. Senkel et al.
Fig. 4. Contracting interval box over time for a system with two states in two points
of view, where the considered interval is only deﬁned in x1-direction (comparable to
the measurement interval of subsystem S1).
identity matrix I). As it can be seen from the previous four cases in Fig. 3, now
only two cases for S2 are necessary (for S1 again only one case is used). A new
matrix Mk+1 and a vector ρk+1 using Eqs. (31) and (32) are calculated in each
time step which then represent the new enclosure vector [xk+1] still referring to
the constant initial interval vector [x0]. This procedure is successful for both sub-
systems, where the results for S1 from the aﬃne system representation are equal
to the results from the state-space transformation procedure. Figure 6 shows the
connection between the sliding mode observer estimating point values and the
observer estimating conﬁdence regions. Note that currently both observers are
implemented with an open-loop control. In future work, a closed-loop control will
be performed by using the states and parameters estimated from the observer
calculating conﬁdence regions with the aﬃne system representation in simulation
(dashed line in Fig. 6).
Fig. 5. Structure diagram: Aﬃne system representation.

Sliding Mode Approaches Considering Uncertainty for Reliable Control
93
Fig. 6. Overview of point-value observer and conﬁdence region estimator.
5
Results
In this section, the results of the point valued sliding mode observer and the
extension to the estimation of conﬁdence regions are shown. Note that the follow-
ing visualizations result from open-loop control. The connection to closed-loop
control in experiments is subject of future work. In Fig. 7, the conﬁdence regions
for the parameters Fig. 7(a) and (b) and the states Fig. 7(d) and (e) are visual-
ized. Obviously, the intervals become smaller over time which was proposed in
Fig. 4. Additionally, the results of the point valued observer for the parameter
estimates can be seen in Fig. 7(a) and (b) labeled by ˆαk and ˆβk. In this context,
Fig. 7(c) presents the estimated point valued parameters from experimental data.
These estimates are located inside the conﬁdence regions depicted in Fig. 7(a)
and (b). Moreover in Fig. 7(f) and (g), the errors between the estimated states
and the desired state trajectories are shown which provide good results taking
into consideration that the states are aﬀected by noise processes. In order to vali-
date the results of the parameter identiﬁcation in each time step from the second
subsystem, a comparable procedure is necessary [11]. Therefore, a least-squares
parameter identiﬁcation has been performed. There, the parameter estimates are
assumed to be constant for time intervals of 8 s taking into account the second
system equation ˙x2,m = ˆαkx2,m + ˆβkum which has to be valid also for mea-
surements (index m). From this, the parameters can be calculated for several
measurements over 8 s according to
α
β

=
⎛
⎜
⎝
⎡
⎢⎣
x2,m(t1)
um(t1)
...
...
x2,m(tf)
um(tf)
⎤
⎥⎦
⎞
⎟
⎠
+ ⎡
⎢⎣
˙x2,m(t1)
...
˙x2,m(tf)
⎤
⎥⎦
(33)
where tf −t1 = 8 s. The time derivatives x2,m and ˙x2,m of the measurable state x1
have been calculated by ﬁrst-order low pass ﬁltered derivative approximations.

94
L. Senkel et al.
Fig. 7. Conﬁdence regions for the system parameters (a), (b) with ([Δym,1] =
[−0.1; 0.1], [Δym,2] = [−0.1; 0.1], [Δym,3] = [−0.01; 0.01], hs,max = 400) including
simulative point-valued estimates ˆαk and ˆβk from ISMO; (c) experimental estimated
parameters of the ISMO; (d), (e) conﬁdence regions for the two system states; (f) and
(g) estimation errors of the reconstructed point-valued states using ISMO.
The measurements for the motor torque um have also been low-pass ﬁltered.
With the estimated parameter values of both, the sliding mode observer as well as
the least-squares method, the system (1) described in Sect. 2 has been simulated
again separately including the parameter estimates of each method. The results
can be seen in Fig. 8, which shows the root mean square errors for Δx1 = x1−x1,m
and Δx2 = x2 −x2,m (x2,m results from diﬀerentiation of x1,m) of both methods.
It becomes obvious, that the ISMO provides better estimates due to smaller root

Sliding Mode Approaches Considering Uncertainty for Reliable Control
95
Fig. 8. Results: Standard deviations
of the estimation errors (subsystem
S1) compared to the standard devi-
ation of the simulated measurement
noise (top); validation of the parameter
identiﬁcation by least-squares (LS) and
interval-based sliding mode observer
(ISMO) (bottom).
Fig. 9. Stability proof for S2.
mean square errors than the least-squares estimation. Moreover, Fig. 9 shows the
successful stability proof for the second subsystem, because L(V (S2)
O
) is negative
for the complete time horizon.
6
Conclusions and Outlook
In this paper, sliding mode approaches for control and estimation tasks were
shown under consideration of uncertainty as well as stochastic processes. Non-
measurable states and unknown parameters are estimated by observer strategies,
ﬁrstly with point valued results and secondly by estimating conﬁdence intervals
using the interval toolbox IntLab, the C++ library C-XSC and s-functions in
Matlab/Simulink. In future work, these strategies will be coupled as closed-
loop control and evaluated online on the described test rig. The real-time capa-
bility is already secured. Moreover, the inﬂuence of time discretization errors
will be investigated. The control procedure will especially be applied to other
mechanical and thermodynamic systems.
References
1. Engell, S.: Entwurf nichtlinearer Regelungen. R. Oldenbourg Verlag, M¨unchen
(1995)
2. Gennat, M., Tibken, B.: Guaranteed bounds for uncertain systems: methods using
linear lyapunov-like functions, diﬀerential inequalities and a midpoint method. In:
12th GAMM - IMACS International Symposium on Scientiﬁc Computing, Com-
puter Arithmetic and Validated Numerics (2006)

96
L. Senkel et al.
3. Heeks, J.: Charakterisierung unsicherer Systeme mit intervallarithmetischen Meth-
oden. Fortschritt-Berichte VDI, Reihe 8 Mess-, Steuerungs- und Regelungstechnik,
Nr. 919 (2001). (in German)
4. Jaulin, L., Kieﬀer, M., Didrit, O., Walter, ´E.: Applied Interval Analysis. Springer-
Verlag, London (2001)
5. Krasnochtanova, I., Rauh, A., Kletting, M., Aschemann, H., Hofer, E.P., Schoop,
K.-M.: Interval methods as a simulation tool for the dynamics of biological waste-
water treatment processes with parameter uncertainties. Appl. Math. Model.
34(3), 744–762 (2010)
6. Kushner, H.: Stochastic Stability and Control. Academic Press, New York (1967)
7. Senkel, L., Rauh, A., Aschemann, H.: Sliding mode techniques for robust trajectory
tracking as well as state and parameter estimation. Math. Comput. Sci. 8(3–4),
543–561 (2014)
8. Rauh, A.: Theorie und Anwendung von Intervallmethoden f¨ur Analyse und
Entwurf robuster und optimaler Regelungen dynamischer Systeme. Fortschritt-
Berichte VDI, Reihe 8 Mess-, Steuerungs- und Regelungstechnik, Nr. (1148) (2008).
(in German)
9. Rauh, A., Aschemann, H.: Interval-based sliding mode control and state estima-
tion of uncertain systems. In: Proceedings of IEEE International Conference on
Methods and Models in Automation and Robotics MMAR, Miedzyzdroje, Poland
(2012)
10. Senkel, L., Rauh, A., Aschemann, H.: Robust sliding mode techniques for control
and state estimation of dynamic systems with bounded and stochastic uncertainty.
In: Second International Conference on Vulnerability and Risk Analysis and Man-
agemet, ICVRAM 2014 (2014)
11. Senkel, L., Rauh, A., Aschemann, H.: Robust sliding mode techniques for control
and state estimation of dynamic systems with bounded and stochastic uncertainty.
In: Proceedings of 18th European Conference on Mathematics for Industry, ECMI
(2014)
12. Shtessel, Y., Edwards, C., Fridman, L., Levant, A. (eds.): Sliding Mode Control
and Observation. Springer, Birkh¨auser, Heidelberg, Boston (2014)
13. Smith, H.L.: Monotone Dynamical Systems: An Introduction to the Theory of
Competitive and Cooperative Systems. Mathematical Surveys and Monographs,
vol. 41. American Mathematical Society, Providence (1995)

Linear Algebra

Eﬃciency of Reproducible Level 1 BLAS
Chemseddine Chohra1,2(B), Philippe Langlois1,2, and David Parello1,2
1 Digits, Architectures et Logiciels Informatiques,
Univ. Perpignan Via Domitia, 66860 Perpignan, France
{chemseddine.chohra,philippe.langlois,david.parello}@univ-perp.fr
2 Laboratoire d’Informatique Robotique et de Micro´electronique de Montpellier,
Univ. Montpellier II, UMR 5506, CNRS, 34095 Montpellier, France
Abstract. Numerical reproducibility failures appear in massively paral-
lel ﬂoating-point computations. One way to guarantee this reproducibil-
ity is to extend the IEEE-754 correct rounding to larger computing
sequences, e.g. to the BLAS. Is the extra cost for numerical reproducibil-
ity acceptable in practice? We present solutions and experiments for the
level 1 BLAS and we conclude about their eﬃciency.
1
Introduction
Numerical reproducibility is an open question for current high performance com-
puting platforms. Dynamic scheduling and non-deterministic reduction on mul-
tithreaded systems aﬀect the operation order. This leads to non-reproducible
results because the ﬂoating-point addition is not associative. Numerical repro-
ducibility is important for debugging and for validating results, particularly if
legal agreements require the bitwise reproduction of the execution results. Fail-
ures have been reported in numerical simulation for energy science, dynamic
weather forecasting, atomic or molecular dynamic, ﬂuid dynamic – entries in [8].
Solutions provided at the middleware level forbid the dynamic behavior and
so impact the performances — see [11] for TBB, [15] for OpenMP or Intel MKL.
A ﬁrst algorithmic solution has been recently proposed in [6]. Their summation
algorithms, ReprodSum and FastReprodSum, guarantee the reproducibility inde-
pendently from the computation order. They return about the same accuracy
as the performance optimized algorithm only running a small constant times
slower.
Correctly rounded results ensure numerical reproducibility. IEEE754–2008
ﬂoating-point arithmetic is correctly rounded in its four rounding modes [1].
We propose to extend this property to the level 1 routines of the BLAS that
depend on the summation order: asum, dot and nrm2, respectively the sum of
the absolute values, the dot product and the vectorial Euclidean norm. Recent
algorithms that compute the correctly rounded sum of n ﬂoating-point values
allow us to implement such reproducible parallel computation. The main issue is
to investigate whether the running-time overhead of these reproducible routines
remains reasonable enough in practice. In this paper we present experimental
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 99–108, 2016.
DOI: 10.1007/978-3-319-31769-4 8

100
C. Chohra et al.
answers to this question. Our experimental framework is signiﬁcant of the cur-
rent computing practice: it consists in a shared memory parallel system with
several sockets of multicore x86 processing units. We apply standard optimiza-
tion techniques to implement eﬃcient sequential and parallel level 1 routines. We
show that for large vectors, reproducible and accurate routines introduces almost
no overhead compared to their original counterparts in a performance-optimized
library, Intel MKL [7]. For shorter ones, reasonable overheads are measured and
presented in Table 2. Since Level 1 BLAS performance is mainly dominated by
the memory transfers, additional computation does not signiﬁcantly increase the
running time, especially for large vectors.
The paper is organized as follows. In Sect. 2, we brieﬂy present some accurate
summation algorithms, their optimizations and an experimental performance
analysis to decide how to eﬃciently implement the level 1 BLAS. The experimen-
tal framework used throughout the paper is also described in this part. Section 3
is devoted to the performance analysis of the sequential implementation of the
level 1 BLAS routines. Section 4 describes their parallel implementations and the
measure of their eﬃciency; we gather plots for all previously presented sequential
and parallel algorithms. We conclude describing the future developments of this
ongoing project towards eﬃcient and reproducible BLAS.
2
Choice of Optimized Floating-Point Summation
Level 1 BLAS subroutines mainly rely on ﬂoating-point sums. It exists several
correctly rounded summation algorithms. Our ﬁrst step aims to derive optimized
implementations of such algorithms and to choose the most eﬃcient ones. In
the following, we brieﬂy describe these accurate algorithms and then, how to
optimize and to compare them.
All ﬂoating-point computations satisfy the IEEE754–2008. Let fl( pi) be
the computed sum of a length n ﬂoating-point vector p. The relative error of the
classical accumulation is of the order of u · n · cond( pi), where cond( pi) =
 |pi|/|  pi| is the condition number of the sum. u is the machine precision
that equals 2−53 for IEEE754 binary64.
2.1
Some Accurate or Reproducible Summation Algorithms
Algorithm SumK [10] reduces the previous relative error bound as if the classical
accumulation is performed in K times the working precision:
|SumK(p) − pi|
|  pi|
≤
(n · u)K
1 −(n · u)K · cond(

pi) + u.
(2.1)
SumK replaces the ﬂoating-point add by Knuth’s TwoSum algorithm that com-
putes both the sum and its rounding error [9]. SumK iteratively accumulates
these rounding errors to enhance the ﬁnal result accuracy. The correct round-
ing could be achieved by choosing a large enough K to vanish the eﬀect of the
condition number in Eq. (2.1) — but in practice this latter is usually unknown.

Eﬃciency of Reproducible Level 1 BLAS
101
Algorithm iFastSum [18] repeats SumK to error-free transform the entry vector.
As [2], this distillation process terminates returning a correctly rounded result
thanks to a dynamic control of the error.
Algorithms AccSum
[14] and FastAccSum [13] also rely on error-free transfor-
mations of the entry vector. They split the summands, relatively to max |pi|
and n, such that their higher order parts are then exactly accumulated. This
split-and-accumulate steps are iterated to enhance the accuracy up to return a
faithfully rounded sum. These algorithms return the correctly rounded sum and
FastAccSum requires 25 % less ﬂoating-point operations than AccSum.
HybridSum [18] and OnlineExact sum [19] exploit the short range of the ﬂoating-
point number exponents. These algorithms accumulate the summands with a
same exponent in a speciﬁc way to produce a short vector with no rounding error.
The length of the output vector of this error-free transform step is the exponent
range. HybridSum splits the summands such that ﬂoating-point numbers can be
used as error-free accumulators. OnlineExact uses two ﬂoating-point numbers to
simulate a double length accumulator. These algorithms then apply iFastSum to
evaluate the correctly rounded sum of the error-free short vector(s).
ReprodSum and FastReprodSum
[6] respectively rely on AccSum and FastAcc-
Sum to compute not fully accurate but reproducible sums independently of the
summation order. So numerical reproducibility of parallel sums is ensured for
every number of computing units.
2.2
Experimental Framework and Implementation
Table 1 describes our experimental framework. Aggressive compiler options as -
ffast-math are disabled to prevent the modiﬁcation of the sensitive ﬂoating-
point properties of these algorithms.
Rounding intermediate results to the binary64 format (53 bit mantissa) and
value safe optimizations are provided with -fp-model double and -fp-model
strict options. For a fair comparison, all algorithms are manually optimized
by a best eﬀort process. AVX vectorization, data prefetching and loop unrolling
are carefully combined to pull out the best implementation of each algorithm.
Optimization details are presented in Appendix A of [3]. Source code is available
at [12].
Runtimes are measured in cycles with the hardware counters thanks to the
RDTSC assembly instruction. We display the minimum cycle measures over more
than ﬁfty runs for each data. Condition dependant data are computed with the
dot product generator from [10].
2.3
Test and Results
Figure 1a and b present the runtime measured in cycles divided by the vector
size (y-axis). Vector lengths vary between 210 and 225 (x-axis) and two condition
numbers are considered : 108 and 1032.

102
C. Chohra et al.
Table 1. Experimental framework
Software
Compiler (language) ICC 14.0.2 (C99)
Options
-O3 -axCORE-AVX-I -fp-model double -fp-model strict
-funroll-all-loops
Parallel library
OpenMP 4.0
BLAS library
Intel MKL 11
Hardware
Processor
Xeon E5 2660 (Sandy Bridge) at 2.2 GHz
Cache
L1: 32 KB, L2: 256 KB, shared L3 for each socket: 20 MB
Bandwidth
51.2 GB/s
#cores
2 × 8 cores (hyper-threading disabled)
Fig. 1. Runtime/size ratio for optimized summation algorithms
It is not a surprise that HybridSum and OnlineExact are interesting for larger
size vectors. These algorithms produce one or two short vectors (length = 2048
in binary64) whose distillation is of constant time compared to the linear times
of the data preprocessing step (exponent extraction) or also, of the successive
error free transformations in the other algorithms. Moreover they are very less
sensitive to the conditioning of the entry vector. Shorter size vectors beneﬁt
from the other algorithms, especially from FastAccSum while their conditioning
remains small.
In the following we take advantage of these diﬀerent behaviors according to
the size of the entry vector. We call it a “mixed solution”. In practice for the
level 1 BLAS routines, FastAccSum or iFastSum are useful for short vectors while
larger ones beneﬁt from HybridSum or OnlineExact as we will explain it.

Eﬃciency of Reproducible Level 1 BLAS
103
3
Sequential Level 1 BLAS
Now we focus on the sum of the absolute value vector (asum), the dot product
(dot) and the 2-norm (nrm2). Note that other level 1 BLAS subroutines do not
suﬀer neither of accuracy nor of reproducibility failures. In this section, we start
with sequential algorithms detailing our implementations and their eﬃciency.
3.1
Sum of Absolute Values
The condition number of asum equals 1. So SumK is enough to eﬃciently get
a correctly rounded result. According to Eq. (2.1), K is chosen such that n ≤
u1/K−1.
Figure 2a exhibits that the correctly rounded asum costs less than 2 × the
optimized MKL dasum. Indeed K = 2 applies for the considered sizes. Note that
K = 3 is enough until n ≤235, i.e. until 256 Terabyte of data.
3.2
Dot Product
The dot product of two n-vectors is transformed into a sum of a 2n-vector with
Dekker’s TwoProd [5]. This sum is correctly rounded using a “mixed solution”.
Short vectors are correctly rounded with FastAccSum. For large n, we avoid
to build and read this intermediate 2n-vector: the two TwoProd results are
directly exponent-driven accumulated into the short vectors of OnlineExact. This
explains why this latter is interesting for shorter dot products than what we can
expect from Sect. 2.3.
Figure 2c shows this runtime divided by the input vector size — the condition
number is 1032. Despite the previous optimizations, the extra cost ratio compared
to MKL dot is between 3 and 6. This is essentially justiﬁed by the additional
computations (memory transfers are unchanged). If a fused-multiply-and-add
unit (FMA) is available, the 2MultFMA algorithm [9] that only costs 2 FMA
(compared to the TwoProd’s 17 ﬂop) certainly improves these values.
3.3
Euclidean Norm
It is not diﬃcult to implement an eﬃcient and reproducible Euclidean norm.
Reproducibility is ensured by the correct rounding of the sum of the squares
and then by the correct rounding of the IEEE-754 square root. Of course this
reproducible 2-norm is only faithfully rounded. Hence a “mixed solution” is
similar to the dot one.
Here the MKL nrm2 is not used as the comparison reference since we measure
very disappointing runtime for it. We implement a non-reproducible simple and
eﬃcient 2-norm with the optimized MKL dot (cblas ddot). We named it nOrm2.
The memory transfer cost dominates the computing one for dot and nOrm2:
compared to dot, nOrm2 halves the memory transfer volume, performs the same
number of ﬂoating-point operations and runs twice faster, see Fig. 2c and e.

104
C. Chohra et al.
10
15
20
25
0
1
2
3
4
log2 of size
runtime(cycles) / size
mklasum
Rasum
10
15
20
25
0
1
2
3
4
log2 of size
runtime(cycles) / size
asum
FastReprodasum
Rasum
10
15
20
25
0
5
10
15
20
25
30
log2 of size
runtime(cycles) / size
mkldot
Rdot
10
15
20
25
0
1
2
3
4
log2 of size
runtime(cycles) / size
mkldot
FastReprodDot
Rdot
10
15
20
25
0
5
10
15
20
25
30
log2 of size
runtime(cycles) / size
nOrm2
Rnrm2
10
15
20
25
0
1
2
3
4
log2 of size
runtime(cycles) / size
nOrm2
FastReprodNrm2
Rnrm2
Fig. 2. Runtime/size ratio of sequential and parallel level 1 BLAS (cond = 1032)

Eﬃciency of Reproducible Level 1 BLAS
105
As previously mentioned, the “mixed solution” dot product is still computation-
dominated. This justiﬁes that the previous dot ratios prohibitively double for
our sequential nrm2.
4
Reproducible Parallel Level 1 BLAS
Now we consider the parallel implementations. As in the previous section, parallel
asum relies on parallel SumK while parallel dot and nrm2 derive from a parallel
version of a “mixed solution” for the dot product. We start introducing these
two parallel algorithms. Then we derive the parallel reproducible level 1 BLAS
and perform its performance analysis.
4.1
From Parallel Sums to Reproducible Level 1 BLAS
Parallel SumK. It derives from the sequential version and has already been
introduced in [17]. It consists in 2 steps. Step 1 applies the SumK algorithm on
the local data without the ﬁnal error compensation for every K iterations. Hence
it returns a K-length vector S such that (Sj)j=1,K is the sum of the jth layer in
SumK applied to the local subvector. Step 2 gathers these K-length vectors to
the master unit and applies the sequential SumK.
Parallel dot “mixed solution”. Every n-length entry vector is split within P
threads (or computing units) and N denotes the length of these local subvectors.
The key point is to perform eﬃcient error-free transformations of these N-vectors
until the last reduction step. This consists in a 4 step process presented with
Fig. 3 for P = 2. Steps 1 and 2 are processed by the P threads with local
private vectors. Step 1 is similar to the sequential case and produces one vector
of size = 2N or 2048 or 4096: TwoProd transforms short N-vectors into a 2N-
one while this latter is not built for larger entries but directly exponent-driven
accumulated into the size-length vector as for HybridSum or OnlineExact. Step
2: the size-length vector is distilled (as for iFastSum) into a smaller vector of
non overlapping ﬂoating-point numbers. Step 3: every thread fuses this small
vector into a global shared one. Step 4 is performed by the master thread that
computes the correctly rounded result of the global vector with FastAccSum.
Let us remark that the small vector issued from Step 2 is at most of length
40 in binary64. Hence the distillation certainly beneﬁts from cache eﬀect. The
next fusing step moves across the computing units these vectors of length 40 in
the worst case. This induces a communication over-cost especially for distributed
memory environments. Nevertheless it introduces no more reduction step than
a classic parallel summation.
The Reproducible Parallel Level 1 BLAS. The reproducible parallel Rasum
derives from parallel SumK as in Sect. 3.1. The parallel dot “mixed solution”
gives
reproducible
parallel
Rdot
and
Rnrm2.
In
practice,
the
parallel
implementation of the Step 1 diﬀers from the sequential one as follows.

106
C. Chohra et al.
Fig. 3. Parallel dot “mixed solution”
For shorter vectors, iFastSum is preferred to FastAccSum to minimize the
Step 3 communications. For medium sized vectors, HybridSum is preferred to
OnlineExact for Rdot to minimize the Step 2 distillation cost. Otherwise Onli-
neExact is chosen to minimize the exponent extraction cost.
4.2
Test and Results
The experimental framework is unchanged. Each physical core runs at most one
thread thanks to the KMP AFFINITY variable. For every routine, we run from 1 to
16 threads on 16 cores to select the most eﬃcient conﬁgurations with respect to
the vector size. This optimal number of threads is given in parentheses in Table 2
except when it corresponds to the maximum possible resources (16). Intel MKL’s
(hidden) choice is denoted with a ⋆.
For the next performance comparisons, optimized parallel routines are nec-
essary as references. We use the MKL parallel dot and we implement asum
and nrm2 parallel versions. Our parallel asum runs up to 16 MKL dasum and
performs a ﬁnal reduction. Our parallel nOrm2 derives similarly from the sequen-
tial nOrm2 introduced in Sect. 3.3. These implementations exhibit the best
performances in Fig. 2. As in Sect. 2.3, our implementations of ReprodSum and
Table 2. Runtime extra cost for the reproducibility of parallel level 1 BLAS
Vector size
103
104
105 106 107
Rasum/asum
2.0 (1/1) 1.5 (4/2) 1.3
1.1
1
Rdot/mkldot
6.4 (8/⋆) 3.8 (8/⋆) 1.6
1.1
1
Rnrm2/nOrm2
9.1 (8/⋆) 7.1 (8/⋆) 3.4
1.6
1.5
Rasum/FastReprodasum
0.9 (1/1) 0.9 (4/4) 1.0
0.8
0.5
Rdot/FastReprodDot
1.5 (8/1) 1.5 (8/8) 0.9
0.7
0.6
Rnrm2/FastReprodNrm2 1.7 (8/1) 1.5 (8/8) 0.9
0.5
0.4

Eﬃciency of Reproducible Level 1 BLAS
107
FastReprodSum are optimized in a fair way using again AVX vectorization, data
prefetching and loop unrolling. The latter one is selected for the sequel.
We compare our reproducible Rasum, Rdot and Rnrm2, to the optimized
but non-reproducible reference implementations, and to the one derived from
FastReprodSum. Results are presented in Fig. 2 and Table 2.
Our reproducible Rasum compares very well to the optimized asum: the
initial 2 × extra cost tends to 1 for n about 106, see Fig. 2. Compared to the
sequential cases and since it operates now on 16 × smaller local vectors, our
reproducible Rdot and Rnrm2 reach their optimal linear performance for larger
entry sizes. Nevertheless the reproducible Rdot runs less than 2 × slower than
the MKL reference for vector size up to 105, see Fig. 2d. For the same reasons as
in the sequential case (Sect. 3.3), our reproducible Rnrm2 is not enough eﬃcient
to exhibit the same optimal tendency. Nevertheless the Rnmr2 overhead now
reduces to the more convincing ratios compared to nOrm2, see Fig. 2f.
Finally our fully accurate reproducible level 1 routines compare quite favor-
ably to those derived from the reproducible FastReprodSum, especially for large
vectors: see Fig. 2. Those latest algorithms read twice the entry vector and thus
suﬀer from cache eﬀects for large vectors. It is not the case for our algorithms. On
the other hand, the additional computation required by OnlineExact or Hybrid-
Sum beneﬁt from the ﬂoating-point unit availability.
5
Conclusion and Future Developments
This experimental work illustrates that reproducible level 1 BLAS can be imple-
mented with a reasonable extra cost compare to the performance-optimized non-
reproducible routines. Moreover our implementations oﬀer full accuracy almost
for free compared to the existing reproducible solutions.
Indeed the ﬂoating-point peak performance of current machines is far to be
exploited by level 1 BLAS. So the additional ﬂoating-point operations required
by our accuracy enhancement do not signiﬁcantly increase their execution time.
Of course these results are quantitatively linked to the experimental frame-
work. Nevertheless the same tendencies should be observed in other current com-
puting contexts. Work is ongoing to beneﬁt from FMA within dot and nrm2,
to validate an hybrid OpenMP + MPI implementation on larger HPC cluster,
to port and optimize this approach to accelerators (as Intel Xeon Phi) and to
compare it to the expansions and software long accumulator of [4].
Finally there is alas no reason to be optimistic for the BLAS level 3 where
the ﬂoating-point units have no space left for extra computation. Reproducible
solutions need to be implemented from scratch, for example following [16].

108
C. Chohra et al.
References
1. IEEE 754–2008, Standard for Floating-Point Arithmetic. Institute of Electrical and
Electronics Engineers, New York (2008)
2. Bohlender, G.: Floating-point computation of functions with maximum accuracy.
IEEE Trans. Comput. C-26(7), 621–632 (1977)
3. Chohra, C., Langlois, P., Parello, D.: Implementation and Eﬃciency of Repro-
ducible Level 1 BLAS (2015). http://hal-lirmm.ccsd.cnrs.fr/lirmm-01179986
4. Collange, S., Defour, D., Graillat, S., Iakimchuk, R.: Reproducible and accurate
matrix multiplication in ExBLAS for high-performance computing. In: SCAN 2014,
W¨urzburg, Germany (2014)
5. Dekker, T.J.: A ﬂoating-point technique for extending the available precision.
Numer. Math. 18, 224–242 (1971)
6. Demmel, J.W., Nguyen, H.D.: Fast reproducible ﬂoating-point summation. In: Pro-
ceedings of 21th IEEE Symposium on Computer Arithmetic. Austin, Texas, USA
(2013)
7. Intel Math Kernel Library. http://www.intel.com/software/products/mkl/
8. J´ez´equel, F., Langlois, P., Revol, N.: First steps towards more numerical repro-
ducibility. ESAIM: Proc. 45, 229–238 (2013)
9. Muller, J.M., Brisebarre, N., de Dinechin, F., Jeannerod, C.P., Lef`evre, V.,
Melquiond, G., Revol, N., Stehl´e, D., Torres, S.: Handbook of Floating-Point Arith-
metic. Birkh¨auser, Boston (2010)
10. Ogita, T., Rump, S.M., Oishi, S.: Accurate sum and dot product. SIAM J. Sci.
Comput. 26(6), 1955–1988 (2005)
11. Reinders, J.: Intel Threading Building Blocks, 1st edn. O’Reilly & Associates Inc.,
Sebastopol (2007)
12. http://webdali.univ-perp.fr/ReproducibleSoftware
13. Rump, S.M.: Ultimately fast accurate summation. SIAM J. Sci. Comput. 31(5),
3466–3502 (2009)
14. Rump, S.M., Ogita, T., Oishi, S.: Accurate ﬂoating-point summation - part I:
faithful rounding. SIAM J. Sci. Comput. 31(1), 189–224 (2008)
15. Story, S.: Numerical reproducibility in the Intel Math Kernel Library. Salt Lake
City, November 2012
16. Van Zee, F.G., van de Geijn, R.A.: BLIS: a framework for rapidly instantiating
BLAS functionality. ACM Trans. Math. Software 41(3), 14:1–14:33 (2015)
17. Yamanaka, N., Ogita, T., Rump, S., Oishi, S.: A parallel algorithm for accurate
dot product. Parallel Comput. 34(68), 392–410 (2008)
18. Zhu, Y.K., Hayes, W.B.: Correct rounding and hybrid approach to exact ﬂoating-
point summation. SIAM J. Sci. Comput. 31(4), 2981–3001 (2009)
19. Zhu, Y.K., Hayes, W.B.: Algorithm 908: online exact summation of ﬂoating-point
streams. ACM Trans. Math. Softw. 37(3), 37:1–37:13 (2010)

Tight Bounds on the Radius of Nonsingularity
David Hartman1,2(B) and Milan Hlad´ık1
1 Department of Applied Mathematics, Faculty of Mathematics and Physics,
Charles University, Malostransk´e n´am. 25, 11800 Prague, Czech Republic
{hartman,hladik}@kam.mff.cuni.cz
2 Institute of Computer Science Academy of Sciences,
18207 Prague 8, Czech Republic
hartman@cs.cas.cz
Abstract. Radius of nonsingularity of a square matrix is the minimal
distance to a singular matrix in the maximum norm. Computing the
radius of nonsingularity is an NP-hard problem. The known estimations
are not very tight; one of the best one has the relative error 6n. We pro-
pose a randomized approximation method with a constant relative error
0.7834. It is based on a semideﬁnite relaxation. Semideﬁnite relaxation
gives the best known approximation algorithm for MaxCut problem,
and we utilize similar principle to derive tight bounds on the radius of
nonsingularity. This gives us rigorous upper and lower bounds despite
randomized character of the algorithm.
Keywords: Radius
of
nonsingularity ·
Bounds ·
Semideﬁnite
programming
1
Introduction
It is well known that (non)singularity of the matrix is a determinative prop-
erty for many phenomena in optimization or system theory. Computationally
(non)singularity of a real value matrix is not a hard problem. In real-word,
however, systems under study are analysed via data that are often subject
to uncertainties and measurement errors. For these reasons it is beneﬁcial in
many situations to be able to detect existence of a singular matrix for range of
possible values of its corresponding elements, which starts to be more complex
problem – as we will see later even NP-hard.
Let be more speciﬁc for a while to show motivational example. We may
assume that there is a matrix A as a result of series of measurements with
elements aij, where the corresponding elements are subject to uncertainty. Let
moreover assume that we can ensure that there exists a precise bound for the
measurement precision represented by δ. This in fact means that the resulting
D. Hartman—Supported by grants 13-17187S and 13-10660S of the Czech Science
Foundation.
M. Hlad´ık—Supported by grant 13-10660S of the Czech Science Foundation.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 109–115, 2016.
DOI: 10.1007/978-3-319-31769-4 9

110
D. Hartman and M. Hlad´ık
values are in intervals [aij −δ, aij + δ] for all i, j. Mentioned results can be
therefore represented as an interval matrix [8,9]:
A = [A, A] = {A ∈Rn×n; A ≤A ≤A}
Determining a speciﬁc property of an interval matrix is usually represented
by determining this property for all realizations from the corresponding intervals.
In the same way, non-singularity of interval matrix can be seen as a search for
singular matrix within speciﬁed interval. The determination of radius of non-
singularity, on the other hand, goes even further and searches for the maximal
interval within which there is no singular matrix. More formally, an interval
matrix is called singular if it contains a singular matrix. Then the following
decision problem can be considered [2]:
Instance: Let us have a square interval matrix A, where both matrices
A and A are rational
Question: Is A singular?
Except these computational motivations there is at least one scientiﬁc ﬁeld
that may use results about radius of non-singularity. This connection is repre-
sented in notion of structured singular value [11] which is closely related with
radius of nonsingularity [2]. This term has relatively long history in automation
and control and its main use is in analysis of feedback systems with uncertainties,
especially concerning robustness and stability [10].
The determination of radius of non-singularity or, more precisely, bounds of
this characteristic seems to be however easier using tools from ordinary matrix
theory. Let us therefore concentrate on ordinary matrices. Given a matrix A ∈
Rn×n, the radius of nonsingularity [1–6] is deﬁned by
d(A) := inf {ε > 0; (∃singular B)(∀i, j) : |aij −bij| ≤ε}.
In other words, it is the minimum distance of A to a singular matrix in the
maximum norm. There was also studied generalization [1–4] in the form of
d(A, Δ) := inf {ε > 0; (∃singular B)(∀i, j) : |aij −bij| ≤εΔij},
where Δ ∈Rn×n is a given non-negative matrix. Thus, d(A) is a special case
of d(A, Δ) when putting Δ := E, and the matrix E consists of all ones, i.e. all
elements eij = 1.
In above form the determination of this characteristic for any matrix A seems
to be a hard task. In [2–4] it was shown that
d(A) =
1
∥A−1∥∞,1
,
(1)
where ∥· ∥∞,1 is a matrix norm deﬁned as
∥M∥∞,1 := max {∥Mx∥1; ∥x∥∞= 1} = max {∥Mz∥1; z ∈{±1}n}.

Tight Bounds on the Radius of Nonsingularity
111
Since computing ∥·∥∞,1 is known to be an NP-hard problem as shown in [2], the
same is true for the radius of nonsingularity. Moreover, it has been shown that
there is no polynomial time algorithm for approximating d(A) with a relative
error at most
1
4n2 provided P ̸= NP; see [3,4]. That is why there were investi-
gated various lower and upper bounds. In [3,4], one has
1
ρ(|A−1|E) ≤d(A) ≤
1
max
i=1,...,n(E|A−1|)ii
,
where ρ(A) is the spectral radius of A and E, similarly to the above case, is a
matrix consisting of all ones.
Rump [5,6] developed the other bounds as
1
ρ(|A−1|E) ≤d(A) ≤
6n
ρ(|A−1|E).
In both mentioned bounds, the matrix E can be substituted by matrix Δ
and the resulting equations then provide bounds for general version d(A, Δ) of
radius of nonsingularity, see e.g. [3].
Except these basic solutions there is one method introduced recently by
Kolev [13]. He is dealing with this problem from interval analysis perspective
and transforms its solution into search of real maximum magnitude eigenvalue
of an associated generalized eigenvalue problem. By iterative solution of the
last mentioned problem the original problem can be solved assuming additional
conditions on the whole process.
Roughly speaking, these conditions say that all solutions have to be computable
and moreover meet some additional sign conditions. Although the resulting algo-
rithm is polynomial, these conditions can represent an obstacle in solution and it
is therefore still reasonable to ask for better bounds or approximate solutions.
2
SDP Relaxation
We develop an approximation for ∥M∥∞,1. In view of (1), it will give the same
accurate approximation for d(A), too. Our approach is based on semi-deﬁnite
relaxation, and ﬁrst we proceed in a similar manner as for a randomized 0.878-
approximation algorithm for MaxCut problem [7].
The problem of computing ∥M∥∞,1 can be formulated as
max
n

i,j=1
mijxiyj subject to x, y ∈{±1}n.
(2)
The semideﬁnite programming relaxation consists of replacing discrete variables
xi, yj ∈{±1}, i, j = 1, . . . , n by unit vectors ui, vj ∈Rn, i, j = 1, . . . , n, as
follows

112
D. Hartman and M. Hlad´ık
max
n

i,j=1
mijuT
i vj subject to ui, vi ∈Rn,
∥ui∥2 = ∥vi∥2 = 1, i = 1, . . . , n.
(3)
For any feasible solution x, y to (2), we ﬁnd a corresponding solution to (3) by
putting ui := (0, . . . , 0, xi) and vi := (0, . . . , 0, yi), i = 1, . . . , n. This shows that
(3) is a relaxation to the optimization problem (2). That is, the optimal value
to (3) is an upper bound on the optimal value to (2).
This already represents problem with real variables. We have to go further to
obtain a system for which there are known approximate solutions. Let therefore
deﬁne the matrix Z ∈R2n×2n as
Z := U T U,
(4)
where U has the columns u1, . . . , un, v1, . . . , vn. The matrix Z is obviously pos-
itive semideﬁnite, denoted by Z ⪰0, and the diagonal entries are equal to 1.
Contrary, any positive semideﬁnite matrix Z can be factorized as in (4), and
ones on the diagonal of Z imply that the columns of U are unit vectors. Hence
(3) takes the equivalent form of a semideﬁnite program
max
n

i,j=1
mijzi,j+n subject to Z ⪰0,
zii = 1, i = 1, . . . , 2n.
(5)
The semideﬁnite program (5) can be solved in polynomial time with an arbi-
trary (a priori given) precision, see [12] or in connection with actual relaxation
in [7] (veriﬁcation of the numerically computed solutions were dealt in [14]). Let
ε > 0 be a given precision and denote by γ the computed approximate optimal
value to (5). Then
γ ≥∥M∥∞,1 −ε,
or
γ + ε ≥∥M∥∞,1.
This gives us an upper bound on the (∞, 1)-norm.
In order to calculate a lower bound on ∥M∥∞,1, we utilize an optimal solution
of (5) resp. (3) to ﬁnd a good feasible solution to (2). Let u∗
1, . . . , u∗
n, v∗
1, . . . , v∗
n
be an (approximative) optimal solution to (3). Compared with the solution of (2)
this is highly dimensional. To obtain original lower dimension, we have to adopt
some sort of “rounding”. Let p ∈Rn be a unit vector and deﬁne the mapping
w →

1
if pT w ≥0,
−1
otherwise
(6)
We can see that the resulting vector is dependent not only on the original,
but also on the value of p. This p is meant to be chosen randomly, which results

Tight Bounds on the Radius of Nonsingularity
113
in so called randomized rounding sampling p uniformly distributed within the
corresponding ball [7].
We utilize the following lemma from [7].
Lemma 1. Let u, v ∈Rn be unit vectors. The probability that (6) maps u and
v to diﬀerent values is 1
π arccos uT v.
Consider a feasible solution x∗
1, . . . , x∗
n, y∗
1, . . . , y∗
n to (2) determined by images
of u∗
1, . . . , u∗
n, v∗
1, . . . , v∗
n with respect to the mapping (6). By Lemma 1, the
expected objective value of this solution is

i,j
mij

1 −1
π arccos u∗T
i v∗
j

−mij
1
π arccos u∗T
i v∗
j
=

i,j
mij

1 −2
π arccos u∗T
i v∗
j

.
(7)
Let α ≈0.87856723 be the Goemans-Williamson value characterizing the
approximation ratio of their approximation algorithm for MaxCut [15]. It repre-
sents an optimal value of the problem minz∈[−1,1]
2 arccos z
π(1−z) . The following lemma
gives an auxiliary result for bounding (7) by means of a linear function.
Lemma 2. For each z ∈[−1, 1] we have
αz + α −1 ≤1 −2
π arccos z ≤αz + 1 −α.
Proof. We prove the ﬁrst inequality αz + α −1 ≤1 −2
π arccos z. The second one
is a straightforward corollary since 1 −2
π arccos z is an odd function.
We want to ﬁnd values a and b such that az + b ≤1 −2
π arccos z for every
z ∈[−1, 1]. Since the value of the function 1 −2
π arccos z at z = −1 is −1, we
focus on the line ax + b coming through the point (−1, −1) and supporting the
function from below. Thus the line reads ax + a −1 and a is the maximal value
such that az + a −1 ≤1 −2
π arccos z for every z ∈[−1, 1]. In other words,
a ≤2 −2
π arccos z
z + 1
,
∀z ∈[−1, 1].
Hence
a =
min
z∈[−1,1]
2 −2
π arccos z
z + 1
=
min
z∈[−1,1]
2π −2 arccos z
π(z + 1)
.
Substituting w := −z, we obtain
a =
min
w∈[−1,1]
2π −2 arccos (−w)
π(1 −w)
=
min
w∈[−1,1]
2π −2(π −arccos w)
π(1 −w)
=
min
w∈[−1,1]
2 arccos w
π(1 −w) = α.
⊓⊔

114
D. Hartman and M. Hlad´ık
Now, we use Lemma 2 to ﬁnd a lower bound to expected value of the solution
represented by (7). Let i, j ∈{1, . . . , n}, then
mij

1 −2
π arccos z

≥

mij(αz + α −1)
if mij ≥0,
mij(αz −α + 1)
otherwise,
or,
mij

1 −2
π arccos z

≥mijαz + |mij|(α −1).
Thus the lower bound to (7) can be established as follows

i,j
mij

1 −2
π arccos u∗T
i v∗
j

≥

i,j
mijαu∗T
i v∗
j + |mij|(α −1)
= αγ + (α −1)eT |M|e,
where e is a vector that consists of ones.
Hence we have an expected lower bound on ∥M∥∞,1
∥M∥∞,1 ≥αγ + (α −1)eT |M|e.
The right-hand side depends on the entries of M, but we can employ the estimate
∥M∥∞,1 ≤eT |M|e to obtain
∥M∥∞,1 ≥αγ + (α −1)eT |M|e ≥αγ + (α −1)∥M∥∞,1,
whence
∥M∥∞,1 ≥
α
2 −αγ.
This gives us a randomized algorithm with the approximation ratio
α
2−α ≈
0.78343281.
References
1. Poljak, S., Rohn, J.: Radius of Nonsingularity. Technical report KAM Series (88–
117), Department of Applied Mathematics, Charles University, Prague (1988)
2. Poljak, S., Rohn, J.: Checking robust nonsingularity is NP-hard. Math. Control
Signals Syst. 6(1), 1–9 (1993)
3. Rohn, J.: Checking properties of interval matrices. Technical report, Institute of
Computer Science, Academy of Sciences of the Czech Republic, Prague, 686 (1996)
4. Kreinovich, V., Lakeyev, A., Rohn, J., Kahl, P.: Computational Complexity and
Feasibility of Data Processing and Interval Computations. Kluwer Academic Pub-
lishers, Dordrecht (1998)
5. Rump, S.M.: Almost sharp bounds for the componentwise distance to the nearest
singular matrix. Linear Multilinear Algebra 42(2), 93–107 (1997)

Tight Bounds on the Radius of Nonsingularity
115
6. Rump, S.M.: Bounds for the componentwise distance to the nearest singular
matrix. SIAM J. Matrix Anal. Appl. 18(1), 83–103 (1997)
7. G¨artner, B., Matouˇsek, J.: Approximation Algorithms and Semideﬁnite Program-
ming. Springer, Heidelberg (2012)
8. Moore, R.E., Kearfott, R.B., Cloud, M.J.: Introduction to Interval Analysis. SIAM,
Philadelphia (2009)
9. Neumaier, A.: Interval Methods for Systems of Equations. Cambridge University
Press, Cambridge (1990)
10. Packard, A., Doyle, J.C.: The complex structured singular value. Automatica 29,
71–109 (1993)
11. Stein, G., Doyle, J.C.: Beyond singular values and loop shapes. J. Guidance Control
Dyn. 14(1), 5–16 (1991)
12. Gr¨otschel, M., Lov´asz, L., Schrijver, A.: The ellipsoid method and its consequences
in combinatorial optimization. Combinatorica 1, 169–197 (1981)
13. Kolev, L.V.: A method for determining the regularity radius of interval matrices.
Reliable Comput. 16(1), 1–26 (2011)
14. Jansson, C., Chaykin, D., Keil, C.: Rigorous error bounds for the optimal value in
semideﬁnite programming. SIAM J. Numer. Anal. 46(1), 180–200 (2007)
15. Goemans, M.X., Williamson, D.P.: Improved approximation algorithms for max-
imum cut and satisﬁability problems using semideﬁnite programming. J. ACM
42(6), 1115–1145 (1995)

Optimal Preconditioning for the Interval
Parametric Gauss–Seidel Method
Milan Hlad´ık(B)
Department of Applied Mathematics, Faculty of Mathematics and Physics,
Charles University, Malostransk´e n´am. 25, 118 00 Prague, Czech Republic
hladik@kam.mff.cuni.cz
Abstract. We deal with an interval parametric system of linear equa-
tions, and focus on the problem how to ﬁnd an optimal preconditioning
matrix for the interval parametric Gauss–Seidel method. The optimality
criteria considered are to minimize the width of the resulting enclosure,
to minimize its upper end-point or to maximize its lower end-point.
We show that such optimal preconditioners can be computed by solv-
ing suitable linear programming problems. We also show by examples
that, in some cases, such optimal preconditioners are able to signiﬁcantly
decrease an overestimation of the results of common methods.
Keywords: Interval computation · Interval parametric system ·
Preconditioner · Linear programming
1
Introduction
Consider an interval linear system of equations
Ax = b,
A ∈A, b ∈b,
(1)
where
A := [A, A] = {A ∈Rn×n; A ≤A ≤A},
b := [b, b] = {b ∈Rn; b ≤b ≤b}
are an interval matrix and an interval vector, respectively. The solution set is
deﬁned as
Σ := {x ∈Rn; ∃A ∈A, ∃b ∈b : Ax = b}.
Since it is nonconvex in general, the problem is usually to compute an interval
vector enclosing the solution set. Computing the smallest enclosure is an NP-hard
problem [1], so the known polynomial-time methods overestimate more-or-less
the optimal enclosure. There are, however, plenty of methods varying in time
complexity and tightness of the resulting enclosures [1,4,11].
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 116–125, 2016.
DOI: 10.1007/978-3-319-31769-4 10

Optimal Preconditioning for the Interval Parametric Gauss–Seidel Method
117
Notation. The midpoint and the radius matrices corresponding to an interval
matrix A are deﬁned respectively as
Ac := 1
2(A + A),
AΔ := 1
2(A −A).
Similarly we deﬁne interval vectors and intervals. The ith column of a matrix
C ∈Rn×n is denoted by C∗i.
Preconditioning. Many methods for enclosing the solution set use precondi-
tioning. Let C ∈Rn×n. Then the interval system (1) preconditioned by C reads
A′x = b′,
A′ ∈(CA), b′ ∈(Cb),
where CA and Cb are calculated by interval arithmetic [11]. The solution set
corresponding to the preconditioned system contains the original one as a subset,
so by preconditioning we do not miss any solution. Even though the solution set
inﬂates by preconditioning, most of the methods used perform better when the
system is preconditioned by a suitable matrix.
It is commonly recommended to use the preconditioner C = (Ac)−1 or its
numerical approximation. Some theoretical properties justifying this choice were
stated by Neumaier [10,11]. This does not mean, however, that the midpoint
inverse preconditioner yields the best results for each method and for each input
data.
Kearfott [5] initiated a research in constructing an optimal preconditioning
matrix [6–8]. The authors investigated the interval Gauss–Seidel method with an
application in nonlinear equation solving by the interval Newton method. They
showed that the optimal precodnitioner for the interval Gauss–Seidel method
can be formulated in terms of a linear programming, so it is polynomially com-
putable. A hybrid preconditioning strategy combining the midpoint inverse and
a certain kind of optimal preconditioners was proposed by Gau and Stadtherr
[2], and some numerical tests and an application in global optimization were
presented by Lin and Stadtherr [9].
The Interval Gauss–Seidel Method. Let us recall the interval Gauss–Seidel
method brieﬂy. Let x ⊇Σ be an initial enclosure of the solution set. One interval
Gauss–Seidel iteration for the preconditioned system is based on the operations
zi :=
1
(CA)ii
⎛
⎝(Cb)i −

j̸=i
(CA)ijxj
⎞
⎠,
xi := xi ∩zi,
for i = 1, . . . , n.

118
M. Hlad´ık
Interval Parametric Systems. An interval linear parametric system of equa-
tions is a family of systems
A(p)x = b(p),
p ∈p,
where the constraint matrix and the right-hand side vector linearly depend on
parameters p1, . . . , pK,
A(p) =
K

k=1
Akpk,
b(p) =
K

k=1
bkpk.
Herein, A1, . . . , AK ∈Rn×n are given matrices, b1, . . . , bK ∈Rn are given vec-
tors, and p = (p1, . . . , pK) is a given interval vector. The corresponding solution
set is deﬁned as
Σp := {x ∈Rn; ∃p ∈p : A(p)x = b(p)}.
Methods for computing an enclosure of the solution set were discussed, e.g.,
in [3,17]. A parametrized version of interval Gauss–Seidel iteration in particular
was addressed in Popova [13]. For parametric systems, preconditioning is applied,
too.
In principle, a parametric system can be relaxed and the problem reduced to
solving the standard interval system
Ax = b,
A ∈A, b ∈b,
where
A :=
K

k=1
Akpk,
b :=
K

k=1
bkpk
are evaluated by interval arithmetic. For a preconditioned system by C ∈Rn×n,
the tightest relaxation is done by evaluating
A :=
K

k=1
(CAk)pk,
b :=
K

k=1
(Cbk)pk.
Notice that a relaxation leads to overestimation of the solution set in general
since we lose information about dependencies between the interval parameters.
The interval Gauss–Seidel iteration for preconditioned parametric system
reads
zi :=
1
K
k=1(CAk)iipk
	
⎛
⎝
K

k=1
(Cbk)ipk −

j̸=i

 K

k=1
(CAk)ijpk

xj
⎞
⎠,
(2)
xi := xi ∩zi,
for i = 1, . . . , n.

Optimal Preconditioning for the Interval Parametric Gauss–Seidel Method
119
For parametric systems, a residual form of enclosures is often employed. Let
x0 ∈Rn, for example the solution of A(pc)x = b(pc). Then the residual form
enclosure of Σp has the form of x = x0 + y, where y encloses the solution set to
the parametric system
A(p)x = b(p) −A(p)x0,
p ∈p.
The interval Gauss–Seidel iteration (2) for this system works in the same man-
ner as for the original system, only the vectors bk are replaced by bk −Akx0,
k = 1, . . . , K.
Goal. The purpose of this paper is to extend the above mentioned results to
interval parametric systems of linear equations by designing an optimal precon-
ditioner for the parametric interval Gauss–Seidel method.
2
Optimal Preconditioners
In this section, we show how to construct an optimal preconditioner for (2). We
focus on the direct version only since for the residual form it works analogously.
Since the ith step of (2) depends only on the ith row of C, we will design C
row by row. For this purpose, let i ∈{1, . . . , n} be ﬁxed, and consider the ith
row of C, denoted by c.
Optimality of the preconditioner can be viewed from diverse perspectives;
see various criteria surveyed in Kearfott et al. [7,8]. We will be concerned with
the following objectives
– minimize the resulting width, that is, the objective is min 2zΔ
i ,
– minimize the resulting upper bound, that is, the objective is min zi,
– maximize the resulting lower bound, that is, the objective is max zi.
If we apply both the second and the third preconditioners, we obtain the
smallest interval as a result after the intersection. This observation relies on
standard interval arithmetic. Provided we allow division by zero-containing inter-
vals and utilize generalized arithmetic, then tighter results are possible; see
S-preconditioners in [7,8].
In the following, we will discuss the ﬁrst and the second criteria only since
the third criterion is easily reduced to the second one.
2.1
Minimal Width
Now, we deal with the ﬁrst mentioned criterion – to minimize 2zΔ
i . Suppose that
0 ∈x and 0 ∈zi. This is the case, for instance, when we apply the residual form
and x0 ∈Σp. However, the resulting preconditioner seems to perform well even
if the assumption is not satisﬁed despite it needn’t be optimal.
In order that zi is bounded, we will assume that the denominator in (2) does
not contain the zero. Moreover, we will normalize c such that the denominator

120
M. Hlad´ık
has the form of [1, r] for some r ≥1. Then, from our assumptions it follows that
the operation in (2) is simpliﬁed to
K

k=1
(cbk)pk −

j̸=i

 K

k=1
(cAk
∗j)pk

xj
Denote
βk := |cbk|,
k = 1, . . . , K,
αjk := |cAk
∗j|,
j = 1, . . . , n, k = 1, . . . , K,
ηj :=
K
k=1(cAk
∗j)pk

xj,
j ̸= i,
ψj :=
K
k=1(cAk
∗j)pk

xj,
j ̸= i.
Then our objective function reads
min
K

k=1
2pΔ
k βk +

j̸=i
(ηj −ψj).
(3)
Now, we set up the constraints. By the deﬁnition of βk, we have
βk ≥cbk, βk ≥−cbk,
k = 1, . . . , K.
(4)
Since βk is minimized in the objective function, at least one of the inequalities
will hold as equation, whence βk = |cbk|. Similarly for αjk we obtain
αjk ≥cAk
∗j, αjk ≥−cAk
∗j,
j = 1, . . . , n, k = 1, . . . , K.
(5)
The condition that the denominator is has the form of [1, r] is formulated as the
equation
c
K

k=1
Ak
∗ipc
k −
K

k=1
pΔ
k αik = 1.
(6)
Eventually, we reformulate conditions on ηj and ψj. Since 0 ∈xj, the upper
end-point of the interval product in the deﬁnition of ηj is attained either by the
product of their upper end-points or their lower end-points. Thus, we get
ηj ≥c
K

k=1
Ak
∗jpc
kxj −
K

k=1
pΔ
k xjαjk,
j ̸= i,
(7)
ηj ≥c
K

k=1
Ak
∗jpc
kxj +
K

k=1
pΔ
k xjαjk,
j ̸= i.
(8)

Optimal Preconditioning for the Interval Parametric Gauss–Seidel Method
121
Similarly for ψj,
ψj ≤c
K

k=1
Ak
∗jpc
kxj +
K

k=1
pΔ
k xjαjk,
j ̸= i,
(9)
ψj ≤c
K

k=1
Ak
∗jpc
kxj −
K

k=1
pΔ
k xjαjk,
j ̸= i.
(10)
Since ηj is maximized and ψj is minimized in the objective function, at least
one of the inequalities is fulﬁlled as equation. Analogous considerations hold for
αjk. Therefore, we gathered all the constraints to formulate the optimization
problem.
Optimization Problem. The optimal preconditioner of the ﬁrst type is found
by solving the optimization problem (3) under the constraints (4)–(10). This as
a linear programming problem with Kn + K + 3n −2 unknowns c, βk, αjk, ηj,
and ψj, and 2Kn + 2K + 4n −3 constraints.
Notice that for standard interval linear Eq. (1), our approach would require
approximately n3 variables as there is a quadratic number of parameters. This
is more than the linear programming formulation from Kearfott [6,7] using only
a linear number of variables. His method, however, cannot be directly extended
to parametric systems.
Overall, to determine the optimal preconditioner C, we have to solve n linear
programs, which is a polynomial time problem. Moreover, C needn’t be calcu-
lated in a veriﬁed way since any matrix can serve as a preconditioner.
On the other hand, solving n linear programs requires some computational
eﬀort, so it would be ineﬃcient to compute an optimal C in each iteration of the
interval Gauss–Seidel method. It seems more suitable to call the standard version
using midpoint inverse preconditioner (or any other method), and after that to
tighten the resulting enclosure by running several iterations with an optimal C.
2.2
Minimal Upper Bound
Herein, the criterion is to minimize zi. Suppose ﬁrst that zi > 0. Using deﬁnitions
of c, βk, αjk, and ψj from the previous section, the objective is formulated as
min c
K

k=1
bkpc
k +
K

k=1
pΔ
k βk −

j̸=i
ψj.
The constraints (4)–(6), (9)–(10) are employed in this problem, too. In addition,
we have to take into account the remaining two possibilities for which ψj can be
attain, and hence we involve also the inequalities

122
M. Hlad´ık
ψj ≤c
K

k=1
Ak
∗jpc
kxj −
K

k=1
pΔ
k xjαjk,
j ̸= i,
ψj ≤c
K

k=1
Ak
∗jpc
kxj +
K

k=1
pΔ
k xjαjk,
j ̸= i.
If zi ≤0, then we just replace (6) by the equation
c
K

k=1
Ak
∗ipc
k +
K

k=1
pΔ
k αik = 1,
(11)
which normalizes the denominator in (2) to have the form of [r, 1] for some
r ∈(0, 1]. In this case, we have to include the condition r ≥0, which draws
c
K

k=1
Ak
∗ipc
k −
K

k=1
pΔ
k αik ≥0.
The situation r = 0 makes practically no harm (even theoretically by realizing
what will be the result if extended arithmetic is used).
The weak point is that we do not know a priori whether zi > 0 or not. We
recommend to use the condition xi > 0 instead. It means, if xi > 0, then we use
(6), otherwise we use (11). The only possible fail may occur when zi ≤0 and xi > 0.
In this case, the optimization problem does not ﬁnd the optimal solution, however,
the optimal value would be non-positive. Therefore, the upper bound is reduced
substantially (with respect to sign change) from xi > 0 to a non-positive value.
The resulting linear program has less variables by n −1 than the previous
one from Sect. 2.1, and the number of constraints is the same. That is why the
time complexities are almost the same.
3
Examples
The examples below show how optimal preconditoners behave for various initial
enclosures, for various optimality criteria and for both versions (direct and resid-
ual) of the Gauss–Seidel method. For the residual form of the interval Gauss–
Seidel iteration, we employed the minimal width approach (Sect. 2.1), and for the
direct version, we used both the minimum upper and maximum lower bounds
preconditioners.
The main purpose if the examples is to illustrate that while in some cases
an optimal preconditioner makes no improvement, in another cases it may sig-
niﬁcantly reduce the overestimation. The computations were done in MATLAB
with help of the interval toolbox INTLAB v7.1 (see Rump [16]).
Example 1. Consider Example 4 from Popova [12], where
A(p) :=
 1 p1
p1 p2

,
b(p) :=
p3
p3

,
p ∈p = ([0, 1], −[1, 4], [0, 2])T .

Optimal Preconditioning for the Interval Parametric Gauss–Seidel Method
123
The initial enclosure of Σp is obtained by calling the verifylss function from
Intlab on the relaxed system A(p)x = b(p),
x = ([−4.4849, 6.6667], [−5.3334, 4.9697])T .
First, we call the residual form of the interval parametric Gauss–Seidel method.
For the center x∗:= xc and the residual interval vector y := x−xc, one iteration
yields the same result
y1 = ([−5.3940, 5.3940], [−4.1516, 4.1516])T
for the respectively midpoint inverse and the minimal width preconditioners
(Ac)−1 =
0.9091 0.1818
0.1818 −0.3636

,
C =
 1 0.2
0.5 −1

.
The corresponding contracted enclosure is
x1 = ([−4.3031, 6.4850], [−4.3334, 3.9698])T .
In contrast, the direct interval parametric Gauss–Seidel iteration with mid-
point inverse preconditioning gives
x2 = ([−4.2668, 6.6668], [−4.3334, 3.3334])T ,
which tightened about 13.77 % of the interval width on average, whereas the
optimal preconditioners yields
x3 = ([−4.0308, 6.6283], [−3.8769, 2.6812])T ,
which tightened about 20.38 % of the interval width on average. This enclosure
was computed by calculating separately the upper and the lower end-points by
using respectively the preconditioners
Cu =

1
0.2115
0.5978
−1

,
Cl =

1
0.1889
0.4022
−1

.
Comparing x1 and x3, we see that no one is better than the other one w.r.t.
inclusion.
It is interesting to consider the interval hull of the relaxed system, x4 =
([0, 4], [−2, 2])T , as an initial enclosure, too. For the residual form method, the
midpoint inverse preconditioner does not improve this enclosure, but the optimal
preconditioner reduces it to
x5 = ([0.0000, 3.8182], [−2.0001, 1.7686])T .
For the direct version, the midpoint inverse preconditioner also fails to tighten
x4, whereas the optimal preconditioner reduces the second component by half
to
x6 = ([0, 4], [−2, 0])T .

124
M. Hlad´ık
Example 2. In Example 5.2 from Popova and Kr¨amer [15], a resistive network
was considered with uncertain resistances. The output voltage was computed by
solving the interval parametric system
A(p) :=
⎛
⎜
⎜
⎜
⎜
⎝
30
−10
−10
−10
0
−10 10 + p1 + p2
−p1
0
0
−10
−p1
15 + p1 + p3
−5
0
−10
0
−5
15 + p4 0
0
0
−5
5
1
⎞
⎟
⎟
⎟
⎟
⎠
,
b(p) :=
⎛
⎜
⎜
⎜
⎜
⎝
1
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
,
where p ∈p = [8, 12] × [4, 8] × [8, 12] × [8, 12].
We will consider the enclosure computed by the residual and the direct inter-
val Gauss–Seidel method with the inverse midpoint preconditioner and initiated
by the verifylss enclosure for the relaxed system.
The residual form yields the enclosure
x1 = ([0.0595, 0.0851], [0.0262, 0.0587], [0.0247, 0.0514],
[0.0251, 0.0479], [−0.0352, 0.0499])T ,
which is no further improved by the optimal preconditioner.
The direct form yields in 0.2194 s the enclosure
x2 = ([0.0575, 0.0871], [0.0268, 0.0660], [0.0247, 0.0557],
[0.0267, 0.0491], [−0.0527, 0.0674])T .
Using the optimal preconditioner, it takes 1.2535 s to reduce the enclosure radii
by 15 % on average, and the resulting enclosure is
x3 = ([0.0626, 0.0862], [0.0293, 0.0646], [0.0273, 0.0541],
[0.0276, 0.0482], [−0.0359, 0.0573])T .
For comparison, verifylss enclosure for the system preconditioned by the
inverse midpoint reads
x4 = ([0.0576, 0.0871], [0.0187, 0.0662], [0.0202, 0.0558],
[0.0240, 0.0491], [−0.0525, 0.0672])T .
Hence, our enclosure x3 has by about 22 % (on average) smaller radii than x4.
4
Conclusion
We proposed a linear programming based method to compute an optimal pre-
conditioning matrix for the parametric interval Gauss–Seidel iterations. Even
though large numerical studies would be needed, some illustrative examples show
that the optimal preconditioner can sometimes reduce the ubiquitous overestima-
tion. Besides that, future research may be addressed to other types of optimality

Optimal Preconditioning for the Interval Parametric Gauss–Seidel Method
125
(S-preconditioners, pivoting preconditioners, and others), or to directly focus on
the interval Newton method (as done in Kearfott [5,6,8]). It would be also inter-
esting to investigate optimality of various preconditioners in generalized interval
systems, for instance for AE solutions of (non)-parametric interval systems [14].
Acknowledgments. The author was supported by the Czech Science Foundation
Grant P402-13-10660S.
References
1. Fiedler, M., Nedoma, J., Ram´ık, J., Rohn, J., Zimmermann, K.: Linear Optimiza-
tion Problems with Inexact Data. Springer, New York (2006)
2. Gau, C.-Y., Stadtherr, M.A.: New interval methodologies for reliable chemical
process modeling. Comput. Chem. Eng. 26(6), 827–840 (2002)
3. Hlad´ık, M.: Enclosures for the solution set of parametric interval linear systems.
Int. J. Appl. Math. Comput. Sci. 22(3), 561–574 (2012)
4. Hlad´ık, M.: New operator and method for solving real preconditioned interval linear
equations. SIAM J. Numer. Anal. 52(1), 194–206 (2014)
5. Kearfott, R.B.: Preconditioners for the interval Gauss-Seidel method. SIAM J.
Numer. Anal. 27(3), 804–822 (1990)
6. Kearfott, R.B.: Decomposition of arithmetic expressions to improve the behavior
of interval iteration for nonlinear systems. Comput. 47(2), 169–191 (1991)
7. Kearfott, R.B.: A comparison of some methods for bounding connected and dis-
connected solution sets of interval linear systems. Comput. 82(1), 77–102 (2008)
8. Kearfott, R.B., Hu, C., Novoa, M.: A review of preconditioners for the interval
Gauss-Seidel method. Interval Comput. 1991(1), 59–85 (1991)
9. Lin, Y., Stadtherr, M.A.: Advances in interval methods for deterministic global
optimization in chemical engineering. J. Glob. Optim. 29(3), 281–296 (2004)
10. Neumaier, A.: New techniques for the analysis of linear interval equations. Linear
Algebra Appl. 58, 273–325 (1984)
11. Neumaier, A.: Interval Methods for Systems of Equations. Cambridge University
Press, Cambridge (1990)
12. Popova, E.: Quality of the solution sets of parameter-dependent interval linear
systems. ZAMM, Z. Angew. Math. Mech. 82(10), 723–727 (2002)
13. Popova, E.D.: On the solution of parametrised linear systems. In: Kr¨amer, W.,
von Gudenberg, J.W. (eds.) Scientiﬁc Computing, Validated Numerics, Interval
Methods, pp. 127–138. Kluwer (2001)
14. Popova, E.D., Hlad´ık, M.: Outer enclosures to the parametric AE solution set. Soft
Comput. 17(8), 1403–1414 (2013)
15. Popova, E.D., Kr¨amer, W.: Visualizing parametric solution sets. BIT 48(1), 95–115
(2008)
16. Rump, S.M.: INTLAB - INTerval LABoratory. In: Csendes, T. (ed.) Developments
in Reliable Computing, pp. 77–104. Kluwer Academic Publishers, Dordrecht (1999)
17. Rump, S.M.: Veriﬁcation methods: Rigorous results using ﬂoating-point arithmetic.
Acta Numer. 19, 287–449 (2010)

Reproducible and Accurate Matrix
Multiplication
Roman Iakymchuk1,2,3(B), David Defour4, Sylvain Collange5,
and Stef Graillat1,2
1 Sorbonne Universit´es UPMC Univ Paris 06, UMR 7606, LIP6, 75005 Paris, France
{roman.iakymchuk,stef.graillat}@lip6.fr
2 CNRS, UMR 7606, LIP6, 75005 Paris, France
3 Sorbonne Universit´es UPMC Univ Paris 06, ICS, 75005 Paris, France
4 DALI–LIRMM, Universit´e de Perpignan,
52 Avenue Paul Alduy, 66860 Perpignan, France
david.defour@univ-perp.fr
5 INRIA – Centre de Recherche Rennes – Bretagne Atlantique,
Campus de Beaulieu, 35042 Rennes Cedex, France
sylvain.collange@inria.fr
Abstract. Due to non-associativity of ﬂoating-point operations and
dynamic scheduling on parallel architectures, getting a bit-wise repro-
ducible ﬂoating-point result for multiple executions of the same code
on diﬀerent or even similar parallel architectures is challenging. In this
paper, we address the problem of reproducibility in the context of matrix
multiplication and propose an algorithm that yields both reproducible
and accurate results. This algorithm is composed of two main stages: a
ﬁltering stage that uses fast vectorized ﬂoating-point expansions in con-
junction with error-free transformations; an accumulation stage based
on Kulisch long accumulators in a high-radix carry-save representation.
Finally, we provide implementations and performance results in parallel
environments like GPUs.
Keywords: Matrix multiplication · Reproducibility · Accuracy ·
Kulisch long accumulator · Error-free transformation · Floating-point
expansion · Rounding-to-nearest · GPUs
1
Introduction
In many ﬁelds of science and engineering, the process of ﬁnding the solution for a
speciﬁc problem requires solving a system of linear equations, or a least squares
problem, or eigenvalue problem. A common approach is to develop solvers for
each speciﬁc task and then spend a tremendous amount of time on tuning them.
However, best practice suggests to use already optimized solver-routines con-
tained in linear algebra libraries.
The development of linear algebra libraries began in the early 1970s. Since
that time many libraries have been released. With the inﬂuence of common HPC
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 126–137, 2016.
DOI: 10.1007/978-3-319-31769-4 11

Reproducible and Accurate Matrix Multiplication
127
computers, which were based on vector processors, in 1979 a ﬁrst set of Basic
Linear Algebra Subprograms (BLAS-1) was designed as a set of basic vector
operations. In 1988 the idea of BLAS was developed further, yielding a second
set of routines for matrix-vector operations (BLAS-2). For those routines the
amount of data required and ﬂoating-point operations (Flops) performed have
quadratic complexity.
When architectures with multiple layers of cache memory appeared, the per-
formance of both BLAS-1 and BLAS-2 operations became an issue: for these
routines the ratio between the numbers of Flops and memory accesses is only
O(1). In order to attain high performance on architectures with a hierarchical
memory system, in 1990 the third level of BLAS (BLAS-3) with matrix-matrix
operations was deﬁned. These routines perform O(n3) Flops over O(n2) data,
giving the opportunity to hide memory latency and oﬀer performance close to
the achievable peak.
A generic implementation of the BLAS speciﬁcation is provided since the
announcement of the library in 1979. This reference implementation is equipped
with the complete functionality, but it is not optimized for any architecture.
Thus, processor manufacturers as well as scientists developed tuned implemen-
tations of the BLAS for each architecture. Prominent examples of these imple-
mentations are Intel MKL, AMD ACML, IBM ESSL, ATLAS, and GotoBLAS
(now OpenBLAS). ATLAS [1] is based on an auto-tuned empirical approach
while GotoBLAS [2] is a hand-tuned machine-speciﬁc implementation of the
BLAS. Due to the raising popularity of GPUs for high-performance computing,
NVIDIA provided a GPU-version of the BLAS (cuBLAS).
The core of the BLAS library is xGEMM1, which is a BLAS-3 routine, that
computes the matrix-matrix products as
C := αop(A)op(B) + βC,
(1)
where α and β are scalars; op(A), op(B), and C are general matrices with op(A)
a m × k matrix, op(B) a k × n matrix, and C a m × n matrix; op(X) repre-
sents either a non-transposed X or a transposed XT matrix. xGEMM performs
2mnk ﬂoating-point operations over mk + kn + mn data. All the other BLAS-3
routines can be expressed in terms of xGEMM. Moreover, when diﬀerent imple-
mentations of BLAS are compared, the ﬁrst criteria used for this comparison is
the performance of xGEMM.
The proﬁtable ratio between the computation and the memory references
of the BLAS-3 routines has a strong impact on the design and automatic gen-
eration of linear algebra algorithms. For instance, in order to exploit the opti-
mized BLAS implementations, the Linear Algebra PACKage (LAPACK) builds
its blocked algorithms on top of the BLAS-3 operations. Furthermore, scientists
either try to generate algorithms relying more on the BLAS-3 routines, in par-
ticular xGEMM, or try to rewrite their algorithms in order to beneﬁt from the
performance provided by the BLAS-3 routines [3].
1 In general, x stands for four diﬀerent formats, but in the scope of this article we
consider x to correspond to single (S) or double (D) precision.

128
R. Iakymchuk et al.
In general, matrix-matrix products relies on optimized version of parallel
reduction and dot-product involving ﬂoating-point additions and multiplications
which are non-associative operations. Hence, as the order of operations may vary
from one parallel machine to another or even from one run to another [4], repro-
ducibility of results is not guaranteed. These discrepancies worsen on heteroge-
neous architectures – such as clusters composed of standard CPUs in conjunction
with GPUs and/or accelerators like Intel Xeon Phi – which combine together
diﬀerent programming environments that may obey various ﬂoating-point mod-
els and oﬀer diﬀerent intermediate precision or diﬀerent operators [5,6]. In
some cases, such non-reproducibility of ﬂoating-point computations on paral-
lel machines causes validation and debugging issues, and may even lead to
deadlocks [7].
By reproducibility, we mean getting a bit-wise identical ﬂoating-point result
from multiple runs of the same code on the same data. Numerical reproducibility
can be addressed by targeting either the order of operations or the error resulting
from ﬁnite arithmetic. One solution consists in providing the deterministic con-
trol over rounding errors by, for example, enforcing the execution order for each
operation. However, these approach is not portable and/or does not scale well
with the number of processing cores. The other solution aims at avoiding cancel-
lation and rounding errors by using, for instance, a long accumulator such as the
one proposed by Kulisch [8]. This solution increases the accuracy at the price of
more operations and memory transfers per output data. Because of that, for a
long time, it was considered too expensive for the little beneﬁt it was providing.
To enhance reproducibility, Intel proposed a “Conditional Numerical Repro-
ducibility” (CNR) in its Math Kernel Library (MKL). Although, CNR guaran-
tees reproducibility, it does not ensure correct rounding, meaning the accuracy
is arguing. Additionally, the cost of obtaining reproducible results with CNR is
high. For instance, for large arrays the MKL’s summation with CNR is 85−93 %
slower than both the regular MKL’s and our reproducible summation; the later
two deliver comparable performance. The performance gap between the MKL’s
reproducible matrix multiplication and its classic implementation is even higher
and is roughly 3 −4 times.
Demmel and Nguyen introduced a family of algorithms for reproducible sum-
mation in ﬂoating-point arithmetic [9]. They have extended this concept to
reproducible BLAS routines (covering, mainly, the BLAS-1 routines) that are
distributed in the ReproBLAS library2.
Recently, we introduced in [10] an approach to compute deterministic sums
of ﬂoating-point numbers. Our approach is based on a multi-level algorithm
that combines eﬃciently ﬂoating-point expansions and long accumulators. The
proposed implementations on recent Intel desktop and server processors, on Intel
Xeon Phi co-processors, and on both AMD and NVIDIA GPUs, showed that
the numerical reproducibility and bit-perfect accuracy can be achieved at no
additional cost for large sums that have dynamic ranges of up to 90 orders of
2 http://bebop.cs.berkeley.edu/reproblas/.

Reproducible and Accurate Matrix Multiplication
129
magnitude. This speed-up is possible thanks to arithmetic units that are left
underused by the standard reduction algorithms.
In this article, we propose an approach to ensure both reproducibility and
accuracy (rounding-to-nearest) of the product of two matrices composed of
ﬂoating-point numbers. The derived algorithm is based on the standard non-
deterministic xGEMM and our deterministic summation algorithm. Moreover,
we provide implementations of this algorithm on GPU accelerators. To our
knowledge, this is the ﬁrst work on reproducible matrix-matrix multiplication.
The paper is organized as follows. Section 2 reviews related aspects of ﬂoating-
point arithmetic and highlights ﬂoating-point expansions and long accumulators.
Section 3 presents our approach to derive exact, meaning both reproducible and
accurate, matrix-matrix product. In Sect. 4, we expose implementations and per-
formance results on GPU accelerators. Finally, we draw conclusions in Sect. 5.
2
Background
Without loss of generality, in the rest of this article, we will consider double
precision format (binary64) from the IEEE-754 standard [11]. Floating-point
representation of numbers allows to cover a wide dynamic range. Dynamic range
refers to the absolute ratio between the number with the largest magnitude
and the number with the smallest non-zero magnitude in a set. For instance,
binary64 can represent positive numbers from 4.9 × 10−324 to 1.8 × 10308, so
it covers a dynamic range of 3.7 × 10631.
Non-associativity of ﬂoating-point addition implies that the result depends
on the order of the operations. For example, in double precision (−1⊕1)⊕2−100
is diﬀerent from −1 ⊕(1 ⊕2−100) where ⊕denotes the result of a ﬂoating-
point addition. Thus, the accuracy of a ﬂoating-point summation depends on
the order of evaluation. More details about this phenomenon can be found in
the main references [12,13].
Two approaches exist to execute one ﬂoating-point addition without intro-
ducing rounding error. The ﬁrst solution aims at computing the error which
occurred during rounding using ﬂoating-point expansions in conjunction with
error-free transformations, see Sect. 2.1. The second solution exploits the ﬁnite
range of representable ﬂoating-point numbers by storing every bit in a very long
vector of bits, see Sect. 2.2.
2.1
Floating-Point Expansion
Floating-point expansions represent the result as an unevaluated sum of ﬂoating-
point numbers, whose components are ordered in magnitude with minimal over-
lap to cover a wide range of exponents. Floating-point expansions of sizes 2 and
4 are described in [14,15], accordingly. They are based on error-free transfor-
mation. Indeed, when working with rounding-to-nearest, the rounding error in
addition or multiplication can be represented as a ﬂoating-point number and can

130
R. Iakymchuk et al.
also be computed in ﬂoating-point arithmetic. The traditional error-free trans-
formation for addition is TwoSum [16], see Algorithm 1, and for multiplication is
TwoProduct, see Algorithm 2. For TwoSum, it means that r + s = a + b with
r = a⊕b and s, which is a ﬂoating-point number that corresponds to the round-
ing error. For TwoProduct, we use the fused multiply and add (FMA) instruction
that is widely available on modern architectures. FMA(a, b, c) makes it possible
to compute a × b + c with only one rounding. Thus, we have r + s = a × b
with r = a ⊗b and s = FMA(a, b, −r), where ⊗stands for ﬂoating-point
multiplication.
Algorithm 1. Error-free transformation for sum of two ﬂoating-point
numbers.
Function [r, s] = TwoSum(a, b)
r ←a + b
z ←r −a
s ←(a −(r −z)) + (b −z)
Algorithm 2. Error-free transformation for product of two ﬂoating-point
numbers.
Function [r, s] = TwoProduct(a, b)
r ←a × b
s ←FMA(a, b, −r)
Adding one ﬂoating-point number to an expansion is an iterative operation.
The ﬂoating-point number is ﬁrst added to the head of the expansion and the
rounding error is recovered as a ﬂoating-point number using an error-free trans-
formation such as TwoSum. The error is then recursively accumulated to the
remainder of the expansion.
With expansions of size p – that correspond to the unevaluated sum of p
ﬂoating-point numbers – it is possible to accumulate ﬂoating-point numbers
without losing accuracy as long as every intermediate result can be represented
exactly as a sum of p ﬂoating-point numbers. This situation occurs when the
dynamic range of the sum is lower than 253·p in case of binary64.
The main advantage of expansions is that they can be placed in registers
during the whole computation. However, the accuracy is insuﬃcient for the
summation of numerous ﬂoating-point numbers or sums with large dynamic
ranges. Moreover, the complexity of this algorithm grows linearly with the size
of expansion.
2.2
Long Accumulator
An alternative algorithm to ﬂoating-point expansions uses very long ﬁxed-point
accumulators. This accumulator can be viewed as a projection of the set of
ﬂoating-point numbers from minimum (emin) to maximum (emax) exponents
into a long register, where each spot covers numbers with a certain exponent
range. The length of the accumulator is selected in such a way that it represents

Reproducible and Accurate Matrix Multiplication
131
Fig. 1. Kulisch long accumulator.
every bit of information of the input format, e.g. binary64; this covers the range
from the smallest representable ﬂoating-point value to the largest one, indepen-
dently of the sign. For instance, Kulisch [8] proposed to use an accumulator of
4288 bits to handle the dot product of two vectors composed of binary64 values.
The summation is performed without loss of information by accumulating every
ﬂoating-point input numbers in the long accumulator, see Fig. 1. The long accu-
mulator is the perfect solution to produce the exact result of a very large amount
of ﬂoating-point numbers of arbitrary magnitude. However, for a long period this
approach was considered impractical as it induces a very large memory overhead.
Furthermore, without dedicated hardware support, its performance is limited by
indirect memory accesses that makes vectorization challenging.
3
Exact Matrix-Matrix Multiplication
In order to achieve best performance for linear algebra kernels, machine-speciﬁc
hand tuning of those kernels is often applied; a good example is the Goto’s imple-
mentation of xGEMM. Scientists aim at optimizing this process for existing and
upcoming architectures through the automatic generation of linear algebra ker-
nels. As the matrix-matrix multiplication is the core of the BLAS library, in
several works [1,17] the problem of optimizing this routine for a given architec-
ture was tackled by applying the automatic generation approach. For instance,
the ATLAS project [1] provides a very good implementation of BLAS by tuning
routines for various architectures; those are centralized around a highly tuned
matrix-matrix product that is automatically optimized for diﬀerent levels of
memory hierarchy. The idea of auto-tuning was extended to GPUs architectures
applying diﬀerent programming models such as CUDA and OpenCL. Apart from
both code generation and heuristic search in conjunction with OpenCL, Mat-
sumoto et. al. [17] proposed to store data in memory not only in a standard
row-/column-major order, but also in a block-major order. We revise these
ideas and employ some of them in our implementations of exact xGEMM,
which is described in Sect. 4.1. Therefore, we combine together auto-tuning for
standard non-deterministic xGEMM and machine-speciﬁc hand tuning for our
reproducible approach.

132
R. Iakymchuk et al.
3.1
Hierarchical Approach for Matrix-Matrix Multiplication
We introduced in [10] a hierarchical superaccumulation scheme for the summa-
tion of ﬂoating-point numbers (parallel reduction) that relies on ﬂoating-point
expansions with error-free transformations and long accumulators as described
in Sect. 2. Thanks to the latter, this approach guarantees both reproducible and
accurate results. This allows us to propose a reproducible and accurate matrix
multiplication scheme that divides computations into three stages: ﬁltering, pri-
vate superaccumulation, and rounding. This decomposition is suitable for the
nested parallelism of modern architectures and it makes a full use of SIMD and
multi-threads.
In the ﬁrst stage, each partial product is computed using error-free trans-
formation. In order to ensure accuracy, this steps generates two ﬂoating-point
numbers, see Algorithm 2: the result and the rounding error. Both resulting
ﬂoating-point numbers are accumulated using Algorithm 1 into an expansion of
size p (p ≥3) that is stored in registers or private memory for each threads. This
step beneﬁts from vectorization and pipelining by maintaining one expansion per
GPU thread.
In case the accuracy provided by ﬂoating-point expansions for product and/or
summation is not enough, meaning a non-zero residue x still remains after ﬁlter-
ing, each residue x is added to a long accumulator. We also propose an optimized
version of ﬂoating-point expansions of size p that relies on the stopping crite-
ria (x ≡0) in the accumulation loop. This technique is called early-exit and
exhibits performance which depends on the distribution of input numbers and
the ability of the architecture to handle irregular branches.
A trade-oﬀbetween speed and usage of the hardware resources lies in the
proper choice of the size p of the ﬂoating-point expansion. A small value of p will
lead to numerous transfers from the expansion towards the long accumulators,
which will slow down the computation. A large value of p will lead to the overuse
of registers and eventually to the register spilling.
Once all the input number are accumulated, each ﬂoating-point expansion
is ﬂushed to a long accumulator, independently of the parameter p. Hence, the
second stage is based on superaccumulation, meaning summation to long accu-
mulators, and it is involved either when the accuracy provided by expansions
is not enough or at the end of the computation. Depending on the amount of
memory available, long accumulators are stored in either fast local memory, e.g.
cache or shared memory, or global memory.
In the third stage, rounding of private long accumulators back to the desired
ﬂoating-point format is performed in order to obtain reproducible and correctly
rounded results.
4
Implementations and Experimental Results
This section presents our implementations of the multi-level reproducible matrix
multiplication and their evaluation on both NVIDIA and AMD GPUs, see
Table 1 for the detailed description of these GPU architectures. We compared

Reproducible and Accurate Matrix Multiplication
133
Table 1. Hardware platforms used for the experiments.
A NVIDIA Tesla K20c
13 SMs × 192 CUDA cores 0.705 GHz
B AMD Radeon HD 7970 32 CUs × 64 units
0.925 GHz
the accuracy of our implementations with results produced by the multiple
precision library MPFR on CPUs; the MPFR library is not multi-threaded
and does not support GPUs. In case of binary64, we used 4196 bits (2 ×
(emin + emax + mantissa) = 2 × (1022 + 1023 + 53)) within MPFR in order
to guarantee the bit-wise reproducibility as well as the accuracy of the results
independently of rounding errors and dynamic ranges.
4.1
Implementations
We follow the strategy proposed by Matsumoto et al. [17] regarding their matrix
partitioning technique in order to exploit multi-level memory hierarchies on GPU
architectures, see Fig. 2. An adequate matrix partitioning improves signiﬁcantly
the reuse of data and keeps the computational units busy while performing
memory transfers.
Our solution is diﬀerent from Matsumoto’s one, as we divide memory space
among matrices, ﬂoating-point expansions, and long accumulators. The latter
may require 76 (76 × 64 bits is the size of each long accumulator) times more
storage, because the matrix C is entirely composed of long accumulators in the
non-optimized case, meaning when long accumulators are not reused. Thus, we
use two levels of blocking in our matrix multiplication algorithms to amortize
the cost of data accesses to the three levels of memory on GPUs, namely private
(registers), local or data caches, and global. The ﬁrst level of blocking focuses
on enhancing the access latency between the global and local memories for each
group of threads (or warp or work-group on GPUs). We assume that ml, nl, and
kl are three block sizes multiple of m, n, and k, respectively. Figure 2a represents
the partitioning of the matrices C, A, and B into blocks of sizes ml × nl, ml × kl,
and kl × nl, accordingly. Each ml × nl block of C is computed by a work-group
that involves an ml × k panel of A and a k × nl panel of B
This panel-panel multiplication iterates k/kl times in the outermost loop
of our xGEMM algorithm using the block-block multiplication. Thus, on each
iteration the work-group updates each resulting ml × nl block of C with the
product of an ml × kl block of A by a kl × nl block of B. This second level
of blocking optimizes the use of private memory for each thread (work-item on
GPUs). Figure 2b shows further partitioning of matrices within their blocks in
such a way that each work-item in the work-group is responsible for updating
an ms × ns sub-block of C through the multiplication of an ms × kl sub-panel
of A by a kl × ns sub-panel of B.
In order to ensure both reproducibility and accuracy of xGEMM, we
use one ﬂoating-point expansion with error-free transformation per thread.

134
R. Iakymchuk et al.
Fig. 2. Partitioning of matrix-matrix multiplication.
When the accuracy provided by these expansions is not enough, we switch to long
accumulators that are allocated for each thread of a given work-group. However,
this induces pressure on the memory hierarchy due to the required storage. So,
we reuse both ﬂoating-point expansions and long accumulators for computing
multiple elements of the resulting matrix.
Our implementations attempt to get the maximum performance by using
all resources of the considered GPU architectures: SIMD instructions, fused
multiply-add, private and local memory as well as atomic instructions. We devel-
oped both unique and hand-tuned OpenCL implementations for NVIDIA and
AMD GPUs.
We use a long accumulator of ﬁnite length that represents the whole range of
double precision ﬂoating-point numbers (4196 bits in case of binary64). We use
such a long accumulator to avoid partial over/underﬂow that may occurs while
accumulating partial product of the same sign. For instance, for matrices of size
n × n, only n partial-products need to be summed per resulting element, which
leads to only log2(n) carry bits. With matrix size of 220 × 220 that requires 8
Terabytes, only 20 extra bits are necessary to ensure that this phenomena will
not occur.
4.2
Performance Results
As a baseline we consider the vectorized and parallelized non-deterministic dou-
ble precision matrix multiplication. We prefer our tuned implementation to the
one from cuBLAS and base our ExGEMM on it, because cuGEMM squeezes
every percent of the architecture performance and does not leave a room for our
approach. Figures 3a and b present the measured time achieved by the matrix
multiplication algorithms as a function of the matrix size n on two GPUs, see
Table 1. Apart from “Parallel DGEMM”, all implementations are ours: “Super-
acc” corresponds to our matrix multiplication algorithm that is solely based
on long accumulators and it is the slowest due to its extensive memory usage;
“FPEp + Superacc” stands for algorithm with ﬂoating-point expansions of size
p (p = 3 : 8) in conjunction with error-free transformations and long accumu-
lators; “FPE4EE + Superacc” represents an optimized version of the expansion

Reproducible and Accurate Matrix Multiplication
135
Fig. 3. The matrix-matrix multiplication performance results on GPUs, see Table 1.
of size 4 with the early-exit technique. The implementations with expansions
obtain better performance than with long accumulators only. However, due to
switching to the long accumulator at the ﬁnal stage of computing each element
of the resulting matrix C as well as when the accuracy of expansions is not
enough, the performance of implementations with expansions is bounded and it
is at most 12 and 16 times oﬀthe DGEMM’s performance on NVIDIA and AMD
GPUs, respectively. We believe that there is a possibility to tune these prelim-
inary implementations in order to be within 10 times slower. Nevertheless, our
matrix multiplication algorithm delivers constantly reproducible and accurate
results.
5
Conclusions and Future Work
xGEMM is the core of the BLAS library and all the other BLAS-3 routines are
virtually built on top of it. Furthermore, the development and automatic gener-
ation of linear algebra algorithms are driven by the goal of achieving best per-
formance on various architectures. One step towards this goal is made by using
blocked versions of algorithms that are capable to obtain much higher perfor-
mance compared to non-blocked algorithmic variants. This is achieved thanks
to the usage of BLAS-3 routines, in particular xGEMM. Understanding such
importance of the matrix multiplication routine, we targeted xGEMM and for
the ﬁrst time delivered a multi-level reproducible approach along with implemen-
tations. Even though the performance, which corresponds to roughly 5 % of the
eﬃciency, can be argued (we think that a 10 times overhead at most for repro-
ducible compute-bound algorithms is reasonable), the output of ExGEMM is
consistently bit-wise reproducible and accurate, in terms of rounding-to-nearest,
independently of threads scheduling, instruction set, and data partitioning.
Our ultimate goal is to apply the multi-level approach to derive a repro-
ducible, accurate, and fast library for fundamental linear algebra operations –
like those included in the BLAS library – on new parallel architectures such as

136
R. Iakymchuk et al.
Intel Xeon Phi many-core processors and GPU accelerators. Moreover, we plan
to conduct a priori error analysis of the derived ExBLAS (Exact BLAS) routines.
More information on the ExBLAS project as well as its sources can be found
at https://exblas.lip6.fr.
Acknowledgement. This work undertaken (partially) in the framework of CALSIM-
LAB is supported by the public grant ANR-11-LABX-0037-01 overseen by the French
National Research Agency (ANR) as part of the “Investissements d’Avenir” program
(reference: ANR-11-IDEX-0004-02). This work was also (partially) supported by the
FastRelax project through the ANR public grant (reference: ANR-14-CE25-0018-01).
References
1. Whaley, R.C., Dongarra, J.J.: Automatically tuned linear algebra software. In:
Proceedings of the 1998 ACM/IEEE Conference on Supercomputing (CDROM).
Supercomputing 1998, 1–27. IEEE Computer Society (1998)
2. Goto, K., van de Geijn, R.A.: High-performance implementation of the level-3
BLAS. ACM Trans. Math. Softw. 35(1), 1–14 (2008)
3. Fabregat-Traver, D., Bientinesi, P.: Computing petaﬂops over terabytes of data:
the case of genome-wide association studies. ACM Trans. Math. Softw. 40(4),
27:1–27:22 (2014)
4. Bergman, K., al.: Exascale computing study: technology challenges in achieving
exascale systems. DARPA report, September 2008
5. Whitehead, N., Fit-Florea, A.: Precision & performance: Floating point and IEEE
754 compliance for NVIDIA GPUs. Technical report, NVIDIA (2011)
6. Corden, M.: Diﬀerences in ﬂoating-point arithmetic between Intel Xeon processors
and the Intel Xeon PhiTM coprocessor. Technical report, Intel (2013)
7. Doertel, K.: Best known method: Avoid heterogeneous precision in control ﬂow
calculations. Technical report, Intel (2013)
8. Kulisch, U., Snyder, V.: The exact dot product as basic tool for long interval
arithmetic. Computing 91(3), 307–313 (2011)
9. Demmel, J., Nguyen, H.D.: Fast reproducible ﬂoating-point summation. In: Pro-
ceedings of the 21st IEEE Symposium on Computer Arithmetic, Austin, Texas,
USA, pp. 163–172 (2013)
10. Collange, S., Defour, D., Graillat, S., Iakymchuk, R.: Full-Speed Deterministic
Bit-Accurate Parallel Floating-Point Summation on Multi- and Many-Core Archi-
tectures. Technical report HAL: hal-00949355, INRIA, DALI-LIRMM, LIP6, ICS,
February 2014
11. IEEE Computer Society: IEEE Standard for Floating-Point Arithmetic. IEEE
Standard 754–2008, August 2008
12. Higham, N.J.: Accuracy and stability of numerical algorithms, 2nd edn. Society
for Industrial and Applied Mathematics (SIAM), Philadelphia, PA (2002)
13. Muller, J.M., Brisebarre, N., de Dinechin, F., Jeannerod, C.P., Lef`evre, V.,
Melquiond, G., Revol, N., Stehl´e, D., Torres, S.: Handbook of Floating-Point Arith-
metic. Birkh¨auser, Boston (2010)
14. Li, X.S., Demmel, J.W., Bailey, D.H., Henry, G., Hida, Y., Iskandar, J., Kahan,
W., Kang, S.Y., Kapur, A., Martin, M.C., Thompson, B.J., Tung, T., Yoo, D.J.:
Design, implementation and testing of extended and mixed precision BLAS. ACM
Trans. Math. Softw. 28(2), 152–205 (2002)

Reproducible and Accurate Matrix Multiplication
137
15. Hida, Y., Li, X.S., Bailey, D.H.: Algorithms for quad-double precision ﬂoating point
arithmetic. In: Proceedings of the 15th IEEE Symposium on Computer Arithmetic,
CA, USA, 155–162. IEEE Computer Society Press, Los Alamitos (2001)
16. Knuth, D.E.: The Art of Computer Programming. Seminumerical Algorithms, vol.
2, 3rd edn. Addison-Wesley, Boston (1997)
17. Matsumoto, K., Nakasato, N., Sakai, T., Yahagi, H., Sedukhin, S.G.: Multi-level
optimization of matrix multiplication for gpu-equipped systems. In: ICCS. Procedia
Computer Science, vol. 4, pp. 342–351. Elsevier (2011)

Outer Bounds for the Parametric Controllable
Solution Set with Linear Shape
Evgenija D. Popova(B)
Institute of Mathematics and Informatics, Bulgarian Academy of Sciences,
Acad. G. Bonchev Street, Block 8, 1113 Soﬁa, Bulgaria
epopova@bio.bas.bg
Abstract. We consider linear algebraic equations, where the elements of
the matrix and of the right-hand side vector are linear functions of inter-
val parameters, and their parametric AE-solution sets, which are deﬁned
by universal and existential quantiﬁers for the parameters. We present
how some suﬃcient conditions for a parametric AE-solution set to have
linear boundary can be exploited for obtaining sharp outer bounds of
that parametric AE-solution set. For a parametric controllable solution
set having linear boundary we present a numerical method for outer
interval enclosure of the solution set. The new method has better prop-
erties than some other methods available so far.
Keywords: Interval linear systems · Parameter dependencies · AE-
solution set · Controllable solution set · Solution enclosure · Iteration
method
1
Introduction
Consider linear algebraic systems involving linear dependencies between a num-
ber of interval parameters p = (p1, . . . , pK)⊤∈p = (p1, . . . , pK)⊤
A(p)x = b(p)
A(p) := A0 +
K

k=1
pkAk,
b(p) := b0 +
K

k=1
pkbk,
(1)
where Ak ∈Rm×n, bk ∈Rm, k = 0, . . . , K, and Rm×n is the set of real m × n
matrices, Rm := Rm×1 denotes the set of real vectors with m components.
A real compact interval is a = [a1, a2] := {a ∈R | a1 ≤a ≤a2; a1, a2 ∈R}. By
IRm, IRm×n we denote the sets of interval m-vectors and interval m×n matrices,
respectively.
We consider the parametric AE-solution sets of the system (1)
Σp
AE := {x ∈Rn | (∀pA ∈pA)(∃pE ∈pE)(A(p)x = b(p))} ,
(2)
This work is dedicated to the memory of Prof. Dr. Walter Kr¨amer (1952-2014), Univ.
of Wuppertal, Germany.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 138–147, 2016.
DOI: 10.1007/978-3-319-31769-4 12

Outer Bounds for the Parametric Controllable Solution Set
139
for the sets of indexes A, E such that A∪E = {1, . . . , K}, A∩E = ∅. For a given
index set Π = {π1, . . . , πk}, pΠ denotes (pπ1, . . . , pπk). Among the AE-solution
sets most studied and of particular practical interest are: the (parametric) united
solution set
Σuni(A(p), b(p), p) := {x ∈Rn | (∃p ∈p)(A(p)x = b(p))},
the (parametric) tolerable solution set
Σ(A(pA), b(pE), p) := {x ∈Rn | (∀pA ∈pA)(∃pE ∈pE)(A(pA)x = b(pE))}
and the (parametric) controllable solution set
Σ(A(pE), b(pA), p) := {x ∈Rn | (∀pA ∈pA)(∃pE ∈pE)(A(pE)x = b(pA))}.
A parametric solution set is usually smaller [4, Theorem 5.6] than the solution
set of the corresponding nonparametric interval linear system. Therefore, the
former describes more precisely a physical phenomenon, whose model is a linear
algebraic system involving dependencies between interval model parameters; for
practical examples see [3,5] and the references given therein.
A nonempty parametric AE-solution set, in general, has a complicated struc-
ture, see [4]. Its boundary is deﬁned by parts of polynomials that may have arbi-
trary high degree. It is proven in [4] that the universally quantiﬁed parameters
contribute linearly to the boundary of a parametric AE-solution set and there-
fore only the existentially quantiﬁed parameters determine the shape (boundary)
of a nonempty parametric AE-solution set. On the other hand, the existentially
quantiﬁed parameters can be classiﬁed in two groups: parameters which con-
tribute linearly to the boundary of a solution set and parameters which deter-
mine the nonlinear boundary of a parametric AE-solution set. Recently, in [5],
some suﬃcient conditions for an existentially quantiﬁed parameter to contribute
linearly to the boundary of a parametric united solution set were proven. Based
on these conditions, the scope of applicability of an eﬃcient interval method [3]
ﬁnding outer bounds for the parametric united solution set with linear shape
was greatly expanded.
The goal of the present work is two-fold:
(a) to present the applicability of the above suﬃcient conditions for obtaining
sharp outer bounds of a parametric AE-solution set;
(b) to generalize the interval method, proposed in [3], for parametric controllable
solution sets.
The paper is organized as follows. Section 2 introduces some notions that will
be used. Section 3 discusses the parametric AE-solution sets with linear shape
and the goal (a). A new interval method for outer enclosure of the parametric
controllable solution set with linear shape is presented in Sect. 4 together with
some illustrative examples. The paper ends by some conclusions.

140
E.D. Popova
2
Theoretical Background
For a = [a1, a2], deﬁne mid-point ˇa := (a1 + a2)/2, radius ˆa := (a2 −a1)/2,
width (diameter) ω(a) := 2ˆa, magnitude (absolute value) |a| and mignitude
⟨a⟩by
|a| := max{|a1|, |a2|}
⟨a⟩:= min{|a1|, |a2|} if 0 ̸∈a, ⟨a⟩:= 0 otherwise.
These functions are applied to interval vectors and matrices componentwise.
Without loss of generality and in order to have a unique representation of the
parameter dependencies, we assume that ˆpk > 0 for all 1 ≤k ≤K. For a
bounded Σp
AE ̸= ∅, □Σp
AE := {x ∈IRn | x ⊇Σp
AE}.
In order to simplify the presentation, in Sect. 4 we use the arithmetic on
proper and improper intervals [1,2], called Kaucher complete arithmetic or modal
interval arithmetic, and its properties. The set of proper intervals IR is extended
in [2] by the set IR := {[a1, a2] | a1, a2 ∈R, a1 ≥a2} of improper intervals
obtaining thus the set IR  IR = {[a1, a2] | a1, a2 ∈R} of all ordered couples of
real numbers called also generalized intervals. The conventional interval arith-
metic and lattice operations, order relations and other interval functions are
isomorphically extended onto the whole set IR  IR, [2]. Modal interval analysis
imposes a logical-semantic background on generalized intervals (considered there
as modal intervals) and allows giving a logical meaning to the interval results,
see [1] for more details.
An element-to-element symmetry between proper and improper intervals is
expressed by the “Dual” operator, Dual(a) := [a2, a1] for a = [a1, a2] ∈IR  IR.
Dual is applied componentwise to vectors and matrices. For a, b ∈IR  IR
Dual(Dual(a)) = a,
Dual(a ◦b) = Dual(a) ◦Dual(b), ◦∈{+, −, ×, /},
a ⊆b ⇔Dual(a) ⊇Dual(b).
(3)
The generalized interval arithmetic structure possesses group properties with
respect to the operations addition and multiplication of intervals that do not
involve zero. For a, b ∈IR  IR, 0 ̸∈b
a −Dual(a) = 0,
b/Dual(b) = 1.
(4)
Lattice operations are closed with respect to the inclusion relation; handling of
norm and metric are very similar to norm and metric in linear spaces [2]. Mid-
point, radius, absolute value and mignitude are extended on generalized intervals
by the same formulae. For a ∈IR, ω(a) := 2|ˆa|. For
b ∈IR

IR,
b ̸= 0,
0 ̸∈interior of

b
if b ∈IR
Dual(b)
if b ∈IR,
(5)
sgn(b) := {1 if b1, b2 ≥0, −1 if b1, b2 ≤0}. For a ∈IR, 0 ∈a, and b ∈IR
satisfying (5),
a ∗b = sgn(b)⟨b⟩a.
(6)

Outer Bounds for the Parametric Controllable Solution Set
141
3
Parametric AE-solution Sets with Linear Shape
Since the universally quantiﬁed parameters contribute linearly to the boundary
of a parametric AE-solution set, cf. [4], the theory about parametric united
solution sets with linear shape can be generalized to parametric AE-solution
sets with linear shape; the latter are also called polyhedral AE-solution sets,
cf. [9]. Theorem 2 and Lemma 1 from [5] imply the following theorem.
Theorem 1. A parameter pk, k ∈E, contributes linearly to the boundary of a
parametric AE-solution set if some of the following three equivalent conditions
holds true
(i) the nonzero elements of Akx −bk are linearly dependent
(ii) if Ak = 0 or the polynomial greatest common divisor (GCD) of the elements
of Akx −bk is a nonconstant linear polynomial of x1, . . . , xn
(iii) rank((Ak|bk)) = 1, where (Ak|bk) ∈Rm×(n+1) is the matrix obtained by
augmenting the columns of Ak with the vector bk.
The following theorem follows from the property of the universally quantiﬁed
parameters mentioned above and [5, Theorem 3].
Theorem 2. Let E = E1∪E2, E1∩E2 = ∅be such that Ak ̸= 0 for k ∈E1. Denote
k1 := Card(E1), k2 := Card(E2). Denote by gk(x) the GCD of the elements of
Akx and let gk(x) be a nonconstant linear polynomial for every k ∈E1. Deﬁne
L := (l1| . . . |lk1) ∈Rm×k1, where lk := Akx/gk(x) ∈Rm
R := (r1| . . . |rk1)⊤∈Rk1×n, where rk := (∂gk(x)
∂x1
, . . . , ∂gk(x)
∂xn
)⊤∈Rn.
If there exists tk ∈R such that tklk = bk := ∂b(p)/∂pk for every k ∈E1, then
A0x −b0 +

k∈E∪A
pk(Akx −bk) = LDRx −LDt −F(p1, . . . , pk2)⊤+
A0x −b0 +

k∈A
pk(Akx −bk),
where F := (b1| . . . |bk2) ∈Rm×k2, t = (t1, . . . , tk1)⊤and D = Diag(p1, . . . , pk1).
Theorem 2 contains Theorem 3 of [5] as a special case. The following corollary
is important for ﬁnding sharp outer bounds of parametric AE-solution sets.
Corollary 1 ([5], Corollary 4). For a bounded Σp
AE ̸= ∅,
inf{□Σp
AE}i
and
sup{□Σp
AE}i,
i = 1, . . . , n,
are attained at particular end-points of the intervals for the parameters that
contribute linearly to the boundary of the solution set.
For a given index set A, deﬁne the set BA of all end-points (vertices) of pA.

142
E.D. Popova
Proposition 1 ([6], Corollary 1). For a bounded parametric AE-solution set
Σp
AE ̸= ∅and a set B′
A, such that B′
A ⊆BA and Σ(A(˜pA, pE), b(˜pA, pE), pE) is
bounded for every ˜pA ∈B′
A, we have
□Σp
AE ⊆

˜pA∈B′
A
□Σ(A(˜pA, pE), b(˜pA, pE), pE).
Proposition 1 shows that outer bounds of a parametric AE-solution set can be
found by bounding only parametric united solution sets. Then, the methodology
for ﬁnding sharp bounds of parametric united solution sets with linear shape
applies to parametric AE-solution set via Proposition 1.
Let E = E1 ∪E2, E1 ∩E2 = ∅be such that pk, k ∈E1, satisfy Theorem 1.
Then,
□Σp
AE ⊆

˜pA∈B′
A
□
⎛
⎝

˜pE1∈BE1
□Σ(A(˜pA, ˜pE1, pE2), b(˜pA, ˜pE1, pE2), pE2)
⎞
⎠.
(7)
Note, that the above methodology can be applied to parametric AE-solution
sets such that not all existentially quantiﬁed parameters satisfy Theorem 1 or
contribute linearly to the boundary of the parametric AE-solution set. A larger
discussion is contained in [5].
4
Enclosure of Σctrl(A(p), b(q), p, q) with Linear Shape
Consider a parametric interval algebraic system
A(p)x = b(q),
p ∈p ∈IRKp, q ∈q ∈IRKq,
A(p) := A0 +
Kp

k=1
pkAk,
b(q) := b0 +
Kq

k=1
qkbk.
(8)
We assume that the structure of the dependencies between the parameters p is
such that the conditions deﬁned in Theorem 1 hold true for all the parameters
and, therefore, the system (8) has the equivalent representation
(A0 + LDiag(p)R) x = b0 + Fq,
p ∈p ∈IRKp, q ∈q ∈IRKq,
(9)
with suitable numerical matrices L, R, F found by Theorem 2.
We search for an outer interval enclosure of the parametric controllable solu-
tion set
Σp
ctrl = Σctrl(A(p), b(q), p, q)
:= {x ∈Rn | (∀q ∈q)(∃p ∈p)(A(p)x = b(q))}.

Outer Bounds for the Parametric Controllable Solution Set
143
4.1
Iteration Method
Theorem 3. If x ∈Σp
ctrl ̸= ∅for the system (9), D0 ∈Diag(p) and A0 +LD0R
is invertible, then x ∈(A0 + LD0R)−1(b0 + FDual(q) + Ld), wherein
d := (D0 −Diag(p))Rx,
(10)
and b0+FDual(q)+Ld is a proper interval vector in Kaucher interval arithmetic.
Proof. If x ∈Σp
ctrl ̸= ∅, according to [7, Theorem 3.2] we have
(A0 + LDiag(p)R) x ⊇b0 + Fq.
(11)
We apply the Dual operator to the above inclusion relation, the relation
(3), and the distributivity of multiplication by a point vector x. Then, we
add −LDiag(p)Rx to both sides of the obtained inclusion. Due to (4) we obtain
A0x ⊆b0 + FDual(q) −LDiag(p)Rx.
(12)
The relation (10) implies −Diag(p)Rx = d −D0Rx which we substitute in (12)
and obtain
(A0 + LD0R)x ⊆b0 + FDual(q) + Ld.
The inclusion (11), which holds true for x ∈Σp
ctrl ̸= ∅, implies ω(Fq) ≤ω(Ld).
The latter implies that b0 + FDual(q) + Ld is a proper interval vector since Ld
is a proper interval vector and FDual(q) is an improper one. Due to invertibility
of A0 + LD0R, we obtain x ∈(A0 + LD0R)−1(b0 + FDual(q) + Ld).
Theorem 4. Let D0 ∈RKp×Kp, D0 ∈Diag(p) = D be such that A0 + LD0R
is invertible and put
C := (A0 + LD0R)−1 .
Deﬁne
w′ := w −|D0 −D| |RCL|w,
(13)
w′′ := |D0 −D|⟨RCb0 + (RCF)Dual(q)⟩,
(14)
for some vector w ≥0, and
u := [−αw, αw],
α = max
i
w′′
i
w′
i
.
(15)
(i) x = x(p, q) ∈Σp
ctrl ̸= ∅for (9) is related to y = Rx(p, q) by the inclusions
x ∈Cb0 + (CF)Dual(q) + CLd,
(16)
y ∈RCb0 + (RCF)Dual(q) + (RCL)d,
(17)
where
d = (D0 −D)y.
(18)
(ii) If w′ > 0 and 0 ̸∈b0 + Fq, then d ⊆u.
(iii) If x := Cb0 + (CF)Dual(q) + (CL)u is a proper interval vector, then
every x ∈Σp
ctrl satisﬁes x ∈x.

144
E.D. Popova
Proof. (i) follows from Theorem 3.
(ii) Since w′ > 0 we put
β = max
i
|di|/wi
and note that |d| ≤βw, with equality in some component i. The deﬁnition of α
and 0 ̸∈b0 + Fq imply 0 ≤w′′ ≤αw′. From (16)–(18) and a subdistributive law
in Kaucher arithmetic we have
d ⊆(D0 −D)(RCb0 + (RCF)Dual(q) + (RCL)d)
⊆(D0 −D)(RCb0 + (RCF)Dual(q)) + (D0 −D)(RCL)d.
Then, formula (6) implies
|d|
≤
|D0 −D|⟨RCb0 + (RCF)Dual(q)⟩+ |D0 −D||RCL|βw
(14),(13)
≤
w′′ + β(w −w′) ≤αw′ + β(w −w′).
Thus βwi = |di| ≤αw′
i + β(wi −w′
i), hence βw′
i ≤αw′
i. As w′ > 0, we conclude
that β ≤α, and d ⊆u follows.
(iii) follows by (ii) and Theorem 3.
In the computations we take D0 as the midpoint of D, and w, e.g., as the
vector with all entries one. In order to provide guaranteed enclosures, w′ should
be rounded downward, w′′ and α should be rounded upward. If w′ ≤0 in (13),
we may apply the approach proposed in [3] to compute the largest eigenvalue ϱ
(= the spectral radius) of the nonnegative matrix
M := |D0 −D||RCL|.
If ϱ < 1, any w > 0 suﬃciently close to an associated eigenvector makes w′ > 0.
In practice, one could run a Lanczos iteration and stop as soon as an intermediate
eigenvector approximation w > 0 satisﬁes Mw < w, [3].
The computed initial interval enclosure u of d can be further improved by
iterating and intersecting with the previously computed enclosures. It is suﬃcient
to iterate the enclosures for y and d, and compute the enclosures for x when the
intersected results no longer improve signiﬁcantly. Thus we iterate
y = {(RCb0) + (RCF)Dual(q) + (RCL)u)} ∩y,
u = {(D0 −D)y} ∩u
until some stopping test holds, and then get the enclosure
x := (Cb0) + (CF)Dual(q) + (CL)u
for all x that belong to Σp
ctrl of (9). In the implementation of the method we
used the stopping criterion proposed in [3], namely, the iteration stops when the
sum of widths of the components of u does not improve by a factor of 0.999 or
after at most 10 iterations.
The method presented here can be considered as an extension of the so-called
formal (algebraic) approach, cf. [10], for enclosing nonparametric AE-solution
sets to parametric controllable solution sets.

Outer Bounds for the Parametric Controllable Solution Set
145
4.2
Numerical Examples
Here we illustrate the advantages of the above parametric iterative method by
some numerical examples and compare this method to the only discussed by
now methods for outer enclosure of the parametric controllable solution set pre-
sented in [6]. The implementations and the numerical computations are done
in the environment of Mathematica® using the package directed.m [8]. The
latter package supports the arithmetic of proper and improper intervals and
provides compatibility with the conventional interval arithmetic supported by
the Mathematica® kernel. The numerical computations are done exactly if all
input data are represented exactly (e.g., by rational numbers) or by appropri-
ate directed rounding in ﬂoating-point if some input data are in ﬂoating point
arithmetic. This software environment and the implementation provide obtain-
ing numerical interval vectors which are guaranteed to contain the considered
parametric solution set.
The iteration method, proposed in Sect. 4.1, can be implemented in any soft-
ware environment which does not support the arithmetic of proper and improper
intervals if the lower and upper bounds of the corresponding intervals are com-
puted separately applying the corresponding formulae for the arithmetic oper-
ations and applying correct directed rounding in ﬂoating-point arithmetic. The
iteration method from [3] is implemented in the environment of Matlab, see [3],
and in C-XSC, see [11].
Explicit representation of any parametric controllable solution set, considered
below, is obtained by methods from [4] as a system of real inequalities in the
coordinate variables, which is then solved by suitable Mathematica® functions.
Example 1. Find an enclosure of the parametric controllable solution set to the
system

p1 + p2, p1 −p2
p1 −p2, p1 + p2

x =

3/2 + q
q

,
where p1, p2
∈
[1/2, 3/2], q
∈
[−1/10, 1/10]. According to Theorem 1 the
parametric controllable solution set has linear shape and the application of
Theorem 4 with an iterative reﬁnement gives the following (rounded outward)
interval enclosure

[0.099951, 1.400049]
[−0.650049, 0.650049]

.
Since the parametric matrix is not strongly regular, the parametric Bauer-Skeel
method from [6, Corollary 6] fails. Due to the same reason Proposition 1 cannot
be applied together with the parametric Bauer-Skeel method while it can be
applied together with Theorem 4.
Example 2. Find an enclosure of the parametric controllable solution set to
 p1 + p2, p1 −2p2
p1 −p2/2, p1 + p2

x =
3/2 + q/3
q/2

,

146
E.D. Popova
where p1 ∈[1, 3/2], p2 ∈[−1, −1/2], q ∈[9/10, 11/10]. According to Theorem 1
the parametric controllable solution set has linear shape and the application of
Theorem 4 with an iterative reﬁnement gives the interval enclosure
([−0.17779, 0.39507], [0.39875, 0.89507])⊤,
while the parametric Bauer-Skeel method from [6] gives the enclosure
([−0.32840, 0.54568], [0.29135, 0.1.00247])⊤.
The former interval enclosure overestimates the exact interval hull
([−59
405, 3
10], [421
810, 239
270])⊤
of the parametric controllable solution set by (22.2, 26.4)⊤%, while the latter
enclosure overestimates the hull by (49, 48.6)⊤%. In general, the percentage of
overestimation depends on the particular problem (parameter dependencies),
the problem size, the number of the parameters, and the width of the parameter
intervals. The parametric Bauer-Skeel method can be applied to the above sys-
tem, where the parameter intervals p1, p2 are with enlarged radius from r = 1/4
(the intervals considered above) to a radius r = 0.481 which still provides strong
regularity of the parametric matrix. In the latter case the overestimation of
the corresponding exact interval hull is (99.81, 99.79)⊤%. The method from
Theorem 4 is applicable to the above system where the parameter intervals
p1, p2 have radius r = 0.749 still providing regularity of the parametric matrix.
The overestimation of the corresponding exact interval hull for these intervals is
(35.86, 37.94)⊤%.
Further improvement of an enclosure obtained by some applicable enclosure
method (or by some method for obtaining the exact interval hull of Σp
uni with
linear shape) could be obtained by Proposition 1, respectively (7), at the expense
of a bigger computational eﬀort. The application of (7) to Example 2 with the
initial parameter intervals gives an interval enclosure which overestimates the
exact interval hull of the parametric controllable solution set by (0, 5.7)⊤%.
In the following example, we consider the behaviour of the proposed method
on parametric controllable solution sets that are empty sets or unbounded.
Example 3. Consider the parametric linear system
p1 −p2, p2
p1 + p2, −p2

x =
2q
2q

.
(a) For p1 ∈[3/4, 5/4], p2 ∈[0, 1], q ∈[1, 2], Σp
ctrl = ∅. The application of
the method considered above yields w′ < 0 and the largest eigenvalue (= the
spectral radius) of the matrix |D0 −D||RCL| equal to 1.
(b) With the same data as in (a) but twice bigger radius of p1, p1 ∈[1/2, 3/2],
the corresponding Σp
ctrl is unbounded and deﬁned by 8/3 ≤x1 ≤4, x2 ∈R. The
application of the method proposed in this paper yields the same output as in (a).

Outer Bounds for the Parametric Controllable Solution Set
147
The method, proposed in this paper, for bounding parametric controllable
solution sets with linear shape possesses the same scalability property as the
methods from [3,5] for bounding parametric united solution sets. Examples of
large parametric linear systems with over 5000 variables and over 10 000 parame-
ters which appear in ﬁnite element analysis of uncertain truss structures can be
found in [3], while [5] presents some examples coming from modeling of electrical
circuits and models in biology.
5
Conclusion
We presented a new interval method for outer enclosure of a class of nonempty
and bounded parametric controllable solution sets with linear shape. Contrary
to other available so far interval methods for bounding general parametric con-
trollable solution sets, which require strong regularity of the parametric matrix,
the new method does not have such a restriction. Furthermore, the method is
applicable to parametric linear systems of high dimensions that involve many
parameters, see [3], and when the parameter intervals are large, see the end of
Example 2. Further improvement of the solution enclosure may be achieved by
methods presented in Sect. 3 and [5, Sect. 2].
References
1. Sainz, M.A., Armengol, J., Calm, R., Herrero, P., Jorba, L., Vehi, J.: Modal Interval
Analysis: New Tools for Numerical Information. Lecture Notes in Mathematics, vol.
2091. Springer, Switzerland (2014)
2. Kaucher, E.: Interval analysis in the extended interval space IR. Computing, Suppl.
2, 33–49 (1980). http://www.math.bas.bg/∼epopova/Kaucher-80-CS 33-49.pdf
3. Neumaier, A., Pownuk, A.: Linear systems with large uncertainties, with applica-
tions to truss structures. Reliable Comput. 13, 149–172 (2007)
4. Popova, E.D.: Explicit description of AE-solution sets for parametric linear sys-
tems. SIAM J. Matrix Anal. Appl. 33, 1172–1189 (2012)
5. Popova, E.D.: Improved enclosure for some parametric solution sets with linear
shape. Comput. Math. Appl. 68(9), 994–1005 (2014)
6. Popova, E.D., Hlad´ık, M.: Outer enclosures to the parametric AE-solution set. Soft
Comput. 17, 1403–1414 (2013)
7. Popova, E.D., Kr¨amer, W.: Characterization of AE-solution sets to a class of para-
metric linear systems. C. R. Acad. Bulgare Sci. 64(3), 325–332 (2011)
8. Popova, E.D., Ullrich, C.: Directed interval arithmetic in Mathematica: implemen-
tation and applications. Technical report 96-3, Universit¨at Basel (1996). http://
www.math.bas.bg/∼epopova/papers/tr96-3.pdf
9. Sharaya, I.A.: Boundary intervals method for visualization of polyhedral solution
sets. Comput. Technol. 20(1), 75–103 (2015). (in Russian)
10. Shary, S.P.: A new technique in systems analysis under interval uncertainty and
ambiguity. Reliable Comput. 8(5), 321–418 (2002)
11. Zimmer, M.: Software zur hocheﬃzienten Loesung von Intervallgleichungssystemen
mit C-XSC, Ph.D. thesis, Bergische Universit¨at Wuppertal (2013)

Reserve of Characteristic Inclusion
as Recognizing Functional for Interval
Linear Systems
Irene A. Sharaya and Sergey P. Shary(B)
Institute of Computational Technologies, 6, Lavrentiev ave., Novosibirsk, Russia
shary@ict.nsc.ru
http://www.nsc.ru/interval/sharaya
Abstract. The paper considers the interval linear inclusion Cx ⊆d in
the Kaucher interval arithmetic. We introduce a quantitative measure of
its fulﬁllment, called “reserve”, and investigate its properties and applica-
tion. We show that the reserve proves useful in the study of AE-solutions
and quantiﬁer solutions to interval linear problems. In particular, using
the reserve can help to recognize position of a point with respect to the
solution set, emptiness of the solution set and of its interior, etc.
Keywords: Interval linear system · AE-solutions · Quantiﬁer solutions ·
Solution set · Characteristic inclusion · Reserve · Recognizing functional
1
Introduction
Let KR = {[v, v] | v, v ∈R} be the set of Kaucher intervals, and KR =
{[v, v] | v, v ∈R} be the set of Kaucher intervals over the extended real axis
R = R ∪{−∞, ∞} (see [5,6]). It makes sense to remind that the Kaucher com-
plete interval arithmetic KR, apart from usual (proper) intervals, also includes
improper intervals [z, z] with z > z.
Our object under study is the interval linear inclusion
Cx ⊆d,
(1)
where x ∈Rn, C is an m × n-matrix with the elements from KR, d is an
m-vector made up of the extended intervals from KR. We introduce and investi-
gate a quantitative measure, called “reserve”, of the fulﬁllment of inclusion (1).
Then an answer to the question ‘What can the reserve serve for?’ is given.
The set of (formal) solutions to inclusion (1) is naturally deﬁned as
Ξ =

x ∈Rn | Cx ⊆d

=

x ∈Rn 

j Cijxj ≥di, 
j Cijxj ≤di, i = 1, 2, . . . , m

.
The inclusion Cx ⊆d in the Kaucher arithmetic and its solutions prove use-
ful for many purposes. First of all, they provide a tool for uniﬁed treatment
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 148–167, 2016.
DOI: 10.1007/978-3-319-31769-4 13

Reserve of Characteristic Inclusion as Recognizing Functional
149
of so-called AE-solutions and quantiﬁer solutions to interval linear problems.
In the next section, we remind brieﬂy the corresponding concepts and main
results (see details in [13,15,16]).
Our notation follows mainly the informal standard [7].
2
Quantiﬁer Solutions and AE-solutions
In practice, we usually consider intervals
• from the set of proper intervals IR =

v = [v, v] | v, v ∈R, v ≤v

and
• in connection with a certain property P(v) that can be either fulﬁlled or
not fulﬁlled for the point members v of the intervals.
For instance, the property P may have the form “to be a solution to an equa-
tion”, “to be a solution to a problem” with some parameters that can take values
from prescribed intervals, and so on. Then the following diﬀerent situations may
occur:
(1) either the property P(v) holds for all members v from the given interval v,
(2) or the property P(v) holds only for some members v from the interval v,
not necessarily all, or even for a single value from v.
Formally, the above distinction can be expressed by logical quantiﬁers:
−In the ﬁrst case, we write “(∀v ∈v) P(v) ”
and speak of interval A-uncertainty,
−In the second case, we write “(∃v ∈v) P(v) ”
and speak of interval E-uncertainty.
(2)
We thus have to distinguish between the two above types of interval uncer-
tainty. The quantiﬁer solutions and AE-solutions to interval system of relations
are the solutions that take into account the diﬀerence between the A-type and
E-type of uncertainty in the input interval parameters [13,15,16].
Let us consider an interval system of relations
F(a, x) σ b,
(3)
where F =

F1(a, x), F2(a, x), . . . , Fm(a, x)
	⊤is a vector-function with some
mappings Fi : Rl × Rn →R as components, a ∈IRl, x ∈Rn, σ ∈{=, ≤, ≥}m,
b ∈IRm. Taking into account our conclusion about uncertainty types, we can
assume that the property determining “solutions” to system (3) should look like
(Q1vπ1 ∈vπ1)(Q2vπ2 ∈vπ2) · · · (Ql+mvπl+m ∈vπl+m)

F(a, x) σ b
	
,
(4)

150
I.A. Sharaya and S.P. Shary
where
Q1, Q2, . . . , Ql+m are the logical quantiﬁers “∀” or “∃”,
( v1, v2, . . . , vl+m) := ( a1, a2, . . . , al, b1, b2, . . . , bm) ∈Rl+m
is an aggregated parameter vector,
( v1, v2, . . . , vl+m) := ( a1, a2, . . . , al, b1, b2, . . . , bm) ∈IRl+m
is an aggregated interval vector of their possible values,
( π1, π2, . . . , πl+m) is a permutation of the positive integer
numbers 1, 2, . . . , l + m.
We will call the logical formula (4) selecting predicate of the solutions to (3).
A vector y will be referred to as quantiﬁer solution to the interval system
of relations F(a, x) σ b if the selecting predicate (4) is true for x = y. This is a
very general construction that can describe a great variety of speciﬁc solutions
(their total number far exceeding 2l+m). Sometimes, it makes sense to somehow
restrict the generality by specializing the form of the selecting predicate.
A quantiﬁer solution to the interval system of relations for which, in the
selecting predicate, all occurrences of the universal quantiﬁer “∀” precede those
of the existential quantiﬁer “∃” will be referred to as AE-solution. The AE-
solutions are thus a particular case of the quantiﬁer solutions obtained by ﬁxing a
certain order of the logical quantiﬁers in the logical formula (selecting predicate)
that determines the solution.
For the last two decades, since their introduction in [15], the AE-solutions
have been the area of active research. They have a clear practical interpretation
as solutions to single-step decision making processes with intervally deﬁned data
when we have to choose a compromise between perturbations (expressed by
interval parameters with A-uncertainty) and our controls (expressed by interval
parameters with E-uncertainty) [16]. A generalization of the concept of AE-
solutions has penetrated into fuzzy sets theory and its applications (see [2]).
The interested reader can ﬁnd a lot of further results on AE-solutions, e.g., in
the works [3,4,8,9,11,12] and others.
The inclusion Cx ⊆d arises in connection with quantiﬁer solutions and
AE-solutions to the systems of interval linear relations of the form
Ax σ b,
where σ ∈{=, ≤, ≥}m, A ∈IRm×n, x ∈Rn, and b ∈IRm. If the quanti-
ﬁer matrix A ∈{∀, ∃}m×n and the quantiﬁer vector β ∈{∀, ∃}m specify the
uncertainty types of the separate interval parameters Aij, bi for all i and j, we
introduce the auxiliary interval matrices A∀, A∃and vectors b∀, b∃as follows:
A∀
ij :=

Aij,
if Aij = ∀,
0,
if Aij = ∃,
b∀
i :=

bi,
if βi = ∀,
0,
if βi = ∃,
A∃
ij :=

Aij,
if Aij = ∃,
0,
if Aij = ∀,
b∃
i :=

bi,
if βi = ∃,
0,
if βi = ∀.
(5)

Reserve of Characteristic Inclusion as Recognizing Functional
151
Then the AE-solution set to the interval linear system of equations Ax = b can
be alternatively deﬁned as the set
ΞAβ(A, b) =

x ∈Rn |
(∀A′ ∈A∀) (∀b′ ∈b∀) (∃A′′ ∈A∃) (∃b′′ ∈b∃)

(A′ + A′′) x = b′ + b′′	
.
Besides, the following equivalent characterization is valid in the Kaucher interval
arithmetic [16]:
x ∈ΞAβ(A, b)
⇐⇒

A∀+ dual A∃	
x ⊆dual b∀+ b∃,
(6)
where “dual” means dualization operator dual : KR →KR reverting the interval
endpoints, i. e., such that dual [ z, z ] = [ z, z ]. Inclusion (6), which coincides with
(1) in form, is called characteristic inclusion for the AE-solution set determined
by the uncertainty distribution (5) over the interval elements of the system
Ax = b. See examples in Sect. 3.
The next important particular case is quantiﬁer solutions to the systems of
interval linear inequalities
Ax ≤b
or
Ax ≥b,
(7)
where A ∈IRm×n, x ∈Rn, and b ∈IRm. A remarkable fact about the interval
linear systems of inequalities is that the order of logical quantiﬁers in the select-
ing predicate does not matter, and general quantiﬁer solutions coincide with
AE-solutions for interval linear inequalities [13]. Speciﬁcally, if Q(A, b, A, β) is
a quantiﬁer preﬁx of the selecting predicate, made up of the quantiﬁer terms
that correspond to individual interval parameters, then
Q(A, b, A, β)(Ax ≥b)
⇐⇒

A∀+ dual A∃	
x ⊆

b
∀+ b∃, +∞

,
Q(A, b, A, β)(Ax ≤b)
⇐⇒

A∀+ dual A∃	
x ⊆

−∞, b∀+ b
∃
;
see details in [13].
As a particular case of the inclusion Cx ⊆d, it is worth noting some special
quantiﬁer solutions to the interval linear system of relations Ax σ b. Let the
selecting predicate of the solutions be such that, for the interval parameters
from the equality relations, all the quantiﬁers “∀” precede all the quantiﬁers
“∃”. If Qσ(A, b, A, β) is the quantiﬁer preﬁx of the above selecting predicate,
then there holds (see [13]):
Qσ(A, b, A, β)(Ax σ b)
⇐⇒

A∀+ dual A∃	
x ⊆dual b∀+ b∃+ w,
where the interval m-vector w is deﬁned as
wi :=
⎧
⎨
⎩
0,
if σi is “=”,
[0, ∞],
if σi is “≥”,
[−∞, 0], if σi is “≤”.

152
I.A. Sharaya and S.P. Shary
To sum up, considering the inclusion Cx ⊆d in the Kaucher complete inter-
val arithmetic enables us to study all the particular cases of the quantiﬁer solu-
tions to interval linear inequality systems and of the AE-solutions to interval
linear systems of relations
▶simultaneously and in a uniform way,
▶by interval methods.
3
Deﬁnition and Main Properties of Reserve
By reserve of the inclusion Cx ⊆d (or just reserve), we call the maximal
number Rsv ∈R such that
Cx + [−Rsv , Rsv ] e ⊆d
for m-vector e = (1, 1, . . . , 1)⊤. Notice that, if Rsv < 0 , then [−Rsv , Rsv ] is
an improper interval, that is, all the arithmetic operations and relations are
understood in the Kaucher complete interval arithmetic.
From the above deﬁnition, one can easily deduce the following representation
for Rsv:
Rsv =
min
1≤i≤m min

Ci:x −di, −Ci:x + di

=
min
1≤i≤m min

Ci:x+ −Ci:x−−di, −Ci:x+ + Ci:x−+ di

=
min
1≤i≤m min
⎧
⎨
⎩
n

j=1
C−sgn xj
ij
xj −di, −
n

j=1
C sgn xj
ij
xj + di
⎫
⎬
⎭,
(8)
where x+, x−∈Rn
+, x+ = max{0, x}, x−= max{0, −x} are positive and nega-
tive parts of the vector x respectively, and for every i
C−sgn xj
ij
=

Cij,
if xj ≥0,
Cij,
otherwise,
C sgn xj
ij
=

Cij,
if xj ≥0,
Cij,
otherwise.
For ﬁxed C and d, we can consider the reserve as a functional of x, that is,
as a function Rsv (x) : Rn →R. Then it characterizes properties of the point
x with respect to the interval inclusion Cx ⊆d and interval linear systems of
relations described by this inclusion (see Sect. 2). From (8), it follows that the
functional Rsv (x) is deﬁned on the entire Rn. Also, it is evidently continuous
and even Lipschitz continuous.
We are reminded that the set of points x = (x1, x2, . . . , xn)⊤∈Rn having
a deﬁnite sign of each their component xj, supplemented with its boundary, is
called orthant of the space Rn.

Reserve of Characteristic Inclusion as Recognizing Functional
153
Proposition 1. The function Rsv (x) is concave in each orthant of Rn.
Proof. In every ﬁxed orthant of the space Rn, the values of C−sgn xj
ij
and C sgn xj
ij
are constant. Therefore, the last formula of (8) implies that the functional
Rsv (x) is the minimum of 2m linear functions

n

j=1
C−sgn xj
ij
xj −di

and

−
n

j=1
C sgn xj
ij
xj + di

,
i = 1, 2, . . . , m,
within any ﬁxed orthant.
Proposition 2. The function Rsv (x) is piecewise-linear.
Proof. It is almost obvious from representations (8) and the proof of the preced-
ing proposition.
As follows from Sect. 2, the reserve of inclusion (1) is a very general construction
that covers many particular instances of interval linear systems of relations and
a lot of their solution sets. Still, some special cases of the reserve have been suc-
cessfully applied in the earlier works on interval systems of equations, although
under the name of “recognizing functionals”.
Historically, the ﬁrst recognizing functional, Tol (x), was proposed for the tol-
erable solution set to interval linear systems of equations [14]. Given an interval
linear system Ax = b, its tolerable solution set is deﬁned as the set
Ξtol = { x ∈Rn | (∀A ∈A)(∃b ∈b)(Ax = b)},
(9)
formed by all such x’s that the product Ax falls into b for any possible A ∈A.
The functional
Tol(x) = min
i

rad bi −
 mid bi −Ai:x
 
(10)
is the reserve of the corresponding characteristic inclusion Ax ⊆b (see [14] for
further details and history of the problem).
For the interval system of linear equations Ax = b, the united solution set
is known to be deﬁned as
Ξuni = { x ∈Rn | (∃A ∈A)(∃b ∈b)(Ax = b)},
(11)
being the solution set for the collection of all the systems Ax = b with A ∈A
and b ∈b. So, the selecting predicate is (∃A ∈A)(∃b ∈b)(Ax = b), and the
corresponding characteristic inclusion is (dual A) x ⊆b. Its reserve as a function
of x coincides with the recognizing functional
Uss(x) = min
i

rad bi + (rad Ai:) |x| −
 mid bi −(mid Ai:) x
 
introduced in [19,20] (see also [18]).

154
I.A. Sharaya and S.P. Shary
Fig. 1. Graph of the reserve (recognizing functional Uss) for system (12)
Example 1. For the united solution set to the interval linear equation system

[2, 4] [−1, 1]
[−1, 1] [2, 4]

x =

[−3, 3]
0

,
(12)
the functional Rsv (x) (i. e., Uss deﬁned by (11)) has the graph depicted at Fig. 1
(see also [18]).
For what purpose can one use the reserve? We will show that, using the
reserve as a functional of x, one can get extensive information on
position of a point with respect to the solution set,
whether the solution set Ξ is empty or not,
whether the interior of Ξ is empty or not,
the ‘best’ points for the inclusion Cx ⊆d.
4
Properties of the Solution Set
Prior to formulating the main results of our paper, we need to revise some
geometric and topological properties of the solution set Ξ to the interval linear
inclusion Cx ⊆d.

Reserve of Characteristic Inclusion as Recognizing Functional
155
The intersection of the solution set Ξ with each orthant of the space Rn is
a convex polyhedral set. This fact is well-known for particular solution sets to
interval linear relations (see, e. g., [1,14,16]), but it does not hurt to outline its
substantiation for the general inclusion Cx ⊆d.
The membership of a vector y in an orthant of Rn is determined by ﬁxing the
signs of its components yi, i = 1, 2, . . . , n. Also, for any interval m × n-matrix
C, the components of the product Cy = ( (Cy)1, (Cy)2, . . . , (Cy)m)⊤can be
represented as
(Cy)i =
n

j=1
Cijyj =

n

j=1
Cijyj ,
n

j=1
Cijyj

=

n

j=1
C′
ijyj ,
n

j=1
C′′
ijyj

,
(13)
where C′
ij and C′′
ij are numbers from the endpoint set { Cij, Cij}, and they
are ﬁxed for any separate orthant containing y. Next, writing out the inclusion
Cy ⊆d in a componentwise manner and changing, on the basis of (13), each
one-dimensional inclusion to two inequalities between the interval endpoints, we
arrive at a system of 3n linear inequalities
⎧
⎨
⎩
C′y ≥d,
C′′y ≤d,
conditions on the signs of yj, j = 1, 2, . . . , n,
(14)
where C′, C′′ are point matrices formed by endpoints of the entries of C. Each
non-strict inequality of the system (14) determines a closed half-space of Rn,
and the solution set to the entire system is the intersection of the half-spaces,
that is, a convex polyhedral set in the space Rn.
Fig. 2. Disconnected solution set to the interval linear inclusion (15)

156
I.A. Sharaya and S.P. Shary
Example 2. For the interval linear inclusion system

1
0
[2, −2] [−3, −1]

x ⊆
[−2, 2]
2

,
(15)
the solution set is depicted at Fig. 2. It is polyhedral within each orthant, but
disconnected as a whole.
Proposition 3. A point belongs to the interior of the solution set Ξ if and only
if every suﬃciently small perturbation along each coordinate axis do not cause
the point to leave the solution set. To be more speciﬁc,
y ∈int Ξ
⇔
y ∈Ξ and (∃Δ > 0)(∀j ∈{1, 2, . . . , n})(∀|ε| < Δ)

x + εej ∈Ξ
	
,
(16)
where ej = (0, . . . , 0, 1, 0, . . . , 0)⊤is the vector with the only nonzero component
at the j-th place, i.e., the unit vector of the coordinate axis 0xj.
Proof. In equivalence (16), the direct (downward) implication is obvious, and,
in fact, we have to prove only the upward implication. It will follows from the
convexity of the solution set Ξ within every orthant of the space Rn.
Our substantiation of the upward implication in (16) is constructive. Assum-
ing that the point y complies with the “coordinate-wise stability”, i. e., there
exists such Δ > 0 that y ± εej ∈Ξ for every ε satisfying |ε| < Δ and each
j = 1, 2, . . . , n, we explicitly produce a neighborhood of y that is entirely con-
tained in the solution set Ξ.
Fig. 3. Illustration of the proof of Proposition 3

Reserve of Characteristic Inclusion as Recognizing Functional
157
Without loss in generality, we suppose that the value of Δ is taken so that
the signs of nonzero components yj are preserved in all the perturbations y±εej,
|ε| < Δ. Otherwise, we can always decrease the positive Δ to meet the above
requirement.
We take the convex hull S of the 2n + 1 points y, y ± εej, j = 1, 2, . . . , n
(see Fig. 3). Since the unit vectors ej are n linearly independent vectors, then
the convex set S also has the dimension n. Hence, S is a bodily convex set in
Rn, having nonempty interior [10]. Then S must contain an open ball centered
at y. As a consequence, the proof of Proposition 3 is completed if we show that
S is included in the solution set Ξ. The latter will be proved by demonstration
that the intersection of S with each orthant is included in Ξ.
Since the points y ±εej, j = 1, 2, . . . , n, represent perturbations of y directed
along the coordinate axis, we can claim that the intersection of S with an orthant
O is the convex hull of the points from the set {y, y ± εej, j = 1, 2, . . . , n} that
belong to the orthant O itself. Let us denote this convex hull as SO. In general,
SO may be a proper subset of the intersection S ∩O, but in our speciﬁc case the
structure of the point set {y, y ± εej, j = 1, 2, . . . , n} is “in conformity” with the
partition of Rn to orthants, since all ej are the unit coordinate vectors.
It only remains to note that SO, being the convex hull of points from O, is
included in the solution set Ξ, because Ξ is convex within the orthant O.
5
Position of a Point with Respect to the Solution Set
From the deﬁnition of the reserve and continuity of the functional Rsv (x), it is
obvious that
Rsv (y) ≥0 ⇐⇒y ∈Ξ,
(17)
Rsv (y) > 0 =⇒y ∈int Ξ,
(18)
Rsv (y) = 0 ⇐= y ∈∂Ξ,
(19)
where int Ξ is the topological interior of the solution set Ξ, and ∂Ξ is the
boundary of the solution set Ξ. A natural question is whether we can reverse
the logical implications in the second and third cases, thus getting equivalences.
That would allow us to completely investigate the position of a point with respect
to the interior and boundary of solution sets. Localizing the position of a point
within the solution set has practical signiﬁcance. In particular, if the point is
in the interior of the solution set, it is stable under data perturbations and,
moreover, we can construct an inner estimating box around the point as a center
(see, e.g., [14,17]).
Simple examples show, however, that additional requirements should be
imposed on the system under study as well as on the point y in order to make
the two-sided implications in (18)–(19) possible.

158
I.A. Sharaya and S.P. Shary
x
−1
1
int Ξ
1
Rsv (x)
Fig. 4. Graph of the reserve for system (20) in Example 3: the points −1 and 1 is the
boundary ∂Ξ of the solution set Ξ
Example 3. Let us consider the following 2 × 1-system of interval inclusions
 [0, −1] x ⊆[0, 1],
[−2, 2] x ⊆[−2, 2].
(20)
The graph of its reserve is depicted at Fig. 4. We can see that Rsv (0.5) = 0,
while 0.5 is in the interior of the solution set Ξ = [−1, 1].
Given an inclusion of the form Cx ⊆d and a point y ∈Rn, let us divide the
index sets of the matrix elements to the following index subsets:
for the row index i ∈{1, 2, . . . , m},
L :=

i | Ci:y = di

,
R :=

i | Ci:y = di

,
¬L :=

i | Ci:y ̸= di

,
¬R :=

i | Ci:y ̸= di

;
for the column index j ∈{1, 2, . . . , n},
P := { j | yj > 0 },
N := { j | yj < 0 },
E := { j | yj = 0 }.
Overall, we have
L ∪¬L = {1, 2, . . . , m},
R ∪¬R = {1, 2, . . . , m},
P ∪N ∪E = {1, 2, . . . , n}.
The special condition on C, d and the point y, which we denote SpeC (y), is
formulated as follows:
SpeC (y) :=

CLP = 0,
CRP = 0,
CLN = 0,
CRN = 0,
C(L∪R)E ⊆0,
(21)

Reserve of Characteristic Inclusion as Recognizing Functional
159
where, for example, CLP means a submatrix within the matrix C formed by all
the elements having their index pair (i, j) in the set L × P. (L ∪R) means the
union of the index subsets L and R, and so on.
Proposition 4. Let Ξ be the solution set to an interval inclusion Cx ⊆d with
C ∈KRm×n, d ∈KRm. For any y ∈Rn, there holds
y ∈int Ξ
⇐⇒

Rsv (y) > 0
	
or

Rsv (y) = 0 & SpeC (y)
	
,
y ∈∂Ξ
⇐⇒

Rsv (y) = 0 & ¬ SpeC (y)
	
,
where “ ¬” is the logical negation.
Proof. Our intention is to prove the ﬁrst equivalence of Proposition 4 using the
result of Proposition 3. The second equivalence of Proposition 4 is, in fact, a log-
ical consequence of the ﬁrst one.
We take a point y ∈Rn and ﬁx an index j ∈P (providing that P ̸= ∅), i. e.,
such that yj > 0. If ε > 0, then
C( y + εej) =

k̸=j
C:kyk + C:j(yj + ε)
(22)
=

k̸=j
C:kyk +

C :j(yj + ε), C:j(yj + ε)

(23)
since yj + ε > 0
=

k̸=j
C:kyk +

C :jyj + C :jε, C:jyj + C:jε

since C:j and C:j are point (noninterval)
=

k̸=j
C:kyk +

C :jyj, C:jyj

+

C :jε, C:jε

=
n

k=1
C:kyk +

C :jε, C:jε

since yj > 0
=

Cy + C :jε, Cy + C:jε

.
(24)
The membership y + εej ∈Ξ is equivalent to C(y + εej) ⊆d, which means,
due to (24), that
Cy + C:j ε ≥d
and
Cy + C:j ε ≤d.
(25)
If y ∈Ξ, then, from the deﬁnition of the index subset L, we get, ﬁrst,
CL: y = dL,
and, second,
C¬L: y > d¬L.
The latter strict inequality remains true for suﬃciently small perturbations ε,
while the former equality has nontrivial consequences.

160
I.A. Sharaya and S.P. Shary
The requirement that y + εej ∈Ξ entails
CL: y + CLj ε ≥dL,
and the inequality is satisﬁed only for CLj ε ≥0, which means, in view of ε > 0,
that CLj ≥0. Also, similar arguments applied to the second inequality from
(25) and the index subset R imply that CRj ≤0.
On the other hand, we can take the point (y −εej) instead of (y + εej) in
our above reasoning, starting from (22). The only reservation is that ε should
be chosen suﬃciently small to keep the inequality yj + ε > 0 so as we could
pass from (22) to (23). This leads to the conclusion that CLj ≤0 and CRj ≥0,
which, combined with our previous results on CLj and CRj, yields CLj = 0 and
CRj = 0 for every j ∈P. To put it another way, CLP = 0 and CRP = 0.
In exactly the same manner, after ﬁxing an index j ∈N (providing that
N ̸= ∅), we can prove that CLN = 0 and CRN = 0; we omit the expanded
reasoning for brevity.
To prove that C(L∪R)E ⊆0, we ﬁx an index j ∈E among the components of
the point y (providing that E ̸= ∅). As the result of considering ε-perturbations
of y along the j-th axis, similar to what has been done in the several preceding
paragraphs, we get the inequalities C(L∪R)j ≥0 and C(L∪R)j ≤0. Hence,
C(L∪R)E ⊆[0, 0] in the Kaucher complete interval arithmetic, as is required.
The details are again omitted.
Summing up, we get
y ∈int Ξ
⇐⇒
y ∈Ξ &
CLP = 0 & CRP = 0 & CLN = 0 & CRN = 0 & C(L∪E)E ⊆0.
In the right-hand side of the above equivalence, the condition at the second line
is nothing but SpeC (y). We can further transform this result taking into account
that the membership y ∈Ξ means Rsv (y) ≥0:
y ∈int Ξ
⇐⇒
Rsv (y) ≥0
&
SpeC (y)
⇐⇒

Rsv (y) > 0 & SpeC (y)
	
∨

Rsv (y) = 0 & SpeC (y)
	
⇐⇒
Rsv (y) > 0 ∨

Rsv (y) = 0 & SpeC (y)
	
,
since for Rsv (y) > 0 we have L = ∅and R = ∅, and then the condition SpeC
holds true. The last logical formula is exactly what stands in the right-hand side
of the ﬁrst equivalence of Proposition 4.
Finally, we have to prove the second equivalence of Proposition 4. In fact, it is
the negation of the ﬁrst equivalence we have already substantiated, taken under
the condition that y ∈Ξ. Since every point of the solution set Ξ is either

Reserve of Characteristic Inclusion as Recognizing Functional
161
interior or boundary, then the negation of x ∈int Ξ is the membership x ∈∂Ξ.
Next, negate the logical formula in the right-hand side of the ﬁrst equivalence:
¬
 
Rsv (y) > 0
	
∨

Rsv (y) = 0 & SpeC (y)
	 
⇔
by de Morgan’s law
¬

Rsv (y) > 0
	
& ¬

Rsv (y) = 0 & SpeC (y)
	
⇔
by de Morgan’s law

Rsv (y) = 0
	
&

Rsv (y) > 0 ∨¬ SpeC (y)
	
⇔
by distributivity of “&” and “∨”

Rsv (y) = 0 & Rsv (y) > 0
	
∨

Rsv (y) = 0 & ¬ SpeC (y)
	
⇔
Rsv (y) = 0 & ¬ SpeC (y),
since ( Rsv (y) = 0 & Rsv (y) > 0 ) is always false. The logical formula we
have obtained coincides with the right-hand side of the second equivalence in
Proposition 4.
The special condition SpeC (y) can be reduced to a more convenient, although
less general, form. To give its formulation, we need the concept of vertex of an
interval vector u ∈KR
l: it is any such u ∈R
l that uk ∈{uk, uk}, k = 1, 2, . . . , l.
Proposition 5. Assume that
(i) at least one of the following conditions is true:
• y does not lie on a coordinate hyperplane,
• the matrix C is proper;
(ii) the augmented matrix (C, d) does not have rows with zero vertices.
Then
y ∈int Ξ
⇐⇒
Rsv (y) > 0,
y ∈∂Ξ
⇐⇒
Rsv (y) = 0.
Proof. Due to Proposition 4,
y ∈int Ξ
⇐⇒

Rsv (y) > 0
	
∨

Rsv (y) = 0 & SpeC (y)
	
.
So, to substantiate Proposition 5, it suﬃces to show that the second term of
the right-hand side disjunction, i. e., the condition

Rsv (y) = 0 & SpeC (y)
	
is
incompatible with the premise of Proposition 5.
To put the above plan into practice, we are going to demonstrate that if
both

Rsv (y) = 0 & SpeC (y)
	
and the condition (i) hold true, then the
condition (ii), i. e.,
∀i ∈{1, 2, . . . , m}

0 ̸∈vert (Ci:, di)
	
,
is violated. As far as

162
I.A. Sharaya and S.P. Shary
dL = CLP xP + CLN xN + CLE · 0,
dR = CRP xP + CRN xN + CRE · 0,
we have
SpeC (y)
=⇒
dL = 0 & dR = 0.
(26)
At the same time, it is obvious that
Rsv (y) = 0
=⇒
L ̸= ∅∨R ̸= ∅.
(27)
Implications (26) and (27) entail that, under

Rsv (y) = 0 & SpeC (y)
	
, the
following is true:
(∃l)

ClP = 0 & ClN = 0 & ClE ⊆0 & dl = 0
	
or (∃r)

CrP = 0 & CrN = 0 & CrE ⊆0 & dr = 0
	
.
(28)
If the point y does not lie on a coordinate hyperplane, then E = ∅and (28)
implies that there exists such i that 0 ∈vert (Ci:, di), which runs contrary to
the premise (ii).
If the matrix C is proper, then ClE ⊆0 is equivalent to ClE = 0, and
CrE ⊆0 is equivalent to CrE = 0. Again, (28) implies that the premise (ii) is
violated.
6
Solvability of the Inclusion Cx ⊆d
In this section, we study the solvability of the inclusion Cx ⊆d, that is, answer
the question whether its solution set is empty or not. In the sequel, we denote
for brevity
max Rsv := max
x∈Rn Rsv (x).
If Rsv (x) is unbounded from above, we assign max Rsv = ∞.
From the equivalence

Rsv (y) ≥0 ⇐⇒y ∈Ξ
	
, it follows that
Ξ ̸= ∅
⇐⇒
max Rsv ≥0.
Therefore, examination of solvability of the inclusion Cx ⊆d (and of the related
interval linear problems as well) amounts to the solution of the unconstrained
optimization problem
ﬁnd
max
x∈Rn Rsv (x).
Then one has to inquire into the sign of the maximum.
Finally, we can consider the question on whether the topological interior of Ξ
is empty or not. In other words, do there exist solutions to the inclusion Cx ⊆d
that are stable under small perturbations in their position? In the general case,
it follows from the implication

Rsv (y) > 0 =⇒y ∈int Ξ
	
that
max Rsv > 0 =⇒int Ξ ̸= ∅.
The following counterexample shows that the reverse implication may prove
false.

Reserve of Characteristic Inclusion as Recognizing Functional
163
Example 4 ( ̸⇐=). For the inclusion [0, 1] x ⊆[0, 1], we have int Ξ = ]0, 1[ ̸= ∅,
but max Rsv = 0. The graph of the reserve is depicted at Fig. 5.
x
int Ξ
Rsv (x)
0
1
Fig. 5. Zero maximum of the reserve in Example 4
Nevertheless, after imposing special conditions on C and d, we can draw
conclusions in the opposite direction too.
Proposition 6. If the augmented matrix (C, d) does not have rows with zero
vertices, then
int Ξ ̸= ∅⇐⇒max Rsv > 0.
Proof. If max Rsv > 0, there exists such y ∈Rn that Rsv (y) > 0. Therefore,
y ∈int Ξ in view of (18), and so int Ξ ̸= ∅.
Conversely, let int Ξ ̸= ∅. Then int Ξ contains an open ball B. Within B, we
can take a point y that does not belong to the coordinate planes. Additionally,
the augmented matrix (C, d) does not have rows with zero vertices, and this is
why we can apply Proposition 5 concluding that Rsv (y) > 0. Hence, max Rsv ≥
Rsv (y) > 0, as required.
7
The ‘best’ Points for the Inclusion Cx ⊆d
It follows from the above constructions that the function Rsv (x) provides us
with a quantitative measure of ‘how strong’ (how ‘good’) the inclusion Cx ⊆d
is fulﬁlled. The points where the maximum of the function Rsv (x) is attained
are of special importance, since they satisfy the inclusion Cx ⊆d in the largest
possible amount. Such points are usually the ‘best’ points (in a certain sense)
from the solution set or points that comply with some additional optimality
conditions.
Below, we brieﬂy describe the corresponding results, using the notation
Arg max := { y ∈Rn | Rsv (y) = max Rsv }.

164
I.A. Sharaya and S.P. Shary
We distinguish three cases, when max Rsv is positive, zero, and negative
respectively.
x
int Ξ
Rsv (x)
max Rsv
Arg max
Fig. 6. Positive maximum of the reserve
Case max Rsv > 0 (see Fig. 6):
• Arg max consists of all such points for which Cx ⊆d
holds with maximum positive reserve.
• Arg max ⊆int Ξ.
We can regard the points from Arg max as ‘the most stable’ under data pertur-
bations, i. e., under variations in C and d.
x
Ξ
Arg max
Rsv (x)
Fig. 7. Zero maximum of the reserve
Case max Rsv = 0 (see Fig. 7):
• Arg max consists of all such points for which Cx ⊆d
holds with maximum reserve, although this reserve is zero.
• Arg max = Ξ.

Reserve of Characteristic Inclusion as Recognizing Functional
165
x
Rsv (x)
max Rsv
Arg max
Fig. 8. Negative maximum of the reserve
Case max Rsv < 0 (see Fig. 8):
• Arg max consists of all such points for which Cx ⊆d
is violated in the minimum amount.
• Ξ = ∅. Arg max is the solution set to the inclusion
Cx ⊆d + e [ max Rsv , −max Rsv ].
Although the solution set is empty, the points from the set Arg max can be
taken as ‘pseudosolutions’ to the corresponding system of interval relations, since
such points minimize the discrepancy in the characteristic inclusion Cx ⊆d. In
particular, the points from Arg max are the ﬁrst points that appear in nonempty
solution set after uniform widening of the right-hand side d by max Rsv . This
follows from (8).
It is worth noting that a particular case of the above construction has been
implemented in [18,19] as a promising approach to data ﬁtting problem under
interval uncertainty that works even for inconsistent input data.
8
Computational Complexity
We conclude the paper with a discussion on the complexity of the developed
technique that relies on the use of the reserve function. Let us consider the ﬁrst
formula of (8):
Rsv (x) =
min
1≤i≤m min

Ci:x −di, −Ci:x + di

.
We can see that computation of the value Rsv(x) requires 2n multiplications and
2 additions for each of the two subexpressions under inner minimum. The overall
expression for Rsv thus takes m(4n + 4) arithmetic operations, which is quite
cheap. Since testing condition SpeC (21) and its derivatives is also inexpensive,
we can assert that examining position of a point with respect to AE-solution
sets based on the reserve function (Sect. 5) is computationally eﬃcient.
Solvability issues considered in Sect. 6 and the constructions related to the
“best points” of the solution sets from Sect. 7 require unconstrained maximiza-
tion of the reserve function. This is a hard problem since in general Rsv (x) may

166
I.A. Sharaya and S.P. Shary
be a non-smooth multiextremal function whose graph looks like that depicted at
Fig. 1. However, one should not perceive this as a drawback of our technique as
far as it cannot be easier, in principle, than the theoretical complexity of recog-
nition of the solution set Ξ. For interval linear systems of equations and their
AE-solution sets, Lakeyev [8] has inquired into the question and discovered that
the problem of testing whether an AE-solution set is nonempty turns out NP-
complete if the interval linear system has “suﬃciently many” interval elements
with E-type of uncertainty. In terms of the inclusion Cx ⊆d, Lakeyev’s result
is equivalent to the statement that the solution set Ξ is NP-complete to recog-
nize providing that the matrix C and right-hand side vector d have, in total,
suﬃciently many elements that are improper in C and proper in d.
On the other hand, if an interval linear system of relations has “few” interval
elements with E-type of uncertainty, then its solution set can be recognized by
polynomial time algorithms. Then the reserve function Rsv (x) is not hard for
maximization too. Such is, for instance, the situation with the tolerable solution
set (9) and its recognizing functional (10) which can be eﬃciently maximized for
polynomial time by modern non-smooth optimization procedures.
Anyway, reducing examination of the solvability of the inclusion Cx ⊆d
to the unconstrained maximization problem with the objective function Rsv (x)
provides ﬂexibility in choosing our instruments and in further actions. In par-
ticular, for a speciﬁc problem, we can take this or that optimization procedure
depending on our needs, convenience, capability and available resources.
References
1. Beeck, H.: ¨Uber die Struktur und Absch¨atzungen der L¨osungsmenge von linearen
Gleichungssystemen mit Intervallkoeﬃzienten. Computing 10(3), 231–244 (1972).
doi:10.1007/BF02316910
2. Dymova, L.: Soft Computing in Economics and Finance. Springer, Heidelberg
(2011). doi:10.1007/978-3-642-17719-4
3. Goldsztejn, A., Chabert, G.: On the approximation of linear AE-solution sets.
Proceedings of the 12th GAMM-IMACS International Symposium on Scientiﬁc
Computing, Computer Arithmetic and Validated Numerics SCAN 2006, Duisburg,
Germany, 26–29 September 2006, 18 p (2006). doi:10.1109/SCAN.2006.33
4. Hlad´ık, M.: AE-solutions and AE-solvability to general interval linear systems.
Linear Algebra Appl. 465, 221–238 (2015). doi:10.1016/j.laa.2014.09.030
5. Kaucher, E.: Algebraische Erweiterungen der Intervallrechnung unter Erhaltung
der Ordnungs- und Verbandsstrukturen. Comput. Suppl. 1, 65–79 (1977)
6. Kaucher, E.: Interval analysis in the extended interval space IR. Comput. Suppl.
2, 33–49 (1980)
7. Kearfott, R.B., Nakao, M., Neumaier, A., Rump, S., Shary, S.P., van Hentenryck,
P.: Standardized notation in interval analysis. Comput. Technol. 15(1), 7–13 (2010)
8. Lakeyev, A.V.: Computational complexity of estimation of generalized solu-
tion sets for interval linear systems. Comput. Technol. 8(1), 12–23 (2003).
http://www.nsc.ru/interval/lakeyev/publications/03ct.pdf
9. Popova, E.: Explicit description of AE-solution sets for parametric linear sys-
tems. SIAM Journal on Matrix Analysis and Applications 33(4), 1172–1189 (2012).
doi:10.1137/120870359

Reserve of Characteristic Inclusion as Recognizing Functional
167
10. Rockafellar, R.T.: Convex Analysis. Princeton University Press, Princeton (1997).
(Reprint of the 1979 Princeton Mathematical Series 28th edn.)
11. Rohn, J.: A Handbook of Results on Interval Linear Problems. Electronic book,
Institute of Computer Science, Academy of Sciences of the Czech Republic, Prague
(2012). http://uivtx.cs.cas.cz/∼rohn/publist/!aahandbook.pdf
12. Sainz, M.´A., Garde˜nes, E., Jorba, L.: Interval estimations of solution sets to real-
valued systems of linear or non-linear equations. Reliable Comput. 8, 283–305
(2002). doi:10.1023/A:1016385132064
13. Sharaya,
I.A.:
Quantiﬁer-free
descriptions
of
interval-quantiﬁer
linear
sys-
tems.
Proc.
Inst.
Math.
Mech.
UB
RAS
(Trudy
Instituta
Matematiki
i
Mekhaniki UrO RAN) 20(2), 311–323 (2014). (in Russian) http://www.nsc.ru/
interval/sharaya/Papers/trIMM14.pdf
14. Shary, S.P.: Solving the linear interval tolerance problem. Math. Comput. Simul.
39, 53–85 (1995)
15. Shary, S.P.: Algebraic solutions to interval linear equations and their applica-
tions. In: Alefeld, G., Herzberger, J. (eds.) Numerical Methods and Error Bounds,
Proceedings of IMACS-GAMM International Symposium on Numerical Methods
and Error Bounds, Oldenburg, Germany, July 9–12, 1995. Mathematical Research,
vol. 89, pp. 224–233. Akademie Verlag, Berlin (1996). http://www.nsc.ru/interval/
shary/Papers/Herz.pdf
16. Shary, S.P.: A new technique in systems analysis under interval uncertainty
and ambiguity. Reliable Comput. 8(5), 321–418 (2002). http://www.nsc.ru/
interval/shary/Papers/ANewTech.pdf
17. Shary, S.P.: A new method for inner estimation of solution sets to interval linear
systems. In: Rauh, A., Auer, E. (eds.) Modeling, Design, and Simulation of Systems
with Uncertainties. Mathematical Engineering, vol. 3, pp. 21–42 (2011). doi:10.
1007/978-3-642-15956-5 2
18. Shary, S.P.: Maximum consistency method for data ﬁtting under interval uncer-
tainty. J. Glob. Optim. 1–16 (2015). doi:10.1007/s10898-015-0340-1
19. Shary, S.P., Sharaya, I.A.: Recognizing solvability of interval equations and its
application to data analysis. Comput. Technol. 18(3), 80–109 (2013). (in Russian)
20. Shary, S.P., Sharaya, I.A.: On solvability recognition for interval linear systems of
equations. Optim. Lett. 10(2), 247–260 (2015). doi:10.1007/s11590-015-0891-6

Global Optimisation

Convergence and Inclusion Isotonicity
of the Tensorial Rational Bernstein Form
J¨urgen Garloﬀ1(B) and Tareq Hamadneh2
1 University of Applied Sciences/HTWG Konstanz, Konstanz, Germany
garloff@htwg-konstanz.de
2 University of Konstanz, Konstanz, Germany
tareq.hamadneh@uni-konstanz.de
Abstract. A method is investigated by which tight bounds on the range
of a multivariate rational function over a box can be computed. The app-
roach relies on the expansion of the numerator and denominator polyno-
mials in Bernstein polynomials. Convergence of the bounds to the range
with respect to degree elevation of the Bernstein expansion, to the width
of the box and to subdivision are proven and the inclusion isotonicity of
the related enclosure function is shown.
Keywords: Bernstein polynomial · Rational function · Range bounding
1
Introduction
The expansion of a given (multivariate) polynomial p into Bernstein polynomials
provides bounds on the range of p over a box. This is now a well-established tool
as documented in [6]. In [8] the approach is extended to rational functions,
however, without any proof of the convergence of the bounds to the range. In
this paper we aim at ﬁlling this gap. Furthermore, we show that the related
rational Bernstein form is inclusion isotone, a property which is of fundamental
importance in interval computations, see, e.g., [9, Sect. 1.4]. The organization
of our paper is as follows. In Sects. 2 and 3 we recall the polynomial and the
rational Bernstein forms. In Sect. 4 we present our main results. Related results
for the simplicial Bernstein form which relies on the expansion of a polynomial
into Bernstein polynomials over a simplex are given in [13]. The Bernstein form
considered in this paper is also called the tensorial Bernstein form. But for
simplicity we use here only the term ‘Bernstein form’.
2
The Polynomial Bernstein Form
In this section we brieﬂy recall the most important properties of the Bernstein
expansion, which will be used in the following sections. Let I(R) be the set of
the compact, non-empty real intervals. We denote the distance q between two
intervals A = [a, a], B = [b, b] by
q([a, a], [b, b]) := max{|a −b|, |a −b|}.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 171–179, 2016.
DOI: 10.1007/978-3-319-31769-4 14

172
J. Garloﬀand T. Hamadneh
Without loss of generality we may consider the unit box I := [0, 1]n since
any compact non-empty box in Rn can be mapped thereupon by an aﬃne trans-
formation.
Comparisons and arithmetic operations on multiindices i = (i1, . . . , in)T are
deﬁned componentwise. For x ∈Rn its monomials are xi := xi1
1 . . . xin
n . Using the
compact notation k
i=0 := k1
i1=0 · · · kn
in=0,
k
i

:= n
μ=1
kμ
iμ

, an n-variate
polynomial p, p(x) = l
i=0 aixi, can be represented as
p(x) =
k

i=0
b(k)
i
(p) B(k)
i
(x), x ∈I,
(1)
where
B(k)
i
(x) =
k
i

xi(1 −x)k−i
(2)
is the ith Bernstein polynomial of degree k ≥l, and the so-called Bernstein
coeﬃcients b(k)
i
(p) are given by
b(k)
i
(p) =
i

j=0
i
j

k
j
aj, 0 ≤i ≤k, where aj := 0 for l ≤j, j ̸= l.
(3)
In particular, we have the endpoint interpolation property
b(k)
i
(p) = p( i
k ), for all i, 0 ≤i ≤k,
(4a)
with
iμ ∈{0, kμ}.
(4b)
A fundamental property for our approach is the convex hull property, which
states that the graph of p over I is contained within the convex hull of the control
points derived from the Bernstein coeﬃcients, i.e.,
	 x
p(x)

: x ∈I

⊆conv

i
k
b(k)
i
(p)

: 0 ≤i ≤k

,
(5)
where conv denotes the convex hull. This implies the interval enclosing
property [1]
min
0≤i≤k b(k)
i
(p) ≤p(x) ≤max
0≤i≤k b(k)
i
(p), for all x ∈I.
(6)
Equality holds on the left or right hand side of (6), if the minimum or maximum,
respectively, is attained at an index i satisfying (4b). This condition is called
the vertex condition. For an eﬃcient computation of the Bernstein coeﬃcients,
see [4].
A disadvantage of the direct use of (3) is that the number of the Bernstein
coeﬃcients to be computed explicity grows exponentially with the number of

Convergence and Inclusion Isotonicity of the Tensorial Rational
173
variables n. Therefore, it is advantageous to use a method [11] by which the
number of coeﬃcients which are needed for the enclosure only grows approxi-
mately linearly with the number of the terms of the polynomial.
In many cases it is desired to calculate the Bernstein expansion of p over a
general n-dimensional box X in the I(R)n,
X = [x1, x1] × · · · × [xn, xn]
with
xμ < xμ, μ = 1, . . . , n.
The width of X is denoted by w(X),
w(X) := x −x.
It is possible to ﬁrstly apply the aﬃne transformation which maps X on the
unit box I and to apply (3) using the coeﬃcients of the transformed polynomial.
However, in Sect. 4 it will be useful to consider the direct computation. Here,
the ith Bernstein polynomial of degree k ≥l is given by
B(k)
i
(x) =
k
i

(x −x)i(x −x)k−iw(X)−k, 0 ≤i ≤k.
(7)
The Bernstein coeﬃcients b(k)
i
of p of degree k over X are given by
b(k)
i
(p) =
i

j=0
i
j

k
j
 cj, 0 ≤i ≤k,
(8)
where
cj = w(X)j
k

τ=j
τ
j

aτxτ−j
(9)
with the convention aj := 0 for l ≤j, l ̸= j.
The interval
B(k)(p, X) := [ min
0≤i≤k b(k)
i
, max
0≤i≤k b(k)
i
]
encloses the range of p over X and is called the polynomial Bernstein form of p.
If the degree of the Bernstein expansion is elevated, the Bernstein coeﬃcients
of order k + 1 can easily be computed as convex combinations of the coeﬃcients
of order k, e.g., [2, formula (13)], [4, formula (3.11)]. It follows that
B(k+1)(p, X) ⊆B(k)(p, X).
(10)
The following theorem, see [10, formula (16)] for the univariate case and
[4, Theorem 3] for its multivariate extension, will be used to derive our main
results.

174
J. Garloﬀand T. Hamadneh
Theorem 1. For l ≤k, the following bound holds for the overestimation of the
range p(X) of p over X by the Bernstein form
q(p(X), B(k)(p, X)) ≤
l

i=0
n

μ=1
[max(0, iμ −1)]2
kμ
|ci|,
(11)
where the coeﬃcients ci are given by (9).
Remark 1. If 2 ≤kμ the bound on the right hand side of (11) can be improved
slightly, see [10, formula (17)]. For later use we note an extension of [10, Theorem 4].
Let x(k)
i
be the grid point the μth component of which is given by
x(k)
i,μ = xμ + iμ
kμ
(xμ −xμ), μ = 1, . . . , n.
(12)
Then by [4, p. 42] the diﬀerence |(p(x(k)
i
) −b(k)
i
| can be bounded from above
for all i, 0 ≤i ≤k, by the right-hand side of (11).
3
The Rational Bernstein Form
Let p and q be polynomials in variables x1, . . . , xn with Bernstein coeﬃcients
b(k)
i
(p) and b(k)
i
(q), 0 ≤i ≤k, over a box X, respectively. We consider the
rational function f := p/q. We may assume that both p and q have the same
degree l since otherwise we can elevate the degree of the Bernstein expansion of
either polynomial by component where necessary to ensure that their Bernstein
coeﬃcients are of the same order k ≥l. We call
b(k)
i
(f) := b(k)
i
(p)
b(k)
i
(q)
, 0 ≤i ≤k,
the rational Bernstein coeﬃcients of f.
Theorem 2. [8, Theorem 3.1] Assume that all Bernstein coeﬃcients b(k)
i
(q)
have the same sign and are non-zero (this implies that q(x) ̸= 0, for all x ∈X).
Then the following enclosure for the range of f over X holds:
m(k) := min
0≤i≤k b(k)
i
(f) ≤f(x) ≤max
0≤i≤k b(k)
i
(f) =: m(k), for all x ∈X.
(13)
The interval spanned by the left and right hand sides of (13) constitutes the
rational Bernstein form B(f, X),
B(k)(f, X) := [m(k), m(k)].
Remark 2. The convex hull property (5) does not in general carry over to rational
functions and control points formed from the rational Bernstein coeﬃcients even
in the univariate case (n = 1). For a counterexample see [8].

Convergence and Inclusion Isotonicity of the Tensorial Rational
175
4
Main Results
Let throughout f = p/q be a rational function, where p and q are polynomials
of degree l and let the range of f over X be f(X) = [f, f]. Without loss of
generality we assume that
0 < b(l)
i (q), for all i, 0 ≤i ≤l,
(14)
and prove the statements only for the upper bounds since the proofs for the
lower bounds are entirely analogous. The polynomial r,
r := p −m(k)q,
(15)
will serve as a vehicle to convey the results from the polynomial to the rational
case. Note that the Bernstein coeﬃcients of a polynomial are linear, hence
b(k)
i
(r) = b(k)
i
(p) −m(k)b(k)
i
(q).
(16)
First we show that the vertex condition remains in force.
Proposition 3. It holds that m(k) = f (m(k) = f) if and only if m(k) (m(k)) =
b(k)
i
(f) with i satisfying (4b).
Proof. By (4a), b(k)
i
(f) with i satisfying (4b) is a value of f at a vertex of X.
If follows that m(k) is sharp if it is attained at such a Bernstein coeﬃcient.
Conversely, assume that m(k) = f,
m(k) = b(k)
i0 (f), for some i0, 0 ≤i0 ≤k,
(17)
and f = f(ˆx) for some ˆx ∈X. Then we can conclude that
r(ˆx)
q(ˆx) = f(ˆx) −m(k) = 0,
hence r(ˆx) = 0. Since r is nonpositive on X it attains its maximum at ˆx. On the
other hand, we have by (16)
b(k)
i
(r) ≤0, for all i, 0 ≤i ≤k,
(18)
and by (17) b(k)
i0 (r) = 0. So we can conclude that
max
x∈X r(x) = b(k)
i0 (r).
(19)
By the polynomial vertex condition if follows that the index i0 satisﬁes (4b). ⊓⊔

176
J. Garloﬀand T. Hamadneh
4.1
Linear Convergence with Respect to Degree Elevation
We start with the observation that the monotonicity property (10) carries over
to the rational case.
Proposition 4. For l ≤k it holds that B(k+1)(f, X) ⊆B(k)(f, X).
Proof. By application of (10) to polynomial r (15) and noting (16) we obtain
for all j, 0 ≤j ≤k + 1,
b(k+1)
j
(p) −m(k)b(k+1)
j
(q) ≤
max
0≤i≤k+1{b(k+1)
i
(p) −m(k)b(k+1)
i
(q)}
≤max
0≤i≤k{b(k)
i
(p) −m(k)b(k)
i
(q)} ≤0,
hence b(k+1)
j
(f) ≤m(k) .
⊓⊔
Theorem 5. For l ≤k it holds that
q(f(X), B(k)(f, X)) ≤β
k ,
(20)
where β is a constant not depending on k.
Proof. Without loss of generality we consider only the case 0 ≤m(k). We assume
again that (17) holds and use the corresponding grid point x(k)
i0 , see (12). By (10)
we may estimate for l ≤k
m(k) ≤m(l) ≤max b(l)
i (p)
min b(l)
i (q)
=: β′.
(21)
We can conclude from (17) that
m(k) −f ≤m(k) −f(x(k)
i0 )
= m(k) · q(x(k)
i0 ) −p(x(k)
i0 ) + b(k)
i0 (p) −m(k) · b(k)
i0 (q)
q(x(k)
i0 )
= m(k)(q(x(k)
i0 ) −b(k)
i0 (q)) + b(k)
i0 (p) −p(x(k)
i0 )
q(x(k)
i0 )
.
Taking absolute values and using Remark 1 and (10) we can estimate
m(k) −f ≤β′ β1
k + β2
k
min b(l)
i (q)
,
(22)
where β1, β2 are constants not depending on k, which completes the proof.
⊓⊔

Convergence and Inclusion Isotonicity of the Tensorial Rational
177
4.2
Quadratic Convergence with Respect to the Width
of an Interval
Inspection of (22) shows that we can extract the square of maxn
μ=1(xμ−xμ) from
the constant β in (20), cf. (9), (11). Therefore, we obtain the following extension
of [12, Corollary 3.4.16].
Theorem 6. Let A ∈I(R)n be ﬁxed. Then for all X ∈I(R)n, X ⊆A, and l ≤k
it holds that
q(f(X), B(k)(f, X)) ≤γ ||w(X)||2
∞,
(23)
where γ is a constant not depending on X.
4.3
Quadratic Convergence with Respect to Subdivision
Since the convergence with respect to degree elevation is only linear we will
choose k = l in the sequel and reserve in this subsection the upper index of
the Bernstein coeﬃcients for the subdivision level. For simplicity we consider
the unit box I. Repeated bisection of I(0,1) := I in all n coordinate direc-
tions results at subdivision level 1 ≤h in subboxes I(h,ν) of edge length 2−h,
ν = 1, . . . , 2nh. Denote the Bernstein coeﬃcients of f over I(h,ν) by b(h,ν)
i
(f).
For their computation see [4,14]. Put
B(h)(f) := [
min
0≤i≤l,
1≤ν≤2nh
b(h,ν)
i
(f),
max
0≤i≤l,
1≤ν≤2nh
b(h,ν)
i
(f)].
We obtain the following extension of [3, formula (23)].
Theorem 7. For each 1 ≤h it holds
q(f(X), B(h)(f)) ≤δ(2−h)2,
(24)
where δ is a constant not depending on h.
Proof. Assume that
max
0≤i≤l,
1≤ν≤2nh
b(h,ν)
i
= max
0≤i≤l b(h,ν0)
i
, for some ν0, 0 ≤ν0 ≤2nh.
Then it follows by Theorem 6
max
0≤i≤l,
1≤ν≤2nh
b(h,ν)
i
−max
x∈I f(x) ≤max
0≤i≤l b(h,ν0)
i
−
max
x∈I(h,ν0) f(x)
≤δ||w(I(h,ν0))||2
∞= δ (2−h)2.
⊓⊔
Remark 3. Note that by (9), (11) the constants β, γ and δ in (20), (23), and (24)
can be given explicity.

178
J. Garloﬀand T. Hamadneh
4.4
Inclusion Isotonicity
We continue with choosing k = l and suppress therefore the upper index for the
Bernstein coeﬃcients. An interval function F : I(R)n −→I(R) is called inclusion
isotone, if, for all X, Y ∈I(R)n, X ⊆Y implies F(X) ⊆F(Y ).
In [7] it was shown by a lengthy proof that the polynomial Bernstein form
is inclusion isotone. In [5] a brief proof of this property and an extension to the
multivariate case are presented. We show that the inclusion isotonicity carries
over to rational functions.
Theorem 8. The rational Bernstein form is inclusion isotone.
Proof. We consider without loss of generality the unit box I and denote the
Bernstein coeﬃcients of the rational function f over I by bi(f), 0 ≤i ≤l.
It suﬃces to show that the inclusion isotonicity holds if we shrink only one
edge of I and this is done in turn separately at its left and right endpoint.
Without loss of generality we consider only the ﬁrst case and the ﬁrst component
interval of I and denote by b∗
i (f), 0 ≤i ≤l, the Bernstein coeﬃcients of f over
[ϵ, 1] × [0, 1]n−1, 0 < ϵ < 1. Put
m∗:= max
0≤i≤l b∗
i (f).
We proceed by contradiction and assume that
m∗= b∗
i0(f), for some i0, 0 ≤i0 ≤l,
(25)
and
m := max
0≤i≤l bi(f) < m∗.
(26)
Since the Bernstein form of the polynomial p −m∗q is inclusion isotone we
obtain from (26) that
b∗
i0(p) −m∗b∗
i0(q) ≤max
0≤i≤l{b∗
i (p) −m∗b∗
i (q)}
≤max
0≤i≤l{bi(p) −m∗bi(q)}
< max
0≤i≤l{bi(p) −m bi(q)} ≤0
from which we get a contradiction to (25).
⊓⊔
Acknowledgements. The
authors
gratefully
acknowledge
support
from
the
University of Applied Sciences/HTWG Konstanz through the SRP program.

Convergence and Inclusion Isotonicity of the Tensorial Rational
179
References
1. Cargo, G.T., Shisha, O.: The Bernstein form of a polynomial. J. Res. Nat. Bur.
Stand. Sect. B 70B, 79–81 (1966)
2. Farouki, R.T.: The Bernstein polynomial basis: a centennial retrospective. Comput.
Aided Geom. Des. 29, 379–419 (2012)
3. Fischer, H.C.: Range computation and applications. In: Ulrich, C. (ed.) Contribu-
tions to Computer Arithmetic and Self-Validating Numerical Methods, pp. 197–
211. Balzer, Amsterdam (1990)
4. Garloﬀ, J.: Convergent bounds for the range of multivariate polynomials. In: Nickel,
K. (ed.) Interval Mathematics 1985. LNCS, vol. 212, pp. 37–56. Springer, Heidel-
berg (1986)
5. Garloﬀ, J., Jansson, C., Smith, A.P.: Inclusion isotonicity of convex-concave exten-
sions for polynomials based on Bernstein expansion. Computing 70, 111–119 (2003)
6. Garloﬀ, J., Smith, A.P.: Special issue on the use of Bernstein polynomials in reliable
computing: a centennial anniversary. Reliab. Comput. 17 (2012)
7. Hong, H., Stahl, V.: Bernstein form is inclusion monotone. Computing 55, 43–53
(1995)
8. Narkawicz, A., Garloﬀ, J., Smith, A.P., Mu˜noz, C.A.: Bounding the range of a
rational function over a box. Reliab. Comput. 17, 34–39 (2012)
9. Neumaier, A.: Interval Methods for Systems of Equations. Encyclopedia of Math-
ematics and Its Applications, vol. 37. Cambridge University Press, Cambridge
(1990)
10. Rivlin, T.: Bounds on a polynomial. J. Res. Nat. Bur. Stand. Sect. B 74B, 47–54
(1970)
11. Smith, A.P.: Fast construction of constant bound functions for sparse polynomials.
J. Global Optim. 43, 445–458 (2009)
12. Stahl, V.: Interval methods for bounding the range of polynomials and solving
systems of nonlinear equations. Dissertation, Johannes Kepler University, Linz
(1995)
13. Titi, J., Hamadneh, T., Garloﬀ, J.: Convergence of the simplicial rational Bernstein
form. In: Le Thi, H.A., Pham, D.T., Nguyen, N.T. (eds.) MCO 2015 - Part I. AISC,
vol. 359, pp. 433–441. Springer, Heidelberg (2015)
14. Zettler, M., Garloﬀ, J.: Robustness analysis of polynomials with polynomial para-
meter dependency using Bernstein expansion. IEEE Trans. Automat. Control 43,
425–431 (1998)

The Bernstein Branch-and-Bound
Unconstrained Global Optimization Algorithm
for MINLP Problems
Bhagyesh V. Patil1(B) and P.S.V. Nataraj2
1 Universit´e de Nantes, 2 rue de la Houssini´ere, 44322 Nantes Cedex 3, France
bhagyesh.patil@gmail.com
2 Systems and Control Engineering, IIT Bombay, Powai, Mumbai 400 076, India
nataraj@sc.iitb.ac.in
Abstract. In this work a Bernstein global optimization algorithm to
solve unconstrained polynomial mixed-integer nonlinear programming
(MINLP) problems is proposed. The proposed algorithm use a branch-
and-bound framework and possesses several new features, such as a
modiﬁed subdivision procedure, the Bernstein box consistency and the
Bernstein hull consistency procedures to prune the solution search space.
The performance of the proposed algorithm is numerically investigated
and compared with previously reported Bernstein global optimization
algorithm on a set of 10 test problems. The ﬁndings of the tests estab-
lishes the eﬃcacy of the proposed algorithm over the previously reported
Bernstein algorithm in terms of the chosen performance metrics.
1
Introduction
Global optimization of MINLP problems is a promising research area and has
been a point of attraction to many researchers from academia as well as indus-
try. In this work, we attempt to solve such MINLP problems which are of the
following form:
min
x
f(x)
xk ∈x ⊆R, k = 1, 2, . . . , ld
(1)
xk ∈Z, k = ld + 1, . . . , l ,
where f : Rl →R is the (possibly polynomial nonlinear) objective function,
x := [x, x] is an interval in R, xk (k = 1, 2, . . . , ld) are continuous decision
variables, and the rest of xk (k = ld + 1, . . . , l) are integer decision variables.
A widely used strategy for solving MINLP problems of the form (1) is to
use a branch-and-bound (BB) framework [5]. Speciﬁcally, a relaxed nonlinear
programming (NLP) problem is solved at each node of the branch-and-bound
tree. Diﬀerent variants of the BB approach have been reported in the literature
and are widely adapted by several state-of-the-art MINLP solvers (cf. Bonmin
[2], SBB [6], BARON [14]). Recently, a new technique based on the separable
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 180–198, 2016.
DOI: 10.1007/978-3-319-31769-4 15

The Bernstein Branch-and-Bound Unconstrained Global Optimization
181
underestimators for box-constrained MINLP problems has been proposed in [3],
and found to be well competent with state-of-the-art MINLP solvers. We direct
the interested readers to [4] and references therein for more details about state-
of-the-art available tools for solving MINLP problems. However, despite of the
widespread enjoyed interest by the BB approach in the ﬁeld of MINLPs, we
note that sometimes the type of NLP solver used has found to limit its perfor-
mance in practice. This seems to be true since most of the NLP solvers assume
generalized convexity. To solve polynomial NLP problems, an alternative app-
roach is provided by the Bernstein global optimization algorithms which use
the Bernstein form of a polynomial function. A notable feature of the Bernstein
form is that the Bernstein range enclosure of a function is sharper than those
obtained with most interval forms [13]. Hence, algorithms based on the Bern-
stein form in practice are found to be more eﬀective than existing interval algo-
rithms. Several variants of such Bernstein algorithms to solve unconstrained and
constrained polynomial NLPs have been reported in literature (see, for instance,
[9,11]). However, we note that no work has yet been reported in the literature for
global optimization of bound constrained polynomial MINLP problems using the
Bernstein form. This motivates us to investigate and dig more ﬁndings with this
elegant Bernstein form approach in the direction of bound constrained MINLP
problems.
In this work, we propose a Bernstein algorithm for bound constrained global
optimization of MINLPs of the form (1). The proposed algorithm is similar in
philosophy to interval branch-and-bound procedures for the global optimization
of bound constrained NLPs and extends the work proposed for solving uncon-
strained NLP problems by [9]. The proposed algorithm use combination of the
several enhanced tools, such as the monotonicity and the concavity tests, a mod-
iﬁed subdivision procedure, and the Bernstein box consistency and the Bernstein
hull consistency procedures to prune the solution search space. It may be noted
that the Bernstein algorithm in [9] lack such solution search space pruning pro-
cedures and hence may be computationally expensive. Further, the performance
of the proposed algorithm is evaluated on a collection of 10 test problems taken
from NLP literature. These problems are appropriately modiﬁed as MINLPs and
the test results are compared with the Bernstein algorithm in [9]1.
The rest of the paper is organized as follows. In Sect. 2, we introduce the
reader with some background of the Bernstein form. In Sect. 3, we suggest some
improvements in the Bernstein algorithm used to solve NLPs. We also present
consistency techniques based on the Bernstein form to prune the solution search
space. Finally, we present our main proposed unconstrained global optimiza-
tion algorithm for the MINLP problems. In Sect. 4, numerical experiments are
reported along with their ﬁndings. Lastly, we draw some conclusions from the
present work in the Sect. 5.
1 Albeit, the Bernstein global optimization algorithm in [9] is for NLP problems, we
modify it at appropriate places to handle integer decision variables.

182
B.V. Patil and P.S.V. Nataraj
2
Background
In this section, we introduce few notions about the Bernstein form. We would like
to direct the interested reader to reference [11] for more details about the topic.
Let l ∈N be the number of variables and x = (x1, x2, ..., xl) ∈Rl. A multi-
index I is deﬁned as I = (i1, i2, ..., il) ∈Nl and the multi-power xI is deﬁned
as xI = (xi1
1 , xi2
2 , ..., xil
l ). A multi-index N is deﬁned as N = (n1, n2, ..., nl).
Inequalities I ≤N for multi-indices are meant component-wise. With I =
(i1, ..., ir−1, ir, ir+1, ..., il) we associate the index Ir,k given by Ir,k = (i1, ..., ir−1,
ir + k, ir+1, ..., il), where 0 ≤ir + k ≤nr. Also we write
N
I

for
n1
i1

· · ·
nl
il

and
(N/I) for (n1/i1, n2/i2, ..., nl/il) provided that 0 < ik, k = 1, 2, . . . , l.
A real bounded and closed interval x is deﬁned as
x = [x, x] := [inf x, sup x] ∈IR,
where IR denotes the set of compact intervals. Let w(x) denote the width of x,
that is w(x) := x −x, and m(x) denote the midpoint of x, that is m(x) := (x +
x)/2. Similarly, for an l-dimensional interval vector or box x = (x1, x2, . . . , xl) ∈
IRl, the width of x is w(x) := max(w(x1), w(x2), . . . , w(xl)).
We can write an l-variate polynomial p in the form
p(x) =

I≤N
aIxI,
x ∈Rl
(2)
with N being the degree of p. We expand a given multivariate polynomial into
Bernstein polynomials to obtain bounds for its range over an l-dimensional box x.
The Ith Bernstein basis polynomial of degree N is deﬁned as
BN
I (x) = Bn1
i1 (x1) · · · Bnl
il (xl),
x ∈Rl,
(3)
where, for ij = 0, 1, ..., nj, j = 1, 2, . . . , l
Bnj
ij (xj) =

nj
ij
 (xj −xj)ij(xj −xj)nj−ij
(xj −xj)nj
.
(4)
The Bernstein coeﬃcients bI(x) of p over the box x are given by
bI (x) =

J≤I
 I
J

 N
J
w(x)J 
K≤J
 K
J

(inf x)K−JaK, I ≤N.
(5)
The Bernstein form of a multivariate polynomial p is deﬁned by
p (x) =

I≤N
bI (x) BN
I (x) .
(6)
The Bernstein coeﬃcients are collected in an array (bI(x))I∈S, where S = {I :
I ≤N}. We denote S0 as a special subset of the index set S comprising indices
of the vertices of this array, that is
S0 := {0, n1} × {0, n2} × · · · × {0, nl}.

The Bernstein Branch-and-Bound Unconstrained Global Optimization
183
Theorem 1. (Range enclosure property) Let p be a polynomial of degree N, and
let p(x) denote the range of p on a given box x ∈IRl. Then,
p(x) ⊆B(x) :=

min (bI(x))I∈S , max (bI(x))I∈S

.
(7)
Proof: See [7].
Remark 1. The above theorem says that the minimum and maximum coeﬃ-
cients of the array (bI(x))I∈S provide lower and upper bounds for the range. This
forms the Bernstein range enclosure, deﬁned by B(x) in Eq. (7). The Bernstein
range enclosure can successively be sharpened by the continuous domain subdi-
vision procedure [7].
Lemma 2. (Vertex property) [7] Consider the Bernstein form in Eq. (6) for a
polynomial p of degree N, and let the range p(x) = [a, b]. Then
a =
min
0≤I≤N (bI(x)) if and only if
min
0≤I≤N (bI(x)) = min
I∈S0 (bI(x))
b = max
0≤I≤N (bI(x)) if and only if max
0≤I≤N (bI(x)) = max
I∈S0 (bI(x))
Remark 3. The above Lemma says that the lower bound (respectively upper
bound) is sharp if and only if min (bI(x))I∈S (respectively max(bI(x))I∈S) is
attained at a Bernstein coeﬃcients of the array (bI(x)) with I ∈S0. This con-
dition is known as the vertex property.
3
Proposed Algorithm
In this section, we present a modiﬁed subdivision procedure which is an exten-
sion of a classical subdivision procedure used by the Bernstein algorithms. We
then introduce new constraints formulated based on the gradient and upper
bound on the global minimum of the objective function (f). Further, the consis-
tency techniques based on the Bernstein form are introduced. We shall use this
Bernstein consistency techniques in combination with our new formulated con-
straints to isolate those stationary points from the solution search space that are
not the global minimum. Finally, we shall combine all these enhancements and
present our main proposed Bernstein algorithm to solve unconstrained MINLP
problems.
3.1
Modiﬁed Subdivision Procedure
As explained in Remark 1, the range enclosure obtained using Bernstein coef-
ﬁcients can be improved by subdividing the domain of decision variables.
A subdivision in the rth direction (1 ≤r ≤l) is a bisection perpendicular
to this direction. Let
x = [x1, x1] × · · · × [xr, xr] × · · · × [xl, xl],
(8)

184
B.V. Patil and P.S.V. Nataraj
be any subbox. Generally, x is bisected along the rth component direction (for
NLPs having only continuous decision variables); resulting into two subboxes xA
and xB as
xA = [x1, x1] × · · · × [xr, m(xr)] × · · · × [xl, xl],
(9)
xB = [x1, x1] × · · · × [m(xr), xr] × · · · × [xl, xl],
(10)
where m(xr) denotes the midpoint of [xr, xr].
Similar to the above, we suggest following modiﬁcation in the subdivision
procedure to cope with the integer decision variables in our proposed Bernstein
algorithm. We bisect x along the rth component direction; such that, two sub-
boxes xA and xB are formed as below
xA = [x1, x1] × · · · × [xr, ⌊m(xr)⌋] × · · · × [xl, xl],
(11)
xB = [x1, x1] × · · · × [⌊m(xr) + 1⌋, xr] × · · · × [xl, xl],
(12)
where ⌊m(xr)⌋denotes the ﬂoor of midpoint of [xr, xr]. We shall use the subdivi-
sion procedure in Eqs. (9) and (10), if the rth component direction is a continuous
decision variable. Similarly, the subdivision procedure in Eqs. (11) and (12) will
be used, if the rth component direction is a integer decision variable.
Similarly, other more sophisticated approaches to branch integer variables in
a branch-and-bound tree are reported in a literature. For instance, [2,14] inves-
tigate use of a valid inequalities to discard fractional solutions at each node of
a branch-and-bound tree. We direct the interested reader to [14], and references
therein for more speciﬁc details about the topic.
3.2
Constraint Formulation
In the constrained global optimization algorithms, we determine the global min-
imum subjected to some (inequality and equality) feasibility constraints (see, for
instance, work by Nataraj et al. [10]. We can apply the consistency techniques
to these feasibility constraints to contract the bounds on the decision variables.
However, in the bound constrained global optimization algorithms there are no
such feasibility constraints, and this can defy the application of these consistency
techniques. To alleviate this problem, we introduce the constraints based on the
gradient and upper bound on the global minimum of the objective function (f),
wherein we can apply these consistency techniques.
– Assume that the objective function (f) is continuously diﬀerentiable. Then,
the gradient (∇f) of f is zero at local minima, at maxima, at saddle points,
and at the global minima. Thus, we can ﬁnd the zero(s) of the gradient
at which f has a global minimum by discarding any that are not a global
minimum of f. In practice the gradient will have l components and we can
form l constraints corresponding these l components as
f ′
xr = 0,
r = 1, 2, . . . , l,
(13)
where l being the number of variables in the objective function and ∇f =
[f ′
x1 f ′
x2 . . . f ′
xl]T .

The Bernstein Branch-and-Bound Unconstrained Global Optimization
185
– In the global optimization algorithm, at the outset we may compute an upper
bound (say, 	f) on the global minimum (f ∗) on the box x, that is
	f ≥f ∗.
(14)
Thus, we can delete any point (or subbox) of x for which f > 	f. This serves
to delete a subbox that bounds a nonoptimal stationary point of f.
We shall apply the Bernstein box and Bernstein hull consistency techniques
(refer to the Sect. 3.3) to Eqs. (13) and (14) to delete nonoptimal points from
the box x. Henceforth, we shall indicate the application of the Bernstein box
consistency and the Bernstein hull consistency techniques by ﬂags BCF and
HCF, respectively to the Eqs. (13) and (14).
3.3
Consistency Techniques
We now present algorithms based on the consistency techniques that help prun-
ing the solution search space. The pruning is achieved by assessing consistency
of the algebraic equations (in our case inequality and equality constraints) over a
given box, and thereby discarding regions of a box where no guarantee of global
minimum lies. We note in the literature two types of consistency notions exist;
box consistency which use a one-dimensional interval Newton method to com-
pute a box consistent region for a given set of the algebraic equations, and hull
consistency which use a constraint inversion procedure to compute a hull con-
sistent region for the given set of the algebraic equations. The interested reader
can refer [8] for more details. In sequel, we now present algorithms on the consis-
tency ideas borrowed from [8], and expanded in context of the Bernstein form.
Henceforth, we shall call these algorithms as Bernstein box consistency (BBC)
and Bernstein hull consistency (BHC) algorithms. We shall use these Bernstein
consistency algorithms for pruning purpose in our main proposed global opti-
mization algorithm (see, algorithm IBBBU in Sect. 3.4).
Algorithm Bernstein box consistency: x′
= BBC((bg(x)), x, r, xstatus,r,
eq type)
Inputs: The Bernstein coeﬃcient array (bg(x)) of a given constraint function
g (x), the l-dimensional box x, the direction r for which the bounds are to be
contracted, ﬂag xstatus,r to indicate whether rth direction (variable) is continuous
(xstatus,r = 0) or integer (xstatus,r = 1), and ﬂag eq type to indicate whether
g (x) is equality constraint (eq type = 0) or inequality constraint (eq type = 1).
Outputs: A box x′ that is contracted using Bernstein box consistency technique
for a given constraint function g (x).

186
B.V. Patil and P.S.V. Nataraj
BEGIN Algorithm
1. Set a = inf xr, b = sup xr.
2. Compute the derivative enclosure g
′
xr in the direction xr2.
3. (Consider left endpoint of xr). Obtain the Bernstein range enclosure g(a)
as the minimum to maximum from the Bernstein coeﬃcient array of (bg(x))
for xr = a.
4. If eq type = 1, then modify g(a) as g(a) = [min g(a), inf].
5. If 0 ∈g(a), then we cannot increase a. Go to step 8 and try from the right
endpoint b of the interval xr.
6. Do one iteration of the univariate Bernstein Newton contractor
N (xr) = a −(g(a)/g′
xr).
x′
ra = xr ∩N (xr) .
7. If x′
ra = ∅, then there is no zero of g on entire interval xr and hence the
constraint g is infeasible over box x. EXIT the algorithm in this case with
x′ = ∅.
8. (Consider right endpoint of xr). Obtain the Bernstein range enclosure g(b)
as the minimum to maximum from the Bernstein coeﬃcient array of (b(x))
for xr = b.
9. If eq type = 1, then modify g(b) as g(b) = [min g(b), inf].
10. If 0 ∈g(b), then we cannot decrease b. Go to step 13
11. Do one iteration of the univariate Bernstein Newton contractor
N (xr) = b −(g(b)/g′
xr).
x′
rb = xr ∩N (xr) .
12. If x′
rb = ∅, EXIT the algorithm with x′ = ∅.
13. Compute x′
r as follows:
(a) x′
r = x′
ra∩x′
rb,
if both x′
ra and x′
rb are computed.
(b) x′
r = x′
ra or x′
rb,
which ever is computed.
(c) x′
r = xr
(both x′
ra and x′
rb are not computed).
14. for k = 1, 2 if xstatus,r = 1 then
(a) if x(r, k) and x′
r(r, k) are equal then go to substep (e).
(b) Set ta = x(r, k) and tb = x′
r(r, k).
(c) if ta > tb then set x′
r(r, k) = ⌊x′
r(r, k)⌋.
(d) if ta < tb then set x′
r(r, k) = ⌈x′
r(r, k)⌉.
(e) end (of k-loop).
15. Return x′ = x′
r.
END Algorithm
Algorithm Bernstein hull consistency: x′ = BHC((bg(x)), aI, I, x, xstatus,
eq type)
2 The derivative of a polynomial function in a particular direction can be found from
the Bernstein coeﬃcients of the original polynomial function [13].

The Bernstein Branch-and-Bound Unconstrained Global Optimization
187
Inputs: The Bernstein coeﬃcient array (bg(x)) of a given constraint function
g (x), coeﬃcient aI of the selected term t, power I of the each variable in
term t, the l−dimensional box x, a column vector xstatus describing the sta-
tus (continuous or integer) of the each variable xi (i = 1, 2, . . . , l), and ﬂag
eq type to indicate whether g(x) is equality constraint (eq type = 0) or inequal-
ity constraint(eq type = 1).
Outputs: A box x′, that is contracted using Bernstein hull consistency technique
applied to a given constraint g (x) and selected term t.
BEGIN Algorithm
1. Compute the Bernstein coeﬃcient array of the selected term t as (bt (x)).
2. Obtain the Bernstein coeﬃcients of the constraint inverse polynomial by sub-
tracting (bg(x)) from (bt (x)), and then obtain its Bernstein range enclosure
as the minimum to maximum of these Bernstein coeﬃcients. Denote it as h′.
3. if eq type = 1 then
(a) Compute an interval y as y = [−∞, 0] ∩[min (bg(x)), max (bg(x))].
(b) if y = ∅then set x′ = ∅, and EXIT the algorithm. Else modify h′ as
h′ = h′ + y.
4. (a) for r = 1, 2, . . . , l (r := number of variables)
(b) Compute x′
r =

h′
aI
 x
ik
k
1/ir 
 xr
(c) for k = 1, 2 if xstatus(r) = 1 then
(i) if x(r, k) and x′
r(r, k) are equal then go to substep (v).
(ii) Set ta = x(r, k) and tb = x′
r(r, k).
(iii) if ta > tb then set x′
r(r, k) = ⌊x′
r(r, k)⌋.
(iv) if ta < tb then set x′
r(r, k) = ⌈x′
r(r, k)⌉.
(v) end (of k−loop).
(d) end (of r−loop).
5. Return x′.
END Algorithm
3.4
Main Proposed Global Optimization Algorithm
We now propose an algorithm for bound constrained global optimization of
multivariate MINLP problems, called as improved Bernstein branch-and-bound
unconstrained (IBBBU) algorithm. The proposed algorithm use a modiﬁed sub-
division procedure presented in the Sect. 3.1, the Bernstein box and hull consis-
tency algorithms presented in the Sect. 3.3, and the accelerating devices, such as
the cut-oﬀtest, the monotonicity test, and the concavity tests3.
3 Due to lack of space and time, we skip the presentation of the accelerating devices.
However, the interested reader can refer to [8,10] for the exact details about the
topic.

188
B.V. Patil and P.S.V. Nataraj
We next present our proposed algorithm.
Algorithm unconstrained optimization: [	y, 	p, U]=IBBBU(N, aI, x, xstatus,
ϵp, ϵx)
Inputs: Degree N of the variables occurring in the objective function, the coef-
ﬁcients aI of the objective function in the power form, the initial search domain
x, a column vector xstatus describing the status (continuous or integer) of the
each variable xi (i = 1, 2, . . . , l), the tolerance parameters ϵp and ϵx on the global
minimum and global minimizer(s).
Outputs: A lower bound 	y and an upper bound 	p on the global minimum f ∗,
along with a set U containing all the global minimizer(s) x(i).
BEGIN Algorithm
1. Set y := x and ystatus := xstatus.
2. From aI, compute the Bernstein coeﬃcient array of the objective function
on the box y as (bo(y)).
3. Set 	p := ∞and y := min (bo(y)).
4. Initialize list L := {(y, (bo(y)) , y)}, Lsol := {}.
5. If L is empty then go to step 24. Otherwise, pick the ﬁrst item (y, (bo(y)) , y)
from L, and delete its entry from L.
6. Apply the Bernstein hull consistency algorithm to the relation f(y) ≤	p. If
the result is empty, then delete item (y, (bo(y)) , y) and go to step 5.
y′ = BHC((bo(y)) , aI, I, y, ystatus, 1),
7. Set y := y′ and compute the Bernstein coeﬃcient array of the objective
function on the box y as (bo(y)). Also set y := min (bo(y)).
8. Apply the Bernstein box consistency algorithm to the f(y) ≤	p. If the result
is empty, then delete item (y, (bo(y)) , y) and go to step 5.
y′ = BBC((bo(y)) , y, r, ystatus,r, 1)
r = 1, 2, . . . , l,
where bound contraction will be applied in the rth direction.
9. Set y := y′ and compute the Bernstein coeﬃcient array of the objective
function on the box y as (bo(y)). Also set y := min (bo(y)).
10. {Monotonicity test} If 0 /∈f ′
r(y) for any r ∈{1, 2, ..., l}, discard the item
(y, (bo(y)) , y) and go to step 5.
11. {Concavity test} If Hrr(y) < 0 for some r = 1, . . . , l, discard the item
(y, (bo(y)) , y) and go to step 5.
12. Apply the Bernstein hull consistency algorithm to the relation f ′
yr = 0(r =
1, 2, . . . , l), that is each component of the gradient of f(y). If the result is
empty, then delete item (y, (bo(y)) , y) and go to step 5.
y′ = BHC((br(y)) , aI,r, I, y, ystatus, 0)
r = 1, 2, . . . , l,
where (br(y)) is a Bernstein coeﬃcient array of the ith component of gradient
of the objective function f(y), and aI,r is coeﬃcient of the ith component
of gradient of the objective function f(y).

The Bernstein Branch-and-Bound Unconstrained Global Optimization
189
13. Set y := y′ and compute the Bernstein coeﬃcient array of the objective
function on the box y as (bo(y)). Also set y := min (bo(y)).
14. Apply the Bernstein box consistency algorithm to the relation f ′
yr = 0(r =
1, 2, . . . , l), that is each component of the gradient of f(y). If the result is
empty, then delete item (y, (bo(y)) , y) and go to step 5.
y′ = BBC((br(y)) , y, r, ystatus,r, 0)
r = 1, 2, . . . , l,
where (br(y)) is a Bernstein coeﬃcient array of the ith component of gradient
of the objective function f(y).
15. Set y := y′ and compute the Bernstein coeﬃcient array of the objective
function on the box y as (bo(y)). Also set y := min (bo(y)).
16. Choose a coordinate direction λ parallel to which y1 × · · · × yl has an edge
of maximum length, that is λ ∈{i : w(y) := w(yi), i = 1, 2, . . . , l}.
17. Bisect y normal to direction λ, getting boxes v1, v2 such that y = v1 ∪v2.
We shall use the modiﬁed subdivision procedure given in Sect. 3.1.
18. for k = 1, 2
(a) Find the Bernstein coeﬃcient array and the corresponding Bernstein
range enclosure of the objective function (f) over vk as (b0(vk)) and
B0(vk), respectively.
(b) Set dk := min Bo(vk).
(c) If 	p < dk, then go to substep (f).
(d) Set 	p := min(	p, max Bo(vk)).
(e) Enter (vk, (bo(vk)) , dk) into the list L such that the third members of
all items of the list do not decrease.
(f) end (of k−loop).
19. {Cut-oﬀtest} Discard all items (z, (bo(z)) , z) in the list L that satisfy 	p < z.
20. Denote the ﬁrst item of the list by (y, (bo(y)) , y).
21. {Check the vertex condition} For the item (y, (bo(y)) , y), if min (bo(y))
satisﬁes vertex condition (see Remark 3), then enter (y, (bo(y)) , y) in the
solution list Lsol and return to step 5.
22. If (w(y) < ϵx) & (max Bo(y) −min Bo(y)) < ϵp then remove the item from
the list L, and enter it into the solution list Lsol.
23. Go to step 5.
24. {Compute the global minimum} Set the global minimum 	y to the minimum
of the third entries over all the items in Lsol.
25. {Compute the global minimizers} Find all those items in Lsol for which the
third entries are equal to 	y. The ﬁrst entries of these items contain the global
minimizer(s) x(i).
26. Return the lower bound 	y and upper bound 	p on the global minimum f ∗,
along with the set U containing all the global minimizer(s) x(i).
4
Numerical Tests
In this work, we performed diﬀerent numerical tests with our proposed algorithm
IBBBU. For all computations, we used a desktop PC with Pentium IV 2.40 GHz

190
B.V. Patil and P.S.V. Nataraj
Table 1. Test problems with their characteristics and the global minimum obtained.
Example
l
li f *
Camel back
2 1
−1
Booth
2 1
0
Reaction diﬀusion
3 2
−10.32
Caprasse’s
4 2
−2.87
Adaptive LV
4 2
−0.1
AH Wright
5 4
−30
Magnetism in physics (6) 6 3
−0.25
Butcher
6 1
−1.78
Magnetism in physics (7) 7 6
−0.25
Heart dipole problem
8 5
−5.50
l = Total number of decision variables.
li = Total number of integer decision vari-
ables.
processor with 2 GB RAM and all our presented algorithms are implemented in
MATLAB [1]. We specify an accuracy ϵ = 10−6 for computing the global mini-
mum and global minimizer(s) and allow a maximum number of 500 subdivisions
in the proposed algorithm IBBBU.
We consider a set of 10 test problems. These test problems are taken from
[12,15]. These problems are appropriately modiﬁed to MINLPs assuming some of
the decision variables are integer in nature. We report all these test problems in
Appendix for the sake of completeness. For these test problems, we ﬁrst compare
the performance of the proposed algorithm IBBBU without accelerating devices,
only cut-oﬀtest (that is, the Bernstein algorithm for unconstrained optimization
reported in [9]), only monotonicity test, only concavity test, combinations of the
cut-oﬀ, the monotonicity, and the concavity tests. We next compare the perfor-
mance of the proposed algorithm IBBBU with the use of Bernstein hull consis-
tency (BHC) and Bernstein box consistency (BBC) techniques (see Sect. 3.3) in
combination with the three accelerating devices. Speciﬁcally, we shall apply the
BHC and BBC techniques to the Eqs. (13) and (14) to delete nonoptimal points
from the box x consequently reducing the overall width of the box x.
Table 1 reports the test problems, their dimensions (l), the total number of
integer variables (li), and the global minimum obtained (f ∗). Table 2 gives the
performance comparison of the proposed algorithm IBBBU without accelerating
devices, and the diﬀerent combinations of the accelerating devices. We found
that without accelerating devices the proposed algorithm IBBBU took maxi-
mum number of subdivisions for almost all the problems, except Adaptive LV,
Butcher, and Magnetism in physics (7). Moreover, for the one test problem (AH
Wright) the proposed algorithm IBBBU failed due to the out of memory error in
MATLAB. In the sequel, we also observed the less improvement in the number of
boxes processed with the use of concavity test. Finally, we found the combination

The Bernstein Branch-and-Bound Unconstrained Global Optimization
191
Table 2. Comparison of the number of boxes processed and computational time taken (in seconds) to ﬁnd the global minimum with
diﬀerent combinations of the accelerating devices in the proposed algorithm IBBBU.
Example
Statistics
No accelerating
devices
Only cut-oﬀOnly
monotonicity
Only
concavity
Cut-oﬀ+ Monotonicity Cut-oﬀ+
Monotonicity
+ Concavity
Camel back
Boxes
1004
386
682
1004
362
362
Time
1.44
0.31
0.8
1.59
0.30
0.37
Booth
Boxes
1004
236
342
1004
152
152
Time
1.59
0.18
0.28
1.73
0.11
0.14
Reaction diﬀusion
Boxes
1004
18
2
362
2
2
Time
21.2
0.04
0.05
21.57
0.04
0.04
Caprasse’s
Boxes
1004
634
1004
1004
384
384
Time
3.64
1.44
9.79
3.81
2.09
2.12
Adaptive LV
Boxes
64
56
62
64
54
54
Time
0.07
0.02
0.08
0.06
0.08
0.09
AH Wright
Boxes
*
42
2
20
2
2
Time
0.13
0.11
0.11
0.2
0.12
Magnetism in physics (6) Boxes
1004
610
1004
1004
610
610
Time
9.92
1.93
8.18
10.22
2.21
3.20
Butcher
Boxes
10
10
6
10
6
6
Time
0.02
0.02
0.04
0.02
0.04
0.05
Magnetism in physics (7) Boxes
262
262
262
262
262
262
Time
0.85
0.88
0.80
1.21
0.82
1.23
Heart dipole problem
Boxes
>5000
36
>5000
>5000
36
36
Time
0.12
0.84
0.93
* The algorithm returned “out of memory error”.
* Entry ‘1004’ indicates the algorithm took maximum number of subdivisions to reach the solution.

192
B.V. Patil and P.S.V. Nataraj
Table 3. Comparison of the number of boxes processed and computational time taken
(in seconds) to ﬁnd the global minimum with use of the cut-oﬀtest, the Bernstein hull
and Bernstein box consistencies (to the Eq. (14)) in the proposed algorithm IBBBU.
Example
Statistics Cut-oﬀtest Cut-oﬀ+ BCF Cut-oﬀ+ HCF Cut-oﬀ+ (BCF + HCF)
Camel back
Boxes
386
254
380
232
Time
0.31
0.20
0.32
0.18
Booth
Boxes
236
148
166
110
Time
0.18
0.21
0.15
0.15
Reaction
diﬀusion
Boxes
18
8
6
2
Time
0.04
0.03
0.03
0.01
Caprasse’s
Boxes
634
206
632
204
Time
1.44
1.34
1.45
1.36
Adaptive LV
Boxes
56
26
26
26
Time
0.05
0.05
0.04
0.01
AH Wright
Boxes
42
16
16
8
Time
0.13
0.15
0.16
0.12
Magnetism in
physics (6)
Boxes
610
504
504
390
Time
1.93
9.02
22.62
1.10
Butcher
Boxes
10
10
6
6
Time
0.02
0.03
0.01
0.02
Magnetism in
physics (7)
Boxes
262
262
262
262
Time
0.88
1.40
1.50
2.89
Heart dipole
problem
Boxes
36
4
6
2
Time
0.12
0.11
0.13
0.01
of the cut-oﬀ, and the monotonicity tests to be the most eﬃcient amongst all
others (nearly 50 % reduction in the average number of boxes processed and the
computational time taken to found the global minimum).
We now present numerical ﬁndings which speciﬁcally reports the beneﬁts
obtained (in terms of the number of boxes processed) with pruning tools and
trade-oﬀassociated in terms of the computational time to reach the solution.
Tables 3, 4, and 5 give the performance comparison of the proposed algorithm
IBBBU with the BHC and BBC techniques applied to the Eq. (14) (reported as
ﬂags HCF and BCF, respectively) with its combination with the three accelerat-
ing devices. Overall, we found the combination of all three accelerating devices
with the BCF to be more eﬃcient than HCF, resulting on an average 50 % reduc-
tion in the number of boxes processed and the computational time. Similarly,

The Bernstein Branch-and-Bound Unconstrained Global Optimization
193
Table 4. Comparison of the number of boxes processed and computational time taken
(in seconds) to ﬁnd the global minimum with use of the monotonicity test, the Bern-
stein hull and Bernstein box consistencies (to the Eq. (14)) in the proposed algorithm
IBBBU.
Example
Statistics
Monotonicity
test
Monotonicity +
BCF
Monotonicity +
HCF
Monotonicity +
(BCF + HCF)
Camel back
Boxes
682
114
182
112
Time
0.80
0.12
0.23
1.22
Booth
Boxes
342
96
122
80
Time
0.28
0.20
0.19
0.31
Reaction
diﬀusion
Boxes
2
2
2
2
Time
0.05
0.06
0.08
0.12
Caprasse’s
Boxes
1004
192
344
178
Time
9.79
0.5
0.98
1.92
Adaptive LV
Boxes
62
44
28
26
Time
0.08
0.4
0.50
1.42
AH Wright
Boxes
2
2
2
8
Time
0.11
0.12
0.13
0.12
Magnetism in
physics (6)
Boxes
1004
500
610
140
Time
8.18
1.90
1.20
2.78
Butcher
Boxes
6
34
6
6
Time
0.04
0.35
0.05
0.32
Magnetism in
physics (7)
Boxes
262
262
262
262
Time
0.80
4.47
2.50
5.91
Heart dipole
problem
Boxes
>5000
4
4
2
Time
0.45
0.60
0.67
we observed that the combination of BCF and HCF with all three accelerat-
ing devices to be more eﬃcient for three problems (Camel back, Booth, and
Magnetism in physics (6)). We found on an average more than 50 % reduc-
tion in number of processed boxes, but with an average 5–10 % increase in the
computational time. Further, we note that the Bernstein box consistency algo-
rithm to be more eﬃcient when the domain is small (see, for instance, results for
Caprasse’s in Tables 3, 4, and 5). This is evident from the fact that it involves
Newton operator for pruning which has good convergence properties near the
solution point [8]. Similarly, we note that the Bernstein hull consistency algo-
rithm to be more eﬃcient when the domain is large (see, for instance, results for
Adaptive LV in Tables 3, 4, and 5; result for Camel back in Table 6).
Table 6 reports for the 10 test problems the total number of boxes processed,
and the computational time taken in seconds to ﬁnd the global minimum by the

194
B.V. Patil and P.S.V. Nataraj
Table 5. Comparison of the number of boxes processed and computational time taken
(in seconds) to ﬁnd the global minimum with use of the concavity test, the Bern-
stein hull and Bernstein box consistencies (to the Eq. (14)) in the proposed algorithm
IBBBU.
Example
Statistics
Concavity test
Concavity + BCF
Concavity + HCF
Concavity +
(BCF + HCF)
Camel back
Boxes
1004
254
380
232
Time
1.59
0.74
2.39
2.12
Booth
Boxes
1004
148
166
110
Time
1.73
0.41
0.59
1.02
Reaction
dif-
fusion
Boxes
362
8
6
2
Time
21.57
0.11
0.14
0.11
Caprasse’s
Boxes
1004
180
564
134
Time
3.81
0.90
4.81
4.54
Adaptive LV
Boxes
64
44
28
26
Time
0.06
0.47
0.81
1.42
AH Wright
Boxes
20
16
16
8
Time
0.11
0.19
0.21
0.25
Magnetism in
physics (6)
Boxes
1004
504
504
390
Time
10.22
3.63
5.38
7.92
Butcher
Boxes
10
10
6
6
Time
0.02
0.03
0.16
0.25
Magnetism in
physics (7)
Boxes
262
262
262
262
Time
1.21
14.91
15.31
21.32
Heart
dipole
problem
Boxes
>5000
4
6
2
Time
0.22
0.45
0.62
proposed algorithm IBBBU. Speciﬁcally, we compare the proposed algorithm
IBBBU based on the three diﬀerent ﬂags (A, B, and C) explained as below4:
– A: Application of the Bernstein hull consistency to the Eq. (13). We also apply
the cut-oﬀand the monotonicity tests.
– B: Application of the Bernstein box consistency to the Eq. (13). We also apply
the cut-oﬀand the monotonicity tests.
– C: Application of both Bernstein hull and Bernstein box consistencies to the
Eq. (13) along with the cut-oﬀand the monotonicity tests.
We found the performance of the ﬂags A and B almost similar in terms of number
boxes processed with a very little variation in the computational time, except for
the two test problems (Camel back and Magnetism in physics (6)). On the other
4 We note that concacity test is found to give small improvement in the number of
boxes processed. Hence, we skipped its application in this numerical experimentation.

The Bernstein Branch-and-Bound Unconstrained Global Optimization
195
Table 6. Comparison of the number of boxes processed and computational time taken
(in seconds) to ﬁnd the global minimum with use of the cut-oﬀtest, the monotonicity
test, and the Bernstein hull and Bernstein box consistencies (to the Eq. (13)) in the
proposed algorithm IBBBU.
Example
Statistics A
B
C
Camel back
Boxes
18
170
15
Time
0.28 0.32 0.21
Booth
Boxes
24
24
20
Time
0.22 0.24 0.43
Reaction diﬀusion
Boxes
2
2
2
Time
0.39 0.03 0.03
Caprasse’s
Boxes
346
346
342
Time
2.10 3.10 4.51
Adaptive LV
Boxes
50
50
50
Time
0.71 0.72 0.85
AH Wright
Boxes
2
2
2
Time
0.09 0.08 0.08
Magnetism in physics (6) Boxes
290
382
289
Time
0.58 0.48 0.67
Butcher
Boxes
6
6
6
Time
0.01 0.03 0.04
Magnetism in physics (7) Boxes
40
44
39
Time
2.32 2.31 2.29
Heart dipole problem
Boxes
25
25
23
Time
8.91 9.20 9.57
hand, we found very little improvement with the use of ﬂag C, except for the two
test problems (Camel back and Caprasse’s). Overall, we found this combination
of the pruning tool to be the most eﬃcient one in terms of the number of boxes
processed. However, a signiﬁcant increase (more than 50 %) in the number of
the boxes processed and the computational time was observed, speciﬁcally for a
8-dimensional heart dipole problem.
5
Conclusions
We presented a Bernstein algorithm for ﬁnding the global minimum of the uncon-
strained MINLP problems. The proposed algorithm was composed of several new
tools, such as box partitioning procedure for integer variables, the Bernstein
box (BBC) and the Bernstein hull consistency (BHC) techniques to prune the
solution search space. Diﬀerent numerical experiments were performed with the

196
B.V. Patil and P.S.V. Nataraj
proposed algorithm on a collection of 10 test problems with dimensions rang-
ing from 2 to 8 with the number of integer variables varying from 1 to 6. The
ﬁndings revealed the proposed algorithm to be more eﬃcient than the classical
Bernstein algorithm reported in the literature. Speciﬁcally, it was noted that the
proposed algorithm composed of diﬀerent accelerating devices resulted on an
average 50 % reduction of in the number of boxes processed and the computa-
tional time. Similarly, numerical investigations with the BBC and BHC resulted
on an average 40–50 % reduction in the number of boxes processed and the com-
putational time. In sequel, it was noted that the application of a BBC and BHC
for an 8-dimensional heart dipole problem computationally resulted in a signif-
icant increase (more than 50 %) in the number of the boxes processed and the
computational time. Finding the eﬃcient ways to handle such problems is left
as a future research direction.
Appendix
We list below the test problems studied in this work for conducting diﬀerent
numerical experiments. We denote the test function as f(xk), and the initial
bounds as xk, k = 1, 2, . . . , l.
1. Camel back: The six hump camel back function
min f(x) = 4x2
1 −2.1x4
1 + (1/3)x6
1 + x1x2 −4x2
2 + 4x4
2
x1 ∈Z, x2 ∈R
where xk = [−3, 3], k = 1, 2.
2. Booth: The function deﬁned by Booth
min f(x) = 74 −38x1 + 5x2
1 −34x2 + 8x1x2 + 5x2
2
x1 ∈Z, x2 ∈R
where xk = [−5, 5], k = 1, 2.
3. Reaction diﬀusion: A three dimensional reaction diﬀusion problem
min f(x) = −x1 + 2x2 −x3 −0.835634534x2(1 −x2)
x1, x2 ∈Z, x3 ∈R
where xk = [−5, 5], k = 1, 2, 3.
4. Caprasse’s: The system deﬁned by Caprasse
min f(x) = −x1x3
3 + 4x2x2
3x4 + 4x1x3x2
4 + 2x2x3
4 + 4x1x3
+ 4x2
3 −10x2x4 −10x2
4 + 2
x1, x3 ∈Z, x2, x4 ∈R
where xk = [−1, 1], k = 1, 3 and xk = [−0.5, 0.5], k = 2, 4.

The Bernstein Branch-and-Bound Unconstrained Global Optimization
197
5. Adaptive LV: A neural network modeled by an adaptive Lotka-Volterra
system
min f(x) = x1x2
2 + x1x2
3 + x1x2
4 −1.1x1 + 1
x1, x2 ∈Z, x3, x4 ∈R
where x1 = [0, 1], x2 = [−20, 20], xk = [−2, 2], k = 3, 4.
6. AH Wright: The system deﬁned by Wright
min f(x) = x1 + x2 + x3 + x4 −x5 + x2
5 −10
x1, x2 ∈R, x3, x4, x5 ∈Z
where xk = [−5, 5], k = 1, . . . , 5.
7. Magnetism in Physics (6): A six variable magnetism in physics problem
min f(x) = 2x2
1 + 2x2
2 + 2x2
3 + 2x2
4 + 2x2
5 + x2
6 −x6
x1, x2, x3 ∈Z, x4, x5, x6 ∈R
where xk = [−1, 1], k = 1, . . . , 6.
8. Butcher: A function deﬁned by Butcher
min f(x) = x6x2
2 + x5x2
3 −x1x2
4 + x3
4 + x2
4 −(1/3)x1 + (4/3)x4
x1 ∈Z, xk ∈R, k = 2, . . . , 6
where xk = [−1, 1], k = 1, 2, 3, x4 = [−0.1, 0.2], x5 = [−0.3, 1.1], x6 =
[−1.1, −0.3].
9. Magnetism in physics (7): Seven variable magnetism in physics problem
min f(x) = x2
1 + 2x2
2 + 2x2
3 + 2x2
4 + 2x2
5 + 2x2
6 + 2x2
7 −x1
x1 ∈R, xk ∈Z, k = 2, . . . , 7
where xk = [−1, 1], k = 1, . . . , 7.
10. Heart dipole: A heart dipole problem
min f(x) = −x1x3
6 + 3x1x6x2
7 −x3x3
7 + 3x3x7x2
6 −x2x3
5
+ 3x2x5x2
8 −x4x3
8 + 3x4x8x2
5 −0.9563453
xk ∈Z, k = 1, . . . , 5, xk ∈R, k = 5, 7, 8
where xk = [−1, 1], k = 1, 2, 3, x4 = [−1, 0], x5 = [0, 1], x6 = [−0.1, 0.2]
x7 = [−0.3, 1.1], x8 = [−1.1, −0.3].

198
B.V. Patil and P.S.V. Nataraj
References
1. The Mathworks Inc., MATLAB version 7.1 (R14), Natick, MA (2005)
2. Bonami, P., Biegler, L.T., Conn, A., Cornuejols, G., Grossmann, I.E., Laird, C.,
Lee, J., Lodi, A., Margot, F., Sawaya, N., W¨achter, A.: An algorithmic framework
for convex mixed integer nonlinear programs. Discrete Optim. 5(2), 186–204 (2008)
3. Buchheim, C., D’Ambrosio, C.: Box-constrained mixed-integer polynomial opti-
mization using separable underestimators. In: Lee, J., Vygen, J. (eds.) IPCO 2014.
LNCS, vol. 8494, pp. 198–209. Springer, Heidelberg (2014)
4. D’Ambrosio, C., Lodi, A.: Mixed integer nonlinear programming tools: an updated
practical overview. Ann. Oper. Res. 204(1), 301–320 (2013)
5. Floudas, C.A.: Nonlinear and Mixed-integer Optimization: Fundamentals and
Applications. Oxford University Press, New York (1995)
6. GAMS Development Corp.: GAMS-The solver manuals, Washington, DC (2009)
7. Garloﬀ, J.: The Bernstein algorithm. Interval Comput. 2, 154–168 (1993)
8. Hansen, E., Walster, G.W.: Global Optimization Using Interval Analysis. CRC
Press, New York (2004)
9. Nataraj, P.S.V., Arounassalame, M.: A new subdivision algorithm for the Bernstein
polynomial approach to global optimization. Int. J. Autom. Comput. 4(4), 342–352
(2007)
10. Nataraj, P.S.V., Arounassalame, M.: Constrained global optimization of multi-
variate polynomials using Bernstein branch and prune algorithm. J. Glob. Optim.
49(2), 185–212 (2011)
11. Patil, B.V., Nataraj, P.S.V., Bhartiya, S.: Global optimization of mixed-integer
nonlinear (polynomial) programming problems: the Bernstein polynomial app-
roach. Computing 94(2–4), 325–343 (2012)
12. Ratz, D., Csendes, T.: On the selection of subdivision directions in interval branch-
and-bound methods for global optimization. J. Glob. Optim. 7(2), 183–207 (1995)
13. Stahl, V.: Interval methods for bounding the range of polynomials and solving
systems of nonlinear equations. Ph.D. thesis, Johannes Kepler University, Linz
(1995)
14. Tawarmalani, M., Sahinidis, N.V.: Convexiﬁcation and Global Optimization in
Continuous and Mixed-Integer Nonlinear Programming: Theory, Algorithms, Soft-
ware, and Applications. Nonconvex Optimization and its Applications. Kluwer
Academic Publishers, Dordrecht (2002)
15. Verschelde, J.: PHC pack, the database of polynomial systems. Technical report,
Mathematics Department, University of Illinois, Chicago, USA (2001)

Dynamical Systems

Interval Regularization Approach to the Firordt
Method of the Spectrophotometric Analysis
of the Non-separated Mixtures
Valentin Golodov(B)
Faculty of Computational Mathematics and Informatics, South Ural State University
(National Research University), Lenin prospekt 76, 454080 Chelyabinsk, Russia
avaksa@gmail.com, golodovva@susu.ac.ru
http://computer.susu.ac.ru
Abstract. This paper describes the author’s experiences with applica-
tion the so-called interval regularization approach to one of the chemical
analysis methods, speciﬁcally the Firordt method – the spectrophotomet-
ric analysis of non-separated mixtures. In our approach, the uncertainty is
described using intervals. The solution can be found for well-determined
and overdetermined systems (when the number of the measurements
exceeds number of the mixture components), also interval statement con-
siders measurements errors. Exact rational computations are important
part of the technique of solving interval task.
Keywords: System of linear equations · Interval uncertainty · Interval
regularization · Firordt method · Exact computations
1
Introduction
The Firordt method is one of the methods of the analysis of the non-separated
mixtures [1]. According to the Firordt’s method, we can determine the concen-
tration cj of the each of the m components by solving the following system of
the equations:
bi =
m

j=1
aij · cj · l,
(1)
where:
• bi is the measured absorbancy of the analyzed mixture on the i-th analytical
wave length (AWL),
• aij is an molar coeﬃcient of the absorption (or extinction) of the j-th com-
ponent on i-th AWL (measured in advance for each component),
• l is the thickness of the absorbing layer.
Number of the AWL(k) (number of the equations) usually is equal to the number
of the components (m) in the mixture. Overdetermined systems with k > m may
be used for the enhanced accuracy.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 201–208, 2016.
DOI: 10.1007/978-3-319-31769-4 16

202
V. Golodov
Spectrophotometric measurements are always performed with some measure-
ment errors, so, we have some imprecise system of linear algebraic equations for
analysis with equations of the form (1). Here and below in the paper we use
the standard notation of interval analysis [2], particularly, all interval values are
written using bold type, e.g. A, bi are interval matrix and interval value corre-
spondingly. Non-interval values typed with mathematical italic type as usual.
bi =
m

j=1
aij · cj · l,
(2)
System 2 will become simpler if all measurements are performed using l equal
to 1 centimeter, then system takes on the form
bi =
m

j=1
aij · cj,
(3)
or, in matrix form, Ax = b, where x – is the sought for vector of the components
concentrations.
2
Interval Regularization Approach
2.1
Essence of the Method
We consider interval system of linear algebraic equations Ax = b, with an inter-
val matrix A and interval right-hand side vector b, as a model of imprecise
system of linear algebraic equations of the same form.
We use a new regularization procedure proposed in [5] that reduces the solu-
tion of the imprecise linear system to computing an point from the tolerable
solution set (Ξtol(A, b) = {x ∈Rm | (∀A ∈A)(∃b ∈b)(Ax = b)}) of the interval
linear system with a widened right-hand side.
Tolerable solution set is the least sensitive, among all the solution sets [3], to
the change in the interval matrix of the system Ax = b, it may be demonstrated
by following representation of the Ξtol(A, b).
Ξtol(A, b) =

A∈A

x ∈Rm  (∃b ∈b) (Ax = b)

,
(4)
In the above formula,

x ∈Rm  (∃b ∈b)(Ax = b)

is the solution set to the
interval system Ax = b with the interval uncertainty concentrated only in the
right-hand side vector. We exploit this idea that may be called interval regular-
ization for the system of equations of the Firordt method (3).
Straightforward replacement of the Ax = b to the Ax = b often leads to
empty Ξtol(A, b). Right-hand part of the interval system may be extended using
some non-negative parameter z ∈R, z >= 0.
So we have to ﬁnd point from the tolerable solution set of the system
Ξtol(A, b(z)) with the widened right-hand part. The form of the extension may

Interval Regularization Approach to the Firordt Method
203
vary from one task to another by using expertise. For the present proved that
extension of the form b(z) = [b−zp, b+zq], where p, q ∈Rm, p > 0, q > 0 enables
to ﬁnd point from the tolerable solution set Ξtol(A, b(z)) for any given A, b.
Minimization of the extension of the right-hand part leads to the smallest
possible set Ξtol(A, b(z)). Usually, when z ̸= 0, it contains only one point,
although in the general case, of course, Ξtol(A, b(z)) may contains more than
one point.
Minimum z∗of the parameter z, ensuring non-empty Ξtol(A, b(z∗)), jointly
with the corresponding extension of the right-hard part of the system b(z∗) =
[b −z∗p, b + z∗q] also usable as the measure of degeneracy or the measure of
instability of the initial task. Also big value of the z∗shows that form of the
extension (vectors p, q) is unsatisfactory to the solving problem.
Well known regularization methods has the known failures. One well known
method of solving system with non-square or degenerated matrix is normal
pseudo-solution that is the solution of the system A⊤Ax = A⊤b. However, e.g.
the simple 2 × 2 system:

(1 + ε)x + y = 1,
ε ≥0
x + y = 1,
(5)
have traditional solution (x, y) = (0, 1)⊤for any ε ̸= 0 and normal pseudo-
solution (x, y) = (1/2, 1/2)⊤for ε = 0, consequently we have no convergence
of the traditional solution to the normal pseudo-solution when ε →0. For the
non-degenerated systems pseudo-solution is equal to its traditional solution.
Other well known method is the Tikhonov regularization procedure, with
regard to the linear system of equations it leads to solving of the system (A⊤A−
δE)x = A⊤b, where (A⊤– is transposed matrix of the system, E – is unity
matrix, δ – is the parameter of the regularization, selection of the δ is the theme
of a lot of papers. However, e.g., for the system with Hilbert matrix the procedure
doesn’t leads to success [4].
The interval regularization not constrains type of the initial imprecise sys-
tem of linear equations type or possible the system’s degeneracy. The initial
system of linear equations could be underdetermined, well-determined or overde-
termined. Next section deals with essential aspects of the computing technique
what allows to operate even with strongly ill-conditioned systems using exact
computations [6].
For the consistency, solution in the traditional sense of the point system
of linear equations Ax = b is, obviously, agree with solution in the interval
regularization sense of the interval linear system Ax = b, where A = [A, A],
b = [b, b].
2.2
Computing Technique
Computing technique is based on the theorem [5]:

204
V. Golodov
Theorem 1. There exists a solution x+∗and x−∗∈Rm, z∗∈R to the linear
programming problem
min
x+, x−, z z,
(6)
m

j=1
(aijx+
j −aijx−
j ) ≥bi −zpi,
i = 1, 2, . . . , k,
(7)
m

j=1
(aijx+
j −aijx−
j ) ≤bi + zqi,
i = 1, 2, . . . , k,
(8)
x+
j , x−
j , z ≥0,
j = 1, 2, . . . , k.
(9)
In addition, the vector x∗= x+∗−x−∗belongs to Ξtol(A, b(z∗)). Vectors p, q ∈
R+k are used to manipulate with form of the right-hand part extension.
We use simplex method to solve linear programming task (6)–(9), as one
of the simplest and practically fast, but typical implementations of the simplex
method could fail because of the linear programming task’s strong degeneracy.
In our approach, we use exact computations [6] and procedure described in [7]
to prevent simplex method cycling, such synergy allows to solve ill-conditioned
problems sensitive to the data precision.
Simplex method requires only basic arithmetic (plus, minus, multiply and
divide) during calculation, so, for the linear programming task with rational
coeﬃcients all calculation are being performed in rational number ﬁeld. Interval
regularization approach is usable even in the cases where well known regular-
ization methods are inapplicable, e.g. for the tasks with the disturbed Gilbert
and Vandermonde matrix or tasks with the disturbed matrix of the Godunov
task [4]. Some computing experiments with Gilbert matrix was introduced pre-
viously in [5].
Computational complexity of the interval regularization approach is deﬁned
by complexity of the simplex method solving the corresponding linear program-
ing task (6)–(9). The discussion about the simplex method complexity is given
in [7], in most cases method requires linear (depended on task size) number of
the iterations. Theoretically proved that solving of any linear programming task
with rational coeﬃcients has polynomial complexity [7].
2.3
Applying the Approach to the Firordt Method
The Firordt method requires to measure the extinction aij for each individual
component i and each analytical wave length λj. Values aij are calculated using
proportion A = a · c · l with c = 1 mole per liter and l = 1 centimeter. Because
of measurements errors during solution preparation and error of the spectropho-
tometer during extinction measuring each value aij is actually the interval value
aij. Minimum relative error of the data in the computing experiment below is
1 %, more realistically relative error 5 % of all measurements.

Interval Regularization Approach to the Firordt Method
205
We apply our approach to the system Ax = b where aij and bi are data
of the absorbancy measurements for the individual components and mixture
correspondingly. Vector x is unknown concentrations of the components.
For the case when Ξtol(A, b(z)) is empty useful extension form is b(z) =
[bi−z, bi+z]i=1...n, but when absorption levels diﬀer a lot for taken wave lengths
proportional extension b(z) = [bi−z|bi|, bi+z|bi|], i = 1, . . . , k could be preferred.
Advantages of the interval analog of the Firordt method are:
• The approach results in robust solutions, we can use larger or smaller error
estimates in the measurements data and the ﬂuctuations will be minor.
• The approach is useful for the raw data when optimal set of the AWL is
unknown.
• The approach has more correct model for the solution of the overdetermined
systems, than, e.g., normal pseudo-solution.
• Value z∗optimal parameter of the right-hand part extension could be indicator
of the goodness of the linear model of absorbancy of the mixture.
• Using of the exact rational calculations gives absolute repeatability of the
calculation experiments.
3
Computing Experiment
Tables 1, 2, 3, 4 and Fig. 1 demonstrate example of the spectrophotometric data
for the model solutions with the ions of the Cu, Ni and its mixture.
Let us consider collection of examples with the given data. First, since given
measurements was done on the model solution, we know real concentrations of
the mixture components cCu = 0.5 cNi = 0.5 mole per liter, so, relation errors
for the all results δ = (δCu, δNi)⊤below will be calculated using this values.
To apply the traditional Firordt method we should choose two analytical wave
lengths (AWL) and solve corresponding system with A ∈R2×2, b ∈R2×1. Proper
selection of data subset is non-trivial problem, data set contains measurements
Table 1. Data of the model solutions with the ions of the Cu, Ni and its mixture (1).
λ AWL (nm) 410
420
430
440
450
460
470
480
490
500
aCu
0.044 0.038 0.036 0.032 0.03
0.03
0.028 0.028 0.03
0.034
aNi
1.252 0.832 0.45
0.244 0.162 0.124 0.088 0.054 0.034 0.026
bNiCu
0.662 0.428 0.244 0.138 0.096 0.078 0.06
0.044 0.034 0.033
Table 2. Data of the model solutions with the ions of the Cu, Ni and its mixture (2).
λ AWL (nm) 510
520
530
540
550
560
570
580
590
600
aCu
0.04
0.048 0.064 0.084 0.114 0.15
0.198 0.266 0.352 0.45
aNi
0.026 0.032 0.04
0.046 0.052 0.062 0.082 0.11
0.148 0.198
bNiCu
0.037 0.044 0.055 0.069 0.087 0.109 0.142 0.187 0.25
0.319

206
V. Golodov
Table 3. Data of the model solutions with the ions of the Cu, Ni and its mixture (3).
λ AWL (nm) 610
620
630
640
650
660
670
680
690
700
aCu
0.59
0.754 0.942 1.21
1.46
1.81
2.158 2.564 2.95
3.388
aNi
0.254 0.33
0.398 0.484 0.546 0.572 0.562 0.566 0.588 0.616
bNiCu
0.409 0.536 0.681 0.829 1.001 1.191 1.352 1.545 1.766 2.016
Table 4. Data of the model solutions with the ions of the Cu, Ni and its mixture (4).
λ AWL (nm) 710
720
730
740
750
760
770
780
790
800
aCu
3.792 4.21
4.546 4.546 5.024 5.15
5.25
5.262 5.228 5.206
aNi
0.64
0.654 0.646 0.61
0.572 0.512 0.45
0.39
0.338 0.288
bNiCu
2.204 2.407 2.552 2.722 2.849 2.846 2.879 3.038 3.008 3.007
Fig. 1. Individual absorption levels of the mixture components (Cu, Ni) and mixture
absorption level (Ni+Cu). Also allowable measurement error borders are given (Upper
*** Border, Lower *** Border).
on 40 diﬀerent AWL and diﬀerent pairs of AWL could give essentially distinct
results.
Papers devoted to selection of the best subset of the measurements, gives
some tips [1]: e.g., data corresponding to the peaks of the absorbancy of the indi-
vidual components or data corresponding to the measurements where absorbancy
levels for individual components are most diﬀer. More strictly matrix of the sys-
tem should have least condition number between other possible matrixes [1].
Pair of equations which yields the matrix A ∈R2×2 with the least possible
condition number among other 2×2 matrixes, is the pair of equations correspond-
ing to λ1 = 410 (nm) and λ2 = 660 (nm), condition number μ440,660 = 1.53.
For this system traditional solution of the system is: cCu = 0.496427 cNi =

Interval Regularization Approach to the Firordt Method
207
0.511308 (relative errors are δCu = 0.71 % and δNi = 2.26 % correspondingly).
Though it is good result best result is: cCu = 0.5 cNi = 0.5 (relative errors
are δCu = 0.0 % and δNi = 0.0 % correspondingly) for pairs corresponding to
the λ1 = 440 (nm) and λ2 = 450 (nm), condition number μ440,450 = 77.17 or
λ1 = 440 (nm) and λ2 = 590 (nm), condition number μ440,590 = 1.89, and some
other ones.
Interval regularization approach gives equal results for the speciﬁed pairs of
AWL and consideration of the (1 %) and (5 %) measurement error does not aﬀect
to the result values, result are the same because of the small matrixes are not
so strongly ill-conditioned.
So, traditional method is sensitive to selection of the wave lengths, the matrix
with the least condition number does not guarantee best result and matrix with
relatively great condition number may give very good results.
A lot of pairs of AWL gives results with relative error more than 100 % or
gives negative values of concentrations of components. Interval regularization
can not fully smooth over eﬀect of bad data without additional good data but
provide additional information in coeﬃcient of the right-hand part extension z∗.
E.g., for λ1 = 760 (nm), λ2 = 780 (see the Table 4), cCu = 0.649 cNi = −0.977
(relative errors are δCu = 29.95 % and δNi = 295.451 % correspondingly). Using
interval regularization approach we can see that solution is very unstable, see
Table 5: Data for the Cu component are relatively consistent with the model but
for Ni component data are inconsistent. Here interval regularization gives some
additional information, that may be used to further calculation experiments.
Table 5. Unstable system of equation. Dependence between supposed measurement
error Δ and the solution of the interval task.
Δ(%)
(0 %)
(1 %)
(5 %)
(10 %) (20 %)
cCu 0.649
0.648
0.641
0.641
0.563
0.562
cNi
−0.977 −0.95
−0.891 −0.891 0.000
0.000
z∗
0.0
0.0098 0.0716
0.0456
0.063
0.062
Further improvement of the result accuracy may be produced by using full
set of the experimental data in calculation, so we have overdetermined system
Ax = b, A ∈Rm×2. It may be solved, for example, using pseudo-solution, i.e.
the solution of the system A⊤Ax = A⊤b and we can use all available data
corresponding to 40 diﬀerent AWL. So, we have overdetermined system Ax = b,
A ∈R40×2 solution is cCu = 0.5234 cNi = 0.4390 (relative error is 4.69 % and
12.21 % correspondingly).
Our approach gives robust solution ci
Cu = 0.5156 ci
Ni = 0.5277 (relative
error is 3.12 % and 5.54 % correspondingly) as a point from the tolerable solu-
tion set Ξtol(A, b(z)) of the corresponding interval system Ax = b(z)) with
b(z) = [b −z, b + z] z∗= 0.0716. Parameter z∗= 0.0716 and corresponding
b(z∗) = [b −z∗, b + z∗] displays that data correlates accurately with the Firordt
method model, the same result is for data with 1 % and 5 % measurement errors.

208
V. Golodov
Results are consolidated in Table 6.
Table 6. Consolidated results. Value ci corresponds to measurement error Δ = 1 %.
c∗⊤= (0.5, 0.5)
λ1 = 410, λ2 = 660 λ1 = 440, λ2 = 450, etc. A ∈R40×2
c⊤= (cCu, cNi)
(0.496, 0.511)
(0.500, 0.500)
—
δ⊤= (δCu, δNi)
(0.71 %, 2.26 %)
(0.00, 0.00)
—
cps⊤= (ci
Cu, ci
Ni) (0.496, 0.511)
(0.500, 0.500)
(0.5234, 0.4390)
δps⊤= (δps
Cu, δps
Ni) (0.71 %, 2.26 %)
(0.00 %, 0.00 %)
(4.69 %, 12.21 %)
ci⊤= (ci
Cu, ci
Ni)
(0.496, 0.511)
(0.500, 0.500)
(0.515, 0.527)
δi⊤= (δi
Cu, δi
Ni)
(0.71 %, 2.26 %)
(0.00 %, 0.00 %)
(3.12 %, 5.54 %)
z
0.0
0.0
0.0716
4
Conclusions
With regards to the system of equations of the Firordt method (especially overde-
termined) so-called interval regularization technique provides the enhanced
robustness and accuracy. Interval analog of the Firordt method gives robust
result and provides additional information about consistency between data
and the Firordt method’s linear model in minimal right-hand part extension
coeﬃcient z∗.
Proposed computing technique essentially uses exact rational computations,
it allows to solve sensitive and ill-conditioned problems [6] and provides full
repeatability of the computing experiment. Also, interval regularization app-
roach may be useful for other linear and linearizable non-linear mathematical
models.
References
1. Vlasova, I.V., Vershinin, V.I.: Determination of binary mixture components by
the Firordt method with errors below the speciﬁed limit. J. Anal. Chem. 64(6),
553–558 (2009)
2. Kearfott, R.B., Nakao, M.T., Neumaier, A., Rump, S.M., Shary, S.P., van Henten-
ryck, P.: Standardized notation in interval analysis. Comput. Technol. 15(1), 7–13
(2010)
3. Shary, S.P.: A new technique in systems analysis under interval uncertainty and
ambiguity. Reliable Comput. 8(5), 321–418 (2002)
4. Dikoussar, V.V.: Some numerical methods for solving linear algebraic equations.
Soros Educ. J. 9, 111–120 (1998). (in Russian)
5. Panyukov, A.V., Golodov, V.A.: Computing best possible pseudo-solutions to inter-
val linear systems of equations. Reliable Comput. 19(2), 215–228 (2013)
6. Golodov, V.A., Panyukov, A.V.: Library of classes “Exact Computation 2.0”. State.
Reg. 201361818, 14 March 2013: Oﬃcial Bulletin of Russian Agency for Patents
and Trademarks (in Russian). In: Series “Programs for Computers, Databases,
Topology of VLSI”, No. 2, Federal Service for Intellectual Property (2013)
7. Shrejver, A.: Theory of Linear and Integer Programming. Wiley, New York (1986)

Computing Capture Tubes
Luc Jaulin1(B), Daniel Lopez4, Vincent Le Doze2,
St´ephane Le Menec2, Jordan Ninin1, Gilles Chabert3,
Mohamed Saad Ibnseddik1, and Alexandru Stancu4
1 Lab-STICC, ENSTA-Bretagne, Brest, France
lucjaulin@gmail.com
2 MBDA-Airbus Group, Paris, France
3 Ecole des Mines de Nantes, Nantes, France
4 School of Electrical and Electronic Engineering,
Control Systems Research Group, University of Manchester,
Manchester M139PL, UK
Abstract. Many mobile robots such as wheeled robots, boats, or plane
are described by nonholonomic diﬀerential equations. As a consequence,
they have to satisfy some diﬀerential constraints such as having a radius
of curvature for their trajectory lower than a known value. For this type
of robots, it is diﬃcult to prove some properties such as the avoidance of
collisions with some moving obstacles. This is even more diﬃcult when
the initial condition is not known exactly or when some uncertainties
occur. This paper proposes a method to compute an enclosure (a tube)
for the trajectory of the robot in situations where a guaranteed interval
integration cannot provide any acceptable enclosures. All properties that
are satisﬁed by the tube (such as the non-collision) will also be satisﬁed
by the actual trajectory of the robot.
Keywords: Capture tube · Contractors · Interval arithmetic ·
Robotics · Stability
1
Introduction
A dynamic system can generally be described a state equation of the form:
Sf : ˙x (t) = f (x (t) , t) .
(1)
In the situation where the system is uncertain, the state equation becomes a
time dependent diﬀerential inclusion:
SF : ˙x (t) ∈F (x (t) , t) .
(2)
Validation of the stability properties of such systems is an important and diﬃcult
problem [15]. Most of the time, this problem can be transformed into proving the
inconsistency of a constraint network. For invariant systems (i.e., f or F do not
depend on t), it has been shown [10] that the V-stability approach combined with
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 209–224, 2016.
DOI: 10.1007/978-3-319-31769-4 17

210
L. Jaulin et al.
interval analysis [16] can solve the problem eﬃciently. Here, we extend this work
to systems where f depends on time. Moreover, we will show how to compute
a capture tube, i.e., a set-valued function which associate to each t a subset of
Rn and such that a feasible trajectory cannot escape. For this, we will need to
combine guaranteed integration and Lyapunov theory, such as in [19] or [13], in
order to compute this capture tube.
The paper is organized as follows. Section 2 deﬁnes the notion of capture tube,
which is a speciﬁc set of trajectories that encloses the unknown trajectory for
the robot. Section 3 explains how tubes can be represented inside the computer
and how we can calculate a tube for a trajectory which satisﬁes a diﬀerential
inclusion. Section 4 provides a new algorithm that is able to calculate an interval
of tubes which encloses the smallest capture tube which contains one candidate
tube. An illustrative test-case is presented in Sect. 5 and a conclusion of the
paper is given in Sect. 6.
2
Capture Tube
A tube G (see e.g., [1]) is a function which associates to each t ∈R a subset of
Rn. Tubes are used for several applications in nonlinear control such as model
predictive control [12] or state estimation [2].
Notations. Depending on the context, a tube G will be seen as a set-valued
function t →P (Rn), or also as a subset of R × P (Rn), where P (Rn) is the set
of subsets of Rn. It will often be written as G (·) or also G (t) to recall that it is
a function of t. For instance, when we write x(t) ∈G(t), we mean ∀t, x(t) ∈G(t)
and when we write (ta, a) ∈G(t), we mean a ∈G(ta).
■
Consider an autonomous system described by a state equation Sf : ˙x =
f (x, t) or a diﬀerential inclusion SF : ˙x ∈F (x, t). A tube G(t) is said to be a
capture tube [5] (or also called positive invariant tube) for Sf or SF if we have
the following implication:
Fig. 1. A tube (painted gray) and possible trajectories for diﬀerent initial conditions.
If a trajectory such as the one represented by the dotted curve exists then the tube is
not a capture tube

Computing Capture Tubes
211
x(ta) ∈G(ta), τ > 0 ⇒x(ta + τ) ∈G(ta + τ).
(3)
Figure 1 gives some feasible trajectories and a tube G(t) (in gray). In this ﬁgure,
all the trajectories are consistent with the assumption that G(t) is a capture tube,
except the trajectory represented by the dotted curve at the bottom, which was
able to escape from the tube for t = ta. Consider the tube
G (·) : t →{x | g (x, t) ≤0} ,
(4)
where g : Rn × R →Rm is assumed to be diﬀerentiable with respect to both
x and t. The following theorem shows that the problem of proving that G (t)
is a capture tube can be cast into proving that a set of inequalities has no
solution.
Theorem 1a. If the system of constraints (called the cross-out conditions)
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
(i) ∂gi
∂x (x, t) · f(x, t) + ∂gi
∂t (x, t)


	
˙gi(x,t)
≥0,
(ii)
gi (x, t) = 0,
(iii)
g (x, t) ≤0,
(5)
is inconsistent (i.e., for all x, all t ≥0, and all i ∈{1, . . . , m}, the inequalities
are not satisﬁed), then G (·) : t →{x | g (x, t) ≤0} is a capture tube for the
system ˙x = f (x, t).
Sketch of proof (see [21,23] for more details). If G (t) is not a capture tube, it
means that there exists one trajectory, which leaves G (t), i.e., which crosses the
ith boundary gi (x, t) = 0 from inside to outside. This means that there exists a
time-space pair (a, ta) on the boundary of G (t) (i.e., such that (ii) and (iii) are
satisﬁed) and such that ˙gi (x, t) ≥0 (otherwise the trajectory cannot leave the
tube).
■
Example 1. Consider again Fig. 1 where we assume that the gray tube corre-
sponds to G (·) : t →{x | g1 (x, t) ≤0}. The dotted trajectory leaves the tube
at a time-space point (ta, a), such that g1 (a, ta) = 0 and ˙g1 (a, ta) > 0. If such a
trajectory is feasible, then G (·) cannot be a capture tube.
Example 2. We now illustrate the diﬃculty to get a capture tube on the simple
pendulum described by the state equations

 ˙x1 = x2
˙x2 = −sin x1 −0.15 · x2
(6)
where x1 is the position of the pendulum and x2 its rotational speed (see Fig. 2).
To ﬁnd a positive invariant set (i.e., a capture tube) for such a mechanical system
the classical method is to take sublevel sets of the energy of the system. Indeed,
since the energy of the system
E (x) = 1
2 ˙x2
1 −cos x1 + 1 = 1
2x2
2 −cos x1 + 1
(7)

212
L. Jaulin et al.
Fig. 2. Simple pendulum
is supposed to decrease with time, we may think that it may be a good candidate
for the function g. Let us propose for g (x, t), which deﬁnes our candidate for
the capture tube (or positive invariant tube):
g (x, t) = E (x) −1 = 1
2x2
2 −cos x1,
(8)
which is here time independent. The cross-out conditions of Theorem 1a are
⎧
⎨
⎩
(i)
sin x1 x2
 
x2
−sin x1 −0.15 · x2

= −0.15 · x2
2 ≥0,
(ii)
1
2x2
2 −cos x1 = 0.
(9)
Note that, since g (x) is scalar, we have i = 1 and the condition (iii) is a
consequence of (ii). This system has two solutions: x =

± π
2 , 0

. Therefore,
Theorem 1a cannot conclude that our tube is positive invariant. Note that, even
for this simple two dimensional example which is time-invariant and for which
we have a good intuition of a function (the energy) which decreases (almost
always), getting a capture tube is diﬃcult. We will see in Sect. 3 how a capture
tube can be computed automatically.
Theorem 1b. If the system of constraints (cross-out conditions)
⎧
⎪
⎪
⎨
⎪
⎪
⎩
(i1) ∂gi
∂x (x, t) · a + ∂gi
∂t (x, t) ≥0,
(i2)
a ∈F (x, t) ,
(ii)
gi (x, t) = 0,
(iii)
g (x, t) ≤0,
(10)
is inconsistent for all x, all a, all t ≥0, and all i ∈{1, . . . , m} then G (·) : t →
{x | g (x, t) ≤0} is a capture tube for the diﬀerential inclusion ˙x ∈F (x, t).
Proof. The proof is a direct consequence of Theorem 1a. See also [23].

Computing Capture Tubes
213
Consequence. From Theorems 1a and 1b, we conclude that checking that “a
tube deﬁned by inequalities is a capture tube” amounts to checking that a set
of constraints (here (5) or (10)) is inconsistent. This type of results was already
known since several decades [9,23]. Now, proving such an inconsistency can easily
be performed [21] using ontractor-based methods [7].
We now have a procedure to prove that a tube is a capture tube. In practice,
such a capture tube is diﬃcult to obtain, especially for nonholonomic robots.
Even if we have a good intuition of the system and if we are very conﬁdent on
a potential tube, a contractor-based algorithm often ﬁnds a counterexample. In
the following section, we will give a new method able to compute automatically
capture tubes.
3
Computing with Tubes
3.1
Representation of Tubes
Recall that a tube is a function which associates to any t ∈R a subset of Rn. In
the case where these subsets are intervals or boxes, a tube can be represented in
the computer by stepwise functions (see [2,4]) as illustrated in Fig. 3.
Fig. 3. In numerical computations, a tube [f] (t) can be approximated by a lower and
an upper stepwise functions f −(t) and f + (t). The tube [f] (t) encloses an uncertain
trajectory f (t)
Another possible representation of a tube (see [16]) is an interval expression,
which depends on t. For instance,
[f] (t) = [1, 2] · t + sin ([1, 3] · t)
(11)

214
L. Jaulin et al.
corresponds to such a tube. Interval polynomials [16] also enter within this class.
An example of a third degree polynomial tube is given by
[f] (t) = [a0] + [a1] t + [a2] t2 + [a3] t3,
(12)
where the [ai] are known intervals. The advantage of interval polynomial is that
all operations on scalar polynomials (such as integral, composition, etc.) can
easily be extended to this class. For instance
 t
0
[f] (τ) dτ = [a0] t + [a1] t2
2 + [a2] t3
3 + [a3] t4
4 .
(13)
It has been proved [16] for the integration, for the composition, and other oper-
ations (such as +, −, /, ·) that the fundamental inclusion property is satisﬁed.
More precisely, for the integration, this inclusion property is
f (·) ∈[f] (·) ⇒∀t,
 t
0
f (τ) dτ ∈
 t
0
[f] (τ) dτ.
(14)
Remark. For the derivative, this extension cannot be done. For a counterex-
ample, consider the relation
sin(ωt) · t ∈[−1, 1] · t.
(15)
It is clear that we cannot conclude that
ω cos(ωt) · t + sin(ωt) ∈[−1, 1] .
(16)
Thus, the fundamental inclusion property, which is required by all set-
membership approaches, is not satisﬁed for the derivative.
3.2
Guaranteed Integration
For the problem we consider in this paper, i.e., computing capture tubes, the
guaranteed integration will be needed. Guaranteed integration is a set of tech-
niques, which make it possible to compute a tube that encloses the solution of a
state equation or to enclose all solutions of a diﬀerential inclusion. We here recall
the principle of these techniques. For more details on the guaranteed integra-
tion of state equations, see [14,17] or [3,18]. To our knowledge in the literature,
the extension of these techniques to diﬀerential inclusion is rarely done. This is
why we present here the basic concepts of the guaranteed integration in order to
show how they can be extended to the uncertain case, i.e., to diﬀerential inclu-
sions. More details and more eﬃcient algorithms for the interval integration of
diﬀerential inclusions can be found in [11,22]
Brouwer Theorem. Any continuous function f mapping a compact convex set
X into itself has a ﬁxed point, i.e.,
∃x ∈X | f (x) = x.
(17)
Note that a direct corollary of this theorem is that these ﬁxed points also belong
to the set f (X).

Computing Capture Tubes
215
Example 3. Take f (x) = sin (x) · cos (x) and X = [−2, 2]. Since
f ([−2, 2]) ⊂sin ([−2, 2]) · cos ([−2, 2]) = [−1, 1] · [−1, 1] = [−1, 1] ⊂X.
(18)
From the Brouwer theorem, we have
∃x ∈[−2, 2] | sin (x) · cos (x) = x.
(19)
The Brouwer theorem is the corner stone that will make it possible to compute a
tube containing the solution of a state equation. For its extension to diﬀerential
inclusions, the uncertain case will be treated using a parametric version of the
Brouwer theorem.
Parametric Brouwer Theorem. If f : X × U →X, where X is a convex
compact set and f is continuous with respect to x ∈X, then
∀u ∈U, ∃x ∈X| f (x, u) = x.
(20)
Example 4. Take f (x) = sin (x + u) · cos (2x −u) and X = [−2, 2] and u ∈R.
Since
f ([−2, 2] , R) ⊂[−1, 1] ⊂X,
(21)
we have
∀u ∈R, ∃x ∈[−2, 2] | sin (x + u) · cos (2x −u) = x.
(22)
Guaranteed Integration of State Equations. Consider the system ˙x = f (x),
where f is Lipschitz continuous. The initial condition x∗
0 is known. We want to
have an interval enclosure for the trajectory x∗(·)1. Deﬁne the Picard-Lindel¨of
operator as
T : x (·) →

t →x∗
0 +
 t
0
f (x (τ)) dτ

.
(23)
Since f is Lipschitz continuous, T has a unique ﬁxed point which corresponds to
the solution x∗(·) of the state equation. Take an interval tube [x] (·). By interval
tube, we mean that for all t, [x] (t) is a box of Rn and not any subset of Rn, as it
is allowed for general tubes of Rn. From the Brouwer theorem and since T has
a unique ﬁxed point, we have
T ([x] (·)) ⊂[x] (·) ⇒x∗(·) ∈[x] (·) .
(24)
Figure 4 provides a representation of the tubes [x] (·) and T ([x] (·)). Note that,
due to the speciﬁc form of T , around the initial instant t = 0, the tube T ([x] (·))
is thin. Note also that we do not have T ([x] (·)) ⊂[x] (·) (i.e., T ([x] (t)) is
included in [x] (t) only for t ≤t1) and the trajectory may leave the tubes. If we
restrict application of T over the interval [0, t1], we get the inclusion. Therefore,
∀t ∈[0, t1] , x∗(t) ∈T ([x] (t)) ,
(25)
1 A trajectory x, which is a function from R to Rn, can be denoted equivalently x (t)
or x (·). When no ambiguity may exist, i.e., when t is already used in the same
paragraph, we shall often prefer x (t), for simplicity.

216
L. Jaulin et al.
Fig. 4. Illustration of the Picard-Lindel¨of operator to the tube [x] (t)
where
t1 = max

t ∈R+ | ∀τ ∈[0, t] , T ([x] (τ)) ⊂[x] (τ)

.
(26)
Of course, the operator can be called several times, i.e.,
∀i ≥0, ∀t ∈[0, t1] , x∗(t) ∈T i ([x] (t)) .
(27)
Case with Uncertainties. Assume now, that x0 is uncertain and that the
system now depends on an uncertain input vector u (·) More precisely, the system
is described by
˙x = f (x, u) ,
(28)
where x0 ∈[x0] and u (·) ∈[u] (·). By setting F (x, t) = {f (x, u) | u (t) ∈[u] (t)},
we obtain that a diﬀerential inclusion can be described with this formalism. We
assume that f is Lipschitz continuous with respect to x. The Picard operator
Tx0,u : x (·) →x0 +
 t
0
f (x (τ) , u (τ)) dτ,
(29)
has uncertainty now. For all x0, and all u (·), the operator Tx0,u has a unique
ﬁxed point x∗(t). Consider a tube X (·). If
Tx0,u (X (·)) ⊂X (·)
(30)
then, from the Brouwer theorem, X (·) contains at least one ﬁxed point, i.e.,
x∗(·) ∈X (·).
Methodology. For a guaranteed integration, we ﬁrst have to ﬁnd a potential
tube for which we think that it contains the unique solution of the state equation
or contain all solutions of the diﬀerential inclusion. This candidate could be
obtained using an Euler integration method from [x0] followed by an inﬂation.
Then we compute a tube T + ([x] (t)) which encloses the tube
T ([x] (t)) = [x (0)] +
 t
0
f ([x] (τ) , τ) dτ,
(31)

Computing Capture Tubes
217
or the tube
T ([x] (t)) = [x (0)] +
 t
0
F ([x] (τ) , τ) dτ,
(32)
in the case we have to deal with a diﬀerential inclusion. As illustrated in Fig. 4,
we compute
t1 = max
t≥0

t | ∀τ ∈[0, t] , T + ([x] (τ)) ⊂[x] (τ)

.
(33)
Within the interval [0, t1], from the Brouwer theorem, we conclude that the tube
T + ([x] (·)) encloses the solution.
High Order Taylor Method. For a more eﬃcient integration [20], we can
replace the Picard-Lindel¨of ﬁxed point equation:
x (t) = x0 +
 t
0
˙x (τ) dτ
(34)
by the higher order ﬁxed points Taylor equation with the integral remainder
x (t) = x0 +
k

i=1
1
i!

x(i) (0)

ti +
 t
0
x(k+1) (τ)
k!
(t −τ)k dτ.
(35)
Note that for k = 0, we get the Picard-Lindel¨of equation. This high order method
is particularly suited to situations where [x0] is known (or small). Indeed, when
x0, is known, the ﬁxed point Taylor operator becomes
T ([x] (t)) = x0 +
k

i=1
1
i!

x(i) (0)

ti +
 t
0
[x](k+1) (τ)
k!
(t −τ)k dτ.
(36)
All uncertainties, stored inside [x](k+1), are divided by k!. Now, in practice,
the width of [x](k+1) (τ) increases polynomially with k, whereas k! increases
exponentially. Thus, the accuracy increases with k. The tube [x](k+1) (t) for
x(k+1) (t) is computed from the tube [x] (t) using the expression of the state
equation ˙x = f (x, u).
Remark. Consider the particular case where k = 2 and the system ˙x = f (x, u).
We have:
¨x = ∂f
∂x (x, u) · f (x, u) + ∂f
∂x · ˙u = ψ2 (x, u, ˙u) .
(37)
For a more general k ≥0, we get:
x(k+1)= ψk+1 
x, u, ˙u, . . . , u(k)
.
(38)
We have an analytical expression ψk+1 
x, u, ˙u, . . . , u(k)
, but this expression
depends on ˙u, . . . , u(k). Now, a tube for ˙u, . . . , u(k) is not available in the case

218
L. Jaulin et al.
of diﬀerential inclusions. More precisely, ˙x ∈F (x, t) can be cast into the form
˙x = f (x, u) , u ∈[u] but nothing can be deduced on ˙u, ¨u, etc. Thus, high order
methods will have diﬃculties to deal with diﬀerential inclusions. To deal with
uncertain dynamics using a k-order ﬁxed point Taylor method, we need to be able
to express the system in the form ˙x = f (x, u) with u ∈[u] , . . . , u(k) ∈

u(k)
.
4
Computing Capture Tubes
4.1
Basic Idea
If a candidate G (t) for a capture tube is available, we can prove that G (t)
is a capture tube by checking the inconsistency of a set of onlinear equations
(see the previous sections). This inconsistency can then easily be checked using
interval analysis. Now, for many systems such as for nonholonomic systems, we
rarely have a candidate for a capture tube and we need to ﬁnd one. The main
contribution of this paper is to provide a method that can help us to ﬁnd such a
capture tube. The idea is to start with a non-capture tube G(t) (the candidate)
and to try to characterize the smallest capture tube which encloses G(t). To do
this, we predict for all (x, t), which satisfy the cross-out conditions, a guaranteed
envelope for the trajectory within ﬁnite time-horizon window [t, t + t2] (where
t2 > 0 is ﬁxed). If all corresponding x(t + t2) belong to G(t + t2), then the union
of all trajectories and the initial G (t) (in the (x, t) space) corresponds to the
smallest capture tube enclosing G (t).
4.2
Lattice and Capture Tubes
First, let us remark that since the set of subsets of Rn is a lattice with respect to
the inclusion ⊂, the set of tubes (T, ⊂) is also a lattice. When we introduced the
basic idea of how we could compute a capture tube, we wrote that we wanted
to compute the smallest tube, which encloses the candidate G(t). This notion of
the smallest tube makes sense because of the following theorem.
Theorem 2. Consider a state space system Sf : ˙x = f (x, t) or a diﬀerential
inclusion SF : ˙x ∈F (x, t). The set of capture tubes (Tc, ⊂) for Sf or SF is a
sublattice of the set of tubes (T, ⊂).
Proof. Consider two captures tubes G1(t) and G2(t). If the trajectory x(t)
belongs to both G1(t) and G2(t), then x(t) will leave neither G1(t) nor G2(t).
Thus, the intersection G1(t) ∩G2(t) is a capture tube. The same reasoning can
be done for the union of the two tubes. Since G1(t) ∩G2(t) is the largest tube
included in G1(t) and G2(t) and since G1(t) ∪G2(t) is the smallest tube which
contains G1(t) and G2(t), we conclude that (Tc, ⊂) is a lattice. Since all capture
tubes are also tubes, we get that (Tc, ⊂) is a sublattice of (T, ⊂).
■
Consequences. Since Tc is a sublattice of T, for any tube G(t) ∈T, we can
deﬁne the following operator:
capt (G(t)) =
 
G(t) ∈Tc | G(t) ⊂G(t)

.
(39)
This set corresponds to the smallest capture tube which encloses G(t).

Computing Capture Tubes
219
Interval of Tubes. The set of tubes is a lattice with respect to the inclusion ⊂.
Thus, we can deﬁne intervals of tubes. This notion is important in this paper,
because we need to compute a tube, in a guaranteed way. Now, this tube may
probably not be representable in the computer. This new notion of interval of
tubes will be needed in order to characterize the tube we want to calculate.
4.3
Computing Capture Tubes
Since the set of tubes (T, ⊂) is a lattice, we can deﬁne intervals of tubes as
follows.
Deﬁnition. An interval of tubes [G] is a subset of the set of tubes T which
satisﬁes
[G] = {G ∈T | G ⊂∨[G] and G ⊃∧[G]} .
(40)
Here, G+ = ∨[G] denotes the smallest outer bound of [G] and G−= ∧[G]
denotes the largest inner bound of [G]. The set of intervals of tubes will be
denoted by IT. Note that we could also deﬁne the notion of interval of capture
tubes, but this notion is not interesting in our context since it is very diﬃcult
to get (exactly) even one capture tube.
Problem to be Solved. Given a tube G(·) : t →{x | g (x, t) ≤0} in T, com-
pute an interval [C−(t), C+(t)] ∈IT such that
capt (G(t)) ∈

C−(t), C+(t)

.
(41)
This is illustrated in Fig. 5. Of course, since G(t) ⊂capt(G(t)), we can take
C−(t) = G(t). Thus, the main diﬃculty is to get a tube C+(t), which is not too
large.
Flow. The ﬂow associated with the system Sf : ˙x = f (x, t) is a function φt0,t1 :
Rn →Rn such that
˙x = f (x, t) ⇒φt0,t1 (x (t0)) = x (t1) .
(42)
This means that if the trajectory x (t) is a solution of Sf, we are able to go from
the state at instant t0 to the state at instant t1 using the ﬂow.
The ﬂow associated with the diﬀerential inclusion SF : ˙x ∈F (x, t) is a
function φt0,t1 : Rn →P (Rn),
˙x ∈F (x, t) ⇒x (t1) ∈φt0,t1 (x (t0)) .
(43)
φt0,t1 should also be the smallest with respect to the inclusion which satisﬁes this
property. Equivalently, φt0,t1 (x (t0)) corresponds to the set of all states that can
be reached at instant t1 ≥t0 by a trajectory consistent with SF and initialized
at x (t0) for t = t0.

220
L. Jaulin et al.
Fig. 5. The capture tube capt(G(t)), that we want to compute, will be enclosed by an
interval of tubes

C−(t), C+(t)

Theorem 3a. Consider the system Sf : ˙x = f (x, t). The tube
C(·) : t →{x |∃(x0, t0) , x0 ∈G(t0), t ≥t0, x = φt0,t (x0)},
(44)
where φt0,t is the ﬂow function of Sf, corresponds to capt(G(t)).
Proof of Theorem 3a. We will show that C(t), is the smallest capture tube
which encloses G(t). For the proof, we will prove (i) that C(t) contains G(t), (ii)
that C(t) is a capture tube and (iii) that C(t) is the smallest one.
(i) To prove that G(t) ⊂C(t), it suﬃces to take t0 = t and x0 = x.
(ii) We now prove that C(t) is a capture tube. Take a pair (xta, ta) such that
xta ∈C(ta). From (44), we have
∃(x0, t0) , x0 ∈G(t0), ta ≥t0, xta = φt0,ta (x0) .
(45)
Take τ > 0 and deﬁne the point xta+τ = φta,ta+τ (xta). From (45), we have
∃(x0, t0) , x0 ∈G(t0), ta ≥t0, xta+τ = φt0,ta+τ (x0) .
(46)
Therefore, we have proved that
xta ∈C(ta), τ ≥0 ⇒φta,ta+τ

xta
∈C (ta + τ) ,
(47)
i.e., C(t) is a capture tube.

Computing Capture Tubes
221
(iii) We will now prove by contradiction that C(t) is the smallest capture tube
that encloses G(t). Take a capture tube G(t) such that G(t) ⊃G(t) which is
enclosed strictly in C(t). By strictly, we mean that ∃(t1, x1), x1 ∈C(t1) and x1 /∈
G(t1). From (44), ∃(x0, t0) , x0 ∈G(t0), x1 = φt0,t1 (x0). The corresponding
trajectory crosses the tube G(t) from inside to outside which is inconsistent
with the fact that G(t) is a capture tube.
■
Theorem 3b. Consider the system SF : ˙x ∈F (x, t). The tube
C(t) : t →{x | ∃(x0, t0) , x0 ∈G(t0), t ≥t0, x ∈φt0,t (x0)},
(48)
where φt0,t is the set membership ﬂow function of SF, corresponds to capt(G(t)).
Proof. The proof is a direct consequence of Theorem 3a.
■
Theorem 4a. Consider the system Sf : ˙x = f (x, t). We have
capt (G(t)) = G(t) ∪ΔG(t),
(49)
with
ΔG(t) = t →{x | ∃(x0, t0) satisfying (5),
t ≥t0, x = φt0,t (x0) and x /∈G(t) }.
(50)
Proof. To build capt(G(t)), it suﬃces to add to the tube G(t) all pairs (x1, t1)
outside G(t) that can be reached from a pair (xa, ta) in G(t). The corresponding
trajectory will cross the boundary of the tube G(t) at instant t0 at the state x0,
i.e., (x0, t0) satisﬁes (5). This is illustrated in Fig. 6.
■
Theorem 4b. Consider the diﬀerential inclusion SF : ˙x = F (x, t). We have
capt (G(t)) = G(t) ∪ΔG(t),
(51)
with
ΔG(t) = t →{x | ∃(x0, t0) satisfying (10),
t ≥t0, x ∈φt0,t (x0) and x /∈G(t) }.
(52)
Fig. 6. ΔG(t) contains all pairs (x1, t1) outside G (t) that can be reached from a pair
(x0, t0) leaving G (t)

222
L. Jaulin et al.
Consequences. An interval [C−(t), C+(t)] for capt(G(t)) will be composed by
the tube C−(t) = G(t) and by adding to C−(t) an enclosure of all trajectories
generated from one pair (x0, t0) satisfying (5) or (10).
5
Test Case
Consider the pendulum presented in Sect. 2. Here, we do not consider the sublevel
sets of the energy anymore, which only applies on a small class of systems.
Instead, we consider, as an candidate tube, the one associated with the function
g (x, t) = x2
1 + x2
2 −1.
We have chosen here a time-invariant tube in order to be able to draw pictures.
Indeed, both G(t) and ΔG(t) do not depend on t and become subsets of R2. Our
algorithm provides the results shown in Fig. 7. Subﬁgure (a) depicts a subpaving
Fig. 7. (a) Boxes which enclose the points satisfying the cross-out conditions; (b) guar-
anteed integration ΔG of these boxes; (c) inner approximation C−of Capt(G); (d) outer
approximation C+ of Capt(G)

Computing Capture Tubes
223
which encloses all points satisfying the cross-out conditions. The guaranteed
integration ΔG of all these boxes are shown on Subﬁgure (b). The integration
has been performed using the Dynibex library [8]. Subﬁgure (c) represents a
subpaving made with boxes shown to be inside G. Since G ⊂capt(G), this
subpaving also corresponds to an inner approximation C−of capt(G). Subﬁgure
(d) shows C+ which is the union of light gray boxes (back plane) and dark gray
boxes (front plane). This union forms an outer approximation of capt(G).
6
Conclusion
Proving that a controlled nonlinear system always stays inside a time moving
bubble (or tube) amounts to proving a set of nonlinear inequalities. Now, in
practice, even with a good intuition, ﬁnding such a signiﬁcant capture tube is
diﬃcult. This paper proposes a new method for computing an approximation
of the smallest tube, which encloses a candidate tube G (t). Even if G (t) is
generally chosen as rather attractive, it is often possible to cross G (t) from inside
to outside during the initialization of the system. Since this tube may not be
representable in the computer, the method calculates an interval of tubes which
encloses the capture tube we want to compute. The principle of the approach
is to integrate (with a guaranteed interval integration) the state vectors that
cross the candidate tube from inside to outside and to add all the corresponding
trajectories to the candidate tube. Now, since the less we integrate, the more we
are eﬃcient, to deal with large scale systems, it should be necessary to limit the
number of integration by giving more importance to the Lyapunov part of the
resolution. This could be done, for instance, by computing barrier functions [6].
References
1. Aubin, J.P., Frankowska, H.: Set-Valued Analysis. Birkh¨auser, Boston (1990)
2. Le Bars, F., Sliwka, J., Reynet, O., Jaulin, L.: State estimation with ﬂeeting data.
Automatica 48(2), 381–387 (2012)
3. Berz, M., Makino, K.: Veriﬁed integration of ODEs and ﬂows using diﬀerential
algebraic methods on high-order Taylor models. Reliable Comput. 4(3), 361–369
(1998)
4. Bethencourt, A., Jaulin, L.: Solving non-linear constraint satisfaction problems
involving time-dependant functions. Math. Comput. Sci. 8(3), 503–523 (2014)
5. Blanchini, F., Miani, S.: Set Theoretic Methods in Control. Birkhauser, Boston
(2008)
6. Bouissou, O., Chapoutot, A., Djaballah, A., Kieﬀer, M.: Computation of paramet-
ric barrier functions for dynamical systems using interval analysis. In: IEEE CDC,
Los Angeles, United States (2014)
7. Chabert, G., Jaulin, L.: Contractor programming. Artif. Intell. 173, 1079–1100
(2009)
8. Chapoutot, A., Alexandre dit Sandretto, J., Mullier, O.: Dynibex. ENSTA (2015).
http://perso.ensta-paristech.fr/∼chapoutot/dynibex/
9. Fernandes, M.L., Zanolin, F.: Remarks on strongly ﬂow-invariant sets. J. Math.
Anal. Appl. 128, 176–188 (1987)

224
L. Jaulin et al.
10. Jaulin, L., Le Bars, F.: An interval approach for stability analysis: application to
sailboat robotics. IEEE Trans. Robot. 27(5), 282–287 (2012)
11. Kapela, T., Zgliczynski, P.: A lohner-type algorithm for control systems and ordi-
nary diﬀerential inclusions. Discrete Continuous Dyn. Syst. 11(2), 365–385 (2009)
12. Langson, W., Chryssochoos, I., Rakovic, S.V., Mayne, D.Q.: Robust model predic-
tive control using tubes. Automatica 40(1), 125–133 (2004)
13. Lhommeau, M., Jaulin, L., Hardouin, L.: Capture basin approximation using inter-
val analysis. Int. J. Adap. Control Sig. Process. 25(3), 264–272 (2011)
14. Lohner, R.: Enclosing the solutions of ordinary initial and boundary value prob-
lems. In: Kaucher, E., Kulisch, U., Ullrich, C.H. (eds.) Computer Arithmetic:
Scientiﬁc Computation and Programming Languages, pp. 255–286. BG Teubner,
Stuttgart (1987)
15. Le Menec, S.: Linear diﬀerential game with two pursuers and one evader. In: Bre-
ton, M., Szajowski, K. (eds.) Advances in Dynamic Games, vol. 11, pp. 209–226.
Birkhauser, Boston (2011)
16. Moore, R.E.: Interval Analysis. Prentice-Hall, Englewood Cliﬀs (1966)
17. Nedialkov, N.S., Jackson, K.R., Corliss, G.F.: Validated solutions of initial value
problems for ordinary diﬀerential equations. Appl. Math. Comput. 105(1), 21–68
(1999)
18. Raissi, T., Ramdani, N., Candau, Y.: Set membership state and parameter esti-
mation for systems described by nonlinear diﬀerential equations. Automatica 40,
1771–1777 (2004)
19. Ratschan, S., She, Z.: Providing a basin of attraction to a target region of polyno-
mial systems by computation of Lyapunov-like functions. SIAM J. Control Optim.
48(7), 4377–4394 (2010)
20. Revol, N., Makino, K., Berz, M.: Taylor models and ﬂoating-point arithmetic: proof
that arithmetic operations are validated in COSY. J. Logic Algebraic Program. 64,
135–154 (2005)
21. Stancu, A., Jaulin, L., Bethencourt, A.: Stability analysis for time-dependent non-
linear systems: an interval approach. Internal report, University of Manchester
(2015)
22. Wilczak, D., Zgliczynski, P.: Cr-Lohner algorithm. Schedae Informaticae 20, 9–46
(2011)
23. Yorke, J.A.: Invariance for ordinary diﬀerential equations. Math. Syst. Theor. 1(4),
353–372 (1967)

Some Remarks on the Rigorous Estimation
of Inverse Linear Elliptic Operators
Takehiko Kinoshita1,2(B), Yoshitaka Watanabe3, and Mitsuhiro T. Nakao4
1 Center for the Promotion of Interdisciplinary Education and Research,
Kyoto University, Kyoto 606-8501, Japan
kinosita@kurims.kyoto-u.ac.ip
2 Research Institute for Mathematical Sciences,
Kyoto University, Kyoto 606-8502, Japan
3 Research Institute for Information Technology,
Kyushu University, Fukuoka 812-8581, Japan
4 National Institute of Technology, Sasebo College, Nagasaki 857-1193, Japan
Abstract. This paper presents a new numerical method to obtain the rigorous
upper bounds of inverse linear elliptic operators. The invertibility of a linearized
operator and its norm estimates give important informations when analyzing the
nonlinear elliptic partial differential equations (PDEs). The computational costs
depend on the concerned elliptic problems as well as the approximation proper-
ties of used ﬁnite element subspaces, e.g., mesh size or so. We show the proposed
new estimate is effective for an intermediate mesh size.
1
Introduction
The main aim of this paper is to provide an efﬁcient estimates of a solution of the fol-
lowing linear elliptic partial differential equations (PDEs) with the Dirichlet boundary
condition:
−△u+(b·∇)u+cu = f
in Ω,
u = 0
on ∂Ω,
(1a)
(1b)
for an arbitrary f ∈L2(Ω). Here, Ω ⊂Rd, (d ∈{1,2,3}) is a bounded polygonal
or polyhedral domains, b ∈L∞(Ω)d, and c ∈L∞(Ω). As well known, many physical
problems have a linearized problem of the form (1a)-(1b), e.g., the stationary Burgers
equations [7].
Now let L2(Ω) be the set of all measurable functions from Ω to C with square
integrable, which is a Hilbert space with associated inner product (u,v)L2(Ω) :=

Ω u(x)v(x)dx, where · shows the complex conjugate. Let H1
0(Ω) := {u ∈H1(Ω)u = 0
on∂Ω} be the usual Sobolev space with respect to the inner product (u,v)H1
0 (Ω) :=
(∇u,∇v)L2(Ω)d. Let L : H1
0(Ω)×H1
0(Ω) →C be a bilinear form deﬁned by
L(u,v) := (∇u,∇v)L2(Ω)d +((b·∇)u,v)L2(Ω) +(cu,v)L2(Ω) ,
∀u,v ∈H1
0(Ω).
c⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 225–235, 2016.
DOI: 10.1007/978-3-319-31769-4 18

226
T. Kinoshita et al.
We deﬁne the weak solution u ∈H1
0(Ω) of (1a)-(1b) by a solution of the following
variational equation:
L(u,v) = (f,v)L2(Ω) ,
∀v ∈H1
0(Ω).
(2)
If we assume the coercivity of L, then, by the Lax-Milgram theorem, there exists a
unique solution for (2). Moreover, it can be proved that this weak solution is a solution
of (1a)-(1b) by a regularity argument (see e.g., [1]). This fact means that the linear
elliptic operator L := −△+b·∇+c has the inverse operator.
On the other hand, Plum [10], Oishi [9], Nakao-Hashimoto-Watanabe [7] and
Kinoshita-Watanabe-Nakao [5] proposed a computational technique to verify the exis-
tence of L −1 even though the coercivity of L is not assumed. In this paper, we also do
not assume the coercivity to L at all. Moreover, we try to ﬁnd the quantitative value of
CL2,H1
0 satisfying
L −1
L(L2(Ω),H1
0 (Ω)) ≤CL2,H1
0 .
(3)
The constant CL2,H1
0 plays an essential role in the numerical veriﬁcation of solu-
tions for the boundary value problems for nonlinear elliptic PDEs [9,10] and it is
desirable to compute CL2,H1
0 as small as possible. Particularly, the constant CL2,H1
0
proposed by Watanabe-Kinoshita-Nakao [13] is expected to converge to exact norm
L −1
L(L2(Ω),H1
0 (Ω)) as the discretization parameter h →0 on the suitable assump-
tions. Therefore, in the asymptotic sense, the estimates of (3) by [13] would give better
bounds than the results in [7]. Indeed, many numerical examples show this situation.
However, in order to get successful calculation of CL2,H1
0 in [13], we often need smaller
mesh size h than [7]. In other words, we could verify the existence of L −1 by the
method in [7] with smaller computational costs than [13].
In this paper, we present a new method to compute the constant CL2,H1
0 in (3) based
on the perturbation theory of linear operator with technique in [7]. The veriﬁcation
condition of the existence of L −1 by the proposed method is essentially same as in [7].
But as shown in the numerical results, the proposed CL2,H1
0 is often better.
The contents of this paper are as follows: In Sect. 2, we deﬁne the necessary nota-
tions and function spaces. In Sect. 3, we introduce previous results of the invertibility
of L and its a posteriori estimates. In Sect. 4, we propose a new veriﬁcation condi-
tion for the invertibility of L and its a posteriori estimates. In Sect. 5, we show several
veriﬁcation results for the proposed procedures.
2
Notations
Let X and Y be the Banach spaces. We represent the space of the bounded linear
operators from X to Y by L (X ,Y ). Especially, L (X ) denotes L (X ,X ). Let
LC(X ,Y ) ⊂L (X ,Y ) be the space of the compact operators from X to Y . More-
over, LF(X ,Y ) ⊂L (X ,Y ) denotes the set of the bounded Fredholm operators
from X to Y . For any linear operator A : X →Y , D(A ), R(A ), and N(A ) denote
the domain, range, and kernel of A , respectively. We deﬁne the norm of D(A ) by
∥u∥D(A ) := ∥u∥X + ∥A u∥Y , which is called graph norm. As well known, if A is a
closed operator then D(A ) becomes a Banach space with respect to ∥·∥D(A ).

Some Remarks on the Rigorous Estimation of Inverse Linear Elliptic Operators
227
Let −△: D(−△) ⊂L2(Ω) →L2(Ω) be a Laplace operator, where the domain
D(−△) is deﬁned by
D(−△) =

u ∈H1
0(Ω) ; −△u ∈L2(Ω)

.
Then, −△is a closed operator from L2(Ω) to L2(Ω). We deﬁne the differential
operator B ∈L

H1
0(Ω),L2(Ω)

by B := b · ∇+ c. The differential operators are
treated as the closed operators in many cases. However, it is more convenient to treat
the differential operators as the bounded operators in our veriﬁcation method. Let
Ie : D(−△) →H1
0(Ω) be an embedding operator. Then, Ie ∈LC

D(−△),H1
0(Ω)

is
satisﬁed by the Rellich compactness theorem because Ω is in a class of the bounded
domain with the Lipschitz continuous boundary. Moreover, BIe ∈LC

D(−△),L2(Ω)

is satisﬁed because composition operator of the bounded operator and compact operator
is a compact operator. The bounded operator L ∈L

D(−△),L2(Ω)

is represented
by L := −△+ BIe = −△+ b · ∇Ie + cIe. Especially, the domain of L is deﬁned by
D(L ) = D(−△). Then, L ∈LF

D(−△),L2(Ω)

and ind(L ) = 0 by [5].
The norms of Banach space L∞(Ω)d and L∞(Ω) are deﬁned by
∥b∥L∞(Ω)d := ess sup
x∈Ω
	
|b1(x)|2 +···+|bd(x)|2,
∥c∥L∞(Ω) := ess sup
x∈Ω
|c(x)|,
respectively.Let Cs,2 be a positive constant satisfying ∥u∥L2(Ω) ≤Cs,2 ∥u∥H1
0 (Ω) for all
u ∈H1
0(Ω), which is called the Poincar´e constant.
Let Sh(Ω) be an approximate ﬁnite dimensional subspace of H1
0(Ω) dependent on
the parameter h. For example, Sh(Ω) is considered to be a ﬁnite element subspace with
the mesh size h or a set of polynomials less than a ﬁxed degree. Let n be a degree of
freedom for Sh(Ω) and {φi}n
i=1 be the basis functions of Sh(Ω). Namely, Sh(Ω) :=
span 1≤i≤n{φi}.
We denote the self-adjoint positive deﬁnite (SPD) matrices Dφ and Lφ in Cn×n by
Dφ,i, j := (∇φ j,∇φi)L2(Ω)d ,
Lφ,i, j := (φ j,φi)L2(Ω) ,
∀i, j ∈{1,...,n}.
Since Dφ and Lφ are SPD, these have the Cholesky factorization. Let D1/2
φ
and L1/2
φ
be
the Cholesky factors of Dφ and Lφ, respectively, i.e.,
Dφ = D1/2
φ DH/2
φ
,
and
Lφ = L1/2
φ LH/2
φ
where DH/2
φ
shows the conjugate matrix of D1/2
φ . We deﬁne the H1
0 projection P1
h :
H1
0(Ω) →Sh(Ω) by

u−P1
h u,vh

H1
0 (Ω) = 0,
∀vh ∈Sh(Ω).
(4)
Therefore, the problems of the solvability of the variational Eq. (4) and the nonsingular-
ity of Dφ are equivalent. Because the matrix Dφ is positive deﬁnite, the projection P1
h is
well deﬁned. Now, we assume that the following error estimates of P1
h hold throughout
this paper.

228
T. Kinoshita et al.
Assumption 1. There exists a positive constant C(h) > 0 satisfying
u−P1
h u

H1
0 (Ω) ≤C(h)∥△u∥L2(Ω) ,
∀u ∈D(−△),
u−P1
h u

L2(Ω) ≤C(h)
u−P1
h u

H1
0 (Ω) ,
∀u ∈H1
0(Ω).
(5)
(6)
Assumption 1 is the most basic error estimates in the Galerkin method. For example, in
the case of the one dimensional bounded interval as Ω, if Sh(Ω) is a ﬁnite element space
using piecewise linear polynomials, the valueC(h) is known byC(h) = h
π . Alternatively,
in the case of piecewise quadratic polynomials, Assumption 1 is satisﬁed by C(h) = h
2π .
Moreover, these approximations give the optimal constants (e.g., [6]). In case that N
degree polynomials are used, Assumption 1 is satisﬁed by C(h) = O( h
N ). However, in
these cases, the optimal constants are unknown (e.g., [3]). In case of the two or three
dimensional bounded rectangular or rectangular cuboid domain as Ω, if Sh(Ω) is a
ﬁnite element space using the tensor product of one dimensional piecewise polynomial
spaces, C(h) is attained same constants in one dimensional case (e.g., [6]). In case of
the two dimensional bounded polygonal domain as Ω, if Sh(Ω) is the P1 ﬁnite element
space with triangular mesh, Assumption 1 is satisﬁed. The details of C(h) are shown in
e.g., [2].
Let Gφ be a matrix in Rn×n, where each elements are deﬁned by
Gφ,i, j := L(φ j,φi) = (∇φ j,∇φi)L2 +((b·∇)φ j,φi)L2 +(cφ j,φi)L2 ,
∀i, j ∈{1,...,n}.
We assume that Gφ is nonsingular throughout this paper. Applying the proposed ver-
iﬁcation method, it is necessary to conﬁrm the nonsingularity of Gφ by validated
computations.
3
Previous Results
In this section, we introduce the results for the invertibility condition of the operator L
and its a posteriori estimates. We deﬁne the following constants:
C1 := ∥b∥L∞(Ω)d +Cs,2 ∥c∥L∞(Ω) ,
C2 := ∥b∥L∞(Ω)d +C(h)∥c∥L∞(Ω) ,
M11
φ (h) :=
DH/2
φ
G−1
φ D1/2
φ

2 ,
M10
φ (h) :=
DH/2
φ
G−1
φ L1/2
φ

2
where ∥·∥2 is the matrix two-norm, i.e., the maximum singular value.
Theorem 1 ([7, Theorem 2.1 & Corollary 1 & Theorem 2.3] & [8]). Let ˜K(h) > 0
be deﬁned by
˜K(h) :=
⎧
⎪
⎨
⎪
⎩
C(h)

Cs,2 ∥divb∥L∞(Ω) +C1

i f b ∈W 1,∞(Ω)d,
Cs,2C2
i f b ∈L∞(Ω)d\W 1,∞(Ω)d.

Some Remarks on the Rigorous Estimation of Inverse Linear Elliptic Operators
229
And let ˜κφ > 0 be a constant satisfying
˜κφ := C(h)

C1M11
φ (h) ˜K(h)+C2) < 1.
(7)
Then, there exists L −1 ∈L

L2(Ω),D(−△)

and CL2,H1
0 in (3) can be taken as
CL2,H1
0 =
Cs,2
1−˜κφ

M11
φ (h)

1−C2C(h)

M11
φ (h) ˜K(h)
M11
φ (h)C1C(h)
1

2
.
(8)
If b has sufﬁcient regularity, from the fact that ˜K(h) = O

C(h)

, the CL2,H1
0 deﬁned
(8) converges to:
CL2,H1
0 →Cs,2

M11
φ (0) 0
0
1

2
= Cs,2 max

M11
φ (0),1

(9)
as h →0, where M11
φ (0) := limh→0 M11
φ (h). This a posteriori estimates fails to converge
to its exact operator norm. On the other hand, Watanabe-Kinoshita-Nakao proposed
another a posteriori estimates in [5,13] as follows.
Theorem 2 ([13, Theorem 4.2] & [5, Theorem 4.3]). Assume that ˆκφ > 0 satisfy
ˆκφ := C(h)C2

1+M10
φ (h)C1

< 1.
(10)
Then, there exists L −1 ∈L

L2(Ω),D(−△)

and CL2,H1
0 in (3) can be taken as
CL2,H1
0 =
	
M10
φ (h)2 +C(h)2
1+M10
φ (h)C1
2
1−ˆκφ
.
(11)
The right hand side of (11) is expected to converge to the exact operator norm as h →0.
Therefore, we expect that (11) would give better estimates than (8). In fact, we can prove
M10
φ (h) ≤Cs,2M11
φ (h) for arbitrary h > 0. However, in the actual veriﬁcation process,
we often meet the situation such that the criterion (10) is harder than (7) for a ﬁxed
h. Therefore, Theorem 1 should be effective for the problem that h cannot be taken
so small. We now try to derive CL2,H1
0 smaller than (8) in Theorem 1 with the same
criterion (7).
Note that, in order to obtain the values of M11
φ (h) and M10
φ (h), it is necessary to
solve numerically some corresponding generalized matrix eigenvalue problems. If it
succeeded in the veriﬁcation of the ﬁnite upper bound of M11
φ (h) or M10
φ (h), it means
that Gφ is nonsingular. Rump proposed an efﬁcient method for solving this eigenvalue
problem with result veriﬁcation in [12].
4
Main Theorem
We describe a main theorem of this paper as Theorem 3 in this section. Before describ-
ing it, we need to get several lemmas as below.

230
T. Kinoshita et al.
Lemma 1. Let b ∈L∞(Ω)d and c ∈L∞(Ω). Then, we obtain the following estimates:
P1
h u

H1
0 (Ω) ≤M11
φ (h)
P1
h (−△)−1
(b·∇+c)(u−P1
h u)−L u

H1
0 (Ω)
(12)
for all u ∈D(−△).
Proof. For an arbitrary u ∈D(−△), let u⊥:= u−P1
h u and f := L u = −△u+b·∇u+
cu ∈L2(Ω). Then, u satisﬁes (2). We take a test function v as v = vh ∈Sh(Ω) ⊂H1
0(Ω)
in (2), from the deﬁnition of H1
0-projection, we have
(∇u,∇vh)L2(Ω)d +(b·∇u+cu,vh)L2(Ω) = (f,vh)L2(Ω)
L(P1
h u,vh) = (−b·∇u⊥−cu⊥+ f,vh)L2(Ω) .
(13)
We set ψ := (−△)−1
−b·∇u⊥−cu⊥+ f

∈D(−△). In (13), from the deﬁnition of
H1
0-projection, we obtain
L(P1
h u,vh) = (−b·∇u⊥−cu⊥+ f,vh)L2(Ω) ,
∀vh ∈Sh(Ω),
=

−△(−△)−1
−b·∇u⊥−cu⊥+ f

,vh

L2(Ω)
= (∇ψ,∇vh)L2(Ω)d
=

∇P1
h ψ,∇vh

L2(Ω)d .
(14)
Since P1
h u and P1
h ψ are elements of Sh(Ω), they are represented as linear combinations
of the basis of Sh(Ω). Namely, there exist α = (α1,...,αn)T, γ = (γ1,...,γn)T ∈Cn
such that
P1
h u =
n
∑
i=1
αiφi,
P1
h ψ =
n
∑
i=1
γiφi.
Then, (14) is rewritten using α and γ to have
Gφα = Dφγ.
Therefore, we obtain
P1
h u
2
H1
0 (Ω) = αHDφα =

DH/2
φ
α
H 
DH/2
φ
G−1
φ D1/2
φ

DH/2
φ
γ

≤
P1
h u

H1
0 (Ω)
DH/2
φ
G−1
φ D1/2
φ

2
P1
h ψ

H1
0 (Ω) ,
which proves the lemma.
Let L∞
div(Ω)d :=

u ∈L∞(Ω)d ; divu ∈L∞(Ω)

. The right hand side of (12) can be
estimated by the following lemma.
Lemma 2. Let b ∈L∞
div(Ω)d and c ∈L∞(Ω). Then, we obtain the following estimates:
P1
h (−△)−1(b·∇+c)(u−P1
h u)

H1
0 (Ω) ≤K1(h)
u−P1
h u

H1
0 (Ω)
(15)
for all u ∈H1
0(Ω), where K1(h) := C(h)

Cs,2 ∥divb∥L∞(Ω) +C1

.

Some Remarks on the Rigorous Estimation of Inverse Linear Elliptic Operators
231
Proof. For an arbitrary u ∈H1
0(Ω), let u⊥:= u −P1
h u ∈H1
0(Ω) and ψ := (−△)−1
(b·∇+c)u⊥∈D(−△). Then, we have
P1
h ψ
2
H1
0 (Ω) =

∇ψ,∇P1
h ψ

L2(Ω)d
=

−△ψ,P1
h ψ

L2(Ω)
=

(b·∇)u⊥,P1
h ψ

L2(Ω) +

cu⊥,P1
h ψ

L2(Ω)
= −

u⊥,div(bP1
h ψ)

L2(Ω) +

u⊥,cP1
h ψ

L2(Ω)
≤
div(bP1
h ψ)

L2(Ω) +
cP1
h ψ

L2(Ω)

∥u⊥∥L2(Ω)
≤
P1
h ψdivb

L2(Ω) +
(b·∇)P1
h ψ

L2(Ω) +
cP1
h ψ

L2(Ω)

∥u⊥∥L2(Ω)
≤

∥divb∥L∞
P1
h ψ

L2 +∥b∥L∞
∇P1
h ψ

L2 +∥c∥L∞
P1
h ψ

L2

∥u⊥∥L2
≤(Cs,2 ∥divb∥L∞+∥b∥L∞+Cs,2 ∥c∥L∞)
∇P1
h ψ

L2 ∥u⊥∥L2 .
Applying (6), we obtain (15).
Even if the regularity of b is only L∞(Ω)d, there exists the following lemma by [4].
Lemma 3 ([4, Theorem 3.3]). Let b ∈L∞(Ω)d, c ∈L∞(Ω) and let Wh(Ω) be a ﬁnite
element space of H(div,Ω) := {φ ∈L2(Ω)d; divφ ∈L2(Ω)}. For an arbitrary ψh ∈Sh,
let (wh,vh) ∈Wh(Ω)×Sh(Ω) be the solution of the following problem:
(wh,w∗
h)L2(Ω)d +(∇vh,w∗
h)L2(Ω)d = (bψh,w∗
h)L2(Ω)d
∀w∗
h ∈Wh(Ω),
(wh,∇v∗
h)L2(Ω)d = 0
∀v∗
h ∈Sh(Ω),
And deﬁne σ0(h) and σ1(h) as follows
σ0(h) :=
sup
Sh∋ψh̸=0
∥wh +∇vh −bψh∥L2(Ω)d
∥∇ψh∥L2(Ω)d
,
σ1(h) :=
sup
Sh∋ψh̸=0
∥divwh∥L2(Ω)
∥∇ψh∥L2(Ω)d .
Then, we have
P1
h (−△)−1(b·∇+c)(u−P1
h u)

H1
0 (Ω) ≤K0(h)
u−P1
h u

H1
0 (Ω)
for all u ∈H1
0(Ω), where K0(h) := σ0(h)+C(h)σ1(h)+C(h)Cs,2 ∥c∥L∞(Ω).
Now, let K(h) be a positive constant deﬁned by:
K(h) :=

K1(h)
if b ∈L∞
div(Ω)d,
min

Cs,2C2, K0(h)

if b ∈L∞(Ω)d\L∞
div(Ω)d.
(16)
From Lemmas 2 and 3, (12) is estimated by
P1
h u

H1
0 (Ω) ≤M11
φ (h)
P1
h (−△)−1
(b·∇+c)(u−P1
h u)−L u

H1
0 (Ω)
≤M11
φ (h)K(h)
u−P1
h u

H1
0 (Ω) +M11
φ (h)
P1
h (−△)−1L u

H1
0 (Ω)
≤M11
φ (h)K(h)
u−P1
h u

H1
0 (Ω) +M11
φ (h)Cs,2 ∥L u∥L2(Ω)
(17)

232
T. Kinoshita et al.
Lemma 4. Let b ∈L∞(Ω)d and c ∈L∞(Ω). Then, we obtain the following estimates:
u−P1
h u

H1
0 (Ω) ≤C(h)

C1
P1
h u

H1
0 (Ω) +C2
u−P1
h u

H1
0 (Ω) +∥L u∥L2(Ω)

(18)
for all u ∈D(−△).
Proof. For an arbitrary u ∈D(−△), let u⊥:= u −P1
h u. From the Poincar´e inequality
and (6), we have
∥△u∥L2 = ∥L u−b·∇u−cu∥L2(Ω)
≤∥L u∥L2(Ω) +∥b∥L∞(Ω)d ∥∇u∥L2(Ω)d +∥c∥L∞(Ω) ∥u∥L2(Ω)
≤∥L u∥L2 +∥b∥L∞
P1
h u

H1
0 +∥u⊥∥H1
0

+∥c∥L∞
P1
h u

L2 +∥u⊥∥L2

≤∥L u∥L2 +

∥b∥L∞+Cs,2 ∥c∥L∞
P1
h u

H1
0 +

∥b∥L∞+C(h)∥c∥L∞

∥u⊥∥H1
0 .
Therefore, from (5), we obtain
∥∇u⊥∥L2(Ω)d ≤C(h)∥△u∥L2(Ω)
≤C(h)

C1
P1
h u

H1
0 (Ω) +C2 ∥u⊥∥H1
0 (Ω) +∥L u∥L2(Ω)

.
By the effective use of the above lemmas, we propose the following estimates based
on the Fredholm theory.
Theorem 3. Let K(h) > 0 be deﬁned by (16). And let κφ > 0 be a constant satisfying
κφ := C(h)

C1M11
φ (h)K(h)+C2) < 1.
(19)
Then, there exists L −1 ∈L

L2(Ω),D(−△)

and CL2,H1
0 in (3) can be taken as
CL2,H1
0 =

M11
φ (h)2

Cs,2 +C(h)

K(h)−Cs,2C2
2
+C(h)2

1+Cs,2M11
φ (h)C1
2
1−κφ
.
(20)
Proof. For an arbitrary u ∈D(−△), we set u⊥:= u −P1
h u ∈H1
0(Ω). From (17) and
(18), we obtain

1
−K(h)M11
φ (h)
−C(h)C1
1−C(h)C2
P1
h u

H1
0 (Ω)
∥u⊥∥H1
0 (Ω)

≤
Cs,2M11
φ (h)
C(h)

∥L u∥L2(Ω)
where the inequality is meant componentwise. From the assumption (19),
det

1
−K(h)M11
φ (h)
−C(h)C1
1−C(h)C2

= 1−κφ > 0
is satisﬁed. Therefore, the solution of this simultaneous inequalities can be written as
P1
h u

H1
0 (Ω)
∥u⊥∥H1
0 (Ω)

≤
1
1−κφ
1−C(h)C2 K(h)M11
φ (h)
C(h)C1
1
Cs,2M11
φ (h)
C(h)

∥L u∥L2(Ω)
=
1
1−κφ
Cs,2M11
φ (h)+C(h)M11
φ (h)(K(h)−Cs,2C2)
C(h)

1+Cs,2M11
φ (h)C1


∥L u∥L2(Ω) .

Some Remarks on the Rigorous Estimation of Inverse Linear Elliptic Operators
233
Then, we have
∥u∥2
H1
0 (Ω) =
P1
h u
2
H1
0 (Ω) +∥u⊥∥2
H1
0 (Ω)
≤

Cs,2M11
φ (h)+C(h)M11
φ (h)(K(h)−Cs,2C2)
1−κφ
2
∥L u∥2
L2(Ω)
+
⎛
⎝
C(h)

1+Cs,2M11
φ (h)C1

1−κφ
⎞
⎠
2
∥L u∥2
L2(Ω) .
Finally, the invertibility of L is followed by the same arguments in [5, Theorem4.3].
Remark 1. If b ∈W 1,∞(Ω)d, the criterion (19) is equal to (7) because K(h) = ˜K(h).
Therefore, the attainability of criteria (19) and (7) are essentially same. On the other
hand, even if the convergence order K(h) = O(1), namely, independent of smoothness
of the function b, the constant CL2,H1
0 of (20) converges to Cs,2M11
φ (0) as h →0. Com-
paring this result with (9), we can say that (20) is better than (8) in the asymptotic sense
as h →0.
5
Numerical Results
In this section, we show some veriﬁed computation results of constants CL2,H1
0 by (8),
(11), and (20). Let L = −△+b·∇+c : D(−△) →L2(Ω) be a non-self-adjoint oper-
ator with b := R

−x2 +1/2
x1 −1/2

, R ∈R, and c ∈C on Ω := (0,1) × (0,1) ⊂R2. We
adopted P1 ﬁnite element space with uniform triangular meshes as Sh(Ω). Then, dis-
cretization parameter h > 0 is the element side length. In this case, Assumption 1 holds
with C(h) = 0.493h([2]) and Cs,2 =
1
π
√
2. Note that, of course our arguments above can
also be applied for not only P1 element but also any ﬁnite element spaces. We use the
interval arithmetic toolbox INTLAB [11] Version 7 with MATLAB 8.0.0.783 (R2012b)
on Intel Core i7 3.4 GHz with Mac OSX 10.8.3.
Table 1. R = 10,
c = 15
Theorem 1
Theorem 2
Theorem 3
1/h M11
φ (h) M10
φ (h)
˜κφ
CL2,H1
0
ˆκφ
CL2,H1
0
κφ
CL2,H1
0
5
0.9732
0.1270
1.8758 ——
1.9610 ——
1.8758 ——
8
0.9903
0.1276
0.9032 3.3368 1.1493 ——
0.9032 2.6387
10
0.9939
0.1277
0.6488 0.8671 0.8987 1.6951 0.6488 0.6589
20
0.9986
0.1279
0.2497 0.3543 0.4284 0.2453 0.2497 0.2760
50
0.9999
0.1279
0.0818 0.2632 0.1663 0.1559 0.0818 0.2316
100 1.0001
0.1279
0.0379 0.2426 0.0823 0.1400 0.0379 0.2267

234
T. Kinoshita et al.
In Table 1, the short line segment means that the corresponding criteria (7), (10),
or (19) were not satisﬁed, which also implies we failed to compute the rigorous upper
bounds CL2,H1
0 . From these results, we can say that, for sufﬁciently small h, the estimates
(11) should be ﬁnest. On the other hand, if h is not so small, then our proposed estimates
(20) is better than others. Therefore, we conclude that three kinds of methods would
have their own ranges of suitable applicability depending on each problem.
6
Conclusion
We presented an alternative approach to the numerical veriﬁcation method for linear
ellitipc problems based on Theorem 3. It is proved that our new method gives a better
results from the viewpoint in computational costs. As the future subjects, we will show
that the present method can also be applied to fourth order elliptic problems or more
general linear elliptic operators.
Acknowledgments. The authors are very grateful to two anonymous reviewers. This work was
supported by the Grant-in-Aid from the Ministry of Education, Culture, Sports, Science and Tech-
nology of Japan (No. 23740074, No. 24340018, and No. 24540151) and supported by Program
for Leading Graduate Schools “Training Program of Leaders for Integrated Medical System for
Fruitful Healthy-Longevity Society.”
References
1. Grisvard, P.: Singularities in Boundary Value Problems. Springer, New York (1992)
2. Kikuchi, F., Liu, X.: Determination of the Babuska-Aziz constant for the linear triangular
ﬁnite element. Jpn. J. Ind. Appl. Math. 23(1), 75–82 (2006)
3. Kimura, S., Yamamoto, N.: On the L2 a priori error estimates to the ﬁnite element solution
of elliptic problems with singular adjoint operator. Bull. Inform. Cybern. 31(2), 109–115
(1999)
4. Kinoshita, T., Hashimoto, K., Nakao, M.T.: The L2 a priori error estimates for singular
adjoint operator. Numer. Func. Anal. Optim. 30(3–4), 289–305 (2009)
5. Kinoshita, T., Watanabe, Y., Nakao, M.T.: An improvement of the theorem of a posteriori
estimates for inverse elliptic operators. NOLTA 5(1), 47–52 (2014)
6. Nakao, M.T., Yamamoto, N., Kimura, S.: On the best constant in the error bound for the
H1
0 -projection into piecewise polynomial spaces. J. Approx. Theory 93, 491–500 (1998)
7. Nakao, M.T., Hashimoto, K., Watanabe, Y.: A numerical method to verify the invertibility of
linear elliptic operators with applications to nonlinear problems. Computing 75, 1–14 (2005)
8. Nakao, M.T., Watanabe, Y., Kinoshita, T., Kimura, T., Yamamoto, N.: Some considerations
of the invertibility veriﬁcations for linear elliptic operators. Jpn. J. Ind. Appl. Math. 32(1),
19–31 (2015)
9. Oishi, S.: Numerical veriﬁcation of existence and inclusion of solutions for nonlinear opera-
tor equations. J. Comput. Appl. Math. 60(1–2), 171–185 (1995)
10. Plum, M.: Computer-assisted proofs for semilinear elliptic boundary value problems. Jpn. J.
Ind. Appl. Math. 26(2–3), 419–442 (2009)
11. Rump, S.M.: INTLAB - INTerval LABoratory. In: Csendes, T. (ed.) Developments
in Reliable Computing, pp. 77–104. Kluwer Academic Publishers, Dordrecht (1999).
http://www.ti3.tu-harburg.de/rump/

Some Remarks on the Rigorous Estimation of Inverse Linear Elliptic Operators
235
12. Rump, S.M.: Veriﬁed bounds for singular values, in particular for the spectral norm of a
matrix and its inverse. BIT Numer. Math. 51(2), 367–384 (2011)
13. Watanabe, Y., Kinoshita, T., Nakao, M.T.: A posteriori estimates of inverse operators for
boundary value problems in linear elliptic partial differential equations. Math. Comput. 82,
1543–1557 (2013)

Veriﬁed Parameter Identiﬁcation for Dynamic
Systems with Non-Smooth Right-Hand Sides
Andreas Rauh(B), Luise Senkel, and Harald Aschemann
Chair of Mechatronics, University of Rostock,
Justus-von-Liebig-Weg 6, D-18059 Rostock, Germany
{andreas.rauh,luise.senkel,harald.aschemann}@uni-rostock.de
Abstract. Modeling of systems in engineering involves two major
stages. First, a system structure is derived that is based on the fun-
damental laws from physics that characterize the relevant processes.
Second, speciﬁc parameter values are determined by minimizing the dis-
tance between the measured and simulated system outputs. In previous
work, strategies for veriﬁed parameter identiﬁcation using techniques
from interval analysis were developed. These techniques are extended
in this paper to a veriﬁed estimation for systems with non-smooth ordi-
nary diﬀerential equations. Suitable experimental results for parameter
estimation of a mechanical system with friction conclude this contribu-
tion to highlight the practical applicability of the developed identiﬁcation
procedure.
Keywords: Non-smooth ordinary diﬀerential equations · Veriﬁed para-
meter identiﬁcation · Interval analysis · Mechanical systems · Friction
1
Introduction
Dynamic system models given by ordinary diﬀerential equations (ODEs) with
non-smooth right-hand sides are widely used in engineering. They can, for exam-
ple, be employed to describe transitions between static and sliding friction in
mechanical systems and to represent variable degrees of freedom for dynamic
applications in robotics with contacts between at least two (rigid) bodies.
The veriﬁed simulation of such systems has to detect those points of time at
which either one of the discrete model states (in a representation of the ODEs
by means of a state transition diagram) becomes active or at which one of the
discrete model states is deactivated [1,7,8,14]. As long as mechanical systems
are taken into consideration that are described by position and velocity as corre-
sponding state variables, it is guaranteed that the trajectories (i.e., the solutions
of the ODE) remain continuous if switchings between diﬀerent submodels occur.
For practical applications, however, it is on the one hand necessary to derive
veriﬁed simulation techniques and to compute state variables that can be reached
within a given time horizon under consideration of a predeﬁned control law. Such
a control law is usually given by the actuator signal (e.g. force or torque) acting
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 236–246, 2016.
DOI: 10.1007/978-3-319-31769-4 19

Veriﬁed Parameter Identiﬁcation for Dynamic Systems
237
onto the (mechanical) system [10,13]. On the other hand, a system identiﬁ-
cation is necessary to determine parameter values that comply with both the
non-smooth system model and the measured data. In engineering applications,
these measurements are usually subject to uncertainty that is often in the same
order of magnitude as the measured data themselves. For large uncertainty, it
is in general not reliable to determine point values for the system parameters.
In [2], for example, it has been shown that the naive application of least squares
techniques for the minimization of the distance between point-valued measured
and simulated system outputs (computed in pure ﬂoating point arithmetic) may
lead to results that do not comply with a veriﬁed set-valued enclosure. Such set-
valued enclosures represent those parameter ranges that are at the same time
compatible with the system model and bounded measurement uncertainty.
For this reason, two options for the veriﬁed parameter identiﬁcation are dis-
cussed in this paper with respect to their applicability to systems with state-
dependent transitions between diﬀerent piecewise smooth ODE representations.
The identiﬁcation makes use of a veriﬁed simulation of ODEs with non-
smooth right-hand sides. This routine employs a generalized Taylor series-based
integration to determine guaranteed state enclosures that are reachable over
some time span. As shown by the parameter estimation of a test rig for the
longitudinal dynamics of a vehicle, the minimum series expansion order leads
already to state enclosures that are suitable for a veriﬁed identiﬁcation. For
this test rig, parameters related to the mass moment of inertia as well as the
static and sliding friction coeﬃcients are estimated. Besides veriﬁed integration
of non-smooth ODEs, the reliable identiﬁcation exploits an interval subdivision
procedure.
This paper is structured as follows. Section 2 gives an overview of the class of
systems for which parameters are estimated in this paper. In Sect. 3, a brief
review of the veriﬁed interval-based simulation routine for ODEs with non-
smooth right-hand sides is given. Section 4 describes diﬀerent options for the
implementation of veriﬁed identiﬁcation procedures. A summary of identiﬁca-
tion results for a laboratory test rig at the Chair of Mechatronics, University of
Rostock, is given in Sect. 5. Conclusions and an outlook on future work can be
found in Sect. 6.
2
Dynamic Systems with Non-Smooth Right-Hand Sides
In this paper, parameter identiﬁcation strategies are considered for (open-loop)
dynamical systems with l diﬀerent continuous-time models S = {S1, S2, . . . , Sl},
which are each given by the state-space representations
˙x (t) = fSi (x (t) , p, u (t) , t)
for
i ∈{1, . . . , l} .
(1)
In (1), the vector x ∈Rn denotes the state vector and p ∈Rnp the vector of
uncertain parameters that are identiﬁed by the subsequent procedure. Moreover,
u ∈Rnu is the vector of control variables. In the case of open-loop systems, this
vector is assumed to be piecewise constant for a time interval t ∈

tk ; tk+1

.

238
A. Rauh et al.
Because closed-loop feedback control procedures are assumed to be implemented
in a discrete-time manner throughout this paper, the input u (t) is also piecewise
constant in this case. The input is then given as a state-dependent function that
is evaluated at each point of time t = tk for the current state vector x(tk).
The explicit time dependency of (1) is then used to describe the inﬂuence of a
predeﬁned reference trajectory xd(t) on the dynamic system.
For a complete speciﬁcation of the system behavior, conditions T j
i (x, u) for
the transition from the model state Si to Sj, i, j ∈{1, . . . , l}, have to be given
additionally. These conditions are speciﬁed in the following by means of a state
transition diagram, where all discrete model states Si are assumed to be mutually
exclusive in a real-life experiment. This also holds for simulations in the case of
exactly known parameters and system states as long as no time discretization
errors inﬂuence the system dynamics. However, multiple states Si can be active
simultaneously during simulations in the uncertain case, including the eﬀect of
time discretization. Then, a deﬁnite distinction between two diﬀerent models Si
and Sj is no longer possible due to the before-mentioned uncertainties.
Note that the case i = j refers to the operating conditions for which the
current state Si remains active. However, a veriﬁed simulation does not only have
to account for scenarios in which the transition between two diﬀerent states Si
and Sj occurs exactly at a sampling point tk. The simulation also has to detect
transitions that take place between two subsequent points tk and tk+1.
As a representative benchmark application, the drive train test rig depicted in
Fig. 1 is considered. It represents a simpliﬁed model for the longitudinal dynam-
ics of a vehicle. After introducing the benchmark application, a suitable veriﬁed
simulation technique is brieﬂy reviewed. It is the basis for the subsequent para-
meter estimation, where measurements of y(tk) = g (x(tk), p) are assumed to
be available at discrete points of time t = tk.
To describe the system dynamics in a reliable way, three diﬀerent operating
conditions are distinguished. The models S1 and S3 represent sliding friction
for the motion in backward and forward direction, respectively. Obviously, these
models are valid for non-zero motor angular velocities ˙ϕM = ωM = x2(t) ̸= 0.
Additionally, the breakaway point is included as an activation condition in S1
and S3. This is the operating condition in which the actuator torque overcomes
the static friction TF,s. The corresponding state equations are given by
˙x(t) = fI (x(t), p, u(t), t) =

x2(t)
α · x2(t) + β · (u(t) −TF(t))

for
I ∈{S1, S3},
(2)
with the friction term TF(t) = TF,s·sign(x2(t)), the parameters p =

α β TF,s
T ,
the state vector x(t) =
x1(t) x2(t)T =
ϕM(t) ωM(t)T , and the motor torque
as the piecewise constant control signal u(t) = TM(t), nu = 1. For the static
friction case, the angular velocity x2(t) becomes zero and the additional condition
|u(t)| ≤TF,s holds with the state equations ˙x(t) = fS2 (x(t), p, u(t), t) =
0 0T .
In detail, the parameter α represents the ratio between velocity-proportional
friction and the overall mass moment of inertia; β is the reciprocal of the mass

Veriﬁed Parameter Identiﬁcation for Dynamic Systems
239
Fig. 1. Benchmark application: Test rig for the longitudinal dynamics of a vehicle.
moment of inertia; TF,s is the static friction coeﬃcient, which may vary after each
standstill of the test rig. A state transition diagram for the uncertain dynamic
system is shown in Fig. 2. It contains the nominal system model if the parameter
intervals [α], [β], and [TF,s] as well as the control signal u(t) are replaced by point
values. Further generalizations of the modeling approach are described in [1].
sliding friction
motion”
“forward

inf
[u]
 ≤−inf
[TF,s]
&

inf
[x2]
 < 0
 			
inf
[x2]
 = 0


sup [u]
 ≥inf
[TF,s]
&
inf
[x2]
 = 0


sup [x2]
 > 0
 			
sliding friction
motion”
“backward
no motion
static friction
[u] ∩

T max
F,s

̸= ∅

&
[x2] ∩0 ̸= ∅

[x2] ∩0 ̸= ∅

[u] ∩

T max
F,s

̸= ∅

&
˙x2 (t) = 0
˙x1 (t) = 0
Model S2
˙x2 (t) = αx2(r) + β˜u(t)
˙x1 (t) = x2 (t)
Model S3
˙x2 (t) = αx2(t) + β˜u(t)
˙x1 (t) = x2 (t)
Model S1
u ≤−TF,s
& x2 = 0

x2 < 0
 |
x2 = 0
x2 > 0
 |
u ≥TF,s
& x2 = 0

sup [u]
 ≥inf
[TF,s]
& [x2] ∩0 ̸= ∅

inf
[u]
 ≤−inf
[TF,s]

sup [u]
 ≥inf
[TF,s]

inf
[u]
 ≤−inf
[TF,s]
& [x2] ∩0 ̸= ∅

[u] ∩

T max
F,s

̸= ∅
Fig. 2. State transition diagram of the benchmark application with the interval para-
meters α ∈[α] = [α ; α], β ∈[β] =

β ; β

, TF,s ∈[TF,s] =

T F,s ; T F,s

, and the
system input ˜u(t) := u(t) −TF(t) with [T max
F
] :=

−T F,s ; T F,s

.
3
Veriﬁed Simulation of ODEs with Non-Smooth
Right-Hand Sides
The prerequisite for the following veriﬁed parameter identiﬁcation scheme is
the computation of guaranteed state enclosures for the uncertain system models
given in the previous section. For that purpose, a generalization of a Taylor series-
based enclosure technique is employed. Details about this simulation approach
are published in [1,10,13]. Hence, only a short overview is given in this section.

240
A. Rauh et al.
The basic assumption of Taylor series-based simulation procedures is the
discretization of the considered time horizon. For the system in Fig. 1, in which
measured data and control variables are available at equidistant points of time
tk, it is assumed that the time discretization mesh is represented by an integer
divisor of the control sampling period tk+1 −tk, i.e., h = tk+1−tk
N
, N ∈N.
Then, a Taylor series expansion of the solution of the initial value problem
for a continuous-time system model with respect to time — given by the ODEs
˙x(tk) = f (x (tk) , p, u (tk) , tk) with the initial state x(0) — leads to
x (tk + h) = x (tk)+
ν

i=1
hi
i! f (i−1) (x (tk) , p, u (tk) , tk)+e (x (ξ) , p, u (ξ) , ξ) (3)
with the before-mentioned integration step-size h. Since the uncertain parame-
ters p ∈[p] are assumed to be constant and since changes of control signals u(tk)
only occur at the discrete points of time t = tk, the expression (3) is evaluated
recursively until the point of time t = tk+1 is reached. Here, a new control signal
u(tk+1) becomes active and new measured data are available.
Moreover, Eq. (3) involves the computation of the total derivatives f (i−1)
(resp. Taylor series coeﬃcients) in terms of the smooth right-hand side of the
ODE with ˙p = 0 and ˙u(t) = 0, t ∈(tk ; tk + h) as well as x(tk) ∈[x] (tk)
and p ∈[p]. The iterative computation of state enclosures is completed by the
calculation of guaranteed bounds for the discretization error
e (x (ξ) , p, u (ξ) , ξ) ⊆[ek] :=
hν+1
(ν + 1)!f (ν) ([Bx,k] , [p] , u ([τk]) , [τk])
(4)
with ξ ∈[tk ; tk + h]. Note that the prerequisite for the applicability of this
fundamental computation scheme (included e.g. also in VNODE and AWA
[5,6]) is that the right-hand side of the ODE belongs to the set f ∈Cν of at least
ν times continuously diﬀerentiable functions. In addition, it is necessary that a
bounding box [Bx,k] (representing all reachable states over the discretization
period) as well as guaranteed parameter and control enclosures [p] and u ([τk])
are available for the time interval [τk] := [tk ; tk + h]. To extend the use of (3),
(4) to ODE systems with non-smooth right-hand sides, the following extensions
are necessary for the iteration scheme as well as for the Picard iteration that is
employed to determine the bounding box [Bx,k] (for details, cf. [1, Sec. 3.3]).
Step S1. Calculation of a bounding box [Bx,k] = [Ba,k] for the time interval
[τk], where f (·) = fa (·) is a continuously diﬀerentiable function describing
the union of all system models from the set S which are active at t = tk.
Step S2. Check, whether additional models from the set S are activated within
the interval [τk]: If additional models are activated, repeat Step S1 after
modifying the continuously diﬀerentiable enclosure fa by consideration of all
additionally activated models; otherwise, continue with Step S3.

Veriﬁed Parameter Identiﬁcation for Dynamic Systems
241
Step S3. Interval evaluation of the series expansion for f (·) = fa (·) according
to (3), (4). Note that generally ν > 1 can be chosen. However, if measured
data are available after a few discretization steps N (as in the case of the
veriﬁed parameter identiﬁcation that is considered in this paper), it is often
suﬃcient to restrict the series expansion order to ν ≡1.
Step S4. Deactivation of system models which can no longer be active at t =
tk+h and continue with Step S1 for the next time interval [tk + h ; tk + 2h].
4
Veriﬁed Parameter Identiﬁcation
For veriﬁed parameter identiﬁcation, two fundamentally diﬀerent approaches
exist. The ﬁrst one is based on subdividing an initial parameter domain into
subintervals, afterwards performing a veriﬁed integration of the ODEs for these
subintervals, and subsequently checking the resulting enclosures for admissi-
bility. A parameter box is treated as consistent with the measured data if
the simulated state enclosures are subsets of intervals for the measured data
[ym,q] (tk) = ym,q(tk) + [Δym,q] (tk) for each sampling time tk and each sensor q,
where [Δym,q] (tk) is the measurement tolerance. All parameter intervals which
lead to an enclosure that does not overlap with [ym,q] (tk) for at least one q
and k are inconsistent. All remaining interval boxes can be divided further and
investigated for consistency [9]. However, this procedure is disadvantageous if
parameters are varying over time (depending on the states x). This is the case
for the application scenario considered in this paper. It is characterized by the
fact that the static friction coeﬃcient changes its value after each standstill of
the drive and therefore has to be re-identiﬁed within the initial bounds for [TF,s].
In such cases, the second option for veriﬁed parameter identiﬁcation is rea-
sonable. It has the same structure as the well-known Kalman ﬁlter (or Luen-
berger observer) [4] for dynamic systems with stochastic disturbances, namely
(i) a prediction phase in which the (nonlinear) system model is evaluated
between two subsequent measurement points tk−1 and tk and (ii) a correction
step in which an intersection between the predicted state intervals and their
bounds — resulting from the sensor information — is performed, see Fig. 3.
In the correction step, parameter intervals which lead to an empty intersection
of both before-mentioned estimates are guaranteed to be inconsistent and can,
hence, be eliminated. All undecided interval boxes are evaluated by the following
algorithm. For the sake of simplicity, it is assumed that the sensors q provide
a direct measurement of selected state variables. If this was not the case, i.e.,
if ym,q(tk) is a (generally nonlinear) function of (multiple) state variables, tech-
niques for constraint propagation or veriﬁed Newton methods become necessary
in the correction step [3]. Note that resetting parameter intervals to their ini-
tial domains is an easy task for this second type of identiﬁcation procedure. In
the following, the proposed parameter identiﬁcation procedure is described in
detail.

242
A. Rauh et al.
with uncertainties
system model
nonlinear dynamic
Evaluation of
Step S1–S4
Step I2
Step I1
Interval subdivision
Intersection of
both veriﬁed
state enclosures
Step I3
Consistency test
and reduction of
interval number
Step I4–I7
unit delay
ym,ny (tk)
Fusion of
measurement
information
Sensor 1
Sensor 2
Sensor ny
Nonlinear measurement
model with uncertainties
ym,1 (tk)
ym,2 (tk)
Estimate in the
correction step
State and parameter estimate in the prediction step
Improved estimate after transition from tk−1 to tk
ˆx (tk−1)
Fig. 3. Block diagram of the veriﬁed parameter identiﬁcation procedure.
Veriﬁed Parameter Identiﬁcation: Prediction-Correction-Framework
Step I1. Description of the state enclosure by a list of L interval boxes, where z
is a vector containing time-varying state variables and constant parameters

z⟨l⟩
(tk) :=


x⟨l⟩
(tk)
T 
p⟨l⟩
(tk)
T T
,
l ∈{1, . . . , L};
(5)
Perform M interval subdivisions, if at least one interval l is characterized by
n+np

j=1
diam

z⟨l⟩
j

(tk)

̸= 0,
(6)
leading to a new interval list of length L + M −1. Here, the candidates to
be subdivided are determined as the boxes with the largest pseudo-volume
l∗= arg max
l=1,...,L′
n+np

j=1
diam

z⟨l⟩
j

(tk)

,
L′ ≥L.
(7)
Unnecessarily conservative interval bounds in the prediction step have to
be avoided to reduce ambiguities between static and sliding friction in the
computation of the state enclosures (multiple model states S may be active).
Therefore, the following heuristic, application-dependent scheme for detect-
ing the vector component of

z⟨l∗⟩
to be subdivided is used in the remainder
of this paper:
(a) Split the static friction interval

T ⟨l∗⟩
F,s

according to the following proce-
dure if the condition [u] (tk) ∩hull

−

T ⟨l∗⟩
F,s

,

T ⟨l∗⟩
F,s

̸= ∅holds
– Select the splitting point u(tk) + ϵ, ϵ > 0 for [u] (tk) > 0 with
T ⟨l∗⟩
F,s < u(tk) and T
⟨l∗⟩
F,s > u(tk)
– Select the splitting point u(tk) −ϵ, ϵ > 0 for [u] (tk) < 0 with
−T
⟨l∗⟩
F,s < u(tk) and −T ⟨l∗⟩
F,s > u(tk)
– Else: Splitting of

T ⟨l∗⟩
F,s

at its midpoint

Veriﬁed Parameter Identiﬁcation for Dynamic Systems
243
(b) Split the angular velocity interval1 
x⟨l∗⟩
2

if it is the major source for
large interval diameters, i.e., if diam

x⟨l∗⟩
2

≥diam

β⟨l∗⟩
holds
(c) Split the interval

β⟨l∗⟩
(typically at its midpoint) if

α⟨l∗⟩
·

x⟨l∗⟩
2

∩

β⟨l∗⟩
·

[u] (tk) −

T ⟨l∗⟩
F,s

̸= ∅
(8)
(d) Else: Split the interval

α⟨l∗⟩
(typically at its midpoint)
Step I2. Veriﬁed integration of the IVP2 until the next measurement point tk+1
to compute the enclosures

z⟨l⟩
(tk+1)
Step I3. Intersection of all interval boxes with the measured data z1(tk+1) ∈
[ym] (tk+1) (assuming that only a direct scalar measurement exists for the
ﬁrst state variable, i.e., the angle measurement ϕM,m in Fig. 1)

˜z⟨l⟩
1

(tk+1) :=

z⟨l⟩
1

(tk+1) ∩[ym] (tk+1)
(9)
Step I4. Replace

z⟨l⟩
1

(tk+1) by

˜z⟨l⟩
1

(tk+1) for all l ∈{1, . . . , L + M −1}
Step I5. Delete all subintervals with

˜z⟨l⟩
1

(tk+1) = ∅in (9) from the list
Step I6. Replace static friction intervals with the initial range

T ini
F,s

if standstill
is detected for a minimum time span (detected by a binary signal from the
velocity sensor ωM,m):
(a) For each list entry l ∈{1, . . . , L′}, deﬁne new static friction subintervals

T ⟨l⟩
a

:=

T ini
F,s ; T ⟨l⟩
F,s

,

T ⟨l⟩
b

:=

T ⟨l⟩
F,s ; T
⟨l⟩
F,s

,

T ⟨l⟩
c

:=

T
⟨l⟩
F,s ; T
ini
F,s

(b) Create a list of up to 3L′ subintervals3, where

T ⟨l⟩
F,s

is replaced by each
of the intervals

T ⟨l⟩
a

,

T ⟨l⟩
b

,

T ⟨l⟩
c

with non-zero diameter
(c) Subsequent merging of intervals avoids the combination of intervals with
diﬀerent active model states Si, i ∈{1, 2, 3}
Step I7. Reduce the number of subintervals4 by a convex hull with suﬃciently
small overestimation [11]: New list length L := L∗(for further extensions,
see [12])
1 Optional: Trisectioning of

x⟨l∗⟩
2

around the value zero if static and sliding friction
are possible simultaneously in the simulation of the uncertain system model.
2 The presented integration procedure is implemented by using the toolbox IntLab,
where a parallelization of the evaluation can be achieved in a straightforward manner
if the state equations are evaluated after a distribution onto multiple CPU cores.
The Parallel Computing Toolbox can be utilized for this purpose in Matlab.
3 The increase of the list length from L′ to 3L′ has the advantage that information
about the parameter splitting before the reset is not lost. Usually, the static friction is
similar after standstill, even if it does not remain identical. In this case, the splitting
information speeds up the identiﬁcation and elimination of inconsistent subdomains.
4 Note that the interval replacement (Step I6) and the reduction of the interval
number (Step I7) can be employed interchangeably.

244
A. Rauh et al.
5
Identiﬁcation Results
In this section, the veriﬁed identiﬁcation procedure is applied to determine
enclosures for the parameters α, β, and TF,s of the drive train test rig in
Fig. 1. The initial domains are: [α] = −[1 ; 6] 1
s , [β] = [10 ; 400]
rad
Nm·s2 , and
[TF,s] = [0.01 ; 0.30] Nm; measured data are available with a discretization
period of 10 ms. In accordance with the available motor angle sensor, the tol-
erance bounds are [Δym] = [−0.1 ; 0.1] rad. Figure 4a gives an overview of the
control signal, and the angle measurement for the identiﬁcation time span of
tf = 80 s.
Now, the identiﬁcation procedure is run until the ﬁnal point of time tf, while
the interval for the static friction coeﬃcient is reset to its initial bound after
each standstill. It can be seen that especially the interval for the parameter β is
signiﬁcantly reduced in this ﬁrst run of the identiﬁcation. Because the parameters
α and β remain uncertain but constant, the identiﬁcation is repeated ﬁve times,
where the reinitialization values at t = 0 s correspond to the ﬁnal parameter
intervals [α], [β] at the end of the previous run (Fig. 4b). Subdomains of α and β
are classiﬁed as inconsistent with the measured data during the ﬁrst four runs,
while the results remain constant in run ﬁve. The corresponding results can, for
example, be used for the speciﬁcation of intervals in which control and online
state estimation procedures have to be robust and asymptotically stable.
u(t) in Nm
0
t in s
20
40
60
80
0.8
0.4
0
−0.4
−0.8
x1(t) in rad
0
t in s
20
40
60
80
160
120
80
40
0
interval bounds
measured data
(a) Control signal u(t) and
motor angle x1(t) = ϕM(t).
t in s
40
80
t in s
40
80
t in s
40
80
t in s
40
80
t in s
40
80
[α](t)
[β](t)
[TF,s](t)
0
0
0.3
reset at
standstill
run 1
run 2
run 3
run 4
run 5
0.2
0.1
0
−3
−6
0
400
200
(b) Intervals for the estimated parameters [p] (t).
Fig. 4. Identiﬁcation results (M = 50, ﬁve repetitions of the procedure).
6
Conclusions and Outlook on Future Work
In this paper, veriﬁed integration of ODEs with non-smooth right-hand sides
was combined in a novel way with an oﬄine-applicable reliable parameter iden-
tiﬁcation procedure. Real-life results were presented for a laboratory test rig

Veriﬁed Parameter Identiﬁcation for Dynamic Systems
245
at the Chair of Mechatronics, University of Rostock. Future work will deal with
an extension of the identiﬁcation procedure to systems with larger initial search
domains for a-priori unknown parameters. Moreover, more complex friction and
hysteresis models will be identiﬁed. For this purpose, it will be necessary to
investigate how the interval subdivision routine — in combination with the
parallelized integration of ODEs — scales to higher-dimensional sets of state
equations. Finally, the computed intervals will be employed to initialize online-
applicable estimation procedures which are based on sliding mode principles.
References
1. Auer, E., Kiel, S., Rauh, A.: A veriﬁed method for solving piecewise smooth initial
value problems. Int. J. Appl. Math. Comput. Sci. AMCS 23(4), 731–747 (2013)
2. Hofer, E.P., Rauh, A.: Applications of interval algorithms in engineering. In: CD-
Proceedings of 12th GAMM-IMACS International Symposium on Scientiﬁc Com-
puting, Computer Arithmetic, and Validated Numerics SCAN 2006. IEEE Com-
puter Society, Duisburg, Germany (2007)
3. Jaulin, L., Kieﬀer, M., Didrit, O., Walter, ´E.: Applied Interval Analysis. Springer,
London (2001)
4. Kalman, R.E.: A new approach to linear ﬁltering and prediction problems. Trans.
ASME - J. Basic Eng. 82(Series D), 35–45 (1960)
5. Lohner, R.: Enclosing the solutions of ordinary initial and boundary value prob-
lems. In: Kaucher, E.W., Kulisch, U.W., Ullrich, C. (eds.) Computer Arithmetic:
Scientiﬁc Computation and Programming Languages, pp. 255–286. Wiley-Teubner
Series in Computer Science, Stuttgart (1987)
6. Nedialkov, N.S.: Interval tools for ODEs and DAEs. In: CD-Proceedings of 12th
GAMM-IMACS International Symposium on Scientiﬁc Computing, Computer
Arithmetic, and Validated Numerics SCAN 2006. IEEE Computer Society, Duis-
burg, Germany (2007)
7. Nedialkov, N.S., von Mohrenschildt, M.: Rigorous simulation of hybrid dynamic
systems with symbolic and interval methods. In: Proceedings of the American
Control Conference ACC, Anchorage, USA, pp. 140–147 (2002)
8. Ramdani, N., Nedialkov, N.S.: Computing reachable sets for uncertain nonlinear
hybrid systems using interval constraint-propagation techniques. Nonlinear Anal.
Hybrid Syst. 5(2), 149–162 (2011)
9. Rauh, A., D¨otschel, T., Auer, E., Aschemann, H.: Interval methods for control-
oriented modeling of the thermal behavior of high-temperature fuel cell stacks.
In: Proceedings of 16th IFAC Symposium on System Identiﬁcation SysID 2012.
Brussels, Belgium (2012)
10. Rauh, A., Kletting, M., Aschemann, H., Hofer, E.P.: Interval methods for sim-
ulation of dynamical systems with state-dependent switching characteristics. In:
Proceedings of the IEEE International Conference on Control Applications CCA
2006, pp. 2243–2248. Munich, Germany (2006)
11. Rauh, A., Kletting, M., Aschemann, H., Hofer, E.P.: Reduction of overestimation
in interval arithmetic simulation of biological wastewater treatment processes. J.
Comput. Appl. Math. 199(2), 207–212 (2007)
12. Rauh, A., Senkel, L., Aschemann, H.: Experimental comparison of interval-based
parameter identiﬁcation procedures for uncertain odes with non-smooth right-hand
sides. In: CD-Proceedings of IEEE International Conference on Methods and Mod-
els in Automation and Robotics MMAR 2015. Miedzyzdroje, Poland (2015)

246
A. Rauh et al.
13. Rauh, A., Siebert, C., Aschemann, H.: Veriﬁed simulation and optimization of
dynamic systems with friction and hysteresis. In: Proceedings of ENOC 2011.
Rome, Italy (2011)
14. Rihm, R.: Enclosing solutions with switching points in ordinary diﬀerential equa-
tions. In: Computer Arithmetic and Enclosure Methods. Proceedings of SCAN 91.
North-Holland, Amsterdam, pp. 419–425 (1992)

Exponential Enclosure Techniques for Initial
Value Problems with Multiple Conjugate
Complex Eigenvalues
Andreas Rauh1(B), Ramona Westphal1,
Harald Aschemann1, and Ekaterina Auer2
1 Chair of Mechatronics, University of Rostock,
Justus-von-Liebig-Weg 6, 18059 Rostock, Germany
{andreas.rauh,harald.aschemann}@uni-rostock.de
2 University of Applied Sciences Wismar,
Faculty of Engineering, 23952 Wismar, Germany
ekaterina.auer@hs-wismar.de
Abstract. The computation of guaranteed state enclosures has a large
variety of applications in engineering if initial value problems for sets
of ordinary diﬀerential equations are concerned. One possible scenario
is the use of such state enclosures in the design and veriﬁcation of lin-
ear and nonlinear feedback controllers as well as in predictive control
procedures. In many of these applications, system models are charac-
terized by a dominant linear part (commonly after a suitable coordi-
nate transformation) and by a not fully negligible nonlinear part. To
compute guaranteed state enclosures for such systems, general purpose
approaches relying on a Taylor series expansion of the solution can be
employed. However, they do not exploit knowledge about the speciﬁc
system structure. The exponential state enclosure technique makes use
of this structure, allowing users to compute tight enclosures that con-
tract over time for asymptotically stable dynamics. This paper ﬁrstly
gives an overview of exponential enclosure techniques, implemented in
ValEncIA-IVP, and secondly focuses on extensions to dynamic systems
with single and multiple conjugate complex eigenvalues.
Keywords: Ordinary diﬀerential equations · Initial value problems ·
Complex interval arithmetic · ValEncIA-IVP
1
Introduction
ValEncIA-IVP is a veriﬁed solver providing guaranteed enclosures for solu-
tions to initial value problems (IVPs) for sets of ordinary diﬀerential equations
(ODEs). In the basic version of this solver, the veriﬁed solution is computed
as the sum of a non-veriﬁed approximate solution (computed, for example,
This work was performed while R. Westphal was with the Chair of Mechatronics,
University of Rostock.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 247–256, 2016.
DOI: 10.1007/978-3-319-31769-4 20

248
A. Rauh et al.
by Euler’s method) and additive guaranteed error bounds determined using a
simple iteration scheme [1].
The disadvantage of this iteration scheme, however, is that the widths of
the resulting state enclosures might get larger even for asymptotically stable
ODEs [9]. This phenomenon is caused by the so-called wrapping eﬀect which
arises if non-axis-parallel state enclosures are described by axis-aligned interval
boxes in a state-space of dimension n > 1. In general purpose solvers such as
VNODE-LP [7] or VSPODE [5], the corresponding counter-measure against
this type of overestimation is the preconditioning of state equations (e.g. using
Lohner’s QR decomposition approach) in combination with a high-order series
expansion of the solution to the IVP over time. However, these approaches may
become quite time consuming for large system orders n.
A possible approach to deal with the overestimation in such a way as to
allow real-time implementations (e.g. for predictive control [10]), that is, without
increasing the computational cost too much, is to transform the ODEs into a
suitable canonical form. For the case of linear ODEs with real eigenvalues of
multiplicity one, this is given by the Jordan canonical form. The transformation
results in a decoupling of the vector-valued set of state equations. A solution
of this transformed IVP can then be determined by an exponential enclosure
technique which guarantees that asymptotically stable solutions are represented
by contracting interval bounds if a suitable time discretization step size is chosen.
For real eigenvalues, this property holds as long as the value zero is not included
in any vector component of the solution interval.
As shown in [11,12], the before-mentioned advantageous contraction prop-
erty can be preserved for linear ODEs with conjugate complex eigenvalue
pairs if a transformation into the complex Jordan canonical form is employed.
Then, a complex-valued interval iteration scheme is used to determine state
enclosures [12]. The corresponding solution procedure — originally derived for
dynamic systems with eigenvalues of multiplicity one — is extended in this paper
to more general situations with several multiple real and complex eigenvalues.
This paper is structured as follows. Sect. 2 gives an overview of the real-valued
and complex-valued iteration schemes that are applicable inside the exponential
state enclosure approach. Extensions to eigenvalues with multiplicity greater
than one are discussed in Sect. 3. Representative simulation results for a techni-
cally motivated benchmark system from control engineering, typically containing
bounded uncertainty in initial values and parameters, are presented in Sect. 4.
Conclusions and an outlook on future work can be found in Sect. 5.
2
Basic Exponential State Enclosure Approach
Throughout this paper, it is assumed that dynamic system models are given by
the set of ODEs
˙x (t) = f (x (t)) ,
x ∈Rn,
f : Rn →Rn,
(1)

Exponential Enclosure Techniques for Initial Value Problems
249
with smooth right-hand sides f (x (t)) and the uncertain initial conditions
x (0) ∈[x0] := [x] (0) = [x (0); x (0)] .
(2)
In (1) and (2), external control (input) signals u(t) = u(x(t)) are directly
included in the corresponding expression for f (x (t)). The same holds for time-
invariant uncertain system parameters pj ∈

pj; pj

, j = 1, . . . , np, with the
corresponding derivatives ˙pj = 0. Together with the enclosures for the time-
varying system states, they are contained in the component-wise deﬁned interval
vectors [x] =

[x1] . . . [xn]
T with the individual vector entries [xi] = [xi; xi],
xi ≤xi ≤xi, i = 1, . . . , n.
In the basic implementation of ValEncIA-IVP, the state enclosure [x] (t)
of the true solution x∗(t) to an IVP is deﬁned by x∗(t) ∈[x] (t) := ˜x(t) +
[R] (t), where ˜x(t) is an approximate solution computed in usual (non-veriﬁed)
ﬂoating point arithmetic. Veriﬁed error bounds [R] (t) are then computed by an
appropriate iteration scheme [9,12].
Note that without suitable counter-measures, the diameters of the solution
enclosures may diverge even for asymptotically stable systems. This is mostly
caused by the wrapping eﬀect that can be compensated for systems with a
dominant (locally) linear behavior by using the following exponential enclosure
technique. As a fundamental ansatz for the representation of contracting state
enclosures, the expression
x∗(t) ∈[xe] (t) := exp ([Λ] · t) · [xe] (0)
(3)
is used, with 0 ̸∈[xe,i] (0), [xe] (0) = [x0], and the diagonal matrix [Λ] :=
diag {[λi]}, i = 1, . . . , n, with the element-wise negative real entries λi.
After deﬁning exp ([Λ] · t) := diag {exp ([λ1] · t) , . . . , exp ([λn] · t)} as the cor-
responding interval matrix exponential, a Picard iteration scheme [2,6]
x∗(t) ∈[xe](κ+1) (t) := [x0] +
 t
0
f

[xe](κ) (s)

ds
(4)
can be employed to determine intervals [λi] so that all reachable states are con-
tained in the time-dependent interval enclosure functions [xe] (t). To derive the
iteration scheme for [xe] (t), the Picard iteration (4) is reformulated as the time-
dependent expression
x∗(t) ∈exp

[Λ](κ+1) · t

· [xe] (0) = [xe](κ+1) (t)
=: [x0] +
 t
0
f

exp

[Λ](κ) · s

· [xe] (0)

ds.
(5)
Its diﬀerentiation with respect to time and the evaluation for t ∈[0; T] leads to
˙x∗([0; T]) ∈diag
	
[λi](κ+1)
· exp

[Λ](κ+1) · [0; T]

· [xe] (0)
⊆f

exp

[Λ](κ) · [0; T]

· [xe] (0)

.
(6)

250
A. Rauh et al.
Suppose that the convergence condition
exp

[Λ](κ+1) · t

· [xe] (0) ⊆exp

[Λ](κ) · t

· [xe] (0) ,
(7)
that is equivalent to [λi](κ+1) ⊆[λi](κ) and [Λ](κ+1) ⊆[Λ](κ), is fulﬁlled. Then,
the ﬁnal iteration formula is given by
[λi](κ+1) :=
fi

exp

[Λ](κ) · [0 ; T]

· [xe] (0)

exp

[λi](κ) · [0 ; T]

· [xe,i] (0)
, i = 1, . . . , n
(8)
with the guaranteed state enclosure at the point t = T
x∗(T) ∈[xe] (T) := exp ([Λ] · T) · [xe] (0) .
(9)
A detailed derivation of this iteration approach is given in [12].
The above-mentioned iteration can be simpliﬁed for linear state equations
fi (x(t)) =
n

j=1
aij · xj(t)
according to
(10)
[λi](κ+1) :=
n

j=1,i̸=j

aij · exp

[λj](κ) −[λi](κ)
· [0 ; T]

· [xe,j] (0)
[xe,i] (0)

+ aii
with
aij ∈[aij] .
(11)
From this simpliﬁcation, it becomes obvious that the computation of [λi] is free of
overestimation if the expressions in (10) are decoupled with aij = 0 for all i ̸= j.
In cases in which the linear parts of fi (x) represent the dominant features of
the system dynamics, an (approximate) decoupling of the ODEs is only possible
if pairwise diﬀerent real eigenvalues are present. Then, the linear part of the
system model is transformed into real-valued Jordan canonical form.
Already in the case of linear systems with conjugate complex eigenvalue
pairs, there exist points of time at which the iteration (8) is no longer deﬁned
due to the fact that the value zero may be included in the true solution set and
hence also in the denominator of (8). The latter problem can easily be solved by
replacing the real-valued Jordan canonical form [3,4]
Σ = blkdiag{. . . , ¯
Σi, . . .},
¯
Σi =

σi ωi
−ωi σi

(12)
by its complex-valued generalization [8] with the corresponding ODEs ˙z(t) =
Σ·z(t), the initial conditions z(0) ∈Cn, z(0) ∈[z] (0), and the diagonal matrices
Σ = blkdiag{. . . , Σi, . . .},
Σi =

σi + jωi
0
0
σi −jωi

.
(13)

Exponential Enclosure Techniques for Initial Value Problems
251
This transformation is possible if the linear state Eq. (10) have only eigen-
values of multiplicity δi = 1. For the exact solutions zi(t) = e(σi+jωi)·t · zi(0),
zi+1(t) = e(σi−jωi)·t · zi+1(0) of the IVP, the iteration procedure (11) is always
applicable for 0 ̸∈[zi] (0) due to
|zi(t)|2 =

e(σi+jωi)·t · e(σi−jωi)·t
· |zi(0)|2 = e2σit · |zi(0)|2 ̸= 0.
(14)
As shown in [12], the corresponding enclosures show contracting behavior not
only for purely linear asymptotically stable systems but also for nonlinear models
if a linearization of the state equations approximates the dominant dynamic
features suﬃciently well for some ﬁnite time interval. However, according to [12],
the complex-valued iteration (and also its real-valued counterpart) are applicable
for arbitrary time spans only if the eigenvalue multiplicity is δi ≡1. Therefore,
novel extensions for δi > 1 are derived in the following section.
3
Exponential State Enclosures for Multiple Eigenvalues
In this section, extensions are described for the exponential enclosure approach
in the cases that linear state equations with multiple identical eigenvalues or
nonlinear models with multiple eigenvalues of their linear parts are considered.
3.1
Linear State Equations
Even for linear system models, the dynamics can no longer be fully decoupled
if eigenvalues have a multiplicity δi > 1. In the case of real eigenvalues, the
corresponding Jordan blocks are given by
˙z(t) = Σ · z(t)
with
Σ = blkdiag{λ1, λ2 . . . , Σi, . . . λn},
Σi =
⎡
⎢⎢⎢⎢⎣
λi 1 . . . 0
0 λi
... ...
... ... ... 1
0 . . . 0 λi
⎤
⎥⎥⎥⎥⎦
∈Rδi×δi
and
z(0) ∈[z] (0),
(15)
while the case of multiple complex eigenvalues leads to the canonical form
Σ = blkdiag{. . . , Σ+
i , Σ−
i , . . .}
with
λ+
i = σi + jωi,
λ−
i = σi −jωi ,
Σ+
i =
⎡
⎢⎢⎢⎢⎣
λ+
i
1 . . . 0
0 λ+
i
...
...
...
... ... 1
0 . . . 0 λ+
i
⎤
⎥⎥⎥⎥⎦
∈Cδi×δi
and
Σ−
i =
⎡
⎢⎢⎢⎢⎣
λ−
i
1 . . . 0
0 λ−
i
...
...
...
... ... 1
0 . . . 0 λ−
i
⎤
⎥⎥⎥⎥⎦
∈Cδi×δi (16)
for each eigenvalue pair λ±
i = σi ±jωi with δi > 1. In both the real and complex
cases, all decoupled state equations can be solved independently from the Jordan

252
A. Rauh et al.
block corresponding to the multiple eigenvalues. Overestimation is minimized if
the enclosures for states of the Jordan blocks (15) and (16) are computed in a
“bottom to top” manner, that is in the order zi+δi−1, . . . , zi+1, zi.
Since the analytic representation of the solutions zi+j(t), j = 0, . . . , δi −1,
for the eigenvalue λ+
i can be stated explicitly as
z∗
i+j(t) =
⎛
⎝
δi−1

ζ=j
tζ−j
(ζ −j)! · zi+ζ(0)
⎞
⎠· e(σi+jωi)·t
(17)
in the case δi > 1, the iteration scheme for computation of state enclosures is
derived for a redeﬁned enclosure that is given by
[zi+j] (t) =
⎛
⎝
δi−1

ζ=j
tζ−j
(ζ −j)! [zi+ζ] (0)
⎞
⎠· e[λi+j]t
(18)
for all j = 0, . . . , δi −1. The corresponding time derivative of (18) is
[ ˙zi+j] (t) = [λi+j] ·
⎛
⎝
δi−1

ζ=j
tζ−i
(ζ −i)! [zi+ζ] (0)
⎞
⎠· e[λi+j]t
+
⎛
⎝
δi−1

ζ=j+1
tζ−(j+1)
(ζ −(j + 1))! [zi+ζ] (0)
⎞
⎠· e[λi+j]t.
(19)
Evaluating these enclosures for the interval initial conditions zζ(0) ∈[zζ] (0)
and for the solution parameter λi+j ∈[λi+j] with the discretization time span
t ∈[0; T], a modiﬁed iteration scheme is obtained by following exactly the same
arguments as in Sect. 2. The interval enclosures [λi], . . . , [λi+δi−1] are given by
[λi+j](κ+1) :=
λ∗
i ·

δi−1

ζ=j
tζ−i
(ζ−i)!zi+ζ(0)

· e[λi+j](κ)t

δi−1

ζ=j
tζ−i
(ζ−i)!zi+ζ(0)

· e[λi+j](κ)t
+

δi−1

ζ=j+1
tζ−(j+1)
(ζ−(j+1))!zi+ζ(0)

·

e[λi+j+1]t −e[λi+j](κ)t

δi−1

ζ=j
tζ−i
(ζ−i)!zi+ζ(0)

· e[λi+j](κ)t
(20)
for each subsystem model
˙zi(t) = λ∗
i · zi(t) + zi+1(t)
˙zi+1(t) = λ∗
i · zi+1(t) + zi+2(t)
...
(21)
˙zi+δi−1(t) = λ∗
i · zi+δi−1(t).

Exponential Enclosure Techniques for Initial Value Problems
253
Here, the one-sided decoupling in (21) can be exploited eﬃciently, since [λi+j]
depends on the result for [λi+j+1] but not vice versa. Note that iteration (20)
satisﬁes the inclusion property λ∗
i ∈[λi+j], zζ(0) ∈[zζ] (0), t ∈[0; T], where λ∗
i
is the true multiple eigenvalue.
If the system models (15), (16), or (21) are linear, the iteration formula (20)
can be simpliﬁed symbolically as
[λi+j](κ+1) := λ∗
i +

δi−1

ζ=j+1
tζ−(j+1)
(ζ−(j+1))!zi+ζ(0)

·

e([λi+j+1]−[λi+j](κ))t −1


δi−1

ζ=j
tζ−j
(ζ−j)!zi+ζ(0)

.
(22)
In this way, the overestimation due to multiple dependencies on common interval
variables is reduced as much as possible. As before, (20) and (22) have to be
evaluated for all λi+j ∈[λi+j], zζ(0) ∈[zζ] (0), and t ∈[0; T].
3.2
Generalization to Nonlinear State Equations
The iteration procedure introduced in the previous subsection needs to be gen-
eralized in the practically important case of nonlinear terms on the right-hand
sides of (21). In particular, system models given by ˙zi = λ∗
i · zi + zi+1 + gi(z),
˙zi+1 = λ∗
i · zi+1 + zi+2 + gi+1(z), . . ., ˙zi+δi−1 = λ∗
i · zi+δi−1 + gi+δi−1(z) with
gi(z), . . . , gi+δi−1(z) : Cn →C are considered subsequently. Then, a vector-
valued iteration has to be performed with the convergence condition

[λi](κ+1) [λi+1](κ+1) . . . [λi+δi−1](κ+1)T
!⊂

[λi](κ) [λi+1](κ) . . . [λi+δi−1](κ)T
(23)
and the modiﬁed iteration scheme
[λi+j](κ+1) := λ∗
i +

δi−1

ζ=j+1
tζ−(j+1)
(ζ−(j+1))!zi+ζ(0)

·

e([λi+j+1](κ)−[λi+j](κ))t −1


δi−1

ζ=j
tζ−j
(ζ−j)!zi+ζ(0)

+ ˜gi+j

[z](κ) (t)

(24)
with the nonlinear state-dependent enclosure term
˜gi+j

[z](κ) (t)

:=
gi+j

[z](κ) (t)


δi−1

ζ=j
tζ−i
(ζ−i)!zi+ζ(0)

· e[λi+j](κ)t
, zζ(0) ∈[zζ] (0), t ∈[0; T] .
(25)

254
A. Rauh et al.
3.3
Simpliﬁed Enclosures
The procedure described in Sect. 3.2 can be simpliﬁed for systems with small
time constants, especially if −σi ≫ωi holds in (16). Then, it is suﬃcient to
restrict the analytic expression for the enclosure and its time derivative to the
terms
zi+j = (zi+j(0) + t · zi+j+1(0)) · eλi+jt
˙zi+j = λi+j (zi+j(0) + t · zi+j+1(0)) · eλi+jt + zi+j+1(0) · eλi+jt, j < δi −1.
(26)
In analogy to (20) and (22), this leads to the simpliﬁed iteration procedure
[λi+j](κ+1) := λ∗
i + e([λi+j+1]−[λi+j](κ))t −1
zi+j(0)
zi+j+1(0) + t
,
zζ(0) ∈[zζ] (0),
t ∈[0; T] , (27)
which typically yields wider interval bounds than the exact representation from
the previous subsections. However, the simpliﬁcation of the expressions leads to
a reduction of the computational cost in the iteration. This simpliﬁed iteration is
equally applicable to both the linear and nonlinear cases studied in this section.
4
Simulation Results
Linear dynamic systems with conjugate complex eigenvalues are a common
model for a large variety of control systems. A real-life application scenario,
namely oscillation damping for ﬂexible high-bay rack feeder systems, was dis-
cussed in [11,12]. This system is characterized by the fact that — after feedback
control design — only asymptotically stable eigenvalues of multiplicity δi = 1
occur. However, many system models in drive trains with elasticities, e.g., series
connections of several identical mass-spring-damper elements or series connec-
tions of electric oscillators have clusters of multiple identical eigenvalues. Trans-
forming these state equations into Jordan canonical form (15) or (16) yields
system models that are similar to the following illustrative application scenario.
Assume that — after transformation into Jordan canonical from (16) — the
benchmark system (δi = 2) is given by the initial conditions and system matrix
z(0) ∈
⎡
⎢⎢⎣
⟨−5, 0.1⟩
⟨−4, 0.1⟩
⟨−5, 0.1⟩
⟨−4, 0.1⟩
⎤
⎥⎥⎦,
Σ ∈
⎡
⎢⎢⎣
⟨λ+⟩
1
0
0
0
⟨λ+⟩
0
0
0
0
⟨λ−⟩
1
0
0
0
⟨λ−⟩
⎤
⎥⎥⎦,
(28)
with the uncertain eigenvalues ⟨λ+⟩= ⟨−2 + 3j, 0.1⟩and ⟨λ−⟩= ⟨−2 −3j, 0.1⟩.
All uncertain initial conditions and eigenvalues are given in the complex-
valued midpoint-radius notation that is, e.g., available in the Matlab tool-
box IntLab [13]. Since the subsystems for eigenvalues with positive and neg-
ative imaginary parts are decoupled, the complete system model can be split

Exponential Enclosure Techniques for Initial Value Problems
255
up into two independent processes for simulation purposes. In the following,
simulation results are only summarized for the state variables z1 and z2 since
they show the same principle behavior as the remaining states z3 and z4.
It can be seen that despite the non-diagonal structure of the matrix Σ,
leading to a one-sided coupling of the state equations, no relevant wrapping eﬀect
occurs and that the asymptotic stability of the dynamic system is preserved in
the computed state enclosures despite the fact that the exact solutions of the
IVP are no longer pure exponential functions as it has been shown in (17).
For practical applications, the state enclosures in Fig. 1 further have to be
transformed back into the original real-valued coordinates x(t) ∈[x] (t) by left-
multiplying the enclosures [z] (t) with the matrix of eigenvectors that has been
used for the transformation into Jordan canonical form and — subsequently —
taking the real part of the resulting interval boxes. However, this transformation
preserves the presented contraction properties for suﬃciently large t > 0.
[z1](t)}
0
t
0.5
1.0
1.5
2.0
2.5
3.0
−6.0
−4.0
−2.0
0
2.0
(a) Real part of z1(t).
[z1](t)}
0
t
0.5
1.0
1.5
2.0
2.5
3.0
0
1.0
−1.0
−2.0
−3.0
(b) Imaginary part of z1(t).
[z2](t)}
0
t
0.5
1.0
1.5
2.0
2.5
3.0
−2.0
0
−4.0
−5.0
−3.0
−1.0
1.0
(c) Real part of z2(t).
[z2](t)}
0
t
0.5
1.0
1.5
2.0
2.5
3.0
−2.0
0.5
0
−0.5
−1.0
−1.5
(d) Imaginary part of z2(t).
Fig. 1. Guaranteed state enclosures for the illustrative example (28).
5
Conclusions and Outlook on Future Work
In this paper, practically relevant extensions for exponential state enclosure tech-
niques were presented for dynamic systems with both real and conjugate complex
eigenvalues of a multiplicity larger than one. By the proposed extensions it is pos-
sible to compute guaranteed state enclosures for asymptotically stable systems
which converge towards the steady-state operating points despite uncertainties

256
A. Rauh et al.
and oscillations in the solution. The corresponding complex-valued interval iter-
ation procedure will be included in future work in the design of sensitivity-based
predictive control and path planning procedures [10]. Moreover, an analysis of
the possible step sizes for which the suggested iteration scheme converges will
be performed. It can be expected that the computational eﬀort is signiﬁcantly
lower than for Taylor series-based approaches, since the computation of Taylor
coeﬃcients can be avoided and only a few evaluations of the functions, describing
the ODEs, are necessary with a suitable initialization of the iteration scheme.
References
1. Auer, E., Rauh, A., Hofer, E.P., Luther, W.: Validated modeling of mechanical
systems with SmartMOBILE: improvement of performance by ValEncIA-IVP.
In: Hertling, P., Hoﬀmann, C.M., Luther, W., Revol, N. (eds.) Real Number Algo-
rithms. LNCS, vol. 5045, pp. 1–27. Springer, Heidelberg (2008)
2. Deville, Y., Janssen, M., van Hentenryck, P.: Consistency techniques for ordinary
diﬀerential equations. Constraint 7(3–4), 289–315 (2002)
3. Hairer, E., Nørsett, S., Wanner, G.: Solving Ordinary Diﬀerential Equations I, 2nd
edn. Springer, Berlin Heidelberg (2000)
4. Jordan, C.: Trait´e des substitutions et des ´equations alg´ebriques. Gauthier-Villars,
Paris (1870). in French
5. Lin, Y., Stadtherr, M.A.: Validated solution of initial value problems for odes
with interval parameters. In: NSF Workshop Proceeding on Reliable Engineering
Computing. Savannah GA, February 22–24 2006
6. Nedialkov, N.S.: Computing Rigorous Bounds on the Solution of an Initial Value
Problem for an Ordinary Diﬀerential Equation. Ph.D. thesis, Graduate Department
of Computer Science, University of Toronto (1999)
7. Nedialkov, N.S.: Implementing a rigorous ODE solver through literate pro-
gramming. In: Rauh, A., Auer, E. (eds.) Modeling, Design, and Simulation
of Systems with Uncertainties. Mathematical Engineering, pp. 3–19. Springer,
Heidenberg (2011)
8. Petkovi´c, M., Petkovi´c, L.: Complex Interval Arithmetic and Its Applications.
Wiley-VCH Verlag GmbH, Berlin (1998)
9. Rauh, A., Auer, E., Hofer, E.P.: ValEncIA-IVP: a comparison with other initial
value problem solvers. In: CD-Proceedings of 12th GAMM-IMACS Intenational
Symposium on Scientiﬁc Computing, Computer Arithmetic, and Validated Numer-
ics SCAN 2006. IEEE Computer Society, Duisburg, Germany (2007)
10. Rauh, A., Kersten, J., Auer, E., Aschemann, H.: Sensitivity-based feedforward and
feedback control for uncertain systems. Computing 2–4, 357–367 (2012)
11. Rauh, A., Westphal, R., Aschemann, H.: Veriﬁed simulation of control systems
with interval parameters using an exponential state enclosure technique. In:
CD-Proceedings of IEEE International Conference on Methods and Models in
Automation and Robotics MMAR. Miedzyzdroje, Poland (2013)
12. Rauh, A., Westphal, R., Auer, E., Aschemann, H.: Exponential enclosure tech-
niques for the computation of guaranteed state enclosures in ValEncIA-IVP. In:
Proceedings of 15th GAMM-IMACS International Symposium on Scientiﬁc Com-
puting, Computer Arithmetic, and Validated Numerics SCAN 2012, vol. 19(1), pp.
66–90. Novosibirsk, Russia, Special Issue of Reliable Computing (2013)
13. Rump, S.M.: IntLab - INTerval LABoratory. In: Csendes, T. (ed.) Developments in
Reliable Computing, pp. 77–104. Kluver Academic Publishers, Dordrecht (1999)

PDE

Curve Veering for the Parameter-dependent
Clamped Plate
Henning Behnke(B)
Institut f¨ur Mathematik, TU Clausthal, Erzstraße 1,
38678 Clausthal-Zellerfeld, Germany
behnke@math.tu-clausthal.de
Abstract. The computation of vibrations of a thin rectangular clamped
plate results in an eigenvalue problem with a partial diﬀerential equation
of fourth order. If we change the geometry of the plate for ﬁxed area,
this results in a parameter-dependent eigenvalue problem. For certain
parameters, the eigenvalue curves seem to cross. We give a numerically
rigorous proof of curve veering, which is based on the Lehmann-Goerisch
inclusion theorems and the Rayleigh-Ritz procedure.
Keywords: Partial
diﬀerential
equations ·
Paramenter-dependent
eigenvalue problem · Upper and lower eigenvalue bounds · Interval arith-
metic
1
Parameter-dependent Eigenvalue Problems
Parameter-dependent eigenvalue problems occur in many applications, for exam-
ple in the computation of vibrations of turbine blades [2], in computing sloshing
frequencies of a liquid in a container, in studying molecule geometries, in com-
puting vibrations of free plates [4] or in computing the vibrations of a thin
rectangular clamped plate.
The lattermost problem is described by a partial diﬀerential equation of
fourth order:
∂4
∂x4 ϕ + P
∂4
∂x2∂y2 ϕ + Q ∂4
∂y4 ϕ = λϕ in Ω,
(1)
ϕ = 0 and ∂ϕ
∂n = 0 on ∂Ω,
ϕ(x, y) = ϕ(−x, y) = ϕ(x, −y) in Ω,
here P,Q ∈R, P > 0, Q > 0, and Ω = (−a
2, a
2) × (−b
2, b
2) ⊆R2. The diﬀerential
operator in (1) is self-adjoint and the eigenvalues are positive.
We consider the eigenvalues as functions of s = a/b for F = a b = 4. Here a
and b are the side lengths of Ω. An approximate computation is shown in Fig. 1.
The locations left of #3 and right of #4 are remarkable. It is not clear whether
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 259–268, 2016.
DOI: 10.1007/978-3-319-31769-4 21

260
H. Behnke
the veering of the eigenvalue curves is an eﬀect introduced by the discretization or
whether the eigenvalues of (1) intersect. (There are further “possible crossings”
#1 and #2 near s = 1.6 and s = 1.3 which are not marked in Fig. 1, since #3
and #4 seem to be more interesting.)
The situation is even more astonishing, if we consider eigenfunctions. Figure 2
shows, that the shape of eigenfunctions is preserved along the eventual “cross-
ings”. In this paper we give a numerically rigorous proof for curve veering at
positions #3 and #4. This paper shows that the resulting algebraic eigenvalue
problems which can be treated successfully, can be considerably larger than in [2].
There are several papers dealing with curve veering for plate problems, but
so far there exists no numerically rigorous proof.
Fig. 1. Eigenvalues λ3, ..., λ9 as functions of s = a/b with F = a b = 4
2
Inclusion Methods
In this section we brieﬂy describe the Rayleigh-Ritz and the Lehmann-Goerisch
methods for computing upper and lower eigenvalue bounds, respectively. For
proofs see [9].
Let (H, (.|.)) be a Hilbert space with inner product (.|.) and norm ||.|| and
V be a densely deﬁned subspace of H. Denote by [.|.] the inner product in V
and let (V, [.|.]) be a Hilbert space (||| . ||| denotes the norm in V ), the embedding
V →H is assumed to be compact.
Then the eigenvalue problem reads as follows
Determine λ ∈R and ϕ ∈V , ϕ ̸= 0 such that
[ϕ|v] = λ(ϕ|v) for all v ∈V.
(2)
Problem (2) has a countable spectrum which consists of eigenvalues
0 < λ1 ≤λ2 ≤· · ·
,
lim
j→∞λj = ∞.

Curve Veering for the Parameter-dependent Clamped Plate
261
Fig. 2. Eigenvalues λ4, ..., λ6 and approximate eigenfunctions as functions of s = a/b
with F = a b = 4
The Rayleigh-Ritz procedure is a discretization of the Poincar´e principle:
λj =
min
E⊂V
dim E=j
max
u∈E
u̸=0
[u|u]
(u|u) ,
j ∈N .
(3)
Now choose u1, . . . , un ∈V , n ∈N , linearly independent, and deﬁne
Vn := span(u1, . . . , un) .
A restriction of (3) to Vn instead of V results in
Λ[n]
1
≤Λ[n]
2
≤· · · ≤Λ[n]
n ,
and the upper bounds
λj ≤Λ[n]
j
,
j = 1, . . . , n .
We call Λ[n]
j
Rayleigh-Ritz bound for λj, the Λ[n]
j
can be computed easily. Deﬁne
A0 :=

(ui|uk)

i,k=1,...,n ,
A1 :=

[ui|uk]

i,k=1,...,n ,
then the Rayleigh-Ritz bounds are the eigenvalues of
A1x = Λ[n]A0x ,
(Λ[n], x) ∈R × Rn .
The Rayleigh-Ritz bounds are monotonously decreasing in n ∈N.

262
H. Behnke
The Lehmann-Goerisch procedure can be interpreted as discretization of a
variational principle as well. Let ρ ∈R be a spectral parameter, for a N ∈N, let
λN < ρ < λN+1 .
(4)
Let
λN+1−i = ρ + 1
σi
,
i = 1, . . . , N .
We assume σi < 0.
For u ∈V , let wu ∈H be the unique solution of
[u|v] = (wu|v)
for all v ∈V ,
then σi is characterized by
σi =
inf
E⊂V
dim E=i
max
u∈E
u̸=0
[u|u] −ρ(u|u)
(wu|wu) −2ρ[u|u] + ρ2(u, u) ,
i = 1, . . . , N .
(5)
A negative upper bound for σi yields a lower bound for λN+1−i. For a discretiza-
tion of (5) determine w1, . . . , wn ∈H such that
[ui|v] = (wi|v)
for all v ∈V .
Let
A2 :=

(wi|wk)

i,k=1,...,n ,

A1 −ρA0

x = τ

A2 −2ρA1 + ρ2A0

x ,
(τ, x) ∈R × Rn .
(6)
If the condition Λ[n]
N
< ρ holds true for some n ∈N, (6) has exactly N
negative eigenvalues
τ1 ≤τ2 ≤. . . ≤τN < 0 ≤. . . ≤τn .
We have (σi ≤τi, i = 1, . . . , N)
Λρ[n]
j
:= ρ +
1
τN+1−j
≤λj ,
j = 1, . . . , N .
This discretization is the Lehmann-Goerisch procedure. We call Λρ[n]
j
Lehmann-Goerisch bound for λj.
3
Application to the Plate Problem
In order to apply the inclusion theorems to (1), we deﬁne
H := {u ∈L2(Ω) | u(x, y) = u(−x, y) = u(x, −y), (x, y) ∈Ω},
V := {u ∈H2
0(Ω) | u(x, y) = u(−x, y) = u(x, −y), (x, y) ∈Ω},
(f|g) :=

Ω
f g dΩ for f, g ∈H,
[f|g] :=

Ω
∂2f
∂x2
∂2g
∂x2 + P ∂2f
∂x2
∂2g
∂y2 + Q ∂2f
∂y2
∂2g
∂y2

dΩ
for f, g ∈V.

Curve Veering for the Parameter-dependent Clamped Plate
263
The eigenvalue problem reads as follows:
Determine λ(s) ∈R, φ ∈V such that
[f|φ] = λ(s) (f|φ) for all f ∈V.
Let ui ∈V ∩C4(Ω) and
M f := ∂4
∂x4 f + P
∂4
∂x2∂y2 f + Q ∂4
∂y4 f ,
now deﬁne
wi ∈H by wi := M ui .
Then
[f|ui] = (f|wi) for all f ∈V.
Deﬁne
˜fi(x) :=
a2
4 −x2
(i+1)
, ˜gj(y) :=
b2
4 −y2
(j+1)
, i, j ∈N,
˜ui,j(x, y) := ˜fi(x) ˜gj(y) i, j ∈N.
The polynomials ˜ui,j satisfy the boundary and symmetry conditions, i.e.
˜ui,j ∈V .
To avoid the well known numerical problems with ill-conditioned matrices
(see e.g. [5]), we construct orthogonal polynomials fi and gj from the ˜fi and ˜gj
using the Gram-Schmidt process and a computer algebra system (for example
Mathematica [8] ). Then we deﬁne
ui,j(x, y) := fi(x)gj(y) for i, j ∈N.
In order to determine ρ (see (4)), we need rough lower eigenvalue bounds (the
Rayleigh-Ritz procedure yields upper bounds). These can be obtained from the
eigenvalues of the problem
∂4
∂x4 ϕ + Q ∂4
∂y4 ϕ = λϕ in Ω,
ϕ = 0 and ∂ϕ
∂n = 0 on ∂Ω,
ϕ(x, y) = ϕ(−x, y) = ϕ(x, −y) in Ω,
which can be solved in closed form by separation of variables. If these rough
bounds are too crude, a homotopy-method can be used ([3,7]). Using the
described methods and interval arithmetic, veriﬁed bounds for eigenvalues of
(1) for ﬁxed s can be obtained, see Table 1. The results have been obtained using
PROFIL/BIAS, see [6].

264
H. Behnke
Table 1. Bounds for eigenvalues of the orthotropic plate, λi ∈[inf[ΛLG
i
], sup[ΛRR
i
]]
P = 2 ,
Q = 2 ,
F = 4 ,
s = 1 ,
ρ = 30404.9542
λi
n = 66
n = 136
n = 210
1
112.34380300370
29815764 E ± 0 112.3438024882
48900881 E ± 0 112.34380248479
51494059 E ± 0
2
1.1217536545890
047907552 E + 3
1.1217536102349
451413903 E + 3
1.1217536090521
49228742 E + 4
3
2.000984490746
895785651 E + 3
2.000984420846
68829397 E + 3
2.000984418602
79054004 E + 3
4
3.945291688717
3268707890 E + 3
3.945290445259
4883713625 E + 3
3.94529039462
02810079 E + 3
5
6.010039214042
0911402918 E + 3
6.01003872475
09841992629 E + 3
6.01003870327
0999971107 E + 3
6
1.0613069606701
59334754544 E + 4
1.0613062508540
0821906680 E + 4
1.061306213463
214563549 E + 4
7
1.154007769061
377723907 E + 4
1.154007687367
3951000071 E + 4
1.154007683119
399698021 E + 4
8
1.5244047455105
087332590
E + 4
1.5244038538533
348240199
E + 4
1.5244038028899
224606136 E + 4
9
2.0032337136971
176596685
E + 4
2.003233489129
287182259 E + 4
2.00323347626
16212849 E + 4
10 2.5347032604487
47927946537
E + 4
2.534699761186
199957246
E + 4
2.534699534913
167375697 E + 4
11 2.7233110451279
6757640428
E + 4
2.723308076912
074138273
E + 4
2.723307897462
195766257
E + 4
4
The Parameter-dependent Problem
4.1
Temple Quotients
In order to prove the curve veering for an interval [α, β], we need to calculate
veriﬁed bounds for λi(s) of the form
qi(s) −ε ≤λi(s) ≤qi(s) + ε for all s ∈[α, β],
for ε small and qi “simple” explicitly known functions. Since the application of
the inclusion theorems to parameter-dependent eigenvalue problems results in
parameter-dependent matrices A0, A1 and A2, this can be achieved by treating
parameter-dependent matrix eigenvalue problems [1].
For this aim consider the matrix eigenvalue problem
A x = Λ B x,
for A, B ∈Rn×n, A = AT , B = BT , B positive deﬁnite. Let ρ ∈R, u ∈Rn,
u ̸= 0 and v = B−1A u. Deﬁne
a0,A,B := uT B u ,
a1,A,B := uT A u ,
a2,A,B := vT B v .
For a1,A,B −ρa0,A,B ̸= 0, deﬁne the Temple Quotient
τA,B(ρ) := a2,A,B −ρa1,A,B
a1,A,B −ρa0,A,B

Curve Veering for the Parameter-dependent Clamped Plate
265
and
τA,B(±∞) := a1,A,B
a0,A,B
.
Then the interval

(ρ, τA,B(ρ)] if
ρ < τA,B(ρ)
[τA,B(ρ), ρ) if τA,B(ρ) < ρ
contains at least one eigenvalue of the eigenvalue problem A x = Λ B x. (The
case ρ = τA,B(ρ) can not happen, since ρ = τA,B(ρ) implies v = ρ u and this
results in a1,A,B −ρa0,A,B = 0.)
These enclosures will be applied to parameter-dependent matrices, hence v
and a2,A,B are not known exactly. Thus we mention, that the statement remains
valid if a2,A,B is replaced by an upper bound [2]
a2,A,B ≤˜a2,A,B.
Let c ∈R, 0 < c ≤Λmin(B), ˜v ∈Rn.
Then [1] or [3]
a2,A,B = vT B v
≤˜vT A u −˜vT (B ˜v −A u)
+1
c (B ˜v −A u)T (B ˜v −A u)
=: ˜a2,A,B.
In the applications we choose
˜v ≈B−1Au .
Based on Temple-Quotients, an easy procedure for computing upper eigen-
value bounds for
Λ1 < Λ2 < . . . < Λn
can be constructed:
Procedure for upper bounds:
1. Calculate 0 < c ≤Λmin(B)
2. Let ρ := −∞and i := 1.
3. Choose an appropriate u ∈Rn, let v ≈B−1Au and
calculate τA,B(ρ) using ˜a2,A,B.
4. If τA,B(ρ) ≤ρ then break down.
5. Set the interval Λi to (ρ, τA,B(ρ)].
6. If i < n let ρ := τA,B(ρ) and i := i + 1, goto 3.

266
H. Behnke
If this procedure does not break down:
Λi ∈Λi
for
i = 1, . . . , n.
In general the bound
Λi ≤max(Λi)
is very sharp, if
u = ˜xi,
where ˜xi is an approximate eigenvector for Λi.
The computation of sharp lower eigenvalue bounds can be done similarily:
Start the procedure with ρ = +∞and i = n.
4.2
Application to Parameter-dependent Matrices
If A(s) and B(s) are parameter-dependent matrices, then a0,A,B, a1,A,B and
˜a2,A,B depend on s. Thus, (τA,B(ρ))(s) is also a real function.
If in the i-th step bounds for Λi are to be calculated, we determine an approx-
imation polynomial qi for Λi in [α, β] and deﬁne
Hi(s) := A(s) −qi(s) B(s) .
The eigenvalues of
Hi(s) x(s) = ΛHi(s) B(s) x(s)
and
A(s) x(s) = Λ(s) B(s) x(s)
are closely related. Indeed, we have
Λj,Hi(s) = Λj(s) −qi(s) ,
and
Λi,Hi ≈0
in [α, β] .
Now we compute τHi,B(ρ −qi) and determine bounds for the range of
τHi,B(ρ −qi).
The elements of this range are close to zero if the approximation polynomial
is good:
−εi ≤{ (τHi,B(ρ −qi))(s) | s ∈[α, β] } ≤εi
This results in the bounds
qi(s) −εi ≤Λi(s) ≤qi(s) + εi for all s ∈[α, β] .

Curve Veering for the Parameter-dependent Clamped Plate
267
5
Results
Now we can apply the theory to the clamped plate problem. For the upper
bounds we compute upper bounds for the eigenvalues of (1) with the Rayleigh-
Ritz procedure and upper bound for the eigenvalues of the resulting parameter-
dependent matrix eigenvalue problem. For the lower bounds we apply the
Lehmann-Goerisch method to (1) and compute lower bounds to the eigenval-
ues of the resulting parameter-dependent matrix eigenvalue problem. Since the
Fig. 3. Bounds for eigenvalues λ4 and λ5 near #3
Table 2. Bounds for eigenvalues λ4 and λ5 near #3
Parameter-interval s2 ∈[5.01, 5.03], i.e. s ∈[2.23, 2.24];
location #3
i
c0
c1
c2
εi
4 −1.1728135 E + 6 4.6910569 E + 5
−4.6720581 E + 4 1.7336
5 1.1851818 E + 6
−4.7045191 E + 5 4.6872968 E + 4
1.1327
n=128; ρ = 20000
Fig. 4. Bounds for eigenvalues λ5 and λ6 near #4

268
H. Behnke
Table 3. Bounds for eigenvalues λ5 and λ6 near #4
Parameter-interval s2 ∈[4.29, 4.33], i.e. s ∈[2.07, 2.09];
location #4
i
c0
c1
c2
εi
5 −4.5180060 E + 5 2.1221805 E + 5
−2.4630601 E + 4 0.95431
6 4.6776638 E + 5
−2.1453928 E + 5 2.4883714 E + 4
1.2748
n=400; ρ = 12800
matrices depend on s2, the resulting approximation polynomial is a polyno-
mial in s2. The coeﬃcients and εi = max(εi, εi) are given in Tables 2 and 3.
Figures 3 and 4 show the results. Thus it is rigorously proved that no crossing
of the eigenvalue curves occurs.
The author thanks the referees for their valuable advice.
References
1. Behnke, H.: Bounds for Eigenvalues of Parameter-dependent Matrices. Computing
49, 159–167 (1992)
2. Behnke, H.: A Numerically Rigorous Proof of Curve Veering in an Eigenvalue
Problem for Diﬀerential Equations. Z. Anal. Anwendungen 15, 181–200 (1996)
3. Behnke, H., Goerisch, F.: Inclusions for eigenvalues of selfadjoint problems. In:
Herzberger, J. (ed.) Topics in validated computations, pp. 277–322. Elsevier, Ams-
terdam, Lausanne, New York, Oxford, Shannon, Tokyo (1994)
4. Behnke, H., Mertins, U.: Eigenwertschranken f¨ur das Problem der freischwingen-
den rechteckigen Platte und Untersuchungen zum Ausweichph¨anomen. ZAMM 75,
342–363 (1995)
5. Fried, I.: Numerical Solution of Diﬀerential Equations. Academic Press [Harcourt
Brace Jovanovich, Publishers], New York, London (1979). Computer Science and
Applied Mathematics
6. Kn¨uppel, O.: PROFIL/BIAS–a fast interval library. Computing 53(3–4), 277–287
(1994). International Symposium on Scientiﬁc Computing, Computer Arithmetic
and Validated Numerics (Vienna, 1993)
7. Plum, M.: Eigenvalue inclusions for second-order ordinary diﬀerential operators by
a numerical homotopy method. Z. Angew. Math. Phys. 41(2), 205–226 (1990)
8. Wolfram, S.: The Mathematica R
⃝book, 4th edn. Wolfram Media, Inc., Champaign,
IL; Cambridge University Press (1999)
9. Zimmermann, S., Mertins, U.: Variational bounds to eigenvalues of self-adjoint
problems with arbitrary spectrum. Z. Anal. Anwend. 14, 327–345 (1995)

Numerical Veriﬁcation for Elliptic Boundary
Value Problem with Nonconforming
P1 Finite Elements
Tomoki Uda(B)
Department of Mathematics, Kyoto University, Kitashirakawa Oiwake-cho, Sakyo-ku,
Kyoto 606-8502, Japan
uda@math.kyoto-u.ac.jp
Abstract. We propose a numerical method with the nonconforming P1
FEM to verify the existence of solutions to an elliptic boundary value
problem. Formulating the boundary value problem as a ﬁxed-point prob-
lem on the sum space of the nonconforming P1 ﬁnite element space with
the Sobolev space of 1st order with zero Dirichlet condition, we construct
the numerical veriﬁcation method based on the Schauder ﬁxed-point the-
orem. We show a constructive inequality for a boundary integral that
appears due to the discontinuity of a nonconforming P1 ﬁnite element
function. Finally, we present a numerical example to show our proposed
method works well.
Keywords: Nakao’s method · Numerical veriﬁcation · Elliptic bound-
ary value problem · Nonconforming P1 ﬁnite element
1
Introduction
A ﬁnite element method (FEM), which is based on piecewise polynomial approx-
imation, is widely used to solve partial diﬀerencial equations numerically. In a
certain boundary value problem (BVP), when we use a lower-order conforming
FEM, the numerical solution converges to the exact solution slowly, which is
known as a locking eﬀect [1]. To avoid the locking eﬀect for such a BVP, it is
known that a certain nonconforming FEM is helpful. For example, Lee et al. [5]
proposed an optimal and robust nonconforming FEM for the planar linear elas-
ticity problem.
In 1988, M. T. Nakao [7] developed a method based on a FEM to verify
the existence of solutions to an elliptic BVP. Nakao’s method is useful not only
in verifying the existence of solutions mathematically but also in estimating an
a-posteriori error for a numerical solution to the BVP. Nakao’s theory implic-
itly assumed the ﬁnite element (FE) space to be conforming in order to deduce
veriﬁcation conditions [7]. Especially it relies on continuity of a conforming FE
function, whereas a nonconforming FE function is discontinuous in general. There-
fore, it is not obvious how to apply Nakao’s method to a nonconforming FEM.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 269–279, 2016.
DOI: 10.1007/978-3-319-31769-4 22

270
T. Uda
Thus, in this paper, we generalize Nakao’s method for the nonconforming P1 FEM,
where P1 means piecewise linear approximation over a triangulation.
The structure of the paper is given as follows. In Sect. 2 we introduce the
notations for several function spaces. Section 3 is the main part of the paper. In
Sect. 3.1 we show some basic facts about the nonconforming P1 FEM. In Sect. 3.2
we formulate a BVP as a ﬁxed-point problem in order to apply the Schauder
ﬁxed-point theorem. In Sects. 3.3, 3.4 and 3.5 we derive suﬃcient conditions for
applying the Schauder ﬁxed-point problem. In Sect. 3.6 we show a constructive
inequality for a boundary integral arising due to the discontinuity of the non-
conforming P1 FE function. A numerical test is presented in Sect. 4 and ﬁnally
in Sect. 5 we summarize our proposed method.
2
Preliminaries
For a bounded open convex polygon Ω ⊊R2, L2(Ω) and Hk(Ω) (k = 1, 2)
denote the set of all R-valued square-integrable functions and the Sobolev space
of order k, respectively. Each function v ∈L2(Ω) belongs to Hk(Ω) if and
only if the distributional derivative ∂αv belongs to L2(Ω) for all multi-indices
|α| ≤k. H1
0(Ω) is the subspace of H1(Ω) endowed with zero Dirichlet boundary
conditions. Namely, H1
0(Ω) :=

v ∈H1(Ω) | v = 0 on ∂Ω

. L2(Ω) is equipped
with the standard inner product ( · , · )L2 and the standard norm ∥· ∥L2. Sim-
ilarly, Hk(Ω) is equipped with the standard norm ∥· ∥Hk. Especially, the H2
seminorm and the H2 norm are deﬁned as follows.
|u|2
H2(Ω) :=
2

i,j=1
∂xi∂xju
2
L2(Ω)
(u ∈H2(Ω) ) ,
∥u∥2
H2(Ω) := ∥u∥2
L2(Ω) + ∥∇u∥2
L2(Ω) + |u|2
H2(Ω)
(u ∈H2(Ω) ) .
In what follows, we omit the domain Ω in these notations unless otherwise stated.
We sometimes use the following Proposition 1 in estimating the norm
∥△· ∥L2 where △:= ∂2
x1 +∂2
x2 denotes the Laplace operator △: H2 ∩H1
0 →L2.
See Theorem 4.3.1.4 of [4].
Proposition 1. |u|H2(Ω) = ∥△u∥L2(Ω) holds for all u ∈H2(Ω) ∩H1
0(Ω).
3
Nakao’s Method with Nonconforming P1 FEM
We consider the following (nonlinear) elliptic boundary value problem.
Problem 1 (Elliptic BVP). For given f : H1 →L2, ﬁnd u ∈H2(Ω) such that

−△u = f(u)
in Ω ,
u = 0
on ∂Ω .
(1)

Numerical Veriﬁcation for Elliptic Boundary Value Problem
271
A nonlinear BVP is also considered in our formulation, but in Sect. 4 we give a
numerical test only for the linear case.
Since the Laplace operator △endowed with zero Dirichlet boundary con-
dition is invertible, we deﬁne the inverted operator △−1 from L2 to H2 ∩H1
0.
Thus, the BVP (1) is reforumulated as the ﬁxed-point problem on H1
0.
Problem 2 (Fixed-point Problem). For given f : H1
0 →L2, ﬁnd u ∈H1
0 such
that
u = −△−1 f(u) .
(2)
In order to apply the Schauder ﬁxed-point theorem to the ﬁxed-point problem
(2), M. T. Nakao [7] assumed that the map f satisﬁes the following property.
Assumption 1. The map f : H1
0 →L2 is continuous and bounded where H1
0
is equipped with the inner product (∇· , ∇· )L2. That is to say, for any bounded
subset U in H1
0, the image f(U) is also bounded in L2.
3.1
Nonconforming P1 Finite Elements
Let (Th)h>0 be a regular family of triangulations of the domain Ω. Here, the
subscript h denotes the mesh size of the triangulation Th. We denote by ei
(i = 1, . . . , Nh) a side of each triangle T ∈Th. Here Nh denotes the number of
all sides ei. Let Nh0 be the number of all internal sides ei and we suppose that
each ei is numbered so that ei ̸⊆∂Ω holds for i = 1, . . . , Nh0 and ei ⊊∂Ω for
i = Nh0 + 1, . . . , Nh. For each side ei (i = 1, . . . , Nh), let Mi be the midpoint of
ei. Deﬁne by Xh0 the nonconforming P1 FE space over Th:
Xh0 :=
⎧
⎨
⎩ϕ ∈L2






ϕ is piecewise linear over the triangulation Th,
continuous on each midpoint Mi for i = 1, . . . , Nh
and ϕ(Mi) = 0 for Nh0 < i ≤Nh
⎫
⎬
⎭.
We call ϕ ∈Xh0 a nonconforming P1 FE function. We denote by {ϕi}Nh0
i=1 ⊊Xh0
the basis functions of Xh0 with ϕj(Mi) = δij for i, j = 1, . . . , Nh0, where δij is
the Kronecker delta.
Remark 1. Xh0 is not a subspace of H1
0 because ϕ ∈Xh0 is discontinuous on
some edges ei in general. Each ϕ vanishes only on all the boundary midpoints
{Mi | i > Nh0}, and thus ϕ does not satify zero Dirichlet boundary condition
exactly.
We introduce an inner product and a norm for u, v ∈Xh0 + H1
0(Ω) as follows.
(u, v)h :=

T ∈Th
(∇u, ∇v)L2(T ) , ∥u∥2
h := (u, u)h .

272
T. Uda
Note that, if u, v ∈H1
0(Ω), this inner product (u, v)h coincides with (∇u, ∇v)L2.
It is well-known that (Xh0 + H1
0, ( · , · )h) is a Hilbert space. For details, see
Theorem 4.2.4 of [2].
We now deﬁne the nonconforming P1 interpolator Πnc
h
: H1
0 →Xh0 so that

ei
u ds =

ei
(Πnc
h u)|T ds
(3)
holds for all functions u ∈H1
0, each side ei (i = 1, . . . , Nh0) and each associated
triangle T ∈Th with ei ⊊∂T. Equation (3) uniquely determines the FE function
Πnc
h u ∈Xh0 and Πnc
h is thus well-deﬁned. A constructive error estimate for the
nonconforming P1 interpolator Πnc
h has been given by Liu [6].
Proposition 2 (Liu ’09). Let CP(Th) be the maximum of the optimal Poincar´e
constant on T ∈Th that is,
CP(Th) := max
T ∈Th sup

∥v −v∥L2
∥∇v∥L2





 v ∈H1(T) \ {0} , v := 1
|T|

T
v dx dy

.
Then, for all u ∈H2(Ω) ∩H1
0(Ω), the following inequality holds.
∥u −Πnc
h u∥h ≤CP(Th) |u|H2 .
Note
that
the
optimal
Poincar´e
constant
on
T
is
O(diam(T))
where
diam(T) := supx,y∈T |x −y| is the diameter of T, from which it follows CP(Th) =
O(h). We can see an exact value of CP(Th) in [6].
In fact, Πnc
h coincides with the orthogonal projection Ph from Xh0 +H1
0 onto
Xh0. Let u ∈Xh0 + H1
0. Owing to Gauss-Green Theorem and (3), we have
(u −Πnc
h u, ϕi)h =

T

−(u −Πnc
h u, △ϕi)L2 +

∂T
(u −Πnc
h u)∂ϕi
∂ν ds

= 0 ,
where ν denotes the outward unit normal vector on ∂T. Since this holds for all
basis functions ϕi, we obtain Ph = Πnc
h . The orthogonal space of Xh0 in the
Hilbert space (Xh0 + H1
0, ( · , · )h) is represented by X⊥
h0 := (I −Ph)

H1
0

.
3.2
Extended Problem
In Nakao’s theory, the conforming P1 FE space, which is a subspace of H1
0(Ω),
plays an important role in verifying the existence of solutions to (2). However,
the theory is not applicable directly to the ﬁxed-point problem with the non-
conforming P1 FE space Xh0, since it is not a subspace of H1
0. We thus consider
the following extended ﬁxed-point problem on the sum space Xh0 + H1
0(Ω) for
ﬁxed h > 0 under the following Assumption 2.
Assumption 2. There exists a continuous and bounded map fh : Xh0 +H1
0 →
L2 such that fh is an extended map of f, that is, fh|H1
0 = f.

Numerical Veriﬁcation for Elliptic Boundary Value Problem
273
Problem 3 (Extended Fixed-point Problem). Find u ∈Xh0 + H1
0 such that
u = −△−1 fh(u) ≡F(u) .
(4)
Remark 2. If f is of the form f(u) = g(u, ∇u) with a continuous map g : R3 →
R, then we always have the natural extended map fh satisfying Assumption 2.
For this, we just replace ∇u by ∇hu where ∇h denotes the discrete diﬀeren-
cial operator ∇h : Xh0 + H1
0 →L2 with the property (∇hu)|T = ∇(u|T ) for
any T ∈Th.
Remark 3. This extended ﬁxed-point problem (4) is equivalent to (2), because,
if there exists a solution u ∈Xh0+H1
0 to (4), u is an element of D(△) = H2∩H1
0
and thus (2) holds owing to Assumption 2.
Remark 4. The Schauder ﬁxed-point theorem states that if U is a nonempty
bounded closed convex subset of a Banach space X and F
: U →U is a
continuous compact map, then F has a ﬁxed point. In (4), F = −△−1 fh :
Xh0 + H1
0 →Xh0 + H1
0 is a compact map, which is the assumption of the
Schauder ﬁxed-point theorem. Thus, in order to show the existence of a ﬁxed-
point to (4), it is suﬃcient to prove that there exists a certain bounded, convex
and closed subset U ⊊Xh0 + H1
0 that includes its image F(U).
3.3
Candidate Set
In validated numerical computations, one needs to introduce a computable set in
order to derive suﬃcient conditions that satisfy the assumption of the Schauder
ﬁxed-point thorem. Let IR denote the set of all bounded closed intervals in
R, that is, IR := {[x, x] | x, x ∈R , x ≤x}. We identify an interval vector x =
(x i)N
i=1 ∈IRN with a closed set N
i=1 x i ⊊RN. In this paper, letters denoting
an interval and an interval vector are boldface.
Deﬁnition 1 (Candidate Set). For uh ∈Xh0, v ∈IRNh0 and α > 0, deﬁne
a bounded, convex and closed subset U = uh + Uh + U∗⊊Xh0 + H1
0 such that
Uh =
Nh0

i=1
uiϕi





 (ui) ∈D−1v

, U∗=

u∗∈X⊥
h0|∥u∗∥h ≤α

,
(5)
where Dij := (ϕj, ϕi)h is the Nh0 × Nh0 matrix. We call U a candidate set.
We usually take a numerical solution obtained by nonconforming P1 FEM as uh.
Then, the candidate set U is a closed neighborhood of the approximate solution
in which we expect the existence of the exact solution. On the other hand, v and
α are iteration parameters in our method. We practically choose them suﬃciently
small at the ﬁrst iteration.
Using the orthogonal decomposition of Xh0 + H1
0, we get the following two
suﬃcient conditions for the existence of a ﬁxed-point (recall Ramark 4).

274
T. Uda
(I −Ph)F(U) ⊆U∗,
(A)
PhF(U) ⊆uh + Uh .
(B)
Note that (A) and (B) are inclusion relations in the inﬁnite dimensional space
X⊥
h0 and the ﬁnite dimensional space Xh0, respectively. Since it is not possible
to verify these conditions directly in computers, we derive computable suﬃcient
conditions instead of the conditions (A) and (B).
3.4
Suﬃcient Condition for Inﬁnite-Dimensional Part
The following theorem gives a suﬃcient condition for (A), which allows us to
verify (A) numerically rigorously in computers.
Theorem 1 (Suﬃcient Condition for (A)). The condition (A) holds, if
CP(Th) sup
u∈U
∥fh(u)∥L2 ≤α .
(6)
We can prove Theorem 1 in the same way as in [7].
Proof. Assume CP(Th) supu∈U∥fh(u)∥L2 ≤α. Let u be an arbitrary element
of U. Then, owing to Propositions 1 and 2 and the assumption, we have
∥(I −Ph)F(u)∥h =
(I −Πnc
h ) △−1 fh(u)

h
≤CP(Th)


△−1 fh(u)


H2 = CP(Th) ∥fh(u)∥L2 ≤α .
Consequently, (I −Ph)F(u) ∈U∗holds.
⊓⊔
Since CP(Th) = O(h), we expect that the inequality (6) holds for a suﬃciently
small mesh size h.
3.5
Suﬃcient Condition for Finite-Dimensional Part
We deﬁne by Ki the support of ϕi. Using the integration by parts, we have, for
each ϕi and all v ∈H2(Ω),

Ki
(△v)ϕi dx dy =

∂Ki
∂v
∂ν ϕi ds −

T ∈Th ,T ⊊Ki

T
∇v · ∇ϕi|T dx dy .
(7)
Note that, in general, the boundary integral in the right hand side of (7) does
not vanish, since ϕi is discontinuous on ∂Ki. We thus deﬁne a continuous linear
operator b( · ; ϕi) : H2 ∩H1
0 →R by
b(v; ϕi) :=

∂Ki
∂v
∂ν ϕi ds ,
with which we can rewrite (7) as follows.
(△v, ϕi)L2 = b(v; ϕi) −(v, ϕi)h .
(8)
Now the following theorem yields a computable suﬃcient condition for (B).

Numerical Veriﬁcation for Elliptic Boundary Value Problem
275
Theorem 2 (Suﬃcient
Condition
for
(B)).
Take
an
interval
vector
d := (di) ∈IRNh0 such that

(fh(u), ϕi)L2 −(uh, ϕi)h −b

△−1 fh(u) ; ϕi
 

 u ∈U

⊆di ,
(9)
for each i = 1, . . . , Nh0. Then, the condition (B) holds, if
d ⊆v .
(10)
We also prove Theorem 2 in the same way as is done in [7], but owing to the
discontinuity of ϕi, the formula (9) is slightly diﬀerent from the one in [7]. We
remark that the suﬃcient condition (10) and the one in [7] are also diﬀerent,
because the ﬁnite-dimensional part (5) in our candidate set is modiﬁed from the
original one.
Proof. Assume d ⊆v. Let u be an arbitrary element of the candidate set U.
Then, by the deﬁnition of Ph, we get −Ph △−1 f(u) = uh+vh for some vh ∈Xh0.
It is suﬃcient to show vh ∈Uh for any u ∈U. We set by v := (vi) the coeﬃcient
vector of vh, that is, vh = Nh0
i=1 viϕi. Then, owing to (8) and (9), we have
(vh, ϕi)h =

−Ph △−1 f(u) −uh, ϕi

h =

−△−1 f(u), ϕi

h −(uh, ϕi)h
= (f(u), ϕi)L2 −b

△−1 f(u) ; ϕi

−(uh, ϕi)h ∈d i ,
which means Dv ∈d holds. Therefore, it follows from the assumption that
Dv ∈v h. Consequently, owing to (5), we conclude vh ∈Uh.
⊓⊔
3.6
Estimates for Boundary Integrals
In (9), we have the boundary integral term b(w; ϕi) with w := △−1 fh(u). Hence,
we still have a diﬃculty in computing the left hand side of (9) on computers
rigorously, since the condition (9) contains the term b

△−1 fh(u) ; ϕi

with the
inverse operator △−1. Instead, we get O(h) estimates for |b(w; ϕi)| so that we
can replace the set {b(w; ϕi)} with an interval of O(h) width.
Lemma 1 (Dupont et al. ’79). Let K be a convex polygon in R2. Denote by
P1(K) the set of all polynomial functions on K of at most 1st degree. Deﬁne the
optimal Poincar´e constant CP(K) as follows.
CP(K) := sup

∥w∥L2
∥∇w∥L2





 w ∈H1(K) \ {0} ,

K
w dx dy = 0

.
Then, there exists a positive constant CBH(K) such that, for every w ∈H2(K),
inf
p∈P1(K)∥w + p∥H2(K) ≤CBH(K) |w|H2(K) .
(11)

276
T. Uda
x
y
O
h
h
2h
2h
1
1
N
(a)
ˆx
ˆy
O
1
1
ˆ
K0
(b)
ˆx
ˆy
O
1
1
−1
ˆ
K1
(c)
Fig. 1. (a) Triangulation Th, (b) ˆK0 and (c) ˆK1
Inequality (11) is well-known as Bramble-Hilbert Lemma, but the constructive
proof for Lemma 1 has been given by Dupont et al. [3, ’79].
For simplicity, we consider the case Ω := (0, 1)2 and the triangulation Th
as shown in Fig. 1. (a). We divide the interval (0, 1) into N sections both in
x-axis and in y-axis. We assume for h = 1/N. With this triangulation Th,
the support Ki of ϕi is similar to either a rectangle ˆK0 := [0, 1]2 or a paral-
lelogram ˆK1 := {0 ≤x + y ≤1 , 0 ≤y ≤1}. See Figs. 1. (b) and (c). We intro-
duce an aﬃne map Φi : Ki
1:1
−→
ˆKk(i) that induces the change of variable
(ˆx, ˆy) = Φi(x, y). Here, we choose k(i) as 0 or 1 so that Ki and ˆKk(i) are similar
to each other. Let ˆw := w ◦Φ−1
i
∈H2 ˆKk(i)

and ˆϕk(i) := ϕi ◦Φ−1
i
∈H1 ˆKk(i)

.
Since the basis function ˆϕk depends only on k and not on i, we omit i and denote
k = k(i) later.
By using the change of variable Φi, we can rewrite the boundary integral
term b(w; ϕi) as follows.
b(w; ϕi) =

∂Ki
∂w
∂ν ϕi ds =

∂ˆ
Kk
∂ˆw
∂ˆν ˆϕk dˆs =: βk( ˆw) ,
where βk : H2( ˆKk) →R becomes a continuous linear operator on H2( ˆKk). Now,
we prove the following constructive estimate for the boundary integral term.
Theorem 3. For all w ∈H2(Ω) ∩H1
0(Ω), the following inequality holds.
|b(w; ϕi)| ≤CBH( ˆKk) h
√
2∥ˆϕk∥H1( ˆ
Kk) ∥△w∥L2(Ω) .
Proof. First, we evaluate the operator norm of βk. Owing to Gauss-Green the-
orem, Cauchy-Schwarz inequality and Proposition 1, we have
|βk( ˆw)| =





∂ˆ
Kk
∂ˆw
∂ˆν ˆϕk dˆs




 =





ˆ
Kk
∇ˆw · ∇ˆϕk dˆx dˆy +

ˆ
Kk
(△ˆw) ˆϕk dˆx dˆy




≤∥∇ˆw∥L2 ∥∇ˆϕk∥L2 + ∥△ˆw∥L2 ∥ˆϕk∥L2
≤

(∥∇ˆw∥2
L2 + ∥△ˆw∥2
L2)(∥∇ˆϕk∥2
L2 + ∥ˆϕk∥2
L2)
1/2
≤
√
2∥ˆw∥H2( ˆ
Kk) ∥ˆϕk∥H1( ˆ
Kk) .

Numerical Veriﬁcation for Elliptic Boundary Value Problem
277
Hence, we get the following inequality for the operator norm of βk.
∥βk∥≤
√
2∥ˆϕk∥H1( ˆ
Kk) .
(12)
Second, we apply Bramble-Hilbert Lemma 1 to ˆw. Take a polynomial function
p ∈P1( ˆKk) arbitrarily. Then, βk(p) = 0 holds, because the normal derivative of
p is constant and ˆϕk is linear on each side of the quadrilateral ∂ˆKk. Thus, owing
to Lemma 1, we have
|βk( ˆw)| =
inf
p∈P1( ˆ
Kk)|βk( ˆw −p)| ≤
inf
p∈P1( ˆ
Kk)∥βk∥∥ˆw −p∥H2
≤CBH( ˆKk) ∥βk∥| ˆw|H2( ˆ
Kk) ≤CBH( ˆKk) ∥βk∥h|w|H2(Ω) .
(13)
At last, applying Proposition 1 and combining inequalities (12) and (13), we
complete the proof.
⊓⊔
4
Numerical Results
We show a numerical result to conﬁrm our proposed veriﬁcation method works
well. Let f : H1
0 →L2 be the following map.
f(u) := 0.001π2u + 1.999 sin(πx) sin(πy) .
(14)
Then, u = π−2 sin(πx) sin(πy) is a solution to (1). The function f satisﬁes
Assumption 1. Furthermore, we extend the domain of deﬁnition of f to Xh0 +H1
0
with the same deﬁnition (14). Thus, f also satisﬁes Assumption 2. Note that inter-
val arithmetic allows us to deal with parametric uncertainty of a PDE, which is
discussed by Nakao [7]. We omit the detail since our case is similar to [7].
Table 1. Numerical veriﬁcation by Nakao’s method with mesh size h = 1/8
# of iterations Candidate set U
Another candidate set V
V ⊆U
maxi |v i|
α
maxi |d i|
β
1
0.00525252 0.1
0.0132067 0.123234
2
0.013867
0.173234 0.013209
0.123246
OK
Table 2. Numerical veriﬁcation by proposed method with mesh size h = 1/8
# of iterations Candidate set U
Another candidate set V
V ⊆U
maxi |v i|
α
maxi |d i| β
1
0.00101012 0.02
0.89045
0.0795599
2
0.899354
0.0895599 0.91856
0.0820849
3
0.927745
0.0920849 0.919445
0.0821645
OK

278
T. Uda
0.1
1
10
1
10
100
N
h
maxi |vi|
N
0.01
0.1
1
1
10
100
N
h
α
N
Fig. 2. Asymptotic behaviors of a-posteriori errors maxi|v i| and α with respect to N
We show numerical results by Nakao’s method and by our proposed method
in Tables 1 and 2, respectively. The 2nd and 3rd columns are the size of the
candidate set U. Here |v i| denotes max {|x| | x ∈v i}. The 4th and 5th columns
represent another candidate set V where d i (resp. β) is computed by L.H.S. of
(9) (resp. (6)). The 6th column veriﬁes (6) and (10). We consider maxi|v i| and
α as a-posteriori errors for an FE approximate solution uh, that is, these values
in the last row represent the diﬀerence between the numerical solution and the
exact solution. We see the a-posteriori error maxi|v i| in Table 2 is larger than
one in Table 1. This is because the boundary integral term b( · ; ϕi) is enclosed
by an interval with a relatively large width in proposed method. In Fig. 2, we
also show asymptotic behaviors of the a-posteriori errors by proposed method
as the mesh size h tends to 0. We observe both maxi|v i| and α decrease in O(h).
5
Summary
We successfully generalize Nakao’s method for the nonconforming P1 FEM. The
key idea is formulating the BVP as a ﬁxed-point problem on Xh0 + H1
0(Ω).
Owing to the discontinuity of the nonconforming P1 FE function ϕi, a boundary
integral term b( · ; ϕi) arises in our veriﬁcation conditions, which makes it diﬃcult
to obtain a candidate set numerically satisfying the assumption of the Schauder
ﬁxed-point theorem. In order to resolve this diﬃculty, we get an O(h) estimate
for b( · ; ϕi) and thus we replace the boundary integral term with an interval of
O(h) width. Finally, we show a numerical test to conﬁrm that our veriﬁcation
method works well.
There are several future works related to our result. A numerical veriﬁcation
for the planar linear elasticity problem (or other elliptic BVPs from engineering),
which may cause a locking eﬀect, should be considered so that we compare a-
posteriori errors by Nakao’s method and by our method. In order for this, a
possible extension of our result is, for example, using the FEs proposed by Lee
et al. [5]. There is a disadvantage in our method which should be improved

Numerical Veriﬁcation for Elliptic Boundary Value Problem
279
in future that the boundary integral term is enclosed by an interval with a
relatively large width. Thus, other veriﬁcation approaches than [7] also should
be considered.
References
1. Babuˇska, I., Suri, M.: On locking and robustness in the ﬁnite element method. SIAM
J. Numer. Anal. 29(5), 1261–1293 (1992)
2. Ciarlet, P.G.: The Finite Element Method for Elliptic Problems. Society for Indus-
trial and Applied Mathematics, Philadelphia (2002)
3. Dupont, T., Scott, L.: Polynomial approximation of functionals in sobolev spaces.
Math. Comput. 34, 441–463 (1979)
4. Grisvard, P.: Elliptic Problems in Nonsmooth Domains. Classics in Applied Math-
ematics. SIAM, Boston (1985)
5. Lee, C.O., Lee, J., Sheen, D.: A locking-free nonconforming ﬁnite element method
for planar linear elasticity. Adv. Comput. Math. 19, 277–291 (2003)
6. Liu, X.: Analysis of error constants for linear conforming and nonconforming ﬁnite
elements. Ph.D. thesis, University of Tokyo, March 2009
7. Nakao, M.T.: A numerical approach to the proof of existence of solutions for elliptic
problems. Jpn J. Appl. Math. 5(2), 313–332 (1988)

Bibliography of Prof. Walter Kr¨amer
Werner Hofschuster1, Evgenija D. Popova2, and Ralph Baker Kearfott3
1 Faculty of Mathematics and Natural Sciences, Bergische Universit¨at Wuppertal,
Gaußstr. 20, 42097 Wuppertal, Germany
hofschuster@math.uni-wuppertal.de
2 Institute of Mathematics and Informatics, Bulgarian Academy of Sciences,
Acad. G. Bonchev str., block 8, 1113 Soﬁa, Bulgaria
epopova@bio.bas.bg
3 Department of Mathematics, University of Louisiana,
Lafayette, LA 70504-1010, USA
rbk@louisiana.edu
Abstract. This is a bibliography of prof. Walter Kr¨amer’s (1952–2014)
work.
Keywords: Bibliography · Enclosure methods · Mathematical software ·
Result veriﬁcation · Standard and special functions · Multiple/arbitrary
precision computations · High performance veriﬁed computing · Com-
puter algebra systems · C-XSC
Walter Kr¨amer (1952–2014) studied mathematics and computer science at the
University of Karlsruhe. After graduating, he was a faculty assistant and research
assistant at the Institute of Applied Mathematics, University of Karlsruhe. He
received his PhD in 1987 and habilitated in 1993. During 1993–1999, Prof. Dr.
Kr¨amer was managing director and visiting professor at the Institute for Sci-
entiﬁc Computing and Mathematical Modelling at the University of Karlsruhe.
From the winter semester 1999/2000 until his death, he was a full professor of
Scientiﬁc Computing/Software Engineering in the Department of Mathematics,
University of Wuppertal.
Professor Kr¨amer’s research was on the development of numerical methods
with result veriﬁcation and software tools supporting them. A major focus of the
activities of W. Kr¨amer and his research group at the University of Wuppertal
was on the maintenance and further development of the open-source program-
ming language extension C-XSC.
Here we present a tentatively complete bibliography of professor Kr¨amer’s
work. The electronic version of this bibliography contains ﬁelds not listed in the
printed version, including cross-references, ISBN/ISSN, keywords, some abstracts,
more hyperlinks (when electronic version of the article is available), etc.
The BibTeX source is available from
http://www2.math.uni-wuppertal.de/wrswt/pubs-WKraemer.bib
and
http://interval.louisiana.edu/Kraemer-bib/pubs-WKraemer.bib.
c
⃝Springer International Publishing Switzerland 2016
M. Nehmeier et al. (Eds.): SCAN 2014, LNCS 9553, pp. 281–290, 2016.
DOI: 10.1007/978-3-319-31769-4

282
W. Hofschuster et al.
References
1. Bohlender, G., Gr¨uner, K., Kaucher, E., Klatte, R., Kr¨amer, W., Kulisch, U.,
Miranker, W.L., Rump, S.M., Ullrich, C., Wolﬀvon Gudenberg, J.: Pascal-SC: a
Pascal for contemporary scientiﬁc computation. Research Report RC 9009, IBM
Thomas J. Watson Research Center, Yorktown Heights, New York (1981)
2. Bohlender, G., B¨ohm, H., Gr¨uner, K., Kaucher, E., Klatte, R., Kr¨amer, W.,
Kulisch, U., Miranker, W.L., Rump, S.M., Ullrich, C., Wolﬀvon Gudenberg,
J.: Matrix Pascal. Res. Rep. RC 9577, IBM Thomas J. Watson Research Center,
Yorktown Heights, New York (1982) Published in [5]
3. Bohlender, G., B¨ohm, H., Gr¨uner, K., Kaucher, E., Klatte, R., Kr¨amer, W.,
Kulisch, U., Rump, S.M., Ullrich, C., Wolﬀvon Gudenberg, J.: Proposal for arith-
metic speciﬁcation in Fortran 8x. Interner Bericht Des Instituts F. Angew. Math.,
Universit¨at Karlsruhe, Karlsruhe, Germany (1982)
4. Bohlender, G., B¨ohm, H., Braune, K., Gr¨uner, K., Kaucher, E., Klatte, R.,
Kr¨amer, W., Kulisch, U., Miranker, W.L., Ullrich, C., Wolﬀvon Gudenberg, J.:
Application module: scientiﬁc-computation for Fortran 8x. Modiﬁed proposal for
arithmetic speciﬁcation according to guidelines of the X3J3-meetings in Tulsa and
Chapel Hill. Interner bericht, Instituts F. Angew. Math. Universit¨at Karlsruhe,
Karlsruhe, Germany (1983)
5. Bohlender, G., B¨ohm, H., Gr¨uner, K., Kaucher, E., Klatte, R., Kr¨amer, W.,
Kulisch, U., Miranker, W.L., Rump, S.M., Ullrich, C., Wolﬀvon Gudenberg,
J.: Matrix Pascal. In: Kulisch, U.W., Miranker, W.L. (eds.) A new approach
to scientiﬁc computation. Notes and Reports in Computer Science and Applied
Mathematics, vol. 7, pp. 311–384. Academic Press, New York (1983) see also [2]
6. Bohlender, G., B¨ohm, H., Gr¨uner, K., Kaucher, E., Klatte, R., Kr¨amer, W.,
Kulisch, U., Miranker, W.L., Rump, S.M., Ullrich, C., Wolﬀvon Gudenberg,
J.: Arithmetic speciﬁcation in Fortran 8x. In: Ford, B., Rault, J.C., Thomasset,
F. (eds.) Tools, Methods and Languages For Scientiﬁc and Engineering Compu-
tation, Amsterdam, The Netherlands, pp. 213–243. Elsevier Science Publishers,
North Holland (1984)
7. Kr¨amer,
W.:
Dokumentation
und
Fehlerabsch¨atzungen
der
hochgenauen
ACRITH (Intervall-) Standardfunktionen (inverse Funktionen), 220p. Booklet,
IBM Deutschland (1984)
8. Braune, K., Kr¨amer, W.: Standard functions for intervals with maximum accu-
racy. In: Proceedings of 11th IMACS Congress on System Simulation and Scien-
tiﬁc Computation, Oslo, Norway, August 5–9, 1985, vol. I, pp. 167–170 (1985) see
also [9]
9. Braune, K., Kr¨amer, W.: High-accuracy standard functions for intervals. In:
Ruschitzka, M. (ed.) Computer Systems: Performance and Simulation, pp. 341–
347. Elsevier Science Publishers B.V. (North-Holland) (1986) see also [8]
10. Kr¨amer, W.: Fehlerabsch¨atzungen der hochgenauen komplexen ACRITH Inter-
vallstandardfunktionen (inverse Funktionen). Booklet, IBM Deutschland (1986)
11. Braune, K., Kr¨amer, W.: High-accuracy standard functions for real and com-
plex intervals. In: Kaucher, E., Kulisch, U., Ullrich, C. (eds.) Computer Arith-
metic: Scientiﬁc Computation and Programming Languages, pp. 81–114. Teubner,
Stuttgart (1987)
12. Kr¨amer, W.: Inverse Standardfunktionen f¨ur reelle und komplexe Intervallargu-
mente mit a priori Fehlerabsch¨atzungen f¨ur beliebige Datenformate. Dissertation,
Universit¨at Karlsruhe, Karlsruhe, Germany (1987)

Bibliography of Prof. Walter Kr¨amer
283
13. Kr¨amer, W.: Mehrfachgenaue reelle und intervallm¨aßige staggered-correction
Arithmetik mit zugeh¨origen Standardfunktionen, pp. 1–80. Bericht, Inst. f.
Angew. Mathematik, Universit¨at Karlsruhe (1988)
14. Kr¨amer, W.: Inverse standard functions for real and complex point and interval
arguments with dynamic accuracy. In: Kulisch, U., Stetter, H.J. (eds.) Scien-
tiﬁc Computation with Automatic Result Veriﬁcation. Computing Supplemen-
tum, vol. 6, pp. 185–212. Springer, Vienna (1988)
15. Cordes, D., Kr¨amer, W.: Vom Problem zum Einschließungsalgorithmus. In:
Kulisch, U. (ed.) Wissenschaftliches Rechnen mit Ergebnisveriﬁkation, pp. 167–
181. Vieweg, Braunschweig (1989)
16. Kr¨amer, W., Walter, W.V.: FORTRAN-SC: a FORTRAN extension for engineer-
ing/scientiﬁc computation with access to ACRITH: General information notes and
sample programs, pp. 1–51. Booklet, IBM Deutschland GmbH, Stuttgart (1989)
17. Kr¨amer, W.: Fehlerschranken f¨ur h¨auﬁg auftretende Approximationsausdr¨ucke.
Z. Angew. Math. Mech. 69(4), t44–t47 (1989)
18. Kr¨amer, W.: Berechnung der Gammafunktion Γ(x) f¨ur reelle Punkt- und Inter-
vallargumente. Z. Angew. Math. Mech. 70(6), T581–T584 (1990)
19. Kr¨amer, W.: Highly accurate evaluations of program parts with applications.
In: Ullrich, C. (ed.) Contributions to Computer Arithmetic and Self-validating
Numerical Methods. IMACS Annals on Computing and Applied Mathematics,
vol. 7, pp. 397–410. J.C. Baltzer AG, Basel (1990)
20. Kr¨amer, W.: Einschluß eines Paares konjugiert komplexer Nullstellen eines reellen
Polynoms. Z. Angew. Math. Mech. 71(6), T820–T824 (1991)
21. Kr¨amer, W.: Genaue Berechnung von komplexen Polynomen in mehreren Vari-
ablen. Wissenschaftliche Zeitschrift der Technischen Hochschule Leipzig Heft 9,
pp. 401–407 (1991)
22. Kr¨amer, W.: Die Berechnung von Standardfunktionen in Rechenanlagen. In:
Chatterji, S., Kulisch, U., Laugwitz, D., Liedl, R., Purkert, W. (eds.) Jahrbuch
¨Uberblicke Mathematik, pp. 97–115. Vieweg, Braunschweig (1992)
23. Kr¨amer, W.: Computation of interval bounds for elliptic integrals. In: Herzberger,
J., Atanassova, L. (eds.) Computer Arithmetic and Enclosure Methods (Olden-
burg, 1991), pp. 289–298. North-Holland, Amsterdam (1992)
24. Kr¨amer, W.: Evaluation of polynomials in several variables with high accuracy.
In: Kaucher, E., Markov, S., Mayer, G. (eds.) Computer Arithmetic, Scientiﬁc
Computation and Mathematical Modelling. IMACS Annals on Computing and
Applied Mathematics, vol. 12, pp. 239–249. Baltzer, Basel (1991)
25. Kr¨amer, W.: Veriﬁed solution of eigenvalue problems with sparse matrices. In:
Brezinski, C., Kulisch, U. (eds.) Computational and Applied Mathematics, I.
Algorithms and Theory, pp. 277–287. North-Holland, Amsterdam (1992)
26. Kr¨amer, W.: Die Berechnung von Funktionen und Konstanten in Rechenanlagen.
Habilitationsschrift, University of Karlsruhe, Karlsruhe, Germany (1993)
27. Kr¨amer, W., Barth, B.: Computation of interval bounds for Weierstrass’ elliptic
function ℘(z). In: Albrecht, R., Alefeld, G., Stetter, H. (eds.) Validation Numer-
ics: Theory and Applications. Computing Supplementum, vol. 9, pp. 147–159.
Springer, Wien (1993)
28. Kr¨amer, W.: Eine portable Langzahl- und Langzahlintervallarithmetik mit
Anwendungen. Z. angew. Math. Mech. 73(7–8), T849–T853 (1993)
29. Kr¨amer, W.: Multiple-precision computations with result veriﬁcation. In: Adams,
E., Kulisch, U. (eds.) Scientiﬁc Computing with Automatic Result Veriﬁcation.
Mathematics in Science and Engineering, vol. 189, pp. 325–356. Academic Press
Inc., Boston (1993)

284
W. Hofschuster et al.
30. Kr¨amer, W.: Numerische Berechnung von Schranken f¨ur π. In: Chatterji, S.,
Fuchssteiner, B., Kulisch, U., Liedl, R. (eds.) Jahrbuch ¨Uberblicke Mathematik,
pp. 57–72. Vieweg, Braunschweig (1993)
31. Bohlender, G., Kr¨amer, W., Miranker, W.L.: Grading of basic arithmetical opera-
tions and functions. Technical Report RC 19593 (86059), IBM Research Division,
T. J. Watson Research Center (1994)
32. Kr¨amer, W.: Bericht ¨uber die Begutachtung des IWRMM im Dezember 1993.
Preprint IWRMM 94/3, IWRMM, University of Karlsruhe, Karlsruhe, Germany
(1994)
33. Kr¨amer, W., Kulisch, U., Lohner, R.: Numerical Toolbox for Veriﬁed Computing
II — Advanced Numerical Problems. unpublished draft, Karlsruhe (1994)
34. Kr¨amer, W.: Schranken f¨ur Werte der Weierstraßschen ℘-Funktion. Z. angew.
Math. Mech. 74(6), t660–t662 (1994)
35. Kr¨amer, W.: Grundlagen des veriﬁzierten numerischen Rechnens (Bases of veriﬁed
computing). Z. angew. Math. Mech. 75(Suppl 2), s425–s428 (1995)
36. Kr¨amer, W.: Validated function evaluation using polynomial approximation from
truncated Chebyshev series. Reliab. Comput. Supplementum, 113–113 (1995)
37. Hofschuster, W., Kr¨amer, W.: Ein rechnergest¨utzter Fehlerkalk¨ul mit Anwendung
auf ein genaues Tabellenverfahren, 35p. Preprint IWRMM 96/5, IWRMM, Uni-
versity of Karlsruhe, Karlsruhe, Germany (1996)
38. Kr¨amer, W., Wedner, S.: Computing narrow inclusions for Cauchy principal value
integrals. In: Alefeld, G., Frommer, A., Lang, B. (eds.) Scientiﬁc Computing and
Validated Numerics, pp. 45–51. Mathematical Research, Akademie Verlag, Berlin
(1996)
39. Kr¨amer, W.: Sichere und genaue Absch¨atzung des Approximationsfehlers bei
rationalen Approximationen. Bericht 3/1996, Forschungsschwerpunkt Comput-
erarithmetik, Intervallrechnung und Numerische Algorithmen mit Ergebnisveri-
ﬁkation (CAVN), Karlsruhe, Germany (1996)
40. Kr¨amer, W., Wedner, S.: Two adaptive Gauss-Legendre type algorithms for the
veriﬁed computation of deﬁnite integrals. Reliab. Comput. 2(3), 241–253 (1996)
41. Kr¨amer, W., Niethammer, W., (eds.): Tagungsband zum Workshop Wis-
senschaftliches Rechnen in den Ingenieurwissenschaften, Februar 1996, Karlsruhe.
Preprint IWRMM 96/6, IWRMM, University of Karlsruhe, Karlsruhe, Germany
(1996)
42. Niethammer, W., Kr¨amer, W. (eds.): Wissenschaftliches Rechnen in der Univer-
sit¨at Karlsruhe. Universit¨at Karlsruhe (1996)
43. Blomquist, F., Kr¨amer, W.: Algorithmen mit garantierten Fehlerschranken f¨ur die
Fehler- und die komplement¨are Fehlerfunktion. Preprint IWRMM 97/3, IWRMM,
University of Karlsruhe, Karlsruhe, Germany (1997)
44. Hofschuster, W., Kr¨amer, W.: A computer oriented approach to get sharp reliable
error bounds. Reliab. Comput. 3(3), 239–248 (1997)
45. Hofschuster, W., Kr¨amer, W.: A fast public domain interval library in ANSI C.
In: Sydow, A. (ed.) Proceedings of the 15th IMACS World Congress on Scientiﬁc
Computation, Modelling and Applied Mathematics, vol 2, pp. 395–400 (1997)
46. Kr¨amer, W.: A priori worst-case error bounds for ﬂoating-point computations.
In: Lang, T., Muller, J.M., Takagi, N. (eds.) 13th IEEE Symposium on Computer
Arithmetic: Proceedings, July 6–9, 1997, Asilomar, California, USA. Symposium
on Computer Arithmetic, vol. 13, pp. 64–73. IEEE Computer Society Press (1997)
See also [52]

Bibliography of Prof. Walter Kr¨amer
285
47. Kr¨amer, W.: Eine Fehlerfaktorarithmetik f¨ur zuverl¨assige a priori Fehlerab-
sch¨atzungen. Bericht 5/1997, Forschungsschwerpunkt Computerarithmetik, Inter-
vallrechnung und Numerische Algorithmen mit Ergebnisveriﬁkation (CAVN),
Karlsruhe, Germany (1997)
48. Kr¨amer, W.: Eﬀective range computations for rational functions. In: Sydow, A.,
(ed.) Proceedings of the 15th IMACS World Congress on Scientiﬁc Computation,
Modelling and Applied Mathematics, vol. 2, pp. 421–426 (1997)
49. Bantle, A., Kr¨amer, W.: Ein Kalk¨ul f¨ur verl¨aßliche absolute und relative Fehlera-
bsch¨atzungen. Preprint IWRMM 98/5, IWRMM, University of Karlsruhe, Karl-
sruhe, Germany (1998)
50. Hofschuster, W., Kr¨amer, W.: FI LIB, eine schnelle und portable Funktionsbib-
liothek f¨ur reelle Argumente und reelle Intervalle im IEEE-double-Format, 227p.
Preprint IWRMM 98/7, IWRMM, University of Karlsruhe, Karlsruhe, Germany
(1998)
51. Kr¨amer, W.: Automatisierte a-priori-Fehlerabsch¨atzung bei gleitkommam¨aßiger
Ausdrucksauswertung. Z. angew. Math. Mech. 78(Suppl 3), 979–980 (1998)
52. Kr¨amer, W.: A priori worst case error bounds for ﬂoating-point computations.
IEEE Trans. Comput. 47(7), 750–756 (1998) see also [46]
53. Kr¨amer, W.: Constructive error analysis. J.UCS 4(2), 147–163 (1998)
54. Bantle, A., Kr¨amer, W.: Implementierung, Handhabung und Beispielanwendun-
gen eines verl¨asslichen Vorw¨artsfehlerkalk¨uls. Bericht 2/1999, Forschungsschwer-
punkt Computerarithmetik, Intervallrechnung und Numerische Algorithmen mit
Ergebnisveriﬁkation, Karlsruhe, Germany (1999)
55. Br¨auer, M., Kr¨amer, W.: R¨uckw¨artsmethode zur automatischen Berechnung von
worst-case Fehlerschranken. Bericht 3/1999, Forschungsschwerpunkt Computer-
arithmetik, Intervallrechnung und Numerische Algorithmen mit Ergebnisveriﬁka-
tion (CAVN), Karlsruhe, Germany (1999)
56. B¨udding, G., Fausten, D., Kr¨amer, W., M¨ollers, T., Traczinski, H.: Veriﬁka-
tion von L¨osungen ausgew¨ahlter Probleme aus der Modellierung von Manip-
ulatoren. Schriftenreihe des Fachbereichs Mathematik, Universit¨at Duisburg,
Gesamthochschule, 463 (1999)
57. Geulig, I., Kr¨amer, W.: Intervallrechnung in Maple — Die Erweiterung intpakX
zum Paket intpak der Share-Library. Report, IWRMM, University of Karlsruhe,
Karlsruhe, Germany (1999)
58. Kr¨amer, W.: Gleichm¨aßige (Rundungs-)Fehlerschranken f¨ur Gleitkommaalgorith-
men ¨uber Datenbereichen. Z. angew. Math. Mech. 79(Suppl 1), 245–246 (1999)
59. Kr¨amer, W.: Modiﬁkationen eines Satzes von Ehlich/Zeller zur Wertebereichsbes-
timmung bei rationalen Funktionen. Z. angew. Math. Mech. 79(Suppl 3), s865–
s866 (1999)
60. Bantle, A., Kr¨amer, W.: Automatische Bestimmung von relativen worst case
Fehlerschranken. Z. angew. Math. Mech. 80, S819–S820 (2000)
61. Hofschuster, W., Kr¨amer, W.: Mathematical function software on the web –
are such codes useful for veriﬁcation algorithms? Reliab. Comput. 6(2), 207–218
(2000)
62. Hofschuster, W., Kr¨amer, W.: Rechnerisches Nachvollziehen und Analyse der
praktischen Umsetzung des Finanzausgleichsgesetzes. Preprint BUW-WRSWT
2000/1, Bergische Universit¨at Wuppertal, Wuppertal, Germany (2000)
63. Kaucher, E., Kr¨amer, W.: L¨osungseinschließung bei unendlichen linearen Gle-
ichungssystemen. Z. angew. Math. Mech. 80(Suppl 3), S823–S824 (2000)

286
W. Hofschuster et al.
64. Kr¨amer, W., Blomquist, F.: Algorithms with guaranteed error bounds for the
error function and the complementary error function. Preprint BUW-WRSWT
2000/2, Bergische Universit¨at Wuppertal, Wuppertal, Germany (2000)
65. Br¨auer, M., Hofschuster, W., Kr¨amer, W.: Steigungsarithmetiken in C-XSC.
Preprint BUW-WRSWT 2001/3, Bergische Universit¨at Wuppertal, Wuppertal,
Germany (2001)
66. Hofschuster, W., Kr¨amer, W., Wedner, S., Wiethoﬀ, A.: C-XSC 2.0 — C++
class library for extended scientiﬁc computing. Preprint BUW-WRSWT 2001/1,
Bergische Universit¨at Wuppertal, Wuppertal, Germany (2001)
67. Kr¨amer, W., Bantle, A.: Automatic forward error analysis for ﬂoating point algo-
rithms. Reliab. Comput. 7(4), 321–340 (2001)
68. Kr¨amer, W., Geulig, I.: Interval calculus in Maple — the extension intpakX to the
package intpak of the share-library. Preprint BUW-WRSWT 2001/2, Bergische
Universit¨at Wuppertal, Wuppertal, Germany (2001)
69. Kr¨amer,
W.,
Wolﬀ
von
Gudenberg,
J.
(eds.):
Scientiﬁc
Comput-
ing,
Validated
Numerics,
Interval
Methods.
Kluwer
Academic/Plenum,
Boston/Dordrecht/London (2001)
70. Lerch, M., Tischler, G., Wolﬀvon Gudenberg, J., Hofschuster, W., Kr¨amer, W.:
The interval library ﬁlib++ 2.0 – design, features and sample programs. Preprint
BUW-WRSWT 2001/4, Bergische Universit¨at Wuppertal, Wuppertal, Germany
(2001)
71. Hofschuster, W., Kr¨amer, W.: C-XSC 2.0 — a C++ class library for extended sci-
entiﬁc computing. Preprint BUW-WRSWT 2002/4, Bergische Universit¨at Wup-
pertal, Wuppertal, Germany (2002)
72. Kr¨amer, W.: Advanced software tools for validated computing. In: Proceedings
of the 31st Spring Conference of the Union of Bulgarian Mathematicians, pp.
344–355 (2002)
73. Miehe, D., Kr¨amer, W., Hofschuster, W.: Visualization of resulting sets coming
from multiplication and division of complex intervals. Preprint BUW-WRSWT
2002/3, Bergische Universit¨at Wuppertal, Wuppertal, Germany (2002)
74. H¨olbig, C.A., Kr¨amer, W.: Selfverifying solvers for dense systems of linear equa-
tions realized in C-XSC. Preprint BUW-WRSWT 2003/1, Bergische Universit¨at
Wuppertal, Wuppertal, Germany (2003)
75. Kr¨amer, W., Wolﬀvon Gudenberg, J.: Extended interval power function. Reliab.
Comput. 9(5), 339–347 (2003)
76. Popova, E.D., Kr¨amer, W.: Parametric ﬁxed-point iteration implemented in C-
XSC. Preprint BUW-WRSWT 2003/3, Bergische Universit¨at Wuppertal, Wup-
pertal, Germany (2003)
77. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Realisierung der hyperbolischen
Cotangens-Funktion in einer Staggered Correction Intervallarithmetik in C-XSC.
Preprint BUW-WRSWT 2004/3, Bergische Universit¨at Wuppertal, Wuppertal,
Germany (2004)
78. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Sichere a priori Absch¨atzungen und
Realisierung der Funktion
√
x2 −1 in C-XSC. Preprint BUW-WRSWT 2004/5,
Bergische Universit¨at Wuppertal, Wuppertal, Germany (2004)
79. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Sichere a priori Fehlerabsch¨atzung
und Implementierung der Funktion zweier Variabler log(

x2 + y2). Preprint
BUW-WRSWT 2004/1, Bergische Universit¨at Wuppertal, Wuppertal, Germany
(2004)

Bibliography of Prof. Walter Kr¨amer
287
80. Hofschuster, W., Kr¨amer, W.: C-XSC 2.0 — a C++ library for extended scientiﬁc
computing. In: Alt, R., Frommer, A., Kearfott, R., Luther, W. (eds.) Numerical
Software with Result Veriﬁcation. LNCS, vol. 2991, pp. 15–35. Springer, Berlin
(2004)
81. H¨olbig, C.A., Kr¨amer, W., Diverio, T.A.: An accurate an eﬃcient selfverifying
solver for systems with banded coeﬃcient matrix. In: Joubert, G.R., Nagel, W.E.,
Peters, F.J., Walter, W.V. (eds.) Parallel Computing: Software Technology, Algo-
rithms, Architectures and Applications, PARCO 2003. Advances in Parallel Com-
puting, vol. 13, pp. 283–290. Elsevier, Amsterdam (2004)
82. Kr¨amer, W., Popova, E.D.: Zur Berechnung von verl¨asslichen Außen- und
Inneneinschließungen bei parameterabh¨angigen linearen Gleichungssystemen.
Proc. Appl. Math. Mech. (PAMM) 4(1), 670–671 (2004)
83. Blomquist, F., Kr¨amer, W.: Computing a priori error bounds for ﬂoating-point
evaluations of arithmetic expressions. Preprint BUW-WRSWT 2006/1, Bergische
Universit¨at Wuppertal, Wuppertal, Germany (2006)
84. Blomquist, F., Hofschuster, W., Kr¨amer, W., Neher, M.: Complex interval func-
tions in C-XSC. Preprint BUW-WRSWT 2005/2, Bergische Universit¨at Wupper-
tal, Wuppertal, Germany (2005)
85. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Reliable computation of the complex
interval function arcsin(z) in C-XSC. Preprint BUW-WRSWT 2005/1, Bergische
Universit¨at Wuppertal, Wuppertal, Germany (2005)
86. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Real and complex Taylor arithmetic
in C-XSC. Preprint BUW-WRSWT 2005/4, Bergische Universit¨at Wuppertal,
Wuppertal, Germany (2005)
87. Kr¨amer, W.: Generalized intervals and the dependency problem. Proc. Appl.
Math. Mech. (PAMM) 6(1), 683–684 (2006)
88. Kr¨amer, W.: Pitfalls in Maple. Preprint BUW-WRSWT 2006/7, Bergische Uni-
versit¨at Wuppertal, Wuppertal, Germany (2006)
89. Lerch, M., Tischler, G., Wolﬀvon Gudenberg, J., Hofschuster, W., Kr¨amer, W.:
FILIB++, a fast interval library supporting containment computations. ACM
Trans. Math. Software 32(2), 299–324 (2006)
90. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Vermeidung von ¨Uber- und Unter-
lauf und Verbesserung der Genauigkeit bei reeller und komplexer staggered
Intervall-Arithmetik. Preprint BUW-WRSWT 2007/8, Bergische Universit¨at
Wuppertal, Wuppertal, Germany (2007)
91. Grimmer, M., Kr¨amer, W.: An MPI extension for veriﬁed numerical computa-
tions in parallel environments. In: Arabnia, H.R., Yang, J.Y., Yang, M.Q. (eds.)
Proceedings of the 2007 International Conference on Scientiﬁc Computing, CSC
2007, June 25–28, 2007, Las Vegas, Nevada, USA, pp. 111–117. CSREA Press
(2007)
92. Kr¨amer, W.: Accurate computation of chaotic dynamical systems. In: Aggarwal,
A. (ed.) Proceedings of the 8th WSEAS International Conference on Mathematics
and Computers in Biology and Chemistry, MCBC 2007, pp. 74–79. WSEAS (2007)
93. Kr¨amer, W.: Bugs, errors, and unexpected results in computer algebra packages.
Proc. Appl. Math. Mech. (PAMM) 7(1), 2140009–2140010 (2007)
94. Kr¨amer, W.: Computing and visualizing solution sets of interval linear systems.
Serdica J. Comput. 1(4), 455–468 (2007)
95. Kr¨amer, W.: intpakX — an interval arithmetic package for Maple. In: Interna-
tional Symposium on Scientiﬁc Computing, Computer Arithmetic and Validated
Numerics, 2006. SCAN 2006. 12th GAMM - IMACS, IEEE Computer Society
Press (2007) (Article number 4402417)

288
W. Hofschuster et al.
96. Kr¨amer, W.: Introduction to the Maple power tool intpakX. Serdica J. Comput.
1(4), 469–504 (2007)
97. Popova, E.D., Kr¨amer, W.: Inner and outer bounds for the solution set of para-
metric linear systems. J. Comput. Appl. Math. 199(2), 310–316 (2007)
98. Blomquist, F., Hofschuster, W., Kr¨amer, W.: A modiﬁed staggered correction
arithmetic with enhanced accuracy and very wide exponent range. In: Cuyt,
A.A.M., Kr¨amer, W., Luther, W., Markstein, P.W. (eds.) Numerical Validation
in Current Hardware Architectures. LNCS, vol. 8021, pp. 41–67. Springer, Berlin
(2008) see also [107]
99. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Real and complex staggered (inter-
val) arithmetics with wide exponent range (in German). Preprint BUW-WRSWT
2008/1, Bergische Universit¨at Wuppertal, Wuppertal, Germany (2008)
100. Cuyt, A.A.M., Kr¨amer, W., Luther, W., Markstein, P.W.: Summary — numeri-
cal validation in current hardware architectures. In: Cuyt, A.A.M., Kr¨amer, W.,
Luther, W., Markstein, P.W. (eds.) Numerical Validation in Current Hardware
Architectures. LNCS, vol. 08021. Springer, Berlin (2008) see also [108]
101. Grimmer, M., Kr¨amer, W.: An open source parallel interval solver for systems
of linear Fredholm integral equations of the second kind. In: Arabnia, H.R. (ed.)
Proceedings of the 2008 International Conference on Scientiﬁc Computing, CSC
2008, July 14–17, 2008, Las Vegas, Nevada, USA, pp. 204–210. CSREA Press
(2008)
102. Hofschuster, W., Kr¨amer, W., Neher, M.: C-XSC and closely related software
packages. In: Cuyt, A., Kr¨amer, W., Luther, W., Markstein, P. (eds.) Numerical
Validation in Current Hardware Architectures. LNCS, vol. 8021. Springer, Berlin
(2008) see also [109]
103. Kolberg, M., Kr¨amer, W., Zimmer, M.: A note on solving problem 7 of the SIAM
100-digit challenge using C-XSC. In: Cuyt, A., Kr¨amer, W., Luther, W., Mark-
stein, P. (eds.) Numerical Validation in Current Hardware Architectures. LNCS,
vol. 8021. Springer, Berlin (2008) see also [110]
104. Luther, W., Cuyt, A.A.M., Kr¨amer, W., Markstein, P.W.: Abstracts collec-
tion - numerical validation in current hardware architectures. In: Cuyt, A.A.M.,
Kr¨amer, W., Luther, W., Markstein, P.W. (eds.) Numerical Validation in Current
Hardware Architectures. LNCS, vol. 8021. Springer, Berlin (2008) see also [108]
105. Popova, E.D., Kr¨amer, W.: Visualizing parametric solution sets. BIT 48(1), 95–
115 (2008)
106. Zimmer, M., Kr¨amer, W.: Fast (parallel) dense linear interval systems solvers in C-
XSC using error free transformations and BLAS. In: Cuyt, A.A.M., Kr¨amer, W.,
Luther, W., Markstein, P.W. (eds.) Numerical Validation in Current Hardware
Architectures. LNCS, vol. 8021. Springer, Berlin (2008) see also [111]
107. Blomquist, F., Hofschuster, W., Kr¨amer, W.: A modiﬁed staggered correction
arithmetic with enhanced accuracy and very wide exponent range. In: Cuyt,
A.A.M., Kr¨amer, W., Luther, W., Markstein, P.W. (eds.) Numerical Validation
in Current Hardware Architectures.. LNCS, vol. 5492, pp. 41–67. Springer, Berlin
(2008) see also [98]
108. Cuyt, A., Kr¨amer, W., Luther, W., Markstein, P. (eds.): Numerical Validation
in Current Hardware Architectures. LNCS, vol. 5492. Springer, Berlin (2009) see
[100, 104]
109. Hofschuster, W., Kr¨amer, W., Neher, M.: C-XSC and closely related software
packages. In: Cuyt, A.A.M., Kr¨amer, W., Luther, W., Markstein, P.W. (eds.)
Numerical Validation in Current Hardware Architectures. LNCS, vol. 5492, pp.
68–102. Springer, Berlin (2009) see also [102]

Bibliography of Prof. Walter Kr¨amer
289
110. Kolberg, M.L., Kr¨amer, W., Zimmer, M.: A note on solving problem 7 of the SIAM
100-digit challenge using C-XSC. In: Cuyt, A.A.M., Kr¨amer, W., Luther, W.,
Markstein, P.W. (eds.) Numerical Validation in Current Hardware Architectures.
LNCS, vol. 5492, pp. 250–261. Springer, Berlin (2009) see also [103]
111. Kr¨amer, W., Zimmer, M.: Fast (parallel) dense linear system solvers in C-XSC
using error free transformations and BLAS. In: Cuyt, A.A.M., Kr¨amer, W.,
Luther, W., Markstein, P.W. (eds.) Numerical Validation in Current Hardware
Architectures. LNCS, vol. 5492, pp. 230–249. Springer, Berlin (2009) see also [106]
112. Kr¨amer, W.: Computer-assisted proofs and symbolic computations. Serdica J.
Comput. 4(1), 73–84 (2010)
113. Kr¨amer, W.: High performance veriﬁed computing using C-XSC. In: Reiser,
R.H.S., Pilla, M.L. (eds.) IntMath-TSD: Interval Mathematics and Connections in
Teaching and Scientiﬁc Development, pp. 3–14. Universidade Federal de Pelotas,
Ed. Universitria (2010)
114. Kr¨amer, W.: Veriﬁcation methods and symbolic computations. Albanian J. Math.
4(4), 123–133 (2010)
115. Popova, E.D., Kr¨amer, W.: Communicating functional expressions from Mathe-
matica to C-XSC. In: Fukuda, K., Hoeven, J.v.d., Joswig, M., Takayama, N. (eds.)
Mathematical Software, ICMS 2010. LNCS, vol. 6327, pp. 354–365. Springer,
Berlin (2010)
116. Popova, E.D., Kr¨amer, W., Russev, M.: Integration of C-XSC automatic diﬀeren-
tiation in Mathematica. Preprint BUW-WRSWT 2010/1, Bergische Universit¨at
Wuppertal, Wuppertal, Germany (2010)
117. Popova, E.D., Kolev, L., Kr¨amer, W.: A solver for complex-valued parametric
linear systems. Serdica J. Comput. 4(1), 123–132 (2010)
118. Zimmer, M., Kr¨amer, W., Bohlender, G., Hofschuster, W.: Extension of the C-
XSC library with scalar products with selectable accuracy. Serdica J. Comput.
4(3), 349–370 (2010)
119. Zimmer, M., Kr¨amer, W., Hofschuster, W.: Sparse matrices and vectors in C-XSC.
Reliab. Comput. 14(1), 138–160 (2010)
120. Blomquist, F., Hofschuster, W., Kr¨amer, W.: C-XSC-Langzahlarithmetiken f¨ur
reelle und komplexe Intervalle basierend auf den Bibliotheken MPFR und MPFI.
Preprint BUW-WRSWT 2011/1, Bergische Universit¨at Wuppertal, Wuppertal,
Germany (2011)
121. Kolberg, M., Kr¨amer, W., Zimmer, M.: Eﬃcient parallel solvers for large dense
systems of linear interval equations. Reliab. Comput. 15(3), 193–206 (2011)
122. Kr¨amer, W.: C-XSC: a powerful environment for reliable computations in the
natural and engineering sciences. In: Ding, Y., et al. (ed.) 2011 4th International
Conference on Biomedical Engineering and Informatics (BMEI), vol. 4, pp. 2130–
2134. IEEE (2011)
123. Kr¨amer, W.: C-XSC, a sophisticated environment for reliable computing. In:
Ratschan, S. (ed.) Proceedings of the 4-th International Conference on Mathemat-
ical Aspects of Computer and Information Sciences (MACIS 2011), pp. 115–125
(2011)
124. Popova, E.D., Kr¨amer, W.: Characterization of AE solution sets to a class of
parametric linear systems. C. R. Acad. Bulgare Sci. 64(3), 325–332 (2011)
125. Popova, E.D., Kr¨amer, W.: Embedding C-XSC nonlinear solvers in Mathematica.
C. R. Acad. Bulgare Sci. 64(1), 11–20 (2011)
126. Blomquist, F., Hofschuster, W., Kr¨amer, W.: Umfangreiche C-XSCLangzahlpakete
f¨ur beliebig genaue reelle und komplexe Intervallrechnung. Preprint BUW-
WRSWT 2012/2, Bergische Universit¨at Wuppertal, Wuppertal, Germany (2012)

290
W. Hofschuster et al.
127. Kr¨amer, W., Blomquist, F.: Arbitrary precision complex interval computations
in C-XSC. In: Wyrzykowski, R., Dongarra, J., Karczewski, K., Wasniewski, J.
(eds.) Parallel Processing and Applied Mathematics. LNCS, vol. 7204, pp. 457–
466. Springer, Berlin (2012)
128. Kr¨amer, W.: Multiple/arbitrary precision interval computations in C-XSC. Com-
puting 94(2–4), 229–241 (2012)
129. Kr¨amer, W., Zimmer, M., Hofschuster, W.: Using C-XSC for high performance
veriﬁed computing. In: J´onasson, K. (ed.) Applied Parallel and Scientiﬁc Com-
puting. LNCS, vol. 7134, pp. 168–178. Springer, Berlin (2012)
130. Zimmer, M., Kr¨amer, W., Popova, E.D.: Solvers for the veriﬁed solution of para-
metric linear systems. Computing 94(2–4), 109–123 (2012)
131. Kr¨amer, W.: High performance veriﬁed computing using C-XSC. Comput. Appl.
Math. 32(3), 385–400 (2013)
132. Zimmer, M., Rebner, G., Kr¨amer, W.: An overview of C-XSC as a tool for interval
arithmetic and its application in computing veriﬁed uncertain probabilistic models
under Dempster-Shafer theory. Soft Comput. 17(8), 1453–1465 (2013)

Author Index
Anguelov, Roumen
3
Aschemann, Harald
77, 236, 247
Auer, Ekaterina
247
Behnke, Henning
259
Chabert, Gilles
209
Chohra, Chemseddine
99
Collange, Sylvain
126
Defour, David
126
Dobronets, Boris
43
Garloff, Jürgen
171
Golodov, Valentin
201
Graillat, Stef
126
Hamadneh, Tareq
171
Hartman, David
109
Hladík, Milan
109, 116
Hofschuster, Werner
281
Iakymchuk, Roman
126
Ibnseddik, Mohamed Saad
209
Jaulin, Luc
209
Kearfott, Ralph Baker
281
Kinoshita, Takehiko
225
Kosheleva, Olga
54
Kreinovich, Vladik
54, 66
Kupriianova, Olga
14
Langlois, Philippe
99
Lauter, Christoph
14
Le Doze, Vincent
209
Le Menec, Stéphane
209
Longpré, Luc
54
Lopez, Daniel
209
Lorkowski, Joe
66
Markov, Svetoslav
3
Nakao, Mitsuhiro T.
225
Nataraj, P.S.V.
180
Ninin, Jordan
209
Parello, David
99
Patil, Bhagyesh V.
180
Popova, Evgenija D.
138, 281
Popova, Olga
43
Pryce, John
23
Rauh, Andreas
77, 236, 247
Senkel, Luise
77, 236
Sharaya, Irene A.
148
Shary, Sergey P.
148
Stancu, Alexandru
209
Uda, Tomoki
269
Watanabe, Yoshitaka
225
Westphal, Ramona
247


Ángela I. Barbero · Vitaly Skachek
Øyvind Ytrehus (Eds.)
 123
LNCS 10495
5th International Castle Meeting, ICMCTA 2017
Vihula, Estonia, August 28–31, 2017
Proceedings
Coding Theory
and Applications

Lecture Notes in Computer Science
10495
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407

Ángela I. Barbero
• Vitaly Skachek
Øyvind Ytrehus (Eds.)
Coding Theory
and Applications
5th International Castle Meeting, ICMCTA 2017
Vihula, Estonia, August 28–31, 2017
Proceedings
123

Editors
Ángela I. Barbero
University of Valladolid
Valladolid
Spain
Vitaly Skachek
University of Tartu
Tartu
Estonia
Øyvind Ytrehus
Simula@UiB and University of Bergen
Bergen
Norway
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-66277-0
ISBN 978-3-319-66278-7
(eBook)
DOI 10.1007/978-3-319-66278-7
Library of Congress Control Number: 2017951310
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
It is a pleasure to welcome you to the 5th International Castle Meeting on Coding
Theory and Applications (5ICMCTA), held in Vihula Manor, Estonia, during the
period of August 28–31, 2017. This volume contains the extended abstracts of the 24
submitted papers that were accepted for presentation at 5ICMCTA.
Previous workshops in the ICMCTA series were organized in La Mota Castle,
Medina del Campo, Spain (1999 and 2008), Parador de Cardona, Spain (2011), and
Pousada de Palmela, Portugal (2014). In the spirit of ICMCTA, we aim to bring
together, in a beautiful and historical environment, young researchers as well as leading
experts in order to present and discuss problems and results at the research frontier,
related to the theoretical issues and practical applications of coding and information
theory, and to identify new challenges and foster new collaborations. 5ICMCTA is
highly international: the submitted papers were co-authored by researchers from about
20 countries, and when we include the program committees, people from almost 30
countries are involved.
The program contains presentations on relevant research areas in modern Coding
Theory, including codes and combinatorial structures, algebraic geometric codes, group
codes, convolutional codes, network coding, other applications to communications, and
applications of coding theory in cryptography. All papers have passed through a
thorough review process, which assures the high quality of the program.
In addition to the submitted talks, we are proud to present talks by our distinguished
invited speakers: Gérard Cohen, Tor Helleseth, Camilla Hollanti, Raquel Pinto, and
Paul Siegel.
The 5ICMCTA is being organized by the University of Tartu. In particular we wish
to acknowledge the efforts of Yauhen Yakimenka, Anneli Vainumäe, Eva Pruusapuu,
and Raquel Pinto.
We are very grateful to:
• the authors and participants who make up the program and who, we are convinced,
will make 5ICMCTA a memorable event,
• the Scientiﬁc Committee and the sub-reviewers who performed excellently during
the reviewing process,
• Easychair.org who provided the tools for efﬁcient submission, management, and
reviewing,
• The
Norwegian-Estonian
research
cooperation
programme
and
the
project
EMP133, which facilitated the organization of 5ICMCTA,
• Springer’s LNCS for publishing this volume of extended abstracts,
• Springer’s Cryptography and Communications for agreeing to publish a special
volume of selected papers after 5ICMCTA,

• University of Tartu ASTRA project PER ASPERA Doctoral School of Information
and Communication Technologies for providing funding for the student workshop.
We wish you a wonderful Castle Meeting!
July 2017
Ángela Barbero
Vitaly Skachek
Øyvind Ytrehus
VI
Preface

Organization
General Chair
Vitaly Skachek
Institute of Computer Science, University of Tartu, Estonia
Scientiﬁc Committee
Co-chairs
Ángela I. Barbero
Department of Applied Mathematics, Universidad de
Valladolid, Spain
Øyvind Ytrehus
Simula@UiB and University of Bergen, Norway
Alexander Barg
Irina E. Bocharova
Eimear Byrne
Joan-Josep Climent
Gérard Cohen
Olav Geil
Marcus Greferath
Tor Helleseth
Tom Høholdt
Camilla Hollanti
Kees A. Schouhamer Immink
Frank R. Kschischang
Boris D. Kudryashov
San Ling
Daniel E. Lucani
Gary McGuire
Sihem Mesnager
Muriel Médard
Diego Napp
Frédérique E. Oggier
Patric R.J. Östergård
Raquel Pinto
Josep Rifà
Paula Rocha
Joachim Rosenthal
Eirik Rosnes
Moshe Schwartz
Vladimir Sidorenko
Patrick Sole
Leo Storme
Rüdiger Urbanke
Mercè Villanueva
Pascal O. Vontobel
Dejan Vukobratovic
Jos Weber
Gilles Zémor
Administration
Anneli Vainumäe
Eva Pruusapuu
Publicity
Yauhen Yakimenka

Contents
New Bounds for Linear Codes of Covering Radius 2 . . . . . . . . . . . . . . . . .
1
Daniele Bartoli, Alexander A. Davydov, Massimo Giulietti,
Stefano Marcugini, and Fernanda Pambianco
Multidimensional Decoding Networks for Trapping Set Analysis . . . . . . . . .
11
Allison Beemer and Christine A. Kelley
Erasure Correction and Locality of Hypergraph Codes. . . . . . . . . . . . . . . . .
21
Allison Beemer, Carolyn Mayer, and Christine A. Kelley
Reed-Muller Codes: Information Sets from Defining Sets. . . . . . . . . . . . . . .
30
José Joaquín Bernal and Juan Jacobo Simón
Distance Properties of Short LDPC Codes and Their Impact on the BP,
ML and Near-ML Decoding Performance. . . . . . . . . . . . . . . . . . . . . . . . . .
48
Irina E. Bocharova, Boris D. Kudryashov, Vitaly Skachek,
and Yauhen Yakimenka
Decoding a Perturbed Sequence Generated by an LFSR. . . . . . . . . . . . . . . .
62
Sara D. Cardell, Joan-Josep Climent, and Alicia Roca
A Construction of Orbit Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
Joan-Josep Climent, Verónica Requena, and Xaro Soler-Escrivà
Analysis of Two Tracing Traitor Schemes via Coding Theory . . . . . . . . . . .
84
Elena Egorova and Grigory Kabatiansky
Reliable Communication Across Parallel Asynchronous Channels
with Glitches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Shlomo Engelberg and Osnat Keren
On the Kernel of Z2s-Linear Hadamard Codes . . . . . . . . . . . . . . . . . . . . . .
107
Cristina Fernández-Córdoba, Carlos Vela, and Mercè Villanueva
Random Network Coding over Composite Fields . . . . . . . . . . . . . . . . . . . .
118
Olav Geil and Daniel E. Lucani
Bounding the Minimum Distance of Affine Variety Codes
Using Symbolic Computations of Footprints. . . . . . . . . . . . . . . . . . . . . . . .
128
Olav Geil and Ferruh Özbudak

On Binary Matroid Minors and Applications to Data Storage
over Small Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
Matthias Grezet, Ragnar Freij-Hollanti, Thomas Westerbäck,
and Camilla Hollanti
Absorbing Set Analysis of Codes from Affine Planes . . . . . . . . . . . . . . . . .
154
Kathryn Haymaker
Asymptotic Bounds for the Sizes of Constant Dimension Codes
and an Improved Lower Bound. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
Daniel Heinlein and Sascha Kurz
On Quasi-Abelian Complementary Dual Codes. . . . . . . . . . . . . . . . . . . . . .
192
Somphong Jitman, Herbert S. Palines, and Romar B. dela Cruz
Relative Generalized Hamming Weights and Extended Weight
Polynomials of Almost Affine Codes. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Trygve Johnsen and Hugues Verdure
On the Performance of Block Woven Codes Constructions
with Row-Wise Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
Alexey Kreshchuk, Igor Zhilin, and Victor Zyablov
New Lower Bounds on Error-Correcting Ternary, Quaternary
and Quinary Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
Antti Laaksonen and Patric R.J. Östergård
A State Space Approach to Periodic Convolutional Codes . . . . . . . . . . . . . .
238
Diego Napp, Ricardo Pereira, and Paula Rocha
Column Rank Distances of Rank Metric Convolutional Codes . . . . . . . . . . .
248
Diego Napp, Raquel Pinto, Joachim Rosenthal, and Filipa Santana
On Minimality of ISO Representation of Basic 2D Convolutional Codes . . . .
257
Raquel Pinto and Rita Simões
A New Construction of Minimum Distance Robust Codes . . . . . . . . . . . . . .
272
Hila Rabii and Osnat Keren
Constructions and Bounds for Batch Codes with Small Parameters . . . . . . . .
283
Eldho K. Thomas and Vitaly Skachek
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
X
Contents

New Bounds for Linear Codes
of Covering Radius 2
Daniele Bartoli1, Alexander A. Davydov2(B), Massimo Giulietti1,
Stefano Marcugini1, and Fernanda Pambianco1
1 Department of Mathematics and Computer Science,
Perugia University, Perugia, Italy
{daniele.bartoli,massimo.giulietti,
stefano.marcugini,fernanda.pambianco}@unipg.it
2 Institute for Information Transmission Problems (Kharkevich Institute),
Russian Academy of Sciences, Moscow, Russian Federation
adav@iitp.ru
Abstract. The length function ℓq(r, R) is the smallest length of a q-ary
linear code of covering radius R and codimension r. New upper bounds
on ℓq(r, 2) are obtained for odd r ≥3. In particular, using the one-to-one
correspondence between linear codes of covering radius 2 and saturating
sets in the projective planes over ﬁnite ﬁelds, we prove that
ℓq(3, 2) ≤

q(3 ln q + ln ln q) +

q
3 ln q + 3
and then obtain estimations of ℓq(r, 2) for all odd r ≥5. The new upper
bounds are smaller than the previously known ones. Also, the new bounds
hold for all q, not necessary large, whereas the previously best known
estimations are proved only for q large enough.
Keywords: Covering codes · Saturating sets · The length function ·
Upper bounds · Projective spaces
1
Introduction
Let Fq be the Galois ﬁeld with q elements. Let F n
q be the n-dimensional vector
space over Fq. Denote by [n, n−r]q a q-ary linear code of length n and codimen-
sion (redundancy) r, that is, a subspace of F n
q of dimension n −r. The sphere
of radius R with center c in F n
q is the set {v : v ∈F n
q , d(v, c) ≤R} where d(v, c)
is the Hamming distance between vectors v and c.
Deﬁnition 1. (i) The covering radius of a linear [n, n −r]q code is the least
integer R such that the space F n
q is covered by spheres of radius R centered
at codewords.
(ii) A linear [n, n−r]q code has covering radius R if every column of F r
q is equal
to a linear combination of at most R columns of a parity check matrix of the
code, and R is the smallest value with such property.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 1–10, 2017.
DOI: 10.1007/978-3-319-66278-7 1

2
D. Bartoli et al.
Deﬁnition 1(i) and (ii) are equivalent. Let an [n, n−r]qR code be an [n, n−r]q
code with covering radius R. For an introduction to coverings of vector Hamming
spaces over ﬁnite ﬁelds, see [3,4].
The covering density μ of an [n, n −r]qR-code is deﬁned as
μ = 1
qr
R

i=0
(q −1)i
n
i

≥1.
The covering quality of a code is better if its covering density is smaller. For
ﬁxed q, r, and R the covering density of an [n, n −r]qR code decreases with
decreasing n.
Deﬁnition 2 ([3,4]). The length function ℓq(r, R) is the smallest length of a
q-ary linear code with covering radius R and codimension r.
Codes investigated from the point view of the covering quality are usually
called covering codes; see an online bibliography in [13].
In this paper we consider covering codes with radius R = 2.
The known lower bound on ℓq(r, 2), based on Deﬁnition 1(ii), is
ℓq(r, 2) >
√
2q(r−2)/2.
(1)
Really, in a parity check matrix of an [n, n −r]q2 code, one can take
n
2

dis-
tinct pair of columns and then form q2 linear combinations from every pair. By
Deﬁnition 1(ii), it holds that
n
2

q2 ≥qr whence (1) follows.
For arbitrary q, covering codes of length close to this lower bound are known
only for r even [5,7,9,10]. In particular, the following bounds are obtained by
algebraic constructions [7, Sect. 4.3, Eq. (4.6)], [9, Theorem 9]:
ℓq(r, 2) ≤2q(r−2)/2 + q(r−4)/2, q ≥7, q ̸= 9, r = 2t ≥4, t = 2, 3, 5,
and t ≥7.
ℓq(r, 2) ≤2q(r−2)/2 + q(r−4)/2 + q(r−6)/2 + q(r−8)/2, q ≥7, q ̸= 9, r = 8, 12.
If r is odd, covering codes of length close to lower bound (1) are known only
when q is an even power of a prime, i.e. more exactly when q = (q′)2 and q =
(q′)4, where q′ is a prime power, and when q = p6 with prime p ≤73 [5–7,10,12].
In particular, the following bounds are obtained by algebraic constructions, see
[5, Example 6, Eq. (33)], [6], [7, Sect. 4.4, Eqs. (4.12), (4.13), (4.15)], [12], and the
references therein:
ℓq(r, 2) ≤

3 −1
√q

q(r−2)/2 +

q(r−5)/2
, q = (q′)2 ≥16, r = 2t + 1 ≥3.
ℓq(r, 2) ≤

2 + 2
4√q + 2
√q

q(r−2)/2 +

q(r−5)/2
, q = (q′)4, r = 2t + 1 ≥3.
ℓq(r, 2) ≤

2 + 2
6√q + 2
3√q + 2
√q

q(r−2)/2 + 2

q(r−5)/2
, q = (q′)6,
q′ ≤73 prime, r = 2t + 1 ≥3, r ̸= 9, 13.

New Bounds for Linear Codes of Covering Radius 2
3
The goal of this work is to obtain new upper bounds on the length function
ℓq(r, 2) with r odd and arbitrary q, not necessarily having the form q = (q′)2
where q′ is a prime power. It is a hard open problem. The ﬁrst and the most
important step in this problem is ﬁnding of upper bounds on ℓq(3, 2). It is usually
considered as a separate open problem.
Let PG(N, q), N ≥2, be the N-dimensional projective space over the ﬁeld
Fq; see [11] for an introduction to the projective spaces over ﬁnite ﬁelds. Eﬀective
methods obtaining upper bounds on ℓq(r, 2) with r odd, in particular on ℓq(3, 2),
are connected with saturating sets in PG(N, q), N ≥2.
Deﬁnition 3. A point set S ⊂PG(N, q) is saturating if any point of PG(N, q)\S
is collinear with two points in S.
Saturating sets are considered in [5–10,12,14,15], see also the references
therein. In the literature, saturating sets are also called “saturated sets” [5,15],
“spanning sets”, “dense sets”, and “1-saturating sets” [6–8,12].
Let s(N, q) be the smallest size of a saturating set in PG(N, q).
If q-ary positions of a column of an r×n parity check matrix of an [n, n−r]q2
code are treated as homogeneous coordinates of a point in PG(r −1, q) then this
parity check matrix deﬁnes a saturating set of size n in PG(r −1, q) [5–7]. So,
there is the one-to-one correspondence between [n, n −r]q2 codes and saturating
sets in PG(r −1, q). Therefore,
ℓq(r, 2) = s(r −1, q),
in particular, ℓq(3, 2) = s(2, q).
In [1,2], by probabilistic methods the following upper bound is obtained in
the geometrical language.
s(2, q) ≤2
	
(q + 1) ln(q + 1) + 2 ∼2
	
q ln q.
(2)
Also, in [1,2] one can ﬁnd the previous results and the references on this topic.
In [14], the following bound is proved for the projective plane PG(2, q).
s(2, q) ≤(
√
3 + o(1))
	
q ln q.
(3)
The proof of (3) is given in [14] by two approaches: probabilistic and algorithmic.
In both the approaches, starting with some stage of the proof, it is assumed (by
the context) that q is large enough. As the result of the algorithmic proof of [14],
the following form of the bound can be derived.
s(2, q) ≤

	
3q ln q

+
1
2
√q

≤
	
3q ln q + 1
2
√q + 2,
q large enough.
(4)
Note that the ﬁrst steps of the algorithmic proof in [14] do not need q large
enough; this allows us to use these steps in Sect. 2.
Throughout the paper we denote
Υ(q) =
	
3 ln q + ln ln q +

1
3 ln q + 3
√q .
(5)
Our new results are collected in Theorem 4 based on Theorems 7 and 11.

4
D. Bartoli et al.
Theorem 4. Let q be an arbitrary prime power. Let the value of q be not nec-
essarily large. Let r be odd. For the length function ℓq(r, 2) and for the smallest
size s(r−1, q) of a saturating set in the projective space PG(r−1, q) the following
upper bounds hold.
(i)
ℓq(3, 2) = s(2, q) ≤Υ(q) · q(3−2)/2 = Υ(q)√q.
(6)
(ii)
ℓq(r, 2) = s(r −1, q) ≤Υ(q) · q(r−2)/2 + 2q(r−5)/2,
r = 2t + 1 ≥5,
(7)
where r ̸= 9, 13, t = 2, 3, 5, and t ≥7, q ≥19.
ℓq(r, 2) = s(r −1, q) ≤Υ(q) · q(r−2)/2 + 2q(r−5)/2 + q(r−7)/2 + q(r−9)/2,
(8)
where r = 9, 13.
These upper bounds are smaller (i.e. better) than the previously known ones,
see Sect. 4.
The paper is organized as follows. In Sect. 2, a new upper bound on the length
function ℓq(3, 2) is obtained. In Sect. 3, upper bounds on the length function
ℓq(r, 2), r ≥5 odd, are considered on the base of the results of Sect. 2. Finally,
in Sect. 4 we compare the obtained new bounds with the previously known ones.
2
An Upper Bound on the Length Function ℓq(3, 2)
Assume that in PG(2, q) a saturating set is constructed by a step-by-step algo-
rithm adding one new point to the set in every step.
Let i > 0 be an integer. Denote by Si the running set obtained after the i-th
step of the algorithm. A point P of PG(2, q) \ Si is covered by Si if P lies on
a t-secant of Si with t ≥2. Let Ri be the subset of PG(2, q) \ Si consisting of
points not covered by Si.
In [14] the following ingenious greedy algorithm is proposed. One takes the
line ℓskew to Si such that the cardinality of intersection |Ri ∩ℓ| is the minimal
among all skew lines. Then one adds to Si the point on ℓproviding the greatest
number of new covered points (in comparison with other points of ℓ). As a result
we obtain the set Si+1 and the corresponding set Ri+1.
In [14, Proposition 3.3, Proof], the following inequality is proved without
requirement that q is large enough:
|Ri+1| ≤|Ri| ·

1 −i(q −1)
q(q + 1)

.
(9)
The running set S2 contains two points; we consider the line through them.
All points on this line are covered by S2. So, always R2 = (q2+q+1)−(q+1) = q2
where q2 + q + 1 and q + 1 are the number of points in PG(2, q) and in the line,
respectively. Starting from R2 = q2 and iteratively applying the relation (9), we
obtain for some k the following:
|Rk+1| ≤q2fq(k),

New Bounds for Linear Codes of Covering Radius 2
5
where
fq(k) =
k

i=2

1 −i(q −1)
q(q + 1)

.
Now we consider a truncated iterative process. We will stop the iterative
process when |Rk+1| ≤ξ where ξ ≥1 is some value that we may assign arbitrary
to improve estimations.
By [14, Lemma 2.1] after the end of the iterative process we can add at most
⌈|Rk+1|/2⌉points to the running subset Sk+1 in order to get the ﬁnal saturating
set S. Therefore, the size s of the obtained saturating set S is
s ≤k + 1 +
ξ
2

under condition q2fq(k) ≤ξ.
(10)
Using the inequality 1 −x ≤e−x, we obtain that
fq(k) < e
−
k

i=2
i(q−1)/(q2+q)
= e−(k2+k−2)(q−1)/(2q2+2q),
which implies
fq(k) < e−(k2+k−2)(q−1)/(2q2+2q) < e−k2/(2q+2),
(11)
provided that
(k2 + k −2)(q −1)
q
> k2
or, equivalently,
k2
k −2 < q −1,
k < q −4.
(12)
Lemma 5. Let ξ ≥1 be a ﬁxed value independent of k. The value
k ≥

	
2(q + 1)

ln q2
ξ

(13)
satisﬁes inequality q2fq(k) ≤ξ.
Proof. By (11), to provide q2fq(k) ≤ξ it is suﬃcient to ﬁnd k such that
e−k2/(2q+2) < ξ
q2 .
⊓⊔

6
D. Bartoli et al.
Theorem 6. Let q be an arbitrary prime power. In the projective plane PG(2, q)
it holds that
s(2, q) ≤
	
2(q + 1)

ln q2
ξ + ξ
2 + 3,
ξ ≥1,
(14)
where ξ is an arbitrarily chosen value.
Proof. We substitute the value k from (13) to (10). The summand “+3” takes
into account that the size of a saturating set is an integer.
⊓⊔
In order to get a “good” estimation of s(2, q), we are trying to reduce the
right part of (14). For it, let us consider the function of ξ of the form
φ(ξ) =
	
2(q + 1)

ln q2
ξ + ξ
2 + 3.
Its derivative by ξ is
φ′(ξ) = 1
2 −1
ξ

q + 1
2 ln q2
ξ
.
It is easy to check that φ′(1) < 0, φ′(q) > 0, and φ′(ξ) is an increasing function
of ξ. This means that for some value ξ0 > 1 it holds that φ′(ξ0) = 0. Moreover,
for ξ < ξ0, the derivative φ′(ξ) < 0 and φ(ξ) decreases, while for ξ > ξ0, the
derivative is positive and φ(ξ) increases. So, in the point ξ = ξ0 we have the
minimum of φ(ξ). Now we will ﬁnd a value of ξ such that φ′(ξ) is close to 0 and,
in addition, the expression of the results is relatively simple.
Put φ′(ξ) = 0. Then it is easy to see that
ξ2 =
q + 1
ln q −1
2 ln ξ .
(15)
We ﬁnd ξ in the form ξ =

q+1
c ln q. By (15),
c = 1 −ln(q + 1)
4 ln q
+ ln c + ln ln q
4 ln q
.
We choose c ≈1 −ln(q+1)
4 ln q
≈3
4 and put ξ =

4q
3 ln q. The value
φ′
 4q
3 ln q

= 1
2 −1
2

3(q + 1) ln q
q

3 ln q + ln ln q + ln 3
4

is close to zero for growing q. Also, see below, the expression of the results for
such ξ is quite simple.
So, the choice ξ =

4q
3 ln q in (14) seems to be convenient.
Theorem 7. Let q be an arbitrary prime power.

New Bounds for Linear Codes of Covering Radius 2
7
(i) In PG(2, q), there is a saturating set of size ≤Υ(q)√q.
(ii) There exists an [n, n −3]q2 code with n ≤Υ(q)√q.
Proof. (i) We substitute ξ =

4q
3 ln q in (14) and obtain
s(2, q) ≤

(q + 1)

3 ln q + ln ln q + ln 3
4

+

q
3 ln q + 3.
It can be shown (e.g. by considering the corresponding derivatives) that

(q + 1)

3 ln q + ln ln q + ln 3
4

+

q
3 ln q + 3 < Υ(q)√q for q ≥43.
Also, the necessary condition (12) holds as Υ(q)√q < q −4.
So, we have proved that a saturating set of size ≤Υ(q)√q exists in PG(2, q)
for q ≥43.
Now note that in [7, Table 1], the smallest known (up to September 2010) sizes
of saturating sets in PG(2, q), q ≤1217, are given. All these sizes (including
the region q < 43) are smaller than Υ(q)√q.
The assertion (i) is proved.
(ii) The one-to-one correspondence between saturating sets and covering codes,
see Introduction, implies the existence of an [n, n −3]q2 code with n ≤
Υ(q)√q.
⊓⊔
Theorem 7 immediately implies the estimation (6) of Theorem 4(i).
Remark 8. Let ξ = 1. From (14) we have
s(2, q) ≤2
	
(q + 1) ln q + 3,
(16)
that practically coincides with bound (2) from [1,2].
Let ξ = √q. From (14) we obtain the estimation
s(2, q) ≤
	
3(q + 1) ln q + 1
2
√q + 3
(17)
which practically coincides with bound (4) of [14].
However, the value ξ =

4q
3 ln q gives the estimation (6) that is smaller (i.e.
better) than (16) and (17), see Sect. 4.
Remark 9. In fact, the estimations (2) from [1,2], (3) and (4) of [14], and the
new estimation (6), proved in this section, hold in an arbitrary ﬁnite plane of
order q, not necessarily Desarguesian. But in a non-Desarguesian plane we have
not the one-to-one correspondence between [n, n−3]q2 codes and saturating sets.
It is why we consider here only the Desarguesian plane PG(2, q).

8
D. Bartoli et al.
3
Upper Bounds on the Length Function ℓq(r, 2), r ≥5
Odd
For upper bounds on the length function ℓq(r, 2), r ≥5 odd, an important tool
is the inductive construction of [5,7] providing the following code parameters.
Proposition 10 ([5, Example 6] [7, Theorem 4.4]). Let an [nq, nq −3]q2 code
exist. Then the following holds.
(i) Under conditions nq < q and q + 1 ≥2nq, there is an inﬁnite family of
[n, n −r]q2 codes with the parameters
n = nqq(r−3)/2 + 2q(r−5)/2, r = 2t −1 ≥5, r ̸= 9, 13, t = 3, 4, 6,
and t ≥8.
(18)
(ii) Under condition nq < q there is an inﬁnite family of [n, n −r]q2 codes with
n = nqq(r−3)/2 + 2q(r−5)/2 + q(r−7)/2 + q(r−9)/2, r = 9, 13.
(19)
Theorem 11. Let q be an arbitrary prime power. Then there exists an inﬁnite
family of [n, n −r]q2 codes with the parameters
n = Υ(q) · q(r−2)/2 + 2q(r−5)/2,
r = 2t + 1 ≥5, r ̸= 9, 13,
(20)
where t = 2, 3, 5, and t ≥7, q ≥19.
Also there exists an inﬁnite family of [n, n −r]q2 codes with the parameters
n = Υ(q) · q(r−2)/2 + 2q(r−5)/2 + q(r−7)/2 + q(r−9)/2, r = 9, 13.
(21)
Proof. Since Υ(q)√q < q, we may put that the starting [nq, nq −3]q2 code of
Proposition 10 is the [n, n −3]q2 code, n ≤Υ(q)√q, of Theorem 7. It is easy
to check directly that the condition q + 1 ≥2Υ(q)√q holds for q ≥79. Now,
similarly to the proof of Theorem 7, we use the smallest known sizes of saturat-
ing sets in PG(2, q) from [7, Table 1]. For q < 79, these sizes are smaller than
Υ(q)√q and, moreover, for 19 ≤q < 79 they provide the condition q + 1 ≥2nq
for Proposition 10. Now the relations (20) and (21) follow from (18) and (19),
respectively.
⊓⊔
Theorem 11 immediately implies the estimations (7) and (8) of Theorem 4(ii).
4
Comparison with the Previously Known Results
Surveys on the results on non-binary covering codes in [7,10] show that the
inductive approach of Proposition 10 is the main tool to obtain upper bounds on
the length function ℓq(r, 2), r ≥5 odd. Proposition 10 uses the length function
ℓq(3, 2) as the base for inductive estimations. Therefore upper bounds on ℓq(3, 2),
smaller than the known ones, provide bounds on ℓq(2t + 1, 2), 2t + 1 ≥5, that

New Bounds for Linear Codes of Covering Radius 2
9
are less than the corresponding known results. So, in the beginning we should
compare the new bound on ℓq(3, 2), see (6), with the best corresponding known
bound, see (4).
First of all we should emphasize that the new bound (6) holds for all q, not
necessary large, whereas the known bound (4) is proved only for q large enough.
Then we consider the diﬀerence Δ(q) between the bounds (4) and (6) where
Δ(q) =
	
3q ln q + 1
2
√q + 2 −Υ(q)√q.
It can be shown (e.g. by considering the derivatives) that Δ(q) > 0 for q ≥337
and, moreover, Δ(q) and Δ(q)
√q are increasing functions of q. For illustration, see
Fig. 1 where the top curve shows Δ(q) while the bottom one
	
q/7 is given for
comparison.
Fig. 1. The diﬀerence Δ(q) (top dashed-dotted curve) vs

q/7 (bottom solid curve)
Note also that
lim
q→∞
Δ(q)
√q
= lim
q→∞
	
3 ln q + 1
2 −
	
3 ln q + ln ln q −
1
√3 ln q −1
√q

= 1
2.
Finally, if one uses Proposition 10 to estimate ℓq(r, 2), r ≥5 odd, then the
diﬀerence between new and known results will be of order Δ(q)q(r−3)/2. It means
that our improvements for r = 3 directly expand to odd r ≥5.
Acknowledgements. The research of D. Bartoli, M. Giulietti, S. Marcugini, and
F. Pambianco was supported in part by Ministry for Education, University and
Research of Italy (MIUR) (Project “Geometrie di Galois e strutture di incidenza”)
and by the Italian National Group for Algebraic and Geometric Structures and their
Applications (GNSAGA - INDAM). The research of A.A. Davydov was carried out at
the IITP RAS at the expense of the Russian Foundation for Sciences (project 14-50-
00150).

10
D. Bartoli et al.
References
1. Bartoli, D., Davydov, A.A., Giulietti, M., Marcugini, S., Pambianco, F.: On upper
bounds on the smallest size of a saturating set in a projective plane. [math.CO]
(2015). https://arxiv.org/abs/1505.01426
2. Bartoli, D., Davydov, A.A., Giulietti, M., Marcugini, S., Pambianco, F.: New upper
bounds on the smallest size of a saturating set in a projective plane. In: 2016
XV International Symposium Problems of Redundancy in Information and Con-
trol Systems (REDUNDANCY), St. Petersburg, pp. 18–22. IEEE (2016). http://
ieeexplore.ieee.org/document/7779320
3. Brualdi, R.A., Litsyn, S., Pless, V.S.: Covering radius. In: Pless, V.S., Huﬀman,
W.C., Brualdi, R.A. (eds.) Handbook of coding theory, vol. 1, pp. 755–826. Elsevier,
Amsterdam (1998)
4. Cohen, G., Honkala, I., Litsyn, S., Lobstein, A.: Covering Codes. North-Holland
Mathematical Library, vol. 54. Elsevier, Amsterdam (1997)
5. Davydov, A.A.: Constructions and families of nonbinary linear codes with covering
radius 2. IEEE Trans. Inf. Theor. 45(5), 1679–1686 (1999)
6. Davydov, A.A., Giulietti, M., Marcugini, S., Pambianco, F.: Linear covering codes
over nonbinary ﬁnite ﬁelds. In: XI International Workshop on Algebraic and Com-
bintorial Coding Theory (ACCT 2008), Pamporovo, pp. 70–75 (2008). http://www.
moi.math.bas.bg/acct2008/b12.pdf
7. Davydov, A.A., Giulietti, M., Marcugini, S., Pambianco, F.: Linear nonbinary cov-
ering codes and saturating sets in projective spaces. Adv. Math. Commun. 5(1),
119–147 (2011)
8. Davydov, A.A., Marcugini, S., Pambianco, F.: On saturating sets in projective
spaces. J. Comb. Theor. Ser. A 103(1), 1–15 (2003)
9. Davydov, A.A., ¨Osterg˚ard, P.R.J.: Linear codes with covering radius R = 2, 3 and
codimension tR. IEEE Trans. Inf. Theor. 47(1), 416–421 (2001)
10. Giulietti, M.: The geometry of covering codes: small complete caps and saturating
sets in Galois spaces. In: Blackburn, S.R., Holloway, R., Wildon, M. (eds.) Surveys
in Combinatorics 2013. London Mathematical Society Lecture Note Series, vol.
409, pp. 51–90. Cambridge Univ Press, Cambridge (2013)
11. Hirschfeld, J.W.P.: Projective Geometries Over Finite Fields. Oxford Mathematical
Monographs, 2nd edn. Clarendon Press, Oxford (1998)
12. Kiss, G., K´ovacs, I., Kutnar, K., Ruﬀ, J., ˇSparl, P.: A note on a geometric construc-
tion of large cayley graphs of given degree and diameter. Studia Univ. Babes-Bolyai
Math. 54(3), 77–84 (2009)
13. Lobstein, A.: Covering radius, an online bibliography. http://perso.telecom-
paristech.fr/˜lobstein/bib-a-jour.pdf
14. Nagy, Z.L.: Saturating sets in projective planes and hypergraph covers. [math.CO]
(2017). arxiv:1701.01379
15. Ughi, E.: Saturated conﬁgurations of points in projective Galois spaces. Eur. J.
Comb. 8(3), 325–334 (1987)

Multidimensional Decoding Networks
for Trapping Set Analysis
Allison Beemer(B) and Christine A. Kelley
University of Nebraska-Lincoln, Lincoln, NE, USA
allison.beemer@huskers.unl.edu
Abstract. We present a novel multidimensional network model as a
means to analyze decoder failure and characterize trapping sets of graph-
based codes. We identify a special class of these decoding networks, which
we call transitive networks, and show how they may be used to identify
trapping sets and inducing sets. Many codes have transitive decoding
network representations. We conclude by investigating the decoding net-
works of codes arising from product, half-product, and protograph code
constructions.
Keywords: Trapping sets · Iterative decoding · Multidimensional
decoding networks · Finite state machines · LDPC codes · Redundancy
1
Introduction
Failure of message-passing decoders of low-density parity-check (LDPC) codes
has been shown to be characterized by graphical (sub)structures in the code’s
Tanner graph, such as pseudocodewords, absorbing sets, trapping sets, and stop-
ping sets (a subclass of trapping sets) [1–4]. In particular, these structures con-
tribute to persistent error ﬂoors in the Bit Error Rate (BER) curves of these
codes. The presence of such structures naturally depends on the choice of Tan-
ner graph representation used in decoding. However, while absorbing sets are
combinatorially-deﬁned, trapping sets are heavily decoder dependent, making
them more diﬃcult to analyze. Nevertheless, for many practical channels, the
error ﬂoor behavior is dominated by the harmful trapping sets in the graph [3].
In this work, we present a multidimensional network model for analyzing
hard-decision message-passing decoders. The structure of this network is depen-
dent on the code, as well as the choice of Tanner graph representation and
decoder. Thus, our model takes into account all parameters determining the
presence of harmful trapping sets. We show how these decoding networks may be
used to identify trapping sets, and therefore analyze decoder behavior of LDPC
codes, as well as provide insight into the optimal number of iterations to be run
on a given code (and representation) with a chosen decoder. We show that this
analysis is simpliﬁed for networks with a transitivity property, and discuss the
connection between transitive networks and redundancy in their corresponding
parity-check matrices. Finally, we relate the decoding networks of product and
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 11–20, 2017.
DOI: 10.1007/978-3-319-66278-7 2

12
A. Beemer and C.A. Kelley
half-product codes to those of their underlying component codes, and examine
the connection between the decoding networks of a protograph and its lift.
While the results herein present the case of binary linear codes transmitted
over the binary erasure and symmetric channels (BEC and BSC), the deﬁnitions
and results may be extended for linear codes over larger ﬁelds. Moreover, the
decoding network concept and its applications may be extended to other chan-
nels and decoding algorithms. This paper is organized as follows. In Sect. 2, we
provide notation and background. We present multidimensional decoding net-
works for decoder analysis in Sect. 3. In Sect. 4, we deﬁne trapping sets and
describe a method for identifying them using this model. In Sect. 5, we discuss
the connection between redundancy and transitivity, and apply the decoding
network framework to various codes.
2
Preliminaries
In this section we introduce relevant background. Let C be a binary LDPC code of
length n with associated Tanner graph G, to be decoded with some chosen hard-
or soft-decision decoder. Following the notation and deﬁnitions in [5], suppose
that the codeword x is transmitted, and ˆx is received. Let ˆxℓbe the output after
ℓiterations of the decoder are run on G. A node ˆxi, with 1 ≤i ≤n, is said to
be eventually correct if there exists L ∈Z≥0 such that ˆxℓ
i = xi for all ℓ≥L.
Deﬁnition 1. Let T (ˆx) denote the set of variable nodes that are not eventually
correct for a received word ˆx, and let G[T ] denote the subgraph induced by T (ˆx)
and its neighbors in the graph G. If G[T ] has a variable nodes and b odd-degree
check nodes, T (ˆx) is said to be an (a, b)-trapping set. In this case, the set of
variable nodes in error in the received word ˆx is called an inducing set for T (ˆx).
The critical number of a trapping set T , denoted m(T ), is the minimum number
of variable nodes in an inducing set of T .
Note that the critical number may arise from an inducing set not fully con-
tained in T . Moreover, observe that stopping sets [4] are simply trapping sets
over the BEC, all of whose inducing sets contain the stopping set itself. Since a
stopping set is an inducing set for itself, the critical number of a stopping set is
equal to its size.
Due to their complexity, trapping sets are typically analyzed under hard-
decision decoders, such as Gallager A/B [6,7], although interesting work has
also been done on trapping set analysis for soft-decision decoders on the AWGN
channel [8,9]. Simulation results suggest that trapping sets with respect to hard
decision decoders also aﬀect the error ﬂoor performance of soft-decision decoders
[5,10]. Recall that the Gallager A algorithm operates for transmission over the
BSC; messages are calculated at nodes and sent across edges of the associated
Tanner graph. Variable to check (μv→c) and check to variable (μc→v) messages
are calculated as follows:
μv→c(r, m1, . . . , md(v)−1) =

r + 1 if m1 = · · · = md(v)−1 = r + 1
r else,
(1)

Multidimensional Decoding Networks for Trapping Set Analysis
13
where r is the received value at variable node v, dv is the degree of v, and
m1, . . . , md(v)−1 are the most recent messages received from the check nodes
which are not those to which the current message is being calculated. In the
other direction,
μc→v(m1, . . . , md(c)−1) =
d(c)−1

i=1
mi ,
(2)
where here m1, . . . , md(c)−1 are messages received from variable nodes. Note that
calculations are performed in F2. Gallager B relaxes the unanimity condition of
μv→c. Throughout the examples in this work which use Gallager decoding, we
consider Gallager A decoding with the ﬁnal decoder output at a variable node v
given by the received value at v, unless incoming messages μc→v for c adjacent
to v unanimously disagree with this received value. Further results for Gallager
B and more relaxed decoding rules will be treated in a forthcoming work.
3
Multidimensional Network Framework
Suppose we decode a code C using a hard-decision decoder, and consider the
labeled directed graph (digraph) for a ﬁxed ℓ∈Z>0, denoted Dℓ, with vertex
and edge sets V = {x : x ∈S} and E = {(xi, xj, ℓ) : xℓ
i = xj}, respectively,
where S is the set of possible received words, (xi, xj, ℓ) denotes an edge from xi
to xj with edge label ℓ, and xℓ
i is the output of the decoder after ℓiterations with
input xi. Note that we allow loops, which are edges of the form (xi, xi, ℓ). For
simplicity, we refer to the label of a vertex – that is, its corresponding word in S –
interchangeably with the vertex itself. There will be a potentially distinct digraph
on this same vertex set for each choice of ℓ∈Z>0. We call the union of these
digraphs for all ℓ∈Z>0 the (multidimensional) decoding network corresponding
to the code C and the speciﬁc choice of decoder, as we may consider the digraph
which incorporates the information for all ℓas a multidimensional network.
Deﬁnition 2 ([11]). A multidimensional network is an edge-labeled directed
graph D = (V, E, D), where V is a set of vertices, D a set of edge labels, called
dimensions, and E is a set of triples (u, v, d) where u, v ∈V and d ∈D. We say
that an edge (or vertex) belongs to a given dimension d if it is labeled d (or is
incident to an edge labeled d).
In this framework, the decoding network is a multidimensional network with
D = Z>0, and each edge labeled with the number of decoder iterations, ℓ, to
which it corresponds. Notice that, in any given dimension (i.e. iteration), every
vertex has outdegree equal to one. Next, we introduce an important type of
decoding network.
Deﬁnition 3. We say that a decoding network is transitive if (v1, v2, ℓ) ∈E if
and only if for every choice of 1 ≤k ≤ℓ−1, there exists vk ∈V such that
(v1, vk, k), (vk, v2, ℓ−k) ∈E. We say a decoder is transitive for a code C and a
choice of representation of C if its resulting decoding network is transitive.

14
A. Beemer and C.A. Kelley
Transitivity corresponds to the case in which running i iterations of the
decoder on a received word, and using that output as input to a subsequent
j decoding iterations, is equivalent to running i + j iterations on the original
received word.
Let Dℓdenote the digraph corresponding to the ℓth dimension of the decod-
ing network D, and let A(Dℓ) denote the adjacency matrix of the digraph Dℓ.
Observe that a decoding network D is transitive if and only if A(Dℓ) = (A(D1))ℓ
for all ℓ≥1, as the product (A(D1))ℓgives directed paths of length ℓin dimen-
sion 1 of D.
Example 1. Consider a simple binary parity-check code of length n, with parity-
check matrix given by the 1×n all-ones matrix. The Tanner graph representation
of such a code is a single check node with n variable node leaves. Thus, if code-
words are sent over the BSC and decoded with the Gallager A algorithm, the
message the solitary check node sends to each of its adjacent variable nodes is
either (a) the node’s channel value, if a codeword is received, or (b) the opposite
of its originally-received value, otherwise. Each variable node will always send
back its channel value. For any number of iterations, codewords will decode to
codewords, and any non-codeword ˆx will be decoded to ˆx + 1. If n is odd, every
received word will decode to a codeword, and the network will be transitive. If n
is even, ˆx+1 will not be a codeword, and the network will not be transitive. The
cases n = 3 and n = 4 are shown in Fig. 1; in both networks, edges belonging to
higher dimensions are suppressed, as all dimensions are identical.
Fig. 1. The decoding networks of parity-check codes of lengths 3 (left) and 4 (right).
Example 2. Consider the binary Hamming code of length 7 = 23 −1, denoted
H3. Recall that this code’s canonical 3 × 7 parity-check matrix has columns
consisting of all nonzero binary words of length 3. The corresponding Tanner
graph may be seen in representation A of Fig. 2. However, H3 may also be deﬁned
by the parity-check matrix whose Tanner graph is representation B in Fig. 2.
Under Gallager A decoding, representation A does not yield a transitive decoding
network. However, if representation B is decoded via Gallager A, the resulting
decoding network is transitive: every word decodes under a single iteration to a
codeword, and decodes identically for any higher number of iterations.
If a decoder is transitive for all representations of all codes C, we say that
it is universally transitive. Any decoder which ignores channel values at each
subsequent iteration will be universally transitive; some examples are given next.

Multidimensional Decoding Networks for Trapping Set Analysis
15
Fig. 2. Two distinct Tanner graph representations of H3, along with their parity-check
matrices. Variable nodes are denoted by •, and check nodes are denoted by ♦.
Example 3. If codewords from a code C are sent over the BEC, and words are
decoded using a peeling decoder which iteratively corrects erasures, then the
corresponding decoding network of C is universally transitive. Indeed, correc-
tions at each iteration are performed regardless of previous iterations. Similarly,
iterative bit-ﬂipping decoders over the BSC are universally transitive.
4
Trapping Set Characterization
Within this multidimensional decoding network framework, trapping sets of a
code transmitted over the BSC may be determined by looking at the supports of
the words corresponding to vertices in the decoding network that have nonzero
indegree in an inﬁnite number of dimensions. That is,
Theorem 1. For each vertex x ∈V = Fn
2 in a decoding network D = (V, E, D)
for a code C, let Mx be the set of vertices y ∈V for which there is an edge
(x, y, ℓ) ∈E for inﬁnitely many choices of ℓ. Then the set of variable nodes
corresponding to

y∈Mx
supp(y) ,
denoted T (x), is a trapping set with an inducing set given by the variable nodes
corresponding to supp(x). Furthermore, the set of trapping sets of the code C is
{T (x) : x ∈Fn
2} ,
and, given a trapping set T , its set of inducing sets is given by the variable nodes
corresponding to
{supp(x) : T (x) = T } ,
and its critical number is
m(T ) = min{|supp(x)| : T (x) = T } .
Proof. Assuming that the all-zero codeword was sent over the BSC, any decoding
errors will be given by 1’s. If, during the process of decoding the received word x,
there is some word y such that an edge from x to y occurs in an inﬁnite number
of network dimensions, the support of y gives variable node positions which are

16
A. Beemer and C.A. Kelley
not eventually correct. By deﬁnition, these variable nodes belong to a trapping
set induced by the variable nodes of the support of x. However, these may not be
the only variable nodes that are not eventually correct given the received word
x. Taking the union of the supports of all such y gives us our expression for
T (x), the trapping set induced by x. Repeating this for each possible received
word, we ﬁnd all trapping sets of the code. Note that each trapping set may be
induced by multiple received words.
Example 4. Let C be the binary repetition code of length 3, with the parity-check
matrix
H =

1 1 0
0 1 1

The associated Tanner graph is a path of length 4 with variable nodes v1, v2,
and v3. Let D be the (non-transitive) decoding network of C under Gallager
A, shown in Fig. 3. Assuming 0 was transmitted, the received word 011, for
example, decodes in one iteration to the codeword 111, but decodes for any
number of iterations greater than 1 to the (non-code)word 110. Thus, {v1, v2} is
a (2,1)-trapping set. Note that the support of 011, which induces this trapping
set, is not contained in the trapping set. Similarly, {v1, v2, v3} is a trapping set
corresponding to the codeword 111, with inducing sets {v1, v3} and {v1, v2, v3}.
Other trapping sets of the code include {v2, v3} (induced by {v1, v2}), {v1}
(induced by {v3}), and {v3} (induced by {v1}).
Fig. 3. The decoding network of a binary repetition code of length 3.
In the case of transitive decoding networks, trapping sets may be identiﬁed
by looking only at dimension 1, as follows:
Corollary 1. If the decoding network, D, of a code C is transitive, then the
trapping sets are given by (1) the sets of variable nodes corresponding to supports
of vertices with loops in D1, and (2) the sets of variable nodes corresponding to
unions of the supports of vertices forming directed cycles in D1. Furthermore,
inducing sets of trapping sets in a transitive decoding network are given by the
variable nodes corresponding to the support of any vertex which has a directed
path to either a (nonzero) vertex with a loop, or to a directed cycle, regardless
of where that path enters the cycle.
Proof. In a transitive decoding network, the edges in dimension ℓcorrespond
to directed paths of length ℓin D1. Thus, in order for a word to appear as

Multidimensional Decoding Networks for Trapping Set Analysis
17
the output of the decoder in an inﬁnite number of dimensions, it must be the
terminating vertex of inﬁnitely many directed paths (of distinct lengths) from
the received word. Because the decoding network for a code is ﬁnite, and the
outdegree of every vertex in D1 is equal to 1, this can only occur if there is a
loop at that vertex, or if it belongs to a directed cycle. The result follows from
Theorem 1.
In the adjacency matrix of dimension 1 of a decoding network, nonzero diag-
onal entries indicate loops. There are numerous algorithms for ﬁnding directed
cycles in a digraph, such as Depth-First Search (DFS) [12]. We further note
that several works have addressed the computational aspects of ﬁnding and/or
removing trapping sets [10,13–15].
We conclude this section by discussing the ideal number of iterations given
a ﬁxed code and decoder. Note that in Example 4, if the all-zero codeword was
sent, more received words are decoded correctly if only a single iteration of
the decoder is run, rather than multiple iterations. To this end, we introduce a
parameter we call the decoding diameter.
Deﬁnition 4. Let D = (V, E, D) be the decoding network of a code C under
a ﬁxed decoder. For x ∈V , let Lx be the minimum nonnegative integer such
that for all ℓ≥Lx, xℓ, the output of the decoder after ℓiterations, appears an
inﬁnite number of times in the sequence {xk}∞
k=Lx. Then, the decoding diameter
of D is given by Δ(D) = maxx∈V Lx.
After running Δ(D) iterations, all errors will be contained in trapping sets of
the code. In Example 2, H3 with canonical graph representation A decoded via
the Gallager A algorithm has decoding diameter 3, and H3 with representation
B has decoding diameter 1. The decoding diameter of a transitive decoding
network is the maximum distance in D1 of any vertex to either a vertex with
loop or to a directed cycle.
5
Representations Yielding Transitivity
Due to the eﬀect of the choice of representation on a decoding network’s struc-
ture, it is natural to ask which representations, if any, ensure that a code’s decod-
ing network is transitive under a ﬁxed decoder. Recall from Example 2 that the
canonical representation of the Hamming code H3 does not yield a transitive
decoding network under Gallager A, while the decoding network arising from the
representation given by the parity-check matrix including all nonzero codewords
of the dual code is transitive. In fact,
Theorem 2. Every binary linear code has a parity-check matrix representation
whose corresponding decoding network is transitive under Gallager A decoding.
In particular, adding exactly the nonzero codewords of the dual to the parity-
check matrix of a code (a representation which we will refer to as the com-
plete representation) will result in a transitive decoding network under Gallager

18
A. Beemer and C.A. Kelley
A decoding: using symmetries of the complete representation, we may show that
after a single iteration of the decoder, the received word either decodes to a code-
word and continues decoding to that codeword for any number of iterations, or
it will decode to itself under any number of iterations. The full proof of this
result will appear in the full version of this paper.
While the complete representation establishes the existence of a representa-
tion yielding a transitive network for any code, this level of redundancy is not
necessarily required, and in fact may create an excess of trapping sets in the
code. In Example 2, adding row seven of representation B to the canonical rep-
resentation A gives a transitive network, as does adding any three additional
rows to the canonical representation. However, any other combination of row
additions does not yield a transitive network. Thus, it is interesting to deter-
mine the minimum level of redundancy needed to yield a transitive decoding
network for a code with a ﬁxed choice of decoder.
In the remainder of this section, we apply the decoding network model to
product and half-product codes, which have been subject to renewed interest
[16–18], as well as to codes constructed from protographs. By phrasing the decod-
ing networks of these classes of codes in terms of the decoding networks of their
component codes, we can identify trapping sets in the larger codes with fewer
up-front computations. Proofs have been omitted for space.
Deﬁnition 5. Let C1 and C2 be binary linear codes of lengths n and m, respec-
tively. Then the product code C1 × C2 is the set of m × n binary arrays such that
each row forms a codeword in C1, and each column forms a codeword in C2.
Consider a product code, C1 ×C2, with a decoder that operates by iteratively
decoding one component code at a time. At each iteration, channel information is
dispensed with and decoding is performed based solely on the current estimate.
Theorem 3. Let C1 and C2 be codes of lengths n and m, respectively, with decod-
ing networks D1 and D2. Let A1 be the adjacency matrix of the directed graph
product (D1
1)m, and let A2 be the adjacency matrix of (D2
1)n. Then, the adjacency
matrix of dimension ℓof the transitive decoding network, D, of the product code
C1 × C2 is given by (A1A2)ℓ.
Deﬁnition 6 ([17]). Let C be a binary linear code of length n, and let ˜CH =
{X −XT : X ∈C × C}. The half-product code with component code C, denoted
CH, is obtained from ˜CH by setting the symbols below the diagonal of each
element of ˜CH equal to zero.
A decoder runs by iteratively decoding at each of the n constraints corre-
sponding to “folded” codewords, as in [17,18]. Again, channel information is dis-
pensed with at each subsequent iteration. Let Ai be the adjacency matrix of the
digraph on the vertex set of the decoding network of the half-product code, DH,
which gives the behavior of a single decoder iteration run on the ith constraint
code. While decoding is performed on the ith constraint, all (n −1)(n −2)/2
symbols not participating in constraint i are ﬁxed. Let Di be the decoding net-
work associated with the ith constraint code. Then, Ai is the adjacency matrix

Multidimensional Decoding Networks for Trapping Set Analysis
19
of a disjoint union of 2(n−1)(n−2)/2 copies of Di, corresponding to all the ways
non-participating symbols may be ﬁxed. Permute the rows and columns of the
Ai’s so that they all correspond to a single ordering of the vertices in DH, and
let Sn denote the symmetric group on n elements.
Theorem 4. The product (Aσ(1) · · · Aσ(n))ℓ, where σ ∈Sn, gives the adjacency
matrix of DH
ℓ, dimension ℓof the decoding network of the half-product code CH,
where the component constraints are decoded in the order determined by σ.
Theorem 5. Let C be a binary linear code with Tanner graph G and decoding
network D with respect to a ﬁxed decoder. Viewing G as a protograph, let ˆC be
the code corresponding to a degree h lift of G, denoted ˆG, and (with an abuse of
notation), let ˆD be the decoding network of ˆC with respect to the same decoder.
Then, there exists a subgraph of ˆD that is isomorphic to D. In particular, if D
is not transitive, then ˆD is not transitive. However, transitivity of D does not
necessarily imply transitivity of ˆD.
6
Conclusions
We presented a multidimensional network framework for the analysis of trapping
sets under hard-decision message-passing decoders. We showed that when the
decoding network of a code is transitive, trapping sets and their inducing sets
can be found by examining the behavior of the decoder in a single iteration. We
further showed that all codes have a transitive decoding network representation
under Gallager A decoding.
This work leads to many interesting avenues for future pursuit: determining,
for certain classes of codes, the minimum number of parity-check matrix rows
needed to obtain a transitive decoding network, using the transitive networks
of these codes to improve existing results on their trapping and inducing sets,
characterizing which lifts of transitive decoding networks preserve transitivity,
developing an eﬃcient method of ﬁnding trapping sets and predicting the decod-
ing diameter for non-transitive networks, and extending these results to Gallager
B and other decoding algorithms. As a ﬁrst direction, an interesting open prob-
lem is to determine a graph theoretic condition (or equivalently, a condition on
the parity-check matrix) that guarantees transitivity.
Acknowledgements. We thank the anonymous reviewers for their useful comments
that improved the quality of the paper.
References
1. Koetter, R., Vontobel, P.: Graph-covers and iterative decoding of ﬁnite length
codes. In: Proceedings of the 3rd International Symposium on Turbo Codes (2003)
2. Dolecek, L., Zhang, Z., Anantharam, V., Wainwright, M., Nikolic, B.: Analysis of
absorbing sets for array-based LDPC codes. In: IEEE International Conference on
Communications, pp. 6261–6268 (2007)

20
A. Beemer and C.A. Kelley
3. Richardson, T.: Error-ﬂoors of LDPC codes. In: Proceedings of the 41st Allerton
Conference on Communication, Control and Computing, pp. 1426–1435 (2003)
4. Di, C., Proietti, D., Telatar, I.E., Richardson, T.J., Urbanke, R.L.: Finite-length
analysis of low-density parity-check codes on the binary erasure channel. IEEE
Trans. Inf. Theor. 48, 1570–1579 (2002)
5. Nguyen, D.V., Chilappagari, S.K., Marcellin, M.W., Vasic, B.: On the construction
of structured LDPC codes free of small trapping sets. IEEE Trans. Inf. Theor. 58,
2280–2302 (2012)
6. Sankaranarayanan, S., Chilappagari, S.K., Radhakrishnan, R., Vasi´c, B.: Failures
of the Gallager B decoder: analysis and applications. In: IEEE International Con-
ference on Communications (2006)
7. Gallager, R.: Low-density parity-check codes. IRE Trans. Inf. Theor. 8, 21–28
(1962)
8. Sun, J.: Studies on graph-based coding systems. Ph.D. Dissertation, Department
of Electrical and Computer Engineering, University of California, Columbus (2004)
9. Butler, B.K., Siegel, P.H.: Error ﬂoor approximation for LDPC codes in the AWGN
channel. IEEE Trans. Inf. Theor. 60, 7416–7441 (2014)
10. Vasic, B., Chilappagari, S.K., Nguyen, D.V., Planjery, S.K.: Trapping set ontology.
In: Proceedings of the 47th Allerton Conference on Communication, Control, and
Computing, pp. 1–7 (2009)
11. Berlingerio, M., Coscia, M., Giannotti, F., Monreale, A., Pedreschi, D.: Foun-
dations of multidimensional network analysis. In: International Conference on
Advances in Social Networks Analysis and Mining, pp. 485–489 (2011)
12. Cormen, T.H., Leiserson, C.E., Rivest, R.L., Stein, C.: Introduction to Algorithms,
3rd edn. MIT Press, Cambridge (2009)
13. Beemer, A., Kelley, C.A.: Avoiding trapping sets in SC-LDPC codes under win-
dowed decoding. In: Proceedings IEEE International Symposium on Information
Theory and Applications (2016)
14. Karimi, M., Banihashemi, A.H.: Eﬃcient algorithm for ﬁnding dominant trapping
sets of LDPC codes. IEEE Trans. Inf. Theor. 58, 6942–6958 (2012)
15. Hashemi, Y., Banihashemi, A.H.: Characterization and eﬃcient exhaustive search
algorithm for elementary trapping sets of irregular LDPC codes. In: Proceedings
of IEEE International Symposium on Information Theory (2017)
16. Justesen, J.: Performance of product codes and related structures with iterated
decoding. IEEE Trans. Commun. 59, 407–415 (2011)
17. Mittelholzer, T., Parnell, T., Papandreou, N., Pozidis, H.: Improving the error-ﬂoor
performance of binary half-product codes. In: Proceedings of IEEE International
Symposium Information Theory and Applications (2016)
18. Pﬁster, H.D., Emmadi, S.K., Narayanan, K.: Symmetric product codes. In: Infor-
mation Theory and Applications Workshop, pp. 282–290 (2015)

Erasure Correction and Locality
of Hypergraph Codes
Allison Beemer, Carolyn Mayer(B), and Christine A. Kelley
University of Nebraska-Lincoln, Lincoln, NE, USA
cmayer@huskers.unl.edu
Abstract. We examine erasure correction and locality properties of
regular and biregular hypergraph codes. We propose a construction of
t-uniform t-partite biregular hypergraphs based on (c, d)-regular bipar-
tite graphs. We show that for both the regular and biregular case, when
the underlying hypergraph has expansion properties, the guaranteed era-
sure correcting capability for the resulting codes is improved. We provide
bounds on the minimum stopping set size and cooperative locality of
these hypergraph codes for application in distributed storage systems.
Keywords: Hypergraphs · Generalized LDPC codes · Locality · (r, ℓ)-
cooperative locality · Availability · Stopping sets · Expander graphs
1
Introduction
The past decade has seen an increased interest in coding for distributed storage
systems (DSS) due to the increasing amounts of data that need to be stored and
accessed across many servers. A primary focus in this area is the design of codes
with locality properties, where error correction of small sets of symbols may be
performed eﬃciently without having to access all symbols or all information from
accessed symbols, and where data may be protected by multiple repair groups.
Coding techniques optimizing diﬀerent aspects of the storage and retrieval prob-
lem yield diﬀerent families of codes, such as batch codes, locally repairable codes,
private information retrieval codes, and local regeneration codes, [1–3], with con-
nections among them [4,5].
Codes from regular hypergraphs with expansion-like properties were intro-
duced and analyzed in [6,7]. As was shown for expander codes that have under-
lying expander graph representations, the authors show that better expansion
(referred to as ϵ-homogeneity in the hypergraph case) implies improved mini-
mum distance and error correction. Indeed, [8] gives a construction of one of
the few explicit algebraic families of asymptotically good codes known to date,
and there have since been several papers addressing the design and analysis of
codes using expander graphs [9,10]. In this paper, we consider codes based on
regular hypergraphs (i.e. all vertices have the same degree) as well as biregular
hypergraphs (i.e. vertices have one of two distinct degrees), and present bounds
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 21–29, 2017.
DOI: 10.1007/978-3-319-66278-7 3

22
A. Beemer et al.
on their error-correction capabilities in the context of distributed storage, specif-
ically the minimum stopping set size and cooperative locality of the codes.
This paper is organized as follows. Section 2 presents relevant background.
We derive bounds on the minimum stopping set size and cooperative locality
of regular hypergraph codes in Sect. 3. In Sect. 4, we show how to construct a
t-uniform t-partite biregular hypergraph from an underlying (c, d)-regular bipar-
tite graph and provide similar bounds. Section 5 concludes the paper.
2
Preliminaries
A hypergraph H is a set of vertices V along with a set of edges E, where E is
a set of subsets of V . A hypergraph is t-uniform if every edge contains exactly
t vertices, and it is t-partite if the vertex set V can be partitioned into t vertex
sets V1, . . . , Vt such that no edge contains more than one vertex from any part.
In this paper we will use H = (V1, V2, . . . , Vt; E) to denote a t-uniform t-partite
hypergraph in which each edge contains exactly one vertex from each part. A
hypergraph is Δ-regular if every vertex belongs to Δ edges. We assume that all
hypergraphs considered in this paper have no parallel edges. That is, no two
distinct edges are comprised of exactly the same set of vertices.
Such t-uniform t-partite Δ-regular hypergraphs were used in [6,7] to design
codes where the edges of the hypergraph represent the code symbols, and the
vertices of the hypergraph represent the constraints. Speciﬁcally, when there are
n vertices in each part, the block length of the corresponding hypergraph code is
nΔ and the number of constraint vertices is nt. As in [11], each constraint node
represents a linear block length Δ code (commonly referred to as a “subcode”),
and is satisﬁed when the assignment on the edges incident to that constraint
node form a codeword of the subcode. This is similar to generalized low density
parity-check (LDPC) codes as deﬁned in [11], in which the constraint nodes may
be more sophisticated than simple parity-check codes.
Sipser and Spielman [8] show that the guaranteed error correction capabilites
of a code may be improved when the underlying graph is a good expander. Let
G be a (c, d)-regular bipartite graph with m left vertices of degree c and n right
vertices of degree d, and let μ be the second largest eigenvalue (in absolute value)
of the adjacency matrix of G. Then for subsets S and T of the left and right
vertices, respectively, the number of edges in the subgraph induced by S ∪T is
at most |E(S, T)| ≤
d
m|S||T| + μ
2 (|S| + |T|) [12]. Loosely speaking, a graph is a
good expander if small sets of vertices have large sets of neighbors; graphs with
small second largest eigenvalue have this property. Bilu and Hoory introduced
an analogous notion for hypergraph expansion as follows:
Deﬁnition 1. [6] Let H = (V1, V2, . . . , Vt; E) be a t-uniform t-partite Δ-regular
hypergraph with n vertices in each part. Then H is ϵ-homogeneous if for every
choice of A1, A2, . . . , At with Ai ⊆Vi and |Ai| = αin,
|E(A1, A2, . . . , At)|
nΔ
≤
t
i=1
αi + ϵ√ασ(1)ασ(2),

Erasure Correction and Locality of Hypergraph Codes
23
where σ ∈St is a permutation on [t] such that ασ(i) ≤ασ(i+1) for each i ∈[t−1],
and E(A1, . . . , At) denotes the set of edges which intersect all of the Ai’s.
Let [N, K, D] denote a binary linear code with block length N, dimension
K, and minimum distance D. The following bounds on the rate and minimum
distance of a code Z from an ϵ-homogeneous t-uniform t-partite Δ-regular hyper-
graph with n vertices in each part and a [Δ, ΔR, Δδ] subcode C at each con-
straint node are given in [6]:
rate(Z) ≥tR −(t −1)
(1)
dmin(Z) ≥nΔ

δ
t
t−1 −c(ϵ, δ, t)

(2)
where c(ϵ, δ, t) →0 as ϵ →0. Note that nΔ is the blocklength of Z.
The locality of a code measures how many code symbols must be used to recover
an erased code symbol. While there are a variety of locality notions relevant
to coding for distributed storage, in this paper we focus on (r, ℓ)-cooperative
locality and (r, τ)-availability [13,14].
Deﬁnition 2. A code C has (r, ℓ)-cooperative locality if for any y ∈C, any set
of ℓsymbols in y are functions of at most r other symbols. Furthermore, C has
(r, τ)-availability if any symbol in y can be recovered by using any of τ disjoint
sets of symbols each of size at most r.
3
Bounds on Regular Hypergraph Codes
In this section, we examine the erasure correction and cooperative locality of
regular hypergraph codes. First, a stopping set in the Tanner graph representing
a generalized LDPC code is a subset S of the variable nodes such that each
neighbor of S has at least dmin(C) neighbors in S, where C is the subcode
represented by the constraint vertices [15,16]. It was initially shown in [17] that
stopping sets characterize iterative decoder failure for simple LDPC codes (i.e.
when the subcodes are simple parity check codes) on the binary erasure channel.
We ﬁrst deﬁne stopping sets for regular hypergraph codes and give a lower bound
on the minimum stopping set size.
Deﬁnition 3. Let Z be a code on a hypergraph H = (V1, . . . , Vt; E), with edges
representing code symbols and vertices representing the constraints of a subcode C.
Then a stopping set S is a subset of the edges of H such that every vertex contained
in an element of S is contained in at least dmin(C) elements of S.
Though the size of a minimum stopping set depends on both the hypergraph
representation and the choice of subcode, we denote this size by smin(H), and
assume that the subcode is clear from context.

24
A. Beemer et al.
Theorem 1. Let H be a t-uniform t-partite Δ-regular hypergraph. If the vertices
of H represent constraints of a subcode C with minimum distance dmin(C) and
block length Δ, then the size of the minimum stopping set, smin(H), is bounded
by
smin(H) ≥dmin(C)t/(t−1).
(3)
Proof. Let H be as above, and let S be a minimum stopping set. Each edge
in S contains exactly one constraint node from each of the t parts of H, so
each part of H has exactly |S| = smin(H) incident edges belonging to S. Each
constraint node contained in an edge in S must be contained in at least dmin(C)
edges in S. By the pigeonhole principle, the number of vertices in any part of
H that are contained in some edge in S is bounded above by smin(H)/dmin(C).
Indeed, were there more than smin(H)/dmin(C) vertices incident to S in a single
part, some vertex must have fewer than smin(H)/(smin(H)/dmin(C)) = dmin(C)
incident edges from S, a contradiction. Now consider the maximum size of S:
this amounts to counting the number of edges possible, given that each edge
is incident to exactly one vertex of (at most) smin(H)/dmin(C) vertices in each
of the t parts of H. That is, there are at most (smin(H)/dmin(C))t edges in S.
Thus,
smin(H)
dmin(C)
t
≥smin(H) ⇒smin(H) ≥dmin(C)t/(t−1).
The bound of Theorem 1 is tight. For example, when H is a complete
3-uniform 3-partite hypergraph with at least two vertices in each part and con-
straint code C such that dmin(C) = 4, it is easy to show that smin(H) = 8.
Since the errors of particular relevance to DSS are erasures (such as a server going
down), we can use the stopping set bound to characterize how many errors can
be corrected. Theorem 1 guarantees that we may correct any dmin(C)t/(t−1) −1
erasures using iterative decoding. If C is a code with locality r1, at most
(smin(H)/dmin(C)) · r1 · t other codeword symbols are involved in the repair
of the erasures in the decoding process. This yields the following:
Corollary 1. If the subcodes C of the regular hypergraph code Z have r1 locality,
then Z has (r, ℓ)-cooperative locality where
r = r1tsmin(H)/dmin(C)
(4)
smin(H) −1 ≥ℓ≥dmin(C)t/(t−1) −1.
(5)
Observe that if the subcode C has (r, τ)-availability, then the hypergraph
code Z has at least (r, τ)-availability.
We now extend the result to codes on hypergraphs with known ϵ-homogeneity.

Erasure Correction and Locality of Hypergraph Codes
25
Theorem 2. Let H = (V1, V2, . . . , Vt; E) be a t-uniform t-partite Δ-regular
ϵ-homogeneous hypergraph where there are n vertices in each of the t parts. If
the subcodes C have minimum distance dmin(C),
smin(H) ≥

1 −
ϵΔ
dmin(C)
 nt−1dmin(C)t
Δ
1/(t−1)
.
(6)
For ϵ < dmin(C)(nt−1−Δ)
Δnt−1
, this gives an improvement on the bound in Theorem 1.
Proof. Let S
be a minimum stopping set. By Theorem 1, smin(H)
≥
dmin(C)t/(t−1). Now, let Ai ⊆Vi be the set of vertices in Vi, for i ∈[t], con-
tained in an edge in S. By ϵ-homogeneity,
smin(H) = |S| ≤|E(A1, . . . , At)| ≤nΔ
 t
i=1
αi + ϵ√ασ(1)ασ(2)

.
Since |Ai| ≤smin(H)/dmin(C) for all i, αi ≤smin(H)/ndmin(C). Thus, the above
inequality simpliﬁes to obtain the result:
smin(H) ≤nΔ
 smin(H)
ndmin(C)
t
+ ϵ smin(H)
ndmin(C)

.
Observe that we have shown in general that
smin(H) ≥

1 −
ϵΔ
dmin(C)
 nt−1
Δ
1/(t−1)
dmin(C)t/(t−1).
Then, if

1 −
ϵΔ
dmin(C)

nt−1
Δ
1/(t−1)
> 1, this gives a better lower bound for
smin(H) than that found in Theorem 1. Simplifying, we have our condition on ϵ.
Corollary 2. Using iterative decoding on a code Z based on a t-uniform
t-partite Δ-regular ϵ-homogeneous hypergraph with vertices representing con-
straints of a subcode C, up to

1 −
ϵΔ
dmin(C)
 nt−1dmin(C)t
Δ
1/(t−1)
−1
erasures may be corrected.
In other words, if δ is the relative minimum distance of C, and N is the total
number of edges in the hypergraph (that is, the block length of the code Z), we
may correct up to a δ(δ −ϵ)1/(t−1) −1
N fraction of erasures.

26
A. Beemer et al.
Remark 1. We may correct up to a δt/(t−1) −1
N −c(ϵ, δ, t) fraction of erasures,
where c(ϵ, δ, t) →0 as ϵ →0. It can be shown that the bound in Corollary 2
improves the error correction capability of
t −1
t/2
−2/t δ
2
(t+2)/t
−c′(ϵ, δ, t)
in [6] for any 0 < δ < 1 and t ≥2 (i.e. for all relevant cases) and large block
length. Note that c′(ϵ, δ, t) ̸= c(ϵ, δ, t), but that both vanish as ϵ →0.
It is important to note that we are focusing solely on erasures, while [6] gives
a decoding algorithm and correction capabilities for more general errors.
4
Bounds on Biregular Hypergraph Codes
A construction of t-uniform t-partite Δ-regular hypergraphs is presented in [6]
based on an underlying regular bipartite expander graph. In this section we
show how to obtain a t-uniform t-partite (Δ1, Δ2)-biregular hypergraph from a
(c, d)-regular bipartite graph in a similar way. We provide bounds on the stop-
ping set size, cooperative locality, rate, and minimum distance for the resulting
hypergraph codes.
Deﬁnition 4. We say that a t-uniform t-partite hypergraph H = (V1, . . . , Vt; E)
is (Δ1, Δ2)-biregular if the parts can be labeled such that each vertex in an odd
(resp., even) index part is contained in Δ1 (resp., Δ2) edges.
Construction 1. Let G = V ∪W be a (c, d)-regular bipartite expander graph
with |V | ≥|W|. For t ∈N, construct a t-uniform t-partite hypergraph H
with parts V1, . . . , Vt as follows. For odd (resp., even) i, let Vi be a copy of
V (resp., W). Take E(H) to be the set of edges corresponding to walks of length
t −1 in G. That is (v1, . . . , vt) with vi ∈Vi is in E(H) if and only if (v1, . . . , vt)
corresponds to a walk in G.
Note that H is indeed t-uniform and t-partite, and has vertices of degree
Δ1 = c⌈t
2 ⌉d⌊t
2 ⌋−1 (resp., Δ2 = c⌈t
2 ⌉−1d⌊t
2 ⌋) in odd (resp., even) index parts.
The deﬁnition of a stopping set may be extended to biregular hypergraph codes.
Deﬁnition 5. Let Z be a code on a hypergraph H = (V1, V2, . . . , Vt; E), with the
edges representing the code symbols and the vertices representing the constraints
of a subcode C1 (resp., C2) if the vertex is in an odd (resp., even) index part.
Then a stopping set S is a subset of the edges of H such that every vertex
contained in an element of S is contained in at least dmin(C1) (resp., dmin(C2))
elements of S if the vertex is in an odd (resp., even) index part.
We now give a bound on the minimum stopping set size and (r, ℓ)-cooperative
locality of codes resulting from Construction 1. The proofs are similar to those
in Sect. 3 and are thus omitted.

Erasure Correction and Locality of Hypergraph Codes
27
Theorem 3. Let H be a t-uniform t-partite (Δ1, Δ2)-biregular hypergraph. If
the vertices in an odd (resp., even) index part of H represent constraints of a
subcode C1 (resp., C2) with block length Δ1 (resp., Δ2), then the size of the
minimum stopping set, smin(H), is bounded by
smin(H) ≥

dmin(C1)⌈t
2 ⌉dmin(C2)⌊t
2 ⌋1/(t−1)
.
(7)
Corollary 3. If the subcodes C1 (resp., C2) of the biregular hypergraph code Z
have r1 (resp., r2) locality then Z has (r, ℓ)-cooperative locality where
r = r1⌈t
2⌉smin(H)
dmin(C1) + r2⌊t
2⌋smin(H)
dmin(C2)
(8)
smin(H) −1 ≥ℓ≥

dmin(C1)⌈t
2 ⌉dmin(C2)⌊t
2 ⌋1/(t−1)
−1.
(9)
Observe that Z has at least the (r, τ)-availability of its subcodes.
We next extend the deﬁnition of ϵ-homogeneity to biregular hypergraphs and
give an improved minimum stopping set bound for the corresponding codes.
Deﬁnition 6. Let H = (V1, V2, . . . , Vt; E) be a t-uniform t-partite (Δ1, Δ2)-
biregular hypergraph with n1 vertices in each odd index part and n2 vertices in
each even index part. We say that H is ϵ-homogeneous if for every choice of
A1, A2, . . . , At, with Ai ⊆Vi,
|E(A1, A2, . . . At)|
Δ1n1
≤
t
i=1
αi + ϵ√ασ(1)ασ(2),
where σ is a permutation on [t] such that ασ(i) ≤ασ(i+1) for each i ∈[t −1],
and |Ai| = αin1 if i is odd and |Ai| = αin2 if i is even.
Theorem 4. Let H = (V1, . . . , Vt; E) be a t-uniform t-partite (Δ1, Δ2)-regular
ϵ-homogeneous hypergraph where there are n1 (resp., n2) vertices in each of the
odd (resp., even) index parts. Let C1 and C2 be the subcodes of the odd and even
index parts, respectively. Then smin(H) is bounded below by

(n1dmin(C1))⌈t
2 ⌉(n2dmin(C2))⌊t
2 ⌋
n1Δ1

1 −
ϵn1Δ1
mini=1,2{nidmin(Ci)}

1
t−1
.
(10)
For ϵ <

1 −
n1Δ1
n
⌈t
2 ⌉
1
n
⌊t
2 ⌋
2

mini=1,2{nidmin(Ci)}
n1Δ1
, this improves the Theorem 3 bound.
We now give bounds on the rate and minimum distance of a length n1Δ1 code
Z from an ϵ-homogeneous t-uniform t-partite (Δ1, Δ2)-regular hypergraph with

28
A. Beemer et al.
n1 (resp., n2) vertices in each odd (resp., even) index part and [Δ1, Δ1R1, Δ1δ1]
subcodes C1 (resp., [Δ2, Δ2R2, Δ2δ2] subcodes C2).
rate(Z) ≥R1⌈t
2⌉+ R2⌊t
2⌋−(t −1)
(11)
dmin(Z) ≥n1Δ1

(δ
⌈t
2 ⌉
1
δ
⌊t
2 ⌋
2
)
1
t−1 −c(ϵ, δ1, δ2, t)

(12)
where c(ϵ, δ1, δ2, t) →0 as ϵ →0. The proofs are similar to those for the regular
hypergraph bounds given in [6] and are thus omitted.
Ramanujan graphs have the largest possible gap between their ﬁrst and sec-
ond largest eigenvalues (in absolute value), and are thus the best in terms of
expansion. In [18], the authors give an explicit algebraic construction of regular
Ramanujan graphs. The existence of inﬁnite families of (c, d)-regular bipartite
Ramanujan graphs was shown in [19]. Moreover, the regular t-uniform t-partite
hypergraphs constructed in [6] from regular expander graphs with second largest
eigenvalue λ were shown to be 2(t−1)λ-homogeneous. We conjecture that when
Construction 1 starts with a (c, d)-regular bipartite expander graph, the result-
ing hypergraph will be ϵ-homogeneous, where ϵ depends on the second largest
eigenvalue of the underlying expander graph.
5
Conclusions
We examined the erasure correcting capability of regular hypergraph codes as
well as a new code construction based on biregular hypergraphs. We provided
lower bounds on the minimum stopping set size, cooperative locality, and avail-
ability of these codes, and gave improved parameters for ϵ-homogeneous hyper-
graph codes. Designing explicit families of hypergraphs that are optimal with
respect to their corresponding codes’ locality properties is a line for future work.
An interesting open problem is whether the proposed construction of biregular
hypergraphs can be proven to yield biregular ϵ-homogeneous hypergraphs when
starting from biregular expander graphs.
Acknowledgements. We thank the anonymous reviewers for their useful comments
that improved the quality of the paper.
References
1. Ishai, Y., Kushilevitz, E., Ostrovsky, R., Sahai, A.: Batch codes and their applica-
tions. In: ACM Symposium on Theory of Computation, Chicago, IL, pp. 262–271,
June 2004
2. Dimakis, A.G., Ramchandran, K., Wu, Y., Suh, C.: A survey on network codes for
distributed storage. In: Proceedings of IEEE, vol. 99, no. 3, March 2011
3. Kamath, G.M., Prakash, N., Lalitha, V., Kumar, P.V.: Codes with local regenera-
tion. In: IEEE Information Theory and Application Workshop, pp. 1–5, San Diego,
CA, February 2013

Erasure Correction and Locality of Hypergraph Codes
29
4. Ernvall, T., Westerb¨ack, T., Freij-Hollanti, R., Hollanti, C.: A connection between
locally repairable codes and exact regenerating codes. In: Proceedings of IEEE
International Symposium on Information Theory, Barcelona, July 2016
5. Skachek, V.: Batch and PIR Codes and their Connections to Locally Repairable
Codes. arXiv:1611.09914v2, March 2017
6. Bilu, Y., Hoory, S.: On codes from hypergraphs. Eur. J. Comb. 25, 339–354 (2004)
7. Barg, A., Z´emor, G.: Codes on hypergraphs. In: IEEE International Symposium
on Information Theory, Toronto, ON, pp. 156–160, July 2008
8. Sipser, M., Spielman, D.A.: Expander codes. IEEE Trans. Inf. Theory 42, 1710–
1722 (1996)
9. Janwa, H.L., Lal, A.K.: On expander graphs: parameters and applications,
preprint. arXiv:0406048
10. Z´emor, G.: On expander codes. IEEE Trans. Inf. Theory 47(2), 835–837 (2001)
11. Tanner, R.M.: A recursive approach to low complexity codes. IEEE Trans. Inf.
Theory 27, 533–547 (1981)
12. Janwa, H., Lal, A.K.: On Tanner codes: minimum distance and decoding. Proc.
AAECC 13, 335–347 (2003)
13. Rawat, A.S., Mazumdar, A., Vishwanath, S.: Cooperative local repair in distrib-
uted storage. arXiv:1409.3900
14. Rawat, A.S., Papailiopoulos, D.S., Dimakis, A.G., Vishwanath, S.: Locality and
availability in distributed storage. IEEE Trans. Inf. Theory 62(8), 4481–4493
(2016)
15. Kelley, C.A., Sridhara, D.: Eigenvalue bounds on the pseudocodeword weight of
expander codes. Adv. Math. Commun 1, 287–307 (2007)
16. Miladinovic, N., Fossorier, M.P.C.: Generalized LDPC codes and generalized stop-
ping sets. IEEE Trans. Commun. 56, 201–212 (2008)
17. Di, C., Proietti, D., Richardson, T., Teletar, E., Urbanke, R.: Finite-length analysis
of low-density parity-check codes on the binary erasure channel. IEEE Trans. Inf.
Theory 48, 1570–1579 (2002)
18. Lubotzky, A., Phillips, R., Sarnak, P.: Ramanujan graphs. Combinatorica 8, 261–
277 (1988)
19. Marcus, A.W., Spielman, D.A., Srivastava, N.: Interlacing families I: bipartite
Ramanujan graphs of all degrees. Ann. Math. 182, 307–325 (2015)

Reed-Muller Codes: Information Sets
from Deﬁning Sets
Jos´e Joaqu´ın Bernal(B) and Juan Jacobo Sim´on
Departamento de Matem´aticas, Universidad de Murcia, 30100 Murcia, Spain
{josejoaquin.bernal,jsimon}@um.es
Abstract. As aﬃne-invariant codes, Reed-Muller codes are extension
of cyclic group codes and they have a deﬁning set that determines them
uniquely. In this paper we identify those cyclic codes with multidimen-
sional abelian codes and we use the techniques introduced in [3] to con-
struct information sets for ﬁrst and second order Reed-Muller codes from
its deﬁning set.
1
Introduction
We are interested in ﬁnding information sets for Reed-Muller codes, a question
that has been addressed for many authors earlier. From the geometric point
of view several ideas for ﬁnding information sets has been presented. In [5,10]
Moorhouse and Blokhuis gave bases formed by the incidence vectors of cer-
tain lines valid for a more general family of geometric codes. Later, J. D. Key,
T. P. McDonough and V. C. Mavron extended these deﬁnitions in [8] in order to
apply the permutation decoding algorithm. Finally, in [9], the same authors gave
a description of information sets for Reed-Muller codes by using the polynomial
approach.
In this work we use the fact that Reed-Muller codes can be seen as aﬃne-
invariant codes to get information sets. From this context any Reed-Muller code
is a parity check extension of a cyclic group code, so any information set of that
cyclic code is obviously an information set for the Reed-Muller code; moreover,
there exists a direct connection between the respective deﬁning sets. On the
other hand, in [3] we introduced a method for constructing information sets for
any abelian code starting from its deﬁning set. Then, the goal of this paper
is to obtain information sets for Reed-Muller codes of ﬁrst and second order
respectively by applying those techniques shown in [3] to the punctured cyclic
group code seen as a multidimensional abelian code.
The paper is structured as follows. Section 2 include the basic notation and
the necessary preliminaries about abelian codes. Then, Sect. 3 contains the expla-
nation of how the method given in [3] works in the particular case of two-
dimensional abelian codes. Section 4 shows some essential results related to the
This work was partially supported by MINECO, project MTM2016-77445-P, and
Fundaci´on S´eneca of Murcia, project 19880/GERM/15.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 30–47, 2017.
DOI: 10.1007/978-3-319-66278-7 4

Reed-Muller Codes: Information Sets from Deﬁning Sets
31
case of cyclic codes seen as two-dimensional cyclic codes. In Sect. 5 we recall
the deﬁnition of Reed-Muller codes as aﬃne-invariant codes in an abelian group
algebra and then we show how we apply the results in the previous sections
to them. The core of this paper are Sects. 6 and 7 where we present the main
results, to wit, explicit descriptions of the information sets for ﬁrst and second
order Reed-Muller codes respectively.
2
Preliminaries
In this paper we deal with Reed-Muller codes identiﬁed as abelian codes, so for
the convenience of the reader we give an introduction to abelian codes just in
the binary case. Then all throughout F denotes the ﬁeld with two elements.
A binary abelian code is an ideal of a group algebra FG, where G is an
abelian group. It is well-known that there exist integers r1, . . . , rl such that G
is isomorphic to the direct product Cr1 × · · · × Crl, with Cri the cyclic group
of order ri, i = 1, . . . , l. Moreover, this decomposition yields an isomorphism of
F-algebras from FG to
F[X1, . . . , Xl]/ ⟨Xr1
1 −1, . . . , Xrl
l −1⟩.
We denote this quotient algebra by A(r1, . . . , rl) and we identify the code-
words with polynomials P(X1, . . . , Xl) such that every monomial satisfy that
the degree of the indeterminate Xi is in Zri, the ring of integers modulo
ri, that we always write as canonical representatives. We write the elements
P ∈A(r1, . . . , rl) as P = P(X1, . . . , Xl) =  ajXj, where j = (j1, . . . , jl) ∈
Zr1 × · · · × Zrl, Xj = Xj1
1 · · · Xjl
l
and aj ∈F. We always assume that ri is
odd for every i = 1, . . . , l, that is, we assume that A(r1, . . . , rl) is a semisimple
algebra.
Our main tool to study the construction of information sets for abelian codes
is the notion of deﬁning set.
Deﬁnition 1. Let C ⊆A(r1, . . . , rl) be an abelian code. Let Ri be the set of ri-th
roots of unity, i = 1, . . . , l. Then the root set of C is given by
Z(C) =

(β1, . . . , βl) ∈
l
i=1
Ri | P(β1, . . . , βl) = 0 for all P(X1, . . . , Xl) ∈C

.
Then, ﬁxed a primitive ri-th root of unity αi in some extension of F, i =
1, . . . , l, the deﬁning set of C with respect to α = {α1, . . . , αl} is
Dα (C) = {(a1, . . . , al) ∈Zr1 × · · · × Zrl | (αa1
1 , . . . , αal
l ) ∈Z(C)} .
It can be proved that, ﬁxed a collection of primitive roots of unity, every abelian
code is totally determined by its deﬁning set.
In order to describe the structure of the deﬁning set of an abelian code we
need to introduce the following deﬁnitions.

32
J.J. Bernal and J.J. Sim´on
Deﬁnition 2. Let a, r and γ be integers. The 2γ-cyclotomic coset of a modulo
r is the set
C2γ,r(a) =

a · 2γ·i | i ∈N

⊆Zr.
We shall write Cr(a) when γ = 1.
Deﬁnition 3. Given (a1, . . . , al) ∈Zr1 ×· · ·×Zrl, its 2-orbit modulo (r1, . . . , rl)
is the set
Q(a1, . . . , al) =

a1 · 2i, . . . , al · 2i	
| i ∈N

⊆Zr1 × · · · × Zrl.
It is well known that for every abelian code C ⊆A(r1, . . . , rl), D (C) is closed
under multiplication by 2 in Zr1 × · · · × Zrl, and so D(C) is a disjoint union of
2-orbits modulo (r1, . . . , rl). Conversely, every union of 2-orbits modulo
(r1, . . . , rl) deﬁnes an abelian code in A(r1, . . . , rl). From now on, we will only
write 2-orbit, and the tuple of integers will always be clear by the context.
Now, we need to give the notion of information set of a code in the context
of abelian codes. Let P =  ajXj ∈A(r1, . . . , rl) and let I be a subset of
Zr1 × · · · × Zrl, we denote by PI the vector (aj)j∈I ∈F|I|. Now, for an abelian
code C ⊆A(r1, . . . , rl) we denote by CI the linear code {PI : P ∈C} ⊆F|I|.
Deﬁnition 4. An information set for an abelian code C ⊆A(r1, . . . , rl) with
dimension k is a set I ⊆Zr1 × · · · × Zrl such that |I| = k and CI = Fk.
The complementary set (Zr1 × · · · × Zrl) \ I is called a set of check positions
for C.
To ﬁnish this section, let us recall that, as usual, we denote by C⊥the dual
code of C, that is, the set of codewords v ∈A(r1, . . . , rl) such that v · u = 0, for
all u ∈C, where “·” denotes the usual inner product. It is easy to see that any
information set for C is a set of check positions for C⊥and vice versa.
3
Information Sets for Abelian Codes
In [3] we introduced a method for constructing information sets for any multidi-
mensional abelian code just in terms of its deﬁning set. Although one may apply
this construction for any value of l, for the shake of simplicity, in this section
we only recall the two-dimensional construction. So, from now on we take l = 2
and the ambient space will be A(r1, r2).
Let e = (e1, e2) ∈Zr1 × Zr2. We deﬁne
m(e1) = |Cr1(e1)|
and
m(e) = m(e1, e2) =


C2m(e1),r2(e2)


 .
(1)
The construction of information sets is based on the computation of the
parameters deﬁned in (1) on a special subset of the deﬁning set of the given
abelian code. Speciﬁcally this set has to satisfy the conditions described in the
following deﬁnition. For any subset A ⊂Zr1 × Zr2 we denote its projection onto
the ﬁrst coordinate by A1.

Reed-Muller Codes: Information Sets from Deﬁning Sets
33
Deﬁnition 5. Let D be a union of 2-orbits modulo (r1, r2) and D ⊂D a com-
plete set of representatives. Then D is called a set of restricted representatives
if D1 is a complete set of representatives of the 2-cyclotomic cosets modulo r1
in D1.
Example 1. Consider r1 = 3, r2 = 5 and let C ⊆A(3, 5) be the abelian code with
deﬁning set D(C) = Q(1, 1) ∪Q(0, 1) ∪Q(2, 0), where Q(1, 1) = {(1, 1), (2, 2),
(1, 4), (2, 3)}, Q(0, 1) = {(0, 1), (0, 2), (0, 3), (0, 4)} and Q(2, 0) = {(1, 0), (2, 0)}.
Then, according to Deﬁnition 5, the set of representatives {(1, 1), (0, 1), (2, 0)}
is not restricted, because C3(1) = C3(2), while {(1, 1), (0, 1), (1, 0)} is indeed
restricted.
Now, let C ⊆A(r1, r2) be an abelian code with deﬁning set Dα ⊆Zr1 × Zr2,
with respect to α = {α1, α2}. Take D ⊂Dα a set of restricted representatives.
Given e1 ∈D1, let
R(e1) = {e2 ∈Zr2 | (e1, e2) ∈D}.
For each e1 ∈D1, we deﬁne
M(e1) =

e2∈R(e1)
m (e1, e2)
(2)
and we consider the values {M(e1)}e1∈D1. Then we denote
f1 = max
e1∈D1
{M(e1)}
and
fi = max
e1∈D1
{M(e1) | M(e1) < fi−1}.
So, we obtain the sequence
f1 > · · · > fs > 0 = fs+1,
(3)
that is, we denote by fs the minimum value of the parameters M(·) and we set
fs+1 = 0 by convention. Note that M(e1) > 0, for all e1 ∈D1, by deﬁnition.
From the previous values f• we deﬁne for i = 1, . . . , s
gi =

M(e1)≥fi
m(e1)
(4)
and then we obtain the sequence
g1 < g2 < · · · < gs.
(5)
Finally, we deﬁne the set
Γ(C) = {(i1, i2) ∈Zr1 × Zr2 | there exists
(6)
1 ≤j ≤s with fj+1 ≤i2 < fj, and 0 ≤i1 < gj}.
The following theorem, proved in [3] for any abelian code, establishes that
Γ(C) is a set of check positions for C, and consequently Γ(C) deﬁnes an infor-
mation set for C⊥.

34
J.J. Bernal and J.J. Sim´on
Theorem 1. Let r1, r2 be odd integers and let C be an abelian code in A(r1, r2)
with deﬁning set Dα(C) with respect to α = {α1, α2}. Then Γ(C) is a set of check
positions for C.
Let us observe that given an abelian code C ⊆A(r1, r2) with deﬁning set
Dα(C) if one chooses diﬀerent primitive roots of unity, say γ = {γ1, γ2}, then
the structure of the q-orbits in Dγ(C) is the same as in Dα(C), that is, we obtain
the same values for the parameters (1) and (2), so we get the same set of check
positions Γ(C) (see [4, p. 100]). That is why we do not use any reference to the
roots of unity taken in the notation of the set of check positions. So, in the rest
of the paper, for any abelian code C we denote its deﬁning set by D(C) and
the corresponding set of check positions by Γ(C) without any mention to the
primitive roots.
Example 2. We continue with the code C considered in Example 1. Then D(C) =
Q(1, 1)∪Q(0, 1)∪Q(2, 0) and we take D = {(1, 1), (0, 1), (1, 0)} as set of restricted
representatives. From (2) we have that M(1) = m(1, 1) + m(1, 0) = 2 + 1 = 3
and M(0) = m(0, 1) = 4. So f1 = 4 > f2 = 3 > f3 = 0. On the other hand, from
(4) we obtain g1 = m(0) = 1, g2 = g1 + m(1) = 3. Therefore,
Γ(C) = {(i1, i2) ∈Zr1 × Zr2 |
(1 ≤i2 < 4 and 0 ≤i1 < 1) or (0 ≤i2 < 3 and 0 ≤i1 < 3)}
= {(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)}
is a set of check positions for C.
Remark 1. In [3] we showed how to construct the previous set of check posi-
tions for any multidimensional abelian code. As we have already mentioned, in
this paper we only need to use the two-dimensional case which yields the same
information set that was introduced by H. Imai in [7].
4
Cyclic Codes as Two-Dimensional Cyclic Codes
We are going to construct an information set for the punctured cyclic code
of a Reed-Muller code viewed as a multidimensional abelian code. So, we are
interested in applying the results of the previous section when the original abelian
code is in fact cyclic.
Let C∗be a binary cyclic code with length n = r1 · r2. All throughout this
section we assume that gcd(r1, r2) = 1 and r1, r2 are odd. Let T : Zn →Zr1 ×Zr2
be the isomorphism given by the Chinese Remainder Theorem, and let us denote
T = (T1, T2); that is, Ti(e) is the projection of T(e) onto Zri, for i = 1, 2 and
any e ∈Zn.
Lemma 1. For every e ∈Zn the equality T (Cn(e)) = Q (T(e)) holds; in partic-
ular |Cn(e)| = |Q(T(e))|. Moreover, one has that the projection of Q(T(e)) onto
Zr1 is equal to Cr1(T1(e)).

Reed-Muller Codes: Information Sets from Deﬁning Sets
35
Let D∗= Dα(C∗) ⊆Zn be the deﬁning set of C∗with respect to an arbitrary
primitive n-th root of unity α. Then, since there exist integers η1, η2 such that
η1r1 +η2r2 = 1, we have that α1 = αη2r2 and α2 = αη1r1 are primitive r1-th and
r2-th roots of unity respectively. Set T(1) = (δ1, δ2); observe that gcd(δ1, r1) = 1
and gcd(δ2, r2) = 1. We deﬁne the abelian code C = CC∗⊆A(r1, r2) as the code
with deﬁning set D = D(C) = T(D∗), with respect to (β1, β2) = (α
δ−1
1
1
, α
δ−1
2
2
). In
this situation, we have that C is the image of C∗by the map
A(n) −→A(r1, r2)

aiXi →

bjlXjY l,
where bjl = ai if and only if T(i) = (j, l). Therefore, I is an information set for C
if and only if T −1(I) is an information set for C∗. Usually, we omit the reference
to the original cyclic code in the notation of the new abelian code, and we will
write C instead of CC∗; that reference will by clear by the context. For the rest
of this section we assume that we have ﬁxed an election of α and consequently,
the roots β1, β2 are also ﬁxed.
Remark 2. Let us observe that, since the deﬁning set D∗= Dα(C∗) depends on
the election of α, if we ﬁx another one β, we get a new deﬁning set Dβ(C∗) and
consequently we obtain a diﬀerent abelian code C. However, this new abelian
code has a deﬁning set with the same structure of q-orbits so it yields the same
set Γ(C) (see paragraph after Theorem 1).
On the other hand, it is easy to prove that any change of the isomorphism
T is equivalent to a change of the primitive root α in order to get the same
abelian code in A(r1, r2). Nevertheless, this implies that, since we get the same
set Γ(C), we could obtain a diﬀerent set of check positions T −1(Γ(C)) at the
end. Therefore, by the method we are describing in this section we could get at
most as many information sets as isomorphisms from Zn to Zr1 × Zr2 exist. To
simplify the development of this extended abstract we continue assuming that
we always manage the isomorphism given by the Chinese Remainder Theorem.
Then, the goal of this section is to describe the values of the parameters deﬁned
in (2), used to get a set of check positions for C (6), just in terms of the deﬁning
set of C∗. This will allow us to deﬁne an information set for the original cyclic
code C∗(via T) without any mention to the abelian code C.
Let D∗⊆D∗⊆Zn be a complete set of representatives of the 2-cyclotomic
cosets modulo n in D∗. As we have noted in Lemma 1, for any e ∈D∗, T(Cn(e)) =
Q(T(e)), so T(D∗) is a complete set of representatives of the 2-orbits modulo
(r1, r2) in T(D∗). However, it might not be a set of restricted representatives
according to Deﬁnition 5. Then we introduce the following deﬁnition.
Deﬁnition 6. Let D∗⊆D∗be a complete set of representatives of the 2-
cyclotomic cosets modulo n in D∗. Then D∗is said to be a suitable set of repre-
sentatives if T(D∗) is a set of restricted representatives of the 2-orbits in T(D∗).
The next lemma says that we will always be able to take a suitable set of
representatives in D∗.

36
J.J. Bernal and J.J. Sim´on
Lemma 2. Let D∗be the deﬁning set of a cyclic code C∗⊆A(n). Let C ⊆
A(r1, r2) be the abelian code with deﬁning set T(D∗). Then, there always exists
a suitable set of representatives D∗⊆D∗.
Example 3. Let us consider the binary cyclic code C∗⊆A(15) whose deﬁning
set, with respect to a 15-th primitive root of unity α, is the following union of
2-cyclotomic cosets modulo 15, D∗= {1, 2, 4, 8} ∪{3, 6, 12, 9} ∪{5, 10}. Take T :
Z15 →Z3×Z5 the isomorphism given by the Chinese Remainder Theorem. Then
C denotes the abelian code in A(3, 5) with deﬁning set D = T(D∗). The reader
may check that C is the code of Examples 1 and 2. The set B = {1, 3, 5} ⊆D∗
is a complete set of representatives of the 2-cyclotomic cosets in D∗; however
it is not a suitable set of representatives since T(B) = {(1, 1), (0, 3), (2, 0)} is
not a restricted set of representatives in D (note that C3(1) = C3(2)). We may
solve this problem by substituting 5 by 10. Then D∗= {1, 3, 10} is a suitable
set of representatives because T(D∗) = {(1, 1), (0, 3), (1, 0)} is a restricted set of
representatives in D.
Now we deal with the construction of a set of check positions for the abelian
code C = CC∗⊆A(r1, r2) just in terms of D∗, the deﬁning set of the cyclic
code C∗.
Given D∗⊆D∗a suitable set of representatives, we consider ∼the equiva-
lence relation on D∗given by the rule
a ∼b ∈D∗if and only if a ≡b
mod r1.
(7)
From now on, we denote by D∗a suitable set of representatives and U ⊆D∗
a complete set of representatives of the equivalence classes related to ∼. In
addition, for any u ∈U we write
O(u) = {a ∈D∗| a ∼u}.
Observe that if D = T(D∗) then D1 = T1(U). Furthermore, for any e ∈D∗
there exists a unique u ∈U such that T1(u) = T1(e). By abuse of notation, we
will write
Cr1(e) = Cr1(T1(e)) = Cr1(T1(u)) = Cr1(u).
Example 4. Following Example 3, from the suitable set of representatives D∗=
{1, 3, 10} we may deﬁne, for instance, U = {1, 3}. Note that O(1) = {1, 10}.
The following theorem says how to obtain the values M(·) (deﬁned in (2))
corresponding with the set D = T(D∗) ⊂Zr1 × Zr2, in terms of the elements in
U ⊆Zn.
Theorem 2. For each (e1, e2) ∈D = T(D∗), there exists a unique u ∈U such
that T1(u) = e1 and
M(e1) =
1
|Cr1(u)|

v∈O(u)
|Cn(v)| =

v∈O(u)
|C2|Cr1 (u)|,r2(T2(v))|.

Reed-Muller Codes: Information Sets from Deﬁning Sets
37
Therefore, ﬁxed a n-th primitive root of unity α, for a given cyclic code
C∗in A(n) with deﬁning set with respect to α, D∗, we consider C ⊆A(r1, r2)
the abelian code with deﬁning set D = T(D∗) (taking as reference the suitable
primitive roots of unity mentioned at the beginning of this section). Then, we
have obtained that we can take a suitable set of representatives D∗⊆D∗and a
set U ⊆D∗, with T1(U) = D1, in such a way that we are able to construct the
set of check positions given in (6) for C in terms of U as follows:
Since for any e1 ∈D1 there exists a unique element u ∈U that satisﬁes
T1(u) = e1, by abuse of notation, we may write M(u) = M(T1(u)) = M(e1).
Then, the set of values (2) can be described as
⎧
⎨
⎩M(u) =
1
|Cr1(u)|

v∈O(u)
|Cn(v)| | u ∈U
⎫
⎬
⎭
(8)
which yields the sequence f1 > · · · > fs > 0 = fs+1 (see (3)). On the other
hand, for any k = 1, . . . , s the values (4) can be computed as
gk =

u∈U
M(u)≥fk
|Cr1(u)|
which give us the sequence g1 < g2 < · · · < gs (see (5)) and hence the set Γ(C).
Finally, T −1(Γ(C)) is a set of check positions for C∗described in terms of U
uniquely.
Example 5. We apply the results in this section to the code C∗given in Exam-
ple 3. Recall that its deﬁning set is D∗= {1, 2, 4, 8} ∪{3, 6, 12, 9} ∪{5, 10}. We
have chosen D∗= {1, 3, 10} as suitable set of representatives and U = {1, 3}.
Then, from (8) we have
M(1) =
1
|C3(1)| (|C15(1)| + |C15(10)|) = 1
2(4 + 2) = 3,
M(3) =
1
|C3(3)| · |C15(3)| = 4,
and so f1 = 4 > f2 = 3 > f3 = 0. On the other hand, g1 = m(0) = 1 < g2 =
1 + m(1) = 3. These sequences yield the set of check positions for C
Γ(C) = {(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)}.
Finally, we have that T −1(Γ(C)) = {0, 1, 2, 3, 5, 6, 7, 10, 11, 12} is a set of check
positions for C∗.
5
Reed-Muller Codes
In this section we shall introduce Reed-Muller codes from the group-algebra
point of view. Speciﬁcally, we are going to present Reed-Muller codes as a type

38
J.J. Bernal and J.J. Sim´on
of codes contained in the family of so-called aﬃne-invariant extended cyclic codes
(see, for instance, [1,2] or [6]).
Recall that F denotes the binary ﬁeld. Let G be the additive subgroup of the
ﬁeld of 2m elements. So G is an elementary abelian group of order |G| = 2m
and G∗= G \ {0} is a cyclic group. From now on we denote n = 2m −1. We
consider the group algebra FG which will be the ambient space for Reed-Muller
codes. We denote the elements in FG as 
g∈G agXg. Notice that X0 is the unit
element in FG.
The following deﬁnitions introduce the family of aﬃne-invariant extended
cyclic codes. All throughout this section we ﬁx α, a generator of the cyclic group
G∗, that is, a primitive n-th root of unity.
Deﬁnition 7. Let α be a generator of G∗. A code C ⊆FG is an extended cyclic
code if for any 
g∈G
agXg ∈C one has that 
g∈G
agXαg ∈C and 
g∈G
ag = 0.
Deﬁnition 8. We say that an extended cyclic code C ⊆FG is aﬃne-invariant
if for any 
g∈G
agXg ∈C one has that 
g∈G
agXhg+k belongs to C for all h, k ∈
G, h ̸= 0.
It is clear that if C ⊆FG is aﬃne-invariant then C is an ideal in FG and
C∗⊆FG∗, the punctured code at the position X0, is cyclic in the sense that it
is the projection to FG∗of the image of a cyclic code via the map
F[X]/⟨Xn −1⟩−→FG
n−1

i=0
aiXi →

−
n−1

i=0
ai

X0 +
n−1

i=0
aiXαi,
(9)
where α is the ﬁxed n-th root of unity.
Now, for any s ∈{0, . . . , n = 2m −1} we consider the F-linear map φs :
FG →G given by
φs
⎛
⎝
g∈G
agXg
⎞
⎠=

g∈G
aggs
where we assume 00 = 1 ∈F by convention.
Deﬁnition 9. Let C ⊆FG be an aﬃne-invariant code. The set
D(C) = {i | φi(x) = 0 for all x ∈C}
is called the deﬁning set of C.
Note ﬁrst that since C is an extended-cyclic code, one has that 0 ∈D(C)
because φ0


g∈G
agXg

= 
g∈G
agg0 = 
g∈G
ag. Furthermore, it follows from the
equality φ2s(x) = (φs(x))2 (x ∈C) that D(C) is a union of 2-cyclotomic cosets

Reed-Muller Codes: Information Sets from Deﬁning Sets
39
modulo n. On the other hand, keeping in mind the map (9), one has that the
zeros of the cyclic code C∗are {αs | s ∈D(C), s ̸= 0}; that is, the deﬁning
set of C∗, according to Deﬁnition 1 (and with respect to that root of unity), is
D(C∗) = D(C) \ {0}.
It is easy to prove that any aﬃne-invariant code is totally determined by its
deﬁning set. Conversely, any subset of {0, . . . , n} which is a union of 2-cyclotomic
cosets and contains 0 deﬁnes an aﬃne-invariant code in FG.
Remark 3. It may occur that n, 0 ∈D(C) which could yield confusion in order
to consider the 2-cyclotomic cosets modulo n. Those elements are considered
distinct and they indicate diﬀerent properties of the code C; to wit, 0 always
belongs to D(C) because C is an extended cyclic code, while if n belongs to D(C)
then the cyclic code C∗is even-like which implies that C is a trivial extension.
Finally, to introduce the family of Reed-Muller codes as aﬃne-invariant codes
we need to recall the notions of binary expansion and 2-weight. For any natural
number k its binary expansion is the sum 
r≥0 kr2r = k with kr = 0, 1. The
2-weight or simply weight of k is wt(k) = 
r≥0 kr.
Deﬁnition 10. Let 0 < ρ < m. The Reed-Muller code of order ρ and length 2m,
denoted by R(ρ, m), is the aﬃne-invariant code in FG with deﬁning set
D(R(ρ, m)) = {i | 0 ≤i < 2m −1 and wt(i) < m −ρ}.
From the classical point of view, Reed-Muller codes are also deﬁned for the
cases ρ = 0, m, but they correspond with the trivial cases R(m, m) = FG and
R(0, m) = ⟨
g∈G g⟩(the repetition code) which do not have interest in the
context of this paper. On the other hand, it is well known that R(ρ, m)⊥=
R(m −ρ −1, m) and that R(m −1, m) is the code of all even weight vectors in
FG (the reader may see [2]). Then R(m −1, m)⊥= R(0, m) and therefore the
problem of searching for information sets has no interest in the case ρ = m −1,
so in the rest of the paper we will assume that ρ < m −1.
As a consequence of Deﬁnition 10 we have that the deﬁning set of the punc-
tured code R∗(ρ, m), at the position X0 and with respect to the ﬁxed n-th root
of unity α, is given by the following union of cyclotomic cosets modulo n
D(R∗(ρ, m)) =

i∈D(R(ρ,m))\{0}
Cn(i).
Remark 4. Note that, ﬁxed any primitive root of unity α, following the notation
used to describe the elements in FG,

−
n−1

i=0
ai

X0 +
n−1

i=0
aiXαi ∈FG, a set
I ⊆{0, α0, . . . , αn−1} is an information set for a code C ⊆FG, with dimension
k, if |I| = k and CI = F|I|.
On the other hand, since R∗(ρ, m) is contained in FG∗we have that any
information set for it will be a subset of {α0, . . . , αn−1}. Obviously, an informa-
tion set for R∗(ρ, m) is an information set for R(ρ, m) too. However, note that
if Γ is a set of check positions for R∗(ρ, m) then a set of check positions for
R(ρ, m) is Γ ∪{0}.

40
J.J. Bernal and J.J. Sim´on
In the following sections we will deal with the application of the results
contained in Sect. 4 to the cyclic code R∗(ρ, m) in order to obtain an information
set for R(ρ, m). To use a notation congruent with that used in that section we
write D∗= D(R∗(ρ, m)). We assume that there exist integers r1, r2 such that
n = 2m −1 = r1 · r2 with r1, r2 odd and gcd(r1, r2) = 1. Now, we ﬁx some
notation and introduce some deﬁnitions and basic results that will be needed.
For any integer 0 < K < m −1 we deﬁne
Ω(K) = {0 < j < 2m −1 | wt(j) = K}
=

2t1 + · · · + 2tK | 0 ≤t1 < · · · < tK < m

and hence,
D∗=
m−ρ−1

K=1
{0 < j < 2m −1 | wt(j) = K} =
m−ρ−1

K=1
Ω(K).
Example 6. Take m = 4. Then n = 24 −1 = 15, r1 = 3, r2 = 5. For these
parameters one has that
Ω(1) = {1, 2, 4, 8}, Ω(2) = {3, 5, 6, 9, 10, 12} and Ω(3) = {7, 11, 13, 14}.
The code R(1, 4) has deﬁning set D(R(1, 4)) = {0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12}
and so D∗= D(R∗(1, 4)) = Ω(1)∪Ω(2) = {1, 2, 3, 4, 5, 6, 8, 9, 10, 12}. The reader
may check that D∗corresponds to the code of Examples 1 to 5.
Finally, the code R(2, 4) has deﬁning set D(R(2, 4)) = {0, 1, 2, 4, 8}, which
implies D∗= D(R∗(2, 4)) = Ω(1).
Now, we take D∗a suitable set of representatives in D∗and a set U as in the
previous section (see Deﬁnition 6 and subsequent paragraphs).
We are interested in handling convenient elements in the ﬁxed suitable set of
representatives in order to compute the necessary cyclotomic cosets. It is clear
that if e = 2s ∈Ω(1), with 0 ≤s < m, then Cn(e) = Cn(1). On the other hand,
it is easy to check that given any element e ∈Ω(2) there always exists
e′ = 1 + 2t,
(10)
with t ≤
 m
2

such that Cn(e) = Cn(e′).
As we will see in the next section, this fact becomes essential for our purposes.
In addition, it is important to note that the elements in a given suitable set of
representatives might not verify condition (10); we include the next example to
show that case.
Example 7. Let us consider the Reed-Muller code R(3, 6). So m = 6, n = 26−1 =
63, r1 = 7, r2 = 9. For these parameters we have
Ω(1) = C63(1) = {1, 2, 4, 8, 16, 32}

Reed-Muller Codes: Information Sets from Deﬁning Sets
41
and
Ω(2) = C63(3) ∪C63(5) ∪C63(9) = {3, 5, 6, 9, 10, 12, 17, 18, 20, 24, 33, 34, 36, 40, 48}.
The deﬁning set of R∗(3, 6) is D∗= Ω(1) ∪Ω(2). The set D∗= {1, 3, 5, 36} is a
suitable set of representatives according to Deﬁnition 6, however while 1, 3 and
5 satisfy condition (10) the element 36 = 22 +25 does not satisfy it. There exists
a unique element in C63(36) = C63(9) that satisﬁes that condition, to wit, 9.
6
Information Sets for First-Order Reed-Muller Codes
In this section we are applying the results of Sect. 4 to the punctured codes of
ﬁrst-order Reed-Muller codes. Speciﬁcally, we shall construct an information set
for the punctured code R∗(1, m) by using those techniques and then we shall
give an information set for the Reed-Muller code R(1, m). To use the mentioned
results we need to assume that there exist odd integers r1, r2 such that n =
2m −1 = r1 · r2 and gcd(r1, r2) = 1, r1, r2 > 1.
All throughout this section we ﬁx a primitive n-th root of unity α; recall
that T : Zn →Zr1 × Zr2 is the isomorphism of groups given by the Chinese
Remainder Theorem.
We have two diﬀerent possibilities to get an information set for the code
R(1, m); to wit, from an information set of R∗(1, m), which comes from the
deﬁning set of R∗(1, m), and from a check of set positions of R(1, m)⊥= R(m −
2, m), which depends on the deﬁning set of R∗(m −2, m). In general it is more
convenient to develop the results in terms of R∗(m −2, m) because its deﬁning
set is much smaller than that of R∗(1, m).
Let us denote C∗= R∗(m −2, m). Then, following the notation ﬁxed in the
previous section, we have that
D∗= Ω(1)
where Ω(1) = {2t
|
0 ≤t < m}. Observe that Ω(1) = Cn(1) so we take
D∗= {1} as our suitable set of representatives (see Deﬁnition 6). Then U = {1}.
The following theorem gives us the description of an information set for the
code R(1, m). We denote by Ordr1(2) the order of 2 modulo r1, that is, the
smallest integer such that 2Ordr1(2) ≡1 modulo r1.
Theorem 3. Suppose that n = 2m −1 = r1 · r2, where r1, r2 are odd integers
such that gcd(r1, r2) = 1, r1, r2 > 1. Let C∗= R∗(m −2, m) and a = Ordr1(2).
Let D∗⊆Zn be the deﬁning set of C∗with respect to α, a primitive n-th root of
unity, and let C ⊆A(r1, r2) be the abelian code with deﬁning set D(C) = T(D∗).
Then, the set T −1 (Γ) where
Γ = Γ(C) =

(i1, i2) ∈Zr1 × Zr2 | 0 ≤i1 < a, 0 ≤i2 < m
a

is a set of check positions for R∗(m −2, m). Furthermore, {0, αi | i ∈T −1 (Γ)}
is an information set for R(1, m) and {αi | i /∈T −1 (Γ)} is an information set
for R(m −2, m).

42
J.J. Bernal and J.J. Sim´on
Example 8. The ﬁrst value for m that satisﬁes the required conditions is m = 4.
In this case, n = 24 −1 = 15, r1 = 3, r2 = 5, Ord3(2) = 2. So, let us give an
information set for R(1, 4). By Theorem 3 we have that
Γ = {(i1, i2) ∈Zr1 × Zr2 | 0 ≤i1 < 2, 0 ≤i2 < 2} = {(0, 0), (1, 0), (0, 1), (1, 1)}.
Then, we have that T −1(Γ) = {0, 1, 6, 10} and then {0, 1, α, α6, α10} is an infor-
mation set for R(1, 4). Moreover {αi | i ̸= 0, 1, 6, 10} is an information set for
R(2, 4).
The next value for m is m = 6. In this case, n = 26 −1 = 63, r1 = 7,
r2 = 9, a = 3. So
Γ = {(i1, i2) ∈Zr1 × Zr2 such that 0 ≤i1 < 3, 0 ≤i2 < 2}
= {(0, 0), (1, 0), (2, 0), (0, 1), (1, 1), (2, 1)}.
Since T −1(Γ) = {0, 1, 9, 28, 36, 37} one has that {0, 1, α, α9, α28, α36, α37} is an
information set for R(1, 6) and {αi | i ̸= 0, 1, 9, 28, 36, 37} is an information set
for R(4, 6).
Finally, let us see the case m = 8, that is, the Reed-Muller codes of length 256.
We consider the decomposition of n = 28 −1 = 255 deﬁned as (r1 = 3, r2 = 85).
Then T −1(Γ) = {0, 1, 3, 85, 87, 88, 171, 172}.
7
Information Sets for Second-Order Reed-Muller
Codes
In this section we deal with second-order Reed-Muller codes. As in the pre-
vious section, we shall apply the results of Sect. 4, so, again, we need to
assume that there exist integers r1, r2 such that n = 2m −1 = r1 · r2 and
gcd(r1, r2) = 1, r1, r2 > 1. Again, we ﬁx a primitive n-th root of unity α.
In a similar way to the case of R(1, m), we have two possibilities to get an
information set for the code R∗(2, m), to wit, (Zr1 × Zr2) \ Γ(R∗(2, m)) and
Γ

R∗(2, m)⊥	
. Once more, we consider more convenient to develop the results
in terms of R(m −3, m) = R∗(2, m)⊥.
Throughout this section we denote C∗= R∗(m −3, m). In this case we have
that
D∗= Ω(1) ∪Ω(2)
where Ω(1) = {2t | 0 ≤t < m} and Ω(2) = {2t1 + 2t2 | 0 ≤t1 < t2 < m}. Let
D∗⊆D∗be a suitable set of representatives and take U ⊆D∗a complete set
of representatives of the equivalence classes modulo r1 (see (7)). We will always
assume that 1 ∈U ⊆D∗(recall that Ω(1) = Cn(1)).
To get the expressions (8) we need to compute the cardinalities |Cn(e)|, for
all e ∈D∗, and |Cr1(u)| for any element u in the ﬁxed set U. As we have seen
in the previous section, these computations can be made directly in the case of
elements in Ω(1); however, they turn to be much more complicated when we
work with the set Ω(2). So, we impose some conditions on r1 in order to make

Reed-Muller Codes: Information Sets from Deﬁning Sets
43
the mentioned computations avaliable, to wit, all throughout we also assume
that r1 = 2a −1, with a an integer. Then, we can sum up the restrictions on the
paremeters as follows
n = 2m −1 = r1 · r2, where r1 = 2a −1 and gcd(r1, r2) = 1, r1, r2 > 1.
(11)
Note that from these conditions one follows that a = Ordr1(2) and so the
notation is consistent with that used in Theorem 3. In what follows we write
m = ab.
The ﬁrst lemma, probably a well-known result, shows that the value of the
cardinalities |Cn(e)| and |Cr1(e)| can be easily computed for the elements in
Ω(2) of the form 1 + 2t.
Lemma 3. Let μ, ν integers such that μ = 2ν −1. Then |Cμ (1)| = ν and for
any natural number 0 < t ≤ν −1


Cμ

1 + 2t	

 =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
ν
2
in case
t = ν
2
ν
otherwise
As we have observed in the previous section, the elements in the suitable set
D∗might not verify condition (10). However, we can relate any element in D∗
with another one in the same 2-cyclotomic coset modulo n that satisﬁes that
condition. The following result details this relationship.
Proposition 1. Let D∗be a suitable set of representatives. There exists a bijec-
tion ε : D∗\ {1} −→{1, . . . ,
 m
2

} where ε(e) is the unique integer such that
1 + 2ε(e) belongs to Cn(e) and veriﬁes condition (10).
Remark 5. Observe that for all e = 2t1 + 2t2 ∈Ω(2) one has that 1 + 2δ, with
δ = min{t2 −t1, m −t2 + t1}, belongs to Cn(e) and veriﬁes (10), therefore
ε(e) = min{t2 −t1, m −t2 + t1}.
Under the notation introduced by Proposition 1 we can say that for any e ∈
Ω(2) the element 1+2ε(e) is the unique element in Cn(e) satisfying condition (10).
Now, from Proposition 1 and Lemma 3 we obtain the next result which gives
us the desired cardinalities.
Proposition 2. Let n = 2m −1 = r1 · r2 satisfying (11), that is, m = ab, with
a such that r1 = 2a −1 and (r1, n/r1) = 1. For any e ∈Ω(2) one has that
1. |Cn(e)| =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
m
2
in case
ε(e) = m
2
m
otherwise
2. |Cr1(e)| =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
a
2
in case
ε(e) ≡a
2 mod a
a
otherwise

44
J.J. Bernal and J.J. Sim´on
Now, all that is required to obtain the expressions (8) is the description of
the sets O(u) with u ∈U, that is, O(1) and O(e) with e ∈Ω(2) ∩U. The next
results solve this problem. The ﬁrst one gives the information we need about
O(1). Recall that all throughout D∗denotes a suitable set of representatives.
Proposition 3. 1. O(1) = {1} ∪{e ∈D∗| ε(e) = λa with 1 ≤λ ≤
 b
2

}.
2. |O(1)| = 1 + ⌊b/2⌋.
3. |Cr1(1)| = a.
4. If b is even then ε−1(m/2) ∈O(1),


Cn(ε−1(m/2))


 = m/2, and |Cn(e)| = m
for any e ∈O(1) \ {ε−1(m/2)}.
5. If b is odd then |Cn(e)| = m for any e ∈O(1).
Remark 6. As we have already seen we assume that 1 ∈U ⊆D∗. Furthermore,
depending on the parity of a and b we will take the following elements in U by
convention:
1. If m is even we also assume that ε−1(m/2) ∈U unless ε−1(m/2) ∈O(1).
2. If a is even we assume that ε−1(a/2) ∈U unless ε−1(a/2) ∈O(ε−1(m/2)).
Observe that ε−1(a/2) never belongs to O(1).
The next result yields the description of the set O(e) for any e ∈U \{1}. Let
us observe that e ∈U \ {1} implies e ∈U ∩Ω(2).
Proposition 4. Let e ∈U \ {1}. Then
O(e) = {e′ ∈D∗| ε(e′) ≡ε(e) mod a or ε(e′) ≡a −ε(e) mod a}.
The following propositions complete the information about the sets O(e),
with e ∈U \ {1}, by making a distinction between the cases b even and b odd
(m = ab).
Proposition 5. Let e ∈U \ {1} and suppose that b is even. Then
1. For any e′ ∈O(e) one has that |Cn(e′)| = m.
2. If a is odd then |O(e)| = b and |Cr1(e)| = a.
3. If a is even then in case e = ε−1(a/2) one has that |O(e)| = b/2, |Cr1(e)| =
a/2 and otherwise |O(e)| = b, |Cr1(e)| = a.
Proposition 6. Let e ∈U \ {1} and suppose that b is odd. Then |Cn(e′)| = m
for all e′ ∈O(e) \ {e} and
1. If a is odd then |O(e)| = b, |Cr1(e)| = a and |Cn(e)| = m.
2. If a is even then
(a) If e
=
ε−1(m/2) then |O(e)|
=
(b + 1)/2, |Cn(e)|
=
m/2 and
|Cr1(e)| = a/2.
(b) If e ̸= ε−1(m/2) then |O(e)| = b, |Cn(e′)| = m and |Cr1(e)| = a.

Reed-Muller Codes: Information Sets from Deﬁning Sets
45
The last result we need before enunciating our main theorem gives us the
cardinality of any election of the set U. It uses the mentioned concept of
2-weight of an integer (see paragraph before Deﬁnition 10). We talk about the
2-weight of a 2-cyclotomic coset when one of its elements (and so all of them)
has that 2-weight.
Lemma 4. For any election of the set of representatives U one has that
|U| = 1 + |{Cr1(e) | e ∈U ∩Ω(2)}| = 1 + ⌊a/2⌋.
Now, we can present the main result for second-order Reed-Muller codes.
Theorem 4. Let C∗= R∗(m −3, m). Suppose that n = 2m −1 = r1 · r2, where
m = ab, r1 = 2a −1 and gcd(r1, r2) = 1, r1, r2 > 1. Let D∗⊆Zn be the deﬁning
set of C∗, with respect to α, a primitive n-th root of unity, and C ⊆A(r1, r2) the
abelian code with deﬁning set D(C) = T(D∗). Then, the values that appear in
(3) and (5) are
f1 = b2, f2 = b(b + 1)
2
and
g1 = a(a −1)
2
, g2 = a(a + 1)
2
,
respectively.
Therefore the set T −1 (Γ) where
Γ = Γ(C) = {(i1, i2) ∈Zr1 × Zr2 such that

0 ≤i1 < a(a −1)
2
and 0 ≤i2 < b2

or
a(a −1)
2
≤i1 < a(a + 1)
2
and 0 ≤i2 < b(b + 1)
2
 
is a set of check positions for R∗(m −3, m). Furthermore, {0, αi | i ∈T −1 (Γ)}
is an information set for R(2, m) and {αi | i /∈T −1 (Γ)} is an information set
for R(m −3, m).
Example 9. The ﬁrst value for m that satisﬁes conditions (11) is m = 4. In this
case, n = 24 −1 = 15, r1 = 3, r2 = 5, a = b = 2. So, let us give an information
set for R(2, 4). By Theorem 4 we have that
Γ = {(0, 0), (1, 0), (2, 0), (0, 1), (1, 1), (2, 1), (0, 2), (1, 2), (2, 2), (0, 3)}.
Then, we have T −1(Γ) = {0, 1, 2, 3, 5, 6, 7, 10, 11, 12}. So {0, αi | i ∈T −1 (Γ)} is
an information set for R(2, 4) and {αi | i /∈T −1 (Γ)} is an information set for
R(1, 4).
The next value for m is m = 6. In this case, n = 26 −1 = 63, r1 = 7,
r2 = 9, a = 3 and b = 2. So

46
J.J. Bernal and J.J. Sim´on
Γ = {(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (0, 1), (1, 1), (2, 1), (3, 1),
(4, 1), (5, 1), (0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (5, 2), (0, 3), (1, 3), (2, 3)}.
Then, we obtain
T −1(Γ) = {0, 1, 2, 9, 10, 11, 18, 19, 21, 28, 29, 30, 36, 37, 38, 45, 46, 47, 54, 56, 57},
so we have that {0, αi | i ∈T −1(Γ)} is an information set for R(2, 6) and
{αi | i /∈T −1(Γ)} is an information set for R(3, 6).
Finally, we see the case m = 8. We consider the decompositions of n =
28 −1 = 255 deﬁned by (r1 = 3, r2 = 85). Then,
T −1(Γ) = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 99,
170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 183}.
So we have that {0, αi | i ∈T −1(Γ)} is an information set for R(2, 8) and
{αi | i /∈T −1(Γ)} is an information set for R(5, 8).
8
Conclusions
We have described explicitly how to construct information sets for ﬁrst and
second order Reed-Muller codes only in terms of their deﬁning sets as aﬃne-
invariant codes. To get these results we have used those given in [3]. The step
from ﬁrst to second order involves much more complicated technical results, so
it is an open problem to study the generalization to order greater than 2. On
the other hand, in [4] we show how to use the special structure of this kind of
information sets to apply the permutation decoding algorithm, so it is expected
that from the new information sets introduced in this paper we can study the
applicability of that decoding procedure in a diﬀerent way from that showed in
[8,9] or [11].
References
1. Huﬀman, W.C.: Codes and groups. In: Pless, V.S., Huﬀman, W.C., Brualdi, R.A.
(eds.) Handbook of Coding Theory, vol II. North-Holland, Amsterdam (1998)
2. Assmus Jr., E.F., Key, J.D.: Polynomial codes and ﬁnite geometries. In: Pless,
V.S., Huﬀman, W.C., Brualdi, R.A. (eds.) Handbook of Coding Theory, vol II.
North-Holland, Amsterdam (1998)
3. Bernal, J.J., Sim´on, J.J.: Information sets from deﬁning sets in abelian codes. IEEE
Trans. Inform. Theory 57(12), 7990–7999 (2011)
4. Bernal, J.J.: C´odigos de grupo. Conjuntos de informaci´on. Decodiﬁcaci´on por per-
mutaci´on. Ph.D. thesis (2011)
5. Blokhuis, A., Moorhouse, G.E.: Some p-ranks related to orthogonal spaces. J. Alge-
braic Combin. 4, 295–316 (1995)
6. Charpin, P.: Codes cycliques etendus invariants sous le group aﬃne. These de
Doctorat d’Etat, Universite Par´es VII (1987)

Reed-Muller Codes: Information Sets from Deﬁning Sets
47
7. Imai, H.: A theory of two-dimensional cyclic codes. Inform. Control 34, 1–21 (1977)
8. Key, J.D., McDonough, T.P., Mavron, V.C.: Partial permutation decoding for
codes from ﬁnite planes. Eur. J. Combin. 26, 665–682 (2005)
9. Key, J.D., McDonough, T.P., Mavron, V.C.: Information sets and partial permu-
tation decoding for codes from ﬁnite geometries. Finite Fields Appl. 12, 232–247
(2006)
10. Moorhouse, G.E.: Bruck nets, codes and characters of loops. Des. Codes Cryptogr.
1, 7–29 (1991)
11. Seneviratne, P.: Permutation decoding for hte ﬁrst-order Reed-Muller codes. Dis-
crete Math. 309, 1967–1970 (2009)

Distance Properties of Short LDPC Codes
and Their Impact on the BP, ML
and Near-ML Decoding Performance
Irina E. Bocharova1,2, Boris D. Kudryashov1, Vitaly Skachek2(B),
and Yauhen Yakimenka2
1 St. Petersburg University of Information Technologies, Mechanics and Optics,
St. Petersburg 197101, Russia
kudryashov boris@bk.ru
2 University of Tartu, Tartu 50409, Estonia
{irinaboc,vitaly,yauhen}@ut.ee
Abstract. Parameters of LDPC codes, such as minimum distance, stop-
ping distance, stopping redundancy, girth of the Tanner graph, and their
inﬂuence on the frame error rate performance of the BP, ML and near-
ML decoding over a BEC and an AWGN channel are studied. Both ran-
dom and structured LDPC codes are considered. In particular, the BP
decoding is applied to the code parity-check matrices with an increasing
number of redundant rows, and the convergence of the performance to
that of the ML decoding is analyzed. A comparison of the simulated BP,
ML, and near-ML performance with the improved theoretical bounds on
the error probability based on the exact weight spectrum coeﬃcients and
the exact stopping size spectrum coeﬃcients is presented. It is observed
that decoding performance very close to the ML decoding performance
can be achieved with a relatively small number of redundant rows for
some codes, for both the BEC and the AWGN channels.
Keywords: LDPC code · Minimum distance · Stopping distance · Stop-
ping redundancy · BP decoding · ML decoding
1
Introduction
It is well-known that typically binary LDPC codes have minimum distances
which are smaller than those of the best known linear codes of the same rate
and length. It is not surprising, since minimum distance does not play an impor-
tant role in iterative (belief propagation (BP)) decoding. On the other hand, a
signiﬁcant gap in the frame error rate (FER) performance of BP and maximum-
likelihood (ML) decoding motivates developing near-ML decoding algorithms for
LDPC codes.
This work is supported in part by the Norwegian-Estonian Research Cooperation
Programme under the grant EMP133 and by the Estonian Research Council under
the grant PUT405.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 48–61, 2017.
DOI: 10.1007/978-3-319-66278-7 5

Distance Properties of Short LDPC Codes and Their Impact
49
There are two main approaches to improving the BP decoding performance.
First one is based on post-processing in case of BP decoder failure. Diﬀerent
post-processing techniques for decoding of binary LDPC codes over additive
white Gaussian noise (AWGN) channels are studied in [7,11,24,31]. A similar
approach to decoding of nonbinary LDPC codes over extensions of the binary
Galois ﬁeld is considered in [2]. Near-ML decoding algorithms for LDPC codes
over binary erasure channel (BEC) can be found in [16,21,23].
The second approach is based on identifying and destroying speciﬁc structural
conﬁgurations such as trapping and stopping sets of the Tanner graph of the
code. In particular, this can be done by adding redundant rows to the code
parity-check matrix (see, for example, [14,17,19]).
Suboptimality of the above modiﬁcations of BP decoding rises the following
question: which properties of LDPC codes and to what extent inﬂuence their
decoding FER performance? In this paper, we are trying to partially answer
this question by studying short LDPC codes. We consider both binary LDPC
codes and binary images of nonbinary random LDPC codes over extensions
of the binary ﬁeld, as well as quasi-cyclic (QC) LDPC codes constructed by
using an optimization technique in [4]. Parameters such as minimum distance,
stopping distance, girth of the Tanner graph and estimates on the stopping
redundancy are tabulated. Near-ML decoding based on adding redundant rows
to the code parity-check matrix is analyzed. Simulated over the BEC and the
AWGN channel, the FER performance of the BP, ML and near-ML decoding of
these classes of LDPC codes is presented and compared to the improved upper
bounds on the performance of ML decoding of regular LDPC codes [5] over the
corresponding channels. The presented error probability bounds rely on precise
average enumerators for given ensembles which makes these bounds tighter than
known bounds (see e.g. [28]). By using an approach similar to that in [5], an
improved upper bound on the performance of the BP decoding of binary images
of nonbinary regular LDPC codes over BEC is presented.
The paper is organized as follows. In Sect. 2, all necessary notations and
deﬁnitions are given. In Sect. 3, a near-ML decoding method, which is based on
adding redundant rows to the code parity-check matrix, is revisited. In Sect. 4,
a recurrent procedure for computing the exact coeﬃcients of the weight and
stopping set size spectra is described. The improved upper bound on the ensem-
ble average performance of the BP decoding over BEC is derived. Tables of the
computed code parameters along with the simulation results for the BP, ML and
near-ML decoding are presented in Sect. 5. A comparison with the theoretical
bounds is done and conclusions are drawn in Sect. 6.
2
Preliminaries
2.1
Ensembles of Binary and Binary Images of Nonbinary Regular
LDPC Codes
For a binary linear [n, k] code C of rate R = k/n denote by r = n −k its redun-
dancy. We use a notation {An,w}0≤w≤n for a set of code weight enumerators,

50
I.E. Bocharova et al.
where An,w is a number of codewords of weight w. Let H be an r×n parity-check
matrix which deﬁnes C.
By viewing H as a biadjacency matrix [1], we obtain a corresponding bipar-
tite Tanner graph. The girth g is the length of the shortest cycle in the Tanner
graph.
When decoded over a BEC, the FER performance of the BP decoding is
determined by the size of the smallest stopping set called stopping distance
dstop (see, for example, [9]). In turn, a stopping set is deﬁned as a subset of
indices of columns in a parity-check matrix, such that a matrix constructed
from these columns does not have a row of weight one. The asymptotic behavior
of a stopping set distribution for ensembles of binary LDPC codes is studied
in [22]. In this paper, we study both the average performance of the ensembles
of random LDPC codes and of QC LDPC codes widely used in practical schemes.
Two ensembles of random regular LDPC codes are studied below. First we
study the Gallager ensemble [13] of (J, K)-regular LDPC codes, where J and K
denote the number of ones in each column and in each row of the code parity-
check matrix, respectively. Codes of this ensemble are determined by random
parity-check matrices H , which consist of the strips H i of width M = r/J rows
each, i = 1, 2, . . . , J. All strips are random column permutations of the strip
where the jth row contains K ones in positions (j−1)K+1, (j−1)K+2, . . . , jK,
for j = 1, 2, . . . , n/K.
Next, we study the ensemble of binary (J, K)-regular LDPC codes, which is
a special case of the ensemble described in [26, Deﬁnition 3.15]. We refer to this
ensemble as the Richardson-Urbanke (RU) ensemble of (J, K)-regular LDPC
codes.
For a ∈{1, 2, ...} denote by am a sequence (a, a, ..., a) of m identical
symbols a. In order to construct an r × n parity-check matrix H of an LDPC
code from the RU ensemble, one does the following:
– construct the sequence a = (1J, 2J, ..., nJ);
– apply a random permutation b = π(a) to obtain a sequence b = (b1, ..., bN),
where N = Kr = Jn;
– set to one the entries in the ﬁrst row of H in columns b1, ..., bK, the entries
in the second row of H in columns bK+1, ..., b2K, etc. The remaining entries
of H are zeros.
In fact, an LDPC code from the RU ensemble is (J, K)-regular if for a given
permutation π all elements of subsequences (biK−K+1, ..., biK) are diﬀerent for
all i = 1, ..., r. It is shown in [20] that the fraction of regular codes among the
RU LDPC codes is roughly
exp

−1
2(K −1)(J −1)

,
which means that most of the RU codes are irregular. In what follows, we ignore
this fact and interpret the RU LDPC codes as the (J, K)-regular codes, and call
them “almost regular”.

Distance Properties of Short LDPC Codes and Their Impact
51
Generally, the design rate R = 1 −J/K is a lower bound on the actual code
rate since the rank of randomly constructed parity-check matrix can be smaller
than the number of its rows. However, in our study the best generated almost
regular RU codes always have the rate equal to the design rate. For this reason,
we do not distinguish between the design rate and the actual rate.
In order to construct random binary images of nonbinary (J, K)-regular
LDPC codes, we use the standard two-stage procedure. It consists of labeling
a proper binary base parity-check matrix by random nonzero elements of the
extension of the binary Galois ﬁeld. In our work, we select a parity-check matrix
of a binary LDPC code from the Gallager or the RU ensembles as the base
matrix.
In what follows, the Gallager ensembles of binary regular LDPC codes and
binary images of nonbinary regular LDPC codes are used only for the theoretical
analysis, while for the simulations we use almost regular LDPC codes from the
RU ensemble. The reason for this choice is that in the simulations, the RU LDPC
ensembles outperform the Gallager LDPC codes with the same parameters.
2.2
QC LDPC Codes
The QC LDPC codes represent a class of LDPC codes which is very intensively
used in communication standards. Rate R = b/c QC LDPC codes are determined
by a (c −b) × c polynomial parity-check matrix of their parent convolutional
code [18]
H (D) =
⎛
⎜
⎜
⎜
⎝
h11(D)
h12(D)
. . .
h1c(D)
h21(D)
h22(D)
. . .
h2c(D)
...
...
...
...
h(c−b)1(D) h(c−b)2(D) . . . h(c−b)c(D)
⎞
⎟
⎟
⎟
⎠,
(1)
where hij(D) is either zero or a monomial entry, that is, hij(D) ∈{0, Dwij} with
wij being a nonnegative integer, wij ≤μ, and μ = maxi,j{wij} is the syndrome
memory.
The polynomial matrix (1) determines an [Mc, Mb] QC LDPC block code
using a set of polynomials modulo DM −1. By tailbiting the parent convolutional
code to length M > μ, we obtain the binary parity-check matrix
H TB =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
H 0 H 1
. . .
H µ−1
H µ
0
. . .
0
0
H 0 H 1
. . .
H µ−1 H µ . . .
0
...
...
...
...
...
...
H µ
0
. . .
0
H 0
H 1 . . . H µ−1
...
...
...
...
...
...
...
...
H 1
. . .
H µ
0
. . .
0
. . .
H 0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(2)
of an equivalent (in the sense of column permutation) TB code (see [18, Chap. 2]),
where Hi, i = 0, 1, . . . , μ, are binary (c −b) × c matrices in the series expansion
H(D) = H0 + H1D + · · · + HμDμ,

52
I.E. Bocharova et al.
and 0 is the all-zero matrix of size (c −b) × c. If each column of H (D) contains
J nonzero elements, and each row contains K nonzero elements, the QC LDPC
block code is (J, K)-regular. It is irregular otherwise.
Another form of the equivalent [Mc, Mb] binary QC LDPC block code can
be obtained by replacing the nonzero monomial elements of H (D) in (1) by the
powers of the circulant M × M permutation matrix P, whose rows are cyclic
shifts by one position to the right of the rows of the identity matrix.
The polynomial parity-check matrix H (D) (1) can be interpreted as a (c −
b) × c binary base matrix B labeled by monomials, where the entry in B is one
if and only if the corresponding entry of H (D) is nonzero, i.e.
B = H (D)|D=1.
All three matrices B, H (D), and H can be interpreted as bi-adjacency matrices
of the corresponding Tanner graphs.
3
Stopping Redundancy and Convergence to the ML
Decoding Performance
The idea to improve the performance of iterative decoding of linear codes over
a BEC by using redundant parity checks was studied, for example, in [27,34].
This approach was further explored in [29] (for BEC) and in [35] (for BSC and
AWGN). The idea of using redundant parity checks was also studied in the
context of linear-programming decoding [12], the reader can refer, for example,
to [32].
A straightforward method to extend a parity-check matrix of an LDPC code
is based on appending a predetermined number of dual codewords to the parity-
check matrix. In this approach, the BP decoder uses the redundant matrix
instead of the original parity-check matrix. One of the strategies used to extend
the parity-check matrix consists of appending dual codewords in the order of
their increasing weights starting with the minimum weight ddual. A problem
of searching for low-weight dual codewords has high computational complexity
in general, yet for short LDPC codes it is feasible. We apply this approach in
the sequel, and study the convergence of the FER of BP decoding of LDPC
codes determined by their extended parity-check matrices to the FER of the ML
decoding (for both BEC and AWGN channels).
The stopping redundancy is deﬁned as the minimum number of rows in a
parity-check matrix required to ensure that the stopping distance of the code
dstop is equal to the code minimum distance dmin. For a set of the selected LDPC
codes, we compute estimates on the minimum number of the rows required in
order to ensure removal of stopping sets of a certain size. Next, we describe
this approach in more detail. By ℓ-th stopping redundancy, ρℓ, we denote the
minimum number of rows in any parity-check matrix of the code, such that
all ML-decodable stopping sets of size less than or equal to ℓare removed. In
particular, ρr is the minimum number of rows in any parity-check matrix of the

Distance Properties of Short LDPC Codes and Their Impact
53
code, such that there are no ML-decodable stopping sets of size up to r (incl.),
i.e. no stopping sets which, if erased, still can be decoded by the ML decoder.
Our deﬁnition of ℓ-th stopping redundancy is analogous to its counterpart in [15].
However, we stress the diﬀerence between the updated deﬁnition of the ℓ-th
stopping redundancy for ℓ≥d and its counterpart in [15]. In fact, the stopping
sets of size ℓ≥d that are not ML-decodable, are exactly the supports of the
codewords.1
In order to calculate the upper bounds on the ℓ-th stopping redundancy with
a method based on [33], we ﬁrst estimate by sampling ui, the number of ML-
decodable stopping sets of size i in a particular parity-check matrix. Then, we use
the estimates on ui (i = 1, 2, . . . , r) with the method similar to [33, Theorem 1,2]
in order to obtain the approximate upper bounds on the stopping redundancy
hierarchy, i.e. the stopping redundancies ρ1, ρ2, . . . , ρr.
In Table 1, we present estimates on ρℓ, ℓ= dmin, dmin +1, dmin +2, and ℓ= r,
along with dmin, dstop, ddual and g, for a set of selected LDPC codes. In Sect. 5,
we also present the simulated FER performance of the BP and ML decoding over
the BEC for this set of codes with varying number of redundant rows. The same
set of LDPC codes with varying number of redundant rows in their parity-check
matrices is also simulated over the AWGN channel.
The LDPC codes from the following four families were selected:
– Random regular LDPC codes from the RU ensemble (rows 2 and 3 in Table 1)
– QC LDPC codes (row 4)
– Binary images of nonbinary regular LDPC codes (row 5)
– Linear codes represented in a “sparse form” (row 1)
Two random RU codes were selected by an exhaustive search among 100000 code
candidates. As a search criteria, we used the minimum distance and the ﬁrst
spectrum coeﬃcient Admin,n. The QC LDPC code was obtained by optimization
of lifting degrees for a constructed base matrix in order to guarantee the best
possible minimum distance under a given restriction on the girth value of the
code Tanner graph. For comparison, we simulated the best linear code with
the same length and dimension determined by a parity-check matrix with the
lowest possible correlation between its rows. Next, we refer to this form of the
Table 1. Parameters of studied [48, 24] codes
Code dmin Admin,n dstop ddual g J,K ρdmin, ρdmin+1, ρdmin+2 ρr
Type
1
12
17296
4
12
4 6,12 6240,12151,23468
13 761 585 ‘L’
2
8
13
4
6
4 6,12 261,581,1254
13 683 513 ‘RU’
3
7
1
5
5
4 4,8
83,175,380
12 549 204 ‘RU’
4
7
8
7
5
6 3,6
58,130,274
9 876 964 ‘QC’
5
8
7
4
7
4 3,6
355,751,1551
13 819 276 ‘NB’
1 We recall that a support of a codeword is a stopping set.

54
I.E. Bocharova et al.
parity-check matrix as a “sparse form”. Parameters of the selected codes are
presented in Table 1. Here we use the notations ‘RU’ for random LDPC codes,
‘L’ for the best linear code with parity-check matrix in ‘sparse form’, ‘NB’ for
the binary image of nonbinary regular LDPC code and ‘QC’ for QC LDPC code,
respectively.
4
Upper Bounds on ML and BP Decoding Error
Probability for Ensembles of LDPC Codes
In this section, we analyze the Gallager ensembles of binary and binary images
of nonbinary (J, K)-regular LDPC codes. By following the approach in [5] we
derive estimates on the decoding error probability of the ML and BP decoding
by using precise coeﬃcients of the average weight spectrum and average stopping
set size spectrum, respectively. Additionally to the bounds on the performance
of the ML decoding obtained in [5], in this paper we derive the improved bounds
on the performance of BP decoding for both binary LDPC codes and binary
images of nonbinary regular LDPC codes.
The main idea behind the approach in [5] is computing the average spectra
coeﬃcients recurrently with complexity linear in n (see also [8]). The result-
ing coeﬃcients are substituted into the union-type upper bound on the error
probability of the ML decoding over a BEC [3]
Pe ≤
n

i=d
min
n
i

,
i

w=d
Sw
n −w
i −w

εi(1 −ε)n−i,
(3)
where Sw is the w-th weight (stopping set size) spectrum coeﬃcient, ε is the
erasure probability and d denotes the minimum distance (stopping distance).
In order to upper-bound the error probability of the ML decoding over an
AWGN channel, the average weight spectrum coeﬃcients are substituted into
the tangential-sphere bound [25].
Consider the Gallager ensemble of q-ary LDPC codes, where q = 2m, m ≥1
is an integer. The weight generating function of q-ary sequences of length n
satisfying the nonzero part of one q-ary parity-check equation is given in [13] as
g(s) = (1 + (q −1)s)K + (q −1)(1 −s)K
q
.
(4)
It is easy to derive the weight generating function of q-ary sequences of length
K and q-ary weight not equal to 1:
gstop(s) =

w=0,2,3,...,K
K
w

(q −1)wsw = (1 + (q −1)s)K −K(q −1)s.
(5)
Each q-ary symbol can be represented as a binary sequence (image) of length m.
It is easy to see that diﬀerent representations of a ﬁnite ﬁeld of characteristic

Distance Properties of Short LDPC Codes and Their Impact
55
two will lead to diﬀerent generating functions of binary images for the same
ensemble of nonbinary LDPC codes. Following the techniques in [10], we study
an average binary weight spectrum for the ensemble of m-dimensional binary
images. By assuming uniform distribution on the m-dimensional binary images
of the non-zero q-ary symbols, we obtain the generating function of the average
binary weights of a q-ary symbol in the form
φ(s) =
1
q −1
m

w=1
m
w

sw = (1 + s)m −1
q −1
.
(6)
The average binary weight generating function for one strip is given by
G(s) =

g(φ(s))
M =
nm

w=0
Nnm,wsw,
where Nnm,w denotes the average number of binary sequences β of weight w and
of length nm satisfying βBT
i = 0. Here, Bi denotes the average binary image
of H i. We obtain the average binary weight enumerator of nonbinary regular
LDPC code as
E{Anm,w} =
nm
w

p(w)
J =
nm
w
1−J
N J
nm,w,
(7)
where p(w) =
nm
w
−1Nnm,w. By substituting (6) into (5), similarly to (7), we
obtain the average binary stopping set size spectrum coeﬃcient.
It is known that if the generating function is represented as a degree of
another generating function it can be easily computed by applying a recurrent
procedure. Details of the recurrent procedure for computing coeﬃcients of the
average weight spectra can be found in [5]. We proceed by computing Nnm,w
recursively.
5
Simulation Results
We simulate the BP and ML decoding over the BEC and AWGN channel for the
ﬁve LDPC codes whose parameters are presented in Table 1. In Fig. 1, the FER
performance of the BP and ML decoding over the BEC and the AWGN channel
is compared. It is easy to see that the best BP decoding performance both over
the BEC and over the AWGN channel (and at the same time the worse ML
decoding performance) is shown by the QC LDPC code with the most sparse
parity-check matrix and the largest girth value of its Tanner graph. We remark
that the best linear [48,24,12] code determined by a parity-check matrix in a
“sparse form”, as expected, has the best ML decoding performance over the
both channels. Its BP decoding performance is worse than that of the selected
LDPC codes except for the binary image of nonbinary LDPC code.

56
I.E. Bocharova et al.
1
2
3
4
5
SNR, dB
10-6
10-4
10-2
100
FER
BP:(3,6)-NB
BP:(3,6)-QC
BP:(4,8)-RU
BP:(48,24)-code
ML:(3,6)-NB
ML:(3,6)-QC
ML:(4,8)-RU
ML:(48,24)-code
0.2
0.3
0.4
0.5
erasure probability
10-5
10-4
10-3
10-2
10-1
100
FER
BP:(3,6)-NB
BP:(3,6)-QC
BP:(4,8)-RU
BP:(48,24)-code
ML:(3,6)-NB
ML:(3,6)-QC
ML:(4,8)-RU
ML:(48,24)-code
Fig. 1. Comparison of the FER performance of BP and ML decoding over the BEC
and the AWGN channel for LDPC codes of length n = 48 and rate R = 1/2
Figure 2 shows the BP decoding performance over the BEC and AWGN chan-
nel of the codes ‘QC’ and ‘L’ from Table 1, when their parity-check matrices
are extended. We call the corresponding decoding technique “redundant par-
ity check” (RPC) decoding. The number next to “RPC” in Fig. 2 indicates the
number of redundant rows that was added. The best convergence of the FER
performance of the BP decoding over the BEC to that of the ML decoding is
demonstrated by the QC LDPC code, while the best linear code has the slowest
convergence of its BP performance to the ML decoding performance. We observe
that the obtained simulation results are consistent with the estimates on the
stopping redundancy hierarchy given in Table 1. Surprisingly, similar behavior
can also be observed for the FER performance of RPC decoding over the AWGN
channel.
6
Discussion
In this section, we compare the simulated FER performance of the BP, ML and
near-ML (RPC) decoding over the BEC and the AWGN channel with improved
bounds on the ML and BP decoding performance. In Fig. 3, the FER perfor-
mance over the BEC for the binary image of nonbinary (3, 6)-regular LDPC code
over GF(24) (‘NB’ code in Table 1) and the corresponding bounds are shown.
As it is shown in the presented plots, the ML performance of the ‘NB’ code
is rather close to the ML performance of the ‘L’ code, but the convergence of the
FER performance of the RPC decoding to the performance of the ML decoding
for the ‘NB’ code is much faster than for the ‘L’ code.
In Fig. 4, the FER performance of the BP, ML and RPC decoding over the
BEC and the AWGN channel is compared to the corresponding upper and lower
bounds on the performance of the ML decoding. In particular, for comparison

Distance Properties of Short LDPC Codes and Their Impact
57
1
2
3
4
5
SNR, dB
10-6
10-5
10-4
10-3
10-2
10-1
100
FER
BP, (48,24)
BP, (3,6)-QC
RPC-16, (48,24)
RPC-16, (3,6)-QC
RPC-64, (48,24)
RPC-64, (3,6)-QC
RPC-256, (48,24)
RPC-256, (3,6)-QC
RPC-1024, (48,24)
RPC-1024, (3,6)-QC
ML, (48,24)
ML, (3,6)-QC
0.2
0.3
0.4
0.5
erasure probability
10-5
10-4
10-3
10-2
10-1
100
FER
BP, (48,24)
BP, (3,6)-QC
RPC-16, (48,24)
RPC-16, (3,6)-QC
RPC-64, (48,24)
RPC-64, (3,6)-QC
RPC-256, (48,24)
RPC-256, (3,6)-QC
RPC-1024, (48,24)
RPC-1024, (3,6)-QC
ML, (48,24)
ML, (3,6)-QC
Fig. 2. FER performance of RPC decoding over the BEC and the AWGN channel for
‘L’ and ‘QC’ codes.
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
channel erasure probability 
10-4
10-3
10-2
10-1
100
FER
ML: [48,24,12]-code
ML S-bound (3,6) NB GF(24)
BP S-bound (3,6) NB GF(24)
ML:(3,6)-NB
BP:(3,6)-NB
RPC-16: (3,6)-NB
RPC-64: (3,6)-NB
RPC-256: (3,6)-NB
Fig. 3. Comparison of the FER performance of BP and RPC decoding over the BEC
with improved union-type bounds (3) on the ML and BP decoding performance.

58
I.E. Bocharova et al.
0.2
0.3
0.4
0.5
erasure probability
10-5
10-4
10-3
10-2
10-1
100
FER
Lower bound
Random coding
Upper bound, J=3
ML, (48,24)
BP (3,6)-QC
RPC-256 (3,6)-QC
1
2
3
4
5
SNR, dB
10-8
10-6
10-4
10-2
100
FER
Lower bound
Random coding
Upper bound, J=3
ML, (48,24)
BP (3,6)-QC
RPC-256 (3,6)-QC
Fig. 4. Comparison of the FER performance of BP, ML and RPC decoding with upper
and lower bounds on the ML decoding performance.
of the performance over the BEC, we use the improved upper bound (3) com-
puted for the precise ensemble average spectrum coeﬃcients for both random
linear code and (3, 6)-regular random binary LDPC code. As a lower bound, we
consider the tighten sphere-packing bound in [6]. For comparison of the perfor-
mance over the AWGN channel, we show the tangential-sphere upper bound [25]
computed with the precise ensemble average spectrum coeﬃcients for the same
two ensembles and the Shannon lower bound [30].
Based on the presented results, we conclude the following:
– Although it is commonly believed that the stopping sets inﬂuence the BP
decoding performance over the BEC only, the behavior of the analyzed codes
over the BEC and the AWGN channel is very similar. In particular, for short
codes, the FER performance of the BP decoding over the AWGN channel
can be signiﬁcantly improved by adding redundant rows to the parity-check
matrix.
– Convergence of the RPC decoding performance to the ML decoding perfor-
mance is faster for those codes which are most suitable for iterative decoding,
that is, codes with large girth of the Tanner graph.
– RPC decoding has a decoding threshold. When a small number of redundant
rows is added, the FER performance rapidly improves, but after adding a
certain number of redundant rows, the performance improvement becomes
practically unjustiﬁed due to growing complexity.
– The FER performance of the RPC decoding achieves the FER performance
of the ML decoding over the BEC with exponential (in length) complexity.
However, a signiﬁcant reduction in the FER compared to the FER of BP
decoding can be achieved with a signiﬁcantly lower complexity than that of
the ML decoding.

Distance Properties of Short LDPC Codes and Their Impact
59
– Binary images of nonbinary LDPC codes with RPC decoding demonstrate
good FER performance over the BEC. In order to apply RPC decoding to
these codes over the AWGN channel it is required to add q-ary parity-checks
to their parity-check matrices. This method looks promising and is subject of
our future research.
References
1. Asratian, A.S.: Bipartite Graphs and Their Applications, vol. 131. Cambridge Uni-
versity Press, Cambridge (1998)
2. Baldi, M., Chiaraluce, F., Maturo, N., Liva, G., Paolini, E.: A hybrid decoding
scheme for short non-binary LDPC codes. IEEE Commun. Lett. 18(12), 2093–
2096 (2014)
3. Berlekamp, E.R.: The technology of error-correcting codes. Proc. IEEE 68(5), 564–
593 (1980)
4. Bocharova, I.E., Kudryashov, B., Johannesson, R.: Searching for binary and non-
binary block and convolutional LDPC codes. IEEE Trans. Inform. Theory 62(1),
163–183 (2016)
5. Bocharova, I.E., Kudryashov, B.D., Skachek, V.: Performance of ML decoding
for ensembles of binary and nonbinary regular LDPC codes of ﬁnite lengths. In:
Proceeding of IEEE International Symposium on Information Theory (ISIT), 2017,
pp. 794–798 (2017)
6. Bocharova, I.E., Kudryashov, B.D., Skachek, V., Rosnes, E., Ytrehus, Ø.:
ML and Near-ML Decoding Performance of LDPC Codes over BEC: Bounds
and Decoding Algorithms (preprint) (August 2017). http://kodu.ut.ee/∼vitaly/
Papers/BEC-bounds/becbounds.pdf
7. Bocharova, I.E., Kudryashov, B.D., Skachek, V., Yakimenka, Y.: Low complexity
algorithm approaching the ML decoding of binary LDPC codes. In: Proceeding
of IEEE International Symposium on Information Theory (ISIT), 2016, pp. 2704–
2708 (2016)
8. Bocharova, I.E., Kudryashov, B.D., Skachek, V., Yakimenka, Y.: Average spec-
tra for ensembles of LDPC codes and their applications. In: Proceeding of IEEE
International Symposium on Information (ISIT), 2017, pp. 361–365 (2017)
9. Di, C., Proietti, D., Telatar, I.E., Richardson, T.J., Urbanke, R.L.: Finite-length
analysis of low-density parity-check codes on the binary erasure channel. IEEE
Trans. Inform. Theory 48(6), 1570–1579 (2002)
10. El-Khamy, M., McEliece, R.J.: Bounds on the average binary minimum distance
and the maximum likelihood performance of Reed Solomon codes. In: Proceeding
of 42nd Allerton Conference on Communication, Control and Computing (2004)
11. Fang, Y., Zhang, J., Wang, L., Lau, L.: BP-Maxwell decoding algorithm for LDPC
codes over AWGN channels. In: 6th International Conference on Wireless Commu-
nications, Networking and Mobile Computing (WiCOM), 2010, pp. 1–4 (2010)
12. Feldman, J., Wainwright, M.J., Karger, D.R.: Using linear programming to decode
binary linear codes. IEEE Trans. Inform. Theory 51(3), 954–972 (2005)
13. Gallager, R.G.: Low-Density Parity-Check Codes. M.I.T. Press, Cambridge (1963)
14. Hehn, T., Huber, J.B., Laendner, S.: Improved iterative decoding of LDPC codes
from the IEEE WiMAX standard. In: Proceeding International ITG Conference
on Source and Channel Coding (SCC), 2010, pp. 1–6 (2010)

60
I.E. Bocharova et al.
15. Hehn, T., Milenkovic, O., Laendner, S., Huber, J.B.: Permutation decoding and the
stopping redundancy hierarchy of cyclic and extended cyclic codes. IEEE Trans.
Inform. Theory 54(12), 5308–5331 (2008)
16. Hosoya, G., Matsushima, T., Hirasawa, S.: A decoding method of low-density
parity-check codes over the binary erasure channel. In: Proceeding of 27th Sym-
posium on Information Theory and its Applications (SITA), 2004, pp. 263–266
(2004)
17. Jianjun, M., Xiaopeng, J., Jianguang, L., Rong, S.: Parity-check matrix extension
to lower the error ﬂoors of irregular LDPC codes. IEICE Trans. Commun. 94(6),
1725–1727 (2011)
18. Johannesson, R., Zigangirov, K.S.: Fundamentals of Convolutional Coding, vol.
2015. Wiley (2015)
19. Laendner, S., Hehn, T., Milenkovic, O., Huber, J.B.: When does one redundant
parity-check equation matter? In: Global Telecommunications Conference (Globe-
com), 2006, pp. 1–6 (2006)
20. Litsyn, S., Shevelev, V.: On ensembles of low-density parity-check codes: asymp-
totic distance distributions. IEEE Trans. Inform. Theory 48(4), 887–908 (2002)
21. Olmos, P.M., Murillo-Fuentes, J.J., P´erez-Cruz, F.: Tree-structure expectation
propagation for decoding LDPC codes over binary erasure channels. In: Proceed-
ing of IEEE International Symposium on Information Theory (ISIT), 2010, pp.
799–803 (2010)
22. Orlitsky, A., Viswanathan, K., Zhang, J.: Stopping set distribution of LDPC code
ensembles. IEEE Trans. Inform. Theory 51(3), 929–953 (2005)
23. Pishro-Nik, H., Fekri, F.: On decoding of low-density parity-check codes over the
binary erasure channel. IEEE Trans. Inform. Theory 50(3), 439–454 (2004)
24. Pishro-Nik, H., Fekri, F.: Results on punctured low-density parity-check codes and
improved iterative decoding techniques. IEEE Trans. Inform. Theory 53(2), 599–
614 (2007)
25. Poltyrev, G.: Bounds on the decoding error probability of binary linear codes via
their spectra. IEEE Trans. Inform. Theory 40(4), 1284–1292 (1994)
26. Richardson, T., Urbanke, R.: Modern Coding Theory. Cambridge University Press,
Cambridge (2008)
27. Santhi, N., Vardy, A.: On the eﬀect of parity-check weights in iterative decoding. In:
Proceeding of International Symposium on Information Theory (ISIT), Chicago,
IL, p. 322 (2004)
28. Sason, I., Shamai, S.: Improved upper bounds on the ensemble performance of ML
decoded low density parity check codes. IEEE Commun. Lett. 4(3), 89–91 (2000)
29. Schwartz, M., Vardy, A.: On the stopping distance and the stopping redundancy
of codes. IEEE Trans. Inform. Theory 52(3), 922–932 (2006)
30. Shannon, C.E.: Probability of error for optimal codes in a Gaussian channel. Bell
Syst. Tech. J. 38(3), 611–656 (1959)
31. Varnica, N., Fossorier, M.P., Kavcic, A.: Augmented belief propagation decoding
of low-density parity check codes. IEEE Trans. Commun. 55(7), 1308–1317 (2007)
32. Vontobel, P.O., K¨otter, R.: Graph-cover decoding and ﬁnite-length analysis of
message-passing iterative decoding of LDPC codes, arXiv preprint cs/0512078
(2005)
33. Yakimenka, Y., Skachek, V.: Reﬁned upper bounds on stopping redundancy of
binary linear codes. In: Proceeding of IEEE Information Theory Workshop (ITW),
2015, pp. 1–5 (2015)

Distance Properties of Short LDPC Codes and Their Impact
61
34. Yedidia, J.S., Chen, J., Fossorier, M.: Generating code representations suitable for
belief propagation decoding. In: Proceeding of 40-th Allerton Conference Commu-
nication, Control, and Computing, Monticello, IL (2002)
35. Zumbr¨agel, J., Flanagan, M.F., Skachek, V.: On the pseudocodeword redundancy
of binary linear codes. IEEE Trans. Inform. Theory 58(7), 4848–4861 (2012)

Decoding a Perturbed Sequence
Generated by an LFSR
Sara D. Cardell1, Joan-Josep Climent2(B), and Alicia Roca3
1 Instituto de Matem´atica, Estat´ıstica e Computa¸c˜ao Cient´ıﬁca,
Universidade Estadual de Campinas (UNICAMP), R. S´ergio Buarque de Holanda,
651, Cidade Universit´aria, Campinas, SP 13083-859, Brazil
sdcardell@ime.unicamp.br
2 Departament de Matem`atiques, Universitat d’Alacant,
Ap. Correus 99, 03080 Alacant, Spain
jcliment@ua.es
3 Departamento de Matem´atica Aplicada, IMM,
Universitat Polit`ecnica de Val`encia, Cam´ı de Vera, s/n, 46022 Val`encia, Spain
aroca@mat.upv.es
Abstract. Given a sequence of bits produced by a linear feedback shift
register (LFSR), the Berlekamp-Massey algorithm ﬁnds a register of min-
imal length able to generate the sequence. The situation is diﬀerent when
the sequence is perturbed; for instance, when it is sent through a trans-
mission channel. LFSRs can be described as autonomous systems. A
perturbed sequence of bits generated by an LFSR can be interpreted as
a codeword in the binary linear code generated by the corresponding
observability matrix. The problem of ﬁnding the original sequence can
then be stated as the decoding problem, “given the received codeword,
ﬁnd the information transmitted”. We propose two decoding algorithms,
one based on a brute force attack and the other one based on the repre-
sentation technique of the syndromes introduced by Becker, Joux, May,
and Meurer (2012).
Keywords: LFSR · Correlation attack · Keystream sequence · Com-
panion matrix · Autonomous system · Syndrome decoding · Decoding
representation technique
1
Introduction
Binary linear feedback shift registers (LFSR) are used to generate pseudoran-
dom sequences with desirable properties for keystream (see, for example, [7,11]).
However, given a sequence of bits produced by an LFSR, the Berlekamp-Massey
algorithm [13] ﬁnds a register of minimal length able to generate the sequence,
without any further information. Linearity must be destroyed in order to produce
a sequence useful as a keystream.
A common method aiming at destroying the predictability of the output of
LFSRs is to use the outputs of several LFSRs as inputs of a suitably designed
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 62–71, 2017.
DOI: 10.1007/978-3-319-66278-7 6

Decoding a Perturbed Sequence Generated by an LFSR
63
nonlinear Boolean function in order to produce a keystream. Very frequently,
in these cases, a correlation appears between the keystream of the nonlinear
generator and the output of one of its LFSR components (like the Geﬀe generator
[5] or the A5/1 algorithm [6]). Correlation attacks are a class of plaintext attacks
that exploit the statistical weakness that arises from a poor choice of the Boolean
function.
The ﬁrst correlation attack was devised by Siegenthaler [18] and is based on
a model where the keystream is viewed as a noisy version of the output of some
of the constituent LFSRs. He assumed that noise was additive and independent
of the underlying LFSR sequence. Later, Meier and Staﬀelbach [15] presented
two diﬀerent algorithms for fast correlation attacks using a correlation between
the keystream and the output stream of an LFSR. Other correlations attacks
have been developed since then; see [1,3,4,8,12,14,17,19], among others.
In this paper the LFSR sequence is understood as a codeword of a certain
linear block code, and this work is devoted to decode it.
The paper is organized as follows. In Sect. 2, the correlation problem is
expressed as a coding theory problem. In Sect. 3, we propose two diﬀerent algo-
rithms to recover the initial state of the LFSR given the keystream are proposed.
2
A Decoding Problem
Assume that the characteristic polynomial of an LFSR is
f(x) = c0 + c1x + c2x2 + · · · + ck−1xk−1 + xk ∈F2[x].
Let us consider the companion matrix
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0 0 0 · · · 0
c0
1 0 0 · · · 0
c1
0 1 0 · · · 0
c2
...
...
...
...
...
0 0 0 · · · 0 ck−2
0 0 0 · · · 1 ck−1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
∈Fk×k
2
of f(x), and the column matrix
C =
1 0 0 · · · 0 0	T ∈Fk×1
2
.
The LFSR deﬁned by f(x) can be described as the autonomous system
x t+1 = x tA
yt = x tC

t = 0, 1, 2 . . .
(1)
where x t is the state of the system at instant t. We denote by x 0 the initial
state and we say that system (1) is in canonical observability form (see [9]).
If we consider an initial state x 0 = (y0, y1, . . . , yk−1), then the t-th stream
bit yt, for t ≥k, can be computed using expression (1). Moreover, assuming

64
S.D. Cardell et al.
that f(x) is a primitive polynomial, then we know that the output sequence
y0, y1, . . . , yk−1, yk, yk+1, . . . has the maximum period 2k −1.
Let n be a positive integer such that k < n < 2k −1. We can compute any
output sequence y = (y0, y1, y2, . . . , yk−1, yk, yk+1, . . . , yn−1) as y = x 0G where
G =

C AC A2C A3C · · · An−2C An−1C
	
is the observability matrix of the system given by expression (1).
Assume that we know neither the sequence y nor the initial state x 0. Assume
also that we know the sequence
z = (z0, z1, z2, . . . , zk−1, zk, zk+1, . . . , zn−1)
which is correlated to the sequence y with correlation probability 1 −ε (usually
0.25 ≤ε ≤0.5). The idea of the correlation attack is to view the sequence z as a
perturbation of the sequence y by a binary symmetric memoryless noise channel
(see Fig. 1) with Pr(zt = yt) = 1 −ε (see [15,16]).
1 −ε
1 −ε
0
1
0
1
ε
ε
zt
yt
Fig. 1. Transmission over a binary symmetric channel
Thus, the LFSR sequence y is interpreted as a codeword in the [n, k] binary
linear code C generated by matrix G, and the keystream sequence z as the
received channel output. Therefore, the correlation attack can now be reformu-
lated as: Given a received word z ∈Fn
2, ﬁnd the transmitted codeword y ∈C.
This means that z = y + e, where e ∈Fn
2 is a vector with average Hamming
weight wt(e) = ⌊εn⌋.
Assume now that n = mk for some positive integer m, and that k < n < 2k −
1. It follows that 1 < m < 2k−1
k
. As the system (1) is in canonical observability
form, it follows that

C AC A2C A3C · · · Ak−1C
	
= Ik
where Ik denotes the k × k identity matrix. Consequently

AkC Ak+1C Ak+2C Ak+3C · · · A2k−1C
	
= Ak 
C AC A2 A3 · · · Ak−1C
	
= Ak.
So, we can write the generator matrix G of C as
G =

Ik Ak A2k A3k · · · A(m−2)k A(m−1)k	

Decoding a Perturbed Sequence Generated by an LFSR
65
which is in systematic form. Consequently,
H =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
(Ak)T
Ik
(A2k)T
Ik
(A3k)T
Ik
...
...
(A(m−2)k)T
Ik
(A(m−1)k)T
Ik
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
(2)
is a parity-check matrix of C.
3
The Decoding Algorithm
From now on, we will assume that the output sequence of the LFSR deﬁned by
f(x), and the received sequence are
y = (y0, y 1, y2, . . . , y m−2, y m−1)
and
z = (z 0, z 1, z 2, . . . , z m−2, z m−1)
respectively, where
yi = (yik, yik+1, yik+2, . . . , y(i+1)k−2, y(i+1)k−1)
z i = (zik, zik+1, zik+2, . . . , z(i+1)k−2, z(i+1)k−1)
for i = 0, 1, 2, . . . , m −2, m −1.
Since y ∈C, it follows that yHT = 0, i.e.,
y0

Ak A2k A3k · · · A(m−2)k A(m−1)k	
+
y1 y2 y3 · · · ym−2 ym−1
	
=

0 0 0 · · · 0 0
	
and therefore
y0Aik + yi = 0,
for
i = 1, 2, . . . , m −1.
This means that (y0, yi) is a codeword of the [2k, k] binary code Ci whose
parity-check matrix is Hi =

(Aik)T Ik
	
, for i = 1, 2, . . . , m −1.
On the other hand, let s ∈F(m−2)k
2
be the syndrome of z, i.e., zHT = s,
i.e.,
z 0

Ak A2k A3k · · · A(m−2)k A(m−1)k	
+

z 1 z 2 z 3 · · · z m−2 z m−1
	
=
s0 s1 s2 · · · sm−3 sm−2
	
or equivalently,
z 0A(i+1)k + z i+1 = si,
for
i = 0, 1, 2, . . . , m −2,
where
si = (sik, sik+1, sik+2, . . . , s(i+1)k−2, s(i+1)k−1),
for
i = 0, 1, 2, . . . , m −2.

66
S.D. Cardell et al.
Since z = y + e; i.e., z i = yi + ei, for i = 0, 1, 2, . . . , m −1, it follows that
e0A(i+1)k + ei+1 = si,
for
i = 0, 1, 2, . . . , m −2,
or equivalently,
e0BT + ˜e = s,
(3)
where
BT =

Ak A2k A3k · · · A(m−2)k A(m−1)k	
,
˜e =

e1 e2 e3 · · · em−2 em−1
	
.
Remember that we are looking for a vector e ∈Fn
2 such that eHT = s and
wt(e) = w. If we assume that the ω nonzero components of e are uniformly
distributed, we can assume that wt(e0) = ⌊εn/m⌋= ⌊εk⌋.
Now, expression (3) suggests the brute force attack given by Algorithm 1
below.
Algorithm 1.
input : H, s, and ω
output: e ∈Fn
2 such that eHT = s and wt(e) = ω
1 for e0 ∈Fk
2 such that wt(e0) = ⌊εk⌋do
2
compute ˜e as ˜e = e0BT + s
% see expression (3)
3
let e = (e0, ˜e)
4
if wt(e) = ω then
5
STOP
6
end
7 end
The second algorithm is based on a modiﬁcation of the representation tech-
nique of the syndromes proposed by Becker, Joux, May, and Meurer [2]. Recall
that we are looking for a vector e ∈Fn
2 such that eHT = s and wt(e) = w. To
improve calculations, we introduce additional hypotheses on the weight distrib-
ution of the vector e. We search for vectors e, which can be decomposed into
e = (˜e0, ˜e1) ∈F2k
2 × Fn−2k
2
where wt(˜e0) = p, for some p such that 1 ≤p ≤2k,
wt(˜e1) = w −p, and eHT = s. As we are assuming that the errors are uni-
formly distributed, in average, p = wt(˜e0) = ⌊2kε⌋. Then, as a consequence of
expression (2), after obtaining the vector ˜e0 from
s0 = ˜e0HT
1
(4)
we can ﬁnd the vector ˜e1 as being
˜e1 = e0 BT
1 + ˜s1
(5)

Decoding a Perturbed Sequence Generated by an LFSR
67
where ˜e0 = (e0, e1), ˜s1 = (s1, s2, . . . , sm−2) and
BT
1 =

A2k A3k · · · A(m−2)k A(m−1)k	
.
Therefore, we must focus on eﬃciently computing ˜e0. In paper [2], the
authors propose an algorithm to decode random linear codes. The algorithm
contains two main parts. The ﬁrst one consists of transforming the parity check
matrix H into a systematic form through elementary operations, and the second
constitutes the proper decoding algorithm.
From now on, we use the following notation: if u = (u1, u2, . . . , uℓ) ∈Fℓ
2
is a given vector and 1 ≤i ≤j ≤ℓ; then we denote by u[i : j] the vector
(ui, ui+1, . . . , uj) ∈Fj−i+1
2
.
Because of the structure of matrix H (see (2)), our decoding problem is
already stated in systematic form. Therefore, we can apply directly the second
part of the algorithm of [2] on it. We describe next the main ideas on which the
algorithm is based.
– Essentially the method approaches the value of ˜e0 in three steps. In each step
a set of intermediate error vectors u ∈F2k
2
of certain weight is obtained such
that for a given number r, the last r components of uHT
1 match a chosen
target vector in Fr
2. The process is gradual in such a way that in the last
step the vectors u ∈F2k
2
selected have weight p and uHT
1 coincides with s0.
Therefore satisfying (4), as desired.
– In each step the procedure is analogous, and uses the Merge-Join algorithm
of [2], which in turn is based on a classical matching algorithm by Knuth [10].
For given ρ, ν where 1 ≤ρ ≤2k, 1 ≤ν ≤k and t ∈Fν
2, and two sets of
vectors U and V in F2k
2 of weight approximately ρ/2, the Merge-Join procedure
produces a new set of vectors W of weight ρ satisfying a matching condition
as follows:
W =

u + v ∈F2k
2
| u ∈U, v ∈V,
wt(u + v) = ρ,

(u + v) HT
1

[k −ν + 1 : k] = t

.
– The Merge-Join algorithm is run four times in step 2, two times in step 1,
and one time in the ﬁnal step 0. The input sets to each step of the procedure
come from the outputs of previous steps in the same procedure. To start the
procedure, four pairs of sets of p2/2-weight vectors are prepared by brute
search, in a way that the nonzero components of the vectors in each pair set
appear in disjoint positions with respect to the other one.
– Another idea to be highlighted. As mentioned previously, in each step sets
of pi-weight vectors are obtained adding vectors coming from pairs of sets
of pi+1-weight vectors, where pi+1 = pi
2 + εi+1, for i = 0, 1, 2, and p0 = p
and ε3 = 0. This means that the added vectors must match in εi+1 nonzero
components. Notice that it allows the existence of
 k
pi+1

possible vectors

68
S.D. Cardell et al.
instead of
 k
pi/2

and as
 k
pi+1

>
 k
pi/2

, whenever pi+1 ≤k/2. It follows
that we have more possible vectors to analyze.
The algorithm is described next (see Algorithm 2 below). Given the matrix
H1, the vector s0, and the integer p with 1 ≤p ≤2k, the algorithm allows us to
obtain a set L of vectors in F2k
2
whose elements are candidates for the solution
of (4).
First we analyze the calculations performed in the diﬀerent layers. The
remarks below are intended to be an explanation of the behavior of the
algorithm.
Remark 1. Assume that vectors v 1 ∈Fr1
2 and u1, u3 ∈Fr2
2 are chosen (see line 3
of Algorithm 2), and the vectors v 2 ∈Fr1
2 and u2, u4 ∈Fr2
2 deﬁned (see line 4
of Algorithm 2). Then, for i = 1, 2, 3, 4, the basic lists

L(3)
2i−1, L(3)
2i

are created.
Next, for i = 1, 2, 3, 4, if a(i)
1
∈L(3)
2i−1 and a(i)
2
∈L(3)
2i , then
a(i)
1 + a(i)
2
∈L(2)
i
and

a(i)
1 + a(i)
2

HT
1

[k −r2 + 1 : k] = ui,
and therefore

a(1)
1
+ a(1)
2
+ a(2)
1
+ a(2)
2
+ a(3)
1
+ a(3)
2
+ a(4)
1
+ a(4)
2

HT
1

[k −r2 + 1 : k]
= u1 + u2 + u3 + u4 = (v 1 + v 2) [k −r2 + 1 : k] = s0[k −r2 + 1 : k].
As the vectors of the ﬁnal list will be selected as sums of vectors in L(2)
i , this
property guarantees that the ﬁnal selected vectors will match the last r2 com-
ponents of the syndrome block s0.
Similarly, for j = 1, 2, if b(j)
1
∈L(2)
2j−1 and b(j)
2
∈L(2)
2j , then
b(j)
1
+ b(j)
2
∈L(1)
j
and

b(j)
1
+ b(j)
2

HT
1

[k −r1 + 1 : k] = v j
and therefore

b(1)
1
+ b(1)
2
+ b(2)
1
+ b(2)
2

HT
1

[k −r1 + 1 : k] = v 1 + v 2 = s0[k −r1 + 1 : k].
In the same way, we see that with the selection of vectors at this step, the last
r1 components of the syndrome block s0 are guaranteed, for r1 ≥r2.
Finally, if c1 ∈L(1)
1
and c2 ∈L(1)
2 , then
c1 + c2 ∈L
and
(c1 + c2) HT
1 = s0,
which was our target.
Once the set L has been determined, we use Algorithm 3 to obtain a vector
e such that wt(e) = w y eHT = s.

Decoding a Perturbed Sequence Generated by an LFSR
69
Algorithm 2.
input : H1, s0, and p with 1 ≤p ≤2k
output: L =
˜e0 ∈F2k
2
| wt(˜e0) = p, ˜e0HT
1 = s0

% see expression (4)
1 Deﬁne p1 = p/2 + ε1 and p2 = p1/2 + ε2 such that p1 and p2 are even numbers;
2 Choose random integers r1, r2 such that 1 ≤r2 ≤r1 ≤k;
3 Choose random vectors v 1 ∈Fr1
2 , and u1, u3 ∈Fr2
2 ;
4 Deﬁne vectors
v 2 = s0[k −r1 + 1 : k] + v 1,
u2 = v 1[r1 −r2 + 1 : r1] + u1,
u4 = v 2[r1 −r2 + 1 : r1] + u3;
5 for i = 1, 2, 3, 4 do
6
choose random partitions Pi = (P2i−1, P2i) of {1, 2, . . . , 2k} such that
|P2i−1| = |P2i|;
7
obtain the pairs of basic sets

L(3)
2i−1, L(3)
2i

such that
L(3)
2i−1 =

a = (a1, a2, . . . , a2k) ∈F2k
2
| wt(a) = p2/2
al = 0 for all l ∈P2i

,
L(3)
2i =

a = (a1, a2, . . . , a2k) ∈F2k
2
| wt(a) = p2/2
al = 0 for all l ∈P2i−1

,
8 end
9 for i = 1, 2, 3, 4 do
10
use the basic sets

L(3)
2i−1, L(3)
2i

, the vector ui, the integer p2, and the
Merge-Join algorithm to obtain the set
L(2)
i
=

a(i)
1
+ a(i)
2
∈F2k
2
| a(i)
1
∈L(3)
2i−1, a(i)
2
∈L(3)
2i ,
wt

a(i)
1
+ a(i)
2

= p2 and

(a(i)
1
+ a(i)
2 )HT
1

[k −r2 + 1 : k] = ui

11 end
12 for j = 1, 2 do
13
use the sets

L(2)
2j−1, L(2)
2j

, the vector v j, the integer p1, and the Merge-Join
algorithm to obtain the set
L(1)
j
=

b(j)
1
+ b(j)
2
∈F2k
2
| b(j)
1
∈L(2)
2j−1, b(j)
2
∈L(2)
2j ,
wt

b(j)
1
+ b(j)
2

= p1 and

(b(j)
1
+ b(j)
2 )HT
1

[k −r1 + 1 : k] = v j

14 end
15 Use the sets

L(1)
1 , L(1)
2

, the vector s0, the integer p, and the Merge-Join
algorithm to obtain the set
L =

c1 + c2 ∈F2k
2
| c1 ∈L(1)
1 , c2 ∈L(1)
2 ,
wt(c1 + c2) = p and (c1 + c2)HT
1 = s0


70
S.D. Cardell et al.
Algorithm 3.
input : H, s, L, ω
output: e ∈Fn
2 such that eHT = s and wt(e) = ω
1 for ˜e0 ∈L do
% wt(˜e 0) = p, ˜e 0 = (e 0, e 1)
2
Compute ˜e1 ∈Fn−2k
2
as ˜e1 = e0 BT
1 + ˜s1
% see expression (5)
3
if wt(˜e1) = ω −p then
4
deﬁne e = (˜e0, ˜e1)
5
end
6 end
Remark 2. Preliminary calculations performed for registers of length k in the
range 8 ≤k ≤16 show that in 95 % of the cases we obtain the desired solution.
Algorithm 1 presents better performance in terms of running time for small
values of k, whilst Algorithms 2 and 3 seem to be more eﬃcient when k increases.
In the case of Algorithm 2, we have taken p = wt(˜e0) = ⌊2kε⌋, and for the
rest of parameters: ε1, ε2, r1 and r2, we assign the values recommended in [2];
i.e.,
– 0 < ε1 < 2k −p,
– 0 < ε2 < 2k −p1,
– r1 ≈log2 R1, where R1 =
 p
p/2
2k−p
ε1

,
– r2 ≈log2 R2, where R2 =
 p1
p1/2
2k−p1
ε2

.
As future work, we aim to run the algorithms for higher values of k (in the
range of 16 < k ≤64). Strategies to decrease the amount of computation in
order to prepare the lists are being explored.
Acknowledgements. The ﬁrst author was supported by FAPESP with number of
process 2015/07246-0. The second author was partially supported by grants MIMECO
MTM2015-68805-REDT and MTM2015-69138-REDT. The third author was partially
supported by grants MINECO MTM2013-40960-P and MTM2015-68805-REDT.
References
1. ˚Agren, M., L¨ondahl, C., Hell, M., Johansson, T.: A survey on fast correlation
attacks. Crypt. Commun. 4(3–4), 173–202 (2012)
2. Becker, A., Joux, A., May, A., Meurer, A.: Decoding random binary linear codes
in 2n/20: how 1 + 1 = 0 improves information set decoding. In: Pointcheval, D.,
Johansson, T. (eds.) EUROCRYPT 2012. LNCS, vol. 7237, pp. 520–536. Springer,
Heidelberg (2012). doi:10.1007/978-3-642-29011-4 31
3. Canteaut, A., Naya-Plasencia, M.: Correlation attacks on combination generators.
Crypt. Commun. 4(3–4), 147–171 (2012)
4. Chepyzhov, V.V., Johansson, T., Smeets, B.: A simple algorithm for fast correlation
attacks on stream ciphers. In: Goos, G., Hartmanis, J., Leeuwen, J., Schneier, B.
(eds.) FSE 2000. LNCS, vol. 1978, pp. 181–195. Springer, Heidelberg (2001). doi:10.
1007/3-540-44706-7 13

Decoding a Perturbed Sequence Generated by an LFSR
71
5. Geﬀe, P.: How to protect data with ciphers that are really hard to break. Electronics
46(1), 99–101 (1973)
6. Goli´c, J.D.: Cryptanalysis of alleged A5 stream cipher. In: Fumy, W. (ed.) EURO-
CRYPT 1997. LNCS, vol. 1233, pp. 239–255. Springer, Heidelberg (1997). doi:10.
1007/3-540-69053-0 17
7. Golomb, S.W.: Shift Register-Sequences. Aegean Park Press, Laguna Hill (1982)
8. Johansson, T., J¨onsson, F.: Theoretical analysis of a correlation attack based on
convolutional codes. IEEE Trans. Inf. Theory 48(8), 2173–2181 (2002)
9. Kailath, T.: Linear Systems. Prentice-Hall, Upper Saddle River (1980)
10. Knuth, D.E.: The Art of Computer Programming. Sorting and Searching. Addison-
Wesley, Boston (1998)
11. Lidl, R., Niederreiter, H.: Introduction to Finite Fields and Their Applications.
Cambridge University Press, New York (1986)
12. Lu, P., Huang, L.: A new correlation attack on LFSR sequences with high error
tolerance. Prog. Comput. Sci. Appl. Logic 23, 67–83 (2004)
13. Massey, J.L.: Shift-register synthesis and BCH decoding. IEEE Trans. Inf. Theory
15(1), 122–127 (1969)
14. Meier, W.: Fast correlation attacks: methods and countermeasures. In: Joux, A.
(ed.) FSE 2011. LNCS, vol. 6733, pp. 55–67. Springer, Heidelberg (2011). doi:10.
1007/978-3-642-21702-9 4
15. Meier, W., Staﬀelbach, O.: Fast correlation attacks on stream ciphers. In: Barstow,
D., Brauer, W., Brinch Hansen, P., Gries, D., Luckham, D., Moler, C., Pnueli,
A., Seegm¨uller, G., Stoer, J., Wirth, N., G¨unther, C.G. (eds.) EUROCRYPT
1988. LNCS, vol. 330, pp. 301–314. Springer, Heidelberg (1988). doi:10.1007/
3-540-45961-8 28
16. Meier, W., Staﬀelbach, O.: Fast correlation attacks on certain stream ciphers. J.
Cryptology 1(3), 159–176 (1989)
17. Molland, H., Mathiassen, J.E., Helleseth, T.: Improved fast correlation attack using
low rate codes. In: Paterson, K.G. (ed.) Cryptography and Coding 2003. LNCS,
vol. 2898, pp. 67–81. Springer, Heidelberg (2003). doi:10.1007/978-3-540-40974-8 7
18. Siegenthaler, T.: Decrypting a class of stream ciphers using ciphertext only. IEEE
Trans. Comput. 34(1), 81–85 (1985)
19. Zhang, B., Wu, H., Feng, D., Bao, F.: A fast correlation attack on the shrink-
ing generator. In: Menezes, A. (ed.) CT-RSA 2005. LNCS, vol. 3376, pp. 72–86.
Springer, Heidelberg (2005). doi:10.1007/978-3-540-30574-3 7

A Construction of Orbit Codes
Joan-Josep Climent1(B), Ver´onica Requena2, and Xaro Soler-Escriv`a1
1 Departament de Matem`atiques, Universitat d’Alacant,
Ap. Correus 99, 03080 Alacant, Spain
{jcliment,xaro.soler}@ua.es
2 Departamento de Estad´ıstica, Matem´aticas e Inform´atica,
Universidad Miguel Hern´andez de Elche,
Avda. Universidad, s/n, Elche, 03202 Alicante, Spain
vrequena@umh.es
Abstract. Given a ﬁnite ﬁeld Fq, a constant dimension code is a set
of k-dimensional subspaces of Fn
q . Orbit codes are constant dimension
codes which are deﬁned as orbits when the action of a subgroup of the
general linear group on the set of all subspaces of Fn
q is considered. In
this paper we present a construction of an Abelian non-cyclic orbit code
whose minimum subspace distance is maximal.
Keywords: Random linear network coding · Subspace codes ·
Grassmannian · Group action · General linear group
1
Introduction
Random linear network coding is a powerful tool for disseminating information
in networks [10]. K¨otter and Kschischang [11] proposed a mathematical descrip-
tion of network communications in which the transmitted messages are vector
subspaces, rather than vectors, of a given vector space over a ﬁnite ﬁeld. This
approach is known as subspace codes and their study has led to many papers in
recent years (see, for example, [2–5,7–9,12,13,15–17] to mention only the most
recent). Most of these authors focus on the study of constant dimension codes;
that is, codes in which all subspaces have the same dimension.
We focus our attention on constant dimension codes which are obtained as
orbits of certain groups. This concept traces back to Slepian [14], where Euclid-
ean spaces were considered. In the linear network coding setting, Trautmann
et al. [17] deﬁned such codes as orbit codes by the action of subgroups of the
general linear group. Since then, several papers have been written about the
structure of orbit codes (see for instance [1,6,7,15,16]).
When we consider cyclic subgroups of the general linear group, we talk about
cyclic orbit codes. These codes were introduced by Trautmann et al. in [16],
where the authors gave a characterization of a particular subclass of them.
In this paper we are interested on Abelian orbit codes; that is, orbit codes
which are generated by Abelian subgroups of the general linear group. Since
Abelian subgroups are direct product of cyclic subgroups, it seems a natural
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 72–83, 2017.
DOI: 10.1007/978-3-319-66278-7 7

A Construction of Orbit Codes
73
extension of the research on cyclic orbit codes. Speciﬁcally, we present a con-
struction of an Abelian non-cyclic orbit code for which we calculate its cardinality
and its minimum subspace distance. Moreover, the minimum subspace distance
of our code is maximal.
The rest of the paper is structured as follows: In Sect. 2 we give some pre-
liminaries, ﬁrst about random linear network coding and orbit codes and then a
result about upper triangular matrices. The main body of the paper is Sect. 3,
where the whole construction of our Abelian non-cyclic orbit code is given. We
ﬁnish the paper with some open questions in Sect. 4.
2
Preliminaries
Let Fq be the ﬁnite ﬁeld of q elements, where q is a prime power. For any integer
n ≥1, the set of all vector subspaces of Fn
q , denoted by Pq(n), forms a metric
space with respect to the subspace distance deﬁned by (see [11])
d(U, V) = dim(U + V) −dim(U ∩V),
for all U, V ∈Pq(n).
(1)
For any integer k, where 0 ≤k ≤n, the set of all k-dimensional vector subspaces
of Fn
q is called Grassmann variety (or simply Grassmannian) and is denoted
by Gq(k, n). Obviously, Pq(n) = ∪n
k=0Gq(k, n).
A subspace code of length n is a nonempty subset C of Pq(n); a codeword
of such a code is a vector subspace of C. We call C a constant dimension code
if all codewords of C have the same dimension, i.e., if C ⊆Gq(k, n) for some k.
The minimum distance [11] of a subspace code C ⊆Pq(n) is deﬁned as
d(C) = min {d(U, V) | U, V ∈C, U ̸= V} .
The complementary code (or dual code) corresponding to a subspace
code C is the subspace code
C⊥=

U⊥⊆Pq(n) | U ∈C

obtained from the orthogonal subspaces of the codewords of C. It is easy to see
that d(U⊥, V⊥) = d(U, V), and therefore, d(C⊥) = d(C) (see [11]).
In this paper we consider Gq(k, n), and we will assume, without loss of gen-
erality, that k ≤n/2. Otherwise, we can take C⊥which has the same length,
cardinality and minimum distance.
In order to represent a k-dimensional subspace U ∈Gq(k, n), we use a gen-
erator matrix U ∈Fk×n
q
whose rows form a basis of U, that is
U = rowspace (U) =

xU | x ∈Fk
q

.
Note that in Gq(k, n) the subspace distance (1) is given by
d(U, V) = 2(k −dim(U ∩V)) = 2 rank

U
V

−2k
(2)

74
J.-J. Climent et al.
for any U, V ∈Gq(k, n) and some respective matrix representations U, V ∈Fk×n
q
.
We focus on constant dimension codes arising from group actions, which are
simply called orbit codes and which were introduced in [17] (see also [16]).
The general linear group of degree n, denoted by GLn , is the set of all
invertible n × n matrices with entries in Fq. Given a full-rank matrix U ∈Fk×n
q
,
U = rowspace (U) ∈Gq(k, n) its rowspace, and A ∈GLn , we deﬁne
U · A = rowspace (UA) .
Provided that this operation is independent from the representation of U (see
[17]) and since any invertible matrix maps vector subspaces to vector subspaces of
the same dimension, we obtain a group action (from the right) on the Grassmann
variety:
Gq(k, n) × GLn −→Gq(k, n)
(U, A)

→
U · A
It is well-known that two k-dimensional subspaces can be mapped onto each
other by an invertible matrix. Therefore, the orbit of any k-dimensional sub-
space U under the action of GLn is the whole set Gq(k, n). That is, GLn acts
transitively on Gq(k, n).
Now, let G be a subgroup of GLn and consider the action of G on Gq(k, n).
Then the orbit of a subspace U ∈Gq(k, n) under the action of G is
C = {U · A | A ∈G} ,
which is called the orbit code generated by the action of G on U (see [17]).
The stabilizer of U under this action is
stabG (U) = {A ∈G | U · A = U} = G ∩stabGLn (U)
and the size of the orbit code C is
|C| =
|G|
|stabG (U)|.
On the other hand (see [17, Proposition 8]), the subspace distance is GLn -
invariant, that is, d(U, V) = d(U · A, V · A), for all A ∈GLn ; this property allow
us to compute the minimum distance of orbit codes in a simple manner:
d(C) = min {d(U, U · A) | A ∈G\ stabG (U)}
The following lemma on block upper triangular matrices will be useful for
the rest of the paper. The proof is straightforward and we omit it.
Lemma 1. Assume that A ∈Fr×r, B ∈Fs×s, and P ∈Fr×s and consider the
block upper triangular matrix
M =

A P
O B


A Construction of Orbit Codes
75
where O denotes the zero matrix of the appropriate size. If h is a nonnegative
integer, then
M h =

Ah σh(A, B, P)
O
Bh

where
σh(A, B, P) =
⎧
⎪
⎨
⎪
⎩
O,
if h = 0,
h

i=1
Ah−iPBi−1,
if h ≥1.
Our aim is to construct an orbit code C by considering the orbit of Uk, the
standard k-dimensional vector subspace, i.e., the subspace generated by the ﬁrst
k unit vectors of Fn
q , under the action of a concrete subgroup H of GLn . If
Ik denotes the identity matrix of size k × k, then Uk = rowspace

Ik O

. In
order to choose our group H, we have taken into account diﬀerent factors. First,
Lemma 1 which provides an easy tool for working with block upper triangu-
lar matrices and for this reason we take H as a subgroup of upper triangular
matrices. Secondly, since the stabilizer of Uk in GLn is composed of block lower
triangular matrices, it turns out that if H consists of upper triangular matrices,
the stabilizer of Uk in H is readily described (see expression (6) below). Thirdly,
our group H must be a product of cyclic groups to obtain an Abelian group.
Moreover, we have chosen the generators of H in such a way that H is non-cyclic.
Assume that H is a subgroup of upper triangular matrices of GLn and con-
sider the following block partition of any matrix
H =

H11 H12
O H22

∈H,
(3)
where H11 ∈GLk , H12 ∈Fk×(n−k)
q
, H22 ∈GLn−k , and O denotes the zero
matrix of the appropriate size. Then, the orbit code C can be written as
C = {Uk · H | H ∈H} =

rowspace
H11 H12

| H ∈H

.
(4)
Moreover, according to (2) and (3), for any H ∈H we have
d(Uk, Uk · H) = 2 rank
 Ik
O
H11 H12

−2k = 2 rank (H12) ≤2k
(5)
since k ≤n −k. Therefore d(C) ≤2k.
On the other hand, it can be readily veriﬁes that the stabilizer of Uk in GLn
is
stabGLn (Uk) =
 A O
B C

| A ∈GLk , B ∈F(n−k)×k
q
and C ∈GLn−k

.
Since stabH (Uk) = H∩stabGLn (Uk), and using the previous partition of matri-
ces of H given by (3), the stabilizer of Uk in H can be written as
stabH (Uk) =
H11 H12
O H22

∈H | H12 = O

.
(6)

76
J.-J. Climent et al.
3
Our Construction
From now on, we will assume that q is a prime number such that q ≥n −k, and
let λ be a generator of the multiplicative cyclic group F∗
q, i.e., λ is a primitive
element of Fq (in particular o(λ) = q −1 is the multiplicative order of λ).
Let us denote by Jℓthe upper unitriangular matrix of size ℓ× ℓ, with 1’s on
the second upper diagonal and 0’s elsewhere, that is, Jℓis a Jordan block of size
ℓ× ℓassociated to 1. The following result gives the multiplicative order of Jℓ.
Lemma 2. If qt−1 < ℓ≤qt for some nonnegative integer t, then o(Jℓ) = qt.
Proof. Let Jℓ= Iℓ+ Nℓwhere
Nℓ=
⎡
⎢⎢⎢⎢⎢⎣
0 1 0 · · · 0
0 0 1 · · · 0
... ... ...
...
0 0 0 · · · 1
0 0 0 · · · 0
⎤
⎥⎥⎥⎥⎥⎦
∈Fℓ×ℓ
q
is the canonical nilpotent matrix with index ℓ. Then Jqt
ℓ
= Iqt
ℓ+ N qt
ℓ
= Iℓ. Now,
since qt−1 < ℓ≤qt it follows that o(Jℓ) = qt.
⊓⊔
In order to construct our subgroup H of GLn , we consider the following two
matrices of GLn :
S =

λIn−k X
O
Ik

,
T =

Jn−k Y
O
λIk

(7)
where X, Y ∈F(n−k)×k
q
are arbitrary matrices.
Lemma 3. For the matrices S and T given by (7), it follows that
(a) o(S) = o(λ) = q −1.
(b) o(T) = o(Jn−k) o(λ) = q(q −1).
Proof. The proof follows from the choice of λ, the deﬁnition of S and T, and
Lemma 2.
⊓⊔
As a consequence of the previous result the subgroups of GLn generated by
the matrices S and T are cyclic subgroups of order q−1 and q(q−1) respectively,
that is:
⟨S⟩= {Sa | 0 ≤a < q −1} ∼= Cq−1, ⟨T⟩=

T b | 0 ≤b < q(q −1)
 ∼= Cq(q−1).
Lemma 4. The intersection of the groups ⟨S⟩and ⟨T⟩is trivial.
Proof. If Sa = T b for some a and b where 0 ≤a < q −1 and 0 ≤b < q(q −1),
then it follows that a = b = 0, by the deﬁnition of S and T, and Lemma 3.
⊓⊔

A Construction of Orbit Codes
77
The following lemma is crucial for our construction. The proof is straightfor-
ward and we omit it.
Lemma 5. If X ̸= O and Y =
1
1−λ (λIn−k −Jn−k) X in (7), then ST = TS.
Now we are ready to present our group. From now on, we assume that matri-
ces X and Y are deﬁned as in Lemma 5. Since ST = TS and ⟨S⟩∩⟨T⟩= {In}
we obtain
H = ⟨S, T⟩= ⟨S⟩× ⟨T⟩
=

SaT b | 0 ≤a < q −1, 0 ≤b < q(q −1)
 ∼= Cq−1 × Cq(q−1),
(8)
that is, H is an Abelian non-cyclic group of order q(q −1)2 and the elements of
H present the following form
SaT b =

λaIn−k σa(λIn−k, Ik, X)
O
Ik
 Jb
n−k σb(Jn−k, λIk, Y )
O
λbIk

=
λaJb
n−k λaσb(Jn−k, λIk, Y ) + λbσa(λIn−k, Ik, X)
O
λbIk

.
(9)
The following results will help us to understand the structure of SaT b. The
ﬁrst one is well known and, therefore, we omit the proof.
Lemma 6. For any nonnegative integer b it follows that
Jb
n−k =
⎡
⎢⎢⎢⎣
1
b
1
 b
2

· · ·

b
n−k−1

0 1
b
1

· · ·

b
n−k−2

...
...
...
...
0 0
0 · · ·
1
⎤
⎥⎥⎥⎦.
Lemma 7. For matrices X and Y as in Lemma 5, then
λaσb(Jn−k, λIk, Y ) + λbσa(λIn−k, Ik, X)
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
c e1 e2 · · · en−k−2 en−k−1
0 c e1 · · · en−k−3 en−k−2
0 0 c · · · en−k−4 en−k−3
...
...
...
...
...
0 0 0 · · ·
c
e1
0 0 0 · · ·
0
c
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
X,
where c = λa−λb
λ−1
and ei =
λa
λ−1
b
i

, for i = 1, 2, . . . , n −k −1.

78
J.-J. Climent et al.
Proof. According to Lemma 1 we have that
λaσb(Jn−k, λIk, Y ) + λbσa(λIn−k, Ik, X)
= λa 
Jb−1
n−k + λJb−2
n−k + · · · + λb−2Jn−k + λb−1In−k

Y + λbdaX
=
 λa
λ −1

Jb−1
n−k + λJb−2
n−k + · · · + λb−2Jn−k + λb−1In−k




Z
(λIn−k −Jn−k)
+ λbdaIn−k

X.
Notice that in the last expression, we have an upper triangular matrix whose
diagonal elements are all equal to c, multiplying X. Moreover,
c =
λa
1 −λdb(λ −1) + λbda = λbda −λadb
= λb
λa −1
λ −1

−λa
λb −1
λ −1

= λa −λb
λ −1 .
Now we describe the elements ei, for i = 1, 2, . . . , n−k−1. First, if we denote
by zij the (i, j)-entry of Z, then
Z(λIn−k −Jn−k)
=
⎡
⎢⎢⎢⎢⎢⎣
(λ −1)z11 −z11 + (λ −1)z12 · · · −z1(n−k−1) + (λ −1)z1(n−k)
0
(λ −1)z11
· · · −z1(n−k−2) + (λ −1)z1(n−k−1)
...
...
...
0
0
· · ·
−z11 + (λ −1)z12
0
0
· · ·
(λ −1)z11
⎤
⎥⎥⎥⎥⎥⎦
.
Therefore, for i = 1, 2, . . . , n −k −1, it follows that
ei =
λa
1 −λ

−z1i + (λ −1)z1(i+1)

.
(10)
Now, notice that for i = 2, 3, . . . , n −k, we have
z1i =
b−1

j=0
λj
b −1 −j
i −1

.
Therefore, using that
n+1
i

−
n
i

=
 n
i−1

, we obtain
(λ −1)z1(i+1) = −
b −1
i

+ λ
b −1
i

−
b −2
i

+ λ2
b −2
i

−
b −3
i

+ · · · + λb−(i+1)
i + 1
i

−
i
i

+ λb−1
i
i

= −
b
i

+ z1i

A Construction of Orbit Codes
79
and by means of (10) it follows that
ei =
λa
λ −1
b
i

,
(11)
for i = 1, 2, . . . , n −k −1.
⊓⊔
The proof of the following result is straightforward and we omit it.
Lemma 8. With the notation of Lemma 7, we have that c = 0 if, and only if,
a ≡b (mod (q −1)).
Note that, according to (4) and (5), to deﬁne the code C, we lay aside the
block partition of SaT b given by (9); instead, we focus on the ﬁrst k rows of
SaT b; and more speciﬁcally, on the ﬁrst k rows and the last n −k columns of
SaT b. So, we consider the following partition
SaT b =
(SaT b)11 (SaT b)12 (SaT b)13
O
(SaT b)22 (SaT b)23
O
O
(SaT b)33
⎡
⎢⎢⎢⎣
⎤
⎥⎥⎥⎦
n −2k
k
k
n −2k
where, according to (9),
λaJb
n−k =

(SaT b)11 (SaT b)12
O
(SaT b)22

,
λaσb(Jn−k, λIk, Y ) + λbσa(λIn−k, Ik, X) =
(SaT b)13
(SaT b)23

,
(SaT b)11 = λaJb
k,
(SaT b)22 = λaJb
n−2k,
(SaT b)33 = λbIk,
and, (SaT b)12 is the submatrix of λaJb
n−k formed by the ﬁrst k rows and the
last n −2k columns; therefore, as a consequence of Lemma 6,
(SaT b)12 = λa
⎡
⎢⎢⎢⎢⎣
b
k
  b
k+1

· · ·

b
n−k−1

 b
k−1
 b
k

· · ·

b
n−k−2

...
...
...
b
1

b
2

· · ·

b
n−2k

⎤
⎥⎥⎥⎥⎦
.
(12)
Moreover, if we assume that
X =
X1
X2

where X1 ∈Fk×k
q
, X2 ∈F(n−2k)×k
q
,

80
J.-J. Climent et al.
then, from Lemma 7, it follows that
(SaT b)13 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
c e1 e2 · · · ek−2 ek−1
0 c e1 · · · ek−3 ek−2
0 0 c · · · ek−4 ek−3
... ...
...
...
...
0 0 0 · · ·
c
e1
0 0 0 · · ·
0
c
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
X1
+
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
ek
ek+1 ek+2 · · · en−k−2 en−k−1
ek−1
ek
ek+1 · · · en−k−3 en−k−2
ek−2 ek−1
ek
· · · en−k−4 en−k−3
...
...
...
...
...
e2
e3
e4
· · · en−2k−2 en−2k−1
e1
e2
e3
· · · en−2k−3
en−2k
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
X2.
(13)
According to (4), the orbit code C generated by the action of H on Uk is
C =

Uk · SaT b | 0 ≤a < q −1, 0 ≤b < q(q −1)

=

rowspace

(SaT b)11 (SaT b)12 (SaT b)13

| 0 ≤a < q −1, 0 ≤b < q(q −1)

.
Notice that, according to (5),
d(Uk, Uk · SaT b) = 2 rank

(SaT b)12 (SaT b)13

≤2k.
Our next goal will be to choose X in such a way that
rank

(SaT b)12 (SaT b)13

= k,
for all SaT b ∈H\ stabH (Uk). Consequently, d(C) = 2k and the minimum dis-
tance of our code will be maximal for a concrete choice of X.
Theorem 1. If X1 ̸= O, then the stabilizer in H of Uk is the cyclic subgroup of
H generated by ST q; i.e., stabH (Uk) = ⟨ST q⟩.
Proof. Recall that by means of (6) we obtain
stabH (Uk) =

SaT b ∈H | (SaT b)12 = O and (SaT b)13 = O

.
Moreover, ⟨ST q⟩=

SaT b ∈H | b = qa, 0 ≤a < q −1

which is a cyclic sub-
group of order q −1.
First, we prove that ⟨ST q⟩is included into stabH (Uk). Consider SaT b ∈
⟨ST q⟩, and let us see that (SaT b)12 = O and (SaT b)13 = O. Since b is a multiple
of q, it follows that (SaT b)12 = O, by (12), and that
λaσb(Jn−k, λIk, Y ) + λbσa(λIn−k, Ik, X) = cX

A Construction of Orbit Codes
81
by Lemma 7. Moreover, since b = qa, then c = 0 by Lemma 8. In particular
(SaT b)13 = O and we obtain ⟨ST q⟩⊆stabH (Uk).
On the other hand, if SaT b ∈stabH (Uk), it follows that (SaT b)12 = O and
(SaT b)13 = O. Since (SaT b)12 = O, its (k, 1)-entry will be zero; this entry is
λab
1

= λab. Then b ≡0 (mod q) and then b = qu where 0 ≤u < q −1. This
means in particular that b ≡u (mod (q −1)). Moreover, by (13) we have
O = (SaT b)13 = cX1.
Now, since X1 ̸= O, we obtain c = 0. Then by Lemma 8, a ≡b (mod (q −1)).
Therefore a ≡u (mod (q −1)) and then a = u, since 0 ≤a, u < q −1. Thus
b = qa and SaT b ∈⟨ST q⟩.
⊓⊔
Now, we are going to specify matrix X such that the minimum distance of
the code C is maximal. So we consider
X1 = Ik
and
X2 =
⎡
⎢⎢⎢⎣
1 0 · · · 0
0 0 · · · 0
...
...
...
0 0 · · · 0
⎤
⎥⎥⎥⎦.
(14)
Theorem 2. For X1 and X2 in (14), it follows that
rank

(SaT b)12 (SaT b)13

= k,
for all SaT b ∈H\ stabH (Uk).
Proof. Let SaT b ∈H\ stabH (Uk); that is, 0 ≤a < q −1, 0 ≤b < q(q −1) and
b ̸= qa by Theorem 1. In particular, this means that e1 ̸= 0 by Lemma 7.
Now, by (13) and (14), it follows that
(SaT b)13 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
c + ek e1 e2 e3 · · · ek−1
ek−1
c e1 e2 · · · ek−2
ek−2
0 c e1 · · · ek−3
ek−3
0 0 c · · · ek−4
...
...
...
...
...
e1
0 0 0 · · ·
c
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
If c = 0 then det(SaT b)13 = (−1)kek
1 ̸= 0 and we obtain the desired result.
Thus, we may assume that c ̸= 0. In this case, it follows that
k −1 ≤rank(SaT b)13 ≤k.
If rank(SaT b)13 = k, the result holds. On the other hand, if rank(SaT b)13 = k−1,
then
0 = det(SaT b)13 = det
c + ek u
v T
U

(15)
where u =

e1 e2 e3 · · · ek−2

and v =

ek−1 ek−2 ek−3 · · · e1

.

82
J.-J. Climent et al.
Arguing by contradiction, assume that rank

(SaT b)12 (SaT b)13

= k −1.
Therefore, by (11), if we consider the ﬁrst column of
1
λa (SaT b)12 and the last
k −1 columns of (SaT b)13, we have that
rank
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
b
k

e1 e2 e3 · · · ek−1
 b
k−1

c e1 e2 · · · ek−2
 b
k−2

0 c e1 · · · ek−3
 b
k−3

0 0 c · · · ek−4
...
...
...
...
...
b
1

0 0 0 · · ·
c
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
= rank
 ek u
v T U

= k −1
(16)
and by means of (15) and (16),
0 = det
c + ek u
v T
U

= det
 c u
0 T U

+ det
 ek u
v T U

= ck + 0,
which is a contradiction since c ̸= 0. It follows that rank

(SaT b)12 (SaT b)13

= k, for all SaT b ∈H\ stabH (Uk) and the result holds.
⊓⊔
Finally, as a consequence of the above results, we obtain our main theorem.
Theorem 3. Assume that q is a prime number such that q ≥n −k where k
and n are nonnegative integers where k ≤n/2. Assume also that λ ∈Fq is a
primitive element. Consider the upper triangular matrices S and T deﬁned in
(7), and X =
X1
X2

where X1 and X2 are deﬁned in (14). If H = ⟨S, T⟩is the
Abelian non-cyclic subgroup of GLn given in (8), and C is the orbit code deﬁned
in (4), then |C| = q(q −1) and d(C) = 2k.
Proof. |C| = q(q−1) as a consequence of (8) and Theorem 1. Moreover, d(C) = 2k
results from (4), (5) and Theorem 2.
⊓⊔
4
Open Questions
We would like to know if it is possible to extend our construction to any ﬁnite
ﬁeld Fq, where q is a prime power. Moreover, we wonder if it is possible to obtain
longer, constant dimension codes starting from our Abelian orbit code without
compromising the distance.
Acknowledgements. The ﬁrst author was supported by grants MIMECO MTM2015-
68805-REDT and MTM2015-69138-REDT.

A Construction of Orbit Codes
83
References
1. Bardestani, F., Iranmanesh, A.: Cyclic orbit codes with the normalizer of a Singer
subgroup. J. Sci. Islamic Republic of Iran 26(1), 49–55 (2015)
2. Bartoli, D., Pavese, F.: A note on equidistant subspace codes. Discrete Appl. Math.
198, 291–296 (2016)
3. Ben-Sasson, E., Etzion, T., Gabizon, A., Raviv, N.: Subspace polynomials and
cyclic subspace codes. IEEE Trans. Inf. Theory 62(3), 1157–1165 (2016)
4. Cossidente, A., Pavese, F.: On subspace codes. Des. Codes Crypt. 78(2), 527–531
(2016)
5. Etzion, T., Vardy, A.: Error-correcting codes in projective space. IEEE Trans. Inf.
Theory 57(2), 1165–1173 (2011)
6. Ghatak, A.: Construction of Singer subgroup orbit codes based on cyclic diﬀerence
sets. In: Proceedings of the Twentieth National Conference on Communications
(NCC 2014), pp. 1–4. IEEE, Kanpur, India, February 2014
7. Gluesing-Luerssen, H., Morrison, K., Troha, C.: Cyclic orbit codes and stabilizer
subﬁelds. Adv. Math. Commun. 9(2), 177–197 (2015)
8. Gluesing-Luerssen, H., Troha, C.: Construction of subspace codes through linkage.
Adv. Math. Commun. 10(3), 525–540 (2016)
9. Gorla, E.G., Ravagnani, A.: Equidistant subspace codes. Linear Algebra Appl.
490, 48–65 (2016)
10. Ho, T., Koetter, R., M´edard, M., Karger, D.R., Eﬀros, M.: The beneﬁts of coding
over routing in a randomized setting. In: Proceedings of the 2003 IEEE Interna-
tional Symposium on Information Theory (ISIT 2003), p. 442. IEEE, Yokohama,
Japan, June/July 2003
11. Koetter, R., Kschischang, F.R.: Coding for errors and erasures in random network
coding. IEEE Trans. Inf. Theory 54(8), 3579–3591 (2008)
12. Rosenthal, J., Trautmann, A.L.: A complete characterization of irreducible cyclic
orbit codes and their Pl¨ucker embedding. Des. Codes Crypt. 66, 275–289 (2013)
13. Silberstein, N., Trautmann, A.L.: New lower bounds for constant dimension codes.
In: Proceedings of the 2013 IEEE International Symposium on Information Theory
(ISIT 2013), pp. 514–518. IEEE, Istanbul, July 2013
14. Slepian, D.: Group codes for the Gaussian channel. Bell Syst. Tech. J. 47(4), 575–
602 (1968)
15. Trautmann, A.L.: Isometry and automorphisms of constant dimension codes. Adv.
Math. Commun. 7(2), 147–160 (2013)
16. Trautmann, A.L., Manganiello, F., Braun, M., Rosenthal, J.: Cyclic orbit codes.
IEEE Trans. Inf. Theory 59(11), 7386–7404 (2013)
17. Trautmann, A.L., Manganiello, F., Rosenthal, J.: Orbit codes - a new concept in
the area of network coding. In: Proceedings of the 2010 IEEE Information Theory
Workshop (ITW 2010). IEEE, Dublin, Ireland, August 2010

Analysis of Two Tracing Traitor Schemes
via Coding Theory
Elena Egorova and Grigory Kabatiansky(B)
Skolkovo Institute of Science and Technology (Skoltech),
Moscow Region 143025, Russia
elena.egorova@skolkovotech.ru, G.Kabatyansky@skoltech.ru
Abstract. We compare two popular tracing traitor schemes (1) using
non-binary codes with identiﬁable parent property (IPP-codes) and
(2) using family of sets with identiﬁable parent property. We establish
a natural basis for comparing and show that the second approach is
stronger than IPP-codes. We also establish a new lower bound on the
cardinality of the family of sets with identiﬁable parent property.
1
Introduction
In [1] Chor, Fiat and Naor introduced traitor tracing schemes in the context of
broadcast encryption. Their main idea was to encrypt data in such a way that
it prevents illegal redistribution of digital content. Namely, when a malicious
coalition of users (“traitors”) of a limited size creates a “device” for an unau-
thorized user then the distributor is able to identify at least one traitor. To this
end, the distributor encrypts a data block with the corresponding session key
and gives the authorized users personal keys to decrypt them. In order to cre-
ate unauthorized decryption keys (decoders), some coalition of traitors creates a
forged key/decoder based on their common knowledge of keys/decoders. Assum-
ing that the cardinality of the coalition is not greater than t, once a forged key is
observed, the distributor should be able to trace back at least one traitor from a
malicious coalition. Such schemes could be “open” and “secret” in the notations
of [1], where a “secret” scheme is in fact a family of “open” schemes but the
particular choice of one of them is unknown to the traitors. Another diﬀerence
is that for a secret scheme it is allowed to trace traitor with the probability of
error enough small (tending to zero with the “length” of a scheme) but an open
scheme should provide traitor traicing with zero-error probability. One simple
class of open schemes was proposed in [1] and developed further under the name
of codes with the identiﬁable parent property (IPP-codes), and its particular
case called t-traceability codes, in [2]. These codes were extensively studied, see
e.g. [3–5], also a detailed overview can be found in [6].
The original idea of [1] was to establish a traitor tracing (TT) schemes (like
t-IPP codes) based on perfect secret sharing schemes (SSS for short). Perfect SSS
were discovered in [7,8]. From this point of view t-IPP codes can be considered
as TT schemes based on the simplest (n, n)-threshold SSS. Extension of this idea
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 84–92, 2017.
DOI: 10.1007/978-3-319-66278-7 8

Analysis of Two Tracing Traitor Schemes via Coding Theory
85
to arbitrary (w, n)-threshold SSS was proposed in [9] under the name of family
of IPP-sets, and further developed in [10,11]. Surprisingly no relationship as well
as comparison of these two types of TT schemes were given before. We do it in
this paper, by returning, in some sense, to the paper [1]. Another result is a new
lower bound on the cardinality of the best family of IPP-sets.
2
Open Tracing Traitor Schemes - How Do They Work?
Consider a broadcasting scenario where a dealer distributes some digital content
to M users. In order to prevent illegal redistribution, the dealer sends the content
in an encrypted form obtained by using of some secret key k, which serves as
a session key and should be changed for distributing another portion of digital
content. For distribution the key k to users, the dealer transmits some blocks of
information e1, . . . , eN, which allow any legal user to recover k. In order to trace
member(s) of a malicious coalition the dealer forms ej as encrypted version of
some secret information sj. The i-th user receives the corresponding set of keys
{f1i, . . . , fni} during the initialization phase. Let us cite [1]:
“We devise t-resilient traceability scheme with the following properties: 1. Either
the cleartext information itself is continuously transmitted to the enemy by a
traitor, or 2. Any captured pirate decoder will correctly identify a traitor and
will protect the innocent even if up to t traitors combine and collude....
Deﬁnition 1. An n user open TT scheme is called t resilient if for every
coalition of at most t traitors the following holds: suppose the coalition uses the
information its members got in the initialization phase to construct a pirate
decoder. If this decoder is capable of applying the decryption scheme, then the
tracing traitors algorithm will correctly identify one of the coalition members.”
Below we make this deﬁnition more precise for two particular and most pop-
ular tracing traitors schemes. Let us start from the scheme proposed in [2].
We assume that the session key k belongs to some q-ary alphabet. Below it
will be the ﬁnite ﬁeld GF(q). The corresponding secret information “symbols”
s1, . . . , sn are random uniformly distributed variables with values from GF(q)
with the property that
s1 + . . . + sn = k
(1)
where summation is taken in the ﬁeld GF(q). In other words, s1, . . . , sn ∈
GF(q), where s1, . . . , sn−1 are independent random uniformly distributed ele-
ments of GF(q) and sn = k −n−1
1
si. The values sj are called shares of
k (see below for Secret Sharing Scheme) and every share sj is encrypted on
q diﬀerent subkeys from the set F (j) = {fj1, . . . , fjq}, resulting in the set
of q encrypted shares {ej1, ..., ejq}. The set of nq corresponding encrypted
shares {e11, . . . , e1q, . . . , en1, . . . , enq} is transmitted by the dealer along with
the encrypted portion of digital content.
The i-th user receives during initialization phase the set of keys {f1i1, . . . , fnin},
where fjij ∈F (j), i.e., each user receives exactly one key from every set F (j).

86
E. Egorova and G. Kabatiansky
Since |F (j)| = q for all j = 1, 2, . . . , n one can consider the set {f1i1, . . . , fnin}
as a q-ary codevector ci ∈GF(q)n assigned to the i-th user, and the set of all such
vectors is called a ﬁngerprinting code C ⊂GF(q)n. If a coalition of malicious
users (traitors) U ⊂{1, . . . , M} wants to create a “device” (“decoder”) which
will be able to decrypt every transmitted encrypted portion of digital content
then the coalition have to create a new set Y = {y1, . . . , yn} of subkeys with the
property that yj ∈F (j) for all j ∈{1, 2, . . . , n}. It is very important to note that
yj ∈{fjuj : u ∈U}, i.e. that the coalition can choose subkeys only from the set
of the coalition subkeys. Hence the resulting problem can be formulated in the
language of codes as it was done in [2]. Namely, deﬁne for any set U ⊂GF(q)n
and any coordinate i its the i-th projection Pi(U) of the set V as
Pi(U) =

u∈U
ui.
For a ﬁngerprinting code C we shall denote by U a coalition of traitors as
well as the corresponding to them set of codevectors. Then we denote by <U>
the set of all false ﬁngerprints (also called descendants [2]) that the coalition U
can create, namely,
<U> = {x = (x1, . . . , xn) ∈F n
q : ∀i xi ∈Pi(U)}
(2)
Let
Et(C) = ∪U⊂C: |U|≤t<U>
denote the set of all false ﬁngerprints which can be created by all coalitions U
of size at most t.
Deﬁnition 1 ([2]). A code C has the identiﬁable parent property of order t, or
C is t-IPP code for short, if for all z ∈Et(C)
Ct(z) :=

U: z∈<ϕ(U)>, |U|≤t
U ̸= ∅
(3)
Hence, if the ﬁngerprints form a code possessing the Identiﬁable Parent Prop-
erty, then from any false ﬁngerprint z created by a coalition U, at least one user
from U will be identiﬁed without any doubt.
It is easy to see that q-ary t-IPP codes do not exist for t ≥q. On the other
hand, for t < q there exist families of t-IPP codes with non-vanishing rate, i.e.
with a number of codewords growing exponential in n, see [2–4]. Note that for
the binary case there are no t-IPP codes even for coalitions of size 2.
Nevertheless there is a natural way to represent q-ary IPP-codes as binary
codes but with q times larger length. Namely, consider one of the simplest con-
catenated codes, proposed in [15], when j-th q-ary symbols is replaced by the
corresponding binary vector of length q, which all coordinates are zeroes except of
j-th coordinate which is 1. Denote by XN
q ⊂{0, 1}N the set of all binary vectors
x = (x1, . . . , xN) of length N = nq such that the Hamming weight of every block

Analysis of Two Tracing Traitor Schemes via Coding Theory
87
xj = (x(j−1)q+1, . . . , xjq) equals to 1, j = 1, . . . , n. We shall say that a binary vec-
tor x = (x1, . . . , xN) is covered by a binary vector y = (y1, . . . , yN) and denote
it as y ≻x if yi ≥xi for all i. For every coalition U = {u1, . . . , ut} ⊂XN the
corresponding set of descendants <U> consists of all vectors z ∈XN which are
covered by the vector U = u1 ∨. . . ∨ut. Then the corresponding set <U>bin of
all false binary ﬁngerprints (binary descendants) that the coalition U can create
equals <U>bin = {x ∈XN : U ≻x}.
Now the Identiﬁable Parent Property can be reformulated for a binary ﬁn-
gerprinting code Cbin of length N = nq in the following way: for every z ∈XN
either all coalitions which can create z have at least one common member, i.e.,

U: z∈<U>bin, |U|≤t
U ̸= ∅,
(4)
or no coalition of size at most t can create a given z.
The IPP-codes can be explained by the following toy example. Imagine that
the dealer should allow access to a treasury chest for M users, but some of them
are dishonest. The dealer locks the treasury chest by n diﬀerent locks and each
lock has q diﬀerent key holes and correspondingly q diﬀerent keys. Therefore the
dealer provides to every user a unique set of n keys - by one key for every lock.
Then a malicious coalition U can create a fraud set of keys by giving one key for
every lock from the keys at their disposal. The aim of the dealer is to form the
users’ sets of keys in such a way that from any suﬃcient set of keys created by
a coalition, the dealer can for sure reveal at least one member of the coalition.
In fact, the basic property of the above described TT scheme is that it is
based on perfect secret sharing scheme, and as it was noted in [1] that any
such scheme can be used for constructing a TT scheme. This was demonstrated
in [1] by the so-called two-level scheme. But there is a more natural and simple
generalization of IPP TT-scheme, namely, by using of w-out-of-n SSS. Note that
IPP-codes correspond to the case of n-out-of-n SSS. Let us recall the well known
Shamir’s polynomial w-out-of-n SSS.
Let there be N participants, k be a common secret key, GF(q) be the ﬁeld
of q elements with q > N and let α1, ..., αN be N diﬀerent nonzero elements of
GF(q). The dealer chooses a random polynomial
f(x) = k + f1x + ... + fw−1xw−1
(5)
with coeﬃcients f1, ..., fw−1 ∈GF(q) which are independent random variables,
uniformly distributed on GF(q). The share assigned to the i-th participant is
si = f(αi). Then any w (or more) participants can recover the secret key k since
the degree of polynomial f is w −1, and any set of less than w participants get
no information about k. This is the famous w-out-of-n threshold secret sharing
scheme [7].
Based on this secret sharing scheme, the following TT scheme, called the
t-IPP family of sets, was proposed in [9]. Namely, the dealer generates a random
polynomial f(x) of degree w −1, chooses for the i-th user its personal subset
Ai ⊂{1, ..., N}, where |Ai| = w, and secretly distributes to the i-th user the

88
E. Egorova and G. Kabatiansky
values of the polynomial f(x) at points αj : j ∈Ai. In order to distribute shares
to users in a secret way, the dealer encrypts each share sj = f(αj) by its key
Ej, distributes during the initialization step to the i-th user its encryption keys
Ej : j ∈Ai, and ﬁnally sends all N encrypted shares via a broadcast channel.
Any authorized user can decrypt w shares, then recover from them the secret key
k, and hence can decrypt the transmitted digital content. A malicious coalition
U can create a fraud “decoder” by arranging together at least w diﬀerent key
Ej which belong to members of U. Denote the corresponding set of keys by
ˆE =

j∈ˆ
A
Ej,
where ˆA ⊂
i∈U Ai and its cardinality | ˆA| should be at least w. Thus the set of
descendants of the coalition U equals to
<U>set = {B ⊂{1, ..., N} : B ⊂

i∈U
Ai, |B| ≥w}
(6)
Now the Identiﬁable Parent Property can be reformulated for a family of sets
in the following way.
Deﬁnition 2 ([9]). We shall say that a family F of w-subsets of a N-set
{1, ..., N} has the identiﬁable parent property of order t, or F is t-IPP family of
sets for short, if for any ˆA ⊂{1, ..., N} such that | ˆA| ≥w either

U: ˆ
A∈<U>set, |U|≤t
U ̸= ∅,
(7)
or there is no U such that |U| ≤t and ˆA ∈<U>set
In words, a family F of w-subsets of a N-set {1, ..., N} is a t-IPP family of
sets if for any w-subset which belongs to the union of some t sets of F at least
on of these sets can be uniquely determined. In particular, it means that no one
set of F belongs to the union of t other sets of F. Such families of sets are very
popular, thanks to [13,14]. They appeared ﬁrst in coding theory under the name
of superimposed codes, see [15–17].
3
How to Compare t-IPP Codes and t-IPP Family
of Sets?
In order to compare t-IPP codes and t-IPP family of sets we use descriptions of
both schemes in the language of binary vectors. Recall that a q-ary t-IPP code of
length n can be represented as a binary code of length N = nq, in which vectors
have block structure where every block of length q consists of all zeroes except
for a single 1, and the code satisﬁes the property (4).
As for t-IPP family of sets, it can be reformulated in the language of binary
vectors almost in the similar way. To construct a binary t-IPPS code C of length

Analysis of Two Tracing Traitor Schemes via Coding Theory
89
N and weight w, one should replace every set of a t-IPP family of sets with its
characteristic vector. We call the code C a binary t-IPPS code. A collusion attack
of a malicious coalition U ⊂C in binary-vectors language proceeds as follows.
The coalition U creates a fraud vector z = (z1, . . . , zN) of the Hamming weight
at least w under the following rule which is an analog of Marking Assumption
for digital ﬁngerprinting codes, see [12]:
if all users of U have 0 in j-th coordinate then zj = 0,
and zj could be 0 or 1 if at least one user has 1 in this coordinate.
This description reveals the main diﬀerence between IPPS codes and IPP
codes. In the binary representation of q-ary IPP codes, if all vectors of a coalition
in the j-th block have 1 at the same position, then this position in a fraud
vector must also be 1. But in IPPS code, both 0 an 1 can be placed in a fraud
vector in this case, nevertheless the resulting fraud vector should have weight
at least t. Hence IPPS codes resists a more general attack than IPP codes. On
the other hand, IPPS codes have less restrictions as codes, namely, code-vectors
should have weight w = n without the need for having each of the n blocks has
weight exactly one. Therefore our concatenated construction of IPPS codes is
of particular interest since the corresponding codes have the structure of IPP
codes (every of n blocks has weight exactly one) but resists a wider attacks. To
compare both types of codes, it is natural to put w := n for IPPS codes and it is
very interesting to construct IPPS codes with cardinality larger than of known
IPP codes.
Another reason to compare these two types of TT schemes on the basis of
equal parameter N is that N is proportional to the overhead which the dealer
should transmit over a broadcast channel. Indeed, in the case of IPP codes,
the dealer should send n shares and each of them q times diﬀerently encrypted.
In the case of the IPP family of sets the dealer should send encrypted values
of the polynomial f(x) in all N points. Note that for the case of IPP codes
it is ineﬃcient to choose a large value of q. Let Mt(n, q) denotes the maximal
possible cardinality of t-IPP code, and Rt(n, q) = N −1 log2 Mt(n, q) denotes the
corresponding binary code rate. Then
Rt(n, q) ≤(nq)−1n log2 q = log2q
q
and it tends to zero with growing q. For example, the best known lower bound
on the rate of 2-IPP codes [2] achieves its maximum for q = 3 (the minimal
possible q).
4
New Lower Bound on the Size of IPPS Codes
Let ω := w/N be the relative weight, and M(N, ω) be the maximum possible
size of t-IPPS code of length N and relative weight ω. Denote by
Rt(n, ω) := log2 M(N, ω)
N

90
E. Egorova and G. Kabatiansky
the maximum possible rate and by Rt(N) denote the maximum of Rt(N, ω) over
ω ∈[0, 1].
Deﬁnition 3. A family F = {S1, ..., SM} of w-subsets of {1, . . . , N} is called a
t-traceability set system (t-TSS) if for any coalition U = {i1, ..., ik} ⊂[M],
k ≤t and any S ∈<U>set, it holds
|S ∩Sj| < |S ∩Sil| for any l=1,...,k and any j ̸∈U
This deﬁnition means that for a t-traceability set system, search for malicious
user(s) reduces to the search of “closest” sets. Moreover, any t-TSS is also t-IPP
family of sets, so a lower bound on the size of t-TSS is also a lower bound for
the cardinality of t-IPPS codes.
The notion of t-traceability was coined in [9] and further studied in [10,11].
These papers provide results about upper bounds as well as about lower bounds
on the size of traceability set systems. We are interested in ﬁnding a new lower
bound for t-TSS since it automatically gives us a new lower bound on the size
of t-IPPS codes.
4.1
The Previous Results
In paper [1] the authors provided the following bound: Rt ≥
1
8t4 with ω =
1
2t2 .
In recent paper [11] the authors proved that
|M(n, ω)| ≥

n
⌈w/t2⌉


w
⌈w/t2⌉
,
which asymptotically means Rt(ω) ≥H( ω
t2 ) −2ωH( 1
t2 ).
Below we improve the aforementioned bounds. The following simple lemma
establishes a natural condition on a set system to be t-TSS (very similar to
original approach of [1]):
Lemma 1. If |Si∩Sj| < w/t2 for any Si, Sj ∈F, i ̸= j, then F is a t-traceability
set system.
Proof. Consider any coalition U = {i1, ..., ik}, k ≤t and any S ∈<U>. Then,
maxl∈{1,...,k} |Sil ∩S| ≥w/t since |S| ≥w. On the other hand, for any j ̸∈U,
|Sj ∩S| < |Sj ∩(Si1 ∪... ∪Sik)| < |Sj ∩Si1| + ... + |Sj ∩Sik| < t · w
t2 = w
t .
4.2
Constant Weight Codes with Large Distance and Traceability
Set System
Obviously, from the set systems paradigm we can move to a language of binary
vectors. This will imply the change of “distance”. Indeed, if we consider the set
systems we appeal to the cardinality of intersections, but if we consider the code
consisting of binary vectors we will appeal to the Hamming distance.

Analysis of Two Tracing Traitor Schemes via Coding Theory
91
More formally, consider the binary code CF , corresponding to family F,
consisting of the characteristic vectors of sets Si ∈F. Then, CF is a constant
weight code of weight w = ωn with the minimal Hamming distance d ≥2τn
where τ = ω(1−1/t2) and vice versa. Application of VG-bound for the constant
weight codes, see [18], leads to the following new lower bound.
Theorem 1. There exists a t-IPPS code with rate
Rt(ω) ≥H (ω) −ωH
 τ
ω
	
−(1 −ω) H

τ
1 −ω

where H is the binary entropy function and τ = ω(1 −1/t2).
If we compare the received bound with previous ones, we can note that the new
one is better for arbitrary values of parameter t. Indeed, straight calculations
show that for t = 2: Theorem 1 implies that R2 = maxω Rt(ω) ≥0.0181 and
the best known bound is R2,M = 0.0159 [11]. For t = 3: the new bound is
R3 = 0.00316 and the best known bound is R3,M = 0.0027 [11]. For arbitrary t
it is also true since it can be seen from the following inequality
H(ω) + H(t−2) ≥(1 −ω)H

t−2ω/(1 −ω)

+ H(t−2ω)
that the new bound is always better than previously known ones.
References
1. Chor, B., Fiat, A., Naor, M.: Tracing traitors. In: Desmedt, Y.G. (ed.) CRYPTO
1994. LNCS, vol. 839, pp. 257–270. Springer, Heidelberg (1994). doi:10.1007/
3-540-48658-5 25
2. Hollmann, H.D., van Lint, J.H., Linnartz, J.P., Tolhuizen, L.M.: On codes with the
identiﬁable parent property. J. Comb. Theor. Ser. A 82(2), 121–133 (1998)
3. Barg, A., Cohen, G., Encheva, S., Kabatiansky, G., Z´emor, G.: A hypergraph app-
roach to the identifying parent property: the case of multiple parents. SIAM J.
Discrete Math. 14(3), 423–431 (2001)
4. Alon, N., Cohen, G., Krivelevich, M., Litsyn, S.: Generalized hashing and parent-
identifying codes. J. Comb. Theor. Ser. A 10(1), 207–215 (2003)
5. Staddon, J.N., Stinson, D.R., Wei, R.: Combinatorial properties of frameproof and
traceability codes. IEEE Trans. Inf. Theor. 47, 1042–1049 (2001)
6. Blackburn, S.R.: Combinatorial schemes for protecting digital content. Surv. Comb.
307, 43–78 (2003)
7. Shamir, A.: How to share a secret. Commun. ACM 22(11), 612–613 (1979)
8. Blakley, G.R.: Safeguarding cryptographic keys. Proc. Natl. Comput. Conf. 48,
313–317 (1979)
9. Stinson, D.R., Wei, R.: Combinatorial properties and constructions of traceability
schemes and frameproof codes. SIAM J. Discrete Math. 11(1), 41–53 (1998)
10. Collins, M.J.: Upper bounds for parent-identifying set systems. Des. Codes Cryp-
togr. 51(2), 167–173 (2009)
11. Gu, Y., Miao, Y.: Bounds on traceability schemes. arXiv preprint arXiv:1609.08336
(2016)

92
E. Egorova and G. Kabatiansky
12. Boneh, D., Shaw, J.: Collusion-secure ﬁngerprinting for digital data. IEEE Trans.
Inf. Theor. 44, 1897–1905 (1998)
13. Erdos, P., Frankl, P., Furedi, Z.: Families of ﬁnite sets in which no set is covered
by the union of two others. J. Comb. Theor. Ser. A 33(2), 158–166 (1982)
14. Furedi, Z., Erdos, P., Frankl, P.: Families of ﬁnite sets in which no set is covered
by the union ofr others. Isr. J. Math. 51(1), 79–89 (1985)
15. Kautz, W., Singleton, R.: Nonrandom binary superimposed codes. IEEE Trans.
Inf. Theor. 10(4), 363–377 (1964)
16. Dyachkov, A.G., Rykov, V.V.: Bounds on the length of disjunctive codes. Probl.
Inf. Transm. 18(2), 166–171 (1982)
17. Quang, A.N., Zeisel, T.: Bounds on constant weight binary superimposed codes.
Probl. Control Inf. Theor. 17, 223–230 (1988)
18. Zinov’ev, V.A., Ericson, T.: On concatenated constant-weight codes beyond the
Varshamov-Gilbert bound. Probl. Inf. Transm. 23(1), 110–111 (1987)

Reliable Communication Across Parallel
Asynchronous Channels with Glitches
Shlomo Engelberg1 and Osnat Keren2(B)
1 Department of Electrical and Electronics Engineering,
School of Engineering and Computer Science,
Jerusalem College of Technology, Jerusalem, Israel
2 Faculty of Engineering, Bar-Ilan University, Ramat Gan, Israel
osnat.keren@biu.ac.il
Abstract. Transmission across asynchronous communication channels
is subject to laser injection attacks which cause glitches, pulses that
are added to the transmitted signal at arbitrary times, and delays. We
present self-synchronizing coding schemes with low latency at the receiver
that require no acknowledgement and can decode transmissions subject
to random delays and distorted by random glitches.
Keywords: Random delays · Glitch · Parallel asynchronous communi-
cations
1
Introduction
In this work, we consider a channel composed of multiple wires each of which
suﬀers from random delays and glitches. Because the wires suﬀer from random
delays, a receiver that sees their output must decide how to group the bits from
the signals seen on the N wires to form N bit binary vectors. Such a channel is
an example of an asynchronous channel. The error mechanisms and models in
such channels diﬀer signiﬁcantly from those of synchronous channels.
Synchronization errors in synchronous channels may cause insertions, dele-
tions or substitution of symbols [9] which change the block boundaries. For exam-
ple, the channel input may be a sequence of, say, n symbols and the output a
sequence of m symbols, m being a random variable depending on the number of
insertions/deletions. The positions of insertions, deletions, and substitutions are
random and unknown to both the transmitter and the receiver. Capacity bounds
and coding schemes for such channels are described in the literature [5,6,8]. Syn-
chronous channels may also suﬀer from delays that change the arrival time of
symbols by a random (yet, bounded) number of time slots. Consider, for example,
the parallel indistinguishable channels studied in [11] in which the information
is conveyed via the number of “particles” transmitted in each time slot.
The research of the second author was supported by the ISRAEL SCIENCE FOUN-
DATION (grant No. 923/16). A preliminary version of part of this work was pre-
sented at TRUEDEVICE 2016, Barcelona, Spain.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 93–106, 2017.
DOI: 10.1007/978-3-319-66278-7 9

94
S. Engelberg and O. Keren
In many synchronous binary channels, the receiver “sees” both zeros and
ones. This is not the case in communication over asynchronous binary channels
where the sender and receiver have diﬀerent clock frequencies. A typical parallel
asynchronous channel without feedback consists of N wires connecting two units.
When transmitting a zero, the transmitter leaves a wire in its default state, and
when transmitting a one, the transmitter sends a (positive) pulse down the wire.
Transmissions on all wires take place simultaneously, however the arrival times
may vary.
The receiver is composed of an asynchronous front-end followed by a buﬀer
and a synchronous block. The asynchronous front-end consists of edge detectors
and an N →log2(N) encoder. (See Fig. 1.) Upon detecting the rising edge of a
pulse on a wire, the asynchronous front-end determines that it has seen a pulse
on that wire and updates the buﬀer accordingly. The buﬀer maintains an ordered
list of integers that represent the wires on which pulses have appeared and the
order in which they appeared since the list was last read by the receiving block.
Unless a properly crafted code is used, the receiver will not be able to distinguish
between a single transmission over k wires and k (or fewer) transmissions over
the same wires. In this work, all channels are taken to be asynchronous channels
without feedback.
Fig. 1. Communication scheme over an asynchronous channels. The transmitting and
receiving blocks have diﬀerent (unsynchronized) clocks which may experience jitter and
vary in time.
In some asynchronous channels, the propagation time over the parallel wires
may cause a skew. That is, pulses from diﬀerent transmissions may mix with one
another. In a skew-less asynchronous channel, the pulses sent in any transmission
can be received in any order but all the pulses from the ith transmission arrive at
the receiver before any pulse from the (i + 1)th transmission arrives. In a skew-
less, noise-free channel, unordered codes (codes for which no valid codeword is
“contained” in another valid codeword [2]) are zero-error codes.
Some asynchronous channels suﬀer from glitches – informally, unwanted sig-
nals that are erroneously identiﬁed by the receiver as valid pulses. Our contribu-
tion is the description of techniques for designing zero-latency (instantaneously
decodable) and low-latency codes for asynchronous skew-less channels that suﬀer
from glitches and providing bounds on the rate such codes can achieve.

Parallel Asynchronous Channels with Glitches
95
The techniques described in this paper allow one to ameliorate the eﬀects
of both naturally occurring and intentionally inserted glitches on asynchronous
channels. As error injection can sometimes be used to obtain information about
cryptographic keys [1], the techniques presented here can also play a role in
securing certain communications systems against fault injection attacks.
The rest of this paper is organized as follows. The problem is formulated in
the next section. Section 3 reviews related work, and in Sect. 4 we describe three
techniques for reliable low-latency communication across asynchronous channels
with glitches. Section 5 concludes the paper.
2
Problem Formulation
Let p(t) be the pulse transmitted when a one is to be sent, let ˆp(t) represent a
typical (physical) glitch, and let Tk be the time between the (k −1)th and the
kth transmission; Tk is unknown to the receiver. Let the waveform transmitted
on the ith wire be
Si(t) =
W

k=1
ci,kp

t −
 k

l=1
Tl

where {ci,k} is the sequence of ones and zeros transmitted on the ith wire. We
say that the vector
c:,k = (c1,k, c2,k, . . . , cN,k)
is the codeword transmitted over the channel at the kth time slot. The set of
possible codewords that can be transmitted at the kth time slot forms the code Ck
for this time slot. (That is, diﬀerent codes can be used in diﬀerent time slots.) In
what follows, we describe coding schemes in which the set of possible codewords
at time-slot k is a function of k mod W, W ≥1. We call the set of all sequences
of W codewords {ci,k}N,W
i=1,k=1 ⊆ZN×W
2
a coding framework and denote it by F.
We are interested in reliable communication in the presence of physical-
glitches. The received waveform is modeled as
Ri(t) =
W

k=1
ci,kp

t −
 k

l=0
Tl

−τ −τi,k

+
W

k=1
ei,k ˆp

t −
 k

l=0
Tl

−τ −θi,k

where {ei,k} represents the sequence of glitches on the ith wire, τ is the average
of the propagation delay over all the wires, where τi,k and θi,k are the delays of
the pulses and the glitches, respectively, and where 0 ≤τi,k, θi,k < Ti+1 do not
cause pulses on the same line to interfere with one another.
We assume that the physical-glitches are uncorrelated with the transmitted
pulses and that the delays between the wires are uncorrelated; that is, for all
i ̸= j, m, k, the delays τi,k and τj,m are statistically independent. (In practice,
this may not be the case. As the glitches and delays may be being caused by

96
S. Engelberg and O. Keren
a laser beam, they may be correlated. Any correlation could be used to enable
reliable communication with less redundancy.) We also assume that there are N
wires in use and that the probability of a glitch occurring on a wire in a single
time-slot is ϵ. Thus, the expected number of glitches on N wires is ϵN.
A simple example of a possible set of transmitted signals in which a single
glitch has been inserted in several time-slots is given in Fig. 2. The coding frame-
work used F ⊂Z4×1
2
has three words – c1 = 1010, c2 = 1100, c3 = 0011 where
the leftmost bit indicates the value transmitted on the ﬁrst wire, the next bit
indicates the value transmitted on the second wire, etc. For clarity, the trans-
mitted pulses are depicted with solid lines and the physical-glitches with dotted
lines. In this example, the transmitter sends the word c1 twice. The receiver sees
the sequence 1, 3, 2, 1, 3, 4 . . . where each number indicates the wire on which a
pulse was received and the order of the number indicate the order in which the
pulses were received. The receiver cannot distinguish between the pulses and the
physical-glitches; hence it may decide that the sequence c1, c2, c3 was sent.
In our analysis of the communication system, it is necessary to distinguish
between a physical glitch and a logical glitch; a physical glitch is an unwanted
pulse – ˆp

t −
k
l=0 Tl

−τ −θi,k

. When it arrives at the receiver, if it appears
on a wire where no pulse is supposed to arrive (ai,k+1 = 0) then the physical-
glitch is also a logical-glitch. However, when it appears on a wire where a pulse
is expected to arrive (ai,k+1 = 1), then if the glitch precedes the pulse, the glitch
is treated as the pulse, and the true pulse will (temporarily) be referred to as
a logical-glitch. In Fig. 3, for example, the physical glitch on the second wire is
not a logical glitch as there is supposed to be a one in that location. The second
“true” pulse becomes a logical-glitch as after the glitch is identiﬁed as a one, the
“true” pulse “becomes” an unwanted pulse – becomes a logical glitch. In fact,
at any given time a logical glitch can be renamed as a pulse if it takes the place
of an expected pulse. We show both that it is possible to remove logical glitches
and to correctly decode the transmission (in the presence of physical-glitches).
In general, physical glitches may:
1. Cause a decoding error. In Fig. 2, the second c1 was decoded as c2.
2. Create a new word. In Fig. 2, the word c3 was inserted after c2.
3. Aggregate. Namely, up to a certain point the correct sequences are identiﬁed,
and the “leftovers” are pushed forward.
Figure 3 shows a scenario in which three codewords are used: c1 = 110, c2 =
101 and c3 = 011. The sequence c1, c3 is transmitted, and the received sequence,
to which glitches have been added, is correctly decoded, yet, the decoder cannot
get rid of the additional pulses, and they are aggregated and then interpreted as
a codeword.
Since glitches can aggregate, a conventional unordered code with a prede-
ﬁned minimum distance cannot provide reliable transmission over this channel.
The question addressed in this paper is: What is the maximal code rate of
a coding framework that enables near-zero-latency and reliable com-
munication? That is, we are looking to ﬁnd coding frameworks F ⊂ZN×W
2

Parallel Asynchronous Channels with Glitches
97
Fig. 2. The transmitted pulses (left). There are four wires and two transmission peri-
ods. The received signals (right). The numbers inside the pulses and glitches indicate
the transmission with which the receiver associates the pulse or glitch.
Fig. 3. The transmitted pulses with no more than a single glitch per time-slot. There
are three wires and two transmission periods. The transmitted signals are shown on
the left. The signals with delays and glitches is shown in the middle. An equivalent
scenario is shown on the right hand side.
that maximize
R =
lim
N,W →∞
log2(|F|)
NW
.
3
Related Work
The problem of delay-insensitive codes is considered in [16]. Various ways of
dealing with parallel asynchronous channels that suﬀer from a relatively small
number of skews and of dealing with a single channel (N = 1) that suﬀers
from bit shifts have been considered in the past [2,3,13]. A recent work on
communication via parallel channels that suﬀer from skews of arbitrary size [7]
presented a zero-latency, zero-error code that achieves a rate of log2((
√
5+1)/2)
asymptotically as the bus width, N, tends to inﬁnity.

98
S. Engelberg and O. Keren
Skews and glitches as asymmetric errors (Z-channels): Consider a Z-channel –
a channel described by:
P(Y = 1|X = 1) = 1
P(Y = 1|X = 0) = ϵ
P(Y = 0|X = 0) = 1 −ϵ
where X is the input to the channel and Y is the output of the channel. That
is, consider a channel for which the probability of a glitch occurring is ϵ. Let
Hb(p) = −p logb(p) −(1 −p) logb(1 −p). Then the capacity of such a channel is
CZ(ϵ) = H2

1
1 + exp(He(ϵ)/(1 −ϵ))
	
−
1
1 −ϵ
1
1 + exp(He(ϵ)/(1 −ϵ))H2(ϵ).
(1)
(See, for example, [10,14].) As long as the probability of error is not too large, the
capacity of a Z-channel is greater than that of the binary symmetric channel [15].
One way to decode transmissions across asynchronous channels subject to
glitches is to wait until the transmitted codeword (and additional ones) is “seen”
at the decoder. That is, the decoder only starts decoding the current transmis-
sion when enough ones have been received since the previous transmission was
decoded that it can be (almost) certain that every one from the current trans-
mission has been received. Of course, glitches, pieces of the previous transmission
or pieces of the next transmission may have arrived too causing additional ones
to be present at the decoder. Though our wires are not Z-channels, when we
treat the wires in this way, the data seen on the N wires can be treated as N
bits received at the output of a Z-channel. Because the decoder waits until it
is (almost) certain that all the bits of the current transmission have arrived we
know that (with very high probability) all errors are due to zeros that were seen
as ones – all errors are asymmetric. In what follows we deﬁne coding schemes
for which the decoder can interpret the sequence of pulses over the bus as a
sequence of codewords transmitted over a Z-channel.
4
Coding Schemes
We now consider several codes to correct glitches in a skew-less asynchronous
channel. In the following, we assume that the decoder has successfully decoded
all previous transmissions. This serves as a kind of inductive hypothesis.
We face three principal challenges.
1. Decoding words despite not knowing precisely where one word ends and the
next begins (and this challenge exists even in a glitch-free channel).
2. Correctly decoding a word despite the presence of a number of glitches.
3. Preventing glitches from accumulating to the point that they make transmis-
sion of information impossible or impractical.

Parallel Asynchronous Channels with Glitches
99
Challenge 1 can be dealt with by using ﬁxed weight codes. The code’s weight
(and the number of wires and the probability of a glitch occurring) serves to tell
the decoder by which point the current codeword ends (with high probability)
and allows the decoder to determine the point by which the next word may
begin (with high probability). Challenge 3 can be dealt with by not allowing two
ones to be transmitted back-to-back – by forcing ones to appear alone. That is,
assuming that ck was decoded correctly, when decoding the (k + 1)th codeword
we require that supp(ck) ∩supp(ck+1) = ∅. This property enables us to elimi-
nate logical glitches and prevent aggregation (though, as we shall see, there are
other ways of dealing with challenge 3). Challenge 2 can be addressed by meth-
ods that allow an asynchronous bus suﬀering from glitches to be treated like a
Z-channel. A sketch of a proof that coding frameworks with these three proper-
ties can be used to provide practically error free transmission of information is
given in Appendix A.1. (Appendix A.1 should be read after reading this section.)
We develop several coding frameworks that make use of the various combinations
of the methods described above. We will see that when looking to maximize the
rate, the choice of coding framework depends upon the probability of a glitch
appearing on a particular wire.
We start with by presenting a property and several deﬁnitions.
Property 1. One may choose p ∈[0, 1] and use the characteristics of the chan-
nel and the ideas used in the proof of Shannon’s noisy channel theorem in [4] to
generate constant weight codes appropriate to the channel where the density of
ones in the code can be made arbitrarily close to p.
Deﬁnition 1. A Δ-self-synchronizing code is a code that allows one to correctly
decode the kth transmission before signals from the (k + 1 + Δ)th transmission
arrive at the decoder without using external synchronization signals or feedback.
An instantaneous (zero latency) code is one for which Δ = 0, and a near instan-
taneous (single-slot-latency) code is one for which Δ = 1.
In what follows, all the techniques lead to near instantaneous (1-self-
synchronizing) codes and, in this way, allow us to overcome challenge 1.
Deﬁnition 2. An admissible sequence of length W is a sequence in which no
more than a single one occurs between successive zeros (and this deﬁnition diﬀers
from that in [7]).
Though it is not necessary to use admissible sequences in each transmission in
order to meet challenge 3, we will ﬁnd it necessary to arrange our transmissions
in such a way that from time-to-time we can identify and delete ones that have
aggregated.
Property 2. If one uses admissible sequences on each wire – as the rows of a
coding framework, then the coding framework’s rate is upper bounded by
R ≤
lim
W →∞
log2(A(W))
W
	
= log2(φ)
(2)

100
S. Engelberg and O. Keren
where A(W) is the number of admissible sequences of length W and φ is the
golden ratio: φ ≡(1 +
√
5)/2.
The correctness of (2) follows from [12, p. 70] because A(W) satisﬁes A(W) =
A(W −1) + A(W −2) and is equal to the (W + 1)th Fibonacci number. Thus,
asymptotically, the number of admissible sequences tends to φW .
In principle, if we knew when each new codeword began (and hence, when
the previous codeword ended), if we had a genie who could give us this piece of
information, we could accumulate all the ones related to a single transmission
and all the glitches that arrived during that period, and then we would have a
collection of Z-channels.
We now consider several techniques that use constant-rate codes and allow
us to “convert” our channel into a channel whose rate is closely related to that
of the Z-channel. By making use of the admissible sequence idea, we are able to
formulate coding frameworks that allow for (near) error-free communication.
4.1
Single-Code Based Coding Frameworks (W = 1)
Single-code based frameworks make use of the same underlying code in each
time-slot, and the error correcting capability in each time slot is ﬁxed.
Transmitting on Half the Wires. Consider a system in which one transmits
information on N/2 wires in each time-slot and one transmits zeros on the
rest of the wires. In the next time-slot, one transmits information on the other
N/2 channels while transmitting zeros on the channel on which information was
transmitted in the preceding time-slot.
Let ψk be the probability of at least one glitch appearing on a wire between
k transmission. Then ψk ≡1−(1−ϵ)k. Following our assumption that a (single)
glitch may be injected on a wire in between transmissions, a glitch may occur
before the transmission and a glitch that may be injected after the transmission
may arrive before the last (wanted) symbol (due to delays). Thus, at most two
glitches may appear on a single wire before decoding begins, and the probability
of at least one glitch appearing on a wire is ψ2 = 1 −(1 −ϵ)2 = 2ϵ −ϵ2.
Let μ be a very small positive number, let
ν ≡N −1/2+μ, 0 < μ << 1,
and let the minimal weight of any codeword be at least (2ϵ −ϵ2 + 2ν)(N/2). At
the receiver, at time slot i the decoder starts by waiting for (2ϵ −ϵ2 + ν)(N/2)
ones on the set of wires on which information was not transmitted in the ith time
slot. When this condition is met (and as N →∞, it will be met with probability
→1), the receiver knows that ones from the (i + 1)th time-slot are arriving and
all the ones from the ith time-slot have arrived. At this point, the set of wires
used for information transmission at the ith time slot forms a Z-channel in which
the probability of a zero becoming a one is 2ϵ −ϵ2. The capacity of the N/2
wires is at least (N/2)CZ(2ϵ−ϵ2). After determining what the transmitted word

Parallel Asynchronous Channels with Glitches
101
is, the receiver knows that all other ones that had been aggregated on this set
of wires until now are glitches (as no ones are being transmitted on these N/2
lines in this time-slot) and, therefore, erases them.
This technique uses admissible sequences, but it is relatively ineﬃcient. In
Sect. 4.2, we ﬁnd that it can proﬁtably be used as a part of another technique.
The Ones-Alone Code. In the next single-code based coding framework we
consider, the encoder works by always placing m ones on the N wires and never
placing ones in the same locations in which they were placed in the previous
transmission. Consequently, the sequences transmitted over each wire are admis-
sible sequences.
The value of m determines the code rate; at each stage, there are C(N−m, m)
possible ways to place the ones, and thus the maximal rate per column is Rmax =
log2(C(N −m, m))/N.
Theorem 1. For N →∞, the maximal rate Rmax tends to log2(φ) for m →δN
where
δ ≡
1
φ + 2.
Proof. See Appendix A.2.
In this scheme, the receiver waits to see (δ + 2ϵ −ϵ2 + ν)N ones and then
determines the transmitted codeword. It then searches for the codeword in the
received pulses on the (1 −δ)N wires on which ones or zeros may appear and
“declares” that the codeword was seen as soon as all the bits in the codeword
appeared on those wires. At this point, it “discards” all the pulses seen before
the last bit of the received word appeared – and this prevents aggregation of
logical glitches.
As N →∞, the best rate for a code for which the density of ones is δ tends
towards
(1 −δ)I(X; Y )
where in the distribution associated with X the probability of a one is δ/(1−δ),
where the distribution associated with Y is the distribution that follows from
the conditional probabilities that deﬁne the channel, and where I(X; Y ) is the
mutual information between X and Y . (It follows from Property 1 that the
codebook can be made into a constant rate code and that as N →∞the weight
tends towards δN.)
Let p = δ/(1 −δ) and, consequently, δ(p) = p/(1 + p), and let ϵ be the
probability of a glitch occurring. Then the probability of at least one glitch
occurring on a given wire between two “receptions” is ϵeﬀ≡ψ2.
The information capacity per “active” wire of a our channel when a constant
weight code with weight δN is used and when the glitch probability is ϵ is the
mutual information between a binary random variable for which the probability
of a one occuring is p and the output of a Z-channel with glitch probability ϵ
with such input. When dealing with a constant weight one’s alone code, there

102
S. Engelberg and O. Keren
are always δN wires that carry no information. Making use of the properties of
the Z-channel we arrive at Theorem 2.
Theorem 2. Let a = ϵeﬀ(1 −p) + p, let b = ϵeﬀ(1 −p), and let c = 1 −p. Then
the information capacity of our channel is greater than or equal to
max
p
Cones alone(p),
(3)
where
Cones alone(p) = −(a log2(a) −b log2(b) + c log2(c)) (1 −δ(p))
is the average information capacity per wire of a Z-channel for which the
probability of a one occurring on a wire in which ones may occur is p.
4.2
A Multiple-Code Based Coding Framework (W > 1)
A second general technique for protecting transmissions that may suﬀer from (a
fairly limited number of) glitches is to add error correction to each transmission
and only to use a ones-alone transmission periodically to clear the receiver of
accumulated glitches. In this section the coding framework is designed to use the
diﬀerent codes in each time-slot. Here, the error correcting capability increases
in each time slot.
We make use of a transmission scheme in which two sets of N/2 wires are
treated somewhat separately. In each period of W transmissions, each set is used
to transmit data W −1 times. For each set of N/2 wires, one of the W time-slots
is used to transmit zeros – and this cannot be the time-slot used for the other
N/2 wires. In this way, for each set of N/2 wires, we “wipe the slate clean” once
every W transmissions. This leads to error probabilities of ψ2, ψ3, . . . ψW −1 on
each set of N/2 channels (though not in the same time-slot). Asymptotically,
the maximal rate for this coding scheme is
R = 1
W
W −1

k=1
CZ(ψk+1)
(4)
and when W = 2, we ﬁnd that we revert to the technique of Sect. 4.1. In order
to make the coding as eﬃcient as possible, in time-slots in which all N wires are
used to transmit data, a single, eﬃcient code should be used to code the data
for all N wires.
4.3
A Comparison of the Eﬃciency of the Techniques
In Fig. 4 we compare:
– the capacity of a Z-channel (given in (1)) for which the probability of a zero
becoming a one is ϵ;

Parallel Asynchronous Channels with Glitches
103
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
ε − the probability of a glitch occuring
Achievable Rate
Capacity of Z Channel
Multiple−Code Based
Ones Alone Code
Trans. on N/2 Wires
Fig. 4. The capacity of the Z-channel and of the three techniques we consider. For
small probabilities of error, the multiple-code based coding framework is best. For
higher probabilities of error, the ones alone – Z-channel is preferable.
– the best achievable rate for the multiple-code based coding framework where
W is chosen so that (4) is maximized;
– the best achievable rate for the ones alone code as given by (3);
– and the best achievable rate when using N/2 channels – which is half the
capacity of a Z-channel (given in (1)) for which the probability of a zero
becoming a one is 2ϵ −ϵ2.
We ﬁnd that the multiple-code based coding framework is most useful when
there are not too many glitches and aggregation is not too much of a problem.
The ones alone – Z channel is most useful for large probabilities of error as it
prevents the aggregation of glitches. Though using only N/2 wires in any given
transmission is not in and of itself an eﬀective strategy, it is an integral part of
the multiple-code based coding framework.
5
Summary
A globally asynchronous locally synchronous system is prone to delays and fault
injection attacks which may cause glitches. This paper presents three simple
to use, zero-latency, self-synchronizing coding frameworks. These coding frame-
works enable us to treat an asynchronous channel as a Z-channel, prevent aggre-
gation of glitches, and provide reliable transmission over the channel.

104
S. Engelberg and O. Keren
A
Appendices
A.1
A Sketch of the Proof that the Transmission of Information
Is Practically Error Free
We consider a transmitter that makes use of constant-weight codes appropriate
to a Z-channel and a decoder that looks for a valid codeword whose ones are
contained in a buﬀer that maintains an ordered list of the pulses seen so far on
the channel’s wires and that were not identiﬁed as true pulses or glitches during
the decoding of previous transmissions. (In our channel, the ones of the actual
transmission are always present. Glitches add additional ones.)
When the ones-alone code of Sect. 4.1 is used, all “old” glitches are disre-
garded. The probability of a glitch aﬀecting any given wire will be less than
or equal to ψ2. Using a code appropriate to a Z-channel in which zeros become
ones with probability ψ2, one can make certain that each transmission is decoded
properly with very high probability.
If one uses the multiple-code based coding framework of Sect. 4.2, then at each
stage one must use a code appropriate to a Z-channel with glitch probability ψn
– where n increases by one after each not-all-zeros transmission. For such a code
and large enough N the probability of correctly decoding the transmission can
be made to approach one arbitrarily closely. Every W −1 transmissions, zeros
are transmitted on half the wires, and this reveal the glitches on those wires. We
ﬁnd that the probability of correctly decoding each transmission tends to one as
N tends to inﬁnity.
We assume that from time to time we resynchronize so that if there was a
mistake in the reception of a transmission, the mistake does not propagate for
too long.
A.2
The Proof that When Rmax Is Maximized, the Density
of Ones Tends to
1
φ+2
In order to determine the optimal m, we ﬁrst determine the density of ones
among admissible sequences, δ. We then calculate log2(C(N −⌈δ·N⌉, ⌈δ·N⌉))/N
and show that as N →∞this limit tends to log2(φ).
We know that A(W) = A(w−1)+A(w−2). Let O(W) be the number of ones
in all the admissible sequences of length W. Then O(W) = O(W −1)+O(W −2)+
A(W −2) as one builds the sequences of length W by taking all the sequences of
length W −1 and appending zeros to them and by taking all sequences of length
W −2 and appending a zero and a one to them. Asymptotically, the number
of solutions satisﬁes A(W) →αφW . It is easy to see that when calculating the
density of ones only this term need be considered. As φ satisﬁes the homogeneous
version of this recurrence relation, it is not hard to show that
1
φ + 2αWφW

Parallel Asynchronous Channels with Glitches
105
a particular solution of O(W) = O(W −1) + O(W −2) + αφW . As the total
number of sequences of length W tends to αφW and the number of elements in
each sequence is W, the average number of ones per element is δ =
1
φ+2.
Estimating C(N −m, m) while keeping in mind that we intend to take this
function’s logarithm and divide by N, we ﬁnd that
C(N −m, m) ∼
(N/m) −1
(N/m) −2
	N−m
(N/m −2)m.
Assuming that as N →∞we choose m to satisfy m/N →δ, we ﬁnd that the
rate of the code tends towards

1 −
1
φ + 2
	
× log2
φ + 1
φ
	
+
1
φ + 2 log2 φ
= φ + 1
φ + 2 log2(φ + 1) −
φ
φ + 2 log2(φ).
Note, however, that φ2 = φ+1 (as φ solves the homogeneous recurrence relation),
so that the above term is equal to
2φ + 1
φ + 2 log2(φ) −
φ
φ + 2 log2(φ) = log2 φ.
References
1. Barenghi, A., Breveglieri, L., Koren, I., Naccache, D.: Fault injection attacks on
cryptographic devices: theory, practice and countermeasures. Proc. IEEE 100(11),
3056–3076 (2012)
2. Blaum, M., Bruck, J.: Unordered error-correcting codes and their applications. In:
FTSC-22, pp. 486–493 (1992)
3. Blaum, M., Bruck, J.: Coding for tolerance and detection of skew in parallel asyn-
chronous communications. IEEE Trans. Inf. Theor. 46(7), 2329–2335 (2000)
4. Cover, T.M., Thomas, J.A.: Elements of Information Theory. Wiley, New York
(1991)
5. Davey, M.C., MacKay, D.J.C.: Reliable communication over channels with inser-
tions, deletions, and substitutions. IEEE Trans. Inf. Theor. 47, 687–698 (2001)
6. Dobrushin, R.L.: Shannon’s theorems for channels with synchronization errors.
Prob. Inf. Transm. 3(4), 11–26 (1967)
7. Engelberg, S., Keren, O.: Reliable communications across parallel asynchronous
channels with arbitrary skews. IEEE Trans. Inf. Theor. 63(1), 1120–1129 (2017)
8. Fertonani, D., Duman, T.M., Erden, M.F.: Bounds on the capacity of channels with
insertions, deletions and substitutions. IEEE Trans. Commun. 59(1), 2–6 (2011)
9. Gallager, R.: Sequential decoding for binary channels with noise and synchroniza-
tion errors. Lincoln Group Report, 2502 (1961)
10. Golomb, S.W.: The limiting behavior of the Z-channel. IEEE. Trans. Inf. Theor.
26(3), 372 (1980)
11. Kovacevic, M., Popovski, P.: Zero-error capacity of a class of timing channels.
IEEE. Trans. Inf. Theor. 60(11), 6796–6800 (2014)

106
S. Engelberg and O. Keren
12. Marcus, B., Roth, R., Siegel, P.: Introduction to coding for constrained systems.
http://www.math.ubc.ca/∼marcus/Handbook/index.html
13. Shamai, S., Zehavi, E.: Bounds on the capacity of a channel with bit shift. IEEE
Trans. Inf. Theor. 37(3), 863–872 (1991)
14. Tallini, L.G., Al-Bassam, S., Bose, B.: On the capacity and codes for the Z-channel.
ISIT 2002, Lausanne, Switzerland, 30 June–5 July, p. 422 (2002)
15. Tallini, L.G., Al-Bassam, S., Bose, B.: Feedback codes achieving the capacity of
the Z-channel. IEEE Trans. Inf. Theor 54(3), 1357–1362 (2008)
16. Verhoeﬀ, T.: Delay-Insensitive codes-an overview. Distrib. Comput. 3(1), 1–8
(1988)

On the Kernel of Z2s-Linear Hadamard Codes
Cristina Fern´andez-C´ordoba, Carlos Vela(B), and Merc`e Villanueva
Department of Information and Communications Engineering,
Universitat Aut`onoma de Barcelona, Ediﬁcio Q, 08193 Cerdanyola del Vall`es, Spain
{cristina.fernandez,carlos.vela,merce.villanueva}@uab.cat
http://ccsg.uab.cat
Abstract. The Z2s-additive codes are subgroups of Zn
2s, and can be seen
as a generalization of linear codes over Z2 and Z4. A Z2s-linear Hadamard
code is a binary Hadamard code which is the Gray map image of a Z2s-
additive code. For s = 2, the kernel of Z4-linear Hadamard codes can be
used for their complete classiﬁcation. In this paper, the kernel of Z2s-
linear Hadamard codes is given for s > 2. However, unlike for s = 2, we
show that the dimension of the kernel just allows a partial classiﬁcation
of these Hadamard codes.
Keywords: Kernel · Hadamard code · Z2s-linear code · Z2s-additive
code · Gray map · Classiﬁcation
1
Introduction
Let Z2s be the ring of integers modulo 2s with s ≥1. The set of n-tuples over
Z2s is denoted by Zn
2s. In this paper, the elements of Zn
2s will also be called
vectors over Z2s of length n. A binary code of length n is a nonempty subset of
Zn
2, and it is linear if it is a subspace of Zn
2. Equivalently, a nonempty subset
of Zn
2s is a Z2s-additive code if it is a subgroup of Zn
2s. Note that, when s = 1,
a Z2s-additive code is a binary linear code and, when s = 2, it is a quaternary
linear code or a linear code over Z4.
Two binary codes C1 and C2 are said to be equivalent if there is a vector
a ∈Zn
2 and a permutation of coordinates π such that C2 = {a + π(c) : c ∈C1}.
Two Z2s-additive codes C1 and C2 are said to be permutation equivalent if they
diﬀer only by a permutation of coordinates, that is, if there is a permutation of
coordinates π such that C2 = {π(c) : c ∈C1}.
The Hamming weight of a binary vector u ∈Zn
2, denoted by wtH(u), is the
number of nonzero coordinates of u. The Hamming distance of two binary vectors
u, v ∈Zn
2, denoted by dH(u, v), is the number of coordinates in which they
diﬀer. Note that dH(u, v) = wtH(v−u). The Lee weight of an element i ∈Z2s is
wtL(i) = min{i, 2s −i} and the Lee weight of a vector u = (u1, u2, . . . , un) ∈Zn
2s
This work has been partially supported by the Spanish MINECO under Grants
TIN2016-77918-P (AEI/FEDER, UE) and MTM2015-69138-REDT, and by the
Catalan AGAUR under Grant 2014SGR-691.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 107–117, 2017.
DOI: 10.1007/978-3-319-66278-7 10

108
C. Fern´andez-C´ordoba et al.
is wtL(u) = n
j=1 wtL(uj) ∈Z2s. The Lee distance of two vectors u, v ∈Zn
2s
is dL(u, v) = wtL(v −u). The minimum distance of a Z2s-additive code C is
d(C) = min{dL(u, v) : u, v ∈C, u ̸= v} and the minimum distance of a binary
code C is d(C) = min{dH(u, v) : u, v ∈C, u ̸= v}.
In [7], a Gray map from Z4 to Z2
2 is deﬁned as φ(0) = (0, 0), φ(1) = (0, 1),
φ(2) = (1, 1) and φ(3) = (1, 0). There exist diﬀerent generalizations of this
Gray map, which go from Z2s to Z2s−1
2
[4,5]. The one given in [4] is the map
φ : Z2s →Z2s−1
2
deﬁned as follows:
φ(u) = (us−1, . . . , us−1) + (u0, . . . , us−2)Y,
(1)
where u ∈Z2s, [u0, u1, . . . , us−1]2 is the binary expansion of u, that is u =
s−1
i=0 2iui (ui ∈{0, 1}), and Y is a matrix of size (s −1) × 2s−1 which columns
are the elements of Zs−1
2
. Note that (us−1, . . . , us−1) and (u0, . . . , us−2)Y are
binary vectors of length 2s−1. This Gray map can also be deﬁned in terms of
a ﬁrst order Reed-Muller code [8]. Then, we deﬁne Φ : Zn
2s →Zn2s−1
2
as the
component-wise Gray map φ.
Let C be a Z2s-additive code of length n. We say that its binary image
C = Φ(C) is a Z2s-linear code of length 2s−1n. Since C is a subgroup of Zn
2s,
it is isomorphic to an abelian structure Zt1
2s × Zt2
2s−1 × · · · × Zts−1
4
× Zts
2 , and
we say that C, or equivalently C = Φ(C), is of type (n; t1, . . . , ts). Note that
|C| = 2st12(s−1)t2 · · · 2ts. Unlike linear codes over ﬁnite ﬁelds, linear codes over
a ring do not have a basis, but there exists a generator matrix with minimum
number of rows. If C is a Z2s-additive code of type (n; t1, . . . , ts), then a generator
matrix of C with minimum number of rows has exactly t1 + · · · + ts rows.
Two structural properties of binary codes are the rank and the dimension of
the kernel. The rank of a binary code C is simply the dimension of the linear
span, ⟨C⟩, of C. The kernel of a binary code C is deﬁned as K(C) = {x ∈Zn
2 :
x+C = C} [2]. If the all-zero vector belongs to C, then K(C) is a linear subcode
of C. Note also that if C is linear, then K(C) = C = ⟨C⟩. We denote the rank
of a binary code C as rank(C) and the dimension of the kernel as ker(C). These
parameters can be used to distinguish between nonequivalent binary codes, since
equivalent ones have the same rank and dimension of the kernel.
A binary code of length n, 2n codewords and minimum distance n/2 is
called a Hadamard code. Hadamard codes can be constructed from normalized
Hadamard matrices [1,10]. Note that linear Hadamard codes are in fact ﬁrst
order Reed-Muller codes, or equivalently, the dual of extended Hamming codes
[10, Chap. 13 Sect. 3]. The Z2s-additive codes that, under the Gray map Φ, give a
Hadamard code are called Z2s-additive Hadamard codes and the corresponding
binary images are called Z2s-linear Hadamard codes.
For s = 2, the Z4-linear Hadamard codes can be classiﬁed by using either the
rank or the dimension of the kernel. It is known that for a Z4-linear Hadamard
code C of type (2t−1; t1, t2), ker(C) = t1 + t2 + 1 if t1 > 2, and ker(C) = 2t1 + t2
if t1 = 1 or 2, where t2 = t + 1 −2t1 [9,11]. Therefore, for any integer t ≥3 and
each t1 ∈{1, . . . , ⌊(t + 1)/2⌋}, there is a unique (up to equivalence) Z4-linear
Hadamard code of type (2t−1; t1, t + 1 −2t1), and all these codes are pairwise

On the Kernel of Z2s-Linear Hadamard Codes
109
nonequivalent, except for t1 = 1 and t1 = 2, where the codes are equivalent to
the linear Hadamard code [9]. The number of nonequivalent Z4-linear Hadamard
codes of length 2t is ⌊t−1
2 ⌋for all t ≥3, and it is 1 for t = 1 and for t = 2.
In this paper, in order to try to classify Z2s-linear Hadamard codes for s > 2,
we establish the kernel and its dimension for these codes, and we point out that
this invariant does not provide a complete classiﬁcation. This correspondence
is organized as follows. In Sect. 2, we recall and prove some results related to
the generalized Gray map. In Sect. 3, we describe the construction of Z2s-linear
Hadamard codes of type (n; t1, . . . , ts). In Sect. 4, we establish for which types
these codes are linear, and we give the kernel and its dimensions whenever they
are nonlinear. Through several examples, we show that, unlike for s = 2, the
dimension of the kernel is not enough to classify completely Z2s-linear Hadamard
codes with s = 3 or s = 4. Finally, in Sect. 5, we give some conclusions and
further research on this topic.
2
Generalized Gray Map
In this section, we present some results about the generalized Gray map in order
to show the main results related to Z2s-linear Hadamard codes.
Let ei be the vector that has 1 in the ith position and 0 otherwise. Let
u, v ∈Z2s and [u0, . . . , us−1]2, [v0, . . . , vs−1]2 be the binary expansions of u and
v, respectively. The operation “⊙” on Z2s is deﬁned as u ⊙v = s−1
i=0 2iuivi.
Note that the binary expansion of u ⊙v is [u0v0, . . . , us−1vs−1]2.
Proposition 1. [12] Let u, v ∈Z2s. Then, φ(u) + φ(v) = φ(u + v −2(u ⊙v)).
Corollary 1. Let u ∈Z2s and 0 ≤p ≤s −1. Then, φ(u) + φ(2p) = φ(u + 2p −
up2p+1), where [u0, u1 . . . , us−1]2 is the binary expansion of u.
Corollary 2. Let u ∈Z2s. Then, φ(u) + φ(2s−1) = φ(u + 2s+1).
Lemma 1. Let u ∈{2s−2, . . . , 2s−1 −1} ⊂Z2s. Then, φ(u + 2s−2 + 2s−1) =
φ(u) + φ(2s−2).
Proof. By Proposition 1, we have that φ(u)+φ(2s−2) = φ(u+2s−2−2(u⊙2s−2)).
The binary expansion of 2s−2 is [0, . . . , 0, 1, 0]2 and, if u ∈{2s−2, . . . , 2s−1 −1},
the binary expansion of u is [u0, . . . , us−3, 1, 0]2. Then, −2(u⊙2s−2) = 2s−1 and
the statement follows.
⊓⊔
Corollary 3. Let v ∈{2s−2, 3 · 2s−2} and U = {2s−2, . . . , 2s−1 −1} ∪{3 ·
2s−2, . . . , 2s −1} ⊂Z2s. Then,
φ(u) + φ(v) =

φ(u + v + 2s−1) if u ∈U
φ(u + v)
if u ∈Z2s\U.
Proof. Straightforward from Corollary 1 and Lemma 1.
⊓⊔

110
C. Fern´andez-C´ordoba et al.
Lemma 2. Let qi ∈Z2, i ∈{0, . . . , s−2}. Then, s−2
i=0 qiφ(2i) = φ(s−2
i=0 qi2i),
where 2i ∈Z2s.
Proof. Let yi be the ith row of Y . By the deﬁnition of φ given by (1), we
know that s−2
i=0 qiφ(2i) = s−2
i=0 qiei+1Y = s−2
i=0 qiyi+1 = qY , where q =
(q0, . . . , qs−2). Since [q0, . . . , qs−2, 0]2 is the binary expansion of s−2
i=0 qi2i, then
we have that qY = φ(s−2
i=0 qi2i).
⊓⊔
Proposition 2. [4] Let u, v ∈Z2s. Then, dH(φ(u), φ(v)) = wtH(φ(u −v)).
Lemma 3. Let u ∈Z2s. Then, dH(φ(u), φ(2s−1)) + dH(φ(u), φ(0)) = 2s−1.
Proof. By the properties of the distance, we have that dH(φ(u), φ(2s−1)) +
dH(φ(u), φ(0)) = wtH(φ(2s−1) −φ(u)) + wtH(φ(u)). Then, since φ(2s−1) = 1,
wtH(φ(2s−1) −φ(u)) = 2s−1 −wtH(φ(u)), and the result follows.
⊓⊔
Corollary 4. Let u, v ∈Z2s. Then, dH(φ(u), φ(v + 2s−1)) + dH(φ(u), φ(v)) =
2s−1.
Proof. Straightforward from Corollary 1 and Lemma 3.
⊓⊔
3
Construction of Z2s-Linear Hadamard Codes
The description of a generator matrix having minimum number of rows for a
Z4-additive Hadamard code, as long as recursive constructions of these matrices,
are given in [9]. In this section, we generalize these results for any s > 2 and
give another proof of the main theorem, already proved in [8], that establishes
that the constructed matrices generate Z2s-linear Hadamard codes.
Let Ti = {j · 2i−1 : j ∈{0, 1, . . . , 2s−i+1 −1}} for all i ∈{1, . . . , s}. Note
that T1 = {0, . . . , 2s −1}. Let t1, t2,. . . ,ts be nonnegative integers with t1 ≥1.
Consider the matrix At1,...,ts whose columns are of the form zT , z ∈{1} ×
T t1−1
1
×T t2
2 ×· · ·×T ts
s . Let 0, 1, 2, . . . , 2s −1 be the vectors having the elements
0, 1, 2, . . . , 2s −1 from Z2s repeated in each coordinate, respectively. The order
of a vector u over Z2s, denoted by ord(u), is the smallest positive integer m such
that mu = 0.
Example 1. For s = 3, for example, we have the following matrices:
A1,0,1 =

1 1
0 4

,
A1,1,0 =

1 1 1 1
0 2 4 6

,
A2,0,0 =

1 1 1 1 1 1 1 1
0 1 2 3 4 5 6 7

,
A1,1,1 =
⎛
⎝
1 1 1 1 1 1 1 1
0 2 4 6 0 2 4 6
0 0 0 0 4 4 4 4
⎞
⎠,
A2,0,1 =
⎛
⎝
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
0 0 0 0 0 0 0 0 4 4 4 4 4 4 4 4
⎞
⎠,
A2,1,0 =
⎛
⎝
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 4 4 4 4 4 4 4 4 6 6 6 6 6 6 6 6
⎞
⎠.

On the Kernel of Z2s-Linear Hadamard Codes
111
Note that any matrix At1,...,ts can be obtained by applying the following
iterative construction. We start with A1,0,...,0 = (1). Then, if we have a matrix
At1,...,ts, we may construct
Ai = At′
1,...,t′
s =
At1,...,ts At1,...,ts · · ·
At1,...,ts
0 · 2i−1 1 · 2i−1 · · · (2s−i+1 −1) · 2i−1

,
(2)
where i ∈{1, . . . , s}, t′
j = tj for j ̸= i and t′
i = ti + 1.
Example 2. From the matrix A1,0,0 = (1), we obtain the matrix A2,0,0; and from
A2,0,0 we can construct A2,0,1, where A2,0,0 and A2,0,1 are the matrices given
in Example 1. Note that we can also generate another matrix A2,0,1 as follows:
from A1,0,0 = (1) we construct the matrix A1,0,1 given in Example 1, and from
A1,0,1 we obtain the matrix
A2,0,1 =
⎛
⎝
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4
0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7
⎞
⎠,
which is diﬀerent to the previous one. These two matrices A2,0,1 generate per-
mutation equivalent codes.
Along this paper, we consider that the matrices At1,t2,...,ts are constructed
recursively starting from A1,0,...,0 in the following way. First, we add t1 −1 rows
of order 2s, up to obtain At1,0,...,0; then t2 rows of order 2s−1 up to generate
At1,t2,0,...,0; and so on, until we add ts rows of order 2 to achieve At1,t2,...,ts.
Let Ht1,...,ts be the Z2s-additive code generated by the matrix At1,...,ts, where
t1, . . . , ts ≥0 with t1 ≥1. Let n = 2t−s+1, where t = (s
i=1(s −i + 1) · ti) −
1. It is easy to see that Ht1,...,ts is of length n and has |Ht1,...,ts| = 2sn =
2t+1 codewords. Note that this code is of type (n; t1, t2, . . . , ts). Let Ht1,...,ts =
Φ(Ht1,...,ts) be the corresponding Z2s-linear code.
Remark 1. The code H1,0,...,0 is generated by A1,0,...,0 = (1), so H1,0,...,0 = Z2s.
This code has length n = 1, cardinality 2s and minimum distance 1. Thus,
H1,0,...,0 = Φ(H1,0,...,0) has length N = 2s−1, cardinality 2N = 2s and minimum
distance 2s−2 = N/2, so it is a binary linear Hadamard code of length 2s−1
[4], or equivalently, the ﬁrst order Reed-Muller code of length 2s−1, denoted by
RM(1, s −1) [10, Chap. 13 Sect. 3].
The result given by Theorem 1 is already proved in [8], where it is also
shown that each Z2s-linear Hadamard code is equivalent to Ht1,...,ts for some
t1, . . . , ts ≥0 with t1 ≥1. We present a new proof of this theorem, which does
not use neither the dual of the Z2s-additive codes nor another generalization of
the Gray map for these dual codes, unlike the proof given in [8].
Let G be a generator matrix of a Z2s-additive code C of length n. Then,
(G · · · G) is a generator matrix of the r-fold replication code of C, (C, . . . , C) =
{(c, . . . , c) : c ∈C} of length r · n.

112
C. Fern´andez-C´ordoba et al.
Theorem 1. [8] Let t1, . . . , ts be nonnegative integers with t1 ≥1. The Z2s-
linear code Ht1,...,ts of type (n; t1, t2, . . . , ts) is a binary Hadamard code of length
2t, with t = (s
i=1(s −i + 1) · ti) −1 and n = 2t−s+1.
Proof. We prove this theorem by induction on the integers ti, i ∈{1, . . . , s}.
First, by Remark 1, the code H1,0,...,0 is a Hadamard code.
Let H = Ht1,...,ts be the Z2s-additive code of length n generated by the
matrix A = At1,...,ts. We assume that H = Φ(H) is a Hadamard code of length
N = 2s−1n. Let i ∈{1, . . . , s}. Deﬁne Ai = At′
1,...,t′
s as in (2), where t′
j = tj for
j ̸= i and t′
i = ti + 1. Let Hi = Ht′
1,...,t′
s be the Z2s-additive code generated by
the matrix Ai. Now, we shall prove that Hi = Φ(Hi) is a Hadamard code.
Note that Hi can be seen as the union of 2s−i+1 cosets of the 2s−i+1-fold
replication code of H, (H, . . . , H), which are
(H, . . . , H) + r · wi,
(3)
for r ∈{0, . . . 2s−i+1−1}, where wi = (0, 2i−1, 2·2i−1, . . . , (2s−i+1−1)·2i−1).
The code H of length n has cardinality 2sn. It is easy to see that Hi has length
ni = 2s−i+1n and cardinality 22s−i+1n. Therefore, the length of Hi = Φ(Hi) is
Ni = 2s−1ni and the cardinality 2Ni. Now, we just have to prove that the
minimum distance of Hi is Ni/2.
By Proposition 2, the minimum distance of Hi is equal to the minimum
weight of Hi. Thus, we just have to check that the minimum weight of any coset
(3) is Ni/2. When r = 0, we have that wtH(Φ((u, . . . , u))) = 2s−i+1wtH(Φ(u)) =
2s−i+1N/2 = Ni/2. Otherwise, when r ̸= 0, we consider
wtH(Φ((u, . . . , u) + r · wi)) = dH(Φ((u, . . . , u)), Φ(r · wi)).
(4)
Note that, by construction, the coordinates of any nonnegative multiple of wi
can be partitioned into two multisets V and V ′ such that |V | = |V ′| = 2s−i and
there is a bijection from V to V ′ mapping any element v ∈V into an element
v′ ∈V ′ such that v′ −v = 2s−1. Therefore, (4) can be written as

v∈V
dH(Φ(u), Φ(v)) +

v′∈V ′
dH(Φ(u), Φ(v′)) =

v∈V
dH(Φ(u), Φ(v)) + dH(Φ(u), Φ(v + 2s−1)) =
|V | · 2s−1n = 2s−i2s−1n = Ni/2,
(5)
where (5) holds by Corollary 4.
⊓⊔
4
Partial Classiﬁcation of Z2s-Linear Hadamard Codes
The computation of the kernel and its dimension for Z4-linear codes are given in
[9,11]. In this section, we determine them for Z2s-linear Hadamard codes with
s > 2. First, we determine when these codes are linear, and, in the case that
they are nonlinear, we construct the kernel and its dimension, which allow us to
establish a partial classiﬁcation of these codes.

On the Kernel of Z2s-Linear Hadamard Codes
113
Proposition 3. The Z2s-linear Hadamard codes H1,0,...,0 and H1,0,...,0,1,0 are
linear.
Proof. By Remark 1, we know that H1,0...,0 is linear.
Recall that the code H1,0,...,0,1,0 is generated by
A1,0,...,0,1,0 =

1
1
1
1
0 2s−2 2s−1 3 · 2s−2

.
Let βββi = (2i, 2i, 2i, 2i) for 0 ≤i ≤s −1, βββs = (0, 2s−1, 0, 2s−1) and βββs+1 =
(0, 2s−2, 2s−1, 3 · 2s−2). Let C be the linear code generated by B = {Φ(βββi) : 0 ≤
i ≤s + 1}. Now, we prove that C ⊆H1,0,...,0,1,0. Let c = s+1
i=0 qiΦ(βββi) ∈C,
where qi ∈Z2. By Corollary 2, we only have to see that
c′ = qs+1Φ(βββs+1) +
s−2

i=0
qiΦ(βββi) ∈H1,0,...,0,1,0.
Since s−2
i=0 qiΦ(βββi) = Φ(s−2
i=0 qiβββi) by Lemma 2, if qs+1 = 0, then we have that
c′ ∈H1,0,...,0,1,0. Assume qs+1 = 1. We have that c′ = Φ((0, 2s−2, 2s−1, 3·2s−2))+
Φ((u, u, u, u)), for u = s−2
i=0 qi2i. Let U = {2s−2, . . . , 2s−1−1}∪{3·2s−2, . . . , 2s−
1} ⊂Z2s. Then, by Corollary 3, c′ = Φ((0, 2s−2, 2s−1, 3 · 2s−2) + (u, u, u, u) +
(0, 2s−1, 0, 2s−1)) if u ∈U, and c′ = Φ((0, 2s−2, 2s−1, 3 · 2s−2) + (u, u, u, u)) if
u ∈Z2s\U. In both cases, c′ ∈H1,0,...,0,1,0.
Since |C| = |H1,0,...,0,1,0| = 2s+2, then C = H1,0,...,0,1,0, and thus H1,0,...,0,1,0
is linear.
⊓⊔
In [6], it is proved that the codes H1,0,...,0,ts, ts ≥0, are linear. The next
result shows that these codes together with the codes H1,0,...,1,ts are linear, and
they are the only Z2s-linear Hadamard codes which are linear.
Theorem 2. The codes H1,0,...,1,ts and H1,0,...,0,ts, with ts ≥0, are the unique
Z2s-linear Hadamard codes which are linear.
Proof. First, we show that these codes are linear by induction on ts. By
Proposition 3, the codes H1,0,...,0 and H1,0,...,0,1,0 are linear. We assume that
H = Φ(H), where H = H1,0,...,0,ts−1,ts, ts−1 ∈{0, 1} and ts ≥0, is linear.
Now, we prove that the code Hs = H1,0,...,0,ts−1,ts+1 is linear. Since H is a
linear Hadamard code of length 2ts+2ts−1−1, then it is the Reed-Muller code
RM(1, ts + 2ts−1 −1) [10, Chap. 13 Sect. 3]. By the iterative construction (2),
we have that Hs = {Φ((h, h) + (0, v)) : h ∈H, v ∈{0, 2s−1}}. By Corollary 2,
Hs = {(Φ(h), Φ(h) + Φ(v)) : h ∈H, v ∈{0, 2s−1}} = {(h′, h′ + v′) : h′ ∈
H, v′ ∈{0, 1}} which corresponds to the Reed-Muller code RM(1, ts + 2ts−1).
Therefore, Hs is linear.
Now, we prove the nonlinearity of H = Φ(H), where H = H1,0,...,0,2,0. Let
r = (0, 2s−2, 2s−1, 3 · 2s−2). Recall that H has length 16 and is generated by
A1,0,...,0,2,0 =
⎛
⎝
1
1
1
1
r
r
r
r
0 2s−2 2s−1 3 · 2s−2
⎞
⎠.

114
C. Fern´andez-C´ordoba et al.
By Corollaries 2 and 3, we have that Φ((r, r, r, r)) + Φ((0, 2s−2, 2s−1,
3 · 2s−2)) = Φ(z), where z = (0, 2s−2, 2s−1, 3·2s−2, 2s−2, 0, 3·2s−2, 2s−1, 2s−1, 3·
2s−2, 0, 2s−2, 3 · 2s−2, 2s−1, 2s−2, 0). Since H is linear over Z2s, z ∈H if and only
if z + (r, r, r, r) + (0, 2s−2, 2s−1, 3 · 2s−2) = z′ ∈H, where
z′ = (0, 0, 0, 0, 0, 2s−1, 0, 2s−1, 0, 0, 0, 0, 0, 2s−1, 0, 2s−1).
Since wtH(Φ(z′)) = 2s−1 · 4 = N/4, where N is the length of H, Φ(z′) ̸∈H, so
Φ(z) ̸∈H. Therefore, H = H1,0,...,0,2,0 is nonlinear.
We consider (t1, . . . , ts) = (1, 0, . . . , 0, 1, 0) and Hi = Ht′
1,...,t′
s, t′
i = ti + 1
and t′
j = tj for j ̸= i. Next, we prove that the Z2s-linear Hadamard code
Hi = Φ(Hi) is nonlinear for any i ∈{1, . . . , s −2}. Note that Hi is permu-
tation equivalent to the code Hi, generated by the matrix Ai constructed as in
(2) with A = A1,0,...,0,1,0. Moreover it is easy to see that the 2s−i−1 = (2s−i+1/4)-
fold replication code of H1,0,...,0,2,0 is contained in Hi. Since H1,0,...,0,2,0 is non-
linear, by using the same argument as before, there exist u, v ∈H1,0,...,0,2,0,
such that Φ(u) + Φ(v) = Φ(z′) ̸∈H1,0,...,2,0, so Φ((u, . . . , u)) + Φ((v, . . . , v)) =
Φ((z′, . . . , z′)) ̸∈Φ(Hi) and Hi is nonlinear.
Finally, we prove that if Ht1,...,ts is nonlinear, then Ht′
1,...,t′
s is nonlinear for
any t′
i ≥ti, i ∈{1, . . . , s}. Assume that Ht′
1,...,t′
s is linear. Then, by the iterative
construction (2), for any u, v ∈Ht1,...,ts, we have that (u, . . . , u), (v, . . . , v) ∈
Ht′
1,...,t′
s. Moreover, since Ht′
1,...,t′
s is linear, Φ((u, . . . , u)) + Φ((v, . . . , v)) =
Φ((a, . . . , a) + λ(0 · 2i−1, 1 · 2i−1, . . . , (2s−i+1 −1) · 2i−1)) ∈Ht′
1,...,t′
s, where
a ∈Ht1,...,ts and λ ∈Z2s. Therefore, Φ(u) + Φ(v) = Φ(a) ∈Ht1,...,ts, and
we have that Ht1,...,ts is linear and the result follows.
⊓⊔
Let At1,...,ts be the generator matrix of Ht1,...,ts, considered along this paper,
and let wi be the ith row vector of At1,...,ts. By construction, w1 = 1 and
ord(wi) ≤ord(wj) if i > j. We deﬁne σ ∈{1, . . . , s} as the integer such that
ord(w2) = 2s+1−σ. Note that σ = 1 if t1 > 1, and σ = min{i : ti > 0, i ∈
{2, . . . , s}} if t1 = 1.
Theorem 3. Let H = Ht1,...,ts be a Z2s-additive Hadamard code of type (n; t1,
. . . , ts) such that Φ(H) is nonlinear. Let Hb be the subcode of H which contains
all codewords of order two. Let P = {2p}σ−2
p=0 if σ ≥2, and P = ∅if σ = 1.
Then,

Φ(Hb), Φ(P), Φ(
s−2

i=0
2i)

= K(Φ(H)),
and ker(Φ(H)) = σ + s
i=1 ti.
The following example shows that the dimension of the kernel can not be
used to classify completely Z8-linear Hadamard codes of length 256.
Example 3. The Z8-linear Hadamard codes of length 2t = 256 are the following:
H1,0,6, H1,1,4, H1,2,2, H1,3,0, H2,0,3, H2,1,1 and H3,0,0. The ﬁrst two are equiva-
lent as they are linear by Theorem 2. The remaining ones have kernels of dimen-
sion 7, 6, 6, 5 and 4, respectively, by Theorem 3. Therefore, by using this invari-
ant, we can say that all of them are nonequivalent, with the exception of H1,3,0

On the Kernel of Z2s-Linear Hadamard Codes
115
and H2,0,3 which have the same dimension of the kernel. For these two codes, by
using the computer algebra system Magma [3], we have that rank(H1,3,0) = 12
and rank(H2,0,3) = 11, so they are also nonequivalent. Actually, all these non-
linear codes have ranks 10, 12, 11, 13 and 17, respectively, so we can use the rank
instead of the dimension of the kernel to have a classiﬁcation of these codes.
Example 4. Table 1 shows the type and the pair (r, k), where r is the rank and
k the dimension of the kernel, of all Z2s-linear Hadamard codes of length 2t for
s ∈{2, 3, 4, 5} and t ∈{8, 9}. As we can see, for a given ring Z2s and length
2t, all nonlinear codes have a diﬀerent value of the rank, but not all of them
have diﬀerent values of the dimension of the kernel, when s = 3 or 4. Therefore,
for these cases, as in Example 3, we have a complete classiﬁcation by using the
rank, and just a partial classiﬁcation by using the kernel. From this table, we
Table 1. Type, rank and kernel of all Z2s-linear Hadamard codes of length 2t.
t = 8
t = 9
type
(r, k)
type
(r, k)
Z4
(27; 1, 7)
(9,9)
(28; 1, 8)
(10,10)
(27; 2, 5)
(9,9)
(28; 2, 6)
(10,10)
(27; 3, 3)
(10,7)
(28; 3, 4)
(11,8)
(27; 4, 1)
(12,6)
(28; 4, 2)
(13,7)
(29; 5, 0)
(16,6)
Z8
(26; 1, 0, 6)
(9,9)
(27; 1, 0, 7)
(10,10)
(26; 1, 1, 4)
(9,9)
(27; 1, 1, 5)
(10,10)
(26; 1, 2, 2)
(10,7)
(27; 1, 2, 3)
(11,8)
(26; 1, 3, 0)
(12,6)
(27; 1, 3, 1)
(13,7)
(26; 2, 0, 3)
(11,6)
(27; 2, 0, 4)
(12,7)
(26; 2, 1, 1)
(13,5)
(27; 2, 1, 2)
(14,6)
(26; 3, 0, 0)
(17,4)
(27; 2, 2, 0)
(17,5)
(27; 3, 0, 1)
(18,5)
Z16
(25; 1, 0, 0, 5)
(9,9)
(26; 1, 0, 0, 6)
(10,10)
(25; 1, 0, 1, 3)
(9,9)
(26; 1, 0, 1, 4)
(10,10)
(25; 1, 0, 2, 1)
(10,7)
(26; 1, 0, 2, 2)
(11,8)
(25; 1, 1, 0, 2)
(11,6)
(26; 1, 0, 3, 0)
(13,7)
(25; 1, 1, 1, 0)
(13,5)
(26; 1, 2, 0, 0)
(18,5)
(25; 2, 0, 0, 1)
(15,4)
(26; 1, 1, 0, 3)
(12,7)
(26; 1, 1, 1, 1)
(14,6)
(26; 2, 0, 0, 2)
(16,5)
(26; 2, 0, 1, 0)
(20,4)
Z32
(24; 1, 0, 0, 0, 4)
(9,9)
(25; 1, 0, 0, 0, 5)
(10,10)
(24; 1, 0, 0, 1, 2)
(9,9)
(25; 1, 0, 0, 1, 3)
(10,10)
(24; 1, 0, 0, 2, 0)
(10,7)
(25; 1, 0, 0, 2, 1)
(11,8)
(24; 1, 0, 1, 0, 1)
(11,6)
(25; 1, 0, 1, 0, 2)
(12,7)
(24; 1, 1, 0, 0, 0)
(15,4)
(25; 1, 0, 1, 1, 0)
(14,6)
(25; 1, 1, 0, 0, 1)
(16,5)
(25; 2, 0, 0, 0, 0)
(26,3)

116
C. Fern´andez-C´ordoba et al.
Table 2. Number of nonequivalent Z2s-linear Hadamard codes of length 2t.
t
3 4 5 6 7 8 9 10 11
Z4
1 1 2 2 3 3 4 4
5
Z8
1 1 2 3 4 6 7 9
11
Z16 1 1 1 2 4 5 8 10 14
also have that there are at least 7 nonequivalent Z2s-linear Hadamard codes of
length 28, and at least 11 of length 29.
We can extend these results for other values of t. Table 2 summarizes these
computations by showing how many nonequivalent Z2s-linear Hadamard codes
of length 2t exist for each s ∈{2, 3, 4} and t ∈{3, . . . , 11}.
5
Conclusions
The kernel of Z2s-linear Hadamard codes has been study. We compute the kernel
of these codes and its dimension in order to classify them. Examples 3 and 4
show that we can not use the dimension of the kernel to distinguish between
nonequivalent Z2s-linear Hadamard codes for a given s > 2 and length 2t. How-
ever, they also show that the rank seems to be enough to classify them. A further
research on this topic would be to determine the rank of Z2s-linear Hadamard
codes, and establish their complete classiﬁcation by using this invariant.
By using Magma [3], we also noticed that, for lengths 28 and 29, Z4-linear
and Z8-linear Hadamard codes with the same rank (and dimension of the ker-
nel) are equivalent. This means that, up to equivalence, the same code can be
Z4-linear and Z8-linear. Another further research in this sense would be to estab-
lish whether all Z2s-linear Hadamard codes of length 2t having the same rank
(and dimension of the kernel) are equivalent.
References
1. Assmus, E.F., Key, J.D.: Designs and Their Codes. Cambridge University Press,
Great Britain (1992)
2. Bauer, H., Ganter, B., Hergert, F.: Algebraic techniques for nonlinear codes. Com-
binatorica 3(1), 21–33 (1983)
3. Bosma, W., Cannon, J.J., Fieker, C., Steel, A.: Handbook of Magma Functions,
2.22 edn., p. 5669 (2016). http://magma.maths.usyd.edu.au/magma/
4. Carlet, C.: Z2k-linear codes. IEEE Trans. Inform. Theory 44(4), 1543–1547 (1998)
5. Dougherty, S.T., Fern´andez-C´ordoba, C.: Codes over Z2k, gray map and self-dual
codes. Adv. Math. Commun. 5(4), 571–588 (2011)
6. Gupta, M.K.: On some linear codes over Z2s. Indian Institute of Technology, Kan-
pur (1999)
7. Hammons, A.R., Kumar, P.V., Calderbank, A.R., Sloane, N.J.A., Sol´e, P.: The Z4-
linearity of Kerdock, Preparata, Goethals and related codes. IEEE Trans. Inform.
Theory 40(2), 301–319 (1994)

On the Kernel of Z2s-Linear Hadamard Codes
117
8. Krotov, D.S.: On Z2k-dual binary codes. IEEE Trans. Inf. Theory 53(4), 1532–1537
(2007)
9. Krotov, D.S.: Z4-linear Hadamard and extended perfect codes. In: International
Workshop on Coding and Cryptography. Electronic Notes Discrete Mathematics,
vol. 6, pp. 107–112 (2001)
10. MacWilliams, F.J., Sloane, N.J.A.: The Theory of Error-Correcting Codes, vol. 16.
Elsevier, Amsterdam (1977)
11. Phelps, K.T., Rif`a, J., Villanueva, M.: On the additive (Z4-linear and non-Z4-
linear) Hadamard codes: rank and kernel. IEEE Trans. Inf. Theory 52(1), 316–319
(2006)
12. Tapia-Recillas, H., Vega, G.: On Z2k-linear and quaternary codes. SIAM J. Discrete
Math. 17(1), 103–113 (2003)

Random Network Coding over Composite Fields
Olav Geil1(B)
and Daniel E. Lucani2
1 Department of Mathematical Sciences, Aalborg University, Aalborg, Denmark
olav@math.aau.dk
2 Department of Engineering, Aarhus University, Aarhus, Denmark
daniel.lucani@eng.au.dk
Abstract. Random network coding is a method that achieves multicast
capacity asymptotically for general networks [1,7]. In this approach, ver-
tices in the network randomly and linearly combine incoming informa-
tion in a distributed manner before forwarding it through their outgo-
ing edges. To ensure success, the involved ﬁnite ﬁeld needs to be large
enough [2,7], which can be an obstacle if some inner (intermediate) nodes
have less computational power than others. In this work, we analyze what
can be achieved if diﬀerent nodes are allowed to use diﬀerent ﬁnite ﬁelds
from a selection of ﬁelds all contained in some composite extension ﬁnite
ﬁeld [3,5].
Keywords: Composite ﬁelds · Random network coding · Success
probability
1
Introduction
In the seminal paper [1], Ahlswede, Cai, Li and Yeung showed that if in a net-
work with one sender and several receivers the unicast problem is solvable for
each of the receivers, then the multicast problem of sending simultaneously the
information to all receivers is also solvable. That is, the multicast problem is
solvable if and only if the information rate does not exceed the minimum of the
min-cuts of the network, where cuts are with respect to the sender on the one
side and to each of the receivers on the other side. The insight is that informa-
tion should not be treated as a ﬂuid, but as mathematical objects that can be
manipulated by intermediate nodes in the network.
Surprisingly, when solvable, the multicast problem can always be dealt with
using simple linear algebra, i.e. the vertices of the network linearly combine
incoming information before passing it on to other vertices. The linear algebra
approach is always possible if the ﬁnite ﬁeld Fq under consideration is of size
at least that of the number of receivers [1,8,11]. In [7], it was further shown
that allowing vertices to choose coding coeﬃcients at random from elements of
the ﬁnite ﬁeld in a distributed manner will result in successful communication
with high probability, provided that the ﬁeld is large enough. This method now is
known as random linear network coding (RLNC). More concretely, lower bounds
on the success probability were given in terms of the ﬁeld’s size and certain
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 118–127, 2017.
DOI: 10.1007/978-3-319-66278-7 11

Random Network Coding over Composite Fields
119
characteristics of the network, such as the number of edges in a ﬂow system (we
formally deﬁne ﬂow systems in Sect. 2). The analysis in [7] is algebraic using the
insight from [11].
In another direction, a simple polynomial time algorithm was given in [8],
which ﬁnds a solution whenever it exists. As it is noted by the authors, this
method gives as a corollary also a lower bound on the success probability of
RLNC. Whereas the method in [8] is purely combinatorial in contrast to [7], it
focuses on edges rather than vertices in much the same way as [7] does. In [2],
it was recognized that using the combinatorial approach from [8], but focusing
instead on vertices, often results in tighter bounds on the success probability.
In the present paper, we follow a trend in more practical network coding
[6,9,13,17] to use small ﬁelds or even composite ﬁelds [3,5,12], i.e. ﬁelds of
varying sizes, to address the problem of restricted computational power in some
network devices. The key intuition is that manipulating objects from a small
ﬁeld requires fewer operations than in a large ﬁeld and can be computed faster
in commercial devices [4,16]. The purpose of our paper is to build a bridge
between this more practical point of view and the purely mathematical point
of view taken in [2,7,8,11] by revisiting the purely mathematical ﬁndings in a
setting of varying ﬁeld sizes (i.e., of composite extension ﬁelds). As each vertex
in this case is assumed to conduct calculations in a ﬁxed ﬁeld corresponding to
its computational power, we shall apply the method from [2], where the focus is
on vertices rather than edges.
To build the bridge, Sect. 2 explains the mathematical problem of multicast
in a network and Sect. 3 provides brief summary on how to implement the use
of varying ﬁeld sizes in connection with communication in networks presented
in [5]. To derive our main result, Sect. 4 ﬁrst presents a strongly modiﬁed version
of the polynomial time algorithm from [8], which in its original form ﬁnds a
solution if one exists. Instead, our modiﬁed algorithm checks if a given random
encoding is successful. From that, we obtain estimates on the success probability
given information on which ﬁeld sizes are used by diﬀerent vertices, following
the method in [2].
2
The Multicast Problem
Let G = (V, E) be a cycle free directed graph, where E = {1, . . . , |E|}. is the
set of edges and V is the set of vertices. Fix a vertex s ∈V called a sender
and R other vertices r1, . . . , rR ∈V called receivers. A message vector X =
(X1, . . . , Xh) is a variable that takes on values in Fh
q , where Fq is the ﬁnite ﬁeld
with q elements. The edges are considered as channels of capacity 1, and the
multicast problem is to get the h messages through to each of the receivers in
one time-step, meaning that each edge is allowed to be used at most once during
the transmission. Vertices are allowed to linearly combine incoming information
before forwarding it to outgoing edges, and receivers will do decoding on the
information traveling on their incoming edges to hopefully reconstruct X. To
describe the communication problem, we introduce variables a1,j, . . . , ah,j and

120
O. Geil and D.E. Lucani
fi,j where 1 ≤i, j ≤|E|. For 1 ≤m ≤h and 1 ≤j ≤|E|, am,j ≡0 if j is not
an outgoing edge of s and for 1 ≤i, j ≤|E|, fi,j ≡0 if there is no vertex having
both i as incoming edge and j as outgoing edge. All variables take on values in
Fq. For a vertex v, let in(v) denote the incoming edges and out(v) the outgoing
edges. Similarly for an edge i = (u, v) we have in(i) = in(u) and out(i) = out(v).
The directed graph being cycle free the vertices can be ordered by an ancestral
ordering ≺V , meaning that if u ≺V v then there does not exist a path from
v to u. Similarly, the edges can be ordered by an ancestral ordering ≺E. For
simplicity, we shall assume in the following that the sender s has no incoming
edges1.
For each edge in the network, we have a variable Y (j) which takes on values
in Fq. For j = out(s), we have
Y (j) =
h

i=1
ai,jXi
(1)
and for other edges we deﬁne
Y (j) =

i∈in(j)
fi,jY (i)
(2)
if there exists a path from s using j. Otherwise, we have Y (j) ≡0. Observe, that
the above is well-deﬁned due to the existence of an ancestral ordering ≺E, and
that for each edge j ∈E
Y (j) = c1(j)X1 + · · · + ch(j)Xh
(3)
for some c1(j), . . . , ch(j) ∈Fq. The task is to choose, if possible, the coeﬃcients
ai,j and fi,j in such a way that a receiver from the Y (j)’s, as seen on its incoming
edges, can reconstruct X = (X1, . . . , Xh). Deﬁne a ﬂow Fℓof size c from s to
receiver rℓ, ℓ∈{1, . . . , R}, to be a set of c edge disjoint paths from s to rℓ. The
union F = ∪R
ℓ=1Fℓis then called a ﬂow system of size c. It is well known [1,8,11]
that the multicast problem is solvable if and only if a ﬂow system of size h
exists, and in this case considering only linear encoding (as described above) is
no restriction. It is also known that for a solvable multicast problem one can use
any ﬁeld Fq, where q ≥R (the number of receivers).
The purpose of the present work is to reinvestigate previous work on the
topic to see which vertices can be allowed to use smaller ﬁelds when linearly
combining messages from the large ﬁeld and still obtain a high probability of
success. We shall always assume that receivers have full computational power, as
they need to perform operations in the big ﬁeld. Such an analysis is relevant as
1 Neither this nor the assumption that we have only one sender is really a restriction,
as one can always add an artiﬁcial vertex s′ to any cycle free network, assume the
messages are generated in s′, and for each message add one edge from s′ to the
source originally generating it. In this case, the ai,j’s will need to be chosen in an
obvious particular way to reﬂect the situation.

Random Network Coding over Composite Fields
121
some vertices may represent physical devices having less computational power
than others. To make (1) and (2) still valid in this situation, we will restrict to
subﬁelds of the overall ﬁeld Fq. Before continuing the discussion of the multicast
problem, we brieﬂy recall in the next section material from [5] on how to use
varying ﬁelds sizes in network communication.
3
Calculations in Composite Fields
In the proposed model, we consider linear combinations of elements from a large
ﬁeld but with coeﬃcients coming from a possible smaller subﬁeld. Such calcu-
lations of course can be done in the large ﬁeld, but what we need is a method
to perform the calculations using only operations in the small (sub)ﬁeld. Fortu-
nately, a method to do exactly this was described in [5]. We illustrate the idea
by an example.
Example 1. Consider F2 ⊂F4 ⊂F16. We write F16 = F4[X4]/⟨F4(X4)⟩, where
F4 ∈F4[X4] and deg F4 = 2 and we let α4 = [X4]+⟨F4⟩. Similarly, we write F4 =
F2[X2]/⟨F2(X2)⟩where F2 ∈F2[X2], deg F2 = 2, and we let α2 = [X2] + ⟨F2⟩.
In this way, we obtain
F16 = {aα4 + b | a, b ∈F4}
(4)
F4 = {cα2 + d | c, d ∈F2}
(5)
We now identify an element β ∈F16 with the following tuples (the latter being
the one traveling through the network):

β(4)
1
β(4)
2

∈F2
4,
⎛
⎜
⎜
⎜
⎝
β(2)
1
β(2)
2
β(2)
3
β(2)
4
⎞
⎟
⎟
⎟
⎠∈F4
2.
(6)
Here,
β = β(4)
1 α4 + β(4)
2 ,
β(4)
1
= β(2)
1 α2 + β(2)
2 ,
and β(4)
2
= β(2)
3 α2 + β(2)
4 ,
where we used the description of F16 and F4 from (4) and (5). To perform
multiplication with γ ∈F4 one simply multiplies each coordinate in the left
vector of (6) by γ. These operations take place in F4. Then, the resulting vector
can be translated into a new vector in F4
2 using a similar approach as above.
Multiplying β with λ ∈F2 is much simpler: one simply needs to multiply each
entry in the right vector in (6) by λ. Finally, addition of elements in F16 is always
done by adding the two corresponding vectors in F4
2. The translation between
the diﬀerent descriptions can either be done algebraically or by look-up tables
(or even by a combination).

122
O. Geil and D.E. Lucani
In the previous example, we considered nested ﬁelds, but the method applies
to all subﬁelds of any given ﬁeld Fq. Hence, we could use it for F64 despite the
corresponding subﬁelds not being nested. The important fact is that for ﬁelds
of characteristic 2, which is the relevant case for current applications, it is the
binary presentation that travels through the network, and indeed F2 is contained
in any ﬁeld of characteristic 2. A similar remark of course holds for any ﬁnite
characteristic p as long as it is the p-ary vector that travels through the network.
Remark 1. Network coding over varying ﬁelds as described in [3,5] and illus-
trated in this section support the use of outer codes in exactly the same way as
classical network coding [1,7] does. This being subspace codes [10], rank-metric
codes [14] or hybrid codes [15].
4
Success Probability
In this section, we establish a lower bound on the success probability of RLNC
when composite extension ﬁelds are used. To better explain the results, we
present an algorithm inspired by the one proposed by Jaggi et al. in [8], but
using, rather than update on edges, updates on vertices as in [2]. We stress that
our algorithm is only intended as a tool to obtain estimates on the success prob-
ability of random network coding – not as an algorithm to ﬁnd actual solutions
of the network coding problem. In contrast to the algorithm in [8], which ﬁnds a
solution to a multicast problem whenever it exists (with ﬁxed ﬁeld size at least
that of the number of receivers), our modiﬁcation simply checks suﬃcient con-
ditions for given coding coeﬃcients ai,j and fi,j to work successfully, in which
case it returns “success”. The estimate on the success probability is found by
estimating from below the probability that the algorithm will return “success”
under the assumption that the coding coeﬃcients ai,j and fi,j not a priori2 set to
0, are chosen uniformly and independently at random from the respective ﬁelds.
By this we mean that ai,j is chosen from the ﬁeld supported by the sender s and
that fi,j is chosen from the ﬁeld supported by the vertex having i as incoming
edge (and j as outgoing edge). The algorithm uses the concept of global coding
coeﬃcients.
Deﬁnition 1. Let X = (X1, . . . , Xh) be the information vector introduced in
Sect. 2 (recall that the entries are variables). Given chosen values of the coding
coeﬃcients ai,j and fi,j of the network, let for an edge ℓ∈E, Y (ℓ) to be as in (3).
The global coding coeﬃcient for the edge ℓnow is cg(ℓ) = (c1(ℓ), . . . , ch(ℓ)).
As previously described, the algorithm takes as input some choice of the
coding coeﬃcients ai,j and fi,j. It also takes as input some positive integer h
less than or equal to the minimum of the min-cuts taken with respect to all the
receivers. It starts by ﬁnding a ﬂow system of size h. The algorithm works with
2 We recall from Sect. 2 that coeﬃcients are set to 0 when they do not correspond to
existing connections.

Random Network Coding over Composite Fields
123
two sets of lists related to this ﬂow system, namely, Cℓand Bℓ, where the index
refers to the receiver rℓ, ℓ= 1, . . . , R. The list Cℓis a cut in the ﬂow system with
respect to rℓand Bℓis the list of global coding vectors of the edges in the cut.
The algorithm returns “failure” if in a step some Bℓis linearly dependent. The
algorithm starts by considering the cuts C1, . . . , CR corresponding to having
s in the one vertex set and all other vertices in the other vertex sets. If the
corresponding B1, . . . , BR are all linearly independent sets, then it visits the
next vertex according to a given ancestral ordering. The cuts C1, . . . , CR are
now the cuts of the ﬂow system corresponding to having s and the newly added
vertex in the one vertex set and all other vertices in the other vertex sets. Again
a check is done to see if all corresponding B1, . . . , BR are linearly independent.
The procedure continues in this way until this is no longer the case or until all
vertices in the ﬂow system, except the receivers, have been visited. If a ﬂow to
some receiver passes through some other receiver, then after having passed the
other receiver, the algorithm will continue by working on a ﬂow system with one
less ﬂow than before. If the above criteria for returning “failure” is not satisﬁed
during the run, the algorithm ends by returning “success”. Clearly, the coding
coeﬃcients allow for decoding at all the receivers in this case. Hence, we can
estimate from below the success probability of random network coding by giving
a lower bound of the algorithm to return “success”.
The analysis of success probability is based on the following lemma.
Lemma 1. Let k, μ be non-negative integers, and let h be a positive integer such
that k + μ < h. Given a ﬁeld Fq consider a basis {b1, . . . , bh} for Fh
q as a vector
space over Fq and let b′
k+1, . . . , b′
k+μ be such that
V = SpanFq{b1, . . . bk, b′
k+1, . . . , b′
k+μ}
is of dimension k + μ as a vector space over Fq. Given c ∈Fh
q the number of
choices of (ak+1, . . . , ah) ∈Fh−k
q
such that c + ak+1bk+1 + · · · + ahbh ∈V equals
qμ. If Ft is a proper subﬁeld of Fq then number of choices of (ak+1, . . . , ah) ∈
Fh−k
t
such that c + ak+1bk+1 + · · · + ahbh ∈V is at most tμ.
Proof. We choose additional vectors b′
k+μ+1, . . . , b′
h such that
Fh
q = SpanFq{b1, . . . , bk, b′
k+1, . . . , b′
h}.
Without loss of generality we may assume that {b′
k+1, . . . , b′
h} is a basis for
U = SpanFq{bk+1, . . . , bh} as a vector space over Fq.
Write
c = c1b1 + · · · + ckbk + ck+1b′
k+1 + · · · + chb′
h.
We have that x ∈U satisﬁes c + x ∈V if and only x is of the form
x = xk+1b′
k+1 + · · · + xk+μb′
k+μ −ck+μ+1b′
k+μ+1 −· · · −chb′
h
That is, there are exactly qμ possible choices of x ∈U with c + x ∈V . But then
there are exactly qμ choices of (ak+1, . . . , ah) ∈Fh−k
q
such that x = ak+1bk+1 +
· · · + ahbh satisﬁes c + x ∈V .

124
O. Geil and D.E. Lucani
To prove the lemma, we must estimate the number of (ak+1, . . . , ah) ∈Fh−k
t
that satisfy
c + ak+1bk+1 + · · · + ahbh ∈V.
(7)
If there are no solutions, then we are done. Assume therefore that a solution
(dk+1, . . . , dh) ∈Fk−h
t
exists. Then, (ek+1, . . . , eh) ∈Fh−k
t
is a solution to the
same problem if and only if
(dk+1 −ek+1)bk+1 + · · · + (dh −eh)bh ∈SpanFq{b′
k+1, . . . , b′
k+μ}.
(8)
But,
{(yk+1, . . . , yh) ∈Fh−k
t
| yk+1bk+1 + · · · + yhbh ∈SpanFq{b′
k+1, . . . , b′
h}}
is a vector space over Ft, as it is closed under addition and scalar multiplication.
Therefore,
{(dk+1 −ek+1, . . . , dh −eh) | (ek+1, . . . , eh) ∈Fh−k
t
is a solution to (7)}
(9)
is a vector space over Ft. Aiming for a contradiction we assume that there are
more than tμ solutions over Ft. Consequently, a basis for (9) as a vector space
over Ft is of size at least μ + 1. Linear independence of vectors in Fh−k
t
over Ft
implies that the same vectors are linear independent over any extension ﬁeld.
The span over Fq of such linear independent vectors by (8) constitutes a subspace
of a space of dimension μ. We have reached a contradiction.
⊓⊔
In the following analysis we shall use qu to denote the ﬁeld size used in vertex
u and ρu to denote the number of ﬂows in the ﬂow system passing through u.
Recall, that all the ﬁelds must be contained in the ﬁeld Fq from which the
messages X1, . . . , Xh are generated, and also recall that all receivers r1, . . . , rR
need to be able to work in the same ﬁeld Fq.
To analyze the probability that the algorithm returns success when taking
as input randomly generated coding coeﬃcients, we may start by deﬁning initial
values of B1, . . . , BR to be
B1 = · · · = BR = {(1, 0 . . . , 0), (0, 1, 0, . . . , 0), . . . , (0, . . . , 0, 1)}
(all vectors are of length h) before running the algorithm. This describes the
situation, where we have h diﬀerent sources X1, . . . , Xh from the beginning.
The global coding vectors traveling on the outgoing edges of the sender s in
an obvious way can be understood as linearly combinations of these vectors
using the coeﬃcients ai,j. Using Lemma 1, we now estimate from below the
probability for not returning failure in one update of the algorithm. Recall, that
we reach a given step in the algorithm provided that all B1, . . . , BR are linearly
independent before the update. Consider a vertex u in which an update takes
place. The existing B1, . . . , BR are to be updated by replacing in Bℓglobal coding

Random Network Coding over Composite Fields
125
vectors on incoming edges in the ﬂow Fℓwith global coding vector on outgoing
edges. In the ﬁrst update where the sender is visited, there are no incoming edges
and all vectors should be updated using the coeﬃcients ai,j’s. To analyze the
probability of not returning “failure”, we study the probability that for each of
the receivers rℓthe new Bℓis linearly independent. For a given ﬁxed receiver
rℓ, let b1, . . . , bk be the global coding vectors in Bℓnot corresponding to edges
that touch u. Let bk+1, . . . , bh be the vectors traveling on the incoming edges to
u in Fℓ. We shall estimate the probability that vectors traveling on the h −k
outgoing edges from u in Fℓtogether with b1, . . . , bk constitute a basis for Fh
q .
Adapting the notation from the lemma, the vectors traveling on the outgoing
edges from u in Fℓwill be denoted b′
k+1, . . . , b′
h. Hence, we are interested in
the probability that b1, . . . , bk, b′
k+1, . . . , b′
h are linearly independent over Fq.
Applying ﬁrst Lemma 1 to estimate the probability that b′
k+1 is independent
from b1, . . . , bk, we must choose μ = 0 in the lemma. Hence, the number of
local encoding coeﬃcients related to the incoming edges in u of Fℓand the
particular outgoing edge under consideration, which will result in a failure, is at
most q0
u = 1. Given that this has been successful, the number of unsuccessful
choices of local encoding coeﬃcients when updating for the next outgoing edge
becomes q1
u as now we must use μ = 1 in the lemma. Continuing in this way and
assuming the worst case, the probability of success for a given receiver when the
incoming edges of a given node u is replaced in the ﬂow Fℓwith the outgoing
edges becomes
h−k

i=1
qh−k
u
−qi
u
qh−k
u
≥1 −
1
qu −1.
Thus, we conclude that the probability for success to hold for all receivers is at
least
1 −
ρu
qu −1
when updating from one cut in the ﬂow system F to the next. Hence, we obtain
the following theorem.
Theorem 1. Consider a cycle free network with one sender s and R receivers.
Assume that h messages from Fq are generated in s and that the network contains
a ﬂow system F of size h. Then, the probability of success when choosing coding
coeﬃcients independently and uniformly at random is at least

u ∈F, u has
outgoing edges in F

1 −
ρu
qu −1

≥

u∈F

1 −
ρu
qu −1

.
provided that all receivers are able to conduct computations in Fq.
From this theorem, it is clear that if the network is sparse in the sense that inner
vertices only are contained in few ﬂows of the ﬂow system then these vertices
can use small subﬁelds of Fq and still the probability of success can be high.
A similar insight was obtained in [3, p. 363 and Sec. 9.9] using the approach
from [8].

126
O. Geil and D.E. Lucani
5
Concluding Remarks
From the estimate in Theorem 1, it is clear that a solution to the multicast
problem can always be found if ρv < qv −1 holds for all vertices. We note that
it is actually possible to prove that the problem is solvable under the milder
condition ρv ≤qv. The proof of this result requires a more direct use of Jaggi
et al.’s algorithm. We also note that following the approach from [2] it is possible
to obtain improved information on the success probability when the information
rate is strictly smaller than the minimum of the min-cuts with respect to all
receivers.
Acknowledgments. The ﬁrst listed author gratefully acknowledge the support from
The Danish Council for Independent Research (Grant No. DFF–4002-00367). The sec-
ond listed author acknowledges the support of the TuneSCode project (Grant No.
DFF - 1335-00125) granted by the Danish Council for Independent Research and by
the Cisco University Research Program Fund (Project CG No. 593761), Gift No. 2015-
146035 (3696).
References
1. Ahlswede, R., Cai, N., Li, S.-Y.R., Yeung, R.W.: Network information ﬂow. IEEE
Trans. Inf. Theor. 46(4), 1204–1216 (2000)
2. Balli, H., Yan, X., Zhang, Z.: On randomized linear network codes and their error
correction capabilities. IEEE Trans. Inf. Theor. 55(7), 3148–3160 (2009)
3. Barbero, A.I., Ytrehus, Ø.: An introduction to network coding for acyclic and cyclic
networks. Sel. Topics Inf. Coding Theor. 7, 339–421 (2010)
4. Guenther, S.M., Riemensberger, M., Utschick, W.: Eﬃcient GF arithmetic for lin-
ear network coding using hardware SIMD extensions. In: International Symposium
on Network Coding (NetCod), pp. 1–6, June 2014
5. Heide, J., Lucani, D.E.: Composite extension ﬁnite ﬁelds for low overhead network
coding: telescopic codes. In: 2015 IEEE International Conference on Communica-
tions (ICC), pp. 4505–4510. IEEE (2015)
6. Heide, J., Pedersen, M.V., Fitzek, F.H.P., Larsen, T.: Network coding for mobile
devices - systematic binary random rateless codes. In: 2009 IEEE International
Conference on Communications Workshops, pp. 1–6, June 2009
7. Ho, T., M´edard, M., Koetter, R., Karger, D.R., Eﬀros, M., Shi, J., Leong, B.:
A random linear network coding approach to multicast. IEEE Trans. Inf. Theor.
52(10), 4413–4430 (2006)
8. Jaggi, S., Sanders, P., Chou, P.A., Eﬀros, M., Egner, S., Jain, K., Tolhuizen,
L.M.G.M.: Polynomial time algorithms for multicast network code construction.
IEEE Trans. Inf. Theor. 51(6), 1973–1982 (2005)
9. Jones, A.L., Chatzigeorgiou, I., Tassi, A.: Binary systematic network coding for
progressive packet decoding. In: IEEE International Conference on Communica-
tions (ICC), pp. 4499–4504, June 2015
10. Koetter, R., Kschischang, F.R.: Coding for errors and erasures in random network
coding. IEEE Trans. Inf. Theor. 54(8), 3579–3591 (2008)
11. Koetter, R., M´edard, M.: An algebraic approach to network coding. IEEE/ACM
Trans. Netw. (TON) 11(5), 782–795 (2003)

Random Network Coding over Composite Fields
127
12. Lucani, D.E., Pedersen, M.V., Ruano, D., Sørensen, C.W., Fitzek, F.H.P., Heide,
J., Geil, O.: Fulcrum network codes: a code for ﬂuid allocation of complexity. arXiv
preprint arXiv:1404.6620 (2014)
13. Paramanathan, A., Pedersen, M.V., Lucani, D.E., Fitzek, F.H.P., Katz, M.: Lean
and mean: network coding for commercial devices. IEEE Wirel. Commun. 20(5),
54–61 (2013)
14. Silva, D., Kschischang, F.R., Koetter, R.: A rank-metric approach to error control
in random network coding. IEEE Trans. Inf. Theor. 54(9), 3951–3967 (2008)
15. Skachek, V., Milenkovic, O., Nedi´c, A.: Hybrid noncoherent network coding. IEEE
Trans. Inf. Theor. 59(6), 3317–3331 (2013)
16. Soerensen, C.W., Paramanathan, A., Cabrera, J.A., Pedersen, M.V., Lucani, D.E.,
Fitzek, F.H.P.: Leaner and meaner: network coding in SIMD enabled commercial
devices. In: IEEE Wireless Communications and Networking Conference (WCNC),
pp. 1–6, April 2016
17. Trullols-Cruces, O., Barcelo-Ordinas, J.M., Fiore, M.: Exact decoding probability
under random linear network coding. IEEE Commun. Lett. 15(1), 67–69 (2011)

Bounding the Minimum Distance of Aﬃne
Variety Codes Using Symbolic Computations
of Footprints
Olav Geil1(B)
and Ferruh ¨Ozbudak1,2
1 Department of Mathematical Sciences,
Aalborg University, Aalborg, Denmark
olav@math.aau.dk
2 Department of Mathematics and Institute of Applied Mathematics,
Middle East Technical University, Ankara, Turkey
ozbudak@metu.edu.tr
Abstract. We study a family of primary aﬃne variety codes deﬁned
from the Klein quartic. The duals of these codes have previously been
treated in [12, Example 3.2]. Among the codes that we construct almost
all have parameters as good as the best known codes according to [9]
and in the remaining few cases the parameters are almost as good. To
establish the code parameters we apply the footprint bound [7,10] from
Gr¨obner basis theory and for this purpose we develop a new method
where we inspired by Buchbergers algorithm perform a series of symbolic
computations.
Keywords: Aﬃne variety codes · Buchberger’s algorithm · Klein curve ·
Minimum distance
1
Introduction
Aﬃne variety codes [5] are codes deﬁned by evaluating multivariate polynomials
at the points of an aﬃne variety. Despite having a simple description such codes
constitute the entire class of linear codes [5, Pro. 1]. Given a description of a code
as an aﬃne variety code it is easy to determine the length n and dimension k, but
no simple general method is known which easily estimates the minimum distance
d. Of course such methods exists for particular classes of aﬃne variety codes.
For instance the Goppa bound for one-point algebraic geometric codes extends
to an improved bound on the more general class of order domain codes [6,11],
and in larger generality the Feng-Rao bounds and their variants can be success-
fully applied to many diﬀerent types of codes [2–4,6,8,12,13]. In this paper we
consider a particular family of primary aﬃne variety codes for which none of
the above mentioned bounds provide accurate information. More precisely we
consider primary codes deﬁned from the Klein quartic using the same weighted
degree lexicographic ordering as in [12, Example 3.2] where they studied the
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 128–138, 2017.
DOI: 10.1007/978-3-319-66278-7 12

Bounding the Minimum Distance of Aﬃne Variety Codes
129
corresponding dual codes. A common property of the Feng-Rao bound for pri-
mary codes and its variants are that they can be viewed [6,8] as consequences of
the footprint bound [7,10] from Gr¨obner basis theory. To establish more accu-
rate information for the codes under consideration it is therefore natural to try
to apply the footprint bound in a more direct way, which is exactly what we
do in the present paper using ingredients from Buchberger’s algorithm and by
considering an exhaustive number of special cases. Our analysis reveals that the
codes under consideration are in most cases as good as the best known codes
according to [9] and for the remaining few cases the minimum distance is only
one less than the best known codes of the same dimension.
The paper is organized as follows. In Sect. 2 we introduce the footprint of an
ideal and deﬁne aﬃne variety codes. We then describe how the footprint bound
can be applied to determine the Hamming weight of a code word. Then in Sect. 3
we apply symbolic computations leading to estimates on the minimum distance
on each of the considered codes the information of which we collect in Sect. 4.
2
Aﬃne Variety Codes and the Footprint Bound
The footprint (also called the delta-set) is deﬁned as follows:
Deﬁnition 1. Given a ﬁeld k, a monomial ordering ≺and an ideal J ⊆
k[X1, . . . , Xm] the footprint of J is
Δ≺(J) = {M | M is a monomial which is not leading monomial
of any polynomial in J}
From [1, Property 7, Sect. 5.3] we have the following well-known result.
Theorem 1. Let the notation be as in the above deﬁnition. The set
{M + J | M ∈Δ≺(J)}
is a basis for k[X1, . . . , Xm]/J as a vector space over k.
Recall that by deﬁnition a Gr¨obner basis is a ﬁnite basis for the ideal J from
which one can easily determine the footprint. Concretely a monomial is a leading
monomial of some polynomial in the ideal if and only if it is divisible by a leading
monomial of some polynomial in the Gr¨obner basis. The following corollary is
an instance of the more general footprint bound [10].
Corollary 1. Let I
⊆Fq[X1, . . . , Xm] be an ideal and Iq
= I + ⟨Xq
1 −
X1, . . . , Xq
m −Xm⟩. The variety of Iq is of size #Δ≺(Iq) for any monomial
ordering ≺.
Proof. Let the variety of Iq be {P1, . . . , Pn} with Pi ̸= Pj for i ̸= j. The ﬁeld Fq
being perfect, the ideal Iq is radical because it contains a univariate square-free
polynomial in each variable and by the ideal-variety correspondence therefore

130
O. Geil and F. ¨Ozbudak
Iq is in fact the vanishing ideal of {P1, . . . , Pn}. Therefore the evaluation map
ev : Fq[X1, . . . , Xm]/Iq →Fn
q given by ev(F + Iq) = (F(P1), . . . , F(Pn)) is
injective. On the other hand the evaluation map is also surjective which is seen
by applying Lagrange interpolation. We have demonstrated that ev is a bijection
and the corollary follows from Theorem 1.
⊓⊔
We are now ready to deﬁne primary aﬃne variety codes formally.
Deﬁnition 2. Let the notation be as in the proof of Corollary 1. Given an ideal
I ⊆Fq[X1, . . . , Xm] and a monomial ordering ≺choose L ⊆Δ≺(Iq). Then
C(I, L) = SpanFq{ev(M + Iq) | M ∈L}
is called a primary aﬃne variety code.
From the above discussion it is clear that C(I, L) is a code of length n = #Δ≺(Iq)
and dimension k = #L. Given a code word c = ev(F + Iq) then by Corollary 1
we have
wH(c) = n −#Δ≺w(⟨F⟩+ Iq) = #Δ≺w(Iq) ∩lm(⟨F⟩+ Iq) = #□≺w(F),
where □≺w(F) := Δ≺w(Iq) ∩lm(⟨F⟩+ Iq). Reducing a polynomial modulo a
Gr¨obner basis for Iq one obtains a (unique) polynomial which has support in
the footprint Δ(Iq) (this is the result behind Theorem 1). Hence we shall always
assume that F is of this form. In the rest of the paper we concentrate on estimat-
ing for a concrete class of codes #□≺(F) using only information on the leading
monomial.
3
Code Words from the Klein Curve
In the remaining part of the paper I will always be the ideal
I = ⟨Y 3 + X3Y + X⟩⊆F8[X, Y ]
and consequently I8 = ⟨Y 3 + X3Y + X, X8 + X, Y 8 + Y ⟩. The corresponding
variety is of size 22, hence we write it as {P1, . . . , P22}. The evaluation map then
becomes ev(F + I8) = (F(P1), . . . , F(P22)).
As monomial ordering we choose the same ordering as in [12, Example 3.2],
namely the weighted degree lexicographic ordering ≺w given by the rule that
XαY β ≺w XγY δ if either (i) or (ii) below holds
(i) 2α + 3β < 2γ + 3δ,
(ii) 2α + 3β = 2γ + 3δ but β < δ.
By inspection {Y 3 +X3Y +X, X8 −X, X7Y +Y } is a Gr¨obner basis for I8 with
respect to ≺w. Hence, the footprint Δ≺w(I8) and the corresponding weights are
as in Fig. 1. We remind the reader that for L ⊆Δ≺w(I8) the code C(I, L) equals
ev(SpanF8(L) + I8) which is of length n = 22 and dimension k = #L.

Bounding the Minimum Distance of Aﬃne Variety Codes
131
Y 2 XY 2 X2Y 2 X3Y 2 X4Y 2 X5Y 2 X6Y 2
Y
XY
X2Y
X3Y
X4Y
X5Y
X6Y
1
X
X2
X3
X4
X5
X6
X7
6
8
10
12
14
16
18
3
5
7
9
11
13
15
0
2
4
6
8
10
12
14
Fig. 1. The footprint Δ≺w(I8) with corresponding weights.
Our method to estimate #□≺w(F) (which corresponds to estimating the
Hamming weight of the corresponding code word) consists in two parts. First
we observe that all monomials in Δ≺w(I8) divisible by the leading monomial of
F are in □≺w(F). In the second part we then for a number of exhaustive special
cases ﬁnd more monomials in □≺w(F) by establishing clever combinations of
polynomials that we already know are in ⟨F⟩+ Iq. To describe how such com-
binations are derived we will need the following notation. Consider polynomials
S(X, Y ), D(X, Y ) and R(X, Y ). By
S(X, Y )
D(X,Y )
−→
R(X, Y )
(1)
we shall indicate that R(X, Y ) = S(X, Y ) −Q(X, Y )D(X, Y ) for some polyno-
mial Q(X, Y ). The important fact – which we shall use frequently throughout
the paper – is that R(X, Y ) ∈⟨S(X, Y ), D(X, Y )⟩.
Remark 1. The Feng-Rao bound can be applied to any aﬃne variety code; but
it works most eﬃciently when the ideal I and the monomial ordering ≺under
consideration satisfy the order domain conditions [6, Sect. 7]. That is,
1. The ordering ≺must be a weighted degree lexicographic ordering (or in larger
generality a generalized weighted degree ordering [6, Deﬁnition 8]).
2. A Gr¨obner basis for I must exist with the property that any polynomial in it
contains in its support (exactly) two monomials of the highest weight.
3. No two diﬀerent monomials in Δ≺(I) are of the same weight.
In such cases the method often establishes many more monomials in □≺(F) than
those divisible by the leading monomial of F. In [8] an improved Feng-Rao bound
was presented which treats in addition eﬃciently certain families of cases where
the conditions 1. and 2. are satisﬁed, but 3. is not. Even though the ideal and
monomial ordering studied in the present section exactly satisfy conditions 1.
and 2., but not 3, the improved Feng-Rao bound produces the same information
as the Feng-Rao bound in this case. By inspection both methods only “detect”
monomials divisible by the leading monomial of F as being members of □≺w(F).
In the following, for simplicity, we shall in our calculations always assume
that the leading coeﬃcient of F is 1.

132
O. Geil and F. ¨Ozbudak
3.1
Leading Monomial Equal to Y
Consider c = ev(F + I8) where F(X, Y ) = Y + a1X + a2. Clearly
{Y, Y 2, XY, XY 2, . . . , X6Y, X6Y 2} ⊂□≺w(F).
We next establish more monomials in □≺w(F) under diﬀerent conditions on the
coeﬃcients a1, a2. Consider
Y 2F(X, Y )
Y 3+X3Y +X
−→
X3Y + a1XY 2 + a2Y 2 + X
F (X,Y )
−→
a1X4 + (a3
1 + a2)X3 + a2
1a2X2 + (a1a2
2 + 1)X + a3
2.
If a1 ̸= 0 then we have
{X4, X5, X6, X7} ⊂□≺w(F).
Next assume a1 = 0. If a2 ̸= 0 then we obtain
{X3, X4, X5, X6, X7} ⊂□≺w(F).
Finally, assume a1 = a2 = 0 in which case we have
{X, X2, X3X4, X5, X6, X7} ⊂□≺w(F).
In conclusion we have shown that □≺w(F) contains at least 14+4 = 18 elements
which implies wH(c) ≥18.
3.2
Leading Monomial Equal to Y 2
Consider a codeword c = ev(F + I8) where
F(X, Y ) = Y 2 + a1X3 + a2XY + a3X2 + a4Y + a5X + a6.
Independently of the coeﬃcients a1, . . . , a6 we see that
{Y 2, XY 2, . . . , X6Y 2} ⊂□≺w(F).
(2)
We next consider an exhaustive series of conditions under which we establish
more monomials in □≺w(F). We have
Y F(X, Y )
Y 3+X3Y +X
−→
(a1 + 1)X3Y + a2XY 2 + a3X2Y
+a4Y 2 + a5XY + a6Y + X.
(3)
If a1 ̸= 1 then the leading monomial of the last polynomial becomes X3Y and
consequently
{X3Y, X4Y, X5Y, X6Y } ∈□≺w(F).
(4)

Bounding the Minimum Distance of Aﬃne Variety Codes
133
Continuing the calculations for this case we obtain:
Y ((a1 + 1)X3Y + a2XY 2 + a3X2Y + a4Y 2 + a5XY + a6Y + X)
F (X,Y )
−→
(a1 + 1)(a1X6 + a2X4Y + a3X5 + a4X3Y + a5X4 + a6X3)
+a2XY 3 + a3X2Y 2 + a4Y 3 + a5XY 2 + a6Y 2 + XY.
If a1 ̸= 0 then we also have
{X6, X7} ⊂□≺w(F).
Assuming next that a1 = 0 the above expression becomes
a2X4Y + a3X5 + a4X3Y + a5X4 + a6X3 + a2XY 3
+a3X2Y 2 + a4Y 3 + a5XY 2 + a6Y 2 + XY
Y 3+X3Y +X
−→
a3X5 + a4X3Y + a5X4 + a6X3 + a3X2Y 2 + a4Y 3
+a5XY 2 + a6Y 2 + XY + a2X2
F (X,Y )
−→
a3X5 + a5X4 + a6X3 + a3a2X3Y + a2
3X4 + a3a4X2Y
+a3a5X3 + a3a6X2 + a5XY 2 + a6Y 2 + XY + a2X2.
If a3 ̸= 0 then
{X5, X6, X7} ⊂□≺w(F).
Hence, continuing under the assumption a3 = 0 we are left with
a5X4 + a6X3 + a5XY 2 + a6Y 2 + XY + a2X2
F (X,Y )
−→
a5X4 + a6X3 + a5a2X2Y + a5a4XY + a2
5X2 + a5a6X + a6Y 2
+XY + a2X2.
if a5 ̸= 0 then
{X4, X5, X6, X7} ⊂□≺w(F).
Hence, assume a5 = 0 and we are left with
a6X3 + a6Y 2 + XY + a2X2
F (X,Y )
−→
a6X3 + a6a2XY + a6a4Y + a2
6 + XY + a2X2.
If a6 ̸= 0 then
{X3, X4, X5, X6, X7} ⊂□≺w(F).
If on the other hand a6 = 0 then we are left with XY + a2X2 in which case we
obtain
{XY, X2Y } ⊂□≺w(F).
In conclusion, for the case a1 ̸= 1 we obtained in addition to the elements in (2)
the elements in (4) and at least 2 more. That is, in addition to the elements

134
O. Geil and F. ¨Ozbudak
in (2) at least 6 more.
Assume in the following that a1 = 1 and continue the reduction from (3)
F (X,Y )
−→
a2X4 + (a3 + a2
2)X2Y + (a2a3 + a4)X3 + a5XY
+(a2a5 + a3a4)X2 + (a6 + a2
4)Y + (1 + a4a5)X + a4a6.
(5)
If a2 ̸= 0 then
{X4, X5, X7, X4Y, X5Y, X6Y } ⊂□≺w(F).
Next assume a2 = 0. If a3 ̸= 0 then
{X2Y, X3Y, X4Y, X5Y, X6Y } ⊂□≺w(F).
(6)
Continuing the reduction under the assumption a3 ̸= 0 we multiply (5) by Y
and continue the reduction:
a3X2Y 2 + a4X3Y + a5XY 2 + a3a4X2Y + (a6 + a2
4)Y
+(1 + a4a5)X + a4a6
F (X,Y )
−→
a3X5 + a2
3X4 + a3a4X2Y + a3a5X3 + a3a6X2 + a4X3Y + a5XY 2
+a3a4X2Y + (a6 + a2
4)Y + (1 + a4a5)X + a4a6.
As a3 ̸= 0 we obtain in addition to (2) and (6) that
{X5, X6, X7} ⊂□≺w(F).
That is, in addition to (2) we found in total 8 more elements in □≺w(F).
Next assume a3 = 0 and continue from (5). If a4 ̸= 0 then
{X3, X4, X5, X6, X7, X3Y, X4Y, X5Y, X6Y } ⊂□≺w(F).
Next assume a4 = 0. if a5 ̸= 0 then
{XY, X2Y, X3Y, X4Y, X5Y, X6Y } ⊂□≺w(F).
Hence, assume a5 = 0. If a6 ̸= 0 then
{Y, XY, X2Y, X3Y, X4Y, X5Y, X6Y } ⊂□≺w(F).
Finally, assume a6 = 0. But then
{X, X2, X3, X4, X5, X6, X7, XY, X2Y, X3Y, X4Y, X5Y, X6Y } ⊂□≺w(F).
In conclusion, we have at least 7 + min{6, 6, 8, 9, 6, 7, 13} = 13 monomials in
□≺w(F) and therefore wH(c) ≥13.

Bounding the Minimum Distance of Aﬃne Variety Codes
135
3.3
Leading Monomial Equal to XY
Consider c = ev(F + I8) where
F(X, Y ) = XY + a1X2 + a2Y + a3X + a4.
For sure
{XY, X2Y, X3Y, X4Y, X5Y, X6Y,
XY 2, X2Y 2, X3Y 2, X4Y 2, X5Y 2, X6Y 2} ⊂□≺w(F).
(7)
We next consider an exhaustive series of conditions under which we establish
more monomials in □precw(F). We have
Y 2F(X, Y )
Y 3+X3Y +X
−→
a1X5 + a3X4 + a4X3
+a1(a2
1X4 + a2
2Y 2 + a2
3X2 + a2
4)
+a3XY 2 + a4Y 2 + X2 + a2X.
If a1 ̸= 0 then
{X5, X6, X7} ⊂□≺w(F).
Hence, assume a1 = 0 and continue the reduction:
F (X,Y )
−→
a3a2Y 2 + a2
3XY + a3a4Y + a3X4 + a4X3 + a4Y 2 + X2 + a2X.
If a3 ̸= 0 then
{X4, X5, X6, X7} ⊂□≺w(F).
Hence, assume a3 = 0 in which case the above becomes
a4Y 2 + a4X3 + X2 + a2X.
If a4 = 0 then
{X2, X3, X4, X5, X6, X7} ⊂□≺w(F).
Hence, assume a4 ̸= 0, in which case we have
{Y 2} ⊂□≺w(F).
We continue the calculations to add more elements. We have:
X2(a4Y 2 + a4X3 + X2 + a2X)
F (X,Y )
−→
a4X5 + a2
2Y 2 + a2X + a2
4.
But then
{X5, X6, X7} ⊂□≺w(F).
That is, for the case a4 ̸= 0 □≺w(F) contains in addition to (7) at least 1+3 = 4
more monomials.
In conclusion wH(c) ≥12+min{3, 4, 6, 4}=15, and if a1 = 0 then wH(c) ≥16.

136
O. Geil and F. ¨Ozbudak
3.4
The Remaining Cases
Using a similar approach as above we can treat the remaining 19 possible cases
corresponding to other choices of leading monomial. For the cases that the lead-
ing monomial is in {XY 2, X2Y, X2Y 2, X3Y, X3Y 2} our analysis is as involved –
or a little more involved – than in the last three subsections. Due to lack of space
these calculations are not included in the present paper. For the last 14 out of
the 19 cases the analysis is simply done by using the fact that all monomials
in Δ≺w(I8) divisible by the leading monomial of F must be in □≺w(F). Our
ﬁndings are collected in Fig. 2.
13
10
7
5
3
2
1
18
15 12 9
6
4
2
22
19 16 13 10 7
4
1
Fig. 2. Lower bounds on #□≺w(F) where lm(F) are as in Fig. 1
4
Code Parameters
As code construction we use
SpanF8{ev(M + I8) | M ∈Δ≺w(I8), δ(M) ≥s},
where δ(M) are the estimates of #□≺w(F) as depicted in Fig. 2. In this way
we obtain the best possible codes, according to our estimates. The resulting
parameters are shown in Table 1. In almost all cases, given a dimension in the
table, then the corresponding estimate on the minimum distance equals the best
value known to exist according to [9]. The only exceptions are the dimensions
4, 14, 15 and 18 where the best minimum distances known to exist are one more
than we obtain.
Table 1. Parameters [n, k, d]8 of codes from the Klein quartic. Here, n and k are sharp
values, whereas d represents a lower bound estimate.
[22, 1, 22]8 [22, 2, 19]8
[22, 3, 18]8
[22, 4, 16]8 [22, 5, 15]8
[22, 7, 13]8
[22, 8, 12]8 [22, 10, 10]8 [22, 11, 9]8
[22, 13, 7]8 [22, 14, 6]8
[22, 15, 5]8
[22, 17, 4]8 [22, 18, 3]8
[22, 20, 2]8

Bounding the Minimum Distance of Aﬃne Variety Codes
137
5
Concluding Remarks
In [12, Example 3.2] the authors estimated the minimum distances of the duals of
the codes studied in the present paper using the Feng-Rao bound for dual codes.
We believe that is should be possible to improve (possibly even drastic) upon
their estimates of the minimum distance in the same way as we in this paper
improved upon the Feng-Rao bound for primary codes. We leave this question for
future research. The method of the present paper also applies to estimate higher
weights (possible relative). We leave it for future research to establish examples
where this gives improved information compared to what can be derived from
the Feng-Rao bound. In the light of Remark 1 and the information established
in Sect. 3, evidently our new method sometimes signiﬁcantly improves upon the
previous known methods. We stress that our method is very general in that it
can be applied to any primary aﬃne variety code. In particular it works for
any monomial ordering and consequently also without any of the order domain
conditions (Remark 1). Finding more families of good aﬃne variety codes using
our method is subject to future work.
Acknowledgments. The authors gratefully acknowledge the support from The
Danish Council for Independent Research (Grant No. DFF–4002-00367). They are also
grateful to Department of Mathematical Sciences, Aalborg University for supporting
a one-month visiting professor position for the second listed author. The research of
Ferruh ¨Ozbudak has been funded by METU Coordinatorship of Scientiﬁc Research
Projects via grant for projects BAP-01-01-2016-008 and BAP-07-05-2017-007.
References
1. Cox, D.A., Little, J., O’Shea, D.: Ideals, Varieties, and Algorithms: An Introduction
to Computational Algebraic Geometry and Commutative Algebra, vol. 4. Springer,
New York (2015)
2. Feng, G.L., Rao, T.R.N.: Decoding algebraic-geometric codes up to the designed
minimum distance. IEEE Trans. Inform. Theory 39(1), 37–45 (1993)
3. Feng, G.L., Rao, T.R.N.: A simple approach for construction of algebraic-geometric
codes from aﬃne plane curves. IEEE Trans. Inform. Theory 40(4), 1003–1012
(1994)
4. Feng, G.L., Rao, T.R.N.: Improved geometric Goppa codes part I: basic theory.
IEEE Trans. Inform. Theory 41(6), 1678–1693 (1995)
5. Fitzgerald, J., Lax, R.F.: Decoding aﬃne variety codes using Gr¨obner bases. Des.
Codes Crypt. 13(2), 147–158 (1998)
6. Geil, O.: Evaluation codes from an aﬃne variety code perspective. In: Mart´ınez-
Moro, E., Munuera, C., Ruano, D. (eds.) Advances in Algebraic Geometry Codes.
Coding Theory and Cryptology, vol. 5, pp. 153–180. World Scientiﬁc, Singapore
(2008)
7. Geil, O., Høholdt, T.: Footprints or generalized Bezout’s theorem. IEEE Trans.
Inform. Theory 46(2), 635–641 (2000)
8. Geil, O., Martin, S.: An improvement of the Feng-Rao bound for primary codes.
Des. Codes Crypt. 76(1), 49–79 (2015)

138
O. Geil and F. ¨Ozbudak
9. Grassl, M.: Bounds on the minimum distance of linear codes and quantum codes
(2007). http://www.codetables.de. Accessed 20 Apr 2017
10. Høholdt, T.: On (or in) Dick Blahut‘s’ footprint’. Codes, Curves and Signals,
pp. 3–9 (1998)
11. Høholdt, T., van Lint, J.H., Pellikaan, R.: Algebraic geometry codes. In: Pless, V.S.,
Huﬀman, W.C. (eds.) Handbook of Coding Theory, vol. 1, pp. 871–961. Elsevier,
Amsterdam (1998)
12. Kolluru, M.S., Feng, G.L., Rao, T.R.N.: Construction of improved geometric Goppa
codes from Klein curves and Klein-like curves. Appl. Algebra Engrg. Comm. Com-
put. 10(6), 433–464 (2000)
13. Salazar, G., Dunn, D., Graham, S.B.: An improvement of the Feng-Rao bound on
minimum distance. Finite Fields Appl. 12, 313–335 (2006)

On Binary Matroid Minors and Applications
to Data Storage over Small Fields
Matthias Grezet(B), Ragnar Freij-Hollanti, Thomas Westerb¨ack,
and Camilla Hollanti
Department of Mathematics and Systems Analysis,
Aalto University, Espoo, Finland
{matthias.grezet,ragnar.freij-hollanti,
thomas.westerback,camilla.hollanti}@aalto.fi
Abstract. Locally repairable codes for distributed storage systems have
gained a lot of interest recently, and various constructions can be found in
the literature. However, most of the constructions result in either large
ﬁeld sizes and hence too high computational complexity for practical
implementation, or in low rates translating into waste of the available
storage space.
In this paper we address this issue by developing theory towards
code existence and design over a given ﬁeld. This is done via exploiting
recently established connections between linear locally repairable codes
and matroids, and using matroid-theoretic characterisations of linearity
over small ﬁelds. In particular, nonexistence can be shown by ﬁnding
certain forbidden uniform minors within the lattice of cyclic ﬂats. It is
shown that the lattice of cyclic ﬂats of binary matroids have additional
structure that signiﬁcantly restricts the possible locality properties of
F2-linear storage codes. Moreover, a collection of criteria for detecting
uniform minors from the lattice of cyclic ﬂats of a given matroid is given,
which is interesting in its own right.
Keywords: Binary matroids · Distributed storage systems · Lattice of
cyclic ﬂats · Locally repairable codes · Uniform minors
1
Introduction
The need for large-scale data storage is continuously increasing. Within the past
few years, distributed storage systems (DSSs) have revolutionised our traditional
ways of storing, securing, and accessing data. Storage node failure is a frequent
obstacle, making repair eﬃciency an important objective. Network coding tech-
niques for DSSs were considered in [6], characterising a storage space–repair
bandwidth tradeoﬀ.
A bottle-neck for repair eﬃciency, measured by the notion of locality [13], is
the number of contacted nodes needed for repair. To this end, our motivation
in this paper comes from locally repairable codes (LRCs), which are, informally
speaking, storage systems where a small number of failing nodes can be recovered
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 139–153, 2017.
DOI: 10.1007/978-3-319-66278-7 13

140
M. Grezet et al.
by boundedly many other (close-by) nodes. Repair-eﬃcient LRCs are already
implemented on HDFS-Xorbas used by Facebook [14] and Windows Azure stor-
age [11]. Here, the ﬁeld size is not yet a huge concern, as the coding is done
over a small number (<20) of nodes. Nevertheless, if we wish for more ﬂexibility
in terms of the code parameters, the ﬁeld size quickly becomes a critical issue.
Hence, a question arises as to how and when can we maintain a small ﬁeld size,
regardless of the number of the storage nodes. Some explicit constructions of
LRCs over small ﬁelds can be found in the literature, e.g., [4,8,12,16,17].
Let us denote by (n, k, d, r, δ), respectively, the code length, dimension, global
minimum distance, locality, and local minimum distance. In terms of a storage
system employing an (n, k, d, r, δ)-LRC, this means that we encode k information
symbols into n code symbols that are then stored on n storage nodes, and can
globally tolerate d −1 node failures while still being able to repair by contacting
k nodes. Locally, if we lose at most δ −1 nodes (δ ≤d), we can repair those by
contacting at most r < k close-by nodes.
It was shown in [18] that the (r, δ = 2)-locality of a linear LRC is a matroid
invariant. The connection between matroid theory and linear LRCs was exam-
ined in more detail in [20]. In addition, the parameters (n, k, d, r, δ) for linear
LRCs were generalised to matroids, and new results for both matroids and linear
LRCs were given therein.
In this paper, we develop theory towards matroids that are representable
over the binary ﬁeld or some other ﬁxed-sized ﬁeld. We show that well-known
matroid-theoretic criteria on binary linear codes give conditions on the lattice
of cyclic ﬂats, which govern the locality properties of the associated storage
codes. As a consequence, we get stronger structural constraints on binary LRCs
compared to those previously known for general LRCs. Moreover, a collection
of criteria for detecting uniform minors from the lattice of cyclic ﬂats of a given
matroid is given, which is interesting in its own right.
2
Preliminaries on LRCs and Matroids
To study LRCs in more detail, we consider punctured codes C|Y , where Y ⊆E
is a set of coordinates of the code C. For a ﬁxed code C, we denote by dY the
minimum Hamming distance of the punctured code C|Y . As is common practice,
we say that C is an (n, k, d)-code if it has length n, dimension k and minimum
Hamming distance d. A linear (n, k, d)-code C over a ﬁeld is a non-degenerate
storage code if d ≥2 and there is no zero column in a generator matrix of C.
Deﬁnition 1. Here, a linear (n, k, d, r, δ)-LRC over a ﬁnite ﬁeld F is a non-
degenerate linear (n, k, d)-code C over FE such that any coordinate x ∈E of C
has locality (r, δ), meaning that there is a subset R of E, called repair set of x,
such that x ∈R, |R| ≤r + δ −1 and dR ≥δ.
The parameters (n, k, d, r, δ) can immediately be deﬁned and studied for
matroids in general, as in [18,20].

On Binary Matroid Minors and Applications to Data Storage
141
2.1
Matroid Fundamentals
Matroids were ﬁrst introduced by Whitney in 1935, to capture and generalise
the notion of linear dependence in purely combinatorial terms. Indeed, the com-
binatorial setting is general enough to also capture many other notions of depen-
dence occurring in mathematics, such as cycles or incidences in a graph, non-
transversality of algebraic varieties, or algebraic dependence of ﬁeld extensions.
Of special interest for linear LRCs is the connection between linear algebra and
matroids.
Matroids have many equivalent deﬁnitions in the literature. Here, we choose
to present matroids via their rank functions. Much of the contents in this section
can be found in more detail in [9].
Deﬁnition 2 (Matroid). A (ﬁnite) matroid M = (ρ, E) is a ﬁnite set E
together with a rank function ρ : 2E →Z such that for all subsets X, Y ⊆E
(R.1) 0 ≤ρ(X) ≤|X|,
(R.2) X ⊆Y
⇒
ρ(X) ≤ρ(Y ),
(R.3) ρ(X) + ρ(Y ) ≥ρ(X ∪Y ) + ρ(X ∩Y ).
A subset X ⊆E is called independent if ρ(X) = |X|. If X is independent
and ρ(X) = ρ(E), then X is called a basis. Strongly related to the rank function
is the nullity function η : 2E →Z, deﬁned by η(X) = |X| −ρ(X) for X ⊆E.
Any matrix G over a ﬁeld F generates a matroid MG = (ρ, E), where E
is the set of columns of G, and ρ(X) is the rank of G(X) over F, where G(X)
denotes the submatrix of G formed by the columns indexed by X. As elementary
row operations preserve the row space of G(X) for all X ⊆E, it follows that
row-equivalent matrices generate the same matroid.
Thus, there is a straightforward connection between linear codes and
matroids. Let C be a linear code over a ﬁeld F. Then any two diﬀerent gen-
erator matrices of C will have the same row space by deﬁnition, so they will
generate the same matroid. Therefore, without any inconsistency, we can denote
the matroid associated to these generator matrices by MC = (ρC, E). The rank
function ρC can be deﬁned directly from the code without referring to a gener-
ator matrix, via ρC(X) = dim(C|X) for X ⊆E.
Example 1. Let C be the linear code generated by the following matrix G
over F2:
G =
1 2 3 4 5 6
1 0 1 0 1 1
0 1 1 0 1 1
0 0 0 1 1 1
Then, for the matroid MC = (ρC, {1, 2, 3, 4, 5, 6}),
ρC(∅) = 0, ρC({1, 2, 3}) = ρC({3, 4, 5}) = 2, ρC({1, 2, 3, 4, 5, 6}) = 3.

142
M. Grezet et al.
Two matroids M1 = (ρ1, E1) and M2 = (ρ2, E2) are isomorphic if there exists
a bijection ψ : E1 →E2 such that ρ2(ψ(X)) = ρ1(X) for all subsets X ⊆E1.
Deﬁnition 3. A matroid that is isomorphic to MG for some matrix G over
F is said to be representable over F. We also say that such a matroid is F-
representable. A binary matroid is a matroid that is F2-representable.
Deﬁnition 4. The uniform matroid U k
n = (ρ, [n]) is a matroid with a ground
set [n] = {1, 2, . . . , n} and a rank function ρ(X) = min{|X|, k} for X ⊆[n].
The following straightforward observation gives a characterisation of max-
imum distance separable (MDS) codes and also shows that uniform matroids
constitute a subclass of representable matroids.
Proposition 1. A linear code C is an (n, k, n −k + 1)-MDS code if and only if
MC is the uniform matroid U k
n.
There are several elementary operations that are useful for explicit construc-
tions of matroids, as well as for analysing their structure. The operations that
we will need for this paper are dualisation, contraction, and deletion.
Deﬁnition 5. Let M = (ρ, E) be a matroid and X, Y ⊆E, and denote by
¯X = E −X for any X ⊆E. Then
(i) The restriction of M to Y is the matroid M|Y = (ρ|Y , Y ), where ρ|Y (A) =
ρ(A) for A ⊆Y .
(ii) The contraction of M by X is the matroid M/X = (ρ/X, ¯X), where
ρ/X(A) = ρ(A ∪X) −ρ(X) for A ⊆¯X.
(iii) A minor of M is the matroid M|Y/X = (ρ|Y/X, Y −X) obtained from M
by restriction to Y and contraction by X. Observe that this does not depend
on the order in which the restriction and contraction are performed.
(iv) The dual of M is the matroid M ∗= (ρ∗, E), where ρ∗(A) = |A| + ρ( ¯A) −
ρ(E) for A ⊆E.
The restriction operation to Y is also referred to as deletion of the set E −Y .
Let M = MC be a representable matroid. Then, restriction, contraction, and
dualisation of M correspond to puncturing, shortening, and orthogonal comple-
ment of the code C, respectively.
Example 2. Let C be the code generated by the matrix G in Example 1. Then
for the matroid MC = (ρ = ρC, [6]), X = {1}, and Y = {2, 3, 4, 5},
ρ/X({2, 3, 4}) = 2, ρ|Y ({2, 3, 4}) = 3, and ρ∗({2, 3, 4}) = 2.
The minors of uniform matroids are very easily described:
Lemma 1. Let U k
n = (ρ, [n]) be a uniform matroid, and let X ⊆Y ⊆E.
Then the minor U k
n|Y/X is isomorphic to U k′
n′ , where k′ = max{0, k −|X|} and
n′ = |Y | −|X|. In particular, M is a minor of U k
n if and only if M ∼= U k′
n′ , for
some 0 ≤k′ ≤k and 0 ≤n′ −k′ ≤n −k.

On Binary Matroid Minors and Applications to Data Storage
143
In general there is no simple criterion to determine if a matroid is repre-
sentable. However, there is a simple criterion for when a matroid is binary.
Theorem 1 ([19]). Let M = (ρ, E) be a matroid. The following two conditions
are equivalent.
1. M is linearly representable over F2.
2. There are no sets X ⊆Y ⊆E such that M|Y /X is isomorphic to the uniform
matroid U 2
4 .
In essence, this means that the only obstruction that needs to be overcome
in order to be representable over the binary alphabet, is that no more than
three nonzero points can ﬁt in the same plane. Clearly, if M is representable
over F, then so are all its minors. The following result, Theorem 2, is a far-going
extension of Theorem 1, that was ﬁrst conjectured by Gian-Carlo Rota in 1970.
A proof of this conjecture was announced by Geelen, Gerards, and Whittle in
2014, but the details of the proof still remain to written up [10].
Theorem 2 ([10]). For any ﬁnite ﬁeld F, there is a ﬁnite set L(F) of matroids
such that any matroid M is representable if and only if it contains no element
from L(F) as a minor.
Since the 1970’s, it has been known that a matroid is representable over F3
if and only if it avoids the uniform matroids U 2
5 , U 3
5 , the Fano plane P2(F2), and
its dual P2(F2)∗as minors. The list L(F4) has seven elements, and was given
explicitly in 2000. For larger ﬁelds, the explicit list is not known, and there is
little hope to even ﬁnd useful bounds on its size.
By the Critical Theorem [5], the matroid MC determines the supports of
a linear code C. Consequently, since binary codes are determined uniquely by
the support of the codewords, binary matroids are in one-to-one correspondence
with binary codes. This is in sharp contrast to linear codes over larger ﬁelds,
where many interesting properties are not determined by the associated matroid.
An important example of such a property is the covering radius [3].
2.2
Fundamentals on Cyclic Flats
The main tool from matroid theory in this paper are the cyclic ﬂats. We will
deﬁne them using the closure and cyclic operator.
Let M = (ρ, E) be a matroid. The closure operator cl : 2E →2E and cyclic
operator cyc : 2E →2E are deﬁned by
(i) cl(X) = X ∪{e ∈E −X : ρ(X ∪e) = ρ(X)},
(ii) cyc(X) = {e ∈X : ρ(X −e) = ρ(X)}.
A subset X ⊆E is a ﬂat if cl(X) = X and a cyclic set if cyc(X) = X. Therefore,
X is a cyclic ﬂat if
ρ(X ∪y) > ρ(X)
and
ρ(X −x) = ρ(X)

144
M. Grezet et al.
for all y ∈E −X and x ∈X. The collection of ﬂats, cyclic sets, and cyclic ﬂats
of M are denoted by F(M), U(M), and Z(M), respectively.
It is easy to verify, as in [2], that the closure operator induces ﬂatness and
preserves cyclicity, and that the cyclic operator induces cyclicity and preserves
ﬂatness. Thus we can write
cl :

2E →F(M)
U(M) →Z(M),
and
cyc :

2E →U(M)
F(M) →Z(M).
In particular, for any set X ⊆E, we have cyc(cl(X)) ∈Z(M) and cl(cyc(X)) ∈
Z(M). Some more fundamental properties of ﬂats, cyclic sets, and cyclic ﬂats
are given in [2].
The cyclic ﬂats of a linear code C of FE can be described as sets X ⊆E such
that
C|(X ∪y) ⊋C|X
and
C|(X −x) = C|X
for all y ∈E−X and x ∈X. Thus, we have the following immediate proposition.
Proposition 2. Let M = (ρ, E) be a matroid. The following are equivalent:
(i) M is the uniform matroid U k
n
(ii) Z = Z(M) is the two element lattice with bottom element 0Z = ∅, top
element 1Z = E and ρ(1Z) = k
Non-degeneracy of a truncated code can be observed immediately from the
lattice of cyclic ﬂats of the associated matroid, as follows.
Proposition 3. Let C be a linear code over FE and X ⊆E. Then C|X is
non-degenerate if and only if 1Z(MC|X) = X and 0Z(MC|X) = ∅.
The minimum distances and the ranks of punctured codes can be computed
from the lattice of cyclic ﬂats via the following theorem. In [20], this was used
to construct matroids and linear LRCs with prescribed parameters (n, k, d, r, δ).
Theorem 3 ([20]). Let C be a linear code over FE and X ⊆E. Then, if C|X
is non-degenerate, it has dimension kX = ρ(1Z(MC|X)) and minimum distance
dX = η(X) + 1 −max{η(Y ) : Y ∈Z(MC|X) and Y ̸= X}.
Example 3. Let MC = (ρC, E = [6]) be the matroid associated to the linear
code C generated by the matrix G given in Example 1. The lattice of cyclic ﬂats
(Z, ⊆) of MC is given in Fig. 1, where the cyclic ﬂat is given inside the node and
its rank is labelled outside the node on the right.
From the lattice of cyclic ﬂats given above, we can conclude that C is a
(6, 3, 2)-code.

On Binary Matroid Minors and Applications to Data Storage
145
∅
0
56
1
123
2
3456
2
E
3
Fig. 1. Lattice of cyclic ﬂats of MC.
3
Suﬃcient Conditions for Uniformity
Our main goal is to study criteria for when M|Y /X is uniform, for X ⊆Y ⊆
E(M). As a preparation, observe that for a uniform matroid M ∼= U k
n, the only
cyclic ﬂats are ∅and E(M), with |E(M)| = n and ρ(E) = k.
Our ﬁrst interest is in the case when X and Y are themselves cyclic ﬂats.
Then we have a straightforward characterisation of cyclic ﬂats in M|Y /X, via
the following two lemmas:
Lemma 2. Let M be a matroid, and let X ⊆Y ⊆E(M) be two sets with
Y ∈F(M). Then F(M|Y/X) = {F ⊆Y −X, F ∪X ∈F(M)}.
Proof. A set S is ﬂat in M|Y /X precisely if ρ(S ∪X ∪i) > ρ(S ∪X) for all
i ∈(Y −X) −S. Since Y is ﬂat, the inequality ρ(S ∪X ∪i) > ρ(S ∪X) will
hold for all i ∈¯Y regardless of S. Thus, S is ﬂat in M|Y /X if and only if S ∪X
is ﬂat in M.
Lemma 3. Let M be a matroid, and let X ⊆Y ⊆E(M) be two sets with
X ∈U(M). Then U(M|Y/X) = {U ⊆Y −X, U ∪X ∈U(M)}.
This is the dual statement, and thus an immediate consequence, of Lemma 2.
We write out the proof explicitly only for illustration.
Proof. A set S is cyclic in M|Y/X precisely if ρ((S ∪X) −i) = ρ(S ∪X) for
all i ∈S. For i ∈X, this will hold regardless of S, since X is cyclic. Thus, S is
cyclic in M|Y/X if and only if S ∪X is cyclic in M.
The previous lemmas give the following immediate corollary:
Corollary 1. Let M = (E, ρ) be a matroid, and let X ⊆Y ⊆E(M) be two sets
with X ∈U(M) and Y ∈F(M). Then Z(M|Y/X) = {Z ⊆Y −X, Z ∪X ∈
Z(M)}, with the rank function ρ|Y/X(Z) = ρ(Z ∪X) −ρ(X).
As a consequence, we get a suﬃcient condition for uniformity of minors, that
only depends on the Hasse diagram of Z(M). Recall that v is said to cover u in
a poset P if u < v and there is no w with u < w < v. If this is the case, then we
write u ⋖P v. This is equivalent to (u, v) being an upwards directed edge in the
Hasse diagram of P.

146
M. Grezet et al.
Theorem 4. Let X and Y be two cyclic ﬂats in M with X ⋖Z(M) Y . Let n =
|Y | −|X| and k = ρ(Y ) −ρ(X). Then M|Y/X ∼= U k
n.
Proof. By Corrolary 1, we have Z(M|Y/X) = {∅, Y −X} as there are no cyclic
ﬂats with X ⊂Z ⊂Y . Again by Corrolary 1, we have ρ(Y −X) = ρ(Y )−ρ(X) =
k. Thus, by Proposition 2, M|Y/X is the uniform matroid U k
n.
Corollary 2. Let M be a matroid that contains no U k
n minors. Then, for every
edge X ⋖Z(M) Y in the Hasse diagram of Z(M), we have ρ(Y ) −ρ(X) < k or
η(Y ) −η(X) < n −k.
Proof. Assume for a contradiction that X ⋖Z(M) Y has ρ(Y ) −ρ(X) = k′ ≥k
or η(Y ) −η(X) = n′ −k′ ≥n −k. Then by Theorem 4, M|Y/X ∼= U k′
n′ , and so
contains U k
n as a minor by Lemma 1.
Now, we are going to need formulas for how to compute the lattice operators
in Z(M|Y/X) in terms of the corresponding operators in Z(M). These can
be derived from corresponding formulas for the closure and cyclic operator. To
derive these, we will need to generalise Corollary 1 to the setting where the
restriction and contraction are not necessarily performed at cyclic ﬂats.
Theorem 5. For X ⊆Y ⊆E, we have
1. Z(M|Y ) = {cyc(Z ∩Y ) : Z ∈Z(M)}
2. Z(M/X) = {cl(X ∪Z) −X : Z ∈Z(M)}
3. Z(M|Y/X) =

cl

X ∪cyc

Z ∩Y

∩

Y −X

: Z ∈Z(M)

=

cyc

cl

X ∪Z

∩Y

−X : Z ∈Z(M)

Proof 1. First, observe that the cyclic operator in M|Y is the same as that in
M, and that the ﬂats in M|Y are {F ∩Y : F ∈F(M)}. Thus we have
Z(M|Y ) = {cyc(F ∩Y ) : F ∈F(M)} ⊇{cyc(Z ∩Y ) : Z ∈Z(M)}.
On the other hand, let A ∈Z(M|Y ), so cyc(A) = A and cl(A) ∩Y = A. But
the closure operator preserves cyclicity, so cl(A) ∈Z(M). We then observe
that
A = cyc(cl(A) ∩Y ) ∈{cyc(Z ∩Y ) : Z ∈Z(M)}.
This proves the reverse inclusion
Z(M|Y ) = {cyc(F ∩Y ) : F ∈F(M)} ⊆{cyc(Z ∩Y ) : Z ∈Z(M)}.
2. This is the dual statement of 1., and so follows immediately by applying 1.
to the matroid M ∗| ¯X/ ¯Y .
3. We ﬁrst apply 2. and then 1. to the restricted matroid M|Y , and get
Z(M|Y/X) = {cl|Y (X ∪cyc(Z ∩Y )) −X : Z ∈Z(M)}.

On Binary Matroid Minors and Applications to Data Storage
147
But if T ⊆Y , then cl|Y (T) = cl(T) ∩Y . Then,
Z(M|Y/X) = {cl(X ∪cyc(Z ∩Y )) ∩(Y −X) : Z ∈Z(M)}
For the second equality of 3. we need to study the operator cyc/X. Suppose
T ⊆E −X. Using duality and the formula for cl| ¯
X, we ﬁnd that
cyc/X(T) = cyc(X ∪T) −X.
Now we are ready to prove the last equality. Applying ﬁrst 1. and then 2. to
the contracted matroid M/X, we get
Z(M|Y/X) = {cyc/X((cl(X ∪Z) −X) ∩Y ) : Z ∈Z(M)}
Applying the formula for cyc/X, we obtain
Z(M|Y/X) = {cyc((cl(X ∪Z) ∩Y −X) ∪X) −X : Z ∈Z(M)}
= {cyc(cl(X ∪Z) ∩Y ) −X : Z ∈Z(M)},
where the last equality follows as X ⊆cl(X ∪Z). This concludes the proof.
4
Criteria for Uniformity via Cyclic Flats
We can use Theorem 5 to ﬁnd conditions for a minor to be isomorphic to a uni-
form matroid. The idea is to detect when Z(M|Y/X) as calculated in Theorem
5 is precisely {∅, Y −X}. Using this, we will be able to ﬁnd some conditions on
the sets X and Y , as well as on the matroid itself.
4.1
Minors Given by Restriction or Contraction only
We will begin by considering a simpler case when the minor is the result of a
restriction only, i.e., when the minor is given by M|Y . So, let M = (E, ρ) be a
matroid and Y an arbitrary subset of E. We can use Corollary 1 to restrict the
amount of information we need to consider. Indeed, by properties of minors, we
have
M|Y = M|cl(Y ) \ (cl(Y ) −Y ).
(1)
Then, Theorem 5 states that the cyclic ﬂats of M|Y depend only on the
cyclic ﬂats of M|cl(Y ). Furthermore, according to Corollary 1 the cyclic ﬂats
of M|cl(Y ) are exactly the cyclic ﬂats of M contained in cl(Y ). Hence, we can
restrict the study to the case when M = (E, ρ) is a matroid and Y is a subset
of full rank. Deﬁne k := ρ(Y ) and n := |Y |. With this setup, we obtain the
following theorem.
Theorem 6. Let M = (E, ρ) be a matroid and Y a subset of full rank. M|Y is
isomorphic to the uniform matroid U k
n if and only if either Y is a basis of M or
the following two conditions are satisﬁed:

148
M. Grezet et al.
1. Y is a cyclic set of M.
2. For all Z ∈Z(M) with ρ(Z) < k, we have that Z ∩Y is independent in M.
Before stating the proof, we will need one useful lemma about the properties
of the closure and cyclic operator.
Lemma 4. Let M = (E, ρ) be a matroid and Y ⊆E. Then
1. cl(cyc(Y )) ∩Y = cyc(Y ).
2. cyc(cl(Y )) ∪Y = cl(Y ).
The proof of Lemma 4 is straightfrorward from the deﬁnition of the operators
together with the submodularity of the rank function. Details of the proof can
be found in [2]. We now present the proof of Theorem 6.
Proof. We know that M|Y ∼= U k
n if and only if Z(M|Y ) = {∅, Y }. On the other
hand, we know by Theorem 5, that Z(M|Y ) = {cyc(Z ∩Y ) : Z ∈Z(M)}. Then
we have
M|Y ∼= U k
n if and only if cyc(Z ∩Y ) ∈{∅, Y } for all Z ∈Z(M).
Now consider the cyclic ﬂat ZY := cl(cyc(Y )). Using Lemma 4, we have
cyc(ZY ∩Y ) = cyc(Y ). Two cases can occur. If cyc(Y ) = ∅then Y was a basis
of M and we end up with a minor isomorphic to U k
k . If not, then cyc(Y ) must
be equal to Y . So, we have that Y ∈Z(M|Y ) if and only if Y is a cyclic set and
we obtain the ﬁrst condition. Since Y already has full rank, there is only one
cyclic ﬂat that contains Y , namely cl(cyc(Y )) = E. Therefore, for every other
cyclic ﬂat Z, i.e., for all Z with ρ(Z) < k, we have cyc(Z ∩Y ) ⊆Z ∩Y ̸= Y .
But, by Theorem 5, cyc(Z ∩Y ) is a cyclic ﬂat of M|Y . Thus, for all Z ∈Z(M)
with ρ(Z) < k, we have cyc(Z ∩Y ) = ∅, or equivalently, Z ∩Y is independent.
Notice that, combined with the ﬁrst condition, this implies immediately that
Z(M|Y ) = {∅, Y }. This concludes the proof.
Corollary 3. Under the above assumptions, if M|Y is isomorphic to U k
n then
the ground set E must be a cyclic ﬂat, i.e., E ∈Z(M).
Example 4. We will look at the matroid MC arising from the binary matrix in
Example 1. Since it is a binary matroid, we cannot ﬁnd a minor isomorphic to
U 2
4 . Using the lattice of cyclic ﬂats of Fig. 1, we can ﬁnd a minor isomorphic
to U 2
3 if we look at, e.g., MC|{1, 2, 3}. We can also ﬁnd a minor isomorphic to
U 3
4 . To this end, we look at MC|{1, 2, 4, 5}. The set {1, 2, 4, 5} is indeed a cyclic
set because cyc({1, 2, 4, 5}) = {1, 2, 4, 5}. Furthermore, there is no cyclic ﬂat
properly contained in {1, 2, 4, 5}, and every intersection of a cyclic ﬂat diﬀerent
from E with the set {1, 2, 4, 5} gives us an independent set. Thus, the conditions
from the previous theorem are met and we get a minor isomorphic to U 3
4 .
Now we can do the same for M/X and use duality to get back to the restric-
tion case. By minor properties, we have
M/X = M/cyc(X)/(X −cyc(X)).
(2)

On Binary Matroid Minors and Applications to Data Storage
149
Then, Corollary 1 states that the cyclic ﬂats of M/cyc(X) are the cyclic ﬂats
of M that contain cyc(X). Thus, we will consider a matroid M = (E, ρ) and X
an independent subset of E. Deﬁne k := ρ(E) −ρ(X) and n := |E −X|. Then,
we have the following dual statement of Theorem 6.
Theorem 7. Let M = (E, ρ) be a matroid and X an independent subset of E.
M/X is isomorphic to the uniform matroid U k
n if and only if either X is a basis
of M or the following two conditions are satisﬁed:
1. X is a ﬂat of M.
2. For all Z ∈Z(M) with ρ(Z) > 0, we have cl(X ∪Z) = E.
Corollary 4. Under the above assumptions, if M/X is isomorphic to U k
n then
the empty set must be a cyclic ﬂat, i.e., ∅∈Z(M).
4.2
Minors Given by both Restriction and Contraction
This part combines the two previous situations into a more general statement.
We will see that, when we allow both a restriction and a contraction to occur,
we lose some conditions on the matroid that are then replaced by conditions on
the sets used in the minor.
Let M = (E, ρ) be a matroid and X ⊂Y ⊆E two sets. Combining minor
properties (1) and (2) and Corollary 1, it is suﬃcient to only consider the cyclic
ﬂats between cyc(X) and cl(Y ). In addition, we want to avoid some known cases,
namely when Y is a basis (we will obtain U k
k ) and when X has full rank (we will
obtain U 0
0 ). Deﬁne k := ρ(E) −ρ(X) and n := |Y −X|. We get the following
theorem.
Theorem 8. Let M = (E, ρ) be a matroid and X ⊂Y ⊆E two sets such that
Y is a dependent full-rank set and X is an independent set with ρ(X) < ρ(E).
The minor M|Y /X is isomorphic to a uniform matroid U k
n if and only if
1. cl(X) ∩Y = X,
2. Y −X ⊆cyc(Y ),
3. for all Z ∈Z(M) either Z ∩Y is independent or
cl(X ∪cyc(Z ∩Y )) = E.
Proof. Using Theorem 5, we have that M|Y/X ∼= U k
n if and only if
cl(X ∪cyc(Z ∩Y )) ∩(Y −X) ∈{∅, Y −X} for all Z ∈Z(M).
In particular, it holds for Z = 0Z. Let Z′
0 := cl(X ∪cyc(0Z ∩Y )). Using the
properties of the closure, we have
cl(X) ⊆Z′
0 ⊆cl(X ∪0Z) = cl(X).
Thus, we have a chain of equalities and, in particular, ρ(Z′
0) < ρ(E). This means
that Z′
0 = ∅in order to have ∅∈Z(M|Y/X). But, since Z′
0 = cl(X) then Z′
0 = ∅
is equivalent to cl(X) ∩Y = X and Condition 1 is proved.

150
M. Grezet et al.
Now, consider the cyclic ﬂat ZY := cl(cyc(Y )). First, using again Lemma 4,
we have cyc(ZY ∩Y ) = cyc(Y ). Since Y is a dependent subset, cyc(Y ) ̸= ∅and
thus X ⊊X ∪cyc(Y ). Then, the closure cannot be contained in X and we must
have
cl(X ∪cyc(Y )) ∩(Y −X) = Y −X.
Deﬁne Z′
Y := cl(X ∪cyc(Y )). The above equality means that Y −X ⊆Z′
Y .
On the other hand, we have that X ⊆Z′
Y . Then, Y ⊆Z′
Y and Z′
Y = E. In
particular, we must have Y −X ⊆cyc(Y ). Indeed, assume by contradiction that
there exists a ∈Y −X and a /∈cyc(Y ). Then, by deﬁnition of the cyclic operator,
ρ(Y −a) < ρ(Y ). But since a /∈cyc(Y ) and a /∈X then X ∪cyc(Y ) ⊆Y −a.
This implies that ρ(X ∪cyc(Y )) ≤ρ(Y −a) < ρ(Y ) which is a contradiction.
The condition Y −X ⊆cyc(Y ) is also suﬃcient to guarantee that Y −X ∈
Z(M|Y/X). This proves Condition 2.
Finally, for every other cyclic ﬂat of M, cl(X∪cyc(Z∩Y ))∩(Y −X) = ∅if and
only if cyc(Z ∩Y ) ⊆X. But since X is independent, this is equivalent to Z ∩Y
being independent. On the other hand, cl(X ∪cyc(Z ∩Y )) ∩(Y −X) = Y −X
if and only if cl(X ∪cyc(Z ∩Y )) = E. This concludes the proof.
5
Stuctural Properties of Binary LRCs
From a practical viewpoint, storage systems over alphabets of bounded size
are of special interest. The ﬁeld size is important both because it governs the
complexity of the computations involved in repair and retrieval, and because
it restricts the size of the data items stored. We are therefore interested in
understanding the matroidal structure of LRCs that are linearly representable
over the ﬁnite ﬁeld Fq, where q is small.
Assuming the MDS conjecture [15], a matroid M that is linearly representable
over Fq must avoid U k
q+2 as a minor, for k = 2, 4 ≤k ≤q −2, and k = q. If q is
odd, M must also avoid U 3
q+2 and U q−1
q+2 minors. The MDS conjecture is widely
believed to be true, and is proven when q is prime [1].
By Corollary 2, matroids avoiding U k
n minors have a rather special structure
in their lattice of cyclic ﬂats. In particular, it tells us that whenever X ⋖Z Y ,
we cannot simultaneously have ρ(Y ) −ρ(X) > k and η(Y ) −η(X) > n −k. This
observation can be exploited to bound locality parameters of an LRC in terms
of the ﬁeld size.
Of special interest are binary storage codes that are linear over F2. It is
known that a matroid is representable over F2 if and only if it avoids U 2
4 as
a minor. In this case, Corollary 2 tells us that we cannot simultaneously have
ρ(Y )−ρ(X) > 1 and η(Y )−η(X) > 1. On the other hand, we know by Theorem
3.2 in [2] or by direct calculation, that we always have ρ(Y ) −ρ(X) ≥1 and
η(Y ) −η(X) ≥1. Thus, if M is representable over F2, then every edge X ⋖Z Y
in the Hasse diagram of Z(M) satisﬁes exactly one of the following:
(i) ρ(Y ) −ρ(X) = l > 1. We call such an edge a rank edge, and label it ρ = l.
Such an edge corresponds to a U l
l+1 minor in M.

On Binary Matroid Minors and Applications to Data Storage
151
(ii) η(Y )−η(X) = l > 1. We call such an edge a nullity edge, and label it η = l.
Such an edge corresponds to a U 1
l+1 minor in M.
(iii) ρ(Y ) −ρ(X) = 1 and η(Y ) −η(X) = 1. We call such an edge an elementary
edge. Such an edge corresponds to a U 1
2 minor in M.
As an example, the matroid from Examples 1 and 2 gets an edge labelling as
illustrated in Fig. 2:
∅
56
123
3456
E
ρ = 2
η = 2
Fig. 2. Lattice of cyclic ﬂats of MC.
It is clear that this representation is enough to reconstruct the so-called
conﬁguration of the matroid, i.e., the isomorphism type of the lattice of cyclic
ﬂats, together with the cardinality and rank of the cyclic ﬂats. However, this
data does not uniquely determine the matroid, as is shown in [7].
As is proven in [20], the conﬁguration of a representable matroid determines
the minimum distance of the corresponding code, via the formula
dC = η(E) + 1 −
max
Z∈Z(M)−{E} η(Z).
In particular, for a binary code C with dC > 2, all edges Z ⋖E on the “top
level” of Z(MC) must be nullity edges. Moreover, the minimum distance is then
one higher than the smallest label of a top level edge in Z(MC).
Recall that an (n, k, d)-storage code is said to have locality (r, δ) if every
storage node i ∈[n] is contained in a set X with |X| ≤r + δ −1 and dX ≥δ.
This is equivalent to that every node i ∈[n] is contained in a set X with sX :=
|X| −dX + 1 ≤r and dX ≥δ. Now, notice that sX = ρ(X) + maxZ<ZX η(Z),
and so sX does not increase when replacing X by its closure, which is a cyclic
ﬂat as X is cyclic. This gives a lattice-theoretic description of Z(MC), when C
is a binary code with (r, δ)-locality.
Surprisingly, the descriptions are qualitatively diﬀerent depending on
whether δ = 2 or δ > 2, i.e., whether one or more erasures can be corrected
locally. This is in sharp contrast to the case when the ﬁeld size is ignored, as
in [20]. We conclude this paper by formulating the locality criteria for binary stor-
age codes in terms of lattices of cyclic ﬂats. Exploiting this description to obtain

152
M. Grezet et al.
quantitative bounds on the parameters (n, k, d, r, δ) is left for future research.
Such bounds are also likely to suggest explicit constructions of extremal LCFs
satisfying the conditions of Theorem 9.
Theorem 9. Let d > 2 and let C be a linear (n, k, d, r, δ)-LRC over F2. Then
Z = Z(MC) satisﬁes the following:
1. ∅and [n] are cyclic ﬂats.
2. Every covering relation Z⋖Z[n] is a nullity edge labeled with a number ≥d−1.
3. If δ = 2, then for every i ∈[n], there is X ∈Z with i ∈X such that ρ(X) ≤r.
4. If δ > 2, then for every i ∈[n], there is X ∈Z with i ∈X such that
(i) Every covering relation Y ⋖Z X is a nullity edge labeled with a number
≥δ −1.
(ii) Every cyclic ﬂat Y with Y ⋖Z X has size ≤r −1.
6
Conclusions and Future Work
We have studied the lattice of cyclic ﬂats of matroids, with a special emphasis
on identifying uniform minors, and with applications to locally repairable codes
over small ﬁelds. Necessary and suﬃcient criteria for a speciﬁed minor M|Y /X
to be uniform are derived in general, and in the special case of U 2
4 -minors, nec-
essary global criteria for U 2
4 -avoidance are given. Finally, it is shown how these
criteria dictate the structure of binary storage codes with prescribed locality
parameters. Future work include translating these structural results to quan-
titative parameter bounds. Similar arguments are likely to be applicable when
studying storage codes over other small ﬁelds, although new methods would then
be needed to identify other minors than uniform ones.
Acknowledgment. The authors gratefully acknowledge the ﬁnancial support from
the Academy of Finland (grants #276031 and #303819).
References
1. Ball, S.: On sets of vectors of a ﬁnite vector space in which every subset of basis
size is a basis. J. Eur. Math. Soc. 14, 733–748 (2012)
2. Bonin, J.E., de Mier, A.: The lattice of cyclic ﬂats of a matroid. Ann. Comb. 12,
155–170 (2008)
3. Britz, T., Rutherford, C.G.: Covering radii are not matroid invariants. Discrete
Math. 296, 117–120 (2005)
4. Cadambe, V., Mazumdar, A.: An upper bound on the size of locally recoverable
codes. In: International Symposium on Network Coding, pp. 1–5 (2013)
5. Crapo, H., Rota, G.C.: On the Foundations of Combinatorial Theory: Combinato-
rial Geometries, Preliminary edition edn. MIT Press, Cambridge (1970)
6. Dimakis, A., Godfrey, P.B., Wu, Y., Wainwright, M.J., Ramchandran, K.: Network
coding for distributed storage systems. IEEE Trans. Inf. Theory 56(9), 4539–4551
(2010)

On Binary Matroid Minors and Applications to Data Storage
153
7. Eberhardt, J.: Computing the tutte polynomial of a matroid from its lattice of
cyclic ﬂats. Electron. J. Comb. 21, 12 (2014)
8. Ernvall, T., Westerb¨ack, T., Freij-Hollanti, R., Hollanti, C.: Constructions and
properties of linear locally repairable codes. IEEE Trans. Inf. Theory 62, 5296–
5315 (2016)
9. Freij-Hollanti, R., Hollanti, C., Westerb¨ack, T.: Matroid theory and storage codes:
bounds and constructions (2017), arXiv: 1704.0400
10. Geelen, J., Gerards, B., Whittle, G.: Solving Rota’s conjecture. Not. Am. Math.
Soc. 61, 736–743 (2014)
11. Huang, C., Simitci, H., Xu, Y., Ogus, A., Calder, B., Gopalan, P., Li, J., Yekhanin,
S.: Erasure coding in Windows Azure storage. In: Proceedings of the USENIX
Annual Technical Conference, pp. 15–26 (2012)
12. Huang, P., Yaakobi, E., Uchikawa, H., Siegel, P.H.: Binary linear locally repairable
codes. IEEE Trans. Inf. Theory 62, 5296–5315 (2016)
13. Papailiopoulos, D., Dimakis, A.: Locally repairable codes. In: International Sym-
posium on Information Theory, pp. 2771–2775. IEEE (2012)
14. Sathiamoorthy, M., Asteris, M., Papailiopoulos, D., Dimakis, A.G., Vadali, R.,
Chen, S., Borthakur, D.: Xoring elephants: novel erasure codes for Big Data. Proc.
VLDB. 6, 325–336 (2013)
15. Segre, B.: Curve razionali normali e k-archi negli spazi ﬁniti. Annali di Matematica
39, 357–359 (1955)
16. Silberstein, N., Zeh, A.: Optimal binary locally repairable codes via anticodes
(2015), arXiv: 1501.07114v1
17. Tamo, I., Barg, A., Frolov, A.: Bounds on the parameters of locally recoverable
codes. IEEE Trans. Inf. Theory 62(6), 3070–3083 (2016)
18. Tamo, I., Papailiopoulos, D., Dimakis, A.: Optimal locally repairable codes and
connections to matroid theory. IEEE Trans. Inf. Theory 62, 6661–6671 (2016)
19. Tutte, W.: A homotopy theorem for matroids, I, II. Trans. Am. Math. Soc. 88,
148–178 (1958)
20. Westerb¨ack, T., Freij-Hollanti, R., Ernvall, T., Hollanti, C.: On the combinatorics
of locally repairable codes via matroid theory. IEEE Trans. Inf. Theory 62, 5296–
5315 (2016)

Absorbing Set Analysis of Codes
from Aﬃne Planes
Kathryn Haymaker(B)
Villanova University, 800 E. Lancaster Ave, Villanova, PA 19085, USA
kathryn.haymaker@villanova.edu
Abstract. We examine the presence of absorbing sets, fully absorbing
sets, and elementary absorbing sets in low-density parity-check (LDPC)
codes arising from certain classes of ﬁnite geometries. In particular, we
analyze the absorbing set spectra of LDPC codes from ﬁnite Euclidean
planes. For some parameters, we classify the absorbing sets present and
give exact counts on their multiplicities.
Keywords: Absorbing sets · Fully absorbing sets · Finite geometry
LDPC codes · Euclidean geometries · Finite planes
1
Introduction
Codes based on low-density parity-check (LDPC) matrices have been in the fore-
front of research in coding theory due to their low-complexity eﬃcient decoders
and near capacity performance at long block lengths. A geometric approach
to constructing these codes was given in [13,14]. The resulting ﬁnite geometry
LDPC (FG-LDPC) codes are based on the points and lines of ﬁnite Euclidean
and projective geometries, the structure of which can be used to prove para-
meters of the code. Stopping sets and pseudocodewords of FG-LDPC codes
were studied in [11,22]. Trapping sets of FG-LDPC codes were studied in [5,16],
while absorbing sets of LDPC codes from ﬁnite planes were analyzed in [17]. The
research in this paper is inspired by the results in [17], particularly the authors’
discussion describing the diﬃculty of dealing with a subgeometry of EG(2, q).
In this paper we analyze the absorbing set structures of the EG(2, q) classes of
FG-LDPC codes, where we consider an incidence matrix of the entire Euclidean
geometry instead of a subgeometry.
Research into the error ﬂoor phenomenon that occurs in the bit error rate
(BER) curves for structured families of LDPC codes under message-passing iter-
ative decoders has shown that certain graphical substructures contribute to the
persistence of the error ﬂoor. Speciﬁcally, structural properties of the Tanner
graph of the code are tied to pseudocodewords, absorbing sets, trapping sets, and
stopping sets [4,7,12,18]. While trapping sets depend on the decoder, absorbing
sets are a combinatorial substructure of the Tanner graph that are independent
of the channel and in certain cases are also stable under bit-ﬂipping decoding.
For EG(2, q) classes of FG-LDPC codes, the error performance of the codes is
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 154–162, 2017.
DOI: 10.1007/978-3-319-66278-7 14

Absorbing Set Analysis of Codes from Aﬃne Planes
155
good under the sum-product and other iterative decoding algorithms [14]. This
paper proves theoretical results on the non-existence of small absorbing sets
in these codes, which is consistent with the decoding simulations of EG codes
[13,14].
This paper is organized as follows. In Sect. 2, we give the necessary back-
ground and notation for absorbing sets, and deﬁne the classes of ﬁnite geometry
codes that we consider in this paper. In Sect. 3 we present results for codes based
on ﬁnite Euclidean geometries. Section 4 concludes the paper.
2
Preliminaries
In [14], Kou, Lin, and Fossorier describe families of cyclic or quasi-cyclic LDPC
codes with parity-check matrices determined by the incidence structure of ﬁnite
Euclidean and projective geometries. The Euclidean geometry constructions
involve deﬁning a subgeometry without the origin point, and creating incidence
matrices of points and lines for these families of subgeometries. These matrices
alone can be used as parity-check matrices of LDPC codes; they can also be
extended by a column splitting process that results in a code of longer length.
The cyclic or quasi-cyclic structure of these codes is an advantage, however the
parity-check matrices can have extra redundancy in the number of rows. Higher
redundancy in the parity-check matrices results in increased decoding complex-
ity, but it also has a positive eﬀect on the decoding performance of the codes
[8,11,19].
We recall the basic properties of ﬁnite projective and Euclidean geometries,
starting with the deﬁnitions of aﬃne and projective space [3].
Deﬁnition 1. A linear space is a collection of points and lines such that any
line has at least two points, and two points are on precisely one line. A hyperplane
of a linear space is a maximal proper subspace. A projective plane is a linear
space in which any two lines meet, and there exists a set of four points, no three
of which are collinear. (A projective plane has dimension 2.) A projective space
is a linear space in which any two-dimensional subspace is a projective plane.
An aﬃne space is a projective space with one hyperplane removed.
Like the Euclidean space Rn, the set of points formed by m-tuples with entries
from the ﬁnite ﬁeld Fq forms an aﬃne space, called a ﬁnite Euclidean geometry.
A ﬁnite Euclidean geometry satisﬁes the axioms listed in Deﬁnition 1, and com-
prises the family of ﬁnite geometries that we will consider in this paper. In the
case of m = 2, lines are sets of points (x, y) satisfying an equation y = mx + b
or x = a, where m, b, a ∈Fq. The m-dimensional ﬁnite Euclidean geometry
EG(m, ps) has the following parameters. There are pms points and the number
of lines is
ps(m−1)(pms −1)
ps −1
.

156
K. Haymaker
Each line contains ps points and each point is on pms −1
ps −1 lines. Any two points
have exactly one line in common and any two lines either have one point in
common or are parallel (i.e., have no points in common).
A μ-dimensional subspace of a ﬁnite geometry is called a μ-ﬂat.
An LDPC code can be formed from an m-dimensional ﬁnite geometry by
taking the incidence matrix of μ1-ﬂats and μ2-ﬂats, where 0 ≤μ1 < μ2 ≤m.
Taking μ1 = 0 and μ2 = 1 gives the incidence matrix of points and lines in a
ﬁnite geometry, which is one of the constructions presented in [14]. However,
in [14], the authors eliminate the origin point and all lines incident to it in the
Euclidean geometry before creating the incidence matrix, because it results in
a cyclic or quasi-cyclic code. In this paper, we include the origin point, in order
to use the full geometric structure for absorbing set analysis (see [17] for the
subgeometry viewpoint). Keeping the origin point also allows us to view the
ﬁnite plane codes as a special case of the Gallager-like codes constructed in [20].
We use the notation HEG(m, ps) to denote a parity-check matrix formed
as the incidence matrix of points and lines in EG(m, ps). Points correspond
to columns in HEG(m, ps), and lines in the geometry correspond to rows in
the parity-check matrix. Notice that HEG(m, ps) has entries from F2. The code
deﬁned by HEG(m, ps) is denoted CEG(m, ps). When we refer to the EG code
formed by puncturing the geometry on the origin point, we denote that sub-
geometry code by C∗EG(m, ps). In this paper we will focus on codes of form
CEG(2, q). The minimum distance of such codes is given by: d ≥q + 2. (To see
this, consider a point in the geometry and the q + 1 lines containing it.)
Let G = (V, W; E) be a bipartite Tanner graph corresponding to an LDPC
code, where V and W denote the sets of variable nodes and check nodes, respec-
tively, and E is the set of edges. For a subset S of V , let GS = (S, WS; ES)
denote the subgraph induced by S in G. In the induced subgraph, WS is the set
of constraint neighbors of S, and ES is the set of edges between S and WS.
Deﬁnition 2 (Dolecek et al., [7]). An (a, b) absorbing set is a subset S of
V where |S| = a, and there are b odd degree vertices in WS, with the property
that every vertex v ∈S has more even-degree than odd-degree neighbors in GS.
Let O(S) (resp., E(S)) denote the vertices in WS with odd degree (even degree)
in GS. If in addition, all variable nodes in V \S have strictly more neighbors
in W\O(S) than in O(S), then S is a fully absorbing set. An elementary
absorbing set is an absorbing set in which all vertices in WS have degree one
or two in GS [1].
Although an absorbing set is a special subgraph of the Tanner graph of H,
for brevity we will refer to an absorbing set of H.
Constraint nodes in the Tanner graph of HEG(2, q) correspond to lines in the
geometry, so we say a line is odd if its corresponding constraint node is in O(S)
with respect to some ﬁxed set S. An even line has a constraint node in E(S).
The smallest (a, b) absorbing set of the Tanner graph of a code is the size of
the smallest a, and the corresponding smallest b for that given a, for which an
absorbing set exists. Small absorbing sets may provide information that can be

Absorbing Set Analysis of Codes from Aﬃne Planes
157
used to design low error-ﬂoor decoders [15], and the structure inherent in FG-
LDPC codes allows for enumeration and explicit description of these structures.
There are a wide variety of ﬁnite geometry codes, including codes for which
a parity-check matrix is the incidence matrix of lines and points (the transpose
of H), along with many other variations [14]. Creative constructions of codes
using other ﬁnite incidence structures such as generalized quadrangles and latin
squares have also been studied extensively [10,11,21].
We now deﬁne an important geometry substructure that has been used to
ﬁnd absorbing sets in other work [17].
Deﬁnition 3. A (k, d)-arc in a ﬁnite aﬃne or projective plane is a set of k
points, such that d of them are collinear, and any collection of d + 1 are not
collinear. Often (k, 2)-arcs are simply referred to as k-arcs.
Arcs can be deﬁned for higher-dimensional ﬁnite geometries as well; these
geometric structures are the subject of ongoing research in geometry [2,9].
Remark 1. The following rephrases Lemma 1 of [6] in terms of codes from ﬁnite
Euclidean planes, CEG(2, q).
Lemma 1 (Dolecek, [6]).
The parameters (a∗, b∗) of the smallest absorbing
sets for HEG(2, q) satisfy a∗≥2 +
q
2

, and b∗≥a∗·
q
2

.
3
Finite Euclidean LDPC Codes
Our ﬁrst result concerns small absorbing sets in ﬁnite Euclidean geometries.
Proposition 1. The only nontrivial EG-LDPC code that has a (3, 3) absorbing
set is the code with parity-check matrix HEG(2, 2). There are four such sets in
HEG(2, 2).
Proof. First we show that if a set of three points of EG(m, q) forms an absorbing
set, then m = q = 2. Consider a subset A of three points in a nontrivial ﬁnite
geometry (m, q ≥2). In order for A to be an absorbing set, each point in A
must be on more even lines than odd lines. The number of even lines containing
p ∈A can be at most two, since there are two other points in A, and at best
they each contribute a distinct even line. Thus, the total number of lines that
p can be on is three. There is one ﬁnite Euclidean geometry in which m, q ≥2
and each point is on three lines: EG(2, 2).
Next we will demonstrate a (3, 3) absorbing set in the code constructed from
EG(2, 2) (see Fig. 1). In the Tanner graph for HEG(2, 2) there are four variable
nodes and six check nodes. Since any subset of three variable nodes will form a
(3, 3) absorbing set, there are
4
3

= 4 such sets.
⊓⊔
Next we show that the smallest absorbing sets of HEG(2, 4) are (4, 8) absorb-
ing sets, which are the minimal parameters according to the bound in Lemma 1.
The four points in the absorbing set A must be chosen from two parallel
lines, two on each line, as shown by the X’s in Fig. 2. Then each pair of points

158
K. Haymaker
Fig. 1. The Euclidean geometry EG(2, 2). Any subset of three points forms a (3, 3)
absorbing set of HEG(2, 2).
Fig. 2. The set {A, B, C, D} forms a (4, 8) absorbing set in EG(2, 4). The only ‘lines’
represented are those containing the point A—three are shown by dotted lines. The
fourth line containing A is denoted by open circles and the ﬁfth by diamonds.
has a unique line containing them, which is even with respect to A. Since each
point is on ﬁve lines, with three even and two odd, the set is an absorbing set.
The general approach in [17] for ﬁnding the smallest absorbing sets in
C∗EG(2, q) is to ﬁrst ﬁnd a k-arc, where k is the minimal possible value for
a, k = 2 + ⌊q
2⌋. However, we immediately run into problems with this approach,
as the full geometry EG(2, q) has no such k-arcs for suﬃciently large q.
Lemma 2. When q ≥6, no k-arc exists in EG(2, q) for k ≥
q
2

+ 2.
Proof. Consider a k-arc in EG(2, q), denoted A. Since no three points in A
are collinear, we have that any line through two points in A has all other points
outside of A. Since every pair of points in EG(2, q) has a unique line that contains
those points, there are
|A|
2

lines through pairs of points in A, each of which
contains q −2 points not in A. Therefore there are at least
|A|
2

· (q −2)
points outside of A. When q ≥6, this number is less than qm −|A| only if
k < ⌊q
2⌋+ 2.
⊓⊔
Since the approach of ﬁnding small absorbing sets by starting with k-arcs will
not work for q ≥6, we will use other geometric substructures, such as parallel
bundles of lines. In the following theorem, we see that the parameters of smallest
absorbing sets of HEG(2, 8) are strictly larger than the lower bound given in
Lemma 1. For comparison, in [17] the authors show that the smallest absorbing
sets for both projective geometry codes and the Euclidean subgeometry codes
(in which a point is deleted) all meet the bound in Lemma 1.

Absorbing Set Analysis of Codes from Aﬃne Planes
159
Theorem 1. The smallest absorbing set of HEG(2, 8) has a = 8, b = 32. There
are 9 ·
9
2

·
8
4
2
= 1, 587, 600 distinct smallest absorbing sets in HEG(2, 8).
Proof. By the bound in Lemma 1, the smallest (a, b) absorbing set in CEG(2, 8)
has the following: a ≥6 and b ≥24.
Consider the case when a = 6. By Lemma 2, we have that there is no 6-arc
in EG(2, 8). Thus, any collection of six points must have at least three collinear.
Suppose that A is a subset of the points of EG(2, 8), where |A| = 6. Then there
exists a line L in EG(2, 8) such that 3 ≤|L∩A| ≤6. In the Tanner graph, denote
the subgraph induced by the variable nodes corresponding to A as (A, WA; EA).
There are three subcases:
1. Suppose that |L ∩A| = 3. Then L contains three points of A, so L ∈O(A).
Consider a point p ∈L ∩A. There are nine lines in EG(2, 8) that contain p,
and we claim that at least six of them must be odd with respect to A. Indeed,
since p is on L, there are three other points of A that are not on L, each of
which lies on a line with p. If each of those lines is even, p is still incident to
six odd lines. Thus A is not an absorbing set.
2. Suppose |L ∩A| = 4. Consider a point p ∈L ∩A. The number of even lines
in WA containing p can be at most three—L and the lines containing p and
the points in A\L. Therefore the number of odd lines incident to p is at least
six.
3. Suppose |L ∩A| = 5 or 6. Then for a point p on L, the number of odd lines
containing p in WA is eight.
In all cases above A is not an absorbing set. A similar analysis shows that a
set A of size 7 cannot be an absorbing set.
The existence of an (8, 32) absorbing set in HEG(2, 8) is demonstrated by the
following construction. Consider two parallel lines L1 and L2 in EG(2, 8). Let
the set A consist of four points on L1 and four points on L2. We claim that A is
an absorbing set. Consider a point p ∈L1 ∩A. The number of even lines in WA
that contain p is ﬁve—L1, along with the four unique lines through p and the
points in A ∩L2. Notice that a line containing p and a point q in A ∩L2 cannot
contain any other points in A, since a third point r would have to lie on either
L1 or L2. Suppose r lies on L1. There is a unique line through the points p and
r, so that line would have to be L1. But the set A was chosen so that q /∈L1.
Similar reasoning shows that any point in A is on ﬁve even lines and four
odd lines, and so A is an absorbing set of HEG(2, 8).
The size of O(A) is 32, since all eight points of A are on four distinct odd
lines, and all 32 of these lines are distinct.
Every absorbing set with a = 8 must be arranged as two sets of four points
on parallel lines in order to guarantee that every point in the absorbing set has a
majority of even line neighbors. First choose a parallel bundle in EG(2, 8), then
two lines in that bundle, then four points on each of those lines.
⊓⊔

160
K. Haymaker
Corollary 1. The absorbing set construction in Theorem 1 generalizes to show
the existence of at least (2s + 1) ·
2s + 1
2

·
 2s
2s−1
2
absorbing sets with para-
meters (2s, 22s+1) in HEG(2, 2s).
Proof. Start with two parallel lines in EG(2, 2s). Let A consist of 2s−1 points on
each line. Then A is an absorbing set with the parameters given above.
⊓⊔
In fact, the parallel line construction leads to results on (a, b) absorbing sets
in CEG(2, q) for a range of values of a and q.
Proposition 2. Let a be an even integer, a = 2α.
If a ≡0 (mod 4) and
q
2
	
≤α ≤q, then there are at least (q+1)·
q
2

·
q
α
2
type (a, b) absorbing sets in HEG(2, q).
If a is even but a ̸≡0 (mod 4), and
q
2
	
+ 1 ≤α ≤q, then there are at least
(q + 1) ·
q
2

·
q
α
2
type (a, b) absorbing sets in HEG(2, q).
Proof. Parallel bundles of lines are the basis for this construction of (a, b) absorb-
ing sets. Let a = 2α be an integer such that ⌈q
2⌉≤a ≤2q, where a = ⌈q
2⌉only if
a ≡0 (mod 4). Choose two lines, L1 and L2, from a parallel bundle. Designate
α points on each line, and call this set of points A. Then A forms an absorbing
set with a vertices. Indeed, for a given point p ∈A (say p ∈L1), p is on a
total of q + 1 lines. The lines joining points in A ∩L1 to points in A ∩L2 are
all even, since they each have exactly two points from A on them. The point
p is on α such lines. Moreover, if α is even, the line L1 is even with respect to
A. Therefore any point p is on more even lines than odd, so A is an absorbing
set. By counting the parallel classes and choice of positions for the points in the
absorbing set, we obtain the enumeration in the proposition.
⊓⊔
Conjecture 1. The smallest absorbing set in HEG(2, 2s) has a > 2s−1 + 2, for
s ≥3.
Using the computational system GAP, we have veriﬁed cases showing the
non-existence of an absorbing set with a = 12 or a = 14 for EG(2, 16), which
provides some evidence for the conjecture. Moreover, the construction in Propo-
sition 2 shows that a (2s, 22s−1) absorbing set always exists in HEG(2, 2s).
3.1
Fully Absorbing Sets and Elementary Absorbing Sets
The (4, 8) absorbing set in HEG(2, 4) is a fully absorbing set. Figure 2 shows the
absorbing set points denoted with X’s, and the ﬁve lines through the point A.
Consider a point outside of the absorbing set A, say L. There is a line (not drawn)
containing the points B, C, O, L, which is in E(A). Since L is on ﬁve lines and
only two of them are in O(A), it must be that L is on more lines corresponding

Absorbing Set Analysis of Codes from Aﬃne Planes
161
to check nodes in W \ O(A) than O(A). A similar argument applies to all other
points in V \A. Therefore A is a fully absorbing set. Any absorbing set with a = 4
must have two points on parallel lines, and so the symmetry of the geometry
would yield the same result regardless of the choice of parallel bundle.
The (4, 8) absorbing sets in HEG(2, 4) are also all elementary absorbing sets,
since each line of the geometry contains either 0, 1, or 2 points in A.
However, we now show that codes from ﬁnite Euclidean plane geometries
over larger ﬁelds do not have elementary absorbing sets.
Proposition 3. There are no elementary absorbing sets in HEG(2, q) for q ≥6.
Proof. Recall that an elementary absorbing set given by the subgraph GS =
(S, WS; ES) is an absorbing set in which all vertices in WS have degree one or
two in GS. Lemma 2 shows that for q ≥6 and |S| ≥5, there must be at least
three collinear points in S, so some line corresponds to a check node in WS with
degree at least three. Combined with Lemma 1, which shows that an absorbing
set in HEG(2, q) has |S| ≥2 +
q
2

, we conclude that there are no elementary
absorbing sets in HEG(2, q) for q ≥6.
⊓⊔
Remark 2. The fact that the minimal absorbing sets in codes from the geometries
EG(2, q) have relatively large values of a∗and b∗indicates that these combinato-
rial structures may not impede iterative decoding processes. While knowledge of
the structure of these minimal absorbing sets may not be necessary to optimize
BP or SPA decoding of these codes, the bounds are useful in contributing to the
theoretical basis for the robustness of the error-performance of the codes.
4
Conclusions
We constructed and enumerated certain absorbing sets for codes from ﬁnite aﬃne
planes. We showed that k-arcs do not exist in EG(2, q) for k ≥
q
2

+ 2 when
q ≥6. We enumerated the smallest absorbing sets in HEG(2, 8) and described
their structure, which demonstrates that a sharper lower bound than Lemma 1
on the size of the smallest absorbing sets in HEG(2, q) is possible for this class of
codes. We also gave a general construction and enumeration for absorbing sets in
HEG(2, q) for a range of values of a and q. Ongoing work includes obtaining an
improved lower bound for the size of minimal (a, b) absorbing sets in HEG(2, q),
as well as extending these ideas to codes from geometries with dimension larger
than two.
References
1. Amiri, B., Lin, C.W., Dolecek, L.: Asymptotic distribution of absorbing sets and
fully absorbing sets for regular sparse code ensembles. IEEE Trans. Commun.
61(2), 455–464 (2013)
2. Ball, S., Weiner, Z.: An introduction to ﬁnite geometry. Preprint (2011)

162
K. Haymaker
3. Batten, L.M.: Combinatorics of Finite Geometries, 2nd edn. Cambridge University
Press, Cambridge (1997)
4. Di, C., Proietti, D., Telatar, I.E., Richardson, T.J., Urbanke, R.L.: Finite-length
analysis of low-density parity-check codes on the binary erasure channel. IEEE
Trans. Inform. Theory 48(6), 1570–1579 (2002)
5. Diao, Q., Tai, Y.Y., Lin, S., Abdel-Ghaﬀar, K.: Trapping set structure of LDPC
codes on ﬁnite geometries. In: Information Theory and Applications Workshop
(ITA), pp. 1–8. IEEE (2013)
6. Dolecek, L.: On absorbing sets of structured sparse graph codes. In: Information
Theory and Applications Workshop (ITA), pp. 1–5. IEEE (2010)
7. Dolecek, L., Zhang, Z., Anantharam, V., Wainwright, M., Nikolic, B.: Analysis of
absorbing sets for array-based LDPC codes. In: IEEE International Conference on
Communications, pp. 6261–6268 (2007)
8. Feldman, J., Wainwright, M.J., Karger, D.R.: Using linear programming to decode
binary linear codes. IEEE Trans. Inform. Theory 51(3), 954–972 (2005)
9. Hirschfeld, J.W., Storme, L.: The packing problem in statistics, coding theory and
ﬁnite projective spaces: update 2001. In: Finite Geometries, pp. 201–246. Springer,
US (2001)
10. Johnson, S.J., Weller, S.R.: Codes for iterative decoding from partial geometries.
IEEE Trans. Commun. 52(2), 236–243 (2004)
11. Kelley, C.A., Sridhara, D., Rosenthal, J.: Tree-based construction of LDPC codes
having good pseudocodeword weights. IEEE Trans. Inform. Theory 53(4), 1460–
1478 (2007)
12. Koetter, R., Vontobel, P.O.: Graph covers and iterative decoding of ﬁnite-length
codes. In: Proceedings of the 3rd International Symposium on Turbo Codes and
Related Topics, Brest, France, pp. 75–82 (2003)
13. Kou, Y., Lin, S., Fossorier, M.: Construction of low density parity check codes: a
geometric approach. In: 2nd IEEE International Symposium on Turbo Codes and
Related Topics, Brest, France, pp. 137–140 (2000)
14. Kou, Y., Lin, S., Fossorier, M.: Low density parity-check codes based on ﬁnite
geometries: a rediscovery and new results. IEEE Trans. Inform. Theory 47(7),
2711–2736 (2001)
15. Kyung, G.B., Wang, C.C.: Finding the exhaustive list of small fully absorbing sets
and designing the corresponding low error-ﬂoor decoder. IEEE Trans. Commun.
60(6), 1487–1498 (2012)
16. Landner, S., Milenkovic, O.: Algorithmic and combinatorial analysis of trapping
sets in structured ldpc codes. In: Wireless Networks, Communications, and Mobile
Computing, vol. 1, pp. 630–635. IEEE (2005)
17. Liu, H., Li, Y., Ma, L., Chen, J.: On the smallest absorbing sets of LDPC codes
from ﬁnite planes. IEEE Trans. Inform. Theory 58(6) (2012)
18. Richardson, T.: Error ﬂoors of LDPC codes. In: Proceedings of the Annual Allerton
Conference on Communication, Control, and Computing, vol. 41 (2003)
19. Schwartz, M., Vardy, A.: On the stopping distance and the stopping redundancy
of codes. IEEE Trans. Inform. Theory 52, 922–932 (2006)
20. Tang, H., Xu, J., Lin, S., Abdel-Ghaﬀar, K.A.S.: Codes on ﬁnite geometries. IEEE
Trans. Inform. Theory 51(2), 572–596 (2005)
21. Tanner, R.M.: Explicit concentrators from generalized n-gons. SIAM J. Algebraic
Discrete Methods 5(3), 287–293 (1984)
22. Xia, S.T., Fu, F.W.: On the stopping distance of ﬁnite geometry LDPC codes.
IEEE Commun. Lett. 10(5), 381–383 (2006)

Asymptotic Bounds for the Sizes of Constant
Dimension Codes and an Improved Lower Bound
Daniel Heinlein(B) and Sascha Kurz
University of Bayreuth, Bayreuth, Germany
{Daniel.Heinlein,Sascha.Kurz}@uni-bayreuth.de
Abstract. We study asymptotic lower and upper bounds for the sizes of
constant dimension codes with respect to the subspace or injection dis-
tance, which is used in random linear network coding. In this context we
review known upper bounds and show relations between them. A slightly
improved version of the so-called linkage construction is presented which
is e.g. used to construct constant dimension codes with subspace dis-
tance d = 4, dimension k = 3 of the codewords for all ﬁeld sizes q,
and suﬃciently large dimensions v of the ambient space. It exceeds the
MRD bound, for codes containing a lifted MRD code, by Etzion and
Silberstein.
Keywords: Constant dimension codes · Subspace distance · Injection
distance · Random network coding
1
Introduction
Let V
∼= Fv
q be a v-dimensional vector space over the ﬁnite ﬁeld Fq with
q elements. By
 V
k

we denote the set of all k-dimensional subspaces in V ,
where 0 ≤k ≤v, which has size [ v
k ]q := k
i=1
qv−k+i−1
qi−1
. More general,
the set P(V ) of all subspaces of V forms a metric space with respect to
the subspace distance deﬁned by ds(U, W) = dim(U + W) −dim(U ∩W) =
dim(U) + dim(W) −2 dim(U ∩W), see [32], and the injection distance deﬁned
by di(U, W) = max{dim(U), dim(W)}−dim(U ∩W), see [40]. Coding Theory on
P(V ) is motivated by K¨otter and Kschischang [32] via error correcting random
network coding, see [4]. In this context it is natural to consider codes C ⊆P(V )
where each codeword, i.e., each element of C, has the same dimension k, called
constant dimension code, since this knowledge can be exploited by decoders. For
constant dimension codes we have ds(U, W) = 2di(U, W), so that we will only
consider the subspace distance in this paper. By (v, N, d; k)q we denote a constant
dimension code in V with minimum (subspace) distance d and size N, where the
dimensions of each codeword is k ∈{0, 1, . . . , v}. As usual, a constant dimension
The work was supported by the ICT COST Action IC1104 and grants KU 2430/3-1,
WA 1666/9-1 – “Integer Linear Programming Models for Subspace Codes and Finite
Geometry” – from the German Research Foundation.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 163–191, 2017.
DOI: 10.1007/978-3-319-66278-7 15

164
D. Heinlein and S. Kurz
code C has the minimum distance d, if d ≤ds(U, W) for all U ̸= W ∈C and
equality is attained at least once. If #C = 1, we set the minimum distance to ∞.
The corresponding maximum size is denoted by Aq(v, d; k), where we allow the
minimum distance to be larger than d. The authors of [32] provided lower and
upper bounds for Aq(v, d; k) which are less than a factor of 4 apart. For increas-
ing ﬁeld size q this factor tends to 1. Here, we tighten the corresponding analysis
and end up in a factor of less than 2 for the binary ﬁeld q = 2 and a strictly
better factor for larger values of q. With respect to lower bounds, we slightly gen-
eralize the so-called linkage construction by Gluesing-Luerssen, Troha, Morrison
[21,22] and Silberstein, Trautmann [38]. This improvement then gives the best
known lower bounds for Aq(v, d; k) for many parameters, cf. the online tables
http://subspacecodes.uni-bayreuth.de associated with [23]. For codes contain-
ing a lifted maximum rank distance (lifted MRD) code as a subcode an upper
bound on the size has been presented in [16] for some inﬁnite series of parame-
ters. Codes larger than this MRD bound are very rare. Based on the improved
linkage construction we give an inﬁnite series of such examples.
In this context we mention the following asymptotic result based on the non-
constructive probabilistic method. If the subspace distance d and the dimension
k of the codewords is ﬁxed, then the ratio of the lower and upper bound tends
to 1 as the dimension v of the ambient space approaches inﬁnity, see [18, The-
orem 4.1], which is implied by a more general result of Frankl and R¨odl on
hypergraphs. The same result, with an explicit error term, was also obtained
in [8, Theorem 1]. If d and v −k is ﬁxed we have the same result due to the
orthogonal code. If the parameter k can vary with the dimension v, then our
asymptotic analysis implies there is still a gap of almost 2 between the lower
and the upper bound of the code sizes for d = 4 and k = ⌊v/2⌋, which is the
worst case.
The remaining part of the paper is organized as follows. In Sect. 2 we collect
the basic facts and deﬁnitions for constant dimension codes. Upper bounds on
the achievable code sizes are reviewed in Sect. 3. Here, we extend the current
knowledge on the relation between these bounds. While most of them were known
around 2008, see [2,17,29,32,43], there are some recent improvements for the
subclass of partial spreads, where d = 2k, which we summarize in Subsect. 3.1.
In Sect. 4 we present the mentioned improvement of the linkage construction.
Asymptotic bounds for the ratio between lower and upper bounds for code sizes
are studied in Sect. 5. We continue with the upper bound for constant dimension
codes containing a lifted MRD code in Sect. 6, including some numerical results,
before we draw a short conclusion in Sect. 7.
2
Preliminaries
For the remainder of the paper we set V ∼= Fv
q, where q is a prime power. By v we
denote the dimension of V . Using the language of projective geometry, we will
call the 1-dimensional subspaces of Fv
q points and the 2-dimensional subspaces
lines. First, we observe that the q-binomial coeﬃcient [ v
k ]q indeed gives the

Asymptotic Bounds for the Sizes of Constant Dimension Codes
165
cardinality of
 V
k

. To this end, we associate with a subspace U ∈
 V
k

a unique
k × v matrix XU in row reduced echelon form (rref) having the property that
⟨XU⟩= U and denote the corresponding bijection
τ :

Fv
q
k

→{XU ∈Fk×v
q
| rk(XU) = k, XU is in rref}.
An example is given by XU = ( 1 0 0
0 1 1 ) ∈F2×3
2
, where U = τ −1(XU) ∈

F3
2
2

is a
line that contains the three points (1, 0, 0), (1, 1, 1), and (0, 1, 1). Counting those
matrices gives
#
 V
k

=
k−1

i=0
qv −qi
qk −qi =
k

i=1
qv−k+i −1
qi −1
= [ v
k ]q
for all integers 0 ≤k ≤v. Especially, we have [ v
v ]q = [ v
0 ]q = 1. Given a non-
degenerate bilinear form, we denote by U ⊥the orthogonal subspace of a sub-
space U, which then has dimension v −dim(U). Then, we have ds(U, W) =
ds(U ⊥, W ⊥), so that [ v
k ]q = [
v
v−k ]q. The recurrence relation for the usual bino-
mial coeﬃcients generalize to [ v
k ]q = qk  v−1
k

q +
 v−1
k−1

q. In order to remove the
restriction 0 ≤k ≤v, we set [ a
b ]q = 0 for a ∈N≥0 and b ∈Z, whenever b < 0
or a < b. This extension goes in line with the interpretation of the number of
b-dimensional subspaces of Fa
q and respects the orthogonality relation. In order
to write v−1
j=0 qj = [ v
1 ]q for positive integers q in later formulas, we apply the
deﬁnition of [ v
k ]q also in cases where q is not a prime power and set [ v
k ]1 =
	v
k

for q = 1.
Using the bijection τ we can express the subspace distance between two k-
dimensional subspaces U, W ∈
 V
k

via the rank of a matrix:
ds(U, W) = 2 dim(U + W) −dim(U) −dim(W) = 2

rk

τ(U)
τ(W )

−k

.
(1)
Using
 V
k

as vertex set, we obtain the so-called Grassmann graph, where
two vertices are adjacent iﬀthe corresponding subspaces intersect in a space of
dimension k −1. It is well-known that the Grassmann graph is distance regular.
The injection distance di(U, W) corresponds to the graph distance in the Grass-
mann graph. Considered as an association scheme one speaks of the q-Johnson
scheme.
If C ⊆
 V
k

is a constant dimension code with minimum subspace distance
d, we speak of a (v, #C, d; k) constant dimension code. In the special case of
d = 2k one speaks of so-called partial spreads, i.e., collections of k-dimensional
subspaces with pairwise trivial intersection.
Besides the injection and the subspace distance we will also consider the
Hamming distance dh(u, w) = #{i | ui ̸= wi}, for two vectors u, w ∈Fv
2, and
the rank distance dr(U, W) = rk(U −W), for two matrices U, W ∈Fm×n
q
. The
latter is indeed a metric, as observed in [20]. A subset C ⊆Fm×n
q
is called a
rank metric code. If the minimum rank-distance of C is given by dr, we will also

166
D. Heinlein and S. Kurz
speak of an (m×n, #C, dr)q rank metric code in order to specify its parameters.
A rank metric code C ⊆Fm×n
q
is called linear if C forms a subspace of Fm×n
q
,
which implies that #C has to be a power of the ﬁeld size q.
Theorem 1. (see [20])
Let m, n ≥d be positive integers, q a prime power,
and C ⊆Fm×n
q
be a rank metric code with minimum rank distance d. Then,
#C ≤qmax{n,m}·(min{n,m}−d+1).
Codes attaining this upper bound are called maximum rank distance (MRD)
codes. They exist for all (suitable) choices of parameters, which remains true
if we restrict to linear rank metric codes, see [20]. If m < d or n < d, then
only #C = 1 is possible, which can be achieved by a zero matrix and may be
summarized to the single upper bound #C ≤

qmax{n,m}·(min{n,m}−d+1)
. Using
an m×m identity matrix as a preﬁx one obtains the so-called lifted MRD codes.
Theorem 2. [39, Proposition 4] For positive integers k, d, v with k ≤v, d ≤
2 min{k, v −k}, and d even, the size of a lifted MRD code in
 V
k

with subspace
distance d is given by
M(q, k, v, d) := qmax{k,v−k}·(min{k,v−k}−d/2+1).
If d > 2 min{k, v −k}, then we have M(q, k, v, d) := 1.
The Hamming distance can be used to lower bound the subspace distance
between two codewords (of the same dimension). To this end let p : {M ∈
Fk×v
q
| rk(M) = k, M is in rref} →{x ∈Fv
2 | v
i=1 xi = k} denote the pivot
positions of the matrix in rref. For our example XU we we have p(XU) = (1, 1, 0).
Slightly abusing notation we also write p(U) for subspaces U ∈
 V
k

instead of
p(τ(U)).
Lemma 1. [15, Lemma 2] For two subspaces U, W ≤Fv
q, we have ds(U, W) ≥
dh(p(U), p(W)).
3
Upper Bounds
In this section we review and compare known upper bounds for the sizes of
constant dimension codes. Here we assume that v, d, and k are integers with
2 ≤k ≤v −2, 4 ≤d ≤2 min{k, v −k}, and d even in all subsequent results.
The bound 0 ≤k ≤v just ensures that
 V
k

is non-empty. Note that ds(U, W) ≤
2 min{k, v −k} and ds(U, W) is even for all U, W ∈
 V
k

. Restricting to the set
case, we trivially have Aq(v, d; k) = #
 V
k

= [ v
k ]q for d ≤2 or k ≤1, so that
we assume k ≥2 and d ≥4, which then implies k ≤v −2 and v ≥4. We
remark that some of the latter bounds are also valid for parameters outside the
ranges of non-trivial parameters considered by us. Since the maximum size of a
code with certain parameters is always an integer and some of the latter upper
bounds can produce non-integer values, we may always round them down. To
ease the notation we will commonly omit the ﬁnal rounding step.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
167
The list of known bounds has not changed much since [29], see also [17].
Comparisons of those bounds are scattered among the literature and partially
hidden in comments, see e.g. [6]. Additionally some results turn out to be wrong
or need a reinterpretation at the very least.
Counting k-dimensional subspaces having a large intersection with a ﬁxed
m-dimensional subspace gives:
Lemma 2. For integers 0 ≤t ≤k ≤v and k −t ≤m ≤v we have
#

U ∈
 V
k

| dim(U ∩W) ≥k −t

=
t

i=0
q(m+i−k)i [ m
k−i ]q
 v−m
i

q ,
where W ≤V and dim(W) = m.
Proof. Let us denote dim(U ∩W) by k −i, where max{0, k −m} ≤i ≤min{t,
v −m}. With this, the number of choices for U is given by
	
qm −q0
·
	
qm −q1
· · ·
	
qm −qk−i−1
·
	
qv −qm+1
· · ·
	
qv −qm+i−1
(qk −q0) · (qk −q1) · · · (qk −qk−1)
= [ m
k−i ]q · (qm)i
(qk−i)i ·
 v−m
i

q = q(m+i−k)i [ m
k−i ]q
 v−m
i

q .
Finally apply the convention [ a
b ]q = 0 for integers with b < 0 or b > a.
⊓⊔
Note that dim(U ∩W) ≥k −t is equivalent to ds(U, W) ≤m −k + 2t. The
fact that the Grassmann graph is distance-regular implies:
Theorem 3. (Sphere-packing bound) [32, Theorem 6]
Aq(v, d; k) ≤
[ v
k ]q
⌊(d/2−1)/2⌋

i=0
qi2 [ k
i ]q
 v−k
i

q
We remark, that we can obtain the denominator of the formula of Theorem 3 by
setting m = k, 2t = d/2 −1 in Lemma 2 and applying

k
k−i

q = [ k
i ]q. The right
hand side is symmetric with respect to orthogonal subspaces, i.e., the mapping
k →v −k leaves it invariant.
By deﬁning a puncturing operation one can decrease the dimension of the
ambient space and the codewords. Since the minimum distance decreases by
at most two, we can iteratively puncture d/2 −1 times, so that Aq(v, d; k) ≤

v−d/2+1
k−d/2+1

q =

v−d/2+1
v−k

q since Aq(v′, 2; k′) =
 v′
k′

q. Considering either the
code or its orthogonal code gives:
Theorem 4. (Singleton bound) [32, Theorem 9]
Aq(v, d; k) ≤

v−d/2+1
max{k,v−k}

q

168
D. Heinlein and S. Kurz
Referring to [32] the authors of [29] state that even a relaxation of the Singleton
bound is always stronger than the sphere packing bound for non-trivial codes.
However, for q = 2, v = 8, d = 6, and k = 4, the sphere-packing bound gives
an upper bound of 200787/451 ≈445.20399 while the Singleton bound gives an
upper bound of [ 6
4 ]2 = 651. For q = 2, v = 8, d = 4, and k = 4 it is just the other
way round, i.e., the Singleton bound gives [ 7
3 ]2 = 11811 and the sphere-packing
bound gives [ 8
4 ]2 = 200787. Examples for the latter case are easy to ﬁnd. For
d = 2 both bounds coincide and for d = 4 the Singleton bound is always stronger
than the sphere-packing bound since
 v−1
k

q < [ v
k ]q. The asymptotic bounds [32,
Corollaries 7 and 10], using normalized parameters, and [32, Fig. 1] suggest that
there is only a small range of parameters where the sphere-packing bound can
be superior to the Singleton bound.1
Given an arbitrary metric space X, an anticode of diameter e is a subset
whose elements have pairwise distance at most e. Since the q-Johnson scheme is
an association scheme the Anticode bound of Delsarte [12] can be applied. As a
standalone argument we go along the lines of [2] and consider bounds for codes on
transitive graphs. By double-counting the number of pairs (a, g) ∈A × Aut(Γ),
where g(a) ∈B, we obtain:
Lemma 3. [2, Lemma 1], cf. [3, Theorem 1’] Let Γ = (V, E) be a graph that
admits a transitive group of automorphisms Aut(Γ) and let A, B be arbitrary
subsets of the vertex set V . Then, there exists a group element g ∈Aut(Γ) such
that
|g(A) ∩B|
|B|
≥|A|
|V |.
Corollary 1. [2, Corollary 1], cf. [3, Theorem 1] Let CD ⊆
 V
k

be a code with
(injection or graph) distances from D = {d1, . . . , ds} ⊆{1, . . . , v}. Then, for an
arbitrary subset B ⊆
 V
k

there exists a code C∗
D(B) ⊆B with distances from D
such that
|C∗
D(B)|
|B|
≥|CD|
[ v
k ]q
.
If CD ⊆
 V
k

is a constant dimension code with minimum injection distance
d, i.e., D = {d, . . . , v}, and B is an anticode with diameter d −1, we have
#C∗
D(B) = 1, so that we obtain Delsarte’s Anticode bound
#CD ≤
[ v
k ]q
#B .
(2)
The set of all elements of
 V
k

which contain a ﬁxed (k−d/2+1)-dimensional
subspace is an anticode of diameter d −2 with

v−k+d/2−1
d/2−1

q elements. By
orthogonality, the set of all elements of
 V
k

which are contained in a ﬁxed
(k + d/2 −1)-dimensional subspace is also an anticode of diameter d −2 with
1 By a tedious computation one can check that the sphere-packing bound is strictly
tighter than the Singleton bound iﬀq = 2, v = 2k and d = 6.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
169

k+d/2−1
k

q =

k+d/2−1
d/2−1

q elements. Frankl and Wilson proved in [19, Theo-
rem 1] that these anticodes have the largest possible size, which implies:
Theorem 5. (Anticode bound)
Aq(v, d; k) ≤
[ v
k ]q

max{k,v−k}+d/2−1
d/2−1

q
Using diﬀerent arguments, Theorem 5 was proved in [42, Theorem 5.2] by
Wang, Xing, Safavi-Naini in 2003. Codes that can achieve the (unrounded) value
[ v
k ]q /

max{k,v−k}+d/2−1
d/2−1

q are called Steiner structures. It is a well-known
and seemingly very hard problem to decide whether a Steiner structure for
v = 7, d = 4, and k = 3 exists. For q = 2 the best known bounds are
333 ≤A2(7, 4; 3) ≤381. Additionally it is known that a code attaining the
upper bound can have automorphisms of at most order 2, see [30]. So far, the
only known Steiner structure corresponds to A2(13, 4; 3) = 1597245 [9]. The
reduction to Delsarte’s Anticode bound can be found e.g. in [17, Theorem 1].
Since the sphere underlying the proof of Theorem 3 is also an anticode,
Theorem 3 is implied by Theorem 5. For d = 2 both bounds coincide. In [43,
Sect. 4] Xia and Fu veriﬁed that the Anticode bound is always stronger than the
Singleton bound for the ranges of parameters considered by us.
Mimicking a classical bound of Johnson on binary error-correcting codes with
respect to the Hamming distance, see [28, Theorem 3] and also [41], Xia and Fu
proved:
Theorem 6. (Johnson type bound I) [43, Theorem 2]
If
	
qk −1

2 > (qv −1)
	
qk−d/2 −1

, then
Aq(v, d; k) ≤
	
qk −qk−d/2
(qv −1)
(qk −1)2 −(qv −1)
	
qk−d/2 −1

.
However, the required condition of Theorem 6 is rather restrictive and can be
simpliﬁed considerably.
Proposition 1. For 0 ≤k < v, the bound in Theorem 6 is applicable iﬀd =
2 min{k, v −k} and k ≥1. Then, it is equivalent to
Aq(v, d; k) ≤
qv −1
qmin{k,v−k} −1.
Proof. If k = 0 we have
	
qk −1

2 = 0, so that we assume k ≥1 in the following.
If k ≤v −k and d ≤2k −2, then
(qv−1)

qk−d/2−1

≥
	
q2k−1

(q−1) ≥q2k−1
q≥2,k≥1
>
q2k−2qk+1 =
	
qk−1

2 .

170
D. Heinlein and S. Kurz
If k ≥v −k + 1 and d ≤2v −2k −2, then
(qv−1)

qk−d/2−1

≥(qv−1)
	
q2−1

 q≥2,v≥1
>

q(v+1)/2 −1
2
≥
	
qk−1

2 .
If d = 2 min{k, v −k}, q ≥2, and k ≥1, then it can be easily checked that the
condition of Theorem 6 is satisﬁed and we obtain the proposed formula after
simpliﬁcation.
⊓⊔
For k = v Theorem 6 gives Aq(v, d; v) ≤1 which is trivially satisﬁed with
equality. In Subsect. 3.1 we will provide tighter upper bounds for the special case
where d = 2k, i.e., partial spreads. Indeed, the bound stated in Proposition 1
corresponds to the most trivial upper bounds for partial spreads that is tight
iﬀk divides v, as we will see later on. So, due to orthogonality, Theorem 6 is
dominated by the partial spread bounds discussed later on.
While the previously mentioned generalization of a classical bound of Johnson
on binary error-correcting codes yields the rather weak Theorem 6, generalizing
[28, Inequality (5)] yields a very strong upper bound:
Theorem 7. (Johnson type bound II) [43, Theorem 3], [17, Theorem 4,5]
Aq(v, d; k) ≤qv −1
qk −1Aq(v −1, d; k −1)
(3)
Aq(v, d; k) ≤
qv −1
qv−k −1Aq(v −1, d; k)
(4)
Note that for d = 2k Inequality (3) gives Aq(v, 2k; k) ≤

qv−1
qk−1

since we have
Aq(v −1, 2k; k −1) = 1 by deﬁnition. Similarly, for d = 2(v −k), Inequality (4)
gives Aq(v, 2v −2k; k) ≤

qv−1
qv−k−1

.
Some sources like [43, Theorem 3] list just Inequality 3 and omit Inequality 4.
This goes in line with the treatment of the classical Johnson type bound II for
binary error-correcting codes, see e.g. [35, Theorem 4 on p. 527], where the other
bound is formulated as Problem (2) on page 528 with the hint that ones should
be replaced by zeros. Analogously, we can consider orthogonal codes:
Proposition 2. Inequality (3) and Inequality (4) are equivalent using orthogo-
nality, cf. [17, Sect. 3, esp. Lemma 13].
Proof. We have
Aq(v, d; k) = Aq(v, d; v −k)
(3)
≤
qv −1
qv−k −1Aq(v −1, d; v −k −1)
=
qv −1
qv−k −1Aq(v −1, d; k),
which is Inequality (4), and
Aq(v, d; k) = Aq(v, d; v −k)
(4)
≤qv −1
qk −1Aq(v −1, d; v −k)
= qv −1
qk −1Aq(v −1, d; k −1),

Asymptotic Bounds for the Sizes of Constant Dimension Codes
171
which is Inequality (3).
⊓⊔
Of course, the bounds in Theorem 7 can be applied iteratively. In the classical
Johnson space the optimal choice of the corresponding inequalities is unclear,
see e.g. [35, Research Problem 17.1]. Denoting the maximum size of a binary
constant-weight block code of length n, Hamming distance d and weight k by
A(n, d, w), the two corresponding variants of the inequalities in Theorem 7 are
A(n, d, w) ≤⌊n/w·A(n−1, d, w−1)⌋and A(n, d, w) ≤⌊n/(n−w)·A(n−1, d, w)⌋.
Applying the ﬁrst bound yields
A(28, 8, 13) ≤⌊28/13 · A(27, 8, 12)⌋≤⌊28/13 · 10547⌋= 22716
while applying the second bound yields
A(28, 8, 13) ≤⌊28/15 · A(27, 8, 13)⌋≤⌊28/15 · 11981⌋= 22364
using the numerical bounds from
http://webﬁles.portal.chalmers.se/s2/research/kit/bounds/cw.html, cf. [1].
The authors of [17,29] state that the optimal choice of Inequality (3) or Inequal-
ity (4) is unclear, too. However, this question is much easier to answer for con-
stant dimension codes.
Proposition 3. For k ≤v/2 we have
qv −1
qk −1Aq(v −1, d; k −1)

≤
 qv −1
qv−k −1Aq(v −1, d; k)

,
where equality holds iﬀv = 2k.
Proof. By considering orthogonal codes we obtain equality for v = 2k. Now we
assume k < v/2 and show
qv −1
qk −1Aq(v −1, d; k −1) + 1 ≤
qv −1
qv−k −1Aq(v −1, d; k),
(5)
which implies the proposed statement. Considering the size of the lifted MRD
code we can lower bound the right hand side of Inequality (5) to
qv −1
qv−k −1Aq(v −1, d; k) ≥qv −1
qv−k · q(v−k−1)(k−d/2+1).
Since
 v−1
k−1

q

v−k+d/2−1
d/2−1

q
=
k−1

i=1
qv−k+i−1
qi−1
d/2−1

i=1
qv−k+i−1
qi−1
≤
k−1

i=d/2
qv−k+i
qi −1 = q(v−k)(k−d/2)
k−1

i=d/2
1
1 −q−i

172
D. Heinlein and S. Kurz
we can use the Anticode bound to upper bound the left hand side of Inequality (5)
to
qv −1
qk −1Aq(v −1, d; k −1) + 1 ≤qv −1
qk −1 · q(v−k)(k−d/2) · μ(k −1, d/2, q) + 1,
where μ(a, b, q) :=
a
i=b
	
1 −q−i
−1. Thus, it suﬃces to verify
qk−d/2+1
qk −1
· μ(k −1, d/2, q) + 1
f ≤1,
(6)
where we have divided by
f := qv −1
qv−k · q(v−k−1)(k−d/2+1) = qv −1
q
· q(v−k−1)(k−d/2).
Since d ≥4, we have μ(k−1, d/2, q) ≤
∞

i=2
	
1 −q−i
−1 ≤
∞

i=2
	
1 −2−i
−1 < 1.74.
Since v ≥4 and q ≥2, we have 1
f ≤
2
15. Since k ≥2, we have qk−d/2+1
qk−1
≤
q
q2−1,
which is at most 3
8 for q ≥3. Thus, Inequality (6) is valid for all q ≥3.
If d ≥6 and q = 2, then μ(k −1, d/2, q) ≤
∞

i=3
	
1 −2−i
−1 < 1.31 and
qk−d/2+1
qk−1
≤1
3, so that Inequality (6) is satisﬁed.
In the remaining part of the proof we assume d = 4 and q = 2. If k = 2, then
μ(k −1, d/2, q) = 1 and qk−d/2+1
qk−1
= 2
3. If k = 3, then μ(k −1, d/2, q) = 4
3 and
qk−d/2+1
qk−1
= 4
7. If k ≥4, then qk−d/2+1
qk−1
≤
8
15, μ(k −1, d/2, q) ≤1.74, and 1
f ≤
2
255
due to v ≥2k ≥8. Thus, Inequality (6) is valid in all cases.
⊓⊔
Knowing the optimal choice between Inequality (3) and Inequality (4), we
can iteratively apply Theorem 7 in an ideal way initially assuming k ≤v/2:
Corollary 2. (Implication of the Johnson type bound II)
Aq(v, d; k) ≤
qv−1
qk−1
qv−1−1
qk−1−1

. . .
qv−k+d/2+1−1
qd/2+1−1
Aq(v−k+d/2, d; d/2)

. . .

We remark that this upper bound is commonly stated in an explicit version,
where Aq(v−k+d/2, d; d/2) ≤

qv−k+d/2−1
qd/2−1

is inserted, see e.g. [17, Theorem 6],
[29, Theorem 7], and [43, Corollary 3]. However, currently much better bounds
for partial spreads are available.
It is shown in [43] that the Johnson bound of Theorem 7 improves on the
Anticode bound in Theorem 5, see also [6]. To be more precise, removing the
ﬂoors in the upper bound of Corollary 2 and replacing Aq(v −k + d/2, d; d/2)
by qv−k+d/2−1
qd/2−1
gives
k−d/2

i=0
qv−i −1
qk−i −1 =
k−1
i=0
qv−i−1
qk−i−1
k−1
i=k−d/2+1
qv−i−1
qk−i−1
=
[ v
k ]q

v−k+d/2−1
d/2−1

q
,

Asymptotic Bounds for the Sizes of Constant Dimension Codes
173
which is the right hand side of the Anticode bound for k ≤v −k. So, all upper
bounds mentioned so far are (weakly) dominated by Corollary 2, if we addition-
ally assume k ≤v −k. As a possible improvement [2, Theorem 3] was mentioned
as [29, Theorem 8]. Here, we correct typos and give a slightly enlarged proof,
thanks to a personal communication with Aydinian.
Theorem 8. [2, Theorem 3] For integers 0 ≤t < r ≤k, k −t ≤m ≤v, and
t ≤v −m we have
Aq(v, 2r; k) ≤
[ v
k ]q Aq(m, 2r −2t; k −t)
t
i=0 qi(m+i−k) [ m
k−i ]q
 v−m
i

q
.
Proof. Let W be a ﬁxed subspace with dim(W) = m and deﬁne
B =

U ∈
 V
k

| dim(U ∩W) ≥k −t

,
so that #B is given by Lemma 2. Consider a (v, #C∗, d; k) code C∗⊆B and take
C′ := C∗∩W noting that the latter has a minimum distance of at least 2r −2t.
Two arbitrary codewords U1 ̸= U2 ∈C′ have distance ds(U1, U2) ≥2r−2t+i+j,
where we write dim(U1) = k−t+i and dim(U2) = k−t+j for integers 0 ≤i, j ≤t.
Replacing each codeword of C′ by an arbitrary k −t-dimensional subspace, we
obtain a constant dimension code C with a minimum distance of at least 2r −2t.
Since t < r we have #C∗= #C′ = #C, so that Corollary 1 gives the proposed
upper bound.
⊓⊔
As Theorem 8 has quite some degrees of freedom, we partially discuss the opti-
mal choice of parameters. For t = 0 and m ≤v −1, we obtain Aq(v, d; k) ≤
[ v
k ]q / [ m
k ]q · Aq(m, d; k), which is the (v −m)-fold iteration of Inequality (4) of
the Johnson bound (without rounding). Thus, m = v −1 is the best choice
for t = 0, yielding a bound that is equivalent to Inequality (4). For t = 1 and
m = v −1 the bound can be rewritten to Aq(v, d; k) ≤Aq(v −1, d −2; k −1),
see the proof of Proposition 4. For t > v −m the bound remains valid but is
strictly weaker than for t = v −m. Choosing m = v gives the trivial bound
Aq(v, 2r; k) ≤Aq(m, 2r −2t; k −t). For the range of parameters 2 ≤q ≤9,
4 ≤v ≤100, limited facing numerical pitfalls, and 4 ≤d ≤2k ≤v, where q is of
course a prime power and d is even, the situation is as follows. If d ̸= 2k, there
are no proper improvements with respect to Theorem 7. For the case d = 2k, i.e.,
partial spreads treated in the next subsection, we have some improvements com-
pared to ⌊(qv −1)/(qk −1)⌋which is the most trivial bound for partial spreads.
Within our numerical range, most of them are covered by the following proposi-
tion, where we apply Theorem 8 with t = 1 and m = v −1 to Aq(v, 2k; k). The
other cases are due to the fact that Theorem 14 is tighter than Theorem 16 for
larger values of z. In no case a proper improvement with respect to the tighter
bounds from the next subsection emerged.
Proposition 4. For w ≥1 and k ≥qw + 3 we have Aq(2k + w, 2k; k) ≤
⎢⎢⎢⎣
 2k+w
k

q Aq(2k + w −1, 2k −2; k −1)
1
i=0 qi(k+w−1+i)  2k+w−1
k−i

q
 (2k+w)−(2k+w−1)
i

q
⎥⎥⎥⎦<
q2k+w −1
qk −1

= qk+w+qw

174
D. Heinlein and S. Kurz
Proof. Note that k ≥qw + 3 implies w < k. The left hand side simpliﬁes to
 2k+w
k

q Aq(2k + w −1, 2k −2; k −1)
1
i=0 qi(k+w−1+i)  2k+w−1
k−i

q
 (2k+w)−(2k+w−1)
i

q
= Aq(2k+w−1, 2k−2; k−1).
Then we apply Theorem 16 with t = 2, r = w + 1, and z = [ w
1 ]q −1, which
yields Aq(2k + w −1, 2k −2; k −1) ≤qk+w + 1 + qw −q < qk+w + qw for k −1 ≥
qw + 2.
⊓⊔
We remark that applying Theorems 14 and 16 directly is at least as good as the
application of Theorem 8 with t = 1 and m = v −1 for d = 2k.
The Delsarte linear programming bound for the q-Johnson scheme was
obtained in [11]. However, numerical computations indicate that it is not better
than the Anticode bound, see [6]. For d ̸= 2 min{k, v −k}, i.e., the non-partial
spread case, besides the stated bound only the following two speciﬁc bounds,
based on extensive computer calculations, are known:
Theorem 9. [26, Theorem 1] A2(6, 4; 3) = 77
Proposition 5. [24] A2(8, 6; 4) ≤272
As the authors of [24] have observed, the Johnson bound of Theorem 7 does not
improve upon Corollary 2 when applied to Theorem 9 or Proposition 5.
If we additionally restrict ourselves to constant dimension codes, that contain
a lifted MRD code, another upper bound is known:
Theorem 10. [16, Theorem 10 and 11] Let C ⊆

Fv
q
k

be a constant dimension
code, with v ≥2k and minimum subspace distance d, that contains a lifted MRD
code.
– If d = 2(k −1) and k ≥3, then #C ≤q2(v−k) + Aq(v −k, 2(k −2); k −1);
– if d = k, where k is even, then #C ≤q(v−k)(k/2+1) +

v−k
k/2

q
qv−qv−k
qk−qk/2 +
Aq(v −k, k; k).
3.1
Upper Bounds for Partial Spreads
The case of constant dimension codes with maximum possible subspace dis-
tance d = 2k is known under the name partial spreads. Counting points,
i.e., 1-dimensional subspaces, in Fv
q and Fk
q gives the obvious upper bound
Aq(v, 2k; k) ≤[ v
1 ]q / [ k
1 ]q = (qv −1) /
	
qk −1

. In the case of equality one speaks
of spreads, for which a handy existence criterion is known from the work of Segre
in 1964.
Theorem 11. [37, Sect. 6]
Fv
q contains a spread if and only if k is a divisor
of v.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
175
If k is not a divisor of v, far better bounds are known including some recent
improvements, which we will brieﬂy summarize. For a more detailed treatment
we refer to e.g. [27]. The best known parametric construction was given by
Beutelspacher in 1975:
Theorem 12. [7]
For positive integers v, k satisfying v = tk + r, t ≥2 and
1 ≤r ≤k −1 we have Aq(v, 2k; k) ≥1 + t−1
i=1 qik+r =
qv−qk+r+qk−1
qk−1
with
equality for r = 1.
The determination of A2(v, 6; 3) for v ≡2 (mod 3) was achieved more than
30 years later in [14] and continued to A2(v, 2k; k) for v ≡2 (mod k) and arbi-
trary k in [33]. Besides the parameters of A2(8+3l, 6; 3), for l ≥0, see [14] for an
example showing A2(8, 6; 3) ≥34, no partial spreads exceeding the lower bound
from Theorem 12 are known.
For a long time the best known upper bound on Aq(v, 2k; k) was the one
obtained by Drake and Freeman in 1979:
Theorem 13. [13, Corollary 8] If v = kt + r with 0 < r < k, then
Aq(v, 2k; k) ≤
t−1

i=0
qik+r −⌊θ⌋−1 = qr · qkt −1
qk −1 −⌊θ⌋−1,
where 2θ =

1 + 4qk(qk −qr) −(2qk −2qr + 1).
Quite recently this bound has been generalized to:
Theorem 14. [34, Theorem 2.10]
For integers r ≥1, t ≥2, y ≥max{r, 2},
z ≥0 with λ = qy, y ≤k, k = [ r
1 ]q + 1 −z > r, v = kt + r, and l = qv−k−qr
qk−1 , we
have Aq(v, 2k; k) ≤lqk +

λ −1
2 −1
2

1 + 4λ (λ −(z + y −1)(q −1) −1)

.
The construction of Theorem 12 is asymptotically optimal for k ≫r =
v mod k, as recently shown by N˘astase and Sissokho:
Theorem 15. [36, Theorem 5] Suppose v = tk + r with t ≥1 and 0 < r < k.
If k > [ r
1 ]q then Aq(v, 2k; k) = 1 + t−1
i=1 qik+r = qv−qk+r+qk−1
qk−1
.
Applying similar techniques, the result was generalized to k ≤[ r
1 ]q:
Theorem 16. [34, Theorem 2.9] For integers r ≥1, t ≥2, u ≥0, and 0 ≤z ≤
[ r
1 ]q /2 with k = [ r
1 ]q + 1 −z + u > r we have Aq(v, 2k; k) ≤lqk + 1 + z(q −1),
where l = qv−k−qr
qk−1
and v = kt + r.
Using Theorem 14 the restriction z ≤[ r
1 ]q /2 can be removed from Theorem 16,
see [27].
Currently, Theorems 11, 14, and 16 constitute the tightest parametric bounds
for Aq(v, 2k; k). The only known improvements, by exactly one in every case,
are given by the 21 speciﬁc bounds stated in [34], which are based on the linear
programming method applied to projective qk−1-divisible linear error-correcting

176
D. Heinlein and S. Kurz
codes over Fq with respect to the Hamming distance, see [27]. As this connection
seemed to be overlooked before, it may not be improbable that more sophisti-
cated methods from classical coding theory can improve further values, which
then imply improved upper bounds for constant dimension codes via the Johnson
bound of Theorem 7.
4
The Linkage Construction Revisited
A very eﬀective and widely applicable construction of constant dimension codes
was stated by Gluesing-Luerssen and Troha:
Theorem 17. [22, Theorem 2.3], cf. [38, Corollary 39]
Let Ci
be a
(vi, Ni, di; k)q constant dimension code for i = 1, 2 and let Cr be a (k ×
v2, Nr, dr)q linear rank metric code. Then
{τ −1(τ(U) | M) : U ∈C1, M ∈Cr} ∪{τ −1(0k×v1|τ(W)) : W ∈C2}
is a (v1 + v2, N1NR + N2, min{d1, d2, 2dr}; k)q constant dimension code.
Here A|B denotes the concatenation of two matrices with the same number of
rows and 0m×n denotes the m×n-matrix consisting entirely of zeros. The result-
ing code depends on the choice of the codes C1, C2, Cr and their representatives
within isomorphism classes, so that one typically obtains many isomorphism
classes of codes with the same parameters.
We remark that [38, Theorem 37] corresponds to the weakened version of
Theorem 17 where the codewords from the constant dimension code C2 are not
taken into account, cf. [21, Theorem 5.1]. In [38, Corollary 39] Silberstein and
(Horlemann-)Trautmann obtain the same lower bound, assuming d1 = d2 = 2dr,
which is indeed the optimal choice, and 3k ≤v.2
The main idea behind Theorem 17 is to consider two sets of codewords with
disjoint pivot vectors across the two sets and to utilize the interplay between the
rank and the subspace distance for a product type construction. Using Lemma 1
the restriction of the disjointness of the pivot vectors can be weakened, which
gives the following improvement:
Theorem 18. Let Ci be a (vi, Ni, di; k)q constant dimension code for i = 1, 2,
d ∈2N≥0 and let Cr be a (k × (v2 −k + d/2), Nr, dr)q linear rank metric code.
Then
C = {τ −1(τ(U) | M) : U ∈C1, M ∈Cr} ∪{τ −1(0k×(v1−k+d/2)|τ(W)) : W ∈C2}
is a (v1 + v2 −k + d/2, N1NR + N2, min{d1, d2, 2dr, d}; k)q constant dimension
code.
2 It can be veriﬁed that for 2k ≤v ≤3k−1 the optimal choice of Δ in [38, Corollary39]
is given by Δ = v −k. In that case the construction is essentially the union of a
lifted MRD code with an (v −k, #C′, d; k)q code C′. Note that for v −k < Δ ≤v
the constructed code is an embedded (Δ, #C′, d; k)q code C′.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
177
Proof. The dimension of the ambient space and the codewords of C directly
follow from the construction. Since the constructed matrices all are in rref and
pairwise distinct, C is well deﬁned and we have #C = N1NR + N2. It remains to
lower bound the minimum subspace distance of C.
Let A, C ∈C1 and B, D ∈Cr. If A ̸= C, we have
ds(τ −1((τ(A) | B)), τ −1((τ(C) | D))) = 2

rk

τ(A) B
τ(C) D

−k

≥2

rk

τ(A)
τ(C)

−k

= ds(A, C) ≥d1
using Eq. (1) in the ﬁrst step. If A = C but B ̸= D, we have
ds(τ −1((τ(A) | B)), τ −1((τ(C) | D))) = 2

rk

τ(A) B
τ(C) D

−k

≥2

rk
τ(A)
B
0
D −B

−k

= 2(k + rk(D −B) −k) ≥2dr.
For A′ ̸= C′ ∈C2 applying Eq. (1) gives
ds(τ −1(0k×(v1−k+d/2) | τ(A′)), τ −1(0k×(v1−k+d/2) | τ(C′))) = ds(A′, C′) ≥d2.
Finally, for two codewords U ∈{τ −1(τ(U) | M) | U ∈C1, M ∈Cr} and
W ∈{τ −1(0k×(v1−k+d/2) | τ(W)) | W ∈C2}, we can use the shape of the
pivot vectors and apply Lemma 1. The pivot vector p(U) has its k ones in
the ﬁrst v1 positions and the pivot vector p(W) has its k ones not in the ﬁrst
v1 −k + d/2 positions, so that the ones can coincide at most at the positions
{v1−k+d/2+1, . . . , v1}. Thus, dh(p(U), p(W)) ≥k−(k−d/2)+k−(k−d/2) = d.
Lemma 1 then gives ds(U, W) ≥d.
⊓⊔
An example where Theorem 18 yields a larger code than Theorem 17 is
e.g. given for the parameters q = 2, v = 7, d = 4, and k = 3. In order to apply
Theorem 17 we have to choose v1+v2 = 7, 3 ≤v1 ≤4, and 3 ≤v2 ≤4. For v1 = 3
we obtain #C1 ≤A2(3, 4; 3) = 1 and #C2 ≤A2(4, 4; 3) = 1. Since the size of the
rank metric code is bounded by

24(3−2+1)
= 28, the constructed code has a size
of at most 1 · 28 + 1 = 257. For v1 = 4 the roles of C1 and C2 interchange. Since
the size of the rank metric code is bounded by

23(3−2+1)
= 26, the constructed
code has a size of at most 1·26 +1 = 65. In Theorem 18 we can choose d = 4, so
that we can drop one column of the zero matrix preceding the matrices of the
second set of codewords, i.e., v1 + v2 = 7 + 1 = 8. Choosing v1 = 3 and v2 = 5
we can achieve #C1 = A2(3, 4; 3) = 1 and #C2 = A2(5, 4; 3) = 9. Since the size
of the rank metric code can attain

24(3−2+1)
= 28 we can construct a code of
size 1 · 28 + 9 = 265. While for these parameters sill larger codes are known, the
situation signiﬁcantly changes in general. Considering the range of parameters
2 ≤q ≤9, 4 ≤v ≤19, and 4 ≤d ≤2k ≤v, where q is of course a prime power
and d is even, Theorem 17 provides the best known lower bound for Aq(v, d; k)
in 41.8% of the cases, while Theorem 18 provides the best known lower bound in

178
D. Heinlein and S. Kurz
65.6% of the cases. Since the sizes of both constructions can coincide, the sum of
both fractions gives more than 100%. In just 34.4% of the cases strictly superior
constructions are known compared to Theorem 18, where most of them arose
from the so-called Echelon-Ferrers construction or one of their variants, see [23]
and the corresponding webpage.3
If one is interested in codes of large size, then one should choose the parame-
ters d1, d2, dr, and d, in Theorem 18, as small as possible in order to maximize
the sizes N1, N2, and Nr, i.e., we can assume d1 = d2 = 2dr = d. Moreover, the
codes C1, C2, and Cr should have the maximum possible size with respect to their
speciﬁed parameters. For Cr the maximum possible size is M(q, k, v2 + d/2, d)
and for Ci the maximum possible size is Aq(v1, d; k), where i = 1, 2.
Corollary 3. For positive integers k ≤min{v1, v2} and d ≡0 (mod 2) we have
Aq(v1 + v2 −k + d/2, d; k) ≥Aq(v1, d; k) · M(q, k, v2 + d/2, d) + Aq(v2, d; k).
Instead of Aq(v1, d; k) or Aq(v2, d; k) we may also insert any lower bound of these
commonly unknown values. By a variable transformation we obtain:
Corollary 4. For positive integers k ≤m ≤v −d/2 and d ≡0 (mod 2) we
have Aq(v, d; k) ≥Aq(m, d; k) · M(q, k, v −m + k, d) + Aq(v −m + k −d/2, d; k).
For the parameters of spreads the optimal choice of the parameter m in
Corollary 4 can be determined analytically:
Lemma 4. If d = 2k and k divides v, then Corollary 4 gives Aq(v, d; k) ≥qv−1
qk−1
for all m = k, 2k, . . . , v −k and smaller sizes otherwise.
Proof. Using Aq(v′, 2k; k) = (qv′ −1)/(qk −1) for all integers v′ being divisible
by k, we obtain
Aq(v, d; k) ≥Aq(m, d; k) · M(q, k, v −m + k, 2k) + Aq(v −m, 2k; k)
= qm −1
qk −1 · qv−m + qv−m −1
qk −1
= qv −1
qk −1
if k divides m. Otherwise, Aq(m, 2k; k) ≤(qm −1)/(qk −1) −1 gives a lower
bound.
⊓⊔
We remark that the tightest implications of Corollary 4 can be evaluated by
dynamic programming. To this end we consider ﬁxed parameters q, d, k and use
the abbreviations a(n) := Aq(n, d; k) and b(n) := M(q, k, n + k, d) for integers
n, so that the inequality of Corollary 4 reads
a(v) ≥a(m) · b(v −m) + a(v −m + k −d/2).
(7)
For a given maximal value v we initialize the values a(n) for 1 ≤n ≤v by the
best known lower bounds for Aq(n, d; k) from other constructions. Then we loop
over n from k to v and eventually replace a(n) by
max{a(m) · b(n −m) + a(n −m + k −d/2) | k ≤m ≤n −d/2}.
3 Entries of type improved linkage(m) correspond to Corollary 4 with m chosen as
parameter.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
179
By an arithmetic progression we can use (7) in order to obtain a lower bound
for a(v) = Aq(v, d; k) given just two initial a(i)-values.
Proposition 6. For positive integers k ≤v0, 2s ≥d, and l ≥0, we have
a(v0 + ls) ≥a(v0) · b(s)l + a(s −d/2 + k) [ l
1 ]b(s) .
If additionally, v0 ≥2k −d/2 and k ≥d/2, then we have
a(v0 + ls) ≥a(s + k −d/2) · (qk−d/2+1)
n0−k+d/2 [ l
1 ]qs(k−d/2+1) + a(v0).
Proof. Using Inequality (7) with v = v0 + ls and m = v0 + (l −1)s gives
a(v0 + ls) ≥a(v0 + (l −1)s) · b(s) + a(s + k −d/2).
By induction, we obtain
a(v0 + ls) ≥a(v0 + (l −i)s) · b(s)i + a(s + k −d/2) [ i
1 ]b(s)
for all 0 ≤i ≤l.
For the second part, applying Inequality (7) with v = v0 + ls and m =
s + k −d/2 gives
a(v0 + ls) ≥a(s + k −d/2) · b(v0 + (l −1)s −k + d/2) + a(v0 + (l −1)s).
By induction, we obtain
a(v0 + ls) ≥a(s + k −d/2) ·
i

j=1
b(v0 + (l −j)s −k + d/2) + a(v0 + (l −i)s)
for all 0 ≤i ≤l.
If v0 ≥2k −d/2 and k ≥d/2, then
b(v0 + (l −j)s −k + d/2) = (qk−d/2+1)
v0+(l−j)s−k+d/2,
so that
l

j=1
b(v0 + (l −j)s −k + d/2) =
l

j=1
(qk−d/2+1)
v0+(l−j)s−k+d/2
= (qk−d/2+1)
v0−k+d/2 l−1

r=0
(qs(k−d/2+1))
r = (qk−d/2+1)
v0−k+d/2 [ l
1 ]qs(k−d/2+1) .
⊓⊔
Example 1. Using A2(13, 4; 3) = 1597245 [9] and A2(7, 4; 3) ≥333 [23], applying
Proposition 6 with s = 6 gives
A2(13 + 6l, 4; 3) ≥4096l · 1597245 + 333 · 4096l −1
4095

180
D. Heinlein and S. Kurz
and
A2(13 + 6l, 4; 3) ≥333 · 16777216 · 4096l −1
4095
+ 1597245
for all l ≥0.
In the next section we will see that the ﬁrst lower bound almost meets the
Anticode bound.
We remark that Theorem 18 can be easily generalized to a construction based
on a union of m ≥2 sets of codewords.
Corollary 5. For positive integers k, m, and i = 1, . . . , m let
– Ci be an (vi, Ni, di; k)q constant dimension code,
– δi ∈N≥0, δm = 0,
– CR
i be a (k×vR
i , N R
i , dR
i )q linear rank metric code, where vR
i = i−1
j=1(vj −δj)
and i ̸= 1,
– CR
1 = ∅, vR
1 = 0, N R
1 = 1, and dR
1 = ∞.
Then
m

i=1
{τ −1(0k×(v−vi−vR
i ) | τ(Ui) | Mi) : Ui ∈Ci, Mi ∈CR
i }
is a (v, N, d; k)q constant dimension code with
– v = m
i=1(vi −δi),
– N = m
i=1 Ni · N R
i , and
– d = min{di, 2dR
i , 2(k −δi) | i = 1, . . . , m}.
Proof. We prove by inductively applying Theorem 18 m −1 times. Denote
˜Ci := {τ −1(0k×(v−vi−vR
i ) | τ(Ui) | Mi) : Ui ∈Ci, Mi ∈CR
i }
for i = 1, . . . , m, i.e., ˜Ci is a padded (vi +vR
i , Ni ·N R
i , min{di, 2dR
i }; k)q constant
dimension code. Applying Theorem 18 for ˜C1 and ˜C2 with d = 2(k −δ1) yields
a (v1 + v2 −δ1, N1 + N2 · N R
2 , min{d1, d2, 2dR
2 , 2(k −δ1)}; k)q constant dimension
code. If the ﬁrst m′ codes, ˜C1, . . . , ˜Cm′ yield an (m′
i=1(vi −δi) + δm′, m′
i=1 Ni ·
N R
i , min{di, 2dR
i , 2(k−δi) | i = 1, . . . , m′}; k)q constant dimension code ˜C1,...,m′,
then performing Theorem 18 for this code and ˜Cm′+1 with d = 2(k −δm′) yields
an (m′
i=1(vi−δi)+δm′+n2−δm′, m′
i=1 Ni·N R
i +Nm′+1·N R
m′+1, min{di, 2dR
i , 2(k−
δi) | i = 1, . . . , m′ + 1}; k)q constant dimension code.
⊓⊔
Since the proof uses multiple applications of Theorem 18 this code can be
found by the dynamic programming approach based on Theorem 18, i.e., Corol-
lary 5 is redundant. However, it can be used to prove:

Asymptotic Bounds for the Sizes of Constant Dimension Codes
181
Corollary 6 (cf. [22, Theorem 4.6]). Let CR be an (k × v1 + v2, d)q linear
MRD code, where k ≤vi, for i = 1, 2 and let Ci be an (vi−2, Ni, 2d; k)q constant
dimension codes for i = 3, 4. Then
{τ −1(Ik×k | A) | A ∈CR}
∪{τ −1(0k×k | τ(A) | 0k×v2) | A ∈C3}
∪{τ −1(0k×k | 0k×v1 | τ(A)) | A ∈C4}
is a (v1 + v2 + k, q(v1+v2)(k−d+1) + N3 + N4, 2 min{d, k}; k)q constant dimension
code. Note that k < d implies N3, N4 ≤1.
Proof. Applying Corollary 5 with
– m = 3
– ¯C1 = C4, ¯C2 = C3,
– ¯C3 = {τ −1(Ik×k)} (i.e., an (k, 1, ∞; k)q constant dimension code)
– δ1 = δ2 = δ3 = 0
– ¯CR
1 = ∅
– ¯CR
2 = {0k×v2} (i.e., an (k × v2, 1, ∞)q rank metric code)
– ¯CR
3 an (k × (v1 + v2, d))q MRD code
yields an (v1 +v2 +k, N4 +N3 +q(v1+v2)(k−d+1), 2 min{d, k}; k)q constant dimen-
sion code:
{τ −1(Ik×k | M3) : M3 ∈¯CR
3 }
∪{τ −1(0k×k | τ(U2) | 0k×v2) : U2 ∈C3}
∪{τ −1(0k×(v1+k) | τ(U1)) : U1 ∈C4}
⊓⊔
We remark that {(A | B) : A ∈CR
1 , B ∈CR
2 } is a (k × (v1 + v2), d)q linear
MRD code, since each codeword has rk(A | B) ≥rk(A) ≥d. The other direction
is not necessarily true, e.g.,
	 Ik−1
0
| 0 | . . . | 0 | w

, where w is a non-zero column,
cannot be split in two matrices
	 Ik−1
0
| 0 | . . . | 0

and (0 | . . . | 0 | w) both having
rank distance at least d for d ≥2. Hence, this corollary constructs codes of the
same size as Theorem 4.6 in [22] but these codes are not necessarily equal.
5
Asymptotic Bounds
K¨otter and Kschischang have stated the bounds
1 < q−k(v−k) · [ v
k ]q < 4
for 0 < k < v in [32, Lemma 4] for the q-binomial coeﬃcients. They used this
result in order to prove that the lifted MRD codes, they spoke about linearized
polynomials, have at least a size of a quarter of the Singleton bound if v tends

182
D. Heinlein and S. Kurz
to inﬁnity. Actually, they have derived a more reﬁned bound, which can be best
expressed using the so called q-Pochhammer symbol (a; q)n := n−1
i=0
	
1 −aqi
specializing to (1/q; 1/q)n = n
i=1
	
1 −1/qi
:
1 ≤
[ v
k ]q
qk(v−k) ≤
1
(1/q; 1/q)k
≤
1
(1/q; 1/q)∞
≤
1
(1/2; 1/2)∞
≈3.4627,
(8)
where (1/q; 1/q)∞denotes limn→∞(1/q; 1/q)n, cf. the estimation for the Anti-
code bound in the proof of Proposition 3. The sequence (1/q; 1/q)∞is monoton-
ically increasing with q and approaches (q −1)/q for large q, see e.g. [29,32] for
some numerical values.
Lemma 5. For each b ∈N≥0 we have lim
a→∞
 a+b
b

q
qab
=
1
(1/q;1/q)b .
Proof. Plugging in the deﬁnition of the q-binomial coeﬃcient, we obtain
lim
a→∞
 a+b
b

q
qab
= lim
a→∞
b
i=1
qa+i−1
qi−1
qab
=
b

i=1
qi
qi −1 =
b

i=1
1
1 −1/qi =
1
(1/q; 1/q)b
.
⊓⊔
Using this asymptotic result we can compare the size of the lifted MRD codes
to the Singleton and the Anticode bound.
Proposition 7. For k ≤v −k the ratio of the size of a lifted MRD code divided
by the size of the Singleton bound converges for v →∞monotonically decreasing
to (1/q; 1/q)k−d/2+1 ≥(1/2; 1/2)∞> 0.288788.
Proof. Setting z = k −d/2 + 1 and s = v −k the ratio is given by g(s) :=
qsz
[ s+z
z ]q
, so that Lemma 5 gives the proposed limit. The sequence is monotonically
decreasing, since we have 0 ≤z −1 < z ≤s + z and
g(s)
g(s + 1) =
qsz [ s+1+z
z
]q
[ s+z
z ]q q(s+1)z =
qz [ s+z
z ]q +
 s+z
z−1

q
[ s+z
z ]q qz
= 1 +
 s+z
z−1

q
[ s+z
z ]q qz > 1.
⊓⊔
Proposition 8. For k ≤v −k the ratio of the size of a lifted MRD code divided
by the size of the Anticode bound converges for v →∞monotonically decreasing
to
(1/q;1/q)k
(1/q;1/q)d/2−1 ≥
q
q−1 · (1/q; 1/q)k ≥2 · (1/2; 1/2)∞> 0.577576.
Proof. The lifted MRD code has cardinality q(v−k)(k−d/2+1) and the Anticode
bound is [ v
k ]q /

v−k+d/2−1
d/2−1

q. From Lemma 5 we conclude
lim
v→∞

(v−k)+k
k

q
q(v−k)k
=
1
(1/q; 1/q)k
and
lim
v→∞

(v−k)+(d/2−1)
d/2−1

q
q(v−k)(d/2−1)
=
1
(1/q; 1/q)d/2−1
,

Asymptotic Bounds for the Sizes of Constant Dimension Codes
183
so that the limit follows. The subsequent inequalities follow from d ≥4, the
monotonicity of (1/q; 1/q)n, and q ≥2.
It remains to show the monotonicity of the sequence
g(v) :=
q(v−k)(k−d/2+1) 
v−k+d/2−1
d/2−1

q
[ v
k ]q
.
Using the abbreviation s = v −k we deﬁne
f(x) :=
 s+x
s+1

q
qx [ s+x
s ]q
=
s+1
i=1
qx−1+i−1
qi−1
qx s
i=1
qx+i−1
qi−1
=
qx−1
qs+1−1
qx
= 1 −q−x
qs+1 −1
and observe that f is strictly monotonically increasing in x, so that f(k) >
f(d/2 −1). Using routine manipulations of q-binomial coeﬃcients we compute
g(v)
g(v + 1) = (1 + f(k)) · (1 + f(d/2 −1))−1 > 1.
⊓⊔
In other words the ratio between the best known lower bound and the
best known upper bound for constant dimension codes is strictly greater than
0.577576 for all parameters and the most challenging parameters are given by
q = 2, d = 4, and k = ⌊v/2⌋.
Replacing the Anticode bound by the Johnson bound of Theorem 2 does not
change the limit behavior of Proposition 8 when v tends to inﬁnity. As stated
above, we obtain the Anticode bound if we remove the ﬂoors in Corollary 2
and replace Aq(v −k + d/2, d; d/2) by qv−k+d/2−1
qd/2−1
. First we consider the latter
weakening. Applying the lower bound of Theorem 12 for Aq(v′, 2k′; k′), where
v′ = tk′ + r with 1 ≤r ≤k′ −1, we consider
qv′ −qk′+r + qk′ −1
qk′ −1
/ qv′ −1
qk′ −1 = 1 −qk′ · (qr −1)
qv′ −1
If v′ ≥3k′, then the subtrahend on the right hand side is at most qv′/3·(qv′/3−1)
qv′−1
.
Otherwise we have 2k′ ≤v′ < 3k′, so that v′ = 2k′ +r. Since qk′ ·(qr −1)·(qk′ +
1) = q2k′+r −q2k′ + qk′+r −qk′ ≤q2k′+r −1 the subtrahend on the right hand
side is at most 1/

qv′/3 + 1

. Thus, the ratio between the lower and the upper
bound for partial spreads tends to 1 if v′ = v −k + d/2 tends to inﬁnity. Since

184
D. Heinlein and S. Kurz
qv−1
qk−1
qv−1−1
qk−1−1

. . .
qv−k+d/2+1−1
qd/2+1−1
qv−k+d/2 −1
qd/2 −1

. . .

≥
qv−1
qk−1
qv−1−1
qk−1−1

. . .
qv−k+d/2 −1
qd/2 −1
−1

. . .

−1

−1

−1
≥
[ v
k ]q

v−k+d/2−1
d/2−1

q
−(k −d/2 + 1) ·
 v−1
k−1

q

v−k+d/2−1
d/2−1

q
≥
[ v
k ]q

v−k+d/2−1
d/2−1

q
·

1 −4 (k −d/2 + 1)
qv−k

the ratio between Corollary 2 and the Anticode bound tends to 1 as v tends to
inﬁnity.
Next, we consider the ratio between the lower bound from the ﬁrst construc-
tion of Proposition 6 and the Anticode bound when l tends to inﬁnity.
Proposition 9. For integers satisfying the conditions of Proposition 6, k ≤s
and d ≤2k, we have
lim
l→∞

b(s)la(v0) + a(s −d/2 + k) [ l
1 ]b(s)

/
 v0+ls
k

q

v0+ls−k+d/2−1
d/2−1

q
=
a(v0) +
a(s−d/2+k)
qs(k−d/2+1)−1
q(v0−k)(k−d/2+1)
·
k

i=d/2

1 −1
qi

Proof. For k ≤s and k −d/2 + 1 ≥1 we have b(s) ̸= 1, so that
b(s)la(v0) + a(s′) [ l
1 ]b(s) = qlsk′a(v0) + a(s′)qlsk′ −1
qsk′ −1
= qlsk′ 
a(v0) +
a(s′)
qsk′ −1

−
a(s′)
qsk′ −1,
where we abbreviate s′ = s −d/2 + k and k′ = k −d/2 + 1. Thus,
lim
l→∞

b(s)la(v0) + a(s′) [ l
1 ]b(s)

/qlsk′ = a(v0) +
a(s′)
qsk′ −1.
Plugging in the deﬁnition of the q-binomial coeﬃcients gives
 v0+ls
k

q

v0+ls−k+d/2−1
d/2−1

q
=
k
i=1
qv0+ls−k+i−1
qi−1
d/2−1

i=1
qv0+ls−k+i−1
qi−1
=
k

i=d/2
qv0+ls−k+i −1
qi −1
,
so that
lim
l→∞
 v0+ls
k

q

v0+ls−k+d/2−1
d/2−1

q
/qlsk′ =
k

i=d/2
qv0−k+i
qi −1 = q(v0−k)k′ ·
k

i=d/2
1
1 −1
qi
.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
185
Dividing both derived limits gives the proposed result.
⊓⊔
For Example 1 with d = 4 and k = 3, we obtain a ratio of

1597245 + A2(7, 4; 3)
4095

· 21/225 ∈[0.99963386, 0.99963388]
for v = 13 + 6l with l →∞using 333 ≤A2(7, 4; 3) ≤381, i.e., the Anticode
bound is almost met by the underlying improved linkage construction.
6
Codes Better Than the MRD Bound
For constant dimension codes that contain a lifted MRD code, Theorem 10 gives
an upper bound which is tighter than the Johnson bound of Theorem 7. In [5]
two inﬁnite series of constructions have been given where the code sizes exceed
the MRD bound of Theorem 10 for q = 2, d = 4, and k = 3. Given the data
available from [23] we mention that, besides d = 4, k = 3, the only other case
where the MRD bound was superseded is A2(8, 4; 4) ≥4801 > 4797, see [10].
Next, we show that for d = 4 and k = 3 the MRD bound can be superseded
for all ﬁeld sizes q if v is large enough. For the limit of the achievable ratio we
obtain:
Proposition 10. For q ≥3 we have lim
v→∞
Aq(v,4;3)
q2v−6+
 v−3
2

q
≥1 +
1
2q3 .
Proof. For q ≥2, [25, Theorem 4] gives
Aq(7, 4; 3) ≥q8 + q5 + q4 + q2 −q ≥q8 + q5 + q4.
With this, we conclude
Aq(v0, 4; 3) ≥Aq(7, 4; 3) · q2v0−14 + Aq(v0 −6, 4; 3) ≥q2v0−10 ·
	
q4 + q + 1

from Corollary 4 choosing m = 7. Applying Proposition 6 with s = 3 gives
Aq(v0 + 3l, 4; 3) ≥q6lAq(v0, 4; 3) + q6l −1
q6 −1 ≥q6lAq(v0, 4; 3)
for v0 ∈{12, 13, 14}, so that Aq(v, 4; 3) ≥q2v−10 ·
	
q4 + q + 1

for all v ≥12.
From Lemma 5 we conclude
lim
v→∞
q2v−6 +
 v−3
2

q
q2v−10
= q4 + (1/q; 1/q)2 = q3(q4 −q3 −q2 + q + 1)
(q −1)2(q + 1)
.
Since
	
q4 + q + 1

/q3(q4 −q3 −q2 + q + 1)
(q −1)2(q + 1)
= 1 + 1
q3 −
q + 1
q2(q4 −q3 −q2 + q + 1),
the statement follows for q ≥3.
⊓⊔

186
D. Heinlein and S. Kurz
For q = 2 the estimations of the proof of Proposition 10 are too crude in order
to obtain a factor larger than one. However, for the binary case better codes
with moderate dimensions of the ambient space have been found by computer
searches – with the prescription of automorphisms as the main ingredient in
order to reduce the computational complexity, see e.g. [31].
Proposition 11. For v ≥19 we have
A2(v,4;3)
22v−6+
 v−3
2

2
≥1.3056.
Proof. Applying Proposition 6 with s = 3 and using A2(4, 4; 3) ≥0 gives A2(v0+
3l, 4; 3) ≥A2(v0, 4; 3) · 26l for all v0 ≥6 and l ≥0, so that
A2(v0 + 3l, 4; 3)
22(v0+3l)−6 +
 (v0+3l)−3
2

2
≥A2(v0, 4; 3)
7
3 · 22v0−7 .
(9)
Using A2(7, 4; 3) ≥333 [23], A2(8, 4; 3) ≥1326 [10], A2(9, 4; 3) ≥5986 [10],
and A2(13, 4; 3) = 1597245 [9] we apply Corollary 4 with m = 13 to obtain
lower bounds for A2(v0, 4; 3) with 19 ≤v0 ≤21. For these values of v0 the
minimum of the right hand side of Inequality (9) is attained at v0 = 20 with
value 1.3056442377.
⊓⊔
Note that the application of Proposition 6 was used in a rather crude esti-
mation in the proof of Proposition 11. Actually, we do not use the codewords
generated by the codewords of the constant dimension code C2 in Theorem 18, so
that we might have applied [38, Theorem 37] directly for this part of the proof –
similarly for Proposition 10, which then allows to consider just one instead of
s = 3 starters. In the latter part of the proof of Proposition 11 the use of Corol-
lary 4 is essential in order to obtain large codes for medium sized dimensions
of the ambient space from A2(13, 4; 3) = 1597245 and relatively good lower
bounds for small dimensions. This is a relative typical behavior of Corollary 4
and Proposition 6, i.e., the ﬁrst few applications yield a signiﬁcant improvement
which quickly bottoms out – in a certain sense. As column bklb of Table 3 sug-
gests, we may slightly improve upon the value stated in Proposition 11 by some
ﬁne-tuning eﬀecting the omitted less signiﬁcant digits.
In Tables 1, 2, and 3 we compare the sizes of diﬀerent constructions with the
lifted MRD and the best known upper bound. Here bklb and bkub stand for best
known lower and upper bound respectively. The values of Theorem 10 are given
in column mrdb. Applying Theorems 17 and 18 to the best known codes give
the columns lold and lnew, respectively. The results obtained in [5] are stated
in column ea. The achieved ratio between the mentioned constructions and the
MRD bound can be found in Table 3. Since diﬀerences partially are beyond the
given accuracy, we give absolute numbers in Table 1. Note that the values in
column bklb of Table 3 show that Proposition 11 is also valid for v ≥16, while
we have a smaller ratio for v < 16. The relative advantage over lifted MRD codes
is displayed in Table 2.
To conclude this section, we remark that an application of Corollary 4 with
2k ≤m ≤v −k using a lifted MRD in the constant dimension code C1 cannot
generate a code that exceeds the MRD bound of Theorem 10.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
187
Table 1. Lower and upper bounds for A2(v, 4; 3).
v
bklb
mrdb
bkub
lold
lnew
ea
6
77
71
77
65
65
7
333
291
381
257
265
301
8
1326
1179
1493
1033
1101
1117
9
5986
4747
6205
4929
4929
4852
10
23870
19051
24698
21313
21313
18924
11
97526
76331
99718
85249
85257
79306
12
385515
305579
398385
383105
383105
309667
13
1597245
1222827
1597245
1532417
1532425
1287958
14
6241665
4892331
6387029
6241665
6241665
4970117
15
24966665
19571371
25562941
24966657
24966665
20560924
16
102223681
78289579
102243962
102223681
102223681
79608330
17
408894729
313166507
409035142
408894721
408894729
18
1635578957
1252682411
1636109361
1635578889
1635578957
19
6542315853
5010762411
6544674621
6542315597
6542315853
5200895489
Table 2. Lower and upper bounds for A2(v, 4; 3) divided by the size of a corresponding
lifted MRD code.
v
bklb
mrdb
bkub
lold
lnew
ea
6
1.203125 1.109375 1.203125 1.015625 1.015625
7
1.300781 1.136719 1.488281 1.003906 1.035156 1.175781
8
1.294922 1.151367 1.458008 1.008789 1.075195 1.090820
9
1.461426 1.158936 1.514893 1.203369 1.203369 1.184570
10 1.456909 1.162781 1.507446 1.300842 1.300842 1.155029
11 1.488129 1.164719 1.521576 1.300797 1.300919 1.210114
12 1.470623 1.165691 1.519718 1.461430 1.461430 1.181286
13 1.523252 1.166179 1.523252 1.461427 1.461434 1.228292
14 1.488129 1.166423 1.522786 1.488129 1.488129 1.184968
15 1.488129 1.166545 1.52367
1.488129 1.488129 1.225527
16 1.523252 1.166606 1.523554 1.523252 1.523252 1.186257
17 1.523252 1.166636 1.523775 1.523252 1.523252
18 1.523252 1.166651 1.523746 1.523252 1.523252
19 1.523252 1.166659 1.523801 1.523252 1.523252 1.210928
Lemma 6. Using the notation of Theorem 18, let k ≤min{v1 −k, v2 −k+d/2},
Cr a linear MRD code, dr = d1/2, and C1 contains a lifted MRD code (in

Fv1
q
k

). Then, the codes constructed in Theorem 18 contain a lifted MRD code
(in

Fv1+v2−k+d/2
q
k

).

188
D. Heinlein and S. Kurz
Table 3. Lower and upper bounds for A2(v, 4; 3) divided by the corresponding MRD
bound.
v
bklb
mrdb bkub
lold
lnew
ea
6
1.084507 1.0
1.084507 0.915493 0.915493
7
1.144330 1.0
1.309278 0.883162 0.910653 1.034364
8
1.124682 1.0
1.266327 0.876166 0.933842 0.947413
9
1.261007 1.0
1.307141 1.038340 1.038340 1.022119
10 1.252953 1.0
1.296415 1.118734 1.118734 0.993334
11 1.277672 1.0
1.306389 1.116833 1.116938 1.038975
12 1.261589 1.0
1.303705 1.253702 1.253702 1.013378
13 1.306190 1.0
1.306190 1.253176 1.253182 1.053263
14 1.275806 1.0
1.305519 1.275806 1.275806 1.015900
15 1.275673 1.0
1.306140 1.275672 1.275673 1.050561
16 1.305712 1.0
1.305972 1.305712 1.305712 1.016845
17 1.305678 1.0
1.306127 1.305678 1.305678
18 1.305661 1.0
1.306085 1.305661 1.305661
19 1.305653 1.0
1.306124 1.305653 1.305653 1.037945
Proof. Let {τ −1(Ik×k | M) : M ∈R} ⊆C1 be the lifted MRD code in C1. Since
R is a (k × (v1 −k), d1/2)q MRD code, we have #R = q(v1−k)(k−d1/2+1). The
ﬁrst set of the construction contains
{τ −1(Ik×k | M | A) : M ∈R, A ∈Cr}
in which {(M | A) : M ∈R, A ∈Cr} forms a (k × (v1 + v2 −2k + d/2), N, dr)q
rank metric code of size N = q(v1+v2−2k+d/2)(k−dr+1), hence it is a maximum
rank metric code.
⊓⊔
7
Conclusion
In this paper we have considered the maximal sizes of constant dimension codes.
With respect to constructive lower bounds we have improved the so-called link-
age construction, which then yields the best known codes for many parameters,
see Footnote 2. With respect to upper bounds there is a rather clear picture. The
explicit Corollary 2, which refers back to bounds for partial spreads, is the best
known parametric bound in the case of d ̸= 2 min{k, v −k}, while Theorem 8 or
the linear programming method may possibly yield improvements. Since Theo-
rem 8 implies the Johnson bound and so Corollary 2, it would be worthwhile to
study whether it can be strictly sharper than Theorem 7 for d ̸= 2 min{k, v −k}
at all. Compared to Corollary 2, the only two known improvements are given for
the speciﬁc parameters from Theorem 9 and Proposition 5. In the case of partial
spreads we have reported the current state-of-the-art mentioning that further
improvements are far from being unlikely.

Asymptotic Bounds for the Sizes of Constant Dimension Codes
189
In general we have shown that the ratio between the best-known lower and
upper bound is strictly larger than 0.577576 for all parameters. The bottleneck is
formed by the parameters q = 2, d = 4, and k = ⌊v/2⌋, where no known method
can properly improve that factor, see Footnote 2 for the linkage construction.
For d = 4, k = 3 and general ﬁeld sizes q we have applied the improved linkage
construction in order to show that Aq(v, d; k) is by a factor, depending on q,
larger than the MRD bound for suﬃciently large dimensions v.
Acknowledgement. The authors would like to thank Harout Aydinian for providing
an enlarged proof of Theorem 8, Natalia Silberstein for explaining the restriction 3k ≤v
in [38, Corollary 39], Heide Gluesing-Luerssen for clarifying the independent origin of
the linkage construction, and Alfred Wassermann for discussions about the asymptotic
results of Frankl and R¨odl. We thank the reviewers for their comments that helped us
to improve the presentation of the paper.
References
1. Agrell, E., Vardy, A., Zeger, K.: Upper bounds for constant-weight codes. IEEE
Trans. Inform. Theory 46(7), 2373–2395 (2000)
2. Ahlswede, R., Aydinian, H.: On error control codes for random network coding.
In: Workshop on Network Coding, Theory, and Applications, NetCod 2009, pp.
68–73. IEEE (2009)
3. Ahlswede, R., Aydinian, H.K., Khachatrian, L.H.: On perfect codes and related
concepts. Des. Codes Crypt. 22(3), 221–237 (2001)
4. Ahlswede, R., Cai, N., Li, S.Y.R., Yeung, R.W.: Network information ﬂow. IEEE
Trans. Inf. Theory 46(4), 1204–1216 (2000)
5. Ai, J., Honold, T., Liu, H.: The expurgation-augmentation method for constructing
good plane subspace codes. arXiv preprint arXiv:1601.01502 (2016)
6. Bachoc, C., Passuello, A., Vallentin, F.: Bounds for projective codes from semidef-
inite programming. Adv. Math. Commun. 7(2), 127–145 (2013)
7. Beutelspacher, A.: Partial spreads in ﬁnite projective spaces and partial designs.
Math. Z. 145(3), 211–229 (1975)
8. Blackburn, S.R., Etzion, T.: The asymptotic behavior of grassmannian codes. IEEE
Trans. Inform. Theory 58(10), 6605–6609 (2012)
9. Braun, M., Etzion, T., ¨Osterg˚ard, P.R.J., Vardy, A., Wassermann, A.: Existence
of q-analogs of steiner systems. Forum Math. Pi 4, 1–14 (2016)
10. Braun, M., ¨Osterg˚ard, P.R.J., Wassermann, A.: New lower bounds for binary
constant-dimension subspace codes. Exp. Math., 1–5. doi:10.1080/10586458.2016.
1239145
11. Delsarte, P.: Hahn polynomials, discrete harmonics, and t-designs. SIAM J. Appl.
Math. 34(1), 157–166 (1978)
12. Delsarte, P.: An algebraic approach to the association schemes of coding theory.
Ph.D. thesis, Philips Research Laboratories (1973)
13. Drake, D., Freeman, J.: Partial t-spreads and group constructible (s, r, μ)-nets. J.
Geom. 13(2), 210–216 (1979)
14. El-Zanati, S., Jordon, H., Seelinger, G., Sissokho, P., Spence, L.: The maximum
size of a partial 3-spread in a ﬁnite vector space over GF(2). Des. Codes Crypt.
54(2), 101–107 (2010)

190
D. Heinlein and S. Kurz
15. Etzion, T., Silberstein, N.: Error-correcting codes in projective spaces via rank-
metric codes and Ferrers diagrams. IEEE Trans. Inform. Theory 55(7), 2909–2919
(2009)
16. Etzion, T., Silberstein, N.: Codes and designs related to lifted MRD codes. IEEE
Trans. Inform. Theory 59(2), 1004–1017 (2013)
17. Etzion, T., Vardy, A.: Error-correcting codes in projective space. IEEE Trans.
Inform. Theory 57(2), 1165–1173 (2011)
18. Frankl, P., R¨odl, V.: Near perfect coverings in graphs and hypergraphs. Eur. J.
Comb. 6(4), 317–326 (1985)
19. Frankl, P., Wilson, R.M.: The Erd˝os-Ko-Rado theorem for vector spaces. J. Comb.
Theory, Ser. A 43(2), 228–236 (1986)
20. Gabidulin, E.: Theory of codes with maximum rank distance. Problemy Peredachi
Informatsii 21(1), 3–16 (1985)
21. Gluesing-Luerssen, H., Morrison, K., Troha, C.: Cyclic orbit codes and stabilizer
subﬁelds. Adv. Math. Commun. 9(2), 177–197 (2015)
22. Gluesing-Luerssen, H., Troha, C.: Construction of subspace codes through linkage.
Adv. Math. Commun. 10(3), 525–540 (2016)
23. Heinlein, D., Kiermaier, M., Kurz, S., Wassermann, A.: Tables of subspace codes.
arXiv preprint arXiv:601.02864 (2016)
24. Heinlein, D., Kurz, S.: A new upper bound for subspace codes. arXiv preprint
arXiv:1703.08712 (2017)
25. Honold, T., Kiermaier, M.: On putative q-analogues of the Fano plane and related
combinatorial structures. In: Dynamical Systems, Number Theory and Applica-
tions, pp. 141–175. World Sci. Publ, Hackensack, NJ (2016)
26. Honold, T., Kiermaier, M., Kurz, S.: Optimal binary subspace codes of length 6,
constant dimension 3 and minimum subspace distance 4. In: Topics in ﬁnite ﬁelds,
Contemp. Math., v ol. 632, pp. 157–176. Amer. Math. Soc., Providence, RI (2015)
27. Honold, T., Kiermaier, M., Kurz, S.: Partial spreads and vector space partitions.
arXiv preprint arXiv:1611.06328 (2016)
28. Johnson, S.: A new upper bound for error-correcting codes. IRE Trans. Inform.
Theory 8(3), 203–207 (1962)
29. Khaleghi, A., Silva, D., Kschischang, F.R.: Subspace codes. In: Parker, M.G. (ed.)
IMACC 2009. LNCS, vol. 5921, pp. 1–21. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-10868-6 1
30. Kiermaier, M., Kurz, S., Wassermann, A.: The order of the automorphism group
of a binary q-analog of the fano plane is at most two. Des. Codes Crypt. (2017).
doi:10.1007/s10623-017-0360-6
31. Kohnert, A., Kurz, S.: Construction of large constant dimension codes with a
prescribed minimum distance. In: Calmet, J., Geiselmann, W., M¨uller-Quade, J.
(eds.) Mathematical Methods in Computer Science. LNCS, vol. 5393, pp. 31–42.
Springer, Heidelberg (2008). doi:10.1007/978-3-540-89994-5 4
32. K¨otter, R., Kschischang, F.R.: Coding for errors and erasures in random network
coding. IEEE Trans. Inform. Theory 54(8), 3579–3591 (2008)
33. Kurz, S.: Improved upper bounds for partial spreads. Des. Codes Crypt. 85(1),
97–106 (2017). doi:10.1007/s10623-016-0290-8
34. Kurz, S.: Packing vector spaces into vector spaces. Australas. J. Comb. 68(1),
122–130 (2017)
35. MacWilliams, F.J., Sloane, N.J.A.: The theory of error-correcting codes. II. North-
Holland Publishing Co., Amsterdam-New York-Oxford , North-Holland Mathemat-
ical Library, vol. 16 (1977)

Asymptotic Bounds for the Sizes of Constant Dimension Codes
191
36. N˘astase, E., Sissokho, P.: The maximum size of a partial spread in a ﬁnite projective
space. arXiv preprint arXiv:1605.04824 (2016)
37. Segre, B.: Teoria di galois, ﬁbrazioni proiettive e geometrie non desarguesiane.
Annali di Matematica 64(1), 1–76 (1964)
38. Silberstein, N., Trautmann, A.L.: Subspace codes based on graph matchings, fer-
rers diagrams, and pending blocks. IEEE Trans. Inform. Theory 61(7), 3937–3953
(2015)
39. Silva, D., Kschischang, F., K¨otter, R.: A rank-metric approach to error control in
random network coding. IEEE Trans. Inform. Theory 54(9), 3951–3967 (2008)
40. Silva, D., Kschischang, F.R.: On metrics for error correction in network coding.
IEEE Trans. Inform. Theory 55(12), 5479–5490 (2009)
41. Tonchev, V.D.: Codes and designs. In: Handbook of Coding Theory, vol. 2, pp.
1229–1267 (1998)
42. Wang, H., Xing, C., Safavi-Naini, R.: Linear authentication codes: bounds and
constructions. IEEE Trans. Inform. Theory 49(4), 866–872 (2003)
43. Xia, S.T., Fu, F.W.: Johnson type bounds on constant dimension codes. Des. Codes
Crypt. 50(2), 163–172 (2009)

On Quasi-Abelian Complementary Dual Codes
Somphong Jitman1
, Herbert S. Palines2,3(B)
, and Romar B. dela Cruz3
1 Department of Mathematics, Faculty of Science, Silpakorn University,
Nakhon Pathom 73000, Thailand
sjitman@gmail.com
2 Institute of Mathematical Sciences and Physics,
University of the Philippines Los Ba˜nos, College, 4031 Laguna, Philippines
hspalines@up.edu.ph
3 Institute of Mathematics, College of Science, University of the Philippines Diliman,
1101 Quezon City, Philippines
rbdelacruz@math.upd.edu.ph
Abstract. Linear codes that meet their dual trivially are also known
as linear complementary dual codes. Quasi-abelian complementary dual
codes are characterized using a known decomposition of a semisimple
group algebra. Consequently, enumeration of such codes are obtained.
More explicit formulas are given for the number of quasi-abelian comple-
mentary dual codes of index 2 with respect to Euclidean and Hermitian
inner products. A sequence of asymptotically good binary quasi-abelian
complementary dual codes of index 3 is constructed from an existing
sequence of asymptotically good binary self-dual quasi-abelian codes of
index 2.
Keywords: Quasi-abelian codes · Linear complementary dual codes ·
Asymptotically good codes
1
Introduction
Linear complementary dual (LCD) codes, introduced by Massey [21], are linear
codes that have trivial intersection with their dual. It was reported in the same
paper that the class of LCD codes is asymptotically good and this class can oﬀer
an optimum solution for the two-user binary adder channel. In [27], it was shown
that LCD codes meet the Gilbert-Varshamov bound (GV-bound). LCD codes
are also useful in information protection from side-channel attacks and hardware
trojan horses (see [3,4,9,12,22,23]).
Another interesting class of linear codes are quasi-cyclic codes because of its
rich mathematical theory and various practical applications (see [16–19,24,26],
and references therein). This class contains cyclic codes which are well-studied
for a relatively long period of time to date. The authors in [28] combined the
concepts of LCD codes and cyclic codes and presented a necessary and suﬃcient
condition for a cyclic code to be an LCD. This idea has been extended to quasi-
cyclic codes with complementary duals (see [8,11]). It is interesting to note
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 192–206, 2017.
DOI: 10.1007/978-3-319-66278-7 16

On Quasi-Abelian Complementary Dual Codes
193
that the two said papers used diﬀerent techniques to characterize such codes.
In [8], the authors oﬀered a necessary and suﬃcient condition for a quasi-cyclic
code to be an LCD code using properties of polynomial generators. On the
other hand, [11] used the concatenated structure of quasi-cyclic codes in their
characterization. In addition, asymptotic results are also derived. A technique
of characterization similar to that of [11] is used in this work.
Quasi-abelian codes constitute an extensive class of codes which contains
cyclic, quasi-cyclic and abelian codes. The readers are referred to [6,14,30] for a
good background on quasi-abelian codes. Notably, there exist numerous asymp-
totically good quasi-abelian codes attaining the GV-bound [10]. This work is
intended to characterize and study quasi-abelian codes with complementary
duals.
A well-known decomposition of semisimple algebras (see [7,14]) is used and
hence, quasi-abelian codes are treated as Cartesian products of linear codes of
same length over some ﬁnite extension ﬁelds (possibly of diﬀerent orders). This
leads to a nice characterization and enumeration of quasi-abelian codes with
complementary duals with respect to Euclidean and Hermitian inner products.
More explicit formulas are derived for quasi-abelian complementary dual codes
of index 2. Using an existing class of asymptotically good binary self-dual quasi-
abelian codes of index 2 given in [14], a sequence of asymptotically good quasi-
abelian complementary dual codes of index 3 is constructed.
The material is organized as follows. Section 2 recalls deﬁnitions, some basic
properties of quasi-abelian codes and a well-known decomposition of quasi-
abelian codes. In Sect. 3, characterization and enumeration of quasi-abelian codes
with complementary duals are given. Construction of asymptotically good binary
quasi-abelian complementary dual codes is presented in Sect. 4.
2
Preliminaries
Deﬁnitions and some basic properties of quasi-abelian codes are recalled in this
section. Moreover, we also discuss two important decompositions of quasi-abelian
codes which lead to convenient forms of Euclidean dual and Hermitian dual of
such codes. These decompositions serve as a main tool in Sect. 3.
2.1
Notations
Let R denote a ﬁnite commutative ring with unity and let n be a positive integer.
A linear code of length n over R is deﬁned to be an R-submodule of Rn. A linear
code B is said to be a linear complementary dual (LCD) code if B∩B⊥= {0}. In
particular, if the given inner product is of Euclidean (resp., Hermitian) type, B
is said to be a Euclidean (resp., Hermitian) complementary dual code or simply
ECD (resp., HCD) code. Please see [11,21], respectively, for these deﬁnitions.
Denote by Fq the ﬁnite ﬁeld of order q, where q is a prime power. Consider a
ﬁnite abelian group G of order n, written additively. Let Fq[G] denote the group
algebra of G over Fq. The elements in Fq[G] will be written as 
g∈G αgY g,

194
S. Jitman et al.
where αg ∈Fq. The addition and the multiplication in Fq[G] are given as in
the usual polynomial rings over Fq with the indeterminate Y , where the indices
are computed additively in G. As convention, Y 0 is treated as the multiplicative
identity of Fq[G], where 0 is the identity of G. A (linear) code C in Fq[G] is an
Fq-subspace of Fq[G]. This can be regarded as a linear code of length n over Fq
by indexing each n-tuple codeword in the code by the elements of G.
Let H be a subgroup of G and l := [G : H]. A code C in Fq[G] is called
an H-quasi-abelian code (speciﬁcally, an H-quasi-abelian code of index l) if C
is an Fq[H]-module, i.e., C is closed under addition and multiplication by the
elements in Fq[H]. Note that since Fq is naturally embedded in Fq[H], C can
be viewed also as an Fq-vector space. If H is a non-cyclic subgroup of G, then
we say that C is a strictly H-quasi-abelian code. It is interesting to note the
following speciﬁc cases of quasi abelian codes: C is a quasi-cyclic code if H is a
proper cyclic subgroup, C is an abelian code if H = G, and C is a cyclic code
if H = G is a cyclic group. If it is clear in the context or if H is not speciﬁed,
such a code will be called simply a quasi-abelian code.
Let C be a code in Fq[G]. The Hamming weight of a codeword u =

g∈G αgY g in C, denoted by wt(u), is deﬁned to be the number of nonzero αg
in u. The minimum Hamming distance of C is deﬁned by d(C) := min{wt(u) |
u ∈C, u ̸= 0}. Denote by dimFq(C) the dimension of a code C as an Fq-vector
space in Fq[G].
In Fn
q , the Euclidean inner product of u = (u1, u2, . . . , un) and
v = (v1, v2, . . . , vn) is deﬁned to be ⟨u, v⟩e := n
i=1 uivi. In a similar manner,
the Euclidean inner product of u = 
g∈G αgY g and v = 
g∈G βgY g in Fq[G]
is deﬁned as
⟨u, v⟩E :=

g∈G
αgβg.
If q = q2
0, where q0 is a prime power, then the Hermitian inner product of u and
v in Fn
q can be deﬁned as ⟨u, v⟩h := n
i=1 uivi, where ¯ is the automorphism
on Fq deﬁned by α →αq0 for all α ∈Fq. At the same time, the Hermitian inner
product in Fq[G] is deﬁned by
⟨u, v⟩H :=

g∈G
αgβg
for all u = 
g∈G αgY g and v = 
g∈G βgY g in Fq[G]. The Euclidean dual and
Hermitian dual of a code C ⊆Fq[G] is given by
C⊥E := {u ∈Fq[G] | ⟨u, v⟩E = 0 for all v ∈C}
and
C⊥H := {u ∈Fq[G] | ⟨u, v⟩H = 0 for all v ∈C},
respectively. For a linear code B in Fn
q , let B⊥e and B⊥h denote its Euclidean
dual and Hermitian dual, respectively.

On Quasi-Abelian Complementary Dual Codes
195
An H-quasi-abelian code C is said to be an H-quasi-abelian Euclidean com-
plementary dual (H-QAECD) code if C ∩C⊥E = {0}. The H-quasi-abelian Her-
mitian complementary dual (H-QAHCD) code C is deﬁned in the same fashion,
i.e. if C ∩C⊥H = {0}. If it is clear in the context, we simply use QACD to
indicate a quasi-abelian Euclidean or Hermitian complementary dual code.
Consider a ﬁxed set of representatives of cosets of H in G given by
{g1, g2, . . . , gl}. Let R := Fq[H]. Deﬁne Φ : Fq[G] →Rl by
Φ

h∈H
l

i=1
αh+giY h+gi

= (α1(Y ), α2(Y ), . . . , αl(Y )) ,
(1)
where αi(Y ) = 
h∈H αh+giY h ∈R, for all i = 1, 2, . . . , l. It is well known that
Φ is an R-module isomorphism interpreted as follows.
Lemma 1. The map Φ induces a one-to-one correspondence between H-quasi-
abelian codes in Fq[G] and linear codes of length l over R.
Deﬁne an involution ∗on R to be the Fq-linear map that ﬁxes Fq and sends
Y h to Y −h. Following [14, Remark 2.1], the dual of a code D in Rl is given by
D⊥∗= {x ∈Rl | [x, y]∗= 0, for all y ∈D},
where the map [·, ·]∗: Rl × Rl →R is given by
[x, y]∗:=
l

i=1
xiyi
∗
for all x = (x1, x2, . . . , xl), y = (y1, y2, . . . , yl) ∈Rl. This map resembles a
Hermitian form in the sense that it is R-linear in the ﬁrst component and behaves
as a Hermitian form under conjugation.
2.2
Decompositions
The discussion in this section is found mainly in [13–15]. The concepts of q-
cyclotomic classes and primitive idempotents are instrumental in this work thus,
the readers are referred to [13, Sect. 2] and [15, Sect. 2] for details.
Given coprime positive integers i and j, the multiplicative order of j modulo
i, denoted by ordi(j), is deﬁned to be the smallest positive integer s such that i
divides js −1. For each a ∈H, denote by ord(a) the additive order of a in H.
Assume gcd(|H|, q) = 1. A q-cyclotomic class of H containing a ∈H, denoted
by Sq(a), is deﬁned to be the set
Sq(a) :={qi · a | i = 0, 1, . . . } = {qi · a | 0 ≤i < ordord(a)(q)},
where qi · a :=
qi

j=1
a in H. For a positive integer r and a ∈H, denote by −r · a
the element r · (−a) ∈H.

196
S. Jitman et al.
We now describe the types of q-cyclotomic classes that play an important role
in the decomposition of quasi-abelian codes with Euclidean duals. For a ∈H,
the q-cyclotomic class Sq(a) is of one of the following three types: type IE if
a = −a (i.e. Sq(a) = Sq(−a)), type IIE if Sq(a) = Sq(−a) and a ̸= −a, or type
IIIE if Sq(a) ̸= Sq(−a). For more details, see [13, Sect. 2].
An idempotent in a ring is a non-zero element e such that e2 = e, and it
is called a primitive idempotent if, for every other idempotent f, either ef = e
or ef = 0. The primitive idempotents in R := Fq[H] are induced by the q-
cyclotomic classes of H [7, Proposition II.4].
Assume that H contains t q-cyclotomic classes. Let {a1 = 0, a2, . . . , at}
be a complete set of representatives of the q-cyclotomic classes of H. Let
{e1, e2, . . . , et} be the set of primitive idempotents of R induced by {Sq(ai) |
i = 1, 2, . . . , t}, respectively.
In [25], R := Fq[H] is decomposed in terms of ei’s. Later, the components in
the decomposition of R are rearranged in [13] and the following is obtained:
R =
t

i=1
Rei ∼=
rIE

i=1
Fq

×
⎛
⎝
rIIE

j=1
Ej
⎞
⎠×
rIIIE

r=1
(Kr × K′
r)

,
(2)
where Fq ∼= Rei for i = 1, 2, . . . , rIE, Ej ∼= RerIE+j, and Kr ∼= K′r ∼=
RerIE+rIIE+r are ﬁnite extension ﬁelds of Fq for all j = 1, 2, . . . , rIIE and
r = 1, 2, . . . , rIIIE; rIE, rIIE and 2rIIIE correspond to the number of types IE,
IIE and IIIE q-cyclotomic classes of H, respectively; and t = rIE +rIIE +2rIIIE.
Note that ⊕denotes internal direct sum in R while  and × denote Cartesian
products.
Remark 2. The ﬁeld extensions in the right-hand side of (2) are given
as follows: Ej
∼=
Fqsj , Kr
∼=
Fqtr
and K′
r
∼=
Fqt′r , where sj
:=
|Sq(arIE+j)|, tr := |Sq(arIE+rIIE+r)|, and t′
r := |Sq(arIE+rIIE+rIIIE+r)|, such that
arIE+rIIE+rIIIE+r = −arIE+rIIE+r, for j = 1, 2, . . . , rIIE and r = 1, 2, . . . , rIIIE.
In this particular order of q-cyclotomic classes of type IIIE, it will follow that
tr = t′
r for each r = 1, 2, . . . , rIIIE and thus, Kr ∼= K′
r for each r = 1, 2, . . . , rIIIE.
From (1), (2) and Remark 2,
Fq[G] ∼= Rl ∼=
rIE

i=1
Fl
q

×
⎛
⎝
rIIE

j=1
Fl
qsj
⎞
⎠×
rIIIE

r=1

Fl
qtr × Fl
qt′r

,
(3)
where the isomorphisms can be viewed as Fq-module isomorphisms. Conse-
quently, an H-quasi-abelian code C in Fq[G] can be represented as
C ∼=
rIE

i=1
Bi

×
⎛
⎝
rIIE

j=1
Cj
⎞
⎠×
rIIIE

r=1
(Dr × D′
r)

,
(4)

On Quasi-Abelian Complementary Dual Codes
197
where Bi, Cj, Dr and D′
r are linear codes, each of length l, over ﬁnite ﬁelds
Fq, Fqsj , Fqtr and Fqt′r , respectively, for i = 1, 2, . . . , rIE, j = 1, 2, . . . , rIIE and
r = 1, 2, . . . , rIIIE. From [14, Proposition 4.1], the Euclidean dual of C in (4) is
given by
C⊥E ∼=
rIE

i=1
B⊥e
i

×
⎛
⎝
rIIE

j=1
C⊥h
j
⎞
⎠×
rIIIE

r=1

(D′
r)⊥e × D⊥e
r


.
(5)
Let q = q2
0. In the Hermitian case, the following types of q-cyclotomic classes
are considered: type IH if Sq(a) = Sq(−q0 · a) or type IIH if Sq(a) ̸= Sq(−q0 · a),
for a ∈H. In a similar manner, let {a1 = 0, a2, . . . , at} be the set of represen-
tatives of q-cyclotomic classes H and let {e1, e2, . . . , et} be the set of primitive
idempotents in R := Fq[H] induced by {Sq(ai) | i = 1, 2, . . . , t}, respectively.
Let si = |Sq(ai)|, tj = |Sq(arIH+j)| and t′
j = |Sq(arIH+rIIH+j)|, such that
arIH+rIIH+j = −arIH+j for all i = 1, 2, . . . , rIH and j = 1, 2, . . . , rIIH. Following
the above discussion for the Euclidean case,
R =
t

i=1
Rei ∼=
rIH

i=1
Ei

×
⎛
⎝
rIIH

j=1
Kj × K′
j
⎞
⎠,
(6)
and
Fq[G] ∼= Rl ∼=
rIH

i=1
Fl
qsi

×
⎛
⎝
rIIH

j=1
(Fl
tj × Fl
t′
j)
⎞
⎠,
(7)
where Ei ∼= Rei ∼= Fqsi , Kj ∼= RerIH+j ∼= Fqtj and K′
j ∼= RerIH+rIIH+j ∼= Fq
t′
j , for
i = 1, 2, . . . rIH and j = 1, 2, . . . , rIIH; rIH and 2rIIH correspond to the number of
q-cyclotomic classes of H of types IH and IIH, respectively; and t = rIH +2rIIH.
Note that tj = t′
j and hence, Kj ∼= K′
j for all j. As a result, a quasi-abelian code
C in Fq[G] can be also represented as
C ∼=
rIH

i=1
Ci

×
⎛
⎝
rIIH

j=1

Dj × D′
j

⎞
⎠,
(8)
where Ci, Dj and D′
j are linear codes, each of length l, over Fqsi, Fqtj and Fq
t′
j ,
respectively.
Following similar arguments as in the proofs of [15, Proposition 2.7] and
[14, Proposition 4.1], it can be deduced that the Hermitian dual of C in (8) is
of the form
C⊥H ∼=
rIH

i=1
C⊥h
i

×
⎛
⎝
rIIH

j=1

(D′
j)⊥e × D⊥e
j

⎞
⎠.
(9)

198
S. Jitman et al.
3
Characterization and Enumeration of QACD Codes
In this section, QACD codes are characterized using decompositions (4), (5), (8)
and (9), accordingly. Enumeration of QACD codes naturally follows from the
said characterization.
From the deﬁnition of a QACD code, it is clear that a code C given in (4) is
an H-QAECD code if every factor in its decomposition meet each corresponding
factor in C⊥E in (5) trivially. Adding the fact that D′
r ∩(Dr)⊥e = {0} is equiva-
lent to Dr ⊕(D′
r)⊥e = Fl
qtr , for each r = 1, 2, . . . , rIIIE, the following proposition
is obtained.
Proposition 3. Let H ≤G be ﬁnite abelian groups such that gcd(|H|, q) = 1
and l = [G : H]. Assume that Fq[H] contains rIE, rIIE and 2rIIIE primitive
idempotents of types IE, IIE and IIIE, respectively. Assume further that the
primitive idempotents of types IE, IIE and IIIE are induced by q-cyclotomic
classes of sizes equal to s′
i = 1, sj, tr = t′
r, respectively, for all i = 1, 2, . . . , rIE,
j = 1, 2, . . . , rIIE and r = 1, 2, . . . , rIIIE. Then a quasi-abelian code C of index l
given in (4) is an H-QAECD code if and only if the following conditions hold:
(i) Bi is an ECD code, for each i = 1, 2, . . . rIE,
(ii) Cj is an HCD code, for each j = 1, 2, . . . rIIE, and
(iii) Dr ⊕(D′
r)⊥e = Fl
qtr for each r = 1, 2, . . . , rIIIE.
A similar result for the Hermitian case is given below.
Proposition 4. Let H ≤G be ﬁnite abelian groups such that gcd(|H|, q) = 1,
q = q2
0 and l = [G : H]. Assume further that Fq[H] contains rIH and 2rIIH primi-
tive idempotents of types IH and IIH, respectively. Assume further that the prim-
itive idempotents of types IH and IIH are induced by q-cyclotomic classes of sizes
equal to si, tj = t′
j, respectively, for all i = 1, 2, . . . , rIH and j = 1, 2, . . . , rIIH.
Then a quasi-abelian code C of index l given in (8) is an H-QAHCD code if and
only if the two conditions hold:
(i) Ci is an HCD code, for each i = 1, 2, . . . rIH, and
(ii) Dj ⊕(D′
j)⊥e = Fl
qtj for each j = 1, 2, . . . , rIIH.
Remark 5. An [n, k] linear code over a ﬁnite ﬁeld, generated by a k × n matrix
A, is a Euclidean or Hermitian LCD code if and only if AAT is invertible (see
[21, Proposition 1] and [11, Proposition 3.5]). Note that from conditions (iii) and
(ii) of Propositions 3 and 4, respectively, it is necessary that Dr and D′
r (resp.,
Dj and D′
j) have the same dimensions for each r = 1, 2, . . . , rIIIE (resp., j =
1, 2, . . . , rIIH) for C to be an H-QAECD (resp., H-QAHCD) code. Indeed, if D
and D′ satisfy condition (iii) of Proposition 3 or condition (ii) of Proposition 4,
then dim(D) = l −dim((D′)⊥) = l −(l −dim(D′)).
Propositions 3 and 4 provide a convenient way to count QACD codes. The
general counting strategy can be summarized into three parts: (1) counting ECD
codes, (2) counting HCD codes and (3) counting complementary linear codes D

On Quasi-Abelian Complementary Dual Codes
199
and (D′)⊥e such that D ⊕(D′)⊥e = F l, where D and D′ are linear codes of
length l over some ﬁnite extension ﬁeld F of Fq.
Let NECD(q, l) (resp., NHCD(q, l)) be the number of distinct ECD (resp.,
HCD) codes over Fq of length l. Moreover, consider the Gaussian binomial coef-
ﬁcient, denoted by
n
k

q, which gives the number of distinct k-dimensional sub-
spaces of Fn
q .
Lemma 6. Suppose D is an [l, k] linear code over Fq. Let N⊕(q, k, l) denotes
the number of [l, l −k] linear codes B over Fq such that D ⊕B = Fl
q. Then
N⊕(q, k, l) = (qk)l−k.
Proof. Suppose T = {v1, v2, . . . , vk} ⊆Fl
q is a basis of D. The number of ways
that T can be extended to the set {v1, v2, . . . , vk, vk+1, . . . , vl} as a basis of Fl
q is
1
(l −k)!
l−1

i=k

ql −qi
.
By taking {vk+1, vk+2, . . . , vl} as a basis for B and considering the fact that
the number of diﬀerent bases for B is given by
1
(l−k)!
l−k−1
i=0

ql−k −qi
[20, Theorem 4.1.15], then it follows that
N⊕(q, k, l) =
l−1
i=k

ql −qi
l−k−1
i=0
(ql−k −qi)
=
l−1

i=k
qk 
ql−k −qi−k
(ql−k −qi−k)
= (qk)l−k.
⊓⊔
Lemma 6 is useful to the following propositions.
Proposition 7. Let H ≤G be ﬁnite abelian groups such that gcd(|H|, q) = 1
and l = [G : H]. Assume that Fq[H] contains rIE, rIIE and 2rIIIE primitive
idempotents of types IE, IIE and IIIE, respectively. Assume further that the
primitive idempotents of types IE, IIE and IIIE are induced by q-cyclotomic
classes of sizes equal to s′
i = 1, sj, tr = t′
r, respectively, for all i = 1, 2, . . . , rIE,
j = 1, 2, . . . , rIIE and r = 1, 2, . . . , rIIIE. Then the number of H-QAECD codes
C in (4), is given by
⎛
⎝
rIE

i=1
NECD(q, l)
⎞
⎠
⎛
⎝
rIIE

j=1
NHCD(qsj , l)
⎞
⎠
⎛
⎝
rIIIE

r=1
⎛
⎝2 +
l−1

k=1
	l
k

qtr · N⊕(qtr , k, l)
⎞
⎠
⎞
⎠. (10)
Proof. It is clear from (4) and Proposition 3 that we need to count the number of
ECD codes Bi and HCD codes Cj, for each i = 1, 2, . . . , rIE and j = 1, 2, . . . , rIIE,
to obtain the ﬁrst and second factors in (10). The third factors are obtained by
getting the number of codes (Dr, D′
r) such that Dr ⊕(D′
r)⊥E = Fl
qtr for each
r = 1, 2, . . . , rIIIE and for all possible dimensions k of Dr, where 0 ≤k ≤l.
Apply Lemma 6 and consider the fact that there are
l
k

qtr distinct k-dimensional
subspaces in Fl
qtr , N⊕(qtr, 0, l) = 1 = N⊕(qtr, l, l), and
l
0

qtr = 1 =
l
l

qtr , for
each r and k.
⊓⊔

200
S. Jitman et al.
The following result is for the Hermitian case where its proof follows from that
of the Euclidean case.
Proposition 8. Let H ≤G be ﬁnite abelian groups such that gcd(|H|, q) = 1,
q = q2
0 and l = [G : H]. Assume further that Fq[H] contains rIH and 2rIIH primi-
tive idempotents of types IH and IIH, respectively. Assume further that the prim-
itive idempotents of types IH and IIH are induced by q-cyclotomic classes of sizes
equal to si, tj = t′
j, respectively, for all i = 1, 2, . . . , rIH and j = 1, 2, . . . , rIIH.
Then the number of H-QAHCD codes C in (8), is given by
rIH

i=1
NHCD(qsi, l)
 ⎛
⎝
rIIH

j=1

2 +
l−1

k=1
l
k

qtj
· N⊕(qtj, k, l)
⎞
⎠.
(11)
Remark 9. It should be noted that the requirement q = q2
0 in Proposition 8 is
needed to make sense of the Hermitian dual of C. However, in Proposition 7,
q can be a non-square even if there are Hermitian dual codes in the factors of
C⊥E in (5). Since these factors correspond to q-cyclotomic classes of type IIE,
wherein sj is even (see [13, Remark 2.6]) for each j = 1, 2, . . . , rIIE, then qsj is
always a square and hence, set q0 = qsj/2.
Considering a relatively small ﬁxed value of the index l serves some the-
oretical and practical use (see for instance [5,17,19]). In this work, ﬁxing the
index l, say l = 2, gives rise to speciﬁc formulas for NECD(q, 2), NHCD(q, 2) and
N⊕(q, k, 2), which can be seen in the following results.
In the lemma below, some properties of the square map on F∗
q := Fq \ {0}
are recalled. This can be found also in [29, Theorem 6.18].
Lemma 10. Let f : F∗
q →F∗
q, such that f(x) = x2.
(i) The mapping f is a group homomorphism.
(ii) If q = 2r, for some r > 0, then f is an isomorphism. Consequently, every
element in F∗
q has a unique square root. That is, x2 = a has a unique
solution in F∗
q for each a ∈F∗
q.
(iii) If q is odd, then every square element in F∗
q has two distinct square roots
in F∗
q.
(iv) Let q be odd. An element α ∈F∗
q is a square if and only if α
q−1
2
= 1.
Consequently, −1 ∈F∗
q is a square if and only if q ≡1 (mod 4).
Lemma 11. Let q be a prime power. Then
(i)
NECD(q, 2) =
⎧
⎪
⎨
⎪
⎩
q + 2
if q is even,
q + 1
if q ≡1 (mod 4)
q + 3
if q ≡3 (mod 4),

On Quasi-Abelian Complementary Dual Codes
201
(ii) if q = q2
0,
NHCD(q, 2) = q −q0 + 2
Proof.
(i) Let C be a linear code over Fq of length 2. If C = {0} or C = F2
q, then
C is trivially an ECD code. Suppose C is of dimension 1. Then, C has the
following possible distinct generator matrices: [α, 0], [0, α] and [1, α] where
α ∈F∗
q. It is clear that the ﬁrst two generator matrices will generate ECD
codes since α2 is invertible in Fq. We only need to count the number of
distinct generator matrices of the form [1, α] such that 1 + α2 ̸= 0. Deﬁne
the map f : F∗
q →F∗
q such that f(α) = α2 for all α ∈F∗
q. Let O := {α ∈
F∗
q | f(α) = α2 = −1}. Note |O| = 1 if q is even (Lemma 10 (ii)) or |O| = 2
if q ≡1 (mod 4) (Lemma 10 (iii) and (iv)) or |O| = 0 if q ≡3 (mod 4)
(Lemma 10 (iv)). So, if q is even, there are (q −1) −|O| = q −2 elements α
in F∗
q such that 1+α2 is nonzero, if q ≡1(mod4) or q ≡3(mod4), there are
(q −1) −2 = q −3 or q −1 such elements, respectively. Sum up the number
of LCD codes of dimensions 0, 1 and 2 for each case of q.
(ii) For the Hermitian case, note that q = q2
0. This proof is similar to that of
the Euclidean case. However, the norm function Nrm : F∗
q →F∗
q0, given
by Nrm(α) = αα for all α ∈F∗
q, is needed to derive the formula. Note
that Nrm is surjective. Let O′ := {α ∈F∗
q | Nrm(α) = αα = −1}. It is
known that |Nrm−1(u)| =
|F∗
q|
|F∗
q0| = |Nrm−1(v)| for any u, v ∈F∗
q0. Thus,
|O′| = |Nrm−1(−1)| =
q−1
q0−1 = q0 + 1. Thus, there are (q −1) −|O′| =
q −q0 −2 elements α ∈F∗
q such that [1, α] is a generator of an HCD code,
i.e. 1 + αα ̸= 0. Combining all the numbers we got from HCD codes of
dimensions 0, 1, and 2, we have 1 + (2 + q −q0 −2) + 1 = q −q0 + 2.
⊓⊔
The following corollary follows directly from Propositions 7 and 8 and Lemma 11.
Corollary 12. Using the same assumptions given in Propositions 7 and 8 and
letting l = 2, the following statements hold.
(i) The number of H-QAECD codes C of index 2 in (4), is given by
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
rIE

i=1
(q + 2)
 rIIE

j=1
(qsj −qsj/2 + 2)
 rIIIE

r=1
	
q2tr + qtr + 2


if q is even,
rIE

i=1
(q + 1)
 rIIE

j=1
(qsj −qsj/2 + 2)
 rIIIE

r=1
	
q2tr + qtr + 2


if q ≡1 (mod 4),
rIE

i=1
(q + 3)
 rIIE

j=1
(qsj −qsj/2 + 2)
 rIIIE

r=1
	
q2tr + qtr + 2


if q ≡3 (mod 4).

202
S. Jitman et al.
(ii) The number of H-QAHCD codes C of index 2 in (8), is given by
rIH

i=1
(qsi −qsi/2 + 2)
 ⎛
⎝
rIIH

j=1

q2tj + qtj + 2

⎞
⎠.
Proof. (i) Use Proposition 7 and Lemma 11 (i) correspondingly for each case
of q.
(ii) Use Proposition 8 and Lemma 11 (ii) to obtain the result.
⊓⊔
Example 13.
1. Consider q = 2 and H = (Z3)s, for s = 1, 2. Let Z3 := {0, 1, 2} and
(Z3)2 := {(x, y) | x, y ∈Z3}. The 2-cyclotomic classes of Z3 are given by
S2(0) = {0} and S2(1) = {1, 2} which are of type IE and type IIE, respec-
tively. For (Z3)2, its 2-cyclotomic classes consist of S2((0, 0)) which is of type
IE and all the other four are of type IIE with cardinality 2 each. It follows
that F2[Z3] ∼= F2 × F22 and F2[(Z3)2] ∼= F2 ×
4
j=1 F22

. This informa-
tion is summarized in Table 1 which also shows the number of corresponding
H-QAECD codes of index 2 in the last column. Let A := rIE
i=1 NECD(q, 2)
and B := rIIE
j=1 NHCD(qsj, 2).
2. For the Hermitian case, let q = 4 and H = (Z3)s, for s = 1, 2. In this case,
the 4-cyclotomic classes of Z3 and (Z3)2 are all of type IH with cardinality
1 each. Thus, we can write F4[Z3] ∼= F4 × F4 × F4 and F4[(Z3)2] ∼=
9
i=1 F4.
The number of corresponding H-QAHCD codes of index 2 are given in the
last column of Table 2.
Table 1. Number of H-QAECD codes of index 2 in F2[Z2 × H], H = (Z3)s
q s
H
rIE
rIIE
rIIIE
sj
A B
A · B
2 1
Z3
1
1
-
s1 = 2
4
4
16
2 (Z3)2
1
4
-
sj = 2 for all j
4
44
1024
Table 2. Number of H-QAHCD codes of index 2 in F4[Z2 × H], H = (Z3)s
q s
H
rIH
rIIH
si
rIH
i=1 NHCD(qsi, 2)
4 1
Z3
3
-
si = 1 for all i
64
2 (Z3)2
9
-
si = 1 for all i
49

On Quasi-Abelian Complementary Dual Codes
203
4
Asymptotically Good Binary QAECD Codes of Index 3
In this section, a sequence of asymptotically good binary QAECD codes of index
3 is constructed through an existing sequence of asymptotically good binary self-
dual quasi-abelian codes of index 2 presented in [14]. Recall that R := Fq[H] where
H is a subgroup of a ﬁnite abelian group G, written additively. Moreover from
Lemma 1, the map Φ : Fq[G] →Rl induces a one-to-one correspondence between
linear codes of length l = [G : H] over R and H-quasi abelian codes in Fq[G].
Thus, quasi-abelian codes can be studied as linear codes over R of length l.
If C is an H-quasi abelian code in Fq[G], then Φ(C)⊥∗= Φ(C⊥E) [14, Corol-
lary 2.1]. This means that C ∩C⊥E = {0} if and only if Φ(C) ∩Φ(C)⊥∗= {0}.
Hence, the map Φ also induces a one-to-one correspondence between LCD codes
in Fq[G] and Rl.
Consider linear codes C(a,b) := {(fa, fb) | f ∈R} ⊆R2 and C(a,b,1) :=
{(fa, fb, f) | f ∈R} ⊆R3. Note that C(a,b) and C(a,b,1) correspond to quasi-
abelian codes in Fq[G] of index 2 and index 3, respectively.
Starting from this point, assume that R := F2[H] (gcd(|H|, 2) = 1 is no longer
required). From [14, Lemma 7.1], C(a,b) is self-dual if and only if aa∗= bb∗∈
U(R), where U(R) denotes the group of units in R. Since char(R) = 2, the given
condition is equivalent to aa∗+ bb∗= 0 and a, b ∈U(R). It follows that if C(a,b)
is self-dual, then C(a,b,1) is an ECD code from the fact that aa∗+ bb∗+ 1 = 1.
Indeed, it can be easily shown that if (u, v, w) ∈C(a,b,1)∩C⊥∗
(a,b,1), then (u, v, w) =
(0, 0, 0). In general, we have the following proposition.
Proposition 14. Let H ≤G be ﬁnite abelian groups and let R := Fq[H]. Sup-
pose C is a 1-generator quasi-abelian code in Fq[G] generated by g ∈R. If
[g, g]∗∈U(R), then C is an H-QAECD code.
We are now ready to show that there exists a sequence of asymptotically good
binary QAECD codes of index 3. First, consider the main tool for that purpose.
Let m be an odd integer and let s(m) denote the multiplicative order of 2 modulo
m. Consequently, let l(m) := min{s(p) | p is a prime divisor of m} which is the
smallest dimension of a nontrivial F2-representation of H (or equivalently, the
smallest size of nonzero 2-classes of H) [1, Lemma 2.5].
Theorem 15 [14, Theorem 7.1]. Let (Hmk) be a sequence of abelian groups of
odd order mk, where (mk) is a strictly increasing sequence, s(mk) is odd and
l(mk) ≫log mk. Let Ak = Hmk ⊕Z4, Rk = F2[Ak] and
Ω = {(a, b) ∈U(Rk) × U(Rk) | wt(a) + wt(b) ≡0 (mod 4)}.
For each k, assume that (a, b) ∈Ω is chosen as random and let C(k)
(a,b) :=
{(fa, fb) | f ∈Rk}. Then
(i) C(k)
(a,b) is a self-dual doubly even quasi-abelian code, and

204
S. Jitman et al.
(ii) there exists δ > 0 such that for k large enough, the codes C(k)
(a,b) have relative
minimum distance
d

C(k)
(a,b)

8mk
≥δ.
(12)
Theorem 15 gives the assurance that there is a sequence of self-dual codes

C(k)
(a,b)

over Rk which is asymptotically good. Equivalently, there exists a
sequence of asymptotically good binary self-dual quasi-abelian codes of index
2. This sequence of codes is used to construct a sequence of asymptotically good
binary QAECD codes of index 3, as presented in the following proposition.
Proposition 16. Let (Hmk) be a sequence of abelian groups of odd order mk,
where (mk) is a strictly increasing sequence, s(mk) is odd and l(mk) ≫log mk.
Let Ak = Hmk ⊕Z4, Rk = F2[Ak] and
Ω = {(a, b) ∈U(Rk) × U(Rk) | wt(a) + wt(b) ≡0 (mod 4)}.
For each k, assume that (a, b) ∈Ω is chosen as random and let C(k)
(a,b) :=
{(fa, fb) | f ∈Rk}. Deﬁne a sequence of codes given by

C(k)
(a,b,1)

where
C(k)
(a,b,1) := {(fa, fb, f) | f ∈Rk}. Then
(i) C(k)
(a,b,1) is a QAECD code for each k,
(ii) the codes C(k)
(a,b,1) have relative minimum distance
d

C(k)
(a,b,1)

12mk
> 0,
and have a positive constant relative rate.
Proof.
(i) Follows directly from Theorem 15 (i) and from the discussion above.
(ii) From Theorem 15 (ii), it follows that the relative minimum distance of the
codes C(k)
(a,b,1) is given by
d

C(k)
(a,b,1)

12mk
≥
d

C(k)
(a,b)

12mk
= 8
12 ·
d

C(k)
(a,b)

8mk
≥2
3 · δ > 0,
for some δ > 0 and for k large enough.
The relative rate of the codes C(k)
(a,b,1) is given by
dimF2

C(k)
(a,b,1)

12mk
=
4mk
12mk = 1
3
for all k.
⊓⊔
Note that in [11], it was shown that there exists an asymptotically good
sequence of quasi-cyclic complementary dual codes. We have to emphasize that
Proposition 16 also considers the case of strictly quasi-abelian codes since Ak
can be non-cyclic for each k.

On Quasi-Abelian Complementary Dual Codes
205
Summary
Through a known decomposition of semisimple group algebras, quasi-abelian
complementary dual codes are characterized and enumerated. Explicit formulas
for the number of quasi-abelian Euclidean and Hermitian complementary dual
codes of index 2 were given. A class of asymptotically good binary index 3 quasi-
abelian Euclidean complementary dual codes were constructed from an existing
class of asymptotically good binary self-dual quasi-abelian codes of index 2.
Recall that if H ≤G are ﬁnite abelian groups and R := Fq[H], then Fq[G] ∼= Rl
as R-modules, where l := [G : H]. For non-semisimple case (i.e. gcd(|H|, q) ̸= 1),
one might study the algebraic structure of quasi-abelian complementary dual codes
purely as linear codes over R or as linear codes in Fq[G]. The asymptotic goodness of
QACD codes of index 2 is still a viable problem for semisimple and non-semisimple
cases.
Acknowledgment. S. Jitman was supported by the Thailand Research Fund under
Research Grant MRG6080012. H. S. Palines would like to extend his sincerest gratitude
to the following institutions: University of the Philippines Los Ba˜nos, University of the
Phillipines System, Department of Science and Technology-Science Education Institute
(DOST-SEI) of the Philippines, and Mathematics Department, Faculty of Science,
Silpakorn University, Nakhon Pathom, Thailand.
References
1. Bazzi, L.M.J., Mitter, S.K.: Some randomized code constructions from group
actions. IEEE Trans. Inf. Theory 52, 3210–3219 (2006)
2. Bosma, W., Cannon, J., Playoust, C.: The Magma algebra system I: the user
language. J. Symb. Comput. 24, 235–265 (1997)
3. Carlet, C., Guilley, S.: Complementary dual codes for countermeasures to side-
channel attacks. Coding Theor. Appl. 3, 97–105 (2015)
4. Carlet, C., Daif, A., Danger, J.L., Guilley, S., Najm, Z., Ngo, X.T., Portebouef, T.,
Tavernier, C.: Optimized linear complementary codes implementation for hardware
trojan prevention. In: Proceedings of European Conference on Circuit Theory and
Design, 24–26 August 2015, Trondheim, Norway. IEEE, Piscataway (2015)
5. Dey, B.K.: On existence of good self-dual quasi-cyclic codes. IEEE Trans. Inform.
Theory 50, 1794–1798 (2004)
6. Dey, B.K., Rajan, B.S.: Codes closed under arbitrary abelian group of permuta-
tions. SIAM J. Discrete Math. 18, 1–18 (2004)
7. Ding, C., Kohel, D.R., Ling, S.: Split group codes. IEEE Trans. Inform. Theory
46, 485–495 (2000)
8. Esmaeili, M., Yari, S.: On complementary-dual quasi-cylic codes. Finite Fields
Their Appl. 15, 375–386 (2009)
9. Etesami, J., Hu, F., Henkel, W.: LCD codes and iterative decoding by projections,
a ﬁrst step towards an intuitive description of iterative decoding. In: Proceedings
of IEEE Globecom, 5–9 December 2011, Texas, USA. IEEE, Piscataway (2011)
10. Fan, Y., Lin, L.: Thresholds of random quasi-abelian codes. IEEE Trans. Inform.
Theory 61, 82–90 (2015)

206
S. Jitman et al.
11. Guneri, C., Ozkaya, B., Sol´e, P.: Quasi-cylic complementary dual codes. Finite
Fields Their Appl. 42, 67–80 (2016)
12. Ishai, Y., Sahai, A., Wagner, D.: Private circuits: securing hardware against prob-
ing attacks. In: Boneh, D. (ed.) CRYPTO 2003. LNCS, vol. 2729, pp. 463–481.
Springer, Heidelberg (2003). doi:10.1007/978-3-540-45146-4 27
13. Jitman, S., Ling, S., Liu, H., Xie, X.: Abelian codes in principal ideal group alge-
bras. IEEE Trans. Inform. Theory 59, 3046–3058 (2013)
14. Jitman, S., Ling, S.: Quasi-abelian codes. Des. Codes Crypt. 74, 511–531 (2015)
15. Jitman, S., Ling, S., Sol´e, P.: Hermitian self-dual abelian codes. IEEE Trans.
Inform. Theory 60, 1496–1507 (2014)
16. Lally, K., Fitzpatrick, P.: Algebraic structure of quasicyclic codes. Discrete Appl.
Math. 111, 157–175 (2001)
17. Ling, S., Sol´e, P.: On the algebraic structure of quasi-cyclic codes I: ﬁnite ﬁelds.
IEEE Trans. Inform. Theory 47, 2751–2760 (2001)
18. Ling, S., Sol´e, P.: Good self-dual quasi-cyclic codes exist. IEEE Trans. Inform.
Theory 49, 1052–1053 (2003)
19. Ling, S., Sol´e, P.: On the algebraic structure of quasi-cyclic codes III: generator
theory. IEEE Trans. Inform. Theory 51, 2692–2700 (2005)
20. Ling, S., Xing, C.: Coding Theory, A First Course. Cambridge University Press,
New York (2004)
21. Massey, J.L.: Linear codes with complementary duals. Discrete Math. 106(107),
337–342 (1992)
22. Ngo, X.T., Guilley, S., Bhasin, S., Danger, J.L., Najm, Z.: Encoding the state of
integrated circuits: a proactive and reactive protection against hardware trojans
horses. In: Proceedings of WESS 2014, 12–17 October 2014, New Delhi, India.
ACM, New York (2014)
23. Ngo, X.T., Bhasin, S., Danger, J.L., Guilley, S., Najm, Z.: Linear complementary
dual code improvement to strengthen encoded cirucit against Hardware Trojan
Horses. In: Proceedings of IEEE International Symposium on Hardware Oriented
Security and Trust (HOST): 2015 May 2015, Washington DC Metropolitan Area,
USA. IEEE, Piscataway (2015)
24. Pei, J., Zhang, X.: 1-generator quasi-cyclic codes. J. Syst. Sci. Complex. 20, 554–
561 (2007)
25. Rajan, B.S., Siddiqi, M.U.: Transform domain characterization of abelian codes.
IEEE Trans. Inform. Theory 38, 1817–1821 (1992)
26. S´eguin, G.: A class of 1-generator quasi-cyclic codes. IEEE Trans. Inform. Theory
50, 1745–1753 (2004)
27. Sendrier, N.: Linear codes with complementary duals meet the Gilber-Varshamov
bound. Discrete Math. 285, 345–347 (2004)
28. Yang, X., Massey, J.L.: The condition for a cyclic code to have a complementary
dual. Discrete Math. 126, 391–393 (1994)
29. Wan, Z.X.: Finite Fields and Galois Rings. World Scientiﬁc Pub. Co. Pte. Ltd.,
Singapore (2012)
30. Wasan, S.K.: Quasi abelian codes. Publ. Inst. Math. 35, 201–206 (1977)

Relative Generalized Hamming Weights
and Extended Weight Polynomials
of Almost Aﬃne Codes
Trygve Johnsen(B) and Hugues Verdure
UiT - The Arctic University of Norway, 9037 Tromsø, Norway
{Trygve.Johnsen,Hugues.Verdure}@uit.no
Abstract. This paper is devoted to giving a generalization from linear
codes to the larger class of almost aﬃne codes of two diﬀerent results.
One such result is how one can express the relative generalized Hamming
weights of a pair of codes in terms of intersection properties between the
smallest of these codes and subcodes of the largest code. The other result
tells how one can ﬁnd the extended weight polynomials, expressing the
number of codewords of each possible weight, for each code in an inﬁnite
hierarchy of extensions of a code over a given alphabet. Our tools will
be demi-matroids and matroids.
Keywords: Pairs of almost aﬃne codes · Relative generalized hamming
weights · Extended weight polynomials
1
Introduction
We will focus on almost aﬃne codes as deﬁned in [14], that is: C ⊂F n for some
ﬁnite alphabet F, and the projection CX has cardinality |F|s for a non-negative
integer s for each X ⊂{1, · · · , n}. It is well known that this is a class of codes,
which contain linear codes over ﬁelds F as a proper subclass. The intermediate
class of aﬃne codes are translates of linear codes. Another intermediate class is
that of multilinear codes. It is also well known ([14]) that C deﬁnes a matroid
MC through the rank function
r(X) = log|F | |CX|.
Such
codes
were
studied
in
connection
with
access
structures
over
E = {1, 2, · · · , n} and are strongly related to ideal perfect secret sharing schemes
for such access structures. See e.g. [14].
In this note we will demonstrate how two diﬀerent results for linear codes
can be generalized to ﬁnd analogous results for almost aﬃne codes in general.
First, we will recall some known results and terminology for almost aﬃne
codes in Sect. 2. Then, in Sect. 3, we will study generalized Hamming weight
(RLDP, in the sense of [2]) of pairs C2 ⊂C1 of almost aﬃne codes, and we
will investigate to which extent it is possible to generalize the results in [12,15],
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 207–216, 2017.
DOI: 10.1007/978-3-319-66278-7 17

208
T. Johnsen and H. Verdure
where one only treats linear codes. There one expresses these relative generalized
weights as the minimum weights of subcodes of C1 of various dimensions, inter-
secting C2 only in the zero element. In one of our two main results, Theorem 2,
we show an analogue of this result for almost aﬃne codes. In Remark 3 we show,
however, that the situation is not completely like in the case of linear codes.
There are many applications of relative generalized Hamming weights, as
referred to in [12,15], for pairs of linear codes. In addition, relating to secret
sharing schemes, which is a particularly natural topic, when working with almost
aﬃne codes, we would like to mention the signiﬁcance of relative generalized
Hamming weights described in [3,11].
In the last chapter, Sect. 4, we study another aspect of the relationship
between almost aﬃne codes and matroids. In [6], and in [5, p. 323], one points
out that for linear block codes of length n over a ﬁnite ﬁeld Fq, one can produce
an inﬁnite series of codes by extending the alphabet to Fqs, for s = 1, 2, · · · , and
nevertheless ﬁnd polynomials A0, · · · , An, such that Aj(qs) computes the num-
ber of codewords of weight j, for all s simultaneously, for each of j = 0, · · · , n.
We will show that a corresponding result holds for almost aﬃne codes, and we
use the arguments in [7, Sect. 3] as a stepping stone to ﬁnd weight polynomials
for a similar inﬁnite series of almost aﬃne codes Cs, all of the same block length,
but over growing alphabets F s as s grows. A main point in the linear case is
that the polynomials Aj are only dependent on the associated matroid of C,
and that we have matroids that play a completely analogous role, and that are
equally simple to handle in the general case of almost aﬃne codes.
2
Matroids, Demi-Matroids and Almost Aﬃne Codes
In this section, we essentially recall relevant material that will be needed in the
sequel, and we do not claim to have any new results here. We refer to [13] for
the theory of matroids, to [1] for an introduction on demi-matroids and to [14]
for an introduction on almost aﬃne codes, and we will use their notation.
2.1
Matroids and Demi-Matroids
A matroid is a combinatorial structure that extend the notion of linear
(in)dependency. There are many equivalent deﬁnitions, but we will give just
one here.
Deﬁnition 1. A matroid is a pair M = (E, r) where E is a ﬁnite set, and r a
function on the power set of E into N satisfying the following axioms:
(R1) r(∅) = 0,
(R2) for every subset X ⊂E and x ∈E, r(X) ⩽r(X ∪{x}) ⩽r(X) + 1,
(R3) for every X ⊂E and x, y ∈E, if r(X) = r(X ∪{x}) = r(X ∪{y}), then
r(X ∪{x, y}) = r(X).
Demi-matroids were introduced in [1]. They are a generalization of matroids
in the following way:

Relative Generalized Hamming Weights and Extended Weight Polynomials
209
Deﬁnition 2. A demi-matroid is a pair M = (E, r) where E is a ﬁnite set,
and r a function on the power set of E into N satisfying axioms (R1) and (R2)
above. The rank of M is r(E).
Matroids and demi-matroids have duals deﬁned in the following way:
Proposition 1. Let M = (E, r) be a matroid (respectively a demi-matroid).
Then M ∗= (E, r∗) with r∗deﬁned as
r∗(X) = |X| + r(E\X) −r(E)
is a matroid (respectively a demi-matroid). Moreover, (M ∗)∗= M.
The matroid (respectively demi-matroid) M ∗is called the dual (respectively
the dual or ﬁrst dual) of M. It has rank |E|−r(M). Demi-matroids have another
dual, called the supplement dual or second dual. See [1, Theorem 4]:
Proposition 2. Let M = (E, r) be a demi-matroid. Then M = (E, r) with r
deﬁned as
r(X) = r(E) −r(E\X)
is a demi-matroid. Moreover, we have M = M and M ∗= M
∗.
2.2
Almost Aﬃne Codes
Almost aﬃne codes were ﬁrst introduced in [14], and are a combinatorial gener-
alization of aﬃne codes.
Deﬁnition 3. An almost aﬃne code over a ﬁnite alphabet F, of length n and
dimension k, is a subset C ⊂F n such that |C| = |F|k and such that for every
subset X ⊂E = {1, · · · , n},
log|F | |CX| ∈N,
where CX is the puncturing of C with respect to E\X.
An almost aﬃne subcode of C is a subset D ⊂C which is itself an almost
aﬃne code over the same alphabet.
Remark 1. Any linear or aﬃne code is obviously an almost aﬃne code.
To any almost aﬃne code C of length n and dimension k on the alphabet
F, we can associate a matroid MC on the ground set E = {1, · · · , n} and with
rank function
r(X) = log|F | |CX|,
for X ⊂E.

210
T. Johnsen and H. Verdure
Deﬁnition 4. Let C be a block code of length n, and let c ∈C be ﬁxed. The
c-support of any codeword w is
Supp(w, c) = {i, ci ̸= wi}.
The c-support of C is
Supp(C, c) =

w∈C
Supp(w, c).
Note that the c-support of an almost aﬃne code is independent of the choice
of c ∈C (see [8, Lemma 1]), and it will therefore be denoted by Supp(C) without
reference to any codeword. This observation gives rise to:
Deﬁnition 5. The weight of an almost code C is w(C) = |Supp(C)|.
Deﬁnition 6. Let C be an almost aﬃne code of length n, and let c ∈F n be
ﬁxed. Then
C(X, c) = {w ∈C, wX = cX},
where cX is the projection of w to X. Such a subcode of C is called a standard
subcode.
This might be empty, or not be an almost aﬃne code, but when we take
c ∈C, we get the following ([14, Corollary 1]):
Proposition 3. Let C be an almost aﬃne code of length n and dimension k
over the alphabet F. Let c ∈C. Let X ⊂{1, · · · , n}. Then C(X, c) is an almost
aﬃne subcode of C. Its asscociated matroid MC(X,c) is the contracted matroid
MC/X with rank function ρ given by
ρ(Y ) = r(X ∪Y ) −r(X)
where r is the rank function of the matroid MC. In particular,
|C(X, c)| = |F|k−r(X).
Remark 2. Not all subcodes of C are of the form C(X, c), i.e. not all subcodes
are standard subcodes.
Corollary 1. Every almost aﬃne code C of dimension k has almost aﬃne sub-
codes of dimension 0 ⩽i ⩽k.
2.3
Generalized Hamming Weights
For a demi-matroid D = (E, r) of rank n −k we deﬁne:
Deﬁnition 7. The generalized Hamming weights for a demi-matroid of dimen-
sion k are
mi(D) = min{|X|, n(X) = |X| −r(X) = i}
for 1 ⩽i ⩽k.

Relative Generalized Hamming Weights and Extended Weight Polynomials
211
Deﬁnition 8. The generalized Hamming weights for an almost aﬃne code C of
dimension k are
di(C) = mi(M ∗
C) = min{|X|, |X| −r∗(X) = r(X) = i}
for 1 ⩽i ⩽k, where r∗is the rank function of M ∗
C, and r is the rank function
of MC.
In fact the following was proved in [9]:
Theorem 1. Let C be an almost aﬃne code of length n and dimension k on
an alphabet F of cardinality q, and let c ∈C. Then the generalized Hamming
weights for C are
di(C) = min{|Supp(D)|, D is an almost aﬃne subcode of dim. i of C}
= min{|Supp(D)|, D is a standard subcode of dim. i of C}
= n −max{|X|, |C(X, c)| = qj},
for 1 ⩽i ⩽k.
3
Equivalent Formulations of Some Hamming Weights
of Pairs of Codes
From [10] we have:
Proposition 4. Let C2 ⊂C1 be two almost aﬃne codes with rank functions r2
and r1. Then the pair (E, ρ) is a demi-matroid, for ρ = r1 −r2.
Deﬁnition 9. For 0 ⩽i ⩽dim C1 −dim C2, we deﬁne the RLDP (Relative
Length/Dimension Proﬁle), or relative generalized Hamming weight, of the pair
(C1, C2) as follows:
mi = min{|X|, ρ(X) = i}.
We observe that if C2 = 0, this is the di associated to C1.
For linear codes the most usual way in the literature (see e.g. [12]) to express
the mi is perhaps as
min{|X|| dim(C1(E\X, 0) −dim(C2(E\X, 0) ⩾i}.
It is easy to see that our Deﬁnition 9 above of the mi gives the same values for
linear codes. In [12, Lemma 1], and [15, Proposition 2], however, one gives an
alternative, and less trivial reformulation of the mi for linear codes. This is as:
min{w(D), |D ∩C2| = 1, D ⊂C1 is a linear subcode with dim D = i}.
Another, similar variant is given in [15, Prop.4]. We will now investigate the
possibility of a reformulation of the mi in analogous ways, not only for linear
codes, but for almost aﬃne codes in full generality. Our result is given in the
theorem below:

212
T. Johnsen and H. Verdure
Theorem 2. Let C2 ⊂C1 be a pair of almost aﬃne codes with associated demi-
matroid (E, ρ). Then for 0 ⩽i ⩽dim C1 −dim C2,
mi = min{w(D), |D ∩C2| = 1, D ⊂C1 is a standard subcode with dim D = i}.
Proof. Let bi be the right hand side of the above equality. We ﬁx v ∈C2. All the
standard subcodes, and all supports, considered in this proof, are with respect
to v, and we omit its reference in the rest of the proof. For simplicity, denote
kj = dim Cj and rj = rCj for j = 1, 2.
Let X ⊂E be such that |X| = mi and ρ(X) = i, that is
k1 −r1(E\X) −k2 + r2(E\X) = i
or equivalently
dim C1(E\X) −dim C2(E\X) = i.
We have the obvious inclusions
Supp(C2(E\X)) ⊂Supp(C1(E\X)) ⊂X.
We claim that the second inclusion is actually an equality. Indeed, if not, let
y ∈X\Supp(C1(E\X)) and consider Y = X\{y}. Then, for j = 1, 2, if w ∈
Cj(E\X), wy = vy since y is not in the support, and in turn, the natural
inclusions
Cj(E\Y ) ⊂Cj(E\X)
are equalities. This contradicts the minimality of X since Y also satisﬁes ρ(Y ) =
ρ(X) = i.
Let Z ⊂E\X be a maximal independent subset of E\X for the matroid
MC2, that is
|Z| = r2(Z) = r2(E\X).
Let Z′ ⊂X be such that Z ∪Z′ is a basis of MC2. Obviously, we have
r2(Z′) = |Z′| = k2 −|Z| = k2 −r2(E\X).
Let W = X −Z. Note that Z ∪Z′ ⊂E\W. Then
C2 ∩C1(E\W) = {v}.
Namely,
v ∈C2 ∩C1(E\W) = C2(E\W)
and
dim C2(E\W) = k2 −r2(E\W) = 0.
Moreover, we have
r1(E\W) ⩾r1(E\X) + |Z′|
⩽k1 −k2 + r2(E\X) −i + k2 −r2(E\X)
⩽k1 −i

Relative Generalized Hamming Weights and Extended Weight Polynomials
213
that is,
dim C1(E\W) ⩾i.
Take now any standard subcode of C1(E\W) of dimension i. Then of course we
have
v ∈D ∩C2 ⊂C1(E\W) ∩C2 = {v}
and
Supp(D) ⊂Supp(C1(E\W)) ⊂Supp(C1(E\X)) = X,
which implies that
bi ⩽Supp(D) ⩽|X| = mi.
For the converse, let Y ⊂E be such that |C1(Y )∩C2| = 1. Then C1(Y )∩C2 =
{v}. Assume that w(C1(Y )) = bi and dim C1(Y ) = i. Let Y ′ = E\Supp(C1(Y )).
Obviously, Y ⊂Y ′. Let w ∈C1(Y ). For any y ∈Y ′, y ̸∈Supp(C1(Y )) so that
wy = vy, and in turn w ∈C1(Y ′). Hence, the natural inclusion C1(Y ′) ⊂C1(Y )
is actually an equality.
Let X = E −Y ′. Then we have
|X| = |E\Y ′| = |E\(E\Supp(C1(Y ))| = |Supp(C1(Y ))| = bi
and
C2(E\X) = C2(Y ′) = C2 ∩C1(Y ′) = C2 ∩C1(Y ) = {v},
which implies that
dim C1(E\X) −dim C2(E\X) = dim C1(Y ′) −0 = dim C1(Y ) = i
and ﬁnally
mi ⩽|X| = bi.
3.1
An Open Question Concerning Subcodes
Remark 3. Let
b′
i = min{w(D), |D ∩C2| = 1, D ⊂C1 is a subcode with dim D = i},
that is we allow D to be any subcode, not only a standard subcode. Obviously,
for 1 ⩽i ⩽k1 −k2, we have
mi = bi ⩾b′
i.
It is an open question whether the last inequality is an equality, and will be the
topic of further research. For linear codes, [15, Proposition 2] gives an analogous
statement with equality. On the other hand, while bi is deﬁned just for 0 ⩽i ⩽
k1−k2 (for i > k1−k2 it is not diﬃcult to show that any standard subcode of C1
will have a non-trivial intersection with C2), b′
i might be deﬁned for i > k1 −k2.
Consider namely the following codes: let F = {0, 1, 2, 3} and C1 = F 3. Let C2
and D be the subcodes
{000, 012, 023, 031, 103, 110, 121, 132, 201, 213, 222, 230, 302, 311, 320, 333}

214
T. Johnsen and H. Verdure
and
{000, 011, 022, 033, 102, 113, 120, 131, 203, 210, 221, 232, 301, 312, 323, 330}
respectively. Both subcodes have dimension 2, while C1 has dimension 3. But
we have C2 ∩D = {000} and dim D > dim C1 −dim C2 = 1. Hence b′
2 is deﬁned
(and is at most 3), while m2 and b2 could be said to be ∞if one insists on
deﬁning them.
4
Extended Weight Polynomials of Almost Aﬃne Codes
In [6], and in [5, p. 323], one points out that for linear block codes of length
n over a ﬁnite ﬁeld Fq, one can produce an inﬁnite series of codes by extend-
ing the alphabet to Fqs, for s = 1, 2, · · · , and nevertheless ﬁnd polynomials
A0, · · · , An, such that Aj(qs) computes the number of codewords of weight j,
for all s simultaneously, for each of j = 0, · · · , n. Hence knowledge of a ﬁnite
number of coeﬃcients of the Aj compute an inﬁnite number of weights. We
will show that a corresponding result holds for almost aﬃne codes, and we will
mimick the arguments in [7, Section 3] to ﬁnd weight polynomials for an inﬁnite
series of almost aﬃne codes Cs, which we will now deﬁne.
Let q = |F|, where F is the alphabet over which an almost aﬃne code C of
block length n is deﬁned. Then Cs is a code of block length n over the alphabet
F s, if an element ((c1,1, · · · , c1,n), · · · , (cs,1, · · · , cs,n)) instead is interpreted as:
((c1,1, · · · , cs,1), · · · , (c1,n, · · · , cs,n)).
(1)
It is then automatic that |(Cs)X| = (qs)r if |CX| = qr, for some X ⊂E =
{1, 2, · · · , n}, and natural number r. Hence Cs is an almost aﬃne code over F s,
since C is an almost aﬃne code over F. Moreover the matroid MCs = MC since
the rank functions are the same. Call the rank function r. Put k = r(E).
Let U ⊂E, and let cQ be a ﬁxed codeword in Cs. Similarly as in [7] we
deﬁne: SU(s) is the subset of Cs, viewed over F s, with the same coordinates
as cQ in the positions corresponding to U, in other words SU(s) = Cs(U, cQ).
But, since Cs is an almost aﬃne code we see that |SU(s)| = (qs)k−r(U). In the
next deﬁnition, there is no explicit reference to the codeword cQ, since this is
independent of the word chosen.
Deﬁnition 10. For each j = 1, · · · , n let AC,j(s) be the number of codewords
of weight j in Cs.
Using the exclusion/inclusion principle we obtain the same formula as in
[7, Formula (9) p. 638]:
AC,n(s) = (−1)n 
U⊂E
(−1)|U|(qs)n∗(U).
Here n∗(Y ) = |Y | −r∗(Y ) is the nullity function associated to the dual matroid
(E, r∗) of MC.
For each X ⊂E let aX,C(s) be the number of codewords with support exactly
X. We then obtain in a similar way:

Relative Generalized Hamming Weights and Extended Weight Polynomials
215
Lemma 1.
aX,C(s) = (−1)|X| 
U⊂X
(−1)|U|(qs)n∗
X(U),
where n∗
X is the nullity function of the dual of the rank function associated to
the code C(E\X, cQ).
A reﬁned study, using Proposition 3, also gives
Lemma 2. For any U ⊂X we have: n∗
X(U) = n∗(U).
Combining Lemmas 1 and 2 we then obtain an analogous formula as in
[7, p. 638], and obtain:
This gives:
Proposition 5. For each j = 0, 1, · · · , n there are polynomials
AC,j(s) = (−1)j 
|X|=j

Y ⊂X
(−1)|Y |(qs)n∗(Y ).
counting the number over codewords of weight j in Cs.
In [7, Sects. 4 and 5], one shows how this matroid expression can be expressed
by N0-graded Betti numbers of the Stanley-Reisner rings of the matroid M ∗
C
and its elongations, viewed as simplicial complexes via their independence sets
([7, Theorem 5.1]). From the arguments above we now see that its concequence,
[7, Corollary 5.1], formulated for linear codes in that corollary, carries over to
almost aﬃne codes, except that the matroid M(H) appearing in [7, Corollary
5.1], must be replaced by the matroid dual M ∗
C. See also [7, Proposition 4.1],
which can be applied to determine the generalized Hamming weights for almost
aﬃne codes from the degrees of the polynomials Aj(s).
We also observe:
Example 1. As one sees from Proposition 5 the formula for AC,j(s) is only depen-
dent on the polynomial (in the variable Q)
Pj(Q) = (−1)j 
|X|=j

Y ⊂X
(−1)|Y |(Q)n∗(Y ),
which is deﬁned for any (demi-)matroid, since it only uses its dual nullity
function.
Let C be the almost aﬃne code in [14, Example 2]. This is a code of rank
3 over the alphabet F2
3 of cardinality 9. Its length is also 9, and its well known
([14, Example 2]) that its associated matroid MC is the non-Pappus matroid. In
[4, p. 102] one calculated the polynomials Pj(Q), for j = 0, · · · , 9 without relating
them to any code, since one knew that this matroid is not linearly representable.
The results, however, automatically carry over to determining the AC,j(s) for
the non-linear almost aﬃne code C, and we obtain from [4, p. 102], or from usual
inclusion/exclusion methods:

216
T. Johnsen and H. Verdure
AC,0(s) = 1,
AC,1(s) = AC,2(s) = AC,3(s) = AC,4(s) = AC,5(s) = 0,
AC,6(s) = 8qs −8,
AC,7(s) = 12qs −12,
AC,8(s) = 3q2s −18qs + 15,
AC,9(s) = q3s −9q2s + 28qs −20.
q3s = |Cs|.
References
1. Britz, T., Johnsen, T., Mayhew, D., Shiromoto, K.: Wei-type duality theorems for
matroids. Des. Codes Cryptogr. 62(3), 331–341 (2012)
2. Forney, G.F.: Dimension/length proﬁles and trellis complexity of linear block codes.
IEEE Trans. Inform. Theory 40(6), 1741–1752 (1994)
3. Geil, O., Martin, S., Matsumoto, R., Ruano, D.: Relative generalized Hamming
weights of one-point algebraic geometric codes. IEEE Trans. Inform. Theory
60(10), 5938–49 (2014)
4. Huerga Represa, V.: Towers of Betti Numbers of Matroids and Weight Distribution
of Linear Codes and their Duals, Master’s thesis in Pure Mathematics, University
of Tromsø - The Arctic University of Norway (2015). http://hdl.handle.net/10037/
7736
5. Jurrius, R.P.M.J.: Weight enumeration of codes from ﬁnite spaces Des. Codes
Crypt. 63(3), 321–330 (2012)
6. Jurrius, R.P.M.J., Pellikaan, G.R.: Algebraic geometric modeling in information
theory. In: Codes, arrangements and matroids. Seroes on Coding Theory and Cryp-
tology. World Scientiﬁc Publishing, Hackensack (2001)
7. Johnsen, T., Roksvold, J., Verdure, H.: Generalized weight polynomials of
matroids. Discrete Math. 339(2), 632–645 (2016)
8. Johnsen, T., Verdure, H.: Hamming weights of linear codes and Betti numbers of
Stanley-Reisner rings associated to matroids. Appl. Algebra Engrg. Comm. Com-
put. 24(1), 73–93 (2013)
9. Johnsen, T., Verdure, H.: Generalized Hamming weights for almost aﬃne codes.
IEEE Trans. Inform. Theory 63(4), 1941–1953 (2017)
10. Johnsen, T., Verdure, H.: Flags of almost aﬃne codes, arXiv:1704.02819 (2017)
11. Kurihara, J., Uyematsu, T., Matsumoto, R.: Secret sharing schemes based on linear
codes can be precisely characterized by the relative generalized Hamming weights.
IEICE Trans. Fundam. Electron. Commun. Comput. Sci. 95(11), 2067–75 (2012)
12. Liu, Z., Chen, W., Luo, Y.: The relative generalized Hamming weight of linear
q-ary codes and their subcodes. Des. Codes Crypt. 48(2), 111–123 (2008)
13. Oxley, J.G.: Matroid Theory. Oxford University Press, New York (1992)
14. Simonis, J., Ashikhmin, A.: Almost aﬃne codes. Des. Codes Crypt. 14(2), 179–197
(1998)
15. Zhuang, Z., Dai, B., Luo, Y., Han-Vinck, A.J.: On the relative proﬁles of a linear
code and a subcode. Des. Codes Crypt. 72(2), 219–247 (2014)

On the Performance of Block Woven Codes
Constructions with Row-Wise Permutations
Alexey Kreshchuk(B)
, Igor Zhilin
, and Victor Zyablov
Laboratory 3, Institute for Information Transmission Problems,
Russian Academy of Science, Moscow, Russia
{krsch,zhilin,zyablov}@iitp.ru
Abstract. In this paper we propose a woven block code construction
based on two convolutional codes. We also propose a soft-input decoder
that allows this construction to have better error correction performance
than the turbo codes with a conventional decoder. Computer simulation
has showed a 0.1 dB energy gain relative to the LTE turbo code. Asymp-
totically the proposed code has distance greater than the product of free
distances of component codes.
1
Introduction
Evolution of communication systems’ standards requires improving coding gain
and achieving new goals along with keeping complexity of the proposed codecs
low enough. One of the ways to achieve these goals is to carefully design con-
catenated codes. It allows one to use simpler codes like convolutional codes as
constituent codes to reduce implementation cost and time.
Our main goal is to design a code construction that would have better error
correction performance than the widely used LTE turbo code.
In this paper we propose a block woven code construction that is based on
two convolutional codes. Like turbo codes woven codes are a particular case of
the concatenated codes. Earlier works mostly considered normal woven codes [8]
and direct product convolutional codes [10] which are both convolutional codes.
This makes it hard to improve the error-correction performance by introducing
interleavers like it was done in turbo codes. In this work we also propose a
block woven code with row-wise permutations that has better error correction
performance than a block woven code without additional permutations. A similar
construction without row-wise permutations and its distance properties were
studied in [11].
This work is organized as follows. In Sect. 2 we describe the proposed code
construction and the encoding algorithm. Section 3 is devoted to describing the
proposed decoding algorithm. In Sect. 4 we study distance properties of the pro-
posed code. In Sect. 5 we present results of computer simulation of proposed
code and its soft decoder.
This work has been supported by RScF, research project No. 14-50-00150. This
work was carried out using high-performance computing resources of federal center
for collective usage at NRC “Kurchatov Institute”, http://computing.kiae.ru/.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 217–227, 2017.
DOI: 10.1007/978-3-319-66278-7 18

218
A. Kreshchuk et al.
2
Block Woven Code Construction and Encoding
Woven convolutional codes were introduced in 1997 [8]. Their distance properties
and encoder design are studied in [7] and their error rates and decoder design
are studied in [9].
An important reason to consider woven codes is the existence of woven con-
volutional codes with distance growing linearly with the number of constituent
codes [4] whereas the distance of turbo codes always grows sublinearly [2].
Two block code constructions similar to the one considered in this paper were
described in [3,5]. Let us describe the main diﬀerences. In [3] Serially Concate-
nated Block Codes use a random interleaver whereas the code considered uses a
combination of matrix transpose and a row-wise permutation. In [5] each row of
the Woven Block Code (with outer warp) word was terminated separately and
the code considered has all rows encoded as one word of the outer code. Overall
the code we propose in this work can be considered as a combination of ideas
behind these two papers.
We consider a block code that uses terminated convolutional codes as con-
stituent codes. Let us ﬁrst make simpliﬁed description of the proposed woven
code that doesn’t account for parity check symbols introduced by termination
and row-wise permutations that generate an ensemble of woven block codes.
2.1
Simpliﬁed Deterministic Description
Let us describe a simpliﬁed construction. We consider this construction with
two constituent codes that can be the same code. This construction was earlier
described in [12]. It does not picture termination symbols and row-wise permu-
tations.
Let us write information symbols of this code as matrix:
I =
kB
kA
At ﬁrst the information matrix is read in row-wise order and encoded by the
outer convolutional coder, I =
kB
kA

On the Performance of Block Woven Codes Constructions
219
The resulting matrix is written in row-wise order too, IA = EncB(I) =
nB
kA
Information sequence is terminated in a usual way. White cells represent
information symbols and grey cells represent parity-check symbols. It is worth
noting that all this matrices are processed row-by-row by a single convolutional
encoder. This diﬀers, for example, from the work [6] where authors considered
encoding all rows by several independent encoders.
Then IA is read in column-wise order by inner convolutional code encoder,
IA =
nB
kA
And written in the same column-wise order to a matrix that is a codeword,
C = EncA(IA) = EncA(EncB(I)) =
nB
nA
The result is a codeword:
C = EncA(EncB(I)) =
nB
nA
Shaded cells correspond to parity-check symbols.

220
A. Kreshchuk et al.
Note: a single encoder is used for encoding all rows. Then a single encoder is
used for encoding all columns.
2.2
Proposed Code Ensemble
In a more precise description matrix I has a couple of termination symbols (they
are in fact parity check symbols) in the lower right corner:
I =
kB
kA
We introduce an ensemble of woven block codes by applying random permu-
tations to each row of matrix IA to get I′
A. The permutation for diﬀerent rows
are selected independently and uniformly on the ensemble of all permutations.
I′
A =
nB
kA
Note, that the last row of IA has more parity check symbols than the others.
Matrix C has some additional symbols introduced by termination that do not
ﬁt into the matrix.
C =
nB
nA
3
Decoding Algorithm
The proposed decoding algorithm is iterative decoding algorithm based on
sequential decoding of inner and outer codes by BCJR [1] soft-input soft-output
algorithm. It has some resemblance of turbo codes’ decoding algorithm in gen-
eral, but has some signiﬁcant diﬀerences due to speciﬁc features of the construc-
tion under consideration. The main diﬀerence is the fact that the inner and
outer codes have diﬀerent length, thus the decoder of the inner code can not get
extrinsics for its parity-check symbols from the outer code.
We won’t stop on the details of the BCJR decoding implementation since it
is a very well-known decoder. Let us only note that BCJR decoder can output

On the Performance of Block Woven Codes Constructions
221
BCJR, inner
+
+
BCJR, outer
+
+
+
-
-
Fig. 1. Decoder scheme
LLRs for either each code symbol or each information symbol. Both modes will
be used in the proposed decoder.
Let us describe decoding of the construction in general. Let us denote all
decoder outputs by o and the inputs by i. The channel output is ochannel. The
decoder is iterative and each iteration is split into these steps (schematically
depicted on Fig. 1):
1. Compute input for the inner code. For information symbols it is:
iinner,i
n
= ochannel + oouter
n−1 ,
where oouter
n−1 is the extrinsics of the outer code on the previous iteration. For
parity check symbols it is just:
iinner,pc
n
= ochannel.
2. Decode the inner code and get LLRs per information symbol:
oinner
n
= DecA(iinner
n
) −oouter
n−1 .
3. If this is not the last iteration, decode the outer code and get LLRs per code
symbol:
oouter
n
= DecB(oinner
n
) −oinner
n
.
4. If this is the last iteration, decode the outer code and stop:
o = DecB(oinner
n
).
This algorithm has hard output and it always performs the same number of
iterations. On the ﬁrst iteration oouter
0
= 0.
There are two main diﬀerences between the proposed algorithm and the turbo
decoder arising from the diﬀerences between their codes.
1. On step 2 the decoder does not output the extrinsics. That is a simple way
to add nA channel LLRs to the nB outer code symbols.
2. On step 3 we get LLRs for each code symbols, not just for information sym-
bols.

222
A. Kreshchuk et al.
4
Distance Properties
Let us ﬁnd the probability that the proposed code would have distance greater
then dAdB. To do this we must ﬁrst estimate the number of segments of length
nA taken from words of inner code of weight not greater than dA.
There are generally three kinds of these code segments:
1. having zero syndrome both at the start and at the end,
2. having nonzero syndrome either at the start or at the end,
3. having nonzero syndrome both at the start and at the end.
The number of segments belonging to the last two kinds is limited by some
constant c0 due to increasing active distance of the inner code. The number of
segments belonging to the ﬁrst kind is less than nAc1, where c1 is the number
of words of minimal weight. For convolutional code deﬁned by its generator
polynomials (7, 5), c1 = 1.
Next let us calculate the probability that a random permutation will map
a word of weight w and length n to a speciﬁed word. The number of such
permutations is w!(n −w)!, and the number of all permutations is n!. Therefore
this probability is pp =
n
w
−1.
Every word of the proposed code of weight dAdB has a dense submatrix of
size dB × dA (with all elements equal to 1). The corresponding matrix I′
A has
a dense submatrix of size wB × dA, where wB is the weight of the information
sequence generating a outer code sequence of minimal weight. For code (7, 5),
wB = 3. The proposed code will contain no such words if all matrices IA with
wB rows of weight dA are permuted so that no dense submatrix. The number of
these matrices is:
NIA ≤
nB
wB

(c0 + c1nA).
The probability that all nonzero rows of a row-wise permutation of matrix
IA will have ones at the same positions is:
p1 = pwB−1
p
=
nA
dA
−(wB−1)
The probability the all matrices IA with wB rows of weight dA are permuted
so that no dense submatrix of size dA × dB exist is:
pg ≥1 −NIAp1 ≥1 −
nB
wB

(c0 + c1nA)
nA
dA
−(wB−1)
So if limnA,nB→∞NIAp1 = 0 then most of the codes don’t have codewords
of weight dAdB. Let’s compute this limit for nB = αnA:
NIAp1 ≤
nB
wB

(c0 + c1nA)
nA
dA
−(wB−1)
≤nwB
B
wB! c1nA
nA
dA
−(wB−1)dA
= c1d(wB−1)dA
A
αwB
wB!
nwB+1−(wB−1)dA
A

On the Performance of Block Woven Codes Constructions
223
For wB ≥2 and dA ≥3 this expression tends to zero with nA →∞. We can
state this fact as a theorem:
Theorem 1. If wB ≥2 and dA ≥3 then for nA →∞, nB →∞, nA
nB →const
the probability of the proposed code having distance dAdB or less tends to zero.
Proof. Described earlier in this section.
5
Numerical results
We implemented the decoding algorithm described above and measured its per-
formance for selected codes.
The codes were simulated in an additive white gaussian noise (AWGN) chan-
nel with quadrature phase shift keying (QPSK).
The following code parameters were chosen: kA = 15, nA = 30, kB = 24,
nB = 48. Woven code has dimension K = kAkB −νA = 358, length N = nAnB +
2νB = 1444 and rate R = 0.248, where ν = 2 denotes the delay of convolutional
code. Both inner and outer codes have generator polynomials (7, 5).
−3
−2.8
−2.6
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
−1
10−3
10−1
SNR, dB
Frame Error Rate
8 iterations
16 iterations
50 iterations
LTE turbo code
Fig. 2. Comparison with the LTE turbo code. Component codes are (7, 5) convolutional
codes.
Figure 2 compares frame error rates between the proposed code and the LTE
turbo code. LTE turbo code has k = 352, n = 1440 and rate 0.244. Its decoder
used 8 iterations. Its constituent codes’ generator polynomials are (13, 15) there-
fore their trellises have twice as many states as the one for (7, 5). That’s why
it is reasonable to compare the LTE turbo codes performance to the proposed
woven code decoded with 16 iterations. Then the energy gain of the proposed
code relative to the LTE is about 0.1 dB. Increasing the number of iterations
allows for greater energy gains.
Figure 3 shows the corresponding bit error rates. The constituent codes are
(7, 5) convolutional codes as in previous ﬁgure. Its main point is the intersection
of decoder BER with the channel BER. From its location we can conclude that
the decoder starts improving error rates at SNR −3.25 dB.

224
A. Kreshchuk et al.
−3
−2.8
−2.6
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
−1
10−4
10−2
100
SNR, dB
Bit Error Rate
8 iterations
16 iterations
50 iterations
Without coding
Fig. 3. Bit error rate depending on the number of iterations. Component codes are
(7, 5) convolutional codes.
−3
−2.8
−2.6
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
−1
10−3
10−1
SNR, dB
Bit Error Rate
15 17
15 17 permutation
7 5
7 5 permutation
Without coding
Fig. 4. Bit error rate depending on the constituent code with and without row-wise
permutations
Figure 4 displays the eﬀect of the row-wise permutations in the code construc-
tion: the diﬀerence in slope. The construction with (7, 5) constituent codes and
row-wise permutations has much steeper slope than the one without permuta-
tions. Supposedly, it is due to an increase in the code distance. Simulation showed
no diﬀerence in slope for the construction with (15, 17) constituent codes, but
it is possible that this diﬀerence will appear at higher SNR. The permutations
were selected at random, without any heuristics or picking.
Let us look at Fig. 4 to compare the error correction performance for diﬀerent
constituent codes. The construction with (7, 5) constituent code starts correcting
errors 0.25 earlier than the one with (15, 17), but without row-wise permutations
it has less steep slope. These permutations allow (7, 5) construction to have
the same slope as the (15, 17) construction. Let us study the error correction
performance of the constituent codes separately.
The diﬀerence between error rates of (7, 5) and (15, 17) convolutional codes
might seem very small on Fig. 5, but as we have seen earlier it has huge eﬀect
on the proposed construction error rates. The (15, 17) code has better error

On the Performance of Block Woven Codes Constructions
225
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
10−2
10−1
SNR, dB
Bit Error Rate
15 17
7 5
Without coding
Fig. 5. Component codes bit error rates
−3
−2.8
−2.6
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
−1
0.1
0.2
0.3
SNR, dB
Bit Error Rate
15 17
7 5
Without coding
Fig. 6. Component codes bit error rates, enlarged
−3
−2.8
−2.6
−2.4
−2.2
−2
−1.8
−1.6
−1.4
−1.2
−1
10−3
10−1
SNR, dB
Frame Error Rate
Length 1444
Length 12964
Fig. 7. Error rates depending on code length.
correction performance for SNR above 0.25 dB and we are conﬁdent that the
proposed code with (15, 17) constituent code would have lower error rates in this
SNR range. Unfortunately it is very hard to test with computer simulation as
these rates seem to be very small. The fact that without row-wise permutations
(15, 17) construction has lower BER than (7, 5) construction above −2 dB might

226
A. Kreshchuk et al.
be due to fact that on later iterations the eﬀective channel is “better” than
0.25 dB AWGN. The same bit error rates are plotted enlarged on Fig. 6.
Figure 7 shows that increasing the length of the code makes the slope steeper.
The shorter code is the same as in Fig. 2, the longer one has kA = 72, nB = 90
and rate 0.25.
6
Conclusion
We have presented new block woven code construction with random row-wise
permutations. We consider it a competitor to the turbo codes. We also propose
a decoder for the proposed construction. The error correction performance was
studied through a computer simulation. It has shown a 0.1 dB energy gain rela-
tive to the LTE turbo code at rate 0.24 and length 1440. We have proved that the
code distance of the proposed code is asymptotically greater than the product
of free distances of the component codes.
References
1. Bahl, L., Cocke, J., Jelinek, F., Raviv, J.: Optimal decoding of linear codes for
minimizing symbol error rate (corresp.). IEEE Trans. Inf. Theory 20(2), 284–287
(1974). doi:10.1109/tit.1974.1055186
2. Bazzi, L., Mahdian, M., Spielman, D.A.: The minimum distance of turbo-like codes.
IEEE Trans. Inf. Theory 55(1), 6–15 (2009). doi:10.1109/tit.2008.2008114
3. Benedetto, S., Divsalar, D., Montorsi, G., Pollara, F.: Serial concatenation of inter-
leaved codes: performance analysis, design, and iterative decoding. IEEE Trans.
Inf. Theory 44(3), 909–926 (1998). doi:10.1109/18.669119
4. Bocharova, I.E., Kudryashov, B.D., Johannesson, R., Zyablov, V.V.: Asymptoti-
cally good woven codes with ﬁxed constituent convolutional codes. In: Proceeding
of ISIT 2007. IEEE, June 2007. doi:10.1109/isit.2007.4557169
5. Freudenberger, J., Bossert, M., Zyablov, V., Shavgulidze, S.: Woven codes with
outer warp: variations, design, and distance properties. IEEE J. Sel. Areas Com-
mun. 19(5), 813–824 (2001). doi:10.1109/49.924865
6. Gazi, O., Yilmaz, A.O.: Turbo product codes based on convolutional codes. ETRI
J. 28(4), 453–460 (2006). doi:10.4218/etrij.06.0105.0187
7. H¨ost, S., Johannesson, R., Zyablov, V.: Woven convolutional codes. I. encoder
properties. IEEE Trans. Inf. Theory 48(1), 149–161 (2002). doi:10.1109/18.971745
8. H¨ost, S., Johannesson, R., Zyablov, V.V.: A ﬁrst encounter with binary woven
convolutional codes. In: 4th International Symposium on Communication Theory
and Applications (1997)
9. Jordan, R., H¨ost, S., Johannesson, R., Bossert, M., Zyablov, V.: Woven convo-
lutional codes II: decoding aspects. IEEE Trans. Inf. Theory 50(10), 2522–2529
(2004). doi:10.1109/tit.2004.834790
10. Sidorenko, V., Bossert, M., Vatta, F.: Properties and encoding aspects of direct
product convolutional codes. In: Proceeding ISIT 2012, pp. 2351–2355. IEEE, July
2012. doi:10.1109/isit.2012.6283934
11. Zhilin, I., Kreshchuk, A., Zyablov, V.: On the code distance of a woven block
code construction. In: 2017 IEEE International Symposium on Information Theory
(ISIT 2017), Aachen, Germany, pp. 16–20, June 2017

On the Performance of Block Woven Codes Constructions
227
12. Zhilin, I., Zyablov, V., Zigangirov, D.: A binary block concatenated code based on
two convolutional codes. In: Fifteenth International Workshop on Algebraic and
Combinatorial Coding Theory ACCT2016, pp. 307–312, June 2016

New Lower Bounds on Error-Correcting
Ternary, Quaternary and Quinary Codes
Antti Laaksonen(B) and Patric R.J. ¨Osterg˚ard
Department of Communications and Networking, School of Electrical Engineering,
Aalto University, P.O. Box 15400, 00076 Aalto, Finland
{antti.2.laaksonen,patric.ostergard}@aalto.fi
Abstract. Let Aq(n, d) denote the maximum size of a q-ary code with
size n and minimum distance d. For most values of n and d, only lower
and upper bounds on Aq(n, d) are known. In this paper we present 19
new lower bounds where q ∈{3, 4, 5}. The bounds are based on codes
whose automorphisms are prescribed by transitive permutation groups.
An exhaustive computer search was carried out to ﬁnd the new codes.
Keywords: Bounds on codes · Error-correcting codes · Transitive
groups
1
Introduction
A q-ary code C of length n is a subset of Zn
q where Zq = {0, 1, . . . , q −1}. Each
element c ∈C is called a codeword, and Zq is called the alphabet of C. The
size of C is |C|, and the minimum distance of C is mina,b∈C,a̸=b dH(a, b) where
dH denotes the Hamming distance. A q-ary code with length n, size M and
minimum distance d is called an (n, M, d)q code.
Let Aq(n, d) denote the maximum size of an (n, M, d)q code. As it is diﬃcult
to determine exact values of Aq(n, d), an important problem in coding theory is
to ﬁnd lower and upper bounds on the function. While binary codes have received
the most attention [1,4,17], also ternary [8], quaternary [5] and quinary [7] codes
have been studied.
Lower bounds on Aq(n, d) can be found by discovering codes: if there is
an (n, M, d)q code, then Aq(n, d) ≥M. Computer search techniques are often
used to ﬁnd such codes. However, as the search space is typically very large,
assumptions about the structure of the code are usually needed to make the
search eﬃcient enough.
One way to limit the search space is to assume that there are symmetries
in the code and only consider codes with prescribed automorphisms [12,14,19].
Such automorphisms may permute coordinates and coordinate values of code-
words. In [16], new binary codes were found by focusing on codes whose groups
of automorphisms are transitive permutation groups. In this paper, we extend
this approach to ternary, quaternary and quinary codes.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 228–237, 2017.
DOI: 10.1007/978-3-319-66278-7 19

New Lower Bounds
229
We carry out computer searches to systematically go through transitive per-
mutation groups and search for codes with automorphisms prescribed by those
groups. For a ﬁxed group of automorphisms, the problem of ﬁnding a large code
can be transformed into a graph problem where each vertex of the graph consists
of an orbit of codewords and each clique in the graph corresponds to a code with
the given automorphisms.
It turns out that several lower bounds on the maximum size of ternary,
quaternary and quinary error-correcting codes can be improved by creating codes
whose symmetries are prescribed by transitive permutation groups. We present
17 new codes, each of which yields a new lower bound on Aq(n, d) where q ∈
{3, 4, 5}. In addition, two more lower bounds can be derived from the new codes.
The structure of the rest of the paper is as follows: In Sect. 2, we discuss
the method using which we construct codes with prescribed automorphisms. In
Sect. 3, we describe the computer searches we used to ﬁnd the new codes. Finally,
in Sect. 4, we present the new lower bounds on Aq(n, d).
2
Code Construction
An automorphism of a code is a mapping from the code to itself that may
permute coordinates and coordinate values of codewords. The general idea in
our work is to search for codes with prescribed groups of automorphisms, i.e.,
to focus on codes that have certain symmetries.
In this context, it is convenient to represent codewords as sets of integers as
follows: Let [n] = {1, 2, . . . , n}. The representation of a q-ary codeword c1c2 · · · cn
is a set
{ckn + k | k ∈[n]},
so that each codeword is an n-element subset of [nq]. The idea in using this
representation is that we can permute both coordinates and coordinate values
of codewords using permutations of [nq].
Our general method to construct a q-ary code of length n is as follows: Let
G be a permutation group of degree nq such that the group has a block system
where each block is of the form
{k, n + k, . . . , (q −1)n + k}
where k ∈[n]. Such a group corresponds to a group of automorphisms of a q-ary
code of length n. The orbit of a codeword c is
{gc | g ∈G}.
We construct the code as a union of orbits of codewords. Thus, for each orbit,
we either include all words in the code or none of them.
Example: Let us construct a ternary code of length 4 whose group of auto-
morphisms is
G = ⟨(1 5 9)⟩.

230
A. Laaksonen and P.R.J. ¨Osterg˚ard
Here G determines that whenever we include a codeword c1c2c3c4 in the code,
we also include all other codewords of the form xc2c3c4 where x is any element.
For example, we may create a code
C = {0000, 1000, 2000, 0111, 1111, 2111, 0222, 1222, 2222}
that consists of 3 orbits whose representatives are 0000, 1111 and 2222.
To prove that Aq(n, d) ≥M, it suﬃces to ﬁnd a code with size M and
minimum distance d. This corresponds to ﬁnding a clique of weight M in a
graph that is generated as follows: Each vertex of the graph corresponds to an
orbit where the minimum distance of any two words is at least d. The weight of
such a vertex is the number of words in the orbit. There is an edge between two
vertices if the minimum distance between any two words in the corresponding
orbits is at least d.
Thus, to ﬁnd a maximum-size code based on a permutation group, we should
ﬁnd a maximum-weight clique in a graph. Unfortunately, this is an NP-hard
problem in general graphs. However, in many cases, it may be possible to ﬁnd
a clique whose weight is large by using, for example, backtracking or stochastic
algorithms. The beneﬁt in focusing on codes that consist of orbits of codewords
is that the size of the resulting graph is moderate.
3
Computer Search
We carried out computer searches to ﬁnd codes whose automorphisms are pre-
scribed by transitive permutation groups. In [16], new binary codes were found
using this approach; now we focus on ternary, quaternary and quinary codes.
Transitive permutation groups have been classiﬁed [9,13] up to degree 47, so it
is possible to systematically go through them for the parameters considered in
this work.
Let T denote the collection of transitive permutation groups up to degree 47.
We performed four separate searches over all groups in T. We describe the ﬁrst
search in detail, and the other searches are variations of it. Consider a group
G ∈T whose degree is d. We ﬁrst generate all block systems of G with block
size q ∈{3, 4, 5}. Then, for each such block system, we relabel the elements of
[d] so that the blocks are of the form
{k, n + k, . . . , (q −1)n + k}
where n = d/q and k ∈[n]. This yields a group of automorphisms for a q-ary code
of length n. Finally, we select the orbits that produce the code by conducting a
clique search in the corresponding graph.
In the second search, we searched for q-ary codes of length n+1 such that G
acts transitively on n coordinates (in the manner described above) and ﬁxes one
coordinate. In the third search, we searched for q-ary codes of length nk using
k copies of G that act transitively and simultaneously on n coordinates each.
Finally, in the fourth search, we searched for q-ary codes of length nk + 1 by
combining the two previous techniques.

New Lower Bounds
231
We used the Cliquer software [18] to ﬁnd maximum-weight cliques in orbit
graphs. We restricted ourselves to graphs that contain at most 5000 vertices,
because processing larger graphs would have been too slow. Each clique search
was run for at most 1000 s; in most cases the maximum-weight clique was found
in a couple of seconds.
4
New Lower Bounds
We found several new codes that improve lower bounds on Aq(n, d) where q ∈
{3, 4, 5}. The groups and orbits are given in the Appendix.
Table 1 summarizes the new lower bounds. As many as 17 new lower bounds
follow directly from the new codes. In addition, using the facts
Aq(n, d) ≥Aq(n + 1, d)/q
and
Aq(n, d) ≥Aq(n + 1, d + 1)
we obtain two more lower bounds (on A3(14, 4) and A4(8, 5)), resulting in a total
of 19 new lower bounds.
Table 1. The new lower bounds
Old lower bound
New lower bound
A3(13, 4) ≥8559 [10]
A3(13, 4) ≥13122
A3(14, 4) ≥24786 [10] A3(14, 4) ≥27702
A3(15, 4) ≥72171 [20] A3(15, 4) ≥83106
A3(15, 5) ≥6561 [11]
A3(15, 5) ≥7812
A3(15, 6) ≥2187 [11]
A3(15, 6) ≥3321
A3(16, 7) ≥729 [2]
A3(16, 7) ≥1026
A3(16, 8) ≥297 [20]
A3(16, 8) ≥387
A4(8, 4) ≥320 [5]
A4(8, 4) ≥352
A4(8, 5) ≥70 [5]
A4(8, 5) ≥76
A4(9, 4) ≥1024 [3]
A4(9, 4) ≥1152
A4(9, 6) ≥64 [3]
A4(9, 6) ≥76
A4(10, 3) ≥17408 [5]
A4(10, 3) ≥24576
A4(10, 4) ≥4096 [3]
A4(10, 4) ≥4192
A4(11, 3) ≥65536 [15] A4(11, 3) ≥77056
A5(8, 4) ≥1125 [7]
A5(8, 4) ≥1225
A5(8, 5) ≥160 [7]
A5(8, 5) ≥165
A5(9, 4) ≥3750 [7]
A5(9, 4) ≥4375
A5(9, 5) ≥625 [6]
A5(9, 5) ≥725
A5(10, 4) ≥15625 [3]
A5(10, 4) ≥17500

232
A. Laaksonen and P.R.J. ¨Osterg˚ard
One of the codes in the Appendix gives the bound A3(15, 5) ≥7452 when
prescribing the given group G. By augmenting this code with 360 additional
codewords, the ﬁnal lower bound A3(15, 5) ≥7812 is obtained. The additional
codewords are presented via another group H—which is a subgroup of G—and
orbits under the action of H. No codewords can be added to the other codes.
Acknowledgments. This work was supported in part by the Academy of Finland,
Project #289002.
Appendix: Codes for the New Lower Bounds
Bound: A3(13, 4) ≥13122
Generators of G:
(1 10 35 32 31 28)(2 14 36 22 19 5)(3 25 20 4 24 21)
(6 18 15 27 23 9)(7 17 37 34 29 12)(8 16 38 33 30 11),
(1 25 22 21 5 17)(2 24 32 3 23 33)(4 27 38 35 8 31)
(6 16 36 7 28 11)(9 34 18 30 14 12)(10 20 15 37 19 29)
Orbit representatives:
1112000000000, 2001001000000, 0110201000000, 0220021000000,
0202002000000, 1201000000001, 2111010000001, 2022110000001,
2200120000001, 0002000100001, 1011100000002, 2220010000002,
0212110000002, 1202011000002, 2200021100002
Bound: A3(15, 4) ≥83106
Generators of G:
(1 41 6)(2 45 7 5 42 25)(3 4 8 9 28 44)(10 32 30 37 35 27)(11 21 31)
(12 40 17 15 22 20)(13 14 18 19 38 39)(16 26 36)(23 24 43 29 33 34),
(1 2 10 44)(3 33)(4 11 7 35)(5 19 26 37)(6 27 45 39)(8 43)(9 36 42 30)
(12 15 24 21)(13 38)(14 31 32 25)(16 17 40 29)(18)(20 34 41 22)(23 28)
Orbit representatives:
200101000000000, 111201000000000, 022202000000000, 120112000000000,
222210100000000, 012120100000000, 102200200000000, 021111010000000,
020022110000000, 220100210000000, 210021210000000, 201210020000000,
001022020000000, 222001001000000, 210201201000000, 011012011000000,
012021111000000, 211001121000000, 201222221000000, 222211012000000,
210101112000000, 211110222000000, 001102222000000, 000000110100000,
221021021100000, 010112112200000
Bound: A3(15, 5) ≥7812
Generators of G:
(1 13 34 22)(2 6 23 39 17 21 38 9 32 36 8 24)
(3 14 27 26 33 44 42 41 18 29 12 11)(4 7 16 43)
(5 45 35 30 20 15)(19 37 31 28),
(1 45 41 25 21 5)(2 4 12 29 7 9)(3 23 28)
(6 35 16 30 26 10)(8 13 18)(11 40 36 20 31 15)

New Lower Bounds
233
(14 22 24 17 34 27)(19 42 44 37 39 32)(33 38 43)
Orbit representatives:
212221100000000, 002122200000000, 210110001000000, 112100102000000,
121212212000000, 000002022010000, 010202102122000
Generators of H:
(16 4 7 10 43)(31 19 37 40 28)(2 35 38 26 29)(17 5 8 41 14)
(32 20 23 11 44)(3 36 39 42 15)(18 21 24 27 45)(33 6 9 12 30)
(34 22 25 13 1)
Orbit representatives:
000011021101212, 000022002020201, 000111111111010, 000112122210020,
000122202220010, 000201021012210, 000201122122112, 000210120202021,
000220012002020, 000220120210100, 001010010100011, 001011112001020,
001022001012200, 001110111110022, 001121201222012, 001122121212102,
001200121121111, 001201102011212, 002010011001022, 002021000011202,
002022120122210, 002121120211101, 002200101010211, 002200220120102,
010002012000221, 010012110022220, 010102212200000, 010200100220120,
010211001022200, 010211102102102, 010212110100100, 011000101100201,
011001011002220, 011101210120002, 011102101222122, 011211112021202,
011211201101120, 012001010021222, 012002100102200, 012010111020221,
012210111101101, 012210200100122, 020010022000211, 020110112200110,
020112221101020, 021012021002210, 021111221212022, 022010020011212,
022110220211021, 100001221212110, 100020212202220, 100102000212100,
100111211001212, 100112210200221, 100120202210101, 101000220211112,
101101002211102, 101110220000111, 102000001210111, 102002222010110,
102110011200222, 110011201222100, 110100212220121, 110112010222120,
111010200221102, 111101211202120, 111111012221122, 112012202220101,
112100210201122, 120112222210111, 121112221202110, 122111220201112
Bound: A3(15, 6) ≥3321
Generators of G:
(1 33 20 7 24 11 13 45 17 34 36 8 40 27 44)
(2 4 6 23 10 42 29 31 18 5 22 39 26 28 15)
(3 35 37 9 41 43 30 32 19 21 38 25 12 14 16),
(1 36 11)(2 10 42 5 22 15)(3 29 43 39 38 4)
(6 26 31)(7 45 17 40 27 20)(8 19 33 14 13 9)
(12 35 37 30 32 25)(16 21 41)(18 44 28 24 23 34)
Orbit representatives:
022102200000000, 111000121000000, 222222012000000, 110012000100000,
101120020200000
Bound: A3(16, 7) ≥1026
Generators of G:
(1 21 18 38)(2 22 17 37)(3 23 36 8)(4 24 35 7)(5 34 6 33)
(9 29 26 46)(10 30 25 45)(11 31 44 16)(12 32 43 15)
(13 42 14 41)(19 39 20 40)(27 47 28 48),

234
A. Laaksonen and P.R.J. ¨Osterg˚ard
(1 35 5)(2 36 6)(3 37 33)(4 38 34)(9 43 13)(10 44 14)
(11 45 41)(12 46 42)(17 19 21)(18 20 22)(25 27 29)(26 28 30)
Orbit representatives:
2000000020000000, 0122222201000000, 2221220000100000,
2010010012212000, 0112121210101010, 2001010102101010,
0221120112201010, 1012022020102010, 2002122110201020
Bound: A3(16, 8) ≥387
Generators of G:
(1 20 7 8 5 38)(2 3 18 35 34 19)(4 23 40 21 22 17)(6 33 36 39 24 37)
(9 28 15 16 13 46)(10 11 26 43 42 27)(12 31 48 29 30 25)(14 41 44 47 32 45),
(1 23 8 3 34 22 36)(2 38 4 33 7 24 35)(6 20 17 39 40 19 18)
(9 31 16 11 42 30 44)(10 46 12 41 15 32 43)(13)(14 28 25 47 48 27 26)
Orbit representatives:
2200210222100000, 0201000120210000, 0000221122221100,
0202211002022110, 2020120102022110, 1111002202022110
Bound: A4(8, 4) ≥352
Generators of G:
(1 20)(2 19)(3 26 11 18 27 10)(4 25 12 17 28 9)
(5 24)(6 23)(7 30 15 22 31 14)(8 29 16 21 32 13),
(1 3)(2 4)(5 7)(6 8)(9 27)(10 28)(11 25)(12 26)(13 31)
(14 32)(15 29)(16 30)(17 19)(18 20)(21 23)(22 24),
(1 17 9)(2 18 10)(3 11 19)(4 12 20)(5 21 13)(6 22 14)(7 15 23)(8 16 24)
Orbit representatives:
30100000, 21320000, 22002200, 11112200, 10201010, 01021010, 21212010,
12122010, 33003010, 03322110, 30232110, 30321210, 03231210, 11220310
Bound: A4(9, 4) ≥1152
Generators of G:
(1 8 28 35 10 17 19 26)(2 7 29 34 11 16 20 25)
(3 33 21 6 12 24 30 15)(4 32 22 5 13 23 31 14),
(1 32 30 34 10 23 21 25)(2 33 31 35 11 24 22 26)
(3 16 28 5 12 7 19 14)(4 17 29 6 13 8 20 15),
(1 13)(2 12)(3 11)(4 10)(5 24)(6 23)(7 35)(8 34)(14 33)
(15 32)(16 26)(17 25)(19 22)(20 21)(28 31)(29 30)
Orbit representatives:
210020000, 022220000, 203220100, 202030300, 332020001, 311030101,
123230101, 130220201, 111220002, 013020102, 320220102, 031230302,
112030003, 221230003, 121020103, 303020203
Bound: A4(9, 6) ≥76
Generators of G:
(1 30 24 14)(2 35 25 36)(3 33 23 19)(4 13)(5 10 12 6)(7 9 11 8)
(15 32 28 21)(16 18 29 17)(20 26 34 27)(22 31),

New Lower Bounds
235
(1 11 34)(2 7 28)(3 22 17)(4 8 30)(5 9 33)(6 32 27)(10 29 16)
(12 31 35)(13 26 21)(14 18 24)(15 23 36)(19 20 25)
Orbit representatives:
221012000, 000322200
Bound: A4(10, 3) ≥24576
Generators of G:
(1 37 31 7)(2 26 12 36)(3 5)(4 14)(6 22 16 32)(8 40 18 30)(9 19)
(10 38 20 28)(11 27 21 17)(13 15)(23 35)(24 34)(25 33)(29 39),
(1 6 21 36)(2 35)(3 4 13 14)(5 22)(7 10)(8 39 18 29)(9 38 19 28)
(11 16 31 26)(12 25)(15 32)(17 20)(23 34 33 24)(27 30)(37 40)
Orbit representatives:
1310000000, 3120220000
Bound: A4(10, 4) ≥4192
Generators of G:
(1 29 4 6 11 39 24 36)(2 30 3 5 12 40 23 35)(7 38 27 18)(8 37 28 17)
(9 14 16 21 19 34 26 31)(10 13 15 22 20 33 25 32),
(1 19)(2 20)(3 7 23 27 33 37)(4 8 24 28 34 38)(5 36 25 6 35 26)
(9 11 29 31 39 21)(10 12 30 32 40 22)(13 17)(14 18)(15 16)
Orbit representatives:
0000000000, 3311000000, 0110301000, 1001301000, 2332301000,
1032121000, 1210303010
Bound: A4(11, 3) ≥77056
Generators of G:
(1 32 23 10)(2 31)(3 41 14 19)(4 7 37 40)(5 6 16 28)(8 25 30 36)
(9 35)(12 21 34 43)(13 42)(15 18 26 29)(17 27 39 38)(20 24),
(1 35)(2 34 24 23 13 12)(3 10 36 43 14 32)(4 9 26 31 37 42)
(5 30 27 19 38 8)(6 29 39 40 17 7)(15 20)(16 41)(18 28)(21 25)
Orbit representatives:
10120000000, 02130000000, 23330000000, 02312000000, 12000000001,
31220000001, 20130000001, 33000000002, 10200000002, 01120000002,
11100000003, 22300000003, 30002000003
Bound: A5(8, 4) ≥1225
Generators of G:
(1 20 40 10 27 7 17 36 16 26 3 23 33 12 32 2 19 39 9 28 8 18 35 15 25 4 24 34 11
31) (5 22 37 14 29 6 21 38 13 30),
(1 28 5 26 3 30)(2 27 6 25 4 29)(7 32)(8 31)(9 20 13 18 11 22)
(10 19 14 17 12 21)(15 24)(16 23)(33 36 37 34 35 38)(39 40)
Orbit representatives:
00000000, 41131000, 02241000, 24332000, 43411010, 34212010,
01010110, 02020220

236
A. Laaksonen and P.R.J. ¨Osterg˚ard
Bound: A5(8, 5) ≥165
Generators of G:
(1 32 19 5 33 24 11 37 25 16 3 29 17 8 35 21 9 40 27 13)
(2 31 20 6 34 23 12 38 26 15 4 30 18 7 36 22 10 39 28 14),
(1 10 17 26 33 2 9 18 25 34)(3 12 19 28 35 4 11 20 27 36)
(5 14 21 30 37 6 13 22 29 38)(7 16 23 32 39 8 15 24 31 40)
Orbit representatives:
33330000, 40304100, 13013200, 12340210, 30134210
Bound: A5(9, 4) ≥4375
Generators of G:
(1 32 10 41 19 5 28 14 37 23)(2 33 11 42 20 6 29 15 38 24)
(3 43 21 25 39 7 12 34 30 16)(4 44 22 26 40 8 13 35 31 17),
(1 3 20 13)(2 4 19 12)(5 16 6 17)(7 33 26 23)(8 32 25 24)
(10 30 11 31)(14 34 42 44)(15 35 41 43)(21 29 40 37)(22 28 39 38)
Orbit representatives:
200020000, 112040000, 104010100, 232030100, 311000001, 023010101,
342010201, 121020301, 433000002, 143040102, 231010003, 411020103,
244000004, 322020004, 402040304
Bound: A5(9, 5) ≥725
Generators of G:
(1 44 37 26)(2 45 38 27)(3 43 39 25)(7 12 16 30)(8 10 17 28)(9 11 18 29)
(13 22 40 31)(14 23 41 32)(15 24 42 33)(19 35)(20 36)(21 34),
(1 9 13 19 27 31 37 45 4 10 18 22 28 36 40)
(2 7 15 20 25 33 38 43 6 11 16 24 29 34 42)
(3 8 14 21 26 32 39 44 5 12 17 23 30 35 41)
Orbit representatives:
444222000, 231140100, 312401100, 003121100, 123014100
Bound: A5(10, 4) ≥17500
Generators of G:
(1 32 33 44 5)(2 3 14 35 21)(4 45 11 42 43)(6 37 38 49 10)
(7 8 19 40 26)(9 50 16 47 48)(12 13 24 25 31)
(15 41 22 23 34)(17 18 29 30 36)(20 46 27 28 39),
(1 42)(2 11)(3 5 13 45 23 35 33 25 43 15)(4 44 34 24 14)
(6 47)(7 16)(8 10 18 50 28 40 38 30 48 20)(9 49 39 29 19)
(12 21)(17 26)(22 31)(27 36)(32 41)(37 46),
(1 32 33 44 25)(2 3 14 5 21)(4 15 11 42 43)(6 37 38 49 30)
(7 8 19 10 26)(9 20 16 47 48)(12 13 24 45 31)
(17 18 29 50 36)(22 23 34 35 41)(27 28 39 40 46)
Orbit representatives:
0000000000, 1331000000, 4342110000, 4112220000, 0442030000,
3114040000, 2333240000, 1222340000

New Lower Bounds
237
References
1. Agrell, E., Vardy, A., Zeger, K.: A table of upper bounds for binary codes. IEEE
Trans. Inform. Theory 47, 3004–3006 (2001)
2. Assmus, E.F., Mattson, H.F.: New 5-designs. J. Combin. Theory 6, 122–151 (1969)
3. Assmus, E.F., Mattson, H.F.: On weights in quadratic-residue codes. Discrete
Math. 3, 1–20 (1972)
4. Best, M., Brouwer, A.E., MacWilliams, F.J., Odlyzko, A.M., Sloane, N.J.A.:
Bounds for binary codes of length less than 25. IEEE Trans. Inform. Theory 24,
81–93 (1978)
5. Bogdanova, G.T., Brouwer, A.E., Kapralov, S.N.,
¨Osterg˚ard, P.R.J.: Error-
correcting codes over an alphabet of four elements. Des. Codes Cryptogr. 23,
333–342 (2001)
6. Boukliev, I., Kapralov, S., Maruta, T., Fukui, M.: Optimal linear codes of dimen-
sion 4 over F5. IEEE Trans. Inform. Theory 43, 308–313 (1997)
7. Bogdanova, G.T., ¨Osterg˚ard, P.R.J.: Bounds on codes over an alphabet of ﬁve
elements. Discrete Math. 240, 13–19 (2001)
8. Brouwer, A.E., H¨am¨al¨ainen, H.O., ¨Osterg˚ard, P.R.J., Sloane, N.J.A.: Bounds on
mixed binary/ternary codes. IEEE Trans. Inform. Theory 44, 140–161 (1998)
9. Cannon, J.J., Holt, D.F.: The transitive permutation groups of degree 32. Exp.
Math. 17, 307–314 (2008)
10. C´odigo [pseud.], A discussion on Foros de Free1X2.com, cited by Andries Brouwer
at https://www.win.tue.nl/∼aeb/codes/ternary-1.html
11. Conway, J.H., Pless, V., Sloane, N.J.A.: Self-dual codes over GF(3) and GF(4) of
length not exceeding 16. IEEE Trans. Inform. Theory 25, 312–322 (1979)
12. Elssel, K., Zimmermann, K.-H.: Two new nonlinear binary codes. IEEE Trans.
Inform. Theory 51, 1189–1190 (2005)
13. Hulpke, A.: Constructing transitive permutation groups. J. Symbolic Comput. 39,
1–30 (2005)
14. Kaikkonen, M.K.: Codes from aﬃne permutation groups. Des. Codes Cryptogr.
15, 183–186 (1998)
15. Kschischang, F.R., Subbarayan, P.: Some ternary and quaternary codes and asso-
ciated sphere packings. IEEE Trans. Inform. Theory 38, 227–246 (1992)
16. Laaksonen, A., ¨Osterg˚ard, P.R.J.: Constructing error-correcting binary codes using
transitive permutation groups (submitted). Preprint at. arXiv:1604.06022
17. MacWilliams, F.J., Sloane, N.J.A.: The Theory of Error-Correcting Codes, North-
Holland, Amsterdam (1977)
18. Niskanen, S., ¨Osterg˚ard, P.R.J.: Cliquer User’s Guide, Version 1.0, Communica-
tions Laboratory, Helsinki University of Technology, Espoo, Finland, Technical
report T48 (2003)
19. ¨Osterg˚ard, P.R.J.: Two new four-error-correcting binary codes. Des. Codes Cryp-
togr. 36, 327–329 (2005)
20. Plotkin, M.: Binary codes with speciﬁed minimum distance. IRE Trans. Inform.
Theory 6, 445–450 (1960)

A State Space Approach to Periodic
Convolutional Codes
Diego Napp1, Ricardo Pereira1(B), and Paula Rocha2
1 CIDMA - Center for Research and Development in Mathematics
and Applications, Department of Mathematics,
University of Aveiro, Aveiro, Portugal
{diego,ricardopereira}@ua.pt
2 SYSTEC, Faculty of Engineering, University of Porto, Porto, Portugal
mprocha@fe.up.pt
Abstract. In this paper we study periodically time-varying convolu-
tional codes by means of input-state-output representations. Using these
representations we investigate under which conditions a given time-inva-
riant convolutional code can be transformed into an equivalent periodic
time-varying one. The relation between these two classes of convolutional
codes is studied for period 2. We illustrate the ideas presented in this
paper by constructing a periodic time-varying convolutional code from a
time-invariant one. The resulting periodic code has larger free distance
than any time-invariant convolutional code with equivalent parameters.
Keywords: Convolutional codes · Periodically time-varying codes ·
Input-state-output representations
1
Introduction
Convolutional codes [10] are an important type of error correcting codes that can
be represented as a time-invariant discrete linear system over a ﬁnite ﬁeld [20].
They are used to achieve reliable data transfer, for instance, in mobile commu-
nications, digital video and satellite communications [10,23].
Since the sixties it has been widely known that convolutional codes and linear
systems deﬁned over a ﬁnite ﬁeld are essentially the same objects [20]. More
recently, there has been a new and increased interest in this connection and
many advances have been derived from using the system theoretical framework
This work was supported in part by the Portuguese Foundation for Science and
Technology (FCT-Funda¸c˜ao para a Ciˆencia e a Tecnologia), through CIDMA - Cen-
ter for Research and Development in Mathematics and Applications, within project
UID/MAT/04106/2013 and also by Project POCI-01-0145-FEDER-006933 - SYS-
TEC - Research Center for Systems and Technologies - funded by FEDER funds
through COMPETE2020 - Programa Operacional Competitividade e Internacional-
iza¸c˜ao (POCI) - and by national funds through FCT - Funda¸c˜ao para a Ciˆencia e a
Tecnologia.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 238–247, 2017.
DOI: 10.1007/978-3-319-66278-7 20

A State Space Approach to Periodic Convolutional Codes
239
when dealing with convolutional codes, see [12,17]. Most of the large body of
literature on convolutional codes and on the relation of these codes with linear
systems has been devoted to the “time-invariant” case.
In this work we aim at studying time-varying convolutional codes from a sys-
tem theoretical point of view. These codes have attracted much attention after
Costello conjectured in [5] that nonsystematic time-varying convolutional codes
can attain can larger free distance than the nonsystematic time-invariant ones.
Since then, several researchers have investigated such codes [3,15,16,18]. More-
over, in combination with wavelets [6] time-varying convolutional codes yield
unique trellis structures that resulted in fast and low computational complexity
decoding algorithms. However, little is known on the relation of time-varying con-
volutional codes and time-varying linear systems and very few general construc-
tions of time-varying convolutional codes with designed distances are known.
Here we deal with periodically time-varying convolutional codes (for short,
periodic codes) using input-state-output representations, and investigate some
of their special properties and structures. In particular, we associate periodic
codes with suitably deﬁned time-invariant convolutional codes. This allows us to
derive constructions of (periodic) input-state-output representations for periodic
codes from the better understood time-invariant class.
2
Preliminaries
In the sequel we shall follow the system theory notation and consider column
vectors rather than row vectors.
2.1
Time-Invariant Convolutional Codes
A time-invariant convolutional code is a set of ﬁnite support sequences, called
codewords, obtained as the image of a polynomial shift operator (the encoder)
acting on ﬁnite support sequences that correspond to the original information.
More precisely this can be deﬁned as follows.
Deﬁnition 1. Let F be a ﬁnite ﬁeld and n, k be positive integers with k < n. A
time-invariant convolutional code C of rate k/n is a set of ﬁnite support sequences
described as
C =

v : v(ℓ) =

G

σ−1
u

(ℓ); ℓ∈N0, u ∈

FkN0
FS

where G(z) ∈Fn×k[z] is a full column rank n × k polynomial matrix over F,
called the encoder, u taking values in Fk is the information sequence and v is
the codeword. Moreover, σ−1 denotes the shift

σ−1u

(ℓ) = u(ℓ−1), and the
subindex FS aﬀecting a set of sequences indicates that only its ﬁnite support
elements are considered.

240
D. Napp et al.
The encoders of a code C are not unique; however they only diﬀer by right
multiplication by unimodular matrices over F[z]. An encoder matrix G is called
basic if it has a polynomial right inverse; from now on we shall only consider
basic encoders, and refer to them simply as encoders. The encoder G is called
minimal if the sum of its column degrees attains the minimal possible value.
We deﬁne the degree δ of a convolutional code as the sum of the column
degrees of one, and hence any, minimal encoder. Note that the list of column
degrees (also known as Forney indices) of a minimal encoder is unique up to
a permutation. The maximum of the Forney indices is called the memory of a
code, and is denoted by m. A code C of rate k/n, degree δ and memory m is said
to be an (n, k, δ) code or an (n, k, δ, m) code if the memory is to be speciﬁed.
2.2
Periodically Time-Varying Convolutional Codes
In this work we consider convolutional codes C with P-periodic encoders, i.e.:
C =

v : v(Pℓ+ t) =

Gt 
σ−1
u

(Pℓ+ t); t = 0, . . . , P −1;
ℓ∈N0, u ∈

FkN0
FS

,
(1)
where each Gt(z) is an n × k time-invariant (basic) encoder. Such codes will be
called P-periodic.
Inspired by the ideas developed in [1,13] for the case of behaviors, considering
the linear map
Lp : (Fn)N0 →

FP nN0
deﬁned by
(Lpv)(ℓ) =
⎡
⎢⎢⎢⎣
v(Pℓ)
v(Pℓ+ 1)
...
v(Pℓ+ P −1)
⎤
⎥⎥⎥⎦, P ∈N
we associate with C a time-invariant convolutional code CL, the lifted version
of C, deﬁned as
CL =

v ∈

FP nN0 : v = Lpv, v ∈C

.
Note that, since

Gt 
σ−1
u

(Pℓ+ t) =
 
σtGt 
σ−1
u

(Pℓ),
the equation in (1) can also be written as

ΩP,n (σ) v

(Pℓ) =

G

σ, σ−1
u

(Pℓ), ℓ∈N0,
where for r ∈N
ΩP,r (σ) =
⎡
⎢⎢⎢⎣
Ir
σIr
...
σP −1Ir
⎤
⎥⎥⎥⎦

A State Space Approach to Periodic Convolutional Codes
241
is a polynomial matrix operator in the shift σ and
G

σ, σ−1
=
⎡
⎢⎢⎢⎣
G0 
σ−1
σG1 
σ−1
...
σP −1GP −1 
σ−1
⎤
⎥⎥⎥⎦
is a polynomial matrix operator in the shifts σ and σ−1.
Moreover, it is possible to show that the matrix G can be decomposed as
G

σ, σ−1
= GL 
σ−P 
ΩP,k (σ)
where
GL 
σ−1
=

GL0 
σ−1
| GL1 
σ−1
| · · · | GLP −1 
σ−1
and the blocks GLj 
σ−1
have size Pn × k, j = 0, . . . , P −1.
Thus, the lifted code can be represented as
CL =

v : v(ℓ) = (GL 
σ−1
u)(ℓ), ℓ∈N0, u ∈

FkP N0
FS

,
where v = LP v and u = LP u.
2.3
Distance Properties
In recent years great eﬀort has been dedicated to developing constructions of non-
binary convolutional codes having good distance [2,9,14]. However, in contrast to
block codes, the theoretical tools for the construction of convolutional codes with
good designed distance have not been fully exploited. In fact, most convolutional
codes used in practice have been found by systematic computer search and their
distance properties must be also computed by full search.
One of our objectives will be the construction of convolutional codes with a
large free distance, which is deﬁned as follows.
Deﬁnition 2. The free distance of a convolutional code C is given by
dfree(C) = min
 ∞

ℓ=0
wt

v(ℓ)

: v ∈C \ {0}

,
where wt denotes the Hamming weight, that is, wt(v(ℓ)) corresponds to the num-
ber of nonzero components of v(ℓ).
Rosenthal and Smarandache [21] showed that the free distance of a time-
invariant (n, k, δ) convolutional code is upper bounded by
dfree(C) ≤(n −k)
 δ
k

+ 1

+ δ + 1.

242
D. Napp et al.
This bound is called the generalized Singleton bound. It is well-known [21] that
over suﬃciently large ﬁnite ﬁelds, there always exist convolutional codes that
achieve this bound for any given set of parameters (n, k, δ).
However, in this paper we will consider instead the Griesmer bound deﬁned
in the next theorem. This bound is always less than or equal to the general-
ized Singleton bound, and can be considerably lower for codes over small ﬁelds,
whereas it coincides with the Singleton for codes over suﬃciently large ﬁnite
ﬁelds.
Theorem 1 ([7,10,19]). Let (n, k, δ, m) be a 4-tuple of nonnegative integers such
that k < n, consider q ∈N and ˆN =

N
if km = δ
N0 if km > δ . Let further
GBq(n, k, δ, m) = max
⎧
⎨
⎩d′ :
k(m+i)−δ−1

j=0
 d′
qj

≤n(m + i), ∀i ∈ˆN
⎫
⎬
⎭.
Then, every (n, k, δ, m)-convolutional code C over the ﬁeld Fq is such that
dfree(C) ≤GBq(n, k, δ, m).
GBq(n, k, δ, m) is known as the Griesmer bound.
3
State Space Realizations
A state space system
x(ℓ+ 1) = Ax(ℓ) + Bu(ℓ)
v(ℓ)
= Cx(ℓ) + Du(ℓ) , l ∈N0,
denoted by (A, B, C, D), where A ∈Fδ×δ, B ∈Fδ×k, C ∈Fn×δ and D ∈Fn×k, is
said to be a state space realization of the time-invariant (n, k, δ) convolutional
code C if C is the set of ﬁnite support output sequences v corresponding to ﬁnite
support input sequences u and zero inicial conditions, i.e., x(0) = 0.
Remark 1. This deﬁnition implicitly assumes that (A, B, C, D) is a minimal real-
ization of C, i.e., that A has the minimal possible dimension. This implies that
A is nilpotent, (A, B) is controllable and (A, C) is observable, i.e., the polyno-
mial matrices
z−1I −A | B
and
"z−1I −A
C
#
have, respectively, right and left
polynomial inverses (in z−1).
State space realizations for convolutional codes can be obtained as minimal
state space realizations of minimal encoders.
The next proposition, adapted from [8, Proposition 2.3], provides a state
space realization for a given (not necessarily minimal) encoder.

A State Space Approach to Periodic Convolutional Codes
243
Proposition 1. Let G ∈Fn×k[z] be a polynomial matrix with rank k and column
degrees ν1, . . . , νk. Consider ¯δ = $k
i=1 νi. Let G have columns gi = $νi
ℓ=0 gℓ,izℓ,
i = 1, . . . , k where gℓ,i ∈Fn. For i = 1, . . . , k deﬁne the matrices
Ai =
⎡
⎢⎢⎢⎢⎣
0 · · · · · · 0
1
...
...
...
1 0
⎤
⎥⎥⎥⎥⎦
∈Fνi×νi, Bi =
⎡
⎢⎢⎢⎣
1
0
...
0
⎤
⎥⎥⎥⎦∈Fνi, Ci =

g1,i · · · gνi,i

∈Fn×νi.
Then a state space realization of G is given by the matrix quadruple
(A, B, C, D) ∈F¯δ×¯δ × F¯δ×k × Fn×¯δ × Fn×k where
A =
⎡
⎢⎣
A1
...
Ak
⎤
⎥⎦, B =
⎡
⎢⎣
B1
...
Bk
⎤
⎥⎦, C =

C1 · · · Ck
	
, D =

g0,1 · · · g0,k
	
= G(0).
In the case where νi = 0 the ith block is missing and in B a zero column
occurs.
In this realization (A, B) is controllable, and if G is a minimal encoder,
(A, C) is observable.
4
Constructing Periodically Time-Varying Convolutional
Codes
In comparison to the literature on time-invariant convolutional codes, there exist
few algebraic constructions of time-varying convolutional codes with good prop-
erties [11,22]. Here we present a new technique to build time-varying convo-
lutional codes from time-invariant ones. In particular, in this section we focus
on constructing 2-periodic codes with optimal free distance. We illustrate our
approach by means of an example of a 2-periodic (3, 2, 2, 1) code having larger
distance than any (3, 2, 2, 1) time-invariant convolutional code.
We ﬁrst investigate the problem of ﬁnding periodic state space representa-
tions of periodic convolutional codes. As shown in Sect. 2.1, using a lifting tech-
nique one can transform a time-varying periodic linear system into an equivalent
time-invariant one. Following [1], we study the relationship between the periodic
state space representations of a given code and the time-invariant state space
representations of its lifted version. For the sake of simplicity we assume that
the period is P = 2. However, whereas in [1] only single-input/single-output
systems were considered, here we deal with codes of general rate k/n that are
closely related to multi-input/multi-output (MIMO) systems.
Assume that Σ(·) = (A(·), B(·), C(·), D(·)) is a δ-dimensional state space
representation of a code C, as present below:

x(ℓ+ 1) = A(ℓ)x(ℓ) + B(ℓ)u(ℓ)
v(ℓ)
= C(ℓ)x(ℓ) + D(ℓ)u(ℓ) , l ∈N0
(2)

244
D. Napp et al.
where (A(·), B(·), C(·), D(·)) ∈Fδ×δ ×Fδ×k ×Fn×δ ×Fn×k are periodic functions
with period 2. Letting
w(ℓ) = x(2ℓ)
uL(ℓ) =
"
u(2ℓ)
u(2ℓ+ 1)
#
vL(ℓ) =
"
v(2ℓ)
v(2ℓ+ 1)
#
we obtain the following time-invariant δ-dimensional state space representation
ΣL = (E, F, H, J) for the lifted code CL:

w(ℓ+ 1) = Ew(ℓ) + FuL(ℓ)
vL(ℓ)
= Hw(ℓ) + JuL(ℓ) ,
(3)
with
E = A(1)A(0)
F =
A(1)B(0) B(1)
H =
"
C(0)
C(1)A(0)
#
J =
"
D(0)
0
C(1)B(0) D(1)
#
.
The representation ΣL = (E, F, H, J) of CL is said to be induced by the
representation Σ(·) = (A(·), B(·), C(·), D(·)) of C, or equivalently, Σ(·) is said to
induce ΣL. Moreover, a time-invariant representation ΣL = (E, F, H, J) of CL
is called induced whenever it is induced by some periodic representation Σ(·) of
C.
The following proposition is a generalization of [1, Proposition 3.1] (with
identical proof) and characterizes induced representations.
Proposition 2. Let C be a 2-periodic code and CL the lifted code associated to
C. Then a δ-dimensional state space representation ΣL = (E, F, H, J) of CL,
with
E ∈Fδ×δ
F =

F1 F2

∈Fδ×2k
H =
"
H1
H2
#
∈F2n×δ J =
"
J11 J12
J21 J22
#
∈F2n×2k.
is induced if and only if
rank M =
" E F1
H2 J21
#
≤δ.
Moreover, in this case, decomposing the matrix M as
M =
"N1
N2
# Q1 Q2

,
the 2-periodic δ-dimensional state space representation of C that induces ΣL is
Σ(·) = (A(·), B(·), C(·), D(·)), where
A(0) = Q1
A(1) = N1
B(0) = Q2
B(1) = F2
C(0) = H1
C(1) = N2
D(0) = J11
D(1) = J22.

A State Space Approach to Periodic Convolutional Codes
245
Hence, Proposition 2 characterizes the state space realizations of time-
invariant convolutional codes from which (2-periodic) time-varying codes can
be constructed. In this way one can use the large body of literature and con-
structions for the time-invariant case in order to build time-varying convolu-
tional codes with good properties. In the next section we illustrate this with an
example.
4.1
2-Periodic (3, 2, 2, 1) Convolutional Code with Free Distance 4
It is know from the literature that (3, 2, 2, 1) time-invariant convolutional codes
have at most free distance 3, whereas the Griesmer bound for this kind of codes
is 4. In this section we construct a 2-periodic (3, 2, 2, 1) convolutional code with
free distance 4 based on the construction of a time-invariant code whose state
space realization is induced by a 2-periodic realization. This shows that time-
varying convolutional codes can attain larger free distance than time-invariant
ones.
Example 1. Consider the (6, 4, 2, 1) time-invariant convolutional code, CL, over
F2 with generator matrix G = G1z + G0, where
G0 =
⎡
⎢⎢⎢⎢⎢⎢⎣
1 1 0 0
0 1 0 0
1 0 0 0
1 0 1 0
1 1 1 1
0 1 1 0
⎤
⎥⎥⎥⎥⎥⎥⎦
and
G1 =
⎡
⎢⎢⎢⎢⎢⎢⎣
0 0 1 1
0 0 0 1
0 0 0 1
0 0 0 0
0 0 0 0
0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎦
.
As
"
G0
G1
#
is an encoder of a (12, 3) block code of free distance 4, we conclude
that the free distance of CL is at most 4. Moreover, it can be computed via a
program that the free distance of CL is indeed 4. Since the column degrees of G
are ν1 = 0, ν2 = 0, ν3 = 1, ν4 = 1, by Proposition 1, a state space realization of
G is given by (E, F, H, J) ∈F2×2 × F2×4 × F6×2 × F6×4 where
E =
"0 0
0 0
#
, F =
"0 0 1 0
0 0 0 1
#
, H =
⎡
⎢⎢⎢⎢⎢⎢⎣
1 1
0 1
0 1
0 0
0 0
0 0
⎤
⎥⎥⎥⎥⎥⎥⎦
, J = G0.
(4)
Now, the matrix M deﬁned in Proposition 2 is:
M =
⎡
⎢⎢⎢⎢⎣
0 0 0 0
0 0 0 0
0 0 1 0
0 0 1 1
0 0 0 1
⎤
⎥⎥⎥⎥⎦
.

246
D. Napp et al.
Since rank M = 2 ≤δ, by this proposition we conclude that realization (4)
is induced by a 2-periodic (3, 2, 2, 1) convolutional code, with realization Σ(·) =
(A(·), B(·), C(·), D(·)) where
A(0) =
"
0 0
0 0
#
A(1) =
"
0 0
0 0
#
B(0) =
"
1 0
0 1
#
B(1) =
"
1 0
0 1
#
C(0) =
⎡
⎣
1 1
0 1
0 1
⎤
⎦C(1) =
⎡
⎣
1 0
1 1
0 1
⎤
⎦D(0) =
⎡
⎣
1 1
0 1
1 0
⎤
⎦D(1) =
⎡
⎣
1 0
1 1
1 0
⎤
⎦.
This 2-periodic code can also be described as in (1) with P = 2, and Gt(z) =
C(t)

z−1I −A(t)
−1 B(t) + D(t), for t = 0, 1, which yields
G0(z) =
⎡
⎣
1 1
0 1
0 1
⎤
⎦z +
⎡
⎣
1 1
0 1
1 0
⎤
⎦
and
G1(z) =
⎡
⎣
1 0
1 1
0 1
⎤
⎦z +
⎡
⎣
1 0
1 1
1 0
⎤
⎦.
This example is equivalent to the one presented by Palazzo in [18].
5
Conclusions
In this paper we have studied the relation between time-invariant and time-
varying convolutional codes by means of input-state-output representations.
Using a well known lifting technique we have shown how it is possible to trans-
form a given periodically time-varying convolutional code into a time-invariant
one. Moreover, we have provided conditions, in terms of input-state-output
representations, to transform a time-invariant convolutional code into a time-
varying one. Using these ideas, we have illustrated how to construct a 2-periodic
(3, 2, 2, 1) convolutional code with optimal free distance from a (3, 2, 2, 1) time-
invariant one. This showed that time-varying convolutional codes can attain
larger free distance than time-invariant ones. Constructions of periodic convo-
lutional codes of higher periods and with other parameters are currently under
investigation.
References
1. Aleixo, J.C., Rocha, P., Willems, J.C.: State space representation of SISO periodic
behaviors. In: Proceedings of the 50th IEEE Conference on Decision and Control
and European Control Conference (CDC-ECC), Orlando, FL, USA, pp. 1545–1550,
12–15 December 2011
2. Almeida, P., Napp, D., Pinto, R.: A new class of superregular matrices and MDP
convolutional codes. Linear Algebra Appl. 439(7), 2145–2147 (2013)
3. Bocharova, I., Kudryashov, B.: Rational rate punctured convolutional codes for
soft-decision Viterbi decoding. IEEE Trans. Inf. Theory 43(4), 1305–1313 (1997)

A State Space Approach to Periodic Convolutional Codes
247
4. Climent, J.-J., Herranz, V., Perea, C., Tom´as, V.: A systems theory approach to
periodically time-varying convolutional codes by means of their invariant equiva-
lent. In: Bras-Amor´os, M., Høholdt, T. (eds.) AAECC 2009. LNCS, vol. 5527, pp.
73–82. Springer, Heidelberg (2009). doi:10.1007/978-3-642-02181-7 8
5. Costello, D.: Free distance bounds for convolutional codes. IEEE Trans. Inf. Theory
20(3), 356–365 (1974)
6. Fekri, F., Sartipi, M., Mersereau, R.M., Schafer, R.W.: Convolutional codes using
ﬁnite-ﬁeld wavelets: time-varying codes and more. IEEE Trans. Signal Process.
53(5), 1881–1896 (2005)
7. Gluesing-Luerssen, H., Schmale, W.: Distance bounds for convolutional codes and
some optimal codes, arXiv:math/0305135v1 (2003)
8. Gluesing-Luerssen, H., Schneider, G.: State space realizations and monomial equiv-
alence for convolutional codes. Linear Algebra Appl. 425, 518–533 (2007)
9. Gluesing-Luerssen, H., Rosenthal, J., Smarandache, R.: Strongly-MDS convolu-
tional codes. IEEE Trans. Inf. Theory 52(2), 584–598 (2006)
10. Johannesson, R., Zigangirov, K.S.: Fundamentals of Convolutional Coding. IEEE
press, New York (1999)
11. Justesen, J.: New convolutional code constructions and a class of asymptotically
good time-varying codes. IEEE Trans. Inf. Theory 19(2), 220–225 (1973)
12. Kuijper, M., Polderman, J.W.: Reed-Solomon list decoding from a system-theoretic
perspective. IEEE Trans. Inf. Theory 50(2), 259–571 (2004)
13. Kuijper, M., Willems, J.C.: A behavioral framework for periodically time-varying
systems. In: Proceedings of the 36th IEEE Conference on Decision & Control -
CDC 1997, vol. 3, San Diego, California USA, 10–12 December 1997, pp. 2013–
2016 (1997)
14. La Guardia, G.: Convolutional codes: techniques of construction. Comput. Appl.
Math. 35(2), 501–517 (2016)
15. Lee, P.J.: There are many good periodically time-varying convolutional codes.
IEEE Trans. Inf. Theory 35(2), 460–463 (1989)
16. Mooser, M.: Some periodic convolutional codes better than any ﬁxed code. IEEE
Trans. Inf. Theory 29(5), 750–751 (1983)
17. Napp, D., Perea, C., Pinto, R.: Input-state-output representations and construc-
tions of ﬁnite support 2D convolutional codes. Adv. Math. Commun. 4(4), 533–545
(2010)
18. Palazzo, R.: A time-varying convolutional encoder better than the best time-
invariant encoder. IEEE Trans. Inf. Theory 39(3), 1109–1110 (1993)
19. Porras, J.M., Curto, J.I.: Classiﬁcation of convolutional codes. Linear Algebra
Appl. 432(10), 2701–2725 (2010)
20. Rosenthal, J.: Connections between linear systems and convolutional codes. In:
Marcus, B., Rosenthal, J. (eds.) Codes, Systems, and Graphical Models, vol. 123,
pp. 39–66. Springer, New York (2001). doi:10.1007/978-1-4613-0165-3 2
21. Rosenthal, J., Smarandache, R.: Maximum distance separable convolutional codes.
Appl. Algebra Eng. Commun. Comput. 10, 15–32 (1999)
22. Truhachev, D., Zigangirov, K., Costello, D.: Distance bounds for periodically time-
varying and tail-biting LDPC convolutional codes. IEEE Trans. Inf. Theory 56(9),
4301–4308 (2010)
23. Viterbi, A.J.: Convolutional codes and their performance in communication sys-
tems. IEEE Trans. Commun. Technol. 19(5), 751–772 (1971)

Column Rank Distances of Rank Metric
Convolutional Codes
Diego Napp1, Raquel Pinto1(B), Joachim Rosenthal2, and Filipa Santana1
1 Department of Mathematics, CIDMA – Center for Research and Development
in Mathematics and Applications, University of Aveiro, Aveiro, Portugal
{diego,raquel,vfssantana}@ua.pt
2 Department of Mathematics, University of Zurich,
Winterthurstrasse 190, 8057 Z¨urich, Switzerland
rosenthal@math.uzh.ch
Abstract. In this paper, we deal with the so-called multi-shot network
coding, meaning that the network is used several times (shots) to propa-
gate the information. The framework we present is slightly more general
than the one which can be found in the literature. We study and intro-
duce the notion of column rank distance of rank metric convolutional
codes for any given rate and ﬁnite ﬁeld. Within this new framework we
generalize previous results on column distances of Hamming and rank
metric convolutional codes [3,8]. This contribution can be considered as
a continuation follow-up of the work presented in [10].
1
Introduction
The theory of Random Linear Network Coding has been mainly devoted to non-
coherent one-shot network coding, meaning that the random structure of the
network is used just once to propagate information. One of the problems in this
situation is that in order to increase the error-correcting capabilities of a code,
one necessarily needs to increase the ﬁeld size or the packet size and this might
not be optimal or impossible in many applications.
Hence, in these situations one of the solutions proposed is to create dependen-
cies across multiple shots aiming to approach the channel capacity. In fact, it was
been recently shown that spreading redundancy among the transmitted code-
words (row spaces) at diﬀerent instances (shots) can improve the error-correction
capabilities of the code. These ideas gave rise to the area of multi-shot network
coding. Although the potential of using multi-shot network coding was already
observed in the seminal paper [5], only recently this interesting approach has
been investigated [1,7,14,16].
There are basically two ways for constructing multi-shot codes: one using
concatenation of codes and other using rank metric convolutional codes. In [14],
D. Napp, R. Pinto, F. Santana—This work was supported by Portuguese funds
through the Center for Research and Development in Mathematics and Applica-
tions (CIDMA), and The Portuguese Foundation for Science and Technology (FCT
- Funda¸c˜ao para a Ciˆencia e a Tecnologia), within project UID/MAT/04106/2013.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 248–256, 2017.
DOI: 10.1007/978-3-319-66278-7 21

Column Rank Distances of Rank Metric Convolutional Codes
249
a concatenated code was introduced based on a multilevel code construction. In
[12], a concatenation scheme was presented using a Hamming metric convolu-
tional code as an outer code and a rank metric code as an inner code. A diﬀerent
type of concatenation was introduced in [7] where the authors use codes that
layer both Maximum Sum Rank (MSR) codes and Gabidulin in order to achieve
the streaming capacity for the Burst Erasure Channel.
Apart from concatenated codes, another very natural way to spread redun-
dancy across codewords is by means of convolutional codes [2–4,9,13]. Adapting
this class of codes to the context of networks brought about the notion of rank
metric convolutional codes and interestingly there has been little research on
these codes, see [1,6–8,16]. The work in [16] was pioneer in this direction by pre-
senting the ﬁrst class of rank metric convolutional codes together with a decoding
algorithm able to deal with errors, erasures and deviations. However, the results
were only valid for unit memory convolutional codes and in [1,6–8] (see also the
references therein) an interesting and more general class of rank metric convo-
lutional codes was introduced to cope with network streaming applications.
In this paper we continue our work in [10] and propose a framework slightly
more general than the existing ones in the literature on rank metric convolutional
codes. In the proposed framework, rank metric codes can be deﬁned for all rates
and ﬁelds. In this setting, an extension of the standard rank metric has been
considered to provide the proper measure for the number of rank erasures that
a multi-shot network code can tolerate. Here we continue this line of work and
investigate the notion of column rank distance of rank metric convolutional codes
in this more general setting. We show that the existing results on column distance
in both contexts of Hamming [3] and rank metric [8] can be generalized to this
more general point of view.
2
Convolutional Codes
Let F be a ﬁnite ﬁeld and F[D] be the ring of polynomials with coeﬃcients in F.
A convolutional code C of rate k/n is an F[D]-submodule of F[D]n, with rank
k. If G(D) ∈F[D]k×n is a full row rank matrix such that
C = imFq[D]G(D) =

u(D)G(D) : u(D) ∈F[D]k
,
then G(D) is called an encoder of C.
Any other encoder ˜G(D) of C diﬀer from G(D) by left multiplication by a
unimodular matrix U(D) ∈F[D]k×k, i.e., ˜G(D) = U(D)G(D). Therefore, if C
admits a left prime convolutional encoder then all its encoders are left prime.
Such a code is called observable.
A convolutional code always admits a minimal encoder, i.e., in row reduced
form1. The sum of the row degrees of a minimal encoder attains its minimum
1 A polynomial matrix G(D) ∈F[D]k×n is in row reduced form if the constant matrix
Glrc, called leading row coeﬃcient matrix, constituted by the coeﬃcients of the term
of degree equal to the row degree, is full row rank.

250
D. Napp et al.
among all the encoders of C. Such sum is usually denoted by δ and called the
degree of C. A rate k/n convolutional code C of degree δ is called an (n, k, δ)
convolutional code [9].
The free distance and the column distances of a convolutional code are impor-
tant measures of the capability of error detection and error correction of the code.
The free distance of a convolutional code C is given by
dfree(C) =
min
v(D)∈C,v(D)̸=0 wt

v(D)

,
where wt

v(D)

is the Hamming weight of a polynomial vector
v(D) =

i∈N0
viDi ∈F[D]n,
deﬁned as
wt

v(D)

=

i∈N0
wt(vi),
being wt(vi) the number of the nonzero components of vi.
Rosenthal and Smarandache [15] showed that the free distance of an (n, k, δ)
convolutional code is upper bounded by
dfree(C) ≤(n −k)
 δ
k
	
+ 1

+ δ + 1.
This bound was called the generalized Singleton bound. An (n, k, δ) convolutional
code whose free distance is equal to the generalized Singleton bound is called
maximum distance separable (MDS) code [15].
Let us now consider the column distances of an (n, k, δ) convolutional code
C. For that we will consider that C is observable. Observable convolutional codes
admit a kernel representation H(D) ∈F[D](n−k)×n, i.e. such that C = ker H(D).
Let
G(D) =
ν

j=0
GjDj ∈F[D]k×n, Gi ∈Fk×n, Gν ̸= 0
be an encoder of C and
H(D) =
μ

j=0
HjDj ∈F[D](n−k)×n, Hi ∈F(n−k)×n, Hμ ̸= 0
be a parity-check matrix of C. For every j ∈N0, the truncated sliding generator
matrices Gc
j ∈F(j+1)k×(j+1)n and the truncated sliding parity-check matrices
Hc
j ∈F(j+1)(n−k)×(j+1)n are given by
Gc
j =
⎡
⎢⎢⎢⎣
G0 G1 · · · Gj
G0 · · · Gj−1
...
...
G0
⎤
⎥⎥⎥⎦

Column Rank Distances of Rank Metric Convolutional Codes
251
Hc
j =
⎡
⎢⎢⎢⎣
H0
H1
H0
...
...
...
Hj Hj−1 . . . H0
⎤
⎥⎥⎥⎦,
respectively, and when j > ν, we let Gj = 0 and when j > μ, Hj = 0.
Using the above assumptions the j-th column distance of C is given by
dc
j = min{wt((v(D))|[0,j]) : v0 ̸= 0}
= min{wt([v0v1 · · · vj]) : [v0v1 · · · vj] = [u0u1 · · · uj]Gc
j, ui ∈Fk, u0 ̸= 0}
= min{wt(v), v = (v0, . . . , vj) ∈F(j+1)n, v(Hc
j )T = 0, v0 ̸= 0},
where v(D) =

i∈N0
viDi and (v(D))|[0,j] =
j

i=0
viDi.
The following results give a bound on the column distances of an (n, k, δ)
convolutional code and some properties of these distances.
Proposition 1 ([3, Proposition 2.2]). Let C be an (n, k, δ) convolutional code.
For every j ∈N0 we have
dc
j ≤(n −k)(j + 1) + 1.
Corollary 2 ([3, Corollary 2.3]). Let C be an (n, k, δ) convolutional code. If
dc
j = (n −k)(j + 1) + 1 then dc
i = (n −k)(i + 1) + 1, for every i ≤j.
Proposition 3 ([3, Proposition 2.7]). Let C be an MDS (n, k, δ) convolutional
code with column distances dc
j, j ∈N0 and free distance dfree. Let M = min{j ∈
N0, dc
j = dfree}. Then,
M ≥
 δ
k
	
+

δ
n −k

.
3
Rank Metric Convolutional Codes
In this section we will deﬁne rank metric convolutional codes whose codewords
are polynomials matrices in F[D]n×m and we aim to further explore this more
general approach in order to introduce the deﬁnition of column rank distances
of a rank metric convolutional code and to propose a bound on this important
measure.
A rank metric convolutional code C ⊂Fn×m is the image of an homomor-
phism ϕ : F[D]k →F[D]n×m. We write ϕ = ψ◦γ as a composition of a monomor-
phism γ and an isomorphism ψ:
ϕ : F[D]k
γ
−→
F[D]nm
ψ
−→F[D]n×m
u(D) 	→v(D)=u(D)G(D)	→V (D)
(1)

252
D. Napp et al.
where G(D) ∈Fk×nm is a full row rank polynomial matrix, called encoder of C,
and let V (D) = rmatn×m

v(D)

, such that Vi,j(D) = vmi+j(D), i.e., the rows
of V (D) are n consecutive blocks with m elements of v(D).
As for convolutional codes, two encoders of C diﬀer by left multiplication
by a unimodular matrix and therefore C always admits minimal encoders. The
degree δ of a rank metric convolutional code C is the sum of the row degrees of
a minimal encoder of C, i.e. the minimum value of the sum of the row degrees of
its encoders. Rank metric convolutional codes with left prime encoders will also
be called observable.
A rank metric convolutional code C of degree δ, deﬁned as in (1), is called
an (n × m, k, δ)-rank metric convolutional code.
When dealing with rank metric codes a diﬀerent measure of distance must
be considered. The rank weight of a polynomial matrix A(D) = 
i∈N0 AiDi ∈
F[D]n×m, is given by
rwt

A(D)

=

i∈N0
rankAi.
(2)
If B(D) = 
i∈N0 Bi ∈F[D]n×m, we deﬁne the sum rank distance between
A(D) and B(D) as
dsr

A(D), B(D)

= rwt

A(D) −B(D)

(3)
=

i∈N0
rank(Ai −Bi).
Lemma 4. The sum rank distance dsr is a distance in F[D]n×m.
Next we will focus on two sum rank distances deﬁnitions of a rank metric
convolutional code. The sum rank distance deﬁned in [10,11] and the novel notion
of column rank distance.
The sum rank distance of a rank metric convolutional code C is deﬁned as
dsr(C) =
min
V (D),U(D)∈C,V (D)̸=U(D) dsr(V (D), U(D))
=
min
0̸=V (D)∈C rwt

V (D)

.
Next theorem establishes a bound on the sum rank distance of a rank metric
convolutional code. Analogously as for the free distance of a convolutional code,
this bound is referred as the Singleton bound for rank metric convolutional codes.
Theorem 5 ([10, Theorem 3] [11, Theorem 3]). Let C be an (n × m, k, δ) rank
metric convolutional code. Then the sum rank distance of C is upper bounded by
dSR(C) ≤n
 δ
k
	
+ 1

−

k(
 δ
k

+ 1) −δ
m

+ 1.
(4)
An (n × m, k, δ) rank metric convolutional code whose sum rank distance
attains the Singleton bound is called Maximum Rank Distance (MRD).
Let us now restrict to (n × m, k, δ) observable codes.

Column Rank Distances of Rank Metric Convolutional Codes
253
Deﬁnition 6. Let C be an (n × m, k, δ) observable rank metric convolutional
code. For j ∈N0 we deﬁne the j-th column rank distance of C as
dcr
j = min{rwt(V (D)|[0,j]) : V (D) ∈C and V0 ̸= 0},
where for V (D) =

i∈N0
ViDi we deﬁne V (D)|[0,j] =
j

i=0
ViDi.
Theorem 7. Let C be an (n×m, k, δ) observable rank metric convolutional code.
Then the j-th column rank distance of C is upper bounded by
dcr
j ≤j

n −
 k
m
	
+ n −
k −1
m
	
Proof. Let G(D) =

i∈N0
GiDi be an encoder of C. Since G0 is full row rank it
admits an invertible k × k submatrix. We can assume without loss of generality
that the k × k submatrix of G0 constituted by the ﬁrst k columns is invertible.
We will prove the theorem by induction on j. For j = 0 let u0 ∈Fk be such
that v0 = u0G0 has the ﬁrst k−1 entries equal to zero, i.e., wt(v0) ≤nm−k+1,
and let V0 = rmatn×m(v0). Then the ﬁrst
 k−1
m

rows of V0 are equal to zero
and therefore rwt(V0) ≤n −
 k−1
m

and therefore dcr
0 ≤n −
 k−1
m

.
Let us suppose now that dcr
j ≤j

n −
 k
m

+n−
 k−1
m

and let us prove that
dcr
j+1 ≤(j+1)

n −
 k
m

+n−
 k−1
m

. Let u(D) ∈F[D]k, v(D) = u(D)G(D) and
V (D) = rmatn×m(v(D)) =

i∈N0
ViDi ∈C be such that rwt(V (D)|[0,j]) = dcr
j .
Moreover, since the k × k submatrix of G0 constituted by the ﬁrst k columns is
invertible, we can consider uj+1 such that vj+1 = uj+1G0+uj−1G1+· · ·+u0Gj+1
has the ﬁrst k entries equal to zero. Then
dcr
j+1 ≤rwt((V (D))|[0,j+1])
= dcr
j + rwt(Vj+1)
≤j

n −
 k
m
	
+ n −
k −1
m
	
+ n −
 k
m
	
= (j + 1)

n −
 k
m
	
+ n −
k −1
m
	
.
With a similar reasoning as in the proof of the above theorem we can prove
that if the j-th column distance of a rank metric convolutional code achieves the
corresponding bound then the same happens for all the i-th column distances
for i < j.
Theorem 8. Let C be an (n×m, k, δ) observable rank metric convolutional code.
If dcr
j
= j

n −
 k
m

+ n −
 k−1
m

for some j ∈N0, then dcr
i
= i

n −
 k
m

+
n −
 k−1
m

for all i ≤j.

254
D. Napp et al.
Proof. It is enough to prove that dcr
j
= j

n −
 k
m

+ n −
 k−1
m

implies
that dcr
j−1 = (j −1)

n −
 k
m

+ n −
 k−1
m

. Let us assume that dcr
j−1 <
(j −1)

n −k
m

+ n −
 k−1
m

and let u(D) ∈F[D]k, v(D) = u(D)G(D) and
V (D) = rmatn×m(v(D)) =

i∈N0
ViDi ∈C be such rwt(V (D))|[0,j−1] = dcr
j−1. Let
uj be such that vj = u0Gj+u1Gj−1+· · ·+uj−1G1+ujG0 has weight nm−k. Then
rank(Vj) ≤n−
 k
m

and, therefore,wrank(V (D)[0,j]) < j

n −
 k
m

+n−
 k−1
m

.
Consequently, dcr
j < j

n −
 k
m

+ n −
 k−1
m

It is obvious that the sequence of column rank distances of the code is non-
decreasing. However, there exists an M ∈N0 such that dcr
M = dcr
j
for j > M
since the column rank distances of a rank convolutional code can not be greater
than the sum rank distance of the code. If the code is MRD then M is precisely
determined as stated in the next result.
Proposition 9. Let C be an MRD (n×m, k, δ) observable rank metric convolu-
tional code with column rank distances dcr
j , j ∈N0, and sum rank distance dSR.
Let M = min{j ∈N0, dcr
j = dSR}. Then,
M =
⎡
⎢⎢⎢⎢⎢
n
 δ
k

+

δ−k⌊δ
k⌋
m
	
n −
 k
m

⎤
⎥⎥⎥⎥⎥
Proof. Let ˜
M =
n⌊δ
k⌋+

δ−k⌊δ
k⌋
m

n−⌊k
m⌋
. We will consider two cases, when m | k and
when m ∤k.
Case 1: m | k. Then
˜
M

n −
 k
m
	
+ n −
k −1
m
	
= n
 δ
k
	
+

δ −k
 δ
k

m

+ n −
k −1
m
	
= n
 δ
k
	
+ 1

−k
m
 δ
k
	
+
 δ
m
	
−
k −1
m
	
.
Then, since
 k−1
m

= k
m −1, we have that
˜
M

n −
 k
m
	
+ n −
k −1
m
	
= n
 δ
k
	
+ 1

−k
m
 δ
k
	
+ 1

+
 δ
m
	
+ 1
= n
 δ
k
	
+ 1

−

k(
 δ
k

+ 1) −δ
m

+ 1
= dsr.

Column Rank Distances of Rank Metric Convolutional Codes
255
Case 2: m ∤k. In this case
˜
M

n −k
m

+ n −
 k −1
m

= n
 δ
k

+ 1

+

δ −k
 δ
k
	
m

−
 k −1
m

= n
 δ
k

+ 1

−

k
 δ
k
	
+ 1

−δ −k
m

−
 k −1
m

= n
 δ
k

+ 1

−

k
 δ
k
	
+ 1

−δ
m

−
 k
m

−
 k −1
m

= n
 δ
k

+ 1

−

k
 δ
k
	
+ 1

−δ
m

+ 1
= dsr,
because
 k
m

−
 k−1
m

= 1.
In both cases M = ⌈˜
M⌉.
References
1. Badr, A., Khisti, A., Tan, W.-T., Apostolopoulos, J.: Layered constructions for
low-delay streaming codes. IEEE Trans. Inf. Theor. (2013)
2. Climent, J.J., Napp, D., Perea, C., Pinto, R.: Maximum distance separable 2D
convolutional codes. IEEE Trans. Inf. Theor. 62(2), 669–680 (2016)
3. Gluesing-Luerssen, H., Rosenthal, J., Smarandache, R.: Strongly MDS convolu-
tional codes. IEEE Trans. Inf. Theor. 52(2), 584–598 (2006)
4. Johannesson, R., Zigangirov, K.S.: Fundamentals of Convolutional Coding. IEEE
Press, New York (1999)
5. K¨otter, R., Kschischang, F.R.: Coding for errors and erasures in random network
coding. IEEE Trans. Inf. Theor. 54(8), 3579–3591 (2008)
6. Mahmood, R.: Rank metric convolutional codes with applications in network
streaming. Master of applied science (2015)
7. Mahmood, R., Badr, A., Khisti, A.: Streaming-codes for multicast over burst era-
sure channels. IEEE Trans. Inf. Theor. 61(8), 4181–4208 (2015)
8. Mahmood, R., Badr, A., Khisti, A.: Convolutional codes with maximum column
sum rank for network streaming. IEEE Trans. Inf. Theor. 62(6), 3039–3052 (2016)
9. McEliece, R.J.: The algebraic theory of convolutional codes. In: Pless, V., Huﬀman,
W.C. (eds.) Handbook of Coding Theory, vol. 1, pp. 1065–1138. Elsevier Science
Publishers, Amsterdam (1998)
10. Napp, D., Pinto, R., Rosenthal, J., Vettori, P.: Rank metric convolutional codes.
In: Proceedings of the 22nd International Symposium on Mathematical Theory of
Network and Systems (MTNS), Minnesota (2016)
11. Napp, D., Pinto, R., Rosenthal, J., Vettori, P.: MRD rank metric convolutional
codes. In: IEEE International Symposium on Information Theory (ISIT) (2017)
12. Napp, D., Pinto, R., Sidorenko, V.R.: Concatenation of convolutional codes and
rank metric codes for multi-shot network coding. Des. Codes Cryptogr
13. Napp, D., Pinto, R., Toste, M.: On MDS convolutional codes over Zpr. Des. Codes
Cryptogr. 83, 101–114 (2017)
14. N´obrega, R.W., Uchoa-Filho, B.F.: Multishot codes for network coding using rank-
metric codes. In: Wireless Network Coding Conference (WiNC), pp. 1–6. IEEE,
June 2010

256
D. Napp et al.
15. Rosenthal, J., Smarandache, R.: Maximum distance separable convolutional codes.
Appl. Algebra Eng. Commun. Comput. 10(1), 15–32 (1999)
16. Wachter-Zeh, A., Stinner, M., Sidorenko, V.: Convolutional codes in rank metric
with application to random network coding. IEEE Trans. Inf. Theor. 61(6), 3199–
3213 (2015)

On Minimality of ISO Representation
of Basic 2D Convolutional Codes
Raquel Pinto and Rita Sim˜oes(B)
CIDMA – Center for Research and Development in Mathematics and Applications,
Department of Mathematics, University of Aveiro,
Campus Universit´ario de Santiago, 3810-193 Aveiro, Portugal
{raquel,ritasimoes}@ua.pt
Abstract. In this paper we study the minimality of input-state-output
(ISO) representations of basic two-dimensional (2D) convolutional codes.
For that we consider the Fornasini-Marchesini ISO representations of
such codes. We deﬁne the novel property of strongly modally reachable
representations and we show that such representations are minimal rep-
resentations of a basic 2D convolutional code. Moreover, we prove that
the dimension of such minimal representations equals the complexity of
the code.
1
Introduction
Two-dimensional (2D) convolutional codes are a natural generalization of one-
dimensional (1D) convolutional codes. These codes are naturally suitable to deal
with data recorded in two dimensions, like pictures, video, data storage, etc.
[9,16,19]. 2D/nD convolutional codes were introduced in [7,8,17,18]. In [1,3] the
authors introduce the “locally invertible encoders” and the “Two-Dimensional
Tail-Biting Convolutional Codes” with the objective of obtaining constructions
of 2D convolutional codes with particular decoding properties. Decoding of 2D
convolutional codes over the erasure channel was investigated in [4]. In [12]
the authors deﬁne input-state-output (ISO) representations of 2D convolutional
codes, in which the codewords of a code are generated by a 2D linear system.
One of the most important problems studied in the theory of convolutional
codes is the minimality of representations of these codes, i.e., the determination
of ISO representations with minimal dimension among all ISO representations
of the code. Minimality of an ISO representation leads to more eﬃcient practical
implementations in terms of the memory space required. This problem is com-
pletely solved when we consider 1D convolutional codes [14,15]. However, this
seems to be very hard in the 2D case. We address this problem by considering the
Fornasini-Marchesini state-space model, originally studied in the theory of 2D
This work was supported by Portuguese funds through the Center for Research and
Development in Mathematics and Applications (CIDMA), and The Portuguese Foun-
dation for Science and Technology (FCT - Funda¸c˜ao para a Ciˆencia e a Tecnologia),
within project UID/MAT/04106/2013.
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 257–271, 2017.
DOI: 10.1007/978-3-319-66278-7 22

258
R. Pinto and R. Sim˜oes
linear systems [6]. Unlike the 1D case, it does not exist necessary and suﬃcient
conditions for the minimality of a realization of a 2D polynomial matrix (i.e., a
polynomial matrix in two indeterminates), which makes very hard to solve the
general problem of minimality of ISO representations of 2D convolutional codes.
In this paper we consider basic 2D convolutional codes, i.e., 2D convolutional
codes which are image of a zero left prime 2D polynomial matrix. We introduce
the concept of strongly modally 2D linear system and we show that if a 2D basic
convolutional code admits a strongly modally reachable ISO representation then
this representation is minimal. Moreover, we prove that the dimension of this
minimal ISO representation is equal to the complexity of the code.
2
Convolutional Codes
In this section we will introduce 1D and 2D convolutional codes and a repre-
sentation of these codes by means of a linear system. We start by giving some
preliminaries on polynomial matrices in one indeterminate and in two indeter-
minates that will be important for the deﬁnition of these codes.
2.1
Polynomial Matrices
Let F be a ﬁeld and let F denote the algebraic closure of F. Denote by F[z]
the ring of polynomials in one indeterminate with coeﬃcients in F, by F(z) the
ﬁeld of fractions of F[z] and by F[[z]] the ring of formal powers series in one
indeterminate with coeﬃcients in F.
Deﬁnition 1 (Chap. 6, [10]). A matrix U(z) ∈F[z]n×k, with n ≥k is,
(a) unimodular (i.e., it admits a polynomial inverse) if n = k and det(U(z)) ∈
F\{0};
(b) right prime (rP) if for every factorization
U(z) = U(z)T(z),
with U(z) ∈F[z]n×k and T(z) ∈F[z]n×n, T(z) is unimodular.
A matrix is left prime (ℓP) if its transpose is rP.
The following lemma gives characterizations of right primeness that will be
needed later.
Lemma 1 (Chap. 6, [10]). Let U(z) ∈F[z]n×k, with n ≥k. Then the following
are equivalent:
(a) U(z) is rP;
(b) there exists P(z) ∈F[z]n×(n−k) such that
U(z) P(z)
is unimodular;
(c) U(z) admits a polynomial left inverse;
(d) the k × k minors of U(z) have no common factor;
(e) for all ˆv(z) ∈F(z)n, ˆv(z)T U(z) ∈F[z]n implies that ˆv(z) ∈F[z]n;
(f) U(λ) is full column rank, for all λ ∈F.

On Minimality of ISO Representation of Basic 2D Convolutional Codes
259
Let us consider now polynomial matrices in two indeterminates. Denote by
F[z1, z2] the ring of polynomials in two indeterminates with coeﬃcients in F, by
F(z1, z2) the ﬁeld of fractions of F[z1, z2] and by F[[z1, z2]] the ring of formal
powers series in two indeterminates with coeﬃcients in F.
Deﬁnition 2 ([7]). A matrix G(z1, z2) ∈F[z1, z2]n×k, with n ≥k is,
(a) unimodular (i.e., it admits a polynomial inverse) if n = k and det
(G(z1, z2)) ∈F\{0};
(b) right factor prime (rFP) if for every factorization
G(z1, z2) = G(z1, z2)T(z1, z2),
with G(z1, z2) ∈F[z1, z2]n×k and T(z1, z2) ∈F[z1, z2]k×k, T(z1, z2) is uni-
modular;
(c) right zero prime (rZP) if the ideal generated by the k×k minors of G(z1, z2)
is F[z1, z2].
A matrix is left factor prime (ℓFP)/left zero prime (ℓZP) if its transpose is
rFP/rZP, respectively. When we consider polynomial matrices in one indeter-
minate, the notions (b) and (c) of the above deﬁnition are equivalent. However
this is not the case for polynomial matrices in two indeterminates. In fact, zero
primeness implies factor primeness, but the contrary does not happen. The fol-
lowing lemmas give characterizations of right factor primeness and right zero
primeness that will be needed later (see [11,13]).
Lemma 2. Let G(z1, z2) ∈F[z1, z2]n×k, with n ≥k. Then the following are
equivalent:
(a) G(z1, z2) is right factor prime;
(b) for all ˆu(z1, z2) ∈F(z1, z2)k, G(z1, z2)ˆu(z1, z2) ∈F[z1, z2]n implies that
ˆu(z1, z2) ∈F[z1, z2]k;
(c) the k × k minors of G(z1, z2) have no common factor.
Lemma 3. Let G(z1, z2) ∈F[z1, z2]n×k, with n ≥k. Then the following are
equivalent:
(a) G(z1, z2) is right zero prime;
(b) G(z1, z2) admits a polynomial left inverse;
(c) G(λ1, λ2) is full column rank, for all λ1, λ2 ∈F.
It is well known (see [7]) that given a full column rank polynomial matrix
G(z1, z2) ∈F[z1, z2]n×k, there exists a square polynomial matrix V (z1, z2) ∈
F[z1, z2]k×k and a rFP matrix ¯G(z1, z2) ∈F[z1, z2]n×k such that
G(z1, z2) = ¯G(z1, z2)V (z1, z2).
The following lemma will be needed in the sequel. Let H(z1, z2)
∈
F[z1, z2](n−k)×n, G(z1, z2) ∈F[z1, z2]n×k, n > k, ci the ith column of H(z1, z2)

260
R. Pinto and R. Sim˜oes
and rj the jth row of G(z1, z2). We say that the full size minor of H(z1, z2)
constituted by the columns ci1, . . . , cin−k and the full size minor of G(z1, z2)
constituted by the rows rj1, . . . , rjk are corresponding maximal order minors of
H(z1, z2) and G(z1, z2), if
{i1, ..., in−k} ∪{j1, ..., jk} = {1, . . . , n}
and {i1, ..., in−k} ∩{j1, ..., jk} = ∅.
Lemma 4 (Proposition A.4., [7]). Let H(z1, z2) ∈F[z1, z2](n−k)×n and
G(z1, z2) ∈F[z1, z2]n×k be a ℓFP and a rFP matrices, respectively, such
that H(z1, z2)G(z1, z2) = 0. Then the corresponding maximal order minors of
H(z1, z2) and G(z1, z2) are equal, modulo a unit of the ring F[z1, z2].
2.2
1D Convolutional Codes
A 1D (ﬁnite support) convolutional code C of rate k/n is a (free) F[z]-submodule
of F[z]n, where k is the rank of C. A full column rank matrix G(z) ∈F[z]n×k
such that
C = ImF[z] G(z)
=

ˆv(z) ∈F[z]n | ˆv(z) = G(z)ˆu(z), with ˆu(z) ∈F[z]k
,
is called an encoder of C. The elements of C are called codewords. Two full
column rank matrices G(z), ¯G(z) ∈F[z]n×k are equivalent encoders, i.e. they
generate the same 1D convolutional code, if and only if G(z)U(z) = ¯G(z) for
some unimodular matrix U(z) ∈F[z]k×k. We denote the complexity (or degree)
δ of a 1D convolutional code C as the maximum of the degree of the k×k minors
of any encoder of C and we say that C is an (n, k, δ) 1D convolutional code.
Note that the fact that two equivalent encoders diﬀer by unimodular matrices
also implies that the primeness properties of the encoders of a code are preserved,
i.e., if C admits a rP encoder then all its encoders are rP. A 1D convolutional
code C that admits a rP encoder is called basic (or noncatastrophic) [14,15].
Another way of obtaining the codewords of a 1D convolutional code is by
means of a 1D linear system. A 1D linear system, denoted by Σ =(A, B, C, D),
is given by the updating equations
x(t + 1) = Ax(t) + Bu(t)
y(t) = Cx(t) + Du(t),
(1)
where A ∈Fs×s, B ∈Fs×k, C ∈F(n−k)×s, D ∈F(n−k)×k, s, n, k ∈N, n > k and
with x(0) = 0. We say that Σ has dimension s. The vectors x(t), u(t) and y(t)
represent the local state, input and output at instant t, respectively.
The input, state and output 1D sequences (trajectories), {u(t)}t∈N,
{x(t)}t∈N, {y(t)}t∈N, respectively, can be represented as formal power series:
ˆu(z) =

t∈N
u(t)zt ∈F[[z]]k,

On Minimality of ISO Representation of Basic 2D Convolutional Codes
261
ˆx(z) =

t∈N
x(t)zt ∈F[[z]]δ,
ˆy(z) =

t∈N
y(t)zt ∈F[[z]]n−k.
In the sequel we shall use the sequence and the corresponding series interchange-
ably.
Since the codewords of a 1D convolutional code have ﬁnite support, we will
only consider the ﬁnite support input-output trajectories (ˆu(z), ˆy(z)) of (1).
Moreover, we will restrict to the ﬁnite support input-output trajectories with
corresponding state trajectory ˆx(z) also with ﬁnite support, otherwise the system
would remain indeﬁnitely excited. The ﬁnite support input-output trajectories
(ˆu(z), ˆy(z)) with corresponding state ˆx(z) also having ﬁnite support are called
ﬁnite-weight input-output trajectories. The set of all these trajectories form a 1D
convolutional code, as it is stated in the following theorem (see [15]).
Theorem 1. The set of ﬁnite-weight input-output trajectories of (1) is a 1D
convolutional code of rate k/n.
We denote by C(A, B, C, D) the 1D convolutional code whose codewords
are the ﬁnite-weight input-output trajectories of the 1D linear system Σ =
(A, B, C, D). Moreover, Σ is called an input-state-output (ISO) representation
of C(A, B, C, D). All the 1D convolutional codes admit (many) ISO representa-
tions. Next we will consider some properties of an ISO representation of a 1D
convolutional code C and see how these properties are reﬂected on C.
Deﬁnition 3 (Chap. 6, [10]).
Let Σ = (A, B, C, D) be a 1D linear system
with dimension s.
(a) Σ is reachable if the reachability matrix
R =
B AB A2B · · · An−1B
is full row rank, or equivalently, if the matrix
Is −Az Bz
is ℓP.
(b) Σ is observable if the observability matrix
O =
⎡
⎢⎢⎢⎣
C
CA
...
CAn−1
⎤
⎥⎥⎥⎦
is full column rank, or equivalently, if the matrix

Is −Az
C

is rP.
Theorem 2 (Lemma 2.1.1., [14]). Let Σ be a reachable ISO representation
of a 1D convolutional code C. Then C is basic (or noncatastrophic) if and only
if Σ is observable.

262
R. Pinto and R. Sim˜oes
An ISO representation of a 1D convolutional code is said to be minimal if it
has minimal dimension among all the ISO representations of the code. Minimal-
ity is an important property in the sense that minimal ISO representations are
more eﬃcient because they require less memory space in their implementation.
Moreover, such representations have also strong structural properties which can
be useful in the construction of good codes or in the implementation of decoding
algorithms.
Next theorem gives a characterization of the minimal ISO representations of
a 1D convolutional code and shows how these minimal ISO representations are
related.
Lemma 5 (Theorem 3.4., [14]). Let Σ be an ISO representation of an (n, k, δ)
1D convolutional code C. Then Σ is an minimal ISO representation of C if and
only if Σ is reachable.
Moreover, a minimal ISO representation Σ = (A, B, C, D) of C has dimen-
sion δ and any other minimal ISO representation of C is of the form ˜Σ =

SAS−1, SB, CS−1, D

, where S is a δ × δ invertible constant matrix.
Note that if Σ is a minimal ISO representation of C then the complexity of
C is equal to the dimension of Σ.
We can obtain a minimal ISO representation of an (n, k, δ) 1D convolutional
code C from any ISO representation Σ = (A, B, C, D) of C, with dimension s ≥δ.
For that we consider a s × s invertible constant matrix S such that
SAS−1 =

A11 A12
0
A22

,
SB =

B1
0

,
CS−1 =
C1 C2

where A11 ∈Fδ×δ, B1 ∈Fδ×k and C1 ∈F(n−k)×δ and
Iδ −A11z B1

is ℓP.
Such representation is in the Kalman reachability canonical form (see [10]). Then
Σ1 = (A11, B1, C1, D) is a minimal ISO representation of C (see [15]).
2.3
2D Convolutional Codes
A 2D (ﬁnite support) convolutional code C of rate k/n is a free F[z1, z2]-
submodule of F[z1, z2]n, where k is the rank of C. A full column rank matrix
G(z1, z2) ∈F[z1, z2]n×k whose columns constitute a basis for C, i.e., such that
C = ImF[z1,z2] G(z1, z2)
=

ˆv(z1, z2) ∈F[z1, z2]n | ˆv(z1, z2) = G(z1, z2)ˆu(z1, z2), with ˆu(z1, z2) ∈F[z1, z2]k
,
is called an encoder of C. The elements of C are called codewords. Two full column
rank matrices G(z1, z2), ¯G(z1, z2) ∈F[z1, z2]n×k are equivalent encoders if they
generate the same 2D convolutional code, i.e., if
ImF[z1,z2] G(z1, z2) = ImF[z1,z2] ¯G(z1, z2),

On Minimality of ISO Representation of Basic 2D Convolutional Codes
263
which happens if and only if there exists a unimodular matrix U(z1, z2) ∈
F[z1, z2]k×k such that G(z1, z2)U(z1, z2) = ¯G(z1, z2) (see [17]).
Note that the fact that two equivalent encoders diﬀer by unimodular matrices
also implies that the primeness properties of the encoders of a code are preserved,
i.e., if C admits a rFP (rZP) encoder then all its encoders are rFP (rZP). A
2D convolutional code C that admits rFP encoders is called noncatastrophic and
it is named basic if all its encoders are rZP. Finally, we denote the complexity δ
of a 2D convolutional code C as the maximum of the degree of the k × k minors
of any encoder of C.
2D convolutional codes can also be represented by a linear system. Unlike the
1D case, there are several state space models of a 2D linear system. In this paper
we consider the Fornasini-Marchesini state-space models (see [6]). In this model
a ﬁrst quarter plane 2D linear system, denoted by Σ =(A1, A2, B1, B2, C, D), is
given by the updating equations
x(i+1, j+ 1) = A1x(i, j+1) + A2x(i+1, j) + B1u(i, j+1) + B2u(i+1, j)
y(i, j) = Cx(i, j) + Du(i, j),
(2)
where A1, A2 ∈Fs×s, B1, B2 ∈Fs×k, C ∈F(n−k)×s, D ∈F(n−k)×k, s, n, k ∈N,
n > k and with past ﬁnite support of the input and of the state (i.e., u(i, j) = 0
and x(i, j) = 0, where 0 denotes the zero vector of appropriate lenght,t for
i < 0 or j < 0) and zero initial conditions (i.e., x(0, 0) = 0). We say that Σ
has dimension s. The vectors x(i, j), u(i, j) and y(i, j) represent the local state,
input and output at (i, j), respectively.
We will also represent the input, state and output 2D trajectories,
{u(i, j)}(i,j)∈N2, {x(i, j)}(i,j)∈N2, {y(i, j)}(i,j)∈N2 as formal power series,
ˆu(z1, z2)=

(i,j)∈N2
u(i, j)zi
1zj
2 ∈F[[z1, z2]]k,
ˆx(z1, z2)=

(i,j)∈N2
x(i, j)zi
1zj
2 ∈F[[z1, z2]]δ,
ˆy(z1, z2)=

(i,j)∈N2
y(i, j)zi
1zj
2 ∈F[[z1, z2]]n−k.
For the same reasons stated for 1D convolutional codes we will restrict our-
selves to ﬁnite support input-output trajectories (ˆu(z1, z2), ˆy(z1, z2)) with cor-
responding state ˆx(z1, z2) also having ﬁnite support, i.e., to the ﬁnite-weight
input-output trajectories. Next theorem states that the set of these trajectories
also constitute a 2D convolutional code.
Theorem 3 (Theorem 1, [12]). The set of ﬁnite-weight input-output trajec-
tories of (2) is a 2D convolutional code of rate k/n.
Proof. Let us denote by S and Sio the set of ﬁnite-weight trajectories and the
set of ﬁnite-weight input-output trajectories of (2), respectively. Then
S = kerF[[z1,z2]] X(z1, z2) ∩F[z1, z2]n+δ = kerF(z1,z2) X(z1, z2) ∩F[z1, z2]n+δ,

264
R. Pinto and R. Sim˜oes
where
X(z1, z2) =
Is −A1z1 −A2z2 −B1z1 −B2z2
0
−C
−D
In−k

.
Since kerF(z1,z2) X(z1, z2) has dimension k, there exists an rFP matrix such that
kerF(z1,z2) X(z1, z2) = ImF(z1,z2) ˜L(z1, z2),
and as ˜L(z1, z2) is rFP, we use Lemma 2 to conclude that S = ImF[z1,z2] ˜L(z1, z2).
Representing
˜L(z1, z2) =
˜L1(z1, z2)
˜L2(z1, z2)

,
with ˜L1(z1, z2) ∈F[z1, z2]δ×k and ˜L2(z1, z2) ∈F[z1, z2]n×k, it follows that Sio =
ImF[z1,z2] ˜L2(z1, z2). Let F(z1, z2) ∈F[z1, z2](δ+n−k)×(δ+n−k) be a nonsingular
square matrix such that
X(z1, z2) = F(z1, z2)
M1(z1, z2) M2(z1, z2) M3(z1, z2)
,
where
M1(z1, z2)
∈
F[z1, z2](δ+n−k)×δ,
M2(z1, z2) ∈F[z1, z2](δ+n−k)×k, M3(z1, z2) ∈F[z1, z2](δ+n−k)×(n−k) are such
that

M1(z1, z2) M2(z1, z2) M3(z1, z2)

is ℓFP. Then

M1(z1, z2) M2(z1, z2) M3(z1, z2)
 ˜L1(z1, z2)
˜L2(z1, z2)

= 0.
Since det

Iδ −A1z1 −A2z2
0
−C
In−k

is nonzero, it immediately follow that the we
have that det
M1(z1, z2) M3(z1, z2)
̸= 0 and, by Lemma 4, the corresponding
maximal order minor of ˜L2(z1, z2) is also nonzero, which implies that ˜L2(z1, z2)
is full column rank, and therefore Sio is a 2D ﬁnite support convolutional code
with rate k/n.
We denote by C(A1, A2, B1, B2, C, D) the 2D convolutional code whose code-
words are the ﬁnite-weight input-output trajectories of the 2D linear system
Σ = (A1, A2, B1, B2, C, D). Moreover, Σ is called an input-state-output (ISO)
representation of C(A1, A2, B1, B2, C, D) (see [12]).
A 2D convolutional code admits many ISO representations and as happens
in the 1D case, properties of the ISO representations reﬂect on the properties
of the code. 2D linear systems as in (2) admit two types of reachability and
observability notions stated in the following deﬁnition (see [6]).
Deﬁnition 4. Let Σ = (A1, A2, B1, B2, C, D) be a 2D linear system with
dimension s.
(a) Σ is locally reachable if the reachability matrix
R = [R1 R2 R3 · · · ] is full row rank,

On Minimality of ISO Representation of Basic 2D Convolutional Codes
265
where Rk represents the block matrix including all columns deﬁned by

A1
i−1Δj A2

B1 +

A1
iΔj−1 A2

B2
with i + j = k, for i, j ≥0 and
A1
rΔt A2 = 0, when either r or t is negative,
A1
rΔ0 A2 = Ar
1,
A1
0Δt A2 = At
2, for r, t ≥0,
A1
rΔt A2 = A1

A1
r−1Δt A2

+ A2

A1
rΔt−1 A2

, for r, t ≥1.
(b) Σ is modally reachable if the matrix

Is −A1z1 −A2z2 B1z1 + B2z2

is ℓFP.
(c) Σ is modally observable if the matrix

Is −A1z1 −A2z2
C

is rFP.
We will not consider the notion of local observability in this paper. For 1D
linear systems, the notions (a) and (b) (and the corresponding observability
notions) presented in the above deﬁnitions are equivalent. Such equivalence is
stated in the Deﬁnition 3 (see [10]). However, this does not happen in the 2D
case. There are systems which are locally reachable (observable) but not modally
reachable (observable) and vice-versa (see [6]).
Given an input trajectory ˆu(z1, z2) with corresponding state ˆx(z1, z2) and
output ˆy(z1, z2) trajectories obtained from (2), the matrix
ˆr(z1, z2) =
⎡
⎣
ˆx(z1, z2)
ˆu(z1, z2)
ˆy(z1, z2)
⎤
⎦
is called an input-state-output trajectory of Σ = (A1, A2, B1, B2, C, D). The set
of input-state-output trajectories of Σ is given by
kerF[[z1,z2]] X(z1, z2) =

ˆr(z1, z2) ∈F[[z1, z2]]s+n | X(z1, z2)ˆr(z1, z2) = 0

(3)
where
X(z1, z2) =

Is −A1z1 −A2z2 −B1z1 −B2z2
0
−C
−D
In−k

∈F(s+n−k)×(s+n).
(4)
Moreover, there exist polynomial matrices L(z1, z2) ∈F[z1, z2]s×k and
G(z1, z2) ∈F[z1, z2]n×k such that
X(z1, z2)

L(z1, z2)
G(z1, z2)

= 0

266
R. Pinto and R. Sim˜oes
where
 L(z1, z2)
G(z1, z2)

is rFP and G(z1, z2) is an encoder of C(A1, A2, B1, B2, C, D)
(see [12]).
The next result gives us a necessary and suﬃcient condition for modal reach-
ability in terms of the matrix X(z1, z2).
Lemma 6 (Lemma III.4, [5]). Let Σ = (A1, A2, B1, B2, C, D) be a 2D linear
system and X(z1, z2) the corresponding matrix deﬁned in (4). Then Σ is modally
reachable if and only if the matrix X(z1, z2) is ℓFP.
If S is an invertible constant matrix, it is said that the 2D linear systems
Σ = (A1, A2, B1, B2, C, D)
and
˜Σ =

SA1S−1, SA2S−1, SB1, SB2, CS−1, D

are algebraically equivalent (see [6]). Such systems represent the same code, as
stated in the following lemma.
Lemma 7 (Proposition 4, [12]).
Let Σ = (A1, A2, B1, B2, C, D) be a 2D
linear system with dimension s and S a s × s invertible constant matrix. Then
C(A1, A2, B1, B2, C, D)
= C

SA1S−1, SA2S−1, SB1, SB2, CS−1, D

.
An ISO representation of a 2D convolutional code is said to be minimal if
it has minimal dimension among all the ISO representations of the code. Also
in [6], Fornasini and Marchesini generalized the Kalman reachability canonical
form for 2D linear systems, considered in the next deﬁnition, and showed that
every 2D linear system is algebraically equivalent to a system in the Kalman
reachability form.
Deﬁnition 5 ([6]). A 2D linear system Σ = (A1, A2, B1, B2, C, D), with dimen-
sion s, k inputs and n −k outputs is in the Kalman reachability canonical form
if
A1 =

A(1)
11 A(1)
12
0
A(1)
22

, A2 =

A(2)
11 A(2)
12
0
A(2)
22

, B1 =

B(1)
1
0

, B2 =

B(2)
1
0

, C = [C1 C2]
where A(1)
11 , A(2)
11
∈Fδ×δ, B(1)
1 , B(2)
1
∈Fδ×k, C1 ∈F(n−k)×δ, with s ≥δ
and the remaining matrices of suitable dimensions, and Σ1 =

A(1)
11 , A(2)
11 ,
B(1)
1 , B(2)
1 , C1, D

is a locally reachable system, which is the largest locally reach-
able subsystem of Σ.

On Minimality of ISO Representation of Basic 2D Convolutional Codes
267
Proposition 1 (Proposition 4, [12]). Let Σ = (A1, A2, B1, B2, C, D) be a
ISO representation of a 2D convolutional code C. Let S be an invertible constant
matrix such that
˜Σ =

SA1S−1, SA2S−1, SB1, SB2, CS−1, D

is in the Kalman reachability canonical form and let
˜Σ1 =

˜A(1)
11 , ˜A(2)
11 , ˜B(1)
1 , ˜B(2)
1 , ˜C1, D

be the largest locally reachable subsystem of
˜Σ. Then C
= C

˜A(1)
11 , ˜A(2)
11 ,
˜B(1)
1 , ˜B(2)
1 , ˜C1, D

.
The next result follows immediately.
Corollary 1. Minimal ISO representations of a 2D convolutional code must be
locally reachable.
However, it does not exist a suﬃcient condition for minimality of ISO rep-
resentations of a 2D convolutional code. In fact, minimality of these ISO rep-
resentations is a hard problem investigated by many authors which has been
open for many decades. In the next section we will investigate minimality of ISO
representations of basic 2D convolutional codes and we will obtain a suﬃcient
condition for an ISO representation to be minimal.
3
On Minimality of ISO Representations of Basic 2D
Convolutional Codes
In this section we only consider basic 2D convolutional codes.
Let Σ = (A1, A2, B1, B2, C, D) be a modally reachable ISO representation of
a basic 2D convolutional code C and let G(z1, z2) be an encoder of C. Then
X(z1, z2)

L(z1, z2)
G(z1, z2)

= 0
where X(z1, z2) is deﬁned in (4) and L(z1, z2) is a suitable polynomial matrix.
Since G(z1, z2) is rZP, so it is

L(z1, z2)
G(z1, z2)

and, by Deﬁnition 2 and Lemma 4,
X(z1, z2) must be ℓZP.
Next lemma relates the property of left zero primeness of the matrix X(z1, z2)
of a 2D linear system Σ with a special type of modal reachability of Σ.
Lemma 8. Let A1, A2 ∈Fs×s, B1, B2 ∈Fs×k, C ∈F(n−k)×s, D ∈F(n−k)×k,
s, n, k ∈N, n > k. Then

Is −A1z1 −A2z2 B1z1 + B2z2

is ℓZP if and only if the corresponding matrix X(z1, z2) deﬁned in (4) is ℓZP.

268
R. Pinto and R. Sim˜oes
Proof. Suppose that the matrix

Is −A1z1 −A2z2 B1z1 + B2z2

is ℓZP; then
there exist U1(z1, z2) ∈F[z1, z2]s×s and U2(z1, z2) ∈F[z1, z2]k×s such that
Is −A1z1 −A2z2 B1z1 + B2z2
 
U1(z1, z2)
U2(z1, z2)

= Is
So
Is −A1z1 −A2z2 −B1z1 −B2z2
0
−C
−D
In−k
 ⎡
⎣
U1(z1, z2)
0
−U2(z1, z2)
0
CU1(z1, z2) −DU2(z1, z2) In−k
⎤
⎦=Is+n−k
and therefore X(z1, z2) is ℓZP. The other implication follows trivially.
This means that ISO representations Σ = (A1, A2, B1, B2, C, D) with dimen-
sion s of 2D basic convolutional codes which are modally reachable are such
that
Is −A1z1 −A2z2 B1z1 + B2z2

is ℓZP. This property will very impor-
tant throughout the paper. So we propose the next deﬁnition.
Deﬁnition 6. Let
Σ
=
(A1, A2, B1, B2, C, D)
be
a
2D
linear
sys-
tem
with
dimension
s.
Σ
is
said
to
be
strongly
modally
reachable
if

Is −A1z1 −A2z2 B1z1 + B2z2

is ℓZP.
It is obvious that strongly modally reachable systems are also modally reachable,
but the converse is not true.
Next we will consider the projections of a 2D convolutional code C onto
the two semi-axis {ℓei| ℓ∈N}, for i = 1, 2, with e1 = (1, 0) and e2 = (0, 1),
respectively:
C1 = projz1 C = {ˆv(z1, 0) : ˆv(z1, z2) ∈C}
and
C2 = projz2 C = {ˆv(0, z2) : ˆv(z1, z2) ∈C} .
C1 and C2 are 1D convolutional codes (see [15]). Moreover, if G(z1, z2) ∈
F[z1, z2]n×k is an encoder of C then
C1 = ImF[z1] G(z1, 0)
and
C2 = ImF[z2] G(0, z2)
Note that G(z1, 0) or G(0, z2) may not have full column rank and therefore
they may not be encoders of C1 and C2, respectively. Furthermore, the non-
catastrophicity of C does not imply the noncatastrophicity of C1 and C2 (see
[12]). However, if C is basic then C1 and C2 are basic. In fact, if G(z1, z2) is rZP
then there exists Y (z1, z2) ∈F[z1, z2]k×n such that
Y (z1, z2)G(z1, z2) = Ik ⇒Y (z1, 0)G(z1, 0) = Ik
i.e., G(z1, 0) is rP and then C1 is basic and G(z1, 0) is its encoder. Analogously,
we prove that C2 is basic and G(0, z2) is its encoder. Moreover, if C has rate k/n,
C1 and C2 also have rate k/n.

On Minimality of ISO Representation of Basic 2D Convolutional Codes
269
Furthermore, let Σ
= (A1, A2, B1, B2, C, D) be an ISO representation
of a 2D convolutional code C and consider the restriction of a trajectory
of {x(i, j), u(i, j), y(i, j)}(i,j)∈N2 of Σ to the semi-axis {ℓe1| ℓ
∈
N}, i.e.,
{x(i, 0), u(i, 0), y(i, 0)}i∈N. By the zero initial condition and the past ﬁnite sup-
port property of the input and state, we have that
x(i + 1, 0) = A1x(i, 0) + B1u(i, 0)
y(i, 0) = Cx(i, 0) + Du(i, 0)
with x(0, 0) = 0. This means that the 1D linear system Σ1 = (A1, B1, C, D)
generates the restrictions to the semi-axes {ℓe1| ℓ∈N}, of all trajectories of Σ,
i.e., Σ1 is an ISO representation of C1, and analogously, Σ2 = (A2, B2, C, D) is
an ISO representation of C2.
Theorem 4. Let Σ = (A1, A2, B1, B2, C, D) be a strongly modally reachable
ISO representation of a 2D convolutional code with dimension s. Then Σ is a
minimal ISO representation.
Proof. By the previous lemma, X(z1, z2) is ℓZP and therefore X(z1, 0) and
X(0, z2) are ℓP. In fact, if X(z1, z2) is ℓZP then there exists

X(z1, z2) ∈
F[z1, z2](s+n)×(s+n−k) such that X(z1, z2) 
X(z1, z2) = Is+n−k which implies that
X(z1, 0) 
X(z1, 0) = Is+n−k
and
X(0, z2) 
X(0, z2) = Is+n−k
which means that
X(z1, 0) =

Is −A1z1 −B1z1
0
−C
−D
In−k

and X(0, z2) =

Is −A2z2 −B2z2
0
−C
−D
In−k

are ℓP.
Then

Is −A1z B1z

and

Is −A2z B2z

are ℓP
and therefore, by
Deﬁnition 3, Σ1 = (A1, B1, C, D) and Σ2 = (A2, B2, C, D) are reachable. Thus,
by Lemma 5, Σi is a minimal ISO representation of Ci = C(Ai, Bi, C, D), for
i = 1, 2.
Now suppose that Σ is not a minimal ISO representation of C. Then there
exists Σ =

A1, A2, B1, B2, C, D

a minimal ISO representation of C with
dimension s < s. Then, for i = 1, 2, Σi =

Ai, Bi, C, D

is an ISO representa-
tion of Ci with smaller dimension than Σi, which contradicts the fact that Σi
is a minimal ISO representation of Ci. Then Σ is a minimal ISO representation
of C.
The next results follow immediately. It shows that if C is a basic 2D con-
volutional code with a strongly modally reachable ISO representation Σ =
(A1, A2, B1, B2, C, D), then the complexity of C is equal to the dimension of
a minimal ISO representation of C.

270
R. Pinto and R. Sim˜oes
Corollary 2. Let C be a basic 2D convolutional code of rate k/n with a strongly
modally reachable ISO representation Σ of dimension s. Then C has complexity s.
Moreover, the projections of C onto the semi-axes {ℓe1| ℓ∈N} and {ℓe2| ℓ∈N},
respectively have rate k/n and complexity s.
Proof. Let us assume that Σ = (A1, A2, B1, B2, C, D) is a strongly modally
reachable ISO representation of C with dimension s. By the above theorem
Σ is a minimal ISO representation of C and Σ1 = (A1, B1, C, D) and Σ2 =
(A2, B2, C, D) are also minimal ISO representations of C1 and C2, respectively,
with dimension δ. Then, by Lemma 5, C1 and C2 have complexity s.
Let G(z1, z2) be an encoder of C and L(z1, z2) a suitable polynomial matrix
such that
X(z1, z2)

L(z1, z2)
G(z1, z2)

= 0,
where X(z1, z2) is deﬁned in (4). Since the full size minors of X(z1, z2) have
degree smaller or equal than s, it follows from Lemma 4 that also the full size
minors of G(z1, z2) have degree less or equal than s. On the other hand, as C1 has
complexity s, G(z1, 0) is an encoder of C1 that has one full size minor of degree
s and therefore G(z1, z2) has one full size minor of degree greater or equal than
s. Conseguently, the greatest degree of the full size minors of G(z1, z2) is s and
therefore C has complexity s.
Corollary 3. Let Σ be a strongly modally reachable 2D linear system. Then Σ
is locally reachable.
Proof. It follows from Corollary 1.
4
Conclusion
In this paper we have investigated the minimality of ISO representations of basic
2D convolutional codes. We have showed that if a basic 2D convolutional code
admits a strongly modally reachable ISO representation then this ISO repre-
sentation is minimal with dimension equal to the complexity of the code. This
result is a natural generalization of the characterization of minimal ISO repre-
sentations of basic (or noncatastrophic) 1D convolutional codes. We believe that
all basic 2D convolutional codes admit a strongly modally reachable ISO repre-
sentation and that, as happens in the 1D case, all minimal ISO representations
of a basic 2D convolutional code are algebraically equivalent. We will investigate
this problem in the future. For that we will make use of the so-called ﬁrst order
representations of a code.
References
1. Alfandary, L., Raphaeli, D.: Ball codes - Two-dimensional tail-biting convolutional
codes. In: Proceedings 2010 IEEE Global Communications Conference (GLOBE-
COM 2010), Miami, FL (2010)

On Minimality of ISO Representation of Basic 2D Convolutional Codes
271
2. Benedetto, S., Divsalar, D., Montorsi, G., Pollara, F.: Serial concatenation of inter-
leaved codes: performance analysis, design, and iterative decoding. IEEE Trans.
Inf. Theory 44(3), 909–926 (1998)
3. Charoenlarpnopparut, C.: Applications of Grobner bases to the structural descrip-
tion and realization of multidimensional convolutional code. Sci. Asia 35, 95–105
(2009)
4. Climent, J.-J., Napp, D., Pinto, R., Sim˜oes, R.: Decoding of 2D convolutional codes
over the erasure channel. Adv. Math. Commun. 10(1), 179–193 (2016)
5. Climent, J.-J., Napp, D., Pinto, R., Sim˜oes, R.: Series concatenation of 2D convo-
lutional codes. In: Proceedings IEEE 9th International Workshop on Multidimen-
sional (nD) Systems (nDS). Vila Real, Portugal (2015)
6. Fornasini, E., Marchesini, G.: Structure and properties of two-dimensional systems.
In: Tzafestas, S.G. (ed.) Multidimensional Systems, Techniques and Applications.
Electrical and Computer Engineering, vol. 29, pp. 37–88 (1986)
7. Fornasini, E., Valcher, M.E.: Algebraic aspects of two-dimensional convolutional
codes. IEEE Trans. Inf. Theory 40(4), 1068–1082 (1994)
8. Gluesing-Luersen, H., Rosenthal, J., Weiner, P.A.: Duality between mutidimensinal
convolutional codes and systems. In: Colonius, F., Helmke, U., Wirth, F., Pratzel-
Wolters, D. (eds.) Advances in Mathematical Systems Theory, A Volume in Honor
of Diedrich Hinrichsen, pp. 135–150, Birkhauser, Boston (2000)
9. Justesen, J., Forchhammer, S.: Two Dimensional Information Theory and Coding:
With Applications to Graphics Data and High-Density Storage Media. Cambridge
University Press, Cambridge (2010)
10. Kailath, T.: Linear Systems. Prentice-Hall, Englewood Cliﬀs (1980)
11. L´evy, B.C.: 2d-polynomial and rational matrices and their applications for the
modelling of 2-d dynamical systems. Ph.D. dissertation, Department of Electrical
Engineering, Stanford University, Stanford, CA (1981)
12. Napp, D., Perea, C., Pinto, R.: Input-state-output representations and construc-
tions of ﬁnite support 2D convolutional codes. Adv. Math. Commun. 4(4), 533–545
(2010)
13. Rocha, P.: Structure and representation of 2-d systems. Ph.D. dissertation,
University of Groningen, Groningen, The Netherlands (1990)
14. Rosenthal, J., Schumacher, J.M., York, E.V.: On behaviors and convolutional
codes. IEEE Trans. Inf. Theory 42(6), 1881–1891 (1996)
15. Rosenthal, J., York, E.V.: BCH convolutional codes. IEEE Trans. Inf. Theory
45(6), 1833–1844 (1999)
16. Singh, J., Singh, M.L.: A new family of two-dimensional codes for optical CDMA
systems. Optik Int. J. Light Electr. Opt. 120(18), 959–962 (2009)
17. Valcher, M.E., Fornasini, E.: On 2D ﬁnite support convolutional codes: an algebraic
approach. Multidimension. Syst. Signal Process. 5, 231–243 (1994)
18. Weiner, P.A.: Multidimensional convolutional codes. Ph.D. dissertation, Depart-
ment of Mathematics, University of Notre Dame, Indiana, USA (1998)
19. Zhou, X.-L., Hu, Y.: Multilength two-dimensional codes for optical CDMA systems.
Optoelectron. Lett. 1(3), 232–234 (2005)

A New Construction of Minimum Distance
Robust Codes
Hila Rabii and Osnat Keren(B)
Faculty of Engineering, Bar-Ilan University, Ramat Gan, Israel
hila.rabii@live.biu.ac.il, osnat.keren@biu.ac.il
Abstract. Robust codes are codes that can detect any nonzero error
with nonzero probability. This property makes them useful in protect-
ing hardware systems from fault injection attacks which cause arbitrary
number of bit ﬂips. There are very few high rate robust codes, non of
them has minimum distance greater than two. Therefore, robust codes
with error correction capability are derived by concatenation of linear
codes with high rate robust codes. This paper presents a new construc-
tion of non-linear robust codes with error correction capability. The codes
are built upon linear codes; however, the redundant symbols that were
originally allocated to increase the minimum distance of the code, are
modiﬁed to provide both correction capability and robustness. Conse-
quently, the codes are more eﬀective and have higher rate than concate-
nated codes of the same error masking probability.
Keywords: Fault injection attacks · Security oriented codes · Robust ·
Nonlinear · Error correction
1
Introduction
The need to secure hardware systems and memories against side channel attacks
(SCA) has become more apparent as the attacker capabilities have strengthened.
One of the most powerful side-channel attacks is the fault injection attack [1]
in which an attacker injects a fault into the device to change its behavior and
manipulate its functioning. An eﬃcient and eﬀective approach to protect hard-
ware against this kind of attack is use of error detecting codes which can detect
errors of arbitrary multiplicity (Hamming weight) [2,3]. These codes are termed
‘security-oriented codes’.
Codes from classic coding theory are designed to address the problem of the
reliability of information transmitted over a noisy channel or stored in storage
media. In classic coding theory, the errors are assumed to be random and the
probability that the channel will introduce an error is relatively small. Thus, the
minimum distance of the code plays a crucial role in the code’s eﬀectiveness.
A diﬀerent class of problems addresses the security of information transmitted
This research was supported by the ISRAEL SCIENCE FOUNDATION (grant No.
923/16).
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 272–282, 2017.
DOI: 10.1007/978-3-319-66278-7 23

A New Construction of Minimum Distance Robust Codes
273
over a noisy channel or stored in storage media. In this class of problems it is
assumed that errors are injected by an attacker and therefore can be of any
multiplicity. The eﬀectiveness of security oriented codes is then measured in
terms of the ability to detect arbitrary errors with nonzero probability. A robust
code is a security oriented code that can detect any nonzero error.
Security oriented codes can have a deterministic encoding or random encod-
ing [4–7]. The eﬀectiveness of codes with random-encoding depends on the
entropy of the random portion. In practice, it is diﬃcult and expensive to imple-
ment in hardware a true (i.e., maximal entropy) random number generator nor
to protect it from fault injection attacks which could neutralized it. This fact
makes codes with deterministic encoding an attractive alternative. Indeed, in
some cases, when properly designed, codes with deterministic encoding can be
more eﬀective than random codes of the same rate [8]. In this paper we consider
robust codes with deterministic encoding.
As far as we know, there are two basic high rate binary systematic robust
codes, the Quadratic-Sum (QS) code [9] and the Punctured-Cubic (PC) code
[10,11]. All other systematic robust codes (e.g., the codes in [12]) employ them
as ground codes. The QS code is an optimum robust code for the case where
k = 2sr and q is any power of a prime. However, it does not have any correction
capabilities since its minimum distance equals one. The PC code is a close to
optimum robust code for any 1 < r ≤k where q is a power of two. As shown in
[11], also this code does not have correction capabilities.
Furthermore, there are some minimum distance partially robust codes, such
as the Vasil’ev code, the Phelps code, the One Switching Code and the gener-
alized cubic code [9,13]. These codes have 2 ≤d ≤5; however, they are not
robust.
To date, the only way to provide reliability and security is by concatenation
of codes. Namely, use of a linear code with correction capability and a nonlin-
ear code to obtain a robust concatenated code [14]. Overall, in existing coding
schemes rnl + rl redundancy symbols are required in order to provide both reli-
ability and security.
This paper presents a new construction of q-ary codes, which utilizes the rl
symbols which were originally allocated to provide reliability, to provide robust-
ness (i.e., security). The codes are q-ary codes robust code with q = 2m, m odd,
and minimum distance d.
The paper is organized as follows. In Sect. 2, basic deﬁnitions and theoretical
background are presented. Section 3 introduces the new construction and Sect. 4
concludes the paper.
2
Eﬀectiveness Criterion for Robust Codes
Notations: Double stroke capital letters are used to denote an algebraic struc-
ture; e.g., Fq is a ﬁnite ﬁeld with q elements. Calligraphic capital letters are used
to denote codebooks; e.g., C, where |C| is the number of codewords in C. The
operators ⊕and ⊖denote addition and subtraction in a ﬁnite ﬁeld, respectively.

274
H. Rabii and O. Keren
A code C of size |C| and length n is a subset of Fn
q . A systematic code is
a code of the form C = {c|c = (x, w(x)), x ∈Fk
q, w ∈Fr
q}, where x contains k
information symbols and w contains r redundant symbols, n = k+r. Systematic
codes are widely used in hardware systems since they have a simple, low cost
hardware implementation, and the information can be processed by the receiving
block while its correctness is being checked in parallel.
A fault in a system may manifests itself as an error at the output of the
circuit or at the output of a communication channel. It is convenient to model
the correct output as a codeword c ∈C and the injected fault as an additive
error e = (ex, ew) ∈Fn
q , where ex and ew are the errors in the information and
redundancy portions, respectively. The Hamming weight of the error vector is
termed error multiplicity. Random errors are characterized by small multiplicity
since the probability of a bit ﬂip is smaller than 0.5, whereas injected errors are
assumed to have an arbitrary multiplicity. An error vector e is detected by a
codeword c ∈C if c ⊕e /∈C. Similarly, an error e is undetected (masked) by a
codeword c ∈C if c ⊕e ∈C.
Deﬁnition 1. The masking probability of an error e, Q(e), is the probability
that an additive error e is not detected by the code, that is,
Q(e) =

c∈C
Pr(c)δC(c ⊕e),
(1)
where Pr(c) is the probability that the codeword c was used and δC is the char-
acteristic function of the code C,
δC(c) =

1 c ∈C
0 c /∈C.
The number of codewords that mask an error e is denoted by R(e) and equals
R(e) =

v∈Fn
q
δC(v)δC(v ⊕e).
Hence, if the codewords are uniformly distributed, Eq. 1 becomes
Q(e) = R(e)
R(0).
(2)
Note that R is called the autocorrelation function of the code.
Robust codes detect all nonzero errors with some nonzero probability.
Formally,
Deﬁnition 2 (Robust codes). [15] A code C is robust if Q(e) < 1 for any
nonzero error e.
The detection kernel of a code is denoted by Kd and consists of all the error
vectors that are never detected; that is, Kd = {e|Q(e) = 1}. The detection kernel

A New Construction of Minimum Distance Robust Codes
275
Kd is a linear subspace and its dimension is denoted by ωd. Robust codes have
|Kd| = 1, that is, Kd contains only the all-zero word. Partially robust codes
are codes whose detection kernel is of size 1 < |Kd| < |C|. If C is a linear code
Kd = C and hence linear codes are not robust and cannot be used for security.
For any error vector e ∈GF(2n), one of the following may occur:
1. The error vector e will always be detected. That is, there exists no c ∈C such
that c + e ∈C, and therefore, Q(e) = 0.
2. The error vector e will never be detected; i.e., the error vector will always be
masked. That is, c + e ∈C, for every c ∈C, and therefore, Q(e) = 1. This
group of errors is called the kernel of the code and is denoted by Kd.
3. The error vector e will be detected with nonzero probability 0 < Q(e) < 1.
That is, there exists at least one c ∈C that satisﬁes c + e ∈C; in addition,
there is at least one ˜c ∈C that does not solve it, this ˜c allows to detect the
presence of the error.
These three possible scenarios are depicted in Fig. 1.
Fig. 1. The error e1 is always detected, the error e2 is never detected and the error e3
is detected with some probability.
Denote by wt(e) the Hamming weight of e, then,
Property 1. [16] A code is of a minimum distance d if and only if Q(e) = 0 for
all the nonzero error vectors e having 0 < wt(e) < d.
Deﬁnition 3 (Error masking probability of a code). The error masking
probability of the code C is deﬁned as
Q = max
e̸=Kd Q(e).
In the binary case, for given k and r the error masking probability Q is lower
bounded by Q ≥max{2−r, 2−k+1} [16]. A code that satisﬁes equality in the
lower bound on Q is called optimum.
The following theorem sets a lower bound on the error masking probability
of a systematic code of minimum distance d.

276
H. Rabii and O. Keren
Property 2. Consider a systematic code C with minimum distance d over Fq,
q = pm and p is a prime number. The error masking probability of the code is
lower bounded by
Q ≥max

qk −qωd
qn −qr −qωd + d
r
d

(q −1)d −1, q−k

.
(3)
Proof. The maximal correlation value is greater than or equal to the average
one. Thus, the error masking probability of the code is lower bounded by
Q = maxe/∈Kd R(e)
R(0)
≥Ravr
R(0)
where the average autocorrelation Ravr is computed over all the error vectors of
interest.
The sum of the autocorrelation values of a code at positions that correspond
to error vectors that are not in the detection kernel of the code equals

e/∈Kd
R(e) =

e/∈Kd

v∈Fn
q
δC(v)δC(v ⊕e) =

v∈Fn
q
δC(v)

e/∈Kd
δC(v ⊕e) = qk(qk −qωd).
(4)
Thus, for a general code we have
max
e/∈Kd(R(e)) ≥qk(qk −qωd)
qn −qωd
.
However, in a systematic code, all the qr −1 nonzero errors of the form ex = 0
and ew ̸= 0 are always detected. Moreover, the code has a minimum distance
d and hence nonzero errors with Hamming weight smaller than d are always
detected; There are d−1
j=1
n
j

(q −1)j errors of this type. Therefore, at most
qn −(qωd + qr + d−1
j=0(
n
j

−
r
j

)(q −1)j) error vectors that are neither always
detected nor always masked contribute to the sum in Eq. 4. Consequently,
max
e/∈Kd(R(e)) ≥
qk(qk −qωd)
qn −(qωd + qr + d−1
j=0
n
j

−
r
j
	
(q −1)j)
,
and hence,
Q ≥
(qk −qωd)
qn −qωd −qr + d
r
d

(q −1)d −1.
In Addition, since for there exist an error, say e = c1 ⊖c2, c1, c2 ∈C, that is
masked by at least one codeword (c2) then Q ≥1/qk. Note that if q = 2m then
e is masked by at least two codewords c1 and c2, hence, Q ≥2/qk.
In robust codes, wd = 0, hence asymptotically, Q ≥q−r
1
1−ϵ where ϵ →0 for
k/n →1.

A New Construction of Minimum Distance Robust Codes
277
3
Construction of Robust Codes with Distance
In this section we present a construction of a robust code with distance. In order
to simplify the description and the analysis, the cubic function (f(x) = x3) is
employed. Nevertheless, any perfect nonlinear function can be used as well.
Construction 1. Let m be an odd integer and q = 2m. Let G = (I|A) be a
generator matrix of a systematic linear q-ary code C with minimum distance dL
where A = {aij}k,r
i,j=1, aij ∈F2m. Let x = (x1, x2, . . . , xk) where xi ∈F2m for
1 ≤i ≤k. Code ˜C is deﬁned as follows,
˜C = {(x, w) : x ∈Fk
2m, w ∈Fr
2m}
where w = (w1, w2, . . . , wr), wj ∈F2m for 1 ≤j ≤r, and
wj =
k

i=1
aijx3
i
Property 3. Let m be an odd integer and q = 2m, then the minimum distance
of code ˜C is dL.
The correctness of this property follows from the fact that C and ˜C are equiv-
alent codes. Namely, since for odd values of m the cubic function is invertible, it
is possible to obtain one code from the other by a permutation of the alphabet
in each column.
Theorem 1. Let m be an odd integer and q = 2m. The code ˜C is a robust code
with Q ≤( 2
q). All the nonzero errors of Hamming weight (wt) less than dL are
always detected.
Errors e = (ex, ew) with wt(ex) ≥dL are masked with probability Q(e) ≤
( 2
q)dL−1, and errors having 0 < wt(ex) ≤dL −1 are masked with probability
Q(e) ≤( 2
q)wt(ex). There are at most

k
wt(ex)

(q −1)wt(ex) min
q
2
	wt(ex)
, qr

errors e = (ex, ew), wt(ex) > 0, that are being masked with a probability Q(e) >
0; the remaining errors of this form are always detected.
Proof. Let c = (x, w) be a codeword and let e = (ex, ew) be an error vector,
where ex = (ex1, . . . exk) ∈Fk
2m and ew = (ew1, . . . ewr) ∈Fr
2m. Any nonzero
error of Hamming weight smaller than dL is detected because of the minimum
distance of the code. Additionally, since the code is systematic, an error of the
form e = (0, ew) is always detected.

278
H. Rabii and O. Keren
An error e = (ex ̸= 0, ew) is masked by a codeword c iﬀ(x ⊕ex, w ⊕ew) ∈C.
The r error masking equations of the code are then
k

i=1
aij(xi ⊕exi)3 = ewj ⊕
k

i=1
aijx3
i ,
1 ≤j ≤r.
(5)
In other words, the error is masked by the j’th redundancy check, 1 ≤j ≤r, if
k

i=1
aij(exix2
i ⊕e2
xixi ⊕e3
xi) = ewj.
(6)
This set of error masking equations can be represented in a matrix form as:
AT z = ew
(7)
where
z = (z1, z2, . . . , zk)T ∈Fk
2m,
and
zi = (exix2
i ⊕e2
xixi ⊕e3
xi) ∈F2m,
1 ≤i ≤k.
This quadratic equation in xi over Fq has two distinct solutions if the trace of
yi =
zi
e3
xi + 1 equals zero, that is
Tr(yi) =
m−1

j=0
y2j
i
= 0
Otherwise, it has no solutions in F2m.
Recall that the check matrix of the code is of the form H = (−AT |I). Since
the code has minimum distance dL, any subset of dL −1 columns of AT are
linearly independent.
Denote by S the support of ex, and denote by α the weight of the error in
the information part, i.e., α = wt(ex) = |S|. There are two cases:
– CASE I: 0 < α ≤dL −1:
In this case, also the size of support of z equals α. Any set of dL −1 columns
of AT , and in particular the ones deﬁned by the support of z, are linearly
independent. Hence, depending on the value of ew, Eq. 7 is either satisﬁed by
a single vector z of the required form, or, it has no such solution. (Although
Eq. 7 has qk−r solutions, not all of them have zi = 0 for all i /∈S).
Consequently, if for a given ex, ew there exists a z which satisﬁes Eq. 7, has
zi = 0 for i /∈S, and has Tr(yi) = 0 for i ∈S, the error e is masked by
codewords in C. Otherwise, it is always detected.
There are at most R(e) codewords that mask e, where
R(e) =
1

single solution
to Eq. 7
·
2α

xi, i ∈S
takes two values
·
qk−α
  
xi, i /∈S
takes q values
.

A New Construction of Minimum Distance Robust Codes
279
Hence, the error masking probability of e is upper bounded by
Q(e) ≤

2
q
α
.
The number of error vectors that are masked with this probability is at most

k
α

(q −1)α q
2
	α
.
because ex can take
k
α

(q −1)α distinct values, and because that for a given
ex there are only α (out of r) degrees of freedom in choosing the ew vector.
However, if one (or more) zi for i ∈S will not satisfy the trace restriction
(Tr(yi) = 0), the error will be always detected. Therefore, ew can take
 q
2
α·1
values.
– CASE II: α ≥dL:
Since there are at least dL −1 elements in the support of ex, at least dL −1
equations out of the r equations in zi are linearly independent. Therefore,
there are at most qα−(dL−1) solutions Fk
2m to Eq. 7 for which zi = 0 for all
i /∈S. Let Z be this set of solutions to Eq. 7. Z is a coset in Fk
2m, hence, for
each i, 1 ≤i ≤k, either zi = 0 in all z ∈Z, or zi takes each value in F2m
exactly |Z|/q times.
Recall that zi, i ∈S, deﬁnes a quadratic equation in xi over Fq and if Tr(yi) =
0 this equation has two solution xi ∈F2m. Since the trace of half of the
elements in F2m equals zero, and the trace of the other half equals one. Thus,
only half of the quadratic equations associated with zi, i ∈S have solutions.
Namely, at most
 q
2
α−(dL−1) vectors in Fk
2m are associated with x’s that
mask the error e. Consequently,
R(e) ≤
q
2
	α−(dL−1)
· 2α · qk−α,
and the error masking probability of e is upper bounded by
Q(e) ≤

2
q
dL−1
.
Using the same arguments as before, the number of error vectors that are
masked with this probability is at most

k
α

(q −1)α min
q
2
	α
, qr
.
In conclusion, the code is robust and its error masking probability is Q ≤( 2
q).
Note that asymptotically the percentage of error vectors having this masking
probability is relatively small.
In the following examples a linear shortened Reed-Solomon code and a robust
code derived from it are introduced and the autocorrelation functions of these
two codes are calculated.

280
H. Rabii and O. Keren
Example 1. Let C be a linear shortened Reed-Solomon code [3, 3, 5]q where q =
23 with the following generator matrix:
G =
⎡
⎣
1 0 0 1 1
0 1 0 4 2
0 0 1 3 2
⎤
⎦.
The autocorrelation function of code C is shown in Fig. 2. The x-axis is the
decimal value of the error vector e when referred to as a number in base two; the
y-axis is the autocorrelation value R(e). The number of nonzero error vectors
is qn −1 = (23)5 −1 = 32767. Out of them, 511 errors are never detected,
R(e) = 512; all other nonzero errors are always detected, R(e) = 0. Note that
since the code is a linear code, all the 83 −1 undetected errors are codewords.
Clearly, this code is not a robust code and cannot provide security.
Fig. 2. Autocorrelation values for linear code C. 512 errors have R(e) = 512 and all
other errors have R(e) = 0.
Example 2. Let ˜C be the code constructed according to Construction 1 with the
generator matrix from the previous example. The autocorrelation function of
code ˜C is shown in Fig. 3. Out of 32768 errors, 84 errors have R(e) = 128 ,the all-
zero vector has R(e) = 512 and all the other errors have smaller autocorrelation
values. Note that since ˜C inherits the minimum distance of the linear code C, all
the error vectors of Hamming weight less or equal to three are always detected;
i.e., R(e) = 0.
The code ˜C is robust, its error masking probability is Q = 128
512 = 2−2 = 0.25.
As expected, the number of errors with error masking probability of
2
q is 84
errors which is smaller than the upper bound
5
1

(8 −1)1(8/2)1 = 140 errors.
It is interesting to note that these codes are more eﬀective and have higher
rate than concatenated codes of the same error masking probability and the
same distance; Let CL[nL, kL, dL] be a linear q-ary code. Let Cconc be the code
constructed from CL and a q-ary robust code Cnl(nnl = nL + 1, knl = nL) by

A New Construction of Minimum Distance Robust Codes
281
Fig. 3. Autocorrelation values for code ˜C. 10752 errors have R(e) = 8, 5376 errors have
R(e) = 16, 2464 errors have R(e) = 32, 84 errors have R(e) = 128 and the all-zero
error vector has R(e) = 512.
concatenation, and let ˜C be the code constructed according to Construction 1.
Cconc is a code of dimension kL, length nL+1, distance dL and ¯Q = 2/q. Whereas
˜C is a code with the same dimension, distance and ¯Q, but of length nnl. Moreover,
let e = (ex, ew) be an error of Type III, i.e., an error which is detected by some
of the codewords and is masked by the others. The concatenated code Cconc
masks all the errors of Type III with probability 2/q, whereas ˜C masks such an
error with probability (2/q)wt(ex). That is, on average, ˜C provides a smaller error
masking probability than the concatenated code.
4
Summary
Security oriented codes are designed to cope with fault injection attacks. This
paper presents a new class of q-ary security oriented codes with minimum dis-
tance d for q = 2m, m odd. The codes provide reliability for d ≥3, and provide
security as they can detect any arbitrary error with probability greater than
1 −(2/q). The proposed construction can be generalized to other q-ary alpha-
bets by use of nonlinear functions other than the cubic function.
References
1. Biham, E., Shamir, A.: Diﬀerential fault analysis of secret key cryptosystems.
In: Kaliski, B.S. (ed.) CRYPTO 1997. LNCS, vol. 1294, pp. 513–525. Springer,
Heidelberg (1997). doi:10.1007/BFb0052259
2. Tomashevich, V., Neumeier, Y., Kumar, R., Keren, O., Polian, I.: Protecting cryp-
tographic hardware against malicious attacks by nonlinear robust codes. In: 2014
IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nan-
otechnology Systems (DFT), pp. 40–45. IEEE (2014)

282
H. Rabii and O. Keren
3. Verbauwhede, I.M. (ed.): Secure Integrated Circuits and Systems. Springer,
New York (2010)
4. Cramer, R., Dodis, Y., Fehr, S., Padr´o, C., Wichs, D.: Detection of algebraic manip-
ulation with applications to robust secret sharing and fuzzy extractors. In: Smart,
N. (ed.) EUROCRYPT 2008. LNCS, vol. 4965, pp. 471–488. Springer, Heidelberg
(2008). doi:10.1007/978-3-540-78967-3 27
5. Wang, Z., Karpovsky, M.: Algebraic manipulation detection codes and their appli-
cations for design of secure cryptographic devices. In: 2011 IEEE 17th International
on On-Line Testing Symposium (IOLTS), pp. 234–239. IEEE (2011)
6. Ngo, X.T., Bhasin, S., Danger, J., Guilley, S., Najm, Z.: Linear complementary
dual code improvement to strengthen encoded circuit against hardware Trojan
Horses. In: IEEE International Symposium on Hardware Oriented Security and
Trust, HOST 2015, Washington, DC, USA, 5–7 May 2015, pp. 82–87 (2015)
7. Dziembowski, S., Pietrzak, K., Wichs, D.: Non-malleable codes. Cryptology ePrint
Archive, Report 2009/608 (2009). http://eprint.iacr.org/2009/608
8. Keren, O., Karpovsky, M.: Relations between the entropy of a source and the error
masking probability for security-oriented codes. IEEE Trans. Commun. 63(1), 206–
214 (2015)
9. Karpovsky, M.G., Kulikowski, K.J., Wang, Z.: Robust error detection in com-
munication and computational channels. In: International Workshop on Spectral
Methods and Multirate Signal Processing, SMMSP 2007, Citeseer (2007)
10. Admaty, N., Litsyn, S., Keren, O.: Puncturing, expurgating and expanding the
q-ary BCH based robust codes. In: 2012 IEEE 27th Convention of Electrical Elec-
tronics Engineers in Israel (IEEEI), pp. 1–5, November 2012
11. Neumeier, Y., Keren, O.: Robust generalized punctured cubic codes. IEEE Trans.
Inf. Theory 60(5), 2813–2822 (2014)
12. Rabii, H., Neumeier, Y., Keren, O.: Low complexity high rate robust codes derived
from the quadratic-sum code. In: 12th International Workshop on Boolean Prob-
lems (2016)
13. Engelberg, S., Keren, O.: A comment on the karpovsky-taubin code. IEEE Trans.
Inf. Theory 57(12), 8007–8010 (2011)
14. Neumeier, Y., Keren, O.: A new eﬃciency criterion for security oriented error
correcting codes. In: 2014 19th IEEE European Test Symposium (ETS), pp. 1–6.
IEEE (2014)
15. Wang, Z., Karpovsky, M.G., Kulikowski, K.J.: Replacing linear hamming codes
by robust nonlinear codes results in a reliability improvement of memories. In:
IEEE/IFIP International Conference on Dependable Systems & Networks, DSN
2009, pp. 514–523. IEEE (2009)
16. Wang, Z., Karpovsky, M., Kulikowski, K.J.: Design of memories with concurrent
error detection and correction by nonlinear SEC-DED codes. J. Electron. Test.
26(5), 559–580 (2010)

Constructions and Bounds for Batch Codes
with Small Parameters
Eldho K. Thomas(B) and Vitaly Skachek(B)
Institute of Computer Science, University of Tartu, Tartu, Estonia
{eldho.thomas,vitaly.skachek}@ut.ee
Abstract. Linear batch codes and codes for private information
retrieval (PIR) with a query size t and a restricted size r of the recon-
struction sets are studied. New bounds on the parameters of such codes
are derived for small values of t or r by providing corresponding con-
structions. By building on the ideas of Cadambe and Mazumdar, a new
bound in a recursive form is derived for batch codes and PIR codes.
Keywords: PIR codes · Batch codes · Private information retrieval ·
Locally repairable codes · Distributed data storage
1
Introduction
Batch codes are proposed in [10] for load balancing in the distributed server
systems. They can be broadly classiﬁed as linear batch codes and combinatorial
batch codes. A particular version of the former is known as switch codes and
were mainly studied in [5,18,19] in the context of network switches. Some works
on linear batch codes and on combinatorial batch codes can be found in [6,11,21]
and in [1–3], respectively.
Locally repairable codes (LRC codes), or codes with locality, which are deeply
studied in [4,8,9,12,13], share lots of similarities with batch codes, and therefore
many of the properties of these two code families are expected to be related
to each other. In [21], new upper bounds on the parameters of batch codes
based on the classical Singleton bound which do not depend on the size of the
underlying alphabet are derived. Batch codes turn out to be a special case of
private information retrieval (PIR) codes [7]. Indeed, PIR codes support only
queries of type (xi, xi, . . . , xi), 1 ≤i ≤k, whereas batch codes support queries of
a more general form (xi1, xi2, . . . , xit), possibly for diﬀerent indices i1, i2, . . . , it.
It follows that batch codes can be used as PIR codes. In [17], batch codes with
unrestricted size of reconstruction sets are considered and some bounds on the
optimal length of batch and PIR codes for a given batch size and dimension are
proposed.
In this work, we construct new families of batch codes with restricted size r
of reconstruction sets. We also generalize the existing bounds on the dimension
of LRC codes proposed in [12] to batch codes using the connections between
the two families. This paper is organized as follows. In Sect. 3, we propose an
c
⃝Springer International Publishing AG 2017
´A.I. Barbero et al. (Eds.): ICMCTA 2017, LNCS 10495, pp. 283–295, 2017.
DOI: 10.1007/978-3-319-66278-7 24

284
E.K. Thomas and V. Skachek
optimal construction of batch codes with t = 2, r ≥2, and a construction with
t ≥3, r = 2. In Sect. 4.1, we present constructions of PIR codes with arbitrary
t and r for r|k. In Sect. 5, we derive a new upper bound on the dimension k of
batch codes.
2
Notations and Related Works
We start with introducing some notations. Denote by N the set of nonnegative
integers. For n ∈N we denote [n] = {1, 2, . . . , n}. A k × k identity matrix will
be denoted by Ik, all-one column vector by 1. We use 0 to denote an all-zero
column vector and a zero matrix. The right dimensions will be clear from the
context. Let x be a vector of length n indiced by [n]. Take S ⊆[n]. Then xS
stands for a sub-vector of x indiced by S. If A is a matrix, then A[i] denotes the
i-th column in A.
Let
Q
be
a
ﬁnite
alphabet.
Consider
an
information
vector x
=
(x1, x2, . . . , xk) ∈Qk. The code is a set of vectors {y = C(x) | x ∈Qk} ⊆Qn,
where C : Qk →Qn is a bijective mapping, and n ∈N. By slightly abusing the
notation, C will also be used to denote the above code.
In this work, we study (primitive, multiset) batch codes with restricted size
of the recovery sets, as they are deﬁned in [21] (see also [17]).
Deﬁnition 1. An (n, k, r, t) batch code C over a ﬁnite alphabet Q is deﬁned by
an encoding mapping C : Qk →Qn, and a decoding mapping D : Qn × [k]t →
Qt, such that
1. For any x ∈Qk and a multiset (i1, i2, · · · , it) ⊆[k]t,
D (y = C(x), i1, i2, · · · , it) = (xi1, xi2, · · · , xit) .
2. The symbols in the query (xi1, xi2, · · · , xit) can be reconstructed from t respec-
tive disjoint recovery sets of symbols of y of size at most r each (the symbol
xiℓis reconstructed from the ℓ-th recovery set for each ℓ, 1 ≤ℓ≤t).
If the alphabet Q is a ﬁnite ﬁeld, and the associated encoding mapping C
:
Qk →Qn is linear over Q, the corresponding code is termed linear. In that
case, for each ﬁxed (i1, i2, · · · , it) ∈[k]t, the corresponding decoding mapping
D from Qn to Qt is linear over Q too. Additionally, if the encoding mapping
C : x →y is such that x is a sub-vector of y, then the corresponding code is
called systematic.
This setup has ﬁrst appeared in [11]. It was shown therein that the minimum
distance d of a batch code satisﬁes d ≥t. It is worth mentioning that batch codes
are closely related to locally repairable codes, which have been extensively stud-
ied in the context of the distributed data storage. The main diﬀerence between
them is that in batch codes we are interested in the reconstruction of information
symbols in x, while in locally repairable codes we are interested in the recovery
of coded symbols in y.
The following property of batch codes is stated as Corollary III.2 in [21].

Constructions and Bounds for Batch Codes with Small Parameters
285
Lemma 1. Let C be a linear (n, k, r, t) batch code over Q, and x ∈Qk,
whose encoding is y ∈C. Let R1, R2, · · · , Rt ⊆[n] be t disjoint recovery
sets for the coordinate xi. Then, there exist indices a2 ∈R2, a3 ∈R3, · · · ,
at ∈Rt, such that if we ﬁx the values of all coordinates of y indexed by the
sets R1, R2\{a2}, R3\{a3}, · · · , Rt\{at}, then the values of the coordinates of y
indexed by {a2, a3, · · · , at} are uniquely determined.
There is a number of bounds on the parameters of batch codes in the liter-
ature, but it is often diﬃcult to make a comparison due to slight variations in
the models and assumptions made. Thus, it is proven in [21] that for a linear
(n, k, r, t) batch code over Q,
n ≥k + d + max
1≤β≤t

(β −1)

k
rβ −β + 1

−1

−1 .
(1)
In particular, when the code is systematic, the bound can be tighten a bit, as
follows:
n ≥k + d + max
2≤β≤t

(β −1)

k
rβ −β −r + 2

−1

−1 .
(2)
If the queries in Deﬁnition 1 are restricted to i1 = i2 = · · · = it, then the
corresponding code is called an (n, k, r, t) code for private information retrieval
(PIR) [7], or simply (n, k, r, t) PIR code. In particular, all batch codes are PIR
codes with the corresponding parameters. It should be mentioned that the proofs
of (1) and (2) in [21] work in analogous way for the PIR codes too, and there-
fore these two bounds hold for general and systematic (n, k, r, t) PIR codes,
respectively.
Systematic linear (n, k, r, t) PIR codes can be viewed as LRC codes with
locality of information symbols and availability [13]. A number of bounds on the
parameters of the latest family were derived in [13], and in subsequent works.
Speciﬁcally, when re-written for systematic batch code setting, the following
bound holds:
d + k +
(t −1)(k −1) + 1
(t −1)(r −1) + 1

−2 ≤n.
(3)
A comparison between (3) and (2) is not always straightforward, in particular
due to the minimization term in (2). However, with the aid of a computer, we
veriﬁed that the bound (3) gives equal or slightly higher values of n compared
to (2) for small values of d, k, r and t. Hereafter, we employ the bound (3) in
the analysis of the optimal values of n. However, this bound is not always tight,
especially for small alphabets.
Binary simplex codes of length n = 2m −1 are shown to be optimal batch
codes with parameters k = m, t = 2m−1 −2 and r = 2 (for any m ∈N) [18], yet
those codes exist only for very speciﬁc parameters.
The LRC codes were extensively studied in the last years. Thus, in [16], a
lower bound on the length n is presented for an LRC with locality of all symbols
and availability. It should be noted that codes with locality (and availability) of

286
E.K. Thomas and V. Skachek
all symbols are a special case of codes with locality (and availability) of infor-
mation symbols, and therefore the bounds derived for the former family are not
directly applicable to the latter family.
It is shown in [16] that it is possible to construct a t-fold power of the binary
(r + 1, r) single parity check code in order to obtain an LRC with availability
(termed direct product code) for speciﬁc parameters. An algebraic construction
of binary LRC codes in [20] further improves on the rate of the direct prod-
uct code. However, in general, the resulting construction is non-systematic, and
thus it is not straightforward how to derive an analogous result for batch/PIR
codes, see [15, Example 5]. It would be interesting to extend those techniques to
batch/PIR codes, but that is left out of the scope of this paper.
In Sect. 4.1, we present constructions of binary PIR codes for arbitrary r
and t achieving rate
r
r+t−1 for k ≥r2 similar to their counterparts in [20]. For
t ∈{2, 3, 4} these codes are batch codes. The achieved rate is close to optimal,
especially for small values of t and r.
A special case of (n, k, r, t) batch and PIR codes, where the size of the recovery
sets r is not restricted (for example, it can be assumed that r = n), is studied
in [17]. Let B(k, t) be the shortest length n of any systematic linear batch code
with unrestricted size of the recovery set, and P(k, t) be the shortest length n
of any linear systematic PIR code with unrestricted size of the recovery set.
Then the optimal redundancy of batch and PIR codes, respectively, is deﬁned
as γB(k, t) = B(k, t) −k and γP(k, t) = P(k, t) −k.
Proposition 1. [17] It holds B(k, t) = P(k, t) for 1 ≤t ≤4, and γB(k, t) ≤
γP(k, t) + 2⌈log(k)⌉· γP(k/2, t −2) for 5 ≤t ≤7.
Hereafter, we denote the optimal length of a linear systematic batch code and
PIR code with the size of the reconstruction sets r as B(k, r, t) and P(k, r, t),
respectively.
3
Batch Codes with r = 2 or t = 2
3.1
Optimal Batch Codes with r ≥2 and t = 2
In this and subsequent sections, we construct (n, k, r, t) batch codes for speciﬁc
values of t and r. To this end, consider an (n, k, r, t) systematic batch code with
r ≥2 and t = 2. Then, by using d ≥t, from the bound in (3), we have
n ≥
k
r

+ k .
(4)
The construction of codes attaining this bound, for t = 2 and r = 2, is presented
in [21, Example 2]. In the sequel, we generalize that construction to other values
of t and r.
First, we show that the bound (4) is optimal for t = 2 and any r ≥2. We
achieve that by constructing corresponding (n, k, r, t) batch codes.

Constructions and Bounds for Batch Codes with Small Parameters
287
Take G to be a k × n binary systematic generator matrix of a code C deﬁned
as follows:
G =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
Ir 0 · · · 0 0 1 0 · · · 0 0
0 Ir · · · 0 0 0 1 · · · 0 0
...
... ... ...
...
...
... ... ...
...
0 0 · · · Ir 0 0 0 · · · 1 0
0 0 · · · 0 Is 0 0 · · · 0 1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
(5)
where s = k mod r, and recall that 1 denotes all-one column vector.
It is easy to see that C supports any query of size t = 2.
If r|k, then n = k
r (r+1), which satisﬁes the lower bound (4) with equality. When
r ∤k, we have s = k −
 k
r

r, and
n =
k
r

(r + 1) + s + 1 =
k
r

+ k ,
which also satisﬁes (4) with equality.
Since B(k, r, t) ≥P(k, r, t), we can summarize the result as in the following
proposition.
Proposition 2. For any k, t = 2 and r ≥2,
B(k, r, t) = P(k, r, t) =
k
r

+ k .
Corollary 1. For any k ≥1, t = 2 and r ≥2, the optimal length of a non-
systematic batch and PIR code, Bn(k, r, t) and Pn(k, r, t), respectively, satisﬁes
k
r

+ k ≥Bn(k, r, t) ≥Pn(k, r, t) ≥

k
2r −1

+ k .
The right-most inequality is obtained from the bound (1) by substituting d ≥
t = 2 and β = 2.
3.2
Batch Codes and PIR Codes with t ≥2 and r = 2
In this section, we propose a construction of (systematic) binary PIR codes such
that
2 ≤t ≤max
k
r

, r

+ 2
(6)
and r = 2. We achieve this by using a generator matrix with columns of weight
at most 2. The constructed codes are batch codes for t = 2, 3, 4. In particular,
for t = 2 and r = 2, the construction is identical to the one in the previous
section.
Let G be a binary generator matrix deﬁned as G = [ Ik | A ]. In the sequel
we describe how to construct the sub-matrix A.

288
E.K. Thomas and V. Skachek
When k is even or t −1 is even, the sub-matrix A has all its columns of
weight 2 and all its rows of weight t −1, such that there is no 1-square pattern.
In other words, for any i1, i2, j1, j2, i1 ̸= i2, j1 ̸= j2, at least one of the entries
Ai1,j1, Ai1,j2, Ai2,j1 and Ai2,j2 in A is zero.
The total number of columns in G is n = k+(t−1) k
2. In particular, for r = 2
and t = 2, this bound coincides with (4). We remark that the above construction
requires some modiﬁcation when k = 2 and 3. This is due to the fact that it is
impossible to avoid 1-squares if the number of rows in G is two or three (for
k = 2, t = 3, 4, the corresponding values of n are 5 and 7, respectively, and
for k = 3, t = 3, 4, the values of n are 6 and 9, respectively). Moreover, the
right-hand inequality in (6) is required in order to ﬁt t −2 ones per row, while
avoiding 1-squares.
When k is odd and t −1 is odd, then A has all (except the last) columns of
weight 2, and the last column of weight 1. All its rows are of weight t −1, and
there is no 1-square pattern in A. Therefore the total number of columns in G is
n = k +

(t −1) · k
2

.
Proposition 3. The code C deﬁned by the above generator matrix G for t = 3
supports any query of the form (xi, xj, xℓ) with recovery sets of size at most 2,
i, j, ℓ∈[k].
Proposition 4. The code C deﬁned by the above generator matrix G for t = 4
supports any query of the form (xi, xj, xℓ, xh) with recovery sets of size at most
2, i, j, ℓ, h ∈[k].
Proposition 5. The code C deﬁned by the above generator matrix G for general
t ≥5 supports any query of size t of the form (xi, xi, · · · , xi) with recovery sets
of size at most 2.
Proposition 6. For r = 2 and 3 ≤t ≤max{
 k
r

, r} + 2,
k + t −2 +
t −1)(k −1) + 1
t

≤P(k, r, t) ≤k +

(t −1) · k
2

.
The left-most inequality in Proposition 6 is obtained from (3) by substituting
r = 2.
Proposition 7. For r = 2 and t ∈{3, 4},
k + t −2 +
t −1)(k −1) + 1
t

≤B(k, r, t) ≤k +

(t −1) · k
2

.
The following examples illustrate the above constructions.

Constructions and Bounds for Batch Codes with Small Parameters
289
Example 1. Consider a binary (n, k, r, t) batch (PIR) code C with k = 5, t = 3
and r = 2. From Proposition 7, we have 9 ≤B(5, 2, 3) ≤10. We construct a
batch (PIR) code C of length 10 with the above parameters using the following
5 × 10 generator matrix:
G =
⎛
⎜
⎜
⎜
⎜
⎝
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1

1 0 0 1 0
0 1 0 0 1
0 0 1 1 0
0 0 1 0 1
1 1 0 0 0
⎞
⎟
⎟
⎟
⎟
⎠
.
Example 2. Take a binary (n, k, r, t) batch (PIR) code C with k = 5, t = 4 and
r = 2. From Proposition 7, we have 11 ≤B(5, 2, 4) ≤13. We construct a batch
(PIR) code C of length 13 with the above parameters using the following 5 × 13
generator matrix:
G =
⎛
⎜
⎜
⎜
⎜
⎝
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1

1 0 0 1 0 1 0 0
0 1 0 0 1 0 1 0
0 0 1 1 0 0 1 0
0 0 1 0 1 1 0 0
1 1 0 0 0 0 0 1
⎞
⎟
⎟
⎟
⎟
⎠
.
4
PIR Codes for Arbitrary t > 2 and r > 2
4.1
Case r|k
In this section, by using a generator matrix with columns of weight at most r,
we generalize the construction in Sect. 3 to systematic PIR codes with arbitrary
parameters 2 < t ≤max{ k
r , r} + 2 and r > 2, r | k. The corresponding upper
bounds are implied by the construction.
Let G be a generator matrix of the form G = [Ik | A | B], where Ik is the
systematic part. Here, A is a k × k
r matrix with the jth column of the form
A[j] = (a1,j, a2,j, · · · , ak,j)T , 1 ≤j ≤k
r , where ai,j = 1 if (j −1)r + 1 ≤i ≤jr,
and ai,j = 0 otherwise. The matrix B is deﬁned as follows. Each column has
weight min{r, k/r}, every row in B has weight t −2, and there are no 1-squares
in [ A | B ]. That is, B is constructed in such a way that all rows of G have
weight t, columns have weight at most r, and there are no 1-squares in [ A | B ].
The absence of 1-squares is instrumental in ﬁnding disjoint recovery sets for all
information symbols.
– Case 1: k
r < r. In that case we choose B to be a k × (t −2)r matrix deﬁned
as per the rules above. Then,
n = k + k
r + (t −2)r = (r + 1)k
r + (t −2)r ,
and the code rate is
k
n =
k
(r + 1) k
r + (t −2)r <
k
(r + 1) k
r + (t −2) k
r
=
r
r + t −1 ,

290
E.K. Thomas and V. Skachek
where the inequality is due to k
r < r.
– Case 2: k
r ≥r. In this case, we can choose B as a k × (t −2) k
r matrix as
deﬁned above. To this end,
n = (r + 1)k
r + (t −2)k
r ,
and the rate of the code is
k
n =
k
(r + 1) k
r + (t −2) k
r
=
r
r + t −1 .
By a suitable choice of k, it is always possible to construct a PIR code (or
batch code if t ∈{2, 3, 4}) achieving the rate
r
r+t−1, which is close to the
optimal rate given in [16]. Note that the condition t ≤max{ k
r , r} + 2 is
necessary to make sure that no 1-squares are generated.
Proposition 8. For r > 2 and 2 < t ≤ζ + 2 with r|k,
P(k, r, t) ≤(r + 1)k
r + (t −2)ζ
where ζ = max{ k
r , r}.
Proposition 9. Let t = 3. The code C deﬁned by the generator matrix G sup-
ports any query of the form (xi, xj, xℓ) with recovery sets of size at most r,
i, j, ℓ∈[k]. Therefore it is a batch code.
Proposition 10. Let t = 4. The code C deﬁned by the generator matrix G
supports any query of the form (xi, xj, xℓ, xh) with recovery sets of size at most
r, for i, j, ℓ, h ∈[k]. Therefore it is a batch code.
Proposition 11. Let t ≥5. The code C deﬁned by the above generator matrix
G supports any query of size t of the form (xi, xi, · · · , xi) with recovery sets of
size at most r. Therefore it is a PIR code.
Example 3. Let k = 8, r = 4, t = 3 so that k/r = 2. Then the following
generator matrix G generates a batch code of length n = 14.
G =
I4 0
0 I4

1 0
0 1

I4
I4

.

Constructions and Bounds for Batch Codes with Small Parameters
291
Example 4. Let k = 12, r = 3, t = 5 so that k/r = 4. Then the following
generator matrix G generates a PIR code of length n = 12 + 4 + 3 · 4 = 28.
G =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
I3 0 0 0
0 I3 0 0
0 0 I3 0
0 0 0 I3

1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1

1 1 1 0
0 0 0 0
0 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
1 0 0 0
0 0 0 1
0 1 0 0
0 0 0 1
0 0 1 0
1 0 0 0

0 0 0 0
1 1 1 0
0 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
0 0 0 1
1 0 0 0

0 0 0 0
0 0 0 0
1 1 1 0
0 1 0 0
0 0 1 0
0 0 0 1
0 0 0 1
0 1 0 0
1 0 0 0
1 0 0 0
0 0 0 1
0 0 1 0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
The rate of the above code is
k
n = 12
28 = 3
7 =
r
r + t −1 .
This rate is greater than the rate of the direct product code in [16].
4.2
Case t = 3 and r ∤k
In the sequel, we extend the results in the previous subsection towards the case
where t = 3 and r ∤k. Let G be the generator matrix of the block form G =
[Ik | A | B | C].
A is a k ×
 k
r

matrix with the jth column of the form A[j]
=
(a1,j, a2,j, · · · , ak,j)T , 1 ≤j ≤
 k
r

, where ai,j = 1 if (j −1)r + 1 ≤i ≤jr,
and ai,j = 0 otherwise.
Denote s = k mod r. The matrix B is a k × (s + 1) block matrix, deﬁned as
B =

BT
1 |BT
2
T , where B1 is (k −s) × (s + 1) and B2 is an s × (s + 1) matrix
[1 | Is].
We take τ
≜
min

r −s,
 k
r

. The ﬁrst column of B1, B[1]
1 , has τ
entries 1, each entry appears in a diﬀerent block of rows [(j −1)r + 1, jr]
for j
= 1, 2, · · · ,
 k
r

. We take also η
≜min

r −1,
 k
r

. The columns
B[2]
1 , B[3]
1 , · · · , B[s+1]
1
, all have η entries 1, each entry appears in a diﬀerent block
of rows. Additionally, every row in B1 contains at most one non-zero entry.
We observe that every column in B has at most r ones. Denote γ ≜
min

r,
 k
r

. The matrix C is constructed according to the following rules:
– Each column in C has γ ones (except possibly for the last column);
– The last s rows in C are zeros;
– Each row in [ A | B | C ] has two ones;
– There are no 1-squares in [ B | C ].

292
E.K. Thomas and V. Skachek
Next, we estimate the total number of columns in G. The number of ones
in the ﬁrst k −s positions of B[1] is τ. The number of ones in the ﬁrst k −s
positions of each of B[2], B[3], · · · , B[s+1] is η. Since there are two ones in each
of the ﬁrst k −s rows of [ A | B | C ], the total number of columns in G is
n = (r + 1)
k
r

+ 2s + 1 +
(k −s) −τ −η · s
γ

.
Proposition 12. For t = 3 and r ≥3,
k+1+
2k −1
2r −1

≤B(k, r, t) ≤

(r + 1) k
r + ζ
if r|k
(r + 1)
 k
r

+ 2s + 1 +

(k−s)−τ−η·s
γ

if r ∤k
where ζ = max{ k
r , r}.
Proposition 13. The code C deﬁned in this section by the generator matrix G
supports any query of the form (xi, xj, xℓ) with recovery sets of size at most r,
i, j, ℓ∈[k].
Example 5. Let k = 11, r = 3, t = 3 so that ⌊k/r⌋= 3 and s = 2. Then the
following generator matrix G generates a batch code of length n = 19.
G =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
I3 0 0 0
0 I3 0 0
0 0 I3 0
0 0 0 I2

1 0 0
0 1 0
0 0 1
0 0 0

1
0
0
0
0
0
0
0
0
1

0 0
0 1
0 0
1 0
0 1
0 0
1 0
0 0
0 0
I2

0 0
0 0
1 0
0 0
0 0
1 0
0 0
0 1
1 0
0 0
0 0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
We summarize the bounds on B(k, r, t) and P(k, r, t) for t ∈{2, 3, 4} and for
r ≥2 in Tables 1 and 2.
Table 1. Lower bounds for B(k, r, t) and P(k, r, t)
t = 2
t = 3
t = 4
r ≥2 k + ⌈k/r⌉k + 1 +

2k−1
2r−1

k + 2 +

3k−2
3r−2

Table 2. Upper bounds for B(k, r, t) and P(k, r, t)
t = 2
t = 3
t = 4
r = 2, k > 3
k + ⌈k/2⌉
2k
k + ⌈3k/2⌉
r ≥3
k +
 k
r

⎧
⎨
⎩
(r + 1) k
r + ζ
if r|k
(r + 1)
 k
r

+ 2s + 1 +
	
(k−s)−τ−η·s
γ

if r ∤k
(r + 1) k
r + 2ζ, if r|k

Constructions and Bounds for Batch Codes with Small Parameters
293
5
Bounds on the Dimension of a Batch Code
Let kopt
q
(n, d) denote the largest possible dimension of a linear code of length
n and minimum distance d, for a given alphabet Q of size q. More formally, by
following on the notations in [4], denote:
kopt
q
(n, d) ≜max log |C|
log q ,
where the maximum is taken over all possible linear codes C of length n with
minimum distance d.
Let I ⊆[n] be a set of coordinates. Deﬁne
H(I) = log |{xI : x ∈C}|
log q
.
The following result appears as Lemma 2 in [4].
Lemma 2. Consider an [n, k, d] code over Q where there exists a set I ⊆[n]
such that H(I) ≤m. Then, there exists an [n −|I|, (k −m)+, d] code over Q,
where the + symbol denotes that the dimension is at least k −m.
Cadambe and Mazumdar show in [4] that, for any r-locally recoverable
(n, k, d) code over the alphabet Q, it holds:
k ≤min
t

tr + kopt
q
(n −t(r + 1), d)

.
(7)
By using similar techniques, in the sequel we show a bound on k for a linear
(n, k, r, t) batch code. We restrict our discussion to linear codes only.
Proposition 14. Let C be a linear (n, k, r, t) batch code over an alphabet Q of
size q with minimum distance d, and n −tr ≥d. Then,
k ≤tr −(t −1) + kopt
q
(n −tr, d) .
(8)
Proof. Since C is an (n, k, r, t) batch code, for a query (xi, . . . , xi) of size t, for
some i ∈[k], there exist t disjoint recovery sets R1, . . . , Rt with |Rj| ≤r for all
j ∈[t].
Denote I ≜R1 ∪R2 ∪· · ·∪Rt. Clearly, |I| ≤tr. Take an arbitrary word y ∈C.
By Lemma 1, there exist indices a2 ∈R2, a3 ∈R3, · · · , at ∈Rt such that if we ﬁx
the values of all coordinates ofy indexed by the sets R1, R2\{a2}, R3\{a3}, . . . , Rt\
{at}, then the values of the coordinates of y indexed by S ≜{a2, a3, . . . , at} are
uniquely determined. It follows that there is one-to-one mapping between yI\S
and yI. Therefore, H(I) ≤tr −(t −1).
The main statement now follows by applying Lemma 2.
We remark that the condition n −tr ≥d in Proposition 14 is necessary for the
existence of a code of length n −tr and minimum distance d. However, it should
be noted that any (n, k, r, t) batch code is a (n, k, r, β) batch code for 1 ≤β ≤t.
Thus, if t is too large to satisfy this condition, we can take a smaller value β
instead. The following result follows.

294
E.K. Thomas and V. Skachek
Corollary 2. Let C be a linear (n, k, r, t) batch code over an alphabet Q of size
q with minimum distance d. Then,
k ≤min
1≤β≤t

βr −(β −1) + kopt
q
(n −βr, d)

.
(9)
Comparison with the Bounds in the Literature
Example 6. The asymptotic versions of the classical Singleton and the sphere-
packing bounds for a code over alphabet Q of size q are R ≤1 −δ + o(1) and
R ≤1−hq(δ/2)+o(1), respectively, where R = k/n, δ = d/n, and hq(·) denotes
the q-ary entropy function [14, Chap. 4].
The asymptotic versions of the bounds (1) and (9) can be rewritten as (when
ignoring the o(1) term, for any speciﬁc value of β):
R ≤1 −δ −β −1
n

k
βr −β + 1

−1

,
(10)
and
R ≤β(r −1)
n
+ Ropt
q
(n −βr, d) ,
(11)
respectively, where Ropt
q
(n, d) denotes the maximum rate of any code of length
n and minimum distance d over Q.
For n −d ≫tr ≥βr, the bound (10) becomes R ≤(1 −δ) ·
 
1 −β−1
βr
!
. For
comparison, when we use the sphere-packing bound for Ropt
q
(n, d), then (11)
can be rewritten as
R ≤β(r −1)
n
+ 1 −hq(δ/2) ≈1 −hq(δ/2) .
Therefore, for large blocklength, n −d ≫tr, and for a range of δ and r, the
bound (9) is tighter than (1) (for any β ≥1).
Acknowledgment. The work of E. Thomas and V. Skachek is supported by the
Estonian Research Council under the grants PUT405 and IUT2-1. The authors wish
to thank Mart Simisker for valuable comments on the earliest version of this work.
References
1. Bhattacharya, S., Ruj, S., Roy, B.: Combinatorial batch codes: a lower bound and
optimal constructions. Adv. Math. Commun. 6(2), 165–174 (2012)
2. Brualdi, R.A., Kiernan, K., Meyer, S.A., Schroeder, M.W.: Combinatorial batch
codes and transversal matroids. Adv. Math. Commun. 4(3), 419–431 (2010)
3. Bujtas, C., Tuza, Z.: Batch codes and their applications. Electron. Notes Discrete
Math. 38, 201–206 (2011)
4. Cadambe, V., Mazumdar, A.: Bounds on the size of locally recoverable codes. IEEE
Trans. Inf. Theory 61(11), 5787–5794 (2015)

Constructions and Bounds for Batch Codes with Small Parameters
295
5. Chee, Y.M., Gao, F., Teo, S.T.H., Zhang, H.: Combinatorial systematic switch
codes. In: Proceedings of the IEEE International Symposium on Information The-
ory (ISIT), Hong Kong, China, pp. 241–245 (2015)
6. Dimakis, A.G., Gal, A., Rawat, A.S., Song, Z.: Batch codes through dense graphs
without short cycles, arXiv:1410.2920, October 2014
7. Fazeli, A., Vardy, A., Yaakobi, E.: PIR with low storage overhead: coding instead
of replication, arXiv:1505.06241, May 2015
8. Forbes, M., Yekhanin, S.: On the locality of codeword sysmbols in non-linear codes.
Discrete Math. 324, 78–84 (2014)
9. Gopalan, P., Huang, C., Simitci, H., Yekhanin, S.: On the locality of codeword
symbols. IEEE Trans. Inform. Theory 58(11), 6925–6934 (2012)
10. Ishai, Y., Kushilevitz, E., Ostrovsky, R., Sahai, A.: Batch codes and their appli-
cations. In: Proceeding of the 36th ACM Symposium on Theory of Computing
(STOC), Chicago, June 2004
11. Lipmaa, H., Skachek, V.: Linear batch codes. In: Proceedings of the 4th Inter-
national Castle Meeting on Coding Theory and Applications, Palmela, Portugal,
September 2014
12. Rawat, A.S., Mazumdar, A., Vishwanath, S.: Cooperative local repair in distrib-
uted storage. EURASIP J. Adv. Sign. Process. 1, 107 (2015)
13. Rawat, A.S., Papailiopoulos, D.S., Dimakis, A.G., Vishwanath, S.: Locality and
availability in distributed storage. IEEE Trans. Inform. Theory 62(8), 4481–4493
(2016)
14. Roth, R.M.: Introduction to Coding Theory. Cambridge University Press, Cam-
bridge (2006)
15. Skachek, V.: Batch and PIR Codes and Their Connections to Locally Repairable
Codes, arXiv:1611.09914, November 2016
16. Tamo, I., Barg, A.: Bounds on Locally Recoverable Codes with Multiple Recovering
Sets (2014). arXiv:1402.0916v1
17. Vardy, A., Yaakobi, E.: Constructions of batch codes with near-optimal redun-
dancy. In: Proceedings of the IEEE International Symposium on Information The-
ory (ISIT), Barcelona (2016)
18. Wang, Z., Kiah, H.M., Cassuto, Y.: Optimal binary switch codes with small query
size. In: Proceeding of the IEEE International Symposium on Information Theory
(ISIT), Hong Kong, pp. 636–640, June 2015
19. Wang, Z., Shaked, O., Cassuto, Y., Bruck, J.: Codes for network switches. In:
Proceeding of the IEEE International Symposium on Information Theory (ISIT),
Istanbul (2013)
20. Wang, A., Zhang, Z.: Achieving Arbitrary Locality and Availability in Binary
Codes, arXiv:1501.04264v1 (January 2015)
21. Zhang, H., Skachek, V.: Bounds for batch codes with restricted query size. In:
Proceeding of the IEEE International Symposium on Information Theory (ISIT),
Barcelona, pp. 1192–1196 (2016)

Author Index
Bartoli, Daniele
1
Beemer, Allison
11, 21
Bernal, José Joaquín
30
Bocharova, Irina E.
48
Cardell, Sara D.
62
Climent, Joan-Josep
62, 72
Davydov, Alexander A.
1
dela Cruz, Romar B.
192
Egorova, Elena
84
Engelberg, Shlomo
93
Fernández-Córdoba, Cristina
107
Freij-Hollanti, Ragnar
139
Geil, Olav
118, 128
Giulietti, Massimo
1
Grezet, Matthias
139
Haymaker, Kathryn
154
Heinlein, Daniel
163
Hollanti, Camilla
139
Jitman, Somphong
192
Johnsen, Trygve
207
Kabatiansky, Grigory
84
Kelley, Christine A.
11, 21
Keren, Osnat
93, 272
Kreshchuk, Alexey
217
Kudryashov, Boris D.
48
Kurz, Sascha
163
Laaksonen, Antti
228
Lucani, Daniel E.
118
Marcugini, Stefano
1
Mayer, Carolyn
21
Napp, Diego
238, 248
Östergård, Patric R.J.
228
Özbudak, Ferruh
128
Palines, Herbert S.
192
Pambianco, Fernanda
1
Pereira, Ricardo
238
Pinto, Raquel
248, 257
Rabii, Hila
272
Requena, Verónica
72
Roca, Alicia
62
Rocha, Paula
238
Rosenthal, Joachim
248
Santana, Filipa
248
Simões, Rita
257
Simón, Juan Jacobo
30
Skachek, Vitaly
48, 283
Soler-Escrivà, Xaro
72
Thomas, Eldho K.
283
Vela, Carlos
107
Verdure, Hugues
207
Villanueva, Mercè
107
Westerbäck, Thomas
139
Yakimenka, Yauhen
48
Zhilin, Igor
217
Zyablov, Victor
217


Multidimensional
Signal, Image, and
Video Processing
and Coding

Multidimensional
Signal, Image, and
Video Processing
and Coding
Second Edition
John W. Woods
Rensselaer Polytechnic Institute
Troy, New York
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Academic Press is an imprint of Elsevier

Academic Press is an imprint of Elsevier
225 Wyman Street, Waltham, MA 02451, USA
The Boulevard, Langford Lane, Kidlington, Oxford, OX5 1GB, UK
c⃝2012 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or
mechanical, including photocopying, recording, or any information storage and retrieval system, without
permission in writing from the publisher. Details on how to seek permission, further information about the
Publisher’s permissions policies and our arrangements with organizations such as the Copyright Clearance
Center and the Copyright Licensing Agency, can be found at our website: www.elsevier.com/permissions
This book and the individual contributions contained in it are protected under copyright by the Publisher (other
than as may be noted herein).
Notices
Knowledge and best practice in this ﬁeld are constantly changing. As new research and experience broaden our
understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using
any information, methods, compounds, or experiments described herein. In using such information or methods
they should be mindful of their own safety and the safety of others, including parties for whom they have a
professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability
for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or
from any use or operation of any methods, products, instructions, or ideas contained in the material herein.
MATLAB R⃝is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not
warrant the accuracy of the text or exercises in this book. This book’s use or discussion of MATLAB R⃝software
or related products does not constitute endorsement or sponsorship by The MathWorks of a particular
pedagogical approach or particular use of the MATLAB R⃝software.
Library of Congress Cataloging-in-Publication Data
Woods, John W. (John William), 1943-
Multidimensional signal, image, and video processing and coding / John W. Woods.—2nd ed.
p. cm.
Includes bibliographical references.
ISBN 978-0-12-381420-3
1. Image processing—Digital techniques. 2. Signal processing—Digital techniques. 3. Video
compression—Standards. 4. Digital video. I. Title.
TA1637.W67 2012
621.382’2—dc22
2011002927
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
For information on all Academic Press publications
visit our website at www.elsevierdirect.com
Typeset by: diacriTech, India
Printed in the United States of America
11 12 13 14
9 8 7 6 5 4 3 2 1

Preface
This is a textbook for a ﬁrst- or second-year graduate course for electrical and com-
puter engineering (ECE) students in the area of digital image and video processing
and coding. The course might be called Digital Image and Video Processing (DIVP)
or some such, and has its heritage in the signal processing and communications areas
of ECE. The relevant image (and video) processing problems can be categorized as
image-in/image-out, rather than image-in/analysis-out types of problems. Though for
this second edition, we have broadened our coverage by adding introductory material
on image analysis.
The required background is an undergraduate digital signal processing (DSP)
course and a course in probability. In addition, some knowledge of random processes
and information theory is required for some sections. So in this second edition, we
have added introductory material on random processes and information theory in
chapter appendices. In addition to its role as a graduate text, the book is also suitable
for self-study by graduate engineers with a communications and signal processing
background.
The DIVP course at Rensselaer Polytechnic Institute has been offered for the last
20 years, now on an alternate year basis, having started as a course in multidimen-
sional DSP. Over the years we brought in an emphasis ﬁrst on image processing and
coding, and then on video, and ﬁnally, some coverage of network transmission of
video. The book, as well as the DIVP course, starts out with two-dimensional (2-D)
signal processing theory, including 2-D systems, partial difference equations, Fourier
and Z-transforms, ﬁlter stability, discrete transforms such as DFT and DCT and their
fast algorithms, ending up with 2-D or spatial ﬁlter design. We also introduce the sub-
band/wavelet transform (SWT) here, along with coverage of the DFT and DCT. This
material is contained in the ﬁrst ﬁve chapters and constitutes the ﬁrst part of the book.
However, there is also a later chapter on 3-D and spatiotemporal signal processing,
strategically positioned between the image and the video processing chapters.
The second part of the book, comprising the remaining seven chapters, covers
image and video processing and coding, along with the introduction to video trans-
mission on networks. We start out with image perception and sensing, followed by
basic ﬁlters for image processing, and then treat image enhancement and image anal-
ysis. This is followed by four individual chapters on estimation/restoration and source
coding, ﬁrst for images and then for video. The ﬁnal chapter on network transmission
summarizes basic networking concepts and then goes on to consider effects of packet
loss and their amelioration.
This paragraph and the next provide detailed chapter information. Starting out the
ﬁrst part, Chapter 1 introduces 2-D systems and signals along with the BIBO stability
concept, Fourier transform, and spatial convolution. Chapter 2 covers sampling and
considers both rectangular and general regular sampling patterns, e.g., diamond and
hexagonal sample patterns. Chapter 3 introduces 2-D difference equations and the Z
transform including recursive ﬁlter stability theorems. Chapter 4 treats the discrete
xv

xvi
Preface
Fourier and cosine transforms along with their fast algorithms and brieﬂy covers
2-D sectioned-convolution. We also introduce the ideal subband/wavelet transform
(SWT) here, postponing its design problem to the next chapter. Chapter 5 covers
2-D ﬁlter design, mainly through the separable and circular window method, but also
introducing the problem of 2-D recursive ﬁlter design, along with some coverage of
general or fully recursive ﬁlters.
The second part of the book, the part on image and video processing and coding,
starts out with Chapter 6, which presents basic concepts in human visual perception
and color space, as well as basic coverage of common image sensors and displays.
Chapter 7 ﬁrst covers the basic image processing ﬁlters: box, Prewitt, and Sobel, and
then employs them for simple image enhancement. The chapter also includes basic
image analysis, including edge detection and linking, image segmentation, object
detection, and template matching.
Chapter 8 covers image estimation and restoration, including adaptive or inhomo-
geneous approaches, and an introduction to image- and blur-model parameter
identiﬁcation via the EM algorithm. We also include material on compound Gauss-
Markov models and their MAP estimation via simulated annealing. We look at new
approaches such as Gaussian scale mixtures and nonlocal estimators. Chapter 9
covers image compression built up from the basic concepts of transform, scalar and
vector quantization, and variable-length coding. We cover basic DCT coders and
also include material on fully embedded coders such as EZW, SPIHT, and EZBC
and introduce the main concepts of the JPEG 2000 standard. Then Chapter 10 on
three-dimensional and spatiotemporal or multidimensional signal processing
(MDSP) extends the 2-D concepts of Chapters 1–5 to the 3-D or spatiotemporal
case of video. Also included here are rational system models and spatiotemporal
Markov models culminating in a spatiotemporal reduced-update Kalman ﬁlter. Next,
Chapter 11 studies motion estimation including block matching, variable size block
matching, optical ﬂow, and mesh-based matching, and the techniques of motion
compensation. Applications include motion-compensated Kalman ﬁltering, frame-
rate change, and deinterlacing. The chapter ends with the Bayesian approach to
joint motion estimation and segmentation, and a new section on super-resolution.
Chapter 12 covers video compression with both hybrid and spatiotemporal trans-
form approaches and includes coverage of video coding standards such as MPEG 2
and H.264/AVC. This second edition includes coverage on the new scalable video
standard H.264/SVC and the 3-D stereoscopic standard H.264/MVC. We include
coverage of highly scalable coders based on the motion-compensated temporal ﬁlter
(MCTF).
Finally, Chapter 13 is devoted to video on networks, starting out with network
fundamentals to make the chapter accessible to signal processing and commu-
nication graduate students without a networking background. The chapter then
proceeds to robust methods for network video. We discuss error-concealment fea-
tures in H.264/AVC. We include methods of error concealment and robust scalable
approaches using MCTF and embedded source coding. We present multiple descrip-
tion coding in combination with forward error correction as an efﬁcient way to deal

Preface
xvii
with packet losses. This second edition now includes basic introductory material for
practical network coding. We look at combinations of multiple description coding
and network coding for efﬁcient heterogeneous multicast.
This
book
also
has
an
associated
Web
site
(http://elsevierdirect.com/
9780123814203) that contains many short MATLAB programs that complement
examples and exercises on MDSP. There is also a .pdf document on the site that
contains high-quality versions of all the ﬁgures and images in the book. There are
numerous short video clips showing applications in video processing and coding.
The Web site has a copy of the vidview video player for playing .yuv video ﬁles
on a Windows PC. Other video ﬁles can generally be decoded and played by the
commonly available media decoder/players. Also included is an illustration of effect
of packet loss on H.264/AVC coded bitstreams. (Your media decoder/player would
need an H.264/AVC decoder component to play these, however, some .yuv ﬁles are
included here in case it doesn’t.)
This textbook can be utilized in several ways depending on the course level and
desired learning objectives. One path is to ﬁrst cover Chapters 1–5 on MDSP, and
then go on to Chapters 6 through 9 to cover image processing and coding, followed
by some material on video processing and coding from later chapters, and this is
how we have most often used it at Rensselaer. Alternatively, after Chapters 1–5,
one could go on to image and video processing in Chapters 6–8 and 10–11. Or, and
again after covering Chapters 1–5, go on to image and video compression in Chapter
6–7, 9–10, and 12. The network transmission material from Chapter 13 could also
be included, time permitting. To cover the image and video processing and coding
in Chapters 6–12 in a single semester, some signiﬁcant sampling of the ﬁrst ﬁve
chapters would probably be needed. One approach may be to skip (or very lightly
cover) Chapter 3 on 2-D systems and Z transforms and Chapter 5 on 2-D ﬁlter
design, but cover Chapters 1, 2, and part of Chapter 4. Still another possibility is to
cover Chapters 1 and 2, and then move on to Chapters 6–12, introducing topics from
Chapters 3–5 only as needed. An online solutions manual will be made available
to instructors at http://textbooks.elsevier.com with completion of registration in the
Electronics and Electrical Engineering subject area.
John W. Woods
Rensselaer Polytechnic Institute
Troy, New York
Spring 2011
ACKNOWLEDGMENTS
I started out writing the ﬁrst edition of this book while teaching from the text-
book Two-Dimensional Digital Image Processing by Jae Lim, and readers familiar
with that book will certainly see a similarity to mine, especially in the ﬁrst ﬁve

xviii
Preface
chapters. Thanks to colleagues Janusz Konrad, Aggelos Katsaggelos, Edward Delp,
Siwei Lyu, and Dirk-Jan Kroon for providing examples. Thanks also to Fure-Ching
Jeng, Shih-Ta Hsiang, Seung-Jong Choi, T. Naveen, Peisong Chen, Anil Murching,
Allesandro Dutra, Ju-Hong Lee, Yongjun Wu, Eren Soyak, Yufeng Shan, Adarsh
Ramasubramonian, and Han Huang. Special thanks to former student Ivan Baji´c for
contributing Section 13.2. Thanks also to my wife Harriet, who has cheerfully put up
with this second edition.

CHAPTER
Two-Dimensional Signals
and Systems
1
This chapter sets forth the main concepts of two-dimensional (2-D) signals and sys-
tems as extensions of the linear systems concepts of 1-D signals and systems. We
concentrate on the discrete-space case of digital data, including the corresponding
2-D Fourier transform. We also introduce the continuous-space Fourier transform
to deal with angular rotations and prove the celebrated projection-slice theorem of
computer tomography. In later chapters we will encounter the central role of motion
compensation in image sequences and video, where motion is often not an exact
number of pixels, which makes the interplay of discrete parameters and continu-
ous parameters quite important. While this ﬁrst chapter focuses on discrete space,
Chapter 2 focuses on sampling of 2-D continuous-space functions.
1.1 TWO-DIMENSIONAL SIGNALS
A scalar 2-D signal s(n1,n2) is mathematically a complex bi-sequence, or a mapping
of the 2-D integers into the complex plane. Our convention is that the signal s is
deﬁned for all ﬁnite values of its integer arguments n1,n2 using zero padding as nec-
essary. Occasionally we will deal with ﬁnite extent signals, but we will clearly say so
in such instances. We will adopt the simpliﬁed term sequence over the more correct
bi-sequence.
A simple example of a 2-D signal is the impulse δ(n1,n2), deﬁned as follows:
δ(n1,n2) ≜
1 for (n1,n2) = (0,0),
0 for (n1,n2) ̸= (0,0),
a portion of which is plotted in Figure 1.1–1.
A general 2-D signal can be written as an inﬁnite sum over shifted impulses,
which will be found useful later:
x(n1,n2) =
X
k1,k2
x(k1,k2)δ(n1 −k1,n2 −k2),
(1.1–1)
where the summation is taken over all integer pairs (k1,k2). By the deﬁnition of the
2-D impulse, for each point (n1,n2), there is exactly one term on the right side that is
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00001-1
c⃝2012 Elsevier Inc. All rights reserved.
1

2
CHAPTER 1 Two-Dimensional Signals and Systems
−10
−5
0
5
10
−10
−5
0
5
10
0
0.2
0.4
0.6
0.8
1
FIGURE 1.1–1
MATLAB plot of 2-D spatial impulse bi-sequence δ(n1,n2).
nonzero, the term x(n1,n2) · 1, and hence the result (1.1–1) is correct. This equality is
called the shifting representation of the signal x.
Another basic 2-D signal is the impulse line, e.g., a straight line at 45◦,
δ(n1 −n2) =
1,
n1 = n2,
0,
else.
A portion of this line is sketched in Figure 1.1–2.
We can also consider line impulses at 0◦, 90◦, and −45◦in discrete space, but
other angles give “gaps in the line.” Toward the end of this chapter, we will look at
continuous-space signals; there the line impulse can be at any angle.
Another basic 2-D signal class is step functions, perhaps the most common of
which is the ﬁrst-quadrant unit step function u(n1,n2), deﬁned as follows:
u(n1,n2) ≜
1,
n1 ≥0, n2 ≥0,
0,
elsewhere.
A portion of this bi-sequence is plotted in Figure 1.1–3.
Actually, in two dimensions, there are several step functions of interest. The one
shown in Figure 1.1–3 is called the ﬁrst quadrant unit step function and is more
generally denoted as u++(n1,n2).

1.1 Two-Dimensional Signals
3
−10
−5
0
5
10
0
10
0
0.2
0.4
0.6
0.8
1
FIGURE 1.1–2
A portion of the unit impulse line δ(n1 −n2) at 45◦.
−5
0
5
10
15
−5
0
5
10
15
0
0.5
1
1.5
2
n1
n2
u++
FIGURE 1.1–3
A portion of the unit step bi-sequence u(n1,n2).

4
CHAPTER 1 Two-Dimensional Signals and Systems
−15
−10
−5
0
5
−5
0
5
10
15
0
0.5
1
1.5
2
n1
n2
u−+
FIGURE 1.1–4
A portion of the second quadrant unit step bi-sequence u−+(n1,n2).
We will ﬁnd it convenient to use the word support to denote the set of all argument
values for which the function is nonzero. In the case of the ﬁrst quadrant unit step
u++(n1,n2), this becomes
supp(u++) = {n1 ≥0, n2 ≥0}.
Three other unit step functions can be deﬁned for the other three quadrants. They
are denoted u−+, u+−, and u−−, with support on the second, fourth, and third quad-
rants, respectively. A plot of a portion of the second quadrant unit step is shown in
Figure 1.1–4. We often write simply u++ = u for the ﬁrst quadrant unit step. Later,
we will also ﬁnd it convenient to talk about halfplane support unit steps deﬁned
analogously.
A real example of a ﬁnite support 2-D sequence is the image Eric shown in three
different ways in Figures 1.1–5 through 1.1–7. The ﬁrst, Figure 1.1–5, is a contour
plot of Eric, a 100- × 76-pixel, 8-bit grayscale image. The second, Figure 1.1–6, is
a perspective (or mesh) plot. The third, Figure 1.1–7, is an image (or intensity) plot,
with the largest value being white and the smallest value being black.
Separable Signals
A separable signal (sequence) satisﬁes the equation
x(n1,n2) = x1(n1)x2(n2)
for all n1 and n2,

1.1 Two-Dimensional Signals
5
5
10
15
20
25
30
35
40
45
50
5
10
15
20
25
30
35
FIGURE 1.1–5
A contour plot of Eric.
0
5
10
15
20
25
30
35
40
45
50
0
5
10
15
20
25
30
35
40
0
100
200
300
FIGURE 1.1–6
A mesh plot of Eric.

6
CHAPTER 1 Two-Dimensional Signals and Systems
FIGURE 1.1–7
An intensity plot of Eric.
for some 1-D signals x1(n1) and x2(n2). If we think of the ﬁnite support case, where
x(n1,n2) can be represented by a matrix, then x1(n1) and x2(n2) can be represented
as column and row vectors, respectively. So, we see that separability is the same
as saying that the matrix X can be written as the outer product X = x1xT
2 , which
is the same as saying that the matrix X has only one singular value in its singular
value decomposition (SVD).1 Clearly, this is very special. Note that while an N × N
matrix X has N2 degrees of freedom (number of variables), the outer product x1xT
2
has only 2N degrees of freedom. Nevertheless, separable signals play an important
role in multidimensional signal processing (MDSP) as representation bases (e.g.,
Fourier transform) and simple ﬁlter impulse responses. A “real” image, like Eric in
Figure 1.1–7, regarded as a 100 × 76 matrix would have extremely many terms in its
SVD, i.e., highly nonseparable.
Periodic Signals
A 2-D sequence x(n1,n2) is periodic with period (N1, N2), also written as N1 × N2, if
the following equalities hold for all integers n1and n2:
x(n1,n2) = x(n1 + N1,n2),
= x(n1,n2 + N2),
where N1 and N2 are positive integers.
1The SVD is a representation of a matrix in terms of its eigenvalues and eigenvectors and is written for
a real square matrix as X = PλieieT
i , where the λi are the eigenvalues and the ei are the eigenvectors
of X.

1.1 Two-Dimensional Signals
7
The period (N1,N2) deﬁnes a 2-D grid (either spatial or space-time) over which
the signal repeats or is periodic. Since we are in discrete space, the period must be
composed of positive integers. This type of periodicity occurs often for 2-D sig-
nals and is referred to as rectangular periodicity. We call the resulting period the
rectangular period.
Example 1.1–1: A Periodic Signal
An example is the signal sin(2πn1/8 + 2πn2/16), for which the rectangular period is easily
seen to be (8,16). A separable signal with the same period is sin(2πn1/8)sin(2πn2/16).
If, however, we remove the factor 2π in the arguments, then we get sin(n1/8 + n2/16)
and sin(n1/8)sin(n2/16), neither of which is periodic at all! This is because we only allow
integer values n1 and n2 in discrete-parameter signal space.
Given a periodic function, the period effectively deﬁnes a basic cell in the plane,
which can be repeated to form the function over all integers n1 and n2. As such, we
often want the minimum size unit cell for efﬁciency of both speciﬁcation and storage.
In the case of the rectangular period, we seek the smallest nonzero integers that will
sufﬁce for N1 and N2 to form this basic cell.
Example 1.1–2: Horizontal Wave
Consider the sine wave
x(n1,n2) = sin(2πn1/4).
The horizontal period is N1 = 4. In the vertical direction, the signal is constant though. So
we can use any positive integer N2, and we choose the smallest such value, N2 = 1. Thus
the rectangular period is N1 × N2 = 4 × 1, and the basic cell consists of the set of points
{(n1,n2) = [(0,0),(1,0),(2,0),(3,0)]} or any translate of this set.
General Periodicity
There is a more general deﬁnition of periodicity that we will encounter from time
to time in this text. It is a repetition of blocks, not necessarily rectangular blocks
or blocks occurring on a rectangular repeat grid. For the general case, we need to
represent the periodicity with two integer vectors, N1 and N2:
N1 =
N11
N21

and N2 =
N12
N22

.

8
CHAPTER 1 Two-Dimensional Signals and Systems
Then we can write that the 2-D signal x(n1,n2) is general periodic with period
(N1,N2) ≜N if the following equalities hold for all integers n1 and n2:
x(n1,n2) = x(n1 + N11,n2 + N21)
= x(n1 + N12,n2 + N22).
To avoid degenerate cases, we restrict the integers Nij with the condition
det(N1,N2) ̸= 0.
The matrix N is called the periodicity matrix. In matrix notation, the correspond-
ing periodic signal satisﬁes
x(n) = x(n + Nr)
for all integer vectors r =
r1
r2

. In words, we can say that the signal repeats itself at
all multiples of the shift vectors N1 and N2.
Example 1.1–3: General Periodic Signal
An example is the signal sin [2π(n1/8 + n2/16)], which is constant along the line
2n1 + n2 = 16. We can compute shift vectors N1 =
 
4
8
!
and N2 =
 
1
−2
!
.
We note that the special case of rectangular periodicity occurs when the periodic-
ity matrix N is diagonal, for then
N =
N1
0
0
N2

,
and the rectangular period is (N1,N2), as above. Also in this case, the two period
vectors N1 =
N1
0

and N2 =
 0
N2

lie along the horizontal and vertical axes,
respectively.
Example 1.1–4: Cosine Wave
Consider the signal g(n1,n2) = cos2π( f1n1 + f2n2) = cos2πfTn. In continuous space, this
signal is certainly periodic, and the rectangular period would be N1 × N2, with period
vectors N1 =
 
f −1
1
0
!
and N2 =
 
0
f −1
2
!
. However, this is not a correct answer in discrete
space unless the resulting values f −1
1
and f −1
2
are integers. Generally, if f1 and f2 are
rational numbers, we can get an integer period as follows: Ni = pif −1
i
, where fi = pi/qi,

1.1 Two-Dimensional Signals
9
i = 1,2. If either of the fi are not rational, there will be no exact rectangular period for
this cosine wave. Regarding general periodicity, we are tempted to look for repeats in the
direction of the vector ( f1,f2)T since fTn is maximized if we increment the vector n in this
direction. However, again we have the problem that this vector would typically not have
integer components. We are left with the conclusion that common cosine and sine waves
are generally not periodic in discrete space, at least not exactly periodic. The analogous
result is also true, although not widely appreciated, in the 1-D case.
2-D Discrete-Space Systems
As illustrated in Figure 1.1–8, a 2-D system is deﬁned as a general mathematical
operator T that maps each input signal x(n1,n2) into a unique output signal y(n1,n2).
In equations we write
y(n1,n2) = T[x(n1,n2)],
where we remind the signals are assumed to be deﬁned over the entire 2-D dis-
crete space (−∞,+∞) × (−∞,+∞), unless otherwise indicated. There is only one
restriction on the general system operator T: it must provide a unique mapping; that
is, for each input sequence x there is only one output sequence y. Of course, two
input sequences could agree only over some area of the plane, but differ elsewhere.
Then there can be different output y′s corresponding to these two inputs, since these
two inputs are not equal everywhere. In mathematics, an operator such as T is just
the generalization of the concept of function, where the input and output spaces are
now sequences instead of numbers. The operator T may have an inverse or not. We
say that T is invertible if to each output sequence y there corresponds only one input
sequence x (i.e., the output determines the input). We denote the inverse operator, if
it exists, by T−1.
x(n1, n2) 
y(n1, n2) 
T{  } 
FIGURE 1.1–8
A general 2-D discrete-space system.
Example 1.1–5: Some Simple Systems
Consider the following:
(a) y(n1,n2) = T[x(n1,n2)] = 2x(n1,n2)
(b) y(n1,n2) = T[x(n1,n2)] = x2(n1,n2) + 3x(n1,n2) + 5
(c) y(n1,n2) = T[x(n1,n2)] = 3x(n1,n2) + 5
(d) y(n1,n2) = T[x(n1,n2)] = 1
2[x(n1,n2) + x(n1 −1,n2)]

10
CHAPTER 1 Two-Dimensional Signals and Systems
We note by inspection that the 2-D system operators given in items (a) and (c) are
invertible, while that given in item (b) is not. The system given by item (d) has memory
while those in items (a)–(c) do not. If we call (n1,n2) the present, then a memoryless
system is one whose present output only depends on present inputs.
A special case of the general 2-D system is the linear system (Deﬁnition 1.1–1).
Deﬁnition 1.1–1: Linear System
A 2-D discrete system is linear if the following equation holds for all input-output sequence
pairs xi −yi and all scaling constants ai:
L[a1x1(n1,n2) + a2x2(n1,n2)] = a1L[x1(n1,n2)] + a2L[x2(n1,n2)],
where we have denoted the linear operator by L.
As in 1-D signal processing, linear systems are very important to the theory of
2-D signal processing. The reasons are the same: (1) we know the most about linear
systems, (2) approximate linear systems arise a lot in practice, and (3) many adaptive
nonlinear systems are composed of linear blocks, designed using linear system theory.
In the previous example, systems (a) and (d) are linear by this deﬁnition. A simple
necessary condition for linearity is that the output doubles when the input doubles,
and this rules out systems (b) and (c).
Deﬁnition 1.1–2: Shift Invariance
A system T is shift-invariant if for an arbitrary input x, any ﬁnite shift of x produces the
identical shift in the corresponding output y. That is, if T[x(n1,n2)] = y(n1,n2), then for all
(integer) shifts (m1,m2) we have T[x(n1 −m1,n2 −m2)] = y(n1 −m1,n2 −m2).
Often we think in terms of a shift vector, denoted m = (m1,m2)T. In fact, we can
just as well write the preceding two deﬁnitions in the more compact vector notation as
linearity: L[a1x1(n) + a2x2(n)] = a1L[x1(n)] + a2L[x2(n)]
shift invariance: T[x(n −m)] = y(n −m) for all integer shift vectors m
The vector notation is very useful for 3-D systems, such as those occurring in video
signal processing (see Chapter 11).
2-D Convolution
Shift invariant linear systems can be represented by convolution. If a system is linear
shift-invariant (LSI), then we can write, using the shifting representation (1.1–1),
y(n1,n2) = L

X
k1,k2
x(k1,k2)δ(n1 −k1,n2 −k2)



1.1 Two-Dimensional Signals
11
=
X
k1,k2
x(k1,k2)L[δ(n1 −k1,n2 −k2)]
=
X
k1,k2
x(k1,k2)h(n1 −k1,n2 −k2),
where the sequence h is called the LSI system’s impulse response, deﬁned as
h(n1,n2) ≜L[δ(n1,n2)]. We deﬁne the 2-D convolution operator ∗as
(x ∗h)(n1,n2) ≜
X
k1,k2
x(k1,k2)h(n1 −k1,n2 −k2).
(1.1–2)
It then follows that for a 2-D LSI system we have
y(n1,n2) = (x ∗h)(n1,n2)
= (h ∗x)(n1,n2),
where the latter equality follows easily by substitution in (1.1–2). Again, we remind
that all the summations over k1,k2 range from −∞to +∞.
Properties of 2-D Convolution or Convolution Algebra
1. Commutativity: x ∗y = y ∗x
2. Associativity: (x ∗y) ∗z = x ∗(y ∗z)
3. Distributivity: x ∗(y + z) = x ∗y + x ∗z
4. Identity element: δ(n1,n2) with property δ ∗x = x
5. Zero element: 0(n1,n2) = 0 with property 0 ∗x = 0 2
All of these properties of convolution hold for any 2-D signals x, y, and z, for which
convolution is deﬁned (i.e., for which the inﬁnite sums exist).
Example 1.1–6: Spatial Convolution
In Figure 1.1–9, we see an example of 2-D or spatial convolution. The impulse response
h has been reversed or reﬂected through the origin in both parameters to yield h(n1 −k1,
n2 −k2), shown in this k1 × k2 plane ﬁgure for n1 = n2 = 0. This sequence is then shifted
around the k1 × k2 plane via integer vector shifts (n1,n2), and then a sum of products is
taken with input x(k1,k2) to give output y(n1,n2).
In general, for signals with rectangular support, we have the following result for
the support of their convolution:
If supp(x) = N1 × N2 and supp(h) = M1 × M2,
then supp(y) = (N1 + M1 −1) × (N2 + M2 −1).
2Note that 0 here is the zero sequence deﬁned as 0(n1,n2) = 0 for all (n1,n2).

12
CHAPTER 1 Two-Dimensional Signals and Systems
h
k2
k1
x
k2
k1
n2
n1
y
FIGURE 1.1–9
An example of 2-D or spatial convolution.
For signals of non rectangular support, this result can be used to rectangularly bound
the support of the output.
Stability in 2-D Systems
Stable systems are those for which a small change in the input gives a small change
in the output. As such, they are very useful in applications. We can mathematically
deﬁne bounded-input bounded-output (BIBO) stability for 2-D systems analogously
to that in 1-D system theory. A spatial or 2-D system will be stable if the response
to every uniformly bounded input is itself uniformly bounded. It is generally very
difﬁcult to verify this condition, but for an LSI system the condition is equivalent to
the impulse response being absolutely summable; that is,
X
k1,k2
|h(k1,k2)| < ∞.
(1.1–3)
Theorem 1.1–1: LSI Stability
A 2-D LSI system is BIBO stable if and only if its impulse response h(n1,n2) is absolutely
summable.
Proof We start by assuming that (1.1–3) is true. Then, by the convolution representation,
we have
|y(n1,n2)| =

X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2)


1.2 2-D Discrete-Space Fourier Transform
13
≤
X
k1,k2
|h(k1,k2)||x(n1 −k1,n2 −k2)|
≤

X
k1,k2
|h(k1,k2)|

max|x(n1,n2)| < ∞,
for any uniformly bounded input x, thus establishing sufﬁciency of (1.1–3). To show neces-
sity, following Oppenheim and Schafer [1] in the one-dimensional case, we can choose the
uniformly bounded input signal x(n1,n2) = exp−jθ(−n1,−n2), where θ(n1,n2) is the argu-
ment function of the complex function h(n1,n2); then the system output at (n1,n2) = (0,0)
will be given by (1.1–3), thus showing that absolute summability of the impulse response
is also necessary for a bounded output. Having shown both sufﬁciency and necessity, we
are done.
In mathematics, the function space of absolutely summable 2-D sequences is often
denoted l1, so we can also write h ∈l1 as the condition for an LSI system with impulse
response h to be BIBO stable. Thus, this well-known 1-D mathematical result [1]
easily generalizes to the 2-D case.
1.2 2-D DISCRETE-SPACE FOURIER TRANSFORM
The Fourier transform is important in 1-D signal processing because it effectively
explains the operation of linear time-invariant (LTI) systems via the concept of fre-
quency response. This frequency response is just the Fourier transform of the system
impulse response. While convolution provides a complicated description of the LTI
system operation, where generally the input at all locations n affects the output at all
locations, the frequency response provides a simple interpretation as a scalar weight-
ing in the Fourier domain, where the output at each frequency ω depends only on the
input at that same frequency. A similar result holds for 2-D systems that are LSI, as
we show in the following discussions.
Deﬁnition 1.2–1: 2-D Fourier Transform
We deﬁne the 2-D Fourier transform as follows:
X(ω1,ω2) ≜
+∞
X
n1,n2
−∞
x(n1,n2)exp−j(ω1n1 + ω2n2).
(1.2–1)
The radian frequency variable ω1 is called horizontal frequency, and the variable ω2
is called vertical frequency. The domain of (ω1,ω2) is the entire plane (−∞,+∞) ×
(−∞,+∞) ≜(−∞,+∞)2.

14
CHAPTER 1 Two-Dimensional Signals and Systems
One of the easy properties of the 2-D Fourier transform is that it is periodic with
rectangular period 2π × 2π, a property originating from the integer argument values
of n1 and n2. To see this property simply note
X(ω1 ± 2π,ω2 ± 2π)
=
+∞
X
n1,n2
−∞
x(n1,n2)exp−j[(ω1 ± 2π)n1 + (ω2 ± 2π)n2]
=
+∞
X
n1,n2
−∞
x(n1,n2)exp−jω1n1 exp−j2πn1 exp−jω2n2 exp−j2πn2
=
+∞
X
n1,n2
−∞
x(n1,n2)(exp−jω1n1) · 1 · (exp−jω2n2) · 1
= X(ω1,ω2).
Thus the 2-D Fourier transform only needs be calculated for one period, usually
taken to be [−π,+π] × [−π,+π]. It is analogous to the 1-D case where the Fourier
transform X(ω) has period 2π and is usually just calculated for [−π,+π]. Upon close
examination, we can see that the 2-D Fourier transform is closely related to the 1-D
Fourier transform. In fact, we can rewrite (1.2–1) as
X(ω1,ω2) =
X
n1
X
n2
x(n1,n2)exp−j(ω1n1 + ω2n2)
=
X
n2
X
n1
x(n1,n2)exp−j(ω1n1 + ω2n2)
=
X
n2
"X
n1
x(n1,n2)exp−jω1n1
#
exp−jω2n2
=
X
n2
X(ω1;n2)exp−jω2n2,
where X(ω1;n2) ≜P
n1 x(n1,n2)exp−jω1n1, the Fourier transform of row n2. In
words, we say that the 2-D Fourier transform can be decomposed as a set of 1-
D Fourier transforms on all the rows of x, followed by another set of 1-D Fourier
transforms on the columns X(ω1;n2) that result from the ﬁrst set of transforms. We
call the 2-D Fourier transform a separable operator, because it can be performed as
the concatenation of 1-D operations on the rows followed by 1-D operations on the
columns, or vice versa. Such 2-D operators are common in MDSP and offer great
computational simpliﬁcation when they occur.

1.2 2-D Discrete-Space Fourier Transform
15
The Fourier transform has been studied for convergence of the inﬁnite sum in
several senses. First, if we have an absolutely sumable signal, then the Fourier
transform sum will converge in the sense of uniform convergence; as a result, it
will be a continuous function of ω1 and ω2. A weaker form of convergence of the
Fourier transform sum is mean-square convergence [2]. This is the type of con-
vergence that leads to a Fourier transform that is a discontinuous function. A third
type of convergence is as a generalized function, such as δ(ω1,ω2), the 2-D Dirac
delta function. It is used for periodic signals such as cos2π(α1n1 + α2n2), whose
Fourier transform will be shown to be the scaled and shifted 2-D Dirac impulse
(2π)2δ(ω1 −α1,ω2 −α2) when |α1| < π and |α2| < π, and when attention is limited
to the unit cell [−π,+π] × [−π,+π]. Outside this cell, the function must repeat.
In operator notation, we can write the Fourier transform relation as
X = FT[x],
indicating that the Fourier transform operator FT maps 2-D sequences into 2-D func-
tions X that happen to be continuous-parameter and periodic functions with period
2π × 2π.
Example 1.2–1: Fourier Transform of Rectangular Pulse Function
Let
x(n1,n2) =
(
1
0 ≤n1 ≤N1 −1, 0 ≤n2 ≤N2 −1
0
else .
Then its 2-D Fourier transform can be determined as
X(ω1,ω2) =
N1−1
X
n1=0
N2−1
X
n2=0
1 exp−j(ω1n1 + ω2n2)
=


N1−1
X
n1=0
exp−jω1n1




N2−1
X
n2=0
exp−ω2n2


=
1 −e−jω1N1
1 −e−jω1
1 −e−jω2N2
1 −e−jω2

= e−jω1(N1−1)/2 sinω1N1/2
sinω1/2 e−jω2(N2−1)/2 sinω2N2/2
sinω2/2
= e−j[ω1(N1−1)/2+ω2(N2−1)/2] sinω1N1/2
sinω1/2
sinω2N2/2
sinω2/2 ,
where the second to last line follows by factoring out the indicated phase term from each
of the two factors in the line above, and then using Euler’s equality sinθ = 1
2j(ejθ −e−jθ)
in top and bottom terms of each factor. We see the result is just the product of two

16
CHAPTER 1 Two-Dimensional Signals and Systems
1-D Fourier transforms for this separable and rectangular 2-D pulse function. If N1 and
N2 are odd numbers, we can shift the pulse to be centered on the origin (0,0) and
thereby remove the linear phase shift term out in front, corresponding to a shift (delay)
of

1
2(N1 −1), 1
2(N2 −1)

. However, because of the way we have written the signal x as
starting at (0,0), there remains this linear phase shift. A contour plot of the log magnitude
of this function is provided in Figure 1.2–1, and the corresponding 3-D perspective plot is
shown in Figure 1.2–2. Both of these plots were produced with MATLAB.
Example 1.2–2: Fourier Transform of Line Impulse
Consider a line impulse of the form
x(n1,n2) = δ(n1 −n2) =
(
1,
n2 = n1,
0,
else,
which is a line of slope 1 in the n1 × n2 plane or an angle of 45◦. We can take the Fourier
transform as
X(ω1,ω2) =
X
n1,n2
δ(n1 −n2)exp−j(ω1n1 + ω2n2)
(1.2–2)
=
+∞
X
n1=−∞
exp−jω1n1 exp−jω2n1
=
+∞
X
n1=−∞
exp−j(ω1 + ω2)n1
= 2πδ(ω1 + ω2)
for (ω1,ω2) ∈[−π,+π]2,
(1.2–3)
which is a Dirac impulse line in the 2-D frequency domain, along the line ω2 = −ω1.
Thus the Fourier transform of an impulse line is a Dirac impulse line in the 2-D frequency
domain. Note that the angle of this line is that of the spatial domain line plus/minus 90◦;
that is, the two lines are perpendicular to one another. Why is this result reasonable?
Inverse 2-D Fourier Transform
The inverse 2-D Fourier transform is given as
x(n1,n2) =
1
(2π)2
Z
[−π,+π]×[−π,+π]
X(ω1,ω2)exp+ j(ω1n1 + ω2n2)dω1dω2.
(1.2–4)
Note that we only integrate over a “unit” cell [−π,+π] × [−π,+π] in the ω1 × ω2
frequency domain. To justify this formula, we can plug in (1.2–1) and interchange

1.2 2-D Discrete-Space Fourier Transform
17
−0.3
−0.2
−0.1
0
ω1
0.1
0.2
0.3
0.3
0.2
0.1
0
−0.1
−0.2
−0.3
ω2
FIGURE 1.2–1
Zoomed-in contour plot of log magnitude of 2-D rectangular sinc function with
N1 = N2 = 50.
FIGURE 1.2–2
3-D perspective plot of log magnitude of 2-D rectangular sinc function.

18
CHAPTER 1 Two-Dimensional Signals and Systems
the order of summation and integration, just as done in the 1-D case. Alternatively,
we can use the fact that the 2-D FT is a separable operator and just use the known
results for transform and inverse in the 1-D case to arrive at this same answer for the
inverse 2-D Fourier transform. In operator notation, we denote the inverse Fourier
transform as IFT and write
x = IFT[X].
A 2-D proof of this result follows closely the 1-D result [1]. First, we insert
(1.2–1) into (1.2–4) to obtain
x(n1,n2) =
1
(2π)2
Z
[−π,+π]×[−π,+π]


+∞
X
l1,l2
−∞
x(l1,l2)exp−j(ω1l1 + ω2l2)


× exp+j(ω1n1 + ω2n2)dω1dω2
=
X
l1,l2
x(l1,l2)



1
(2π)2
Z
[−π,+π]×[−π,+π]
×exp+ j[ω1(n1 −l1) + ω2(n2 −l2)]dω1dω2



=
X
l1,l2
x(l1,l2)




1
2π
Z
[−π,+π]
exp+ jω1(n1 −l1)dω1


×

1
2π
Z
[−π,+π]
exp+ jω2(n2 −l2)dω2





.
Now,

1
2π
Z
[−π,+π]
exp+ jωm dω1

= exp+ jωm
j2πm

+π
−π
= sinπm
πm
= δ(m),
so substituting this result in the above equation yields
x(n1,n2) =
X
l1,l2
x(l1,l2)δ(l1)δ(l2)
= x(n1,n2)
as was to be shown.

1.2 2-D Discrete-Space Fourier Transform
19
A main application of 2-D Fourier transforms is to provide a simpliﬁed view of
spatial convolution as used in linear ﬁltering, our next topic.
Fourier Transform of 2-D or Spatial Convolution
Theorem 1.2–1: Fourier Convolution Theorem
If
y(n1,n2) = h(n1,n2) ∗x(n1,n2),
then
Y(ω1,ω2) = H(ω1,ω2)X(ω1,ω2).
Proof
Y(ω1,ω2) =
+∞
X
n1,n2
−∞
y(n1,n2)exp−j(ω1n1 + ω2n2)
=
X
n1,n2

X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2)

exp−j(ω1n1 + ω2n2)
=
X
n1,n2
X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2)
× exp−j{ω1 [k1 + (n1 −k1)] + ω2 [k2 + (n2 −k2)]}
=
X
n1,n2
X
k1,k2
h(k1,k2)exp−j(ω1k1 + ω2k2)x(n1 −k1,n2 −k2)
× exp−j[ω1(n1 −k1) + ω2(n2 −k2)]
=
X
k1,k2
h(k1,k2)exp−j(ω1k1 + ω2k2)
×



X
n1,n2
x(n1 −k1,n2 −k2)exp−j[ω1(n1 −k1) + ω2(n2 −k2)]



=
X
k1,k2
h(k1,k2)exp−j(ω1k1 + ω2k2)


X
n′
1,n′
2
x(n′
1,n′
2)exp−j(ω1n′
1 + ω2n′
2)

,
with n′
1 ≜n1 −k1 and n′
2 ≜n2 −k2.

20
CHAPTER 1 Two-Dimensional Signals and Systems
Now, since the limits of the sums are inﬁnite, the shift by any ﬁnite value (k1,k2)
makes no difference, and the limits on the inside sum over (n′
1,n′
2) remain at (−∞,+∞) ×
(−∞,+∞). Thus we can bring its double sum outside the sum over (k1,k2) and recog-
nize it as X(ω1,ω2), the Fourier transform of the input signal x. What is left inside is the
frequency response H(ω1,ω2), and so we have ﬁnally
=

X
k1,k2
h(k1,k2)exp−j(ω1k1 + ω2k2)




X
n′
1,n′
2
x(n′
1,n′
2)exp−j(ω1n′
1 + ω2n′
2)


= H(ω1,ω2) · X(ω1,ω2)
as was to be shown.
Since we have seen that a 2-D or spatial LSI system is characterized by its impulse
response h(n1,n2), we now see that its frequency response H(ω1,ω2) also sufﬁces to
characterize such a system. And the Fourier transform Y of the output equals the prod-
uct of the frequency response H and the Fourier transform X of the input. When the
frequency response H takes on only values 1 and 0, we call the system an ideal ﬁlter,
since such an LSI system will ﬁlter out some frequencies and pass others unmodi-
ﬁed. More generally, the term ﬁlter has grown to include all such LSI systems, and
has been extended to shift-variant and even nonlinear systems through the concept of
the Voltera series of operators [3].
Example 1.2–3: Fourier Transform of Complex Plane Wave
Let x(n1,n2) = Aexpj(ω0
1n1 + ω0
2n2) ≜e(n1,n2), where
ω0
i
 < π; then E(ω1,ω2) = cδ(ω1 −
ω0
1,ω2 −ω0
2) in the basic square [−π,+π]2. Finding the constant c by inverse Fourier
transform, we conclude that c = (2π)2A. Inputting this plane wave into an LSI system
or ﬁlter with frequency response H(ω1,ω2), we obtain the Fourier transform output
signal Y(ω1,ω2) = (2π)2AH(ω0
1,ω0
2)δ(ω1 −ω0
1,ω2 −ω0
2) = H(ω0
1,ω0
2)E(ω0
1,ω0
2), or in the
space domain y(n1,n2) = H(ω0
1,ω0
2)e(n1,n2), thus showing that complex exponentials
(i.e., plane waves) are the eigenfunctions of spatial LSI systems, and the frequency
response H evaluated at the plane wave frequency (ω0
1,ω0
2) becomes the corresponding
eigenvalue.
Some Important Properties of the FT Operator
1. Linearity: ax + by ⇔aX(ω1,ω2) + bY(ω1,ω2)
2. Convolution: x ∗y ⇔X(ω1,ω2)Y(ω1,ω2)
3. Multiplication: xy ⇔(X ∗Y)(ω1,ω2) ≜
1
(2π)2
R +π
−π
R +π
−π X(ν1,ν2)
Y(ω1 −ν1,ω2 −ν2)dν1dν2
4. Modulation: x(n1,n2)expj(ν1n1 + ν2n2) ⇔X(ω1 −ν1,ω2 −ν2) for integers
ν1 and ν2
5. Shift (delay): x(n1 −m1,n2 −m2) ⇔X(ω1,ω2)exp−j(ω1m1 + ω2m2)

1.2 2-D Discrete-Space Fourier Transform
21
6. Partial differentiation in frequency domain:
−jn1x(n1,n2) ⇔∂X
∂ω1
−jn2x(n1,n2) ⇔∂X
∂ω2
7. “Initial” value:
x(0,0) =
1
(2π)2
Z
[−π,+π]×[−π,+π]
X(ω1,ω2)dω1dω2
8. “DC” value:
X(0,0) =
+∞
X
n1,n2
−∞
x(n1,n2)
9. Parseval’s theorem:
X
n1,n2
x(n1,n2)y∗(n1,n2) =
1
(2π)2
Z
[−π,+π]×[−π,+π]
X(ω1,ω2)Y∗(ω1,ω2)dω1dω2
with the “power” version, obtained by letting y = x
X
n1,n2
|x(n1,n2)|2 =
1
(2π)2
Z
[−π,+π]×[−π,+π]
|X(ω1,ω2)|2 dω1dω2
10. Separable signal:
x1(n1)x2(n2) ⇔X1(ω1)X2(ω2)
Some Useful Fourier Transform Pairs
1. Constant c:
FT{c} = (2π)2c δ(ω1,ω2)
in the unit cell [−π,+π]2
2. Complex exponential for spatial frequency (ν1,ν2) ∈[−π,+π]2:
FT{expj(ν1n1 + ν2n2)} = (2π)2δ(ω1 −ν1,ω2 −ν2)
in the unit cell [−π,+π]2
3. Constant in n2 dimension:
FT{x1(n1)} = 2πX1(ω1)δ(ω2),
where X1(ω1) is a 1-D Fourier transform and δ(ω2) is a 1-D impulse function
4. Ideal lowpass ﬁlter (rectangular passband):
Hr(ω1,ω2) = Iωc(ω1)Iωc(ω2),
where Iωc ≜
1
|ω| ≤ωc,
0
else,

22
CHAPTER 1 Two-Dimensional Signals and Systems
with ωc the cutoff frequency of the ﬁlter. Necessarily ωc ≤π. The function Iωc is
sometimes called an indicator function, since it indicates the passband. Taking the
inverse 2-D Fourier transform of this separable function, we proceed as follows
to obtain the ideal impulse response:
hr(n1,n2) =
1
(2π)2
Z
[−π,+π]×[−π,+π]
Hr(ω1,ω2)exp + j(ω1n1 + ω2n2)dω1dω2
=

1
2π
Z
[−π,+π]
Iωc(ω1)exp+ jω1n1dω1


×

1
2π
Z
[−π,+π]
Iωc(ω2)exp+ jω2n2dω2


= sinωcn1
πn1
· sinωcn2
πn2
,
−∞< n1,n2 < +∞.
5. Ideal lowpass ﬁlter (circular passband) ωc < π
Hc(ω1,ω2) =
(
1
q
ω2
1 + ω2
2 ≤ωc,
0
else,
for (ω1,ω2) ∈[−π,+π] × [−π,+π].
The inverse Fourier transform of this circular symmetric frequency response can be
represented as the integral
hc(n1,n2) =
1
(2π)2
Z
Z
q
ω2
1+ω2
2≤ωc
1 exp+ j(ω1n1 + ω2n2) dω1dω2
=
1
(2π)2
ωc
Z
0
+π
Z
−π
exp+ ju(n1 cosθ + n2 sinθ) ududθ
=
1
(2π)2
ωc
Z
0
u



+π
Z
−π
exp+j[urcos(θ −φ)] dθ


du
=
1
(2π)2
ωc
Z
0
u


+π
Z
−π
exp+ j(urcosθ) dθ

du,
where we have used, ﬁrst, polar coordinates in frequency, ω1 = ucosθ and ω2 =
usinθ, and then in the next line, polar coordinates in space, n1 = rcosφ and n2 =
rsinφ. Finally, the last line follows because the integral over θ does not depend on φ
since it is an integral over the full period 2π. The inner integral over θ can now be

1.2 2-D Discrete-Space Fourier Transform
23
recognized in terms of the zeroth-order Bessel function of the ﬁrst kind J0(x), with
integral representation [4, 5]
J0(x) = 1
2π
+π
Z
−π
cos(xcosθ) dθ = 1
2π
+π
Z
−π
cos(xsinθ) dθ.
To see this, we note the integral
+π
Z
−π
exp+ j(urcosθ) dθ =
+π
Z
−π
cos(urcosθ) dθ,
since the imaginary part, via Euler’s formula, is an odd function integrated over even
limits, and hence is zero. So, continuing, we can write
hc(n1,n2) =
1
(2π)2
ωc
Z
0
u


+π
Z
−π
exp+ j(urcosθ) dθ

du
= 1
2π
ωc
Z
0
uJ0(ur)du
= ωc
2πrJ1(ωcr)
=
ωc
2π
q
n2
1 + n2
2
J1(ωc
q
n2
1 + n2
2),
where J1 is the ﬁrst-order Bessel function of the ﬁrst kind J1(x), satisfying the known
relation [6, p. 484]
xJ1(x) =
x
Z
0
uJ0(u)du.
Comparing these two ideal lowpass ﬁlters, one with a square passband and the
other circular with diameter equal to a side of the square, we get the two impulse
responses along the n1 axis:
hr(n1,0) = ωc
π
sinωcn1
πn1
and
hc(n1,0) =
ωc
2πn1
J1(ωcn1).
These 1-D sequences are plotted via MATLAB in Figure 1.2–3. Note their
similarity.

24
CHAPTER 1 Two-Dimensional Signals and Systems
0
5
10
15
20
25
30
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
FIGURE 1.2–3
Two impulse responses of 2-D ideal lowpass ﬁlters. The solid curve is “rectangular” and the
dashed curve is “circular.”
Example 1.2–4: Fourier Transform of Separable Signal
Let x(n1,n2) = x1(n1)x2(n2), then when we compute the Fourier transform, the following
simpliﬁcation arises:
X(ω1,ω2) =
X
n1
X
n2
x(n1,n2)exp−j(ω1n1 + ω2n2)
=
X
n1
X
n2
x1(n1)x2(n2)exp−j(ω1n1 + ω2n2)
=
X
n1
x1(n1)

X
n2
x2(n2)exp−jω2n2

exp−jω1n1
=

X
n1
x1(n1)exp−jω1n1



X
n2
x2(n2)exp−jω2n2


= X1(ω1)X2(ω2).

1.2 2-D Discrete-Space Fourier Transform
25
A consequence of this result is that in 2-D signal processing, multiplication in the spatial
domain does not always lead to convolution in the frequency domain! Can you reconcile
this fact with the basic 2-D Fourier transform property 3? (See problem 9 at the end of this
chapter.)
Symmetry Properties of the Fourier Transform
Now we will consider some symmetry properties of the Fourier transform of complex
signal x(n1,n2). We start with the original FT pair:
x(n1,n2) ⇔X(ω1,ω2).
First, reﬂection through the origin (a generalization of time reversal in the 1-D
case): We ask, what is the FT of the signal x(−n1,−n2), where each of the axes are
reversed? For an image, this corresponds to ﬂipping it right to left, and then ﬂipping
it top to bottom. We seek
X
n1
X
n2
x(−n1,−n2)exp−j(ω1n1 + ω2n2)
=
X
n′
1
X
n′
2
x(n′
1,n′
2)exp−j[ω1(−n′
1) + ω2(−n′
2)]
upon setting n′
1 = −n1 and n′
2 = −n2, so moving the minus signs to the ωi terms, we
have
X
n′
1
X
n′
2
x(n′
1,n′
2)exp−j[(−ω1)n′
1 + (−ω2)n′
2],
which can be seen as X(−ω1,−ω2). Thus we have the new transform pair
x(−n1,−n2) ⇔X(−ω1,−ω2).
If the original spatial signal is instead conjugated, we have
x∗(n1,n2) ⇔X∗(−ω1,−ω2),
(1.2–5)
which generalizes the 1-D FT pair x∗(n) ⇔X∗(−ω). Combining these two 2-D
transform pairs, we get the transform of x∗(−n1,−n2),
x∗(−n1,−n2) ⇔X∗(ω1,ω2),
resulting in the conjugate of the original X(ω1,ω2). We can organize these four
basic transform pairs using the concept of conjugate symmetric and conjugate
antisymmetric parts, as in one dimension [1].
Deﬁnition 1.2–2: Signal Symmetries
The conjugate symmetric part of x is
xe(n1,n2) ≜1
2

x(n1,n2) + x∗(−n1,−n2)

.

26
CHAPTER 1 Two-Dimensional Signals and Systems
The conjugate antisymmetric part of x is
xo(n1,n2) ≜1
2

x(n1,n2) −x∗(−n1,−n2)

.
From Deﬁnition 1.2–2, we get the following two transform pairs:
xe(n1,n2) ⇔ReX(ω1,ω2),
xo(n1,n2) ⇔jImX(ω1,ω2).
Similarly, the conjugate symmetric and antisymmetric parts in the 2-D frequency are
deﬁned.
Deﬁnition 1.2–3:
The conjugate symmetric part of X is
Xe(ω1,ω2) ≜1
2

X(ω1,ω2) + X∗(−ω1,−ω2)

.
The conjugate antisymmetric part of X is
Xo(ω1,ω2) ≜1
2

X(ω1,ω2) −X∗(−ω1,−ω2)

.
Using these symmetry properties, we get the following two general transform
pairs:
Rex(n1,n2) ⇔Xe(ω1,ω2),
jImx(n1,n2) ⇔Xo(ω1,ω2).
Symmetry Properties of Real-valued Signals
There are special properties of great interest for real valued signals; that is,
x(n1,n2) = Rex(n1,n2), which implies that Xo(ω1,ω2) = 0. Thus when the signal x
is real valued, we must have
X(ω1,ω2) = X∗(−ω1,−ω2).
(1.2–6)
Directly from this equation, we get the following transform pairs, or symme-
try properties, by taking the real, imaginary, magnitude, and phase of both sides of
(1.2–6) and setting the results equal:
ReX(ω1,ω2) = ReX(−ω1,−ω2), read “real part is even”
ImX(ω1,ω2) = −ImX(−ω1,−ω2), read “imaginary part is odd”
|X(ω1,ω2)| = |X(−ω1,−ω2)|, read “magnitude is even”
argX(ω1,ω2) = −argX(−ω1,−ω2), read “argument (phase) may be taken as odd”
Note that “even” here really means symmetric through the origin—i.e., x(n1,n2) =
x(−n1,−n2). Combining results, we see that the 2-D FT of an even real valued signal
will be real and even, and this condition is necessary also.

1.2 2-D Discrete-Space Fourier Transform
27
Continuous-Space Fourier Transform
While this course focuses on digital image and video, we need to be aware of the
generalization of continuous-time Fourier transforms to two and higher dimensions.
Here, we look at the 2-D continuous-parameter Fourier transform, with application to
continuous-space images (e.g., ﬁlm). We will denote the two continuous parameters
as t1 and t2, but time is not the target here, although one of the two parameters could
be time (e.g., acoustic array processing in geophysics [7]). We could equally have
used the notation x1 and x2 to denote these free parameters. We have chosen t1 and t2
so we can use x for signal.
2-D Fourier Continuous Transform
We start with a continuous-parameter function of two dimensions t1 and t2, prop-
erly called a bi-function, and herein denoted fc(t1,t2) deﬁned over −∞< t1 <
+∞,−∞< t2 < +∞. We write the corresponding 2-D Fourier transform as the
double integral
Fc(1,2) ≜
+∞
Z
−∞
+∞
Z
−∞
fc(t1,t2)exp−j(1t1 + 2t2) dt1dt2.
We recognize that this FT operator is separable and consists of the 1-D FT repeatedly
applied in the t1 domain for each ﬁxed t2, followed by the 1-D FT repeatedly applied
in the t2 domain for each ﬁxed 1, ﬁnally culminating in the value Fc(1,2). Twice
applying the inverse 1-D Fourier theory, once for 1 and once for 2, we arrive at
the inverse continuous-parameter Fourier transform,
fc(t1,t2) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
Fc(1,2)exp+ j(1t1 + 2t2) d1d2,
assuming only that the various integrals converge, meaning they are well deﬁned
in some sense. One very useful property of the FT operator on such functions is the
so-called rotation theorem, which states that the FT of any rotated version of the orig-
inal function fc(t1,t2) is just the rotated FT—i.e., the corresponding rotation operator
applied to Fc(1,2). We call such an operator rotationally invariant. Notationally,
we can economically denote such rotations via the matrix equation
t′ ≜Rt,
(1.2–7)
where the vectors are t ≜(t1,t2)T, t′≜(t′
1,t′
2)T, and the rotation matrix R is given in
terms of the rotation angle θ as
R ≜
 cosθ
sinθ
−sinθ
cosθ

,
with the sense deﬁned in Figure 1.2–4.

28
CHAPTER 1 Two-Dimensional Signals and Systems
t1
t′1
t′2
t2
θ
0
FIGURE 1.2–4
Coordinate system t′ is rotated from coordinate system t by rotation angle +θ.
Theorem 1.2–2: Rotational Invariance of FT
The continuous-parameter Fourier transform FT is rotationally invariant; that is, if gc(t) ≜
fc(Rt) with R a rotation matrix, then Gc() = Fc(R).
Proof We start at
gc(t) ≜fc(Rt)
= fc(t1 cosθ + t2 sinθ,−t1 sinθ + t2 cosθ),
which corresponds to a rotation of axes by +θ degrees.3 Then its Fourier transform is
given by
Gc(1,2) =
+∞
Z
−∞
+∞
Z
−∞
gc(t1,t2)exp−j(1t1 + 2t2) dt1dt2
=
+∞
Z
−∞
+∞
Z
−∞
gc(t)exp−j(ΩTt) dt
=
+∞
Z
−∞
+∞
Z
−∞
fc(Rt)exp−j(ΩTt) dt,
3While the axes are rotated by +θ in going from t to t′, the function gc(t) = fc(Rt) = fc(t′), and so the
function gc is a rotated version of fc with rotation −θ.

1.2 2-D Discrete-Space Fourier Transform
29
where Ω≜(1,2)T. Then we use (1.2–7) to rotate coordinates to get
Gc(1,2) =
+∞
Z
−∞
+∞
Z
−∞
fc(t′)exp−j(ΩTR−1t′) |R−1|dt′,
where |•| denotes the determinant,
=
+∞
Z
−∞
+∞
Z
−∞
fc(t′)exp−j(ΩTRTt′) |R−1|dt′,
since the inverse of R is just its transpose RT,
=
+∞
Z
−∞
+∞
Z
−∞
fc(t′)exp−j{(RΩ)Tt′} dt′,
and the determinant of R is one,
= Fc(RΩ)
= Fc(1 cosθ + 2 sinθ,−1 sinθ + 2 cosθ)
= Fc(Ω′),
upon setting Ω′ ≜RΩ. This completes the proof.
Projection-Slice Theorem
In the medical/industrial 3-D imaging technique, computed tomography (CT), two-
dimensional slices are constructed from a series of X-ray images taken on an arc.
A key element in this approach is the projection-slice theorem presented here. First,
we deﬁne the Radon transform.
Deﬁnition 1.2–4: Radon Transform and Projection
We deﬁne the Radon transform for angles, −π ≤θ ≤+π, as
pθ(t) ≜
+∞
Z
−∞
fc(tcosθ −usinθ,tsinθ + ucosθ)du,
where fc(t,u) is an integrable and continuous function deﬁned over all of R2.
Using the rotation matrix R and writing it as a function of θ, we have R = R(θ)
and can express the radon transform as
pθ(t) =
+∞
Z
−∞
fc

R(θ)
t
u

du.
We can then state the projection-slice theorem.

30
CHAPTER 1 Two-Dimensional Signals and Systems
Theorem 1.2–3: Projection-Slice
Let continuous function fc(t,u) have Fourier transform Fc(1,2). Let the Radon trans-
form of fc be deﬁned and given as pθ(t) with 1-D Fourier transform Pθ() for each angle
θ. Then we have the equality
Pθ() = Fc(cosθ,sinθ).
(1.2–8)
Proof By the rotational invariance of the continuous FT operator, we only have to show
(1.2–8) for θ = 0. So
P0() = FT[p0(t)]
=
+∞
Z
−∞
[
+∞
Z
−∞
fc(t,u)du]e−jtdt
= Fc(,0),
which agrees with (1.2–8) as was to be shown.
In words, we can sweep out the full 2-D spatial transform Fc in terms of the 1-D
frequency components Pθ of the radon transform at all angles θ ∈[−π,+π].
CONCLUSIONS
This chapter has introduced 2-D or spatial signals and systems. We looked at how the
familiar step and impulse functions of 1-D digital signal processing (DSP) general-
ize to the 2-D case. We then extended the Fourier transform and studied some of its
important properties. We looked at some ideal spatial ﬁlters and other important dis-
crete space Fourier transform pairs. We then turned to the continuous-space Fourier
transform and treated an application in computer tomography, which is understood
using the rotational invariant feature of this transform. The projection-slice theorem
is only approximated in the discrete-space Fourier transform and when the sampling
rate is high. We discuss sampling in the next chapter. A good alternative reference
for these ﬁrst few chapters is Lim [6].
PROBLEMS
1. Plot the following signals:
(a) s(n1,n2) = u(n1,n2)u(5 −n1,n2)
(b) s(n1,n2) = δ(n1 −n2)
(c) s(n1,n2) = δ(n1,n2 −1)
(d) s(n1,n2) = δ(n1 −3n2)u(−n1,2 −n2)

Problems
31
2. Let a given periodic signal satisfy
s(n1,n2) = s(n1 + 5,n2 + 3)
= s(n1 −1,n2 −3) for all n1,n2.
(a) What is the corresponding periodicity matrix N?
(b) Is s periodic also with vectors
n1 =
4
0

and
n2 =
 3
−3

?
3. This problem concerns the period of cosine waves in two dimensions, both in
continuous space and in discrete space:
(a) What is the period in continuous space of the cosine wave
cos2π
4t1 + t2
16

?
(b) What is the period of the cosine wave
cos2π
4n1 + n2
16

?
You may ﬁnd several answers. Which one is best? Why?
4. Convolve the spatial impulse response
h(n1,n2) =
 1
2,
(n1,n2) = (0,0),(1,0),(0,1), or (1,1),
0,
otherwise,
and the input signal
x(n1,n2) =
1,
(n1,n2) ∈[−2,+2] × [−2,+2],
0,
otherwise.
Call the output signal y(n1,n2) and ﬁnd all of its nonzero values along with their
locations.
5. Convolve the spatial impulse response
h(n1,n2) =
 1
4,
(n1,n2) = (0,0),(1,0),(0,1), or (1,1),
0,
otherwise,
and the input signal u++(n1,n2), the ﬁrst quadrant unit step function.
6. Which of the following 2-D systems is LSI?
(a) y(n1,n2) = 3x(n1,n2) −x(n1 −1,n2)
(b) y(n1,n2) = 3x(n1,n2) −y(n1 −1,n2) Any additional information needed?
(c) y(n1,n2) = P
(k1,k2)∈W(n1,n2) x(k1,k2) Here, W(n1,n2) ≜{(n1,n2),
(n1 −1,n2),(n1,n2 −1),(n1 −1,n2 −1)}.

32
CHAPTER 1 Two-Dimensional Signals and Systems
(d) For the same region W(n1,n2), but now
y(n1,n2) =
X
(k1,k2)∈W(n1,n2).
x(n1 −k1,n2 −k2)
What can you say about the stability of each of these systems?
7. Consider the 2-D signal
x(n1,n2) = 4 + 2cos
2π
8 (n1 + n2)

+ 2cos
2π
8 (n1 −n2)

,
for all −∞< n1,n2 < +∞. Find the 2-D Fourier transform of x and give a
labeled sketch of it.
8. Compute and simplify the Fourier transform of
x(n1,n2) = 2sin(ω0
1n1 + ω0
2n2)IN×N(n1,n2),
where IN×N is the indicator function for the N × N square region {0 ≤n1 ≤N −
1,0 ≤n1 ≤N −1}. Simplify your ﬁnal result as much as possible.
9. In general, the Fourier transform of a product of 2-D sequences corresponds
through Fourier transformation to the periodic convolution of their transforms:
x(n1,n2)y(n1,n2) ⇔X(ω1,ω2) ⊛Y(ω1,ω2).
However, if the product is a separable product, then the product of 1-D sequences
corresponds through Fourier transformation to the product of their Fourier
transforms:
x(n1)y(n2) ⇔X(ω1)Y(ω2).
Reconcile these two facts by writing x(n1) and y(n2) as 2-D sequences, taking
their 2-D Fourier transforms, and showing that the resulting periodic convolu-
tion in the 2-D frequency domain reduces to the product of two 1-D Fourier
transforms.
10. Take the inverse Fourier transform of (1.2–3) to check that we obtain the 45◦
impulse line δ(n1 −n2).
11. For a general complex signal x(n1,n2), show that (1.2–5) is correct.
12. Let the signal x(n1,n2) = 1δ(n1,n2) + 0.5δ(n1 −1,n2) + 0.5δ(n1,n2 −1) have
Fourier transform X(ω1,ω2). What is the inverse Fourier transform of
|X(ω1,ω2)|2 ?
13. A certain ideal lowpass ﬁlter with cutoff frequency ωc = π
2 has impulse
response
hd(n1,n2) =
1
q
n2
1 + n2
2
J1
π
2
q
n2
1 + n2
2

.
What is the passband gain of this ﬁlter? (Hint: lim
x→0
J1(x)
x
= 1
2)

Problems
33
14. Consider the ideal ﬁlter with elliptically shaped frequency response
He(ω1,ω2) =

1
(ω1/ωc1)2 + (ω2/ωc2)2 ≤1,
0
else,
in [−π,+π] × [−π,+π],
and ﬁnd the corresponding impulse response he(n1,n2).
15. We know that the ideal impulse response for a circular support lowpass ﬁlter is
given in terms of the Bessel function J1. Here, we consider the generalization
from circle to ellipse.
(a) What is the impulse response he(n1,n2) of an elliptical support lowpass ﬁlter
(see Figure 1.P–1). Assume the major axis cutoff frequency is ωc1 and the
minor axis cutoff frequency is ωc2, where both cutoff frequencies are posi-
tive and less than π. (Hint: Try a transformation of variables on the known
circular solution.)
0
ω1
ω2
+π
+π
−π
−π
−π/2
+π/2
−π/2
+π/2
FIGURE 1.P–1
Ideal lowpass ﬁlter with elliptical support in the frequency domain, with cutoff frequencies
ωc1 and ωc2.
(b) What is the impulse response of the elliptical support lowpass ﬁlter after
rotation by angle θ (see Figure 1.P–2), where θ is taken as the positive
angle between the major axis of the ellipse and the horizontal frequency axis
ω1. Call the resulting impulse response he,θ(n1,n2). (Hint: Consider using a
rotation matrix as in (1.2–7), but rotation in the frequency domain now.)
16. The ideal frequency-domain diamond lowpass ﬁlter with cutoff frequency ωc
satisfying 0 < ωc < π is shown in Figure 1.P–3. Here, the ideal frequency
response H(ω1,ω2) is 1 in the gray region of the ﬁgure and is zero elsewhere
in [−π,+π]2. Find the explicit expression in the spatial domain for this ﬁlter;
i.e., ﬁnd the ideal diamond lowpass ﬁlter impulse response h(n1,n2).

34
CHAPTER 1 Two-Dimensional Signals and Systems
0
ω1
ω2
+π
+π
−π
−π
−π/2
+π/2
−π/2
+π/2
θ
FIGURE 1.P–2
Ideal ﬁlter of Figure 1.P–1 with passband rotated with angle θ as indicated.
ω1
ω2
ωc
ωc
−ωc
−ωc
π
−π
−π
FIGURE 1.P–3
Ideal diamond lowpass ﬁlter with passband cutoff frequency ωc
REFERENCES
[1] A. Oppenheim, R. W. Schafer, and J. R. Buck, Discrete-Time Signal Processing, 2nd Ed.,
Prentice-Hall, Englewood Cliffs, NJ, 1999.
[2] W. Rudin, Real and Complex Analysis, McGraw-Hill, New York, 1966.
[3] S. Thurnhofer and S. K. Mitra, “A General Framework for Quadratic Volterra Filters for
Edge Enhancement,” IEEE Trans. Image Process., vol. 5, June, pp. 950–963, 1996.
[4] F. B. Hildebrand, Advanced Calculus for Applications, pp. 254–5, Prentice-Hall, Engle-
wood Cliffs, NJ, 1962.

References
35
[5] M. Abramowitz and I. A. Stegun, Handbook of Mathematical Functions, p. 360 (9.1.18)
and p. 484 (11.3.20), Dover, NY, 1965.
[6] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
[7] D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing,
Prentice-Hall, Englewood Cliffs, NJ, 1983.

CHAPTER
Sampling in Two
Dimensions
2
In two- and higher-dimensional signal processing, sampling and the underlying
continuous-space play a bigger role than in 1-D signal processing. This is due to the
fact that some digital images, and many digital videos, are undersampled. When work-
ing with motion in video, half-pixel accuracy is often the minimal accuracy needed.
Also there is a variety of sampling possible in space that does not exist in the case of the
1-D time axis. We start out with the so-called rectangular sampling theorem, and then
move on to more general but still regular sampling patterns and their corresponding
sampling theorems. An example of a regular nonrectangular grid is the hexagonal array
of light-receptive cones on the human retina. In spatiotemporal processing, diamond
sampling is used in interlaced video formats, where one dimension is the vertical image
axis and the second dimension is time. Two-dimensional continuous-space Fourier
transform theory is often applied in the study of lens systems in optical devices (e.g.,
in cameras and projectors), wherein the optical intensity ﬁeld at a distance of one focal
length from the lens is approximated well by the Fourier transform. This study is called
Fourier optics, a basic theory used in the design of lenses.
2.1 SAMPLING THEOREM—RECTANGULAR CASE
We assume that the continuous-space function xc(t1,t2) is given. It could correspond
to a ﬁlm image or some other continuous spatial data (e.g., a focused image through
a lens system). We can think of the axes t1 and t2 as being orthogonal, but that is not
necessary. We proceed to produce samples via the rectangular (orthogonal) sampling
pattern
t1 = n1T1
and
t2 = n2T2,
thereby producing the discrete-space data
x(n1,n2) ≜xc(t1,t2)|t1=n1T1,t2=n2T2.
We call this process rectangular sampling.
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00002-3
c⃝2012 Elsevier Inc. All rights reserved.
37

38
CHAPTER 2 Sampling in Two Dimensions
Theorem 2.1–1: Rectangular Sampling
Let x(n1,n2) ≜xc(t1,t2)|t1=n1T1,t2=n2T2, with T1,T2 > 0, a regular sampling on the
continuous-space function xc on the space (space-time axes) t1 and t2. Then the Fourier
transform of the discrete-space sequence x(n1,n2) is
X(ω1,ω2) =
1
T1T2
X
all k1,k2
Xc
ω1 −2πk1
T1
, ω2 −2πk2
T2

.
Proof We start by writing x(n1,n2) in terms of the samples of the inverse Fourier trans-
form of Xc(1,2):
x(n1,n2) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
Xc(1,2)exp+ j(1n1T1 + 2n2T2)d1d2.
Next, we let ω1 ≜1T1 and ω2 ≜2T2 in this integral to get
x(n1,n2) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
1
T1T2
Xc
ω1
T1
, ω2
T2

exp+ j(ω1n1 + ω2n2)dω1dω2
=
1
(2π)2
X
all k1,k2
Z
SQ(k1,k2)
1
T1T2
Xc
ω1
T1
, ω2
T2

exp +j(ω1n1 +ω2n2)dω1dω2,
(2.1–1)
where SQ(k1,k2) is a 2π × 2π square centered at position (2πk1,2πk2); that is,
SQ(k1,k2) ≜[−π + 2πk1,+π + 2πk1] × [−π + 2πk2,+π + 2πk2].
Then, making the change of variables ω′
1 ≜ω1 −2πk1 and ω′
2 ≜ω2 −2πk2 separately
and inside each of the above integrals, we get
x(n1,n2) =
1
(2π)2
X
all k1,k2
+π
Z
−π
+π
Z
−π
1
T1T2
Xc
ω′
1 + 2πk1
T1
, ω′
2 + 2πk2
T2

× exp+ j(ω′
1n1 + ω′
2n2)dω′
1dω′
2
=
1
(2π)2
+π
Z
−π
+π
Z
−π

X
all k1,k2
1
T1T2
Xc
ω′
1 + 2πk1
T1
, ω′
2 + 2πk2
T2


× exp+ j(ω′
1n1 + ω′
2n2)dω′
1dω′
2
= IFT

X
all k1,k2
1
T1T2
Xc
ω1 + 2πk1
T1
, ω2 + 2πk2
T2


as was to be shown.

2.1 Sampling Theorem—Rectangular Case
39
Thus we have established the important and basic Fourier sampling relation
X(ω1,ω2) =
1
T1T2
X
k1,k2
Xc
ω1 −2πk1
T1
, ω2 −2πk2
T2

,
(2.1–2)
which can also be written in terms of analog frequency as
X(T11,T22) =
1
T1T2
X
k1,k2
Xc

1 −2πk1
T1
,2 −2πk2
T2

,
(2.1–3)
showing more clearly where the aliased components in X(ω1,ω2) come from in the
analog frequency domain. The aliased components are each centered on analog fre-
quency locations

2πk1
T1 , 2πk2
T2

for all integer grid locations (k1,k2). Of course, for
Xc lowpass, and for the case where the sampling density is not very low, we would
expect that the main contributions to aliasing would come from the eight nearest
neighbor bands corresponding to (k1,k2) = (±1,0),(0,±1), and (±1,±1) in (2.1–2)
or (2.1–3), which are sketched in Figure 2.1–1.
This rectangular sampling produces a 2-D spatial frequency aliasing of the
continuous-space Fourier transform. As such, we would expect to avoid most of the
aliasing, for a nonideal lowpass signal, by choosing the sampling periods T1,T2 small
Ω1
Ω2
Ωc1
Ωc2
+p /T2
+p /T1
−p/T2
−p/T1
FIGURE 2.1–1
An illustration of the effect of nearest neighbor aliases for the circular symmetric case.

40
CHAPTER 2 Sampling in Two Dimensions
enough. We notice that a variety of aliasing can occur. It can be horizontal, vertical,
and/or diagonal aliasing.
We notice that if the input signal is rectangularly bandlimited in the sense that
Xc(1,2) = 0
for
|1| ≥π
T1
OR
|2| ≥π
T2
or equivalently that supp{Xc(1,2)} = (−π/T1,+π/T1) × (−π/T2,+π/T2), then
we have no aliasing in the baseband, and
X(ω1,ω2) =
1
T1T2
Xc
ω1
T1
, ω2
T2

for
|ω1| ≤π
AND
|ω2| ≤π,
or equivalently in terms of analog frequencies,
X(T11,T22) =
1
T1T2
Xc(1,2)
for
|T11| ≤π
AND
|T22| ≤π,
so that Xc can be recovered exactly, where it is nonzero, from the Fourier transform
of its sampled version, by
Xc(1,2) = T1T2 X(T11,T22)
for
|1| ≤π/T1
AND
|2| ≤π/T2.
More generally, these exact reconstruction results will be true for any signal
rectangularly bandlimited to [−c1,+c1] × [−c2,+c2] whenever
c1 ≤π/T1
AND
c2 ≤π/T2.
This is illustrated in Figure 2.1–2 for a circularly bandlimited signal.
Ωc1
Ω1
+p/T1
Ω2
Ωc2
+p/T2
−p/T1
−p/T2
FIGURE 2.1–2
A continuous Fourier transform that will not alias when sampled at all the multiples
of (T1,T2).

2.1 Sampling Theorem—Rectangular Case
41
Reconstruction Formula
At this point we have found the effect of rectangular sampling in the frequency
domain. We have seen that no information is lost if the horizontal and vertical sam-
pling rate is high enough for rectangular bandlimited signals. Next, we investigate
how to reconstruct the original signal from these samples. To obtain xc, we start out
by writing the continuous-space IFT:
xc(t1,t2) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
Xc(1,2) × exp+ j(1t1 + 2t2)d1d2
=
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
T1T2 X(T11,T22)Ic1,c2(1,2)
× exp+ j(1t1 + 2t2)d1d2,
where we have made use of the indicator function
Ic1,c2(1,2) ≜
1,
|1| < c1
and
|2| < c2,
0,
elsewhere.
Continuing,
xc(t1,t2) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
T1T2 X(T11,T22)Ic1,c2(1,2)
× exp+ j(1t1 + 2t2)d1d2
= T1T2
(2π)2
+c1
Z
−c1
+c2
Z
−c2
X(T11,T22)exp+ j(1t1 + 2t2)d1d2.
Next, we substitute X with its FT expression in terms of the samples x(n1,n2),
X(T11,T22) =
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)exp−j(T11n1 + T22n2),
to obtain
xc(t1,t2) = T1T2
(2π)2
+c1
Z
−c1
+c2
Z
−c2
"
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)exp−j(T11n1 + T22n2)
#
× exp+ j(1t1 + 2t2)d1d2,

42
CHAPTER 2 Sampling in Two Dimensions
and then interchange the sums and integrals to obtain
xc(t1,t2) = T1T2
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)



1
(2π)2
+c1
Z
−c1
+c2
Z
−c2
×exp + j[1(t1 −n1T1) + 2(t2 −n2T2)]d1d2



= T1T2
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)h(t1 −n1T1,t2 −n2T2),
(2.1–4)
where
h(t1,t2) ≜sinc1t1
πt1
sinc2t2
πt2
,
−∞< t1,t2 < +∞.
The interpolation function h is the continuous-space IFT
1
(2π)2
+c1
Z
−c1
+c2
Z
−c2
exp+ j(1t1 + 2t2)d1d2,
which is the impulse response of the ideal rectangular lowpass ﬁlter
Ic1,c2(1,2) ≜
1,
|1| ≤c1
and
|2| ≤c2,
0,
elsewhere.
Equation 2.1–4 is known as the reconstruction formula for the rectangular sampling
theorem and is valid whenever the sampling rate satisﬁes
T−1
1
≥c1
π
and
T−1
2
≥c2
π .
We note that the reconstruction consists of an inﬁnite weighted sum of delayed ideal
lowpass ﬁlter impulse responses multiplied by a gain term of T1T2 centered at each
sample location and weighted by the sample value x(n1,n2). As in the 1-D case [1],
we can write this in terms of ﬁltering as follows. First, we deﬁne the continuous-space
impulse train, sometimes called the modulated impulse train,
xδ(t1,t2) ≜
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)δ(t1 −n1T1,t2 −n2T2),

2.1 Sampling Theorem—Rectangular Case
43
and then we put this impulse train function through the ﬁlter with impulse response
T1T2h(t1,t2).
When the sample rate is minimal, we have the critical sampling case, and the
interpolation function
T1T2h(t1,t2) = T1T2
sinc1t1
πt1
sinc2t2
πt2
becomes
= T1T2
sin π
T1 t1
πt1
sin π
T2 t2
πt2
=
sin π
T1 t1
π
T1 t1
sin π
T2 t2
π
T2 t2
= sinc1t1
c1t1
sinc2t2
c2t2
.
For this critical sampling case, the reconstruction formula becomes
xc(t1,t2) =
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)
sin π
T1 (t1 −n1T1)
π
T1 (t1 −n1T1)
sin π
T2 (t2 −n2T2)
π
T2 (t2 −n2T2)
(2.1–5)
=
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2) sinc1(t1 −n1T1)
c1(t1 −n1T1)
sinc2(t2 −n2T2)
c2(t2 −n2T2) ,
(2.1–6)
since π/Ti = ci in this case.
For this critical sampling case, the interpolating functions
sinc1(t1 −n1T1)
c1(t1 −n1T1)
sinc2(t2 −n2T2)
c2(t2 −n2T2)
have the property that each one is equal to 1 at its sample location (n1T1,n2T2) and
equal to 0 at all other sample locations. Thus at a given sample location, only one of
the terms in the double inﬁnite sum is nonzero.
If this critical sampling rate is not achieved, then aliasing can occur as shown in
Figure 2.1–1. Note from this ﬁgure that if the continuous-space Fourier transform
has the indicated circular support, then the main alias contributions will come from
the horizontally and vertically nearest neighbor aliased terms; in (2.1–3), the terms
correspond to (k1,k2) = (±1,0) and (0,±1), with the diagonal terms at (±1,±1)
being the
√
2 further away.

44
CHAPTER 2 Sampling in Two Dimensions
LPF
xc(t1, t2)
y(n1, n2)
Sample
(T1, T2)
FIGURE 2.1–3
A system diagram for ideal rectangular sampling to avoid all aliasing error.
Note that even if the analog signal is sampled at high enough rates to avoid any
aliasing, there is a type of alias error that can occur on reconstruction due to an inad-
equate or nonideal reconstruction ﬁlter. Sometimes in the 1-D case, this distortion,
which is not due to undersampling, is called imaging distortion to distinguish it from
true aliasing error [2]. This term may not be the best to use in an image and video
processing text, so if we refer to this spectral imaging error in the sequel, we will put
“image” in quotation marks.
Ideal Rectangular Sampling
In the case where the continuous-space Fourier transform is not bandlimited, one
simple expedient is to provide an ideal continuous-space lowpass ﬁlter prior to the
spatial sampler (Figure 2.1–3). This sampler would be ideal in the sense that the
maximum possible bandwidth of the signal is preserved alias free.
The lowpass ﬁlter (LPF) would pass the maximum band that can be represented
with rectangular sampling with period T1 × T2, which is the analog frequency band
[−π/T1,+π/T1] × [−π/T2,+π/T2] with passband gain = 1.
Example 2.1–1: Nonisotropic Signal Spectra
Consider a nonisotropic signal with continuous-space Fourier transform support, as shown
in Figure 2.1–4, that could arise from rotation of a texture that is relatively lowpass in
one direction and broadband in the perpendicular direction. If we apply the rectangular
sampling theorem to this signal, we get the minimal or Nyquist sampling rates
T−1
1
= c1/π
and
T−1
2
= c2/π,
resulting in the discrete-space Fourier transform support shown in Figure 2.1–5.
Clearly, there is a lot of wasted spectrum here. If we lower the sampling rates judi-
ciously, we can move to a more efﬁciently sampled discrete-space Fourier transform with
support, as shown in Figure 2.1–6. This ﬁgure shows aliased replicas that do not overlap,
but yet cannot be reconstructed properly with the ideal rectangular reconstruction for-
mula (2.1–5). If we reconstruct with an appropriate ideal diagonal support ﬁlter, though,
we see that it is still possible to reconstruct this analog signal exactly from this lower

2.1 Sampling Theorem—Rectangular Case
45
Ω1
Ωc1
Ωc2
Ω2
−Ωc1
−Ωc2
FIGURE 2.1–4
A continuous-space Fourier transform with “diagonal” support.
Ω2
Ωc2
Ω1
Ωc1
2Ωc2
2Ωc1
FIGURE 2.1–5
The effect of rectangular sampling at rectangular Nyquist rate.
sample-rate data. Effectively, we are changing the basic cell in the analog frequency
domain from [−π/T1,+π/T1] × [−π/T2,+π/T2] to the diagonal basic cell shown in
Figure 2.1–7. The dashed-line aliased repeats of the diagonal basic cell show the spectral
aliasing resulting from the rectangular sampling, illustrating the resulting periodicity in the
sampled Fourier transform.

46
CHAPTER 2 Sampling in Two Dimensions
Ω1
Ω2
p/T2
p/T1
FIGURE 2.1–6
The effect after lowering the vertical sampling rate below the rectangular Nyquist rate.
Ω2
Ω1
2p/T2
p/T2
p/T1
2p/T1
FIGURE 2.1–7
A basic cell, indicated by heavy lines, for diagonal analog Fourier transform support.

2.1 Sampling Theorem—Rectangular Case
47
From this last example, we see that, more so than in one dimension, there is
a wider variety of the support or shape of the incoming analog Fourier-domain
data, and it is this analog frequency domain support that, together with the sam-
pling rates, determines whether the resulting discrete-space data are aliased or not.
In Example 2.1–1, a rectangular Fourier support would lead to aliased discrete-space
data for such low spatial sampling rates, but for the indicated diagonal analog Fourier
support shown in Figure 2.1–6, we see that aliasing will not occur if we use this new
basic cell. The next example shows one way such diagonal analog frequency domain
support arises naturally.
Example 2.1–2: Propagating Plane Waves
Consider the geophysical spatiotemporal data given as
sc(t,x) = g(t −x/v),
(2.1–7)
where v is a given velocity, v ̸= 0. We can interpret this as a plane wave propagating in the
+x direction when v is positive, with wave crests given by the equation t −x/v = constant.
Here, g is a given function indicating the wave shape. At position x = 0, the signal value
is just g(t) = sc(t,0). At a general position x, we see this same function delayed by the
propagation time x/v. Taking the continuous parameter time-space Fourier transform, we
have
FT[sc] =
Z Z
sc(t,x)exp−j(t + Kx)dtdx,
where  as usual denotes continuous-time radian frequency, and the continuous variable
K denotes continuous-space radian frequency referred to as the wavenumber. Plugging
in the plane-wave equation (2.1–7), we obtain
Sc(,K) =
Z Z
g(t −x/v)exp−j(t + Kx)dtdx
=
Z Z
g(t −x/v)exp−jtdt

exp−jKxdx
=
Z
G()exp(−jx/v)exp−jKxdx
= G()
Z
exp−j(x/v + Kx)dx
= 2πG()δ(K + /v),
a delta function in the frequency domain, concentrated along the line K + /v = 0, plotted
in Figure 2.1–8.

48
CHAPTER 2 Sampling in Two Dimensions
Ω
K
0
K=−Ω/n
FIGURE 2.1–8
Fourier transform illustration of ideal plane wave at velocity v > 0.
If we relax the assumption of an exact plane wave in Example 2.1–2, we get some
spread out from this ideal impulse line, and hence ﬁnd a diagonal Fourier transform
support as illustrated in Figure 2.1–9. More on the application of multidimensional
geophysical processing is contained in [3].
Ω
K
0
K =−Ω/ν
FIGURE 2.1–9
Fourier transform illustration of approximate plane wave at velocity v.
Example 2.1–3: Alias Error in Images
In image processing, aliasing energy looks like ringing that is perpendicular to high
frequency or sharp edges. This can be seen in Figure 2.1–10, where there was exces-
sive high-frequency information around the palm fronds. We can see the aliasing in the
zoomed-in image shown in Figure 2.1–11, where we see ringing parallel to the fronds,
caused by some prior ﬁltering, and aliased energy approximately perpendicular to the
edges, appearing as a ringing approximately perpendicular to the fronds. A small amount
of aliasing is usually not very annoying in images.

2.1 Sampling Theorem—Rectangular Case
49
FIGURE 2.1–10
2000 × 1000-pixel image with aliasing.
FIGURE 2.1–11
Zoomed-in section of aliased image.
To appreciate how the aliasing (or “imaging” error) energy can appear nearly per-
pendicular to a local diagonal component, refer to Figure 2.1–12, where we see two
alias components coming from above and below in opposite quadrants to those of the
main signal energy, giving the alias error signal a distinct high-frequency directional
component.

50
CHAPTER 2 Sampling in Two Dimensions
Ω1
Ω2
Ωc1
Ωc2
+p /T2
−p/T1
−p/T2
+p /T1
FIGURE 2.1–12
An illustration of how an alias (or “imaging”) error can arise in a more directional case.
2.2 SAMPLING THEOREM—GENERAL REGULAR CASE
Now consider more general, but still regular, nonorthogonal sampling on the regular
grid of locations or lattice,
t1
t2

=
v1 v2
n1
n2

,
for sampling vectors v1and v2, or what is the same t= n1v1 + n2v2 for all integers n1
and n2. Thus we have the sampled data
x(n) ≜xc(Vn),
(2.2–1)
with sampling matrix V ≜
v1 v2

. The sampling matrix is assumed always invertible,
since otherwise, the sampling locations would not cover the plane (i.e., would not
be a lattice). The rectangular or Cartesian sampling we encountered in the previous
section is the special case v1 = (T1,0)T and v2 = (0,T2)T.

2.2 Sampling Theorem—General Regular Case
51
Example 2.2–1: Hexagonal Sampling Lattice
A sampling matrix of particular importance,
V =
"
1
1
1/
√
3
−1/
√
3
#
,
results in a hexagonal sampling pattern. The resulting hexagonal sampling pattern is
sketched in Figure 2.2–1, where we note that the axes n1 and n2 are no longer ortho-
gonal to one another. The hexagonal sampling grid shape comes from the fact that
the two sampling vectors have angle ±30◦with the horizontal axis, and that they are
of equal length. It is easily seen by example, though, that these sample vectors are
not unique to produce this grid. One could as well use v1 = (0,1/
√
3)T together with
v2 = (0,2/
√
3)T to generate this same lattice. We will see later that the hexagonal samp-
ling grid can be more efﬁcient than the Cartesian grid in many common image-processing
situations.
t2
t1
n1
n2
2
−2 
1
−1
FIGURE 2.2–1
Hexagonal sampling grid in space.

52
CHAPTER 2 Sampling in Two Dimensions
To develop a theory for the general sampling case of (2.2–1), we start by writing
the continuous-space inverse Fourier transform,
xc(t) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
Xc(Ω)exp+ j
 ΩTt

dΩ,
with analog frequency Ω≜(1,2)T. Then, by deﬁnition of the sample locations,
we have the discrete data as
x(n) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
Xc(Ω)exp+ j(ΩTVn)dΩ,
which upon writing ω ≜VTΩbecomes
x(n) =
1
(2π)2
+∞
Z
−∞
+∞
Z
−∞
Xc(V−Tω)exp+ j(ωTn)
dω
|detV|.
(2.2–2)
Here, V−T denotes the inverse of the transpose sampling matrix VT, where the
notational simpliﬁcation is permitted because the order of transpose and inverse
commutes for invertible matrices. Next, we break up the integration region in this
equation into squares of support [−π,+π]2 ≜[−π,+π] × [−π,+π] and write
x(n) =
1
(2π)2
+π
Z
−π
+π
Z
−π
1
|detV|
(X
all k
Xc[V−T(ω −2πk)]
)
exp+ j(ωTn)dω,
just as in (2.1–1) of the last section, and valid for the same reason. We can now invoke
the uniqueness of the inverse Fourier transform for 2-D discrete space to conclude
that the discrete-space Fourier transform X(ω) must be given as
X(ω) =
1
|detV|
(X
all k
Xc[V−T(ω −2πk)]
)
,
where the discrete-space Fourier transform X(ω) is P
n x(n)exp(−jωTn), just as
usual.
We now introduce the periodicity matrix U ≜2πV−T and note the fundamental
equation
UTV = 2πI,
(2.2–3)
where I is the identity matrix. This equation relates any regular sampling matrix and
its corresponding periodicity matrix in the analog frequency domain.
In terms of periodicity matrix U, we can write
X(ω) =
1
|detV|
(X
all k
Xc
 1
2π U(ω −2πk)
)
,
(2.2–4)

2.2 Sampling Theorem—General Regular Case
53
which can be written also in terms of the analog frequency variable Ω
X(VTΩ) =
1
|detV|
(X
all k
Xc(Ω−Uk)
)
,
showing that the alias location points of the discrete-space Fourier transform have
the periodicity matrix U when written in the analog frequency variable Ω.
For the rectangular sampling case with sampling matrix
V =
"
T1
0
0
T2
#
,
the periodicity matrix is
U =
"
2π/T1
0
0
2π/T2
#
,
showing consistency with the results of the previous section, i.e., (2.2–4) simpliﬁes
to (2.1–2).
Example 2.2–2: General Hexagonal Case
In the case where the sampling matrix is hexagonal,
V =
"
T
T
T/
√
3
−T/
√
3
#
,
with T being an arbitrary scale coefﬁcient for the sampling vectors. The corresponding
periodicity matrix is easily seen to be
U =
"
π/T
π/T
π
√
3/T
−π
√
3/T
#
= [u1,u2]
and |detV| = 2T2/
√
3. The resulting repetition or alias anchor points in analog frequency
space are shown in Figure 2.2–2. Note that the hexagon appears rotated (by 30◦) from
the one hexagonal sampling grid in the spatial dimension in Figure 2.2–1. At each alias
anchor point, the analog Fourier transform would be seated. Aliasing will not occur if the
support of the analog Fourier transform is circular and less than π
√
3/T in radius.
We next turn to the reconstruction formula for the case of hexagonal sampling.

54
CHAPTER 2 Sampling in Two Dimensions
Ω2
Ω1
π/T
2π/T
−π/T
−2π /T
π  3/T
−π  3/T
ω1
ω2
FIGURE 2.2–2
Hexagonal alias repeat grid in analog frequency domain.
Hexagonal Reconstruction Formula
First, we have to scale down the hexagonal grid with scaling parameter T until there
is no aliasing (assuming ﬁnite analog frequency domain support). Then we have
X(VTΩ) =
1
|detV|Xc(Ω)
=
√
3
2T2 Xc(Ω),
and so
xc(t) = 2T2
√
3
 1
2π
2 X
n
x(n)
Z
B
exp[jΩT(t −Vn)]dΩ,
where B is the hexagonal frequency domain basic cell shown in Figure 2.2–3. This
basic cell is determined by bounding planes placed to bisect lines joining spectral
alias anchor points and the origin. The whole Ωplane can be covered without gaps
or overlaps by repeating this basic cell centered at each anchor point (tessellated). It
is worthwhile noting that an analog Fourier transform with circular support would ﬁt
nicely in such a hexagonal basic cell.
Upon deﬁning the spatial impulse response
h(t) ≜2T2
√
3
 1
2π
2 Z
B
exp(jΩTt)dΩ,

2.2 Sampling Theorem—General Regular Case
55
Ω1
Ω2
ω1
π/T
2π/T
−π/T
−2π /T
π  3/T
−π   3/T
ω2
FIGURE 2.2–3
Hexagonal basic cell in analog frequency domain (heavy line).
we can write the reconstruction formula for hexagonal sampling as
xc(t) =
X
n
x(n)h(t −Vn).
Of course, we could subsequently evaluate t on a different sampling lattice and
thereby have a way to convert between sampling lattices in two dimensions.
Example 2.2–3: Sampling Efﬁciency for Spherical Baseband
We can generalize the hexagonal lattice to three dimensions, where it is called a 3-D rhom-
bic dodecahedron, and continue to four and higher dimensions. We would notice then that
a spherical baseband ﬁts snugly into a generalized hexagon, much better than into a gene-
ralized square or cube. Dudgeon and Mersereau [3] have obtained the following results
showing the sampling efﬁciency of these generalized hexagonal lattices with respect to the
Cartesian lattice:
Dimension M
1
2
3
4
Efﬁciency
1.000
0.866
0.705
0.505
We see a 30% advantage for the 3-D case (e.g., spatiotemporal or 3-D space) and a 50%
advantage in 4-D (e.g., the three spatial dimensions + time, or true 3-D video).

56
CHAPTER 2 Sampling in Two Dimensions
In spite of its increased sampling efﬁciency, the 3-D generalization of the hexa-
gonal lattice has not found much use in applications that sample 3-D data. However,
a variation of the diamond sampling lattice has found application in the interlaced
sensors, commonly found in video cameras, both standard deﬁnition (SD) and high
deﬁnition (HD).
Example 2.2–4: Diamond-shaped Sampling Lattice
A special sampling lattice is given by sampling matrix
V =
"
T
T
T
−T
#
and periodicity matrix U =
"
π/T
π/T
π/T
−π/T
#
,
resulting in the so-called diamond-shaped lattice, illustrated in Figure 2.2–4 for the
normalized case T = 1.
If we look to the periodicity matrix in analog frequency space, we obtain Figure 2.2–5.
t2
n1
n2
t1
2
4
−2
−4
FIGURE 2.2–4
Diamond sampling grid with T = 1.
Ω2
Ω1
−π
−π
π
π
ω1
ω2
FIGURE 2.2–5
Alias anchor points for diamond sampling with T = 1.

2.2 Sampling Theorem—General Regular Case
57
If we restrict our 2-D sampling to the vertical and temporal axes of video data, we
get the following example of diamond sampling in conventional video.
Example 2.2–5: Interlaced Video
Consider a video signal sc(x,y,t), sampled on a rectangular or Cartesian grid, to get the so-
called progressive digital video signal s(n1,n2,n) ≜sc(n111,n212,n1), where (11,12)T
is the sample vector in space and 1 is the sampling interval in time. But in SD and HD
video, it is also common for it to be sampled on an interlaced lattice consisting of two
ﬁelds making up each frame. One ﬁeld just contains the even lines, while the second ﬁeld
gets the odd scan lines. In equations, we have
s(n1,2n2,2n) ≜sc(n111,2n212,2n1)
and
s(n1,2n2 + 1,2n + 1) ≜sc(n111,(2n2 + 1)12,(2n + 1)1)
whose sampling period is illustrated in Figure 2.2–6, where we only show the vertical and
time axes. We can ﬁx the horizontal variable at, say, n1 = n0
1 and then regard the data as
2-D (vertical-time) data. In the two mentioned cases, this becomes progressive:
gc(y,t) ≜sc(x0,y,t)
and
g(n2,n) = gc(n212,n1),
or interlaced:
g(2n2,2n) ≜gc(2n212,2n1)
and
g(2n2 + 1,2n + 1) ≜gc((2n2 + 1)12,(2n + 1)1).
y
t
0
1
2
3
4
5
6
1
2
3
4
5
6
FIGURE 2.2–6
An illustration of interlaced sampling in a 2-D vertical-temporal domain.

58
CHAPTER 2 Sampling in Two Dimensions
The sampling matrix for this 2-D interlaced data is easily seen to be
V =
"
12
12
1
−1
#
,
corresponding to diamond sampling in the vertical-time domain. Applying the diamond-
shaped sample lattice theory, we get a diamond or rotated square baseband to keep alias
free. So, if sampled at a high enough spatiotemporal rate, we can reconstruct the con-
tinuous space-time signal from the diamond sampled or interlaced digital video data. We
would do this, conceptually at least, via 2-D processing in the vertical-temporal domain,
for each horizontal value n1.
In performing the processing in Example 2.2–5, we consider the 3-D spatiotem-
poral data as a set of 2-D y × t data planes at each horizontal location x = x0. Thus the
2-D processing would be done on each such y × t plane separately, and then all the
results would be combined. In practice, since there is no interaction between the y × t
planes, this processing is performed interleaved in time, so that the data are processed
in time locality (i.e., those frames near frame n are processed together).
Interlace has gotten a bad name in video, despite its increased spectral efﬁciency
over progressive sampling. This is because of three reasons: First, the proper space-
time ﬁltering to reconstruct the interlaced video is hardly ever done, and in its place
is inserted a lowpass vertical ﬁlter, sacriﬁcing typically about 30% of the vertical res-
olution. Second, almost all ﬂat-panel display devices are progressive; thus interlaced
images have to be transformed to progressive by the display hardware, often with less
than optimal results, in addition to the vertical resolution loss previously mentioned.
Lastly, today arguably the best video sources, both ﬁlm and digital, are progressive,
not interlaced.
2.3 CHANGE OF SAMPLE RATE
Returning to the more common case of rectangular sampling, it is often necessary in
image and video signal processing to change the sample rate, either up or down. This
is commonly the case in the resizing of images and videos for display on a digital
image or video monitor.
Downsampling by Integers M1 × M2
We deﬁne decimation or downsampling by integers M1 and M2 as follows:
xd(n1,n2) ≜x(M1n1, M2n2),
denoted via the system element shown in Figure 2.3–1. The correspondence to
rectangular downsampling in the frequency domain is
Xd(ω1,ω2) =
1
M1M2
M1−1
X
i1=0
M2−1
X
i2=0
X
ω1 −2πi1
M1
, ω2 −2πi2
M2

.
(2.3–1)

2.3 Change of Sample Rate
59
M1× M2
x (n1,n2)
xd(n1,n2)
FIGURE 2.3–1
Downsample system element.
One way of deriving this equation is to ﬁrst imagine an underlying continuous-
space function xc(t1,t2), of which the values x(n1,n2) are its samples on the grid
T1 × T2. Then we can regard the downsampled signal xd as the result of sampling xc
on the less dense sample grid M1T1 × M2T2. From the rectangular sampling theorem,
we have
Xd(ω1,ω2) =
1
M1T1M2T2
+∞
X
k1,k2=−∞
Xc
ω1 −2πk1
M1T1
, ω2 −2πk2
M2T2

.
(2.3–2)
Comparing this equation with (2.1–2), we can see that it includes the alias compo-
nents of (2.3–2) when k1 = M1l1 and k2 = M2l2 with l1 and l2 integers, plus many
others in between. So we can substitute the following sums into (2.3–2):
k1 = i1 + M1l1,
where i1 : 0,...,M1 −1,
k2 = i2 + M2l2,
where i2 : 0,...,M2 −1,
and then rewrite the equation as
Xd(ω1,ω2)
=
1
M1T1M2T2
+∞
X
l1,l2=−∞
M1−1
X
i1=0
M2−1
X
i2=0
× Xc
ω1 −2πi1
M1T1
−2πl1
T1
, ω2 −2πi2
M2T2
−2πl2
T2

=
1
M1M2
M1−1
X
i1=0
M2−1
X
i2=0
1
T1T2
+∞
X
l1,l2=−∞
× Xc
ω1 −2πi1
M1T1
−2πl1
T1
, ω2 −2πi2
M2T2
−2πl2
T2

=
1
M1M2
M1−1
X
i1=0
M2−1
X
i2=0
X
ω1 −2πi1
M1
, ω2 −2πi2
M2

,
(2.3–3)
in terms of the discrete-space Fourier transform X, by evaluation of (2.1–2) at the
locations

ω1−2πi1
M1
, ω2−2πi2
M2

.

60
CHAPTER 2 Sampling in Two Dimensions
Example 2.3–1: M1 = M2 = 2 Case for 2 × 2 Decimation
In this case, we have xd(n1,n2) ≜x(2n1,2n2); thus in the frequency domain with M1 =
M2 = 2, (2.3–3) becomes
Xd(ω1,ω2) = 1
4
1
X
i1=0
1
X
i2=0
X
ω1 −2πi1
2
, ω2 −2πi2
2

= 1
4

X
ω1
2 , ω2
2

+ X
ω1
2 , ω2
2 −π

+ X
ω1
2 −π, ω2
2

+ X
ω1
2 −π, ω2
2 −π

.
(2.3–4)
Figure 2.3–2 shows the characteristic periodic support pattern of a lowpass Fourier
transform X(ω1,ω2) plotted out to beyond 2π in each variable. Figure 2.3–3 shows the
corresponding Fourier transform X
  1
2ω1, 1
2ω2

plotted out to a range beyond 4π in each
variable. We see how the periodic support pattern has spread out now, and note that this
function has a period of 4π × 4π; therefore, by itself, it cannot be a valid Fourier transform.
With reference to this ﬁgure, we can see how the other three terms in (2.3–4) can ﬁt in
without overlap, each centered at (0,2π),(2π,0), and (2π,2π), respectively, to restore the
2π × 2π periodicity and make Xd a valid Fourier transform. A necessary condition of no-
overlap is seen to be supp{X} ⊆(−1
2π,+ 1
2π) × (−1
2π,+ 1
2π). If this condition is met, as
shown in Figure 2.3–2, then no information is lost and perfect reconstruction is possible
from the subsampled signal.
ω2
ω1
+π
−π
+π
−π
+2π
−2π
+2π
−2π
FIGURE 2.3–2
Illustration of lowpass X, plotted out beyond 2π in each variable.

2.3 Change of Sample Rate
61
ω 2
ω 1
+π
−π
+π
+2π
−π
+ 4π
+2π
−4π
+4π
−4π
FIGURE 2.3–3
Fourier transform X

1
2ω1, 1
2ω2

plotted out to a range beyond 4π in each variable.
LPF
y
M1 × M2
x(n1, n2)
yd(n1, n2)
FIGURE 2.3–4
System diagram for ideal decimation, which avoids aliasing error in the decimated signal.
Ideal Decimation
In order to avoid alias error in the case of a general signal x and to preserve as much of
its frequency content as possible, we can choose to insert an ideal rectangular lowpass
ﬁlter ahead of the downsampler, as shown in Figure 2.3–4. Here, the passband of
the ﬁlter should have gain 1, and the passband is chosen as [−π/M1,+π/M1] ×
[−π/M2,+π/M2] to avoid aliasing in the decimated signal. This process has been
called ideal decimation.

62
CHAPTER 2 Sampling in Two Dimensions
Example 2.3–2: Ideal 2 × 2 Decimation
We have a ﬁltered signal here, so upon 2 × 2 decimation, xd(n1,n2) ≜(h ∗x)(2n1,2n2),
and in the frequency domain1
Xd(ω1,ω2) = 1
4
1
X
i1=0
1
X
i2=0
(HX)
ω1 −2πi1
2
, ω2 −2πi2
2

= 1
4
h
(HX)
ω1
2 , ω2
2

+ (HX)
ω1
2 , ω2
2 −π

+ (HX)
ω1
2 −π, ω2
2

+ (HX)
ω1
2 −π, ω2
2 −π
i
.
(2.3–5)
We can see that the baseband that can be preserved alias free in the decimated signal
xd is [−π/2,+π/2] × [−π/2,+π/2], which is 1/4 of the full bandwidth. This region is
shown as the gray area in Figure 2.3–5, a subband of the original fullband signal HX.
Since this subband is lowpass in each frequency variable, it is called the LL subband. If
we preﬁlter with an ideal lowpass ﬁlter with gain 1, whose frequency domain support is
shown as gray in Figure 2.3–5,
H(ω1,ω2) =
(
1,
(ω1,ω2) ∈[−π/2,+π/2] × [−π/2,+π/2],
0,
else,
then we can preserve the LL subband [−π/2,+π/2] × [−π/2,+π/2] from the original
signal X.
0
ω1
ω2
+π
+π
−π
−π
+π/2
−π/2
−π/2
+π/2
FIGURE 2.3–5
The gray area is the LL subband preserved under 2 × 2 ideal decimation.
1Here, (h ∗x) denotes the resulting function of convolution, and (HX) denotes the function resulting
from pointwise product.

2.3 Change of Sample Rate
63
The higher frequency components or subbands of the original signal X can be
obtained as follows. Consider the gray area shown in Figure 2.3–6. We can separate
out this part, called the HL subband, by preﬁltering before the decimation with the
ideal bandpass ﬁlter, with the passband shown as the gray region in Figure 2.3–6.
Similarly, the LH and HH subbands can be separated out, using ﬁlters with supports
as shown in Figures 2.3–7 and 2.3–8, respectively. Together, these four subbands
contain all the information from the original fullband signal.2
0
1
/2
/2
/2
/2
2
FIGURE 2.3–6
Frequency domain support of the HL subband.
0
1
/2
/2
/2
/2
2
FIGURE 2.3–7
Frequency domain support of the LH subband.
2Care must be taken on the boundaries of these subbands so that information is not duplicated or lost
by this ideal ﬁlter decomposition.

64
CHAPTER 2 Sampling in Two Dimensions
0
1
/2
/2
/2
/2
2
FIGURE 2.3–8
Frequency domain support of the HH subband.
We will see in the sequel that decomposition of a fullband signal into its sub-
bands can offer advantages for both image processing and compression. In fact, the
international image compression standard JPEG 2000 is based on subband analysis.
Upsampling by Integers L1 × L2
We deﬁne the upsampled signal by integers L1 × L2 as
xu(n1,n2) ≜
(
x

n1
L1 , n2
L2

,
when L1 divides n1 and L2 divides n2,3
0,
else.
So, just as in one dimension, we deﬁne 2-D upsampling as inserting zeros for the
missing samples. The system diagram for this is shown in Figure 2.3–9.
The expression for upsampling in the Fourier domain can be found as
Xu(ω1,ω2) =
X
all n1,n2
xu(n1,n2)exp−j(ω1n1 + ω2n2)
=
X
n1=L1k1,n2=L2k2
xu(L1k1,L2k2)exp−j(ω1L1k1 + ω2L2k2)
=
X
all k1,k2
xu(L1k1,L2k2)exp−j(ω1L1k1 + ω2L2k2)
= X(L1ω1,L2ω2).
We note that this expression is what we would expect from the 1-D case and that no
aliasing occurs due to upsampling, although there are spectral “images” appearing.
3‘L divides n’ means just that n/L is an integer.

2.3 Change of Sample Rate
65
L1× L2
x(n1, n2)
xu(n1, n2)
FIGURE 2.3–9
Upsample system element.
We see that upsampling effectively shrinks the frequency scale in both dimensions
by the corresponding upsampling factors. Since the Fourier transform is 2π × 2π
periodic, this brings in many spectral repeats. Note that there is no overlap of com-
ponents, though, and these repeats can therefore be removed by a lowpass ﬁlter with
bandwidth [−π/L1,+π/L1] × [−π/L2,+π/L2], which leads to ideal interpolation.
Ideal Interpolation
By ideal interpolation, we mean that interpolation that will yield the same samples as
though we sampled a corresponding continuous-space function at the higher sample
rate. Using the rectangular sampling reconstruction formula (2.1–5), we can evaluate
at (t1,t2) = (n1T1/L1,n2T2/L2) to obtain
xc
n1T1
L1
, n2T2
L2

=
+∞
X
k1=−∞
+∞
X
k2=−∞
x(k1,k2)
sin π
T1

n1T1
L1 −k1T1

π
T1

n1T1
L1 −k1T1

sin π
T2

n2T2
L2 −k2T2

π
T2

n2T2
L2 −k2T2

=
+∞
X
k1=−∞
+∞
X
k2=−∞
x(k1,k2)
sinπ

n1
L1 −k1

π

n1
L1 −k1

sinπ

n2
L2 −k2

π

n2
L2 −k2
 ,
which begins to look similar to a convolution. To achieve a convolution exactly,
though, we can introduce the upsampled function xu and write the ideal interpola-
tion as
y ≜xc
n1T1
L1
, n2T2
L2

=
+∞
X
k1=−∞
+∞
X
k2=−∞
x(k1,k2)
sinπ

n1
L1 −k1

π

n1
L1 −k1

sinπ

n2
L2 −k2

π

n2
L2 −k2

=
+∞
X
k′
1,k′
2=multiples of L1,L2
x
 k′
1
L1
, k′
2
L2
 sinπ
 n1−k′
1
L1

π
 n1−k′
1
L1

sinπ
 n2−k′
2
L2

π
 n2−k′
2
L2

=
+∞
X
all k′
1,k′
2
xu(k′
1,k′
2)
sinπ
 n1−k′
1
L1

π
 n1−k′
1
L1

sinπ
 n2−k′
2
L2

π
 n2−k′
2
L2
 ,

66
CHAPTER 2 Sampling in Two Dimensions
LPF
xu
L1× L2
x(n1, n2)
y (n1, n2)
FIGURE 2.3–10
System diagram for ideal interpolation by integer factor L1 × L2.
which is just the 2-D convolution of the upsampled signal xu and an ideal impulse
response h,
h(n1,n2) =
sinπ

n1
L1

π

n1
L1

sinπ

n2
L2

π

n2
L2
 ,
which
corresponds
to
an
ideal
rectangular
lowpass
ﬁlter
with
bandwidth
[−π/L1,+π/L1] × [−π/L2,+π/L2] and passband gain = L1L2, with the system
diagram shown in Figure 2.3–10.
Example 2.3–3: Oversampling Camera
Image and video cameras only have the optical system in front of a charge-coupled device
(CCD) or complementary metal oxide semiconductor (CMOS) sensor to do the anti-aliasing
ﬁltering. As such, there is often alias energy evident in the image frame, especially in
the lower sample density video case. So-called oversampling camera chips have been
introduced, which ﬁrst capture the digital image at a high resolution and then do digital
ﬁltering combined with downsampling to produce the lower resolution output image. Since
aliasing is generally conﬁned to the highest spatial frequencies, the oversampled image
sensor can result in a signiﬁcant reduction in aliasing error.
2.4 SAMPLE-RATE CHANGE—GENERAL CASE
Just as 2-D sampling is not restricted to rectangular or Cartesian schemes, so also
decimation and interpolation can be accomplished more generally in two dimen-
sions. The 2-D grid is called a lattice. When we subsample a lattice, we create a
sublattice—i.e., a lattice contained in the original one. If the original data came from
sampling a continuous-space curve, the overall effect is then the same as sampling
the continuous-space function with the sublattice. General upsampling can be viewed
similarly, but we wish to go to a superlattice—i.e., one for which the given lattice
is a sublattice. Here, we assume that we already have data, however obtained, and
wish to either subsample it or to interpolate it. Applications occur in various areas
of image and video processing, with one being the conversion of sampling lattices.

2.4 Sample-Rate Change—General Case
67
Some images are acquired from sensors on diamond sampling grids and must be
upsampled to a Cartesian lattice for display on common image displays. Another
application is to the problem of conversion between interlaced and progressive video
by 2-D ﬁltering in the vertical-time domain.
General Downsampling
Let the 2 × 2 matrix M be nonsingular and contain only integer values. Then, given a
discrete-space signal x(n), where we have indicated position with the column vector
n, i.e., (n1,n2)T ≜n, a decimated signal xd(n) can be obtained as follows:
xd(n) ≜x(Mn),
where we call M the decimation matrix.
If we decompose M into its two column vectors as
M =
m1 m2

,
then we see that we are subsampling at the locations
n1m1 + n2m2,
and thus we can say that these two vectors m1 and m2 are the basis vectors of the
sublattice generated by M.
Example 2.4–1: Diamond Sublattice
If we choose to subsample with the decimation matrix
M =
"
1
1
−1
1
#
,
(2.4–1)
then we get the sublattice consisting of all the points
n1
"
1
−1
#
+ n2
"
1
1
#
,
a portion of which is shown in Figure 2.4–1. We see the generated sublattice (i.e.,
the retained sample locations denoted by large ﬁlled-in circles). The left-out points are
denoted by small ﬁled-in circles. Thus the original lattice consists of all the ﬁlled-in circles.
Note that we also indicate the various multiples n1 and n2 via the plotted axes, which then
become the coordinate axes of the subsampled signal.

68
CHAPTER 2 Sampling in Two Dimensions
1
2
-1
-2
1
2
-1
-2
n1
n2
FIGURE 2.4–1
Illustration of portion of sublattice generated by diamond subsampling. Large ﬁlled-in
circles are sublattice. Large and small ﬁlled-in circles together are the original lattice.
The corresponding result in frequency, derived by [4], is
Xd(ω) =
1
|detM|
( X
certain k
X[M−T(ω −2πk)]
)
,
(2.4–2)
with ω = (ω1,ω2)T, which is seen to be very close to Example 2.2–4 for the general
sampling case, with the decimation matrix M substituted for the sampling matrix V
of Section 2.2. Also we use X in place of the continuous-space Fourier transform
Xc used there. While in (2.2–4), the sum is over all k; here, the aliasing sum is only
over the additional alias points introduced by the decimation matrix M. For example,
looking at Figure 2.4–1, we see that the small ﬁlled-in circles constitute an equivalent
lattice that has been left behind by the chosen subsampling. There is a shift of (1,0)T
in this example. The derivation of (2.4–2) is considered in end-of-chapter problem 16.
Example 2.4–2: Effect of Diamond Subsampling on Frequency
Since we have the generator matrix (2.4–1) from (2.4–1), we calculate its determinant as
det M = 2 and the transposed inverse as
M−T=1
2
"
1
1
−1
1
#
.

Problems
69
Then substituting into (2.4–2), we sum over k = (0,0)T and k = (1,0)T to obtain
Y(ω) = 1
2X
 
ω1
2
"
1
−1
#
+ ω2
2
"
1
1
#!
+ 1
2X
 
(ω1 −2π)
2
"
1
−1
#
+ ω2
2
"
1
1
#!
= 1
2X
ω1 + ω2
2
, −ω1 + ω2
2

+ 1
2X
(ω1 −2π)
2
+ ω2
2 , −(ω1 −2π)
2
+ ω2
2

,
= 1
2X
ω1 + ω2
2
, −ω1 + ω2
2

+ 1
2X
ω1 + ω2
2
−π, −ω1 + ω2
2
+ π

.
We can interpret the ﬁrst term as a diamond-shaped baseband and the second term as
the single high-frequency aliased component. This alias signal comes from the fullband
[−π,+π]2 signal outside the diamond, with corners at (0,π),(π,0),(−π,0), and (0,−π).
CONCLUSIONS
This chapter has focused on how sampling theory extends to two dimensions. We ﬁrst
treated rectangular sampling and looked into various analog frequency supports that
permit perfect reconstruction (i.e., an alias-free discrete-space representation). Then
we turned to general but regular sampling patterns and focused on the commonly used
hexagonal and diamond-shape patterns. Finally, we looked at sample rate change for
the common rectangular sampled case and also brieﬂy looked at the general regular
subsampling problem in two dimensions.
PROBLEMS
1. A certain continuous-space signal sc is given as
sc(x1,x2) = 100 + 20cos6πx1 + 40sin(10πx1 + 6πx2),
for −∞< x1,x2 < +∞.
(a) What is the rectangular bandwidth of this signal? Express your answer in
radians using cutoff variables c1,c2.
(b) Sample the function sc at sample spacing (11,12) = (0.05,0.10) to obtain
discrete-space signal s(n1,n2) ≜sc(n111,n212). Write an expression for s.
Has overlap aliasing error occurred? Why or why not?
(c) Find and plot the Fourier transform S ≜FT{s}.
2. Denote the (discrete-space) Fourier transform as X (ω1,ω2), and assume the
corresponding signal x(n1,n2) resulted from rectangularly sampling with hori-
zontal spacing T1 = 10−3 meters and vertical spacing T2 = 10−4 meters. Assum-
ing no aliasing, express the continuous-space Fourier transform Xc(1,2) in

70
CHAPTER 2 Sampling in Two Dimensions
0
ω1
ω 2
+π
+π
−π
−π
−π /2
+π /2
−π /2
+π /2
FIGURE 2.P–1
Ideal lowpass ﬁlter with elliptical support in the frequency domain, with cutoff frequencies
ωc1 and ωc2.
terms of the Fourier transform X (ω1,ω2). Also provide a labeled sketch of Xc
based on the (discrete-space) Fourier transform X shown in Figure 2.P–1.
3. A rectangularly bandlimited, continuous-space signal sc(x,y) is sampled at sam-
ple spacing T1 and T2, respectively, which is sufﬁcient to avoid spatial frequency
aliasing. The resulting discrete-space signal (image) is s(n1,n2). It is desired to
process this signal to obtain what would have been obtained had the sample
spacing been halved—i.e., using T1/2 and T2/2, respectively. What should the
ideal ﬁlter impulse response h(n1,n2) be? Call the low-rate signal sL and the
high-rate signal sH.
4. In the reconstruction formula (2.1–4), show directly that the interpolation func-
tions provide the correct result by evaluating (2.1–4) at the sample locations
t1 = n1T1, t2 = n2T2 for the critical sampling case.
5. It is known that the continuous time 1-D signal xc(t) = exp−αt2 has Fourier
transform Xc() =
q
α
π exp−1
4α 2, where α > 0. Next, we sample x(t) at times
t = n to obtain a discrete-time signal x(n) = xc(n) = exp−αn2.
(a) Write the Fourier transform X(ω) of x(n) in terms of the parameter α. This
should be an aliased sum involving Xc. Use sum index k.
(b) Find an upper bound on α so that the k = ±1 aliased terms in X(ω) are no
larger than 10−3q
α
π .
(c) Consider the 2-D signal x(n1,n2) = exp−α(n2
1 + n2
2), and repeat part (a),
ﬁnding now X(ω1,ω2). Use aliased sum index (k1,k2).
(d) Find an upper bound on α so that the (k1,k2) = (±1,0) and (0,±1) aliased
terms in X(ω1,ω2) are no larger than 10−3 α
π .

Problems
71
6. An ideal lowpass ﬁlter for upsampling is found in Section 2.3:
h(n1,n2) =
sinπ

n1
L1

π

n1
L1

sinπ

n2
L2

π

n2
L2
 .
Find the Fourier transform of this ﬁlter and determine its passband support and
passband gain.
7. If we use 2 × 2 ideal interpolation for the input signal x(n1,n2) = sin(2πn1/16 +
4πn2/16), then the ideal interpolation output should be sin(2πn1/32 +
4πn2/32). Show that the system of Figure 2.3–10 achieves this by checking the
Fourier transform of the input x, the Fourier transform of the upsampled signal
xu, and the effect of the ideal lowpass ﬁlter, as determined in the reconstruction
equations that follow Figure 2.3–10 in the text. Note that the passband gain of
this ﬁlter is here given as 2 · 2 = 4.
8. Consider the 2-D signal with continuous-space Fourier transform support as
shown in the gray area of Figure 2.P–2:
Ω1
Ω2
−Ωc1
Ωc2
−Ωc2
Ωc1
FIGURE 2.P–2
Fourier transform support indicated by gray regions.
(a) What is the minimum permissible rectangular sampling pattern, based on an
assumption of [−c1,c1] × [−c2,c2] Fourier transform support?
(b) What is the minimum permissible sampling pattern for the Fourier transform
support shown in the ﬁgure?
In each case, answer by specifying the sample matrix V.
9. Consider the continuous-space signal xc(t1,t2) with bandpass Fourier transform
support as indicated by the dark gray areas in Figure 2.P–3. The support of
Xc(1,2) does not include the boundaries of the dark gray boxes.

72
CHAPTER 2 Sampling in Two Dimensions
0
Ω1
Ω2
+π
+2π
+3π
−3π
−2π
−π
FIGURE 2.P–3
Bandpass Fourier transform support of Xc(1,2) indicated by dark gray areas.
(a) What is the minimal spatial sampling pattern to capture the full content of xc
so that, from these samples alone, one can reconstruct the continuous-space
function xc without any error?
(b) Specify the corresponding reconstruction system in detail.
10. Use Figure 2.2–4 to ﬁnd the diamond-shaped unit cell in the frequency domain
for sampling matrix V =
1
1
1
−1

. (Hint: Draw straight lines connecting the
alias anchor points.) What does the midpoint of each line tell us about the
boundary of the unit cell?
11. Find the impulse response corresponding to a continuous-space ideal diamond-
shaped lowpass ﬁlter, with passband equal to the frequency domain unit cell
found in problem 10. This is the ideal anti-alias ﬁlter that should be used prior
to such sampling. Assume T1 = T2 = 1.
12. Show that Xd(ω1,ω2) given in (2.3–5) is rectangularly periodic with period 2π ×
2π, as it must be to be correct.
13. Consider the 2-D signal from problem 7 of Chapter 1,
x(n1,n2) = 4 + 2cos
2π
8 (n1 + n2)

+ 2cos
2π
8 (n1 −n2)

,
for all −∞< n1,n2 < +∞.
(a) Let x now be input to the 2 × 2 decimator in Figure 2.P–4
x(n1,n2)
xd(n1,n2),
2× 2
FIGURE 2.P–4
A 2 × 2 decimator.

Problems
73
Give a simple expression for the output xd(n1,n2) and its Fourier transform
Xd(ω1,ω2). (Hint: Consider each of the terms in x separately, starting with
the constant term 4.)
(b) Check and verify that
Xd(ω1,ω2) = 1
4
1
X
l1=0
1
X
l2=0
X
ω1 −2πl1
2
, ω2 −2πl2
2

.
14. Let the signal x(n1,n2) have Fourier transform
X(ω1,ω2) =
1,
on [−π,+π] × [−π
2 ,+ π
2 ],
0,
otherwise on [−π,+π]2.
We then subsample 2 × 2 the signal x to obtain
xd(n1,n2) = x(2n1,2n2).
Working directly in the 2-D Fourier transform domain, ﬁnd and plot the corre-
sponding Fourier transform of the subsampled signal Xd(ω1,ω2) on [−π,+π]2.
15. Consider a fullband signal with circular symmetric frequency domain contour
plot as sketched in Figure 2.P–5.
0
ω1
ω2
+π
+π
−π
−π
−π/2
−π/2
+π/2
+π/2
FIGURE 2.P–5
Contour plot sketch of a fullband signal.
Use a ﬁlter with frequency-domain support as shown in Figure 2.3–6 to gen-
erate the so-called HL subband. Usually this signal is then decimated 2 × 2 for
efﬁciency, generating what is normally called the subband signal xHL(n1,n2).
Sketch the frequency domain contour plot of this signal. (Note that there has
been an inversion of higher and lower horizontal frequencies.)

74
CHAPTER 2 Sampling in Two Dimensions
16. This problem relates to the formula for frequency domain effect of general sub-
sampling (2.4–2) and its derivation. Things are made much simpler by using the
alternative deﬁnition of the discrete-space Fourier transform [5]:
X′(ω) ≜
X
n
x(n)exp(−jωTVn)
= X(VTω).
With this deﬁnition, the warping evident in (2.2–4) does not occur, and we have
the simpler expression for aliasing due to general sampling:
X′(ω) =
1
|detV|
(X
all k
Xc(ω−Uk)
)
.
Note that in the rectangular case, there is no distinction between these two,
except for the familiar scaling of the analog frequency axes.
(a) Using the alternative FT, ﬁnd the frequency domain equation corresponding
to direct sampling with sampling matrix MV.
(b) Show that this same equation must correspond to decimating by M after ﬁrst
sampling with V.
(c) Use your result in part (b) to justify (2.4–2) as expressed in terms of the
alternative FT
X′
d(ω) =
1
|detM|
( X
certain k
X(ω −2πM−Tk)
)
.
(d) Now, can you justify our using k = (0,0)T and k = (1,0)T in Example
2.4–2?
REFERENCES
[1] A. Oppenheim, R. W. Schafer, and J. R. Buck, Discrete-Time Signal Processing, 2nd Ed.,
Prentice-Hall, Englewood Cliffs, NJ, 1999.
[2] A. Gersho and R. M. Gray, Vector Quantization and Signal Compression, Kluwer
Academic Press, 1992.
[3] D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing,
Prentice-Hall, Englewood Cliffs, NJ, 1983.
[4] P. P. Vaidyanathan, Multirate Systems and Filter Banks, (see Section 12.4), Prentice-Hall,
Englewood Cliffs, NJ, 1993.
[5] R. M. Mersereau and T. C. Speake, “The Processing of Periodically Sampled Multi-
dimensional Signals,” IEEE Trans. Accoust., Speech, and Signal Process., vol. ASSP-31,
pp. 188–194, February 1983.

CHAPTER
Two-Dimensional Systems
and Z-Transforms
3
In this chapter we look at the 2-D Z-transform. It is a generalization of the 1-D
Z-transform used in the analysis and synthesis of 1-D linear constant coefﬁcient dif-
ference equation-based systems. In two and higher dimensions, the corresponding
linear systems are partial difference equations. The analogous continuous parameter
systems are partial differential equations. In fact, one big application of partial dif-
ference equations is in the numerical or computer solution of the partial differential
equations of physics. We also look at LSI stability in terms of its Z-transform system
function and present several stability conditions in terms of the zero-root locations of
the system function.
3.1 LINEAR SPATIAL OR 2-D SYSTEMS
The spatial or 2-D systems we will mainly be concerned with are governed by dif-
ference equations in the two variables n1 and n2. These equations can be realized by
logical interconnection of multipliers, adders, and shift or delay elements via either
software or hardware. For the most part, the coefﬁcients of such equations will be
constant, hence the name linear constant coefﬁcient difference equations (LCCDEs).
The study of 2-D or partial difference equations is much more involved than that
of the corresponding 1-D LCCDEs, and much less is known about the general case.
Nevertheless, many practical results have emerged, the most basic of which will be
presented here. We start with the general input/output equation:
X
(k1,k2)∈Ra
ak1,k2y(n1 −k1,n2 −k2) =
X
(k1,k2)∈Rb
bk1,k2x(n1 −k1,n2 −k2),
(3.1–1)
where x is the known input and y is the output to be determined. We consider the
coefﬁcients ak1,k2 and bk1,k2 to be arrays of real numbers and call bk1,k2 the feedfor-
ward coefﬁcients and ak1,k2 the feedback coefﬁcients. We wish to solve (3.1–1) by
ﬁnding output value y for every point in a prescribed region Ry given needed input
values x plus output values y on the boundary of Ry. We denote this boundary region
somewhat imprecisely as Rbc. The highest values of k1 and k2 on the left-hand side
of (3.1–1) determine the order of the difference equation. In general, such equations
have to be solved via matrix or iterative methods, but our main interest is 2-D ﬁlters
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00003-5
c⃝2012 Elsevier Inc. All rights reserved.
75

76
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
n1
n2
bc
bc
bc
a
y
FIGURE 3.1–1
An example of the solution region of a spatial difference equation solution region using a
nonsymmetric half-plane (NSHP) coefﬁcient support Ra.
for which the output y can be calculated in a recursive manner from the input x by
scanning through the data points (n1,n2).
Keeping only the output value y(n1,n2) on the left-hand side of (3.1–1), assuming
a0,0 ̸= 0 and (0,0) ∈Ra, we can write
y(n1,n2) = −
X
(k1,k2)∈Ra
a′
k1,k2y(n1 −k1,n2 −k2) +
X
(k1,k2)∈Rb
b′
k1,k2x(n1 −k1,n2 −k2),
(3.1–2)
where the a′
k1,k2 and b′
k1,k2 are the normalized coefﬁcients (i.e., those divided by a0,0).
Then, depending on the shape of the region Ra, we may be able to calculate the solu-
tion recursively. For example, we would say that the direction of recursion of (3.1–2)
is “downward and to the right” in Figure 3.1–1, which shows a scan proceeding left-
to-right and top-to-bottom.1 Note that the special shape of the output mask Ra in
Figure 3.1–1 permits such a recursion because of its property of not including any
outputs that have not already been scanned and processed in the past (i.e., “above
and to the left”).
Example 3.1–1 shows how such a recursion proceeds in the case of a simple
ﬁrst-order 2-D difference equation.
Example 3.1–1: Simple Difference Equation
We now consider the simple LCCDE
y(n1,n2) = x(n1,n2) + 1
2[y(n1 −1,n2) + y(n1,n2 −1)]
(3.1–3)
1The vertical axis is directed downward, as is common in image processing, where typically the
processing proceeds from top to bottom of the image.

3.1 Linear Spatial or 2-D Systems
77
to be solved over the ﬁrst quadrant (i.e., Ry = {n1 ≥0,n2 ≥0}). In this example, we
assume that the input x is everywhere zero, but that the boundary conditions given on
Rbc = {n1 = −1} ∪{n2 = −1} are nonzero and speciﬁed by
y(−1,1) = y(−1,2) = y(−1,3) = 1,
y(−1,else) = 0,
y(else,−1) = 0.
To calculate the solution recursively, we ﬁrst determine a scanning order. In this case, it is
the so-called raster scan used in video monitors: ﬁrst we process the row n2 = 0, starting
at n1 = 0 and incrementing by one each time; then we increment n2 by one, and process
the next row. With this scanning order, the difference equation (3.1–3) is seen to only use
previous values of y at the “present time,” and so is recursively calculable. Proceeding to
work out the solution, we obtain
n2↓
0
0
1
1
1
0
0
···
0
0
1
2
3
4
7
8
7
16
7
32
···
0
0
1
4
1
2
11
16
18
32
25
64
···
0
0
1
8
5
16
16
32
34
64
···
···
0
0
1
16
···
→n1.
In Example 3.1–1 we have computed the solution to a spatial difference equation
by recursively calculating out the values in a suitable scanning order, for a nonzero
set of boundary “initial” conditions, but with zero input sequence. In Example 3.1–2
we consider the same 2-D difference equation to be solved over the same output
region, but with zero initial boundary conditions and a nonzero input. By linearity of
the partial difference equation, the general case of nonzero boundaries and nonzero
input follows by superposition of these two zero-input and zero-state solutions.
Example 3.1–2: Simple Difference Equation (cont’d)
We consider the simple LCCDE
y(n1,n2) = x(n1,n2) + 1
2[y(n1 −1,n2) + y(n1,n2 −1)]
(3.1–4)
to be solved over output solution region Ry = {n1 ≥0,n2 ≥0}. The boundary condi-
tions given on Rbc = {n1 = −1} ∪{n2 = −1} are taken as all zeros. The input sequence
is x(n1,n2) = δ(n1,n2). Starting at (n1,n2) = (0,0), we begin to generate the impulse
response of the difference equation. Continuing the recursive calculation for the next few

78
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
columns and rows, we obtain
n2↓
0
0
0
0
0
0
0
···
0
1
1
2
1
4
1
8
1
16
···
···
0
1
2
1
2
3
8
4
16
5
32
···
···
0
1
4
3
8
6
16
10
32
···
···
···
0
1
8
4
16
10
32
···
···
···
···
→n1.
It turns out that this spatial impulse response has a closed-form analytic solution
[1, 2],
y(n1,n2) = h(n1,n2) =
 
n1 + n2
n1
!
2−(n1+n2)u++(n1,n2),
where
 
n1 + n2
n1
!
is the combinatorial symbol for “n1 + n2 things taken n1 at a time,”
 
n1 + n2
n1
!
= (n1 + n2)!
n1!n2!
,
for
n1 ≥0,n2 ≥0,
with 0! taken as 1, and where u++(n1,n2) = u(n1,n2) is the ﬁrst quadrant unit step
function.
Though it is usually the case that 2-D difference equations do not have a closed-
form impulse response, the ﬁrst-order difference equation of Example 3.1–2 is one
of the few exceptions. From these two examples, we can see it is possible to write
the general solution to a spatial linear difference equation as a sum of a zero-input
solution given rise by the boundary values plus a zero-state solution driven by the
input sequence
y(n1,n2) = yZI(n1,n2) + yZS(n1,n2).
This generalizes the familiar 1-D systems theory result. To see this, consider a third
example with both nonzero input and nonzero boundary conditions. Then note that
the sum of the two solutions from these examples will solve this new problem.
In general, and depending on the output coefﬁcient support region Ra, there can
be different recursive directions for (3.1–1), which we can obtain by bringing other
terms to the left-hand side and recursing in other directions. For example, we can take
(3.1–4) from Example (3.1–2) and bring y(n1,n2 −1) to the left-hand side to yield
y(n1,n2 −1) = −2y(n1,n2) + 2x(n1,n2) + y(n1 −1,n2),
or equivalently,
y(n1,n2) = −2y(n1,n2 + 1) + y(n1 −1,n2 + 1) + 2x(n1,n2 + 1),
with direction of recursion upwards, to the right or left. So the direction in which a
2-D difference equation can be solved recursively, or recursed, depends on the
support of the output or feedback coefﬁcients (i.e., Ra). For a given direction of
recursion, we can calculate the output points in particular orders that are constrained

3.2 Z-Transforms
79
by the shape of the coefﬁcient support region Ra, resulting in an order of computa-
tion. In fact, there are usually several such orders of computation that are consistent
with a given direction of recursion. Further, usually several output points can be
calculated in parallel to speed the recursion.
Such recursive solutions are appropriate when the boundary conditions are only
imposed “in the past” of the recursion—i.e., not on any points that must be calculated.
In particular, with reference to Figure 3.1–1, we see no boundary conditions on the
bottom of the solution region. In the more general case where there are both “initial”
and “ﬁnal” conditions, we can fall back on the general matrix solution for a ﬁnite
region.
To solve LCCDE (3.1–1) in a ﬁnite solution region, we can use linear algebra and
form a vector of the solution y scanned across the region in any prespeciﬁed manner.
Doing the same for the input x and the boundary conditions ybc, we can write all the
equations with one very large dimensioned vector equation,
x = Ay + Bybc,
for appropriately deﬁned coefﬁcient matrices A and B. For a 1000 × 1000 image, the
dimension of y would be 1,000,000. Here, Ay provides the terms of the equations
where y is on the region, and Bybc provides the terms when y is on the boundary. A
problem at the end of the chapter asks you to prove this fact.
If the solution region of the LCCDE is inﬁnite, then as in the 1-D case, it is often
useful to express the solution in terms of a Z-transform, which is our next topic.
3.2 Z-TRANSFORMS
Deﬁnition 3.2–1: Z-Transform
The 2-D Z-transform of a two-sided sequence x(n1,n2) is deﬁned as follows:
X(z1,z2) ≜
+∞
X
n1=−∞
+∞
X
n2=−∞
x(n1,n2)z−n1
1
z−n2
2
,
(3.2–1)
where (z1,z2) ∈C2, the “2-D” (really 4-D) complex Cartesian product space. In general,
there will be only some values of (z1,z2)T ≜z for which this double sum will converge.
Only absolute convergence,
+∞
X
n1=−∞
+∞
X
n2=−∞
|x(n1,n2)z−n1
1
z−n2
2
|
=
+∞
X
n1=−∞
+∞
X
n2=−∞
|x(n1,n2)| |z1|−n1 |z2|−n2 < ∞,
is considered in the theory of complex variables [3, 4], so we look for joint values of |z1|
and |z2| that will yield absolute convergence. The set of z for which this occurs is called

80
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
the region of convergence, denoted Rx. In summary, a 2-D Z-transform is speciﬁed by its
functional form X(z1,z2) and its convergence region Rx.
Similar to the 1-D case, the Z-transform is simply related to the Fourier transform,
when both exist:
X(z1,z2)|z1=ejw1
z2=ejw2
= X(ω1,ω2),
with the customary abuse of notation.2
A key difference from the 1-D case is that the 2-D complex variable z exists in
a 4-D space and is hard to visualize. The familiar unit circle becomes something a
bit more abstract, the unit bi-circle in C2 [4]. The unit disk then translates over to
the unit bi-disk, {|z1|2 + |z2|2 ≤1} ∈C2. Another key difference for two and higher
dimensions is that the zeros of the Z-transform are no longer isolated. Two different
zero loci can intersect.
Example 3.2–1: Zero Loci
Consider the following signal x(n1,n2):
n2\n1
0
1
0
1
2
1
2
1
,
with assumed support {0,1} × {0,1}. This simple four-point signal could serve, after nor-
malization, as the impulse response of a simple directional spatial averager, giving an
emphasis to structures at 45◦. Proceeding to take the Z-transform, we obtain
X(z1,z2) = 1 + 2z−1
1
+ 2z−1
2
+ z−1
1 z−1
2 .
This Z-transform X is seen to exist for all C2 except for z1 = 0 or z2 = 0. Factoring X, we
obtain
X(z1,z2) = 1 + 2z−1
2
+ z−1
1 (2 + z−1
2 ),
which upon equating to zero gives the zero (z1,z2) locus
z1 = −2z2 + 1
z2 + 2 ,
for
z2 ̸= −2,
z1 = +∞, otherwise.
2To avoid confusion, when the same symbol X is being used for two different functions, we note that
the Fourier transform X(ω1,ω2) is a function of real variables, while the Z-transform X(z1,z2) is a
function of complex variables. A pitfall, for example X(1,0), can be avoided by simply writing either
X(ω1,ω2)|ω1=1
ω2=0
or X(z1,z2)|z1=1
z2=0
, whichever is appropriate, in cases where confusion could arise.

3.3 Regions of Convergence
81
We notice that for each value of z2 there is a corresponding value of z1 for which the
Z-transform X takes on the value of zero. Notice also that, with the possible exception of
z2 = −2, the zero locus value z1 = f (z2) is a continuous function of the complex variable
z2. This ﬁrst-order 2-D system thus has one zero locus.
We next look at a more complicated second-order case where there are two root
loci that intersect, but without being identical; therefore, we cannot just cancel the
factors out. In the 1-D case that we are familiar with, the only way there can be a
pole and zero at the same z location is when the numerator and denominator have
a common factor. Example 3.2–2 shows that this is not true in general for higher
dimensions.
Example 3.2–2: Intersecting Zero Loci
Consider the Z-transform
X(z1,z2) = (1 + z1)/(1 + z1z2),
for which the zero locus is easily seen to be (z1,z2) = (−1,∗), and the pole locus is
(z1,z2) = (α,−1/α), where ∗represents an arbitrary complex number and α is any
nonzero complex number. These two distinct zero sets are seen to intersect at (z1,z2) =
(−1,1). One way to visualize these root loci is root mapping, which we will introduce later
when we study the stability of 2-D ﬁlters (see Section 3.5).
Next, we turn to the topic of convergence for the 2-D Z-transform. As in the 1-D
case, we expect that knowledge of the region in z space where the series converges
will be essential to the uniqueness of the transform, and hence to its inversion.
3.3 REGIONS OF CONVERGENCE
Given a 2-D Z-transform X(z1,z2), its region of convergence (ROC) is given as the
set of z for which
=
+∞
X
n1 =−∞
+∞
X
n2=−∞
|x(n1,n2)|
z1|−n1 z2|−n2
=
+∞
X
n1=−∞
+∞
X
n2=−∞
|x(n1,n2)| r−n1
1
r−n2
2
< ∞,
(3.3–1)
where r1 ≜|z1| and r2 ≜|z2| are the moduli of the complex numbers z1 and z2. The
ROC can then be written in terms of such moduli values as
Rx ≜{(z1,z2)| |z1| = r1, |z1| = r2, and (3.3 −1) holds}.

82
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
0
|z1|
|z2|
1
1
|z1
0|
|z2
0|
+
FIGURE 3.3–1
The 2-D complex magnitude plane. Here, (•) denotes the unit bi-circle and (+) denotes an
arbitrary point at (z0
1,z0
2).
Since this speciﬁcation only depends on magnitudes, we can plot ROCs in the
convenient magnitude plane (Figure 3.3–1).
Example 3.3–1: Z-Transform Calculation
We consider the spatial, ﬁrst quadrant step function
x(n1,n2) = u++(n1,n2) = u(n1,n2).
Taking the Z-transform, we have the following from (3.2–1):
X(z1,z2) =
+∞
X
n1=−∞
+∞
X
n2=−∞
u(n1,n2)z−n1
1
z−n2
2
=
+∞
X
n1=0
+∞
X
n2=0
z−n1
1
z−n2
2
=
+∞
X
n1=0
z−n1
1
·
+∞
X
n2=0
z−n2
2
=
1
1 −z−1
1
1
1 −z−1
2
for
|z1| > 1
and
|z2| > 1,
=
z1
z1 −1
z2
z2 −1
with
Rx = {|z1| > 1,|z2| > 1}.
We can plot this ROC on the complex z-magnitude plane as in Figure 3.3–2. Note that
we have shown the ROC as the gray region and moved it slightly outside the lines |z1| = 1
and |z2| = 1 in order to emphasize that this open region does not include these lines. The
zero loci for this separable signal are the manifold z1 = 0 and the manifold z2 = 0. These
two distinct loci intersect at the complex point z1 = z2 = 0. The pole loci are also two in
number and occur at the manifold z1 = 1 and the manifold z2 = 1. We note that these two
pole loci intersect at the single complex point z1 = z2 = 1.

3.3 Regions of Convergence
83
0
1
1
|z1|
|z2|
FIGURE 3.3–2
The gray area illustrates the ROC for the Z-transform of the ﬁrst quadrant unit step function
u(n1,n2) = u++(n1,n2).
Next, we consider how the Z-transform changes when the unit step switches to
another quadrant.
Example 3.3–2: Unit Step Function in the Fourth Quadrant
Here, we consider a unit step function that has support on the fourth quadrant. We denote
it as u+−(n1,n2):
u+−(n1,n2) ≜
(
1,
n1 ≥0,n2 ≤0,
0,
else.
So, setting x(n1,n2) = u+−(n1,n2), we next compute
X(z1,z2) =
+∞
X
n1=−∞
+∞
X
n2=−∞
u+−(n1,n2)z−n1
1
z−n2
2
=
+∞
X
n1=0
z−n1
1
·
0
X
n2=−∞
z−n2
2
=
+∞
X
n1=0
z−n1
1
·
+∞
X
n′
2=0
z
n′
2
2
with
n′
2 ≜−n2
=
1
1 −z−1
1
1
1 −z2
for
|z1| > 1
and
|z2| < 1,
= −
z1
z1 −1
1
z2 −1
with
Rx = {|z1| > 1,|z2| < 1}.
The ROC is shown as the gray area in Figure 3.3–3.
All four quarter-plane support, unit step sequences have the special property of
separability. Since the Z-transform is a separable operator, this makes the calculation
split into the product of two 1-D transforms in the n1 and n2 directions, as we have

84
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
0
1
1
|z1|
|z2|
FIGURE 3.3–3
The ROC (gray area) for the fourth quadrant unit step function u+−(n1,n2).
just seen. The ROC then factors into the Cartesian product of the two 1-D ROCs. We
look at a more general case next.
More General Case
In general, we have the Z-transform
X(z1,z2) = B(z1,z2)
A(z1,z2),
where both B and A are polynomials in coefﬁcients of some partial difference
equation,
B(z1,z2) =
+N1
X
n1=−N1
+N2
X
n2=−N2
b(n1,n2)z−n1
1
z−n2
2
and
A(z1,z2) =
+N1
X
n1=−N1
+N2
X
n2=−N2
a(n1,n2)z−n1
1
z−n2
2
.
To study the existence of this Z-transform, we focus on the denominator and rewrite
A as
A(z1,z2) = z−N1
1
z−N2
2
eA(z1,z2),
where eA is a strict-sense polynomial in z1 and z2 (i.e., no negative powers of z1 or
z2). Grouping together terms in zn
1, we can write
eA(z1,z2) =
eN1
X
n=0
an(z2)zn
1,
yielding eN1 poles (N1 at most!) for each value of z2,
zi
1 = fi(z2),
i = 1,...,eN1.
A sketch of such a pole surface is plotted in Figure 3.3–4. Note that we are only
plotting the magnitude of one surface here, and this plot therefore does not tell the

3.4 Some Z-Transform Properties
85
Re(z2)
Im(z2)
|zi
1|=|fi(z2)|
0
FIGURE 3.3–4
Sketch of pole magnitude |zi
1| surface as a function of a point in the z2 complex plane.
whole story. Also there are eN1 such sheets. Of course, there will be a similar number
of zero loci or surfaces that come about from the numerator
eB(z1,z2) =
eN1
X
n=0
bn(z2)zn
1,
where B(z1,z2) = z−N1
1
z−N2
2
eB(z1,z2). Note that these zero surfaces can intersect the
pole surfaces (as well as each other) without being identical. Thus indeterminate
0
0 situations can arise that cannot be simply canceled out. One classic example
is [5]
z1 + z2 −2
(z1 −1)(z2 −1),
which evaluates to 0
0 at the point (z1,z2) = (1,1), and yet has no cancelable factors.
3.4 SOME Z-TRANSFORM PROPERTIES
Here, we list some useful properties of the 2-D Z-transform that we will use in the
sequel. Many are easy extensions of known properties of the 1-D Z-transform, but
some are essentially new. In listing these properties, we introduce the symbol Z for
the 2-D Z-transform operator.
Linearity property:
Z{ax(n1,n2) + by(n1,n2)} = aX(z1,z2) + bY(z1,z2),
with ROC = Rx ∩Ry.

86
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
Delay property:
Z{x(n1 −k1,n2 −k2)} = X(z1,z2)z−k1
1
z−k2
2
,
with ROC = Rx.
Convolution property:
Z{x(n1,n2) ∗y(n1,n2)} = X(z1,z2)Y(z1,z2),
with ROC = Rx ∩Ry.
Symmetry properties:
Z{x∗(n1,n2)} = X∗(z∗
1,z∗
2),
with ROC = Rx.
Z{x(−n1,−n2)} = X(z−1,z−1
2 ),
with ROC = {(z1,z2)|(z−1,z−1
2 ) ∈Rx}.
The proofs of these properties are very similar to the proofs in the 1-D case.
Theorem 3.4–1 provides a statement and proof of the very important 2-D convolution
property.
Theorem 3.4–1: Z-Transform Convolution
Let x(n1,n2) ←→X(z1,z2) and y(n1,n2) ←→Y(z1,z2); then
Z{x(n1,n2) ∗y(n1,n2)} = X(z1,z2)Y(z1,z2),
with ROC = Rx ∩Ry.
Proof We use the vector index notation, n ≜(n1,n2) and z ≜(z1,z2), and deﬁne the
output g(n) ≜x(n) ∗y(n) = P
k x(k)y(n −k), and its 2-D Z-transform
G(z) =
X
n
g(n)z−n.
Note the perhaps strange symbol z−n ≜z−n1
1
z−n2
2
, which is used for notational simplicity.
Then we can write
G(z) =
X
n
X
k
x(k)y(n −k)z−n
=
X
n
X
k
x(k)z−ky(n −k)z−(n−k)
=
X
k
x(k)z−k
"X
n
y(n −k)z−(n−k)
#
=
X
k
x(k)z−k Y(z)
= X(z)Y(z).
For the ROC, clearly we need to have z in both regions of convergence for the product to be
deﬁned; thus we set Rg = Rx ∩Ry. If the intersection is null, then there is no Z-transform
in this case.
A “new” named property for 2-D Z-transforms is given next. It is new in the sense
that the corresponding 1-D result is rather insigniﬁcant.

3.4 Some Z-Transform Properties
87
Linear Mapping of Variables
Consider two signals x(n1,n2) and y(n1,n2), which are related by the so-called linear
mapping of variables
 n′
1
n′
2

=
l11
l12
l21
l22
n1
n2

,
so that x(n1,n2) = y(n′
1,n′
2) = y(l11n1 + l12n2,l21n1 + l22n2), where the matrix com-
ponents lij are all integers, as required. The following relation then holds for the
corresponding Z-transforms X and Y:
Y(z1,z2) = X

zl11
1 zl21
2 ,zl12
1 zl22
2

,
(3.4–1)
with convergence region
Ry =

(z1,z2)


zl11
1 zl21
2 ,zl12
1 zl22
2

∈Rx

.
This integer mapping of variables gives a warping of the spatial points (n1,n2),
which is useful when we discuss stability tests and conditions for systems via their
Z-transforms in a later section. A 1-D corresponding result might be the sample rate
increase x(n) = y(n′) = y(2n). Note that we cannot, in general, go back from x to y in
either one or two dimensions. However, in two dimensions, and for nontrivial cases,
the inverse mapping matrix may have integer coefﬁcients. In that case, the two signals
carry the same information and are just warped versions of each other. Note that the
only 1-D case then would be x(n) = y(±n), which is almost completely constrained
and not very interesting.
Example 3.4–1: Linear Mapping of Variables
In this example we use linear integer mapping of variables to map a signal from a general
wedge support3 to a ﬁrst quadrant support. We set
x(n1,n2) = y(n1 + n2,n2),
with x having the support indicated in Figure 3.4–1. Now this transformation of variables
also has an inverse with integer coefﬁcients, so it is possible to also write y in terms of x:
y(n′
1,n′
2) = x(n′
1 −n′
2,n′
2).
By the relevant Z-transform property, we can say
Y(z1,z2) = X(z1,z1z2),
(3.4–2)
3By “wedge support” we mean that the signal support is ﬁrst quadrant plus a wedge from the second
quadrant, with the wedge indicated by a line at angle θ. For this example, θ = 45◦.

88
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
but also, because the inverse linear mapping is also integer valued, the same property
says
X(z1,z2) = Y(z1,z−1
1 z2),
which is alternatively easily seen by solving (3.4–2) for X(z1,z2).
y
n1
n2
n2
n1
x
FIGURE 3.4–1
Example of linear mapping of variables.
In a later section we will develop Z-transform–based stability tests for ﬁrst quad-
rant ﬁlters. Linear mapping of variables will be a way to extend these tests to other,
more general ﬁlters—i.e., those whose denominator coefﬁcient support is wedge
shaped.
Inverse Z-Transform
The inverse Z-transform is given by the contour integral [3, 4]
x(n1,n2) =
1
(2πj)2
I
C1
I
C2
X(z1,z2)zn1−1
1
zn2−1
2
dz1dz2,
where the integration path C1 × C2 lies completely in Rx, the ROC of X, as it must.
We can think of this 2-D inverse Z-transform as the concatenation of two 1-D inverse
Z-transforms:
x(n1,n2) =
1
2πj
I
C2

1
2πj
I
C1
X(z1,z2)zn1−1
1
dz1

zn2−1
2
dz2.
(3.4–3)
For a rational function X, the internal inverse Z-transform on the variable z1 is
straightforward albeit with poles and zeros that are a function of the other variable
z2. For example, either partial fraction expansion or the residue method [3] could
be used to evaluate the inner contour integral. Unfortunately, the second, or outer,
inverse Z-transform over z2 is often not of a rational function,4 and, in general, is not
amenable to closed-form expression. Some simple cases can be done, though.
4The reason it is not generally a rational function has to do with the formulas for the roots of a poly-
nomial. In fact, it is known that above fourth order, these 1-D polynomial roots cannot be expressed in
terms of a ﬁnite number of elementary functions of the coefﬁcients [6].

3.4 Some Z-Transform Properties
89
Example 3.4–2: Simple Example Resulting in a Closed-Form Solution
A simple example where the preceding integrals can be easily carried out results from the
Z-transform function
X(z1,z2) =
1
1 −az−1
1 z2
,
with
Rx =

|z2| < |z1|
|a|

,
where we take the case |a| < 1. We can illustrate this region of convergence as shown in
Figure 3.4–2.
0
1
1
|z1|
|z2|
FIGURE 3.4–2
Illustration of ROC (shaded area) of the example Z-transform.
Proceeding with the inverse transform calculation (3.4–3), we get
x(n1,n2) =
1
2πj
I
C2

1
2πj
I
C1
1
1 −az−1
1 z2
zn1−1
1
dz1

zn2−1
2
dz2.
Now, the inner integral corresponds to the ﬁrst-order pole at z1 = az2, whose 1-D inverse
Z-transform can be found using the residue method, or any other 1-D evaluation
method, as
1
2πj
I
C1
1
1 −az−1
1 z2
zn1−1
1
dz1 = (az2)n1 u(n1).
Thus the overall 2-D inverse Z-transform reduces to
x(n1,n2) =
1
2πj
I
C2
(az2)n1 u(n1)zn2−1
2
dz2
= an1u(n1) 1
2πj
I
C2
zn1
2 zn2−1
2
dz2
= an1u(n1)δ(n1 + n2),

90
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
with support on the diagonal set {n1 ≥0} ∩{n2 = −n1}. Rewriting, this result becomes
x(n1,n2) =
(
an1,
n1 ≥0
and
n2 = −n1,
0,
else.
Other methods to invert the 2-D Z-transform are:
•
Direct long division of the polynomials
•
Known series expansion
•
Use of Z-transform properties on known transform pairs
Example 3.4–3: Long Division Method for Inverse Z-Transform
We illustrate the long division method with the following example. Let
X(z1,z2) =
1
1 −z−1
1 z2
,
with
Rx = {|z2| < |z1|}.
We proceed to divide the denominator into the numerator as follows:
1 + z−1
1 z2 + z−2
1 z2
2 + ···
1 −z−1
1 z2
√
1
1 −z−1
1 z2
z−1
1 z2
z−1
1 z2 −z−2
1 z2
2
z−2
1 z2
2
z−2
1 z2
2 −z−3
1 z3
2 .
+ ···
So
1
1 −z−1
1 z2
= 1 + z−1
1 z2 + z−2
1 z2
2 + z−3
1 z3
2 + ··· ,
which converges for
z−1
1 z2
 < 1 (i.e., |z2| < |z1|).
The 1-D partial fraction expansion method for inverse Z-transform does not carry
over to the 2-D case. This is a direct result of the nonfactorability of polynomials in
more than one variable.
Example 3.4–4: 2-D Polynomials Do Not Factor
In the 1-D case, all high-order polynomials factor into ﬁrst-order factors, and this prop-
erty is used in a partial fraction expansion. In the multidimensional case, polynomial

3.4 Some Z-Transform Properties
91
factorization cannot be guaranteed anymore. In fact, most of the time it is absent, as the
following example illustrates. Consider a signal x, with 3 × 3 support, corresponding to a
2 × 2 order polynomial in z1 and z2. If we could factor this polynomial into ﬁrst-order fac-
tors, that would be the same as representing this signal as the convolution of two signals,
say a and b, of support 1 × 1. We would have
x02
x12
x22
x01
x11
x21
x00
x10
x20
= a01
a11
a00
a10
⊛b01
b11
b00
b10
,
or in Z-transforms (polynomials), we would have
X(z1,z2) = A(z1,z2)B(z1,z2).
The trouble here is that a general such x has nine degrees of freedom, while the total num-
ber of unknowns in a and b is only eight. If we considered factoring a general N × N array
into two factors of order N/2 × N/2, the deﬁciency in number of unknowns (variables)
would be much greater—i.e., N2 equations versus 2(N/2)2 = N2/2 unknowns!
The fact that multidimensional polynomials do not factor has a number of other
consequences, beyond the absence of a partial fraction expansion. It means that in
ﬁlter design, one must take the implementation into account at the design stage. We
will see in Chapter 5 that if one wants, say, the 2-D analog of second-order factors,
then one has to solve the design approximation problem with this constraint built in.
But also a factored form may have larger support than the corresponding nonfactored
form, thus possibly giving a better approximation for the same number of coefﬁ-
cients. So this nonfactorability is not necessarily bad. Since we can always write the
preceding ﬁrst-order factors as separable, then 2-D polynomial factorability down to
ﬁrst-order factors would lead back to a ﬁnite set of isolated poles in each complex
plane.
Generally, we think of the Fourier transform as the evaluation of the Z-transform
on the unit polycircle {|z1| = |z2| = 1}; however, this assumes the polycircle is in
the region of convergence of X(z1,z2), which is not always true. The next example
demonstrates this exception.
Example 3.4–5: Comparison of Fourier Transform and Z-Transform
Consider the ﬁrst quadrant unit step function u++(n1,n2). Computing its Z-transform, we
earlier obtained
U++(z1,z2) =
1
1 −z−1
1
1
1 −z−1
2
,
which is well behaved in its region of convergence Rx = {|z1| > 1,|z2| > 1}. Note, however,
that the corresponding Fourier transform has a problem here and needs impulses to get

92
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
by. In fact, using the separability of u++(n1,n2) and noting the 1-D Fourier transform pair,
u(n) ⇔U(ω) = πδ(ω) +
1
1 −e−jω ,
we obtain the 2-D Fourier transform,
U++(ω1,ω2) =

πδ(ω1) +
1
1 −e−jω1

πδ(ω2) +
1
1 −e−jω2

.
If we consider the convolution of two of these step functions, this should correspond to
multiplying the transforms together. For the Z-transform, there is no problem, and the
region of convergence remains unchanged. For the Fourier transform, though, we would
have to be able to interpret [δ(ω1)]2 which is not possible, as powers and products of
singularity functions are unstudied and so not deﬁned.
Conversely, a sequence that has no Z-transform but does have a Fourier transform is
sinωn, which in two dimensions becomes the plane wave sin(ω1n1 + ω2n2). Thus each
transform has its own useful place. The Fourier transform is not strictly a subset of the
Z-transform, because it can use impulses and other singularity functions, which are not
permitted to Z-transforms.
We next turn to the stability problem for 2-D systems and ﬁnd that the Z-transform
plays a prominent role just as in one dimension. However, the resulting 2-D stability
tests are much more complicated, since the zeros and poles are functions, not points,
in 2-D z space.
3.5 2-D FILTER STABILITY
Stability is an important concept for spatial ﬁlters, as in the 1-D case. The ﬁnite
impulse response (FIR) ﬁlters are automatically stable due to the ﬁnite number of pre-
sumably ﬁnite-valued coefﬁcients. Basically, stability means that the ﬁlter response
will never get too large, if the input is bounded. For a linear ﬁlter, this means that
nothing unpredictable or chaotic could be caused by extremely small perturbations in
the input. A related notion is sensitivity to inaccuracies in the ﬁlter coefﬁcients and
computation, and stability is absolutely necessary to have the desired low sensitivity.
Stability is also necessary so that boundary conditions (often unknown in practice)
will not have a big effect on the response far from the boundary. So, stable systems
are preferred for a number of practical reasons. We start with the basic deﬁnition of
stability of an LSI system.
Deﬁnition 3.5–1: Stability of an LSI System
We say a 2-D LSI system with impulse response h(n1,n2) is bounded-input bounded-
output (BIBO) stable if
∥h∥1 ≜
+∞
X
k1,k2=−∞
|h(k1,k2)| < ∞.

3.5 2-D Filter Stability
93
The resulting linear space of signals is called l1, and the norm ∥h∥1 is called the l1 norm.
Referring to this signal space, we can say that the impulse response is BIBO stable if and
only if (iff) it is an element of the l1 linear space; that is,
system is stable ⇐⇒h ∈l1.
(3.5–1)
Clearly, this means that for such a system described by convolution, the output
will be uniformly bounded if the input is uniformly bounded. As we recall from
Chapter 1, Section 1.1,
|y(n1,n2)| =

X
k1,k2
x(n1 −k1,n2 −k2)h(k1,k2)

≤max|x(k1,k2)| ·

X
k1,k2
h(k1,k2)

= M · ∥h∥1
< ∞,
where M < ∞is the assumed ﬁnite uniform bound on the input signal magnitude |x|,
and the l1 norm ∥h∥1 is assumed ﬁnite.
This BIBO stability condition is most easily expressed in terms of the Z-transform
system function H. Because for Z-transforms convergence is really absolute conver-
gence, the system is stable if
the point (z1,z2) = (1,1) ∈Rh,
where Rh is the ROC of H(z1,z2). In general, for a rational system, we can write
H(z1,z2) = B(z1,z2)
A(z1,z2),
where A and B are 2-D polynomials in the variables z1 and z2. In one dimen-
sion, the corresponding statement would be H(z) = B(z)/A(z), and stability would
be determined by whether 1 ∈ROC of 1/A; that is, we could ignore the numera-
tor’s presence, assuming no pole-zero cancellations. However, in the case of two and
higher dimensions, this is no longer true, and it has been shown [5] that partial pole-
zero cancellations can occur in such a way that no common factor can be removed,
and so that the resulting ﬁlter is stable because of the effect of this numerator. Since
this is a somewhat delicate situation, here we look for a more robust stability, and so
look at just the poles of the system H, which are the zeros of the denominator poly-
nomial A(z1,z2). For such robust stability, we must require A(z1,z2) ̸= 0 in a region
including (z1,z2) = (1,1), analogously to the 1-D case.

94
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
First Quadrant Support
If a 2-D digital ﬁlter has ﬁrst quadrant support, then we have the following property
for its Z-transform. If the Z-transform converges for some point (z0
1,z0
2), then it must
also converge for all (z1,z2) ∈{|z1| ≥
z0
1
,|z2| ≥
z0
2
} because for such (z1,z2) we
have
X
k1≥0,k2≥0
|h(k1, k2)| |z1|−k1 |z2|−k2 ≤
X
k1≥0,k2≥0
|h(k1,k2)|
z0
1

−k1 z0
2

−k2 .
Thus since ﬁlters that are stable must have (1,1) ∈Rh, ﬁrst quadrant support ﬁlters
that are stable must have an ROC that includes {|z1| ≥1,|z2| ≥1}, as sketched in
Figure 3.5–1 as the gray region. Note that the region slightly overlaps the lines
|z1| = 1 and |z2| = 1 in order to emphasize that these lines must be contained in the
open region that is the ROC of a ﬁrst quadrant support and stable ﬁlter.
We are not saying that the ROC of ﬁrst quadrant support stable ﬁlters will look
like that sketched in Figure 3.5–1, just that the convergence region must include the
gray area.
Second Quadrant Support
If a 2-D digital ﬁlter has second quadrant support, then we have the following prop-
erty for its Z-transform. If the Z-transform converges for some point (z0
1,z0
2), then it
must also converge for all (z1,z2) ∈{|z1| ≤
z0
1
,|z2| ≥
z0
2
}, because for such (z1,z2)
we have
X
k1≤0,k2≥0
|h(k1,k2)| |z1|−k1 |z2|−k2 ≤
X
k1≤0,k2≥0
|h(k1,k2)|
z0
1

−k1 z0
2

−k2 .
Thus since ﬁlters that are stable must have (1,1) ∈Rh, second quadrant support ﬁl-
ters that are stable must have an ROC that includes {|z1| ≤1,|z2| ≥1}, as sketched
in Figure 3.5–2 as the gray region. Note that the region slightly overlaps the lines
|z1| = 1 and |z2| = 1 in order to emphasize that these lines must be contained in the
open region that is the ROC of a second quadrant support and stable ﬁlter.
0
|z2|
|z1|
1
1
FIGURE 3.5–1
Region that must be included in the ROC of a stable ﬁrst quadrant support ﬁlter.

3.5 2-D Filter Stability
95
0
1
1
|z2|
|z1|
FIGURE 3.5–2
Region that must be included in the ROC of a second quadrant support stable ﬁlter.
Example 3.5–1: A Region of Convergence
Consider the spatial ﬁlter with impulse response
h(n1,n2) =
(
an1,
n1 ≤0 and n2 = −n1,
0,
else,
with |a| > 1. Note that the support of this h is in the second quadrant.
Computing the Z-transform, we obtain
H(z1,z2) = 1 + a−1z1z−1
2
+ a−2z2
1z−2
2
+ ···
=
1
1 −a−1z1z−1
2
,
with
Rh = {|a|−1 |z1||z2|−1 < 1}.
We can sketch this ROC as shown in Figure 3.5–3.
0
1
1
|z2|
|z1|
0
FIGURE 3.5–3
Sketch of ROC (gray area) of example Z-transform.

96
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
If a 2-D ﬁlter has support in either of the remaining two quadrants, the ROC
minimal size region can easily be determined similarly. We have thus proved our ﬁrst
theorems on spatial ﬁlter stability.
Theorem 3.5–1: Shanks et al. [7]
A 2-D or spatial ﬁlter with ﬁrst quadrant support impulse response h(n1,n2) and rational
system function H(z1,z2) = B(z1,z2)/A(z1,z2) is BIBO stable if A(z1,z2) ̸= 0 in a region
including {|z1| ≥1,|z2| ≥1}. Ignoring the effect of the numerator B(z1,z2), this is the same
as saying that H(z1,z2) is analytic (i.e., H ̸= ∞) in this region.
For second quadrant impulse response support, we can restate this theorem.
Theorem 3.5–2:
A 2-D or spatial ﬁlter with second quadrant support impulse response h(n1,n2) and rational
system function H(z1,z2) = B(z1,z2)/A(z1,z2) is BIBO stable if A(z1,z2) ̸= 0 in a region
including {|z1| ≤1,|z2| ≥1}.
Similarly, here are the theorem restatements for ﬁlters with impulse response
support on either of the remaining two quadrants.
Theorem 3.5–3:
A 2-D or spatial ﬁlter with third quadrant support impulse response h(n1,n2) and rational
system function H(z1,z2) = B(z1,z2)/A(z1,z2) is BIBO stable if A(z1,z2) ̸= 0 in a region
including {|z1| ≤1,|z2| ≤1}.
Theorem 3.5–4:
A 2-D or spatial ﬁlter with fourth quadrant support impulse response h(n1,n2) and rational
system function H(z1,z2) = B(z1,z2)/A(z1,z2) is BIBO stable if A(z1,z2) ̸= 0 in a region
including {|z1| ≥1,|z2| ≤1}.
If we use the terminology ++ to refer to impulse responses with ﬁrst quadrant
support, −+ referring to those with second quadrant support, and so forth, then all
four zero-free regions for denominator polynomials A can be summarized in the
diagram of Figure 3.5–4. In words, we say “a ++ support ﬁlter must have ROC
including {|z1| ≥1,|z2| ≥1},” which is shown as the ++ region in this ﬁgure. A
general support spatial ﬁlter can be made up from these four components as either a

3.5 2-D Filter Stability
97
0
1
−+
++
−−
+−
1
|z2|
|z1|
0
FIGURE 3.5–4
Illustration of necessary convergence regions for all four quarter-plane support ﬁlters.
sum or convolution product of these quarter-plane impulse responses:
h(n1,n2) = h++(n1,n2) + h+−(n1,n2) + h−+(n1,n2) + h−−(n1,n2) or
h(n1,n2) = h++(n1,n2) ∗h+−(n1,n2) ∗h−+(n1,n2) ∗h−−(n1,n2).
Both these general representations are useful for 2-D recursive ﬁlter design, as we
will see in Chapter 5.
To test for stability using the preceding theorems would be quite difﬁcult for all
but the lowest order polynomials A, since we have seen that the zero loci of 2-D
functions are continuous functions in one of the complex variables, hence requiring
the evaluation of an inﬁnite number of roots. Fortunately, it is possible to simplify
these theorems. We will take the case of the ++ or ﬁrst quadrant support ﬁlter here
as a prototype.
Theorem 3.5–5: Simpliﬁed ++ Test
A ++ ﬁlter with ﬁrst quadrant support impulse response h(n1,n2) and rational system
function H(z1,z2) = B(z1,z2)/A(z1,z2) is BIBO stable if
(a) A(ejω1,z2) ̸= 0 in a region including {all ω1,|z2| ≥1}
(b) A(z1,ejω2) ̸= 0 in a region including {|z1| ≥1, all ω2}
Proof We can construct a proof by relying on Figure 3.5–5. Here, the ×× represent
known locations of the roots when either |z1| = 1 or |z2| = 1. Now, since it is known in
mathematics that the roots must be continuous functions of their coefﬁcients [6], and
since the coefﬁcients, in turn, are simple polynomials in the other variable, it follows that
each of these roots must be a continuous function of either z1 or z2. Now, as |z1| ↗, beyond
1, the roots cannot cross the line |z2| = 1 because of condition b. Also, as |z2| ↗, beyond
1, the roots cannot cross the line |z1| = 1 because of condition a. We thus conclude that
the region {|z1| ≥1,|z2| ≥1} will be zero free by virtue of conditions a and b as was to be
shown.

98
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
0
1
1
x
x x
x
x
xx
...
...
by (b)
by (a)
|z2|
|z1|
FIGURE 3.5–5
Figure used in proof of Theorem 3.5–5.
This theorem has given us considerable complexity reduction, in that the 3-D
conditions a and b can be tested with much less work than can the original 4-D C2
condition. To test condition a of the theorem, we must ﬁnd the roots in z2 of the
indicated polynomial, whose coefﬁcients are a function of the scalar variable ω1,
with the corresponding test for theorem condition b.
Root Maps
We can gain more insight into the previous theorem by using the technique of root
mapping. The idea is to ﬁx the magnitude of one complex variable, say |z1| = 1,
and then ﬁnd the location of the root locus in the z2 plane as the phase of z1 varies.
Looking at condition a of Theorem 3.5–5, we see that as ω1 varies from −π to +π, the
roots in the z2 plane must stay inside the unit circle there, in order for the ﬁlter to be
stable. In general, there will be N such roots or root maps. Because the coefﬁcients of
the z2 polynomial are continuous functions of z1, as mentioned previously, it follows
from a theorem in algebra that the root maps will be continuous functions [6]. Since
the coefﬁcients are periodic functions of ω1, the root maps will additionally be closed
curves in the z2 plane. This is illustrated in Figure 3.5–6. Thus a test for stability based
on root maps can be constructed from Theorem 3.5–5 by considering both sets of root
maps corresponding to part (a) and part (b) and showing that all the root maps stay
inside the unit circle in the target z plane.
Two further Z-transform stability theorem simpliﬁcations have been discovered
as indicated in the following two stability theorems.
Theorem 3.5–6: Huang [8]
The stability of a ﬁrst quadrant or ++ quarter-plane ﬁlter of the form H(z1,z2) =
B(z1,z2)/A(z1,z2) is assured by the following two conditions:
(a) A(ejω1,z2) ̸= 0 in a region including {all ω1,|z2| ≥1}
(b) A(z1,1) ̸= 0 for all |z1| ≥1

3.5 2-D Filter Stability
99
1
1
0
0
Im z1
Im z2
Re z1
Re z2
FIGURE 3.5–6
Illustration of root map of condition (a) of the previous Theorem 3.5–5.
Proof By condition a, the root maps in the z2 plane are all inside the unit circle. But
also, by condition a, none of the root maps in the z1 plane can cross the unit circle. Then
condition b states that one point on these z1 plane root maps is inside the unit circle (i.e.,
the point corresponding to ω2 = 0). By the continuity of the root map, the whole z1 plane
root map must lie inside the unit circle. Thus by Theorem 3.5–5, the ﬁlter is stable.
A ﬁnal simpliﬁcation results in the next theorem.
Theorem 3.5–7: DeCarlo–Strintzis [9]
The stability of a ﬁrst quadrant or ++ quarter-plane ﬁlter of the form H(z1,z2) =
B(z1,z2)/A(z1,z2) is assured by the following three conditions:
(a) A(ejω1,ejω2) ̸= 0 for all ω1,ω2
(b) A(z1,1) ̸= 0 for all |z1| ≥1
(c) A(1,z2) ̸= 0 for all |z2| ≥1
Proof Here, condition a tells us that no root maps cross the respective unit circles. So, we
know that the root maps are either completely outside the unit circles or completely inside
them. Conditions b and c tell us that there is one point on each set of root maps that is
inside the unit circles. We thus conclude that all root maps are inside the unit circles in
their respective z planes.
A simple example of the use of these stability theorems in stability tests follows.
Example 3.5–2: Filter Stability Test
Consider the spatial ﬁlter with impulse response support on the ﬁrst quadrant and system
function
H(z1,z2) =
1
1 −az−1
1
+ bz−1
2
,

100
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
so that we have A(z1,z2) = 1 −az−1
1
+ bz−1
2 . By Theorem 3.5–1, we must have A(z1,z2) ̸= 0
for {|z1| ≥1,|z2| ≥1}. Thus all the roots z2 = f(z1) must satisfy |z2| = |f(z1)| < 1 for all
|z1| > 1. In this case, we have, by setting A(z1,z2) = 0, that
z2 = f(z1) =
b
1 −az−1
1
.
Consider |z1| > 1; assuming that we must have |a| < 1, then
|z2| =
|b|
1 −az−1
1

≤
|b|
1 −|a|,
with the maximum value being achieved for some phase angle of z1. Hence, for BIBO
stability of this ++ ﬁlter, we need
|b|
1 −|a| < 1 or equivalently |a| + |b| < 1.
The last detail now is to deal with the assumption that |a| < 1. Actually, it is easy to see
that this is necessary, by setting |z2| = ∞and noting the root at z1 = a, which must be
inside the unit circle.
Stability Criteria for NSHP Support Filters
The ++ quarter-plane support ﬁlter is not the most general one that can follow a
raster scan (horizontal across, and then down) and produce its output recursively, or
what is called a recursively computable set of equations. We saw in Example 3.4–1
a wedge support polynomial that can be used in place of the quarter-plane A(z1,z2)
of the previous subsection. The most general wedge support, which is recursively
computable in this sense, would have support restricted to a nonsymmetric half-plane
(NSHP), deﬁned as
a NSHP region ≜{n1 ≥0,n2 ≥0} ∪{n1 < 0,n2 > 0},
and illustrated in Figure 3.5–7. With reference to this ﬁgure, we can see that an NSHP
ﬁlter makes wider use of the previously computed outputs, assuming a conventional
raster scan, and hence is expected to have some advantages. We can see that this
NSHP ﬁlter is a generalization of a ﬁrst quadrant ﬁlter, which includes some points
from a neighboring quadrant, in this case the second. Extending our notation, we can
call such a ﬁlter a ⊕+NSHP ﬁlter, with other types being denoted ⊖+,+⊖, etc. We
next present a stability test for this type of spatial recursive ﬁlter.
Theorem 3.5–8: (NSHP Filter Stability [10])
A ⊕+NSHP support spatial ﬁlter is stable in the BIBO sense if its system function
H(z1,z2) = B(z1,z2)/A(z1,z2) satisﬁes:
(a) H(z1,∞) is analytic, i.e., free of singularities, on {|z1| ≥1}
(b) H(e+jω1, z2) is analytic on {|z2| ≥1}, for all ω1 ∈[−π,+π]

3.5 2-D Filter Stability
101
n2
n1
FIGURE 3.5–7
An illustration of NSHP coefﬁcient array support.
Discussion Ignoring possible effects of the numerator on stability, condition a
states that A(z1,∞) ̸= 0 on {|z1| ≥1}, and condition b is equivalently the condition
A(e+jω1,z2) ̸= 0 on {|z2| ≥1} for all ω1 ∈[−π,+π], which we have seen several
times before. To see what the stability region should be for a ⊕+ NSHP ﬁlter, we
must realize that now we can no longer assume that n1 ≥0, so if the Z-transform
converges for some point (z0
1,z0
2), then it must also converge for all (z1,z2) ∈{|z1| =
z0
1
,|z2| ≥
z0
2
}. By ﬁlter stability, we know that h ∈l1 so that the Z-transform H
must converge for some region including {|z1| = |z2| = 1}; thus the ROC for H a ⊕+
NSHP ﬁlter must only include {|z1| = 1,|z2| ≥1}. To proceed further, we realize that
for any ⊕+ NSHP ﬁlter H = 1/A, we can write
A(z1,z2) = A(z1,∞)A1(z1,z2),
with A1(z1,z2) ≜A(z1,z2)/A(z1,∞). Then the factor A1 will not contain any coef-
ﬁcient terms a1(n1,n2) on the current line. As a result its stability can be com-
pletely described by the requirement that A1(e+jω1,z2) ̸= 0 on {|z2| ≥1} for all
ω1 ∈[−π,+π].5 Similarly, to have the ﬁrst factor stable, in the +n1 direction, we
need A(z1,∞) ̸= 0 on {|z1| ≥1}. Given both conditions a and b then, the ⊕+ NSHP
ﬁlter will be BIBO stable.
This stability test can be used on ﬁrst quadrant quarter-plane ﬁlters since they
are a subset of the NSHP support class. If we compare to Huang’s theorem
(Theorem 3.5–6), for example, we see that condition b here is like condition a there,
but the 1-D test (condition a here) is slightly simpler than the 1-D test (condition
b there). Here, we just take the 1-D coefﬁcient array on the horizontal axis for
this test, while in Theorem 3.5–6, we must add the coefﬁcients in vertical columns
ﬁrst.
5Filters with denominators solely of form A1 are called symmetric half-plane (SHP).

102
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
Example 3.5–3: Stability Test of ++ Filter Using NSHP Test
Consider again the spatial ﬁlter with impulse response support on the ﬁrst quadrant (i.e.,
a ++ support ﬁlter) and system function
H(z1,z2) =
1
1 −az−1
1
+ bz−1
2
,
so that we have A(z1,z2) = 1 −az−1
1
+ bz−1
2 . Since it is a subclass of the ⊕+NSHP ﬁl-
ters, we can apply the theorem just presented. First, we test condition a: A(z1,∞) =
1 −az−1
1
= 0 implies z1 = a, and hence we need |a| < 1. Then, testing condition b, we
have: A(e+jω1,z2) = 1 −ae−jω1 −bz−1
2
= 0, which implies that
z2 =
b
1 −ae−jω1 .
Since we need |z2| < 1, we get the requirement |a| + |b| < 1, just as before.
The next example uses the NSHP stability test on an NSHP ﬁlter, where it is
needed.
Example 3.5–4: Test of NSHP Filter
Consider an impulse response with ⊕+ NSHP support, given as
H(z1,z2) =
1
1 −az−1
1
+ bz1z−1
2
,
where we see that the recursive term in the previous line is now “above and to the
right,” instead of “above,” as in a ++ quarter-plane support ﬁlter. First, we test condi-
tion a: A(z1,∞) = 1 −az−1
1
= 0 implies z1 = a, and hence we need |a| < 1. Then, testing
condition b, we have: A(e+jω1,z2) = 1 −ae−jω1 −bejω1z−1
2
= 0, which implies that
z2 =
bejω1
1 −ae−jω1 .
Since we need |z2| < 1, we get the requirement |a| + |b| < 1, just as before.
CONCLUSIONS
This chapter has looked at how the Z-transform generalizes to two dimensions. We
have also looked at spatial difference equations and their stability in terms of the

Problems
103
Z-transform. The main difference with the 1-D case is that 2-D polynomials do not
generally factor into ﬁrst, or even lower order factors. As a consequence, we found
that poles and zeros of Z-transforms were not isolated and have turned out to be
surfaces in the multidimensional complex space. Filter stability tests are much more
complicated, although we have managed some simpliﬁcations that are computation-
ally effective given today’s computer power. Later, when we study ﬁlter design, we
will incorporate the structure and a stability constraint into the formulation. We also
introduced ﬁlters with the more general nonsymmetric half-plane (NSHP) support
and brieﬂy investigated their stability behavior in terms of Z-transforms.
PROBLEMS
1. Find the 2-D Z-transform and region of convergence (ROC) of each of the
following:
(a) u++(n1,n2)
(b) ρ(n1+n2), |ρ| < 1
(c) b(n1+2n2)u(n1,n2)
(d) u−+(n1,n2)
2. Show that
y(n1,n2) =
  n1 + n2 n1

an1nn2u++(n1,n2)
satisﬁes the spatial difference equation
y(n1,n2) = ay(n1 −1,n2) + by(n1,n2 −1) + δ(n1,n2)
over the region n1 ≥0,n2 ≥0. Assume initial rest—i.e., all boundary conditions
on top- and left-hand sides are zero.
3. For the impulse response found in Example 3.1–2, use Stirling’s formula6 for
the factorial to estimate whether this difference equation can be a stable ﬁlter
or not.
4. In Section 3.1, it is stated that the solution of a set of LCCDEs over a region can
be written as
y(n1,n2) = yZI(n1,n2) + yZS(n1,n2),
where yZI is the solution to the boundary conditions with the input x set to zero,
and yZS is the solution to the input x subject to zero boundary conditions. Show
why this is true.
6A simple version of Stirling’s formula (Stirling, 1730) is as follows: n!≈
√
2πn nne−n.

104
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
5. Find the Z-transform system function H(z1,z2) of the LCCDE
y(n1,n2) = x(n1,n2) + 2x(n1 −1,n2)
+ 0.4y(n1 −1,n2) + 0.5y(n1,n2 −1),
with supp{h} = {n1 ≥0,n2 ≥0}. What is the resulting ROCh?
6. Consider the 2-D impulse response
h(n1,n2) = 5ρn1+n2
1
u(n1,n2) ∗ρn1−n2
2
u(n1,n2),
where u is the ﬁrst quadrant unit step function and the ρi are real numbers that
satisfy −1 < ρi < +1 for i = 1,2. (Note the convolution indicated by ∗.)
(a) Find the Z-transform H(z1,z2) along with its ROC.
(b) Can h be the impulse response of a stable ﬁlter?
7. This problem concerns stability and recursive ﬁlters.
(a) For what values of the parameters a and b is the impulse response
h(n1,n2) = a|n1|b|n2| stable in the bounded-input bounded-output sense?
(b) For stable values of a and b, ﬁnd a realization of the ﬁlter with this impulse
response in terms of four stable 1-D recursive ﬁlters running in various
directions on the discrete plane.
8. Find the inverse Z-transform of
X(z1,z2) =
1
1 −.9z−1
1
1
1 −.9z−1
2
+
.9z1
1 −.9z1
.9z2
1 −.9z2
,
with ROCx ⊃{|z1| = |z2| = 1}. Is your resulting x absolutely summable?
9. Prove the Z-transform relationship in (3.4–1) for linear mapping of variables.
10. Find the inverse Z-transform of
X(z1,z2) =
1
1 −

z−1
1
+ z−1
2
,
with ROC = {(z1,z2) | |z1|−1 + |z2|−1 < 1} via series expansion.
11. Find the inverse Z-transform of
X(z1,z2) =
z1 + z2
1 −1
2

z−1
1
+ z−1
2
, with ROCx ⊃{|z1| > 1,|z2| > 1}.
12. The multinomial theorem can be stated as
(x1 + x2 + ··· + xm)n =
X
l1,l2,...,lm
li≥0 and Pm
i=1 li=n

n
l1,l2,...,lm

xl1
1 xl2
2 ···xlm
m ,

Problems
105
where n is a positive integer and the xi,i = 1 to m are variables. Here the
multinomial coefﬁcient is given as

n
l1,l2,...,lm

≜
n!
l1!l2!···lm!
for non-negative integers li that sum to n. Note that this multinomial coefﬁcient
is simply related to the binomial coefﬁcient when m = 2. In fact, denoting the
binomial coefﬁcient as
 n
l

B, we have

n
l,n −l

=
n
l

B
.
Use the multinomial theorem to ﬁnd a closed-form expression for the inverse
2-D Z-transform of
X(z1,z2) =
1
1 −(az−1
1
+ bz−1
2
+ cz−1
1 z−1
2 )
with support of x(n1,n2) on the ﬁrst quadrant—i.e., supp (x)={n1 ≥0,n2 ≥0}.
Your answer should be a ﬁnite sum over multinomial coefﬁcients and the vari-
ables a, b, and c raised to various integer powers, for each coordinate (n1,n2) on
the ﬁrst quadrant. (Hint: First make use of the series expansion
1
1 −x = 1 + x + x2 + x3 + ···
for appropriate choice of x.)
13. Show that an N × N support signal can always be written as the sum of N or
fewer separable factors by regarding the signal as a matrix and applying the
singular value decomposition [11].
14. Use Z-transform root maps to numerically test the following ﬁlter for stability
in the ++ causal sense:
H(z1,z2) =
1
1 −1
2z−1
1
−1
4z−1
1 z−1
2
−1
4z−2
2
.
Use MATLAB and the functions roots and poly.
15. Prove Theorem 3.5–3, the Z-transform stability condition for a spatial ﬁlter
H(z1,z2) = 1/A(z1,z2) with third quadrant support impulse response h(n1,n2).
16. Consider the following 2-D difference equation:
y(n1,n2) + 2y(n1 −1,n2) + 3y(n1,n2 −1) + 7y(n1 −1,n2 −1) = x(n1,n2).
In answering the following questions, use the standard image-processing coor-
dinate system: n1 axis horizontal and n2 axis downward.
(a) Find a stable direction of recursion for this equation.
(b) Sketch the impulse response support of the resulting system of part (a).

106
CHAPTER 3 Two-Dimensional Systems and Z-Transforms
(c) Find the Z-transform system function along with its associated ROC for the
resulting system of part (a).
17. Consider the 3-D linear ﬁlter
H (z1,z2,z3) = B(z1,z2,z3)
A(z1,z2,z3),where a(0,0,0) = 1,
where z1 and z2 correspond to spatial dimensions and z3 corresponds to the time
dimension. Assume that the impulse response h(n1,n2,n3) has ﬁrst octant sup-
port in the following parts (a) and (b). (Note: Ignore any contribution of the
numerator to system stability.)
(a) If this ﬁrst octant ﬁlter is stable, show that the ROC of H(z1,z2,z3) must
include {|z1| = |z2| = |z3| = 1}. Then extend this ROC as appropriate for the
speciﬁed ﬁrst octant support of the impulse response h(n1,n2,n3). Hence,
conclude that stability implies that A(z1,z2,z3) cannot equal zero in this
region; that is, this is a necessary condition for stability for ﬁrst octant ﬁlters.
What is this stability region?
(b) Show that the condition that A(z1,z2,z3) ̸= 0 on the stability region of part
(a) is a sufﬁcient condition for stability of ﬁrst octant ﬁlters.
(c) Let’s say that a 3-D ﬁlter is causal if its impulse response has support on
{n |n3 ≥0}. Note that there is now no restriction on the spatial support, (i.e.,
in the n1 and n2 dimensions). What is the stability region of a causal 3-D
ﬁlter?
18. Consider computing the output of a NSHP ﬁlter on a quarter-plane region.
Speciﬁcally, the ﬁlter is
y(n1,n2) = 0.2y(n1 −1,n2) + 0.4y(n1,n2 −1) + 0.3y(n1 + 1,n2 −1) + x(n1,n2),
and the solution region is {n1 ≥0,n2 ≥0}, with a boundary condition of zero
along the two edges n1 = −1 and n2 = −1. This then speciﬁes a system T
mapping quarter-plane inputs x into quarter-plane outputs y (i.e., y = T[x]).
(a) Is T a linear operator?
(b) Is T a shift-invariant operator?
(c) Is T a stable operator?
REFERENCES
[1] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
[2] D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing,
Prentice-Hall, Englewood Cliffs, NJ, 1983.
[3] N. Levinson and R. M. Redheffer, Complex Variables, Holden-Day, San Francisco, 1970.
[4] S. G. Krantz, Function Theory of Several Complex Variables, Wiley-Interscience, New
York, 1982.

References
107
[5] D. M. Goodman, “Some Stability Properties of Two-Dimensional Liner Shift-Invariant
Digital Filters,” IEEE Trans. on Video Tech., vol. CAS-24, pp. 201–209, April 1977.
[6] G. A. Bliss, Algebraic Functions, Dover, NY, 1966.
[7] J. L. Shanks, S. Treitel, and J. H. Justice, “Stability and Synthesis of Two-Dimensional
Recursive Filters,” IEEE Trans. Audio Electroacoust., vol. AU-20, pp. 115–128, June
1972.
[8] T. S. Huang, “Stability of Two-Dimensional Recursive Filters,” IEEE Trans. Audio
Electroacoust., vol. AU-20, pp. 158–163, June 1972.
[9] M. G. Strintzis, “Test of Stability of Multidimensional Filters,” IEEE Trans. on Video
Tech., vol. CAS-24, pp. 432–437, August 1977.
[10] M. P. Ekstrom and J. W. Woods, “Two-Dimensional Spectral Factorization with Applica-
tions in Recursive Digital Filtering,” IEEE Trans. Acoust., Speech, and Signal Process.,
vol. ASSP-24, pp. 115–128, April 1976.
[11] G. Strang, Linear Algebra and its Applications, Academic Press, New York, 1976.

CHAPTER
2-D Discrete-Space
Transforms
4
In this chapter we look at discrete-space transforms such as discrete Fourier series,
discrete Fourier transform (DFT), and discrete cosine transform (DCT) in two dimen-
sions. We also discuss fast and efﬁcient realizations of the DFT and DCT. The
DFT is a heavily used tool in image and multidimensional signal processing. Block
transforms can be obtained from scanning the data into small blocks and then per-
forming the DFT or DCT on each block. The block DCT is used extensively in
image and video compression for transmission and storage. We also consider the
subband/wavelet transform (SWT), which can be considered as a generalization
of the block DCT transform wherein the basis functions are allowed to overlap
from block to block. These SWTs can also be considered as a generalization of the
Fourier transform wherein resolution in space can be traded off versus resolution in
frequency.
4.1 DISCRETE FOURIER SERIES
The discrete Fourier series (DFS) has been called “the Fourier transform for periodic
sequences,” in that it plays the same role for them that the Fourier transform plays
for nonperiodic (ordinary) sequences. The DFS also provides a theoretical step-
ping stone toward the DFT, which has great practical signiﬁcance in signal and
image processing as a Fourier transform for ﬁnite support sequences. Actually, we
can take the Fourier transform of periodic sequences, but only with impulse func-
tions. The DFS can give an equivalent representation without the need for Dirac
impulses.
Since this section will mainly be concerned with periodic sequences, we establish
the following convenient notation.
Notation: We write ex(n1,n2) to denote a sequence that is rectangularly periodic
with period N1 × N2 when only one period is considered. When two or more peri-
ods are involved in a problem, we will extend this notation and denote the periods
explicitly.
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00004-7
© 2012 Elsevier Inc. All rights reserved.
109

110
CHAPTER 4 2-D Discrete-Space Transforms
Deﬁnition 4.1–1: Discrete Fourier Series
For a periodic sequenceex(n1,n2), with rectangular period N1 × N2 for positive integers N1
and N2, we deﬁne its DFS transform as
eX(k1,k2) ≜
N1−1
X
n1=0
N2−1
X
n2=0
ex(n1,n2)exp−j2π
n1k1
N1
+ n2k2
N2

,
for all integers k1 and k2 in the lattice Z2 (i.e., −∞< k1,k2 < +∞).1
Since k1 and k2 are integers, we can easily see that the DFS is itself periodic with
period N1 × N2; thus the DFS transforms or maps periodic sequences into periodic
sequences. It may be interesting to note that the Fourier transform for discrete space
does not share the analogous property, since it maps discrete-space functions into
continuous-space functions over [−π,+π]2. Even though the Fourier transform over
continuous space did map this space into itself, its discrete-space counterpart did not.
In part because of this self-mapping feature of the DFS, the inverse DFS formula is
very similar to it.
Theorem 4.1–1: Inverse DFS
Given eX(k1,k2), the DFS of periodic sequence ex(n1,n2), the inverse DFS (or IDFS) is
given as
ex(n1,n2) =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
eX(k1,k2)exp+j2π
n1k1
N1
+ n2k2
N2

,
for all −∞< n1,n2 < +∞.
Proof We start by inserting the DFS into this claimed inversion formula:
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0


N1−1
X
n′
1=0
N2−1
X
n′
2=0
ex(n′
1,n′
2)exp−j2π
n′
1k1
N1
+ n′
2k2
N2


× exp+j2π
n1k1
N1
+ n2k2
N2

=
N1−1
X
n′
1=0
N2−1
X
n′
2=0
ex(n′
1,n′
2)



1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
exp+j2π
(n1 −n′
1)k1
N1
+ (n2 −n′
2)k2
N2



=
N1−1
X
n′
1=0
N2−1
X
n′
2=0
ex(n′
1,n′
2)


+∞
X
k1=−∞
δ(n1 −n′
1 + k1N1)
+∞
X
k2=−∞
δ(n2 −n′
2 + k2N2)


=ex((n1)N1 ,(n2)N2),
1Note that −∞< k1,k2 < +∞is shorthand for −∞< k1 < +∞and −∞< k2 < +∞.

4.1 Discrete Fourier Series
111
i.e., a periodic extension ofex off its base period [0,N1 −1] × [0,N2 −1].
=ex(n1,n2),
sinceex is periodic with rectangular period N1 × N2, thus completing the proof.
In fact, one could just take the DFS of a DFS. The reader may care to show that
the result would be a scaled and reﬂected-through-the-origin version of the original
periodic functionex. The following example calculates and plots the DFS of a simple
periodic discrete-space pulse function.
Example 4.1–1: Calculation of a DFS
Let ex(n1,n2) = I4×4(n1,n2) for 0 ≤n1,n2 ≤7, where IS is the indicator function2 of the
coordinate set S. Here, N1 and N2 = 8. Looking at the resulting periodic function, we see
a periodic pulse function on the plane with 25% ones and 75% zeros, which could serve
as an optical image test pattern of sorts. Then
eX(k1,k2) =
3
X
n1=0
3
X
n2=0
Wn1k1
8
Wn2k2
8
, with WN ≜exp−j2π
N
=
3
X
n1=0
Wn1k1
8
3
X
n2=0
Wn2k2
8
= 1 −W4k1
8
1 −Wk1
8
1 −W4k2
8
1 −Wk2
8
= W
3
2 k1
8
W
3
2 k2
8
sin

πk1
2

sin

πk1
8

sin

πk2
2

sin

πk2
8
.
We give a perspective plot of the amplitude part
sin

πk1
2

sin

πk1
8

sin

πk2
2

sin

πk2
8
,
in Figure 4.1–1.
Properties of the DFS Transform
The properties of the DFS are similar to those of the Fourier transform but distinct
in several important aspects, arising both from the periodic nature of ex as well as
2The indicator function IS gives a 1 for a coordinate (n1,n2) contained in the set S and a 0 elsewhere,
thereby indicating the set S.

112
CHAPTER 4 2-D Discrete-Space Transforms
y
x
5
5
15
FIGURE 4.1–1
Plot of amplitude part of the DFS in Example 4.1–1.
from the fact that the frequency arguments k1 and k2 are integers. We consider two
sequencesex andey with the same rectangular period N1 × N2, having DFS transforms
eX and eY, respectively. We offer some proofs below.
1. Linearity:
aex + bey ⇔aeX + beY,
when both sequences have the same period N1 × N2.
2. Periodic convolution:
We deﬁne periodic convolution with the operator symbol ⊗, for two periodic
sequences with the same period as:
(ex ⊗ey )(n1,n2) ≜
N1−1
X
l1=0
N2−1
X
l2=0
ex(l1,l2)ey(n1 −l1,n2 −l2).
We then have the following transform pair:
(ex ⊗ey )(n1,n2) ⇔eX(k1,k2)eY(k1,k2).
The proof is given below.
3. Multiplication:
ex(n1,n2)ey(n1,n2) ⇔
1
N1N2
eX(k1,k2) ⊗eY(k1,k2).
4. Separability:
e
x1(n1)e
x2(n2) ⇔e
X1(k1)e
X2(k2),
the product of a 1-D N1-point and a 1-D N2-point DFS, respectively.
5. Shifting (delay):
ex(n1 −m1,n2 −m2) ⇔eX(k1,k2)exp−j2π
m1k1
N1
+ m2k2
N2

,
where the shift vector (m1,m2) is integer valued. The proof is given below.

4.1 Discrete Fourier Series
113
6. Parseval’s theorem:
N1−1
X
n1=0
N2−1
X
n2=0
ex(n1,n2)ey∗(n1,n2) =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
eX(k1,k2)eY∗(k1,k2),
with a special case for x = y, the “energy balance formula,” since the left-hand
side can be viewed as the “energy” in one period,
N1−1
X
n1=0
N2−1
X
n2=0
|ex(n1,n2)|2 =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
eX(k1,k2)
2 .
The proof is assigned as an end-of-chapter problem.
7. Symmetry properties:
(a) Conjugation:
ex∗(n1,n2) ⇔eX∗(−k1,−k2).
(b) Arguments reversed (reﬂection through origin):
ex(−n1,−n2) ⇔eX(−k1,−k2).
(c) Real-valued sequences (special case): By the conjugation property (a), app-
lying it to a real-valued sequence ex, we have the conjugate symmetry
property,
eX(k1,k2) = eX∗(−k1,−k2).
From this equation, the following four important symmetry properties of real
periodic bi-sequences follow easily:
i. ReeX(k1,k2) is even through the origin:
ReeX(k1,k2) = ReeX(−k1,−k2).
ii. ImeX(k1,k2) is odd through the origin:
ImeX(k1,k2) = −ImeX(−k1,−k2).
iii. |eX(k1,k2)| is even through the origin:
|eX(k1,k2)| = |eX(−k1,−k2)|.
iv. argeX(k1,k2) is odd through the origin:
argeX(k1,k2) = −argeX(−k1,−k2).
These last properties are used for reducing required data storage for the DFS by an
approximate factor of 1/2 in the real-valued image case.
Periodic Convolution
In 2-D, periodic convolution is very similar to the 1-D case for periodic sequences
ex(n) of one variable. Rectangular periodicity, as considered here, allows an easy gene-
ralization. As in the 1-D case, we note that regular convolution is not possible with
periodic sequences, since they have inﬁnite energy.

114
CHAPTER 4 2-D Discrete-Space Transforms
We offer next a proof of the periodic convolution property.
Proof of property 2: Let
ey(n1,n2) ≜(e
x1 ⊗e
x2)(n1,n2)
=
N1−1
X
l1=0
N2−1
X
l2=0
ex1(l1,l2)ex2(n1 −l1,n2 −l2).
Then
eY(k1,k2) =
N1−1
X
n1=0
N2−1
X
n2=0
ey(n1,n2)exp−j2π
n1k1
N1
+ n2k2
N2

=
X
n1,n2
X
l1,l2
ex1(l1,l2)ex2(n1 −l1,n2 −l2)Wn1k1
N1 Wn2k2
N2 ,
with WN ≜exp−j 2π
N . Thus
eY(k1,k2) =
X
n1,n2
X
l1,l2
ex1(l1,l2)Wl1k1
N1 Wl2k2
N2
h
ex2(n1 −l1,n2 −l2)W(n1−l1)k1
N1
W(n2−l2)k2
N2
i
=
X
l1,l2
ex1(l1,l2)Wl1k1
N1 Wl2k2
N2
"X
n1,n2
ex2(n1 −l1,n2 −l2)W(n1−l1)k1
N1
W(n2−l2)k2
N2
#
=
X
l1,l2
ex1(l1,l2)Wl1k1
N1 Wl2k2
N2
eX2(k1,k2)

,
by periodicity,
= eX1(k1,k2)eX2(k1,k2),
as was to be shown.
Shifting or Delay Property
According to DFS property 5 in this section, Properties of the DFS Transform,
ex(n1 −m1,n2 −m2) has DFS eX(k1,k2)exp−j2π( m1k1
N1 + m2k2
N2 ), which can be seen
directly as follows. First, by deﬁnition,
eX(k1,k2) =
N1−1
X
n1=0
N2−1
X
n2=0
ex(n1,n2)exp−j2π
n1k1
N1
+ n2k2
N2

,
so the DFS of ex(n1 −m1,n2 −m2) is given by
N1−1
X
n1=0
N2−1
X
n2=0
ex(n1 −m1,n2 −m2)exp−j2π
n1k1
N1
+ n2k2
N2

=
N1−1−m1
X
n′
1=−m1
N2−1−m2
X
n′
2=−m2
ex(n′
1,n′
2)exp−j2π
" n′
1 + m1

k1
N1
+
 n′
2 + m2

k2
N2
#

4.2 Discrete Fourier Transform
115
=


N1−1−m1
X
n′
1=−m1
N2−1−m2
X
n′
2=−m2
ex(n′
1,n′
2)exp−j2π
n′
1k1
N1
+ n′
2k2
N2

exp−j2π
m1k1
N1
+ m2k2
N2

,
=


N1−1
X
n′
1=0
N2−1
X
n′
2=0
ex(n′
1,n′
2)exp−j2π
n′
1k1
N1
+ n′
2k2
N2

exp−j2π
m1k1
N1
+ m2k2
N2

= eX(k1,k2)exp−j2π
m1k1
N1
+ m2k2
N2

,
where we have substituted n′
1 ≜n1 −m1 and n′
2 ≜n1 −m1 in the ﬁrst step, and where
the second to last line follows from the rectangular periodicity ofex.
Comments
1. If two rectangularly periodic sequences have different periods, we can use the
DFS properties if we ﬁrst ﬁnd their common period, given as Ni = LCM(Nx
i ,Ny
i ),
where sequenceex is periodic Nx
1 × Nx
2 and similarly forey. (Here, LCM stands for
least common multiple.)
2. The DFS separability property 4 in Section 4.1 comes about because the DFS
operator is a separable operator and the operandex(n1,n2) is assumed there to be
a separable function. A problem at the end of the chapter explores whether this is
an exception to the rule that “multiplication in the space domain corresponds to
convolution in the frequency domain,” or not.
4.2 DISCRETE FOURIER TRANSFORM
The discrete Fourier transform (DFT) is “the Fourier transform for ﬁnite length
sequences” because, unlike the Fourier transform, the DFT has a discrete argument
and can be stored in a ﬁnite number of inﬁnite word-length locations. Yet, still it
turns out that the DFT can be used to exactly implement convolution for ﬁnite size
arrays. Following [1, 2], our approach to the DFT will be through the DFS, which
is made possible by the isomorphism between rectangular periodic and ﬁnite length
rectangular support sequences.
Deﬁnition 4.2–1: Discrete Fourier Transform
For a ﬁnite support sequence x(n1,n2) with support [0,N1 −1] × [0,N2 −1], we deﬁne its
DFT X(k1,k2) for integers k1 and k2 as follows:
X(k1,k2) ≜



N1−1
X
n1=0
N2−1
X
n2=0
x(n1,n2)Wn1k1
N1 Wn2k2
N2 ,
(k1,k2) ∈[0,N1 −1] × [0,N2 −1]
0,
else.
(4.2–1)

116
CHAPTER 4 2-D Discrete-Space Transforms
We note that the DFT maps ﬁnite support rectangular sequences into themselves.
Images of the DFT basis functions of size 8 × 8 are shown next, with real parts in
Figure 4.2–1 and imaginary parts in Figure 4.2–2 The real parts of the basis func-
tions represent the components of x that are symmetric with respect to the 8 × 8
square, while the imaginary parts of these basis functions represent the nonsymmetric
parts. In these ﬁgures, the color white is maximum positive (+1), midgray is 0, and
black is minimum negative (−1). Each basis function occupies a small square, and
the squares are then arranged into an 8 × 8 mosaic. Note that the highest frequen-
cies are in the middle at (k1,k2) = (4,4) and correspond to the Fourier transform
FIGURE 4.2–1
Image of the real part of 8 × 8 DFT basis functions.
FIGURE 4.2–2
Image of the imaginary part of 8 × 8 DFT basis functions.

4.2 Discrete Fourier Transform
117
at (ω1,ω2) = (π,π). So the DFT is seen as a projection of the ﬁnite support input
sequence x(n1,n2) onto these basis functions. The DFT coefﬁcients then are the rep-
resentation coefﬁcients for this basis. The inverse DFT (IDFT) exists and is given by
x(n1,n2) =



1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
X(k1,k2)W−n1k1
N1
W−n2k2
N2
,
(n1,n2) ∈[0,N1 −1] × [0,N2 −1],
0,
else.
(4.2–2)
This can be seen as a representation of x in terms of the basis functions
1
N1N2 W−n1k1
N1
W−n2k2
N2
and the expansion coefﬁcients X(k1,k2).
The correctness of this 2-D IDFT formula can be seen in a number of different
ways. Perhaps the easiest at this point is to realize that the 2-D DFT is a separable
transform; as such, we can realize it as the concatenation of two 1-D DFTs. Since we
can see that the 2-D IDFT is just the inverse of each of these 1-D DFTs in a given
order, say ﬁrst by row and then by column, we have the desired result based on the
known validity of the 1-D DFT/IDFT transform pair, applied twice.
A second method is to rely on the DFS, which we have established in the previous
section. Indeed, the main value of the DFS is its use as an aid in understanding DFT
properties. The key concept is that rectangular periodic and rectangular ﬁnite sup-
port sequences are isomorphic to one another; that is, givenex, we can deﬁne a ﬁnite
support x as x ≜exIN1×N2, and given a ﬁnite support x, we can ﬁnd the corresponding
ex asex = x[(n1)N1,(n2)N2], where we use the notation (n)N meaning “nmodN.” Still
a third method is to simply insert (4.2–1) into (4.2–2) and perform the 2-D proof
directly.
Example 4.2–1: Ones on Diagonal of Square
Consider a spatial sequence of ﬁnite extent x(n1,n2) = δ(n1 −n2) IN×N, as illustrated in
the left-hand side of Figure 4.2–3. Proceeding to take the DFT, we obtain for (k1,k2) ∈
[0,N −1] × [0,N −1] the following:
X(k1,k2) =
N−1
X
n1=0
N−1
X
n2=0
δ(n1 −n2)Wn1k1
N
Wn2k2
N
=
N−1
X
n1=0
Wn1(k1+k2)
N
= Nδ(k1 + k2) for (k1,k2) ∈[0,N −1] × [0,N −1]
and, of course, X(k1,k2) = 0 elsewhere, as illustrated on the right-hand side of
Figure 4.2–3. Note that this is analogous to the case of the continuous-parameter Fourier
transform of an impulse line (see Chapter 1). This ideal result only occurs when the line is
exactly at 0, 45, or 90, or 135 degrees and the DFT size is square. For other angles, we get
a sinNk/N sink type of approximation that tends toward the impulse line solution more and
more as N grows larger without bound. Of course, as was true for the continuous-space

118
CHAPTER 4 2-D Discrete-Space Transforms
n2
k2
n1
k1
x(n1, n2)
X(k1, k2)
0
0
N−1
N−1
N−1
N−1
FIGURE 4.2–3
DFT of ones on diagonal of square.
(parameter) Fourier transform, the line of impulses in frequency is perpendicular, or at 90
degrees, to the impulse line in space.
DFT Properties
The properties of the DFT are similar to those of the DFS. The key difference is that
the support of the sequence x and of the DFT X is ﬁnite. We consider two sequences
x and y with the same rectangular N1 × N2 support, having DFT transforms X and Y,
respectively. We then offer proofs of some of these properties below.
1. Linearity:
ax + by ⇔aX + bY,
when both sequences have the same support [0,N1 −1] × [0,N2 −1].
2. Periodic convolution:
We deﬁne periodic convolution for two ﬁnite support sequences with the same
period as
(x ⊗y)(n1,n2) ≜
N1−1
X
l1=0
N2−1
X
l2=0
x(l1,l2)y[(n1 −l1)N1 ,(n2 −l2)N2] IN1×N2(n1,n2),
(4.2–3)
using the operator symbol ⊗. We then have the following transform pair:
(x ⊗y)(n1,n2) ⇔X(k1,k2)Y(k1,k2).
The proof is given in the following section.
3. Multiplication:
x(n1,n2)y(n1,n2) ⇔
1
N1N2
X(k1,k2) ⊗Y(k1,k2).
4. Separability:
x1(n1)x2(n2) ⇔X1(k1)X2(k2),
the product of a 1-D N1-point and a 1-D N2-point DFT, respectively.

4.2 Discrete Fourier Transform
119
5. Shifting (delay):
x[(n1 −m1)N1 ,(n2 −m2)N2] IN1×N2(n1,n2)
⇔X(k1,k2)exp−j2π
m1k1
N1
+ m2k2
N2

,
where the shift vector (m1,m2) is integer valued. The proof is given in the
following section.
6. Parseval’s theorem:
N1−1
X
n1=0
N2−1
X
n2=0
x(n1,n2)y∗(n1,n2) =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
X(k1,k2)Y∗(k1,k2),
with special case for x = y, the “energy balance formula,” since the left-hand side
then becomes the energy of the signal
Ex =
N1−1
X
n1=0
N2−1
X
n2=0
|x(n1,n2)|2 =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
|X(k1,k2)|2 .
The proof is assigned as end-of-chapter problem 8.
7. Symmetry properties:
(a) Conjugation:
x∗(n1,n2) ⇔X∗[(−k1)N1,(−k2)N2] IN1×N2(k1,k2).
(b) Arguments reversed (modulo reﬂection through origin):
x[(−n1)N1 ,(−n2)N2]IN1×N2(n1,n2) ⇔X[(−k1)N1 ,(−k2)N2] IN1×N2(k1,k2).
(c) Real-valued sequences (a special case): By applying the preceding conju-
gation property to a real-valued sequenceex, we have the conjugate symmetry
property
X(k1,k2) = X∗[(−k1)N1 ,(−k2)N2] IN1×N2(k1,k2).
(4.2–4)
From this equation, the following four properties follow easily:
i. ReX(k1,k2) is even:
ReX(k1,k2) = ReX[(−k1)N1 ,(−k2)N2] IN1×N2(k1,k2).
ii. ImX(k1,k2) is odd:
ImX(k1,k2) = −ImX[(−k1)N1 ,(−k2)N2] IN1×N2(k1,k2).
iii. |X(k1,k2)| is even:
|X(k1,k2)| = |X[(−k1)N1 ,(−k2)N2]| IN1×N2(k1,k2).

120
CHAPTER 4 2-D Discrete-Space Transforms
iv. argX(k1,k2) may be taken as3 odd:
argX(k1,k2) = −argX(k1,k2).
These last properties are used for reducing required data storage for the DFT by an
approximate factor of 1/2 in the real-valued image case.
Proof of DFT Circular Convolution Property 2
Key property 2 says that multiplication of 2-D DFTs corresponds to the deﬁned
circular convolution in the spatial domain, in a manner very similar to that in one
dimension. In fact, we can see from (4.2–3) that this operation is separable into the 1-
D circular convolution along the rows, followed by the 1-D circular convolution over
the columns. The correctness of this property can then be proved by using the 1-D
proof twice; once for the rows and once for the columns. Another way to see the result
is to consider the corresponding periodic sequencesex andey. Then (4.2–3) can be seen
as the ﬁrst period from their periodic convolution sinceex(n1,n2) = x[(n1)N1,(n2)N2]
and ey = y[(n1)N1,(n2)N2]. The ﬁrst period of their DFS product must be the corre-
sponding DFT result; thus, by property 2 of DFS, we have the result here, since by
the correspondenceex ∼x, it must be that eX ∼X.
Proof of DFT Circular Shift Property 5
Since ex(n1,n2) = x[(n1)N1,(n2)N2], it follows that the periodic shift ofex agrees with
the circular shift of x,
ex(n1 −m1,n2 −m2) = x[(n1 −m1)N1 ,(n2 −m2)N2],
for (n1,n2) ∈[0,N1 −1] × [0,N2 −1], so the DFS of the left-hand side must equal
the DFT of the right-hand side, over the fundamental period in frequency; that is,
(k1,k2) ∈[0,N1 −1] × [0,N2 −1]. We thus have the DFT
eX(k1,k2)exp−j2π
m1k1
N1
+ m2k2
N2

IN1×N2(k1,k2)
= X(k1,k2)exp−j2π
m1k1
N1
+ m2k2
N2

.
An end-of-chapter problem asks you to use this circular shift property to prove
the 2-D DFT circular convolution property directly in the 2-D DFT domain.
Example 4.2–2: 2-D Circular Convolution
Let N1 = N2 = 4. The diagram in Figure 4.2–4 shows an example of the 2-D circular con-
volution of two small arrays x and y. In this ﬁgure, the two top plots show the arrays
x(n1,n2) and y(n1,n2), where the open circles indicate zero values of these 4 × 4 support
signals. The nonzero values are denoted by ﬁlled-in circles in the 3 × 3 triangle support x,
and various ﬁlled-in shapes on the square 2 × 2 support y. To perform their convolution,
3We cannot use “is” here because you have to select the right multiple of 2π to add to the phase curve
to make it “odd,” since phase is a multiple-valued function.

4.2 Discrete Fourier Transform
121
n2
I2
n2
n1
l1
l1
I2
I2
I2
l1
l1
n1
x(n1, n2)
y(n1, n2)
x(I1, I2)
y(I1, I2)
y((−l1)4, (−l2)4)
y((1−l1)4, (1−l2)4)
FIGURE 4.2–4
Example of 2-D circular convolution of small triangle support x and small square support y,
both considered as N1 × N2 = 4 × 4 support.
over dummy variables (l1,l2), the middle plots of the ﬁgure show the inputs as functions
of these dummy variables. The bottom line then shows plots of y circularly shifted to cor-
respond to the output points (n1,n2) = (0,0) on the left and (n1,n2) = (1,1) on the right.
Continuing in this way, we can see that for all (n1,n2) we get the linear convolution result.
This has happened because the circular wrap-around points in y only occur at zero values
for x. We see that this was due to the larger value we took for N1 and N2, i.e., larger than
the necessary value to hold the output.
We see in this example that the circular or wrap-around does not affect all output
points. In fact, by enclosing the small triangular support signal and the small pulse in
a larger 4 × 4 square, we have avoided the wrap-around for all (n1 ≥1, n2 ≥1). We
can generalize this result as follows. Let the supports of signals x and y be given as
supp{x} = [0,M1 −1] × [0,M2 −1]
and
supp{y} = [0,L1 −1] × [0,L2 −1].
Then if the DFT size is taken as N1 = M1 + L1 −1 or larger, and N2 = M2 + L2 −1
or larger, we get the linear convolution result. Thus, to avoid the circular or spatial-
aliasing result, we simply have to pad the two signals with zeros out to a DFT size
that is large enough to contain the linear convolution result—the result of ordinary
noncircular convolution.

122
CHAPTER 4 2-D Discrete-Space Transforms
ω2
ω1
+π
+π
+ +
−+
−−
+ −
k1
+ +
−+
−−
+ −
k2
0
N2−1
N2/2
N1/2
N1−1
FIGURE 4.2–5
Mapping of FT samples to DFT locations.
Relation of DFT to Fourier Transform
For a ﬁnite support sequence x, we can compute both the Fourier transform X(ω1,ω2)
as well as the DFT X(k1,k2). We now answer the question of the relation of these two.
Comparing (1.2–1) of Chapter 1 to (4.2–1), we have
X(k1,k2) = X(ωk1,ωk2),
where ωki ≜2πki/Ni, i = 1,2,
(4.2–5)
for (k1,k2) ∈[0,N1 −1] × [0,N2 −1]. This means that the central or primary period
of the Fourier transform X(ω1,ω2) is not what is sampled, but rather an area of equal
size in its ﬁrst quadrant. Of course, by periodicity this is equivalent to sampling in
the primary period. Based on such considerations, we can construct the diagram of
Figure 4.2–5 indicating where the DFT samples come from in the Fourier transform
plane. In particular, all the FT samples along the ωi axes map into ki = 0 in the DFT,
and the possible samples (which only occur for Ni even) at ωi = π map into ki =
Ni/2. Also note the indicated mapping of the four quadrants of [−π,+π]2 in the
ω1 × ω2 plane onto [0,N1 −1] × [0,N2 −1] in the ﬁrst quadrant of the k1 × k2 plane.
Example 4.2–3: DFT Symmetry in Real-valued Signal Case
When the image (signal) is real valued, we have the following DFT symmetry property:
X(k1,k2) = X∗[(−k1)N1 ,(−k2)N2] IN1×N2(k1,k2). When the DFT is stored, we only need to
consider the locations [0,N1 −1] × [0,N2 −1]. For these locations, we can then write
X(k1,0) = X∗(N1 −k1,0) for k2 = 0,
X(0,k2) = X∗(0,N2 −k2) for k1 = 0,
X(k1,k2) = X∗(N1 −k1,N2 −k2) otherwise.
This then gives us the conjugate symmetry through the point (k1,k2) = (N1/2,N2/2), as
shown in Figure 4.2–6. The big square with round corners shows an essential part of
the DFT coefﬁcients and comprises locations {0 ≤k1 ≤N1/2,0 ≤k2 ≤N2/2}. The other

4.2 Discrete Fourier Transform
123
0
N1−1
N1/2
N2−1
N2/2
k2
k1
FIGURE 4.2–6
An illustration of the conjugate symmetry in DFT storage, for the real-valued image case.
essential part is one of the two smaller squares with rounded corners that share a
side with this large square. For example, the upper square, which comprises points
{1 ≤k1 < N1/2,N2/2 < k2 ≤N2 −1}. The other smaller square is not needed by the pre-
ceding symmetry condition. Neither are the two narrow regions along the axes. Such
symmetry can be used to reduce storage by approximately one-half. We only need to
store the coefﬁcients for the resulting nonsymmetric half-space region.
Effect of Sampling in Frequency
Let x(n1,n2) be a general signal with Fourier transform X(ω1,ω2); then we can sam-
ple it as in (4.2–5). If we take the IDFT of the function X(k1,k2)IN1×N2(k1,k2), we
then obtain
y(n1,n2) =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
X(k1,k2)W−n1k1
N1
W−n2k2
N2
(4.2–6)
=
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0

X
all m1,m2
x(m1,m2)Wm1k1
N1
Wm2k2
N2

W−n1k1
N1
W−n2k2
N2
=
X
all m1,m2
x(m1,m2)


1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
W(m1−n1)k1
N1
W(m2−n2)k2
N2


(4.2–7)
=
X
all m1,m2
x(m1,m2)

X
all l1,l2
δ(m1 −n1 + l1N1,m2 −n2 + l2N2)


=
X
all l1,l2
x(n1 −l1N1,n2 −l2N2),
(4.2–8)

124
CHAPTER 4 2-D Discrete-Space Transforms
which displays the spatial domain aliasing caused by sampling in frequency. If the
original signal x had a ﬁnite support [0,M1 −1] × [0,M2 −1] and we took samples
of its Fourier transform, satisfying N1 ≥M1 and N2 ≥M2, then we would have no
overlap in (4.2–8), or equivalently
x(n1,n2) = y(n1,n2)IN1×N2(n1,n2),
and will avoid the aliased terms from coming into this spatial support region. One
interpretation of (4.2–6) is as a numerical approximation to the IFT, whose exact
form is not computationally feasible. We thus see that the substitution of an IDFT
for the IFT can result in spatial domain aliasing, which however can be controlled by
taking the uniformly spaced samples at high enough density to avoid signiﬁcant (or
any) spatial overlap (alias).
Interpolating the DFT
Since the DFT consists of samples of the FT, as in (4.2–5), for an N1 × N2 support
signal, we can take the inverse, or IDFT, of these samples to express the original
signal x in terms of N1N2 samples of its Fourier transform X(ω1,ω2). Then, taking
one more FT, we can write X(ω1,ω2) in terms of its samples:
X(ω1,ω2) = FT{IDFT{X(ωk1,ωk2)}},
thus constituting a 2-D sampling theorem for (rectangular) samples in frequency.
Actually, performing the calculation, we proceed
X(ω1,ω2) =
N−1
X
n1=0
N−1
X
n2=0


1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
X(2πk1/N1,2πk2/N2)W−n1k1
N1
W−n2k2
N2


× exp−j(ω1n1 + ω2n2)
=
N1−1
X
k1=0
N2−1
X
k2=0
X(2πk1/N1,2πk2/N2)
×


1
N1N2
N−1
X
n1=0
N−1
X
n2=0
W−n1k1
N1
W−n2k2
N2
exp−j(ω1n1 + ω2n2)


=
N1−1
X
k1=0
N2−1
X
k2=0
X(2πk1/N1,2πk2/N2) 1
N1
sin N1
2

ω1 −2πk1
N1

sin 1
2

ω1 −2πk1
N1

× 1
N2
sin N2
2

ω2 −2πk2
N2

sin 1
2

ω2 −2πk2
N2
 exp−jN1 −1
2

ω1 −2πk1
N1

× exp−jN2 −1
2

ω2 −2πk2
N2

.

4.3 2-D Discrete Cosine Transform
125
Upon deﬁnition of the 1-D interpolation functions
8i(ω) ≜1
Ni
sin Ni
2 ω
sin 1
2ω
exp−jNi −1
2
ω,
for i = 1,2,
we can write
X(ω1,ω2) =
N1−1
X
k1=0
N2−1
X
k2=0
X(2πk1/N1,2πk2/N2)81

ω1 −2πk1
N1

82

ω2 −2πk2
N2

.
This is completely analogous to the 1-D case [2]. As there, we must note that the
key is that we have taken a number of samples N1 × N2 that is consistent with the
spatial support of the signal x. Of course, larger values of the Ni are permissible,
but smaller values would allow spatial aliasing—in which case, this 2-D frequency-
domain sampling theorem would not be valid.
4.3 2-D DISCRETE COSINE TRANSFORM
One disadvantage of the DFT for some applications is that the transform X is com-
plex valued, even for real data. A related transform, the discrete cosine transform
(DCT), does not have this problem. The DCT is a separate transform and not the
real part of a Fourier transform. It is widely used in image and video compression
applications (e.g., JPEG and MPEG). It is also possible to use DCT for ﬁltering
using a slightly different form of convolution called symmetric convolution (see
end-of-chapter problem 16).
Deﬁnition 4.3–1: 2-D DCT
Assume that the data array has ﬁnite rectangular support on [0,N1 −1] × [0,N2 −1]; then
the 2-D DCT is given as
XC(k1,k2) ≜
N1−1
X
n1=0
N2−1
X
n2=0
4x(n1,n2)cos πk1
2N1
(2n1 + 1)cos πk2
2N2
(2n2 + 1),
(4.3–1)
for (k1,k2) ∈[0,N1 −1] × [0,N2 −1]. Otherwise, XC(k1,k2) ≜0.
The DCT basis functions for size 8 × 8 are shown in Figure 4.3–1. The mapping
between the mathematical values and the colors (gray levels) is the same as in the
DFT ﬁgures earlier. Each basis function occupies a small square, and the squares
are then arranged into an 8 × 8 mosaic. Note that unlike the DFT where the highest
frequencies occur near (N1/2,N2/2), the highest frequencies of the DCT occur at the
highest indices (k1,k2) = (7,7).

126
CHAPTER 4 2-D Discrete-Space Transforms
FIGURE 4.3–1
Image of the basis functions of the 8 × 8 DCT with (0,0) in the upper left-hand corner and
the k2 axis pointing downward.
The inverse DCT exists and is given for (n1,n2) ∈[0,N1 −1] × [0,N2 −1] as
x(n1,n2) =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
w(k1)w(k2)XC(k1,k2)cos πk1
2N1
(2n1 + 1)cos πk2
2N2
(2n2 +1),
where the weighting function w(k) is given just as in the case of 1-D DCT by
w(k) ≜
1/2,
k = 0,
1,
k ̸= 0.
By observation of (4.3–1), we see that the 2-D DCT is a separable operator. As such
it can be applied to the rows and then the columns, or vice versa. Thus the 2-D the-
ory can be developed by repeated application of the 1-D theory. Next, we relate the
1-D DCT to the 1-D DFT of a symmetrically extended sequence. This not only pro-
vides an understanding of the DCT but also enables its fast calculation via methods
intended for the DFT. Later we present a fast DCT calculation that can avoid the
use of complex arithmetic in the usual case where x is a real-valued signal (e.g., an
image).
The next two subsections can be skipped by the reader familiar with the 1-D DCT
theory.
Review of 1-D DCT
In the 1-D case, the DCT is deﬁned as
XC(k) ≜



N1−1
X
n=0
2x(n)cos πk
2N (2n + 1),
k ∈[0,N −1],
0,
else,
(4.3–2)

4.3 2-D Discrete Cosine Transform
127
for every N-point signal x having support [0,N −1]. The corresponding inverse
transform, or IDCT, can be written as
x(n) =



1
N
N1−1
X
k=0
w(k)XC(k)cos πk
2N (2n + 1),
n ∈[0,N −1],
0,
else.
(4.3–3)
It turns out that this 1-D DCT can be understood in terms of the DFT of a
symmetrically extended sequence4
y(n) ≜x(n) + x(2N −1 −n),
(4.3–4)
with support [0,2N −1]. In fact, on deﬁning the 2N point DFT Y(k) ≜DFT2N{y(n)},
we will show that the DCT is alternatively expressed as
XC(k) =

Wk/2
2N Y(k),
k ∈[0,N −1],
0,
else.
(4.3–5)
Thus the DCT is just the DFT analysis of the symmetrically extended signal (4.3–4).
Looking at this equation, we see that there is no overlap in its two components, which
ﬁt together without a gap. We can see that right after x(N −1) comes x(N −1) at
position n = N, which is then followed by the rest of the nonzero part of x in reverse
order, out to n = 2N −1, where sits x(0). We can see a point of symmetry midway
between n = N −1 and N (i.e., at n = N −1
2). If we consider its periodic extension
ey(n), we will also see a symmetry about the point n = −1
2. We thus expect that the
2N-point DFT Y(k) will be real valued except for the phase factor W−k/2
2N
. So the
phase factor in (4.3–5) is just what is needed to cancel out the phase term in Y and
make the DCT real, as it must if the two equations (4.3–1) and (4.3–5) are to agree
for real-valued inputs x.
To reconcile these two deﬁnitions, we start with (4.3–5) and proceed as follows:
Y(k) =
N−1
X
n=0
x(n)Wnk
2N +
2N−1
X
n=N
x(2N −1 −n)Wnk
2N
=
N−1
X
n=0
x(n)Wnk
2N +
N−1
X
n′=0
x(n′)W−(n′+1)k
2N
, with n′ ≜2N −1 −n
= W−k/2
2N
N−1
X
n=0
x(n)W(n+.5)k
2N
+ W−k/2
2N
N−1
X
n=0
x(n)W−(n+.5)k
2N
= W−k/2
2N
N−1
X
n=0
2x(n)cos πk
2N (2n + 1),
for k ∈[0,2N −1],
4This is not the only way to symmetrically extend x, but this method results in the most widely used,
sometimes called DCT-2 [2].

128
CHAPTER 4 2-D Discrete-Space Transforms
the last line following from W−(n+.5)k
2N
= exp( j2π(n + .5)k/2N) and Euler’s relation,
which agrees with the original deﬁnition (4.3–2).
The formula for the inverse DCT (4.3–3) can be established similarly, starting
from
x(n) =
"
1
2N
2N−1
X
k=0
Y(k)W−nk
2N
#
IN(n).
Example 4.3–1: 1-D DCT
In this example, we use MATLAB to take the DCT of the ﬁnite support function x given on
its support region [0,N −1] as
x(n) = 20(1 −n/N).
We choose N = 16 and call the MATLAB function dct using the .m ﬁle DCTeg2.m that is
found at this book’s Web site:
clear
N=16;
for i1=1:N,
n=i1-1;
signal(i1)=20*(1-n/N);
end
stem(signal)
figure
x=fft(signal);
xm=abs(x);
stem(xm)
figure
xc=dct(signal);
stem(xc),
0
2
4
6
8
10
12
14
16
0
2
4
6
8
10
12
14
16
18
20
FIGURE 4.3–2
MATLAB plot of x(n) over its support.

4.3 2-D Discrete Cosine Transform
129
0
2
4
6
8
10
12
14
16
0
20
40
60
80
100
120
140
160
180
FIGURE 4.3–3
MATLAB plot of DFT magnitude |X(k)|.
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
30
35
40
45
FIGURE 4.3–4
MATLAB plot of DCT XC(k).
where we also calculate the FFT magnitude for comparison purposes. The resulting plots
are shown in Figures 4.3–2 through 4.3–4. We see that for this highly asymmetric signal
the DCT is a much more efﬁcient representation than is the DFT.
Some 1-D DCT Properties
1. Linearity:
ax + by ⇔aXC + bYC.
2. Energy conservation:
N−1
X
n=0
|x(n)|2 = 1
2N
N−1
X
k=0
w(k)|XC(k)|2 .
(4.3–6)

130
CHAPTER 4 2-D Discrete-Space Transforms
3. Symmetry:
(a) General case:
x∗(n) ⇔X∗
C(k).
(b) Real-valued case:
real x(n) ⇒real XC(k).
4. Eigenvectors of unitary DCT:
Deﬁne the column vector x ≜[x(0),x(1), ..., x(N −1)]T and deﬁne the matrix C
with elements
Ck′,n′ ≜



q
1
N ,
k′ = 1,
q
2
N cos π
2N (k′ −1)(2n′ −1),
1 < k′ ≤N.
Then the vector y = Cx is the unitary DCT, whose elements are given as
y ≜
"r
1
N XC(0),
r
2
N XC(1),...,
r
2
N XC(N −1)
#T
.
A unitary matrix is one whose inverse is the same as the transpose C−1 = CT. For
the unitary DCT, we have
x = CTy,
and for the energy balance equation,
xTx = yCCTy
= yTy,
a slight modiﬁcation on the DCT Parseval equation of (4.3–6). So the unitary DCT
preserves the energy of the signal x.
It turns out that the eigenvectors of the unitary DCT are the same as those of the
symmetric tridiagonal matrix [3],
Q =


1 −α
−α
0
···
···
0
−α
1
−α
0
···
0
0
−α
1
−α
...
...
...
0
−α
...
...
0
...
...
...
...
1
−α
0
···
···
0
−α
1 −α


,
and this holds true for arbitrary values of the parameter α.

4.3 2-D Discrete Cosine Transform
131
We can relate this matrix Q to the inverse covariance matrix of a 1-D ﬁrst-order
stationary Markov random sequence [4], with correlation coefﬁcient ρ, necessarily
satisfying |ρ| < 1,
R−1 = 1
β2


1 −ρα
−α
0
···
···
0
−α
1
−α
0
···
0
0
−α
1
−α
...
...
...
0
−α
...
...
0
...
...
...
...
1
−α
0
···
···
0
−α
1 −ρα


,
where α ≜ρ/(ρ + 1) and β2 ≜(1 −ρ2)/1 + ρ2. The actual covariance matrix of the
Markov random sequence is
R =


1
ρ
ρ2
ρ3
···
ρN−1
ρ
1
ρ
ρ2
...
...
ρ2
ρ
1
ρ
...
ρ3
ρ3
ρ2
ρ
...
...
ρ2
...
...
...
...
...
ρ
ρN−1
···
ρ3
ρ2
ρ
1


,
with corresponding ﬁrst-order difference equation
x(n) = ρx(n −1) + w(n).
It can further be shown that when ρ ≃1, the matrix Q ≃β2R−1, so that their eigen-
vectors approximate each other too. Because the eigenvectors of a matrix and its
inverse are the same, we know that the unitary DCT basis vectors approximate
the Karhunen–Loeve expansion [4], with basis vectors given as the solution to the
matrix-vector equation
R8 = 38,
and corresponding Karhunen-Loeve transform (KLT) given by
y = 8†x.
Thus, the 1-D DCT of a ﬁrst-order Markov random vector of dimension N should
be close to the KLT of x when its correlation coefﬁcient ρ ≃1. This ends the review
of the 1-D DCT.
Symmetric Extension in 2-D DCT
Since the 2-D DCT
XC(k1,k2) =
N1−1
X
n1=0
N2−1
X
n2=0
4x(n1,n2)cos πk1
2N1
(2n1 + 1)cos πk2
2N2
(2n2 + 1)

132
CHAPTER 4 2-D Discrete-Space Transforms
(0, 0)
2N2−1
N2−1
N1−1
2N1−1
n1
n2
FIGURE 4.3–5
An illustration of the 2-D symmetric extension used in the DCT.
is just the separable operator resulting from application of the 1-D DCT along ﬁrst
one dimension and then the other, the order being immaterial, we can easily extend
the 1-D DCT properties to the 2-D case. In terms of the connection of the 2-D DCT
with the 2-D DFT, we thus see that we must symmetrically extend in, say, the hori-
zontal direction and then symmetrically extend that result in the vertical direction.
The resulting symmetric function (extension) becomes
y(n1,n2) ≜x(n1,n2) + x(n1,2N2 −1 −n2) + x(2N1 −1 −n1,n2,)
+ x(2N1 −1 −n1,2N2 −1 −n2),
which is sketched in Figure 4.3–5, where we note that the symmetry is about the lines
N1 −1
2 and N2 −1
2. Then from (4.3–5), it follows that 2-D DCT is given in terms of
the N1-×N2-point DFT as
XC(k1,k2) =
(
Wk1/2
2N1 Wk2/2
2N2 Y(k1,k2),
(k1,k2) ∈[0,N1 −1] × [0,N2 −1],
0,
else.
Comments
1. We see that both the 1-D and 2-D DCTs only involve real arithmetic for real-
valued data, and this may be important in some applications.
2. The symmetric extension property can be expected to result in fewer high-
frequency coefﬁcients in DCT with respect to DFT. Such would be expected
for lowpass data, since there would often be a jump at the four edges of the
N1 × N2 period of the corresponding periodic sequence ex(n1,n2), which is not
consistent with small high-frequency coefﬁcients in the DFS or DFT. Thus the
DCT is attractive for lossy data storage applications, where the exact value of the
data is not of paramount importance.

4.4 Subband/Wavelet Transform
133
3. The DCT can be used for a symmetrical type of ﬁltering with a symmetrical ﬁlter.
(See end-of-chapter problem 16.)
4. 2-D DCT properties are easy generalizations of 1-D DCT properties reviewed in
this section.
4.4 SUBBAND/WAVELET TRANSFORM5
The DCT transform is widely used for compression, in which case it is almost always
applied as a block transform. In a block transform, the data are scanned, a num-
ber of lines at a time, and the data are then mapped into a sequence of blocks.
These blocks are then operated upon by 2-D DCT. In the image compression stan-
dard JPEG, the DCT is used with a block size of 8 × 8. Subband/wavelet transforms
(SWTs) can be seen as a generalization of such block transforms, a generaliza-
tion that allows the blocks to overlap. SWTs make direct use of decimation and
expansion and LSI ﬁlters. In Example 2.3–1 of Chapter 2, we saw that ideal rect-
angular ﬁlters could be used to decompose the 2π × 2π unit cell of the frequency
domain into smaller regions. Decimation could then be used on these smaller regions
to form component signals at lower spatial sample rates. The collection of these
lower sample rate signals then would be equivalent to the original high-rate sig-
nal. However, being separate signals, now they are often amenable to processing
targeted to their reduced frequency range, and this has been found useful in many
applications.
Ideal Filter Case
To be speciﬁc, consider the four-subband decomposition shown in Figure 4.4–1.
Using ideal rectangular ﬁlters, we can choose H00 to pass the LL subband {|ω1| ≤
π/2,|ω2| ≤π/2} and reject other frequencies in [−π,+π]2. Then set H10 to pass
the HL subband {π/2 < |ω1|,|ω2| ≤π/2}, set H01 to pass the LH subband {|ω1| ≤
π/2,π/2 < |ω2|}, and ﬁnally H11 to pass the HH subband {π/2 < |ω1|,π/2 < |ω2|},
all deﬁned as subsets of [−π,+π]2. Then after 2 × 2 decimation, the four-subband
signals x00,x10,x01, and x11 will contain all the information from the input signal x,
each with a four-times lower spatial sample rate. For example, using (2.3–1) from
Section 2.3, we have for the 2 × 2 decimation case, where M1 = M2 = 2,
Xij(ω1,ω2) = 1
4
1
X
k1=0
1
X
k2=0
Hij
ω1 −2πk1
2
, ω2 −2πk2
2

X
ω1 −2πk1
2
, ω2 −2πk2
2

.
(4.4–1)
5What we here call the subband/wavelet transform (SWT) is usually referred to as either the discrete
wavelet transform (DWT) or the subband transform.

134
CHAPTER 4 2-D Discrete-Space Transforms
H00
H01
H10
H11
x00
x01
x10
x11
2 × 2
2 × 2
2 × 2
2 × 2
x
FIGURE 4.4–1
An illustration of a 2 × 2 rectangular SWT.
For the case i = j = 0, and noting the ﬁlter H00 has unity gain in its [−π/2,+π/2]2
passband, we have the LL subband
X00(ω1,ω2) = 1
4X
ω1
2 , ω2
2

on [−π,+π]2,
(4.4–2)
because the other three terms in (4.4–1), with i and j not both 0, are all zero in the
frequency domain unit cell [−π,+π]2. Thus we can write the LL subband signal x00
as6
x00(n1,n2) = IFT
1
4X(ω1/2,ω2/2) on [−π,+π]2

.7
Similarly, regarding the LH subband x01, we can say that
x01(n1,n2) = IFT
1
4X(ω1/2,ω2/2 −π) on [−π,+π]2

,
and so forth for x10 and x11. Thus we have the following frequency domain expres-
sions, valid in the unit frequency cell [−π,+π]2 and then periodically repeated
6Note that 1
4X(ω1/2,ω2/2) is not periodic with period 2π × 2π and so is not a complete expression for
the FT. Still, over the fundamental period [−π,+π]2, it is correct. The complete expression for the FT
just repeats this expression, periodically, outside [−π,+π]2.
7We write the function to be inverse Fourier transformed in the unconventional way 1
4X(ω1/2,ω2/2)
on [−π,+π]2 to emphasize that 1
4X(ω1/2,ω2/2) by itself is not a Fourier transform, as mentioned in
the preceding note.

4.4 Subband/Wavelet Transform
135
elsewhere:
X00(ω1,ω2) = 1
4X(ω1/2,ω2/2),
X01(ω1,ω2) = 1
4X(ω1/2,ω2/2 −π),
X10(ω1,ω2) = 1
4X(ω1/2 −π,ω2/2), and
X11(ω1,ω2) = 1
4X(ω1/2 −π,ω2/2 −π).
A sketch of an isotropic X and its corresponding LH subband X01 are shown in
Figure 4.4–2 and Figure 4.4–3, respectively. Note that the LH subband has captured
the part of X that is low frequency in ω1 and high frequency in ω2.
To complete this 2 × 2 rectangular SWT, we must show the inverse transform to
reconstruct the original signal x from these four-subband signals x00,x01,x10, and x11.
The inverse transform system diagram is shown in Figure 4.4–4. With reference to
(2.3–4) from Chapter 2, we can write the Fourier transform output of each ﬁlter as
Gkl(ω1,ω2)Xkl(2ω1,2ω2), for k,l = 0,1.
Combining these results, we obtain the overall equation for transform followed by
inverse transform as
X(ω1,ω2) =
X
k,l=0,1
Gkl(ω1,ω2)Xkl(2ω1,2ω2).
(4.4–3)
0
ω1
ω2
π
π 
−π
−π
−π/2
−π/2
+π/2
+π/2
X
FIGURE 4.4–2
Fourier transform X of isotropic signal.

136
CHAPTER 4 2-D Discrete-Space Transforms
π
0
π
ω2
X01
ω1
−π
−π
FIGURE 4.4–3
Fourier transform X01 of LH subband of isotropic signal.
H00
G01
G10
G11
x00
x01
x10
x11
x
2×2
2×2
2×2
2×2
FIGURE 4.4–4
An illustration of a 2 × 2 rectangular subband/wavelet inverse transform.
Now if we set the gains of the ideal synthesis ﬁlters Gkl = 4Hkl, and deﬁne their
frequency-domain regions of support identical to those of the analysis ﬁlters Hkl,
we can obtain perfect reconstruction as follows. First, note that each of the four
Xkl(2ω1,2ω2) terms are periodic with the smaller period π/2 × π/2, which matches
exactly the support regions of the ideal bandpass ﬁlters Gkl. We can thus con-
sider the separate regions, one at at time. First, consider the LL subband region
[−π/2,+π/2]2 where G00 has its support and X00(2ω1,2ω2) = 1
4X(ω1,ω2) from
(4.4–3). Now the other three terms in the preceding reconstruction equation are zero
here; thus, for this LL subband region, we have the perfect reconstructed output
X(ω1,ω2), as desired.

4.4 Subband/Wavelet Transform
137
Next, consider the LH subband region [−π/2,+π/2] × [π/2,π], where G01
has its support, and we use (4.4–3) to ﬁnd X01(2ω1,2ω2) = 1
4X(ω1,ω2 −π) on
[−π/2,+π/2]2 and periodically repeated outside. So we need its ﬁrst vertical alias
valid in the region [−π/2,+π/2] × [π/2,π], which we get by adding 2π to the
argument ω2 in (4.4–3) to ﬁrst get X01(ω1,ω2) = 1
4X(ω1/2,(ω2 + 2π)/2 −π) =
1
4X(ω1/2,ω2/2), valid in the region [−π,+π] × [π,3π]; therefore, X01(2ω1,2ω2) =
1
4X(ω1,ω2) in the region [−π/2,+π/2] × [π/2,3π/2], which is the same as
the union of the two regions [−π/2,+π/2] × [π/2,π] and [−π/2,+π/2] ×
[−π,−π/2] by the necessary 2π × 2π periodicity. Reference to Figure 4.4–3
may help here. Thus for the LH subband region where G01 is nonzero, we have
X01(2ω1,2ω2) = 1
4X(ω1,ω2 −π + π) = 1
4X(ω1,ω2) and thereby get perfect recon-
struction with a passband gain of 4. The perfect reconstruction for the other two
subband regions HL [π/2,π] × [−π/2,+π/2] and HH [π/2,π] × [−π/2,+π/2]
follows similarly.
Putting the four results together, we have
X(ω1,ω2) =



X(ω1,ω2),
|ω1| ≤π/2,|ω2| ≤π/2,
X(ω1,ω2),
|ω1 −π| < π/2,|ω2| ≤π/2,
X(ω1,ω2),
|ω1| ≤π/2,|ω2 −π| < π/2,
X(ω1,ω2),
|ω1 −π| < π/2,|ω2 −π| < π/2,
= X(ω1,ω2)
✓.
We can regard this as an ideal SWT in the sense that the Fourier transform is
an ideal frequency transform, while the DFT and DCT are more practical ones; that
is, they involve a ﬁnite amount of computation and do not have ideal ﬁlter shapes.
We can substitute nonideal ﬁlters for the preceding ideal ﬁlter case and get a more
practical type of SWT. The resulting transform, while not avoiding aliasing error in
the subband signals themselves, does manage to cancel out this aliasing error in the
reconstructed or inverse transformed signal x. To distinguish the two, we can call the
preceding an ideal SWT and a practical type, with ﬁnite order ﬁlters, just an SWT. Of
course, there is no unique SWT, as it will depend on the ﬁlters used.
1-D SWT with Finite Order Filters
We can avoid the need for ideal ﬁlters if we can accept some aliasing error in the
subbands. We ﬁrst review the 1-D case for a two-channel ﬁlter bank. Then we will
construct the required 2-D ﬁlters as separable products. A simple two-channel 1-D
SWT is shown in Figure 4.4–5. If we construct the 1-D ﬁlters to cancel out any alias-
ing in the reconstruction, then the same will be true of the separable 2-D transform.
Using the 1-D theory, we ﬁnd
X0(ω) = 1
2 [H0(ω/2)X(ω/2) + H0(ω/2 −π)X(ω/2 −π)]
and
X1(ω) = 1
2 [H1(ω/2)X(ω/2) + H1(ω/2 −π)X(ω/2 −π)].

138
CHAPTER 4 2-D Discrete-Space Transforms
H1
x
2
2
H0
x0
x1
FIGURE 4.4–5
A system diagram for a two-channel 1-D SWT.
x0
x1
G1
x
2
2
G0
FIGURE 4.4–6
An illustration of the 1-D ISWT.
The inverse 1-D SWT is shown in Figure 4.4–6. From this ﬁgure, the reconstructed
signal (inverse transform) is given as
bX(ω) = G0(ω)X0(2ω) + G1(ω)X1(2ω)
= 1
2 [G0(ω)H0(ω) + G1(ω)H1(ω)]X(ω)
+ 1
2 [G0(ω)H0(ω −π) + G1(ω)H1(ω −π)]X(ω −π).
Therefore, to cancel out aliasing, we need
G0(ω)H0(ω −π) + G1(ω)H1(ω −π) = 0,
(4.4–4)
and then to achieve perfect reconstruction, we additionally need
G0(ω)H0(ω) + G1(ω)H1(ω) = 2.
(4.4–5)
The ﬁrst solution to this problem was the quadrature mirror condition [5, 6],
G0(ω) ≜H1(ω −π)
and
G1(ω) ≜−H0(ω −π),
(4.4–6)
which cancels out the aliasing term (4.4–4). In order to get a lowpass–highpass ﬁlter
pair, we then set
H1(ω) = H0(ω −π),
(4.4–7)
where we see the “mirror symmetry,” as sketched in Figure 4.4–7.

4.4 Subband/Wavelet Transform
139
π
π
ω
ω
0
0
Mirror
images
|H|0
|H|1
FIGURE 4.4–7
Illustration of magnitude responses of a quadrature magnitude ﬁlter (QMF) pair.
We can see that if H0 is chosen as a lowpass ﬁlter, then H1 will be highpass, and
similarly the reconstruction ﬁlter G0 will be lowpass and G1 will be highpass. To get
perfect reconstruction with zero delay, we then need
H2
0(ω) −H2
0(ω −π) = 2.
(4.4–8)
Often, a positive integer delay is inserted on the right-hand side of the condi-
tion (4.4–8) to allow causal ﬁlters to be used in both analysis (SWT) and synthesis
(ISWT).
Example 4.4–1: Haar Filter Pair
Nearly the simplest subband/wavelet ﬁlter pair is obtained by forming the sum and dif-
ference of successive pairs of inputs, as can be accomplished with the so-called Haar
ﬁlters
h0(n) ≜1
√
2
[δ(n) + δ(n −1)]
and
h1(n) ≜1
√
2
[δ(n) −δ(n −1)],
which agree with the mirror symmetry condition in (4.4–7). Then, with reference to
Figure 4.4–5, we have the SWT transform functions
x0(n) = 1
√
2
[x(2n) + x(2n −1)]
and x1(n) = 1
√
2
[x(2n) −x(2n −1)].
Next, following the synthesis condition in (4.4–6), we obtain
g0(n) = 1
√
2
[δ(n) + δ(n −1)]
and
g1(n) = 1
√
2
[−δ(n) + δ(n −1)].
Taking the Fourier transforms of these ﬁlters and inserting into the left-hand side of
(4.4–8), we get
H2
0(ω) −H2
0(ω −π) = 1
2(1 + 2e−jω + e−j2ω) −1
2(1 −2e−jω + e−j2ω)
= 1
24e−jω
= 2e−jω,
indicating perfect reconstruction is possible, but with a delay of one sample.

140
CHAPTER 4 2-D Discrete-Space Transforms
An alternative and preferable formulation of the alias cancellation for 1-D SWT
was found by Simoncelli and Adelson [7]. In their approach,
G0(ω) ≜H0(−ω)
and
G1(ω) ≜H1(−ω),
(4.4–9)
which results in the reconstruction formula
bX(ω) = 1
2 [H0(ω)H0(−ω) + H1(ω)H1(−ω)]X(ω) + 1
2[H0(ω −π)H0(−ω)
+ H1(ω −π)H1(−ω)]X(ω −π)
= 1
2
h
|H0(ω)|2 + |H1(ω)|2i
X(ω) + alias terms,
since the hi are assumed real. Upon setting
H0(ω) = H(−ω) and H1(ω) = e−jωH(ω + π),
(4.4–10)
again the alias terms cancel out. Here, H(ω) is chosen as a lowpass ﬁlter designed to
achieve
|H0(ω)|2 + |H1(ω)|2 =
(4.4–11)
|H(ω)|2 + |H(ω + π)|2 = 2.
Example 4.4–2: Alternative Haar Filter Pair
If we follow this alternative method for the Haar lowpass ﬁlter h0(n) =
1
√
2 [δ(n) + δ(n −1)],
we get the alternative highpass ﬁlter h1(n) =
1
√
2 [−δ(n) + δ(n −1)] via (4.4–10) and then
via (4.4–9), the pair of noncausal reconstruction ﬁlters
g0(n) = 1
√
2
[δ(n) + δ(n + 1)]
and
g1(n) = 1
√
2
[−δ(n) + δ(n + 1)].
Upon insertion into (4.4–11), we get perfect reconstruction with zero delay for this
noncausal pair.
Note the similarity in the two Haar SWTs in the last two examples. In fact, if we
put a delay of one sample into the noncausal reconstruction (ISWT) in this exam-
ple, they become equivalent solutions. Unfortunately for ﬁlter orders higher than
one, exact reconstructions are not possible, and approximation to the reconstruction
formulas is thus necessary. Some methods of subband/wavelet ﬁlter design will be
discussed in Chapter 5.
2-D SWT with Finite Impulse Response (FIR) Filters
We can apply the just-reviewed 1-D SWT theory separably to the horizontal and
vertical axes, resulting in four subbands Xij(ω1,ω2), just as in (4.4–1). Since the 2-D
separable case can be thought of as the sequential application of the earlier 1-D SWT

4.4 Subband/Wavelet Transform
141
to the rows and columns, we know immediately that the corresponding separable 2-D
SWT will achieve alias cancellation and perfect reconstruction also,
Hij(ω1,ω2) = Hi(ω1)Hj(ω2) and Gij(ω1,ω2) = Gi(ω1)Gj(ω2).
Due to ﬁnite order ﬁlters, though, equations such as (4.4–2) will not hold and
alias error will be present in the subbands Xij. However, if we use as a basis for our
separable ﬁltering, ﬁlters such as the preceding ones that have the property of cancel-
ing out aliasing in the reconstruction, then inverse SWT (ISWT) exists as indicated
in (4.4–3). Depending on the ﬁlters used and their length, the amount of aliasing in
the subbands may be signiﬁcant. Filter designs for SWT can ameliorate this problem,
mainly by going to longer ﬁlters.
Relation of SWT to DCT
Consider an 8 × 8 rectangular SWT. It would have 64 subbands, with center frequen-
cies evenly distributed over the frequency unit cell. Now consider applying the DCT
to the same data, but blockwise using 8 × 8 DCTs. We can easily identify each set of
DCT coefﬁcients with one of the subbands. In fact, if we were to use the DCT basis
functions, reﬂected through the origin as the subband/wavelet ﬁlters,
hkl(n1,n2) = 4cos πk
2N1
(−2n1 + 1)cos πl
2N2
(−2n2 + 1),
then the subband values would be exactly the sequence of DCT coefﬁcients at that
frequency k,l. We can thus see the SWT as a generalization of the block DCT trans-
form, wherein the basis functions (subband ﬁlter impulse responses) are allowed to
overlap in space.
Relation of SWT to Wavelets
Wavelet theory is essentially the continuous-time theory that corresponds to dyadic
subband transforms—i.e., those where the L (LL) subband is recursively split over
and over. Wavelet analysis of a continuous-time signal begins as follows. Let f(t) ∈
L2 (L2 being the space of square integrable functions
R +∞
−∞| f(t)|2 dt < ∞), specify a
mother wavelet ψ(t) as some
ψ(t) ∈L1 satisfying
+∞
Z
−∞
ψ(t)dt = 0,
i.e., a speciﬁed highpass function. We then deﬁne a continuous wavelet transform
(CWT) of f as [8, 9]
γ (s,τ) ≜
+∞
Z
−∞
f(t)ψs,τ(t)dt,

142
CHAPTER 4 2-D Discrete-Space Transforms
with wavelets ψs,τ(t) ≜
1
√sψ( t−τ
s ). The positive parameter s is called the scale, and
τ is called the delay. We can see that if ψ is concentrated in time, a CWT gives
a time-varying analysis of f into scales, or resolutions, where increasing the scale
parameter s provides a lower resolution analysis. You might think that f is being
overanalyzed here, and indeed it is. Wavelet theory shows that a discrete version of
this analysis sufﬁces to describe the function. A discrete wavelet transform (still for
continuous-time functions) is deﬁned via the discrete family of wavelets [6, 11]:
ψk,l(t) ≜
1
q
sk
0
ψ
 
t −lτ 0sk
0
sk
0
!
,
= 2−k/2ψ(2−kt −l), with s0 = 2 and τ 0 = 1,
as
γ (k,l) ≜
+∞
Z
−∞
f(t)ψk,l(t)dt.
This analysis can be inverted as follows:
f(t) =
X
k,l
γ (k,l)ψk,l(t),
(4.4–12)
called an inverse discrete wavelet transform (IDWT), thus creating the analysis–
synthesis pair that justiﬁes the use of the word “transform.” Now the k index denotes
scale, so that as k increases, the scale gets larger and larger, while the l index denotes
delay or position of the scale information on the time axis. The values of the DWT
γ (k,l) are also the called wavelet coefﬁcients of the continuous-time function f.
Here, both indices k,l are doubly inﬁnite (i.e., −∞< k,l < +∞) in order to span
the full range of scales and delays.
Rewriting (4.4–12) as a sum over scales,
f(t) =
X
k
"X
l
γ (k,l)ψk,l(t)
#
,
(4.4–13)
we can break the scales up into two parts,
f(t) =
X
k<k0
"X
l
γ (k,l)ψk,l(t)
#
+
X
k≥k0
"X
l
γ (k,l)ψk,l(t)
#
,
= low-scale (frequency) part + ﬁne-scale (frequency) part,
which is reminiscent of a SWT that we have deﬁned only for discrete-time functions
earlier in this chapter. In fact, wavelet theory shows that the two theories ﬁt together in
the following sense. If a continuous-time function is wavelet analyzed with a DWT at
a ﬁne scale, then the wavelet coefﬁcients at the different scales are related by a SWT.
Going a bit further, to each DWT there corresponds a dyadic SWT that recursively

4.5 Fast Transform Algorithms
143
calculates the wavelet coefﬁcients. Unfortunately, there are SWTs that do not corre-
spond to wavelet transforms due to lack of convergence as the scale gets larger and
larger. This is the so-called regularity problem. However, it can be generally said that
most SWTs currently being used have good enough regularity properties, given the
rather small number of scales that are used both for image analysis and for image cod-
ing, typically less than seven. More information on wavelets and wavelet transforms
can be obtained from many sources, including the classic works of [8, 9].
Before leaving the topic of wavelets, notice that the scale parameter k in (4.4–
12) never gets to −∞, as the inﬁnite sum is really just the limit of ﬁnite sums.
However, this is OK because there cannot be any “dc” value or constant in any func-
tion f(t) ∈L2 over the inﬁnite interval (−∞,+∞), since we require that the mother
wavelet and, as a result, the actual wavelet basis functions, have zero mean, i.e.,
R +∞
−∞ψ(t)dt = 0.
4.5 FAST TRANSFORM ALGORITHMS
The DFT and DCT are widely used in image and video signal processing, due in
part to the presence of fast algorithms for their calculation. These fast algorithms are
largely inherited from the 1-D case due to the separable nature of these transforms.
Fast DFT Algorithm
Here, we brieﬂy look at efﬁcient algorithms to calculate the DFT. We cover the
so-called row–column approach, which expresses the 2-D DFT as a series of 1-D
transforms. To understand this method we start out with
X(k1,k2) =
N1−1
X
n1=0
N2−1
X
n2=0
x(n1,n2)Wn1k1
N1 Wn2k2
N2 ,
valid for (k1,k2) ∈[0,N1 −1] × [0,N2 −1], and bring the sum over n1 inside, to
obtain
=
N2−1
X
n2=0


N1−1
X
n1=0
x(n1,n2)Wn1k1
N1

Wn2k2
N2
=
N2−1
X
n2=0
X(k1;n2)Wn2k2
N2
(4.5–1)
upon deﬁning the row-transforms X(k1;n2) as
X(k1;n2) ≜
N1−1
X
n1=0
x(n1,n2)Wn1k1
N1 ,

144
CHAPTER 4 2-D Discrete-Space Transforms
for each row n2 of the original data array x. If we store these values back in the same
row, then we see that (4.5–1) is then just the 1-D column DFTs of this intermediate
data array. If the data are stored sequentially on a tape or disk, it may be advanta-
geous to perform a data transposition step between the column and row transforms
to minimize access to this secondary slower storage [1]. When there is sufﬁcient
random access storage, such an intermediate step is not necessary. If we use an in-
place 1-D FFT routine to implement the row and column transforms, then the amount
of RAM needed would be determined by the size of the data array x with support
N1 × N2.
Computation
Here, we assume that both N1 and N2 are powers of 2.
Step 1: We use the Cooley-Tukey 1-D FFT algorithm to do the row transforms,
either decimation in time (DIT) or decimation in frequency (DIF) with N2 · N1
2 log2 N1
complex multiplies and N2 · N1 log2 N1 complex additions.
Step 2: We do any necessary data mappings due to insufﬁcient RAM (e.g., matrix
transposition).
Step 3: We use the Cooley-Tukey 1-D FFT algorithm again to do the
column transforms in N1 · N2
2 log2 N2 complex multiplies and N1 · N2 log2 N2 complex
additions.
The total amount of computation then comes to N1N2
2
log2 N1N2 complex multi-
plications and N1N2 log2 N1N2 complex additions. These are the so-called radix-2
algorithms. There are also radix-4 algorithms for 1-D FFT and vector radix algo-
rithms for 2-D FFT 1. They both can offer some modest improvement over the
radix-2 case considered here. For example, if N is a power of 4, then the radix-4
approach can save about 25% in computation.
Fast DCT Methods
We can employ a fast method of DFT calculation to the DCT. We would simply
calculate the 2N-point DFT of the symmetrically extended sequence y. This would be
an efﬁcient N log2 N method but would involve complex arithmetic. An alternative
method involving only real arithmetic for a real-valued x(n) has been obtained by
[10]. They deﬁne the real and imaginary parts of the N-point DFT as
cos-DFTN(k) ≜
X
x(n)cos 2πnk
N
and
sin-DFTN(k) ≜
X
x(n)sin 2πnk
N
.

4.6 Sectioned Convolution Methods
145
DCTN
cos-DFTN
sin-DFTN
cos-DFTN/2
sin-DFTN/2
DCTN/4
DCTN/4
FIGURE 4.5–1
Tree diagram of fast DCT by [10].
Then they ﬁnd
DFTN(k) = cos-DFTN(k) −jsin-DFTN(k),
DCTN(N) = 0,
DCTN(−k) = DCTN(+k),
DCTN(2N −k) = −DCTN(k),
cos-DFTN(N −k) = cos-DFTN(k), and
sin-DFTN(N −k) = −sin-DFTN(k),
where in these equations, and others of this section, the DCT and DFT transform is
not automatically set to zero outside its fundamental period. Using these relations and
some others, the dependency tree of Figure 4.5–1 is developed [10].
The resulting computation for N, a positive integral power of two becomes
approximately N
2 log2 N real multiplications and N
2 (3log2 N −2) + 1 real additions
(see [10] for details). The 2-D N1 × N2 DCT, via a row–column method, would then
require N1 · N2
2 log2 N2 + N2 · N2
2 log2 N2 real multiplications.
4.6 SECTIONED CONVOLUTION METHODS
Consider an image of size M1 × M2, to be convolved with an FIR ﬁlter of size
L1 × L2, where usually we have Li << Mi,i = 1,2. In such a case, it is not efﬁcient
to directly implement the 2-D convolution in the DFT domain. However, we can
ﬁrst section the input into adjoining rectangular sections xi,j(n1,n2) of intermediate
support, so that
x(n1,n2) =
X
i, j
xi, j(n1,n2).
Then we can write the convolution y = h ∗x as
y(n1,n2) =
X
i, j
h(n1,n2) ∗xi, j(n1,n2),

146
CHAPTER 4 2-D Discrete-Space Transforms
where each section output yi, j(n1,n2) ≜h(n1,n2) ∗xi,j(n1,n2) is of much smaller
extent, and so can be efﬁciently calculated via DFT methods. In the 2-D overlap-and-
add (O&A) method [2], we select the section size as (N1 −L1 + 1) × (N2 −L2 + 1)
and the DFT size as N1 × N2 so as to avoid spatial aliasing. The section outputs yi,j
then overlap in vertical strips of width L1 and horizontal strips of height L2.
The total number of sections for the image Ns is given as
Ns =

M1
N1 −L1 + 1

M2
N2 −L2 + 1

,
where ⌈·⌉indicates the greatest integer function. The computation needed per
section is two DFTs and N1N2 complex multiplies and additions. Using row–column
FFTs and assuming that N1 and N2 are integral powers of 2, we obtain the total
requirement as

M1
N1 −L1 + 1

M2
N2 −L2 + 1
 
2N1N2 log2(N1N2) + 2N1N2

real multiplications.
Now the direct computation, assuming x and h are real valued, is M1M2L1L2. We
can offer the following comparisons. For L1 = L2 ≥5, the O&A method has less
computation according to these formulas, and for L1 = L2 ≥10, the O&A method
may be actually preferable, considering the overhead of implementing the required
DFTs. In the literature on block ﬁltering, typically N1 = N2 = 64 or 128.
CONCLUSIONS
The DFS, DFT, and DCT can easily be extended to two dimensions as separable
operators. The familiar concepts of circular convolution and now spatial aliasing
were visited again, due to the sampling in frequency space. We investigated the
SWT and interpreted it as an extension to overlapping bases functions of the DCT.
Fast algorithms for both the DFT and DCT were presented relying on the so-called
row–column method that exploits operator separability. Sectioned convolution allows
“linear convolution” results to be achieved via DFT-based circular convolution. More
on the 2-D DFT, including a special DFT for general sampling lattices, can be found
in Chapter 2 of Dudgeon and Mersereau [11].
PROBLEMS
1. Consider the rectangularly periodic sequenceex(n1,n2), with period N1 × N2, and
express its Fourier transform in terms of impulse functions. Relate your result to
the corresponding DFS eX(k1,k2).

Problems
147
2. Show that the sum and product of two periodic functionsex andey that have the
same periods are periodic with the common period.
3. Find the N-point DFT of x(n) = u(n) −u(n −N) and then the same size DFT of
x(n) = u(n) −u(n −N + 1).
4. Let the signal x(n1,n2) have support [0,N1 −1] × [0,N2 −1]. Express the N1 ×
N2 point DFS or DFT, as appropriate, of each of the following in terms of
X(ω1,ω2), the Fourier transform of x:
(a) x[(n1)N1,(n2)N2]
(b) P
all k1
P
all k2 x(n1 −k1N1,n2 −k2N2) (Remember, k1 and k2 are integers.)
(c) x[(N1 −n1)N1,(N2 −n2)N2]
(d) x(N1 −1 −n1,N2 −1 −n2)
5. Find the DFT of x(n1,n2) = 1δ(n1,n2) + 2δ(n1 −1,n2) + 1δ(n1 −2,n2).
6. Let an image x(n1,n2) have support n1,n2 := 0, ..., N −1. Assuming the image
is real valued, its DFT X(k1,k2) will display certain symmetry properties. In
particular, show that X(k1,k2) = X∗(N −k1,N −k2) for k1,k2 := 0,..., N −1.
7. Show that the 2-D DFT can be written in matrix–product form as the triple
product
X = FxFT,
for image data {x(n1,n2),0 ≤n1 ≤N1 −1,0 ≤n2 ≤N2 −1} entered into the
N1 × N2 matrix x as
xi, j = x(i −1, j −1) for 1 ≤i ≤N1,1 ≤j ≤N2,
by choosing appropriate entries for the matrix F. Show how the DFT is stored
in the matrix X in detail. This is a consequence of the separable operator nature
of the DFT. Interpret this result in terms of row-DFTs and column-DFTs.
8. Prove Parseval’s theorem for the N1 × N2-point DFT,
N1−1
X
n1=0
N2−1
X
n2=0
x(n1,n2)y∗(n1,n2) =
1
N1N2
N1−1
X
k1=0
N2−1
X
k2=0
X(k1,k2)Y∗(k1,k2).
Your proof is to be direct from the deﬁnitions of DFT and IDFT and should not
use any other properties.
9. Find the 2-D DFT of signal x, with support [0,N1 −1] × [0,N2 −1], given on its
support as
x(n1,n2) = 10 + 2cos(2π5n1/N1) + 5sin(2π8n2/N2).
Assume that N1 > 5 and N2 > 8.
10. Prove the correctness of DFT property 2 (Section 4.2) directly in the 2-D DFT
transform domain, using the DFT circular shift or delay property 5.

148
CHAPTER 4 2-D Discrete-Space Transforms
11. Prove the following 2-D DFT properties. Your proof should be direct (i.e.,
should not make use of other DFT or DFS properties). Take the DFT size to
be N1 × N2.
(a) x∗(n1,n2) ⇔X∗[(−k1)N1,(−k2)N2]IN1×N2(k1,k2).
(b) x∗[(−n1)N1,(−n2)N2]IN1×N2(n1,n2) ⇔X∗(k1,k2).
12. Start with a general signal s(n1,n2) and take its Fourier transform to get
S(ω1,ω2). Then sample this Fourier transform at ω1 = 2πk1
256 and ω2 = 2πk2
256 ,
where k1,k2 : 0, ... ,255 to get S(k1,k2). Find an expression for IDFT{S(k1,k2)}
in terms of the original s(n1,n2). This is called spatial aliasing.
13. Fill in the details in going from (4.2–7) to (4.2–8), thus deriving the conse-
quences of sampling in the 2-D frequency domain.
14. Let the 1-D ﬁnite support signal x be given over its support region [0,N −1] as
x(n) = 10 + 8cos
h π
2N (2n + 1)
i
+ 2cos
10π
2N (2n + 1)

.
(a) Sketch the plot of x(n).
(b) Find the DFT of x.
(c) Find the DCT of x.
(d) Use MATLAB to perform a 2-D version of this problem for the 2-D ﬁnite-
support function x given over its support region [0,N −1]2 as
x(n1,n2) = 10 + 8cos
h π
2N (2n1 + 1)
i
cos
h π
2N (2n2 + 1)
i
+ 2cos
10π
2N (2n1 + 1)

cos
10π
2N (2n2 + 1)

.
Please provide plots of x(n1,n2),|X(k1,k2)|, and XC(k1,k2).
15. This problem concerns DCT properties, both in 1-D and 2-D cases.
(a) Let XC(k) be the DCT of x(n) with support [0,N −1]. Find a simple expres-
sion for the DCT of x(N −1 −n). Express your answer directly in terms of
XC(k).
(b) Repeat for a 2-D ﬁnite support sequence x(n1,n2) with support on
[0,N −1]2, and ﬁnd a simple expression for the 2-D DCT of x(N −1 −n1,
N −1 −n2) in terms of XC(k1,k2). (Please do not slight this part. It is
the general case and, therefore, does not follow directly from the result of
part a.)
16. This problem concerns ﬁltering with the DCT. Take the version of DCT deﬁned
in the text with N points and consider only the 1-D case.
(a) If possible, deﬁne a type of symmetrical convolution so that zero-phase ﬁl-
tering may be done with the DCT. Justify your deﬁnition of symmetrical
convolution.

Problems
149
(b) Consider a symmetrical zero-phase FIR ﬁlter with support [−M,+M] and
determine whether linear convolution can be obtained for a certain relation
between M and N.
(c) Is there a version of sectioned convolution that can be done here? Explain.
17. Let the signal (image) x(n1,n2) have ﬁnite support [0,M1 −1] × [0,M2 −1].
Then we have seen it is sufﬁcient to sample its Fourier transform X(ω1,ω2)
on a uniform Cartesian grid of N1 × N2 points, where N1 ≥M1 and N2 ≥M2.
Here, we consider two such DFTs (i.e., DFTN1×N2 and DFTM1×M2).
(a) Express DFTN1×N2 in terms of DFTM1×M2 as best as you can.
(b) Consider the case where Ni = LiMi for i = 1,2, where the Li are integers,
and ﬁnd an amplitude and phase closed-form representation for the resulting
interpolator function.
18. We have been given an FIR ﬁlter h with support on the square [0,N −1] ×
[0,N −1]. We must use a 512 × 512-point row–column FFT to approximately
implement this ﬁlter for a 512 × 512-pixel image. Assume N ≪512.
(a) First, evaluate the number of complex multiplies used to implement the ﬁl-
ter as a function of M for an M × M image, assuming the component 1-D
FFT uses the Cooley-Tukey approach (assume M is a power of 2). Then
specialize your result to M = 512. Assume that the FIR ﬁlter is provided as
H in the DFT domain.
(b) What portion of the output will be the correct (linear convolution) result?
19. We want to perform linear ﬁltering of the signal (image) x(n1,n2) with the FIR
linear ﬁlter with impulse response coefﬁcients h(n1,n2) in the DFT domain. The
supports of the image and ﬁlter impulse response are given as follows:
supp{x} = [0,N −1] × [0,N −1]
and
supp{h} = [−L,+L] × [−L,+L].
Assume that the positive integers L and N satisfy N >> L. (Disregard any image
positivity constraints.)
(a) Employ DFTs of size N × N and specify in detail a method of placing the
impulse response coefﬁcients h(n1,n2) into the DFT domain [0,N −1] ×
[0,N −1] so that the ﬁlter output y = x ∗h will be represented by the product
of the DFT coefﬁcients Y(k1,k2) = X(k1,k2)H(k1,k2), possibly with some
delay (shift) and some circularity effect around the edges. Specify the loca-
tion of these bad (due to circularity effect) pixels. (Note that you cannot
take the DFT of h directly because of its symmetric support.)
(b) Specify a mapping of the impulse response coefﬁcients h(n1,n2) onto the
DFT domain [0,N −1] × [0,N −1] such that there is zero shift (delay).
Justify your answer.
20. This problem concerns 2-D vectors, which are rectangular N1 × N2 arrays of val-
ues that correspond to ﬁnite sets of 2-D data, ordinarily thought of as matrices.
We call them 2-D vectors because we want to process them by linear operators
to produce output 2-D vectors. Such operators can be thought of as 4-D arrays,

150
CHAPTER 4 2-D Discrete-Space Transforms
the ﬁrst two indices corresponding to a point in the output 2-D vector, and the
last two indices corresponding to a point in the input 2-D vector. We will call
these linear operators 4-D matrices; thus, notationally we can write
Y = HX,
where bold capital values X,Y denote 2-D vectors and script value H denotes
the 4-D matrix.
(a) Show that the set of 2-D N1 × N2 vectors constitutes a ﬁnite dimensional
vector space.
(b) Deﬁne the appropriate 2-D vector addition and 4-D matrix multiplication in
this vector space.
(c) Show that this notationally simple device is equivalent to stacking, wherein
the 2-D vectors X,Y are scanned in some way into 1-D vectors x,y and
then the general linear operator becomes a 1-D or ordinary matrix H,
which has a block structure.8 Finally, relate the blocks in H in terms of
the elements of H.
(d) Deﬁne a transpose for 2-D vectors X and show it is not the same as
regular matrix transpose. (Use superscript t for the 2-D vector trans-
pose to distinguish it from regular matrix transpose that is denoted by
superscript T.)
(e) How can we ﬁnd the determinant detH? Then, if detH ̸= 0, how can we
deﬁne and then ﬁnd H−1?
21. Show that the 1-D Haar ﬁlters in Example 4.4–2 satisfy the perfect reconstruc-
tion condition (4.4–11) and that the corresponding ISWT actually equals x(n) by
direct calculation in the 1-D frequency or time domain.
REFERENCES
[1] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
[2] A. V. Oppenheim, R. W. Schafer, and J. R. Buck, Discrete-Time Signal Processing, 2nd
Ed., Prentice-Hall, Englewood Cliffs, NJ, 1999.
[3] A. K. Jain, Fundamentals of Digital Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, p. 153–154 and 163–164, 1989.
[4] H. Stark and J. W. Woods, Probability and Random Processes with Application in Signal
Processing, 3rd Ed., Prentice-Hall, Upper Saddle River, NJ, 2002.
[5] A. Croisier, D. Esteban, and C. Galand, “Perfect Channel Splitting by Use of Inter-
polation, Decimation, and Tree Decomposition Techniques,” Proc. Intl. Conf. on Inform.
Sciences/Systems, Patras, Greece, August, pp. 443–446, 1976.
8Often the scanning used is lexicographical order, meaning left-to-right, and then top-to-bottom, also
called raster scan.

References
151
[6] D. Esteban and C. Galand, “Application of Quadrature Mirror Filters to Split Band Voice
Coding Schemes,” Proc. Intl. Conf. Accoust., Speech, and Signal Proc. (ICASSP), May,
pp. 191–195, 1977.
[7] E. P. Simoncelli and E. H. Adelson, “Non-separable Extensions of Quadrature Mirror
Filters to Multiple Dimensions,” Proc. IEEE, vol. 78, No. 4, pp. 652–664, April 1990.
[8] S. Mallat, “A Theory for Multiresolution Signal Decomposition: The Wavelet Rep-
resentation,” IEEE Trans. Pattern Anal. and Machine Intell, vol. 11, pp. 674–693,
July 1989.
[9] M. Vetterli and C. Herley, “Wavelets and Filter Banks: Theory and Design,” IEEE Trans.
Signal Process., vol. 40, pp. 2207–2232, September 1992.
[10] M. Vetterli and H. J. Nussbaumer, “Simple FFT and DCT Algorithms with Reduced
Number of Operations,” Signal Processing, vol. 6, pp. 267–278, 1984.
[11] D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing,
Prentice-Hall, Englewood Cliffs, NJ, 1983.

CHAPTER
Two-Dimensional Filter
Design
5
This chapter discusses the design of spatial and other 2-D digital ﬁlters. We will
introduce both ﬁnite impulse response (FIR) and inﬁnite impulse response (IIR) ﬁl-
ter design. In the FIR case, we ﬁrst present window function design of rectangular
support ﬁlters. We then brieﬂy present a 1-D to 2-D transformation method due to
McClellan and a method based on successive projections onto convex sets of fre-
quency domain speciﬁcations. In the IIR case, we look at computer-aided design
methods that guarantee BIBO stability and also offer approximation for both magni-
tude and phase. We consider both conventional spatial-recursive ﬁlters as well as a
more general class called fully recursive. Finally, we discuss subband/wavelet ﬁlter
designs for use in the SWT.
5.1 FIR FILTER DESIGN
FIR support ﬁlters are often preferred in applications because of their ease of imple-
mentation and freedom from instability worries. However, they generally call for a
greater number of multiplies and adds per output point. Also they typically require
more temporary storage than IIR designs that are covered in Section 5.2.
FIR Window Function Design
The method of window function design carries over simply from the 1-D case.
We start with an ideal, but inﬁnite order, impulse response hI(n1,n2), which may
have been obtained by inverse Fourier transform of an ideal frequency response
HI(ω1,ω2). We then proceed to generate our FIR ﬁlter impulse response h(n1,n2)
by simply multiplying the ideal impulse response by a prescribed window function
w(n1,n2) that has ﬁnite rectangular support:
h(n1,n2) ≜w(n1,n2)hI(n1,n2),
with corresponding multiplication in the frequency domain,
H(ω1,ω2) = W(ω1,ω2) ⊛HI(ω1,ω2),
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00005-9
c⃝2012 Elsevier Inc. All rights reserved.
153

154
CHAPTER 5 Two-Dimensional Filter Design
where ⊛indicates periodic continuous-parameter convolution with fundamental
period [−π,+π] × [−π,+π], speciﬁcally
H(ω1,ω2) =
1
(2π)2
Z
Z
[−π,+π]×[−π,+π]
W(φ1,φ2)HI(ω1 −φ1,ω2 −φ2)dφ1dφ2.
In applying this method, it is important that the window function support supp(w)
coincide with the largest coefﬁcients of the ideal impulse response hI, and such can
be accomplished by shifting one or the other until they align.
We would like the window function and its Fourier transform to have the
following properties:
•
w should have rectangular N1 × N2 support.
•
w should approximate a circularly symmetric function (about its center point) and
be real valued.
•
The volume of w should be concentrated in the space domain.
•
The volume of W should be concentrated in the frequency domain.
Ideally we would like W to be an impulse in frequency, but we know even from
1-D signal processing that this will not be possible for a space-limited function w.
The desired symmetry in the window w will permit the window-designed impulse
response h to inherit any symmetries present in the ideal impulse response hI, which
can be useful for implementation. In the 2-D case, we can consider two classes of
window functions: separable windows and circular windows.
Separable (Rectangular) Windows
Here, we simply deﬁne the 2-D window function w(n1,n2) as the product of two 1-D
window functions,
ws(n1,n2) ≜w1(n1)w2(n2).
This approach generally works well when the component functions are good 1-D
windows.
Circular (Rotated) Windows
The circular window is deﬁned in terms of a 1-D continuous time window function
w(t), as
wc(n1,n2) ≜w
q
n2
1 + n2
2

,
which has the potential, at least, of offering better circular symmetry in the smoothing
of the designed ﬁlter.
Common 1-D Continuous Time Windows
We deﬁne the following windows as centered on t = 0; however, you should keep in
mind that when they are applied in ﬁlter design, they must be shifted to line up with
the signiﬁcant coefﬁcients in the ideal impulse response.

5.1 FIR Filter Design
155
•
Rectangular window:
w(t) ≜
1,
|t| < T,
0,
else.
•
Bartlett (triangular) window:
w(t) ≜



1 −t/T,
0 ≤t ≤T,
1 + t/T,
−T ≤t ≤0,
0,
else.
•
Hanning window:
w(t) ≜
( 1
2(1 + cosπt/T),
|t| < T,
0,
else.
•
Kaiser window:
w(t) ≜



I0

β
q
1 −(t/T)2
.
I0(β),
|t| < T,
0,
else,
where I0(t) is the modiﬁed Bessel function of the ﬁrst kind and of zero order [1].
The free parameter β in this deﬁnition means that the Kaiser window is actually
a family of windows. As β is varied over a range, typically [0,8], the transition
bandwidth goes up while the ﬁlter attenuation in the stop band goes down, in such
a manner as to better or equal the performance of the other known 1-D window
functions. We thus concentrate on only Kaiser window function designs for our
2-D ﬁlters.
Approximate design formulas have been developed for 2-D circular symmetric
lowpass ﬁlters [2] to estimate the required values of the ﬁlter size N1 and N2 and the
Kaiser parameter1 β. They are based on the desired ﬁlter transition bandwidth △ω
and stopband attenuation ATT, deﬁned as follows:
△ω ≜ωs −ωp,
where ωs and ωp denote the circular radii of the desired stopband and passband,
respectively, and each with maximum value of π. The attenuation parameter ATT is
given as
ATT ≜−20log10
 p
δsδp

,
where δs and δp denote the desired peak frequency domain errors in the stopband and
passband, respectively. The estimated ﬁlter orders are given for the square support
case N1 = N2 as separable:
Ns ≈ATT −8
2.10 △ω,
1This β parameter is denoted as α in Oppenheim, Schafer, and Buck [3].

156
CHAPTER 5 Two-Dimensional Filter Design
and circular:
Nc ≈ATT −7
2.18 △ω.
The estimate of β is then given as separable:
βs ≈

0.42(ATT −19.3)0.4 + 0.089(ATT −19.3),
20 < ATT < 60,
0,
else,
and circular:
βc ≈

0.56(ATT −20.2)0.4 + 0.083(ATT −20.2),
20 < ATT < 60,
0,
else.
Example 5.1–1: Window Design Using MATLAB
We have designed an 11 × 11 lowpass ﬁlter with ﬁlter cutoff fc = 0.3 (ωc = 0.6π) and using
two Kaiser windows, with β = 8, both separable and circular. Figure 5.1–1 shows the
magnitude response of the ﬁlter obtained using the separable Kaiser window. The contour
plot of the magnitude data is shown in Figure 5.1–2. Figure 5.1–3 shows the 11 × 11
impulse response of the designed FIR ﬁlter. Figure 5.1–4 shows a contour plot of the
impulse response to demonstrate its degree of “circularity.”
1
0.8
0.6
0.4
0.2
0
80
60
40
20
0 0
20
40
60
80
FIGURE 5.1–1
Magnitude response of Kaiser window FIR ﬁlter design with β = 8 (64 × 64 DFT with (0,0)
shifted to center).
The next set of ﬁgures shows corresponding results for a circular Kaiser window.
Figure 5.1–5 shows the magnitude response, followed by Figure 5.1–6 showing its contour
plot in frequency. Figure 5.1–7 shows the corresponding impulse response plot, and then
Figure 5.1–8 shows the contour plot. Note that both the circular- and separable-designed

5.1 FIR Filter Design
157
10
20
30
40
50
60
10
20
30
40
50
60
FIGURE 5.1–2
Contour plot of separable Kaiser window-designed ﬁlter β = 8 (64 × 64 DFT with (0,0)
shifted to center).
0
5
10
15
0
5
10
15
−0.1
0
0.1
0.2
0.3
FIGURE 5.1–3
Plot of 11 × 11 impulse response.
ﬁlters display a lot of circular symmetry, which they inherit from the exact circular symme-
try of the ideal response HI. Both designs are comparable. A lower value of β would result
in lower transition bandwidth, but less attenuation in the stopband and more ripple in the
passband.

158
CHAPTER 5 Two-Dimensional Filter Design
1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10
11
FIGURE 5.1–4
Contour plot of impulse response.
1
0.5
0
−0.5
80
60
40
20
0
0
20
40
60
80
FIGURE 5.1–5
β = 8, but with circular Kaiser window (64 × 64 DFT with (0,0) shifted to center).
The MATLAB .m ﬁles WinDes permit easy experimentation with Kaiser win-
dow design of spatial FIR ﬁlters. The program WinDesS.m uses separable window
design, while WinDesC.m uses the circular window method. Both programs make
use of the MATLAB image processing toolbox function fwind1.m, which uses
1-D windows. The other toolbox function fwind2.m does window design with a
2-D window that you supply. It is not used in either WinDes program. As an exercise,
try using WinDesC with the parameters of Example 5.1–1, except that β = 2. These
programs are available at the book’s Web site.

5.1 FIR Filter Design
159
10
20
30
40
50
60
10
20
30
40
50
60
FIGURE 5.1–6
Contour plot of magnitude response (64 × 64 DFT with (0,0) shifted to center).
0
5
10
15
0
5
10
15
−0.1
0
0.1
0.2
0.3
FIGURE 5.1–7
11 × 11 impulse response of Kaiser circular window-designed ﬁlter.
Example 5.1–2: Filter Eric Image
We take the ﬁlter designed with the circular window and apply it to ﬁltering the Eric image
from Figure 1.1-7 of Chapter 1. The result is shown in Figure 5.1–9 where we note the
visually blurring effect of the lowpass ﬁlter.

160
CHAPTER 5 Two-Dimensional Filter Design
1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10
11
FIGURE 5.1–8
Contour plot of impulse response.
FIGURE 5.1–9
Lowpass ﬁltering of the Eric image from Figure 1.1-7 of Chapter 1.
We now turn to an FIR design technique that transforms 1-D ﬁlters to obtain 2-D
ﬁlters.
Design by Transformation of 1-D Filter
Another common and powerful design method is by transformation. The basic idea
starts with a 1-D ﬁlter and then transforms it to two dimensions in such a way that
the 2-D ﬁlter response is constant along contours speciﬁed by the transformation.
Thus, for contours with an approximate circular shape, and a 1-D lowpass ﬁlter, we
can achieve approximate circular symmetry in a 2-D lowpass ﬁlter. The basic idea
is due to McClellan [4], and it is developed much more fully in Lim’s text [5] than

5.1 FIR Filter Design
161
here, where we present only the basics. The method is also available in the MATLAB
image processing toolbox.
We start with a 1-D FIR ﬁlter, of type I with zero phase [3], so that we can write
the frequency response as the real function
H(ω) =
M
X
n=0
a(n)cos(nω),
(5.1–1)
where the ﬁlter coefﬁcients h(n) can be expressed in terms of the a(n). Next, we
rewrite this equation in terms of Chebyshev polynomials cos(nω) = Tn [cosω] as
H(ω) =
M
X
n=0
a′(n)cosn ω,
(5.1–2)
where the a′(n) can be expressed in terms of the a(n) (see end-of-chapter problem 8).
Note that the ﬁrst few Chebyshev polynomials are given as
T0(x) = 1,
T1(x) = x,
T2(x) = 2x2 −1,
and in general
Tn(x) = 2xTn−1(x) −Tn−2(x),
n > 1.
Next, we introduce a transformation from the 2-D frequency plane to cosω, given
as [4]
F(ω1,ω2) = A + Bcosω1 + Ccosω2 + Dcos(ω1 −ω2) + Ecos(ω1 + ω2), (5.1–3)
where the constants A,B,C,D, and E are adjusted to the constraint |F(ω1,ω2)| ≤1 so
that F will equal cosω for some ω.
At this point, we parenthetically observe that when the 1-D ﬁlter is expressed in
terms of a Z-transform, the term cosω will map over to 1
2(z + z−1), and the transfor-
mation F will map into the 2-D Z-transform of the 1 × 1 order symmetric FIR ﬁlter f
given as
f (n1,n2) =



1
2D
1
2C
1
2E
1
2B
A
1
2B
1
2E
1
2C
1
2D



,
with axes →n1 and n2 upwards, with 0 and the center. Thus we can take a realization
of the 1-D ﬁlter and, replacing the delay boxes z−1 with the ﬁlter F(z1,z2), obtain
a realization of the 2-D ﬁlter. Of course, this can be generalized in various ways,
including the use of higher order transformations and the other types (II and IV) of
1-D FIR ﬁlters.

162
CHAPTER 5 Two-Dimensional Filter Design
ω1
ω2
+π
+π
(π, π)
(0,0)
ω = 0.1π
ω = 0.9π
ω= 0.5π
FIGURE 5.1–10
Illustration of 1 × 1 order McClellan transformation for nearly circular symmetric contours.
If we take the values A = −0.5,B = C = 0.5, and D = E = 0.25, we get con-
tours F(ω1,ω2) with a nearly circular shape, even near (±π,±π), as is sketched
in Figure 5.1–10, which is very nearly circular in shape out to about midfrequency
≈π/2.
If we start with a 1-D ﬁlter that is optimal in the Chebyshev sense (i.e., l∞optimal
in the magnitude response), then it has been shown [5] that the preceding 1 × 1 order
transformation preserves optimality in this sense for the 2-D ﬁlter if the passband and
stopband contours of the desired ﬁlter are isopotentials of the transformation.
The following examples show the transformation of 1-D optimal Chebyshev
lowpass ﬁlters into 2-D ﬁlters via this method.
Example 5.1–3: 9 × 9 Lowpass Filter
We start with the design of a nine-tap (eighth-order) FIR lowpass ﬁlter with passband edge
fp = 0.166 (ωp = 0.332π) and stopband edge fs = 0.333 (ωs = 0.666π), using the Remez
(also called Parks-McClellan) algorithm. The result is a passband ripple of δ = 0.05 and
a stopband attenuation ATT = 30dB, and the resulting linear and logarithmic magnitude
frequency response shown in Figure 5.1–11. When this 9-tap ﬁlter is transformed with the
nearly circular 1 × 1-order McClellan transform, we obtain the 2-D amplitude response
shown in Figure 5.1–12, where the zero frequency point f = 0 (ω = 0) is in the middle of
this 3-D perspective plot.
We note that this design has resulted in a very good approximation of circular sym-
metry, as we expect from the contour plot of Figure 5.1–10 for the transformation used.
For this particular transformation, at ω2 = 0, we get cosω = cosω1, so that the passband
and stopband edges are located at the same frequencies, on the axes at least, as those
of the 1-D prototype ﬁlter (i.e., fp = 0.166 (ωp = 0.332π) and fs = 0.333 (ωs = 0.666π),
respectively).

5.1 FIR Filter Design
163
40.00
−40.00
Magnitude in dB
−80.00
0
0
0.08
0.16
0.24
Normalized frequency
0.32
0.40
0.48
1.20
0.40
Magnitude
0
0.80
0
0.08
0.16
0.24
Normalized frequency
0.32
0.40
0.48
FIGURE 5.1–11
Magnitude response of nine-tap FIR type I ﬁlter.
FIGURE 5.1–12
Plot of a 9 × 9 FIR ﬁlter designed as a 2-D transform of a nine-tap lowpass ﬁlter.

164
CHAPTER 5 Two-Dimensional Filter Design
Example 5.1–4: Application of Transform Lowpass Filter
In this example, we design and apply a transformation-designed lowpass ﬁlter to a dig-
ital image by use of MATLAB functions. Using the transformation of Example 5.1–3,
we convert a lowpass 1-D ﬁlter with passband edge ωp = 0.025π and stopband edge
ωp = 0.25π into an 11 × 11 near-circular symmetric lowpass ﬁlter. The resulting 2-D fre-
quency response is shown in Figure 5.1–13. The input image is shown in Figure 5.1–14,
and the output image is shown in Figure 5.1–15. The difference image (biased up by
+128 for display on [0, 255]) is shown in Figure 5.1–16.
1.5
1
0.5
0
1
0.5
0
−0.5
−1 −1
−0.5
0
0.5
1
Frequency
Frequency
Magnitude
FIGURE 5.1–13
A lowpass ﬁlter for image ﬁltering. Note: MATLAB deﬁnes frequency f = ω/π.
FIGURE 5.1–14
Lena 512 × 512 input image.

5.1 FIR Filter Design
165
FIGURE 5.1–15
Output of lowpass ﬁlter in Example 5.1–4.
FIGURE 5.1–16
Corresponding difference image (biased for display by +128).
The corresponding MATLAB .m ﬁle MacDesFilt.m is given at the book’s Web site.
There you can experiment with changing the passband, ﬁlter type, image, etc. (Note this
routine requires MATLAB with the image processing toolbox.) Similarly, using an approx-
imately circular highpass ﬁlter, we can get the output shown in Figure 5.1–17, where we
note the similarity to Figure 5.1–16.

166
CHAPTER 5 Two-Dimensional Filter Design
FIGURE 5.1–17
Output of McClellan transformed near circular highpass ﬁlter.
Projection Onto Convex Sets
The method of projection onto convex sets (POCS) is quite general and has been
applied to several image processing optimization problems [6]. Here, we follow an
application to 2-D FIR ﬁlter design due to Abo-Taleb and Fahmy [7]. Using an
assumed symmetry in the ideal function and the desired FIR ﬁlter, we rewrite the
ﬁlter response in terms of a set of frequency domain basis functions as
H(ω1,ω2) =
M
X
k=1
a(k)φk(ω1,ω2),
where the ﬁlter coefﬁcients h(n1,n2) can be simply expressed in terms of the a(k),
using built-in symmetries assumed for h. Next, we densely and uniformly discretize
the basic frequency cell with N2 points as xi = (ωi
1,ωi
2), for i = 1,...,N2. Then we
can express the ﬁlter response at grid point xi as
H(xi) =< a,φ(xi) >,
where a is a column vector of the a(k) and φ is a vector of the φk, each of dimension
M, and < ·,· > denotes the conventional inner product. The frequency domain error
at the point xi then becomes
e(xi) = I(xi)−< a,φ(xi) >.
We can then write the approximation problem as
|I(xi)−< a,φ(xi) >| ≤δ
for all i = 1,...,N2.

5.1 FIR Filter Design
167
To apply the POCS method, we must ﬁrst observe that the sets
Qi ≜{a :|I(xi)−< a,φ(xi) >|≤δ}
are convex2 and closed. We note that we want to ﬁnd a vector a that is contained in
the set intersection
Q ≜∩N2
i=1Qi,
if such a point exists. In that case, the POCS algorithm is guaranteed to converge to
such a point(s) [7]. Note that the existence of a solution will depend on the tolerance
δ, and that we do not know how small the tolerance should be. What this means in
practice is that a sequence of problems will have to be solved for a decreasing set of
δ’s until the algorithm fails to converge, at which time the last viable solution is taken
as the result. Of course, one could also specify a needed value δ and then increase the
ﬁlter order, in terms of M, until a solution is obtained. The basic POCS algorithm can
be given as follows.
Basic POCS Algorithm
1. Scan through the frequency grid points xi to ﬁnd the frequency xp of maximum
error e(xp) = ep.
2. If ep ≤δ, stop.
3. Else, update the coefﬁcient vector as the orthogonal projection onto Qp [6, 7],
an+1 =
(
an −(δ −|ep|)φ(xp) sgn(ep)
||φ(xp)||,
if ep > δ,
an,
else.
4. Return to step 1.
The key step of this algorithm is step 3, which performs an orthogonal projec-
tion of the M-dimensional ﬁlter coefﬁcient vector an onto the constraint set Qp. As
the algorithm progresses, the solution point a may move into and out of any given
set many times, but is guaranteed to converge ﬁnally to a point in the intersection
of all the constraint sets, if such a point exists. An illustrative diagram of POCS
convergence is shown in Figure 5.1–18.
The references [6, 7] contain several examples of ﬁlters designed by this method.
The obtained ﬁlters appear to be quite close to the Chebyshev optimal linear phase
FIR designs of Harris and Mersereau [8], which are not covered here. The POCS
designs appear to have a signiﬁcant computational advantage, particularly for larger
ﬁlter supports.
2A convex set Q is one for which, if points p1 and
p2 are in the set, then so must be
p = αp1 + (1 −α)p2 for all 0 ≤α ≤1.

168
CHAPTER 5 Two-Dimensional Filter Design
Qp
Qi
a1
FIGURE 5.1–18
An illustration of the convergence based on orthogonal projection onto two convex sets.
5.2 IIR FILTER DESIGN
In this section, we ﬁrst look at conventional 2-D recursive ﬁlter design. This is fol-
lowed by a discussion of a generalization called fully recursive ﬁlters, which offers
the potential of even better performance.
2-D Recursive Filter Design
The Z-transform system function of a spatial IIR or recursive ﬁlter is given as
H(z1,z2) = B(z1,z2)
A(z1,z2),
where both B and A are polynomial3 functions of z1,z2, which in the spatial domain
yields the computational procedure, assuming a0,0 = 1,
y(n1,n2) = −
X
(k1,k2)∈Ra−(0,0)
ak1,k2y(n1 −k1,n2 −k2)
+
X
(k1,k2)∈Rb
bk1,k2x(n1 −k1,n2 −k2),
where the coefﬁcient support regions of the denominator and numerator are denoted
as Ra and Rb, respectively. Here, we consider a so-called direct form or unfactored
design, but factored designs are possible, and even preferable for implementation in
ﬁnite word-length arithmetic, as would be expected from the 1-D case. However, as
3Strictly speaking, the “polynomial” may include both positive and negative powers of the zi, but we
do not make the distinction here. We call both polynomials, if they are just ﬁnite order.

5.2 IIR Filter Design
169
we have seen earlier, various factored forms will not be equivalent due to the lack of
a ﬁniteorder factorization theorem in two and higher dimensions.
Typical Design Criteria
The choice of error criteria is important and related to the expected use of the ﬁlter. In
addition to the choice of either spatial-domain or frequency-domain error criteria, or
a combination of both, there is the choice of error norm. The following error criteria
are often used:
•
Space-domain design: Often a least-squares design criteria is chosen,
∥hI(n1,n2) −h(n1,n2)∥2,
via use of the so-called l2 error norm
∥f∥2 ≜
rXX
|f|2 (n1,n2).
•
Magnitude-only approximation:
∥|HI|(ω1,ω2) −|H|(ω1,ω2)∥,
where |HI| denotes an ideal magnitude function and |H| is the magnitude of the
ﬁlter being designed. The most common choice for the norm ∥·∥is the so-called
L2 norm
∥F∥2 ≜
v
u
u
u
t
+π
Z
−π
+π
Z
−π
|F|2 (ω1,ω2)dω1dω2.
Magnitude-only design is generally a carryover from 1-D ﬁlter design where the
common IIR design methods are based on transforming an analog ﬁlter via the
bilinear transform. These methods lead to very good ﬁlter magnitude response,
but with little control over the phase. For single channel audio systems, these
ﬁlter phase distortions can be unimportant. For image processing, though, where
2-D ﬁlters see a big application, phase is very important. So a magnitude-only
design of an image processing ﬁlter probably will not be adequate.
•
Zero-phase design:
HI(ω1,ω2) −|H|2(ω1,ω2)
,
where the ideal function HI is assumed to be real valued and nonnegative, often
the case in image processing. The ﬁlter is then realized by two passes. In the ﬁrst
pass, the image data is ﬁltered by h(n1,n2) in its stable direction, generally from
left-to-right and then top-to-bottom, the so-called raster scan of image processing.
The output data from this step is then ﬁltered by h∗(−n1,−n2) in its stable direc-
tion, from right-to-left and bottom-to-top. The overall realized frequency response
is then |H|2(ω1,ω2), which is guaranteed zero-phase. Note that this method only
yields non-negative frequency responses.

170
CHAPTER 5 Two-Dimensional Filter Design
•
Magnitude and phase (delay) design:
∥HI(ω1,ω2) −H(ω1,ω2)∥
or
∥|HI|(ω1,ω2) −|H|(ω1,ω2)∥+ λ∥argHI(ω1,ω2) −argH(ω1,ω2)∥,
where the Lagrange parameter λ controls the weight given to phase error ver-
sus magnitude error. A particular choice of λ will give a prescribed amount of
phase error. This critical value of λ, though, will usually only be approximately
determined after a series of designs for a range of λ values.
Space-Domain Design
We look at two methods, ﬁrst the design method called Pad´e approximation, which
gives exact impulse response values but over a usually small range of points. Then
we look at least-squares extensions, which minimize a modiﬁed error.
Pad´e Approximation
This simple method carries over from the 1-D case. Let
e(n1,n2) = hI(n1,n2) −h(n1,n2).
The squared l2 norm then becomes
∥e∥2
2 =
X
e2(n1,n2)
≜f(a,b),
a nonlinear function of the denominator and numerator coefﬁcient vectors a and b,
respectively. The Pad´e approximation method gives a linear closed-form solution that
achieves zero-impulse response error over a certain support region, whose number of
points equals dim(a) + dim(b).
Let x = δ so that y = h; then
h(n1,n2) = −
X
(k1,k2)>(0,0)
a(k1,k2)h(n1 −k1,n2 −k2) + b(n1,n2)
̸= hI(n1,n2),
where we have assumed that coefﬁcient a(0,0) = 1 without loss of generality. The
error function then becomes
e(n1,n2) = hI(n1,n2) +
X
(k1,k2)>(0,0)
a(k1,k2)h(n1 −k1,n2 −k2) + b(n1,n2),
which is nonlinear in the parameters a and b through the function h. To avoid the
nonlinearities, we now deﬁne the modiﬁed error:
eM(n1,n2) ≜hI(n1,n2) +
X
(k1,k2)>(0,0)
a(k1,k2)hI(n1 −k1,n2 −k2) −b(n1,n2)
= a(n1,n2) ∗hI(n1,n2) −b(n1,n2).
(5.2–1)

5.2 IIR Filter Design
171
Comparison of these two equations reveals that we have substituted hI for h inside
the convolution with a, which may be reasonable given a presumed smoothing
effect from coefﬁcient sequence a. To this extent we can hope that eM ≈e. Note
in particular, though, that the modiﬁed error eM is linear in the coefﬁcient vectors
a and b.
If dim(a) = p and dim(b) = q + 1, then there are N = p + q + 1 unknowns. We
can then set eM = 0 on RPad´e = {a connected N point region}. We have to choose
the region so that the ﬁlter masks slide over the set, rather than bringing new points
(pixels) into the equations. A simple example sufﬁces to illustrate this linear FIR
design method.
Example 5.2–1: Pad´e Approximation
Consider the 1- × 1-order system function
H(z1,z2) =
b00 + b01z−1
2
1 + a10z−1
1
+ a01z−1
2
,
where we have p = 2 and q + 1 = 2, equivalent to N = 4 unknowns. We look for a ﬁrst
quadrant support for the impulse response h and set RPad´e = {(0,0),(1,0),(0,1),(1,1)}.
Setting eM = 0, we have the following recursion:
hI(n1,n2) = −a10hI(n1 −1,n2) −a01hI(n1,n2 −1) + b00δ(n1,n2) + b01δ(n1,n2 −1).
Assuming zero initial (boundary) conditions, we evaluate this equation on the given region
RPad´e to obtain the four equations,
hI(0,0) = b00,
hI(1,0) = −a10hI(0,0),
hI(0,1) = −a01hI(0,0) + b01,
hI(1,1) = −a10hI(0,1) −a01hI(1,0),
which can be easily solved for the four coefﬁcient unknowns.
We have not yet mentioned that the ﬁlter resulting from Pad´e approximation may
not be stable! Unfortunately, this is so. Still if it is stable, and if the region is taken
large enough, good approximations can result. If the ideal impulse response is stable
(i.e., hI ∈l1), then the Pad´e approximate ﬁlter should also be stable, again if the
region is taken large enough. We also note that, because of the shape of the Pad´e
region RPad´e, effectively honoring the output mask of the ﬁlter, that eM = e there,
and so the obtained impulse response values are exact on that region. Thus, the key
is to include in the region RPad´e all the “signiﬁcant values” of the ideal impulse
response. Of course, this is not always easy and can lead to rather large ﬁlter orders.

172
CHAPTER 5 Two-Dimensional Filter Design
We next turn to an extension of Pad´e approximation called Prony’s method, which
minimizes the l2 spatial domain modiﬁed error over (in theory) its entire support
region, rather than just setting it to zero over the small region RPad´e.
Prony’s Method (Shank’s)
This is a linear least-squares method, which minimizes the modiﬁed error (5.2–1),
but again ignores stability. We write
EM =
X
e2
M(n1,n2)
=
X
Rb
e2
M(n1,n2) +
X
Rc
b
e2
M(n1,n2)
≜EM,Rb + EMRc
b,
making the obvious deﬁnitions, and note that the second error term will be indepen-
dent of coefﬁcient vector b. This because on Rc
b we can write
eM(n1,n2) = a(n1,n2) ∗hI(n1,n2)
= f(a),
so we can perform linear least squares to minimize EMRc
b over a. Again, we empha-
size that we cannot just choose a = 0 to make the modiﬁed error eM = 0, since
we have the constraint a(0,0) = 1 such that a(0,0) is not an element in the design
vector a.
Considering the practical case where there are a ﬁnite number of points to con-
sider, we can order the values eM(n1,n2) on Rc
b onto a vector, say eM, and then
minimize eT
MeM over the choice of denominator coefﬁcient vector a. A typical term
in eT
MeM would look like
e2
M(n1,n2) = [hI(n1,n2) + a(1,0)hI(n1 −1,n2) + a(0,1)hI(n1,n2 −1) + ···]2
since a(0,0) = 1. Thus taking partials with respect to the a(k1,k2), we obtain
∂(eT
MeM)/∂a(k1,k2) = 2
X
n1,n2
hI(n1 −k1,n2 −k2)

hI(n1,n2) + a(1,0)hI(n1 −1,n2)
+a(0,1)hI(n1,n2 −1) + ···


= 2
X
n1,n2
hI(n1 −k1,n2 −k2)

hI(n1,n2)
+
X
l1,l2
a(l1,l2)hI(n1 −l1,n2 −l2)

,

5.2 IIR Filter Design
173
and setting these partial derivatives to zero, we obtain the so-called normal equations
X
l1,l2
RI(k1 −l1,k2 −l2)a(l1,l2) = −RI(k1,k2) for (k1,k2) ∈supp{a},
(5.2–2)
with the deﬁnition of the “correlation terms”
RI(k1,k2) ≜
X
n1,n2
hI(n1 −k1,n2 −k2)hI(n1,n2).
Finally, (5.2–2) can be put into matrix-vector form and solved for the denominator
coefﬁcient vector a in the case when the matrix corresponding to the “correlation
function” is nonnegative deﬁnite, fortunately most of the time. Turning now to the
ﬁrst error term EM,Rb, we can write
eM(n1,n2) = a(n1,n2) ∗hI(n1,n2) −b(n1,n2)
(5.2–3)
= 0,
upon setting b(n1,n2) = a(n1,n2) ∗hI(n1,n2), using a(0,0) = 1 and the values of a
obtained in the least-squares solution. We then obtain EM,Rb = 0, and hence have
achieved a minimum for the total modiﬁed error EM.
Example 5.2–2: 1 × 1-order Case
Consider the design problem with a 1 × 1-order denominator with variable coefﬁcients
{a(1,0),a(0,1),a(1,1)} and a numerator consisting of coefﬁcients {b(0,0),b(1,0)}. Then set
Rc
b = {n1 ≥2,n2 = 0} ∪{n1 ≥0,n2 > 0} for a designed ﬁrst quadrant support. The comple-
mentary region is then Rb = {(0,0),(1,0)}. For a given ideal impulse response, with sup
p{hI} = {n1 ≥0,n2 ≥0}, we next compute RI(k1,k2) for (k1,k2) ∈Rc
b. Then we can write
the normal equations as
RI(0,0)a(1,0) + RI(1,−1)a(0,1) + RI(0,−1)a(1,1) = −RI(1,0),
RI(−1,1)a(1,0) + RI(0,0)a(0,1) + RI(−1,0)a(1,1) = −RI(0,1),
RI(0,1)a(1,0) + RI(1,0)a(0,1) + RI(0,0)a(1,1) = −RI(1,1),
which can be put into matrix form
RIa = −rI
and solved for vector a = [a(1,0),a(0,1),a(1,1)]T. Finally, we go back to (5.2–3) and solve
for b(0,0) and b(1,0) as b(n1,n2) = a(n1,n2) ∗hI(n1,n2) for (n1,n2) = (0,0) and (1,0).
As mentioned previously, there is no constraint of ﬁlter stability here. The result-
ing ﬁlter may be stable or it may not. Fortunately, the method has been found useful
in practice, depending on the stability of the ideal impulse response hI that is being
approximated. More on Prony’s method, and an iterative improvement method, is
contained in Lim [5].

174
CHAPTER 5 Two-Dimensional Filter Design
Fully Recursive Filter Design
The Z-transform system function of a fully recursive ﬁlter (FRF) is given as
H(z1,z2) = B(z1,z2)
A(z1,z2),
where both B and A are rational functions of z1 and polynomial functions of z2. The
2-D difference equation is given by
+∞
X
k1=−∞
LD
X
k2=0
a(k1,k2)y(n1 −k1,n2 −k2) =
+∞
X
k1=−∞
LN
X
k2=0
b(k1,k2)x(n1 −k1,n2 −k2),
which can be expressed in row-operator form as
a0(n1) ∗yn2(n1) = −
LD
X
k2=1
ak2(k1) ∗yn2−k2(n1) +
LN
X
k2=0
bk2(k1) ∗xn2−k2(n1),
(5.2–4)
where the 1-D coefﬁcient row sequences are given as
a0(n1) ≜a(n1,0),
b0(n1) ≜b(n1,0),
an2(n1) ≜a(n1,n2),
bn2(n1) ≜b(n1,n2),
and the convolution symbol ∗indicates the 1-D operation on each row, whether input
or output. Also the input and output row sequences are deﬁned as
yn2(n1) ≜y(n1,n2)
and
xn2(n1) ≜x(n1,n2).
If the coefﬁcient row sequences are 1-D FIR, then we have a conventional 2-D
recursive ﬁlter, but here we consider the possibility that they may be of inﬁnite
support. Note that even 2-D FIR ﬁlters are included in the FRF class, by setting
a0(n1) = δ(n1), other an2(n1) = 0, and bn2(n1) = b(n1,n2) = FIR. Upon taking the
2-D Z-transform of (5.2–4), we obtain
A0(z1)Y(z1,z2) = −
LD
X
k2=1
Ak2(z1)Y(z1,z2)z−k2
2
+
LN
X
k2=0
Bk2(z1)X(z1,z2)z−k2
2
.
Now we introduce the following rational forms for the 1-D coefﬁcient Z-transforms
Ak2 and Bk2,
Ak2(z1) =
Na
k2(z1)
Da
k2(z1)
and
Bk2(z1) =
Nb
k2(z1)
Db
k2(z1)
,
where the numerator and denominators are ﬁniteorder polynomials in z1. They thus
constitute rational row operators that can be implemented via recursive ﬁlters sepa-
rately processing the present and most recent LN input rows and LD previous output
rows. The row coefﬁcients an2(n1) and bn2(n1) are then just the impulse responses
of these 1-D row operators. The row operator on the current output row A0 can then

5.2 IIR Filter Design
175
be implemented via inverse ﬁltering, after completion of the sums indicated on the
righthand side of (5.2–4).
The overall FRF system function can then be written as
H(z1,z2) =
PLN
k2=0
Nb
k2(z1)
Db
k2(z1)z−k2
2
PLD
k2=0
Na
k2(z1)
Da
k2(z1)z−k2
2
.
We pause now for an example.
Example 5.2–3: First-Order FRF with Nonsymmetric Half-Plane Impulse
Response Support
Let LN = 1 and LD = 1. Take the feedback row operators A0 and A1 with the following
system functions
A0(z1) = 1 + 0.8z−1
1
1 + 0.9z−1
1
and
A1(z1) = 0.8 2 + z−1
1
1 + 0.6z−1
1
,
with ROC containing {|z1| ≤1} and input row operators B0 and B1 with system functions
B0(z1) =
1 + z−1
1
1 + 0.7z−1
1
and
B1(z1) = 0.7 1 + z−1
1
1 + 0.8z−1
1
,
also with ROC containing {|z1| ≤1}. Then the overall FRF system function is
H(z1,z2) = B0(z1) + B1(z1)z−1
2
A0(z1) + A1(z1)z−1
2
=
1+z−1
1
1+0.7z−1
1
+
1+z−1
1
1+0.8z−1
1
z−1
2
1+0.8z−1
1
1+0.9z−1
1
+
2+z−1
1
1+0.6z−1
1
z−1
2
.
Observations
1. Each 1-D row operator can be factored into poles inside and poles outside the
unit circle and then split into ⊕and ⊖components, where the ⊕component
is recursively stable as a right-sided (causal) sequence and the ⊖component is
recursively stable as a left-sided (anticausal) sequence.
2. If the inverses A−1
0 (z1) and B−1
0 (z1) are stable and right-sided, then we get non-
symmetric half-plane (NSHP) support for the overall FRF impulse response. We
get symmetric half-plane (SHP) response support as shown in Figure 5.2–1 when
A−1
0
and B−1
0
are stable and two-sided.
3. It is important to note, in either case, that the impulse responses are not restricted
to wedge support as would be the case for a conventional NSHP recursive ﬁlter.

176
CHAPTER 5 Two-Dimensional Filter Design
n2
n1
NSHP
n2
n1
SHP
FIGURE 5.2–1
Illustration of FRF support options. Note that FRF impulse response support extends to the
complete SHP or NSHP region.
The stability of FRF ﬁlters is addressed in [9]. The following ﬁlter design example
uses a numerical measure of stability as an additional design constraint, thus ensuring
that the designed ﬁlter satisﬁes a numerical version of the FRF stability test [9]. The
example concerns an FRF design for the ideal 2-D Wiener ﬁlter, an optimal linear ﬁl-
ter for use in estimating signals in noise, which will be derived and used in Chapter 8
on image processing.
Example 5.2–4: SHP Wiener Filter Design
Using the Levenberg-Marquardt optimization method, the following SHP Wiener ﬁlter was
designed in [9]. This FRF used 65 coefﬁcients, approximately equally spread over the
numerator and denominator and involving ﬁve input and output rows. Figures 5.2–2 and
5.2–3 show the ideal and designed magnitude response, and Figures 5.2–4 and 5.2–5
show the respective phase responses.
We see that all the main features of the ideal magnitude and phase response of the
Wiener ﬁlter are achieved by the fully recursive design. This is because the FRF impulse
response is not constrained to have support on a wedge of the general causal half-space
like the conventional NSHP recursive ﬁlter. The measured magnitude MSE was 0.0028
and the phase MSE was 0.042. As seen in [9], the conventional recursive NSHP ﬁlter was
not able to achieve this high degree of approximation. (See Chapter 8 for more on 2-D
Wiener ﬁlters and their applications).
FIGURE 5.2–2
Ideal SHP Wiener ﬁlter magnitude (origin in middle).

5.3 Subband/Wavelet Filter Design
177
FIGURE 5.2–3
SHP Wiener FRF magnitude (origin in middle).
FIGURE 5.2–4
Ideal SHP Wiener phase response (origin in middle).
FIGURE 5.2–5
SHP Wiener FRF phase response (origin in middle).
5.3 SUBBAND/WAVELET FILTER DESIGN
Most spatial subband/wavelet or SWT ﬁlter design uses the separable product
approach. Here, we just discuss some 1-D design methods for the component ﬁlters as
shown in Figure 5.3–1. Johnston [10] designed pairs of even-length quadrature mirror

178
CHAPTER 5 Two-Dimensional Filter Design
x(n)
x(n)
h0(n)
g0(n)
2↓
h1(n)
g1(n)
2↑
2↑
2↓
+
FIGURE 5.3–1
1-D diagram of analysis/synthesis SWT/ISWT system.
ﬁlters (QMF) with linear phase using a computational optimization method. Aliasing
was canceled out via the method of Esteban and Galand [11], which makes the
synthesis choice G0(ω) ≜H1(ω −π) and G1(ω) ≜−H0(ω −π), and the lowpass–
highpass property of the analysis ﬁlterbank was assured via H1(ω) = H0(ω −π).
Thus the design can concentrate on the transfer function
T(ω) ≜1
2
h
H2
0(ω) −H2
0(ω −π)
i
.
In the perfect reconstruction case, T(ω) = 1, and sobx(n) = x(n). In the case of John-
ston QMFs, perfect reconstruction is not possible, but we can achieve a very good
approximation bx(n) ≃x(n), as will be seen here. For a given stopband spec ωs and
energy tolerance ϵ, Johnston seeks to minimize
π/2
Z
0
[|T(ω)| −1]2 dω subject to constraint
π
Z
ωs
|H0(ω)|2 dω ≤ϵ,
using the Hooke and Jeeves nonlinear optimization algorithm [12]. His design algo-
rithm took advantage of linear phase by working with the real-valued quantity f
H0
deﬁned by the relation
f
H0(ω) ≜e+jω(N−1)/2H0(ω).
He used a dense frequency grid to approximate the integrals and used the Lagrangian
method to bring in the stopband constraint, resulting in minimization of total error
function E:
E ≜Er + λEs,
with
Er ≜2
π/2
X
ωi=0.
h
f
H0
2(ωi) + f
H0
2(π −ωi) −1
i2
,
(5.3–1)
Es ≜
π
X
ωi=ωs
f
H0
2(ωi).

5.3 Subband/Wavelet Filter Design
179
He thus generated a family of even-length, linear-phase FIR ﬁlters parameterized
by their length N, stopband ωs, and stopband tolerance ϵ. Results were reported in
terms of transition bandwidth, passband ripple, and stopband attenuation. Johnston
originally designed his ﬁlters for audio compression. Note that since we trade off Er
versus Es, we do not get perfect reconstruction with these QMF designs; still, the
approximation in the total transfer function T(ω) can be very accurate, i.e., within
0.02 dB, which is quite accurate enough so that errors are not visible in typical image
processing displays.
Note that design equation (5.3–1) is only consistent with T(ω) = 1/2, so an
overall multiplication by 2 would be necessary when using these Johnston ﬁlters.
Example 5.3–1: Johnston’s Filter 16C
Here, we look at the step response and frequency response of a 16-tap QMF designed by
Johnston [10] using the Hooke and Jeeves nonlinear optimization method. For the 16C
ﬁlter, the value of λ = 2 achieved stopband attenuation of 30 dB. The transition band-
width was 0.10 radians and the achieved overall transmission was ﬂat to within 0.07 dB.
Another similar ﬁlter, the 16B, achieved 0.02 dB transmission ﬂatness. The step response
in Figure 5.3–2 shows about a 5–10% amount of overshoot or ringing.
1.2
0.8
0.6
0.4
0.2
0
10
20
30
40
50
Sample
Step response
60
70
80
90
100
−0.2
0
1
FIGURE 5.3–2
Step response of Johnston 16C linear-phase QMF.
The frequency response in Figure 5.3–3 shows good lowpass and highpass analysis
characteristics.
Other ﬁlters, due to Simoncelli and Adelson [13], were designed using a similar
optimization approach, but using a min–max or l∞norm and incorporating a modiﬁed
1/|ω| weighting function with bias toward low frequency to better match the human

180
CHAPTER 5 Two-Dimensional Filter Design
1.2
0.8
0.6
0.4
0.2
0
0.5
1
1.5
2
2.5
Frequency
Gain
3
3.5
0
1
FIGURE 5.3–3
Magnitude frequency response of Johnston 16C ﬁlter.
visual system. Their designed ﬁlters are useful for both odd and even lengths N due
to their use of the alternative QMF condition (4.4–10) and reconstruction equations
(4.4–9) in Chapter 4. Notable is their 9-tap ﬁlter, which performs quite well in image
coding.
Wavelet (Biorthogonal) Filter Design Method
The 1-D subband ﬁlter pairs designed by wavelet analysis generally relate to what
is called maximally ﬂat design in signal processing literature [14], and the ﬁlters
are then used for separable 2-D subband analysis and synthesis as discussed pre-
viously. The term biorthogonal is used in the wavelet literature to denote the case
where the analysis ﬁlter set {h0,h1} is different from the synthesis ﬁlter set {g0,g1}.
Generally the extra freedom in the design of biorthogonal ﬁlters results in a more
accurate design for the lowpass ﬁlters combined with perfect reconstruction. On the
other hand, departures from orthogonality generally have a negative effect on cod-
ing efﬁciency. So the best biorthogonal wavelet ﬁlters for image coding are usually
nearly orthogonal.
Rewriting the perfect reconstruction SWT equations from Section 4.4 of
Chapter 4 in terms of Z-transforms, we have the transfer function
H0(z)G0(z) + H1(z)G1(z) = 2,
(5.3–2)
given the aliasing cancellation condition
H0(−z)G0(z) + H1(−z)G1(z) = 0.
(5.3–3)
In the papers by Antonini et al. [15] and Cohen et al. [16] on biorthogonal
subband/wavelet ﬁlter design, a perfect reconstruction solution is sought under the

5.3 Subband/Wavelet Filter Design
181
constraint that the analysis and synthesis lowpass ﬁlters h0 and g0 be symmetric and
FIR—that is, linear phase with phase zero. They cancel out aliasing (5.3–3) via the
choices [17]
H1(z) = zG0(−z)
and
G1(z) = z−1H0(−z).
(5.3–4)
The transfer function (5.3–2) then becomes
H0(z)G0(z) + H0(−z)G0(−z) = 2,
which when evaluated on the unit circle z = ejω gives the overall system frequency
response
H0(ω)G0(ω) + H0(ω −π)G0(ω −π) = 2,
(5.3–5)
owing to the use of real and symmetric lowpass ﬁlters h0 and g0.
It now remains to design h0 and g0 to achieve this perfect reconstruction (5.3–2)
or (5.3–5). For this they use a spectral factorization method and introduce a pre-
scribed parameterized form for the product H0(ω)G0(ω) that is known to satisfy
(5.3–5) exactly. Since it is desired that both the analysis and reconstruction ﬁlters
have a high degree of ﬂatness, calling for zeros of the derivative at ω = 0 and π, the
following equation for the product H0G0 was proposed in [16]:
H0(ω)G0(ω) = cos2K(ω/2)


L−1
X
p=0
 L −1 + p
p

sin2p(ω/2)

,
(5.3–6)
where cos(ω/2) provides zeros at ω = π and the terms sin(ω/2) provide zeros at
ω = 0. Then actual ﬁlter solutions can be obtained with varying degrees of spectral
ﬂatness and various lowpass characteristics, all guaranteed to give perfect reconstruc-
tion, by factoring the function on the right-hand side of (5.3–6) for various choices
of K and L.
The polynomial on the right-hand side of (5.3–6), call it |M0(ω)|2, is half of a
power complementary pair deﬁned by the relation
|M0(ω)|2 + |M0(ω −π)|2 = 2,
which had been considered earlier by Smith and Barnwell [18] for the design of per-
fect reconstruction orthogonal subband/wavelet ﬁlters. A particularly nice treatment
of this wavelet design method is presented in Taubman and Marcellin [19].
Example 5.3–2: The 9/7 Cohen-Daubechies-Feauveau (CDF) Filter
In [15], the authors consider several example solutions. The one that works the best in
their image coding example corresponds to factors H0 and G0 in (5.3–6) that make h0
and g0 have nearly equal lengths. A solution corresponding to K = 4 and L = 4 resulted
in the following 9-tap/7-tap (or simply 9/7) ﬁlter pair:

182
CHAPTER 5 Two-Dimensional Filter Design
n
0
±1
±2
±3
±4
2−1/2h0(n)
0.602949
0.266864
−0.078223
−0.016864
0.026749
2−1/2g0(n)
0.557543
0.295636
−0.028772
−0.045636
0.0
Figure 5.3–4 shows the lowpass and highpass analysis system frequency response
plotted from a 512-point FFT. The corresponding analysis lowpass ﬁlter step response
is shown in Figure 5.3–5. The synthesis lowpass ﬁlter step response is given in
Figure 5.3–6. This synthesis step response seems quite similar to that of the Johnston
ﬁlter in Figure 5.3–2, with perhaps a bit less ringing.
This ﬁlter pair has become the most widely used method of SWT analysis and
synthesis (ISWT) and is chosen as the default ﬁlter for the ISO standard JPEG 2000
[17, 19]. It is regarded for its generally high coding performance and reduced amount
of ringing when the upper subband is lost for coding reasons such as quantizing
coefﬁcients to 0. This ﬁlter set is generally denoted as “CDF 9/7,” for the authors in
[16], or often as just “Daubechies 9/7.”
In comparing the QMFs with the wavelet-designed ﬁlters, note that the main dif-
ference is that the QMFs have a better passband response with a sharper cutoff, which
can reduce aliasing error in the lowpass band. The wavelet-designed ﬁlters tend to
have poorer passband response but less ringing in the synthesis ﬁlter step response,
and hence less visual artifacts under strong coefﬁcient quantization.
0
100
200
300
400
500
600
0
0.2
0.4
0.6
0.8
1
1.2
1.4
FIGURE 5.3–4
Frequency response of CDF 9/7 analysis ﬁlters (512 pt. DFT).

5.3 Subband/Wavelet Filter Design
183
0
5
10
15
20
25
30
35
40
45
50
−0.2
0
0.2
0.4
0.6
0.8
1.2
1
FIGURE 5.3–5
Step response of CDF 9/7 analysis lowpass ﬁlter.
0
5
10
15
20
25
30
35
40
45
50
−0.2
0
0.2
0.4
0.6
1.2
0.8
1
FIGURE 5.3–6
The step response of CDF 9/7 synthesis lowpass ﬁlter.

184
CHAPTER 5 Two-Dimensional Filter Design
Example 5.3–3: Anti-alias Filtering
Here, we take a 4CIF (704 × 576) monochrome image and subsample it to CIF (352 × 288)
using two different anti-alias ﬁlters. On the left in Figure 5.3–7 is the result using the
CDF 9/7 subband/wavelet ﬁlter. The image on the right was obtained using the MPEG4-
recommended lowpass ﬁlter prior to the decimator. The MPEG4 lowpass recommendation
has the form
n
0
±1
±2
±3
±4
±5
±6
64h(n)
24
19
5
−3
−4
0
2
(a) CDF 9/7 filtered
(b) MPEG4 rec. filtered
FIGURE 5.3–7
An illustration of using two different anit-alias ﬁlters for downsampling.
We can see that the image on the right in Figure 5.3–7 is softer, but also that the one
on the left has signiﬁcant spatial frequency aliasing that shows up on the faces of some of
the buildings. It should be noted that this image frame from the MPEG test video City has
an unusual amount of high-frequency data. Most 4CIF images would not alias this badly
with the CDF 9/7 lowpass ﬁlter.
CONCLUSIONS
In this chapter we introduced 2-D and spatial ﬁlter design. We studied FIR ﬁlters with
emphasis on the window design method but also considered the 1-D to 2-D transfor-
mation method and the application of the POCS method to spatial ﬁlter design. In
the case of IIR or recursive ﬁlters, we studied some simple spatial-domain design
methods and then brieﬂy overviewed computer-aided design of spatial IIR ﬁlters.
We introduced FRFs and their design, and saw an example of using an FRF ﬁlter

Problems
185
to give a good approximation to the frequency response of an ideal spatial Wiener
ﬁlter (to be presented in Chapter 8). Finally, we discussed the design problem for
subband/wavelet ﬁlters as used in a separable SWT.
PROBLEMS
1. A real-valued ﬁlter h(n1,n2) has support on [−L,+L] × [−L,+L] and has a
Fourier transform H(ω1,ω2) that is real valued also. What kind of symmetry
must h have in the n1,n2 plane?
2. Let the real-valued impulse response have quadrantal symmetry:
h(n1,n2) = h(−n1,n2) = h(n1,−n2) = h(−n1,−n2).
What symmetries exist in the Fourier transform? Is it real? How can this be used
to ease the computational complexity of ﬁlter design?
3. This problem concerns FIR ﬁlter design with the constraint of quadrantal
symmetry on the impulse response:
h(n1,n2) = h(−n1,n2) = h(n1,−n2) = h(−n1,−n2).
Let the support of this FIR ﬁlter h be given as [−M,+M]2.
(a) Show that the corresponding frequency response has the following sym-
metry:
H(ω1,ω2) = H(−ω1,ω2) = H(+ω1,−ω2) = H(−ω1,−ω2),
and hence the frequency-domain error need only be evaluated in the ﬁrst
quadrant of [−π,+π]2. (Assume that the ideal frequency response function
HI(ω1,ω2) has quadrantal symmetry too.)
(b) Determine a set of design variables ak1,k2 and basis functions φk1,k2(ω1,ω2)
to efﬁciently express the frequency response
H(ω1,ω2) =
X
(k1,k2)∈R
ak1,k2φk1,k2(ω1,ω2),
where R = [0,M]2. The ak1,k2 should be expressed in terms of the h(k1,k2),
and the φk1,k2 should be expressed in terms of the cosω1k1 and cosω2k2.
4. A certain ideal lowpass ﬁlter with cutoff frequency ωc = π
2 has impulse
response
hd(n1,n2) =
1
q
n2
1 + n2
2
J1
π
2
q
n2
1 + n2
2

.
(a) What is the passband gain of this ﬁlter?

Hint: lim
x →0
J1(x)
x
= 1
2


186
CHAPTER 5 Two-Dimensional Filter Design
(b) We want to design an N × N-point, linear phase, FIR ﬁlter h(n1,n2) with ﬁrst
quadrant support using the ideal function hd(n1,n2) given above. We choose
a separable 2-D Kaiser window with parameter β = 2. The continuous-time
1-D Kaiser window is given as
wα(t) =

I0(β
p
1 −(t/τ)2)/I0(β),
|t| < τ,
0,
else.
Write an expression for the ﬁlter coefﬁcients of h—i.e., h(n1,n2) : n1 =
0,...,N −1, n2 := 0,...,N −1.
(c) If we use a 512 × 512-point row–column FFT to approximately implement
the preceding designed ﬁlter h for a 512- × 512-pixel image,
•
First, evaluate the number of complex multiplies used as a function
of M for an M × M image, assuming the component 1-D FFT uses
the Cooley-Tukey approach. (The 1-D Cooley-Tukey FFT algorithm
needs 1
2M log2 M complex multiplies when M is a power of 2 approach.)
Specialize your result to M = 512.
•
What portion of the output will be the correct (linear convolution) result?
Specify which pixels will be correct.
5. We want to design an N × N-point, linear phase, FIR ﬁlter h(n1,n2) with ﬁrst
quadrant support using the ideal function hi(n1,n2) given as
hi(n1,n2) = 1
4
J1

π
2
q
n2
1 + n2
2

q
n2
1 + n2
2
.
We choose a separable 2-D Kaiser window with parameter β = 2. The continu-
ous time 1-D Kaiser window is given as
wα(t) =

I0(α
p
1 −(t/τ)2 )/I0(α),
|t| < τ,
0,
else.
(a) Write down the equation for the ﬁlter coefﬁcients of h—i.e., h(n1,n2) : n1 =
0,...,N −1, n2 = 0,...,N −1. Assume N is odd.
(b) Use MATLAB to ﬁnd the magnitude of the frequency response of the
designed ﬁlter. Plot the result for N = 11.
6. Let 0 < ωp < π, and deﬁne the 1-D ideal ﬁlter response
HI(ω) ≜

1,
|ω| ≤1
2
 ωs + ωp

,
0,
else,
on [−π,+π].
Then deﬁne HI1(ω) ≜W1(ω) ⊗HI(ω), where ⊗denotes periodic convolution
and where
W1(ω) ≜
(
1
ωs−ωp ,
|ω| ≤1
2
 ωs −ωp

,
0,
else,
on [−π,+π], with π > ωs > ωp.

Problems
187
0
HI1(ω)
+π
−π
1.0
ωp
ωs
−ωs
−ωp
ω
FIGURE 5.P–1
Alternative ideal ﬁlter response.
HI1(ω) is as shown in Figure 5.P–1.
(a) Find the impulse response hI1(n). Note that it is of inﬁnite support.
(b) Deﬁne
and
sketch
the
2-D
ideal
ﬁlter
response
HI1(ω1,ω2) ≜
HI1(ω1)HI1(ω2) and give an expression for its impulse response hI1(n1,n2).
(c) Find the so-called l2 or minimum least-squares error approximation to the
ideal spatial ﬁlter you found in part (b), with support [−N,+N]2.
7. Modify the MATLAB program MacDesFilt.m (available at the book’s Web
site) to design a nearly circular highpass ﬁlter with stopband edge ωs = 0.1 and
ωp = 0.3, and apply the ﬁlter to the Lena image. (Note: This requires MATLAB
with image processing toolbox.) Is the output what you expected? Try scaling
and shifting the output y to make it ﬁt in the output display window [0,255].
(Hint: The output scaling y=128+2*y works well.)
8. In deriving the McClellan transformation method, we need to ﬁnd the a′(n) coef-
ﬁcients in (5.1–2) in terms of the a(n) coefﬁcients in (5.1–1). In this problem,
we work out the case for M = 4.
(a) Use Tn(x) for n = 0,4 to evaluate cosnω in terms of powers of cosk ω for
k = 0,4.
(b) Substitute these expressions into (5.1–1) and determine a′(n) in terms of
a(n) for n = 0,4.
(c) Solve the equations in part (b) for a(n) in terms of a′(n).
9. In the design by transformation method due to McClellan, assume the 1-D linear-
phase (type I or N odd) FIR ﬁlter has coefﬁcients h(0),h(1),...,h(N −1).
(a) Express the resulting 2-D ﬁlter’s coefﬁcients h(n1,n2) in terms of the a′(n)
derived from the 1-D ﬁlter coefﬁcients h(n). Do this for the general trans-
formation F(ω1,ω2) in terms of parameters A,B,C,D, and E as given in
(5.1–3). In your expression for h(n1,n2), you may use the convolution power
deﬁned as
x∗k ≜x ∗x∗(k−1),
k > 1 with x∗1 ≜x.
(b) Devise an implementation method that minimizes the number of nontrivial
multiplications assuming that A−E are negative powers of 2.

188
CHAPTER 5 Two-Dimensional Filter Design
10. Let the ﬁlter h with ﬁrst quadrant impulse response support have system
function
H(z1,z2) =
b00
1 + a10z−1
1
+ a01z−1
2
+ a11z−1
1 z−1
2
.
(a) Find the coefﬁcients {b00;a10,a01,a11} such that the impulse response h
agrees with the four prescribed values:
h(0,0) = 1.0,
h(1,0) = 0.8,
h(0,1) = 0.7,
h(1,1) = 0.6.
(b) Determine whether the resulting ﬁlter is stable or not. (Hint: Try the root
mapping method.)
11. In 2-D ﬁlter design, we often resort to a general optimization program such as
Fletcher-Powell to perform the design. Such programs often ask for analytic
forms for the partial derivatives of the speciﬁed error function with respect to
each of the ﬁlter coefﬁcients.
Consider a simple recursive ﬁlter with frequency response
H(ω1,ω2) =
a
1 −be−jω1 −ce−jω2 ,
where the three ﬁlter parameters a,b, and c are real variables.
(a) Find the three partial derivatives of H with respect to each of a,b, and c, for
ﬁxed ω1,ω2.
(b) Find the corresponding partial derivatives of the conjugate function H∗.
(c) Choose a weighted mean-square error function as
E =
1
(2π)2
+π
Z
−π
+π
Z
−π
W(ω)|I(ω) −H(ω)|2dω,
for given ideal frequency-response function I and positive weighting func-
tion W. Find the partial derivatives ∂E/∂a,∂E/∂b, and ∂E/∂c, making use
of your results in parts (a) and (b). (Hint: Rewrite |A|2 = AA∗ﬁrst.) Express
your answers in terms of W,I, and H.
12. Consider a zero-phase or two-pass ﬁlter design with NSHP recursive ﬁlters
H(ω1,ω2) ≜H⊕+(ω1,ω2)H⊖−(ω1,ω2) = |H(ω1,ω2)|2,
where H⊖−(ω1,ω2) ≜H∗
⊕+(ω1,ω2). Equivalently, in the spatial domain
h(n1,n2) = h⊕+(n1,n2) ∗h⊖−(n1,n2).
Let the NSHP ﬁlter H⊕+(ω1,ω2) have real coefﬁcients with numerator support
and denominator support as indicated in Figure 5.P–2.

Problems
189
numerator
n1
n2
1 2
−1
2
1
denominator
n1
n2
1
2
−1
1 2 3
FIGURE 5.P–2
Numerator and denominator coefﬁcient support region indicated by open circles.
(a) Find the spatial support of h⊕+(n1,n2) assuming that this ﬁlter is stable in
the +n1,+n2 direction.
(b) State the spatial support of h⊖−(n1,n2).
(c) What is the support of h(n1,n2)?
13. Show that the biorthogonal subband/wavelet constraint equation (5.3–4)
achieves alias cancellation in (5.3–3) and that the transfer function (5.3–2)
becomes
H0(z)G0(z) + H0(−z)G0(−z) = 2,
which when evaluated on the unit circle z = ejω becomes
H0(ω)G0(ω) + H0(ω −π)G0(ω −π) = 2,
owing to the use of real and symmetric lowpass ﬁlters h0 and g0.
14. The lifted SWT is deﬁned starting from the lazy SWT, which just separates the
input sequence x into even and odd indexed terms. In the lifted SWT, this is
followed by prediction and update steps as speciﬁed by the operators P and U,
respectively, as shown in Figure 5.P–3. The output multipliers α0 and α1 may
be needed for proper scaling.
x(n)
2↓
2↓
+
z−1
+
P
U
Σ−
y1(n)
y0(n)
α1
α0
FIGURE 5.P–3
An illustration of the lifted SWT.

190
CHAPTER 5 Two-Dimensional Filter Design
(a) Find the corresponding prediction and update operators for the SWT that
uses the Haar ﬁlter set
h0(n) = δ(n) + δ(n −1),
h1(n) = δ(n) −δ(n −1).
(b) A key property of lifting is that the operators P and U are not constrained at
all. Show the ISWT for the lifted transform in Figure 5.P–3 by starting from
the right-hand side of the ﬁgure and ﬁrst undoing the update step and then
undoing the prediction step. Note that this is possible even for nonlinear
operators P and U.
(c) Can you do the same for the LeGall-Tabatabai (LGT) 5/3 analysis ﬁlter set
h0(n) =

−1
8, 1
4, 3
4, 1
4,−1
8

h1(n) =

−1
2,1,−1
2

?
The corresponding synthesis 5/3 ﬁlter set is
g0(n) =
1
2,1, 1
2

g1(n) =

−1
8,−1
4, 3
4,−1
4,−1
8

.
Here, h0 and g0 are centered on n = 0, while h1 is centered at n = −1 and g1
is centered on n = +1. Reference to [17] and/or [19] may be helpful here.
REFERENCES
[1] J. F. Kaiser, “Nonrecursive Digital Filter Design Using I0−sinh Window Function,” Proc.
IEEE Sympos. on Circuits and Systems, pp. 20–23, 1974.
[2] D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing,
Prentice-Hall, Englewood Cliffs, NJ, 1983.
[3] A. Oppenheim, R. W. Schafer, and J. R. Buck, Discrete-Time Signal Processing, 2nd Ed.
Prentice-Hall, Englewood Cliffs, NJ, 1999.
[4] J. H. McClellan and D. S. K. Chan, “A 2-D FIR Filter Structure Derived from the
Chebyshev Recursion,” IEEE Trans. Circuits Systems, vol. CAS-24, pp. 372–378,
July 1977.
[5] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
[6] H. Stark and Y. Yang, Vector Space Projections, John Wiley, New York, 1998.
[7] A. Abo-Taleb and M. M. Fahmy, “Design of FIR Two-Dimensional Digital Filters by
Successive Projections,” IEEE Trans. Circuits and Sys., vol. CAS-31, pp. 801–805,
September 1984.

References
191
[8] D. B. Harris and R. M. Mersereau, “A Comparison of Algorithms for Minimax Design of
Two-Dimensional Linear Phase FIR Digital Filters,” IEEE Trans. Accoust., Speech, and
Signal Process. vol. ASSP-25, pp. 492–500, December 1977.
[9] J.-H. Lee and J. W. Woods, “Design and Implementation of Two-Dimensional Fully
Recursive Digital Filters,” IEEE Trans. Accoust., Speech, and Signal Process. vol.
ASSP-34, pp. 178–191, February 1986.
[10] J. D. Johnston, “A Filter Family Designed for Use in Quadrature Mirror Filter Banks,”
Proc. IEEE Intl. Conf. Accoust., Speech, and Signal Process (ICASSP), pp. 291–294,
Denver, CO, 1980.
[11] D. Esteban and C. Galand, “Application of Quadrature Mirror Systems to Split Band
Voice Coding Schemes,” Proc. ICASSP, pp. 191–195, May 1977.
[12] Hooke & Jeeves, “Direct Search Solution of Numerical and Statistical Problems,”
Journal of the ACM, vol. 8, pp. 212–229, April 1961.
[13] E. P. Simoncelli and E. H. Adelson, “Subband Transforms,” Chapter 4 in Subband Image
Coding, Ed. J. W. Woods, Kluwer, Dordrecht, NL, pp. 143–192, 1991.
[14] P. P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice-Hall, Englewood Cliffs,
NJ, 1993.
[15] M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, “Image Coding Using Wavelet
Transform,” IEEE Trans. Image Process., vol. 1, pp. 205–220, April 1992.
[16] A. Cohen, I. Daubechies, and J.-C. Feauveau, “Biorthogonal Bases of Compactly
Supported Wavelets,” Comm. Pure and Applied Math., vol. XLV, pp. 485–560, 1992.
[17] M. Rabbani and R. Joshi, “An Overview of the JPEG 2000 Still Image Compression
Standard,” Signal Process Image Comm., vol. 17, pp. 3–48, 2002.
[18] M. J. T. Smith and T. P. Barnwell III, “Exact Reconstruction for Tree-structured Subband
Coders,” IEEE Trans. Accoust., Speech, and Signal Process. vol. ASSP-34, pp. 431–441,
June 1986.
[19] D. S. Taubman and M. W. Marcellin, JPEG2000: Image Compression, Fundamentals,
Standards, and Practice, Kluwer Academic Publishers, Norwell, MA, 2002.

CHAPTER
Image Perception
and Sensing
6
This chapter covers some basics of image processing that are necessary in the
applications in later chapters. We will discuss light, sensors, and the human visual
system. We will talk about spatial and temporal properties of human vision and
present a basic spatiotemporal frequency response of the eye–brain system. This
information, interesting in its own right, will be useful for the design and analysis
of image and video signal processing systems, which produce images and video to be
seen by human observers.
6.1 LIGHT AND LUMINANCE
We use the notation pi(x,y,t,λ) for incident radiant ﬂux or light intensity (inci-
dent power) as a function of position (x,y) and time t at the spatial wavelength λ.
The human visual system (HVS) does not perceive this radiant ﬂux directly. A rel-
ative luminous efﬁciency v(λ) has been deﬁned to relate our perceived luminance
(brightness) to the incoming radiant ﬂux
l(x,y,t) ≜K
∞
Z
0
pi(x,y,t,λ)v(λ)dλ,
where the constant K = 685 lumens/watt and the luminance l is expressed in
lumens/square-meter [1]. Suppressing the dependence on space and time, we can
write this equation more simply as
l ≜K
∞
Z
0
pi(λ)v(λ)dλ
and think of it as deﬁning equivalence classes of radiant intensity functions pi(λ)
that all have the same luminance—i.e., all look equally bright to a human observer.
We can think of the relative luminous efﬁciency v(λ) as a kind of a wavelength
ﬁlter response of the human eye. As such it would be expected to vary somewhat
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00006-0
c⃝2012 Elsevier Inc. All rights reserved.
193

194
CHAPTER 6 Image Perception and Sensing
400
450
500
550
600
650
700
750
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Wavelength in nm
CIE relative luminous efficiency
FIGURE 6.1–1
CIE 1929 relative luminous efﬁciency (standard observer) curve ν(λ).
from person to person, but a so-called standard observer was deﬁned in 1929 by the
Commission Internationale de L’eclairage (International Commission on Illumina-
tion), known as the CIE, and is plotted in Figure 6.1–1, thereby deﬁning a standard
luminance with symbol Y.
The question naturally arises as to how such a relative luminous efﬁciency func-
tion v(λ) can be measured. Actually, it was done using a clever brightness matching
experiment where volunteer viewers were shown reference and test images on a split
screen in a narrow viewing angle of 2◦and asked to report when they just noticed
a brightness difference between the two nearby spectral pairs [2]. Since the original
experiments, various reﬁnements in the method have yielded modest improvements
in accuracy, principally in the blue or short wavelength area. However, the original
CIE curve is still used as the reference for most photometric work, photometry being
the science of measuring the human visual effect of various light stimuli.
An alternative to this split-screen approach is the ﬂicker method, wherein two
different spectra are presented to the viewer alternately and at an intermediate speed,
and ﬂickering can be seen if the colors are different. The intensities are varied
until the viewer does not perceive the ﬂickering anymore. Another question is how
much variation there is in this standard response between individuals. The answer

6.2 Still Image Visual Properties
195
is that there is a modest variation, which has been quantiﬁed in the experimental
data plots from which the standard observer curve v(λ) was created. However, it has
been found that the standard curve can predict reasonably well how the HVS will
respond.
Note that luminance is a quantity averaged over wavelength1 and, as such,
is called a monochrome or gray-level measure of light intensity. Looking at
Figure 6.1–1, normalized to 1, we see that the response of human vision peaks around
555 nanometers (nm), which is green light, and has a range of about 420 to 670 nm,
with 450 nm called blue and 650 nm called red. The low end of visibility is violet,
and the range below the visible is called ultraviolet. At the high end we have red
light, and beyond this end of the visible region is called infrared.
We can also deﬁne similar response functions for various sensors; suppressing the
spatiotemporal dependency on x,y,t, we have
l ≜K
∞
Z
0
pi(λ)sBW(λ)dλ,
where sBW is the black–white (gray-level) sensitivity of the imaging device. Exam-
ples of image sensors are charge-coupled devices (CCD), various types of image
pickup tubes, and coupled metal-oxide semiconductor (CMOS) image sensors. By
inserting optical ﬁlters (wavelength ﬁlters) in front of these devices, we can get differ-
ent responses from various color regions or channels. It is common in today’s cameras
to employ optical ﬁlters to create three color channels—red, green, and blue—to
match what is known about the color perception of the HVS (i.e., the human eye–
brain system). Analogous spectral response functions are used to characterize image
displays. We will return to color later in this chapter.
6.2 STILL IMAGE VISUAL PROPERTIES
Here, we look at several visual phenomena that are important in image and video
perception. First, Weber’s law deﬁnes contrast and introduces the concept of just-
noticeable difference (JND). We then present the contrast sensitivity function, also
called the visual magnitude transfer function (MTF), as effectively the human visual
frequency response. We then introduce the concept of spatial adaptation of contrast
sensitivity.
1Wavelength and spatial frequency: Image and video sensors record energy (power × time) on a scale
of micrometers or more (for typical 1- to 5-cm sensor chip) and so are not sensitive to the oscillations
of the electromagnetic ﬁeld at the nanometer level. Our spatial frequency is a variation of this average
power on the larger scale. So there is no interaction between the wavelength of the light and the spatial
frequency of the image, at least for present-day sensors.

196
CHAPTER 6 Image Perception and Sensing
Weber’s Law
Psychovisual researchers early on found that the eye–brain response to a uniform
step in intensity is not the same over the full range of human perception. In fact, they
found that the just-noticeable percent is nearly constant over a wide range. This is
known now as Weber’s law. Writing I for the incident intensity (or luminance) and
△I for the just-noticeable change, we have
△I
I
≈constant,
with the constant value in the range [.01,.03], and this value holds constant for at
least three decades in logI, as sketched in Figure 6.2–1. We note that Weber’s law
says we are more sensitive to light intensity changes in low light levels than in strong
ones.
A new quantity called contrast was then deﬁned via
△C = △I
I ,
which in the limit of small △becomes the differential equation dC/dI = I−1, which
integrates to
C = ln(I/I0).
(6.2–1)
Sometimes we refer to signals I as being in the intensity domain, and the nonlinearly
transformed signals C as being in the contrast or density domain, the latter name
coming from a connection with photographic ﬁlm, whose optical density possesses a
similar nonlinearity in response to light intensity (see Film in Section 6.6). We note
that the contrast domain is most likely the better choice for quantizing a value with
uniform step size. This view should be tempered with an understanding of how the
psychovisual data were taken to verify Weber’s law.
Δl/l
I
0
FIGURE 6.2–1
Average just-resolvable intensity difference versus background intensity.

6.2 Still Image Visual Properties
197
With reference to Figure 6.2–2, human subjects were presented with a disk of
incremented intensity I + △I in a uniform background of intensity I and were asked
if they could notice the presence of the disk or not. These results were then aver-
aged over many observers, whereby the threshold effect seen in Weber’s law was
found. The actual JND threshold is typically set at the 50% point in these distribu-
tions. Figure 6.2–3 illustrates the concept of such a JND test, showing ﬁve small
disks of increasing contrast (+2,+4,+6,+8,+10) on three different backgrounds,
with contrasts 50, 100, and 200 on the 8-bit range [0,255].
Contrast Sensitivity Function
Another set of psychovisual experiments has determined what has come to be
regarded as the spatial frequency response of the HVS. In these experiments, a
uniform plane wave is presented to viewers at a given distance, and the angular period
FIGURE 6.2–2
A stimulus that can be used to test Weber’s law.
FIGURE 6.2–3
Equal increments at three different brightness values—50, 100, 200—on range [0,255].

198
CHAPTER 6 Image Perception and Sensing
of the image focused on their retina is calculated. The question is at what intensity
does the plane wave (sometimes called an optical grating) ﬁrst become visible. The
researcher then plots this value as a function of angular spatial frequency, expressed
in units of cycles/degree. These values are then averaged over many observers to
come up with a set of threshold values for the so-called standard observer. The recip-
rocal of this function is then taken as the human visual frequency response and called
the contrast sensitivity function (CSF), as seen plotted in Figure 6.2–4, reprinted from
[3, 4]. Of course, there must be a uniform background used that is of a prescribed
value, on account of Weber’s law. Otherwise the threshold would change. Also, for
very low spatial frequencies, the threshold value should be given by Weber’s obser-
vation, since he used a plain or ﬂat background. The CSF is based on the assumption
that the above-threshold sensitivity of the HVS is the same as the threshold sensitiv-
ity. In fact, this may not be the case, but it is the current working assumption. Another
objection could be that the threshold may not be the same with two stimuli present
(i.e., the sum of two different plane waves). Nevertheless, with the so-called linear
hypothesis, we can weight a given disturbance presented to the HVS with a function
that is the reciprocal of these threshold values and effectively normalize them. If the
overall intensity is then gradually reduced, all the gratings will become invisible (not
noticeable to about 50% of human observers with normal acuity) at the same point.
In that sense then, the CSF is the frequency response of the HVS.
An often-referenced formula that well approximates this curve was obtained by
Mannos and Sakrison [5] and is expressed in terms of the frequency fr ≜
q
f 2
1 + f 2
2 in
cycles/degree,
H( fr) = A

α + fr
f0

exp−
 fr
f0
β
,
fr ≥0,
1000
Contrast sensitivity S
Spatial frequency u (cycles/deg)
100
10
0.1
1
10
100
1
Retinal illum.
900 Td
90
9
0.9
0.09
0.009
0.0009
model
FIGURE 6.2–4
Contrast sensitivity measurements of van Ness and Bouman. (JOSA, 1967 [4])

6.2 Still Image Visual Properties
199
with A = 2.6, α = 0.0192, f0 = 8.772, and β = 1.1. (Here, f = ω/2π.) In this for-
mula, the peak occurs at fr = 8 (cycles/degree) and the function is normalized to
maximum value 1. A linear amplitude plot of this function is given in Figure 6.2–5.
Note that sensitivity is quite low at zero spatial frequency, so some kind of spatial
structure or texture is almost essential if a feature is to be noticed by our HVS.
Local Contrast Adaptation
In Figure 6.2–6 we see a pair of small squares, one on a light background and the
other on a dark background. While the small squares appear to have different gray
levels, in fact the gray levels are the same. This effect occurs because the HVS adapts
to surrounding brightness levels when it interprets the brightness of an object.
There is also a local contrast adaptation wherein the JND moves upward as the
background brightness moves away from the average contrast of the object. Such a
test can be performed via the approach sketched in Figure 6.2–7, where we note that
the small central square is split and slightly darker on the left. For most people, this
effect is more evident from the display on the left, where the contrast with the local
background is only slight. However, on the right the large local contrast with the local
0
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
Spatial frequency (cycles/degree)
Contrast sensitivity
FIGURE 6.2–5
Plot of the Mannos and Sakrison CSF [5].

200
CHAPTER 6 Image Perception and Sensing
FIGURE 6.2–6
An illustration of the local adaptation property of the HVS.
FIGURE 6.2–7
Illustration of dependence of JND on local background brightness.
background makes this effect harder to perceive. Effectively, this means that the JND
varies somewhat with local contrast, or said another way, there is some kind of local
masking effect.
6.3 TIME-VARIANT HUMAN VISUAL SYSTEM PROPERTIES
The same kind of psychovisual experiments used in the preceding discussion can
be employed to determine the visibility of time-variant features. By employing a
spatial plane wave grating and then modulating it sinusoidally over time, the con-
trast sensitivity of the HVS can be measured in a spatiotemporal sense. Plots of
experiments by Kelly (1979) [6] and (1971) [7], as reprinted by Barten [3], are
shown here. The ﬁrst, Figure 6.3–1, shows slices of the spatiotemporal CSF plotted

6.3 Time-Variant Human Visual System Properties
201
1000
Contrast sensitivity S
Spatial frequency u (cycles/deg)
100
10
1
0.1
1
10
100
Temp. frequ.
2 Hz
13.5
17
23
model
FIGURE 6.3–1
Spatiotemporal CSF from Kelly. (JOSA, 1979 [6])
1000
Contrast sensitivity S
Temporal frequency w (Hz)
100
10
1
1
10
100
3 cycles/deg
uniform field
model
FIGURE 6.3–2
Temporal CSF with spatial parameter from Kelly. (JOSA, 1971 [7])
versus spatial frequency, with the slices at various temporal frequencies, over the
range 2 to 23 Hz. We see that maximum response occurs around 8 cycles/degree
(cpd) at 2 Hz. Also, we see no bandpass characteristic at the higher temporal fre-
quencies. Figure 6.3–2 shows slices of the CSF as measured by Kelly (1971) where

202
CHAPTER 6 Image Perception and Sensing
there are two slices at spatial frequencies of 0 and 3 cpd, plotted versus temporal
frequency. Again, we see the bandpass characteristic at the very low spatial fre-
quency, while there is a lowpass characteristic at the higher spatial frequencies. Note
that the highest contrast sensitivity is at temporal frequency 0 and spatial frequency
3 cpd.
A 3-D perspective log plot of the spatiotemporal CSF was shown in Lam-
brecht and Kunt [8] and is reprinted in Figure 6.3–3. Note that these 3-D CSFs
are not separable functions. Note also that there is an order of magnitude difference
between the sensitivity at DC or origin and that at the peak of maybe about 10 Hz
and 6 cpd.
A summary comment is that the HVS seems to be sensitive to change, either
spatially or temporally, and that is what tends to get noticed. Lesser variations
tend to be ignored. These results can be used to perceptually weight various error
criteria for image and video signal processing, to produce a result more visually
pleasing. An excellent source for further study is the thesis monograph of Barten
[3]. Human visual perception is also covered in the review Chapter 8.2 by Pap-
pas et al. in Handbook of Image and Video Processing, 2nd Edition [9]. Basic
image processing techniques are also covered by Bovik in Chapter 2.1 of the same
handbook.
101
100
10−1
10−2
10−1
10−3
100
101
10−1
10−2
10−2
100
101
102
Spatial frequency (cpd)
Temporal frequency (Hz)
Sensitivity
FIGURE 6.3–3
A perspective plot of a spatiotemporal CSF from Lambrecht and Kunt (Image
Communication 1998 [8]) (normalized to 10).

6.4 Color
203
6.4 COLOR
The human eye itself is composed of an iris, a lens, and an imaging surface or retina.
The purpose of the iris is to let in the right amount of light to avoid either saturation
or under-illumination. The lens focuses this light on the retina, which is composed of
individual cones and rods. The rods are spread over a very large area of the retina but
at a considerably lower spatial density than the cones. These rods are of one type and
therefore can only provide a monochrome response. They are very sensitive to light,
though, and provide our main perception at dim light levels, such as at night or in a
very darkened room. This is why we do not perceive color outdoors at night.
The cones are deployed at higher density in a small central part of the retina
called the fovea. These cones are of three color efﬁciencies, responding roughly to
red, green, and blue. Labeling these luminous efﬁciencies as sR(λ),sG(λ), and sB(λ),
we see the result of psychovisual experiments, sketched in Figure 6.4–1, with the
green and blue response being fairly distinct, and the red response perhaps being
more appropriately called yellow-green, overlapping considerably with the green.
We can then characterize the average human cone response to an incident ﬂux
distribution pi(λ) as
R = K
∞
Z
0
pi(λ)sR(λ)dλ,
(6.4–1)
G = K
∞
Z
0
pi(λ)sG(λ)dλ, and
(6.4–2)
B = K
∞
Z
0
pi(λ)sB(λ)dλ.
(6.4–3)
λ
100%
50%
460
540
620
700
(nm)
Blue
Green
Red
FIGURE 6.4–1
Sketch of average sensitivity functions of three types of color receptors (cones) in the fovea
of the human eye.

204
CHAPTER 6 Image Perception and Sensing
These three equations are a linear model for color that is in accord with experimen-
tally observed Grassmann “laws” [10, 11] of color perception. As such, we have a
3-D subset of a function space with three basis functions, sR(λ),sG(λ), and sB(λ). All
incident power densities pi(λ) that have the same R,G, and B values as calculated in
(6.4–1 and 6.4–3) will have the same “color.” This set of power densities is then said
to be metameric, or of the same color [12].
Each such color [R,G,B] forms an equivalence class [11] in the linear space of
possible incident ﬂux distributions pi(λ) (i.e., a class of functions pr that all give the
same color perception). The three cone response functions are the basis for this space,
and [R,G,B] is the expression of the color in this basis. Note that this equivalence
class is not a linear subspace because of necessary positivity constraints. Indeed,
all the quantities in (6.4–1 through 6.4–3) must be nonnegative, and this leads to
difﬁculties in color matching between and among image sensors and displays.
In 1931 the CIE performed colorimetric experiments that deﬁned three color-
matching functions for three narrowband primaries centered at R = 435.8 nm,
G = 546.1 nm, and B = 700 nm [10]. These measured quantities are plotted in
Figure 6.4–2, where negative values indicate that color could not be matched with
the three chosen primaries. In these cases, the negative of the indicated value (now
positive) was added to the test color in order to make the match. For example, we
see in the ﬁgure that negative sR is indicated between about 440 and 550 nm. So if
we are trying to match a monochromatic color in this range, we cannot do it with
additional R. Instead, we add some R into that monochromatic test color, and in the
positive amount −R, to make a match. Mathematically, this is equivalent to using a
negative amount of primary R [10]. Subsequently, to avoid the negative values in
350
400
450
500
550
600
650
700
750
800
−0.1
−0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Wavelength in nm
CIE 1931 color-matching functions
sB
sG
sR
FIGURE 6.4–2
CIE RGB color-matching (color sensitivity) functions (1931).

6.4 Color
205
350
400
450
500
550
600
650
700
750
800
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Wavelength in nm
CIE xyz color-matching functions
FIGURE 6.4–3
CIE 1931 X,Y,Z color-matching functions (tristimulus values), also called ¯x, ¯y,¯z functions.
these basis (sensitivity) function curves, the CIE deﬁned a linear transformation
on the color-matching functions that avoids the negative values, albeit now with
unrealizable primaries, yielding the so-called CIE XYZ color-matching functions
[10] that are in wide use to this day. These color-matching functions are plotted in
Figure 6.4–3.
The success of this color matching depends on keeping the background illu-
mination constant. This is because of local background adaptation, illustrated in
Figure 6.4–4, which shows two equal-colored blocks against light- and dark-colored
backgrounds. We can see the block on the left does not seem to exactly match
the color of the one on the right. The color background maintained by the CIE in
their experiments was an approximation of daylight, which is the 6500◦blackbody
radiation D65 [10].
Chromaticity Values
Focusing just on chrominance, and neglecting luminance, we can form a so-called
chromaticity diagram made from the various tristimulus values by sweeping through
wavelength λ and the plotting chromaticity (x,y):
x ≜X/(X + Y + Z),
y ≜Y/(X + Y + Z).

206
CHAPTER 6 Image Perception and Sensing
FIGURE 6.4–4
The small block has the same color in both sides of the ﬁgure.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
x
y
CIE x-y chromaticity diagram
D65white
FIGURE 6.4–5
CIE 1931 chromaticity diagram of the standard observer.
The chromaticity diagram is thus a normalized plot of the color response of the CIE
1931 standard observer. Each point on the curved boundary line in Figure 6.4–5 is
plotted from the tristimulus values that result from a single-wavelength stimulus.
Taking the individual CIE 1931 XYZ color-matching values for each wavelength,

6.4 Color
207
we can make a plot as a hypothesized narrowband source sweeps across the visi-
ble wavelength 380 ≤λ ≤780 nm. The whitepoint is indicated for the D65 standard
illuminant available at the CIE Web site [13].
Color Processing
When the human eye is directly exposed to the light intensity, the CIE standard
observer provides a good indication of the visual system response, which we quan-
tify as XYZ or CIE RGB tristimulus values, the “color.” But when we place an image
sensor and image display device between the object and the observer, things change.
First, the typical image sensor has three color channels with their own sensitivity
curves, producing sensor R,G,B values Rs,Bs,Gs. These values in turn are input to
an image display with typically three output colors with their own intensity curves.
The following example illustrates this situation.
Example 6.4–1: Color Image Sensed and Displayed
Consider a single color object that is photographed (sensed) and then displayed with the
color spectrum of Figure 6.4–6.
λ
460
540
700
620
100%
50%
(nm)
Color object
FIGURE 6.4–6
Wavelength spectrum of a color object.
While it is sensed, it is illuminated by a white light (i.e., a ﬂat color spectrum) and
recorded with the image sensor’s sensitivity functions shown in Figure 6.4–7.
The resulting sensed Rs Bs Gs values are the integrals of the point-by-point product of
these two curves [12]. Assuming no signal processing of these values, the display device
adds in its sensitivity curves based on its red, green, and blue light sources, with different
wavelength curves shown in Figure 6.4–8, and creates the displayed image.
Finally, the output light of the display device excites the HVS sensitivity curves that
we model as XYZ. After these various transformations, the human perceived color may be
much different than that which would have been perceived on direct viewing of the object.

208
CHAPTER 6 Image Perception and Sensing
λ
460
540
700
620
100%
50%
Image sensor responses
(nm)
Blue
Green
Red
FIGURE 6.4–7
Possible color sensor response functions.
λ
460
540
700
620
100%
50%
Display color content
(nm)
Blue
Green
Red
FIGURE 6.4–8
Sketch of possible display color primaries.
Since we just have a solid color here, the main question is whether the single displayed
color is metameric to the original one. The displayed output will be a weighted combination
of the three display curves in Figure 6.4–8, which clearly cannot equal the object spectral
response shown in Figure 6.4–6. However, from the viewpoint of our linear XYZ model of
the HVS response, we just want the displayed color to be perceived as the same. For this
we need only for the integrals in (6.4–1), (6.4–2), and (6.4–3) to be the same, because
then, up to an approximation due to this linear model, the cones will output the same signal
as when the color object was directly perceived. Clearly, without careful choice of these
curves and scaling of their respective strengths, the perceived color will not be the same
more often than not. Careful calibration [10, 11] of the color display monitor is necessary.
An approximate solution to this problem is to white balance the system. In this
process, a pure white object is imaged ﬁrst and then the system output R, G, and B values
are scaled so as to produce a white perception in the HVS of the (standard) viewer. The
actual signal captured by the camera may not be white because of possible (probable)
nonwhite illumination, but the scaling is to make the output image be perceived as white
when viewed in its display environment.

6.5 Color Spaces
209
12
Red
Green
Blue
10
8
6
Spectral radiance (W/sr/m2)
4
2
0
−2
×10−3
400
450
500
550
Wavelength (nm)
600
650
700
750
FIGURE 6.4–9
Spectral radiance functions from the R, G, and B channels of an LCD panel. (from [20]
c⃝2002 IEEE)
You might guess that the color display problem would be simpler if the display
primaries were monochromatic, but this is usually not possible. Figure 6.4–9 shows
an example of the RGB display response functions of an LCD panel [14] circa 2002.
We see that red and green are rather impulsive, with blue being decidedly less so.
We can characterize the color spaces of such image input/output devices and relate
them to CIE XYZ space. These would be device-dependent color spaces and are often
proprietary.
6.5 COLOR SPACES
Some device-independent color spaces have become useful both for standardiza-
tion purposes and for evaluation of perceived color uniformity. A few of them are
presented here. First, we look at the CIELAB space that targets perceptual uni-
formity. Then we look at the so-called Rec. 709 color space of high-deﬁnition
television (HDTV) and the sRGB color space of digital photography and printing.
These device-independent color spaces can serve as common reference points when

210
CHAPTER 6 Image Perception and Sensing
converting from the device-dependent color space of a scanner or camera to the
device-dependent color space of a display or printer. The industry-supported Interna-
tional Color Consortium (ICC) [15] speciﬁcation permits device color management
systems to easily perform such conversions through a well-deﬁned proﬁle connection
space.
CIELAB
This color space is a nonlinear transformation on the CIE XYZ space with the purpose
of making the space more uniform with respect to human perceived color differences.
The CIELAB color coordinates are given as
L∗≜116f(Y/Yn) −16,
a∗≜500[ f(X/Xn) −f(Y/Yn)],
b∗≜200[ f(Y/Yn) −f(Z/Zn)].
Here, the Xn,Yn,Zn are the tristimulus values of the reference white under a reference
illumination, and f is the pointwise nonlinearity
f(x) ≜

x1/3,
x > 0.008856,
7.787x + 16/116,
x < 0.008856.
The CIELAB space is often used for perceptual comparisons in image sci-
ence because a small change (|1L∗|,|1a∗|,|1b∗|) has been found to have a much
more uniform perceptual effect [10] than does the corresponding small change
(|1X|,|1Y|,|1Z|). This color space can be used to evaluate the color uniformity
of a color image display. It is an example of a nonlinear color space.
International Telecommunication Union (ITU) Recomendation 709
Another transformation from XYZ is speciﬁed in ITU Recommendation ITU-
R BT.709 [16] and is given by Poynton [17]. First, a linear transformation is
speciﬁed as


R709
G709
B709

=


3.240479
−1.537150
−0.498535
−0.969256
1.875992
0.041556
0.055648
−0.204043
1.057311




X
Y
Z

.
The inverse transformation is given as


X
Y
Z

=


0.412453
0.357580
0.180423
0.212671
0.715160
0.072169
0.019334
0.119193
0.950277




R709
G709
B709

.
(6.5–1)

6.5 Color Spaces
211
These are linear RGB values—i.e., linear in terms of light intensity. Second, the ITU
Rec. 709 speciﬁcation for HDTV puts forth the precise nonlinear function
g(x) =
4.5x,
x ≤0.018,
1.099x0.45 −0.099,
x > 0.018
to be applied to each linear R709, G709, and B709 intensity domain value to produce
the more perceptually uniform components R′
709, G′
709, and B′
709, respectively:
R′
709 ≜g(R709), G′
709 ≜g(G709), and B′
709 ≜g(B709).
The inverse of this transform, performed (approximately) at the image display (video
monitor), is given as
g−1(x′) =
(x′/4.5,
x′ ≤0.081,

x′+0.099
1.099
1/0.45
,
x′ > 0.081.
Usually Rec. 709 color space means the nonlinear R′
709,G′
709,B′
709 color space, rather
than the linear one, R709,G709,B709.
A related nonlinear color space is Rec. 709 Y′,CB,CR, deﬁned via the transforma-
tion [18]
Y′ ≜0.2126R′
709 + 0.7152G′
709 + 0.0722B′
709,
CB ≜0.5389(B′
709 −Y′
709),
CR ≜0.6350(R′
709 −Y′
709).
The advantage of this space is efﬁciency, since the three R′
709,G′
709,B′
709 channels are
highly correlated, while the Y′,CB,CR channels are approximately decorrelated. It is
used mainly in image data compression for storage and transmission purposes.
sRGB
In the sRGB color space, ﬁrst, a matrix transformation [10, 11] is performed on CIE
XYZ tristimulus values:


R
G
B

≜


3.2406
−1.5372
−0.4986
−0.9689
1.8758
0.0415
0.0557
−0.2040
1.0570




X
Y
Z

.
Then the following nonlinear transformation [10, 11] is applied:
g(x) ≜
12.92x,
x ≤0.0031308,
1.055x0.41666 −0.055,
0.0031308 < x ≤1.
The sRGB color space values then are R′ ≜g(R),G′ ≜g(G), and B′ ≜g(B). This
space is often referenced in color-still cameras and printing devices.
Both Rec. 709 and sRGB nonlinear color spaces are much more perceptually uni-
form than is CIE XYZ but not nearly as uniform as CIELAB. Their main use is as

212
CHAPTER 6 Image Perception and Sensing
standard intermediate color spaces for sharing images and video. In this way the cam-
era designer tries to optimize the camera for display in, say, the sRGB space. Then
the color printer designer assumes that the input image is expressed in this space
and goes on to design the display system to do a good job of conversion between
the standard sRGB space and the proprietary color space of that output device. These
standard color spaces then serve as an intermediate target for the manufacturers of the
various components in the image chain (i.e., camera, processor, coder, decoder, dis-
play, and printer). More on color representation and display is contained in Trussell’s
Chapter 8 of The Essential Guide to Image Processing [19].
6.6 IMAGE SENSORS AND DISPLAYS
In this section we will overview both electronic and ﬁlm sensors, followed by a brief
introduction to image and video display devices.
Electronic Sensors
Electronic image sensors today are mainly solid state and of two types: charge-
coupled device (CCD) and coupled metal-oxide semiconductor (CMOS). At present,
CCD sensors tend to occupy the nitch of higher quality and cost, while CMOS sensors
generally have somewhat lower quality but at a much lower cost, having beneﬁted
from extensive silicon memory technology. These sensors are present in virtually all
current video cameras as well as digital image cameras. They are generally charac-
terized by the number of pixels, today running up to a total of 16M pixels for widely
available still-image cameras and 36M for high-end professional still cameras. While
still image sensors read out their frames rather slowly (e.g., 1–10 frames per sec-
ond fps), video sensors must output their image frames at a faster rate (i.e., 24–100
fps), and so their pixel counts tend to be lower. Also, still-image sensors usually have
deeper charge wells and so accommodate a wider dynamic range, as measured in
f-stops2 or bit depth, currently at 10 bits or more. For a digital still camera, access
to its 12- to 14-bit image sensor data is usually available only in the camera’s .raw
format that is typically prior to gamma compensation3 and demosaicing. While the
format is company proprietary, there are intermediate standards such as .dng and
.tiff/ep with converters widely available on the Internet. When .raw data is loss-
lessly compressed, the ﬁle size is much larger than typical JPEG or .jpg ﬁle outputs
that are normally gamma compensated, demosaiced, and of 8-bit depth on each color.
2The f-stops on a camera are measures of the aperature, or opening in front of the camera lens. Each
increment in f-stop results in a halving of the light intensity on the lens.
3Gamma compensation is a nonlinearity inserted by camera makers to compensate for an assumed
nonlinearity (x2.2) of display devices. See Gamma later in this section.

6.6 Image Sensors and Displays
213
In common digital still cameras today, there is only one sensor for all three
primary colors. A mosaic color ﬁlter called color ﬁlter array (CFA) is applied
to the front end of the imaging chip to effectively subsample the three colors.
An alternative, often used in video cameras, is a color prism and three imag-
ing chips, one for each of R, G, and B. A novel multilayer CMOS sensor [20]
has been developed that stacks R, G, and B pixels vertically and uses the wave-
length ﬁltering properties of silicon to avoid the use of the relatively heavy and
cumbersome prism. However, it has not been widely accepted. Both of these solu-
tions have an advantage over the CFA sensor in that three-color information is
available at every pixel location, rather than only on a checkerboard pattern. Of
course, one can just put in more pixels to partially make up for the lost reso-
lution, but that usually means smaller pixels with lessened lowlight capability, if
the sensor size is kept constant. Nevertheless, it is the solution of choice for digi-
tal still cameras, and the CFA solution has recently moved into the realm of high-end
professional video cameras that use single large 35-mm size CMOS sensors.
The most widely used CFA is the Bayer array4 [21] shown in Figure 6.6–1. We see
that a basic 2 × 2 cell is repeated to tile the sensor surface, effectively subsampling
each of the three color primaries R, G, and B. The Bayer CFA favors green, as does
the HVS, and it subsamples red and blue by a larger factor. The actual subsampling
of G is 2:1 on a diamond lattice, with R and B subsampled on offset 2 × 2 Cartesian
lattices. Some internal processing in the digital camera, called demosaicing, tries to
make up for the resulting aliasing, which can be controlled by a slightly defocused
FIGURE 6.6–1
2×2-pixel cell in Bayer CFA.
4Named for Bryce Bayer of Eastman Kodak Company, U.S. Patent No. 3,971,065, Color Imaging
Array, 1976.

214
CHAPTER 6 Image Perception and Sensing
lens design. The in-camera postprocessing generally does a good job, but still, for
certain images with much high-frequency color information that is near periodic,
aliasing errors are visible [21]. Strictly speaking, the sensors do not really sample,
rather they count up photons inside a square or rectangular pixel, called the aperture
effect. As such, it can better be modeled as a box ﬁlter followed by an ideal sampler,
thus giving some small degree of lowpass ﬁltering, which tends to suppress spatial
frequency alias energy.
The quality of an image sensor is also deﬁned by what its light sensitivity is and
how much light it takes to saturate it. A large dynamic range for electronic image
sensors is difﬁcult to achieve, without a large cell area to collect the light, hence
forcing a trade-off with pixel count for a given silicon area. The other variable is
exposure time. Generally, higher dynamic range can be achieved by lengthening the
exposure time, but that can lead to either motion blurring or too low a frame rate for
use in a video sensor. Large-format CMOS sensors use a Bayer ﬁlter array and have
no color prism. Because of their large size, they have deeper electron wells for a given
resolution, and hence larger dynamic range. Scientiﬁc or sCMOS sensors [22] have
been announced with dynamic ranges of 16 bits at 30 fps. These electronic image
sensors tend to provide an output that is linear in intensity, having simply counted
the photons coming in. Nevertheless, it is common for camera makers to put in a
nonlinearity to compensate for the nonlinearity of the previously common cathode
ray tube (CRT). This is the so-called gamma compensation function. We talk more
about gamma in a following section on image displays.
The sensor noise of CCD and CMOS tends to be photon counting related. First,
there is a background radiation giving a noise ﬂoor, typically assumed to be Poisson
distributed. Then the signal itself is simply modeled as a Poisson random variable,
with mean equal to the average input intensity, say λ, at that pixel cell. A key charac-
teristic of the Poisson distribution is that the variance is equal to the mean, so that the
rms signal-to-noise ratio becomes λ/
√
λ =
√
λ. This means the SNR is better at high
levels than at low levels, and the noise level itself is intensity dependent. For even
modest counts, the Poisson distribution can be approximated as Gaussian [23], with
mean λ and standard deviation
√
λ, or what is the same, variance λ. This means that
there is an additive noise in the intensity signal, but rather than being constant vari-
ance, it has a variance that depends on the intensity, a so-called intensity-modulated
white Gaussian noise.
Sensor Fill Factor
The image sensor usually does not have an active photon collecting area equal to
its footprint, giving rise to a ratio called sensor ﬁll factor. If the ﬁll factor is large,
this helps to reduce the aliasing due to spatially sampling the continuous image ﬁeld.
However, if the ﬁll factor is small, then some kind of anti-alias ﬁltering should be
used. It can take the form of the camera lens, maybe slightly defocused, or there
can be some spatial lowpass ﬁltering of the sensor output image to reduce the high
frequencies where aliasing is most likely to occur.

6.6 Image Sensors and Displays
215
Some high-resolution image sensors on digital still cameras also function as video
sensors with a reduced resolution. The reduced resolution can be obtained in two
ways. The preferable way would be to process the full-resolution image with a spa-
tial lowpass ﬁlter and then digitally subsample the result down to the lower desired
video resolution. An alternative, cheaper, and faster way is just to output the sub-
sampled sensor cells directly, however this can sometimes give rise to serious spatial
frequency aliasing error in highly detailed images. To provide some speciﬁc numbers
for this problem, as of this writing, an image sensor on a widely available digital still
camera may have about 4000–5000 pixels horizontally and 3000–4000 pixels verti-
cally. Typical high-deﬁnition (HD) video resolutions are around 2000 by 1000 pixels,
and standard deﬁnition (SD) resolutions are around 700 by 500 pixels. A 3:1 vertical
subsampling is commonly employed to achieve HD video frame rates (24–30 fps).
Serious spatial frequency aliasing errors have been reported on the video output from
such cameras. Camera operators have developed various “workarounds,” all related
to defocusing such aliased areas.
Film
The highest resolution and dynamic range image capture has historically been ﬁlm
based. Images are captured on 70-mm ﬁlm in IMAX cameras at resolutions in excess
of 8M × 6M pixels. The spatial resolution of conventional 35-mm movie ﬁlm is gen-
erally thought to be in a range from 3M to 4M pixels across, depending on whether
we measure the camera negative or a so-called distribution print, which may be sev-
eral generations removed from the in-camera negative. Further, ﬁlm has traditionally
excelled in dynamic range—that is, the ability to simultaneously resolve small dif-
ferences in gray level in both very bright and very dark regions. Because of Weber’s
law, the human eye is more sensitive to changes in the dark regions, where ﬁlm gen-
erally has had an advantage over electronic sensors, although this is currently being
challenged by the latest solid-state sensors.
Digital images often were initially captured on ﬁlm in an effort to achieve the
highest quality. The ﬁlm is then scanned by a device that shines white light through
it and records the optical density of the ﬁlm. There are various names for such a
device, including ﬁlm scanner, microdensitometer, and telecine. Film is generally
characterized by a D −logE curve, where D is the ﬁlm’s optical density and E is
exposure, deﬁned as the time integral of intensity over the exposure time interval.
A typical D −logE curve is sketched in Figure 6.6–2. We see that there is a broad
linear range bordered by “saturation” in the bright region and “fog” in the dark region.
The linear part of the D −logE curve gives the useful dynamic range of the ﬁlm, often
expressed in terms of f-stops. Note that density in the linear region of this curve is
quite similar to contrast as we have deﬁned it in (6.2–1) and is therefore much more
nearly perceptually uniform than is intensity. Digitization of ﬁlm is thus most often
done in the “density domain,” giving us effectively a record of contrast. Since ﬁlm is
completely continuous, there is no spatial frequency aliasing to worry about, at least
neglecting its random grain structure.

216
CHAPTER 6 Image Perception and Sensing
log E
D
Saturation
Fog
FIGURE 6.6–2
Sketch of the D −logE curve of ﬁlm.
Film itself is composed of “grains” that have a probability of being “exposed”
when struck by an incoming photon. As such, ﬁlm-like CCD and CMOS chips also
counts photons, both incoming from the imaged object and background radiation. The
fog area in Figure 6.6–2 corresponds to ﬁlm grains exposed by this background radi-
ation. The saturation area in the D −logE curve corresponds to the situation where
most all the grains are already exposed, so a longer exposure time or greater average
incoming light intensity will not expose many more grains, and ﬁnally the density of
the ﬁlm will not increase any further.
Image and Video Display
Images, once acquired and processed, can be displayed on many devices, both analog
and digital. In the “old” technology area, we have photographic prints and slides,
both positive and negative. In the classic video area, we have ﬁlm projectors and
CRTs for electronic video, such as television. Modern technology involves electronic
projectors such as liquid crystal digital (LCD), liquid crystal on silicon (LCOS), and
digital light processing (DLP) and plasma. There are also ﬂat-panel plasma displays
made using miniature tubes, as well as the ubiquitous smaller LCD panels used in
both stationary and portable computers. Each of these display categories has issues
that affect its use for a particular purpose (e.g., display resolution, frame rate, gamma,
dynamic range, noise level, color temperature). It can be said that no one display
technology is the best choice for all applications, so that careful choice of alternatives
is needed.
Common LCD displays are a transmissive technology and use a single cold cath-
ode ﬂuorescent lamp (CCFL) light source behind the screen constituting a so-called
backlight. It is hard to obtain true black in such a situation because, even in the pixel
off-state, some light shines through the LCD panel. Recently, LED light sources have

6.6 Image Sensors and Displays
217
begun to replace the CCFL backlight due to reduced energy requirements and longer
lifetimes. A low-resolution array of LED pixels can be used to provide local dim-
ming behind a high-resolution LCD array for the purpose of increased perceived
dynamic range. Commonly, though, the LEDs are only arranged on the perimeter of
the LCD display, and this does not give any increase in perceived dynamic range; it
can, however, make the display thinner and more energy efﬁcient.
Flicker was a problem with the CRT because the pixel was only illuminated
instantaneously followed by a rapid phosphor decay for the rest of the display cycle.
For solid-state displays, though, due to their constant pixel illumination, ﬂicker is
not a problem. However, there is a new problem with judder, the discontinuous or
stepwise motion that can be perceived from these displays. While judder had been
seen in movie projectors, it is relatively unknown in CRT displays.
LCD displays are ubiquitous as computer monitors and image projectors offer
good spatial resolution and color rendition. They have had a problem with transient
response, which tends to leave a trail on the screen from moving objects, but this has
been largely addressed by 120 fps models (also called 120 Hz).
Plasma panels are very bright, and the technology can scale to large sizes, but
there can be visible noise in the dark regions due to the pulse-width modulation sys-
tem used in their gray-level display method. In a bright room, this is not usually a
problem. They tend to have the best dark levels, though, better than LCDs, which use
a transmissive technology.
In the projector area, very high spatial resolutions have been achieved by a variant
of LCOS in a 4K digital cinema projector. Another projection technology is DLP
chips that contain micromirrors at each pixel location. Each micromirror is tilted
under control of each digital pixel value to vary the amount of light thrown on the
screen for that pixel. It can offer both excellent black levels and transient response
in a properly designed projector. DLPs have also achieved 4K resolution, meaning a
resolution of 4K pixels horizontally and 2K or more vertically.
The CRT is still the only common device that directly displays interlaced video—
i.e., without deinterlacing it ﬁrst.5 Since deinterlacing is not perfect, this is an
advantage for an interlaced source such as common SD video and HD 1080i. Today,
however, much program origination is done by progresive ﬁlm and video cameras
(720p and 1080p) that well suit the now-common LCD and plasma displays.
Gamma
Until recently, the standard of digital image display was the CRT. These devices have
a nonlinear display characteristic, usually parameterized by γ as follows:
I = vγ
in,
where vin is the CRT input voltage, I is the light intensity output, and the gamma
value γ usually ranges from 1.8 to 2.5. Because of this nonlinearity, and because of
5Deinterlacing is covered in Chapter 10.

218
CHAPTER 6 Image Perception and Sensing
the widespread use of CRTs in the past, video camera manufacturers have routinely
implemented a gamma correction into their cameras for nominal value γ = 2.2, thus
precompensating the camera output voltage as
vout = v1/γ
in .
Since vin is proportional to I for a CCD or CMOS image sensor, this near square-root
function behaves somewhat similar to the log used in deﬁning contrast, and so the
usual camera output is more nearly contrast than intensity. Modern ﬂat panel display
devices are near linear and don’t need gamma compensation, however current models
implement the gamma nonlinearity in their processing chain.
Notice that ﬁlm density has a useful response region, shown in Figure 6.6–2 that
is linear in exposure E. For constant intensity I, over a time interval T, we then have
approximately, and in the linear region,
d = γ logI + d0,
so that ﬁlm density is contrast-like also and thereby, via Weber’s law, obtains the
advantage of a more uniform perceptual space.
High Dynamic Range and Tone Mapping
The dynamic range of the human eye is quite large. Also the range of light intensities
seen in nature is much larger than that. Quantizing in density, we typically use 8 bits
for digital images and achieve only a window on what is possible. The term high
dynamic range (HDR) means imaging with more than 8 bits in contrast (density).
When images are viewed in a bright room, usually an HDR display is not required;
however, common viewing conditions for movie theaters involve darkened rooms
where the HDR range of human vision can be more fully addressed. For a full HDR
system, we need HDR cameras (sensors) as well as HDR displays/projectors. When
the full captured dynamic range cannot be displayed, a pointwise nonlinear function,
or tone mapping, is used to reduce the dyanmic range to ﬁt the display and viewing
environment.
One way to acquire HDR images with a conventional camera is to take short and
long exposures in rapid succession. Some commonly available digital still and smart
phone cameras take this approach, tone mapping the result before storing in JPEG
format. Access to camera .raw ﬁles can permit higher dynamic range output from a
single exposure.
In the output area, and as mentioned earlier, HDR display is possible with LED
low-resolution backlight sandwidched with an LCD transmissive display [24]. This
way, the local brightness can be modulated to achieve higher dynamic range. In one
implementation, the LED array would be given the upper 8 bits of a local image
value, while the LCD array would show the detail in the lower 8 bits of the pixel
value. An HDR display based on this approach was introduced in 2005 [25], but has
not become widely available.

Problems
219
CONCLUSIONS
The basic properties of images, sensors, displays, and the human visual system
(HVS) studied in this chapter are essential topics, providing a link between the multi-
dimensional signal processing of Chapters 1–5 and the image and video processing
of the following chapters. In the coming chapter, we will make use of several of these
properties for both image/video processing and compression.
PROBLEMS
1. One issue with the YCRCB color space comes from the constraint of positivity for
the corresponding RGB values.
(a) Using 10-bit values, the range of RGB data is [0,1023]. What is the resulting
range for each of Y, CR, and CB?
(b) Does every point in the range you found in part (a) correspond to a valid RGB
value—i.e., a value in the range [0,1023]?
2. From the Mannos and Sakrison human visual system (HVS) response function
shown in Figure 6.2–5 that is normalized to 1, how many pixels should there be
vertically on a 100-inch vertical screen when viewed at a distance of 3H (i.e., 300
inches)? At 6H? Assume we want the HVS response to be down to 0.01 at the
Nyquist frequency.
3. Assume the measured JND is 1I = 1 at I = 50. Hence,
1I
I = 0.02.
Consider a range of intensities I = I0 = 1 to I = 100.
(a) If we quantize I with a ﬁxed step size, what should this step size be so that the
quantization will not be noticeable? How many steps will there be to cover
the full range of intensities with this ﬁxed step size? How many bits will this
require in a ﬁxed length indexing of the quantizer outputs?
(b) If we instead quantize contrast
C = ln(I/I0),
with a ﬁxed step-size quantizer, what should 1C be? How many steps are
needed? How many bits for ﬁxed-length indexing of these values?
4. Referring to the three-channel theory of color vision (6.4–1)–(6.4–3), assume
three partially overlapping human visual response curves (luminous efﬁciencies)
sR(λ), sG(λ), and sB(λ) are given. Let there be a camera with red, green, and blue
sensors with response functions scR(λ), scG(λ), and scB(λ), where the subscript c
indicates “camera.” Let the results captured by the camera be displayed with three
very narrow-wavelength beams, here modeled as monochromatic s(λ) = δ(λ),

220
CHAPTER 6 Image Perception and Sensing
additively superimposed, and centered at the wavelength peak of each of the
camera luminous efﬁciencies. Assuming white incident light (i.e., uniform inten-
sity at all wavelengths), what conditions are necessary so that we perceive the
same R, G, and B sensation as if we viewed the scene directly? Neglect any
nonlinear effects.
5. In “white balancing” a camera, we capture the image of a white card in the ambi-
ent light, yielding three R, G, and B values. We might like these three values to be
equal; however, because of the ambient light not being known, they may not be.
How should we modify these R, G, and B values to “balance” the camera in this
light? If the ambient light source changes, should we white balance the camera
again? Why?
6. Consider the “sampling” that a CCD or CMOS image sensor does, as reﬂected in
the input Fourier transform X(1,2). Assume the sensor is of inﬁnite size and
that its pixel size is uniform and square with size T × T. Assume that, for each
pixel, the incoming light intensity (monochromatic) is integrated over the square
cell and that the sample value becomes this integral for each pixel (cell).
(a) Express the Fourier transform of the resulting sample values; call it
Xsensor(ω1,ω2) in terms of the continuous Fourier transform X(1,2).
(b) Assuming spatial frequency aliasing is not a problem, ﬁnd the resulting
discrete-space transform Xsensor(ω1,ω2) by specializing your result from
part (a).
7. Use MATLAB to compute the whitepoint of the CIE chromaticity diagram of
Figure 6.4–5 using the standard D65 illuminant available for download at the CIE
Web site [13]. Find and plot the R, G, and B points on the chromaticity diagram
corresponding to R = 435.8 nm, G = 546.1 nm, and B = 700 nm.
8. Use the column vectors in the linear color space transformation in (6.5–1) to
compute X,Y,Z values of the Rec. 709 spectral primaries. Then calculate the cor-
responding x,y values and plot them on the chromaticity diagram of Figure 6.4–5.
Finally, argue that the color gamut of Rec. 709 color space is a triangle with these
three points at vertices.
9. Redo problem 8 for the sRGB color space.
REFERENCES
[1] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
[2] J. E. Hardis, “100 Years of Photometry and Radiometry,” Proc. SPIE, vol. 4450, 2001.
[3] P. G. J. Barten, Contrast Sensitivity of the Human Eye and Its Effects on Image Quality,
PhD thesis, Technical Univ. of Eindhoven, 1999.
[4] F. L. van Ness and M. A. Bouman, “Spatial Modulation Transfer Function of the Human
Eye,” J. Optical Soc. of America (JOSA), vol. 57, pp. 401–406, 1967.

References
221
[5] J . L. Mannos and D. J. Sakrison, “The Effects of a Visual Error Criteria on the Encoding
of Images,” IEEE Trans. Information Theory, vol. IT-20, pp. 525–536, July 1974.
[6] D. H. Kelly, “Motion and Vision, II Stabilized Spatio-temporal Threshold Surface,”
Journal of Optical Soc. America, vol. 69, pp. 1340–1349, 1979.
[7] D. H. Kelly, “Theory of Flicker and Transient Responses, II Counterphase Gratings,”
Journal of Optical Soc. America, vol. 61, pp. 632–640, 1971.
[8] C. J. vd. B. Lambrecht and M. Kunt, “Characterization of Human Visual Sensitivity for
Video Imaging Applications,” Signal Processing, vol. 67, pp. 255–269, June 1998.
[9] A. C. Bovik, Ed., Handbook of Image and Video Processing, 2nd Ed., Elsevier Academic
Press, Burlington, MA, 2005.
[10] H. J. Trussell and M. J. Vrhel, Fundamentals of Digital Imaging, Cambridge University
Press, Cambridge, UK, 2008.
[11] E. Dubois, The Structure and Properties of Color Spaces and the Representation of Color
Images, Morgan & Claypool Publishers, 2010.
[12] E. J. Giorgianni and T. E. Madden, Digital Color Management: Encoding Solutions,
Addison-Wesley, Reading, MA, 1998.
[13] CIE
(2010–2011).
Downloads.
Available
at
http://www.cie.co.at/index.php/
DOWNLOADS. Also available at Web site of Colour and Vision Research Lab,
UCL: http://cvrl.ucl.ac.uk/
[14] G. Sharma, “LCDs versus CRTs—Color Calibration and Gamut Consideration,” Proc.
IEEE, vol. 90, no. 4, pp. 605–622, April 2002.
[15] International Color Consortium. Available at http://www.color.org/index.xalter
[16] ITU, Recommendation ITU-R BT.709, Basic Parameter Values for the HDTV Standard
for the Studio and for International Programme Exchange (1990), Geneva: ITU, 1990.
[formerly CCIR Rec. 709].
[17] C. Poynton, A Guided Tour of Color Space. Available at http://www.poynton.com/
papers/Guided tour/abstract.html
[18] Wikipedia (2010). YCbCr. Available at http://en.wikipedia.org/wiki/YCbCr
[19] A. C. Bovik, Ed., The Essential Guide to Image Processing, 2nd Ed., Elsevier Academic
Press, Burlington, MA, 2009.
[20] Foveon (2010). Available at http://www.foveon.com
[21] B. K. Gunturk, J. Glotzbach, Y. Altunbasak, R. W. Schafer, and R. M. Mersereau, “Demo-
saicking: Color Filter Array Interpolation,” IEEE Signal Process. Magazine, vol. 22,
pp. 44–54, January 2005.
[22] Andor Technology (2011). sCMOS Camera. Available at http://www.andor.com/
scientiﬁc cameras/neo scmos camera/
[23] H. Stark and J. W. Woods, Probability and Random Processes with Applications to Signal
Processing, 3rd Ed., Prentice-Hall, Upper Saddle River, NJ, 2002.
[24] H. Seetzen, W. Heidrich, W. Stuerzlinger, G. Ward, L. Whitehead, M. Trentacoste,
A. Ghosh, and A. Vorozcovs, “High Dynamic Range Display Systems,” ACM Trans-
actions on Graphics, 2004.
[25] Wikipedia (2008). DR37-P. Available at http://en.wikipedia.org/wiki/DR37-P

CHAPTER
Image Enhancement
and Analysis
7
In this chapter we begin by looking at some simple image processing ﬁlters. Then
we will use some of them for image enhancement, generaly taken to mean improving
the image in some sense, such as less noise, sharp edges, good contrast, etc. Then we
move on to the problem of image analysis, meaning the extraction of structure and
other useful information from an image. We will ﬁrst look at detection of lines and
edges and then brieﬂy discuss the edge-linking problem that occurs when the located
edge pixels on an object boundary have gaps. We then raise our level of analysis
to objects and look at problems of image segmentation. Simple manual thresholding
is followed by the more powerful K-means algorithm. Segmented regions may be
disconnected, so we also look at region growing, a segmentation technique that uses
pixel location giving connected regions. Finally, we will brieﬂy consider the problem
of detecting objects in an image.
7.1 SIMPLE IMAGE PROCESSING FILTERS
Here, we introduce a few basic image processing ﬁlters, some of which we will use
in later chapters, but all of these simple ﬁlters are used extensively in the practice
of image and video signal processing. These ﬁlters and more, with extensive pro-
cessed image examples, can be found in the image processing text by Gonzales and
Woods [1].1
Box Filter
Perhaps the simplest of the basic image processing ﬁlters is the box ﬁlter used to
perform linear averages over image regions. The most common size of box ﬁlter is
3 × 3, giving the ﬁnite impulse response (FIR) impulse response h(n1,n2) with mask
1
9


1
1
1
1
1
1
1
1
1

,
1Richard Woods (no relation).
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00007-2
c⃝2012 Elsevier Inc. All rights reserved.
223

224
CHAPTER 7 Image Enhancement and Analysis
where the n2 axis is directed downward and the n1 axis is directed across. In order
to make the lowpass ﬁlter zero-phase, we place the point (n1,n2)T = 0 at the center
of the matrix. Box ﬁlters come in all square and rectangular sizes, with the scaling
constant adjusted to create the average of these pixel values. From Chapter 1, the
Fourier transform or frequency response of the L1 × L2 box ﬁlter is given as
H(ω1,ω2) =
1
L1L2
sinω1L1/2
sinω1/2
sinω2L2/2
sinω2/2 ,
which is not a very good lowpass ﬁlter due to its large side lobes. Figure 7.1–1 shows
the DFT magnitude plot of a 3 × 3 box ﬁlter (see box.m in folder M-ﬁles on the
book’s Web site).
However, a box ﬁlter only requires L1L2 additions and one multiply (divide) in
a straight-forward implementation and so is often used. Problem 1 at the end of this
chapter leads you to show how this ﬁlter can be implemented recursively to greatly
reduce the number of adds.
0
20
40
60
80
0
20
40
60
80
0
0.2
0.4
0.6
0.8
1
k2
k1
|H(k1,k2)|
FIGURE 7.1–1
Magnitude frequency response of a box ﬁlter obtained with 64 × 64 DFT in MATLAB.
Gaussian Filter
A simple Gaussian lowpass ﬁlter is given in terms of its impulse response as
h(n1,n2) = exp−1
2σ 2

n2
1 + n2
2

,

7.1 Simple Image Processing Filters
225
with approximate frequency response, over [−π,+π]2,
H(ω1,ω2) = 2π exp−1
2σ 2 
ω2
1 + ω2
2

,
when σπ ≫1. Since this impulse response has inﬁnite support, it is truncated to a
ﬁnite L × L window centered on the origin. Usually the value of L must be large com-
pared to the box ﬁlter, but for this price we are guaranteed no ringing in the lowpass
output. A small 5 × 5 ﬁlter with approximate Gaussian shape has been suggested by
Burt and Adelson [2]. The ﬁlter is separable h(n1,n2) = h(n1)h(n2), with
h(n) =



0.4,
n = 0,
0.25,
n = ±1,
0.05,
n = ±2.
This ﬁlter has been extensively used for image smoothing to reduce both high-
frequency content and noise. It can also be applied recursively to create the so-called
Gaussian pyramid [2, 3].
Prewitt Operator
The Prewitt operator is an unscaled, ﬁrst-order approximation to the gradient of a
supposed underlying continuous-space function x(t1,t2) computed as
∂x
∂t1
≈x(t1 + 1t1,t2) −x(t1 −1t1,t2)
21t1
,
but given by a 3 × 3 FIR impulse response h(n1,n2) with mask2


−1
0
1
−1
0
1
−1
0
1

,
again centered on the origin to eliminate delay. Note that the Prewitt operator aver-
ages three ﬁrst-order approximations to the gradient, from the three closest scan lines
of the image to reduce noise effects. The corresponding approximation of the vertical
gradient,
∂x
∂t2
≈x(t1,t2 + 1t2) −x(t1,t2 −1t2)
21t2
,
is given by the 3 × 3 FIR impulse response h(n1,n2) with mask


−1
−1
−1
0
0
0
1
1
1

,
again centered on the origin to eliminate delay, and again with the n2 axis directed
downwards and the n1 axis directed across.
2Note this does not match matrix notation. To do that would require a matrix transpose of this array.

226
CHAPTER 7 Image Enhancement and Analysis
Sobel Operator
The Sobel operator is a slight variation on the Prewitt operator, with horizontal and
vertical masks given as


−1
0
1
−2
0
2
−1
0
1


and


−1
−2
−1
0
0
0
1
2
1

,
with the central row being weighted up to achieve some emphasis on the current row
or column.
Both the Sobel and Prewitt operators are used widely in image analysis [4] to
help locate edges in images. Location of edges in images can be a ﬁrst step in image
understanding and object segmentation, where the output of these “edge detectors”
must be followed by some kind of regularization—i.e., smoothing, thinning, and gap
ﬁlling [1]. A plot of the imaginary part of the frequency response of the Sobel oper-
ator, centered on zero, is shown in Figure 7.1–2, with a contour plot in Figure 7.1–3.
We can see that the plots approximate a scaled version of the frequency function ω1,
but only at low frequencies. In particular, the side lobes at high ω2 indicate that this
ﬁlter should only be used on lowpass data, or alternatively be used in combination
with a suitable lowpass ﬁlter to reduce these side lobes.
0
80
10
5
0
−5
−10
k1
|H(k1, k2)|
60
20
40
0
40
50
60
k2
10
20
30
70
FIGURE 7.1–2
Imaginary part of horizontal Sobel operator frequency response, obtained with 64 × 64 DFT.

7.1 Simple Image Processing Filters
227
k2
k1
60
50
40
20
10
20
30
40
50
60
10
30
FIGURE 7.1–3
Contour plot of the imaginary part of horizontal Sobel operator, obtained with 64 × 64 DFT.
Laplacian Filter
In image processing, the name Laplacian ﬁlter often refers to the simple 3 × 3 FIR
ﬁlter


0
−1
0
−1
4
−1
0
−1
0

,
used as a ﬁrst-order approximation to the Laplacian of an assumed underlying
continuous-space function x(t1,t2):
∇2x(t1,t2) = ∂2x(t1,t2)
∂2t1
+ ∂2x(t1,t2)
∂2t2
.
The magnitude of the frequency response of this Laplacian ﬁlter is shown in
Figure 7.1–4, where again we note the reasonable approximation at low frequencies
only.
The zero-crossing property of the Laplacian ﬁlter can be used for edge location.
Often these derivative ﬁlters are applied to a smoothed function to avoid problems
with image noise ampliﬁcation [1]. Another Laplacian approximation is available
using the Burt and Adelson Gaussian ﬁlter [2].

228
CHAPTER 7 Image Enhancement and Analysis
k2
k1
60
50
40
20
10
20
30
40
50
60
10
30
FIGURE 7.1–4
Contour plot of the magnitude frequency response of the Laplacian ﬁlter.
7.2 IMAGE ENHANCEMENT
The goal of image enhancement is to improve an image in some manner, possibly
by reducing the visual noise content or sharpening a blurred image. In this section
we will look at two approaches, the ﬁrst using linear ﬁlters and the second using a
nonlinear one known as the median ﬁlter.
Linear Filtering
We assume a 2-D FIR ﬁlter h of size M × M. For the case of additive noise distortion,
we take the noise w to be independent and Gaussian ∼N(0,σ 2), so that the noisy
image is written as
y(n1,n2) = x(n1,n2) + w(n1,n2).
The noisy sailboat image is shown in Figure 7.2–1 for additive noise standard devi-
ation σ = 32 (variance σ 2 = 1024), giving a mean square error (MSE) of 1024 with
respect to the noise-free image x.
To enhance this noisy image, we consider using a 2-D FIR ﬁlter h of size M × M
to produce the estimatebx as
bx(n1,n2) = (h ∗y)(n1,n2).
Using the 3 × 3 box ﬁlter mentioned previously, we get the result shown in
Figure 7.2–2 with an MSE of 183. We see a smoothing effect on the image and a
reduction in the noise. After two passes through the box ﬁlter, we get the image in

7.2 Image Enhancement
229
FIGURE 7.2–1
Noisy sailboat (512 × 512) image with noise variance of 1024.
FIGURE 7.2–2
Output after 3 × 3 box ﬁltering (ﬁltered once).

230
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.2–3
Output after two passes of 3 × 3 box ﬁlter.
Figure 7.2–3 showing more smoothing of the image structures along with a further
reduction in the noise and an MSE of 165. Note that repeated convolution with a
box ﬁlter will eventually result in an output very close to that of a ﬁnitesupport
approximation to a separable Gaussian kernel.
One way of thinking about these ﬁlters is that we are obtaining an approxima-
tion to the sample mean for the local data in its convolution window. Since the
sample mean minimizes the least-squares error [5], this approach is mathematically
motivated.
Filtering in Intensity Domain
Images are normally available after gamma compensation, what we loosely called
here the density domain. If we convert the gamma-compensated images (cf. Gamma
in Section 6.6) to an approximate intensity-domain image via
xint = x2.2,
(7.2–1)
we get the image shown in Figure 7.2–4. This image has too much contrast because
of the gamma nonlinearity that is part of the image display. However, we can try
ﬁltering in this intensity domain and then convert back to density for purposes of
display. In that case, care must be taken since the noisy image y may have negative
values after we add the noise; thus we implement
yint = max(yin,0)2.2.
Performing box ﬁltering on the intensity-domain image yint and then converting
back to the density domain via the inverse of (7.2–1), we get the image shown in

7.2 Image Enhancement
231
FIGURE 7.2–4
Intensity domain image scaled for display.
Figure 7.2–5 after a single pass with an MSE of 267. The corresponding two-pass
output is then shown in Figure 7.2–6.
In general, linear ﬁltering of images can be performed in either domain, or in
fact, on any nonlinear transformation of the image sensor data. Linear ﬁltering in the
intensity domain is analogous to the ﬁltering that can be performed using lenses via
the theory of Fourier optics [6]. But linear ﬁltering in the density or contrast domain
may be thought more appropriate due to the often additive nature of the approximate
Gaussian nature of the noise there (e.g., photon noise and ﬁlm grain noise). Still, and
as pointed out in Chapter 6, this additive noise will tend to have a signal-modulated
variance and therefore will not be strictly independent of the signal.
Often the actual nonlinear mapping used in gamma compensation is not known,
so a conversion back to intensity is not possible, and maybe not even advisable. For
this reason, most of the literature on image noise reduction treats ﬁltering directly
upon the given data. This may be starting to change with the advent of so-called
.raw digital camera output, which often is the full range, say, 16-bit intensity
data.
Median Filtering
In addition to the mean, the median is often useful in simple image enhancement.
While the mean is the least-squares estimator of a data set, the median is the esti-
mate that minimizes the mean absolute error (MAE) [7]. The simple median ﬁlter
computes the median or middle number of the pixel values in a sliding square

232
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.2–5
Result of 3 × 3 box ﬁltering in intensity domain.
FIGURE 7.2–6
Result of two passes through the box ﬁlter in intensity domain.

7.2 Image Enhancement
233
FIGURE 7.2–7
Output of 3 × 3 median ﬁlter applied once.
window. It is most easily seen in terms of an ordering operation followed by output
of the midvalue. Such nonlinear ﬁlters have been found to be much more difﬁcult
to analyze than their linear counterparts. However, they produce signiﬁcantly less
blurring of image features and produce good results for certain types of degrading
noises.
The image in Figure 7.2–7 shows the effect of 3 × 3 median ﬁltering on the noisy
image in Figure 7.2–1, with resulting MSE of 238. We see that the visual effect of the
median ﬁlter is similar to that of the linear box ﬁlter, although the box ﬁlter has done
the better job of suppressing the Gaussian noise. If the noise distribution were very
different, though, there can be a radical difference. For example, using so-called “salt
and pepper” noise—which means that each pixel is independently set to zero with a
certain probability p, here, p = 0.15—we get the noisy image shown in Figure 7.2–8.
Such a noise is often used to approximately model the effect of direct transmission of
a digital image over a binary symmetric channel [8, Sect. 7.4.3]. The median ﬁltered
result is seen in Figure 7.2–9. These results were obtained using MATLAB routines
SailboatFiltxxx.m and at the book’s Web site.
We see that the median ﬁlter has almost removed all the salt and pepper noise,
and has done this without much visible blurring of edges. For this type of noise,
the linear box ﬁlter would not do nearly as well. More on nonlinear image ﬁlters,
including the median ﬁlter, can be found in Chapter 12 of The Essential Guide to
Image Processing [8].

234
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.2–8
Sailboat with salt and pepper noise, with p = 0.15.
FIGURE 7.2–9
Output of 3 × 3 median ﬁlter of the salt and pepper noisy sailboat (ﬁltered once).

7.3 Image Analysis
235
7.3 IMAGE ANALYSIS
Image analysis differs from image enhancement in that the desire is not to produce
an “improved” output image, but rather to analyze or understand the image in some
way and provide as an output a description of the found structure. An example
would be to ﬁnd the object edges that are very important in human recognition and
also serve as building blocks for objects. Other examples could include local means
and variances, location of signiﬁcant feature points and shapes, etc. The resulting
increased knowledge of the structure of the image can then aid in computer vision
and robotic applications as well as in some advanced image processing methods that
take advantage of the image structure.
Edge Detection
The Prewitt and Sobel operators, and other differential operators, can be used to ﬁnd
edges in images. First the image is ﬁltered with, say, the Sobel operator. Then the
absolute value of the ﬁlter output y is compared to a predetermined ﬁxed threshold
γ, and a decision is made. We deﬁne the logical variable Edge(n1,n2) to denote the
detection of an edge at location (n1,n2) as
Edge(n1,n2) ≜
1,
|y|(n1,n2) ≥γ ,
0,
|y|(n1,n2) < γ .
The logical image Edge(n1,n2) then shows the found locations of edges in the input
image y. Consider the house image in Figure 7.3–1. If we use the horizontal Sobel
operator, we get the absolute value output image in Figure 7.3–2 and the horizontal
Edge image in Figure 7.3–3 for threshold γ = 200. The same can be done with the
vertical Sobel operator, resulting in the vertical Edge image seen in Figure 7.3–4.
FIGURE 7.3–1
256 × 256 gray-level version of the house image.

236
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.3–2
Absolute value of output of the horizontal Sobel ﬁlter.
FIGURE 7.3–3
Thresholded horizontal Sobel ﬁlter output at γ = 200.
If we raised the threshold, we would get fewer edge pixels, and the gaps with
“missing” edge pixels would get larger. On the other hand, the edge would get “thin-
ner” and more precise. By lowering the threshold, we could get fewer gaps, but the
“edges” would get “wider.” Combining the horizontal and vertical absolute values
y = yv + yr, and then thresholding, we get an Edge image that is a combination of
horizontal and vertical edges, as seen in Figure 7.3–5 with threshold value γ = 200.
These
images
were
obtained
using
the
short
MATLAB
programs
HouseSobVerEdge, HouseSobHorEdge, and
HouseSobEdge at this book’s
Web site. Note that while the combined image contains good edge pixels, there are
gaps and the “edges” are a bit wide in places.

7.3 Image Analysis
237
FIGURE 7.3–4
Thresholded output of vertical Sobel ﬁlter with γ = 200.
FIGURE 7.3–5
Output of threshold on the sum of absolute values of horizontal and vertical Sobel ﬁlters
with γ = 200.
We can reduce the gaps by lowering the threshold; for example, at threshold
γ = 100, we get the image in Figure 7.3–6 where the gaps on diagonal edges are
much reduced. However, if there is some noise in the image, this improvement comes
at a cost. In the image of Figure 7.3–7, we show the result of the Sobel edge detection
with threshold γ = 100, when the original image contains a small amount of random
noise, in this case Gaussian with µ = 0 and σ = 5.
If we use a simple 3 × 3 box ﬁlter to smooth the noisy image prior to taking
the Sobel ﬁlter analysis, we obtain the image in Figure 7.3–8 at the same threshold
value of 100. More on edge detection is contained in The Essential Guide to Image
Processing [8, Chapter 19].

238
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.3–6
Combined Sobel output with threshold γ = 100 for a noise-free image.
FIGURE 7.3–7
Threshold γ = 100 on combined Sobel output with a noisy image.
Edge Linking
The next step in a typical image analysis would be to attempt to link these edge pixels
together to form curves or lines. One way to do this is called edge relaxation [9] in
which the strengths of the candidate edge pixels (e.g., absolute values of gradient
outputs) are locally used to increase or decrease edge conﬁdence in an iterative way.
The procedure is a simple yet effective way to ﬁll in gaps in edges, and thus to form
lines.
A formal edge-linking approach called sequential edge linking (SEL) was pro-
posed by Eichel et al. [10] and then expanded to a multiresolution method in Cook
and Delp [11]. This technique starts at a known edge pixel on the unthresholded gradi-
ent image and then hypothesizes potential edge paths using a log-likelihood method

7.3 Image Analysis
239
FIGURE 7.3–8
Result of Sobel edge detection on the smoothed noisy image with threshold γ = 100.
and the Zigangirov-Jelinek (Z-J) search algorithm3 borrowed from channel-coding
research, sometimes called the stack algorithm. The hard decision of thresholding is
thus postponed until the path gets relatively long, thus giving increased robustness
against image noise. On a rectangular lattice, there are eight nearest neighbors of
each edge point, but a limited angular constraint (< π/2) on the developing paths
reduces the number of pixels to search to three for the next edge on the path, or 3n
for an n−length edge curve, where the pointwise decisions are L(left), S(straight),
or R(right). To avoid computational explosion, only the most promising paths, as
measured by the likelihood-based path metric, are extended.
In an example from [11], the test image shown in Fig. 7.3–9a is immersed
in independent and identically distributed (i.i.d.) Gaussian noise with mean zero,
and standard deviation σ = 40. The magnitude of the gradient image is shown in
Figure 7.3–9b. Figure 7.3–10 shows the SEL outputs: (a) from the basic algorithm
presented in [10] and (b) from the multiresolution extension presented in [11]. Both
these images are seen to show extremely good edge linking on this given test image.
Segmentation
Regions or objects in images are often delineated by gray-level and color edges, but
as we have seen, computed edge maps can often contain gaps and are not there-
fore closed. So they do not really provide a segmentation that constitutes the next
level upward in image analysis (i.e., an image understanding). Thus various methods
have been developed to segment the images directly using similarity of gray-level
(and color) values and local textures. The simplest methods are based on the image
3This is a type of sequential decoding method. See Section 6.4 in Wozencraft and Jacobs [14]. Paths
are searched in a depth-ﬁrst manner using a discard criterion based on a log-likelihood ratio. Upon
discard of the present hypothesized path, backtracking occurs. Computation and memory requirements
are variable and generally grow with the noise level.

240
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.3–9
(a) The original 334 × 432 gray-level image and (b) the noisy gradient image (from Cook
and Delp [11] c⃝1995 IEEE).
FIGURE 7.3–10
(a) SEL output from a two-level pyramid and (b) three-level multiresolution SEL output
(from Cook and Delp [11] c⃝1995 IEEE).
histogram. An automatic way to perform this is using the K-means algorithm. After
regions are obtained, higher level semantic information can be used to compose these
regions into meaningful objects.
Simple Thresholding
If we compute the image histogram, we may see that there are various minima appar-
ent. These minima can be used to set thresholds for region segmentation. Manually
setting thresholds will work well if the various objects in the image have sufﬁciently
different gray levels and the gray-level variation within each object is sufﬁciently
small.
Example 7.3–1: Manual Histogram Thresholding
Figure 7.3–11 shows the histogram of the House image from Figure 7.3–1. We can see two
minima in this histogram located at gray levels 90 and 170, approximately. If we thresh-
old the image at these points, we get the segmented image shown in Figure 7.3–12.
Because we have used two thresholds on the histogram to do the separation, we have
three segmented regions. The sky has been segmented together with the side edge of

7.3 Image Analysis
241
12000
10000
8000
6000
4000
2000
00
50
100
150
200
250
FIGURE 7.3–11
Histogram of the gray-level House image.
FIGURE 7.3–12
Result of manually thresholding the gray-level house image at the two most prominent
minima of the histogram, 90 and 170.
the roof4 and parts of the windows of the house. The house has been segmented into
two regions, one for the sun lit part and the other for the shadow. A MATLAB routine,
HouseManThresh.m, has been provided for easily experimenting with different thresh-
olds and their number.
4While the side edge of the roof looks brighter than the sky in this segmented image, they are in fact
the same gray value, 200. This is then an example of how surrounding affects perceived brightness.

242
CHAPTER 7 Image Enhancement and Analysis
K-means Algorithm
The K-means algorithm is a powerful method for image segmentation, which
attempts to segment the pixel values of an image into K classes in such a way that
the mean value of each class represents the image in a best sense as determined by a
chosen distance measure, usually MSE.
Let the pixel values be given as x(n1,n2) over the square region [0,N1 −1] ×
[0,N2 −1], and let there be K classes. We then want to ﬁnd K numbers r1,r2,...,rK−1
that cluster the range of x ∈[0,255] for standard 8-bit images. These numbers are the
means of each cluster region. So we also need a set of K decision values dk that
deﬁne cluster regions Ck = {dk−1 ≤x(n1,n2) < dk}, k = 1 to K, where d0 = 0 and
dK = 255. Ideally we would like to choose the rk such that the least-squares error
K
X
k=1
X
(n1,n2)∈Ck

x(n1,n2) −rk
2
(7.3–1)
is minimized. Effectively, we are replacing the actual pixel values in each cluster with
representative numbers rk in such a way that the least-squares error in the representa-
tion is minimized. This is a highly nonlinear problem to solve exactly, but we can see
the two following properties of the optimal solution, called the optimality conditions.
Optimal Values for the Ck
Given a set of cluster centers (means), the minimal value of (7.3–1) will occur when
values of generic pixels x are clustered to the nearest rk with corresponding clus-
ter region Ck = {dk−1 ≤x < dk}. This will occur when we choose dk to be halfway
between the rk values,
dk = 1
2(rk−1 + rk),
(7.3–2)
because otherwise pixel x is closer to rk−1 and therefore should not be clustered to rk.
Optimal Values for the rk
Given a set of cluster regions Ck, the minimal value of the sum P
(n1,n2)∈Ck
[x(n1,n2) −rk]2 will be obtained by the sample mean
rk = 1
Nk
X
(n1,n2)∈Ck
x(n1,n2),
(7.3–3)
with Nk = |Ck|—the size of (number of pixels in) Ck.
Using these two optimality conditions, we can construct the following K-means
algorithm, guaranteed to converge to at least a local minimum of (7.3–1).
1. Start with the initial guess of rk (possibly obtained by inspection of the image
histogram).
2. Calculate values for the Ck by using (7.3–2).
3. Calculate improved values for the rk by applying (7.3–3).
4. Return to step 2 as long as a stopping criteria are not satisﬁed.
We can see that at each step the value of the metric (7.3–1) will not increase (most
likely it will strictly decrease) so that a local minimum will be approached. We can

7.3 Image Analysis
243
set the stopping criteria at a maximum number of iterations, or based on the decrease
in (7.3–1) from the last iteration with a set minimal threshold. We show an example
next using MATLAB.
Example 7.3–2: K-means
Using the MATLAB routine kmeans, which is part of the Statistical Toolbox, we segmented
the 256 × 256 cameraman image into four classes. The original cameraman image is
shown in Figure 7.3–13.
FIGURE 7.3–13
Original 256 × 256 cameraman image.
The initial values for the four mean values were randomly selected and the ﬁnal con-
verged values were 183.4, 113.9, 156.8, and 19.7. Note that these intensity values are not
ordered because the initial values were chosen randomly. It took 20 iterations to converge,
as shown in the plot of Figure 7.3–14.
The corresponding label image is shown in Figure 7.3–15. We can see that the sky
and camera operator are segmented quite well. The grass in the foreground is less so,
with spotty errors and gaps present. The actual representative values, or class means, are
displayed as an image in Figure 7.3–16.
This clustering method can easily be extended to the case of vector x where the
cluster centers are necessarily also vectors rk and an appropriate distance measure
d(x, r) must be deﬁned. In this case, optimality condition 1 gets replaced by
Ck = {x|d(x, rk) ≤d(x, rm) for all m ̸= k},
i.e. choose the nearest cluster Ck in distance d.

244
CHAPTER 7 Image Enhancement and Analysis
0
5
10
15
20
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
2
2.1
Iteration number n
Sum sq error xE-07
Convergence of K-means algorithm
FIGURE 7.3–14
Plot of convergence of sum squares of K-means algorithm.
FIGURE 7.3–15
Image showing the four class indices resulting from the K-means algorithm.
The following example shows the K-means algorithm applied to a color image.
Instead of scalar gray-level values, the algorithm now works on pixel vectors. The
cluster centers are then distinct colors.

7.3 Image Analysis
245
FIGURE 7.3–16
Image showing the class means—i.e., the representative values in each class.
Example 7.3–3: K-means for Color Images
We can also use the K-means algorithm for color images. Using the MATLAB routine
kmeans again, we segment the 256 × 256 color house image into four classes, with
random initialization points. It takes 20 iterations to converge to the four local mean
RGB vectors:
c(1) = (156.3778,108.1687,102.5225),
c(2) = (157.7117,194.5358,217.0254),
c(3) = (214.1900,216.6208,217.0121),
c(4) = (95.7656,62.5379,78.5369).
The resulting index image is shown in Figure 7.3–17. The resulting output color image is
shown in Figure 7.3–18. The distance used is sum of squares of differences in CIELAB
space (cf. CIELAB in Section 6.5), called simply Lab space in MATLAB.
We could go on to apply the K-means method to small blocks of image values, say,
2 × 2 or 2 × 2, thus beginning to explore segmentation based on local texture as well
gray (color) pointwise values. Instead, we turn to introduce a method based on spatial
location that results in the property of the segmented region(s) being connected.
Region Growing
The segmentation techniques discussed so far do not give attention to spatial loca-
tion and so will tend to produce unconnected regions. Here, we look at a method

246
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.3–17
The index image of the K-means result for color image house.
FIGURE 7.3–18
Image showing the class means—i.e., the representative values in each class.
that builds in connectedness, called region growing. The following region-growing
algorithm is based on the centroid linkage algorithm [12]:
1. Specify a comparison threshold T. Specify a seed pixel and note its value.
This is the ﬁrst point in the region. Its value then becomes the region average.
2. Compare the nearest eight neighbor pixel values to the average value in the region.
If the difference is less than threshold T, then add such pixels to the region.
Otherwise, do not add the pixel.

7.3 Image Analysis
247
3. Compare all nearest-neighbor pixels on the boundary of the region.
If the difference is less than the threshold, then add such pixels to the region.
Otherwise, do not add the pixel.
4. Go to step 3 if more pixels remain; otherwise stop.
Example 7.3–4: Growing a Region
We will apply a simple region-growing algorithm to the 512 × 480 gray-level ﬂower image
in Figure 7.3–19. The pixel values have been scaled to the interval [0,1], and the point
(328, 341) located on a ﬂower petal has been selected as the seed. We show two results,
the ﬁrst with threshold 0.08 in Figure 7.3–20 and the second with threshold 0.2 in
Figure 7.3–21. We see that the threshold of 0.2 has done a pretty good job of segmenting
FIGURE 7.3–19
Gray-level 512 × 480 ﬂower image.
FIGURE 7.3–20
Region grown from seed location (328, 341) with threshold 0.08.

248
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.3–21
Region grown from seed location (328, 341) with threshold 0.2.
out the ﬂower. (Performed using MATLAB Central routine regiongrowing.m provided
by Dirk-Jan Kroon.)
Several seeds can be used in region growing to segment multiple objects or parts
of one object. Signiﬁcant interaction may be required to get the desired results. As
with clustering using K-means, pixel values can be replaced by textures on small
blocks to get a segmentation based on local texture in addition to pixel values. Sim-
ilarly, RGB color images can be treated using a distance in the relevant color space.
If segmenting for human viewing, then the more perceptually uniform color space
CIELAB may be used.
7.4 OBJECT DETECTION
We now brieﬂy look at two techniques for ﬁnding objects in images. First, we look
at segmentation of a given image into a small number of regions based on a distance
threshold. Then we look at a matched ﬁltering approach to detecting objects, called
template matching.
Object Segmentation
We can extend the region-growing algorithm to detect objects in color images. The
basic algorithm must be modiﬁed to compute distance on the YUV or Lab coordi-
nates. In addition, a raster scan of the pixels can detect undeﬁned pixels in an outer
loop added around the preceding algorithm. Each pass through this outer loop will
deﬁne a separate region, and the threshold T will determine the number of regions

7.4 Object Detection
249
found. Additionally, some small regions are formed due to noise in the image. These
must be merged into their nearest neighbors, in a YUV distance-deﬁned region.
Example 7.4–1: Object Segmentation5
In this example, an object detection strategy was applied to a frame from the Miss
America color video test clip of size 360 × 288 luma or Y pixels, with the U and V pixels
subsampled by 2 × 2. An appropriate Euclidean distance was formed based on the YUV
pixel values. The threshold was set to T = 40 for this 8-bit image. Figure 7.4–1 shows the
original frame.
Figure 7.4–2 shows a map of the four segments detected using this approach.
We see that the face and part of the neck is detected as one object, with the
FIGURE 7.4–1
A frame from the Miss America test clip.
FIGURE 7.4–2
A four-region segmentation of a color image frame from the Miss America test clip.
5Example taken from the PhD thesis of Soo Chul Han [13].

250
CHAPTER 7 Image Enhancement and Analysis
shadow under the chin as a second object. The red tunic is detected as two separate
objects, and the background is merged with her hair and black shirt. We thus see two
types of errors: oversegmentation (red tunic) and overmerging (hair, background, black
shirt).
Some semantic knowledge can be used at this point to further improve segmen-
tation accuracy if one is interested in true object recognition. In Chapter 11 on video
processing, we will return to this example, using it as a basis for motion-based
multiframe segmentation.
Template Matching
Often we have an example patch or small object given by an FIR template p(n1,n2)
that we seek to ﬁnd or locate in an image x. A generally good way to accomplish this
is with template matching. One way to accomplish this is with 2-D matched ﬁltering,
where we set the ﬁlter equal to the patch reﬂected through the origin,
h(n1,n2) = p(−n1,−n2),
then the matched ﬁlter output is just y = (h ∗x)(n1,n2). We look for peaks in this
output and compare them to a ﬁxed threshold γ ,
y(n1,n2) ≷γ .
If y(n1,n2) ≥γ , we say we have detected an instance of the object at location (n1,n2)
in the image x; otherwise the object is not detected. Under the assumption of a known
object possibly (or not) present at a known location, and an image background com-
posed of white Gaussian noise, the matched ﬁlter detector is known to be optimal
[14]. While such ideal conditions are not often the case in image processing, still
template matching is a commonly used and generally useful technique for locating
known objects in cluttered backgrounds. We have already encountered a simple case
of template matching in the edge detectors presented earlier, where the template was
the edge model.
Example 7.4–2: Template Matching
In this example we seek to ﬁnd the black jelly beans in a noisy image using tem-
plate matching. The noisy image is shown in Figure 7.4–3 and was produced by
adding computer-generated white Gaussian noise (σ = 50) to the 256 × 256 monochrome
image shown in Figure 7.4–4. The template image for this example was formed by
cropping out the bottom rightmost black jelly bean to give a 18 × 15 - pixel template
shown in Figure 7.4–5. After normalizing this template to have zero mean, it was
reﬂected through the origin and convolved with the noisy image in the MATLAB program

7.4 Object Detection
251
FIGURE 7.4–3
Noisy jelly beans image.
FIGURE 7.4–4
Original jelly beans image.
FIGURE 7.4–5
Black jelly bean template.
TemplateMatch.m, available at this textbook’s Web site. The ﬁlter output image is
shown in Figure 7.4–6. When subjected to a threshold of 125, this image produced
the logical mask image shown in Figure 7.4–7. Figure 7.4–8 shows an overlay of the

252
CHAPTER 7 Image Enhancement and Analysis
FIGURE 7.4–6
Matched ﬁlter output image.
FIGURE 7.4–7
Threshold output image, where white (250) indicates a black jelly bean and black (0)
indicates its absence.
detector-thresholded output over the original image. We see a generally successful detec-
tion of the black jelly beans, although with some false alarms, mostly due to the large noise
level. The threshold can be adjusted to trade off the number of black beans detected ver-
sus the false alarms (i.e., white or gray miss-detected). You can try out this routine yourself
with a lower noise level and different threshold.

Problems
253
FIGURE 7.4–8
Detector output superposed over the original jelly bean image.
CONCLUSIONS
In this chapter, we introduced some simple image processing ﬁlters that are com-
monly used for ehancement and noted their shortcomings from the frequency-
response viewpoint. Design techniques from Chapter 5 can usefully be applied here to
improve on some of these simple image processing ﬁlters. We used some of the sim-
ple ﬁlters to perform basic image denoising. We looked at the median ﬁlter, a simple
nonlinear ﬁlter, and applied it to denoising problems. We then looked at introductory
image analysis, ﬁrst ﬁnding edges and then linking them together. We also looked at
segmentation of regions and objects as a second level in image analysis. We will ﬁnd
several of these techniques applied in later chapters on image and video processing.
Finally, we introduced the problem of object detection and template matching. Basic
image processing techniques are covered by Bovik in Chapter 2.1 of [15].
PROBLEMS
1. The box ﬁlter was introduced in Section 7.1 as a simple FIR ﬁlter that ﬁnds wide
use in image processing practice. Here, we consider a square L × L box ﬁlter and
implement it recursively to reduce the required number of additions.
(a) Consider an odd-length 1-D L = 2M + 1 point box ﬁlter with unscaled
output
Ly(n) =
n+M
X
k=n−M
x(k),

254
CHAPTER 7 Image Enhancement and Analysis
and show that this sum can be realized recursively by
Ly(n) = Ly(n −1) + x(n + M) −x(n −M −1).
How many adds and multiplies per point are required for this 1-D ﬁlter?
(b) Find a 2-D method to realize the square L × L box ﬁlter for odd L. How much
intermediate storage is required by your method? (Intermediate storage is the
temporary storage needed for the processing. It does not include any storage
that may be needed to store either the input or output arrays.)
2. For the horizontal derivative 3 × 3 FIR approximation, called the Sobel operator,
consider calculating samples of its frequency response with a 2-D DFT program.
Assume the DFT is of size N × N for some large value of N.
(a) Where can you place the Sobel operator coefﬁcients in the square [0,N −1]2
if you are only interested in samples of the magnitude response?
(b) Where must you place the Sobel operator coefﬁcients in the square [0,N −
1]2 if you are interested in samples of the amplitude of the Fourier transform?
3. Image processing texts recommend that the Sobel operator (ﬁlter)



−1
0
+1
−2
0
+2
−1
0
+1



should only be used on a smoothed image. So, consider the simple 3 × 3
smoothing ﬁlter



1
1
1
1
1
1
1
1
1



with output taken at the center point. Let us smooth ﬁrst and then apply the Sobel
operator to the smoothed output.
(a) What is the size of the resulting combined FIR ﬁlter?
(b) What is the combined ﬁlter’s impulse response?
(c) Find the frequency response of the combined ﬁlter.
(d) For a spatial domain or direct implementation, is it more efﬁcient to apply
these two 3 × 3 ﬁlters in series or to apply the combined ﬁlter? Why?
4. Use the available MATLAB routine HouseSobEdgeNoise and the WinDesC ﬁl-
ter design routine to determine a “best” smoothing lowpass ﬁlter in terms of cutoff
frequency for a 5 × 5 FIR ﬁlter. Measure “best” by the visual quality of the result-
ing edge mask in terms of connectedness, thinness, gaps or missed edge pixels,
and false edge pixels. Of course, your ﬁndings will be very subjective. First, vary
cutoff frequency for the ﬁxed threshold value 100. Then experiment locally about

References
255
this “optimum” for various thresholds in the range 80–150. Compare the result to
that of the 3 × 3 box ﬁlter.
5. Prove optimality condition 2 (7.3–3) for the K-means algorithm by ﬁnding the
optimal value of rk to minimize the sum
X
(n1,n2)∈Ck
[x(n1,n2) −rk]2
for a given Ck.
6. Use the MATLAB routine kmeans.m (from the Statistical Toolbox) to ﬁnd
a locally optimal segmentation of the monochrome Barbara image based on
2 × 2 blocks into four and eight clusters. You may use the provided routine
kmeans4HouseRandom.m as a base.
7. Experiment with other color spaces for cluster-based segmentation using the color
512 × 512 Lena image and the MATLAB kmeans.m routine. The image pro-
cessing toolbox has color conversion routines for sRGB, XYZ, and CIELAB
(LAB).
REFERENCES
[1] R. C. Gonzalez and R. E. Woods, Digital Image Processing, 2nd Ed., Prentice-Hall,
Upper Saddle River, NJ, 2002.
[2] P. J. Burt and E. H. Adelson, “The Laplacian Pyramid as a Compact Image Code,” IEEE
Trans. Comm., vol. COM-31, pp. 532–540, April 1983.
[3] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1990.
[4] A. K. Jain, Fundamentals of Digital Image Processing, Prentice-Hall, Englewood Cliffs,
NJ, 1989.
[5] H. Stark and J. W. Woods, Probability and Random Processes with Appl. to Signal
Process., 3rd Ed., Prentice-Hall, Upper Saddle River, NJ, 2002.
[6] J. W. Goodman, Intro. to Fourier Optics, 3rd Ed., Roberts and Company Publishers,
Greenwood Village, CO, 2005.
[7] Y.-S. Lee, “Graphical Demonstration of an Optimality Property of the Median,” The
American Statistician, vol. 49, no. 4, pp. 369–372, 1995.
[8] A. C. Bovik, Ed., The Essential Guide to Image Processing, 2nd Ed., Elsevier Academic
Press, Burlington, MA, 2009.
[9] D. H. Ballard and C. M. Brown, Computer Vision, Prentice-Hall, Englewood Cliffs, NJ,
1982.
[10] P. H. Eichel, E. J. Delp, K. Koral, and A. J. Buda, “A Method for a Fully Automatic Deﬁ-
nition of Coronary Arterial Edges from Cineangiograms,” IEEE Trans. Medical Imaging,
vol. 7., pp. 313–320, December 1988.
[11] G. W. Cook and E. J. Delp, “Multiresolution Sequential Edge Linking,” IEEE Int. Conf.
on Image Process (ICIP), Washington DC, October 1995.

256
CHAPTER 7 Image Enhancement and Analysis
[12] R. Haralick and L. Shapiro, Computer and Robot Vision, Addison-Wesley, Reading, MA,
1992.
[13] S.-C. Han, Object-Based Representation and Compression of Image Sequences, PhD
thesis, ECSE Department, Rensselaer Polytechnic Institute, Troy, NY, 1997.
[14] J. M. Wozencraft and I. M. Jacobs, Principles of Communication Engineering, John
Wiley & Sons, New York, 1965.
[15] A. C. Bovik, Ed., Handbook of Image and Video Process., 2nd. Ed., Elsevier Academic
Press, Burlington, MA, 2005.

CHAPTER
Image Estimation and
Restoration
8
Here, we apply the linear systems and basic image processing knowledge of the
previous chapters to modeling, developing solutions, and experimental testing of
algorithms for two dominant problems in digital image processing: image estima-
tion and image restoration. By image estimation we mean the case where a clean
image has been contaminated with noise, usually through sensing, transmission, or
storage. We will treat the independent and additive noise case. The second problem,
image restoration, means that in addition to the noise, there is some blurring due to
motion or lack of focus. We attempt to “restore” the image in this case. Of course,
the restoration will only be approximate.
We ﬁrst develop the theory of linear estimation in two dimensions and then
present some example applications for monochrome image processing problems.
We then look at some space-variant and nonlinear estimators. This is followed
by a section on image and/or blur model parameter identiﬁcation (estimation) and
combined image restoration. We next look at some more recent work, including
non-Bayesian approaches. We then present a third problem, image superresolution,
which extends image restoration to the case where several similar copies of an image
are available, and a spatial upsampling is required. Finally, we make some brief
comments on extensions of the methods presented in this chapter to color images.
All of the image data in this chapter are thought to be “density data” (i.e., image
data after gamma compensation). This is the common space used in image process-
ing, probably for two reasons: ﬁrst, this space (domain) is more subjectively uniform
than the original intensity space, and second, most often the exact nature of the
gamma compensation is unknown.
8.1 TWO-DIMENSIONAL RANDOM FIELDS1
A random sequence in two dimensions is called a random ﬁeld. Images that display
clearly random characteristics include sky with clouds, textures of various types, and
sensor noise. It may perhaps be surprising that we also model general images that
1This section requires a background in 1-D random sequences and processes. An introduction and
summary is provided in the appendix to this chapter.
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00008-4
c⃝2012 Elsevier Inc. All rights reserved.
257

258
CHAPTER 8 Image Estimation and Restoration
are unknown as random ﬁelds with an eye to their estimation, restoration, and, in a
later chapter, transmission and storage. But this is a necessary ﬁrst step in designing
systems that have an optimality across a class of images sharing common statistics.
The theory of 2-D random ﬁelds builds upon random process and probability theory.
Many references exist including [1]. We start with some basic deﬁnitions.
Deﬁnition 8.1–1: Random Field
A 2-D random ﬁeld x(n1,n2) is a mapping from a probability sample space  to the class
of 2-D sequences. As such, for each outcome ζ ∈, we have a deterministic sequence,
and for each location (n1,n2), we have a random variable.
We are not using capital letters to distinguish between a random variable and the
value it takes on, i.e., X = x. This is because we are reserving capitalization for matri-
ces and Fourier transforms. For a random sequence x(n1,n2), we deﬁne the following
low-order moments:
Deﬁnition 8.1–2: Second-Order Moments
The mean function of a random sequence x(n1,n2) is denoted
µx(n1,n2) ≜E{x(n1,n2)}.
Correlation function,
Rx(n1,n2;m1,m2) ≜E{x(n1,n2)x∗(m1,m2)}.
Covariance function,
Kx(n1,n2;m1,m2) ≜E{xc(n1,n2)x∗
c (m1,m2)},
where xc(n1,n2) ≜x(n1,n2) −µx(n1,n2) and is called the centered version of x.
We immediately have the following relation between these ﬁrst- and second-order
moment functions,
Kx(n1,n2;m1,m2) = Rx(n1,n2;m1,m2) −µx(n1,n2)µ∗
x(m1,m2),
and we also deﬁne the variance function
σ 2
x(n1,n2) ≜Kx(n1,n2;n1,n2)
= var{x(n1,n2)} = E{|xc(n1,n2)|2},
and note
σ 2
x(n1,n2) ≥0.
When all the statistics of a random ﬁeld do not change with position (n1,n2), we
say that the random ﬁeld is homogeneous, analogously to the stationarity property
from random sequences [1]. The homogeneity assumption is often relied upon in

8.1 Two-Dimensional Random Fields
259
order to estimate the needed statistical quantities from a given realization or sample
sequence or image. Often the set of images used to estimate these statistics is called
the training set.
Deﬁnition 8.1–3: Homogeneous Random Field
A random ﬁeld is homogeneous when the Nth-order joint probability density function (pdf)
is invariant with respect to the N positions, and this holds for all positive integers N; that
is, for all N and locations ni, we have
fx(x(n1),x(n2),...,x(nN)) = fx(x(n1 + m),x(n2 + m),...,x(nN + m)),
independent of the shift vector m.2
Usually we do not have such a complete description of a random ﬁeld as is
afforded by a complete set of joint pdf’s of all orders, and so must resort to par-
tial descriptions, often in terms of the low-order moment functions. This situation
calls for the following classiﬁcation that is much weaker than (strict) homogeneity.
Deﬁnition 8.1–4: Wide-Sense Homogeneous
A 2-D random ﬁeld is wide-sense homogeneous (wide-sense stationary) if
1. µx(n1,n2) = µx(0,0) = constant, and
2. Rx(n1 + m1,n2 + m2;n1,n2) = Rx(m1,m2;0,0), independent of n1 and n2.
In the strict-sense homogeneous case, for notational simplicity we deﬁne the con-
stant mean µx ≜µx(0,0) and the two-parameter correlation function Rx(m1,m2) ≜
Rx(m1,m2;0,0) and covariance function Kx(m1,m2) ≜Kx(m1,m2;0,0).
Example 8.1–1: Independent and Identically Distributed Gaussian Noise
Let w(n1,n2) be independent and identically distributed (i.i.d.) Gaussian noise with mean
function µw(n1,n2) = µw, and standard deviation σ w > 0. Clearly, we have wide-sense
homogeneity here. The two-parameter correlation function is given as
Rw(m1,m2) = σ 2
wδ(m1,m2) + µ2
w
and covariance function
Kw(m1,m2) = σ 2
wδ(m1,m2).
2Be careful. While the notation fx(x(n1),x(n2),...,x(nN)) seems friendly enough, it is really short-hand
for fx(x(n1),x(n2),...,x(nN);n1,n2,...,nN), where the locations are given explicitly. This extended
notation must be used when we evaluate fx at speciﬁc choices for the x(ni).

260
CHAPTER 8 Image Estimation and Restoration
The ﬁrst-order pdf becomes
fx(x(n)) =
1
√
2πσ w
exp−
 x(n) −µw
2
2σ 2w
∽N(µw,σ 2
w).
The i.i.d. noise ﬁeld is called white noise when its mean is zero.
Filtering a 2-D Random Field
Let G(ω1,ω2) be the frequency response of a spatial ﬁlter, with impulse response
g(n1,n2). Consider the convolution of the ﬁlter impulse response with an i.i.d. noise
ﬁeld w(n1,n2) whose mean is µw. Calling the output random ﬁeld x(n1,n2), we have
x(n1,n2) = g(n1,n2) ∗w(n1,n2)
=
X
k1,k2
g(k1,k2)w(n1 −k1,n2 −k2).
We can then compute the mean and covariance function of x as follows:
µx(n1,n2) = E{x(n1,n2)}
= E



X
k1,k2
g(k1,k2)w(n1 −k1,n2 −k2)



=
X
k1,k2
g(k1,k2)E{w(n1 −k1,n2 −k2)}
=
X
k1,k2
g(k1,k2)µw(n1 −k1,n2 −k2)
=
X
k1,k2
g(k1,k2)µw
= G(0,0) µw.
The covariance function of the generated random ﬁeld x(n1,n2) is calculated as
Kx(n1,n2;m1,m2) = E{xc(n1,n2)x∗
c (m1,m2)
=
X
k1,k2
X
l1,l2
g(k1,k2)g∗(l1,l2)E{wc(n1 −k1,n2 −k2)
× w∗
c (m1 −l1,m2 −l2)}
=
X
k1,k2
X
l1,l2
g(k1,k2)g∗(l1,l2)Kw(n1 −k1,n2 −k2;m1 −l1,m2 −l2)
= g(n1,n2) ∗Kw(n1,n2;m1,m2) ∗g∗(m1,m2)
= g(n1,n2) ∗σ 2
wδ(n1 −m1,n2 −m2) ∗g∗(m1,m2).
(8.1–1)

8.1 Two-Dimensional Random Fields
261
Such a “coloring” of the i.i.d. noise is often useful in generating models for real corre-
lated noise. A warning on our notation in (8.1–1): the ﬁrst ∗denotes 2-D convolution
on the (n1,n2) variables, while the second ∗denotes 2-D convolution on the (m1,m2)
variables.
Generalizing a bit, we can write the output of a ﬁlter H(ω1,ω2) with general
colored noise input x(n1,n2) as
y(n1,n2) = h(n1,n2) ∗x(n1,n2)
=
X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2).
This could be a ﬁltering of the random ﬁeld model we just created. Calculating the
mean function of the output y, we ﬁnd
µy(n1,n2) = E{y(n1,n2)}
= E



X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2)



=
X
k1,k2
h(k1,k2)E{x(n1 −k1,n2 −k2)}
=
X
k1,k2
h(k1,k2)µx(n1 −k1,n2 −k2)
= h(n1,n2) ∗µx (n1,n2).
And for the correlation function,
Ry(n1,n2;m1,m2) = E{y(n1,n2)y∗(m1,m2)}
=
X
k1,k2
X
l1,l2
h(k1,k2)h∗(l1,l2)E{x(n1 −k1,n2 −k2)
× x∗(m1 −l1,m2 −l2)}
=
X
k1,k2
X
l1,l2
h(k1,k2)h∗(l1,l2)Rx(n1 −k1,n2 −k2;m1 −l1,m2 −l2)
= h(n1,n2) ∗Rx(n1,n2;m1,m2) ∗h∗(m1,m2),
and covariance function,
Ky(n1,n2;m1,m2) = E{yc(n1,n2)y∗
c (m1,m2)}
=
X
k1,k2
X
l1,l2
h(k1,k2)h∗(l1,l2)E{xc(n1 −k1,n2 −k2)
× x∗
c (m1 −l1,m2 −l2)}
=
X
k1,k2
X
l1,l2
h(k1,k2)h∗(l1,l2)Kx(n1 −k1,n2 −k2;m1 −l1,m2 −l2)
= h(n1,n2) ∗Kx(n1,n2;m1,m2) ∗h∗(m1,m2).

262
CHAPTER 8 Image Estimation and Restoration
If we specialize to the homogeneous case, we obtain
µy(n1,n2) =
X
k1,k2
h(k1,k2)µx
or, more simply,
µy = H(0,0) µx.
The correlation function is given as
Ry(m1,m2) =
X
k1,k2
X
l1,l2
h(k1,k2)h∗(l1,l2)Rx(m1 −k1 + l1,m2 −k2 + l2)
=
X
k1,k2
X
l1,l2
h(k1,k2)Rx(m1 −k1 + l1,m2 −k2 + l2)h∗(l1,l2)
= h(m1,m2) ∗Rx(m1,m2) ∗h∗(−m1,−m2)
= (h(m1,m2) ∗h∗(−m1,−m2)) ∗Rx(m1,m2).
(8.1–2)
Similarly, for the covariance function,
Ky(m1,m2) = (h(m1,m2) ∗h∗(−m1,−m2)) ∗Kx(m1,m2).
Taking Fourier transforms, we can move to the power spectral density (PSD)
domain
Sx(ω1,ω2) ≜
X
m1,m2
Rx(m1,m2)exp−j(m1ω1 + m2ω2)
and obtain the Fourier transform of (8.1–2) as
Sy(ω1,ω2) = H(ω1,ω2)Sx(ω1,ω2)H∗(ω1,ω2)
= |H(ω1,ω2)|2 Sx(ω1,ω2).
(8.1–3)
Example 8.1–2: Power Spectrum of Images
One equation is often used to represent the PSD of a typical image [2],
Sx(ω1,ω2) =
K
 1 +
 ω2
1 + ω2
2

/ω2
0
3/2 for
|ω1|,|ω2| ≤π,
and was used in [3] with K = π/42.19 to model a video conferencing image frame. The
resulting log plot from MATLAB is given in Figure 8.1–1 (ref. SpectImage.m). A 64- × 64-
point discrete Fourier transform (DFT) was used, and zero frequency is in the middle of
the plot, at k1 = k2 = 32. Note that the image appears to be very lowpass in nature, even
on this decibel or log scale. Note that this PSD is not rational and so does not correspond

8.1 Two-Dimensional Random Fields
263
−20
−30
−40
−50
−60
−70
−80
80
60
40
20
0
0
10
20
30
40
50
60
70
FIGURE 8.1–1
Log or dB plot of example spectra.
to a ﬁnite 2-D difference equation model, although it could be well approximated
by such.
Autoregressive Random Signal Models
A particularly useful model for random signals and noises is the autoregressive (AR)
model. Mathematically this is a 2-D difference equation driven by white noise. As
such it generates a correlated zero-mean ﬁeld, which can be colored by the choice of
its coefﬁcients. The 2-D AR model is given as
x(n1,n2) =
X
(k1,k2)∈Ra−(0,0)
ak1,k2x(n1 −k1,n2 −k2) + w(n1,n2).
We can ﬁnd the coefﬁcients ak1,k2 by solving the 2-D linear prediction problem
bx(n1,n2) =
X
(k1,k2)∈Ra−(0,0)
ak1,k2x(n1 −k1,n2 −k2),
(8.1–4)
which can be done using the orthogonality principle of estimation theory [1].

264
CHAPTER 8 Image Estimation and Restoration
Theorem 8.1–1: Optimal Linear Prediction
The (1,0)-step linear prediction coefﬁcients in (8.1–4) that minimize the mean-square
prediction error,
E{|x(n1,n2) −bx(n1,n2)|2},
can be determined by the orthogonality principle, which states that the error e ≜bx −x
must be orthogonal to the data used in the linear prediction:
e(n1,n2) ⊥x(n1 −k1,n2 −k2) for (k1,k2) ∈Ra −(0,0).
For two random variables, orthogonal means that their correlation is zero, so we have
E{e(n1,n2)x∗(n1 −k1,n2 −k2)} = 0 for (k1,k2) ∈Ra −(0,0). To actually ﬁnd the coefﬁ-
cients, we start with
E{e(n1,n2)x∗(n1 −k1,n2 −k2)} = 0,
and substitute e =bx −x, to obtain
E{bx(n1,n2)x∗(n1 −k1,n2 −k2)} = E{x(n1,n2)x∗(n1 −k1,n2 −k2)},
or
E



X
l1,l2
al1,l2x(n1 −l1,n2 −l2)x∗(n1 −k1,n2 −k2)


= E{x(n1,n2)x∗(n1 −k1,n2 −k2)},
X
l1,l2
al1,l2Rx(n1 −l1,n2 −l2;n1 −k1,n2 −k2) = Rx(n1,n2;n1 −k1,n2 −k2)
for (k1,k2) ∈Ra −(0,0),
which is an ordinary set of N linear equations in N unknown coefﬁcients—i.e., the num-
ber of points in the set Ra −(0,0). This assumes we know the correlation function Rx for
the indicated range. Once these coefﬁcients are found, the mean-square power (variance)
of e(n1,n2) can be determined.
In the homogeneous case, these equations simplify to
X
l1,l2
al1,l2Rx(k1 −l1,k2 −l2) = Rx(k1,k2) for (k1,k2) ∈Ra −(0,0).
Deﬁning a coefﬁcient vector a of dimension N, these equations can easily be put
into matrix form. The resulting correlation matrix R will be invertible if the correla-
tion function Rx is positive deﬁnite. The mean-square prediction error is then easily

8.2 Estimation for Random Fields
265
obtained as
E{|e(n1,n2)|2} = E{e(n1,n2)(bx −x)∗(n1,n2)}
= −E{e(n1,n2)x∗(n1,n2)}
= −E{(bx −x)(n1,n2)x∗(n1,n2)}
= E{|x|2(n1,n2)} −E{bx(n1,n2)x(n1,n2)∗}
= σ 2
x(n1,n2) −
X
l1,l2
al1,l2Rx(n1 −l1,n2 −l2;n1,n2),
which specialized to the homogeneous case becomes
σ 2
e = σ 2
x −
X
l1,l2
al1,l2Rx(−l1,−l2).
Example 8.1–3: (1×1)-order QP Predictor
Let the predictor coefﬁcient support of the 2-D linear predictor be given as
Ra −(0,0) = {(1,0),(0,1),(1,1)}.
Assume the random ﬁeld x is homogeneous with the zero mean and correlation (covari-
ance) function given as Rx(m1,m2). Then the three so-called Normal equations are given
in matrix form as


Rx(0,0)
Rx(1,−1)
Rx(0,−1)
Rx(−1,1)
Rx(0,0)
Rx(−1,0)
Rx(0,1)
Rx(1,0)
Rx(0,0)




a1,0
a0,1
a1,1

=


Rx(1,0)
Rx(0,1)
Rx(1,1)

.
8.2 ESTIMATION FOR RANDOM FIELDS
Here, we consider the problem of estimating a random ﬁeld x from observations
of another random ﬁeld y. We assume that x and y are zero mean and write the
estimate as
bx(n1,n2) =
X
(k1,k2)∈Rh
hk1,k2y(n1 −k1,n2 −k2).3
Using the orthogonality principle, with e =bx −x, we have
bx(n1,n2) −x(n1,n2) ⊥{y(n1 −k1,n2 −k2) for (k1,k2) ∈Rh},
3If x and y had nonzero means, µx and µy, respectively, then the appropriate estimate would have the
formbx=µx + h ∗(y −µy).

266
CHAPTER 8 Image Estimation and Restoration
which becomes
E{bx(n1,n2)y∗(n1 −k1,n2 −k2)} = E{x(n1,n2)y∗(n1 −k1,n2 −k2)}, or
E



X
l1,l2
hl1,l2y(n1 −l1,n2 −l2)y∗(n1 −k1,n2 −k2)


= E{x(n1,n2)y∗(n1 −k1,n2 −k2)},
X
l1,l2
hl1,l2Ryy(n1 −l1,n2 −l2;n1 −k1,n2 −k2) = Rxy(n1,n2;n1 −k1,n2 −k2)
for (k1,k2) ∈Rh.
Specialized to the homogeneous case, we have
X
(l1,l2)∈Rh
hl1,l2Ryy(k1 −l1,k2 −l2) = Rxy(k1,k2) for (k1,k2) ∈Rh.
(8.2–1)
Inﬁnite Observation Domain
In the case where the observation region Rh is inﬁnite, (8.2–1) becomes a convolu-
tion. For example, if Rh = (−∞,+∞)2, we get
h(n1,n2) ∗Ryy(n1,n2) = Rxy(n1,n2), where −∞< n1,n2 < +∞.
We can express this convolution in the frequency domain,
H(ω1,ω2)Syy(ω1,ω2) = Sxy(ω1,ω2),
or
H(ω1,ω2) = Sxy(ω1,ω2)
Syy(ω1,ω2),
at those frequencies where Syy > 0,4 which is the general equation for the 2-D
noncausal (unrealizable) Wiener ﬁlter. We consider a few special cases.
1. y = x + n with x ⊥n (i.e., x and n are orthogonal),
Sxy(ω1,ω2) = Sxx(ω1,ω2) and
Syy(ω1,ω2) = Sxx(ω1,ω2) + Snn(ω1,ω2),
so that the Wiener ﬁlter is
H(ω1,ω2) =
Sxx(ω1,ω2)
Sxx(ω1,ω2) + Snn(ω1,ω2),
the so-called estimation case. Notice that the ﬁlter is approximately 1 at those
frequencies where the signal-to-noise ratio (SNR) is high, with the SNR deﬁned
as Sxx(ω1,ω2)/Snn(ω1,ω2). The ﬁlter then attenuates other values.
4At those frequencies where Syy = 0, the exact value of H does not matter.

8.2 Estimation for Random Fields
267
2. y = g ∗x + n with x ⊥n, then
Sxy(ω1,ω2) = Sxx(ω1,ω2)G∗(ω1,ω2)
and
Syy(ω1,ω2) = |G(ω1,ω2)|2Sxx(ω1,ω2) + Snn(ω1,ω2),
so that the Wiener ﬁlter is
H(ω1,ω2) =
G∗(ω1,ω2)Sxx(ω1,ω2)
|G(ω1,ω2)|2Sxx(ω1,ω2) + Snn(ω1,ω2),
the so-called restoration case. Notice that the ﬁlter looks like an inverse ﬁlter at
those frequencies where the SNR is high. The ﬁlter tapers off and provides less
gain at other values.
3. y = u + n with u ⊥n, and we want to estimate x ≜b ∗u. In this case the Wiener
ﬁlter is just b convolved with the estimate in case 1,
H(ω1,ω2) =
B(ω1,ω2)Suu(ω1,ω2)
Suu(ω1,ω2) + Snn(ω1,ω2).
4. y(n1,n2) = x(n1 −1,n2), the linear prediction case, as treated earlier.
A good application of case 3 would be the estimation of a derivative of a signal.
Wiener Filter—Alternative Derivation
A direct derivation of the Wiener ﬁlter through the concept of a 2-D whitening ﬁlter
is possible. We ﬁrst give a brief summary of 2-D spectral factorization, whose factor
will yield the needed whitening ﬁlter. Then we re-derive the noncausal and go on to
ﬁnd the causal Wiener ﬁlter.
Theorem 8.2–1: Spectral Factorization
Given a homogeneous random ﬁeld x(n1,n2) with PSD Sxx(ω1,ω2) > 0 on [−π,+π]2, there
exists a spectral factorization
Sxx(ω1,ω2) = σ 2B⊕+(ω1,ω2)B⊖−(ω1,ω2),
with σ > 0, and B⊕+(z1,z2) is stable and causal with a stable causal inverse.
In terms of Z-transforms, we have B⊖−(z1,z2) = B⊕+(z−1
1 ,z−1
2 ), where we
assume real coefﬁcients in the spectral factors. Unfortunately, even when the PSD
Sxx(ω1,ω2) is rational, the spectral factors B⊕+ and B⊖−will generally be inﬁnite
order which can then be used as ideal functions in a 2-D ﬁlter design approximation.
Alternatively, the factors can be related to the linear prediction ﬁlters of the previous
section, and approximation can be obtained in that way too. More on 2-D spectral
factorization is contained in [4, 5].

268
CHAPTER 8 Image Estimation and Restoration
y(n1, n2)
w(n1, n2)
x∧(n1, n2)
G(z1, z2)
1/B++ (z1, z2)
FIGURE 8.2–1
Whitening ﬁlter realization of Wiener ﬁlter.
Consider observations consisting of the homogenous random ﬁeld y(n1,n2), and
assume the causal spectral factor of Syy(ω1,ω2) is given as B⊕+(z1,z2). Since this
factor has a stable and causal inverse, we can use B−1
⊕+(z1,z2) to whiten the spectra
of y and obtain whitened output w(n1,n2), with variance σ 2
w, often called the 2-D
innovations sequence. We can then just as well base our estimate on w and ﬁlter it
with G(z1,z2) to obtain the estimatebx(n1,n2), as shown in Figure 8.2–1.
We can deﬁne the estimation error at the output of G as
e(n1,n2) ≜x(n1,n2) −
X
l1,l2
g(l1,l2)w(n1 −l1,n2 −l2),
(8.2–2)
and then express the mean-square error (MSE) of the estimate in the real-valued
case as
E{e2(n1,n2)} = E




x(n1,n2) −
X
l1,l2
g(l1,l2)w(n1 −l1,n2 −l2)


2


= Rxx(0,0) −2E



X
l1,l2
g(l1,l2)x(n1,n2)w(n1 −l1,n2 −l2)



+ E




X
l1,l2
g(l1,l2)w(n1 −l1,n2 −l2)


2


.
Next, we use the whiteness of the innovations w(n1,n2) to obtain
E{e2(n1,n2)} = Rxx(0,0) −2
X
l1,l2
g(l1,l2)Rxw(l1,l2) + σ 2
w
X
l1,l2
g2(l1,l2)
= Rxx(0,0) +
X
l1,l2
[σ wg(l1,l2) −Rxw(l1,l2)/σ w]2
−1
σ 2w
X
l1,l2
R2
xw(l1,l2),
(8.2–3)
where the last step comes from completing the square. At this point we observe that
minimizing over choice of g is simple because it only affects the middle term, which

8.2 Estimation for Random Fields
269
clearly has its minimum at 0, which is easily obtained by setting
g(l1,l2) = 1
σ 2w
Rxw(l1,l2).
The overall linear minimum MSE ﬁlter is obtained by convolving the whitening ﬁlter
with g. In the Z-transform domain we have
G(z1,z2) = 1
σ 2w
Sxw(z1,z2)
= 1
σ 2w
Sxy(z1,z2)
B⊕+(z−1
1 ,z−1
2 )
.
So
H(z1,z2) =
1
B⊕+(z1,z2)G(z1,z2)
=
Sxy(z1,z2)
σ 2wB⊕+(z1,z2)B⊕+(z−1
1 ,z−1
2 )
= Sxy(z1,z2)
Syy(z1,z2).
NSHP Causal Wiener Filter
If we restrict the sum over l1 and l2 in (8.2–2) to the inﬁnite NSHP region
R⊕+ = {n1 ≥0,n2 ≥0} ∪{n1 < 0,n2 > 0},
we have a causal Wiener ﬁlter. Equation (8.2–3) changes to
E{e2(n1,n2)} = Rxx(0,0) +
X
(l1,l2)∈R⊕+
[σ wg(l1,l2) −Rxw(l1,l2)/σ w]2
−1
σ 2w
X
(l1,l2)∈R⊕+
R2
xw(l1,l2),
which is minimized at
g(l1,l2) =
( 1
σ 2w Rxw(l1,l2),
(l1,l2) ∈R⊕+
0,
else.
,
In the Z-transform domain, we have
G(z1,z2) = 1
σ 2w
[Sxw(z1,z2)]⊕+ ,
where the subscript notation ⊕+ on the cross-power spectra Sxw means that we only
include the part that has support on R⊕+. We can write the full expression for the

270
CHAPTER 8 Image Estimation and Restoration
2-D causal Wiener ﬁlter as
G(z1,z2) = 1
σ 2w
Sxw(z1,z2)
=
1
σ 2wB⊕+(z1,z2) [Sxw(z1,z2)]⊕+
=
1
σ 2wB⊕+(z1,z2)
"
Sxy(z1,z2)
B⊕+(z−1
1 ,z−1
2 )
#
⊕+
.
Again, this is an inﬁnite-order ideal ﬁlter. For approximation, any of the ﬁlter design
methods we have considered can be used. However, if the design is in the frequency
domain, then both magnitude and phase error must be taken into account. An example
was shown at the end of Chapter 5 in the design of a fully recursive Wiener ﬁlter.
Incidentally, the Wiener ﬁlter has been found particularly effective for joint electro-
optical or lens-DSP system design [6].
8.3 TWO-DIMENSIONAL RECURSIVE ESTIMATION
First, we review recursive estimation in one dimension, where we focus on the
Kalman ﬁlter as the linear estimation solution for an AR signal model. After a brief
review of the 1-D Kalman ﬁlter, we then extend this approach to two dimensions.
1-D Kalman Filter
We start with an AR signal model in the 1-D case,
x(n) =
M
X
k=1
ckx(n −k) + w(n),
where n ≥0. The mean µw = 0 and correlation Rw(m) = σ 2
wδ(m) for the white model
input noise w. The initial condition is initial rest: x(−1) = x(−2) = ··· = x(−M) = 0.
We do not observe the signal x directly. Instead, we see the signal x convolved with
an M-tap FIR ﬁlter h in the presence of an observation noise v that is orthogonal to
the signal generation noise w. The so-called observation equation is then given as
y(n) = h(n) ∗x(n) + v(n),
for n ≥0.
Re-expressing this model in vector form, we have, upon deﬁning the
M-dimensional signal column vectors
x(n) ≜[x(n),x(n −1),...,x(n −M + 1)]T,
w(n) ≜[w(n),0,...,0]T,

8.3 Two-Dimensional Recursive Estimation
271
and system matrix
C ≜


c1
c2
c3
···
cM
1
0
0
···
0
0
1
0
...
...
...
...
...
...
...
0
···
0
1
0


and Qw ≜


σ 2
w
0
0
···
0
0
0
0
···
0
0
0
0
···
0
...
...
...
...
...
0
···
0
0
0


,
the signal state equation
x(n) = Cx(n −1) + w(n), for n ≥0, subject to x(−1) = 0.
The advantage of this vector notation is that the Mth-order case can be treated with
vector algebra just as simply as the ﬁrst-order case. The scalar observation equation,
based on a linear combination of the elements of the state vector, can be written in
vector form as
y(n) = hTx(n) + v(n), for n ≥0,
by deﬁning the column vector h as
h ≜[h0,h1,...,hM−1]T,
where hl = h(l), l = 0,...,M −1. In these equations the observation noise v is a white
noise also Rv(m) = σ 2
vδ(m), but orthogonal to white model noise w (i.e., v(n) ⊥x(m)
for all n and m).
Since we have only speciﬁed the second order properties (i.e., mean and corre-
lation functions) of these signals and noises, we do not have a complete stochastic
characterization of this estimation problem. However, we can ﬁnd the best estima-
tor that is constrained to be linear from this information, as we did with the Wiener
ﬁlter nonrecursive formulation in the last section. The result is the so-called linear
minimum mean-square error (LMMSE) estimator.
Some comments are in order:
1. The signal model is strictly Markov-p with p = M only when the signal generating
noise w is Gaussian distributed.
2. When we have also that the observation noise is Gaussian, it turns out that
the LMMSE estimator is globally optimal in the unconstrained minimum mean-
square error (MMSE) case also. This occurs because, in the joint Gaussian case,
the optimal MMSE solution is linear in the data.
3. In applications, the support of the smoothing function h may not match the order
of the state equation. In that case, either equation may be padded with zeros to
come up with a match. Equivalently, we set M = max[these two integers].
4. In many applications, the matrix C is time-variant. Dynamic physics-based mod-
els are used in the mechanical and aerospace industries to stochastically model
vehicles and aircraft. The 1-D Kalman ﬁlter is widely used there.

272
CHAPTER 8 Image Estimation and Restoration
From the study of estimation theory [1], we know that the MMSE solution to the
problem is equivalent to ﬁnding the conditional mean
E{x(n)|y(n),y(n −1),...,y(0)} ≜bxa(n),
called the ﬁltering estimate. As a by-product, we also obtain the so-called one-step
predictor estimate
E{x(n)|y(n −1),y(n −2),...,y(0)} ≜bxb(n).
The solution for this estimation problem, when the signal and noise are jointly
Gaussian distributed, is contained in many texts on 1-D statistical signal processing,
(e.g. [1]).
1-D Kalman Filter Equations
Prediction:
bxb(n) = Cbxa(n −1),
n ≥0.
Updates:
bxa(n) =bxb(n) + g(n)[y(n) −hTbxb(n)].
Here, g(n) is the Kalman gain vector that scales the observation prediction error
term y(n) −hTbxb(n) prior to employing it as an additive update term to the state
prediction vector bxb(n). This gain vector g(n) is determined from error covariance
equations of the state vector x(n), which proceed via the nonlinear iterations:
Error covariance:
Pb(n) = CPa(n −1)CT + Qw,
n > 0,
Pa(n) = (I −g(n)hT)Pb(n),
n ≥0.
Here, Pb(n) and Pa(n) are the error-covariance matrices, before and after updating,
respectively. Their deﬁnition is
Pb(n) ≜E{(x(n) −bxb(n))((x(n) −bxb(n))T} and
Pa(n) ≜E{(x(n) −bxa(n))((x(n) −bxa(n))T}.
The gain vector then is given in terms of these as
Gain vector:
g(n) ≜Pb(n)h

hTPb(n)h + σ 2
v
−1
,
n ≥0.
We note from the AR signal model that x(0) = w(0), or equivalently x(0) =
(w(0),0,...,0)T, from which follows the initial conditionsbxb(0) = 0 and Pb(n) = Qw
for the preceding ﬁlter equations. For a complete derivation, in the special no-blur
case (i.e., h = δ), the reader is referred to Section 9.2 in [1], with the general blur
case treated in problem 9.10 of that text.
Since the underlying signal model and observation model are scalar, these vector
equations can be recast into equivalent scalar equations to offer more insight, which
we do next.

8.3 Two-Dimensional Recursive Estimation
273
Scalar Kalman Filtering Equations
Predictor:
bx(n)
b (n) =
M
X
l=1
clbx(n−1)
a
(n −l),
with bx(n)
b (m) =bx(n−1)
a
(m) for m < n.
(8.3–1)
Updates:
bx(n)
a (m) =bx(n)
b (m) + g(n)(n −m)[y(n) −
M−1
X
l=0
hlbx(n)
b (n −l)], for n −(M −1) ≤m ≤n.
(8.3–2)
We see that there is ﬁrst a prediction estimate bx(n)
b (n) based on the past observa-
tions y(m), m < n, but after this there are multiple updated estimatesbx(n)
a (m) based on
the current observation y(n), beginning at time n = m and proceeding to future time
n = m + (M −1). The estimate resulting from the ﬁrst update is called the Kalman
ﬁlter estimate, and the remaining ones are called Kalman smoother estimates. The
former is a causal estimate, while the latter involve a ﬁxed lag behind the observa-
tions y(n). Note that the estimates must not get worse, and normally will get better,
as they are successively updated. This follows from the corresponding optimality of
this recursive estimation procedure for the MSE criteria. Thus we expect the ﬁxed
lag smoother estimate of maximum lagbx(n)
a (n −(M −1)) to be the best one.
The scalar error-covariance equations become the following:
Before updates:
R(n)
b (n;k) =
M
P
l=1
clR(n)
a (n −l;m), for k < n,
R(n)
b (n;n)=
M
P
l=1
clR(n)
a (n;n −l) + σ 2
w.
After updates:
R(n)
a (k;l)= R(n)
b (k;l) −g(n)(n −k)
·
M−1
P
o=0
h(o)R(n)
b (n −o;l), for n −(M −1) ≤k,l ≤n.
The Kalman gain vector becomes in scalar form the gain array,
g(n)(k) =
M−1
X
l=0
h(l)R(n)
b (n −l;n −k)
, M−1
X
l=0
M−1
X
o=0
h(l)h(o)R(n)
b (n −l;n −o) + σ 2
v
!
,
for 0 ≤k ≤M −1.

274
CHAPTER 8 Image Estimation and Restoration
2-D Kalman Filtering
If we consider a 2-D or spatial AR signal model
x(n1,n2) =
X
(k1,k2)∈R⊕+
ck1,k2x(n1 −k1,n2 −k2) + w(n1,n2),
(8.3–3)
and scalar observation model
y(n1,n2) =
X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2) + v(n1,n2),
(8.3–4)
over a ﬁnite observation region O, we can process these observations in row-scanning
order (also called a raster scan) to map them to an equivalent 1-D problem. The
2-D AR model then maps to a 1-D AR model, but with a much larger state vector
with large internal gaps. If the ⊕+ AR model is of order (M,M), and the observa-
tion region is a square of side N (i.e., O = [0,N −1]2), then the equivalent 1-D AR
model is of order O(MN). This situation is illustrated in Figure 8.3–1. Looking at the
scalar ﬁltering equations (8.3–1) and (8.3–2), we see that the prediction term is still
of low order O(M2), but the update term is of order O(MN). The Kalman gain array
is then also of order O(MN), so that the nonlinear error-covariance equations must
be run to calculate each of these update gain coefﬁcients and are hence also of order
O(MN). Since an image size is normally very large compared to the AR signal model
orders, this situation is completely unsatisfactory. We thus conclude that the very
efﬁcient computational advantage of the Kalman ﬁlter in 1-D is a special case and is
lost in going to higher dimensions. Still, there are useful approximations that can be
employed based on the observation that, for most reasonable image signal models,
the gain terms should be mainly conﬁned to a small region surrounding the current
observations. This process then results in the reduced update Kalman ﬁlter that has
found good use in image processing when a suitable image signal model (8.3–3) and
image blur model (8.3–4) are available.
.
Past
Future
Present state
FIGURE 8.3–1
Illustration of the global state vector of a spatial Kalman ﬁlter.

8.3 Two-Dimensional Recursive Estimation
275
The scalar ﬁltering equations for the 2-D Kalman ﬁlter are as follows:
Prediction:
bx(n1,n2)
b
(n1,n2) =
X
(ll,l2)∈R⊕+
cl1l2bx(n1−1,n2)
a
(n1 −l1,n2 −l2),
bx(n1,n2)
b
(k1,k2) =bx(n1−1,n2)
a
(k1,k2),
for (k1,k2) ∈S⊕+(n1,n2).
Updates:
bx(n1,n2)
a
(k1,k2) =bx(n1,n2)
b
(k1,k2) + g(n1,n2)(n1 −k1,n2 −k2)

y(n1,n2) −
X
(l1,l2)∈R⊕+
h(l1,l2)bx(n1,n2)
b
(n1 −l1,n2 −l2)

,
for
(k1,k2) ∈S⊕+(n1,n2),
where S⊕+(n1,n2) is the 2-D global state region that must be updated at scan
location (n1,n2) as shown in the gray area in Figure 8.3–1. The corresponding
error-covariance equations are as follows:
Before updates:
R(n1,n2)
b
(n1,n2;k1,k2) =
X
(ll,l2)∈R⊕+
cl1l2R(n1,n2)
a
(n1 −l1,n2 −l2;k1,k2),
for
(k1,k2) ∈S⊕+(n1,n2),
R(n1,n2)
b
(n1,n2;n1,n2) =
X
(ll,l2)∈R⊕+
cl1l2R(n1,n2)
a
(n1,n2;n1 −l1,n2 −l2) + σ 2
w.
After updates:
R(n1,n2)
a
(k1,k2;l1,l2) = R(n1,n2)
b
(k1,k2;l1,l2) −g(n1,n2)(n1 −k1,n2 −k2)
·
X
(o1,o2)∈Sh
h(o1,o2)R(n1,n2)
b
(n1 −o1,n2 −o2;l1,l2),
for all
(k1,k2) and (l1,l2) ∈S⊕+(n1,n2).
The 2-D gain array is given as
g(n1,n2)(k1,k2)
=
P
(l1,l2)∈Sh h(l1,l2)R(n1,n2)
b
(n1 −l1,n2 −l2;n1 −k1,n2 −k2)
P
(l1,l2)∈Sh
P
(o1,o2)∈Sh h(o1,o2)h(l1,l2)R(n1,n2)
b
(n1 −l1,n2 −l2;n1 −o1,n2 −o2) + σ 2v
for all(k1,k2) ∈S⊕⊕(n1,n2).
(8.3–5)
In the case of no blurring (i.e., h = δ), the 2-D Kalman gain array simpliﬁes to
g(n1,n2)(k1,k2) = R(n1,n2)
b
(n1,n2;n1 −k1,n2 −k2)
.
R(n1,n2)
b
(n1,n2;n1,n2) + σ 2
v

.

276
CHAPTER 8 Image Estimation and Restoration
Reduced Update Kalman Filter
As a simple approximation to the preceding spatial Kalman ﬁltering equations, we
see that the problem is with the update, as the prediction is already very efﬁcient. So,
we decide to update only nearby previously processed data points in our raster scan.
We will deﬁnitely update those points needed directly in the upcoming predictions
but will omit update of points further away. Effectively, we deﬁne a local update
region U⊕+(n1,n2) of order O(M2) with the property that
R⊕+(n1,n2) ⊂U⊕+(n1,n2) ⊂S⊕+(n1,n2).
This is an approximation since all the points in S⊕+(n1,n2) will be used in some
future prediction. However, if the current update is only signiﬁcant for points in the
O(M2) neighborhood of (n1,n2), then omitting the updates of points further away
should have negligible effect. The choice of how large to make the update region
U⊕+(n1,n2) has been a matter of experimental determination.
The spatial recursive ﬁltering equations for the reduced update Kalman ﬁlter
(RUKF) then become the manageable set:
Prediction:
bx(n1,n2)
b
(n1,n2) =
X
(ll,l2)∈R⊕+
cl1l2bx(n1−1,n2)
a
(n1 −l1,n2 −l2),
bx(n1,n2)
b
(k1,k2) = bx(n1−1,n2)
a
(k1,k2),
for (k1,k2) ∈U⊕+(n1,n2).
Updates:
bx(n1,n2)
a
(k1,k2) = bx(n1,n2)
b
(k1,k2) + g(n1,n2)(n1 −k1,n2 −k2)

y(n1,n2) −
X
(l1,l2)∈R⊕+
h(l1,l2)bx(n1,n2)
b
(n1 −l1,n2 −l2)

,
for (k1,k2) ∈U⊕+(n1,n2),
with computation per data point of O(M2). In this way, experience has shown that a
good approximation to spatial Kalman ﬁltering can often be obtained [7].
Approximate RUKF
While the RUKF results in a very efﬁcient set of prediction and update equa-
tions, the error covariance equations are still very high order computationally.
This is because the error covariance of each updated estimate must be updated
with each nonupdated estimate—i.e., for (k1,k2) ∈U⊕+(n1,n2) and, most impor-
tantly, all (l1,l2) ∈S⊕+(n1,n2). To further address this problem, we approximate

8.3 Two-Dimensional Recursive Estimation
277
the RUKF by also omitting these error-covariance updates beyond a larger covari-
ance update region T⊕+(n1,n2) that is still O(M2). Experimentally, it is observed
that the resulting approximate RUKF can be both very computationally efﬁcient
as well as very close to the optimal linear MMSE estimator [7]. However, the
choice of the appropriate size for the update region U⊕+ and the covariance update
region T⊕+ is problem dependent, and so some trial and error may be neces-
sary to get near the best result. These various regions then satisfy the inclusion
relations
R⊕+(n1,n2) ⊂U⊕+(n1,n2) ⊂T⊕+(n1,n2) ⊂S⊕+(n1,n2),
where only the last region S⊕+, the global state region, is O(N2).
Steady-State RUKF
For the preceding approximations to work, we generally need a stable AR signal
model. In this case, experience has shown that the approximate reduced update
ﬁlter converges to an LSI ﬁlter as (n1,n2) ↗(∞,∞). For typical AR image
models, as few as 5–10 rows and columns are often sufﬁcient. In that case, the
gain array becomes independent of the observation position, and to an excellent
approximation, the error-covariance equations no longer have to be calculated. The
resulting LSI ﬁlter, for either RUKF or approximate RUKF, then consists of two
steps:
Prediction:
bx(n1,n2)
b
(n1,n2) =
X
(ll,l2)∈R⊕+
cl1l2bx(n1−1,n2)
a
(n1 −l1,n2 −l2),
bx(n1,n2)
b
(k1,k2) =bx(n1−1,n2)
a
(k1,k2),
for (k1,k2) ∈U⊕+(n1,n2).
Updates:
bx(n1,n2)
a
(k1,k2) =bx(n1,n2)
b
(k1,k2) + g(n1 −k1,n2 −k2)

y(n1,n2) −
X
(l1,l2)∈R⊕+
h(l1,l2)bx(n1,n2)
b
(n1 −l1,n2 −l2)

,
for (k1,k2) ∈U⊕+(n1,n2).
LSI Estimation and Restoration Examples with RUKF
We present three examples of the application of the steady-state RUKF linear recur-
sive estimator to image estimation and restoration problems. The ﬁrst example is an
estimation problem.

278
CHAPTER 8 Image Estimation and Restoration
Example 8.3–1: Image Estimation
Figure 8.3–2a shows an original image Lena, which is 256 × 256 and monochrome. In
Figure 8.3–2b is seen the same image plus white Gaussian noise added in the contrast
domain (also called density domain) of Chapter 6. Since the ﬁlters assume zero mean,
the global mean is subtracted from the noisy image prior to processing and then added
in after processing to produce the ﬁnal estimate. Figure 8.3–3 shows the steady-state
RUKF estimate based on the noisy data at SNR =10 dB in Figure 8.3–2b. The SNR of the
estimate is 14.9 dB, so that the SNR improvement (ISNR) is 4.9 dB. These results come
from [8]. We can see the visible smoothing effect of this ﬁlter.
FIGURE 8.3–2
(a) 256 × 256 Lena—original. (b) Lena plus noise at 10-dB input SNR.
FIGURE 8.3–3
RUKF estimate of Lena from 10-dB noisy data.

8.3 Two-Dimensional Recursive Estimation
279
The next two examples consider the case where the blur function h is operative
(i.e., image restoration). The ﬁrst one is for a linear 1-D blur, which can simulate lin-
ear camera or object motion. If a camera moves uniformly in the horizontal direction
for exactly M pixels during its exposure time, then an M × 1 FIR blur can result,
h(n) = 1
M [u(n) −u(n −M)].
Nonuniform motion will result in other calculable but linear blur functions.
Example 8.3–2: Image Restoration from 1-D Blur
Figure 8.3–4 shows the 256 × 256 monochrome cameraman image blurred horizontally
by a 10 × 1 FIR blur. The input blurred SNR (BSNR) = 40 dB. Again, the global mean is
ﬁrst subtracted from the noisy image prior to processing and added in after processing to
produce the ﬁnal estimate. Figure 8.3–5 shows the result of the 3-gain restoration algo-
rithm making use of RUKF. The SNR improvement is 12.5 dB. Note that while there
is considerable increase in sharpness, there is clearly some ringing evident. There is
more on this example in [8]. This example uses the inhomogeneous image model of next
Section 8.4.
The next example shows RUKF restoration performance for a simulated uni-
form area blur of size 7 × 7. Such area blurs are often used to simulate the camera’s
lack of perfect focus, or simply being out of focus.5 Also, certain types of sensor
FIGURE 8.3–4
Cameraman blurred by horizontal FIR blur of length 10. BSNR = 40 dB.
5Fourier optics theory shows that lens blur can be approximated with LSI convolution in the original
image intensity space. Since we are typically ﬁltering in a (partially unknown) gamma-compensated
space, LSI convolution is only a ﬁrst-order or linear approximation, perhaps best suited only to low-
contrast image data.

280
CHAPTER 8 Image Estimation and Restoration
FIGURE 8.3–5
Inhomogeneous Gaussian using 3-gains and residual model.
vibration can cause the information obtained at a pixel to be close to a local
average of incident light. In any case, a uniform blur is a challenging case, com-
pared to more realistic tapered blurs that are more concentrated for the same total
support.
Example 8.3–3: Image Restoration from Area Blur
This is an example of image restoration from a 7 × 7 uniform blur. The simulation was done
in ﬂoating point, and white Gaussian noise was added to the blurred cameraman image
to produce a BSNR = 40 dB. Again, the global mean is ﬁrst subtracted from the noisy
image prior to processing and added in after processing to produce the ﬁnal estimate. We
investigate the effect of boundary conditions on this restoration RUKF estimate. We use
the image model of [9]. Figure 8.3–6 shows the restoration using a circulant blur, where
we can see ringing coming from the picture boundaries where the circulant blur does not
match the assumed linear blur of the RUKF, and Figure 8.3–7 shows the restoration using
a linear blur model, where we notice much less picture-boundary induced ringing due to
the assumed linear blur model of the RUKF. The update regions for both RUKF steady-
state estimates were (−12,10,12), meaning (−west, +east, +north), a 12 × 12 update
region “northwest” of, and including the present observation, plus 10 more columns
“northeast.” For Figure 8.3–6, the ISNR was 2.6 dB, or leaving out a border of 25 pix-
els on all sides 3.8 dB. For Figure 8.3–7, the corresponding ISNR = 4.4 dB and leaving
out the 25-pixel boundary 4.4 dB. In both cases, the covariance update region was four

8.3 Two-Dimensional Recursive Estimation
281
FIGURE 8.3–6
RUKF restoration using circulant blur model from area blur.
FIGURE 8.3–7
RUKF restoration from linear area blur model.
columns wider than the update region. Interestingly, the corresponding improvement of
the DFT-implemented Wiener ﬁlter [9] was reported at 3.9 dB. We should note that the
RUKF results are not very good for small error covariance update regions. For example, in
[9] they tried (−6,2,6) for both the update and covariance update region and found only
0.6-dB improvement. But then again, 7 × 7 is a very large uniform blur support.

282
CHAPTER 8 Image Estimation and Restoration
Reduced Order Model
An alternative to the RUKF is the reduced order model (ROM) method. The ROM
approach to Kalman ﬁltering is well established [10], with applications in image pro-
cessing [11]. In that method, approximation is made to the signal (image) model to
constrain its global state to order O(M2), and then there is no need to reduce the
update. The ROM Kalman ﬁlter (ROMKF) of Angwin and Kaufman [12] has been
prized for its ready adaptation capabilities. A relation between ROMKF and RUKF
has been explained in [13].
8.4 INHOMOGENEOUS GAUSSIAN ESTIMATION
Here, we extend our AR image model to the inhomogeneous case by including a
local mean and local variance [8]. Actually, these two must be estimated from the
noisy data, but usually the noise remaining in them is rather small. So we write our
inhomogeneous Gaussian image model as
x(n1,n2) ≜xr(n1,n2) + µx(n1,n2),
where µx(n1,n2) is the space-variant mean function of x(n1,n2), and xr(n1,n2) is a
residual model given as
xr(n1,n2) =
X
(k1,k2)∈Rc−(0,0)
ck1,k2xr(n1 −k1,n2 −k2) + wr(n1,n2),
(8.4–1)
where, as usual, wr(n1,n2) is a zero-mean, white Gaussian noise with variance
function σ 2
wr(n1,n2), a space-variant function. The model noise wr describes the
uncertainty in the residual image signal model.
Now, considering the estimation case, the observation equation is
y(n1,n2) = x(n1,n2) + v(n1,n2),
which upon subtraction of the mean µy(n1,n2) becomes
yr(n1,n2) = xr(n1,n2) + v(n1,n2),
(8.4–2)
where yr(n1,n2) ≜y(n1,n2) −µy(n1,n2), using the fact that the observation noise
is assumed zero mean. We then apply the RUKF to the model and observation pair,
(8.4–1) and (8.4–2). The RUKF will give us an estimate of xr(n1,n2), and the residual
RUKF estimate of the signal then becomes
bx(n1,n2) =bxr(n1,n2) + µx(n1,n2).
In a practical application, we would have to estimate the space-variant mean
µx(n1,n2), and this can be accomplished in a simple way, using a box ﬁlter (cf.
Section 7.1) as
bµx(n1,n2) ≜
1
(2M + 1)2
X
(m1,m2)∈[−M,+M]2
y(n1 −m1,n2 −m2),

8.4 Inhomogeneous Gaussian Estimation
283
Σ
+
+
RUKF
Local
mean
y(n1, n2)
µ∧
x(n1, n2)
x ∧
r(n1, n2)
x ∧(n1,n2)
−
FIGURE 8.4–1
System diagram for inhomogeneous Gaussian estimation with RUKF.
and relying on the fact that µx = µy since the observation noise v is zero mean. We
can also estimate the local variance of the residual signal xr through an estimate of
the local variance of the residual observations yr, as
bσ 2
yr(n1,n2)≜
1
(2M + 1)2
X
(m1,m2)∈[−M,+M]2
 y(n1 −m1,n2 −m2) −bµx(n1,n2)
2
and
bσ 2
xr(n1,n2)≜max{bσ 2
yr(n1,n2) −σ 2
v,0}.
The overall system diagram is shown in Figure 8.4–1, where we see a two-channel
system, consisting of a low-frequency local mean channel and a high-frequency
residual channel. The RUKF estimator of xr can be adapted or modiﬁed by the
local spatial variance bσ 2
xr(n1,n2) on the residual channel—i.e., by the residual signal
variance estimate.
Example 8.4–1: Simple Inhomogeneous Gaussian Estimation
Here, we assume that the residual signal model is white Gaussian noise with zero mean
and ﬁxed variance σ 2
xr. Since the residual observation noise is also white Gaussian, with
variance σ 2
v, we can construct the simple Wiener ﬁlter for the residual observations as
h(n1,n2) =
σ 2
xr
σ 2xr + σ 2v
δ(n1,n2).
The overall ﬁlter then becomes
bx(n1,n2) =
σ 2
xr
σ 2xr + σ 2v
byr(n1,n2) + µx(n1,n2),
with approximate realization through box ﬁltering as
bx(n1,n2) =
bσ 2
xr
bσ 2
xr + σ 2v
byr(n1,n2) +bµx(n1,n2).

284
CHAPTER 8 Image Estimation and Restoration
This ﬁlter has been known for some time in the image processing literature, where it is
referred to often as the simplest adaptive Wiener ﬁlter, discovered by Wallis [14]. This
is essentially the wiener2 function in the Image Processing Toolbox of MATLAB
(R2010a).
Inhomogeneous Estimation with RUKF
Now for a given ﬁxed residual model (8.4–1), there is a multiplying constant A
that relates the model residual noise variance σ 2
wr to the residual signal variance
σ 2
xr. The residual model itself can be determined from a least-squares prediction on
similar, but noise-free, residual image data. We can then construct a space-variant
RUKF on these data, where the residual image model is constant except for changes
in the signal model noise variance. Instead of actually running the space-variant
RUKF error covariance equations, we simplify the problem by quantizing the esti-
mate bσ 2
xr into three representative values and then use a steady-state RUKF for each
of these three, just switching between them as the estimated model noise variance
changes [8].
Example 8.4–2: Inhomogeneous RUKF Estimate
Here, we observe the Lena image in 10-dB white Gaussian noise, but we assume
an inhomogeneous Gaussian model. The original image and input 10-dB noisy image
are the same as in Figure 8.3–2. Four estimates are then shown in Figure 8.4–2:
(a) is an LSI estimate, (b) is the simple Wallis ﬁlter estimate, (c) is the residual RUKF
estimate, and (d) is a normalized RUKF estimate that is deﬁned by [8]. We can notice that
all three adaptive or inhomogeneous estimates look better than the LSI RUKF estimate,
even though the Wallis estimate has the lowest ISNR, as seen in Table 8.4–1.
Table 8.4–1 Obtained ISNRs for Five
Filter Types
Input SNR
10 dB
3 dB
LSI RUKF
4.9
8.6
Simple Wiener (Wallis)
4.3
7.8
3-Gain residual RUKF
6.0
9.3
3-Gain normalized RUKF
5.2
9.1
An inhomogeneous RUKF restoration was given in Example 8.3–2.

8.5 Estimation in the Subband/ Wavelet Domain
285
FIGURE 8.4–2
Various inhomogeneous Gaussian estimates:
a
b
c
d

. (a) LSI, (b) Wallis ﬁlter, (c) residual
RUKF, and (d) normalized RUKF.
8.5 ESTIMATION IN THE SUBBAND/ WAVELET DOMAIN
Generalizing the preceding two-channel system, consider that ﬁrst a subband/wavelet
decomposition is performed on the input image. We would have one low-frequency
channel, similar to the local mean channel, and then many intermediate and
high-frequency subband channels. As before, we can, in a suboptimal fashion,
perform estimates of the signal content on each subband separately. Finally, we

286
CHAPTER 8 Image Estimation and Restoration
create an estimate in the spatial domain by performing the corresponding inverse
subband/wavelet transform (SWT). A basic contribution in this area was by Donoho
[15] who applied noise-thresholding to the subband/wavelet coefﬁcients. Later, Zhang
et al. [16] introduced simple adaptive Wiener ﬁltering for use in this wavelet image
denoising.
In these works, it has been found useful to employ the so-called overcomplete
subband/wavelet transform (OCSWT), which results when the decimators are omit-
ted in order to completely remove any shift-varying effects. Effectively there are four
phases computed at the ﬁrst splitting stage; this then progresses geometrically down
the subband/wavelet tree. In the reconstruction phase, an average over these phases
must be done to arrive at the overall estimate in the image domain. Both hard and
soft thresholds have been considered, the basic idea being that small coefﬁcients are
most likely due to the assumed white observation noise. If the wavelet coefﬁcient, or
subband value, is greater than a well-determined threshold, it is likely to be “signal”
rather than “noise.” The estimation of the threshold is done based on the assumed
noise statistics. While only constant thresholds are considered in [15], a spatially
adaptive thresholding is considered in [17].
Example 8.5–1: SWT Denoising
Here, we look at estimates obtained by thresholding in the SWT domain. We start with
the 256 × 256 monochrome Lena image and then add Gaussian white noise to make the
PSNR6 = 22 dB. Then we input this noisy image into a ﬁve-stage SWT using an eight-tap
orthogonal wavelet ﬁlter due to Daubechies. Calling the SWT domain image y(n1,n2), we
write the thresholding operation as
by =
(
0,
|y| < t,
y,
|y| ≥t,
where the noise-threshold value was taken as t = 40 on the 8-bit image scale [0,256].
Using this hard threshold, we obtain the image in Figure 8.5–1. We also tried the soft
threshold given as
by =
(
sgn(y)(|y| −t),
|y| < t,
y,
|y| ≥t,
with the result shown in Figure 8.5–2. Using the OCSWT and hard threshold, we obtain
Figure 8.5–3. Finally, the result of soft threshold in the OCSWT domain is shown in
Figure 8.5–4.
6PSNR stands for peak SNR and is deﬁned as 20log10(255/MSE) for 8-bit images. This widely used
objective measure of image quality is closely related to SNR, but with 255 substituted for the image
standard deviation. Note that 255 is the largest 8-bit image intensity, the values being 0 to 256 −1. For
images with higher bit depths, the peak will be different, e.g., for 10 bits the peak value changes from
255 to 210 −1 = 1023.

8.5 Estimation in the Subband/ Wavelet Domain
287
50
100
150
200
250
50
100
150
200
250
FIGURE 8.5–1
Estimate using hard threhsold in the SWT domain.
50
100
150
200
250
50
100
150
200
250
FIGURE 8.5–2
Estimate using soft threshold in the SWT domain.

288
CHAPTER 8 Image Estimation and Restoration
50
100
150
200
250
50
100
150
200
250
FIGURE 8.5–3
Hard threshold t = 40 in the OCSWT domain.
50
100
150
200
250
50
100
150
200
250
FIGURE 8.5–4
Estimate using soft threshold in the OCSWT domain.

8.6 Bayesian and Maximum a Posteriori Estimation
289
For the SWT, the choice of this threshold t = 40 resulted in output PSNR = 25 dB in
the case of hard thresholding, while the soft threshold gave 26.1 dB. For the OCSWT, we
obtained 27.5 dB for the hard threshold and 27 dB for the soft threshold. In the case of
the OCSWT, there is no inverse transform because it is overcomplete, so a least-squares
inverse was used.
Besides these simple noise-thresholding operations, we can perform the Wiener
or Kalman ﬁlter in the subband domain. An example of this appears at the end of
Section 8.7 on image and blur model identiﬁcation.
8.6 BAYESIAN AND MAXIMUM A POSTERIORI ESTIMATION
All the estimates considered thus far were Bayesian, meaning they optimized over
not only the a posteriori observations but also jointly over the a priori image model
in order to obtain an estimate that was close to minimum MSE. We mainly used
Gaussian models that resulted in linear and sometimes space-variant linear esti-
mates. In this section, we consider a nonlinear Bayesian image model that must
rely on a global iterative recursion for its solution. We use a so-called Gibbs model
for a conditionally Gaussian random ﬁeld, where the conditioning is on a lower
level (unobserved) line ﬁeld that speciﬁes the location of edge discontinuities in the
upper level (observed) Gaussian ﬁeld. The line ﬁeld is used to model edgelike lin-
ear features in the image. It permits the image model to have edges, across which
there is low correlation, as well as smooth regions of high correlation. The result-
ing estimates then tend to retain the edges that would otherwise be smoothed over
by an LSI ﬁlter. While this method can result in much higher quality estimations
and restorations, it is by far the most computationally demanding of the estima-
tors presented thus far. The Gauss-Markov and compound Gauss-Markov models
are introduced next.
Gauss-Markov Image Model
We start with the following noncausal, homogeneous Gaussian image model,
x(n1,n2) =
X
(k1,k2)∈Rc
c(k1,k2)x(n1 −k1,n2 −k2) + u(n1,n2),
(8.6–1)
where the coefﬁcient support Rc is a noncausal neighborhood region centered on 0,
and the image model noise u is Gaussian and zero mean, with correlation given as
Ru(m1,m2) =



σ 2
u,
(m1,m2) = 0,
−c(m1,m2)σ 2
u,
(m1,m2) ∈Rc,
0,
elsewhere.
(8.6–2)

290
CHAPTER 8 Image Estimation and Restoration
This image model is then Gaussian and Markov in the 2-D or spatial sense [18]. The
image model coefﬁcients ck1,k2 ≜c(k1,k2) provide an MMSE interpolation bx based
on the neighbor values in Rc,
bx(n1,n2) =
X
(k1,k2)∈Rc
ck1,k2x(n1 −k1,n2 −k2),
and the image model noise u is then the resulting interpolation error.
Note that u is not a white noise, but is colored and with a ﬁnite correlation sup-
port of small size Rc, beyond which it is uncorrelated, and because it is Gaussian,
also independent. This noncausal notion of Markov is somewhat different from the
NSHP causal Markov we have seen earlier. The noncausal Markov random ﬁeld is
deﬁned by
fx(x(n)| all other x) = fx(x(n)| x(n −k),k ∈Rc),
where Rc is a small neighborhood region centered on, but not including, 0. If the
random ﬁeld is Markov in this noncausal or spatial sense, then the best estimate
of x(n) can be obtained using only those values of x that are neighbors in the
sense of belonging to the set {x|x(n −k),k ∈Rc}. A helpful diagram illustrating
the noncausal Markov concept is Figure 8.6–1, which shows a central region G+
where the random ﬁeld x is conditionally independent of its values on the outside
region G−, given its values on a boundary region ∂G of the minimum width given
by the neighborhood region Rc. The “width” of Rc then gives the “order” of the
Markov ﬁeld. If we would want to compare this noncausal Markov concept to that
of the NSHP causal Markov random ﬁelds considered earlier, we can think of the
boundary region ∂G as being stretched out to inﬁnity in such a way that we get the
situation depicted in Figure 8.6–2a, where the boundary region ∂G then becomes just
the global state vector support in the case of the NSHP Markov model for the 2-D
Kalman ﬁlter. Another possibility is shown in Figure 8.6–2b, which denotes a vector
concept of causality wherein scanning proceeds a full line at a time. Here, the present
is the whole line x(n), in an obvious notation. In all three concepts, region G−is
called the past, boundary region ∂G is called the present, and region G+ is called
the future. While the latter two are consistent with some form of causal or sequential
processing in the estimator, the noncausal Markov, illustrated in Figure 8.6–1, is not,
and thus requires iterative processing for its estimator solution.
FIGURE 8.6–1
Illustration of dependancy regions for the noncaucal Markov ﬁeld.

8.6 Bayesian and Maximum a Posteriori Estimation
291
8
8
+
−
8
+
8
−
.
(a)
(b)
FIGURE 8.6–2
Illustration of two causal Markov concepts.
Turning to the Fourier transform of the correlation of the image model noise u,
we see that the PSD of the model noise random ﬁeld u is
Su(ω1,ω2) = σ 2
u

1 −
X
(k1,k2)∈Rc
c(k1,k2)exp−j(k1ω1 + k2ω2)


and that the PSD of the image random ﬁeld x is then given as, via application of
(8.1–3),
Sx(ω1,ω2) =
σ 2
u
h
1 −P
(k1,k2)∈Rc c(k1,k2)exp−j(k1ω1 + k2ω2)
i.
(8.6–3)
In order to write the pdf of the Markov random ﬁeld, we turn to the theory of Gibbs
distributions, as shown in [19], where it was established that the unconditional joint
pdf of a Markov random ﬁeld x can be expressed as
fx(X) = K exp−Ux(X),
(8.6–4)
where the matrix X denotes x restricted to the ﬁnite region X, Ux(X) is an energy
function deﬁned in terms of potential functions V as
Ux(X) ≜
X
cn∈Cn
Vcn(X),
(8.6–5)
where Cn denotes a clique system in the ﬁnite region X, and K is a normalizing
constant. Here, a clique is a link between x(n) and its immediate neighbors in the
neighborhood region Rc. An example of these concepts is given next.
Example 8.6–1: First-Order Clique System
Let the homogeneous random ﬁeld x be noncausal Markov with neighborhood region Rc
of the form
Rc = {(1,0),(0,1),(−1,0),(0,−1)},

292
CHAPTER 8 Image Estimation and Restoration
which
we
can
call
ﬁrst-order
noncausal
Markov.
Then
the
conditional
pdf
of
x(n) = x(n1,n2) can be given as
fx(x(n)| all other x) = fx
 x(n)
{x(n −(1,0)),x(n −(0,1)),x(n −(−1,0)),x(n −(0,−1))}

,
and the joint pdf of x over ﬁnite region X can be written in terms of energy function
(8.6–5), with potentials Vcn of the form
Vcn = x2(n)
2σ 2u
or Vcn = −c(k)x(n)x(n −k)
2σ 2u
for each k ∈Rc.
Compound Gauss-Markov Image Model
The partial difference equation model for the compound Gauss-Markov (CGM)
random ﬁeld is given as
x(n1,n2) =
X
(k1,k2)∈R
cl(n1,n2)(k1,k2)x(n1 −k1,n2 −k2) + ul(n1,n2)(n1,n2),
where l(n1,n2) is a vector of four nearest neighbors from a line ﬁeld l(n1,n2), which
originated in Geman and Geman [20]. This line ﬁeld exists on an interpixel grid and
takes on two values to indicate whether a bond is in-place or broken between two
pixels, both horizontal and vertical neighbors. An example of this concept is shown
in Figure 8.6–3, which shows a portion of a ﬁctitious image with an edge going
downwards, as modeled by a line ﬁeld. The black lines indicate broken bonds (l = 0).
The model interpolation coefﬁcients cl(n1,n2)(k1,k2) vary based on in this case
the 4 nearest neighbor line ﬁeld values, as captured in the line ﬁeld vector l(n1,n2).
The image model does not attempt to smooth over an edge if the line ﬁeld indicates
a broken bond. For the line ﬁeld potentials V, we have used the model suggested
in [20] and shown in Figure 8.6–4, where a broken bond is indicated by a line, an
in-place bond by no line, and the pixel locations with large dots. Note that only
one rotation of the indicated neighbor pattern is shown. We see that the potentials
favor (with V = 0.0) bonds being in-place, with the next most favorable situation
FIGURE 8.6–3
Example of a line ﬁeld modeling edge in a portion of a ﬁctitious image.

8.6 Bayesian and Maximum a Posteriori Estimation
293
V =0.0
V = 0.9
V = 1.8
V =1.8
V =2.7
V = 2.7
FIGURE 8.6–4
Line ﬁeld potential V values for indicated nearest neighbors (black lines indicate bond
broken).
being a horizontal or vertical edge (with V = 0.9), and so on to less often occurring
conﬁgurations.
Then the overall Gibbs probability mass function (pmf) for the line ﬁeld can be
written as
p(L) = K1 exp−Ul(L),
with
Ul(L) ≜
X
cl∈Cl
Vcl(L),
with L denoting a matrix of all line ﬁeld values for the image. The overall joint
pdf/pmf for the CGM ﬁeld over a ﬁnite region can then be written as
f(X,L) = K2 exp−(Ux(X) + Ul(L)).
We should note that L needs about twice as many points in it as X, since there is a link
to the right and below each observed pixel, except for the last row and last column.
Simulated Annealing
In the simulated annealing (SA) method, a temperature parameter T is added to the
preceding joint pdf, so that it becomes
f(X,L) = K2 exp−1
T (Ux(X) + Ul(L)).
At T = 1, we have the correct joint-mixed pdf, but as we slowly lower T ↘0, the
global maximum moves relatively higher than the set of local maxima, and this
property can be used in iteratively locating the maximum a posteriori (MAP) estimate
(bX,bL) ≜argmax
X,L f(X,L|R).
This iterative solution method, called simulated annealing (SA), is developed in
Jeng and Woods [21]; it alternates between pixel update and line ﬁeld update, as it

294
CHAPTER 8 Image Estimation and Restoration
completes passes through the received data R. At the end of each complete pass, the
temperature T is reduced a small amount in an effort to eventually freeze the process
at the joint MAP estimates (bX,bL). A key aspect of SA is determining the conditional
pdf’s for each pixel and line ﬁeld location given the observations R and all the other
pixel and line ﬁeld values. The following conditional pdf is derived in [21]:
fx(n1,n2) = K3 exp−
" x(n1,n2) −P
(k1,k2)∈R cl(n1,n2)(k1,k2)x(n1 −k1,n2 −k2)
2
2Tσ 2
u,l(n1,n2)
−
P
(k1,k2)∈Rh
 x(n1 + k1,n2 + k2) −P
(m1,m2)∈Rh h(m1,m2)x(n1 + k1 −m1,n2 + k2 −m2)
2
2Tσ 2v
#
,
and for each line ﬁeld location (n1,n2) between pixels (i1,i2) and (j1,j2)7, both
vertically and horizontally, the conditional updating pmf
pl(n1,n2) = K4 exp−
"
x2(i1,i2)
2Tσ 2
u,l(i1,i2)
−cl(j1,j2)(i1 −j1,i2 −j2)x(i1,i2)x(j1,j2)
Tσ u,l(i1,i2)σ u,l(j1,j2)
+ x2(j1,j2)
2Tσ 2
u,l(j1,j2)
+ 1
T
X
(n1,n2)∈Cl
Vcl(L)

,
where K3 and K4 are normalizing constants. These conditional distributions are sam-
pled as we go through the sweeps, meaning that at each location n, the previous
estimates of l and x are replaced by values drawn from these conditional distribu-
tions. In evaluating these conditional distributions, the latest available estimates of
the other l or x values are always used. This procedure is referred to as a Gibbs
sampler [20]. The number of iterations that must be done is a function of how fast
the temperature decreases. Theoretical proofs of convergence [20, 21] require a very
slow logarithmic decrease of temperature with the sweep number n; that is,
T = C/log(1 + n).
However, in practice a faster decrease is used. A typical number of sweeps necessary
was experimentally found to be in the 100s.
Example 8.6–2: SA for Image Estimation
In [21], we modeled the 256 × 256 monochrome Lena image by a CGM model with order
1 × 1 and added white Gaussian noise to achieve an input SNR = 10 dB. The global
mean was then subtracted before processing. For comparison purposes, we ﬁrst show the
Wiener ﬁlter result in Figure 8.6–5. There is substantial noise reduction but also visible
blurring.
7We should note that the line ﬁeld l is on the interpixel grid, with about twice the number of points as
pixels in image x, so that x(n) is not the same location as l(n).

8.6 Bayesian and Maximum a Posteriori Estimation
295
The SA result, after 200 iterations, is shown in Figure 8.6–6, where we see a much
stronger noise reduction combined with an almost strict preservation of important visible
edges.
FIGURE 8.6–5
Example of Wiener ﬁltering for the Gauss-Markov model at input SNR = 10 dB.
FIGURE 8.6–6
SA estimate for a CGM model at input SNR = 10 dB.

296
CHAPTER 8 Image Estimation and Restoration
Example 8.6–3: SA for Image Restoration
In [21], we blurred the Lena image with a 5 × 5 uniform blur function and then added a
small amount of white Gaussian noise to achieve a BSNR = 40 dB, with the result shown
in Figure 8.6–7. The global mean was then subtracted before processing. We restored this
noisy and blurred image with a Wiener ﬁlter to produce the result shown in Figure 8.6–8,
FIGURE 8.6–7
Input blurred image as BSNR = 40 dB.
FIGURE 8.6–8
Blur restoration via the Wiener ﬁlter.

8.6 Bayesian and Maximum a Posteriori Estimation
297
FIGURE 8.6–9
Blur restoration via SA.
which is considerably sharper but contains some ringing artifacts. Then we processed the
noisy blurred image with SA at 200 iterations to produce the image shown in Figure 8.6–9,
which shows a sharper result and with reduced image ringing artifacts.
A nonsymmetric half-plane (NSHP) causal CGM image model, called doubly
stochastic Gaussian (DSG), was also formulated [21], and a sequential noniterative
solution method for an approximate MMSE was developed using a so-called
M-algorithm. Experimental results are provided in [21] and also show considerable
improvement over the LSI Wiener and RUKF ﬁlters. The M-algorithms are named for
their property of following M paths of chosen directional NSHP causal image models
through the data. A relatively small number of paths sufﬁced for the examples in [21].
Extensions of Simulated Annealing
The preceding SA restoration algorithm is very adaptable to massive parallel pro-
cessing [22]. Also in addition to MAP estimates, the Gibbs sampler can be used to
generate approximate MMSE estimates via
bXMS = 1
K
K
X
k=1
Xk,
where the Xk are the result of the kth run, all done with temperature T = 1.
Example 8.6–4: Parallel SA
The following estimates were generated in a parallel implementation of SA. Figure 8.6–10a
shows the parallel MAP restoration result for the previous 5 × 5 blur with BSNR = 40 dB.

298
CHAPTER 8 Image Estimation and Restoration
FIGURE 8.6–10
Parallel SA restoration results: (a) MAP estimate. (b) MMSE estimate.
Figure 8.6–10b is the MMSE estimate computed by the Gibbs sampler with constant
temperature T = 1. Both images are the result of 200 iterations.
The SNR improvement was ISNR = 6.33 dB for the MAP estimate and ISNR = 6.85
for the MMSE estimate.
An extension to the CGM approach with improved convergence properties for
severely blurred images appears in [23]. A variation on the line ﬁeld prior is the
CGM with edge-directed prior contained in [24].
8.7 IMAGE IDENTIFICATION AND RESTORATION
Here, we consider the additional practical need in many applications to estimate the
signal model parameters, as well as the parameters of the observation, including the
blur function in the restoration case. This can be a daunting problem, with no solution
admitted in some cases. Fortunately, this identiﬁcation or model parameter estimation
problem can be solved in many practical cases. Here, we present a method of com-
bined identiﬁcation and restoration using the expectation-maximization algorithm.
Expectation-Maximization Algorithm Approach
We will use the 2-D vector / 4-D matrix notation of problem 20 in Chapter 4 and
follow the development of Lagendijk et al. [25]. First, we establish an AR signal
model in 2-D/4-D matrix/vector form as8
X = CX + W,
(8.7–1)
8The reader should note that the matrix-vector equations are the same as for a stacked vector and block
matrix. The difference is internal to the denoted matrix and vector objects.

8.7 Image Identiﬁcation and Restoration
299
and then write the observations as
Y = DX + V,
(8.7–2)
with the signal model noise W and the observation noise V both Gaussian, zero mean,
and independent of one another, with variances σ 2
w and σ 2
v, respectively. We assume
that the 4-D matrices C and D are parameterized by the ﬁlter coefﬁcients c(k1,k2) and
d(k1,k2), respectively. Each such ﬁlter is ﬁnite order and restricted to an appropriate
region, that is to say, an NSHP region of support for the model coefﬁcients c, and
typically a rectangular region centered on the origin for the blur model coefﬁcients d.
All model parameters can be conveniently written together in the vector parameter
2 ≜{c(k1,k2),d(k1,k2),σ 2
w,σ 2
v}.
To ensure uniqueness, we assume that the blurring coefﬁcients d(k1,k2) are normal-
ized to sum to one,
X
k1,k2
d(k1,k2) = 1,
(8.7–3)
which is often reasonable when working in the intensity domain, where this
represents a conservation of power, as would be approximately true with a lens.
This parameter vector is unknown and must be estimated from the noisy data
Y, and we seek the maximum-likelihood (ML) estimate [1] of this unknown but
nonrandom parameter,
b
2ML ≜argmax
2 {logfY(Y;2)},
(8.7–4)
where fY is the pdf of the noisy observation vector Y. We note that this ML estimate
is the choice of the parameter 2 that makes the given observation most probable
for the given family of densities fY, and thus seems reasonable. In fact, the ML esti-
mate of unknown parameters is much studied and used in estimation and detection
applications.
Since X and Y are jointly Gaussian, we can write
fX(X) =
s
det(I −C)2
(2π)N2 detQW
exp−1
2
n
XT(I −C)tQ−1
W (I −C)X
o
and
fY|X(Y|X) =
1
p
(2π)N2 detQV
exp−1
2
n
(Y −DX)tQ−1
V (Y −DX)
o
.
Combining (8.7–1) and (8.7–2), we have
Y = D(I −C)−1W + V,
with covariance matrix
KYY = D(I −C)−1QW(I −C)−1Dt + QV,
so the needed ML estimate of the parameter vector 2 can be expressed as
b
2ML = argmax
2

−log(det(KYY)) −YtK−1
YYY	,
(8.7–5)

300
CHAPTER 8 Image Estimation and Restoration
which is unfortunately highly nonlinear and not amenable to closed-form solution.
Further, there are usually several to many local maxima to worry about. Thus we take
an alternative approach and arrive at the expectation maximization (EM) algorithm,
an iterative method that converges to the local optimal points of this equation.
The EM method (see also Section 9.4 in [1]) talks about so-called complete and
incomplete data. In our case, the complete data are {X,Y}, and the incomplete data
are just {Y}. Given the complete data, we can easily solve for b2ML; speciﬁcally, we
would obtain c(k1,k2) and σ 2
w as the solution to a 2-D linear prediction problem,
expressed as a 2-D Normal equation (see Section 8.1). Then the parameters d(k1,k2)
and σ 2
v would be obtained via
bd(k1,k2),bσ 2
v = argmaxf(Y|X) ∼N(DX,QV),
which is easily solved via classical system theory. Note that this follows only because
the pdf of the complete data separates, i.e.,
f(X,Y) = f(X)f(Y|X),
with the image model parameters only affecting the ﬁrst factor, and the observation
parameters only affecting the second factor. The EM algorithm effectively converts
the highly nonlinear problem (8.7–5) into a sequence of problems that can be solved
as simply as if we had the complete data. In fact, it is crucial in an EM problem to
formulate it, if possible, so that the ML estimate is easy to solve given the complete
data. Fortunately, this is the case here.
EM Algorithm
Start at k = 0 with an initial guess b2
0.
Then alternate the following E-step and
M-step until convergence:
E-step; L(2; b2
(k)) ≜E[logf(X,Y;2)|Y; b2
(k)]
=
Z
logf(X,Y;2)f(X|Y; b2
(k))dX,
M-step;
b2
(k+1) = argmax
2 L(2; b2
(k)).
It is proven [26] that this algorithm will monotonically improve the likelihood of
the estimate b2
(k) and so result in a local optimum of the objective function given in
(8.7–4).
We now proceed with the calculation of L(2; b2
(k)) by noting that
f(X,Y;2) = f(Y|X)f(X;2) and
f(X|Y; b2
(k)) = f(X,Y; b2
(k))
f(Y; b2
(k))
=
1
q
(2π)N2 det b
K
(k)
EE
exp−1
2

(X −bX(k))t 
b
K
(k)
EE
−1
(X −bX(k))

,

8.7 Image Identiﬁcation and Restoration
301
where bX(k) and b
K
(k)
EE are, respectively, the conditional mean and conditional variance
matrices of X at the kth iteration. Here, we have
bX(k) ≜E[X|Y; b2
(k)] = b
K
(k)
EEDtQ−1
V Y,
with estimated error covariance
b
K
(k)
EE ≜cov[X|Y; b2
(k)] = {(I −C)tQ−1
w (I −C)t + DtQ−1
v D}−1.
Thus the spatial Wiener ﬁlter designed with the parameters b2
(k) will give us also the
likelihood function L(2; b2
(k)) to be maximized in the M-step.
Turning to the M-step, we can express L(2; b2
(k)) as the sum of terms
L(2;b2
(k)) = c −N2 log(σ 2
wσ 2
v) + logdet(I −C)2
(8.7–6)
−1
σ 2v
YT Y+ 2
σ 2v
tr(D b
KXY) −1
σ 2v
tr(D b
K
(k)
XXDt)
−1
σ 2w
tr{(I −C)b
K
(k)
XX(I −C)t},
where c is a constant and
b
K
(k)
XY ≜E[XYt|Y; b2
(k)] = bX(k)Yt
(8.7–7)
and
b
K
(k)
XX ≜E[XXt|Y; b2
(k)] = b
K
(k)
EE +bX(k)bX(k)t.
(8.7–8)
Notice that (8.7–6) separates into two parts, one of which depends on the signal
model parameters and one of which only depends on the observation model param-
eters; thus the maximization of L(2; b2
(k)) can be separated into these two distinct
parts. The ﬁrst part involving image model identiﬁcation becomes
arg
max
c(k1,k2),σ 2w

logdet(I −C)2 −N2 logσ 2
w −1
σ 2w
tr
n
(I −C)b
K
(k)
XX(I −C)to
,
and the ﬁrst term can be deleted since an NSHP causal model must satisfy
det(I −C) = 1. The resulting simpliﬁed equation is quadratic in the image model
coefﬁcients c(k1,k2) and is solved by the 2-D Normal equations. The remaining part
of (8.7–6) becomes
arg
max
d(k1,k2),σ 2v

−N2 logσ 2
v −1
σ 2v
YTY + 2
σ 2v
tr(D b
KXY) −1
σ 2v
tr(D b
K
(k)
XXDt)

,
which is quadratic in the blur coefﬁcients d(k1,k2) and thus easy to maximize for any
σ 2
v. Then the estimate of σ 2
v can be found.
If the preceding equations are circulant approximated, then due to the assumption
of constant coefﬁcients and noise variances, we can write scalar equations for each

302
CHAPTER 8 Image Estimation and Restoration
of these two identiﬁcation problems. We get from (8.7–7) and (8.7–8),
bK(k)
XX(m1,m2) = bK(k)
EE(m1,m2) + 1
N2
X
k1,k2
bx(k)(k1,k2)bx(k)(k1 −m1,k2 −m2)
(8.7–9)
bK (k)
XY (m1,m2) = 1
N2
X
k1,k2
bx(k)(k1,k2)y(k1 −m1,k2 −m2),
(8.7–10)
where we have summed over all the element pairs with shift distance (m1,m2). Then
for the image model identiﬁcation, we get the Normal equations
bK(k)
XX(m1,m2) =
X
Sc
bc(k1,k2)bK(k)
XX(m1 −k1,m2 −k2),
for all (m1,m2) ∈Sc,
bσ 2
w = bR(k)
XX(0,0) −
X
k1,k2
bc(k1,k2)bK(k)
XX(k1,k2),
where Sc is the NSHP support of the AR signal model c.
For the blur model identiﬁcation, the following equations can be derived in the
homogeneous case [25]:
bK(k)
XY(−m1,−m2) =
X
(k1,k2)∈Sd
bd(k1,k2)bK(k)
XX(m1 −k1,m2 −k2),
for all (m1,m2) ∈Sd,
bσ 2
v = 1
N2
(N−1,N−1)
X
(n1,n2)=(0,0)
y2(n1,n2) −
X
k1,k2
bd(k1,k2)bK(k)
XY(−k1,−k2),
where Sd is the support of the FIR blur impulse response d. The resulting d values
can then be normalized via (8.7–3).
EM Method in the Subband/Wavelet Domain
In [27], the preceding joint identiﬁcation and restoration method was extended to
work in the subband/wavelet domain. In this case, we can have separate image mod-
els in each of the subbands, where we can employ a local power level to modulate
the signal power in the various subbands (equivalently wavelet coefﬁcients). This
can result in much improved clarity in the restored image, as illustrated in the set
of images in 8.7–1. We note that the linear space-variant (LSV) restoration of the
individual subbands works best from a visual standpoint. Its PSNR improvement is
7.3 dB while the fullband restoration only achieved 5.8-dB improvement.
Gaussian Scale Mixtures
Using an overcomplete subband/wavelet decomposition known as the steerable
pyramid [28] the authors in [29] model the individual subbands as the product

8.7 Image Identiﬁcation and Restoration
303
FIGURE 8.7–1
Subband EM restoration of cameraman at BSNR = 40 dB,
a
b
c
d
e
f

: (a) original,
(b) blurred, (c) fullband restored, (d) LL subband restored, (e) subband (LSI), and (f)
subband (LSV). (from [27] c⃝1994 IEEE)
of a Gauss-Markov random ﬁeld u(n1,n2) and an independent positive random
variable z as
x(n1,n2) = √zu(n1,n2),
called a Gaussian scale mixture (GSM). One can compute the pdf of this ﬁeld on a
ﬁnite rectangular region as follows. Calling the 2-D vector of the x values X, we have
fX(X) =
Z
fU
 1
√zX

fZ(z)dz
=
Z
1
(2πz)N2/2 |KU|1/2 exp
 
−Xt (zKU)−1 X
2
!
fZ(z)dz
=
Z
1
(2πz)N2/2 |KX|1/2 exp
 
−Xt (zKX)−1 X
2
!
fZ(z)dz.
since we take E[Z] = 1, then x and u have the same covariance 4-D matrix, i.e.,
KU = KX. In [29], a separate (independent) GSM model was used for each subband
scale. The motivation comes from the fact that the individual subband values tend to
follow non-Gaussian kurtotic densities (i.e., those with sharper peaks and longer tails
than Gaussian). In fact, subband/wavelet coefﬁcients (values), DCT coefﬁcients, and
prediction errors have been found by many researchers to better ﬁt Laplacian than
Gaussian density models.

304
CHAPTER 8 Image Estimation and Restoration
It turns out that this same equation had been suggested earlier, taking u to be
exponentially distributed, as a multivariate Laplace distribution [30]—that is to say,
as a multivariate generalization of the Laplace distribution. However, other distribu-
tions have been mentioned for z in [29], including the log Normal density (i.e., logz
Gaussian). In that case, X does not follow a multivariate Laplacian model, but it still
can be a useful ﬁt.
The GSM model is generalized in [31] to the ﬁeld of Gaussian scale mixtures
(FoGSM) model, which models each subband independently with the model
x(n1,n2) =
p
z(n1,n2)u(n1,n2),
where u and logz are now independent homogeneous Gaussian random ﬁelds. The
joint pdf on a ﬁnite square region can then be written in terms of matrix X as
fX(X) =
Z
fU(X/
√
Z)fZ(Z)dZ,
where both the indicated (vector) divisions X/
√
Z and logZ deﬁned to be computed
termwise. The density fZ is given as
fZ(Z) =
1
(2π)N2/2 
5N2
i=1zi
KlogZ
1/2 exp
 
−(logZ)t  KlogZ
−1 (logZ)
2
!
,
zi > 0 for all i,
where the argument of the exp term can be written in scalar form as
−1
2
X
(i,j)∈[1,N]2
logz(i,j)
X
(k,l)∈[1,N]2
clogZ(k −i,l −j)logz(k,l),
due to its Gauss-Markov structure, and can be implemented via 2-D convolu-
tion. Here, the coefﬁcients are the so-called Markov-generating kernel values or
neighborhood sets of the ﬁeld logz(n1,n2) (see (8.6–2)). The density fU is given as
fU(X/
√
Z) =
1
(2π)N2/2 
5N2
i=1zi

|KU|1/2 exp

−

X/
√
Z
t
(KU)−1 
X/
√
Z

2

,
with argument of the exp term given as
X
(i,j)∈[1,N]2
x(i,j)/
p
z(i,j)
X
(k,l)∈[1,N]2
cU(k −i,l −j) x(k,l)/
p
z(k,l),
in terms of the generating kernel values cu(k,l) of homogeneous Gaussian random
ﬁeld u(n1,n2).

8.7 Image Identiﬁcation and Restoration
305
Pairwise densities of subband/wavelet values and coefﬁcients were experimen-
tally found [31] to have a good match with Gauss-Markov members of this family
using small neighborhood sets of 5 × 5 support; one for the random ﬁeld u(n1,n2) and
one for logz(n1,n2). A MAP estimation method was then developed to ﬁnd estimates
(bX,bZ) = argmax
X,Z f(X,Z|Y)
= argmax
X,Z logf(X,Z|Y)
for observations on the square lattice [1,N]2 of the form Y = X + V, with V being an
independent Gaussian white noise of variance σ 2. The method is an iterative one that
alternates between an estimate of X assuming Z = bZ and an estimate of Z under the
assumption that X = bX and initialized at the GSM estimate of [29].
Example 8.7–1: FoGSM Estimate
White Gaussian noise was again added to the 256 × 256 Lena image to produce an SNR =
10 dB, shown in Figure 8.7–2. The global mean was then subtracted before processing.
Figure 8.7–3 shows the FoGSM estimate obtained by the technique of Lyu and Simoncelli
[31] with an SNR = 17.9 dB for an overall SNR improvement of ISNR = 7.9 dB.
This estimate is performed using the iterative algorithm of [31], which is implemented
in the 2-D frequency domain using 2-D FFTs via a block circulant approximation of the
matrices involved. The 5 × 5 neighborhood coefﬁcient arrays were jointly identiﬁed as part
of the iterations. (Simulation results were generously provided by Professor Siwei Lyu.)
FIGURE 8.7–2
Lena image with Gaussian noise added to yield input SNR = 10 dB.

306
CHAPTER 8 Image Estimation and Restoration
FIGURE 8.7–3
FoGSM estimate with ISNR = 7.9 dB.
8.8 NON-BAYESIAN METHODS
Here we look at some non-Bayesian methods including nonlocal means, least-
squares, and total variation approaches.
Nonlocal Means
The simplest image estimator we have considered was the 3 × 3 box ﬁlter of
Chapter 7. It shares one commonality with the much more advanced algorithms we
have seen since, and that is its emphasis on using only nearby values to smooth each
noisy pixel. A departure from such local estimators is to look for small blocks of
noisy pixels in a larger search area that have a good match to a block centered at
the current pixel, and then to average these blocks to provide the smoothed estimate.
Such an approach is introduced in Buades et al. [32]. Again we take Y = X + V, with
V and independent white Gaussian noise.
The nonlocal means estimate of [32] is formed as
bx(n) =
M
X
i=1
w(n,ki)y(ki),
where the ki are the centers of M square windows in a rather large search region S
surrounding n, and the weights are computed via the normalized Gaussian kernel
w(n,k) ∝exp
 
−∥v(n) −v(k)∥2
2
h2
!
,

8.8 Non-Bayesian Methods
307
where the v(k) are vectors of pixels in a small square window centered on pixel k and
the L2 or square norm is used. The parameter h is chosen to control the weighting so
that only pixel values in the center of quite similar windows will receive high weights.
The normalization of weights is such that the M weights w(n,ki) sum to one.
Example 8.8–1: Nonlocal Means Approach
Here is an example of the nonlocal means approach of [32] where the Lena image is
shown at a Gaussian white noise level of σ = 20. The search window is of size 21 × 21
pixels so M = 441. The square neighborhoods are 7 × 7. The value of h was set at
h = 10σ. The results are shown in Figure 8.8–1. The MSE was measured at 68. The input
MSE = σ 2 = 400, so that the ISNR = 10log(400/68) = 7.7 dB. Left is a portion of the
noisy image, and right is a portion of the nonlocal (NL) image estimate. Note that the esti-
mate is very clean and without any ringing distortion due to the fact that no spatial ﬁltering
is done.
Dabov et al. [33] expand on the nonlocal processing idea by introducing ﬁl-
tering on the 3-D blocks obtained by stacking the matched patches. They call
this ﬁltering collaborative ﬁltering and provide results for the combination of var-
ious 3-D separable transforms together with so-called wavelet shrinkage, which
is the same as scalar Wiener ﬁltering on the 3-D transformed samples (see
transform-domain Wiener ﬁltering in Section 8.2). A two-step estimate is per-
formed, where in the ﬁrst step only hard thresholding of the 3-D transform
coefﬁcients is performed. In the second (ﬁnal) step, Wiener ﬁltering is done.
Since the blocks partially overlap, some combining of estimates is needed. This
is termed aggregation and is performed at the end of each step, right after the
3-D ﬁltering. They call the overall algorithm block-matching and 3-D ﬁltering
(BM3D).
FIGURE 8.8–1
Nonlocal means estimator: (left) noisy segment, (right) NL estimate segment. (from
Figure 5 of [32] c⃝2005 IEEE)

308
CHAPTER 8 Image Estimation and Restoration
FIGURE 8.8–2
Collaborative ﬁltering on the 256 × 256 house image: (left) noisy original, (right) estimate.
(from Dabov et al. [33] c⃝2007 IEEE)
Example 8.8–2: BM3D estimate
The house image was contaminated with additive white Gaussian noise to a noise level
σ = 25 to produce the noisy image shown on the left in Figure 8.8–2. The resulting BM3D
estimate shown on the right. The output PSNR was given as 32.86 dB.
Least Squares and Total Variation
The general image restoration problem we have considered is of the 4-D matrix form
Y = HX + V,
(8.8–1)
where we typically assume that the noise V is independent of the signal X, and we
have an assigned distribution for the image X. For example, if the blur function is an
LSI convolution, then
Hi,j;k,l = h(i −k,j −l).
Our goal has been to form an estimate bX (linear or not) that either minimizes the MSE
or maximizes the a posteriori probability density
bX = argmax
X fX|Y(X|Y).
(8.8–2)
Now by Bayes’ theorem, we have
fX|Y(X|Y) = fY|X(Y|X)fX(X)/fY(Y)
∼fV(Y −HX)fX(X),

8.8 Non-Bayesian Methods
309
since the observation Y is ﬁxed and known, where ∼means “is proportional to.” If
we now assume that both V and X are Gaussian i.i.d. then we get
log fX|Y(X|Y) ∼log fV(Y −HX) + logfX(X),
so that, assuming the means are subtracted or equivalently µX = µY = 0, ﬁnding the
argmaxX in (8.8–2) is then equivalent to
argmin
X
n
σ −2
v ∥Y −HX∥2
2 + σ −2
X λ∥X∥2
2
o
(8.8–3)
for some positive number λ, where the indicated norm is L2, given as ∥A∥2
2 ≜
P
n1,n2 a2(n1,n2).
On the other hand, a deterministic approach to image restoration given the obser-
vations (8.8–1) might well seek to ﬁnd the image that minimizes the L2 norm or
“energy” of the signal (image) given some constraint on ∥Y −HX∥2
2,
argmin
X
n
∥X∥2
2 + λ∥Y −HX∥2
2
o
.
(8.8–4)
Effectively, we would look for the “smallest” signal with a close distance to the
observations, both distances being measured by the L2 norm. Taking the matrix
derivative9 of the function in (8.8–4) with respect to the 2-D vector X, with g ≜
∥X∥2
2 + λ∥Y −HX∥2
2, we get
∇Xg = ∇X

∥X∥2
2 + λ∥Y −HX∥2
2

= ∇X

XtX + λ(Y −HX)t(Y −HX)

= 2

X −λ
 HtY + HtHX

.
Then setting the result to zero, and assuming invertability, we get the least-squares
estimate bXLS given as
bXLS =
 I + λHtH
−1 HtY.
Constrained Least Squares
A more interesting constraint would be one on the variation of X. In terms of the
Laplacian operator L, we may want to constrain the L2 norm of LX in place of
the constraint on the energy ∥X∥2
2. Doing so will yield the constrained least-squares
problem
∥LX∥2
2 + λ∥Y −HX∥2
2 ,
9The matrix derivative of a 4-D matrix proceeds the same as for conventional matrix derivatives; that
is, the derivative of AX with respect to X is At, and the derivative of XtAX is AX+AtX.

310
CHAPTER 8 Image Estimation and Restoration
whose minimization yields the estimate bXCLS given as
bXCLS =
 LtL + λHtH
−1 HtY.
This method was introduced by Hunt [34], who considered a radially symmetric
version of the discrete second difference operator
0
−1
0
−1
4
−1
0
−1
0
,
called the Laplacian ﬁlter in Chapter 7. Examples are provided in [34].
The reader may have wondered how to choose the Lagrangian parameter λ. Var-
ious estimates of suitable λ values have been obtained by statistical considerations,
but most often the value of λ is set by trial and error of the experimenter to obtain
best performance.
Total Variation Method
More generally, we could have exponential distributions for the noise such that the
image pdf f is proportional to the exponential of the negative of an L1 norm as
f ∼exp(−∥LX∥1),
which would give the deterministic minimization problem
∥Y −HX∥2
2 + λ∥LX∥1.
(8.8–5)
A motivation for using this method is as follows. We have seen that the L2 norm
gives rise to excessive smoothing at edges, which after all are large excursions of the
function from its mean. Thus the L1 norm should provide less smoothing of edges, the
main criticism of linear ﬁltering of images. We could also consider the i.i.d. additive
noise to be Laplacian rather than Gaussian; then the total variation problem becomes
ﬁnding the value of X that minimizes
∥Y −HX∥1 + λ∥LX∥1.
The method is called total variation because in the 1-D case, the total variation of a
function is written as P
n |f(n) −f(n −1)| = ∥1f∥. Unfortunately, the presence of an
L1 norm prevents an analytical solution, so recourse is made to numerical methods,
and steepest descent has been used successfully. The derivatives of these quantities
are readily calculated and presented in Farsiu et al. [35].
Example 8.8–3: Total Variation Estimate
We look at estimation (denoising) for the Lena 256 × 256 grayscale image using a total
variation (TV) method based on (8.8–5) with no blurring (i.e., H = I), the identity matrix.

8.9 Image Superresolution
311
FIGURE 8.8–3
The 256 × 256 Lena 10-dB noisy image on the left produced the TV-denoised output image
on the right, ISNR = 6.1 dB.
We use the freely available package tvreg from Pascal Getreuer available for download
[36]. White Gaussian noise was added to produce an input SNR = 10 dB on the noisy
input image. Three values of λ were tried, 20, 25, and 30, with 25 producing the best
MSE result, which corresponded to an output SNR = 16.1 dB. Thus the SNR improve-
ment ISNR = 6.1 dB. Figure 8.8–3 shows both the noisy input image on the left and the
tvdenoise output on the right.
We see considerable noise reduction without the oversmoothing that generally occurs
with the L2 norm. On the other hand, the ISNR is not the best. While the algorithm runs
very fast, even in MATLAB, iteration is required to ﬁnd the best λ value.
8.9 IMAGE SUPERRESOLUTION
When additional versions of an image are available, we have an image sequence. If
these frames are spatially aliased, then they may provide new unknown samples of
the original image. Superresolution is a technique that combines (fuses) these aliased
images to get a high-resolution (HR) output image from the input low-resolution (LR)
frames. One can think of this as a type of restoration beyond the sampling limit of
the individual frames, that is only permitted when multiple samplings of the data are
present. So some slight camera and/or object motion between frames is crucial to the
success of this method.

312
CHAPTER 8 Image Estimation and Restoration
The ﬁrst such result is due to Tsai and Huang [37] using a frequency-domain
approach via fast Fourier transforms and was restricted to global translational
motion between the frames. Many papers have appeared on superresolution since
this ﬁrst one. This ﬁrst paper did not use the word superresolution (SR), but
rather referred to image restoration and registration, which are constituent steps
of SR.10
In matrix terms, the SR problem can be formulated, following Elad and Hel-Or
[39], as
Y(n) = DnHnFnX + V(n),
n = 1,...,N,
where N is the number of images of X available and the V(n) is a white Gaussian
noise, independent from frame to frame. The 4-D matrices DnHnFn reﬂect the
deterministic distortion between images and are respectively decimation, blurring,
and shift, or more generally warp. Taking the case where the decimation and blur are
constant (i.e., Dn = D and Hn = H for all n), the maximum-likelihood11 estimate
can then be written as
bX = argmin
X
( N
X
n=1
[Y(n) −DHFnX]t [Y(n) −DHFnX]/σ 2
V
)
,
(8.9–1)
where σ 2 is the variance of the added noise, assumed constant over the image
sequence Y(n). In practice, the shift operator Fn has to be estimated from the LR
image frames. Such is properly the topic of motion estimation to be covered in
Chapter 11 on video processing. For now we just assume that this shift operator is
known.
Taking a matrix derivative and setting to zero, we obtain
RbX = P,
(8.9–2)
where R ≜PN
n=1 Ft
nHtDtDHFn/σ 2 and P ≜PN
n=1 Ft
nHtDtY(n)/σ 2. Noticing
that we can cancel out the σ 2 term, and applying a steepest descent algorithm to ﬁnd
10There is a new type of SR that tries to work within a single image to ﬁnd missing HR data. It does
this by a combination of template matching and scale-space (or SWT) analysis [38]. We do not discuss
this method here.
11Maximum likelihood estimates [1] treat the quantity to be estimated X as unknown but not random.
They then try to maximize the pdf f(data|X) over all possible values of the unknown quantity X. In
estimation theory, this density is called the a posteriori density.

8.9 Image Superresolution
313
the solution, we obtain the iteration
bXl+1 = bXl + µ

P−RbXl

,
l = 1,2,...
= bXl + µ
N
X
n=1
Ft
nHtDt[Y(n) −DHFnbXl]
= bXl + µHt
N
X
n=1
Ft
nDt[Y(n) −DFnHbXl],
(8.9–3)
if we assume that Ft
n and Ht commute (i.e., Ft
nHt = HtFt
n), which is possible if
the 4-D matrices are circulant. In practice this is approximated by various techniques
such as windowing the input image frames.
Now if we deﬁne bZ ≜HbX and multiply (8.9–3) by H, we get
bZl+1 = bZl + µHHt
N
X
n=1
Ft
nDt[Y(n) −DFnb
Zl],
l = 1,2,...,
(8.9–4)
thus separating the image SR problem into two separable parts. First, perform itera-
tion (8.9–4) to fuse the images into one at the high resolution (usually only a factor
of two to four higher in each dimension), then perform a second step to allow image
restoration on the noisy image bZ∞to get a ﬁnal HR estimate bX based on the relation
Z = HX. In principle, this second step can be done by a conventional image restora-
tion method, such as the ones discussed earlier in this chapter. For more details on
this approach to SR, see [39].
In practice, one may not know the exact values of the shifts or displacements
between the input images. In this case, one has to use a motion estimation method
to obtain these shifts with respect to one of them, a chosen target image. Basically,
this will involve some kind of template matching, as discussed in Chapter 7. More
detailed analysis of motion estimation, mainly for the video application, comes in
Chapter 11.
Example 8.9–1: Image SR
This example is from Elad and Hel-Or [39] and consists of 16 images taken by a digital
camera of size 300 × 360 pixels. The resolution was then increased by a factor of two
in each dimension. The point spread function (psf) was assumed to be a Gaussian blur
with operator-selected variance. Results are shown in Figure 8.9–1, with a close-up in
Figure 8.9–2. Notice the increased sharpness in Figure 8.9–1, and notice the reduction
in aliasing of the vertical lines in the close-up in Figure 8.9–2.

314
CHAPTER 8 Image Estimation and Restoration
FIGURE 8.9–1
Results of image SR from Elad and Hel-Or [39, Figure 3]. (copyright IEEE, 2001)
FIGURE 8.9–2
Close-up of Figure 8.9–1.
The success of SR depends on the accuracy of the registration between the LR
frames. While aliasing is necessary to get the extra resolution (i.e., missing high-
frequency content), the presence of aliasing makes the registration problem more
difﬁcult. See Vandewalle et al. [40] for an accurate method of measuring the global
shift and rotation between aliased images.
8.10 COLOR IMAGE PROCESSING
Since color images are simply multicomponent or vector images, usually of the three
components, red, green, and blue, one could apply all the methods of this chapter

Conclusions
315
to each component separately. Alternatively, one can develop vector versions of the
estimation methods for application to the vector images. Usually, though, color image
processing is conducted in the Y′CRCB domain, by just processing the luma com-
ponent Y′. For example, in color image deblurring, often just the luma image is
restored, while the chroma components are left alone, owing to the lower chroma
bandwidth of the HVS and lesser sensitivity to error. Regarding blur function esti-
mation, often this is determined from the luminance data alone, and then the same
blur function is assumed for the chrominance channels. Such relatively simple color
processing, however, has been criticized as leading to color shifts that can some-
times leave unnatural colors in the restored images. Vector restoration procedures
have been especially developed to deal with this problem. A special issue of Signal
Processing Magazine [41] deals extensively with the overall topic of processing color
images.
Of course, any color space can be used for image processing. Recently so-called
.raw data has been made available as digital camera output. These data are direct
from the sensors without any gamma correction. Also, in most cases they have not
been demosaiced yet. In that case, there is the opportunity to perform restoration
directly on the higher bitdepth sensor data.
CONCLUSIONS
This chapter introduced the problem of estimation and restoration for spatial sys-
tems. We presented both nonrecursive and recursive approaches for 2-D estimation.
These methods extend the well-known 1-D discrete-time Wiener ﬁlter and pre-
dictor to the 2-D or spatial case. We considered recursive estimation, reviewed
the 1-D Kalman ﬁlter, and then extended it to the 2-D case, where we saw that
the state vector spread out to the width of the signal ﬁeld and was therefore not
as efﬁcient as the 1-D Kalman ﬁlter for the chosen AR model. We then pre-
sented a reduced update version of 2-D recursive linear estimation that regains
some of the lost properties. We also looked at space-variant extensions of these
LSI models for adaptive Wiener and Kalman ﬁltering and provided some exam-
ples. Then we considered estimation in the subband/wavelet domain as an extension
of a simple adaptive method involving local mean and variance functions. We
looked at so-called Bayesian methods using compound Gauss-Markov models and
simulated annealing in their estimator solution. We introduced the problem of
parameter estimation combined with image estimation and restoration and looked
at how this EM technique can be carried to the SWT observations. We looked
at some non-Bayesian approaches from the current literature. We considered the
related problem of image superresolution, applicable to a situation where multiple
aliased LR images of a scene are available and an HR result is desired. Finally,
we brieﬂy considered some issues in extending our restoration techniques to color
images.

316
CHAPTER 8 Image Estimation and Restoration
PROBLEMS
1. Carry out the indicated convolutions in (8.1–1) to obtain the covariance function
of white noise of variance σ 2
w having been ﬁltered with the impulse response
g(n1,n2). Using the notation g−(n1,n2) ≜g(−n1,−n2), express your answer in
terms of σ 2
w and a direct convolution of g and g∗
−.
2. Extend the method in Example 8.1–3 to solve for the (1,1)-order NSHP
predictor, where the coefﬁcient support of the 2-D linear predictor is given as
Ra −(0,0) ≜{(1,0),(−1,1),(0,1),(1,1)}.
Write the 4 × 4 matrix Normal equation for this case in terms of a homogeneous
random ﬁeld correlation function Rx(m1,m2).
3. This problem concerns a least-squares version of 2-D linear prediction. The
difference here is that we consider the signal x(n1,n2) as nonrandom. Then
instead of minimizing the mean-square error, we minimize the deterministic
least-squares error
E ≜
N1−1
X
n1=1
N2−1
X
n2=1
(x(n1,n2) −bx(n1,n2))2 ,
where supp{x} = [0,N1 −1] × [0,N2 −1], and the 2-D linear prediction bx is
given as
bx(n1,n2) ≜a1,0x(n1 −1,n2) + a0,1x(n1,n2 −1).
Notice the limits of summation in E that ensure we only reference existing data
(i.e., nonzero values of the signal x).
(a) Find the 2 × 2 matrix-vector equation that the coefﬁcients must satisfy.
Deﬁne the column vector a = [a1,0,a0,1]T.
(b) Solve the resulting equation for the predictor coefﬁcients a1,0 and a0,1.
4. Write the equations that specify a 5 × 5-tap FIR Wiener ﬁlter h with support on
[−2,+2]2 for the signal correlation function Rx(m1,m2) = σ 2
xρ|m1|+|m2|, where
σ 2
x is a given positive value and ρ satisﬁes |ρ| < 1. Assume the observation
equation is speciﬁed as
y(n1,n2) = 5x(n1,n2) + 3w(n1,n2),
where w is a white noise of variance σ 2
w and w ⊥x.
5. Reconsider problem 4, but now let the Wiener ﬁlter h have inﬁnite support and
actually write the solution in the frequency domain—i.e., ﬁnd H(ω1,ω2).
6. To determine a general causal Wiener ﬁlter, we need to ﬁrst ﬁnd the spectral
factors of the noisy observations Sy(ω1,ω2), in the sense of Theorem 8.2–1.
Show that the power spectral density
Sy(ω1,ω2) =
1
1.65 + 1.6cosω1 + 0.2cosω2 + 0.16cos(ω1 −ω2)

Problems
317
has stable and QP causal spectral factor
B++(z1,z2) =
1
1 + 0.8z−1
1
+ 0.1z−1
2
.
What is B−−(z1,z2) here? What is σ 2?
7. The 2-D Wiener ﬁlter has been designed using the orthogonality principle of
linear estimation theory. Upon setting
bx(n1,n2) ≜
X
(k1,k2)∈Rh
hk1,k2y(n1 −k1,n2 −k2),
we found in the homogeneous, zero-mean case, that h is determined as the
solution to the Normal equations
X
(l1,l2)∈Rh
hl1,l2Ryy(k1 −l1,k2 −l2) = Rxy(k1,k2) for (k1,k2) ∈Rh.
By ordering the elements (k1,k2) ∈Rh onto vectors, this equation can be put into
matrix-vector form and then solved for vector h in terms of correlation matrix
Ryy and cross-correlation vector rxy, as determined by the chosen element order.
In this problem, our interest is in determining the resultant mean-square value
E[|e(n1,n2)|2] of the estimation error e(n1,n2) =bx(n1,n2) −x(n1,n2).
(a) First, using the orthogonality principle, show that
E[|e(n1,n2)|2] = −E[x(n1,n2)e∗(n1,n2)]
= E[|x(n1,n2)|2] −E[x(n1,n2)bx∗(n1,n2)]
= Rxx(0,0) −
X
(k1,k2)∈Rh
h∗
k1,k2Rxy(k1,k2).
(b) Then show that the solution to the Normal equations and the result of part
(a) can be combined and written in matrix-vector form as
σ 2
e = σ 2
x −r∗T
xy R−1
yy rxy.
Note that E[|e(n1,n2)|2] = σ 2
e since we assume the means of x and y are
zero.
8. In the 2-D Kalman ﬁlter, what is the motivation for omitting updates that are
far from the observations? How does the RUKF differ from the approximate
RUKF? For constant coefﬁcient AR signal models and observation models, we
experimentally ﬁnd that a steady state is reached fairly rapidly as we process
into the image. What role does model stability play in this?

318
CHAPTER 8 Image Estimation and Restoration
9. Write out the steady-state RUKF equations for a 1 × 1-order ⊕+ model and
update region
U⊕+(n1,n2) = {(n1,n2),(n1 −1,n2),(n1 −2,n2),
(n1 + 2,n2 −1),(n1 + 1,n2 −1),(n1,n2 −1),(n1 −1,n2 −1),
(n1 −2,n2 −1)}.
Write these equations in terms of model coefﬁcients {c10,c01,c11,c−1,1} and
assumed steady-state gain values
{g(0,0),g(1,0),g(2,0),g(−2,1),g(−1,1),g(0,1),g(1,1),g(2,1)}.
10. With reference to problem 9, list the various estimates provided by the steady-
state RUKF in terms of their increasing 2-D delay. For example, the ﬁrst estimate
is the prediction estimatebx(n1,n2)
b
(n1,n2) with a delay of (−1,0) (i.e., an advance
of one pixel horizontally).
11. Continuing along the lines of problem 1 of Chapter 7, show how to efﬁciently
calculate the local variance used in Section 8.4 by extending the box ﬁlter con-
cept. Assume the box is 2M + 1 × 2M + 1. How many multiplies per pixel?
Adds per pixel? How much intermediate storage is needed?
12. Show that the Markov random ﬁeld satisfying the noncausal 2-D difference
equation (8.6–1), with random input satisfying (8.6–2), has the PSD (8.6–3).
13. Derive the scalar (8.7–9) and (8.7–10) from the 4-D matrix equations (8.7–7)
and (8.7–8). Make use of the fact that, in the homogeneous case,

b
K
(k)
XX

n1,n2;n1−k1,n2−k2
=

b
K
(k)
XX

0,0;−k1,−k2
,
for all (n1,n2).
14. Using moment-generating functions and characteristic functions, show that the
scalar Laplace random X variable can be written as the product of the square
root of an exponential random variable Z and an independent Gaussian random
variable U. Write the corresponding equation for probability densities.
15. Download the tvreg package [36] and repeat Example 8.8–3 for the range of
λ : 16–30 at intervals of two, and plot your results in terms of ISNR. Which
estimated image looks best to you?
16. Show that (8.9–2) results from taking the 2-D vector derivative12 of the function
in the argmax in (8.9–1) with regard to the 2-D vector X and setting it to the zero
2-D vector zero.
12This derivative is simply the 2-D vector composed of the partial derivatives of the function within
the argmax in (8.9–1) with regard to each of the variables X(n1,n2).

Appendix: Random Processes
319
APPENDIX: RANDOM PROCESSES
Summary of Random Sequences and Processes
A random process is an extension of the concept of random variable as studied in a
course on probability [1]. Basically, and generalizing the concept of random variable,
the theory of random processes treats random 1-D functions. Usually they are ran-
dom functions of time, but equally well this parameter could be space, such as along
the scan line of an image or the acoustic signal from a line of sensors in geophysical
exploration. In the discrete-time (discrete-parameter) case, we call this set of func-
tions a random sequence. We can equally well think of this random sequence as an
inﬁnite set of random variables together with a time index n.
We start this section with a short review of basic probability. Then we summa-
rize some important concepts for random sequences and random processes, whose
knowledge is needed for both this and later chapters of this book.
Probability Review
A probability space is composed of a set of outcomes zeta ζ of an experiment. The
set of all outcomes is called the sample space . Subsets of the space  are called
events. A ﬁeld of events F is then deﬁned to include all the important subsets of
this sample space of outcomes . Finally, a probability measure P[A] is deﬁned for
each event A in the ﬁeld F. The resulting probability space is denoted (,F,P).
Every ﬁeld of events must contain the null event φ and the sure event . Also, for
mathematical completeness, it must contain all countable intersections and unions of
its sets (events).
The probability measure P must satisfy three axioms:
1. P[A] ≥0 for all events A ∈F.
2. P[] = 1.
3. P[A ∪B] = P[A] + P[B], for all events A ∩B = φ, called the null event.
Now a random variable X is simply a function deﬁned on the sample space X(ζ)
that satisﬁes certain regularity properties:
1. The set {ζ|X(ζ) ≤x} must be in the ﬁeld of events F for every −∞< x < +∞.
2. P[{ζ|X(ζ) = −∞}] = P[{ζ|X(ζ) = +∞}] = 0.
This random variable maps the events to the real line or the complex plane,
thereby giving a numerical value for each outcome of the experiment.
For each random variable X deﬁned on a probability space (,F,P), we can
construct a cumulative distribution function (CDF)
FX(x) ≜P[{ζ|X(ζ) ≤x}].
Thus the distribution function FX(x) is just the probability of the event {ζ|X(ζ) ≤x}.
For notational simplicity, we generally write such events as just {X ≤x}, and their

320
CHAPTER 8 Image Estimation and Restoration
corresponding probability as
FX(x) = P[X ≤x].
A valid distribution function must then have the following properties:
1. FX(−∞) = 0 and FX(+∞) = 1.
2. For all x1 ≤x2, we have FX(x1) ≤FX(x2); that is, the distribution function is
monotonic nondecreasing.
3. For the event {x1 < X ≤x2}, we have the probability
P[{x1 < X ≤x2}] = P[x1 < X ≤x2] = FX(x2) −FX(x1).
If the distribution function is differentiable, then we can deﬁne a probability
density function (pdf) as
fX(x) ≜dFX(x)
dx
,
with the following properties:
1. fX(x) ≥0.
2.
R +∞
−∞fX(x)dx = 1.
3. FX(x) =
R x
−∞fX(η)dη.
4. FX(x2) −FX(x1) =
R x2
x1 fX(η)dη, for all x2 ≥x1.
In basic probability, we generally study one or a few random variables and also
functions of these random variables. For two random variables X and Y, their joint
distribution function is given as
FX,Y(x,y) ≜P[{ζ|X(ζ) ≤x} ∩{ζ|X(ζ) ≤x}],
or with short notation,
FX,Y(x,y) = P[{X ≤x,Y ≤y}] = P[X ≤x,Y ≤y],
with the following properties:
1. FX,Y(−∞,−∞) = FX,Y(−∞,+∞) = FX,Y(+∞,−∞) = 0
and FX(+∞,+∞) = 1.
2. For all x1 ≤x2 and y1 ≤y2, we have FX,Y(x1,y1) ≤FX,Y(x2,y2).
3. For the event {x1 < X ≤x2,y1 < Y ≤y2}, we have the probability
P[{x1 < X ≤x2,y1 < Y ≤y2}] = P[x1 < X ≤x2,y1 < Y ≤y2]
= FX,Y(x2,y2) −FX,Y(x1,y2) −FX,Y(x2,y1) + FX,Y(x1,y1),
which, of course, must always be nonnegative for any valid joint distribution
function.
Note that condition 3 is more than simple monotonicity of the marginal distribu-
tions FX and FY. It can be seen as a kind of 2-D monotonic nondecreasing condition.

Appendix: Random Processes
321
Correspondingly, the joint pdf, when it exists, is given as
fX,Y(x,y) ≜∂FX,Y(x,y)
∂x∂y
,
with the following properties:
1. fX,Y(x,y) ≥0.
2.
R +∞
−∞
R +∞
−∞fX(x,y)dxdy = 1.
3. FX,Y(x,y) =
R x
−∞
R y
−∞fX,Y(η,ν)dηdν.
4. P[x1 < X ≤x2,y1 < Y ≤y2] =
R x2
x1
R y2
y1 fX,Y(η,ν)dηdν, whenever both x2 ≥x1 and
y2 ≥y1.
In general, two random variables will be dependent, meaning that the value taken
on by one affects the probability distribution of the other. In fact, this is the general
case. In exceptional cases, two random variables can be statistically independent, or
simply independent.
Deﬁnition 8.A–1: Independent Random Variables
Two random variables X and Y are said to be independent if their joint distribution function
factors or separates as
FX,Y(x,y) = FX(x)FY(y) for all −∞< x,y < +∞.
Equivalently, for joint pdf’s,
fX,Y(x,y) = fX(x)fY(y) for all −∞< x,y < +∞.
We can go on to generalize these concepts to three and more random variables
coming to the concept of the random vector. In basic probability, we also study inﬁ-
nite sequences of random variables, but only in the jointly independent case, under
the topics of Central Limit Theorems and Laws of Large Numbers [1].
Random Sequences
As mentioned in the introduction to this appendix, a random sequence generalizes
the concept of random variable. Instead of deﬁning just one function on the sample
space, we deﬁne a sequence of such functions.
Deﬁnition 8.A–2: Random Sequence
Given a probability space (,F,P), a random sequence is deﬁned as the mapping X(n,ζ)
for each outcome ζ ∈, satisfying the property that for each ﬁxed −∞< n < +∞, the
mapping must be a random variable.
So we can look at a random sequence as an inﬁnite sequence of possibly depen-
dent random variables. For ﬁxed time n, we have a random variable, and for ﬁxed

322
CHAPTER 8 Image Estimation and Restoration
outcome ζ, we have a deterministic sequence. If we sample a random sequence at
a ﬁnite set of times, we have a random vector. Looking at a speciﬁc K ≥1 times
n1,n2,...,nK, we can deﬁne the Kth-order distribution function
FX(x1,x2,...,xK;n1,n2,...,nK) ≜P[X(n1) ≤x1,X(n2) ≤x2,...,X(nK) ≤xK],
to probabilistically describe this K-dimensional sample of the random sequence. If we
know these distribution functions for all 1 ≤K < ∞, and for all times n1,n2,...,nK,
then we say that we have characterized or deﬁned the random sequence X(n). With
some possibility of confusion, we often abbreviate this notation to simply
FX(xn1,xn2,...,xnK) = FX(xn1,xn2,...,xnK;n1,n2,...,nK),
by leaving off explicit notation of the times involved in this Kth-order distribution
function. The corresponding result for density functions is
fX(x1,x2,...,xK;n1,n2,...,nK) ≜∂KFX(x1,x2,...,xK;n1,n2,...,nK)
∂x1∂x2 ...∂xK
,
with simpliﬁed notation
fX(xn1,xn2,...,xnK) = fX(xn1,xn2,...,xnK;n1,n2,...,nK).
As a partial characterization of random process X(n), we can deﬁne the useful
ﬁrst and second moment functions:
Mean function:
µX(n) ≜E[X(n)] =
+∞
Z
−∞
xn fX(xn)dxn.
Correlation function:
RX(n1;n2) ≜E[X(n1)X∗(n2)] =
+∞
Z
−∞
+∞
Z
−∞
xn1x∗
n2fX(xn1,xn2)dxn1dxn2.
Covariance function:
KX(n1;n2) ≜E[(X(n1) −µX(n1))(X(n2) −µX(n2))∗]
= RX(n1;n2) −µX(n1)µ∗
X(n2).
To these functions we can add the variance function, σ 2
X(n) ≜KX(n1;n2) =
E[|X(n)|2].
Stationary Random Sequences
The distribution functions that characterize a random sequence may not change with
time. In that case, the complete set of distribution functions will only be a function

Appendix: Random Processes
323
of the intersample intervals involved. In such a case, the mean function µX(n) will
be constant, and the correlation function RX(n1;n2) will be just a function of the time
difference between the two observation times n1 and n2.
Deﬁnition 8.A–3: Stationary Random Sequence
A random sequence X(n) is stationary if for every K ≥1, the Kth-order distribution
functions are invariant with respect to time shift m, for all −∞< m < +∞; that is,
FX(x1,x2,...,xK;n1,n2,...,nK) = FX(x1,x2,...,xK;n1 + m,n2 + m,...,nK + m),
and for all sample times −∞< n1,n2,...,nK < +∞.
Note that here we must use a complete rather than the short notation
FX(xn1,xn2,...,xnK) in order make this concept clear.
In terms of the ﬁrst- and second-order moment functions, we have the weaker
concept of wide-sense stationarity.
Deﬁnition 8.A–4: Wide-Sense Stationarity
A random sequence X(n) is wide-sense stationary (WSS) if
1. µX(n) = µX(0), a constant, and
2. RX(n1;n2) = RX(n1 −n2;0) for all n1 and n2,
3. KX(n1;n2) = KX(n1 −n2;0) for all n1 and n2.
For a WSS random sequence, we can write the basic moment functions simply
as µX ≜µX(0) and RX(m) ≜RX(m;0), a one-parameter function of the variable m.
Similarly, for the covariance function, we can write KX(m) ≜KX(m;0). Thus we have
the following for any WSS random sequence X(n):
µX = E[X(n)],
RX(m) = E[X(n + m)X∗(n)],
KX(m) = E[(X(n + m) −µX)(X(n) −µX)∗],
independent of the value of n.
The Fourier transform of the WSS correlation function RX(m) is called the power
spectral density (PSD),
SX(ω) ≜
+∞
X
m=−∞
Rx(m)exp−jmω.
The PSD has the interpretation of average power density on the frequency axis
−∞< ω < +∞. As such, we can ﬁnd the average power of X(n) in a frequency
band [ω0,ω1] by integrating the PSD over that band [1].

324
CHAPTER 8 Image Estimation and Restoration
Example 8.A–1: Geometric Correlation Function
Consider the correlation function
RX(m) = σ 2
Xρ|m| +
µX
2 ,
where µX,σ X(> 0) and ρ are given constants with |ρ| < 1. When |m| is very large, we see
that RX(m) ≈
µX
2; that is, X(n + m) and X(n) are approximately uncorrelated. When
m = 0, we have RX(0) = σ 2
X +
µX
2 = E[|X(n)|2], the average power in the random
sequence.
To ﬁnd the corresponding PSD SX(ω), we plug into the deﬁnition to get
SX(ω) =
X
−∞<m<+∞
Rx(m)exp−jmω
=
X
−∞<m<+∞

σ 2
Xρ|m| +
µX
2
exp−jmω
= σ 2
X
X
−∞<m<+∞
ρ|m|e−jmω +
µX
2
X
−∞<m<+∞
exp−jmω
= σ 2
X
"
−1
X
m=−∞

ρe+jω−m
+
+∞
X
m=0

ρe−jωm
#
+ 2π
µX
2 δ(ω)
= σ 2
X


+∞
X
m′=1

ρe+jωm′
+
+∞
X
m=0

ρe−jωm

+ 2π
µX
2 δ(ω),
with m′ ≜−m,
= σ 2
X

ρe+jω
1 −ρe+jω +
1
1 −ρe−jω

+ 2π
µX
2 δ(ω)
= σ 2
X
 1 −ρ2
 1 + ρ2
−2ρ cosω + 2π
µX
2 δ(ω)
for
−π ≤ω ≤+π.
So the PSD has a Dirac impulse of area 2π
µX
2 at the origin ω = 0, and a continuous term
with peak value σ 2
X at the origin, smoothly decreasing in both directions, to a minimum
value σ 2
X
 1 −ρ2
/(1 −ρ2) at ω = ±π.
Random Processes
Random processes exist in continuous rather than discrete time. As such, for each
outcome zeta ζ in the sample space , we now have a continuous-time function.
Deﬁnition 8.A–5: Random Process
Given a probability space (,F,P), a random process is deﬁned as the mapping X(t,ζ)
for each outcome ζ ∈, satisfying the property that for each ﬁxed −∞< t < +∞, the
mapping must be a random variable—i.e., the set {ζ|X(t,ζ) ≤x} must be in the ﬁeld of
events F.

References
325
Similarly to the case for a random sequence, we can deﬁne mean, correlation, and
covariance functions for a random process. The concept of stationarity for random
process is analogous to that of random sequences. Stationary random processes have
power spectral densities that are the Fourier transforms of their stationary correlation
functions. There are also sampling theorems that apply for bandlimited stationary
random processes. Much more information on random sequences and processes can
be found in textbooks such as [1].
REFERENCES
[1] H. Stark and J. W. Woods, Probability and Random Processes with Appl. to Signal
Process., 3rd Ed., Prentice-Hall, EnglewoodCliffs, NJ, 2002.
[2] J. R. Jain and A. K. Jain, “Displacement Measurement and Its Application in Interframe
Image Coding,” IEEE Trans. Comm., vol. COM-29, pp. 1799–1804, December 1981.
[3] B. Girod, “The Efﬁciency of Motion-Compensating Prediction for Hybrid Coding
of Video Sequences,” IEEE J. Select. Areas in Comm., vol. SAC-5, pp. 1140–1154,
August 1987.
[4] J. Lim, Two-Dimensional Signal and Image Processing, Prentice-Hall, EnglewoodCliffs,
NJ, 1990.
[5] M. P. Ekstrom and J. W. Woods, “Two-Dimensionial Spectral Factorization with Applica-
tions in Recursive Digital Filtering,” IEEE Trans. Acoust., Speech, and Signal Process.,
vol. ASSP-24, pp. 115–128, April 1976.
[6] M. D. Robinson and D. G. Stork, “Joint Design of Lens Systems and Digital Image
Processing,” Proc. SPIE Intl. Optical Design Conf., vol. 6342 (63421G), 2006.
[7] J. W. Woods and V. K. Ingle, “Kalman Filtering in Two Dimensions: Further Results,”
IEEE Trans. Acoust., Speech, and Signal Process., vol. 29, pp. 188–197, 1981.
[8] F.-C. Jeng and J. W. Woods, “Inhomogeneous Gaussian Image Models for Estimation and
Restoration,” IEEE Trans. Acoust., Speech, and Signal Process., vol. 36, pp. 1305–1312,
August 1988.
[9] M. R. Banham and A. K. Katsaggelos, “Digital Image Restoration,” IEEE Signal Pro-
cess. Magazine, pp. 24–41, March 1997. (See also “RUKF Performance Revisited” in
Signal Processing Forum section of the November 1997 issue of the same magazine,
p. 12 and 14.)
[10] B. Friedland, “On the Properties of Reduced-Order Kalman Filters,” IEEE Trans. Auto.
Control, vol. 34, pp. 321–324, March 1989.
[11] A. J. Patti, A. M. Tekalp, and M. I. Sezan, “A New Motion-Compensated Reduced-
Order Model Kalman Filter for Space-Varying Restoration of Progressive and Interlaced
Video,” IEEE Trans. Image Process., vol. 7, pp. 543–554, April 1998.
[12] D. L. Angwin and H. Kaufman, “Image Restoration Using Reduced Order Models,”
Signal Processing, vol. 16, pp. 21–28, January 1988.
[13] J. Kim and J. W. Woods, “A New Interpretation of ROMKF,” IEEE Trans. Image
Process., vol. 6, pp. 599–601, April 1997.
[14] R. Wallis, “An Approach to the Space Variant Restoration and Enhancement of Images,”
Proc. Sympos. Current Math. Problems in Image Science, Naval Postgraduate School,
Monterey, CA, pp. 107–111, November 1976.
[15] D. L. Donoho, “De-Noising by Soft-Thresholding,” IEEE Trans. Inform. Theory., vol. 41,
pp. 613–627, May 1995.

326
CHAPTER 8 Image Estimation and Restoration
[16] H. Zhang, A. Nosratinia, and R. O. Wells, Jr., “Image Denoising via Wavelet-
Domain Spatially Adaptive FIR Wiener Filtering,” Proc. ICASSP 2000, Vancouver, BC,
pp. 2179–2182, May 2000.
[17] S. G. Chang, B. Yu, and M. Vetterli, “Spatially Adaptive Wavelet Thresholding with
Context Modeling for Image Denoising,” IEEE Trans. Image Process., vol. 9, pp. 1522–
1531, September 2000.
[18] J. W. Woods, “Two-dimensional Discrete Markovian Random Fields,” IEEE Trans.
Inform. Theory, vol. IT-18, pp. 232–240, March 1972.
[19] J. Besag, “On the Statistical Analysis of Dirty Pictures,” J. Royal Statis. Soc. B, vol. 48,
pp. 259–302, 1986.
[20] S. Geman and D. Geman, “Stochastic Relaxation, Gibbs Distributions, and the Bayesian
Restoration of Images,” IEEE Trans. Pattern Anal. and Machine Intell., vol. PAMI-6,
pp. 721–741, November 1984.
[21] F.-C. Jeng and J. W. Woods, “Compound Gauss-Markov Random Fields for Image
Estimation,” IEEE Trans. Signal Process., vol. 39. pp. 683–697, March 1991.
[22] S. Rastogi and J. W. Woods, “Image Restoration by Parallel Simulated Annealing Using
Compound Gauss-Markov Models,” Proc. ICASSP-91, Toronto, Canada, 1991.
[23] R. Molina, A. K. Katsaggelos, J. Mateos, A. Hermoso, and C. A. Segall, “Restoration
of Severely Blurred High Range Images Using Stochastic and Deterministic Relaxation
Algorithms in Compound Gauss-Markov Random Fields,” Pattern Recognition, Elsevier
Pergamon, vol. 33, pp. 555–571, 2000.
[24] G. K. Chantas, N. P. Galatsanos, and A. C. Likas, “Bayesian Restoration Using a New
Nonstationary Edge-Preserving Image Prior,” IEEE Trans. Image Process., vol. 15,
no. 10, pp. 2987–2997, October 2006.
[25] R. L. Lagendijk, J. Biemond, and D. E. Boekee, “Identiﬁcation and Restoration of Noisy
Blurred Images Using the Expectation-Maximization Algorithm,” IEEE Trans. Acoust.,
Speech and Signal Process., vol. 38, pp. 1180–1191, July 1990.
[26] A. P. Demster, N. M. Laird, and D. B. Rubin, “Maximum Likelihood from Incomplete
Data via the EM Algorithm,” J. Royal Statis. Soc. B., vol. 39, no. 1, pp. 1–38, 1977.
[27] J. Kim and J. W. Woods, “Image Identiﬁcation and Restoration in the Subband Domain,”
IEEE Trans. Image Process., vol. 3, pp. 312–314, 1994 plus “Erratum,” pp. 873, 1994.
[28] W. T. Freeman and E. H. Adelson, “The Design and Use of Steerable Filters,” IEEE
Pattern Anal. Machine Intell.(PAMI), vol. 13, no. 9, pp. 891–906, 1991.
[29] J. Portilla, V. Strela, M. J. Wainwright, and E. P. Simoncelli, “Image Denoising Using
Scale Mixtures of Gaussians in the Wavelet Domain, IEEE Trans. Image Process.,
vol. 12, no. 11, pp. 1328–1351, November 2003.
[30] S. Kotz, T. J. Kozubowksi, and K. Podgorski, The Laplace Distribution and Generaliza-
tions, Birkhauser, Boston, MA, 2001.
[31] S. Lyu and E. P. Simoncelli, “Modeling Multiscale Subbands of Photographic Images
with Fields of Gaussian Scale Mixtures,” IEEE Trans. Pattern Anal. and Machine Intell.
(PAMI), vol. 31, no. 4, pp. 693–706, April 2009.
[32] A. Buades, B. Coll, and J.-M. Morel, “A Non-local Algorithm for Image Denoising,”
Proc. IEEE Computer Vision and Pattern Recog. (CVPR), 2005.
[33] K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, “Image Denoising by Sparse 3-D
Transform-Domain Collaborative Filtering,” IEEE Trans. Image Process., vol. 16, no. 8,
pp. 2080–2095, August 2007.

References
327
[34] B. R. Hunt, “The Application of Constrained Least Squares Estimation to Image Restora-
tion by Digital Computer,” IEEE Trans. Computers, vol. C-22, no. 9, pp. 805–812,
September 1973.
[35] S. Farsiu, M. D. Robinson, M. Elad, and P. Milanfar, “Fast and Robust Multiframe Super
Resolution,” IEEE Trans. Image Process., vol. 13, no. 10, pp. 1327–1344, October 2004.
[36] P. Getreuer, tvreg: Variational Imaging Methods for Denoising, Deconvolution, Inpaint-
ing, and Segmentation. Available at http://www.sciweavers.org/sourcecode/tvreg-
variational-imaging-methods-denoising-deconvolution-inpainting-and-segmentation.
[37] R. Y. Tsai and T. S. Huang, “Moving Image Restoration and Registration,” IEEE ICASSP,
pp. 418–421, April 1980.
[38] D. Glasner, S. Bagon, and M. Irani, “Super-Resolution from a Single Image,” Interna-
tional Conf. on Computer Vision (ICCV), 2009.
[39] M. Elad and Y. Hel-Or, “A Fast Super-Resolution Reconstruction Algorithm for Pure
Translational Motion and Common Space-Invariant Blur,” IEEE Trans. Image Process.,
vol. 10, no. 8, pp. 1187–1193, August 2001.
[40] P. Vandewalle, S. Susstrunk, and M. Vetterli, “A Frequency Domain Approach to Reg-
istration of Aliased Images with Application to Super Resolution,” EURASIP J. Appl.
Signal Process., vol. 2006, pp. 1–14.
[41] H. J. Trussell, E. Saber, and M. Vrhel, Eds., ”Color Image Processing Special Issue,”
IEEE Signal Process. Magazine, vol. 22, January 2005.

CHAPTER
Digital Image Compression 9
Images are perhaps the most inﬂuential of media that we encounter on a daily basis.
This chapter will apply 2-D statistical signal processing to their compression for both
efﬁcient transmission and storage. We consider a generic model consisting of ﬁrst
transformation, then quantization, and then entropy coding. We cover the popular
DCT-based image coding as well as SWT-based scalable image coders such as JPEG
2000. Finally, we present some current ideas for improving image compression.
9.1 INTRODUCTION
With reference to Figure 9.1–1, we see a universal diagram for the transmission or
storage of digital images. On the extreme left, we have the input image ﬁle. We con-
sider the case where the input image is digital, with word format either ﬂoating point
or ﬁxed point. The theory, though, assumes that it is composed of real numbers, and,
in fact, our statistical model for the input image is a continuous-valued random ﬁeld.
From the viewpoint of the designer of an image communication system, the image
is certainly unknown. Over the lifetime of the communications device, we see an
ensemble of images that will be transmitted over it. This ensemble will have various
statistical properties, such as histogram, joint histogram, mean function, correlation
function, etc. So, it is reasonable to impose a statistical model for the image source.
The output of this image source is input to the source coder, whose purpose is to
compress the image ﬁle by representing it as a ﬁnite string of binary digits (bits). If
the input image is ﬁnite wordlength, then the source coder can be lossless, meaning
that the input image can be reconstructed exactly from the compressed data. Sim-
ilarly, a lossy coder is one for which only approximate reconstruction of the input
image is possible. Lossy source coding is the main topic of this chapter. Next in the
general communication diagram of Figure 9.1–1 is the channel coder, whose purpose
is to adapt or strengthen the compressed bitstream to survive the digital channel. One
way it does this is by appending error correction bits to the compressed data words
that make up the source-coded bitstream. Examples of digital channels are digital
radio and TV broadcasts, optical ﬁber lines, cable modems, cell phone and Wi-Fi
networks, and digital storage channels as found in disk memory systems. At a cer-
tain level of abstraction, we have the Internet and its protocol as a very common
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00009-6
c⃝2012 Elsevier Inc. All rights reserved.
329

330
CHAPTER 9 Digital Image Compression
Source
coder
Source
decoder
Channel
coder
Image
Channel
decoder
Digital
channel
Decoded
image 
FIGURE 9.1–1
Generic digital image communication system.
Transform
Quantization
Entropy
coder
FIGURE 9.1–2
Generic source-coding system diagram.
collection of digital channels. The remaining parts of Figure 9.1–1 show the decod-
ing. First the channel decoder attempts to recover the source coded bitstream, then
the source decoder tries to reconstruct the source image. Figure 9.1–2 shows a use-
ful decomposition valid for many source coders, which consist of a transformation,
followed by a quantizer and then an entropy coder.
The purpose of the transformation is to remove or suppress the redundant parts
of the data, hopefully yielding components that are independent or at least uncorre-
lated. Examples are differential PCM (DPCM)1, orthonormal transforms based on
frequency decompositions (DFT, DCT, and some SWTs), and the optimal Karhunen-
Loeve transform (KLT) [1]. These transforms can be applied to the entire image, or
1In differential pulse-code modulation the transform and the quantizer are linked together in a feedback
loop. Therefore it is not right to say that the transform precedes the quantizer always. In DPCM the roles
of the transform and the quantizer are intertwined.

9.2 Transformation
331
as is commonly done, to relatively small blocks of pixel values. Usually these blocks
do not overlap, but in some methods (e.g., lapped orthogonal transform [LOT] and
SWT) they do. The quantization block in Figure 9.1–2 is very important because this
is where the actual data compression occurs by a shortening of the data wordlength.
The transform block is invertible, so that no data compression actually occurs there.
A quantizer is a nonlinear, zero-memory device that chooses representative values for
ranges of input data coming from the transform, one at a time, called scalar quantiza-
tion, or several at a time, called vector quantization. Most quantizers are scalar, and
common examples are uniform quantization (also called an A/D converter), nonuni-
form quantization, optimal uniform, optimal nonuniform, etc. The entropy coder
block in Figure 9.1–2 converts the representative values, outputs of the quantizer,
to efﬁcient variable-length codewords. If we omit this last block, then the quantizer
output is converted into ﬁxed-length codewords, which are usually less efﬁcient.
Examples of variable-length codes are Huffman and arithmetic codes, which will
be brieﬂy described in Section 9.4 (see also Section 9.10 Appendix on Information
Theory).
9.2 TRANSFORMATION
The role of the transformation in data compression is to decorrelate, and more gener-
ally, remove dependency between the data values, but without losing any information.
For example, the transformation should be invertible, at least with inﬁnite word-
length arithmetic. For Gaussian random ﬁelds, rate-distortion theory [1, 2] states that
the optimal transformation for this purpose is the KLT. The transform can be applied
in small blocks or to the entire image. Common block transforms include the DFT,
DCT, and the overlapped extension of DCT called LOT. The DCT is motivated by
being close to the KLT for a 1-D ﬁrst-order Gauss-Markov random sequence, when
the correlation coefﬁcient ρ ⪅1.0. The practical advantage of the DCT in these cases
is that it has a fast algorithm, unlike the general KLT. For images, the 2-D DCT is
motivated by the separable Gaussian random ﬁeld with the correlation function given
as a separable product of the 1-D Markov correlation function,
Rx(m1,m2) = σ 2
xρ|m1|
1
ρ|m2|
2
,
(9.2–1)
where ρ1 and ρ2 are the horizontal and the vertical correlation coefﬁcients, respec-
tively, 0 < ρ1,ρ2 < 1, which should both be close to one.
DCT
As we have seen in Chapter 4, for a data array of rectangular support [0,N1 −1] ×
[0,N2 −1], the 2-D DCT is given as
XC(k1,k2) ≜
N1−1
X
l1=0
N2−1
X
l2=0
4x(n1,n2)cos πk1
2N1
(2n1 + 1)cos πk2
2N2
(2n2 + 1),

332
CHAPTER 9 Digital Image Compression
for (k1,k2) ∈[0,N1 −1] × [0,N2 −1]. Otherwise, XC(k1,k2) ≜0. However, in this
chapter, we only consider (k1,k2) ∈[0,N1 −1] × [0,N2 −1]. For the common block-
DCT and an input image of size L1 × L2, we decompose it into N1 × N2 blocks
and perform the DCT for each successive block. The total number of blocks then
becomes ⌈L1/N1⌉× ⌈L2/N2⌉, where the last block in each line and column may be
a partial block. This transforms the image data into a sequence of DCT coefﬁcients,
which can be stored blockwise or can be stored with all like-frequency coefﬁcients
together, effectively making a small ‘image’ out of each DCT coefﬁcient. Of these,
the so-called DC image made up of the XC(0,0) coefﬁcients is a thumbnail version
of the original image, and, in fact, is just an (N1 × N2) ↓subsampled version of the
original after N1 × N2 box ﬁltering.
Owing to the nearness of a typical natural image correlation function to (9.2–1),
the DCT coefﬁcients in a given block are approximately uncorrelated. We can
expect the coefﬁcients from different blocks to be approximately uncorrelated
because of their distance from one another, if the blocksize is not too small. Even
for neighboring blocks, the argument of uncorrelatedness is quite valid for the upper
frequency or AC coefﬁcients and is only signiﬁcantly violated by the DC coefﬁ-
cients and a few low-frequency coefﬁcients. This interblock coefﬁcient-correlation
effect is almost ignored in standard coding algorithms such as JPEG, where only
the DC coefﬁcients of blocks are singled out for further decorrelation process-
ing, usually consisting of a prediction followed by a coding of the prediction
residual.
A key property of a transform like DCT used in data compression is that it is
orthonormal, with its transformation matrix being called unitary. This means that
there is a Parseval-like relation between the sum of the magnitude-square values in
the transform space (i.e., the coefﬁcients) and the data or image space. Since the
transform is linear and constant, this energy balance must also hold for the coding
error. So if we approximate these coefﬁcients via quantization, and are concerned
with minimizing the sum magnitude-squared error, we can just as well do this in the
coefﬁcient or transform domain, where each coefﬁcient can be quantized (or coded
more generally) separately, using a scalar quantizer,
N1−1
X
n1=0
N2−1
X
n2=0
(x(n1,n2) −bx(n1,n2))2 =
1
4N1N2
N1−1
X
k1=0
N2−1
X
k2=0
w(k1)w(k2)
×

XC(k1,k2) −bXC(k1,k2)
2
.
A generalization of the block-DCT transformation is the fast LOT [3]. Here, over-
lapping 16 × 16 LOT transform blocks are made up by cascading neighboring 8 × 8
DCT blocks with an orthogonalizing transformation on their outputs. Actually, this
is done in one dimension and then the procedure is just copied to the 2-D case, via
row–column separable processing. An advantage of the LOT is that the objection-
able blocking effect of DCT compression is reduced by the overlapping LOT input
windows. Also, the LOT respects the digital signal processing theoretical method of

9.2 Transformation
333
2-D sectioned convolution (cf. Section 4.6) by employing block overlap in imple-
mentation of its block processing. Another way of providing overlapped blocks, and
perhaps eliminating blocks altogether, is the SWT.
SWT
Subband/wavelet transformation (SWT) can be viewed as a generalization of block-
based DCT. This is because there is a ﬁlter bank interpretation of the block-based
DCT, wherein the ﬁlter impulse responses are the DCT basis functions, and the
decimation is (N1 × N2) ↓downsampling. Usually in SWTs, the ﬁlter support is
larger than the decimation ratio, which is normally (2 × 2) ↓. So, SWT is seen as
a generalization of DCT to overlapping blocks, with expected reductions or elimi-
nation of blocking artifacts. A general subband analysis/synthesis bank is shown in
Figure 9.2–1.
Usually more stages of subband decomposition are conducted than just this one
level. In the common dyadic (a.k.a. octave and wavelet) decomposition, each result-
ing LL subband is further split recursively, as illustrated in Figure 9.2–2, showing a
three-level subband/wavelet decomposition.
While this dyadic decomposition is almost universal, it is known to be suboptimal
when a lot of high spatial frequency information is present (e.g., the well-known
test image Barbara). When the subband splitting is optimized for a particular set
of image statistics, the name wavelet packet [4] has emerged for the corresponding
transformation.
In summary, SWT is seen to be a generalization of the block-DCT- and LOT-based
transformations and, as such, can only offer the possibility for better performance.
H00
H01
H10
H11
F00
F01
F10
F11
2 ×2
2 ×2
2 ×2
2 ×2
2 ×2
2×2
2×2
2×2
FIGURE 9.2–1
A general 2-D SWT or inverse SWT analysis/synthesis bank.

334
CHAPTER 9 Digital Image Compression
HH
I(x,y)
LL
LL
LL
LH
HL
HH
LH
HL
LH
HL
HH
LH HL
FIGURE 9.2–2
Illustration of dyadic (octave, wavelet) subband decomposition to three levels.
+
+
+
−
Σ
Σ
Q[  ]
2-D linear
predictor
x(n1, n2)
x(n1, n2)
x(n1, n2)
∼
e(n1, n2)
∼
e(n1, n2)
∼
e(n1, n2)
^
x(n1, n2)
^
x(n1, n2)
^
.
FIGURE 9.2–3
Diagram of a 2-D DPCM coder.
Best performance in this transform class can then be obtained by optimizing over the
subband/wavelet ﬁlters used and the nature and manner of the subband splittings. The
goal of all such transformations is to decorrelate (i.e., remove linear dependencies in)
the data prior to quantization.
DPCM
A 2-D extension of differential pulse-code modulation (DPCM) has been extensively
used for image compression. We see in Figure 9.2–3 that the main signal ﬂow path
across the top of the ﬁgure quantizes an error signal that is generated via a predic-
tion bx(n1,n2) that is fed back and, in turn, generated from only the past values of

9.3 Quantization
335
the quantized error signal ee(n1,n2). Here, the transformation and the quantization
are intertwined, not matching our paradigm of transform coming before quantizer
(see Figure 9.1–2). We will look at quantizers in some detail in the next section, but
for now, just consider it a device that approximates the incoming prediction error
e(n1,n2) with a ﬁnite number of levels. The signal estimatebx(n1,n2) is produced by
2-D linear (1,0)-step prediction based on its inputex(n1,n2). Note that this feedback
loop can also be run at the decoder, and, in fact, it is the decoder, withex(n1,n2) as the
decoder output. Here, we assume that the quantized error signalee(n1,n2) is somehow
losslessly conveyed to the receiver and its source decoder; that is, no channel errors
occur.
One nice property of 2-D DPCM that directly extends from the 1-D case can be
seen now. Consider the overall error at any given data location (n1,n2):
x(n1,n2) −ex(n1,n2) = [x(n1,n2) −bx(n1,n2)] −[ex(n1,n2) −bx(n1,n2)]
= e(n1,n2) −ee(n1,n2),
as can be seen from the two equations at the summing junctions in Figure 9.2–3. So
if we design the quantizer Q to minimize the MSE between its input and output, we
are also ﬁnding the quantizer that minimizes the MSE at the DPCM decoder output.
This very useful property would be lost if we moved the quantizer Q to the right
and outside of the feedback loop. This is because a prediction-error ﬁlter does not
constitute an orthogonal transformation, and quantization errors would accumulate at
the decoder output.
There remains the problem of how to calculate the 2-D linear predictor coef-
ﬁcients. If we are concerned with high-quality (read high-bitrate) coding, then we
can expect that ex(n1,n2) ≈x(n1,n2) to a sufﬁcient degree of approximation to per-
mit use of the linear (1,0)-step prediction ﬁlter that can be designed based on the
quantization-noise-free input x(n1,n2) itself. At low bitrates, some optimization can
be done iteratively, based on starting with this ﬁlter, then generating the correspond-
ingex(n1,n2), and then generating a new linear (1,0)-step prediction ﬁlter, and so on.2
However, DPCM performance deteriorates at low bitrates.
9.3 QUANTIZATION
The quantization operation is one of truncation or rounding. Figure 9.3–1 shows a
quantizer Q, with input variable x and output variable bx = Q(x), and Figure 9.3–2
shows its quantizer function Q(x) plotted versus input x, which may be continuous
valued or already quantized on a relatively ﬁne scale to that of this quantizer.
2Starting this second time through, the ﬁlter is really a prediction estimator since its design input isex
and not x anymore.

336
CHAPTER 9 Digital Image Compression
Q(·)
x
x^
FIGURE 9.3–1
A scalar quantizer.
x
Q(x)
d0
d2
d1
di−1
di+1 di+2
dL−1
dL
di
...
...
ri
rL
ri+2
ri+1
ri−1
r2
r1
FIGURE 9.3–2
A quantizer characteristic that “rounds to the left.”
The scalar quantizer (SQ) is speciﬁed in general by its number of output values or
levels L, its decision levels di, and its representation levels ri, also the output values.
The deﬁning equation for the quantizer function is then
Q(x) ≜



r1,
d0 < x ≤d1,
...
...
ri,
di−1 < x ≤di,
...
...
rL,
dL−1 < x ≤dL.
Here, we have taken the input range cells or bins as (di−1,di], but they could equally
be [di−1,di). With reference to Figure 9.3–2, the total range of the quantizer is [r1,rL]
and its total domain is [d0,dL], where normally d0 = xmin and dL = xmax. For exam-
ple, in the Gaussian case, we would have d0 = xmin = −∞, and dL = xmax = +∞.
We deﬁne the quantizer error eQ(x) ≜Q(x) −x =bx −x, and measure the distor-
tion as d(x,bx ) ≜|eQ| or more generally |eQ|p, for positive integers p.

9.3 Quantization
337
The average quantizer distortion D is given as
D = E[d2(x,bx)]
=
+∞
Z
−∞
d2(x,bx)fx(x)dx
=
L
X
i=1
di
Z
di−1
|x −ri|2fx(x)dx,
where fx is the pdf of the assumed random variable x.
Uniform Quantization
For 1 ≤i ≤L, set di −di−1 = 1, called the uniform quantizer step size. To complete
the uniform quantizer, we set the representation value as
ri ≜1
2(di + di−1),
(9.3–1)
at the center of the input bin (di−1,di], for 1 ≤i ≤L.
Example 9.3–1: Nine-Level Uniform Quantizer on |x| ≤1
Let xmin = −1 and xmax = +1. Then we set d0 = −1 and dL = +1 and take L = 9. If the
step size is 1, then the decision levels are given as di = −1 + i1, for i = 0,9, so that
d9 = −1 + 91. Now this last value must equal +1, so we get 1 = 2/9. (More generally we
would have 1 = (xmax −xmin)/L.) Then from (9.3–1), we get
ri = 1
2(di + di−1)
= −1 +

i −1
2

1
= di −1
21.
Note that there is an output at zero. It corresponds to i = 5, with d5 = +1/9. This input
bin, centered on zero, is commonly called the deadzone of the quantizer.
If a uniform quantizer is symmetrically located with respect to input value zero,
then when the number of levels L is an odd number, we have a midtread quantizer,
characterized by an input bin centered on 0, which is also an output value. Similarly,
for such a uniform symmetric quantizer, when L is even, we have a midrise quantizer,
characterized by a decision level at 0, and no zero output value.

338
CHAPTER 9 Digital Image Compression
Clearly, the MSE distortion of a quantizer will depend on the step sizes and on
the total number of output levels L as well as upon the pdf fx of the random variable
x. For a uniform quantizer, this would translate into dependence on the step size 1
and number of levels L. A moment’s consideration reveals that the quantizer MSE
D should be proportional to the variance σ 2 of the pdf. In the case of ﬁne quantiza-
tion (i.e, many levels, closely spaced), we would expect that the pdf would be fairly
constant across the decision intervals (bins), and so halving 1, and a doubling of
L to keep the range the same, should lead to one-quarter the MSE. We thus expect
quantizer models of the form
D(1) = k1σ 212
= k1σ 2/L2,
where k1 and k2 are constants of proportionality that would depend on the pdf fx.
Further, if we were to encode each of the L output values as a ﬁxed-length codeword,
it would require on the order of b = log2 L bits, so we can also expect that at least
for ﬁxed-length quantization, the MSE as a function number of bits b can be usefully
modeled as
D(b) = kσ 22−2b,
in the high rate or ﬁne quantization case.
Example 9.3–2: Uniform Quantization of a Uniform Random Variable
In this example apply the uniform quantizer to a random variable that is uniformly dis-
tributed U[−1,+1]. The number of levels is chosen as L = 2b for the range 0 ≤b ≤9 or L
range 1 to 512. The results are shown in Figure 9.3–3. The computation was done using
the MATLAB program UniUniformQuant.m available at this book’s Web site. Figure 9.3–4
is a plot of log2 D(b) that displays a straight line of approximate slope −2.
Optimal MSE Quantization
The average MSE of the quantizer output can be expressed as a function of the L
output values and the L −1 decision levels. We have, in the real-valued case,
D =
+∞
Z
−∞
d2(x,bx) fx(x)dx
=
L
X
i=1
di
Z
di−1
(x −ri)2 fx(x)dx
≜g(r1,r2,...,rL;d1,d2,...,dL−1).

9.3 Quantization
339
1
0
0.02
0.04
0.06
0.08
0.1
0.12
Mean square distortion D(b)
0.14
0.16
0.18
Plot of D(b) for uniform random variable U(−1, +1)
Number of bits b+ 1
2
3
4
5
6
7
8
9
10
FIGURE 9.3–3
Plot of MSE of uniform quantization of uniform random variable U[−1,+1] versus number
of bits b.
−2
−3
−4
−5
−6
−7
−8
−9
−10
−11
−12
1
2
3
4
5
6
7
8
9
10
log2(D(b))
Plot of log2(D(b)) for uniform random variable U(−1, +1)
Number of bits b+1
FIGURE 9.3–4
Plot of log2 D(b) versus b for uniform quantization of uniform random variable U[−1,+1].

340
CHAPTER 9 Digital Image Compression
We can optimize this function by taking the partial derivatives with respect to ri
to obtain
∂D
∂ri
=
di
Z
di−1
∂
∂ri
(x −ri)2fx(x)dx
=
di
Z
di−1
2(x −ri)fx(x)dx.
Setting these equations to zero, we get a necessary condition for representation
level ri:
ri =
R di
di−1 xfx(x)dx
R di
di−1 fx(x)dx
= E[x|di−1 < x ≤di],
for 1 ≤i ≤L.
This condition is very reasonable; we simply set the output (representation) value
for the bin (di−1,di] equal to the conditional mean of the random variable x, given
that it is in this bin. Taking the partial derivatives with respect to the di, we
obtain
∂D
∂di
= ∂
∂di



di
Z
di−1
(x −ri)2fx(x)dx +
di+1
Z
di
(x −ri+1)2fx(x)dx



= (di −ri)2px(di) −(di −ri+1)2px(di).
Setting this equation to zero and assuming that fx(di) ̸= 0, we obtain the relation
di = 1
2(ri + ri+1),
for 1 ≤i ≤L −1,
remembering that d0 and dL are ﬁxed at the range of the input pdf support. This equa-
tion gives the necessary condition that the optimal SQ decision points must be at the
arithmetic average of the two neighboring representation values. This is somewhat
less obvious, but can be seen as picking the output value nearest to input x, certainly
necessary for optimality.
An SQ Design Algorithm
The following algorithm makes use of these necessary equations to iteratively arrive
at the optimal MSE quantizer. It is experimentally found to converge. We assume the
pdf has inﬁnite support here.

9.3 Quantization
341
1. Given L, and the probability density function fx(x), we set d0 = −∞, dL = +∞,
and set index i = 1. We make a guess for r1.
2. Use ri =
R di
di−1 xfx(x)dx
R di
di−1 fx(x)dx to ﬁnd di by integrating forward from di−1 until a match is
obtained.
3. Use ri+1 = 2di −ri to ﬁnd ri+1.
4. Set i ←i + 1 and go back to step 2, unless i = L.
5. At i = L, check
rL ≶
R ∞
dL−1 xfx(x)dx
R ∞
dL−1 fx(x)dx .
If “rL > ”, then reduce initial value r1. Otherwise, increase r1. Then return to
step 2. Continue this until convergence.
The amount of increase/decrease in r1 has to be empirically determined, but this
relatively straightforward algorithm works well to ﬁnd approximate values for the
decision and representation levels. Note that numerical integration may be needed in
step 2, depending on the pdf fx. An alternative algorithm is the scalar version of the
Linde, Buzo, and Gray (LBG) algorithm for vector quantization, to be shown next.
Vector Quantization
Information theory says that quantizing signal samples one at a time (i.e., scalar
quantization) can never be optimal [1, 5]. Certainly this is clear if the successive
samples are correlated or otherwise dependent, and, in fact, that is the main rea-
son for the transformation in our generic source compression system. But even in
the case where the successive samples are independent, scalar quantization is still
theoretically not the best way [1, 5], although the expected gain would certainly be
much less in this case. Vector quantization (VQ) addresses the nonindependent data
problem by quantizing a group of data samples simultaneously [2]. As such, it is
much more complicated than scalar quantization, with the amount of computation
increasing exponentially in the vector size, with practical VQ sizes limited to 4 × 4
and below, although some experimental work has been done for 8 × 8 blocksize [6].
Another difference is that VQ is typically designed from a training set, while SQ
is typically designed from a probability model. This difference gives a signiﬁcant
advantage to VQ in that it can take use the “real” multidimensional pdf of the data,
even in the absence of any viable theoretical multidimensional pdf model, other than
joint Gaussian. As such, VQ coding results can be much better than those of SQ. In
this regard, it should be noted that via its internal multidimensional pdf, the VQ effec-
tively removes all dependencies in the data vector, and not just the so-called linear
dependencies dealt with via a linear transformation. The downside, of course, is that
4 × 4 is a very small block. As a consequence, VQ is not often used alone without a
transformation, but the two together can be quite powerful [7].
We later present a data-based design algorithm for VQ generally attributed to
Linde, Buzo, and Gray (LBG), but we start out assuming that the theoretical multidi-
mensional pdf is known. We consider an N × N random vector x with joint pdf fx (x)

342
CHAPTER 9 Digital Image Compression
with support X. Then we have a set of regions Cl in X that decompose this vector
space in a mutually exclusive and collectively exhaustive manner—i.e.,
X = ∪L
l=1Cl and Cl ∩Cm = φ, for all l ̸= m,
where φ is the null set.
On deﬁning a set of representation vectors {r1,r2,...,rL}, the vector quantizer
then works as follows
bx = Q(x) ≜



r1,
x ∈C1,
...
...
rL,
x ∈CL.
(9.3–2)
So in VQ the input-space decision regions Cl play the role of the input decision inter-
vals (bins) in SQ. Unfortunately, these input regions are much harder to deﬁne than
simple bins, and this makes the actual quantizing operation Q(x) computationally
difﬁcult. In order to perform the mapping (9.3–2), we have to ﬁnd what region Cl
contains x.
As was the case for scalar quantization, we can write the VQ error eQ ≜bx −x,
and express the total MSE as
D = E[(bx −x)T (bx −x)]
=
+∞
Z
−∞
d2(x,bx) fx(x)dx
=
L
X
l=1
Z
Cl
d2(x,rl) fx(x)dx.
One can show the following two necessary conditions for optimality:
•
Optimality condition 1 (useful for design of input regions Cl and for actually
quantizing the data x):
Q(x) = rl iff d(x,rl) ≤d(x,rm), for all l ̸= m.
•
Optimality condition 2 (useful for determining the representation vectors rl):
rl = E[x|x ∈Cl].
(9.3–3)
The ﬁrst optimality condition states that, for a given set of representation vectors
{r1,r2,...,rL}, the corresponding decision regions should decompose the input space
in such a way as to cluster each input vector x to its nearest representation vector in
the sense of Euclidean distance d(x,rl). The second optimality condition says that if
the input decision regions Cl are given, then the best choice for their representation
vector is their conditional mean.
From the ﬁrst optimality condition, we can write
Cl = {x|d(x,rl) ≤d(x,rm), for all l ̸= m},

9.3 Quantization
343
and such disjoint sets are called Voronoi regions [2]. So the large part of designing a
VQ is to carve up the input space into these Voronoi regions, each determined by its
property of being closer to its own representation vector rl than to any other.
Now this is all well and good, but the fact remains that we have no suitable
theoretical joint pdf’s fx(x), other than multidimensional Gaussian, with which to
calculate the conditional means in (9.3–3). However, if we were given access to a
large set of input vectors {x1,x2,...,xM}, all drawn from the same distribution, called
the training set, the following data-based algorithm has been found to converge to a
local minimum of the total square error over this training set [2], i.e., to provide a
least-squares solution to VQ, with error metric
M
X
i=1
d2[xi,rl(xi)],
(9.3–4)
where rl(xi) is just the VQ output for training vector xi.
LBG Algorithm [2]3
1. Initially guess the L output representation vectors r1,r2,...,rL.
2. Use optimality condition 1 to quantize each input vector in the training set; that
is, ﬁnd the vector rl that minimizes the distance d(xi,rl). When this step ﬁnishes,
we have partitioned the training set into the initial decision regions Cl.
3. Use optimality condition 2 to update the representation vectors as
rl = 1
Mi
X
xj∈Cl
xj,
where Mi ≜|{x|xj ∈Cl}|, denoting the number of training vectors in Cl.
4. Go back to step 2 and iterate until convergence.
A bit of consideration of the matter reveals that steps 2 and 3 can only improve
the optimality—i.e., reduce the error (9.3–4)—and never cause an increase in error.
Hence the LBG algorithm converges to at least a local minimum of the total least-
squares error. After design, the resulting VQ can be tested on a separate set of input
data called the test set, that is assumed to have the same or similar joint distribution
function.
Example 9.3–3: Vector Quantization
Consider a simple example with M = 5 training vectors,
x1 =
"
1
1
#
,x2 =
"
0
0
#
,x3 =
"
2
0
#
,x4 =
"
−1
1
#
,x5 =
"
0
−1
#
,
3For comparison, also see the K-means algorithm of Section 7.3.

344
CHAPTER 9 Digital Image Compression
to be vector quantized to L = 3 output vectors, r1,r2,r3, using the LBG algorithm. We start
with the initial guess
r1 =
"
0
0
#
,r2 =
"
1
0
#
,r3 =
"
−1
0
#
,
and proceed into iteration one of the LBG algorithm. In step 2, we quantize (classify) the
training data as follows:
x1 →r2, x2 →r1, x3 →r2, x4 →r3, and x5 →r1.
Then in step 3, we update the three representation vectors as,
r1 = 1
2(x2 + x5) =
"
0
−.5
#
,r2 = 1
2(x1 + x3) =
"
1.5
0.5
#
,
and r3 = x4 =
"
−1
1
#
.
We then proceed into iteration two of the LBG algorithm. In step 2 we get the mapping
x1 →r2, x2 →r1, x3 →r2, x4 →r3, and x5 →r1,
which is unchanged from step 2 of iteration one. Thus the second, or update, step will
leave the representation vectors ri unchanged from those of the ﬁrst iteration. So we are
converged at iteration two, with representation vectors
r1 =
"
0
−0.5
#
,r2 =
"
1.5
0.5
#
,r3 =
"
−1
1
#
.
It is interesting to note that the optimality is only local for the LBG algo-
rithm, meaning the convergence point depends on the initial guess. For example,
if in this example we had started with the initial guess r1 =
 0
0

,r2 =
 1
0

,
r3 =
 1/2
−1

, then LBG would converge to a different VQ—i.e., different set of rep-
resentation vectors (see problem 9.7). For this reason, users often try several starting
points to ensure a good minimum.
VQ is not used in today’s image coding standards, but the related technique trellis-
coded quantization (TCQ) appears as an optional part in the JPEG 2000 standard [8].
We will discuss the basic part of this image compression standard in Section 9.7.
Example 9.3–4: VQ on Cameraman
Here, we perform LBG VQ on the 256 × 256 gray-level cameraman image. We used the
freely available software VcDemo [9] to form 4 × 4 codebooks with random initialization,
for 1–12 bits per vector, ﬁxed-length codewords. The resulting PSNR versus bits/pixel is

9.3 Quantization
345
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
17.5
20.0
22.5
25.0
27.5
30.0
32.5
Bitrate (bpp)
PSNR (dB)
Vector quantization
FIGURE 9.3–5
Plot of PSNR versus bits for 4 × 4 VQ on the cameraman image.
(a)
(b)
FIGURE 9.3–6
LBG VQ 4 × 4 result on the cameraman image: (a) 4 bits/vector, (b) 8 bits/vector.
plotted in Figure 9.3–5 as produced by VcDemo. We also show 2 of these 12 results
in Figure 9.3–6. The image on the left is made from 16 4 × 4 vectors (i.e., 4 bits/
vector), and the one on the right is made from 256 4 × 4 vectors (i.e., 8 bits per vec-
tor). They are, respectively, 1/4 bits per pixel (bpp) and 1/2 bpp. The quantized image
with 16 = 24 vectors shows poor quality, while the one made from 256 = 28 vectors begins
to look acceptable, at least for some uses.

346
CHAPTER 9 Digital Image Compression
9.4 ENTROPY CODING4
After the quantization, we must send the message set of quantizer outputs to the chan-
nel or channel coder. Without loss of generality, we can think in terms of binary digits
or bits for this information.5 In general we can use a ﬁxed-length or variable-length
code for this purpose. Now, the mathematical theory of communication deﬁnes infor-
mation [1, 5] quantitatively for an independent, discrete-valued, stationary random
source as
I(xi) ≜log2
1
p(xi),
and its average value entropy as
H(X) =
X
i
p(xi)log2
1
p(xi) .
(9.4–1)
It is proved in [1, 5] that any coding of such a source into binary digits must have an
average codeword length greater than or equal to this entropy. The so-called source
coding theorem states that, for the encoding of one message at a time,
H(X) ≤l < H(X) + 1,
(9.4–2)
where l denotes the average codeword length l ≜P
i p(xi)l(xi). If we were to go to
the expense of jointly coding M messages simultaneously, theory says that this bound
becomes
H(X) ≤l < H(X) + 1
M ,
due to the assumed independence of the random source. Unfortunately, such coding
of the so-called extension source is not often practical. In both these equations, H is
the entropy per single message.
Huffman Coding
In the 1950s, David Huffman came up with an optimal variable-length encoding pro-
cedure, which came to be known as Huffman coding [1, 5]. It is a method for coding
a ﬁnite number of messages into variable-length binary codewords in such a way that
the average codeword length is minimized. Huffman coding creates a binary tree by
ﬁrst joining together the two messages with the lowest probabilities, thereby creat-
ing a reduced source with one fewer messages. It then proceeds recursively to join
together the two messages with lowest probability for this reduced source, proceeding
4This section requires a background in information theory. An introduction to information is provided
in the appendix to this chapter.
5This is the so-called quantizer index set. We assume that the receiver knows the quantizer design and
can map the set of quantizer indices back into the actual representation values. Such a mapping is often
given the misnomer inverse quantization, and in practice would be communicated to the receiver as the
default values in an international standard or perhaps as header information in a packet or ﬁle.

9.4 Entropy Coding
347
down to a root node whose probability is one. When the branches of this tree are
populated with ones and zeros, a variable length code can be read off in the reverse
order. This is perhaps best illustrated by example.
Example 9.4–1: Huffman Coding
Let the source have M = 5 messages, with probabilities p1 = 0.5,
p2 = 0.3,
p3 = 0.1,
p4 = 0.05,
p5 = 0.05. Then we can construct the Huffman code as shown in
Figure 9.4–1. Here, starting from the left, messages 4 and 5 have the lowest probabilities
and are therefore combined ﬁrst. Their total probability is then 0.1. It happens that this
node is also a smallest probability in the reduced source, and it is joined with message
3 with p(3) = 0.1 at the second level. We continue on to complete the tree. The variable-
length codewords are now read from the right and become, reading from message 1 to
message 5: 0,10,110,1110, and 1111, with lengths li = 1,2,3,4, and 4, respectively,
in this example. We note that this variable-length code is uniquely decodable, because no
codeword is the preﬁx of another codeword. It can be seen that the Huffman code tree
guarantees that this will always be true. Computing the average codeword length, we get
l ≜
X
i
p(xi)l(xi)
= 0.5 × 1 + 0.3 × 2 + 0.1 × 3 + 0.05 × 4 + 0.05 × 4
= 1.8 bits/message.
In this case the entropy from (9.4–1) is H = 1.786. We note that the average codeword
length for this example is quite close to the entropy of the source. This is not always the
case, especially in the case when the entropy per message is much less than one, as
forewarned in (9.4–2).
p(1)= .5
p(2)= .3
p(3)= .1
p(4)= .05
p(5)= .05
1
1
1
1
0
0
0
0
FIGURE 9.4–1
Example of Huffman coding tree.
In passing, we note the the Huffman variable-length tree is complete, i.e., all
branches terminate in valid messages. Hence, the resulting code stream will be
sensitive to channel errors. More on this later.
Arithmetic Coding
The Huffman coding procedure codes a ﬁxed number of messages, usually one, into
variable-length binary strings. Another popular coding method, in contrast, codes a

348
CHAPTER 9 Digital Image Compression
p3
p1
p2
p1
p2
p3
p1
p2
p3
2
1
2
1
0
1
FIGURE 9.4–2
An essential aspect of arithmetic coding.
variable number of messages into ﬁxed-length binary strings. This method, called
arithmetic coding (AC), can usually achieve a very close match to the entropy of the
source. AC works with subintervals of [0,1], with lengths equal to the probabilities
of the messages to be coded.
As illustrated in Figure 9.4–2, as messages are processed, the subinterval is suc-
cessively split according to the message probabilities. First, in this example, we
“send” message 2 by centering attention on the middle interval of length p2. Then
we send message 1 by centering attention on the subinterval p1. We then “send” the
following messages 2 and 1 by the same approach. This continues until a ﬁxed, very
small subinterval size is reached, or is about to be exceeded. If we “convey” the ﬁnal
subinterval to the receiver, then the decoder can decode the string of messages that
lead up to this ﬁnal subinterval of [0,1]. We note in passing that there is nothing to
prevent the message probabilities from changing in some prescribed manner as the
successive messages are “encoded” into subintervals. The ﬁnal subinterval is actually
conveyed to the receiver by sending a ﬁxed-length binary string of sufﬁcient num-
ber of digits to point uniquely to the pre-chosen ﬁxed but small subinterval. This is
indicated by the pointer in Figure 9.4–2. A typical pointer value is 14 bits or so. Note
that decoding can proceed while the data are transmitted in a ﬁrst-in, ﬁrst-out fash-
ion, because as the subinterval shrinks in size, the leading digits of the ﬁxed-length
binary string are successively determined. A lot of practical details are left out of the
simple argument here, including the important question of numerical signiﬁcance for
a ﬁxed-length computer word implementation; however, the basic idea is as shown.

9.4 Entropy Coding
349
The question remains, how efﬁcient is this method? To provide a crude answer to
this question, we note that the ﬁnal subinterval size will be the product of the message
probabilities that are encoded 5ip(xi), and that the approximate number of binary
digits to uniquely point to such an interval is
L = −log2(2−L) ≃−log2 [5ip(xi)] =
X
i
−log2 [p(xi)],
where the term on the right is the sample total, the average value of which is the
entropy of this independent string of messages from the source. By the law of large
numbers of probability theory, for an ergodic source, we can expect convergence to
the entropy (9.4–1). So we can expect to get very close to the entropy of a stationary
ergodic source in this way. A more thorough derivation of AC, including algorithms,
can be found in Sayood [10] and Gersho and Gray [2].
ECSQ and ECVQ
It is more efﬁcient to combine quantization and entropy coding into one operation.
Entropy-coded scalar quantization (ECSQ) and entropy-coded vector quantization
(ECVQ) accomplish just that. For a nonstructured VQ(SQ) we can start with the VQ
design problem of Section 9.3 (Vector Quantization) and augment the error criteria
with the Lagrange cost as
E = D + λH(R),
=
L
X
l=1
Z
Cl
d2(x,rl)fx(x)dx + λH(R),
where H(R) is the resulting entropy of the quantized source X with representation
vectors rl, and λ is a Lagrange multiplier. The parameter λ then becomes a vari-
able controlling a tradeoff of rate H(R) versus distortion D. Here, the entropy is
computed as
H(R) =
L
X
l=1
pl log2
1
pl
,
with pl being the probability mass function (pmf) of the quantizer outputs rl.
Experimentally, we can use the LBG algorithm on a training set as in Section 9.3
(LBG Algorithm), just augmenting the error (9.3–4) with lambda times the experi-
mental entropy computed from a histogram over the training vectors. To get the
complete distortion-rate function of the resulting ECVQ(ECSQ) system, we merely
run this modiﬁed LBG algorithm for a range of λ values and plot the resulting points.
These points can then be connected with straight lines to get the ﬁnal estimate of the
joint quantizer-entropy model. Since conditional AC has been found to be extremely
efﬁcient for image-coding applications, one can effectively take the resulting rate-
distortion characteristic and regard it as a virtually joint quantizer-AC model also.
Taking a structured SQ, such as a UTQ with stepsize 1 and deadzone 21, and
number of levels L, we can plot the rate-distortion characteristics of an ECSQ directly

350
CHAPTER 9 Digital Image Compression
D(R)
R
0
L1
L3
L2
FIGURE 9.4–3
Illustration of ECSQ joint quantizer and entropy encoder.
as a function of parameter 1, typically getting plots such as shown in Figure 9.4–3.
We see that large values of L do not penalize results below a certain maximum rate, at
which point saturation begins to occur. This is mainly a consequence of the fact that
messages with very small probability do not add much to the entropy of a source, via
ϵ log2 ϵ ↘0 as ϵ ↘0. By making sure to operate at lower rates, quantizer saturation
can be pushed out of the range by using a large number of levels, without hurting the
lower rate performance. More on ECVQ and ECSQ is contained in the paper by Kim
and Modestino [11].
Error Sensitivity
Both Huffman and arithmetic coding are examples of variable-length coding (VLC).
In any coding system that uses VLC, there is an increased error sensitivity because a
bit error will probably cause loss of synchronization, with resultant incorrect decod-
ing until a fortuitous chance event occurs later in the bitstream, i.e., until by chance,
a correct decoding occurs. This is not the case in ﬁxed-length coding. As a result,
the extra compression efﬁciency that can be obtained using VLC leads to increased
sensitivity to errors. In practice, sync words and limitation on the length of VLC bit-
streams can effectively deal with this increased sensitivity problem, but only with
added complexity. A VLC coder that does not have such error-resilience features is
not really practical, as a single bit error anywhere in the image would most likely
terminate correct decoding for the rest of the image. So, all but the most pristine
channels or storage media will need some kind of error-resilience features. We talk
more about this in Chapter 13 for the image sequence or video case.
9.5 DCT CODER
DCT coders, the most representative of these being the international standard JPEG,
use the tools presented here. The input image frame is ﬁrst split into its Y, U, and
V components. The Y, or luma, component is then subjected to block-DCT with
nonoverlapping 8 × 8 blocks. After this the DCT coefﬁcients are split into one DC
and 63 AC coefﬁcients. They are then quantized by scalar uniform quantizers, with

9.5 DCT Coder
351
Input
Output
0
FIGURE 9.5–1
Illustration of UTQ. Note its central deadzone.
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
FIGURE 9.5–2
Illustration of zigzag or serpentine scan of AC coefﬁcients of 8 × 8 DCT.
possibly a central deadzone, with step size determined by a quantization matrix
that gives a preassigned step size to each of the 64 DCT coefﬁcients. A quantizer
with a central dead zone is called a uniform threshold quantizer (UTQ) as seen in
Figure 9.5–1. It can be speciﬁed by its number of levels L, step size 1, and the width
of its deadzone.6 After quantization, the DC coefﬁcient may undergo lossless DPCM
coding across the blocks, while the AC coefﬁcients are scanned in a certain zigzag or
serpentine scan (seen in Figure 9.5–2) and input to a 2-D run-value or 2-D Huffman
code. This code assigns codewords to pairs of messages indicating a run of xx zeros
terminated by a nonzero DCT value yy. This 2-D VLC is stored in a table in the coder
and the decoder.
There remains the problem of bit assignment to the DCT coefﬁcients. This can
be done as follows. We ﬁrst assume a quantizer model for the DCT coefﬁcient with
index k1,k2, say
Dk1,k2 = gσ 2
k1,k22−2Rk1,k2,
(9.5–1)
6The central deadzone is often useful for discriminating against noise in the coefﬁcients to be quantized.
Also, a central deadzone of 21 arises naturally in so-called bit-plane coding in embedded scalable
coding (see Section 9.6).

352
CHAPTER 9 Digital Image Compression
where σ 2
k1,k2 is the coefﬁcient variance (we assume zero mean) and Rk1,k2 is the num-
ber of bits assigned to this coefﬁcient. Due to orthogonality of the unitary DCT, the
total MSE in the reconstruction will be
D =
X
k1,k2
Dk1,k2,
and the average bitrate will be
R = 1
N2
X
k1,k2
Rk1,k2,
for an N × N DCT. To minimize the MSE, we introduce the Lagrangian parameter λ
and seek to minimize the function
D + λR =
X
k1,k2
Dk1,k2 + λ

R −
X
k1,k2
Rk1,k2

.
Taking the partial derivatives with respect to each of Rk1,k2 and setting them to zero,
we can solve for the parameter λ and eventually obtain the following solution:
Rk1,k2 = R + 1
2 log2(σ 2
k1,k2/σ 2
wgm),
(9.5–2)
with
σ 2
wgm ≜

5σ 2
k1,k2
1/N2
.
The resulting total MSE for the N2-pixel block then becomes simply
D = N2 gσ 2
wgm2−2R.
(9.5–3)
It is surprising that the result only depends on the average bits/pixel R and the geo-
metric mean of the coefﬁcient variances σ 2
wgm. There is some variation in MSE across
the N × N block due to the fact that pixels near the block boundaries do not have as
many neighbors to help reduce the coding error as do those in the middle of the block.
This effect is not large, though.
Example 9.5–1: Block-DCT Coding
The 252 × 256 monochrome cameraman image was coded by the DCT coder in VcDemo
[9] at approximately 1 bpp. An 8 × 8 block-DCT was used, with DPCM coding of the DC
coefﬁcient from block-to-block, and PCM coding of the AC coefﬁcients. Then VLC was
applied to the quantized coefﬁcient indices. The original image is shown in Figure 9.5–3
and the coded image at actual bitrate 0.90 bpp is shown in Figure 9.5–4.
The original block-DCT coefﬁcients are shown in Figure 9.5–5, and the quantized coef-
ﬁcients are shown in Figure 9.5–6. We can see that about half of the DCT coefﬁcients have

9.5 DCT Coder
353
FIGURE 9.5–3
Original 252 × 256 cameraman image.
FIGURE 9.5–4
DCT-coded image at 0.90 bpp.
been set to zero, yet the coded image still looks quite good. A close-up of Figure 9.5–4,
though, reveals coding distortion, especially around edges in the image. The PSNR is
29.4 dB.

354
CHAPTER 9 Digital Image Compression
FIGURE 9.5–5
DCT coefﬁcients of the cameraman image.
FIGURE 9.5–6
Quantized DCT coefﬁcients of the cameraman image.
9.6 SWT CODER
Due to their advantages of no blocking artifacts and somewhat better compression
efﬁciency, subband/wavelet coders have been extensively studied in recent years.
Here, we consider the popular dyadic or recursive subband decomposition, also called
the wavelet decomposition, which was mentioned previously. This transformation

9.6 SWT Coder
355
is followed by scalar quantization of each coefﬁcient. Rate assignment then splits
up the total bits for the image into bit assignments for each subband. Assuming an
orthogonal analysis/synthesis system is used, the average MSE distortion D due to
the individual quantizations with distortion Dm(Rm), with Rm bits assigned to the mth
subband quantizer, can be written as the weighted sum of the mean-square distortions
over a total of M subbands as
D =
M
X
m=1
Nm
N Dm(Rm),
(9.6–1)
where Nm is the number of samples in the mth subband, and the total number of pixels
(samples) is N. Similarly, the average rate is given as
R =
M
X
m=1
Nm
N Rm.
Note that these weighted averages over the M subbands are the same as unweighted
averages over the N subband samples.
If we model the individual quantizers as Dm = gσ 2
m2−2Rm, all with the same quan-
tizer efﬁciency factor g, then the optimal bit assignment for an average bitrate of R
bits/pixel, can be found with some assumptions, via the Lagrange multiplier method
[8], to yield
Rm = R + 1
2 log2
 
σ 2
m
σ 2wgm
!
, m = 1,...,M,
(9.6–2)
where the weighted geometric mean σ 2
wgm is deﬁned as
σ 2
wgm ≜
YM
m=1

σ 2
m
Nm/N
,
where N is the total number of samples, i.e., N = PM
m=1 Nm. The resulting MSE in
the reconstruction then becomes
D = g σ 2
wgm2−2R,
which is analogous to (9.5–3) of DCT coding.
If we were to compare a DCT coder to a SWT coder, using this bit assignment
method, all that would be necessary is to compare the corresponding weighted geo-
metric means of the variances. Note that there are a number of assumptions here.
First, we assume the quantizer model of (9.5–1), which is strictly only valid at high
bitrates. Second, rate-distortion theory says this type of coding is only optimum for
jointly Gaussian distributions. In practice, the quantizer model of (9.5–1) is only use-
ful as a ﬁrst approximation. More exact and complicated quantizer models have been
developed for speciﬁc applications.

356
CHAPTER 9 Digital Image Compression
Example 9.6–1: Subband Coding
The original 512 × 512 Lena image of Figure 9.6–1 was coded by the subband coding
module of VcDemo [9] at 1 bpp. Two levels of subband decomposition were obtained
using 16-tap separable ﬁlters, yielding 16 subbands. The DC subband was quantized
by DPCM using a 1 × 1-order NSHP linear predictor, while the remaining AC subbands
were coded by uniform quantization. VLC was then applied to the quantizer outputs. Bit
assignment was done according to the preceding procedure and resulted in the bit assign-
ments shown in Table 9.6–1, resulting in an overall average bitrate of 0.99 bpp. The
result is shown in Figure 9.6–2 where the actual bits used are 0.95 bpp and the PSNR
is reported to be 37.4 dB for a standard deviation error of 3.4 on this 8-bit image with
range [0,255].
FIGURE 9.6–1
Original 512 × 125 Lena—8 bits.
Table 9.6–1 Bit assignments
for 4 × 4 DCT with (0,0) in
the upper left-hand corner
4.64
3.41
2.74
1.81
1.81
0.00
1.50
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

9.6 SWT Coder
357
FIGURE 9.6–2
512 × 512 Lena coded by subband coding at 1 bpp.
Example 9.6–2: SWT Coder Performance
An example SWT image coder is reported in Choi [12] where a ﬁlter study is undertaken
to evaluate the performance of various subband/wavelet ﬁlters for compression of the
512 × 512 Lena image. We used the SB-FSSQ embedded coder [13] and compared
three ﬁlters using a 10-band dyadic subband decomposition. The ﬁlters considered were
Daubechies 9/7, Johnston’s 16B, and Adelson and Simoncelli’s 9 tap. The PSNR results
in Figure 9.6–3 show quite close performance with the top-performing PSNR curve corre-
sponding to the Daubechies 9/7 ﬁlter. Interestingly enough, the 16B ﬁlter came out on top
when we coded all 16 subbands rather than taking a dyadic or wavelet decomposition.
One additional comment is that the Daubechies 9/7 is said to have only a small amount of
ringing distortion when the LL subband is displayed alone, as happens in scalable coding.
In fact, a key property of subband/wavelet coders is their inherent scalability in resolution
to match display and communications link bandwidth requirements.
A more precise approach to bit assignment may be done as follows. First we write
the total rate as
R =
N
X
i=1
Ri,
where we assume N channels (or coefﬁcients). We now wish to ﬁnd the channel rate
assignment R = (R1,...,RN)T, such that the total distortion in (9.6–1) is minimized.

358
CHAPTER 9 Digital Image Compression
Lena 512× 512
10 Band
Solid : Daubechies 9/7
dashdot : Adelson and Simoncelli 9 tap
dashed : Johnston 16B
40
39
38
37
36
35
34
PSNR (dB)
33
32
31
300
0.1
0.2
0.3
0.4
0.5
Rate (bits/pixel)
0.6
0.7
0.8
0.9
1
FIGURE 9.6–3
Comparison of three subband/wavelet ﬁlters on the Lena image.
Effectively we seek the
R ≜argmin
R
N
X
i=1
Di(Ri).
This problem of minimization with constraint can be formulated with a Lagrange
multiplier as follows: D(R) ≜PN
i=1 Di(Ri), and then
f(R) = D(R) + λ
 
R −
N
X
i=1
Ri
!
.
Taking partial derivatives with respect to each Ri, we obtain the so-called constant
slope conditions
∂f
∂Rk
= ∂Di
∂Ri
−λ = 0,
meaning that at optimality we should operate at points on the channel distortion-rate
curves that all have the same slope,
∂Di
∂Ri
= λ
(9.6–3)
known as the equal slope condition. As we vary the value λ, we then can sweep out
the entire distortion-rate curve D(R).

9.6 SWT Coder
359
Example 9.6–3: Discrete Quantizer Set
Often in practice we only have a discrete number of bit assignment points because a lim-
ited number of ﬁxed quantizers can be used. Then there are no continuous distortion-rate
curves Di(Ri) for the individual channels. In that case, if we try all the combinations of
the various quantizers, we get a large but ﬁnite set of data, as sketched in Figure 9.6–4.
We notice that the best points are to the left and downward, because for a given rate,
these values have the lowest total distortion. This is called the convex hull of this set of
data points. At the highest total rate, we just use the smallest step size available among all
the quantizers. Likewise, the optimal lowest rate would be obtained by using the largest
step size for each quantizer, sometimes even leaving that channel(s) out altogether (i.e.,
assigning it zero bits). A method to solve for the intervening points based on tree opti-
mization is the BFOS algorithm [14], which is discussed in a problem at the end of this
chapter.
D
0
R
min
ΔD
ΔR
FIGURE 9.6–4
Illustration of D(R) in the case of a discrete number of quantizer step sizes.
Multiresolution SWT Coding
Figure 9.2–2 shows a dyadic SWT with three recursive applications of the basic sep-
arable four-band SWT. We thus generate three lower resolution images. We call the
ﬁrst generated LL subband LL1 and the second generated LL subband LL2, and so
on. For completeness we call the original image LL0. Here, we consider only two lev-
els of decomposition giving a lowest resolution of LL2, medium resolution of LL1,
and full resolution LL0. A scalable coder, embedded in resolution, can then be con-
structed by ﬁrst coding the lowest resolution data LL2, and then an enhancement band
for the subbands included in LL1, and ﬁnally a further enhancement band consisting
of the remaining subbands LH, HL, and HH, making up LL0. If we simply trans-
mitted these three scales, LLi, i = 0,3, we would have what is called simulcast, an

360
CHAPTER 9 Digital Image Compression
inherently inefﬁcient method of transmission. To make this scheme more efﬁcient, a
residual, or error, from the lower scale can be passed on to the next higher scale and
coded. A diagram illustrating this residual coding approach is shown in Figure 9.6–5.
Following partition based on scale, the low resolution coder C0 is a conventional
SWT image coder. The medium resolution coder C1 gets all the data up to the LL2 or
midspatial resolution. However, the coded error from C0 is also fed to the intermedi-
ate coder C1, which internally codes this error in place of the low resolution subbands
in LL2, thereby providing a reﬁnement of these data as well as coding with the new
midfrequency data. This process is repeated again for coder C2. Decoding proceeds
as follows. To decode low-resolution data, there is no change from conventional cod-
ing. To decode medium-resolution data, the delta1 enhancement layer is decoded and
used to supplement the low-resolution layer. A similar method then uses bitstream
delta2 to achieve the full spatial resolution.
One might ask why the coder error has to be propagated across the scales in a scal-
able coder; in other words, “Why not just code the high-frequency subbands (−LH,
−HL, and −HH) at the new scale?” The reason is that the rate assignment (9.6–2)
will give different numbers of bits to the LL2 subband depending on whether it is
part of the low-, the medium-, or the high-resolution image. This assigned number of
bits will generally increase with resolution, if the desired PSNR is around the same
number or is increasing with resolution, which is often the case. So recoding the cod-
ing error of the bands already transmitted effectively increases the bit assignment for
that band. Focusing on the LL2 subband, for instance, it is coded once, then recoded
for the medium-resolution image, and then recoded a second time for the high- or
full-resolution image. For a given subband component of the LL2 image, we can see
a concatenation of quantizers, as shown in Figure 9.6–6. The input signal is quantized
at rate R1 to produce bitstream b1. The rate 12 is then used to reﬁne the representa-
tion, with corresponding enhancement bitstream b2. The process is repeated again
with enhancement rate 13 and corresponding bitstream b3. A decoder receiving
only b1 can decode a coarse version of the input. With the addition of b2, a decoder
can reﬁne this coarse representation, with information about the initial quantization
error. Reception of the second enhancement layer can then reﬁne the representation
SWT
analysis
C0
image
LL2
delta1
delta2
LL1
full
low res
Σ
C2
C1
Σ
+
−
+
−
FIGURE 9.6–5
Multiresolution image coder with three decoded image resolutions (full, 1/2, and 1/4).

9.6 SWT Coder
361
Q1
Σ
+
−
Q2
Σ
+
−
Q3
Σ
+
−
R1
Δ3
Δ2
b1
b2
b3
Input
...
FIGURE 9.6–6
A cascade of three quantizers provides a quality scalable coding of input signal.
FIGURE 9.6–7
Illustration of scalar quantizer embedding. Solid lines are coarse quantizer decision levels.
Dashed and solid lines together constitute decision levels of the combined embedded
quantizer.
even more. This is then a rate-scalable way to transmit a pixel source, giving three
possible bitrates.
Such quantizers are said to be embedded, because the successively higher accu-
racy quantizers deﬁned in this way have the property that each stage reﬁnes the
decision levels of the previous stage. This is illustrated in Figure 9.6–7, which shows
the overall quantizer bins for two stages of embedding. Here, the solid vertical lines
are decision levels for the coarse quantizer Q1 and the dashed lines are decision levels
provided by Q2, thus yielding an equivalent quantizer consisting of all the decision
levels.
In an embedded quantizer, the ﬁrst stage can be a MSE optimal quantizer, but
the second and succeeding stages are restricted to using these same decision lev-
els plus some more, which would not normally be optimal. A key paper by Equitz
and Cover [15] considered the optimality of such cascaded or reﬁned quantizers and
concluded that the source must have a certain Markov property holding across the
reﬁnements for optimality. In particular, successive reﬁnement bits from the source
must be conditionally dependent only on the next higher signiﬁcance bit. In general,
a reﬁnement bit would be dependent on all the prior more signiﬁcant bits. Still, the
greatest dependency would be expected to occur on the next higher signiﬁcance bit.
If all of the quantizers are uniform and the step sizes are formed as decreasing powers
of two (i.e., 2−k), then the outputs bk become just the successively smaller bits from a
binary representation of the input value. In this case all of the quantizers are uniform
threshold and are said to be fully embedded.

362
CHAPTER 9 Digital Image Compression
Nondyadic SWT Decompositions
As mentioned earlier, the dyadic or wavelet decomposition does not always give the
best compression performance; so-called wavelet packets optimize over the subband
tree, splitting or not, to get the best coding quality for that frame [4]. For the case
when the splitting is allowed to change within an image (frame), they formulated
a so-called double tree algorithm to ﬁnd a spatially adaptive decomposition. Note,
however, that this is not an easy problem, since the best subband/wavelet decompo-
sition for a given image class will depend on the quantizing method used as well on
the entropy coder.
Fully Embedded SWT Coders
This class of coders follows the SWT by a variable-length coder that ﬁrst codes the
most signiﬁcant subband samples/coefﬁcients at their most signiﬁcant bit plane and
then proceeds down in signiﬁcance, bit plane by bit plane, both noting newly sig-
niﬁcant coefﬁcients and also reﬁning those coefﬁcients already coded at higher bit
planes. The generally acknowledged ﬁrst of these was embedded zero-tree wavelet
(EZW) by Shapiro [16]. The EZW coding algorithm was followed by the SPIHT
coder of Said and Pearlman [17]. The coding standard JPEG 2000, which can be con-
sidered a kind of fruition of this type of coder, is covered thoroughly in the text by
Taubman and Marcellin [8]. We also present the basic concept of the embedded zero
block coder (EZBC) [18] that has gained interest in scalable video coding research.
A common diagram of the subband structure of the SWT coder is shown in
Figure 9.6–8, which represents three stages of subband/wavelet decomposition. The
ﬁrst stage results in subbands LL1, LH1, HL1, and HH1. The second stage then
decomposes LL1 to yield subbands LL2, LH2, HL2, and HH2. Finally, there is a
third stage of decomposition that results in subbands LL3, LH3, HL3, and HH3.
Subband LL3 is not further decomposed here and plays the role of baseband. We
can then look at the sets of three high-frequency subbands {LHk, HLk, and HHk} for
k = 3,2,1, as enhancement data that permit an increasing scale (resolution) over the
HH1
LH1
HL1
HH2
LH2
HL2
LL3 
LH3 HH3
HL3
FIGURE 9.6–8
Illustration of subband/wavelet structure for the dyadic (also called octave band or wavelet)
decomposition, where the subbands XYk are at the kth stage.

9.6 SWT Coder
363
coarse scale representation in the baseband. It is important to realize that this diagram
is in the spatial domain, in that each subband is displayed spatially within each
subwindow XYk. Equivalently, we are looking at wavelet coefﬁcients.
As already mentioned, embedded coders proceed to code the data of Figure 9.6–8
by proceeding from most signiﬁcant bit (MSB) to least signiﬁcant bit (LSB), bit plane
by bit plane. Of course, this implies a quantization; in fact, it’s a UTQ, speciﬁed by
its step size 1 and deadzone centered on the origin and of width 21. Assuming for
the subband to be coded, that the LSB bit plane is 2b, then 1 = 2b. This LSB bit
plane is usually conservatively chosen at such a level that its quality will never need
to be exceeded.
Embedded Zerotree Wavelet (EZW) Coder
The EZW bit-plane coder proceeds from one quite signiﬁcant observation: at low and
even into medium bitrates, there are a lot of zeros in all but the MSB plane, and once
a zero is encountered in a bit plane at a given position, then for natural images it is
very likely accompanied by zeros at higher frequency subbands corresponding to the
same location. Shapiro deﬁned a special symbol, called zero-tree root [16], to code
all these zeros together. He regards the quantized data in Figure 9.6–8 as composing
separate data trees, each with roots in the base subband LLN (here N = 3), where
N is the number of levels of decomposition. Figure 9.6–9 shows the parent–child
relationships for the three trees of coefﬁcients (subband samples) growing out of one
spatial location in the LL3 subband.
In EZW coding, the ﬁrst step is to ﬁnd the MSB plane by computing the maximum
over all the subband magnitude values,
2T ≤max|ci, j| < 2T+1,
for subband data c(i,j).
HH1
LH1
HL1
HH2
LH2
HL2
LL3
LH3
HH3
HL3
FIGURE 9.6–9
Illustration of parent–child relationship for trees of coefﬁcients in EZW. Tree depth N = 3
levels.

364
CHAPTER 9 Digital Image Compression
Then we can normalize the data and introduce the binary notation
q[i,j] = s[i,j] + q0[i,j] + q1[i,j]2−1 + q2[i,j]2−2 + ··· ,
where s is the sign bit, q0 is the MSB, q1 is the next MSB, etc. The scale factor would
then be coded in a header, along with such factors as the image dimensions, pixel
depth, and color space. Two new symbols are used in the coder:
•
Zero-tree root (ZTR): ﬁrst zero that ﬂows down to all zero remaining tree at
current bit plane.
•
Isolated zero (IZ): a zero at this position in current bit plane, but not further down
the data tree.
Other messages used in the EZW coder are + and −to indicate the sign of a subband
value (wavelet coefﬁcient) and then 0 and 1 to represent the binary values of the
lower signiﬁcance bits.
We proceed iteratively, with two passes through the bit-plane data corresponding
to Figure 9.6–9. The ﬁrst pass is called the dominant pass and creates a list of those
coefﬁcients not previously found to be dominant. The second subordinate pass reﬁnes
those coefﬁcients found to be signiﬁcant in a previous dominant pass. The dominant
pass scanning order is zigzag, right-to-left and then top-to-bottom, within each scale
(resolution), before proceeding to the next higher scale. On subsequent dominant
passes, only those coefﬁcients not yet found to be signiﬁcant are scanned.
EZW Algorithm
A simpliﬁed version of the algorithm can be written as follows:
1. Set threshold T1 such that {2T1 > max|ci,j| and T1 is smallest such}, and normal-
ize the data.
2. Set k = 0.
3. Conduct dominant pass by scanning through the data qk(i,j) and for each newly
signiﬁcant pixel, output a message: +,−,IZ, and ZTR for conditional adaptive
arithmetic coder (CAAC).7 But ﬁrst time through, the coder must visit all the
samples.
4. Conduct a subordinate pass by scanning through the data qk(i,j) to reﬁne pixels
already known to be signiﬁcant in the current bit plane. Such bit values may be
output directly or also given to CAAC.
5. Set k ←−k + 1.
6. Stop (if the stopping criterion is met) or go to step 3.
The stopping criterion may be a ﬁxed number of bit planes coded, but is usually
a given numerical level. This is where the minimum step size of the quantizer comes
in. If we want to code at highest quality with a step size of 1 for each subband,
then we will stop at k = arg{Tk = 1}. If we decode the full bitstream, we get this
quality. If we only decode the earlier part of the bitstream, we get a lesser quality, i.e.,
7In the literature, the abbreviation CABAC is widely used. It stands for conditional adaptive binary
arithmetic coder. The acronym CAAC is not widely used.

9.6 SWT Coder
365
corresponding to the quantizer step of whatever bit plane we stop at. Such a coder is
thus fully bit-plane embedded and permits ﬁne-grain scalability in quality (PSNR) or
alternatively in bitrate. To achieve a ﬁxed bitrate, one simply stops decoding when
that number of bits is reached.
We note that a scalable image coder really has three parts. First is the precoder,
which codes the data into a coded image ﬁle archive with only a maximum bitrate or
quality in mind. The second part is the extractor, which pulls bits from the precoder’s
archive. These two steps together make up the scalable encoder. The last part is the
decoder.
Set Partitioning in Hierarchical Trees Coder
The famous set partitioning in hierarchical trees (SPIHT) coder of Said and Pearlman
[17] replaces the zero trees with three lists that indicate signiﬁcance and insigniﬁ-
cance of sets of pixels and their descendents. Its generally higher performance
compared to EZW has made it the coder of choice for many applications, and it
is currently the one to beat in the research literature. The parent–child relation in
SPIHT is somewhat different from EZW, as shown in Figure 9.6–10. In the coarsest
subband LLN, we have sets of 2 × 2 pixels that each initiate three trees, in horizontal,
diagonal, and vertical directions. Pixels in locations (2i,2j) do not have descendants.
SPIHT introduces three lists of subband pixels (wavelet coefﬁcients):
1. List of insigniﬁcant pixels (LIP)
2. List of signiﬁcant pixels (LSP)
3. List of insigniﬁcant sets (LIS)
The LIS is further broken down into two types of sets of insigniﬁcant pixels: type A,
in which all descendants are zero, and type B, in which all grandchildren and further
descendants are zero.
HH1
HH2
LH1
HL1
LH2
HL2
*
FIGURE 9.6–10
Illustration of SPIHT parent–child dependencies for a three-level tree.

366
CHAPTER 9 Digital Image Compression
To describe the algorithm more easily, we deﬁne new sets with respect to a pixel
at location [i,j]:
•
C[i,j] denotes its children.
•
D[i,j] denotes its descendants.
•
G[i,j] denotes its grandchildren.
Clearly, we have G = D −C, where the sign −denotes set subtraction. Also, for a
given set of pixels B, we deﬁne the indicator function
Sk(B) =
1,
B contains a pixel with qk[i,j] = 1,
0,
otherwise,
where k is the bit plane index and 0 is the MSB.
We are now in a position to state the SPIHT coding algorithm.
SPIHT Coding Algorithm
1. Set k = 0, LSP= φ, LIP = {all coordinates [i,j] of LLN}, LIS = {all coordinates
[i,j] of LLN that have children}.
2. Signiﬁcance pass
•
For each [i,j] ∈LIP, output qk[i,j]. If qk = 1, output sign bit s[i,j] and move
[i,j] to end of LSP.
•
For each [i,j] ∈LIS:
–
If set is of type A, output Sk(D[i,j]). If Sk(D[i,j]) = 1, then:
∗
For each [l,m] ∈C[i,j], output qk[l,m]. If qk[l,m] = 0, add [l,m] to the
LIP. Else, output s[l,m] and add [l,m] to LSP.
∗
If G[i,j] ̸= φ, move [i,j] to end of LIS as set of type B. Else delete [i,j]
from LIS.
–
If set is of type B, output Sk(G[i,j]). If Sk = 1, then add each [l,m] ∈C[i,j]
to the end of the LIS as sets of type A and delete [i,j] from LIS.
3. Reﬁnement pass
•
For each [i,j] ∈LSP, output qk[i,j] using old LSP.
4. Set k ←k + 1 and go to step 2.
Note that for sets of type A, we process all descendents, while for sets of type B,
only grandchildren are processed. The SPIHT decoder is almost identical with the
encoder. Initialization is the same, then the data are input from the binary codestream
at each point in the coding algorithm where an output is indicated. In this way the
decoder is able to process the data in the same sequence as the coder, and so, no
positional information has to be transmitted. While this is very efﬁcient, one bit error
somewhere in the SPIHT codestream can make the decoder lose track and start pro-
ducing useless results. In particular, a bit error in the signiﬁcance pass will cause such
a problem. Of course, it can be said that any image coder that uses VLC potentially
has this problem. The SPIHT coder outputs can also be arithmetic coded for added
coding efﬁciency.

9.6 SWT Coder
367
Embedded Zero Block Coder
The main idea in the embedded zero block coder (EZBC) is to replace the zero tree
across the scales (subbands) with separate quadtrees within each subband to indicate
the signiﬁcance of the data. This permits resolution scalability to accompany quality
or SNR scalability in a more efﬁcient manner than with zero trees, which necessar-
ily carry information on all scales higher than the present one. We need some new
notation to describe the quadtrees.
We denote quantized subband pixels as c(i,j) with MSB m(i,j) and pixel value
in subband k written as ck(i,j). Also note that as quadtree level l goes up, quadtree
resolution goes down, with level l = 0 being full resolution.
Deﬁnition 9.6–1: EZBC Algorithm Notation
QTk[l](i,j) ≜quadtree data at quadtree level l, in subband k, at position (i,j),
K ≜number of subbands,
Dk ≜quadtree depth for subband k,
Dmax ≜maximum quadtree depth,
LINk[l] ≜list of insigniﬁcant nodes for subband k and quadtree level l,
LSPk ≜list of signiﬁcant pixels from subband k,
Sn(i,j) ≜signiﬁcance of node (i,j) at bit plane n,
Sn(i,j) ≜
(
1,
if n ≤m(i,j),
0,
else.
Coder initialization:
QTk[0](i,j) = |ck(i,j)|
QTk[l](i,j) = max{QTk[l −1](2i,2j),QTk[l −1](2i −1,2j),QTk[l −1](2i,2j −1),
QTk[l −1](2i −1,2j −1)}.
LINk[l] =
{(0,0)},
l = Dk,
φ,
otherwise.
EZBC Coding Algorithm
1. Initialization
•
LINk[l] =
{(0,0)},
l = Dk,
φ,
otherwise,
•
LSPk = φ, ∀k,
•
n =

log2 maxk,i,j{|ck(i,j)|}

. (Note that here the bit-plane index is n and that
it decreases with successive coding passes.)

368
CHAPTER 9 Digital Image Compression
2. For l = 0,Dmax:
•
For k = 0,K −1
– CodeLIN(k,l)
3. For k = 0,K −1
•
– CodeLSPk
•
n ←n −1 and go back to step 2. Stop when hit target bits.
We now describe the functions CodeLIN(k,l) and CodeLSPk.
CodeLIN(k,l)
•
For each (i,j) ∈LINk[l]:
– code Sn(i,j).
– If (Sn(i,j) = 0), (i,j) remains in LINk[l].
– Else:
∗
If (l = 0), then code the sign bit of ck(i,j) and add (i,j) to LSPk.
∗
Else CodeDescendentNodes(k,l,i,j).
CodeDescendentNodes(k,l,i,j)
•
For each node (x,y) ∈{(2i,2j), (2i,2j + 1), (2i + 1,2j), (2i + 1, 2j + 1)} of quad-
tree level l −1 in subband k:
– code Sn(x,y).
– If (Sn(x,y) = 0), add (x,y) to LSPk.
– Else CodeDescendentNodes(k −1,l,i,j).
CodeLSPk
•
For each pixel (i,j) ∈LSPk, code bit n of |ck(i,j)|.
The word code in this algorithm is like the word output in the SPIHT algorithm,
in that they both specify algorithm output. Here, though, we take the algorithm output
and send it to a CAAC rather than outputting it directly to the channel.
Context-Based AC in EZBC
The context-based AC used in EZBC is described next with reference to
Figure 9.6–11. The context model is binary and is based on the quadtree neighbors
of the current datum. There are eight nearest neighbors from the current subband and
quadtree levels, plus one context node from the next higher subband level and next
lower quadtree level, which has the same position. The actual value of this 9-bit con-
text is determined by the values encountered at the previous location, as scanned in
a normal progressive raster. In this way, dependence between bit planes, and with
nearby prior encoded neighbors, is not lost and so efﬁcient coding therefore results.
Instead of using 29 states, states with similar conditional statistics are judiciously
merged similarly to the method used in EBCOT and JPEG 2000. Keeping the number

9.7 JPEG 2000
369
Quadtree level
Subband
level
l=0
l =1
o
o o
o
o
o
o
o
o Context nodes
o
Current node
FIGURE 9.6–11
Illustration of context modeling for AC in EZBC.
of these states small permits more rapid and accurate adaptation to varying statistics
and hence better compression.
9.7 JPEG 2000
The JPEG 2000 standard is based on embedded block coding with optimal truncation
(EBCOT) [19] and is similar to the other embedded algorithms already discussed,
with the main exception that the subband values are separated into code blocks of
typical size 64 × 64, which are coded separately. Individual rate-distortion functions
are established for each code block, and the overall total bits are apportioned to the
code blocks based on the equal slope condition (9.6–3), thereby funneling additional
bits to wherever they are most needed to maximize the decrease in the total distortion,
assumed to be approximately additive over the code blocks. So, some optimality is
lost by breaking up the code blocks, but increased performance is obtained by the
functionality of being able to send bits to code blocks where they are most needed.8
The “optimal truncation” part of the EBCOT acronym refers to this bit assignment,
since the code blocks’ bitstreams are embedded, and thus can be truncated to get
reduced quality points. It also follows that if each code block is bit-plane coded
down to a sufﬁciently low bit plane, then the optimization can be done after the
bit-plane coding in a “postcompression” pass. Then only occasionally and at very
high total bit values, would the binary version of the code block not be of sufﬁcient
accuracy.
EBCOT does not use any quadtree coding of signiﬁcance at each bit-plane level.
Rather each bit plane is coded in three so-called fractional passes. With reference
to Figure 9.7–1, wavelet coefﬁcients (subband samples) are scanned in a horizontal
8The relatively small code blocks permit a region of interest (ROI) type of scalability, wherein higher
accuracy representation may be given to certain high-interest areas of the image.

370
CHAPTER 9 Digital Image Compression
Code-block width
FIGURE 9.7–1
An illustration of how JPEG 2000 coder scans a four-row stripe in a code block of wavelet
coefﬁcients (subband samples).
stripe fashion with a stripe width of 4 pixels vertically. A signiﬁcance state is kept
for each coefﬁcient, and the ﬁrst fractional pass signiﬁcant propagation visits just
those coefﬁcients that neighbor a signiﬁcant coefﬁcient, since they have the highest
likelihood of becoming signiﬁcant and, when they do, will reduce the distortion the
most for a given incremental number of bits. When a signiﬁcant value is detected,
its sign is coded again with a context-based arithmetic coder. A second fractional
pass magnitude reﬁnement codes those coefﬁcients already found to be signiﬁcant
in a previous bit plane. The last cleanup pass treats those coefﬁcients that have not
yet been coded, as they have the least expected contribution to average distortion
reduction. Here, a special run mode is used to efﬁciently deal with the expected runs
of zero bit-plane values.
In order to make the the EBCOT method practical, there must be relatively efﬁ-
cient means to approximate the rate-distortion functions of the code blocks, and
also to do the optimized truncation. Means of doing both are included in [8], which
also treats certain simpliﬁcational differences between EBCOT and the JPEG 2000
standard.
9.8 COLOR IMAGE CODING
As we have seen in Chapter 6, color images are simply multicomponent vector
images, commonly composed of red, green, and blue components. Normally images
are stored in the transformed YCrCb color space, where often the two chroma chan-
nels Cr and Cb have been subsampled. Common subsampling strategies are labeled
4:2:2 and 4:2:0 and are illustrated in Figure 9.8–1, with 4:2:2 referring to the hori-
zontal sampling rate of the Y, Cr, and Cb components, respectively, and meaning that

9.8 Color Image Coding
371
there are only two Cr and Cb samples for each four luma Y samples, i.e., the chroma
Cr and Cb signals are decimated horizontally by the factor 2.
The commonly used notation 4:2:0 extends the chroma decimation into the ver-
tical direction, again by the factor 2, meaning chroma components are decimated
2 × 2. The justiﬁcation for this decimation is that, as we have seen in Chapter 6, the
HVS is less sensitive to spatial high frequencies in color than in luma. Sometimes,
but not always, the chroma channels have been preﬁltered to avoid aliasing error
before being converted to 4:2:2 or 4:2:0. For deﬁniteness, the full color space, with
no chroma subsampling, is denoted 4:4:4.
In Figure 9.8–1, we have shown the chroma subsamples sited at the locations of
a corresponding luma sample, but this is not commonly the case. Figure 9.8–2(b)
shows an example of 4:2:0 with the chroma samples situated more symmetrically
between luma samples and is used in the DV and HDV digital video tape standards,
as well as in the DVD and Blu-ray video disks. Note that conversion of video with
the color format of Figure 9.8–2(a) to that of Figure 9.8–2(b) can be effected by a
unity gain chroma ﬁlter approximating a half-sample delay in each dimension.
Normally, each of the three components Y, Cr, and Cb are separately coded by
any of the methods of this chapter, just as though they were separate gray-level or
monochrome images. The new issue, thus, is to apportion the total bitrate to the three
color components. Due to the color subsampling that is almost universal, the number
of luma samples is twice that of each chroma component in 4:2:2 and four times in
4:2:0. Usually the chroma components are easier to code than the luminance, because
of their general lack of detail and often lowpass character. Efforts have been made
to come up with a total distortion criterion for the three components [20]. A general
4:2:2
x x x x x
x x x x x
x x x x x
4:2:0
x x x x x
x x x x x
x x x x x
FIGURE 9.8–1
Illustration of color subsampling strategies 4:2:2 and 4:2:0.
4:2:0
4:2:0
(a)
O
O
O
X X X X
X X X X
O
O
O
X X X X
X X X X
(b)
X X X X
X
X
X X X X
XO
XO
X
X
XO
XO
FIGURE 9.8–2
An illustration of different possible sitings of chroma subsamples in 4:2:0.

372
CHAPTER 9 Digital Image Compression
Table 9.8–1 PSNR for Some JPEG 2000 Test Set Images at 0.55 bpp
PSNR (dB)
Lena
Barbara
Goldhill
Cafe
Bike
Hotel
SPIHT
37.2
31.4
33.1
26.5
33.0
33.6
EZBC
37.6
32.2
33.6
27.1
33.7
34.7
JP2K
37.2
32.1
33.2
26.7
33.4
34.0
EBCOT
37.3
32.3
33.2
26.9
33.5
34.1
SPECK
37.1
31.5
33.0
26.3
32.7
—
good practice, however, is to assign bits to the components so as to make the quantizer
step sizes about equal in the three color components, with this then generating the
most pleasing visual result, and also somewhat higher PSNR for the two chroma
components as compared to that of the luma channel.
Scalable Coder Results Comparison
Table 9.8–1, taken from [21], lists PSNR comparisons between some of the SWT
coders, using a wide range of natural images that were part of the test set of the JPEG
2000 competition. If there is more than one version, we compare the scalable version.
While the PSNR performance of all the coders are comparable, the advantage goes to
the EZBC coder in this comparison. This is somewhat surprising in that the EBCOT
and JPEG2000 (JP2K) coders are rate-distortion optimized. However, the advantage
of the rate-distortion optimization is said to show up mainly in non-natural or com-
posite images composed of combined images, text, and graphics [8]. Of course, in
that case, it may be more optimal still to decompose such a complex composite image
into its constituents and then code the graphics and the text separately.
9.9 DIRECTIONAL TRANSFORMS
New directional transforms have been developed for image coding. In one approach
[22], an image block to be transform coded is ﬁrst scanned in one of eight directions
and then subject to 1-D DCT. After this step, 1-D DFTs are conducted on the rows or
columns to complete the directional DCT transform. In addition to DC, horizontal,
and vertical modes, the six other directions are shown in Figure 9.9–1, from [22].
Depending on the chosen mode, we see that the 1-D DCTs are of quite variable length,
ranging from 1 to 8 and then down to 1 again for mode 3 on the 8 × 8 blocks shown.
Horizontal 1-D DCTs are used as the second stage transform in the top three
modes, but vertical 1-D DCTs are used in the bottom three modes. Coded results
demonstrate both objective (PSNRs) and subjective merit to this approach. A reﬁned
and extended version of this method, termed direction-adaptive partitioned block
transform, is presented in [23] and shows considerable improvement over the earlier
JPEG standard (cf. Section 9.5).

9.9 Directional Transforms
373
Mode 6 (horizontal-down)
Mode 7 (vertical-left)
Mode 8 (horizontal-up)
Mode 5 (vertical-right)
Mode 3 (diagonal down-left)
Mode 4 (diagonal down-right)
FIGURE 9.9–1
Illustration of six new modes for directional DCT in [22]. ( c⃝2008 IEEE)
In an alternative approach, the SWT is used in a lifting implementation to perform
directional ﬁltering in a subband/wavelet image coder [24]. First, the local correla-
tion is estimated, then the prediction operator in a 1-D lifting-based SWT is warped
to this direction. Speciﬁcally, they modify the lifting implementation (see problem 14
in Chapter 5) of a vertical 1-D SWT by predicting the polyphase samples on odd lines
from the neighboring polyphase samples on even lines with angles θi,−4 ≤i ≤+4,
using sinc function interpolation to quarter-pixel accuracy. Then they do the same
kind of modiﬁcation to the horizontal 1-D SWT to provide their adaptive directional
lifting (ADL) transform. Fixed ADL transforms are used in variable-size quadtree
blocks with decomposition determined by BFOS search [14] using a Lagrangian cost
function. An indication of the directions and segmentation is shown in Figure 9.9–2
from [24]. The ADL transform method was implemented into an EBCOT coder
and compared to the JPEG 2000 coder on several test images. Figure 9.9–3a and
Figures 9.9–3b show the comparison results for coding the Barbara image at 0.3 bpp
with their ADL version of the 5/3 SWT, with the JP2K result in Figures 9.9–3a and
ADL result in Figure 9.9–3b.
The PSNR comparison here is approximately 28 dB for the JPEG 2000 coder
using the 5/3 SWT and about 30 dB for the ADL coder. Since the ADL transform
is generally aligned with image structure, ringing artifacts are reduced and coding
efﬁciency is improved versus the conventional 2-D separable SWT, for which Ding
et al. [24] introduce the term rectilinear since the ADL is separable, too, just not

374
CHAPTER 9 Digital Image Compression
FIGURE 9.9–2
Illustration of ADL quadtree and chosen directions overlayed on the Barbara image. (from
Ding et al. [24] c⃝2007 IEEE)
FIGURE 9.9–3a
Barbara JPEG 2000 coded with 5/3 SWT at 0.3 bpp. (from Ding et al. [24] c⃝2007 IEEE)
separable in strict horizontal and vertical terms. An ADL approach using direc-
tional multidimensional ﬁlters is presented in [25] and shows good performance
with reduced complexity. A review article on both these approaches to directional
transforms appears in [26].

9.10 Robustness Considerations
375
FIGURE 9.9–3b
Barbara ADL coded with 5/3 SWT at 0.3 bpp. (from Ding et al. [24] c⃝2007 IEEE)
9.10 ROBUSTNESS CONSIDERATIONS
Almost all the speciﬁc image coding algorithms that we have discussed thus far are
not robust to errors. This is because they use VLC. The Huffman code tree is com-
plete, in the sense that all binary strings are possible. So a decoder, upon encountering
a bit error, will start decoding erroneous values from that point forward, loosely
referred to as decoder failure. Practical coding algorithms additionally have mark-
ers such as end of block (EOB) and other sync words, which serve to terminate or at
least control the error propagation. These sync words are inserted into the coder bit-
stream at appropriate points. Also, alternative non-Huffman VLCs may be employed,
whose code tree, while reasonably efﬁcient, is not complete. Then illegal codewords
can be detected and used to reset the decoder.
In practical coders, positional information and the block length in pixels are
included in headers that start each code block. Also an overall header precedes the
entire bitstream, giving image size, bit depth, color space, etc. Finally, the decoder
needs to know how to act when it encounters a problem, e.g., if the code block is
of ﬁxed size, and the EOB comes too soon, indicating an error situation. A com-
mon response would be to discard the data in the current block, insert black into
the now-missing block, and search for the next sync word, and start decoding again
at that point. In essence, it is the syntax or structure of a coder, a structure that
is created by the insertion of header and sync word information, that enables the
decoder to detect errors. Then, if the decoder knows how to react to these errors,
it can be considered at least somewhat robust. Of course, if errors are expected

376
CHAPTER 9 Digital Image Compression
to occur with some frequency, then error control coding and acknowledgement-
retransmission (ACK/NACK) schemes can be used. Also, error concealment schemes
can be employed at the decoder. More on this in the context of video coding will be
presented in Chapter 13.
CONCLUSIONS
Image source compression is important for the transmission and storage of digital
image and other 2-D data. Compression can be lossless, consisting of just an inver-
tible transformation and variable-length coding, or lossy, employing a quantizer.
Most practical applications to natural images involve lossy coding due to the signif-
icantly higher compression ratios that can be achieved. Lossless coding is generally
conﬁned to medical images, works of art, and other high-value images. Its compres-
sion ratios are very modest by comparison, typically around two to one or so. In
between these two, there is the category visually lossless that means the HVS cannot
detect the loss (in most cases); i.e., the loss is below the JND, and used in digital
cinema compression.
The old standard image compression algorithm JPEG uses the DCT and is non-
scalable. The more recent international standard JPEG 2000 features an SWT and
embedded scalable VLC. There is also the option for lossless coding in this cod-
ing algorithm. We brieﬂy covered some new image techniques based on adaptive
directional transforms, both DCT and SWT.
Lastly, we introduced the issue of coder robustness, achieved only by leaving in or
reinserting some controlled redundancy that allows the decoder to detect and recover
from even isolated single bit errors.
PROBLEMS
1. Interpret 8 × 8 block DCT image coding as a type of subband/wavelet coder
with the DCT basis functions acting as the ﬁlters. What is the number of sub-
bands? What is the 2-D decimation ratio? Do the ﬁlter supports of each subband
overlap on the plane after subsampling? What is the passband of the ﬁlter used
to produce the lowpass subband—i.e., the one including frequency (0,0)? What
is its minimum stopband attenuation?
2. In an optimal MSE scalar quantizer, the decision values must satisfy
di = 1
2(ri + ri+1),
in terms of the neighboring representation values. This was derived as a station-
ary point, using partial derivatives, but here you are to conclude that it must be
so, based on ﬁrst principles. Hint: Consider the effect of moving the decision
point di to the right or left. What does this do to the quantizer output?

Problems
377
3. An approximation had been derived (Panter and Dite, 1949) for so-called ﬁne
scalar quantization wherein the pdf of the signal x is approximately constant
over a quantization bin, i.e.,
fx(x) ≈fx
1
2 (di−1 + di)

, for di−1 < x ≤di,
with di−1 ≈di,
resulting in the ﬁne quantization approximation formula
D =
1
12L2


+∞
Z
−∞
fx(x)1/3dx


3
.
(a) Evaluate this approximate distortion in the case where x is N(0,σ 2).
(b) Evaluate in the case where x is Laplacian distributed,
fx(x) = 1
α exp−2
α |x|,
for
−∞< x < +∞, with α > 0,
and where α =
√
2σ.
4. Work out the logarithmic bit assignment rule of (9.5–2) and (9.5–3). What are
its shortcomings?
5. This problem concerns optimal bit assignment across quantizers after a unitary
transform, i.e., the optimal bit allocation problem. We assume
R =
X
rn
and
D =
X
dn,
where n runs from 1 to N, the number of coefﬁcients (channels). Here, rn and dn
are the corresponding rate and distortion pair for channel n. Assume the allowed
bit allocations are M in number, 0 ≤b1 < b2 < ... < bM, and that the compo-
nent distortion-rate functions dn = dn(bm) are given for all n and for all m and is
assumed to be convex.
(a) Argue that the assignment rm = b1, for all m, must be on the optimal
assignment curve as the lowest bitrate point, at total rate R = Nb1.
(b) Construct a straight line to all lower distortion solutions, and argue that
the choice resulting in the lowest such line must also be on the optimal
distortion-rate curve.
(c) In part (b), does it sufﬁce to try switching in the next higher bit assignment
b2 for each channel, one at a time?
6. Making use of the SQ model
D = gσ 22−2R,

378
CHAPTER 9 Digital Image Compression
ﬁnd the approximate MSE for DPCM source coding of a random ﬁeld satisfying
the AR model
x(n1,n2) = 0.8x(n1 −1,n2) + 0.7x(n1,n2 −1) −0.56x(n1 −1,n2 −1)
+ w(n1,n2),
where w is a white noise of variance σ 2
w = 3. You can assume that the coding is
good enough so that bx(n1,n2), the (1,0) step prediction based on ex(n1 −1,n2),
is approximately the same as that based on x(n1 −1,n2) itself.
7. Repeat the VQ of Example 9.3–3 with a different initial guess for the rep-
resentation vectors: r1 =
 0
0

,r2 =
 1
0

,r3 =
 1/2
−1

. Compute the average
least-squares error in each case, and determine which local minimum is better,
the one found here or the one in Example 9.3–3.
8. Consider image coding with 2-D DPCM, but put the quantizer outside the pre-
diction loop (i.e., complete the prediction-error transformation ahead of the
quantizer operation). Discuss the effect of this coder modiﬁcation. If you design
the quantizer for this open-loop DPCM coder to minimize the quantizing noise
power for the prediction error, what will be the effect on the reconstructed sig-
nal at the decoder? (You may assume the quantization error is independent from
pixel to pixel.) What should the open-loop decoder be?
9. Consider using the logarithmic bit assignment rule, for Gaussian variables,
Bi = B
N + 1
2 log2
σ 2
i
σ 2gm
,i = 1,...,N,
with σ 2
gm ≜
 5iσ 2
i
1/N and N = number of channels (coefﬁcients). Apply this
rule to the following 2 × 2 DCT output variance set:
coef. map =
 00
10
01
11

and corresponding variances =
 22
4
8
2

.
Assume the total number of bits to assign to these four pixels is B = 16. Resolve
any possible negative bit allocations by removing that pixel from the set and
reassigning bits to those remaining. Noninteger bit assignments are OK since
we plan to use variable-length coding. Give the bits assigned to each coefﬁcient
and the total number of bits assigned to the four pixels.
10. In this problem, you are to use the software VcDemo [9] to perform both DCT
and subband coding video compression on the Lena 512 × 512 monochrome
image at a range of bitrates: 0.2–1.0. Then plot the MSE and PSNR results versus
bitrate. Compare the results.
11. Reconcile the constant slope condition for bit assignment that we get from opti-
mal channel (coefﬁcient) rate-distortion theory in the continuous case, i.e., the
constant slope condition dDi/dRi = λ, with the BFOS algorithm [14] for the

Appendix on Information Theory
379
discrete case, where we prune the quantizer tree of the branch with minimal
ratio of distortion increase divided by rate decrease, i.e., argmini |1Di/1Ri|.
Hint: Consider pruning a tree, as the number of rate values for each branch
approaches inﬁnity. Starting at a high rate, note that after a possible initial
transient, we would expect to be repeatedly encountering all the channels
(coefﬁcients, values) at a near-constant slope.
12. Show that the distortion and rate models in (9.6–1) and (9.6–2) imply that in
scalable SWT coding of two resolution levels, the lower resolution level must
always be reﬁned at the higher resolution level if
σ 2
wgm(base) > σ 2
wgm(enhancement),
where σ 2
wgm(base) is the weighted geometric mean of the subband variances at
the lower resolution and σ 2
wgm(enhancement) is the geometric mean of those
subband variances in the enhancement subbands.
APPENDIX ON INFORMATION THEORY
In order to deal with efﬁcient transmission or storage of images, we need a measure of
information to enable us to quantify our compression results. Clearly this is true in the
transmission of information to a distant source over a communications channel, but
less clearly the same situation is present when we store and later retrieve information
in a memory via the so-called storage channel. Excellent texts on information theory
are Gallagher [1] and Cover and Thomas [5].
Information Measure
Assume that a source has M messages to send to a remote location. We could say that
the amount of information is M or we could use any monotonic increasing function
of M to measure the information. Now consider two independent sources, the ﬁrst
one with M messages and the second one also with M messages. The total number
of messages is now the square M2. On the other hand, one feels intuitively that the
amount of information has only doubled here. If we adopt a logarithmic measure
of information, then the amounts of information in the two cases are log2 M and
2log2 M, respectively, consistent with a doubling of information.
In our seemingly digital world, we transmit bits and so may like to represent
information in terms of bits. For M messages, we would need about log2 M bits to
represent them; for example, for M = 8, we represent each message with a sym-
bol consisting of a binary string of 3 = log2 8 bits. These binary symbols would
be sent over a channel, and then the receiver would perform the inverse trans-
formation back to the message set, such that receiving the binary string 010, the
receiver would output the message 3. More generally, if two independent sources
had different message numbers, say M1 and M2, then their total or combined
information would be, in terms of our logarithmic measure, log2 M1 + log2 M2,

380
CHAPTER 9 Digital Image Compression
Message
Symbol
Message
Symbol
1
000
5
100
2
001
6
101
3
010
7
110
4
011
8
111
and information would be an additive quantity expressed in bits. This is essentially
the information concept of Hartley [27] that existed prior to Shannon’s classic work
[28], Claude Shannon being considered the father of information theory. We see some
limitations in Hartley’s approach; for example, what if the two sources are not inde-
pendent? Certainly in the extreme case where two sources are totally dependent, the
combined and single information should be the same. In between, we would expect
a good measure of combined information to increase as the dependence between
two sources decreases, reaching the additive (double) value in the limit of total
independence of the sources.
There is another problem with Hartley’s measure log2 M, in that there is no rea-
son to believe that it is the smallest number of bits necessary for a general source
with M messages. In the interests of compression and efﬁciency, we want a measure
of the least amount of bits necessary to represent or store a message source. Now,
considering again just one source with M messages, Hartley’s information measure
represents each message with the same number of bits log2 M. If all the messages
are equally likely, this is reasonable, but what if they are not? Let’s take the case of
a source of M messages {m1,m2,...,mM} with probabilities p1,p2,...,pM satisfying
pi ≥0, for each i and PM
i=1 pi = 1. Again, our intuition tells us that improbable mes-
sages carry more information than probable ones. One could say that it is no news
when it snows in Alaska! So some kind of increasing function of 1/pi may be a good
measure of information. Interestingly, in the case where all M messages are equally
likely, we have 1/pi = M, and thus we can match Hartley’s information measure with
log2(1/pi) bits for each message.
Deﬁnition 9.A–1: Source Entropy
The overall average information or entropy of source X then becomes
H(X) ≜
M
X
i=1
pi log 1
pi
= E

log
1
p(X)

.
The Shannon information measure source entropy was ﬁrst presented in his classic
paper [28], where he showed this entropy to be the unique measure satisfying some
simple continuity and compound message properties like what we have just explored.
We will take Shannon’s entropy as our information measure in this course.
We usually choose 2 as our logarithm base (i.e., log = log2), and we speak of
bits. However, other choices are sometimes used. For example, for four-value logic,

Appendix on Information Theory
381
it would be convenient to measure information using log4, and we could speak of
‘quads.’ Sometimes for mathematical simplicity the logarithm base is chosen as e,
giving natural logarithms (i.e., log = ln), and we speak of nats. Converting between
two scales (e.g., nats and bits) is simple. Just use the relation log2 x = lnx/ln2. Then
H in bits equals H in nats divided by ln2 = 0.69315, or equivalently multiplied by
log2 e = 1.44267. Note that H in bits is larger than H in nats (by about 44%) because
the base 2 is smaller than the base e.
Example 9.A–1: Discrete Message Source
Again, we take the case of a source of M messages {m1,m2,...,mM} with probabilities
p1,p2,...,pM satisfying pi ≥0, for each i and PM
i=1 pi = 1. In particular, we take M = 4
with probabilities as given in the following table:
p1
p2
p3
p4
1/2
1/4
1/8
1/8
Then the information per symbol becomes
−logp1
−logp2
−logp3
−logp4
1
2
3
3
and we can construct binary strings to represent the messages, called source codewords,
using the tree structure in Figure 9.A–1.
p(1)=1/2
p(2) = 1/4
p(3) = 1/8
p(4) = 1/8
1
0
0
0
1
1
0
10
110
111
Message
probabilities
Binary
codewords
Code tree
Codeword
lengths
1
3
2
3
FIGURE 9.A–1
A convienent tree structure to calculate binary codewords given message probabilities.
Calculating the average codeword length l, we ﬁnd for this example
l = 1
2 × 1 + 1
4 × 2 + 1
8 × 3 + 1
8 × 3
= 1.75 bits
Note that the individual codeword lengths are equal to −logpi, and so the average code-
word length is equal to the entropy here. In the general case, this is not so. It only occurs
here because this source is dyadic; that is, all individual probabilities are of the form
pi = 2−k for some positive integer k.

382
CHAPTER 9 Digital Image Compression
We note that Shannon’s entropy, here 1.75 bits, is less than Hartley’s information
measure of 2 bits and that this efﬁciency is achieved in this simple example by the
variable-length binary codewords shown. Here are some properties of entropy.
Lemma 1 Entropy
For any source (random variable) X, we have H(X) ≥0.
Proof Consider a probability p. We know 0 ≤p ≤1, and so log2 p ≥0. Then
H(X) = E
h
log 1
p
i
≥0.
Entropy is a measure of uncertainty. Consider a random variable X taking on M
ﬁxed values. If one of the values has probability 1, then all the remaining M −1
values have probability zero, and H = 1log2 1 + 0log2 ∞= 0 since we take 0log2 ∞
at its limiting value limk→∞1
k log2 k = 0. In fact, the maximum value of the entropy
occurs when all the probabilities are equal (i.e., pi = 1
M where H = PM
i=1
1
M log2 M =
log2 M). We have the following theorem.
Theorem 9.A–1: Bounds on Entropy
For a random variable taking on M discrete values, we have the following bounds on
entropy:
0 ≤H(X) ≤log2 M,
where the upper bound occurs when all values are equally likely and the minimum value
0 occurs when any one of the M values has probability 1.
Proof The upper limit follows since log is a concave function (i.e., Ppi log 1
pi ≤
logPpi 1
pi = logM).
Given two random variables X and Y, we can deﬁne joint and conditional entropy,
respectively, as
H(X,Y) =
X
x,y
p(x,y)log
1
p(x,y)
and
H(Y|X) =
X
x,y
p(x,y)log
1
p(y|x).
We then have the following theorem.
Theorem 9.A–2: Chain Rule for Entropy
For any two random variables X and Y, we have
H(X,Y) = H(X) + H(Y|X).
Proof Result is immediate upon substitution of the various deﬁnitions and recalling the
chain rule for probabilities, we have p(x,y) = p(x)p(y|x).

Appendix on Information Theory
383
Now, when the random variables are independent, i.e., p(y|x) = p(y), we have
H(X,Y) = H(X) + H(Y). Note that the chain rule can be extended to N random
variables by repeated application, so we also have
H(X1,X2,...,XN) = H(X1) + H(X2|X1) + H(X3|X1,X2) + ··· ,
and in the case where the Xi are jointly independent, this simpliﬁes to H(X1,
X2, ... , XN) = H(X1) + H(X2) + H(X3) + ····
A most important quantity in information theory is mutual information between
random variables X and Y, deﬁned as
I(X;Y) ≜
X
x,y
p(x,y)log p(x,y)
p(x)p(y).
We speak of the mutual information between X and Y since it easily follows that
I(X;Y) = I(Y;X). Mutual information is related to entropy as
I(X;Y) = H(X) −H(X|Y),
(9.A–1)
which follows immediately from the deﬁnitions and the fact that
p(x,y)
p(x)p(y) = p(x|y)
p(x) . In
words, we say that the mutual information between X and Y is equal to the uncer-
tainty (entropy) of X less the uncertainty that remains in X given that Y is known.
Thus, if X is the input to a communications channel (link) and Y is its output, it
seems that I(X;Y) might actually measure the amount of information carried by Y
about X. In fact, the channel coding theorem actually proves this and shows that
the channel capacity (suitably deﬁned) can be obtained by maximizing the mutual
information.
Theorem 9.A–3: Mutual Information and Entropy
0 ≤I(X;Y) ≤H(X),
H(X|Y) ≤H(X).
Proof By (9.A–1), both of these two equations will be true if we can show mutual
information I(X;Y) ≥0, because entropy and hence conditional entropy are always
nonnegative. To show that mutual information is always positive, we proceed as
I(X;Y) =
X
x,y
p(x,y)log p(x,y)
p(x)p(y)
= (loge)
X
x,y
p(x,y)ln p(x,y)
p(x)p(y).

384
CHAPTER 9 Digital Image Compression
Now, since lnx ≤x −1 for x > 0, we can say that ln 1
x ≥1 −x there. Substituting x with
p(x)p(y)
p(x,y) , we then obtain
X
x,y
p(x,y)ln p(x,y)
p(x)p(y) ≥
X
x,y
p(x,y)

1 −p(x)p(y)
p(x,y)

=
X
x,y
p(x,y) −
X
x,y
p(x)p(y)
= 1 −1
= 0,
as was to be shown.
Data Compression
We do not know yet that entropy is the minimum amount of information to represent
a source, but a central result in information theory called the source coding theorem
will show that it is. We start by looking at a variable-length code such as seen earlier
in Example 9.A–1 corresponding to a tree with a branching factor of 2. The messages
to be coded are at the leaves of the tree, and the codeword bits are along the branches.
This makes what is called a preﬁx code, one where no codeword is the preﬁx of
another, and so guarantees unique decodability. We next show that codeword lengths
must satisfy the Kraft inequality.
Theorem 9.A–4: Kraft Inequality
Let the M codewords of a binary preﬁx code have the lengths l1 ≤l2 ≤··· ≤lM; then these
codeword lengths must satisfy the inequality
M
X
i=1
2−li ≤1.
This is also sufﬁcient for the existence of a code with these codeword lengths.
Proof Let the longest codeword have length lmax ≜lM; then a full or complete binary tree
will have 2lmax leaves.
As can be seen from the example in Figure 9.A–2, each codeword with length li is seen
to prune off a total of 2lmax−li leaves. Taking the sum of these removed leaves, we have
PM
i=1 2lmax−li ≤2lmax. Canceling out 2lmax from both sides, we have the desired result.
Existence comes from the fact that, because of this inequality, there is always enough
room for the next codeword to be assigned.
Now, the set of codewords has probabilities p1,p2,...,pM, that sum to 1. Thus the
Kraft inequality tempts us to write
pi = 2−li,

Appendix on Information Theory
385
0
0
1
1
0
0
1
1
root
1
0
00
010
011
10
110
111
FIGURE 9.A–2
An illustration of the variable-length code tree embedded in a full tree.
the only problem being that the corresponding codeword length li = log2
1
pi may not
be an integer. However, if it is an integer, i.e., we have a dyadic source, then the
average codeword length l will be
l =
X
lipi
=
X
log2
1
pi

pi
= H(X).
So, if we have such a dyadic source, by the Kraft inequality, there is a binary preﬁx
code for that source such that the entropy H(X) equals the average codeword length.
But what about the general case when log2
1
pi is not an integer? In that case, we
can round up this value to the next integer, calling the result l∗
i . This will make 2−li
smaller and thus ensure the existence of a preﬁx code. Then the average codeword
length will be
l =
X
l∗
i pi
=
X
log2
1
pi

pi,
where ⌈⌉is the greatest integer function
<
X
log2
1
pi
+ 1

pi
< H(X) + 1.
We have thus established the following theorem.
Theorem 9.A–5: Source Coding
The average codeword length of an optimal binary preﬁx code satisﬁes
H(X) ≤l < H(X) + 1.

386
CHAPTER 9 Digital Image Compression
This theorem essentially justiﬁes the deﬁnition of entropy as a measure of the
uncertainty, or what is the same the information content, of a discrete message
source. Its proof suggests a way of getting a valid code—i.e., setting li =
l
log2
1
pi
m
—
referred to as Shannon coding. It turns out that Huffman coding, to be introduced
in Section 9.4, is usually somewhat better than Shannon coding and, in fact, can be
shown to be optimal in the sense of minimizing the average codeword length [1, 5].
Instead of coding individual messages from the source (random variable) X, we
could code vectors of length n source symbols Xn. The resulting vector source is
called the extended source. Then applying the preceding result to the extended source,
we get
H(Xn) ≤ln < H(Xn) + 1.
(9.A–2)
But if we assume that the source is i.i.d., then the individual entropies add and
H(Xn) = nH(X). Since we are coding n messages at a time, we have ln = nl, so that
upon insertion into (9.A–2), we get
H(X) ≤l < H(X) + 1
n,
with the source entropy giving even a tighter bound on what can be achieved in terms
of source-coding efﬁciency. In the limit as n →∞, we have l∞= H(X).
In summary, we have thus shown that entropy is the desired mathematical infor-
mation (uncertainty) measure that can be used to characterize sources, including
image sources, in terms of the minimum amount of bits to store or transmit them to
a remote location. However, another way of looking at this is through a consequence
of the law of large numbers.
Asymptotic Equipartition Principle
By the law of large numbers (LLN), if we have an i.i.d. sequence of random variables
X1,...,Xn, the sample average converges to the expected value E[X1] ≜E[X]. The
asymptotic equipartition principle (AEP) considers the information random variables
−logp(Xi) and applies the LLN to their sample average
−1
n
n
X
i=1
logp(Xi) = −1
n logp(X1,...,Xn).
Now the expected value of the random variable −logp(Xi) is
E[−logp(X1)] =
n
X
i=1
pi logpi
= H(X1) ≜H(X).
Thus by the LLN, we have for large n that
p(X1,X2,...,Xn) = p(Xn) ≈2−nH,

Appendix on Information Theory
387
called the AEP [1, 5]. The set of such xn is called the typical set and denoted An.
Neglecting other probabilities than those xn in the typical set, we have
1 =
X
all xn
p(xn)
≈
X
xn∈An
2−nH
=
An2−nH,
where |An| is deﬁned as the number of vectors in the typical set An. Solving for
|An|, we get that with high probability, the number of vectors we have to code is
|An| = 2nH. Now, the number of ﬁxed-length binary codewords of length L is 2L, so
we need length L = nH ﬁxed-length codewords to code the vector (nth extension)
source, or equivalently an average of H bits per source message.
Continuous Sources
If a source X takes on a continuum of values, then the entropy as deﬁned here can be
inﬁnite. In this case we introduce a new type of entropy, with an integral instead of a
summation. The resulting quantity is called differential entropy and shares only some
properties of entropy.
Deﬁnition 9.A–2: Differential Entropy
Let X be a continuous random variable (continuous source) with pdf f(x), then we deﬁne
differential entropy as
h(X) ≜
Z
S
f(x)logf(x) dx,
where S is the support of f, i.e., S = sup{f} ≜{x|f(x) > 0}.
Differential or continuous entropy is always nonnegative, but generally has no
ﬁnite upper bound. If we quantize such a continuous source X with step size 1, we
would expect to often get a ﬁnite value for the entropy, and calling the quantized
random variable X1, it can be shown that
H(X1) + log1 ≈h(X),
whenever 1 is sufﬁciently small. The argument essentially reduces to approximation
of the above integral with a Riemann sum with bin width 1. Turning this result
around, we get an approximate result for the entropy of a quantized continuous
random variable with quantizer sufﬁciently small step size of 1,
H(X1) ≈h(X) + log 1
1,
(9.A–3)
relating differential entropy to entropy.

388
CHAPTER 9 Digital Image Compression
By our previous results for discrete random variables, H(X1) is the number of bits to
represent the quantized source X1 asymptotically in terms of sequences of samples
of length n.
Example 9.A–2: (Gaussian Random Variable)
Let the random variable X be Gaussian distributed as N(0,σ 2), then we have differential
entropy
h(X) = −
+∞
Z
−∞
1
√
2πσ 2 exp
 
−x2
2σ 2
!"
−x2
2σ 2 −ln
p
2πσ 2
#
dx
= σ 2
2σ 2 + ln
p
2πσ 2
= 1
2 + 1
2 ln2πσ 2
= 1
2 ln2πeσ 2
nats.
Converting to base 2 logarithms, we have h(X) = 1
2 log2
 2πeσ 2
. We thus see that for
a Gaussian random variable, the differential entropy h(X) varies with standard deviation
σ as log2 σ. Also, when σ/1 is large, i.e., when the quantization is ﬁne enough, then
H(X1) ≈1
2 ln2πe(σ 2/12) by (9.A–3).
Rate-Distortion Theory
Rate-distortion theory is a branch of information theory that deals with the source
coding of continuous random sources. Since in general a continuous source has
inﬁnite entropy, some kind of approximation must be made to allow its storage or
transmission with a ﬁnite number of bits. Rate-distortion theory tries to formulate the
optimal trade-off of rate versus distortion, with some common distortion measures
being MSE and mean-absolute error. The simple quantizer offers a way to trade off
rate versus error. Generally, the MSE of a quantizer can be expressed in terms of the
quantizer step size 1 with a formula such as MSE = k12, which often holds for sufﬁ-
ciently small 1, where k is a constant that depends on the pdf of the source. Adopting
MSE as the distortion, we then have
D = k12.
Using the result for a quantized Gaussian random variable in the preceding example,
we can say that asymptotically for a uniform step-size quantizer, rate R = H(X1)
in bits per source symbol. Combining results, we get the operational rate-distortion

Appendix on Information Theory
389
function of the quantizer, in this case as
R(D) ≈1
2 log2[2πe(σ 2/12)]
= 1
2 log2[k2πe(σ 2/D)].
However, this is only a suboptimal result for a Gaussian random variable. For one
thing, it is only an approximation, and that for the case when 1 ≪σ. Also, quantizers
are not optimal in themselves. The optimal result has been derived using the rate-
distortion theory, and it is stated next without proof.
Theorem 9.A–6: Gaussian Rate-Distortion Function
For a Gaussian source X distributed as N(0,σ 2) and mean-square distortion measure
d(x,bx) = |x −bx|2, the (optimal) rate-distortion function is
R(D) =
(
1
2 log2(σ 2/D),
0 < D < σ 2,
0,
D ≥σ 2.
A plot of the Gaussian rate-distortion function for σ = 5 is given in Figure 9.A–3.
We see that, as D approaches zero, the rate R asymptotically becomes inﬁnite for this
continuous random variable, as it should. Also, the rate required above distortion
3.5
3
2.5
2
1.5
1
0.5
00
5
10
15
20
Plot of Gaussian R(D)
D
R(D)
25
30
35
FIGURE 9.A–3
Plot of Gaussian R(D) function for σ = 5.

390
CHAPTER 9 Digital Image Compression
D = σ 2 is zero, since the receiver is assumed to know the parameters of this distribu-
tion. So it can always estimate bX = µ(= 0 here) and incur a mean-square distortion
equal to the variance σ 2.
Another basic rate-distortion function for image coding is that of the Laplace
random variable, which ﬁts the measured density of image prediction errors and DCT
coefﬁcients other than DC. It has been found in closed form by Berger [29] for the
case of mean-absolute distortion and is given here without proof.
Theorem 9.A–7: Laplace Rate-Distortion Function
For Laplace source of variance σ 2(> 0) and mean-absolute distortion measure d(x,bx) =
|x −bx|, the (optimal) rate-distortion function is given in terms of α ≜
√
2/σ:
R(D) =
(
−log2 αD,
0 < D < α,
0,
D ≥α.
A plot of the Laplace rate-distortion function for σ = 30 is given in Figure 9.A–4.
While the plot looks similar to that of the Gaussian, note that the standard deviations
are quite different, as also is the distortion criterion, the Gaussian being square error
while the Laplacian rate-distortion function is for absolute error. For a development
of rate-distortion theory, see [1, 2, 5].
7
6
5
4
3
2
1
00
5
10
15
20
Plot of Laplace R(D)
D
R(D)
25
30
35
FIGURE 9.A–4
Plot of the Laplace rate-distortion function for σ = 30.

References
391
REFERENCES
[1] R. G. Gallagher, Information Theory and Reliable Communication, John Wiley, 1968.
[2] R. Gersho and R. M. Gray, Vector Quantization and Signal Compression, Kluwer
Academic Press, 1992.
[3] H. S. Malvar and D. H. Staelin, “The LOT: Transform Coding Without Blocking Effects,”
IEEE Trans. Acoust., Speech, and Signal Process., vol. 37, pp. 553–559, April 1989.
[4] K. Ramchandran and M. Vetterli, “Best Wavelet Packet Bases in a Rate-Distortion
Sense,” IEEE Trans. Image Process., vol. 2, pp. 160–175, April 1993.
[5] T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd Ed., Wiley & Sons,
Inc., New Jersey, 2006.
[6] Z. Zhang and J. W. Woods, “Large Block VQ for Image Sequences,” Proc. IEE IPA-99,
Manchester, UK, July 1999.
[7] H.-M. Hang and J. W. Woods, “Predictive Vector Quantization of Images,” IEEE Trans.
Comm., vol. COM-33, pp. 1208–1219, November 1985.
[8] D. S. Taubman and M. W. Marcellin, JPEG 2000 Image Compression Fundamentals,
Standards, and Practice, Kluwer Academic Press, Norwell, MA, 2002.
[9] R. L. Lagendijk, VcDemo. Available at http://siplab.tudelft.nl/content/image-and-video-
compression-learning-tool-vcdemo, Delft University of Technology (DUT), Delft, The
Netherlands.
[10] K. Sayood, Introduction to Data Compression, Morgan Kaufman, 1996.
[11] Y. H. Kim and J. W. Modestino, “Adaptive Entropy Coded Subband Coding of Images,”
IEEE Trans. Image Process., vol. 1, pp. 31–48, January 1992.
[12] S.-J. Choi, Three-Dimensional Subband/Wavelet Coding of Video with Motion Com-
pensation, PhD thesis, ECSE Department, Rensselaer Polytechnic Institute, Troy, NY,
1996.
[13] T. Naveen and J. W. Woods, “Subband Finite State Scalar Quantization,” IEEE Trans.
Image Process., vol. 5, pp. 150–155, January 1996.
[14] E. A. Riskin, “Optimal Bit Allocation via the Generalized BFOS Algorithm,” IEEE
Trans. Inform. Theory, vol. 37, pp. 400–402, March 1991.
[15] W. Equitz and T. Cover, “Successive Reﬁnement of Information,” IEEE Trans. Inform.
Theory, vol. 37, pp. 269–275, March 1991.
[16] J. M. Shapiro, “Embedded Image Coding Using Zerotrees of Wavelet Coefﬁcients,” IEEE
Trans. Signal Process., vol. 41, pp. 3445–3462, December 1993.
[17] A. Said and W. A. Pearlman, “A New, Fast, and Efﬁcient Image Codec Based on Set
Partitioning in Hierarchical Trees,” IEEE Trans. Circ. and Sys. Video Technology, vol. 6,
pp. 243–250, June 1996.
[18] S.-T. Hsiang and J. W. Woods, “Embedded Image Coding Using Zeroblocks of Subband/
Wavelet Coefﬁcients and Context Modeling,” Proc. ISCAS 2000, May, Geneva,
Switzerland, 2000.
[19] D. S. Taubman, “High performance scalable image coding with EBCOT,” IEEE Trans.
Image Process., vol. 9, pp. 1158–1170, July 2000.
[20] P. H. Westerink, Subband Coding of Images, PhD thesis, Delft University of Technology,
The Netherlands, October 1989.
[21] S.-T. Hsiang, Highly Scalable Subband/Wavelet Image and Video Coding, PhD thesis,
ECSE Department, Rensselaer Polytechnic Institute, Troy, NY, January 2002.
[22] B. Zeng and J. Fu, “Directional Discrete Cosine Transforms—A New Framework for
Image Coding,” IEEE Trans. Circ. and Sys. Video Technology, vol. 18, no. 3, March,
pp. 305–313, 2008.

392
CHAPTER 9 Digital Image Compression
[23] C.-L. Chang, M. Makar, S. S. Tsai, and B. Girod, “Direction-Adaptive Partitioned
Block Transform for Color Image Coding,” IEEE Trans. Image Process., vol. 19, no. 7,
pp. 1740–1755, July 2010.
[24] W. Ding, F. Wu, X. Wu, S. Li, and H. Li, “Adaptive Directional Lifting-Based Wavelet
Transform for Image Coding,” IEEE Trans. Image Process., vol. 16, no. 2, pp. 416–427,
February 2007.
[25] Y. Tanaka, M. Ikehara, and T. Q. Nguyen, “Multiresolution Image Representation Using
Combined 2-D and 1-D Directional Filter Banks,” IEEE Trans. Image Process., vol. 18,
no. 2, pp. 269–280, February 2009.
[26] J. Xu, B. Zeng, and F. Wu, “An Overview of Directional Transforms in Image Coding,”
Proc. of ISCAS, pp. 3036–3039, 2010.
[27] R. V. Hartley, “Transmission of Information,” Bell System Technical Journal, vol. 7,
pp. 535, 1928.
[28] C. E. Shannon, “A Mathematical Theory of Communications,” Bell System Technical
Journal, vol. 27, pp. 379–423, 623–656, 1948.
[29] T. Berger, Rate Distortion Theory, Prentice-Hall, Inc., Upper Saddle River, NJ 1971.

CHAPTER
Three-Dimensional and
Spatiotemporal Processing10
In Chapter 2 we looked at 2-D signals and systems. Here, we look at the 3-D case,
where often the application is to spatiotemporal processing, i.e., two spatial dimen-
sions and one time dimension. Of course, 3-D more commonly refers to the three
orthogonal spatial dimensions. Here, we will be mostly concerned with convolution
and ﬁltering, and so need a regular grid for our signal processing. While regular grids
are commonplace in spatiotemporal 3-D, they are often missing in spatial 3-D appli-
cations. In any event, the theory developed in this section can apply to any 3-D signals
on a regular grid. In many cases, when a regular grid is missing or not dense enough
(often the case in video processing), interpolation may be used to infer values on a
regular supergrid, maybe a reﬁnement of an existing grid. Often such an interpolative
resampling will be done anyway for display purposes.
10.1 3-D SIGNALS AND SYSTEMS
We start with a deﬁnition of a 3-D linear system.
Deﬁnition 10.1–1: 3-D Linear System
A system is linear when its operator T satisﬁes the equation
L{a1x1(n1,n2,n3) + a2x2(n1,n2,n3)} = a1L{x1(n1,n2,n3)} + a2L{x2(n1,n2,n3)},
for all (complex) scalars a1 and a2, and for all signals x1 and x2. When the system T is
linear, we usually denote this operator as L.
Deﬁnition 10.1–2: 3-D Linear Shift Invariant System
Let the 3-D linear system L have output y(n1,n2,n3) when the input is x(n1,n2,n3); that is,
y(n1,n2,n3) = L{x(n1,n2,n3)}. Then the system is 3-D LSI if it satisﬁes the equation
y(n1 −k1,n2 −k2,n3 −k3) = L{x(n1 −k1,n2 −k2,n3 −k3)},
for all integer shift values k1,k2, and k3.
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00010-2
© 2012 Elsevier Inc. All rights reserved.
393

394
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
Often such systems are called linear constant-parameter or constant-coefﬁcient.
An example would be a multidimensional ﬁlter. In the 1-D temporal case, we gen-
erally need to specify initial conditions and/or ﬁnal conditions. In the 2-D spatial
case we often have some kind of boundary conditions. For 3-D systems, the solu-
tion space is a 3-D region, and we need to specify boundary conditions generally
on all the boundary surfaces. This is particularly so in the 3-D spatial case. In the
3-D spatiotemporal case, where the third parameter is time, initial conditions often
sufﬁce in that dimension. However, we need both initial and boundary conditions to
completely determine a ﬁlter output.
Deﬁnition 10.1–3: 3-D Convolution Representation
For an LSI system, with impulse response, h the input and output are related by the 3-D
convolution,
y(n1,n2,n3) =
XXX
h(k1,k2,k3)x(n1 −k1,n2 −k2,n3 −k3)
= (h ∗x)(n1,n2,n3),
where the triple sums are over all values of k1,k2, and k3, where −∞< k1,k2,k3 < +∞.
We can see that 3-D convolution is a commutative operation, so that h ∗x = x ∗h
as usual.
Deﬁnition 10.1–4: 3-D Separability
A 3-D LSI system is a separable system if its impulse response factors or separates, such
as h(n1,n2,n3) = h1(n1)h2(n2)h3(n3) or h(n1,n2,n3) = h1(n1,n2)h2(n3). A signal is a sepa-
rable signal if it separates into two or three factors. A separable operator factors into the
concatenation of two or three operators, such as
Tn3{Tn2{Tn1[·]}}
or
Tn3{Tn1,n2[·]}.
Examples of separable operators in 3-D are the familiar transforms, i.e., Fourier,
DFT, DCT, and SWT, which, in this way, extend to the 3-D case naturally.
Example 10.1–1: Separable System and Separable Convolution
A given LSI system has input x and output y and separable impulse response
h(n1,n2,n3) = h1(n1,n2)h2(n3). Expressing the 3-D convolution, we have
y(n1,n2,n3) =
XXX
h1(k1,k2)h2(k3)x(n1 −k1,n2 −k2,n3 −k3)
(10.1–1)

10.1 3-D Signals and Systems
395
=
XX
h1(k1,k2)

X
k3
h2(k3)x(n1 −k1,n2 −k2,n3 −k3)


= h1(n1,n2) ∗[h2(n3) ∗x(n1,n2,n3)]
= h2(n3) ∗[h1(n1,n2) ∗x(n1,n2,n3)].1
(10.1–2)
The result is a 2-D convolution with the 2-D signal consisting of x for each ﬁxed value of
n3, perhaps denoted as
xn3(n1,n2) ≜x(n1,n2,n3),
followed by 1-D convolutions in n3 for the 1-D output signal obtained by ﬁxing n1,n2 and
then ranging through all such n1 and n2—i.e., xn1,n2(n3) ≜x(n1,n2,n3). Of course, and by
linearity, the 1-D and 2-D convolutions in (10.1–1) can be done in either order.
Deﬁnition 10.1–5: 3-D Fourier Transform
X(ω1,ω2,ω3) ≜
+∞
X
n1=−∞
+∞
X
n2=−∞
+∞
X
n3=−∞
x(n1,n2,n3)exp −j(ω1n1 + ω2n2 + ω3n3).
Note that X is continuous and triply periodic with period 2π ×2π ×2π.
Deﬁnition 10.1–6: Inverse 3-D Fourier Transform
x(n1,n2,n3) =
1
(2π)3
+π
Z
−π
+π
Z
−π
+π
Z
−π
X(ω1,ω2,ω3)exp + j(ω1n1 + ω2n2 + ω3n3) dω1dω2dω3,
which amounts to a 3-D Fourier series, with the roles of “transform” and “signal”
reversed, analogously to the 2-D and 1-D cases.
Properties of 3-D Fourier Transform
Ideal shift property (shift by k1,k2,k3):
FT{x(n1 −k1,n2 −k2,n3 −k3)} = X(ω1,ω2,ω3)exp −j(ω1k1 + ω2k2 + ω3k3)
Linearity:
FT{a1x1(n1,n2,n3) + a2x2(n1,n2,n3)} = a1X1(ω1,ω2,ω3) + a2X2(ω1,ω2,ω3)
1Beware the notation here! The ﬁrst convolution is just on the n3 parameter. The second one is just on
n1,n2.

396
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
y(n1, n2, n3)
x(n1, n2, n3)
H(ω1,ω2,ω3)
FIGURE 10.1–1
A 3-D system characterized by its frequency response.
Convolution:
FT{x1(n1,n2,n3) ∗x2(n1,n2,n3)} = X1(ω1,ω2,ω3)X2(ω1,ω2,ω3)
Separable operator:
FT{h1(n1,n2) ∗[h2(n3) ∗x(n1,n2,n3)]} = H1(ω1,ω2)H2(ω3)X(ω1,ω2,ω3)
This last property can be interpreted as a 2-D ﬁltering cascaded with a 1-D ﬁl-
tering along the remaining axis. While this is not general, it can be expedient and
sometimes is all that is required to implement a given transformation.
3-D Filters
Consider a constant coefﬁcient 3-D difference equation
y(n1,n2,n3) = −
X
(k1,k2,k3)∈Ra−(0,0,0)
ak1,k2,k3y(n1 −k1,n2 −k2,n3 −k3)
+
X
(k1,k2,k3)∈Rb
bk1,k2,k3x(n1 −k1,n2 −k2,n3 −k3),
where Ra and Rb denote the 3-D ﬁlter coefﬁcient support regions. By using linear-
and shift-invariant properties of 3-D FTs, we can transform the preceding relation to
multiplication in the frequency domain with a system function H(ω1,ω2,ω3),
H(ω1,ω2,ω3) = B(ω1,ω2,ω3)
A(ω1,ω2,ω3),
given in terms of the Fourier transform B(ω1,ω2,ω3) of the ﬁlter feed-forward
coefﬁcients b(k1,k2,k3) ≜bk1,k2,k3 and the Fourier transform A(ω1,ω2,ω3) of the ﬁl-
ter feedback coefﬁcients a(k1,k2,k3) ≜ak1,k2,k3, where a(0,0,0) = +1. A 3-D LSI
system diagram is shown in Figure 10.1–1.
10.2 3-D SAMPLING AND RECONSTRUCTION
The 3-D sampling theorem follows easily from the corresponding 2-D theorem
in Chapter 2. The method of proof also extends, so that the same method can

10.2 3-D Sampling and Reconstruction
397
be used. As before, we use the variables ti for the continuous parameters of the
function xc(t1,t2,t3) to be sampled, with its corresponding continuous-parameter FT
Xc(1,2,3).
Theorem 10.2–1: 3-D Sampling Theorem
Let x(n1,n2,n3) ≜xc(n1T1,n2T2,n3T3), with sampling periods, Ti, i = 1,2,3; then:
X(ω1,ω2,ω3) =
1
T1T2T3
X
k1,k2,k3
Xc
ω1 −2πk1
T1
, ω2 −2πk2
T2
, ω3 −2πk3
T3

.
Aliasing occurs when the shifted FTs on the right overlap. This will occur when the Ti’s are
not small enough. If Xc is ideally bandlimited in time and space, then π
Ti = ci, for signal
bandwidth limits ci, will work ﬁne.
Proof We start by writing x(n1,n2,n3) in terms of the samples of the inverse Fourier
transform of Xc(1,2,3):
x(n1,n2,n3) =
1
(2π)3
+∞
Z
−∞
+∞
Z
−∞
+∞
Z
−∞
Xc(1,2,3)
× exp+ j(1n1T1 + 2n2T2 + 3n3T3) d1d2d3.
Next, we let ωi ≜iTi in this triple integral to obtain
x(n1,n2,n3) =
1
(2π)3
+∞
Z
−∞
+∞
Z
−∞
+∞
Z
−∞
1
T1T2T3
Xc
ω1
T1
, ω2
T2
, ω3
T3

× exp+ j(ω1n1 +ω2n2 +ω3n3)dω1dω2dω3
=
1
(2π)3
X
all k1,k2,k3
Z
SQ(k1,k2,k3)
1
T1T2T3
Xc
ω1
T1
, ω2
T2
, ω3
T3

× exp+ j(ω1n1 + ω2n2 + ω3n3) dω1dω2dω3,
(10.2–1)
where SQ(k1,k2,k3) is a 2π ×2π ×2π cube centered at position (2πk1,2πk2,2πk3):
SQ(k1,k2,k3) ≜[−π +2πk1, +π +2πk1] × [−π + 2πk2, +π + 2πk2]
× [−π + 2πk3,+π + 2πk3].

398
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
Then making the change of variables ω′
i = ωi −2πki inside each of the preceding
integrals, we get
x(n1,n2,n3) =
1
(2π)2
X
all k1,k2,k3
+π
Z
−π
+π
Z
−π
+π
Z
−π
1
T1T2T3
× Xc
ω′
1 + 2πk1
T1
, ω′
2 + 2πk2
T2
, ω′
3 + 2πk3
T3

× exp+ j(ω′
1n1 + ω′
2n2 + ω′
3n3)dω′
1dω′
2dω′
3
=
1
(2π)2
+π
Z
−π
+π
Z
−π
+π
Z
−π

X
all k1,k2
1
T1T2T3
× Xc
ω′
1 + 2πk1
T1
, ω′
2 + 2πk2
T2
, ω′
3 + 2πk3
T3


× exp+ j(ω′
1n1 + ω′
2n2 + ω′
3n3)dω′
1dω′
2dω′
3
= IFT


X
all k1,k2,k3
1
T1T2T3
Xc
ω′
1 + 2πk1
T1
, ω′
2 + 2πk2
T2
, ω′
3 + 2πk3
T3

,
as was to be shown.
This last equation says that x is the 3-D discrete-space IFT of X. If any Ti is too large,
then this linear sampling theory says we should preﬁlter before sampling to avoid
aliasing caused by the offendingly coarse Ti’s.
General 3-D Sampling
Consider more general, but still regular sampling at locations
t =


t1
t2
t3

= V


n1
n2
n3


= n1v1 + n2v2 + n3v3,
where V ≜[v1,v2,v3] is the 3-D sampling matrix formed from three noncolinear sam-
pling basis vectors v1,v2, and v3. The corresponding aliased representation in the
discrete-space frequency domain is expressed in terms of the periodicity matrix U,
which must satisfy
UTV = 2πI.
Just as in the 2-D case, one can show that
X(ω) =
1
|detV|
X
k
Xc
 1
2π U(ω −2πk)

,
(10.2–2)

10.3 Spatiotemporal Signal Processing
399
where Xc is the 3-D continuous-space Fourier transform, and X is the 3-D Fourier
transform of the samples. We can also express (10.2–2) in terms of the analog
frequency  as
X(VT) =
1
|detV|
X
k
Xc( −Uk),
analogously to the 2-D general sampling case in Chapter 2.
When Xc is bandlimited, and in the absence of aliasing, we have
Xc() = |detV|X(VT),
which we can use for reconstruction from the 3-D samples. We start with the 3-D
continuous-space IFT relation
xc(t) =
1
(2π)3
Z Z Z
Xc()exp(jTt)d,
and substitute to obtain
xc(t) = |detV|
(2π)3
Z Z Z
X(VT)exp(jTt)d
= |detV|
(2π)3
Z Z Z X
n
x(n)exp(−jTVn)exp(jTt)d
=
X
n
x(n)|detV|
(2π)3
Z Z Z
exp(jT(t −Vn))d.
We thus see that the reconstruction interpolation ﬁlter is
h(t) = |detV|
(2π)3
Z Z Z
exp(jTt)d,
where the 3-D integral is taken over the bandlimited support of Xc, say |||| ≤c.
We then have the reconstruction formula
xc(t) =
X
n
x(n)h(t −Vn).
For more on general 3-D sampling and sample rate change, see the text by
Vaidyanathan [1].
10.3 SPATIOTEMPORAL SIGNAL PROCESSING
In spatiotemporal processing usually data are undersampled in the time dimension
and there is no preﬁlter! The situation should improve as technology permits higher
frame rates than 24–30 frames per second (fps), such as 60–100 fps, etc. Here, we
specialize 3-D processing to two spatial dimensions and one time dimension. We can
then treat the ﬁltering of image sequences or video. Notationally we replace n3 by n,

400
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
which will denote discrete time. Also, the third frequency variable is written simply
as ω radians or f when the unit is Hz, giving the spatiotemporal FT pair
x(n1,n2,n) ↔X(ω1,ω2,ω).
Time-domain causality now applies in the third dimension n. We can use a matrix
notation
x(n)≜{x(n)}
with
n ≜(n1,n2,n)T
to denote a frame of video data at time n. The video is then a sequence of these
matrices:
x(0),x(1),x(2),x(3),x(4),x(5),....
Spatiotemporal Sampling
Example 10.3–1: Progressive Video
We apply rectangular sampling to the spatiotemporal continuous signal xc(t1,t2,t), sam-
pled spatially with sample period T1 × T2 and temporally with period △t. We then have the
sampling and periodicity matrices,
V =


T1
0
0
0
T2
0
0
0
△t


and
U =


2π/T1
0
0
0
2π/T2
0
0
0
2π
△t

.
This spatiotemporal sampling is called progressive scanning, or simply noninterlaced, by
video engineers.
Computer manufacturers have long favored progressive, also called noninter-
laced, because small characters do not ﬂicker as much on a computer display. For
natural video, progressive avoids the interline ﬂicker of interlaced systems and is
easier to interface with motion estimation and compensation schemes, which save
channel bits. Nevertheless, interlaced CRT displays are cheaper to build, because the
pixel rate is lower for interlace given the same display resolution. Now solid-state
displays are most all progressive.
Example 10.3–2: Interlaced Video
In Chapter 2, we introduced this concept for analog video as just 2-D sampling in the
vertical and time directions (v,t) = (t2,t), with sampling and periodicity matrices,
V =
"
1
1
1
−1
#
and
U =
"
π
π
π
−π
#
.
It is only 2-D sampling because in analog video there is no sampling horizontally. However,
in digital video, there is horizontal sampling, so we need 3-D sampling. The interlaced

10.3 Spatiotemporal Signal Processing
401
digital video has sampling matrix in space-time and periodicity matrix in spatiotemporal
frequency given as
V =


1
0
0
0
1
1
0
1
−1


and
U =


2π
0
0
0
π
π
0
π
−π

,
in terms of sampling structure (h,v,t) = (t1,t2,t). Here, we have normalized the sampling
periods for simplicity, and the resulting simple sampling pattern is called quincunx. Of
course, in a real system, the sample spacings would not be unity, and we would have
sampling in terms of distances T1 and T2 on the frame, and T as the time between
ﬁelds.2
Spatiotemporal frequency aliasing is a real problem in video signal processing
because optical lenses are poor bandlimiting ﬁlters and, further, the temporal dimen-
sion is usually grossly undersampled (i.e., 30 fps is often not high enough). A frame
rate of 100 fps or more has been often claimed as necessary for common scenes. Only
now with HD cameras and beyond is digital video beginning to become alias free
in space. Alias-free with respect to time sampling, however, remains elusive, with-
out the use of special high frame-rate cameras. They are used in certain industrial
inspection applications and in the movie industry to create slow motion effects.
Spatiotemporal Filters
The variety of spatiotemporal ﬁlters includes ﬁnite impulse response (FIR), inﬁnite
impulse response (IIR), statistically designed ﬁlters, and adaptive ﬁlters. Addition-
ally, there can be hybrids of FIR and IIR (e.g., the ﬁlter can be IIR in time and FIR in
space). This kind of hybrid ﬁlter has become very popular due to the need to minimize
memory in the temporal direction, thus minimizing the use of frame memories. The
frame predictor in a hybrid video coder (cf. Chapter 12) is an FIR spatiotemporal ﬁl-
ter. The decoder must run the inverse of this ﬁlter, which is thus an IIR spatiotemporal
ﬁlter. It must therefore be 3-D multidimensionally stable.
Example 10.3–3: Temporal Averager
A very simple example of a useful spatiotemporal ﬁlter is the temporal averager, whose
impulse response is given as
h(n1,n2,n) = 1
N δ(n1,n2)(u(n) −u(n −N)).
2We remember from Chapter 2 that an interlaced frame is composed of two ﬁelds, the upper and lower.

402
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
For input x(n1,n2,n), the output y(n1,n2,n) is given as
y(n1,n2,n) = 1
N
N−1
X
k=0
x(n1,n2,n −k),
or just the average of the present and past N −1 frames. This is a 3-D FIR ﬁlter that is
causal in the time or n variable. Such a ﬁlter is often used to smooth noisy video data,
most often due to image sensor noise, when the frames are very similar i.e., have little
interesting change over the time scale of N frames.
If objects in the frame move much, then the temporal averager will blur their
edges, so often such a device is combined with a suitable motion or change detector
that will control the number of frames actually used. It can be based on some norm
of the frame difference ||x(n1,n2,n) −x(n1,n2,n)||. Optimal ﬁltering of noisy image
sequences can be based on statistical models of the noise and data. One such model
that is quite common is the spatiotemporal autoregressive moving average (ARMA)
random ﬁeld sequence (i.e., a sequence of random ﬁelds), which we discuss next.
Spatiotemporal ARMA Model
Deﬁnition 10.3–1: Autoregressive Moving Average
x(n1,n2,n) =
X
(k1,k2,k)∈Ra−(0,0,0)
ak1,k2,kx(n1 −k1,n2 −k2,n −k)
+
X
(k1,k2,k)∈Rb
bk1,k2,kw(n1 −k1,n2 −k2,n −k)
(10.3–1)
Here, the input spatiotemporal sequence w is taken as a white noise random-
ﬁeld sequence. If Rb = {(0,0,0)}, then the model is AR, or autoregressive. We get
a Markov random ﬁeld sequence (cf. Section 10.4) if the input w is jointly indepen-
dent as well as spectrally white. If additionally Ra only has coefﬁcients for t ≥1,
then the model is said to be frame-based. A frame-based model could use parallel
computation capability to calculate a whole frame at a time.
Intraframe Filtering
The equation for intraframe ﬁltering with ﬁlter impulse response h(n1,n2) is
y(n1,n2,n) =
X
k1,k2
h(k1,k2)x(n1 −k1,n2 −k2,n).

10.3 Spatiotemporal Signal Processing
403
Intraframe ﬁltering is appropriate to restore mild spatial blur in the presence of good
SNR, i.e., 30 dB or higher, and also when there is low interframe dependency, (e.g.,
a very low frame rate such as < 1fps). Still other reasons are to retain frame identity,
such as in video editing, using so-called I frames in MPEG parlance,3 to reduce
complexity and expense.
The intraframe ARMA signal model is given as
x(n1,n2,n) =
X
(k1,k2)∈Ra−(0,0)
ak1,k2x(n1 −k1,n2 −k2,n)
+
X
(k1,k2)∈Rb
bk1,k2w(n1 −k1,n2 −k2,n).
(10.3–2)
Normally the input term w is taken as spatiotemporal white noise. This ARMA
consists of spatial ﬁltering of each input frame, with the 2-D recursive ﬁlter with
coefﬁcient arrays {a},{b}. The ﬁlter coefﬁcients are constant here across the frames
but could easily be time variant {a(k1,k2,k)},{b(k1,k2,k)}, making a time-varying
intraframe ARMA model.
Intraframe Wiener Filter
Taking the 2-D Fourier transform of (10.3–2), we obtain
X(ω1,ω2;n) =
B(ω1,ω2)
1 −A(ω1,ω2)W(ω1,ω2;n),
for each frame (n). We could also write this in terms of 3-D Fourier transforms as
X(ω1,ω2,ω) =
B(ω1,ω2)
1 −A(ω1,ω2)W(ω1,ω2,ω)
but will generally reserve 3-D transform notation for the interframe case to be treated
in the following subsections.
For homogeneous random processes that possess power spectral densities (PSDs),
we can write
Sx(ω1,ω2;n) =

B(ω1,ω2)
1 −A(ω1,ω2)

2
Sw(ω1,ω2;n) and
Sx(ω1,ω2,ω) =

B(ω1,ω2)
1 −A(ω1,ω2)

2
Sw(ω1,ω2,ω),
where often Sw = σ 2
w (i.e., spatiotemporal white noise). Here, A and B are the spa-
tial or 2-D Fourier transforms of the spatial ﬁlter coefﬁcients a(k1,k2) and b(k1,k2).
3I-frame is an MPEG compression term that denotes an image frame compressed without reference to
other image frames (i.e., intraframe compression). MPEG compression is covered in Chapter 12.

404
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
We can then use these spectral densities to design a 2-D or spatial Wiener ﬁlter for
the observations
y(n1,n2,n) = h(n1,n2) ∗x(n1,n2,n) + v(n1,n2,n),
where the spatial blur h is known and the noise v is white with variance σ 2
v and
uncorrelated with the signal x. Then the intraframe Wiener ﬁlter is given as
G(ω1,ω2;n) =
H∗(ω1,ω2)Sx(ω1,ω2;n)
|H|2 (ω1,ω2)Sx(ω1,ω2;n) + σ 2w
(10.3–3)
at each frame n, where the PSDs Sx could have been estimated via an AR or
ARMA modeling procedure. Effectively, this is just image processing done for each
frame (n). Note that the PSDs can vary with n, i.e., be time variant.
Example 10.3–4: Intraframe Wiener Filter
Let r = x + v with x ⊥v (i.e., x and v orthogonal). Also assume that the random ﬁeld
sequences x and n are homogeneous in space with PSDs Sx and Sv, respectively. Then
the noncausal intraframe Wiener ﬁlter for frame n is given by
G(ω1,ω2;n) =
Sx(ω1,ω2;n)
Sx(ω1,ω2;n) + Sv(ω1,ω2;n).
Additionally, assume a spatial AR model for the signal x as in (10.3–1) and also that the
observation noise is white with PSD σ 2
v. Then the intraframe Wiener ﬁlter becomes
G(ω1,ω2;n) =
 B(ω1,ω2)
1−A(ω1,ω2)

2
σ 2
w
 B(ω1,ω2)
1−A(ω1,ω2)

2
σ 2w + σ 2v
=
|B(ω1,ω2)|2 σ 2
w
|B(ω1,ω2)|2 σ 2w + |1 −A(ω1,ω2)|2 σ 2v
.
In the case of Wiener ﬁltering, the optimality of this process would, of course,
require that the frames be uncorrelated, both for the signal x and for the noise w. This
is even though the blur h is conﬁned to the individual frames—i.e., h = h(n1,n2)—
which is also, of course, a necessary condition for the optimality of the linear ﬁlter
(10.3–3).
Interframe Filtering
The equation for interframe ﬁltering with ﬁlter impulse response h(n1,n2,n) is
y(n1,n2,n) =
X
k1,k2
h(k1,k2,k)x(n1 −k1,n2 −k2,n −k).
Interframe ﬁltering is the general case for spatiotemporal LSI processing and is the
basis for powerful motion-compensated models to be treated in Chapter 11.

10.3 Spatiotemporal Signal Processing
405
The interframe ARMA signal model is given as
x(n1,n2,n) =
X
(k1,k2,k)∈Ra−(0,0)
ak1,k2,kx(n1 −k1,n2 −k2,n −k)
+
X
(k1,k2,k)∈Rb
bk1,k2,kw(n1 −k1,n2 −k2,n −k).
(10.3–4)
Often these interframe models are categorized by how many prior frames they use. If
we restrict to just one prior frame, we can write
x(n1,n2,n) =
X
(k1,k2)∈Ra−(0,0)
ak1,k2,0x(n1 −k1,n2 −k2,n)
+
X
(k1,k2)∈Ra−(0,0)
ak1,k2,1x(n1 −k1,n2 −k2,n −1)
+
X
(k1,k2)∈Rb
bk1,k2,0w(n1 −k1,n2 −k2,n)
+
X
(k1,k2)∈Rb
bk1,k2,1w(n1 −k1,n2 −k2,n −1),
(10.3–5)
which explicitly shows the operation as FIR spatial ﬁlterings of the present and past
input and output frames, and then their output summation to produce the next output
point in the current frame.
In the frequency domain, we can write this ﬁltering using 3-D Fourier trans-
forms as
X(ω1,ω2,ω) =
B(ω1,ω2,ω)
1 −A(ω1,ω2,ω)W(ω1,ω2,ω),
in the general case of (10.3–4) and
X(ω1,ω2,ω) =
B0(ω1,ω2) + B1(ω1,ω2)e−jω
1 −A0(ω1,ω2) −A1(ω1,ω2)e−jω W(ω1,ω2,ω),
in the ﬁrst-order temporal case of (10.3–5).
Interframe Wiener Filter
For the more general observation equation, with both blurring and observation noise,
y(n1,n2,n) = h(n1,n2,n) ∗x(n1,n2,n) + v(n1,n2,n),
we will need the interframe Wiener ﬁlter, expressed in 3-D Fourier transforms as
G(ω1,ω2,ω) =
H∗(ω1,ω2,ω)Sx(ω1,ω2,ω)
|H|2 (ω1,ω2,ω)Sx(ω1,ω2,ω) + σ 2v
,
where we see the 3-D PSD Sx(ω1,ω2,ω) and the observation noise v is here assumed
to be uncorrelated in time as well as in space, i.e., Rv(m1,m2,m) = σ 2
vδ(m1,m2,m),
with δ the 3-D Kronecker delta function.

406
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
Example 10.3–5: First-Order Temporal Interframe Wiener Filter
For the case of the ﬁrst-order temporal ARMA model of (10.3–5), we can compute the
PSD as
Sx(ω1,ω2,ω) =

B0(ω1,ω2) + B1(ω1,ω2)e−jω
1 −A0(ω1,ω2) −A1(ω1,ω2)e−jω

2
σ 2
w,
so that the resulting interframe Wiener ﬁlter becomes
G(ω1,ω2,ω)
=
H∗(ω1,ω2,ω)

B0(ω1,ω2)+B1(ω1,ω2)e−jω
1−A0(ω1,ω2)−A1(ω1,ω2)e−jω

2
σ 2
w
|H|2 (ω1,ω2,ω)

B0(ω1,ω2)+B1(ω1,ω2)e−jω
1−A0(ω1,ω2)−A1(ω1,ω2)e−jω

2
σ 2w + σ 2v
=
H∗(ω1,ω2,ω)
B0(ω1,ω2) + B1(ω1,ω2)e−jω2 σ 2
w
|H|2 (ω1,ω2,ω)
B0(ω1,ω2)+B1(ω1,ω2)e−jω2 σ 2w +|1−A0(ω1,ω2)−A1(ω1,ω2)e−jω|2σ 2v
.
We notice that when |H|2 (ω1,ω2,ω)
B0(ω1,ω2) + B1(ω1,ω2)e−jω2 σ 2
w ≫|1 −A0(ω1,ω2)
−A1(ω1,ω2)e−jω|2σ 2
v, then G is very close to an inverse ﬁlter. This condition is just the case
where the blurred signal PSD |H|2 Sx ≫σ 2
v, the power density of the white observation
noise.
More on intraframe and interframe Wiener ﬁlters, along with application to both
estimation and restoration, is contained in the text by Tekalp [2].
In a 2-D ARMA model, if the input coefﬁcient ﬁltering through bk1,k2,k is absent,
we have just the 2-D NSHP AR model, which can be NSHP Markov in the case where
the input random ﬁeld is i.i.d. In particular, if the input sequence w is white Gaussian,
then the resulting random ﬁeld is NSHP Markov. Next we turn to spatiotemporal or
3-D Markov.
10.4 SPATIOTEMPORAL MARKOV MODELS
We start out with the general deﬁnition of Markov random ﬁeld sequence.
Deﬁnition 10.4–1: Discrete Markov Random Field Sequence
Let x be a random ﬁeld sequence on Z3, the 3-D lattice. Let a band of minimum width p,
∂G (“the present”) separate Z3 into two regions: G+ (“the future”) and G−(“the past”).
Then x is Markov-p if {x|G+ given x|∂G} is independent of {x|G−} for all ∂G.
These regions are illustrated in Figure 10.4–1, which shows a spherical region G+
inside a sphere of thickness ∂G. Outside the sphere is the region G−.

10.4 Spatiotemporal Markov Models
407
+
∂
−
FIGURE 10.4–1
Noncausal Markov ﬁeld regions. G+ is the region inside a sphere of thickness ∂G.
For the homogeneous, zero-mean Gaussian case, this noncausal Markov deﬁni-
tion can be expressed by the following recursive equation model:
x(n1,n2,n) =
X
(k1,k2,k)∈Dp
c(k1,k2,k)x(n1 −k1,n2 −k2,n −k) + u(n1,n2,n),
(10.4–1)
where
E{x(n1,n2,n)u(k1,k2,k)} = σ 2
uδn1,k1δn2,k2δn,k,
Dp ≜{n1,n2,n|n2
1 + n2
2 + n2 ≤p2 and (n1,n2,n) ̸= (0,0,0)}.
Here, u(n1,n2,n) is a Gaussian, zero-mean, homogeneous random-ﬁeld sequence
with correlation function of bounded support,
Ru(n1,n2,n) =



σ 2
u,
(n1,n2,n) = 0,
−ck1,k2,tσ 2
u,
(n1,n2,n) ∈Dp −0,
0,
elsewhere.
The ck1,k2,k are the interpolation coefﬁcients of the minimum mean-square error
(MMSE) linear interpolation problem:
Given data x for (n1,n2,n) ̸= 0, ﬁnd the best linear estimate of s(0). The solution
is given as the conditional mean,
E{x(0)| x on (n1,n2,n) ̸= 0}
=
X
(k1,k2,k)∈Dp
c(k1,k2,k)x(−k1,−k2,−k).
In this formulation, σ 2
u is the mean-square interpolation error.

408
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
Actually, in Fig. 10.4–1 we could swap the past and future, G−and G+, and this
would correspond to running a 1-D Markov process either forward or backward in
time. However, as labeled in the Figure, with the ‘future’ inside, when we shrink the
sphere down to just one interior point, we get the interpolative model (10.4–1).
Causal and Semicausal 3-D Field Sequences
The future G+ can be deﬁned in various ways that are constrained by the support of
the coefﬁcient array supp(c):
•
{n > 0} in the temporally causal case.
•
{(n1,n2) ̸= (0,0),n = 0} ∪{n > 0} in the temporally semicausal case.
•
{n1 ≥0,n2 ≥0,n = 0} ∪{n1 < 0,n2 > 0,n = 0} ∪{n > 0}, the totally ordered
temporally causal case, also called a nonsymmetric half-space (NSHS).
The future-present-past diagram of the temporally causal model is shown in
Figure 10.4–2. It can be obtained from the noncausal model by stretching out the
radius of the sphere in Figure 10.4–1 to inﬁnity. Then the thickness of the sphere
becomes the plane shown in the temporally causal Figure 10.4–2 for the Markov-1
case. A ﬁrst-order temporally causal model can be constructed as follows:
x(n1,n2,n) =
X
k1,k2
c(k1,k2,1)x(n1 −k1,n2 −k2,n −1) + u(n1,n2,n),
u(n1,n2,n) =
X
k1,k2
c1(k1,k2,1)x(n1 −k1,n2 −k2,n) + u1(n1,n2,n).
Here, u(n1, n2,n) is uncorrelated in the time direction only, and u1(n1,n2,n) is strict
near-neighbor correlated in space but completely uncorrelated in time. Effectively,
we have a sequence of 2-D Gauss-Markov4 random ﬁelds u, driving a frame-wise
predictive recursion for x.
The future-present-past diagram for a temporally semicausal model is shown in
Figure 10.4–3.
FIGURE 10.4–2
Temporally causal. The dot (·) indicates the present, and the plane below is the
immediate past.
4The ﬁeld x becomes Markov in the Gaussian case. Otherwise, it is only wide-sense Markov.

10.4 Spatiotemporal Markov Models
409
FIGURE 10.4–3
Temporally semicausal. The dot (·) indicates the present, and it is surrounded by the
immediate past.
We can construct a Markov pth-order temporal example:
x(n1,n2,n) =
X
Dp
c(k1,k2,k)x(n1 −k1,n2 −k2,n −k) + u(n1,n2,n),
where
Ru(n1,n2,n) =



σ 2
u,
n = 0 and (n1,n2) = 0,
−ck1,k2,tσ 2
u,
n = 0 and (n1,n2) ∈Dp −0,
0,
n ̸= 0 or (n1,n2) /∈Dp.
The reader should note that the classiﬁcation of 3-D ﬁeld sequences introduced
here goes beyond Markov models. It is generally useful to consider spatiotempo-
ral models as belonging to one of the three classes: temporally causal, temporally
semicausal, and totally ordered temporally causal, as introduced in this section.
Reduced Update Spatiotemporal Kalman Filter
For a 3-D recursive ﬁlter, we must separate the past from future of the 3-D random
ﬁeld sequence. One way is to assume that the random ﬁeld sequence is scanned in
line-by-line and frame-by-frame (i.e., the totally ordered temporally causal case):
x(n1,n2,n) =
X
(k1,k2,k)∈Sc
c(k1,k2,k)x(n1 −k1,n2 −k2,n −k) + w(n1,n2,n),
where w(n1,n2,n) is a white noise ﬁeld sequence, with variance σ 2
w, and where Sc =
{n1 ≥0,n2 ≥0,n = 0} ∪{n1 < 0,n2 > 0,n = 0} ∪{n > 0}.
Our observed image model is given by
r(n1,n2,n) =
X
(k1,k2,k)∈Sh
h(k1,k2,k)x(n1 −k1,n2 −k2,n −k) + v(n1,n2,n).
The region Sh is the support of the spatiotemporal psf h(k1,k2,k), which can model
a blur or other distortion extending over multiple frames. The observation noise
v(n1,n2,n) is assumed to be an additive, zero-mean, homogeneous Gaussian ﬁeld
with covariance Qv(n1,n2,n) = σ 2
vδ(n1,n2,n), i.e., a white Gaussian noise.
In recursive ﬁltering, only a ﬁnite subset of the NSHS is updated at each step,
called the update region. This update region is slightly enlarged from the model
support or local state for an (M1,M2,M)th-order ⊕⊕+ model of the NSHS variety.

410
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
Local State and Update Region
The local state vector of 3-D spatiotemporal RUKF that is of order M = 1 in the
temporal direction, and spatial order M1 = M2, is given as
x(n1,n2,n) = [x(n1,n2,n),...,x(n1 −M1 + 1,n2,n);
x(n1 + M1 + 1,n2 −1,n),...,s(n1 −M1 + 1,n2 −1,n);
...; x(n1 + M1 + 1,n2 −M2,n),...,x(n1 −M1 + 1,n2 −M2,n);
x(n1 + M1 + 1,n2 + M2,n −1),...,x(n1 −M1 + 1,n2 + M2,n −1);
...; x(n1 + M1 + 1,n2 −M2,n −1),...,x(n1 −M1 + 1,n2 −M2,n −1)]T.
Figure 10.4–4 shows a 1 × 1 × 1-order local state region as the dark pixels, with
a 2 × 2 × 1-order update region U⊕⊕+(n1,n2,n), to be updated recursively, shown
as the medium dark pixels in the ﬁgure. The total region shown is the covariance
update support region T⊕⊕+(n1,n2,n). We see that these regions are NSHP in the
current frame (the top layer) but of a symmetric support in the previous frame (lower
layer). We only show two frames here, but, of course, there could be more, which
would constitute a higher order recursive processing. The current pixel location in
the current frame (n1,n2,n) is shown as the black dot in the ﬁgure.
The resulting approximate RUKF equations are given as
bxb(n1,n2,n) =
X
(k1,k2,k)∈Sc
c(k1,k2,k)bxa(n1 −k1,n2 −k2,n −k),
bxa(k1,k2,k) =bxb(k1,k2,k) + g(n1,n2,n)(n1 −k1,n2 −k2,n −k)
×

r(n1,n2,n) −
X
(l1,l2,l)∈Sh
h(l1,l2,l)bxb(n1 −l1,n2 −l2,n −l)

,
for all (k1,k2,k) ∈U⊕⊕+(n1,n2,n), the current update region.
FIGURE 10.4–4
An illustration of 3-D Kalman-reduced support regions.

10.4 Spatiotemporal Markov Models
411
The gain factors g(n1,n2,n)(n1 −k1,n2 −k2,n −k) are given in terms of error
covariance functions R(n1,n2,n)
b
by
g(n1,n2,n)(k1,k2,k)
=
P
(l1,l2,l)∈Sh h(l1,l2,l)R(n1,n2,n)
b
(n1 −l1,n2 −l2,n −l;n1 −k1,n2 −k2,n −k)
P
(l1,l2,l)∈Sh
P
(o1,o2,o)∈Sh h(l1,l2,l)R(n1,n2,n)
b
(n1 −l1,n2 −l2,n −l;n1 −o1,n2 −o2,n −o) + σ 2v
,
for all (k1,k2,k) ∈U⊕⊕+(n1,n2,n). The error-covariance equations are given recur-
sively as, before update,
R(n1,n2,n)
b
(n1,n2,n;k1,k2,k)
=
X
(l1,l2,l)∈Sc
c(l1,l2,l)R(n1,n2,n)
a
(n1 −l1,n2 −l2,n −l;k1,k2,k),
for all (k1,k2,k) ∈T⊕⊕+(n1,n2,n),
and
R(n1,n2,n)
b
(n1,n2,n;n1,n2,n)
=
X
(l1,l2,l)∈Sc
c(l1,l2,l)R(n1,n2,n)
a
(n1,n2,n;n1 −l1,n2 −l2,n −l) + σ 2
w,
and after update,
R(n1,n2,n)
a
(k1,k2,k;l1,l2,l) = R(n1,n2,n)
b
(k1,k2,k;l1,l2,l) −g(n1,n2,n)(n1 −k1,n2 −k2,n −k)
×
X
(o1,o2,o)∈Sh
h(o1,o2,o)R(n1,n2,n)
b
(n1 −o1,n2 −o2,n −o;l1,l2,l),
for all (k1,k2,k) ∈U⊕⊕+(n1,n2,n) and for all (l1,l2,l) ∈T⊕⊕+(n1,n2,n).
As processing proceeds into the data, and away from any spatial boundaries,
stability of the model generally has been found to lead to convergence of these equa-
tions to a steady state fairly rapidly, under the condition that the covariance update
region T⊕⊕+(n1,n2,n) is sufﬁciently large. In that case, the superscripts on the error
covariances and gain terms can be dropped, and we have simply
g(k1,k2,k)
=
P
(l1,l2,l)∈Sh h(l1,l2,l)Rb (n1 −l1,n2 −l2,n −l;n1 −k1,n2 −k2,n −k)
P
(l1,l2,l)∈Sh
P
(o1,o2,o)∈Sh h(l1,l2,l)h(o1,o2,o)Rb(n1 −l1,n2 −l2,n −l;n1−o1,n2 −o2,n −o)+σ 2v
,
for all (k1,k2,k) ∈U⊕⊕+(n1,n2,n), in which case the 3-D RUKF reduces to a spatio-
temporal LSI ﬁlter.
As in the 2-D and 1-D cases, we see that the nonlinear error-covariance equations
do not involve the data, just the signal model and variance parameters. A good way
to design the steady-state 3D RUKF is to run the error-covariance equations with the
chosen stable signal model, not on the real image sequence, but just on a ﬁctitious

412
CHAPTER 10 Three-Dimensional and Spatiotemporal Processing
but much smaller sequence. Typically, a sequence of 10 frames of 20×10 pixels
is enough to obtain a quite accurate steady-state gain array g for commonly used
image models. The 3-D RUKF has been extended to ﬁlter along motion trajectories
[3, 4], and this version will be covered in the next chapter after introducing motion
estimation.
CONCLUSIONS
This chapter has provided extensions to three dimensions of some of the earlier 2-D
signal processing methods. In the important spatiotemporal case, we have provided
results of a general nature for both deterministic and random models. Chapter 11
will apply these methods to the processing of the spatiotemporal signal that is
video. Chapter 12 will provide applications of spatiotemporal processing in video
compression and transmission.
PROBLEMS
1. Consider a 9 × 9 × 9-order FIR ﬁlter (i.e., 10 taps in each dimension). How
many multiples are necessary per pixel to implement this ﬁlter in general? If 3-D
separable? If each separable factor is linear phase?
2. In Example 10.1–1, we wrote the separable convolution representation
y(n1,n2,n3) = h2(n3) ∗[h1(n1,n2) ∗x(n1,n2,n3)]
of a system with two convolution operations. Carefully determine whether each
one should be a 1-D or 2-D convolution by comparing to previous lines in this
example.
3. Prove the 3-D Fourier transform property
FT

x(n)exp + jωT
0 n
	
= X(ω −ω0),
and specialize to the case
FT

(−1)n1+n2+n3x(n)
	
= X(ω1 ± π,ω2 ± π,ω3 ± π).
4. Extend the 2-D separable window ﬁlter design method of Chapter 5 to the 3-D
case. Would there be any changes needed to the Kaiser window functions?
5. Extend Example 10.3–2 to the case of interlaced 3-D sampling with general sam-
pling distances T1 and T2 in space, and T in time. Display both the sampling
matrix V and the periodicity matrix U.

References
413
6. If we sample the continuous-parameter function xc(t1,t2,t3) with sample vectors
v1 =


1
0
0

,
v2 =


1
1
0

,
and
v3 =


0
0
1

,
express the resulting discrete-parameter Fourier transform X(ω1,ω2,ω3) in terms
of the continuous-parameter Fourier transform Xc(1,2,3). Specialize your
result to the case where there is no aliasing.
7. Consider the ﬁrst noncausal Wiener ﬁlter design method (Section 8.2 of
Chapter 8), and show that the method easily extends to the 3-D and spatiotem-
poral cases.
8. Estimate the memory size needed to run the 3-D RUKF equations on an image
sequence of N frames, each frame being N × N pixels. Assume the model order
is 1 × 1 × 1, the update region is 2 × 2 × 1, and the covariance update region is
4 × 4 × 1.
REFERENCES
[1] P. P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice-Hall, Englewood Cliffs,
NJ, 1993.
[2] A. M. Tekalp, Digital Video Processing, Prentice-Hall, Englewood Cliffs, NJ, 1995.
[3] J. W. Woods and J. Kim, “Motion Compensated Spatiotemporal Kalman Filter,”
Chapter 12 in Motion Analysis and Image Sequence Processing, Eds. I. Sezan and R. L.
Lagendijk, Kluwer, Boston, MA, 1993.
[4] J. Kim and J. W. Woods, “Spatiotemporal Adaptive 3-D Kalman Filter for Video,” IEEE
Trans. Image Process., vol. 6, pp. 414–424, March 1997.

CHAPTER
Digital Video Processing 11
In Chapter 10 we learned the generalization of multidimensional signal processing to
the 3-D and spatiotemporal cases along with some relevant notation. In this chapter
we apply and extend this material to processing the 3-D signal that is video—two
spatial dimensions plus one time dimension. We look at the general interframe or
cross-frame processing of digital video. We study the various methods of motion esti-
mation for use in motion-compensated temporal ﬁltering and motion-compensated
extensions of 3-D Wiener and Kalman ﬁlters. We then look at the video processing
applications of deinterlacing and frame-rate conversion. We next consider Bayesian
methods for motion estimation and segmentation. We brieﬂy consider the applica-
tion of restoration of old ﬁlm/video. We then look at joint motion estimation and
segmentation. Finally, we consider super-resolution applied to video.
11.1 INTERFRAME PROCESSING
Interframe processing is a type of video processing of the present frame that makes
use of input and processed video data from other frames. One area of application
involves the estimation of a noisy video using a recursive ﬁlter that employs prior
processed frames in its estimate of the current frame.
Example 11.1–1: Spatiotemporal RUKF
The spatiotemporal RUKF introduced in Chapter 10 has application in noisy image
sequence processing. Again, the video scanning is assumed to be in the totally ordered
temporal causal sense, and the RUKF processes the data in this scan-ordered sense. In
an example from [1] the spatiotemporal RUKF processes a noisy version of the test clip
salesman at the spatial resolution of 360 × 280 pixels, and the frame rate of 15 frames/sec
(fps). We created the noisy input by adding Gaussian white noise to set up an input signal-
to-noise ratio (SNR) of 10 dB. After processing, the SNR improved by 6 to 7 dB, to 16 to
17 dB after an initial start-up transient of approximately 10 frames.
We see a frame from the noise-free salesman clip in Figure 11.1–1, followed by the
noisy version in Figure 11.1–2. The smoothed result from the spatiotemporal (3-D) RUKF
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00011-4
c⃝2012 Elsevier Inc. All rights reserved.
415

416
CHAPTER 11 Digital Video Processing
FIGURE 11.1–1
A frame from the original clip salesman.
FIGURE 11.1–2
A frame from the salesman clip with white noise added to achieve SNR = 10 dB.
is shown in Figure 11.1–3. We see that the white noise has been reduced and the image
frame has become somewhat blurred or oversmoothed.
The limit to what can be achieved by LSI spatiotemporal ﬁltering is determined
by the overlap of the 3-D spectra of signal and noise. For fairly low-resolution signal
and white noise, there will be a lot of overlap, and hence a smoothing of the signal
is unavoidable. For higher resolution video, such as obtained from HD and SHD

11.1 Interframe Processing
417
FIGURE 11.1–3
A frame from the 3-D RUKF estimate.
cameras or the high-resolution scanning of 35- and 70-mm ﬁlm, the overlap of the
signal and noise spectra can be much less. In that case, more impressive results can
be obtained.
The next example shows how deterministic multidimensional ﬁlters can be used
in processing analog standard deﬁnition television signals to obtain what is called
improved deﬁnition. A little known fact is that the National Television System
Committee (NTSC) color television lost some luma resolution that was present in
the prior monochrome standard. Using multidimensional ﬁlters, it was possible to
largely get back the lost resolution [2].
Example 11.1–2: Improved-Deﬁnition Television
The 3-D spectra of the old analog NTSC terrestrial broadcast television1 is sketched in
Figure 11.1–4, where the luma power is concentrated at low frequencies, and the chroma
data have been modulated up to the sites shown via a so-called chroma subcarrier. Com-
positing the chroma data together with the luma data like this required, ﬁrst lowpass
ﬁltering of the luma data, and then rather severe lowpass ﬁltering of the chroma data. This
was the compromise solution that the NTSC decided upon to provide compatible color
television in mid-20th century America (that is compatible with the previously authorized
monochrome, or luma-only TV). The downside of the NTSC compatible color TV was that
the luma resolution went down slightly and the chroma (color) resolution was only about
a sixth to an eighth of that of the luma (monochrome) resolution, and that led to rather
1This is also known as a composite signal since bandlimited chroma data are composited together with
the bandlimited luma data.

418
CHAPTER 11 Digital Video Processing
fy
fy
fy
fx
fx
fx
Y
I
Q
FIGURE 11.1–4
2-D ﬁlter responses for generalized NTSC encoding of Y, I, and Q components.
([2] c⃝1988 SMPTE)
blurry color. Further complicating matters, typical receivers of the time were not able to
completely separate the two signals, leading to cross-color and cross-luma artifacts. In the
mid-1990s, better solutions to separate the components at the transmitter were achieved
through the use of multidimensional ﬁltering. Perspective plots of three preprocessing ﬁl-
ters are shown in Figure 11.1–4. The diamond band shape of these 11 × 31-point ﬁnite
impulse response (FIR) ﬁlters was obtained using a nonseparable design method by
Dubois and Schreiber [2]. The Y, I, Q color space that was employed in NTSC was similar
to the current Y′,CB,CR color space used for digital TV coding and transmission, being a
similar but somewhat different linear transformation on R′,B′,G′ (see Section 6.5).
These ﬁlters could be employed prior to forming an NTSC composite signal with very
little to no overlap of the luma and quadrature-modulated chroma components. The ﬁlters
in Figure 11.1–4 marked I and Q would then be used to bandlimit the chroma com-
ponents prior to their modulation up to ﬁt in the spectral holes that the ﬁlter marked Y
has created in the Y component. Together with appropriate 2-D postprocessing ﬁlters,
cross-color and cross-luma artifacts were effectively eliminated [2].
Receiver-processing multidimensional ﬁlters went by various names in the con-
sumer electronics business, such as comb ﬁlter, line comb, and frame comb. Clearly,
the best way to avoid these cross-contamination problems is to keep the luma and
chroma signals separate, as in RGB and YIQ component format.
The next example shows modiﬁcation of a designed spatial lowpass ﬁlter to meet
spatial domain step-response characteristics.

11.1 Interframe Processing
419
Example 11.1–3: Video Processing Example (due to Schr¨oder and Blume [3])
Video engineers are very concerned with the spatial impulse and step response of the
ﬁlters used in video processing. In particular, they want the ﬁrst overshoot of the step
response to be close in to the main rising section and the further undershoots and over-
shoots to be small enough to be not visually noticed. One approach would be to use a
general optimization program, extending the design methods of Chapter 5. First, choose
an error criterion based on pth errors (p even) in both the frequency and space domains,
with a weighting parameter λ to control the relative importance of frequency and spatial
errors:
∥HI(ω1,ω2) −H(ω1,ω2)∥p + λ∥hI(n1,n2) −h(n1,n2)∥p .
However, a simpler procedure, due to Schr¨oder [4], is to cascade an enhancement net-
work with the lowpass ﬁlter that was conventionally designed. The goal of an enhancement
circuit is to optimize the overall step response. The three-tap enhancement ﬁlter with
impulse response
hen(n1) = −α
2 δ(n1 −2) + (1 + α)δ(n1) −α
2 δ(n1 + 2)
and frequency response
Hen(ω1) = 1 + α −α cos2ω1
can optimize the horizontal step response of a 2-D lowpass ﬁlter. The value of α is used
to achieve the desired visual deﬁnition characteristics. Figure 11.1–5 shows a properly
optimized step response on a vertical bar test pattern.
FIGURE 11.1–5
Properly optimized or compensated lowpass ﬁltering of test signal. ([4] c⃝2000 John Wiley)

420
CHAPTER 11 Digital Video Processing
FIGURE 11.1–6
Nonoptimized ﬁltering showing oversmoothing. ([4] c⃝2000 John Wiley)
FIGURE 11.1–7
Overoptimized ﬁltering showing ringing. ([4] c⃝2000 John Wiley)
Figure 11.1–6 shows an underoptimized or nonoptimized result and displays an overly
smoothed appearance. Figure 11.1–7 shows the result of an overly optimized ﬁlter (i.e., too
sharp a transition band) showing ringing around the vertical edges. The same enhance-
ment of the visual deﬁnition can be done in the vertical direction by adding the needed

11.2 Motion Estimation and Motion Compensation
421
vertical component to the enhancement ﬁlter, making the overall ﬁlter separable with
3 × 3 support.
11.2 MOTION ESTIMATION AND MOTION COMPENSATION
Motion compensation (MC) is very useful in video ﬁltering to remove noise and
enhance signal. It is useful since it allows the ﬁlter or coder to process through the
video on a path of near-maximum correlation based on following motion trajectories
across the frames making up the image sequence or video. Motion compensation
is also employed in all distribution-quality video coding formats, since it is able to
achieve the smallest prediction error, which is then easier to code. Motion can be
characterized in terms of either a velocity vector v or displacement vector d and is
used to warp a reference frame onto a target frame. Motion estimation is used to
obtain these displacements, one for each pixel in the target frame.
Several methods of motion estimation are commonly used:
•
Block matching
•
Hierarchical block matching
•
Pel-recursive motion estimation
•
Direct optical ﬂow methods
•
Mesh-matching methods
Optical ﬂow is the apparent displacement vector ﬁeld d = (d1,d2) we get from
setting (i.e., forcing) equality in the so-called constraint equation
x(n1,n2,n) = x(n1 −d1,n2 −d2,n −1).
(11.2–1)
All ﬁve approaches start from this basic equation, which is really just an ide-
alization. Departures from the ideal are caused by the covering and uncovering of
objects in the viewed scene, lighting variation both in time and across the objects in
the scene, movement toward or away from the camera, as well as rotation about
an axis (i.e., 3-D motion). Often the constraint equation is only solved approxi-
mately in the least-squares sense. Also, the displacement is not expected to be an
integer as assumed in (11.2–1), often necessitating some type of interpolation to
be used.
Motion cannot be determined on a pixel-by-pixel basis since there are two compo-
nents for motion per pixel, and hence twice the number of unknowns as equations. A
common approach then is to assume the motion is constant over a small region called
the aperture. If the aperture is too large, then we will miss detailed motion and only
get an average measure of the movement of objects in our scene. If the aperture is too
small, the motion estimate may be poor to very wrong. In fact, the so-called aperture
problem concerns the motion estimate in the square region shown in Figure 11.2–1.

422
CHAPTER 11 Digital Video Processing
v
FIGURE 11.2–1
Illustration of the aperture problem with the square indicating the aperture size.
Changed region
Background
Covered
Uncovered
Object
Frame n
Frame
n −1
Background
Background
Background
FIGURE 11.2–2
Illustration of covering and uncovering of background by an object moving in the
foreground.
If the motion of the uniform dark region is parallel to its edge, then this motion can-
not be detected. Since this situation would typically only hold for small regions in
natural images, the aperture effect leads us to choose a not-too-small aperture size.
Thus, ﬁnding the right aperture size is an important problem that depends on the
video content.
Another issue is covering and uncovering, as illustrated in Figure 11.2–2, show-
ing a 1-D depiction of two successive frames n and n −1, with an object moving to
the right. We assume a simple object translating in the foreground over a ﬁxed back-
ground, not an unreasonable local approximation of video frames. We see that part of
the background region in target frame n is uncovered, while part of the background
region in reference frame n −1 is covered. Motion estimation that tries to match
regions in the two frames will not be able to ﬁnd good matches in either the cov-
ered or uncovered regions. However, within the other background regions, matches
should be good and matching should also be good within a textured object, at least if

11.2 Motion Estimation and Motion Compensation
423
Search area in
frame (n −1)
Fixed block
in frame (n)
n
Time
n −1
FIGURE 11.2–3
Illustration of simple block matching.
it moves in a trackable way, and the pixel samples are dense enough. The problem in
the relatively small covered/uncovered regions is that there are two motions present
there.
Block-Matching Method
We intend to estimate a displacement vector at the location (n1,n2,n) in the target
frame. In block matching (BM) [5], we use template matching of the block centered
on this point to blocks in a speciﬁed search area in the reference frame, as illustrated
in Figure 11.2–3, where we take the immediately prior frame as reference.
Often the search area size is given as (±M1,±M2) and centered on the current
pixel location (n1,n2) in the reference frame, then a total of (2M1 + 1)(2M2 + 1)
searches must be done in a full search, and this must be done for each pixel where
the motion is desired. Often the block matching is not conducted at every pixel in the
target frame and an interpolation method is used to estimate the motion in between
these points. Common error criteria are mean-square error (MSE), mean-absolute
error (MAE),2 or even number of pixels in the block actually disagreeing for discrete-
amplitude or digital data.
2Actually preferred in practical applications because MAE works a bit better than MSE owing to
being more robust, and it only involves additions. MAE goes by other acronyms, too: mean absolute
difference (MAD) and sum absolute difference (SAD).

424
CHAPTER 11 Digital Video Processing
We express the MSE in block matching as
E(d) ≜
X
k
 x(n + k,n) −x(n + k −d,n −1)
2,
(11.2–2)
for a square block centered at position n = (n1,n2)T as a function of the vector
displacement d = (d1,d2)T. We seek the displacement vector that minimizes this
error
do≜argmin
d E(d).
Since the MSE in (11.2–2) is susceptible to outliers, often the MAE is used
in applications. In addition to its being less sensitive to statistical outliers, another
advantage of MAE is that it is simpler to compute.
In addition to the computationally demanding full-search method, there are sim-
pler approximations involving a much reduced number of evaluations of the error
(11.2–2). The methods either involve sampling the possible locations in the search
region or sampling the elements in the block when calculating (11.2–2). Two exam-
ples of the former strategy are 2-D log search and three-step search. With reference to
Figure 11.2–4, the three-step search proceeds as follows: First, the search window is
broken up into four quadrants, and motion vectors are tested on a 3 × 3 grid with cor-
ners centered in the four quadrants. Here, we illustrate a case where the lower right
corner is the best match at the ﬁrst step, the top right corner is best at the second step,
and the top right corner is best at the third and ﬁnal step. A performance comparison
is shown in Figure 11.2–5 from [6].
We note that all three techniques perform much better than simple frame differ-
encing, which equivalently treats all displacement vectors as zero. While both fast
FIGURE 11.2–4
An illustration of three-step block matching.

11.2 Motion Estimation and Motion Compensation
425
25
24
23
22
21
20
19
18
17
16
150
5
10
15
Frame number
Exhaustive search
Frame difference
Three-step search
2D-log search
Peak signal-to-noise ratio (dB)
20
25
30
FIGURE 11.2–5
Illustration of PSNR performance of exhaustive, 2-D log, three-step search, and simple
frame difference. ([6] c⃝1995 Academic Press)
methods cut computation by a factor of 10 or more versus full-search block match-
ing, they can lose up to 1–2 dB in prediction peak SNR (PSNR), measured in decibels
(dB) as
PSNR = 10log10
2552
E(d)

,
for the 8-bit images being considered, since their peak value would be 255. For 10-bit
images, the formula would substitute 1023 for the peak value.
The other class of methods to speed up block matching involve sampling in the
calculation of the error (11.2–2). So the amount of computation in evaluating the dis-
tortion for each searched location is reduced. Liu and Zaccarin [7] presented a method
involving four phases of subsampling and alternated subsampling patterns among the
blocks, while using the MAE error criterion. This method achieved approximately a
four-fold reduction in the amount of computation, but only a slight increase in the
average prediction MSE.
There are some unavoidable problems with the block-matching approach. A small
block size can track small moving objects, but the resulting displacement estimate is

426
CHAPTER 11 Digital Video Processing
then sensitive to image noise.3 For example, a small block might just span a ﬂat
region in the image, where the displacement cannot be deﬁned. A large block size is
less sensitive to noise, but cannot track the motion of small objects. Similarly, a large
search area can track fast motion but is computationally intensive. A small search
area may not be large enough to catch or track the real motion.
The best matching block is often good enough for a block-based predictive
video compression, where bits can be spent coding the prediction residual. How-
ever, in video ﬁltering, when the estimated motion is not the true physical motion,
visible artifacts will often be created. Hence, we need an improvement on the
basic block-matching method for the ﬁltering application. The same is true for MC
interpolation, frame-rate conversion, and pre- and postprocessing in video com-
pression. Also, some highly scalable video coders use MC in a temporal ﬁltering
structure to generate lower frame-rate versions of the original video. Highly accu-
rate motion vectors are important in this case too. A variation of block matching,
called hierarchical block matching, can achieve a much better estimate of the “true
motion.” 4
Hierarchical Block Matching
The basic idea of hierarchical algorithms is to ﬁrst estimate a coarse motion vec-
tor at low spatial resolution. Then this estimated motion is reﬁned by increasingly
introducing higher spatial frequencies. Both subband/wavelet pyramids and Gaussian
pyramids have been used for this purpose. An often cited early reference on
hierarchical block matching (HBM) is Thoma and Bierling [8].
We start by creating a spatial pyramid, with resolution levels set at a power of
two. Typically three or four stages of resolution are employed. We start at the lowest
resolution level (highest pyramid level) and perform simple block matching. It has
been found helpful to have the block size there agree with the average size of the
largest moving areas in the video. Next, we start down the pyramid, increasing the
resolution by the factor 2 × 2 at each level. We double the displacement vector from
the previous level to get the initial search location at the present level. We ﬁnish up at
the pyramid base level, which is full resolution. Both the block sizes and the search
regions can be chosen distinctly for each resolution and are generally in the range
from 4 × 4 to 32 × 32. The maximum search area is usually small at each resolution
(i.e., ±2), since only reﬁnements are needed. The search area may also be small at the
initial pyramid level because of the low resolution. Thus we can expect considerable
savings in complexity for HBM. Some other improvements to block matching include
subpixel accuracy, variable-size block matching (VSBM), overlapping blocks, block
prediction of motion vectors, and hierarchical VSBM (HVSBM).
3By the way, it turns out that some amount of noise is always present in real images. This comes from
the common practice of setting the bit depth on sensors and scanners to reach the ﬁrst one or two bits
of the physical noise level, caused by photons, ﬁlm grains, etc.
4Really it is optical ﬂow—i.e., the apparent movement that comes from our 2-D observations.

11.2 Motion Estimation and Motion Compensation
427
Splitting 
Splitting 
Refining
Refining
Refining
Initial motion vector tree 
FIGURE 11.2–6
An illustration of the reﬁning and spliting process in HVSBM.
A diagram of HVSBM is shown in Figure 11.2–6. We start with a spatial pyramid
that can be obtained as a succession of LL subbands by subband/wavelet ﬁltering
and 2 × 2 decimation. Starting at the coarsest level, at the top of the pyramid, a
block-matching motion estimation is performed. Then this displacement estimate d0
is propagated down the pyramid one level, and 2d0 is used to initialize a search over
a small region to reﬁne the motion value to d1. At this time, if the MC error measure
is too large, the block is split into four, and the process of reﬁning is repeated to gen-
erate d1, and this process of reﬁning and splitting is carried down the pyramid to the
bottom, resulting in a variable size block-based motion ﬁeld. In a computationally
more demanding variation of HVSBM, we start at the coarsest resolution with the
smallest block size, and reﬁne this motion ﬁeld to the bottom of the pyramid (i.e., the
highest resolution level). Then this resulting motion ﬁeld is pruned back by merging
nodes to a variable-size block-based motion ﬁeld. This can be done using the BFOS
algorithm, and this bottom-up approach generally results in a more accurate motion
ﬁeld than the top-down method mentioned in the previous paragraph, but it is more
computationally intensive.
In a video coding application, the error criteria can be composed of motion ﬁeld
error, either MSE or MAE, to weigh the increase in motion-vector rate due to the
split (top-down) or decrease in motion-vector rate due to the merge (bottom-up). A
Lagrangian approach is often used to control the bitrate of the motion information.
More on coding motion vectors for video compression is contained in Chapter 12.
Overlapped Block Motion Compensation
The motivation to overlap the blocks used in a conventional block-matching estimate
is to increase the smoothness of the resulting velocity ﬁeld. This can be considered as

428
CHAPTER 11 Digital Video Processing
a method to reduce the spatial frequency aliasing in sampling the underlying velocity
ﬁeld. While one could simply overlap the blocks used in a simple block-matching
estimate, this could mean much more computation. For example, if the blocks were
overlapped by 50% horizontally and vertically, it would be four times more com-
putation if the block-matching estimation were done independently, as well as four
times more velocity information to transmit in the video compression application
of Chapter 12. So, effectively we are more interested in smoothing than in alias
reduction, and the overlapped block motion compensation (OBMC) technique [9, 10]
simply weights each velocity vector with four neighbor velocity estimates from the
four nearest neighbor nonoverlapping blocks. Thus we effectively overlap the veloc-
ity vectors without overlapping the blocks themselves. This is usually done with a
few prescribed weighting windows.
A theoretical motivation for the overlapping can be obtained from (4) of [10],
bx(n, n) = E{x(n, n)|X(n −1),Vn}
=
Z
fn(n|Vn)x(n −v1t,n −1)dv,
only slightly changed for our notation. Here, we are performing a motion-
compensated estimate of frame X(n), as a conditional mean over shifted versions
of frame X(n −1), with interframe interval 1t, making use of the conditional pdf
fn(v|Vn), which depends on Vn, the motion vector data sent in this and neighboring
blocks. Assuming linear weights (i.e., no dependence on the data values in Vn), they
obtain
bxn(n) =
X
Nb(x)
wb(n)x(n −vb1t,n −1),
(11.2–3)
where the sum is over velocity vectors in the neighboring blocks Nb(n). A formula
for obtaining an optimized block weighting function wb(n) is given in [10]. Simple
weighting windows are given there too.
The initial estimate obtained in this way can be improved upon by iteratively
updating the velocity estimates from the various blocks, one at a time, by utilizing
the resulting overlapped estimate (11.2–3) in the error calculation (11.2–2). OBMC
is used in the H.263 video compression standard for visual conversation and has also
been adapted for use in some SWT video coders. In the compression application, it
can smooth the velocity ﬁeld without the need to transmit additional motion vector
bits, since the block overlapping can be done separately at the receiver given the
transmitted motion vectors, still only one for each block. The overlapping of the
blocks makes the velocity ﬁeld smoother and removes the artiﬁcial blocked structure.
This is especially important for SWT coders, where a blocky motion vector ﬁeld
could lead, through motion compensation, to a blocky prediction residual that would
have false and excessively high spatial frequency information.

11.2 Motion Estimation and Motion Compensation
429
Pel-Recursive Motion Estimation
This iterative method recursively calculates a displacement vector for each pixel
(a.k.a. pel) in the current frame. We start with an estimate d = (d1,d2)T for the
current displacement. Then we use the iterative method,
bd(k+1)
1
=bd(k)
1
−ϵ ∂E
∂d1
|d=bd(k),
bd(k+1)
2
=bd(k)
2
−ϵ ∂E
∂d2
|d=bd(k),
with initial value supplied by the ﬁnal value at the previously scanned pixel,
bd(0)(n1,n2) =bd(ﬁnal)(n1 −1,n2).
A key reference is [11]; see also [6]. The method works well with just a few iterations
when the motion is small, but often fails to converge when the displacements are
large. In [12], this differential displacement approach was extended to hierarchically
structured motion estimation, with application to image sequence frame interpolation.
Optical Flow Methods
Optical ﬂow is a differential method that works by approximating the derivatives
rather than the function error itself, as in block matching. It is a least-squares
approximation to the spatiotemporal constraint equation,
vx
∂f
∂x + vy
∂f
∂y + ∂f
∂t = 0,
(11.2–4)
which is derived by partial differentiation of the optical ﬂow equation (11.2–1),
rewritten as a function of real variables (x,y) with velocity parameters vx and vy,
f(x,y,t) = f(x −vxdx,y −vydy,t −1t).
Because of noiseinthe frame,(11.2–4)is thensubjectedtoleast squaresapproximation
to give the optical ﬂow velocity estimate. Speciﬁcally, we form the error criteria
EMV ≜

vx
∂f
∂x + vy
∂f
∂y + ∂f
∂t
2
,
to be minimized over local regions.
In practice, a smoothing term must be added to this error term to regularize the
estimate, which otherwise would be much too rough—i.e., too much high-frequency
energy in the estimate bv(x,y,t). In the Horn and Schunck method [13], a gradient
smoothness term is introduced via a Lagrange multiplier as
λES ≜λ
h
∥∇vx∥2 +
∇vy
2i
= λ
"∂vx
∂x
2
+
∂vx
∂y
2
+
∂vy
∂x
2
+
∂vy
∂x
2#
,

430
CHAPTER 11 Digital Video Processing
which, for large values of the positive parameter λ, makes the velocity estimate
change slowly as a function of the spatial variables x and y.
Integrating over the area of the image, we get the total error to be minimized as
ET =
Z Z
(EMV + λES)dxdy
=
Z Z (
vx
∂f
∂x + vy
∂f
∂y + ∂f
∂t
2
+ λ
"∂vx
∂x
2
+
∂vx
∂y
2
+
∂vy
∂x
2
+
∂vy
∂x
2#)
dxdy.
We seek the minimizing velocity vector
bv ≜argminET(v).
The calculus of variations is then used to ﬁnd the minimum of this integral in terms
of the unknown functions vx(x,y,t) and vy(x,y,t) for each ﬁxed frame t. The result-
ing equations are then approximated using ﬁrst-order approximations for the various
derivatives involved. Longer digital ﬁlters may provide improved estimates of these
derivatives of the, assumed bandlimited, analog image frames [14]. An iterative
solution is then obtained using Gauss-Seidel iterations.
While this estimate has been used extensively in computer vision, it is not often
used in video compression because of its rather dense velocity estimate. However,
optical ﬂow estimates have been used extensively in video ﬁltering, where the need
to transmit the resulting motion vectors does not occur. There it can give a smooth
and consistent performance, with few motion artifacts. A modern optical ﬂow method
is presented in Section 5.4 of Chapter 5 in The Essential Guide to Video [15]. The
main problem with optical ﬂow methods is that the smoothness of their motion does
not allow discontinuities of motion across object boundaries in the scene.
Mesh-Based Methods
In a mesh-based method, similar to block-based, a regular grid of velocity points
called control points is set up in the target frame and the corresponding matched
points in a reference frame. But unlike block matching, the motion is not consid-
ered constant between the control points. Rather, these points are used to set up an
afﬁne motion model. An afﬁne model has six parameters and represents rotation and
translation, projected onto the image plane, as
d1(x1,x2) = a11x1 + a12x2 + a13,
d2(x1,x2) = a21x1 + a22x2 + a23,
where the position vector x = (x1,x2)T and d(x) is the displacement vector. We can
see translational motion as the special case where only a13 and a23 are nonzero and
the displacement is constant at (d1,d2) = (a13,a23) in this block. The motion warping

11.2 Motion Estimation and Motion Compensation
431
FIGURE 11.2–7
Illustration of regular triangular mesh grid on target frame.
effect of an afﬁne motion model has been found to well approximate the apparent
motion or optical ﬂow of pixels on rigid objects. If we break up the squares of the
regular grid in the target frame into triangles, we get triangular patches, with three
control points at the vertices of each triangle, as seen in Figure 11.2–7, and a separate
afﬁne model can be determined for each triangle patch—i.e., six linear equations in
the six unknowns (a11,a21,a12,a22,a13,a23).
Because the control points are shared between two adjoining patches, the resulting
velocity ﬁeld will be continuous across the patch boundaries unlike the case in block
matching. In the reference frame, when the control points are properly matched, the
triangular grid will appear warped. This grid shape indicates that spatially warped
prediction is being applied from the reference frame to the target frame. The con-
tinuity of the velocity ﬁeld makes the prediction more subjectively acceptable but
does not usually lead to better objective error performance in either an MSE or MAE
sense. Basically, a small geometric error is hardly noticeable, but it can affect the
objective error a lot. Pseudo code for performing mesh matching follows:
FOR each grid point
Do block matching, with block centered by grid point, to find di.
Ti = 1;
END FOR
WHILE not exceed maximum iterations
FOR each grid point Vi
IF Ti == 1
Refine the motion vector by mindi
P
k Ek, where Ek is
the prediction error of each triangle that connected
to this grid point
IF di does not change
Ti = 0;
ELSE
Ti = 1;

432
CHAPTER 11 Digital Video Processing
FOR any grid point Vj that edge-connects with Vi
Tj = 1;
END FOR
END FOR
IF PTi < 1 BREAK
END WHILE
This algorithm ﬁrst performs block matching to get initial displacement estimates
for the control grid points. It then iteratively visits all the grid points in succession,
looking for better afﬁne model ﬁts. The Ti variable keeps track of whether the con-
trol vector at that grid point is converged or not. The following example has used
this algorithm with the MAE error criteria to perform a mesh-based motion estima-
tion. The max number of iterations was set to three, and the search area was set at
(±31,±31).
Example 11.2–1: Mesh Matching Versus Block Matching
We look at two examples of warping frame 93 of the CIF5 clip foreman at 30 fps to match
the next frame 94. There is enough motion between these two frames to illustrate the
shortcomings of each approach. We use ﬁxed-size 32 × 32 blocks here. Figure 11.2–8
shows frame 94 with the ﬁxed-size triangular grid overlaid upon it. Figure 11.2–9 shows
frame 93 with the found warped triangular mesh overlaid. We see that there is consid-
erable movement between these two frames, and it is mainly in the foreman’s mouth
and chin region. Figure 11.2–10 shows the resulting spatially warped estimate of frame
94. It clearly displays warping errors, most obvious in the lower facial region. Finally, in
FIGURE 11.2–8
Frame 94 of the foreman clip with ﬁxed-size triangular grid overlaid.
5See appendix on video formats at the end of this chapter.

11.2 Motion Estimation and Motion Compensation
433
FIGURE 11.2–9
Frame 93 of the foreman clip with warped grid overlaid.
FIGURE 11.2–10
Warped prediction of frame 94 of the foreman clip from the preceding frame using
triangular ﬁxed-size mesh matching.
Figure 11.2–11 we show the corresponding block-matching estimate for the same 32 ×
32 ﬁxed size grid. We can see obvious blocking artifacts here. We see evident distortions
in each prediction of frame 94, mostly near the mouth region of the foreman’s face. You
can watch the full videos included in a folder on this book’s Web site.
The mesh may be ﬁxed size or variable size, and the motion parameters aij may
be estimated hierarchically or not. A popular choice is variable-size mesh matching
(VSMM). Use of variable size blocks can reduce both the blocking and warping
artifacts in this example. Finally, and distinct from mesh matching, there also exist
generalized block models with afﬁne or more general polynomial motion models
within each block [16].

434
CHAPTER 11 Digital Video Processing
FIGURE 11.2–11
Fixed-size block-matching prediction of frame 94 of the foreman clip from the preceding
frame.
11.3 MOTION-COMPENSATED FILTERING
If the motion is slow or there is no motion at all, then simple temporal ﬁltering can be
very effective for estimation, restoration, frame-rate change, interpolation, or pre- or
postprocessing. In the presence of strong motion, however, artifacts begin to appear
in the simple temporal ﬁlter outputs and the needed coherence in the input signal
begins to break down. Such coherence is needed to distinguish the signal from the
noise, distortion, interference, and artifacts that may also be present. A solution is to
modify the ﬁlter trajectory so that it follows along the trajectory of motion. In this
way, signal coherence is maintained, even with moderate to fast motion.
The basic idea is to modify an LSI ﬁlter as follows:
y(n1,n2,n) =
X
k1,k2
h(k1,k2,0)x(n1 −k1,n2 −k2,n)
+
X
k1,k2
h(k1,k2,1)x(n1 −d1 −k1,n2 −d2 −k2,n −1)
(11.3–1)
+ etc.
Here, d1 = d1(n1,n2,n) is the horizontal component of the displacement vector
between frames n and n −1, and d2 is the vertical component of displacement. In
order to get the corresponding terms for frame n −2, we must add the displacement
vectors from the frame pair n −1 and n −2 to get the correct displacement. We should
add them vectorially:
d′
1(n1,n2) = d1(n1,n2,n) + d1
 n1 −d1(n1,n2,n),n2 −d2(n1,n2,n),n −1

,
d′
2(n1,n2) = d2(n1,n2,n) + d2
 n1 −d1(n1,n2,n),n2 −d2(n1,n2,n),n −1

.

11.3 Motion-Compensated Filtering
435
n
n −1
n−2
n−3
Time
(n1−d1, n2−d2)
(n1, n2)
FIGURE 11.3–1
An illustration of motion-compensated ﬁltering along a motion path.
Here, we assume that the displacement vectors are known, most likely because they
were estimated previously, and that they are integer valued. If they are not inte-
ger valued, which is most of the time, then the corresponding signal value, such as
x(n1 −d1 −k1,n2 −d2 −k2,n −1), must itself be estimated via interpolation. Most
often, spatial interpolation is used based on the use of various lowpass ﬁlters. Effec-
tively, in (11.3–1) we are ﬁltering along the motion paths rather than simply ﬁltering
straight forward (or backward) in time (n) at each spatial location. One way to
conceptualize this is via the diagram in Figure 11.3–1.
MC-Wiener Filter
In Figure 11.3–2, MC denotes motion-compensated warping performed on the noisy
observations y(n1,n2,n). The Wiener ﬁltering is then done on the warped data in
the MC domain, with signal and noise PSDs calculated from some similar MC data.
Finally, the inverse motion compensation (IMC) operator dewarps the frames back
to original shape to produce the output estimatebx(n1,n2,n). Three-dimensional MC-
Wiener ﬁltering was introduced in [17]. The concept of IMC depends on the motion
ﬁeld being one-to-one. In a real image sequence, there is a relatively small number of
pixels where this is not true due to coverings and uncoverings of objects in the scene,
the so-called occlusion problem. In these areas, some approximation must be used in
Figure 11.3–2 in order to avoid introducing artifacts into the ﬁnal video estimate. It
is common to resort to intraframe ﬁltering in these occluded areas.
Because of the strong correlation in the temporal direction in most video, there
is often a lot to gain by processing the video jointly in both the temporal and spatial
directions. Since a Wiener ﬁlter is usually implemented as an FIR ﬁlter, exploitingthe

436
CHAPTER 11 Digital Video Processing
spatiotemporal
Wiener filter
MC
IMC
y
x
FIGURE 11.3–2
Illustration of MC warping followed by Wiener ﬁlter followed by IMC warping (IMC).
temporal direction in this way means that several to many frames must be kept in
active memory. An alternative to this method is the spatiotemporal Kalman ﬁlter,
which can use just one frame of memory to perform its recursive estimate, a motion-
compensated version of which is presented next. Of course, both methods require the
estimation of a suitable image sequence model and noise model. The signal model
spectra can be obtained via estimation on similar noise-free data for the Wiener ﬁlter,
while the Kalman ﬁlter needs parameter estimation of an autoregressive model. Both
models must be trained or estimated on data that have been warped by the motion
compensator, since this warped domain is where their estimate is performed.
MC-Kalman Filter
The basic idea here is that we can apply the totally ordered temporal 3-D RUKF of
Chapter 10 along the motion trajectory using a good motion estimator that approxi-
mates true motion. As before, we can use multiple models for both motion and image
estimation. To reduce object blurring and sometimes even double images, we effec-
tively shift the temporal axis of the ﬁlter to be aligned with motion trajectories. When
a moderate-sized moving object is so aligned, we can then apply the ﬁlter along the
object’s trajectory of motion by ﬁltering the MC video. Since the MC video has a
strong temporal correlation, its image sequence model will have a small prediction
error variance. This suggests that high spatial frequencies can be retained even at low
input SNRs via motion-compensated Kalman ﬁltering. The overall block diagram of
a motion compensated 3-D Kalman ﬁlter of Woods and Kim [1], or MC-RUKF, is
shown in Figure 11.3–3.
This motion-compensated spatiotemporal ﬁlter consists of three major parts: the
motion estimator, the motion compensator, and the 3-D RUKF. While ﬁltering a
video, two different previous frames could be used for motion estimation; one could
use either the previous smoothed frame E{x(n −1)|y(n),y(n −1),...} or the previ-
ous noisy frame y(n −1). In our work, we have generally found it best to use the
smoothed previous frame, since it is the best estimate currently available. For motion
estimation, we used an HBM method.
The motion estimate is used to align a set of local frames along the motion
trajectory. To effect this local alignment, the smoothed previous frame estimate is
displaced to align with the current frame. In an iterative method extension shown
in Figure 11.3–3, two smoothed frames are used to improve on the initial motion
estimates. These smoothed frames retain spatial high frequencies and have reduced

11.3 Motion-Compensated Filtering
437
Motion estimation
Noisy
image
sequence
Inverse
motion compensation
Kalman filter
Segmentation (3 regions)
Motion compensation
Change detection
Smoothed
image sequence 
Iteration
FIGURE 11.3–3
System diagram for motion-compensated spatiotemporal Kalman ﬁlter.
noise, so that these frames can now be used for motion estimation with a smaller
size block. A motion vector ﬁeld is then estimated from these two frames (i.e.,
E{x(n −1)|y(n),y(n −1),...} and E{x(n)|y(n + 1),y(n),y(n −1),...}), followed by a
second application of the steady-state 3-D RUKF. A few iterations sufﬁce.
Multimodel MC-RUKF
The local correlation between two MC frames depends on the accuracy of the motion
estimation. Since the SNR of the noisy observed video will be low, the motion esti-
mation has a further limitation on its accuracy, due to statistical variations. To deal
with the resulting limited motion vector accuracy, we can use a variable number of
models to match the various motion regions in the image; for example, we can use
three motion models: still, predictable, and unpredictable. The motivation here is that
in the still region, we can perform unlimited temporal smoothing at each pixel loca-
tion. In the predictable region, there is motion, but it is motion that can be tracked
well by our motion estimator. Here, we can smooth along the found motion trajectory
with conﬁdence. Finally, in the unpredictable region, we ﬁnd that our motion estimate
is unreliable and so fall back on the spatial RUKF there. This multiple model version
(MM MC-RUKF) results in a very high temporal coherence in the still region, high
temporal coherence in the predictable region, and no motion blurring in the unpre-
dictable region. The segmentation is based on local variance of the displaced frame
difference (DFD).
As mentioned earlier, we employ a block-matching method for motion estimation.
Even when there is no correspondence between two motion-compensated frames,
the block-matching method chooses a pixel in the search area that minimizes the
displaced frame difference measure. However, the estimate will probably not have
much to do with the real motion, and this can lead to low temporal correlation in
the unpredictable region. This is the so-called noisy motion vectors problem. We can
compensate for this, in the case of still regions, by detecting them with an extra step,

438
CHAPTER 11 Digital Video Processing
based on the frame difference. We ﬁlter the frame difference and use a simple 7 × 7
box ﬁlter to reduce the effect of the observation noise. Also, a 3 × 3 box ﬁlter is
used on the MC output to detect the predictable region. The outputs are then fed
into local variance detectors. We found that when a pixel in a still region was miss-
detected as in the predictable region, a visual error in the ﬁltered image sequence
was noticeable, while in the opposite case, the error was not noticeable. Hence, we
detect the still region again in the ﬁltering step. Three spatiotemporal AR models
are obtained from the residual video of the original sequence for our simulation. For
more details, see [1].
Example 11.3–1: MM MC-RUKF Experimental Result
We used the CIF video salesman, which is monochrome and of size 360 × 280 pixels at
15 fps. We then added white Gaussian noise to achieve a 10-dB SNR. The processing
parameters of the MC-RUKF were as follows: image model order 1 × 1 × 1, update
region 2 × 2 × 1, and ﬁnal MC block sizes of both 9 × 9 and 5 × 5. The 3-D AR
model obtained from the original (modiﬁed) video was used. This model could also have
been obtained from the noisy video or from a prototype noise-free, with some loss of
performance. (Based on existing work in the identiﬁcation of 2-D image models, it is our
feeling that the additional loss would not be great.) We used a steady-state gain array,
calculated off-line on a small ﬁctitious image sequence. The SNR improvement achieved
was 6–8 dB with the 3-D RUKF alone, with an additional MC-RUKF improvement of about
1 dB. Using the multimodel feature, a further MM MC-RUKF improvement of about 1 dB
was achieved, totaling to an 8- to 10-dB improvement or an output SNR of 18–20 dB.
FIGURE 11.3–4
A frame from the MC-RUKF.

11.3 Motion-Compensated Filtering
439
The restored video in Figures 11.3–4 and 11.3–5 showed motion artifacts visible in some
motion areas but was generally quite visually pleasing.
The resulting SNR improvement curves are given in Figure 11.3–6. We notice that
the MM MC-RUKF provides the best objective performance by this MSE-based measure.
We can see an initial start-up transient of about 10 frames. We notice also how the up
FIGURE 11.3–5
A frame from the MM MC-RUKF.
10
9
8
7
6
5
dB
4
3
2
1
00
10
20
30
40
Frame number
MC 3-D RUKF with three models
MC 3-D RUKF with one model
3-D RUKF with one model
50
60
70
80
90
Solid line:
Dashed line:
Dashdot line:
FIGURE 11.3–6
Plot of SNR improvements versus frame number for the MM MC-RUKF (3 models),
MC-RUKF, and 3-D RUKF on the noisy salesman clip.

440
CHAPTER 11 Digital Video Processing
to 10-dB improvement varies across the frame number; this is caused by the motion of
objects and moving shadows in the scene. Videos are available for download at the book’s
Web site.
Frame-Rate Conversion
Frame-rate conversion is needed today due to the coexistence of multiple standard
frame rates (60, 30, 25, and 24 fps), and also leads to a convenient separation of
acquisition format, transmission standard, and viewing or display format. Frame-rate
up-conversion is also often used to double (or quadruple) the frame rate for display
(e.g., 25 fps to 50 or 100 fps). There are various methods for increasing the frame rate,
the simplest being frame repeat (i.e., sample and hold-in time). Somewhat more com-
plicated is making use of a straight temporal average, without motion compensation.
The ﬁltering is a 1-D interpolation (i.e., linear ﬁltering) in the temporal direction,
done for each pixel separately. A potentially more accurate method for frame-rate
increase is MC-based frame interpolation, ﬁrst suggested in [8].
Example 11.3–2: Frame-rate Up Conversion
We have applied the method of [8] to the test clip Miss America, which is color and
SIF sized6, with 150 frames at 30 fps. To perform our simulation, we ﬁrst decimated it
down to 5 fps and used only this low frame rate as input. Then we used the following
strategies to interpolate the missing frames (i.e., raise the frame rate back up to 30 fps):
frame replication, linear averaging, and motion-compensated interpolation. As a motion
estimation method, we employed HBM, with the result smoothed by a simple lowpass
ﬁlter.
Figure 11.3–7 shows a frame from temporal up-conversion using a simple linear aver-
aging ﬁlter. Note that during this time period, there was motion that has caused a double
image effect on the up-converted frame. Figure 11.3–8 shows the result of using the
motion-compensated up-conversion at a frame number near to that of the linear result in
Figure 11.3–7. We do not see any double image, and the up-conversion result is generally
artifact-free.
In this case our translational motion model worked very well, in part because of the
rather simple motion displayed in this Miss America test clip. However, it does not always
work this well, and MC up-conversion remains a challenging problem. Videos are available
for download at the book’s Web site.
Deinterlacing
As mentioned in Example 2.2–5 of Chapter 2, deinterlacing is used to convert from
a conventional interlaced format to one that is progressive or noninterlaced. In so
6Please see appendix on video formats.

11.3 Motion-Compensated Filtering
441
FIGURE 11.3–7
A frame from a linearly interpolated temporal up-conversion of the Miss America clip from
5 to 30 fps.
FIGURE 11.3–8
A frame from the motion-compensated temporal up-conversion of the Miss America clip
from 5 to 30 fps.
doing, the deinterlacer must estimate the missing data—i.e., the odd lines in the so-
called even frames and the even lines in the odd frames. A conventional deinterlacer
uses a diamond v–t multidimensional ﬁlter in upsampling the data to progressive
format. If the interlaced video had been preﬁltered prior to its original sampling on

442
CHAPTER 11 Digital Video Processing
this lattice to avoid spatial frequency aliasing, then using an ideal ﬁlter, the progres-
sive reconstruction can be exact, but still with the original spatiotemporal frequency
response. If proper preﬁltering had not been done at the original interlaced sampling,
then aliasing error may be present in both the interlaced and progressive data. In this
case, a nonideal frequency response for the conversion ﬁlter can help to suppress
the alias energy that usually occurs at locations of high spatiotemporal frequency.
While interlaced video is not as ubiquitous as it once was, the ATSC broadcast inter-
laced standard 1080i is common and needs conversion to 1080p for a progressive
display.
Example 11.3–3: Conventional Deinterlacer
This example uses a 9 × 9 diamond-shaped support in the v × t plane. The ﬁlter coef-
ﬁcients, shown in Table 11.3–1, were obtained via window-based FIR ﬁlter design. The
frequency response of this ﬁlter is shown in Figure 11.3–9, where we can see a broader
response along both temporal and vertical frequency axes than along diagonals, hence
approximating a diamond pattern in the v × t (n2 × n) frequency domain.
Figure 11.3–10 shows one ﬁeld from the interlaced salesman sample, obtained by ﬁl-
tering and downsampling from the corresponding progressive clip. It serves as the starting
point for our deinterlacing experiments. Figure 11.3–11 shows a frame from the result-
ing progressive (noninterlaced) output. We note that the result is generally pleasing, if
somewhat soft (slightly blurred). From the frequency response in Figure 11.3–9, we can
see that the image frame sharpness should be generally preserved for low temporal fre-
quencies (i.e., slowly moving or stationary objects). Fast-moving objects, corresponding
to diagonal support on the v × t ﬁlter frequency response function, will be blurred. Videos
are available for download at the book’s Web site.
While blurring of fast-moving objects is generally consistent with the limitations
of the human visual system response function, coherent motion can be tracked by
the viewer. As such, it appears as low temporal frequency on the tracking viewer’s
retina, and hence the blurring of medium- to fast-moving objects can be detected for
so-called trackable motion.
Table 11.3–1 Diamond Filter Coefﬁcients
0
0
0
0
0.001247
0
0
0
0
0
0
0
0.004988 −0.005339
0.004988
0
0
0
0
0
0.007481 −0.016016 −0.013060 −0.016016
0.007481
0
0
0
0.004988 −0.016016 −0.036095
0.162371 −0.036095 −0.016016
0.004988 0
0.001247 −0.005339 −0.013060
0.162371
0.621808
0.162371 −0.013060 −0.005339 0.001247
0
0.004988 −0.016016 −0.036095
0.162371 −0.036095 −0.016016
0.004988 0
0
0
0.007481 −0.016016 −0.013060 −0.016016
0.007481
0
0
0
0
0
0.004988 −0.005339
0.004988
0
0
0
0
0
0
0
0.001247
0
0
0
0

11.3 Motion-Compensated Filtering
443
1.2
1
0.8
0.6
0.4
0.2
0
10
8
6
4
2
0
0
2
4
6
8
10
FIGURE 11.3–9
Sketch of diamond ﬁlter response in the vertical–temporal frequency domain.
FIGURE 11.3–10
One ﬁeld from the interlaced version of the salesman clip.
Example 11.3–4: Median Deinterlacer
An alternative to the use of the classic multdimensional ﬁlter is the vertical–temporal
median ﬁlter. The most common method uses a three-pixel median ﬁlter, with one pixel

444
CHAPTER 11 Digital Video Processing
FIGURE 11.3–11
A progressive frame from the diamond ﬁlter (v × t) output for an interlaced input.
x
x
x
x
x
x
x
x
n
n −1
n −2
Time
Vertical 
A
B
C
D
FIGURE 11.3–12
Illustration of pixels input (B, C, and D) to the median ﬁlter deinterlacer.
above and below the current pixel, and one pixel right behind it in the previous ﬁeld. On
the progressive lattice, this median operation can be written as follows:
for odd frames,
bx(n1,2n2,n) = median{x(n1,2n2 + 1,n),x(n1,2n2 −1,n),x(n1,2n2,n −1)},
for even frames,
bx(n1,2n2 + 1,n) = median{x(n1,2(n2 + 1),n),x(n1,2n2,n),x(n1,2n2 + 1,n −1)}.
In Figure 11.3–12, circles indicate pixels (lines) present in a ﬁeld, while ×’s rep-
resent missing pixels (lines). We see three input pixels (B, C, and D) and one output

11.3 Motion-Compensated Filtering
445
FIGURE 11.3–13
A deinterlaced frame of the salesman clip by the adaptive median ﬁlter.
pixel (A), which represent a missing pixel at ﬁeld n. The statistical median of the three
input values (B, C, and D) will tend to favor D if there is no motion, but to favor B or
C in the case of motion. Thus this simple median deinterlacer switches back and forth
between temporal and spatial (vertical) interpolation to ﬁll in the missing pixel lines in
the even and odd ﬁelds. A vertical–temporal median deinterlaced frame is shown in
Figure 11.3–13. While the result is sharper than that of the multidimensional ﬁlter, this
sharpness is obtained at the cost of small artifacts occurring on fast-moving objects.
Videos are available for download at the book’s Web site.
A more powerful alternative is motion-compensated deinterlacing. It uses motion
estimation to ﬁnd the best pixels in the previous ﬁeld or ﬁelds for the prediction of
the current pixel in the missing lines of the current ﬁeld.
Example 11.3–5: Motion-Compensated Deinterlacer
In this example we try to detect and track motion [18] and then use it to perform the
deinterlacing. Using an HBM motion estimator based on a QMF SWT, we ﬁrst determine
if the velocity is zero or not, based on looking at a local average of the mean-square frame
difference and comparing it to a threshold. A simple three-tap vertical interpolation ﬁlter
was used to deinterlace at this ﬁrst stage. The motion is then said to be “trackable” if the
local motion-compensated MSE is below a second threshold. The algorithm then proceeds
as follows:
•
When no motion is detected, we smooth in the temporal direction only (i.e., use the
pixel at the same position in the previous ﬁeld).

446
CHAPTER 11 Digital Video Processing
•
When motion is detected, and with reference to Figure 11.3–14, we project the motion
path onto the previous two ﬁelds, with a cone of uncertainty opening to 0.1–0.2 pixels
at the just prior ﬁeld. If the cone includes a pixel in the ﬁrst prior ﬁeld, then that pixel
is copied to the missing pixel location A in the current ﬁeld. Otherwise, we look to the
second prior ﬁeld. If no such previous pixels exist in the “cone regions,” we perform
linear spatiotemporal interpolation.
The result in Figure 11.3–15 is potentially the best of these examples shown, albeit the
most computationally demanding due to the motion estimation. It is sharp and clear, but
unfortunately suffers from some motion artifacts, which could at least be partially ame-
liorated by more sophisticated motion estimation and compensation methods. The frame
rate of the salesman clip was 15 fps.
Time
Vertical
n
n - 1
n- 2
A
x
FIGURE 11.3–14
An illustration of the cone approach to motion-compensated deinterlacing.
FIGURE 11.3–15
An MC deinterlaced frame from the salesman clip.

11.4 Bayesian Method for Estimating Motion
447
One last comment on deinterlacing: Of course, missing data not sampled cannot
be recovered without some assumptions on the original continuous-parameter data. In
the speciﬁc case of deinterlacing, and in the worst case, one can imagine a small fea-
ture one pixel high, moving at certain critical velocities, such that it is either always
or never present on the interlaced grid. In real life, the velocity would not exactly
match a critical velocity and so the feature would appear and disappear at a nonzero
temporal frequency. If this feature is the edge of a rectangle or part of a straight line,
the ﬂickering can be noticed by the eye, but may be very hard to correct. Therefore,
an issue in MC deinterlacers is the consistency of the estimate. A recursive block
estimate showing a high degree of visual consistency is given in de Haan et al. [19].
11.4 BAYESIAN METHOD FOR ESTIMATING MOTION
In Chapter 8 we introduced Bayesian methods for image estimation and restoration,
which use a Gibbs-Markov signal model together with some, most often iterative,
solution method, such as simulated annealing (SA). Other methods used include
deterministic iterative methods such as iterated conditional mode (ICM) and mean
ﬁeld annealing (MFA). ICM is a method that sweeps through the sites (pixels) and
maximizes the conditional probability of each pixel in sequence. It is fast but tends
to get stuck at local optima of the joint conditional probability, rather than ﬁnd the
global maxima. MFA is an annealing technique that assumes that the effect of a neigh-
boring clique’s potential function can be well modeled with its mean value. While
this changes the detailed global energy function, the iteration proceeds much more
quickly and reportedly provides very nice results, generally classiﬁed as being some-
where between ICM and SA. A general review of these approaches is contained in
the review article by Stiller and Konrad [20].
To extend the Gibbs-Markov model, we need to model the displacement vector
or motion ﬁeld d, which can be done as
fd(D) = K exp[−Ud(D)],
where the matrix D contains the values of the vector ﬁeld d on the image region or
frame. The energy function Ud(D) is the sum of potential function over all the pixels
(sites) n and their corresponding cliques,
Ud(D) ≜
X
n
X
cn∈C
Vcn(D),
where Cn denotes the clique system for the displacement ﬁeld d, over frame n.
A common setup calls for the estimation of the displacement between two frames,
Xn and Xn−1, and using the MAP approach, we seek the estimate
bD = argmax
D f(D|Xn,Xn−1).
(11.4–1)
This simple model can be combined with a line ﬁeld on an interpixel grid, as in
Chapter 8, to allow for smooth estimates of displacement that respect the sharp

448
CHAPTER 11 Digital Video Processing
boundaries of apparent motion that occur at object boundaries. The assumption is
that moving objects generally have the same velocity, while different objects, and the
background, are free to move at much different velocities. An early contribution to
such estimates is the work of Konrad and Dubois [21].
Now as was the case in Chapter 8, there are so many variables in (11.4–1) that
simultaneous joint maximization is out of the question. On the other hand, iterative
schemes that maximize the objective function one site at a time are practical. The nice
thing about Gibbs models is that it is easy to ﬁnd the local or marginal model. One
simply gathers all the potential functions that involve that site. All the other terms
go into the normalizing constant. Thus, in the ICM approach, we seek the peak of
the resulting conditional pdf. In the SA technique, we take a sample from this pdf,
proceed through all the sites, and then reduce the temperature a small amount. In
MFA, we ﬁnd the conditional mean of the conditional pdf, and iterate through all the
sites in this manner.
Example 11.4–1: Motion Estimation for MC Prediction
This example comes from the review article [20] and shows both the predicted frame and
the prediction error frame for three types of motion estimate: block-based, pixel-based,
and region-based, the latter being based on segmenting the moving areas in the scene in
some way (e.g., using line ﬁelds). The data come from the monochrome videophone test
clip carphone in QCIF7 resolution. Figure 11.4–1 shows the result of block-based motion,
using fairly large 16 × 16 blocks, with the block structure being clearly evident, espe-
cially around the mouth. Its prediction MSE is reported as 31.8 dB. The predicted
frame in Figure 11.4–2, resulting from a dense motion estimate, is much better visu-
ally, with the much better prediction MSE of 35.9 dB. The region-based estimate, shown
FIGURE 11.4–1
Predicted frame of the carphone clip via block-based motion estimate. ( c⃝1999 IEEE)
7See appendix on video formats at the end of this chapter.

11.4 Bayesian Method for Estimating Motion
449
FIGURE 11.4–2
Predicted frame via dense motion estimate. ( c⃝1999 IEEE)
FIGURE 11.4–3
Predicted frame via region-based estimate. ( c⃝1999 IEEE)
FIGURE 11.4–4
Prediction error frame for block-based motion. ( c⃝1999 IEEE)

450
CHAPTER 11 Digital Video Processing
FIGURE 11.4–5
Prediction error frame for dense motion. ( c⃝1999 IEEE)
FIGURE 11.4–6
Prediction error frame for region-based motion. ( c⃝1999 IEEE)
in Figure 11.4–3, is almost as good as the dense one but has many fewer motion vectors.
Its prediction MSE is reported at 35.4 dB.
The corresponding prediction error frames are shown in Figures 11.4–4, 11.4–5, and
11.4–6. Regarding the prediction error frames for the dense and region-based motion, we
can see some blurring of object edges caused by the dense motion estimate, yet the object
boundaries are respected better in the region-based estimate. Details on this example are
presented in [22].
Joint Motion Estimation and Segmentation
An alternative to motion estimation with a line ﬁeld to prevent oversmoothing at
object edges is to jointly estimate an object segmentation along with the motion [23].

11.4 Bayesian Method for Estimating Motion
451
The object boundary then serves to provide a linear feature over which motion should
not be smoothed.
Example 11.4–2: A Joint Segment and Displacement Potential
Stiller and Konrad [20] provide the following example of a joint segmentation S and dis-
placement D potential function. Here, the segment label s(n) can take on a ﬁxed number
of label values 1,...,L corresponding to an assumed number of objects in the video
frames. The potential for pairwise cliques was suggested as
Vn1,n2(d(n1),d(n2),s(n1),s(n2)) = λd ∥d(n1) −d(n2)∥2 δ(s(n1) −s(n2))
+ λl(1 −δ(s(n1) −s(n2))).
(11.4–2)
Here, λd and λl are weights and δ is the discrete-time impulse function. We see that if the
labels are the same, the ﬁrst term penalizes the potential if the motion vectors are different
at the two neighbor pixels (sites) n1 and n2. The second term in the potential penalizes
the case where the labels are different at these two neighbor sites. The overall potential
function then provides a compromise between these two effects.
One fairly complete formulation of the joint motion-segmentation problem is in
the thesis of Han [24], where he applied the resulting estimates to video frame-rate
increase and low bitrate video coding. His energy function followed the approach of
Stiller [23] and was given as
 bDn,bSn

=arg max
Dn,Sn
f(Dn,Sn|Xn,Xn−1)
=arg max
Dn,Sn
f(Xn−1|Dn,Sn,Xn)f(Dn|Sn,Xn)f(Sn|Xn),
(11.4–3)
where the three factors are the pdf’s or pmf’s of the video likelihood model, motion
ﬁeld Dn prior model, and segmentation ﬁeld Sn prior model, respectively.
The likelihood model was Gibbsian with energy function,
U(Xn−1|Dn,Xn) =
1
2σ 2
X
n
(x(n,n) −x(n −d(n,n),n −1))2,
which is simpliﬁed to not depend on the segmentation.
The motion ﬁeld prior model is also Gibbsian and given in terms of energy
function,
Ud(Dn|Sn) = λ1
X
n
X
m∈Nn
∥d(n,n) −d(m,n)∥2 δ(s(n,n) −s(m,n))
+ λ2
X
n
∥d(n,n) −d(n −d(n,n),n −1)∥2
−λ3
X
n
δ(s(n,n) −s(n −d(n,n),n −1)).

452
CHAPTER 11 Digital Video Processing
Here, he made the assumption that given the segmentation of the current frame, the
motion ﬁeld does not depend on the frame data. The ﬁrst term in Ud sums over the
local neighborhoods of each pixel and raises the motion ﬁeld prior energy function
if, and only if, the labels agree and the l2 norm of the motions are different. The
second term increases this energy if the motion vector has changed along the motion
path to the previous frame. The third term lowers the energy function if the labels
agree along the motion paths. Of course, careful selection of the lambda parameters
is necessary to achieve the best trade-off among these various factors, which try to
enforce spatial smoothness of motion along with temporal smoothness of motion and
segmentation along the motion path.
The segmentation ﬁeld prior model is given in terms of the energy function
Us(Sn|Xn) =
X
n
X
m∈Nn
Vm(s(n,n),s(m,n)|Xn),
where
Vm(s(n),s(m)|Xn) =



−γ ,
s(n) = s(m) and l(n) = l(m),
0,
s(n) = s(m) and l(n) ̸= l(m),
+γ ,
s(n) ̸= s(m) and l(n) = l(m),
0,
s(n) ̸= s(m) and l(n) ̸= l(m),
where l(n) is a (deterministic) label ﬁeld determined from Xn alone via a stan-
dard region-growing algorithm [24]. The overall intent is to encourage smooth
(a)
(b)
(c)
(d)
FIGURE 11.4–7
(a) Original, (b) joint segmentation, (c) block-based motion, (d) joint motion.

11.5 Restoration of Degraded Video and Film
453
segmentation labels that agree with the 2-D label ﬁeld determined just by color
(gray level), penalize changes in segmentation ﬁeld that do not agree with changes
in color, and treat neutrally the remaining discrepancies. This is because the motion
segmentation can change when an object starts or stops moving, and usually it should
correspond to a color segmentation.
It is common to try to iteratively maximize (11.4–3) via an alternating method,
where one sweep of all the sites tries to improve the criterion by updating bDn given
the current bSn, while the next or alternate sweep updatesbSn given the current bDn.
The procedure starts with a segmentation determined by region growing on the ﬁrst
color frame. Figure 11.4–7 shows results of the alternating motion-segmentation opti-
mization from [24]. We notice that the determined motion in Figure 11.4–7(d) is very
smooth within the objects found in Figure 11.4–7(b), unlike the generally more noisy
motion vectors in Figure 11.4–7(c) determined by block matching.
In the next chapter, we will show how joint motion and segmentation estimation
can be used in an object-based low-bitrate video coder. This application can use the
preceding formulation that assumes clean data are available.
11.5 RESTORATION OF DEGRADED VIDEO AND FILM
Another class of applications uses Bayesian methods to restore degraded image
sequence data such as for restoring old movie ﬁlm and video tape. Various kinds
of problems can be present in addition to noise and blurring, including blotches or
areas in each frame of missing data due to dirt, scratches, chemical deterioration,
video tape dropout, etc. The observation model is given in terms of a binary masking
function b(n,n) that indicates the presence of a blotch (b = 1) and a replacement
noise model v(n,n) that models the statistics of the blotch, as well as the additive
random noise w(n,n). The noisy and distorted observations y(n,n) are then written as
y(n,n) = (1 −b(n,n))x(n,n) + b(n,n)v(n,n) + w(n,n),
(11.5–1)
where n = (n1,n2). A simple approach to detect blotches ﬁrst performs a robust
and dense motion estimation, and then applies a spike detector to the energy in the
motion-compensated residual. A spike-detector index (SDI) can then be given as
SDI(n,n) =min{(y(n,n) −y(n −d[n;n,n −1],n −1))2,
(y(n,n) −y(n −d[n;n,n + 1],n + 1))2},
where d[n;n,n −1] and d[n;n,n + 1] represent the forward and backward motion-
estimated displacements, respectively. This index is then compared to a threshold to
detect a blotch as
b(n,n) =
0,
SDI(n,n) ≤T,
1,
otherwise,
where T(> 0) is some set threshold. More on this topic appears in Chapter 4 of The
Essential Guide to Video [15]. The detected blotch location together with the motion

454
CHAPTER 11 Digital Video Processing
Motion
estimation
Blotch
detection
Frame
restoration
y(n,n)
x(n,n)
^
FIGURE 11.5–1
An illustration of video restoration in the presence of blotch-type artifacts.
information is fed to an estimation stage where both are jointly applied in restoring
the current frame to obtain the estimatebx(n,n), as shown in Figure 11.5–1.
While this simple approach often works, it will face problems when motion fails,
in part because the blotch is not explicitly considered when the motion is being
estimated. Also, there is still the problem of estimating the clean image pixel x(n,n),
and explicit consideration of occlusions is omitted.
A Bayesian Approach
Kokaram has presented a comprehensive Bayesian approach to this joint detec-
tion/estimation problem [25] based on earlier work [26]. His model also uses (11.5–1)
and includes motion and occlusion models used for predicting x(n,n) from the near-
est neighbor frames y(n,n −1) and y(n,n + 1). Two other binary functions ob(n,n)
and of (n,n) model occlusion in backward and forward directions, respectively, with
oi = 1 indicating occlusion. A pixel state s(n,n) is then given by s ≜[b,ob,of ]. The
variables to be estimated at each pixel are thus
θ(n,n) ≜[x(n,n),s(n,n),v(n,n)].
It has been observed that blotch corruption does not usually occur at the same location
in consecutive frames; thus it is assumed that no blotch is present in frames n −1
or n + 1 at the same location. The Bayesian a posteriori probability then becomes
equivalent to the product of a likelihood term and a prior term, given as
p[θ(n,n)|x(n,n −1),y(n,n),x(n,n + 1)] ∝p[y(n,n)|θ(n,n),x(n,n −1),x(n,n + 1)]
× p[θ(n,n)|x(n,n −1),x(n,n + 1)].
(11.5–2)
Details on factoring and expanding this equation in terms of corruption likelihood
and speciﬁc priors are given in [25], where an ICM rather than MAP solution method
is used to limit computation. The motion is estimated initially using block match-
ing, and initial blotch detection is made by a deterministic scheme. An iterative
solution scheme then alternates between (1) an updated estimate of the current state
s(n,n) and image frame x(n,n), given the current estimate of motion (displacement)
d(n,n) and observation noise variance σ 2
w(n), and (2) a blockwise scan of the current

11.6 Super-Resolution of Video
455
image frame n that updates the estimate of the displacement d(n,n), given the cur-
rent estimate for state s and image x. Good experimental performance for this joint
detection/estimation approach is reported.
11.6 SUPER-RESOLUTION OF VIDEO
Restoration from a small number of images by super-resolution (SR) methods was
presented in Chapter 8, but the goal was to produce just one high-resolution (HR)
image from the low-resolution (LR) input frames. Patti et al. [27] presented a quite
general SR algorithm for conversion of SD to HD video based on a projections-onto-
convex-sets formulation (see Section 5.1, Chapter 5) [28]. A dynamic or recursive
formulation of SR for video was presented in Farsiu et al. [29] based on an earlier
formulation of Elad and Feuer [30].
One way to handle the video case for SR would be to repeat the image SR solution
for each frame in the video, but clearly this is not efﬁcient or even practicable because
of the implied growing memory of such an estimate. So, a recursive solution was
sought. In [29], taking the monochrome case, they ﬁrst set up the following dynamic
equations:
Xn = Fn|n−1Xn−1 + Un,
(11.6–1)
Yn = DHXn + Wn,
(11.6–2)
for the HR dynamic system (11.6–1) and LR observations (11.6–2), where Fn|n−1
represents motion between frames (assumed known), D is a decimation operator,
H is a blur operator, and Un and Wn are independent model and observation noise
frames, respectively. We again use the 2-D vector / 4-D matrix notation introduced
in problem 20 of Chapter 4. Note that this spatiotemporal model is in the tem-
porally causal sense (see Section 10.4 in Chapter 10) since the prediction step in
(11.6–1) only involves the prior frame, frame Xn−1. A temporally causal Kalman
ﬁlter could be deﬁned here with a global state equal in size to a full image frame,
analogously to the totally ordered temporally causal Kalman ﬁlter developed in
Section 10.4.
Before presenting their estimator, however, they perform a separation of the SR
problem into two parts: a data-fusing part and a restoration part, similar to the case
in Section 8.8 of Chapter 8. They point out that, upon deﬁnition of the blurred video
Zn ≜HXn, the preceding equations can be written in terms of the 2-D state vector
Zn on the HR grid, as
Zn = Fn|n−1Zn−1 + Vn,
(11.6–3)
Yn = DZn + Wn,
(11.6–4)
where Vn = HUn, thus permitting the problem to be decomposed into a data-fusing
step giving Zn and then the conventional ﬁltering equation
Zn = HXn.

456
CHAPTER 11 Digital Video Processing
From this decomposition, they decide to ﬁrst solve the estimation problem (11.6–3
and 11.6–4) to get the estimate bZn, and then to solve a restoration problem to ﬁnd bXn,
the desired estimate of the HR video Xn.
The estimator for the ﬁrst problem (11.6–3 and 11.6–4) is then chosen as a single
update version of the temporally causal Kalman ﬁlter and is of the form
bZn = Fn|n−1bZn−1 + Kn
 Yn −DFn|n−1bZn−1

,
where the update or gain operator Kn is diagonal and given in [29] in terms of a recur-
sion based on an assumed diagonal form for cov(Vn). We note that the concatenated
operator DFn|n−1 is just a decimate and shift operator and hence relatively easy to
implement. Since the Kalman gain operator (4-D matrix) is diagonal, there is only
one update per pixel in their procedure. But notice the key role of the motion com-
pensation or shifting operator Fn|n−1 in creating this estimate. It effectively decides
what LR pixels will be included according to whether the composite shifts move into
the ﬁeld of view of Zn.
The second step is to estimate the HR video Xn from the estimate bZn, and this is
done using a robust deblurring method in [29] as
min
Xn
n
||HXn −bZn||2
2 + λ0(Xn)
o
,
(11.6–5)
where the constraint term 0(Xn) is given in terms of a locally weighted L1 norm of a
spatial regularization term
0(Xn) =
X
max(|m1|,|m2|)≤p
α|m1|+|m2|||Xn −Sm1
1 Sm2
2 Xn||1,
(11.6–6)
and where S1 and S2 are horizontal and vertical shift operators, respectively. The
parameters λ > 0,0 < α < 1 and small positive number p are experimentally chosen
to optimize a steepest descent solution for (11.6–5).
For the this algorithm to be of practical importance, there must be available an
efﬁcient subpixel accurate motion estimation procedure to supply the assumed known
displacement F operator on the LR frames. Since the algorithm has no knowledge
of (does not account for) motion errors, the motion estimate will have to be highly
accurate as well. Since the LR frames are signiﬁcantly aliased, in order to be able
to expect signiﬁcant SR performance improvements, the required motion estimation
may be a challenge. As mentioned in Section 8.8 of Chapter 8, motion estimation
methods speciﬁcally adapted to the presence of aliasing may be needed [31].
Demosaicing
One place where there can be signiﬁcant alias energy is in a digital image captured
with a Bayer color ﬁlter array (CFA) (see Section 6.6, Chapter 6). The processing
needed to restore the missing pixels is called demosaicing, and there have been many
attempts to demosaic still images [32]. Such processing has not always succeeded
due to the ill-conditioned and under-determined nature of this problem. However,
processing a sequence of such images, with known displacements, while keeping the

11.6 Super-Resolution of Video
457
basic ﬁeld of view of the camera intact, can much improve the situation by providing
the missing samples as well as reducing the observation noise, by averaging over the
frames along the motion trajectories.
The basic fusion step of estimating the frames Zn can remain as before, with a
separate estimate for each color component R, G, and B. Then in the restoration step
of estimating Xn from bZn, a weighted L2 norm can be formulated as

AR

HX(R)
n
−bZ(R)
n


2
2 +

AG

HX(G)
n
−bZ(G)
n


2
2 +

AB

HX(B)
n
−bZ(B)
n


2
2 ,
where the weights are diagonal operators (diagonal 4-D matrices) that have zero
weights for those pixels with no samples [29]. This error term was regularized with
the addition of three penalty terms: an L1 penalty term on the luminance, follow-
ing (11.6–6), an L2 penalty term on the Laplacian of two chrominance terms, and an
innovative orientation penalty term [29] that tends to orient edges similarly across
the three color channels. Steepest descent is recommended for the solution.
Example 11.6–1: SR of Mosaiced Video
For a color camera that uses a Bayer array, there is a loss or resolution due to the CFA.
The resulting subsampling operator D can be illustrated as in Figure 11.6–1 from [29],
which shows seven LR mosaiced color image frames and their hypothetical position on a
super-resolved HR image.
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
...
1
4
3
1
4
3
1
4
3
?
6
5
?
6
5
?
6
5
7
2
?
7
2
?
7
2
1
4
3
1
4
3
1
4
3
?
6
5
?
6
5
?
6
5
7
2
?
7
2
?
7
2
?
1
4
3
1
4
3
1
4
3
?
6
5
?
6
5
?
6
5
7
2
?
7
2
7
2
?
?
?
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
FIGURE 11.6–1
An illustration of color ﬁlter subsampling effect. (Figure 11.1–3 from [29] with permission).

458
CHAPTER 11 Digital Video Processing
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
FIGURE 11.6–2
Some results of the Farsiu et al. study. (Figure 11.1–5 [29]).
A simulation study was conducted in [29] involving a 128 × 128 section of a large
color image that was blurred, shifted one pixel at a time up, down, or right, and then
subsampled 4 × 4, effectively taking a random walk through the large original image. A

Problems
459
Bayer color ﬁlter was then applied to each LR frame, followed by the addition of white
Gaussian noise to achieve an SNR = 30 dB. A total of 250 LR frames were created in this
way. The images in Figure 11.6–2 show the results, with parts (a) and (b) being the original
images of frames 50 and 250, respectively. Parts (c) and (d) are the corresponding LR
images, (e) and (f) are the recursively fused HR frames, and (g) and (h) are the restored
and demosaiced frames.
Note that the SR terminology can be applied to other video processing as well,
e.g., 480i and 1080i deinterlacing. These operations, by their nature, are SR; that
is, they come up with estimates for the missing samples by shifting nearby samples
guided by accurate motion estimation/compensation. A possible new application of
SR is to the subsampled HD video coming from current DSLR cameras. Possible
application to motion-compensated ﬁltering in the presence of subpixel accuracy
should also be possible.
Finally, we point out that recently the nonlocal means approach (see Section 8.7,
Chapter 8) has been extended to super-resolution [33] in a way that can alleviate the
need for explicit motion estimation in SR processing. Using nonlocal means, it is
possible to achieve super-resolution from a single image [34].
CONCLUSIONS
Multidimensional ﬁltering plays an important role in video format conversion.
Whenever an image or video is resized to ﬁt a given display, a properly designed
multidimensional ﬁlter will improve results over commonly employed decimation
and 1-D ﬁltering alone. Motion-compensated ﬁltering turns out to be very effective
on noisy or distorted video data, for the purpose of restoration. Motion-compensated
multidimensional ﬁltering also offers increased sharpness in video standards conver-
sion. However, unless the MC estimates are very good, artifacts can result, and this
is called motion model failure. This is where a simple translational or afﬁne image
model does not match the data that well. We then looked at Bayesian methods for
joint motion estimation and segmentation. We overviewed the application area of
restoring old ﬁlm and video. Finally, we presented super-resolution of video, which
makes use of accurate motion to increase the resolution of trackable moving objects
in the input video.
PROBLEMS
1. How many pixels per second must be processed in standard deﬁnition D1
video?8 How much storage space in gigabytes (GB) for 10 minutes of D1?
8A description of the D1 format is provided in the appendix of this chapter.

460
CHAPTER 11 Digital Video Processing
2. Use MATLAB to design a spatial lowpass ﬁlter and then optimize it visually
with the approach of Example 11.1–3. Can you see the difference? What value
of α did you determine? How does it relate to the transition bandwidth of the
original lowpass ﬁlter?
3. Compare the computation (multiplies and adds) of full-search versus three-step
search block matching, both using the MSE method. Assume an M × M block
and N × N search window, and that block-matching estimates are calculated on
an M × M decimated grid. Take the image size as CIF (i.e., 352 × 288).
4. In overlapping block motion compensation (OBMC), the weighting matrices
of [10] are used to smooth the motion vectors over the ﬁxed block boundaries.
Comment on how this would be expected to affect the computational complexity
of OBMC versus regular block matching.
5. In this problem you will show the connection between the spatiotemporal
constraint equation (11.2–4) and direct minimization of the error,
E(dx,dy) ≜
X
R(x,y)

f(x,y,t) −f(x −dx,y −dy,t −1t)
2 .
First, calculate ∂E/∂dx and ∂E/∂dy and set them both to zero. Then argue that
if at least one of the spatial gradients ∂f/∂x and ∂f/∂x are not zero, then for
sufﬁciently small region R(x,y) and time interval (t,t + 1t), the spatiotemporal
constraint equation must hold, with dx = vx1t and dy = vy1t. (Hint: expand the
function f(x,y,t) in a Taylor series and keep only the ﬁrst-order terms.)
6. Show in block-diagram form a system realizing the diamond ﬁlter deinterlac-
ing method of Example 11.3–3. Your diagram should explicitly include the
up-samplers and the ﬁlter. For ATSC 1080i format, approximately how many
multiplies per second will be required?
7. Extending Example 11.3–5 on motion-compensated deinterlacing, we can look
back over several ﬁeld pairs (frames) and ﬁnd multiple references for the missing
pixel. Then the estimate for the missing pixel can be a weighted average of the
references from the different frames. Suggest a good way of weighting these
multiple references? You may care to look at [35] for some ideas.
8. Can mean ﬁeld annealing (MFA) be used to estimate the segment labels in a joint
motion-segmentation approach to Bayesian motion estimation? How would an
iterated conditional mode (ICM) estimate be obtained?
9. Separate the three energy functions corresponding to the Gibbs pdf/pmf factors
in (11.4–3) into two partially overlapping sums: one with all the s(n,n) terms
and the other with all the d(n,n) terms.
10. According to the constant intensity assumption for motion, we have
x(n1,n2,n) = x(n1 −d1,n2 −d2,n −1),

Appendix: Digital Video Formats
461
where we assume that the displacement vector d has integer elements d1 and d2.
Thus, by induction, we can write the video (image sequence) signal x as
x(n1,n2,n) = x(n1 −nd1,n2 −nd2,0)
= g(n1 −nd1,n2 −nd2),
for the 2-D function g(n1,n2) ≜x(n1,n2,0) and for a sequence of images starting
with the ﬁrst frame at n = 0.
(a) Find and express the 3-D Fourier transform X(ω1,ω2,ω) in terms of the
2-D Fourier transform G(ω1,ω2) and the integer displacements d1 and d2.
Assume there is no aliasing.
(b) Assuming given positive integer values of displacements d1 and d2, what
isotropic limit on the spatial bandwidth of g is necessary to avoid temporal
frequency aliasing in x? Use |ω|max to denote the spatial bandwidth of g,
0 < |ω|max < π.
11. Show that triangular mesh matching with afﬁne models produces a continuous-
velocity ﬁeld across triangular patch edge boundaries, independent of the non-
shared control point velocities. For example, in Figure 11.P–1 with the indicated
control point velocities vi, both the upper and lower triangular patches share v1
and v4 but do not share v2 and v3. Note the (x,y) origin at the upper left.
(0,0)
x
y
v1
v2
v4
v3
FIGURE 11.P–1
One cell of a triangular mesh with indicated velocities vi,i = 1 −4, at the control points.
12. Obtain a copy of Kokaram’s 2004 paper [25] and complete the expansion of the
MAP expression (11.5–2).
APPENDIX: DIGITAL VIDEO FORMATS
This appendix presents some common video formats. We start with the common half-
resolution formats SIF and CIF and then discuss the standard-deﬁnition (SD) format
ITU 601. In passing, we consider a smaller quarter-size format called QCIF used

462
CHAPTER 11 Digital Video Processing
for low-bitrate video conferencing. We also list current high-deﬁnition (HD) formats
speciﬁed by the ATSC and the digital cinema formats of DCI.
SIF
The source interchange format (SIF) is the format of MPEG1. It has 352 × 240 pixels
at 30 fps, in a noninterlaced or progressive lattice. This is the U.S. version, more
formally called SIF-525. The European version, SIF-625, is 352 × 288. The MPEG
committee has given recommendations for the conversion of ITU 601 to SIF and for
the conversion of SIF to ITU 601 using certain suggested ﬁlters. These ﬁlters are
commonly called MPEG half-band ﬁlters.
Decimation ﬁlter:
−29
0
88
138
88
0
−29//2569
Interpolation ﬁlter:
−12
0
140
256
140
0
−12 //256
CIF
The common intermediate format (CIF) has 352×288-pixel, progressive image
frames, with a sometimes variable frame rate. CIF gets its dimensions from the larger
of these two (i.e., SIF-525 and SIF-625). Also common is quarter CIF (QCIF), the
format of H.263, a video telephone 176 × 144 standard that runs at typically 5–15
fps. Finally, going in the direction of higher resolution, there is 4CIF with resolution
704 × 576. Together, 4CIF, CIF, and QCIF make an embedded set of resolutions in
spatial resolution.
ITU 601 Digital SDTV (a.k.a. SMPTE D1 and D5)
This is the component digital standard of the D1 (3/4 in.) digital SD tape recorders.
The signal is sampled in 4:2:2 Y′,CR,CB format, and interlaced 2:1. The ﬁeld rate is
59.94 Hz (in the U.S.), with the frame rate 29.97 Hz (in the U.S.). The aspect ratio is
4:3, with the bit depth of 8 or 10 bits per pixel.
The intent was to digitize SD analog TV as seen in the United States and Europe.
This standard is meant for viewing distance of 5–6 times the picture height. There are
various members of the ITU 601 family, the most prominent being the so-called 4:2:2
member. The sampling frequency was standardized at 13.5 MHz for both 525 and 625
line systems (i.e., “4” means 13.5 MHz). This choice results in a static orthogonal
sampling pattern for both 525/60 and 625/50 systems, since 13.5 and 6.75 MHz are
integer multiples of 2.25 MHz, the least common multiple of the scan line frequen-
cies of these systems. There are 720 active luma samples/line and 486 active lines
(i.e., 720 × 486 pixels in the U.S. version at 60 ﬁelds/sec, and in Europe, 720 × 576
pixels at 50 ﬁelds/sec). The quantization is uniform PCM, again to either 8 or 10 bits.
9To get the actual coefﬁcients, divide the indicated integer value by 256.

Appendix: Digital Video Formats
463
The standard provides ﬁlter specs for both the sampling of luma and for the
subsampling of the chroma channels. The overall data rate was speciﬁed as 27 MHz.
From the digital point of view, there are 486 real or active lines, not the 525 lines
in the U.S. SD system. The 525 line ﬁgure includes so-called “invisible lines,” which
allowed for an analog CRT to “retrace” or return to both the beginning of a line
(horizontal retrace interval) and return to the top left corner for display of the next
ﬁeld or frame (vertical retrace).
The Society of Motion Picture and Television Engineers (SMPTE) recommended
the following color difference components or color space:


Y′
CR
CB

=


0.299
0.587
0.114
0.500
−0.418
−0.081
−0.169
−0.331
0.500




R′
709
G′
709
B′
709

,
where the R′
709, G′
709, and B′
709 components are gamma corrected as in Section 6.5
of Chapter 6. In the common 4:2:2 mode, the CR and CB samples are co-sited with
odd samples of Y′ on each line (i.e., 1st, 3rd, 5th, etc.). There is a highest qual-
ity 4:4:4 mode where there is no color subsampling. The 4:4:4 mode is generally
used in graphics and matting work, where an image is superposed on a different
background.
What is 4:2:0?
The 4:2:0 speciﬁcation means that the CR and CB chroma signals are subsampled
by two vertically as well. This gives a combined chroma sample rate of 50% of the
luma sample rate. Whenever there is subsampling of the color information, as in
4:2:2 and 4:2:0 rasters, there is a need to specify any offsets between the chroma
and luma rasters. Converting from one representation to another is best done using
multidimensional ﬁlters. This 4:2:0 color format is used in DV and HDV video tape
formats, as well as DVD video.
ATSC Broadcast Formats
The digital formats in the United States were set up by the Advanced Television Sys-
tem Committee (ATSC) in the early 1990s. They mainly consist of two HD formats
and two SD formats. The SD formats are 720 × 486 interlaced at 60 ﬁelds/sec and
720 × 486 progressive at 60 fps. The main HD formats are 1280 × 720 at 60 fps,
often referred to as 720p, and 1920 × 1080 interlaced at 60 ﬁelds/sec, often referred
to as 1080i. In recent years, a 1080p speciﬁcation has also emerged also at 60 fps,
with an uncompressed data rate of 3 Gbps, twice that of 1080i. Two bit depths are
supported for the color components, 8 and 10 bits per sample. The ATSC keeps a
Web site at http://www.atsc.org where further information on these formats can be
downloaded. The SMPTE D5 HD tape provides a high-quality intraframe recording
capability for these ATSC video standards.
DCI Formats
The DCI format (http://en.wikipedia.org/wiki/Digital Cinema Initiatives) was ﬁrst
issued in 2005 but has been updated since, with the latest version being 1.2

464
CHAPTER 11 Digital Video Processing
(http://www.dcimovies.com/DCIDigitalCinemaSystemSpecv1 2.pdf), which is the
de facto standard for digital cinema in the United States. It describes the so-called
digital cinema distribution master (DCDM) format, compression method, transport,
security, projection systems, etc.
The image format is speciﬁed as either 2K (2048 × 1024) at 24 or 48 fps or 4K
(4096 × 2160) at 24 fps. Within these formats, one can specify what pixels are to be
displayed, the active pixels. For example, a movie with a 2.39 aspect ratio can ﬁt in
the 4K format as 4096 × 1716 or in the 2K format as 2048 × 858. For this reason,
the 4K and 2K distribution formats are sometimes referred to as 4K and 2K contain-
ers. The color space is speciﬁed as X′Y′Z′ with a bit depth of 12 bits and no color
subsampling. Additionally, there is a hierarchical image structure that permits a 2K
projection system to decode a 2K version from the 4K ﬁles. The resolution-embedded
JPEG 2000 image coder has been adopted for intraframe compression of digital
cinema. Special proﬁles of JPEG 2000, Part 1 have been developed for digital cinema.
REFERENCES
[1] J. W. Woods and J. Kim, “Motion Compensated Spatiotemporal Kalman Filter,”
Chapter 12 in Motion Analysis and Image Sequence Processing, Eds. I. Sezan and
R. L. Lagendijk, Kluwer, Boston, MA, 1993.
[2] E. Dubois and W. Schreiber, “Improvements to NTSC by Multidimensional Filtering,”
vol. 97, SMPTE J., pp. 446–463, June 1988.
[3] H. Schr¨oder and H. Blume, One- and Multidimensional Signal Processing, John Wiley
and Sons, 2000.
[4] H. Schr¨oder, “On Vertical Filtering for Flicker Free Television Reproduction,” IEEE
Trans. Circuits, Sys., and Signal Process., vol. 3, pp. 161–176, 1984.
[5] J. R. Jain and A. K. Jain, “Displacement Estimation and Its Application in Interframe
Image Coding,” IEEE Trans. Comm., vol. COM-29, pp. 1799–1808, December 1981.
[6] H.-M. Hang and Y.-M. Chou, “Motion Estimation for Image Sequence Compression,”
Chapter 5 in Handbook of Visual Comm., H.-M. Hang and J. W. Woods, Eds., Academic
Press, New York, 1995.
[7] B. Liu and A. Zaccarin, “New Fast Algorithms for the Estimating of Block Motion
Vectors,” IEEE Trans. CSVT, vol. 3, pp. 148–157, April 1993.
[8] R. Thoma and M. Bierling, “Motion-Compensated Interpolation Considering Covered
and Uncovered Background,” Signal Process.: Image Comm., Elsevier, vol. 1, no. 2,
pp. 191–212, October 1989.
[9] S. Nogaki and M. Ohta, “An Overlapped Block Motion Compensation for High Qual-
ity Motion Picture Coding,” Proc. IEEE Int. Sympos. Circuts Systems, pp. 184–187,
May 1992.
[10] M. T. Orchard and G. J. Sullivan, “Overlapped Block Motion Compensation: An
Estimation-Theoretic Approach,” IEEE Trans. Image Process., vol. 3, pp. 693–699,
September 1994.
[11] A. N. Netravalli and J. D. Robbins, “Motion Compensated Coding: Some New Results,”
Bell Sys. Tech. J., vol. 59, no. 9, pp. 1735–1745, November 1980.

References
465
[12] M. Bierling and R. Thoma, “Motion Compensating Field Interpolation Using a Hierar-
chically Structured Displacement Estimator,” Signal Processing, vol. 11, pp. 387–404,
December 1986.
[13] B. K. P. Horn and B. G. Schunck, “Determining Optical Flow,” Artiﬁcial Intell., vol. 17,
pp. 185–203, August 1981.
[14] H. Farid and E. P. Simoncelli, “Differentiation of Discrete Multidimensional Signals,”
IEEE Trans. Image Process., vol. 13, pp. 496–508, April 2004.
[15] Chapters in The Essential Guide to Video, A. Bovik, Ed., Elsevier/Academic Press,
Burlington, MA, 2009.
[16] R. D. Forni and D. S. Taubman, “On the Beneﬁts of Leaf Merging in Quad-tree Motion
Models,” Proc. IEEE ICIP, vol. 2, pp. 858–861, September 2005.
[17] M. K. Ozkan, I. Sezan, A. T. Erdam, and A. M. Tekalp, “Motion Compensated Multi-
frame Wiener Restoration of Blurred and Noisy Image Sequences,” Proc. IEEE ICASSP,
vol. 3, pp. 293–296, San Francisco, CA, March 1992.
[18] J. W. Woods and S.-C. Han, “Hierarchical Motion Compensated De-interlacing,” Proc.
SPIE Visual Communications and Image Processing (VCIP) VI, vol. 1605, no. 805,
Boston, November 1991.
[19] G. de Haan, P. W. A. C. Biezen, H. Huijgen, and O. A. Ojo, “True-Motion Estimation
with 3-D Recursive Search Block Matching,” IEEE Trans. CSVT, vol. 3, pp. 368–379,
October 1993.
[20] C. Stiller and J. Konrad, “Estimating Motion in Image Sequences,” IEEE Signal Process.
Magazine, vol. 6, pp. 70–91, July 1999.
[21] J. Konrad and E. Dubois, “Estimation of Image Motion Fields: Bayesian Formulation
and Stochastic Solution,” Proc. IEEE ICASSP, vol. 2, pp. 1072–1075, April 1988.
[22] V.-B. Dang, A. R. Mansouri, and J. Konrad, “Motion Estimation for Regional-Based
Video Coding,” Proc. IEEE ICIP, vol. 2, pp. 198–192, Washington, DC, October 1995.
[23] C. Stiller, “Object Based Estimation of Dense Motion Fields,” IEEE Trans. Image
Process., vol. 6, pp. 234–250, February 1997.
[24] S.-C. Han, Object-Based Representation and Compression of Image Sequences, PhD
thesis, ECSE Department, Rensselaer Polytechnic Institute, Troy, NY, 1997.
[25] A. C. Kokaram, “On Missing Data Treatment for Degraded Video and Film Archives:
a Survey and a New Bayesian Approach,” IEEE Trans. Image Process., vol. 13,
pp. 397–415, March 2004.
[26] A. C. Kokaram and S. J. Godsill, “MCMC for Joint Noise Reduction and Missing Data
Treatment in Degraded Video,” IEEE Trans. Signal Process., vol. 50, no. 2, pp. 189–205,
February 2002.
[27] A. J. Patti, M. I. Sezan, and A. M. Tekalp, “Superresolution Video Reconstruction with
Arbitrary Lattices and Nonzero Aperture Time,” IEEE Trans. Image Process., vol. 6,
no. 8, pp. 1064–1076, August 1997.
[28] H. Stark and P. Oskoui, “High Resolution Image Recovery from Image Plane Arrays
Using Convex Projections,” J. Optical Soc. Amer. A, vol. 6, pp. 1715–1726, 1989.
[29] S. Farsiu, Mi. Elad, and P. Milanfar, “Video-to-Video Dynamic Super-Resolution for
Grayscale and Color Sequences,” EURASIP J. on Advances in Signal Processing,
Hindawi, vol. 2006, article ID 61859, pp. 1–15, 2006.
[30] M. Elad and A. Feuer, “Super-Resolution Restoration of an Image Sequence: Adaptive
Filtering Approach,” IEEE Trans. Image Process., vol. 8, no. 3, pp. 387–395, March
1999.

466
CHAPTER 11 Digital Video Processing
[31] P. Vandewalle, S. Susstrunk, and M. Vetterli, “A Frequency Domain Approach to Reg-
istration of Aliased Images with Application to Super Resolution,” EURASIP J. Appl.
Signal Process., vol. 2006, pp. 1–14.
[32] B. K. Gunturk, Y. Altunbasak, and R. M. Mersereau, “Color Plane Interpolation Using
Alternating Projections,” IEEE Trans. Image Process., vol. 11, no. 9, pp. 997–1013,
September 2002.
[33] M. Protter, M. Elad, H. Takeda, and P. Milanfar, “Generalizing the Nonlocal-Means
to Super-Resolution Reconstruction,” IEEE Trans. Image Process., vol. 18, no. 1,
pp. 36–51, January 2009.
[34] D. Glasner, S. Bagon, and M. Irani, “Super-Resolution from a Single Image,” Proc. Int.
Conf. Comput. Vision (ICCV), pp. 349–356, Kyoto, Japan, September 2009.
[35] T. Wiegand, X. Zhang, and B. Girod, “Long-term Memory Motion-Compensated
Prediction,” IEEE Trans. CSVT, vol. 9, pp. 70–84, February 1999.

CHAPTER
Digital Video Compression12
Video coders compress a sequence of images, so this chapter is closely related to
Chapter 9 on image compression. Many of the techniques introduced there will be
used and expanded upon in this chapter. At the highest level of abstraction, video
coders comprise two classes: interframe and intraframe, based on whether they use
statistical dependency between frames or are restricted to using dependencies only
within a frame. The most common intraframe video coders use the DCT and are
very close to JPEG; for video, they are called M-JPEG, wherein the “M” can be
thought of as standing for “motion,” as in “motion picture.” Also, common intraframe
video compression is the DV coder in standard-deﬁnition video camcorders. By
contrast, interframe coders exploit dependencies across frame boundaries to gain
increased efﬁciency. The most efﬁcient of these coders make use of the apparent
motion between video frames to achieve their signiﬁcantly larger compression ratio.
An interframe coder will generally code the ﬁrst frame using intraframe coding but
will then use interframe coding on the other frames. Common interframe video coders
are HDV and AVCHD consumer camcorders.
MPEG coders restrict their interframe coding to a group of pictures of relatively
small size, say 12–60 frames, to prevent error propagation. These MPEG coders use a
transform compression method for both the frames and prediction residual data, thus
exemplifying hybrid coding, since the coder is a hybrid of the block-based transform
spatial coder of Chapter 9 and a predictive or DPCM temporal coder. The current
standards-based coder H.264/AVC uses a mixture of transform and spatial predic-
tion on the prediction residuals. When transforms are used in both spatial and time
domains, the coder is commonly called a 3-D transform coder.
All video coders share the need for a source buffer to smooth the output of the
variable-length coder for each frame. The overall video coder can be constant bitrate
(CBR) or variable bitrate (VBR) depending on whether the buffer output bitrate is
constant. Often, intraframe video coders are CBR, in that they assign or use a ﬁxed
number of bits for each frame. In the case of interframe video coders, the bitrate
is more highly variable. For example, an action sequence may need a much higher
bitrate to achieve a good quality than would a so-called “talking head,” but this is
only apparent from the interframe viewpoint.
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00012-6
c⃝2012 Elsevier Inc. All rights reserved.
467

468
CHAPTER 12 Digital Video Compression
Table 12.0–1 Types of Video with Uncompressed and Compressed Bitrates
Video
Pixel Size
Frame Rate
Rate (uncomp.)
Rate (comp.)
Teleconference (QCIF)
176 × 144
5–10 fps
3–6 Mbps
32–64 Kbps
Multimedia (SIF)
352 × 288
30
61 Mbps
200–300 Kbps
Standard deﬁnition (SD)
720 × 480
30
243 Mbps
4–7 Mbps
High deﬁnition (HD)
1920 × 1080
24, 30
1.0 Gbps
20–35 Mbps
Digital cinema (DC)
4096 × 2160
24
7.6 Gbps
100–200 Mbps
Table 12.0–1 shows the various types of digital video, along with frame size in
pixels and frame rate in frames per second (fps). Uncompressed bitrate assumes an
8-bit pixel depth, except for 12-bit digital cinema, and includes a factor of three for
RGB color. The last column gives an estimate of compressed bitrates using technol-
ogy, such as H.264/AVC and MPEG 2. We see fairly impressive compression ratios
comparing the last two columns of the table, in the vicinity of 100 in several cases.
While the given “pixel size” may not relate directly to a chosen display size, it does
give an indication of recommended display size for general visual content, with stan-
dard deﬁnition (SD) and above generally displayed at full screen height, multimedia
at half screen height, and teleconference displayed at one-quarter screen height, on a
normal display terminal. In terms of distance from a viewing screen, it is conserva-
tively recommended to view SD at 6 times the picture height, and HD at three times
the picture height, and DC at 1.5 times the picture height (see Chapter 6).
12.1 INTRAFRAME CODING
In this section we look at three popular intraframe coding methods for video. They
are block DCT, motion M-JPEG, and SWT. The new aspect over the image coding
problem is the need for rate control, which arises because we may want to have a
variable bit assignment across the frames to get more uniform quality. In fact, if we
would like to have constant quality across the frames that make up our video, then
the bit assignment must adapt to frame complexity, resulting in a VBR output of the
coder.
Figure 12.1–1 shows the system diagram of a transform-based intraframe video
coder. We see the familiar transform, quantize, and VLC structure. What is new is
the buffer on the output of the VLC and the feedback of a rate control from the
buffer. Here, we have shown the feedback as controlling the quantizer. If we need
CBR video, then the buffer output bitrate must be constant, so its input bitrate must
be controlled so as to avoid overﬂow or underﬂow, the latter corresponding to the
case where this output buffer is empty and therefore unable to supply the required
CBR. The common way of controlling the bitrate is to monitor buffer fullness and
then feed back this information to the quantizer. Usually the step size of the quantizer
is adjusted to keep the buffer fullness around its midpoint, or half full.

12.1 Intraframe Coding
469
Transform
VLC
Buffer
Quantize
Transmit
Rate control
Video
source
FIGURE 12.1–1
Illustration of intraframe video coding with rate control.
Input
Output
0
FIGURE 12.1–2
Illustration of uniform threshold quantizer (UTQ), named for its deadzone at the origin.
As mentioned in Section 9.3, a uniform quantizer has a constant step size, so that
if the number of output levels is an odd number and the input domain is symmetric
around zero, then zero will be an output value. If we enlarge the step size around zero,
say by a factor of two, then the quantizer is said to be a uniform threshold quantizer
(UTQ) (Figure 12.1–2). The bin that gets mapped into zero is called the dead zone and
can be very helpful to reduce noise and “insigniﬁcant” details. Another nice property
of such UTQs is that they can be easily nested for scalable coding, which we discuss
later in this chapter.
If we let the reconstruction levels be bin-wise conditional means and then entropy
code this output, then UTQs are known to be close to optimal for common image
probability density functions such as Gaussian and Laplacian. In order to control
the generated bitrate via a buffer placed at output of the coder, a quality factor has
been introduced. Both CBR and VBR strategies are of interest. We next describe the
M-JPEG procedure.
M-JPEG Pseudo Algorithm
1. Input a new frame.
2. Scan to next 8- ×8-image block.
3. Take 8 × 8 DCT of this block.

470
CHAPTER 12 Digital Video Compression
4. Quantize AC coefﬁcients in each DCT block, making use of a quantization
matrix Q = {Q(k1,k2)},
[
DCT(k1,k2) = Int
DCT(k1,k2)
s Q(k1,k2)

,
(12.1–1)
where s is a scale factor used to provide a rate control. As s increases, more
coefﬁcients will be quantized to zero and hence the bitrate will decrease. As s
decreases toward zero, the bitrate will tend to go up. This scaling is inverted
by multiplication by s Q(k1,k2) at the receiver. The JPEG quality factor Q is
inversely related to the scale factor s.
5. Form the difference of successive quantized DC terms 1DC (effectively a
noiseless spatial DPCM coding of the quantized DC terms).
6. Scan the 63 AC coefﬁcients in a conventional zigzag scan.
7. Variable length code this zigzag sequence as follows:
a. Obtain run length (RL) to next nonzero symbol.
b. Code the pair (RL, nonzero symbol value) using a 2-D Huffman table.
8. Transmit (store, packetize) bitstream with end-of-block (EOB) markers. In addi-
tion to this coded data, a ﬁle header would contain information on quantization
matrix Q, Huffman table, image size in pixels, color space used, bit depth, frame
rate, etc.
9. If there are more data in frame, return to step 2.
10. If there are more frames in sequence, return to step 1.
Variations on this M-JPEG algorithm have been used in both consumer and pro-
fessional camcorders and video editing software. A key aspect needed when JPEG is
extended to video is rate control, which calculates the scale factor s in this algorithm.
The following example shows a simple type of rate control with the goal of achieving
CBR on a frame-to-frame basis (i.e., intraframe rate control). It should be mentioned
that while JPEG is an image coding standard, M-JPEG is not a video coding standard.
So various ﬂavors of M-JPEG exist and are not exactly compatible.
Example 12.1–1: M-JPEG
Here, we present two nearly constant bitrate examples obtained with a buffer and a simple
feedback control to adjust the JPEG quality factor Q. Our control strategy is to ﬁrst ﬁt a
straight line of slope γ to plot the Q factor versus log R of a sample frame in the clip to
estimate an appropriate step size, and then employ a few iterations of steepest descent.
Further, we use the Q factor of the prior frame as the ﬁrst guess for the present frame.
The update equation becomes
Qnew = Qprev + γ (logRtarget −log Rprev).
We ended up with 1.2–1.4 iterations per frame on average to produce the following
results. We have done two SIF examples: Susie at 1.3 Mbps and Table Tennis at 2.3 Mbps.

12.1 Intraframe Coding
471
45
40
35
PSNR in dB
30
0
10
20
30
40
Frame number
M-JPEG coding of monochrome SIF version of Susie at 1.31 Mbps
Solid : With rate control
Dashed : No rate control, fixed Q= 50.0
50
60
70
80
Rate in kbytes
Solid : With rate control
Dashed : No rate control, fixed Q= 50.0
0
10
20
30
40
Frame number
50
60
70
80
60
50
40
30
Q factor
Solid : With rate control
Dashed : No rate control, fixed Q= 50.0
0
10
20
30
40
Frame number
50
60
70
80
80
60
40
FIGURE 12.1–3
Rate control of M-JPEG on the Susie clip.
Figure 12.1–3 shows the results for the Susie clip, from top to bottom: peak signal-to-noise
ratio (PSNR), rate in KB/frame, and Q factor. We see a big variation in rate without rate
control but a fairly constant bitrate using our simple rate control algorithm. Results from
the second example, Table Tennis, are shown in Figure 12.1–4 where we note that the
rate is controlled to around 76 KB/frame quite well.
Notice that in both of these cases, the constraint of constant rate has given rise to
perhaps excessive increases in PSNR in the easy-to-code frames that contain blurring
due to fast camera and/or object motion.
Most SD video has been, and continues to be, interlaced. However, in the pres-
ence of fast motion, grouping of two ﬁelds into a single frame prior to 8 × 8 DCT
transformation is not very efﬁcient. As a result, when M-JPEG is used on interlaced

472
CHAPTER 12 Digital Video Compression
35
PSNR in dB
25
30
1
26
51
76
Frame number
M-JPEG coding of monochrome SIF version of Tennis at 2.3387 Mbps
101
126
1
26
51
76
Frame number
101
126
100
80
60
Rate in kbytes
40
Solid: With rate control
Dashed: No rate control, fixed Q= 50.0
1
26
51
76
Frame number
101
126
70
60
50
Q factor
40
Solid: With rate control
Dashed: No rate control, fixed Q= 50.0
Solid: With rate control
Dashed: No rate control, fixed Q= 50.0
FIGURE 12.1–4
An illustration of rate control performance for M-JPEG coding of the Table Tennis clip.
SD video, usually each ﬁeld is coded separately, and there is no provision for com-
bining ﬁelds into frames, which would be more efﬁcient in the case of low motion.
The common DV coder, still based on DCT, has such an adaptive feature, giving it
an efﬁciency edge over M-JPEG.
DV Codec
The source coder almost universally used in consumer SD camcorders is called
DV coder [1], and there is some variation in the details of how they work. These
intraframe coders ﬁrst subsample the chroma components in the NTSC system in
4:1:1 fashion,1 as shown in Figure 12.1–5, where the x symbols indicate luma
1A PAL version is available that uses a 4:2:0 color space at 25 interlaced fps.

12.1 Intraframe Coding
473
FIGURE 12.1–5
4:1:1 color subsamling pattern of DV in the so-called 525/30 NTSC system.
Y
Cr
Cb
8×8 blocks
FIGURE 12.1–6
The DV macroblock for the NTSC (525/30) system.
samples and the o symbols represent chroma (both Cr and Cb) sample locations in a
ﬁeld. Note that the picture sites of the two chroma samples occur at a luma sample
site. This is not always the case for 4:1:1 but is true for NTSC DV coders.
There is a provision for combining the two ﬁelds of the interlaced NTSC system
into a frame or not, depending on a mode decision (not standardized, but left up to
the DV coder designer). Then 8 × 8 DCTs of either ﬁelds or frames are performed
and grouped into macroblocks as shown in Figure 12.1–6.
Thus a DV macroblock consists of six blocks: four luma and two chroma blocks.
These macroblocks are then quantized and variable-length coded. But ﬁrst, they are
mapped one-to-one onto sync blocks and then stored K at a time into segments.
Each segment is assigned a ﬁxed code length in order to permit robust reading of
digital tape at high speed, for so-called “trick play,” like search in fast-forward.
Thus VLC is only used in DV coders within segments. The quantization is per-
formed similar to (12.1–1), with the scaling variable s controlled to achieve a ﬁxed
target bitrate per segment. In what is called a feed-forward or look-ahead strat-
egy, 8 to 16 different values of s are tried within the segment to meet the goal
for ﬁxed bit assignment. The best one is then selected for the actual coding of
the segment. The DV standard adopted K = 30 macroblocks per segment, and they
are distributed “randomly” around the image frame to improve the uniformity of
picture quality, which at 25 Mbps is generally between 35 and 40 dB for typical
interlaced SD video content. The DV standard for SD video has been largely sup-
planted by HDV introduced in 2003 as a consumer format for HD video. It is not an
intraframe coder, but rather is based on the interframe coder MPEG 2, to be discussed
in Section 12.3.
Both the M-JPEG and DV coder have a constant bitrate per frame and so are CBR.
This means that the video quality will be variable, depending on the difﬁculty of the

474
CHAPTER 12 Digital Video Compression
various image frames. They effectively are buffered at the frame level to achieve this
CBR property. Use of a longer buffer would allow VBR operation, wherein we could
approach constant quality per frame.
Intraframe SWT Coding
Here, we look at intraframe video coders, which use SWT in place of DCT. Again,
the need for some kind of control of the video bitrate is the new main issue over SWT
image coding. First, we consider the rate assignment over the spatial subbands within
each frame.
We must split up the total bits for each frame into the bit assignment for each
subband. If we assume an orthogonal analysis/synthesis SWT, then the total mean-
square error distortion D due to the individual quantizations, with assumed distortion
Dm(Rm) for Rm bits assigned to the mth subband quantizer, can be written as the
weighted sum of the mean-square distortions over the M subbands as
D =
M
X
m=1
Nm
N Dm(Rm),
(12.1–2)
as pointed out for the image compression case in Chapter 9, where Nm is the num-
ber of samples in the mth subband, and N is the total number of samples. Then
modeling the individual quantizers as Dm = gσ 2
m2−2Rm, all with the same quantizer
efﬁciency factor g, the optimal bit assignment for an average bitrate of R bits/pixel,
was determined as
Rm = R + 1
2 log2
 
σ 2
m
σ 2wgm
!
, m = 1,..., M,
(12.1–3)
where the weighted geometric mean σ 2
wgm is deﬁned as
σ 2
wgm ≜
YM
m=1

σ 2
m
Nm/N
.
The resulting MSE per frame in the reconstruction then becomes
D = gσ 2
wgm2−2R,
which is analogous to the case of SWT image coding (see Chapter 9).
If we choose to have CBR, then this is the solution. However, we may want to
either achieve constant distortion or minimum average distortion. This then requires
that we vary the bit assignment over the frames. Given the image frames’ subband
variances, we can compute the weighted geometric mean of each frame σ 2
wgm(n); the

12.1 Intraframe Coding
475
distortion at frame D(n) will then be
D(n) = gσ 2
wgm(n)2−2R(n),
so that the total distortion over all the frames becomes
DT =
N
X
n=1
D(n)
=
N
X
n=1
gσ 2
wgm(n)2−2R(n),
(12.1–4)
where R(n) is the bit assignment to the nth frame, and the average bitrate is then
1
N RT = 1
N
N
X
n=1
R(n).
(12.1–5)
This pair of operational rate and distortion functions (12.1–4) and (12.1–5) can then
be optimized for best average performance over time. Normally this minimum will
not occur at the point where all the frames have the same distortion D(n) = D.
In that case, one has to make the choice of either optimizing the constant distor-
tion performance or optimizing for best average performance. They are generally
different.
One issue in SWT coding is the choice of the subband/wavelet ﬁlter to use, as
considered in the following example.
Example 12.1–2: SWT Filter Study
The goal is to see the effect of different SWT ﬁlters on an efﬁcient intraframe coder. Indi-
vidual frames are ﬁrst subband analyzed and then individually quantized by UTQs. The
output of the quantizers is then separately arithmetic coded. The closed form log bit-
assignment rule of this section was used. We also used the arithmetic coding algorithm
of [2]. Generalized Gaussian models, whose parameters were estimated from ﬁrst frame,
were employed. The AC output was grouped into sync blocks: LL-LL subband blocked to
two lines, LL-LH, –HL, –HH subbands blocked to four lines, and the higher subbands not
blocked. A 5-bit end-of-block marker was used to separate these sync blocks. The results
from [3] for the HD test clip MIT sequence are shown in Figure 12.1–7.
We see that there is not a drastic PSNR difference in the performance of these
ﬁlters, but here the longer QMFs do better. The popular CDF 9/7 wavelet ﬁlter was not
included in the study but was later found to have similar performance, slightly better

476
CHAPTER 12 Digital Video Compression
42
40
38
PSNR
36
34
32
0.6
0.8
1
1.2
Actual av. rate (bpp)
1.4
1.6
1.8
2
QMF 16B
QMF 32C
AS 9× 9
W 8× 8
QMF 12B
W 6× 6
K 7× 7
G 5× 3
K 9× 7
FIGURE 12.1–7
Comparison of various SWT ﬁlters for intraframe video compression.
than the QMFs for dyadic or wavelet decomposition, but slightly worse for full subband
decomposition [4].
M-JPEG 2000
M-JPEG 2000 is ofﬁcially JPEG 2000, Part 3, and amounts to a standardized ﬁle
format or container for a sequence of JPEG 2000 image ﬁles. It can be CBR or VBR
within the standard. Various methods of bitrate control can be applied to the resulting
sequence of coded frames, making them either constant quality, maximum average
quality, HVS weighted quality, etc. The M-JPEG 2000 standard is just the standard
for the decoder, thus permitting any variable numbers of bits per frame. As always,
rate control is only an encoder problem.
As mentioned in Chapter 9, JPEG 2000 uses operational rate-distortion optimiza-
tion over tiles on the image frame. These tiles are then coded using the constant slope
condition, resulting in minimum average mean-square coding error. This constant
slope matching of JPEG 2000 can be extended between frames to achieve a mini-
mum mean-square coding error for the whole image sequence. Such an approach may
mean an unacceptable delay, so a matching of slope points on a local frame-by-frame

12.2 Interframe Coding
477
basis may be chosen as an alternative VBR solution that has been found much closer
to constant quality than is CBR.
Example 12.1–3: Digital Cinema
In 2004, Digital Cinema Initiatives (DCI) ran a test of compression algorithms for the digital
cinema distribution at both 4K and 2K resolution, meaning horizontal number of pixels
and aspect ratios around 2.35. DCI had been created 2 years earlier by the major U.S.
motion picture companies. A test suite was generated for the tests by the American Society
of Cinematographers (ASC) and consisted of 4K RGB image frames at 24 fps. The bit
depth of each component was 12 bits. After extensive testing, involving both expert and
nonexpert viewers, DCI made the recommendation of M-JPEG 2000 as the best choice
for digital cinema. In part this was because of the 4K/2K scalability property, and in part
because of its excellent compression performance. The tests used the CDF 9/7 ﬁlter or
lossy version of the M-JPEG 2000 coder. The JPEG committee later standardized a new
proﬁle of JPEG 2000 Part 1 specially for digital cinema use. Bitrates are in the vicinity of
100–200 Mbps at the 4K resolution.
We now turn to the generally more efﬁcient interframe coders. While these coders
offer more compression efﬁciency than the preceding intraframe coders, they are
not so artifact-free and are mainly used for distribution quality purposes. Intraframe
coders, with the exception of the digital cinema distribution standard, have been
mainly used for contribution quality (i.e., professional applications) because of their
high quality at moderate bitrates and their ease of editing.
12.2 INTERFRAME CODING
Now we look at coders that make use of the dependence between or inter frame.
We consider spatiotemporal (3-D) DPCM, basic MC-DCT concepts, MPEG/VCEG
video coding, H.26x visual conferencing, and MC-SWT coding. We start with the
1-D DPCM coder and generalize it to encode entire image frames in the temporal
direction.
Generalizing 1-D DPCM to Interframe Coding
We replace the 1-D scalar value x(n) with the video frame x(n1,n2,n) and do the pre-
diction from a nonsymmetric half-space (NSHS) region. However, in practice the
prediction is usually based only on the prior frame or frames. Then we quan-
tize the prediction error difference or residual. We thus have the system shown in
Figure 12.2–1. We can perform conditional replenishment [5], which only transmits
pixel signiﬁcant differences (i.e., beyond a certain threshold). These signiﬁcant dif-
ferences tend to be clustered in the frame so that we can efﬁciently transmit them

478
CHAPTER 12 Digital Video Compression
x (n1, n2, n)
e(n1, n2, n)
3-D linear
predictor
+
−
+
Σ
x∼
e (n1, n2, n)
∧
Q[  ]
FIGURE 12.2–1
Spatiotemporal generalization of DPCM with spatiotemporal predictor.
with a context-adaptive VLC. The average bitrate at buffer output can be controlled
by a variable threshold for signiﬁcant differences. For an excellent summary of this
and other early contributions, see the 1980 review article by Netravali and Limb [6].
Looking at Figure 12.2–1, we can generalize it by putting any 2-D spatial or
intraframe coder in place of the scalar quantizer Q[·] in the generalized DPCM. If
we use block DCT for the spatial coder block that replaced the quantizer, we have a
hybrid coder, being temporally DPCM and spatially transform based. We can use a
frame-based predictor, whose most general case would be a 3-D nonlinear predictor
operating on a number of past frames. Often, though, just a frame delay is used, effec-
tively predicting that the current frame will be the same as the motion-warped version
of one previous frame. In some current coding standards, a spatial ﬁlter (called a loop
ﬁlter since it is inside the feedback loop) is used to shape the quantizer noise spec-
trum and to add temporal stability to this otherwise only marginally stable system.
Another way to stabilize the DPCM loop is to put in an Intra frame every so often.
Calling such frame an I frame, and the intercoded frames P, we can denote this as
IPPP ... PIPPP ... PIPPP .... Another idea, in the same context, would be to ran-
domly insert I blocks instead of complete I frames. While the former refresh method
is used in the MPEG 2 entertainment standard, the latter method is used in the H.263
visual conferencing standard.
MC Spatiotemporal Prediction
There are two types of motion-compensated hybrid coders. They differ in the kind
of motion estimation they employ: forward motion estimation or backward motion
estimation. The MC block in Figure 12.2–2 performs the motion compensation
(warping) after the motion estimation block computes the forward motion. The quan-
tity ex(n1,n2,n) seen in the encoder is also the decoded output; that is, the encoder
contains a decoder. The actual decoder is shown in Figure 12.2–3; it consists of ﬁrst a
spatial decoder followed by the familiar DPCM temporal decoding loop, as modiﬁed
by the motion compensation warping operation MC that is controlled by the received
motion vectors.
In forward MC, motion vectors are estimated between the current input frame
and the prior decoded frame. Then the motion vectors must be coded and sent along
with the MC residual data as side information since the decoder does not have access
to the input frame x(n1,n2,n). In backward MC, we base the motion estimation on
the previous two decoded frames ex(n1,n2,n −1) and ex(n1,n2,n −2), as shown in

12.2 Interframe Coding
479
+
−
Σ
Spatial
coder
+
Spatial
decoder
Motion
estimator
Motion
vectors
MC residual
MC
Z−1
x (n1, n2, n)
e (n1, n2, n)
e(n1, n2, n)
∼
x (n1, n2, n)
∼
x(n1, n2, n)
∧
FIGURE 12.2–2
Illustrative system diagram for motion-compensated DPCM.
+
MC
Z−1
Spatial
decoder
Motion
vectors
x (n1, n2, n)
∼
e (n1, n2, n)
∼
FIGURE 12.2–3
The hybrid decoder.
+
−
Σ
Spatial
coder
Spatial
decoder
Motion
estimation
MC
residual
MC
+
x (n1, n2, n)
e(n1, n2, n)
∼x (n1, n2, n)
∼e(n1, n2, n)
z−1
z−1
FIGURE 12.2–4
Illustration of backward motion compensation.
Figure 12.2–4. Then there is no need to transmit motion vectors as side information,
since in the absence of channel errors the decoder can perform the same calcula-
tion as the encoder and come up with the same motion vectors. Of course, from
the viewpoint of total computation, backward MC means doing the computationally
intensive motion estimation and compensation work twice, once at the coder and once

480
CHAPTER 12 Digital Video Compression
at the decoder. (You are asked to write the decoder for backward motion-compensated
hybrid coding as an end-of-chapter problem.)
One question with interframe coding is the required accuracy in representing the
motion vectors. There has been some theoretical work on this subject [7] that uses
a very simple motion model. Even so, insight is gained on required motion vector
accuracy, especially as regards the level of noise in the input frames.
12.3 EARLY INTERFRAME CODING STANDARDS
The MPEG committee is a part of the International Standards Organization (ISO)
and introduced its ﬁrst video compression standard for CD-based video in 1988. It
was called MPEG 1. A few years later, they released a standard for SD video called
MPEG 2. This standard has been widely used for distribution of digital video over
satellite, DVD, terrestrial TV broadcast, and cable. It was later extended to HD video.
These are really standards for decoders. Manufacturers are left free to create encoders
with proprietary features that they feel improve picture quality, reduce needed bitrate,
or improve functionality. The only requirement is that these coders create a bitstream
that can be decoded by the standard decoder, thus ensuring interoperability. Since
you cannot really create a decoder without an encoder, MPEG developed various
test model coders, or veriﬁcation models, some of which have become widely avail-
able on the Web. However, the performance of these “MPEG coders” varies and
should not be considered to indicate the full capabilities of the standard. In the par-
lance of MPEG, these veriﬁcation model coders are nonnormative (i.e., not part of the
standard). The best available coders for an MPEG standard are usually proprietary.
MPEG 1
The ﬁrst MPEG standard, MPEG 1, was intended for the SIF resolution, about half
NTSC television resolution in each dimensions. Exactly, this is 352 × 240 luma pix-
els. The chroma resolution is 2 × 2 reduced by half again to 176 × 120 chroma pixels.
Temporally, SIF has a 30-Hz frame rate, that is progressive or noninterlaced. The
aspect ratio is 4:3. The MPEG 1 standard was meant for bitrates between 1 and 2
Mbit/sec.
The overall coding method was intended for digital video and audio on 1x CDs
in the late 1980s with a maximum data rate of 1.5 Mbps, with 1.2 Mbps used for
video. They wished to provide for fast search, fast backward search, easy still-frame
display, limited error propagation, and support for fast image acquisition starting at
an arbitrary point in the bitstream. These needed functionalities make it difﬁcult to
use conventional frame-difference coding—i.e., to use temporal DPCM directly on
all the frames. The MPEG solution is to block a number of frames into groups of
pictures (GOPs), and then to code these GOPs separately.

12.3 Early Interframe Coding Standards
481
Each GOP must start with a frame that is intracoded, the I frame. This will limit
efﬁciency, but facilitate random search, which can be conducted on the I frames. An
illustration of GOP structure is shown in Figure 12.3–1. If the GOP is not too long,
say half a second, then the rapid image acquisition functionality can be achieved
too. After the initial I frame, the remaining frames in the GOP are interframe coded.
MPEG has two types of intercoded frames, B bi-directional MC-interpolated and
P frames that are MC-predicted. The typical GOP size in video is N = 15, with I = 1
intraframe, U = 4 predicted frames, and B = 10 interpolated frames. A parameter
M = 2 here denotes the number of B frames between each P frame. After the I frame,
we next predictively code the P frames one by one, as seen in Figure 12.3–2. After its
neighboring P frames are coded, the bidirectional interpolation error of the B frames
is calculated (and quantized), as illustrated in Figure 12.3–3. In MPEG parlance,
with reference to this ﬁgure, the neighboring I and P frames serve as references for
the coding of the B frames.
In MPEG 1, motion compensation is conducted by block matching on mac-
roblocks of size 16 × 16. Exactly how to do the block matching is left open in
the standard, as well as the size of the search area. Accuracy of the motion vec-
tors was speciﬁed at the integer pixel level, sometimes called “full pixel accuracy,”
the accuracy naturally arising from block-matching motion estimation. In fact, most
MPEG 1 coders use a fast search method in their block matching.
I
P
P
I
B
B
B
B
B
B
Group of pictures (GOP) 
First frame in group is coded by itself—intraframe coding 
FIGURE 12.3–1
First code I frames (N frames in group).
I
P
P
I
B
B
B
B
B
B
Prediction step—creates some intermediate frames 
FIGURE 12.3–2
Illustration of predictive coding of P frames in MPEG 1.

482
CHAPTER 12 Digital Video Compression
Group of pictures (GOP) 
I
I
P
P
I
B
B
B
B
B
B
Interpolation step—creates two intermediate frames
FIGURE 12.3–3
Illustration of coding the B frames based on bidirectional references.
We can see that MPEG 1 uses a layered coding approach. At the highest level
is the GOP layer. The GOP in turn is made up of pictures I, P, and B. Each picture
is composed of an array of macroblocks. Each macroblock consists of a 2 × 2 array
of 8 × 8 blocks. The DCT is performed at this block level. There is one additional
level, the slice level, that occurs within a picture. A slice is just a row or contigu-
ous set of rows of macroblocks. Its main purpose is synchronization in the face of
channel errors or lost packets in transmission. VLC is performed only within a slice,
and every slice is terminated with a unique end of slice (EOS) codeword. When this
EOS codeword is detected in the sequence, end of slice is declared, and any rem-
nant effects of past channel errors terminate. Another advantage of partitioning the
coded data into slices is to provide more decoder access points, since there is usu-
ally a slice header that speciﬁes frame number and position information. Usually
cross-references across slices are disallowed, thus incurring at least a slight bitrate
efﬁciency penalty for using small slices.
MPEG 1 achieves CBR by employing a so-called video buffer veriﬁer in the coder.
It serves as a stand in for the buffer that must be used at a decoder, which it is fed
by a constant bitrate from the channel. By keeping the video buffer veriﬁer from
underﬂow or overﬂow during coding, one can guarantee that there will be no buffer
problems during decoding.
MPEG 2—A “Generic Standard”
The MPEG2 coder was developed jointly with the VCEG committee of the Interna-
tional Telecommunications Union (ITU) and is also known as H.262 [8]. It includes
MPEG 1 as subset and does this by introducing proﬁles and levels. The level refers to
the image size (i.e., CIF, SIF, or SD), frame rate, and whether the data are interlaced
or progressive. The proﬁle refers to the MPEG 2 features that are used in the coding.
MPEG 2 also established new prediction modes for interlaced SD, permitting predic-
tion to be ﬁeld based or frame based, or a combination of the two. The predictions
and interpolations had to consider the interlaced nature of common SD NTSC and
PAL data. As a result, various combinations of ﬁeld-based and frame-based predic-
tion and interpolation modes are used in MPEG 2. For example, when bidirectionally
predicting (interpolating) the B frames in Figure 12.3–4, there are both ﬁeld-based

12.3 Early Interframe Coding Standards
483
and frame-based prediction modes. In ﬁeld-based mode, which usually works best
for fast motion, each ﬁeld is predicted and each difference ﬁeld is coded separately.
In frame-based mode, which works best under conditions of slow or no motion, the
frame is treated as a whole and coded as a unit. In ﬁeld-based mode there is the ques-
tion of what data to use as the reference ﬁeld—i.e., which ﬁeld, upper or lower.2
The additional overhead information of the prediction mode, ﬁeld or frame, has to
be coded and sent along as overhead. This provision for interlaced video makes
MPEG 2 more complicated than MPEG 1, which only handled progressive video,
but reportedly gives it about a 0.5 to 2.0 dB advantage [9] versus frame-based mode
alone. This is due to the combined and adaptive use of frame/ﬁeld prediction and
frame/ﬁeld DCT. Additionally, whether interlaced or progressive, MPEG 2 permits
use of half-pixel accurate motion vectors for improved prediction and interpolation
accuracy at the cost of higher motion vector rate. Most all MPEG 2 coders make use
of this feature.
Commonly used color spaces in MPEG 2 are 4:2:2 for professional applications
and 4:2:0 for consumer delivery, such as on DVD or digital satellite television. The
corresponding subsampling structures are shown in parts a and b, respectively, in
Figure 12.3–5 [8].
I
I
P
P
I
B
B
B
B
B
B
Frame- or field-based bidirectional prediction
Each frame holds two fields—upper and lower
FIGURE 12.3–4
New in MPEG2 was the need to process interlaced frame data.
4:2:2
4:2:0
(a)
(b)
FIGURE 12.3–5
MPEG 2 4:2:2 (left) and 4:2:0 (right) luma and chroma subsampling sites.
2In MPEG 2, all data are stored in frames, and the ﬁeld and frame numbers in a GOP are numbered
starting with 0. Each frame consists of an upper or even ﬁeld, which comes ﬁrst, and a lower or odd
ﬁeld, which comes second.

484
CHAPTER 12 Digital Video Compression
Example 12.3–1: MPEG 2 Example
We coded the ITU 601 or SD test sequence Susie with a standard MPEG 2 coder available
on the Web. The bitrate was 3 Mbps at a frame rate of 30 fps. We ﬁrst used all I frames
(i.e., GOP = 1), then we set GOP = 15 or a half second, and coded without B frames, and
then with B frames. The PSNR results are shown in the Table 12.3–1. We see a jump in
PSNR going from I frame only to I and P, showing the efﬁciency of the P frame in a hybrid
coder. However, not much objective improvement for adding on the B frame is seen in this
visual conferencing test clip. Videos are available for download at the book’s Web site.
Table 12.3–1 Some MPEG 2
Coding Results at 3 Mbps for
the Susie SD Test Clip
PSNR
Y(dB)
U(dB)
V(dB)
I
33.8
44.7
44.6
I,P
39.0
45.7
45.5
I,B,P
39.1
46.1
45.8
A recent paper reported optimization of an MPEG 2 coder for IPPP ... mode,
indicating a 2 to 3 dB advantage [10] at high bitrates for the ﬁrst 101 frames of
quarter CIF (QCIF) Foreman versus the coding result of the widely used MPEG 2
test model 5 (TM5) [11] software.
The Missing MPEG 3—High-Deﬁnition Television
Standard deﬁnition video spatial resolution is not really adequate, so researchers
worked for many years on an increased resolution system. Reading from report 801–
4 of CCIR (now ITU) in 1990 they deﬁned HDTV as “a system designed to allow
viewing at about three times picture height, so that the system is virtually transparent
to the quality of portrayal that would have been perceived in the original scene or
performance by a discerning viewer with normal visual acuity.”
Researchers at NHK in Japan started modern research on HDTV in the early
1970s [12]. They came up with an analog system called Hi-Vision that enjoyed
some deployment, mainly in Japan, but the equipment was very bulky, heavy,
and expensive. This spurred U.S. companies and researchers to try to invent an
all-digital approach in the mid- to late-1980s. A set of competing digital HDTV
proposals then emerged in the United States. The Grand Alliance was formed in May
1993 from these proponents and offered a solution, eventually based on extending
MPEG 2. It covered both interlaced and progressive scanning formats in a 16:9
aspect ratio (near to the 2:1 and higher aspect ratio of motion pictures). The resulting
digital TV standard from the group now known as the Advanced Television Systems
Committee (ATSC) offered 1080i and 720p digital video for terrestrial broadcast in

12.3 Early Interframe Coding Standards
485
North America. More information on these formats can be found in the appendix
at the end of Chapter 11. Since ATSC essentially voted in favor of MPEG 2, the
HD and SD resolutions were essentially folded together into one coding family, just
extending the proﬁles and levels of MPEG 2. Thus HD is coded with MPEG 2—main
proﬁle, at High level (MP@HL). As mentioned earlier, HDV camcorders also use
MPEG 2 at HL.
MPEG 4—Natural and Synthetic Combined
The original concept of MPEG 4 was to ﬁnd a much more efﬁcient coder at low
bitrates (e.g., 64 Kbps). When this target proved elusive, the effort turned to object-
based video coding. In MPEG 4, so-called video objects (VO) are the basic elements
to be coded. Their boundaries are coded by shape coders and their interiors are coded
by so-called texture coders. The method was eventually extended up to SD and HD
and is backward compatible with MPEG 2. A synthetic natural hybrid coder (SNHC)
is capable of combining natural and synthetic (cartoon, graphics, text) together in a
common frame. Nevertheless, most widely available embodiments of MPEG 4 work
only with rectangular objects and one object per frame. Segmenting natural video
into reasonable objects remains a hard problem.
Also starting with MPEG 4, there was an effort to separate the video coding from
the actual transport bitstreams. They wanted MPEG 4 video to be carried on many dif-
ferent networks with different packet or cell sizes and qualities of service. So MPEG
4 simply provides a delivery multimedia integration framework for adapting the out-
put of the MPEG 4 video coder, the so-called elementary streams (ES) to any of
several already existing transport formats, such as MPEG 2 transport stream (TS),
ATM, and IP networks (RTP/IP). Complete information on MPEG 4 is available in
the resource book by Pereira and Ebrahimi [13].
Video Processing of MPEG-coded Bitstreams
Various video processing tasks have been investigated for coded data and MPEG2
in particular. In video editing of MPEG bitstreams, the question is how to take two
or more MPEG input bitstreams and make one composite MPEG output stream. The
problem with decoding/recoding is that it introduces additional coding noise and arti-
facts and is computationally demanding. Staying as much in the MPEG2 compressed
domain as possible and reusing the editing mode decisions and motion vectors has
been found essential. Since the GOP boundaries may not align, it is necessary to
recode the one GOP where the edit point lies. Even then original bitrates may make
the output video buffer veriﬁer overﬂow. The solution is to requantize this GOP and
perhaps neighboring GOPs to reduce the likelihood of such buffer overﬂow. The
introduction of the HDV format, featuring MPEG 2 compression of HD video in cam-
era, brought the MPEG bitstream editing problem to the forefront. Several software
products are available that promise to edit HDV video in close to native mode.
The transcoding of MPEG question is how to go from a high quality level of
MPEG to a lower one, without decoding and recoding at the desired output rate.

486
CHAPTER 12 Digital Video Compression
Transcoding is of interest for video-on-demand (VOD) applications. Transcoding is
also of interest in video production work where IPIP ... may be used internally for
editing and program creation, while the longer IBBPBBP ... GOP structure is neces-
sary for program distribution. Finally, there is the worldwide distribution problem,
where 525- and 625-line systems3 continue to coexist. Here, a motion-compensated
transcoding of the MPEG bitstream is required, due to the differing 60 and 50 Hz
frame rates. Current HD formats have been standardized at 1080i and 720p, but
60 and 50 Hz frame rate variants continue to coexist. For more on transcoding, see
Section 6.3 in Bovik’s handbook [14].
H.263 Coder for Visual Conference
This coder from the VCEG group at ITU evolved from their earlier H.261 or px64
coder. As the original name implies, it is targeted at rates that are a multiple of 64
Kb/sec. To achieve such low bitrates, they resort to a small QCIF frame size and a
variable and low frame rate, with bitrate control based on buffer fullness. If there is a
lot of motion in detailed areas that generate a lot of bits, then the buffer ﬁlls and the
frame rate is reduced (i.e., frames are dropped at the encoder). The user can specify a
target frame rate, but often the H.263 coder at, say 64 Kbps will not achieve a target
frame rate of 10 fps. The H.263 coder features a group of blocks (GOB) structure,
rather than a GOP, with I blocks being inserted randomly for refresh. While there are
no B frames, there is the option for so-called PB frames. The coder has half-pixel
accurate motion vectors like MPEG 2 and can use overlapped motion vectors from
neighboring blocks to achieve a smoother motion ﬁeld and reduced blocking artifacts.
Also there is an advanced prediction mode option and an arithmetic coder option.
The reason for targeting the GOB structure versus the GOP structure is the need to
avoid the I frames in a GOP structure that require a lot of bits to transmit, a difﬁculty
in video phone that H.263 targeted as a main application. In video phone, low bitrates
and a short latency requirement (≤200 msec) mitigate the bit-hungry I frames. As
a result, in H.263, slices or GOBs are updated by I slices randomly, thus reducing
the variance on coded frame sizes that occurs with the GOP structure. High variance
of bits/frame was not a problem in MPEG 2 because of its targeted entertainment
applications like video streaming, including multicasting, digital broadcasting, and
DVD. Some low bitrate H.263 coding results are available at the book’s Web site.
Current interframe coding standards are covered in Section 12.6.
12.4 INTERFRAME SWT CODERS
The ﬁrst 3-D subband coding was done by Karlson [15] in 1989. This ﬁrst 3-D sub-
band coding used Haar or two-tap subband/wavelet ﬁlters in the temporal direction
3The reader should note that the 525-line system only has 486 visible lines, meaning it is SD, which is
720 × 486. A similar statement is true for the 625-line system. The remaining lines are hidden!

12.4 Interframe SWT Coders
487
and separable LeGall/Tabatabai (LGT) 5/3 spatial ﬁlters [16], whose 1-D component
ﬁlters are
H0(z) = 1
4(−1 + 2z−1 + 6z−2 + 2z−3 −z−4),
H1(z) = 1
4(1 −2z−1 + z−2),
G0(z) = 1
4(1 + 2z−1 + z−2), and
G1(z) = 1
4(1 + 2z−1 −6z−2 + 2z−3 + z−4).
The system was subjectively optimized using scalar quantization, and DPCM to
increase coding efﬁciency for the base subband. The 3-D frequency domain sub-
band division of Karlson is shown in Figure 12.4–1 and achieves the 3-D frequency
decomposition shown in Figure 12.4–2.
A simple 3-D subband coder was also designed by Glenn et al. [17], which
used perceptual coding based on the human contrast sensitivity function but like the
Karlson coder, no motion compensation. A real-time demonstration hardware offered
30:1 compression of SD video up to a bitrate of 6 Mbits/sec. The visual performance
was said to be comparable to MPEG 2, in that it traded slight MPEG artifacts for
this coder’s slight softening of moving scenes. Such softening of moving objects is
typical of 3-D transform coders that do not use motion compensation.
Motion-Compensated SWT Hybrid Coding
Naveen and Woods [18, 19] produced a hybrid SWT coder with a multiresolution
feature. Both pel-recursion in the context of backward motion estimation and block
HP
LP
HP
LP
HP
LP
HP
LP
HP
LP
HP
LP
HP
LP
HP
LP
HP
LP
HP
LP
Temporal 
Horizontal
Vertical 
Vertical 
Horizontal 
11
10
9
8
7
6
5
4
3
2
1
= 2:1
FIGURE 12.4–1
First 3-D subband ﬁlter, due to Karlson.

488
CHAPTER 12 Digital Video Compression
ωy
ωx
ωt
FIGURE 12.4–2
Frequency decomposition of Karlson’s ﬁlter tree.
x(n)
Original
∼x(n−1)
Reconstructed
∼e(n)
Residual
d2
d1
d0
FIGURE 12.4–3
Illustration of the multiresolution coder.
matching for forward motion estimation were considered. They used a subband pyra-
mid for hierarchical motion estimation for the multiresolution transmission of HD
and SD video. Their basic motion compensation approach placed the motion com-
pensation inside the hierarchical coding loop to avoid drift. There is an implicit
assumption here that the LL subband of the full or HD resolution is a suitable
canditate for the SD video.
An illustration of their motion estimation pyramid and coding pyramid is given
in Figure 12.4–3, which shows two subband/wavelet pyramids on the left for the
previous reconstructed frame ex(n −1) and, in the middle, the current input frame
x(n). The prediction error residual, to be coded and transmitted, e(n), is shown on
the right. First, hierarchical block matching determines a set of displacement vector
ﬁelds d0,d1, and d2 of increasing resolution. A prediction residual is then created for
the lowest resolution level and is coded and transmitted. The corresponding low res-
olution reconstructed value is then used to predict the LL subband of the next higher
resolution level. For this level, the prediction error is transmitted for the LL subband,
while the coded residuals are transmitted for the higher resolutions. The process is
repeated for the third spatial resolution. In this scheme, there arises the need to make
a bit assignment for the various levels to try to meet given quality requirements.

12.4 Interframe SWT Coders
489
In the context of block-matching motion estimation with SWT coding of the
residual, the motion vector discontinuities at the block boundaries can increase the
energy in the high-frequency coefﬁcients. This situation does not arise in DCT coding
because the DCT block size is kept at a subpartition size with respect to the motion
vector blocks; for example, common DCT size 8 × 8 with common motion vector
block sizes are 16 × 16 and 8 × 8 in MPEG 2. Thus the DCT transform does not
see the artiﬁcial edge in the prediction residual frame caused by the motion vector
discontinuity at the motion vector block boundaries. Quite the contrary for the SWT
transform where there are no block boundaries, so that any discontinuities caused by
the block motion vector prediction are a serious problem. To counter this, a small
amount of smoothing of motion vectors at the block boundaries was used in [19] for
the case of forward motion compensation. The problem does not arise in the back-
ward motion compensation case where a dense motion estimate can be used, such as
the pel-recursive method, since the motion ﬁeld does not have to be transmitted.
Gharavi [20] used block matching and achieved integer pixel accuracy in sub-
bands permitting the parallel processing of each subband at four times the lower
sample rate. However, Vandendorp [21] pointed out that neighboring subbands must
be used for inband (in-subband) motion compensation. Bosveld [22] continued the
investigation and found MSE prediction gains almost equal to those in the spatial
domain case. The result is more embedded data sets that facilitate scalability. Usually
just the nearest or bordering subbands contribute signiﬁcantly to in-band values.
3-D or Spatiotemporal Transform Coding
Kronander [23] took the temporal DCT in blocks of four or eight frames and motion
compensated then towards a central frame in the block. A spatial subband/wavelet
ﬁlter was then used to code the MC residual frames in the block. Decoding proceeded
by ﬁrst decoding the residual frames and then inverting the MC process.
Consider a block of compressed images available at time k, and denote the lth
such image
yk,l = xk,l(t −vk,l(t) △t) ; l = 0,...,L −1,
where t is a spatial position with velocity vk,l to some central reference frame l′. We
then perform spatial subband/wavelet coding/decoding of the temporal prediction
residue,
yk,l →eyk,l.
Finally, we approximately invert the displacement information to obtain the decoded
frames,
exk,l(t) =eyk,l(t −uk,l(t) △t),
where uk,l is the inverse to vk,l, at least approximately, and then proceed to the next
block, k + 1.

490
CHAPTER 12 Digital Video Compression
Conditions for invertibility, simply stated, are “if the transformation were painted
on a rubber sheet ... a good compensation algorithm should distort the rubber but
not fold it” [23]. This effectively precludes covered/uncovered regions. The motion
estimation methods used were full-search block matching with a 8 × 8 block and
16 × 16 search area, and hierarchical block matching with a 7 × 7 block and 3 × 3
search at each level. Motion was inverted by simple negation of displacement vectors.
Ohm’s SWT coder MC-SBC [24] featured a motion-compensated temporal ﬁl-
ter (MCTF), a type of 3-D subband/wavelet ﬁlter that attempts to ﬁlter along the
local motion paths. For the temporal ﬁltering, Ohm made use of the two-tap Haar
transform. Due to the fact that MCTF is nonrecursive in time, we can expect lim-
ited error propagation and importantly no temporal drift in scalable applications. A
four-temporal level MCTF with GOP = 16 is shown in Figure 12.4–4. This MCTF
uses the Haar ﬁlter, with motion ﬁelds between successive pairs of frames indicated
with curved arcs. Proceeding down the diagram, we go to lower temporal levels or
frame rates by a factor of 2 each time. The lowest frame rate here is 1/16th of the full
frame rate. The numbers in parentheses denote the temporal levels, with 1 being the
lowest and 5, or full frame rate, being the highest. Note that the motion vectors are
numbered with the temporal levels that they serve to reconstruct. In Figure 12.4–4,
we see motion vectors represented as rightward arrows, terminating at the location
of t −H frames. Different with the now somewhat old forward and backward MC
terminology used in the hybrid coding, in MCTF parlance, this is called forward
motion compensation, with backward motion compensation being reserved for the
case where the arrows point to the left. In both cases, the motion vectors must be
transmitted.
A corresponding three-level diagram for the LGT 5/3 ﬁlter is shown in
Figure 12.4–5, where we notice that twice as much motion information is needed
(3) 
(4) 
(4) 
(4) 
(4) 
(5) 
(5) 
(5) 
(5) 
(5) 
(5) 
(5) 
(5) 
(3) 
(4) 
(4) 
(4) 
(4) 
(5) 
(5) 
(5) 
(5) 
(3) 
(3) 
(5) 
(5) 
(5) 
(5) 
(1) 
(2) 
(2) 
t−L1
t−L4
t −L3
t−L2
A
B
B
B
A
B
B
A
A
A
...
FIGURE 12.4–4
Illustration of four-level Haar ﬁlter MCTF.

12.4 Interframe SWT Coders
491
t−L1
t −L3
t−L2
A
B
B
B
A
B
A
A
FIGURE 12.4–5
Illustration of LGT 5/3 ﬁlter MCTF.
as in the Haar ﬁlter MCTF. Also note that the GOP concept is blurred by the refer-
ences going outside the set of eight frames. So for 5/3 and longer SWT ﬁlter sets, it
is better to talk of levels of the MCTF than the GOP size.
In equations, we can write the lifting steps, at each temporal level, for each of
these cases as follows:
Haar case:
h(n) = xA(n) −xB(n −df ),
(12.4–1)
l(n) = xB(n) + 1
2h(n + df ).
(12.4–2)
LGT 5/3 case:
h(n) = xA(n) −1
2

xBf (n −df ) + xBr(n −dr)

,
(12.4–3)
l(n) = xB(n) + 1
4

hf (n + df ) + hr(n + dr)

.
(12.4–4)
In these equations, we have labeled the frames in terms of A and B, as shown
in Figures 12.4–4 and 12.4–5, and also used df and dr to refer to the forward and
backward motion vectors, from frame A to reference frames B on either side (i.e., Bf
and Br), where f stands for forward and r stands for reverse. These operations are then
repeated at the second and subsequent levels of decomposition. We also notice that
backward or reverse motion vectors (i.e., d = −df ) are used in the update step for the
LGT 5/3 lifting implementation. (This is also done in the Haar case.) Finally, in the
subpixel accurate case, many of the terms in these two equations must be computed
by interpolation. The beauty of the lifting implementation, however, is that this is

492
CHAPTER 12 Digital Video Compression
exactly invertible in the corresponding SWT synthesis operation, so long as the same
interpolation method is used.
Example 12.4–1: Haar MCTF with Four Levels
With respect to Figure 12.4–4, note that frame t −L4 is an intraframe to be coded and sent
every 16 frames. Hence there is a natural temporal block size of 16 here in this four-level
Haar case. So we say GOP = 16 here. The t −L4 frame subsequence has been ﬁltered
along the motion trajectory prior to being temporally subsampled by 16. In addition to the
t −L4 sequence, we must send the t −H subbands at levels 1 through 4, for a total of
16 frames per block. Of course, it is also necessary to transmit all the motion vectors d.
One issue with the MCTF approach is delay. A four-stage MCTF decomposition
requires 16 frames, which may be too much delay for some applications, such as
visual conferencing. However, one can combine the nonrecursive MCTF with the
hybrid coder by limiting the number of MCTF stages and then applying displaced
frame difference (DFD) prediction (i.e., a hybrid coder such as MPEGx) to the t −Lk
subband. In a video conferencing scenario, one may want to limit this kind of built-in
algorithmic delay to 100 msec, which implies a frame rate greater than or equal to
10 fps. At this frame rate, no stages of MCTF could be allowed. If we raise the frame
rate to 20 fps, then we can have one stage. At 50 fps, we can have two stages, etc., and
still meet the overall goal of 100-msec algorithmic delay. So as the frame rate goes up,
the number of MCTF stages can grow too, even in the visual conferencing scenario.
Of course, in streaming video, there is no problem with these small amounts of delay,
and one can use ﬁve and six stages of MCTF with no difﬁculty due to algorithmic
delay.
The Choi and Woods coder [25] had two levels of MCTF with two-tap Haar ﬁl-
ters and spatial wavelet analysis with Daubechies D4 ﬁlters and variable block-size
motion compensation. This was followed by UTQs with adaptive arithmetic coding
and prediction based on subband ﬁnite-state scalar quantization (FSSQ) coding tech-
nique [26]. An optimized rate allocation was performed across 3-D subbands using
the Lagrangian method. The resulting coding performance in terms of PSNR at 1.2
Mbps is shown in Table 12.4–1 for various CIF test clips.
Table 12.4–1 Choi and Woods’ Coder
Results at 1.2 Mbps
Sequence
Y(dB)
U(dB)
V(dB)
Mobile
27.0
31.2
31.4
Table Tennis
33.8
39.4
39.8
Flower Garden
27.6
32.6
30.8

12.5 Scalable Video Coders
493
12.5 SCALABLE VIDEO CODERS
In many applications of video compression, it is not known to the encoder what res-
olutions, frame rates, and/or qualities (bitrates) are needed at the receivers. Further,
in a streaming or broadcast application, different values of these key video param-
eters may be needed across the communication paths or links. Scalable coders [27]
have been advanced to solve this problem without the need for complicated and lossy
transcoding.
Deﬁnition 12.5–1: Scalability
One coded video bitstream that can be efﬁciently broken up for use at many spatial
resolutions (image sizes), frame rates, regions of interest, and bitrates.
A scalable bitstream that has all four types of scalability will be referred to as
fully scalable. We can think of region of interest capability as a restricted type of
object-based scalability that can, together with resolution scalability, support zoom
and pan functionality. This capability can support browsing, wherein a small version
of a high-resolution image may be “zoomed and panned” to locate interesting parts
for closer looks at higher resolution.
Scalability is needed in several areas, including digital television, heterogeneous
computer networks, database browsing, matching of various display formats (to
adjust to window size on screen), matching of various display frame rates, loading
of a VOD server, etc. Scalability in database browsing facilitates efﬁcient pyramid
search of image and video databases. Scalability on heterogeneous networks can
enable a network manager to do dynamic load control to match link capacities as
well as terminal computer capabilities. A scalable encoder can also better match a
variable carrier-to-noise ratio (CNR) on wireless channels.
The early standard coders H.263 and MPEG 2 have a limited amount of scala-
bility, usually just one or two levels. For spatial scalability, MPEG 2 uses a pyramid
coding method, where the base layer is coded conventionally, and then an enhance-
ment layer supplements this base layer for higher resolution, frame rate, or quality.
Figure 12.5–1 shows an MPEG 2 coder targeted for spatial scalability in HD and SD
television.
Separate spatial and temporal scalability proﬁles, with two or three levels only,
were standardized in MPEG 2, but are seldom used in practice. The limitations of
these scalable proﬁles are lack of data conservation, limited range of scalability, cod-
ing errors not limited to the baseband, and drift for the frequency scalable coder (up
to 7 dB over 12 frames [28]).
In the research area, Burt and Adelson [29] introduced a scalable pyramid coder
with the desirable quantizer-in-loop property that can frequency shape (roll-off) lower
resolution layers. Unfortunately, due to use of Gaussian-Laplacian lowpass pyramid,
there is lack of data conservation. Also due to the pyramid based coding-and-recoding

494
CHAPTER 12 Digital Video Compression
structure, the coding noise and artifacts spread from low (baseband) to higher
spatial frequencies. Naveen’s multiresolution SWT coder [19, 30], as sketched in
Figure 12.4–3, can be scalable in resolution with three spatial levels. There is no drift
at lower resolution because the same motion information is used at the encoder as at
the decoder. It uses efﬁcient hierarchical MC that can incorporate rate constraints and
frequency roll-off to make the lower resolution frames more videolike (i.e., reduce
their spatial high frequencies). Table 12.5–1 gives some average luma PSNR values
for HD test clips MIT and Surfside, obtained using forward MC in this coder.
The Taubman and Zakhor [31] MCTF algorithm featured global pan-vector
motion compensation, layered (embedded) quantization, and conditional zero cod-
ing to facilitate SNR scalability. The published PSNR results for two common CIF
test clips at 1.5 Mbps, coding only the luma (Y) component are shown in Table 12.5–
2. Results were good on panning motion clips but less efﬁcient on clips with detailed
local motion. Also provided is an MPEG 1 result for comparison.
Woods and Lilienﬁeld [32] published results on a resolution scalable coder
(RSC), which performed spatial subband analysis ﬁrst, and then one stage of MCTF
at each resolution. RSC then applied the hybrid technique of DFD on the lowest spa-
tiotemporal level to yield three spatial resolutions and two frame rates. The approach
was nonembedded in bitstream and used UTQs based on the generalized Gaussian
MPEG 2
coder
MPEG 2
decode
Upsample
and smooth
Filter and
decimate
MPEG 2
coder
Input
video
Residual
SD level
HD
level
+
−
Σ
FIGURE 12.5–1
Illustration of resolution scalability for SD and HD video using MPEG 2.
Table 12.5–1 PSNR Results—Forward Motion
Compensation
PSNR (dB)
MIT (0.5 bpp)
Surfside (0.25 bpp)
Low res.
38.0
42.2
Medium
33.5
41.6
High res.
34.3
42.5
High (not scal.)
35.0
42.8

12.5 Scalable Video Coders
495
distribution together with multistage quantizers to achieve both resolution and frame-
rate scalability. The RSC coder is of the type “2-D+t” since it ﬁrst performs a spatial
subband/wavelet pyramid on the incoming data in Figure 12.5–2. The various spa-
tial levels are then fed into stages of motion estimation and MCTF, with outputs
shown on the right, consisting of temporal low Li and high Hi frames. Referring to
Figure 12.5–2, the low frame rate video hierarchy is given as Vi0, and the high rate
hierarchy, denoted Vi1, is obtained by the addition of the corresponding Hi frame.
Average PSNR results for the luma channel are given in Table 12.5–3, where each
channel or spatiotemporal resolution was coded at 0.35 bpp. We see that the high-
frequency roll-off is moderately successful at keeping the PSNR relatively constant
across the levels. Without such roll-off, the low resolution frames appear excessively
sharp and may contain alias error.
Hsiang and Woods [33] presented an MCTF scalable video coder, wherein
an invertible motion-compensated 3-D SWT ﬁlter bank was utilized for video
analysis/synthesis. The efﬁcient embedded zero-block image coding scheme EZBC
[34] was extended for use in coding the video subbands. Comparison on Mobile in
CIF format to a commonly available MPEG 2 coder showed PSNR improvements
ranging from 1 to 3 dB over a broad range of bitrates for this scalable coder.
Table 12.5–2 Motion Compensation
via Global Pan Vector at 1.5 Mbps
PSNR (dB)
MPEG 1
Highly Scalable
Football
33.6
34.6
Table Tennis
32.8
34.6
Temporal analysis
Spatial analysis
Low-temporal subbands
High-temporal subbands
V20
V00
V10
MCTF
MCTF
MCTF
L2
L1
L0
H2
H1
H0
Motion
estimation
Motion
estimation
Motion
estimation
Motion
estimation
V21
(Original)
V11
V01
FIGURE 12.5–2
RSC demonstration software: three resolutions and two frame rates ( c⃝IEEE).

496
CHAPTER 12 Digital Video Compression
Table 12.5–3 RSC Average PSNR (dB) on HD
Test Clips Mall and MIT
Low frame rate
High frame rate
V00
V10
V20
V01
V11
V21
video level
PSNR (dB)
Mall
w/o roll-off
33.8
37.3
42.3
34.5
37.0
40.7
with roll-off
36.0
39.4
42.2
37.2
39.5
40.6
MIT
w/o roll-off
30.4
28.7
32.1
31.9
31.1
33.2
with roll-off
33.1
31.7
32.0
34.8
34.1
33.2
B: Frame 2
(b −a)/2
(b −a)/2
High
Low
(a) Input frames
: Connected pixel
: Unconnected pixel
(b) Temporal SWT output
A: Frame 1
B
A
(b + a)/2
a
t
t1
t2
FIGURE 12.5–3
Illustration of MCTF using Haar ﬁlters.
More on MCTF
MCTF plays a very important role in interframe SWT coding. In general, we try
to concentrate the energy onto the temporal low subband, with the energy in the
temporal high subbands thus as small as possible. But covered and uncovered pixels
must be considered in achieving this goal. With reference to the Haar ﬁlter MCTF
shown in Figure 12.5–3(a), let frame A be the reference frame; then some pixels in
frame B will be uncovered pixels, meaning they will have no reference in frame A.
In Figure 12.5–3(b) we show the resulting t −L and t −H frames, as computed in
place, with the horizontal lines indicating a smallest block-matching size of 4 × 4.
We see many connected pixels, but some in frame A are unconnected. They are either
covered in frame B or else are just not the best match. The latter situation can happen

12.5 Scalable Video Coders
497
due to expansion and contraction of motion ﬁelds as objects move toward or away
from the camera.
We can see pixels in Figure 12.5–3 classiﬁed as connected and unconnected. If
there is a one-to-one connection between two pixels, they are classiﬁed as connected
pixels. If several pixels in frame A connect to the same pixel in frame B, only one
of them is classiﬁed as the connected pixel; the others are declared unconnected.
These unconnected pixels in frame B are indicated by no reference between frame
A and B. After the classiﬁcation, we can perform the actual MC ﬁltering on the
connected pixels. For the unconnected pixels in frame B, their scaled original values
are inserted into the temporal low subband without creating any problem. For the
unconnected pixels in frame A, a scaled DFD can be substituted into the temporal
high subband [35], if there is a pixel in frame B or some earlier or later frame pair4
that has a good (but maybe not best) match. Otherwise, some spatial or intraframe
prediction/interpolation can be used, analogous to an I block in MPEG-coded video.
Detection of Covered Pixels
In order to make the covered pixels correspond to the real occurrence of the occlusion
effect, we can ﬁrst use a detection phase before the actual MC ﬁltering. Here is a
simple pixel-based algorithm to ﬁnd the true covered pixels illustrated for the Haar
MCTF case.
Covered Pixel Detection Algorithm
There are four steps, as illustrated in Figure 12.5–4, for the Haar MCTF case:
Step 1. Do backward motion estimation from frame B to frame A, in a frame pair.
Step 2. Get the state of connection of each pixel in frame A. There are three states:
Unreferred: a pixel that is not used as reference.
Uni-connected: a pixel that is used as reference by only one pixel in frame B.
Multi-connected: a pixel that is used as reference by more than one pixel in
the current frame.
A multi-connected pixel in frame A has several corresponding pixels in frame
B, so we compute the absolute DFD value with each of them and just keep the
minimum as uni-connected.
Step 3. Get the state of connection of each pixel in frame B. There are just two
states:
Uni-connected: Here we have three cases:
Case 1: a pixel whose reference in frame A is uni-connected.
Case 2: a pixel whose reference in frame A is multi-connected; except if
its absolute DFD value with the reference pixel is the only minimum, we
declare it uni-connected.
4Remember, this is all feed-forward, so any future reference only incurs a small additional delay at
the transmitter and receiver. Of course, this could be a problem in some applications, such as visual
conferencing.

498
CHAPTER 12 Digital Video Compression
B
A
: A special case of
  uni-connected pixels 
: Uni-connected pixel
: Multi-connected pixel
: Unreferred pixel
B: Second frame
A: First frame
Backward motion estimation
t
t1
t2
FIGURE 12.5–4
Illustration of the covered pixels.
Case 3: if there are several pixels in frame B pointing to the same reference
pixel, and having the same minimum absolute DFD value, we settle this
tie using the scan order.
Multi-connected: remaining pixels in frame B.
Step 4. If more than half of the pixels in a 4 × 4 block of frame B are multi-
connected, we try forward motion estimation. If motion estimation in this direction
has smaller MCP error, we call this block a covered block and pixels in this block
are said to be covered pixels. We then use forward MCP to code this block. We
thus have created a bidirectional motion ﬁeld.
In Figure 12.5–5, we see the detections by this algorithm of the covered and
uncovered regions, mainly on the side of the tree, but also at some image boundaries.
With reference to Figure 12.5–6, we see quite strange motion vectors obtained around
the sides of the tree as it “moves” due to the camera panning motion in ﬂower garden.
The bidirectional motion ﬁeld is quite different, as seen in Figure 12.5–7, where the
motion vector arrows generally look much more like the real panning motion in this
scene. These ﬁgures were obtained from the MCTF operating two temporal levels
down—that is, with a frame separation of four times that which is normal.
Bidirectional MCTF
Using the covered pixel detection algorithm, the detected covered and uncovered
pixels are more consistent with the true occurrence of the occlusion effect. We can

12.5 Scalable Video Coders
499
FIGURE 12.5–5
Example of unconnected blocks detected in ﬂower garden.
FIGURE 12.5–6
Example of unidirectional motion ﬁeld.
now describe a bidirectional MCTF process. We ﬁrst get backward motion vectors for
all 4 × 4 blocks via HVSBM. The motion vectors typically have quarter- or eighth-
pixel accuracy, and we use the MAE block-matching criterion. Then we ﬁnd covered
blocks in frame A using the covered pixel detection algorithm. The forward motion
estimation is a by-product of the process of ﬁnding the covered blocks. Then we

500
CHAPTER 12 Digital Video Compression
FIGURE 12.5–7
Example of bidirectional motion ﬁeld.
A
A
Current frame pair
B
B
Connected pixels
Covered pixels
FIGURE 12.5–8
Illustration of bidirectional interpolation in the context of two-tap Haar MCTF.
do optimal tree pruning [35], and thus we realize a bidirectional variable block size
motion compensation.
Figure 12.5–8 shows a variation on the bidirectional MCTF, where we ﬁrst choose
forward motion compensation, and then resort to backward motion compensation
only for detected covered blocks. The shaded block in frame A is covered. For those
covered pixels, we perform MCP. For the other regions, the MCTF is similar to that
in [35].
In either case, at GOP boundaries, we have to decide whether to use open or
closed GOP, depending on delay and complexity issues.

12.5 Scalable Video Coders
501
FIGURE 12.5–9
A frame output from unidirectional MCTF at four temporal levels down.
FIGURE 12.5–10
Output of bidirectional MCTF at four temporal levels down.
We show in Figure 12.5–9 a t −L4 output from a unidirectional MCTF at tempo-
ral level four (i.e., one-sixteenth of the original frame rate). There are a lot of artifacts
evident, especially in the tree. Figure 12.5–10 shows the corresponding t −L4 output

502
CHAPTER 12 Digital Video Compression
from a bidirectional MCTF; we can detect a small amount of blurring but no large
artifact areas as produced by the unidirectional MCTF.
Some MC-EZBC experimental coding results using the bidirectional Haar MCTF
are contained in Chen and Woods [36] for the CIF test clips mobile, ﬂower garden, and
coast guard. Some MC-EZBC results in coding of 2K digital cinema data are contained
in the MCTF video coding review article [37]. Video comparisons of MC-EZBC to
both H.264/AVC and MPEG 2 results are available at this book’s Web site.
Enhanced MC-EZBC
An enhanced version of MC-EZBC appeared in Wu et al. [38] that combined an
adaptive LGT/Haar MCTF [39] with scalable motion vector coding and OBMC. The
adaptive MCTF is shown in Figure 12.5–11, borrowed from [38]. It shows across the
top an incoming video, broken into two frame phases, denoted A and B. The more
efﬁcient 5/3 LGT ﬁlter is preferred, but the two-tap Haar ﬁlter is used at scene change
and when the motion compensation error is large. We can see the adaptive 5/3 MCTF
as a generalization of the bidirectional Haar MCTF covered above.
A Lagrange-based comparison is made of eight different coding modes, weighting
both MC error and MV rate on a block-by-block basis. A coding result from this
Video sequence
1
1
1
1
1
1
Highpass
sequence
IMC
IMC
IMC
IMC
IMC
Lowpass
sequence
Scene change
Lt−3
Ht−2
Ht
At −3
Bt−2
At−1
 
Bt
At +1
Bt +2
At+3
Lt−1
Lt+1
Lt+3
Ht+2
....
....
....
....
....
....
IMC
IMC
MC
MC
MC
MC
MC
MC
MC
1
4
1
4
1
4
1
4
1
2
1
2
−
1
2
−
1
2
−
1
2
−
−1
FIGURE 12.5–11
Adaptive LGT/Haar MCTF. (Figure 1 from [38] c⃝IEEE 2008)

12.6 Current Interframe Coding Standards
503
1200
1000
Bitrate(kbps)
800
600
400
200
PSNR (Y)
24
26
28
30
32
34
EBCOT-SMV
MC-EZBC (4 level)
MC-EZBC (3 level)
MSRA-Codec
Enh MC-EZBC
FIGURE 12.5–12
A coding comparison.
work is shown in Figure 12.5–12 for the PSNR(Y) of the test clip Bus at full (CIF)
resolution and full (30 fps) frame rate. Two curves are plotted; one for four temporal
levels of MCTF and a lower performance curve for three temporal levels. Also plotted
for comparison purposes are two other transform coders. The one labeled MSRA-
Codec is taken from [40], and the EBCOT-SMV result with three temporal levels
is taken from [41]. Also shown in [38] is a comparison with an early version of
H.264/AVC that shows a rough parity of performance in terms of luminance PSNR.
12.6 CURRENT INTERFRAME CODING STANDARDS
In this section we discuss the modern coding standards developed jointly by MPEG
and the Video Coding Experts Group (VCEG), collectively denoted as H.264/xxx.
The ﬁrst one, H.264/AVC, is for nonscalable coding of video and achieves a 50%
bitrate savings over MPEG 2. The second one, H.264/SVC, is for scalable video
coding and claims a 10% penalty on average for providing scalability. The last,
H.264/MVC, is for stereo and other multiview applications, where MVC stands for
multiview coding; it has been released most recently and is an area still currently
under development by these standards groups.

504
CHAPTER 12 Digital Video Compression
H.264/AVC
Research on increasing coding efﬁciency continued through the late 1990s, and
it was found that up to a 50% increase in coding efﬁciency could be obtained by
various improvements to the basic hybrid coding approach of MPEG 2. Instead of
using one hypothesis for the motion estimation, multiple hypotheses from multiple
locations in previous frames could be used [42] together with a Lagrangian approach
to allocate the bits. By 2001 the video standards groups VCEG at ITU, and MPEG
ISO convened a joint video team (JVT) to work on the new standard, called H.264
by ITU/VCEG and Advanced Video Coder (AVC) by the ISO/MPEG. The resulting
H.264/AVC coder is also sometimes referred to as MPEG 4, Part 10 and is illustrated
in Figure 12.6–1, where we see that more frame memory to store past frames has
been added to the basic hybrid coder of Figure 12.2–2. The H.264/AVC coder is free
to choose any of the frames in its multi-frame memory, termed the decoded picture
buffer (DPB), to perform the prediction. We also see the addition of a loop ﬁlter that
serves to smooth out blocking artifacts. Further, before the intra transform, there is
intra- or directional spatial prediction that improves coding efﬁciency, but introduces
the need to switch between these new intra modes, as well as the interprediction
modes.
There are many new ideas in H.264/AVC, which allow it to perform at twice the
efﬁciency of the MPEG 2 standard, also known as H.262. There is a new variable
block size motion estimation with block sizes ranging from 16 × 16 down to 4 × 4,
and motion vector accuracy raised to quarter pixel from the half-pixel accuracy of
MPEG 2. The permitted block size choices are shown in Figure 12.6–2. The 16 × 16
x(n1, n2, n)
e(n1, n2, n)
+
+
−
Σ
Motion
estimator
x(n1, n2, n)
^
MC
residual
e(n1, n2, n)
~
x(n1, n2, n)
~
Intratransform
Loop
filter
Motion
vectors
Inverse
intratransform
Frame
memories
Intrapredict
Interpredict
FIGURE 12.6–1
A system diagram of the H.264/AVC coder.

12.6 Current Interframe Coding Standards
505
16×16
16 × 8
8 ×16
8 ×8
4 ×4
4× 8
8 ×4
8×8
FIGURE 12.6–2
Allowed MV block sizes in H.264/AVC.
macroblock can be split in three ways to get 16 × 8, 8 × 16, or 8 × 8, where by 16 × 8
splitting we mean split into two 16 × 8 submacroblocks, as shown. If the predic-
tion accuracy of 8 × 8 blocks is not enough, one more round of such splitting ﬁnally
results in the submacroblocks 8 × 4, 4 × 8, or 4 × 4. Note that in addition to what
we would get by quadtree splitting (see Chapter 11), we get the possible rectangular
blocks, which can be thought of as a level inserted between two quadtree block levels.
To match this smallest block size, a 4 × 4 integer-based transform that is DCT-like
is introduced, and separable based on the 1-D four-point transform
H =


1
1
1
1
2
1
−1
−2
1
−1
−1
1
1
−2
2
−1

.
(12.6–1)
The H.264/AVC coder is based on slices, with I, B, and P slices, as well as two new
switching slices SP and SI. The slices are in turn made up of 16 × 16 macroblocks.
The P slice can have I or P macroblocks. The B slice can have I, B, or P macroblocks.
There is no mention of GOP, but there are I pictures, needed at the start to initialize
this hybrid coder. Of course, there is nothing to prohibit a slice from being the size of
a whole frame, so that there can effectively be P and B frames as well.
In H.264/AVC, an I slice is deﬁned as one whose macroblocks are all intracoded.
A P slice has macroblocks that can be predictively coded with up to one motion vector
per block. A B slice has macroblocks predictively (interpolatively) coded using up to
two motion vectors per block. Additionally, there are new switching slices SP and
SI that permit efﬁcient jumping from place to place within or across bitstreams (cf.
Section 13.3).
Within an I slice, there is intrapicture prediction, done blockwise based on pre-
viously coded blocks. The intraprediction block error is then subject to the 4 × 4
integer-based transform and quantization. The intraprediction is adaptive and direc-
tional based for 4 × 4 blocks, as indicated in Figure 12.6–3. A prespeciﬁed ﬁxed
blending of the available boundary values is tried in each of the eight prediction
directions. Also available is a so-called DC option that predicts the whole block as a

506
CHAPTER 12 Digital Video Compression
constant. For 16 × 16 blocks, only four intraprediction modes are possible: vertical,
horizontal, DC, and plane, the last one coding the values of a best-ﬁt plane for the
macroblock. The motion compensation can use multiple references, so for example,
a block in a P slice can be predicted by one to four reference blocks in earlier frames.
The standard speciﬁes the amount of reference frame storage in the DPB, that must
be available to store these past pictures.
Figure 12.6–4 illustrates the comparative PSNR versus bitrate performance of the
test models of some of the MPEG/VCEG coders on the 15 fps CIF test clip Tempete.
The ﬁgure is from Sullivan and Wiegand’s review article [43] and shows consider-
able improvement versus MPEG 2, of about a factor of two increased compression
xx
x(1,1)
x(2,2)
x(1,2)
x(2,1)
xbc(0,3)
xbc(0,2)
xbc(0,1)
xbc(0,0)
xbc(1,0) xbc(2,0) xbc(3,0)
. . .
...
...
. . .
. . .
...
. . .
8 prediction directions
FIGURE 12.6–3
Illustration of directional prediction modes of H.264/AVC in the case of 4 × 4 blocks.
Bitrates [kbit/s]
25
0
256
512
768
1024
1280
1536
26
27
28
29
30
31
32
33
34
35
36
37
Y-PSNR [dB]
MPEG-2 
H.263 HLP
MPEG-4 ASP
H.264/AVC MP
FIGURE 12.6–4
PSNR vs. bitrate for the 15 fps CIF test clip Tempete. ( c⃝IEEE 2005.)

12.6 Current Interframe Coding Standards
507
at ﬁxed PSNR. This improvement in efﬁciency is largely due to the greater exploita-
tion of motion information made possible by the revolutionary increases in affordable
computational power of the last 10 years. More information on the H.264/AVC stan-
dard is available in the review article by Wiegand et al. [44] that introduces a special
journal issue on this topic [45].
Video Coder Mode Control
A video coder like H.264/AVC has many coder modes to be determined. For each
macroblock, there is the decision of inter or intra, quantizer step size, and motion
vector and block modes. Each choice has a different number of coded bits for the
macroblock. We can write the relevant Lagrangian form for block k as
Jk ≜Dk(bk,mk,Qs) + λmodeRk(bk,mk,Qs),
(12.6–2)
where Dk and Rk are the corresponding mean-square distortion and rate for block bk
when coding with mode mk and Qs is the quantizer step size. Here, Rk must include
the quantizer bits for coding the block plus estimated bits for the motion vector(s) and
mode decisions. For the moment, assume that the motion vectors have been deter-
mined prior to this optimization. Then we can sum over all the blocks in a frame to
get the total distortion and rate for that frame
J = D + λmodeR
=
X
k
{Dk(bk,mk,Qs) + λmodeRk(bk,mk,Qs)}
=
X
k
Jk,
where D and R are the total distortion and rate. Normally the blocks in a frame are
scanned in the 2-D raster manner (i.e., left to right and then top to bottom). The joint
optimization of all the modes and Qs values for these blocks is a daunting task. In
practical coders, often what optimization there is, is done macroblock by macroblock,
wherein the best choice of mode mk and Qs is done for block bk conditional on the
past of the frame in the NSHP sense, resulting in at most a stepwise optimal result.
We can achieve this stepwise or greedy optimal result by evaluating Jk in (12.6–2)
for all the modes mk and quantizer step sizes Qs and then choosing the lowest value.
This point will then be on the optimized operational D(R) curve for some rate Rk.
To generate the entire curve, we would then sweep through the parameter λmode.
This is still too much computation for a practical coder, so in an exhaustive set of
experiments, Wiegand and Girod [46] searched through λmode values for a wide range
of Q values and came up with the following equation that is best in an average sense:
λmode = cQ2
s,

508
CHAPTER 12 Digital Video Compression
with the value c = 0.85. This work, ﬁrst done on a test model for H.263, then
becomes: Choose the mode mk such that
mk = argmin
k
n
Dk(bk,mk,Qs) + cQ2
sRk(bk,mk,Qs)
o
.
In the case of inter mode, the motion vector must also be determined, which is done
using a similar Lagrangian form with a measure of DFD error in place of coding
distortion and also motion vector rate in place of total rate [42, 46]. If MSE is used
as the distortion then the same lambda value is recommended (i.e., λmotion = λmode),
but if MAE is used, then λmotion = √λmode is used.
When these experiments were redone for the H.264/AVC test model, with its
larger set of coding and motion modes, a similar formula emerged. However, since
the quantizer parameter Qp was now deﬁned logarithmically in the step size, the
formula that emerged [47] was
λmode = c2(Qp−12)/3.
In both cases, this operational rate-distortion optimization at constant Qp, results
in a sequence of macroblocks approximately optimized in the so-called constant-
slope sense. To actually get a CBR-coded sequence, a further optimization (called
simply rate control in H.264/AVC) has to be applied to decide what value of Qp to
use for each macroblock. An easy VBR case results from the choice of constant Qp,
but then the total bitrate is unconstrained and a search over Qp must be done to get
the target bitrate at the end of the frame, GOP, or movie. Also in practice, different
Qp values are used for I, P, and B slices or frames. Often this choice is ﬁxed, with
step size increasing in a predetermined manner (usually +1 or +2) as we move from
I to P to B frames. Recent frame-by-frame optimization results for H.264/AVC have
been reported in [48], which show a 6–12% decrease in bitrate on QCIF sequences
versus the test model software JM8.2 of the joint video team [49].
Network Adaptation
In H.264/AVC, there is a separation of the video coding layer (VCL), which we
have been discussing, from the actual transport bitstream (TS). As in MPEG 4 video,
H.264/AVC is intended to be carried on many different networks with different packet
or cell sizes and qualities of service. So MPEG simply provides a network abstraction
layer (NAL) speciﬁcation about how to adapt the output of the VCL to any of several
transport streams. There is much more on network video in Chapter 13, where we
further discuss some basic issues in network video. More on H.264/AVC is found in
Chapter 10 of The Essential Guide to Video Processing [50].
H.264/SVC
In 2007 the ISO and ITU, via their MPEG and VCEG committees, issued an amend-
ment to H.264/AVC for scalable coding. The new Scalable Video Coder (SVC) is a

12.6 Current Interframe Coding Standards
509
Spatially
decimate
Input
video
Layer 0
Layer 1 
MC and
intraprediction
Multiplex
MC and
Intraprediction
Base layer
coding  
Base layer
coding 
Texture
Motion
Texture
Motion
SNR scalable
coding 
Interlayer
prediction 
SNR scalable
coding  
Scalable
bitstream 
FIGURE 12.6–5
MPEG Scalable Video Coder.
layered scalable coder that has an H.264/AVC base layer and uses many of the AVC
tools in the enhancement layers [51]. A simpliﬁed diagram in Figure 12.6–5 shows a
two-layer spatial scalability version of SVC. Layer 0 is a conventional H.264/AVC
coder with the addition of SNR or quality scalability of its texture (residual) output
and the provision of so-called interlayer prediction quantities to help the Layer 1
(enhancement layer) coder.
Comparing to the modes possible in AVC, here we have additional modes
for possible prediction from the present value of the reference layer, Layer 0 in
Figure 12.6–5. There are two interlayer prediction modes: interlayer texture pre-
diction, available only when the reference macroblock is completely intramode,
and interlayer residual prediction, only available when the reference block is com-
pletely in intermode. Since the quantized version of this Layer 0 residual can be
decoded directly at the receiver, it is not necessary to run a separate motion com-
pensation loop when decoding Layer 1. Both interlayer predictions are upsampled
with H.264/SVC speciﬁed ﬁlters. The interlayer texture prediction is directly used as
the Layer 1 predictor, but the inter-layer residual prediction is added to the Layer 1
motion-compensated prediction to form the Layer 1 prediction.
Another key concept in H.264/SVC is the use of hierarchical B frames, as illus-
trated in Figure 12.6–6 showing a temporal scalability version of SVC. We see the
example with a GOP of length 8 starting and ending on an I frame, the I frame subse-
quence thus at 8 times lower frame rate, termed temporal level T0. Two neighboring
I frames are then used to predict (interpolate) a B frame in the middle marked B1,
thus creating a sequence at four times lower than the full frame rate, called temporal
level T1. Then B2 frames are created by interpolation at two frames distance, creat-
ing the T2 temporal level. Continuing this procedure one more time, we get the full
frame rate, T3 temporal level. We now have a hierarchy of B frames with coding

510
CHAPTER 12 Digital Video Compression
I
I
B1
B2
B3
B3
8-frame GOP
Hierarchical B frames—four temporal levels 0, 1, 2, and 3
B3
B3
B2
FIGURE 12.6–6
Hierarchical B frame concept.
order indicated by their numbers. Interestingly, this hierarchy of B frames is identical
to what happens in a 5/3 MCTF if we omit the update terms [37]. Such hierarchical
interpolation structures have been found to be very efﬁcient and are consistent with
the general reference frame structure of H.264/AVC where they can offer more than
1-dB advantage over IBBP ... type structures [52] for CIF-sized test clips at 30 fps
and GOP size of 16. The drawback, though, is a structural or algorithmic delay that
varies within the GOP, and in the case of Figure 12.6–6 is seven frames for the second
picture (frame) in the GOP. Lower structural delays are possible by limiting the use
of forward prediction, but at a penalty in performance.
The use of an I frame every 8 frames is rather inefﬁcient but permits high ran-
dom access. A more efﬁcient structure would combine hierarchical B frames with
P frames, as shown in Figure 12.6–7.5 The spatial and temporal scalability can be
combined to produce a spatiotemporal scalable version of SVC, such as QCIF at 15
fps and CIF at 30 fps. Results are shown for this scenario for the Football test clip
in Figure 12.6–8 showing the SVC results obtained with the JSVM software v. 9 1
along with single-layer results of AVC and the simulcast solution—that is, the solu-
tion when two separate single layers are sent, one for each spatiotemporal resolution.
In this ﬁgure, just the results for the CIF layer are plotted, since the QCIF base layer
is coded by an H.264/AVC compatible coder. Hierarchical B frames are used here for
all coders, as they had been found to be more efﬁcient. The plotted points here corre-
spond to separate encoder runs and are connected together for plotting purposes only.
We see that the SVC result is above the midpoint between single layer and simulcast.
In later work, joint optimization was tried [53] to produce an SVC coder more bal-
anced between base and enhancement layers, with results shown in [54]. This review
5In connection with these two ﬁgures on hierarchical B frames, it is important to note that for
H.264/AVC style coders, these diagrams do not indicate the frames used in prediction since multiple
frames can be used from the already-coded frame memory. The ﬁgures do indicate the highest temporal
level of the frames that can be used though—i.e., all already-coded frames lower than the present one.

12.6 Current Interframe Coding Standards
511
I
P
B1
B2
B3
B3
Hierarchical B frames with P frame
B3
B3
B2
8-frame GOP
FIGURE 12.6–7
More efﬁcient hierarchical B frame structure with GOP size 8.
Football
37.5
37
36.5
36
35.5
34.5
33.5
34
33
600
800
1000
1200
1400
1600
1800
Bitrate (kbit/s)
PSNR (dB)
35
SVC (JSVM)
Simulcast
Single layer
FIGURE 12.6–8
Reproduced Figure 6b from [55], c⃝IEEE 2007.
article shows approximately a 10% penalty in rate for H.264/SVC over single layer
H.264/AVC for spatial scalability, as seen in Figure 12.6–9 taken from [54]. Since
we have a joint R-D optimization over base and enhancement layers, the base layer
is compromised somewhat too, with both SVC performances approximately 10%
worse than the single-level encoding with H.264/AVC. While this result is shown for
Soccer, similar results hold for the other test sequences. Again, here the individual
dots represent different encoder runs (i.e., quality scalability is not shown). A result

512
CHAPTER 12 Digital Video Compression
42
41
40
39
38
37
36
35
34
33
32
0
Y-PSNR [dB]
1,000
2,000
Bit rate [kb/s]
Soccer, CIF 30Hz and QCIF 30 Hz
3,000
4,000
CIF
QCIF
Single layer CIF and QCIF(H.264/AVC)
Single layer with 10% rate increase
SVC with spatial scalability
Simulcast of CIF and QCIF (for QCIF)
FIGURE 12.6–9
Illustration of H.264/SVC performance for spatiotemporal scalability—QCIF at 30 fps and
CIF at 15 fps (from [54], c⃝IEEE 2008).
for quality scalability alone was also given in [54] and is shown in Figure 12.6–10
which again shows a 10% penalty in rate, here for 6 layers of quality scalability. All
six plotted points come from the same encoded stream, with successively discarding
of so-called quality enhancement packets. The various types of scalability may be
combined, but results were not shown. It is expected that performance will suffer.
We have seen that the H.264/SVC coders are layered with motion determined
from the bottom up for the chosen B-frame temporal hierarchy. For comparison
purposes, the MCTF coders commonly determine their motion ﬁrst at the ﬁnest
temporal resolution and then scale it down and adjust it for the lower frame rates.
Another difference that should be noted is that embedded MCTF scalable coders (e.g.,
MC-EZBC) provide simultaneous spatial, temporal, and quality scalability with no
restriction to a coarse set of quality layers.
SVC for Video Conferencing
In video conferencing, overall delay is an important factor. Generally it is agreed that
this delay should be kept below 120–200 msec to enable participants to talk naturally
without pausing to wait for the other to ﬁnish. Due to this need, the use of B frames is
discouraged in this application because they cause additional delay. Yet hierarchical
B frames are very effective for compression efﬁciency. However, a restricted version

12.6 Current Interframe Coding Standards
513
Bit rate [kb/s]
0
32
33
34
35
36
37
38
Soccer, CIF 30 Hz
Y-PSNR [dB]
100
200
300
400
500
600
700
800
SVC with fidelity scalability
Simulcast of 2 rates (for highest rate)
Single layer with 10% rate increase
Single layer (H.264/AVC)
FIGURE 12.6–10
Illustration of quality scalability from the soccer clip with CIF at 30 fps. (from [54], c⃝IEEE
2008)
8-frame GOP 
Threaded L frames with P frame 
I
L3
P
L1
L2
L3
L3
L3
L2
FIGURE 12.6–11
Efﬁcient threaded hierarchical structure without increased structural delay of B frames.
of hierarchical B frames, referred to as threading [56], can be used because it only
depends on prior coded slices or frames. In Figure 12.6–11, the thread frame type is
denoted by L and two levels are shown.
Example 12.6–1: Multipoint Video Conferencing
In video conferencing with multiple participants at separate locations, the link bandwidth
and user computational capability often dictate the use of different resolutions, frame

514
CHAPTER 12 Digital Video Compression
MCU
Network
User 1
User 2
User 3
User 4
FIGURE 12.6–12
Illustration of conventional multipoint video conferencing architecture.
rates, and bitrates for the various receivers. A common method to deal with this problem
in the case of nonscalable coders is shown in Figure 12.6–12. This traditional approach
[57] uses a device called multipoint control unit (MCU) to perform transcoding among the
various formats used. We have indicated the various link bandwidths by the thickness of
the connecting lines.
Unfortunately, this approach introduces extra delay in the video conference due to the
use of transcoding (decode/compose/recode) to serve the various users’ requirements.
Usually all participants would have to agree on some minimal quality for the video confer-
ence, but this is not really satisfactory in many cases. With the advent of SVC, the MCU is
not needed because transcoding is not needed. Using the SVC standard, so-called video
routers can be substituted for the MCUs. These routers only have to forward and delete
packets as appropriate to that user’s connection and needs, so little extra delay is added.
In effect, video packets can be routed by the video router based on their headers, just like
other network packets. Each user then has a simple client application that decodes and
composes the various videos together into a suitable browser window. Such a system was
simulated in [58], and signiﬁcant advantage was shown in terms of reduced multipoint
video conferencing delay. Two channels were employed: one with high reliability (HRC),
used for the base layer, and one with low reliability (LRC) and 5% packet loss used for
enhancement layers. Figure 12.6–13 from [58], plots luminance PSNR for two systems,
one with MCU and one with SVC instead. The curves plotted in the right of the ﬁgure are for
a single-layer coder and include the MCU delay. The curves plotted on the left incorporate
SVC and show improved performance with much smaller delays, due to hierarchical B and
threaded L frames. The MCU system is seen to suffer signiﬁcant extra delay, in the vicin-
ity of 150–200 msec, making it marginal for conversation. The MCU system also suffers
more from packet loss, while the SVC system (denoted SVCS for scalable video conferenc-
ing server) has been able to decode the base layer sent over the HRC and suffered only a

12.6 Current Interframe Coding Standards
515
0.0
50.0
100.0
Delay (msec)
150.0
200.0
Single layer (no MCU)
Single layer (with MCU)
Single layer (with
transcoding MCU)
Single layer (with transcoding
MCU, 5% loss)
Threaded (with SVCS)
Spatially scalable/threaded
(with SVCS, 5% loss)
38.0
37.0
36.0
35.0
34.0
33.0
32.0
31.0
Y-PSNR (dB)
FIGURE 12.6–13
A simulated performance comparison of PSNR versus delay. (from [58] c⃝JZUS A 2006)
small PSNR loss. The H.264/AVC coder has also suffered a partial loss of its bitstream on
the LRC, but with much larger consequences due to its nonscalable structure.
H.264/MVC
An extension of the H.264/AVC standard for multiview or stereo 3-D coding was
released by ITU/ISO in 2008 to support the simultaneous compression of video
from several cameras with largely overlapping ﬁelds of view. The new standard
H.264/MVC uses a hierarchical B-frame structure with the addition of inter-view pre-
diction modes, as seen in Figure 12.6–14. In this ﬁgure, with ﬁve cameras, we ﬁrst
notice the hierarchical B-frame temporal hierarchies indicated for coding the individ-
ual camera streams. But there are additional coding dependencies shown between the
camera (views) representing the inter-view predictions. We see that the ﬁrst frame
in the cam 3 sequence is predicted by the ﬁrst coded frame in the cam 1 sequence,
and then in turn the ﬁrst frame in the cam 5 sequence. These ﬁrst frames are in
turn used to predict the B1-level frames in their individual sequences, and so on in
sequences 3 and 5. After the GOPs are complete, the cam 2 and cam 4 sequences
are coded with the new option for inter-view prediction from the already-coded
frames. Just as we talk of temporal level in the B-frame hierarchy, we can talk of
view level here, meaning the frame’s level or coding order, in the inter-view B-frame
hierarchy.
We see in Figure 12.6–14 that the ﬁrst B frame in the cam 2 sequence can be
predicted by the corresponding B frames in the cam 1 and cam 3 sequences or by the
B frame on either side in its own sequence, or by any already-coded frame of lower
temporal or view level in the DPB. Again, the diagram does not indicate what frame
is used, just what frames cannot be used (i.e., those whose level is higher in terms of
inter-view levels).

516
CHAPTER 12 Digital Video Compression
It was noticed that due to the great ﬂexibility of picture memory referencing
allowed in H.264/AVC, the new MVC coding could be conducted with this soft-
ware by stringing the multiview frames together into a single stream [59]. The DPB
must be very large to accommodate this. The objective (PSNR) coding results of
Time
Cam 1
Cam 2
Cam 3
Cam 4
Cam 5
B
I
B
B
B
B
B
B
I
B
B
P
B
B
B
B
B
B
P
B
B
P
B
B
B
B
B
B
P
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
FIGURE 12.6–14
Illustration of H.264/MVC inter-view prediction hierarchy. (from public MPEG site,
http://mpeg.chiariglione.org/)
36.0
35.0
34.0
33.0
PSNR Y [dB]
32.0
31.0
30.0
192
256
320
384
Avg. rate [kbps]
448
512
576
Anchor
Simulcast
KS_PIP
KS_IBP
KS_IPP
AS_IBP
AS_IPP
FIGURE 12.6–15
H.264/MVC result from Merkle et al. [59], c⃝IEEE 2007.

12.7 Nonlocal Intraprediction
517
Figure 12.6–15 are from [59] and show H.264/MVC PSNR-Y values achieved on
the multiview test clip Ballroom consisting of eight cameras in a linear array with
19.5-cm intercamera spacing, with all the cameras pointing exactly parallel and nor-
mal to the array line, and each camera with a resolution of 640 × 480 pixels in
4:2:0 format at 25 fps and a sequence length 250 frames [60]. In this ﬁgure, the
indicated anchor sequence is the total bitrate for conventional H.264/AVC coding
in IBBP ... mode of all the views. The simulcast is the same, but with hierarchical
B-frame structure. The other curves marked AS xxx and KS xxx correspond to vari-
ous strategies for MVC using the preceding method. We can see a clear advantage for
the joint or MVC coding, amounting to several dB at CBR or about 25% savings in
bitrate for constant PSNR-Y. Based on usage statistics, it is found that the inter-view
prediction modes are chosen about 15–20% of the time on average. One reason for
the low usage is that the individual cameras cannot be exactly calibrated to match
one another. Another reason is the large inter-view displacements that can occur for
objects moving in the foreground. There have been some positive results for warped
view interpolation [61] for improving coding performance. In this work, pairs of
views are jointly warped and interpolated toward a target, and then these synthesized
views are inserted into the DPB as possible alternate references. They also compen-
sated for camera mismatch via a look-up table method. Reportedly about 15% of the
bitrate was saved for comparable performance.
12.7 NONLOCAL INTRAPREDICTION
Recently, proposals have come forth for powerful new ways to perform the intra pre-
diction step in the AVC-based coders. They generally rely on searching for matching
image blocks in the NSHP past of the current frame and then using them to predict
the current or target block. This is then added as an additional mode decision for
intraframe coding. There are two general methods, one based on the past original
image and the other based on the past coded image.
Intra-Macroblock Displacement Compensation
This idea generalizes the motion compensation in interframe coding to searching for
block matches in the current frame for I-frame coding. A search region is deﬁned
in the NSHP past of the current block to be coded, the best match is obtained, usu-
ally with sum of absolute differences (SAD), and then the residual block is transform
coded. In addition a displacement vector must be coded and transmitted. This possi-
bility is then added as an additional mode to the nine existing H.264/AVC predictive
modes. Preliminary results credited Intra-macroblock motion compensation with a
12.4% bitrate savings and an improvement of 0.5 dB for coding the CIF version of
Foreman [62].

518
CHAPTER 12 Digital Video Compression
Template Matching for Intra Prediction
This concept is similar to the preceding one but avoids the need to code and transmit
the displacement overhead information. The search is conducted on the previously
coded data in the NSHP past, with the same search being repeated at the receiver.
The search target is no longer just the block to encode, but also includes its boundary,
as shown for the 2 × 2 case in Figure 12.7–1. The new method was called template
matching. In this exemplar of the technique, a 5-pixel part of a 3 × 3 block is searched
in the coded/decoded past and then the right-bottom 2 × 2 part of the best match
becomes the ﬁrst quadrant of the 4 × 4 target block to be coded. The rest of the target
block is then template coded in the same manner. The computation is equivalent to
search of a 5-pixel region. The idea follows an innovative approach in texture synthe-
sis [64]. The new template-matching method was included as an additional mode for
intra prediction in the H.264/AVC JM 9.6 test coder. Results were reported including
the QCIF coding of Foreman at 15 fps, comparing the new technique template-
matching spatial prediction (TMSP) to the H.264/AVC test coder JM 9.6. The results
are shown in Figure 12.7–2, with QP values 22, 27, 32, and 37, including both 4 × 4
and 8 × 8 transforms and the CABAC entropy coding technique. We can see reduc-
tions in bitrate for equivalent PSNR ranging from a low of 3.3% at high bitrates to a
high of 11.5% reduction at low bitrates. Improvement for other test clips was lower
though.
An improved TMSP coder, including template match averaging, alternative tem-
plate shapes, and larger templates [65], yielded further improvement in intraframe
coding under H.264/AVC.
x
Candidate neighborhood
Candidate sub block
Search region of
reconstructed pixels
Template
Target sub block
4× 4 Target block
y
2 ×2
2× 2
2× 2
2 ×2
2× 2
FIGURE 12.7–1
Illustration of template matching for intra prediction. (from [63])

12.8 Object-Based Coding
519
80
31
31.5
32
32.5
33
33.5
34
34.5
35
35.5
ypsnr (dB)
36.5
37.5
38.5
38
39.5
39
40.5
40
41.5
40
42
37
36
180
11.3%
10.1%
7.5%
5.3%
280
380
Jm 9.5
TMSP
480
580
Foreman QCIF @ 15 fps (QP=22,27,32 & 37, 4×4 & 8 × 8 transforms, CABAC)
Bitrate (kbps)
FIGURE 12.7–2
Improvement in PSNR versus bitrate for Foreman QCIF at 15 fps.
12.8 OBJECT-BASED CODING
Visual scenes are made up of objects and backgrounds. If we apply image and video
analysis to pick out these objects and estimate their motion, this can be a very efﬁcient
type of video compression [66]. This was a goal early on in the MPEG 4 research
and development effort, but since reliable object detection is a difﬁcult and largely
unsolved problem, the object-coding capability in MPEG 4 is largely reserved for
artiﬁcially composed scenes where the objects and their locations are already known.
In object-based coding, there is also the new problem of coding the outline or shape
of the object, known as shape coding. Further, for a high-quality representation, it is
necessary to allow for a slight overlap of the natural objects, or soft edge. Finally,
there is the bit allocation problem, ﬁrst to the object shapes, and then to their inside,
called texture, and their motion ﬁelds.
In Chapter 11 we looked at joint Bayesian estimates of motion and segmen-
tation. From this operation, we get both output objects and motion ﬁelds for
these objects. Each one can then be subject to either hybrid MCP- or MCTF-type
video coding. Here, we show an application of this Bayesian motion/segmentation

520
CHAPTER 12 Digital Video Compression
Motion est. /
segmentation
(d, s)
Input
video
Object
classifier
Texture
coding
Motion/contour
coder
Motion/contour
decoder
(d, s) bitstream
∼∼
(d, s)
∼∼
Coded texture
Object classification
FIGURE 12.8–1
System diagram of the hybrid MCP object video coder in [67].
to an example object-based coder [67]. The various blocks of the object-based coder
are shown in Figure 12.8–1, where we see the input video going into the joint
motion estimation and segmentation block of Chapter 11, whose output is a dense
motion ﬁeld d for each object and a segmentation label s. This output is input to
the motion/contour coder, whose role is to code the motion ﬁeld of each object and
also code its contour. The input video also goes into an object classiﬁer and a texture
coder. The object classiﬁer decides whether the object can be predicted from the pre-
vious frame, a P object, or is a new object in this frame, an I object. Of course, in the
ﬁrst frame, all objects are I objects. The texture coder module computes the MCP of
each object and codes the prediction residual.
The motion coder approximates the dense motion ﬁeld of the Bayes estimate over
each object with an afﬁne motion model ﬁtted in a least-squares sense. An afﬁne
model with six parameters representing rotation and translation is projected onto the
image plane as
d1(x1,x2) = a11x1 + a12x2 + a13,
d2(x1,x2) = a21x1 + a22x2 + a23,
where the position vector x = (x1,x2)T on the object and d(x) is the displacement
vector. The motion warping effect of an afﬁne motion model has been found to well
approximate the apparent motion of the pixels of rigid objects. The motion output of
this coder is denoteded in Figure 12.8–1. The other output of the motion/contour coder
is the shape informationes. Since the Bayesian model enforces a continuity along the
object track as well as a spatial smoothness, this information can be well coded in a
predictive sense. The details are contained in Han and Woods [68].

12.8 Object-Based Coding
521
The texture coding proceeds for each object separately. Note that these are
not really physical objects, but the objects selected by the joint Bayesian MAP
motion/segmentation estimate. Still, they correspond to at least large parts of physi-
cal objects. Secondly, note that it is the motion ﬁeld that has been segmented not the
imaged objects themselves. As a result, when an object stops moving, it gets merged
into the background segment. There are now a number of nice solutions to the cod-
ing of these irregularly shaped objects. There is the shape-adaptive DCT [69] that is
used in MPEG 4. There are also various SWT extensions to nonrectangular objects
[70, 71].
Example 12.8–1: Object-based SWT Coder
Han [67] used the SWT method of Bernard [70] to code the carphone QCIF test clip was
coded at the frame rate of 7.5 fps and at the CBR bitrate of 24 Kbps, and the results were
compared against an H.263 implementation from Telenor Research. Figure 12.8–2 shows
a decoded frame from the test clip coded at 24 Kbps via the object-based SWT coder.
Figure 12.8–3 shows the same frame from the decoded output of the H.263 coder. The
FIGURE 12.8–2
QCIF of the carphone clip coded via object-based SWT at 24 Kbps.
FIGURE 12.8–3
QCIF of the carphone clip coded at 24 Kbps by H.263.

522
CHAPTER 12 Digital Video Compression
average PSNRs are 30.0 dB and 30.1 dB, respectively. So there is a very slight advantage
of 0.1 dB to the H.263 coder. However, we can see that the image out of the object-based
coder is sharper. A typical frame-segmentation results were given in Figure 11.4–7b in
Chapter 11. Since the object-based coding was done at a constant number of bits per
frame, while the H.263 coder had use of a decoded picture buffer, the latter was able to
put more bits into the frames with a lot of motion, and fewer bits in quieter frames. So
there was some measure of improvement possible for the object-based SWT coder.
A comprehensive review article on object-based video coding is contained in [72].
12.9 COMMENTS ON THE SENSITIVITY OF COMPRESSED
VIDEO
Just as with image coding, the efﬁcient variable length code words used in com-
mon video coding algorithms render them sensitive to errors in transmission. So,
any usable codec must include general and repeated header information, giving posi-
tions and length of coded blocks that are separated by sync words such as EOB.
Of course, additionally the decoder has to know what to do upon encountering an
error (i.e., an invalid result), such as EOB in the wrong place or an illegal code-
word resulting from a noncomplete VLC code tree. It must do something reasonable
to recover from such a detected error. Thus, at least some redundancy must be
contained in the bitstream to enable a video codec to be used in a practical envi-
ronment, where even one bit may come through in error, e.g., in a storage application
such as DVD or Blu-Ray disk.
Additionally, when compressed video is sent over a channel or network, it must
be made robust with respect to much more frequent errors and data loss. Again, this
is due to VLCs used in the compression algorithms. As we have seen, the various
coding standards place data into slices or groups of blocks that terminate with a sync
word, which prevents the error propagation across this boundary. For packet-based
wired networks such as the Internet, the main source of error is lost packets, as errors
are typically corrected in lower network layers before the packet is passed up to
the application layer and given to the decoder. Because of VLC, lost packets may
make further packets useless until the end of the current slice or group of blocks.
Forward error correction codes can be used to protect the compressed data. Slice
lengths are designed based on efﬁciency, burst length, and preferred network packet
or cell size. Further, any remaining redundancy, after compression, can be used at the
receiver for a post-decoding cleanup phase, called error concealment in the literature.
Combinations of the two methods can be effective too. Also useful is a request for
retransmission or ACK strategy, wherein a lost packet is detected at the source by not
receiving an acknowledgement from the receiver and, based on its importance, may
be resent. This strategy generally will require a larger output buffer and may not be
suitable for real-time communication.
Scalable coding can be very effective to combat the uncertainty of a variable
transmission medium such as the Internet. The ability to respond to variations in the

Problems
523
available bandwidth (i.e., the bitrate that can be sustained without too much packet
loss) gives scalable coders a big advantage for such situations. The next chapter
concentrates on video for networks.
CONCLUSIONS
Hybrid block transform methods dominate today’s video coding standards. Inter-
esting research coders have been developed based on SWT methods, showing the
high degree of scalability that is inherent in this approach. Subband/wavelet schemes
computationally scale nicely to super high deﬁnition SHD, close to DC resolution,
where the DCI has chosen M-JPEG 2000 as their recommended standard. The cur-
rent H.264/AVC standard has been extended to scalable and to multiview coding,
both using an efﬁcient hierarchical B-frame concept borrowed from MCTF. For fur-
ther reading, an overview of video compression is contained in Chapter 6, “Video
Compression,” in Bovik’s handbook [73]. See also Chapters 9–11 in The Essential
Guide to Video Processing [50].
PROBLEMS
1. How many GBytes are necessary to store 1 hour of SD 720 × 486 progressive
video at 8 bits/pixel and 4:2:2 color space? How much for 4:4:4? How much for
4:2:0?
2. A certain analog video display/monitor has a 30-MHz video bandwidth. Can
it resolve the pixels in a 1080p video stream? Remember, 1080p is the ATSC
format with 1920×1080 pixels and 60 fps.
3. The NTSC DV color space 4:1:1 is speciﬁed in Figure 12.1–5, and the NTSC
MPEG 2 color space 4:2:0 is given by Figure 12.P–1.
(a) First, assume that each format is progressive. What is the speciﬁcation for
the chroma sample rate change required? Specify the upsampler, the ideal
ﬁlter speciﬁcations, and the downsampler.
(b) Do the same for an interlace format noting that the chroma samples are part
of the top ﬁeld only, as shown in Figure 12.P–2.
4:2:0
FIGURE 12.P–1
4:2:0 color space structure of MPEG 2.

524
CHAPTER 12 Digital Video Compression
4:2:0 interlaced
n
n2
Top field
Bottom field
FIGURE 12.P–2
4:2:0 chrominace samples are part of top ﬁeld only.
4. Use a Lagrangian formulation to optimize the intraframe coder with distortion
D(n) as in (12.1–4), and average bitrate R(n) in (12.1–5), both for frame n.
Consider frames 1 to N and ﬁnd an expression for the resulting average mean-
square distortion as a function of the average bitrate.
5. Write the decoder diagram for the backward motion-compensated hybrid
encoder shown in Figure 12.2–4.
6. Discuss the algorithmic (structural) delay of an MPEG hybrid coder based on the
number of B frames that it uses. What is the delay when the successive number
of B frames is M?
7. In an MPEG 2 coder, let there be two B frames between each non-B frame, and
let the GOP size be 15. At the source, we denote the frames in one GOP as
follows:
I1
B2
B3
P4
B5
B6
P7
B8
B9
P10
B11
B12
P13
B14
B15
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
(a) What is the order in which the frames must be coded?
(b) What is the order in which the frames must be decoded?
(c) What is the required display order?
(d) Based on the preceding answers (a–c), what is the inherent delay in this form
of MPEG 2? Can you generalize your result from two successive B frames
to M successive B frames?
8. In MPEG 1 and 2, there are open and closed GOPs. In the closed GOP case,
the bidirectional predictor cannot use any frames outside the current GOP in
calculating B frames. In the open GOP case, they can be used. Consider a GOP
of 15 frames, using pairs of B frames between each P frame. Which frames
would be affected by the difference? How would the open GOP concept affect
decodability?
9. Find the synthesis ﬁlter corresponding to the lifting analysis equations (12.4–1)
and (12.4–2). Is the reconstruction exact even in the case of subpixel accuracy?
Why? Do the same for the LGT 5/3 SWT, as in (12.4–3) and (12.4–4).

References
525
10. In an MCTF-based video coder, unconnected pixels can arise due to expansion
and contraction of the motion ﬁeld but also, and more importantly, due to occlu-
sion. State the difference between these two cases and discuss how they should
be handled in the coding stage that comes after the MCTF.
11. Find the “inverse” transform for the 4 × 4 DCT-like matrix (12.6–1) used in
H.264/AVC. Note that the “inverse” transform used in H.264/AVC is not the
matrix inverse, but rather a kind of scaled version of the inverse. Show that 8-bit
data may be transformed and inverse transformed without any error using 16-bit
arithmetic. Refer to [43] or [14] for help.
12. String the frames in Figure 12.6–14 together so that the indicated MVC coding
can be accomplished by a standards-conforming H.264/AVC coder.
REFERENCES
[1] P. H. N. de With and A. M. A. Rijckaert, “Design Considerations of the Video Compres-
sion System of the New DV Camcorder Standard,” IEEE Trans. Consumer Electr., vol.
43, no. 4, pp. 1160–1179, November 1997.
[2] C. B. Jones, “An Efﬁcient Coding System for Long Source Sequences,” IEEE Trans.
Inform. Theory, vol. IT-27, no. 3, pp. 280–291, May 1981.
[3] T. Naveen and J. W. Woods, “Subband and Wavelet Filters for High-Deﬁnition Video
Compression,” Chapter 8 in Handbook of Visual Comm., Ed. H.-M. Hang and J. W.
Woods, Academic Press, 1995.
[4] J. W. Woods, S.-C. Han, S.-T. Hsiang, and T. Naveen, “Spatiotemporal Subband/Wavelet
Video Compression,” Chapt. 6.2 in Handbook of Image and Video Process., 1st Ed., Al
Bovik, Ed., pp. 575–595, Academic Press, 2000.
[5] J. C. Candy et al., “Transmitting Television as Clusters of Frame-to-Frame Differences,”
Bell Syst. Tech. J., vol. 50, pp. 1889–1917, July–August 1971.
[6] A. Netravali and J. Limb, “Picture Coding: A Review,” Proceedings of the IEEE, no. 3,
March 1980.
[7] B. Girod, “The Efﬁciency of Motion-Compensating Prediction for Hybrid Coding of
Video Sequences,” IEEE J. Select. Areas in Comm., vol. SAC-5, no. 7, pp. 1140–1154,
August 1987.
[8] Generic Coding of Motion Pictures and Assoc. Audio, Rec. H.262 (aka MPEG 2),
ISO/IEC 13818-2, March 1994.
[9] B. G. Haskell, A. Puri, and A. N. Netravali, Digital Video: An Introduction to MPEG-2,
Chapman and Hall, New York, 1997.
[10] E.-H. Yang and L. Wang, “Full Rate Distortion Optimization of MPEG-2 Video Coding,”
Proc. ICIP 2009, Cairo, Egypt, November 2009.
[11] Coded Representation of Picture and Audio Information—MPEG-2 Test Model 5, ISO-
IEC AC-491, April 1993.
[12] K. P. Davies, “HDTV Evolves for the Digital Era,” in HDTV mini issue of IEEE
Communications Magazine, vol. 34, no. 6, pp. 110–112, June 1996.
[13] F. Pereira and T. Ebrahimi, Eds., The MPEG-4 Book, Prentice-Hall, Inc. Upper Saddle
River, NJ, 2002.

526
CHAPTER 12 Digital Video Compression
[14] S. Liu and A. Bovik, “Digital Video Transcoding,” Chapter 6.3, in Handbook of Imaging
and Video Processing, 2nd Ed., A. Bovik, ed., Elsevier/Academic Press, Burlington, MA,
2005.
[15] G. D. Karlson, Subband Coding for Packet Video, MS thesis, Center for Telecommunica-
tions Research Columbia University, 1989.
[16] D. LeGall and A. Tabatabai, “Sub-band Coding of Digital Images Using Symmetric
Short Kernel Filters and Arithmetic Coding Techniques,” Proc. ICASSP 1988, IEEE,
New York, pp. 761–764, April 1988.
[17] W. Glenn et al., “Simple Scalable Video Compression Using 3-D Subband Coding,”
SMPTE Journal, March 1996.
[18] J. W. Woods and T. Naveen, “Subband Coding of Video Sequences,” SPIE Proc. VCIP,
SPIE, pp. 724–732, November 1989.
[19] T. Naveen and J. W. Woods, “Motion Compensated Multiresolution Transmission of
Video,” IEEE Trans. Video Tech., vol. 4, no. 1, pp. 29–43, February 1994.
[20] H. Gharavi, “Motion Estimation Within Subbands,” Chapt. 6 in Subband Image Coding.
Ed. J. W. Woods, Kluwer Academic Pub, 1991.
[21] L. Vandendorpe, Hierarchical Coding of Digital Moving Pictures, PhD thesis, University
of Louvain, Belgium, 1991.
[22] F. Bosveld, Hierarchical Video Compression Using SBC, PhD thesis, TU Delft, The
Netherlands, 1996.
[23] T. Kronander, “Motion Compensated 3–Dimensional Waveform Image Coding,” Proc.
ICASSP, IEEE, Glasgow, Scotland, 1989.
[24] J.-R. Ohm, “Three-dimensional Subband Coding with Motion Compensation,” IEEE
Trans. Image Processing, vol. 3, no. 5, pp. 559–571, September 1994.
[25] S.-J. Choi, Three-Dimensional Subband/Wavelet Coding of Video, PhD thesis, ECSE
Department, Rensselaer Polytechnic, Troy, NY, August 1996.
[26] T. Naveen and J. W. Woods, “Subband Finite-State Scalar Quantization,” IEEE Trans.
Image Processing, vol. 5, no. 1, pp. 150–155, January 1996.
[27] D. Wu, Y. T. Hou, and Y.-Q. Zhang, “Scalable Video Coding and Transport over Broad-
band Wireless Networks,” Proceedings of the IEEE, vol. 89, no. 1, pp. 6–20, January
2001.
[28] A. W. Johnson, T. Sikora, T. K. Tan, and K. N. Ngan, “Filters for Drift Reduction in
Frequency Scalable Video Coding Schemes,” Electron. Letters, vol. 30, no. 6, pp. 471–
472, March 1994.
[29] P. J. Burt and E. H. Adelson, “The Laplacian Pyramid as a Compact Image Code,” IEEE
Trans. Comm., vol. 31, no. 4, pp. 532–540, April 1983.
[30] T. Naveen, F. Bosveld, R. Lagendijk, and J. W. Woods, “Rate Constrained Multire-
solution Transmission of Video,” IEEE Trans. Video Tech., vol. 5, no. 3, pp. 193–206,
June 1995.
[31] D. Taubman and A. Zakhor, “Multirate 3-D Subband Coding of Video,” IEEE Trans.
Image Process., vol. 3, no. 5, pp. 572–588, September 1994.
[32] J. W. Woods and G. Lilienﬁeld, “Resolution and Frame-rate Scalable Video Coding,”
IEEE Trans. Video Tech., vol. 11, no. 9, pp. 1035–1044, September 2001.
[33] S.-T. Hsiang and J. W. Woods, “Embedded Video Coding Using Invertible Motion Com-
pensated 3-D Subband/Wavelet Filter Bank,” Signal Processing: Image Communication,
vol. 16, no. 8, pp. 705–724, May 2001.
[34] S.-T. Hsiang and J. W. Woods, “Embedded Image Coding Using Zeroblocks of Sub-
band/Wavelet Coefﬁcients and Context Modeling,” MPEG-4 Workshop and Exhibition
at ISCAS 2000, IEEE, Geneva, Switzerland, May 2000.

References
527
[35] S. Choi and J. W. Woods, “Motion-compensated 3-D Subband Coding of Video,” IEEE
Trans. Image Process., vol. 8, no. 2, pp. 155–167, February 1999.
[36] P. Chen and J. W. Woods, “Bidirectional MC-EZBC with Lifting Implementation,” IEEE
Trans. Video Tech., vol. 14, no. 10, pp. 1183–1194, October 2004.
[37] J. Ohm, M. vd Schaar, and J. W. Woods, “Interframe Wavelet Coding—Motion Picture
Representation for Universal Scalability,” Signal Processing: Image Communication,
vol. 19, no. 9, pp. 877–908, October 2004.
[38] Y. Wu, K. Hanke, T. Russert, and J. W. Woods, “Enhanced MC-EZBC Scalable Video
Coder,” IEEE Trans. Video Tech., vol. 18, no. 10, pp. 1432–1436, October 2008.
[39] T. Rusert, K. Hanke, and M. Wien, “Optimization for Locally Adaptive MCTF Based on
5/3 Filtering,” Proc. Picture Coding Symposium (PCS), San Francisco, CA, 2004.
[40] R. Xiong, J. Xu, F. Wu, S. Li, and Y. Zhang, “Layered Motion Estimation and Coding for
Fully Scalable 3-D Wavelet Video Coding,” in Proc. ICIP, Singapore, pp. 2271–2274,
October 2004.
[41] A. Secker and D. Taubman, “Highly Scalable Video Compression with Scalable Motion
Coding,” IEEE Trans. Image Process., vol. 13, no. 8, pp. 1029–1041, August 2004.
[42] G. J. Sullivan and T. Wiegand, “Rate-Distortion Optimization for Video Compression,”
IEEE Signal Process. Magazine, vol. 15, no. 6, pp. 74–90, November 1998.
[43] G. J. Sullivan and T. Wiegand, “Video Compression—From Concepts to the H.264/AVC
Standard,” Proc. of the IEEE, vol. 93, no. 1, pp. 18–31, January 2005.
[44] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview of the H.264/AVC
Video Coding Standard,” IEEE Trans. Video Tech., vol. 13, no. 7, pp. 560–576, July 2003.
[45] Special Issue on H.264/AVC, IEEE Trans. for Video Tech., vol. 13, no. 7, July 2003.
[46] T. Wiegand and B. Girod, “Lagrange Multiplier Selection in Hybrid Video Coder
Control,” Proc. IEEE ICIP, 2001.
[47] T. Wiegand, H. Schwarz, A. Joch, F. Kossentini, and G. J. Sullivan, “Rate-Constrained
Coder Control and Comparision of Video Coding Standards,” IEEE Trans. Video Tech.,
vol. 13, no. 7, pp. 688–703, July 2003.
[48] E.-H. Yang and X. Yu, “Rate Distortion Optimization for H.264 Interframe Coding:
A General Framework and Algorithms,” IEEE Trans. Image Process., vol. 16, no. 7,
pp. 1774–1784, July 2007.
[49] H.264 reference software. Available at http://iphome.hhi.de/suehring/tml/.
[50] Al Bovik, Ed., The Essential Guide to Video Processing., Elsevier Academic Press,
Burlington, MA, 2009.
[51] H. Schwarz, D. Marpe, and T. Wiegand, “Overview of the Scalable Video Coding Exten-
sion of the H.264/AVC Standard,” IEEE Trans. Video Tech., vol. 17, no. 9, pp. 1103–1120,
September 2007.
[52] H. Schwarz and D. Marpe, “Analysis of Hierarchical B Pictures and MCTF,” Proc. Int.
Conf. Multimedia Expo. (ICME), Toronto, Canada, July 2006.
[53] H. Schwarz and T. Wiegand, Further Results for an RD-Optimized Multi-loop SVC
Encoder, JVT-W072, Joint Video Team (JVT) of ISO/IEC MPEG & ITU-T VCEG, 23rd
Meeting: San Jose, CA, April 2007.
[54] H. Schwarz and M. Wien, “The Scalable Video Coding Extension of the H.264/AVC
Standard,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 135–141, March 2008.
[55] C. Segall and G. Sullivan, “Spatial Scalability Within the H.264/AVC Scalable Video
Coding Extension,” IEEE Trans. Video Tech., vol. 17, no. 9, pp. 1121–1135, September
2007.
[56] S. Wenger, “Video Redundancy Coding in H.263+,” Proc. Workshop on Audio-Visual
Services for Packet Networks, 1997.

528
CHAPTER 12 Digital Video Compression
[57] M. R. Civanlar, R. D. Gaglinello, and G. L. Cash, “Efﬁcient Multi-Resolution, Multi-
Stream Video Systems Using Standard Codecs,” Journal of VLSI Signal Processing, vol.
17, no. 2–3, pp. 269–279, 1997.
[58] A. Eleftheriadis, M. R. Civanlar, and O. Shapiro, “Multipoint Videoconferencing with
SVC,” Journal of Zhejiang University-Science A, vol. 7, no. 5, pp. 696–705, May 2006.
[59] P. Merkle, A. Smolic, K. M¨uller, and T. Wiegand, “Efﬁcient Prediction Structures for
Multiview Video Coding,” IEEE Trans. Video Tech., vol. 17, no. 11, pp. 1461–1473,
November 2007.
[60] MERL test clips for MVC. Available at ftp://ftp.merl.com/pub/avetro/mvc-testseq.
[61] K. Yamamoto, M. Kitahara, H. Kimata, T. Yendo, T. Fujii, M. Tanimoto, S. Shimizu,
K. Kamikura, and Y. Yashima, “Multiview Video Coding Using View Interpolation and
Color Correction,” IEEE Trans. Video Tech., vol. 17, no. 11, pp. 1436–1449, November
2007.
[62] S.-L. Yu and C. Chrysaﬁs, “New Intra Prediction Using Intra-Macroblock Motion
Compensation,” Joint Video Team of MPEG and VCEG, JVT-C151, Fairfax, VA, May
2002.
[63] T.-K. Tan, C. S. Boon, and Y. Suzuki, “Intra Prediction by Template Matching,” Proc.
IEEE ICIP, Atlanta, GA, October 2006.
[64] L.-Y. Wei and M. Levoy, “Fast Texture Synthesis Using Tree-Structured Vector Quanti-
zation,” Proc. ACM SIGGRAPH, New Orleans, LA, 2000.
[65] T.-K. Tan, C. S. Boon, and Y. Suzuki, “Intra Prediction by Averaged Template Matching
Predictors,” Proc. IEEE CCNC, 2007.
[66] H. Musmann, M. Hotter, and J. Ostermann, “Object-Oriented Analysis-Synthesis Coding
of Moving Images,” Signal Process.: Image Communications, vol. 1, no. 2, pp. 117–138,
October 1989.
[67] S.-C. Han, Object-Based Representation and Compression of Image Sequences, PhD
thesis, ECSE Department, Rensselaer Polytechnic Institute, Troy, NY, 1997.
[68] S.–C. Han and J. W. Woods, “Adaptive Coding of Moving Objects for Very Low
Bitrates,” IEEE J. Select. Areas in Comm., vol. 16, no. 1, pp. 56–70, January 1998.
[69] T. Sikora, “Low-Complexity Shape-Adaptive DCT for Coding of Arbitrarily Shaped
Image Segments,” Signal Process.: Image Comm., vol. 7, no. 4–6, pp. 381–395, 1995.
[70] H. J. Bernard, Image and Video Coding Using a Wavelet Decomposition, PhD thesis,
Delft University of Technology, The Netherlands, 1994.
[71] J. Li and S. Lei, “Arbitrary Shape Wavelet Transform with Phase Alignment,” Proc.
ICIP, vol. 3, pp. 683–687, 1998.
[72] T. Ebrahimi and M. Kunt, “Object-Based Video Coding,” Chapter 6.3 in Handbook of
Image and Video Processing, A. Bovik, Ed., 1st Ed., Academic Press, 2000.
[73] Chapter 6 in Handbook of Image and Video Process., 2nd Ed., Al Bovik, Ed.,
Elsevier/Academic Press, Burlington, MA, 2005.

CHAPTER
Video Transmission over
Networks
13
Many video applications involve transmission over networks, such as visual
conversation, video streaming, video on demand (VOD), and video downloading.
Various kinds of networks are involved, wireless local area networks (WLANs) such
as IEEE 802.11, storage area networks (SANs), private internet protocol (IP) net-
works, public Internet, and wireless networks. If the network lacks congestion and
has no bit errors, then the source coding bitstreams of the previous chapter can be
sent directly over the network. Otherwise, some provision has to be made to protect
the rather fragile compressed data that results from the source coder. In some cases,
even a single bit error can cause the decoder to terminate upon encountering illegal
parameter values.
In this chapter we will encounter some of the new aspects that arise with network
transmission of video. We will review the error-resilient features of some standard
video source coders as well as the transport error protection features of networks and
show how they can work together to protect the video data. We present some robust
transmission methods for scalable SWT-based coders such as MC-EZBC as well as
for standard H.264/AVC. The chapter ends with a section on joint source–network
coding, wherein the source-channel coding is continued into the network on overlay
nodes that also support data item adaptation. This section includes an introduction to
network coding focusing on use of random network codes.
We start with an overview of IP networks. General introductions to networks
themselves can be found in the textbooks by Tenenbaum [1] and Kurose and Ross [2].
13.1 VIDEO ON IP NETWORKS
This section introduces the problem of transmitting video on IP networks such as
the public Internet. We ﬁrst provide an overview of such networks. Then we brieﬂy
introduce the error-resilience features found on common video coders. Finally, we
consider the so-called transport-level coding done at the network level to protect the
resulting error-resilient coded video data.
A basic system diagram for networked transmission of video on a single path is
shown in Figure 13.1–1. In this case the source coder will usually have its error-
resilience features turned on and the network coder will exploit these features at the
Multidimensional Signal, Image, and Video Processing and Coding. DOI: 10.1016/B978-0-12-381420-3.00013-8
c⃝2012 Elsevier Inc. All rights reserved.
529

530
CHAPTER 13 Video Transmission over Networks
Source
coder
Source
decoder
Network
coder
Network
decoder
Network
Decoded
video 
Video
FIGURE 13.1–1
System diagram of networked video transmission.
transport level. The network coder will generally also exploit available feedback on
congestion in the network in its assignment of data to network packets, and choice
of FEC or ACK/NACK strategy, to be discussed later in this chapter. This diagram
is for unicast; thus there is just one receiver for the one video source shown here.
Multicast, or general broadcast,1 introduces additional issues, one of which is the
need for scalability due to the expected heterogeneity of multicast receivers and link
qualities (i.e., usable bandwidth, packet loss, delay and delay variation (jitter), and
bit error rate). While we do not address multicast here, we do look at the scalability-
related problem of digital item adaptation or transcoding later in this chapter.
Overview of IP Networks
Our overview of IP networks starts with the basic concept of a best-efforts network.
We brieﬂy introduce the various network levels, followed by an introduction to the
transfer control protocol (TCP) and the real-time protocol (RTP) for video. Since the
majority of network trafﬁc is TCP, it is necessary that video transmissions over RTP
be what is called TCP-friendly, meaning that, at least on the average, each RTP ﬂow
uses a fraction of network resources similar to that of a TCP ﬂow. We then discuss
how these concepts are used in packet loss prevention.
Best-Efforts Network
Most IP networks, including the public Internet, are best-efforts networks, meaning
that there are no guarantees of speciﬁc performance, only that the network protocol is
fair and makes a best effort to get the packets to their destination, perhaps in a some-
what reordered sequence with various delays. At intermediate nodes in the network,
packets are stored at buffers of ﬁnite size that can overﬂow. When such overﬂow
1The term “broadcast” may mean different things in the video coding community and the networks
community. Here, we simply mean a very large-scale multicast to perhaps millions of subscribers, not
a wholesale ﬂooding of the entire network.

13.1 Video on IP Networks
531
occurs, then the packet is lost, and this packet loss is the primary error that occurs
in modern wired IP networks. So these networks cannot offer a guaranteed quality of
service (QoS) such as is obtained on the private switched networks T1, T2, or ATM.
However, signiﬁcant work of the Internet Engineering Task Force (IETF) has con-
centrated on differentiated services (DiffServ) [3], wherein various priorities can be
given to classes of packet trafﬁc.
Layers and Protocol Stack
Figure 13.1–2a shows the open system interconnection (OSI) network layers, also
called a protocol stack [1]. The lowest level is the physical layer, called layer 1,
the level of bits. At level 2, the data link layer, the binary data are aggregated into
frames, while the next higher layer 3, the network layer, deals only with packets. In
the case of bit errors that cannot be corrected, a packet is rejected at the data link
layer and so not passed up to the network level. However, bit errors are signiﬁcant
only in wireless networks. For our purposes we can skip the presentation and session
layers and go directly to the application layer (see Figure 13.1–2b), where our video
coders and decoders sit. On transmission the application sends a message down the
protocol stack, with each level requesting services from the one below. On reception,
the information is passed back up the stack. Hopefully, the original message is then
reconstructed.
The presentation and session layers are not mentioned in [2], compressing the
stack to ﬁve layers, typical in Internet discussions. The IP works in the network layer.
The TCP works in the transport layer. The IP layer offers no reliability features. It
just packetizes the data, adds a destination header, and sends it on its way. But the IP
protocol at the network layer is responsible for all the routing or path selection to get
from one node to another, all through the network to the ﬁnal addressee node.
Data link
Network
Transport
Session
Presentation
Application
Physical
Data link
Network
Transport
Application
Physical
(a) OSI layers
(b) IP network layers
FIGURE 13.1–2
Network reference models (stack).

532
CHAPTER 13 Video Transmission over Networks
TCP at Transport Level
The extremely reliable TCP controls the main load of the Internet trafﬁc. It attains
the reliability needed for general computer data by retransmitting any lost IP
packets, thus incurring a possibly signiﬁcant increased delay upon packet loss. Most
media trafﬁc does not require such high reliability, but conversely cannot tolerate
excessive variation in time delay, so-called time jitter. The ubiquitous TCP also
contains congestion-avoidance features, which heavily modulate its sending rate to
avoid packet loss, signaled to the sender by small backward-ﬂowing control packets.
TCP ﬂows start up slowly and upon encountering signiﬁcant packet loss, the send-
ing rate is drastically reduced (by half) through a process called additive-increase
multiplicative-decrease [2]. This increase and decrease is conducted at network
epochs as determined by the so-called round trip time (RTT), the time it takes a
packet to make a round trip from source to receiver.
RTP at the Application Layer
For real-time transmission, RTP does not employ TCP at all. The RTP protocol oper-
ates at the application layer and uses the user datagram protocol (UDP), which unlike
TCP simply employs single IP transmissions with no checking for ACKs and no
retransmissions. Each UDP packet or datagram is sent separately, with no throttling
down of rate. Thus video sent via RTP will soon dominate the ﬂows on a network
unless some kind of rate control is applied.
TCP Friendly
The ﬂow of video packets via RTP/UDP must be controlled so as not to dominate the
bulk of network trafﬁc, which ﬂows via TCP. Various TCP-friendly rate equations
have been devised [4, 5] that have been shown to use on average a nearly equal
amount of network bandwidth as TCP. One such equation [5] is
RTCP =
MTU
RTT
q
2p
3 + RTO
q
27p
8 p(1 + 32p2)
,
(13.1–1)
where RTT is the round trip time, RTO is the receiver time-out time (i.e., the time
that a sender can wait for the ACK before it declares the packet lost), MTU is the
maximum transmission unit (packet) size, and p is the packet loss probability, also
called packet-loss rate. If the RTP video sender follows this equation, then its sending
rate will be TCP friendly and should avoid being a cause of network congestion.
As mentioned previously, the main loss mechanism in wired IP networks is packet
overﬂow at the ﬁrst-in ﬁrst-out (FIFO) internal network buffers. If the source con-
trols its sending rate according to (13.1–1), then its average packet loss should be
approximately that of a TCP ﬂow, which is low at 3–6% [6]. So control of the video
sending rate offers two beneﬁts, one to the sender and one to the network. Bit errors
are not considered a problem in wired networks. Also, any possible small number of
bit errors is detected and not passed up the protocol stack, so from the application’s
viewpoint, the packet is lost. In the wireless case, bit errors are important equally
with packet loss.

13.1 Video on IP Networks
533
Error-Resilient Coding
Error-resilient features are often added to a video coder to make it more robust to
channel errors. Here, we discuss data partitioning and slices that are introduced for
purposes of re-synchronization. We also brieﬂy consider reversible VLCs, which
can be decoded starting from either the beginning or the end of the bitstream. We
introduce the multiple description feature, whereby decoding at partial quality is still
possible even if some of the descriptions (packets) are lost.
Data Partitioning and Slices
If all the coded data is put into one partition, then a problem in transmission can cause
the loss of the entire remaining data, due ﬁrst to the use of VLCs and then to the heavy
use of spatial and temporal prediction in video coders. In fact if synchronization is
lost somewhere, it might never be regained without the insertion of sync words2 into
the bitstream. In MPEG 2, data partition (DP) consisted of just two partitions on
a block basis: one for the motion vector and low frequency DCT coefﬁcients, and
a second one for high frequency coefﬁcients [7]. Similar to DP is the concept of
slice, which breaks a coded frame up into separate coded units, wherein reference
cannot be made to other slices in this frame. Slices start (or end) with a sync word,
and contain position, length of slice in pixels, and other needed information in a
header, permitting the slice to be decoded independent of prior data. But because
slices cannot refer to other slices in the same frame, compression efﬁciency suffers
somewhat.
Reversible Variable Length Coding
Variable length codes such as Huffman are constructed on a tree and thus satisfy
a preﬁx condition, which makes them uniquely decodable (i.e., no codeword is the
preﬁx of another codeword). Also, since the Huffman code is optimal, the code tree is
fully populated (i.e., all leaf nodes are valid messages). If we lose a middle segment
in a string of variable length coding (VLC) codewords, in the absence of other errors,
we can only do correct (forward) decoding from the beginning of the string up to the
lost segment. However, at the error location we lose track and will probably not be
able to correctly decode the rest of the string, after the lost segment. On the other
hand, if the VLC codewords satisfy a sufﬁx condition, then we could start at the end
of the string and decode back to the lost segment, and thus recover the lost data.
Now as mentioned in Chapter 9, most Huffman codes cannot satisfy both a sufﬁx
and a preﬁx condition, but methods that ﬁnd reversible VLCs (RVLCs) starting from
a conventional VLC have been discovered [8]. Unfortunately, the resulting RVLCs
lose signiﬁcant efﬁciency versus Huffman codes.
Regarding structural VLCs, the Golomb-Rice codes are near-optimal codes for
the geometric distribution, and the Exp-Golomb codes are near-optimal for distri-
butions more peaky than exponential that can occur in the prediction residuals of
2A sync word is a long and unique codeword that is guaranteed not to occur naturally in the coded
bitstream. See end-of-chapter problem 1.

534
CHAPTER 13 Video Transmission over Networks
video coding. RVLCs with the same length distribution as Golomb-Rice and Exp-
Golomb were discovered by Wen and Villasenor [9], and Exp-Golomb codes are used
in the video coding standard H.264/AVC. Also in Farber et al. [10] and Girod [11],
a method is presented that achieves bidirectional decoding with conventional VLCs.
In this method, forward and reverse codestreams are combined with a delay for the
maximum length codeword, and the compression efﬁciency is almost the same as for
Huffman coding when the string of source messages is long.
Multiple Description Coding
The main concept in multiple description coding (MDC) is to code the video data into
two or more multiple streams that each carry a near-equal amount of information. If
all the streams are received, then decoding proceeds normally. If fewer descriptions
are received, then the video can still be decoded, but at a reduced quality. Also, the
quality should be almost independent of which descriptions are received [12]. If we
now think of the multiple descriptions as packets sent out on a best-efforts network,
we can see the value of an MDC coding method for networked video.
Example 13.1–1: Scalar MDC Quantizer
Consider an example where we have to quantize a scalar random variable X whose range
is [1,10], with a quantizer having 10 equally spaced output values: 1,2,...,9,10. Call this
the original quantizer. We can also accomplish this with two separate quantizers, called
even quantizer and odd quantizer, each with step size 2, that output even and odd rep-
resentation values, respectively. Clearly, if we receive either even or odd quantizer output,
we can reconstruct the value of X up to an error of ±1, and if we receive both even and
odd quantizer outputs, then this is the same as receiving the original quantizer output
x
3
2
1
4
5
6
10
9
8
7
2
4
6
8
10
1
3
5
7
9
Q(x)
FIGURE 13.1–3
Illustration of an MDC scalar quantizer.

13.1 Video on IP Networks
535
having 10 levels, so that the maximum output error is now ±0.5. In Figure 13.1–3 the
even quantizer output levels are denoted by the gray lines and the odd quantizer output
levels are denoted by the black lines. We can see a redundancy in the two quantizers,
because the gray and black horizontal lines overlap horizontally.
One simple consequence we see from this example is that the MDC method intro-
duces a redundancy into the two (or more) codestreams or descriptions. It is this
redundancy that protects the data against loss of one or another bitstream. The con-
cept of MDC has been extended far beyond simple scalar quantizers, including MD
transform coding [13] and the MD-FEC coding [14], that we will apply to a layered
codestream later in this chapter.
Transport-Level Error Control
The preceding error-resilient features can be applied in the video coding or applica-
tion layer. Then the packets are passed down to the transport level for transmission
over the channel or network. We look at two powerful and somewhat com-
plementary techniques, error control coding and acknowledgement-retransmission
(ACK/NACK). Error control coding is sometimes called forward error correction
(FEC) because only a forward channel is used. However, in a packet network there is
usually a backward channel, so that acknowledgments can be fed back from receiver
to transmitter, resulting in the familiar ACK/NAK signal. Using FEC we must know
the present channel quality fairly well or risk wasting error control (parity) bits on the
one hand, or not having enough parity bits to correct the error on the other hand. In the
simpler ACK/NAK case, we do not compromise the forward channel bandwidth at all
and only transmit on the backward channel a very small ACK/NAK packet, but we
do need the existence of this backward channel. In delay-sensitive applications like
visual conferencing, we generally cannot afford to wait for the ACK and the subse-
quent retransmission, due to stringent total delay requirements (≤250 msec [15]).
Some data “channels” where there is no backward channel are the CD, the DVD, and
TV broadcast. There is a back channel in Internet unicast, multicast, and broadcast.
However, in the latter two, multicast and broadcast, there is the need to consolidate
the user feedback at overlay nodes to make the system scale to possibilities of large
numbers of users.
Forward Error Control Coding
The FEC technique is used in many communication systems. In the simplest case, it
consists of a block coding wherein a number n −k of parity bits are added to k binary
information bits to create a binary channel codeword of length n. The Hamming codes
are examples of binary linear codes, where the codewords exist in n-dimensional
binary space. Each code is characterized by its minimum Hamming distance dmin,
deﬁned as the minimum number of bit differences between two different codewords.
Thus, it takes dmin bit errors to change one codeword into another. So the error detec-
tion capability of a code is dmin −1, and the error correction capability of a code

536
CHAPTER 13 Video Transmission over Networks
is ⌊dmin/2⌋, where ⌊·⌋is the least integer function. This last is so because if fewer
than ⌊dmin/2⌋errors occur, the received string is still closer (in Hamming distance)
to its error-free version than to any other codeword. Reed-Solomon (RS) codes are
also linear, but operate on symbols in a so-called Galois ﬁeld with 2l elements. Code-
words, parity words, and minimal distance are all computed using the arithmetic of
this ﬁeld. An example is l = 4, which corresponds to hexadecimal arithmetic with 16
symbols. The (n,k) = (15,9) RS code has hexadecimal symbols and can correct 3
symbol errors. It codes 9 hexadecimal information symbols (36 bits) into 15 symbol
codewords (60 bits) [16]. The RS codes are perfect codes, meaning that the minimum
distance between codewords attains the maximum value dmin = n −k + 1 [17]. Thus
an (n,k) RS code can detect up to n −k symbol errors. The RS codes are very good
for bursts of errors since a short symbol error burst translates into an l times longer
binary error burst, when the symbols are written in terms of their l−bit binary code
[18]. These RS codes are used in the CD and DVD standards to correct error bursts
on decoding.
Automatic Repeat Request
A simple alternative to using error control coding is the automatic repeat request
(ARQ) strategy of acknowledgement and retransmission. It is particularly attractive
in the context of an IP network and is used exclusively by TCP in the transport layer.
There is no explicit expansion needed in available bandwidth, as would be the case
with FEC, and no extra congestion, unless a packet is not acknowledged (i.e., no
ACK is received). TCP has a certain timeout interval [2], at which time the sender
acts by retransmitting the unacknowledged packet. Usually the timeout is set to be
larger than the RTT. (Note that this can lead to duplicate packets being received under
some circumstances.) At the network layer, the IP protocol has a header check sum
that can cause packets to be discarded there. While ARQ techniques typically result
in too much delay for visual conferencing, they are quite suitable for video streaming,
where playout buffers are typically 5 seconds or more in length.
Wireless Networks
The situation in video coding for wireless networks is less settled than that for the
usually more benign wired case. In a well-functioning wired IP network, usually
bit errors in the cells (packets) can almost be ignored, being of quite low prob-
ability, especially at the application layer. In the wireless case, the SNR is highly
variable, with bit errors occurring in bursts and at a much higher rate. Methods for
transporting video over such networks must consider both packet loss and bursty bit
errors.
The common protocol TCP is not especially suitable for the wireless network
because it interprets packet loss as indicating congestion, while in a wireless network
packet loss may be largely due to bit errors, in turn causing the packet to be deleted
at the link layer [2]. Variations of TCP have been developed for wireless transmis-
sion [19], one of which is to just provide for link layer retransmissions. So-called

13.1 Video on IP Networks
537
cross-layer techniques have become popular [20] for exploiting those packets with
bit errors. A simple cross-layer example is that packets with detected errors could be
sent up to the application level instead of being discarded. There, RVLCs can be used
to decode the packet from each end until reaching the location where the error was
detected, thus recovering much of the loss.
Joint Source-Channel Coding
A most powerful possibility is to design the video coder and channel coder together
to optimize overall performance in the operational rate-distortion sense. If we add
FEC to the source coder bitrate, then we have a higher channel bitrate. For exam-
ple, a channel rate RC = 1/2 Reed-Solomon (RS) code would double the channel
bitrate. This channel bitrate translates into a probability of bit error. Depending on
the statistics of the channel (i.e., independent bit errors or bursts), the bit error can
be converted to a word error and frame error, where a frame would correspond to a
packet, which may hold a slice or a number of slices. Typically these packets with bit
errors will not be passed up to the application layer and will be considered lost pack-
ets. Finally, considering error concealment provisions in the decoder, this packet-loss
rate can translate into an average PSNR for the decoded video frames. Optimization
of the overall system is called joint source channel coding (JSCC), and essentially
consists of trading off the source coder bitrate versus FEC parity bits for the purpose
of increasing the average decoder PSNR. Modestino and Daut [21] published an early
paper introducing JSCC for image transmission.
Bystrom and Modestino [22] introduced the idea of a universal distortion-rate
characteristic for the JSCC problem. Writing the total distortion from source and
channel coding as DS+C, they express this total distortion as a function of inverse
bit-error probability on the channel pb for various source coding rates RS. A sketch
of such a function is shown in Figure 13.1–4. This function can be obtained either
analytically or via simulation for the source coder in question, but precomputing such
1/pb
DS +C
RS
FIGURE 13.1–4
Total distortion plotted versus reciprocal of channel bit error probability, with source coding
rate as a parameter.

538
CHAPTER 13 Video Transmission over Networks
a curve can be computationally demanding. The characteristic does not depend on
channel coding, modulation, or channel model. While the method does assume that
successive bit errors are independent, the authors in [22] state that this situation can
be well approximated by standard interleaving methods.
This universal distortion-rate characteristic of the source can be combined with a
standard bit-error probability curve of the channel, as seen in Figure 13.1–5, to obtain
a plot of DS+C versus either source rate RS (source bits/pixel) or what the authors in
[22] call RS+C ≜RS/RC, where RC is the channel coding rate, expressed in terms
of source bits divided by total bits (i.e., source bits plus parity bits). The units of
RS+C are then bits/c.u., where c.u. is channel use. The resulting curve is sketched in
Figure 13.1–6.
To obtain such a curve for a certain Eb/N0 or SNRi on the channel, we ﬁrst ﬁnd pb
from Figure 13.1–5. Then we plot the corresponding vertical line on Figure 13.1–4,
pb
Eb/N0
FIGURE 13.1–5
Probability of bit error versus channel SNR.
DS +C
RS+C
FIGURE 13.1–6
Total distortion versus source bits per channel use.

13.1 Video on IP Networks
539
from which we can generate the plot in Figure 13.1–6 for a particular RC of the used
FEC, assumed modulation, and channel model. Some real applications to an ECSQ
subband video coder and an MPEG 2 coder over an additive white Gaussian noise
(AWGN) channel are contained in [22].
Error Concealment
Conventional source decoding does not make use of any a priori information about
the video source, such as a source model or power spectral density, in effect mak-
ing it an unbiased or maximum likelihood source decoder. However, better PSNR
and visual performance can be expected by using model-based decoding, e.g., a post-
processing ﬁlter. For block-based intraframe coders, early postprocessing concealed
blocking artifacts at low bitrates by smoothing nearest neighbor columns and rows
along DCT block boundaries.
Turning to channel errors and considering the typical losses that can occur in
network transmission, various error-concealment measures have been introduced to
counter the common situations where blocks and reference blocks in prior frames or
motion vectors are missing, due to packet loss. If the motion vector for a block is
missing, it is common to use an average of neighboring motion vectors to perform
the needed prediction. If the coded residue or displaced frame difference (DFD) is
missing for a predicted block (or region), often just the prediction is used without
any update. If an I block (or region) is lost, then some kind of spatial prediction from
the neighborhood is often used to ﬁll in the gap. Such schemes can work quite well for
missing data, especially in rather smooth regions of continuous motion. Directional
interpolation based on local geometric structure has also been found useful for ﬁlling
in missing blocks [23].
A particularly effective method to deal with lost or erroneous motion vectors was
presented by Lam et al. [24]. A boundary-matching algorithm is proposed to estimate
a missing motion vector from a set of candidates. The candidates include the motion
vector for the same block in the previous frame, the available neighboring motion
vectors, an average of available neighboring motion vectors, and the median of the
available neighboring motion vectors. The estimate is then chosen that minimizes
a side-match error—i.e., the sum of squares of the ﬁrst-order difference across the
available top, left, and bottom block boundaries. A summary of error concealment
features in the H.26L test model is presented by Wang et al. [25].
Some error concealment features of H.264/AVC will be presented in Section 13.3.
In connection with the standards, though, it should be noted that error concealment is
nonnormative, meaning that error concealment is not a mandated part of the standard.
Implementors are thus free to use any error concealment methods they ﬁnd appropri-
ate for the standard encoded bitstreams. When we speak of error concealment features
of H.26L or H.264/AVC, we mean error concealment features of the test models for
these coders. An overview of early error concealment methods is contained in the
Katsaggelos and Galatsanos book [26].

540
CHAPTER 13 Video Transmission over Networks
13.2 ROBUST SWT VIDEO CODING (BAJI´C)
In this section we introduce dispersive packetization, which can help the error con-
cealment at the receiver to deal with losses. We also introduce a combination of MDC
with FEC that is especially suitable for matching scalable and embedded coders to a
best-efforts network such as the current Internet.
Dispersive Packetization
Error concealment is a popular strategy for reducing the effects of packet losses
in image and video transmission. When some data from the compressed bitstream
(such as a set of macroblocks, subband samples, and/or motion vectors) are lost, the
decoder attempts to estimate the missing pieces of data from the available ones. The
encoder can help improve estimation performance at the decoder side by packetiz-
ing the data in a manner that facilitates error concealment. One such approach is
dispersive packetization (DP).
The earliest work in this area seems to be the even–odd splitting of coded
speech samples by Jayant [27]. Neighboring speech samples are more correlated
than samples that are far from each other. When two channels are available for
speech transmission, sending even samples over one of the channels and odd sam-
ples over the other will facilitate error concealment at the decoder if one of the
channels fails. This is because each missing even sample will be surrounded by
two available odd samples, which can serve as a basis for interpolation, and vice
versa. If multiple channels (or packets) are available, a good strategy would be to
group together samples that are maximally separated from each other. For exam-
ple, if N channels (packets) are available, the ith group would consist of samples
with indices {i,i + N,i + 2N,...}. In this case, if any one group of samples is lost,
each sample from that group would have N −1 available neighboring samples on
either side. Even–odd splitting is a special case of this strategy for N = 2. The idea
of splitting neighboring samples into groups that are transmitted independently (over
different channels, or in different packets), also forms the basis of DP of images and
video.
DP of Images
In the 1-D example of a speech signal, splitting samples into N groups that contain
maximally separated samples amounts to simple extraction of N possible phases of
the speech signal subsampled by a factor of N. In the case of multidimensional sig-
nals, subsampling by a factor of N can be accomplished in many different ways (see
Chapter 2 and also [28]). The particular subsampling pattern that maximizes the dis-
tance between the samples that are grouped together is the one that solves the sphere
packing problem [29] in the signal domain. Digital images are usually deﬁned on a
subset of the 2-D integer lattice Z2. A fast algorithm for sphere packing onto Z2 was
proposed in [30], and it can be used directly for DP of raw images.

13.2 Robust SWT Video Coding (Baji´c)
541
(a)
(b)
Source
Splitting
Transform
Quantization
Entropy
coding
Source
Splitting
Transform
Quantization
Entropy
coding
FIGURE 13.2–1
Splitting of source samples can be done (a) before the transform, or (b) after the transform.
In practice, however, we are often interested in transmitting transform-coded
images. In this case, there are two ways to accomplish DP, as shown in Figure 13.2–1.
Samples can be split either before (Figure 13.2–1a) or after the transform
(Figure 13.2–1b). The former approach was used in [31], where the authors propose
splitting the image into four subsampled versions and then encoding each version by
the conventional JPEG coder. The latter approach was used in several works targeted
at SWT-coded images [32, 33]. Based on the results reported in these papers, splitting
samples before the transform yields lower compression efﬁciency but higher error
resilience, while splitting after the transform gives higher compression efﬁciency
with lower error resilience. For example, the results in [31] for the Lena image show
the coding efﬁciency reduction of over 3 dB in comparison with conventional JPEG,
while acceptable image quality is obtained with losses of up to 75%. On the other
hand, the methods in [34] and [33] are only about 1 dB less efﬁcient than SPIHT [35]
(which is a couple of dB better than JPEG on the Lena image), but are targeted at
lower losses, up to 10–20%.
We now describe the DP method of [33] in more detail. Figure 13.2–2 shows a
two-level SWT of an image. One sample from the lowest frequency LL subband is
shown in black, and its neighborhood in the space-frequency domain is shown in gray.
In DP, we try to store neighboring samples in different packets. If the packet contain-
ing the sample shown in black is lost, many of its neighbors will still be available.
Due to the intraband and interband relationships among subband/wavelet samples,
the fact that many neighbors of a missing sample are available can facilitate error
concealment.
Given the target image size and the maximum allowed packet size (both in bytes),
we can determine the suitable number of packets, say N. Using the lattice partition-
ing method from [30], a suitable 2-D subsampling pattern p : Z2 →{0,1,...,N −1}
can be constructed for the subsampling factor of N. This pattern is used in the LL
subband; the sample at location (i,j) in the LL subband is stored in the packet p(i,j).
Subsampling patterns for the higher frequency patterns are obtained by modulo shift-
ing in the following way. Starting from the LL subband and going toward higher

542
CHAPTER 13 Video Transmission over Networks
FIGURE 13.2–2
One low-frequency SWT sample (black) and its space-frequency neighborhood (gray).
frequencies in a Z-scan, label the subbands in increasing order from 0 to K. Then,
the sample at location (i,j) in subband k is stored in packet (p(i,j) + k)modN. As
an example, Figure 13.2–3 shows the resulting packetization into N = 4 packets of
a 16 × 16 image with two levels of subband/wavelet decomposition. One LL sample
from packet number 3 is shown in black. Note that out of a total of 23 space-frequency
neighbors (shown in gray) of this LL sample, only three are stored in the same packet.
For details on the entropy coding and quantization, refer to [33] and references
therein.
As mentioned before, DP facilitates easy error concealment. Even simple error
concealment algorithms can produce good results when coupled with DP. For exam-
ple, in [33] error concealment is carried out in the three subbands (LL, HL, and
LH) at the lowest decomposition level, where most of the signal energy is usu-
ally concentrated. Missing samples in the LL subband are interpolated bilinearly
from the four nearest available samples in the horizontal and vertical directions.
Missing samples in the LH and HL subbands are interpolated linearly from near-
est available samples in the direction in which the subband has been lowpass ﬁltered.
Missing samples in higher frequency subbands are set to zero. More sophisticated
error concealment strategies may bring further improvements [36]. As an example,
Figure 13.2–4 shows PSNR versus packet loss comparison between the packetized
zerotree wavelet (PZW) method from [34] and two versions of DP—one paired
with bilinear concealment [32] and the other with adaptive maximum a posteriori

13.2 Robust SWT Video Coding (Baji´c)
543
3
0
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
1
2
2
2
2
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
FIGURE 13.2–3
Example of a 16 × 16 image with two levels of SWT decomposition, packetized into four
packets.
0
5
10
15
20
23
24
25
26
27
28
29
30
31
32
33
% loss
PSNR (dB)
DP + Adaptive maximum a posteriori concealment
DP + Bilinear concealment
Packetized zerotree wavelet 
25
FIGURE 13.2–4
PSNR versus packet loss on the 512 × 512 monochrome Lena image.

544
CHAPTER 13 Video Transmission over Networks
concealment [36]. The example is for the 512 × 512 grayscale Lena image encoded at
0.21 bpp.
DP of Video
Two-dimensional concepts from the previous section can be extended to three dimen-
sions for DP of video. For example, Figure 13.2–5 shows how alternating the
packetization pattern from frame to frame (by adding 1 modulo N) will ensure that
samples from a common SWT neighborhood will be stored in different packets. This
type of packetization can be used for intraframe-coded video. When motion compen-
sation is employed, each level of the motion-compensated spatiotemporal pyramid
consists of an alternating sequence of frames and motion vector ﬁelds. In this case,
we want to ensure that motion vectors are not stored in the same packet as the sam-
ples they point to. Again, adding 1 modulo N to the packetization pattern used for
samples will shufﬂe the pattern so that it can be used for motion vector packetization,
as shown in Figure 13.2–6.
To see the performance of video DP, we present an example that involves the
grayscale SIF football sequence at 30 fps. The sequence was encoded at 1.34 Mbps
with a group of pictures (GOP) size of four frames using the DP video coder from
Bajic and Woods [33]. The Gilbert model was used to simulate transmission over a
1
Frame 1
Frame 0
2
3
3
3
0
0
0
0
0
1
1
1
1
1
1
3
0
2
3
0
1
2
3
0
1
3
0
0
0
0
1
1
1
2
2
2
2
2
3
3
3
3
3
3
0
1
2
2
2
2
2
0
1
2
3
0
1
2
3
3
0
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
1
2
2
2
2
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
FIGURE 13.2–5
Extension of DP to three dimensions.

13.2 Robust SWT Video Coding (Baji´c)
545
3
Frame
Domain of the motion vector field
3
3
3
3
0
0
0
0
1
1
1
1
2
2
2
2
0
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
1
2
2
2
2
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
FIGURE 13.2–6
DP of motion-compensated SWT video.
packet-based network, with the “good” state representing packet reception and the
“bad” state representing packet loss. Average packet loss was 10.4%.
Figure 13.2–7 shows the results of a sample run from the simulation. The top part
shows the PSNR of the decoded video in three cases: no loss, DP, and slice-based
packetization (SP). The SP video is similar to the H.26x and MPEGx recommen-
dations for packet-lossy environments, where each slice (a row of macroblocks) is
stored in a separate packet. The bottom part of the ﬁgure shows the corresponding
packet loss proﬁle. In this case, DP provides an average 3- to 4-dB advantage in
PSNR over SP.
Visual comparison between DP and SP is shown in Figure 13.2–8, which cor-
responds to frame 87 of the football sequence, with 10% loss. The ﬁgure shows
(a) original frame, (b) coded frame with no loss (PSNR = 27.7 dB), (c) decoded
SP frame (PSNR = 20.6 dB), and (d) decoded DP frame (PSNR = 22.2 dB); e and
f are zoomed-in segments of c and d, respectively, illustrating the performance of
the two methods on small details. Observe that details are better preserved in the DP
frame. Video is available for download at the book’s Web site.
Multiple Description FEC
The multiple description forward error correction (MD-FEC) technique is a way
of combining information and parity bits into descriptions or packets in order to
achieve robustness against packet loss through unequal error protection. Among the

546
CHAPTER 13 Video Transmission over Networks
0
10
20
30
40
50
60
70
80
90
100
0
5
10
15
20
25
30
Average packet loss per GOP of 4 frames
Frame number
% loss
0
10
20
30
40
50
60
70
80
90
100
5
10
15
20
25
30
35
40
Comparison of our method with slice-based packetization
Frame number
PSNR (dB)
No loss
DP
SP
FIGURE 13.2–7
Comparitive performance of DP and SP schemes on the football sequence.
ﬁrst works on the topic was the priority encoding transmission (PET) of Albanese
et al. [37]. In PET, a message is broken into smaller segments, and each segment
is assigned a priority level between 0 and 1. Then, each segment is encoded into a
set of packets using a version of the RS error-control codes in a way that guarantees
that the segment is decodable if a fraction of the number of packets at least equal
to its priority, is received. Later, MD-FEC [38, 39] used a very similar strategy, but
assumed that the message (image or video clip) is encoded into an embedded or SNR
scalable bitstream. This also facilitates assignment of priorities based on distortion-
rate (D −R) characteristics. Our description of the method follows the work of
Puri et al. [38].
Consider a message encoded into a scalable and embedded bitstream. The bit-
stream is ﬁrst divided into N sections: (0,R1],(R1,R2],..., (RN−1,RN]. Section k is
split into k subsections, and encoded by a (N,k) RS code. Such a code can correct
up to N −k erasures. Individual symbols of RS codewords are concatenated to make
descriptions. Each description is stored in a separate packet, so we will use the terms
“description” and “packet” interchangeably. An illustration of the MD-FEC proce-
dure for N = 4 is given in Figure 13.2–9. In this example, section 1 is protected

13.2 Robust SWT Video Coding (Baji´c)
547
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 13.2–8
DP demonstration: frame 87 of the football sequence; SIF at 1.34 Mbps.
by RS (4,1) code, section 2 by RS (4,2) code, section 3 by RS (4,3) code, while
section 4 is unprotected. If no packets are lost, the bitstream can be decoded up to
R4. If one packet is lost, we are guaranteed to be able to decode up to R3, since
RS (4,1) can correct any single erasure. However, if the one packet that was lost
was packet 4, then we can decode all the way up to R3 + 3(R4 −R3)/4. In gen-
eral, if j out of N packets are received, we can decode up to some rate in the range
[Rj,Rj + j(Rj+1 −Rj)/( j + 1)].
Given the channel packet-loss characteristics, the goal of FEC assignment in
MD-FEC is to minimize the expected distortion subject to a total rate constraint.
In an embedded bitstream, we have D(Ri) ≤D(Rj) for Ri > Rj, and this fact can be
used to reduce computational burden. Designing the MD-FEC amounts to a joint
optimization over both the source coder (i.e., how much of the source bitstream to
take) and channel coder (i.e., how much parity [FEC chunks] to insert).

548
CHAPTER 13 Video Transmission over Networks
Section
FEC
FEC
FEC
FEC
FEC
FEC
1
1
2
R1
R2
R3
R4
2
3
3
4
4
Description 1
Description 2
Description 3
Description 4
FIGURE 13.2–9
A simple illustration of MD-FEC.
If we characterize the channel as a packet-loss channel, this means that packets
can be lost but not corrupted. Letting the probability of exactly k packets being lost
be pk, we can write the overall expected distortion as
Davg =
N
X
k=1
pkD(Rk)
(13.2–1)
and seek to minimize this quantity over all choices of source-rate breakpoints
0 ≤R1 ≤R2 ≤··· ≤RN such that the bandwidth B of the channel is not exceeded.
We can write this channel bandwidth constraint as
N
X
k=1
αkRk ≤B,
(13.2–2)
for an appropriate choice of the constants αk (see Figure 13.2–9 and end-of-chapter
problem 4). For a convex D(R) function, we have that Davg is a convex function of
rate vector (R1,R2,...,RN) subject to inequality constraints and can be solved by any
number of optimization methods. The reader is referred to [35] for a possible fast
solution.
Robust 3D-SPIHT
The 3D-SPIHT spatiotemporal extension [40] of the SPIHT image coder (cf. Section
9.6), was used in [38] as the video coder to produce an embedded bitstream for
MD-FEC. In a typical application, MD-FEC would be applied on a GOP by GOP

13.2 Robust SWT Video Coding (Baji´c)
549
MD-FEC
Network
FEC
decoding
MC-EZBC
decoder
MC error
concealment
To display
MC-EZBC video bitstream
MC-EZBC
encoded video
Motion vectors
Subband coefficients
FIGURE 13.2–10
Video streaming system based on MC-EZBC.
basis. Channel conditions are monitored throughout the streaming session, and for
each GOP, an FEC assignment is carried out using recent estimates of the channel
loss statistics.
A Robust MC-EZBC
One can also use MC-EZBC (cf. Section 12.5) to produce an embedded bitstream
for MD-FEC. Since motion vectors form the most important part of interframe video
data, they would be placed ﬁrst in the bitstream, followed by SWT samples encoded
in an embedded manner. A video streaming system based on MC-EZBC was pro-
posed in [41]. The block diagram of the system is shown in Figure 13.2–10. As
before, MD-FEC is applied on a GOP-by-GOP basis. A major difference from the
system based on 3D-SPIHT [40] is the MC error-concealment module at the decoder.
Since motion vectors receive the most protection; they are the most likely part of
the bitstream to be received. MC error concealment is used to recover from short
burst losses that are hard to predict by simply monitoring channel conditions and are
therefore often not compensated for by MD-FEC.
Example 13.2–1: MD-FEC
This example illustrates the performance of MD-FEC for video transmission. The Mobile
Calendar sequence (SIF resolution, 30 fps) was encoded into an SNR-scalable bit-
stream using the MC-EZBC video coder. The sequence was encoded with a GOP size
of 16 frames. FEC assignment was computed for each GOP separately, with 64 packets
per GOP and a total rate of 1.0 Mbps, assuming a random packet loss of 10%. Video
transmission was then simulated over a two-state Markov or Gilbert channel, which is
often used to capture the bursty nature of packet loss in the Internet. This channel has
two states: “good,” corresponding to successful packet reception, and “bad,” correspond-
ing to packet loss. The channel was simulated for 10% average loss rate and an average
burst length of two packets.

550
CHAPTER 13 Video Transmission over Networks
10
15
20
25
30
Frame-by-frame PSNR for mobile calendar 
Frame number
PSNR (dB)
0
10
20
30
40
50
60
70
80
90
100 
0
10
20
30
40
50
60
70
80
90
100 
0
5
10
15
20
25
Frame-by-frame packet loss 
Frame number 
% packet loss
Expected loss 
Actual loss 
FIGURE 13.2–11
MD-FEC simulation result on the mobile calendar clip.
Results from a sample simulation run are shown in Figure 13.2–11. The top part of the
ﬁgure shows PSNR for 96 frames of the sequence. The bottom part shows the expected
loss rate of 10% (solid line) and the actual loss (dashed line). As long as the actual loss
does not deviate much from the expected value used in FEC assignment, MD-FEC is able
to provide consistent video quality at the receiver. (See robust SWT video results at this
book’s Web site.)
13.3 ERROR-RESILIENCE FEATURES OF H.264/AVC
The current VCEG/MPEG coding standard H.264/AVC inherits several error-
resilient features from earlier standards such as H.263 and H.26L, but also adds
several new ones. These features of the source coder are intended to work together
with the transport error protection features of the network to achieve a needed
QoS. The error-resilience features in H.264/AVC are syntax, data partitioning, slice
interleaving, ﬂexible macroblock ordering, SP/SI switching frames, reference frame
selection, intrablock refreshing, and error concealment [42].

13.3 Error-Resilience Features of H.264/AVC
551
H.264/AVC is conceptually separated into a video coding layer (VCL) and a net-
work abstraction layer (NAL). The output of the VCL is a slice, consisting of an
integer number of macroblocks. These slices are independently decodable due to the
fact that positional information is included in the header, and no spatial references are
allowed outside the slice. Clearly, the shorter the slice, the lower the probability of
loss or other compromise. For highest efﬁciency, there should be one slice per frame,
but in poorer channels or loss environments, slices consisting of even a single row of
macroblocks have been found useful. The role of the NAL is to map slices to trans-
mission units of the various types of networks, e.g., map to packets for an IP-based
network.
Syntax
The ﬁrst line of defense from channel errors in video coding standards is the seman-
tics and syntax [42]. If illegal syntax elements are received, then it is known that
an error has occurred. At this point, error correction can be attempted or the data
declared lost, and error concealment can commence.
Data Partitioning
By using the data partitioning (DP3) feature, more important data can be pro-
tected with stronger error-control features, such as FEC and acknowledgement-
retransmission, to realize unequal error protection (UEP). In H.264/AVC, coded data
may be partitioned into three partitions: A, B, and C. In partition A is header infor-
mation including slice position. Partition B contains coded block patterns for intra
blocks, followed by their transform coefﬁcients. Partition C contains coded block pat-
terns for inter blocks and their transform coefﬁcients. Coded block patterns contain
the position and size information for each type of block, and incur a small overhead
penalty for choosing DP.
Slice Interleaving and Flexible Macroblock Ordering
An innovative feature of H.264/AVC is ﬂexible macroblock ordering (FMO) that
generalizes slice interleaving from earlier standards. The FMO option permits mac-
roblocks to be distributed across slices in such a way as to aid error concealment
when a single slice is lost. For example, we can have two slices, as indicated in
Figure 13.3–1 (a or b). The indicated squares can be as small as macroblocks of
16 × 16 pixels, or they could be larger. Figure 13.3–1a shows two interleaved slices,
the gray and the white. If one of the two interleaved slices is lost, in many cases it
will be possible to interpolate the missing areas from the remaining slice, both above
and below.
In Figure 13.3–1b, a gray macroblock can more easily be interpolated if its slice is
lost because it has four nearest neighbor macroblocks from the white slice, presumed
received. On the other hand, such extreme macroblock dispersion as shown in this
3Please do not confuse with dispersive packetization. The meaning of “DP” should be clear in context.

552
CHAPTER 13 Video Transmission over Networks
(a) Two slices—alternate lines
(b) Two slices—checkerboard
FIGURE 13.3–1
An interleaved and FMO mapping of macroblocks onto two slices: the gray and the white
slice.
checkerboard pattern would mean that the nearest neighbor macroblocks could not be
used in compression coding of a given macroblock, resulting in loss of compression
efﬁciency. Of course, we must trade off the beneﬁts and penalties of an FMO pattern
for a given channel error environment. One can think of FMO as a type of dispersive
packetization.
Switching Frames
This feature allows switching between two H.264/AVC bitstreams stored on a video
server. They could correspond to different bitrates, in which case the switching per-
mits adjusting the bitrate at the location of the so-called SP or switching frame, while
not waiting for an I frame. While not as efﬁcient as a P frame, the SP frame can be
much more efﬁcient than an I frame. Another use for a switching frame is to recover
from a lost or corrupted frame, which otherwise would propagate error until reset by
an I frame or slice. This can be accomplished, given the existence of such switching
frames, by storing two coded bitstreams in the video server, one with a conventional
reference frame choice of the immediate past frame, and the second bitstream with a
reference frame choice of a frame further back, say 2 or 3 frames back. Then if the
receiver detects the loss of a frame, and this loss can be fed back to the server quickly
enough, the switch can be made to the second bitstream in time for the computation
and display of the next frame, possibly with a slight freeze-frame delay. The SP frame
can also be used for fast-forward searching through the video program.
With reference to Figure 13.3–2, we illustrate a switching situation, where we
want to switch from the bottom bitstream 1 to the top bitstream 2. Normally this
could only be done at the time of an I frame, since the P frame from bitstream 2
needs the decoded reference frame from bitstream 2 in order to decode properly. If
the decoded frame from bitstream 1 is used, as in our switching scenario, then the
reference would not be as expected and an error would occur and propagate via intra
prediction within the frame and via inter prediction to the following frames. Note
that this can just as well be done on a slice basis as well as the frame basis we are
considering here.

13.3 Error-Resilience Features of H.264/AVC
553
P2
P2
P2
P2
I
P2
P1
P1
P1
P1
P1
P1
P1
P1
P2
P2
P2
SP2
SP12
SP1
I
FIGURE 13.3–2
Illustration of switching from bottom bitstream 1 to top bitstream 2 via switching frame SP12.
The question now is how can this switching be done in such a way as to avoid
error in the frames following the switch. First, the method involves special primary
switching frames SP1 and SP2 inserted into the two bitstreams at the point of the
expected switch. The so-called secondary switching frame SP12 then must have the
same reconstructed value as SP2, even though they have different reference frames
and hence different predictions. The key to accomplishing this is to operate with the
quantized coefﬁcients in the block transform domain, instead of making the predic-
tion in the spatial domain. The details are shown by Karczewicz and Kurceren [43],
where a small efﬁciency penalty is noted.
Reference Frame Selection
In H.264/AVC we can use frames for reference other than the immediately last one.
If the video is being coded now, rather than having been coded previously and stored
on a server, then feedback information from the receiver can indicate when one of
the reference frames is lost; thus the reference frame selection (RFS) feature can alter
the choice of reference frames for encoding the current frame, in such a way as to
exclude the lost frame, and therefore terminate any propagating error. Note that so
long as the feedback arrives within a couple of frame times, it can still be useful in
terminating propagation of an error in the output, which would normally only end at
an I frame or perhaps at the end of the current slice. So RFS can make use of the
multiple reference frame capability of H.264/AVC. Thus multiple reference frames
also have a role in increasing the error resilience.

554
CHAPTER 13 Video Transmission over Networks
Intrablock Refreshing
A basic error-resilience method is to insert intraslices instead of intraframes. These
intra-slices can be inserted in a random manner, so as not to increase the local data
rate the way that intraframes do. This has been found to be desirable for keeping
delay down in visual conferencing. Intraslices terminate error propagation within the
slice, the same way that I frames do for the whole frame.
Error Concealment in H.264/AVC
While error concealment is not part of the normative H.264/AVC standard, there
are certain nonnormative (informative) features in the test model [15]. First, a lost
macroblock can be concealed at the decoder using a similar type of directional inter-
polation (prediction) as used in the coding loop at the encoder. Second, an interframe
error concealment can be made from the received reference blocks, wherein the can-
didate with the smallest boundary matching error [24, 25] is selected, with the zero
motion block from the previous frame always a candidate.
Example 13.3–1: Selection of Slice Size (Soyak and Katsaggelos)
In this example, baseline H.264/AVC is used to code slice sizes consisting of both one
row and three rows of macroblocks. The foreman CIF clip was used at 30 fps and 384
Kbps. Because of the increased permitted use of intra prediction by the larger slice size,
the average luma PSNRs without any packet loss were 36.2 and 36.9 dB, respectively.
Then the packets were subjected to a loss simulation as follows. The smaller packets
from the one-row slices were subjected to a 1% packet loss rate, while the approximately
three-times longer packets from the three-row slices were subjected to a comparable 2.9%
packet loss rate. Simple error concealment was used to estimate any missing macroblocks
using a median of neighboring motion vectors.
The resulting average luma PSNRs became 30.8 and 25.6 dB, respectively. Typical
frames grabbed from the video are shown in Figures 13.3–3 and 13.3–4. We can see
FIGURE 13.3–3
This slice consists of one row of macroblocks.

13.4 Joint Source–Network Coding
555
FIGURE 13.3–4
This slice consists of three contiguous rows of macroblocks.
the greater effect of data loss in Figure 13.3–4. Video results are available in the Network
Video folder at this book’s Web site.
13.4 JOINT SOURCE–NETWORK CODING
Here, we talk about methods for network video coding that make use of overlay net-
works. These are networks that exist on top of the public Internet at the application
level. We will use them for transcoding or data item adaptation and in an extension to
MD-FEC. By joint source–network coding (JSNC), we mean that the source coding
and network coding are performed jointly in an effort to optimize the performance of
the combined system. Data item adaptation can be considered as an example of JSNC.
Finally, we provide an introduction to the capacity-achieving improvement over rout-
ing that is called network coding, focusing on practical network coding of multiple
description packets.
Digital Item Adaptation in MPEG 21
MPEG 21 has the goal of making media, including video, available everywhere
[44, 45], and this includes heterogeneous networks and display environments. Hence
there is a need for adaptation of these digital items in quality (bitrate), spatial resolu-
tion, and frame rate within the network. The digital item adaptation (DIA) engine, as
shown in Figure 13.4–1, was proposed to accomplish this.
Digital items such as video packets enter on the left and are operated upon by
the resource adaptation engine for transcoding to the output-modiﬁed digital item. At
the same time, the descriptor ﬁeld of this digital item needs to be adapted also, to
reﬂect the modiﬁed or remaining content properly, e.g., if 4CIF is converted to CIF,
descriptors of the 4CIF content would be removed with only the CIF content descrip-
tors remaining. There would also be backward ﬂows from the receivers, indicating
the needs of the downstream users as well as information about network congestion,

556
CHAPTER 13 Video Transmission over Networks
Resource
adaptation
engine
Digital item adaptation tools
Descriptor
adaptation
engine
Digital item
Adapted
digital item
Digital item adaptation
R
R′
D′
D
FIGURE 13.4–1
A concept DIA model from ISO document [45].
available bitrates, and bit-error rates, that the DIA engine must consider also in the
required adaptation. A general overview of video adaptation was given by Chang and
Vetro [46].
Fine Grain Adaptive FEC
We have seen that JSNC uses an overlay infrastructure to assist video streaming
to multiple users simultaneously by providing lightweight support at intermediate
overlay nodes. For example, in Figure 13.4–2, overlay data service nodes (DSNs)
construct an overlay network to serve heterogeneous users. Users A to G have differ-
ent video requirements (shown as “frame rate/resolution/available bandwidth”), and
Pa to Pg are the packet-loss rates of different overlay virtual links. We do not con-
sider the design of the overlay network here. While streaming, the server sends out
a single ﬁne-grain-adaptive-FEC (FGA-FEC) coded bitstream based on the highest
user requirement (in this case 30/CIF/3M) and actual network conditions. FGA-FEC
divides each network packet into small blocks and packetizes the FEC-coded bit-
stream in such a way that if any original data packets are actively dropped (adapted
by the DIA engine), the corresponding information in parity bits is also completely
removed. The intermediate DSNs can adapt the FEC-coded bitstream by simply drop-
ping a packet or shortening a packet by removing some of its blocks. Since there
is no FEC decoding/re-encoding, JSNC is very efﬁcient in terms of computation.
Furthermore, the data manipulation is at block level, which is precise in terms of
adaptation.
In this section, we use a fully scalable MC-EZBC video coder [47] to show
the FGA-FEC capabilities. As seen in Chapter 12, MC-EZBC produces embedded

13.4 Joint Source–Network Coding
557
Server
DSN
DSN
DSN
30/CIF/3M
A
B
30/CIF/3M
30/CIF/2M
30/CIF/3M
C
30/CIF/1M
D
15/CIF/1M
30/CIF/1M
E
F
30/QCIF/512k
G
15/CIF/384k
15/CIF/384k
Pb
Pa
P1
P2
Pc
P3
Pd
Pe
Pf
Pg
FIGURE 13.4–2
An example overlay network.
A(0,0,0)
A(0,1,0)
A(0,2,0)
A(0,3,0)
A(0,4,0)
A(1,0,0)
A(1,1,0)
A(1,2,0)
A(1,3,0)
A(1,4,0)
A(2,0,0)
A(2,1,0)
A(2,2,0)
A(2,3,0)
A(2,4,0)
A(3,0,0)
A(3,1,0)
A(3,2,0)
A(3,3,0)
A(3,4,0)
A(4,0,0)
A(4,1,0)
A(4,2,0)
A(4,3,0)
A(4,4,0)
A(4,0.1) 
A(4,1,1) 
A(4,2,1) 
A(4,3,1) 
A(4,4,1) 
A(4,0,2) 
A(4,1,2) 
A(4,2,2) 
A(4,3,2) 
A(4,4,2) 
Resolution
Quality 
Frame rate 
FIGURE 13.4–3
Atom diagram of video scalability dimensions [48].
bitstreams supporting three types of scalabilities—temporal, spatial, and SNR. Here,
we use the same notation as [47]. Each GOP coding unit consists of independently
decodable bitstreams {QMV, QYUV}. Let lt ∈{1,2,...,Lt} denote the temporal scale;
then the MV bitstream QMV can be divided into temporal scales and consists of QMV
lt
for 2 ≤lt ≤Lt. Let ls ∈{1,2,...,Ls} denote the spatial scales. The subband coefﬁ-
cient bitstream QYUV is also divided into temporal scales and further divided into
spatial scales as QYUV
lt,ls , for 2 ≤lt ≤Lt and 1 ≤ls ≤Ls. For example, the video at
one-quarter spatial resolution4 and one-half frame rate is obtained from the bitstream
as Q = {QMV
lt
: 1 ≤lt ≤Lt −1} ∪{QYUV
lt,ls : 1 ≤ls ≤Ls −1}. In every sub-bitstream
QYUV
lt,ls , the luma and chroma subbands Y, U, and V are progressively encoded from
the most signiﬁcant bitplane to the least signiﬁcant bitplane (cf. Fully Embedded
SWT Coders in Section 9.6).
4By ‘one-quarter spatial resolution,’ we mean a factor of one-half in each spatial dimension.

558
CHAPTER 13 Video Transmission over Networks
Scaling in terms of quality is obtained by stopping the decoding process at any
point in bitstream Q. The MC-EZBC–encoded bitstream can be further illustrated as
digital items, as in Figure 13.4–3 [48], which shows the video bitstream in view of
three forms of scalability. The video bitstream is represented in terms of atoms, which
are fractional bitplanes [47]. The notation A(F,Q,R) represents an atom of (frame-
rate, quality, resolution) scalability. Choosing a particular casual subset of atoms
corresponds to scaling the resulting video to the desired resolution, frame rate, and
quality. These small pieces of bitstream are interlaced in the embedded bitstream.
Intermediate DSNs adapt the digital items according to user preferences and network
conditions. Since the adaptation can be implemented as simple dropping of corre-
sponding atoms, DSNs do not need to decode and re-encode the bitstream, making
them very efﬁcient. On the other hand, the adaptation is done based on atoms in a
bitstream, which can approximate the quality of pure source coding.
Example 13.4–1: Adaptation Example
Using the MC-EZBC fully scalable video coder, we have adapted the bitrate in the
network by including limited overhead information in the bitstream, allowing bitrate
adaptation across a range of bitrates for the coastguard and mobile calendar CIF test
clips. Figure 13.4–4 shows the resulting PSNR plot versus bitrate. Two plots are given,
corresponding to the ﬁrst extraction at the server, and then a secondary extraction, corre-
sponding to DIA within the network using six quality layers, whose starting locations are
carried in a small header. We can see that the PSNR performance is only slightly reduced
from that at the video server, where full scaling information is available.
Digital Item Adaptation can be done for nonscalable formats too, but only via the
much more difﬁcult method of transcoding. Next, we show a method of combining
DIA with FEC to increase robustness.
DSNs adapt the video bitstream based on user requirements and available band-
width. When parts of the video bitstream are actively dropped by the DIA engine,
FEC codes need to be updated accordingly. This update of FEC codes has the same
basic requirements as the video coding—efﬁciency (low computation cost) and preci-
sion (if a part of the video data is actively dropped, parity bits protecting that piece of
data should also be removed). Based on these considerations, a precise and efﬁcient
FGA-FEC scheme has been proposed [49, 50] based on RS codes.
Given a piece of video bitstream, shown in Figure 13.4–5 (top line), divided into
sections as A,B,C,...,X, the FGA-FEC further divides each section of bitstream into
equal-size blocks. Smaller block size means ﬁner granularity and better adaptation
precision. In the lower part of Figure 13.4–5, the bitstream is applied vertically onto
the blocks as (A1,A2;B1,...,Bi;C1,...,Cj;...;X1,...,Xn). The RS coding compu-
tation is also applied vertically across these blocks to generate the parity blocks,

13.4 Joint Source–Network Coding
559
0
500
28
30
32
34
PSNR (dB)
PSNR (dB)
36
38
40
1000
1500
2000
2500
Rate (kbps)
3000
3500
0
26
28
30
32
34
36
38
40
Mobile CIF
Coastguard CIF
500
1000
1500
2000
2500
Rate (kbps)
3000
3500
Once transcoding
Twice transcoding
Once transcoding
Twice transcoding
FIGURE 13.4–4
PSNR performance comparison (Mobile and Coastguard test clips in CIF format) versus
bitrate for DIA operation in network.

560
CHAPTER 13 Video Transmission over Networks
A
B
C
…
A1 
A2
FEC 
FEC 
…
FEC 
B1
B2
B3
…
…
…
Bi
…
FEC 
FEC 
FEC 
FEC
FEC 
FEC
…
…
…
C1
C2
C3
…
…
…
…
…
C4
FEC
Cj
FEC
FEC
…
…
…
…
…
X
…
…
…
…
…
FEC
X1
…
…
…
…
Xn
Description 1
Description 2
…
Description n
FEC 
FEC 
FEC 
FEC
FEC
FEC
FEC
…
…
FIGURE 13.4–5
FGA-FEC coding scheme.
marked “FEC” in the ﬁgure. Because both the source data mapping and the FEC are
performed vertically, and at a ﬁne-grain (small) block size, FGA-FEC can be much
more efﬁcient for DIA than can MD-FEC, where the data was mapped horizontally
onto the relatively coarse sections. The optimal allocation of FEC to different sections
was described in Section 13.2 and in [14, 41]. After FEC encoding, each horizontal
row of blocks is packetized as one description, i.e., one description is equivalent to
one network packet.
Similar to MD-FEC in Section 13.2, FGA-FEC transforms a priority embedded
bitstream into nonprioritized descriptions to match a best-efforts network. In addition,
the FGA-FEC scheme has the ability of ﬁne granular adaptation at block level. In our
experiments, we set the block level to one byte, which matches the RS code symbol
size. To facilitate intermediate overlay node adaptation, an information packet is sent
ahead of one GOP to tell the intermediate nodes about the block size, FEC codes, and
bitstream information at the block level.
Example 13.4–2: Overlay Network Adaptation
Traditionally, when network congestion occurs, data packets are randomly dropped to
avoid congestion. To avoid this, the JSNC scheme adapts the packets in the intermediate
network nodes to reduce the bandwidth requirement. Given a 1.5-Mbps bitstream and
an available bandwidth of 1455 Kbps, in Figure 13.4–6 we compare PSNR-Y of JSNC
to a random packet-drop scheme with a 3% packet-loss rate. Observe that the proposed
scheme signiﬁcantly outperforms random drop by about 10 dB. In Figure 13.4–7 we show
objective video quality (PSNR) when the available bandwidth changes. Originally, the user
is receiving a 2-Mbps, CIF format, 30-fps bitstream, but starting with frame 100, the user
has only 512 Kbps.

13.4 Joint Source–Network Coding
561
50
100
150
200
250
300
10
15
20
25
30
35
40
45
Frame number
PSNR (dB)
JSNC
Random drop
FIGURE 13.4–6
JSNC versus random packet drop.
50
100
150
200
250
300
10
15
20
25
30
35
40
45
50
Frame number
PSNR
SNR adaptation
Spatial adaptation
T
FIGURE 13.4–7
3-D adaptation in frame rate, resolution, and SNR (bitrate).

562
CHAPTER 13 Video Transmission over Networks
There are three possible choices for the user: (1) SNR adaptation to 512 Kbps, (2)
temporal adaptation to one-quarter of the original frame rate, or (3) spatial adaptation
down to QCIF resolution. Options 2 and 3 need additional SNR adaptation to 512 Kbps.
With FGA-FEC, the users can choose their preference based on their application needs.
We note from Figure 13.4–7, however, that there is a signiﬁcant PSNR penalty to pay
(about 7 dB) for staying with the full frame rate and spatial resolution, and just cut-
ting down on the bitrate. See the Network Video folder at this book’s Web site for video
results.
Network Coding of Video
In unicast routing, information-bearing packets arrive at network routers, which look
only at the packet headers and forward them on an appropriate outgoing link. In the
case of video multicast, these packets generally must be replicated and sent out on
more than one outgoing link to their multiple destinations. This process is termed
multicast routing and is much more efﬁcient than unicast for simultaneously sending
a given video to many receivers. However, it turns out that there is a more efﬁcient
method called network coding that was ﬁrst published by Ahlswede et al. [51] in
2000. Network coding is a generalization of multicast routing (routing with replica-
tion) wherein the outgoing packets are additionally allowed to be linear combinations
of incoming packets, the exact linear combination being the network code. The orig-
inal example shown to justify the claim for improved performance is the butterﬂy
network shown in Figure 13.4–8, consisting of one source S and two receivers R1
and R2 connected by nine directed links and four intermediate nodes. This is an ide-
alized graph model of a network, consisting of edges and nodes, and each edge has a
capacity of 1 bit per use (bpu).
S
1
1
1
1
I1
I2
I3
I4
R1
R2
1
1
1
1
1
FIGURE 13.4–8
Simple directed graph network called butterﬂy.

13.4 Joint Source–Network Coding
563
(a)
(b)
S
ab
a
c
bc
I1
I2
I3
I4
R1
R2
ac
bc
a
c
abc
abc
ab
ab
ab
S
a
a
b
b
I1
I2
I3
I4
R1
R2
a +b
a +b
a+ b
b
a
FIGURE 13.4–9
Butterﬂy net: (a) routing, showing two uses of each link and with replication, (b) network
coding, showing one use of each link.
Figure 13.4–9a shows multicast routing for the problem of sending three binary
messages, a,b, and c, from the source to both receivers in two network (graph) uses.
Thus the throughput of multicast routing is 1.5 bpu. Figure 13.4–9b shows the net-
work coding solution, wherein the central node I3 outputs the sum of messages a
and b, and this enables the two receivers to get two messages a and b in one net-
work use, for a throughput of 2 bpu. It is also shown in [51] that this is actually the
capacity of the butterﬂy network in the sense that no coding scheme can do better.
They show that the multicast capacity of a network with one source and multiple
receivers can be found as the max-ﬂow of the min-cut. In other words, if you cut
the network graph into two pieces, one containing the source and the other contain-
ing a receiver, and then record the net ﬂow across the boundary from the source to
the receiver, then the minimum of these max ﬂows across these cuts is the max-ﬂow
min-cut capacity. Li et al. [52] showed that linear coding is sufﬁcient to achieve the
optimum max-ﬂow from the source to the receivers. Note that we talk of the same
ﬂow to all the receivers. While some receivers may have a high max-ﬂow min-cut
capacity, others may have lower capacities. All that is guaranteed for network coding
is that we achieve the minimum of these possible ﬂows, but to all receivers simultane-
ously, something that multicast routing cannot achieve. More recently this has come
to be called homogeneous multicast capacity to distinguish it from the general case,
where one may attempt to send different messages to the various receivers, such as
in scalable video coding, where we would seek to perform heterogeneous multicast.
We have seen that MD-FEC and its variations can be used to achieve heterogeneous
multicast. In the next section, it will be combined with network coding for greater
efﬁciency.

564
CHAPTER 13 Video Transmission over Networks
Practical Network Coding
Practical network coding (PNC) was introduced by Chou et al. [53] to allow network
coding to be used in real networks where there are random delays, packet losses,
and variable link bandwidths. It is based on the random network coding of Ho et
al. [54], which showed that random linear codes can come very close to achieving
homogeneous multicast capacity, albeit with a certain small probability of nondecod-
ability due to the unlikely occurrence of linearly dependent codewords. However, it
has been shown, if the code symbols are uniformly chosen independently from Galois
ﬁeld GF(28), that the probability of this nondecodability is quite small (i.e., less than
or equal to 2−8). Now, the ﬁnite ﬁeld GF(28) corresponds to 8-bit symbols or bytes
and is easily integrated into real networks. In PNC the code vectors must be transmit-
ted along with the data as a packet header, resulting in a small inefﬁciency for typical
packet sizes around 1500 Bytes. As a packet moves through the network from source
to receiver, the coefﬁcient header gets updated consecutively as it passes through
each intermediate node where the random combination of incoming packets will be
repeated. Another key aspect of PNC is the generation concept: only packets in the
current generation can be linearly coded together. For our work in video application
of this idea, we have chosen the generation size to be one GOP, typically one-half
second. This does introduce a delay issue as the packets are held in an intermediate
node’s buffer for a certain time to accumulate before the output starts from that node.
Example 13.4–3: Practical Network Coding
Here, we present a toy example of PNC using GF(22), consisting of just four elements,
0,1,2, and 3. We start with four data packets of (coincidentally) four elements each
and code them with random coefﬁcient vector [2 3 0 1], as shown in Figure 13.4–10.
2
2
3
3
Linear combination
of packets with
coefficients
[2 3 0 1]
Packet sent
Encoding vector
Data sent
1
2
3
0
0
0
0
0
1
1
1
0
0
0
0
0
0
0
2
2
2
3
2
2
FIGURE 13.4–10
PNC computations at source.

13.4 Joint Source–Network Coding
565
2
2
1
0
3
0
1
2
2
2
3
3
1
0
Linear combination
of packets with
coefficients
[1 2 3 1]
Incoming
packets
Encoding vector
Data
Outgoing
packet
Updated encoding
vector
Data
0
0
1
1
1
0
3
1
0
1
2
2
1
2
1
1
2
2
3
3
3
2
0
1
0
1
FIGURE 13.4–11
PNC computation at intermediate node.
Figure 13.4–11 shows how four network-coded packets are linearly combined with
random coefﬁcient vector [1 2 3 1] at an intermediate node. Finally, Figure 13.4–12
shows four network-coded packets arriving at a receiver and their solution via Gaussian
elimination [55] on the augmented matrix, outputting the original data plus an identity
matrix header in place of the coefﬁcient vectors.
1
1
0
0
0
0
0
0
2
2
1
0
3
0
Gaussian
elimination
Incoming
packets
Encoding vector
Data
1
2
3
3
2
2
3
2
2
1
3
3
3
2
0
2
3
3
3
3
0
1
1
0
1
2
0
0
2
1
0
2
1
0
0
0
0
0
0
2
2
1
0
0
2
0
1
1
3
0
Original data
FIGURE 13.4–12
PND decoding at receiver using Gaussian elimination.

566
CHAPTER 13 Video Transmission over Networks
The addition and multiplication tables for GF(22) are provided in end-of-chapter
problem 9, where you are asked to verify the calculations in Figures 13.4–10 through
13.4–12. Note the overhead due to packet header is large here (50%) due to the short
length of the data packets in this simple example.
PNC Together with MDC
Network coding can achieve a very efﬁcient homogeneous multicast, but if our
receivers have different needs and maxﬂow bandwidths, then we need to augment
PNC with some means to get the right information to the right place. One such
method is layered coding [56], wherein a scalable bitstream is broken into a base
layer and 2–3 optional enhancement layers. The packets in these layers can then be
addressed appropriately to get to the right users, and the optimal way of doing this
would be to construct the required multicast trees relying on complete knowledge
of the network. A much simpler method would be to incorporate MD-FEC methods
to allow the quality of the received video to vary with the total number of pack-
ets received. Such a method was indicated by Chou et al. [53] and developed more
fully in [57]. The method herein called MD-PNC can be understood with reference to
Figure 13.4–13, where across the top we see a scalable bitstream broken into a num-
ber of progressive layers. Below this in the ﬁgure, we see that this bitstream has been
mapped onto a set of packets (descriptions), but zero symbols have been inserted in
place of the parity symbols of an FEC code. To understand how this can work, we
must remember that the linear network coding is vertical across the symbols. So in
Layer 1
Layer 1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Layer 2
Layer 2
Layer 3
Layer 3
Layer 4
Layer 4
Layer 5
Layer 5
Layer 6
Layer 6
Packet 1
Packet 2
Packet 3
Packet 4
Packet 5
Packet 6
Bitstream to be transmitted
FIGURE 13.4–13
Simple illustration of the MD-PNC concept.

13.4 Joint Source–Network Coding
567
the ﬁrst column, only layer 1 data will be present. Thus if any receiver obtains one or
more of the network-coded versions of these packets, it will (only with high probabil-
ity, because of the random code) be able to decode layer 1 in its Gaussian elimination
procedure. Similarly, any two packets will only contain information on layers 1 and
2, and so these layers can be successfully decoded at a receiver that receives any
two packets. Continuing in this way, we eventually see that MD-PNC behaves simi-
larly to MD-FEC and so the distortion equation (13.2–1) will be the same, again with
high probability. However, with MD-PNC some channel rate must be reserved for the
packet headers containing the linear combination coefﬁcients. Thus these coefﬁcient
headers must be added to the packets shown in Figure 13.4–13. The rate equation
(13.2–2) is thus modiﬁed on the right-hand side to B′ < B because of this header.
With MD-PNC, no addresses need be present on the packets, as all the packets are
randomly linear combined at each network node and then sent out at the capacity of
each outbound link.
Example 13.4–4: MD-PNC
A comparison of routing (unicast), routing with replication (multicast), and MD-PNC was
presented in [57] for the simple butterﬂy net with link bandwidths of 500 Kbps and loss
rates of 20%, 10%, and 1%. The Foreman clip was encoded by enhanced MC-EZBC
at CIF resolution at 30 fps, and the PSNR was optimized in place of MSE distortion.
The optimal multicast results were obtained with two paths for each receiver, one with a
bandwidth of 500 Kbps and the second with a bandwidth of 250 Kbps. The results for
20% loss rate are shown in Figure 13.4–14. For this simple network, we see a substantial
improvement due to MD-PNC versus both unicast and multicast. Video clip results are
available at this book’s Web site.
42
45
40
35
30
GOP number
2
4
6
8
10
12
14
16
18
41
40
39
38
37
36
35
34
33
Design loss rate
(a)
(b)
0.15
PSNR (in dB)
PSNR (in dB)
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Only routing
Routing and replication
Network coding
Only routing
Routing and replication
Network coding
FIGURE 13.4–14
The foreman clip PSNRs with a link-loss rate of 20%: (a) variation of PSNR versus design
loss rate, (b) PSNR versus frame number at best-design loss rate.

568
CHAPTER 13 Video Transmission over Networks
CONCLUSIONS
Compressed video data must be protected and adapted to the channel in the case
of lossy transmission. This chapter has introduced the concept of error resilience of
video coders and their use in transport-layer error protection for wired and wire-
less channels, a key difference being the signiﬁcant probability of packet bit errors
in the latter. We have shown examples of scalable video coders together with an
multiple description coding (MDC) and introduced an extension of MD-FEC to
data adaptation useful for heterogeneous networks. We presented network coding
as an improvement over multicast routing that can achieve homogeneous network
capacity. Finally, we presented a combination of multiple descriptions and practical
network coding called MD-PNC, and showed its usefulness for video transmission
on networks.
A nice overview of video communication networks is contained in Schonfeld
[58]. An overview of JSCC for video communications is contained in Zhai
et al. [59].
PROBLEMS
1. This problem concerns the sync word concept, the problem being how to seg-
ment a VLC bitstream into the proper characters (messages). One way to do
this, together with Huffman coding, is to add an additional low probability (say
0 < p ≪1) message to the message source called sync. If we already had a set of
independent messages m1,m2,...,mM with probabilities p1,p2,...,pM, compris-
ing a message source with entropy H, show that for sufﬁciently small p, the total
entropy including the sync word is less than H + ϵ, for any given ϵ > 0. Hint:
Assume that the sync word constitutes an independent message, so that the com-
pound source uses an original message with probability (1 −p)pi or a sync word
with probability p. Note that
lim
p↘0plog 1
p = lim
n↗∞2−n log2n = lim
n↗∞n 2−n = 0.
2. In Example 13.1–1, the outputs of the even and odd scalar quantizers are the two
descriptions to be coded.
(a) For description 1, the output of the even quantizer, what is the entropy (rate)
and distortion, assuming an input random variable uniformly distributed on
[0,10].
(b) Compare the total rate for the two descriptions to the entropy rate for the
original single-description quantizer, assuming the same random variable
input.

Problems
569
3. Assume that bit errors on a certain link are independent and occur at the rate,
i.e., with probability 1 × 10−3. Assume that symbols (messages) are composed of
8-bit bytes. What then is the corresponding byte error rate? Assume that packets
are 100 bytes long. What then is the corresponding packet error rate? Repeat for
300-byte packets.
4. Express the values αk in terms of the source rate breakpoints Rk and the channel
bandwidth B for MD-FEC as presented in (13.2–1) and (13.2–2).
5. This problem is about calculating the total number of bits needed for MD-FEC.
(a) Refer to Figure 13.2–9. Assume that R1 through R4 are expressed in bits. Cal-
culate the total number of bits (both information bits and parity bits) needed
for the four packets (descriptions), in terms of R1 through R4.
(b) Now assume the goal is to create N packets. Calculate the total number of
bits needed for N packets, in terms of R1,R2,...,RN.
6. In channel coding theory, code rate is deﬁned as the ratio of information bits to
the total (information + parity) number of bits.
(a) Using the results from the previous problem, what is the code rate of MD-
FEC for N packets, expressed in terms of R1,R2,...,RN?
(b) Based on the deﬁnition of code rate, any nontrivial code must have a rate in
the interval (0,1]. Let N be given. Assuming that Rk,k = 1,2,...,N can be
real numbers, show that MD-FEC can achieve any rate r ∈[ 1
N ,1]. In other
words, given a number r ∈[ 1
N ,1], ﬁnd R1,R2,...,RN, such that the rate of the
corresponding MD-FEC code is r.
7. In Example 13.4–1 on digital item adaptation (DIA), two coded videos were
bitrate adapted (reduced) as shown in Figure 13.4–4. These ﬁgures show a slight
decrease in PSNR performance at the second transcoding (adaptation), which uses
only six quality layers. Would there be any further losses in quality at subsequent
bitrate adaptations that may happen further on in the network?
8. One GOP consists of MC-EZBC–encoded bitstreams {QMV
lt ,QYUV
lt,ls }, where {1 ≤
lt ≤5} and {1 ≤ls ≤6} (refer to Section 13.4 for notation). For simplicity,
we set the size
QMV
1
 = 4000,
QMV
2
 = 1000,
QMV
3
 = 700,
QMV
4
 = 300, and
QMV
5
 = 0, all in bytes. The sizes of the sub-bitstream partitions {QMV
lt ,QYUV
lt,ls }
are given in Table 13.4–1.
(a) What is the size of the adapted GOP if it is adapted to half resolution and
one-quarter frame rate?
(b) Suppose the original bitstream is encoded with FGA-FEC as in Figure 13.4–
5. Each motion vector sub-bitstream is encoded with a 1/4 code rate FEC
code, and each YUV sub-bitstream is encoded with a 1/2 code rate FEC code.
What is the size of the adapted GOP with FEC if the GOP is again adapted
to half resolution and one-quarter frame rate?

570
CHAPTER 13 Video Transmission over Networks
Table 13.4–1 Sub-bitstream Sizes
lt \ ls
1
2
3
4
5
1
100
100
200
100
200
2
200
200
400
400
500
3
800
800
1200
1000
1000
4
2200
2400
3000
2200
2000
5
4400
5300
5800
3200
2400
6
4600
6400
5000
2400
2000
Galois field arithmetic table
0
+
1
2
3
0
0
1
1
2
2
3
3
2
3
0
1
1
0
0
3
2
1
2
3
0
×
1
2
3
0
0
0
1
0
2
0
3
3
2
3
1
1
2
1
2
3
0
0
0
FIGURE 13.P–1
Arithmetic tables for GF(22).
9. The addition and multiplication tables for GF(22) are shown in Figure 13.P–1.
Use them to verify the computations in the toy example of Figures 13.4–10
through 13.4–12.
REFERENCES
[1] A. S. Tenenbaum, Computer Networks, 3rd. Ed., Prentice-Hall, Upper Saddle River, NJ,
1996.
[2] J. F. Kurose and K. W. Ross, Computer Networking, 3rd Ed., Pearson–Addison Wesley,
Boston, MA, 2005.
[3] IETF Differentiated Services Working Group, homepage. Available at http://www.ietf.
org/html.charters/diffserv-charter.html.
[4] M. Mathis, J. Semke, J. Mahdavi, and T. Ott, “The Macroscopic Behavior of the
TCP Congestive Avoidance Algorithm,” Computer Comm. Review, vol. 27, no. 3, July
1997.
[5] J. Padhye, J. Kurose, and D. Towsley, A TCP-friendly Rate Adjustment Protocol for Con-
tinuous Media Flows Over Best Effort Networks, U. Mass. CMPSCI Tech Report. 98–04,
October 1998.
[6] V. Paxson, “End-to-End Internet Packet Dynamics,” IEEE/ACM Trans. Networking,
vol. 7, no. 3, pp. 277–292, June 1999.
[7] M. Ghanbari, Video Coding: An Introduction to Standard Codecs, IEEE Telecomm.
Series, London, UK, 1999.

References
571
[8] Y. Takashima, M. Wada, and H. Murakami, “Reversible Variable Length Codes,” IEEE
Trans. Comm., vol. 43, no. 243, pp. 158–162, February 1995.
[9] J. Wen and J. Villasenor, “A Class of Reversible Variable Length Codes for Robust Image
and Video Coding,” Proc. IEEE ICIP 1997, pp. 65–68, Santa Barbara, CA, October 1997.
[10] N. Farber, T. Wiegand, and B. Girod, Error Correcting RVLC, ITU/VCEG Q15-E-32,
British Columbia, CA, July 1998.
[11] B. Girod, “Bidirectionally Decodable Streams of Preﬁx Code-Words,” IEEE Comm.
Letters, vol. 3, no. 8, pp. 245–247, August 1999.
[12] V. A. Vaishampayam, “Design of Multiple Description Scalar Quantizers,” IEEE Trans.
Inform. Theory, vol. 39, no. 3, pp. 821–834, May 1993.
[13] Y. Wang, M. Orchard, V. Vaishampayam, and A. R. Reibman, “Multiple Description
Coding Using Pairwise Correlating Transform,” IEEE Trans. Image Process., vol. 10,
no. 3, pp. 351–366, March 2001.
[14] R. Puri and K. Ramchandran, “Multiple Description Source Coding Using Forward Error
Correction Codes,” Proc. 33rd ACSSC, Paciﬁc Grove, CA, October 1999.
[15] T. Stockhammer, M. M. Hannuksela, and T. Wiegand, “H.264/AVC in Wireless Envi-
ronments,” IEEE Trans. Circuits Syst. Video Technol., vol. 13, no. 7, pp. 657–673, July
2003.
[16] J. B. Anderson, Digital Transmission Engineering, IEEE Press, Piscataway, NJ, 1999.
[17] R. G. Gallager, Information Theory and Reliable Communication, John Wiley and Sons,
New York, 1968.
[18] J. G. Proakis and M. Salehi, Communication Systems Engineering, 2nd Ed., Prentice-
Hall, Upper Saddle River, NJ, 2002.
[19] H. Balakrishnan, V. N. Padmanabhan, S. Seshan, and R. H. Katz, “A Comparison of
Mechanisms for Improving TCP Performance over Wireless Links,” IEEE/ACM Trans.
Networking, vol. 5, no. 6, pp. 756–769, December 1997.
[20] T. Rappaport, A. Annamalai, R. M. Buehrer, and W. H. Tranter, “Wireless Communi-
cations: Past Events and a Future Perspective,” IEEE Comm. Magazine., 50th Anniv.
Commem. Issue, no. 5, pp. 148–161, May 2002.
[21] J. W. Modestino and D. G. Daut, “Combined Source-Channel Coding of Images,” IEEE
Trans. Comm., vol. COM-27, no. 11, pp. 1644–1659, November 1979.
[22] M. Bystrom and J. W. Modestino, “Combined Source-Channel Coding Schemes for
Video Transmission over an Additive White Gaussian Noise Channel,” IEEE J. Selected
Areas in Comm., vol. 18, no. 6, pp. 880–890, June 2000.
[23] W. Zeng and B. Liu, “Geometric Structure Based Error Concealment with Novel Applica-
tions in Block-Based Low-Bit-Rate Coding,” IEEE Trans. Circuits Syst. Video Technol.,
vol. 9, no. 4, pp. 648–665, June 1999.
[24] W. M. Lam, A. R. Reibman, and B. Liu, “Recovery of Lost or Erroneously Received
Motion Vectors,” Proc. ICASSP, vol. 5, pp. 417–420, March 1993.
[25] Y.-K. Wang, M. M. Hannuksela, V. Varsa, A. Hourunranta, and M. Gabbouj, “The
Error Concealment Feature of the H.26L Test Model,” Proc. ICIP, vol. 2, pp. 729–732,
Rochester, NY, September 2002.
[26] A. K. Katsaggelos and N. K. Galatsanos, Eds., Signal Recovery Techniques for Image
and Video Compression, Kluwer Academic Publishers, 1998.
[27] N. S. Jayant, “Subsampling of a DPCM Speech Channel to Provide Two ’Self-Contained’
Half-Rate Channels,” Bell Syst. Tech. J., vol. 60, no. 4, pp. 501–509, April 1981.
[28] D. E. Dudgeon and R. M. Mersereau, Multidimensional Digital Signal Processing,
Englewood Cliffs, NJ, Prentice-Hall, 1983.

572
CHAPTER 13 Video Transmission over Networks
[29] J. H. Conway and N. J. A. Sloane, Sphere Packings, Lattices and Groups, Springer-
Verlag, New York, 1988.
[30] I. V. Baji´c and J. W. Woods, “Maximum Minimal Distance Partitioning of the Z2 Lattice,”
IEEE Trans. Inform. Theory, vol. 49, no. 4, pp. 981–992, April 2003.
[31] Y. Wang and D.-M. Chung, “Robust Image Coding and Transport in Wireless Networks
Using Non-Hierarchical Decomposition,” Mobile Multimedia Communications, (D. J.
Goodman and D. Raychaudhury, Eds.), Plenum Press, 1997.
[32] C. D. Creusere, “A New Method of Robust Image Compression Based on the Embedded
Zerotree Wavelet Algorithm,” IEEE Trans. Image Processing, vol. 6, no. 10, pp. 1436–
1442, October 1997.
[33] I. V. Baji´c and J. W. Woods, “Domain-Based Multiple Description Coding of Images and
Video,” IEEE Trans. Image Processing, vol. 12, no. 10, pp. 1211–1225, October 2003.
[34] J. K. Rogers and P. C. Cosman, “Wavelet Zerotree Image Compression with Packetiza-
tion,” IEEE Signal Processing Letters, vol. 5, no. 5, pp. 105–107, May 1998.
[35] A. Said and W. A. Pearlman, “A New, Fast, and Efﬁcient Image Codec Based on Set
Partitioning in Hierarchical Trees,” IEEE Trans. Circuits Syst. Video Technol., vol. 6, no.
3, pp. 243–250, June 1996.
[36] I. V. Baji´c, “Adaptive MAP Error Concealment for Dispersively Packetized Wavelet-
Coded Images,” IEEE Trans. Image Processing, in press.
[37] A. Albanese, J. Bl¨omer, J. Edmonds, M. Luby, and M. Sudan, “Priority Encoding
Transmission,” IEEE Trans. Inform. Theory, vol. 42, no. 6, pp. 1737–1744, November
1996.
[38] R. Puri, K.-W. Lee, K. Ramchandran, and V. Bharghavan, “An Integrated Source
Transcoding and Congestion Control Paradigm for Video Streaming in the Internet,”
IEEE Trans. Multimedia, vol. 3, no. 1, pp. 18–32, March 2001.
[39] A. E. Mohr, E. A. Riskin, and R. E. Laedner, “Unequal Loss Protection: Graceful Degra-
dation Over Packet Erasure Channels Through Forward Error Correction,” IEEE J.
Select. Areas Commun., vol. 18, no. 6, pp. 819–828, June 2000.
[40] B. G. Kim and W. A. Pearlman, “An Embedded Wavelet Video Coder Using Three-
Dimensional Set Partitioning in Hierarchical Trees,” Proc. Data Compress. Conf. (DCC),
pp. 251–260, Snowbird, Utah, March 1997.
[41] I. V. Baji´c and J. W. Woods, “EZBC Video Streaming with Channel Coding and Error
Concealment,” Proc. SPIE VCIP, vol. 5150, pp. 512–522, Lugano, Switzerland, 2003.
[42] S. Kumar, L. Xu, M. K. Mandal, and S. Panchanathan, “Overview of Error Resiliency
Schemes in H.264/AVC Standard,” J. on Visual Communications and Image Representa-
tion, vol. 17, no. 2, pp. 425–450, April 2006.
[43] M. Karczewicz and R. Kurceren, “The SP and SI Frames Design for H.264/AVC,” IEEE
Trans. Circuits Syst. Video Technol., vol. 13, no. 7, pp. 637–644, July 2003.
[44] MPEG-21 Overview V.4, ISO/IEC JTC1/SC29/WG11, MPEG2002/N4801, 2002.
[45] MPEG-21
Digital
Item
Adaptation
WD
(v3.0),
ISO/IEC
JTC1/SC29/WG11,
MPEG2002/N5178, 2002.
[46] S.-F. Chang and A. Vetro, “Video Adaptation: Concepts, Technologies, and Open Issues,”
Proceedings of the IEEE, vol. 93, no. 1, pp. 148–158, January 2005.
[47] S.-T. Hsiang and J. W. Woods, “Embedded Video Coding Using Invertible Motion Com-
pensated 3-D Subband Filter Bank,” Signal Processing: Image Comm., vol. 16, no. 8,
pp. 705–724, May, 2001.

References
573
[48] Hewlet-Packard Labs. Search term, “Structured Scalable Meta-formats (SSM)”. Avail-
able at http://www.hpl.hp.com/techreports.
[49] Y. Shan, I. V. Baji´c, S. Kalyanaraman, and J. W. Woods, “Joint Source-Network Error
Control for Scalable Overlay Video Streaming,” Proc. IEEE ICIP 2005, Genoa, IT,
September 2005.
[50] Y. Shan, I. V. Bajic, J. W. Woods, and S. Kalyanaraman, “Scalable Video Streaming with
Fine-Grain Adaptive Forward Error Correction,” IEEE Trans. Cir. and Sys. for Video
Technology, vol. 19, no. 9, September 2009.
[51] R. Ahlswede, N. Cai. S. Li, and R. Yeung, “Network Information Flow,” IEEE Trans.
Inform. Theory, vol. 46, no. 4, pp. 1204–1215, July 2000.
[52] S.-Y. R. Li, R. W. Yeung, and N. Cai, “Linear Network Coding,” IEEE Trans. Inform.
Theory, vol. 49, no. 2, pp. 371–381, February 2003.
[53] P. Chou, Y. Wu, and K. Jain, “Practical Network Coding,” Proc. 41st Allerton Conf. on
Comm., Control, and Comp., Monticello, IL, October 2003.
[54] T. Ho, R. Koetter, M. Medard, D. R. Karger, and M. Effros, “The Beneﬁts of Coding over
Routing in a Randomized Setting,” Proc. IEEE Intl. Sympos. Inform. Theory, Yokohama,
Japan, July 2003.
[55] http://en.wikipedia.org/wiki/Gaussian elimination.
[56] S. McCanne and M. Vetterli, “Joint Source/Channel Coding for Multicast Packet Video,”
Proc. of IEEE ICIP, vol. 1, 1995.
[57] A. K. Ramasubramonian and J. W. Woods, “Video Multicast Using Network Coding,”
Proc. SPIE Visual Comm. and Image Process. (VCIP), San Jose, CA, January 2009.
[58] D. Schonfeld, “Video Communications Networks,” Chapter 9.3 in Handbook of Image
and Video Processing, 2nd Ed., A. C. Bovik Editor, Elsevier/Academic Press, Burlington,
MA, 2005.
[59] F. Zhai, Y. Eisenberg, and A. G. Katsaggelos, “Joint Source-Channel Coding for Video
Communications,” Chapter 9.4 in Handbook of Image and Video Processing, 2nd Ed.,
A. C. Bovik Editor, Elsevier/Academic Press, Burlington, MA, 2005.

Index
Page numbers followed by n indicates a footnote.
1-D continuous time windows, 154–156
1-D Fourier transform, 21
rectangular pulse function, 16
1-D Kalman ﬁlter, 270
equations, 272–273
LMMSE estimator, 271
observation equation, 270
signal state equation, 271
system matrix, 271
See also 2-D Kalman ﬁlter
2-D ARMA model, 406
2-D discrete-space Fourier transform. See 2-D Fourier transform
2-D discrete-space systems, 9
linear system, 10
shift invariance, 10
simple systems, 9–10
2-D ﬁlter stability, 92
ﬁrst quadrant support, 94, 96
fourth quadrant support, 96, 97
LSI system, 92–93
NSHP, 100–102
ROC, 95–96, 97
root mapping, 98–100
second quadrant support, 95, 96
third quadrant support, 96
See also 2-D systems
2-D Fourier continuous transform, 27
coordinate system rotation, 28
inverse, 27
projection-slice theorem, 29–30
rotational invariance, 28–29
See also 2-D Fourier transform (2-D FT)
2-D Fourier transform (2-D FT), 13
complex plane wave, 20
convergence types, 15
Fourier convolution theorem, 19–20
hexagonal sampling lattice, 52
inverse, 16, 18–19
line impulse, 16
operator properties, 20–21
pairs, 21–22
properties, 14
rectangular pulse function, 15–16, 17
separable operator, 14
separable signal, 24–25
symmetry properties, 25–26
Z-transform comparison, 91–92
See also 1-D Fourier transform; Continuous-space Fourier
transform
2-D FT. See 2-D Fourier transform
2-D innovations sequence, 268
2-D Kalman ﬁlter, 274
error-covariance equations, 275
gain array, 275
global state vector, 274
scalar ﬁltering equations, 275
See also 1-D Kalman ﬁlter
2-D random ﬁeld. See Two-dimensional random ﬁeld
2-D raster manner, 507
2-D rectangular sinc function
2-D recursive ﬁlter design, 168
error criteria, 169–170
Prony example, 173
space-domain design, 170–173
See also Fully recursive ﬁlter design (FRF design)
2-D signal. See Two-dimensional signal
2-D systems
input/output equation, 75
recursion direction, 76
simple difference equation, 76–79
stability in, 12–13
See also 2-D ﬁlter stability
2-D Z-transform, 79–80
calculation, 82–83
closed-form solution, 89–90
convolution, 86
Fourier transform comparison, 91–92
general case, 84–85
intersection, 81
inverse, 88
linear mapping of variables, 87–88
properties, 85–86
zero loci, 80–81
See also Region of convergence (ROC); 2-D Fourier
transform
2-D vector, 149–150
2K containers in Digital Cinema, 464
3-D difference equation, 396
3-D ﬁlters, 396
3-D Fourier transform, 395, 403
interframe ﬁltering, 405
inverse, 395
575

576
Index
3-D Fourier transform (Continued)
properties, 395–396
3-D Kalman-reduced support regions, 410
3-D linear shift invariant system, 393
3-D linear system, 393, 396
convolution, 394, 396
separable operator, 396
3-D perspective plot, 17
zoomed-in contour plot, 17
3-D rhombic dodecahedron, 55
3-D sampling, 398–399
3-D sampling theorem, 397–398
3-D signal, 393, 415
3D-SPIHT, 548–549
3-D transform coder, 467
4-D matrix, 150, 309n
in image restoration, 308
Kalman gain operator, 456
4K containers for digital cinema, 464
4:2:0 speciﬁcation, 463
1080i, 463
A
a posteriori estimation, 289
CGM, 292
clique system, 291–292
Gauss-Markov image model, 289
line ﬁeld modeling edge, 292
line ﬁeld potential values, 293
noncausal Markov random ﬁeld, 290
SA, 293–298
A/D converter. See Uniform quantization
AC. See Arithmetic coding
Acknowledgement retransmission scheme (ACK/NACK
scheme), 376
Adaptive directional lifting transform (ADL transform),
373
coded with 5/3 SWT, 375
quadtree and directions, 374
Additive white Gaussian noise channel (AWGN channel),
539
Additive-increase multiplicative-decrease, 532
ADL transform. See Adaptive directional lifting transform
Advanced Television System Committee (ATSC), 463, 485
broadcast formats, 463
Advanced Video Coder (AVC), 504
AEP. See Asymptotic equipartition principle
Afﬁne motion model, 430
Alias error, 48
2000 × 1000-pixel image, 49
in directional case, 50
zoomed-in section, 49
Anti-alias ﬁltering, 184
Aperture, 421
effect, 214, 422
problem, 421–422
AR random signal model. See Autoregressive random signal
model
Arithmetic coding (AC), 347–349
EZBC context-based, 368–369
ARMA. See Autoregressive moving average
ARQ. See Automatic repeat request
Asymptotic equipartition principle (AEP), 386–387
See also Entropy
ATSC. See Advanced Television System Committee
Automatic repeat request (ARQ), 536
Autoregressive moving average (ARMA), 402
Autoregressive random signal model, 263
optimal linear prediction, 264–265
QP predictor, 265
Available bandwidth, 523
AVC. See Advanced Video Coder
Average
MSE distortion, 355
quantizer distortion, 337
value entropy, 346
AWGN channel. See Additive white Gaussian noise channel
B
Backward motion compensation, 479
Bartlett window, 155
Bayer array, Bayer ﬁlter, 213
Bayesian, 289, 447
detection/estimation, 454
MC prediction, 448–450
model, 520
motion estimation and segmentation, 450–453
motion/segmentation, 519, 521
segment and displacement potential, 451
Bessel function
ﬁrst-order, 23
zeroth-order, 23
Best-efforts network, 530–531
BIBO. See Bounded-input bounded-output
Binary digits (bits), 329
Bit errors, 532
Bitrate, average, 352
bits. See Binary digits (bits)
Block matching (BM), 423
hierarchical, 426
MSE in, 424
problems with, 425–426
PSNR performance, 425
three-step, 424

Index
577
Blurred SNR (BSNR), 279
BM. See Block matching
Boundary-matching algorithm, 539
Bounded-input bounded-output (BIBO), 12, 92
for 2-D systems, 12
LSI system stability, 92–93
NSHP Filter Stability, 100–101
Box ﬁlter, 223
FIR, 223
Fourier transform, 224
magnitude frequency response, 224
See also Image processing
Brightness. See Luminance
BSNR. See Blurred SNR
Butterﬂy network, 562, 563
C
Carrier-to-noise ratio (CNR), 493
Cathode ray tube (CRT), 214, 217
camera output voltage, 218
gamma value, 217
interlaced, 400
Causal Markov concepts, 291
CBR. See Constant bitrate
CCD. See Charge-coupled device
CCFL. See Cold cathode ﬂuorescent lamp
CDF. See Cumulative distribution function
CDF ﬁlter. See Cohen-Daubechies-Feauveau ﬁlter
CFA. See Color ﬁlter array
CGM. See Compound Gauss-Markov
Channel capacity, 383
Channel coder, 329, 547
design with video coder, 537
digital image communication system, 330
Channel coding rate, 538
Charge-coupled device (CCD), 66, 195, 212
sensor noise, 214
Chromaticity, 205
D65 standard illuminant, 207
of standard observer, 206
CIE. See Commission Internationale de L’eclairage
CIELAB, 210
CIF. See Common intermediate format
Circular windows, 154, 156
Cleanup pass, 370
Clique system, 291, 447
ﬁrst-order, 291–292
CMOS. See Complementary metal oxide semiconductor;
Coupled metal-oxide semiconductor
CNR. See Carrier-to-noise ratio
Cohen-Daubechies-Feauveau ﬁlter (CDF ﬁlter), 181
comparison with QMF, 182
frequency response, 182
step response, 183
Cold cathode ﬂuorescent lamp (CCFL), 216
Color
background adaptation, 205, 206
chromaticity values, 205, 206–207
color-matching functions, 204, 205
difference components, 463
display color primaries, 208
human eye, 203
linear model equations, 203, 204
object wavelength spectrum, 207
sensitivity functions, 203
sensor response functions, 207, 208
spectral radiance functions, 209
white balance, 208
Color ﬁlter array (CFA), 213, 456
Color image coding, 370
scalable coder results, 372
subsampling strategies, 371
See also JPEG 2000 standard (JP2K standard)
Color space, 209
CIELAB, 210
ITU Recommendation 709, 210–211
sRGB, 211–212
Commission Internationale de L’eclairage (CIE), 194
Common intermediate format (CIF), 462
Complementary metal oxide semiconductor (CMOS), 66
Compound Gauss-Markov (CGM), 292
Compressed video sensitivity, 522–523
Computed tomography (CT), 29
Conditional replenishment, 477
Conjugate antisymmetric part, 26
Conjugate symmetric part, 25, 26
Constant bitrate (CBR), 467
Constant-coefﬁcient. See Linear constant-parameter
Constraint equation for motion, 421
Continuous wavelet transform (CWT), 141
Continuous-space Fourier transform, 27
nonisotropic signal spectra, 44–47
spatial frequency aliasing, 39
Continuous-space IFT
hexagonal sampling lattice, 52
reconstruction formula, 41
Contrast, 196
adaptation, 199
Contrast sensitivity function (CSF), 197, 198
3-D, 202
linear amplitude plot, 199
spatiotemporal, 201
standard observer, 198
temporal, 201–202

578
Index
Contribution quality, 477
Control points for motion, 430
Convergence
generalized function, 15
mean-square, 15
uniform, 15
Convex hull, 358
Convolution
2-D, 10, 11–12, 86
3-D, 394
methods, sectioned, 145–146
property, 21, 86
representation, 394, 396
Convolution theorem
2-D Z-transform, 86
Fourier, 19–20
Correlation function, 262
2-D random ﬁeld, 258
geometric, 324
random sequence, 322
two-parameter, 259
WSS (wide sense stationary), 323
Cosine wave, 8–9
Coupled metal-oxide semiconductor (CMOS), 195
Bayer ﬁlter, 214
multilayer, 213
sensor noise, 214
Covariance function
2-D random ﬁeld, 258
random ﬁeld calculation, 260
random sequence, 322
wide-sense homogeneity, 259
CRT. See Cathode ray tube
CSF. See Contrast sensitivity function
CT. See Computed tomography
Cumulative distribution function (CDF), 319
See also Distribution function
CWT. See Continuous wavelet transform
D
D5. See International Telecommunication Union (ITU): 601
digital SDTV
D65 standard illuminant, 207
Data array, 331
Data partition (DP), 533, 551
Data service node (DSN), 556
dB. See Decibels
DC value, 21
DCDM. See Digital cinema distribution master
DCI formats, 463–464
DCT. See Discrete cosine transform
Dead zone, 469
DeCarlo–Strintzis theorem, 99
Decibels (dB), 425
Decimation in frequency (DIF), 144
Decimation in time (DIT), 144
Decimation matrix, 67
Decoder
3-D multidimensionally stable, 401
digital image communication system, 330
failure, 375
in image coder, 365
M-JPEG 2000 standard, 476
Degraded video restoration, 453
Bayesian approach, 454
in blotch-type artifacts presence, 454
observation model, 453
Deinterlacer, 441
conventional, 442–443
median, 443–445
motion-compensated, 445–446, 447
Deinterlacing, 440, 441
See also Deinterlacer
Delay property
DFS, 112, 114–115
DFT, 119
FT operator, 21
Z-transform, 86
Demosaicing, 456–459
Density domain. See Contrast
DFD. See Displaced frame difference
DFS. See Discrete Fourier series
DFT. See Discrete Fourier transform
DIA engine. See Digital item adaptation engine
Diamond subsampling
effect on frequency, 68–69
sublattice, 67–68
DIF. See Decimation in frequency
Differential entropy, 387
Differential pulse-code modulation (DPCM), 330n, 334
2-D DPCM coder, 334
motion-compensated, 479
property, 335
spatiotemporal generalization, 478
Differentiated services (DiffServ), 531
Digital
image communication system, 330
items, 555
still camera, 212
Digital cinema distribution master (DCDM), 464
Digital image compression, 329
2-D DCT, 331–333
2-D DPCM coder, 334
dyadic subband decomposition, 334

Index
579
source coder, 329
SWT, 333–334
Digital item adaptation engine (DIA engine), 555, 556
Digital light processing (DLP), 216
Digital video compression, 467
compressed video sensitivity, 522–523
interframe coding, 477, 480, 486–487, 503
intraframe coding, 468
nonlocal intraprediction, 517
object-based coding, 519–522
scalable video coders, 493
Digital video formats, 461
ATSC broadcast formats, 463
CIF, 462
DCI formats, 463–464
ITU 601 digital SDTV, 462–463
SIF, 462
Digital video processing, 415
Bayesian method, 447
degraded video restoration, 453
digital video formats, 461
interframe processing, 415–421
motion estimation and compensation, 421
motion-compensated ﬁltering, 434
super-resolution, 455
Directional transforms, 372
Discrete cosine transform (DCT), 109, 125
1-D case, 126–127
2-D, 331–333
2-D symmetric extension, 131–132
basis function image, 126
directional DCT, 373
fast DCT methods, 144–145
inverse, 126, 127, 128
MATLAB function, 128, 129
properties, 129–131
and SWT, 141
See also Discrete Fourier series (DFS); Discrete Fourier
transform (DFT); Subband/wavelet transform (SWT)
Discrete cosine transform (DCT) coders, 350
AC coefﬁcients, 351
bitrate, average, 352
Block-DCT Coding, 352–354
image, 353, 354
quantization matrix, 351
total MSE, 352
UTQ, 351
Discrete Fourier series (DFS), 109–110
amplitude part, 112
calculation, 111
delay property, 114–115
inverse, 110–111
periodic convolution, 113–114
properties, 111, 112–113
See also Discrete Fourier transform (DFT); Discrete cosine
transform (DCT); Subband/wavelet transform (SWT)
Discrete Fourier transform (DFT), 109, 115
2-D circular convolution, 120–121
basis functions, 116
circular convolution, 120
comparison with DCT, 129
fast DFT algorithm, 143–144
Fourier transform relationship, 122
interpolation, 124–125
ones on diagonal of square, 117–118
properties, 118–120
sampling effect in frequency, 123–124
symmetry in real-valued signal, 122–123
See also Discrete Fourier series (DFS); Discrete cosine
transform (DCT); Subband/wavelet transform (SWT)
Discrete wavelet transform (DWT), 142
See also Subband/wavelet transform (SWT)
Dispersive packetization (DP), 540
demonstration, 547
extension to three dimensions, 544
images, 540–544
of motion-compensated SWT video, 545
performance, 546
source sample splitting, 541
SWT, 542, 543
video, 544–545
Displaced frame difference (DFD), 437, 492, 539
Distortion rate, 359
Distribution function, 319
properties, 320
test set, 343
See also Random variables; Cumulative distribution
function (CDF)
Distribution print, 215
DIT. See Decimation in time
D–logE curve, 215, 216
DLP. See Digital light processing
Dominant pass, 364
Double tree algorithm, 362
Doubly stochastic Gaussian (DSG), 297
Downsampling, 58
2 × 2 decimation, 60–61
anti-alias ﬁlters, 184
general, 67
rectangular, 58, 59
system element, 59
DP. See Data partition (DP); Dispersive packetization
DPCM. See Differential pulse-code modulation
DSG. See Doubly stochastic Gaussian

580
Index
DSN. See Data service node
DV codec, 472–474
DV coder, 467, 472
DV macroblock, 473
Feed-forward or look-ahead strategy, 473
DWT. See Discrete wavelet transform
Dyadic decomposition, 333, 354
binary codeword calculation, 381
subband/wavelet structure, 362
E
EBCOT. See Embedded block coding with optimal truncation
(EBCOT)
ECSQ. See Entropy-coded scalar quantization
ECVQ. See Entropy-coded vector quantization
Edge detection, 235
on smoothed noisy image, 239
Sobel ﬁlter outputs, 236, 238
thresholded output, 237
See also Object detection
Edge linking, 238
gray-level image, 240
noisy gradient image, 240
SEL, 238, 240
Z-J search algorithm, 239n
Eigenvector, DCT, 130–131
Electronic image sensors
Bayer array, 213
CCD, 212, 213, 214
CFA, 213, 214
CMOS, 212, 213, 214
CRT, 214
sensor ﬁll factor, 214–215
See also Film
Electronic projectors, 216
Elementary streams (ES), 485
EM. See Expectation-maximization
Embedded block coding with optimal truncation (EBCOT),
369
ADL transform, 373
means, 370
Embedded zero block coder (EZBC), 362, 367
algorithm, 367–368
context modeling for AC, 369
context-based AC, 368–369
Embedded zero-tree wavelet (EZW), 362, 363
algorithm, 364–365
extractor, 365
precoder, 365
trees of coefﬁcients, 363
Embedding, quantizer bins for, 361
End of block (EOB), 375, 470
End of slice (EOS), 482
Energy function, 291, 447, 451
joint pdf, 292
motion ﬁeld prior model, 451–452
neighboring clique’s potential function, 447
Entropy, 384, 386
bounds on, 382
chain rule for, 382–383
coder, 330, 331
differential, 387–388
See also Kraft inequality; Information theory; Mutual
information; Random variables
Entropy coding, 346
Arithmetic coding (AC), 347–349
ECSQ, 349–350
ECVQ, 249
error sensitivity, 350
Huffman coding, 346–347
VLC, 350
Entropy-coded scalar quantization (ECSQ), 349
joint quantizer and entropy encoder, 350
Entropy-coded vector quantization (ECVQ), 349
EOB. See End of block
EOS. See End of slice
Equal slope condition, 358
Eric image
contour plot, 4, 5
intensity plot, 4, 6
mesh plot, 4, 5
Error
control coding, 534
covariance equations, 272, 273, 275, 411
resilient coding, 533–534
sensitivity, 350
Error concealment, 522, 538
in decoder, 537
dispersive packetization, 540
in H. 264/AVC, 554
MC, 549
nonnormative, 539
ES. See Elementary streams
Euler’s equality, 15
Even quantizer, 534
Event, 319
Expectation-maximization (EM), 300
FoGSM model, 304, 305–306
GSM, 302
image identiﬁcation and restoration, 298–302
pairwise densities, 305
Exp-Golomb codes, 533
Extension source, 346
Extractor, 365
EZBC. See Embedded zero block coder
EZW. See Embedded zero-tree wavelet

Index
581
F
FEC. See Forward error correction
FGA-FEC. See Fine-grain-adaptive-FEC
Field of Gaussian scale mixtures model (FoGSM model), 304
estimate, 305–306
FIFO. See First-in ﬁrst-out
Film
composition, 216
digitization, 215
distribution print, 215
D–logE curve, 215, 216
See also Electronic image sensors
Filter design method, biorthogonal, 180
power complementary pair, 181
system frequency response, 181
transfer function, 180
Filter stability test, 99–100, 102
Fine quantization, 338
Fine-grain-adaptive-FEC (FGA-FEC), 556
2-D SWT with, 140–141
3-D adaptation, 561
coding scheme, 560
JSNC vs. random packet drop, 561
overlay network, 557
PSNR performance comparison vs. bitrate, 559
video scalability dimensions, 557
Finite impulse response (FIR), 92, 223, 401
box ﬁlter, 223
Finite impulse response ﬁlter design (FIR ﬁlter design), 153
1-D continuous time windows, 154–156
by 1-D ﬁlter transformation, 160–163
using MATLAB, 156–159
POCS, 166–168
transform lowpass ﬁlter example, 164–166
window function design, 153–154
See also Inﬁnite impulse response ﬁlter design (IIR ﬁlter
design)
Finite-state scalar quantization (FSSQ), 492
FIR. See Finite impulse response
FIR ﬁlter design. See Finite impulse response ﬁlter design
First-in ﬁrst-out (FIFO), 532
First-order temporally causal model, 408
Flexible macroblock ordering (FMO), 551
mapping of macroblocks, 552
Flicker method, 194–195
FMO. See Flexible macroblock ordering
FoGSM model. See Field of Gaussian scale mixtures model
Forward error control coding, 535–536
Forward error correction (FEC), 534
Forward motion compensation, 490
bidirectional MCTF variation, 500
PSNR results, 494
Fourier convolution theorem, 19–20
Fovea, 203
fps. See Frames per second (fps)
Fractional passes, 369, 370
Frame
based model, 402
memories, 401
rate conversion, 440
repeat, 440, 441
Frames per second (fps), 468
FRF design. See Fully recursive ﬁlter design
FSSQ. See Finite-state scalar quantization
Fully embedded, 361
SWT coders, 362–363
Fully recursive ﬁlter design (FRF design), 174
row sequences, 174
row-operator form, 174
SHP Wiener ﬁlter design, 176–177
stability, 176
system function, 175
Future-present-past diagram, 408
G
Gain array, 411
Gaussian estimation, inhomogeneous
image model, 282
RUKF estimate, 283, 284–285
Wiener ﬁlter construction, 283–284
Gaussian ﬁlter, 224–225
Gaussian rate-distortion function, 389–390
Gaussian scale mixture (GSM), 303
Gibbs distributions, 291
Gibbs model, 289, 448
GOB. See Group of blocks
Golomb-Rice codes, 533
GOP. See Groups of picture
Group of blocks (GOB), 486
Groups of picture (GOP), 480
GSM. See Gaussian scale mixture
H
H. 262. See MPEG: MPEG 2
H. 263 coder, 486
H. 264/AVC, 504, 550, 554
allowed MV block sizes, 505
data partitioning, 551
directional prediction modes, 506
ﬂexible macroblock ordering, 551–552
intrablock refreshing, 554
PSNR vs. bitrate, 506
reference frame selection, 553
switching frames, 552–553
syntax, 551
system diagram, 504

582
Index
H. 264/MVC, 503, 515
inter-view prediction hierarchy, 516
multiview test clip Ballroom, 517
H. 264/SVC, 503, 508
hierarchical B frame, 510, 511
for spatiotemporal scalability, 512
Haar ﬁlter pair, 139–140
Hanning window, 155
HBM. See Hierarchical block matching
HD. See High deﬁnition
HDR. See High dynamic range
HDTV. See High-deﬁnition television
Hexagonal sampling lattice, 51
discrete-space FT, 52
hexagonal alias repeat grid, 54
periodicity matrix, 52, 53
reconstruction formula, 54–55
Hierarchical block matching (HBM), 426–427
motion estimation, 436, 437
multiresolution coder, 488
See also Block matching (BM)
Hierarchical VSBM (HVSBM), 426
backward motion vectors, 499
reﬁning and spliting process, 427
High deﬁnition (HD), 56
formats, 462
resolutions, 215
High dynamic range (HDR), 218
High reliability channel (HRC), 514
High-deﬁnition television (HDTV), 209, 484
MPEG 2, 485
nonlinear function, 211
High-resolution (HR), 311, 455
Hi-Vision, 484
Horizontal wave, 7
HR. See High-resolution
HRC. See High reliability
Huang stability theorem, 98, 99
Huffman coding, 346–347
Human eye, 203
color receptor’s sensitivity functions, 203
dynamic range, 215
relative luminous efﬁciency, 193
sensitivity, 215
visual system response, 207
Human visual system (HVS), 193, 200
3-D CSFs, 202
local adaptation property, 200
spatiotemporal CSF, 201
temporal CSF, 201–202
See also Human eye; Image visual properties, still
HVS. See Human visual system
HVSBM. See Hierarchical VSBM
Hybrid
coder, 478
decoder, 479
ﬁlter, 401
I
ICC. See International Color Consortium
ICM. See Iterated conditional mode
IDCT. See Inverse discrete cosine transform
Ideal decimation, 61
2 × 2 decimation, 62
frequency domain support, 63–64
LL subband, 62
Ideal ﬁlter, 20
Ideal interpolation, 65–66
Ideal lowpass ﬁlter
circular passband, 22–23
impulse responses, 24
rectangular passband, 21–22
IDFS. See Inverse discrete Fourier series
IDFT. See Inverse discrete Fourier transform
IDWT. See Inverse discrete wavelet transform
IETF. See Internet Engineering Task Force
I-frame, 403n, 481
IFT. See Inverse Fourier transform
IIR. See Inﬁnite impulse response
Image
coding robustness, 375–376
enhancement, 228–234
estimation, 257, 278–279
exercises, 316–318, 376–379
restoration, 257, 279–281
superresolution, 311–314
Image analysis, 223, 235
edge detection, 235–238
edge linking, 238–240
segmentation, 239, 340
See also Image: enhancement
Image identiﬁcation and restoration, 298
EM algorithm approach, 298–302
subband EM restoration, 303
See also Expectation-maximization (EM); Non-Bayesian
image estimate
Image processing
box ﬁlter, 223–224
color, 314–315
digital, 257
downward directed vertical axis, 76n
Gaussian ﬁlter, 224–225
Laplacian ﬁlter, 227–228
MATLAB toolbox function, 158, 165

Index
583
monochrome, 257
Prewitt operator, 225
raster scan, 169
Sobel operator, 226–227
See also Image analysis
Image visual properties, still, 195
contrast adaptation, 199, 200
contrast sensitivity function, 197, 198–199
Weber’s law, 196–197
See also Human visual system
Imaging distortion, 44
IMC. See Inverse motion compensation
Improved-Deﬁnition Television, 417–418
Impulse train, modulated, 42–43
Indicator function, 22
Inﬁnite impulse response (IIR), 401
ﬁlter design, 153, 168–173
FRF design, 174–177
See also Finite impulse response (FIR)
Inﬁnite observation domain, 266
SNR, 266
Wiener ﬁlter, 266, 267
Information, 346
measure, 379, 380
mutual, 383–384
Information theory, 341
bounds on entropy, 382
chain rule for entropy, 382–383
continuous sources, 387
data compression, 384
differential entropy, 387
discrete message source, 381–382
Gaussian random variable, 388
information measure, 379
Kraft inequality, 384–385
mutual information, 383–384
Shannon, Claude, 380
source coding theorem, 386–387
source entropy, 380–381
See also Entropy; Kraft inequality; Rate-distortion theory;
Source: coding theorem
Initial value, 21
Interframe
ARMA signal model, 405
video coders, 467
Wiener ﬁlter, 405, 406
Interframe coding, 477
1-D DPCM to interframe coding, 477–478
backward motion compensation, 479
coding standards, 480, 503
H. 263 coder, 486
H. 264/AVC, 504–507
H. 264/MVC, 515–517
H. 264/SVC, 508–512
hybrid decoder, 479
MC spatiotemporal prediction, 478–480
motion-compensated DPCM, 479
MPEG 1, 480–482
MPEG 2, 482–484
MPEG 3, 484–485
MPEG 4, 485
MPEG-coded bitstreams, 485–486
video coder mode control, 507–508
Interframe ﬁltering, 404
using 3-D Fourier transforms, 405
ARMA signal model, 405
Interframe processing, 415
spatiotemporal RUKF, 415–417
visually optimized ﬁlter, 419–421
Interframe SWT coders, 486–487
3-D subband ﬁlter, 487
Frame-rate conversion, 440
invertibility conditions, 490
Karlson’s ﬁlter tree frequency decomposition, 488
MC-deinterlacing, 445
MC-Kalman ﬁlter, 436
MCTF, 490, 491
MC-Wiener ﬁlter, 435
motion-compensated SWT hybrid coding, 487–489
Motion compensated processing, 434
multiresolution coder, 488
spatiotemporal transform coding, 489–492
Interlaced CRT displays, 400
Interlaced display, source, 217
Interlaced video, 57, 58, 400
3-D sampling, 400–401
CRT, 217
diamond sampling, 37
MPEG 2, 483
sampling in 2-D vertical-temporal domain, 57
International Color Consortium (ICC), 210
International Commission on Illumination. See Commission
Internationale de L’eclairage (CIE)
International Standards Organization (ISO), 480
International Telecommunication Union (ITU), 210, 482
inverse transformation, 210
linear transformation, 210
nonlinear color space, 211, 601
digital SDTV, 462–463
Internet Engineering Task Force (IETF), 531
Internet protocol (IP), 529
networks, 530–532
Interpolation function, 42, 43, 125
Intra macroblock displacement compensation, 517

584
Index
Intrablock refreshing, 554
H. 264/AVC, 554
Intraframe ARMA signal model, 403
Intraframe coding, 468
4:1:1 color, 473
DV codec, 472–474
DV macroblock, 473
intraframe SWT coding, 474–476
M-JPEG, 467, 470–471
M-JPEG 2000, 476–477
M-JPEG pseudo algorithm, 469–470
with rate control, 469
SWT coding, 474–476
SWT ﬁlter study, 475–476
Inverse 2-D Fourier transform, 16
proof, 18
Inverse 3-D Fourier transform, 395
Inverse discrete cosine transform (IDCT), 127, 128
Inverse discrete Fourier series (IDFS), 110–111
Inverse discrete Fourier transform (IDFT), 117
Inverse discrete wavelet transform (IDWT), 142
Inverse Fourier transform (IFT), 18
3-D sampling theorem, 397
continuous-space, 52
discrete-space FT, 52
rectangular sampling, 38
symmetric frequency response, 22
Inverse motion compensation (IMC), 435
Inverse subband/wavelet transform (ISWT), 138, 141
Inverse Z-transform, 88, 89
2-D polynomials don’t factor, 90–91
long division method, 90
See also Inverse Fourier transform (IFT)
IP. See Internet protocol
Iris, 203
ISNR. See SNR improvement
ISO. See International Standards Organization
Isolated zero (IZ), 364
ISWT. See Inverse subband/wavelet transform
Iterated conditional mode (ICM), 447
ITU. See International Telecommunication Union
IZ. See Isolated zero
J
JND. See Just-noticeable difference
Johnston’s ﬁlter 16C, 179–180
Joint motion estimation and segmentation, 450–453
Joint source channel coding (JSCC), 537–538
Joint source–network coding (JSNC), 555
ﬁne grain adaptive FEC, 556–562
MPEG 21, digital item adaptation, 555–556
video network coding, 562–563
Joint video team (JVT), 504
JPEG 2000 standard (JP2K standard), 369, 372
PSNR for, 372
scanning, 370
SWT coded image, 374
JSCC. See Joint source channel coding
JSNC. See Joint source–network coding
Just-noticeable difference (JND), 195
on local background brightness, 200
local contrast adaptation, 199
threshold, 197
JVT. See Joint video team
K
Kaiser window, 155
11 × 11 impulse response, 157–158, 159, 160
circular, 158
contour plot, 157
magnitude response, 156, 159
See also Finite impulse response ﬁlter design (FIR ﬁlter
design)
Kalman ﬁlter, 274
estimate, 273
Filtering estimate, 272
See also Image processing
Kalman gain vector, 272, 273
Kalman smoother estimates, 273
Karhunen-Loeve transform (KLT), 131, 330
K-means algorithm, 242
256 × 256 cameraman image, 243
class indices, 244
class means, 245, 246
for color images, 245, 246
convergence, 243, 244
optimal values, 242–243
Kraft inequality, 384–385.
See also Entropy; Information theory
L
L2 norm, 169
LAGRANGE multiplier, 349
Laplace rate-distortion function, 390
Laplacian ﬁlter, 227–228
See also Image processing
Lapped orthogonal transform (LOT), 331
block-DCT transformation, 332
SWT, 333
Lattice, 66
diamond-shaped sampling, 56
hexagonal sampling, 51
Layers and protocol stack, 531
LBG algorithm. See Linde, Buzo, and Gray algorithm

Index
585
LCCDE. See Linear constant coefﬁcient difference
equation
LCD display. See Liquid crystal digital display
LCOS. See Liquid crystal on silicon
Least-squares solution, 343
LED light sources, 216–217
LeGall/Tabatabai (LGT), 487
Lens, 203
LGT. See LeGall/Tabatabai
Linde, Buzo, and Gray algorithm (LBG algorithm), 341,
343
Line ﬁeld modeling edge, 292
Line ﬁeld potential values, 293
Line impulse
in discrete space, 2
Fourier transform, 16
Line system, 486n
Linear
constant-parameter, 393–394
mapping, 87–88
prediction, optimal, 264–265
Linear constant coefﬁcient difference equation (LCCDE), 75
Linear ﬁltering, 228
in intensity domain, 230–231, 232
MSE, 228, 230
noise sailboat image, 228, 229
output after 3 × 3 box ﬁlter, 229–230
See also Median ﬁltering; Image analysis
Linear minimum mean-square error (LMMSE), 271
Linear shift-invariant system (LSI system), 10
estimation, 277–279
ﬁlter, 277
frequency response, 20
impulse response, 11
inhomogeneous Gaussian estimates, 285
restoration, 279–282
stability, 12–13, 92–93
Linear space-variant (LSV), 302
Linear spatial systems. See 2-D systems
Linear time-invariant systems (LTI systems), 13
Linearity property
DCT, 129
DFS, 112
DFT, 118
FT operator, 21
Z-transform, 85
LIP. See List of insigniﬁcant pixels
Liquid crystal digital display (LCD display), 216, 217
spectral radiance functions, 209
Liquid crystal on silicon (LCOS), 216
LIS. See List of insigniﬁcant sets
List of insigniﬁcant pixels (LIP), 365
List of insigniﬁcant sets (LIS), 365
List of signiﬁcant pixels (LSP), 365
LMMSE. See Linear minimum mean-square error
Long division method, 90
Lossless coder, 329
Lossy coder, 329
LOT. See Lapped orthogonal transform
Low reliability (LRC), 514
Lowpass ﬁlter (LPF), 44
Low-resolution (LR), 311
LPF. See Lowpass ﬁlter
LR. See Low-resolution
LRC. See Low reliability
LSI system. See Linear shift-invariant system
LSP. See List of signiﬁcant pixels
LSV. See Linear space-variant
LTI systems. See Linear time-invariant systems
Luminance, 193
CIE, 194
ﬂicker method, 194–195
light, 195
luminous efﬁciency, 193
for sensors, 195
split-screen approach, 194
M
MAD. See Mean absolute difference
MAE. See Mean absolute error
Magnitude
only approximation, 169
and phase design, 170
reﬁnement, 370
Magnitude transfer function (MTF), 195
Main proﬁle, at High level (MP@HL), 485
M-algorithm, 297
Markov random ﬁeld sequence, 406, 407
Markov random sequence covariance matrix, 131
Markov-generating kernel values, 304
Maximum a posteriori (MAP) estimate, 293
Maximum likelihood estimate, 3112n
Maximum transmission unit (MTU), 532
Maximum-likelihood (ML), 299, 312, 539
MC. See Motion compensation
MCTF. See Motion-compensated temporal ﬁlter
MDC. See Multiple description coding
MD-FEC. See Multiple description forward error correction
MD-PNC, 566.
See also Practical network coding (PNC)
MDSP. See Multidimensional signal processing
Mean absolute difference (MAD), 423n
Mean absolute error (MAE), 231, 423
Mean ﬁeld annealing (MFA), 447

586
Index
Mean square error (MSE), 228, 423
in block matching, 424
distortion, 338
of uniform quantization, 339
Median ﬁltering, 231
example, 233
salt and pepper noise, 233, 234
See also Linear ﬁltering; Image analysis
Mesh-based methods, 430–434
control points, 430
example, 432–433
pseudo code, 431–432
regular triangular mesh grid, 431
message source, 381–382
MFA. See Mean ﬁeld annealing
Midrise quantizer, 337
Midtread quantizer, 337
Minimum mean-square error (MMSE), 271, 297,
407
M-JPEG, 467, 470–471
M-JPEG 2000, 476–477
pseudo algorithm, 469–470
rate control, 471, 472
ML. See Maximum-likelihood
MM MC-RUKF. See Multimodel MC-RUKF
MMSE. See Minimum mean-square error
Model noise, 271, 282, 289, 299
Monotonic nondecreasing distribution function, 320
Mother wavelet, 141
Motion compensation (MC), 421, 506
forward, 494
generalization, 517
via global pan vector, 495
MC-EZBC, 502–503, 549
MC-Kalman ﬁlter, 436–437, 437–440
MC-Wiener ﬁlter, 435–436
in MPEG 1, 481
spatiotemporal prediction, 478–480
warping, 436
Motion estimation and compensation, 421
aperture problem, 422
background covering and uncovering, 422
block-matching, 423–426, 426–427
mesh-based methods, 430–434
optical ﬂow methods, 429–430
overlapped block motion compensation, 427–428
pel-recursive motion estimation, 429
Motion model failure, 459
Motion-compensated ﬁltering, 434
deinterlacing, 440
frame-rate conversion, 440
LSI ﬁlter modiﬁcation, 434
MC-Kalman ﬁlter, 436–437
MC-Wiener ﬁlter, 435–436
along motion path, 435
Motion-compensated temporal ﬁlter (MCTF), 490, 496
adaptive LGT/Haar, 502
bidirectional, 498–502
four-level Haar ﬁlter, 490
Haar, 492, 496
LGT 5/3 ﬁlter, 491
scalable video coder, 495, 496–497
MP@HL. See Main proﬁle, at High level
MPEG
coded bitstream video processing, 485–486
half-band ﬁlters, 462
MPEG 1, 480, 481, 482
MPEG 2, 482–484
MPEG 21, digital item adaptation, 555–556
MPEG 4, 485
See also H. 264/AVC
MSB. See Most signiﬁcant bit
MSE. See Mean square error
MTF. See Magnitude transfer function
MTU. See Maximum transmission unit
Multicast routing, 562
Multidimensional signal processing (MDSP), 6
Multimodel MC-RUKF (MM MC-RUKF), 437
experimental result, 438–440
noisy motion vectors problem, 437–438
Multiple description coding (MDC), 533
PNC with, 566–567
redundancy, 535
scalar quantizer, 533–534
Multiple description forward error correction (MD-FEC),
545–548, 549–550
Multiplication property
DFS, 112
DFT, 118
Fourier Transform (FT), 20
Multiresolution SWT coding, 359–361
Multiview coding (MVC), 523
Mutual information, 383
See also Entropy
MVC. See Multiview coding
N
NAL. See Network abstraction layer
National Television System Committee (NTSC), 417
Neighborhood sets of the ﬁeld log, 304
Network abstraction layer (NAL), 508, 551
Network adaptation, 508
Network coder, 529
Network coding, 562

Index
587
Noise-thresholding operations, 286–289
Non-Bayesian image estimate, 306
BM3D estimate, 308
least squares, 308–310
nonlocal means, 306–308
total variation, 310–211
Noncausal Markov
ﬁeld regions for, 290–291, 407
in ﬁrst-order clique system, 291
random ﬁeld, 290
Nondyadic SWT decompositions, 362
Noninterlaced video, 400
See Progressive video
Nonlinear ﬁltering. See Median ﬁltering
Nonlocal intraprediction, 517
intra macroblock displacement compensation,
517
template matching, 518
Nonlocal means estimation, 306
Nonorthogonal sampling
diamond-shaped sampling lattice, 56–57
general hexagonal case, 53–54
hexagonal sampling lattice, 51–53
sampling efﬁciency, 55–56
sampling matrix, 50
See also Rectangular sampling
Nonsymmetric half-plane (NSHP), 76, 100, 175,
297
causal Wiener ﬁlter, 279–280
coefﬁcient array support, 101
ﬁlter stability, 100, 101
stability test, 102
Nonsymmetric half-space (NSHS), 408, 477
random ﬁeld sequence scanning, 409
NSHP. See Nonsymmetric half-plane
NSHS. See Nonsymmetric half-space
NTSC. See National Television System Committee
Null event, 319
O
O&A method. See Overlap-and-add
Object detection, 248
object segmentation, 248–250
template matching, 250–253
See also Edge detection
Object segmentation, 248
in Miss America test clip, 249
See also Region segmentation
Object-based coding, 519
hybrid MCP object video coder, 520
object-based SWT coder, 521–522
OBMC. See Overlapped block motion compensation
Observation equation
2-D Kalman ﬁlter, 274
3-D Kalman, 409
Gaussian estimation, 282
Occlusion problem, 435
OCSWT. See Overcomplete subband/wavelet transform
Octave decomposition. See Dyadic decomposition
Odd/even quantizer, 534
One-quarter spatial resolution, 557n
Open system interconnection (OSI), 531
Optical ﬂow, 421, 426n
methods, 429–430
OSI. See Open system interconnection
Overcomplete subband/wavelet transform (OCSWT), 286
Overlap-and-add (O&A method), 146
Overlapped block motion compensation (OBMC),
427–428
Overlay network, 555, 557
adaptation, 560, 562
DSN, 556
Oversampling camera, 66
P
Packet-based wired networks, 522, 544–545
Packetized zerotree wavelet method (PZW method), 542
Packet-loss rate, 532, 556
Pad´e approximation, 170–171
Parseval’s theorem, 21
DFS, 113
DFT, 119
Partial differentiation in frequency, 21
pdf. See Probability density function
Peak SNR (PSNR), 286n, 425
for JPEG 2000 test set images, 372
for H.264/AVC, 506
vs. packet loss, 543
Pel-recursive motion estimation, 429
Periodic convolution
DFS, 112, 113–114
DFT, 118
Periodic signal, 6–7, 15
cosine wave, 8–9
general periodicity, 7–8
horizontal wave, 7
Periodicity, 7–8
rectangular, 7
Periodicity matrix, 8, 52, 398
in frequency domain, 52–53
diamond sampling, 56
rectangular sampling case, 53
Perspective plot. See Mesh plot
PET. See Priority encoding transmission

588
Index
Plane wave propagation, 47–48
pmf. See Probability mass function
PNC. See Practical network coding
POCS. See Projection onto convex sets
Power spectral density (PSD), 262, 323, 403
of image random ﬁeld, 291
model noise random ﬁeld, 291
spectral factors and, 267
of typical image, 262
Power spectrum, 262–263
Practical network coding (PNC), 564–566
computation at intermediate node, 565
computations at source, 564
with MDC, 566–567
PND decoding, 565
Precoder, 365
Predictor estimate, one-step, 272
Preﬁx code, 384
binary, 385
Preﬁx condition, 533
Prewitt operator, 225
See also Sobel operator
Priority encoding transmission (PET), 546
Probability density function (pdf), 259, 320, 341, 469
Probability mass function (pmf), 293, 349
Probability measure, 319
Probability space, 319
See also Random process; Random sequence
Progressive video, 400
See also Interlaced video
Projection onto convex sets (POCS), 166
algorithm, 167
convergence, 167, 168
Projection-slice theorem, 29–30
Prony’s method, 172–173
Protocol stack, 531
PSD. See Power spectral density
PSNR. See Peak SNR
PZW method. See Packetized zerotree wavelet method
Q
QCIF. See Quarter CIF
QMF. See Quadrature magnitude ﬁlter (QMF); Quadrature
mirror ﬁlter
QoS. See Quality of service
QP predictor, 265
Quadrature magnitude ﬁlter (QMF), 139
Quadrature mirror condition, 138
Quadrature mirror ﬁlter (QMF), 177, 178
Quality of service (QoS), 531
Quantization, 335, 473
in 2-D DPCM, 335
data decorrelation, 334
ﬁne, 338
ﬁxed-length, 338
matrix, 470, 351
optimal MSE, 337, 338
in Panter and Dite, 377
scalar, 331, 487, 492
in source-coding system, 330
TCQ, 344
uniform, 331, 337, 338, 339
vector, 334, 345
See also Entropy-coded scalar quantization (ECSQ);
Entropy-coded vector quantization (ECVQ)
Quantizer, 331, 335, 336
average quantizer distortion, 337
Central deadzone, 351n
concatenation of, 360–361
deadzone of, 337
in differential pulse-code modulation, 330n
embedded, 361
index set, 346n
model, 338, 351, 355
MSE, 338–340
rounds to the left, 336
scalar MDC, 534
scalar quantizer, 336
scalar uniform, 350
SQ, 336, 535
SQ design algorithm, 340
step size, 507
uniform quantization, 338, 339
UTQ, 351, 469
vector quantization, 341
See also Rate-distortion theory
quantizer set, 359
Quarter CIF (QCIF), 462, 484
R
Radon transform, 29
Random ﬁeld, 257
Homogeneous, 258
See also Random variable
Random ﬁeld sequence, 402
3-D, 409
homogeneous, 407
Markov, 406
Random process, 319, 324–325
2-D random ﬁelds, 258
distribution function, 319
geometric correlation function, 324
homogeneous, 403

Index
589
independent random variables, 321
pdf, 320
probability space, 319
WSS, 323
See also Random sequence; Random variable
Random sequence, 257, 319, 321–322
Gaussian noise, 259–260
homogeneous random ﬁeld, 259
Markov, 131
second-order moments, 258–259
stationary, 322–323
wide-sense homogeneous (See also WSS), 259
WSS, 323
Random variable, 319, 321
assumed, 337
discrete, 388
distribution function, 320
Gaussian, 388
independent, 321
Laplace, 377
Poisson, 214
scalar, 534
uniform, 338, 339
Raster scan, 77, 100, 150n, 169, 248
Rate-distortion theory, 331, 355, 388
Gaussian rate-distortion function, 389–390
Laplace rate-distortion function, 390
Real valued signals, 26
DFT symmetry, 26, 122–123
Real-time protocol (RTP), 530
at application layer, 532
Receiver time-out time (RTO), 532
Receiver-processing multidimensional ﬁlters, 418
Reconstruction formula, 42
continuous-space IFT, 41
Haar SWTs, 140
for hexagonal sampling, 54–55
in ideal interpolation, 65
imaging distortion, 44
interpolation function, 43
modulated impulse train, 42–43
Rectangular periodicity, 7
Rectangular pulse function, 15–16
Rectangular sampling, 37
alias error in images, 48–50
effect of, 45
Fourier sampling relation, 39
ideal, 44
nonisotropic signal spectra, 44–47
plane wave propagation, 47–48
reconstruction formula, 41–44
sample rate change, 58, 59
spatial frequency, 39–40
See also Hexagonal sampling lattice
Rectangular windows. See Separable windows
Recursive subband decomposition. See Dyadic decomposition
Reduced order model (ROM), 282
Reduced update Kalman ﬁlter (RUKF), 276
approximate, 276–277
image estimation, 278–279
image restoration, 279–281
inhomogeneous Gaussian estimation, 283, 284–285
MC, 436, 437
MM MC-RUKF, 437–440
spatiotemporal, 415–417
steady-state, 277, 411
Reed-Solomon codes (RS codes), 535
Reference ﬁeld, 483
Reference frame selection (RFS), 553
Region growing, 245, 246
2-D complex magnitude plane, 82
2-D ﬁlter stability, 95–96
algorithm, 246–247
gray-level 512 × 480 ﬂower image, 247
from seed location, 247–248
Region of convergence (ROC), 79–80, 81
unit step function, 83–84
See also 2-D Z-transform
Region segmentation, 239, 240
K-means algorithm, 242–245
manual histogram thresholding, 240–241
region growing, 245, 246–248
See also Edge detection; Edge linking; Object
segmentation
Regularity problem, 143
Replacement noise model, 453
Resolution scalable coder (RSC), 494
average PSNR, 496
demonstration software, 495
Retina, 203
Reversible variable length coding (RVLCs), 533–534
RFS. See Reference frame selection
ROC. See Region of convergence
ROM. See Reduced order model
ROM Kalman ﬁlter (ROMKF), 282
Root mapping, 81, 98
DeCarlo–Strintzis theorem, 99
ﬁlter stability test, 99–100
Huang theorem, 98, 99
Rotated windows. See Circular windows
Rotational invariance, 28, 30
Round trip time (RTT), 532
Row–column approach, 143
Row-scanning order. See Raster scan

590
Index
Row-transforms, 143
RS codes. See Reed-Solomon codes
RSC. See Resolution scalable coder
RTO. See Receiver time-out time
RTP. See Real-time protocol
RTT. See Round trip time
RUKF. See Reduced update Kalman ﬁlter
RVLCs. See Reversible variable length coding
S
SA. See Simulated annealing
SAD. See Sum absolute difference
Sample rate change, 58, 66
diamond subsampling, 67–69
downsampling, 58–61
general downsampling, 67
ideal decimation, 61–64
ideal interpolation, 65–66
upsampling, 64–65
Sampling efﬁciency, 55–56
Sampling lattice, diamond-shaped, 56–57
Sampling matrix, 50, 51, 53
3-D, 398
for 2-D interlaced data, 58
diamond sampling, 56
SANs. See Storage area networks
Scalability, 493–496
resolution, 367
spatial, 511
spatiotemporal, 512
video scalability dimensions, 557
See also Scalable Video Coder (SVC); Subband/wavelet
(SWT) coders
Scalable coder results comparison, 372
Scalable SWT-based coders, 529
Scalable Video Coder (SVC), 493, 508, 523
bidirectional MCTF, 498–502
coding comparison, 503
covered pixels detection, 497–498
enhanced MC-EZBC, 502–503
frame output, 501
MCTF, 496–497
MPEG, 509
quality scalability, 513
SD and HD video, 494
threaded hierarchical structure, 513
unconnected blocks in ﬂower garden, 499
unidirectional motion ﬁeld, 499
video conferencing, 512–515
Scalar Kalman ﬁltering equations, 273
Scalar quantization, 331, 341
ECSQ, 349
ﬁne, 377
FSSQ, 492
interframe SWT coders optimization, 487
Scalar quantizer (SQ), 336
embedding, 361
MDC, 534
SD. See Standard deﬁnition
SDI. See Spike-detector index
SEL. See Sequential edge linking
Sensor ﬁll factor, 214–215
Separability, 6, 112
2-D DCT, 126, 132–133
2-D FT, 18
3-D, 394
3-D LSI system, 394
DFS, 112
DFT, 118–119
Separable operator, 14, 396
DFS operator, 115
Z-transform, 83
Separable signal, 4–6, 394
Fourier transform, 24–25
FT operator, 21
See also Periodic signal
Separable windows, 154, 155, 156
Sequential edge linking (SEL), 238
Set partitioning in hierarchical trees coder (SPIHT coder), 365
algorithm, 366
parent–child dependencies, 365, 720p, 463
Shank’s method. See Prony’s method
Shanks theorem, 96
Shannon, Claude, 380
Shannon coding, 386
Shape coding, 519
Shift invariance, 10
Shift property. See Delay property
Shifting representation, 2, 10
SHP. See Symmetric half-plane
SHP Wiener ﬁlter design, 176
magnitude, 176–177
phase response, 177
Side-match error, 539
SIF. See Source interchange format
Signal, 286
2-D, 1, 30
3-D, 393, 415
ACK/NAK, 535
AR, 270, 274, 298
circularly bandlimited, 40
decimated, 61, 67
decomposition, 64
discrete-space, 67

Index
591
error, 334, 335
Gibbs-Markov, 447
intraframe ARMA, 403
isotropic, 135, 136
model, 436
nonisotropic, 44
NTSC composite, 418
periodic, 6–7, 8
separable, 4, 21, 24
shifting representation of, 2
speech, 540
state equation, 271
step functions, 2
symmetries, 25–26, 122
upsampled, 64
video, 415
wedge support, 87n
Signal spectra, nonisotropic, 44
continuous-space FT, 45
diagonal basic cell, 46
rectangular sampling effect, 45, 46
Signal-to-noise ratio (SNR), 266
Signiﬁcant propagation, 370
Simple difference equation
dimensioned vector equation, 79
LCCDE, 76
recursive calculation, 77–78
solution calculation, 77
Simpliﬁed ++ stability test, 97–98, 99
Simulated annealing (SA), 293
for image estimation, 294–295
for image restoration, 295–297
motion estimation, 447
parallel, 297–298
Simulcast, 359–360, 517
solution, 510
Singular value decomposition (SVD), 6
Slice, 533, 554, 555
lengths, 522
Slice-based packetization (SP), 545
and DP schemes on football sequence, 546
SMPTE. See Society of Motion Picture and Television
Engineers
SMPTE D1. See International Telecommunication Union (ITU):
601 digital SDTV
SNHC. See Synthetic natural hybrid coder
SNR. See Signal-to-noise ratio
SNR improvement (ISNR), 278
Sobel ﬁlter
absolute value, 236
thresholded output, 236, 237
Sobel operator, 226
contour plot, 227
edge detection, 235
imaginary part, 226
Society of Motion Picture and Television Engineers
(SMPTE), 463
Solid-state display, 217, 400
Source
coder, 329, 529, 547
coding theorem, 346, 386–387
decoder, 330
entropy, 380–381
Source interchange format (SIF), 462
bitrates, 468
M-JPEG coding, 472
MPEG, 480, 482
Source-coding system diagram, 330
SP. See Slice-based packetization
Space-domain design, 169, 170–171
Pad´e approximation, 170–171
Prony’s method, 172–173
Spatial convolution. See Convolution: 2-D
Spatial domain aliasing, 125, 146
Spatial frequency, 195n
complex exponential for, 21
contrast sensitivity, 197–199, 201
response
spatiotemporal CSF, 202
Spatial Kalman ﬁltering equations, 276
Spatial signal. See Two-dimensional signal (2-D signal)
Spatiotemporal frequency aliasing, 401
Spatiotemporal constraint equation, 429
Spatiotemporal ﬁlters, 401–406
Spatiotemporal Markov models, 406
3-D Kalman-reduced support regions, 410
causal and semicausal 3-D ﬁeld sequences, 408–409
discrete Markov random ﬁeld sequence, 406–408
ﬁrst-order temporally causal model, 408
gain array, 411
local state vector, 410
reduced update spatiotemporal Kalman ﬁlter, 409
update region, 409, 410–411
Spatiotemporal signal processing, 399
ﬁrst-order temporal interframe Wiener ﬁlter, 406
interframe ﬁltering, 404
interframe Wiener ﬁlter, 405
intraframe ﬁltering, 402–403
intraframe Wiener ﬁlter, 403–404
spatiotemporal ARMA model, 402
spatiotemporal ﬁlters, 401
spatiotemporal sampling, 400–401
temporal average ﬁlter, 401

592
Index
Spatiotemporal transform coding, 489–492
Spectral factorization, 267
2-D innovations sequence, 268
MSE ﬁlter, 269
whitening ﬁlter realization, 268
Spectral radiance functions, 209
SPIHT coder. See Set partitioning in hierarchical trees
coder
Spike-detector index (SDI), 453
Split-screen approach, 194
SQ. See Scalar quantizer
SR. See Superresolution
sRGB color space, 209, 211–212
Stack algorithm. See Zigangirov-Jelinek search algorithm
(Z-J search algorithm)
Standard deﬁnition (SD), 56, 215, 461, 468, 484
Standard observer, 194, 198
chromaticity diagram, 206
CIE, 207
curve, 195
Storage area networks (SANs), 529
Strict-sense polynomial, 84
Subband coding, 356–357
3-D, 486
Subband transform (See Subband/wavelet transform (SWT))
Subband/wavelet domain estimation, 285
PSNR, 286n
signal, 286
SWT denoising, 286–289
Subband/wavelet transform (SWT), 109, 133n, 333
1-D, 140
ADL coded, 375
alias cancellation and reconstruction, 141
analysis/synthesis, 178, 182, 312n, 360
coder, 354
and DCT, 141
denoising, 286–289
directional ﬁltering, 373
dyadic, 142
ﬁlter pairs, 177, 475–476
with FIR ﬁlters, 140–141
Fourier transform output, 135–136
frequency domain expressions, 134–135
fully embedded coders, 362
Haar ﬁlter pair, 139–140
hybrid coding, 487–489
interframe coding, 477, 486
intraframe coding, 474
inverse, 141, 286, 333
inverse 1-D, 138
lazy, 189
lifted, 189, 190, 373
motion-compensated hybrid coding, 487
multiresolution coding, 359
nondyadic decomposition, 362
object-based swt coder, 521–522
overcomplete, 286
rectangular, 133–134, 136
subband region reconstruction, 137
temporal output, 496
transform functions, 139
video coding, 540–550
and wavelet, 141–143
See also Discrete Fourier series (DFS); Discrete Fourier
transform (DFT); Discrete cosine transform (DCT)
Subband/wavelet transform (SWT) robust
video coding, 540
3D-SPIHT, 548–549
dispersive packetization, 540
MC-EZBC, 549
multiple description FEC, 545–548
SWT values, 133
Subband/wavelet transformation (SWT) coder, 354
constant slope conditions, 358
distortion rate, 355, 357
dyadic decomposition, 354
equal slope condition, 358
MC-SBC, 490
performance, 357
rate-distortion optimization, 355
subband coding, 356–357
Subband/wavelet transformation (SWT) ﬁlter design, 177
anti-alias ﬁltering, 184
biorthogonal ﬁlter design method, 180–181
CDF ﬁlter, 181–183
ﬁlter comparison, 358
Johnston’s QMF design, 178–179
transfer function, 178
Sublattice, 66
diamond, 67–68
Subordinate pass, 364
Sum absolute difference (SAD), 423n, 517
Superresolution (SR), 312
image, 257, 311–314
type of, 312n
of video, 455–459
Sure event, 319
SVD. See Singular value decomposition
Switching frames in H.264/AVC, 552–553
SWT. See Subband/wavelet transform
Symmetric convolution, 125
Symmetric half-plane (SHP), 175
Symmetry properties
DCT, 130

Index
593
DFS, 113
DFT, 119–120
Fourier transform, 25
real-valued signals, 26
Z-transform, 86
Sync word, 522, 533n
Syntax, 375, 551
Synthetic natural hybrid coder (SNHC), 485
T
TCP. See Transfer control protocol
TCQ. See Trelliscoded quantization
Template matching, 250–251, 518
BM, 423
for intra prediction, 518
SR, 312n
Template matching spatial prediction (TMSP), 518
Temporally causal, 408
Kalman ﬁlter, 455, 456
model, 408
totally ordered, 408, 409
Temporally semicausal, 409
Test model 5 (TM5), 484
Texture, 519
coders, 485, 520, 521
HVS, 199
interlayer prediction model, 509
nonisotropic signal, 44
segmentation, 239
See also Template matching
Threading, 513
Thresholding, manual histogram, 240
gray-level house image, 241
at minima, 241
Time jitter, 532
TM5. See Test model 5
TMSP. See Template matching spatial prediction
Tone mapping, 218
Totally ordered temporally causal case. See Nonsymmetric
half-space (NSHS)
Trackable motion, 442
Training set, 259, 343
Transfer control protocol (TCP), 530, 536
friendly, 530, 532
at transport level, 532
Transform block, 331, 332
Transform ﬁlter design method
1 × 1 order McClellan, 162
9 × 9 lowpass ﬁlter, 162
magnitude response, 163
symmetric FIR ﬁlter, 160–161
See also Transform lowpass ﬁlter
Transform lowpass ﬁlter design, 164
example, 164
for image ﬁltering, 164
McClellan transformation, 166
See also 1-D ﬁlter transformation design
Transformation, 330, 331
of 1-D ﬁlter, 160–166
in image coding, 330, 334
inverse, 210
invertibility conditions, 490
linear, 205, 210, 418
nonlinear, 210, 211, 231
orthogonal, 335
rate-distortion theory, 331
of variables, 87
See also Information theory; Subband/wavelet
transformation (SWT); Quantization
Transformation and quantization, 335
Transport stream (TS), 485, 508
Transport-level error control, 535–536
Trelliscoded quantization (TCQ), 344
Triangular window. See Bartlett window
TS. See Transport stream
Two-dimensional random ﬁeld (2-D random ﬁeld),
257–258
AR signal models, 263–265
estimation, 265–266
ﬁltering, 260–262
Gaussian noise, 259–260
homogeneous, 258–259
inﬁnite observation domain, 266–267
NSHP causal Wiener ﬁlter, 279–280
power spectrum of images, 262–263
second-order moments, 258
spectral factorization, 267–269
wide-sense homogeneous, 259
Two-dimensional recursive estimation
1-D Kalman ﬁlter, 270–273
2-D Kalman ﬁltering, 274–275
approximate RUKF, 276–277
LSI estimation, 277–279
LSI restoration, 279–281
reduced order model, 282
RUKF, 276
steady-state RUKF, 277
See also Gaussian estimation, inhomogeneous
Two-dimensional signal (2-D signal), 1
2-D convolution, 10–12
2-D discrete-space systems, 9–10
contour plot of Eric, 4, 5
impulse, 1, 2
mesh plot, 4, 5

594
Index
Two-dimensional signal (2-D signal) (Continued)
periodic signals, 6–9
separable signal, 4, 6
stability, 12–13
unit impulse line, 2, 3
unit step function, 2, 3–4
U
UDP. See User datagram protocol
Unequal error protection (UEP), 551
Uniform quantization, 337
MSE of, 339
subband coding, 356
of uniform random variable, 338, 339
Uniform quantizer, 337
Uniform threshold quantizer (UTQ), 351, 469
Unit impulse line, 2, 3
Unit step function
ﬁrst-quadrant, 2, 31, 78, 83, 91, 104
fourth quadrant, 4, 83–84
ROC, 83–84
second, and third quadrants, 4
Unitary matrix, 130
Update region, 409, 410–411
covariance, 277, 411, 413
local, 276
Upsampling, 64–65, 66
conventional deinterlacer, 441
lowpass ﬁlter for, 71
spatial, 257
User datagram protocol (UDP), 532
UTQ. See Uniform threshold quantizer
V
Variable bitrate (VBR), 467, 468
Variable length codes, 346, 533
Variable length coding (VLC), 350
reversible, 533
Variable-length code tree, 385
See also Entropy; Information theory
Variable-size block matching (VSBM), 426
Variable-size mesh matching (VSMM), 433
Variance function, 258, 282, 322
VBR. See Variable bitrate
VCEG. See Video Coding Experts Group
VCL. See Video coding layer
Vector quantization (VQ), 331, 341, 343–344
ECVQ, 349
on image, 344–345
least-squares solution, 343
optimality conditions, 342
PSNR vs. bits, 345
See also Linde, Buzo, and Gray algorithm (LBG algorithm)
Video bitstream, 557, 558
DSNs, 558
MC-EZBC, 549
scalability, 493
Video coder, 467
in application layer, 531
error-resilience, 529, 568
MC, 426
mode control, 507–508
Video Coding Experts Group (VCEG), 503
Video coding layer (VCL), 508, 551
Video network coding, 562–563
PNC together with MDC, 566–567
practical, 564–566
Video objects (VO), 485
Video on demand (VOD), 486, 529
Video on IP networks, 529
error concealment, 538
error-resilient coding, 533
multiple description coding, 534–535
overview, 530
transport-level error control, 535
wireless networks, 536
Video scalability dimensions, 557
Video super-resolution, 455
demosaicing, 456–459
Video transmission, 529
exercises, 568–570
H. 264/AVC, 550
on IP networks, 529
joint source-network coding, 555
robust SWT video coding, 540
system diagram, 530
Video typical bitrates, 468
Visual scenes, 519
VLC. See Variable length coding
VO. See Video objects
VOD. See Video on demand
Voronoi regions, 343
VQ. See Vector quantization
VSBM. See Variable-size block matching
VSMM. See Variable-size mesh matching
W
Wavelength, 195n
Wavelet coefﬁcients, 143
See also SWT values
Wavelet decomposition. See Dyadic decomposition
Wavelet packets, 362

Index
595
Wavelet theory, 141, 142
CWT, 141
DWT, 142
IDWT, 142
regularity problem, 143
Wavenumber, 47
Weber’s law, 196
contrast, 196
increments, 197
intensity difference vs. background intensity, 196
stimulus, 197
White Gaussian noise, intensity-modulated, 214
White noise, 260, 271
2-D difference equation, 263
exercises, 316, 378
Gaussian, 305, 415
low-resolution signal, 416
random ﬁeld sequence, 402, 409
spatiotemporal, 403
See also Signal-to-noise ratio (SNR)
Whitening ﬁlter
2-D, 267
linear minimum MSE, 269
realization of Wiener ﬁlter, 268
Wide-sense stationary (WSS), 259, 323
Wiener ﬁlter, 266, 267
3-D, 435
adaptive, 284
blur restoration, 296
CGM model, 295
DFT-implemented, 281
equation for 2-D noncausal, 266
Gauss-Markov model, 295
interframe, 405–406
intraframe, 403–404
MC, 435–436
nonrecursive formulation, 271
NSHP causal, 269–270
SHP, 176
simple, 283
spatial, 301
wavelet image denoising, 286–289
whitening ﬁlter realization, 268
Window function, 153
classes, 154
ﬁlter impulse response, 153
properties, 154
Wireless local area networks (WLANs), 529
Wireless networks, 536
joint source-channel coding, 537–538
WLANs. See Wireless local area networks
WSS. See Wide-sense stationary
Z
Zero loci, 80–81, 97
Zero-phase design, 169
Zero-tree root (ZTR), 363, 364
Zigangirov-Jelinek search algorithm (Z-J search algorithm),
239n
ZTR. See Zero-tree root


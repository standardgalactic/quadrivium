In
tro
duction
to
Complexit
y
Theory
{
Lecture
Notes
Oded
Goldreic
h
Departmen
t
of
Computer
Science
and
Applied
Mathematics
W
eizmann
Institute
of
Science,
Israel.
Email:
oded@wisdom.weizmann.ac.i
l
July
,
			

I
c
Cop
yrigh
t
			
b
y
Oded
Goldreic
h.
P
ermission
to
mak
e
copies
of
part
or
all
of
this
w
ork
for
p
ersonal
or
classro
om
use
is
gran
ted
without
fee
pro
vided
that
copies
are
not
made
or
distributed
for
prot
or
commercial
adv
an
tage
and
that
new
copies
b
ear
this
notice
and
the
full
citation
on
the
rst
page.
Abstracting
with
credit
is
p
ermitted.

I
I

Preface
Complexit
y
Theory
is
a
cen
tral
eld
of
Theoretical
Computer
Science,
with
a
remark
able
list
of
celebrated
ac
hiev
emen
ts
as
w
ell
as
a
v
ery
vibran
t
presen
t
researc
h
activit
y
.
The
eld
is
concerned
with
the
study
of
the
intrinsic
complexit
y
of
computational
tasks,
and
this
study
tend
to
aim
at
gener
ality:
It
fo
cuses
on
natural
computational
resources,
and
the
eect
of
limiting
those
on
the
class
of
pr
oblems
that
can
b
e
solv
ed.
These
lecture
notes
w
ere
tak
en
b
y
studen
ts
attending
m
y
y
ear-long
in
tro
ductory
course
on
Complexit
y
Theory
,
giv
en
in
		{		
at
the
W
eizmann
Institute
of
Science.
The
course
w
as
aimed
at
exp
osing
the
studen
ts
to
the
basic
results
and
researc
h
directions
in
the
eld.
The
fo
cus
w
as
on
concepts
and
ideas,
and
complex
tec
hnical
pro
ofs
w
ere
a
v
oided.
Sp
ecic
topics
included:

Revisiting
NP
and
NPC
(with
emphasis
on
searc
h
vs
decision);

Complexit
y
classes
dened
b
y
one
resource-b
ound
{
hierarc
hies,
gaps,
etc;

Non-deterministic
Space
complexit
y
(with
emphasis
on
NL);

Randomized
Computations
(e.g.,
ZPP
,
RP
and
BPP);

Non-uniform
complexit
y
(e.g.,
P/p
oly
,
and
lo
w
er
b
ounds
on
restricted
circuit
classes);

The
P
olynomial-time
Hierarc
h
y;

The
coun
ting
class
#P
,
appro
ximate-#P
and
uniqueSA
T;

Probabilistic
pro
of
systems
(i.e.,
IP
,
PCP
and
ZK);

Pseudorandomness
(generators
and
derandomization);

Time
v
ersus
Space
(in
T
uring
Mac
hines);

Circuit-depth
v
ersus
TM-space
(e.g.,
A
C,
NC,
SC);

Av
erage-case
complexit
y;
It
w
as
assumed
that
studen
ts
ha
v
e
tak
en
a
course
in
computabilit
y
,
and
hence
are
familiar
with
T
uring
Mac
hines.
Most
of
the
presen
ted
material
is
quite
indep
enden
t
of
the
sp
ecic
(reasonable)
mo
del
of
com-
putation,
but
some
(e.g.,
Lectures
,
,
and
	{0)
dep
ends
hea
vily
on
the
lo
calit
y
of
computation
of
T
uring
mac
hines.
I
I
I

IV
State
of
these
notes
These
notes
are
neither
complete
nor
fully
pro
ofread,
let
alone
b
eing
far
from
uniformly
w
ell-written
(although
the
notes
of
some
lectures
are
quite
go
o
d).
Still,
I
do
b
eliev
e
that
these
notes
suggest
a
go
o
d
outline
for
an
intr
o
duction
to
c
omplexity
the
ory
course.
Using
these
notes
A
total
of

lectures
w
ere
giv
en,

in
eac
h
semester.
In
general,
the
pace
w
as
rather
slo
w,
as
most
studen
ts
w
ere
rst
y
ear
graduates
and
their
bac
kground
w
as
quite
mixed.
In
case
the
studen
t
b
o
dy
is
uniformly
mor
e
advanc
e
d
one
should
b
e
able
to
co
v
er
m
uc
h
more
in
one
semester.
Some
concrete
commen
ts
for
the
teac
her
follo
w

Lectures

and

revisit
the
P
vs
NP
question
and
NP-completeness.
The
emphasis
is
on
presen
ting
NP
in
terms
of
searc
h
problems,
on
the
fact
that
the
mere
existence
of
NP-complete
sets
is
in
teresting
(and
easily
demonstratable),
and
on
reductions
applicable
also
in
the
domain
of
searc
h
problems
(i.e.,
Levin
reductions).
A
go
o
d
undergraduate
computabilit
y
course
should
co
v
er
this
material,
but
unfortunately
this
is
often
not
the
case.
Th
us,
I
suggest
to
giv
e
Lectures

and

if
and
only
if
the
previous
courses
tak
en
b
y
the
studen
ts
failed
to
co
v
er
this
material.

There
is
something
anal
in
m
uc
h
of
Lectures

and
.
One
ma
y
prefer
to
shortly
discuss
the
material
of
these
lectures
(without
pro
viding
pro
ofs)
rather
than
sp
end

hours
on
them.
(Note
that
man
y
statemen
ts
in
the
course
are
giv
en
without
pro
of,
so
this
will
not
b
e
an
exception.)

One
should
b
e
able
to
merge
Lectures

and

in
to
a
single
lecture
(or
at
most
a
lecture
and
a
half
).
I
failed
to
do
so
due
to
inessen
tial
reasons.
Alternativ
ely
,
ma
y
merge
Lectures
{
in
to
t
w
o
lectures.

Lectures
{
w
ere
dev
oted
to
comm
unication
complexit
y
,
and
circuit
depth
lo
w
er
b
ounds
deriv
ed
via
comm
unication
complexit
y
.
Unfortunately
,
this
sample
fails
to
touc
h
up
on
other
imp
ortan
t
directions
in
circuit
complexit
y
(e.g.,
size
lo
w
er
b
ound
for
A
C0
circuits).
I
w
ould
recommend
to
try
to
correct
this
deciency
.

Lecture

w
as
dev
oted
to
Computational
Learning
Theory
.
This
area,
traditionally
asso
ci-
ated
with
\algorithms",
do
es
ha
v
e
a
clear
\complexit
y"
a
v
our.

Lecture

w
as
sp
en
t
discussing
the
(limited
in
our
opinion)
meaningfulness
of
relativization
results.
The
dilemma
of
whether
to
discuss
something
negativ
e
or
just
ignore
it
is
nev
er
easy
.

Man
y
in
teresting
results
w
ere
not
co
v
ered.
In
man
y
cases
this
is
due
to
the
trade-o
b
et
w
een
their
conceptual
imp
ortance
as
w
eigh
ted
against
their
tec
hnical
dicult
y
.

V
Bibliographic
Notes
There
are
sev
eral
b
o
oks
whic
h
co
v
er
small
parts
of
the
material.
These
include:
.
Garey
,
M.R.,
and
D.S.
Johnson.
Computers
and
Intr
actability:
A
Guide
to
the
The
ory
of
NP-Completeness,
W.H.
F
reeman
and
Compan
y
,
New
Y
ork,
		.
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.
Copies
ha
v
e
b
een
placed
in
the
facult
y's
library
.
.
J.E.
Hop
croft
and
J.D.
Ullman,
Intr
o
duction
to
A
utomata
The
ory,
L
anguages
and
Computa-
tion,
Addison-W
esley
,
		.
.
M.
Sipser.
Intr
o
duction
to
the
The
ory
of
Computation,
PWS
Publishing
Compan
y
,
		.
Ho
w
ev
er,
the
presen
tation
of
material
in
these
lecture
notes
do
es
not
necessarily
follo
w
these
sources.
Eac
h
lecture
is
planned
to
include
bibliographic
notes,
but
this
in
tension
has
b
een
only
partially
fullled
so
far.

VI

Ac
kno
wledgmen
ts
I
am
most
grateful
to
the
studen
ts
who
ha
v
e
attended
the
course
and
partipiated
in
the
pro
ject
of
preparing
the
lecture
notes.
So
thanks
to
Ser
gey
Benditkis,
R
eshef
Eilon,
Michael
Elkin,
A
miel
F
erman,
Dana
Fisman,
Danny
Harnik,
Tzvika
Hartman,
T
al
Hassner,
Hil
lel
Kugler,
Ode
d
L
achish,
Moshe
L
ewenstein,
Y
ehuda
Lindel
l,
Y
o
ad
Lustig,
R
onen
Mizr
ahi,
L
eia
Passoni,
Guy
Pe
er,
Nir
Piterman,
Ely
Por
ate,
Y
o
av
R
o
deh,
A
lon
R
osen,
V
er
e
d
R
osen,
No
am
Sadot,
Il'ya
Safr
o,
T
amar
Se
eman,
Ekaterina
Se
d
letsky,
R
eub
en
Sumner,
Y
ael
T
auman,
Boris
T
emkin,
Er
ez
Waisb
ar
d,
and
Ger
a
Weiss.
I
am
grateful
to
Ran
Raz
and
Dana
Ron
who
ga
v
e
guess
lectures
during
the
course:
Ran
ga
v
e
Lectures
{
(on
comm
unication
complexit
y
and
circuit
complexit
y),
and
Dana
ga
v
e
Lecture

(on
computational
learning
theory).
Thanks
also
to
Paul
Be
ame,
R
ue
diger
R
eischuk
and
A
vi
Wigderson
who
ha
v
e
answ
ered
some
questions
I'v
e
had
while
preparing
this
course.
VI
I

VI
I
I

Lecture
Summaries
Lecture
:
The
P
vs
NP
Question.
W
e
review
the
fundamen
tal
question
of
computer
science,
kno
wn
as
the
P
v
ersus
N
P
question:
Giv
en
a
problem
whose
solution
can
b
e
v
eried
ecien
tly
(i.e.,
in
p
olynomial
time),
is
there
necessarily
an
ecien
t
metho
d
to
actually
nd
suc
h
a
solution?
Lo
osely
sp
eaking,
the
rst
condition
(i.e.,
ecien
t
v
erication)
is
captured
in
the
denition
of
N
P
,
and
the
second
in
that
of
P
.
The
actual
corresp
ondence
relies
on
the
notion
of
self-r
e
ducibility,
whic
h
relates
the
complexit
y
of
determining
whether
a
solution
exists
to
the
complexit
y
of
actually
nding
one.
Notes
tak
en
b
y
Eilon
Reshef.
Lecture
:
NP-completeness
and
Self
Reducibilit
y
.
W
e
pro
v
e
that
an
y
relation
dening
an
NP-complete
language
is
self-reducible.
This
will
b
e
done
using
the
SA
T
self-reducibilit
y
(pro
v
ed
in
Lecture
),
and
the
fact
that
SA
T
is
NP-Hard
under
Levin
Reductions.
The
latter
are
Karp
Reductions
augmen
ted
b
y
ecien
t
transformations
of
NP-witnesses
from
the
original
instance
to
the
reduced
one,
and
vice
v
ersa.
Along
the
w
a
y
,
w
e
giv
e
a
simple
pro
of
of
the
existence
of
NP-Complete
languages
(b
y
pro
ving
that
Bounded
Halting
is
NP-Complete).
Notes
tak
en
b
y
Nir
Piterman
and
Dana
Fisman.
Lecture
:
More
on
NP
and
some
on
DTIME.
In
the
rst
part
of
this
lecture
w
e
discuss
t
w
o
prop
erties
of
the
complexit
y
classes
P
,
NP
and
NPC:
The
prop
ert
y
is
that
NP
con
tains
problems
whic
h
are
neither
NP-complete
nor
in
P
(pro
vided
NP
=
P),
and
the
second
one
is
that
NP-relations
ha
v
e
optimal
searc
h
algorithms.
In
the
second
part
w
e
dene
new
complexit
y
classes
based
on
exact
time
b
ounds,
and
consider
some
relations
b
et
w
een
them.
W
e
p
oin
t
out
the
sensitivit
y
of
these
classes
to
the
sp
ecic
mo
del
of
computation
(e.g.,
one-tap
e
v
ersus
t
w
o-tap
e
T
uring
mac
hines).
Notes
tak
en
b
y
Mic
hael
Elkin
and
Ek
aterina
Sedletsky
.
Lecture
:
Space
Complexit
y
.
W
e
dene
\nice"
complexit
y
b
ounds;
these
are
b
ounds
whic
h
can
b
e
computed
within
the
resources
they
supp
osedly
b
ound
(e.g.,
w
e
fo
cus
on
time-constructible
and
space-constructible
b
ounds).
W
e
dene
space
complexit
y
using
an
adequate
mo
del
of
compu-
tation
in
whic
h
one
is
not
allo
w
ed
to
use
the
area
o
ccupied
b
y
the
input
for
computation.
Before
dismissing
sub-logarithmic
space,
w
e
presen
t
t
w
o
results
regarding
it
(con
trasting
sub-loglog
space
with
loglog
space).
W
e
sho
w
that
for
\nice"
complexit
y
b
ounds,
there
is
a
hierarc
h
y
of
complexit
y
classes
{
the
more
resources
one
has
the
more
tasks
one
can
p
erform.
One
the
other
hand,
w
e
men
tion
that
this
increase
in
p
o
w
er
ma
y
not
happ
en
if
the
complexit
y
b
ounds
are
not
\nice".
Notes
tak
en
b
y
Leia
P
assoni
and
Reub
en
Sumner.
IX

X
Lecture
:
Non-Deterministic
Space.
W
e
recall
t
w
o
basic
facts
ab
out
deterministic
space
complexit
y
,
and
then
dene
non-deterministic
space
complexit
y
.
Three
alternativ
e
mo
dels
for
mea-
suring
non-deterministic
space
complexit
y
are
in
tro
duced:
the
standard
non-deterministic
mo
del,
the
online
mo
del
and
the
oine
mo
del.
The
equiv
alence
b
et
w
een
the
non-deterministic
and
online
mo
dels
and
their
exp
onen
tial
relation
to
the
oine
mo
del
are
pro
v
ed.
W
e
then
turn
to
in
v
esti-
gate
the
relation
b
et
w
een
the
non-deterministic
and
deterministic
space
complexit
y
(i.e.,
Sa
vitc
h's
Theorem).
Notes
tak
en
b
y
Y
oad
Lustig
and
T
al
Hassner.
Lecture
:
Non-Deterministic
Logarithmic
Space
W
e
further
discuss
comp
osition
lemmas
underlying
previous
lectures.
Then
w
e
study
the
complexit
y
class
N
L
(the
set
of
languages
decid-
able
within
Non-Deterministic
Logarithmic
Space):
W
e
sho
w
that
directed
graph
connectivit
y
is
complete
for
N
L
.
Finally
,
w
e
pro
v
e
that
N
L
=
co
N
L
(i.e.,
N
L
class
is
closed
under
complemen-
tation).
Notes
tak
en
b
y
Amiel
F
erman
and
Noam
Sadot.
Lecture
:
Randomized
Computations
W
e
extend
the
notion
of
ecien
t
computation
b
y
al-
lo
wing
algorithms
(T
uring
mac
hines)
to
toss
coins.
W
e
study
the
classes
of
languages
that
arise
from
v
arious
natural
denitions
of
acceptance
b
y
suc
h
mac
hines.
W
e
fo
cus
on
probabilistic
p
olynomial-
time
mac
hines
with
one-sided,
t
w
o-sided
and
zero
error
probabilit
y
(dening
the
classes
RP
(and
co
RP
),
B
P
P
and
Z
P
P
).
W
e
also
consider
probabilistic
mac
hines
that
uses
logarithmic
spaces
(i.e.,
the
class
RL).
Notes
tak
en
b
y
Erez
W
aisbard
and
Gera
W
eiss.
Lecture
:
Non-Uniform
P
olynomial
Time
(P
/P
oly).
W
e
in
tro
duce
the
notion
of
non-
uniform
p
olynomial-time
and
the
corresp
onding
complexit
y
class
P
/p
oly
.
In
this
(somewhat
cti-
tious)
computational
mo
del,
T
uring
mac
hines
are
pro
vided
an
external
advice
string
to
aid
them
in
their
computation
(on
strings
of
certain
length).
The
non-uniformity
is
expressed
in
the
fact
that
an
arbitrary
advice
string
ma
y
b
e
dened
for
ev
ery
dieren
t
length
of
input.
W
e
sho
w
that
P
/p
oly
\upp
er
b
ounds"
the
notion
of
ecien
t
computation
(as
B
P
P

P
/p
oly),
y
et
this
upp
er
b
ound
is
not
tigh
t
(as
P
/p
oly
con
tains
non-recursiv
e
languages).
The
eect
of
in
tro
ducing
uni-
formity
is
discussed,
and
sho
wn
to
collapse
P
/p
oly
to
P
.
Finally
,
w
e
relate
the
P
/p
oly
v
ersus
N
P
question
to
the
question
of
whether
NP-completeness
via
Co
ok-reductions
is
more
p
o
w
erful
that
NP-completeness
via
Karp-reductions.
This
is
done
b
y
sho
wing,
on
one
hand,
that
N
P
is
Co
ok-reducible
to
a
sparse
set
i
N
P

P
=p
oly
,
and
on
the
other
hand
that
N
P
is
Karp-reducible
to
a
sparse
set
i
N
P
=
P
.
Notes
tak
en
b
y
Moshe
Lew
enstein,
Y
eh
uda
Lindell
and
T
amar
Seeman.
Lecture
	:
The
P
olynomial
Hierarc
h
y
(PH).
W
e
dene
a
hierarc
h
y
of
complexit
y
classes
extending
N
P
and
con
tained
in
PSP
A
CE.
This
is
done
in
t
w
o
w
a
ys,
sho
wn
equiv
alen
t:
The
rst
b
y
generalizing
the
notion
of
Co
ok
reductions,
and
the
second
b
y
generalizing
the
denition
of
N
P
.
W
e
then
relate
this
hierarc
h
y
to
complexit
y
classes
discussed
in
previous
lectures
suc
h
as
B
P
P
and
P
/P
oly:
W
e
sho
w
that
B
P
P
is
in
PH,
and
that
if
N
P

P
=p
oly
then
PH
collapses
to
is
second
lev
el.
Notes
tak
en
b
y
Ronen
Mizrahi.

XI
Lecture
0:
The
coun
ting
class
#P
.
The
class
N
P
captures
the
dicult
y
of
determining
whether
a
giv
en
input
has
a
solution
with
resp
ect
to
some
(tractable)
relation.
A
p
oten
tially
harder
question,
captured
b
y
the
class
#P
,
refers
to
determining
the
n
um
b
er
of
suc
h
solutions.
W
e
rst
dene
the
complexit
y
class
#P
,
and
classify
it
with
resp
ect
to
other
complexit
y
classes.
W
e
then
pro
v
e
the
existence
of
#P
-complete
problems,
and
men
tion
some
natural
ones.
Then
w
e
try
to
study
the
relation
b
et
w
een
#P
and
N
P
more
exactly
,
b
y
sho
wing
w
e
can
probabilistically
appro
ximate
#P
using
an
oracle
in
N
P
.
Finally
,
w
e
rene
this
result
b
y
restricting
the
oracle
to
a
w
eak
form
of
S
AT
(called
uniq
ueS
AT
).
Notes
tak
en
b
y
Oded
Lac
hish,
Y
oa
v
Ro
deh
and
Y
ael
T
auman.
Lecture
:
In
teractiv
e
Pro
of
Systems.
W
e
in
tro
duce
the
notion
of
in
teractiv
e
pro
of
systems
and
the
complexit
y
class
IP,
emphasizing
the
role
of
randomness
and
in
teraction
in
this
mo
del.
The
concept
is
demonstrated
b
y
giving
an
in
teractiv
e
pro
of
system
for
Graph
Non-Isomorphism.
W
e
discuss
the
p
o
w
er
of
the
class
IP,
and
pro
v
e
that
co
N
P

I
P
.
W
e
discuss
issues
regarding
the
n
um
b
er
of
rounds
in
a
pro
of
system,
and
v
arian
ts
of
the
mo
del
suc
h
as
public-coin
systems
(a.k.a.
Arth
ur-Merlin
games).
Notes
tak
en
b
y
Dann
y
Harnik,
Tzvik
a
Hartman
and
Hillel
Kugler.
Lecture
:
Probabilistically
Chec
k
able
Pro
of
(PCP).
W
e
in
tro
duce
the
notion
of
Prob-
abilistically
Chec
k
able
Pro
of
(PCP)
systems.
W
e
discuss
some
complexit
y
measures
in
v
olv
ed,
and
describ
e
the
class
of
languages
captured
b
y
corresp
onding
PCP
systems.
W
e
then
demonstrate
the
alternativ
e
view
of
N
P
emerging
from
the
PCP
Characterization
Theorem,
and
use
it
in
order
to
pro
v
e
non-appro
ximabilit
y
results
for
the
problems
maxS
AT
and
maxC
LI
QU
E
.
Notes
tak
en
b
y
Alon
Rosen
and
V
ered
Rosen.
Lecture
:
Pseudorandom
Generators.
Pseudorandom
generators
are
dened
as
ecien
t
deterministic
algorithms
whic
h
stretc
h
short
random
seeds
in
to
longer
pseudorandom
sequences.
The
latter
are
indistiguishable
from
truely
random
sequences
b
y
an
y
ecien
t
observ
er.
W
e
sho
w
that,
for
ecien
tly
sampleable
distributions,
computational
indistiguishabilit
y
is
preserv
ed
under
m
ultiple
samples.
W
e
related
pseudorandom
generators
and
one-w
a
y
functions,
and
sho
w
ho
w
to
increase
the
stretc
hing
of
pseudorandom
generators.
The
notes
are
augmen
ted
b
y
an
essa
y
of
Oded.
Notes
tak
en
b
y
Sergey
Benditkis,
Il'y
a
Safro
and
Boris
T
emkin.
Lecture
:
Pseudorandomness
and
Computational
Dicult
y
.
W
e
con
tin
ue
our
discus-
sion
of
pseudorandomness
and
sho
w
a
connection
b
et
w
een
pseudorandomness
and
computational
dicult
y
.
Sp
ecically
,
w
e
sho
w
ho
w
the
dicult
y
of
in
v
erting
one-w
a
y
functions
ma
y
b
e
utilized
to
obtain
a
pseudorandom
generator.
Finally
,
w
e
state
and
pro
v
e
that
a
hard-to-predict
bit
(called
a
hard-core)
ma
y
b
e
extracted
from
an
y
one-w
a
y
function.
The
hard-core
is
fundamen
tal
in
our
construction
of
a
generator.
Notes
tak
en
b
y
Moshe
Lew
enstein
and
Y
eh
uda
Lindell.

XI
I
Lecture
:
Derandomization
of
BPP
.
W
e
presen
t
an
ecien
t
deterministic
sim
ulation
of
randomized
algorithms.
This
pro
cess,
called
derandomization,
in
tro
duce
new
notions
of
pseudoran-
dom
generators.
W
e
extend
the
denition
of
pseudorandom
generators
and
sho
w
ho
w
to
construct
a
generator
that
can
b
e
used
for
derandomization.
The
new
construction
dier
from
the
generator
that
constructed
in
the
previous
lecture
in
it's
running
time
(it
will
run
slo
w
er,
but
fast
enough
for
the
sim
ulation).
The
b
enet
is
that
it
is
relying
on
a
seemingly
w
eak
er
assumption.
Notes
tak
en
b
y
Erez
W
aisbard
and
Gera
W
eiss.
Lecture
:
Derandomizing
Space-Bounded
Computations.
W
e
consider
derandomiza-
tion
of
space-b
ounded
computations.
W
e
sho
w
that
B
P
L

D
S
P
AC
E
(log

n),
namely
,
an
y
b
ounded-probabilit
y
Logspace
algorithm
can
b
e
deterministically
em
ulated
in
O
(log

n)
space.
W
e
further
sho
w
that
B
P
L

S
C
,
namely
,
an
y
suc
h
algorithm
can
b
e
deterministically
em
ulated
in
O
(log

n)
space
and
(sim
ultaneously)
in
p
olynomial
time.
Notes
tak
en
b
y
Eilon
Reshef.
Lecture
:
Zero-Kno
wledge
Pro
of
Systems.
W
e
in
tro
duce
the
notion
of
zero-kno
wledge
in
teractiv
e
pro
of
system,
and
consider
an
example
of
suc
h
a
system
(Graph
Isomorphism).
W
e
dene
p
erfect,
statistical
and
computational
zero-kno
wledge,
and
presen
t
a
metho
d
for
constructing
zero-kno
wledge
pro
ofs
for
N
P
languages,
whic
h
mak
es
essen
tial
use
of
bit
commitmen
t
sc
hemes.
W
e
men
tion
that
zero-kno
wledge
is
preserv
ed
under
sequen
tial
comp
osition,
but
is
not
preserv
ed
under
the
parallel
rep
etition.
Notes
tak
en
b
y
Mic
hael
Elkin
and
Ek
aterina
Sedletsky
.
Lecture
:
NP
in
PCP[p
oly
,O()].
The
main
result
in
this
lecture
is
N
P

P
C
P
(pol
y
;
O
()).
In
the
course
of
the
pro
of
w
e
in
tro
duce
an
N
P
C
language
\Quadratic
Equations",
and
sho
w
it
to
b
e
in
P
C
P
(pol
y
;
O
()).
The
argumen
t
pro
ceeds
in
t
w
o
stages:
First
assuming
prop
erties
of
the
pro
of
(oracle),
and
then
testing
these
prop
erties.
An
in
termediate
result
that
of
indep
enden
t
in
terest
is
an
ecien
t
probabilistic
algorithm
that
distinguishes
b
et
w
een
linear
and
far-from-linear
functions.
Notes
tak
en
b
y
Y
oad
Lustig
and
T
al
Hassner.
Lecture
	:
Dtime(t)
con
tained
in
Dspace(t/log
t).
W
e
pro
v
e
that
D
time(t())

D
space(t()=
log
t()).
That
is,
w
e
sho
w
ho
w
to
sim
ulate
an
y
giv
en
deterministic
m
ulti-tap
e
T
uring
Mac
hine
(TM)
of
time
complexit
y
t,
using
a
deterministic
TM
of
space
complexit
y
t=
log
t.
A
main
ingredian
t
in
the
sim
ulation
is
the
analysis
of
a
p
ebble
game
on
directed
b
ounded-degree
graphs.
Notes
tak
en
b
y
T
amar
Seeman
and
Reub
en
Sumner.
Lecture
0:
Circuit
Depth
and
Space
Complexit
y
.
W
e
study
some
of
the
relations
b
et
w
een
Bo
olean
circuits
and
T
uring
mac
hines.
W
e
dene
the
complexit
y
classes
N
C
and
AC
,
compare
their
computational
p
o
w
er,
and
p
oin
t
out
the
p
ossible
connection
b
et
w
een
uniform-N
C
and
\ecien
t"
parallel
computation.
W
e
conclude
the
discussion
b
y
establishing
a
strong
connection
b
et
w
een
space
complexit
y
and
depth
of
circuits
with
b
ounded
fan-in.
Notes
tak
en
b
y
Alon
Rosen
and
V
ered
Rosen.

XI
I
I
Lecture
:
Comm
unication
Complexit
y
.
W
e
consider
Comm
unication
Complexit
y
{
the
analysis
of
the
amoun
t
of
information
that
needs
to
b
e
comm
unicated
b
et
w
en
t
w
o
parties
whic
h
wish
to
reac
h
a
common
computational
goal.
W
e
start
with
some
basic
denitions,
considering
b
oth
deterministic
and
probabilistic
mo
dels
for
the
problem,
and
annotating
our
discussion
with
a
few
examples.
Next
w
e
presen
t
a
couple
of
to
ols
for
pro
ving
lo
w
er
b
ounds
on
the
complexit
y
of
comm
unication
problems.
W
e
conclude
b
y
pro
ving
a
linear
lo
w
er
b
ound
on
the
comm
unication
complexit
y
of
probabilistic
proto
cols
for
computing
the
inner
pro
duct
of
t
w
o
v
ectors,
where
initially
eac
h
part
y
holds
one
v
ector.
Notes
tak
en
b
y
Amiel
F
erman
and
Noam
Sadot.
Lecture
:
Circuit
Depth
and
Comm
unication
Complexit
y
.
The
main
result
presen
ted
in
this
lecture
is
a
(tigh
t)
non
trivial
lo
w
er
b
ound
on
the
monotone
circuit
depth
of
s-t-Connectivit
y
.
This
is
pro
v
ed
via
a
series
of
reductions,
the
rst
of
whic
h
is
of
signican
t
imp
ortance:
A
connection
b
et
w
een
circuit
depth
and
comm
unication
complexit
y
.
W
e
then
get
a
comm
unication
game
and
pro
ceed
to
reduce
it
to
other
suc
h
games,
un
til
reac
hing
a
game
called
F
ORK.
W
e
conclude
that
a
lo
w
er
b
ound
on
the
comm
unication
complexit
y
of
F
ORK,
to
b
e
giv
en
in
the
next
lecture,
will
yield
an
analogous
lo
w
er
b
ound
on
the
monotone
circuit
depth
of
s-t-Connectivit
y
.
Notes
tak
en
b
y
Y
oa
v
Ro
deh
and
Y
ael
T
auman.
Lecture
:
Depth
Lo
w
er
Bound
for
Monotone
Circuits
(con
t.).
W
e
analyze
the
f
ork
game,
in
tro
duced
in
the
previous
lecture.
W
e
giv
e
tigh
t
lo
w
er
and
upp
er
b
ounds
on
the
comm
u-
nication
needed
in
a
proto
col
solving
f
ork.
This
completes
the
pro
of
of
the
lo
w
er
b
ound
on
the
depth
of
monotone
circuits
computing
the
function
st-Connectivit
y
.
Notes
tak
en
b
y
Dana
Fisman
and
Nir
Piterman.
Lecture
:
Av
erage-Case
Complexit
y
.
W
e
in
tro
duce
a
theory
of
a
v
erage-case
complexit
y
whic
h
refers
to
computational
problems
coupled
with
probabilit
y
distributions.
W
e
start
b
y
dening
and
discussing
the
classes
of
P-computable
and
P-samplable
distributions.
W
e
then
dene
the
class
DistNP
(whic
h
consists
of
NP
problems
coupled
with
P-computable
distributions),
and
discuss
the
notion
of
a
v
erage
p
olynomial-time
(whic
h
is
unfortunately
more
subtle
than
it
ma
y
seem).
Finally
,
w
e
dene
and
discuss
reductions
b
et
w
een
distributional
problems.
W
e
conclude
b
y
pro
ving
the
existence
of
a
complete
problem
for
DistNP
.
Notes
tak
en
b
y
Tzvik
a
Hartman
and
Hillel
Kugler.
Lecture
:
Computational
Learning
Theory
.
W
e
dene
a
mo
del
of
automoatic
learning
called
probably
appro
ximately
correct
(P
A
C)
learning.
W
e
dene
ecien
t
P
A
C
learning,
and
presen
t
sev
eral
ecien
t
P
A
C
learning
algorithms.
W
e
pro
v
e
the
Occam's
Razor
Theorem,
whic
h
reduces
the
P
A
C
learning
problem
to
the
problem
of
nding
a
succinct
represen
tation
for
the
v
alues
of
a
large
n
um
b
er
of
giv
en
lab
eled
examples.
Notes
tak
en
b
y
Oded
Lac
hish
and
Eli
P
orat.

XIV
Lecture
:
Relativization.
In
this
lecture
w
e
deal
with
relativization
of
complexit
y
classes.
In
particular,
w
e
discuss
the
role
of
relativization
with
resp
ect
to
the
P
vs.
N
P
question;
that
is,
w
e
shall
see
that
for
some
oracle
A,
P
A
=
N
P
A
,
whereas
for
another
A
(actually
for
almost
all
other
A's)
P
A
=
N
P
A
.
Ho
w
ev
er,
it
also
holds
that
I
P
A
=
P
S
P
AC
E
A
for
a
random
A,
whereas
I
P
=
P
S
P
AC
E
Notes
tak
en
b
y
Leia
P
assoni.

Con
ten
ts
Preface
I
I
I
Ac
kno
wledgmen
ts
VI
I
Lecture
Summaries
IX

The
P
vs
NP
Question

.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
Complexit
y
Class
N
P
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Searc
h
Problems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Self
Reducibilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


NP-completeness
and
Self
Reducibilit
y
	
.
Reductions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
All
N
P
-complete
relations
are
Self-reducible
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
B
ounded
H
al
ting
is
N
P
 complete
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
C
ir
cuit
S
atisf
iabil
ity
is
N
P
 complete
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
R
S
AT
is
N
P
 complete
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix:
Details
for
the
reduction
of
B
H
to
C
S
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


More
on
NP
and
some
on
DTIME

.
Non-complete
languages
in
NP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Optimal
algorithms
for
NP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
General
Time
complexit
y
classes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
The
DTime
classes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Time-constructibilit
y
and
t
w
o
theorems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix:
Pro
of
of
Theorem
.,
via
crossing
sequences
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Space
Complexit
y

.
On
Dening
Complexit
y
Classes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Space
Complexit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sub-Logarithmic
Space
Complexit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Hierarc
h
y
Theorems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Odd
Phen
umena
(The
Gap
and
Sp
eed-Up
Theorems)
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

XV

XVI
CONTENTS

Non-Deterministic
Space

.
Preliminaries
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Non-Deterministic
space
complexit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Denition
of
mo
dels
(online
vs
oine)
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Relations
b
et
w
een
N
S
P
AC
E
on
and
N
S
P
AC
E
of
f
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Relations
b
et
w
een
Deterministic
and
Non-Deterministic
space
:
:
:
:
:
:
:
:
:
:
:
:

..
Sa
vitc
h's
Theorem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
A
translation
lemma
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Inside
Non-Deterministic
Logarithmic
Space

.
The
comp
osition
lemma
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
A
complete
problem
for
N
L
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Discussion
of
Reducibilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
The
complete
problem:
directed-graph
connectivit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Complemen
ts
of
complexit
y
classes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Immerman
Theorem:
N
L
=
co
N
L
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Theorem
.	
implies
N
L
=
co
N
L
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Pro
of
of
Theorem
.	
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Randomized
Computations

.
Probabilistic
computations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
classes
R
P
and
coR
P
{
One-Sided
Error
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
class
B
P
P
{
Tw
o-Sided
Error
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
The
class
P
P
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
class
Z
P
P
{
Zero
error
probabilit
y
.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Randomized
space
complexit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
The
denition
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Undirected
Graph
Connectivit
y
is
in
RL
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	0

Non-Uniform
P
olynomial
Time
(P
/P
oly)
	
.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
The
Actual
Denition
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
P
/p
oly
and
the
P
v
ersus
N
P
Question
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
The
P
o
w
er
of
P
/p
oly
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Uniform
F
amilies
of
Circuits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Sparse
Languages
and
the
P
v
ersus
N
P
Question
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
		
	
The
P
olynomial
Hierarc
h
y
(PH)
0
	.
The
Denition
of
the
class
PH
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
	..
First
denition
for
PH:
via
oracle
mac
hines
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
	..
Second
denition
for
PH:
via
quan
tiers
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
	..
Equiv
alence
of
denitions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
	.
Easy
Computational
Observ
ations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
	.
BPP
is
con
tained
in
PH
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0	

CONTENTS
XVI
I
	.
If
NP
has
small
circuits
then
PH
collpases
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix:
Pro
of
of
Prop
osition
	..
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0
The
coun
ting
class
#P

0.
Dening
#P
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0.
Completeness
in
#P
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0.
Ho
w
close
is
#P
to
N
P
?
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
V
arious
Lev
els
of
Appro
ximation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
Probabilistic
Co
ok
Reduction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
Gap

#S
AT
Reduces
to
S
AT
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0.
Reducing
to
uniq
ueS
AT
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix
A:
A
F
amily
of
Univ
ersal

Hash
F
unctions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix
B:
Pro
of
of
Lefto
v
er
Hash
Lemma
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


In
teractiv
e
Pro
of
Systems

.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
Denition
of
IP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Commen
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Example
{
Graph
Non-Isomorphism
(GNI)
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
P
o
w
er
of
IP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
IP
is
con
tained
in
PSP
A
CE
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
coNP
is
con
tained
in
IP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Public-Coin
Systems
and
the
Num
b
er
of
Rounds
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
P
erfect
Completeness
and
Soundness
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Probabilistically
Chec
k
able
Pro
of
Systems
	
.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
The
Denition
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
The
basic
mo
del
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Complexit
y
Measures
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Some
Observ
ations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
PCP
c
haracterization
of
NP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Imp
ortance
of
Complexit
y
P
arameters
in
PCP
Systems
:
:
:
:
:
:
:
:
:
:
:
:

..
The
PCP
Theorem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
The
PCP
Theorem
giv
es
rise
to
\robust"
N
P
-relations
:
:
:
:
:
:
:
:
:
:
:
:

..
Simplifying
assumptions
ab
out
P
C
P
(log
;
O
())
v
eriers
:
:
:
:
:
:
:
:
:
:
:
:

.
PCP
and
non-appro
ximabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Amplifying
Reductions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
PCP
Theorem
Rephrased
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Connecting
PCP
and
non-appro
ximabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


XVI
I
I
CONTENTS

Pseudorandom
Generators

.
Instead
of
an
in
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Computational
Indistinguishabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Tw
o
v
arian
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Relation
to
Statistical
Closeness
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Computational
indistinguishabilit
y
and
m
ultiple
samples
:
:
:
:
:
:
:
:
:
:
:
:

.
PR
G:
Denition
and
amplication
of
the
stretc
h
function
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
On
Using
Pseudo-Random
Generators
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Relation
to
one-w
a
y
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix:
An
essa
y
b
y
O.G.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
The
Denition
of
Pseudorandom
Generators
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Ho
w
to
Construct
Pseudorandom
Generators
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Pseudorandom
F
unctions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
The
Applicabilit
y
of
Pseudorandom
Generators
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
The
In
telectual
Con
ten
ts
of
Pseudorandom
Generators
:
:
:
:
:
:
:
:
:
:
:
:
:

..
A
General
P
aradigm
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

References
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Pseudorandomness
and
Computational
Dicult
y
	
.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Denitions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	0
.
A
Pseudorandom
Generator
based
on
a
-
One-W
a
y
F
unction
:
:
:
:
:
:
:
:
:
:
:
:
	
.
A
Hard-Core
for
An
y
One-W
a
y
F
unction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	

Derandomization
of
B
P
P
		
.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
		
.
New
notion
of
Pseudorandom
generator
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Construction
of
non-iterativ
e
pseudorandom
generator
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
P
arameters
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
T
o
ol
:
An
unpredictable
predicate
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
T
o
ol
:
A
design
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
The
construction
itself
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Constructions
of
a
design
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
First
construction:
using
GF
(l
)
arithmetic
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Second
construction:
greedy
algorithm
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Derandomizing
Space-Bounded
Computations

.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
Mo
del
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Execution
Graphs
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Univ
ersal
Hash
F
unctions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Construction
Ov
erview
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
Pseudorandom
Generator
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Analysis
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	

CONTENTS
XIX
.
Extensions
and
Related
Results
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
B
P
L

S
C
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
F
urther
Results
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Zero-Kno
wledge
Pro
of
Systems

.
Denitions
and
Discussions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Graph
Isomorphism
is
in
Zero-Kno
wledge
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Zero-Kno
wledge
Pro
ofs
for
NP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Zero-Kno
wledge
NP-pro
of
systems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
NP

ZK
(o
v
erview)
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Digital
implemen
tation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
V
arious
commen
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Remark
ab
out
parallel
rep
etition
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Remark
ab
out
randomness
in
zero-kno
wledge
pro
ofs
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


NP
in
PCP[p
oly
,O()]

.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Quadratic
Equations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
main
strategy
and
a
tactical
maneuv
er
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
T
esting
satisabilit
y
assuming
a
nice
oracle
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Distinguishing
a
nice
oracle
from
a
v
ery
ugly
one
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
T
ests
of
linearit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Assuming
linear

testing

's
co
ecien
ts
structure
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Gluing
it
all
together
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix
A:
Linear
functions
are
far
apart
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
App
endix
B:
The
linearit
y
test
for
functions
far
from
linear
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
	
Dtime
vs
Dspace

	.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

	.
Main
Result
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

	.
Additional
Pro
ofs
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

	..
Pro
of
of
Lemma
	..
(Canonical
Computation
Lemma)
:
:
:
:
:
:
:
:
:
:
:

	..
Pro
of
of
Theorem
	.
(P
ebble
Game
Theorem)
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
0
Circuit
Depth
and
Space
Complexit
y

0.
Bo
olean
Circuits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
The
Denition
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
Some
Observ
ations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
F
amilies
of
Circuits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0.
Small-depth
circuits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
The
Classes
N
C
and
AC
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
Sk
etc
h
of
the
pro
of
of
AC
0

N
C

:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0..
N
C
and
P
arallel
Computation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

0.
On
Circuit
Depth
and
Space
Complexit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


XX
CONTENTS
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Comm
unication
Complexit
y

.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Basic
mo
del
and
some
examples
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Deterministic
v
ersus
Probabilistic
Complexit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Equalit
y
revisited
and
the
Input
Matrix
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Rank
Lo
w
er
Bound
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Inner-Pro
duct
lo
w
er
b
ound
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	

Monotone
Circuit
Depth
and
Comm
unication
Complexit
y
	
.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Hard
F
unctions
Exist
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Bounded
Depth
Circuits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Monotone
Circuits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Comm
unication
Complexit
y
and
Circuit
Depth
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
The
Monotone
Case
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
		
..
The
Analogous
Game
and
Connection
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
00
..
An
Equiv
alen
t
Restricted
Game
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Tw
o
More
Games
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0

The
F
ORK
Game
0
.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
The
f
ork
game
{
recalling
the
denition
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
An
upp
er
b
ound
for
the
f
ork
game
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
A
lo
w
er
b
ound
for
the
f
ork
game
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0	
..
Denitions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0	
..
Reducing
the
densit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Reducing
the
length
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Applying
the
lemmas
to
get
the
lo
w
er
b
ound
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Av
erage
Case
Complexit
y

.
In
tro
duction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Denitions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Distributions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Distributional
Problems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Distributional
Classes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Distributional-NP
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Av
erage
P
olynomial
Time
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Reductions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
DistNP-completeness
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix
A
:
F
ailure
of
a
naiv
e
form
ulation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix
B
:
Pro
of
Sk
etc
h
of
Prop
osition
..
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


CONTENTS
XXI

Computational
Learning
Theory

.
T
o
w
ards
a
denition
of
Computational
learning
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Probably
Appro
ximately
Correct
(P
AC
)
Learning
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Occam's
Razor
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Generalized
denition
of
P
AC
learning
algorithm
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Reductions
among
learning
tasks
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Generalized
forms
of
Occam's
Razor
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
(V
C)
V
apnik-Cherv
onenkis
Dimension
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
An
example:
V
C
dimension
of
axis
aligned
rectangles
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
General
b
ounds
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

App
endix:
Filling-up
gaps
for
the
pro
of
of
Claim
..
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Relativization

.
Relativization
of
Complexit
y
Classes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
The
P
?
=
N
P
question
Relativized
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Relativization
with
a
Random
Oracle
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Conclusions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

Bibliographic
Notes
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Lecture

The
P
vs
NP
Question
Notes
tak
en
b
y
Eilon
Reshef
Summary:
W
e
review
the
fundamen
tal
question
of
computer
science,
kno
wn
as
P
?
=
N
P
:
giv
en
a
problem
whose
solution
can
b
e
v
eried
ecien
tly
(i.e.,
in
p
olynomial
time),
is
there
necessarily
an
ecien
t
metho
d
to
actually
nd
suc
h
a
solution?
First,
w
e
de-
ne
the
notion
of
N
P
,
i.e.,
the
class
of
all
problems
whose
solution
can
b
e
v
eried
in
p
olynomial-time.
Next,
w
e
discuss
ho
w
to
represen
t
searc
h
problems
in
the
ab
o
v
e
framew
ork.
W
e
conclude
with
the
notion
of
self-r
e
ducibility,
relating
the
hardness
of
determining
whether
a
feasible
solution
exists
to
the
hardness
of
actually
nding
one.
.
In
tro
duction
Whereas
the
researc
h
in
complexit
y
theory
is
still
in
its
infancy
,
and
man
y
more
questions
are
op
en
than
closed,
man
y
of
the
concepts
and
results
in
the
eld
ha
v
e
an
extreme
conceptual
imp
ortance
and
represen
t
signican
t
in
tellectual
ac
hiev
emen
ts.
Of
the
more
fundamen
tal
questions
in
this
area
is
the
relation
b
et
w
een
dieren
t
a
v
ors
of
a
problem:
the
se
ar
ch
problem,
i.e.,
nding
a
feasible
solution,
the
de
cision
problem,
i.e.,
determining
whether
a
feasible
solution
exists,
and
the
veric
ation
problem,
i.e.,
deciding
whether
a
giv
en
solution
is
correct.
T
o
initiate
a
formal
discussion,
w
e
assume
basic
kno
wledge
of
elemen
tary
notions
of
computabil-
it
y
,
suc
h
as
T
urning
mac
hines,
reductions,
p
olynomial-time
computabilit
y
,
and
so
on.
.
The
Complexit
y
Class
N
P
In
this
section
w
e
recall
the
denition
of
the
complexit
y
class
N
P
and
o
v
erview
some
of
its
basic
prop
erties.
Recall
that
the
complexit
y
class
P
is
the
collection
of
all
languages
L
that
can
b
e
recognized
\ecien
tly",
i.e.,
b
y
a
deterministic
p
olynomial-time
T
uring
mac
hine.
Whereas
the
traditional
denition
of
N
P
asso
ciates
the
class
N
P
with
the
collection
of
languages
that
can
b
e
ecien
tly
recognized
b
y
a
non-deterministic
T
urning
mac
hine,
w
e
pro
vide
an
alternativ
e
denition,
that
in
our
view
b
etter
captures
the
conceptual
con
ten
ts
of
the
class.
Informally
,
w
e
view
N
P
as
the
class
of
all
languages
that
admit
a
short
\certicate"
for
mem-
b
ership
in
the
language.
Giv
en
this
certicate,
called
a
witness,
mem
b
ership
in
the
language
can
b
e
v
eried
ecien
tly
,
i.e.,
in
p
olynomial
time.



LECTURE
.
THE
P
VS
NP
QUESTION
F
or
the
sak
e
of
self-con
tainmen
t,
w
e
recall
that
a
(binary)
relation
R
is
p
olynomial-time
de
cidable
if
there
exists
a
p
olynomial-time
T
uring
mac
hine
that
accepts
the
language
fE
(x;
y
)
j
(x;
y
)

R
g,
where
E
(x;
y
)
is
a
unique
enco
ding
of
the
pair
(x;
y
).
An
example
of
suc
h
an
enco
ding
is
E
(





n
;






m
)

=








n

n
0







n

n
.
W
e
are
no
w
ready
to
in
tro
duce
a
denition
of
N
P
.
Denition
.
The
c
omplexity
class
N
P
is
the
class
of
al
l
languages
L
for
which
ther
e
exists
a
r
elation
R
L

f0;
g


f0;
g

,
such
that

R
L
is
p
olynomial-time
de
cidable.

Ther
e
exists
a
p
olynomial
b
L
such
that
x

L
if
and
only
if
ther
e
exists
a
witness
w
,
jw
j

b
L
(jxj)
for
which
(x;
w
)

R
L
.
Note
that
the
p
olynomial
b
ound
in
the
second
condition
is
required
despite
the
fact
that
R
L
is
p
olynomial-time
decidable,
since
the
p
olynomialit
y
of
R
L
is
measured
with
resp
ect
to
the
length
of
the
pair
(x;
y
),
and
not
with
resp
ect
to
jxj
only
.
It
is
imp
ortan
t
to
note
that
if
x
is
not
in
L,
there
is
no
p
olynomial-size
witness
w
for
whic
h
(x;
w
)

R
L
.
Also,
the
fact
that
(x;
y
)

R
L
do
es
not
imply
that
x

L,
but
rather
that
y
is
not
a
prop
er
witness
for
x.
A
sligh
tly
dieren
t
denition
ma
y
sometimes
b
e
con
v
enien
t.
This
denition
allo
ws
only
p
olynomial
ly-
b
ounde
d
relations,
i.e.,
Denition
.
A
r
elation
R
is
p
olynomially
b
ounded
if
ther
e
exists
a
p
olynomial
p(),
such
that
for
every
(x;
y
)

R
,
jy
j

p(jxj).
Since
a
comp
osition
of
t
w
o
p
olynomials
is
also
a
p
olynomial,
an
y
p
olynomial
in
p(jxj),
where
p
is
a
p
olynomial,
is
also
p
olynomial
in
jxj.
Th
us,
if
a
p
olynomially-b
ounded
relation
R
can
b
e
decided
in
p
olynomial-time,
it
can
also
b
e
decided
in
time
p
olynomial
in
the
size
of
rst
elemen
t
in
the
pair
(x;
y
)

R
.
No
w,
denition
.
of
N
P
can
b
e
also
form
ulated
as:
Denition
.
The
c
omplexity
class
N
P
is
the
class
of
al
l
languages
L
for
which
ther
e
exists
a
p
olynomially-b
ounded
r
elation
R
L

f0;
g


f0;
g

,
such
that

R
L
is
p
olynomial-time
de
cidable.

x

L
if
and
only
if
ther
e
exists
a
witness
w
,
for
which
(x;
w
)

R
L
.
In
this
view,
the
fundamen
tal
question
of
computer
science,
i.e.,
P
?
=
N
P
can
b
e
form
ulated
as
the
question
whether
the
existenc
e
of
a
short
witness
(as
implied
b
y
mem
b
ership
in
N
P
)
necessarily
brings
ab
out
an
ecien
t
algorithm
for
nding
suc
h
a
witness
(as
required
for
mem
b
ership
in
P
).
T
o
relate
our
denitions
to
the
traditional
denition
of
N
P
in
terms
of
a
non-deterministic
T
urning
mac
hine,
w
e
sho
w
that
the
denitions
ab
o
v
e
indeed
represen
t
the
same
complexit
y
class.
Prop
osition
..
N
P
(as
in
denition
.)
=
N
P
(as
in
the
tr
aditional
denition).
Pro
of:
First,
w
e
sho
w
that
if
a
language
L
is
in
N
P
according
to
the
traditional
denition,
then
it
is
also
in
N
P
according
to
denition
..
Consider
a
non-deterministic
T
uring
mac
hine
~
M
L
that
decides
on
L
after
at
most
p
L
(jxj)
steps,
where
p
L
is
some
p
olynomial
dep
ending
on
L,
and
x
is
the
input
to
~
M
L
.
The
idea
is
that
one
can

..
SEAR
CH
PR
OBLEMS

enco
de
the
non-deterministic
c
hoices
of
~
M
L
,
and
to
use
this
enco
ding
as
a
witness
for
mem
b
ership
in
L.
Namely
,
~
M
L
can
alw
a
ys
b
e
assumed
to
rst
mak
e
all
its
non-deterministic
c
hoices
(e.g.,
b
y
writing
them
on
a
separate
tap
e),
and
then
execute
deterministically
,
branc
hing
according
to
the
c
hoices
that
had
b
een
made
in
the
rst
step.
Th
us,
~
M
L
is
equiv
alen
t
to
a
deterministic
T
urning
mac
hine
M
L
accepting
as
input
the
pair
(x;
y
)
and
executing
exactly
as
~
M
L
on
x
with
a
pre-
determined
sequence
of
non-deterministic
c
hoices
enco
ded
b
y
y
.
An
input
x
is
accepted
b
y
~
M
L
if
and
only
if
there
exists
a
y
for
whic
h
(x;
y
)
is
accepted
b
y
M
L
.
The
relation
R
L
is
dened
to
b
e
the
set
of
all
pairs
(x;
y
)
accepted
b
y
M
L
.
Th
us,
x

L
if
and
only
if
there
exists
a
y
suc
h
that
(x;
y
)

R
L
,
namely
if
there
exists
an
accepting
computation
of
M
L
.
It
remains
to
see
that
R
L
is
indeed
p
olynomial-time
decidable
and
p
olynomially
b
ounded.
F
or
the
rst
part,
observ
e
that
R
L
can
b
e
decided
in
p
olynomial
time
simply
b
y
sim
ulating
the
T
uring
mac
hine
M
L
on
(x;
y
).
F
or
the
second
part,
observ
e
that
~
M
L
is
guaran
teed
to
terminate
in
p
olynomial
time,
i.e.,
after
at
most
p
L
(jxj)
steps,
and
therefore
the
n
um
b
er
of
non-deterministic
c
hoices
is
also
b
ounded
b
y
a
p
olynomial,
i.e.,
jy
j

p
L
(jxj).
Hence,
the
relation
R
L
is
p
olynomially
b
ounded.
F
or
the
con
v
erse,
examine
the
witness
relation
R
L
as
in
denition
..
Consider
the
p
olynomial-
time
deterministic
T
uring
mac
hine
M
L
that
decides
on
R
L
,
i.e.,
accepts
the
pair
(x;
y
)
if
and
only
if
(x;
y
)

R
L
.
Construct
a
non-deterministic
T
urning
mac
hine
~
M
L
that
giv
en
an
input
x,
guesses,
non-deterministically
,
a
witness
y
of
size
b
L
(jxj),
and
then
executes
M
L
on
(x;
y
).
If
x

L,
there
exists
a
p
olynomial-size
witness
y
for
whic
h
(x;
y
)

R
L
,
and
th
us
there
exists
a
p
olynomial-time
computation
of
~
M
L
that
accepts
x.
If
x

L,
then
for
ev
ery
p
olynomial-size
witness
y
,
(x;
y
)

R
L
and
therefore
~
M
L
alw
a
ys
rejects
x.
.
Searc
h
Problems
Whereas
the
denition
of
computational
p
o
w
er
in
terms
of
languages
ma
y
b
e
mathematically
con-
v
enien
t,
the
main
computational
goal
of
computer
science
is
to
solv
e
\problems".
W
e
abstract
a
computation
problem

b
y
a
se
ar
ch
pr
oblem
o
v
er
some
binary
relation
R

:
the
input
of
the
problem
at
hand
is
some
x
and
the
task
is
to
nd
a
y
suc
h
that
(x;
y
)

R

(w
e
ignore
the
case
where
no
suc
h
y
exists).
A
particularly
in
teresting
sub
class
of
these
relations
is
the
collection
of
p
olynomial
ly
veriable
relations
R
for
whic
h

R
is
p
olynomially
b
ounded.
Otherwise,
the
mere
writing
of
the
solution
cannot
b
e
carried
out
ecien
tly
.

R
is
p
olynomial-time
recognizable.
This
captures
the
in
tuitiv
e
notion
that
once
a
solution
to
the
problem
is
giv
en,
one
should
b
e
able
to
v
erify
its
correctness
ecien
tly
(i.e.,
in
p
olynomial
time).
The
lac
k
of
suc
h
an
abilit
y
implies
that
ev
en
if
a
solution
is
pro
vided
\b
y
magic",
one
cannot
ecien
tly
determine
its
v
alidness.
Giv
en
a
p
olynomially-v
eriable
relation
R
,
one
can
dene
the
corresp
onding
language
L(R
)
as
the
set
of
all
w
ords
x
for
whic
h
there
exists
a
solution
y
,
suc
h
that
(x;
y
)

R
,
i.e.,
L(R
)

=
fx
j
	y
(x;
y
)

R
g:
(.)
By
the
ab
o
v
e
denition,
N
P
is
exactly
the
collection
of
the
languages
L(R
)
that
corresp
ond
to
searc
h
problems
o
v
er
p
olynomially
v
eriable
relations,
i.e.,
N
P

=
fL(R
)
j
R
is
p
olynomially
v
eriable
g


LECTURE
.
THE
P
VS
NP
QUESTION
Th
us,
the
question
P
?
=
N
P
can
b
e
rephrased
as
the
question
whether
for
ev
ery
p
olynomially
v
eriable
relation
R
,
its
corresp
onding
language
L(R
)
can
b
e
decided
in
p
olynomial
time.
F
ollo
wing
is
an
example
of
a
computational
problem
and
its
form
ulation
as
a
searc
h
problem.
Pr
oblem:
-Coloring
Graphs
Input:
An
undirected
graph
G
=
(V
;
E
).
T
ask:
Find
a
-coloring
of
G,
namely
a
mapping
'
:
V
!
f;
;
g
suc
h
that
no
adjacen
t
v
ertices
ha
v
e
the
same
color,
i.e.,
for
ev
ery
(u;
v
)

E
,
'(u)
=
'(v
).
The
natural
relation
R
C
O
L
that
corresp
onds
to
-Coloring
is
dened
o
v
er
the
set
of
pairs
(G;
'),
suc
h
that
(G;
')

R
C
O
L
if

'
is
indeed
a
mapping
'
:
V
!
f;
;
g.

F
or
ev
ery
(u;
v
)

E
,
'(u)
=
'(v
).
Clearly
,
with
an
y
reasonable
represen
tation
of
',
its
size
is
p
olynomial
in
the
size
of
G.
F
urther,
it
is
easy
to
determine
in
p
olynomial
time
whether
a
pair
(G;
')
is
indeed
in
R
C
O
L
.
The
corresp
onding
language
L(R
C
O
L
)
is
the
set
of
all
-colorable
graphs,
i.e.,
all
graphs
G
that
ha
v
e
a
legal
-coloring.
Jumping
ahead,
it
is
N
P
-hard
to
determine
whether
suc
h
a
coloring
exists,
and
hence,
unless
P
=
N
P
,
no
ecien
t
algorithm
for
this
problem
exists.
.
Self
Reducibilit
y
Searc
h
problems
as
dened
ab
o
v
e
are
\harder"
than
the
corresp
onding
decision
problem
in
the
sense
that
if
the
former
can
b
e
carried
out
ecien
tly
,
so
can
the
latter.
Giv
en
a
p
olynomial-time
searc
h
algorithm
A
for
a
p
olynomially-v
eriable
relation
R
,
one
can
construct
a
p
olynomial-time
decision
algorithm
for
L(R
)
b
y
sim
ulating
A
for
p
olynomially
man
y
steps,
and
answ
ering
\y
es"
if
and
only
if
A
has
terminated
and
pro
duced
a
prop
er
y
for
whic
h
(x;
y
)

R
.
Since
m
uc
h
of
the
researc
h
in
complexit
y
theory
ev
olv
es
around
decision
problems,
a
fundamen-
tal
question
that
naturally
arises
is
whether
an
ecien
t
pro
cedure
for
solving
the
de
cision
problem
guaran
tees
an
ecien
t
pro
cedure
for
solving
the
se
ar
ch
problem.
As
will
b
e
seen
b
elo
w,
this
is
not
kno
wn
to
b
e
true
in
general,
but
can
b
e
sho
wn
to
b
e
true
for
an
y
N
P
-complete
problem.
W
e
b
egin
with
a
denition
that
captures
this
notion:
Denition
.
A
r
elation
R
is
self-reducible
if
solving
the
se
ar
ch
pr
oblem
for
R
is
Co
ok-r
e
ducible
to
de
ciding
the
c
orr
esp
onding
language
L(R
)

=
fx
j
	y
(x;
y
)

R
g.
Recall
that
a
Co
ok
reduction
from
a
problem


to


allo
ws
a
T
uring
mac
hine
for


to
use


as
an
oracle
(p
olynomially
man
y
times).
Th
us,
if
a
relation
R
is
self-reducible,
then
there
exists
a
p
olynomial-time
T
uring
mac
hine
that
solv
es
the
searc
h
problem
(i.e.,
for
eac
h
input
x
nds
a
y
suc
h
that
(x;
y
)

R
),
except
that
the
T
urning
mac
hine
is
allo
w
ed
to
access
an
oracle
that
decides
L(R
),
i.e.,
for
eac
h
input
x
0
outputs
whether
there
exists
a
y
0
suc
h
that
(x
0
;
y
0
)

R
.
F
or
example,
in
the
case
of
-colorabilit
y
,
the
searc
h
algorithm
is
required
to
nd
a
-coloring
for
an
input
graph
G,
giv
en
as
an
oracle
a
pro
cedure
that
tells
whether
a
giv
en
graph
G
0
is
-colorable.
The
searc
h
algorithm
is
not
limited
to
ask
the
oracle
only
ab
out
G,
but
rather
ma
y
query
the
oracle
on
a
(p
olynomially
long)
sequence
of
graphs
G
0
,
where
the
sequence
itself
ma
y
dep
end
up
on
answ
ers
to
previous
in
v
o
cations
of
the
oracle.
W
e
consider
the
example
of
SA
T.

..
SELF
REDUCIBILITY

Pr
oblem:
SA
T
Input:
A
CNF
form
ula
'
o
v
er
fx

;
:
:
:
;
x
n
g.
T
ask:
Find
a
satisfying
assignmen
t

,
i.e.,
a
mapping

:
f;
:
:
:
;
ng
!
fT
;
F
g,
suc
h
that
'(
();
:
:
:
;

(n))
is
true.
The
relation
R
S
AT
corresp
onding
to
SA
T
is
the
set
of
all
pairs
(';

)
suc
h
that

is
a
satisfying
assignmen
t
for
'.
It
can
b
e
easily
v
eried
that
the
length
of

is
indeed
p
olynomial
in
n
and
that
the
relation
can
b
e
recognized
in
p
olynomial
time.
Prop
osition
..
R
S
AT
is
self-r
e
ducible.
Pro
of:
W
e
sho
w
that
R
S
AT
is
self-reducible
b
y
sho
wing
an
algorithm
that
solv
es
the
searc
h
problem
o
v
er
R
S
AT
using
an
oracle
A
for
deciding
S
AT

=
L(R
S
AT
).
The
algorithm
incremen
tally
constructs
a
solution
b
y
building
partial
assignmen
ts.
A
t
eac
h
step,
the
in
v
arian
t
guaran
tees
that
the
partial
assignmen
t
can
b
e
completed
in
to
a
full
satisfying
assignmen
t,
and
hence
when
the
algorithm
terminates,
the
assignmen
t
satises
'.
The
algorithm
pro
ceeds
as
follo
ws.

Query
whether
'

SA
T.
If
the
answ
er
is
\no",
the
input
form
ula
'
has
no
satisfying
assignmen
t.

F
or
i
ranging
from

to
n,
let
'
i
(x
i+
;
:
:
:
;
x
n
)

=
'(

;
:
:
:
;

i 
;
;
x
i+
;
:
:
:
;
x
n
).
Using
the
oracle,
test
whether
'
i

S
AT
.
If
the
answ
er
is
\y
es",
assign

i
 
.
Otherwise,
assign

i
 
0.
Clearly
,
the
partial
assignmen
t

()
=


;
:
:
:
;

(i)
=

i
can
still
b
e
completed
in
to
a
satisfying
assignmen
t,
and
hence
the
algorithm
terminates
with
a
true
assignmen
t.
Consequen
tly
,
one
ma
y
deduce
that
if
SA
T
is
decidable
in
p
olynomial
time,
then
there
exists
an
ecien
t
algorithm
that
solv
es
the
searc
h
problem
for
R
S
AT
.
On
the
other
hand,
if
SA
T
is
not
decidable
in
p
olynomial
time
(whic
h
is
the
more
lik
ely
case),
there
is
no
ecien
t
algorithm
for
solving
the
searc
h
problem.
Therefore,
researc
h
on
the
complexit
y
of
deciding
SA
T
relates
directly
to
the
complexit
y
of
searc
hing
R
S
AT
.
In
the
next
lecture
w
e
sho
w
that
ev
ery
N
P
-complete
language
has
a
self-reducible
relation.
Ho
w
ev
er,
let
us
rst
discuss
the
problem
of
graph
isomorphism,
whic
h
can
b
e
easily
sho
wn
to
b
e
in
N
P
,
but
is
not
kno
wn
to
b
e
N
P
-hard.
W
e
sho
w
that
nev
ertheless,
graph
isomorphism
has
a
self-reducible
relation.
Pr
oblem:
Graph
Isomorphism
Input:
Tw
o
simple

graphs
G

=
(V
;
E

),
G

=
(V
;
E

).
W
e
ma
y
assume,
without
loss
of
generalit
y
,
that
none
of
the
input
graphs
has
an
y
isolated
v
ertices
T
ask:
Find
an
isomorphism
b
et
w
een
the
graphs,
i.e.
a
p
erm
utation
'
:
V
!
V
,
suc
h
that
(u;
v
)

E

if
and
only
if
('(u);
'(v
))

E

.
The
relation
R
GI
corresp
onding
to
the
graph
isomorphism
problem
is
the
set
of
all
pairs
((G

;
G

);
')
for
whic
h
'
is
an
isomorphism
b
et
w
een
G

and
G

.
Prop
osition
..
R
GI
is
self-r
e
ducible.
Pro
of:
T
o
see
that
graph
isomoprphism
is
self-reducible,
consider
an
algorithm
that
uses
a
graph-
isomorphism
mem
b
ership
oracle
along
the
lines
of
the
algorithm
for
SA
T.
Again,
the
algorithm
xes
the
mapping
'()
v
ertex
b
y
v
ertex.

Suc
h
graphs
ha
v
e
no
self-lo
ops
and
no
parallel
edges,
and
so
eac
h
v
ertex
has
degree
at
most
jV
j
 .


LECTURE
.
THE
P
VS
NP
QUESTION
A
t
eac
h
step,
the
algorithm
xes
a
single
v
ertex
u
in
G

,
and
nds
a
v
ertex
v
suc
h
that
the
mapping
'(u)
=
v
can
b
e
completed
in
to
a
graph
isomorphism.
T
o
nd
suc
h
a
v
ertex
v
,
the
algorithm
tries
all
candidate
mappings
'(u)
=
v
for
all
unmapp
ed
v

V
,
using
the
oracle
to
tell
whether
the
mapping
can
still
b
e
completed
in
to
a
complete
isomorphism.
If
there
exists
an
isomorphism
to
b
egin
with,
suc
h
a
mapping
m
ust
exist,
and
hence
the
algorithm
terminates
with
a
complete
isomorphism.
W
e
no
w
sho
w
ho
w
a
partial
assignmen
t
can
b
e
decided
b
y
the
oracle.
The
tric
k
here
is
that
in
order
to
c
hec
k
if
u
can
b
e
mapp
ed
to
v
,
one
can
\mark"
b
oth
v
ertices
b
y
a
unique
pattern,
sa
y
b
y
ro
oting
a
star
of
jV
j
lea
v
es
at
b
oth
u
and
v
,
resulting
in
new
graphs
G
0

,
G
0

.
Next,
query
the
oracle
whether
there
is
an
isomorphism
'
b
et
w
een
G
0

and
G
0

.
Since
the
degrees
of
u
and
v
are
strictly
larger
than
the
degrees
of
other
v
ertices
in
G
0

and
G
0

,
an
isomorphism
'
0
b
et
w
een
G
0

and
G
0

w
ould
exist
if
and
only
if
there
exists
an
isomorphism
'
b
et
w
een
G

and
G

that
maps
u
to
v
.
After
the
mapping
of
u
is
determined,
pro
ceed
b
y
incremen
tally
marking
v
ertices
in
V
with
stars
of
jV
j
lea
v
es,
jV
j
lea
v
es,
and
so
on,
un
til
the
complete
mapping
is
determined.
A
p
oin
t
w
orth
men
tioning
is
that
the
denition
of
self-reducibilit
y
applies
to
relations
and
not
to
languages.
A
particular
language
L

N
P
ma
y
b
e
asso
ciated
with
more
than
one
searc
h
problem,
and
the
self-reducibilit
y
of
a
giv
en
relation
R
(or
the
lac
k
thereof
)
do
es
not
immediately
imply
self-reducibilit
y
(or
the
lac
k
thereof
)
for
a
dieren
t
relation
R
0
asso
ciated
with
the
same
language
L.
It
is
b
eliev
ed
that
not
ev
ery
language
in
N
P
admits
a
self-reducible
relation.
Belo
w
w
e
presen
t
an
example
of
a
language
in
N
P
for
whic
h
the
\natural"
searc
h
problem
is
b
eliev
ed
not
to
b
e
self-reducible.
Consider
the
language
of
comp
osite
n
um
b
ers,
i.e.,
L
C
O
M
P

=
fN
j
N
=
n


n

n

;
n

>
g:
The
language
L
C
O
M
P
is
kno
wn
to
b
e
decidable
in
p
olynomial
time
b
y
a
randomized
algorithm.
A
natural
relation
R
C
O
M
P
corresp
onding
to
L
C
O
M
P
is
the
set
of
all
pairs
(N
;
(n

;
n

))
suc
h
that
N
=
n


n

,
where
n

;
n

>
.
Clearly
,
the
length
of
(n

;
n

)
is
p
olynomial
in
the
length
of
N
,
and
since
R
C
O
M
P
can
easily
b
e
decided
in
p
olynomial
time,
L
C
O
M
P
is
in
N
P
.
Ho
w
ev
er,
the
searc
h
problem
o
v
er
R
C
O
M
P
requires
nding
a
pair
(n

;
n

)
for
whic
h
N
=
n


n

.
This
problem
is
computationally
equiv
alen
t
to
factoring,
whic
h
is
b
eliev
ed
not
to
admit
an
y
(probabilistic)
p
olynomial-time
algorithm.
Th
us,
it
is
v
ery
unlik
ely
that
R
C
O
M
P
is
(random)
self-
reducible.
Another
language
whose
natural
relation
is
b
eliev
ed
not
to
b
e
self-reducible
is
L
QR
,
the
set
of
all
quadratic
residues.
The
language
L
QR
con
tains
all
pairs
(N
;
x)
in
whic
h
x
is
a
quadratic
residue
mo
dulo
N
,
namely
,
there
exists
a
y
for
whic
h
y


x
(mo
d
N
).
The
natural
searc
h
problem
asso
ciated
with
L
QR
is
R
QR
,
the
set
of
all
pairs
((N
;
x);
y
)
suc
h
that
y


x
(mo
d
N
).
It
is
w
ell-
kno
wn
that
the
searc
h
problem
o
v
er
R
QR
is
equiv
alen
t
to
factoring
under
randomized
reductions.
Th
us,
under
the
assumption
that
factoring
is
\harder"
than
deciding
L
QR
,
the
natural
relation
R
QR
is
not
(random)
self-reducible.
Bibliographic
Notes
F
or
a
historical
accoun
t
of
the
\P
vs
NP
Question"
see
[].
The
discussion
regarding
Quadratic
Residiousit
y
is
tak
en
from
[].
This
pap
er
con
tains
also
further
evidence
to
the
existence
of
NP-relations
whic
h
are
not
self-reducible.

..
SELF
REDUCIBILITY

.
M.
Bellare
and
S.
Goldw
asser,
\The
Complexit
y
of
Decision
v
ersus
Searc
h",
SIAM
Journal
on
Computing,
V
ol.
,
pages
	{	,
		.
.
Sipser,
M.,
\The
history
and
status
of
the
P
v
ersus
NP
problem",
Pr
o
c.
th
A
CM
Symp.
on
The
ory
of
Computing,
pages
0{,
		.


LECTURE
.
THE
P
VS
NP
QUESTION

Lecture

NP-completeness
and
Self
Reducibilit
y
Notes
tak
en
b
y
Nir
Piterman
and
Dana
Fisman
Summary:
It
will
b
e
pro
v
en
that
the
relation
R
of
an
y
N
P
 complete
language
is
Self-reducible.
This
will
b
e
done
using
the
S
AT
self
reducibilit
y
pro
v
ed
previously
and
the
fact
that
S
AT
is
N
P
 hard
(under
Levin
reduction).
Prior
to
that,
a
simpler
pro
of
of
the
existence
of
N
P
 complete
languages
will
b
e
giv
en.
.
Reductions
The
notions
of
self
reducibilit
y
and
N
P
 completeness
require
a
denition
of
the
term
reduction.
The
idea
b
ehind
reducing
problem


to
problem


,
is
that
if


is
kno
wn
to
b
e
easy
,
so
is


or
vice
v
ersa,
if


is
kno
wn
to
b
e
hard
so
is


Denition
.
(Co
ok
R
e
duction):
A
Co
ok
reduction
fr
om
pr
oblem


to
pr
oblem


is
a
p
olynomial
or
acle
machine
that
solves
pr
oblem


on
input
x
while
getting
or
acle
answers
for
pr
oblem


.
F
or
example:
Let


and


b
e
decision
problems
of
languages
L

and
L

resp
ectiv
ely
and

L
the
c
haracteristic
function
of
L
dened
to
b
e

L
(x)
=
(

x

L
0
x
=

L
Then


will
b
e
Co
ok
reducible
to


if
exists
an
oracle
mac
hine
that
on
input
x
asks
query
q
,
gets
answ
er

L

(q
)
and
giv
es
as
output

L

(x)
(Ma
y
ask
m
ultiple
queries).
Denition
.
(Karp
R
e
duction):
A
Ka
rp
reduction
(many
to
one
reduction)
of
language
L

to
language
L

is
a
p
olynomial
time
c
omputable
function
f
:


!


such
that
x

L

if
and
only
if
f
(x)

L

.
Claim
..
A
Karp
r
e
duction
is
a
sp
e
cial
c
ase
of
a
Co
ok
r
e
duction.
Pro
of:
Giv
en
a
Karp
reduction
f
()
from
L

to
L

and
an
input
x
to
b
e
decided
whether
x
b
elongs
to
L

,
dene
the
follo
wing
oracle
mac
hine:
.
On
input
x
compute
the
v
alue
f
(x).
	

0
LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
.
Presen
t
f
(x)
to
the
oracle
of
L

.
.
The
oracle's
answ
er
is
the
desired
decision.
The
mac
hine
runs
p
olynomial
time
since
Step

is
p
olynomial
as
promised
b
y
Karp
reduction
and
b
oth
Steps

and

require
constan
t
time.
Ob
viously
M
accepts
x
if
and
only
if
x
is
in
L

.
Hence
a
Karp
reduction
can
b
e
view
ed
as
a
Co
ok
reduction.
Denition
.
(L
evin
R
e
duction):
A
Levin
reduction
fr
om
r
elation
R

to
r
elation
R

is
a
triplet
of
p
olynomial
time
c
omputable
functions
f
;
g
and
h
such
that:
.
x

L(R

)
(
)
f
(x)

L(R

)
.
(x;
y
)

R

;
(f
(x);
g
(x;
y
))

R

.
x;
z
(f
(x);
z
)

R

=
)
(x;
h(x;
z
))

R

Note:
A
Levin
reduction
from
R

to
R

implies
a
Karp
reduction
of
the
decision
problem
(using
condition
)
and
a
Co
ok
reduction
of
the
searc
h
problem
(using
conditions

and
).
Claim
..
Karp
r
e
duction
is
tr
ansitive.
Pro
of:
Let
f

:


 !


b
e
a
Karp
reduction
from
L
a
to
L
b
and
f

:


 !


b
e
a
Karp
reduction
from
L
b
to
L
c
The
function
f


f

()
is
a
Karp
reduction
from
L
a
to
L
c
:

x

L
a
(
)
f

(x)

L
b
(
)
f

(f

(x))

L
c
.

f

and
f

are
p
olynomial
time
computable,
so
the
comp
osition
of
the
functions
is
again
p
olynomial
time
computable.
Claim
..
L
evin
r
e
duction
is
tr
ansitive.
Pro
of:
Let
(f

;
g

;
h

)
b
e
a
Levin
reduction
from
R
a
to
R
b
and
(f

;
g

;
h

)
b
e
a
Levin
reduction
from
R
b
to
R
c
.
Dene:

f

(x)

=
f

(f

(x))

g

(x;
y
)

=
g

(f

(x);
g

(x;
y
))

h

(x;
y
)

=
h

(x;
h

(f

(x);
y
))
W
e
sho
w
that
the
triplet
(f

;
g

;
h

)
is
a
Levin
reduction
from
R
a
to
R
c
:

x

L(R
a
)
(
)
f

(x)

L(R
c
)
Since:
x

L(R
a
)
(
)
f

(x)

L(R
b
)
(
)
f

(f

(x))

L(R
c
)
(
)
f

(x)

L(R
c
)

..
ALL
N
P
-COMPLETE
RELA
TIONS
ARE
SELF-REDUCIBLE


(x;
y
)

R
a
;
(f

(x);
g

(x;
y
))

R
c
Since:
(x;
y
)

R
a
=
)
(f

(x);
g

(x;
y
))

R
b
=
)
(f

(f

(x));
g

(f

(x);
g

(x;
y
)))

R
c
=
)
(f

(x);
g

(x;
y
))

R
c

x;
z
(f

(x);
z
)

R
c
=
)
(x;
h

(x;
z
))

R
a
Since:
(f

(x);
z
)

R
c
=
)
(f

(f

(x));
z
)

R
c
=
)
(f

(x);
h

(f

(x);
z
))

R
b
=
)
(x;
h

(x;
h

(f

(x);
z
)))

R
a
=
)
(x;
h

(x;
z
))

R
a
Theorem
.
If


Co
ok
r
e
duc
es
to


and



P
then



P
.
Here
class
P
denotes
not
only
languages
but
also
an
y
problem
that
can
b
e
solv
ed
in
p
olynomial
time.
Pro
of:
W
e
shall
build
a
deterministic
p
olynomial
time
T
uring
mac
hine
that
recognizes


:
As


Co
ok
reduces
to


,
there
exists
a
p
olynomial
oracle
mac
hine
M

that
recognizes


while
asking
queries
to
an
oracle
of


.
As



P
,
there
exists
a
deterministic
p
olynomial
time
T
uring
mac
hine
M

that
recognizes


.
No
w
build
a
mac
hine
M
,
recognizer
for


that
w
orks
as
follo
wing:

On
input
x,
em
ulate
M

un
til
it
p
oses
a
query
to
the
oracle.

Presen
t
the
query
to
the
mac
hine
M

and
return
the
answ
er
to
M

.

Pro
ceed
un
til
no
more
queries
are
presen
ted
to
the
oracle.

The
output
of
M

is
the
required
answ
er.
Since
the
oracle
and
M

giv
e
the
same
answ
ers
to
the
queries,
correctness
is
ob
vious.
Considering
the
fact
that
M

is
p
olynomial,
the
n
um
b
er
of
queries
and
the
length
of
eac
h
query
are
p
olynomial
in
jxj.
Hence
the
dela
y
caused
b
y
in
tro
ducing
the
mac
hine
M

is
p
olynomial
in
jxj.
Therefore
the
total
run
time
of
M
is
p
olynomial.
.
All
N
P
-complete
relations
are
Self-reducible
Denition
.
(N
P
 complete
language):
A
language
L
is
N
P
-complete
if:
.
L

N
P
.
F
or
every
language
L
0
in
N
P
;
L
0
Karp
r
e
duc
es
to
L.
These
languages
are
the
hardest
problems
in
N
P
,
in
the
sense
that
if
w
e
knew
ho
w
to
solv
e
an
N
P
 complete
problem
ecien
tly
w
e
could
ha
v
e
ecien
tly
solv
ed
an
y
problem
in
N
P
.
N
P
 complete
ness
can
b
e
dened
in
a
broader
sense
b
y
Co
ok
reductions.
There
are
not
man
y
kno
wn
N
P
 complete
problems
b
y
Co
ok
reductions
that
are
not
N
P
 complete
b
y
Karp
reductions.


LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
Denition
.
.
R
is
a
N
P
relation
if
L(R
)

N
P
.
A
r
elation
R
is
N
P
-ha
rd
under
Levin
reduction
if
any
N
P
r
elation
R
0
is
L
evin
r
e
ducible
to
R
.
Theorem
.
F
or
every
N
P
r
elation
R
,
if
L(R
)
is
N
P
 complete
then
R
is
Self-r
e
ducible.
Pro
of:
T
o
pro
v
e
the
theorem
w
e
shall
use
t
w
o
facts:
.
S
AT
is
Self-reducible
(w
as
pro
v
ed
last
lecture).
.
R
S
AT
is
N
P
 hard
under
Levin
reduction
(will
b
e
pro
v
en
later).
Giv
en
an
N
P
relation
R
of
an
N
P
 complete
language,
a
Levin
reduction
(f
;
g
;
h)
from
R
to
R
S
AT
,
a
Karp
reduction
k
from
S
AT
to
L(R
)
and
x,
the
follo
wing
algorithm
will
nd
y
suc
h
that
(x;
y
)

R
(pro
vided
that
x

L(R
)).
The
idea
b
ehind
the
pro
of
is
v
ery
similar
to
the
self
reducibilit
y
of
R
S
AT
:
.
Ask
L(R
)'s
oracle
whether
x

L(R
).
.
On
answ
er
0
no
0
declare:
x
=

L(R
)
and
ab
ort.
.
On
answ
er
0
y
es
0
use
the
function
f
,
that
preserv
es
the
prop
ert
y
of
b
elonging
to
the
language,
to
translate
the
input
x
for
L(R
)
in
to
a
satisable
C
N
F
form
ula
'
=
f
(x).
.
Compute
(

;
:::;

n
)
a
satisfying
assignmen
t
for
'
as
follo
ws:
(a)
Giv
en
a
partial
assignmen
t


;
:::;

i
suc
h
that
'
i
(x
i+
;
:::;
x
n
)
=
'(

;
:::;

i
;
x
i+
;
x
i+
;
:::;
x
n
)

S
AT
,
where
x
i+
;
:::;
x
n
are
v
ariables
and


;
:::;

i
are
constan
ts.
(b)
Assign
x
i+
=

and
compute
'
i
(;
x
i+
;
:::;
x
n
)
=
'(

;
:::;

i
;
;
x
i+
;
:::;
x
n
)
(c)
Use
the
function
k
to
translate
the
C
N
F
form
ula
'
i
(;
x
i+
;
:::;
x
n
)
in
to
an
input
to
the
language
L(R
).
Ask
L(R
)'s
oracle
wheather
k
('
i
(;
x
i+
;
:::;
x
n
))

L(R
).
(d)
On
answ
er
0
y
es
0
assign

i+
=
,
otherwise
assign

i+
=
0.
(e)
Iterate
un
til
i
=
n
 .
.
Use
the
function
h
that
translates
a
pair
x
and
a
satisfying
assignmen
t


;
:::;

n
to
'
=
f
(x)
in
to
a
witness
y
=
h(x;
(

;
:::;

n
))
suc
h
that
(x;
y
)

R
.
Clearly
(x;
y
)

R
.
Note:
The
ab
o
v
e
argumen
t
uses
a
Karp
reduction
of
S
AT
to
L(R
)
(guaran
teed
b
y
the
NP-
completeness
of
the
latter).
One
ma
y
extend
the
argumen
t
to
hold
also
for
the
case
one
is
only
giv
en
a
Co
ok
reduction
of
S
AT
to
L(R
).
Sp
ecically
in
stage
(c)
instead
of
getting
the
answ
er
to
whether
'
i
(;
x
i+
;
:::;
x
n
)
is
in
S
AT
b
y
quering
on
whether
k
('
i
)
is
in
L(R
),
w
e
can
get
the
answ
er
b
y
running
the
oracle
mac
hine
giv
en
in
the
Co
ok
reduction
(whic
h
mak
es
queries
to
L(R
)).

..
B
O
U
N
D
E
D
H
ALT
I
N
G
IS
N
P
 COMPLETE

.
B
ounded
H
al
ting
is
N
P
 complete
In
order
to
sho
w
that
indeed
exist
problems
in
N
P
 complete
(i.e.
the
class
N
P
 complete
is
not
empt
y)
the
language
B
H
will
b
e
in
tro
duced
and
pro
v
ed
to
b
e
N
P
 complete.
Denition
.
(Bounded
Halting):
.
B
H

=
(
(hM
i;
x;

t
)





hM
i
is
the
description
of
a
non-deterministic
machine
that
ac
c
epts
input
x
within
t
steps.
)
.
B
H

=
(
(hM
i;
x;

t
)





hM
i
is
the
description
of
a
deterministic
machine
and
exists
y
whose
length
is
p
olynomial
in
jxj
such
that
M
ac
c
epts
(x;
y
)
within
t
steps.
)
The
t
w
o
denitions
are
equiv
alen
t
if
w
e
consider
the
y
w
an
ted
in
the
second
as
the
sequence
of
non
deterministic
c
hoices
of
the
rst.
The
computation
is
b
ounded
b
y
t
hence
so
is
y
's
length.
Denition
.	
R
B
H

=
(
((hM
i;
x;

t
);
y
)





hM
i
is
the
description
of
a
deterministic
machine
that
ac
c
epts
input
(x;
y
)
within
t
steps.
)
Once
again
the
length
of
the
witness
y
is
b
ounded
b
y
t,
hence
it
is
p
olynomial
in
the
length
of
the
input
(hM
i;
x;

t
).
Directly
from
N
P
's
denition:
B
H

N
P
.
Claim
..
A
ny
language
L
in
N
P
,
Karp
r
e
duc
es
to
B
H
Pro
of:
Giv
en
a
language
L
in
N
P
,
implies
the
follo
wing:

A
witness
relation
R
L
exists
and
has
a
p
olynomial
b
ound
b
L
()
suc
h
that:

(x;
y
)

R
L
;
jy
j

b
L
(jxj)

A
recognizer
mac
hine
M
L
for
R
L
exists
and
its
time
is
b
ounded
b
y
another
p
olynomial
p
L
().
The
reduction
maps
x
to
f
(x)

=
(hM
L
i;
x;

p
L
(jxj+b
L
(jxj))
),
whic
h
is
an
instance
to
B
H
b
y
V
ersion

of
Denition
.
ab
o
v
e.
Notice
that
the
reduction
is
indeed
p
olynomial
since
hM
L
i
is
a
constan
t
string
for
the
reduction
from
L
to
B
H
.
All
the
reduction
do
es
is
prin
t
this
constan
t
string,
concatenate
the
input
x
to
it
and
then
concatenate
a
p
olynomial
n
um
b
er
of
ones.
W
e
will
sho
w
no
w
that
x

L
if
and
only
if
f
(x)

B
H
:
x

L
(
)
Exists
a
witness
y
whose
length
is
b
ounded
b
y
b
L
(jxj)
suc
h
that
(x;
y
)

R
L
(
)
Exists
a
computation
of
M
L
with
t

=
P
L
(jxj
+
b
L
(jxj))
steps
accepting
(x;
y
)
(
)
(hM
i;
x;

t
)

B
H
Note:
The
reduction
can
b
e
easily
transformed
in
to
Levin
reduction
of
R
L
to
R
B
H
with
the
iden
tit
y
function
supplying
the
t
w
o
missing
functions.
Th
us
B
H

N
P
 complete.
Corollary
.0
Ther
e
exist
N
P
 complete
sets.


LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
.
C
ir
cuit
S
atisf
iabil
ity
is
N
P
 complete
Denition
.
(Circuit
Satisabilit
y):
.
A
Circuit
is
a
dir
e
cte
d
a-cyclic
gr
aph
G
=
(V
;
E
)
with
vertic
es
lab
ele
d:
output;
V
;
W
;
:;
x

;
:::;
x
m
;
0;

With
the
fol
lowing
r
estrictions:

a
vertex
lab
ele
d
by
:
has
in-de
gr
e
e
.

a
vertex
lab
ele
d
by
x
i
has
in-de
gr
e
e
0
(i.e.
is
a
sour
c
e).

a
vertex
lab
ele
d
by
0
(or
)
has
in-de
gr
e
e
0.

ther
e
is
a
single
sink
(vertex
of
out-de
gr
e
e
0),
it
has
in-de
gr
e
e

and
is
lab
ele
d
'output'.

in-de
gr
e
e
of
vertic
es
lab
ele
d
V
;
W
c
an
b
e
r
estricte
d
to
.
Given
an
assignment


f0;
g
m
to
the
variables
x

;
:::;
x
m
,
C
(
)
wil
l
denote
the
value
of
the
cir
cuit's
output.
The
value
is
dene
d
in
the
natur
al
manner,
by
setting
the
value
of
e
ach
vertex
ac
c
or
ding
to
the
b
o
ole
an
op
er
ation
it
is
lab
ele
d
with.
F
or
example,
if
a
vertex
is
lab
el
le
d
V
and
the
vertic
es
with
a
dir
e
cte
d
e
dge
to
it
have
values
a
and
b,
then
the
vertex
has
valu
a
V
b.
.
Circuit
Satisabilit
y
C
S

=
fC
:
C
is
a
cir
cuit
and
exists

,
an
input
to
cir
cuit
C
such
that
C
(
)
=
g
.
R
C
S

=
f(C
;

)
:
C
(
)
=
g
The
relation
dened
ab
o
v
e
is
indeed
an
N
P
relation
since:
.

con
tains
assignmen
t
for
all
v
ariables
x

;
x

;
:::;
x
m
app
earing
in
C
and
hence
its
length
is
p
olynomial
in
jC
j.
.
Giv
en
a
couple
(C
;

)
ev
aluating
one
gate
tak
es
O
()
(since
in-degree
is
restricted
to
)
and
in
view
that
the
n
um
b
er
of
gates
is
at
most
jC
j,
total
ev
aluation
time
is
p
olynomial
in
jC
j.
Hence
C
S

N
P
.
Claim
..
C
ir
cuit
S
atisf
iabil
ity
is
N
P
 complete
Pro
of:
As
men
tioned
previously
C
S

N
P
.
W
e
will
sho
w
a
Karp
reduction
from
B
H
to
C
S
,
and
since
Karp
reductions
are
transitiv
e
and
B
H
is
N
P
 complete
,
the
pro
of
will
b
e
completed.
In
this
reduction
w
e
shall
use
the
second
denition
of
B
H
as
giv
en
in
Denition
..
Th
us
w
e
are
giv
en
a
triplet
(hM
i;
x;

t
).
This
triplet
is
in
B
H
if
exists
a
y
suc
h
that
the
determin-
istic
mac
hine
M
accepts
(x;
y
)
within
t
steps.
The
reduction
maps
suc
h
a
triplet
in
to
an
instance
of
C
S
.
The
idea
is
building
a
circuit
that
will
sim
ulate
the
run
of
M
on
(x;
y
),
for
the
giv
en
x
and
a
generic
y
(whic
h
will
b
e
giv
en
as
an
input
to
the
circuit).
If
M
do
es
not
accept
(x;
y
)
within
the
rst
t
steps
of
the
run,
w
e
are
ensured
that
(hM
i;
x;

t
)
is
not
in
B
H
.
Hence
it
suces
to
sim
ulate
only
the
rst
t
steps
of
the
run.
Eac
h
one
of
these
rst
t
congurations
is
completely
describ
ed
b
y
the
letters
written
in
the
rst
t
tap
e
cells,
the
head's
lo
cation
and
the
mac
hine's
state.

..
C
I
RC
U
I
T
S
AT
I
S
F
I
AB
I
LI
T
Y
IS
N
P
 COMPLETE

Hence
the
whole
computation
can
b
e
enco
ded
in
a
matrix
of
size
t

t.
The
en
try
(i;
j
)
of
the
matrix
will
consist
of
the
con
ten
ts
of
cell
j
at
time
i,
an
indicator
whether
the
head
is
on
this
cell
at
time
i
and
in
case
the
head
is
indeed
there
the
state
of
the
mac
hine
is
also
enco
ded.
So
ev
ery
matrix
en
try
will
hold
the
follo
wing
information:

a
i;j
the
letter
written
in
the
cell

h
i;j
an
indicator
to
head's
presence
in
the
cell

q
i;j
the
mac
hine's
state
in
case
the
head
indicator
is

(0
otherwise)
The
con
ten
ts
of
matrix
en
try
(i;
j
)
is
determined
only
b
y
the
three
matrix
en
tries
(i
 ;
j
 );
(i
 ;
j
)
and
(i
 ;
j
+
).
If
the
head
indicator
of
these
three
en
tries
is
o,
en
try
(i;
j
)
will
b
e
equal
to
en
try
(i
 ;
j
).
The
follo
wing
constructs
a
circuit
that
implemen
ts
the
idea
of
the
matrix
and
this
w
a
y
em
ulates
the
run
of
mac
hine
M
on
input
x.
The
circuit
consists
of
t
lev
els
of
t
triplets
(a
i;j
;
h
i;j
;
q
i;j
)
where
0

i

t;


j

t.
Lev
el
i
of
gates
will
enco
de
the
conguration
of
the
mac
hine
at
time
i.
The
wiring
will
mak
e
sure
that
if
lev
el
i
represen
ts
the
correct
conguration,
so
will
lev
el
i
+
.
The
(i;
j
)-th
triplet,
(a
i;j
;
h
i;j
;
q
i;j
),
in
the
circuit
is
a
function
of
the
three
triplets
(i
 ;
j
 );
(i
 ;
j
)
and
(i
 ;
j
+
).
Ev
ery
triplet
consists
of
O
(l
og
n)
bits,
where
n

=
j(hM
i;
x;

t
)j:

Let
G
denote
the
size
of
M
's
alphab
et.
Represen
ting
one
letter
requires
l
og
G
man
y
bits:
l
og
G
=
O
(l
og
n)
bits.

The
head
indicator
requires
one
bit.

Let
K
denote
the
n
um
b
er
of
states
of
M
.
Represen
ting
a
state
requires
l
og
K
man
y
bits:
l
og
K
=
O
(l
og
n)
bits.
Note
that
the
mac
hine's
description
is
giv
en
as
input.
Hence
the
n
um
b
er
of
states
and
the
size
of
the
alphab
et
are
smaller
than
input's
size
and
can
b
e
represen
ted
in
binary
b
y
O
(l
og
n)
man
y
bits
(Alb
eit
doing
the
reduction
directly
from
an
y
N
P
language
to
C
S
,
the
mac
hine
M
L
that
accepts
the
language
L
w
ouldn't
ha
v
e
b
een
giv
en
as
a
parameter
but
rather
as
a
constan
t,
hence
a
state
or
an
alphab
et
letter
w
ould
ha
v
e
required
a
constan
t
n
um
b
er
of
bits).
Ev
ery
bit
in
the
description
of
a
triplet
is
a
b
o
olean
function
of
the
bits
in
the
description
of
three
other
triplets,
hence
it
is
a
b
o
olean
function
of
O
(l
og
n)
bits.
Claim
..
A
ny
b
o
ole
an
function
on
m
variables
c
an
b
e
c
ompute
d
by
a
cir
cuit
of
size
m
m
Pro
of:
Ev
ery
b
o
olean
function
on
m
v
ariables
can
b
e
represen
ted
b
y
a
(m
+
)


m
matrix.
The
rst
m
columns
will
denote
a
certain
input
and
the
last
column
will
denote
the
v
alue
of
the
function.
The

m
ro
ws
are
required
to
describ
e
all
dieren
t
inputs.
No
w
the
circuit
that
will
calculate
the
function
is:
F
or
line
l
in
the
matrix
in
whic
h
the
function
v
alue
is

(f
(l
)
=
),
build
the
follo
wing
circuit:
C
l
=
(
V
input
y
i
=
y
i
)
V
(
V
input
y
i
=0
:y
i
)
No
w
tak
e
the
OR
of
all
lines
(v
alue
):
C
=
W
f
(l
)=
C
l
The
circuit
of
eac
h
line
is
of
size
m
and
since
there
are
at
most

m
lines
of
v
alue
,
the
size
of
the
whole
circuit
is
at
most
m
m
.


LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
So
far
the
circuit
em
ulates
a
generic
computation
of
M
.
Y
et
the
computation
w
e
care
ab
out
refers
to
one
sp
ecic
input.
Similarly
the
initial
state
should
b
e
q
0
and
the
head
should
b
e
lo
cated
at
time
0
in
the
rst
lo
cation.
This
will
b
e
done
b
y
setting
all
triplets
(0;
j
)
as
follo
wing:
Let
x
=
x

x

x

:::x
m
and
n

=
j(hM
i;
x;

t
)j
the
length
of
the
input.

a
0;j
=
(
x
j


j

m
constan
ts
set
b
y
input
x
y
j
 m
m
<
j

t
these
are
the
inputs
to
the
circuit

h
0;j
=
(

j
=

0
j
=


q
0;j
=
(
q
0
j
=

where
q
0
is
the
initial
state
of
M
0
j
=

The
y
elemen
ts
are
the
v
ariables
of
the
circuit.
The
circuit
b
elongs
to
C
S
if
and
only
if
there
exists
an
assignmen
t

for
y
suc
h
that
C
(
)
=
.
Note
that
y
,
the
input
to
the
circuit
pla
ys
the
same
role
as
the
short
witness
y
to
the
fact
that
(hM
i;
x;

t
)
is
a
mem
b
er
of
B
H
.
Note
that
(b
y
padding
y
with
zeros),
w
e
ma
y
assume
without
loss
of
generalit
y
that
jy
j
=
t
 jxj.
So
far
(on
input
y
)
the
circuit
em
ulates
a
running
of
M
on
input
(x;
y
),
it
is
left
to
ensure
that
M
accepts
(x;
y
).
The
output
of
the
circuit
will
b
e
determined
b
y
c
hec
king
whether
at
an
y
time
the
mac
hine
en
tered
the
0
accept
0
state.
This
can
b
e
done
b
y
c
hec
king
whether
in
an
y
of
the
t

t
triplets
in
the
circuit
the
state
is
0
accept
0
.
Since
ev
ery
triplet
(i;
j
)
consists
of
O
(l
og
n)
bits
w
e
ha
v
e
O
(l
og
n)
functions
asso
ciated
with
eac
h
triplet.
Ev
ery
function
can
b
e
computed
b
y
a
circuit
of
size
O
(n
l
og
n),
so
the
circuit
attac
hed
to
triplet
(i;
j
)
is
of
size
O
(n
l
og

n).
There
are
t

t
suc
h
triplets
so
the
size
of
the
circuit
is
O
(n

l
og

n).
Chec
king
for
a
triplet
(i;
j
)
whether
q
i;j
is
0
accept
0
requires
a
circuit
of
size
O
(l
og
n).
This
c
hec
k
is
implemen
ted
for
t

t
triplets,
hence
the
o
v
erall
size
of
the
output
c
hec
k
is
O
(n

l
og
n)
gates.
The
o
v
erall
size
of
the
circuit
will
b
e
O
(n

l
og

n).
Since
the
input
lev
el
of
the
circuit
w
as
set
to
represen
t
the
righ
t
conguration
of
mac
hine
M
when
op
erated
on
input
(x;
y
)
at
time
0,
and
the
circuit
correctly
em
ulates
with
its
i
th
lev
el
the
conguration
of
the
mac
hine
at
time
i,
the
v
alue
of
the
circuit
on
input
y
indicates
whether
or
not
M
accepts
(x;
y
)
within
t
steps.
Th
us,
the
circuit
is
satisable
if
and
only
if
there
exists
a
y
so
that
M
accepts
(x;
y
)
within
t
steps,
i.e.
(hM
i;
x;

t
)
is
in
B
H
.
F
or
a
detailed
description
of
the
circuit
and
full
pro
of
of
correction
see
App
endix.
The
ab
o
v
e
description
can
b
e
view
ed
as
instructions
for
constructing
the
circuit.
Assuming
that
building
one
gate
tak
es
constan
t
time,
constructing
the
circuit
follo
wing
these
instructions
will
b
e
linear
to
the
size
of
the
circuit.
Hence,
construction
time
is
p
olynomial
to
the
size
of
the
input
(hM
i;
x;

t
).
Once
again
the
missing
functions
for
Levin
reduction
of
R
B
H
to
R
C
S
are
the
iden
tit
y
functions.

..
R
S
AT
I
S
N
P
 COMPLETE

.
R
S
AT
is
N
P
 complete
Claim
..
R
S
AT
is
N
P
 hard
under
L
evin
r
e
duction.
Pro
of:
Since
Levin
reduction
is
transitiv
e
it
suces
to
sho
w
a
reduction
from
R
C
S
to
R
S
AT
:
The
reduction
will
map
a
circuit
C
to
an
C
N
F
expression
'
C
and
an
input
y
for
the
circuit
to
an
assignmen
t
y
0
to
the
expression
and
vice
v
ersa.
W
e
b
egin
b
y
describing
ho
w
to
construct
the
expression
'
C
from
C
.
Giv
en
a
circuit
C
w
e
allo
cate
a
v
ariable
to
ev
ery
v
ertex
of
the
graph.
No
w
for
ev
ery
one
of
the
v
ertices
v
build
the
C
N
F
expression
'
v
that
will
force
the
v
ariables
to
comply
to
the
gate's
function:
.
F
or
a
:
v
ertex
v
with
edge
en
tering
from
v
ertex
u:

W
rite
'
v
(v
;
u)
=
((v
W
u)
V
(:u
W
:v
))

It
follo
ws
that
'
v
(v
;
u)
=

if
and
only
if
v
=
:u
.
F
or
a
W
v
ertex
v
with
edges
en
tering
from
v
ertices
u;
w
:

W
rite
'
v
(v
;
u;
w
)
=
((u
W
w
W
:v
)
V
(u
W
:w
W
v
)
V
(:u
W
w
W
v
)
V
(:u
W
:w
W
v
))

It
follo
ws
that
'
v
(v
;
u;
w
)
=

if
and
only
if
v
=
u
W
w
.
F
or
a
V
v
ertex
v
with
edges
en
tering
from
v
ertices
u;
w
:

Similarly
write
'
v
(v
;
u;
w
)
=
((u
W
w
W
:v
)
V
(u
W
:w
W
:v
)
V
(:u
W
w
W
:v
)
V
(:u
W
:w
W
v
))

It
follo
ws
that
'
v
(v
;
u;
w
)
=

if
and
only
if
v
=
u
V
w
.
F
or
the
v
ertex
mark
ed
output
with
edge
en
tering
from
v
ertex
u:
W
rite
'
output
(u)
=
u
W
e
are
ready
no
w
to
dene
'
C
=
V
v
V
'
v
,
where
V
is
the
set
of
all
v
ertices
of
in-degree
at
least
one
(i.e.
the
constan
t
inputs
and
v
ariable
inputs
to
the
circuit
are
not
included).
The
length
of
'
C
is
linear
to
the
size
of
the
circuit.
Once
again
the
instructions
giv
e
a
w
a
y
to
build
the
expression
in
linear
time
to
the
circuit's
size.
W
e
next
sho
w
that
C

C
S
if
and
only
if
'
C

S
AT
.
Actually
,
to
sho
w
that
the
reduction
is
a
Levin-reduction,
w
e
will
sho
w
ho
w
to
ecien
tly
transform
witnesses
for
one
problem
in
to
witnesses
for
the
other.
That
is,
w
e
describ
e
ho
w
to
construct
the
assignmen
t
y
0
to
'
C
from
an
input
y
to
the
circuit
C
(and
vice
v
ersa):
Let
C
b
e
a
circuit
with
m
input
v
ertices
lab
eled
x

;
:::;
x
m
and
d
v
ertices
lab
eled
W
;
V
and
:
namely
,
v

;
:::;
v
d
.
An
assignmen
t
y
=
y

;
:::;
y
m
to
the
circuit's
input
v
ertices
will
propagate
in
to
the
circuit
and
set
the
v
alue
of
all
the
v
ertices.
Considering
that
the
expression
'
C
has
a
v
ariable
for
ev
ery
v
ertex
of
the
circuit
C
,
the
assignmen
t
y
0
to
the
expression
should
consist
of
a
v
alue
for
ev
ery
one
of
the
circuit
v
ertices.
W
e
will
build
y
0
=
y
0
x

;
:::;
y
0
x
m
;
y
0
v

;
y
0
v

;
:::;
y
0
v
d
as
follo
wing:

The
v
ariables
of
the
expression
that
corresp
ond
to
input
v
ertices
of
the
circuit
will
ha
v
e
the
same
assignmen
t:
y
0
x
h
=
y
h
;


h

m.

The
assignmen
t
y
0
v
l
to
ev
ery
other
expression
v
ariable
v
l
will
ha
v
e
the
v
alue
set
to
the
corre-
sp
onding
v
ertex
in
the
circuit,


l

d.


LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
Similarly
giv
en
an
assignmen
t
to
the
expression,
an
assignmen
t
to
the
circuit
can
b
e
built.
This
will
b
e
done
b
y
using
only
the
v
alues
assigned
to
the
v
ariables
corresp
onding
to
the
input
v
ertices
of
the
circuit.
It
is
easy
to
see
that:
C

C
S
(
)
exists
y
suc
h
that
C
(y
)
=

(
)
'
C
(y
0
)
=

(
)
'
C

S
AT
Corollary
.
S
AT
is
N
P
 complete
Bibliographic
Notes
The
initiation
of
the
theory
NP-completeness
is
attributed
to
Co
ok
[],
Levin
[]
and
Karp
[]:
Co
ok
has
initiated
this
theory
in
the
W
est
b
y
sho
wing
that
SA
T
is
NP-complete,
and
Karp
demonstrated
the
wide
scop
e
of
the
phen
umena
(of
NP-completeness)
b
y
demonstrating
a
wide
v
ariet
y
of
NP-
complete
problems
(ab
out
0
in
n
um
b
er).
Indep
enden
tly
,
in
the
East,
Levin
has
sho
wn
that
half
a
dozen
dieren
t
problems
(including
SA
T)
are
NP-complete.
The
three
t
yp
es
of
reductions
discussed
in
the
lecture
are
indeed
the
corresp
onding
reductions
used
in
these
pap
ers.
Whereas
the
Co
ok{
Karp
exp
osition
is
in
terms
of
decision
problems,
Levin's
exp
osition
is
in
terms
of
searc
h
problems
{
whic
h
explains
wh
y
he
uses
the
stronger
notion
of
reduction.
Curren
tly
thousands
of
problems,
from
an
ev
en
wider
range
of
sciences
and
tec
hnologies,
are
kno
wn
to
b
e
NP-complete.
A
partial
(out-dated)
list
is
pro
vided
in
Garey
and
Johnson's
b
o
ok
[].
In
terestingly
,
almost
all
reductions
used
to
establish
NP-completeness
are
m
uc
h
more
restricted
than
allo
w
ed
in
the
denition
(ev
en
according
to
Levin).
In
particular,
they
are
all
computable
in
logarithmic
space
(see
next
lectures
for
denition
of
space).
.
Co
ok,
S.A.,
\The
Complexit
y
of
Theorem
Pro
ving
Pro
cedures",
Pr
o
c.
r
d
A
CM
Symp.
on
The
ory
of
Computing,
pp.
{,
	.
.
Garey
,
M.R.,
and
D.S.
Johnson.
Computers
and
Intr
actability:
A
Guide
to
the
The
ory
of
NP-Completeness,
W.H.
F
reeman
and
Compan
y
,
New
Y
ork,
		.
.
Karp,
R.M.,
\Reducibilit
y
among
Com
binatorial
Problems",
Complexity
of
Computer
Com-
putations,
R.E.
Miller
and
J.W.
Thatc
her
(eds.),
Plen
um
Press,
pp.
{0,
	.
.
Levin,
L.A.,
\Univ
ersal
Searc
h
Problems",
Pr
oblemy
Per
e
daci
Informacii
	,
pp.
{,
	.
T
ranslated
in
pr
oblems
of
Information
T
r
ansmission
	,
pp.
{.
App
endix:
Details
for
the
reduction
of
B
H
to
C
S
W
e
presen
t
no
w
the
details
of
the
reduction
from
B
H
to
C
S
.
The
circuit
that
will
em
ulate
the
run
of
mac
hine
M
on
input
x
can
b
e
constructed
in
the
follo
wing
w
a
y:
Let
(hM
i;
x;
t)
b
e
the
input
to
b
e
determined
whether
is
in
B
H
,
where
x
=
x

x

:::x
m
and
n

=
j(hM
i;
x;
t)j
the
length
of
the
input.
W
e
will
use
the
fact
that
ev
ery
gate
of
in-degree
r
can
b
e
replaced
b
y
r
gates
of
in-degree
.
This
can
b
e
done
b
y
building
a
balanced
binary
tree
of
depth
l
og
r
.
In
the
construction
0
and
0
;
0
or
0
gates
of
v
arying
in-degree
will
b
e
used.
When
analyzing
complexit
y
,
ev
ery
suc
h
gate
will
b
e
w
eighed
as
its
in-degree.
The
n
um
b
er
of
states
of
mac
hine
M
is
at
most
n,
hence
l
og
n
bits
can
represen
t
a
state.
Similarly
the
size
of
alphab
et
of
mac
hine
M
is
at
most
n,
and
therfore
l
og
n
bits
can
represen
t
a
letter.

..
R
S
AT
I
S
N
P
 COMPLETE
	
.
Input
Lev
el
y
is
the
witness
to
b
e
en
tered
at
a
later
time
(assume
y
is
padded
b
y
zeros
to
complete
length
t
as
explained
earlier).

a
0;j
=
(
x
j


j

m
constan
ts
set
b
y
input
x
y
j
 m
m
<
j

t
these
are
the
inputs
to
the
circuit

h
0;j
=
(

j
=

0
j
=


q
0;j
=
(
q
0
j
=

where
q
0
is
the
initial
state
of
M
0
j
=

As
said
b
efore
this
represen
ts
the
conguration
at
time
0
of
the
run
of
M
on
(x;
y
).
This
stage
sets
O
(n
l
og
n)
wires.
.
F
or
0
<
i
<
t
;
h
i+;j
will
b
e
wired
as
sho
wn
in
gure
:
h
i+;j



W






V
V
V
-


h
i;j
 
h
i;j
h
i;j
+





















A
A
A
K
A
A
A
K
A
A
A
K
W
(q
;a)R
((q
i;j
 
=
q
)
V
(a
i;j
 
=
a))




W
(q
;a)S
((q
i;j
=
q
)
V
(a
i;j
=
a))




W
(q
;a)L
((q
i;j
+
=
q
)
V
(a
i;j
+
=
a))




gure


0
LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
The
denition
of
groups
R
;
S;
L
is:
R

=
f(q
;
a)
:
q

K
V
a

f0;
g
V

(q
;
a)
=
(;
;
R
)g
S

=
f(q
;
a)
:
q

K
V
a

f0;
g
V

(q
;
a)
=
(;
;
S
)g
L

=
f(q
;
a)
:
q

K
V
a

f0;
g
V

(q
;
a)
=
(;
;
L)g
The
equations
are
easily
wired
using
an
0
and
0
gate
for
ev
ery
equation.
The
size
of
this
comp
onen
t:
The
last
item
on
ev
ery
en
try
in
the
relation

is
either
R
;
L
or
S
.
F
or
ev
ery
one
of
these
en
tries
there
is
one
comparison
ab
o
v
e.
Since

is
b
ounded
b
y
n
there
are
at
most
n
suc
h
comparisons.
A
comparison
of
the
state
requires
O
(l
og
n)
gates.
Similarly
a
comparison
of
the
letter
requires
O
(l
og
n)
gates.
Hence
the
total
n
um
b
er
of
gates
in
gure

is
O
(n
l
og
n)
.
F
or
0
<
i
<
t
;
q
i+;j
will
b
e
wired
as
sho
wn
in
gure
:
q
i+;j



W






V
V
V
-


h
i;j
 
h
i;j
h
i;j
+






















A
A
A
K
A
A
A
K
A
A
A
K
W
(q
;a;p)R
((q
i;j
 
=
q
)
V
(a
i;j
 
=
a)
V
p)




W
(q
;a;p)S
((q
i;j
=
q
)
V
(a
i;j
=
a)
V
p)




W
(q
;a;p)L
((q
i;j
+
=
q
)
V
(a
i;j
+
=
a)
V
p)




gure

The
denition
of
groups
R
;
S;
L
in:
R

=
f(q
;
a;
p)
:
q
;
p

K
V
a

f0;
g
V

(q
;
a)
=
(p;
;
R
)g
S

=
f(q
;
a;
p)
:
q
;
p

K
V
a

f0;
g
V

(q
;
a)
=
(p;
;
S
)g
L

=
f(q
;
a;
p)
:
q
;
p

K
V
a

f0;
g
V

(q
;
a)
=
(p;
;
L)g
Once
again
ev
ery
comparison
requires
O
(l
og
n)
gates.
Ev
ery
state
is
represen
ted
b
y
l
og
(n)
bits
so
the
gure
has
to
b
e
m
ultiplied
for
ev
ery
bit.
Ov
erall
complexit
y
of
the
comp
onen
t
in
gure

is
O
(n
l
og

n).

..
R
S
AT
I
S
N
P
 COMPLETE

.
F
or
0
<
i
<
t
;
a
i+;j
will
b
e
wired
as
sho
wn
in
gure
:
a
i+;j



W




V
V
-

h
i;j
h
i;j
a
i;j







A
A
A
K
A
A
A
K
W
(q
;a;t)T
((q
i;j
=
q
)
V
(a
i;j
=
a)
V
t)




gure

The
denition
of
T
is:
T

=
f(q
;
a;
t)
:
q

K
V
a;
t

f0;
g
V

(q
;
a)
=
(;
t;
)g
Once
again
all
en
tries
of
the
relation

ha
v
e
to
b
e
c
hec
k
ed,
hence
there
are
O
(n)
comparisons
of
size
O
(l
og
n).
Since
the
letter
is
represen
ted
b
y
O
(l
og
n)
bits,
the
o
v
erall
complexit
y
of
the
comp
onen
t
in
gure

is
O
(n
l
og

n).
.
Finally
the
output
gate
of
the
circuit
will
b
e
a
c
hec
k
whether
at
an
y
lev
el
of
the
circuit
the
state
w
as
accept.
This
will
b
e
done
b
y
comparing
q
i;j
;


j

t;
0

i

t
to
0
accept
0
.
There
are
t

t
suc
h
comparisons,
eac
h
of
them
tak
es
O
(l
og
n)
gates.
T
aking
an
OR
on
all
these
comparisons
costs
O
(n

l
og
n)
gates.
F
or
ev
ery
cell
in
the
t

t
matrix
w
e
used
O
(n

)
gates.
The
whole
circuit
can
b
e
built
with
O
(n

)
gates.
With
this
description,
building
the
circuit
is
linear
to
circuit's
size.
Hence,
this
can
b
e
done
in
p
olynomial
time.
Correctness:
W
e
will
sho
w
no
w
that
(hM
i;
x;

t
)

B
H
if
and
only
if
C
(hM
i;x;t)

C
S
Claim
..
Gates
at
level
i
of
the
cir
cuit
r
epr
esent
the
exact
c
ongur
ation
of
M
at
time
i
on
input
(x;
y
).
Pro
of:
By
induction
on
time
i.

i
=
0,
stage

of
the
construction
ensures
correctness.

Assume
C
's
gates
on
lev
el
i
correctly
represen
t
M
's
conguration
at
time
i
and
pro
v
e
for
i
+
:
Set
j
as
the
p
osition
of
the
head
at
time
i
(h
i;j
=
).
{
The
letter
con
ten
ts
of
all
cells
(i
+
;
k
);
k
=
j
do
es
not
c
hange.
Same
happ
ens
in
the
circuit
since
(a
i;k
V
(:h
i;k
))
=
a
i;k
.
{
Lik
ewise
the
head
can
not
reac
h
cells
(i
+
;
k
)
where
k
<
(j
 )
or
k
>
(j
+
).
Resp
ectiv
ely
h
i;k
=
0
since
h
i;k
 
=
h
i;k
=
h
i;k
+
=
0.


LECTURE
.
NP-COMPLETENESS
AND
SELF
REDUCIBILITY
{
The
same
argumen
t
sho
ws
that
state
bits
for
all
gates
of
similar
k
0
s
will
b
e
reset
to
zero.
Let

(q
i;j
;
a
i;j
)
=
(q
;
a;
m)
W
e
shall
lo
ok
in
to
what
happ
ens
when
mac
hine's
head
sta
ys
in
place,
i.e.
m
=
S
.
The
other
t
w
o
p
osibilities
for
mo
v
emen
t
of
the
head
are
similar.
{
Cell
(i;
j
)
on
the
tap
e
will
c
hange
in
to
a.
Since
h
i;j
=

and
corresp
ondingly
(q
i;j
=
q
i;j
V
a
i;j
=
a
i;j
V
a)
will
return
a
{
The
head
sta
ys
in
place
and
resp
ectiv
ely:
.
h
i+;j
 
=
0
since
h
i;j
=

but

(q
i;j
;
a
i;j
)
is
not
(;
;
L).
.
h
i+;j
=

since
h
i;j
=

and
one

(q
i;j
;
a
i;j
)
=
(;
;
S
)
returns
.
.
h
i+;j
+
=
0
since
h
i;j
=

but

(q
i;j
;
a
i;j
)
is
not
(;
;
R
).
{
The
mac
hine's
next
state
is
q
,
and
resp
ectiv
ely:
.
Similarly
q
i+;j
 
and
q
i+;j
+
will
b
e
reset
to
zero.
.
q
i+;j
will
c
hange
in
to
q
since
h
i;j
=

and
(q
i;j
=
q
i;j
V
a
i;j
=
a
i;j
V
q
)
will
return
q
.
So
at
an
y
time
0

i

t,
gate
lev
el
i
correctly
represen
ts
M
's
conguration
at
time
i.

Lecture

More
on
NP
and
some
on
DTIME
Notes
tak
en
b
y
Mic
hael
Elkin
and
Ek
aterina
Sedletsky
Summary:
In
this
lecture
w
e
discuss
some
prop
erties
of
the
complexit
y
classes
P
,
NP
and
NPC
(theorems
of
Ladner
and
Levin).
W
e
also
dene
new
complexit
y
classes
(DTime
i
),
and
consider
some
relations
b
et
w
een
them.
.
Non-complete
languages
in
NP
In
this
lecture
w
e
consider
sev
eral
items,
that
describ
e
more
closely
the
picture
of
the
complexit
y
w
orld.
W
e
already
kno
w
that
P

N
P
and
w
e
conjecture
that
it's
a
strict
inequalit
y
,
although
w
e
can't
pro
v
e
it.
Another
imp
ortan
t
class
that
w
e
ha
v
e
considered
is
N
P
 compl
ete
(N
P
C
)
problems,
and
as
w
e
ha
v
e
already
seen,
if
there
is
a
gap
b
et
w
een
P
and
N
P
then
the
class
of
N
P
C
problems
is
con
tained
in
this
gap
(N
P
C

N
P
nP
).
The
follo
wing
theorem
of
Ladner
tells
us
additional
information
ab
out
this
gap
N
P
nP
,
namely
that
N
P
C
is
strictly
con
tained
in
N
P
nP
.
F
ormally
,
Theorem
.
If
P
=
N
P
then
ther
e
exists
a
language
L

(N
P
nP
)nN
P
C
:
That
is,
N
P
(or
sa
y
S
AT
)
is
not
(Karp)
reducible
to
L.
Actually
,
one
can
sho
w
that
S
AT
is
not
ev
en
Co
ok-reducible
to
L.
Oded's
Note:
F
ollo
wing
is
a
pro
of
sk
etc
h.
W
e
start
with
an
y
B

N
P
nP
,
and
mo
dify
it
to
B
0
=
B
\
S
,
where
S

P
,
so
that
B
0
is
neither
NP-complete
nor
in
P
.
The
fact
that
S
is
in
P
implies
that
B
0
is
in
N
P
.
The
\seiving"
set
S
will
b
e
constructed
to
to
foil
b
oth
all
p
ossible
p
olynomial-time
algorithms
for
B
0
and
all
p
ossible
reductions.
A
t
the
extreme,
setting
S
=
f0;
g

,
foil
all
algorithms
since
in
this
case
B
0
=
B

P
.
On
the
other
hand,
setting
S
=
;,
foils
all
p
ossible
reductions
since
in
this
case
B
0
=
;
and
so
(under
P
=
N
P
)
cannot
b
e
NP-complete
(as
reducing
to
it
giv
es
nothing).
Note
that
the
ab
o
v
e
argumen
t
extends
to
the
case
S
(resp.,
S
)
is
a
nite
set.
The
\seiving"
set
S
is
constructed
iterativ
ely
so
that
in
o
dd
iterations
is
fails
mac
hines
from
a
standard
en
umeration
of
p
olynomial-time
mac
hines
(so
that
in
iteration
i
 
w
e
fail
the
i
th
mac
hine).
(Here
w
e
don't
need
to
em
ulate
these
mac
hines
in
p
olynomial-time



LECTURE
.
MORE
ON
NP
AND
SOME
ON
DTIME
in
length
of
their
inputs.)
In
ev
en
iterations
w
e
fail
oracle-mac
hines
from
a
standard
en
umeration
of
suc
h
mac
hines
(whic
h
corresp
ond
to
Co
ok-reductions
of
B
to
B
0
)
so
that
in
iteration
i
w
e
fail
the
i
th
oracle-mac
hine.
The
iteration
n
um
b
er
is
determined
b
y
a
deciding
algorithm
for
S
whic
h
is
op
erates
as
follo
ws.
F
or
simplicit
y
,
the
algorithm
puts
z
in
S
i
it
puts

jz
j
in
S
.
The
decision
whether
to
put

n
in
S
is
tak
en
as
follo
ws.
Starting
with
the
rst
iteration,
and
using
a
time-out
mec
hanism
with
a
xed
p
olynomial
b
ound
b(n)
(e.g.,
b(n)
=
n

or
b(n)
=
log
n
will
do),
w
e
try
to
nd
a
input
z

f0;
g

so
that
the
rst
p
olynomial-time
algorithm,
A

,
fails
on
z
(i.e.,
A

(z
)
=

B
(z
)).
In
order
to
decide

B
(z
)
w
e
run
the
ob
vious
exp
onen
tial-time
algorithm,
but
z
is
exp
ected
to
b
e
m
uc
h
shorter
than
n
(or
else
w
e
halt
b
y
time-out
b
efore).
W
e
scan
all
p
ossible
z
's
in
lexicographic
order
un
til
reac
hing
time-
out.
Once
w
e
reac
h
time-out
while
not
nding
suc
h
bad
z
,
w
e
let

n

S
.
Ev
en
tually
,
for
a
sucien
tly
large
n
w
e
will
nd
a
bad
z
within
the
time-out.
In
suc
h
a
case
w
e
let

n

S
and
con
tin
ue
to
the
second
iteration,
where
w
e
consider
the
rst
p
olynomial-
time
oracle-mac
hine,
M

.
No
w
w
e
try
to
nd
an
input
z
for
B
on
whic
h
M

with
oracle
S
.
Note
that
w
e
kno
w
the
v
alue
n
0
<
n
so
that

n
0
w
as
the
rst
string
not
put
in
S
.
So
curren
tly
,
S
is
though
t
of
as
con
taining
only
strings
of
length
smaller
than
n
0
.
W
e
em
ulate
M

while
answ
ering
its
queries
to
B
0
=
B
\
S
accordingly
,
using
the
exp
onen
tial-time
algorithm
for
deciding
B
(and
our
kno
wledge
of
the
p
ortion
of
S
determined
so
far).
W
e
also
use
the
exp
onen
tial-time
algorithm
for
B
to
c
hec
k
if
the
recuction
of
z
to
B
0
is
correct
for
eac
h
z
.
Once
w
e
reac
h
time-out
while
not
nding
suc
h
bad
z
,
w
e
let

n

S
.
Again,
ev
en
tually
,
for
a
sucien
tly
large
n
w
e
will
nd
a
bad
z
within
the
time-out.
In
suc
h
a
case
w
e
let

n

S
and
con
tin
ue
to
the
third
iteration,
where
w
e
consider
the
second
p
olynomial-time
algorithm,
and
so
on.
Some
implemen
tation
details
are
pro
vided
b
elo
w.
Sp
ecically
,
the
algorithm
T
b
elo
w
computes
the
n
um
b
er
of
iterations
completed
wrt
input
x

f0;
g
n
.
Pro
of:
Let
B

N
P
nP
.
Let
A
0
;
A

;
:::
b
e
the
en
umeration
of
all
p
olynomial
time
b
ounded
T
uring
mac
hines,
that
solv
e
decision
problems
and
M
0
;
M

;
:::
b
e
the
en
umeration
of
p
olynomial
time
b
ounded
oracle
T
uring
mac
hines.
Let
L(A
i
)
denote
the
set
recognized
b
y
A
i
and
for
ev
ery
set
S
let
L(M
S
i
)
to
b
e
the
set
recognized
b
y
mac
hine
M
i
with
oracle
S
.
W
e
construct
a
p
olynomial
time
b
ounded
T
uring
mac
hine
T
with
range
fg

in
suc
h
a
w
a
y
that,
for
B
0
=
fx

B
:
jT
(x)j
is
ev
eng,
b
oth
B
0
=

P
and
B
/
C
B
0
(i.e.,
B
is
not
Co
ok
reducible
to
B
0
).
It
follo
ws
that
B
0
=

P
[
N
P
C
.
W
e
sho
w
that
for
an
y
suc
h
T
,
B
0
/
K
B
(B
0
is
Karp
reducible
to
B
)
and
B
0

N
P
follo
ws,
since
diciding
B
0
can
b
e
done
b
y
deciding
B
and
B

N
P
.The
Karp-reduction
(of
B
0
to
B
),
denoted
f
,
is
dened
as
follo
ws.
Let
x
0
=

B
.
(W
e
assume
that
B
=


,
b
ecause
otherwise
B

P
).
The
function
f
will
compute
jT
(x)j
and
if
jT
(x)j
is
ev
en
it
will
return
x
and
otherwise
it
will
return
x
0
.
No
w
if
x

B
0
,
then
x

B
and
jT
(x)j
is
ev
en,
hence
f
(x)
=
x

B
.
Otherwise
x
=

B
0
and
then
there
are
t
w
o
p
ossibilities.
.
If
x
=

B
,
then
for
ev
en
jT
(x)j
holds
f
(x)
=
x,
and
for
o
dd
jT
(x)j
holds
f
(x)
=
x
0
,
and
so
for
an
y
x
holds
f
(x)
=

B
.
.
If
x

B
and
jT
(x)j
is
o
dd,
then
f
(x)
=
x
0
=

B
.
(Recall
that
x
=

B
0
rules
out
\x

B
and
ev
en
jT
(x)j")
T
o
complete
the
construction
w
e
ha
v
e
to
build
a
T
uring
mac
hine
T
suc
h
that

..
OPTIMAL
ALGORITHMS
F
OR
NP

()
B
0
def
=
fx

B
:
jT
(x)j
is
ev
en
g
=
L(A
i
),
for
an
y
i
=
0;
;
;
:::
(and
so
B
0

P
)
()
L(M
B
0
i
)
=
B
,
for
an
y
i
=
0;
;
;
:::
(and
so
B
/
C
B
0
).
A
mac
hine
T
that
satises
these
conditions
ma
y
b
e
constructed
in
the
follo
wing
w
a
y:
On
input

(empt
y
string),
T
prin
ts
.
On
an
input
x
of
length
n
where
x
=
0
n
(unary),
T
prin
ts
T
(0
n
).
It
remains
to
sa
y
what
T
do
es
on
inputs
of
the
form
0
n
where
n

.
On
input
0
n
where
n

,
T
do
es
the
follo
wing:
.
F
or
n
mo
v
es,
try
to
reconstruct
the
sequence
T
();
T
(0);
T
(0

);
:::.
Let
T
(0
m
)
b
e
the
last
n
um
b
er
of
this
sequence
that
is
computed.
.
W
e
consider
t
w
o
cases
dep
ending
on
the
parit
y
of
jT
(0
m
)j.
W
e
asso
ciate
B
with
an
exp
onen
tial-
time
algorithm
deco
ding
B
(b
y
scanning
all
p
ossible
NP-witnesses).
Case(i):
jT
(0
m
)j
is
ev
en.
Let
i
=
jT
(0
m
)j=.
F
or
n
mo
v
es,
try
to
nd
a
z
suc
h
that
B
0
(z
)
=
A
i
(z
).
This
is
done
b
y
successiv
ely
sim
ulating
B
and
T
(to
see
what
B
0
is)
and
A
i
,
on
inputs
;
0;
;
00;
0;
:::.
If
no
suc
h
z
is
found,
prin
t

i
;
otherwise,
prin
t

i+
.
Case(ii):
jT
(0
m
)j
is
o
dd.
Let
i
=
(jT
(0
m
)j
 )=.
F
or
n
mo
v
es,
try
to
nd
a
z
suc
h
that
B
(z
)
=
M
B
0
i
(z
).
This
is
done
b
y
sim
ulating
an
algorithm
B
and
the
pro
cedure
M
i
successiv
ely
on
inputs
;
0;
;
00;
0;
:::.
In
sim
ulating
M
i
on
some
input,
whenev
er
M
i
mak
es
a
query
of
length
at
most
m,
w
e
answ
er
it
according
to
B
0
determined
b
y
B
and
the
v
alues
of
T
computed
in
Step
().
In
case
the
query
has
lengths
exceeding
m,
w
e
b
eha
v
e
as
if
w
e
ha
v
e
already
completed
n
steps.
(The
mo
v
es
in
this
side
calculation
are
coun
ted
among
the
n
steps
allo
w
ed.)
If
no
suc
h
z
is
found,
prin
t

i+
;
otherwise,
prin
t

i+
.
Suc
h
a
mac
hine
can
b
e
programmed
in
a
T
uring
mac
hine
that
runs
in
p
olynomial
time.
F
or
this
sp
ecic
mac
hine
T
w
e
obtain
that
B
0
=
fx

B
:
jT
(x)j
is
ev
en
g,
satises:
B
0

N
P
nP
and
B
/
C
B
0
=
)
B
0
=

N
P
C
=
)
B
0

(N
P
nP
)nN
P
C
.
The
set
B
0
constructed
in
the
ab
o
v
e
pro
of
is
certainly
not
a
natural
one
(ev
en
in
case
B
is).
W
e
men
tion
that
there
are
some
natural
problems
conjectured
to
b
e
in
(N
P
nP
)nN
P
C
:
F
or
example,
Graph
Isomorphism
(the
problem
of
whether
t
w
o
graphs
are
isomorphic).
.
Optimal
algorithms
for
NP
The
follo
wing
theorem,
due
to
Levin,
states
an
in
teresting
prop
ert
y
of
the
class
N
P
.
Informally
,
Levin's
Theorem
tells
us
that
exists
optimal
algorithm
for
an
y
N
P
searc
h
problem.
Theorem
.
F
or
any
N
P
-r
elation
R
ther
e
exist
a
p
olynomial
P
R
()
and
an
algorithm
A
R
()
which
nds
solutions
whenever
they
exist,
so
that
for
every
algorithm
A
which
nds
solutions
and
for
any
input
x
time
A
R
(x)

O
(time
A
(x)
+
P
R
(jxj));
(.)
wher
e
time
A
(x)
is
the
running
time
of
the
algorithm
A
on
input
x.
This
means
that
for
ev
ery
algorithm
A
exists
a
constan
t
c
suc
h
that
for
an
y
sucien
tly
long
x
time
A
R
(x)

c

(time
A
(x)
+
P
R
(jxj)):


LECTURE
.
MORE
ON
NP
AND
SOME
ON
DTIME
This
c
is
not
a
univ
ersal
constan
t,
but
an
algorithm-sp
ecic
one
(i.e.,
dep
ends
on
A).
The
algorithm
A
R
is
optimal
in
the
follo
wing
sense:
for
an
y
other
algorithm
A
there
exists
a
constan
t
c
suc
h
that
for
an
y
sucien
tly
long
x
time
A
(x)


c

time
A
R
(x)
 Q
R
(jxj);
where
Q
R
(jxj)
=

c

P
R
(jxj):
The
algorithms
w
e
are
talking
ab
out
are
not
TM's.
F
or
pro
ving
the
theorem,
w
e
should
dene
exactly
the
computational
mo
del
w
e
are
w
orking
with.
Either
it
will
b
e
a
one-tap
e
mac
hine
or
t
w
o-tap
e
one
and
etc.
Dep
ending
on
the
exact
mo
del
the
constan
t
c
ma
y
b
e
replaced
b
y
some
lo
w-n
function
lik
e
l
og
n.
A
constan
t
ma
y
b
e
ac
hiev
ed
only
in
more
p
o
w
erful/exible
mo
dels
of
computation
that
are
not
discussed
here.
W
e
observ
e
also
that
although
the
pro
of
of
Levin's
Theorem
is
a
constructiv
e
one,
it's
completely
impractical,
since
as
w
e'll
see
it
incorp
orates
a
h
uge
constan
t
in
its
running
time.
On
the
other
hand,
it
illustrates
an
imp
ortan
t
asymptotic
prop
ert
y
of
the
N
P
class.
Pro
of:
The
basic
idea
of
the
pro
of
is
to
ha
v
e
an
en
umeration
of
all
p
ossible
algorithms.
This
set
is
coun
table,
since
the
set
of
all
p
ossible
deterministic
TM's
is
coun
table.
Using
this
en
umeration
w
e
w
ould
lik
e
to
run
all
the
algorithms
in
parallel.
It's,
of
course,
imp
ossible
since
w
e
can't
run
a
coun
table
set
of
TM's
in
parallel,
but
the
solution
to
the
problem
is
to
run
them
in
dieren
t
rates.
There
are
sev
eral
p
ossible
implemen
tations
of
this
idea.
One
of
the
p
ossibilities
is
as
follo
wing.
Let
us
divide
the
execution
of
the
sim
ulating
mac
hine
in
to
rounds.
The
sim
ulating
mac
hine
runs
mac
hine
i
at
round
r
if
and
only
if
r

0
(mo
d
i

).
That
is,
w
e
let
i'th
mac
hine
to
mak
e
t
steps
during
i


t
rounds
of
the
sim
ulating
mac
hine.
Also
the
n
um
b
er
of
steps
made
in
these
r
=
i


t
rounds
is
P
j

j
r
j

j
<
r
.
Suc
h
a
construction
w
ould
fail
if
some
of
these
mac
hines
w
ould
pro
vide
wrong
answ
ers.
T
o
solv
e
this
dicult
y
w
e
"force"
the
mac
hines
to
v
erify
the
v
alidit
y
of
their
outputs.
So,
without
loss
of
generalit
y
,
w
e
assume
that
eac
h
mac
hine
is
augmen
ted
b
y
a
v
erifying
comp
onen
t
that
c
hec
ks
the
v
alidit
y
of
mac
hine's
output.
Since
the
problem
is
in
N
P
,
v
erifying
the
output
tak
es
p
olynomial
amoun
t
of
time.
When
w
e
estimate
the
running
time
of
the
algorithm
A
R
,
w
e
tak
e
in
to
accoun
t
this
p
olynomial
amoun
t
of
time
and
it
is
the
reason
that
P
R
arises
in
Eq.
(.).
So,
without
loss
of
generalit
y
,
the
outputs
of
the
algorithms
are
correct.
Another
dicult
y
,
is
that
some
of
these
mac
hines
could
not
ha
v
e
sucien
t
amoun
t
of
time
to
halt.
On
the
other
hand,
since
eac
h
of
the
mac
hines
solv
es
the
problem,
it
is
sucien
t
for
us
that
one
of
them
will
halt.
Levin's
optimal
algorithm
A
R
is
this
construction
running
in
terc
hangeably
all
these
mac
hines.
W
e'll
claim
the
follo
wing
prop
ert
y
of
the
construction.
Claim
..
Consider
A
that
solves
the
pr
oblem.
L
et
i
b
e
the
p
osition
of
A
in
the
enumer
ation
of
al
l
the
machines.
L
et
time
A
(
)
b
e
the
running
time
of
A.
It
is
run
in
A
R
in

f
(i)
r
ate.
Then
A
R
runs
at
most
f
(i)

time
A
(
).
W
e
to
ok
f
(i)
=
(i
+
)

,
but
that
is
not
really
imp
ortan
t:
w
e
observ
e
that
i
is
a
constan
t
(and
th
us,
so
is
f
(i)).
Of
course,
it's
a
h
uge
constan
t,
b
ecause
eac
h
mac
hine
needs
millions
of
bits
to
b
e
enco
ded
(ev
en
the
simplest
deterministic
TM
that
do
es
nothing
needs
sev
eral
h
undreds
of
bits
to
b
e
enco
ded)
and
the
index
of
mac
hine
M
in
the
en
umeration
(i.e.
i)
is
i


jM
j
,
where
jM
j
is
the
n
um
b
er
of
bits
needed
to
enco
de
mac
hine
M
and
then
f
(i)
will
b
e
(i
+
)

=
(
jM
j
+
)

.
(This
constan
t
mak
es
the
algorithm
completely
impractical.)

..
GENERAL
TIME
COMPLEXITY
CLASSES

.
General
Time
complexit
y
classes
The
class
P
w
as
in
tro
duced
to
capture
our
notion
of
ecien
t
computation.
This
class
has
some
\robustness"
prop
erties
whic
h
mak
e
it
con
v
enien
t
for
in
v
estigation.
.
P
is
not
mo
del-dep
enden
t:
Remain
the
same
if
w
e
consider
one
tap
e
TM
or
t
w
o
tap
e
TM.
This
remains
v
alid
for
an
y
"reasonable"
and
"general"
enough
mo
del
of
computation.
.
P
is
robust
under
"Reasonable"
c
hanges
to
an
algorithm:
Closed
classes
under
"reasonable"
c
hanges
of
the
algorithm,
lik
e
ipping
the
output
and
things
lik
e
that.
This
holds
for
P
,
but
probably
not
for
N
P
and
N
P
C
.
The
same
applies
also
for
()
and
().
.
Closed
under
serial
comp
osition:
Concatenation
of
t
w
o
(ecien
t)
algorithms
from
a
class
will
pro
duce
another
(ecien
t)
algorithm
from
the
same
class.
.
Closed
under
subroutine
op
eration:
Using
one
algorithm
from
a
class
as
a
subroutine
of
another
algorithm
from
the
same
class
pro
vides
an
algorithm
from
the
class
(the
class
is
set
of
problems
and
when
w
e
are
talking
ab
out
an
algorithm
from
the
class
w
e
mean
an
algorithm
for
solving
a
problem
from
the
class
and
the
existence
of
this
algorithm
is
evidence
that
the
problem
indeed
b
elongs
to
this
class).
None
of
these
nice
prop
erties
holds
for
classes
that
w
e
will
no
w
dene.
..
The
DTime
classes
Oded's
Note:
DTime
denotes
the
class
of
languages
whic
h
are
decideable
within
a
sp
ecic
time-b
ound.
Since
this
class
sp
ecies
one
time-b
ound
rather
than
a
family
of
suc
h
b
ounds
(lik
e
p
olinomial-time),
w
e
need
to
b
e
so
ecic
with
resp
ect
to
the
mo
del
of
computation.
Denition
.
D
T
ime
i
(t())
is
the
class
of
languages
de
cidable
by
a
deterministic
i-tap
e
T
uring
Machine
within
t()
steps.
That
is,
L

D
T
ime
i
(
t())
if
ther
e
exists
a
deterministic
i-tap
e
T
uring
Machine
M
which
ac
c
epts
L
so
that
for
any
x

f0;
g

,
on
input
x
machine
M
makes
at
most
t(jxj)
steps.
Usually
,
w
e
consider
i
=

or
i
=

talking
ab
out
one-
and
t
w
o-tap
e
TM's
resp
ectiv
ely
.
When
w
e'll
consider
space
complexit
y
w
e'll
nd
it
v
ery
natural
to
deal
with
-tap
e
TM.
If
there
is
no
index
in
DTime,
then
the
default
is
i
=
.
Using
this
new
notation
w
e
presen
t
the
follo
wing
theorem:
Theorem
.
F
or
every
function
t()
that
is
at
le
ast
line
ar
D
T
ime

(t())

D
T
ime

(t()

)
The
theorem
is
imp
ortan
t
in
the
sense
that
it
enables
us
sometimes
to
skip
the
index,
since
with
resp
ect
to
p
olynomial-time
computations
b
oth
mo
dels
(one-tap
e
and
t
w
o-tap
e)
coincide.
The
pro
of
is
b
y
sim
ulating
the
t
w
o-tap
e
TM
on
a
one-tap
e
one.
Pro
of:
Consider
a
language
L

D
T
ime

(t()).
Therefore,
there
exists
a
t
w
o-tap
e
TM
M

whic
h
accepts
L
in
O
(t()).
W
e
can
imagine
that
the
tap
e
of
a
one-tap
e
TM
as
divided
in
to

trac
ks.
W
e


LECTURE
.
MORE
ON
NP
AND
SOME
ON
DTIME
can
construct
M

,
a
one-tap
e
TM
with

trac
ks,

trac
ks
for
eac
h
of
M

's
tap
es.
One
trac
k
records
the
con
ten
ts
of
the
corresp
onding
tap
e
of
M

and
the
other
is
blank,
except
for
a
mark
er
in
the
cell
that
holds
the
sym
b
ol
scanned
b
y
the
corresp
onding
head
of
M

.
The
nite
con
trol
of
M

stores
the
state
of
M

,
along
with
a
coun
t
of
the
n
um
b
er
of
head
mark
ers
to
the
righ
t
of
M

's
tap
e
head.
Eac
h
mo
v
e
of
M

is
sim
ulated
b
y
a
sw
eep
from
left
to
righ
t
and
then
from
righ
t
to
left
b
y
the
tap
e
head
of
M

,
whic
h
tak
es
O
(t())
time.
Initially
,
M

's
head
is
at
the
leftmost
cell
con
taining
a
head
mark
er.
T
o
sim
ulate
a
mo
v
e
of
M

;
M

sw
eeps
righ
t,
visiting
eac
h
of
the
cells
with
head
mark
ers
and
recording
the
sym
b
ol
scanned
b
y
b
oth
heads
of
M

.
When
M

crosses
a
head
mark
er,
it
m
ust
up
date
the
coun
t
of
head
mark
ers
to
the
righ
t.
When
no
more
head
mark
ers
are
to
the
righ
t,
M

has
seen
the
sym
b
ols
scanned
b
y
b
oth
of
M

's
heads,
so
M

has
enough
information
to
determine
the
mo
v
e
of
M

.
No
w
M

mak
es
a
pass
left,
un
til
it
reac
hes
the
leftmost
head
mark
er.
The
coun
t
of
mark
ers
to
the
righ
t
enables
M

to
tell
when
it
has
gone
far
enough.
As
M

passes
eac
h
head
mark
er
on
the
left
w
ard
pass,
it
up
dates
the
tap
e
sym
b
ol
of
M

"scanned"
b
y
that
head
mark
er,
and
it
mo
v
es
the
head
mark
er
one
sym
b
ol
left
or
righ
t
to
sim
ulate
the
mo
v
e
of
M

.
Finally
,
M

c
hanges
the
state
of
M

recorded
in
M

's
con
trol
to
complete
the
sim
ulation
of
one
mo
v
e
of
M

.
If
the
new
state
of
M

is
accepting,
then
M

accepts.
Finding
the
mark
costs
O
(t());
and
as
there
are
not
more
than
t()
mo
v
es,
totally
the
execution
costs
at
most
O
(t

()).
The
next
theorem
is
less
imp
ortan
t
and
brough
t
from
elegancy
considerations,
and
sa
ys
that
in
general
one
cannot
do
a
b
etter
sim
ulation
than
one
in
Theorem
.
W
e
recall
the
denition
of
"O
",
"
"
and
"o"
notations:

f
(n)
=
O
(g
(n))
means
that
exists
c
suc
h
that
for
an
y
sucien
tly
large
n
f
(n)

c

g
(n).

f
(n)
=

(g
(n))
means
that
exists
c
>
0
suc
h
that
for
an
y
sucien
tly
large
n
f
(n)

c

g
(n).

f
(n)
=
o(g
(n))
means
that
for
an
y
c
exists
N
suc
h
that
for
all
n
>
N
it
holds
f
(n)
<
c

g
(n).
Theorem
.
D
T
ime

(O
(n))
is
not
c
ontaine
d
in
D
T
ime

(o(n

)).
W
e
note
that
it's
m
uc
h
harder
to
pro
v
e
that
some
things
are
"imp
ossible"
or
"can
not
b
e
done",
than
the
opp
osite,
b
ecause
for
the
latter,
constructiv
e
pro
ofs
can
b
e
used.
There
are
sev
eral
p
ossible
w
a
ys
to
pro
v
e
the
theorem.
The
follo
wing
one
uses
the
notion
of
comm
unication
complexit
y
.
Pro
of:
Dene
language
L
=
fxx
:
x

f0;
g

g.
This
language
L
is
clearly
in
D
T
ime

(O
(n)).
W
e
will
sho
w
that
L
=

D
T
ime

(o(n

))
b
y
\reduction"
to
a
comm
unication
complexit
y
problem.
In
tro
duce,
for
the
sak
e
of
pro
of,
computational
complexit
y
mo
del:
t
w
o
parties
A
and
B
ha
v
e
t
w
o
strings,
A
has


f0;
g
n
and
B
has


f0;
g
n
,
resp
ectiv
ely
.
Their
goal
is
to
calculate
f
(;

);
where
f
is
a
function
from
f0;
g
n

f0;
g
n
to
f0;
g.
A
t
the
end
of
computation
b
oth
parties
should
kno
w
f
(;

).
Let
us
also
in
tro
duce
a
notation
R
0
(f
)
to
b
e
the
minim
um
exp
ected
cost
of
a
randomized
proto
col,
that
computes
f
with
zero
error.
In
our
case
it
is
sucien
t
to
consider
Equalit
y
(E
Q)
function
dened
b
y
E
Q(;

)
def
=

if

=

and
0
otherwise
:
W
e
state
without
pro
of
a
lo
w
er
b
ound
on
the
randomized
zero-error
comm
unication
complexit
y
of
E
Q.

..
GENERAL
TIME
COMPLEXITY
CLASSES
	
R
0
(E
Q)
=

(n);
(.)
The
lo
w
er
b
ound
can
b
e
pro
v
en
b
y
in
v
oking
the
lo
w
er
b
ound
on
the
\nondeterministic
comm
unica-
tion
complexit
y"
of
E
Q,
and
from
the
observ
ation
that
nondeterminism
is
generally
stronger
than
(zero-error!)
randomization:
In
tuitiv
ely
,
if
a
randomized
algorithm
reac
hes
the
solution
with
some
non-zero
probabilit
y
,
then
there
is
a
sequence
of
v
alues
of
ipp
ed
coins
that
causes
the
randomized
algorithm
to
reac
h
the
solution.
The
nondeterministic
v
ersion
of
the
same
algorithm
could
just
guess
correctly
all
these
coins'
v
alues
and
to
reac
h
the
solution
as
w
ell.
W
e
no
w
get
bac
k
to
the
pro
of.
Supp
ose
for
con
tradiction,
that
there
exists
a
one-tap
e
T
uring
mac
hine
M
whic
h
decides
L
in
o(n

)
time.
Then
w
e
will
build
a
zero-error
randomized
proto
col

that
solv
es
E
Q
in
exp
ected
complexit
y
o(n),
con
tradicting
Eq.
(.).
This
proto
col
,
on
input

and

eac
h
of
length
n
sim
ulates
the
T
uring
mac
hine
on
input
0
n

0
n
in
the
follo
wing
w
a
y
.
They
output

or
0
dep
ending
on
whether
the
mac
hine
accepts
or
rejects
this
input.
They
rst
c
ho
ose
together,
uniformly
at
random,
a
lo
cation
at
the
rst
0-region
of
the
tap
e.
The
part
y
A
sim
ulates
the
mac
hine
whenev
er
the
head
is
to
the
left
of
this
lo
cation,
and
the
part
y
B
whenev
er
the
head
is
to
the
righ
t
of
this
lo
cation.
Eac
h
time
the
head
crosses
this
lo
cation
only
the
state
of
the
nite
con
trol
(O
()
bits)
need
to
b
e
sen
t.
If
the
total
running
time
of
the
mac
hine
is
o(n

),
then
the
exp
ected
n
um
b
er
of
times
it
crosses
this
lo
cation
(whic
h
has
b
een
c
hosen
at
random
among
n
dieren
t
p
ossibilities)
is
at
most
o(n),
con
tradicting
Eq.
(.).
Therefore,
w
e
ha
v
e
pro
v
ed
that
a
one-tap
e
T
uring
mac
hine
whic
h
decides
L
runs

(n

)
time.
An
alternativ
e
w
a
y
of
pro
ving
the
theorem,
a
direct
pro
of,
based
on
the
notion
of
a
crossing
sequence,
is
giv
en
in
the
App
endix.
..
Time-constructibilit
y
and
t
w
o
theorems
Denition
.
A
function
f
:
N
 !
N
is
time-c
onstructible
if
ther
e
exists
an
algorithm
A
st.
on
input

n
,
A
runs
at
most
f
(n)
steps
and
outputs
f
(n)
(in
binary).
One
motiv
ation
to
the
denition
of
time-constructible
function
is
the
follo
wing:
if
the
mac
hine's
running
time
is
this
sp
ecic
function
of
input
length,
then
w
e
can
calculate
the
running
time
within
the
time
required
to
p
erform
the
whole
calculation.
This
notion
is
imp
ortan
t
for
sim
ulating
results,
when
w
e
w
an
t
to
"ecien
tly"
run
all
mac
hines
whic
h
ha
v
e
time
b
ound
t().
W
e
cannot
en
umerate
these
mac
hines.
Instead
w
e
en
umerate
all
mac
hines
and
run
eac
h
with
time
b
ound
t().
Th
us,
w
e
need
to
b
e
able
to
compute
t()
within
time
t().
Otherwise,
just
computing
t()
will
tak
e
to
o
m
uc
h
time.
F
or
example,
n

,

n
,
n
n
are
all
time-constructible
functions.
Time
Hierarc
h
y:
A
sp
ecial
case
of
the
Time
Hierarc
h
y
Theorem
asserts
that
for
ev
ery
constan
t
c

N
,
for
an
y
i

f;
g
D
T
ime
i
(n
c
)

D
T
ime
i
(n
c+
)
(where
A

B
denotes
that
A
is
strictly
con
tained
in
B
)

0
LECTURE
.
MORE
ON
NP
AND
SOME
ON
DTIME
That
is,
in
this
case
there
is
no
"complexit
y
gaps"
and
the
set
of
problems
that
can
b
e
solv
es
gro
ws
whewn
allo
wing
more
time:
There
are
computational
tasks
that
can
b
e
done
in
O
(n
c+
),
but
can
not
b
e
done
in
O
(n
c
).
The
function
n
c+
(ab
o
v
e)
can
b
e
replaced
ev
en
b
y
n
c+


etc.
The
general
case
of
the
Time
Hierarc
h
y
Theorem
is
Theorem
.
(Time
Hierarc
h
y):
F
or
every
time-c
onstructible
functions
t

;
t

:
N
 !
N
such
that
lim
n!
t

(n)

log
t

(n)
t

(n)
=
0
then
D
T
ime
i
(t

(n))

D
T
ime
i
(t

(n)):
The
pro
of
for
an
analogous
space
hierarc
h
y
is
easier,
and
therefore
w
e'll
presen
t
it
rst,
but
in
follo
wing
lectures.
Linear
Sp
eed-up:
The
follo
wing
Line
ar
Sp
e
e
d-up
The
or
em
allo
ws
us
to
discard
constan
t
factors
in
running-time.
In
tuitiv
ely
,
there
is
no
p
oin
t
in
holding
suc
h
an
accurate
accoun
t
when
one
do
es
not
sp
ecify
other
comp
onen
ts
of
the
algorithm
(lik
e
the
size
of
its
nite
con
trol
and
w
ork-tap
e
alphab
et).
Theorem
.
(Linear
Sp
eed-up):
F
or
every
function
t
:
N
 !
N
and
for
every
i
D
T
ime
i
(t(n))

D
T
ime
i
(
t(n)

+
O
(n)):
The
pro
of
idea
is
as
follo
wing:
let
 b
e
the
original
w
ork
alphab
et.
W
e
reduce
the
time
complexit
y
b
y
a
constan
t
factor
b
y
w
orking
with
larger
alphab
et
 k
=
 
 
:::

 |
{z
}
k
times
,
whic
h
enables
us
to
pro
cess
adjacen
t
sym
b
ols
sim
ultaneously
.
Then
w
e
construct
a
new
mac
hine
with
alphab
et
 k
:
Using
this
alphab
et,
an
y
k
adjacen
t
cells
of
the
original
tap
e
are
replaced
b
y
one
cell
of
the
new
tap
e.
So
the
new
input
will
b
e
pro
cessed
almost
k
times
faster,
but
dealing
with
the
input
will
pro
duce
O
(n)
o
v
erhead.
Let
M

b
e
an
i-tap
e
t(n)
time-b
ounded
T
uring
mac
hine.
Let
L
b
e
a
language
accepted
b
y
M

.
Then
L
is
accepted
b
y
a
i-tap
e
(
t(n)

+
O
(n))
time-b
ounded
TM
M

.
Pro
of:
A
T
uring
mac
hine
M

can
b
e
constructed
to
sim
ulate
M

in
the
follo
wing
manner.
First
M

copies
the
input
on
to
a
storage
tap
e,
enco
ding

sym
b
ols
in
to
one.
F
rom
this
p
oin
t
on,
M

uses
this
storage
tap
e
as
the
input
tap
e
and
uses
the
old
input
tap
e
as
a
storage
tap
e.
M

will
enco
de
the
con
ten
ts
of
M

's
storage
tap
e
b
y
com
bining

sym
b
ols
in
to
one.
During
the
course
of
the
sim
ulation,
M

sim
ulates
a
large
n
um
b
er
of
mo
v
es
of
M

in
one
basic
step
consisting
of
eigh
t
mo
v
es
of
M

.
Call
the
cells
curren
tly
scanned
b
y
eac
h
of
M

's
heads
the
home
cells.
The
nite
con
trol
of
M

records,
for
eac
h
tap
e,
whic
h
of
the

sym
b
ols
of
M

represen
ted
b
y
eac
h
home
cell
is
scanned
b
y
the
corresp
onding
head
of
M

.
T
o
b
egin
a
basic
step,
M

mo
v
es
eac
h
head
to
the
left
once,
to
the
righ
t
t
wice,
and
to
the
left
once,
recording
the
sym
b
ols
to
the
left
and
righ
t
of
the
home
cells
in
its
nite
con
trol.
F
our
mo
v
es
of
M

are
required,
after
whic
h
M

has
returned
to
its
home
cells.
Next,
M

determines
the
con
ten
ts
of
all
of
M

's
tap
e
cells
represen
ted
b
y
the
home
cells
and
their
left
and
righ
t
neigh
b
ors
at
the
time
when
some
tap
e
head
of
M

rst
lea
v
es
the
region
represen
ted

..
GENERAL
TIME
COMPLEXITY
CLASSES

b
y
the
home
cell
and
its
left
and
righ
t
neigh
b
ors.
(Note
that
this
calculation
b
y
M

tak
es
no
time.
It
is
built
in
to
the
transition
rules
of
M

.)
If
M

accepts
b
efore
some
tap
e
head
lea
v
es
the
represen
ted
region,
M

accepts.
If
M

halts,
M

halts.
Otherwise
M

then
visits,
on
eac
h,
the
t
w
o
neigh
b
ors
of
the
home
cell,
c
hanging
these
sym
b
ols
and
that
of
the
home
cell
if
necessary
.
M

p
ositions
eac
h
of
it
heads
at
the
cell
that
represen
ts
the
sym
b
ol
that
M

's
corresp
onding
head
is
scanning
at
the
end
of
the
mo
v
es
sim
ulated.
A
t
most
four
mo
v
es
of
M

are
needed.
It
tak
es
at
least

mo
v
es
for
M

to
mo
v
e
a
head
out
of
the
region
represen
ted
b
y
a
home
cell
and
its
neigh
b
ors.
Th
us
in
eigh
t
mo
v
es,
M

has
sim
ulated
at
least

mo
v
es
of
M

.
Bibliographic
Notes
F
or
a
detailed
pro
of
of
Ladner's
Theorem,
the
reader
is
referred
to
the
original
pap
er
[].
The
existence
of
an
optimal
algorithm
for
an
y
NP-problem
(referred
to
ab
o
v
e
as
Levin's
Theorem)
w
as
pro
v
en
in
[].
The
separtion
of
one-tap
e
T
uring
mac
hines
from
t
w
o-tap
e
ones
(i.e.,
DTime

(O
(n))
is
not
con
tained
in
DTime

(o(n

)))
can
b
e
pro
v
ed
in
v
arious
w
a
ys.
Our
presen
tation
follo
ws
the
t
w
o-step
pro
of
in
[],
while
omitting
the
second
step
(i.e.,
pro
ving
a
lo
w
er
b
ound
on
the
error-free
randomized
comm
unication
complexit
y
of
equality).
The
alternativ
e
(direct)
pro
of,
presen
ted
in
the
app
endix
to
this
lecture,
is
adapted
from
Exercise
.
(for
whic
h
a
solution
is
giv
en)
in
the
textb
o
ok
[].
.
J.E.
Hop
croft
and
J.D.
Ullman,
Intr
o
duction
to
A
utomata
The
ory,
L
anguages
and
Computa-
tion,
Addison-W
esley
,
		.
.
E.
Kushilevitz
and
N.
Nisan.
Communic
ation
Complexity,
Cam
bridge
Univ
ersit
y
Press,
		.
.
R.E.
Ladner,
\On
the
Structure
of
P
olynomial
Time
Reducibilit
y",
Jour.
of
the
A
CM,
,
	,
pp.
{.
.
Levin,
L.A.,
\Univ
ersal
Searc
h
Problems",
Pr
oblemy
Per
e
daci
Informacii
	,
pp.
{,
	.
T
ranslated
in
pr
oblems
of
Information
T
r
ansmission
	,
pp.
{.
App
endix:
Pro
of
of
Theorem
.,
via
crossing
sequences
Consider
one-tap
e
T
M
M
with
transition
function

,
input
w
of
length
m,
suc
h
that
M
accepts
w
and
an
in
teger
i,
0
<
i
<
m.
Denote
b
y
w
j
,
0

j

m,
the
j
'th
sym
b
ol
of
the
w
ord
w
.
Consider
the
computation
of
the
mac
hine
M
on
input
w
.
This
computation
is
uniquely
deter-
mined
b
y
the
mac
hine
description
and
the
input,
since
the
mac
hine
is
deterministic.
The
computation
is
a
sequence
of
ID's
(instan
taneous
descriptions),
starting
with
q
0
w

:::w
n
and
ending
with
w

:::w
n
p
for
some
p

F
(accepting
or
rejecting
state):
Denote
the
elemen
ts
of
the
computation
sequence
b
y
(I
D
j
)
r
j
=
for
some
nite
r
.
Consider
a
sequence
(L
j
)
r
 
j
=
of
pairs
L
j
=
(I
D
j
;
I
D
j
+
).
F
or
some
0

i

m
let
(L
j
l
)
t
l
=
t

r
 
b
e
the
subsequence
of
(L
j
)
r
 
j
=
of
elemen
ts
(I
D
j
l
;
I
D
j
l
+
)
of
the
form:
either
I
D
j
l
=
w

:::w
i 
pw
i
:::w
n
and
I
D
j
l
+
=
w

:::w
i
q
w
i+
:::w
n
;
for
some
p;
q

Q
and
this
sp
ecic
i,
or
I
D
j
l
=
w

:::w
i
pw
i+
:::w
n
and
I
D
j
l
+
=
w

:::w
i 
q
w
i
:::w
n
,
for
some
p;
q

Q
and
this
sp
ecic
i.


LECTURE
.
MORE
ON
NP
AND
SOME
ON
DTIME
By
denition
of
T
uring
mac
hine
computation
I
D
j
l
`
M
I
D
j
l
+
and
therefore
in
the
rst
case

(p;
w
i
)
=
(q
;
w
i+
;
R
)
and
in
the
second
case

(p;
w
i+
)
=
(q
;
w
i
;
L).
F
or
ev
ery


l

t,
let
q
l
b
e
the
state
recorded
in
I
D
j
l+
,
where
I
D
j
l+
are
as
ab
o
v
e.
Then
the
sequence
(q
l
)
t
l
=
is
dened
to
b
e
the
crossing
sequence
of
the
triple
(M
;
w
;
i)
(mac
hine
M
on
input
w
in
the
b
oundary
i).
Consider
L
=
fw
cw
:
w

f0;
g

g;
where
c
is
a
sp
ecial
sym
b
ol.
Clearly
,
a
-tap
e
T
M
will
decide
the
language
in
O
(n)
just
b
y
cop
ying
all
the
sym
b
ols
b
efore
c
to
another
tap
e
and
comparing
sym
b
ol
b
y
sym
b
ol.
Let's
pro
v
e
that
-tap
e
T
M
will
need

(n

)
steps
to
decide
this
language.
F
or
w

f0;
g
m
;
and


i

m
 ;
let
l
w
;i
b
e
the
length
of
the
crossing
sequence
of
(M
;
w
cw
;
i).
Denote
b
y
s
the
n
um
b
er
of
states
of
M
.
Denote
the
a
v
erage
of
l
w
;i
o
v
er
all
m-long
w
ords
w
b
y
p(i).
Then
from
coun
ting
considerations,
at
least
for
half
of
w
's
it
holds
l
w
;i



p(i).
Let
N
=

m
.So
there
are
at
least

m 
w
ords
w
for
whic
h
holds
l
w
;i



p(i).
The
n
um
b
er
of
p
ossible
crossing
sequences
of
length



p(i)
is
p(i)
X
j
=0
s
j
<
s
p(i)+
;
where
s
is
the
n
um
b
er
of
states
of
M
.
So
there
is
at
least

m 
s
p(i)+
w
ords
w
of
length
m
with
the
same
crossing
sequence
for
b
oundary
(i;
i
+
)
(b
y
pigeonhole
principle).
W
e
are
in
terested
in
suc
h
w
ords
w
with
the
same
sux
(i
+
;
:::;
m
sym
b
ols).
The
n
um
b
er
of
suc
h
dieren
t
suxes
is

m i
.
Therefore
if
for
some
i
holds

m 
s
p(i)+
=
m i
>
;
(.)
then
b
y
pigeonhole
principle
there
are
t
w
o
dieren
t
w
's
with
the
same
sux
and
the
same
crossing
sequence
b
et
w
een
i
and
i
+

p
ositions.
W
e'll
sho
w
that
this
leads
to
con
tradiction.
Denote
the
diering
i-prexes
b
y


and


and
the
common
(m
 i)-bit
sux
b
y

.
Consider
the
input
w
ord



c


and
the
input
w
ord



c


.
Since
the
crossing
sequence
b
et
w
een


and

is
the
same
as
the
one
b
et
w
een


and

,
the
mac
hine
will
not
b
e
able
to
distinguish
b
et
w
een
the
t
w
o
cases,
and
will
accept
the
second
w
ord
to
o,
con
tradiction.
So
Eq.
(.)
can
not
hold
for
an
y
i
and
therefore
for
ev
ery
i
it
holds

m 

p(i)+

m i


and
so

i 


p(i)+
;
implying
i
 



p(i)
+
;
and

..
GENERAL
TIME
COMPLEXITY
CLASSES

p(i)

i
 

(.)
follo
ws.
Denote
b
y
T
m
(w
)
the
time
needed
to
mac
hine
M
to
accept
the
w
ord
w
cw
.
Let
us
compute
the
a
v
erage
time
Av
M
(w
)
needed
for
M
to
accept
w
cw
for
an
m-long
w
ord
w
.
Av
M
(w
)
=

N
X
w
T
M
(w
)


N
X
w
m
X
i=
l
w
;i
;
b
ecause
the
running
time
of
a
TM
on
input
w
is
the
sum
of
lengths
of
the
crossing
sequences
of
(M
;
w
;
i)
for
0
<
i
<
m.
W
e
ha
v
e
inequalit
y
here
since
there
are
crossing
sequences
for
i
>
m
in
the
w
ord
w
cw
.
No
w,
w
e
ha
v
e

N
X
w
m
X
i=
l
w
;i
=
m
X
i=
X
w
l
w
;i
N
=
m
X
i=
p(i)
and
so,
b
y
Eq.
(.),
Av
M
(w
)

m
X
i=
p(i)

m
X
i=
i
 

=

(m

):
So
the
a
v
erage
running
time
of
M
on
w
cw
is

(m

)
implying
that
there
exists
an
input
w
cw
of
length


m
+

on
whic
h
M
runs

(m

)
steps.
Therefore,
w
e
ha
v
e
pro
v
ed
a
lo
w
er
b
ound
of
the
w
orst
case
complexit
y
of
the
language-decision
problem
for
L
=
fw
cw
:
w

f0;
g

g
on
one-tap
e
T
uring
mac
hine.
This
lo
w
er
b
ound
is

(m

)
for
O
(m)-long
input.
On
the
other
hand,
this
problem
is
decidable
in
O
(m)
time
b
y
t
w
o-tap
e
T
M
.
Therefore
D
T
ime

(O
(n))

D
T
ime

(o(n

)):
And
so
Theorem

is
tigh
t
in
the
sense
that
there
are
functions
t()
suc
h
that
D
T
ime

(O
(t()))

D
T
ime

(o(t

())):


LECTURE
.
MORE
ON
NP
AND
SOME
ON
DTIME

Lecture

Space
Complexit
y
Notes
tak
en
b
y
Leia
P
assoni
and
Reub
en
Sumner
Summary:
In
this
lecture
w
e
in
tro
duce
space
complexit
y
and
discuss
ho
w
a
prop
er
complexit
y
function
should
b
eha
v
e.
W
e
see
that
prop
erly
c
ho
osing
complexit
y
functions
yields
as
a
result
w
ell-b
eha
v
ed
hierarc
hies
of
complexit
y
classes.
W
e
also
discuss
space
complexit
y
b
elo
w
logarithm.
.
On
Dening
Complexit
y
Classes
So
far
t
w
o
main
complexit
y
classes
ha
v
e
b
een
considered:
N
P
and
P
.
W
e
no
w
consider
general
complexit
y
measures.
In
order
to
sp
ecify
a
complexit
y
class,
w
e
rst
ha
v
e
to
set
the
mo
del
of
c
omputation
w
e
are
going
to
use,
the
sp
ecic
r
esour
c
e
w
e
w
an
t
to
b
ound
{
time
or
space
{
and
nally
the
b
ound
itself,
that
is
the
function
with
resp
ect
to
whic
h
w
e
w
an
t
complexit
y
to
b
e
measured.
What
kind
of
functions
f
:
N
!
N
should
b
e
considered
appropriate
in
order
to
dene
\ade-
quate"
complexit
y
classes?
Suc
h
functions
should
b
e
computable
within
a
certain
amoun
t
of
the
resource
they
b
ound,
and
that
amoun
t
has
to
b
e
a
v
alue
of
the
function
itself.
In
fact,
c
ho
osing
a
to
o
complicated
function
as
a
complexit
y
function
could
giv
e
as
a
result
that
the
function
itself
is
not
computable
within
the
amoun
t
of
time
or
space
it
p
ermits.
These
functions
are
not
go
o
d
in
order
to
understand
and
classify
usual
computational
problems:
ev
en
though
w
e
can
use
any
suc
h
function
in
order
do
formally
dene
its
complexit
y
class,
strange
things
can
happ
en
b
et
w
een
complexit
y
classes
if
w
e
don't
c
ho
ose
these
functions
prop
erly
.
This
is
the
reason
wh
y
w
e
ha
v
e
dened
time
c
onstructible
functions
when
dealing
with
time
complexit
y
.
F
or
the
same
reason
w
e
will
here
dene
sp
ac
e
c
onstructible
functions.
.
Space
Complexit
y
In
space
complexit
y
w
e
are
concerned
with
the
amoun
t
of
space
that
is
needed
for
a
computation.
The
mo
del
of
computation
w
e
will
use
is
a
-tap
e
T
uring
Machine.
W
e
use
this
mo
del
b
ecause
it
is
easier
to
deal
with
it.
W
e
remind
that
an
y
m
ulti-tap
e
TM
can
b
e
sim
ulated
b
y
an
ordinary
TM
with
a
loss
of
eciency
that
is
only
p
olynomial.
F
or
the
reminder
of
this
lecture
notes,
\
T
uring
Mac
hine"
will
refer
to
a
-tap
e
T
uring
Mac
hine.
The

tap
es
are:
.
input
tap
e.
Read-only



LECTURE
.
SP
A
CE
COMPLEXITY
.
output
tap
e.
W
rite-only
.
Usually
considered
unidirectional:
this
assumption
is
not
essen-
tial
but
useful.
F
or
decision
problems,
as
considered
b
elo
w,
one
can
omit
the
output-tap
e
altogether
and
ha
v
e
the
decision
in
the
mac
hine's
state.
.
work
tap
e.
Read
and
write.
Space
complexit
y
is
measured
b
y
the
b
ounds
on
the
mac
hine's
p
osition
on
this
tap
e.
W
riting
is
not
allo
w
ed
on
the
input
tap
e:
this
w
a
y
space
is
measured
only
on
the
w
orktap
e.
If
w
e
allo
w
ed
writing
on
the
input
tap
e
then
the
length
of
the
input
itself
should
b
e
tak
en
in
to
accoun
t
when
measuring
space.
Th
us
w
e
could
only
measure
space
complexities
whic
h
are
at
least
linear.
In
order
to
consider
also
sublinear
space
b
ounds
w
e
restrict
the
input
tap
e
to
b
e
read-only
.
Dene
W
M
(x)
to
b
e
the
index
of
the
righ
tmost
cell
on
the
w
orktap
e
scanned
b
y
M
on
input
x.
Dene
S
M
(n)
=
max
jxj=n
W
M
(x).
F
or
an
y
language
L
dene

L
(x)
so
that
if
x

L
then

L
(x)
=

otherwise

L
(x)
=
0
Denition
.
(Dspace):
Dspace
(s(n))
=
f
L
j	a
T
uring
mac
hine
M
;
M
(x)
=

L
(x)and
n
S
M
(n)

s(n)
g
W
e
ma
y
m
ultiply
s()
b
y
log

j M
j
where
 M
is
the
alphab
et
used
b
y
M
.
Otherwise,
w
e
could
alw
a
ys
linearly
compress
the
n
um
b
er
of
space
cells
using
a
bigger
alphab
et.
W
e
ma
y
also
add
log

(jxj)
to
s(),
where
x
is
the
input.
(Ho
w
ev
er,
this
con
v
en
tion
disallo
w
treatmen
t
of
sub-logarithmic
space,
and
therefore
will
not
b
e
done
when
discussing
suc
h
space
b
ounds.)
This
is
done
in
order
to
ha
v
e
a
corresp
ondence
to
the
n
um
b
er
of
congurations.
Denition
.
(Conguration)
:
A
conguration
of
M
is
an
instantane
ous
r
epr
esentation
of
the
c
omputation
c
arrie
d
on
by
M
on
a
given
input
x.
Ther
efor
e
if
jxj
=
n
a
c
ongur
ation
gives
information
ab
out
the
fol
lowing:

state
of
M
(O()
bits)

c
ontents
of
the
work
tap
e
(s(n)
bits)

he
ad
p
osition
in
the
input
tap
e
(log
(n)
bits)

he
ad
p
osition
in
the
work
tap
e
(log
(s(n))
bits)
.
Sub-Logarithmic
Space
Complexit
y
W
orking
with
sublogarithmic
space
is
not
so
useful.
One
ma
y
b
e
tempted
to
think
that
whatev
er
can
b
e
done
in
o(log
(n))
space
can
also
b
e
done
in
constan
t
space.
F
ormally
this
w
ould
mean
Dspace
(o(log
(n)))

Dspace
(O
())
and
since
ob
viously
Dspace
(O
())

Dspace
(o(log
(n))),
w
e
ma
y
also
(incorrectly)
argue
that
in
fact
Dspace
(o(log
(n)))
=
Dspace
(O
())
This
in
tuition
comes
from
the
follo
wing
imprecise
observ
ation:
if
space
is
not
constan
t,
mac
hine
M
m
ust
determine
ho
w
m
uc
h
space
to
use.
Determining
ho
w
m
uc
h
space
to
use
seems
to
require
the
mac
hine
coun
ting
up
to
at
least
jxj
=
n
whic
h
needs
O
(log
(n))
space.
Therefore
an
y
M
that
uses
less
than
O
(log
(n))
cells,
is
forced
to
use
constan
t
space.
It
turns
out
that
this
in
tuition
is
wrong
and
the
reason
is
that
the
language
itself
can
help
in
deciding
ho
w
m
uc
h
space
to
use.

..
SUB-LOGARITHMIC
SP
A
CE
COMPLEXITY

Oded's
Note:
This
should
serv
e
as
w
arning
against
making
statemen
ts
based
on
v
ague
in
tuitions
on
ho
w
a
\reasonable"
algorithm
should
b
eha
v
e.
In
general,
trying
to
mak
e
claims
ab
out
\reasonable"
algorithms
is
a
v
ery
dangerous
attitude
to
pro
ving
lo
w
er
b
ounds
and
imp
ossibilit
y
results.
It
is
rarely
useful
and
quite
often
misleading.
Note:
It
is
kno
wn
that
Dspace
(O
())
equal
the
set
of
regular
languages.
This
fact
will
b
e
used
to
pro
v
e
the
follo
wing
Theorem
.
Dspace
(o(log
(n)))
is
a
pr
op
er
sup
erset
of
Dspace
(O
()).
Pro
of:
W
e
will
sho
w
that
Dspace
(o(log
(n)))

Dspace
(log
log
(n))
is
not
con
tained
in
Dspace
(O
()).
In
fact
there
is
a
language
L
so
that
L

Dspace
(log
log
(n))
but
L
=

Dspace
(O
()).
F
or
simplicit
y
,
w
e
dene
a
language,
L,
o
v
er
the
alphab
et
f0;
;
$g:
L
=

>
<
>
:
w
=
0



0$0



0$0



00$



$



$







k

N
the
l
-th
substring
of
w
delimited
b
y
$
has
length
k
and
is
the
binary
represen
tation
of
the
n
um
b
er
l
 ,
where
0

l
<

k
	
>
=
>
;
It
can
b
e
easily
sho
wn
that
L
is
not
regular
using
standard
pumping
lemma
tec
hniques.
W
e
then
pro
v
e
that
L

Dspace
(log
log
(n)).
Note
that
L
=
fx
k
:
k

Ng,
where
x
k
=
0
k
 
$0
k
 
0$0
k
 
0$0
k
 
$
:
:
:
$
k
$
First
consider
a
simplied
case
where
w
e
only
measure
space
when
in
fact
x
=
x
k

L;
jx
k
j
=
(k
+
)
k
,
but
w
e
need
to
c
hec
k
if
x

L.
W
e
ha
v
e
to
.
Chec
k
the
rst
blo
c
k
is
all
0's
and
the
last
blo
c
k
is
all
's
.
F
or
an
y
t
w
o
consecutiv
e
in
termediate
blo
c
ks
in
x
k
,
c
hec
k
that
the
second
is
the
binary
incre-
men
t
b
y

of
the
rst
one.
Step
()
can
b
e
done
in
constan
t
space.
In
Step
()
w
e
coun
t
the
n
um
b
er
of
's
in
the
rst
blo
c
k,
starting
from
the
righ
t
delimiter
$
and
going
left
un
til
w
e
reac
h
the
rst
0.
If
the
n
um
b
er
of
's
in
the
rst
blo
c
k
is
i,
w
e
then
c
hec
k
that
in
the
second
blo
c
k
there
are
exactly
i
0's
follo
w
ed
b
y
.
Then
w
e
c
hec
k
the
remaining
k
 i
 
digits
in
the
t
w
o
consecutiv
e
blo
c
ks
are
the
same.
On
input
x
k
,
step

can
b
e
done
in
O
(log
(k
))
space,
whic
h
in
terms
of
n
=
jx
k
j
=
(k
+
)
k
,
means
O
(log
log
(n))
space.
Handling
the
case
where
x
=

L
while
still
using
space
O
(log
log
(n))
is
sligh
tly
tric
kier.
If
w
e
only
pro
ceeded
as
ab
o
v
e
then
w
e
migh
t
b
e
tric
k
ed
b
y
an
input
of
the
form
\0
n
$"
in
to
using
space
O
(log
(n)).
W
e
think
of
x
b
eing
\parsed"
in
to
blo
c
ks
separated
b
y
$,
doing
this
requires
only
constan
t
space.
W
e
a
v
oid
using
to
o
m
uc
h
space
b
y
making
k
passes
on
the
input.
On
the
rst
pass
w
e
mak
e
sure
that
the
last
bit
of
ev
ery
blo
c
k
is
0
then

then
0
and
so
on.
On
the
second
pass
w
e
mak
e
sure
that
the
last
t
w
o
bits
of
ev
ery
blo
c
k
are
00
then
0
then
0
then

and
then
bac
k
to
00
and
so
on.
In
general
on
the
i
th
pass
w
e
c
hec
k
that
the
last
i
bits
of
eac
h
blo
c
k
form
an
increasing
sequence
mo
dulo

i
.
If
w
e
ev
er
detect
consecutiv
e
blo
c
ks
of
dieren
t
length
then
w
e
reject.
Otherwise,
w
e
accept
if
in
some
(i.e.,
i
th
)
pass,
the
rst
blo
c
k
is
of
length
i,
and
the
en
tire
sequence
is
increasing
mo
d

i
.
This
m
ulti-pass
approac
h,
while
requiring
more
time,
is
guaran
teed
nev
er
to
use
to
o
m
uc
h
space.
Sp
ecically
,
on
an
y
input
x,
w
e
use
space
O
(
+
log
i),
where
i
=
O
(log
jxj)
is
the
index
of
the
last
pass
p
erformed
b
efore
termination.


LECTURE
.
SP
A
CE
COMPLEXITY
Going
further
on,
w
e
can
consider
Dspace
(o(log
log
(n))
and
Dspace
(O
()).
W
e
will
sho
w
that
these
t
w
o
complexit
y
classes
are
equiv
alen
t.
The
kind
of
argumen
t
used
to
pro
v
e
their
equiv
alence
extends
the
one
used
to
pro
v
e
the
follo
wing
simpler
fact.
Theorem
.
F
or
any
s(n)
:
s(n)

log
(n)
Dspace
(s(n))

Dtime(
o(s(n))
)
Pro
of:
Fix
an
input
x
:
jxj
=
n
and
a
deterministic
mac
hine
M
that
accepts
x
in
space
s(n).
Let
b
e
C
the
n
um
b
er
of
p
ossible
congurations
of
M
on
input
x.
Then
an
upp
er
b
ound
for
C
is:
C

jQ
M
j

n

s(n)


o(s(n))
where
Q
M
is
the
set
of
states
of
M
,
n
is
the
n
um
b
er
of
p
ossible
lo
cations
of
the
head
on
the
input
tap
e,
s(n)
is
the
n
um
b
er
of
p
ossible
lo
cations
of
the
head
on
the
w
orktap
e
and

o(s)
is
the
n
um
b
er
of
p
ossible
con
ten
ts
in
the
w
ork
tap
e
{
the
exp
onen
t
is
o(s)
b
ecause
the
alphab
et
is
not
necessarily
binary
.
W
e
can
write
s(n)


o(s(n))
=

o(s(n))
and
since
s
is
at
least
logarithmic
,
n


o(s(n))
.
Therefore
C


o(s(n))
M
cannot
run
on
input
x
for
a
time
t(n)
>

s(n)
.
Otherwise,
M
will
go
through
the
same
cong-
uration
at
least
t
wice,
en
tering
an
innite
lo
op
and
nev
er
stop.
Then
necessarily
M
has
to
run
in
time
t(n)


o(s)
.
Theorem
.
Dspace
(o(log

log

(n))
=
Dspace
(O
())
Pro
of:
Consider
a
s()-b
ounded
mac
hine
M
on
the
alphab
et
f0;
g.
Claim:
giv
en
input
x
:
jxj
=
n
suc
h
that
M
accepts
x,
then
M
can
b
e
on
ev
ery
cell
on
the
input
tap
e
at
most
k
=

s(n)

s(n)

jQ
M
j
=
O


s(n)

times.
The
reason
b
eing
that
if
M
w
ere
to
b
e
on
the
cell
more
than
k
times
then
it
w
ould
b
e
in
the
same
conguration
t
wice,
and
th
us
nev
er
terminate.
W
e
dene
a
semi-conguration
as
a
conguration
with
the
p
osition
on
the
input
tap
e
replaced
b
y
the
sym
b
ol
at
the
curren
t
input
tap
e
p
osition.
F
or
ev
ery
lo
cation
i
on
the
input
tap
e,
w
e
consider
all
p
ossible
semi-congurations
of
M
when
passing
lo
cation
i.
If
the
sequence
of
suc
h
congurations
is
C
i
=
C
i

;
:
:
:
;
C
i
r
then
b
y
the
ab
o
v
e
claim
its
length
is
b
ounded:
r

O


s(n)

.
The
n
um
b
er
of
p
ossible
dieren
t
sequences
of
semi-congurations
of
M
,
asso
ciated
with
an
y
p
osition
on
the
input
tap
e,
is
b
ounded
b
y


s(n)

(

s(n)
)
=


O
(s(n))
Since
s(n)
=
o(log

log

n)
then


O
(s(n))
=
o(n)
and
therefore
there
exists
n
0

N
suc
h
that
n

n
0
,


O
(s(n))
<
n

.
W
e
then
sho
w
that
n

n
0
,
s(n)
=
s(n
0
).
Th
us
L

Dspace
(s(n
0
))
=
Dspace
(O
())
pro
ving
the
theorem.
Assume
to
the
con
trary
that
there
exists
an
n
0
suc
h
that
s(n
0
)
>
s(n
0
).
Let
n

=
min
jxj>n
0
fW
M
(x)
>
s(n
0
)g
and
let
x


f0;
g
n

b
e
suc
h
that
W
M
(x

)
>
s(n
0
).
That
is,
x

is
the
shortest
input
on
whic
h
M
uses
space
more
than
s(n
0
).
The
n
um
b
er
of
sequences
of
semi-congurations
at
an
y
p
osition
in
the
input
tap
e
is
<
n


.
So
lab
elling
n

p
ositions
on
the
input
tap
e
b
y
at
most
n


sequences
means
that
there
m
ust
b
e
at
least
three
p
ositions
with
the
same
sequence
of
semi-congurations.
Sa
y
x

=
a
a
a
.
Where
eac
h
of
the
p
ositions
with
sym
b
ol
a
has
the
same
sequence
of
semi-congurations
attac
hed
to
it.

..
HIERAR
CHY
THEOREMS
	
Claim:
The
mac
hine
pro
duces
the
same
nal
semi-conguration
with
either

a
or

a
eliminated
from
the
input.
F
or
the
sak
e
of
argumen
t
consider
cutting

lea
ving
x
0

=
a
a
.
On
x
0

the
mac
hine
pro
ceeds
on
the
input
exactly
as
with
x

un
til
it
rst
reac
hes
the
a.
This
is
the
rst
en
try
in
our
sequence
of
semi-congurations.
Lo
cally
,
M
will
mak
e
the
same
decision
to
go
left
or
righ
t
on
x
0

as
it
did
on
x

since
all
information
stored
in
the
mac
hine
at
at
the
curren
t
read
head
p
osition
is
iden
tical.
If
the
mac
hine
go
es
left
then
its
computation
will
pro
ceed
iden
tically
on
x
0

as
on
x

b
ecause
it
still
hasn't
seen
an
y
dierence
in
input
and
will
either
terminate
or
once
again
come
to
the
rst
a.
On
the
other
hand
consider
the
case
of
the
mac
hine
going
righ
t.
Sa
y
this
is
the
ith
time
at
the
rst
a.
W
e
no
w
compare
the
computation
of
M
to
what
it
did
follo
wing
the
ith
time
going
past
the
second
a
(after
the
no
w
nonexisten
t

).
Since
the
semi-conguration
is
the
same
in
b
oth
cases
then
on
input
x

the
mac
hine
M
also
w
en
t
righ
t
on
the
ith
time
seeing
the
second
a.
The
mac
hine
pro
ceeded
and
either
terminated
or
came
bac
k
for
the
i
+
st
time
to
the
second
a.
In
either
case
on
input
x
0

the
mac
hine
M
is
going
to
do
the
same
thing
but
no
w
on
the
rst
a.
Con
tin
uing
this
argumen
t
as
w
e
pro
ceed
through
the
sequence
of
semi-congurations
(arguing
eac
h
time
that
on
x
0

w
e
will
ha
v
e
the
same
sequence
of
semi-congurations)
w
e
see
that
the
nal
semi-conguration
on
x
0

will
b
e
same
as
for
x

.
The
case
in
whic
h

a
is
eliminated
is
iden
tical.
No
w
consider
the
space
usage
of
M
on
x

.
Let
x

=
a
a
and
x

=
a
a
.
If
p
eak
space
usage
pro
cessing
x

w
as
in
a
or

then
W
M
(x

)
=
W
M
(x

)
=
W
M
(x

).
If
p
eak
space
usage
w
as
in

a
then
W
M
(x

)

W
M
(x

)
=
W
M
(x

).
If
p
eak
space
usage
w
as
in

a
then
W
M
(x

)

W
M
(x

)
=
W
M
(x

).
Cho
ose
x
0


fx

;
x

g
to
maximize
W
M
(x
0

).
Then
W
M
(x
0

)
=
W
M
(x

)
and
jx
0

j
<
jx

j.
This
con
tradicts
our
assumption
that
x

w
as
a
minimal
length
string
that
used
more
than
s(n
0
)
space.
Therefore
no
suc
h
x

exists.
Discussion:
Note
that
the
pro
of
of
Theorem
.
actually
establishes
Dspace
(O
(log
log
n))
=
Dspace
(O
()).
Th
us,
com
bined
with
Theorem
.
w
e
ha
v
e
a
separation
b
et
w
een
Dspace
(O
(log
log
n))
and
Dspace
(o(log
log
n)).
The
rest
of
our
treatmen
t
fo
cuses
on
space
complexit
y
classes
with
space
b
ound
whic
h
is
at
least
logarithmic.
Theorem
.
sa
ys
that
w
e
can
really
dismiss
space
b
ounds
b
elo
w
double-logarithmic
(alas
Theorem
.
sa
ys
there
are
some
things
b
ey
ond
nite-automata
that
one
ma
y
do
with
sub-
logarithmic
space).
.
Hierarc
h
y
Theorems
As
w
e
did
for
time,
w
e
giv
e
no
w
Denition
.
(Space
Constructible
F
unction):
A
space
constructible
function
is
a
function
s
:
N
!
N
for
which
ther
e
exists
a
machine
M
of
sp
ac
e
c
omplexity
at
most
s()
such
that
n
M
(
n
)
=
s(n)
F
or
sak
e
of
simplicit
y
,
w
e
consider
only
mac
hines
whic
h
halt
on
ev
ery
input.
Little
generalit
y
is
lost
b
y
this
{
Lemma
..
F
or
any
sp
ac
e
b
ounde
d
T
uring
Machine
M
using
sp
ac
e
s(n),
wher
e
s(n)
is
at
le
ast
log
(n)
and
sp
ac
e
c
onstructible
we
c
an
c
onstruct
M
0

Dspace
(O
(s(n)))
such
that
L(M
0
)
=
L(M
)
and
machine
M
0
halts
on
al
l
inputs.
Pro
of:
Mac
hine
M
0
rst
calculates
a
time
b
ound
equal
to
the
n
um
b
er
of
p
ossible
congurations
of
M
whic
h
is

s(n)

s(n)

n

jQ
M
j.
This
tak
es
space
s(n),
and
same
holds
for
the
coun
ter
to
b
e

0
LECTURE
.
SP
A
CE
COMPLEXITY
main
tained
in
the
sequel.
No
w
w
e
sim
ulate
the
computation
of
M
on
input
x
and
c
hec
k
at
ev
ery
step
that
w
e
ha
v
e
not
exceeded
the
calculated
time
b
ound.
If
the
sim
ulated
mac
hine
halts
b
efore
reac
hing
its
time
b
ound
w
e
accept
or
reject
reecting
the
decision
of
the
sim
ulated
mac
hine.
If
w
e
reac
h
the
time
b
ound
b
efore
the
sim
ulated
mac
hine
terminates
that
w
e
are
assured
that
the
sim
ulated
mac
hine
will
nev
er
terminate,
in
particular
nev
er
accept,
and
w
e
reject
the
input.
Theorem
.
(Space
Hierarc
h
y
Theorem):
F
or
any
sp
ac
e-c
onstructible
s

:
N
!
N
and
every
at
le
ast
lo
garithmic
function
s

:
N
!
N
so
that
s

(n)
=
o(s

(n)),
the
class
Dspace
(s

(n))
is
strictly
c
ontaine
d
in
Dspace
(s

(n)).
W
e
pro
v
e
the
theorem
only
for
mac
hines
whic
h
halt
on
ev
ery
input.
By
the
ab
o
v
e
lemma,
this
do
es
not
restrict
the
result
in
case
s

is
space-constructible.
Alternativ
ely
,
the
argumen
t
can
b
e
extended
to
deal
also
with
non-halting
mac
hines.
Pro
of:
The
idea
is
to
construct
a
language
L
in
Dspace
(s

(n))
suc
h
that
an
y
mac
hine
M
using
space
s

will
fail
recognizing
L.
W
e
will
en
umerate
all
mac
hines
running
in
space
s

and
w
e
will
use
a
diagonalization
tec
hnique.

Compute
the
allo
w
ed
b
ound
on
input
x:
for
instance
let
it
b
e

0
s

(jxj).

W
rite
the
language:
L
=

>
<
>
:
x

f0;
g








x
is
of
the
form
hM
i0

and
suc
h
that
-
jhM
ij
<

0
s

(jxj)
-
on
input
x;
M
rejects
x
while
using
at
most
space

0
s

(jxj)
	
>
=
>
;
Here
hM
i
is
a
binary
enco
ding
of
the
mac
hine
M
,
so
w
e
can
see
x

L
as
a
description
of
M
itself.

Sho
w
that
L

Dspace
(s

(n))
and
L
=

Dspace
(s

(n)).
T
o
see
that
L

Dspace
(s

(n)),
w
e
write
an
algorithm
that
recognizes
L:
On
input
x:
.
Chec
k
if
x
is
of
the
righ
t
form
.
Compute
the
space
b
ound
S
 

0
s

(jxj)
.
Chec
k
the
length
of
hM
i
is
correct:
jhM
ij
<

0
s

(jxj).
.
Em
ulate
the
computation
of
mac
hine
M
on
input
x.
If
M
exceeds
the
space
b
ound
then
x
=

L
so
w
e
reject.
.
If
M
rejects
x
then
accept.
Else,
reject.
The
computation
in
Step
()
can
b
e
done
in
O
()
space.
The
computation
of
S
in
Step
()
can
b
e
done
in
space
s

(jxj)
b
ecause
s

is
space
constructible.
Step
()
needs
log
(S
)
space.
In
Step()
w
e
ha
v
e
to
mak
e
sure
that
(#
cells
M
scans)

(log

j M
j)
<
S:
Chec
king
that
M
do
es
not
exceed
the
space
b
ound
needs
space
S
.
As
for
the
implemen
tation
of
Step(),
on
the
w
ork
tap
e
w
e
rst
cop
y
the
description
hM
i
and
then
mark
a
sp
ecic
area
in
whic
h
w
e
are
allo
w
ed
to
op
erate.
Then
it
is
p
ossible
to
em
ulate
the
b
eha
vior
of
M
going
bac
k
and
forth
on
the
w
ork
tap
e
from
hM
i
to
the

..
HIERAR
CHY
THEOREMS

sim
ulated
mac
hine's
w
ork
area,
stopping
when
w
e
are
out
of
space.
The
algorithm
then
is
running
in
Dspace
(s

(n)).
Note:
Since
w
e
w
an
t
to
measure
space,
w
e
are
not
concerned
on
ho
w
m
uc
h
time
is
\w
asted"
going
bac
k
and
forth
on
the
w
orktap
e
from
the
description
of
M
to
the
op
erativ
e
area.
No
w
w
e
ha
v
e
to
see
that
L
=

Dspace
(s

(n)).
W
e
will
sho
w
that
for
ev
ery
mac
hine
M
of
space
complexit
y
s

,
L(M
)
=
L.
There
exists
n
:
s

(n)
<

0
s

(n)
since
s

(n)
=
o(s

(n)).
W
e
then
consider
M
:
jhM
ij
<

0
s

(n)
and
see
ho
w
M
acts
on
input
x
of
the
form
x
=
hM
i0
n (jhM
ij+)
{
note
that
it
is
alw
a
ys
p
ossible
to
nd
inputs
of
the
ab
o
v
e
form
for
an
y
sucien
tly
large
n.
There
are
t
w
o
cases:
.
if
M
accepts
x,
then
(b
y
denition
of
L)
x
=

L.
.
if
M
rejects
x,
then
since
jhM
ij
<

0
s

(n)
and
M
(x)
uses
at
most
s

(jxj)
<

0
s

(jxj)
space,
x

L.
In
either
case
L(M
)
=
L.
Therefore
an
y
M
using
space
s

cannot
recognize
L.
Theorem
.
(Time
Hierarc
h
y
Theorem):
F
or
any
time-c
onstructible
t

:
N
!
N
and
every
at
le
ast
line
ar
function
t

:
N
!
N
so
that
lim
n!
t

(n)
log
(t

(n))
t

(n)
=
0,
the
class
Dtime
(t

)
is
strictly
c
ontaine
d
in
Dtime(t

).
Pro
of:
It
is
analogous
to
the
previous
one
used
for
space.
The
only
dierence
is
in
the
denition
of
the
language
L:
L
=

>
<
>
:
x

f0;
g








x
is
of
the
form
hM
i0

and
suc
h
that
-
jhM
ij
<

0
log
(t

(jxj))
-
on
input
x;
M
rejects
x
while
using
at
most
time

0
log
t

(jxj)
t

(jxj)
	
>
=
>
;
Dealing
with
time,
w
e
require
jhM
ij
<
log
(t

(jxj)).
The
reason
for
requiring
a
small
description
for
M
is
that
w
e
cannot
implemen
t
Step
()
of
the
algorithm
as
it
has
b
een
done
with
space:
scanning
hM
i
and
going
bac
k
and
forth
from
hM
i
to
the
op
erativ
e
area
w
ould
blo
w
up
time.
In
order
to
sa
v
e
time
w
e
cop
y
hM
i
in
the
op
erativ
e
area
on
the
w
orktap
e,
shifting
hM
i
while
mo
ving
on
the
w
orktap
e
to
the
righ
t.
If
jhM
ij
<
log
(t

(jxj))
it
tak
es
then
time
log
t

(jxj)
to
cop
y
hM
i
when
needed
and
time
log
(t

(jxj))
to
scan
it.
In
Step
()
eac
h
step
of
the
sim
ulated
mac
hine
tak
es
time
O
(log
(t

(jxj)))
so
the
total
execution
time
will
b
e
log
(t

(jxj))

t

(jxj)
0

log
(t

(jxj))
=
O
(t

(jxj))
The
logarithmic
factor
w
e
ha
v
e
to
in
tro
duce
in
Step
()
for
the
sim
ulation
of
M
is
th
us
the
reason
wh
y
in
Time
Hierarc
h
y
Theorem
w
e
ha
v
e
to
increase
the
time
b
ound
b
y
a
logarithmic
factor
in
order
to
get
a
bigger
complexit
y
class.
The
Hierarc
h
y
Theorems
sho
w
that
increasing
the
time-space
b
ounding
functions
b
y
an
y
small
amoun
t,
giv
es
as
a
result
bigger
time-space
complexit
y
classes
{
whic
h
is
what
w
e
in
tuitiv
ely
w
ould
exp
ect:
giv
en
more
resources,
w
e
should
b
e
able
to
recognize
more
languages.
Ho
w
ev
er,
it
is
also
clear
that
the
complexit
y
classes
hierarc
h
y
is
strict
only
if
w
e
use
prop
er
time/space
b
ounding
functions,
namely
time
and
sp
ac
e
c
onstructible
functions.
This
is
not
the
case
if
w
e
allo
w
an
y
recursiv
e
function
for
dening
complexit
y
classes,
as
it
can
b
e
seen
in
the
follo
wing
theorems.


LECTURE
.
SP
A
CE
COMPLEXITY
.
Odd
Phen
umena
(The
Gap
and
Sp
eed-Up
Theorems)
The
follo
wing
theorems
are
giv
en
without
pro
ofs,
whic
h
can
b
e
found
in
[].
Theorem
.	
(Boro
din's
Gap
Theorem):
F
or
any
r
e
cursive
function
g
:
N
!
N
with
g
(n)

n,
ther
e
exists
a
r
e
cursive
function
s

:
N
!
N
so
that
for
s

(n)
=
g
(s

(n)),
the
class
Dspace
(s

(n))
e
quals
Dspace
(s

(n)).
Theorem
.	
is
in
a
sense
the
opp
osite
of
the
Space
Hierarc
h
y
Theorem:
b
et
w
een
space
b
ounds
s

(n)
and
g
(s

(n))
there
is
no
increase
in
computational
p
o
w
er.
F
or
instance,
with
g
(n)
=
n

one
gets
g
(s

(n))
=
s

(n)

.
The
idea
is
to
c
ho
ose
s

(n)
that
gro
ws
v
ery
fast
and
suc
h
that
ev
en
if
g
(s

(n))
gro
ws
faster,
no
language
can
b
e
recognized
using
a
space
complexit
y
in
b
et
w
een.
Oded's
Note:
The
pro
of
can
b
e
extended
to
the
case
where
g
:
N

N
!
N
and
s

(n)
=
g
(n;
s

(n)).
Th
us,
one
can
ha
v
e
s

(n)
=
n

s

(n),
answ
ering
a
question
raised
in
class.
Theorem
.0
(Blum's
Sp
eed-up
Theorem):
F
or
any
r
e
cursive
function
g
:
N
!
N
with
g
(n)

n,
ther
e
exists
a
r
e
cursive
language
L
so
that
for
any
machine
M
de
ciding
L
in
sp
ac
e
s
:
N
!
N
ther
e
exists
a
machine
M
0
de
ciding
L
in
sp
ac
e
s
0
:
N
!
N
with
s
0
(n)
=
g
 
(s(n)).
So
there
exist
languages
for
whic
h
w
e
can
alw
a
ys
c
ho
ose
a
b
etter
mac
hine
M
recognizing
them.
Oded's
Note:
Note
that
an
analogous
theorem
for
time-complexit
y
(whic
h
holds
to
o),
stands
in
some
con
trast
to
the
optimal
algorithm
for
solving
NP-searc
h
problems
pre-
sen
ted
in
the
previous
lecture.
Bibliographic
Notes
Our
presen
tation
is
based
mostly
on
the
textb
o
ok
[].
A
pro
of
of
the
hierarc
h
y
theorem
can
also
b
e
found
in
[].
The
pro
ofs
of
Theorems
.	
and
.0
can
b
e
found
in
[].
Theorem
.
and
.
are
due
to
[]
and
[]
resp
ectiv
ely
.
.
J.E.
Hop
croft
and
J.D.
Ullman,
Intr
o
duction
to
A
utomata
The
ory,
L
anguages
and
Computa-
tion,
Addison-W
esley
,
		.
.
Lewis,
Stearns,
Hartmanis,
\Memory
b
ounds
for
recognition
of
con
text
free
and
con
text
sen
titiv
e
languages",
in
pro
ceedings
of
IEEE
Switching
Cir
cuit
The
ory
and
L
o
gic
al
Design
(old
F
OCS),
	,
pages
	{0.
.
Stearns,
Lewis,
Hartmanis,
\Hierarc
hies
of
memory
limited
computations",
in
pro
ceedings
of
IEEE
Switching
Cir
cuit
The
ory
and
L
o
gic
al
Design
(old
F
OCS),
	,
pages
	{	0.
.
M.
Sipser.
Intr
o
duction
to
the
The
ory
of
Computation,
PWS
Publishing
Compan
y
,
		.

Lecture

Non-Deterministic
Space
Notes
tak
en
b
y
Y
oad
Lustig
and
T
al
Hassner
Summary:
W
e
recall
t
w
o
basic
facts
ab
out
deterministic
space
complexit
y
,
and
then
dene
non-deterministic
space
complexit
y
.
Three
alternativ
e
mo
dels
for
measuring
non-
deterministic
space
complexit
y
are
in
tro
duced:
the
standard
non-deterministic
mo
del,
the
online
mo
del
and
the
oine
mo
del.
The
equiv
alence
b
et
w
een
the
non-deterministic
and
online
mo
dels
and
their
exp
onen
tial
relation
to
the
oine
mo
del
are
pro
v
ed.
After
the
relationships
b
et
w
een
the
non-deterministic
mo
dels
are
presen
ted
w
e
turn
to
in
v
es-
tigate
the
relation
b
et
w
een
the
non-deterministic
and
deterministic
space
complexit
y
.
Sa
vitc
h's
Theorem
is
presen
ted
and
w
e
conclude
with
a
translation
lemma.
.
Preliminaries
During
the
last
lectures
w
e
ha
v
e
in
tro
duced
the
notion
of
space
complexit
y
,
and
in
order
to
b
e
able
to
measure
sub-linear
space
complexit
y
,
a
v
arian
t
mo
del
of
a
T
uring
mac
hine
w
as
in
tro
duced.
In
this
mo
del
in
addition
to
the
w
ork
tap
e(s)
and
the
nite
state
con
trol,
the
mac
hine
con
tains
t
w
o
sp
ecial
tap
es
:
an
input
tap
e
and
an
output
tap
e.
These
dedicated
tap
es
are
restricted
eac
h
in
it's
o
wn
w
a
y
.
The
input
tap
e
is
read
only
and
the
output
tap
e
is
write
only
and
unidirectional
(i.e.
the
head
can
only
mo
v
e
in
one
direction).
In
order
to
deal
with
non-deterministic
space
complexit
y
w
e
will
ha
v
e
to
c
hange
the
mo
del
again,
but
b
efore
em
barking
on
that
task,
t
w
o
basic
facts
regarding
the
relations
b
et
w
een
time
and
space
complexit
y
classes
should
b
e
reminded.
T
o
simplify
the
description
of
asymptotic
b
eha
viour
of
functions
w
e
dene
:
Denition
.
Given
two
functions
f
:
N
!
N
and
g
:
N
!
R
f
is
at
least
g
if
ther
e
exists
an
n
0

N
s.t.
for
al
l
n

n
0
f
(n)

dg
(n)e.
f
is
at
least
linear
if
ther
e
exists
a
line
ar
function
g
s.t.
f
is
at
le
ast
g
(ther
e
exists
a
c
onstant
c
>
0
s.t.
f
is
at
le
ast
cn).
F
act
..
F
or
every
function
S
()
which
is
at
le
ast
l
og
()
D
S
P
AC
E
(S
)

D
T
I
M
E
(
O
(S
)
).
Pro
of:
Giv
en
a
T
uring
mac
hine
M
,
a
complete
description
of
it's
computational
state
on
a
xed
input
at
time
t
can
b
e
giv
en
b
y
sp
ecifying
:



LECTURE
.
NON-DETERMINISTIC
SP
A
CE

The
con
ten
ts
of
the
w
ork
tap
e(s).

The
lo
cation
of
the
head(s)
on
w
ork
tap
e(s).

The
lo
cation
of
the
head
on
the
input
tap
e.

The
state
of
the
mac
hine.
Denote
suc
h
a
description
a
c
ongur
ation
of
M
.
(Suc
h
a
conguration
ma
y
b
e
enco
ded
in
man
y
w
a
ys,
ho
w
ev
er
in
the
rest
of
the
discussion
w
e
will
assume
a
standard
enco
ding
w
as
xed,
and
w
ould
not
dieren
tiate
b
et
w
een
a
conguration
and
it's
enco
ding.
F
or
example
w
e
migh
t
refer
to
the
space
needed
to
hold
suc
h
a
conguration.
This
is
of
course
the
space
needed
to
hold
the
represen
tation
of
the
conguration
and
therefore
this
is
a
prop
ert
y
of
the
enco
ding
metho
d,
ho
w
ev
er
from
an
asymptotic
p
oin
t
of
view
the
minor
dierences
b
et
w
een
reasonable
enco
ding
metho
ds
mak
e
little
dierence).
A
complete
description
of
an
en
tire
computation
can
b
e
made
simply
b
y
sp
ecifying
the
conguration
at
ev
ery
time
t
of
the
computation.
If
during
a
computation
at
time
t,
mac
hine
M
reac
hed
a
conguration
in
whic
h
it
has
already
b
een
in
at
time
t

<
t,
(i.e.
the
congurations
of
M
at
times
t

and
t
are
iden
tical),
then
there
is
a
cycle
in
whic
h
the
mac
hine
mo
v
es
from
one
conguration
to
the
next
ultimately
returning
to
the
original
conguration
after
t
 t

steps.
Since
M
is
deterministic
suc
h
a
cycle
cannot
b
e
brok
en
and
therefore
M
's
computation
will
nev
er
end.
The
last
observ
ation
sho
ws
that
during
a
computation
in
whic
h
M
stops,
there
are
no
suc
h
cycles
and
therefore
no
conguration
is
ev
er
reac
hed
t
wice.
It
follo
ws
that
the
running
time
of
suc
h
a
mac
hine
is
b
ounded
b
y
the
n
um
b
er
of
p
ossible
congurations,
so
in
order
to
b
ound
the
time
it
is
enough
to
b
ound
the
n
um
b
er
of
p
ossible
congurations.
If
a
mac
hine
M
nev
er
uses
more
than
s
cells,
then
on
a
giv
en
input
x,
the
n
um
b
er
of
congu-
rations
is
b
ounded
b
y
the
n
um
b
er
of
p
ossible
con
ten
ts
of
s
cells
(i.e.
j M
j
s
,
where
 M
is
the
tap
e
alphab
et
of
mac
hine
M
),times
the
n
um
b
er
of
p
ossible
lo
cations
of
the
w
ork
head
(i.e.
s),
times
the
n
um
b
er
of
p
ossible
lo
cations
of
the
input
head
(i.e.
jxj),
times
the
n
um
b
er
the
p
ossible
states
(i.e.
jS
M
j).
If
the
n
um
b
er
of
cells
used
b
y
a
mac
hine
is
a
function
of
the
input's
length
the
same
analysis
holds
and
giv
es
us
a
b
ound
on
the
n
um
b
er
of
congurations
as
a
function
of
the
input's
length.
F
or
a
giv
en
mac
hine
M
and
input
x,
denote
b
y
#conf
(M
;
x)
the
n
um
b
er
of
p
ossible
congurations
of
mac
hine
M
on
input
x.
W
e
ha
v
e
seen
that
for
a
mac
hine
M
that
w
orks
in
space
S
()
on
input
x,
#conf
(M
;
x)
=
j M
j
S
(jxj)

S
(jxj)

jxj

jS
M
j
=

O
(S
(jxj))

jxj
Therefore
in
the
con
text
of
the
theorem
(i.e.
S
(jxj)
=

(l
og
(jxj)))
w
e
get
that
on
input
x
the
time
of
M
's
computation
is
b
ounded
b
y
:
#conf
(M
;
x)
=

O
(S
(jxj))
F
act
..
F
or
every
function
T
()
D
T
I
M
E
(T
)

D
S
P
AC
E
(T
).
Pro
of:
Clearly
no
more
then
T
(jxj)
cells
can
b
e
reac
hed
b
y
the
mac
hine's
head
in
T
(jxj)
steps.
Note
:
In
the
(far)
future
w
e
will
sho
w
a
b
etter
b
ound
(i.e.
D
T
I
M
E
(T
)

D
S
P
AC
E
(
T
l
og
(T
)
))
whic
h
is
non-trivial.
.
Non-Deterministic
space
complexit
y
In
this
section
w
e
dene
and
relate
three
dieren
t
mo
dels
of
non-deterministic
space
complexit
y
.

..
NON-DETERMINISTIC
SP
A
CE
COMPLEXITY

..
Denition
of
mo
dels
(online
vs
oine)
During
our
discussion
on
N
P
w
e
noticed
that
the
idea
of
a
non-deterministic
T
uring
mac
hine
can
b
e
formalized
in
t
w
o
approac
hes,
the
rst
approac
h
is
that
the
transition
function
of
the
mac
hine
is
non-deterministic
(i.e.
the
transition
function
is
a
m
ulti-v
alued
function),
in
the
second
approac
h
the
transition
function
is
deterministic
but
in
addition
to
the
input
the
mac
hine
gets
an
extra
string
(view
ed
as
a
guess):
the
mac
hine
is
said
to
accept
input
x
i
there
exists
a
guess
y
s.t.
the
mac
hine's
computation
on
(x,y)
ends
in
an
accepting
state.
(In
suc
h
a
case
y
is
called
a
witness
for
x).
In
this
section
w
e
shall
try
to
generalize
these
approac
hes
and
construct
a
mo
del
suitable
for
measuring
non-deterministic
space
complexit
y
.
The
rst
approac
h
can
b
e
applied
to
our
standard
turing
mac
hine
mo
del.
Put
formally
,
the
denition
of
a
non-deterministic
T
uring
mac
hine
under
the
rst
approac
h
is
as
follo
ws
:
Denition
.
(non-deterministic
T
uring
mac
hine):
A
non
deterministic
T
uring
machine
is
a
T
ur-
ing
machine
with
a
non-deterministic
tr
ansition
function,
having
a
work
tap
e,
a
r
e
ad-only
input
tap
e,
and
a
unidir
e
ctional
write-only
output
tap
e.
The
machine
is
said
to
ac
c
ept
input
x
if
ther
e
exists
a
c
omputation
ending
in
an
ac
c
epting
state.
T
rying
to
apply
the
second
approac
h
in
the
con
text
of
space
complexit
y
a
natural
question
arises
:
should
the
memory
used
to
hold
the
guess
b
e
metered?
It
seems
reasonable
not
to
meter
that
memory
as
the
mac
hine
do
es
not
\really"
use
it
for
compu-
tation.
(Just
as
the
mac
hine
do
es
not
\really"
use
the
memory
that
holds
the
input).
Therefore
a
sp
ecial
kind
of
memory
(another
tap
e)
m
ust
b
e
dedicated
to
the
guess
and
that
memory
w
ould
not
b
e
metered.
Ho
w
ev
er
if
w
e
do
not
meter
the
mac
hine
for
the
guess
memory
,
w
e
m
ust
restrict
the
access
to
the
guess
tap
e,
just
as
w
e
did
in
the
case
of
the
input
tap
e.
(surely
if
w
e
allo
w
the
mac
hine
to
write
on
the
guess
tap
e
without
b
eing
metered
and
that
w
a
y
get
\free"
auxiliary
memory
that
w
ould
b
e
c
heating).
It
is
clear
that
the
access
to
the
guess
tap
e
should
b
e
read
only
.
Denition
.
(oine
non-deterministic
T
uring
mac
hine):
A
n
oine
non-deterministic
T
uring
ma-
chine
is
a
T
uring
machine
with
a
work
tap
e,
a
r
e
ad-only
input
tap
e,
a
two-way
r
e
ad-only
guess
tap
e,
and
a
unidir
e
ctional
write-only
output
tap
e,
wher
e
the
c
ontents
of
the
guess
tap
e
is
sele
cte
d
non-deterministic
al
ly.
The
machine
is
said
to
ac
c
ept
input
x
if
ther
e
exists
c
ontents
to
the
guess
tap
e
(a
guess
string
y
)
s.t.
when
the
machine
starts
working
with
x
in
the
input
tap
e
and
y
in
the
guess
tap
e
it
eventual
ly
enters
an
ac
c
epting
state.
As
w
as
made
explicit
in
the
denition,
there
is
another
natural
w
a
y
in
whic
h
access
to
the
guess
tap
e
can
b
e
farther
limited:
the
tap
e
can
b
e
made
unidirectional
(i.e.
allo
w
the
head
to
mo
v
e
only
in
one
direction).
Denition
.
(online
non-deterministic
T
uring
mac
hine):
A
n
online
non-deterministic
T
uring
ma-
chine
is
a
T
uring
machine
with
a
work
tap
e,a
r
e
ad-only
input
tap
e,
a
unidir
e
ctional
r
e
ad-only
guess
tap
e
(whos
c
ontents
ar
e
sele
cte
d
non-deterministicly),
and
a
unidir
e
ctional
write-only
output
tap
e.
A
gain,
the
machine
is
said
to
ac
c
ept
x
if
ther
e
exists
a
guess
y
s.t.
the
machine
working
on
(x,y)


LECTURE
.
NON-DETERMINISTIC
SP
A
CE
wil
l
eventual
ly
enter
an
ac
c
epting
state.
An
approac
h
that
limits
the
guess
tap
e
to
b
e
unidirectional
seems
to
corresp
ond
to
an
online
guess-
ing
pro
cess
{
a
non-deterministic
mac
hine
w
orks
and
whenev
er
there
are
t
w
o
(or
more)
p
ossible
w
a
ys
to
con
tin
ue
the
mac
hine
guesses
(online)
whic
h
w
a
y
to
c
ho
ose.
If
suc
h
a
mac
hine
\w
an
ts"
to
\kno
w"
whic
h
w
a
y
it
guessed
in
the
past,
it
m
ust
record
it's
guesses
(use
memory).
On
the
other
hand,
the
approac
h
that
allo
ws
the
guess
tap
e
to
b
e
t
w
o-w
a
y
corresp
onds
to
an
oine
guessing
pro
cess
i.e.
all
the
guesses
are
giv
en
b
efore
hand
(as
a
string)
and
whenev
er
the
mac
hine
w
an
ts
to
c
hec
k
what
w
as
guessed
at
an
y
stage
of
the
computation,
it
can
lo
ok
at
the
guesses
list.
It
turns
out
that
the
rst
non-deterministic
mo
del
and
the
online
mo
del
are
equiv
alen
t.
(Al-
though
the
next
claim
is
phrased
for
language
decision
problems,
it
holds
with
the
same
pro
of
for
other
kinds
of
problems).
Claim
..
F
or
every
language
L
ther
e
exists
a
non-deterministic
T
uring
machine
M
N
that
iden-
ties
L
in
time
O
(T
)
and
sp
ac
e
O
(S
)
i
ther
e
exists
an
online
T
uring
machine
M
on
that
identies
L
in
time
O
(T
)
and
sp
ac
e
O
(S
).
Pro
of:
Giv
en
M
N
it
can
b
e
easily
transformed
to
an
online
mac
hine
M
on
in
the
follo
wing
w
a
y:
M
on
sim
ulates
M
N
and
whenev
er
M
N
has
sev
eral
options
for
a
next
mo
v
e
(it
m
ust
c
ho
ose
non-
deterministicaly
whic
h
option
to
tak
e),
M
on
decides
whic
h
option
to
tak
e
according
to
the
con
ten
t
of
the
cell
scanned
at
the
guess
tap
e,
then
mo
v
e
the
guess
tap
e
head
one
cell
to
the
righ
t.
In
some
cases
w
e
ma
y
w
an
t
to
restrict
the
alphab
et
of
the
guess
(for
example
to
f0;
g).
In
those
cases
there
is
a
minor
a
w
in
the
ab
o
v
e
construction
as
the
n
um
b
er
of
options
for
M
N
's
next
mo
v
e
ma
y
b
e
bigger
than
the
guess
alphab
et
th
us
the
decision
whic
h
option
to
tak
e
cannot
b
e
made
according
to
the
con
ten
t
of
a
single
guess
tap
e
cell.
This
is
only
an
apparen
t
a
w
since
w
e
can
assume
with
out
loss
of
generalit
y
that
M
N
has
at
most
t
w
o
options
to
c
ho
ose
from.
Suc
h
an
assumption
can
b
e
made
since
a
c
hoice
from
an
y
n
um
b
er
of
options
can
b
e
transformed
to
a
sequence
of
c
hoices
from
t
w
o
options
at
a
time
b
y
building
a
binary
tree
with
the
original
options
as
lea
v
es.
This
kind
of
transformation
can
b
e
easily
implemen
ted
on
M
N
b
y
adding
states
that
corresp
ond
to
the
inner
no
des
of
the
tree.
The
time
of
the
transformed
mac
hine
has
increased
at
most
b
y
a
factor
of
the
heigh
t
of
the
tree
whic
h
is
constan
t
in
the
input
size.
The
transformation
from
an
online
mac
hine
M
on
to
a
non-deterministic
mac
hine
is
equally
easy:
If
w
e
w
ould
ha
v
e
demanded
that
the
guess
head
of
M
on
m
ust
adv
ance
ev
ery
time,
the
construction
w
ould
ha
v
e
b
een
trivial
i.e.
at
ev
ery
time
M
on
mo
v
es
according
to
it's
state
and
the
con
ten
ts
of
the
cells
scanned
b
y
the
input-tap
e,
w
ork-tap
e
and
guess-tap
e
heads,
if
the
con
ten
ts
of
the
guess
cell
scanned
are
not
kno
wn
there
ma
y
b
e
sev
eral
mo
v
es
p
ossible
(one
for
eac
h
p
ossible
guess
sym
b
ol),
M
N
could
ha
v
e
simply
c
ho
ose
non-deterministically
b
et
w
een
those.
Ho
w
ev
er
as
w
e
dened
it,
the
guess
tap
e
head
ma
y
sta
y
in
place,
in
suc
h
a
case
the
non-deterministic
mo
v
es
of
the
mac
hine
are
dep
endenden
t
(are
xed
b
y
the
same
sym
b
ol)
un
till
the
guess
head
mo
v
es
again.
This
is
not
a
real
problem,
all
w
e
ha
v
e
to
do
is
remem
b
er
the
curren
t
guess
sym
b
ol,
i.e.
M
N
states
w
ould
b
e
S
M
on


where
S
M
on
is
M
0
on
s
states
and

is
the
guess
alphab
et,
(M
N
b
eing
in
state
(s;
a)
corresp
onds
to
M
on
b
eing
in
state
s
while
it's
guess
head
scans
a).
The
transition
function
of
M
N
is
dened
in
the
natural
w
a
y
.
Supp
ose
M
N
is
in
state
(s;
a)
and
scans
sym
b
ols
b
and
c
in
it's
w
ork
and
input
tap
es,
this
corresp
ond
to
M
on
b
eing
in
state
s
while
scanning
a,b
and
c.
In
this
case
M
on
transition
function
is
w
ell
dened,
(denote
the
new
state
b
y
s
0
),
M
N
will
mo
v
e
the
w
ork
and
input
heads
as
M
on
mo
v
es
it's
heads,
if
the
guess
head
of
M
on
sta
ys
xed
then
the
new
state
of
M
N
is
(s
0
;
a),

..
NON-DETERMINISTIC
SP
A
CE
COMPLEXITY

otherwise
M
on
reads
a
new
guess
sym
b
ol,
so
M
N
c
ho
oses
non-deterministically
a
new
state
of
the
form
(s
0
;
a
0
)
(i.e.
guesses
what
is
read
from
the
new
guess
tap
e
cell).
These
mo
dels
dene
complexit
y
classes
in
a
natural
w
a
y
.
In
the
follo
wing
denitions
M
(x;
y
)
should
b
e
read
as
\the
mac
hine
M
with
input
x
and
guess
y
".
Denition
.
(N
S
P
AC
E
on
):
F
or
any
function
T
:
N
!
N
N
S
P
AC
E
on
(T
)
def
=

>
<
>
:
L










Ther
e
exists
an
online
T
uring
machine
M
on
s.t.
for
any
input
x



ther
e
exists
a
witness
y



for
which
M
on
(x;
y
)
ac
c
epts
i
x

L,
and
that
for
any
y



M
on
uses
at
most
T
(jxj)
sp
ac
e.
	
>
=
>
;
Denition
.
(N
S
P
AC
E
of
f
):
F
or
any
function
T
:
N
!
N
N
S
P
AC
E
of
f
(T
)
def
=

>
<
>
:
L










Ther
e
exists
an
oine
T
uring
machine
M
of
f
s.t.
for
any
input
x



ther
e
exists
a
witness
y



for
which
M
of
f
(x;
y
)
ac
c
epts
i
x

L,
and
that
for
any
y



M
of
f
uses
at
most
T
(jxj)
sp
ac
e.
	
>
=
>
;
..
Relations
b
et
w
een
N
S
P
AC
E
on
and
N
S
P
AC
E
of
f
In
this
section
the
exp
onen
tial
relation
b
et
w
een
N
S
P
AC
E
on
and
N
S
P
AC
E
of
f
will
b
e
established.
Theorem
.
F
or
any
function
S
:
N
!
N
so
that
S
is
at
le
ast
lo
garithmic
and
log
S
is
sp
ac
e
c
onstructible,
N
S
P
AC
E
on
(S
)

N
S
P
AC
E
of
f
(l
og
(S
)).
Giv
en
an
online
mac
hine
M
on
that
w
orks
in
space
b
ounded
b
y
S
w
e
shall
construct
an
oine
ma-
c
hine
M
of
f
whic
h
recognizes
the
same
language
as
M
on
and
w
orks
in
space
b
ounded
b
y
O
(l
og
(S
)).
W
e
will
see
later
(Thero
em
)
the
opp
osite
relation
i.e.
giv
en
an
oine
mac
hine
M
of
f
that
w
orks
in
space
S
,
one
can
construct
an
online
mac
hine
M
on
that
recognizes
the
same
language
and
w
orks
in
space

O
(S
)
.
The
general
idea
of
the
pro
of
is
that
if
w
e
had
a
full
description
of
the
computation
of
M
on
on
input
x,
w
e
can
just
lo
ok
at
the
end
of
the
computation
and
cop
y
the
result
(man
y
of
us
are
familiar
with
the
general
framew
ork
from
our
sc
ho
ol
da
ys).
The
problem
is
that
M
of
f
do
es
not
ha
v
e
a
computation
of
M
on
ho
w
ev
er
it
can
use
the
p
o
w
er
of
non-determinism
to
guess
it.
This
is
not
the
same
as
ha
ving
a
computation,
since
M
of
f
cannot
b
e
sure
that
what
w
as
guessed
is
really
a
computation
of
M
on
on
x.
This
has
to
b
e
c
hec
k
ed
b
efore
cop
ying
the
result.
(The
absence
of
the
last
stage
caused
man
y
of
us
great
troubles
in
our
sc
ho
ol
da
ys).
T
o
pro
v
e
the
theorem
all
w
e
ha
v
e
to
sho
w
is
that
c
hec
king
that
a
guess
is
indeed
a
computation
of
a
space
S
()-online
mac
hine
can
b
e
done
in
l
og
(S
(jxj))
space.
T
o
do
that
w
e
will
rst
need
a
tec
hnical
result
concerning
the
length
of
computations
of
suc
h
a
mac
hine
M
on
,
this
result
is
obtained
using
a
similar
argumen
t
to
the
one
used
in
the
pro
of
of
F
act
..
(D
S
P
AC
E
(S
)

D
T
I
M
E
(
O
(S
)
)).
Pro
of:
(Theorem
.:
N
S
P
AC
E
on
(S
)

N
S
P
AC
E
of
f
(l
og
(S
))):
Giv
en
an
online
mac
hine
M
on
that
w
orks
in
space
b
ounded
b
y
S
w
e
shall
construct
an
oine
mac
hine
M
of
f
whic
h
recognize
the
same
language
as
M
on
and
w
orks
in
space
b
ounded
b
y
O
(l
og
(S
)).
Using
claim
.,
there
exists
a
non-deterministic
mac
hine
M
N
equiv
alen
t
to
M
on
,
so
it
is
enough
to
construct
M
of
f
to
b
e
equiv
alen
t
to
M
N
.
As
in
the
pro
of
of
F
act
..
(D
S
P
AC
E
(S
)

D
T
I
M
E
(
O
(S
)
))
w
e
w
ould
lik
e
to
describ
e
the
state
of
the
computation
b
y
a
conguration.
(As
M
N
uses
a
dieren
t
mo
del
of
computation
w
e


LECTURE
.
NON-DETERMINISTIC
SP
A
CE
m
ust
redene
conguration
to
capture
the
full
description
of
the
computation
at
a
giv
en
momen
t,
ho
w
ev
er
after
re-examination
w
e
disco
v
er
that
the
state
of
the
computation
in
the
non-deterministic
mo
del
is
fully
captured
b
y
the
same
comp
onen
ts
i.e.
the
con
ten
ts
of
the
w
ork
tap
e,
the
lo
cation
of
the
w
ork
and
input
tap
e
heads
and
the
state
of
the
mac
hine,
so
the
denition
of
a
conguration
can
remain
the
same).
Claim
..
If
ther
e
exists
an
ac
c
epting
c
omputation
of
M
N
on
input
x
then
ther
e
exists
such
a
c
omputation
in
which
no
c
ongur
ation
app
e
ars
mor
e
than
onc
e.
Pro
of:
Supp
ose
that
c
0
;
c

;
:
:
:
;
c
n
is
a
description
of
an
accepting
computation
as
a
sequence
congurations
in
whic
h
some
conguration
app
ear
more
than
once.
W
e
can
assume,
without
loss
of
generalit
y
that
b
oth
c
0
and
c
n
app
ear
only
once.
Assume
for
0
<
k
<
l
<
n,
c
k
=
c
l
.
W
e
claim
that
c
0
;
:
:
:
;
c
k
;
c
l
+
;
:
:
:
;
c
n
is
also
a
description
of
an
accepting
computation.
T
o
pro
v
e
that,
one
has
to
understand
when
is
a
sequence
of
congurations
a
description
of
an
accepting
computation,
This
is
the
case
if
the
follo
wing
hold
:
.
The
rst
conguration
(i.e.
c
0
)
describ
es
a
situation
in
whic
h
M
N
starts
a
computation
with
input
x
(initial
state,
the
w
ork
tap
e
empt
y).
.
Ev
ery
conguration
c
j
is
follo
w
ed
b
y
a
conguration
(i.e.
c
j
+
)
that
is
p
ossible
in
the
sense
that,
M
N
ma
y
mo
v
e
in
one
step
from
c
j
to
c
j
+
.
.
The
last
conguration
(i.e.
c
n
)
describ
es
a
situation
in
whic
h
the
M
N
accepts.
When
c
k
+
;
:
:
:
;
c
l
(the
cycle)
is
remo
v
ed
prop
erties

and

do
not
c
hange
as
c
0
and
c
n
remain
the
same.
Prop
ert
y

still
holds
since
c
l
+
is
p
ossible
after
c
l
and
therefore
after
c
k
.
c
0
;
:
:
:
;
c
k
;
c
l
+
;
:
:
:
;
c
n
is
a
computation
with
a
smaller
n
um
b
er
of
iden
tical
congurations
and
clearly
one
can
iterate
the
pro
cess
to
get
a
sequence
with
no
iden
tical
congurations
at
all.
Remark
:
The
pro
of
of
the
last
claim
follo
ws
a
v
ery
similar
reasoning
to
the
pro
of
of
F
act
..
(D
S
P
AC
E
(S
)

D
T
I
M
E
(
O
(S
)
)),
but
with
an
imp
ortan
t
dierence.
In
the
con
text
of
non-
determinism
it
is
p
ossible
that
a
computation
of
a
giv
en
mac
hine
is
arbitrarily
long
(the
mac
hine
can
en
ter
a
lo
op
and
lea
v
e
it
non-deterministicaly).
The
b
est
that
can
b
e
done
is
to
pro
v
e
that
short
computations
exist.
W
e
sa
w
that
also
arbitrarily
long
computations
ma
y
happ
en,
these
computations
do
not
add
p
o
w
er
to
the
mo
del
since
the
same
languages
can
b
e
recognized
if
w
e
forbid
long
computations.
A
similar
question
ma
y
rise
regarding
innite
computations.
A
mac
hine
ma
y
reject
either
b
y
halting
in
a
rejecting
(non-accepting)
state,
or
b
y
en
tering
an
innite
computation,
it
is
kno
wn
that
b
y
demanding
that
all
rejecting
computations
of
a
turing
mac
hine
will
halt,
one
reduces
the
p
o
w
er
of
the
mo
del
(the
class
R
as
opp
osed
to
RE),
the
question
is
is
the
same
true
for
space
b
ounded
mac
hines
?
It
turns
out
that
this
is
not
the
case
(i.e.
w
e
ma
y
demand
with
out
loss
of
generalit
y
that
ev
ery
computation
of
a
space
b
ounded
mac
hine
halts).
By
Claim
..
mac
hine
that
w
orks
in
space
S
w
orks
in
time

O
(S
)
,
w
e
can
transform
suc
h
a
mac
hine
to
a
mac
hine
that
alw
a
ys
halts
b
y
adding
a
time
coun
ter
that
coun
ts
un
till
the
time
limit
has
passed
and
then
halts
in
a
rejecting
state
(time
out).
Suc
h
a
coun
ter
w
ould
only
cost
l
og
(
O
(S
)
)
=
O
(S
)
so
adding
it
do
es
not
c
hange
the
space
b
ound
signican
tly
.
Now
we
have
al
l
we
ne
e
d
to
pr
esent
the
ide
a
of
the
pr
o
of.
Giv
en
input
x
mac
hine
M
of
f
will
guess
a
sequence
of
at
most
#conf
(M
;
x)
of
congurations
of
M
N
,
and
then
c
hec
k
that
it
is
indeed
an
accepting
computation
b
y
v
erifying
prop
erties
{
(in

..
NON-DETERMINISTIC
SP
A
CE
COMPLEXITY
	
the
pro
of
of
Claim
..).
If
the
guess
turns
out
to
b
e
an
accepting
computation,
M
of
f
will
accept
otherwise
reject.
How
much
sp
ac
e
do
es
M
of
f
ne
e
d
to
do
the
task?
The
k
ey
p
oin
t
is
that
in
order
to
v
erify
these
prop
erties
M
of
f
need
only
lo
ok
at

consecutiv
e
congurations
at
a
time
and
ev
en
those
are
already
on
the
guess
tap
e,
so
the
w
ork
tap
e
only
k
eeps
a
xed
n
um
b
er
of
coun
ters
(p
oin
ting
to
the
in
teresting
cell
n
um
b
ers
on
the
guess
and
input
tap
es).
M
of
f
treats
it's
guess
as
if
it
is
comp
osed
of
blo
c
ks,
eac
h
con
tains
a
conguration
of
M
on
.
T
o
v
erify
prop
ert
y
,
all
M
of
f
has
to
do
is
c
hec
k
that
the
rst
blo
c
k
(conguration)
describ
es
an
initial
computational
state
i.e.
c
hec
k
that
M
N
is
in
the
initial
state
and
that
the
w
ork
tap
e
is
empt
y
.
That
can
b
e
done
using
O
()
memory
.
T
o
v
erify
prop
ert
y

for
a
sp
ecic
couple
of
consecutiv
e
congurations
M
of
f
has
to
c
hec
k
that
the
con
ten
ts
of
the
w
ork
tap
e
in
those
congurations
is
the
same
except
p
erhaps
the
cell
on
whic
h
M
N
's
w
ork
head
w
as,
that
the
con
ten
t
of
the
cell
the
head
w
as
on,
the
state
of
the
mac
hine
and
the
new
lo
cation
of
the
w
ork
head
are
the
result
of
a
p
ossible
mo
v
e
of
M
N
.
T
o
do
that
M
of
f
c
hec
ks
that
these
prop
erties
hold
for
ev
ery
t
w
o
consecutiv
e
blo
c
ks
on
the
guess
tap
e.
This
can
b
e
done
using
a
xed
n
um
b
er
of
coun
ters
(eac
h
capable
of
holding
in
tegers
upto
the
length
of
a
single
blo
c
k)
+
O
()
memory
.
T
o
v
erify
prop
ert
y

all
M
has
to
do
is
to
v
erify
the
last
blo
c
k
(conguration)
describ
es
an
accepting
conguration.
That
can
b
e
done
using
O
()
memory
.
All
that
is
left
is
to
calculate
the
space
needed
to
hold
a
coun
ter.
This
is
the
maxim
um
b
et
w
een
log
the
size
of
a
conguration
and
l
og
(jxj).
A
conguration
is
comp
osed
of
the
follo
wing
parts
:

The
con
ten
ts
of
the
w
ork-tap
e
{
O
(S
(jxj))
cells

The
lo
cation
of
the
w
ork
head
{
l
og
(O
(S
(jxj)))
cells

The
state
of
the
mac
hine
M
N
{
O
()
cells

The
lo
cation
of
the
input
head
{
O
(l
og
(jxj))
cells
Since
S
is
at
least
logaithmic,
the
length
of
a
conguration
is
O
(S
(jxj)),
and
the
size
of
a
coun
ter
whic
h
p
oin
ts
to
lo
cation
in
a
conguration
is
O
()
+
log
(S
(jxj))).
Comment:
Tw
o
details
whic
h
w
ere
omitted
are
()
the
lo
w-lev
el
implemen
tation
of
the
v
erication
of
prop
ert
y
,
and
()
dealing
with
the
case
that
the
guess
is
not
of
the
righ
t
form
(i.e.,
do
es
not
consists
of
a
sequence
of
congurations
of
M
on
).
Theorem
.
F
or
any
sp
ac
e
c
onstr
actable
function
S
:
N
!
N
which
is
at
le
ast
lo
garithmic.
N
S
P
AC
E
of
f
(S
)

N
S
P
AC
E
on
(
O
(S
)
).
As
in
the
last
theorem,
giv
en
a
mac
hine
of
one
mo
del
w
e
w
ould
lik
e
to
nd
a
mac
hine
of
the
other
mo
del
accepting
the
same
language.
This
time
an
oine
mac
hine
M
of
f
is
giv
en
and
w
e
w
ould
lik
e
to
construct
an
online
mac
hine
M
on
.
In
suc
h
a
case
the
naiv
e
approac
h
is
sim
ulation,
i.e.
trying
to
build
a
mac
hine
M
on
that
sim
ulates
M
of
f
.
This
approac
h
w
ould
not
giv
e
us
the
space
b
ound
w
e
are
lo
oking
for,
ho
w
ev
er,
trying
to
follo
w
that
approac
h
will
b
e
instructiv
e,
so
that
is
what
w
e
will
do.
The
basic
approac
h
is
to
try
and
sim
ulate
M
of
f
b
y
an
online
mac
hine
M
on
(in
the
previous
theorem
w
e
did
ev
en
b
etter
than
that
b
y
guessing
the
computation
and
only
v
erifying
it's
correctness

0
LECTURE
.
NON-DETERMINISTIC
SP
A
CE
(that
w
a
y
the
memory
used
to
hold
the
computation
w
as
free).
This
kind
of
tric
k
will
not
help
us
here
b
ecause
the
pro
cess
of
v
erication
in
v
olv
es
comparing
t
w
o
congurations
and
in
an
online
mac
hine
that
w
ould
force
us
to
cop
y
a
conguration
to
the
w
ork
tap
e.
Since
holding
a
conguration
on
the
w
ork
tap
e
costs
O
(S
(jxj))
space
w
e
migh
t
as
w
ell
try
to
sim
ulate
M
of
f
in
a
normal
w
a
y).
Since
w
e
only
ha
v
e
an
online
mac
hine
whic
h
cannot
go
bac
k
and
forth
on
the
guess
tap
e,
the
straigh
tforw
ard
approac
h
w
ould
seem
to
b
e
:
guess
the
con
ten
t
of
a
guess
tap
e
for
M
of
f
then
cop
y
it
to
the
w
ork
tap
e
of
the
online
mac
hine
M
on
.
That
giv
es
M
on
t
w
o
w
a
y
access
to
the
guess
and
no
w
M
on
can
sim
ulate
M
of
f
in
a
straigh
t
forw
ard
w
a
y
.
The
only
question
remains
ho
w
m
uc
h
space
w
ould
b
e
needed
?
(clearly
at
least
as
long
as
the
guess)
The
length
of
the
guess
can
b
e
b
ounded
using
a
similar
analysis
to
the
one
w
e
sa
w
at
F
act
..
(D
S
P
AC
E
(S
)

D
T
I
M
E
(
O
(S
)
)),
only
this
time
things
are
a
bit
more
complicated.
If
w
e
lo
ok
on
M
of
f
's
guess
head
during
a
computation
it
mo
v
es
bac
k
and
forth
th
us
it's
mo
v
emen
t
forms
a
\snak
e
lik
e
path"
o
v
er
the
guess
tap
e.
5
76
34
t
t
t
Figure
.:
The
guess
head
mo
v
emen
t
The
guess
head
can
visit
a
cell
on
the
guess
tap
e
man
y
times,
but
w
e
claim
the
n
um
b
er
of
times
a
cell
is
visited
b
y
the
head
can
b
e
b
ounded.
The
idea
is,
as
in
F
act
..,
that
a
mac
hine
cannot
b
e
in
the
exact
same
situation
t
wice
without
en
tering
an
innite
lo
op.
T
o
formalize
the
last
in
tuition
w
e
w
ould
need
a
notion
of
conguration
(a
mac
hine's
exact
situation)
this
time
for
an
oine
mac
hine.
T
o
describ
e
in
full
the
computational
state
of
an
oine
mac
hine
one
w
ould
ha
v
e
to
describ
e
all
w
e
describ
ed
in
a
deterministic
mo
del
(con
ten
ts
of
w
ork
tap
e,
lo
cation
of
w
ork
and
input
head
and
the
mac
hine
state)
and
in
addition
the
con
ten
ts
of
the
guess
tap
e
and
the
lo
cation
of
the
guess
head.
Ho
w
ev
er
w
e
in
tend
to
use
the
conguration
notion
for
a
v
ery
sp
ecic
purp
ose,
in
our
case
w
e
are
dealing
with
a
sp
ecic
cell
on
the
guess
tap
e
while
the
guess
is
xed.
Therefore
denote
b
y
CWG
(conguration
without
guess)
of
M
of
f
its
conguration
without
the
the
guess
tap
e
con
ten
ts
and
the
guess
head
lo
cation.
(exactly
the
same
comp
onen
ts
as
in
the
non-deterministic
conguration).
Once
again
the
com
binatorial
analysis
sho
ws
us
that
the
n
um
b
er
of
p
ossible
CW
Gs
is
j j
S
(jxj)
S
(jxj)jS
M
jl
og
(jxj)
whic
h
is
equal
to
#conf
(M
;
x).
Claim
..
The
numb
er
of
times
during
an
ac
c
epting
c
omputation
of
M
of
f
in
which
the
guess
tap
e
he
ad
visits
a
sp
e
cie
d
c
el
l
is
lesser
or
e
qual
to
#conf
(M
;
x)
M
=

O
(S
)
.
Pro
of:
If
M
of
f
visits
a
single
cell
t
wice
while
all
the
parameters
in
the
CW
G
(con
ten
ts
of
w
ork
tap
e,
lo
cation
of
w
ork
and
input
head
and
state
of
the
mac
hine)
are
the
same
then
the
en
tire
computation
state
is
the
same,
b
ecause
the
con
ten
ts
of
the
guess
tap
e
and
the
input
remains
xed
throughout
the
computation.
Since
M
of
f
's
transition
function
is
deterministic
this
means
that
M
of
f
is
in
an
innite
lo
op
and
the
computation
nev
er
stops.
Since
M
of
f
uses
only
S
(jxj)
space
there
are
only
#conf
(M
;
x)
p
ossible
CW
Gs
and
therefore
#conf
(M
;
x)
b
ounds
the
n
um
b
er
of
times
the
guess
head
ma
y
return
to
a
sp
ecied
cell.
No
w
w
e
can
(almost)
b
ound
the
size
of
the
guess.

..
NON-DETERMINISTIC
SP
A
CE
COMPLEXITY

Claim
..
If
for
input
x
ther
e
exists
a
guess
y
s.t.
the
machine
M
of
f
stops
on
x
with
guess
y
,
then
ther
e
exists
such
a
guess
y
satisfying
jy
j
<
j j

#conf
(M
;
x)
#conf
(M
;x)
=


O
(S
(jxj))
.
Pro
of:
Denote
the
guess
tap
e
cells
c
0
c

:
:
:
c
jy
j
and
their
con
ten
t
y
=
g
o
:
:
:
g
jy
j
.
Giv
en
a
com-
putation
of
M
of
f
and
a
sp
ecied
cell
c
i
the
guess
head
ma
y
ha
v
e
visited
c
i
sev
eral
times
during
the
computation,
eac
h
time
M
of
f
w
as
in
another
CW
G.
W
e
can
asso
ciate
with
ev
ery
cell
c
i
the
sequence
of
CW
Gs
M
of
f
w
as
in
when
it
visited
c
i
,
denote
suc
h
a
sequence
b
y
visiting
se
quenc
e
of
c
i
.
(Th
us
the
rst
CW
G
in
the
visiting
sequence
of
c
i
is
the
CW
G
M
of
f
w
as
in
the
rst
time
the
guess
head
visited
c
i
,
the
second
CW
G
in
the
visiting
sequence
is
the
CW
G
M
of
f
w
as
in
the
second
time
the
guess
head
visited
c
i
and
so
on).
By
the
last
claim
w
e
get
that
the
length
of
a
visiting
sequence
is
b
et
w
een
0
and
#conf
(M
;
x).
Supp
ose
that
for
k
<
l
,
c
k
and
c
l
b
oth
ha
v
e
the
same
visiting
sequence
and
the
same
con
ten
t
i.e.
g
k
=
g
l
.
Then
the
guess
g
0
;
:
:
:
;
g
k
;
g
l
+
;
:
:
:
;
g
jy
j
is
also
a
guess
that
will
cause
M
of
f
to
accept
input
x.
The
idea
is
the
same
as
w
e
sa
w
in
the
pro
of
of
Claim
..,
i.e.
if
there
are
t
w
o
p
oin
ts
in
the
computation
in
whic
h
the
mac
hine
is
in
the
exact
same
situation,
then
the
part
of
the
computation
b
et
w
een
these
t
w
o
p
oin
ts
can
b
e
cut
o
and
the
result
w
ould
still
b
e
a
computation
of
the
mac
hine.
T
o
see
that
this
is
the
case
here,
w
e
need
just
follo
w
the
computation,
when
the
mac
hine
rst
tries
to
mo
v
e
from
cell
c
k
to
cell
c
k
+
(denote
this
time
t
k

)
it's
CW
G
is
the
same
CW
G
that
describ
es
the
mac
hine's
state
when
rst
mo
ving
from
cell
c
l
to
c
l
+
(denote
this
time
t
l

)
therefore
w
e
can
\skip"
the
part
of
the
computation
b
et
w
een
t
k

and
t
l

and
just
put
the
guess
head
on
c
l
+
and
still
ha
v
e
a
\computation"
(the
reason
for
the
quatation
marks
is
that
normal
computations
do
not
ha
v
e
guess
head
telep
ortations).
By
similar
reasoning
whenev
er
the
mac
hine
tries
to
mo
v
e
from
c
l
+
to
c
l
(or
from
c
k
to
c
k
+
)
w
e
can
just
put
the
guess
head
on
c
k
(resp
ectiv
ely
c
l
+
)
and
\cut
o
"
the
part
of
the
computation
b
et
w
een
the
time
it
mo
v
ed
from
c
l
+
to
the
correp
onding
time
it
arriv
ed
at
c
k
(resp
ectiv
ely
c
k
and
c
l
+
).
If
w
e
w
ould
ha
v
e
done
exactly
that
i.e.
alw
a
ys
\telep
orting"
the
head
and
cutting
the
middle
part
of
the
computation,
w
e
w
ould
get
a
\computation"
in
whic
h
the
guess
head
nev
er
en
tered
the
part
of
the
guess
tap
e
b
et
w
een
c
k
and
c
l
+
so
actually
w
e
w
ould
ha
v
e
a
real
computation
(this
time
with
out
the
quotation
marks)
on
the
guess
g
0
g

:
:
:
g
k
g
l
+
g
l
+
:
:
:
g
jy
j
.
Since
w
e
can
iterate
cut
and
paste
pro
cess
un
til
w
e
get
a
guess
with
no
t
w
o
cells
with
iden
tical
visiting
sequences
and
con
ten
t,
w
e
can
assume
the
guess
con
tains
no
t
w
o
suc
h
cells.
There
are
#conf
(M
;
x)
p
ossible
CW
Gs
therefore
#conf
(M
;
x)
n
sequences
of
n
CW
Gs.
Eac
h
visiting
sequence
is
a
sequence
of
CW
Gs
of
length
at
most
#conf
(M
;
x)
so
o
v
er
all
there
are
#conf
(M
;x)
P
i=
#conf
(M
;
x)
i

#conf
(M
;
x)

#conf
(M
;
x)
#conf
(M
;x)
=
#conf
(M
;
x)
#conf
(M
;x)+
=


O
(S
(jxj))
p
ossibilities
for
a
visiting
sequence.
Multiplied
b
y
the
j j
p
ossibilities
for
the
guess
itself
at
eac
h
guess
tap
e
cell,
this
b
ounds
the
length
of
our
short
guess.
W
e
ha
v
e
succeeded
in
b
ounding
the
length
of
the
guess
and
therefore
the
space
needed
to
sim-
ulate
M
of
f
in
an
online
mac
hine
using
a
straigh
tforw
ard
approac
h.
Unfortunately
the
b
ound
is
a
double
exp
onen
tial
b
ound
and
w
e
w
an
t
b
etter.
The
go
o
d
news
is
that
during
the
analysis
of
the
naiv
e
approac
h
to
the
problem
w
e
ha
v
e
seen
almost
all
that
is
necessary
to
pro
v
e
Theorem
..
Pro
of:
(The
or
em
.:
N
S
P
AC
E
of
f
(S
)

N
S
P
AC
E
on
(
O
(S
)
).):
Giv
en
an
oine
mac
hine
M
of
f
w
e
shall
construct
an
online
mac
hine
M
on
that
accepts
the
same
language.
In
the
pro
of
of
the
last
claim
(b
ounding
the
length
of
the
guess)
w
e
sa
w
another
w
a
y
to
describ
e
the
computation.
If
w
e
knew
the
guess,
instead
of
a
conguration
sequence
(with
time
as
an
index),
one
can
lo
ok
at
a
sequence
of
visiting
sequences
(with
the
guess
tap
e
cells
as
index).
Therfore
if


LECTURE
.
NON-DETERMINISTIC
SP
A
CE
w
e
add
the
con
ten
ts
of
the
guess
cell
to
eac
h
visiting
sequence,
the
sequence
of
the
augumen
ted
visiting
sequences
w
ould
describ
e
the
computation.
Our
online
mac
hine
M
on
will
guess
an
M
of
f
computation
describ
ed
in
the
visiting
sequences
form
and
c
hec
k
whether
indeed
the
guess
is
an
accepting
computation
of
M
of
f
(accept
if
so,
reject
otherwise).
The
strategy
is
v
ery
similar
to
what
w
as
done
in
the
pro
of
of
Theorem
.
(where
an
oine
mac
hine
guessed
a
computation
of
an
online
mac
hine
and
v
eried
it).
T
o
follo
w
this
strategy
w
e
need
to
sligh
tly
augmen
t
the
denition
of
a
visiting
sequence.
Giv
en
a
computation
of
M
of
f
and
a
guess
tap
e
cell
c
i
denote
b
y
dir
e
cte
d
visiting
se
quenc
e
(D
VS)
of
c
i
:

The
con
ten
t
of
the
guess
cell
c
i

The
visiting
sequence
of
c
i

F
or
ev
ery
CW
G
in
the
visiting
sequence,
the
direction
from
whic
h
the
guess
head
arriv
ed
to
the
cell
(either
R,
L
or
S
standing
for
Righ
t,
Left
or
Sta
y)
W
e
shall
no
w
try
to
c
haracterize
when
a
string
of
sym
b
ols
represen
ts
an
accepting
computation
in
this
represen
tation.
A
D
VS
has
the
r
e
asonable
r
eturning
dir
e
ction
pr
op
erty
if
:
whenev
er
according
to
a
CW
G
and
cell
con
ten
t
the
guess
head
should
mo
v
e
righ
t,
then
the
direction
asso
ciated
with
the
next
CW
G
(returning
direction)
is
left.
(resp
ectiv
ely
the
returning
direction
from
a
left
head
mo
v
emen
t
is
righ
t,
and
from
sta
ying
is
sta
y).
An
ordered
pair
of
D
VSs
is
called
lo
c
al
ly
c
onsistent
if
they
app
ear
as
if
they
ma
y
b
e
consecutiv
e
in
a
computation
i.e.
whenev
er
according
to
the
CW
G
and
the
guess
sym
b
ol
in
one
of
the
D
VSs
the
guess
head
should
mo
v
e
to
the
cell
that
the
other
D
VS
represen
ts
then
the
CW
G
in
the
other
D
VS
that
corresp
onds
to
the
consecutiv
e
mo
v
e
of
M
of
f
is
indeed
the
CW
G
M
of
f
w
ould
b
e
in
according
to
the
transition
function.
(The
corresp
onding
CW
G
is
w
ell
dened
b
ecause
w
e
can
coun
t
ho
w
man
y
times
did
the
head
lea
v
e
the
cell
of
the
rst
D
VS
in
the
direction
of
the
cell
of
other
D
VS
and
the
corresp
onding
CW
G
can
b
e
found
b
y
coun
ting
ho
w
man
y
time
sthe
head
arriv
ed
from
that
direction).
In
addition
to
that,
b
oth
D
VSs
m
ust
b
e
rst
en
tered
from
the
left,
and
b
oth
m
ust
ha
v
e
the
reasonable
returning
prop
ert
y
.
What
m
ust
b
e
c
hec
k
ed
in
order
to
v
erify
a
candidate
string
is
indeed
an
enco
ded
computation
of
M
of
f
on
input
x
?
.
The
CW
G
in
the
rst
D
VS
is
describing
an
initial
conguration
of
M
of
f
.
.
Ev
ery
t
w
o
consecutiv
e
D
VSs
are
lo
cally
consisten
t.
.
In
some
D
VS
the
last
CW
G
is
describing
an
accepting
conguration.
.
In
the
last
(most
righ
t)
D
VS,
there
is
no
CW
G
that
according
to
it
and
the
sym
b
ol
on
the
guess
tap
e
the
guess
head
should
mo
v
e
to
the
righ
t.
M
on
guesses
a
sequence
of
D
VSs
and
c
hec
ks
the
prop
erties
{.
T
o
do
that,
M
on
nev
er
has
to
hold
more
then
t
w
o
consecutiv
e
D
VSs
+
O
()
memory
.
Since
b
y
Claim
..
the
space
needed
for
a
D
VS
is
l
og
(

O
(S
(jxj))
)
=

O
(S
(jxj))
,
M
on
w
orks
in
space

O
(S
(jxj))
.
The
online
mo
del
is
considered
more
natural
for
measuring
space
complexit
y
(and
is
equiv-
alen
t
to
the
rst
form
ulation
of
a
non-deterministic
T
uring
mac
hine),
therefore
it
is
considered
the
standard
mo
del.
In
the
future
when
w
e
sa
y
\non-deterministic
space"
w
e
mean
as
measured
in
the
online
mo
del.
Th
us,
w
e
shorthand
N
S
P
AC
E
on
b
y
N
S
P
AC
E
.
That
is,
for
an
y
function
S
:
N
!
N,
w
e
let
N
S
P
AC
E
(S
)
def
=
N
S
P
AC
E
on
(S
).

..
RELA
TIONS
BETWEEN
DETERMINISTIC
AND
NON-DETERMINISTIC
SP
A
CE

.
Relations
b
et
w
een
Deterministic
and
Non-Deterministic
space
The
main
thing
in
this
section
is
Sa
vitc
h's
Theorem
asserting
that
non-deterministic
space
is
at
most
quadratically
stronger
than
deterministic
space.
..
Sa
vitc
h's
Theorem
In
this
section
w
e
presen
t
the
basic
result
regarding
the
relations
b
et
w
een
deterministic
and
non
deterministic
space
complexit
y
classes.
It
is
easy
to
see
that
for
an
y
function
S
:
N
!
N,
D
S
P
AC
E
(S
)

N
S
P
AC
E
(S
)
as
deterministic
mac
hines
are
in
particular
degenerated
non-deterministic
mac
hines.
The
question
is
ho
w
m
uc
h
can
b
e
\gained"
b
y
allo
wing
non-determinism.
Theorem
.	
(Sa
vitc
h):
F
or
every
sp
ac
e
c
onstr
actable
function
S
()
which
is
at
le
ast
lo
garithmic
N
S
P
AC
E
(S
)

D
S
P
AC
E
(S

).
F
or
an
y
non-deterministic
mac
hine
M
N
that
accepts
L
in
space
S
,
w
e
will
sho
w
a
deterministic
mac
hine
M
that
accepts
L
in
space
S

.
Denition
.0
(M
's
conguration
graph
o
v
er
x)
:
Given
a
machine
M
which
works
in
sp
ac
e
S
and
an
input
string
x,
M
's
conguration
graph
over
x,
G
x
M
,
is
the
dir
e
cte
d
gr
aph
in
which
the
set
of
vertic
es
is
al
l
the
p
ossible
c
ongur
ations
of
M
(with
input
x)
and
ther
e
exists
a
dir
e
cte
d
e
dge
fr
om
s

to
s

i
it
is
p
ossible
for
M
,
b
eing
in
c
ongur
ation
s

,
to
change
to
c
ongur
ation
s

.
Using
this
terminology
,
M
is
deterministic
i
the
out
degree
of
all
the
v
ertices
in
G
x
M
is
one.
Since
w
e
can
assume
without
loss
of
generalit
y
that
M
accepts
only
in
one
sp
ecic
conguration
(assume
M
clears
the
w
ork
tap
e
and
mo
v
e
the
head
to
the
initial
p
osition
b
efore
accepting),
denote
that
conguration
b
y
accept
M
and
the
initial
conguration
b
y
star
t
M
.
The
question
whether
there
exists
a
computation
of
M
that
accepts
x
can
no
w
b
e
phrased
in
the
graph
terminology
as
\is
there
a
directed
path
from
star
t
M
to
accept
M
in
G
x
M
".
Another
use
of
this
terminology
ma
y
b
e
in
form
ulating
the
argumen
t
w
e
ha
v
e
rep
eatedly
used
during
the
previous
discussions
:
if
there
exists
a
computation
that
accept
x
then
there
exists
suc
h
a
computation
in
whic
h
no
conguration
app
ears
more
than
once.
Phrased
in
the
conguration
graph
terminology
this
reduces
to
the
ob
vious
statemen
t
that
if
there
exists
a
path
b
et
w
een
t
w
o
no
des
in
a
graph
then
there
exists
a
simple
path
b
et
w
een
them.
If
M
w
orks
in
space
S
(jxj)
then
the
n
um
b
er
of
no
des
in
G
x
M
is
jV
x
M
j
=
#conf
(M
;
x)
therefore
if
there
exists
a
path
from
star
t
M
to
accept
M
then
there
is
one
of
length
at
most
jV
x
M
j.
W
e
reduced
the
problem
of
whether
M
accepts
x
to
a
graph
problem
of
the
sort
\is
there
a
directed
path
in
G
from
s
to
t
whic
h
is
at
most
l
long
?".
This
kind
of
problem
can
b
e
solv
ed
in
O
(l
og
(jl
j)

l
og
(jGj))
space.
(The
latter
is
true
assuming
that
the
graph
is
giv
en
in
a
w
a
y
that
enables
the
mac
hine
to
nd
the
v
ertices
and
the
v
ertices
neigh
b
ors
in
a
space
ecien
t
w
a
y
,
this
is
the
case
in
G
x
M
).
Claim
..
Given
a
gr
aph
G
=
(V
;
E
),
two
vertic
es
s;
t

V
and
a
numb
er
l
,
in
a
way
that
solving
the
question
of
whether
ther
e
exists
an
e
dge
b
etwe
en
two
vertic
es
c
an
b
e
done
in
O
(S
)
sp
ac
e,
the
question
\is
ther
e
a
p
ath
of
length
at
most
l
fr
om
s
to
t"
c
an
b
e
answer
e
d
in
sp
ac
e
O
(S

l
og
(l
)).
Pro
of:
If
there
is
a
path
from
s
to
t
of
length
at
most
l
either
there
is
an
edge
from
s
to
t
or
there
is
a
v
ertex
u
s.t.
there
is
a
path
from
s
to
u
of
length
at
most
dl
=e
and
a
path
from
u
to
t


LECTURE
.
NON-DETERMINISTIC
SP
A
CE
of
length
at
most
bl
=c.
It
is
easy
to
implemen
t
a
recursiv
e
pro
cedure
P
A
TH(a;
b;
l
)
to
answ
er
the
question.

b
o
olean
P
A
TH(a;
b;
l
)

if
there
is
an
edge
from
a
to
b
then
return
TR
UE

(otherwise
con
tin
ue
as
follo
ws
:)

for
ev
ery
v
ertex
v

if
P
A
TH(a;
v
;
dl
=e)
and
P
A
TH(v
;
b;
bl
=c)

then
return
TR
UE

otherwise
return
F
ALSE
How
much
sp
ac
e
do
es
P
A
TH(a;
b;
l
)
use?
When
w
e
call
P
A
TH
with
parameter
l
it
uses
O
(S
)
space
to
store
a,
b
and
l
,
c
hec
k
whether
there
is
an
edge
from
s
to
t,
and
handle
the
for-lo
op
con
trol
v
ariable
(i.e.
v
).
In
addition
it
in
v
ok
es
P
A
TH
t
wice
with
parameter
l
=,
but
the
k
ey
p
oin
t
is
that
b
oth
in
v
o
cations
use
the
same
space
(or
in
other
w
ords,
the
second
in
v
o
cations
re-uses
the
space
used
b
y
the
rst).
Letting
W
(l
)
denote
the
space
used
in
in
v
oking
P
A
TH
with
parameter
l
,
w
e
get
the
recursion
W
(l
)
=
O
(S
)
+
W
(l
=),
with
end-condition
W
()
=
O
(S
).
The
solution
of
this
relation
is
W
(l
)
=
O
(S

l
og
(l
)).
(The
solution
is
ob
vious
b
ecause
w
e
add
O
(S
),
l
og
(l
)
times
(halving
l
at
ev
ery
iteration,
it
will
tak
e
l
og
(l
)
iterations
to
get
to
).
The
solution
is
also
easily
v
eried
b
y
induction,
denote
b
y
c

the
constan
t
from
the
O
(S
)
and
c

=
c

,
the
induction
step
:
W
(l
)

c

S
+
c

S

l
og
(l
=)
=
c

S
+
c

S
l
og
(l
)
 c

S
=
c

S
l
og
(l
)
+
(c

 c

)S
and
for
c

>
c

w
e
get
W
(L)

c

S
l
og
(l
=)).
No
w
the
pro
of
of
Sa
vitc
h's
theorem
is
trivial.
Pro
of:
(The
or
em
.	
(Savitch's
the
or
em):
N
S
P
AC
E
(S
)

D
S
P
AC
E
(S

))
:
The
idea
is
to
apply
Claim
..
b
y
asking
\is
there
a
path
from
star
t
M
to
accept
M
in
G
x
M
?"
(w
e
sa
w
that
this
is
equiv
alen
t
to
\do
es
M
accept
x").
It
ma
y
seem
that
w
e
cannot
apply
Claim
..
in
this
case
since
G
x
M
is
not
giv
en
explicitly
as
an
input,
ho
w
ev
er
since
the
deterministic
mac
hine
M
get
x
as
the
input,
it
can
build
G
x
M
so
G
x
M
is
giv
en
implicitly
.
Our
troubles
are
not
o
v
er
since
storing
all
G
x
M
is
to
o
space
consuming,
but
there
is
no
need
for
that,
our
deterministic
mac
hine
can
build
G
x
M
on
the
y
i.e.
build
and
k
eep
in
memory
only
the
parts
it
needs
for
the
op
eration
it
p
erforms
no
w
then
reuse
the
space
to
hold
other
parts
of
the
graph
that
ma
y
b
e
needed
for
the
next
op
erations.
This
can
b
e
done
since
the
v
ertices
of
G
x
M
are
congurations
of
M
N
and
there
is
an
edge
from
v
to
u
i
it
is
p
ossible
for
M
N
b
eing
in
conguration
v
to
c
hange
for
conguration
u,
and
that
can
easily
b
e
c
hec
k
ed
b
y
lo
oking
at
the
transition
function
of
M
N
.
Therefore
If
M
w
orks
in
O
(S
)
space
then
in
G
x
M
w
e
need
O
(S
)
space
to
store
a
v
ertex
(i.e.
a
conguration),
and
l
og
(O
(S
))
space
to
c
hec
k
if
there
is
an
edge
b
et
w
een
t
w
o
stored
v
ertices,
all
that
is
left
is
to
apply
the
Claim
...
..
A
translation
lemma
Denition
.
(NL)
The
c
omplexity
class
Non-Deterministic
loga
rithmic
space,
denote
d
N
L
,
is
dene
d
as
N
S
P
AC
E
(O
(l
og
(n))).
Sometimes
Sa
vitc
h's
theorem
can
b
e
found
phrased
as:
N
L

D
S
P
AC
E
(l
og
(n)

).

..
RELA
TIONS
BETWEEN
DETERMINISTIC
AND
NON-DETERMINISTIC
SP
A
CE

This
lo
oks
lik
e
a
sp
ecial
case
of
the
theorem
as
w
e
phrased
it,
but
is
actually
equiv
alen
t
to
it.
What
w
e
miss
in
order
to
see
the
full
equiv
alence
is
a
pro
of
that
con
tainmen
t
of
complexit
y
classes
\translates
up
w
ards".
Lemma
..
(T
ranslation
lemma):
Given
S

;
S

;
f
sp
ac
e
c
onstr
actable
functions
s.t.
S

(f
)
is
also
sp
ac
e
c
onstr
actible
and
S

(n)

l
og
(n);
f
(n)

n
then
if
N
S
P
AC
E
(S

(n))

D
S
P
AC
E
(S

(n))
then
N
S
P
AC
E
(S

(f
(n)))

D
S
P
AC
E
(S

(f
(n))).
Using
the
translation
lemma,
it
is
easy
to
deriv
e
the
general
Sa
vitc
h's
therorem
from
the
re-
stricted
case
of
NL:
Giv
en
that
N
L

D
S
P
AC
E
(l
og
(n)

),
giv
en
a
function
S
()
c
ho
ose
S

()
=
l
og
();
S

()
=
l
og
()

and
f
()
=

S
()
(f
w
ould
b
e
constractible
if
S
w
as)
no
w,
applying
the
translation
lemma,
w
e
get
that
N
S
P
AC
E
(l
og
(
S
))

D
S
P
AC
E
(l
og
(
S
)

)
whic
h
is
equiv
alen
t
to
N
S
P
AC
E
(S
)

D
S
P
AC
E
(S

).
Pro
of:
Giv
en
L

N
S
P
AC
E
(S

(f
(n)))
w
e
m
ust
pro
v
e
the
existence
of
a
mac
hine
M
that
w
orks
in
space
S

(f
(n))
and
accepts
L.
The
idea
is
simple,
transform
our
language
L
of
non-deterministic
space
complexit
y
S

(f
)
to
a
language
L
pad
of
non-deterministic
space
complexit
y
S

b
y
enlarging
the
input,
this
can
b
e
done
b
y
padding.
No
w
w
e
kno
w
that
L
pad
is
also
of
deterministic
space
complexit
y
S

.
Since
the
w
ords
of
L
pad
are
only
the
w
ords
of
L
padded,
w
e
can
think
of
a
mac
hine
that
giv
en
an
input
pads
it
and
then
c
hec
ks
if
it
is
in
L
pad
.
The
rest
of
the
pro
of
is
just
carrying
out
this
program
carefully
while
c
hec
king
that
w
e
do
not
step
out
of
the
space
b
ounds
for
an
y
input.
There
exists
M

whic
h
w
orks
in
space
S

(f
(n))
and
accepts
L.
Denote
b
y
L
pad
the
language
L
pad
def
=
fx$
i
jx

L
and
M

accp
ets
x
in
S

(jxj
+
i)
space.g
where
$
is
a
new
sym
b
ol.
W
e
claim
no
w
that
L
pad
is
of
non-deterministic
space
complexit
y
S

.
T
o
c
hec
k
whether
a
candidate
string
s
is
in
L
pad
w
e
ha
v
e
to
c
hec
k
that
it
is
of
form
x$
j
for
some
j
(that
can
b
e
done
using
O
()
space).
If
so
(i.e.
s
=
x$
j
),
w
e
ha
v
e
to
c
hec
k
that
M

accepts
x
in
S

(f
(jxj
+
j
))
space
and
do
that
without
stepping
out
of
the
S

space
b
ound
on
the
original
input
(i.e.
S

(jsj)
=
S

(jxj
+
j
)).
This
can
b
e
done
easily
b
y
sim
ulating
M

on
x
while
c
hec
king
that
M

do
es
not
step
o
v
er
the
space
b
ound
(the
space
b
ound
S

(jxj
+
j
)
can
b
e
calculated
since
S

is
space
constructable).
(The
resulting
mac
hine
is
referred
to
as
M

.)
Since
L
pad
is
in
N
S
P
AC
E
(S

)
it
is
also
in
D
S
P
AC
E
(S

);
i.e.,
there
exists
a
deterministic
mac
hine
M

that
recognizes
L
pad
in
S

space.
Giv
en
the
deterministic
mac
hine
M

w
e
will
construct
a
deterministic
mac
hine
M

that
accepts
the
original
L
in
space
S

(f
)
in
the
follo
wing
w
a
y:
On
input
x,
w
e
sim
ulate
M

on
x$
j
for
j
=
;
;
:
:
:
as
long
as
our
space
p
ermits
(i.e.,
using
space
at
most
S

(f
(jxj)),
including
all
our
o
v
erheads).
This
can
b
e
done
as
follo
ws:
If
the
head
of
M

is
within
x,
M

's
input
head
will
b
e
on
the
corresp
onding
p
oin
t
in
the
input
tap
e,
whenev
er
the
head
of
M

lea
v
es
the
x
part
of
the
input,
M

k
eeps
a
coun
ter
of
M

's
input
head
p
osition
(and
supplies
the
sim
ualted
M

with
either
$
or
blac
k
as
appropriate).
Recall
that
w
e
also
k
eep
trac
k
that
M

do
es
not
use
more
that
S

(f
(jxj))
(for
that
reason
w
e
need
S

(f
)
to
b
e
constractible),
and
if
M

tries
to
step
out
of
this
b
ound
w
e
will
treat
it
as
if
M

rejected.
If
during
our
sim
ulations
M

accept
so
do
es
M

otherwise
M

rejects.
Basicly
M

is
trying
to
nd
a
righ
t
j
that
will
cause
M

to
accept,
if
x
is
not
in
L
then
neither
is
x$
j
in
L
pad
(for
an
y
j
)
and
therefore
M

will
not
accept
an
y
suc
h
string
un
till
M

will
ev
en
tually
reject
x
(whic
h
will
happ
en
when
j
is
sucien
tly
large
so
that
log
j
sup
erseeds
S

(f
(jxj))
whic
h
is
our
o
wn
space
b
ound).
If
on
the
other
hand
x
is
in
L
than
M

accepts
it
in
S

(f
(jxj))
space
therefore
M

accepts
x$
j
for
some
j

f
(jxj)
 jxj
(since
to
hold
f
(jxj)
 jxj
one
needs
only
a


LECTURE
.
NON-DETERMINISTIC
SP
A
CE
coun
ter
of
size
l
og
(f
(jxj)
and
S

is
bigger
then
l
og
this
coun
ter
can
b
e
k
ept
within
the
space
b
ound
of
S

(f
(jxj))
and
M

will
get
to
try
the
righ
t
x$
i
and
will
ev
en
tually
accept
x).
Remark:
In
the
last
pro
of
thre
w
as
no
essen
tial
use
of
the
mo
del
deterministic
or
non-deterministic,
so
b
y
similar
argumen
t
w
e
can
pro
v
e
analogous
results
(for
example,
D
S
P
AC
E
(S

)

D
S
P
AC
E
(S

)
implies
D
S
P
AC
E
(S

(f
))

D
S
P
AC
E
(S

(f
))
).
By
a
similar
argumen
t
w
e
ma
y
also
pro
v
e
analogous
results
regarding
time
complexit
y
classes.
In
this
case
w
e
cannot
use
our
metho
d
of
searc
hing
for
the
correct
padding
since
this
metho
d
(while
b
eing
space
ecien
t)
is
time
consuming.
On
the
other
hand,
under
suitable
h
yp
othesis,
w
e
can
can
compute
f
directly
and
so
do
not
need
to
searc
h
for
the
righ
padding.
W
e
dene
L
pad

=
fx$
f
(jxj) jxj
:
x

Lg
and
no
w
M

can
compute
f
(jxj)
and
run
M

on
x$
f
(jxj) jxj
in
one
try
.
There
are
t
w
o
minor
mo
dications
that
ha
v
e
to
b
e
done.
Firstly
,
w
e
assume
all
the
functions
in
v
olv
ed
S

;
S

;
f

n
(this
is
a
reasonable
assumption
when
dealing
with
time-complexit
y
classes).
Secondly
,
M

has
to
c
hec
k
whether
the
input
x$
j
is
indeed
x$
f
(jxj) jxj
;
this
is
easy
if
it
can
compute
f
(jxj)
within
it's
time
b
ounds
(i.e.,
S

(jx$
j
j)),
but
ma
y
not
b
e
the
case
if
the
input
x$
j
is
m
uc
h
shorter
than
f
(jxj).
T
o
solv
e
that,
M

only
has
to
time
itself
while
computing
f
(jxj)
and
if
it
fails
to
compute
f
(jxj)
within
the
time
b
ound
it
rejects.
Bibliographic
Notes
Oded's
Note:
T
o
b
e
done
{
nd
the
references
for
the
relationship
b
et
w
een
the
t
w
o
denitions
of
non-deterministic
space.
Sa
vitc
h's
Theorem
is
due
to
[];
its
pro
of
can
b
e
found
in
an
y
standard
textb
o
ok
(e.g.,
see
textb
o
oks
referred
to
in
the
previous
lecture).
.
W.J.
Sa
vitc
h,
\Relationships
b
et
w
een
nondeterministic
and
deterministic
tap
e
complexities",
JCSS,
V
ol.

(),
pages
-	,
	0.

Lecture

Inside
Non-Deterministic
Logarithmic
Space
Notes
tak
en
b
y
Amiel
F
erman
and
Noam
Sadot
Summary:
W
e
start
b
y
considering
space
complexit
y
of
(decision
and
searc
h)
problems
solv
ed
b
y
using
oracles
with
kno
wn
space
complexities.
Then
w
e
study
the
complexit
y
class
N
L
(the
set
of
languages
decidable
within
Non-Deterministic
Logarithmic
Space);
W
e
sho
w
a
problem
whic
h
is
complete
for
N
L
,
namely
the
Connectivit
y
problem
(De-
ciding
for
a
giv
en
directed
graph
G
=
(V
;
E
)
and
t
w
o
v
ertices
u;
v

V
whether
there
is
a
directed
path
from
u
to
v
).
Then
w
e
pro
v
e
the
somewhat
surprising
result:
N
L
=
co
N
L
(i.e.,
N
L
class
is
closed
under
complemen
tation).
.
The
comp
osition
lemma
The
follo
wing
lemma
w
as
used
implicitly
in
the
pro
of
of
Sa
vitc
h's
Theorem:
Lemma
..
(comp
osition
lemma
{
decision
v
ersion):
Supp
ose
that
machine
M
solves
pr
oblem

while
using
sp
ac
e
s()
and
having
or
acle
ac
c
ess
to
decision
tasks


;
:::;

t
.
F
urther
supp
ose
that
for
every
i,
the
task

i
c
an
b
e
solve
d
within
sp
ac
e
s
i
().
Then,

c
an
b
e
solve
d
within
sp
ac
e
s
0
(),
wher
e
s
0
(n)
def
=
s(n)
+
max
i
fs
i
(exp
(s(n)))g.
Pro
of:
Let
us
x
a
certain
input
x
of
length
n
for
the
mac
hine
M
.
First,
it
is
clear
from
the
denition
of
M
that
a
space
of
length
at
most
s(n)
is
used
on
M
's
w
ork-tap
e
for
the
computation
to
b
e
carried
out.
Next,
w
e
m
ust
consider
all
p
ossible
in
v
o
cations
of
the
decision
tasks
whic
h
M
has
oracle
access
to.
Let
M
i
b
e
(a
determinstic)
T
uring
Mac
hine,
computing
decision
task

i
.
Since
at
eac
h
step
of
its
computation,
M
ma
y
query
some
oracle
M
i
,
it
is
clear
that
the
con
ten
ts
of
eac
h
suc
h
query
dep
ends
on
the
dieren
t
congurations
that
M
w
en
t
through
un
til
it
reac
hed
the
conguration
in
whic
h
it
in
v
ok
ed
the
oracle.
In
this
sense,
the
input
to
M
i
is
a
query
that
M
\decided
on".
W
e
ma
y
deduce
that
an
input
to
an
oracle
is
b
ounded
b
y
the
size
of
the
set
of
all
congurations
of
mac
hine
M
on
the
(xed)
input
x
(this
is
the
maximal
length
of
a
suc
h
a
query).
Let
us
b
ound
the
maximal
size
of
suc
h
a
query:
It
is
the
n
um
b
er
of
all
dieren
t
congurations
of
M
on
input
of
size
n:
j
M
j
s(n)

s(n)

n,
where
w
e
m
ultiply
the
n
um
b
er
of
all
p
ossible
con
ten
ts
of
the
w
ork-tap
e
(whose
length
is
b
ounded
b
y
s(n))
with
the
n
um
b
er
of
p
ossible
p
ositions
(of
the



LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
head)
on
the
w
ork-tap
e
and
with
the
n
um
b
er
of
p
ossible
p
ositions
on
the
input-tap
e
(whose
length
is
n)
resp
ectiv
ely
(
M
is
the
w
ork
alphab
et
dened
for
the
mac
hine
M
).
Since
the
n
um
b
er
of
congurations
of
the
mac
hine
M
on
input
of
length
n
is
exp(s(n)),
it
is
clear
that
the
sim
ulation
of
M
i
w
ould
require
no
more
than
s
i
(exp(s(n))).
Since
w
e
do
not
need
to
store
the
con
ten
ts
of
the
w
ork-tap
e
after
eac
h
suc
h
sim
ulation,
but
rather
in
v
ok
e
eac
h
M
i
whenev
er
w
e
need
it
and
erase
all
con
ten
ts
of
the
w
ork-tap
e
related
to
that
sim
ulation,
it
is
clear
that
in
addition
to
the
space
s(n)
of
w
ork-tap
e
men
tioned
ab
o
v
e,
w
e
need
to
consider
the
maxim
um
space
that
a
certain
M
i
w
ould
need
during
its
sim
ulation,
hence
the
result.
W
e
stress
that
the
ab
o
v
e
lemma
refers
to
decision
problems,
where
the
output
is
a
single
bit.
Th
us,
in
the
sim
ulation
of
the
M
i
's
the
issue
of
storing
parts
of
the
output
of
M
i
do
es
not
arise.
Things
are
dieren
t
if
w
e
comp
ose
searc
h
problems.
(Recall
that
ab
o
v
e
and
b
elo
w
w
e
refer
to
deterministic
space-b
ounded
computations)
Lemma
..
(comp
osition
lemma
{
searc
h
v
ersion):
Supp
ose
that
machine
M
solves
pr
oblem

while
using
sp
ac
e
s()
and
having
or
acle
ac
c
ess
to
searc
h
tasks


;
:::;

t
.
(As
w
e
shall
see,
it
do
es
not
matter
if
mac
hine
M
has
a
one-w
a
y
or
t
w
o-w
a
y
access
to
the
oracle-reply
tap
e.)
F
urther
supp
ose
that
al
l
queries
of
machine
M
have
length
b
ounde
d
by
exp(s(n))
and
that
the
answers
ar
e
also
so
b
ounde
d.

F
urther
supp
ose
that
for
every
i,
the
task

i
c
an
b
e
solve
d
within
sp
ac
e
s
i
().
Then,

c
an
b
e
solve
d
within
sp
ac
e
s
0
(),
wher
e
s
0
(n)
def
=
s(n)
+
max
i
fs
i
(exp
(s(n)))g.
The
em
ulation
of
the
oracles
here
is
more
complex
than
b
efore
since
these
oracles
ma
y
return
strings
rather
than
single
bits.
F
urthermore,
the
replies
to
dieren
t
oracles

i
's
ma
y
b
e
read
concurren
tly
.
In
the
em
ulation
w
e
cannot
aord
to
run
M
i
on
the
required
query
and
store
the
answ
er,
since
storing
the
answ
er
w
ould
use
to
o
m
uc
h
space.
In
order
to
a
v
oid
this,
ev
ery
time
w
e
need
an
y
bit
in
the
answ
er
to

i
(q
),
(where
q
is
a
query)
w
e
need
to
run
M
i
again
on
q
and
fetc
h
the
required
bit
from
the
on-line
generated
output,
scanning
(and
omitting)
all
other
bits;
i.e.,
the
answ
er
that
w
ould
b
e
receiv
ed
from
the
oracle
w
ould
b
e
written
on
the
output
one
bit
at
a
time
and
b
y
using
a
coun
ter,
the
mac
hine
could
tell
when
did
it
reac
h
the
desired
bit
of
the
answ
er
(this
pro
cess
w
ould
halt
since
the
length
of
the
answ
er
is
b
ounded).
Note
that
this
pro
cedure
is
applicable
regardless
if
M
has
one-w
a
y
or
t
w
o-w
a
y
access
to
the
oracle-reply
tap
e.
Note
that
unlik
e
in
Lemma
..,
here
w
e
cannot
b
ound
the
length
of
the
query
b
y
the
n
um
b
er
of
p
ossible
congurations
since
this
n
um
b
er
is
to
o
large
(as
it
includes
the
n
um
b
er
of
p
ossible
oracle
answ
ers).
Instead,
w
e
use
the
h
yp
othesis
in
the
lemma.
Anallagous,
but
m
uc
h
simpler,
result
holds
for
the
time
complexit
y:
Lemma
..
(comp
osition
lemma
{
time
v
ersion):
Supp
ose
that
machine
M
solves
pr
oblem

while
using
time
t()
and
having
or
acle
ac
c
ess
to
decision
tasks


;
:::;

k
.
F
urther
supp
ose
that
for
every
i,
the
task

i
c
an
b
e
solve
d
within
time
t
i
().
Then,

c
an
b
e
solve
d
within
time
t
0
(),
wher
e
t
0
(n)
def
=
t(n)

max
i
ft
i
(t(n))g.
Pro
of:
Similarly
to
the
pro
of
regarding
Space,
w
e
shall
x
a
certain
input
x
of
length
n
for
the
mac
hine
M
.
First,
it
is
clear
from
the
denition
of
M
that
time
t(n)
is
suces
for
the
computation
of
M
on
x
to
b
e
carried
out.
Next,
w
e
m
ust
consider
all
p
ossible
in
v
o
cations
of
the
decision
tasks
whic
h
M
has
oracle
access
to.
Here
at
eac
h
step
of
the
computation
M
could
in
v
ok
e
an
oracle
M
i
and
so
it
is
clear
that
the
time
complexit
y
of
the
computation
w
ould
b
e
t(n)
m
ultiplied
b
y
the

Without
this
assumption,
w
e
cannot
b
ound
the
n
um
b
er
of
congurations
of
mac
hine
M
on
a
xed
input,
as
the
conguration
dep
ends
on
the
lo
cation
of
M
's
head
on
the
oracle-reply
tap
e.

..
A
COMPLETE
PR
OBLEM
F
OR
N
L
	
maximal
time
complexit
y
of
an
oracle.
In
order
to
nd
the
time
complexit
y
of
some
oracle
M
i
,
w
e
ha
v
e
to
consider
the
p
ossible
length
of
a
query
to
M
i
;
since
there
are
t(n)
time
units
during
the
computation
of
M
on
x,
the
size
of
the
query
to
M
i
could
b
e
at
most
t(n).
W
e
deduce
that
the
time
complexit
y
of
some
oracle
M
i
whic
h
is
in
v
ok
ed
at
some
p
oin
t
of
the
computation
of
M
on
x,
could
b
e
at
most
t
i
(t(n)).
According
to
what
w
as
said
ab
o
v
e,
the
time
complexit
y
of
M
on
x
w
ould
b
e
the
n
um
b
er
of
time
units
of
its
computation
-
t(n)
-
m
ultiplied
b
y
the
maximal
time
complexit
y
of
some
oracle
M
i
(

i

k
),
hence
the
result.
.
A
complete
problem
for
N
L
The
complexit
y
class
N
L
is
dened
to
b
e
simply
N
S
P
AC
E
(O
(l
og
(n))).
More
formally
w
e
ha
v
e:
Denition
.
N
L:
A
language
L
b
elongs
to
N
L
if
ther
e
is
a
nondeterminstic
T
uring
machine
M
that
ac
c
epts
L
and
a
function
f
(n)
=
O
(l
og
(n))
such
that
for
every
input
x
and
for
every
c
omputation
of
M
at
most
f
(jxj)
dier
ent
work-tap
e
c
el
ls
ar
e
use
d.
Our
goal
in
this
section
and
the
follo
wing
one
w
ould
b
e
to
study
some
prop
erties
of
the
class
N
L.
T
o
that
end
w
e
dene
the
follo
wing:
Denition
.
A
lo
g-sp
ac
e
r
e
duction
of
L

to
L

is
a
lo
g-sp
ac
e
c
omputable
function
f
such
that
x;
x

L

,
f
(x)

L

Note
that
a
log-space
reduction
is
analagous
to
a
Karp-reduction
(where
space
corresp
onds
to
time
and
the
logarithmic
n
um
b
er
of
cells
corresp
ond
to
p
olynomial
n
um
b
er
of
steps).
Actually
,
since
eac
h
function
that
can
b
e
computed
in
space
s(),
can
also
b
e
computed
in
time
exp(s()),
w
e
ha
v
e
that
a
log-space
reduction
is
a
sp
ecial
case
of
a
p
olynomial
time
reduction.
The
next
denition
w
ould
dene
a
notion
analagous
to
N
P
-completeness
(as
w
e
will
see,
this
w
ould
pro
v
e
useful
in
pro
ving
a
prop
osition
ab
out
N
L
whic
h
is
analagous
to
a
prop
osition
in
N
P
):
Denition
.
L
is
N
L-Complete
if:
()
L

N
L;
and
()
L
0

N
L,
L
0
is
lo
g-sp
ac
e
r
e
ducible
to
L.
..
Discussion
of
Reducibilit
y
As
implied
from
the
denitions
ab
o
v
e,
our
goal
w
ould
b
e
to
nd
a
problem
whic
h
is
complete
for
the
class
N
L.
Prior
to
that,
w
e
m
ust
mak
e
sure
that
the
concept
of
completness
is
indeed
meaningful
for
the
class
N
L.
The
follo
wing
prop
ositions
ensure
exactly
that.
Prop
osition
..
If
L
is
lo
g-sp
ac
e
r
e
ducible
to
L
0
and
L
0
is
solvable
in
lo
g-sp
ac
e
then
L
is
solvable
in
lo
g-sp
ac
e.
Pro
of:
Since
L
0
is
solv
able
in
logarithmic
space,
there
exists
a
mac
hine
M
0
whic
h
decides
L
0
using
logarithmic
space.
F
urthermore,
since
L
is
log-space
reducible
to
L
0
,
there
exists
a
function
f
()
computable
in
log-space,
suc
h
that
x

L
,
f
(x)

L
0
and
so
there
exists
a
mac
hine
M
suc
h
that
for
ev
ery
input
x
w
ould
rst
compute
f
(x)
and
then
w
ould
sim
ulate
M
0
in
order
to
decide
on
f
(x),
b
oth
actions
demanding
log-space
(as
l
g
(jf
(x)j)

l
g
(exp(l
g
(jxj)))
=
O
(l
g
(jxj)))
and
ensuring
that
M
w
ould
accept
x
i
x

L.
In
terestingly
,
suc
h
reduction
also
preserv
e
non-deterministic
space:

0
LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
Prop
osition
..
If
L
is
lo
g-sp
ac
e
r
e
ducible
to
L
0
and
L
0

N
L
than
L

N
L
Instead
of
pro
ving
the
last
prop
osition,
w
e
will
pro
v
e
a
related
prop
osition
regarding
Non-
Deterministic
Time:
Prop
osition
..
If
L
if
Karp-R
e
ducible
to
L
0
and
L
0

N
P
then
L

N
P
Pro
of:
Since
L
is
Karp-Reducible
to
L
0
,
there
is
a
man
y-to-one
function
f
(),
computable
in
p
olynomial
time,
suc
h
that:
x

L
,
f
(x)

L
0
.
F
urthermore,
since
L
0
is
in
N
P
,
there
is
a
Non-Deterministic
T
uring
mac
hine
M
0
that
can
guess
a
witness
y
for
an
input
z
(the
length
of
y
is
a
p
olynomial
in
the
size
of
z)
in
p
olynomial
time
suc
h
that
R
L
0
(z
;
y
)
holds
(where
R
L
0
is
the
relation
that
denes
the
language
L
0
in
N
P
).
W
e
will
construct
a
Non-Determinstic
mac
hine
M
for
deciding
L
in
the
follo
wing
w
a
y:
F
or
a
giv
en
input
x

L,
M
will
compute
f
(x)
(deterministically
in
p
olynomial
time)
and
then
w
ould
just
sim
ulate
M
0
(men
tioned
ab
o
v
e)
on
input
f
(x)
to
nd
a
witness
y
(non-deterministically
in
p
olynomial
time)
suc
h
that
R
L
0
(f
(x);
y
)
w
ould
hold.
Th
us,
M
denes
a
relation
R
L
suc
h
that
for
ev
ery
input
x

L,
it
guesses
a
witness
y
(non-determinstically
in
p
olynomial
time)
suc
h
that
R
L
(x;
y
)
holds
(i.e.,
R
L
(x;
y
)
=
R
L
0
(f
(x);
y
)).
So
b
y
denition,
L
is
in
N
P
.
W
e
can
use
the
pro
of
of
Prop
osition
..
to
pro
v
e
Prop
osition
..:
Instead
of
a
function
f
()
computable
in
p
olynomial
time,
w
e
are
guarn
teed
to
ha
v
e
a
function
f
()
whic
h
is
computed
in
logarithmic
space.
F
urthermore
w
e
ma
y
presume
the
existence
of
a
mac
hine
M
0
deciding
the
language
L
0
in
logarithmic
space
(instead
of
non-determinstic
p
olynomial
time).
It
is
no
w
clear,
that
one
ma
y
construct
a
non-deterministic
mac
hine
M
whic
h
ma
y
decide
the
language
L
in
logarithmic
space
(whic
h
is
analagous
to
the
mac
hine
M
whic
h
decided
L
in
non-deterministic
p
olynomial
time).
Note
that
requiring
the
existence
of
a
Co
ok-Reduction
instead
of
a
Karp-Reduction
in
Prop
osi-
tion
..
w
ould
probably
mak
e
this
prop
osition
false:
This
stems
from
the
fact
that
if
a
language
L
is
Co
ok-Reducible
to
a
language
L
0

N
P
it
do
es
not
necessarily
mean
that
L

N
P
.
In
par-
ticular,
an
y
coN
P
language
is
Co
ok-Reducible
to
its
complemen
t.
Still,
if
N
P
=
co
N
P
w
e
ha
v
e
that
S
AT
=

N
P
(and
y
et
S
AT
is
reducible
to
SA
T).
W
e
conclude
that
if
N
P
=
coN
P
then
Co
ok
reductions
are
strictly
more
p
o
w
erful
than
Karp
reductions
(since
the
class
of
languages
whic
h
are
Co
ok-reducible
to
N
P
con
tains
co
N
P
,
whereas
the
languages
whic
h
are
Karp-reducible
to
N
P
are
exactly
N
P
).
A
more
trivial
example
of
this
dierence
in
p
o
w
er
is
the
fact
that
an
y
language
in
P
is
Co
ok-reducible
to
the
empt
y
set,
whereas
only
the
empt
y
set
is
Karp-reducible
to
the
empt
y
set.
Ho
w
ev
er,
in
the
next
prop
osition,
as
w
ell
as
in
Prop
osition
..,
a
Co
ok-reduction
w
ould
do:
Prop
osition
..
If
L
is
p
olynomial-time
r
e
ducible
to
L
0
and
L
0

P
then
L

P
In
this
last
prop
osition,
if
L
w
ould
b
e
Co
ok-Reducible
to
L
0
then
it
is
clear
that
the
mac
hine
that
em
ulates
the
oracle
mac
hine
and
answ
eres
the
queries
b
y
sim
ulating
the
macine
that
decides
L
0
(and
runs
in
p
olynomial
time),
w
ould
b
e
a
p
olynomial-time
mac
hine
that
decides
L
(here
the
use
of
the
oracle
on
L
0
and
its
actual
sim
ulation
didn't
mak
e
a
dierence
in
the
running
time).
An
analouge
argumen
t
applies
to
Prop
osition
...
That
is,
if
there
exists
a
log-space
oracle
mac
hine
whic
h
deecides
L
b
y
making
p
olynomially-b
ounded
queries
to
L
0
,
and
L
0
is
solv
able
in
log-space
then
so
is
L
(actually
this
follo
ws
from
Lemma
..).

..
A
COMPLETE
PR
OBLEM
F
OR
N
L

..
The
complete
problem:
directed-graph
connectivit
y
The
problem
that
w
e
will
study
w
ould
b
e
graph
connectivit
y
(denoted
as
CONN)
whic
h
w
e
dene
next:
Denition
.
(directed
connectivit
y
{
CONN):
C
O
N
N
is
the
is
dene
d
as
a
set
of
triples,
(G;
v
;
u),
wher
e
G
=
(V
;
E
)
is
a
dir
e
cte
d
gr
aph,
v
;
u

V
ar
e
two
vertic
es
in
the
gr
aph
so
that
ther
e
is
is
a
dir
e
cte
d
p
ath
fr
om
v
to
u
in
G
As
w
e
shall
see,
the
problem
CONN
is
a
natural
problem
to
study
in
the
con
text
of
space
complexit
y
.
In
tuitiv
ely
,
a
computation
of
a
T
uring
mac
hine
(deterministic
or
not)
on
some
xed
input,
could
allw
a
ys
b
e
pictured
as
a
graph
with
no
des
realting
to
the
mac
hine
congurations
and
edges
relating
to
transitions
b
et
w
een
congurations.
Th
us
the
question
of
whether
there
exists
a
certain
accepting
computation
in
the
mac
hine
reduces
to
the
question
of
the
existence
of
a
certain
directed
path
in
a
graph:
that
path
whic
h
connects
the
no
de
whic
h
corresp
onds
to
the
initial
conguration
and
the
no
de
whic
h
corresp
onds
to
an
accepting
conguration
(on
a
certain
input).
W
e
note
that
in
a
deterministic
mac
hine
the
out-degree
of
eac
h
no
de
in
the
graph
w
ould
b
e
exactly
one,
while
in
a
non-determinstic
mac
hine
the
out-degree
of
eac
h
no
de
could
b
e
an
y
non-negativ
e
n
um
b
er
(b
ecause
of
the
p
ossibilt
y
of
the
non-determinstic
mac
hine
to
mo
v
e
to
an
y
one
of
a
certain
congurations),
ho
w
ev
er
in
b
oth
cases
the
out-degree
of
eac
h
no
de
is
constan
t
(dep
ending
only
on
the
mac
hine).
Con
tin
uing
this
line
of
though
t,
it's
not
hard
to
see
that
CONN
could
b
e
pro
v
ed
to
b
e
complete
for
the
class
N
L
,
i.e.
it
is
itself
in
N
L
and
ev
ery
mac
hine
in
N
L
could
b
e
reduced
to
it.
The
details
are
pro
v
ed
in
the
follo
wing:
Theorem
.
C
O
N
N
is
N
L-Complete.
Oded's
Note:
The
follo
wing
pro
of
is
far
to
o
detailed
to
m
y
taste.
The
basic
ideas
are
v
ery
simple.
Firstly
,
it
is
easy
to
design
a
non-deterministic
log-space
mac
hine
whic
h
accepts
C
O
N
N
b
y
just
guessing
an
adequate
directed
path.
Secondly
,
it
is
easy
to
reduce
an
y
language
L

N
L
to
C
O
N
N
b
y
just
considering
the
directed
graph
of
congurations
of
a
log-space
mac
hine
(accepting
L)
on
the
giv
en
input,
denoted
x.
Eac
h
suc
h
conguration
consists
of
a
lo
cation
on
the
input-tap
e,
a
lo
cation
of
the
w
ork-tap
e,
the
con
ten
ts
of
the
w
ork-tap
e
and
the
state
of
the
mac
hine.
A
directed
edge
leads
from
one
congurations
to
another
i
they
are
p
ossible
consequetiv
e
conguration
of
a
computation
on
input
x.
The
k
ey
p
oin
t
is
that
the
edge
relation
can
b
e
determined
easily
b
y
examining
the
t
w
o
congurations
and
the
relev
an
t
bit
of
x
(p
oin
ted
to
in
the
rst
conguration).
Pro
of:
First
w
e
sho
w
that
C
O
N
N

N
L
(see
Denition
.).
W
e
will
build
a
mac
hine
M
that
w
ould
decide
for
the
input
G
=
(V
;
E
)
and
v
;
u

V
whether
there
exists
a
path
in
G
from
v
to
u.
Of
course,
M
w
ould
do
so
non-determinstically
in
O
(l
og
(n))
space
where
n
is
the
size
of
the
input.
The
outline
of
the
algorithm
is
as
follo
ws:
W
e
start
with
the
no
de
v
(giv
en
at
the
input)
and
a
coun
ter
whic
h
is
initialized
to
the
n
um
b
er
of
no
des
in
G.
A
t
eac
h
step
w
e
decremen
t
the
coun
ter
and
guess
a
no
de
whic
h
is
adjacen
t
to
the
curren
t
no
de
w
e
ha
v
e
(initially
v
).
If
the
no
de
w
e
ha
v
e
guessed
is
not
adjacen
t
to
the
no
de
w
e
hold,
w
e
just
reject.
No
harm
is
done
since
this
is
a
non-deterministic
computation:
it
suces
that
there
will
b
e
some
computation
that
w
ould
accept.
This
pro
cedure
concludes
when
either
the
coun
ter
reac
hes
0
(the
path
shold
not
b
e
longer
than
the
n
um
b
er
of
no
des)
or
the
last
no
de
w
e
ha
v
e
guessed
is
u
(the
other
no
de
sp
ecied
in
the
input).
The
actual
guessing
of
a
no
de
could
b
e
done
in
sev
eral
w
a
ys,
one
w
ould
b
e
to
implemen
t
a
small
pro
cedure
that
w
ould
non-deterministically
write
on
the
w
ork
tap
e
sym
b
ols
that
w
ould


LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
enco
de
some
no
de
(b
y
scanning
the
list
of
edges
or
adjacency
matrix
whic
h
are
part
of
the
input).
Then,
it
will
just
c
hec
k
wheather
the
no
de
it
had
written
is
adjacen
t
to
the
curren
t
no
de
w
e
hold.
Correctness
and
complexit
y
analysis
follo
w
the
formal
sp
ecication
of
the
algorithm:
Input:
G
=
(V
;
E
),
v
;
u

V
T
ask:
Find
whether
there
exists
a
directed
path
from
v
to
u
in
G.
.
x
 
v
.
coun
ter
 
jV
j
.
rep
eat
.
decremen
t
coun
ter
b
y

.
guess
a
no
de
y

V
s.t.
(x;
y
)

E
.
if
y
=
u
then
x
 
y
.
un
til
y
=
u
or
coun
ter
=
0
.
if
y
=
u
then
accept,
else
reject
First
w
e
will
pro
v
e
the
correctness
of
this
algorithm:
On
the
one
hand,
supp
ose
that
the
algorithm
accepts
the
input
G
=
(V
;
E
);
v
;
u

V
.
This
implies
that
during
the
rep
eat-un
til
lo
op,
the
algorithm
has
guessed
a
sequence
of
no
des
suc
h
that
eac
h
one
of
them
had
a
directed
edge
to
its
successor
and
that
the
nal
no
de
in
this
sequence
is
u
and
the
initial
no
de
is
v
(from
the
rst
line
and
the
c
hec
k
that
is
made
in
the
last
line).
Clearly
,
this
implies
a
directed
path
from
v
to
u
in
G
(Note
that
from
the
existence
of
the
coun
ter,
the
n
um
b
er
of
steps
in
this
computation
is
b
ounded
b
y
O
(n)).
On
the
other
hand,
supp
ose
that
there
is
a
directed
path
in
G
from
v
to
u.
This
path
is
a
sequence
fv
;
x

;
:
:
:
;
x
k
;
ug
where
k

(n
 )
and
there
is
a
directed
edge
from
eac
h
no
de
in
this
sequence
to
its
successor.
In
this
case
it
is
clear
that
the
computation
of
the
algorithm
ab
o
v
e
in
whic
h
it
guesses
eac
h
one
of
the
no
des
in
the
sequence
starting
from
v
(from
the
rst
line
of
the
algorithm)
and
ending
with
u
(from
the
last
line
of
the
computation)
is
an
accepting
computation,
and
th
us
the
algorithm
w
ould
accept
the
input
G
=
(V
;
E
);
v
;
u

V
.
W
e
conclude
that
there
is
an
accepting
computation
of
the
algorithm
ab
o
v
e
on
the
input
G
=
(V
;
E
);
v
;
u

V
i
there
is
a
directed
path
in
G
from
v
to
u.
All
that
is
left
to
b
e
sho
wn
is
that
the
implemen
tation
of
suc
h
an
algorithm
in
a
non-determinstic
mac
hine
w
ould
require
no
more
than
a
logarithmic
space
in
the
size
of
the
input:
First,
it
is
clear
that
eac
h
one
of
the
v
ariables
required
to
represen
t
a
no
de
in
the
graph
need
not
b
e
represen
ted
in
more
than
logarithmic
n
um
b
er
of
cells
in
the
size
of
the
input
(for
example,
in
order
to
represen
t
a
n
um
b
er
n
in
binary
notation,
w
e
need
no
more
than
l
g
(n)
bits).
The
same
argumen
t
applies
to
the
coun
ter
whic
h
has
to
coun
t
a
n
um
b
er
whic
h
is
b
ounded
b
y
the
size
of
the
input.
Secondly
,
all
other
data
b
esides
the
v
ariables
ma
y
b
e
k
ept
at
a
constan
t
n
um
b
er
of
cells
of
the
w
ork-tap
e
(for
example
a
bit
that
w
ould
indicate
whether
y
=
u
etc.).
As
w
as
sp
ecied
ab
o
v
e
regarding
the
implemen
tation
of
step

(the
guessing
of
the
no
de),
the
actuall
guessing
pro
cedure,
whic
h
w
ould
b
e
done
non-determinstically
,
uses
n
um
b
er
of
cells
whic
h
is
equal
exactly
to
the
length
required
to
the
represen
tation
of
a
no
de
(whic
h
is
again
logarithmic
in
the
size
of
the
input).
W
e
conclude
that
the
implemen
tation
of
the
algorithm
ab
o
v
e
on
a
non-determinstic
mac
hine
M
requires
a
logarithmic

..
A
COMPLETE
PR
OBLEM
F
OR
N
L

space
in
the
size
of
the
input,
and
so
w
e
ma
y
conclude
that
the
mac
hine
M
decides
CONN
in
non-deterministic
logrithmic
space,
i.e.,
C
O
N
N

N
L.
No
w
w
e
need
to
sho
w
that
ev
ery
language
L

N
L
is
log-space
reducible
to
CONN.
Let
L
b
e
a
language
in
N
L,
then
there
is
a
non-deterministic
logarithmic
space
mac
hine
M
that
decides
L.
W
e
will
sho
w
that
for
ev
ery
input
x,
w
e
can
build
in
nondeterministic
logarithmic
space
an
input
(G
=
(V
;
E
);
star
t

V
;
end

V
)
(whic
h
is
a
function
of
mac
hine
M
and
input
x)
suc
h
that
there
is
a
path
in
G
from
star
t
to
end
if
and
only
if
M
accepts
input
x.
The
graph
G
w
e
will
construct
w
ould
simply
b
e
the
graph
of
all
p
ossible
congurations
of
M
giv
en
x
as
an
input.
That
is,
the
no
des
denote
dieren
t
congurations
of
M
while
computing
on
input
x,
and
the
arcs
denote
p
ossible
immediate
transitions
b
et
w
een
congurations.
The
graph
is
constructed
(deterministic)
log-space
as
follo
ws;
Input:
An
input
string
x
(the
mac
hine
M
is
xed)
T
ask:
Output
a
graph
G
=
(V
;
E
)
and
t
w
o
no
des
v
;
u

V
suc
h
that
there
is
a
path
from
v
to
u
in
the
graph
i
x
is
accepted
b
y
M
.
.
compute
n,
the
n
um
b
er
of
dieren
t
congurations
of
M
while
computing
input
x
.
for
i
=

to
n
.
for
j
=

to
n
.
if
there
is
a
transition
(b
y
a
single
step
of
M
)
from
conguration
n
um
b
er
i
to
congu-
ration
n
um
b
er
j
ouput

otherwise
output
0
.
output

and
n
First
w
e
will
sho
w
that
this
pro
cedure
indeed
outputs
the
represen
tation
of
a
graph
and
t
w
o
no
des
in
that
graph
suc
h
that
there
exists
a
directed
path
b
et
w
een
those
t
w
o
no
des
i
the
input
x
is
accepted
b
y
mac
hine
M
.
In
the
rst
line
w
e
compute
the
n
um
b
er
of
all
p
ossible
congurations
of
mac
hine
M
while
computing
on
input
x.
Then,
w
e
consider
ev
ery
ordered
pair
of
congurations
(represen
ted
b
y
n
um
b
ers
b
et
w
een

and
n)
and
output

i
there
is
indeed
a
directed
transition
b
et
w
een
those
t
w
o
congurations
in
the
computation
of
M
on
x.
Our
underlying
assumption
is
that

represen
ts
the
initial
conguration
and
n
represen
ts
the
(only)
accepting
conguration
(if
there
w
ere
sev
eral
accepting
congurations
w
e
dene
a
new
one
and
dra
w
edges
from
the
previous
accepting
congurations
to
the
new
one).
Th
us,
the
output
of
the
ab
o
v
e
pro
cedure
is
simply
an
adjacency
matrix
of
a
graph
in
whic
h
eac
h
of
its
no
des
corresp
ond
to
a
unique
conguration
of
M
while
computing
on
input
x
,
and
a
directed
edge
exists
b
et
w
een
t
w
o
no
des
i
and
j
i
there
is
a
(direct)
transition
in
M
b
et
w
een
the
conguration
represen
ted
b
y
x
and
the
conguration
represen
ted
b
y
y
.
It
is
no
w
clear
that
a
directed
path
from
the
rst
no
de
(according
to
our
en
umeration)
to
the
last
no
de
in
the
graph
w
ould
corresp
ond
to
an
accepting
computation
of
mac
hine
M
on
input
x,
and
that
suc
h
a
path
w
ould
not
exists
should
there
b
e
no
suc
h
accepting
computation.
Next,
w
e
m
ust
sho
w
that
the
ab
o
v
e
pro
cedure
could
indeed
b
e
carried
out
using
no
more
than
a
logarithmic
space
in
the
length
of
the
input
(i.e.,
the
input
x).
In
order
to
do
that,
w
e
will
sho
w
that
the
n
um
b
er
of
dieren
t
congurations
of
M
while
computing
x
is
p
olynomial
in
the
length
of
x.
This
w
ould
imply
that
in
order
to
coun
t
these
congurations
w
e
need
no
more
than
a
logarithmic
space
in
the
length
of
x.
So,
w
e
will
coun
t
the
n
um
b
er
of
p
ossible
congurations


LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
of
M
while
computing
on
a
giv
en
input
x.
That
n
um
b
er
w
ould
b
e
the
n
um
b
er
of
p
ossible
states
(a
constan
t
determined
b
y
M
)
m
ultiplied
b
y
the
n
um
b
er
of
p
ossible
con
ten
ts
of
the
w
ork-tap
e
whic
h
is
j
M
j
O
(l
og
(n))
where

M
is
the
alphab
et
of
M
and
is
also
a
constan
t
determined
b
y
M
(let
us
remem
b
er
that
since
M
is
log-space
b
ounded,
the
n
um
b
er
of
used
w
ork-tap
e
squares
could
not
surpass
O
(l
og
(n))),
m
ultiplied
b
y
the
n
um
b
er
of
dieren
t
p
ositions
of
the
reading
head
on
the
input
tap
e
whic
h
is
n
and
nally
,
m
utiplied
b
y
the
n
um
b
er
of
dieren
t
p
ossible
p
ositions
of
the
reading
head
on
the
w
ork-tap
e
whic
h
is
O
(l
og
(n)).
All
in
all,
the
n
um
b
er
of
dieren
t
congurations
of
M
while
computing
input
x
is:
jS
tates
M
j

j
M
j
O
(l
og
(n))

n

O
(l
og
(n))
=
O
(n
k
)
(where
k
is
a
constan
t).
That
is,
the
n
um
b
er
of
dieren
t
congurations
of
M
while
computing
input
x
is
p
olynomial
in
the
length
of
x.
W
e
ma
y
conclude
no
w
that
the
initial
action
in
the
pro
cedure
ab
o
v
e,
that
of
coun
ting
the
n
um
b
er
of
dieren
t
congurations
of
M
while
computing
on
input
x
could
b
e
carried
out
in
logarithmic
space
in
the
length
of
x.
Secondly
,
w
e
sho
w
that
the
pro
cedure
of
c
hec
king
whether
there
exists
a
(direct)
transition
b
et
w
een
t
w
o
congurations
represen
ted
b
y
t
w
o
in
tegers
can
b
e
implemen
ted
as
w
ell
in
logarithmic
space:
W
e
sho
w
that
there
is
a
mac
hine
M
00
that
receiv
es
as
inputs
t
w
o
in
tegers
and
w
ould
return
a
p
ositiv
e
answ
er
if
and
only
if
there
is
a
(direct)
transition
of
M
b
et
w
een
the
t
w
o
congurations
whic
h
are
represen
ted
b
y
those
in
tegers.
Note
rst
that
in
tegers
corresp
ond
to
strings
o
v
er
some
con
vinien
t
alphab
et
and
that
suc
h
strings
corresp
ond
to
congurations.
(The
corresp
ondance
is
b
y
trivial
computations.)
Th
us,
all
w
e
need
to
determine
is
whether
the
xed
mac
hine
M
can
pass
in
one
(non-deterministic)
step,
on
input
x,
b
et
w
een
a
giv
en
pair
of
congurations.
This
dep
ends
only
on
the
transition
function
of
M
,
whic
h
M
00
has
hard-wired,
and
on
a
single
bit
in
the
input
x;
that
is,
the
bit
the
lo
cation
of
whic
h
is
indicated
in
the
rst
of
the
t
w
o
giv
en
congurations.
That
is,
supp
ose
that
the
rst
conguration
is
of
the
form
(i;
j;
w
;
s),
where
i
is
a
lo
cation
on
the
input-tap
e,
j
a
lo
cation
on
the
w
ork-tap
e,
w
the
con
ten
ts
of
the
w
ork-tap
e
and
s
the
mac
hine's
state.
Same
for
the
second
conguration,
denoted
(i
0
;
j
0
;
w
0
;
s
0
).
Then
w
e
c
hec
k
if
M
when
reading
sym
b
ol
x
i
(the
i
th
bit
of
x)
from
its
input
tap
e,
and
w
j
(the
j
th
sym
b
ol
of
w
)
from
its
w
ork-tap
e
can
mak
e
a
single
transition
resulting
in
the
conguration
(i
0
;
j
0
;
w
0
;
s
0
).
In
particular,
it
m
ust
hold
that
i
0

fi
 ;
i;
i
+
g,
j
0

fj
 ;
j;
j
+
g,
and
w
0
diers
from
w
at
most
on
lo
cation
j
.
F
urthermore,
these
small
c
hanges
m
ust
dep
end
on
the
transition
function
of
M
.
Since
there
is
a
constan
t
n
um
b
er
of
p
ossible
transitions
(in
M
's
transition
function),
w
e
ma
y
just
c
hec
k
all
of
them.
W
e
ha
v
e
sho
wn
that
the
ab
o
v
e
pro
cedure
outputs
a
represen
tation
of
a
graph
and
t
w
o
no
des
in
that
graph
for
a
mac
hine
M
and
input
x,
suc
h
that
there
is
a
directed
path
b
et
w
een
the
no
des
i
there
is
an
accepting
computation
of
M
on
x.
F
urthermore
w
e
ha
v
e
sho
wn
that
this
pro
cedure
ma
y
b
e
implemen
ted
requiring
no
more
than
a
log-space
in
the
size
of
the
input
(the
input
string
x)
whic
h
concludes
the
pro
of.
.
Complemen
ts
of
complexit
y
classes
Denition
.
(complemen
t
of
a
language):
L
et
L

f0;
g

b
e
a
language.
The
complement
of
a
language
L,
denote
d
L
is
the
language
f0;
g

nL.
T
o
mak
e
this
denition
more
accurate,
w
e
assume
that
ev
ery
w
ord
in
f0;
g

represen
ts
an
instance
of
the
problem.
Example
.
:
C
O
N
N
is
the
fol
lowing
set:
f(G;
u;
v
)
:
G
is
a
dir
e
cte
d
gr
aph,
u;
v

V
(G),
ther
e
is
no
dir
e
cte
d
p
ath
fr
om
u
to
v
in
G
g.

..
IMMERMAN
THEOREM:
N
L
=
CON
L

Denition
.
(complemen
t
of
class):
L
et
C
b
e
a
c
omplexity
class.
The
complement
of
the
class
C
is
denote
d
coC
and
is
dene
d
to
b
e
fL
:
L

C
g
It
is
immediately
ob
vious
that
if
C
is
a
deterministic
time
or
space
complexit
y
class,
then
coC
=
C
,
in
particular,
P
=
coP
.
This
is
true,
since
w
e
can
c
hange
the
result
of
the
derministic
mac
hine
from
'y
es'
to
'no'
and
vice
v
ersa.
Ho
w
ev
er,
in
non-deterministic
complexit
y
classes,
this
metho
d
do
es
not
w
ork.
Let
M
b
e
a
T
uring
Mac
hine
that
accepts
a
language
L
non-deterministicly
.
If
x

L,
then
there
is
at
least
one
successful
computation
of
M
on
x
(i.e.,
there
is
a
succinct
v
erication
that
x

L).
W
e
denote
b
y
M
the
non-deterministic
T
uring
Mac
hine
that
do
es
the
same
as
M
,
but
replaces
the
output
from
\y
es"
to
\no"
and
vice
v
ersa.
Hence,
if
the
new
mac
hine
M
accepts
an
input
z
,
there
is
one
accepting
computation
for
M
on
z
,
i.e.
non-accepting
computation
in
M
(b
y
denition).
In
other
w
ords,
M
will
accept
z
,
if
M
has
some
unsuccessful
guesses
to
pro
v
e
that
z

L.
This,
ho
w
ev
er,
do
es
not
mean
that
M
accepts
L,
since
z
could
p
ossibly
b
e
in
L
b
y
other
guesses
of
the
mac
hine
M
.
F
or
example
w
e
don't
kno
w
whether
coN
P
is
equal
to
N
P
.
The
conjecture
is
that
N
P
=
co
N
P
Y
et,
in
the
particular
case
of
nondeterministic
space
equalit
y
do
es
hold.
It
can
b
e
pro
v
en
that
an
y
non-deterministic
space
N
S
P
AC
E
(s(n))
for
s(n)

l
og
(n)
is
closed
under
complemen
tation.
This
result
whic
h
w
as
pro
v
en
b
y
Neil
Immerman
in
	,
is
going
to
b
e
pro
v
en
here
for
the
case
N
L.
By
the
follo
wing
prop
osition,
it
suces
to
sho
w
that
C
O
N
N

co
N
L
(or
equiv
alen
tly
C
O
N
N

N
L.
Prop
osition
..
:
If
for
an
N
L-c
omplete
language
L
it
holds
that
L

coN
L
then
N
L
=
coN
L.
Pro
of:
Let
L
0
b
e
a
language
in
N
L
.
Since
L

N
L-complete,
w
e
ha
v
e
a
log-space
reduction
f
from
L
0
to
L
(see
Denition
.).
The
function
f
satises:
x

L
0
,
f
(x)

L
T
aking
the
opp
osite
direction,
w
e
get:
x

L
0
,
f
(x)

L
By
the
denition
of
the
reduction,
f
is
also
a
reduction
from
L
0
to
L.
By
prop
osition
..
w
e
kno
w
that
since
L

N
L
(b
ecause
b
y
h
yp
othesis
L

co
N
L
)
then
L
0

N
L
(i.e.
L
0

co
N
L).
W
e
conclude
that
for
ev
ery
L
0

N
L,
L
0

N
L
,
th
us
N
L
=
co
N
L
.
.
Immerman
Theorem:
N
L
=
co
N
L
In
this
section,
w
e
are
going
to
pro
v
e
a
surprising
theorem,
whic
h
claims
that
non-deterministic
log-space
is
closed
under
complemen
tation.
Due
to
the
pro
of
in
Theorem
.
that
C
O
N
N

N
L-
complete,
and
using
prop
osition
..,
w
e
only
need
to
pro
v
e
that
C
O
N
N

N
L,
where
C
O
N
N
is
the
complemen
tary
problem
of
C
O
N
N
as
dened
in
Example
..
F
ormally
,
The
decision
problem
of
the
language
C
O
N
N
,
is
obtained
as
the
follo
wing:
Input:
a
directed
graph
G
=
(V
;
E
)
and
t
w
o
no
des
u;
v

V
(G).
Question:
Is
there
no
directed
path
from
v
to
u
?
In
order
to
sho
w
that
C
O
N
N

N
L
,
w
e
use
the
follo
wing
theorem,
whic
h
will
b
e
pro
v
en
later:


LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
Theorem
.	
Given
a
dir
e
cte
d
gr
aph
G
=
(V
;
E
)
and
a
no
de
v

V
(G),
the
numb
er
of
no
des
r
e
achable
fr
om
v
in
G
c
an
b
e
c
ompute
d
by
a
non-deterministic
T
uring
Machine
within
lo
g-sp
ac
e.
A
non-deterministic
T
uring
Mac
hine
that
computes
a
function
f
of
the
input,
as
in
the
theorem
ab
o
v
e,
is
dened
as
follo
ws:
Denition
.0
(non-deterministic
computation
of
functions):
A
non-deterministic
T
uring
Ma-
chine
M
is
said
to
c
ompute
a
function
f
if
on
any
input
x,
the
fol
lowing
two
c
onditions
hold:
.
either
M
halts
with
the
right
answer
f
(x),
or
M
halts
with
output
\failur
e";
and
.
at
le
ast
one
of
the
machine
c
omputations
halts
with
the
right
answer.
..
Theorem
.	
implies
N
L
=
coN
L
Lemma
..
Assuming
The
or
em
.	,
C
O
N
N

N
L
Assuming
Theorem
.	,
w
e
ha
v
e
a
non-deterministic
T
uring
Mac
hine
denoted
C
R
(C
R
def
=
Coun
t
Reac
hable)
that
coun
ts
all
the
no
des
that
are
reac
hable
in
a
directed
graph
G
from
a
single
no
de
v
in
non-deterministic
log-space.
The
idea
of
the
pro
of
is
that
once
w
e
kno
w
to
compute
using
C
R
the
n
um
b
er
of
no
des
reac
hable
from
v
in
G,
w
e
can
also
non-deterministically
scan
all
the
v
ertices
reac
hable
from
v
,
using
this
v
alue.
This
is
done
non-deterministically
b
y
guessing
connected
paths
to
eac
h
of
the
reac
hable
no
des
from
v
.
Once
the
mac
hine
disco
v
ered
all
reac
hable
no
des
from
v
(i.e.
the
n
um
b
er
of
reac
hable
no
des
it
found
equals
the
output
of
C
R
)
and
the
no
de
u
isn't
reac
hable,
it
can
decide
that
there
is
no
connected
path
b
et
w
een
v
and
u
in
G.
Pro
of:
Let
x
=
(G;
u;
v
)
b
e
an
input
for
the
problem
C
O
N
N
.
W
e
x
(G;
v
)
and
giv
e
it
as
an
input
to
the
mac
hine
C
R
,
whic
h
is
the
non-deterministic
mac
hine
that
en
umerates
all
the
no
des
in
the
graph
G
reac
hable
from
the
no
de
v
as
w
as
assumed
b
efore
to
w
ork
in
non-deterministic
log-space.
In
other
w
ords,
w
e
use
C
R
,
as
a
\blac
k
b
o
x".
W
e
construct
a
non-deterministic
mac
hine
here
that
uses
the
follo
wing
sim
ulation
that
solv
es
this
problem:

Firstly
,
it
sim
ulates
C
R
on
the
input
(G;
v
).
If
the
run
fails,
the
mac
hine
rejects.
Otherwise,
w
e
denote
the
answ
er
b
y
N
.

F
or
eac
h
v
ertex
w
in
the
graph,
it
guesses
whether
w
is
reac
hable
from
v
and
if
y
es,
it
guesses
non-deterministicly
a
directed
path
from
v
to
w
,
b
y
guessing
at
most
n
 
v
ertices
(w
e
kno
w
b
y
a
simple
com
binatorial
fact,
that
eac
h
t
w
o
connected
no
des
in
a
grpah
G
=
(V
;
E
)
are
connected
within
path
of
length
less
than
or
equal
to
n
 )
and
v
eries
that
it
is
a
v
alid
path.
F
or
eac
h
correct
path,
it
incremen
ts
a
coun
ter.

If
w
=
u,
and
it
founds
a
v
alid
path
then
it
rejects.

If
counter
=
N
then
the
mac
hine
rejects
the
input,
otherwise
the
mac
hine
accepts
the
in-
put.
(counter
=
N
means
that
not
all
reac
hable
v
ertices
w
ere
found
and
v
eried,
whereas
counter
=
N
means
that
all
w
ere
examined.
If
none
of
these
equals
u
then
w
e
should
indeed
accept).
F
ormally
,
w
e
ha
v
e
the
follo
wing
algorithm:
Input:
G
=
(V
;
E
),
v
;
u

V
(G)
T
ask:
Find
whether
there
is
no
connected
path
b
et
w
een
u
and
v
.

..
IMMERMAN
THEOREM:
N
L
=
CON
L

.
Sim
ulating
C
R
.
If
C
R
fails,
the
algorithm
rejects,
else
N
 
C
R
((G;
v
)).
.
counter
 
0
.
for
w
=

to
n
do
(w
is
a
c
andidate
r
e
achable
vertex)
.
guess
if
w
is
reac
hable
from
v
.
If
not,
pro
ceed
to
next
iteration
of
step
.
(we
c
ontinue
in
steps
-
only
if
we
guesse
d
that
w
is
r
e
achable
fr
om
v
)
.
p
 
0
(c
ounter
for
p
ath
length)
.
v

 
v
(v

is
initial
ly
v
)
.
rep
eat
(guess
and
verify
a
p
ath
fr
om
v
to
w
)
.
p
 
p
+

	.
guess
a
no
de
v

(v

and
v

ar
e
the
last
and
the
curr
ent
no
des)
0.
if
(v

;
v

)

E
then
reject
.
if
v

=
w
then
v

 
v

.
un
til
(v

=
w
)
or
(p
=
n
 )
.
if
(v

=
w
)
then
(c
ounting
al
l
r
e
achable
w
=
u)
.
b
egin
.
counter
 
counter
+

.
if
w
=
u
then
reject
.
end
.
if
N
=
counter
then
reject,
else
accept.
W
e
kno
w
that
C
R
w
orks
in
O
(l
og
(jGj)).
In
eac
h
step
of
the
sim
ulation,
our
algorithm
uses
only

v
ariables
in
addition
of
those
of
C
R
,
namely
the
coun
ters
counter
;
w
;
p,
the
represen
tations
of
the
no
des
v

;
v

and
N
.
the
coun
ters,
and
N
are
b
ounded
b
y
the
n
um
b
er
of
v
ertices,
n.
Ev
ery
new
c
hange
of
one
of
this
v
ariables
will
b
e
written
again
on
the
w
ork
tap
e
b
y
reusing
space.
Therefore,
they
can
b
e
implemen
ted
in
O
(l
og
(n))
space.
The
no
des,
clearly
,
are
represen
ted
in
O
(l
og
(jGj))
space.
Th
us,
w
e
use
no
more
then
O
(l
og
(jGj))
space
in
the
w
ork
tap
e
in
this
mac
hine
(where
x
is
the
input).
The
correctness
is
pro
v
ed
next:
T
o
sho
w
correctness
w
e
need
to
sho
w
that
it
has
a
computation
that
accepts
the
input
if
and
only
if
there
is
no
direct
path
from
v
to
u
in
G.
Consider
rst
the
case
that
the
mac
hine
accepts.
A
necessary
condition
(for
this
ev
en
t)
is
that
counter
=
N
(line
);
that
is,
the
n
um
b
er
of
v
ertices
that
w
ere
found
to
b
e
reac
hable
is
exactly
the
correct
one
(i.e.,
N
).
This
means
that
ev
ery
p
ossible
v
ertex
that
is
reac
hable
from
v
w
as
coun
ted.
But,
if
u
w
as
found
to
b
e
one
of
them
the
mac
hine
should
ha
v
e
rejected
b
efore
(in
line
).
Therefore,
u
cannot
b
e
reac
hable
from
v
b
y
a
directed
path.
Supp
ose,
on
the
other
hand,
that
there
is
no
directed
path
b
et
w
een
u
and
v
in
G.
Then
if
all
guesses
made
are
correct
then
the
mac
hine
will
necessarily
accept.
Sp
ecically
,
w
e
lo
ok
at
a


LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
computation
in
whic
h
()
the
mac
hine
correctly
guesses
(in
line
)
for
eac
h
v
ertex
w
whether
it
is
reac
hable
from
v
;
()
for
eac
h
reac
hable
v
ertex
w
it
guesses
w
ell
a
directed
path
from
v
;
and
()
mac
hine
C
R
did
not
fail
(and
th
us
N
equals
the
n
um
b
er
of
v
ertices
reac
hable
from
v
).
In
this
case
N
=
counter
,
and
since
u
is
not
connected
to
v
the
mac
hine
accepts.
Therefore,
w
e
pro
v
es
the
lemma.
Using
this
result
(under
the
assumption
that
Theorem
.	
is
v
alid),
w
e
obtain
N
L
=
co
N
L
.
Theorem
.
(Immerman
'):
N
L
=
co
N
L
Pro
of:
W
e
pro
v
ed
in
Theorem
.
that
C
O
N
N

N
L-complete.
In
Lemma
..,
w
e
pro
v
ed
that
C
O
N
N

N
L
(or
C
O
N
N

coN
L).
Using
Prop
osition
..,
w
e
get,
that
N
L
=
co
N
L.
An
extension
of
this
theorem
can
sho
w
that
for
an
y
s(n)

l
og
(n),
N
S
P
AC
E
(s(n))
=
coN
S
P
AC
E
(s(n)).
..
Pro
of
of
Theorem
.	
T
o
conclude
this
pro
of
w
e
are
only
left
with
the
pro
of
of
Theorem
.	,
i.e.
the
existness
of
a
mac
hine
C
R
,
that
computes
the
n
um
b
er
of
no
des
reac
hable
from
a
v
ertex
v
in
a
directed
graph
G
=
(V
;
E
).
W
e
use
the
follo
wing
notations
for
a
xed
p
oin
t
v
in
a
xed
directed
graph
G
=
(V
;
E
):
Denition
.
R
j
is
the
set
of
vertic
es
which
ar
e
r
e
achable
fr
om
v
by
a
p
ath
of
length
less
than
or
e
qual
to
j
.
In
addition,
N
j
is
dene
d
to
b
e
the
numb
er
of
no
des
in
R
j
,
namely
jR
j
j.
It
can
b
e
seen
that,
fv
g
=
R
0

R






R
n 
=
R
where
n
denotes
the
n
um
b
er
of
no
des
in
G,
and
R
denotes
the
set
of
v
ertices
reac
hable
from
v
.
There
is
a
strong
connection
b
et
w
een
R
j
and
R
j
 
for
j

,
since
an
y
path
of
length
j
is
a
path
of
length
j
 
with
an
additional
edge.
The
follo
wing
claim
will
b
e
used
later
in
the
dev
elopmen
t
of
the
mac
hine
C
R
:
Claim
..
The
fol
lowing
e
quation
holds:
R
j
=

R
j
 
[
fu
:
w

R
j
 
;
(w
;
u)

E
(G)g
if
j


fv
g
if
j
=
0
Pro
of:
F
or
j
=
0:
Clear
from
denition.
F
or
j

:
R
j
 

R
j
b
y
denition.
fu
:
w

R
j
 
;
(w
;
u)

E
(G)g
represen
ts
all
the
no
des
whic
h
are
adjacen
t
to
R
j
 
,
i.e.
ha
v
e
length
at
most
j
 
+

=
j
.
This
set
is
also
con
tained
in
R
j
.
Th
us,
R
j
 
[
fu
:
w

R
j
 
;
(w
;
u)

E
(G)g

R
j
.
In
the
opp
osite
direction,
ev
ery
no
de
u

R
j
,
whic
h
is
not
v
,
is
reac
hable
from
v
along
a
path
with
length
less
or
equal
to
j
.
Th
us,
its
predecessor
in
this
path
has
length
less
or
equal
to
j
 .
Th
us,
R
j

fu
:
w

R
j
 
;
(w
;
u)

E
(G)g
[
fv
g

R
j
 
[
fu
:
w

R
j
 
;
(w
;
u)

E
(G)g
(since
fv
g

R
j
 
for
an
y
j

).
Therefore,
the
claim
follo
ws.
Corollary
.
F
or
any
j

,
a
no
de
w

R
j
,
if
and
only
if
ther
e
is
a
no
de
r

R
j
 
such
that
r
=
w
or
(r
;
w
)

E
(G).

..
IMMERMAN
THEOREM:
N
L
=
CON
L
	
W
e
no
w
construct
a
non-deterministic
T
uring
Mac
hine,
C
R
,
that
coun
ts
the
n
um
b
er
of
no
des
in
a
directed
graph
G,
reac
hable
from
a
no
de
in
the
graph
v
.
Our
purpuse
in
this
algorithm
is
to
compute
N
n 
where
n
is
the
n
um
b
er
of
no
des
in
G,
to
nd
the
n
um
b
er
of
all
reac
hable
no
des
from
v
.
This
recursiv
e
idea
in
Claim
..
is
the
main
idea
b
ehind
the
follo
wing
algorithm,
whic
h
is
build
iterativ
ely
.
In
eac
h
stage,
the
algorithm
computes
N
j
b
y
using
N
j
 
.
It
has
an
initial
v
alue
N
0
,
whic
h
w
e
kno
w
to
b
e
jfv
gj
=
.
The
itrations
use
the
non-deterministic
p
o
w
er
of
the
mac
hine.
The
high-lev
el
description
of
the
algorithm
is
as
follo
ws:

F
or
eac
h
j
from

to
n
 
,
it
tries
to
calculate
recursiv
ely
N
j
from
N
j
 
.
This
is
done
from
N
0
to
N
n 
,
whic
h
is
the
desired
output.
Here
is
ho
w
N
j
is
computed.
{
F
or
eac
h
no
de
w
in
the
graph,

F
or
eac
h
no
de
r
in
the
graph,
it
guesses
if
r

R
j
 
and
if
the
answ
er
is
y
es,
it
guesses
a
path
with
length
less
than
or
equal
to
j
 ,
from
v
to
r
.
It
v
eries
that
the
path
is
v
alid.
If
it
is,
it
kno
ws
that
r
is
a
no
de
in
R
j
 
.
Otherwise,
it
rejects.
It
coun
ts
eac
h
no
de
r
,
suc
h
that
r

R
j
 
,
b
y
counter
j
 
.
The
mac
hine
c
hec
ks
whether
w
=
r
or
(r
;
w
)

E
(G).
If
it
is,
(b
y
using
Corrolary
.),
w

R
j
,
and
then
it
indicates
b
y
a
ag,
f
l
ag
w
that
w
is
in
R
j
.
(f
l
ag
w
is
initially
0).
{
It
coun
ts
the
n
um
b
er
of
v
ertices
r
in
R
j
 
w
e
found,
and
v
eries
that
it
is
equal
to
N
j
 
.
Otherwise,
it
rejects.
If
the
mac
hine
do
es
not
reject,
w
e
kno
w
that
ev
ery
no
de
r

R
j
 
w
as
found.
Therfore
using
Corrolary
.,
the
mem
b
ership
of
w
in
R
j
is
decided
prop
erly
(i.e.
f
l
ag
w
has
the
righ
t
v
alue).
{
A
t
the
end
of
this
pro
cess
it
sums
up
the
ags,
f
l
ag
w
's
in
to
a
coun
ter,
counter
j
(i.e.
coun
ts
the
n
um
b
er
of
no
des
that
w
ere
found
to
b
e
in
R
j
).

It
stores
the
v
alue
of
counter
j
for
N
j
to
b
egin
a
new
iteration
(or
to
giv
e
the
result
in
case
w
e
reac
h
j
=
n
 ).
W
e
stress
that
all
coun
ters
are
implemen
ted
using
the
same
space.
That
is,
the
only
thing
whic
h
passes
from
iteration
j
 
to
iteration
j
is
N
j
 
.
The
detailed
co
de
follo
ws:
Input:
G
=
(V
;
E
),
v

V
(G)
T
ask:
Find
the
n
um
b
er
of
reac
hable
no
des
from
v
in
G.
.
Computing
n
=
jV
(G)j
.
N
0
 

=
jR
0
j
.
for
j
=

to
n
 
do
.
counter
j
 
0
.
for
w
=

to
n
do
(lines
-
c
ompute
N
j
)
.
counter
j
 
 
0
(w
is
a
p
otential
memb
er
in
R
j
)
.
f
l
ag
w
=
0

0
LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
.
for
r
=

to
n
do
(We
try
to
enumer
ate
R
j
 
using
N
j
 
)
	.
guess
if
r

R
j
 
.
If
not,
pro
ceed
to
next
iteration
of
step
.
(we
c
ontinue
in
steps
0-
only
if
we
guesse
d
that
r

R
j
 
)
0.
v

 
v
(v

is
initial
ly
v
)
.
p
 
0
.
rep
eat
(guess
and
verify
a
p
ath
fr
om
v
to
r
,
such
that
r

R
j
 
)
.
p
 
p
+

.
guess
a
no
de
v

(v

and
v

ar
e
the
last
and
curr
ent
no
des)
.
if
(v

;
v

)

E
then
halt(failure)
.
if
v

=
r
then
v

 
v

.
un
til
(v

=
r
)
or
(p
=
j
 )
.
if
v

=
r
then
halt(failure)
	.
counter
j
 
 
counter
j
 
+

0.
if
(r
=
w
)
or
((r
;
w
)

E
(G))
then
(che
ck
that
w

R
j
)
.
f
l
ag
w
=

.
if
counter
j
 
=
N
j
 
then
halt(failure)
.
counter
j
 
counter
j
+
f
l
ag
w
.
N
j
=
counter
j
.
output
N
n 
Lemma
..
The
machine
C
R
uses
O
(l
og
(n))
sp
ac
e.
Pro
of:
Computing
the
n
um
b
er
of
no
des
(line
)
can
b
e
made
in
at
most
O
(l
og
(n)),
b
y
sim-
ply
coun
ting
the
n
um
b
er
of
no
des
with
a
coun
ter
on
the
input
tap
e.
In
eac
h
other
step
of
the
running
of
the
mac
hine,
it
only
needs
to
kno
w
at
most
ten
v
ariables,
i.e.
the
coun
ters
for
the
'for'
lo
ops:
j;
w
;
r
;
p,
the
v
alue
of
N
j
 
for
the
curren
t
j
,
the
t
w
o
coun
ters
for
the
size
of
the
sets
counter
j
;
counter
j
 
,
t
w
o
no
des
of
the
guessing
part
v

;
v

,
and
the
indicating
ag
f
l
ag
w
.
Oded's
Note:
The
pro
of
migh
t
b
e
more
con
vincing
if
the
co
de
w
as
mo
died
so
that
N
P
RE
V
is
used
instead
of
N
j
 
,
counter
C
U
RR
instead
of
counter
j
,
and
counter
P
RE
V
instead
of
counter
j
 
.
In
suc
h
a
case,
line

will
prepare
for
the
next
iteration
b
y
setting
N
P
RE
V
 
counter
C
U
RR
.
Suc
h
a
description
will
b
etter
emphasise
the
fact
that
w
e
use
only
a
constan
t
n
um
b
er
of
v
ariables,
and
that
their
storage
space
is
re-used
in
the
iterations.

..
IMMERMAN
THEOREM:
N
L
=
CON
L

Whenev
er
is
needed
to
c
hange
information,
lik
e
increasing
a
coun
ter,
or
c
hanging
a
v
ariable,
it
reuses
the
space
it
needs
for
this
goal.
Ev
ery
coun
ter
w
e
use
coun
ts
no
more
than
the
n
um
b
er
of
no
des
in
the
graph,
hence
w
e
can
implemen
t
eac
h
one
of
them
in
O
(l
og
(n))
space.
Eac
h
no
de
is
assumed
to
tak
e
O
(l
og
(n))
to
store,
i.e.
its
n
um
b
er
in
the
list
of
no
des.
And
the
f
l
ag
w
clearly
tak
es
only

bit.
Therefore,
to
store
these
v
ariables,
it
is
enough
to
use
O
(l
og
(n))
space.
Except
for
these
v
ariables,
w
e
don't
use
an
y
additional
space.
All
that
is
done
is
comparing
no
des
with
the
input
tap
e,
and
c
hec
king
whether
t
w
o
no
des
are
adjacen
t
in
the
adjecancy
matrix
that
represen
ts
the
graph.
These
op
erations
can
b
e
done
only
b
y
scanning
the
input
tap
e,
and
tak
e
no
more
then
O
(l
og
(n))
space,
for
coun
ters
that
scan
the
matrix.
Therefore,
this
non-deterministic
T
uring
Mac
hine
uses
only
O
(l
og
(n))
or
O
(l
og
(jxj))
where
x
=
(G;
v
)
is
the
input
to
the
mac
hine.
Lemma
..
If
the
machine
C
R
outputs
an
inte
ger,
then
it
c
orr
e
ctly
gives
the
r
esult
of
N
n 
.
Pro
of:
W
e'll
pro
v
e
it
b
y
induction
on
the
iteration
of
computing
N
j
:
F
or
j
=
0:
It
is
ob
viously
correct.
If
it
computes
correctly
N
j
 
,
and
it
did
not
halt
while
computing
N
j
,
then
it
computes
correctly
N
j
as
w
ell:
By
the
assumption
of
the
induction
w
e
ha
v
e
a
computation
that
computes
N
j
 
,
that
is
stored
correctly
.
All
w
e
ha
v
e
to
pro
v
e
is
that
counter
j
is
incremen
ted
if
and
only
if
the
curren
t
w
is
indeed
in
R
j
(line
),
since
then
N
j
will
ha
v
e
correctly
the
n
um
b
er
of
no
des
in
R
j
.
Since
the
mac
hine
didn't
failed
till
no
w,
counter
j
 
has
to
b
e
equal
to
N
j
 
(line
),
b
y
the
assumption
of
the
induction.
This
means
that
the
mac
hine
indeed
found
all
r

R
j
 
,
since
counter
j
 
is
incremen
ted
for
eac
h
no
de
that
is
found
to
b
e
in
R
j
 
(line
	).
Therefore,
using
Corrolary
.,
w
e
kno
w
that
the
mac
hine
c
hanges
the
ag,
f
l
ag
w
,
of
a
no
de
w
if
only
if
w

R
j
.
And
this
ag
is
the
v
alue
that
is
added
to
counter
j
(line
).
Therefore,
the
coun
ter
is
incremen
ted
if
and
only
if
w

R
j
.
Corollary
.
Machine
C
R
satises
The
or
em
.	.
Pro
of:
W
e
ha
v
e
sho
wn
in
Lemma
..
that
if
the
mac
hine
do
esn't
fail,
it
giv
es
the
righ
t
result.
It
is
left
to
pro
v
e
that
there
exists
a
computation
in
whic
h
the
mac
hine
do
esn't
fail.
The
correct
computation
is
done
as
follo
ws.
F
or
eac
h
no
de
r

R
j
 
,
the
mac
hine
guesses
w
ell
in
line
	
that
indeed
r

R
j
 
and
stops
w
orking
on
this
no
de.
F
or
eac
h
no
de
r

R
j
 
,
the
mac
hine
guesses
in
line
	
so,
and
in
addition
it
guesses
correctly
the
no
des
that
form
the
directed
path
from
v
to
r
in
line
.
In
this
computation,
the
mac
hine
will
not
fail.
In
lines

and
,
there
is
no
failure,
since
only
r

R
j
 
no
des
get
to
these
lines,
and
in
these
lines
it
guesses
corectly
the
connected
path
from
v
.
Therefore,
in
line
,
all
no
des
r

R
j
 
w
ere
coun
ted,
since
the
mac
hine
guesses
them
corectly
,
and
the
mac
hine
will
not
halt
either.
Th
us,
the
mac
hine
do
esn't
fail
on
the
ab
o
v
e
computation.
Using
Lemma
..,
w
e
kno
w
that
the
mac
hine
uses
O
(l
og
(n))
space.
Therefore,
C
R
is
a
non-deterministic
mac
hine
that
satises
Theorem
.	.
Bibliographic
Notes
The
pro
ofs
of
b
oth
theorems
(i.e.,
NL-completeness
of
CONN
and
NL=coNL)
can
b
e
found
in
[].
The
latter
result
w
as
pro
v
ed
indep
enden
tly
b
y
Immerman
[]
and
Szelep
csen
yi
[].


LECTURE
.
INSIDE
NON-DETERMINISTIC
LOGARITHMIC
SP
A
CE
.
N.
Immerman.
Nondeterministic
Space
is
Closed
Under
Complemen
tation.
SIAM
Jour.
on
Computing,
V
ol.
,
pages
0{,
	.
.
M.
Sipser.
Intr
o
duction
to
the
The
ory
of
Computation,
PWS
Publishing
Compan
y
,
		.
.
R.
Szelep
csen
yi.
A
Metho
d
of
F
orced
En
umeration
for
Nondeterministic
Automata.
A
cta
Informatic
a,
V
ol.
,
pages
	{,
	.

Lecture

Randomized
Computations
Notes
tak
en
b
y
Erez
W
aisbard
and
Gera
W
eiss
Summary:
In
this
lecture
w
e
extend
the
notion
of
ecien
t
computation
b
y
allo
wing
algorithms
(T
uring
mac
hines)
to
toss
coins.
W
e
study
the
classes
of
languages
that
arise
from
v
arious
natural
denitions
of
acceptance
b
y
suc
h
mac
hines.
W
e
will
fo
cus
on
p
olynomial
running
time
mac
hines
of
the
follo
wing
t
yp
es:
.
One-sided
error
mac
hines
(R
P
;
coR
P
).
.
Tw
o-sided
error
mac
hines
(B
P
P
).
.
Zero
error
mac
hines
(Z
P
P
)
W
e
will
also
consider
probabilistic
mac
hines
that
uses
logarithmic
spaces
(R
L).
.
Probabilistic
computations
The
basic
though
t
underlying
our
discussion
is
the
asso
ciation
of
ecien
t
computation
with
prob-
abilistic
p
olynomial
time
T
uring
mac
hines.
W
e
will
consider
ecien
t
only
algorithms
that
run
in
time
that
is
no
more
than
a
xed
p
olynomial
in
the
length
of
the
input.
There
are
t
w
o
w
a
ys
to
dene
randomized
computation.
One,
that
w
e
will
call
online
is
to
en
ter
randomized
steps,
and
the
second
that
w
e
will
call
oine
is
to
use
an
additional
randomizing
input
and
ev
aluate
the
output
on
suc
h
random
input.
In
the
ctitious
mo
del
of
non-deterministic
mac
hines,
one
accepting
computation
w
as
enough
to
include
an
input
in
the
language
accepted
b
y
the
mac
hine.
In
the
randomized
mo
del
w
e
will
consider
the
probabilit
y
of
acceptance
rather
than
just
asking
if
the
mac
hine
has
an
accepting
computation.
Then
he
said,
\May
the
L
or
d
not
b
e
angry,
but
let
me
sp
e
ak
just
onc
e
mor
e.
What
if
only
ten
c
an
b
e
found
ther
e?"
He
answer
e
d,
\F
or
the
sake
of
ten,
I
wil
l
not
destr
oy
it."
[Genesis
:].
As
Go
d
didn't
agree
to
sa
v
e
Sedom
for
the
sak
e
of
less
then
ten
p
eoples,
w
e
will
not
consider
an
input
to
b
e
in
the
accepted
language
unless
it
has
a
noticeable
probabilit
y
to
b
e
accepted.
Oded's
Note:
The
ab
o
v
e
illustration
is
certainly
not
m
y
initiativ
e.
Besides
some
reser-
v
ations
regarding
this
sp
ecic
part
of
the
bible
(and
more
so
the
in
terpretations
giv
en



LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
to
it
during
the
cen
turies),
I
fear
that
0
ma
y
not
stric
k
the
reader
as
\man
y"
but
rather
as
closer
to
\existence".
In
fact,
standard
in
terpretations
of
this
passage
stress
the
minimalistic
nature
of
the
c
hallenge
{
barely
ab
o
v
e
unique
existence...
The
online
approac
h:
One
w
a
y
to
lo
ok
at
a
randomized
computation
is
to
allo
w
the
T
uring
mac
hine
to
mak
e
random
mo
v
es.
F
ormally
this
can
b
e
mo
deled
as
letting
the
mac
hine
to
c
ho
ose
randomly
among
the
p
ossible
mo
v
es
that
arise
from
a
nondeterministic
transition
table.
If
the
transition
table
maps
one
(<
state
>;
<
sy
mbol
>)
pair
to
t
w
o
dieren
t
(<
state
>;
<
mov
e
>;
<
sy
mbol
>)
triples
then
the
mac
hine
will
c
ho
ose
eac
h
transition
with
equal
probabilities.
Syn
tactically
,
the
online
probabilistic
T
uring
mac
hine
will
lo
ok
the
same
as
the
nondeterministic
mac
hine.
The
dierence
is
at
the
denition
of
the
accepted
language.
The
criterion
of
an
input
to
b
e
accepted
b
y
a
regular
nondeterministic
mac
hine
is
that
the
mac
hine
will
ha
v
e
at
least
one
accepting
computation
when
it
is
in
v
ok
ed
with
this
input.
In
the
probabilistic
case,
w
e
will
consider
the
probabilit
y
of
acceptance.
W
e
w
ould
b
e
in
terested
in
how
many
accepting
computation
the
mac
hine
has
(or
rather
what
is
the
probabilit
y
of
suc
h
computation).
W
e
p
ostulate
that
the
mac
hine
c
ho
ose
ev
ery
step
with
equal
probabilit
y
,
and
so
get
a
probabilit
y
space
on
p
ossible
computations.
W
e
lo
ok
at
a
computation
as
a
tree,
where
a
no
de
is
a
conguration
and
it's
c
hildren
are
all
the
p
ossible
congurations
that
the
mac
hine
can
pass
to
in
a
single
step.
The
tree
is
describing
the
p
ossible
computations
of
the
mac
hine
when
running
on
a
giv
en
input.
The
output
of
a
probabilistic
T
uring
mac
hine
on
an
input
x
is
not
a
string
but
a
random
v
ariable.
Without
loss
of
generalit
y
w
e
can
consider
only
binary
tree
b
ecause
if
the
mac
hine
has
more
than
t
w
o
p
ossible
steps,
it
is
p
ossible
to
build
another
mac
hine
that
will
sim
ulate
the
giv
en
mac
hine
with
t
w
o
step
transition
table.
This
is
p
ossible
ev
en
if
the
original
mac
hine
had
steps
with
probabilit
y
that
has
innite
binary
expansion.
Let
sa
y
,
for
example,
that
the
mac
hine
has
a
probabilit
y
of


to
get
from
step
A
to
step
B.
Then
w
e
ha
v
e
a
problem
when
trying
to
sim
ulate
it
b
y
un
biased
binary
coins,
b
ecause
there
is
the
binary
expansion
of


is
innite.
But
w
e
can
still
get
as
close
as
w
e
w
an
t
to
the
original
mac
hine,
and
this
is
go
o
d
enough
for
our
purp
oses.
The
oine
approac
h:
Another
w
a
y
to
consider
nondeterministic
mac
hines
is,
as
w
e
did
b
efore,
to
use
an
additional
input
as
a
guess.
F
or
N
P
mac
hines
w
e
ga
v
e
an
additional
input
that
w
as
used
as
a
witness.
The
analogous
idea
is
to
view
the
outcome
of
the
in
ternal
coin
tosses
as
an
auxiliary
input.
The
mac
hine
will
receiv
e
t
w
o
inputs,
the
real
input,
x,
and
the
guess
input,
r
.
Imagine
that
the
mac
hine
receiv
es
this
second
input
from
an
external
`coin
tossing
device'
rather
than
toss
coins
in
ternally
.
Notation:
W
e
will
use
the
follo
wing
notation
to
discuss
v
arious
prop
erties
of
probabilistic
ma-
c
hines:
P
r
ob
r
[M
(x;
r
)
=
z
]
Sometimes,
w
e
will
drop
the
r
and
k
eep
it
implicitly
lik
e
in
the
follo
wing
notation:
P
r
ob[M
(x)
=
z
]
By
this
notations
w
e
mean
the
probabilit
y
that
the
mac
hine
M
with
real
input
x
and
guess
input
r
,
distributed
uniformly
,
will
giv
e
an
output
z
.
The
probabilit
y
space
is
that
of
all
p
ossible
r
tak
en
with
uniform
distribution.
This
statemen
t
is
more
confusing
than
it
seems
to
b
e
b
ecause
the
mac
hine
ma
y
use
dieren
t
n
um
b
er
of
guesses
for
dieren
t
inputs.
It
ma
y
also
use
dieren
t
n
um
b
er
of
guesses
on
the
same
input,
if
the
computation
dep
ends
on
the
outcome
of
previous
guesses.

..
THE
CLASSES
R
P
AND
C
O
R
P
{
ONE-SIDED
ERR
OR

Oded's
Note:
Actually
,
the
problem
is
with
the
latter
case.
That
is,
if
on
eac
h
input
all
computations
use
the
same
n
um
b
er
of
coin
tosses
(or
\guesses"),
denoted
l
,
then
eac
h
suc
h
computation
o
ccurs
with
probabilit
y

 l
.
Ho
w
ev
er,
in
the
general
case,
where
the
n
um
b
er
of
coin
tosses
ma
y
dep
end
on
the
outcome
of
previous
tosses,
w
e
ma
y
just
observ
e
that
a
halting
computation
with
coin
outcome
sequence
r
o
ccurs
with
probabilit
y
exactly

 jr
j
.
Oded's
Note:
An
alternativ
e
approac
h
is
to
mo
dify
the
randomized
mac
hine
so
that
it
do
es
use
the
same
n
um
b
er
of
coin
tosses
in
eac
h
computation
on
the
same
input.
.
The
classes
R
P
and
coR
P
{
One-Sided
Error
The
rst
t
w
o
classes
of
languages
that
arise
from
probabilistic
computations
that
w
e
consider
are
the
one-sided
error
(p
olynomial
running
time)
computable
languages.
If
there
exist
a
mac
hine
that
can
decide
the
language
with
go
o
d
probabilit
y
in
p
olynomial
time
it
is
reasonable
to
consider
the
problem
as
relativ
ely
easy
.
Go
o
d
probabilit
y
here
means
that
the
mac
hine
will
b
e
sure
only
in
one
case
and
will
giv
e
the
righ
t
answ
er
in
the
other
case
but
only
with
go
o
d
probabilit
y
(the
cases
are
when
x

L
and
when
x
=

L).
F
rom
here
on,
a
p
olynomial
probabilistic
T
uring
mac
hine
means
a
probabilistic
mac
hine
that
alw
a
ys
(no
matter
what
coin
tosses
it
gets)
halts
after
a
p
olynomial
(in
the
length
of
the
input)
n
um
b
er
of
steps.
Denition
.
(Random
P
olynomial-time
{
R
P
):
The
c
omplexity
class
R
P
is
the
class
of
al
l
languages
L
for
which
ther
e
exist
a
pr
ob
abilistic
p
olynomial-time
T
uring
machine
M,
such
that
x

L
)
P
r
ob[M
(x)
=
]



:
x
=

L
)
P
r
ob[M
(x)
=
]
=
0:
Denition
.
(Complemen
tary
Random
P
olynomial-time
{
coR
P
):
The
c
omplexity
class
coR
P
is
the
class
of
al
l
languages
L
for
which
ther
e
exist
a
pr
ob
abilistic
p
olynomial-time
T
uring
machine
M,
such
that
x

L
)
P
r
ob[M
(x)
=
]
=
:
x
=

L
)
P
r
ob[M
(x)
=
0]



:
One
can
see
from
the
denitions
that
these
t
w
o
classes
complemen
t
eac
h
other.
If
y
ou
ha
v
e
a
mac
hine
that
decides
a
language
L
with
go
o
d
probabilit
y
(in
one
of
the
ab
o
v
e
senses),
y
ou
can
use
the
same
mac
hine
to
decide
the
complemen
tary
language
in
the
complemen
tary
sense.
That
is,
an
alternativ
e
(and
equiv
alen
t)
w
a
y
to
dene
coR
P
is:
coR
P
=
fL
:
L

R
P
g


LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
Comparing
NP
to
RP:
It
is
instructiv
e
to
compare
the
denitions
of
R
P
and
N
P
.
In
b
oth
classes
w
e
had
the
oine
denition
that
used
an
external
witness
(in
N
P
)
or
randomization
(in
R
P
).
Giv
en
an
R
P
mac
hine,
M
,
since
the
mac
hine
run
in
p
olynomial-time,
the
size
of
the
guesses
that
it
can
use
is
b
ounded
b
y
a
p
olynomial
in
the
size
of
x.
F
or
ev
ery
giv
en
in
teger
n

N
w
e
consider
the
relation:
R
n
def
=
n
(x;
r
)

f0;
g
n

f0;
g
p(n)
:
M
(x;
r
)
=

o
whic
h
consists
of
all
accepted
inputs
of
length
n
and
their
accepting
coin
tosses
(i.e
r
).
The
same
is
also
applicable
for
N
P
mac
hines,
whic
h
run
also
in
p
olynomial-time
and
can
only
use
witnesses
that
are
b
ounded
b
y
a
p
olynomial
in
the
length
of
the
input.
So,
for
N
P
mac
hine
M
,
w
e
consider
the
relation:
R
n
def
=
n
(x;
y
)

f0;
g
n

f0;
g
p(n)
:
M
(x;
y
)
=

o
whic
h
consist
of
all
accepted
inputs
of
length
n
and
their
witnesses
(i.e
y
).
In
b
oth
cases
w
e
will
use
the
relation:
R
=

[
n=
R
n
whic
h
consists
of
all
the
accepted
inputs
and
their
witness/coin-tosses.
Using
this
relation
w
e
can
compare
Denition
.
to
the
denition
of
N
P
in
the
follo
wing
table:
N
P
R
P
x

L
)
	y
,
(x;
y
)

R
x

L
)Prob
r
[(x;
r
)

R
]



x
=

L
)
y
,
(x;
y
)
=

R
x
=

L
)
r
,
(x;
r
)
=

R
F
rom
this
table,
it
is
seems
that
these
t
w
o
classes
are
close.
The
witness
in
the
nondeterministic
mo
del
is
replaced
b
y
the
coin-tosses
and
the
criteria
for
acceptance
has
c
hanged.
The
dierence
is
that,
in
the
nondeterministic
mo
del,
one
witness
w
as
enough
for
us
to
sa
y
that
an
input
is
accepted,
and
in
the
probabilistic
mo
del
w
e
are
asking
for
man
y
coin-tosses.
Clearly
,
Prop
osition
..
N
P

R
P
Pro
of:
Let
L
b
e
an
arbitrary
language
in
R
P
.
If
x

L
then
there
exist
a
T
uring
mac
hine
M
and
a
coin-tosses
y
suc
h
that
M
(x;
y
)
=

(more
than


of
the
coin-tosses
are
suc
h).
So
w
e
can
use
this
y
as
a
witness
(considering
the
same
mac
hine
as
a
nondeterministic
mac
hine
with
the
coin-tosses
as
witnesses).
If
x
=

L
then
P
r
ob
r
[M
(x;
r
)
=
]
=
0
so
there
is
no
witness.
Notice
that
there
is
a
big
dierence
b
et
w
een
nondeterministic
T
uring
mac
hines
and
probabilistic
T
uring
mac
hines.
The
rst
is
a
ctitious
concept
that
is
in
v
en
ted
to
explore
the
prop
erties
of
searc
h
problems,
while
the
second
is
a
realistic
mo
del
that
describ
e
mac
hines
that
one
can
really
build.W
e
use
the
nondeterministic
mo
del
to
describ
e
problems
lik
e
a
searc
h
problem
with
an
ecien
t
v
eri-
cation,
while
the
probabilistic
mo
del
is
used
as
an
ecien
t
computation.
It
is
fair
to
ask
if
a
computer
can
toss-coins
as
an
elemen
tary
op
eration.
W
e
answ
er
this
question
p
ositiv
ely
based
on
our
notion
of
randomness
and
the
abilit
y
of
computers
to
use
random-
generating
instrumen
tation
lik
e
reading
unstable
electric
circuits.
The
question
is
whether
this
random
op
eration
giv
es
us
more
p
o
w
er
than
w
e
had
with
the
regular
deterministic
mac
hines.

..
THE
CLASSES
R
P
AND
C
O
R
P
{
ONE-SIDED
ERR
OR

R
P
is
one-sided
error:
The
denition
of
R
P
do
es
not
ask
for
the
same
b
eha
vior
on
inputs
that
are
in
the
language
as
it
asks
for
inputs
that
are
in
the
language.

If
x
=

L
then
the
answ
er
of
the
mac
hine
m
ust
b
e
correct
no
matter
what
guesses
w
e
mak
e.
In
this
case,
the
probabilit
y
to
get
a
wrong
answ
er
is
zero
so
the
answ
er
of
the
mac
hine
is
righ
t
for
ev
ery
r
.

But,
if
x

L,
the
mac
hine
is
allo
w
ed
to
mak
e
mistak
es.
In
this
case,
w
e
ha
v
e
a
non-zero
probabilit
y
that
the
answ
er
of
the
mac
hine
will
b
e
wrong
(still
this
probabilit
y
is
not
\to
o
big").
The
denition
fa
v
ors
one
t
yp
e
of
mistak
e
while
in
practice
w
e
don't
nd
v
ery
go
o
d
reason
to
fa
v
or
it.
W
e
will
see
later
that
there
are
dieren
t
families
of
languages
that
do
not
fa
v
or
an
y
t
yp
e
of
error.
W
e
will
call
these
languages
t
w
o-sided
error
languages.
It
w
as
reasonable
to
discuss
one-sided
errors
when
w
e
where
dev
eloping
N
P
,
b
ecause
v
erication
is
one-sided
b
y
nature,
but
it
is
less
useful
for
exploring
the
notion
of
ecien
t
computation.
In
v
ariance
of
the
constan
t
and
b
ey
ond:
Recall
that
for
L

R
P
x

L
)
P
r
ob
r
[M
(x;
r
)
=
]



The
constan
t


in
the
denition
of
R
P
is
arbitrary
.
W
e
could
c
ho
ose
ev
ery
constan
t
strictly
threshold
b
et
w
een
zero
and
one,
and
get
the
same
complexit
y
class.
Our
c
hoice
of


is
somewhat
app
ealing
b
ecause
it
sa
ys
that
at
least
half
of
the
witnesses
are
go
o
d.
If
y
ou
ha
v
e,
for
example,
a
mac
hine
that
can
decide
some
language
L
with
a
greater
probabilit
y
than


to
sa
y
\YES"
for
an
input
that
is
in
the
language,
y
ou
can
build
another
mac
hine
that
will
in
v
ok
e
the
rst
mac
hine
three
times
on
ev
ery
input
and
return
the
\YES"
if
one
of
them
answ
ered
\YES".
Ob
viously
this
mac
hine
will
answ
er
correctly
on
inputs
that
are
not
in
the
language
(b
e-
cause
the
rst
mac
hine
will
alw
a
ys
sa
y
\NO"),
and
it
will
sa
y
\YES"
on
inputs
that
are
in
the
language
with
higher
probabilit
y
than
b
efore.
The
original
probabilit
y
of
not
getting
the
correct
answ
er
when
the
input
is
in
the
language
w
as
smaller
than


,
when
rep
eating
the
computation
for
three
time
this
probabilit
y
falls
do
wn
to
less
than





=


meaning
that
w
e
no
w
get
the
correct
answ
er
with
probabilit
y
greater
than
	

(whic
h
is
greater
than


).
So
w
e
could
use


instead
of


without
c
hanging
the
class
of
languages.
This
pro
cedure
of
amplication
can
b
e
used
to
sho
w
the
same
result
for
ev
ery
constan
t,
but
w
e
will
pro
v
e
further
that
one
can
ev
en
use
thresholds
that
dep
end
on
the
length
of
the
input.
W
e
are
lo
oking
at
t
w
o
probabilit
y
spaces:
one
when
x
=

L
and
one
when
x

L,
and
dened
a
random
v
ariable
(represen
ting
the
decision
of
the
mac
hine)
on
eac
h
of
this
spaces.
In
case
x
=

L
the
latter
random
v
ariable
is
iden
tically
zero
(i.e.,
\reject"),
whereas
in
case
x

L
the
random
v
ariable
ma
y
b
e
non-trivial
(i.e.,
is

with
probabilit
y
ab
o
v
e
some
giv
en
threshold
and
0
otherwise).
Mo
ving
from
one
threshold
to
a
higher
one
amoun
t
to
the
follo
wing:
In
case
x

L,
the
fraction
of
p
oin
ts
in
the
probabilit
y
space
assigned
the
v
alue

is
lo
w
er
b
ounded
b
y
the
rst
threshold.
Our
aim
is
to
hit
suc
h
a
p
oin
t
with
probabilit
y
lo
w
er
b
ounded
b
y
a
higher
threshold.
This
is
done
b
y
merely
making
rep
eated
indep
enden
t
samples
in
to
the
space,
where
the
n
um
b
er
of
the
trials
is
easily
determined
b
y
the
relation
b
et
w
een
the
t
w
o
thresholds.
W
e
stress
that
in
case
x
=

L
all
p
oin
ts
in
the
probabilit
y
space
are
iden
tically
assigned
(the
v
alue
0)
and
so
it
do
es
not
matter
ho
w
man
y
times
w
e
try
(w
e'll
alw
a
ys
see
zeros).


LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
W
e
will
sho
w
that
one
can
ev
en
replace
the
constan
t


b
y
either

p(jxj)
or

 
 p(jxj)
,
where
p()
is
an
y
xed
p
olynomial,
and
get
the
same
family
of
languages.
W
e
tak
e
these
t
w
o
margins,
b
ecause
once
w
e
will
sho
w
the
equiv
alence
of
these
t
w
o
thresholds,
it
will
follo
w
that
ev
ery
threshold
that
one
migh
t
think
of
in
b
et
w
een
will
do.
Consider
the
follo
wing
denitions:
Denition
.
(
R
P
):
L
is
in
R
P

if
ther
e
exist
a
p
olynomial
running-time
T
uring
machine
M
and
a
p
olynomial
p()
such
that
x

L
)
P
r
ob
r
[M
(x;
r
)
=
]


p(jxj)
x
=

L
)
P
r
ob
r
[M
(x;
r
)
=
0]
=

Denition
.
(R
P
):
L
is
in
R
P

if
ther
e
exist
a
p
olynomial
running-time
T
uring
machine
M
and
a
p
olynomial
p()
such
that
x

L
)
P
r
ob
r
[M
(x;
r
)
=
]


 
 p(jxj)
x
=

L
)
P
r
ob
r
[M
(x;
r
)
=
0]
=

These
denitions
seems
v
ery
far
from
eac
h
other,
b
ecause
in
R
P

w
e
ask
for
a
probabilistic
algorithm
(T
uring
mac
hine)
that
answ
er
correctly
with
a
v
ery
small
probabilit
y
(but
not
negligible),
while
in
R

w
e
ask
for
an
ecien
t
algorithm
(T
uring
mac
hine)
that
w
e
can
almost
ignore
the
probabilit
y
of
it's
mistak
e.
Ho
w
ev
er,
these
t
w
o
denition
actually
dene
the
same
class
(as
w
e
will
pro
v
e
in
the
next
paragraph).
This
implies
that
ha
ving
an
algorithm
with
a
noticeable
probabilit
y
of
success
implies
existence
of
and
ecien
t
algorithm
with
negligible
probabilit
y
of
error.
Prop
osition
..
RP=RP
Pro
of:
R
P


R
P

This
direction
is
trivial
b
ecause
if
jxj
is
big
enough
then
the
b
ound
in
Denition
.
(i.e

p(jxj)
)
is
smaller
than
the
b
ound
in
Denition
.
(i.e

 
 p(jxj)
)
so
b
eing
in
R
P

implies
b
eing
in
R
P

for
almost
all
inputs.
The
nitely
man
y
inputs
for
whic
h
this
do
es
not
hold
can
b
e
incorp
orated
in
the
mac
hine
of
Denition
..
Th
us
R
P


R
P
.
R
P


R
P

W
e
will
use
a
metho
d
kno
wn
as
amplic
ation:
W
e
will
try
the
w
eak
er
mac
hine
(of
R
P
)
enough
times
so
that
the
probabilit
y
of
giving
a
wrong
answ
er
will
b
e
small
enough.
Assume
that
w
e
ha
v
e
a
mac
hine
M

suc
h
that
x

L
:
P
r
ob
r
[M

(x;
r
)
=
]


p(jxj)
W
e
will
dene
a
new
mac
hine
M

,
up
to
a
function
t(jxj)
that
w
e
will
determine
later,
as
follo
ws:
M

(x)
def
=

>
<
>
:
inv
ok
e
M

(x)
t(jxj)
times
w
ith
dif
f
er
ent
r
andoml
y
sel
ected
r
0
s
if
some
of
these
inv
ocations
r
etur
ned
0
Y
E
S
0
r
etur
n
0
Y
E
S
0
el
se
r
etur
n
0
N
O
0
Let
t
=
t(jxj).
Then
for
x

L
P
r
ob[M

(x)
=
0]
=
(
P
r
ob[M

(x)
=
0])
t(jxj)



 
p(jxj)

t(jxj)

..
THE
CLASS
B
P
P
{
TW
O-SIDED
ERR
OR
	
T
o
nd
the
desired
t(jxj)
w
e
can
solv
e
the
equation:


 
p(jxj)

t(jxj)


 p(jxj)
And
obtain
t(jxj)

p(jxj)

log



 
p(jxj)

 
=
p(jxj)

log

e
Where
e

::::
is
the
natural
logarithm
base.
So
b
y
letting
t(jxj)
=
p(jxj)

in
the
denition
of
M

w
e
get
a
mac
hine
that
run
in
p
olynomial
time
and
decides
L
with
probabilit
y
greater
than

 
 p(jxj)
to
giv
e
righ
t
answ
er
for
x

L
(and
alw
a
ys
correct
on
x
=

L
).
.
The
class
B
P
P
{
Tw
o-Sided
Error
One
ma
y
argue
that
R
P
is
to
o
strict
b
ecause
it
ask
that
the
mac
hine
has
to
giv
e
00%
correct
answ
er
for
inputs
that
are
not
in
the
language.
W
e
deriv
ed
the
denition
of
R
P
from
the
denition
of
N
P
,
but
N
P
didn't
reect
an
actual
computational
mo
del
for
searc
h
problems
but
rather
a
mo
del
for
v
erication.
One
ma
y
nd
that
lo
oking
at
a
t
w
o-sided
error
is
more
app
ealing
as
a
mo
del
for
searc
h
problem
computations.
W
e
w
an
t
a
mac
hine
that
will
recognize
the
language
with
high
probabilit
y
,
where
probabilit
y
refers
to
the
ev
en
t
\The
mac
hine
answ
ers
correctly
on
an
input
x
regardless
if
x

L
or
x
=

L".
This
will
lead
us
to
t
w
o-sided
error
v
ersion
of
the
randomized
computation.
First
recall
the
notation:

L
(x)
def
=
(

x

L
0
x
=

L
Denition
.
(Bounded-Probabilit
y
P
olynomial-time
{
B
P
P
):
The
c
omplexity
class
B
P
P
is
the
class
of
al
l
languages
L
for
which
ther
e
exist
a
pr
ob
abilistic
p
olynomial-time
T
uring
machine
M
,
such
that
x
:
P
r
ob[M
(x)
=

L
(x)]



:
That
me
ans
that:
I
f
x

L
then
P
r
ob[M
(x)
=
]



:
I
f
x
=

L
then
P
r
ob[M
(x)
=
]
<


:
The
phrase
\b
ounded-probabilit
y"
means
that
the
success
probabilit
y
is
b
ounded
a
w
a
y
from
failure
probabilit
y
.
The
B
P
P
mac
hine
is
a
mac
hine
that
mak
es
mistak
es
but
returns
the
correct
answ
er
most
of
the
time.
By
running
the
mac
hine
a
large
n
um
b
er
of
times
and
returning
the
ma
jorit
y
of
the
answ
ers
w
e
are
guaran
teed
b
y
the
la
w
of
large
n
um
b
ers
that
our
mistak
e
will
b
e
v
ery
small.
The
idea
b
ehind
the
B
P
P
class
is
that
M
accept
b
y
ma
jorit
y
with
a
noticeable
gap
b
et
w
een
the
probabilit
y
to
accept
inputs
that
are
in
language
and
the
probabilit
y
to
accept
inputs
that
are
not
in
the
language,
and
it's
running
time
is
b
ounded
b
y
a
p
olynomial.

0
LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
In
v
ariance
of
constan
t
and
b
ey
ond:
The


is,
again,
an
arbitrary
constan
t.
Replacing
the


in
the
denition
b
y
an
y
other
constan
t
greater
than


do
es
not
c
hange
the
class
dened.
If,
for
example
w
e
had
a
mac
hine,
M
that
recognize
some
language
L
with
probabilit
y
p
>


,
meaning
that
P
r
ob[M
(x)
=

L
(x)]

p,
w
e
could
easily
build
a
mac
hine
that
will
recognize
L
with
an
y
giv
en
probabilit
y
q
>
p
b
y
in
v
oking
this
mac
hine
sucien
tly
man
y
times
and
returning
the
ma
jorit
y
of
the
answ
ers.
This
will
clearly
increase
the
probabilit
y
of
giving
correct
answ
er
to
the
w
an
ted
threshold,
and
run
in
p
olynomial
time.
In
the
R
P
case
w
e
had
t
w
o
probabilit
y
spaces
that
w
e
could
distinguish
easily
b
ecause
w
e
had
a
guaran
tee
that
if
x
=

L
then
the
probabilit
y
to
get
one
is
zero,
hence
if
y
ou
get
M
(x)
=

for
some
input
x,
y
ou
could
sa
y
for
sur
e
that
x

L.
In
the
B
P
P
case,
the
amplication
is
less
trivial
b
ecause
w
e
ha
v
e
zero
es
and
ones
in
b
oth
probabilit
y
spaces
(the
probabilit
y
space
is
not
constan
t
when
x

L
nor
when
x
=

L).
The
reason
that
w
e
can
apply
amplication
in
the
B
P
P
case
(despite
the
ab
o
v
e
dierence)
is
that
in
v
oking
the
mac
hine
man
y
times
and
coun
ting
ho
w
man
y
times
it
returns
one
giv
es
us
an
estimation
on
the
fraction
of
ones
in
the
whole
probabilit
y
space.
It
is
useful
to
get
an
estimator
for
the
fraction
of
the
ones
in
the
probabilit
y
space
b
ecause
when
this
fraction
is
greater
than


w
e
ha
v
e
that
x

L,
and
when
this
fraction
is
less
than


w
e
ha
v
e
that
x
=

L
(this
fraction
tells
us
in
whic
h
probabilit
y
space
w
e
are
in).
If
w
e
rewrite
the
condition
in
Denition
.
as:
I
f
x

L
then
P
r
ob[M
(x)
=
]



+


:
I
f
x
=

L
then
P
r
ob[M
(x)
=
]
<


 

:
W
e
could
consider
the
follo
wing
c
hange
of
constan
ts:
I
f
x

L
then
P
r
ob[M
(x)
=
]

p
+
:
I
f
x
=

L
then
P
r
ob[M
(x)
=
]
<
p
 :
for
an
y
giv
en
p

(0;
)
and
0
<

<
min
fp;

 pg.
If
w
e
had
suc
h
a
mac
hine,
w
e
could
in
v
ok
e
the
mac
hine
man
y
times
and
get
increasing
probabilit
y
to
ha
v
e
the
fraction
of
ones
in
our
inno
v
ations
to
b
e
in
an

neigh
b
orho
o
d
of
the
real
fraction
of
ones
in
the
whole
space
(b
y
the
la
w
of
large
n
um
b
ers).
After
some
xed
n
um
b
er
of
iterations
(that
do
es
not
dep
end
on
x),
w
e
can
get
that
probabilit
y
to
b
e
larger
than


.
This
means
that
if
w
e
had
suc
h
a
mac
hine
(with
p
and

instead
of


and


),
w
e
could
build
another
mac
hine
that
will
in
v
ok
e
it
some
xed
n
um
b
er
of
times
and
will
decide
the
same
language
with
probabilit
y
greater
than


.
The
conclusion
is
that
the





is
arbitrary
in
Denition
.,
and
can
b
e
replaced
b
y
an
y
p


suc
h
that
p

(0;
)
and
0
<

<
min
fp;

 pg.
But
w
e
can
do
more
than
that
and
use
threshold
that
dep
end
on
the
length
of
the
input
as
w
e
will
pro
v
e
in
the
follo
wing
claims:
The
w
eak
est
p
ossible
BPP
denition:
Using
the
ab
o
v
e
framew
ork,
w
e'll
sho
w
that
for
ev
ery
p
olynomial-time
computable
threshold,
denoted
f
b
elo
w,
and
an
y
\noticeable"
margin
(represen
ted
b
y
=p
oly
),
w
e
can
reco
v
er
the
\standard"
threshold
(of
=)
and
the
\safe"
margin
of
=.

..
THE
CLASS
B
P
P
{
TW
O-SIDED
ERR
OR

Claim
..
L

B
P
P
if
and
only
if
ther
e
exist
a
p
olynomial-time
c
omputable
function
f
:
N
!
[0;
],
a
p
ositive
p
olynomial
p()
and
a
pr
ob
abilistic
p
olynomial-time
T
uring
machine
M,
such
that:
x

L
:
P
r
ob[M
(x)
=
]

f
(jxj)
+

p(jxj)
x
=

L
:
P
r
ob[M
(x)
=
]
<
f
(jxj)
 
p(jxj)
Pro
of:
It
is
easy
to
see
that
b
y
c
ho
osing
f
(jxj)



and
p(jxj)


w
e
get
the
original
denition
of
B
P
P
(see
Denition
.),
hence
ev
ery
B
P
P
language
satises
the
ab
o
v
e
condition.
Assume
that
w
e
ha
v
e
a
probabilistic
T
uring
mac
hine,
M
,
with
these
b
ounds
on
the
proba-
bilit
y
to
get
.
Then,
for
an
y
giv
en
input
x,
w
e
lo
ok
at
the
random
v
ariable
M
(x),
whic
h
is
a
Bernoulli
random
v
ariable
with
unkno
wn
parameter
p
=
E
xp[M
(x)].
Using
a
w
ell
kno
wn
fact
that
the
exp
ectation
of
a
Bernoulli
random
v
ariable
is
exactly
the
probabilit
y
to
get
one
w
e
get
that
p
=
P
r
ob[M
(x)
=
].
So
b
y
estimating
p
w
e
can
sa
y
something
ab
out
whether
x

L
or
x
=

L.
The
most
natural
esti-
mator
is
to
tak
e
the
mean
of
n
samples
of
the
random
v
ariable
(i.e
the
answ
ers
of
n
indep
enden
t
in
v
o
cations
of
M
(x)).
Then
w
e
will
use
the
kno
wn
statistical
metho
d
of
condence
in
terv
als
on
the
parameter
p.
The
condence
in
terv
al
metho
d
giv
es
a
b
ound
within
whic
h
a
parameter
is
exp
ected
to
lie
with
a
cer-
tain
probabilit
y
.
In
terv
al
estimation
of
a
parameter
is
often
useful
in
observing
the
accuracy
of
an
estimator
as
w
ell
as
in
making
statistical
inferences
ab
out
the
parameter
in
question.
In
our
case
w
e
w
an
t
to
kno
w
with
probabilit
y
higher
than


if
p

h
0;
f
(jxj)
 
p(jxj)
i
or
p

h
f
(jxj)
+

p(jxj)
;

i
.
This
is
enough
b
ecause
p

h
0;
f
(jxj)
 
p(jxj)
i
)
x
=

L
and
p

h
f
(jxj)
+

p(jxj)
;

i
)
x

L
(note
that
p


f
(jxj)
 
p(jxj)
;
f
(jxj)
+

p(jxj)

is
imp
ossible).
So
if
w
e
can
get
a
b
ound
of
size

p(jxj)
within
whic
h
p
is
exp
ected
to
lie
within
a
probabilit
y
greater
than


,
w
e
can
decide
L(M
)
with
this
probabilit
y
(and
hence
L(M
)

B
P
P
b
y
Denition
.).
W
e
dene
the
follo
wing
T
uring
mac
hine
(up
to
an
unkno
wn
n
um
b
er
n
that
w
e
will
compute
later)
:
M
0
(x)
def
=

>
<
>
:
I
nv
ok
e
M
(x)
n
times
(cal
l
the
r
esul
t
of
the
i
0
th
inv
ocation
t
i
):
C
ompute
^
p
 

n


n
i=
t
i
if
^
p
>
f
(jxj)
say
0
Y
E
S
0
el
se
say
0
N
O
0
Note
that
^
p
is
exactly
the
mean
of
a
sample
of
size
n
tak
en
from
the
random
v
ariable
M
(x).
This
mac
hine
do
the
normal
statistical
pro
cess
of
estimating
a
random
v
ariable
b
y
taking
samples
and
using
the
mean
as
an
estimator
for
the
exp
ectation.
If
w
e
will
b
e
able
to
sho
w
that
with
an
appropriate
n
the
estimator
will
not
fall
to
o
far
from
the
real
v
alue
with
a
go
o
d
probabilit
y
,
it
will
follo
w
that
this
mac
hine
answ
ers
correctly
with
the
same
probabilit
y
.
T
o
resolv
e
n
w
e
will
use
Cherno
's
inequalit
y
whic
h
states
that
for
an
y
set
of
n
indep
enden
t
Bernoulli
v
ariables
fX

;
X

;
:::;
X
n
g
with
the
same
exp
ectations
p



and
for
ev
ery

;
0
<


p(p
 ),
w
e
ha
v
e
P
r
ob






n
i=
X
i
n
 p




>


<


e
 

p( p)
n



e
 




n
=


e
 n



LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
So
b
y
taking

=

p(jxj)
and
n
=
 ln




w
e
get
that
our
T
uring
mac
hine
M
0
will
decide
L(M
)
with
probabilit
y
greater
than


suggesting
that
L(M
)

B
P
P
.
The
strongest
p
ossible
BPP
denition:
On
the
other
hand,
one
can
reduce
the
error
proba-
bilit
y
of
B
P
P
mac
hines
to
an
exp
onen
tially
v
anishing
amoun
t.
Claim
..
F
or
every
L

B
P
P
and
every
p
ositive
p
olynomial
p()
ther
e
exist
a
pr
ob
abilistic
p
olynomial-time
T
uring
machine
M
,
such
that:
x
:
P
r
ob[M
(x)
=

L
(x)]


 
 p(jxj)
Pro
of:
If
this
condition
is
true
for
ev
ery
p
olynomial,
w
e
can
c
ho
ose
p(jxj)


and
get
M
suc
h
that:
x
:
P
r
ob[M
(x)
=

L
(x)]


 
 
=


)
x
:
P
r
ob[M
(x)
=

L
(x)]



)
L

B
P
P
Let
L
b
e
a
language
in
B
P
P
and
let
M
b
e
the
mac
hine
guaran
teed
in
Denition
..
W
e
can
amplify
the
probabilit
y
of
righ
t
answ
er
b
y
in
v
oking
M
man
y
times
and
taking
the
ma
jorit
y
of
it's
answ
ers.
Dene
the
follo
wing
mac
hine
(again
up
to
the
n
um
b
er
n
that
w
e
will
nd
later):
M
0
(x)
def
=

>
<
>
:
I
nv
ok
e
M
(x)
n
times
(cal
l
the
r
esul
t
of
the
i
0
th
inv
ocation
t
i
):
C
ompute
^
p
 

n


n
i=
t
i
if
^
p
>


say
0
Y
E
S
0
el
se
say
0
N
O
0
F
rom
Denition
.,
w
e
get
that
if
w
e
kno
w
that
E
xp[M
(x)]
is
greater
than
half
it
follo
ws
that
x

L
and
if
w
e
kno
w
that
E
xp[M
(x)]
is
smaller
than
half
it
follo
ws
that
x
=

L
(b
ecause
E
xp[M
(x)]
=
P
r
ob[M
(x)
=
])
But
Denition
.
giv
es
us
more.
It
sa
ys
that
the
exp
ectation
of
M
(x)
is
b
ounded
a
w
a
y
from


so
w
e
can
use
the
condence
in
terv
al
metho
d.
F
rom
Cherno
's
inequalit
y
w
e
get
that
P
r
ob



M
0
(x)
 E
xp[M
(x)]








 

e
 n

But
if
jM
0
(x)
 E
xp[M
(x)]j
is
smaller
than


w
e
get
from
Denition
.
that
the
answ
er
of
M
0
is
correct,
b
ecause
it
is
close
enough
to
the
the
exp
ectation
of
M
(x)
whic
h
is
guaran
teed
to
b
e
ab
o
v
e


when
x

L
and
b
ello
w


when
x
=

L.
So
w
e
get
that:
P
r
ob

M
0
(x)
=

L
(x)



 

e
 n

Th
us,
for
ev
ery
p
olynomial
p(),
w
e
can
c
ho
ose
n,
suc
h
that

p(jxj)



e
 n

and
get
that:
P
r
ob

M
0
(x)
=

L
(x)



 
p(jxj)
So
M
0
satises
the
claimed
condition.

..
THE
CLASS
P
P

Conclusion:
W
e
see
that
a
gap
of

p(jxj)
and
a
gap
of

 
 p(jxj)
whic
h
lo
ok
lik
e
\w
eak"
and
\strong"
v
ersions
of
B
P
P
are
the
same.
As
sho
wn
ab
o
v
e
the
\w
eak"
v
ersion
is
actually
equiv
alen
t
to
the
\strong"
v
ersion,
and
b
oth
are
equiv
alen
t
to
the
original
denition
of
B
P
P
.
Some
commen
ts
ab
out
B
P
P
:
.
R
P

B
P
P
It
is
ob
vious
that
one-sided
error
is
a
sp
ecial
case
of
t
w
o-sided
error.
.
W
e
don't
kno
w
if
B
P
P

N
P
.
It
migh
t
b
e
so
but
w
e
don't
get
it
from
the
denition
lik
e
w
e
did
in
R
P
.
.
If
w
e
dene
coB
P
P
def
=
fL
:
L

B
P
P
g
w
e
get,
from
the
symmetry
of
the
denition
of
B
P
P
,
that
coB
P
P
=
B
P
P
:
.
The
class
P
P
The
class
P
P
is
wider
than
what
w
e
ha
v
e
seen
so
far.
In
the
B
P
P
case
w
e
had
a
gap
b
et
w
een
the
n
um
b
er
of
accepting
computations
and
non-accepting
computations.
This
gap
enabled
us
to
determine
with
go
o
d
probabilit
y
(using
condence
in
terv
als)
if
x

L
or
x
=

L.
The
gap
w
as
wide
enough
so
w
e
could
in
v
ok
e
the
mac
hine
p
olynomially
man
y
times
and
notice
the
dierence
b
et
w
een
inputs
that
are
in
the
language
and
inputs
that
are
not
in
the
language.
The
P
P
class
don't
put
the
gap
restriction,
hence
the
gap
ma
y
b
e
v
ery
small
(ev
en
one
guess
can
mak
e
a
dierence).
Running
the
mac
hine
p
olynomially
man
y
times
ma
y
not
help.
If
w
e
ha
v
e
a
mac
hine
that
answ
ers
correctly
with
probabilit
y
more
than


,
and
w
e
w
an
t
to
get
another
mac
hine
that
answ
ers
correctly
with
probabilit
y
greater
than


+

(for
a
giv
en
0
<

<


)
w
e
can't
alw
a
ys
do
it
in
p
olynomial
time
b
ecause
w
e
migh
t
not
ha
v
e
the
gap
that
w
e
had
in
Denition
..
Denition
.
P
P
def
=

>
<
>
:
L

f0;
g








Ther
e
exist
a
p
olynomial
time
T
uring
machine
M
s.t
x;
P
r
ob[M
(x)
=

L
(x)]
>


	
>
=
>
;
Note
that
it
is
imp
ortan
t
that
w
e
dene
>
and
not
,
since
otherwise
w
e
can
simply
\ip
a
coin"
and
completely
ignore
the
input
(w
e
can
decide
to
sa
y
0
Y
E
S
0
if
w
e
get
head
and
0
N
O
0
if
w
e
get
tail
and
this
will
satisfy
the
denition
of
the
mac
hine)
and
there
is
no
use
for
a
mac
hine
that
runs
a
lot
of
time
and
giv
es
no
more
kno
wledge
than
what
w
e
already
ha
v
e
(assuming
one
kno
ws
ho
w
to
ip
a
coin).
Ho
w
ev
er
the
actual
denition
of
P
P
giv
es
v
ery
little
as
w
ell:
The
dierence
b
et
w
een
what
happ
ens
in
case
x

L
and
in
case
x
=

L
is
negligible
(rather
than
\noticeable"
as
in
the
denition
of
BPP).
W
e
abuse
this
w
eak
requiremen
t
in
the
pro
of
of
Item

(b
elo
w).
F
rom
the
denition
of
P
P
w
e
get
a
few
in
teresting
facts:
.
P
P

P
S
P
AC
E
Let
L
b
e
a
language
in
P
P
,
let
M
b
e
the
probabilistic
T
uring
mac
hine
that
exists
according
to
Denition
..
Let
p()
b
e
the
p
olynomial
b
ounding
it's
running
time.
W
e
will
build
a
new
mac
hine
M
0
that
decides
L
in
a
p
olynomial
space.
Giv
en
an
input
x,
the
new
mac
hine
will
run
M
on
x
using
all
p
ossible
coin
tosses
with
length
p(jxj)
and
decides
b
y
ma
jorit
y
(i.e
if
M


LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
accepted
the
ma
jorit
y
of
it's
in
v
o
cations
then
M
0
accepts
x,
else
it
rejects
x).
Ev
ery
in
v
o
cation
of
M
on
x
requires
a
p
olynomial
space.
And,
b
ecause
w
e
can
use
the
same
space
for
all
in
v
o
cations,
w
e
see
that
M
0
uses
p
olynomial
space
(the
fact
that
w
e
run
it
exp
onen
tially
man
y
times
do
es
not
matter).
The
answ
er
of
M
0
is
correct
b
ecause
M
is
a
P
P
mac
hine
that
answ
ers
correctly
for
more
than
half
of
the
guesses.
.
Small
V
arian
ts
W
e
men
tioned
that,
in
Denition
.,
w
e
can't
tak
e

instead
of
of
>
b
ecause
this
will
giv
e
us
no
information.
But
what
ab
out
asking
for

when
x
=

L
and
>
when
x

L
(or
the
other
w
a
y
around)
?
W
e
will
sho
w,
in
the
next
claim,
that
this
will
not
c
hange
the
class
of
languages.
A
language
has
suc
h
a
mac
hine
if
and
only
if
it
has
a
P
P
mac
hine.
Consider
the
follo
wing
denition:
Denition
.
P
P

def
=

>
>
>
>
>
<
>
>
>
>
>
:
L

f0;
g












Ther
e
exist
a
p
olynomial
time
T
uring
machine
M
s.t
x

L
)
P
r
ob[M
(x)
=
]
>


x
=

L
)
P
r
ob[M
(x)
=
0]



	
>
>
>
>
>
=
>
>
>
>
>
;
The
next
claim
will
sho
w
that
this
relaxation
will
not
c
hange
the
class
dened:
Claim
..
P
P

=
P
P
Pro
of:
P
P

P
P

:
If
w
e
ha
v
e
a
mac
hine
that
satises
Denition
.
it
also
satises
Denition
.,
so
clearly
L

P
P
)
L

P
P
.
P
P

P
P

:
Let
L
b
e
an
y
language
in
P
P
.
If
M
is
the
mac
hine
guaran
teed
b
y
Denition
.,
and
p()
is
the
p
olynomial
b
ounding
it's
running
time
(and
th
us
the
n
um
b
er
of
coins
that
it
uses),
w
e
can
dene
another
mac
hine
M
0
as
follo
ws:
M
0

x;

a

;
a

;
:::;
a
p(jxj)+
;
b

;
b

;
:::;
b
p(jxj)


def
=
(
if
a

=
a

=
:::
=
a
p(jxj)+
=
0
then
r
etur
n
0
N
O
0
el
se
r
etur
n
M

x;

b

;
b

;
:::;
b
p(jxj)

M
0
c
ho
oses
one
of
t
w
o
mo
v
es.
One
mo
v
e,
whic
h
happ
ens
with
probabilit
y

 (p(jxj)+)
,
will
return
0
N
O
0
.
The
second
mo
v
e,
whic
h
happ
ens
with
probabilit
y

 
 (p(jxj)+)
will
in
v
ok
e
M
with
indep
enden
t
coin
tosses.
This
giv
es
us
that
P
r
ob[M
0
(x)
=
]
=
P
r
ob[M
(x)
=
]



 
 (p(jxj)+)

and
P
r
ob[M
0
(x)
=
0]
=
P
r
ob[M
(x)
=
0]



 
 (p(jxj)+)

+

 (p(jxj)+)

..
THE
CLASS
P
P

The
tric
k
is
to
shift
the
answ
er
of
M
to
w
ards
the
0
N
O
0
direction
with
a
v
ery
small
probabilit
y
.
This
shift
is
smaller
than
the
smallest
probabilit
y
dierence
that
M
could
ha
v
e.
So
if
M
(x)
is
biased
to
w
ards
the
0
Y
E
S
0
,
our
shift
will
k
eep
the
direction
of
the
bias
(it
will
only
lo
w
er
it).
But
if
there
is
no
bias
(or
bias
to
w
ards
NO),
our
shift
will
giv
e
us
a
bias
to
w
ards
the
0
N
O
0
answ
er.
If
x

L
then
P
r
ob[M
(x)
=
]
>


,
hence
P
r
ob[M
(x)
=
]



+

 p(jxj)
(b
ecause
the
dierence
is
at
least
one
computation
whic
h
happ
ens
with
probabilit
y

 p(jxj)
),
so:
P
r
ob[M
0
(x)
=
]




+

 p(jxj)




 
 (p(jxj)+)

=


+

 p(jxj)
 
 (p(jxj)+)
 
 (p(jxj)+)
>


If
x
=

L
then
P
r
ob[M
(x)
=
0]



,
hence
P
r
ob[M
0
(x)
=
0]






 
 (p(jxj)+)

+

 (p(jxj)+)
=


 
 (p(jxj)+)
+

 (p(jxj)+)
>


And,
as
a
conclusion,
w
e
get
that
in
an
y
case
P
r
ob[M
0
(x)
=

L
(x)]
>


So
M
0
satises
Denition
.,
and
th
us
L

P
P
.
.
N
P

P
P
Supp
ose
that
L

N
P
is
decided
b
y
a
nondeterministic
mac
hine
M
with
a
running-time
that
is
b
ounded
b
y
the
p
olynomial
p(jxj).
The
follo
wing
mac
hine
M
0
then
will
decide
L
b
y
means
of
Denition
.:
M
0

x;

b

;
b

;
:::;
b
p(jxj)+


def
=
(
if
b

=

then
r
etur
n
M

x;

b

;
b

;
:::;
b
p(jxj)+

el
se
r
etur
n
0
Y
E
S
0
M
0
uses
it's
random
coin-tosses
as
a
witness
to
M
with
only
one
toss
that
it
do
es
not
pass
to
M
0
.
This
toss
is
used
to
c
ho
ose
it's
mo
v
e.
One
of
the
t
w
o
p
ossible
mo
v
es
gets
it
to
the
ordinary
computation
of
M
with
the
same
input
(and
the
witness
is
the
random
input).
The
other
c
hoice
gets
it
to
a
computation
that
alw
a
ys
accepts.
Consider
a
string
x.
If
M
do
esn't
ha
v
e
an
accepting
computation
then
the
probabilit
y
that
M
0
will
answ
er

is
exactly


(it
is
the
probabilit
y
that
the
rst
coin
will
fall
on
one).
On
the
other
hand,
if
M
has
at
least
one
accepting
computation
then
the
probabilit
y
that
M
0
will
answ
er
correctly
is
greater
than


.
So
w
e
get
that:


LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
x

L
)
P
r
ob[M
0
(x)
=
]
>


x
=

L
)
P
r
ob[M
0
(x)
=
0]



By
Denition
.,
w
e
conclude
that
L

P
P
,
and
b
y
the
previous
claim
(P
P
=
P
P
),
w
e
get
that
L

P
P
.
.
coN
P

P
P
Easily
seen
from
the
symmetry
in
the
denition
of
P
P
.
.
The
class
Z
P
P
{
Zero
error
probabilit
y
.
R
P
denition
is
asymmetric
and
w
e
can't
sa
y
whether
R
P
=
coR
P
.
It
w
ould
b
e
in
teresting
to
examine
the
prop
erties
of
R
P
T
coR
P
whic
h
is
clearly
symmetric.
It
seems
that
problems
whic
h
are
in
R
P
T
coR
P
can
b
enet
from
the
accurate
result
of
R
P
deciding
T
uring
mac
hine
(if
x
=

L)
and
of
coR
P
deciding
T
uring
mac
hine
(if
x

L).
Another
in
teresting
thing
to
consider
is
to
let
the
mac
hine
sa
y
\I
don't
kno
w"
for
some
inputs.
W
e
will
discuss
mac
hines
that
can
return
this
answ
er
but
answ
er
correctly
otherwise.
W
e
will
pro
v
e
that
these
t
w
o
ideas
giv
e
rise
to
the
same
class
of
languages.
Denition
.
(Z
P
P
):
L

Z
P
P
if
ther
e
exist
a
pr
ob
abilistic
p
olynomial-time
T
uring
machine
M,
such
that:
x;
P
r
ob[M
(x)
=?]



x;
P
r
ob[M
(x)
=

L
(x)
or
M
(x)
=?]
=

Wher
e
we
denote
the
unknown
answer
sign
as
?.
Again
the
v
alue


is
arbitrary
and
can
b
e
replaced
lik
e
w
e
did
b
efore
to
b
e
an
ything
b
et
w
een

 p(jxj)
to

 
p(jxj)
.
If
w
e
ha
v
e
a
Z
P
P
mac
hine
that
do
esn't
kno
w
the
answ
er
with
probabilit
y
half,
w
e
can
run
it
p(jxj)
times
and
get
a
mac
hine
that
do
esn't
kno
w
the
answ
er
with
probabilit
y

 p(jxj)
b
ecause
this
is
the
probabilit
y
that
none
of
our
in
v
o
cation
kno
w
the
answ
er
(the
other
w
a
y
is
ob
vious
b
ecause

 p(jxj)
is
smaller
than


for
all
but
nal
inputs).
If
w
e
ha
v
e
a
mac
hine
that
kno
w
the
answ
er
with
probabilit
y

p(jxj)
,
w
e
can
use
it
to
build
a
mac
hine
that
kno
w
the
answ
er
with
probabilit
y


b
y
in
v
oking
it
p(jxj)
times
(the
other
w
a
y
is,
again,
trivial).
Prop
osition
..
Z
P
P
=
R
P
T
coR
P
Pro
of:
T
ak
e
L

Z
P
P
.
Let
M
b
e
the
mac
hine
guaran
teed
in
Denition
..
W
e
will
sho
w
ho
w
to
build
a
new
mac
hine
M
0
whic
h
decides
L
according
to
Denition
.
(this
will
imply
that
Z
P
P

R
P
).
M
0
(x)
def
=

>
<
>
:
b
 
M
(x)
if
b
=?
then
output
0
el
se
output
b
itsel
f
By
doing
so,
if
x
=

L
then
b
y
returning
0
when
M
(x)
=?
w
e
will
alw
a
ys
answ
er
correctly
(b
ecause
in
this
case
M
(x)
=?)
M
0
(x)
=

L
(x)
)
M
0
(x)
=
0).
If
x

L,
the
probabilit
y
of
getting
the
righ
t
answ
er
with
M
0
is
greater
than


b
ecause
M
will
return
a
denite
answ
er
(M
(x)
=?)
with
probabilit
y
greater
than


and
M
's
denite
answ
ers
are

..
RANDOMIZED
SP
A
CE
COMPLEXITY

alw
a
ys
correct
(it
nev
er
return
a
wrong
answ
er
b
ecause
it
returns
?
when
it
is
uncertain).
In
the
same
w
a
y
it
can
b
e
seen
that
Z
P
P

coR
P
(the
mac
hine
that
w
e
will
build
will
return

when
M
is
uncertain),
hence
w
e
get
that
Z
P
P

R
P
T
coR
P
.
Assume
no
w
that
L

R
P
T
coR
P
.
Let
M
RP
b
e
the
R
P
mac
hine
and
M
coRP
the
coR
P
mac
hine
that
decides
L
(according
to
Denition
.
and
Denition
.).
W
e
dene
M
0
(x)
using
M
RP
and
M
coRP
as
follo
ws:
M
0
(x)
def
=

>
<
>
:
r
un
M
RP
(x);
if
it
say
s
0
Y
E
S
0
then
r
etur
n

r
un
M
coRP
(x);
if
it
say
s
0
N
O
0
then
r
etur
n
0
other
w
ise
r
etur
n
?
If
M
RP
sa
ys
0
Y
E
S
0
then,
b
y
Denition
.,
w
e
are
guaran
teed
that
x

L.
Notice
that
it
can
happ
en
that
x

L
and
M
RP
(x)
=
0
but
not
the
other
w
a
y
around
(There
are
's
in
the
probabilit
y
space
M
(x)
when
x

L,
but
the
probabilit
y
space
M
(x)
when
x
=

L
is
all
zero
es.
So
if
M
(x)
returns
0
Y
E
S
0
,
w
e
kno
w
that
the
rst
probabilit
y
space
is
the
case).
In
a
similar
w
a
y
,
if
M
coRP
sa
ys
0
N
O
0
then,
b
y
Denition
.,
w
e
are
guaran
teed
that
x
=

L.
Th
us
w
e
nev
er
get
a
wrong
answ
er.
If
x

L
then,
b
y
Denition
.,
w
e
will
get
a
0
Y
E
S
0
answ
er
form
M
RP
and
hence
from
M
0
with
probabilit
y
greater
than


.
If
x
=

L
than,
b
y
Denition
.,
w
e
will
get
a
0
N
O
0
answ
er
form
M
coRP
and
hence
from
M
0
with
probabilit
y
greater
than


.
So
in
either
cases
w
e
can
b
e
sure
that
M
0
returns
a
denite
(not
?)
and
correct
answ
er
with
prob-
abilit
y
greater
than


.
The
conclusion
is
that
M
0
is
indeed
a
Z
P
P
mac
hine
so
R
P
T
coR
P

Z
P
P
and,
together
with
the
previous
part,
w
e
conclude
that
R
P
T
coR
P
=
Z
P
P
.
Summing
what
w
e
ha
v
e
seen
so
far
w
e
can
write
the
follo
wing
relations
P

Z
P
P

R
P

B
P
P
It
is
b
eliev
ed
that
B
P
P
=
P
so
there
is
no
real
help
that
randomized
computations
can
con
tribute
when
trying
to
solv
e
searc
h
problems.
Also
if
the
b
elief
is
true
then
all
the
distinctions
b
et
w
een
the
ab
o
v
e
classes
are
of
no
use.
.
Randomized
space
complexit
y
Lik
e
w
e
did
with
N
L,
w
e
also
dene
randomized
space
classes.
Here
also,
it
is
p
ossible
to
consider
b
oth
the
online
and
o-line
mo
dels
and
w
e
will
w
ork
with
the
online
mo
del.
..
The
denition
Denition
.	
F
or
any
function
S
:
N
!
N
R
S
P
AC
E
(S
)
def
=

>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
L

f0;
g














Ther
e
exists
a
r
andomize
d
T
uring
machine
M
s.t.
for
any
input
x

f0;
g

x

L
)
P
r
ob[M
(x)
=
]



x
=

L
)
P
r
ob[M
(x)
=
0]
=
0
and
M
uses
at
most
S
(jxj)
sp
ac
e
and
exp(S
(jxj))
time.
	
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
;


LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
W
e
are
in
terested
in
the
case
where
the
space
is
logarithmic.
The
class
whic
h
put
the
logarithmic
space
restriction
is
R
L.
Denition
.0
R
L
def
=
R
S
P
AC
E
(l
og
)
The
time
restriction
is
v
ery
imp
ortan
t.
Let
us
see
what
happ
ens
if
w
e
don't
put
the
time
restriction
in
Denition
.	.
Denition
.
F
or
any
function
S
:
N
!
N
badR
S
P
AC
E
(S
)
def
=

>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
L

f0;
g














Ther
e
exists
a
r
andomize
d
T
uring
machine
M
s.t.
for
any
input
x

f0;
g

x

L
)
P
r
ob[M
(x)
=
]



x
=

L
)
P
r
ob[M
(x)
=
0]
=
0
and
M
uses
at
most
S
(jxj)
sp
ac
e
(no
time
restrictions!)
	
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
;
Prop
osition
..
badRSP
A
CE(S)
=
NSP
A
CE(S)
Pro
of:
W
e
start
with
the
easy
direction.
Let
L

badR
S
P
AC
E
(S
).
If
x

L
then
there
are
man
y
witnesses
but
one
is
enough.
On
the
other
hand
for
x
=

L
there
are
no
witness.
The
other
direction
is
the
in
teresting
one.
Supp
ose
L

N
S
P
AC
E
(S
).
Let
M
b
e
the
Non-
deterministic
T
uring
mac
hine
whic
h
decides
L
in
space
S
(jxj).
Recall
that
for
ev
ery
x

L
there
exists
an
accepting
computation
of
M
on
input
x
whic
h
halts
within
exp(S
(jxj))
steps
(see
previous
lectures!).
Then
if
x

L
there
exist
r
of
length
exp(S
(jxj))
,
so
that
M
(x;
r
)
=

(here
r
denotes
the
oine
non-deterministic
guesses
used
b
y
M
).
Th
us,
selecting
r
uniformly
among
the
strings
of
length
exp(S
(jxj))
,
the
probabilit
y
that
M
(x;
r
)
=

is
at
least

 exp(S
(jxj))
.
So
if
w
e
rep
eatedly
in
v
ok
e
M
(x;
:)
on
random
r
's,
w
e
can
exp
ect
that
after

exp(S
(jxj))
tries
w
e
will
see
an
accepting
computation
(assuming
all
the
time
that
x

L).
Oded's
Note:
Note
that
the
ab
o
v
e
in
tuitiv
e
suggestion
already
abuses
the
fact
that
badRSP
A
CE
has
no
time
b
ounds.
W
e
plan
to
run
in
exp
ected
time
whic
h
is
double
exp
onential
in
the
space
b
ound;
whereas
the
go
o
d
denition
of
RSP
A
CE
allo
ws
only
time
exp
onen
tial
in
the
space
b
ound.
So
w
e
w
an
t
to
run
M
on
x
and
a
newly
randomly
selected
r
(of
length
exp(S
(jxj))
)
for
ab
out

exp(S
(jxj))
times
and
accept
i
M
accepts
in
one
of
these
tries.
A
naiv
e
implemen
tation
is
just
to
do
so.
But
this
requires
holding
a
coun
ter
capable
of
coun
ting
upto
t
def
=

exp(S
(jxj))
,
whic
h
means
using
space
exp(S
(jxj))
(whic
h
is
m
uc
h
more
than
w
e
are
allo
w
ed).
So
w
e
ha
v
e
the
basic
idea
whic
h
is
go
o
d
but
still
ha
v
e
a
problem
ho
w
to
coun
t.
The
solution
will
b
e
to
use
a
\randomized
coun
ter"
that
will
only
use
S
(jxj)
space.
The
randomized
coun
ter
is
implemen
ted
as
follo
ws.
W
e
\ip"
k
=
log

t
coins.
If
all
are
heads
then
w
e
will
stop
otherwise
w
e
go
on.
The
exp
ected
n
um
b
er
of
tries
is

 k
=
t,
exactly
the
n
um
b
er
of
tries
w
e
w
an
ted
to
ha
v
e.
But
this
randomized
coun
ter
requires
only
a
real
coun
ter
capable
of
coun
ting
upto
k
,
and
so
can
b
e
implemen
ted
in
space
log

k
=
log

log

t
=
S
(jxj).
Clearly
,
Claim
..
L

R
L

N
L

..
RANDOMIZED
SP
A
CE
COMPLEXITY
	
..
Undirected
Graph
Connectivit
y
is
in
RL
In
the
previous
lecture
w
e
sa
w
that
directed
connectivit
y
is
NL-Complete.
W
e
will
no
w
sho
w
in
brief
that
undirected
connectivit
y
is
in
R
L.
The
problem
is
dened
as
follo
ws.
Input:
An
undirected
graph
G
and
t
w
o
v
ertices
s
and
t.
T
ask:
Find
if
there
is
a
path
b
et
w
een
s
and
t
in
G.
Claim
..
L
et
n
denote
the
numb
er
of
vertic
es
in
the
gr
aph.
Then,
with
pr
ob
ability
at
le
ast


,
a
r
andom
walk
of
length
n

starting
fr
om
s
visits
al
l
vertic
es
in
the
c
onne
cte
d
c
omp
onent
of
s.
By
a
random
w
alk,
w
e
mean
a
w
alk
whic
h
iterativ
ely
selects
at
random
a
neigh
b
our
of
the
curren
t
v
ertex
and
mo
v
es
to
it.
Pro
of
sk
etc
h:
In
the
follo
wing,
w
e
consider
the
connected
comp
onen
t
of
v
ertex
s,
denoted
G
0
=
(V
0
;
E
0
).
F
or
an
y
edge,
(u;
v
)
(in
E
0
),
w
e
let
T
u;v
b
e
a
random
v
ariable
represen
ting
the
n
um
b
er
of
steps
tak
en
in
a
random
w
alk
starting
at
u
un
til
v
is
rst
encoun
tered.
It
is
easy
to
see
that
E[T
u;v
]

jE
0
j.
Also,
letting
co
v
er
(G
0
)
b
e
the
exp
ected
n
um
b
er
of
steps
in
a
random
w
alk
starting
at
s
and
ending
when
the
last
of
the
v
ertices
of
V
0
is
encoun
tered,
and
C
b
e
an
y
directed
cycle
whic
h
visits
all
v
ertices
in
G
0
,
w
e
ha
v
e
co
v
er
(G
0
)

X
(u;v
)C
E[T
u;v
]

jC
j

jE
0
j
Letting
C
b
e
a
tra
v
ersal
of
some
spanning
tree
of
G
0
,
w
e
conclude
that
co
v
er
(G
0
)
<


jE
0
j

jV
0
j.
Th
us,
with
probabilit
y
at
least
=,
a
random
w
alk
of
length


jE
0
j

jV
0
j
starting
at
s
visits
all
v
ertices
of
G
0
.
The
algorithm
for
deciding
undirected
connectivit
y
is
no
w
ob
vious:
Just
tak
e
a
\random
w
alk"
of
length
n

starting
from
v
ertex
s
and
see
if
t
is
encoun
tered.
The
space
requiremen
t
is
merely
a
register
to
hold
the
curren
t
v
ertex
(i.e.,
log
n
space)
and
a
coun
ter
to
coun
t
upto
n

(again
(log
n)
space).
F
urthermore,
the
use
of
a
coun
ter
guaran
tees
that
the
running
time
of
the
algorithm
is
exp
onen
tial
in
its
(logarithmic)
space
b
ound.
The
implemen
tation
is
straigh
tforw
ard
.
Set
counter
=
0
and
v
=
s.
Compute
n
(the
n
um
b
er
of
v
ertices
in
the
graph).
.
Uniformly
select
a
neigh
b
our
u
of
v
.
.
If
u
=
t
then
halt
and
accept,
else
set
v
=
u
and
counter
=
counter
+
.
.
If
counter
=
n

then
halt
and
reject,
else
goto
Step
().
Cleraly
,
if
s
is
connected
to
t
then,
b
y
the
ab
o
v
e
claim,
the
algorithm
accepts
with
probabilit
y
at
least
=.
On
the
other
hand,
the
algorithm
alw
as
rejects
if
s
is
not
connected
to
t.
Th
us,
UNdirected
graph
CONNectivit
y
(UNCONN)
is
in
R
L.
Note
that
the
straigh
tforw
ard
adaptation
of
the
ab
o
v
e
algorithm
to
the
directed
case
(i.e.,
directed
graph
connectivit
y
considered
in
previous
lecture)
fails:
Consider,
for
example,
a
directed
graph
consisting
of
a
directed
path

!

!



!
n
augmen
ted
b
y
directed
edges
going
from
ev
ery
v
ertex
i
>

to
v
ertex
.
An
algorithm
whic
h
tries
to
tak
e
a
directed
random
w
alk
starting
from

	0
LECTURE
.
RANDOMIZED
COMPUT
A
TIONS
v
ertex

is
highly
unlik
ely
to
reac
h
v
ertex
n
in
p
oly
(n)
man
y
steps.
Lo
osely
sp
eaking,
this
is
the
case
since
in
eac
h
step
from
a
v
ertex
i
>
,
w
e
mo
v
e
to
w
ards
v
ertex
n
with
probabilit
y
=,
but
otherwise
return
to
v
ertex
.
The
fact
that
the
ab
o
v
e
algorithm
fails
should
not
come
as
a
great
surprise,
as
the
directed
connectivit
y
problem
is
NL-complete
and
so
placing
it
in
RL
will
imply
N
L
=
R
L.
Oded's
Note:
N
L
=
R
L
is
not
considred
as
unlik
ely
as
N
P
=
R
P
,
but
ev
en
if
N
L
=
R
L
pro
ving
this
seems
v
ery
hard.
Bibliographic
Notes
Probabilistic
T
uring
Mac
hines
and
corresp
onding
complexit
y
classes
(including
B
P
P
;
RP
;
Z
P
P
and
P
P
)
w
ere
rst
dened
b
y
Gill
[].
The
pro
of
that
NSP
A
CE
equals
badRSP
A
CE
(called
RSP
A
CE
in
[]),
as
w
ell
as
the
tec
hinque
of
a
randomized
coun
ter
is
from
[].
The
robusteness
of
the
v
arious
classes
under
v
arious
thresholds
w
as
established
ab
o
v
e
using
straigh
tforw
ard
amplications
(i.e.,
running
the
algorithm
sev
eral
times
with
indep
enden
t
random
c
hoices).
Randomness-ecien
t
amplication
metho
ds
ha
v
e
b
een
extensiv
ely
studied
since
the
mid
	0's.
See
Section
.
in
[].
The
random-w
alk
algorithm
for
deciding
undirected
connectivit
y
is
due
to
Aleliunas
et.
al.
[].
Other
examples
of
randomized
algorithms
can
b
e
found
in
App
endix
B.
of
[].
W
e
sp
ecically
recommend
the
follo
wing
examples

T
esting
primalit
y
(B..):
This
B
P
P
algorithm
is
dieren
t
from
the
famous
coR
P
algorithm
for
recognizing
the
set
of
primes.

Finding
a
p
erfect
matc
hing
(B..):
This
algorithm
is
arguably
simpler
than
kno
wn
deter-
ministic
p
olynomial-time
algorithms.

Finding
minim
um
cuts
in
graphs
(B..):
This
algorithm
is
arguably
simpler
than
kno
wn
deterministic
p
olynomial-time
algorithms.
A
m
uc
h
more
extensiv
e
treatmen
t
of
randomized
algorithm
is
giv
en
in
[].
.
R.
Aleliunas,
R.M.
Karp,
R.J.
Lipton,
L.
Lo
v
asz
and
C.
Rac
k
o.
Random
w
alks,
univ
ersal
tra
v
ersal
sequences,
and
the
complexit
y
of
maze
problems.
In
0th
F
OCS,
pages
{,
		.
.
J.
Gill.
Computational
complexit
y
of
probabilistic
T
uring
mac
hines.
SIAM
Journal
on
Com-
puting,
V
ol.
(),
pages
{	,
	.
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.
Copies
ha
v
e
b
een
placed
in
the
facult
y's
library
.
.
R.
Mot
w
ani
and
P
.
Ragha
v
an.
R
andomize
d
A
lgorithms,
Cam
bridge
Univ
ersit
y
Press,
		.

Lecture

Non-Uniform
P
olynomial
Time
(P
/P
oly)
Notes
tak
en
b
y
Moshe
Lew
enstein,
Y
eh
uda
Lindell
and
T
amar
Seeman
Summary:
In
this
lecture
w
e
in
tro
duce
the
notion
of
non-uniform
p
olynomial
time
and
the
corresp
onding
complexit
y
class
P
/p
oly
.
In
this
computational
mo
del,
T
uring
mac
hines
are
pro
vided
an
external
advice
string
to
aid
them
in
their
computation.
The
non-uniformity
is
expressed
in
the
fact
that
a
dieren
t
advice
string
ma
y
b
e
dened
for
ev
ery
dieren
t
length
of
input.
W
e
sho
w
that
P
/p
oly
upp
er
b
ounds
ecien
t
com-
putation
(as
B
P
P

P
/p
oly),
y
et
ev
en
con
tains
some
non-recursiv
e
languages.
The
eect
of
in
tro
ducing
uniformity
is
discussed
(as
an
attempt
to
rid
P
/p
oly
of
its
ab-
surd
in
tractable
languages)
and
sho
wn
to
reduce
the
class
to
b
e
exactly
P
.
Finally
,
w
e
sho
w
that,
among
other
things,
P
/p
oly
ma
y
help
us
separate
P
from
N
P
.
W
e
do
this
b
y
sho
wing
that
trivially
P

P
/p
oly
,
and
that
under
a
reasonable
conjecture
N
P

P
/p
oly
.
.
In
tro
duction
The
class
of
P
/p
oly
,
or
non-uniform
p
olynomial
time,
is
the
class
of
T
uring
mac
hines
whic
h
receiv
e
external
advice
to
aid
computation.
More
sp
ecically
for
all
inputs
of
length
n
a
T
uring
mac
hine
is
supplemen
ted
with
a
single
advice
string
a
n
of
p
olynomial
length
.
Alternativ
ely
w
e
ma
y
view
a
non-uniform
mac
hine
as
an
innite
series
of
T
uring
mac
hines
fM
n
g,
where
M
i
computes
for
inputs
of
length
i.
In
this
case
the
advice
is
\hardwired"
in
to
the
mac
hine.
The
class
of
P
/p
oly
pro
vides
an
upp
er
b
ound
on
what
is
considered
to
b
e
ecien
t
computation.
This
upp
er
b
ound
is
not
tigh
t;
for
example,
as
w
e
shall
sho
w
later,
P
/p
oly
con
tains
non-recursiv
e
languages.
Ho
w
ev
er,
the
upp
er
b
ound
ensures
that
ev
ery
ecien
tly
computable
language
is
con-
tained
in
P
/p
oly
.
An
additional
motiv
ation
in
creating
the
class
of
P
/p
oly
is
to
help
separate
the
classes
of
P
and
N
P
.
This
idea
is
explained
in
further
detail
b
elo
w.
	

	
LECTURE
.
NON-UNIF
ORM
POL
YNOMIAL
TIME
(P
/POL
Y)
..
The
Actual
Denition
W
e
no
w
dene
the
class
of
P
/p
oly
according
to
t
w
o
dieren
t
denitions,
and
then
sho
w
that
these
t
w
o
denitions
are
in
fact
equiv
alen
t.
Recall
that:

L
(x)
=

;
if
x

L;
0;
otherwise.
Denition
.
(standard):
L

P
/p
oly
if
ther
e
exists
a
se
quenc
e
of
cir
cuits
fC
n
g,
wher
e
for
e
ach
n,
C
n
has
n
inputs
and
one
output,
and
ther
e
exists
a
p
olynomial
p(:)
such
that
for
al
l
n,
size(C
n
)

p(n)
and
C
n
(x)
=

L
(x)
for
al
l
x

f0;
g
n
.
A
series
of
p
olynomial
circuits
fC
n
g
as
dened
ab
o
v
e
is
called
a
non-uniform
family
of
cir
cuits.
The
non-uniformit
y
is
expressed
in
the
fact
that
there
is
not
necessarily
an
y
connection
b
et
w
een
a
circuit
of
size
n
and
n
+
.
In
fact
for
ev
ery
n
w
e
ma
y
dene
a
completely
dieren
t
\algorithm".
Note
that
the
circuits
in
the
ab
o
v
e
denition
can
b
e
sim
ulated
in
time
linear
to
their
size.
Th
us
although
time
is
not
explicitly
men
tioned
in
the
denition,
it
is
implicit.
Denition
.
(alternativ
e):
L

P
/p
oly
if
ther
e
exists
a
p
olynomial-time
two-input
machine
M
,
a
p
olynomial
p(:),
and
a
se
quenc
e
fa
n
g
of
advic
e
strings,
wher
e
length(a
n
)

p(n),
such
that
for
al
l
n
and
for
al
l
x

f0;
g
n
;
M
(a
n
;
x)
=

L
(x).
If
exp
onen
tially
long
advice
w
ere
allo
w
ed
in
the
ab
o
v
e
denition,
then
a
n
could
b
e
a
lo
ok-up
table
con
taining

L
(x)
for
an
y
language
L
and
ev
ery
input
x
of
length
n.
Th
us
ev
ery
language
w
ould
trivially
b
e
in
suc
h
a
class.
Ho
w
ev
er,
this
is
not
the
case
as
a
n
is
p
olynomially
b
ounded.
Restricting
the
length
of
the
advice
denes
a
more
meaningful
class,
but
as
w
e
ha
v
e
men
tioned,
some
in
tractable
problems
still
remain
\solv
able".
Prop
osition
..
The
two
denitions
of
P
/p
oly
ar
e
e
quivalent.
Pro
of:
()):
Assume
L

P
/p
oly
b
y
Denition
,
i.e.
there
exists
a
family
fC
n
g
of
circuits
deciding
L,
suc
h
that
size(C
n
)
is
p
olynomial
in
n.
Let
desc(C
n
)
b
e
the
description
of
C
n
according
to
a
standard
enco
ding
of
circuits.
Consider
the
univ
ersal
T
uring
mac
hine
M
suc
h
that
for
all
n,
and
all
x
of
length
n,
M
(desc(C
n
),x)
sim
ulates
C
n
(x).
Then
dene
the
sequence
fa
n
g
of
advice
strings
suc
h
that
for
ev
ery
n,
a
n
=
desc(C
n
).
Th
us
L

P
/p
oly
b
y
Denition
.
(():
Assume
L
is
in
P
/p
oly
b
y
Denition
,
i.e.
there
exist
a
T
uring
mac
hine
M
and
a
sequence
of
advice
fa
n
g
deciding
L.
W
e
lo
ok
at
all
p
ossible
computations
of
M
(a
n
;
)
for
n-bit
inputs.
M
(a
n
;
)
is
a
p
olynomial
time-b
ounded
deterministic
T
uring
mac
hine
w
orking
on
n-length
inputs.
In
the
pro
of
of
Co
ok's
Theorem,
in
Lecture
,
w
e
sho
w
ed
that
Bounded
Halting
is
Levin-reducible
to
Circuit
Satisabilit
y
.
Giv
en
an
instance
of
Bounded
Halting
(<
M
(;
)
>;
x;

t
)
the
reduction
is
comprised
of
constructing
a
circuit
C
whic
h
on
input
y
outputs
M
(x;
y
).
The
situation
here
is
iden
tical
since
for
M
(a
n
;
)
a
circuit
ma
y
b
e
constructed
whic
h
on
input
x
outputs
M
(a
n
;
x).
In
other
w
ords
w
e
build
a
sequence
fC
n
g
of
circuits,
where
for
eac
h
n,
C
n
is
an
enco
ding
of
M
(a
n
;
).
Th
us
L
is
in
P
/p
oly
b
y
Denition
.
It
should
b
e
noted
that
in
Denition
,
M
is
a
nite
ob
ject,
whereas
fa
n
g
ma
y
b
e
an
innite
se-
quence
(as
is
the
sequence
fC
n
g
of
circuits
according
to
Denition
).
Th
us
P
/p
oly
is
an
unrealistic
mo
de
of
computation,
since
suc
h
mac
hines
cannot
actually
b
e
constructed.

..
THE
PO
WER
OF
P
/POL
Y
	
..
P
/p
oly
and
the
P
v
ersus
N
P
Question
As
men
tioned
ab
o
v
e,
one
of
the
motiv
ations
in
dening
the
class
of
P
/p
oly
is
to
separate
P
from
N
P
.
The
idea
is
to
sho
w
that
there
is
a
language
whic
h
is
in
N
P
but
is
not
in
P
/p
oly
,
and
th
us
not
in
P
.
In
this
w
a
y
,
w
e
w
ould
lik
e
to
sho
w
that
P
=
N
P
.
T
o
do
so,
though,
w
e
m
ust
rst
understand
the
relationship
of
P
/p
oly
to
the
classes
P
and
N
P
.
T
rivially
,
P

P
/p
oly
b
ecause
the
class
P
ma
y
b
e
view
ed
as
the
set
of
P
/p
oly
mac
hines
with
empt
y
advice,
i.e.
a
n
=

for
all
n.
A
t
rst
glance,
Denition

of
P
/p
oly
app
ears
to
resem
ble
that
of
N
P
.
In
N
P
,
x

L
i
there
exists
a
witness
w
x
suc
h
that
M
(x;
w
x
)
=
.
The
witness
is
somewhat
analogous
to
the
advice
in
P
/p
oly
.
Ho
w
ev
er,
the
denition
of
P
/p
oly
diers
from
that
of
N
P
in
t
w
o
w
a
ys:
.
F
or
a
giv
en
n,
P
/p
oly
has
a
univ
ersal
witness
a
n
as
opp
osed
to
N
P
where
ev
ery
x
of
length
n
ma
y
ha
v
e
a
dieren
t
witness.
.
In
the
denition
of
N
P
,
for
ev
ery
x
=

L,
for
ev
ery
witness
w
,
M
(x;
w
)
=
0.
In
other
w
ords,
there
do
not
exist
false
witnesses.
Ho
w
ev
er,
this
is
not
true
for
P
/p
oly
.
W
e
do
not
claim
that
there
are
no
bad
advice
strings
for
Denition

of
P
/p
oly;
w
e
merely
claim
that
there
exists
a
go
o
d
advice
string.
W
e
therefore
see
that
the
denitions
of
N
P
and
P
/p
oly
dier
from
eac
h
other;
this
raises
the
p
ossibilit
y
that
there
ma
y
b
e
a
language
whic
h
is
in
N
P
but
not
in
P
/p
oly
.
As
w
e
shall
sho
w
later
this
seems
to
b
e
lik
ely
since
a
sucien
t
condition
for
the
existence
of
suc
h
a
language
is
based
up
on
a
reasonable
conjecture.
Since
P
is
con
tained
in
P
/p
oly
,
nding
suc
h
a
language
is
sucien
t
to
fulll
our
goal.
In
fact,
the
original
motiv
ation
for
P
/p
oly
w
as
the
b
elief
that
one
ma
y
b
e
able
to
pro
v
e
lo
w
er
b
ounds
on
sizes
of
circuits
computing
certain
functions
(e.g.,
the
c
haracteriztic
function
of
an
NP-complete
language).
So
far,
no
suc
h
b
ounds
are
kno
wn
(except
if
one
restricts
the
circuits
in
v
arious
w
a
ys;
as
w
e'll
discuss
in
next
semester).
.
The
P
o
w
er
of
P
/p
oly
As
w
e
ha
v
e
men
tioned,
P
/p
oly
is
not
a
realistic
mo
de
of
computation.
Rather,
it
pro
vides
an
upp
er
b
ound
on
what
w
e
consider
ecien
t
computation
(that
is,
an
y
language
not
in
P
/p
oly
should
denitely
not
b
e
ecien
tly
computable).
In
the
last
lecture
w
e
dened
probabilistic
computation
and
reev
aluated
our
view
of
ecien
t
computation
to
b
e
B
P
P
,
rather
than
P
.
W
e
no
w
sho
w
that
B
P
P

P
/p
oly
and
therefore
that
P
/p
oly
also
upp
er
b
ounds
our
\new"
view
of
ecien
t
computation.
Ho
w
ev
er,
w
e
will
also
sho
w
that
P
/p
oly
con
tains
far
more
than
B
P
P
.
This
actually
yields
a
v
ery
high
upp
er
b
ound.
In
fact
P
/p
oly
ev
en
con
tains
non-recursiv
e
languages.
This
con
tainmen
t
should
con
vince
an
y
one
that
P
/p
oly
do
es
not
reect
an
y
lev
el
of
realistic
computation.
Theorem
.
:
P
/p
oly
c
ontains
non-r
e
cursive
languages.
Pro
of:
This
theorem
is
clearly
implied
from
the
follo
wing
t
w
o
facts:
.
There
exist
unary
languages
whic
h
are
non-recursiv
e,
and
.
F
or
ev
ery
unary
language
L,
L

P
/p
oly
.

	
LECTURE
.
NON-UNIF
ORM
POL
YNOMIAL
TIME
(P
/POL
Y)
W
e
remind
the
reader
that
L
is
a
unary
language
if
L

fg

.
Pro
of
of
Claim
:
Let
L
b
e
an
y
non-recursiv
e
language.
Dene
L
0
=
f
index(x)
j
x

Lg
where
index(x)
is
the
p
osition
of
x
in
the
standard
en
umeration
of
binary
strings
(i.e.
w
e
view
the
string
as
a
binary
n
um
b
er).
Clearly
L
0
is
unary
and
non-recursiv
e
(an
y
T
uring
mac
hine
recognizing
L
0
can
trivially
b
e
used
to
recognize
L).
Pro
of
of
Claim
:
F
or
ev
ery
unary
language
L,
dene
a
n
=

;
if

n

L;
0;
otherwise.
A
T
uring
mac
hine
can
trivially
decide
L
in
p
olynomial
(ev
en
linear)
time
giv
en
x
and
a
jxj
,
b
y
simply
accepting
i
x
is
unary
and
a
jxj
=
.
Therefore,
L

P
/p
oly
.
The
abilit
y
to
decide
in
tractable
languages
is
a
result
of
the
non-uniformit
y
inheren
t
in
P
/p
oly
.
There
is
no
requiremen
t
that
the
series
fa
n
g
is
ev
en
computable.
Note
that
this
metho
d
of
reducing
a
language
to
its
unary
equiv
alen
t
cannot
help
us
with
p
olynomial
classes
as
the
reduction
itself
is
exp
onen
tial.
Ho
w
ev
er,
for
recursiv
e
languages
w
e
are
in
terested
in
computabilit
y
only
.
Theorem
.
:
B
P
P

P
/p
oly.
Pro
of:
Let
L

B
P
P
.
By
means
of
amplication,
there
exists
a
probabilistic
T
uring
mac
hine
M
suc
h
that
for
ev
ery
x

f0;
g
n
:
P
r
ob
r
f0;g
poly
(n)
[M
(x;
r
)
=

L
(x)]
>

 
 n
,
(the
probabilities
are
tak
en
o
v
er
all
p
ossible
c
hoices
of
random
strings).
Equiv
alen
tly
,
M
is
suc
h
that
P
r
ob
r
[M
(x;
r
)
=

L
(x)]
<

 n
.
W
e
therefore
ha
v
e:
P
r
ob
r
[	x

f0;
g
n
:
M
(x;
r
)
=

L
(x)]

X
xf0;g
n
P
r
ob
r
[M
(x;
r
)
=

L
(x)]
<

n


 n
=
:
The
rst
inequalit
y
comes
from
the
Union
Bound,
that
is,
for
ev
ery
series
of
sets
fA
i
g
and
ev
ery
random
v
ariable
X
:
P
r
ob(X

n
[
i=
A
i
)

n
X
i=
P
r
ob(X

A
i
):
and
the
second
inequalit
y
is
based
on
the
error
probabilit
y
of
the
mac
hine.
Note
that
if
for
ev
ery
random
string
r
,
there
is
at
least
one
x
suc
h
that
M
(x;
r
)
=

L
(x),
then
the
ab
o
v
e
probabilit
y
w
ould
equal
.
W
e
can
therefore
conclude
that
there
is
at
le
ast
one
string
r
suc
h
that
for
ev
ery
x,
M
(x;
r
)
=

L
(x).
W
e
therefore
set
a
n
=
r
(note
that
r
is
dieren
t
for
dieren
t
lengths
of
input
n,
but
this
is
ne
according
to
the
denition
of
P
/p
oly).
Our
P
/p
oly
mac
hine
sim
ulates
M
,
using
a
n
as
its
random
c
hoices.
This
metho
d
of
pro
of
is
called
the
pr
ob
abilistic
metho
d.
W
e
do
not
kno
w
ho
w
to
nd
these
advice
strings
and
the
pro
of
of
their
existence
is
implicit.
W
e
merely
argue
that
the
probabilit
y
that
a
random
string
is
not
an
adequate
advice
is
strictly
smaller
than
.
This
is
enough
to
obtain
the
theorem.

..
UNIF
ORM
F
AMILIES
OF
CIR
CUITS
	
.
Uniform
F
amilies
of
Circuits
As
w
e
ha
v
e
men
tioned
earlier,
circuits
of
dieren
t
sizes
b
elonging
to
a
non-uniform
family
ma
y
ha
v
e
no
relation
to
eac
h
other.
This
results
in
the
absurd
situation
of
ha
ving
families
of
circuits
deciding
non-recursiv
e
languages.
This
leads
us
to
the
follo
wing
denition
whic
h
attempts
to
dene
families
of
circuits
whic
h
do
matc
h
our
exp
ectations
of
realistic
computation.
Denition
.
(uniform
circuits):
A
family
of
cir
cuits
fC
n
g
is
c
al
le
d
unifo
rm
if
ther
e
exists
a
deterministic
p
olynomial
time
T
uring
machine
M
such
that
for
every
n,
M
(
n
)
=
desc(C
n
),
wher
e
desc(C
n
)
is
a
standar
d
enc
o
ding
of
cir
cuits.
Th
us
a
uniform
family
of
circuits
has
a
succinct
(nite)
description
(or
equiv
alen
tly
for
a
series
of
advice
strings).
Clearly
,
a
uniform
family
of
circuits
cannot
recognize
non-recursiv
e
languages.
Actually
,
the
restriction
of
uniformit
y
is
far
greater
than
just
this.
Theorem
.
:
A
language
L
has
a
uniform
family
of
cir
cuits
fC
n
g
such
that
for
al
l
n
and
for
al
l
x

f0;
g
n
C
n
(x)
=

L
(x)
if
and
only
if
L

P
.
Pro
of:
())
Let
fC
n
g
b
e
a
uniform
family
of
circuits
deciding
L,
and
M
the
p
olynomial
time
T
uring
mac
hine
whic
h
generates
the
family
.
The
follo
wing
is
a
p
olynomial
time
algorithm
for
deciding
L:
On
input
x:

C
jxj
 
M
(
jxj
)

Sim
ulate
C
jxj
(x)
and
return
the
result.
Since
M
is
p
olynomial-time
b
ounded
and
the
circuits
are
of
p
olynomial
size,
the
algorithm
clearly
runs
in
p
olynomial
time.
Therefore
L

P
.
(()
L

P
.
Therefore,
there
exists
a
p
olynomial
time
T
uring
mac
hine
M
deciding
L.
As
in
the
pro
of
of
Co
ok's
Theorem,
a
p
olynomial
size
circuit
deciding
L
on
strings
of
length
n
ma
y
b
e
built
from
M
in
time
p
olynomial
in
n.
The
T
uring
mac
hine
M
0
that
constructs
the
circuits
ma
y
then
b
e
tak
en
as
M
in
the
denition
of
uniform
circuits.
That
is,
giv
en
x,
M
0
calculates
jxj
and
builds
the
appropriate
circuit.
Alternativ
ely
,
b
y
Denition
,
no
advice
is
necessary
here
and
w
e
ma
y
therefore
tak
e
a
n
=

for
ev
ery
n.
.
Sparse
Languages
and
the
P
v
ersus
N
P
Question
In
this
section
w
e
will
see
wh
y
P
/p
oly
ma
y
help
us
separate
b
et
w
een
P
and
N
P
.
W
e
will
rst
dene
sparse
languages.
Denition
.
(sparse
languages):
A
language
S
is
spa
rse
if
ther
e
exists
a
p
olynomial
p()
such
that
for
every
n
jS
\
f0;
g
n
j

p(n).
Example:
T
rivially
,
ev
ery
unary
language
is
sparse
(tak
e
p(n)
=
).

	
LECTURE
.
NON-UNIF
ORM
POL
YNOMIAL
TIME
(P
/POL
Y)
Theorem
.
:
N
P

P
/p
oly
if
and
only
if
for
every
L

N
P
,
the
language
L
is
Co
ok-r
e
ducible
to
a
sp
arse
language.
As
w
e
conjecture
that
no
N
P
-Complete
language
can
b
e
sparse,
w
e
ha
v
e
that
N
P
con
tains
lan-
guages
not
found
in
P
/p
oly
.
Pro
of:
It
is
enough
for
us
to
pro
v
e
that
SA
T

P
/p
oly
if
and
only
if
SA
T
is
Co
ok-reducible
to
some
sparse
language.
())
Supp
ose
that
SA
T

P
/p
oly
.
Therefore
there
exists
a
series
of
advice
strings
fa
n
g
and
a
T
uring
mac
hine
M
as
in
Denition
,
where
n
ja
n
j

q
(n)
for
some
p
olynomial
q
().
Dene
s
n
i
=
0
i 
0
q
(n) i
and
dene
S
=
f
n
0s
n
i
:
for
n

0
where
bit
i
of
a
n
is

g.
Clearly
S
is
sparse
since
for
ev
ery
n
jS
\
f0;
g
n+q
(n)+
j

ja
n
j

q
(n).
W
e
no
w
sho
w
a
Co
ok-reduction
of
SA
T
to
S
:
Input:
'
of
length
n
.
Reconstruct
a
n
b
y
q
(n)
queries
to
S.
Sp
ecically
,
the
queries
are:

n
0s
n

;

n
0s
n

;
:::;

n
0s
n
q
(n)
.
.
Run
M
(a
n
;
')
thereb
y
solving
SA
T
in
(standard)
p
olynomial
time.
W
e
therefore
solv
e
SA
T
with
a
p
olynomial
n
um
b
er
of
queries
to
an
S
-oracle,
i.e.
SA
T
Co
ok-reduces
to
S
.
(()
Supp
ose
that
SA
T
Co
ok-reduces
to
some
sparse
language
S
.
Therefore,
there
exists
a
p
olyno-
mial
time
b
ounded
oracle
mac
hine
M
S
whic
h
solv
es
SA
T.
Let
t()
b
e
M
's
(p
olynomial)
time-b
ound.
Then,
on
input
x,
mac
hine
M
mak
es
queries
of
length
at
most
t(jxj).
Construct
a
n
in
the
follo
wing
w
a
y:
concatenate
all
strings
of
length
at
most
t(n)
in
S.
Since
S
is
sparse,
there
exists
some
p
olynomial
p()
suc
h
that
n
jS
\
f0;
g
n
j

p(n).
The
length
of
the
list
of
strings
of
lengths
exactly
i
in
a
n
is
then
less
than
or
equal
to
i

p(i)
(i.e.
at
most
p(i)
dieren
t
strings
of
length
i
eac
h).
Therefore:
ja
n
j

t(n)
X
i=
i

p(i)
<
t(n)


p(t(n))
So,
a
n
is
p
olynomial
in
length.
No
w,
giv
en
a
n
,
ev
ery
oracle
query
to
S
can
b
e
\answ
ered"
in
p
olynomial
time.
F
or
a
giv
en
string
x,
w
e
c
hec
k
if
x

S
b
y
simply
scanning
a
n
and
seeing
if
x
app
ears
or
not.
Therefore,
M
S
ma
y
b
y
sim
ulated
b
y
a
deterministic
mac
hine
with
access
to
a
n
.
This
mac
hine
tak
es
at
most
t(n)

ja
n
j
time
(eac
h
lo
okup
ma
y
tak
e
as
long
as
scanning
the
advice).
Therefore
SA
T

P
/p
oly
.
As
w
e
ha
v
e
men
tioned,
w
e
conjecture
that
there
are
no
sparse
N
P
-Complete
languages.
This
conjecture
holds
for
b
oth
Karp
and
Co
ok
reductions.
Ho
w
ev
er
for
Karp-reductions,
the
ramica-
tions
of
the
existence
of
a
sparse
N
P
-Complete
language
w
ould
b
e
extreme,
and
w
ould
sho
w
that
P
=
N
P
.
This
is
formally
stated
and
pro
v
ed
in
the
next
theorem.
It
is
in
teresting
to
note
that
our
b
elief
that
N
P

P
/p
oly
is
somewhat
parallel
to
our
b
elief
that
P
=
N
P
when
lo
ok
ed
at
in
the
con
text
of
sparse
languages.
Theorem
.	
P
=
N
P
if
and
only
if
for
every
language
L

N
P
,
the
language
L
is
Karp-
r
e
ducible
to
a
sp
arse
language.

..
SP
ARSE
LANGUA
GES
AND
THE
P
VERSUS
N
P
QUESTION
	
Pro
of:
()):
Let
L

N
P
.
W
e
dene
the
follo
wing
trivial
function
as
a
Karp-reduction
of
L
to
fg:
f
(x)
=

;
if
x

L;
0;
otherwise.
If
P
=
N
P
then
L
is
p
olynomial-time
decidable
and
it
follo
ws
that
f
is
p
olynomial-time
computable.
Therefore,
L
Karp-reduces
to
the
language
fg,
whic
h
is
ob
viously
sparse.
(():
F
or
sak
e
of
simplicit
y
w
e
pro
v
e
a
w
eak
er
result
for
this
direction.
Ho
w
ev
er
the
claim
is
true
as
stated.
Beforehand
w
e
need
the
follo
wing
denition:
Denition
.0
(guarded
sparse
languages):
A
sp
arse
language
S
is
c
al
le
d
gua
rded
if
ther
e
exists
a
sp
arse
language
G
in
P
such
that
S

G.
The
language
that
w
e
considered
in
the
pro
of
of
theorem
:
S
=
f
n
0s
n
i
:
for
n

0,
where
bit
i
of
a
n
is
g
is
an
example
of
a
sparse
guarded
language.
It
is
ob
viously
sparse
and
it
is
guarded
b
y
G
=
f
n
0s
n
i
:
n

0
and


i

q
(n)g.
Note
that
an
y
unary
language
is
also
a
guarded
sparse
language
since
f
n
:
n

0g
is
sparse
and
trivially
in
P
.
The
sligh
tly
w
eak
er
result
that
w
e
pro
v
e
for
this
direction
is
as
follo
ws.
Prop
osition
..
If
SA
T
is
Karp-r
e
ducible
to
a
guar
de
d
sp
arse
language
then
SA
T

P
.
Pro
of:
Assume
that
SA
T
is
Karp-reducible
to
a
sparse
language
S
that
is
guarded
b
y
G.
Let
f
b
e
the
Karp-reduction
of
SA
T
to
S
.
W
e
will
sho
w
a
p
olynomial-time
algorithm
for
SA
T.
Input:
A
Bo
olean
form
ula
'
=
'(x

;
:::;
x
n
).
En
vision
the
binary
tree
of
all
p
ossible
assignmen
ts.
Eac
h
no
de
is
lab
elled

=




:::
i

f0;
g
i
whic
h
corresp
onds
to
an
assignmen
t
of
''s
rst
i
v
ariables.
Let
'

(x
i+
;
:::;
x
n
)
=
'(

;
:::;

i
;
x
i+
;
:::;
x
n
)
b
e
the
CNF
form
ula
corresp
onding
to
.
W
e
denote
x

=
f
('

)
(recall
that
'


SA
T
,
x


S
).
The
ro
ot
is
lab
elled
,
the
empt
y
string,
where
'

=
'.
Eac
h
no
de
lab
elled

has
t
w
o
sons,
one
lab
elled
0
and
the
other
lab
elled

(note
that
the
sons
ha
v
e
one
v
ariable
less
in
their
corresp
onding
form
ulae).
The
lea
v
es
are
lab
elled
with
n-bit
strings
corresp
onding
to
full
assignmen
ts,
and
therefore
to
a
Bo
olean
constan
t.
`
`
`
`
`
`
`
`
`
`
`
`


	


	
X
X
z
l
l
l
l
l
l
l





S
S
S
S
S





L
L
L
L
L





L
L
L
L
L


0
00
0
0

'

=
'(;
x

;
:::;
x
n
)
'

=
'
'
0
=
'(0;
x

;
:::;
x
n
)
The
tree
of
assignments.

	
LECTURE
.
NON-UNIF
ORM
POL
YNOMIAL
TIME
(P
/POL
Y)
The
strategy
w
e
will
emplo
y
to
compute
'
will
b
e
a
DFS
searc
h
on
this
tree
from
ro
ot
to
lea
v
es
using
a
branc
h
and
b
ound
tec
hnique.
W
e
bac
ktrac
k
from
a
no
de
only
if
there
is
no
satisfying
assignmen
t
in
its
en
tire
subtree.
As
so
on
as
w
e
nd
a
leaf
satisfying
',
w
e
halt
returning
the
assignmen
t.
A
t
a
no
de

w
e
consider
x

.
If
x

=

G
(implying
that
x

=

S
),
then
'

is
not
satisable.
This
implies
that
the
subtree
of

con
tains
no
satisfying
assignmen
ts
and
w
e
can
stop
the
searc
h
on
this
subtree.
If
x


G,
then
w
e
con
tin
ue
searc
hing
in
's
subtree.
A
t
a
leaf

w
e
c
hec
k
if
the
assignmen
t

is
satisable
(note
that
it
is
not
sucien
t
to
c
hec
k
that
x


G
since
f
reduces
to
S
and
not
to
G).
This
is
easy
as
w
e
merely
need
to
ev
aluate
a
Bo
olean
expression
in
giv
en
v
alues.
The
k
ey
to
the
p
olynomial
time-b
ound
of
the
algorithm
lies
in
the
sparseness
of
G.
If
w
e
visit
a
n
um
b
er
of
no
des
equal
to
the
n
um
b
er
of
strings
in
G
of
appropriate
length,
then
the
algorithm
will
clearly
b
e
p
olynomial.
Ho
w
ev
er,
for
t
w
o
dieren
t
no
des

and

,
it
ma
y
b
e
that
x

=
x


G
and
w
e
searc
h
b
oth
their
subtrees
resulting
in
visiting
to
o
man
y
no
des.
W
e
therefore
main
tain
a
set
B
suc
h
that
B

G
 S
remains
in
v
arian
t
throughout.
Up
on
bac
ktrac
king
from
a
no
de

(where
x


G),
w
e
place
x

in
B
.
W
e
then
c
hec
k
for
ev
ery
no
de
,
that
x

=

B
b
efore
searc
hing
its
subtree,
th
us
prev
en
ting
a
m
ultiple
searc
h.
Algorithm:
On
input
'
=
'(x

;
:::;
x
n
).
.
B
 
;
.
T
ree-Searc
h()
.
In
case
the
ab
o
v
e
call
w
as
not
halted,
reject
'
as
non-satisable.
In
the
follo
wing
pro
cedure,
returning
from
a
recursiv
e
call
on

indicates
that
the
subtree
ro
oted
in

con
tains
no
satisfying
assignmen
t
(or,
in
other
w
ords,
'

is
not
satisable).
In
case
w
e
reac
h
a
leaf
asso
ciated
with
a
satisfying
assignmen
t,
the
pro
cedure
halts
outputting
this
assignmen
t.
Pro
cedure
T
ree-Searc
h()
.
determine
'

(x
i+
;
:::;
x
n
)
=
'(

;
:::;

i
;
x
i+
;
:::;
x
n
)
.
if
jj
=
n
:
/*
at
a
leaf
-
'

is
a
constan
t
*/
if
'


T
then
output
the
assignmen
t

and
halt
else
return
.
if
jj
<
n
:
(a)
compute
x

=
f
('

)
(b)
if
x

=

G
/*
c
hec
k
able
in
p
oly-time,
b
ecause
G

P
*/
then
return
/*
x

=

G
)
x

=

S
)
'

=

SA
T
*/
(c)
if
x


B
then
return
(d)
T
ree-Searc
h(0)
T
ree-Searc
h()
(e)
/*
W
e
reac
h
here
only
if
b
oth
calls
in
the
previous
step
fail.
*/
if
x


G
then
add
x

to
B
(f
)
return

..
SP
ARSE
LANGUA
GES
AND
THE
P
VERSUS
N
P
QUESTION
		
End
Algorithm.
Correctness:
During
the
algorithm
B
main
tains
the
in
v
arian
t
B

G
 S
.
T
o
see
this
note
that
x

is
added
to
B
only
if
x


G
and
w
e
are
bac
ktrac
king.
Since
w
e
are
bac
ktrac
king
there
are
no
satisfying
assignmen
ts
in
's
subtree,
so
x

=

S
.
Note
that
if
x


S
then
x


G
(S

G)
and
x

=

B
(b
ecause
B
main
tains
B

G
 S
).
Therefore,
if
'
is
satisable
then
w
e
will
nd
some
satisfying
assignmen
t
since
for
all
no
des

on
the
path
from
the
ro
ot
to
the
appropriate
leaf,
x


S
,
and
its
sons
are
dev
elop
ed.
Complexit
y:
T
o
sho
w
that
the
time
complexit
y
is
p
olynomial
it
is
sucien
t
to
sho
w
that
only
a
p
olynomial
p
ortion
of
the
tree
is
\dev
elop
ed".
The
follo
wing
claim
will
yield
the
desired
result.
Claim
..
L
et

and

b
e
two
no
des
in
the
tr
e
e
such
that
()
neither
is
a
pr
ex/anc
estor
of
the
other
and
()
x

=
x

.
Then
it
is
not
p
ossible
that
the
sons
of
b
oth
no
des
wer
e
develop
e
d
(in
Step
d).
Pro
of:
Assume
w
e
arriv
ed
at

rst.
Since

is
not
an
ancestor
of

w
e
arriv
e
at

after
bac
ktrac
king
from
.
If
x

=

G
then
x

=

G
since
x

=
x

and
w
e
will
not
dev
elop
either.
Otherwise,
it
m
ust
b
e
that
x


B
after
bac
ktrac
king
from
.
Therefore
x


B
and
its
sons
will
not
b
e
dev
elop
ed
(see
Step
c).
Corollary
..
Only
a
p
olynomial
p
ortion
of
the
tr
e
e
is
\develop
e
d".
Pro
of:
There
exists
a
p
olynomial
q
(:)
that
time-b
ounds
the
Karp-reduction
f
.
Since
ev
ery
x

is
obtained
b
y
an
application
of
f
,
x


[
iq
(n)
f0;
g
i
.
Y
et
G
is
sparse
so
jG
\
([
iq
(n)
f0;
g
i
)j

p(n)
for
some
p
olynomial
p().
Consider
a
certain
lev
el
of
the
tree.
Ev
ery
t
w
o
no
des

and

on
this
lev
el
are
not
ancestors
of
eac
h
other.
Moreo
v
er
on
this
lev
el
of
the
tree
there
are
at
most
p(n)
dieren
t
's
suc
h
that
x


G.
Therefore
b
y
the
previous
claim
the
n
um
b
er
of
x

's
dev
elop
ed
forw
ard
on
this
lev
el
is
b
ounded
b
y
p(n).
Therefore
the
o
v
erall
n
um
b
er
of
no
des
dev
elop
ed
is
b
ounded
b
y
n

p(n).
SA
T

P
and
the
pro
of
is
complete.
Bibliographic
Notes
The
class
P/p
oly
w
as
dened
b
y
Karp
and
Lipton
as
part
of
a
general
form
ulation
of
\ma-
c
hines
whic
h
tak
e
advise"
[].
They
ha
v
e
noted
the
equiv
alence
to
the
traditional
form
ulation
of
p
olynomial-size
circuits,
the
eect
of
uniformit
y
,
as
w
ell
as
the
relation
to
Co
ok-reducibilit
y
to
sparse
sets
(i.e.,
Theorem
.).
Theorem
.
is
atrriburted
to
Adleman
[],
who
actually
pro
v
ed
RP

P
=p
oly
using
a
more
in
v
olv
ed
argumen
t.
Theorem
.	
is
due
to
F
ortune
[].
.
L.
Adleman,
\Tw
o
theorems
on
random
p
olynomial-time",
in
	th
F
OCS,
pages
{,
	.
.
S.
F
ortune,
\A
Note
on
Sparse
Complete
Sets",
SIAM
J.
on
Computing,
V
ol.
,
pages
{,
		.
.
R.M.
Karp
and
R.J.
Lipton.
\Some
connections
b
et
w
een
non
uniform
and
uniform
complexit
y
classes",
in
th
STOC,
pages
0-0	,
	0.

00
LECTURE
.
NON-UNIF
ORM
POL
YNOMIAL
TIME
(P
/POL
Y)

Lecture
	
The
P
olynomial
Hierarc
h
y
(PH)
Notes
tak
en
b
y
Ronen
Mizrahi
Summary:
In
this
lecture
w
e
dene
a
hierarc
h
y
of
complexit
y
classes
starting
from
N
P
and
y
et
con
tained
in
PSP
A
CE.
This
is
done
in
t
w
o
w
a
ys,
the
rst
b
y
generalizing
the
notion
of
Co
ok
reductions,
and
the
second
b
y
generalizing
the
denition
of
N
P
.
W
e
sho
w
that
the
t
w
o
are
equiv
alen
t.
W
e
then
try
to
mak
e
some
observ
ations
regarding
the
hierarc
h
y
,
our
main
concern
will
b
e
to
learn
when
do
es
this
hierarc
h
y
collapse,
and
ho
w
can
w
e
relate
it
to
complexit
y
classes
that
w
e
kno
w
already
suc
h
as
BPP
and
P/P
oly
.
	.
The
Denition
of
the
class
PH
In
the
literature
y
ou
ma
y
nd
three
common
w
a
ys
to
dene
this
class,
t
w
o
of
those
w
a
ys
will
b
e
presen
ted
here.
(The
third,
via
\alternating"
mac
hines
is
omitted
here.)
	..
First
denition
for
PH:
via
oracle
mac
hines
In
tuition
Recall
the
den
tion
of
a
Co
ok
reduction,
the
reduction
is
done
using
a
p
olynomial
time
mac
hine
that
has
access
to
some
oracle.
Requiring
that
the
oracle
will
b
elong
to
a
giv
en
complexit
y
class
C
,
will
raise
the
question:
What
is
the
complexit
y
class
of
all
those
languages
that
are
Co
ok
reducable
to
some
language
from
C
?
F
or
example:
Let
us
set
the
complexit
y
class
of
the
oracle
to
b
e
N
P
,
then
for
Karp
reduction
w
e
kno
w
that
ev
ery
language
L,
that
is
Karp
reducable
to
some
language
in
N
P
(sa
y
S
AT
),
will
also
b
e
in
N
P
.
Ho
w
ev
er
it
is
not
clear
what
complexit
y
class
will
a
Co
ok
reduction
(to
N
P
)
yield.
P
erliminary
denitions
Denition
	.
(the
language
L(M
A
)):
The
language
L(M
A
)
is
the
set
of
inputs
ac
c
epte
d
by
machine
M
given
ac
c
ess
to
or
acle
A.
0

0
LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)
Notations:

M
A
:
The
orcale
mac
hine
M
with
access
to
oracle
A.

M
A
(x)
:
The
output
of
the
orcale
mac
hine
M
A
on
input
x.
W
e
note
the
follo
wing
in
teresting
cases
for
the
ab
o
v
e
denition:
.
M
is
a
deterministic
p
olynomial
time
mac
hine.
Then
M
is
a
Co
ok
reduction
of
L(M
A
)
to
A.
.
M
is
a
probabilistic
p
olynomial
time
mac
hine.
Then
M
is
a
randomized
Co
ok
reduction
of
L(M
A
)
to
A.
.
M
is
a
non-deteministic
p
olynomial
time
mac
hine
(note
that
the
non
determinism
is
related
only
to
M
,
A
is
an
oracle
and
as
suc
h
it
alw
a
ys
giv
es
the
righ
t
answ
er).
When
w
e
dene
the
p
olynomial
hierarc
h
y
w
e
will
use
this
case.
Observ
e
that
giv
en
one
of
the
ab
o
v
e
cases,
kno
wing
the
complexit
y
class
of
the
oracle,
will
dene
another
complexit
y
class
whic
h
is
the
set
of
languages
L(M
A
),
where
A
is
an
oracle
from
the
giv
en
comlexit
y
class.
The
resulting
complexit
y
class
ma
y
b
e
one
that
is
kno
wn
to
us
(suc
h
as
P
or
N
P
),
or
a
new
class.
Denition
	.
(the
class
M
C
):
L
et
M
b
e
an
or
acle
machine.
Then
M
C
is
the
set
of
languages
obtaine
d
fr
om
the
machine
M
given
ac
c
ess
to
an
or
acle
fr
om
the
class
of
languages
C
.
That
is,
M
C
def
=
fL(M
A
)
:
A

C
g
F
or
example:

M
N
P
=
fL(M
A
)
:
A

N
P
g
Note:
w
e
do
not
gain
m
uc
h
b
y
using
N
P
,
rather
than
an
y
N
P
-complete
language
(suc
h
as
S
AT
).
That
is,
w
e
kno
w
that
an
y
language,
A,
in
N
P
is
Karp
reducable
to
S
AT
,
b
y
using
this
reduction
w
e
can
alter
M
,
and
obtain
a
new
mac
hine
~
M
,
suc
h
that
L(M
A
)
=
L(
~
M
S
AT
).
In
the
follo
wing
denition
w
e
abuse
notation
a
little.
W
e
write
C

C

but
refer
to
mac
hines
natually
asso
ciated
with
the
class
C

,
and
to
their
natural
extension
to
orale
mac
hines.
W
e
note
that
not
ev
ery
class
has
a
natural
en
umeration
of
mac
hines
asso
ciated
with
it,
let
allo
w
a
natural
extension
of
suc
h
mac
hines
to
oracle
mac
hines.
Ho
w
ev
er,
suc
h
asso
ciations
and
extensions
do
hold
for
the
main
classes
w
e
are
in
terested
in
suc
h
as
P
,
N
P
and
B
P
P
.
Denition
	.
(the
class
C

C

{
a
fuzzy
framew
ork):
Assume
that
C

and
C

ar
e
classes
of
languages,
and
also
that
for
e
ach
language
L
in
C

,
ther
e
exists
a
machine
M
L
,
such
that
L
=
L(M
L
).
F
urthermor
e,
c
onsider
the
extension
of
M
L
into
an
or
acle
machine
M
so
that
given
ac
c
ess
to
the
empty
or
acle
M
b
ehaves
as
M
L
(i.e.,
L(M
L
)
=
L(M
;
)).
Then
C

C

is
the
set
of
languages
obtaine
d
fr
om
such
machines
M
L
,
wher
e
L

C

,
given
ac
c
ess
to
an
or
acle
for
a
language
fr
om
the
class
of
languages
C

.
That
is,
C

C

=
fL(M
A
)
:
L(M
;
)

C

&
A

C

g

	..
THE
DEFINITION
OF
THE
CLASS
PH
0
The
ab
o
v
e
framew
ork
can
b
e
prop
erly
instan
tiated
in
some
imp
ortan
t
cases.
F
or
example:

P
C
=
fL(M
A
)
:
M
is
deterministic
p
olynomial-time
oracle
mac
hine
&
A

C
g

N
P
C
=
fL(M
A
)
:
same
as
ab
o
v
e
but
M
is
non-deterministic
g

B
P
P
C
=
fL(M
A
)
:
same
as
ab
o
v
e
but
M
is
probabilistic
g
Here
w
e
mean
that
with
probabilit
y
at
least
=,
mac
hine
M
on
input
x
and
oracle
aceess
to
A

C
correctly
decides
whether
x

L(M
A
).
Bac
k
to
the
motiv
ating
question:
Observ
e
that
sa
ying
that
L
is
Co
ok-reducible
to
S
AT
(i.e.,
L
/
C
S
AT
)
is
equiv
alen
t
to
writing
L

P
N
P
.
W
e
ma
y
no
w
re-address
the
question
regarding
the
p
o
w
er
of
Co
ok
reductions.
Observ
e
that
N
P
[
co
N
P

P
N
P
,
this
is
b
ecause:

N
P

P
N
P
holds,
b
ecause
for
L

N
P
w
e
can
tak
e
the
oracle
A
to
b
e
an
oracle
for
the
language
L,
and
the
mac
hine
M

P
to
b
e
a
trivial
mac
hine
that
tak
es
its
input
asks
the
oracle
ab
out
it,
and
outputs
the
oracle's
answ
er.

co
N
P

P
N
P
holds,
b
ecause
w
e
can
tak
e
the
same
oracle
as
ab
o
v
e,
and
a
dieren
t
(y
et
still
trivial)
mac
hine
M

P
that
asks
the
oracle
ab
out
its
input,
and
outputs
the
b
o
olean
complemen
t
of
the
oracle's
answ
er.
W
e
conclude
that
under
the
assumption
that
N
P
=
co
N
P
,
Co
ok-reductions
to
N
P
giv
e
us
more
p
o
w
er
than
Karp-reductions
to
the
same
class.
Oded's
Note:
W
e
sa
w
suc
h
a
result
already
,
but
it
w
as
quite
articial.
I
refer
to
that
fact
that
P
is
Co
ok-reducible
to
the
class
of
trivial
langugaes
(i.e.,
the
class
f;;
f0;
g

g),
whereas
non-trivial
languages
can
not
b
e
Karp-reduced
to
trivial
ones.
Actual
denition
Denition
	.
(the
class

i
):

i
is
a
se
quenc
e
of
sets
and
wil
l
b
e
dene
d
inductively:



def
=
N
P


i+
def
=
N
P

i
Notations:


i
def
=
co
i


i+
def
=
P

i
Denition
	.
(The
hierarc
h
y
{
PH):
PH
def
=
[

i=

i
The
arbitrary
c
hoice
to
use
the

i
's
(rather
than
the

i
's
or

i
's)
is
justied
b
y
the
follo
wing
observ
ations.

0
LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)
Almost
syn
taxtic
observ
ations
Prop
osition
	..

i
[
i


i+


i+
\

i+
.
Pro
of:
W
e
pro
v
e
eac
h
of
the
t
w
o
con
tainmen
ts:
.

i
[
i


i+
=
P

i
.
The
reason
for
that
is
the
same
as
for
N
P
[co
N
P

P
N
P
=


(see
ab
o
v
e)
.
P

i


i+
\

i+
.
P

i

N
P

i
=

i+
is
ob
vious.
Since
P

i
is
closed
under
complemen
tation,
L

P

i
implies
that
L

P

i


i+
and
so
L


i+
.
Prop
osition
	..
P

i
=
P

i
and
N
P

i
=
N
P

i
.
Pro
of:
Giv
en
a
mac
hine
M
and
an
oracle
A,
it
is
easy
to
mo
dify
M
to
~
M
suc
h
that:
L(M
A
)
=
L(
~
M
A
).
The
w
a
y
w
e
build
~
M
is
b
y
taking
M
and
ipping
ev
ery
answ
er
obtained
from
the
oracle.
In
particular,
if
M
is
deterministic
(resp.
non-deterministic)
p
olynomail-time
then
so
is
~
M
.
Th
us,
for
suc
h
M
and
an
y
class
C
the
classes
M
coC
and
~
M
C
are
iden
tical.
	..
Second
denition
for
PH:
via
quan
tiers
In
tuition
The
approac
h
tak
en
here
is
to
recall
one
of
the
denitions
of
N
P
and
try
to
generalize
it.
Denition
	.
(p
olynomially-b
ounded
relation):
a
k
-ary
r
elation
R
is
c
al
le
d
p
olynomially
b
ounded
if
ther
e
exists
a
p
olynomial
p(:)
such
that:
(x

;
:
:
:
;
x
k
)
;
[(x

;
:
:
:
;
x
k
)

R
=
)
(i)
jx
i
j

p(jx

j)]
Note:
our
denition
requires
that
all
the
elemen
ts
of
the
relation
are
not
to
o
long
with
regard
to
the
rst
elemen
t,
but
the
rst
elemen
t
ma
y
b
e
v
ery
long.
W
e
could
ev
en
require
a
stronger
condition:
ij
jx
i
j

p(jx
j
j),
this
will
promise
that
ev
ery
elemen
t
of
the
relation
is
not
to
o
long
with
regard
to
ev
ery
one
of
the
others.
W
e
do
not
mak
e
this
requiremen
t
b
ecause
the
ab
o
v
e
denition
will
turn
out
to
b
e
satisfactory
for
our
needs,
this
is
b
ecause
in
our
relations
the
rst
elemen
t
is
the
input
w
ord,
and
w
e
need
the
rest
of
the
elemen
ts
in
the
relation
to
b
e
b
ounded
in
the
length
of
the
input.
Also
the
complexit
y
classes,
that
w
e
will
dene
using
the
notion
of
a
p
olynomially
b
ounded
k
-ary
relation,
will
turn
out
the
same
for
b
oth
the
w
eak
and
the
strong
denition
of
the
relation.
W
e
no
w
state
again
the
denition
of
the
complexit
y
class
N
P
:
Denition
	.
(N
P
):
L
N
P
if
ther
e
exists
a
p
olynomial
ly
b
ounde
d
and
p
olynomial
time
r
e
c
o
g-
nizable
binary
r
elation
R
L
such
that:
x

L
i
	y
s.t.
(x;
y
)

R
L
The
w
a
y
to
generalize
this
denition
will
b
e
to
use
a
k
-ary
relation
instead
of
just
a
binary
one.

	..
THE
DEFINITION
OF
THE
CLASS
PH
0
Actual
denition
What
w
e
redene
is
the
sequence
of
sets

i
suc
h
that


will
remain
N
P
.
The
denition
for
PH
remains
the
union
of
all
the

i
's.
Denition
	.
(
i
):
L

i
if
ther
e
exists
a
p
olynomial
ly
b
ounde
d
and
p
olynomial
time
r
e
c
o
gnizable
(i+)-ary
r
elation
R
L
such
that:
x

L
i
	y

y

	y

:
:
:
Q
i
y
i
;
s.t.
(x;
y

;
:
:
:
;
y
i
)

R
L

Q
i
=

if
i
is
even

Q
i
=
	
otherwise
	..
Equiv
alence
of
denitions
W
e
ha
v
e
done
something
that
migh
t
seem
a
mistak
en;
that
is,
w
e
ha
v
e
giv
en
the
same
name
for
an
ob
ject
dened
b
y
t
w
o
dieren
t
denitions.
Ho
w
ev
er,
w
e
no
w
in
tend
to
pro
v
e
that
the
classes
pro
duced
b
y
the
t
w
o
denitions
are
equal.
A
more
con
v
en
tional
w
a
y
to
presen
t
those
t
w
o
denitions
is
to
state
one
of
them
as
the
denition
for
PH,
and
then
pro
v
e
an
"if
and
only
if"
theorem
that
c
haracterizes
PH
according
to
the
other
denition.
Theorem
	.	
:
The
ab
ove
two
denitions
of
P
H
ar
e
e
quivalent.
F
urthermor
e,
for
every
i,
the
class

i
as
in
Denition
	.
is
identic
al
to
the
one
in
Denition
	..
Pro
of:
W
e
will
sho
w
that
for
ev
ery
i,
the
class

i
b
y
the
t
w
o
denitions
is
equal.
In
order
to
distinguish
b
et
w
een
the
clases
pro
duced
b
y
the
t
w
o
denitions
w
e
will
in
tro
duce
the
follo
wing
notation:



i
is
the
set

i
pro
duced
b
y
the
rst
denition.



i
is
the
set

i
pro
duced
b
y
the
second
denition.



i
is
the
set

i
pro
duced
b
y
the
rst
denition.



i
is
the
set

i
pro
duced
b
y
the
second
denition.
P
a
rt
:
W
e
pro
v
e
b
y
induction
on
i
that
i
;


i



i
:

Base
of
induction:


w
as
dened
to
b
e
N
P
in
b
oth
cases
so
there
is
nothing
to
pro
v
e.

W
e
assume
that
the
claim
holds
for
i
and
pro
v
e
for
i
+
:
supp
ose
L



i+
then
b
y
denition
it
follo
ws
that
there
exists
a
relation
R
L
suc
h
that:
x

L
i
	y

y

	y

:
:
:
Q
i
y
i
Q
i+
y
i+
;
s.t.
(x;
y

;
:
:
:
;
y
i
;
y
i+
)

R
L
In
other
w
ords
this
means
that:
x

L
i
	y

;
s.t.
(x;
y

)

L
i
where
L
i
is
dened
as
follo
ws:
L
i
def
=
f(x
0
;
y
0
)
:
y

	y

:
:
:
Q
i
y
i
Q
i+
y
i+
;
s.t.
(x
0
;
y
0
;
:
:
:
;
y
i
;
y
i+
)

R
L
g

0
LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)
W
e
claim
that
L
i



i
,
this
is
b
y
complemen
ting
the
denition
of


i
.
If
w
e
do
this
comple-
men
tation
for
L



i
w
e
get:
x

L
i
	y

y

:
:
:
Q
i
y
i
;
s.t.
(x;
y

;
:
:
:
;
y
i
)

R
L
x

L
i
y

	y

:
:
:
Q
i
y
i
;
s.t.
(x;
y

;
:
:
:
;
y
i
)
=

R
L
This
is
almost
what
w
e
had
in
the
denition
of
L
i
except
for
the
\
=

R
L
"
as
opp
osed
to
\
R
L
".
Remem
b
er
that
deciding
mem
b
ership
in
R
L
is
p
olynomial
time
recognizable,
and
therefore
its
complemen
t
is
also
so.
No
w
that
w
e
ha
v
e
that
L
i



i
,
w
e
can
use
the
inductiv
e
h
yp
othesis


i



i
.
So
far
w
e
ha
v
e
managed
to
sho
w
that:
x

L
i
	y

;
s.t.
(x;
y

)

L
i
Where
L
i
b
elongs
to


i
.
W
e
no
w
claim
that
L

N
P


i
,
this
is
true
b
ecause
w
e
can
write
a
non-deterministic,
p
olynomial-time
mac
hine,
that
decides
mem
b
ership
in
L,
b
y
guessing
y

,
and
using
an
oracle
for
L
i
.
Therefore
w
e
can
further
conclude
that:
L

N
P


i

N
P


i



i+
:
P
a
rt
:
W
e
pro
v
e
b
y
induction
on
i
that
i
;


i



i
:

Base
of
induction:
as
b
efore.

Induction
step:
supp
ose
L



i+
then
there
exists
a
non-deterministic
p
olynomial
time
mac
hine
M
suc
h
that
L

L(M


i
)
whic
h
means
that:
	L
0



i
;
s.t.
;
L
=
L(M
L
0
)
F
rom
the
denition
of
M
L
0
it
follo
ws
that:
x

L
i
	y
;
q

;
a

;
:
:
:
;
q
t
;
a
t
s.t.
:
.
Mac
hine
M
,
with
non-determinstic
c
hoices
y
,
in
teracts
with
its
oracle
in
the
follo
wing
w
a
y:
{

st
query
=
q

and

st
answ
er
=
a

.
.
.
{
t
th
query
=
q
t
and
t
th
answ
er
=
a
t
.
for
ev
ery


j

t:
{
(a
j
=
)
=
)
q
j

L
0
{
(a
j
=
0)
=
)
q
j
=

L
0
where
y
is
a
description
of
the
non-determinstic
c
hoices
of
M
.
Let
us
view
the
ab
o
v
e
according
to
the
second
denition,
that
is,
according
to
the
denition
with
quan
tiers,
then
the
rst
item
is
a
p
olynomial
time
predicate
and
therefore
this
p
oten-
tially
puts
L
in
N
P
.
The
second
item
in
v
olv
es
L
0
.
Recall
that
L
0



i
and
that
b
y
the
inductiv
e
h
yp
othesis


i



i
,
and
therefore
w
e
can
view
mem
b
ership
in
L
0
according
to
the
second
denition,
and
em
b
ed
this
result
within
what
w
e
ha
v
e
ab
o
v
e.
This
will
yield
that
for
ev
ery


j

t:

	..
EASY
COMPUT
A
TIONAL
OBSER
V
A
TIONS
0
{
(a
j
=
)
=
)
	y
(j;)

y
(j;)

:
:
:
Q
i
y
(j;)
i
;
s.t.
(q
j
;
y
(j;)

;
:
:
:
;
y
(j;)
i
)

R
L
0
{
(a
j
=
0)
=
)
y
(j;)

	y
(j;)

:
:
:
Q
i
y
(j;)
i
;
s.t.
(q
j
;
y
(j;)

;
:
:
:
;
y
(j;)
i
)

R
L
0
Let
us
dene:
{
w

is
the
concatenation
of:
y
;
q

;
a

;
:
:
:
;
q
t
;
a
t
,
and
y
(j;)

for
all
j
s.t.
(a
j
=
).
{
w

is
the
concatenation
of:
y
(j;)

for
all
j
s.t.
(a
j
=
0),
and
y
(j;)

for
all
j
s.t.
(a
j
=
).
.
.
.
{
w
i
is
the
concatenation
of:
y
(j;)
i 
for
all
j
s.t.
(a
j
=
0),
and
y
(j;)
i
for
all
j
s.t.
(a
j
=
).
{
w
i+
is
the
concatenation
of:
y
(j;)
i
for
all
j
s.t.
(a
j
=
0).
R
L
will
b
e
the
(i
+
)-ary
relation
dened
in
the
follo
wing
w
a
y:
(w

;
:
:
:
;
w
i+
)

R
L
i
for
ev
ery


j

t:
{
(a
j
=
)
=
)
(q
j
;
y
(j;)

;
:
:
:
;
y
(j;)
i
)

R
L
0
{
(a
j
=
0)
=
)
(q
j
;
y
(j;)

;
:
:
:
;
y
(j;)
i
)

R
L
0
where
the
w
i
's
are
parsed
analogously
to
the
ab
o
v
e.
Since
R
L
0
and
R
L
0
where
p
olynomially
b
ounded,
and
p
olynomial
time
recognizable,
so
is
R
L
.
Altogether
w
e
ha
v
e:
x

L
i
	w

;
w

;
:
:
:
Q
i+
w
i+
;
s.t.
(w

;
:
:
:
;
w
i+
)

R
L
It
no
w
follo
ws
from
the
denition
of


i+
that
L



i+
as
needed.
	.
Easy
Computational
Observ
ations
Prop
osition
	..
P
H

P
S
P
AC
E
Pro
of:
W
e
will
sho
w
that

i

P
S
P
AC
E
for
all
i.
Let
L


i
,
then
w
e
kno
w
b
y
the
denition
with
quan
tiers
that:
x

L
i
	y

y

	y

:
:
:
Q
i
y
i
;
s.t.
(x;
y

;
:
:
:
;
y
i
)

R
L
Giv
en
x
w
e
can
use
i
v
ariables
to
try
all
the
p
ossibilities
for
y

;
:
:
:
;
y
i
and
mak
e
sure
that
they
meet
the
ab
o
v
e
requiremen
t.
Since
the
relation
R
L
is
p
olynomially
b
ounded,
w
e
ha
v
e
a
p
olynomial
b
ound
on
the
length
of
eac
h
of
the
y
i
's
that
w
e
are
c
hec
king.
Th
us
w
e
ha
v
e
constructed
a
deterministic
mac
hine
that
decides
L.
This
mac
hine
uses
i
v
ariables,
the
length
of
eac
h
of
them
is
p
olynomially
b
ounded
in
the
length
of
the
input.
Since
i
is
a
constan
t,
the
o
v
erall
space
used
b
y
this
mac
hine
is
p
olynomial.

0
LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)
Prop
osition
	..
N
P
=
co
N
P
implies
P
H

N
P
(which
implies
P
H
=
N
P
).
In
tuitiv
ely
the
extra
p
o
w
er
that
non-deterministic
Co
ok
reductions
ha
v
e
o
v
er
non-deterministic
Karp
reductions,
comes
from
giving
us
the
abilit
y
to
complemen
t
the
oracle's
answ
ers
for
free.
What
w
e
claim
here
is
that
if
this
p
o
w
er
is
meaningless
then
the
whole
hierarc
h
y
collapses.
Pro
of:
W
e
will
sho
w
b
y
induction
on
i
that
i
;

i
=
N
P
:
.
i
=
:
b
y
denition


=
N
P
.
.
Induction
step:
b
y
the
inductiv
e
h
yp
othesis
it
follo
ws
that

i
=
N
P
so
what
remains
to
b
e
sho
wn
is
that
N
P
N
P
=
N
P
.
Con
tainmen
t
in
one
direction
is
ob
vious
so
w
e
fo
cus
on
pro
ving
that
N
P
N
P

N
P
.
Let
L

N
P
N
P
then
there
exist
a
non-deterministic,
p
olynomial-time
mac
hine
M
,
and
an
oracle
A

N
P
,
suc
h
that
L
=
L(M
A
).
Since
N
P
=
co
N
P
it
follo
ws
that
A

N
P
to
o.
Therefore,
there
exist
relations
R
A
and
R
A
(N
P
relations
for
A
and
A
resp
ectiv
ely)
suc
h
that:

q

A
i
	w
;
s.t.
(q
;
w
)

R
A
.

q

A
i
	w
;
s.t.
(q
;
w
)

R
A
Using
these
relations,
and
the
denition
of
N
P
N
P
w
e
get:
x

L
i
	y
;
q

;
a

;
:
:
:
;
q
t
;
a
t
;
suc
h
that,
for
all


j

t:

a
j
=

(
)
q
j

A
(
)
	w
j
;
(q
j
;
w
j
)

R
A

a
j
=
0
(
)
q
j

A
(
)
	w
j
;
(q
j
;
w
j
)

R
A
.
Dene:

w
is
the
concatenation
of:
y
;
q

;
a

;
:
:
:
;
q
t
;
a
t
;
w

;
:
:
:
;
w
t

R
L
is
a
binary
relation
suc
h
that:
(x;
w
)

R
L
i
for
all


j

t:
{
a
j
=

=
)
(q
j
;
w
j
)

R
A
{
a
j
=
0
=
)
(q
j
;
w
j
)

R
A
.
Since
M
is
a
p
olynomial-time
mac
hine,
t
is
p
olynomial
in
the
length
of
x.
Com
bining
this
fact
with
the
fact
that
b
oth
R
A
and
R
A
are
p
olynomial-time
recognizable,
and
p
olynomially
b
ounded,
w
e
conclude
that
so
is
R
L
.
All
together
w
e
get
that
there
exists
an
N
P
relation
R
L
suc
h
that
:
x

L
i
	w
;
s.t.
(x;
w
)

R
L
Th
us,
L

N
P
.
Generalizing
Prop
osition
	..,
w
e
ha
v
e
Prop
osition
	..
F
or
every
k

,
if

k
=

k
then
P
H
=

k
.
A
pro
of
is
presen
ted
in
the
app
endix
to
this
lecture.

	..
BPP
IS
CONT
AINED
IN
PH
0	
	.
BPP
is
con
tained
in
PH
Not
kno
wing
whether
B
P
P
is
con
tained
in
N
P
,
it
is
of
some
confort
to
kno
w
that
it
is
con
tained
in
the
P
olynomial-Hierarc
h
y
(whic
h
extends
N
P
).
Theorem
	.0
(Sipser
and
Lautemann):
B
P
P



.
Pro
of:
Let
L

B
P
P
then
there
exists
a
probabilistic
p
olynomial
time
mac
hine
A(x;
r
)
where
x
is
the
input
and
r
is
the
random
guess.
By
the
denition
of
BPP
,
with
some
amplication
w
e
get,
for
some
p
olynomial
p(:):
x

f0;
g
n
;
s.t.
Pr
r

R
f0;g
p(n)
[A(x;
r
)
=
(x)]
<

p(n)
where
(x)
=

if
x

L
and
(x)
=
0
otherwise.
Oded's
Note:
A
w
ord
ab
out
the
ab
o
v
e
is
in
place.
Note
that
w
e
do
not
assert
that
the
error
decreases
as
a
fast
xed
function
of
n,
where
the
function
is
xed
b
efore
w
e
determine
the
randomness
complexit
y
of
the
new
algorithm.
W
e
sa
w
result
of
that
kind
in
Lecture
;
but
here
w
e
claim
something
dieren
t.
That
is,
that
the
error
probabilit
y
ma
y
dep
end
on
the
randomness
complexit
y
of
the
new
algorithm.
Still,
the
dep
endency
required
here
is
easy
to
ac
hiev
e.
Sp
ecically
,
supp
ose
that
the
original
algorithm
uses
m
=
p
oly
(n)
coins.
Then
b
y
running
it
t
times
and
ruling
b
y
ma
jorit
y
w
e
decrease
the
error
probabilit
y
to
exp( 
(t)).
The
randomness
complexit
y
of
the
new
algorithm
is
tm.
So
w
e
need
to
set
t
suc
h
that
exp( 
(t))
<
=mt,
whic
h
can
b
e
satised
with
t
=
O
(log
m)
=
O
(log
n).
The
k
ey
observ
ation
is
captured
b
y
the
follo
wing
claim
Claim
	..
Denote
m
=
p(n)
then,
for
every
x

L
\
f0;
g
n
,
ther
e
exist
s

;
:
:
:
;
s
m

f0;
g
m
such
that
r

f0;
g
m
;
m
_
i=
A(x;
r

s
i
)
=

(	.)
Actually
,
the
same
sequence
of
s
i
's
ma
y
b
e
used
for
all
x

L
\
f0;
g
n
(pro
vided
that
m

n
whic
h
holds
without
loss
of
generalit
y).
Ho
w
ev
er,
w
e
don't
need
this
extra
prop
ert
y
.
Pro
of:
W
e
will
sho
w
existence
of
suc
h
s
i
's
b
y
the
Probabilistic
Metho
d:
That
is,
instead
of
sho
wing
that
an
ob
ject
with
some
prop
ert
y
exists
w
e
will
sho
w
that
a
random
ob
ject
has
the
prop
ert
y
with
p
ositiv
e
probabilit
y
.
Actually
,
w
e
will
upp
er
b
ound
the
probabilit
y
that
a
random
ob
ject
do
es
not
ha
v
e
the
desired
prop
ert
y
.
In
our
case
w
e
lo
ok
for
existence
of
s
i
's
satisfying
Eq.
(	.),
and
so
w
e
will
upp
er
b
ound
the
probabilit
y
,
denoted
P
,
that
randomly
c
hosen
s
i
's
do
not
satisfy
Eq.
(	.):
P
def
=
Pr
s

;:::
;s
m

R
f0;g
m
[:r

f0;
g
m
;
m
_
i=
(A(x;
r

s
i
)
=
)]
=
Pr
s

;:::
;s
m

R
f0;g
m
[	r

f0;
g
m
;
m
^
i=
(A(x;
r

s
i
)
=
0)]

X
r
f0;g
m
Pr
s

;:::
;s
m

R
f0;g
m
[
m
^
i=
(A(x;
r

s
i
)
=
0)]

0
LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)
where
the
inequalit
y
is
due
to
the
union
b
ound.
Using
the
fact
that
the
ev
en
ts
of
c
ho
osing
s
i
's
uniformly
are
indep
enden
t,
w
e
get
that
the
probabilit
y
of
all
the
ev
en
ts
happ
ening
at
once
equals
to
the
m
ultiplication
of
the
probabilities.
Therefore:
P

X
r
f0;g
m
m
Y
i=
Pr
s
i

R
f0;g
m
[A(x;
r

s
i
)
=
0]
Since
in
the
ab
o
v
e
probabilit
y
r
is
xed,
and
the
s
i
's
are
uniformly
distributed
then
(b
y
a
prop
ert
y
of
the

op
erator),
the
s
i

r
's
are
also
uniformly
distributed.
Recall
that
w
e
consider
an
arbitrary
xed
x

L
\
f0;
g
n
.
Th
us,
P


m

Pr
s
R
f0;g
m
[A(x;
s)
=
0]
m


m



m

m


The
claim
holds.
Claim
	..
F
or
any
x

f0;
g
n
n
L
and
for
al
l
s

;
:
:
:
;
s
m

f0;
g
m
,
ther
e
exists
r

f0;
g
m
so
that
W
m
i=
A(x;
r

s
i
)
=
0.
Pro
of:
W
e
will
actually
sho
w
that
for
all
s

;
:
:
:
;
s
m
there
are
man
y
suc
h
r
's.
Let
s

;
:
:
:
;
s
m

f0;
g
m
b
e
arbitrary
.
Pr
r
f0;g
m
[
m
_
i=
A(x;
r

s
i
)
=
0]
=

 Pr
r
f0;g
m
[
m
_
i=
A(x;
r

s
i
)
=
]
Ho
w
ev
er,
since
x
=

L
and
Pr
r
f0;g
m
[A(x;
r
)
=
]
<
=m,
w
e
get
Pr
r
f0;g
m
[
m
_
i=
A(x;
r

s
i
)
=
]

m
X
i=
Pr
r
f0;g
m
[A(x;
r

s
i
)
=
]

m


m
=


and
so,
Pr
r
f0;g
m
[
m
_
i=
A(x;
r

s
i
)
=
0]



Therefore
there
exist
(man
y)
suc
h
r
's
and
the
claim
holds.
Com
bining
the
results
of
the
t
w
o
claims
together
w
e
get:
x

L
i
	s

;
:
:
:
s
m

f0;
g
m
;
r
m
_
i=
A(x;
r

s
i
)
=

This
assertion
corresp
onds
to
the
denition
of


,
and
therefore
L



as
needed.
Commen
t:
The
reason
w
e
used
the

op
erator
is
b
ecause
it
has
the
prop
ert
y
that
giv
en
an
arbitrary
xed
r
,
if
s
is
uniformly
distributed
then
r

s
is
also
uniformly
distributed.
Same
for
xed
s
and
random
r
.
An
y
other
ecien
t
binary
op
eration
with
this
prop
ert
y
ma
y
b
e
used
as
w
ell.

	..
IF
NP
HAS
SMALL
CIR
CUITS
THEN
PH
COLLP
ASES

	.
If
NP
has
small
circuits
then
PH
collpases
The
follo
wing
result
sho
ws
that
an
unlik
ely
ev
en
t
regarding
non-uniform
complexit
y
(i.e.,
the
class
P
=p
oly
)
implies
an
unlik
ely
ev
en
t
regarding
uniform
complexit
y
(i.e.,
PH).
Theorem
	.
(Karp
&
Lipton):
If
N
P

P
=p
oly
then


=


,
and
so
P
H
=


.
Pro
of:
W
e
will
only
pro
v
e
the
rst
implication
in
the
theorem.
The
second
follo
ws
b
y
Prop
osition
	...
Sho
wing
that


is
closed
under
complemen
tation,
giv
es
us
that


=


.
So
what
w
e
will
actually
pro
v
e
is
that





.
Let
L
b
e
an
arbitrary
language
in


,
then
there
exists
a
trinary
p
olynomially
b
ounded,
and
p
olynomial
time
recognizable
relation
R
L
suc
h
that:
x

L
i
y
	z
s.t.
;
(x;
y
;
z
)

R
L
Let
us
dene:
L
0
def
=
f(x
0
;
y
0
)
:
	z
;
s.t.
(x
0
;
y
0
;
z
)

R
L
g
Then
w
e
get
that:

x

L
i
y
;
(x;
y
)

L
0

L
0

N
P
Consider
a
Karp
reduction
of
L
0
to
SA
T,
call
it
f
:
x

L
i
y
;
f
(x;
y
)

S
AT
Let
us
no
w
use
the
assumption
that
N
P

P
=P
ol
y
for
SA
T,
then
it
follo
ws
that
SA
T
has
small
circuits
fC
m
g
m
,
where
m
is
the
length
of
the
input.
W
e
claim
that
also
SA
T
has
small
circuits
fC
0
n
g
n
where
n
is
the
n
um
b
er
of
v
ariables
in
the
form
ula.
This
claim
holds
since
the
length
of
a
SA
T
form
ula
is
of
O
(n

)
and
therefore
fC
0
n
g
can
use
the
larger
sets
of
circuits
C

;
:
:
:
;
C
O
(n

)
.
Let
us
em
b
ed
the
circuits
in
our
statemen
t
regarding
mem
b
ership
in
L,
this
will
yield:
x

L
i
	(C
0

;
:
:
:
;
C
0
n
)
(n
def
=
max
y
f#v
ar
(f
(x;
y
))g)
s.t.:

C
0

;
:
:
:
;
C
0
n
correctly
computes
SA
T,
for
form
ulas
with
a
corresp
onding
n
um
b
er
of
v
ariables.

y
;
C
0
#v
ar
(f
(x;y
))
(f
(x;
y
))
=

The
second
item
ab
o
v
e
giv
es
us
that
L



,
since
the
quan
tiers
are
as
needed.
Ho
w
ev
er
it
is
not
clear
that
the
rst
item
is
also
baha
ving
as
needed.
W
e
will
restate
the
rst
item
as
follo
ws:


;
:
:
:
;

n
;
[
n
^
i=
C
0
i
(
i
)
=
(C
0
i 
(
0
i
)
_
C
0
i 
(
00
i
))
^
C
0

oper
ates
cor
r
ectl
y
]
(	.)
Where:

i
(x

;
:
:
:
;
x
i
)
is
an
y
form
ula
o
v
er
i
v
ariables.

0
i
(x

;
:
:
:
;
x
i 
)
def
=

i
(x

;
:
:
:
;
x
i 
;
0)

00
i
(x

;
:
:
:
;
x
i 
)
def
=

i
(x

;
:
:
:
;
x
i 
;
)


LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)
A
stupid
tec
hnicalit
y:
Note
that
assigning
a
v
alue
to
one
of
the
v
ariables,
giv
es
us
a
form
ula
that
is
not
in
CNF
as
required
b
y
SA
T
(as
its
clauses
ma
y
con
tain
constan
ts).
Ho
w
ev
er
this
can
easily
b
e
ac
hiev
ed,
b
y
iterating
the
follo
wing
pro
cess,
where
in
eac
h
iteration
one
of
the
follo
wing
rules
is
applied:

x
_
0
should
b
e
c
hanged
to
x.

x
_

should
b
e
c
hanged
to
.

x
^
0
should
b
e
c
hanged
to
0.

x
^

can
b
e
c
hanged
to
x.

:
can
b
e
c
hanged
to
0.

:0
can
b
e
c
hanged
to
.
If
w
e
end-up
with
a
form
ula
in
whic
h
some
v
ariables
do
not
app
ear,
w
e
can
augmen
t
it
b
y
adding
clauses
of
the
form
x
^
:x.
Oded's
Note:
An
alternativ
e
resolution
of
the
ab
o
v
e
tec
hnicalit
y
is
to
extend
the
deni-
tion
of
CNF
so
to
allo
w
constnats
to
app
ear
(in
clauses).
Getting
bac
k
to
the
main
thing:
W
e
ha
v
e
giv
en
a
recursiv
e
denition
for
a
correct
computation
of
the
circuits
(on
SA
T).
The
base
of
the
recursion
is
c
hec
king
that
a
single
v
ariable
from
ula
is
handled
correctly
b
y
C
0

,
whic
h
is
v
ery
simple
(just
c
hec
k
if
the
single
v
ariable
form
ula
is
satisable
or
not,
and
compare
it
to
the
output
of
the
circuit).
In
order
to
v
alidate
the
(i
+
)
th
circuit,
w
e
wish
to
use
the
i
th
circuit,
whic
h
has
already
b
een
v
alidated.
Doing
so
requires
us
to
reduce
the
n
um
b
er
of
v
ariables
in
the
form
ula
b
y
one.
This
is
done
b
y
assigning
to
one
of
the
v
ariables
b
oth
p
ossible
v
alues
(0
or
),
and
obtaining
t
w
o
form
ulas
up
on
whic
h
the
i
th
circuit
can
b
e
applied.
The
full
form
ula
is
satisable
i
at
least
one
of
the
reduced
form
ulas
is
satisable.
Therefore
w
e
com
bine
the
results
of
applying
the
i
th
circuit
on
the
reduced
form
ulas,
with
the
_
op
eration.
It
no
w
remains
to
compare
it
to
the
v
alue
computed
b
y
the
(i
+
)
th
circuit
on
the
full
form
ula.
This
is
done
for
all
form
ula
o
v
er
i
+

v
ariables
(b
y
the
quan
tication

i+
).
So
all
together
w
e
get
that:
x

L
i
	(C
0

;
:
:
:
;
C
0
n
);
s.t.
y
;
(

;
:
:
:
;

n
);
(x;
(C
0

;
:
:
:
;
C
0
n
);
(y
;


;
:
:
:
;

n
))

R
L
where
R
L
is
a
p
olynomially-b
ounded
-ary
relation
dened
using
the
Karp
reduction
f
,
Eq.
(	.)
and
the
simplifying
pro
cess
ab
o
v
e.
Sp
ecically
,
the
algorithm
recognizing
R
L
computes
the
form
ula
f
(x;
y
),
determines
the
form
ulas

0
i
and

00
i
(for
eac
h
i),
and
ev
aluates
circuits
(the
description
of
whic
h
is
giv
en)
on
inputs
whic
h
are
also
giv
en.
Cleraly
,
this
algorithm
can
b
e
implemen
ted
in
p
olynomial-time,
and
so
it
follo
ws
that
L



as
needed.
Bibliographic
Notes
The
P
olynomial-Time
Hierarc
h
y
w
as
in
tro
duced
b
y
Sto
c
kmey
er
[].
The
third
equiv
alen
t
form
ula-
tion
via
\alternating
mac
hines"
can
b
e
found
in
[].
The
fact
that
B
P
P
is
in
the
P
olynomial-time
hierarc
h
y
w
as
pro
v
en
indep
enden
tly
b
y
Laute-
mann
[]
and
Sipser
[].
W
e
ha
v
e
follo
w
ed
Lautemann's
pro
of.
The
ideas
underlying
Sipser's
pro
of

	..
IF
NP
HAS
SMALL
CIR
CUITS
THEN
PH
COLLP
ASES

found
man
y
applications
in
complexit
y
theory
,
and
will
b
e
presen
ted
in
the
next
lecture
(in
the
appro
ximation
pro
cedure
for
#P
).
Among
these
applications,
w
e
men
tion
Sto
c
kmey
er's
appro
x-
imation
pro
cedure
for
#P
(cf.,
citel	:S),
the
reduction
of
SA
T
to
uniqueSA
T
(cf.
[]
and
next
lecture),
and
the
equiv
alence
b
et
w
een
public-coin
in
teractiv
e
pro
ofs
and
general
in
teractiv
e
pro
ofs
(cf.
[]
and
Lecture
).
The
fact
that
N
P

P
=p
oly
implies
a
collapse
of
the
P
olynomial-time
hierarc
h
y
w
as
pro
v
en
b
y
Karp
and
Lipton
[].
.
A.K.
Chandra,
D.C.
Kozen
and
L.J.
Sto
c
kmey
er.
Alternation.
JA
CM,
V
ol.
,
pages
{,
	.
.
S.
Goldw
asser
and
M.
Sipser.
Priv
ate
Coins
v
ersus
Public
Coins
in
In
teractiv
e
Pro
of
Systems.
A
dvanc
es
in
Computing
R
ese
ar
ch:
a
r
ese
ar
ch
annual,
V
ol.

(Randomness
and
Computation,
S.
Micali,
ed.),
pages
{	0,
		.
Extended
abstract
in
th
STOC,
pages
	{,
	.
.
R.M.
Karp
and
R.J.
Lipton.
\Some
connections
b
et
w
een
non
uniform
and
uniform
complexit
y
classes",
in
th
STOC,
pages
0-0	,
	0.
.
C.
Lautemann.
BPP
and
the
P
olynomial
Hierarc
h
y
.
IPL,
,
pages
{,
	.
.
M.
Sipser.
A
Complexit
y
Theoretic
Approac
h
to
Randomness.
In
th
STOC,
pages
0{,
	.
.
L.J.
Sto
c
kmey
er.
The
P
olynomial-Time
Hierarc
h
y.
The
or
etic
al
Computer
Scienc
e,
V
ol.
,
pages
{,
	.
.
L.
Sto
c
kmey
er.
The
Complexit
y
of
Appro
ximate
Coun
ting.
In
th
STOC,
pages
{,
	.
.
L.G.
V
alian
t
and
V.V.
V
azirani.
NP
Is
as
Easy
as
Detecting
Unique
Solutions.
The
or
etic
al
Computer
Scienc
e,
V
ol.

(),
pages
{	,
	.
App
endix:
Pro
of
of
Prop
osition
	..
Recall
that
our
aim
is
to
pro
v
e
the
claim:
F
or
every
k

,
if

k
=

k
then
P
H
=

k
.
Pro
of:
F
or
an
arbitrary
xed
k
,
w
e
will
sho
w
b
y
induction
on
i
that
i

k
;

i
=

k
:
.
Base
of
induction:
when
i
=
k
,
there
is
nothing
to
sho
w.
.
Induction
step:
b
y
the
inductiv
e
h
yp
othesis
it
follo
ws
that

i
=

k
,
so
what
remains
to
b
e
sho
wn
is
that
N
P

k
=

k
.
Con
tainmen
t
in
one
direction
is
ob
vious
so
w
e
fo
cus
on
pro
ving
that
N
P

k


k
.
Let
L

N
P

k
,
then
there
exist
a
non-deterministic,
p
olynomial-time
mac
hine
M
,
and
an
oracle
A


k
,
suc
h
that
L
=
L(M
A
).
Since

k
=

k
it
follo
ws
that
A


k
to
o.
Therefore,
there
exist
relations
R
A
and
R
A
(k
+
-ary
relations,
p
olynomially
b
ounded,
and
p
olynomial
time
recognizable,
for
A
and
A
resp
ectiv
ely)
suc
h
that
:

q

A
i
	w

;
w

;
:
:
:
;
Q
k
w
k
s.t.
(q
;
w

;
:
:
:
w
k
)

R
A
.


LECTURE
	.
THE
POL
YNOMIAL
HIERAR
CHY
(PH)

q

A
i
	w

;
w

;
:
:
:
;
Q
k
w
k
s.t.
(q
;
w

;
:
:
:
w
k
)

R
A
.
Using
those
relations,
and
the
denition
of
N
P

k
w
e
get:
x

L
i
	y
;
q

;
a

;
:
:
:
;
q
t
;
a
t
s.t.
for
all


j

t:

a
j
=

(
)
q
j

A
(
)
	w
(j;)

;
w
(j;)

;
:
:
:
;
Q
k
w
(j;)
k
s.t.
(q
j
;
w
(j;)

;
:
:
:
w
(j;)
k
)

R
A
.

a
j
=
0
(
)
q
j

A
(
)
	w
(j;0)

;
w
(j;0)

;
:
:
:
;
Q
k
w
(j;0)
k
s.t.
(q
j
;
w
(j;0)

;
:
:
:
w
(j;0)
k
)

R
A
.
Dene:

w

is
the
concatenation
of:
y
;
q

;
a

;
:
:
:
;
q
t
;
a
t
;
w
(;0)

;
:
:
:
;
w
(t;0)

;
w
(;)

;
:
:
:
;
w
(t;)

.
.
.

w
k
is
the
concatenation
of:
w
(;0)
k
;
:
:
:
;
w
(t;0)
k
;
w
(;)
k
;
:
:
:
;
w
(t;)
k

R
L
is
a
k
+
-ary
relation
suc
h
that:
(x;
w

;
:
:
:
;
w
k
)

R
L
i
for
all


j

t:
{
a
j
=

=
)
(q
j
;
w
(j;)

;
:
:
:
w
(j;)
k
)

R
A
.
{
a
j
=
0
=
)
(q
j
;
w
(j;0)

;
:
:
:
w
(j;0)
k
)

R
A
.
Since
M
is
a
p
olynomial
mac
hine,
then
t
is
p
olynomial
in
the
length
of
x.
R
A
and
R
A
are
p
olynomial
time
recognizable,
and
p
olynomially
b
ounded
relations.
Therefore
R
L
is
also
so.
All
together
w
e
get
that
there
exists
a
p
olynomially
b
ounded,
and
p
olynomial
time
recogniz-
able
relation
R
L
suc
h
that
:
x

L
i
	w

;
w

;
:
:
:
;
Q
k
w
k
s.t.
(x;
w

;
:
:
:
;
w
k
)

R
L
By
the
denition
of

k
,
L


k
.

Lecture
0
The
coun
ting
class
#P
Notes
tak
en
b
y
Oded
Lac
hish,
Y
oa
v
Ro
deh
and
Y
ael
T
auman
Summary:
Up
to
this
p
oin
t
in
the
course,
w
e'v
e
fo
cused
on
decision
problems
where
the
questions
are
YES/NO
questions.
No
w
w
e
are
in
terested
in
coun
ting
problems.
In
N
P
an
elemen
t
w
as
in
the
language
if
it
had
a
short
c
hec
k
able
witness.
In
#P
w
e
wish
to
coun
t
the
n
um
b
er
of
witnesses
to
a
sp
ecic
elemen
t.
W
e
rst
dene
the
complexit
y
class
#P
,
and
classify
it
with
resp
ect
to
other
complexit
y
classes.
W
e
then
pro
v
e
the
existence
of
#P
-complete
problems,
and
men
tion
some
natural
ones.
Then
w
e
try
to
study
the
relation
b
et
w
een
#P
and
N
P
more
exactly
,
b
y
sho
wing
w
e
can
probabilistically
appro
ximate
#P
using
an
oracle
in
N
P
.
Finally
,
w
e
rene
this
result
b
y
restricting
the
oracle
to
a
w
eak
form
of
S
AT
(called
uniq
ueS
AT
).
0.
Dening
#P
W
e
used
the
notion
of
an
N
P
-relation
when
dening
N
P
.
Recall:
Denition
0.
(N
P
relation)
:
A
n
N
P
relation
is
a
r
elation
R


?


?
such
that:

R
is
p
olynomial
time
de
cidable.

Ther
e
exists
a
p
olynomial
p()
such
that
for
every
(x;
y
)

R
,
it
holds
that
jy
j

p(jxj).
Giv
en
an
N
P
-relation
R
w
e
dened:
Denition
0.
L
R
def
=
fx


?
j
	y
s.t.
(x;
y
)

R
g
W
e
regard
the
y
's
that
satisfy
(x;
y
)

R
as
witnesses
to
the
mem
b
ership
of
x
in
the
language
L
R
.
The
decision
problem
asso
ciated
with
R
,
is
the
question:
Do
es
there
exist
a
witness
to
a
giv
en
x?
This
is
our
denition
of
the
class
N
P
.
Another
natural
question
w
e
can
ask
is:
Ho
w
man
y
witnesses
are
there
for
a
giv
en
x?
This
is
exactly
the
question
w
e
capture
b
y
dening
the
complexit
y
class
#P
.
W
e
rst
dene:
Denition
0.
F
or
every
binary
r
elation
R


?


?
,
the
c
ounting
function
f
R
:

?
!
N,
is
dene
d
by:
f
R
(x)
def
=
jfy
j
(x;
y
)

R
gj



LECTURE
0.
THE
COUNTING
CLASS
#P
The
function
f
R
captures
our
notion
of
coun
ting
witnesses
in
the
most
natural
w
a
y
.
So,
w
e
dene
#P
as
a
class
of
functions.
Sp
ecically
,
functions
that
coun
t
the
n
um
b
er
of
witnesses
in
an
N
P
-
relation.
Denition
0.
#P
=
ff
R
:
R
is
an
N
P
r
elationg
W
e
encoun
ter
some
problems
when
trying
to
relate
#P
to
other
complexit
y
classes,
since
it
is
a
class
of
functions
while
all
classes
w
e
discussed
so
far
are
classes
of
languages.
T
o
solv
e
this,
w
e
are
forced
to
giv
e
a
less
natural
denition
of
#P
,
using
languages.
F
or
eac
h
N
P
-relation
R
,
w
e
asso
ciate
a
language
#
R
.
Ho
w
do
w
e
dene
#
R
?
Our
rst
attempt
w
ould
b
e:
Denition
0.
(Coun
ting
language
|
rst
attempt)
:
#
R
=
f(x;
k
)
:
jfy
:
(x;
y
)

R
gj
=
k
g
First
observ
e
that
giv
en
an
oracle
to
f
R
,
it
is
easy
to
decide
#
R
.
This
is
a
nice
prop
ert
y
of
#
R
,
since
w
e
w
ould
lik
e
it
to
closely
represen
t
our
other
formalism
using
functions.
F
or
the
same
reason
w
e
also
w
an
t
the
other
direction:
Giv
en
an
oracle
to
#
R
,
w
e
w
ould
lik
e
to
b
e
able
to
calculate
f
R
ecien
tly
(in
p
olynomial
time).
This
is
not
as
trivial
as
the
other
direction,
and
in
fact,
is
not
ev
en
necessarily
true.
So
instead
of
tac
kling
this
problem,
w
e
alter
our
denition:
Denition
0.
(Coun
ting
language
|
actual
denition)
:
#
R
=
f(x;
k
)
:
jfy
:
(x;
y
)

R
gj

k
g.
In
other
wor
ds,
(x;
k
)

#
R
i
k

f
R
(x).
W
e
c
ho
ose
the
last
denition,
b
ecause
no
w
w
e
can
pro
v
e
the
follo
wing:
Prop
osition
0..
F
or
e
ach
N
P
-r
elation
R
:
.
#
R
is
Co
ok
r
e
ducible
to
f
R
.
f
R
is
Co
ok
r
e
ducible
to
#
R
W
e
denote
the
fact
that
problem
P
Co
ok
reduces
to
problem
Q
b
y
P

c
Q.
Pro
of:
.
(#
R
is
Co
ok
r
e
ducible
to
f
R
)
:
Giv
en
(x;
k
),
w
e
w
an
t
to
decide
whether
(x;
k
)

#
R
.
W
e
use
our
oracle
for
f
R
,
b
y
calling
it
with
parameter
x.
As
an
answ
er
w
e
get
:
l
=
jfy
:
(x;
y
)

R
gj.
If
l

k
then
w
e
accept,
otherwise
reject.
.
(f
R
is
Co
ok
r
e
ducible
to
#
R
)
:
Giv
en
x,
w
e
w
an
t
to
nd
f
R
(x)
=
jfy
:
(x;
y
)

R
gj
using
our
oracle.
W
e
kno
w
f
R
(x)
is
in
the
range
f0;
:
:
:
;

jp(x)j
g,
where
p()
is
the
p
olynomial
b
ounding
the
size
of
the
witnesses
in
the
denition
of
an
N
P
-relation.
The
oracle
giv
en
is
exactly
what
w
e
need
to
implemen
t
binary
searc
h.
B
I
N
AR
Y
(x;
Low
er
;
U
pper
)
:

if
(Low
er
=
U
pper
)
output
Low
er
.

M
iddl
e
=
Low
er
+U
pper


if
(x;
M
iddl
e)

#
R
output
B
I
N
AR
Y
(x;
M
iddl
e;
U
pper
)

else
output
B
I
N
AR
Y
(x;
Low
er
;
M
iddl
e)
Where
the
branc
hing
in
the
third
line
is
b
ecause
if
(x;
M
iddl
e)

#
R
,
then
f
R
(x)

M
iddl
e,
so
w
e
need
only
searc
h
for
the
result
in
the
range
[M
iddl
e;
U
pper
].
A
symmetric
argumen
t
explains
the
else
clause.
The
output
is:
f
R
(x)
=
B
I
N
AR
Y
(x;
0;

p(jxj)
).
Binary
searc
h
in
general,
runs
in
time
loga-
rithmic
in
in
terv
al
it
searc
hes
in.
In
our
case
:
O
(log
(
p(jxj)
))
=
O
(p(jxj)).
W
e
conclude,
that
the
algorithm
runs
in
p
olynomial
time
in
jxj.

0..
COMPLETENESS
IN
#P

Notice
that
w
e
could
ha
v
e
c
hanged
our
denition
of
#
R
,
to
b
e:
#
R
=
f(x;
k
)
:
jfy
:
(x;
y
)

R
)gj

k
g
The
prop
osition
w
ould
still
hold.
W
e
could
ha
v
e
also
c
hanged
it
to
a
strict
inequalit
y
,
and
gotten
the
same
result.
>F
rom
no
w
on
w
e
will
use
the
more
natural
denition
of
#P
:
as
a
class
of
functions.
This
do
esn't
really
matter,
since
w
e
sho
w
ed
that
in
terms
of
co
ok-reducibilit
y
,
the
t
w
o
denitions
are
equiv
alen
t.
It
seems
that
the
coun
ting
problem
related
to
a
relation
R
should
b
e
harder
than
the
corre-
sp
onding
decision
problem.
It
is
unkno
wn
whether
it
is
strictly
harder,
but
it
is
certainly
not
w
eak
er.
That
is,
Prop
osition
0..
F
or
every
N
P
-r
elation
R
,
the
c
orr
esp
onding
language
L
R
Co
ok
r
e
duc
es
to
f
R
.
Pro
of:
Giv
en
x


?
,
use
the
oracle
to
calculate
f
R
(x).
No
w,
x

L
R
if
and
only
if
f
R
(x)

.
Corollary
0.
N
P
Co
ok
r
e
duc
es
to
#P
On
the
other
hand
w
e
can
b
ound
the
complexit
y
of
#P
from
ab
o
v
e:
Claim
0..
#P
Co
ok
r
e
duc
es
to
P
SP
A
CE
Pro
of:
Giv
en
x,
w
e
w
an
t
to
calculate
f
R
(x)
using
p
olynomial
space.
Let
p()
b
e
the
p
olynomial
b
ounding
the
length
of
the
witnesses
of
R
.
W
e
run
o
v
er
all
p
ossible
witnesses
of
length

p(jxj).
F
or
eac
h
one,
w
e
c
hec
k
in
p
olynomial
time
whether
it
is
a
witness
for
x,
and
sum
the
n
um
b
er
of
witnesses.
All
this
can
b
e
done
in
space
O
(p(jxj)
+
q
(jxj)),
where
q
()
is
the
p
olynomial
b
ounding
the
running
time
(and
therefore
space)
of
the
witness
c
hec
king
algorithm.
Suc
h
a
p
olynomial
exists
since
R
is
an
N
P
-relation.
0.
Completeness
in
#P
When
one
talks
ab
out
complexit
y
classes,
pro
ving
the
existence,
and
nding
complete
problems
in
the
complexit
y
class,
is
of
great
imp
ortance.
It
helps
reason
ab
out
the
whole
class
using
only
one
sp
ecic
problem.
Therefore,
w
e
are
lo
oking
for
an
N
P
-relation
R
,
s.t.
for
ev
ery
other
N
P
-relation
Q,
there
is
a
Co
ok
reduction
from
f
Q
to
f
R
.
F
ormally:
Denition
0.
(#P
-complete)
:
f
is
#P
-complete
if
.
f
is
in
#P
.
.
F
or
every
g
in
#P
,
g
Co
ok
r
e
duc
es
to
f
.
With
Occam's
Razor
in
mind,
w
e'll
try
to
nd
a
complete
problem,
suc
h
that
all
other
problems
are
reducible
to
it
using
a
v
ery
simple
form
of
reduction.
Note
that
b
y
restricting
the
kind
of
reductions
w
e
allo
w,
w
e
ma
y
rule
out
candidates
for
#P
-complete
problems.
W
e
tak
e
a
restricted
form
of
a
Levin
reduction

from
f
Q
to
f
R
:
x


?
:
f
Q
(x)
=
f
R
((x))


LECTURE
0.
THE
COUNTING
CLASS
#P
By
allo
wing
only
this
kind
of
reduction,
w
e
can
nd
out
sev
eral
things
ab
out
our
candidates
for
#P
-complete
problems.
F
or
example:
f
Q
(x)


,
f
R
((x))


In
other
w
ords
:
x

L
Q
,
(x)

L
R
Whic
h
means
that

is
a
Karp
reduction
from
L
Q
to
L
R
.
This
implies
that
the
decision
problem
related
to
R
m
ust
b
e
N
P
-complete.
Moreo
v
er,
w
e
require
that
the
reduction
preserv
es
the
n
um
b
er
of
witnesses
for
ev
ery
input
x.
W
e
capture
this
notion
in
the
follo
wing
denition:
Denition
0.	
(P
arsimonious)
:
A
r
e
duction

:

?
!

?
,
is
P
arsimonious
w.r.t.
N
P
-r
elations
Q
and
R
if
for
every
x
:
jfy
:
(x;
y
)

Qgj
=
jfy
:
((x);
y
)

R
gj.
Corollary
0.0
if
R
is
an
N
P
-r
elation,
and
for
every
N
P
-r
elation
Q
ther
e
exists

Q
:

?
!

?
s.t.

Q
is
p
arsimonious
w.r.t.
Q
and
R
then
f
R
is
#P
-c
omplete.
As
w
e'v
e
said,
a
parsimonious
reduction
from
f
Q
to
f
R
m
ust
b
e
a
Karp
reduction
from
L
Q
to
L
R
.
Therefore,
w
e'll
try
to
pro
v
e
that
the
Karp
reductions
w
e
used
to
pro
v
e
S
AT
is
N
P
-complete,
are
also
parsimonious,
and
thereb
y
#S
AT
is
#P
-complete.
Denition
0.
R
S
AT
=
(
( 
;

)





 
is
a
b
o
ole
an
formula
on
variables
V
( 
)

is
a
truth
assignment
for
V
( 
)
:
 
(
)
=

)
W
e
ha
v
e
pro
v
ed
S
AT
def
=
L
R
S
AT
is
N
P
-complete
b
y
a
series
of
Karp
reductions.
All
w
e
need
to
sho
w
is
that
eac
h
step
is
in
fact
a
parsimonious
reduction.
Theorem
0.
#S
AT
def
=
f
R
S
AT
is
#P
-c
omplete.
Pro
of:
(outline)
.
Ob
viously
#S
AT
is
in
#P
,
since
R
S
AT
is
an
N
P
-relation.
.

The
reduction
from
a
generic
N
P
-relation
R
to
Bounded-Halting,
is
parsimonious
b
e-
cause
the
corresp
ondence
b
et
w
een
the
witnesses
is
not
only
one-to-one,
it
is
in
fact
the
iden
tit
y
.

The
reduction
from
Bounded-Halting
to
Circuit-S
AT
consists
of
creating
for
eac
h
time
unit
a
set
of
v
ariables
that
can
describ
e
eac
h
p
ossible
conguration
uniquely
.
Since
a
successful
run
is
a
sp
ecic
list
of
congurations,
and
corresp
onds
to
one
witness
of
Bounded-Halting,
w
e
get
the
same
witness
translated
in
to
one
unique
represen
tation
in
binary
v
ariables.

In
the
reduction
from
Circuit-S
AT
to
S
AT
w
e
add
extra
v
ariables
for
eac
h
in
ternal
gate
in
the
circuit.
Eac
h
satisfying
assignmen
t
to
the
original
circuit
uniquely
determines
all
the
v
alues
in
the
in
ternal
gates,
and
therefore
giv
es
us
exactly
one
satisfying
assignmen
t
to
the
form
ula.

0..
COMPLETENESS
IN
#P
	
Notice
that
w
e
actually
pro
v
ed
that
the
coun
ting
problems
asso
ciated
with
Bounded-Halting,
Circuit-S
AT
,
and
S
AT
are
#P
-complete.
Not
only
did
w
e
pro
v
e
#S
AT
to
b
e
#P
-complete,
w
e
also
sho
w
ed
that
for
ev
ery
f
in
#P
,
there
exists
a
parsimonious
reduction
from
f
to
#S
AT
.
The
reader
migh
t
ha
v
e
gotten
the
impression
that
ev
ery
N
P
-relation
R
,
suc
h
that
f
R
is
#P
-
complete
implies
L
R
is
N
P
-complete.
But
the
follo
wing
theorem
sho
ws
the
con
trary:
Theorem
0.
Ther
e
exists
an
N
P
-r
elation
R
s.t.
f
R
is
#P
-c
omplete,
and
L
R
is
p
olynomial
time
de
cidable.
Notice
that
suc
h
a
#P
-complete
function,
do
es
not
ha
v
e
the
prop
ert
y
that
w
e
sho
w
ed
#S
AT
has:
Not
all
other
functions
in
#P
ha
v
e
a
parsimonious
reduction
to
it.
In
fact
it
cannot
b
e
that
ev
ery
#P
problem
has
a
Karp
reduction
to
f
R
,
since
otherwise
L
R
w
ould
b
e
N
P
-complete.
The
idea
in
the
pro
of
is
to
mo
dify
a
hard
to
calculate
relation
b
y
adding
easy
to
recognize
witnesses
to
ev
ery
input,
so
that
the
question
of
existence
of
a
witness
b
ecomes
trivial,
y
et
the
coun
ting
problem
remains
just
as
hard.
Clearly
,
the
#P
-hardness
will
ha
v
e
to
b
e
pro
v
en
b
y
a
non-parsimonious
reduction
(actually
ev
en
a
non-Karp
reduction).
Pro
of:
W
e
dene
:
R
0
S
AT
=

>
<
>
:
(;
(
;

))







((
)
=
)
^
(
=
)
_

=
0
	
>
=
>
;
Ob
viously
L
R
0
S
AT
=

?
,
so
it
is
in
P
.
But
f
R
0
S
AT
is
#P
-complete,
since
for
ev
ery

:
's
witnesses
in
R
0
S
AT
are:
f(
;
)
:
(
)
=
g
[
f(
;
0)g
Whic
h
means:
#S
AT
()
+

jV
ariables()
j
=
f
R
0
S
AT
()
So
giv
en
an
oracle
to
f
R
0
S
AT
w
e
can
easily
calculate
#S
AT
,
meaning
that
f
R
0
S
AT
is
#P
-complete.
W
e
pro
v
ed
the
ab
o
v
e
theorem
b
y
constructing
a
somewhat
unnatural
N
P
-relation.
W
e
will
no
w
nd
a
more
natural
problem
that
giv
es
the
same
result
(i.e.,
whic
h
is
also
#P
-complete).
Denition
0.
(Bipartite
Graph)
:
G
=
(V

[
V

;
E
)
is
a
Bipartite
Graph
if

V

\
V

=
;

E

V


V

Denition
0.
(P
erfect
Matc
hing)
:
L
et
G
=
(V

[
V

;
E
)
b
e
a
bip
artite
gr
aph.
A
P
erfect
Matc
hing
is
a
set
of
e
dges
M

E
,
that
satises:
.
every
v

in
V

app
e
ars
in
exactly
one
e
dge
of
M
.
.
every
v

in
V

app
e
ars
in
exactly
one
e
dge
of
M
.
Denition
0.
(P
erfect
Matc
hing
|
equiv
alen
t
denition)
:
L
et
G
=
(V

[
V

;
E
)
b
e
a
bip
artite
gr
aph.
A
P
erfect
Matc
hing
is
a
one-to-one
and
onto
function
f
:
V

!
V

s.t.
for
every
v
in
V

,
(v
;
f
(v
))

E
.
Pro
of:
(e
quivalenc
e
of
denitions)
:

0
LECTURE
0.
THE
COUNTING
CLASS
#P

Assume
w
e
ha
v
e
a
subset
of
edges
M

E
that
satises
the
rst
denition.
Dene
a
function
f
:
V

!
V
:
f
(v

)
=
v

(
)
(v

;
v

)

M
f
is
w
ell
dened,
b
ecause
eac
h
v

in
V

app
ears
in
exactly
one
edge
of
M
.
It
is
one-to-one
and
on
to
b
ecause
eac
h
v

in
V

app
ears
in
exactly
one
edge
of
M
.
Since
M

E
,
f
satises
the
condition
that
for
all
v

in
V

:
(v

;
f
(v

))
is
in
E
.

Assume
w
e
ha
v
e
a
one-to-one
and
on
to
function
f
:
V

!
V

that
satises
the
ab
o
v
e
condition.
W
e
construct
a
set
M

E
:
M
=
f(v

;
f
(v

))
:
v


V

g
M

E
b
ecause
for
ev
ery
v

in
V

w
e
kno
w
that
(v

;
f
(v

))
is
in
E
.
The
t
w
o
conditions
are
also
satised:
.
Since
f
is
a
function,
ev
ery
v

in
V

app
ears
in
exactly
one
edge
of
M
.
.
Since
f
is
one-to-one
and
on
to,
ev
ery
v

in
V

app
ears
in
exactly
one
edge
of
M
.
Denition
0.
R
P
M
=
f(G;
f
)
:
G
is
a
bip
artite
gr
aph
and
f
is
a
p
erfe
ct
matching
of
G
g
F
act
0..
L
R
P
M
is
p
olynomial
time
de
cidable.
The
idea
of
the
algorithm
is
to
reduce
the
problem
to
a
net
w
ork-o
w
problem
whic
h
is
kno
wn
to
ha
v
e
a
p
olynomial
time
algorithm.
Giv
en
a
bipartite
graph
G
=
(V

[
V

;
E
),
w
e
construct
a
directed
graph
G
0
=
(V

[
V

[
fs;
tg;
E
0
),
so
that:
E
0
=
E
[
f(s;
v

)
:
v


V

g
[
f(v

;
t)
:
v


V

g
where
E
is
view
ed
as
directed
edges
from
V

to
V

.
What
w
e
did,
is
add
a
source
s
and
connect
it
to
one
side
of
the
graph,
and
a
sink
t
connected
to
the
other
side.
W
e
transform
it
to
a
o
w
problem
b
y
setting
a
w
eigh
t
of

to
ev
ery
edge
in
the
graph.
There
is
a
one
to
one
corresp
ondence
b
et
w
een
partial
matc
hings
and
in
teger
o
ws
in
the
graph:
Edges
in
the
matc
hing
corresp
ond
to
edges
in
E
ha
ving
a
o
w
of
.
Therefore,
there
exists
a
p
erfect
matc
hing
i
there
is
a
o
w
of
size
jV

j
=
jV

j.
Theorem
0.
f
R
P
M
is
#P
-c
omplete.
This
result
is
pro
v
ed
b
y
sho
wing
that
the
problem
of
computing
the
p
ermanen
t
of
a
f0;
g
matrix
is
#P
-complete.
W
e
will
sho
w
the
reduction
from
coun
ting
the
n
um
b
er
of
p
erfect
matc
hings
to
computing
the
p
ermanen
t
of
suc
h
matrices.
In
fact,
the
t
w
o
problems
are
computationally
equiv
alen
t.
Denition
0.	
(P
ermanen
t)
:
The
p
ermanen
t
of
an
n

n
matrix
A
=
(a
i;j
)
n
i;j
=
is:
P
er
m(A)
=
X

S
n
n
Y
i=
a
i;
(i)
Wher
e
S
n
=
f
:

is
a
p
ermutation
of
f;
:
:
:
;
ngg.

0..
COMPLETENESS
IN
#P

Note
that
the
denition
of
the
p
ermanen
t
of
a
matrix
closely
resem
bles
that
of
the
determinan
t
of
a
matrix.
In
the
denition
of
determinan
t,
w
e
ha
v
e
the
same
sum
and
pro
duct,
except
that
eac
h
elemen
t
in
the
sum
is
m
ultiplied
b
y
the
sig
n

f ;
g
of
the
p
erm
utation

.
Y
et,
computing
the
determinan
t
is
in
P
,
while
computing
the
p
ermanen
t
is
#P
-complete,
and
therefore
is
b
eliev
ed
not
to
b
e
in
P
.
The
main
result
in
this
section
is
the
(unpro
v
en
here)
theorem:
Theorem
0.0
P
er
m
is
#P
-c
omplete.
T
o
sho
w
the
equiv
alence
of
computing
f
R
P
M
and
P
er
m,
w
e
use:
Denition
0.
(Bipartite
Adjacency
Matrix)
:
Given
a
bip
artite
gr
aph
G
=
(V

[
V

;
E
),
wher
e
V

=
f;
:
:
:
;
ng,
and
V

=
f;
:
:
:
;
mg,
we
dene
the
Bipartite
Adjacency
Matrix
of
the
gr
aph
G,
as
an
n

m
matrix
B(G),
wher
e
:
B
(G)
i;j
=
(

(i;
j
)

E
0
otherwise
Prop
osition
0..
Given
a
bip
artite
gr
aph
G
=
(V

[
V

;
E
)
wher
e
jV

j
=
jV

j,
f
R
P
M
(G)
=
P
er
m(B
(G))
Pro
of:
P
er
m(B
(G))
=
jf

S
n
:
Q
n
i=
b
i;
(i)
=
gj
=
jf

S
n
:
i

f;
:
:
:
;
ng;
b
i;
(i)
=
gj
=
jf

S
n
:
i

f;
:
:
:
;
ng;
(i;

(i))

E
gj
=
jf

S
n
:

is
a
p
erfect
matc
hing
in
Ggj
=
f
R
P
M
(G)
W
e
just
sho
w
ed
that
the
problem
of
coun
ting
the
n
um
b
er
of
p
erfect
matc
hings
in
a
bipartite
graph
Co
ok
reduces
to
the
problem
of
calculating
the
p
ermanen
t
of
a
f0;
g
matrix.
Notice
that
the
other
direction
is
also
true
b
y
the
same
pro
of:
Giv
en
a
f0;
g
matrix,
create
the
bipartite
graph
that
corresp
onds
to
it.
No
w
w
e
will
sho
w
another
graph
coun
ting
problem,
that
is
equiv
alen
t
to
b
oth
of
these:
Denition
0.
(Cycle
Co
v
er)
:
A
Cycle
Co
v
er
of
a
dir
e
cte
d
gr
aph
G,
is
a
set
of
vertex
disjoint
simple
cycles
that
c
over
al
l
the
vertic
es
of
G.
Mor
e
formal
ly:
C

E
is
a
cycle
co
v
er
of
G
if
for
every
c
onne
cte
d
c
omp
onent
V

of
G
0
=
(V
;
C
),
ther
e
is
an
or
dering
V

=
fv
0
;
:
:
:
v
d 
g
s.t.
(v
i
;
v
j
)

C
,
j
=
i
+
(mod
d)
Notice
that
there
is
no
problem
with
connected
comp
onen
ts
of
size
,
b
ecause
w
e
allo
w
self
lo
ops.
Denition
0.
#C
y
cl
e(G)
=
numb
er
of
cycle
c
overs
of
G.
Denition
0.
(Adjacency
Matrix)
:
The
Adjacency
Matrix
of
a
dir
e
cte
d
gr
aph
G
=
(f;
:
:
:
;
ng;
E
)
is
an
n

n
matrix
A(G)
:
A(G)
i;j
=
(

(i;
j
)

E
0
otherwise
Prop
osition
0..
F
or
every
dir
e
cte
d
gr
aph
G,
P
er
m(A(G))
=
#C
y
cl
e(G)


LECTURE
0.
THE
COUNTING
CLASS
#P
In
pro
ving
this
prop
osition
w
e
use
the
follo
wing:
Claim
0..
C
is
a
cycle
c
over
of
G
if
and
only
if
every
v

V
has
an
out-de
gr
e
e
and
in-de
gr
e
e
of

in
G
0
=
(V
;
C
).
Pro
of:
(Claim)

(=
))
Ev
ery
v
ertex
app
ears
in
exactly
one
cycle
of
C
,
b
ecause
the
cycles
are
disjoin
t.
Also,
since
the
cycles
are
simple,
ev
ery
v
ertex
has
an
in-degree
and
out-degree
of
.

((
=)
F
or
ev
ery
connected
comp
onen
t
V
0
of
G
0
,
tak
e
a
v
ertex
v
0

V
0
,
and
create
the
directed
path
:
v
0
;
v

;
:
:
:
;,
where
for
ev
ery
i,
(v
i
;
v
i+
)

C
.
Since
the
out-degree
of
ev
ery
v
ertex
is

in
G
0
,
this
path
is
uniquely
determined,
and:
.
There
m
ust
exist
a
v
ertex
v
i
that
app
ears
t
wice
:
v
i
=
v
j
.
Because
V
is
nite.
.
W
e
claim
that
the
least
suc
h
i
is
0.
Otherwise,
the
in-degree
of
v
i
is
greater
than
.
.
All
the
v
ertices
of
V
0
app
ear
in
our
path
b
ecause
it
is
a
connected
comp
onen
t
of
G
0
.
Th
us
eac
h
V
0
induces
a
directed
cycle,
and
so
G
0
is
a
collection
of
disjoin
t
directed
cycles
whic
h
co
v
er
all
V
.
Pro
of:
(Prop
osition)
W
e'll
dene:

def
=
f

S
n
:
i

f;
:
:
:
;
ng;
(i;

(i))

E
g
It
is
easy
to
see
that
P
er
m(A(G))
=
j
j.
Ev
ery



denes
C

=
f(i;

(i))
:
i

f;
:
:
:
;
ngg

E
and
since

is
a
-
and
on
to
f;
:
:
:
;
ng,
the
out-degree
and
in-degree
of
eac
h
v
ertex
in
C

is
.
So
C

is
a
cycle
co
v
er
of
G.
On
the
other
hand,
ev
ery
cycle
co
v
er
C

G
denes
a
mapping:

C
(i)
=
j
s.t.
(i;
j
)

C
,
and
b
y
the
ab
o
v
e
claim,
this
is
a
p
erm
utation.
0.
Ho
w
close
is
#P
to
N
P
?
The
main
purp
ose
of
this
lecture,
is
to
study
the
class
#P
,
and
classify
it
as
b
est
as
w
e
can
among
other
complexit
y
classes
w
e'v
e
studied.
W
e'v
e
seen
some
examples
of
#P
complete
problems.
W
e
also
ga
v
e
upp
er
and
lo
w
er
complexit
y
b
ounds
on
#P
:
N
P

c
#P

c
P
SP
A
CE
W
e
will
no
w
try
to
rene
these
b
ounds
b
y
sho
wing
that
#P
is
not
as
far
from
N
P
as
one
migh
t
susp
ect.
In
fact,
a
coun
ting
problem
in
#P
can
b
e
probabilistically
appro
ximated
in
p
olynomial
time
using
an
N
P
oracle.

0..
HO
W
CLOSE
IS
#P
TO
N
P
?

0..
V
arious
Lev
els
of
Appro
ximation
W
e
will
start
b
y
in
tro
ducing
the
notion
of
a
range
problem.
A
range
problem
is
a
relaxation
of
the
problem
of
calculating
a
function.
Instead
of
requiring
one
v
alue
for
eac
h
input,
w
e
allo
w
a
full
range
of
answ
ers
for
eac
h
input.
Denition
0.
(Range
Problem)
:
A
Range
Problem

is
dene
d
by
two
functions

=
(
l
;

u
).

l
;

u
:

?
!
N.
s.t.
on
input
x


?
,
the
pr
oblem
is
to
nd
t

(
l
(x);

u
(x)),
or
in
other
wor
ds,
r
eturn
an
inte
ger
t,
s.t.

l
(x)
<
t
<

u
(x).
Note
that
there
is
no
restriction
on
the
functions

l
and

u
,
they
can
ev
en
b
e
non-recursiv
e.
Since
w
e
are
going
to
use
range
problems
to
denote
an
appro
ximation
to
a
function,
w
e
dene
a
sp
ecic
kind
of
range
problems
that
are
based
on
a
function:
Denition
0.
(Strong
Range)
:
F
or
f
:

?
!
N,
and
a
p
olynomial
p(),
we
dene
the
r
ange
pr
oblem
S
tr
ong
R
ang
e
p
(f
)
=
(l
;
u)
wher
e:
l
(x)
=
f
(x)

(
 
p(jxj)
)
u(x)
=
f
(x)

(
+

p(jxj)
)
Strong
range
captures
our
notion
of
a
go
o
d
appro
ximation.
W
e
will
pro
ceed
in
a
series
of
reductions
that
will
ev
en
tually
giv
e
us
the
desired
result.
The
rst
result
w
e
pro
v
e,
is
that
it
is
enough
to
strongly
appro
ximate
#S
AT
.
Prop
osition
0..
If
we
c
an
appr
oximate
#S
AT
str
ongly
we
c
an
just
as
str
ongly
appr
oximate
any
f
in
#P
.
In
other
wor
ds
:
F
or
every
f
in
#P
,
and
every
p
olynomial
p(),
S
tr
ong
R
ang
e
p
(f
)

c
S
tr
ong
R
ang
e
p
(#S
AT
)
Pro
of:
As
w
e'v
e
seen,
for
ev
ery
f
in
#P
,
there
is
a
parsimonious
reduction

f
w.r.t.
f
and
#S
AT
.
Meaning,
for
all
x
:
f
(x)
=
#S
AT
(
f
(x)).
W
e
ma
y
assume
that
j
f
(x)j
>
jxj,
b
ecause
w
e
can
alw
a
ys
pad

f
(x)
with
something
that
will
not
c
hange
the
n
um
b
er
of
witnesses:

f
(x)
^
z

^
z

^
:
:
:
^
z
jxj
W
e
no
w
use
our
oracle
to
S
tr
ong
R
ang
e
p
(#S
AT
)
on

f
(x),
and
get
a
result
t,
that
satises
:
t

(


p(j(x)j)
)

#S
AT
(
f
(x))

(


p(jxj)
)

f
(x)
W
e
no
w
wish
to
dene
a
w
eak
er
form
of
appro
ximation:
Denition
0.
(Constan
t
Range)
:
F
or
f
:

?
!
N,
and
a
c
onstant
c
>
0,
we
dene
the
r
ange
pr
oblem
C
onstantR
ang
e
c
(f
)
=
(l
;
u)
wher
e:
l
(x)
=

c

f
(x)
u(x)
=
c

f
(x)
W
e
w
an
t
to
sho
w
that
appro
ximating
#S
AT
up
to
a
constan
t
suces
to
appro
ximate
#S
AT
strongly
.
W
e'll
in
fact
pro
v
e
a
stronger
result:
that
an
ev
en
w
eak
er
form
of
appro
ximation
is
enough
to
appro
ximate
#S
AT
strongly
.


LECTURE
0.
THE
COUNTING
CLASS
#P
Denition
0.
(W
eak
Range)
:
F
or
f
:

?
!
N,
and
a
c
onstant

>
0,
we
dene
the
r
ange
pr
oblem
W
eak
R
ang
e

(f
)
=
(l
;
u)
wher
e:
l
(x)
=
(


)
jxj
 

f
(x)
u(x)
=

jxj
 

f
(x)
It
is
quite
clear
that
C
onstantR
ang
e
is
a
stronger
form
of
appro
ximation
than
W
eak
R
ang
e:
Claim
0..
F
or
every
0
<

<

and
c
>
0:
W
eak
R
ang
e

(#S
AT
)

c
C
onstantR
ang
e
c
(#S
AT
)
Pro
of:
Simply
b
ecause
for
large
enough
n:
(


;
)
n
 

(

c
;
c)
where
w
e
use
(


;
)
n
 
to
denote
the
range
((


)
n
 
;

n
 
).
No
w
w
e
pro
v
e
the
main
result:
Prop
osition
0..
F
or
every
p
olynomial
p(),
and
c
onstant
0
<

<
,
S
tr
ong
R
ang
e
p
(#S
AT
)

c
W
eak
R
ang
e

(#S
AT
)
Pro
of:
Giv
en
,
a
b
o
olean
form
ula
on
v
ariables
~
x
.
Dene
a
p
olynomial
q
(n)

(n

p(n))


,
and
build

0
:

0
=
q
(jj)
^
i=
(
~
x
i
)
Where
eac
h
~
x
i
is
a
distinct
cop
y
of
the
v
ariables
~
x.
Ob
viously
#S
AT
(
0
)
=
(#S
AT
())
q
(jj)
.
Notice
j
0
j

jj

q
(jj).
No
w,
assuming
w
e
ha
v
e
an
oracle
to
W
eak
R
ang
e

(#S
AT
),
w
e
call
it
on

0
to
get:
t

(


;
)
j
0
j
 

#S
AT
(
0
)

(


;
)
jjq
(jj)
 

(#S
AT
())
q
(jj)
Our
result
w
ould
b
e
s
=
t

q
(jj)
.
And
w
e
ha
v
e
:
s

(


;
)
jj
q
(jj)


#S
AT
()

(


;
)
jj
jjp(jj)

#S
AT
()
=
(


;
)

p(jj)

#S
AT
()

(


p(jj)
)

#S
AT
()
where
the
last
con
tainmen
t
follo
ws
from
:
x


:
(
 
x
)
x



and


(
+

x
)
x
After
a
small
div
ersion
in
to
pro
ving
a
stronger
result
than
needed,
w
e
conclude
that
all
w
e
ha
v
e
to
do
is
to
nd
a
constan
t
c
>
0,
suc
h
that
w
e
can
solv
e
C
onstantR
ang
e
c
(#S
AT
).
W
e
still
ha
v
e
a
hard
time
solving
the
problem
directly
,
so
w
e'll
do
y
et
another
reduction
in
to
a
relaxed
form
of
decision
problems,
called
promise
problems.
While
mac
hines
that
solv
e
decision
problems
are
required
to
giv
e
an
exact
answ
er
for
ev
ery
input,
promise
problems
are
only
required
to
do
so
on
a
predened
'promise'
set.

0..
HO
W
CLOSE
IS
#P
TO
N
P
?

Denition
0.	
(Promise
Problem)
:
A
Promise
Problem

=
(
Y
;

N
),
wher
e

Y
;

N


?
,
and

Y
\

N
=
;,
is
the
question:
Given
x

P
r
omise()
def
=

Y
[

N
,
de
cide
whether
x


Y
.
Notice
that
if
x

P
r
omise(),
there
is
no
requiremen
t.
Also,
promise
problems
are
a
generalization
of
decision
problems,
where
in
decision
problems
P
r
omise()
=

?
,
so
no
promise
is
made.
Denition
0.0
(Gap

#S
AT
)
The
pr
omise
pr
oblem
Gap

#S
AT
=
(Gap

#S
AT
Y
;
Gap

#S
AT
N
),
wher
e:
Gap

#S
AT
Y
=
f(;
K
)
:
#S
AT
()
>
K
g
Gap

#S
AT
N
=
f(;
K
)
:
#S
AT
()
<


K
g
W
e
no
w
con
tin
ue
in
our
reductions.
F
or
this
w
e
c
ho
ose
c
=
,
and
sho
w
w
e
can
solv
e
C
onstantR
ang
e

(#S
AT
)
using
an
oracle
to
Gap

#S
AT
.
Prop
osition
0..
C
onstantR
ang
e

(#S
AT
)
Co
ok
r
e
duc
es
to
Gap

#S
AT
Pro
of:
W
e
run
the
follo
wing
algorithm
on
input
:

i
=
0

While
(Gap

#S
AT
answ
ers
Y
E
S
on
(;

i
))
do
i
=
i
+


return

i 

Denote

=
log

(#S
AT
()).
The
result

k
 

,
satises
:

 
<
k
 

<

+
,
b
ecause
:

F
or
all
i
<

 ,
#S
AT
()
>



i
,
so
(;

i
)

Gap

#S
AT
Y
.
Therefore,
w
e
are
promised
that
in
suc
h
a
case
the
algorithm
will
incremen
t
suc
h
an
i,
and
not
stop.
So,
k


 
follo
ws.

F
or
all
i
>

+
,
#S
AT
()
<




i
,
so
(;

i
)

Gap

#S
AT
N
.
Meaning
that
the
algorithm
m
ust
stop
at
the
rst
suc
h
i
or
b
efore.
The
rst
suc
h
i
that
satises
i
>

+

also
satises
i


+
.
Therefore
k


+
.
No
w
:

 

k


+

+

 
<
k
 

<

+

W
e
conclude:

k
 


(


;
)

#S
AT
()
So
far
w
e'v
e
sho
wn
the
follo
wing
reductions:
S
tr
ong
R
ang
e
p
oly
(#P
)

c
S
tr
ong
R
ang
e
p
oly
(#S
AT
)

c
W
eak
R
ang
e

(#S
AT
)

c
C
onstantR
ang
e

(#S
AT
)

c
Gap

#S
AT
Since
Co
ok
reductions
are
transitiv
e,
w
e
get
:
S
tr
ong
R
ang
e
p
oly
(#P
)

c
Gap

#S
AT
W
e
will
sho
w
ho
w
to
solv
e
Gap

#S
AT
using
an
oracle
to
S
AT
,
but
with
a
small
probabilit
y
of
error.
So
w
e
will
sho
w,
that
in
general,
if
w
e
can
solv
e
a
problem
P
using
an
oracle
to
a
promise


LECTURE
0.
THE
COUNTING
CLASS
#P
problem
Q,
then
if
w
e
ha
v
e
an
oracle
to
Q
that
mak
es
little
mistak
es,
w
e
can
solv
e
P
with
high
probabilit
y
.
Commen
t
:
(A
mplic
ation)
:
F
or
ev
ery
promise
problem
P
,
and
mac
hine
M
that
satises:
for
ev
ery
x

P
r
omise(P
)
:
P
r
ob[M
(x)
=
P
(x)]
>


If
on
input
x
that
is
in
P
r
omise(P
),
w
e
run
M
on
x,
O
(n)
times,
then
the
ma
jorit
y
of
the
results
will
equal
P
(x)
with
probabilit
y
greater
than

 
 n
.
This
w
e
pro
v
ed
when
w
e
talk
ed
ab
out
B
P
P
,
and
the
pro
of
sta
ys
exactly
the
same,
using
Cherno
's
b
ound.
Note
that
w
e
do
not
care
if
mac
hine
M
has
an
oracle
or
not,
and
if
so
ho
w
this
oracle
op
erates,
as
long
as
dieren
t
runs
of
M
are
indep
enden
t.
Prop
osition
0..
Given
a
pr
oblem
P
and
a
pr
omise
pr
oblem
Q,
such
that
P
Co
ok
r
e
duc
es
to
Q,
if
we
have
a
pr
ob
abilistic
machine
Q
0
that
satises:
for
every
x

P
r
omise(Q)
:
P
r
ob[Q
0
(x)
=
Q(x)]
>


then
for
every
p
olynomial
p(),
we
have
a
pr
ob
abilistic
p
olynomial
time
machine
M
that
uses
an
or
acle
to
Q
0
,
and
satises:
P
r
ob[M
Q
0
(y
)
is
a
solution
of
P
on
input
y
]
>

 
 p(jy
j)
Pro
of:
W
e
start
b
y
noticing
that
since
the
reduction
from
P
to
Q
is
p
olynomial,
there
exists
a
p
olynomial
q
(),
suc
h
that
the
oracle
Q
is
called
less
than
q
(jy
j)
times.
Since
w
e
use
Q
0
and
not
Q
as
an
oracle,
w
e
ha
v
e
a
probabilit
y
of
error.
If
eac
h
one
of
these
calls
had
a
probabilit
y
of
error
less
than
:

q
(jy
j)


 p(jy
j)
,
then
b
y
using
the
union
b
ound
w
e
w
ould
get
that
the
probabilit
y
that
at
least
one
of
the
oracle
calls
w
as
incorrect
is
less
than

 p(jy
j)
.
The
probabilit
y
of
M
b
eing
correct,
is
at
least
the
probabilit
y
that
all
oracle
calls
are
correct,
therefore
in
this
case
it
is
greater
than

 
 p(jy
j)
.
Using
the
commen
t
ab
out
amplication,
w
e
can
amplify
the
probabilit
y
of
success
of
eac
h
oracle
call
to

 
q
(jy
j)

 p(jy
j)
,
b
y
calling
it
O
(p(jy
j)

log
q
(jy
j))
n
um
b
er
of
times,
whic
h
is
p
olynomial
in
the
size
of
the
input.
In
conclusion,
all
w
e
ha
v
e
to
do
is
sho
w
that
w
e
can
solv
e
Gap

#S
AT
with
a
probabilit
y
of
error
<


.
Then,
w
e
sho
w
ed
that
w
e
can
nd
a
solution
to
#S
AT
,
that
is
v
ery
close
to
the
real
solution
(S
tr
ong
R
ang
e
p
(#S
AT
)),
with
a
v
ery
high
probabilit
y
of
success.
0..
Probabilistic
Co
ok
Reduction
In
the
next
sections,
w
e
extensiv
ely
use
the
notion
of
probabilistic
reduction.
Therefore,
w
e'll
dene
it
formally
,
and
pro
v
e
some
of
it's
prop
erties.
Denition
0.
(Probabilistic
Co
ok
Reduction)
:
Given
pr
omise
pr
oblems
P
and
Q,
we
say
that
ther
e
is
a
Probabilistic
Co
ok
Reduction
fr
om
P
to
Q
denote
d
P

R
Q,
if
ther
e
is
a
pr
ob
abilistic
p
olynomial
time
or
acle
machine
M
that
uses
Q
as
an
or
acle,
and
satises:
for
every
x

P
r
omise(P
)
:
P
r
ob[M
Q
(x)
=
P
(x)]
>


wher
e
M
Q
(x)
denotes
then
op
er
ation
of
machine
M
on
input
x
when
given
or
acle
ac
c
ess
to
Q.
Whenever
a
query
to
Q
satises
the
pr
omise
of
Q,
the
answer
is
c
orr
e
ct,
but
when
the
query
violates
the
pr
omise
the
answer
may
b
e
arbitr
ary.

0..
HO
W
CLOSE
IS
#P
TO
N
P
?

Notice
that
in
the
denition,
the
oracle
has
no
probabilit
y
of
error.
W
e
no
w
sho
w
that
this
restriction
do
es
not
matter,
and
w
e
can
do
the
same
ev
en
if
the
oracle
is
implemen
ted
with
b
ounded
probabilit
y
of
error.
Prop
osition
0..
If
P
pr
ob
abilistic
al
ly
Co
ok
r
e
duc
es
to
Q,
and
we
have
a
pr
ob
abilistic
machine
Q
0
that
satises
for
every
x

P
r
omise(Q)
:
P
r
ob[Q
0
(x)
=
Q(x)]
>


then
we
have
a
pr
ob
abilistic
p
olynomial
time
or
acle
machine
M
that
uses
Q
0
as
an
or
acle,
and
satises
:
for
every
y

P
r
omise(P
)
:
P
r
ob[M
Q
0
(y
)
=
P
(y
)]
>


Pro
of:
By
the
denition
of
a
probabilistic
Co
ok
reduction,
w
e
ha
v
e
a
probabilistic
p
olynomial
time
oracle
mac
hine
N
that
satises:
for
ev
ery
y

P
r
omise(P
)
:
P
r
ob[N
Q
(y
)
=
P
(y
)]
>


Where
w
e
c
hanged


to


,
using
the
commen
t
ab
out
amplication.
Mac
hine
N
runs
in
p
olynomial
time,
therefore
it
calls
the
oracle
a
p
olynomial
p(jy
j)
n
um
b
er
of
times.
W
e
can
assume
Q
0
to
b
e
correct
with
a
probabilit
y
>

	


p(jy
j)
,
b
y
calling
it
eac
h
time
instead
of
just
once,
O
(log
(p(jy
j)))
times,
and
taking
the
ma
jorit
y
.
Using
the
union
b
ound,
the
probabilit
y
that
all
oracle
calls
(to
this
mo
died
Q
0
)
are
correct
is
greater
than

	
.
When
all
oracle
calls
are
correct,
mac
hine
N
returns
the
correct
result.
Therefore
with
proba-
bilit
y
greater
than




	
=


w
e
get
the
correct
result.
W
e
list
some
prop
erties
of
probabilistic
co
ok
reductions:

Deterministic
Co
ok
reduction
is
a
sp
ecial
case
(i.e.,
P

c
Q
=
)
P

R
Q).

T
r
ansitivity
:
P

R
Q

R
R
=
)
P

R
R
0..
Gap

#S
AT
Reduces
to
S
AT
Our
goal,
is
to
sho
w
that
w
e
can
appro
ximate
an
y
problem
in
#P
using
an
oracle
to
S
AT
.
So
far
w
e'v
e
reduced
the
problem
sev
eral
times,
and
got:
S
tr
ong
R
ang
e
p
oly
(#P
)

c
Gap

#S
AT
No
w
w
e'll
sho
w:
Gap

#S
AT

R
S
AT
And
using
the
ab
o
v
e
prop
erties
of
probabilistic
Co
ok
reductions,
this
will
mean
that
w
e
can
ap-
pro
ximate
#P
v
ery
closely
,
with
an
exp
onen
tially
small
probabilit
y
of
error.
Reminder:
Gap

#S
AT
is
the
promise
problem
on
input
pairs
(;
k
),
where

is
a
b
o
olean
form
ula,
and
k
is
a
natural
n
um
b
er.
Gap

#S
AT
=
(Gap

#S
AT
Y
;
Gap

#S
AT
N
),
where:
Gap

#S
AT
Y
=
f(;
k
)
:
#S
AT
()
>
k
g
Gap

#S
AT
N
=
f(;
k
)
:
#S
AT
()
<


k
g
Ho
w
do
w
e
approac
h
the
problem?
W
e
kno
w,
that
there
is
either
a
v
ery
large
or
a
v
ery
small
n
um
b
er
of
truth
assignmen
t
in
comparison
to
the
input
parameter
k
.
So
if
w
e
tak
e
a
random

k


LECTURE
0.
THE
COUNTING
CLASS
#P
fraction
of
the
assignmen
ts,
with
high
probabilit
y
in
the
rst
case
at
least
one
of
them
is
satisfying,
and
in
the
second,
none
are.
Assume
that
w
e
ha
v
e
a
w
a
y
of
restricting
our
form
ula
to
a
random
fraction
of
the
assignmen
ts
S
that
satises
:
eac
h
assignmen
t

is
in
the
set
with
probabilit
y

k
indep
enden
tly
of
all
other
assignmen
ts.
W
e
set

0
(
)
=
(
)
^
(

S
).
Then
w
e
simply
c
hec
k
satisabilit
y
of

0
.
First
notice:
P
r
ob
S
[
0

S
AT
]
=

 P
r
ob
S
[
s.t.
(
)
=

:


S
]
=

 (
k
 
k
)
#S
AT
()
Therefore:
If
#S
AT
()
>
k
then
P
r
ob
S
[
0

S
AT
]
>

 (
k
 
k
)
k


 
e

>


If
#S
AT
()
<


k
then
P
r
ob
S
[
0

S
AT
]
<

 (
k
 
k
)


k


 
e


<


The
problem
is,
w
e
don't
ha
v
e
an
ecien
t
pro
cedure
to
c
ho
ose
suc
h
a
random
S
.
So
w
e
w
eak
en
our
requiremen
ts,
instead
of
total
indep
endence,
w
e
require
only
pairwise
indep
endence.
Sp
ecically
,
w
e
use
the
follo
wing
to
ol:
Denition
0.
(Univ
ersal

Hashing)
:
A
family
of
functions,
H
n;m
,
mapping
f0;
g
n
to
f0;
g
m
is
c
al
le
d
Univ
ersal

if
for
a
uniformly
sele
cte
d
h
in
H
n;m
,
the
r
andom
variables
fh(e)g
ef0;g
n
ar
e
p
airwise
indep
endent
and
uniformly
distribute
d
over
f0;
g
m
.
That
is,
for
every
x
=
y

f0;
g
n
,
and
a;
b

f0;
g
m
,
P
r
ob
hH
n;m
[h(x)
=
a
&
h(y
)
=
b]
=
(
 m
)

An
ecien
t
construction
of
suc
h
families
is
required
to
ha
v
e
algorithms
for
selecting
and
ev
aluating
functions
in
the
family
.
That
is,
.
sele
cting:
There
exists
a
probabilistic
p
olynomial-time
algorithm
that
on
input
(
n
;

m
),
outputs
a
description
of
a
uniformly
selected
function
in
H
n;m
.
.
evaluating:
There
exists
a
p
olynomial-time
algorithm
that
on
input:
a
description
of
a
func-
tion
h

H
n;m
and
a
domain
elemen
t
x

f0;
g
m
outputs
the
v
alue
h(x).
A
p
opular
example
is
the
family
of
all
ane
transformations
from
f0;
g
n
to
f0;
g
m
.
That
is,
all
functions
of
the
form
h
A;b
(x)
=
Ax
+
b,
where
A
is
an
m-b
y-n
0-
matrix,
b
is
an
m-dimensional
0-
v
ector,
and
arithmetic
is
mo
dulo
.
Clearly
,
this
family
has
an
ecien
t
construction.
In
App
endix
A,
w
e
will
sho
w
that
this
family
is
Univ
ersal

.
Lemma
0..
(Lefto
v
er
Hash
Lemma):
L
et
H
n;m
b
e
a
family
of
Universal

Hash
functions
map-
ping
f0;
g
n
to
f0;
g
m
,
and
let

>
0.
L
et
S

f0;
g
n
b
e
arbitr
ary
pr
ovide
d
that
jS
j


 


m
.
Then:
P
r
ob
h
[jfe

S
:
h(e)
=
0
m
gj

(

)

jS
j

m
]
>

 
The
pro
of
of
this
lemma
app
ears
in
App
endix
B.
W
e
are
no
w
ready
to
construct
a
probabilistic
Co
ok
reduction
from
Gap

#S
AT
to
S
AT
,
using
a
Univ
ersal

family
of
functions.
Sp
ecically
w
e
will
use
the
family
of
ane
transformations.
Theorem
0.
Gap

#S
AT

R
S
AT
Pro
of:
W
e
construct
a
probabilistic
p
olynomial
time
mac
hine
M
whic
h
is
giv
en
oracle
access
to
S
AT
.
On
input
(;

m
),
where

has
n
v
ariables,
M
op
erates
as
follo
ws:

0..
HO
W
CLOSE
IS
#P
TO
N
P
?
	
.
Select
uniformly
h

H
n;m
=
fAne
transformations
from
f0;
g
n
to
f0;
g
m
g.
The
function
h
is
represen
ted
b
y
a
f0;
g
matrix
A
mn
=
(a
i;j
)
i=;:::;m
j
=;:::
;n
and
a
f0;
g
v
ector
b
=
(b
i
)
i=;:::
;m
.
.
W
e
construct
a
form
ula
 
h
,
on
v
ariables
x

;
:::;
x
n
;
y

;
:::;
y
t
,
so
that
for
ev
ery
x

f0;
g
n
h(x)
=
0
m
i
there
exists
an
assignmen
t
to
the
y
i
's
so
that
 
h
(x

;
:::;
x
n
;
y

;
:::;
y
t
)
is
true.
F
ur-
thermore,
in
case
h(x)
=
0
m
,
there
is
a
unique
assignmen
t
to
the
y
i
's
so
that
 
h
(x

;
:::;
x
n
;
y

;
:::;
y
t
)
is
true.
The
construction
of
 
h
can
b
e
presen
ted
in
t
w
o
w
a
ys.
In
the
abstract
w
a
y
,
w
e
just
observ
e
that
applying
the
standard
Co
ok-reduction
to
the
assertion
h(x)
=
0
m
,
results
in
the
desired
form
ula.
(The
claimed
prop
erties
ha
v
e
to
b
e
v
eried
indeed.)
A
more
concrete
w
a
y
is
to
start
b
y
the
follo
wing
observ
ations
h(x

;
:
:
:
x
n
)
=
0
m
m
V
m
i=

P
n
j
=
a
i;j
x
j

b
i
(mo
d
)

m
V
m
i=

(b
i

)

L
n
j
=
(a
i;j
^
x
j
)

In
tro
ducing
auxiliary
v
ariables,
as
in
the
construction
of
the
standard
reduction
from
Circuit{
Satisabilit
y
to
SA
T,
w
e
obtain
the
desired
form
ula
 
h
.
F
or
example,
in
tro
ducing
v
ariables
y

;
:::;
y
n
;
y
;
;
:::;
y
m;n
,
the
ab
o
v
e
form
ula
is
satised
for
a
particular
setting
of
the
x
i
's
i
the
follo
wing
form
ula
is
satisfyiable
for
these
x
i
's
(and
furthermore
for
a
unique
setting
of
the
y
i
's):
m
^
i=
(b
i



y
i
)
^
m
^
i=
0
@
y
i
=
n
M
j
=
y
i;j

A
^
m
^
i=
n
^
j
=
(
y
i;j
=
a
i;j
^
x
j
)
So
all
that
is
left
is
to
write
a
CNF
for
L
n
j
=
y
i;j
,
b
y
using
additional
auxiliary
v
ariables.
T
o
write
a
CNF
for
L
n
j
=
z
j
,
w
e
lo
ok
at
a
binary
tree
of
depth
`
def
=
log

n
whic
h
computes
the
X
OR
in
the
natrual
w
a
y
.
W
e
in
tro
duce
an
auxiliary
v
ariable
for
eac
h
in
ternal
no
de,
and
obtain
w
0;
^
` 
^
i=0

i
^
j
=
(w
i;j
=
w
i+;j
 

w
i+;j
)
^
n
^
j
=
(
w
`;j
=
z
j
)
.
Dene

0
=

^
 
h
.
Use
our
oracle
to
S
AT
on

0
,
and
return
the
result.
The
v
alidit
y
of
the
reduction
is
established
via
the
follo
wing
t
w
o
claims.
Claim
:
If
(;

m
)

Gap

#S
AT
Y
then

0

S
AT
with
probabilit
y
>


.
Claim
:
If
(;

m
)

Gap

#S
AT
N
then

0

S
AT
with
probabilit
y
<


.
Before
pro
ving
these
claims,
w
e
note
that
the
gap
in
the
probabilities
in
the
t
w
o
cases
(i.e.,
(;

m
)

Gap

#S
AT
Y
and
(;

m
)

Gap

#S
AT
N
)
can
b
e
\amplied"
to
obtain
the
desired
probabilities
(i.e.,

0

S
AT
with
probabilit
y
at
least
=
in
the
rst
case
and
at
most
=
in
the
second).
Pro
of
Claim
:
W
e
dene
S

def
=
fx
:
(x)
=
g.
Because
(;

m
)

Gap

#S
AT
Y
,
w
e
kno
w
that
jS

j
>



m
No
w:
P
r
ob
h
[
0

S
AT
]
=
P
r
ob
h
[fx
:
(x)
=

&
h(x)
=
0
m
g
=
;]
=
P
r
ob
h
[fx

S

:
h(x)
=
0
m
g
=
;]

P
r
ob
h
[jfx

S

:
h(x)
=
0
m
gj

(



)
jS

j

m
]
>



0
LECTURE
0.
THE
COUNTING
CLASS
#P
The
last
inequalit
y
is
an
application
of
the
Lefto
v
er
Hash
lemma,
setting

=


,
and
the
claim
follo
ws.

Pro
of
Claim
:
As
(;

m
)

Gap

#S
AT
N
,
w
e
ha
v
e
jS

j
<




m
.
P
r
ob
h
[
0

S
AT
]
=
P
r
ob
h
[fx

S

:
h(x)
=
0
m
g
=
;]
=
P
r
ob
h
[(
S
xS

fx
:
h(x)
=
0
m
g)
=
;]

P
xS

P
r
ob
h
[h(x)
=
0
m
]
<




m


 m
=


The
last
inequalit
y
uses
the
union
b
ound,
and
the
claim
follo
ws.

Com
bining
the
t
w
o
claims
(and
using
amplication),
the
theorem
follo
ws.
In
conclusion,
w
e
ha
v
e
sho
wn:
S
tr
ong
Appr
ox
p
oly
(#P
)

c
Gap

#S
AT

R
S
AT
Whic
h
is
what
w
e
w
an
ted.
0.
Reducing
to
uniq
ueS
AT
W
e'v
e
in
tro
duced
the
notion
of
pr
omise
pr
oblems
as
a
means
to
pro
v
e
that
w
e
can
appro
ximate
#S
AT
using
S
AT
.
But
promise
problems
are
in
teresting
b
y
their
o
wn
righ
t,
so
w
e
will
try
to
in
v
estigate
them
a
bit
more.
W
e'v
e
sho
wn
that
using
an
oracle
to
S
AT
w
e
can
solv
e
Gap

#S
AT
.
The
con
v
erse
is
also
true,
b
ecause
w
e'v
e
sho
wn
w
e
can
appro
ximate
(deterministically)
#S
AT
using
Gap

#S
AT
,
so
all
w
e
ha
v
e
to
do
is
appro
ximate
w
ell
enough,
to
dieren
tiate
0
from
p
ositiv
e
results,
and
th
us,
solv
e
S
AT
.
W
e
will
try
to
rene
this
result,
b
y
sho
wing
that
a
more
restricted
v
ersion
of
Gap

#S
AT
is
enough
to
solv
e
S
AT
(and
ev
en
appro
ximate
#S
AT
).
Denition
0.
Gap

#S
AT
0
is
the
pr
omise
pr
oblem
on
input
p
airs
(;
k
)
dene
d
by:
Gap

#S
AT
0
Y
=
f(;
k
)
:
k
<
#S
AT
()
<
k
g
Gap

#S
AT
0
N
=
f(;
k
)
:
#S
AT
()
<


k
g
Claim
0..
S
AT
Co
ok
r
e
duc
es
to
Gap

#S
AT
0
Pro
of:
Giv
en
,
rst
w
e
will
create
form
ula

0
,
s.t.
#S
AT
(
0
)
=


#S
AT
().
T
ak
e

v
ariables
fx

;
x

;
x

;
x

g
not
app
earing
in
.
and
dene:
 
=
(x

_
x

_
x

_
x

)

0
=

^
 
Observ
e
that
#S
AT
( 
)
=
,
and
since
the
v
ariables
of
 
do
not
app
ear
in
,
the
ab
o
v
e
equalit
y
holds.
So
w
e
kno
w
that
:
#S
AT
(
0
)


(
)


S
AT
#S
AT
(
0
)
=
0
(
)


S
AT
F
or
ev
ery
0

i

jV
ar
iabl
es(
0
)j,
w
e
call
our
oracle:
Gap

#S
AT
0
(
0
;

i
).
W
e
claim
:
One
of
the
answ
ers
is
Y
E
S
i


S
AT
.

0..
REDUCING
TO
U
N
I
QU
E
S
AT


Supp
ose
that


S
AT
.
Then
#S
AT
(
0
)
=
0
<


k
for
all
k
>
0,
therefore
for
all
i,
(
0
;

i
)

Gap

#S
AT
0
N
,
so
w
e
are
promised
to
alw
a
ys
get
a
N
O
answ
er.

Supp
ose


S
AT
,
so
as
w
e
sho
w
ed,
#S
AT
(
0
)

.
Therefore,
log

(#S
AT
(
0
))

log

()
>
.
There
exists
an
in
teger
i

0
s.t.
i
<
log

(#S
AT
(
0
))
 
<
i
+

+

i+
<
#S
AT
(
0
)
<

i+
+



i
<
#S
AT
(
0
)
<



i
And
for
that
i,
w
e
are
guaran
teed
to
get
a
Y
E
S
answ
er.
The
reader
ma
y
w
onder
wh
y
w
e
imp
osed
this
extra
restriction
on
Gap

#S
AT
.
W
e
w
an
t
to
sho
w
that
w
e
can
solv
e
S
AT
using
w
eak
oracles.
F
or
example
Gap

#S
AT
0
is
a
w
eak
oracle.
But
w
e
wish
to
con
tin
ue
in
our
reductions,
and
our
next
step
is:
Denition
0.
f
ew
S
AT
is
the
pr
omise
pr
oblem
dene
d
by:
f
ew
S
AT
Y
=
f
:


#S
AT
()
<
00g

S
AT
f
ew
S
AT
N
=
f
:
#S
AT
()
=
0g
=
S
AT
Prop
osition
0..
Gap

#S
AT
0
pr
ob
abilistic
al
ly
Co
ok
r
e
duc
es
to
f
ew
S
AT
Pro
of:
W
e
will
use
the
same
reduction
w
e
used
when
pro
ving
Gap

#S
AT

R
S
AT
,
except
w
e
no
w
ha
v
e
Gap

#S
AT
0
.
Recall,
w
e
uniformly
select
h

H
n;m
,
and
construct

0
(x)
=
(x)
^
(h(x)
=
0
m
).
W
e
mak
e
analogous
claims
to
the
ones
stated
in
the
former
pro
of:

claim

:
If
(;

m
)

Gap

#S
AT
0
Y
then

0

f
ew
S
AT
with
probabilit
y
>


.

claim

:
If
(;

m
)

Gap

#S
AT
0
N
then

0

f
ew
S
AT
with
probabilit
y
<


.
.
Since
(;

m
)

Gap

#S
AT
0
Y
,
w
e
ha
v
e:



m
<
jS

def
=
fx
:

0
(x)
=
gj
<



m
So
no
w:
P
r
ob
h
[
0

f
ew
S
AT
]
=
P
r
ob
h
[0
<
jfx
:
(x)
=

&
h(x)
=
0
m
gj
<
00]
=
P
r
ob
h
[0
<
jfx

S

:
h(x)
=
0
m
gj
<
00]

P
r
ob
h
[(
 

)


<
jfx

S

:
h(x)
=
0
m
gj
<
(
+


)

]

P
r
ob
h
[jfx

S

:
h(x)
=
0
m
gj

(



)
jS

j

m
]
>


.
In
the
original
pro
of
w
e
sho
w
ed:
if
(;

m
)

Gap

#S
AT
N
then

0
is
not
satisable
with
prob-
abilit
y
greater
than


.
Notice
:Gap

#S
AT
N
=
Gap

#S
AT
0
N
,
so
if
(;

m
)

Gap

#S
AT
0
N
then

0
is
not
satisable
with
probabilit
y
greater
than


,
and
in
that
case,
it's
in
f
ew
S
AT
N
,
so
w
e
are
guaran
teed
to
get
a
N
O
answ
er.


LECTURE
0.
THE
COUNTING
CLASS
#P
As
a
last
step
in
this
endless
crusade
to
understand
the
complexit
y
of
S
AT
promise
problems,
w
e
will
sho
w
that
the
w
eak
est
S
AT
related
promise
problem,
is
in
fact
as
strong
as
the
others.
Denition
0.
uniq
ueS
AT
is
the
pr
omise
pr
oblem
on
input

dene
d
by:
uniq
ueS
AT
Y
=
f
:
#S
AT
()
=
g

S
AT
uniq
ueS
AT
N
=
f
:
#S
AT
()
=
0g
=
S
AT
Prop
osition
0..
f
ew
S
AT
Co
ok
r
e
duc
es
to
uniq
ueS
AT
Pro
of:
Giv
en
a
form
ula
,
w
e
w
an
t
to
solv
e
f
ew
S
AT
.
F
or
eac
h


i
<
00
w
e
construct
a
form
ula

i
,
s.t.
:



S
AT
)

i

S
AT
.


i
has
a
unique
satisfying
assignmen
t
if

has
exactly
i
satisfying
assignmen
ts.
If
w
e
can
do
this,
w
e
can
c
hec
k
all
these

i
's,
with
our
oracle
to
uniq
ueS
AT
.
If
all
of
them
are
N
O
,
then
w
e
return
N
O
,
otherwise
w
e
answ
er
Y
E
S
.
This
is
correct
b
ecause
if
0
<
k
def
=
#S
AT
()
<
00,
then

k
has
exactly
one
satisfying
assignmen
t,
and
therefore
uniq
ueS
AT
returns
Y
E
S
on

k
.
Also,
if


S
AT
,
then
all
for
all
i
:

i

uniq
ueS
AT
N
,
so
all
the
answ
ers
m
ust
b
e
N
O
.
All
that
is
left
is
to
construct

i
:
W
e
create
i
copies
of
,
eac
h
on
a
separate
set
of
v
ariables:
 
i
=
^
i
j
=
(x
j

;
:
:
:
;
x
j
n
)
First
notice,
that
if


S
AT
,
then
so
is
 
i
.
No
w
assume
#S
AT
()
=
i.
Ev
ery
satisfying
assignmen
t
of
 
i
,
corresp
onds
to
i
satisfying
assignmen
ts
of
.
But
w
e
w
an
t
to
force
them
to
b
e
dieren
t,
so
w
e
w
ould
require
that
the
assignmen
ts
are
dieren
t
and
add
this
requiremen
t
to
 
i
.
But
then,
w
e
will
ha
v
e
exactly
i!
satisfying
assignmen
ts
to
the
new
 
i
.
T
o
solv
e
this,
instead
of
just
requiring
that
they
are
dieren
t,
w
e
will
imp
ose
a
lexicographical
ordering
of
the
solutions,
whic
h
will
x
one
satisfying
assignmen
t
from
the
i!
p
ossible.

i
=
 
i
^
i 
^
j
=
(
~
x
j
<
l
ex
~
x
j
+
)
Just
for
the
hec
k
of
it,
w
e'll
list
all
the
reductions
in
order:
S
tr
ong
R
ang
e
p
oly
(#P
)

c
S
tr
ong
R
ang
e
p
oly
(#S
AT
)

c
W
eak
R
ang
e

(#S
AT
)

c
C
onstantR
ang
e

(#S
AT
)

c
Gap

#S
AT

R
S
AT

c
Gap

#S
AT
0

R
f
ew
S
AT

c
uniq
ueS
AT
Some
collapsing
giv
es
us:
S
tr
ong
R
ang
e
p
oly
(#P
)

c
Gap

#S
AT

R
uniq
ueS
AT

0..
REDUCING
TO
U
N
I
QU
E
S
AT

Bibliographic
Notes
The
coun
ting
class
#P
w
as
in
tro
duced
b
y
V
alian
t
[],
who
pro
v
ed
that
computing
the
p
ermanen
t
of
0-
matrices
is
#P
-complete.
V
alian
t's
pro
of
rst
establishes
the
#P
-hardness
of
computing
the
p
ermanen
t
of
in
teger
matrices
(the
en
tries
are
actually
restricted
to
f ;
0;
;
;
g),
and
next
reduces
the
computation
of
the
p
ermanen
t
of
in
teger
matrices
to
the
the
p
ermanen
t
of
0-
matrices.
A
de-constructed
v
ersion
of
V
alinat's
pro
of
can
b
e
found
in
[].
The
appro
ximation
pro
cedure
for
#P
is
due
to
Sto
c
kmey
er
[],
follo
wing
an
idea
of
Sipser
[].
Our
exp
osition
follo
ws
further
dev
elopmen
ts
in
the
area.
The
randomized
reduction
of
SA
T
to
uniqueSA
T
is
due
to
V
alian
t
and
V
azirani
[].
Again,
our
exp
osition
is
a
bit
dieren
t.
.
A.
Ben-Dor
and
S.
Halevi.
Zeo-One
P
ermanen
t
is
#P-Complete,
A
Simpler
Pro
of.
In
nd
Isr
ael
Symp.
on
The
ory
of
Computing
and
Systems
(ISTCS	),
IEEE
Computer
So
ciet
y
Press,
pages
0{,
		.
.
M.
Sipser.
A
Complexit
y
Theoretic
Approac
h
to
Randomness.
In
th
STOC,
pages
0{,
	.
.
L.
Sto
c
kmey
er.
The
Complexit
y
of
Appro
ximate
Coun
ting.
In
th
STOC,
pages
{,
	.
.
L.G.
V
alian
t.
The
Complexit
y
of
Computing
the
P
ermanen
t.
The
or
etic
al
Computer
Scienc
e,
V
ol.
,
pp.
	{0,
		.
.
L.G.
V
alian
t
and
V.V.
V
azirani.
NP
Is
as
Easy
as
Detecting
Unique
Solutions.
The
or
etic
al
Computer
Scienc
e,
V
ol.

(),
pages
{	,
	.
App
endix
A:
A
F
amily
of
Univ
ersal

Hash
F
unctions
In
this
app
endix
w
e
sho
w
that
the
family
of
ane
transformations
from
f0;
g
n
to
f0;
g
m
is
ecien
tly
constructible
and
is
Univ
ersal

.
.
sele
cting:
Simply
selecting
uniformly
and
indep
enden
tly
eac
h
bit
of
A
and
b,
will
output
a
uniformly
selected
ane
transformation.
This
runs
in
O
(nm
+
m)
time,
whic
h
is
p
olynomial
in
the
length
of
the
input.
.
evaluating:
Calculating
Ax
tak
es
O
(mn)
time,
and
the
addition
of
b
adds
O
(m)
time.
All
in
all,
p
olynomial
in
the
size
of
the
input.
Prop
osition:
The
family
of
ane
tr
ansformations
fr
om
f0;
g
n
to
f0;
g
m
is
Universal

.
Pro
of:
Giv
en
x

=
x


f0;
g
n
,
and
y

;
y


f0;
g
m
.
If
x

=
0
n
,
then
P
r
ob
A;b
[h(x

)
=
y

&
h(x

)
=
y

]
=
P
r
ob
A;b
[b
=
y

&
Ax

+
b
=
y

]
=
P
r
ob
A;b
[b
=
y

&
Ax

=
y

 y

]
=
P
r
ob
A
[Ax

=
y

 y

]

P
r
ob
b
[b
=
y

]
=

 m


 m
=
(
 m
)

Where
P
r
ob[Ax

=
y

 y

]
=

 m
,
b
ecause
for
a
giv
en
v
ector
x

=
0
n
,
a
uniformly
c
hosen
linear
transformation
A,
maps
x

uniformly
in
to
f0;
g
m
.
If
x

=
0
the
same
argumen
t
holds.
Assume


LECTURE
0.
THE
COUNTING
CLASS
#P
b
oth
are
dieren
t
than
0
m
.
Since
w
e
c
ho
ose
among
the
linear
transformations
uniformly
,
it
do
es
not
matter
in
what
base
w
e
represen
t
them.
Since
x

;
x

=
0,
and
they
are
b
oth
in
f0;
g
n
,
they
m
ust
b
e
linearly
indep
enden
t.
So
w
e
ma
y
assume
they
are
b
oth
base
v
ectors
in
the
represen
tation
of
A,
meaning
one
column
in
A
:
column
a

in
A
represen
ts
the
image
of
x

,
and
a
dieren
t
column
a

represen
ts
the
image
of
x

.
P
r
ob
A;b
[h(x

)
=
y

&
h(x

)
=
y

]
=
P
r
ob
A;b
[Ax

+
b
=
y

&
Ax

+
b
=
y

]
=
P
r
ob
a

;a

;b
[a

+
b
=
y

&
a

+
b
=
y

]
=
P
r
ob
a

;a

[a

=
y

 b
&
a

=
y

 b]
(for
ev
ery
b)
=
P
r
ob
a

[a

=
y

 b]

P
r
ob
a

[a

=
y

 b]
(for
ev
ery
b)
=

 m


 m
=
(
 m
)

App
endix
B:
Pro
of
of
Lefto
v
er
Hash
Lemma
In
this
app
endix,
w
e
pro
v
e
the
Lefto
v
er
Hash
Lemma
(Lemma
0..).
W
e
rst
restate
the
lemma.
The
Lefto
v
er
Hash
Lemma:
Let
H
n;m
b
e
a
family
of
Univ
ersal

Hash
functions
mapping
f0;
g
n
to
f0;
g
m
,
and
let

>
0.
Let
S

f0;
g
n
b
e
arbitrary
pro
vided
that
jS
j


 


m
.
Then:
P
r
ob
h
[jfe

S
:
h(e)
=
0
m
gj

(

)

jS
j

m
]
>

 
Pro
of:
W
e
dene
for
eac
h
e

f0;
g
n
a
random
v
ariable
X
e
:
X
e
=
(

h(e)
=
0
m
0
otherwise
F
or
eac
h
e

=
e


f0;
g
n
,
w
e
claim
that
X
e

;
X
e

are
sto
c
hastically
indep
enden
t,
b
ecause
they
are
functions
of
the
indep
enden
t
random
v
ariables
h(e

)
and
h(e

)
resp
ectiv
ely
.
That
is,
w
e
use
the
kno
wn
fact
b
y
whic
h
if
X
and
Y
are
indep
enden
t
random
v
ariables
then,
for
ev
ery
function
f
,
f
(X
)
and
f
(Y
)
are
also
indep
enden
t
random
v
ariables.
W
e
compute
:
E
(x
e
)
=
P
r
ob[X
e
=
]
=


m
V
AR
(X
e
)
=
P
r
ob[X
e
=
]

(
 P
r
ob[X
e
=
])
=


m
(
 

m
)
W
e
dene
a
new
random
v
ariable
Y
=
P
eS
X
e
.
In
other
w
ords
:
Y
=
jfe

S
:
h(e)
=
0
m
gj.
Since
the
X
e
's
are
pairwise
indep
enden
t
w
e
get:
E
(Y
)
=
P
eS
E
(X
e
)
=
jS
j

m
V
AR
(Y
)
=
P
eS
V
AR
(X
e
)
=
jS
j

m
(
 

m
)
=
(
 

m
)

E
(Y
)
W
e
will
no
w
use
the
Cheb
yc
hev
inequalit
y
to
pro
v
e:
P
r
ob[jfe

S
:
h(e)
=
0
m
gj

(

)

jS
j

m
]
=
P
r
ob[Y

(

)

E
(Y
)]
=
P
r
ob[jY
 E
Y
j



E
(Y
)]


 V
AR(Y
)
(E
(Y
))

=

 ( 

m
)E
(Y
)


(E
(Y
))

=

 ( 

m
)
m


jS
j


 

(
 

m
)
>

 

Lecture

In
teractiv
e
Pro
of
Systems
Notes
tak
en
b
y
Dann
y
Harnik,
Tzvik
a
Hartman
and
Hillel
Kugler
Summary:
W
e
in
tro
duce
the
notion
of
in
teractiv
e
pro
of
systems
and
the
complexit
y
class
IP,
emphasizing
the
role
of
randomness
and
in
teraction
in
this
mo
del.
The
concept
is
demonstrated
b
y
giving
an
in
teractiv
e
pro
of
system
for
the
graph
non-isomorphism
language.
W
e
discuss
the
p
o
w
er
of
the
class
IP
and
pro
v
e
that
co
N
P

I
P
.
W
e
discuss
issues
regarding
the
n
um
b
er
of
rounds
allo
w
ed
in
a
pro
of
system
and
in
tro
duce
the
class
AM
capturing
languages
recognized
b
y
Arth
ur-Merlin
games.
.
In
tro
duction
A
pro
of
is
a
w
a
y
of
con
vincing
a
part
y
of
a
certain
claim.
When
talking
ab
out
pro
ofs,
w
e
consider
t
w
o
parties:
the
pr
over
and
the
verier.
Giv
en
an
assertion,
the
pro
v
er's
goal
is
to
con
vince
the
v
erier
of
it's
v
alidit
y
,
whereas
the
v
erier's
ob
jectiv
e
is
to
accept
only
a
correct
assertion.
In
mathematics,
for
instance,
the
pro
v
er
pro
vides
a
xed
sequence
of
claims
and
the
v
erier
c
hec
ks
that
they
are
truthful
and
that
they
imply
the
theorem.
In
real
life,
ho
w
ev
er,
the
notion
of
a
pro
of
has
a
m
uc
h
wider
in
terpretation.
A
pro
of
is
a
pro
cess
rather
than
a
xed
ob
ject,
b
y
whic
h
the
v
alidit
y
of
the
assertion
is
established.
F
or
instance,
a
job
in
terview
is
a
pro
cess
in
whic
h
the
candidate
tries
to
con
vince
the
emplo
y
er
that
she
should
hire
him.
In
order
to
mak
e
the
righ
t
decision,
the
emplo
y
er
carries
out
an
in
teractiv
e
pro
cess.
Unlik
e
a
xed
set
of
questions,
in
an
in
terview
the
emplo
y
er
can
adapt
her
questions
according
to
the
answ
ers
of
the
candidate,
and
therefore
extract
more
information,
and
lead
to
a
b
etter
decision.
This
example
exhibits
the
p
o
w
er
of
a
pro
of
pro
cess
rather
than
a
xed
pro
of.
In
particular
it
sho
ws
the
b
enets
of
in
teraction
b
et
w
een
the
parties.
In
man
y
con
texts,
nding
a
pro
of
requires
creativit
y
and
originalit
y
,
and
therefore
attracts
most
of
the
atten
tion.
Ho
w
ev
er,
in
our
discussion
of
pro
of
systems,
w
e
will
fo
cus
on
the
task
of
the
v
erier
{
the
v
erication
pro
cess.
T
ypically
the
v
erication
pro
cedure
is
considered
to
b
e
relativ
ely
easy
while
nding
the
pro
of
is
considered
a
harder
task.
The
asymmetry
b
et
w
een
the
complexit
y
of
v
erication
and
nding
pro
ofs
is
captured
b
y
the
complexit
y
class
NP.
W
e
can
view
NP
as
a
pro
of
system,
where
the
only
restriction
is
on
the
complexit
y
of
the
v
erication
pro
cedure
(the
v
erication
pro
cedure
m
ust
tak
e
at
most
p
olynomial-time).
F
or
eac
h
language
L
NP
there
exists
a
p
olynomial-time
recognizable
relation
R
L
suc
h
that:
L
=
fx
:
	y
s.t.
(x;
y
)

R
L
g



LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
and
(x;
y
)

R
L
only
if
jy
j

pol
y
(jxj).
In
a
pro
of
system
for
an
NP
language
L,
a
pro
of
for
the
claim
\x

L"
consists
of
the
pro
v
er
sending
a
witness
y
,
and
the
v
erier
c
hec
king
in
p
olynomial-time
whether
(x;
y
)

R
L
.
Suc
h
a
witness
exists
only
if
the
claim
is
true,
hence,
only
true
assertions
can
b
e
pro
v
ed
b
y
this
system.
Note
that
there
is
no
restriction
on
the
time
complexit
y
of
nding
the
pro
of
(witness).
A
go
o
d
pro
of
system
m
ust
ha
v
e
the
follo
wing
prop
erties:
.
The
v
erier
strategy
is
ecien
t
(p
olynomial-time
in
the
NP
case).
.
Correctness
requiremen
ts:

Completeness
:
F
or
a
true
assertion,
there
is
a
con
vincing
pro
of
strategy
(in
the
case
of
NP,
if
x

L
then
a
witness
y
exists).

Soundness
:
F
or
a
false
assertion,
no
con
vincing
pro
of
strategy
exists
(in
the
case
of
NP,
if
x

L
then
no
witness
y
exists).
In
the
follo
wing
discussion
w
e
in
tro
duce
the
notion
of
inter
active
pr
o
ofs.
T
o
do
so,
w
e
generalize
the
requiremen
ts
from
a
pro
of
system,
adding
in
teraction
and
randomness.
Roughly
sp
eaking,
an
in
teractiv
e
pro
of
is
a
sequence
of
questions
and
answ
ers
b
et
w
een
the
parties.
The
v
erier
asks
the
pro
v
er
a
question

i
and
the
pro
v
er
answ
ers
with
message

i
.
A
t
the
end
of
the
in
teraction,
the
v
erier
decides
based
the
kno
wledge
he
acquired
in
the
pro
cess
whether
the
claim
is
true
or
false.
e
e
e
-


-

Pro
v
er
V
erier





t

t


.
The
Denition
of
IP
F
ollo
wing
the
ab
o
v
e
discussion
w
e
dene
Denition
.
(in
teractiv
e
pro
of
systems):
A
n
interactive
p
ro
of
system
for
a
language
L
is
a
two-p
arty
game
b
etwe
en
a
verier
and
a
pr
over
that
inter
act
on
a
c
ommon
input
in
a
way
satisfying
the
fol
lowing
pr
op
erties:

..
THE
DEFINITION
OF
IP

.
The
verier
str
ate
gy
is
a
pr
ob
abilistic
p
olynomial-time
pr
o
c
e
dur
e
(where
time
is
measured
in
terms
of
the
length
of
the
common
input).
.
Corr
e
ctness
r
e
quir
ements:

Completeness
:
Ther
e
exists
a
pr
over
str
ate
gy
P
,
such
that
for
every
x

L,
when
in-
ter
acting
on
the
c
ommon
input
x,
the
pr
over
P
c
onvinc
es
the
verier
with
pr
ob
ability
at
le
ast


.

Soundness
:
F
or
every
x

L,
when
inter
acting
on
the
c
ommon
input
x,
any
pr
over
str
ate
gy
P

c
onvinc
es
the
verier
with
pr
ob
ability
at
most


.
Note
that
the
pr
over
str
ate
gy
is
c
omputational
ly
unb
ounde
d.
Denition
.
(The
IP
Hierarc
h
y):
The
c
omplexity
class
IP
c
onsists
of
al
l
the
languages
having
an
inter
active
pr
o
of
system.
We
c
al
l
the
numb
er
of
messages
exchange
d
during
the
pr
oto
c
ol
b
etwe
en
the
two
p
arties,
the
numb
er
of
rounds
in
the
system.
F
or
every
inte
ger
function
r
(),
the
c
omplexity
class
IP(r
())
c
onsists
of
al
l
the
languages
that
have
an
inter
active
pr
o
of
system
in
which,
on
c
ommon
input
x,
at
most
r
(jxj)
r
ounds
ar
e
use
d.
F
or
a
set
of
inte
ger
functions
R
,
we
denote
I
P
(R
)
=
[
r
R
I
P
(r
())
..
Commen
ts

Clearly
,
N
P

I
P
(actually
,
N
P

I
P
()).
Also,
B
P
P
=
I
P
(0).

The
n
um
b
er
of
rounds
in
IP
cannot
b
e
more
than
a
p
olynomial
in
the
length
of
the
common
input,
since
the
v
erier
strategy
m
ust
run
in
p
olynomial-time.
Therefore,
if
w
e
denote
b
y
poly
the
set
of
all
in
teger
p
olynomial
functions,
then
I
P
=
I
P
(poly
).

The
requiremen
t
for
completeness,
can
b
e
mo
died
to
require
p
erfect
completeness
(accep-
tance
probabilit
y
).
In
other
w
ords,
if
x

L,
the
pro
v
er
can
alw
a
ys
con
vince
the
v
erier.
These
t
w
o
denitions
are
equiv
alen
t.
Unlik
e
this,
if
w
e
require
p
erfect
soundness,
in
teractiv
e
pro
of
systems
collapse
to
NP-pro
of
systems.
These
results
will
b
e
sho
wn
in
Section
..

Muc
h
lik
e
in
the
denition
of
the
complexit
y
class
BPP,
the
probabilities


and


in
the
completeness
and
soundness
requiremen
ts
can
b
e
replaced
with
probabilities
as
extreme
as

 
 p()
and

 p()
,
for
an
y
p
olynomial
p().
In
other
w
ords
the
follo
wing
claim
holds:
Claim
..
A
ny
language
that
has
an
inter
active
pr
o
of
system,
has
one
that
achieves
err
or
pr
ob
ability
of
at
most

 p()
for
any
p
olynomial
p().
Pro
of:
W
e
rep
eat
the
pro
of
system
sequen
tially
for
k
times,
and
tak
e
a
ma
jorit
y
v
ote.
Denote
b
y
z
the
n
um
b
er
of
accepting
v
otes.
If
the
assertion
holds,
then
z
is
the
sum
of
k
indep
enden
t
Bernoulli
trials
with
probabilit
y
of
success
at
least


.
An
error
in
the
new
proto
col
happ
ens
if
z
<


k
.


LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
Using
Cherno
's
Bound
:
Pr
[z
<
(
 
)E
(z
)]
<
e
 

E
(z
)

W
e
c
ho
ose
k
=
O
(p())
and

=


and
note
that
E
(z
)
=


k
(so
that





=


)
to
get:
Pr

z
<



k

<

 p()
The
same
argumen
t
holds
for
the
soundness
error
(as
due
to
the
sequen
tial
nature
of
the
in
teraction
w
e
can
assert
that
in
eac
h
of
the
k
iterations,
for
an
y
history
of
prior
in
teractions,
the
success
probabilit
y
of
an
y
c
heating
strategy
is
b
ounded
b
y
=).
The
pro
of
ab
o
v
e
uses
sequen
tial
rep
etition
of
the
proto
col
to
amplify
the
probabilities.
This
suces
for
sho
wing
that
the
class
IP
is
in
v
arian
t
under
the
v
arious
denitions
discussed.
Ho
w
ev
er,
this
metho
d
increases
the
n
um
b
er
of
rounds
used
in
the
pro
of
system.
In
order
to
sho
w
the
in
v
ariance
of
the
class
IP(r
()),
an
analysis
of
the
parallel
rep
etition
v
ersion
should
b
e
giv
en.
(Suc
h
an
argumen
t
is
giv
en
in
App
endix
C.
of
[].)

In
tro
ducing
b
oth
in
teraction
and
randomness
in
the
IP
class
is
essen
tial.
{
By
adding
in
teraction
only
,
the
in
teractiv
e
pro
of
systems
collapse
to
NP-pro
of
systems.
Giv
en
an
in
teractiv
e
pro
of
system
for
a
pro
v
er
and
a
deterministic
v
erier,
w
e
construct
an
NP-
pro
of
system.
The
pro
v
er
can
predict
the
v
erier's
part
of
the
in
teraction
and
send
the
full
transcript
as
an
NP
witness.
The
v
erier
c
hec
ks
that
the
witness
is
a
v
alid
and
accepting
transcript
of
the
original
pro
of
system.
An
alternativ
e
argumen
t
uses
the
fact
that
in
teractiv
e
pro
of
systems
with
p
erfect
soundness
are
equiv
alen
t
to
NP-pro
of
systems
(and
the
fact
that
a
deterministic
v
erier
necessarily
yields
p
erfect
soundness).
{
By
adding
randomness
only
,
w
e
get
a
pro
of
system
in
whic
h
the
pro
v
er
sends
a
witness
and
the
v
erier
can
run
a
BPP
algorithm
for
c
hec
king
its
v
alidit
y
.
W
e
obtain
a
class
IP()
(also
denoted
MA)
whic
h
seems
to
b
e
a
randomized
(and
p
erhaps
stronger)
v
ersion
of
NP.
..
Example
{
Graph
Non-Isomorphism
(GNI)
Tw
o
graphs
G

=
(V

;
E

)
and
G

=
(V

;
E

)
are
called
isomorphic
(denoted
G


=
G

)
if
there
exists
a
-
and
on
to
mapping

:
V

!
V

suc
h
that
(u;
v
)

E

,
(
(u);

(v
))

E

.
The
mapping

,
if
existing,
is
called
an
isomorphism
b
et
w
een
the
graphs.
If
no
suc
h
mapping
exists
then
the
graphs
are
non-isomorphic
(denoted
G



=
G

).
GNI
is
the
language
con
taining
all
pairs
of
non-isomorphic
graphs.
F
ormally
:
GN
I
=
f(G

;
G

)
:
G



=
G

g
An
in
teractiv
e
pro
of
system
for
GNI:

G

and
G

are
giv
en
as
input
to
the
v
erier
and
the
pro
v
er.
Assume
without
loss
of
generalit
y
that
V

=
V

=
f;
;
:::;
ng

The
v
erier
c
ho
oses
i

R
f;
g
and


R
S
n
(
S
n
is
the
group
of
all
p
erm
utations
on
f;
;
:::;
ng
).

..
THE
DEFINITION
OF
IP
	
He
applies
the
mapping

on
the
graph
G
i
to
obtain
a
graph
H
H
=
(f;
;
:::;
ng;
E
H
)
where
E
H
=
f(
(u);

(v
))
:
(u;
v
)

E
i
g
and
sends
the
graph
H
to
the
pro
v
er.

The
pro
v
er
sends
j

f;
g
to
the
v
erier.

The
v
erier
accepts
i
j
=
i.
Motiv
ation
:
if
the
input
graphs
are
non-isomorphic,
as
the
pro
v
er
claims,
then
the
pro
v
er
should
b
e
able
to
distinguish
(not
necessarily
b
y
an
ecien
t
algorithm)
isomorphic
copies
of
one
graph
from
isomorphic
copies
of
the
other
graph.
Ho
w
ev
er,
if
the
input
graphs
are
isomorphic,
then
a
random
isomorphic
cop
y
of
one
graph
is
distributed
iden
tically
to
a
random
isomorphic
cop
y
of
the
other
graph
and
therefore,
the
b
est
c
hoice
the
pro
v
er
could
mak
e
is
a
random
one.
This
fact
enables
the
v
erier
to
distinguish
b
et
w
een
the
t
w
o
cases.
F
ormally:
Claim
..
The
ab
ove
pr
oto
c
ol
is
an
inter
active
pr
o
of
system
for
GNI.
Commen
t:
W
e
sho
w
that
the
ab
o
v
e
proto
col
is
an
in
teractiv
e
pro
of
system
with
soundness
probabilit
y
at
most


rather
than


as
in
the
formal
denition.
Ho
w
ev
er,
this
is
equiv
alen
t
b
y
an
amplication
argumen
t
(see
Claim
..).
Pro
of:
W
e
ha
v
e
to
sho
w
that
the
ab
o
v
e
system
satises
the
t
w
o
prop
erties
in
the
denition
of
in
teractiv
e
pro
of
systems:

The
v
erier's
strategy
can
b
e
easily
implemen
ted
in
probabilistic
p
olynomial
time.
(The
pro
v
er's
complexit
y
is
un
b
ounded
and
indeed,
he
has
to
c
hec
k
isomorphism
b
et
w
een
t
w
o
graphs,
a
problem
not
kno
wn
to
b
e
solv
ed
in
probabilistic
p
olynomial
time.)

{
Completeness
:
In
case
G



=
G

,
ev
ery
graph
can
b
e
isomorphic
to
at
most
one
of
G

or
G

(otherwise,
the
existence
of
a
graph
isomorphic
to
b
oth
G

and
G

implies
G


=
G

).
It
follo
ws
that
the
pro
v
er
can
alw
a
ys
send
the
correct
j
(i.e.
a
j
suc
h
that
j
=
i),
since
H

=
G
i
and
H


=
G
 i
.
{
Soundness
:
In
case
G


=
G

w
e
sho
w
that
the
pro
v
er
con
vinces
the
v
erier
with
probabilit
y
at
most


(the
probabilit
y
ranges
o
v
er
all
the
p
ossible
coin
tosses
of
the
v
erier,
i.e.
the
c
hoice
of
i
and

).
Denote
b
y
H
the
graph
sen
t
b
y
the
v
erier.
G


=
G

implies
that
H
is
isomorphic
to
b
oth
G

and
G

.
F
or
k
=
,
let
S
G
k
=
f

S
n
j

G
k
=
H
g
This
means
that
when
c
ho
osing
i
=
k
,
the
v
erier
can
obtain
H
only
b
y
c
ho
osing


S
G
k
.
Assume


S
n
is
an
isomorphism
b
et
w
een
G

and
G

,
i.e.
G

=

G

.
F
or
ev
ery


S
G

it
follo
ws
that



S
G

(b
ecause


G

=

G

=
H
).
Therefore,

is
a
-
mapping
from
S
G

to
S
G

(since
S
n
is
a
group).
Similarly
,

 
is
a
-
mapping
from
S
G

to
S
G

.
Com
bining
the
t
w
o
argumen
ts
w
e
get
that
jS
G

j
=
jS
G

j.
Therefore,
giv
en
that
H
w
as
sen
t,
the
probabilit
y
that
the
v
erier
c
hose
i
=

is
equal
to
the
probabilit
y
of
the
c
hoice
i
=
.
It
follo
ws
that
for
ev
ery
decision
the
pro
v
er
mak
es
he
has
success
probabilit
y


and
therefore,
his
total
probabilit
y
of
success
is


.
The
ab
o
v
e
in
teractiv
e
pro
of
system
is
implemen
ted
with
only

rounds.
Therefore,
Corollary
.
GNI

IP().

0
LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
.
The
P
o
w
er
of
IP
W
e
ha
v
e
already
seen
that
NP
IP.
The
ab
o
v
e
example
suggests
that
the
p
o
w
er
of
IP
is
ev
en
greater.
Since
GNI
is
not
kno
wn
to
b
e
in
NP
w
e
conjecture
that
NP
IP
(strict
inclusion).
F
urthermore,
the
class
of
languages
ha
ving
in
teractiv
e
pro
of
systems
is
sho
wn
to
b
e
equiv
alen
t
to
the
p
o
w
erful
complexit
y
class
PSP
A
CE.
F
ormally
,
Theorem
.
I
P
=
P
S
P
AC
E
.
W
e
will
only
giv
e
a
partial
pro
of
of
the
theorem.
W
e'll
only
sho
w
that
co
N
P

I
P

P
S
P
AC
E
.
..
IP
is
con
tained
in
PSP
A
CE
W
e
start
b
y
pro
ving
the
less
in
teresting
direction
of
the
theorem
(i.e.,
IP

PSP
A
CE).
This
is
pro
v
en
b
y
sho
wing
that
(for
ev
ery
xed
v
erier),
an
optimal
pro
v
er
strategy
exists
and
can
b
e
implemen
ted
in
p
olynomial-space.
The
Optimal
Pro
v
er:
Giv
en
a
xed
v
erier
strategy
,
there
exists
an
optimal
pro
v
er
strategy;
that
is,
for
ev
ery
common
input
x,
the
optimal
strategy
has
the
highest
p
ossible
probabilit
y
of
con
vincing
the
v
erier.
Note
that
an
optimal
pro
v
er
strategy
is
w
ell-dened,
as
for
ev
ery
input
x
and
xed
pro
v
er
strategy
,
the
probabilit
y
that
the
prescrib
ed
v
erier
accepts
is
w
ell-dened
(and
the
n
um
b
er
of
pro
v
er's
strategies
for
input
x
is
nite).
A
more
explicit
w
a
y
of
arguing
the
existence
of
an
optimal
pro
v
er
strategy
yields
an
algorithm
for
computing
it.
W
e
rst
observ
e
that
giv
en
the
v
erier
strategy
and
the
v
erier's
coin
tosses,
w
e
can
sim
ulate
the
whole
in
teraction
and
it's
outcome
for
an
y
pro
v
er
strategy
.
No
w,
the
optimal
pro
v
er
strategy
ma
y
en
umerate
all
p
ossible
outcomes
of
the
v
erier's
coin
tosses,
and
coun
t
ho
w
man
y
times
eac
h
strategy
succeeds.
The
optimal
strategy
for
eac
h
input,
is
one
that
yields
the
highest
n
um
b
er
of
successes.
F
urthermore,
this
can
b
e
done
in
p
olynomial-space:
Claim
..
The
optimal
pr
over
str
ate
gy
c
an
b
e
c
ompute
d
in
p
olynomial-sp
ac
e.
Pro
of:
W
e
assume
without
loss
of
generalit
y
that
the
v
erier
tosses
all
his
coins
b
efore
the
in
teraction
b
egins.
W
e
also
assume
that
the
v
erier
pla
ys
rst.
Let

i
b
e
the
i
th
message
sen
t
b
y
the
v
erier
and

i
b
e
the
i
th
message
sen
t
b
y
the
pro
v
er.
Let
r
b
e
the
outcome
of
all
the
v
erier's
coin
tosses.
Let
R


;

;:::;
i 
;
i
b
e
the
set
of
all
r
's
(outcome
of
coin
tosses)
that
are
consisten
t
with
the
in
teraction


;


;
:::;

i 
;

i
.
Let
F
(

;


;
:::;

i 
;

i
)
b
e
the
probabilit
y
that
an
in
teraction
(b
et
w
een
the
optimal
pro
v
er
and
the
xed
v
erier)
b
eginning
with


;


;
:::;

i 
;

i
will
result
in
acceptance.
The
probabilit
y
is
tak
en
uniformly
o
v
er
the
v
erier's
relev
an
t
coin
tosses
(only
r
suc
h
that
r

R


;

;:::;
i 
;
i
).
Supp
ose
an
in
teraction
b
et
w
een
the
t
w
o
parties
consists
of


;


;
:::;

i 
;

i
and
it
is
no
w
the
pro
v
er's
turn
to
pla
y
.
Using
the
function
F
,
the
pro
v
er
can
nd
the
optimal
mo
v
e.
W
e
sho
w
that
a
p
olynomial-space
pro
v
er
can
recursiv
ely
compute
F
(

;


;
:::;

i 
;

i
).
F
urthermore,
in
the
pro
cess,
the
pro
v
er
nds
an

i
that
yields
this
probabilit
y
and
hence,
an

i
that
is
an
optimal
mo
v
e
for
the
pro
v
er.
The
b
est
c
hoice
for

i
is
one
that
giv
es
the
highest
exp
ected
v
alue
of
F
(

;


;
:::;

i
;

i+
)
o
v
er
all
of
the
p
ossiblities
of
v
erier's
next
message
(
i+
).
F
ormally
:
()
F
(

;


;
:::;

i 
;

i
)
=
max

i
E

i+
[F
(

;


;
:::;

i
;

i+
)]

..
THE
PO
WER
OF
IP

Let
V
(r
;


;
:::;

i
)
b
e
the
message

i+
that
the
v
erier
sends
after
tossing
coins
r
and
receiving
messages


;
:::;

i
from
the
pro
v
er.
The
probabilit
y
for
eac
h
p
ossible
message

i+
to
b
e
sen
t
b
y
after


;


;
:::;

i
is
the
p
ortion
of
p
ossible
coins
r

R


;

;:::;
i 
;
i
that
yield
the
message

i+
(i.e.

i+
=
V
(r
;


;
:::;

i
)).
This
yields
the
follo
wing
equation
for
the
exp
ected
probabilit
y
:
()
E

i+
[F
(

;


;
:::;

i
;

i+
)]
=

jR


;

;:::;
i
j
X
r
R


;

;:::;
i
F
(

;


;
:::;

i
;
V
(r
;


;
:::;

i
))
Com
bining
()
and
()
w
e
get
the
recursion
form
ula
F
(

;


;
:::;

i 
;

i
)
=
max

i

jR


;

;:::;
i
j
X
r
R


;

;:::;
i
F
(

;


;
:::;

i
;
V
(r
;


;
:::;

i
))
W
e
no
w
sho
w
ho
w
to
compute
the
function
F
in
p
olynomial-space:
F
or
eac
h
p
oten
tial

i
,
w
e
en
umerate
all
p
ossible
v
alues
of
r
.
F
or
eac
h
r
,
all
of
the
follo
wing
can
b
e
done
in
p
olynomial-space:

Chec
king
if
r

R


;

;:::;
i
b
y
sim
ulating
the
v
erier
in
the
rst
i
in
teractions
(when
giv
en
r
the
v
erier
strategy
is
p
olynomial).

Calculating

i+
=
V
(r
;


;
:::;

i
)
again
b
y
sim
ulating
the
v
erier.

Recursiv
ely
computing
F
(

;


;
:::;

i
;

i+
).
In
order
for
the
recursion
to
b
e
p
olynomial-space
computable,
w
e
need
to
sho
w
that
the
recursion
stops
after
p
olynomially
man
y
stages,
and
that
the
last
stage
can
b
e
computed
in
p
olynomial-space.
The
recursion
stops
when
reac
hing
a
full
transcript
of
the
pro
of
system.
In
suc
h
a
case
the
pro
v
er
can
en
umerate
r
and
nd
the
probabilit
y
of
acceptance
among
all
consisten
t
r
b
y
sim
ulating
the
v
erier.
Clearly
,
this
can
b
e
done
in
p
olynomial-space.
Also
the
depth
of
the
recursion
m
ust
b
e
at
most
p
olynomial,
whic
h
is
ob
viously
the
case
here,
since
it
is
b
ounded
b
y
the
n
um
b
er
of
rounds.
Using
p
olynomial-size
coun
ters,
w
e
can
sum
the
probabilities
for
all
consisten
t
r
,
and
nd
the
exp
ected
probabilit
y
for
eac
h

i
.
By
rep
eating
this
for
all
p
ossible

i
w
e
can
nd
one
that
maximizes
the
exp
ectation.
Altogether,
the
pro
v
er's
optimal
strategy
can
b
e
calculated
in
p
olynomial-space.
Note:
All
the
probabilities
are
tak
en
o
v
er
the
v
erier's
coin
tosses
(no
more
than
a
p
olynomial
n
um
b
er
of
coins).
This
enables
us
to
use
p
olynomial-size
memory
for
calculating
all
probabilities
with
exact
resolution
(b
y
represen
ting
them
as
rational
n
um
b
ers
{
storing
the
n
umerator
and
denominator
separately).
Corollary
.
IP

PSP
A
CE
Pro
of:
If
L

IP
then
there
exists
an
in
teractiv
e
pro
of
system
for
L
and
hence
there
exists
a
p
olynomial-space
optimal
pro
v
er
strategy
.
Giv
en
input
x
and
the
v
erier`s
coin
tosses,
w
e
can
sim
ulate
(in
p
olynomial-space)
the
in
teraction
b
et
w
een
the
optimal
pro
v
er
and
the
v
erier
and
determine
this
in
teraction's
outcome.
W
e
en
umerate
o
v
er
all
the
p
ossible
v
erier's
coin
tosses
and
accept
only
if
more
than


of
the
outcomes
are
accepting.
Clearly
,
w
e
accept
if
and
only
if
x

L
and
this
can
b
e
implemen
ted
in
p
olynomial-space.


LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
..
coNP
is
con
tained
in
IP
As
men
tioned
ab
o
v
e,
w
e
will
not
pro
v
e
that
P
S
P
AC
E

I
P
.
Instead,
w
e
pro
v
e
a
w
eak
er
theorem
(i.e.,
co
N
P

I
P
),
whic
h
b
y
itself
is
already
v
ery
in
teresting.
The
pro
of
of
the
w
eak
er
theorem
presen
ts
all
but
one
ingredian
t
of
the
pro
of
P
S
P
AC
E

I
P
(and
the
missing
ingredian
t
is
less
in
teresting).
Theorem
.
co
N
P

I
P
Pro
of:
W
e
pro
v
e
the
theorem
b
y
presen
ting
an
in
teractiv
e
pro
of
system
for
the
coNP-complete
problem
S
AT
(the
same
metho
d
can
w
ork
for
the
problem
S
AT
as
w
ell).
S
AT
is
the
set
of
non-satisable
C
N
F
form
ulae:
Giv
en
a
C
N
F
form
ula
,
it
is
in
the
set
if
no
truth
assignmen
t
to
it's
v
ariables
satises
the
form
ula.
The
pro
of
uses
an
arithmetic
generalization
of
the
b
o
olean
problem,
whic
h
allo
ws
us
to
apply
algebraic
metho
ds
in
the
pro
of
system.
The
Arithmetization
of
a
Bo
olean
CNF
form
ula:
Giv
en
the
form
ula

with
v
ariables
x

;
:::;
x
n
w
e
p
erform
the
follo
wing
replacemen
ts:
Bo
olean
Arithmetic
T
 !
p
ositiv
e
in
tegers
F
 !
0
x
i
 !
x
i
x
i
 !
(
 x
i
)
_
 !
+
^
 !

(x

;
:::;
x
n
)
 !
(x

;
:::;
x
n
)
Ev
ery
b
o
olean
C
N
F
form
ula

is
transformed
in
to
a
m
ulti-v
ariable
p
olynomial
.
It
is
easy
to
see
that
for
ev
ery
assignmen
t
x

;
:::;
x
n
,
w
e
ha
v
e
(x

;
:::;
x
n
)
=
F
(
)
(x

;
:::;
x
n
)
=
0
Summing
o
v
er
all
p
ossible
assignmen
ts,
w
e
obtain
an
equation
for
the
non-satisabilit
y
of
:

is
unsatisable
(
)
X
x

=0;
:::
X
x
n
=0;
(x

;
:::;
x
n
)
=
0
Supp
ose

has
m
clauses
of
length
three
eac
h,
th
us
an
y
0-
assignmen
t
to
x

;
:::;
x
n
giv
es
(x

;
:::;
x
n
)


m
.
Since
there
are

n
dieren
t
assignmen
ts,
the
sum
ab
o
v
e
is
b
ounded
b
y

n


m
.
This
fact
allo
ws
us
to
mo
v
e
our
calculations
to
a
nite
eld,
b
y
c
ho
osing
a
prime
q
suc
h
that
q
>

n


m
,
and
w
orking
mo
dulo
this
prime.
Th
us
pro
ving
that

is
unsatisable
reduces
to
pro
ving
that
X
x

=0;
:::
X
x
n
=0;
(x

;
:::;
x
n
)

0
(mo
d
q
)
W
e
c
ho
ose
q
to
b
e
not
m
uc
h
larger
than

n


m
(this
is
p
ossible
due
to
the
densit
y
of
the
prime
n
um
b
ers).
Th
us,
w
e
obtain
that
all
calculations
o
v
er
the
eld
GF
(q
)
can
b
e
done
in
p
olynomial-
time
(in
the
input
length).
W
orking
o
v
er
a
nite
eld
will
later
help
us
in
the
task
of
uniformly
selecting
an
elemen
t
in
the
eld.
The
in
teractiv
e
pro
of
system
for
S
AT
:

..
THE
PO
WER
OF
IP


Both
sides
receiv
e
the
common
b
o
olean
form
ula
.
They
p
erform
the
arithmetization
pro
ce-
dure
and
obtain
.

The
pro
v
er
pic
ks
a
prime
q
suc
h
that
q
>

n


m
,
and
sends
q
to
the
v
erier.
The
v
erier
c
hec
ks
that
q
is
indeed
a
prime.
If
not
he
rejects.

The
v
erier
initializes
v
0
=
0.

The
follo
wing
is
p
erformed
n
times
(i
runs
from

to
n):
{
The
pro
v
er
sends
a
p
olynomial
P
i
()
of
degree
at
most
m
to
the
v
erier.
{
The
v
erier
c
hec
ks
whether
P
i
(0)
+
P
i
()

v
i 
(mo
d
q
)
and
that
the
p
olynomial's
degree
is
at
most
m.
If
not,
the
v
erier
rejects.
Otherwise,
he
uniformly
pic
ks
r
i

R
GF
(q
),
calculates
v
i
=
P
i
(r
i
)
and
sends
r
i
to
the
pro
v
er.

The
v
erier
accepts
if
(r

;
:::r
n
)

v
n
(mo
d
q
)
and
rejects
otherwise.
Motiv
ation:
The
pro
v
er
has
to
nd
a
sequence
of
p
olynomials
that
satises
a
n
um
b
er
of
re-
strictions.
The
restrictions
are
imp
osed
b
y
the
v
erier
in
the
follo
wing
in
teractiv
e
manner:
after
receiving
a
p
olynomial
from
the
pro
v
er,
the
v
erier
sets
a
new
restriction
for
the
next
p
olynomial
in
the
sequence.
These
restrictions
guaran
tee
that
if
the
claim
holds
(
is
unsatisable),
suc
h
a
sequence
can
alw
a
ys
b
e
found
(w
e
call
it
the
\Honest
pro
v
er
strategy").
Ho
w
ev
er,
if
the
claim
is
false,
an
y
pro
v
er
strategy
has
only
a
small
probabilit
y
of
nding
suc
h
a
sequence
(the
probabilit
y
is
tak
en
o
v
er
the
v
erier`s
coin
tosses).
This
yields
the
completeness
and
soundness
of
the
suggested
pro
of
system.
The
nature
of
these
restrictions
is
fully
claried
in
the
pro
of
of
soundness,
but
w
e
will
rst
sho
w
that
the
v
erier
strategy
is
ecien
t.
The
v
erier
strategy
is
ecien
t:
Most
steps
in
the
proto
col
are
calculations
of
p
olynomials
of
degree
m
in
n
v
ariables,
these
are
easily
calculated
in
p
olynomial-time.
The
transformation
to
an
arithmetic
eld
is
linear
in
the
form
ula's
length.
Chec
king
primalit
y
is
kno
wn
to
b
e
in
BPP
and
therefore
can
b
e
done
b
y
the
v
erier.
F
ur-
thermore,
it
can
b
e
sho
wn
that
primalit
y
testing
is
in
NP,
so
the
pro
v
er
can
send
the
v
erier
an
NP-witness
to
the
fact
that
q
is
a
prime,
and
the
v
erier
c
hec
ks
this
witness
in
p
olynomial-time.
Finally
,
pic
king
an
elemen
t
r

R
GF
(q
)
can
b
e
done
in
O
(log
q
)
coin
tosses,
that
is
p
olynomial
in
the
form
ula's
length.
The
honest
pro
v
er
strategy:
F
or
ev
ery
i
dene
the
p
olynomial:
P

i
(z
)
=
X
x
i+
=0;
:::
X
x
n
=0;
(r

;
:::;
r
i 
;
z
;
x
i+
;
:::;
x
n
)
Note
that
r

;
:::;
r
i 
are
constan
ts
set
b
y
the
v
erier
in
the
previous
stages
and
kno
wn
to
the
pro
v
er
at
the
i
th
stage,
and
z
is
the
p
olynomial's
v
ariable.
The
follo
wing
facts
are
eviden
t
ab
out
P

i
:

Calculating
P

i
ma
y
tak
e
exp
onen
tial-time,
but
this
is
no
obstacle
for
a
computationally
un
b
ounded
pro
v
er.

The
degree
of
P

i
is
at
most
m.
Since
there
are
at
most
m
clauses
in
,
the
highest
degree
of
an
y
one
v
ariable
is
m
(if
it
app
ears
in
all
clauses).


LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
Completeness
of
the
pro
of
system:
When
the
claim
holds,
the
honest
pro
v
er
alw
a
ys
succeeds
in
con
vincing
the
v
erier.
F
or
i
>
:
(:)
P

i
(0)
+
P

i
()
=
X
x
i
=0;
P

i
(x
i
)
=
()
X
x
i
=0;
:::
X
x
n
=0;
(r

;
:::;
r
i 
;
x
i
;
:::;
x
n
)
=
()
P

i 
(r
i 
)

()
v
i 
(mo
d
q
)
Equalit
y
()
is
due
to
the
denition
of
P

i
.
Equalit
y
()
is
due
to
the
denition
of
P

i 
.
Equalit
y
()
is
the
denition
of
v
i 
.
Also
for
i
=
,
since
the
claim
holds
w
e
ha
v
e:
P


(0)
+
P


()
=
X
x

=0;
P


(x

)
=
X
x

=0;
:::
X
x
n
=0;
(x

;
:::;
x
n
)

v
0
(mo
d
q
)
And
nally:
v
n
=
P

n
(r
n
)
=
(r

;
:::;
r
n
).
W
e
sho
w
ed
that
the
p
olynomials
of
the
honest
pro
v
er
pass
all
of
the
v
erier's
tests,
obtaining
p
erfect
completeness
of
the
pro
of
system.
Soundness
of
the
pro
of
system:
If
the
claim
is
false,
an
honest
pro
v
er
will
denitely
fail
after
sending
P


,
th
us
a
pro
v
er
m
ust
b
e
dishonest.
Roughly
sp
eaking,
w
e
will
sho
w
that
if
a
pro
v
er
is
dishonest
in
one
round,
then
with
high
probabilit
y
he
m
ust
b
e
dishonest
in
the
next
round
as
w
ell.
In
the
last
round,
his
dishonest
y
is
rev
ealed.
F
ormally:
Lemma
..
If
P

i
(0)
+
P

i
()

v
i 
(mo
d
q
)
then
either
the
verier
r
eje
cts
in
the
i
th
r
ound,
or
P

i
(r
i
)

v
i
(mo
d
q
)
with
pr
ob
ability
at
le
ast

 m
q
,
wher
e
the
pr
ob
ability
is
taken
over
the
verier's
choic
es
of
r
i
.
W
e
stress
that
P

i
is
the
p
olynomial
of
the
honest
pro
v
er
strategy
(as
dened
ab
o
v
e),
while
P
i
is
the
p
olynomial
actually
sen
t
b
y
the
pro
v
er
(v
i
is
set
using
P
i
).
Pro
of:
(of
lemma)
If
the
pro
v
er
sends
P
i
=
P

i
,
w
e
get:
P
i
(0)
+
P
i
()

P

i
(0)
+
P

i
()

v
i 
(mo
d
q
)
and
the
v
erier
rejects
immeadiately
.
Otherwise
the
pro
v
er
sends
P
i
=
P

i
.
W
e
assume
P
i
passed
the
v
erier's
test
(if
not
the
v
erier
rejects
and
w
e
are
done).
Since
P
i
and
P

i
are
of
degree
at
most
m,
there
are
at
most
m
c
hoices
of
r
i

GF
(q
)
suc
h
that
P

i
(r
i
)

P
i
(r
i
)
(mo
d
q
)
F
or
all
other
c
hoices:
P

i
(r
i
)

P
i
(r
i
)

v
i
(mo
d
q
)
Since
the
v
erier
pic
ks
r
i

R
GF
(q
),
w
e
get
P

i
(r
i
)

v
i
(mo
d
q
)
with
probabilit
y
at
most
m
q
,
Supp
ose
the
v
erier
do
es
not
reject
in
an
y
of
the
n
rounds.
Since
the
claim
is
false
(
is
satisable),
w
e
ha
v
e
P


(0)
+
P


()

v
0
(mo
d
q
).
Applying
alternately
the
lemma
and
the
follo
wing
equalit
y:
for
ev
ery
i


P

i 
(r
i 
)
=
P

i
(0)
+
P

i
()
(due
to
equation
.)
,
w
e
get
that
P

n
(r
n
)

v
n
(mo
d
q
)
with
probabilit
y
at
least
(
 m
q
)
n
.
But
P

n
(r
n
)
=
(r

;
:::;
r
n
)
so
the
v
erier's
last
test
fails
and
he
rejects.
Altogether
the
v
erier
fails
with
probabilit
y
(
 m
q
)
n
>

 nm
q
>


(b
y
the
c
hoice
of
q
).

..
PUBLIC-COIN
SYSTEMS
AND
THE
NUMBER
OF
R
OUNDS

.
Public-Coin
Systems
and
the
Num
b
er
of
Rounds
An
in
teresting
question
is
ho
w
the
p
o
w
er
of
in
teractiv
e
pro
of
systems
is
aected
b
y
the
n
um
b
er
of
rounds
allo
w
ed.
W
e
ha
v
e
already
seen
that
GNI
can
b
e
pro
v
ed
b
y
an
in
teractiv
e
pro
of
with

rounds.
Despite
this
example
of
a
coNP
language,
w
e
conjecture
that
coNP

IP(O
()).
T
ogether
with
the
previous
theorem
w
e
get:
Conjecture
.
I
P
(O
())

I
P
(pol
y
)
(strict
c
ontainment)
A
useful
to
ol
in
the
study
of
in
teractiv
e
pro
ofs,
is
the
public
coin
v
arian
t,
in
whic
h
the
v
erier
can
only
ask
random
questions.
Denition
.
(public-coin
in
teractiv
e
pro
ofs
{
AM):
Public
c
oin
pr
o
of
systems
(known
also
as
A
rthur-Merlin
games)
ar
e
a
sp
e
cial
c
ase
of
inter
active
pr
o
of
systems,
in
which,
at
e
ach
r
ound,
the
verier
c
an
only
toss
c
oins,
and
send
their
outc
ome
to
the
pr
over.
In
the
last
r
ound,
the
verier
de
cides
whether
to
ac
c
ept
or
r
eje
ct.
F
or
every
inte
ger
function
r
(),
the
c
omplexity
class
AM(r
())
c
onsists
of
al
l
the
languages
that
have
an
A
rthur-Merlin
pr
o
of
system
in
which,
on
c
ommon
input
x,
at
most
r
(jxj)
r
ounds
ar
e
use
d.
Denote
AM
=
AM
().
W
e
note
that
the
denition
of
AM
as
Arth
ur-Merlin
games
with
t
w
o
rounds
is
inconsisten
t
with
the
notation
IP=
IP(pol
y
)).
(Unfortunately
,
that's
what
is
found
in
the
literature.)
The
dierence
b
et
w
een
the
Arth
ur-Merlin
games
and
the
general
in
teractiv
e
pro
of
systems
can
b
e
view
ed
as
the
dierence
b
et
w
een
asking
tric
ky
questions,
v
ersus
asking
random
questions.
Surpris-
ingly
it
w
as
sho
wn
that
these
t
w
o
v
ersions
are
essen
tially
equiv
alen
t:
Theorem
.	
(Relating
I
P
()
to
AM()):
r
()
I
P
(r
())

AM
(r
()
+
)
The
follo
wing
theorem
sho
ws
that
p
o
w
er
of
AM(r
())
is
in
v
arian
t
under
a
linear
c
hange
in
the
n
um
b
er
of
rounds:
Theorem
.0
(Linear
Sp
eed-up
Theorem):
r
()


AM(r
())
=
AM(r
())
The
ab
o
v
e
t
w
o
theorems
are
quoted
without
pro
of.
Com
bining
them
w
e
get:
Corollary
.
r
()


I
P
(r
())
=
I
P
(r
()).
Corollary
.
(Collapse
of
constan
t-round
IP
to
t
w
o-round
AM):
I
P
(O
())
=
AM()


LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
.
P
erfect
Completeness
and
Soundness
In
the
denition
of
in
teractiv
e
pro
of
systems
w
e
require
the
existence
of
a
pro
v
er
strategy
that
for
x

L
con
vinces
the
v
erier
with
probabilit
y
at
least


(analogous
to
the
denition
of
the
complexit
y
class
BPP).
One
can
consider
a
denition
requiring
p
erfe
ct
c
ompleteness;
i.e.,
con
vincing
the
v
erier
with
probabilit
y

(analogous
to
coRP).
W
e
will
no
w
sho
w
that
the
denitions
are
equiv
alen
t.
Theorem
.
If
a
language
L
has
an
inter
active
pr
o
of
system
then
it
has
one
with
p
erfe
ct
c
ompleteness.
W
e
will
sho
w
that
giv
en
a
public
coin
pro
of
system
w
e
can
construct
a
p
erfect
completeness
public
coin
pro
of
system.
W
e
use
the
fact
that
public
coin
pro
of
systems
and
in
teractiv
e
pro
of
systems
are
equiv
alen
t
(see
Theorem
.	),
so
if
L
has
an
in
teractiv
e
pro
of
system
it
also
has
a
public
coin
pro
of
system.
W
e
dene:
AM
0
(r
())
=
fL
j
L
has
p
erfect
completeness
r
()
round
public
coin
pro
of
systemg
So
giv
en
an
in
teractiv
e
pro
of
system
w
e
create
a
public
coin
pro
of
system
and
using
the
follo
wing
lemma
con
v
ert
it
to
one
with
p
erfect
completeness.
Th
us,
the
ab
o
v
e
theorem
whic
h
refers
to
arbi-
trary
in
teractiv
e
pro
ofs
follo
ws
from
the
follo
wing
lemma
whic
h
refers
only
to
public-coin
in
teractiv
e
pro
ofs.
Lemma
..
If
L
has
a
public
c
oin
pr
o
of
system
then
it
has
one
with
p
erfe
ct
c
ompleteness
AM
(r
())

AM
0
(r
()
+
)
Pro
of:
Giv
en
an
Arth
ur-Merlin
pro
of
system,
w
e
construct
an
Arth
ur-Merlin
pro
of
system
with
p
erfect
completeness
and
one
more
round.
W
e
use
the
same
idea
as
in
the
pro
of
of
BPP

PH.
Assume,
without
loss
of
generalit
y
,
that
the
Arth
ur-Merlin
pro
of
system
consists
of
t
rounds,
and
that
Arth
ur
sends
the
same
n
um
b
er
of
coins
m
in
eac
h
round
(otherwise,
ignore
the
redundan
t
coins).
Also
assume
that
the
completeness
and
soundness
error
probabilities
of
the
pro
of
system
are
at
most

tm
.
This
is
obtained
using
amplication
(see
Section
..).
W
e
denote
the
messages
sen
t
b
y
Arth
ur
(the
v
erier)
r

;
:::;
r
t
and
the
messages
sen
t
b
y
Merlin
(the
pro
v
er)


;
:::;

t
.
Denote
b
y
hP
;
V
i
x
(r

;
:::;
r
t
)
the
outcome
of
the
game
on
common
input
x
b
et
w
een
the
optimal
pro
v
er
P
and
the
v
erier
V
in
whic
h
the
v
erier
uses
coins
r

;
:::;
r
t
:
hP
;
V
i
x
(r

;
:::;
r
t
)
=
0
if
the
v
erier
rejects
and
hP
;
V
i
x
(r

;
:::;
r
t
)
=

otherwise.
W
e
construct
a
new
pro
of
system
with
p
erfect
completeness,
in
whic
h
Arth
ur
and
Merlin
pla
y
tm
games
sim
ultaneously
.
Eac
h
game
is
lik
e
the
original
game
except
that
the
random
coins
are
shifted
b
y
a
xed
amoun
t.
The
tm
shifts
(one
for
eac
h
game)
are
sen
t
b
y
Merlin
in
an
additional
round
at
the
b
eginning.
Arth
ur
accepts
if
at
least
one
of
the
games
is
accepting.
F
ormally
,
w
e
add
an
additional
round
at
the
b
eginning
in
whic
h
Merlin
sends
the
shifts
S

;
:::;
S
tm
where
S
i
=
(S
i

;
:::;
S
i
t
);
S
i
j

f0;
g
m
for
ev
ery
i
b
et
w
een

and
tm.
Lik
e
in
the
original
pro
of
system
Arth
ur
sends
messages
r

;
:::;
r
t
,
where
r
i

R
f0;
g
m
.
F
or
game
i
and
round
j
,
Merlin
considers
the
random
coins
to
b
e
r
j

S
i
j
and
sends
as
a
message

i
j
where

i
j
is
computed
according
to
(r


S
i

;
:::;
r
j

S
i
j
).
The
en
tire
message
in
round
j
is


j
;
:::;

tm
j
.
A
t
the
end
of
the
proto
col
Arth
ur
accepts
if
at
least
one
out
of
the
tm
games
is
accepting.

..
PERFECT
COMPLETENESS
AND
SOUNDNESS

In
order
to
sho
w
p
erfect
completeness
w
e
will
sho
w
that
for
ev
ery
x

L
there
exist
S

;
:::;
S
tm
suc
h
that
for
all
r

;
:::r
t
at
least
one
of
the
games
is
accepting.
W
e
use
a
probabilistic
argumen
t
to
sho
w
that
the
complemen
tary
ev
en
t
o
ccurs
with
probabilit
y
strictly
smaller
than
.
Pr
S

;:::S
tm
"
	r

;
:::r
t
tm
^
i=
(hP
;
V
i
x
(r


S
i

;
:::;
r
t

S
i
t
)
=
0)
#

()
X
r

;:::r
t
f0;g
m
Pr
S

;:::S
tm
"
tm
^
i=
(hP
;
V
i
x
(r


S
i

;
:::;
r
t

S
i
t
)
=
0)
#

()

tm



tm

tm
<

Inequalit
y
()
is
obtained
using
the
union
b
ound.
Inequalit
y
()
is
due
to
the
fact
that
the
r
j

S
i
j
are
indep
enden
t
random
v
ariables
so
the
results
of
the
games
are
indep
enden
t,
and
that
the
optimal
pro
v
er
fails
to
con
vince
the
v
erier
on
a
true
assertion
with
probabilit
y
at
most

tm
.
W
e
still
ha
v
e
to
sho
w
that
the
pro
of
system
suggested
satises
the
soundness
requiremen
t.
W
e
sho
w
that
for
ev
ery
x

L
and
for
an
y
pro
v
er
strategy
P

and
c
hoices
of
shifts
S

;
:::;
S
tm
the
probabilit
y
that
one
or
more
of
the
tm
games
is
accepting
is
at
most


.
Pr
r

;:::r
t
"
tm
_
i=
(hP

;
V
i
x
(r


S
i

;
:::;
r
t

S
i
t
)
=
)
#

()
tm
X
i=
Pr
r

;:::;r
t
h
hP

;
V
i
x
(r


S
i

;
:::;
r
t

S
i
t
)
=

i

()
tm
X
i=

tm
=


Inequalit
y
()
is
obtained
using
the
union
b
ound.
Inequalit
y
()
is
due
to
the
fact
that
an
y
pro
v
er
has
probabilit
y
of
at
most

tm
of
success
for
a
single
game
(b
ecause
an
y
strategy
that
the
pro
v
er
can
pla
y
in
a
cop
y
of
the
parallel
game
can
b
e
pla
y
ed
in
a
single
game
as
w
ell).
Unlik
e
the
last
theorem,
requiring
p
erfe
ct
soundness
(i.e.
for
ev
ery
x

L
and
ev
ery
pro
v
er
strategy
P

,
the
v
erier
alw
a
ys
rejects
after
in
teracting
with
P

on
common
input
x)
reduces
the
mo
del
to
an
NP-pro
of
system
,
as
seen
in
the
follo
wing
prop
osition:
Prop
osition
..
If
a
language
L
has
an
inter
active
pr
o
of
system
with
p
erfe
ct
soundness
then
L

N
P
.
Pro
of:
Giv
en
an
in
teractiv
e
pro
of
system
with
p
erfect
soundness
w
e
construct
an
NP
pro
of
system.
In
case
x

L,
b
y
the
completeness
requiremen
t,
there
exists
an
accepting
transcript.
The
pro
v
er
nds
an
outcome
of
the
v
erier's
coin
tosses
that
giv
es
suc
h
a
transcript
and
sends
the
full
transcript
along
with
the
coin
tosses.
The
v
erier
c
hec
ks
in
p
olynomial
time
that
the
transcript
is
v
alid
and
accepting
and
if
so
-
accepts.
This
serv
es
as
an
NP-witness
to
the
fact
that
x

L.
If
x

L
then
due
to
the
p
erfect
soundness
requiremen
t,
no
outcome
of
v
erier's
coin
tosses
yields
an
accepting
transcript
and
therefore
there
are
no
NP-witnesses.


LECTURE
.
INTERA
CTIVE
PR
OOF
SYSTEMS
Bibliographic
Notes
In
teractiv
e
pro
of
systems
w
ere
in
tro
duced
b
y
Goldw
asser,
Micali
and
Rac
k
o
[],
with
the
explicit
ob
jectiv
e
of
capturing
the
most
general
notion
of
ecien
tly
v
eriable
pro
of
systems.
The
original
motiv
ation
w
as
the
in
tro
duction
of
zero-kno
wledge
pro
of
systems,
whic
h
in
turn
w
ere
supp
osed
to
pro
vide
(and
indeed
do
pro
vide)
a
p
o
w
erful
to
ol
for
the
design
of
complex
cryptographic
sc
hemes.
First
evidence
that
in
teractiv
e
pro
ofs
ma
y
b
e
more
p
o
w
erful
than
NP-pro
ofs
w
as
giv
en
b
y
Gol-
dreic
h,
Micali
and
Wigderson
[],
in
the
form
of
the
in
teractiv
e
pro
of
for
Graph
Non-Isomorphism
presen
ted
ab
o
v
e.
The
full
p
o
w
er
of
in
teractiv
e
pro
of
systems
w
as
disco
v
ed
b
y
Lund,
F
ortno
w,
Karlo,
Nisan,
and
Shamir
(in
[]
and
[]).
The
basic
tec
hnique
w
as
presen
ted
in
[]
(where
it
w
as
sho
wn
that
co
N
P

I
P
)
and
the
nal
result
(P
S
P
AC
E
=
I
P
)
in
[].
Our
presen
tation
follo
ws
[].
F
or
further
discussion
of
credits,
see
[].
Public-coin
in
teractiv
e
pro
ofs
(also
kno
wn
as
Arth
ur-Merlin
pro
ofs)
w
ere
in
tro
duced
b
y
Babai
[].
The
fact
that
these
restricted
in
teractiv
e
pro
ofs
are
as
p
o
w
erful
as
general
ones
w
as
pro
v
ed
b
y
Gold-
w
asser
and
Sipser
[].
The
linear
sp
eed-up
(in
n
um
b
er
of
rounds)
of
public-coin
in
teractiv
e
pro
ofs
w
as
sho
wn
b
y
Babai
and
Moran
[].
.
L.
Babai.
T
rading
Group
Theory
for
Randomness.
In
th
STOC,
pages
{	,
	.
.
L.
Babai
and
S.
Moran.
Arth
ur-Merlin
Games:
A
Randomized
Pro
of
System
and
a
Hierarc
h
y
of
Complexit
y
Classes.
JCSS,
V
ol.
,
pp.
{,
	.
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.
.
O.
Goldreic
h,
S.
Micali
and
A.
Wigderson.
Pro
ofs
that
Yield
Nothing
but
their
V
alidit
y
or
All
Languages
in
NP
Ha
v
e
Zero-Kno
wledge
Pro
of
Systems.
JA
CM,
V
ol.
,
No.
,
pages
	{	,
		.
Preliminary
v
ersion
in
th
F
OCS,
	.
.
S.
Goldw
asser,
S.
Micali
and
C.
Rac
k
o.
The
Kno
wledge
Complexit
y
of
In
teractiv
e
Pro
of
Systems.
SICOMP,
V
ol.
,
pages
{0,
		.
Preliminary
v
ersion
in
th
STOC,
	.
Earlier
v
ersions
date
to
	.
.
S.
Goldw
asser
and
M.
Sipser.
Priv
ate
Coins
v
ersus
Public
Coins
in
In
teractiv
e
Pro
of
Systems.
A
dvanc
es
in
Computing
R
ese
ar
ch:
a
r
ese
ar
ch
annual,
V
ol.

(Randomness
and
Computation,
S.
Micali,
ed.),
pages
{	0,
		.
Extended
abstract
in
th
STOC,
pages
	{,
	.
.
C.
Lund,
L.
F
ortno
w,
H.
Karlo,
and
N.
Nisan.
Algebraic
Metho
ds
for
In
teractiv
e
Pro
of
Systems.
JA
CM,
V
ol.
	,
No.
,
pages
	{,
		.
Preliminary
v
ersion
in
st
F
OCS,
		0.
.
A.
Shamir.
IP
=
PSP
A
CE.
JA
CM,
V
ol.
	,
No.
,
pages
	{,
		.
Preliminary
v
ersion
in
st
F
OCS,
		0.

Lecture

Probabilistically
Chec
k
able
Pro
of
Systems
Notes
tak
en
b
y
Alon
Rosen
and
V
ered
Rosen
Summary:
In
this
lecture
w
e
in
tro
duce
the
notion
of
Probabilistically
Chec
k
able
Pro
of
(PCP)
systems.
W
e
discuss
some
complexit
y
measures
in
v
olv
ed,
and
describ
e
the
class
of
languages
captured
b
y
corresp
onding
PCP
systems.
W
e
then
demonstrate
the
alter-
nativ
e
view
of
N
P
emerging
from
the
PCP
theorem,
and
use
it
in
order
to
pro
v
e
t
w
o
non-appro
ximabilit
y
results
for
the
problems
maxS
AT
and
maxC
LI
QU
E
.
.
In
tro
duction
Lo
osely
sp
eaking,
a
probabilistically
c
hec
k
able
pro
of
system
(PCP)
for
a
language
consists
of
a
probabilistic
p
olynomial-time
v
erier
ha
ving
direct
access
to
individual
bits
of
a
binary
string.
This
string
(called
oracle)
represen
ts
a
pro
of,
and
t
ypically
will
b
e
accessed
only
partially
b
y
the
v
erier.
Queries
to
the
oracle
are
p
ositions
on
the
bit
string
and
will
b
e
determined
b
y
the
v
erier's
input
and
coin
tosses
(p
oten
tially
,
they
migh
t
b
e
determined
b
y
answ
ers
to
previous
queries
as
w
ell).
The
v
erier
is
supp
osed
to
decide
whether
a
giv
en
input
b
elongs
to
the
language.
If
the
input
b
elongs
to
the
language,
the
requiremen
t
is
that
the
v
erier
will
alw
a
ys
accept
(i.e.
giv
en
access
to
an
adequate
oracle).
On
the
other
hand,
if
the
input
do
es
not
b
elong
to
the
language
then
the
v
erier
will
reject
with
probabilit
y
at
least


,
no
matter
whic
h
oracle
is
used.
One
can
view
PCP
systems
in
terms
of
in
teractiv
e
pro
of
systems.
That
is,
one
can
think
of
the
oracle
string
as
b
eing
the
pro
v
er
and
of
the
queries
as
b
eing
the
messages
sen
t
to
him
b
y
the
v
erier.
In
the
PCP
setting
ho
w
ev
er,
the
pro
v
er
is
considered
to
b
e
memoryless
and
th
us
cannot
adjust
his
answ
ers
based
on
previous
queries
p
osed
to
him.
A
more
app
ealing
in
terpretation
is
to
view
PCP
systems
as
a
p
ossible
w
a
y
of
generalizing
N
P
.
Instead
of
conducting
a
p
olynomial-time
computation
up
on
receiving
the
en
tire
pro
of
(as
in
the
case
of
N
P
),
the
v
erier
is
allo
w
ed
to
toss
coins
and
query
the
pro
of
only
at
lo
cations
of
his
c
hoice.
This
either
allo
ws
him
to
insp
ect
v
ery
long
pro
ofs
(lo
oking
at
no
more
than
p
olynomially
man
y
lo
cations),
or
alternativ
ely
,
lo
ok
at
v
ery
few
bits
of
a
p
ossible
pro
of.
Most
surprisingly
,
PCP
systems
ha
v
e
b
een
used
to
fully
c
haracterize
the
languages
in
N
P
.
This
c
haracterization
has
b
een
found
to
b
e
useful
in
connecting
the
hardness
in
v
olv
ed
in
the
ap-
pro
ximation
of
some
N
P
-har
d
problems
with
the
P
=
N
P
question.
In
other
w
ords,
v
ery
strong
	

0
LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS
non-appro
ximabilit
y
results
for
v
arious
classical
optimization
problems
ha
v
e
b
een
established
using
PCP
systems
for
N
P
languages.
.
The
Denition
..
The
basic
mo
del
In
the
denition
of
PCP
systems
w
e
mak
e
use
of
the
notion
of
a
probabilistic
oracle
mac
hine.
In
our
setting,
this
will
b
e
a
probabilistic
T
uring
mac
hine
whic
h,
in
addition
to
the
usual
features,
will
ha
v
e
direct
access
(coun
ted
as
a
single
step)
to
individual
bits
of
a
binary
string
(the
oracle).
>F
rom
no
w
on,
w
e
denote
b
y
M

(x)
the
output
of
mac
hine
M
on
input
x,
when
giv
en
suc
h
oracle
access
to
the
binary
string

.
Denition
.
(Probabilistically
Chec
k
able
Pro
ofs
-
PCP)
A
probabilistic
c
hec
k
able
pro
of
sys-
tem
for
a
language
L
is
a
pr
ob
abilistic
p
olynomial-time
or
acle
machine
(called
v
erier),
denote
d
M
,
satisfying

Completeness:
F
or
every
x

L
ther
e
exists
an
or
acle

x
such
that:
Pr
[M

x
(x)
=
]
=


Soundness:
F
or
every
x

L
and
every
or
acle

:
Pr
[M

(x)
=
]



wher
e
the
pr
ob
ability
is
taken
over
M
's
internal
c
oin
tosses.
..
Complexit
y
Measures
When
considering
a
randomized
oracle
mac
hine,
some
complexit
y
measures
other
than
time
ma
y
come
in
to
concern.
A
natural
thing
w
ould
b
e
to
coun
t
the
n
um
b
er
of
queries
made
b
y
the
v
erier.
This
n
um
b
er
determines
what
is
the
p
ortion
of
the
pro
of
b
eing
read
b
y
the
v
erier.
Another
concern
w
ould
b
e
to
coun
t
the
n
um
b
er
of
coins
tossed
b
y
the
randomized
oracle
mac
hine.
This
in
turn
determines
what
is
the
total
n
um
b
er
of
p
ossible
executions
of
the
v
erier
(once
an
oracle
is
xed).
It
turns
out
that
the
class
of
languages
captured
b
y
PCP
systems
v
aries
greatly
as
the
ab
o
v
e
men
tioned
resources
of
the
v
erier
are
c
hanged.
This
motiv
ates
a
quan
titativ
e
renemen
t
of
the
denition
of
PCP
systems
whic
h
captures
the
ab
o
v
e
discussed
concept.
Denition
.
(Complexit
y
Measures
for
PCP)
L
et
r
;
q
:
N
!
N
b
e
inte
ger
functions
(in
p
ar-
ticular
c
onstant).
The
c
omplexity
class
P
C
P
(r
();
q
())
c
onsists
of
languages
having
a
pr
ob
abilistic
che
ckable
pr
o
of
system
in
which
it
holds
that:

Randomness
Complexit
y:
On
input
x

f0;
g

,
the
verier
makes
at
most
r
(jxj)
c
oin
tosses.

Query
Complexit
y:
On
input
x

f0;
g

,
the
verier
makes
at
most
q
(jxj)
queries.
F
or
sets
of
inte
ger
functions
R
and
Q,
we
let
P
C
P
(R
;
Q)
def
=
[
r
R;q
Q
P
C
P
(r
();
q
())

..
THE
DEFINITION

In
particular,
w
e
denote
b
y
p
oly
the
set
of
all
in
teger
functions
b
ounded
b
y
a
p
olynomial
and
b
y
log
the
set
of
all
in
teger
functions
b
ounded
b
y
a
logarithmic
function
(e.g.
f

log
i
f
(n)
=
O
(log
n)).
F
rom
no
w
on,
whenev
er
referring
to
a
PCP
system,
w
e
will
also
sp
ecify
its
corresp
onding
complexit
y
measures.
..
Some
Observ
ations

The
denition
of
PCP
in
v
olv
es
binary
queries
to
the
oracle
(whic
h
is
itself
a
binary
string).
These
queries
sp
ecify
lo
cations
on
the
string
whose
binary
v
alues
are
the
answ
ers
to
the
corresp
onding
queries.
F
rom
no
w
on,
when
giv
en
a
query
q
to
an
oracle

the
corresp
onding
binary
answ
er
will
b
e
denoted

q
.
Note
that
an
oracle
string
can
p
ossibly
b
e
of
exp
onen
tial
length
(since
one
can
sp
ecify
an
exp
onen
tially
far
lo
cation
on
the
string
using
p
olynomially
man
y
bits).

A
PCP
v
erier
is
called
non-adaptive
if
its
queries
are
determined
solely
based
on
its
input
and
the
outcome
of
its
coin
tosses.
(A
general
v
erier,
called
adaptiv
e,
ma
y
determine
its
queries
also
based
on
answ
ers
to
previously
receiv
ed
oracle
answ
ers).
F
rom
no
w
on,
whenev
er
referring
to
a
PCP
v
erier
it
will
b
e
assumed
to
b
e
adaptiv
e
(unless
otherwise
sp
ecied).

A
p
ossible
motiv
ation
for
the
in
tro
duction
of
PCP
systems
w
ould
b
e
to
pro
vide
an
alternativ
e
view
of
N
P
,
one
that
will
rid
us
of
the
\rigidit
y"
of
the
con
v
en
tional
view.
In
this
regard
randomness
seems
to
b
e
a
most
imp
ortan
t
ingredien
t,
it
pro
vides
us
the
p
ossibilit
y
to
b
e
\imprecise"
in
the
acceptance
of
false
instances.
This
is
b
est
seen
when
taking
the
probabilit
y
b
ound
in
the
soundness
condition
to
b
e
zero.
This
will
cause
that
no
probabilit
y
is
in
v
olv
ed
in
the
denition
and
will
mak
e
it
collapse
in
to
N
P
.
T
o
see
this,
notice
that
in
the
ab
o
v
e
case,
the
output
of
the
v
erier
do
es
not
v
ary
with
the
outcome
of
its
coin
tosses.
This
means
that
in
order
to
determine
the
v
erier's
decision
on
some
input,
it
suces
to
examine
only
one
of
its
p
ossible
executions
(sa
y
,
when
using
the
all
zero
coin
sequence).
In
suc
h
an
execution
only
a
p
olynomial
p
ortion
of
the
PCP
pro
of
is
b
eing
read
b
y
the
v
erier.
It
is
easy
to
see,
that
in
this
case,
the
PCP
and
N
P
denitions
coincide
(just
treat
the
relev
an
t
p
ortion
of
the
PCP
pro
of
as
an
N
P
-w
itness).
Note
that
in
order
to
b
e
consisten
t
with
the
N
P
denition
w
e
require
p
erfect
completeness
(i.e.
a
true
instance
is
alw
a
ys
accepted).

The
denition
of
PCP
requires
that
for
ev
ery
x
in
L
there
exists
a
pro
of

x
for
whic
h
it
holds
that
Pr
[M

x
(x)
=
]
=
.
This
means
that

x
is
p
oten
tially
dieren
t
for
ev
ery
x.
Ho
w
ev
er,
w
e
can
assume
w.l.o.g.,
that
there
exists
a
pro
of

whic
h
is
common
to
all
x's
in
L.
This

will
simply
b
e
the
concatenation
of
all

x
's
(according
to
some
ordering
of
the
x's
in
L).
Since
the
v
erier
is
p
olynomial
w
e
can
assume
that
all

x
's
are
at
most
exp
onen
tially
long
(the
v
erier
cannot
access
more
than
an
exp
onen
tially
long
prex
of
his
pro
of
).
Therefore,
the
lo
cation
of

x
within

will
not
b
e
more
than
exp
onen
tial
in
jxj
a
w
a
y
,
and
so
can
b
e
accessed
in
p
oly
(jxj)
time.

The
oracle
in
a
PCP
system
is
view
ed
in
a
somewhat
dieren
t
manner
than
previously
.
W
e
demonstrate
this
b
y
comparing
a
PCP
system
to
the
mec
hanism
of
a
C
ook
-r
eduction.
Recall
that
a
language
L

is
C
ook
-r
educibl
e
to
L

if
there
exists
an
oracle
mac
hine
M
suc
h
that
for
all
x

f0;
g

it
holds
that
M
L

(x)
=

L

(x).
Note
that
the
oracle
in
the
C
ook
-r
eduction
mec
hanism
is
the
language
L

,
and
is
supp
osed
to
exist
for
all
x

f0;
g

(regardless
of
the
question
whether
x
is
in
L
or
not).
In
con
trast,
in
the
case
of
PCP
systems
the
oracle


LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS

is
supp
osed
not
to
exist
whenev
er
x
is
not
in
L.
That
is,
ev
ery
oracle
w
ould
cause
the
v
erifer
to
reject
x
with
probabilit
y
at
least


.
Therefore,
in
the
PCP
case
(as
opp
osed
to
the
C
ook
-r
eduction
case)
there
is
a
lac
k
of
\symmetry"
b
et
w
een
the
p
ositiv
e
instances
of
L
and
the
negativ
e
ones.
.
The
PCP
c
haracterization
of
NP
..
Imp
ortance
of
Complexit
y
P
arameters
in
PCP
Systems
As
w
as
already
men
tioned
in
subsection
..,
the
class
of
languages
captured
b
y
PCP
systems
v
aries
greatly
as
the
appropriate
parameters
r
()
and
q
()
are
mo
died.
This
fact
is
demonstrated
b
y
the
follo
wing
assertions:

If
N
P

P
C
P
(o(log
);
o(log
))
then
N
P
=
P

P
C
P
(p
oly
;
p
oly
)
=
N
E
X
P
(=
N
T
I
ME
(
p
oly
))
By
taking
either
one
of
the
complexit
y
measures
to
zero
the
denition
of
PCP
collapses
in
to
one
of
the
follo
wing
degenerate
cases:

P
C
P
(p
oly
;
0)
=
coRP

P
C
P
(0;
p
oly
)
=
N
P
When
lo
oking
at
the
ab
o
v
e
degenerate
cases
of
the
PCP
denition
w
e
do
not
really
gain
an
y
no
v
el
view
on
the
complexit
y
classes
in
v
olv
ed
(in
this
case,
coRP
and
N
P
).
Th
us,
the
whole
p
oin
t
of
in
tro
ducing
the
PCP
denition
ma
y
b
e
missed.
What
w
e
w
ould
lik
e
to
see
are
more
delicate
assertions
in
v
olving
b
oth
non-zero
randomness
and
query
complexit
y
.
In
the
follo
wing
subsection
w
e
demonstrate
ho
w
PCP
systems
can
b
e
used
in
order
to
c
haracterize
the
complexit
y
class
N
P
in
suc
h
a
non-degenerate
w
a
y
.
This
c
haracterization
will
lead
to
a
new
p
ersp
ectiv
e
on
N
P
and
enable
us
to
further
in
v
estigate
the
languages
in
it.
..
The
PCP
Theorem
As
already
stated,
the
languages
in
the
complexit
y
class
N
P
are
trivially
captured
b
y
PCP
systems
using
zero
randomness
and
a
p
olynomial
n
um
b
er
of
queries.
A
natural
question
arises:
can
the
t
w
o
complexit
y
measures
b
e
traded
o,
in
a
w
a
y
that
still
captures
the
class
N
P
?
Most
surprisingly
,
not
only
the
answ
er
to
the
ab
o
v
e
question
is
p
ositiv
e,
but
also
a
most
p
o
w
erful
result
emerges.
The
n
um
b
er
of
queries
made
b
y
the
v
erier
can
b
e
brough
t
do
wn
to
a
constan
t
while
using
only
a
logarithmic
n
um
b
er
of
coin
tosses.
This
result
is
kno
wn
as
the
PCP
theorem
(it
will
b
e
cited
here
without
a
pro
of
).
Our
goal
is
to
c
haracterize
N
P
in
terms
of
PCP
systems.
W
e
start
b
y
demonstrating
ho
w
N
P
upp
er
b
ounds
a
fairly
large
class
in
the
PCP
hierarc
h
y
.
This
is
the
class
of
languages
ha
ving
a
PCP
system
whose
v
erier
mak
es
a
p
olynomial
n
um
b
er
of
queries
while
using
a
logarithmic
n
um
b
er
of
coin
tosses.
Prop
osition
..
P
C
P
(log
;
p
oly
)

N
P
Pro
of:
Let
L
b
e
a
language
in
P
C
P
(log
;
p
oly
).
W
e
will
sho
w
ho
w
to
use
its
PCP
system
in
order
to
construct
a
non-deterministic
mac
hine
M
whic
h
decides
L
in
p
olynomial-time.
This
will
imply
that
L
is
in
N
P
.

..
THE
PCP
CHARA
CTERIZA
TION
OF
NP

Let
M
0
b
e
the
probabilistic-p
olynomial
time
oracle
mac
hine
in
the
ab
o
v
e
P
C
P
(log
;
p
oly
)
system
for
L.
W
e
are
guaran
teed
that
on
input
x

f0;
g

,
M
0
mak
es
p
oly
(jxj)
queries
using
O
(log
(jxj))
coin
tosses.
F
or
the
sak
e
of
simplicit
y
,
w
e
pro
v
e
the
claim
for
a
non-adaptiv
e
M
0
(in
order
to
adjust
the
pro
of
to
the
adaptiv
e
case,
some
minor
mo
dications
are
required).
Denote
b
y
hr

;
:
:
:
;
r
m
i
the
sequence
of
all
m
p
ossible
outcomes
of
the
coin
tosses
made
b
y
M
0
(note
that
jr
i
j
=
O
(log
(jxj))
and
m
=

O
(log
(jxj))
=
p
oly
(jxj)).
Denote
b
y
hq
i

;
:
:
:
;
q
i
n
i
i
the
sequence
of
n
i
queries
made
b
y
M
when
using
the
coin
sequence
r
i
(note
that
n
i
is
p
oten
tially
dieren
t
for
eac
h
i,
and
is
p
olynomial
in
jxj).
Since
M
0
is
non-adaptiv
e,
its
queries
are
determined
as
a
function
of
the
input
x
and
the
coin
sequence
r
i
,
and
do
not
dep
end
on
answ
ers
to
previous
queries.
By
the
completeness
condition
w
e
are
guaran
teed
that
for
ev
ery
x
in
L
there
exists
a
PCP
pro
of

x
,
suc
h
that
the
v
erier
M
0
alw
a
ys
accepts
x
when
giv
en
access
to

x
.
A
natural
candidate
for
an
N
P
-w
itness
for
x
w
ould
b
e

x
.
Ho
w
ev
er,
as
already
stated
in
subsection
..,

x
migh
t
b
e
of
exp
onen
tial
size
in
jxj,
and
therefore
unsuitable
to
b
e
used
as
an
N
P
-w
itness.
W
e
will
therefore
use
a
\compressed"
v
ersion
of

x
,
this
v
ersion
corresp
onds
to
the
p
ortion
of
the
pro
of
whic
h
is
actually
b
eing
read
b
y
the
v
erier
M
0
.
W
e
no
w
turn
to
the
construction
of
a
witness
w
,
giv
en
x

L
and
a
corresp
onding
oracle

x
(for
the
sak
e
of
simplicit
y
w
e
denote
it
b
y

).
Consider
all
p
ossible
executions
of
M
0
on
input
x
giv
en
access
to
the
oracle
string

(eac
h
execution
dep
ends
on
the
coin
sequence
r
i
).
T
ak
e
the
substring
of

con
taining
all
the
bits
examined
b
y
M
0
during
these
executions
(i.e.
n
h
q
i

;
:
:
:
;

q
i
n
i
i
o
m
i=
).
En-
co
de
eac
h
en
try
in
this
substring
as
hindex;

index
i
(that
is,
hq
uer
y
;
answ
er
i),
denote
the
resulting
enco
ded
string
b
y
w

x
(note
that
no
w
jw

x
j
is
p
olynomial
in
jxj).
W
e
no
w
describ
e
the
non-deterministic
mac
hine
M
whic
h
decides
L
in
p
olynomial
time.
Giv
en
input
x,
and
w
on
the
guess
tap
e,
M
will
sim
ulate
the
execution
of
M
0
on
input
x
for
all
p
ossible
r
i
's.
Ev
ery
query
made
b
y
M
0
will
b
e
answ
ered
b
y
M
according
to
the
corresp
onding
answ
ers
app
earing
in
w
(b
y
p
erforming
binary
searc
h
on
the
indices
in
w
).
The
mac
hine
M
will
accept
if
and
only
if
M
0
w
ould
ha
v
e
accepted
x
for
all
p
ossible
r
i
's.
Since
M
sim
ulates
the
execution
of
M
0
exactly
m
times
(whic
h
is
p
olynomial
in
jxj),
and
since
M
0
is
a
p
olynomial
time
mac
hine,
then
M
is
itself
a
p
olynomial-time
mac
hine,
as
required.
It
remains
to
b
e
seen
that
L(M
)
indeed
equals
L:

x

L,
w
e
sho
w
that
there
exists
w
suc
h
that
M
(x;
w
)
=
.
By
the
p
erfect
completeness
condition
of
the
PCP
system
for
L,
there
exists
an
oracle

suc
h
that
Pr
[M
0

(x)
=
]
=
.
Therefore,
it
holds
that
for
all
coin
sequences
r
i
,
the
mac
hine
M
0
accepts
x
while
accessing

.
It
immediately
follo
ws
b
y
denition
that
M
(x;
w

x
)
=
,
where
w

x
is
as
describ
ed
ab
o
v
e.

x

L,
w
e
sho
w
that
for
all
w
's
it
holds
that
M
(x;
w
)
=
0.
By
the
soundness
condition
of
the
PCP
system
for
L,
for
all
oracles

it
holds
that
Pr
[M
0

(x)
=
]



.
Therefore,
for
at
least


of
the
p
ossible
coin
sequences
r
i
,
M
do
es
not
accept
x
while
accessing

.
Assume,
for
the
sak
e
of
con
tradiction,
that
there
exists
a
witness
w
for
whic
h
it
holds
that
M
(x;
w
)
=
.
By
the
denition
of
M
this
means
that
for
all
p
ossible
coin
tosses
M
0
accepts
x
when
giv
en
answ
ers
from
w
.
W
e
can
therefore
use
w
in
order
to
construct
an
oracle,

w
,
for
whic
h
it
holds
that
Pr
h
M
0

w
(x)
=

i
=
,
in
con
tradiction
to
the
soundness
condition.
(the
oracle

w
can
b
e
constructed
as
follo
ws:
for
ev
ery
index
q
that
app
ears
in
w
,
dene

w
q
to
b
e
the
binary
answ
er
corresp
onding
to
q
.
Dene
the
rest
of

w
arbitrarily
.)
Consider
no
w
the
case
of
an
adaptiv
e
M
0
.
In
this
case,
w
e
can
construct
w

x
adaptiv
ely
.
Giv
en
an
input
x

L
and
a
corresp
onding
oracle

,
run
M
0
on
x
for
ev
ery
random
string
r
i
,
and
see
what


LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS
are
the
queries
made
b
y
M
0
(whic
h
dep
end
on
x,
r
i
and
answ
ers
to
previous
queries).
Then
tak
e
w

x
to
b
e
the
substring
of

that
is
dened
b
y
all
these
queries,
as
b
efore.
The
essence
of
the
ab
o
v
e
pro
of,
is
that
giv
en
a
PCP
pro
of
(of
logarithmic
randomness)
for
some
x
in
L
w
e
can
ecien
tly
\pac
k"
it
(compress
it
in
to
p
olynomial
size)
and
transform
it
in
to
an
N
P
-witness
for
x.
This
is
due
to
the
fact
that
the
total
p
ortion
of
the
pro
of
used
b
y
the
v
erier
(in
all
p
ossible
runs,
i.e.
o
v
er
all
p
ossible
coin
sequences)
is
b
ounded
b
y
a
p
olynomial.
In
ligh
t
of
the
ab
o
v
e,
an
y
result
of
the
t
yp
e
N
P

P
C
P
(log
;
q
())
w
ould
b
e
in
teresting,
since
it
implies
that
for
ev
ery
x

L,
w
e
can
construct
a
witness
with
the
additional
prop
ert
y
,
that
enables
a
\lazy"
v
erier
to
toss
coins,
and
decide
mem
b
ership
in
L,
based
only
on
a
tin
y
p
ortion
of
the
N
P
-witness
(as
will
b
e
further
discussed
in
subsection
..).
It
turns
out
that
the
p
olynomial
q
()
b
ounding
the
n
um
b
er
of
queries
in
a
result
of
the
ab
o
v
e
kind
can
b
e
tak
en
to
b
e
a
constan
t.
This
surprising
result
is
what
w
e
refer
to
as
the
PCP
theorem.
Theorem
.
(The
PCP
Theorem)
N
P

P
C
P
(log
;
O
())
The
PCP
theorem
is
a
culmination
of
a
sequence
of
w
orks,
eac
h
establishing
a
meaningful
and
increasingly
stronger
statemen
t.
The
pro
of
of
the
PCP
theorem
is
one
of
the
most
complicated
pro
ofs
in
the
theory
of
computation
and
it
is
b
ey
ond
our
scop
e
to
pro
v
e
it
here.
W
e
state
as
a
side
remark,
that
the
smallest
p
ossible
n
um
b
er
of
queries
for
whic
h
the
PCP
theorem
has
b
een
pro
v
en
is
curren
tly

(whereas
with

queries
one
can
get
arbitrarily
close
to
soundness
error
/).
The
conclusion
is
that
N
P
is
exactly
the
set
of
languages
whic
h
ha
v
e
a
PCP
v
erier
that
asks
a
constan
t
n
um
b
er
of
queries
using
a
logarithmic
n
um
b
er
of
coin
tosses.
Corollary
.
(The
PCP
Characterization
of
N
P
)
N
P
=
P
C
P
(log
;
O
())
Pro
of:
Com
bining
Theorem
.
with
Prop
osition
..,
w
e
obtain
the
desired
result.
..
The
PCP
Theorem
giv
es
rise
to
\robust"
N
P
-relations
Recall
that
ev
ery
language
L
in
N
P
can
b
e
asso
ciated
with
an
N
P
-relation
R
L
(in
case
the
language
is
natural,
so
is
the
relation).
This
relation
consists
of
all
pairs
(x;
y
)
where
x
is
a
p
ositiv
e
instance
of
L
and
y
is
a
corresp
onding
N
P
-w
itness.
The
PCP
theorem
giv
es
rise
to
another
(unnatural)
relation
R
0
L
with
some
extra
prop
erties.
In
the
follo
wing
subsection
w
e
briey
discuss
some
of
the
issues
regarding
the
relation
R
0
L
.
Since
ev
ery
L

N
P
has
a
P
C
P
(log
;
O
())
system
w
e
are
guaran
teed
that
for
ev
ery
x
in
L
there
exists
a
PCP
pro
of

x
,
suc
h
that
the
corresp
onding
v
erier
mac
hine
M
alw
a
ys
accepts
x
when
giv
en
access
to

x
.
In
order
to
dene
our
relation
w
e
w
ould
lik
e
to
consider
pairs
of
the
form
(x;

x
).
Ho
w
ev
er,
in
general,

x
migh
t
b
e
of
exp
onen
tial
size
in
jxj,
and
therefore
unsuitable
to
b
e
used
in
an
N
P
-relation.
In
order
to
\compress"
it
in
to
p
olynomial
size
w
e
can
use
the
construction
in
tro
duced
in
the
pro
of
of
Prop
osition
..
(i.e.
of
a
witness
w
for
the
non-deterministic
mac
hine
M
).
Denote
b
y

0
x
the
resulting
\compressed"
v
ersion
of

x
.
W
e
are
no
w
ready
to
dene
the
relation:

..
THE
PCP
CHARA
CTERIZA
TION
OF
NP

R
0
L
def
=

(x;

0
x
)
j
Pr
[M

x
(x)
=
]
=

	
By
the
denition
of
PCP
it
is
ob
vious
that
x

L
if
and
only
if
there
exists

0
x
suc
h
that
(x;

0
x
)

R
0
L
.
It
follo
ws
from
the
details
in
the
pro
of
of
prop
osition
..
that
R
0
L
is
indeed
recognizable
in
p
olynomial-time.
Although
not
stated
in
the
theorem,
the
pro
of
of
the
PCP
theorem
actually
demonstrates
ho
w
to
ecien
tly
transform
an
N
P
-witness
y
(for
an
instance
x
of
L

N
P
)
in
to
an
oracle
pro
of

x;y
for
whic
h
the
PCP
v
erier
alw
a
ys
accepts
x.
Th
us,
there
is
a
Levin-reduction
b
et
w
een
the
natural
N
P
-relation
for
L
and
R
0
L
.
W
e
conclude
that
an
y
N
P
-witness
of
R
L
can
b
e
ecien
tly
transformed
in
to
an
N
P
-witness
of
R
0
L
(i.e.
an
oracle
pro
of
)
whic
h
oers
a
trade-o
b
et
w
een
the
p
ortion
of
the
N
P
-witness
b
eing
read
b
y
the
v
erier
and
the
amoun
t
of
certain
t
y
it
has
in
its
answ
er.
That
is,
if
the
v
erier
is
willing
to
tolerate
an
error
probabilit
y
of

 k
,
it
needs
to
insp
ect
O
(k
)
bits
of
the
pro
of
(the
v
erier
c
ho
oses
k
random
strings
r

;
:
:
:
;
r
k
uniformly
among
f0;
g
O
(log
)
.
It
will
b
e
con
vinced
with
probabilit
y

 k
that
the
input
x
is
in
L,
if
for
ev
ery
i,
M
accepts
x
using
randomness
r
i
and
giv
en
oracle
access
to
the
appropriate
O
()
queries).
..
Simplifying
assumptions
ab
out
P
C
P
(log;
O
())
v
eriers
When
considering
a
P
C
P
(log
;
O
())
system,
some
simplifying
assumptions
ab
out
the
corresp
onding
v
erier
mac
hine
can
b
e
made.
W
e
no
w
turn
to
in
tro
duce
t
w
o
of
them:
.
An
y
v
erier
in
a
P
C
P
(log
;
O
())
system
can
b
e
assumed
to
b
e
non-adaptiv
e
(i.e.
its
queries
are
determined
as
a
function
of
the
input
and
the
random
tap
e
only
,
and
do
not
dep
end
on
answ
ers
to
previous
queries).
This
is
due
to
the
fact
that
an
y
adaptiv
e
P
C
P
(log
;
O
())
v
erier
can
b
e
con
v
erted
in
to
a
non-adaptiv
e
one
b
y
mo
difying
it
in
suc
h
a
w
a
y
that
it
will
consider
all
p
ossible
sequences
of
f0;
g
answ
ers
giv
en
to
its
queries
b
y
the
oracle.
This
certainly
costs
us
in
an
exp
onen
tial
blo
wup
in
the
query
complexit
y
,
but,
since
the
n
um
b
er
of
queries
made
b
y
the
original
(adaptiv
e)
v
erier
is
constan
t,
so
will
b
e
the
query
complexit
y
of
the
mo
died
(non-adaptiv
e)
v
erier
after
the
blo
wup.
Note
that
in
general,
adaptiv
e
v
eriers
ar
e
more
p
o
w
erful
than
non-adaptiv
e
ones
(in
terms
of
quan
titativ
e
results).
There
are
constructions
in
whic
h
adaptiv
e
v
eriers
mak
e
less
queries
than
non-adaptiv
e
ones
while
ac
hieving
the
same
results.
.
An
y
v
erier
in
a
P
C
P
(log
;
O
())
system
can
b
e
assumed
to
alw
a
ys
mak
e
the
same
(constan
t)
n
um
b
er
of
queries
(regardless
of
the
outcome
of
its
coin
tosses).
T
ak
e
an
y
v
erier
in
a
P
C
P
(log
;
O
())
system
not
satisfying
the
ab
o
v
e
prop
ert
y
.
Let
t
b
e
the
maximal
n
um
b
er
of
queries
made
in
some
execution
of
the
ab
o
v
e
v
erier
(o
v
er
all
p
ossible
outcomes
of
the
coin
tosses).
F
or
ev
ery
p
ossible
outcome
of
the
coin
tosses,
mo
dify
the
v
erier
in
suc
h
a
w
a
y
that
it
will
ask
a
total
n
um
b
er
of
t
queries,
mak
e
him
ignore
answ
ers
to
the
newly
added
queries.
Clearly
,
suc
h
a
v
erier
will
b
e
consisten
t
with
the
original
one,
and
will
still
mak
e
only
a
constan
t
n
um
b
er
of
queries
(whic
h
is
t).
F
rom
no
w
on,
whenev
er
referring
to
P
C
P
(log
;
O
())
systems,
free
use
of
the
ab
o
v
e
assumptions
will
b
e
made
(without
an
y
loss
of
generalit
y).


LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS
.
PCP
and
non-appro
ximabilit
y
Man
y
natural
optimization
problems
are
kno
wn
to
b
e
N
P
-har
d.
Ho
w
ev
er,
man
y
times
an
appro
x-
imation
to
the
exact
v
alue
of
the
solution
could
b
e
sucien
t
for
our
needs.
In
this
section
w
e
will
in
v
estigate
the
existence
(or
rather,
the
inexistence)
of
ecien
t
appro
ximation
algorithms
for
t
w
o
N
P
-compl
ete
problems,
namely
,
maxS
AT
and
maxC
LI
QU
E
.
An
algorithm
for
a
giv
en
problem
is
considered
a
C
-appr
oximation
algorithm
if
for
ev
ery
instance
it
generates
an
answ
er
that
is
o
the
correct
answ
er
b
y
a
factor
of
at
most
C
.
The
question
of
in
terest,
is
giv
en
an
N
P
-compl
ete
problem
,
what
is
the
b
est
C
for
whic
h
there
is
a
C
-
appro
ximation
algorithm
for
.
The
PCP
c
haracterization
of
N
P
pro
vides
us
an
alternativ
e
view
of
languages
in
N
P
.
This
view
is
not
as
rigid
as
the
original
one,
and
th
us
creates
a
framew
ork
whic
h
is
apparen
tly
more
insigh
tful
for
the
study
of
appro
ximabilit
y
.
W
e
start
b
y
rephrasing
the
PCP
theorem
in
an
alternativ
e
w
a
y
.
This
in
turn
will
b
e
used
in
order
to
deriv
e
an
immediate
non-appro
ximabilit
y
result
for
maxS
AT
.
While
rephrasing
the
PCP
theorem,
a
new
t
yp
e
of
p
olynomial-time
reductions,
whic
h
w
e
call
amplifying,
emerges.
..
Amplifying
Reductions
Consider
an
unsatisable
C
N
F
form
ula

.
It
ma
y
b
e
the
case
that
the
form
ula
is
v
ery
\close"
to
b
eing
satisable.
F
or
example,
there
exist
unsatisable
form
ulae
suc
h
that
b
y
remo
ving
only
one
of
their
clauses,
they
suddenly
b
ecome
satisable.
In
con
trast,
there
exist
unsatisable
C
N
F
form
ulae
whic
h
are
m
uc
h
\farther"
from
b
eing
satisable
than
the
ab
o
v
e
men
tioned
form
ulae.
These
form
ulae
ma
y
alw
a
ys
ha
v
e
a
constan
t
fraction
of
unsatised
clauses
(for
all
p
ossible
truth
assignmen
ts).
As
a
consequence,
they
oer
us
the
(most
attractiv
e)
feature
of
b
eing
able
to
probabilistically
c
hec
k
whether
a
certain
truth
assignmen
t
satises
them
or
not
(b
y
randomly
sampling
their
clauses
and
pic
king
with
constan
t
probabilit
y
a
clause
whic
h
is
unsatised
b
y
this
assignmen
t).
Not
surprisingly
,
this
resem
bles
the
features
of
a
PCP
system.
Lo
osely
sp
eaking,
amplifying
reductions
of
SA
T

to
itself
are
K
ar
p-r
eductions,
whic
h,
in
addition
to
the
con
v
en
tional
prop
erties,
ha
v
e
the
prop
ert
y
that
they
map
unsatisable
C
N
F
form
ulae
in
to
unsatisable
C
N
F
form
ulae
whic
h
are
\far"
from
b
eing
satisable
(in
the
ab
o
v
e
sense).
Denition
.
(amplifying
reduction)
A
n
amplifying
reduction
of
SA
T
to
itself
is
a
p
olynomial-
time
c
omputable
function
f
mapping
the
set
of
C
N
F
formulae
to
itself
such
that
for
some
c
onstant

>
0
it
holds
that:

f
maps
satisable
C
N
F
formulae
to
satisable
C
N
F
formulae.

f
maps
non-satisable
C
N
F
formulae
to
(non-satisable)
C
N
F
formulae
for
which
every
truth
assignment
satises
at
most
an

 
fr
action
of
the
clauses.
An
amplifying
reduction
of
a
language
L
in
N
P
to
SA
T,
can
b
e
dened
analogously
.

Recall
that
a
t-C
N
F
form
ula
is
a
b
o
olean
form
ula
consisting
of
a
conjunction
of
clauses,
where
eac
h
clause
is
a
disjunction
of
up
to
t
literals
(a
literal
is
a
v
ariable
or
its
negation).

SA
T
is
the
problem
of
deciding
whether
a
giv
en
C
N
F
form
ula
has
a
satisfying
truth
assignmen
t.

..
PCP
AND
NON-APPR
O
XIMABILITY

..
PCP
Theorem
Rephrased
Amplifying
reductions
seem
lik
e
a
suitable
to
ol
to
b
e
used
in
order
to
construct
a
PCP
system
for
ev
ery
language
in
N
P
.
Not
only
they
are
ecien
tly
computable,
but
they
enable
us
to
map
negativ
e
instances
of
an
y
language
in
N
P
in
to
negativ
e
instances
of
SA
T
whic
h
w
e
ma
y
b
e
able
to
reject
on
a
probabilistic
basis
(analogously
to
the
soundness
condition
in
the
PCP
denition).
It
turns
out
that
the
con
v
erse
is
also
true,
giv
en
a
PCP
system
for
a
language
in
N
P
w
e
are
also
able
to
construct
an
amplifying
reduction
of
SA
T
to
itself.
Theorem
.
(PCP
theorem
rephrased)
The
fol
lowing
ar
e
e
quivalent:
.
N
P

P
C
P
(log
;
O
()).
(The
PCP
Theorem).
.
Ther
e
exists
an
amplifying
r
e
duction
of
SA
T
to
itself.
Pro
of:
W
e
start
with
the
(()
)
())
direction.
Consider
an
y
language
L

N
P
.
By
the
PCP
theorem
L
has
a
P
C
P
(log
;
O
())
system,
w
e
will
no
w
sho
w
ho
w
to
use
this
system
in
order
to
construct
an
amplifying
reduction
from
L
to
SA
T.
This
will
in
particular
hold
for
L
=
SA
T
(whic
h
is
itself
in
N
P
),
and
the
claim
will
follo
w.
Let
M
b
e
the
probabilistic
p
olynomial-time
oracle
mac
hine
in
the
ab
o
v
e
P
C
P
(log
;
O
())
system
for
L.
W
e
are
guaran
teed
that
on
input
x

f0;
g

,
M
mak
es
t
=
O
()
queries
using
O
(log
(jxj))
coin
tosses.
Denote
b
y
hr

;
:
:
:
;
r
m
i
the
sequence
of
all
m
p
ossible
outcomes
of
the
coin
tosses
made
b
y
M
(note
that
jr
i
j
=
O
(log
(jxj))
and
m
=

O
(log
(jxj))
=
p
oly
(jxj)).
Denote
b
y
hq
i

;
:
:
:
;
q
i
t
i
the
sequence
of
t
queries
made
b
y
M
when
using
the
coin
sequence
r
i
.
As
men
tioned
in
subsection
..,
w
e
can
assume
that
M
is
non-adaptiv
e,
therefore
its
queries
are
determined
as
a
function
of
the
input
x
and
the
coin
sequence
r
i
,
and
do
not
dep
end
on
answ
ers
to
previous
queries
(although
not
eviden
t
from
the
notation
q
i
j
,
the
queries
do
not
dep
end
only
on
r
i
,
but
on
x
as
w
ell).
W
e
no
w
turn
to
the
construction
of
the
amplifying
reduction.
Giv
en
x

f0;
g

,
w
e
construct
for
eac
h
r
i
a
(constan
t
size)
C
N
F
b
o
olean
form
ula,
'
x
i
,
describing
whether
M
w
ould
ha
v
e
accepted
the
input
x
(i.e.
describing
all
p
ossible
outputs
of
M
on
input
x,
using
the
coin
sequence
r
i
).
W
e
asso
ciate
to
eac
h
query
q
i
j
a
b
o
olean
v
ariable
z
q
i
j
whose
v
alue
should
b
e
the
answ
er
M
gets
to
the
corresp
onding
query
.
Again,
since
M
is
assumed
to
b
e
non-adaptiv
e,
when
giv
en
its
input
and
coin
tosses,
M
's
decision
is
completely
determined
b
y
the
answ
ers
it
gets
to
its
queries.
In
other
w
ords,
M
's
decision
dep
ends
only
on
the
v
alues
of
hz
q
i

;
:
:
:
;
z
q
i
t
i.
In
order
to
construct
'
x
i
,
b
egin
b
y
computing
the
follo
wing
truth
table:
to
ev
ery
p
ossible
sequence
hz
q
i

;
:
:
:
;
z
q
i
t
i
assign
the
corresp
onding
b
o
olean
decision
of
M
(i.e.
the
output
of
M
on
input
x,
using
the
coin
sequence
r
i
,
and
giv
en
answ
ers
z
q
i
j
to
queries
q
i
j
).
Clearly
,
this
can
b
e
computed
in
p
olynomial-time
(b
y
sim
ulating
M
's
execution).
Therefore,
the
whole
table
can
b
e
computed
in
p
olynomial-time
(since
the
n
um
b
er
of
p
ossible
assignmen
ts
to
hz
q
i

;
:
:
:
;
z
q
i
t
i
is

t
,
whic
h
is
a
constan
t).
W
e
can
no
w
build
a
C
N
F
b
o
olean
form
ula,
'
x
i
,
whic
h
is
consisten
t
with
the
ab
o
v
e
truth
table,
this
is
done
in
the
follo
wing
w
a
y:
.
Construct
a
t-C
N
F
form
ula
 
x
i
=
 
x
i
(z
q
i

;
:
:
:
;
z
q
i
t
)
whic
h
is
consisten
t
with
the
truth
table.
.
Using
a
constan
t
n
um
b
er
of
auxiliary
v
ariables,
transform
it
to
C
N
F
(denoted
'
x
i
).


LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS
Since
the
table
size
is
constan
t,
the
ab
o
v
e
pro
cedure
can
b
e
executed
in
constan
t
time.
Note
that
in
the
transformation
of
t-C
N
F
form
ulae
in
to
C
N
F
form
uae,
eac
h
clause
with
t
literals
is
substituted
b
y
at
most
t
clauses
of

literals.
Since
 
x
i
consists
of
exactly

t
clauses
w
e
conclude
that
the
n
um
b
er
of
clauses
in
'
x
i
is
b
ounded
b
y
t


t
.
Finally
,
giv
en
'
x
i
for
i
=
;
:
:
:
;
m,
w
e
let
our
amplifying
reduction
f
map
x

f0;
g

in
to
the
C
N
F
b
o
olean
form
ula:
'
x
def
=
m
^
i=
'
x
i
Since
for
ev
ery
i
=
;
:
:
:
;
m
the
(constan
t
size)
form
ula
'
x
i
can
b
e
computed
in
p
olynomial-time
(in
jxj),
and
since
m
=
p
oly
(jxj),
it
follo
ws
that
the
mapping
f
:
x
!
'
x
is
p
olynomial-time
computable,
and
j'
x
j
is
p
olynomial
in
jxj
(note
also
that
the
n
um
b
er
of
clauses
in
'
x
is
b
ounded
b
y
m

t


t
).
It
remains
to
b
e
v
eried
that
f
is
indeed
an
amplifying
reduction:

x

L,
w
e
no
w
sho
w
that
'
x
is
in
SA
T,
this
happ
ens
if
and
only
if
the
corresp
onding
t-C
N
F
form
ula
 
x
def
=
V
m
i=
 
x
i
(z
q
i

;
:
:
:
;
z
q
i
t
)
is
in
t-SA
T
(recall
that
 
x
i
w
as
in
tro
duced
in
the
con-
struction
of
'
x
i
).
Since
L

PCP
,
then
there
exists
an
oracle

suc
h
that
Pr
[M

(x)
=
]
=
.
Therefore,
it
holds
that
for
ev
ery
coin
sequence
r
i
,
the
mac
hine
M
accepts
x
while
access-
ing

.
Since
 
x
i
is
consisten
t
with
the
ab
o
v
e
men
tioned
truth
table
it
follo
ws
that
for
all
i
=
;
:
:
:
;
m,
it
holds
that
 
x
i
(
q
i

;
:
:
:
;

q
i
t
)
=
,
and
th
us
 
x
is
in
t-SA
T.
W
e
conclude
that
'
x
is
in
SA
T,
as
required

.

x

L,
w
e
no
w
sho
w
that
ev
ery
truth
assignmen
t
satises
at
most
an

 
fraction
of
'
x
's
clauses.
Since
L

PCP
,
then
for
all
oracles

it
holds
that
Pr
[M

(x)
=
]



.
Therefore,
for
at
least


of
the
p
ossible
coin
sequences
r
i
,
mac
hine
M
do
es
not
accept
x
while
accessing

.
Put
in
other
w
ords,
for
eac
h
truth
assignmen
t
(whic
h
corresp
onds
to
some

)
at
least


of
the
'
x
i
's
are
unsatisable.
Since
ev
ery
unsatisable
b
o
olean
form
ula
alw
a
ys
has
at
least
one
unsatised
clause,
it
follo
ws
that
for
ev
ery
truth
assignmen
t
'
x
has
at
least
m

unsatised
clauses.
Since
the
n
um
b
er
of
clauses
in
'
x
is
b
ounded
b
y
m

t


t
,
b
y
taking

to
b
e
the
constan
t

t
t
w
e
are
guaran
teed
that
ev
ery
truth
assignmen
t
satises
at
most
an

 
fraction
of
'
x
's
clauses.
W
e
no
w
turn
to
the
(()
)
())
direction.
Under
the
assumption
that
there
exists
an
amplifying
reduction
of
SA
T
to
itself
w
e
will
sho
w
that
the
PCP
theorem
holds.
Consider
an
y
language
L

N
P
.
Since
L
is
Karp-r
e
ducible
to
SA
T,
it
is
sucien
t
to
sho
w
that
SA
T

P
C
P
(log
;
O
()).
Let
f
:
C
N
F
!
C
N
F
b
e
an
amplifying
reduction
of
SA
T
to
itself.
And
let

b
e
the
constan
t
guaran
teed
b
y
Denition
..
W
e
no
w
sho
w
ho
w
to
use
f
in
order
to
construct
a
P
C
P
(log
;
O
())
system
for
SA
T.
W
e
start
b
y
giving
an
informal
description
of
the
v
erier
mac
hine
M
.
Giv
en
a
certain
C
N
F
form
ula
',
M
computes
'
0
=
f
(').
It
then
tosses
coins
in
order
to
uniformly
c
ho
ose
one
of
the
clauses
of
'
0
.
By
querying
the
oracle
string
(whic
h
should
b
e
a
p
ossible
truth
assignmen
t
for
'
0
)
M
will
assign
truth
v
alues
to
the
c
hosen
clause's
v
ariables.
M
will
accept
if
and
only
if
the
clause
is
satised.
The
fact
that
f
is
an
amplifying
reduction
implies
that
whenev
er
M
gets
a
negativ
e
instance
of
SA
T,
with
constan
t
probabilit
y
the
c
hosen
clause
will
not
b
e
satised.
In
con
trast,
this
will
nev
er
happ
en
when
lo
oking
at
a
p
ositiv
e
instance.
W
e
no
w
turn
to
a
more
formal
denition
of
the
PCP
v
erier
mac
hine
M
.
On
input
'

C
N
F
and
giv
en
access
to
an
oracle
string

0
,
M
is
dened
in
the
follo
wing
w
a
y:

Note
that
all
'
x
i
's
ha
v
e
disjoin
t
sets
of
auxiliary
v
ariables,
hence
transforming
a
satisfying
assignmen
t
of
 
x
in
to
a
satisfying
assignmen
t
of
'
x
causes
no
inconsistencies.

..
PCP
AND
NON-APPR
O
XIMABILITY
	
.
Find
the
C
N
F
form
ula
'
0
=
'
0
(x

;
:
:
:
;
x
n
0
)
def
=
f
(').
'
0
=
V
m
0
i=
c
i
where
c
i
denotes
a
clause
with

literals.
.
Select
a
clause
c
i
of
'
0
uniformly
.
Denote
b
y
hx
i

;
x
i

;
x
i

i
the
three
v
ariables
whose
literals
app
ear
in
c
i
.
.
Query
the
v
alues
of
h
0
i

;

0
i

;

0
i

i
separately
,
and
assign
them
to
hx
i

;
x
i

;
x
i

i
accordingly
.
V
erify
the
truth
v
alue
of
c
i
=
c
i
(x
i

;
x
i

;
x
i

).
.
Rep
eat
stages
,
for
d


e
times
indep
enden
tly
(note
that
d


e
is
constan
t).
.
Output

if
and
only
if
in
all
iterations
the
truth
v
alue
of
c
i
w
as
.
Clearly
,
M
is
a
p
olynomial-time
mac
hine.
Note
that
f
is
computable
in
p
olynomial-time
(this
also
implies
that
n
0
;
m
0
=
p
oly
(j'j)).
In
addition,
the
n
um
b
er
of
iterations
executed
b
y
M
is
constan
t,
and
in
eac
h
iteration
a
p
olynomial
amoun
t
of
w
ork
is
executed
(dep
ending
on
n
0
;
m
0
whic
h
are,
as
already
men
tioned,
p
olynomial
in
j'j).
W
e
turn
to
ev
aluate
the
additional
complexit
y
measures
in
v
olv
ed.
In
terms
of
randomness,
M
needs
to
uniformly
c
ho
ose
d


e
n
um
b
ers
in
the
set
f;
:::;
m
0
g.
This
in
v
olv
es
O
(log
(m
0
))
=
O
(log
(j'j))
coin
tosses,
as
required.
In
terms
of
queries,
the
n
um
b
er
of
queries
ask
ed
b
y
M
is
exactly


whic
h
is
constan
t,
again
as
required.
It
remains
to
examine
the
completeness
and
soundness
of
the
ab
o
v
e
PCP
system:

c
ompleteness:
If
'

SA
T,
then
'
0

SA
T
(since
f
is
an
amplifying
reduction).
Therefore
there
exists
a
truth
assignmen
t,

0
,
suc
h
that
'
0
(
0
)
=
.
No
w,
since
ev
ery
clause
of
'
0
is
satised
b
y

0
,
it
immediately
follo
ws
that:
Pr
h
M

0
(')
=

i
=


soundness:
If
'

SA
T
then
an
y
truth
assignmen
t
for
'
0
satises
at
most
an

 
fraction
of
the
clauses.
Therefore
for
an
y
p
ossible
truth
assignmen
t
(oracle)

0
it
holds
that
Pr
h
M

0
(')
=

i
=
Pr



d


e
^
j
=
(
c
i
j
is
satisf
ied
by
the
assig
nment

0
)




(
 )
d


e


e
<


where
the
probabilit
y
is
tak
en
o
v
er
M
's
in
ternal
coin
tosses
(i.e.
o
v
er
the
c
hoice
of
i

;
:
:
:
;
i
d


e
).
Corollary
.
Ther
e
exists
an
amplifying
r
e
duction
of
SA
T
to
itself.
Pro
of:
Com
bining
the
PCP
Theorem
with
Theorem
.,
w
e
obtain
the
desired
result.
..
Connecting
PCP
and
non-appro
ximabilit
y
The
c
haracterization
of
N
P
using
probabilistic
c
hec
k
able
pro
of
systems
enabled
the
area
of
ap-
pro
ximabilit
y
to
mak
e
a
signican
t
progress.
In
general,
PCP
systems
for
N
P
yield
strong
non-appro
ximabilit
y
results
for
v
arious
classical
optimization
problems.
The
hardness
of
appro
ximation
is
t
ypically
established
using
the
notion

0
LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS
of
gap
problems,
whic
h
are
a
particular
case
of
promise
problems.
Recall
that
a
promise
problem
consists
of
t
w
o
sets
(A;
B
),
where
A
is
the
set
of
YES
instances
and
B
is
the
set
of
NO
instances.
A
and
B
need
not
b
e
complemen
tary
,
that
is,
an
instance
x

f0;
g

is
not
necessarily
in
either
A
or
B
.
W
e
demonstrate
the
notion
of
a
gap
problem
using
the
promise
problem
g
apC
LI
QU
E
as
an
example.
Denote
b
y
maxC
LI
QU
E
(G)
the
size
of
the
maximal
clique
in
a
graph
G.
Let
g
apC
LI
QU
E
;
b
e
the
promise
problem
(A;
B
)
where
A
is
the
set
of
all
graphs
G
with
maxC
LI
QU
E
(G)

,
and
B
is
the
set
of
all
graphs
G
with
maxC
LI
QU
E
(G)


.
The
gap
is
dened
as
=
.
T
ypically
,
a
hardness
result
will
sp
ecify
a
v
alue
C
of
the
gap
for
whic
h
the
problem
is
N
P
-har
d.
This
means
that
there
is
no
ecien
t
algorithm
that
appro
ximates
the
maxC
LI
QU
E
size
of
a
graph
G
within
a
factor
of
C
(unless
N
P
=
P
).
The
gap
v
ersions
of
v
arious
other
optimization
problems
are
dened
in
an
analogous
w
a
y
.
In
this
subsection
w
e
bring
t
w
o
non-appro
ximabilit
y
results
concerning
the
problems
maxS
AT
and
maxC
LI
QU
E
that
will
b
e
dened
in
the
sequel.
An
immediate
non-appro
ximabilit
y
result
for
maxSA
T
Denition
.
(maxSA
T):
Dene
maxSA
T
to
b
e
the
fol
lowing
pr
oblem:
Given
a
C
N
F
b
o
ole
an
formula
'
nd
the
maximal
numb
er
of
clauses
that
c
an
b
e
simultane
ously
satise
d
by
any
truth
as-
signment
to
the
variables
of
'.
maxS
AT
is
kno
wn
to
b
e
N
P
-har
d.
Therefore,
appro
ximating
it
w
ould
b
e
desirable.
This
motiv
ates
the
denition
of
the
corresp
onding
gap
problem:
Denition
.	
(g
apS
AT
;
)
:
L
et
;


[0;
]
such
that,



.
Dene
g
apS
AT
;
to
b
e
the
fol
lowing
pr
omise
pr
oblem:

The
YES
instanc
es
ar
e
al
l
C
N
F
formulae
',
such
that
ther
e
exists
a
truth
assignment
which
satises
at
le
ast
an
-fr
action
of
the
clauses
of
'.

The
NO
instanc
es
ar
e
al
l
C
N
F
formulae
',
such
that
every
truth
assignment
satises
less
than
a

-fr
action
of
the
clauses
of
'.
Note
that
g
apS
AT
;
is
an
alternativ
e
form
ulation
of
SA
T
(the
decision
problem).
The
follo
wing
claim
states
that,
for
some

<
,
it
is
N
P
-har
d
to
distinguish
b
et
w
een
satisable
C
N
F
form
ulae
and
C
N
F
form
ulae
for
whic
h
no
truth
assignmen
t
satises
more
than
a

-fraction
of
its
clauses.
This
result
implies
that
there
is
some
constan
t
C
>

suc
h
that
maxS
AT
could
not
b
e
appro
ximated
within
C
(unless
N
P
=
P
).
The
claim
is
an
immediate
result
of
Corollary
..
Claim
..
Ther
e
exists
a
c
onstant

<
,
such
that
the
pr
omise
pr
oblem
g
apS
AT
;
is
N
P
-
har
d.
Pro
of:
Let
L

N
P
.
W
e
w
an
t
to
manifest
that
L
is
Karp-r
e
ducible
to
g
apS
AT
;
.
S
AT
is
N
P
-complete,
therefore
there
exists
a
Karp-r
e
duction
f

from
L
to
S
AT
.
By
Corol-
lary
.
there
exists
an
amplifying
reduction
f

(and
a
constan
t

>
0)
from
S
AT
to
itself.
No
w,
tak
e
an
y

 
<

<
:

F
or
x

L,
'
=
f

(f

(x))
is
satisable,
and
is
therefore
a
YES
instances
of
g
apS
AT
;
.

F
or
x

L,
'
=
f

(f

(x))
is
not
satisable.
F
urthermore,
for
ev
ery
truth
assignmen
t,
the
fraction
of
satised
clauses
in
'
is
at
most

 .
Therefore,
'
is
a
NO
instance
of
g
apS
AT
;
.

..
PCP
AND
NON-APPR
O
XIMABILITY

Recen
tly
,
stronger
results
w
ere
pro
v
en.
These
results
sho
w
that
for
ev
ery

>
=,
the
problem
g
apS
AT
;
is
N
P
-har
d.
This
means
that
it
is
infeasible
to
come
with
an
ecien
t
algorithm
that
appro
ximates
maxS
AT
within
a
factor
strictly
smaller
than
=.
On
the
other
hand,
g
apS
AT
;=
is
kno
wn
to
b
e
p
olynomially
solv
able,
and
therefore
the
=-appro
ximation
ratio
is
tigh
t.
MaxCLIQUE
is
non-appro
ximable
within
a
factor
of
t
w
o
W
e
briey
review
the
denitions
of
the
problems
maxC
LI
QU
E
and
g
apC
LI
QU
E
;
:
Denition
.0
(maxCLIQUE):
Dene
maxCLIQUE
to
b
e
the
fol
lowing
pr
oblem:
Given
a
gr
aph
G,
nd
the
size
of
the
maximal
clique
of
G
(a
clique
is
a
set
of
vertic
es
such
that
every
p
air
of
vertic
es
shar
e
an
e
dge).
maxC
LI
QU
E
is
kno
wn
to
b
e
N
P
-har
d.
Therefore,
appro
ximating
it
w
ould
b
e
desirable.
This
motiv
ates
the
denition
of
the
corresp
onding
gap
problem:
Denition
.
(g
apC
LI
QU
E
;
)
:
L
et
;

:
N
!
N
b
e
two
functions,
satisfying
(n)


(n)
for
every
n.
F
or
a
gr
aph
G,
denote
jGj
to
b
e
the
numb
er
of
vertic
es
in
G.
Dene
g
apC
LI
QU
E
;
to
b
e
the
fol
lowing
pr
omise
pr
oblem:

The
YES
instanc
es
ar
e
al
l
the
gr
aphs
G,
with
max
clique
gr
e
ater
than
or
e
qual
to
(jGj).

The
NO
instanc
es
ar
e
al
l
the
gr
aphs
G,
with
max
clique
smal
ler
than
or
e
qual
to

(jGj).
W
e
conclude
our
discussion
on
PCP
systems
b
y
presen
ting
a
nice
theorem
whic
h
demonstrates
the
hardness
of
appro
ximating
maxC
LI
QU
E
.
The
theorem
implies
that
it
is
an
infeasible
task
to
appro
ximate
maxC
LI
QU
E
within
a
constan
t
smaller
than
t
w
o
(unless
N
P
=
P
).
Note,
ho
w
ev
er,
that
this
is
not
the
strongest
result
kno
wn.
It
has
b
een
sho
wn
recen
tly
that
giv
en
a
graph
G
of
size
N
,
the
v
alue
of
maxC
LI
QU
E
is
non-appro
ximable
within
a
factor
of
N
 
(for
ev
ery

>
0).
This
result
is
tigh
t,
since
an
N
 o()
-appro
ximation
algorithm
is
kno
wn
to
exist
(the
latter
is
scarcely
b
etter
than
the
trivial
appro
ximation
factor
of
N
).
Theorem
.
Ther
e
exists
a
function

:
N
!
N
,
such
that
the
pr
omise
pr
oblem
g
apC
LI
QU
E
;=
is
N
P
-har
d.
Pro
of:
Let
L

N
P
b
e
some
language.
W
e
w
an
t
to
sho
w
that
L
is
Karp-r
e
ducible
to
the
language
g
apC
LI
QU
E
;=
(for
some
function

:
N
!
N
whic
h
is
not
dep
enden
t
on
L,
rather
it
is
common
to
all
L's).
Lo
osely
sp
eaking,
giv
en
input
x

f0;
g

w
e
construct
in
an
ecien
t
w
a
y
a
graph
G
x
ha
ving
the
follo
wing
prop
ert
y:
If
x
is
in
L
then
G
x
is
a
YES
instance
of
g
apC
LI
QU
E
,
whereas,
if
x
is
not
in
L
then
G
x
is
a
NO
instance
of
g
apC
LI
QU
E
.
W
e
no
w
turn
to
a
formal
denition
of
the
ab
o
v
e
men
tioned
reduction.
By
the
PCP-theorem,
L
has
a
P
C
P
(O
(log
);
O
())
system.
Therefore,
there
exists
a
probabilistic
p
olynomial-time
oracle
mac
hine
M
,
that
on
input
x

f0;
g

mak
es
t
=
O
()
queries
using
O
(log
(jxj))
random
coin
tosses.
Again,
w
e
let
hr

;
:
:
:
;
r
m
i
b
e
the
sequence
of
all
m
p
ossible
outcomes
of
the
coin
tosses
made
b
y
M
(note
that
m
=
p
oly
(jxj)).
Let
hq
i

;
:
:
:
;
q
i
t
i
denote
the
t
queries
made
b
y
M
when
using
the
coin
tosses
r
i
,
and
let
ha
i

;
:
:
:
;
a
i
t
i
b
e
a
p
ossible
sequence
of
answ
ers
to
the
corresp
onding
queries.
W
e
no
w
turn
to
dene
a
graph
G
0
x
that
corresp
onds
to
mac
hine
M
and
input
x:


LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS
v
ertices:
F
or
ev
ery
p
ossible
r
i
,
the
tuple
(r
i
;
(q
i

;
a
i

);
:
:
:
;
(q
i
t
;
a
i
t
))
is
a
v
ertex
in
G
0
x
if
and
only
if
when
using
r
i
,
and
giv
en
answ
ers
a
i
j
to
queries
q
i
j
,
M
accepts
x.
Note
that
since
M
is
non-adaptiv
e,
once
r
i
is
xed
then
so
are
the
queries
hq
i

;
:
:
:
;
q
i
t
i.
This
implies
that
t
w
o
v
ertices
ha
ving
the
same
r
i
,
also
ha
v
e
the
same
q
i
j
's.
Therefore,
the
n
um
b
er
of
v
ertices
corresp
onding
to
a
certain
r
i
is
smaller
or
equal
to

t
,
and
the
total
n
um
b
er
of
v
ertices
in
G
0
x
is
smaller
or
equal
to
m


t
.
edges:
Tw
o
v
ertices
v
=
(r
i
;
(q
i

;
a
i

);
:
:
:
;
(q
i
t
;
a
i
t
))
and
u
=
(r
j
;
(q
j

;
a
j

);
:
:
:
;
(q
j
t
;
a
j
t
))
will
not
ha
v
e
an
edge
b
et
w
een
them
if
and
only
if
they
are
not
c
onsistent,
that
is,
v
and
u
con
tain
the
same
query
and
eac
h
one
of
them
has
a
dieren
t
answ
er
to
this
query
.
Note
that
if
u
and
v
con
tain
the
same
randomness
(i.e.
r
i
is
equal
to
r
j
)
they
do
not
share
an
edge,
since
they
cannot
b
e
consisten
t
(as
men
tioned
earlier,
v
ertices
ha
ving
the
same
randomness
ha
v
e
also
the
same
queries,
so
u
and
v
m
ust
dier
in
the
answ
ers
to
the
queries).
Finally
,
mo
dify
G
0
x
b
y
adding
to
it
(m


t
 jG
0
x
j)
isolated
v
ertices.
The
resulting
graph
will
ha
v
e
exactly
m


t
v
ertices
and
will
b
e
denoted
G
x
.
Note
that
since
the
ab
o
v
e
mo
dication
do
es
not
add
an
y
edges
to
G
0
x
,
it
do
es
not
c
hange
the
size
of
an
y
clique
in
G
0
x
(in
particular
it
holds
that
maxC
LI
QU
E
(G
x
)
=
maxC
LI
QU
E
(G
0
x
)).
The
ab
o
v
e
reduction
is
ecien
t,
since
the
graph
G
x
can
b
e
constructed
in
p
olynomial
time:
There
are
at
most
m


t
v
ertices
in
G
0
x
(whic
h
is
p
olynomial
in
jxj
since
t
is
a
constan
t),
and
to
decide
whether
(r
i
;
(q
i

;
a
i

);
:
:
:
;
(q
i
t
;
a
i
t
))
is
a
v
ertex
in
G
0
x
,
one
has
to
sim
ulate
mac
hine
M
on
input
x,
randomness
r
i
,
queries
fq
i
j
g
t
j
=
and
answ
ers
fa
i
j
g
t
j
=
and
see
whether
it
accepts
or
not.
This
is,
of
course,
p
olynomial,
since
M
is
p
olynomial.
Finally
,
deciding
whether
t
w
o
v
ertices
share
an
edge
can
b
e
done
in
p
olynomial
time.
Let
(n)
def
=
n=
t
.
Since
jG
x
j
=
m


t
,
it
holds
that
(jG
x
j)
=
m.
It
is
therefore
sucien
t
to
sho
w
a
reduction
from
the
language
L
to
g
apC
LI
QU
E
m;m=
,
this
will
imply
that
the
promise
problem
g
apC
LI
QU
E
;=
is
N
P
-har
d.

F
or
x

L,
w
e
sho
w
that
G
x
con
tains
a
clique
of
size
m.
By
the
PCP
denition
there
exists
a
pro
of

suc
h
that
for
ev
ery
random
string
r
,
mac
hine
M
accepts
x
using
randomness
r
and
giv
en
oracle
access
to

.
Lo
ok
at
the
follo
wing
set
of
m
v
ertices
in
the
graph
G
x
:
S
=
f(r
i
;
(q
i

;

q
i

);
:
:
:
;
(q
i
t
;

q
i
t
))
for


i

mg.
It
is
easy
to
see
that
all
the
v
ertices
in
S
are
indeed
legal
v
ertices,
b
ecause

is
a
pro
of
for
x.
Also,
all
the
v
ertices
in
S
m
ust
b
e
consisten
t,
b
ecause
all
their
answ
ers
are
giv
en
according
to

,
and
therefore,
ev
ery
t
w
o
v
ertices
in
S
share
an
edge.
This
en
tails
that
S
is
an
m-clique
in
G
x
,
and
therefore
G
x
is
a
YES
instance
of
g
apC
LI
QU
E
m;m=
.

F
or
x

L,
w
e
sho
w
that
G
x
do
es
not
con
tain
a
clique
of
size
greater
than
m=.
Supp
ose,
in
con
tradiction,
that
S
is
a
clique
in
G
x
of
size
greater
than
m=.
Dene
no
w
the
follo
wing
pro
of

:
F
or
ev
ery
query
and
answ
er
(q
;
a)
in
one
of
the
v
ertices
of
S
dene

q
=
a.
F
or
ev
ery
other
query
(whic
h
is
not
included
in
either
of
the
v
ertices
of
S
)
dene

q
to
b
e
an
arbitrary
v
alue
in
f0,g.
Since
S
is
a
clique,
all
its
v
ertices
share
an
edge
and
are
therefore
consisten
t.
Note
that

is
w
ell
dened,
the
consistency
requiremen
t
implies
that
same
queries
ha
v
e
same
answ
ers
(for
all
queries
and
answ
ers
app
earing
in
some
v
ertex
in
S
).
Therefore,
it
cannot
b
e
the
case
that
w
e
giv
e
t
w
o
incosisten
t
v
alues
to
the
same
en
try
in

during
its
construction.
No
w,
since
all
the
v
ertices
of
S
ha
v
e
dieren
t
r
i
's
and
jS
j
is
greater
than
m=,
it
holds
that

..
PCP
AND
NON-APPR
O
XIMABILITY

for
more
than


of
the
p
ossible
coin
sequences
r
i
,
mac
hine
M
accepts
x
while
accessing

.
In
other
w
ords,
Pr[M

(x)
=
]
>
=,
in
con
tradiction
to
the
soundness
condition.
W
e
conclude
that
indeed
G
x
do
es
not
ha
v
e
a
clique
of
size
greater
than
m=,
and
is
therefore
a
NO
instance
of
g
apC
LI
QU
E
m;m=
.
Bibliographic
Notes
The
PCP
Characterization
Theorem
is
attributed
to
Arora,
Lund,
Mot
w
ani,
Safra,
Sudan
and
Szegedy
(see
[]
and
[]).
These
pap
ers,
in
turn,
built
on
n
umerous
previous
w
orks;
for
details
see
the
pap
ers
themselv
es
or
[].
In
general,
our
presen
tation
of
PCP
follo
ws
follo
ws
Section
.
of
[],
and
the
in
terested
reader
is
referred
to
[]
for
a
surv
ey
of
further
dev
elopmen
ts
and
more
rened
considerations.
The
rst
connection
b
et
w
een
PCP
and
hardness
of
appro
ximation
w
as
made
b
y
F
eige,
Gold-
w
asser,
Lo
v
asz,
Safra,
and
Szegedy
[]:
They
sho
w
ed
the
connection
to
maxClique
(presen
ted
ab
o
v
e).
The
connection
to
maxSA
T
and
other
\MaxSNP
appro
ximation"
problems
w
as
made
later
in
[].
W
e
did
not
presen
t
the
strongest
kno
wn
non-appro
ximabilit
y
results
for
maxSA
T
and
max-
Clique.
These
can
b
e
found
in
Hastad's
pap
ers,
[]
and
[],
resp
ectiv
ely
.
.
S.
Arora,
C.
Lund,
R.
Mot
w
ani,
M.
Sudan
and
M.
Szegedy
.
Pro
of
V
erication
and
In
tractabil-
it
y
of
Appro
ximation
Problems.
JA
CM,
V
ol.
,
pages
0{,
		.
.
S.
Arora
and
S.
Safra.
Probabilistic
Chec
k
able
Pro
ofs:
A
New
Characterization
of
NP.
JA
CM,
V
ol.
,
pages
0{,
		.
.
U.
F
eige,
S.
Goldw
asser,
L.
Lo
v
asz,
S.
Safra,
and
M.
Szegedy
.
Appro
ximating
Clique
is
almost
NP-complete.
JA
CM,
V
ol.
,
pages
{	,
		.
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.
.
J.
Hastad.
Clique
is
hard
to
appro
ximate
within
n
 
.
T
o
app
ear
in
A
CT
A
Mathematic
a.
Preliminary
v
ersions
in
th
STOC
(		)
and
th
F
OCS
(		).
.
J.
Hastad.
Getting
optimal
in-appro
ximabilit
y
results.
In
	th
STOC,
pages
{0,
		.


LECTURE
.
PR
OBABILISTICALL
Y
CHECKABLE
PR
OOF
SYSTEMS

Lecture

Pseudorandom
Generators
Notes
tak
en
b
y
Sergey
Benditkis,
Boris
T
emkin
and
Il'y
a
Safro
Summary:
Pseudorandom
generators
are
dened
as
ecien
t
deterministic
algorithms
whic
h
stretc
h
short
random
seeds
in
to
longer
pseudorandom
sequences.
The
latter
are
indistiguishable
from
truely
random
sequences
b
y
an
y
ecien
t
observ
er.
W
e
sho
w
that,
for
ecien
tly
sampleable
distributions,
computational
indistiguishabilit
y
is
preserv
ed
under
m
ultiple
samples.
W
e
related
pseudorandom
generators
and
one-w
a
y
functions,
and
sho
w
ho
w
to
increase
the
stretc
hing
of
pseudorandom
generators.
.
Instead
of
an
in
tro
duction
Oded's
Note:
See
in
tro
duction
and
motiv
ation
in
the
App
endix.
Actually
,
it
is
recom-
mended
to
read
the
app
endix
b
efore
reading
the
follo
wing
notes,
and
to
refer
to
the
notes
only
for
full
details
of
some
statemen
ts
made
in
Sections
..
and
..
of
the
app
endix.
Oded's
Note:
Lo
osely
sp
eaking,
pseudorandom
generators
are
dened
as
ecien
t
de-
terministic
algorithms
whic
h
stretc
h
short
random
seeds
in
to
longer
pseudorandom
se-
quences.
W
e
stress
three
asp
ects:
()
the
eciency
of
the
generator;
()
the
stretc
hing
of
seeds
to
longer
strings;
()
the
pseudorandomness
of
output
sequences.
The
third
asp
ect
refers
to
the
k
ey
notion
of
computational
indistinguishabilit
y
.
W
e
start
with
a
denition
and
discussion
of
the
latter.
.
Computational
Indistinguishabilit
y
W
e
ha
v
e
t
w
o
things
whic
h
are
called
"probabilit
y
ensem
bles",
and
denoted
b
y
fX
n
g
nN
and
fY
n
g
nN
.
W
e
are
talking
ab
out
innite
sequences
of
distributions
lik
e
w
e
talk
ab
out
the
lan-
guage
and
eac
h
distribution
seats
on
some
nal
domain.
T
ypicaly
the
distribution
X
n
will
ha
v
e
as
a
supp
ort
strings
of
length
p
olynomial
of
n,
not
more
and
not
m
uc
h
less.
Denition
.
(probabilit
y
ensem
bles):
A
p
robabilit
y
ensemble
X
is
a
family
X
=
fX
n
g
n
such
that
X
n
is
a
pr
ob
ability
distribution
on
some
nite
domain.
What
is
to
sa
y
that
these
ensem
bles
are
computational
indistinguishable?
W
e
w
an
t
to
lo
ok
at
the
particular
algorithm
A
and
w
an
t
to
ask
what
is
the
probabilit
y
of
the
ev
en
t:
when
y
ou
giv
e
to
A



LECTURE
.
PSEUDORANDOM
GENERA
TORS
an
input
X
n
then
it
sa
ys

(or

is
just
arbitrary)
and
lo
ok
at
the
dierence
of
the
probabilities
for
answ
ers
of
execution
this
algorithm
A
for
t
w
o
inputs
fX
n
g
nN
and
fY
n
g
nN
.
And
if
this
dierence
is
negligible,
when
y
ou
lo
ok
at
n
as
the
parameter
then
w
e
will
sa
y
that
w
e
can
not
distinguish
the
rst
ensam
ble
from
the
second
one.
Denition
.
(negligible
functions):
The
function
f
:
N
!
[0;
]
is
negligible
if
for
al
l
p
olyno-
mials
p,
and
for
al
l
suciently
lar
ge
n's,
f
(n)
<
=p(n)
.
Supp
ose
w
e
ha
v
e
t
w
o
pr
ob
ability
ensambles
fX
n
g
nN
and
fY
n
g
nN
,
where
X
n
and
Y
n
are
dis-
tributions
o
v
er
some
nite
domain.
Denition
.
(indistinguishabilit
y
b
y
a
sp
ecic
algorithm):
Consider
some
pr
ob
abilistic
algo-
rithm
A.
We
wil
l
say
that
fX
n
g
and
fY
n
g
ar
e
indistinguishable
b
y
A
if
jPr(A(X
n
)
=
)
 Pr
(A(Y
n
)
=
)
j
<

p(n)
for
every
p
olinomial
p()
and
for
every
suciently
lar
ge
n.
..
Tw
o
v
arian
ts
Our
main
fo
cus
will
b
e
on
indistinguishabilit
y
b
y
an
y
probabilistic
p
olynomial-time
algorithm.
That
is,
Denition
.
(canonic
notion
of
computational
indistinguishabilit
y):
Two
pr
ob
ability
ansam-
bles
fX
n
g
nN
and
fY
n
g
nN
ar
e
computationally
indistinguishable
if
they
ar
e
indistinguishable
by
any
pr
ob
abilistic
p
olynomial-time
algorithm.
That
is,
for
every
pr
ob
abilistic
p
olynomial-time
algorithm
A,
and
every
p
olinomial
p()
ther
e
exists
N
s.t.
for
al
l
n
>
N
jPr(A(X
n
)
=
)
 Pr
(A(Y
n
)
=
)
j
<

p(n)
Another
notion
that
w
e
talk
ab
out
is
indistinguhabilit
y
b
y
circuits.
Denition
.
Two
pr
ob
ability
ensembles
fX
n
g
nN
and
fY
n
g
nN
ar
e
indistinguishable
b
y
small
circuits
if
for
al
l
families
of
p
olynomial-size
cir
cuits
fC
n
g
j
[
Pr
(C
n
(X
n
)
=
)
 Pr
(C
n
(Y
n
)
=
)
]
j
is
ne
glibible.
..
Relation
to
Statistical
Closeness
Oded's
Note:
This
subsection
w
as
rewritten
b
y
me.
The
notion
of
Computational
Indistinguishabilit
y
is
a
relaxation
of
the
notion
of
statistic
al
closeness
(or
statistic
al
indistinguishability).

..
COMPUT
A
TIONAL
INDISTINGUISHABILITY

Denition
.
(statistical
closeness):
The
statistical
dierence
(or
v
ariation
distance)
b
etwe
en
two
distributions,
X
and
Y
,
is
dene
d
by
(X
;
Y
)
def
=



X

jPr[X
=
]
 Pr
[Y
=
]j
Two
pr
ob
ability
ensembles
fX
n
g
nN
and
fY
n
g
nN
ar
e
statistical
close
if
(X
n
;
Y
n
)
is
a
ne
gligible
function
of
n.
That
is,
for
al
l
p
olynomial
p()
ther
e
exists
N
s.t.
for
al
l
n
>
N
(X
n
;
Y
n
)
<
=p(n)
.
An
equiv
alen
t
denition
of
(X
n
;
Y
n
),
is
the
maxim
um
o
v
er
all
subsets,
S
,
of
Pr
[X
n

S
]
 Pr
[Y
n

S
].
(A
set
S
whic
h
obtains
the
maxim
um
is
the
set
of
all
z
's
satisfying
Pr[X
n
=
z
]
>
Pr[Y
n
=
z
],
whic
h
pro
v
es
the
equiv
alence.)
Y
et
another
equiv
alen
t
denition
of
(X
n
;
Y
n
)
is
the
maxim
um
o
v
er
all
Bo
olean
f
's
of
Pr[f
(X
n
)
=
]
 Pr[f
(Y
n
)
=
].
Th
us,
Prop
osition
..
If
two
pr
ob
ability
ensembles
ar
e
statistic
al
close
then
they
ar
e
c
omputational
ly
indistinguishable.
W
e
note
that
there
are
computationally
indistinguishable
probabilit
y
ensem
bles
whic
h
are
not
statistical
close.
..
Computational
indistinguishabilit
y
and
m
ultiple
samples
Oded's
Note:
W
e
sho
w
that
under
certain
conditions,
computational
indistinguishabilit
y
is
preserv
ed
under
m
ultiple
samples.
Denition
.
(constructabilit
y
of
ensem
bles):
The
ensemble
fZ
n
g
nN
is
p
robabilistic
p
olynomial-
time
constructable
if
ther
e
exists
a
pr
ob
abilistic
p
olynomial
time
algorithm
S
such
that
for
every
n,
S
(
n
)

Z
n
.
Theorem
.
L
et
fX
n
g
and
fY
n
g
c
omputational
ly
indistinguishable
(i.e.,
indistinguishable
by
any
pr
ob
abilistic
p
olynomial
time
algorithm).
Supp
ose
they
ar
e
b
oth
pr
ob
abilistic
p
olynomial
time
c
onstructable.
L
et
t()
b
e
a
p
ositive
p
olinomial.
Dene
fX
n
g
nN
and
fY
n
g
nN
in
the
fol
lowing
way:
X
n
=
X

n

X

n

:::

X
t(n)
n
;
Y
n
=
Y

n

Y

n

:::

Y
t(n)
n
The
X
i
n
's
(r
esp.
Y
i
n
's)
ar
e
indep
enden
t
c
opies
of
X
n
(Y
n
).
Then
fX
n
g
and
fY
n
g
ar
e
Pr
ob
abilistic
Polynomial
Time
indistinguishable.
Pro
of:
Supp
ose,
there
exists
a
distinguisher
D
,
b
et
w
een
fX
n
g
and
fY
n
g.
Oded's
Note:
W
e
use
the
\h
ybrid
tec
hnique":
W
e
dene
h
ybrid
distributions
so
that
the
extreme
h
ybrids
coincide
with
fX
n
g
and
fY
n
g,
and
link
distinguishabilit
y
of
neigh
b
oring
h
ybrids
to
distinguishabilit
y
of
fX
n
g
and
fY
n
g.
Then
dene
H
(i)
n
=

X
()
n

X
()
n

:::
X
(i)
n

Y
(i+)
n

:::
Y
(t(n))
n

It
is
easy
to
see
that
H
(0)
n
=
Y
n
,
H
(t(n))
n
=
X
n
.
Oded's
Note:
Also
note
that
H
(i)
n
and
H
(i+)
n
dier
only
in
the
distribution
of
the
i
+

st
comp
onen
t,
whic
h
is
iden
tical
to
Y
n
in
the
rst
h
ybrid
and
to
X
n
in
the
second.
The
idea
is
to
distinguish
Y
n
and
X
n
b
y
pluging
them
in
the
i
+

st
comp
onen
t
of
a
distribution.
The
new
distribution
will
b
e
distributed
iden
tically
to
either
H
(i)
n
or
H
(i+)
n
,
resp
ectiv
ely
.


LECTURE
.
PSEUDORANDOM
GENERA
TORS
Dene
algorithm
D
0
as
follo
ws:
Begin
Algorithm
Distinguisher
Input

,
(tak
en
from
X
n
or
Y
n
)
()
Cho
ose
i
R
f
::
t(n)g
(i.e.,
uniformly
in
f;
:::;
t(n)g)
()
Construct
Z
=

X
()
n

X
()
n

:::
X
(i )
n



Y
(i+)
n

:::
Y
(t(n))
n

Return
D
(Z
)
end.
Pr

D
0
(X
n
)
=


=

t(n)
t(n)
X
i=
Pr
h
D
(X
()
n

X
()
n

:::
X
(i )
n

X
n

Y
(i+)
n

:::
Y
(t(n))
n
)
=

i
=

t(n)
t(n)
X
i=
Pr
h
D
(H
(i)
n
)
=

i
whereas
Pr

D
0
(Y
n
)
=


=

t(n)
t(n)
X
i=
Pr
h
D
(X
()
n

X
()
n

:::
X
(i )
n

Y
n

Y
(i+)
n

:::
Y
(t(n))
n
)
=

i
=

t(n)
t(n)
X
i=
Pr
h
D
(H
(i )
n
)
=

i
:
Th
us,
jPr

D
0
(X
n
)
=


 Pr

D
0
(Y
n
)
=


j
=

t(n)







t(n)
X
i=
Pr
[D
(H
(i)
n
)
=
]
 t(n)
X
i=
Pr
[D
(H
(i )
n
)
=
]






=

t(n)




Pr[D
(H
(t(n))
n
)
=
]
 Pr[D
(H
(0)
n
)
=
]



=

t(n)

jPr
[D
(X
n
)
=
]
 Pr[D
(Y
n
)
=
]j


t(n)

p(n)
for
some
p()
and
for
innitely
man
y
n's
Oded's
Note:
One
can
easily
sho
w
that
computational
indistinguishabilit
y
b
y
small
circuits
is
preserv
ed
under
m
ultiple
samples.
Here
w
e
don't
need
to
assume
probabilistic
p
olynomial-time
constructabilit
y
of
the
ensem
bles.
.
PR
G:
Denition
and
amplication
of
the
stretc
h
function
In
tuitiv
ely
,
a
pseudo-random
generator
tak
es
a
short,
truly-random
string
and
stretc
hes
it
in
to
a
long,
pseudorandom
one.
The
pseudorandom
string
should
lo
ok
\random
enough"
to
use
it
in
place
of
a
truly
random
string.

..
PR
G:
DEFINITION
AND
AMPLIFICA
TION
OF
THE
STRETCH
FUNCTION
	
Denition
.	
(PseudoRandom
Generator
{
PR
G):
The
function
G
:
f0;
g

!
f0;
g

with
str
etch
function
l
(n)
is
a
pseudo-random
generato
r
if:

G
is
a
p
olynomial
time
algorithm

for
every
x,
jG(x)j
=
l
(jxj)
>
jxj

fG(U
n
)g
and
fU
l
(n)
g
ar
e
c
omputational
indistinguishable,
wher
e
U
m
denotes
the
uniform
distribution
over
f0;
g
m
.
Oded's
Note:
The
ab
o
v
e
denition
is
minimalistic
regarding
its
stretc
h
requiremen
t.
A
generator
stretc
hing
n
bits
in
to
n
+

bits
seems
to
b
e
of
little
use.
Ho
w
ev
er,
as
sho
wn
next,
suc
h
minimal
stretc
h
generators
can
b
e
used
to
construct
generators
of
arbitrary
stretc
h.
Theorem
.0
(amplication
of
stretc
h
function):
Supp
ose
we
have
a
Pseudo-R
andom
Gener
a-
tor
G

with
a
str
etch
function
n
+
.
Then
for
al
l
p
olynome
l
(n)
ther
e
exists
a
Pseudo-R
andom
Gener
ator
with
str
etch
function
l
(n).
Pro
of:
Construct
G
as
follo
ws:
W
e
tak
e
the
input
seed
x
(jxj
=
n)
and
feed
it
through
G

.
Then
w
e
sa
v
e
the
rst
bit
of
the
output
of
G

(denote
it
b
y
y

),
and
feed
the
rest
of
the
bits
as
input
to
a
new
in
v
o
cation
of
G

.
W
e
rep
eat
this
op
eration
l
(n)
times,
in
the
i-th
step
w
e
in
v
ok
e
G

on
input
determined
in
the
previous
step,
sa
v
e
the
rst
output
bit
as
y
i
and
use
the
rest
n
bits
as
an
input
to
step
i
+
.
The
output
of
G
is
y
=
y

y

:::
y
l
(n)
.
See
Figure
.
X
G1
G1
.......
y1
y2
n
1
1
n
n
l(n) times
Figure
.
W
e
claim
that
G
is
a
Pseudo-Random
Generator.
The
rst
t
w
o
requiremen
ts
for
Pseudo-
Random
Generator
are
trivial
(b
y
construction/denition
of
G).
W
e
will
pro
v
e
the
rd
one.
The
pro
of
is
b
y
con
tradiction,
again
using
the
h
ybrid
metho
d.
Supp
ose
there
exists
a
distinguisher
A
:
f0;
g

!
f0;
g
l
(n)
suc
h
that
exists
p
olynomial
p()
and
for
innitely
nam
y
n's
j
Pr
[
A(G(U
n
))
=

]
 Pr
h
A(U
l
(n)
)
=

i
j


p(n)
Let
us
mak
e
the
follo
wing
construction
.
Dene
sequence
of
functions
g
(i)
:
g
(0)
is
empt
y
g
(i)
=
[G

(x)]


g
(i )
([G

(x)]
:::(n+)
)
Where
[y
]
i
is
the
notation
of
i-th
bit
of
y
and
[y
]
:::(n+)
denotes
substring
of
y
from
the
second
bit
up
to
n
+
-th
bit.
It
is
easy
to
see
that
g
l
(n)
=
G(x).

0
LECTURE
.
PSEUDORANDOM
GENERA
TORS
Construct
the
class
of
h
ybrid
distributions
fH
i
g
l
(n)
i=
:
H
i
=
U
l
(n) i

g
i
(U
n
)
One
can
ob
eserv
e
that
H
0
=
G(U
n
),
and
H
l
(n)
=
U
l
(n)
.
No
w
w
e
construct
the
distinguisher
D
as
follo
ws:
Begin
Algorithm
Distinguisher
Input

,
jj
=
n
+

(tak
en
from
G

(U
n
)
on
U
n+
)
()
Cho
ose
i
R
f
::
l
(n)g
()
Cho
ose
Z

U
l
(n) i
()
Construct
y
=
Z



g
(i )
(S
),
where

is
rst
bit
of

and
S
its
n-bit
sux
Return
A(y
)
end.
W
e
denote
b
y
Pr
[Aji]
the
c
onitional
pr
ob
ability
of
ev
en
t
A
if
particular
i
w
as
c
ho
osen
in
step
()
of
algorithm
D
.
W
e
see
that
Pr
[
D
(G

(U
n
))
=

]
 Pr
[
D
(U
n+
)
=

]
=

l
(n)
l
(n)
X
i=
(
Pr
[
D
(G

(U
n
))
=

ji]
 Pr
[
D
(U
n+
)
=

ji]
)
:
()
Note
that
Pr
[
D
(G

(U
n
))
=
ji]
=
Pr
h
A(Z
::(l
(n) i)

[G

(U
n
)]


g
(i )
([
G

(U
n
)]
(::n+)
)
=

i
=
Pr
h
A(H
i
)
=

i
and
Pr
[
D
(U
n+
)
=
ji]
=
Pr
h
A(Z
::(l
(n) i)

[U
n+
]


g
(i )
([U
n+
]
::n+
))
=

i
=
Pr
h
A(H
i 
)
=

i
So
equation
()
is

l
(n)
l
(n)
X
i=

Pr
h
A(H
i
)
=

i
 Pr
h
A(H
i 
)
=

i
=

l
(n)

Pr
h
A(H
l
(n)
)
=

i
 Pr
h
A(H
0
)
=

i

=

l
(n)

Pr
[
A(G(U
n
))
=

]
 Pr
h
A(U
l
(n)
)
=

i

so
Pr
[
D
(G

(U
n
))
=

]
 Pr
[
D
(U
n+
)
=

]


l
(n)p(n)

..
ON
USING
PSEUDO-RANDOM
GENERA
TORS

.
On
Using
Pseudo-Random
Generators
Supp
ose
w
e
ha
v
e
a
probabilistic
p
olynomial
time
algorithm
A,
whic
h
on
input
of
length
n
uses
m(n),
random
bits.
Algorithm
A
ma
y
solv
e
either
searc
h
problem
for
some
relation
or
decision
problem
for
some
language
L.
Our
claim
will
b
e
that
for
all
"
>
0
there
exists
a
probabilistic
p
olynomial
time
algorithm
A
0
that
uses
only
n
"
random
bits
and
\b
eha
v
es"
in
the
same
w
a
y
that
A
do
es.
The
construction
of
A
0
bases
on
assumption
that
w
e
are
giv
en
pseudo-random
generator
G
:
f0;
g
n
"
!
f0;
g
m(n)
.
Recall
that
A(x;
R
)
that
A
is
running
on
input
x
with
coins
R
.
Algorithm
A'
Input
x

f0;
g
n
Cho
ose
S

R
f0;
g
n
"
R
 
G(s)
(generate
the
coin
tosses)
Return
A(x;
R
)
(run
A
on
input
x
using
coins
R
)
end.
Prop
osition
..
(imformal):
It
is
infe
asible
given

n
to
nd
x

f0;
g
n
,
such
that
the
\b
e-
haviour"
of
A
0
(x)
is
substantial
ly
difer
ent
fr
om
A(x).
The
meaning
of
this
prop
osition
dep
ends
on
the
computational
problem
solv
ed
b
y
A.
In
case
A
solv
es
some
NP-searc
h
problem,
the
prop
osition
asserts
that
it
is
hard
(i.e.,
feasible
only
with
negligable
probabilit
y)
to
nd
large
x's
suc
h
that
A
can
nd
the
solution
for
x,
and
A
0
(x)
will
fail
to
do
so.
In
case
A
computes
some
function
the
prop
osition
applies
to
o.
Oded's
Note:
But
the
prop
osition
ma
y
not
hold
if
A
solv
es
a
searc
h
problem
in
whic
h
instances
ha
v
e
man
y
p
ossible
solutions
and
are
not
ecien
tly
v
eriable
(as
in
NP-searc
h
problems).
Belo
w
w
e
pro
v
e
the
prop
osition
for
the
case
of
decision
problems
(and
the
pro
of
actually
extends
to
an
y
function
computation).
W
e
assume
that
A
giv
es
the
correct
answ
er
with
probabilit
y
b
ounded
a
w
a
y
from
=.
Pro
of:
Supp
ose
w
e
ha
v
e
a
nder
F
,
whic
h
w
orks
in
p
olynomial
time
,
F
(
n
)
=
x

f0;
g
n
,
suc
h
that
Pr

A
0
(x)
=
X
L
(x)




where
X
L
is
the
c
haracteristic
function
of
a
language
decideable
b
y
A
(i.e.,
Pr[A(x)
=
X
L
(x)]

=
for
all
x's).
Construct
a
distinguisher
D
as
follo
ws:
Begin
Algorithm
D
Input


f0;
g
m(n)
x
 
F
(
n
)
v
 
X
L
(x)
with
o
v
erwhelmingly
high
probabilit
y
(i.e.,
in
v
ok
e
A(x)
p
olynomially
man
y
times
and
tak
e
a
ma
jorit
y
v
ote).
w
 
A(x;
)
If
v
=
w
Then
Return

Else
Return
0


LECTURE
.
PSEUDORANDOM
GENERA
TORS
end.
D
is
con
tradicts
the
pseudorandomness
of
G
b
ecause
A(x;
)
=
(
X
L
(x);
w
:p:



;


U
m(n)
X
L
(x)
=
A
0
(x);
w
:p:



;


G(U
n
"
)
(.)
F
urthermore,
with
probabilit
y
at
least
0:		,
the
v
alue
v
found
b
y
D
(x)
equals
X
L
(x).
Th
us,
Pr
[D
(U
m(n)
)
=
]
>
0:
 0:0
=
0:
Pr[D
(G(U
n

))
=
]

0:
+
0:0
<
0:
whic
h
pro
vides
the
required
gap.
Oded's
Note:
Note
that
for
a
strong
notion
of
pseudorandom
generators,
where
the
output
is
indistinguishable
from
random
b
y
small
circuits
w
e
can
pro
v
e
a
stronger
result;
that
is,
that
there
are
only
nitely
man
y
x's
on
whic
h
A
0
b
eha
v
es
dieren
tly
than
A.
Th
us,
in
case
of
decision
algorithms,
b
y
minor
mo
dication
to
A
0
,
w
e
can
mak
e
A
0
accept
the
same
language
as
A.
.
Relation
to
one-w
a
y
functions
An
imp
ortan
t
prop
ert
y
of
a
pseudo-random
generator
G(S
)
that
it
turns
the
seed
in
to
the
sequence
x
=
G(S
)
in
p
olynomial
time.
But
the
in
v
erse
op
eration
of
nding
the
seed
S
from
G(S
)
w
ould
b
e
hard
(or
else
pseudorandomness
is
violated
as
sho
wn
b
elo
w).
A
pseudo-random
generator
is
ho
w
ev
er,
not
just
a
function
that
hard
to
in
v
ert
it
also
stretc
hes
the
input
in
to
the
larger
sequence
that
lo
ok
random.
Still
pseudo-random
generators
can
b
e
related
to
functions
whic
h
are
\only"
easy
to
compute
and
hard
to
in
v
ert,
as
dened
next.
Denition
.
(One-w
a
y
functions
{
O
WF):
A
function
f
:
f0;
g

 !
f0;
g

such
that
x
jf
(x)j
=
jxj
is
one-w
a
y
if
:

ther
e
is
exists
p
olynomial
time
algorithm
A,
such
that
x
A(x)
=
f
(x)

for
al
l
pr
ob
abilistic
p
olynomial
time
A
0
and
for
al
l
p
olynome
p()
and
for
al
l
suciently
lar
ge
n
's
:
Pr
h
A
0
(f
(U
n
))
=
f
 

f
(U
n
)
i
<

p(n)
In
other
w
ords
this
function
m
ust
b
e
easy
computed
and
hard
in
v
erted.
Note
an
imp
ortan
t
feauture:
the
in
v
ersion
algotihm
m
ust
fail
almost
alw
a
ys.
But
the
probabilit
y
distribution
used
here
is
not
uniform
o
v
er
all
f
(x);
rather,
it
is
the
distribution
f
(x)
when
x
is
c
ho
osen
uniformly
.
Oded's
Note:
The
requiremen
t
that
the
function
b
e
length
preserving
(i.e.,
jf
(x)j
=
jxj
for
all
x's)
ma
y
b
e
relaxed
as
long
as
the
length
of
f
(x)
is
p
olynomially
related
to
jxj.
In
con
trast
a
function
lik
e
f
(x)
def
=
jxj
w
ould
b
e
\one-w
a
y"
for
a
trivial
and
useless
reason
(on
input
n
in
binary
one
cannot
prin
t
an
n-bit
string
in
p
olynomial
(in
log
n)
time).

..
RELA
TION
TO
ONE-W
A
Y
FUNCTIONS

Commen
t:
A
p
opular
candidate
to
b
e
one-w
a
y
function
is
based
on
the
conjectured
in
tractabilit
y
of
the
in
teger
factorization
problem.
The
length
of
input
and
output
to
the
function
will
not
b
e
exactly
n,
only
p
olynomial
in
n:
The
factoring
problem.
Let
x;
y
>

b
e
n-bit
in
tegers.
Dene
f
(x;
y
)
=
x

y
When
x;
y
are
n-bit
primes,
it
is
b
eliev
ed
that
nding
x;
y
from
x

y
is
computationaly
dicult.
Oded's
Note:
So
the
ab
o
v
e
should
b
e
hard
to
in
v
ert
in
these
cases
whic
h
o
ccur
at
densit
y

=n

.
This
do
es
not
satisfy
the
denition
of
one-w
a
yness
whic
h
requires
hardness
of
in
v
ersion
almost
ev
erywhere,
but
suitable
amplication
can
get
us
there.
Alternativ
ely
,
w
e
can
redene
the
function
f
so
that
f
(x;
y
)
=
pr
ime(x)

pr
ime(y
),
where
pr
ime()
is
a
pro
cedure
whic
h
uses
the
input
string
to
generate
a
large
prime
so
that
when
the
input
is
a
random
n-bit
string
the
output
is
a
random
n=O
()-bit
prime.
Suc
h
ecien
t
pro
cedures
are
kno
wn
to
exist.
Using
less
sophisticated
metho
ds
one
can
easily
construct
a
pro
cedure
whic
h
uses
n-bits
to
pro
duce
a
prime
of
length

p
n
=O
().
Theorem
.
Pseudo-R
andom
Gener
ators
exist
if
and
only
if
One-Way
F
unctions
exist.
So
the
computational
hardness
and
pseudorandomness
are
strongly
connect
eac
h
other.
If
w
e
ha
v
e
the
created
randomness
w
e
can
create
the
hardness,
and
vice
v
ersa.
Let
us
pro
v
e
one
part
of
the
theorem
and
giv
e
hin
ts
to
sp
ecial
case
of
other.
PR
G
=
)
O
WF:
Consider
pseudo-random
generator
G
:
f0;
g
n
!
f0;
g
n
.
Let
us
dene
function
f
:
f0;
g
n
!
f0;
g
n
as
follo
ws:
f
(xy
)
=
G(x)
(jxj
=
jy
j
=
n):
W
e
claim,
that
f
is
one-w
a
y
function,
and
the
pro
of
is
b
y
con
tradiction
:
Supp
ose
probabilistic
p
olynomial
time
algorithm
A
0
in
v
erts
f
with
success
probabilit
y
greater
than

p(n)
,
where
p(n)
is
p
olynom.
Consider
a
distinguisher
D
:
input:
,


f0;
g
n
xy
 
A
0
()
if
f
(xy
)
=

return

otherwise
return
0.
Pr
[
D
(G(U
n
))
=
]
=
Pr
[D
(f
(U
n
))
=
]
=
Pr

f
(A
0
(f
(U
n
)))
=
f
(U
n
)

=
Pr
h
A
0
(f
(U
n
))

f
 
f
(U
n
)
i
>

p(n)
where
the
last
inequalit
y
is
due
to
the
con
tradiction
h
yp
othesis.
On
the
other
hand,
there
are
at
most

n
strings
of
length
n
whic
h
ha
v
e
a
preimage
under
G
(and
so
under
f
).
Th
us,
a
uniformly


LECTURE
.
PSEUDORANDOM
GENERA
TORS
selected
random
string
of
length
n
has
a
preimage
under
f
with
probabilit
y
at
most

n
=
n
.
It
follo
ws
that
Pr
[
D
(U
n
)
=
]
=
Pr

f
(A
0
(U
n
))
=
U
n


Pr[U
n
is
in
the
image
of
f
]


n

n
=

 n
Th
us,
Pr
[D
(G(U
n
))
=
]
 Pr
[D
(U
n
)
=
]
>

p(n)
 

n
>

q
(n)
F
or
some
p
olynome
q
()
O
WF
=
)
PR
G:
Oded's
Note:
The
rest
of
this
section
is
an
o
v
erview
of
what
is
sho
wn
in
detail
in
the
next
lecture
(i.e.,
Lecture
).
Let
us
demonstrate
the
other
direction
and
build
an
Pseudo-Random
Generator
if
w
e
ha
v
e
O
WF
of
sp
ecial
form.
Supp
ose
the
function
f
:
f0;
g
n
!
f0;
g
n
is
not
only
O
WF
but
it
is
also

 .
So
it
is
a
p
erm
utation
of
strings
of
length
n.
Assume
that
w
e
can
get
a
random
bit
b
from
the
input,
suc
h
that
b
will
b
e
hard
to
\predict"
from
the
output
of
f
.
In
this
case
w
e
can
construct
a
Pseudo-Random
Generator
as
a
concatenation
of
f
(x)
and
b.
Denition
.
(Hardcore):
L
et
f
b
e
one-way
function,
b
:
f0;
g

 !
f0;
g
is
a
har
dc
or
e
of
f
if:

	
p
olinomial
time
algorithm
A,
such
that
tA(t)
=
b(t)


pr
ob
abilistic
p
olynomial
time
algorithm
A
0

p
olynom
p(:)

suciently
lar
ge
n
0
s
Pr

A
0
(f
(U
n
))
=
b(U
n
)

<


+

p(n)
In
other
wor
ds
this
function
must
b
e
e
asy
to
c
ompute
and
har
d
to
pr
e
dict
out
of
f
(x).
The
follo
wing
theorem
can
b
e
pro
v
en:
Theorem
.
If
f
is
O
W,
f
0
(x;
y
)
=
f
(x)

y
;
(jxj
=
jy
j)
then
b(x;
y
)
=
P
n
i=
x
i
y
i
(mod
)
is
a
har
dc
or
e
of
f
.
This
theorem
w
ould
b
e
pro
v
en
in
next
lecture.
No
w
w
e
can
construct
a
Pseudo-Random
Gen-
erator
G
as
follo
ws:
G(s)
=
f
0
(s)

b(s)
The
t
w
o
rst
prop
erties
of
G
(p
oly-time
and
stretc
hing)
are
trivial.
The
pseudorandomness
of
G
follo
ws
from
the
fact
that
its
rst
n
output
bits
are
uniformly
distributed
and
the
last
bit
is
unpredictable.
Unpredictabilit
y
translates
to
indistinguishabilit
y
,
as
will
b
e
sho
wn
in
the
next
lecture.

..
RELA
TION
TO
ONE-W
A
Y
FUNCTIONS

Bibliographic
Notes
The
notion
of
computational
indistinguishabilit
y
w
as
in
tro
duced
b
y
Goldw
asser
and
Micali
[]
(within
the
con
text
of
dening
secure
encryptions),
and
giv
en
general
form
ulation
b
y
Y
ao
[].
Our
denition
of
pseudorandom
generators
follo
ws
the
one
of
Y
ao,
whic
h
is
equiv
alen
t
to
a
prior
form
ulation
of
Blum
and
Micali
[].
F
or
more
details
regarding
this
equiv
alence,
as
w
ell
as
man
y
other
issues,
see
[].
The
latter
source
presen
ts
the
notion
of
pseudorandomness
discussed
here
as
a
sp
ecial
case
(or
arc
het
ypical
case)
of
a
general
paradigm.
The
disco
v
ery
that
computational
hardness
(in
form
of
one-w
a
yness)
can
b
e
turned
in
to
a
pseudorandomness
w
as
made
in
[].
Theorem
.
(asserting
that
psedorandom
generators
can
b
e
constructed
based
on
an
y
one-w
a
y
function)
is
due
to
[].
It
uses
Theorem
.
whic
h
is
due
to
[].
.
M.
Blum
and
S.
Micali.
Ho
w
to
Generate
Cryptographically
Strong
Sequences
of
Pseudo-
Random
Bits.
SICOMP,
V
ol.
,
pages
0{,
	.
Preliminary
v
ersion
in
r
d
F
OCS,
	.
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.
.
O.
Goldreic
h
and
L.A.
Levin.
Hard-core
Predicates
for
an
y
One-W
a
y
F
unction.
In
st
STOC,
pages
{,
		.
.
S.
Goldw
asser
and
S.
Micali.
Probabilistic
Encryption.
JCSS,
V
ol.
,
No.
,
pages
0{		,
	.
Preliminary
v
ersion
in
th
STOC,
	.
.
J.
H

astad,
R.
Impagliazzo,
L.A.
Levin
and
M.
Lub
y
.
Construction
of
Pseudorandom
Gener-
ator
from
an
y
One-W
a
y
F
unction.
T
o
app
ear
in
SICOMP.
Preliminary
v
ersions
b
y
Impagli-
azzo
et.
al.
in
st
STOC
(		)
and
H

astad
in
nd
STOC
(		0).
.
A.C.
Y
ao.
Theory
and
Application
of
T
rap
do
or
F
unctions.
In
r
d
F
OCS,
pages
0{	,
	.


PSEUDORANDOM
GENERA
TORS
Oded's
Note:
Being
in
the
pro
cess
of
writing
an
essa
y
on
pseudorandomness,
it
feels
a
go
o
d
idea
to
augmen
t
the
notes
of
the
curren
t
lecture
b
y
a
draft
of
this
essa
y
.
The
lecture
notes
actually
expand
on
the
presen
tation
in
Sections
..
and
...
The
other
sections
in
this
essa
y
go
b
ey
ond
the
lecture
notes.
App
endix:
An
essa
y
b
y
O.G.
Summary:
W
e
p
ostulate
that
a
distribution
is
pseudorandom
if
it
cannot
b
e
told
apart
from
the
uniform
distribution
b
y
an
ecien
t
pro
cedure.
This
yields
a
robust
denition
of
pseudorandom
generators
as
ecien
t
deterministic
programs
stretc
hing
short
random
seeds
in
to
longer
pseudoran-
dom
sequences.
Th
us,
pseudorandom
generators
can
b
e
used
to
reduce
the
randomness-complexit
y
in
an
y
ecien
t
pro
cedure.
Pseudorandom
generators
and
computational
dicult
y
are
strongly
related:
lo
osely
sp
eaking,
eac
h
can
b
e
ecien
tly
transformed
in
to
the
other.
..
In
tro
duction
The
second
half
of
this
cen
tury
has
witnessed
the
dev
elopmen
t
of
three
theories
of
randomness,
a
notion
whic
h
has
b
een
puzzling
think
ers
for
ages.
The
rst
theory
(cf.,
[]),
initiated
b
y
Shan-
non
[],
is
ro
oted
in
probabilit
y
theory
and
is
fo
cused
at
distributions
whic
h
are
not
p
erfectly
random.
Shannon's
Information
Theory
c
haracterizes
p
erfect
randomness
as
the
extreme
case
in
whic
h
the
information
c
ontent
is
maximized
(and
there
is
no
redundancy
at
all).
Th
us,
p
erfect
randomness
is
asso
ciated
with
a
unique
distribution
{
the
uniform
one.
In
particular,
b
y
denition,
one
cannot
generate
suc
h
p
erfect
random
strings
from
shorter
random
seeds.
The
second
theory
(cf.,
[,
]),
due
to
Solomono
v
[],
Kolmogoro
v
[]
and
Chaitin
[],
is
ro
oted
in
computabilit
y
theory
and
sp
ecically
in
the
notion
of
a
univ
ersal
language
(equiv.,
univ
ersal
mac
hine
or
computing
device).
It
measures
the
complexit
y
of
ob
jects
in
terms
of
the
shortest
program
(for
a
xed
univ
ersal
mac
hine)
whic
h
generates
the
ob
ject.
Lik
e
Shannon's
the-
ory
,
Kolmogoro
v
Complexit
y
is
quan
titativ
e
and
p
erfect
random
ob
jects
app
ear
as
an
extreme
case.
In
terestingly
,
in
this
approac
h
one
ma
y
sa
y
that
a
single
ob
ject,
rather
than
a
distribution
o
v
er
ob-
jects,
is
p
erfectly
random.
Still,
Kolmogoro
v's
approac
h
is
inheren
tly
in
tractable
(i.e.,
Kolmogoro
v
Complexit
y
is
uncomputable),
and
{
b
y
denition
{
one
cannot
generate
strings
of
high
Kolmogoro
v
Complexit
y
from
short
random
seeds.
The
third
theory
,
initiated
b
y
Blum,
Goldw
asser,
Micali
and
Y
ao
[,
,
],
is
ro
oted
in
complexit
y
theory
and
is
the
fo
cus
of
this
essa
y
.
This
approac
h
is
explicitly
aimed
at
pro
viding
a
notion
of
p
erfect
randomness
whic
h
nev
ertheless
allo
ws
to
ecien
tly
generate
p
erfect
random
strings
from
shorter
random
seeds.
The
heart
of
this
approac
h
is
the
suggestion
to
view
ob
jects
as
equal
if
they
cannot
b
e
told
apart
b
y
an
y
ecien
t
pro
cedure.
Consequen
tly
a
distribution
whic
h
cannot
b
e
ecien
tly
distinguished
from
the
uniform
distribution
will
b
e
considered
as
b
eing
random
(or
rather
called
pseudorandom).
Th
us,
randomness
is
not
an
\inheren
t"
prop
ert
y
of
ob
jects
(or
distributions)
but
rather
relativ
e
to
an
observ
er
(and
its
computational
abilities).
T
o
demonstrate
this
approac
h,
let
us
consider
the
follo
wing
men
tal
exp
erimen
t.
Alice
and
Bob
pla
y
\head
or
tail"
in
one
of
the
follo
wing
four
w
a
ys.
In
all
of
them
Alice
ips
a
coin
high
in
the
air,
and
Bob
is
ask
ed
to
guess
its
outcome
b
efor
e
the
coin
hits
the
o
or.
The
alternativ
e
w
a
ys
dier
b
y
the
kno
wledge
Bob
has
b
efore
making
his
guess.
In
the
rst
alternativ
e,
Bob
has
to
announce
his
guess
b
efore
Alice
ips
the
coin.
Clearly
,
in
this
case
Bob
wins
with
probabilit
y
=.
In
the
second
alternativ
e,

APPENDIX:
AN
ESSA
Y
BY
O.G.

Bob
has
to
announce
his
guess
while
the
coin
is
spinning
in
the
air.
Although
the
outcome
is
determine
d
in
principle
b
y
the
motion
of
the
coin,
Bob
do
es
not
ha
v
e
accurate
information
on
the
motion
and
th
us
w
e
b
eliev
e
that
also
in
this
case
Bob
wins
with
probabilit
y
=.
The
third
alternativ
e
is
similar
to
the
second,
except
that
Bob
has
at
his
disp
osal
sophisticated
equipmen
t
capable
of
pro
viding
accurate
information
on
the
coin's
motion
as
w
ell
as
on
the
en
vironmen
t
eecting
the
outcome.
Ho
w
ev
er,
Bob
cannot
pro
cess
this
information
in
time
to
impro
v
e
his
guess.
In
the
fourth
alternativ
e,
Bob's
recording
equipmen
t
is
directly
connected
to
a
p
owerful
c
omputer
programmed
to
solv
e
the
motion
equations
and
output
a
prediction.
It
is
conceiv
able
that
in
suc
h
a
case
Bob
can
impro
v
e
substan
tially
his
guess
of
the
outcome
of
the
coin.
W
e
conclude
that
the
randomness
of
an
ev
en
t
is
relativ
e
to
the
information
and
computing
resources
at
our
disp
osal.
Th
us,
a
natural
concept
of
pseudorandomness
arises
{
a
distribution
is
pseudo-
r
andom
if
no
ecien
t
pro
cedure
can
distinguish
it
from
the
uniform
distribution,
where
ecien
t
pro
cedures
are
asso
ciated
with
(probabilistic)
p
olynomial-time
algorithms.
..
The
Denition
of
Pseudorandom
Generators
Lo
osely
sp
eaking,
a
pseudorandom
generator
is
an
ecient
program
(or
algorithm)
whic
h
str
etches
short
random
seeds
in
to
long
pseudor
andom
sequences.
The
ab
o
v
e
emphasizes
three
fundamen
tal
asp
ects
in
the
notion
of
a
pseudorandom
generator:
.
Eciency:
The
generator
has
to
b
e
ecien
t.
W
e
asso
ciate
ecien
t
computations
with
those
conducted
within
time
whic
h
is
p
olynomial
in
the
length
of
the
input.
Consequen
tly
,
w
e
p
ostulate
that
the
generator
has
to
b
e
implemen
table
b
y
a
deterministic
p
olynomial-time
algorithm.
This
algorithm
tak
es
as
input
a
se
e
d,
as
men
tioned
ab
o
v
e.
The
seed
captures
a
b
ounded
amoun
t
of
randomness
used
b
y
a
device
whic
h
\generates
pseudorandom
sequences."
The
form
ulation
views
an
y
suc
h
device
as
consisting
of
a
deterministic
pro
cedure
applied
to
a
random
seed.
.
Str
etching:
The
generator
is
required
to
stretc
h
its
input
seed
to
a
longer
output
sequence.
Sp
ecically
,
it
stretc
hes
n-bit
long
seeds
in
to
`(n)-bit
long
outputs,
where
`(n)
>
n.
The
function
`
is
called
the
str
etching
me
asur
e
(or
str
etching
function)
of
the
generator.
.
Pseudor
andomness:
The
generator's
output
has
to
lo
ok
random
to
an
y
ecien
t
observ
er.
That
is,
an
y
ecien
t
pro
cedure
should
fail
to
distinguish
the
output
of
a
generator
(on
a
random
seed)
from
a
truly
random
sequence
of
the
same
length.
The
form
ulation
of
the
last
sen
tence
refers
to
a
general
notion
of
c
omputational
indistinguishability
whic
h
is
the
heart
of
the
en
tire
approac
h.
Computational
Indistinguishabilit
y:
In
tuitiv
ely
,
t
w
o
ob
jects
are
called
computationally
in-
distinguishable
if
no
ecien
t
pro
cedure
can
tell
them
apart.
As
usual
in
complexit
y
theory
,
an
elegan
t
form
ulation
requires
asymptotic
analysis
(or
rather
a
functional
treatmen
t
of
the
running
time
of
algorithms
in
terms
of
the
length
of
their
input).

Th
us,
the
ob
jects
in
question
are
innite

W
e
stress
that
the
asymptotic
(or
functional)
treatmen
t
is
not
essen
tial
to
this
approac
h.
One
ma
y
dev
elop
the
en
tire
approac
h
in
terms
of
inputs
of
xed
lengths
and
an
adequate
notion
of
complexit
y
of
algorithms.
Ho
w
ev
er,
suc
h
an
alternativ
e
treatmen
t
is
more
cum
b
ersome.


PSEUDORANDOM
GENERA
TORS
sequences
of
distributions,
where
eac
h
distribution
has
a
nite
supp
ort.
Suc
h
a
sequence
will
b
e
called
a
distribution
ensemble.
T
ypically
,
w
e
consider
distribution
ensem
bles
of
the
form
fD
n
g
nN
,
where
for
some
function
`
:
N
!
N
,
the
supp
ort
of
eac
h
D
n
is
a
subset
of
f0;
g
`(n)
.
F
urthermore,
t
ypically
`
will
b
e
a
p
ositiv
e
p
olynomial.
Oded's
Note:
In
this
essa
y
,
I'v
e
preferred
the
traditional
mathematical
notations.
Sp
ecif-
ically
,
I
ha
v
e
used
distributions
(o
v
er
strings)
rather
than
our
non-standard
\random
v
ariables"
(whic
h
range
o
v
er
strings).
F
or
a
distribution
D
,
the
traditional
notation
xD
means
x
selected
according
to
distribution
D
.
Denition
..
(Computational
Indistinguishabilit
y
[,
]):
Two
pr
ob
ability
ensembles,
fX
n
g
nN
and
fY
n
g
nN
,
ar
e
c
al
le
d
computationally
indistinguishable
if
for
any
pr
ob
abilistic
p
olynomial-time
al-
gorithm
A,
for
any
p
ositive
p
olynomial
p,
and
for
al
l
suciently
lar
ge
n's
j
Pr
x
X
n
[A(x))
=
]
 Pr
y
Y
n
[A(y
)
=
]
j
<

p(n)
The
pr
ob
ability
is
taken
over
X
n
(r
esp.,
Y
n
)
as
wel
l
as
over
the
c
oin
tosses
of
algorithm
A.
A
couple
of
commen
ts
are
in
place.
Firstly
,
w
e
ha
v
e
allo
w
ed
algorithm
A
(called
a
distinguisher)
to
b
e
probabilistic.
This
mak
es
the
requiremen
t
only
stronger,
and
seems
essen
tial
to
sev
eral
imp
ortan
t
asp
ects
of
our
approac
h.
Secondly
,
w
e
view
ev
en
ts
o
ccuring
with
probabilit
y
whic
h
is
upp
er
b
ounded
b
y
the
recipro
cal
of
p
olynomials
as
ne
gligible.
This
is
w
ell-coupled
with
our
notion
of
eciency
(i.e.,
p
olynomial-time
computations):
An
ev
en
t
whic
h
o
ccurs
with
negligible
probabilit
y
(as
a
function
of
a
parameter
n),
will
o
ccur
with
negligible
probabilit
y
also
if
the
exp
erimen
t
is
rep
eated
for
p
oly
(n)-man
y
times.
W
e
note
that
computational
indistinguishabilit
y
is
a
strictly
more
lib
eral
notion
than
statistical
indistinguishabilit
y
(cf.,
[,
0]).
An
imp
ortan
t
case
is
the
one
of
distributions
generated
b
y
a
pseudorandom
generator
as
dened
next.
Denition
..
(Pseudorandom
Generators
[,
]):
A
deterministic
p
olynomial-time
algorithm
G
is
c
al
le
d
a
pseudo
random
generato
r
if
ther
e
exists
a
stretc
hing
function,
`
:
N
!
N
,
so
that
the
fol
lowing
two
pr
ob
ability
ensembles,
denote
d
fG
n
g
nN
and
fR
n
g
nN
,
ar
e
c
omputational
ly
indistin-
guishable
.
Distribution
G
n
is
dene
d
as
the
output
of
G
on
a
uniformly
sele
cte
d
se
e
d
in
f0;
g
n
.
.
Distribution
R
n
is
dene
d
as
the
uniform
distribution
on
f0;
g
`(n)
.
That
is,
letting
U
m
denote
the
uniform
distribution
over
f0;
g
m
,
we
r
e
quir
e
that
for
any
pr
ob
abilistic
p
olynomial-time
algorithm
A,
for
any
p
ositive
p
olynomial
p,
and
for
al
l
suciently
lar
ge
n's
j
Pr
sU
n
[A(G(s))
=
]
 Pr
r
U
`(n)
[A(r
)
=
]
j
<

p(n)
Th
us,
pseudorandom
generators
are
ecien
t
(i.e.,
p
olynomial-time)
deterministic
programs
whic
h
expand
short
randomly
selected
seeds
in
to
longer
pseudorandom
bit
sequences,
where
the
latter
are
dened
as
computationally
indistinguishable
from
truly
random
sequences
b
y
ecien
t
(i.e.,
p
olynomial-time)
algorithms.
It
follo
ws
that
an
y
ecien
t
randomized
algorithm
main
tains
its
p
er-
formance
when
its
in
ternal
coin
tosses
are
substituted
b
y
a
sequence
generated
b
y
a
pseudorandom
generator.
That
is,

APPENDIX:
AN
ESSA
Y
BY
O.G.
	
Construction
..
(t
ypical
application
of
pseudorandom
generators):
L
et
A
b
e
a
pr
ob
abilistic
algorithm,
and
(n)
denote
a
(p
olynomial)
upp
er
b
ound
on
its
r
andomness
c
omplexity.
L
et
A(x;
r
)
denote
the
output
of
A
on
input
x
and
c
oin
tosses
se
quenc
e
r

f0;
g
(jxj)
.
L
et
G
b
e
a
pseudor
andom
gener
ator
with
str
etching
function
`
:
N
!
N
.
Then
A
G
is
a
r
andomize
d
algorithm
which
on
input
x,
pr
o
c
e
e
ds
as
fol
lows.
It
sets
k
=
k
(jxj)
to
b
e
the
smal
lest
inte
ger
such
that
`(k
)

(jxj),
uniformly
sele
cts
s

f0;
g
k
,
and
outputs
A(x;
r
),
wher
e
r
is
the
(jxj)-bit
long
pr
ex
of
G(s).
It
can
b
e
sho
wn
that
it
is
infeasible
to
nd
long
x's
on
whic
h
the
input-output
b
ehavior
of
A
G
is
notic
e
abl
ly
dier
ent
from
the
one
of
A,
although
A
G
ma
y
use
m
uc
h
few
er
coin
tosses
than
A.
That
is
Theorem
..
L
et
A
and
G
b
e
as
ab
ove.
Then
for
every
p
air
of
pr
ob
abilistic
p
olynomial-time
algorithms,
a
nder
F
and
a
distinguisher
D
,
every
p
ositive
p
olynomial
p
and
al
l
suciently
long
n's
X
xf0;g
n
Pr[F
(
n
)
=
x]


A;D
(x)
<

p(n)
where

A;D
(x)
def
=
j
Pr
r

U
(n)
[D
(x;
A(x;
r
))
=
]
 Pr
sU
k
(n)
[D
(x;
A
G
(x;
s))
=
]
j
and
the
pr
ob
abilities
ar
e
taken
over
the
U
m
's
as
wel
l
as
over
the
c
oin
tosses
of
F
and
D
.
The
theorem
is
pro
v
en
b
y
sho
wing
that
a
triplet
(A;
F
;
D
)
violating
the
claim
can
b
e
con
v
erted
in
to
an
algorithm
D
0
whic
h
distinguishes
the
output
of
G
from
the
uniform
distribution,
in
con-
tradiction
to
the
h
yp
othesis.
Analogous
argumen
ts
are
applied
whenev
er
one
wishes
to
pro
v
e
that
an
ecien
t
randomized
pro
cess
(b
e
it
an
algorithm
as
ab
o
v
e
or
a
m
ulti-part
y
computation)
pre-
serv
es
its
b
eha
vior
when
one
replaces
true
randomness
b
y
pseudorandomness
as
dened
ab
o
v
e.
Th
us,
giv
en
pseudorandom
generators
with
large
stretc
hing
function,
one
c
an
c
onsider
ably
r
e
duc
e
the
r
andomness
c
omplexity
in
any
ecient
applic
ation.
Amplifying
the
stretc
h
function.
Pseudorandom
generators
as
dened
ab
o
v
e
are
only
required
to
stretc
h
their
input
a
bit;
for
example,
stretc
hing
n-bit
long
inputs
to
(n
+
)-bit
long
outputs
will
do.
Clearly
generator
of
suc
h
mo
derate
stretc
h
function
are
of
little
use
in
practice.
In
con
trast,
w
e
w
an
t
to
ha
v
e
pseudorandom
generators
with
an
arbitrary
long
stretc
h
function.
By
the
eciency
requiremen
t,
the
stretc
h
function
can
b
e
at
most
p
olynomial.
It
turns
out
that
pseudorandom
generators
with
the
smallest
p
ossible
stretc
h
function
can
b
e
used
to
construct
pseudorandom
generators
with
an
y
desirable
p
olynomial
stretc
h
function.
(Th
us,
when
talking
ab
out
the
existence
of
pseudorandom
generators,
w
e
ma
y
ignore
the
stretc
h
function.)
Theorem
..
[	]:
L
et
G
b
e
a
pseudor
andom
gener
ator
with
str
etch
function
`(n)
=
n
+
,
and
`
0
b
e
any
p
olynomial
ly
b
ounde
d
str
etch
function,
which
is
p
olynomial-time
c
omputable.
L
et
G

(x)
denote
the
jxj-bit
long
pr
ex
of
G(x),
and
G

(x)
denote
the
last
bit
of
G(x)
(i.e.,
G(x)
=
G

(x)
G

(x)).
Then
G
0
(s)
def
=








`
0
(jsj)
;
wher
e
x
0
=
s,

i
=
G

(x
i 
)
and
x
i
=
G

(x
i 
),
for
i
=
;
:::;
`
0
(jsj)
is
a
pseudor
andom
gener
ator
with
str
etch
function
`
0
.

0
PSEUDORANDOM
GENERA
TORS
Pro
of
Sk
etc
h:
The
theorem
is
pro
v
en
using
the
hybrid
te
chnique
(cf.,
Sec.
..
in
[]):
One
con-
siders
distributions
H
i
n
(for
i
=
0;
:::;
`(n))
dened
b
y
U
()
i
P
`(n) i
(U
()
n
),
where
U
()
i
and
U
()
n
are
in-
dep
enden
t
uniform
distributions
(o
v
er
f0;
g
i
and
f0;
g
n
,
resp
ectiv
ely),
and
P
j
(x)
denotes
the
j
-bit
long
prex
of
G
0
(x).
The
extreme
h
ybrids
corresp
ond
to
G
0
(U
n
)
and
U
`(n)
,
whereas
distinguisha-
bilit
y
of
neigh
b
oring
h
ybrids
can
b
e
w
ork
ed
in
to
distinguishabilit
y
of
G(U
n
)
and
U
n+
.
Lo
osely
sp
eaking,
supp
ose
one
could
distinguish
H
i
n
from
H
i+
n
.
Then,
using
P
j
(s)
=
G

(s)P
j
 
(G

(s))
(for
j

),
this
means
that
one
can
distinguish
H
i
n

(U
()
i
;
G

(U
()
n
);
P
(`(n) i) 
(G

(U
()
n
)))
from
H
i+
n

(U
()
i
;
U
(
0
)

;
P
`(n) (i+)
(U
(
0
)
n
)).
Incorp
orating
the
generation
of
U
()
i
and
the
ev
al-
uation
of
P
`(n) i 
in
to
the
distinguisher,
one
could
distinguish
(f
(U
()
n
);
b(U
()
n
))

G

(U
n
)
from
(U
(
0
)
n
;
U
(
0
)

)

U
n+
,
in
con
tradiction
to
the
pseudorandomness
of
G

.
(F
or
details
see
Sec.
..
in
[].)
..
Ho
w
to
Construct
Pseudorandom
Generators
The
kno
wn
constructions
transform
computation
dicult
y
,
in
the
form
of
one-w
a
y
functions
(de-
ned
b
elo
w),
in
to
pseudorandomness
generators.
Lo
osely
sp
eaking,
a
p
olynomial-time
c
omputable
function
is
called
one-w
a
y
if
an
y
ecien
t
algorithm
can
in
v
ert
it
only
with
negligible
success
prob-
abilit
y
.
F
or
simplicit
y
,
w
e
consider
only
length-preserving
one-w
a
y
functions.
Denition
..
(one-w
a
y
function):
A
one-w
a
y
function,
f
,
is
a
p
olynomial-time
c
omputable
function
such
that
for
every
pr
ob
abilistic
p
olynomial-time
algorithm
A
0
,
every
p
ositive
p
olynomial
p(),
and
al
l
suciently
lar
ge
n's
Pr
xU
n
h
A
0
(f
(x))

f
 
(f
(x))
i
<

p(n)
wher
e
U
n
is
the
uniform
distribution
over
f0;
g
n
.
P
opular
candidates
for
one-w
a
y
functions
are
based
on
the
conjectured
in
tractabilit
y
of
In
teger
F
actorization
(cf.,
[]
for
state
of
the
art),
the
Discrete
Logarithm
Problem
(cf.,
[	]
analogously),
and
deco
ding
of
random
linear
co
de
[].
The
infeasibilit
y
of
in
v
erting
f
yields
a
w
eak
notion
of
unpredictabilit
y:
Let
b
i
(x)
denotes
the
i
th
bit
of
x.
Then,
for
ev
ery
probabilistic
p
olynomial-time
algorithm
A
(and
sucien
tly
large
n),
it
m
ust
b
e
the
case
that
Pr
i;x
[A(i;
f
(x))
=
b
i
(x)]
>
=n,
where
the
probabilit
y
is
tak
en
uniformly
o
v
er
i

f;
:::;
ng
and
x

f0;
g
n
.
A
stronger
(and
in
fact
strongest
p
ossible)
notion
of
unpredictabilit
y
is
that
of
a
hard-core
predicate.
Lo
osely
sp
eaking,
a
p
olynomial-time
c
omputable
predicate
b
is
called
a
hard-core
of
a
function
f
if
all
ecien
t
algorithm,
giv
en
f
(x),
can
guess
b(x)
only
with
success
probabilit
y
whic
h
is
negligible
b
etter
than
half.
Denition
..
(hard-core
predicate
[]):
A
p
olynomial-time
c
omputable
pr
e
dic
ate
b
:
f0;
g

!
f0;
g
is
c
al
le
d
a
ha
rd-co
re
of
a
function
f
if
for
every
pr
ob
abilistic
p
olynomial-time
algorithm
A
0
,
every
p
ositive
p
olynomial
p(),
and
al
l
suciently
lar
ge
n's
Pr
xU
n
(A
0
(f
(x))
=
b(x))
<


+

p(n)
Clearly
,
if
b
is
a
hard-core
of
a
-
p
olynomial-time
computable
function
f
then
f
m
ust
b
e
one-w
a
y
.

It
turns
out
that
an
y
one-w
a
y
function
can
b
e
sligh
tly
mo
died
so
that
it
has
a
hard-core
predicate.

F
unctions
whic
h
are
not
-
ma
y
ha
v
e
hard-core
predicates
of
information
theoretic
nature;
but
these
are
of
no
use
to
us
here.
F
or
example,
for


f0;
g,
f
(
;
x)
=
0f
0
(x)
has
an
\information
theoretic"
hard-core
predicate
b(
;
x)
=

.

APPENDIX:
AN
ESSA
Y
BY
O.G.

Theorem
..
(A
generic
hard-core
[]):
L
et
f
b
e
an
arbitr
ary
one-way
function,
and
let
g
b
e
dene
d
by
g
(x;
r
)
def
=
(f
(x);
r
),
wher
e
jxj
=
jr
j.
L
et
b(x;
r
)
denote
the
inner-pr
o
duct
mo
d

of
the
binary
ve
ctors
x
and
r
.
Then
the
pr
e
dic
ate
b
is
a
har
d-c
or
e
of
the
function
g
.
See
pro
of
in
Ap
dx
C.
in
[].
Finally
,
w
e
get
to
the
construction
of
pseudorandom
generators:
Theorem
..	
(A
simple
construction
of
pseudorandom
generators):
L
et
b
b
e
a
har
d-c
or
e
pr
e
d-
ic
ate
of
a
p
olynomial-time
c
omputable
-
function
f
.
Then,
G(s)
def
=
f
(s)
b(s)
is
a
pseudor
andom
gener
ator.
Pro
of
Sk
etc
h:
Clearly
the
jsj-bit
long
prex
of
G(s)
is
uniformly
distributed
(since
f
is
-
and
on
to
f0;
g
jsj
).
Hence,
the
pro
of
b
oils
do
wn
to
sho
wing
that
distinguishing
f
(s)b(s)
from
f
(s)
,
where

is
a
random
bit,
yields
con
tradiction
to
the
h
yp
othesis
that
b
is
a
hard-core
of
f
(i.e.,
that
b(s)
is
unpr
e
dictable
from
f
(s)).
In
tuitiv
ely
,
suc
h
a
distinguisher
also
distinguishes
f
(s)b(s)
from
f
(s)b(s),
where

=

 
,
and
so
yields
an
algorithm
for
predicting
b(s)
based
on
f
(s).
In
a
sense,
the
k
ey
p
oin
t
in
the
pro
of
of
the
ab
o
v
e
theorem
is
sho
wing
that
the
(ob
vious
b
y
denition)
unpredictabilit
y
of
the
output
of
G
implies
its
pseudorandomness.
The
fact
that
(next
bit)
unpredictabilit
y
and
pseudorandomness
are
equiv
alen
t
in
general
is
pro
v
en
explicitly
in
the
alternativ
e
presen
tation
b
elo
w.
An
alternativ
e
presen
tation.
Our
presen
tation
of
the
construction
of
pseudorandom
genera-
tors,
via
Construction
..
and
Prop
osition
..	,
is
analogous
to
the
original
construction
of
pseudorandom
generators
suggested
b
y
b
y
Blum
and
Micali
[]:
Giv
en
an
arbitrary
stretc
h
function
`
:
N
!
N
,
a
-
one-w
a
y
function
f
with
a
hard-core
b,
one
denes
G(s)
def
=
b(x

)b(x

)



b(x
`(jsj)
)
;
where
x
0
=
s
and
x
i
=
f
(x
i 
)
for
i
=
;
:::;
`(jsj).
The
pseudorandomness
of
G
is
established
in
t
w
o
steps,
using
the
notion
of
(next
bit)
unpredictabilit
y
.
An
ensem
ble
fZ
n
g
nN
is
called
unp
redictable
if
an
y
probabilistic
p
olynomial-time
mac
hine
obtaining
a
prex
of
Z
n
fails
to
predict
the
next
bit
of
Z
n
with
probabilit
y
non-negligiblly
higher
than
=.
Step
:
One
rst
pro
v
es
that
the
ensem
ble
fG(U
n
)g
nN
,
where
U
n
is
uniform
o
v
er
f0;
g
n
,
is
(next-bit)
unpredictable
(from
righ
t
to
left)
[].
Lo
osely
sp
eaking,
if
one
can
predict
b(x
i
)
from
b(x
i+
)



b(x
`(jsj)
)
then
one
can
predict
b(x
i
)
giv
en
f
(x
i
)
(i.e.,
b
y
computing
x
i+
;
:::;
x
`(jsj)
and
so
obtaining
b(x
i+
)



b(x
`(jsj)
)),
in
con-
tradiction
to
the
hard-core
h
yp
othesis.
Step
:
Next,
one
uses
Y
ao's
observ
ation
b
y
whic
h
a
(p
olynomial-time
constructible)
ensem
ble
is
pseudor
andom
if
and
only
if
it
is
(next-bit)
unpr
e
dictable
(cf.,
Sec.
..
in
[]).
Clearly
,
if
one
can
predict
the
next
bit
in
an
ensem
ble
then
one
can
distinguish
this
en-
sem
ble
from
the
uniform
ensem
ble
(whic
h
in
unpredictable
regardless
of
computing
p
o
w
er).
Ho
w
ev
er,
here
w
e
need
the
other
direction
whic
h
is
less
ob
vious.
Still,
one
can
sho
w
that
(next
bit)
unpredictabilit
y
implies
indistinguishabilit
y
from
the
uniform
ensem
ble.
Sp
ecif-
ically
,
consider
the
follo
wing
\h
ybrid"
distributions,
where
the
i
th
h
ybrid
tak
es
the
rst
i
bits
from
the
questionable
ensem
ble
and
the
rest
from
the
uniform
one.
Th
us,
distinguishing
the
extreme
h
ybrids
implies
distinguishing
some
neigh
b
oring
h
ybrids,
whic
h
in
turn
implies
next-bit
predictabilit
y
(of
the
questionable
ensem
ble).


PSEUDORANDOM
GENERA
TORS
A
general
condition
for
the
existence
of
pseudorandom
generators.
Recall
that
giv
en
an
y
one-w
a
y
-
function,
w
e
can
easily
construct
a
pseudorandom
generator.
Actually
,
the
-
requiremen
t
ma
y
b
e
dropp
ed,
but
the
curren
tly
kno
wn
construction
{
for
the
general
case
{
is
quite
complex.
Still
w
e
do
ha
v
e.
Theorem
..0
(On
the
existence
of
pseudorandom
generators
[]):
Pseudor
andom
gener
ators
exist
if
and
only
if
one-way
functions
exist.
T
o
sho
w
that
the
existence
of
pseudorandom
generators
imply
the
existence
of
one-w
a
y
functions,
consider
a
pseudorandom
generator
G
with
stretc
h
function
`(n)
=
n.
F
or
x;
y

f0;
g
n
,
dene
f
(x;
y
)
def
=
G(x),
and
so
f
is
p
olynomial-time
computable
(and
length-preserving).
It
m
ust
b
e
that
f
is
one-w
a
y
,
or
else
one
can
distinguish
G(U
n
)
from
U
n
b
y
trying
to
in
v
ert
and
c
hec
king
the
result:
In
v
erting
f
on
its
range
distribution
refers
to
the
distribution
G(U
n
),
whereas
the
probabilit
y
that
U
n
has
in
v
erse
under
f
is
negligible.
The
in
teresting
direction
is
the
construction
of
pseudorandom
generators
based
on
an
y
one-w
a
y
function.
In
general
(when
f
ma
y
not
b
e
-)
the
ensem
ble
f
(U
n
)
ma
y
not
b
e
pseudorandom,
and
so
Construction
..	
(i.e.,
G(s)
=
f
(s)b(s),
where
b
is
a
hard-core
of
f
)
cannot
b
e
used
dir
e
ctly.
One
idea
of
[]
is
to
hash
f
(U
n
)
to
an
almost
uniform
string
of
length
related
to
its
en
trop
y
,
using
Univ
ersal
Hash
F
unctions
[].
(This
is
done
after
guaran
teeing,
that
the
logarithm
of
the
probabilit
y
mass
of
a
v
alue
of
f
(U
n
)
is
t
ypically
close
to
the
en
trop
y
of
f
(U
n
).)

But
\hashing
f
(U
n
)
do
wn
to
length
comparable
to
the
en
trop
y"
means
shrinking
the
length
of
the
output
to,
sa
y
,
n
0
<
n.
This
foils
the
en
tire
p
oin
t
of
stretc
hing
the
n-bit
seed.
Th
us,
a
second
idea
of
[]
is
to
comp
ensate
for
the
n
 n
0
loss
b
y
extracting
these
man
y
bits
from
the
seed
U
n
itself.
This
is
done
b
y
hashing
U
n
,
and
the
p
oin
t
is
that
the
(n
 n
0
+
)-bit
long
hash
v
alue
do
es
not
mak
e
the
in
v
erting
task
an
y
easier.
Implemen
ting
these
ideas
turns
out
to
b
e
more
dicult
than
it
seems,
and
indeed
an
alternativ
e
construction
w
ould
b
e
most
appreciated.
..
Pseudorandom
F
unctions
Pseudorandom
generators
allo
w
to
ecien
tly
generate
long
pseudorandom
sequences
from
short
random
seeds.
Pseudorandom
functions
(dened
b
elo
w)
are
ev
en
more
p
o
w
erful:
They
allo
w
e-
cien
t
direct
access
to
a
h
uge
pseudorandom
sequence
(whic
h
is
infeasible
to
scan
bit-b
y-bit).
Put
in
other
w
ords,
pseudorandom
functions
can
replace
truly
random
functions
in
an
y
ecien
t
applica-
tion
(e.g.,
most
notably
in
cryptograph
y).
That
is,
pseudorandom
functions
are
indistinguishable
from
random
functions
b
y
ecien
t
mac
hines
whic
h
ma
y
obtain
the
function
v
alues
at
argumen
ts
of
their
c
hoice.
(Suc
h
mac
hines
are
called
oracle
mac
hines,
and
if
M
is
suc
h
mac
hine
and
f
is
a
function,
then
M
f
(x)
denotes
the
computation
of
M
on
input
x
when
M
's
queries
are
answ
ered
b
y
the
function
f
.)
Denition
..
(pseudorandom
functions
[]):
A
pseudo
random
function
(ensem
ble),
with
length
p
ar
ameters
`
D
;
`
R
:
N
!
N
,
is
a
c
ol
le
ction
of
functions
F
def
=
ff
s
:
f0;
g
`
D
(jsj)
!
f0;
g
`
R
(jsj)
g
sf0;g

satisfying

(ecien
t
ev
aluation):
Ther
e
exists
an
ecient
(deterministic)
algorithm
which
given
a
seed,
s,
and
an
`
D
(jsj)-bit
argumen
t,
x,
r
eturns
the
`
R
(jsj)-bit
long
value
f
s
(x).

Sp
ecically
,
giv
en
an
arbitrary
one
w
a
y
function
f
0
,
one
rst
constructs
f
b
y
taking
a
\direct
pro
duct"
of
sucien
tly
man
y
copies
of
f
0
.
F
or
example,
for
x

;
:::;
x
n


f0;
g
n
,
w
e
let
f
(x

;
:::;
x
n

)
def
=
f
0
(x

);
:::;
f
0
(x
n

).

APPENDIX:
AN
ESSA
Y
BY
O.G.


(pseudorandomness):
F
or
every
pr
ob
abilistic
p
olynomial-time
or
acle
machine,
M
,
for
every
p
ositive
p
olynomial
p
and
al
l
suciently
lar
ge
n's



Pr
f

F
n
[M
f
(
n
)
=
]
 Pr
R
n
[M

(
n
)
=
]



<

p(n)
wher
e
F
n
denotes
the
distribution
on
F
obtaine
d
by
sele
cting
s
uniformly
in
f0;
g
n
,
and
R
n
denotes
the
uniform
distribution
over
al
l
functions
mapping
f0;
g
`
D
(n)
to
f0;
g
`
R
(n)
.
Supp
ose,
for
simplicit
y
,
that
`
D
(n)
=
n
and
`
R
(n)
=
.
Then
a
function
uniformly
selected
among

n
functions
(of
a
pseudorandom
ensem
ble)
presen
ts
an
input-output
b
eha
vior
whic
h
is
indistin-
guishable
in
p
oly
(n)-time
from
the
one
of
a
function
selected
at
random
among
all
the


n
Bo
olean
functions.
Con
trast
this
with
the

n
pseudorandom
sequences,
pro
duced
b
y
a
pseudorandom
gener-
ator,
whic
h
are
computationally
indistinguishable
from
a
sequence
selected
uniformly
among
all
the

p
oly
(n)
man
y
sequences.
Still
pseudorandom
functions
can
b
e
constructed
from
an
y
pseudorandom
generator.
Theorem
..
(Ho
w
to
construct
pseudorandom
functions
[]):
L
et
G
b
e
a
pseudor
andom
gener
ator
with
str
etching
function
`(n)
=
n.
L
et
G
0
(s)
(r
esp.,
G

(s))
denote
the
rst
(r
esp.,
last)
jsj
bits
in
G(s),
and
G

jsj





(s)
def
=
G

jsj
(


G


(G


(s))



)
Then,
the
function
ensemble
ff
s
:
f0;
g
jsj
!
f0;
g
jsj
g
sf0;g

,
wher
e
f
s
(x)
def
=
G
x
(s),
is
pseudor
an-
dom
with
length
p
ar
ameters
`
D
(n)
=
`
R
(n)
=
n.
The
ab
o
v
e
construction
can
b
e
easily
adapted
to
an
y
(p
olynomially-b
ounded)
length
parameters
`
D
;
`
R
:
N
!
N
.
Pro
of
Sk
etc
h:
The
pro
of
uses
the
h
ybrid
tec
hnique:
The
i
th
h
ybrid,
H
i
n
,
is
a
function
ensem
ble
consisting
of


i
n
functions
f0;
g
n
!
f0;
g
n
,
eac
h
dened
b
y

i
random
n-bit
strings,
denoted
hs

i
f0;g
i
.
The
v
alue
of
suc
h
function
at
x
=

,
with
jj
=
i,
is
G

(s

).
The
extreme
h
ybrids
corresp
ond
to
our
indistinguishabilit
y
claim
(i.e.,
H
0
n

f
U
n
and
H
n
n

F
n
),
and
neigh
b
oring
h
ybrids
corresp
ond
to
our
indistinguishabilit
y
h
yp
othesis
(sp
ecically
,
to
the
indistinguishabilit
y
of
G(U
n
)
and
U
n
under
m
ultiple
samples).
W
e
men
tion
that
pseudorandom
functions
ha
v
e
b
een
used
to
deriv
e
negativ
e
results
in
compu-
tational
learning
theory
[]
and
in
complexit
y
theory
(cf.,
Natural
Pro
ofs
[0]).
..
The
Applicabilit
y
of
Pseudorandom
Generators
Randomness
is
pla
ying
an
increasingly
imp
ortan
t
role
in
computation:
It
is
frequen
tly
used
in
the
design
of
sequen
tial,
parallel
and
distributed
algorithms,
and
is
of
course
cen
tral
to
cryptog-
raph
y
.
Whereas
it
is
con
v
enien
t
to
design
suc
h
algorithms
making
free
use
of
randomness,
it
is
also
desirable
to
minimize
the
usage
of
randomness
in
real
implemen
tations.
Th
us,
pseudorandom
generators
(as
dened
ab
o
v
e)
are
a
k
ey
ingredien
t
in
an
\algorithmic
to
ol-b
o
x"
{
they
pro
vide
an
automatic
compiler
of
programs
written
with
free
usage
of
randomness
in
to
programs
whic
h
mak
e
an
economical
use
of
randomness.
Indeed,
\pseudo-random
n
um
b
er
generators"
ha
v
e
app
eared
with
the
rst
computers.
Ho
w
ev
er,
t
ypical
implemen
tations
use
generators
whic
h
are
not
pseudorandom
according
to
the
ab
o
v
e
de-
nition.
Instead,
at
b
est,
these
generators
are
sho
wn
to
pass
some
ad-ho
c
statistical
test
(cf.,
[]).


PSEUDORANDOM
GENERA
TORS
Ho
w
ev
er,
the
fact
that
a
\pseudo-random
n
um
b
er
generator"
passes
some
statistical
tests,
do
es
not
mean
that
it
will
pass
a
new
test
and
that
it
is
go
o
d
for
a
future
(un
tested)
application.
F
ur-
thermore,
the
approac
h
of
sub
jecting
the
generator
to
some
ad-ho
c
tests
fails
to
pro
vide
general
results
of
the
t
yp
e
stated
ab
o
v
e
(i.e.,
of
the
form
\for
all
practical
purp
oses
using
the
output
of
the
generator
is
as
go
o
d
as
using
truly
un
biased
coin
tosses").
In
con
trast,
the
approac
h
encompassed
in
Denition
..
aims
at
suc
h
generalit
y
,
and
in
fact
is
tailored
to
obtain
it:
The
notion
of
computational
indistinguishabilit
y
,
whic
h
underlines
Denition
..,
co
v
ers
all
p
ossible
ecien
t
applications
p
ostulating
that
for
all
of
them
pseudorandom
sequences
are
as
go
o
d
as
truly
random
ones.
..
The
In
telectual
Con
ten
ts
of
Pseudorandom
Generators
W
e
shortly
discuss
some
in
telectual
asp
ects
of
pseudorandom
generators
as
dened
ab
o
v
e.
Beha
vioristic
v
ersus
On
tological.
Our
denition
of
pseudorandom
generators
is
based
on
the
notion
of
computational
indistinguishabilit
y
.
The
b
eha
vioristic
nature
of
the
latter
notion
is
b
est
demonstrated
b
y
confron
ting
it
with
the
Kolmogoro
v-Chaitin
approac
h
to
randomness.
Lo
osely
sp
eaking,
a
string
is
Kolmo
gor
ov-r
andom
if
its
length
equals
the
length
of
the
shortest
program
pro
ducing
it.
This
shortest
program
ma
y
b
e
considered
the
\true
explanation"
to
the
phenomenon
describ
ed
b
y
the
string.
A
Kolmogoro
v-random
string
is
th
us
a
string
whic
h
do
es
not
ha
v
e
a
substan
tially
simpler
(i.e.,
shorter)
explanation
than
itself.
Considering
the
simplest
explanation
of
a
phenomenon
ma
y
b
e
view
ed
as
an
on
tological
approac
h.
In
con
trast,
considering
the
eect
of
phenomena
(on
an
observ
er),
as
underlying
the
denition
of
pseudorandomness,
is
a
b
eha
vioristic
approac
h.
F
urthermore,
there
exist
probabilit
y
distributions
whic
h
are
not
uniform
(and
are
not
ev
en
statistically
close
to
a
uniform
distribution)
that
nev
ertheless
are
indistinguishable
from
a
uniform
distribution
b
y
an
y
ecien
t
metho
d
[,
0].
Th
us,
distributions
whic
h
are
on
tologically
v
ery
dieren
t,
are
considered
equiv
alen
t
b
y
the
b
eha
vioristic
p
oin
t
of
view
tak
en
in
the
denitions
ab
o
v
e.
A
relativistic
view
of
randomness.
Pseudorandomness
is
dened
ab
o
v
e
in
terms
of
its
ob-
serv
er.
It
is
a
distribution
whic
h
cannot
b
e
told
apart
from
a
uniform
distribution
b
y
an
y
ecien
t
(i.e.
p
olynomial-time)
observ
er.
Ho
w
ev
er,
pseudorandom
sequences
ma
y
b
e
distinguished
from
random
ones
b
y
innitely
p
o
w
erful
p
o
w
erful
(not
at
our
disp
osal!).
Sp
ecically
,
an
exp
onen
tial-
time
mac
hine
can
easily
distinguish
the
output
of
a
pseudorandom
generator
from
a
uniformly
selected
string
of
the
same
length
(e.g.,
just
b
y
trying
all
p
ossible
seeds).
Th
us,
pseudorandomness
is
sub
jectiv
e
to
the
abilities
of
the
observ
er.
Randomness
and
Computational
Dicult
y
.
Pseudorandomness
and
computational
di-
cult
y
pla
y
dual
roles:
The
denition
of
pseudorandomness
relies
on
the
fact
that
putting
com-
putational
restrictions
on
the
observ
er
giv
es
rise
to
distributions
whic
h
are
not
uniform
and
still
cannot
b
e
distinguished
from
uniform.
F
urthermore,
the
construction
of
pseudorandom
generators
rely
on
conjectures
regarding
computational
dicult
y
(i.e.,
the
existence
of
one-w
a
y
functions),
and
this
is
inevitable:
giv
en
a
pseudorandom
generator,
w
e
can
construct
one-w
a
y
functions.
Th
us,
(non-trivial)
pseudorandomness
and
computational
hardness
can
b
e
con
v
erted
bac
k
and
forth.

APPENDIX:
AN
ESSA
Y
BY
O.G.

..
A
General
P
aradigm
Pseudorandomness
as
surv
ey
ed
ab
o
v
e
can
b
e
view
ed
as
an
imp
ortan
t
sp
ecial
case
of
a
general
paradigm.
A
general
treatmen
t
is
pro
vided
in
[].
A
generic
form
ulation
of
pseudorandom
generators
consists
of
sp
ecifying
three
fundamen
tal
as-
p
ects
{
the
str
etching
me
asur
e
of
the
generators;
the
class
of
distinguishers
that
the
generators
are
supp
osed
to
fo
ol
(i.e.,
the
algorithms
with
resp
ect
to
whic
h
the
c
omputational
indistinguishabil-
ity
requiremen
t
should
hold);
and
the
resources
that
the
generators
are
allo
w
ed
to
use
(i.e.,
their
o
wn
c
omputational
c
omplexity).
In
the
ab
o
v
e
presen
tation
w
e
fo
cused
on
p
olynomial-time
gener-
ators
(th
us
ha
ving
p
olynomial
stretc
hing
measure)
whic
h
fo
ol
an
y
probabilistic
p
olynomial-time
observ
ers.
A
v
ariet
y
of
other
cases
are
of
in
terest
to
o,
and
w
e
briey
discuss
some
of
them.
W
eak
er
notions
of
computational
indistinguishabilit
y
.
Whenev
er
the
aim
is
to
replace
random
sequences
utilized
b
y
an
algorithm
with
pseudorandom
ones,
one
ma
y
try
to
capitalize
on
kno
wledge
of
the
target
algorithm.
Ab
o
v
e
w
e
ha
v
e
merely
used
the
fact
that
the
target
algorithm
runs
in
p
olynomial-time.
Ho
w
ev
er,
for
example,
if
w
e
kno
w
that
the
algorithm
uses
v
ery
little
w
ork-
space
then
w
e
ma
y
able
to
do
b
etter.
Similarly
,
if
w
e
kno
w
that
the
analysis
of
the
algorithm
only
dep
ends
on
some
sp
ecic
prop
erties
of
the
random
sequence
it
uses
(e.g.,
pairwise
indep
endence
of
its
elemen
ts).
In
general,
w
eak
er
notions
of
computational
indistinguishabilit
y
suc
h
as
fo
oling
space-b
ounded
algorithms,
constan
t-depth
circuits,
and
ev
en
sp
ecic
tests
(e.g.,
testing
pairwise
indep
endence
of
the
sequence),
arise
naturally:
Generators
pro
ducing
sequences
whic
h
fo
ol
suc
h
tests
are
useful
in
a
v
ariet
y
of
applications
{
if
the
application
utilizes
randomness
in
a
restricted
w
a
y
then
feeding
it
with
sequences
of
lo
w
randomness-qualit
y
ma
y
do.
Needless
to
sa
y
that
w
e
adv
o
cate
a
rigorous
form
ulation
of
the
c
haracteristics
of
suc
h
applications
and
rigorous
construction
of
generators
whic
h
fo
ol
the
t
yp
e
of
tests
whic
h
emerge.
Alternativ
e
notions
of
generator
eciency
.
The
ab
o
v
e
discussion
has
fo
cused
on
one
asp
ect
of
the
pseudorandomness
question
{
the
resources
or
t
yp
e
of
the
observ
er
(or
p
oten
tial
distin-
guisher).
Another
imp
ortan
t
question
is
whether
suc
h
pseudorandom
sequences
can
b
e
generated
from
m
uc
h
shorter
ones,
and
at
what
cost
(or
complexit
y).
Throughout
this
essa
y
w
e'v
e
required
the
generation
pro
cess
to
b
e
at
least
as
ecien
t
as
the
eciency
limitations
of
the
distinguisher.

This
seems
indeed
\fair"
and
natural.
Allo
wing
the
generator
to
b
e
more
complex
(i.e.,
use
more
time
or
space
resources)
than
the
distinguisher
seems
unfair,
but
still
yields
in
teresting
consequences
in
the
con
text
of
trying
to
\de-randomize"
randomized
complexit
y
classe.
F
or
example,
one
ma
y
consider
generators
w
orking
in
time
exp
onen
tial
in
the
length
of
the
seed.
In
some
cases
w
e
lo
ose
nothing
b
y
b
eing
more
lib
eral
(i.e.,
allo
wing
exp
onen
tial-time
generators).
T
o
see
wh
y
,
w
e
consider
a
t
ypical
derandomization
argumen
t,
pro
ceediing
in
t
w
o
steps:
First
one
replaces
the
true
random-
ness
of
the
algorithm
b
y
pseudorandom
sequences
generated
from
m
uc
h
shorter
seeds,
and
next
one
go
es
deterministically
o
v
er
all
p
ossible
seeds
and
lo
oks
for
the
most
frequen
t
b
eha
vior
of
the
mo
died
algorithm.
Th
us,
in
suc
h
a
case
the
deterministic
complexit
y
is
an
yho
w
exp
onen
tial
in
the
seed
length.
Ho
w
ev
er,
constructing
exp
onen
tial
time
generators
ma
y
b
e
easier
than
constructing
p
olynomial-time
ones.
References

If
fact,
w
e
ha
v
e
require
the
generator
to
b
e
more
ecien
t
than
the
distinguisher:
The
former
w
as
required
to
b
e
a
xed
p
olynomial-time
algorithm,
whereas
the
latter
w
as
allo
w
ed
to
b
e
an
y
algorithm
with
p
olynomial
running
time.


PSEUDORANDOM
GENERA
TORS
.
M.
Blum
and
S.
Micali.
Ho
w
to
Generate
Cryptographically
Strong
Sequences
of
Pseudo-
Random
Bits.
SIAM
Journal
on
Computing,
V
ol.
,
pages
0{,
	.
Preliminary
v
ersion
in
r
d
IEEE
Symp
osium
on
F
oundations
of
Computer
Scienc
e,
	.
.
L.
Carter
and
M.
W
egman.
Univ
ersal
Hash
F
unctions.
Journal
of
Computer
and
System
Scienc
e,
V
ol.
,
		,
pages
{.
.
G.J.
Chaitin.
On
the
Length
of
Programs
for
Computing
Finite
Binary
Sequences.
Journal
of
the
A
CM,
V
ol.
,
pages
{0,
	.
.
T.M.
Co
v
er
and
G.A.
Thomas.
Elements
of
Information
The
ory.
John
Wiley
&
Sons,
Inc.,
New-Y
ork,
		.
.
O.
Goldreic
h.
F
oundation
of
Crypto
gr
aphy
{
F
r
agments
of
a
Bo
ok.
F
ebruary
		.
Av
ailable
from
http
:
==theory:lcs:mit:edu=

oded
=frag:html
.
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.
.
O.
Goldreic
h,
S.
Goldw
asser,
and
S.
Micali.
Ho
w
to
Construct
Random
F
unctions.
Journal
of
the
A
CM,
V
ol.
,
No.
,
pages
	{0,
	.
.
O.
Goldreic
h
and
L.A.
Levin.
Hard-core
Predicates
for
an
y
One-W
a
y
F
unction.
In
st
A
CM
Symp
osium
on
the
The
ory
of
Computing,
pages
{,
		.
	.
O.
Goldreic
h
and
S.
Micali.
Increasing
the
Expansion
of
Pseudorandom
Generators.
Man
uscript,
	.
Av
ailable
from
http://theory.lcs.mit.e
du/
ode
d/pa
pers
.ht
ml
0.
O.
Goldreic
h,
and
H.
Kra
w
czyk,
On
Sparse
Pseudorandom
Ensem
bles.
R
andom
Structur
es
and
A
lgorithms,
V
ol.
,
No.
,
(		),
pages
{.
.
O.
Goldreic
h,
H.
Kra
w
cyzk
and
M.
Lub
y
.
On
the
Existence
of
Pseudorandom
Generators.
SIAM
Journal
on
Computing,
V
ol.
-,
pages
{,
		.
.
S.
Goldw
asser
and
S.
Micali.
Probabilistic
Encryption.
Journal
of
Computer
and
System
Scienc
e,
V
ol.
,
No.
,
pages
0{		,
	.
Preliminary
v
ersion
in
th
A
CM
Symp
osium
on
the
The
ory
of
Computing,
	.
.
J.
H

astad,
R.
Impagliazzo,
L.A.
Levin
and
M.
Lub
y
.
Construction
of
Pseudorandom
Gener-
ator
from
an
y
One-W
a
y
F
unction.
T
o
app
ear
in
SIAM
Journal
on
Computing.
Preliminary
v
ersions
b
y
Impagliazzo
et.
al.
in
st
A
CM
Symp
osium
on
the
The
ory
of
Computing
(		)
and
H

astad
in
nd
A
CM
Symp
osium
on
the
The
ory
of
Computing
(		0).
.
D.E.
Kn
uth.
The
A
rt
of
Computer
Pr
o
gr
amming,
V
ol.

(Seminumeric
al
A
lgorithms).
Addison-
W
esley
Publishing
Compan
y
,
Inc.,
		
(rst
edition)
and
	
(second
edition).
.
A.
Kolmogoro
v.
Three
Approac
hes
to
the
Concept
of
\The
Amoun
t
Of
Information".
Pr
obl.
of
Inform.
T
r
ansm.,
V
ol.
/,
	.
.
L.A.
Levin.
Randomness
Conserv
ation
Inequalities:
Information
and
Indep
endence
in
Math-
ematical
Theories.
Inform.
and
Contr
ol,
V
ol.
,
pages
{,
	.

APPENDIX:
AN
ESSA
Y
BY
O.G.

.
M.
Li
and
P
.
Vitan
yi.
A
n
Intr
o
duction
to
Kolmo
gor
ov
Complexity
and
its
Applic
ations.
Springer
V
erlag,
August
		.
.
A.M.
Odlyzk
o.
The
future
of
in
teger
factorization.
CryptoBytes
(The
tec
hnical
newsletter
of
RSA
Lab
oratories),
V
ol.

(No.
),
pages
-,
		.
	.
A.M.
Odlyzk
o.
Discrete
logarithms
and
smo
oth
p
olynomials.
In
Finite
Fields:
The
ory,
Ap-
plic
ations
and
A
lgorithms,
G.
L.
Mullen
and
P
.
Shiue,
eds.,
Amer.
Math.
So
c.,
Con
temp
orary
Math.
V
ol.
,
pages
	{,
		.
0.
A.R.
Razb
oro
v
and
S.
Rudic
h.
Natural
pro
ofs.
Journal
of
Computer
and
System
Scienc
e,
V
ol.

(),
pages
{,
		.
.
C.E.
Shannon.
A
mathematical
theory
of
comm
unication.
Bel
l
Sys.
T
e
ch.
Jour.,
V
ol.
,
pages
{,
	.
.
R.J.
Solomono.
A
F
ormal
Theory
of
Inductiv
e
Inference.
Inform.
and
Contr
ol,
V
ol.
/,
pages
{,
	.
.
L.
V
alian
t.
A
theory
of
the
learnable.
Communic
ations
of
the
A
CM,
V
ol.
/,
pages
{,
	.
.
A.C.
Y
ao.
Theory
and
Application
of
T
rap
do
or
F
unctions.
In
r
d
IEEE
Symp
osium
on
F
oundations
of
Computer
Scienc
e,
pages
0{	,
	.


PSEUDORANDOM
GENERA
TORS

Lecture

Pseudorandomness
and
Computational
Dicult
y
Notes
tak
en
b
y
Moshe
Lew
enstein
and
Y
eh
uda
Lindell
Summary:
In
this
lecture
w
e
con
tin
ue
our
discussion
of
pseudorandomness
and
sho
w
a
connection
b
et
w
een
pseudorandomness
and
computational
dicult
y
.
More
sp
ecically
w
e
sho
w
ho
w
the
dicult
y
of
in
v
erting
one-w
a
y
functions
ma
y
b
e
utilized
to
obtain
a
pseudorandom
generator.
Finally
,
w
e
state
and
pro
v
e
that
a
hard-to-predict
bit
(called
a
hard-core)
ma
y
b
e
extracted
from
an
y
one-w
a
y
function.
The
hard-core
is
fundamen
tal
in
our
construction
of
a
generator.
.
In
tro
duction
The
main
theme
of
this
lecture
is
the
utilization
of
one-w
a
y
functions
in
order
to
construct
a
pseudorandom
generator.
In
tuitiv
ely
,
a
one-w
a
y
function
is
a
function
that
is
e
asy
to
compute
and
har
d
to
in
v
ert.
Generally
,
\easy"
refers
to
p
olynomial
time
and
\hard"
to
the
fact
that
success
in
the
aver
age
c
ase
requires
more
than
p
olynomial
time
(for
an
y
p
olynomial).
It
is
critical
that
the
dicult
y
b
e
in
the
a
v
erage
case
and
not
in
the
w
orst
case,
as
with
N
P
-Complete
problems.
This
will
b
ecome
clear
later.
Ho
w
can
one-w
a
y
functions
help
us
construct
a
pseudorandom
generator?
The
answ
er
lies
in
the
prop
ert
y
of
unpredictabilit
y
.
This
concept
will
b
e
formalized
in
the
coming
lectures
but
for
no
w
w
e
will
discuss
it
informally
.
Assume
that
w
e
ha
v
e
a
string
s
and
w
e
b
egin
to
scan
it
in
some
giv
en
(computable)
order.
If
at
some
stage
w
e
can
predict
the
next
bit
with
probabilit
y
signican
tly
greater
than
one
half,
then
the
string
is
clearly
not
random
(b
ecause
for
a
random
string,
eac
h
bit
is
c
hosen
indep
endently
with
probabilit
y
exactly
one
half
).
On
the
other
hand,
it
can
b
e
sho
wn
that
if
w
e
c
annot
predict
an
y
\next"
bit
during
our
scan
with
success
signican
tly
more
than


,
then
the
string
is
pseudorandom.
In
this
ligh
t,
the
use
of
computationally
dicult
problems
b
ecomes
clear.
W
e
rely
on
the
dif-
cult
y
of
in
v
erting
a
one-w
a
y
function
f
.
More
sp
ecically
w
e
sho
w
that
there
exists
a
function
b
:
f0;
g
n
!
f0;
g
suc
h
that
giv
en
x
it
is
easy
to
compute
b(x)
y
et
giv
en
only
f
(x)
it
is
di-
cult.
This
function
is
formally
called
a
hard-core
of
f
.
No
w,
although
giv
en
f
(x),
b(x)
is
fully
and
deterministically
dened,
w
e
ha
v
e
no
w
a
y
of
nding
or
predicting
its
v
alue.
Therefore,
the
computational
dicult
y
of
nding
b(x)
pro
vides
us
with
an
unpredictable
bit
whic
h
forms
the
basis
of
our
generator.
	

	0
LECTURE
.
PSEUDORANDOMNESS
AND
COMPUT
A
TIONAL
DIFFICUL
TY
.
Denitions
W
e
no
w
recall
the
concepts
necessary
for
this
lecture.
Denition
.
Pseudorandom
generators:
G
is
a
pseudo
random
generato
r
if:
.
G
op
er
ates
in
(deterministic)
p
olynomial
time
.
F
or
every
s,
jG(s)j
>
jsj
(w.l.o.g.,
assume
that
	l
()
such
that
s

f0;
g
n
jG(s)j
=
l
(n)).
.
fG(U
n
)g
and
fU
l
(n)
g
ar
e
pr
ob
abilistic
p
olynomial
time
indistinguishable
(wher
e
U
n
is
the
uniform
distribution
over
f0;
g
n
).
Denition
.
One-w
a
y
functions:
L
et
f
:
f0;
g

!
f0;
g

b
e
a
length-pr
eserving
function
(i.e.
x
jf
(x)j
=
jxj).
Then
f
is
one-w
a
y
if:
.
f
is
\e
asy"
to
c
ompute.
F
ormal
ly,
ther
e
exists
a
p
olynomial
time
algorithm
A
such
that
x
A(x)
=
f
(x).
.
f
is
\har
d"
to
invert
in
the
aver
age
c
ase.
F
ormal
ly,
for
every
pr
ob
abilistic
p
olynomial
time
algorithm
A,
for
every
p
olynomial
p(),
and
for
al
l
suciently
lar
ge
n's,
P
r
ob[A(f
(U
n
))

f
 
f
(U
n
)]
<

p(n)
The
ab
o
v
e
denition
refers
to
a
length-preserving
function.
This
is
a
simplifying
assumption
w
e
will
use
throughout
this
lecture,
but
it
is
generally
not
necessary
as
long
as
jxj
=
pol
y
(jf
(x)j).
The
requiremen
t
that
the
lengths
of
x
and
f
(x)
b
e
p
olynomially
related
is
necessary
to
ensure
that
the
dicult
y
in
v
olv
ed
in
in
v
erting
f
(x)
is
not
b
ecause
x
is
to
o
long.
Since
the
in
v
erting
algorithm
m
ust
w
ork
in
time
p
olynomial
in
jf
(x)j,
if
jf
(x)j
is
logarithmic
in
jxj,
no
algorithm
can
ev
en
write
x.
In
this
case
there
is
no
computational
dicult
in
inverting
f
and
the
one-w
a
yness
is
due
to
the
ab
o
v
e
tec
hnicalit
y
.
Assuming
that
f
is
length-preserving
a
v
oids
this
problem.
As
w
e
will
see,
there
is
no
requiremen
t
that
f
b
e
length-preserving
in
the
hard-core
theorem
stated
and
pro
v
ed
in
section
.
Ho
w
ev
er,
the
exact
construction
of
the
pseudorandom
generator
in
section

relies
hea
vily
up
on
the
length-preserving
prop
ert
y
of
the
one-w
a
y
function
and
the
assumption
that
it
is

 .
Other
constructions
exist
for
the
more
general
case
but
are
more
complex.
Although
the
denition
of
a
one-w
a
y
function
guaran
tees
that
it
is
dicult
to
nd
the
en
tire
string
x
giv
en
f
(x),
it
ma
y
b
e
v
ery
easy
to
obtain
some
of
the
bits
of
x.
F
or
example,
assuming
that
f
is
one-w
a
y
,
consider
the
function
f
0
(

x)
=


f
(x),
where


f0;
g.
It
is
easy
to
see
that
f
0
is
also
one-w
a
y
.
This
is
rigorously
sho
wn
b
y
assuming
that
f
0
is
not
one-w
a
y
and
sho
wing
ho
w
f
can
b
e
in
v
erted
using
an
algorithm
whic
h
in
v
erts
f
0
.
So
w
e
see
that
despite
the
fact
that
f
0
is
one-w
a
y
,
the
rst
bit
of
the
input
is
completely
rev
ealed.
Therefore,
in
order
to
obtain
unpredictabilit
y
as
desired,
w
e
need
a
sp
ecic
bit
based
on
x,
whic
h
is
pro
v
ably
hidden
giv
en
f
(x).
This
bit
is
called
a
har
d-c
or
e
of
f
.
Reducibilit
y
Argumen
ts:
The
ab
o
v
e-men
tioned
metho
d
of
pro
of
(sho
wing
that
f
0
(

x)
=


f
(x)
is
one-w
a
y)
is
called
a
reduction.
It
is
w
orth
while
discussing
this
tec
hnique
as
it
will
form
the
basis
of
man
y
pro
ofs
that
w
e
will
see.
The
con
text
in
whic
h
it
app
ears
is
when
w
e
tak
e
a
certain
primitiv
e

..
DEFINITIONS
	
and
construct
a
new
primitiv
e
based
on
it.
F
or
example,
w
e
will
need
it
to
pro
v
e
our
construction
of
a
pseudorandom
generator
from
a
one-w
a
y
function.
Although
it
ma
y
b
e
in
tuitiv
ely
clear
that
a
new
construction
is
\correct"
this
sort
of
in
tuition
ma
y
b
e
misleading
and
so
requires
a
sound
pro
of.
W
e
do
this
b
y
sho
wing
that
if
the
newly
constructed
primitiv
e
is
not
secure,
then
an
algorithm
defying
it
ma
y
b
e
used
to
defy
the
original
primitiv
e
as
w
ell.
More
concretely
,
if
our
generator
is
distinguishable,
then
a
distinguishing
algorithm
for
it
ma
y
b
e
used
to
in
v
ert
the
one-w
a
y
function
used
in
the
construction.
W
e
will
see
this
t
yp
e
of
argumen
t
man
y
times
in
the
coming
lectures.
Denition
.
Hard-Core:
The
function
b
:
f0;
g

!
f0;
g
is
a
ha
rd-co
re
of
f
if:
.
b
is
e
asy
to
c
ompute
(in
the
same
sense
as
ab
ove),
.
F
or
every
pr
ob
abilistic
p
olynomial
time
algorithm
A,
for
every
p
olynomial
p(),
and
for
al
l
suciently
lar
ge
n's,
P
r
ob[A(f
(U
n
))
=
b(U
n
)]
<


+

p(n)
Note
that
it
is
trivial
to
predict
b
with
probabilit
y


b
y
simply
guessing.
The
ab
o
v
e
denition
requires
that
w
e
cannot
do
signican
tly
b
etter
than
this
guess.
A
hard-core
of
a
one-w
a
y
function
pla
ys
an
imp
ortan
t
role
in
cryptographic
applications
(as
all
information
is
hidden).
Ho
w
ev
er,
in
our
case,
w
e
will
use
the
computational
dicult
y
in
v
olv
ed
as
the
basis
for
our
construction
of
a
pseudorandom
generator.
This
can
b
e
seen
in
the
follo
wing
w
a
y
.
Giv
en
f
(x),
b(x)
is
fully-dened,
y
et
completely
unpredictable
to
an
y
p
olynomially
b
ound
algorithm.
This
extra
bit
of
unpredictable
information
is
what
will
supply
the
\stretc
h"
eect
necessary
for
the
generator.
The
follo
wing
claim
sho
ws
that
a
hard-core
can
only
exist
for
a
one-w
a
y
function.
This
is
in
tuitiv
ely
ob
vious,
since
if
f
is

 
but
not
one-w
a
y
then
w
e
can
in
v
ert
it
and
compute
b.
This
is
formally
sho
wn
b
elo
w.
Claim
..
If
f
is
-
and
p
olynomial
time
c
omputable,
and
b
is
a
har
d-c
or
e
of
f
,
then
f
is
one-way.
Pro
of:
By
con
tradiction,
assume
that
b
is
a
hard-core
of
f
,
y
et
f
is
not
one-w
a
y
.
W
e
no
w
sho
w
ho
w
an
algorithm
in
v
erting
f
can
b
e
used
to
predict
b(x)
from
f
(x).
Note
once
again
the
reduction
tec
hnique
w
e
use.
f
is
not
one-w
a
y
,
therefore
there
exists
a
probabilistic
p
olynomial
time
algorithm
A
and
a
p
olynomial
p()
suc
h
that
for
innitely
man
y
n's,
P
r
ob[A(f
(x))
=
x]


p(n)
(jxj
=
n).
W
e
no
w
construct
an
algorithm
A
0
for
predicting
b
from
f
:
Input:
y

x
0
 
A(y
)
(attempt
to
in
v
ert
y
,
using
A).

If
f
(x
0
)
=
y
then
output
b(x
0
)

Otherwise,
output
0
or

with
probabilit
y
/.
W
e
no
w
calculate
the
success
probabilit
y
of
A
0
.
P
r
ob[A
0
(f
(x))
=
b(x)]
=
P
r
ob[A(f
(x))
=
x]
+
P
r
ob[A(f
(x))
=
x]



	
LECTURE
.
PSEUDORANDOMNESS
AND
COMPUT
A
TIONAL
DIFFICUL
TY


p(n)


+
(
 
p(n)
)



=


+

p(n)
The
success
probabilit
y
of
A
0
is
greater
than
or
equal
to


+

p(n)
for
innitely
man
y
n's,
thereb
y
con
tradicting
the
fact
that
b
is
a
hard-core
of
f
.
Commen
t:
If
f
is
not
-,
then
the
ab
o
v
e
claim
is
false.
Consider
f
suc
h
that
f
(x)
=
0
jxj
.
Clearly
b(x)
=
\the
st
bit
of
x"
is
a
hard-core.
Ho
w
ev
er
this
is
b
ecause
of
information
theoretic
considerations
rather
than
computational
b
ounds.
The
function
f
dened
here
is
trivial
ly
in
v
erted
b
y
taking
an
y
arbitrary
string
of
length
jxj
as
the
preimage.
Ho
w
ev
er,
b(x)
ma
y
clearly
not
b
e
guessed
with
an
y
probabilit
y
b
etter
than


.
.
A
Pseudorandom
Generator
based
on
a
-
One-W
a
y
F
unc-
tion
In
this
section
w
e
sho
w
a
construction
of
a
pseudorandom
generator
giv
en
a
length-preserving
-
one-w
a
y
function.
The
construction
is
based
on
a
hard-core
of
the
one-w
a
y
function.
In
the
next
section,
w
e
sho
w
ho
w
to
generically
construct
a
hard-core
of
an
y
one-w
a
y
function.
W
e
note
that
constructions
of
a
pseudorandom
generator
exist
based
on
any
one-w
a
y
function
(not
necessarily
length-preserving
and
-).
Ho
w
ev
er,
the
constructions
and
pro
ofs
in
the
more
general
case
are
long
and
complicated
and
w
e
therefore
bring
this
case
only
.
Theorem
.
L
et
f
b
e
a
length-pr
eserving,
-
one-way
function
and
let
b
b
e
a
har
d-c
or
e
of
f
.
Then
G(s)
=
f
(s)b(s)
is
a
pseudor
andom
gener
ator
(str
etching
the
input
by
one
bit).
Pro
of:
W
e
rst
note
that
as
f
is
length-preserving
and
-,
f
(U
n
)
is
distributed
uniformly
o
v
er
f0;
g
n
and
is
therefore
fully
random.
It
remains
to
sho
w
that
for
s

R
f0;
g
n
,
the
com
bination
of
f
(s)
and
b(s)
together
remains
indistinguishable
from
U
n+
.
In
tuitiv
ely
,
as
w
e
cannot
predict
b(s)
from
f
(s),
the
string
lo
oks
random
to
an
y
computationally
b
ound
algorithm.
W
e
will
no
w
sho
w
ho
w
a
successful
distinguishing
algorithm
ma
y
b
e
used
to
predict
b
from
f
.
This
then
pro
v
es
that
no
suc
h
distinguishing
algorithm
exists
(b
ecause
b
is
a
hard-core
of
f
).
By
con
tradiction,
assume
that
there
exists
an
algorithm
A
and
a
p
olynomial
p()
suc
h
that
for
innitely
man
y
n's
jP
r
ob[A(f
(U
n
)b(U
n
))
=
]
 P
r
ob[A(U
n+
)
=
]j


p(n)
As
f
(U
n
)
is
distributed
uniformly
,
this
is
equiv
alen
t
to
A
successfully
distinguishing
b
et
w
een
ff
(U
n
)b(U
n
)g
and
ff
(U
n
)U

g.
It
follo
ws
that
A
can
distinguish
b
et
w
een
X

=
ff
(U
n
)b(U
n
)g
and
X

=
ff
(U
n
)b(U
n
)g.
Let
X
b
e
the
distribution
created
b
y
uniformly
c
ho
osing


R
f;
g
and
then
sampling
from
X

.
Clearly
X
is
iden
tically
distributed
to
ff
(U
n
)U

g.
No
w:
P
r
ob[A(X
)
=
]
=



P
r
ob[A(X

)
=
]
+



P
r
ob[A(X

)
=
]
)
P
r
[A(X

)
=
]
=


P
r
ob[A(X
)
=
]
 P
r
ob[A(X

)
=
]
Therefore:
j
P
r
ob[A(f
(U
n
)b(U
n
))
=
]
 P
r
ob[A(f
(U
n
)b(U
n
))
=
]
j

..
A
PSEUDORANDOM
GENERA
TOR
BASED
ON
A
-
ONE-W
A
Y
FUNCTION
	
=
j
P
r
ob[A(X

)
=
]
 P
r
ob[A(X

)
=
]
j
=
j
P
r
ob[A(X

)
=
]
 P
r
ob[A(X
)
=
]
+
P
r
ob[A(X

)
=
]
j
=
j


P
r
ob[A(X

)
=
]
 

P
r
ob[A(X
)
=
]
j
=


j
P
r
ob[A(f
(U
n
)b(U
n
))
=
]
 P
r
ob[A(U
n+
)
=
]
j


p(n)
Assume,
without
loss
of
generalit
y
,
that
for
innitely
man
y
n's
it
holds
that:
P
r
ob[A(f
(U
n
)b(U
n
))
=
]
 P
r
ob[A(f
(U
n
)b(U
n
)
)
=
]


p(n)
Otherwise
w
e
simply
rev
erse
the
terms
here
and
mak
e
the
appropriate
c
hanges
in
the
algorithm
and
the
remainder
of
the
pro
of
(i.e.
w
e
c
hange
step

of
the
algorithm
b
elo
w
to:
If
A(y


)
=
0
then
output

).
W
e
no
w
construct
A
0
to
predict
b(U
n
)
from
f
(U
n
).
In
tuitiv
ely
,
A
0
adds
a
random
guess
to
the
end
of
it's
input
y
(where
y
=
f
(x)
for
some
x)
and
attempts
to
see
if
it
guessed
b(f
 
(y
))
correctly
based
on
A's
resp
onse
to
this
guess.
The
algorithm
follo
ws:
Input:
y
.
Uniformly
c
ho
ose


R
f0;
g
.
If
A(y


)
=

then
output

.
Otherwise,
output

W
e
no
w
calculate
the
probabilit
y
that
A
0
successfully
computes
b(f
 
(y
)).
As

is
uniformly
c
hosen
from
f0;
g
w
e
ha
v
e:
P
r
ob[A
0
(f
(U
n
))
=
b(U
n
)]
=



P
r
ob[A
0
(f
(U
n
))
=
b(U
n
)
j

=
b(U
n
)]
+



P
r
ob[A
0
(f
(U
n
))
=
b(U
n
)
j

=
b(U
n
)]
No
w,
b
y
the
algorithm
(see
steps

and

resp
ectiv
ely)
w
e
ha
v
e:
P
r
ob[A
0
(f
(U
n
))
=
b(U
n
)
j

=
b(U
n
)]
=
P
r
ob[A(f
(U
n
)b(U
n
))
=
]
and
P
r
ob[A
0
(f
(U
n
))
=
b(U
n
)
j

=
b(U
n
)]
=
P
r
ob[A(f
(U
n
)b(U
n
)
)
=
0]
=

 P
r
ob[A(f
(U
n
)b(U
n
))
=
]
By
our
con
tradiction
h
yp
othesis:
P
r
ob[A(f
(U
n
)b(U
n
))
=
]
 P
r
ob[A(f
(U
n
)b(U
n
)
)
=
]


p(n)
Therefore:
P
r
ob[A
0
(f
(U
n
))
=
b(U
n
)]

	
LECTURE
.
PSEUDORANDOMNESS
AND
COMPUT
A
TIONAL
DIFFICUL
TY
=


P
r
ob[A(f
(U
n
)b(U
n
))
=
]
+


( P
r
ob[A(f
(U
n
)b(U
n
)
)
=
])
=


+


(P
r
ob[A(f
(U
n
)b(U
n
))
=
] P
r
ob[A(f
(U
n
)b(U
n
))
=
])



+

p(n)
whic
h
is
in
con
tradiction
to
the
fact
that
b
is
a
hard-core
of
f
and
cannot
b
e
predicted
with
non-negligible
adv
an
tage
o
v
er


.
W
e
remind
the
reader
that
in
the
previous
lecture
w
e
pro
v
ed
that
a
generator
stretc
hing
the
seed
b
y
ev
en
a
single
bit
can
b
e
deterministically
con
v
erted
in
to
a
generator
stretc
hing
the
seed
b
y
an
y
p
olynomial
length.
Therefore,
it
should
not
b
other
us
that
the
ab
o
v
e
construction
seems
rather
w
eak
with
resp
ect
to
its
\stretc
hing
capabilit
y".
A
t
this
stage
it
should
b
e
clear
wh
y
it
is
crucial
that
the
one-w
a
y
function
b
e
hard
to
in
v
ert
in
the
a
v
erage
case
and
not
just
in
the
w
orst
case.
If
the
function
is
in
v
ertible
in
the
a
v
erage
case,
then
it
is
easy
to
distinguish
b
et
w
een
ff
(U
n
)b(U
n
)g
and
fU
n+
g
most
of
the
time.
This
w
ould
clearly
not
b
e
satisfactory
for
a
pseudorandom
generator.
.
A
Hard-Core
for
An
y
One-W
a
y
F
unction
In
this
section
w
e
presen
t
a
construction
of
a
hard-core
from
an
y
one-w
a
y
function.
Here
there
is
no
necessit
y
that
f
b
e
-
or
ev
en
length-preserving.
As
w
e
ha
v
e
seen
in
the
previous
section,
the
existence
of
a
hard-core
is
essen
tial
in
our
construction
of
a
pseudorandom
generator.
Theorem
.
L
et
f
0
:
f0;
g

!
f0;
g

b
e
a
one-way
function.
Dene
f
(x;
r
)
=
(f
0
(x);
r
)
wher
e
jxj
=
jr
j.
Then
b(x;
r
)
=
P
n
i=
x
i
r
i
mo
d

is
a
har
d-c
or
e
of
f
.
Note
that
since
f
0
is
one-w
a
y
,
f
is
clearly
one-w
a
y
(trivially
,
an
y
algorithm
in
v
erting
f
can
b
e
used
to
in
v
ert
f
0
).
Pro
of:
Assume
b
y
con
tradiction,
that
there
exists
a
probabilistic
p
olynomial
time
algorithm
A
and
a
p
olynomial
p()
suc
h
that
for
innitely
man
y
n's
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)]



+

p(n)
where
the
probabilities
are
tak
en,
uniformly
and
indep
enden
tly
,
o
v
er
x

R
f0;
g
n
,
r

R
f0;
g
n
and
coin
tosses
of
A
(if
an
y).
The
follo
wing
claim
sho
ws
that
there
are
a
signican
t
n
um
b
er
of
x's
for
whic
h
A
succeeds
with
non-negligible
probabilit
y
.
W
e
will
then
sho
w
ho
w
to
in
v
ert
f
for
these
x's.
Claim
..
L
et
S
n

f0;
g
n
b
e
the
set
of
al
l
x's
wher
e
P
r
ob
r
[A(f
(x;
r
))
=
b(x;
r
)]



+

p(n)
.
Then,
P
r
ob
x
[x

S
n
]
>

p(n)
.
Pro
of:
By
a
simple
a
v
eraging
argumen
t.
Assume
b
y
con
tradiction
that
P
r
ob
x
[x

S
n
]


p(n)
.
Then:
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)]
=
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)
j
x

S
n
]

P
r
ob[x

S
n
]
+
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)
j
x
=

S
n
]

P
r
ob[x
=

S
n
]

..
A
HARD-CORE
F
OR
ANY
ONE-W
A
Y
FUNCTION
	
No
w,
trivially
b
oth
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)
j
x

S
n
]


and
P
r
ob[x
=

S
n
]

.
F
urthermore,
b
y
the
con
tradiction
h
yp
othesis
P
r
ob[x

S
n
]


p(n)
.
Finally
,
based
on
the
denition
of
S
n
,
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)
j
x
=

S
n
]
<


+

p(n)
.
Putting
all
these
together:
P
r
ob
x;r
[A(f
(x;
r
))
=
b(x;
r
)]
<



p(n)
+
(


+

p(n)
)


=


+

p(n)
whic
h
con
tradicts
the
assumption
regarding
the
success
probabilit
y
of
A.
It
suces
to
sho
w
that
w
e
can
retriev
e
x
from
f
(x)
for
x

S
n
with
probabilit
y

pol
y
(n)
(b
ecause
then
w
e
can
in
v
ert
an
y
f
(x)
with
probabilit
y


p(n)pol
y
(n)
=

pol
y
(n)
).
So,
assume
that
for
x:
P
r
ob
r
[A(f
(x;
r
))
=
b(x;
r
)]



+

p(n)
(as
in
the
claim).
Denote
B
x
(r
)
=
A(f
(x;
r
).
No
w
x
is
xed
and
B
x
(r
)
is
a
blac
k-b
o
x
returning
b(x;
r
)
with
probabilit
y



+

p(n)
.
W
e
use
calls
to
B
x
to
retriev
e
x
giv
en
f
(x).
W
e
also
denote

=

p(n)
.
As
an
exercise,
assume
that
P
r
ob
r
[B
x
(r
)
=
b(x;
r
)]
>


+
.
There
is
no
reason
to
assume
that
this
is
true
but
it
will
help
us
with
the
pro
of
later
on.
Using
this
assumption,
for
ev
ery
i
=
;
:::;
n
w
e
sho
w
ho
w
to
reco
v
er
x
i
(that
is,
the
i'th
bit
of
x).
Input:
y
=
f
(x)
.
Uniformly
select
r

R
f0;
g
n
.
Compute
B
x
(r
)

B
x
(r

e
i
)
where
e
i
=
(0
i 
0
n i
)
(
in
the
i'th
co
ordinate
and
0
ev
erywhere
else).
No
w,
P
r
ob[B
x
(r
)
=
b(x;
r
)]
<


 
b
y
the
h
yp
othesis.
Therefore,
P
r
ob[B
x
(r

e
i
)
=
b(x;
r

e
i
)]
<


 
(b
ecause
r

e
i
is
also
uniformly
distributed).
So,
the
probabilit
y
that
B
x
(r
)
=
b(x;
r
)
and
B
x
(r

e
i
)
=
b(x;
r

e
i
)
is
greater
than


+

(b
y
summing
the
errors).
In
this
case:
B
x
(r
)

B
x
(r

e
i
)
=
b(x;
r
)

b(x;
r

e
i
).
Ho
w
ev
er,
b(x;
r
)

b(x;
r

e
i
)
=
n
X
j
=
x
j
r
j
+
n
X
j
=
x
j
(r
j
+
e
i
j
)
mod

=
n
X
j
=
x
j
r
j
+x
i
+
n
X
j
=
x
j
r
j
mod

=
x
i
So,
if
w
e
rep
eat
this
O
(
n


)
times
and
tak
e
a
ma
jorit
y
v
ote,
w
e
obtain
a
correct
answ
er
with
a
v
ery
high
probabilit
y
(in
the
order
of

 

n
).
This
is
true
(but
with
probabilit
y


 
n
)
ev
en
if
the
dieren
t
executions
are
only
pairwise
indep
enden
t
(this
can
b
e
deriv
ed
using
Cheb
yshev's
inequalit
y
and
is
imp
ortan
t
later
on).
W
e
do
the
same
for
all
i's
and
in
this
w
a
y
succeed
in
in
v
erting
f
(x)
with
high
probabilit
y
.
Note
that
w
e
can
use
the
same
set
of
random
strings
r
for
eac
h
i
(the
only
dierence
is
in
obtaining
b(x;
r

e
i
)
eac
h
time).
This
in
v
ersion
algorithm
w
ork
ed
based
on
the
unreasonable
assumption
that
P
r
ob
r
[B
x
(r
)
=
b(x;
r
)]
>


+
.
This
w
as
necessary
as
w
e
needed
to
query
B
x
on
t
w
o
dieren
t
p
oin
ts
and
therefore
w
e
had
to
sum
the
error
probabilities.
When
the
probabilit
y
of
success
is
only

ab
o
v
e


,
the
resulting
error
probabilit
y
is
far
to
o
high.
In
order
to
solv
e
this
problem,
remem
b
er
that
w
e
are
really
in
terested
in
calculating
b(x;
e
i
)
=
x
i
.
Ho
w
ev
er,
w
e
cannot
query
B
x
(e
i
)
b
ecause
w
e
ha
v
e
no
information
on
B
x
's
b
eha
viour
at
an
y
giv
en

	
LECTURE
.
PSEUDORANDOMNESS
AND
COMPUT
A
TIONAL
DIFFICUL
TY
p
oin
t
(the
kno
wn
probabilities
are
for
B
x
(r
)
where
r
is
uniformly
distributed).
Therefore
w
e
queried
B
x
at
r
and
r

e
i
(
random
p
oin
ts)
and
inferred
x
i
from
the
result.
W
e
no
w
sho
w
ho
w
to
compute
b(x;
r
)

b(x;
r

e
i
)
for
O
(
n


)
pairwise
indep
enden
t
r
's.
Based
on
what
w
e
ha
v
e
seen
ab
o
v
e,
this
is
enough
to
in
v
ert
f
(x).
Our
strategy
is
based
on
obtaining
v
alues
of
b(x;
r
)
for
\free"
(that
is,
without
calling
B
x
).
W
e
do
this
b
y
guessing
what
the
v
alue
should
b
e,
but
in
suc
h
a
w
a
y
that
the
probabilit
y
of
b
eing
correct
is
non-negligible.
Let
l
=
log

(m
+
)
where
m
=
O
(
n


).
Let
s

;
:::;
s
l

R
f0;
g
n
b
e
l
uniformly
c
hosen
n-bit
strings.
Then
for
ev
ery
non-empt
y
I

f;
:::;
l
g,
dene:
r
I
=
L
iI
s
i
.
Eac
h
r
I
is
an
n-bit
string
constructed
b
y
xoring
together
the
strings
s
i
indexed
b
y
I
.
Eac
h
r
I
is
uniformly
distributed
as
it
is
the
xor
of
random
strings.
Moreo
v
er
eac
h
pair
is
indep
enden
t
since
for
I
=
J
,
	s
i
s.t.
w.l.o.g.
s
i

I
and
s
i
=

J
.
Therefore,
r
I
giv
en
r
J
is
uniformly
distributed
based
on
the
distribution
of
s
i
.
No
w,
let
us
uniformly
c
ho
ose


;
:::;

l

f0;
g.
Assume
that
w
e
w
ere
v
ery
luc
ky
and
that
for
ev
ery
i,

i
=
b(x;
s
i
)
(in
other
w
ords,
w
e
guessed
b(x;
s
i
)
correctly
ev
ery
time).
Note
that
this
luc
ky
ev
en
t
happ
ens
with
the
non-neglible
probabilit
y
of


l
=

n

=

pol
y
(n)
.
The
follo
wing
claim
sho
ws
that
in
this
luc
ky
case
w
e
ac
hiev
e
our
aim.
Claim
..
L
et

I
=
L
iI

i
.
Then,
if
for
every
i,

i
=
b(x;
s
i
)
then
for
every
I
,

I
=
b(x;
r
I
).
Pro
of:
b(x;
r
I
)
=
b(x;
X
iI
s
i
)
=
n
X
j
=
x
j

X
iI
s
i
j
=
X
iI
n
X
j
=
x
j

s
i
j
=
X
iI
b(x;
s
i
)
=
X
iI

i
=

I
where
all
sums
are
mo
d
.
The
ab
o
v
e
claim
sho
ws
that
b
y
correctly
guessing
log
m
v
alues
of

i
w
e
are
able
to
deriv
e
the
v
alue
of
b(x;
)
at
m
pairwise
indep
enden
t
p
oin
ts.
This
is
b
ecause
under
the
assumption
that
w
e
guessed
the

i
's
correctly
,
eac
h

I
is
exactly
b(x;
r
I
)
where
the
r
I
's
are
uniformly
distributed
and
pairwise
indep
enden
t.
Note
that
since
there
are

l
 
=
m
dieren
t
subsets
I
,
w
e
ha
v
e
the
necessary
n
um
b
er
of
dieren
t
p
oin
ts
in
order
to
obtain
x
i
.
The
algorithm
uses
these
p
oin
ts
in
order
to
extract
x
i
instead
of
uniformly
c
hosen
r
's.
(An
alternativ
e
strategy
is
not
to
guess
the

i
's
but
to
try
ev
ery
p
ossible
com
bination
of
them.
Since

l
=
m
+

whic
h
is
p
olynomial,
w
e
can
do
this
in
the
time
a
v
ailable.)
The
Actual
Algorithm:
Input:
y
.
Uniformly
c
ho
ose
s

;
:::;
s
l

R
f0;
g
n
and


;
:::;

l

f0;
g.
.
F
or
ev
ery
non-empt
y
I

f;
:::;
l
g,
dene:
r
I
=
L
iI
s
i
and
compute

I
=
L
iI

i
.
.
F
or
ev
ery
i

f;
:::;
ng
and
non-empt
y
I

f;
:::;
l
g
do

v
I
i
=

i

B
x
(r
I

e
i
)

Guess
x
i
=
ma
jorit
y
I
fv
I
i
g

..
A
HARD-CORE
F
OR
ANY
ONE-W
A
Y
FUNCTION
	
In
order
to
calculate
the
probabilit
y
that
the
ab
o
v
e
algorithm
succeeds
in
in
v
erting
f
(x),
assume
that
for
ev
ery
i,
w
e
guessed

i
suc
h
that

i
=
b(x;
s
i
)
(in
step
).
The
probabilit
y
of
this
ev
en
t
o
ccurring
is


l
=

m+
for
m
=
O
(
n


)
and

=

p(n)
.
In
other
w
ords,
the
probabilit
y
that
w
e
are
luc
ky
is

pol
y
(n)
.
Since
w
e
kno
w
that
for
ev
ery
I
,
B
x
(r
I

e
i
)
is
correctly
computed
with
a
probabilit
y
greater
that


+
,
it
follo
ws
that
b(x;
r
I
)

B
x
(r
I

e
i
)
is
also
correct
with
the
same
probabilit
y
(as
the

i
's
are
correct).
As
previously
men
tioned,
due
to
the
pairwise
indep
endence
of
the
ev
en
ts,
the
probabilit
y
that
w
e
succeed
in
in
v
erting
f
(x)
in
this
case
is
at
least

 
n
(this
is
pro
v
ed
using
Cheb
yshev's
inequalit
y).
It
is
easy
to
see
that
the
probabilit
y
that
w
e
succeed
in
guessing
all

i
's
correctly
and
then
pro
ceed
to
successfully
in
v
ert
f
is
the
pro
duct
of
the
ab
o
v
e
t
w
o
probabilities,
that
is

pol
y
(n)
.
W
e
therefore
conclude
that
the
probabilit
y
of
successfully
in
v
erting
f
is
non-negligible.
This
is
in
con
tradiction
to
the
one-w
a
yness
of
f
.
Therefore,
b
as
dened
is
a
hard-core
of
f
.
Summary
of
the
pro
of.
W
e
b
egan
b
y
assuming
the
existence
of
an
algorithm
A
whic
h
predicts
b(x;
r
)
from
f
(x;
r
)
with
non-negligible
success
probabilit
y
(o
v
er
all
c
hoices
of
x,
r
and
coin
tosses
of
A).
W
e
then
sho
w
ed
that
there
is
a
non-negligible
set
of
x's
for
whic
h
A
succeeds
and
that
w
e
pro
ceed
to
attempt
to
in
v
ert
f
(x)
for
these
x's
only
.
This
enabled
us
to
set
x
and
fo
cus
only
on
the
randomness
in
r
.
In
the
next
stage
w
e
describ
ed
ho
w
w
e
can
obtain
x,
under
the
assumption
that
b
oth
b(x;
r
)
and
b(x;
r

e
i
)
can
b
e
computed
correctly
with
probabilit
y



+

pol
y
(n)
.
This
w
as
easily
sho
wn
as
b(x;
r
)

b(x;
r

e
i
)
=
x
i
and
b
y
rep
eating
w
e
can
ac
hiev
e
a
small
enough
error
so
that
the
probabilit
y
that
w
e
succeed
in
obtaining
x
i
for
all
i
is
at
least

pol
y
(n)
.
Finally
w
e
sho
w
ed
ho
w
to
ac
hiev
e
the
previous
assumption.
This
in
v
olv
ed
realizing
that
pairwise
indep
endence
for
dieren
t
r
's
is
enough
and
then
sho
wing
ho
w
pol
y
(n)
pairwise
indep
enden
t
n-bit
strings
can
b
e
obtained
using
only
l
og
(pol
y
(n))
n-bit
random
strings
s

;
:::;
s
l
.
Based
on
this,
w
e
guess
the
v
alue
of
b(x;
s
i
)
for
eac
h
i.
The
critical
p
oin
t
here
is
that
the
probabilit
y
of
guessing
correctly
for
all
i
is

pol
y
(n)
and
that
the
v
alue
of
b(x;
)
for
all
pol
y
(n)
pairwise
indep
enden
t
n-bit
strings
can
b
e
immediately
deriv
ed
from
these
guesses.
In
short,
w
e
succeeded
(with
non-negligible
probabilit
y)
in
guessing
b(x;
)
correctly
for
a
p
oly-
nomial
n
um
b
er
of
pairwise
indep
enden
t
strings.
These
strings
are
used
as
r
for
b(x;
r
)
in
the
in
v
ersion
algorithm
describ
ed
in
the
middle
stage.
W
e
compute
b(x;
r

e
i
)
using
A.
Assuming
that
w
e
guessed
correctly
,
w
e
ac
hiev
e
the
necessary
success
probabilit
y
for
computing
b
oth
b(x;
r
)
and
b(x;
r

e
i
).
As
w
e
guess
once
for
the
en
tire
in
v
ersion
algorithm
and
this
is
indep
enden
t
of
all
that
follo
ws,
w
e
are
able
to
extract
x
from
f
(x)
with
o
v
erall
probabilit
y
of

pol
y
(n)
.
This
pro
v
es
that
no
suc
h
algorithm
A
exists.
Bibliographic
Notes
This
lecture
is
based
mainly
on
[]
(see
also
App
endix
C
in
[]).
.
O.
Goldreic
h.
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
Pr
o
ofs
and
Pseudor
andomness.
Algorithms
and
Com
binatorics
series
(V
ol.
),
Springer,
		.

	
LECTURE
.
PSEUDORANDOMNESS
AND
COMPUT
A
TIONAL
DIFFICUL
TY
.
O.
Goldreic
h
and
L.A.
Levin.
Hard-core
Predicates
for
an
y
One-W
a
y
F
unction.
In
st
STOC,
pages
{,
		.

Lecture

Derandomization
of
B
P
P
Notes
tak
en
b
y
Erez
W
aisbard
and
Gera
W
eiss
Summary:
In
this
lecture
w
e
presen
t
an
ecien
t
deterministic
sim
ulation
of
random-
ized
algorithms.
This
pro
cess,
called
derandomization,
in
tro
duce
new
notions
of
pseu-
dorandom
generators.
W
e
extend
the
denition
of
pseudorandom
generators
and
sho
w
ho
w
to
construct
a
generator
that
can
b
e
used
for
derandomization.
The
new
construc-
tion
dier
from
the
generator
that
w
e
constructed
in
the
previous
lecture
in
it's
running
time
(it
will
run
slo
w
er,
but
fast
enough
for
the
sim
ulation),
but
most
imp
ortan
tly
in
it's
assumptions.
W
e
are
not
assuming
the
existence
of
one-w
a
y
function
but
w
e
mak
e
another
assumption
whic
h
ma
y
b
e
w
eak
er
than
that.
.
In
tro
duction
Randomness
pla
ys
a
k
ey
role
in
man
y
algorithms.
Ho
w
ev
er
random
sources
are
not
alw
a
ys
a
v
ailable
and
w
e
w
ould
lik
e
to
minimize
the
usage
of
randomness.
A
naiv
e
w
a
y
to
remo
v
e
the
"randomness
elemen
t"
from
algorithm
is
simply
to
go
o
v
er
all
p
ossible
coin
tosses
it
uses
and
act
according
to
the
ma
jorit
y
.
This
ho
w
ev
er
w
e
can't
do
for
B
P
P
in
p
olynomial
time
with
resp
ect
to
the
size
of
the
input.
If
w
e
could
shrink
the
amoun
t
of
randomness
the
algorithm
consumes
(to
logarithmic)
then
w
e
could
imply
the
naiv
e
idea
in
p
olynomial
time.
A
w
a
y
to
use
small
random
source
to
create
m
uc
h
more
randomness
w
as
in
tro
duced
in
the
previous
lecture
-
the
pseudorandom
generator.
Pseudorandom
generator
G
stretc
h
out
random
seed
in
to
a
p
olynomial-long
pseudorandom
sequence
that
can't
b
e
ecien
tly
(in
p
olynomial
time)
distinguished
from
a
truly
random
sequence.
G
:
f0;
g
k
!
f0;
g
pol
y
(k
)
F
or
con
v
enience
w
e
repro
duce
here
the
formal
denition
w
e
ga
v
e
in
the
previous
lecture
for
pseudorandom
generator.
Denition
.
A
deterministic
p
olynomial-time
algorithm
G
is
c
al
le
d
a
pseudor
andom
gener
ator
if
ther
e
exist
a
str
etching
function
l
:
N
!
N
,
so
that
for
any
pr
ob
abilistic
p
olynomial-time
algorithm
D,
for
any
p
ositive
p
olynomial
p,
and
for
al
l
suciently
lar
ge
k
's
jP
r
ob[D
(G(U
k
))
=
]
 P
r
ob[D
(U
l
(k
)
)
=
]j
<

p(k
)
Wher
e
U
n
denotes
the
uniform
distribution
over
f0;
g
n
and
the
pr
ob
ability
is
taken
over
U
k
(r
esp.,
U
l
(k
)
)
as
wel
l
as
over
the
c
oin
tosses
of
D.
		

00
LECTURE
.
DERANDOMIZA
TION
OF
B
P
P
Supp
ose
w
e
ha
v
e
suc
h
a
pseudorandom
generator
then
for
ev
ery

>
0
w
e
can
shrink
the
amoun
t
of
randomness
used
b
y
an
algorithm
A,
deciding
a
language
in
B
P
P
,
from
pol
y
(n)
to
n

(where
n
is
the
length
of
the
input).
The
shrinking
of
randomness
will
not
cause
signicance
c
hange
in
the
b
eha
vior
of
the
algorithm,
meaning
that
it
is
infeasible
to
nd
a
long
enough
input
on
whic
h
the
algorithm
whic
h
uses
less
randomness
will
decide
dieren
tly
than
the
original
one.
The
problem
is
that
the
ab
o
v
e
do
esn't
indicate
that
there
are
no
inputs
for
whic
h
A
will
act
dieren
tly
when
using
the
pseudorandom
source,
but
that
they
are
hard
to
nd.
Th
us
in
order
to
derandomize
B
P
P
w
e
will
need
stronger
notion
of
pseudorandom
generator.
W
e
will
need
a
pseudorandom
generator
whic
h
can
fo
ol
an
y
small
(p
olynomial
size)
circuit
(and
hence
an
y
p
olynomial
time
algorithm).
Denition
.
A
deterministic
p
olynomial-time
algorithm
G
is
c
al
le
d
a
non-uniformly
str
ong
pseudor
andom
gener
ator
if
ther
e
exist
a
str
etching
function
l
:
N
!
N
,
so
that
for
any
family
fC
k
g
k
L
of
p
olynomial-size
cir
cuits,
for
any
p
ositive
p
olynomial
p,
and
for
al
l
suciently
lar
ge
k
's
jP
r
ob[C
k
(G(U
k
))
=
]
 P
r
ob[C
k
(U
l
(k
)
)
=
]j
<

p(k
)
Theorem
.
If
such
G
exist
which
is
r
obust
against
p
olynomial-size
cir
cuit
then

>
0
B
P
P

D
time(
n

)
Pro
of:
L

B
P
P
implies
that
the
algorithm
A
for
deciding
L
do
esn't
only
tak
e
an
input
x
with
size
n
but
also
uses
randomness
R
with
size
l
(when
w
e
write
for
short
A(x)
w
e
really
mean
A(x;
R
)).
The
relation
b
et
w
een
the
size
of
the
input
and
the
size
of
the
randomness
is
l
=
pol
y
(n).
Let
us
construct
a
new
algorithm
A
0
whic
h
will
use
less
randomness
than
A
but
will
act
similar
to
A
0
for
almost
all
inputs.
A
0
(x;
s)
def
=
A(x;
G(s))
Where
s

f0;
g
n

.
A
0
uses
little
randomness
and
w
e
claim
that
A
0
only
dier
from
A
for
nitely
man
y
x's.
Prop
osition
..
F
or
al
l
but
nitely
many
x's
jP
r
ob[A(x)
=
]
 P
r
ob[A
0
(x)
=
]j
<


Pro
of:
The
idea
of
the
pro
of
is
that
if
there
w
ere
innitely
man
y
x's
for
whic
h
A
and
A
0
dier,
then
w
e
could
distinguish
G's
output
from
a
random
string,
in
con
tradiction
to
the
assumption
that
G
is
a
pseudorandom
generator.
In
order
to
con
tradict
Denition
.
it
suces
to
presen
t
a
family
fC
k
g
of
small
circuits
for
whic
h
jP
r
ob[C
k
(G(U
k
))
=
]
 P
r
ob[C
k
(U
l
(k
)
)
=
]j



Supp
ose
to
w
ards
con
tradiction
that
for
innitely
man
y
x's
A
and
A
0
b
eha
v
e
dieren
tly
,
i.e
P
r
ob[A(x)
=
]
 P
r
ob[A
0
(x)
=
]




..
NEW
NOTION
OF
PSEUDORANDOM
GENERA
TOR
0
Then
w
e
incorp
orate
these
inputs
and
A
in
to
a
family
of
small
circuits
as
follo
ws
x
!
C
x
()
def
=
A(x;
)
This
will
enable
us
to
distinguish
the
output
of
the
generator
from
a
uniformly
distributed
source.
The
circuit
C
k
will
b
e
one
of
fC
x
:
A(x)
uses
k
coin
tossesg.
Note
that
if
there
are
innitely
man
y
x's
on
whic
h
A
and
A
0
dier
then
there
are
innitely
man
y
sizes
of
x's
on
whic
h
they
dier.
The
amoun
t
of
randomness
used
b
y
the
algorithm
is
p
olynomial
with
resp
ect
to
the
size
of
the
input.
The
idea
b
ehind
this
construction
is
that
C
k
(G(U
k
))

A
0
(x)
and
C
k
(U
l
(k
)
))

A(x)
Hence
w
e
ha
v
e
a
family
of
circuits
suc
h
that
jP
r
ob[C
k
(G(U
k
))
=
]
 P
r
ob[C
k
(U
l
(k
)
)
=
]j



Whic
h
is
a
con
tradiction
to
the
denition
of
pseudorandom
generator.
Sa
ying
that
algorithm
A
decides
B
P
P
means
that
if
x

L
the
probabilit
y
that
A
will
sa
y
0
Y
E
S
0
is
greater
than


and
if
x
=

L
the
probabilit
y
that
A
will
sa
y
0
Y
E
S
0
is
smaller
than


.
By
the
ab
o
v
e
prop
osition,
for
all
but
nitely
man
y
x's
jP
r
ob[A(x)
=
]
 P
r
ob[A
0
(x)
=
]j
<
=.
Th
us,
for
all
but
nitely
man
y
x's
x

L
)
pr
ob
[A(x;
U
n
)
=
]



)
pr
ob
[A
0
(x;
s)
=
]
>


x
=

L
)
pr
ob
[A(x;
U
n
)
=
]



)
pr
ob
[A
0
(x;
s)
=
]
<


No
w
w
e
dene
the
algorithm
A
00
whic
h
incorp
orate
these
nitely
man
y
inputs,
and
for
all
other
inputs
it
lo
ops
o
v
er
all
p
ossible
s

f0;
g
n

(seeds
of
G)
and
decides
b
y
ma
jorit
y
.
Algorithm
A
00
:
On
input
x
pro
ceed
as
follo
ws.
if
x
is
one
of
those
nitely
x
0
s
return
the
kno
wn
answ
er
else
for
all
s

f0;
g
n

run
A
0
(x;
s)
return
the
ma
jorit
y
of
A
0
answ
ers
Clearly
this
A
00
deterministicly
decides
L
and
run
in
time

n


pol
y
(n)
as
required.
.
New
notion
of
Pseudorandom
generator
The
time
needed
for
A
00
to
decide
if
an
input
x
is
in
L
or
not
w
as
exp
onen
tial
in
the
length
of
it's
seed
s.
F
or
sim
ulation
purp
oses
if
the
random
seed
is
logarithmic
in
the
size
of
the
input,
running
the
pseudorandom
generator
exp
onen
tial
time
in
the
length
of
the
seed
is
really
running

0
LECTURE
.
DERANDOMIZA
TION
OF
B
P
P
it
p
olynomial
time
with
resp
ect
to
the
length
of
the
input
x.
Th
us
the
time
needed
for
sim
ulating
a
randomized
algorithm
whic
h
run
in
p
olynomial
time
remains
p
olynomial
ev
en
if
w
e
allo
w
the
pseudorandom
generator
to
run
exp
onen
tial
time
(with
logarithmic
size
seed).
In
general,
for
the
purp
ose
of
derandomizing
B
P
P
w
e
ma
y
allo
w
the
generator
to
use
exp
onen
tial
time.
There
seems
to
b
e
no
p
oin
t
insisting
that
the
generator
will
use
only
p
olynomial
time
in
the
length
of
the
seed
when
the
running
time
of
the
sim
ulating
algorithm
has
an
exp
onen
tial
factor
in
the
size
of
the
seed.
Motiv
ated
as
ab
o
v
e,
w
e
no
w
in
tro
duce
a
new
notion
of
pseudorandom
generator
with
the
fol-
lo
wing
prop
erties.
.
Indistinguishable
b
y
a
p
olynomial-size
circuit.
.
Can
run
in
exp
onen
tial
time
(
O
(k
)
on
k
-bit
seed).
The
eciency
of
the
generator
whic
h
w
as
one
of
the
k
ey
asp
ects
is
relaxed
in
the
new
notion.
This
new
notion
ma
y
seem
a
little
unfair
at
rst
since
w
e
only
giv
e
p
olynomial
time
to
the
distin-
guisher
and
allo
w
the
generator
to
run
exp
onen
tial
time.
It
ev
en
suggests
that
if
w
e
giv
e
the
seed
as
an
extra
input,
p
olynomial
size
circuit
w
ouldn't
b
e
able
to
tak
e
adv
an
tage
of
that
b
ecause
it
w
ouldn't
b
e
able
to
ev
aluate
the
generator
on
the
giv
en
seed.
The
relaxation
allo
w
us
to
construct
pseudorandom
generators
under
seemingly
w
eak
er
conditions
than
the
ones
required
for
p
olynomial
time
pseudorandom
generators
(the
assumption
ab
out
existence
of
one-w
a
y
functions).
.
Construction
of
non-iterativ
e
pseudorandom
generator
The
dierence
b
et
w
een
the
denition
of
pseudorandom
generator
that
w
e
in
tro
duced
in
this
lecture
and
the
denition
of
pseudorandom
that
w
e
had
b
efore
(the
one
usually
used
for
cryptographic
purp
oses)
is
that
w
e
allo
w
the
generator
to
run
in
time
exp
onen
tial
in
it's
input
size.
This
dierence
enables
us
to
construct
a
random
generator
under
p
ossibly
w
eak
er
conditions
with-
out
damaging
the
derandomization
pro
cess.
In
this
section
w
e
will
demonstrate
ho
w
to
construct
suc
h
a
generator
using
\unpredictable
predicate"
and
a
structure
called
\design"
(w
e
will
giv
e
a
precise
denition
later).
In
the
construction
w
e
use
t
w
o
main
to
ols
whic
h
w
e
assume
to
exist.
W
e
assume
the
existence
of
suc
h
a
predicate
and
the
existence
of
a
design,
but
later
w
e
will
sho
w
ho
w
to
construct
suc
h
a
design
hence
the
only
real
assumption
that
w
e
mak
e
is
the
existence
of
a
predicate.
In
the
previous
class
w
e
pro
v
ed
that
pseudorandom
generators
exist
if
and
only
if
one-w
a
y
p
erm
u-
tation
exist.
W
e
will
sho
w
(in
section
..
b
elo
w)
that
this
assumption
is
not
stronger
than
the
previous
assumption,
i.e
the
existence
of
one-w
a
y
p
erm
utation
implies
the
existence
of
a
predicate
(but
not
necessarily
the
opp
osite
w
a
y).
So
it
seems
b
etter
to
use
the
new
assumption
whic
h
ma
y
b
e
true
ev
en
if
there
exist
no
one-w
a
y
function.
The
previous
construction
uses
a
one-w
a
y
p
erm
utation
f
:
f0;
g
l
!
f0;
g
l
in
the
follo
wing
w
a
y:
F
rom
a
random
string
S
0
=
S
(the
seed),
compute
a
sequence
fS
i
g
b
y
S
i+
=
f
(S
i
).
The
random
bits
are
then
extracted
from
this
sequence
using
a
hard-core
predicate.
W
e
pro
v
ed
that
a
small
circuit
that
is
not
fo
oled
b
y
this
bit
sequence
can
b
e
used
to

..
CONSTR
UCTION
OF
NON-ITERA
TIVE
PSEUDORANDOM
GENERA
TOR
0
demonstrate
that
f
is
not
a
one-w
a
y
p
erm
utation
b
ecause
it
can
b
e
used
to
compute
f
 
.
The
construction
that
w
e
will
giv
e
here
has
a
dieren
t
nature.
Instead
of
creating
the
bits
sequen-
tially
w
e
will
generate
them
in
parallel.
Unpredictable
predicates
supply
an
easy
w
a
y
to
construct
one
additional
random
lo
oking
bit
from
the
seed.
The
problem
is
to
generate
more
bits.
W
e
will
do
it
b
y
in
v
oking
the
predicate
on
nearly
disjoin
t
subsets
of
the
bits
in
the
seed.
This
will
giv
e
us
bits
whic
h
are
nearly
indep
enden
t
(suc
h
that
no
p
olynomial-size
circuit
will
notice
that
they
has
some
dep
endence).
..
P
arameters

k
-
Length
of
seed.

m
-
Length
of
output
(a
p
olynomial
in
k
).

W
e
w
an
t
the
generator
to
pro
duce
an
output
that
is
indistinguishable
b
y
an
y
p
olynomial-size
circuit
(in
k
,
or
equiv
alen
tly
in
m).
..
T
o
ol
:
An
unpredictable
predicate
The
rst
to
ol
that
w
e
will
need
in
order
to
construct
a
pseudorandom
generator
is
a
predicate
that
can
not
b
e
appro
ximated
b
y
p
olynomial
sized
circuits.
Denition
.
We
say
that
an
exp(l
)-c
omputable
pr
e
dic
ate
b
:
f0;
g
l
!
f0;
g
is
unpr
e
dictable
by
smal
l
cir
cuits
(or
simply
unpr
e
dictable)
if
for
every
p
olynomial
p(),
for
al
l
suciently
lar
ge
l
's
and
for
every
cir
cuit
C
of
size
p(l
)
:
P
r
ob
[C
(U
l
)
=
b(U
l
)]
<


+

p(l
)
This
denition
require
that
small
circuits
attempting
to
compute
b
ha
v
e
only
negligible
fraction
of
adv
an
tage
o
v
er
un
biased
coin
toss.
This
is
a
real
assumption,
b
ecause
w
e
don't
kno
w
of
a
w
a
y
to
construct
suc
h
a
predicate
(unlik
e
the
next
to
ol
that
w
e
will
sho
w
ho
w
to
construct
later).
T
o
ev
aluate
the
strength
of
our
construction
w
e
pro
v
e,
in
the
next
claim,
that
the
existence
of
un-
predictable
predicate
is
guaran
teed
if
one-w
a
y
p
erm
utation
exist.
The
other
w
a
y
is
not
necessarily
true,
so
it
is
p
ossible
that
our
construction
can
b
e
applied
(if
someone
will
nd
a
pro
v
able
unpre-
dictable
predicate)
while
other
constructions
that
dep
end
on
the
existence
of
one-w
a
y
p
erm
utations
are
not
applicable
(if
someone
will
pro
v
e
that
suc
h
functions
do
not
exist).
Claim
..
If
f
0
is
a
one-way
p
ermutation
and
b
0
is
a
har
d-c
or
e
of
f
0
,
then
b(x)
def
=
b
0

f
 
0
(x)

is
an
unpr
e
dictable
pr
e
dic
ate.
Pro
of:
W
e
b
egin
b
y
noting
that
b
is
computable
in
exp
onen
tial
time
b
ecause
it
tak
es
exp
onen
tial
time
to
in
v
ert
f
and
together
with
the
computation
of
b
0
,
the
total
time
is
no
more
than
exp
onen
tial
in
the
size
of
x.

0
LECTURE
.
DERANDOMIZA
TION
OF
B
P
P
The
second
prop
ert
y
that
w
e
need
to
sho
w
is
that
it
is
imp
ossible
to
predict
b.
In
order
to
pro
v
e
this
prop
ert
y
w
e
use
the
v
ariable
y
def
=
f
 
0
(x)
to
get:
b
(
f
0
(y
))
=
b
0
(y
)
Assume
to
w
ards
con
tradiction
that
b
is
predictable.
This
means
that
there
exist
an
algorithm
A
and
a
p
olynomial
p()
suc
h
that
for
innite
n
um
b
er
of
n's
:
P
r
ob
[A(U
n
)
=
b(U
n
)]



+

p(n)
A
is
a
p
olynomial-size
algorithm
whic
h
can
predict
b
with
a
noticeable
bias.
But
f
is
a
p
erm
utation
so
w
e
ma
y
write:
P
r
ob
[A
(
f
0
(U
n
))
=
b
(
f
0
(U
n
))
]



+

p(n)
Hence,
from
the
denition
of
b
w
e
get:
P
r
ob
[A
(
f
0
(U
n
))
=
b
0
(U
n
)]



+

p(n)
whic
h
is
a
con
tradiction
to
the
denition
of
b
0
as
a
hard
core.
Recall
that
w
e
demonstrated
a
generic
hard-core
predicate
(the
inner
pro
duct
mod
)
that
is
applicable
to
an
arbitrary
one-w
a
y
p
erm
utation,
hence
essen
tially
the
last
claim
is
only
assuming
the
existence
of
one-w
a
y
p
erm
utation
(b
ecause
the
hard-core
predicate
can
alw
a
ys
b
e
found).
And
w
e
succeeded
to
sho
w
that
the
existence
of
unpredictable
predicate
ma
y
only
b
e
a
w
eak
er
assumption
than
the
existence
of
a
one-w
a
y
p
erm
utation.
W
e
did
not
pro
v
e
that
it
is
really
a
w
eak
er
assumption
but
w
e
did
sho
w
that
it
is
not
stronger.
It
ma
y
b
e
the
case
that
b
oth
assumptions
are
the
same
but
w
e
don't
kno
w
of
an
y
pro
of
for
suc
h
a
claim.
The
assumption
that
w
e
use
to
construct
a
generator
is
the
existence
of
a
"hard"
predicate.
The
w
ord
"hard"
means
that
the
predicate
can
not
b
e
appro
ximated
b
y
small
circuits.
The
hardness
of
a
predicate
is
measured
b
y
t
w
o
parameters:

The
size
of
the
circuit.

The
closeness
of
appro
ximation.
In
this
notes
w
e
use
p
olynomial-size
circuits
and
p
olynomial
appro
ximation.
Other
pap
ers
demon-
strated
that
similar
pro
cess
can
b
e
carried
out
with
dieren
t
conditions
on
this
hardness
parameters.
In
particular,
the
closeness
of
appro
ximation
parameter
is
greatly
relaxed.
..
T
o
ol
:
A
design
The
task
of
generating
a
single
bit
from
a
random
bits
seems
easy
if
y
ou
ha
v
e
the
predicate
that
w
e
assumed
to
exist
in
the
previous
section
b
ecause
the
output
of
the
predicate
m
ust
lo
ok
random
to
ev
ery
p
olynomial-size
circuit.
The
problem
is
ho
w
can
w
e
generate
more
than
one
bit.
W
e
will
do
this
using
a
collection
of
nearly
disjoin
t
sets
to
get
random
bits
that
are
almost
m
utually
indep
enden
t
(almost
means
indistinguishable
b
y
a
p
olynomial-size
circuits).
T
o
formalize
this
idea
w
e
in
tro
duce
the
notion
of
a
design:

..
CONSTR
UCTION
OF
NON-ITERA
TIVE
PSEUDORANDOM
GENERA
TOR
0
Denition
.
A
c
ol
le
ction
of
m
subsets
fI

;
I

;
:::;
I
m
g
of
f;
:::;
k
g
is
a
(k,m,l)-design
(or
simply
design)
if
the
fol
lowing
thr
e
e
pr
op
erties
exist:
.
F
or
every
i

f;
:::;
mg,
jI
i
j
=
l
.
F
or
every
i
=
j

f;
:::;
mg,
jI
i
\
I
j
j
=
o(log
k
)
.
The
c
ol
le
ction
is
c
onstructible
in
exp(k
)-time.
In
our
application
the
set
f;
:::;
k
g
is
all
bit
lo
cations
in
the
seed,
and
the
subsets
fI

;
I

;
:::;
I
m
g
corresp
ond
to
dieren
t
subsequences
extracted
from
the
seed.
F
or
example,
one
ma
y
lo
ok
at
a
ten
bit
seed
S
=
h00000i
The
subset
I
=
f;
;
g

f;
:::;
0g
corresp
ond
to
the
rst,
the
fth
and
the
sev
en
th
bits
in
the
seeds
whic
h
are,
in
this
example
:
S
[I
]
=
h00i
In
general,
for
S
=
h







k
i,
and
I
=
fi

;
:::;
i
l
g

f;
:::;
k
g
w
e
use
the
notation:
S
[I
]
def
=
h
i


i





i
k
i
..
The
construction
itself
W
e
no
w
w
an
t
to
construct
a
pseudorandom
generator
with
a
p
olynomial
stretc
hing
function.
The
size
of
the
seed
will
b
e
k
and
the
size
of
the
output
will
b
e
m
=
m(k
)
whic
h
is
a
p
olynomial
in
k
.
Supp
ose
w
e
ha
v
e
a
design
fI

;
:::;
I
m
g
whic
h
consist
of
m
subsets
of
the
set
f;
:::;
k
g.
If
these
subset
where
completely
disjoin
t
then
it
is
ob
vious
(from
Denition
.)
that
for
ev
ery
unpredictable
predicate
b,
the
sequence
hb(S
[I

])
b(S
[I

])



b(S
[I
m
])i
is
unpredictable.
Since
unpredictabilit
y
implies
pseudorandomness
w
e
get
pseudorandom
generator.
W
e
will
sho
w
that
this
is
also
true
when
the
in
tersection
of
an
y
t
w
o
subsets
is
logarithmic
in
k
and
m
(i.e,
this
is
a
design).
Our
generator
will
blo
w
up
seeds
b
y
applying
the
unpredictable
predicate
to
ev
ery
subsequence
corresp
onding
to
subsets
in
the
design.
Prop
osition
..
L
et
b
:
f0;
g
p
k
!
f0;
g
b
e
unpr
e
dictable
pr
e
dic
ate,
and
fI

;
:::;
I
m
g
b
e
a
(k
;
m;
p
k
)
-
design,
then
the
fol
lowing
function
is
a
pseudor
andom
gener
ator
(as
dene
d
in
se
c-
tion
.):
G(S
)
def
=
hb(S
[I

])
b(S
[I

])



b(S
[I
m
])i
Pro
of:
Based
on
the
denition
of
unpredictable
predicate
and
the
denition
of
a
design
it
follo
ws
that
it
tak
e
no
more
than
exp
onen
tially
man
y
steps
to
ev
aluate
G.
W
e
will
sho
w
no
w
that
no
small
circuit
can
distinguish
the
output
of
G
from
a
random
sequence.
Supp
ose
to
w
ards
con
tradiction
that
there
exist
a
family
of
p
olynomial-size
circuits
fC
k
g
k
N
and
a
p
olynomial
p()
suc
h
that
for
innitely
man
y
k
's
j
P
r
ob[C
k
(G(U
k
))
=
]
 P
r
ob[C
k
(U
m
)
=
]j
>

p(k
)

0
LECTURE
.
DERANDOMIZA
TION
OF
B
P
P
(
This
is
the
negation
of
G
b
eing
a
pseudorandom
generator
b
ecause
the
denition
of
pseudorandom
generator
demands
that
for
ev
ery
sucien
tly
large
k
's
w
e
ha
v
e
j



j
<

p(k
)
whic
h
imply
that
there
are
only
nitely
man
y
k
's
with
j



j
>

p(k
)
)
W
e
will
assume
that
this
expression
is
p
ositiv
e
and
remo
v
e
the
absolute
v
alue,
b
ecause
if
w
e
ha
v
e
innitely
man
y
k
's
suc
h
that
the
ab
o
v
e
is
true,
w
e
ha
v
e
that
half
of
them
ha
v
e
the
same
sign.
W
e
ma
y
tak
e
the
sign
as
w
e
w
an
t
since
w
e
can
alw
a
ys
tak
e
other
sequence
of
circuits
with
the
rev
ersed
sign.
So
without
loss
of
generalit
y
w
e
assume
that
for
innitely
man
y
k
's
P
r
ob[C
k
(G(U
k
))
=
]
 P
r
ob[C
k
(U
m
)
=
]
>

p(k
)
F
or
an
y
0

i

m
w
e
dene
a
"h
ybrid"
distribution
H
i
k
on
f0;
g
m
as
follo
ws:
the
rst
i
bits
are
c
hosen
to
b
e
the
rst
i
bits
of
G(U
k
)
and
the
other
m
 i
bits
are
c
hosen
uniformly
at
random
:
H
i
k
def
=
G(U
k
)
[;:::;i]

U
m i
In
this
notation
w
e
get
H
0
k
=
U
m
and
H
m
k
=
G(U
k
).
If
w
e
lo
ok
at
the
function
:
f
k
(i)
def
=
P
r
ob[C
k
(H
i
k
)
=
]
W
e
get
that,
since
f
k
(m)
 f
k
(0)
>

p(k
)
,
there
m
ust
b
e
some
0

i
k

m
suc
h
that:
f
k
(i
k
+
)
 f
k
(i
k
)
>

m


p(k
)
So
w
e
kno
w
that
there
exist
a
circuit
whic
h
b
eha
v
es
signican
tly
dieren
t
if
the
i
k
'th
bit
is
tak
en
randomly
or
from
the
generator
(the
dierence
is
greater
than
one
o
v
er
the
p
olynomial
p
0
(k
)
def
=
m

k
).
That
is:
P
r
ob[C
k
(H
i
k
+
k
)
=
]
 P
r
ob[C
k
(H
i
k
k
)
=
]
>

p
0
(k
)
W
e
will
use
this
circuit
to
construct
another
circuit
whic
h
will
b
e
able
to
guess
the
next
bit
with
some
bias.
On
i
k
bits
of
input
and
m
 i
k
bits
of
random,
the
new
circuit
C
0
k
will
b
eha
v
e
as
follo
ws:
C
0
k
(hy

;
:::;
y
i
k
i;
hR
i
k
+
;
:::;
R
m
i)
def
=
(
R
i
k
+
if
C
k
(hy

;
:::;
y
i
k
;
R
i
k
+
;
:::;
R
m
i)
=


 R
i
k
+
other
w
ise
where
hy

;
:::;
y
m
i
def
=
G(U
k
).
The
idea
b
ehind
this
construction
is
that
w
e
kno
w
that
the
the
probabilit
y
that
C
k
will
return

is
signican
tly
dieren
t
whether
R
i
k
+
equals
y
i
k
+
or
not.
This
is
true
b
ecause
when
R
i
k
+
equals
y
i
k
+
w
e
see
that
C
k
is
getting
H
i
k
+
k
as
input
and
otherwise
it
get
H
i
k
k
.
The
consequence
of
this
fact
is
that
w
e
can
use
the
answ
er
of
C
k
to
distinguish
these
t
w
o
cases.
T
o
analyze
the
b
eha
vior
of
C
0
k
more
formally
w
e
lo
ok
at
t
w
o
ev
en
ts
(or
Bo
olean
v
ariables):
A
def
=
(C
k
(hy

;
:::;
y
i
k
;
R
i
k
+
;
:::;
R
m
i)
=
)
B
def
=
(
R
i
k
+
=
y
i
k
+
)

..
CONSTR
UCTION
OF
NON-ITERA
TIVE
PSEUDORANDOM
GENERA
TOR
0
Notice
that
C
0
k
will
return
y
i
k
+
in
t
w
o
distinct
scenarios.
The
rst
scenario
is
when
R
i
k
+
=
y
i
k
+
and
C
k
returns
,
and
the
second
scenario
is
when
R
i
k
+
=
y
i
k
+
and
C
k
returns
0.
Using
the
ab
o
v
e
notation
w
e
get
that:
P
r
ob[C
0
k
=
y
i
k
+
]
=
P
r
ob[B
]

P
r
ob[AjB
]
+
P
r
ob[B
c
]

P
r
ob[A
c
jB
c
]
Since

P
r
ob[A]
=
f
(i
k
)
=
P
r
ob[C
k
(H
i
k
k
)
=
]

P
r
ob[AjB
]
=
f
(i
k
+
)
=
P
r
ob[C
k
(H
i
k
+
k
)
=
]

P
r
ob[B
]
=
P
r
ob[B
c
]
=



P
r
ob[A]
=
P
r
ob[B
]

P
r
ob[AjB
]
+
P
r
ob[B
c
]

P
r
ob[AjB
c
]
w
e
get
that:
P
r
ob[C
0
k
=
y
i
k
+
]
=
P
r
ob[B
]

P
r
ob[AjB
]
+
P
r
ob[B
c
]

P
r
ob[A
c
jB
c
]
=
P
r
ob[B
]

P
r
ob[AjB
]
+
P
r
ob[B
c
]
 P
r
ob[B
c
]

P
r
ob[AjB
c
]
=
P
r
ob[B
]

P
r
ob[AjB
]
+
P
r
ob[B
c
]
 (P
r
ob[A]
 P
r
ob[B
]

P
r
ob[AjB
])
=


+
P
r
ob[AjB
]
 P
r
ob[A]
=


+
f
(i
k
+
)
 f
(i
k
)
>


+

p
0
(k
)
Hence
the
conclusion
is:
[P
r
ob[C
0
k
(G(U
k
)
[;:::;i
k
])
=
G(U
k
)
i
k
+
]
>


+

p
0
(k
)
(W
e
use
subscript
notation
to
tak
e
partial
bits
of
a
giv
en
bit
sequence.
In
this
particular
case
G(U
k
)
[;:::;i
k
])
is
the
rst
i
k
bits
of
G(U
k
)
and
G(U
k
)
i
k
+
is
the
i
k
+
's
bit
of
G(U
k
).)
That
is,
C
0
k
can
guess
the
i
k
'th
bit
of
G(U
k
)
with

def
=

p
0
(k
)
adv
an
tage
o
v
er
un
biased
coin
toss.
Let
us
no
w
extend
C
0
to
also
get
complete
seed
as
input
(in
addition
to
S
[I

];
S
[I

];
:::;
S
[I
i
k
])
C
00
k
(S

G(S
)
[;:::;i
k
]
)
def
=
C
0
k
(G(S
)
[;:::;i
k
]
)
Since
G(U
k
)
i
k
+
is
dened
to
b
e
b(S
[I
i
k
+
]),
w
e
ha
v
e:
P
r
ob
s
[C
00
k
(S

G(S
)
[;:::;i
k
]
)
=
b(S
[I
i
k
+
])]
>


+

W
e
claim
that
there
exist


f0;
g
k
 jI
i
k
+
j
suc
h
that:
P
r
ob
s
[C
00
k
(S

G(S
)
[;:::;i
k
]
)
=
b(S
[I
i
k
+
])
j
S
[I
i
k
+
]
=
]
>


+


0
LECTURE
.
DERANDOMIZA
TION
OF
B
P
P
This
claim
is
true
b
ecause
if
w
e
lo
ok
at
a
random
selection
of
S
as
t
w
o
indep
enden
t
selections
of
S
[I
i+
]
and
S
[I
i+
]
w
e
see
that
the
a
v
erage
o
v
er
the
second
selection
is
greater
than


+

so
there
m
ust
b
e
an
elemen
t
with
w
eigh
t
greater
than
that.
No
w
w
e
come
to
the
k
ey
argumen
t
in
this
pro
of.
By
incorp
orating
C
00
k
with

to
a
new
cir-
cuit,
w
e
get
a
circuit
that
can
appro
ximate
the
v
alue
of
b(S
[I
i
k
+
])
but
it
need
the
"help"
of
b(S
[I

]);
b(S
[I

]);
:::;
b(S
[I
i
k
]).
W
e
will
sho
w
no
w
that
w
e
can
do
without
this
"help"
when
fI

;
:::;
I
m
g
is
a
design
(note
that
w
e
didn't
use
the
fact
that
this
is
a
design
un
til
no
w).
T
o
pro
v
e
that
it
is
p
ossible
to
build
a
circuit
that
do
esn't
use
the
"help",
w
e
need
to
sho
w
that
there
exist
a
p
olynomial-size
circuit
that
get
only
S
[I
i
k
+
]
and
can
appro
ximate
b(S
[I
i
k
+
]).
T
o
do
this
w
e
use
the
fact
that
all
the
bits
in
S
[I

];
S
[I

];
:::;
S
[I
i
k
]
dep
end
only
on
small
fraction
of
S
[I
i
k
+
]
so
circuits
for
computing
these
bits
are
relativ
ely
small
and
w
e
can
incorp
orated
them
in
a
circuit
that
include
all
p
ossible
v
alues
of
these
bits.
Details
follo
ws.
T
o
elab
orate
the
last
paragraph,
recall
that
the
in
tersection
of
an
y
t
w
o
subsets
in
a
design
is
at
most
O
(l
og
k
),
hence
giv
en
S
[I
i
k
+
]
=

w
e
kno
w
that
for
ev
ery
i

i
k
the
bits
in
I
i
are
xed
except
for
O
(l
og
k
)
bits
that
ma
y
b
e
in
S
[I
i
k
+
],
and
are
giv
en
as
part
of
the
input.
Since
there
are
at
most
O
(l
og
k
)
suc
h
bits,
there
exist
a
p
olynomial-size
circuit
that
can
"precompute"
the
v
alue
of
b
for
ev
ery
com
bination
of
these
bits.
The
rst
part
of
this
circuit
is
a
collection
of
tables,
one
for
ev
ery
I
i
.
Eac
h
table
is
indexed
b
y
all
p
ossible
v
alues
of
the
\free
bits"
of
an
I
i
(i.e.,
these
in
I
i
k
+
).
The
en
try
for
ev
ery
suc
h
v
alue
(of
S
[I
i
k
+
\
I
i
])
con
tains
the
corresp
onding
b(S
[I
i
])
(under
S
[I
i
k
+
]
=
).
The
second
part
of
this
circuit
just
implemen
ts
a
\selector";
that
is,
it
uses
the
bits
of
S
[I
i
k
+
in
order
to
obtain
the
appropriate
v
alues
of
b(S
[I

]);
b(S
[I

]);
:::;
b(S
[I
i
k
])
from
the
corresp
onding
tables
men
tioned
ab
o
v
e.
Since
w
e
ha
v
e
p
olynomially
man
y
en
tries
in
ev
ery
table
and
p
olynomially
man
y
tables
w
e
get
that
this
is
a
p
olynomial-size
circuit.
The
conclusion
is
that
w
e
got
a
circuit
that
can
appro
ximate
b
with
a
non
negligible
adv
an
tage
o
v
er
un
biased
coin
toss.
There
exist
innitely
man
y
suc
h
circuits
for
innitely
man
y
k
's,
whic
h
con
tradict
the
assumption
of
b
b
eing
unpredictable
predicate.
.
Constructions
of
a
design
In
this
section
w
e
describ
e
ho
w
to
construct
a
design
that
can
b
e
used
for
the
generator
that
w
e
in
tro
duced
in
the
preceding
section.
W
e
need
to
construct
m
dieren
t
subsets
of
the
set
f;
:::;
k
g
eac
h
has
size
l
with
small
in
tersections.
..
First
construction:
using
GF
(l
)
arithmetic
Assume
without
loss
of
generalit
y
that
l
is
a
prime
factor
and
let
k
=
l

(if
l
is
not
a
prime
factor
pic
k
the
smallest
p
o
w
er
of

whic
h
is
greater
than
l
).
F
or
the
eld
F
def
=
GF
(l
),
w
e
get
that
the
Cartesian
pro
duct
F

F
con
tains
k
=
l

elemen
ts
whic
h
w
e
iden
tify
,
with
the
k
elemen
ts
of
f;
:::;
k
g.
Ev
ery
n
um
b
er
in
f;
:::;
k
g
is
assigned
to
a
pair
in
F

F
(in
a
one-to-one
corresp
ondence).
In
the
forgoing
discussion
w
e
will
in
terc
hange
the
pair
and
it's
represen
tativ
e
in
f;
:::;
k
g
freely
.
F
or
ev
ery
p
olynomial
p()
of
degree
d
o
v
er
F
in
tro
duce
the
subset:

..
CONSTR
UCTIONS
OF
A
DESIGN
0	
I
p
def
=
fhe;
p(e)i
:
e

F
g

F

F
W
e
get
that:
.
The
size
of
eac
h
set
is
jI
p
j
=
jF
j
=
l
.
.
F
or
ev
ery
t
w
o
p
olynomials
p
=
q
the
sets
I
p
and
I
q
in
tersects
in
at
most
d
p
oin
ts
(that
is,
jI
p
\
I
q
j

d).
This
is
true
since:
jfhe;
p(e)i
:
e

F
g
\
fhe;
q
(e)i
:
e

F
gj
=
jfhe
:
p(e)
=
q
(e)gj
But
the
p
olynomial
p(e)
 q
(e)
has
degree
smaller
or
equal
to
d
so
it
can
only
ha
v
e
d
zero
es
(due
to
the
F
undamen
tal
Theorem
of
Algebra).
.
There
are
jF
j
d+
=
l
d+
p
olynomials
o
v
er
GF
(l
)
so
for
ev
ery
p
olynomial
P
()
w
e
can
nd
d
suc
h
that
the
n
um
b
er
of
sets
is
greater
than
P
(l
).
.
This
structure
is
constructible
in
exp
onen
tial
(actually
p
olynomial)
in
k
time
b
ecause
all
w
e
need
to
do
is
simple
arithmetic
in
GF
(l
).
The
conclusion
is
that
w
e
ha
v
e
a
design
(see
Denition
.)
that
can
b
e
applied
in
the
construction
of
pseudorandom
generator
that
w
e
ga
v
e
ab
o
v
e.
This
design
remo
v
e
the
second
assumption
that
w
e
made
ab
out
the
existence
of
a
design,
so
w
e
get
(as
promised)
that
the
only
assumption
needed
in
order
to
allo
w
derandomization
of
B
P
P
is
that
there
exist
an
unpredictable
predicate.
..
Second
construction:
greedy
algorithm
In
this
subsection
w
e
in
tro
duce
another
construction
of
a
design.
W
e
call
this
algorithm
greedy
b
ecause
it
just
scans
all
the
subsets
of
the
needed
size
un
til
it
nd
one
that
do
esn't
o
v
erlap
all
previously
selected
subsets
to
o
m
uc
h.
This
algorithm
is
"simpler"
than
the
one
w
e
ga
v
e
in
the
previous
subsection
but
w
e
need
to
sho
w
that
it
w
ork
correctly
.
Consider
the
follo
wing
parameters:

k
=
l


m
=
pol
y
(k
)

W
e
w
an
t
that
for
all
i
to
ha
v
e
jI
i
j
=
l
and
for
all
i
=
j
,
jI
i
\
I
j
j
=
O
(l
og
k
)
F
or
these
parameters
w
e
giv
e
a
simple
algorithm
that
scans
all
subsets
one
b
y
one
to
nd
the
next
set
to
include
in
the
design:
for
i
=

to
m
for
all
I

[k
],
jI
j
=
l
do
f
l
ag
 
F
ALS
E
for
j
=

to
i
 

0
LECTURE
.
DERANDOMIZA
TION
OF
B
P
P
if
jI
i
\
I
j
j
>
l
og
k
then
f
l
ag
 
T
R
U
E
if
f
l
ag
=
T
R
U
E
then
I
i
 
I
This
algorithm
run
in
an
exp
onen
tial
time
b
ecause
there
are

l
subsets
of
size
l
and
w
e
scan
them
m
times.
Since
m


l
<

k
w
e
get
that
ev
en
if
w
e
had
to
scan
all
the
subsets
in
ev
ery
round
w
e
could
nish
the
pro
cess
in
time
exp
onen
tial
in
k
.
T
o
pro
v
e
that
the
algorithm
w
orks
it
is
enough
to
sho
w
that
if
w
e
ha
v
e
I

;
I

;
:::;
I
i 
suc
h
that
.
i

m
.
F
or
ev
ery
j
<
i
:
jI
j
j
=
l
.
F
or
ev
ery
j

;
j

<
i
:
jI
j

\
I
j

j

l
og
m
+

Then
there
exist
another
set
I
i

[k
]
suc
h
that
jI
i
j
=
l
and
for
ev
ery
j
<
i
:
jI
j
\
I
i
j
<

+
l
og
m.
W
e
pro
v
e
this
claim
using
the
Probabilistic
Metho
d.
That
is,
w
e
will
sho
w
that
most
c
hoices
of
I
i
will
do.
In
fact
w
e
sho
w
the
follo
wing
Claim
..
L
et
I

;
I

;
:::;
I
i 

[k
]
e
ach
of
size
l
.
Then
ther
e
exists
an
l
-set,
I
,
such
that
for
al
l
j
's,
it
is
the
c
ase
that
jI
j
\
I
j

log
m
+
.
Pro
of:
W
e
rst
consider
the
probabilit
y
that
a
uniformly
c
hosen
l
-set
has
a
large
in
tersection
with
a
xed
l
-set,
denoted
S
.
Actually
,
it
will
b
e
easier
to
analyze
the
in
tersection
of
S
with
a
set
R
selected
at
random
so
that
for
ev
ery
i

[k
]
P
r
ob[i

R
]
=

l
That
is,
the
size
of
R
is
a
random
v
ariable,
with
exp
ectation
k
=l
(whic
h
equals
l
).
W
e
will
sho
w
that
with
v
ery
high
probabilit
y
the
in
tersection
of
R
with
S
is
not
to
o
big,
and
that
v
ery
high
probabilit
y
jR
j

l
(and
so
w
e
can
nd
an
appropriate
l
-set).
Details
follo
w.
Let
s
i
b
e
the
i'th
elemen
t
in
S
sorted
in
an
y
order
(e.g.,
the
natural
increasing
order).
Consider
the
sequence
fX
i
g
l
i=
of
random
v
ariables
dened
as
X
i
def
=
(

if
s
i

R
0
other
w
ise
Since
these
are
indep
enden
t
Bo
olean
random
v
ariables
with
P
r
ob[X
i
=
]
=

l
for
eac
h
i,
w
e
can
use
Cherno
's
b
ound
to
get:
P
r
ob
[jS
\
R
j
>

+
l
og
m]
=
P
r
ob
h
P
l
i=
X
i
>

+
l
og
m
i
=
P
r
ob

P
l
i=
X
i
l
>

l
+
l
og
m
l

<
P
r
ob





P
l
i=
X
i
l
 
l




>
l
og
m
l

<


e
 l
og

m

=m

..
CONSTR
UCTIONS
OF
A
DESIGN

It
follo
ws
that
for
R
selected
as
ab
o
v
e,
the
probabilit
y
that
there
exists
an
I
j
so
that
jR
\
I
j
j
>

+
log
m
is
b
ounded
ab
o
v
e
b
y
i 
m
<


.
The
only
problem
is
that
suc
h
an
R
is
not
necessarily
of
size
l
.
W
e
shall
sho
w
that
with
high
probabilit
y
the
size
of
R
is
at
least
l
,
and
so
it
con
tains
a
subset
whic
h
will
do
(as
I
i
).
Consider
the
sequence
fY
i
g
k
i=
dened
as
Y
i
def
=
(

if
i

R
0
other
w
ise
Then,
applying
Cherno
's
Bound
w
e
get:
P
r
ob[jR
j
<
l
]

P
r
ob[j

k
P
k
i=
Y
i
 
l
j
<

l
]
<


e
 



Th
us,
for
R
selected
as
ab
o
v
e,
the
probabilit
y
that
either
jR
j
<
l
or
there
exists
an
I
j
so
that
jR
\
I
j
j
>

+
log
m
is
strictly
smal
ler
than
.
Therefore
,
there
exists
a
set
R
suc
h
that
jR
j

l
and
y
et,
for
ev
ery
j
<
i,
w
e
ha
v
e
jR
\
I
j
j


+
log
m.
An
y
l
-subset
of
R
qualies
as
the
set
asserted
b
y
the
claim.
W
e
stress
that
this
discussion
ab
out
a
randomly
selected
set
is
not
part
of
the
algorithm.
The
algorithm
itself
is
totally
deterministic.
The
randomness
is
just
in
our
discussion
{
it
serv
es
as
a
to
ol
to
sho
w
that
the
algorithm
will
alw
a
ys
nd
what
it
is
lo
oking
for
in
ev
ery
step.
Bibliographic
Notes
This
lecture
is
based
on
[].
F
urther
derandomization
results,
building
on
[],
can
b
e
found
in
[],
[]
and
[].
Sp
ecically
,
in
the
latter
pap
er
a
\full
derandomization
of
BPP"
is
pro
vided
under
the
assumption
that
there
exists
a
language
L

E
ha
ving
almost-ev
erywhere
exp
onen
tial
circuit
complexit
y
.
That
is:
L
et
E
def
=
[
c
Dtime
(t
c
),
with
t
c
(n)
=

cn
.
Supp
ose
that
ther
e
exists
a
language
L

E
and
a
c
onstant

>
0
such
that,
for
al
l
but
nitely
many
n's,
any
cir
cuit
C
n
which
c
orr
e
ctly
de
cides
L
on
f0;
g
n
has
size
at
le
ast

n
.
Then,
B
P
P
=
P
.
.
L.
Babai,
L.
F
ortno
w,
N.
Nisan
and
A.
Wigderson.
BPP
has
Sub
exp
onen
tial
Time
Sim
ulations
unless
EXPTIME
has
Publishable
Pro
ofs.
Complexity
The
ory,
V
ol.
,
pages
0{,
		.
.
R.
Impagliazzo.
Hard-core
Distributions
for
Somewhat
Hard
Problems.
In
th
F
OCS,
pages
{,
		.
.
R.
Impagliazzo
and
A.
Wigderson.
P=BPP
if
E
requires
exp
onen
tial
circuits:
Derandomizing
the
X
OR
Lemma.
In
	th
STOC,
pages
0{	,
		.
.
N.
Nisan
and
A.
Wigderson.
Hardness
vs
Randomness.
JCSS,
V
ol.
	,
No.
,
pages
	{,
		.


LECTURE
.
DERANDOMIZA
TION
OF
B
P
P

Lecture

Derandomizing
Space-Bounded
Computations
Notes
tak
en
b
y
Eilon
Reshef
Summary:
This
lecture
considers
derandomization
of
space-b
ounded
computations.
W
e
sho
w
that
B
P
L

D
S
P
AC
E
(log

n),
namely
,
an
y
b
ounded-probabilit
y
Logspace
algorithm
can
b
e
deterministically
em
ulated
in
O
(log

n)
space.
W
e
sho
w
that
B
P
L

S
C
,
namely
,
an
y
suc
h
algorithm
can
b
e
deterministically
em
ulated
in
O
(log

n)
space
and
(sim
ultaneously)
in
p
olynomial
time.
.
In
tro
duction
This
lecture
considers
derandomization
of
space-b
ounded
computations.
Whereas
curren
t
tec
h-
niques
for
derandomizing
time-b
ounded
computations
rely
up
on
unpro
v
en
complexit
y
assumptions,
the
derandomization
tec
hnique
illustrated
in
this
lecture
stands
out
in
its
abilit
y
to
deriv
e
its
results
exploiting
only
the
pure
com
binatorial
structure
of
space-b
ounded
T
uring
mac
hines.
As
in
previous
lectures,
the
construction
yields
a
pseudorandom
generator
that
\fo
ols"
T
uring
mac
hines
of
the
class
at
hand,
when
in
our
case
the
pseudorandom
generator
generates
a
sequence
of
bits
that
lo
oks
truly
random
to
an
y
space-b
ounded
mac
hine.
.
The
Mo
del
W
e
consider
probabilistic
T
uring
mac
hines
along
the
lines
of
the
online
mo
del
discussed
in
Lecture
.
A
probabilistic
T
uring
mac
hine
M
has
four
tap
es:
.
A
read-only
bidirectional
input
tap
e.
.
A
read-only
unidirectional
random
tap
e.
.
A
read-write
bidirectional
w
ork
tap
e.
.
A
write-only
unidirectional
output
tap
e.
W
e
consider
B
P
S
P
AC
E
()
,
the
family
of
b
ounde
d
pr
ob
ability
space-b
ounded
complexit
y
classes.
These
classes
are
the
natural
t
w
o-sided
error
coun
terparts
of
the
single-sided
error
classes
RS
P
AC
E
(),
dened
in
Lecture

(Denition
.	).



LECTURE
.
DERANDOMIZING
SP
A
CE-BOUNDED
COMPUT
A
TIONS
Figure
.:
An
Execution
Graph
of
a
Bounded-Space
T
uring
Machine
F
ormally
,
Denition
.
F
or
any
function
s()
:
N
!
N,
the
c
omplexity
class
B
P
S
P
AC
E
(s())
is
the
set
of
al
l
languages
L
for
which
ther
e
exists
a
r
andomize
d
T
uring
machine
M
such
that
on
an
input
x
.
M
uses
at
most
s(jxj)
sp
ac
e.
.
The
running
time
of
M
is
b
ounde
d
by
exp(s(jxj)).
.
x

L
)
Pr
[M
(x)
=
]

=.
.
x

L
)
Pr
[M
(x)
=
]
<
=.
Recall
that
condition
()
is
imp
ortan
t,
as
otherwise
N
S
P
AC
E
()
=
RS
P
AC
E
()

B
P
S
P
AC
E
()
.
As
usual,
w
e
are
in
terested
in
the
cases
where
s()

log
().
In
particular,
our
tec
hniques
deran-
domize
the
complexit
y
class
B
P
L,
dened
as
Denition
.
B
P
L

=
B
P
S
P
AC
E
(log
)
Throughout
the
rest
of
the
discussion
w
e
assume
that
the
problems
at
hand
are
decision
prob-
lems,
that
all
functions
are
space-constructible,
and
that
all
logarithms
are
of
base
.
.
Execution
Graphs
W
e
represen
t
the
set
of
p
ossible
executions
of
a
B
P
S
P
AC
E
()
T
uring
mac
hine
M
on
an
input
x,
jxj
=
n,
as
a
la
y
ered
directed
graph
G
M
;x
(see
Figure
.).
A
v
ertex
in
the
i-th
la
y
er
of
G
M
;x
corresp
onds
to
a
p
ossible
conguration
of
M
after
it
has
consumed
exactly
i
random
bits,
i.e.,
when
the
head
reading
from
the
random
tap
e
is
on
the
i-th
cell.
Th
us,
the
i-th
la
y
er
of
G
M
;x
corresp
onds
to
the
set
of
all
suc
h
p
ossible
congurations.
G
M
;x

..
EXECUTION
GRAPHS

0
1
0
1
1
0
Figure
.:
Edges
in
the
Execution
Graph
G
M
;x
con
tains
an
edge
from
a
conguration
v
ertex
in
the
i-th
la
y
er
to
a
conguration
v
ertex
in
the
(i
+
)-th
la
y
er
if
there
is
a
p
ossible
transition
b
et
w
een
the
t
w
o
congurations.
F
ormally
,
G
M
;x
=
(V
M
;x
;
E
M
;x
)
is
dened
as
follo
ws.
F
or
eac
h
i
=
;
:
:
:
;
D
,
with
D

exp(s(n)),
let
the
i-th
la
y
er
V
i
M
;x
b
e
the
set
of
all
p
ossible
congurations
of
M
giv
en
that
M
has
consumed
exactly
i
random
bits.
The
v
ertex
set
V
M
;x
is
a
union
of
all
la
y
ers
V
i
M
;x
.
F
or
eac
h
v
ertex
v

V
i
M
;x
,
the
edge
set
E
M
;x
con
tains
t
w
o
outgoing
directed
edges
to
v
ertices
f
0
(v
);
f

(v
)
in
V
i+
M
;x
,
where
f
0
(v
)
(resp.
f

(v
))
corresp
onds
to
the
conguration
M
reac
hes
follo
wing
the
sequence
of
transitions
carried
out
after
reading
a
\0"
(resp.
\")
bit
from
the
random
tap
e,
and
un
til
the
next
bit
is
read
(see
Figure
.).
F
or
the
con
v
enience
of
the
exp
osition
b
elo
w,
assume
that
eac
h
of
the
t
w
o
edges
is
lab
eled
b
y
its
corresp
onding
bit,
i.e.,
\0"
or
\".
Note
that
when
the
lo
cation
of
the
head
on
the
random
tap
e
is
xed,
a
conguration
of
M
is
fully
determined
b
y
the
con
ten
ts
of
the
w
ork
tap
e
and
b
y
the
p
ositions
of
the
heads
on
the
w
ork
tap
e
and
on
the
input
tap
e.
Th
us,
jV
i
M
;x
j


s(n)

s(n)

n

exp(s(n)):
The
initial
conguration
of
M
corresp
onds
to
a
designated
v
ertex
v
0

V
0
M
;x
.
Similarly
,
the
set
of
nal
v
ertices
V
D
M
;x
is
partitioned
in
to
the
set
of
accepting
congurations
V
A
CC
and
the
set
of
rejecting
congurations
V
REJ
.
A
r
andom
walk
on
G
M
;x
is
a
sequence
of
steps,
emanating
from
v
0
,
and
pro
ceeding
along
the
directed
edges
of
G
M
;x
,
where
in
eac
h
step
the
next
edge
to
b
e
tra
v
ersed
is
selected
uniformly
at
random.
Under
this
in
terpretation,
the
probabilit
y
that
a
mac
hine
M
accepts
x
is
exactly
the
probabilit
y
that
a
random
w
alk
emanating
from
v
0
reac
hes
a
v
ertex
in
V
A
CC
.
In
con
trast,
a
guide
d
walk
on
G
M
;x
with
a
guide
R
is
a
sequence
of
steps,
emanating
from
v
0
,
and
pro
ceeding
along
the
directed
edges
of
G
M
;x
,
where
in
the
i-th
step
the
next
edge
to
b
e
tra
v
ersed
is
determined
b
y
the
i-th
bit
of
the
guide
R
,
i.e.,
the
edge
tak
en
is
the
one
whose
lab
el
is
equal
to
the
v
alue
of
the
i-th
bit
of
R
.
Let
AC
C
(G
M
;x
;
R
)
denote
the
ev
en
t
that
a
guided
w
alk
on
G
M
;x
with
a
guide
R
reac
hes
a
v
ertex
in
V
A
CC
.
In
this
view,
Pr[M
accepts
x]
=
Pr
R
R
f0;g
D
[AC
C
(G
M
;x
;
R
)];
when
R
is
selected
uniformly
at
random
from
the
set
f0;
g
D
.
Th
us,
a
language
L
is
in
B
P
L
if
there
exists
a
T
uring
mac
hine
M
with
a
space
b
ound
of
s(n)
=
O
(log
(n))
suc
h
that
for
D
(n)
=
exp(s(n))
=
p
oly
(n),


LECTURE
.
DERANDOMIZING
SP
A
CE-BOUNDED
COMPUT
A
TIONS

Whenev
er
x

L,
Pr
R
R
f0;g
D
(n)
[AC
C
(G
M
;x
;
R
)]

=.

Whenev
er
x

L,
Pr
R
R
f0;g
D
(n)
[AC
C
(G
M
;x
;
R
)]
<
=.
A
(D
;
W
)-gr
aph
G
is
a
graph
that
corresp
onds
to
the
execution
of
some
s()-space-b
ounded
T
uring
mac
hine
on
an
input
x,
with
a
\depth"
(n
um
b
er
of
la
y
ers)
of
D
,
D

exp(s(jxj))
and
a
\width"
(n
um
b
er
of
v
ertices
in
eac
h
la
y
er)
of
W
,
W

exp(s(jxj)).
In
the
sequel,
w
e
presen
t
a
derandomization
metho
d
that
\fo
ols"
an
y
(D
;
W
)-graph
b
y
replacing
the
random
guide
R
with
a
pseudorandom
guide
R
0
.
.
Univ
ersal
Hash
F
unctions
The
construction
b
elo
w
is
based
up
on
a
univ
ersal
family
of
hash
functions,
H
`
=
fh
:
f0;
g
`
!
f0;
g
`
g:
Recall
the
follo
wing
denition:
Denition
.
A
family
of
functions
H
=
fh
:
A
!
B
g
is
c
al
le
d
a
univ
ersal
family
of
hash
functions
if
for
every
x

and
x

in
A,
x

=
x

,
Pr
h
R
H
[h(x

)
=
y

and
h(x

)
=
y

]
=


jB
j


.
Note
that
in
our
case
the
family
H
`
is
degenerate,
since
the
functions
in
H
`
map
`-bit
strings
to
`-bit
strings,
and
th
us
do
not
ha
v
e
an
y
\shrinking"
b
eha
vior
whatso
ev
er.
The
construction
requires
that
the
functions
h
in
H
`
ha
v
e
a
succinct
represen
tation,
i.e.,
jhhij
=
`.
An
example
of
suc
h
a
family
is
the
set
of
all
linear
functions
o
v
er
GF
(
`
),
namely
H
`

=
fh
a;b
j
a;
b

GF
(
`
)g;
where
h
a;b
(x)

=
ax
+
b;
with
GF
(
`
)
arithmetic.
Clearly
,
jhh
a;b
ij
=
`,
and
h
a;b
can
b
e
computed
in
space
O
(`).
F
or
the
purp
ose
of
our
construction,
a
hash
function
h
is
wel
l-b
ehave
d
with
resp
ect
to
t
w
o
sets
A
and
B
,
if
it
\extends
w
ell"
to
the
t
w
o
sets,
i.e.,



Pr
x
R
f0;g
`
[x

A
and
h(x)

B
]
 (A)

(B
)





 `=
;
where
for
an
y
set
S

f0;
g
`
,
w
e
denote
b
y
(S
)
the
probabilit
y
that
a
random
elemen
t
x
hits
the
set
S
,
namely
,
(S
)

=
jS
j

`
=
Pr
xf0;g
`
[x

S
]:
The
follo
wing
prop
osition
asserts
that
for
an
y
t
w
o
sets
A
and
B
,
almost
all
of
the
functions
in
H
`
are
w
ell-b
eha
v
ed
in
the
ab
o
v
e
sense
with
resp
ect
to
A
and
B
.
F
ormally
,
Prop
osition
..
F
or
every
universal
family
H
`
of
hash
functions,
and
for
every
two
sets
A;
B

f0;
g
`
,
al
l
but
a

 `=
fr
action
of
the
h

H
`
satisfy



Pr
xf0;g
`
[x

A
and
h(x)

B
]
 (A)

(B
)





 `=
:

..
CONSTR
UCTION
O
VER
VIEW

.
Construction
Ov
erview
W
e
no
w
turn
to
consider
an
arbitrary
(D
;
W
)-graph
G
represen
ting
the
p
ossible
executions
of
an
s()-space-b
ounded
T
uring
mac
hine
M
on
an
input
x,
where
jxj
=
n.
W
e
construct
a
pseudorandom
generator
H
:
f0;
g
k
!
f0;
g
D
that
em
ulates
a
truly
random
guide
on
G.
F
ormally
,
Denition
.
A
function
H
:
f0;
g
k
!
f0;
g
D
is
a
(D
;
W
)-pseudorandom
generator
if
for
every
(D
;
W
)-gr
aph
G,



Pr
Rf0;g
D
[AC
C
(G;
R
)]
 Pr
R
0
f0;g
k
[AC
C
(G;
H
(R
0
))]




=0:
In
Section
.,
w
e
pro
v
e
the
follo
wing
theorem:
Theorem
.
Ther
e
exists
a
(D
;
W
)-pseudor
andom
gener
ator
H
()
with
k
(n)
=
O
(log
D

log
W
).
F
urther,
H
()
is
c
omputable
in
sp
ac
e
line
ar
in
its
input.
In
particular,
Corollary
.
Ther
e
exists
a
(D
;
W
)-pseudor
andom
gener
ator
H
()
with
the
fol
lowing
p
ar
ame-
ters:

s(n)
=
(log
n).

D
(n)

p
oly
(n).

W
(n)

p
oly
(n).

k
(n)
=
O
(log

n).
By
trying
all
p
ossible
assignmen
ts
for
the
seed
of
H
(),
it
follo
ws
that
Corollary
.
B
P
L

D
S
P
AC
E
(log

n).
In
fact,
as
will
b
e
eviden
t
from
the
construction
b
elo
w,
the
pseudorandom
generator
H
op
erates
in
space
O
(log
n).
Ho
w
ev
er,
the
space
complexit
y
of
the
derandomization
algorithm
is
dominated
b
y
the
space
used
for
writing
do
wn
the
seed
for
H
().
Note
that
this
result
is
not
v
ery
striking,
since
the
same
result
is
kno
wn
to
hold
for
the
single-
sided
error
class
RL,
as
RL

N
L

D
S
P
AC
E
(log

n).
Ho
w
ev
er,
as
sho
wn
b
elo
w,
the
construction
can
b
e
extended
to
yield
more
striking
results.
.
The
Pseudorandom
Generator
In
this
section,
w
e
formally
describ
e
a
(D
;
W
)-pseudorandom
generator
as
dened
in
Theorem
..
Without
loss
of
generalit
y
,
w
e
assume
D

W
.
The
pseudorandom
generator
H
is
based
on
the
univ
ersal
family
of
hash
functions
H
`
,
and
extends
strings
of
length
O
(`

)
to
strings
of
length
D

exp(`),
for
`
=
(log
jW
j).
The
pseudorandom
strings
cannot
b
e
distinguished
from
truly
random
strings
b
y
an
y
(D
;
W
)-graph.
The
input
to
H
is
in
terpreted
as
the
tuple
I
=
(r
;
hh

i;
hh

i;
:
:
:
;
hh
`
0
i);


LECTURE
.
DERANDOMIZING
SP
A
CE-BOUNDED
COMPUT
A
TIONS
r
r
r
r
h1(x)
h1(r)
h2(r)
h2(h1(r))
h3(h2(h1(r))).
Figure
.:
The
Computation
T
ree
T
of
H
()
u
hi(u)
u
Figure
.:
A
No
de
in
the
Computation
T
ree
of
H
where
jr
j
=
`,
h

;
:
:
:
;
h
`
0
are
functions
in
H
`
,
and
`
0
=
log
(D
=`).
It
can
b
e
easily
observ
ed
that
the
length
of
the
input
I
is
indeed
b
ounded
b
y
O
(`

).
No
w,
giv
en
an
input
I
,
it
ma
y
b
e
most
con
v
enien
t
to
follo
w
the
computation
of
the
pseudo-
random
generator
H
b
y
considering
a
computation
o
v
er
a
complete
binary
tree
T
of
depth
`
0
(see
Figure
.).
The
computation
assigns
a
v
alue
to
eac
h
no
de
of
the
tree
as
follo
ws.
First,
set
the
v
alue
of
the
ro
ot
of
the
tree
to
b
e
r
.
Next,
giv
en
that
a
no
de
lo
cated
at
depth
i
 
has
a
v
alue
of
u,
set
the
v
alue
of
its
left
c
hild
to
u,
and
the
v
alue
of
its
righ
t
c
hild
to
h
i
(u)
(see
Figure
.).
Finally
,
H
()
returns
the
concatenation
of
the
binary
v
alues
of
the
lea
v
es
of
the
tree,
left
to
righ
t.
More
formally
,
H
(I
)

=

0



:
:
:



`
0
 
;
where

j
is
dened
suc
h
that
if
the
binary
represen
tation
of
j
is






`
0
,
then

j

=
h

`
0
`
0





h



(r
);
where
h

i
(z
)
=
h
i
(z
)
and
h
0
i
(z
)
=
z
for
ev
ery
z
.
Y
et
another
w
a
y
of
describing
H
()
is
recursiv
ely
as
H
(r
;
hh
i
i;
:
:
:
;
hh
`
0
i)
=
H
(r
;
hh
i+
i;
:
:
:
;
hh
`
0
i)

H
(h
i
(r
);
hh
i+
i;
:
:
:
;
hh
`
0
i);
where
H
(z
)
=
z
.

..
ANAL
YSIS
	
0
l
2l
0101110
1110100
Figure
.:
Contracting
`
La
y
ers
Note
that
the
output
of
H
()
is
comp
osed
of
exactly

`
0
=
D
=`
blo
c
ks,
eac
h
of
length
`,
and
hence
the
length
of
the
output
H
(I
)
is
indeed
D
.
.
Analysis
It
remains
to
see
that
H
()
is
indeed
a
(D
;
W
)-pseudorandom
generator.
Theorem
.
(Theorem
.
rephrased)
H
()
is
a
(D
;
W
)-pseudor
andom
gener
ator.
Pro
of:
Consider
a
(D
;
W
)
graph
G
M
;x
that
corresp
onds
to
the
p
ossible
executions
of
a
prosp
ectiv
e
distinguisher
M
on
an
input
x.
W
e
sho
w
that
H
()
is
a
(D
;
W
)-pseudorandom
generator
b
y
sho
wing
that
a
guided
w
alk
on
G
M
;x
using
the
guide
H
(I
)
b
eha
v
es
\similarly"
to
a
truly
random
w
alk,
where
I
=
(z
;
hh

i;
:
:
:
;
hh
`
0
i)
is
dra
wn
uniformly
as
a
seed
of
H
.
In
an
initial
step,
prune
la
y
ers
from
G
M
;x
,
so
that
only
eac
h
`-th
la
y
er
remains,
con
tracting
edges
as
necessary
(see
Figure
.).
F
ormally
,
construct
a
la
y
ered
m
ultigraph
G
0
whose
v
ertex
set
is
the
union
of
the
v
ertex
sets
V
0
M
;x
;
V
`
M
;x
;
V
`
M
;x
;
:
:
:
;
V
D
M
;x
,
and
whose
edges
corresp
ond
to
directed
paths
of
length
`
in
G
M
;x
.
Lab
el
eac
h
edge
in
the
m
ultigraph
G
0
b
y
an
`-bit
string
whic
h
is
the
concatenation
of
the
lab
els
of
the
edges
along
the
corresp
onding
directed
path
in
G
M
;x
.
Th
us,
ev
ery
m
ultiedge
e
in
G
0
corresp
onds
to
a
subset
of
f0;
g
`
.
Clearly
,
a
random
w
alk
on
G
M
;x
is
equiv
alen
t
to
a
random
w
alk
on
G
0
,
when
at
eac
h
step
an
`-bit
string
R
is
dra
wn
uniformly
,
and
the
edge
tra
v
ersed
is
the
edge
whose
lab
el
is
R
.
The
analysis
asso
ciates
H
()
with
a
sequence
of
coarsenings.
A
t
eac
h
suc
h
coarsening,
a
new
hash
function
h
i
,
uniformly
dra
wn
from
H
`
,
is
used
to
decrease
the
n
um
b
er
of
truly
random
bits
needed
for
the
random
w
alk
b
y
a
factor
of
.
After
`
0
suc
h
coarsenings,
the
only
truly
random
bits

0
LECTURE
.
DERANDOMIZING
SP
A
CE-BOUNDED
COMPUT
A
TIONS
required
for
the
w
alk
are
the
`
random
bits
of
r
,
with
the
additional
bits
used
to
enco
de
the
hash
functions
h

;
:
:
:
;
h
`
0
.
W
e
b
egin
b
y
presen
ting
the
rst
coarsening
step.
In
this
step,
the
random
guide
R
=
(R

;
R

;
:
:
:
;
R
D
=`
)
is
replaced
b
y
a
\semi-random"
guide
R
0
=
(R

;
h
`
0
(R

);
R

;
h
`
0
(R

);
:
:
:
;
R
D
=` 
;
h
`
0
(R
D
=` 
)),
where
h
`
0
and
the
R
i
's
are
dra
wn
uniformly
at
random.
Belo
w
w
e
sho
w
that
the
semi-random
guide
b
eha
v
es
\almost
lik
e"
the
truly
random
guide,
i.e.,
that
for
some
,


Pr
R
[AC
C
(G
0
;
R
)]
 Pr
R
0
[AC
C
(G
0
;
R
0
)]


<
:
W
e
b
egin
with
a
tec
hnical
prepro
cessing
step,
remo
ving
from
G
0
edges
whose
tra
v
ersal
prob-
abilit
y
is
v
ery
small.
F
ormally
,
let
E
ligh
t
denote
the
set
of
all
\ligh
t"
edges,
i.e.,
edges
(u;
v
)
for
whic
h
Pr
R
i

R
f0;g
`
[u
!
v
]
<
=W

.
Create
a
graph
G
0
0
whose
v
ertex
set
is
the
same
as
G
0
's,
but
con
taining
only
edges
in
E
n
E
ligh
t
.
W
e
rst
sho
w
that
the
remo
v
al
of
the
ligh
t
edges
ha
v
e
a
negligible
eect.
F
ormally
,
Lemma
..
F
or


=
=W
,
.
j
Pr
R
[AC
C
(G
0
0
;
R
)]
 Pr
R
[AC
C
(G
0
;
R
)]j
<


.
.
j
Pr
R
0
[AC
C
(G
0
0
;
R
0
)]
 Pr
R
0
[AC
C
(G
0
;
R
0
)]j
<


.
Pro
of:
F
or
the
rst
part,
the
probabilit
y
that
a
random
w
alk
R
uses
an
edge
in
E
ligh
t
is
at
most


=
D

(=W

)

=W
,
and
hence


Pr
R
[AC
C
(G
0
;
R
)]
 Pr
R
[AC
C
(G
0
0
);
R
)]


<


:
F
or
the
second
part,
consider
t
w
o
consecutiv
e
steps
guided
b
y
R
0
along
the
edges
of
G
0
.
The
probabilit
y
that
the
rst
step
of
R
0
tra
v
erses
a
ligh
t
edge
is
b
ounded
b
y
=W

.
By
Prop
osition
..
with
resp
ect
to
the
sets
f0;
g
`
and
the
set
of
ligh
t
edges
a
v
ailable
for
the
second
step,
for
all
but
a
fraction
of

 `=
of
the
hash
functions
h
`
0
,
the
probabilit
y
that
the
second
step
of
R
0
tra
v
erses
a
ligh
t
edge
is
b
ounded
b
y
=W

+

 `=

=W

,
assuming
that
the
constan
t
for
`
is
large
enough.
Hence,
except
for
a
fraction
of
(D
=)


 `=
<


=
of
the
hash
functions,
the
o
v
erall
probabilit
y
that
R
0
tra
v
erses
a
ligh
t
edge
is
b
ounded
b
y
D

(=W

)
<


=.
Th
us,
the
o
v
erall
probabilit
y
of
hitting
a
ligh
t
edge
is
b
ounded
b
y


.
It
th
us
remains
to
sho
w
that
the
semi-random
guide
R
0
b
eha
v
es
w
ell
with
resp
ect
to
the
pruned
graph
G
0
0
.
F
ormally
,
for
some


sp
ecied
b
elo
w,
w
e
sho
w
that
Lemma
..
j
Pr
R
[AC
C
(G
0
0
;
R
)]
 Pr
R
0
[AC
C
(G
0
0
;
R
0
)]j
<


.
Pro
of:
Consider
three
consecutiv
e
la
y
ers
of
G
0
0
,
sa
y
V
0
M
;x
,
V
`
M
;x
,
V
`
M
;x
(see
Figure
.),
and
x
a
triplet
of
v
ertices
u

V
0
M
;x
,
v

V
`
M
;x
,
and
w

V
`
M
;x
for
whic
h
the
edges
(u;
v
)
and
(v
;
w
)
are
in
the
edge
set
of
G
0
0
.
Let
E
u;v
denote
the
set
of
edges
in
G
0
0
connecting
u
to
v
,
and
let
E
v
;w
denote
the
set
of
edges
in
G
0
0
connecting
v
to
w
.
The
probabilit
y
that
a
random
w
alk
emanating
from
u
visits
v
and
reac
hes
w
can
b
e
written
as
P
u v
 w
=
Pr
R

;R

f0;g
`
[
R


E
u;v
and
R


E
v
;w
]
:
Since
the
graph
G
0
0
w
as
constructed
suc
h
that
Pr
R
i

R
f0;g
`
[u
!
v
]

=W

and
Pr
R
i

R
f0;g
`
[v
!
w
]

=W

,
it
follo
ws
that
P
u v
 w

=W

.

..
ANAL
YSIS

u
v
w
Figure
.:
Three
Consecutive
La
y
ers
of
G
0
0
No
w,
the
crux
of
the
construction
is
that
the
ab
o
v
e
random
w
alk
can
b
e
replaced
b
y
a
\semi-
random"
w
alk,
namely
,
a
w
alk
whose
rst
`
steps
are
determined
b
y
a
random
guide
R

,
and
whose
last
`
steps
are
determined
b
y
h
`
0
(R

),
for
some
function
h
`
0
in
H
`
.
Giv
en
h
`
0
,
the
probabilit
y
that
a
semi-random
w
alk
emanating
from
u
reac
hes
w
via
v
is
P
h
`
0
u v
 w
,
where
P
h
u v
 w

=
Pr
R

f0;g
`
[R


E
u;v
and
h(R

)

E
v
;w
]
Ho
w
ev
er,
Prop
osition
..
applied
with
resp
ect
to
the
sets
E
u;v
and
E
v
;w
asserts
that
except
for
a
fraction
of

 `=
of
the
hash
functions
h
`
0
,



P
h
`
0
u v
 w
 P
u v
 w





 `=
:
(.)
Th
us,
except
for
a
fraction
of
at
most


=
W



 `=
of
the
hash
functions,
Equation
(.)
holds
for
every
triplet
of
v
ertices
u,
v
and
w
in
ev
ery
triplet
of
consecutiv
e
la
y
ers,
i.e.,

u;
v
;
w



P
h
`
0
u v
 w
 P
u v
 w





 `=
:
(.)
Next,
x
a
hash
function
h
`
0
whic
h
satises
Equation
(.).
The
o
v
erall
probabilit
y
that
a
truly
random
w
alk
starting
from
u
reac
hes
w
can
b
e
written
as
P
u w
=
X
v
P
u v
 w
;
whereas
the
probabilit
y
that
a
semi-random
w
alk
starting
from
u
reac
hes
w
is
P
h
`
0
u w
=
X
v
P
h
`
0
u v
 w
:
Consequen
tly
,
with
a
suitable
selection
of
constan
ts,
and
since
P
u w

=W

,
jP
h
`
0
u w
 P
u w
j

W


 `=

W



 `=

P
u w


 `=0

P
u w
=



P
u w
;


LECTURE
.
DERANDOMIZING
SP
A
CE-BOUNDED
COMPUT
A
TIONS
for


=

 `=0
.
Ho
w
ev
er,
once
the
hash
function
h
`
0
is
xed,
ev
ery
t
w
o-hop
w
alk
u
 v
 w
dep
ends
only
on
the
corresp
onding
`-bit
R
i
,
and
hence
for
ev
ery
\accepting
path",
i.e.,
a
path
P

f0;
g
D
leading
from
the
initial
v
ertex
v
0
to
an
accepting
v
ertex,


Pr
[R
0
=
P
]
 Pr
[R
=
P
]






Pr
[R
=
P
]:
Since
the
probabilit
y
of
accepting
is
a
sum
o
v
er
all
accepting
paths,


Pr
R
0
[AC
C
(G
0
0
;
R
0
)]
 Pr
R
[AC
C
(G
0
0
;
R
)]






Pr
R
[AC
C
(G
0
0
;
R
)]



:
(.)
Finally
,
consider
the
t
w
o
ev
en
ts
AC
C
(G
0
0
;
R
)
and
AC
C
(G
0
0
;
R
0
),
where
the
hash
function
h
`
0
is
dra
wn
uniformly
at
random.
The
probabilit
y
that
R
0
hits
a
\bad"
hash
function
h
`
0
is
b
ounded
b
y


.
Otherwise,
Equation
(.)
holds,
and
th
us
the
lemma
holds
for


=


+


.
By
the
ab
o
v
e
t
w
o
lemmas,
it
follo
ws
that
the
semi-random
guide
R
0
b
eha
v
es
\w
ell'
in
the
original
graph
G
0
,
i.e.,
Corollary
.	
j
Pr
R
[AC
C
(G
0
;
R
)]
 Pr
R
0
[AC
C
(G
0
;
R
0
)]j
<
,
wher
e

=




+


.
No
w,
to
p
erform
another
coarsening
step,
construct
a
new
m
ultigraph
G

b
y
con
tracting
eac
h
pair
of
adjacen
t
edge
sets
as
follo
ws:

The
v
ertex
set
of
G

is
the
union
of
the
v
ertices
in
the
ev
en
la
y
ers
of
G.

Create
an
edge
for
ev
ery
t
w
o-hop
path
tak
en
b
y
the
semi-random
w
alk.
F
ormally
,
for
ev
ery
t
w
o
adjacen
t
edges
(u;
v
)
and
(v
;
w
)
lab
eled

and
h
`
0
(
)
resp
ectiv
ely
,
create
an
edge
(u;
w
)
in
G

,
and
lab
el
it

.
Reapply
Lemma
..
and
Lemma
..
on
G

,
this
time
using
a
new
hash
function
h
`
0
 
,
yielding
a
new
m
ultigraph
G

,
and
so
on.
It
th
us
follo
ws
that
at
eac
h
step,



Pr
[AC
C
(G
i+
;
R
(i+)
)]
 Pr[AC
C
(G
i
;
R
(i)
)]



<
:
After
`
0
iterations,
the
resulting
graph
G
`
0
is
a
bipartite
graph,
on
whic
h
a
truly
random
guide
r
is
applied.
Since
the
ab
o
v
e
analysis
corresp
onds
to
the
b
eha
vior
of
the
pseudorandom
generator
H
(),



Pr
I

R
f0;g
jI
j
[AC
C
(G;
H
(I
))]
 Pr
R
R
f0;g
D
[
AC
C
(G;
R
)]




`
0



=0;
where
I
=
(r
;
hh

i;
hh

i;
:
:
:
;
hh
`
0
i),
whic
h
concludes
the
pro
of.
Remark:
The
analysis
requires
that
the
constan
t
for
`
=
(log
n)
b
e
large
enough.
In
the
underlying
computational
mo
del,
this
ensure
that
the
mac
hine
M
cannot
retain
ev
en
a
description
of
a
single
function
h
i
.
Otherwise,
M
could
examine
the
rst
four
blo
c
ks
of
the
pseudorandom
sequence,
i.e.,
z
;
h
`
0
(z
);
z
0
;
h
`
0
(z
0
),
and
fully
determine
h
`
0
b
y
solving
t
w
o
linear
equations
with
t
w
o
v
ariables.
This
w
ould
mak
e
it
p
ossible
for
M
to
distinguish
b
et
w
een
a
truly
random
sequence
R
and
the
pseudorandom
sequence
H
(I
).

..
EXTENSIONS
AND
RELA
TED
RESUL
TS

.
Extensions
and
Related
Results
..
B
P
L

S
C
Whereas
Corollary
.
asserts
that
B
P
L

D
S
P
AC
E
(log

n),
the
running
time
of
the
straigh
tfor-
w
ard
derandomized
algorithm
is

(exp(log

(n))),
and
in
particular
is
not
p
olynomial
in
n.
In
this
section
w
e
consider
the
complexit
y
class
T
S
(t();
s()),
whic
h
denotes
the
set
of
all
languages
that
can
b
e
recognized
b
y
a
T
uring
mac
hine
whose
running
time
is
b
ounded
b
y
t()
and
whose
space
is
(sim
ultaneously)
b
ounded
b
y
s().
In
particular,
w
e
consider
S
C
(a.k.a.,
\Stev
e's
Class"),
Denition
.0
S
C

=
T
S
(p
oly
(n);
p
olylog
(n)).
W
e
state
the
follo
wing
theorem:
Theorem
.
B
P
L

S
C
.
Pro
of
Sk
etc
h:
Consider
a
language
L

B
P
L
and
a
corresp
onding
T
uring
mac
hine
M
for
whic
h
L
=
L(M
).
No
w,
instead
of
trying
all
O
(log

(n))
p
ossible
v
alues
of
the
input
I
=
(r
;
hh

i;
:
:
:
;
hh
`
0
i)
of
H
to
determine
whether
an
input
x
is
in
L,
p
erform
the
follo
wing
steps:

\Magically"
nd
a
sequence
of
\go
o
d"
functions
h

;
:
:
:
;
h
`
0
.

Em
ulate
M
only
o
v
er
all
p
ossibilities
of
r
.
Since
jr
j
=
`,
the
em
ulation
can
b
e
carried
out
in
time
exp
onen
tial
in
`,
and
hence
in
p
olynomial
time.
T
o
nd
a
sequence
of
\go
o
d"
functions,
incremen
tally
x
h
`
0
,
h
`
0
 
,
:
:
:
,
h

.
T
o
x
a
single
h
j
,
assume
all
functions
h
`
0
;
:
:
:
;
h
j
+
w
ere
already
xed
and
stored
in
memory
.
Consider
all
functions
h
in
H
`
,
and
test
eac
h
suc
h
h
to
determine
whether
Lemma
..
and
Equation
(.)
hold.
Since
the
existence
of
a
\go
o
d"
function
h
j
is
asserted
b
y
Theorem
.,
it
remains
to
v
erify
that
nding
suc
h
a
function
can
b
e
carried
out
in
time
exp
onen
tial
in
`
(and
hence
in
p
olynomial
time)
and
in
logarithmic
space.
T
o
see
that
this
is
the
case,
recall
that
the
pseudorandom
generator
H
()
can
b
e
written
recur-
siv
ely
as
H
(r
;
hh
i
i;
:
:
:
;
hh
`
0
i)
=
H
(r
;
hh
i+
i;
:
:
:
;
hh
`
0
i)

H
(h
i
(r
);
hh
i+
i;
:
:
:
;
hh
`
0
i);
where
H
(z
)
=
z
.
Consequen
tly
,
once
the
functions
h
`
0
;
:
:
:
;
h
j
+
are
xed,
ev
ery
single
probabilit
y
P
u v
 w
can
b
e
computed
directly
simply
b
y
exhaustiv
ely
considering
all
p
ossible
random
guides
R
(of
length
`).
Similarly
,
giv
en
a
candidate
hash
function
h
`
0
,
ev
ery
single
probabilit
y
P
h
`
0
u v
 w
can
also
b
e
computed
directly
b
y
exhaustiv
ely
considering
all
semi-random
guides
R
0
.
Hence,
to
determine
whether
Equation
(.)
holds,
simply
compare
P
u v
 w
and
P
h
`
0
u v
 w
for
ev
ery
triplet
of
v
ertices
u,
v
and
w
in
adjacen
t
la
y
ers.
T
esting
whether
Lemma
..
holds
can
b
e
carried
out
in
a
similar
manner.
Clearly
,
eac
h
suc
h
test
can
b
e
carried
out
in
time
exp
onen
tial
in
`,
and
since
the
n
um
b
er
of
candidate
functions
h
`
0
is
also
exp
onen
tial
in
`,
the
o
v
erall
running
time
is
exp
onen
tial
in
`.
F
urther,
testing
a
single
function
h
can
b
e
carried
out
in
space
linear
in
`,
and
hence
the
o
v
erall
space
complexit
y
is
dominated
b
y
the
space
needed
to
store
the
functions
h
`
0
;



;
h

,
i.e.,
b
y
O
(`

).
Remark:
The
edge
set
of
the
i-th
m
ultigraph
G
i
dep
ends
up
on
the
hash
functions
dra
wn
in
previous
steps.
Th
us,
although
\almost
all"
functions
h
in
H
`
w
ould
satisfy
Equation
(.),
one
cannot
consider
xing
a
function
h
j
b
efore
committing
on
the
functions
h
`
0
;
:
:
:
;
h
j
+
.


LECTURE
.
DERANDOMIZING
SP
A
CE-BOUNDED
COMPUT
A
TIONS
..
F
urther
Results
Belo
w
w
e
state,
without
a
pro
of,
t
w
o
related
results:
Theorem
.
B
P
L

D
S
P
AC
E
(log
:
n).
Theorem
.
(Informal)
Every
r
andom
c
omputation
that
c
an
b
e
c
arrie
d
out
in
p
olynomial
time
and
in
line
ar
sp
ac
e
c
an
also
c
arrie
d
out
in
p
olynomial
time
and
line
ar
sp
ac
e,
but
using
only
a
line
ar
amount
of
r
andomness.
Bibliographic
Notes
The
main
result
presen
ted
in
this
lecture
is
due
to
Noam
Nisan:
The
generator
itself
w
as
presen
ted
and
analyzed
in
[],
and
the
S
C
derandomization
w
as
later
giv
en
in
[].
Theorems
.
and
.
are
due
to
[]
and
[],
resp
ectiv
ely
.
.
N.
Nisan.
Pseudorandom
Generators
for
Space
Bounded
Computation.
Combinatoric
a,
V
ol.

(),
pages
	{,
		.
.
N.
Nisan.
RL

S
C
.
Journal
of
Computational
Complexity,
V
ol.
,
pages
-,
		.
.
N.
Nisan
and
D.
Zuc
k
erman.
Randomness
is
Linear
in
Space.
T
o
app
ear
in
JCSS.
Preliminary
v
ersion
in
th
STOC,
pages
{,
		.
.
M.
Saks
and
S.
Zhou.
R
S
P
AC
E
(S
)

D
S
P
AC
E
(S
=
).
In
th
F
OCS,
pages
{,
		.

Lecture

Zero-Kno
wledge
Pro
of
Systems
Notes
tak
en
b
y
Mic
hael
Elkin
and
Ek
aterina
Sedletsky
Summary:
In
this
lecture
w
e
in
tro
duce
the
notion
of
zero-kno
wledge
in
teractiv
e
pro
of
system,
and
consider
an
example
of
suc
h
a
system
(Graph
Isomorphism).
W
e
dene
p
erfect,
statistical
and
computational
zero-kno
wledge
and
presen
t
a
metho
d
for
con-
structing
zero-kno
wledge
pro
ofs
for
N
P
languages,
whic
h
mak
es
essen
tial
use
of
bit
commitmen
t
sc
hemes.
Presen
ting
a
zero-kno
wledge
pro
of
system
for
an
N
P
-complete
language,
w
e
obtain
zero-kno
wledge
pro
of
systems
for
ev
ery
language
in
N
P
.
W
e
con-
sider
a
zero-kno
wledge
pro
of
system
for
one
N
P
-complete
language,
sp
ecically
Graph
-Colorabilit
y
.
W
e
men
tion
that
zero-kno
wledge
is
preserv
ed
under
sequen
tial
comp
o-
sition,
but
is
not
preserv
ed
under
the
parallel
rep
etition.
Oded's
Note:
F
or
alteran
tiv
e
presen
tations
of
this
topic
w
e
refer
the
reader
to
either
Section
.
in
[]
(appro
x.

pages),
or
the
0-page
pap
er
[],
or
the
rst
{
sections
in
Chapter

of
[]
(o
v
er
0
pages).
.
Denitions
and
Discussions
Zero-kno
wledge
(Z
K
)
is
quite
cen
tral
to
cryptograph
y
,
but
it
is
also
in
teresting
in
con
text
of
this
course
of
the
complexit
y
theory
.
Lo
osely
sp
eaking,
zero-kno
wledge
pro
of
systems
ha
v
e
the
remark
able
prop
ert
y
of
b
eing
con
vincing
and
yielding
nothing
b
ey
ond
the
v
alidit
y
of
the
assertion.
W
e
sa
y
that
the
pro
of
is
zero-kno
wledge
if
the
v
erier
do
es
not
get
from
it
an
ything
that
he
can
not
compute
b
y
himself,
when
it
assumes
that
the
assertion
is
true.
T
raditional
pro
of
carries
with
it
something
whic
h
is
b
ey
ond
the
original
purp
ose.
The
purp
ose
of
the
pro
of
is
to
con
vince
someb
o
dy
,
but
t
ypically
the
details
of
pro
of
giv
e
the
v
erier
mor
e
than
merely
con
viction
in
the
v
alidit
y
of
the
assertion
and
it
is
not
clear
whether
it
is
essen
tial
or
not.
But
there
is
an
extreme
case,
in
whic
h
the
pro
v
er
giv
es
the
v
erier
nothing
b
ey
ond
b
eing
con
vinced
that
the
assertion
is
true.
If
the
v
erier
assumed
a-priori
that
the
assertion
is
true,
then
actually
the
pro
v
er
supplied
no
new
information.
The
basic
paradigm
of
zero-kno
wledge
in
teractiv
e
pro
of
system
is
that
whatev
er
can
b
e
ecien
tly
obtained
b
y
in
teracting
with
a
pro
v
er,
could
also
b
e
computed
without
in
teraction,
just
b
y
assuming
that
the
assertion
is
true
and
conducting
some
ecien
t
computation.
Recall
that
in
the
denition
of
in
teractiv
e
pro
of
system
w
e
ha
v
e
considered
prop
erties
of
the
v
erier,
whereas
no
requiremen
ts
on
the
pro
v
er
w
ere
imp
osed.
In
zero-kno
wledge
denition
w
e
talk



LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
ab
out
some
fe
atur
e
of
the
prescrib
ed
pro
v
er,
whic
h
captures
pro
v
er's
robustness
against
attempts
to
gain
kno
wledge
b
y
in
teracting
with
it.
V
erier's
prop
erties
are
required
to
ensure
that
w
e
ha
v
e
a
pro
of
system.
A
straigh
tforw
ard
w
a
y
of
capturing
the
informal
discussion
follo
ws.
Denition
.
L
et
A
and
B
b
e
a
p
air
of
inter
active
T
uring
machines,
and
supp
ose
that
al
l
p
ossible
inter
actions
of
A
and
B
on
e
ach
c
ommon
input
terminate
in
a
nite
numb
er
of
steps.
Then
hA;
B
i
(x)
is
the
r
andom
variable
r
epr
esenting
the
(lo
c
al)
output
of
B
when
inter
acting
with
machine
A
on
c
ommon
input
x,
when
the
r
andom-input
to
e
ach
machine
is
uniformly
and
indep
endently
chosen.
Denition
.
L
et
(P
;
V
)
b
e
an
inter
active
pr
o
of
system
for
some
language
L.
We
say
that
(P
;
V
),
actual
ly
P
,
is
zero-kno
wledge
if
for
every
pr
ob
abilistic
p
olynomial
time
inter
active
machine
V

ther
e
exists
an
(ordinary)
pr
ob
abilistic
p
olynomial
time
machine
M

so
that
for
every
x

L
holds
fh
P
;
V

i
(x)g
xL
=
fM

(x)g
xL
;
wher
e
the
e
quality
"="
b
etwe
en
the
ensembles
of
distributions
c
an
b
e
interpr
ete
d
in
one
of
thr
e
e
ways
that
we
wil
l
discuss
later.
Machine
M

is
c
al
le
d
a
sim
ulator
for
the
inter
action
of
V

with
P
.
W
e
stress
that
w
e
require
that
for
every
V

in
teracting
with
P
,
not
merely
for
V
,
there
exists
a
sim
ulator
M

.
This
sim
ulator,
although
not
ha
ving
access
to
the
in
teractiv
e
mac
hine
P
,
is
able
to
sim
ulate
the
in
teraction
of
V

with
P
.
This
fact
is
tak
en
as
evidence
to
the
claim
that
V

did
not
gain
an
y
kno
wledge
from
P
(since
the
same
output
could
ha
v
e
b
een
generated
without
an
y
access
to
P
).
V

is
an
in
teractiv
e
mac
hine,
p
oten
tially
something
more
sophisticated
than
V
,
whic
h
is
the
prescrib
ed
v
erier.
What
V

is
in
terested
in,
is
to
extract
from
the
pro
v
er
more
information
than
the
pro
v
er
is
willing
to
tell.
The
pro
v
er
w
an
ts
to
con
vince
the
v
erier
in
the
fact
that
x

L,
but
the
v
erier
is
in
terested
to
get
mor
e
information.
An
y
ecien
t
w
a
y
,
whic
h
the
v
erier
ma
y
try
to
do
it,
is
captured
b
y
suc
h
in
teracting
pro
cess
or
strategy
V

.
W
e
are
talking
here
ab
out
probabilit
y
ensem
bles,
but
unlik
e
our
denitions
of
the
pseudo-
randomness,
no
w
probabilit
y
ensem
bles
are
dened
using
index
whic
h
is
not
a
natural
n
um
b
er,
but
rather
less
trivial.
W
e
ha
v
e
to
mo
dify
the
formalism
sligh
tly
and
to
enable
indexing
b
y
an
y
coun
table
set.
It
is
imp
ortan
t
to
understand
that
for
ev
ery
x
w
e
ha
v
e
t
w
o
distributions
hP
;
V

i
(x)
and
M

(x).
>F
rom
no
w
on
the
distribution
ensem
bles
are
indexed
b
y
strings
x

L,
so
that
eac
h
distribution
con
tains
only
strings
of
length
p
olynomial
in
the
length
of
the
index.
The
question
is
when
these
t
w
o
probabilit
y
ensem
bles
are
equal
or
close.
There
are
three
natural
notions:
Denition
.
L
et
(P
;
V
)
b
e
an
inter
active
pr
o
of
system
for
some
language
L.
We
say
that
(P
;
V
),
actual
ly
P
,
is
p
erfect
zero-kno
wledge(PZK)
if
for
every
pr
ob
abilistic
p
olynomial
time
inter-
active
machine
V

ther
e
exists
an
(ordinary)
pr
ob
abilistic
p
olynomial
time
machine
M

so
that
for
every
x

L
the
distributions
fh
P
;
V

i
(x)g
xL
and
f
M

(x)g
xL
ar
e
identic
al,
i.e.
fh
P
;
V

i
(x)g
xL

fM

(x)g
xL
:

..
DEFINITIONS
AND
DISCUSSIONS

W
e
emphasize
that
a
distribution
ma
y
ha
v
e
sev
eral
dieren
t
w
a
ys
of
b
eing
generated.
F
or
example,
consider
the
uniform
distribution
o
v
er
n
bits.
The
normal
w
a
y
of
generating
suc
h
a
distribution
w
ould
b
e
to
toss
n
coins
and
to
write
do
wn
their
outcome.
Another
w
a
y
of
doing
it
w
ould
b
e
to
toss


n
coins,
to
ignore
the
coins
in
the
ev
en
p
ositions,
and
only
to
output
the
v
alues
of
the
coins
in
o
dd
p
ositions.
These
are
t
w
o
dieren
t
w
a
ys
of
pro
ducing
the
same
distribution.
Bac
k
to
our
denition,
the
t
w
o
distributions
fh
P
;
V

i
(x)g
xL
and
fM

(x)g
xL
ha
v
e
totally
dieren
t
w
a
ys
of
b
eing
pro
duced.
The
rst
one
is
pro
duced
b
y
in
teraction
of
the
t
w
o
mac
hines
and
the
second
is
pro
duced
b
y
a
traditional
probabilistic
p
olynomial
time
mac
hine.
Consider
a
"honest"
v
erier.
The
t
w
o
distributions
are
supp
osed
to
b
e
exactly
the
same
when
x
is
in
L.
As
w
e
ha
v
e
seen
in
the
example
of
uniform
distribution,
the
t
w
o
generated
in
dieren
t
w
a
ys
distributions
ma
y
b
e
iden
tical,
and
our
denition,
indeed,
requires
them
to
b
e
iden
tical.
The
v
erier
has
sev
eral
other
parameters,
except
of
random
coins
and
partial
message
history
,
and
w
e
do
not
x
them.
The
probabilit
y
that
the
v
erier
accepts
x,
when
x

L;
dep
ends
on
these
parameters.
Ho
w
ev
er,
b
y
completeness
requiremen
t
of
in
teractiv
e
pro
of
system,
this
probabilit
y
should
b
e
close
to

for
an
y
p
ossible
v
alues
of
the
parameters.
When
x
is
in
L,
M

accepts
with
v
ery
high
probabilit
y
,
but
when
x
is
not
in
L,
it
is
not
required
that
distribution
fM

(x)g
w
ould
b
e
similar
to
f
hP
;
V

i
(x)g
,
and
it
ma
y
not
happ
en.
So
mac
hine
M

,
whic
h
is
called
a
sim
ulator,
sim
ulates
the
in
teraction,
assuming
that
x
is
in
L.
When
x
is
not
in
L,
it
can
do
total
rubbish.
T
o
emphasize
this
p
oin
t
consider
the
follo
wing
example.
Example
.:
Consider
a
v
erier
V
that
satises
the
denition
of
in
teractiv
e
pro
of
system,
and
so
when
x

L,
V
will
accept
with
v
ery
high
(close
to
)
probabilit
y
.
When
x
=

L,
V
will
accept
with
v
ery
lo
w
(close
to
0)
probabilit
y
.
Suc
h
hP
;
V
i
can
b
e
sim
ulated
b
y
a
trivial
mac
hine
M
whic
h
alw
a
ys
accepts.
When
x

L
the
distributions
hP
;
V
i
(x)
and
M
(x)
are
v
ery
close
(the
rst
is
v
ery
close
to

and
the
second
is
iden
tically
),
whereas
when
x
=

L
the
t
w
o
distributions
are
totally
dieren
t
(the
rst
one
is
close
to

and
the
second
one
is
iden
tically
0).
This
b
eha
vior
should
not
surprise,
b
ecause
if
w
e
w
ould
b
e
able
to
sim
ulate
hP
;
V
i
b
y
some
non-in
teractiv
e
probabilistic
p
olynomial
time
M
it
w
ould
follo
w
that
I
P

B
P
P
,
whereas
the
denition
w
e
ha
v
e
in
tro
duced
is
m
uc
h
more
general.
Example
.:
Before
w
e
in
tro
duce
an
example
of
zero-kno
wledge
in
teractiv
e
pro
of
system,
let
us
rst
describ
e
an
in
teractiv
e
pro
of
system
that
"hides"
some
information
from
the
v
erier.
Although,
the
system
is
not
zero-kno
wledge,
the
v
erier
can
not
determine
an
y
particular
bit
of
the
"real"
pro
of
with
probabilit
y
bigger
than
/.
Consider
an
N
P
relation
R
0
that
satises
(x;
w
0
)

R
if
and
only
if
(x;
w
0
)

R
.
Suc
h
a
relation
can
b
e
created
from
an
y
N
P
relation
R
b
y
the
follo
wing
mo
dication
R
0

f(x;
0w
)
:
(x;
w
)

R
g
[
f(x;
w
)
:
(x;
w
)

R
g
:
No
w
w
e
are
suggesting
the
follo
wing
in
teractiv
e
pro
of
system
b
y
whic
h
the
pro
v
er
just
sends
to
the
v
erier
a
witness
w
0
.
P
r
ov
er
x
V
er
if
ier
                 !
w
0


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
Giv
en
the
input
x,
the
pro
v
er
selects
uniformly
at
random
either
w
0
or
w
0
,
and
sends
one
of
them
to
the
v
erier.
Both
w
0
and
w
0
are
witnesses
for
x
and
so
it
is
a
v
alid
in
teractiv
e
pro
of
system.
But
if
the
v
erier
is
in
terested
to
get
some
individual
bit
of
the
witness
w
0
,
he
has
no
w
a
y
to
do
it
using
the
data
that
he
obtained
from
the
pro
v
er.
Indeed,
eac
h
individual
bit
that
the
v
erier
receiv
es
from
the
pro
v
er
is
distributed
uniformly
and
the
v
erier
could
pro
duce
suc
h
distribution
b
y
itself
without
in
teraction
with
the
pro
v
er.
Ho
w
ev
er,
w
e
observ
e
that
the
v
erier
gets
some
info
ab
out
the
witness,
since
it
kno
ws
that
he
receiv
ed
either
the
witness
or
its
complemen
t.
Except
of
p
erfect
zero-kno
wledge
w
e
dene
sev
eral
more
lib
eral
notions
of
the
same
class.
One
of
them
requires
that
the
distributions
will
b
e
statistically
close.
By
statistically
close
w
e
mean
that
the
v
ariation
distance
b
et
w
een
them
is
negligible
as
the
function
of
the
length
of
input
x.
Denition
.
The
distribution
ensembles
fA
x
g
xL
and
fB
x
g
xL
ar
e
statistically
close
or
have
negligible
v
ariation
distance
if
for
every
p
olynomial
p()
ther
e
exists
inte
ger
N
such
that
for
every
x

L
with
jxj

N
holds
X

jP
r
ob
[A
x
=
]
 P
r
ob
[B
x
=
]
j


p(j
xj)
:
Denition
.
L
et
(P
;
V
)
b
e
an
inter
active
pr
o
of
system
for
some
language
L.
We
say
that
(P
;
V
),
actual
ly
P
,
is
statistical
zero-kno
wledge
pr
o
of
system
(S
Z
K
)
or
almost
p
erfect
inter
active
pr
o
of
system
if
for
every
pr
ob
abilistic
p
olynomial
time
verier
V

ther
e
exists
non-inter
active
pr
ob-
abilistic
p
olynomial
time
machine
M

such
that
the
ensembles
fh
P
;
V

i
(x)g
xL
and
fM

(x)g
xL
ar
e
statistic
al
ly
close.
Ev
en
more
lib
eral
notion
of
zero-kno
wledge
is
computational
zero-kno
wledge
in
teractiv
e
pro
of
system.
Denition
.
Two
ensembles
f
A
x
g
xL
and
f
B
x
g
xL
ar
e
c
omputational
ly
indistinguishable
if
for
every
pr
ob
abilistic
p
olynomial
time
distinguisher
D
and
for
every
p
olynomial
p()
ther
e
exists
inte
ger
N
such
that
for
every
x

L
with
jxj

N
holds
j
P
r
ob
[D
(x;
A
x
)
=
]
 P
r
ob
[D
(x;
B
x
)
=
]
j


p(jxj)
:
The
probabilistic
p
olynomial
time
distinguisher
D
is
giv
en
an
index
of
the
distribution
in
the
ensem
ble.
This
is
a
general
notion
of
indistinguishabilit
y
of
ensem
bles
indexed
b
y
strings.
Denition
.
L
et
(P
;
V
)
b
e
an
inter
active
pr
o
of
system
for
some
language
L.
We
say
that
(P
;
V
),
actual
ly
P
,
is
c
omputational
zero-kno
wledge
pr
o
of
system
(C
Z
K
)
if
for
every
pr
ob
abilistic
p
olynomial
time
verier
V

ther
e
exists
non-inter
active
pr
ob
abilistic
p
olynomial
time
machine
M

such
that
the
ensembles
f
hP
;
V

i
(x)g
xL
and
fM

(x)g
xL
ar
e
c
omputational
ly
indistinguishable.

..
DEFINITIONS
AND
DISCUSSIONS
	
W
e
should
b
e
careful
ab
out
the
order
of
quan
tifying
in
this
denition.
First
w
e
ha
v
e
to
determine
the
v
erier
V

and
the
sim
ulating
mac
hine
M

,
and
then
w
e
should
c
hec
k
whether
the
distributions
are
indistinguishable
b
y
"trying
all
p
ossible"
probabilistic
p
olynomial
time
distinguishers
D
.
T
ypically
,
when
w
e
sa
y
zero-kno
wledge,
then
w
e
mean
computational
zero-kno
wledge.
This
is
the
most
lib
eral
notion,
but
from
the
p
oin
t
of
view
of
cryptograph
y
applications
it
is
go
o
d
enough,
b
ecause,
basically
,
it
sa
ys
that
the
non-in
teractiv
e
mac
hine
M

can
sim
ulate
the
in
teractiv
e
system
hP
;
V

i
in
suc
h
a
w
a
y
,
that
"no
one"
can
distinguish
b
et
w
een
them.
Essen
tially
,
the
generated
distributions
are
close
in
the
same
sense
as
distribution
generated
b
y
pseudo-random
generator
is
close
to
the
uniform
distribution.
The
idea
is
that
if
the
mac
hine
M

is
able
to
generate
b
y
itself,
without
in
teraction,
the
distribution,
that
is
v
ery
close
in
computational
sense
to
the
distribution
generated
b
y
V

,
that
in
teracts
with
P
,
then
V

gains
nothing
from
in
teraction
with
P
.
Observ
e
that
zero-kno
wledge
in
teractiv
e
pro
of
system
denition
imp
oses
three
requiremen
ts.
Completeness
and
soundness
requiremen
ts
follo
ws
from
in
teractiv
e
pro
of
system
denition,
and
zero-kno
wledge
denition
imp
oses
the
additional
condition.
The
completeness
condition
xes
b
oth
the
pro
v
er
and
the
v
erier,
and
states
that
when
the
b
oth
parties
follo
w
the
prescrib
ed
proto
col
(b
oth
parties
are
"honest")
and
x

L,
then
the
v
erier
accepts
with
high
probabilit
y
.
Observ
e
that
if
either
pro
v
er
or
v
erier
are
"dishonest"
the
condition
ma
y
not
hold.
Indeed,
w
e
can
not
quan
tify
this
condition
o
v
er
all
v
eriers,
since
some
v
erier
ma
y
alw
a
ys
reject
and
then
of
course
the
probabilit
y
of
accepting
will
b
e
zero.
On
the
other
hand,
if
the
pro
v
er
is
"dishonest",
or
in
other
w
ords,
there
is
another
pro
v
er
instead
of
P
,
then
it
ma
y
send
some
rubbish
instead
of
witness
in
the
N
P
case
or
instead
of
follo
wing
the
prescrib
ed
proto
col
in
the
general
case,
and
the
v
erier
will
accept
only
with
lo
w
probabilit
y
,
hence
the
condition
will
not
hold.
The
soundness
condition
xes
only
the
v
erier
and
protects
his
in
terests.
It
sa
ys,
that
the
v
erier
do
esn't
need
to
trust
the
pro
v
er
to
b
e
honest.
Soundness
condition
quan
ties
o
v
er
all
p
ossible
pro
v
ers
and
sa
ys,
that
no
matter
ho
w
the
pro
v
er
b
eha
v
es
he
has
v
ery
small
probabilit
y
of
con
vincing
the
v
erier
to
accept
a
wrong
statemen
t.
Of
course,
it
is
correct
only
for
the
xed
v
erier
V
and
not
for
a
general
one,
since
w
e
ma
y
think
ab
out
a
v
erier
that
alw
a
ys
accepts.
F
or
suc
h
a
v
erier
the
probabilit
y
to
accept
a
wrong
statemen
t
is
,
hence
the
soundness
condition
do
es
not
hold
for
him.
And
the
zero-kno
wledge
condition
protects
the
pro
v
er.
It
tells
the
pro
v
er:
"Y
ou
are
en
tering
the
proto
col
whic
h
enables
y
ou
to
con
vince
the
other
part
y
,
ev
en
if
the
other
part
y
do
es
not
trust
y
ou
and
do
es
not
b
eliev
e
y
ou.
Y
ou
can
b
e
con
vinced,
that
y
ou
do
not
really
giv
e
the
other
part
y
more
than
y
ou
in
tended
to
(i.e.
that
the
statemen
t
is
true,
nothing
b
ey
ond
that)."
The
zero-kno
wledge
condition
xes
the
pro
v
er
and
quan
ties
o
v
er
all
v
eriers.
It
sa
ys
that
for
an
y
v
erier,
sophisticated
and
dishonest
as
m
uc
h
as
he
ma
y
b
e,
he
can
not
gain
from
the
pro
v
er
(whic
h
ob
eys
the
proto
col,
again
bad
pro
v
er
ma
y
send
all
the
information
to
the
v
erier
and
then,
of
course,
zero-kno
wledge
condition
will
not
hold)
more
that
the
v
erier
could
gain
without
in
teracting
with
the
pro
v
er.
W
e
nish
the
section
b
y
pro
ving
that
B
P
P
is
con
tained
in
P
Z
K
.
Indeed,
an
y
probabilistic
p
olynomial
time
algorithm
ma
y
b
e
view
ed
as
an
in
teractiv
e
pro
of
system
without
pro
v
er.
In
suc
h
a
system
no
v
erier
can
gain
kno
wledge
from
the
pro
v
er,
since
there
is
no
pro
v
er.
Th
us
the
system
is
zero-kno
wledge.
W
e
formalize
these
considerations
in
the
follo
wing
claim.
Prop
osition
..
B
P
P

P
Z
K
.
Pro
of:
Consider
an
in
teractiv
e
pro
of
system
in
whic
h
the
v
erier
V
is
just
the
probabilistic
p
olynomial
time
algorithm,
that
decides
the
language
L.
Suc
h
V
exists,
since
L

B
P
P
.
The

0
LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
pro
v
er
P
is
just
a
deterministic
mac
hine,
that
nev
er
sends
data
to
the
v
erier.
<
P
;
V
>
is
an
in
teractiv
e
pro
of
system,
since
V
will
accept
with
probabilit
y



,
when
x

L,
and
will
accept
with
probabilit
y



,
when
x
=

L,
since
L

B
P
P
,
and
V
decides
L.
Hence,
the
completeness
and
soundness
conditions
hold.
Clearly
,
it
is
p
erfect
zero-kno
wledge
since
for
ev
ery
V

,
the
distribution
that
<
P
;
V

>
generates
is
iden
tical
to
the
distribution
generated
b
y
V

itself.
.
Graph
Isomorphism
is
in
Zero-Kno
wledge
Let
I
S
O
f(h
G

i
;
h
G

i)
j
G


=
G

g.
W
e
assume
that
I
S
O
=

B
P
P
,
since
otherwise
I
S
O

Z
K
,
b
y
Claim
..
So
w
e
are
in
terested
in
sho
wing
zero-kno
wledge
in
teractiv
e
pro
of
systems
for
languages
that
are
not
in
B
P
P
,
or
at
least
are
conjectured
not
to
b
e
in
B
P
P
.
Next,
w
e
in
tro
duce
an
in
teractiv
e
proto
col
pro
ving
that
t
w
o
graphs
are
isomorphic.
The
trivial
in
teractiv
e
pro
of
w
ould
b
e
that
the
pro
v
er
will
send
to
the
v
erier
the
isomorphism,
but
this
giv
es
more
information
that
the
mere
fact
that
the
t
w
o
graphs
are
isomorphic.
Instead
of
sending
the
isomorphism,
whic
h
is
not
a
go
o
d
idea,
w
e
will
use
the
follo
wing
con-
struction.
Construction
.
(P
erfect
Zero-Kno
wledge
Pro
of
for
Graph
Isomorphism)

Common
Input:
A
pair
of
t
w
o
graphs,
G

=
(V

;
E

)
and
G

=
(V

;
E

).
Let

b
e
an
isomorphism
b
et
w
een
the
input
graphs,
namely

is
a
-
and
on
to
mapping
of
the
v
ertex
set
V

to
the
v
ertex
set
V

so
that
(u;
v
)

E

if
only
if
((u);
(v
))

E

.
Supp
ose
that
j
V

j
=
j
V

j
=
n
and
the
v
ertices
of
the
b
oth
graphs
are
the
n
um
b
ers
from

to
n.

Pro
v
er's
rst
Step
(P):
The
pro
v
er
selects
a
random
isomorphic
cop
y
of
G

,
and
sends
it
to
the
v
erier.
Namely
,
the
pro
v
er
selects
at
random,
with
uniform
probabilit
y
distribution,
a
p
erm
utation

from
the
set
of
p
erm
utations
o
v
er
the
v
ertex
set
V

,
and
constructs
a
graph
with
v
ertex
set
V

and
edge
set
F
def
=
f(
(u);

(v
))
:
(u;
v
)

E

g
The
pro
v
er
sends
H
=
(V

;
F
)
to
the
v
erier.

Motiv
ating
Remark:
If
the
input
graphs
are
isomorphic,
as
the
pro
v
er
claims,
then
the
graph
sen
t
in
step
P
is
isomorphic
to
b
oth
input
graphs.
Ho
w
ev
er,
if
the
input
graphs
are
not
isomorphic
then
no
graph
can
b
e
isomorphic
to
b
oth
of
them.

V
erier's
rst
Step
(V):
Up
on
receiving
a
graph,
G
0
=
(V
0
;
E
0
),
from
the
pro
v
er,
the
v
erier
asks
the
pro
v
er
to
sho
w
an
isomorphism
b
et
w
een
G
0
and
one
of
the
input
graph,
c
hosen
at
random
b
y
the
v
erier.
Namely
,
the
v
erier
uniformly
selects


f;
g,
and
sends
it
to
the
pro
v
er
(who
is
supp
osed
to
answ
er
with
an
isomorphism
b
et
w
een
G

and
G
0
).

Pro
v
er's
second
Step
(P):
If
the
message,

,
receiv
ed
from
the
v
erier
equals

then
the
pro
v
er
sends

to
the
v
erier.
Otherwise
(i.e.,

=
),
the
pro
v
er
sends



(i.e.,
the
comp
osition
of

on
)
to
the
v
erier.
(Remark:
the
pro
v
er
treats
an
y

=

as

=
.)

..
GRAPH
ISOMORPHISM
IS
IN
ZER
O-KNO
WLEDGE


V
erier's
second
Step
(V):
If
the
message,
denoted
 
,
receiv
ed
from
the
pro
v
er
is
an
isomor-
phism
b
et
w
een
G

and
G
0
then
the
v
erier
outputs
,
otherwise
it
outputs
0.
F
or
the
sc
hematic
represen
tation
of
the
proto
col,
see
the
Diagram
..
The
v
erier
program
presen
ted
ab
o
v
e
is
easily
implemen
ted
in
probabilistic
p
olynomial
time.
In
case
the
pro
v
er
is
giv
en
an
isomorphism
b
et
w
een
the
input
graphs
as
auxiliary
input,
also
the
pro
v
er's
program
can
b
e
implemen
ted
in
probabilistic
p
olynomial
time.
W
e
no
w
sho
w
that
the
ab
o
v
e
pair
of
in
teractiv
e
mac
hines
constitutes
an
almost
p
erfect
zero-kno
wledge
in
teractiv
e
pro
of
system
for
the
language
I
S
O
.
Diagram
.
P
r
ov
er
V
er
if
ier


R
S
y
m([n])
H
 
 
G

     !
H


R
f;
g
 
    
if

=

then
send
 
=

;
otherwise
 
=



 
     !
 
Accept
if
and
only
if
H
=
 
(G

)
Theorem
.
The
c
onstruction
ab
ove
is
an
almost
p
erfe
ct
zer
o-know
le
dge
inter
active
pr
o
of
sys-
tem.
Pro
of:
.
Completeness.
If
the
t
w
o
graphs
are
isomorphic,
w
e
claim
that
the
v
erier
will
alw
a
ys
accept.
Indeed,
if

=
,
then
the
v
erier
comes
to
the
end
of
the
proto
col
with
H
=

(G

)
and
p
erm
utation
 
=

.
The
v
erier
c
hec
ks
whether
H
=
 
(G

):
(.)
W
e
observ
e
that

(G

)
=
H
,
and
 
(G

)
=
 
(G

)
=

(G

),
implying
(.),
hence
the
v
erier
accepts.
If

=
,
then
the
v
erier
comes
to
the
end
of
the
proto
col
with
H
=

(G

)
and
p
erm
utation
 
=


.
Again
he
v
eries
whether
(l:eq)
holds.
W
e
observ
e
that

(G

)
=
H
and
 
(G

)
=

(G

)
=

(G

),
again
implying
(l:eq),
hence
the
v
erier
accepts
in
b
oth
cases
with
probabilit
y
.


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
The
in
tuition
is
v
ery
simple.
If
the
t
w
o
graphs
are
isomorphic
and
the
pro
v
er
created
the
isomorphic
cop
y
to
one
of
them,
he
should
ha
v
e
no
problem
in
sho
wing
isomorphism
to
eac
h
of
the
graphs.
.
Soundness.
Let
G



=
G

:
Consider
an
y
pro
v
er
P

:
If
it
sends
to
V
a
graph
H
,
whic
h
is
not
isomorphic
neither
to
G

nor
to
G

,
then
this
pro
v
er
will
ha
v
e
no
w
a
y
later
to
presen
t
an
isomorphism
from
G

(no
matter
whether

=

or
)
to
H
,
sinc
e
ther
e
is
no
such
isomorphism.
So,
in
this
case,
the
probabilit
y
of
P

to
con
vince
the
v
erier
that
(
hG

i
;
h
G

i)

I
S
O
,
is
zero.
Supp
ose
P

sends
an
H
that
is
isomorphic
either
to
G

or
to
G

.
Without
loss
of
generalit
y
,
assume
H

=
G

.
Then
if
the
v
erier
randomly
selected

=
,
then
P

will
b
e
able
to
sho
w
the
isomorphism
b
et
w
een
H
and
G

=
G

.
Otherwise,
if
the
v
erier
randomly
selected

=
,
then
there
is
no
isomorphism
b
et
w
een
H

=
G

and
G

=
G

(as
otherwise
G

and
G

w
ould
b
e
isomorphic),
hence
P

will
not
b
e
able
to
nd
one,
despite
his
unlimited
computational
p
o
w
er.
Hence,
in
this
case,
P

will
ha
v
e
probabilit
y
of
exactly


to
con
vince
the
v
erier
that
(hG

i
;
h
G

i
)

I
S
O
and
w
e
ha
v
e
sho
wn
that
it
is
his
optim
um
strategy
.
So
P
r
ob[<
P

;
V
>
(hG

i
;
hG

i
)
=
accept
j
G



=
G

]



for
ev
ery
pro
v
er
P

.
By
executing
this
proto
col
t
wice
sequen
tially
,
w
e
obtain
P
r
ob[<
P

;
V
>
(h
G

i
;
h
G

i)
=
accept
j
G



=
G

]



;
hence,
satisfying
the
soundness
condition.
.
Zer
o-know
le
dge.
There
is
no
other
w
a
y
to
pro
v
e
that
the
proto
col
is
zero-kno
wledge,
except
of
building
a
sim
ulator
and
pro
ving
that
it
really
generates
the
same
distribution.
Sim
ulator
M

.
By
denition
of
zero-kno
wledge
w
e
ha
v
e
to
sho
w
that
the
distributions
are
the
same
when
w
e
are
giv
en
an
input
from
the
language.
On
input
x
def
=
(hG

i
;
hG

i
),
sim
ulator
M

pro
ceeds
as
follo
ws:
.
Setting
the
random-tap
e
of
V

:
L
et
q
()
denote
a
p
olynomial
b
ounding
the
running
time
of
V

.
The
simulator
M

starts
by
uniformly
sele
cting
a
string
r

f0;
g
q
(j
xj)
,
to
b
e
use
d
as
the
c
ontents
of
the
r
andom-tap
e
of
V

.
.
Sim
ulating
the
pro
v
er's
rst
step
(P):
The
simulator
M

sele
cts
at
r
andom,with
uniform
pr
ob
ability
distribution,
a
"bit"

=
f;
g
and
a
p
ermutation
 
fr
om
the
set
of
p
ermutations
over
the
vertex
set
V

.
It
then
c
onstructs
an
isomorphic
c
opy
G
00
of
the
gr
aph
G

,
i.e.
G
00
=
 
(G

).
.
Sim
ulating
the
v
erier's
rst
step
(V):
The
simulator
M

initiates
the
exe
cution
of
V

by
placing
x
on
V

's
c
ommon-input-tap
e,
placing
r
(sele
cte
d
in
step
()
ab
ove)
on
V

's
r
andom-tap
e,
and
placing
G
00
(c
onstructe
d
in
step
()
ab
ove)
on
V

's
inc
oming
message-tap
e.
After
exe
cuting
a
p
olynomial
numb
er
of
steps
of
V

,
the
simulator
c
an
r
e
ad
the
outgoing
message
of
V

,
denote
d

.
L
et
us
assume,
without
loss
of
gener
ality,
that
the
message
sent
by
V

is
either

or
.
Inde
e
d,
if
V

sends

=

f;
g,
then
the
pr
over
has
nothing
to
do
with
it
and
we
may
augment
the
pr
over
P
with
a
rule
to
ignor
e

=

f;
g
and
just
to
wait
for
a
"valid"

.
This
would
b
e
very
e
asy
to
simulate.

..
GRAPH
ISOMORPHISM
IS
IN
ZER
O-KNO
WLEDGE

.
Sim
ulating
the
pro
v
er's
second
step
(P):
If

=

then
the
simulator
halts
with
output
(x;
r
;
G
00
;
 
).
.
F
ailure
of
the
sim
ulation:
Otherwise
(i.e.

=

),
the
simulator
halts
with
output
?.
As
could
b
e
seen
from
()
w
e
output
the
full
view
of
the
v
erier.
W
e
stress
that
the
sim
ulator
"has
no
w
a
y
to
kno
w"
whether
V

will
ask
to
see
an
isomorphism
to
G

or
to
G

.
This
description
of
the
sim
ulator
mac
hine
ma
y
confuse.
Indeed,
the
denition
of
zero-kno
wledge
considers
the
distributions
of
t
w
o
random
v
ariables
h
P
;
V

i
(x)
and
M

(x),
the
outputs
of
h
P
;
V

i
and
M

resp
ectiv
ely
.
On
the
other
hand,
here
M

returns
its
whole
view,
that
consists
of
all
the
data
it
p
ossesses,
sp
ecically
,
x,
r
,
G
00
and
 
.
This
inconsistency
can
b
e
treated
b
y
considering
hP
;
V

i
(x)
and
M

(x)
in
the
zero-kno
wledge
denition
as
the
views
of
V

and
M

resp
ectiv
ely
,
and
b
y
sho
wing
that
this
approac
h
is
equiv
alen
t
to
our
denition.
Denition
.	
L
et
(P
;
V
)
b
e
an
inter
active
pr
o
of
system
for
some
language
L.
We
say
that
(P
;
V
),
actual
ly
P
,
is
p
erfect
zero-kno
wledge(PZK)
b
y
view
if
for
every
pr
ob
abilistic
p
olynomial
time
inter
active
machine
V

ther
e
exists
an
(ordinary)
pr
ob
abilistic
p
olynomial
time
machine
M

so
that
for
every
x

L
holds
n
v
iew
(P
;V

)
(x)
o
xL

fM

(x)g
xL
;
wher
e
view
(P
;V

)
(x)
is
the
nal
view
of
V

after
running
hP
;
V

i
on
input
x
and
M

(x)
is,
as
usual,
the
output
of
M

after
running
on
input
x.
Claim
..
A
n
inter
active
pr
o
of
system
is
p
erfe
ct
zer
o-know
le
dge
if
and
only
if
it
is
p
erfe
ct
zer
o-know
le
dge
by
view.
Pro
of:
One
direction
is
straigh
tforw
ard.
Supp
ose
there
is
a
probabilistic
p
olynomial
time
mac
hine
M

,
whic
h
for
ev
ery
input
x

L
outputs
M

(x),
that
is
distributed
iden
tically
to
the
view
of
V

at
the
end
of
execution
of
h
P
;
V

i
on
x.
W
e
observ
e
that
the
last
step
of
V

,
i.e.
prin
ting
the
output,
is
done
without
in
teraction
with
the
pro
v
er.
Note
also
that
M

ma
y
,
instead
of
prin
ting
the
output,
write
it
do
wn
on
its
w
ork-tap
e.
Then
M

has
on
its
w
ork-tap
e
the
nal
view
of
V

.
Hence,
it
is
capable
to
p
erform
the
last
step
of
V

and
output
the
result
and
so
the
mo
died
M

(x)
is
iden
tical
to
hP
;
V

i
,
completing
the
pro
of
of
this
direction.
In
the
opp
osite
direction,
w
e
supp
ose
that
for
ev
ery
V

there
is
a
non-in
teractiv
e
probabilistic
p
olynomial
time
mac
hine
M

,
whic
h
prin
ts
the
same
output,
when
it
runs
on
x
(for
ev
ery
x

L),
as
V

when
hP
;
V

i
mac
hine
runs
on
x.
Consider
some
particular
V

.
W
e
need
to
sho
w
that
there
is
a
mac
hine
that
for
ev
ery
x

L
prin
ts
at
the
end
of
its
execution
on
x
the
output
iden
tical
to
the
view
V

at
the
end
of
execution
of
hP
;
V

i
on
x.
T
o
see
it,
consider
a
v
erier
V

,
that
b
eha
v
es
exactly
lik
e
V

,
but
outputs
its
whole
view
(i.e.,
it
em
ulates
V

except
that
at
the
end
it
outputs
the
view
of
V

).
There
is
a
mac
hine
M

,
suc
h
that
its
output
M

(x)
is
distributed
iden
tically
to
the
output
of
V

,
i.e.
to
the
view
of
V

.
Th
us
M

is
the
required
mac
hine.
It
completes
the
pro
of
of
the
second
direction,
establishing
the
equiv
alency
of
the
denitions.
Recall
that
the
Denitions
.
and
.	
b
oth
require
that
for
ev
ery
probabilistic
p
olynomial
time
in
teractiv
e
mac
hine
V

there
exists
an
(or
dinary)
probabilistic
p
olynomial
time
mac
hine
M



LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
with
certain
prop
erties.
Observ
e
that
in
the
pro
of
of
the
non-trivial
(i.e.,
second)
direction
of
the
ab
o
v
e
claim
w
e
use
the
fact
that
for
ev
ery
V

(constructed
out
of
V

)
there
is
a
corresp
onding
sim
ulator.
W
e
stress
that
this
w
as
not
used
in
the
rst
direction
in
whic
h
w
e
did
not
mo
dify
V

(but
rather
M

).
Statistical
zero-kno
wledge
b
y
view
and
computational
zero-kno
wledge
b
y
view
are
dened
anal-
ogously
.
Similar
claims
ab
out
equiv
alency
b
et
w
een
statistical
zero-kno
wledge
and
statistical
zero-
kno
wledge
b
y
view,
and
b
et
w
een
computational
zero-kno
wledge
and
computational
zero-kno
wledge
b
y
view
can
b
e
pro
v
ed
using
the
same
argumen
t
as
in
the
pro
of
of
Claim
...
W
e
claim
that,
when
t
w
o
graphs
are
isomorphic,
then
H
giv
es
no
information
on

,
b
ecause
w
e
can
dra
w
a
corresp
ondence
b
et
w
een
the
p
ossible
mappings
that
can
generate
H
from
G

and
the
p
ossible
mappings
that
can
generate
H
from
G

b
y
the
isomorphism
b
et
w
een
the
t
w
o
graphs.
It
follo
ws
that
Claim
..
L
et
x
=
(hG

i
;
hG

i
)

I
S
O
.
Then
for
every
string
r
,
gr
aph
H
,
and
p
ermutation
 
,
it
holds
that
P
r
ob
h
v
iew
(P
;V

)
(x)
=
(x;
r
;
H
;
 
)
i
=
P
r
ob
[M

(x)
=
(x;
r
;
H
;
 
)
j
(M

(x)
=?)]
:
Pro
of:
Let
m

(x)
describ
e
M

(x)
conditioned
on
its
not
b
eing
?.
W
e
rst
observ
e
that
b
oth
m

(x)
and
view
(P
;V

)
(x)
are
distributed
o
v
er
quadruples
of
the
form
(x;
r
;
;
),
with
uniformly
distributed
r

f0;
g
q
(j
xj
)
,
for
some
p
olynomial
q
().
Let
v
(x;
r
)
b
e
a
random
v
ariable
describing
the
last
t
w
o
elemen
ts
of
view
(P
;V

)
(x)
conditioned
on
the
second
elemen
t
equals
r
.
Similarly
,
let
(x;
r
)
describ
e
the
last
t
w
o
elemen
ts
of
m

(x)
(conditioned
on
the
second
elemen
t
equals
r
).
Clearly
,
it
suces
to
sho
w
that
v
(x;
r
)
and
(x;
r
)
are
iden
tically
distributed,
for
ev
ery
x
and
r
.
Observ
e
that
once
r
is
xed
the
message

sen
t
b
y
V

on
common
input
x,
random-tap
e
r
,
and
incoming
message
H
,
is
uniquely
dened.
Let
us
denote
this
message
b
y
v

(x;
r
;
H
).
W
e
need
to
sho
w
that
b
oth
v
(x;
r
)
and
(x;
r
)
are
uniformly
distributed
o
v
er
the
set
C
x;r
def
=
f(H
;
 
)
:
H
=
 
(G
v

(x;r
;H
)
)g:
The
pro
of
is
sligh
tly
non-trivial
b
ecause
it
relates
(at
least
implicitly)
to
the
automorphism
group
of
the
graph
G

(i.e.,
the
set
of
p
erm
utations

for
whic
h

(G

)
is
iden
tical,
not
just
isomorphic,
to
G

).
F
or
simplicit
y
,
consider
the
sp
ecial
case
in
whic
h
the
automorphism
group
of
G

consists
of
merely
the
iden
tit
y
p
erm
utation
(i.e.,
G

=
(G

)
if
and
only
if

is
the
iden
tit
y
p
erm
utation).
In
this
case,
(H
;
 
)

C
x;r
if
and
only
if
H
is
isomorphic
to
b
oth
G

and
G

and
 
is
the
isomorphism
b
et
w
een
H
and
G
v

(x;r
;H
)
.
Hence,
C
x;r
con
tains
exactly
jV
j
!
pairs,
eac
h
con
taining
a
dieren
t
graph
H
as
the
rst
elemen
t,
pro
ving
the
claim
in
the
sp
ecial
case.
F
or
the
pro
of
of
the
general
case
w
e
refer
the
reader
to
[]
(or
to
[]).
Recall
that
to
pro
v
e
p
erfect
zero-kno
wledge
w
e
need
to
sho
w
that
view
(P
;V

)
(x)
and
M

(x)
are
iden
tically
distributed.
Here,
it
is
not
the
case.
Although,
view
(P
;V

)
(x)
and
M

(x)j(M

(x)
=?)
are
iden
tically
distributed,
when
M

(x)
=?
the
distributions
are
totally
dieren
t.
A
common
w
a
y
to
o
v
ercome
this
dicult
y
is
to
c
hange
the
denition
of
p
erfect
zero-kno
wledge,
at
least
a
bit.
Supp
ose
w
e
allo
w
the
sim
ulator
to
output
a
sp
ecial
sym
b
ol
whic
h
w
e
call
"failure",
but
w
e
require
that
this
sp
ecial
sym
b
ol
is
outputted
with
probabilit
y
at
most
/.
In
this
case
the
construction

..
ZER
O-KNO
WLEDGE
PR
OOFS
F
OR
NP

w
ould
satisfy
the
denition
and
w
e
could
conclude
that
it
is
a
p
erfect
zero-kno
wledge
pro
of
(under
the
c
hanged
denition).
But
recall
that
Theorem
.
states
that
the
construction
is
almost
p
erfect
zero-kno
wledge.
This
is
indeed
true,
without
an
y
c
hange
of
the
denitions,
if
the
sim
ulator
reruns
steps
()-()
of
the
construction
j
xj
times.
If,
at
least
once,
at
step
()

is
equal
to

,
then
output
(x;
r
;
G
00
;
 
).
If
at
all
jxj
trials

=

,
then
output
rubbish.
In
suc
h
a
case
the
sim
ulation
will
not
b
e
p
erfect,
but
will
b
e
statistically
close,
b
ecause
the
statistical
dierence
will
b
e

 jxj
.
It
remains
to
sho
w
that
the
running
time
of
the
sim
ulator
is
p
olynomial
in
jxj.
In
the
case
when
w
e
run
it
jxj
times,
it
is
ob
vious,
concluding
our
pro
of
that
the
in
teractiv
e
pro
of
system
is
almost
p
erfect
zero-kno
wledge.
In
the
case
when
w
e
c
hange
the
denition
of
p
erfect
zero-kno
wledge
in
the
describ
ed
ab
o
v
e
w
a
y
,
w
e
are
done
b
y
one
iteration,
hence
the
running
time
is
against
p
olynomial
Another
p
ossibilit
y
is
to
allo
w
the
sim
ulator
to
run
exp
ected
p
olynomial
time,
rather
than
strict
p
olynomial
time,
in
suc
h
a
case
the
in
terpretation
w
ould
b
e
to
rerun
steps
()-(),
un
til
the
output
is
not
?.
Ev
ery
time
w
e
try
,
w
e
ha
v
e
a
success
probabilit
y
of
exactly
/.
Hence,
the
exp
ected
n
um
b
er
of
trials
is
.
This
concludes
the
pro
of
of
the
Theorem
..
These
denitions,
one
allo
wing
the
failure
probabilit
y
and
another
allo
wing
an
exp
ected
p
olyno-
mial
time,
are
not
kno
wn
to
b
e
equiv
alen
t.
Certainly
,
if
w
e
ha
v
e
a
sim
ulator
whic
h
with
probabilit
y
at
most
/
outputs
the
failure,
then
it
can
b
e
alw
a
ys
con
v
erted
to
one
whic
h
runs
in
exp
ected
p
olynomial
time.
But
the
opp
osite
direction
is
not
kno
wn
and
is
not
clear.
.
Zero-Kno
wledge
Pro
ofs
for
NP
..
Zero-Kno
wledge
NP-pro
of
systems
W
e
w
an
t
to
sho
w
wh
y
it
w
as
essen
tial
to
in
tro
duce
the
in
teractiv
e
pro
ofs
in
order
to
discuss
zero-
kno
wledge
(in
a
non-trivial
w
a
y).
One
can
also
dene
zero-kno
wledge
for
N
P
-pro
ofs,
but
b
y
the
follo
wing
claim
suc
h
pro
ofs
exist
only
for
B
P
P
(and
are
th
us
"useless").
Prop
osition
..
L
et
L
b
e
a
language
that
admits
zer
o-know
le
dge
N
P
-pr
o
of
system.
Then
L

B
P
P
.
Pro
of:
F
or
the
purp
ose
of
the
pro
of
w
e
use
only
the
fact
that
an
"honest"
v
erier
V
,
whic
h
outputs
its
whole
view,
can
b
e
sim
ulated
b
y
a
probabilistic
p
olynomial
time
mac
hine
M
.
(One
ma
y
think
that
in
view
of
Claim
..
there
is
no
p
oin
t
to
sp
ecify
that
V
outputs
its
whole
view.
It
is
not
correct.
In
the
pro
of
of
the
claim
w
e
used
the
fact
that
if
an
in
teractiv
e
pro
of
system
is
zero-kno
wledge,
then
for
any
v
erier
V

there
is
a
sim
ulator
M

with
certain
prop
erties.
Once
w
e
x
a
v
erier,
suc
h
kind
of
argumen
t
no
longer
w
orks.
It
is
the
reason
that
w
e
sp
ecify
the
output
of
V
.)
Let
L
b
e
the
language
that
the
N
P
pro
of
system
decides
and
R
L
its
N
P
relation.
Let
x

L.
The
view
of
the
v
erier
when
in
teracting
with
a
pro
v
er
on
input
x
will
b
e
the
input
itself
and
the
message,
call
it
w
,
that
he
receiv
ed.
Since
w
e
consider
an
honest
pro
v
er,
the
follo
wing
holds
v
iew
(V
;P
)
(x)
=
(x;
w
)

R
L
:


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
Sim
ulator
M
sim
ulates
this
view
on
input
x.
W
e
sho
w
that
(x;
M
(x))

R
L
with
high
proba-
bilit
y
,
sp
ecically
P
r
ob[(x;
M
(x))

R
L
]



:
(.)
Also,
w
e
will
sho
w
that
for
x
=

L
P
r
ob[(x;
M
(x))

R
L
]
=
0
<


;
(.)
hence,
M
is
the
probabilistic
p
olynomial
time
algorithm
for
L,
implying
L

B
P
P
.
First,
discuss
the
case
of
x

L
and
supp
ose
for
con
tradiction
that
P
r
ob[(x;
M
(x))

R
L
]
<


:
(.)
Then
w
e
claim
that
there
is
a
deterministic
p
olynomial
time
distinguisher
D
,
that
distinguishes
b
et
w
een
the
t
w
o
distributions
h
P
;
V
i
(x)
and
M
(x)
with
non-negligible
probabilit
y
.
Consider
D
()
whic
h
is
dened
as

if


R
L
,
and
0
otherwise,
where

is
a
pair
of
the
form
(x;
w
).
Ob
viously
,
for
x

L,
P
r
ob[D
(x;
hP
;
V
i
(x))
=
]
=

since
P
is
a
"honest"
pro
v
er,
i.e.
a
pro
v
er
that
supplies
witness
for
x

L.
Also,
from
()
follo
ws
that
P
r
ob[D
(x;
M
(x))
=
]
<


:
So
there
is
a
non-negligible
gap
b
et
w
een
the
t
w
o
cases.
It
con
tradicts
the
assumption,
that
the
distribution
of
M
(x)
is
p
olynomially
indistinguishable
from
the
distribution
of
h
P
;
V
i
(x).
On
the
other
hand,
when
x
=

L,
then
b
y
the
denition
of
N
P
,
there
is
no
witness
y
,
suc
h
that
(x;
y
)

R
L
.
P
articularly
,
(x;
M
(x))
=

R
L
.
In
other
w
ords,
P
r
ob[(x;
M
(x))

R
L
]
=
0:
Concluding
the
pro
of
of
()
and
(),
hence
L

B
P
P
.
..
NP

ZK
(o
v
erview)
No
w,
w
e
are
going
to
sho
w
that
N
P
has
a
zero-kno
wledge
in
teractiv
e
pro
of
(N
P

Z
K
).
W
e
will
do
it
assuming,
that
w
e
ha
v
e
some
magic
b
o
xes,
called
commitmen
t
sc
hemes,
and
later
w
e
will
describ
e
their
implemen
tation.
Commitmen
t
sc
hemes
are
used
to
enable
a
part
y
to
commit
itself
to
a
v
alue
while
k
eeping
it
secret.
In
a
latter
stage
the
commitmen
t
is
"op
ened"
and
it
is
guaran
teed
that
the
"op
ening"
can
yield
only
a
single
v
alue
determined
in
the
committing
phase.
Commitmen
t
sc
hemes
are
the
digital
analogue
of
non-transparen
t
sealed
en
v
elop
es.
Nob
o
dy
can
lo
ok
inside
the
en
v
elop
es
and
kno
w
the
v
alue.
By
putting
a
note
in
suc
h
an
en
v
elop
e
a
part
y
commits
itself
to
the
con
ten
ts
of
the
note
while
k
eeping
it
secret.
W
e
presen
t
a
zero-kno
wledge
pro
of
system
for
one
N
P
-complete
language,
sp
ecically
Gr
aph
-Coloring.

..
ZER
O-KNO
WLEDGE
PR
OOFS
F
OR
NP

The
language
Gr
aph
-Coloring,
denoted
GC
,
consists
of
all
simple
graphs
(i.e.,
no
parallel
edges
or
self-lo
ops)
that
can
b
e
vertex-c
olor
e
d
using

colors
so
that
no
t
w
o
adjacen
t
v
ertices
are
giv
en
the
same
color.
F
ormally
,
a
graph
G
=
(V
;
E
),
is

 col
or
abl
e,
if
there
exists
a
mapping

:
V
 !
f;
;
g,
so
that
(u)
=
(v
)
for
ev
ery
(u;
v
)

E
.
In
general,
if
w
e
w
an
t
to
build
a
zero-kno
wledge
in
teractiv
e
pro
of
system
for
some
other
N
P
language,
w
e
ma
y
just
use
standard
reduction
and
run
our
proto
col
on
the
reduced
instance
of
the
graph
colorabilit
y
.
Th
us,
if
w
e
can
sho
w
a
zero-kno
wledge
pro
of
for
one
N
P
 complete
language,
then
w
e
can
sho
w
for
all.
Basically
it
is
correct,
although
inaccurate.
F
or
more
details
see
[].
One
non-zero-kno
wledge
pro
of
is
to
send
the
coloring
to
the
v
erier,
whic
h
w
ould
c
hec
k
it,
and
this
w
ould
b
e
a
v
alid
in
teractiv
e
pro
of
system,
but,
of
course,
not
a
zero-kno
wledge
one.
The
instructions
that
w
e
giv
e
to
the
pro
v
er
can
actually
b
e
implemen
ted
in
p
olynomial
time,
if
w
e
giv
e
the
pro
v
er
some
auxiliary
input,
sp
ecically
the
-coloring
of
the
graph,
whic
h
is
the
N
P
-witness
that
the
graph
is
in
GC
.
Let
 
b
e
a
-coloring
of
G.
The
pro
v
er
selects
at
random
some
p
erm
utation.
But
this
time
it
is
not
a
p
erm
utation
of
the
v
ertices,
but
rather
p
erm
utation
of
the
colors


R
S
y
m([]):
Then
it
sets
(v
)

( 
(v
)),
for
eac
h
v

V
and
puts
eac
h
of
these
p
erm
uted
colors
in
a
separate
lo
c
k
ed
b
o
x
(of
the
t
yp
e
that
w
as
discussed
b
efore),
and
the
b
o
xes
ha
v
e
marks,
so
that
b
oth
the
pro
v
er
and
v
erier
kno
w
the
n
um
b
er
of
eac
h
b
o
x.
()

;
()

;
::::::;
(i)
i
;
:::::;
(n)
n
The
-color
of
v
ertex
i
is
sen
t
in
the
b
o
x
n
um
b
er
i.
The
v
erier
can
not
see
the
con
ten
ts
of
the
b
o
xes,
but
the
pro
v
er
claims,
that
he
has
sen
t
a
legal
coloring
of
the
graph,
whic
h
is
a
p
erm
utation
of
the
original
one.
The
v
erier
selects
an
edge
e
=
(u;
v
)
at
random,
and
sends
it
to
the
pro
v
er.
P
 
                            e
=
(u;
v
)
e

R
E
dg
e(G)
V
Basically
,
it
asks
to
insp
ect
the
colors
of
v
ertices
u
and
v
.
It
exp
ects
to
see
t
w
o
dieren
t
colors,
and
if
they
are
not,
then
the
v
erier
kno
ws
that
something
is
wrong.
The
pro
v
er
sends
bac
k
the
k
ey
to
b
o
x
n
um
b
er
u
and
the
k
ey
to
b
o
x
n
um
b
er
v
.
P
                             !
k
ey
u
and
k
ey
v
V
The
v
erier
uses
the
k
eys
to
op
en
these
t
w
o
b
o
xes
(other
b
o
xes
remain
lo
c
k
ed),
and
lo
oks
inside.
If
he
nds
t
w
o
dieren
t
colors
from
the
set
f;
;
g,
then
it
accepts.
Otherwise,
he
rejects.
This
completes
the
description
of
in
teractiv
e
pro
of
system
for
GC
,
that
w
e
call
GC
proto
col.
Ev
en
more
drastically
then
b
efore,
this
will
b
e
a
v
ery
w
eak
in
teractiv
e
pro
of.
In
order
to
mak
e
an
y
sense
w
e
ha
v
e
to
rep
eat
it
a
lot
of
times.
Ev
ery
time
w
e
rep
eat
it,
a
pro
v
er
selects
a
new
p
erm
utation

,
so
crucially
the
color
that
the
v
erier
sees
in
one
iteration
ha
v
e
nothing
to
do
with
the
colors,
that
he
sees
in
other
iterations.
Of
course,
if
the
pro
v
er
w
ould
alw
a
ys
use
the
same
colors,
then
the
v
erier
w
ould
kno
w
the
original
coloring
b
y
just
asking
sucien
t
n
um
b
er
of
times,
a
dieren
t
edge
eac
h
time.
So
it
is
crucial,
that
in
ev
ery
iteration
the
coloring
is
totally
random.
W
e
observ
e
that
the
randomness


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
of
the
coloring
ma
y
follo
w
also
from
a
random
c
hoice
of
the
original
coloring

and
not
only
from
the
randomness
of
the
p
erm
utation

,
whic
h
has
v
ery
small
sample
space
(only
!=).
A
computationally
un
b
ounded
pro
v
er
will
ha
v
e
no
problem
with
randomly
selecting
a
-coloring
for
a
graph.
On
the
other
hand,
a
p
olynomial
time
b
ounded
pro
v
er
has
to
b
e
feeded
with
all
the
-
colorings
as
auxiliary
inputs,
in
order
to
b
e
able
to
select
randomly
at
uniform
an
original
coloring
.
Observ
e
that
zero-kno
wledge
prop
ert
y
(when
augmen
ting
the
denition
a
bit)
is
preserv
ed
under
sequen
tial
comp
osition.
The
error
probabilities
also
decrease
if
w
e
apply
the
proto
col
in
parallel.
But
in
general,
it
is
not
kno
wn
that
zero-kno
wledge
is
preserv
ed
under
parallel
comp
osition.
In
particular,
the
proto
col
whic
h
is
deriv
ed
b
y
running
GC
proto
col
in
parallel
man
y
times
is
in
some
sense
not
zero-kno
wledge,
or
at
least
probably
is
not
zero-kno
wledge.
Prop
osition
..
GC
pr
oto
c
ol
is
zer
o-know
le
dge
inter
active
pr
o
of
system.
Pro
of:
.
Completeness.
If
the
graph
is
-Colorable,
and
b
oth
the
pro
v
er
and
the
v
erier
follo
w
the
proto
col,
then
the
v
erier
will
alw
a
ys
accept.
Indeed,
since

is
a
legal
coloring,
for
ev
ery
edge
e
=
(u;
v
)
holds
(u)
=
(v
).
Hence,
for
G

GC
P
r
ob[hP
;
V
i
(h
Gi)
=
accept]
=
;
and
w
e
are
done.
.
Soundness.
Let

b
e
a
color-assignmen
t
of
graph
G
that
the
pro
v
er
uses
in
his
trial
to
con
vince
the
v
erier
that
the
graph
is
-colorable.
If
G
=

GC
,
then
b
y
denition
of
-Colorabilit
y
there
exists
an
edge
e
0
=
(u;
v
)

E
dg
es(G);
suc
h
that
either
(u)
=
(v
)
or
(u)
=

f;
;
g.
Without
loss
of
generalit
y
,
supp
ose
(x)

f;
;
g
for
all
x's.
If
the
v
erier
ask
ed
to
op
en
e
0
then
b
y
commitmen
t
prop
ert
y
(sp
ecically
,
b
y
unam
biguit
y
requiremen
t
of
a
commitmen
t
sc
heme,
to
b
e
discuss
it
in
the
next
subsection)
he
will
rev
eal
that
(u)
=
(v
)
and
th
us
he
will
reject.
I.e.
Prob

a
randomly
selected
edge
e
=
(w
;
z
)
satises
(w
)
=
(z
)


j
E
dg
es(G)nfe
0
gj
j
E
dg
es(G)j
=

 
jE
dg
es(G)j
:
Hence,
for
an
y
pro
v
er
P

,
for
G
=

GC
P
r
ob[hP

;
V
i
(G)
=
accept]


 
jE
dg
es(G)j
:
By
rep
eating
the
proto
col
sequen
tially
,
sucien
tly
man
y
times,
sp
ecically
dlog
(
j
E
dg
es(G) j
jE
dg
es(G)j
)


e,
w
e
reduce
this
probabilit
y
to
P
r
ob[hP

;
V
i
(G)
=
acceptj
G
=

GC
]
<


for
an
y
pro
v
er
P

.

..
ZER
O-KNO
WLEDGE
PR
OOFS
F
OR
NP
	
.
Zer
o-Know
le
dge.
In
order
to
sho
w
that
GC
proto
col
is
a
zero-kno
wledge
pro
of
system,
w
e
devise
for
ev
ery
v
erier
V

the
follo
wing
sim
ulating
non-in
teractiv
e
mac
hine
M

,
that
for
ev
ery
G

GC
generates
a
distribution
m

(hGi)
=
M

(h
Gi)j
(
M

(G)
=?)
iden
tical
to
the
distribution
hP
;
V

i
(h
Gi).
Description
of
M

()
Fix
the
random
coins
r
of
V

.
()
Select
at
random
an
edge
e
0
=
(u
0
;
v
0
),
e
0
=
(u
0
;
v
0
)

R
E
dg
es(G):
()
V

sends
to
the
M

(in
fact,
it
is
in
ter-routine
comm
unication,
since
V

is
incorp
orated
in
M

)
b
o
xes
that
are
lled
in
the
follo
wing
w
a
y
.
All
the
b
o
xes
are
lled
with
garbage,
except
the
t
w
o
b
o
xes
of
u
0
and
v
0
,
whic
h
are
lled
with
t
w
o
dieren
t,
randomly
selected
n
um
b
ers
b
et
w
een

and
,
namely
c
=
d

R
f;
;
g:
Put
c
in
u
0
b
o
x,
d
in
v
0
b
o
x.
::::::::::::::::::::
c
::::::::::
d
::::::::::::::::::::
"
"
u
0
v
0
()
If
V

c
ho
oses
e
0
,
M

sends
V

the
k
eys
and
the
sim
ulation
is
completed.
If
the
v
erier
sends
a
dieren
t
edge,
w
e
rerun
ev
erything
from
step
().
T
otally
,
rerun
ev
erything
at
most
jE
dg
es(G)j
times.
If
at
least
once
V

selected
e
0
in
that
iteration,
prin
t
the
output
of
V

.
Otherwise,
output
?.
V

has
no
w
a
y
to
kno
w
what
b
o
xes
are
lled
with
garbage
and
what
b
o
xes
are
lled
with
some-
thing
meaningful,
since
the
b
o
xes
are
"non-transparen
t"
(see
b
elo
w
ab
out
secrecy
requiremen
t
of
a
commitmen
t
sc
heme).
Th
us
the
probabilit
y
,
that
in
eac
h
iteration
V

will
select
e
0
is

j
E
dg
es(G)j
.
By
doing
jE
dg
es(G)j
iterations
w
e
reac
h
a
constan
t
probabilit
y
(of
ab
out
e
 
)
of
generating
distri-
bution
exactly
iden
tical
to
h
P
;
V

i
(h
Gi).
Consider
the
distribution
of
m

(hGi)
=
M

(h
Gi)
j
(
M

(h
Gi)
=?)
:
F
or
ev
ery
G

GC
,
if
M

did
not
output
?,
then
one
of
the
iterations
w
as
successful.
In
this
case
V

has
selected
e
0
,
whic
h
is
a
randomly
selected
edge
from
the
graph
G.
M

prin
ts
the
output
of
V

on
e
0
.
Th
us,
the
distribution
of
m

(h
Gi)
is
iden
tical
to
the
distribution
of
the
output
of
V

when
it
is
giv
en
a
randomly
selected
edge
from
the
graph
G.
No
w
consider
the
distribution
of
hP
;
V

i
(hGi).
The
prescrib
ed
pro
v
er
P
is
xed
and
b
y
the
construction
it
supplies
to
V

a
randomly
selected
edge
from
the
graph
G.
Th
us
the
distribution
of
hP
;
V

i
(h
Gi)
is
also
iden
tical
to
the
distribution
of
the
output
of
V

when
it
is
giv
en
a
randomly
selected
edge
from
the
graph.
Hence,
m

(hGi)
=
hP
;
V

i
(hGi):
So
if
the
b
o
xes
are
p
erfectly
sealed,
then
GC

P
Z
K
.

0
LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
..
Digital
implemen
tation
In
realit
y
w
e
use
c
ommitment
scheme
instead
of
these
"sealed
en
v
elop
es",
that
w
e
discussed
pre-
viously
.
In
general,
a
c
ommitment
scheme
is
an
ecien
t
two-phase
t
w
o-part
y
proto
col
through
whic
h
one
part
y
,
called
the
sender
(S
),
can
commit
itself
to
a
value
so
the
follo
wing
t
w
o
conicting
requiremen
ts
are
satised.
.
Se
cr
e
cy:
A
t
the
end
of
the
rst
phase,
the
other
part
y
,
called
the
r
e
c
eiver
(R
),
do
es
not
gain
an
y
kno
wledge
of
the
sender's
v
alue.
This
requiremen
t
has
to
b
e
satised
for
an
y
p
olynomial-time
receiv
er.
.
Unambiguity:
Giv
en
the
transcript
of
the
in
teraction
in
the
rst
phase,
there
exists
at
most
one
v
alue
whic
h
the
receiv
er
ma
y
later
(i.e.,
in
the
second
phase)
accept
as
legal
"op
ening"
of
the
commitmen
t.
This
requiremen
t
has
to
b
e
satised
ev
en
if
the
sender
tries
to
c
heat
(no
matter
what
strategy
it
emplo
ys).
In
addition,
w
e
require
that
the
proto
col
is
viable
in
the
sense
that
if
b
oth
parties
follo
w
it
then,
at
the
end
of
the
second
phase,
the
receiv
er
gets
the
v
alue
committed
to
b
y
the
sender.
Denote
b
y
S
(s;

)
the
message
that
the
sender
sends
to
the
receiv
er
when
he
w
an
ts
to
commit
himself
to
a
bit

and
his
random
coins
are
s.
The
secrecy
requiremen
t
states
that,
for
random
s,
the
distributions
of
the
random
v
ariables
S
(s;
0)
and
S
(s;
)
are
indistinguishable
b
y
p
olynomial-size
circuits.
Let
view
of
S
(R
),
denoted
as
V
iew
(S
)
(V
iew
(R
)),
b
e
the
collection
of
all
the
information
kno
wn
to
the
sender
(receiv
er).
Denote
b
y
r
the
random
coins
of
R
and
let
m
b
e
the
sequence
of
messages
that
R
receiv
ed
from
S
.
Then
V
iew
(R
)
=
(r
;
m
).
In
case
of
single-round
commit,
m
=
S
(s;

).
When
the
sender
S
w
an
ts
to
commit
itself
to
a
bit

and
his
random
coins
sequence
is
s,
then
V
iew
(S
)
=
(s;

).
The
unam
biguit
y
requiremen
t
states
that
for
all
but
a
negligible
fraction
of
r
's,
there
is
no
suc
h
m
for
whic
h
there
exist
t
w
o
sequences
of
random
coin
tosses
of
S
,
s
and
s
0
suc
h
that
V
iew
(S
)
=
(s;
0)
and
V
iew
(R
)
=
(r
;
m
)
and
V
iew
(S
)
=
(s
0
;
)
and
V
iew
(R
)
=
(r
;
m
):
The
in
tuition
of
this
formalism
is
that
if
suc
h
s
and
s
0
w
ould
exist,
then
the
receiv
er's
view
w
ould
not
b
e
monoseman
tic.
Instead,
it
w
ould
enable
to
sophisticated
sender
to
claim
that
he
committed
either
to
zero
or
to
one,
and
the
receiv
er
w
ould
not
b
e
able
to
pro
v
e
that
the
sender
is
c
heating.
If
w
e
think
ab
out
the
analogy
of
sealed
en
v
elop
es,
w
e
send
them
and
w
e
b
eliev
e
that
their
con
ten
ts
is
already
determined.
Ho
w
ev
er,
if
w
e
do
not
op
en
them,
they
lo
ok
the
same
whatev
er
the
con
ten
ts
is.
In
the
rest
of
this
section
w
e
will
need
commitmen
t
sc
hemes
with
a
seemingly
stronger
se-
crecy
requiremen
t
that
dened
ab
o
v
e.
Sp
ecically
,
instead
of
requiring
secrecy
with
resp
ect
to
all
p
olynomial-time
mac
hines,
w
e
will
require
secrecy
with
resp
ect
to
all
(not
necessarily
uniform)
families
of
p
olynomial-size
circuits.
Assuming
the
existence
of
non-uniformly
one-w
a
y
functions
commitmen
t
sc
hemes
with
non
uniform
secrecy
can
b
e
constructed,
follo
wing
the
same
construc-
tions
used
in
the
uniform
case.

..
ZER
O-KNO
WLEDGE
PR
OOFS
F
OR
NP

Prop
osition
..
The
inter
active
pr
o
of
system
for
GC
that
uses
bit
c
ommitment
schemes
in-
ste
ad
of
the
"magic
b
oxes"
for
sending
c
olors
is
stil
l
zer
o-know
le
dge.
Pro
of:
.
Completeness.
If
the
graph
is
-Colorable,
the
pro
v
er
(the
sender)
will
ha
v
e
no
problem
to
con
vince
the
v
erier
(the
receiv
er)
b
y
sending
the
"righ
t
k
eys"
to
the
commitmen
t
sc
hemes,
that
con
tain
the
colors
of
the
endp
oin
ts
of
the
edge
that
the
v
erier
ask
ed
to
insp
ect.
More
formally
,
b
y
sending
the
"righ
t
k
eys"
w
e
mean
p
erforming
the
rev
eal
phase
of
the
commitmen
t
sc
heme.
It
tak
es
the
follo
wing
form:
.
The
pro
v
er
sends
to
the
v
erier
the
bit

(the
con
ten
ts
of
the
sealed
en
v
elop
e)
and
its
random
coins,
s,
that
he
has
used
when
he
committed
to
the
bit
(w
e
call
it
the
c
ommit
phase).
.
The
v
erier
c
hec
ks
that

and
s
and
his
o
wn
random
coins
r
indeed
yield
messages
that
the
v
erier
has
receiv
ed
in
the
commit
phase.
This
v
erication
is
done
b
y
running
the
segmen
t
of
the
pro
v
er's
program
in
whic
h
it
committed
to
the
bit

and
the
v
erier's
program.
Both
programs
are
no
w
run
b
y
the
v
erier
with
xed
coins
and
tak
e
p
olynomial
time.
Observ
e
that
the
v
erier
could
not
run
the
whole
pro
v
er's
program,
since
it
need
not
to
b
e
p
olynomial.
As
previously
,
the
probabilit
y
of
accepting
a
-Colorable
graph
is
.
.
Soundness.
The
unam
biguit
y
requiremen
t
of
the
commitmen
t
sc
heme
denition
ensures
that
the
soundness
is
satised
to
o.
W
e
will
ha
v
e
a
v
ery
small
increase
in
the
probabilit
y
to
accept
a
non--Colorable
graph,
relativ
ely
to
the
case
when
w
e
used
magic
b
o
xes.
As
in
the
pro
of
of
Claim
.
let
G
b
e
a
non--Colorable
graph
and

the
assignmen
t
of
colors
that
the
pro
v
er
uses.
If
the
in
teractiv
e
pro
of
system
is
based
on
magic
b
o
xes,
then
the
probabilit
y
to
accept
G
is
exactly
equal
to
the
probabilit
y
of
selecting
a
prop
erly
colored
b
y

edge
from
G
while
selecting
one
edge
uniformly
at
random.
As
w
e
ha
v
e
seen
in
the
pro
of
of
Claim
.,
this
probabilit
y
(further
denoted
p
0
)
is
b
ounded
b
y
jE
dg
es(G)j
 
jE
dg
es(G)j
.
In
tuitiv
ely
,
p
0
is
the
probabilit
y
that
the
v
erier
ask
ed
to
insp
ect
an
edge
that
is
prop
erly
colored
b
y
,
although

is
not
a
prop
er
coloring.
If
the
pro
of
system
is
based
on
commitmen
t
sc
hemes
rather
than
on
magic
b
o
xes,
then
except
of
p
0
there
is
a
probabilit
y
that
the
v
erier
ask
ed
to
insp
ect
a
non-prop
erly
colored
edge,
but
the
pro
v
er
has
succeeded
to
c
heat
him.
It
ma
y
happ
en
only
if
the
v
erier's
random
coins
r
b
elong
to
the
fraction
of
all
p
ossible
random
coins
of
the
v
erier,
for
whic
h
there
are
random
coins
of
the
pro
v
er,
whic
h
enable
him
to
pretend
that
he
committed
b
oth
to
0
and
to
.
Unam
biguit
y
requiremen
t
of
the
commitmen
t
sc
heme
ensures
that
this
fraction
is
negligible,
and
hence
the
probabilit
y
(further
denoted
p

)
that
r
b
elongs
to
this
fraction
is
negligible
to
o.
Th
us
w
e
can
b
ound
p

b
y

jE
dg
es(G)j
.
So,
the
total
probabilit
y
to
accept
a
non--Colorable
graph
is
b
ounded
b
y
p
0
+
p


jE
dg
es(G)j
 
j
E
dg
es(G)j
+



jE
dg
es(G)j
=
jE
dg
es(G)j
 =
jE
dg
es(G)j
:
By
rep
eating
the
proto
col
sucien
tly
man
y
times
w
e
can
mak
e
this
probabilit
y
smaller
than


,
th
us
satisfying
the
soundness
prop
ert
y
.
.
Zer
o-Know
le
dge.
The
zero-kno
wledge,
that
will
b
e
guaran
teed
no
w,
is
a
computational
zero-kno
wledge.
T
o
sho
w
this
w
e
pro
v
e
that
M

outputs
?
with
probabilit
y
at
most


,
and
that,
conditioned
on
not
outputting
?,
the
sim
ulator's
output
is
computationally
indistinguishable
from
the
v
erier's
view
in
a
"real
in
teraction
with
the
pro
v
er".


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
Claim
..
F
or
every
suciently
lar
ge
gr
aph,
G
=
(V
;
E
),
the
pr
ob
ability
that
M

(h
Gi)
=?
is
b
ounde
d
ab
ove
by


.
Pro
of:
As
ab
o
v
e,
n
will
denote
the
cardinalit
y
of
the
v
ertex
set
of
G.
Let
e

;
e

;
:::;
e
n
b
e
the
con
ten
ts
of
the
n
"sealed
en
v
elop
es"
that
the
pro
v
er/the
sim
ulator
sends
to
the
v
erier.
Let
s

;
s

;
:::;
s
n
b
e
the
random
coins
that
the
pro
v
er
used
in
the
commit
phase
while
commit-
ting
to
e

;
e

;
:::;
e
n
.
Let
us
denote
b
y
p
u;v
(G;
r
;
(e

;
e

;
:::;
e
n
))
the
probabilit
y
,
tak
en
o
v
er
all
the
c
hoices
of
the
s

;
s

;
:::;
s
n

f0;
g
n
,
that
V

,
on
input
G,
random
coins
r
,
and
pro
v
er
message
(C
s

(e

);
:::;
C
s
n
(e
n
)),
replies
with
the
message
(u;
v
).
W
e
assume,
for
simplicit
y
,
that
V

alw
a
ys
answ
ers
with
an
edge
of
G
(since
otherwise
its
message
is
an
yho
w
treated
as
if
it
w
ere
an
edge
of
G).
W
e
claim
that
for
ev
ery
sucien
tly
large
graph,
G
=
(V
;
E
),
ev
ery
r

f0;
g
q
(n)
,
ev
ery
edge
(u;
v
)

E
,
and
ev
ery
t
w
o
sequences
;


f;
;
g
n
,
it
holds
that
jp
u;v
(G;
r
;
)
 p
u;v
(G;
r
;

)j



jE
j
This
is
pro
v
en
using
the
non-uniform
secrecy
of
the
commitmen
t
sc
heme.
F
or
further
details
w
e
refer
the
reader
to
[]
(or
to
[]).
Claim
..
The
ensemble
c
onsisting
of
the
output
of
M

on
input
G
=
(V
;
E
)

GC
,
c
ondi-
tione
d
on
it
not
b
eing
?,
is
c
omputational
ly
indistinguishable
fr
om
the
ensemble
view
(P
;V

)
(h
Gi)
GGC
.
Namely,
for
every
pr
ob
abilistic
p
olynomial-time
algorithm,
A,
every
p
olynomial
p(),
and
every
suf-
ciently
lar
ge
gr
aph
G
=
(V
;
E
),



P
r
(A(m

(h
Gi))
=
)
 P
r
(A(v
iew
(P
;V

)
(h
Gi))
=
)



<

p(jV
j)
W
e
stress
that
these
ensem
bles
are
v
ery
dieren
t
(i.e.,
the
statistical
distance
b
et
w
een
them
is
v
ery
close
to
the
maxim
um
p
ossible),
and
y
et
they
are
computationally
indistinguishable.
Ac-
tually
,
w
e
can
pro
v
e
that
these
ensem
bles
are
indistinguishable
also
b
y
(non-uniform)
families
of
p
olynomial-size
circuits.
In
rst
glance
it
seems
that
Claim
..
follo
ws
easily
from
the
secrecy
prop
ert
y
of
the
commitmen
t
sc
heme.
Indeed,
Claim
..
is
pro
v
en
using
the
secrecy
prop
ert
y
of
the
commitmen
t
sc
heme,
y
et
the
pro
of
is
more
complex
than
one
an
ticipates
at
rst
glance.
The
dicult
y
lies
in
the
fact
that
the
ab
o
v
e
ensem
bles
consist
not
only
of
commitmen
ts
to
v
alues,
but
also
of
an
op
ening
of
some
of
the
v
alues.
F
urthermore,
the
c
hoice
of
whic
h
commitmen
ts
are
to
b
e
op
ened
dep
ends
on
the
en
tire
sequence
of
commitmen
ts.
Pro
of:
The
pro
of
can
b
e
found
in
[]
(or
to
[]).
This
completes
the
pro
of
of
computational
zero-kno
wledge
and
of
Prop
osition
...
Suc
h
a
bit
commitmen
t
sc
heme
is
not
dicult
to
implemen
t.
W
e
observ
e
that
the
size
of
the
in
tersection
b
et
w
een
the
space
of
commitmen
ts
to
0
and
the
space
of
commitmen
ts
to

is
a
negligible
fraction
of
the
size
of
the
union
of
the
t
w
o
spaces,
since
a
larger
in
tersection
w
ould
violate
the
unam
biguit
y
requiremen
t.
Hence,
the
space
of
commitmen
ts
to
0
and
the
space
of
commitmen
ts
to

almost
do
not
in
tersect.
On
the
other
hand,
commitmen
ts
to
0
should
b
e
indistinguishable
from
commitmen
ts
to
.
Using
mec
hanisms
of
one-w
a
y
functions
and
hard-core
bits,
w
e
can
satisfy
the
seemingly
conicting
requiremen
ts.
Consider
the
follo
wing
construction.

..
ZER
O-KNO
WLEDGE
PR
OOFS
F
OR
NP

Let
f
:
f0;
g
n
 !
f0;
g
n
b
e
one-w
a
y
p
erm
utation,
and
b
:
f0;
g
n
 !
f0;
g
b
e
its
hard-core
bit,
where
n
is
a
securit
y
parameter.
T
o
commit
itself
to
a
bit
v

f0;
g,
the
sender
S
selects
uniformly
at
random
s

R
f0;
g
n
and
sends
(f
(s);
b(s)

v
)
to
the
receiv
er
R
.
R
stores
this
pair
as

=
f
(s)
and

=
b(s)

v
.
In
the
second
phase
(i.e.
when
sending
the
"k
eys"),
S
rev
eals
its
random
coins
s.
R
calculates
v
=


b(s),
and
accepts
v
if

=
f
(s).
Otherwise,
R
rejects,
since
if

=
f
(s),
then
the
sender
tries
to
c
heat.
Prop
osition
..
The
pr
oto
c
ol
is
a
bit
c
ommitment
scheme.
Pro
of:
Se
cr
e
cy:
F
or
ev
ery
receiv
er
R

consider
the
distribution
ensem
bles
hS
(0);
R

i
(
n
)
and
h
S
();
R

i
(
n
).
Observ
e
that
hS
(0);
R

i
(
n
)
=
(f
(s);
b(s))
and
h
S
();
R

i
(
n
)
=
(f
(s);
b(s)
):
By
denition
of
hard-core
bit
b()
of
a
one-w
a
y
function
f
(),
for
ev
ery
probabilistic
p
olynomial
time
algorithm
A,
ev
ery
p
olynomial
p()
and
for
ev
ery
sucien
tly
large
random
string
s
P
r
[A(f
(s))
=
b(s)]
<


+

p(jsj)
:
In
other
w
ords,
the
bit
b(s)
is
unpr
e
dictable
b
y
probabilistic
p
olynomial
time
algorithm
giv
en
f
(s).
Th
us
the
distributions
(f
(s);
b(s))
and
(f
(s);
b(s))
are
probabilistic
p
olynomial
time
indistin-
guishable.
(F
or
the
pro
of
of
equiv
alency
b
et
w
een
indistinguishabilit
y
and
unpredictabilit
y
see
the
previous
lecture.)
Hence
for
an
y
probabilistic
p
olynomial
time
distinguisher
D



P
r
ob[D
(f
(s);
b(s))
=
]
 P
r
ob[D
(f
(s);
b(s)
=
])



<

p(jsj)
pro
ving
the
secrecy
.
Unambiguity:
W
e
claim
that
there
is
no
r
for
whic
h
(r
;
m
)
is
am
biguous,
where
m
is
the
sequence
of
messages
that
S
sen
t
to
R
.
Supp
ose,that
(r
;
m
)
is
a
p
ossible
0-commitmen
t,
i.e.
there
exist
a
string
s
suc
h
that,
m
describ
es
the
messages
receiv
ed
b
y
R
,
where
R
uses
lo
cal
coins
r
and
in
teracts
with
S
,
whic
h
uses
lo
cal
coins
s
and
has
input
v
=
0
and
securit
y
parameter
n.
Also,
supp
ose
for
con
tradiction
that
(r
;
m
)
is
a
p
ossible
-commitmen
t.
Then
there
exists
s

suc
h
that
V
iew
(S
)
=
(s

;
0;

n
),
V
iew
(R
)
=
(r
;
m
).
And
there
exists
s

suc
h
that
V
iew
(S
)
=
(s

;
;

n
),
V
iew
(R
)
=
(r
;
m
).
But
then
m=(f
(s

);
b(s

))=(f
(s

);
b(s

)).
I.e.
f
(s

)
=
f
(s

),
implying
s

=
s

since
f
()
is
a
p
erm
utation.
But
then
b(s

)
=
b(s

),
con
tradicting
b(s

)
=
b(s

).
Hence,
the
assumption
that
there
exists
am
biguous
(r
;
m
)
leads
to
the
con
tradiction.
Therefore,
the
unam
biguit
y
requiremen
t
is
satised,
implying
that
the
proto
col
is
a
one-bit
commitmen
t
sc
heme.


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS
.
V
arious
commen
ts
..
Remark
ab
out
parallel
rep
etition
Recall
that
the
denition
of
zero-kno
wledge
requires
that
for
ev
ery
V

there
exist
a
probabilistic
p
olynomial
time
sim
ulator
M

suc
h
that
for
ev
ery
x

L
the
distributions
of
h
P
;
V

i
(x),
M

(x)
are
iden
tical/statistically
close/computationally
indistinguishable.
A
particular
case
of
this
concept
is
a
blac
kb
o
x
zero-kno
wledge.
Denition
.0
(Blac
kb
o
x
Zero-Kno
wledge):
L
et
(P
;
V
)
b
e
an
inter
active
pr
o
of
system
for
some
language
L.
We
say
that
(P
;
V
)
is
p
erfect/statistical/computational
blac
kb
o
x
zero-kno
wledge
if
ther
e
exists
an
or
acle
machine
M
,
such
that
for
every
pr
ob
abilistic
p
olynomial
time
verier
V

and
for
every
x

L,
the
distributions
of
hP
;
V

i
(x),
M
V

(x)
ar
e
identic
al/statistic
al
ly
close/c
omputational
ly
indistinguishable.
Recall
that
M
V

is
an
oracle
T
uring
mac
hine
M
with
access
to
oracle
V

.
The
follo
wing
theorem
is
giv
en
without
pro
of.
Theorem
.
If
ther
e
is
an
inter
active
pr
o
of
system
(P
;
V
)
(with
negligible
error
probabilit
y)
for
language
L
that
satises

The
pr
escrib
e
d
verier
V
,
sends
the
outc
ome
of
e
ach
c
oin
it
tosses
(i.e.
the
inter
active
pr
o
of
system
is
of
public-c
oin
typ
e).

The
inter
active
pr
o
of
system
c
onsists
of
c
onstant
numb
er
of
r
ounds.

The
pr
oto
c
ol
is
blackb
ox
zer
o-know
le
dge.
Then
L

B
P
P
.
Observ
e
that
GC
proto
col
is
blac
kb
o
x
zero-kno
wledge,
but
its
failure
probabilit
y
is

 
jE
j
,
hence
it
is
not
negligible.
Blac
kb
o
x
zero-kno
wledge
is
preserv
ed
under
sequen
tial
comp
osition.
Th
us
b
y
rep
eating
GC
p
olynomially
man
y
times
w
e
obtain
a
blac
kb
o
x
zero-kno
wledge
proto
col
with
negligible
error
probabilit
y
,
but
the
n
um
b
er
of
rounds
of
this
proto
col
is
no
more
constan
t,
th
us
violating
the
second
condition
of
the
theorem.
Blac
kb
o
x
zero-kno
wledge
is
not
preserv
ed
under
parallel
comp
osition.
Indeed,
there
exist
zero-
kno
wledge
pro
ofs
that
when
rep
eated
t
wice
in
parallel
do
yield
kno
wledge.
Rep
eating
GC
proto
col
p
olynomially
man
y
times
in
parallel
clearly
satises
all
conditions
of
the
Theorem
.
except
blac
kb
o
x
zero-kno
wledge.
Th
us,
unless
N
P

B
P
P
,
this
parallel
proto
col
is
not
blac
k-b
o
x
zero-
kno
wledge.
More
generally
,
our
unabilit
y
to
construct
an
in
teractiv
e
pro
of
system
that
satises
all
the
conditions
of
Theorem
.
GC
do
es
not
surprise,
since
if
w
e
could
construct
suc
h
an
in
teractiv
e
pro
of
system,
then
b
y
Theorem
.,
GC
w
ould
b
elong
to
B
P
P
and
since
GC
is
N
P
-complete
it
w
ould
imply
N
P

B
P
P
.
Oded's
Note:
All
kno
wn
zero-kno
wledge
pro
of
systems
are
pro
v
en
to
b
e
so
via
a
blac
k-
b
o
x
argumen
t,
and
it
seems
hard
to
conceiv
e
an
alternativ
e.
Th
us
practially
sp
eaking,
Theorem
.
ma
y
b
e
understo
o
d
as
sa
ying
that
zero-kno
wledge
pro
ofs
(with
negligible
error)
for
languages
outside
BPP
should
either
use
non-constan
t
n
um
b
er
of
rounds
or
use
\priv
ate
coins"
(i.e.,
not
b
e
of
public-coin
t
yp
e).

..
V
ARIOUS
COMMENTS

..
Remark
ab
out
randomness
in
zero-kno
wledge
pro
ofs
In
in
teractiv
e
pro
ofs
it
w
as
imp
ortan
t
that
the
v
erier
to
ok
random
steps.
But
as
w
e
men
tioned,
the
pro
v
er
could
b
e
deterministic.
The
only
adv
an
tage
of
probabilistic
pro
v
er
in
an
in
teractiv
e
pro
of
system
is
that
it
ma
y
b
e
more
ecien
t.
On
the
other
hand,
the
pro
v
er
in
a
zero-kno
wledge
pro
of
system
has
to
b
e
randomized,
not
b
ecause
it
has
not
enough
p
o
w
er
to
do
things
deterministically
,
but
rather
b
ecause
a
deterministic
pro
v
er
will
not
b
e
able
to
satisfy
the
zero-kno
wledge
requiremen
t.
In
GC
example,
supp
ose
that
the
pro
v
er
selected
the
p
erm
utation

in
a
v
ery
complicated
and
secret
w
a
y
,
but
deterministically
.
Then
a
simple
v
erier
will
just
exhaust
all
the
edges,
when
rep
eating
the
proto
col
sev
eral
times.
Hence,
suc
h
a
proto
col
w
ould
not
b
e
a
zero-kno
wledge.
In
general,
w
e
ma
y
pro
v
e
that
if
a
language
has
a
zero-kno
wledge
pro
of
in
whic
h
either
pro
v
er
or
v
erier
is
deterministic,
then
the
language
is
in
B
P
P
.
Bibliographic
Notes
F
or
the
comprehensiv
e
discussion
of
zero-kno
wledge
and
commitmen
t
sc
heme
see
Chapter

in
[].
Theorem
.
app
ear
in
[]
.
Oded
Goldreic
h,
F
oundations
of
Crypto
gr
aphy
{
fr
agments
of
a
b
o
ok.
F
ebruary
		.
Revised
v
ersion,
Jan
uary
		.
Both
v
ersions
are
a
v
ailable
from
http://theory.lcs.mit.ed
u/o
ded
/fra
g.h
tml.
.
Oded
Goldreic
h,
Mo
dern
Crypto
gr
aphy,
Pr
ob
abilistic
pr
o
ofs
and
Pseudor
andomness,
Springer
V
erlag,
Berlin,
		.
.
O.
Goldreic
h,
S.
Micali,
A.
Wigderson,
Pr
o
ofs
that
Yield
Nothing
but
their
V
alidity
or
A
l
l
L
anguages
in
NP
Have
Zer
o-Know
le
dge
Pr
o
of
Systems.
JA
CM,
V
ol.
,
No.
,
pages
	{
	,
		.
Preliminary
v
ersion
in
th
F
OCS,
	.
.
O.
Goldreic
h
and
H.
Kra
w
czyk,
On
the
Comp
osition
of
Zer
o-Know
le
dge
Pr
o
of
Systems,
SIAM
Journal
on
Computing,
V
ol.,
No.
,
F
ebruary
		,
pages
	-	.


LECTURE
.
ZER
O-KNO
WLEDGE
PR
OOF
SYSTEMS

Lecture

NP
in
PCP[p
oly
,O()]
Notes
tak
en
b
y
T
al
Hassner
and
Y
oad
Lustig
Summary:
The
ma
jor
result
in
this
lecture
is
N
P

P
C
P
(pol
y
;
O
()).
In
the
course
of
the
pro
of
w
e
in
tro
duce
an
N
P
C
language
\Quadratic
Equations",
and
sho
w
it
to
b
e
in
P
C
P
(pol
y
;
O
()),
in
t
w
o
stages
:
rst
assuming
prop
erties
of
the
pro
of
(oracle)
and
then
testing
these
prop
erties.
An
in
termediate
result
that
migh
t
in
v
ok
e
indep
enden
t
in
terest
is
an
ecien
t
probabilistic
algorithm
that
distinguishes
b
et
w
een
linear
and
far-
from-linear
functions.
.
In
tro
duction
In
this
lecture
w
e
revisit
P
C
P
in
hop
e
of
giving
more
a
v
or
of
the
area
b
y
presen
ting
the
result
N
P

P
C
P
(pol
y
;
O
()).
Recall
that
last
semester
w
e
w
ere
sho
wn
without
pro
of
\the
P
C
P
theorem"
N
P
=
P
C
P
(l
og
(n);
O
())
(Corollary
.
in
Lecture
).
(Clearly
P
C
P
(l
og
(n);
O
())

N
P
as
giv
en
only
logarithmic
randomness
the
v
erier
can
only
use
its
randomness
to
c
ho
ose
from
a
p
olynomial
selection
of
queries
to
the
oracle,
therefor
the
answ
ers
to
all
the
p
ossible
queries
can
b
e
enco
ded
in
a
p
olynomial
size
witness
(for
a
more
detailed
pro
of
see
prop
osition
.
in
lecture
).
Tw
o
of
the
in
termediate
results
on
the
w
a
y
to
pro
ving
the
P
C
P
theorem
are
N
P

P
C
P
(pol
y
;
O
())
and
N
P

P
C
P
(l
og
;
pol
y
l
og
).
In
this
lecture
w
e
will
only
see
a
pro
of
of
N
P

P
C
P
(pol
y
;
O
()).
W
e
recall
the
denition
of
a
P
C
P
pro
of
system
for
a
language
L.
A
Probabilistically
Chec
k
able
Pro
of
system
for
a
language
L
is
a
probabilistic
p
olynomial-time
oracle
mac
hine
M
(called
v
erier)
satisfying:
.
Completeness
:
F
or
ev
ery
x

L
there
exists
an
oracle

x
suc
h
that
:
Pr
[M

x
(x)
Accepts]
=
.
.
Soundness
:
F
or
ev
ery
x
=

L
and
for
ev
ery
oracle

:
Pr[M

(x)
Accepts]



.
In
addition
to
limiting
the
v
erier
to
p
olynomial
time
w
e
are
in
terested
in
t
w
o
additional
complexit
y
measures
namely
,
the
r
andomness
M
uses,
and
the
numb
er
of
queries
M
ma
y
p
erform.
W
e
denote
b
y
P
C
P
(f
();
g
())
the
class
of
languages
for
whic
h
there
exists
a
P
C
P
pro
of
system
utilizing
at
most
f
(jxj)
coin
tosses
and
g
(jxj)
queries
for
ev
ery
input
x.
W
e
should
note
that
an
easy
result
is
that
P
C
P
(0;
pol
y
)

N
P
.
In
P
C
P
w
e
allo
w
the
v
erier
t
w
o
adv
an
tages
o
v
er
an
N
P
v
erier.
The
rst
is
probabilistic
soundness
as
opp
osed
to
p
erfect
one,
(whic
h
is
not
relev
an
t
in
this
case
as
the
v
erier
is
deterministic).
The
second
ma
jor
adv
an
tage
is
that
the
P
C
P
\pro
of
"
(oracle)
is
not
limited
to
p
olynomial
length
but
is
giv
en
as
an
oracle.
One
ma
y
lo
ok
at
the
oracle
as
a
pro
of
of
exp
onen
tial
length
for
whic
h
the
v
erier
mac
hine
has
random



LECTURE
.
NP
IN
PCP[POL
Y,O()]
access
(reading
a
bit
from
the
memory
address
addr
equiv
alen
t
to
p
erforming
a
query
ab
out
addr
).
Ho
w
ev
er
the
v
erier
is
limited
to
p
olynomial
time
and
therefor
actually
reads
only
p
olynomial
n
um
b
er
of
bits.
In
case
the
v
erier
is
completely
deterministic
the
bits
it
will
read
are
kno
wn
in
adv
ance
and
therefor
there
is
no
need
to
write
do
wn
the
rest
of
the
bits
i.e.
a
p
olynomial
witness
(as
in
N
P
)
will
suce.
.
Quadratic
Equations
Bac
k
to
our
goal
of
pro
ving
N
P

P
C
P
(pol
y
;
O
()),
it
is
easy
to
see
that
for
an
y
L

N
P
C
pro
ving
L

P
C
P
(pol
y
;
O
())
will
suce,
since
giv
en
an
y
language
L

in
N
P
w
e
can
decide
it
b
y
reducing
ev
ery
instance
x
to
it's
corresp
onding
instance
x
L
and
using
our
P
C
P
(pol
y
;
O
())
pro
of
system
to
decide
whether
x
L
is
in
L
(i
x

L

).
In
this
section
w
e
in
tro
duce
an
N
P
C
language
that
w
e
will
nd
con
v
enien
t
to
w
ork
with.
Denition
.
(Quadratic
Equations):
The
language
Quadratic
Equations
denote
d
QE
c
onsists
of
al
l
satisable
sets
of
quadr
atic
e
quations
over
GF
().
Since
in
GF()
x
=
x

for
all
x
w
e
assume
with
out
loss
of
generalit
y
all
the
summands
are
either
of
degree

or
constan
ts.
QE
form
ulated
as
a
problem
:
Problem:
QE
Input:
A
sequence
of
quadratic
equations
f
n
P
i;j
=
c
(k
)
i;j
x
i
x
j
=
c
(k
)
g
m
k
=
.
T
ask:
Is
there
an
assignmen
t
to
x

:
:
:
x
n
satisfying
all
equations
?
Clearly
QE
is
in
N
P
as
a
satisfying
assignmen
t
is
an
easily
v
eriable
witness.
T
o
see
that
it
is
N
P
 har
d
w
e
will
see
a
reduction
from

 S
AT
.
Giv
en
an
instance
of

 S
AT
,
m
V
i=
(l
i
_
l
i
_
l
i
)
where
eac
h
l
ij
is
a
literal
i.e.
either
an
atomic
prop
osition
p
or
it's
negation
:p.
W
e
will
asso
ciate
with
eac
h
clause
C
i
=
(l
i
_
l
i
_
l
i
)
(for
example
(p

_
:p

_
p
	
)
)
a
cubic
equation
in
the
follo
wing
w
a
y:
With
eac
h
atomic
prop
osition
p
w
e
asso
ciate
a
v
ariable
x
p
.
No
w,
lo
oking
at
C
i
if
l
ij
is
an
atomic
prop
osition
p
then
w
e
set
the
corresp
onding
factor
y
ij
to
b
e
(
 x
p
)
otherwise
l
ij
is
the
negation
of
an
atomic
prop
osition
:p
in
whic
h
case
w
e
set
the
corresp
onding
factor
y
ij
to
b
e
x
p
.
Clearly
,
the
factor
y
ij
equals
0
i
the
literal
l
ij
is
true,
and
the
expression
y
i
y
i
y
i
equals
zero
i
the
disjunction
l
i
_
l
i
_
l
i
is
true
(In
our
example
the
clause
(p

_
:p

_
p
	
)
gets
transformed
to
the
equation
\(
 x

)x

(
 x
	
)
=
0").
Therefor
the
set
of
equations
fy
i
y
i
y
i
=
0g
m
i=
is
satisable
i
our

 C
N
F
is
satisable.
W
e
ha
v
e
reduced

 S
AT
to
a
satisabilit
y
of
a
set
of
cubic
equations
but
still
ha
v
e
to
reduce
it
to
quadratic
ones
(formally
w
e
ha
v
e
to
op
en
paren
thesis
to
get
our
equations
to
the
normal
form
this
can
b
e
easily
done).
What
w
e
need
is
a
w
a
y
to
transform
a
set
of
cubic
equations
in
to
a
set
of
quadratic
ones
(suc
h
that
one
set
is
satisable
i
the
other
is).
The
latter
can
b
e
done
as
follo
ws:
F
or
eac
h
pair
of
v
ariables
x
i
;
x
j
in
the
cubic
system
in
tro
duce
a
new
v
ariable
z
ij
and
a
new
equation
z
ij
=
x
i
x
j
,
no
w
go
through
all
the
equations
and
whenev
er
w
e
nd
a
summand
of
degree

(x
i
x
j
x
k
)
replace
it
b
y
a
summand
of
degree

(z
ij
x
k
).
Our
new
equation
system
is
comp
osed
of
t
w
o
parts,
the
rst
part
in
tro
duces
the
new
v
ariables
(fz
ij
=
x
i
x
j
g
n
i;j
=
quadratic
in
the
size
of
the
original

..
THE
MAIN
STRA
TEGY
AND
A
T
A
CTICAL
MANEUVER
	
input),
and
the
second
part
is
the
transformed
set
of
equations
all
of
degree

(linear
in
the
size
of
the
input)
In
our
example:
(
 x

)x

(
 x
	
)
=
0
(
)
x

 x

x

 x

x
	
+
x

x

x
	
=
0
ma
y
b
e
replaced
b
y:
x

x

=
z


and
x

 x

x

 x

x
	
+
z


x
	
=
0.
The
last
tec
hnical
step
is
to
go
o
v
er
the
equation
set
and
replace
ev
ery
summand
of
degree

(i.e.
x
i
)
b
y
its
square
(i.e.
x

i
).
Since
in
GF()
for
all
a
it
holds
that
a
=
a

this
is
purely
a
tec
hnical
transformation
that
do
es
not
c
hange
in
an
y
w
a
y
the
satisabilit
y
of
the
equations.
Clearly
the
new
set
of
equations
is
satisable
i
the
original
set
is.
Since
the
en
tire
pro
cedure
of
reducing
a

 C
N
F
to
a
set
of
quadratic
equations
can
b
e
done
in
p
olynomial
time
w
e
ha
v
e
reduced

 S
AT
to
QE
.
Note
that
the
tric
k
used
to
reduce
the
degree
of
summands
in
the
p
olynoms
can
b
e
iterated
to
reduce
the
degree
of
higher
degree
summands,
ho
w
ev
er
the
new
equations
of
the
kind
z
ij
=
x
i
x
j
are
of
degree
,
and
therefor
suc
h
a
tric
k
cannot
b
e
used
to
reduce
the
degree
of
equations
to
degree
.
In
fact
degree

equations
are
linear
equations
for
whic
h
w
e
kno
w
an
ecien
t
pro
cedure
of
deciding
satisabilit
y
(Gaussian
Elimination).
Th
us
there
can
b
e
no
w
a
y
of
reducing
a
set
of
quadratic
equations
to
a
set
of
linear
ones
in
p
olynomial
time,
unless
P
=
N
P
.
.
The
main
strategy
and
a
tactical
maneuv
er
The
task
standing
b
efore
us
is
nding
a
P
C
P
(pol
y
;
O
())
pro
of
system
for
QE
.
In
tuitiv
ely
this
means
nding
a
w
a
y
in
whic
h
someone
can
presen
t
a
pro
of
for
satisabilit
y
of
an
equation
set
(the
oracle),
whic
h
migh
t
b
e
v
ery
long
but
one
ma
y
v
erify
its
correctness
(at
least
with
high
probabilit
y)
taking
only
a
constan
t
n
um
b
er
of
\glimpses"
at
the
pro
of
regardless
of
the
size
of
the
equation
system.
The
existence
of
suc
h
a
pro
of
system
seems
coun
ter-in
tuitiv
e
at
rst
since
in
the
pro
of
systems
w
e
are
familiar
with,
an
y
mistak
e
ho
w
ev
er
small
in
an
y
place
of
the
pro
of
cause
the
en
tire
pro
of
to
b
e
in
v
alid.
(Ho
w
ev
er
existence
of
suc
h
pro
of
systems
is
exactly
what
the
\PCP
Theorem"
asserts.)
In
this
section
w
e
try
to
dev
elop
an
in
tuition
of
ho
w
suc
h
a
pro
of
system
can
exist
b
y
outlining
the
main
ideas
of
the
pro
of
that
QE

P
C
P
(pol
y
;
O
()).
W
e
will
deal
with
the
\to
y
example"
of
pro
ving
that
one
linear
expression
can
get
the
v
alue
0
o
v
er
GF()
(of
course
since
suc
h
a
question
is
decidable
in
p
olynomial
time
there
is
a
trivial
pro
of
system
in
whic
h
the
oracle
giv
es
no
information
at
all
(alw
a
ys
answ
ers
),
but
w
e
will
not
mak
e
use
of
that
trivialit
y).
T
o
dev
elop
an
in
tuition
w
e
adopt
a
con
v
en
tion
that
the
pro
of
of
v
alidit
y
for
an
equation
system
(the
oracle
in
the
P
C
P
setting)
is
written
b
y
an
adv
ersary
who
tries
to
c
heat
us
in
to
accepting
non-satisable
equations.
The
goal
of
our
system
is
to
o
v
ercome
suc
h
tries
to
deceiv
e
us
(while
still
enabling
the
existence
of
pro
ofs
for
satisable
pro
of
systems).
Supp
ose
rst
that
w
e
can
restrict
our
adv
ersary
to
writing
only
pro
ofs
of
certain
kinds,
i.e.
ha
ving
sp
ecial
prop
erties.
F
or
example
w
e
ma
y
restrict
the
pro
ofs
to
enco
de
assignmen
ts
in
the
follo
wing
w
a
y
:
Fix
some
enco
ding
of
linear
expressions
in
the
v
ariables
x

;
:
:
:
;
x
n
in
to
natural
n
um
b
ers
(denote
b
y
C(ex)
the
natural
n
um
b
er
enco
ding
the
expression
'ex').
A
reasonable
enco
ding
for
the
linear
expression
n
P
i=

i
x
i
w
ould
b
e
simply
the
n
um
b
er
whose
binary
expansion
is
the
sequence
of
bits




:
:
:

n
).
Supp
ose
the
adv
ersary
is
restricted
to
writing
pro
ofs
in
whic
h
he
enco
des
assignmen
ts.
T
o
enco
de
an
assignmen
t
x

=
a
0
;
:
:
:
;
x
n
=
a
n
,
the
adv
ersary
ev
aluates
all
the
linear
expressions
o
v
er

0
LECTURE
.
NP
IN
PCP[POL
Y,O()]
x

;
:
:
:
;
;
x
n
with
that
sp
ecic
assignmen
t,
and
writes
the
v
alue
ex(a
0
;
:
:
:
;
a
n
)
at
the
place
C
(ex)
in
the
pro
of.
In
the
P
C
P
setting
this
means
the
oracle

answ
ers
ex(a
0
;
:
:
:
;
a
n
)
on
the
query
C
(ex).
F
or
example

(C
(x

+
x

))
w
ould
ha
v
e
the
v
alue
a

+
a

.
If
w
e
can
trust
the
adv
ersary
to
comply
to
suc
h
a
restriction,
all
w
e
ha
v
e
to
do
giv
en
an
expression
ex,
is
to
calculate
C
(ex)
and
query
the
oracle

.
The
problem
of
course
is
that
w
e
cannot
restrict
the
adv
ersary
.
What
w
e
can
do
is
c
hec
k
whether
the
pro
of
giv
en
to
us
is
of
the
kind
w
e
assume
it
is.
T
o
do
that
w
e
will
use
prop
erties
of
suc
h
pro
ofs.
F
or
example
in
our
case
w
e
ma
y
try
to
use
the
linearit
y
of
adding
linear
expressions:
for
ev
ery
t
w
o
expressions
e

;
e

it
holds
that
(e

+
e

)(a
0
;
:
:
:
;
a
n
)
=
e

(a
0
;
:
:
:
;
a
n
)
+
e

(a
0
;
:
:
:
;
a
n
).
Therefor
if
the
pro
of
is
of
the
kind
w
e
assume
it
to
b
e,
then
the
corresp
onding
v
alues
in
the
pro
of
will
also
resp
ect
addition,
i.e.
the
oracle

m
ust
satisfy

(C
(e

+
e

))
=

(C
(e

))
+

(C
(e

)).
In
general
w
e
lo
ok
for
a
c
haracteristic
of
a
sp
ecial
'kind'
of
pro
ofs.
W
e
w
an
t
that
assuming
a
pro
of
is
of
that
'kind',
one
w
ould
b
e
able
to
c
hec
k
its
v
alidit
y
using
only
O
()
queries,
and
that
c
hec
king
whether
a
pro
of
is
of
that
sp
ecial
'kind'
will
also
b
e
feasible
using
O
()
queries.
The
main
strategy
is
to
divide
the
v
erication
of
a
pro
of
to
t
w
o
parts
:
.
Chec
k
whether
the
pro
of
is
of
a
'go
o
d
kind'
(the
oracle
answ
ers
ha
v
e
a
sp
ecial
prop
ert
y),
otherwise
reject.
.
Assuming
that
the
pro
of
is
of
the
'go
o
d
kind',
determine
its
v
alidit
y
,
(for
example
is
the
pro
of
enco
ding
a
satisfying
assignmen
t
to
the
equation
system)
Up
to
this
p
oin
t
w
e
ha
v
e
dev
elop
ed
the
general
strategy
whic
h
w
e
in
tend
to
use
in
our
pro
of.
Our
c
haracteristic
of
a
'go
o
d'
pro
of
w
ould
b
e
that
it
enco
des
an
assignmen
t
to
the
v
ariables
in
a
sp
ecial
w
a
y
.
In
Section

w
e
will
dene
exactly
when
is
a
pro
of
of
the
'go
o
d
kind',
and
see
ho
w
'go
o
d'
pro
ofs'
v
alidit
y
can
b
e
c
hec
k
ed
using
O
()
queries.
In
Section

w
e
will
see
ho
w
to
c
hec
k
whether
a
pro
of
is
of
the
'go
o
d
kind'
in
O
()
queries.
The
bad
news
is
that
our
strategy
as
stated
ab
o
v
e
is
infeasible
if
tak
en
literally
.
W
e
searc
h
for
a
c
haracteristic
of
pro
ofs
that
will
enable
us
to
distinguish
b
et
w
een
'go
o
d'
pro
ofs
and
'bad'
pro
ofs
in
O
()
queries,
but
supp
ose
w
e
tak
e
a
'go
o
d'
pro
of
and
switc
h
just
one
bit.
The
probabilit
y
that
suc
h
a
'a
w
ed'
pro
of
can
b
e
distinguished
from
the
original
pro
of
in
O
()
queries
is
negligible.
(The
probabilit
y
of
ev
en
querying
ab
out
that
sp
ecic
bit
is
the
n
um
b
er
of
queries
o
v
er
the
size
of
the
pro
of,
but
the
size
of
the
pro
of
is
all
the
queries
one
ma
y
p
ose
to
an
oracle
whic
h
is
exp
onen
tial
in
our
case).
On
the
other
hand
this
problem
should
not
b
e
critical,
if
the
adv
ersary
c
hanges
only
one
bit,
it
seems
he
do
esn't
ha
v
e
a
v
ery
go
o
d
c
hance
of
deceiving
us,
since
w
e
probably
w
on't
read
that
bit
an
yw
a
y
and
therefor
it
w
ould
lo
ok
to
us
as
if
w
e
w
ere
giv
en
a
pro
of
of
the
'go
o
d'
kind.
It
seems
that
ev
en
if
distinguishing
b
et
w
een
'go
o
d'
pro
ofs
and
'bad'
ones
is
infeasible,
it
ma
y
suce
to
distinguish
b
et
w
een
'go
o
d'
pro
ofs
and
pro
ofs
whic
h
are
'v
ery
dieren
t'
from
'go
o
d'
pro
ofs.
W
e
should
try
to
nd
a
\kind"
of
pro
ofs
for
whic
h
w
e
can
v
erify
that
a
giv
en
pro
of
\lo
oks
similar"
to.
A
pro
of
will
b
e
\similar"
to
\go
o
d"
if
with
high
probabilit
y
sampling
from
it
will
yield
the
same
results
as
sampling
from
some
really
\go
o
d
pro
of
".
Since
w
e
only
sample
the
pro
of
giv
en
to
us
w
e
can
only
c
hec
k
that
the
pro
of
b
eha
v
es
\on
a
v
erage"
as
a
pro
of
with
the
prop
ert
y
w
e
w
an
t.
This
p
oses
a
problem
in
the
other
part
of
the
pro
of
c
hec
king
pro
cess.
When
w
e
think
of
it,
the
adv
ersary
do
es
not
really
ha
v
e
to
c
heat
in
man
y
places
to
deceiv
e
us,
just
in
the
few
critical
ones
(e.g.
the
lo
cation
C
(ex)
where
ex
is
the
expression
w
e
try
to
ev
aluate).
(Remem
b
er
our
adv
ersary
is
a
ctitious
all
p
o
w
erful
en
tit
y
-
it
kno
ws
all
ab
out
our
algorithm.
Our
only
edge
is
that
the
adv
ersary
m
ust
decide
on
a
pro
of
b
efore
w
e
c
ho
ose
our
random
mo
v
es).

..
TESTING
SA
TISFIABILITY
ASSUMING
A
NICE
ORA
CLE

F
or
example
in
our
\to
y
system"
w
e
decide
whether
the
equation
is
satisable
based
on
the
single
bit
C
(ex)
(whic
h
one
can
calculate
from
the
expression
ex)
so
the
adv
ersary
only
has
to
c
hange
that
bit
to
deceiv
e
us.
In
general
during
the
v
erication
pro
cess
w
e
do
not
ask
random
queries
but
ones
that
ha
v
e
an
in
ten
t,
usually
this
means
our
queries
ha
v
e
a
structure
i.e.
they
b
elong
to
a
small
subset
of
the
p
ossible
queries.
So
the
adv
ersary
can
gain
a
lot
b
y
c
heating
only
on
that
small
subset
while
\on
the
a
v
erage"
the
pro
of
will
still
lo
ok
as
of
the
go
o
d
kind.
Here
comes
our
tactical
maneuv
er:
What
w
e
w
ould
lik
e
is
to
enjo
y
b
oth
w
orlds,
ask
random
queries
and
y
et
get
answ
ers
to
the
queries
that
in
terest
us
(whic
h
are
from
a
small
subset).
It
turns
out
that
sometimes
this
can
b
e
done,
what
w
e
need
is
another
prop
ert
y
of
the
pro
ofs.
Supp
ose
that
w
e
w
an
t
to
ev
aluate
a
linear
function
L
on
a
p
oin
t
x.
W
e
can
c
ho
ose
a
random
shift
r
and
ev
aluate
L(x
+
r
)
and
L(r
)
w
e
can
no
w
ev
aluate
L(x)
=
L(x
+
r
)
 L(r
).
If
r
is
uniformly
distributed
so
is
x
+
r
(also
they
are
dep
enden
t),
so
w
e
ha
v
e
reduced
ev
aluating
L
at
a
sp
ecic
p
oin
t
(i.e.
x)
in
to
ev
aluating
L
at
t
w
o
random
p
oin
ts
(i.e.
r
and
x
+
r
).
In
general
w
e
w
an
t
to
calculate
the
v
alue
of
a
function
f
in
a
p
oin
t
x
b
y
applying
f
only
to
random
p
oin
ts,
w
e
need
some
metho
d
of
calculating
f
(x)
from
our
p
oin
t
x
the
shift
r
and
the
v
alue
of
the
function
on
the
random
p
oin
ts
f
(x
+
r
)
and
f
(r
).
If
on
a
random
p
oin
t
the
c
hance
of
a
mistak
e
is
small
enough
(i.e.
p)
then
the
c
hance
of
a
mistak
e
in
one
of
the
t
w
o
p
oin
ts
x
+
r
;
r
is
at
most
p
whic
h
means
w
e
get
the
v
alue
on
x
with
probabilit
y
greater
or
equal
then

 p.
If
one
can
apply
suc
h
a
tric
k
to
\go
o
d"
pro
ofs
,
w
e
can
ask
the
oracle
only
random
queries,
that
w
a
y
neutralizing
an
adv
ersary
attempt
to
c
heat
us
at
\imp
ortan
t
places".
This
is
called
self-c
orr
e
ction
since
w
e
can
\correct"
the
v
alue
of
a
function
(at
an
imp
ortan
t
p
oin
t)
using
the
same
function.
.
T
esting
satisabilit
y
assuming
a
nice
oracle
This
section
corresp
onds
to
the
second
part
of
our
strategy
i.e.
w
e
are
going
to
c
hec
k
satisabilit
y
while
assuming
the
oracle
answ
ers
are
nice
enough.
The
prop
ert
y
w
e
are
going
to
assume
is
as
follo
ws:
The
oracle

xes
some
assignmen
t
x

=
a

;
:
:
:
;
x
n
=
a
n
and
when
giv
en
a
sequence
of
co
ecien
ts
fb
ij
g
n
i;j
=
answ
ers
n
P
i;j
=
b
ij
a
i
a
j
.
W
e
assume
that
the
testing
phase
(to
b
e
describ
ed
in
Section
)
ensures
us
that
the
oracle

has
this
prop
ert
y
\on
the
a
v
erage"
i.e.
on
at
most
a
fraction
of
0.0
of
the
queries
w
e
migh
t
get
an
arbitrary
answ
er.
The
idea
is
that
the
oracle
enco
des
a
satisfying
assignmen
t
(if
the
equations
are
satisable)
and
that
our
task
is
v
erifying
that
the
assignmen
t
enco
ded
b
y
the
oracle
is
indeed
satisfying.
Note
that
w
e
ma
y
get
the
assignmen
t
a
i
to
an
y
sp
ecic
v
ariable
x
i
b
y
setting
b
ii
=

and
all
the
other
co
ecien
ts
b
k
l
=
0;
But
w
e
cannot
just
nd
the
en
tire
assignmen
t
(and
ev
aluate
the
equations)
since
that
w
ould
tak
e
a
linear
n
um
b
er
of
queries.
W
e
ha
v
e
to
c
hec
k
whether
the
set
f
n
P
i;j
=
c
(k
)
i;j
x
i
x
j
=
c
(k
)
g
m
k
=
is
satisable
using
only
a
constan
t
n
um
b
er
of
queries,
the
to
ols
at
our
disp
osal
allo
w
us
to
ev
aluate
an
y
quadratic
equation
with
the
assignmen
t
the
oracle
enco
des
(also
w
e
migh
t
ha
v
e
to
use
our
self
correction
tric
k
to
ensure
the
oracle
do
es
not
c
heat
on
the
questions
w
e
are
lik
ely
to
ask.
The
self
correction
will
w
ork
since
the
assignmen
t
is
xed
and
therefor
n
P
i;j
=
b
ij
a
i
a
j
is
just
a
linear
function
of
the
b
ij
s).
The
naiv
e
approac
h
of
c
hec
king
ev
ery
equation
in
our
set
will
not
w
ork
for
the
ob
vious
reason
that
the
n
um
b
er
of
queries
using
this
approac
h
equals
the
n
um
b
er
of
equations
whic
h
migh
t
b
e
linear
in
the
size
of
the
input.
W
e
m
ust
nd
a
w
a
y
to
\c
hec
k
man
y
equations
at
once".
Our
tric
k


LECTURE
.
NP
IN
PCP[POL
Y,O()]
will
b
e
random
summations
i.e.
w
e
will
toss
a
coin
for
eac
h
equation
and
sum
the
equations
for
whic
h
the
result
of
the
toss
is
.
If
all
the
equations
are
satised
clearly
the
sum
of
the
equations
will
b
e
satised
(w
e
sum
b
oth
sides
of
the
equations).
Random
summation
test

Cho
ose
a
random
subset
S
of
the
equations
f
n
P
i;j
=
c
(k
)
i;j
x
i
x
j
=
c
(k
)
g
m
k
=

Sum
the
linear
expressions
at
the
left
hand
side
of
the
equations
in
S
and
denote
ex
r
sum
=
P
k
S
(
n
P
i;j
=
c
(k
)
i;j
x
i
x
j
).
After
rearranging
the
summands
presen
t
ex
r
sum
in
normal
form.

Sum
the
constan
ts
at
the
righ
t
hand
side
of
the
equations
in
S
and
denote
c
r
sum
=
P
k
S
c
(k
)
.
(Note
that
if
an
assignmen
t
a

;
:
:
:
;
a
n
is
satisfying
then
ex
r
sum
(a

;
:
:
:
;
a
n
)
=
c
r
sum
).

Query
ab
out
ex
r
sum
using
self
correction
tec
hnique
and
compare
the
answ
er
to
c
r
sum
.
Accept
if
they
are
equal
and
reject
otherwise.
What
is
the
probabilit
y
of
disco
v
ering
that
not
all
the
equations
w
ere
satised
?
If
only
one
equation
is
not
satised
w
e
w
ould
disco
v
er
that
with
probabilit
y


since
if
w
e
include
that
equation
in
the
sum
(set
S
),
the
left
hand
side
of
the
sum
is
not
equal
to
the
righ
t
hand
side
no
matter
whic
h
other
equations
w
e
sum.
But
what
happ
ens
if
w
e
ha
v
e
more
then
one
unsatised
equation?
In
this
case
t
w
o
wrongs
mak
e
a
righ
t
since
if
w
e
sum
t
w
o
unsatised
equations
the
left
hand
side
will
b
e
equal
to
the
righ
t
hand
side
(this
is
the
w
onderful
w
orld
of
GF()).
T
o
see
that
in
an
y
case
the
probabilit
y
of
disco
v
ering
the
existence
of
an
unsatised
equation
is


,
consider
the
last
toss
of
coin
for
an
unsatised
equation
(i.e.
for
all
the
other
unsatised
equations
it
w
as
already
decided
whether
to
include
them
in
the
sum
or
not).
If
the
sum
up
to
no
w
is
satised
then
with
probabilit
y


w
e
will
include
that
equation
in
the
sum
and
therefor
ha
v
e
an
unsatised
sum
at
the
end,
and
if
the
sum
up
to
no
w
is
not
satised
then
with
probabilit
y


w
e
will
not
include
that
last
equation
in
the
sum
and
remain
with
an
unsatised
sum
at
the
end.
W
e
ha
v
e
seen
that
if
an
assignmen
t
do
es
not
satisfy
all
the
equations,
then
with
probabilit
y


it
fails
the
random
sum
test.
All
that
is
left
to
completely
analyze
the
test
(assuming
the
oracle
w
as
tested
and
found
nice)
is
a
little
b
o
ok
k
eeping.
First
w
e
m
ust
note
that
the
fact
the
oracle
passed
the
\oracle
test"
and
found
nice,
do
es
not
assure
us
that
all
its
answ
ers
are
reliable,
there
is
0.0
probabilit
y
of
getting
an
arbitrary
answ
er.
In
fact
since
our
queries
ha
v
e
a
structure
(they
are
all
partial
sums
of
the
original
set
of
equations),
w
e
m
ust
use
self
correction
whic
h
means
asking
t
w
o
queries
for
ev
ery
query
w
e
really
w
an
t
to
ask
(of
the
random
shift
and
of
the
shifted
p
oin
t
of
in
terest).
During
a
random
summation
test
w
e
need
to
ask
only
one
query
(of
the
random
sum),
using
self
correction
that
b
ecomes
t
w
o
queries
(of
the
shifted
p
oin
t
and
of
the
random
shift)
and
therefor
w
e
ha
v
e
a
probabilit
y
of
0.0
that
our
result
will
b
e
arbitrary
.
Assuming
the
answ
ers
w
e
got
w
ere
not
arbitrary
,
the
probabilit
y
of
detecting
an
unsatisfying
assignmen
t
is
0.
.
Therefor
the
o
v
erall
probabilit
y
of
detecting
a
bad
assignmen
t
is
0:	

0:
=
0:	
making
the
probabilit
y
of
failure
0..
Applying
the
test
t
wice
and
rejecting
if
one
of
the
trials
fail
w
e
get
failure
probabilit
y
of
0:

<
0:.

..
DISTINGUISHING
A
NICE
ORA
CLE
FR
OM
A
VER
Y
UGL
Y
ONE

The
result
of
our
\b
o
ok
k
eeping"
is
that
if
w
e
can
test
that
the
oracle
has
our
prop
ert
y
with
failure
probabilit
y
smaller
then
0.,
then
the
probabilit
y
of
the
en
tire
pro
cedure
to
fail
is
smaller
than
0:,
and
w
e
satisfy
the
P
C
P
satisabilit
y
requiremen
t.
(In
our
case
the
completeness
requiremen
t
is
easy
,
just
note
that
an
oracle
enco
ding
a
satisfying
assignmen
t
alw
a
ys
pass).
.
Distinguishing
a
nice
oracle
from
a
v
ery
ugly
one
In
this
section
our
ob
jectiv
e
is
to
presen
t
a
metho
d
of
testing
whether
the
oracle
b
eha
v
es
\t
ypically
w
ell".
W
e
shall
sa
y
that
an
oracle

b
eha
v
es
\w
ell"
if
it
enco
des
an
assignmen
t
in
the
w
a
y
dened
ab
o
v
e,
and
that

b
eha
v
es
\t
ypicly
w
ell"
if
it
agrees
on
more
then
		%
of
the
queries
with
a
function
that
enco
des
an
assignmen
t
that
w
a
y
.
Our
test
should
detect
an
oracle
that
do
es
not
b
eha
v
e
\t
ypicly
w
ell"
with
probabilit
y
greater
then
0.
.
As
describ
ed
in
the
strategy
our
approac
h
is
to
searc
h
some
c
haracterizing
prop
erties
of
the
\go
o
d
oracles".
What
do
es
a
\go
o
d
oracle"
lo
ok
lik
e
?
Assume

is
a
go
o
d
oracle
corresp
onding
to
an
assignmen
t
x

=
a

;
:
:
:
;
x
n
=
a
n
.
W
e
ma
y
lo
ok
at
the
oracle
as
a
function

:
f0;
g
n

!
f0;
g
denoting
its
argumen
t
b
=
(b

;
:
:
:
;
b
n
;
b

;
:
:
:
;
b
nn
)
>
.
Then,
since
a

:
:
:
;
a
n
are
xed,

is
a
linear
function
of
b
i.e.

(b
)
=
n
P
i;j
=

ij
b
ij
.
F
urthermore

's
co
ecien
ts

ij
ha
v
e
a
sp
ecial
structure
-
there
exists
a
sequence
of
constan
ts
fa
i
g
n
i=
s.t.

ij
=
a
i
a
j
.
It
turns
out
that
these
prop
erties
c
haracterize
the
\go
o
d
oracles",
since
if


is
a
linear
function
for
whic
h
there
exists
a
sequence
of
constan
ts
fa

i
g
n
i=
s.t.


's
co
ecien
ts
are


ij
=
a

i
a

j
,
then


is
enco
ding
the
assignmen
t
x

=
a


;
:
:
:
;
x
n
=
a

n
.
Our
\oracle
test"
is
comp
osed
of
t
w
o
stages:
testing
the
linearit
y
of

and
testing
that
the
linearit
y
co
ecien
ts

ij
's
ha
v
e
the
correct
structure.
..
T
ests
of
linearit
y
In
order
to
devise
a
test
and
pro
v
e
its
correctness
w
e
b
egin
with
some
formal
denitions.
Denition
.
(linearit
y
of
a
function)
A
function
f
:
f0;
g
m
!
f0;
g
is
c
al
le
d
linea
r
if
ther
e
exist
c
onstants
a
f

;
:
:
:
;
a
f
m
s.t.
for
al
l

=
(

;
:
:
:
;

m
)
>

f0;
g
m
it
holds
that
f
(
)
=
m
P
i=
a
f
i

i
.
Claim
..
(alternativ
e
denition
of
linearit
y)
A
function
f
:
f0;
g
m
!
f0;
g
is
linea
r
i
for
every
two
ve
ctors


;



f0;
g
m
and
every


;



f0;
g
it
holds
that:


f
(

)
+


f
(

)
=
f
(



+




).
Pro
of:
Supp
ose
rst
that
f
is
linear
b
y
the
denition
i.e.
there
exists
constan
ts
a
f

;
:
:
:
;
a
f
m
s.t.
for
all

=
(

;
:
:
:
;

m
)
>
,
f
(
)
=
m
P
i=
a
f
i

i
.
Then
for
ev
ery


;


it
holds
that


f
(

)
+


f
(

)
=
m
P
i=
a
f
i




i
+
m
P
i=
a
f
i




i
=
m
P
i=
a
f
i
(



i
+




i
)
=
f
(



+




).
Supp
ose
no
w
that
the
claim
holds
i.e.
for
ev
ery
t
w
o
v
ectors


;



f0;
g
m


f
(

)
+


f
(

)
=
f
(



+




).
Denote
b
y
a
f
i
the
v
alue
f
(e
i
)
where
e
i
is
the
i
th
elemen
t
in
the
standard
basis
i.e.
all
e
i
's
co
ordinates
but
the
i
th
are
0
and
the
i
th
co
ordinate
is
.
Ev
ery


f0;
g
m
can
b
e
expressed
as

=
m
P
i=

i
e
i
.


LECTURE
.
NP
IN
PCP[POL
Y,O()]
Then,
f
(
)
=
f
(
m
P
i=

i
e
i
)
and
b
y
the
claim
w
e
get
:
f
(
m
P
i=

i
e
i
)
=
m
P
i=

i
f
(e
i
)
=
m
P
i=

i
a
f
i
Denition
.
(distance
from
linearit
y)
Two
functions
f
;
g
:
f0;
g
m
!
f0;
g
ar
e
said
to
b
e
at
distance
at
least

(or

far)
if
:
Pr


R
f0;g
m
[f
(
)
=
g
(
)]


Two
functions
f
;
g
:
f0;
g
m
!
f0;
g
ar
e
said
to
b
e
at
distance
at
most

(or

close)
if
:
Pr


R
f0;g
m
[f
(
)
=
g
(
)]


Two
functions
f
;
g
:
f0;
g
m
!
f0;
g
ar
e
said
to
b
e
at
distance

if
:
Pr


R
f0;g
m
[f
(
)
=
g
(
)]
=

A
function
f
:
f0;
g
m
!
f0;
g
is
said
to
b
e
at
distance
at
most

from
linea
r
if
ther
e
exists
some
line
ar
function
L
:
f0;
g
m
!
f0;
g
s.t.
f
is
at
distanc
e
at
most

fr
om
L.
In
a
similar
fashion
we
dene
distance
at
least

from
linea
r
and
distance

from
linea
r.
Notice
that
since
there
are
only
nitely
man
y
linear
functions
L
:
f0;
g
m
!
f0;
g
for
ev
ery
function
f
there
is
a
closest
linear
function
(not
necessarily
unique)
and
the
distance
from
linearit
y
is
w
ell
dened.
W
e
dene
no
w
a
v
erication
algorithm
A
()
()
that
accepts
as
input
a
distance
parameter

and
oracle
access
to
a
function
f
(therefor
w
e
actually
run
A
f
()),
and
tries
to
distinguish
b
et
w
een
f
's
whic
h
are
at
distance
at
least

from
linear
and
linear
functions.
A
iterates
a
\basic
pro
cedure"
T
()
that
detects
functions
whic
h
are
\bad"
(-far
from
linear)
with
small
constan
t
probabilit
y
.
Using
enough
iterations
ensures
the
detection
of
\bad"
f
's
with
the
desired
probabilit
y
.
Basic
pro
cedure
T
f
:

Select
at
random
a;
b

R
f0;
g
n
.

Chec
k
whether
f
(a
)
+
f
(b)
=
f
(a
+
b),
if
not
reject.
Linearit
y
T
ester
A
f
():

Rep
eat
for
d


e
times
the
basic
pro
cedure
T
(f
)

Reject
if
T
(f
)
rejects
ev
en
once,
accept
otherwise.
Theorem
.
(Linearit
y
testing):
If
f
is
line
ar
T
f
always
ac
c
epts,
and
if
f
is
at
distanc
e
at
le
ast

fr
om
line
ar
then
Pr[T
f
r
ej
ects]

p
r
ej
def
=


.
Pro
of:
Clearly
if
f
is
linear
T
will
accept.
If
f
is
not
linear
w
e
deal
separately
with
functions
close
to
linear
and
with
functions
relativ
ely
far
from
linear.
Denote
b
y

the
distance
of
f
from
linearit
y
and
let
L
b
e
a
linear
function
at
distance

from
f
.
Denote
b
y
G
=
fa
jf
(a)
=
L(a
)g
the
set
of
\go
o
d"
inputs
on
whic
h
the
functions
agree.
Clearly
jGj
=
(
 
)
m
.
W
e
shall
try
to
b
ound
from
b
elo
w
the
probabilit
y
that
an
iteration
of
A
rejects.

..
DISTINGUISHING
A
NICE
ORA
CLE
FR
OM
A
VER
Y
UGL
Y
ONE

Since
the
v
alue
of
a
linear
function
of
ev
ery
t
w
o
from
the
three
p
oin
ts
a;
b;
a
+
b
xes
the
v
alue
of
third,
it
is
easy
to
see
that
the
algorithm
will
reject
if
t
w
o
of
these
p
oin
ts
are
in
G
and
the
third
is
not
in
G.
Therefor
the
probabilit
y
that
one
iteration
rejects
is
greater
or
equal
to
Pr
[a

G;
b

G;
(a
+
b)
=

G]
+
Pr
[a

G;
b
=

G;
(a
+
b)

G]
+
Pr[a
=

G;
b

G;
a
+
b

G].
The
three
ev
en
ts
are
clearly
disjoin
t.
What
migh
t
b
e
less
ob
vious
is
that
they
are
symmetric.
It
is
easy
to
see
Pr
[a
=

G;
b

G;
(a
+
b
)

G]
=
Pr[a

G;
b
=

G;
(a
+
b)

G].
Notice
also
that
instead
of
c
ho
osing
b
at
random
w
e
ma
y
c
ho
ose
(a
+
b)
at
random
and
then
a
+
(a
+
b)
=
b,
so
the
third
ev
en
t
is
also
symmetric
and
therefor
Pr[a

G;
b

G;
(a
+
b)
=

G]
=
Pr[a
=

G;
b

G;
(a
+
b)

G].
Th
us
the
probabilit
y
of
rejection
is
greater
or
equal
to
Pr[a
=

G;
b

G;
(a
+
b)

G].
The
latter
can
b
e
presen
ted
as
Pr[a
=

G]

Pr[b

G;
(a
+
b)

Gja
=

G].
By
denition
of

it
holds
that
Pr
[a
=

G]
=

.
It
is
also
clear
that
Pr
[b

G;
(a
+
b)

Gja
=

G]
=

 Pr[b
=

G
or
(a
+
b)
=

Gja
=

G].
Therefor
the
probabilit
y
of
rejection
is
greater
or
equal
to

(
 Pr
[b
=

G
or
(a
+
b)
=

Gja
=

G]).
Using
the
symmetry
of
b
and
a
+
b
explained
ab
o
v
e
and
union
b
ound
the
probabilit
y
of
rejection
is
greater
or
equal
to

(
 Pr[b
=

Gja
=

G]).
Since
a
and
b
w
ere
c
hosen
indep
enden
tly
the
probabilit
y
of
rejection
is
greater
or
equal
to

(
 Pr[b
=

G])
=

(
 
).
Notice
that
the
analysis
ab
o
v
e
is
go
o
d
for
small

;
i.e.,
for
functions
whic
h
are
quite
close
to
linear
functions.
Ho
w
ev
er
the
function

(
 
)
drops
to
0
at


,
therefor
the
analysis
is
not
go
o
d
enough
for
functions
whic
h
are
quite
far
from
linear
functions.
The
ob
vious
reason
for
the
failure
of
the
analysis
is
that
if
the
function
is
far
enough
from
a
linear
function
then
the
probabilit
y
that
t
w
o
of
the
three
p
oin
ts
will
fall
inside
the
\go
o
d"
set
of
p
oin
ts
(whose
f
v
alue
is
iden
tical
to
their
v
alue
under
the
closest
linear
function)
is
small.
Th
us,
w
e
need
an
alternativ
e
analysis
for
the
case
of
big

.
Sp
ecically
,
w
e
sho
w
that
if
f
is
at
distance
greater
or
equal


then
the
probabilit
y
of
rejection
is
at
least


.
As
the
pro
of
is
rather
long
and
tec
hnical
it
is
giv
en
in
App
endix
B.
Com
bining
b
oth
cases,
of
functions
relativ
ely
close
to
linear
and
functions
relativ
ely
far
from
linear
w
e
get
the
desired
result.
That
is,
let

b
e
the
distance
of
f
from
linear.
Note
that




,
(since
for
ev
ery
function
f
the
exp
ected
distance
of
f
from
a
random
linear
function
is


).
In
case

>


w
e
ha
v
e
a
rejection
probabilit
y
of
at
least





.
Otherwise,




,
and
w
e
ha
v
e
a
rejection
probabilit
y
of
at
least

(
 
)
>

(
 



)
=



.
Corollary
.
If
f
is
line
ar
then
the
veric
ation
algorithm
A
(f
)
()
always
ac
c
epts
it.
If
f
is

far
fr
om
line
ar
then
A
(f
)
()
r
eje
cts
it
with
pr
ob
ability
lar
ger
then
0.		
Pro
of:
If
f
is
linear
it
alw
a
ys
passes
the
basic
pro
cedure
T
.
Supp
ose
f
is

far
from
linear
then
b
y
Theorem

the
probabilit
y
that
one
iteration
of
the
basic
pro
cedure
T
f
rejects
is
bigger
then


.
Therefor
the
probabilit
y
that
f
passes
all
the
iterations
A
in
v
ok
es
is
smaller
then
(
 


)



.
By
the
inequalit
y

 x

e
 x
the
probabilit
y
that
A
accepts
is
smaller
then
e
 




=
e
 	
<
0:0.
(T
o
pro
v
e
the

 x

e
 x
inequalit
y
notice
equalit
y
holds
for
0,
and
dieren
tiate
b
oth
sides
to
see
that
the
righ
t
hand
side
drops
slo
w
er
then
the
left
hand
side).
..
Assuming
linear

testing

's
co
ecien
ts
structure
If

passes
the
linearit
y
tester
w
e
assume
that
there
exists
a
linear
function
L

:
f0;
g
n

!
f0;
g
at
distance
smaller
or
equal
0.0
from

.
F
or
the
rest
of
the
discussion
L

stands
for
some
suc
h


LECTURE
.
NP
IN
PCP[POL
Y,O()]
xed
linear
function.
F
or

to
b
e
nice
it
is
not
enough
that
it
is
close
to
a
linear
L

,
but
L

m
ust
also
b
e
of
the
sp
ecial
structure
that
enco
des
an
assignmen
t.
W
e
sa
w
that
this
means
that
there
exists
a
sequence
fa
i
g
n
i=
s.t.
L

(b)
=
n
P
i;j
=
a
i
a
j
b
ij
in
other
w
ords
L

's
linearit
y
co
ecien
ts
f
ij
g
n
i;j
=
ha
v
e
the
sp
ecial
structure
whic
h
is

ij
=
a
i
a
j
.
L

is
a
linear
function
from
f0;
g
n

to
f0;
g
and
therefor
its
natural
represen
tation
is
as
a
ro
w
v
ector
of
length
n

(

n

matrix).
Ho
w
ev
er
if
w
e
rearrange
L

's
n

co
ecien
ts
to
an
n

n
matrix
form
the
constrain
t
on
L

can
b
e
phrased
in
a
v
ery
elegan
t
form,
namely
there
exists
a
v
ector
a
=
(a

;
:
:
:
;
a
n
)
>
s.t.
(
ij
)
=
a
a
>
.
(Notice
this
is
not
a
scalar
pro
duct
but
an
outer
pro
duct
and
the
result
is
an
n

n
matrix).
Notice
that
(a
a
>
)
ij
=
a
i
a
j
whic
h
is
exactly
what
w
e
w
an
t
of

ij
.
So
what
has
to
b
e
c
hec
k
ed
is
that
(
ij
)
really
has
this
structure
i.e.
there
exists
a
v
ector
a
s.t.
(
ij
)
=
a
a
>
.
Ho
w
can
this
b
e
done
?
Notice
that
if
(
ij
)
has
this
sp
ecial
structure
then
the
v
ector
a
is
exactly
the
diagonal
of
the
matrix
(
ij
).
This
is
the
case
since

ii
=
a
i
a
i
=
a

i
ho
w
ev
er
in
GF()
x

=
x
for
ev
ery
x,
leading
us
to

ii
=
a
i
.
The
last
observ
ation
means
that
w
e
can
nd
out
ev
ery
co
ecien
t
a
i
in
a
simply
b
y
querying
with
b
ii
=

and
the
rest
of
the
b
k
l
=
0
(w
e
will
ha
v
e
to
use
self
correction
as
alw
a
ys).
This
seems
to
lead
us
to
a
reasonable
test.
First
obtain
a
b
y
querying
its
co
ecien
ts
a
i
,
then
construct
a
a
>
.
What
has
to
b
e
done
no
w
is
to
c
hec
k
whether
(
ij
)
is
indeed
equal
to
the
matrix
w
e
ha
v
e
constructed
a
a
>
.
A
natural
approac
h
for
c
hec
king
this
equalit
y
will
b
e
sampling:
w
e
ha
v
e
access
to
(
ij
)
as
a
function
since
w
e
can
query

,
w
e
can
try
to
sim
ulate
a
a
>
as
a
function
and
compare
their
results
on
random
inputs.
The
problem
with
this
natural
approac
h
is
that
querying
ab
out
ev
ery
co
ecien
t
a
i
w
ould
cost
a
linear
n
um
b
er
of
queries
and
w
e
can
only
aord
a
constan
t
n
um
b
er
of
queries.
It
seems
w
e
will
ha
v
e
to
get
b
y
without
explicitly
constructing
the
en
tire
a.
As
the
\next
b
est
thing"
w
e
ma
y
try
to
\capture"
a
b
y
a
random
summation
of
its
co
ecien
ts.
A
random
sum
of
a
co
ecien
ts
dened
b
y
a
random
string
r

R
f0;
g
n
is
the
sum
of
the
co
ecien
ts
a
i
for
whic
h
r
i
=

i.e.
n
P
i=
r
i
a
i
.
This
is
the
scalar
pro
duct
of
a
with
r
denoted
<
a
;
r
>.
The
random
sum
is
of
course
a
function
of
the
random
string
r
so
w
e
in
tro
duce
the
notation
A()
=<
a
;

>
where
A()
is
the
random
sum
as
a
function
of
r
.
Just
as
w
e
can
nd
an
y
co
ecien
t
a
i
of
a
b
y
querying

with
the
appropriate
bit
on
the
diagonal
b
ii
turned
on,
w
e
can
nd
the
result
of
a
sum
of
co
ecien
ts.
T
o
get
the
result
of
a
sum
of
the
co
ecien
ts
i

;
:
:
:
;
i
k
all
w
e
ha
v
e
to
do
is
query
with
b
i

i

=
;
:
:
:
;
b
i
k
i
k
=

and
all
the
other
bits
0.
The
result
will
b
e
k
P
l
=
x

i
l
=
k
P
l
=
x
i
l
whic
h
is
what
w
e
w
an
t.
(As
alw
a
ys
w
e
ha
v
e
to
use
self
correction).
Ho
w
is
all
this
going
to
help
us
to
compare
the
matrices
a
a
>
to
(
ij
)
?
Most
of
us
immediately
iden
tify
an
n

n
matrix
M
with
its
corresp
onding
linear
function
from
f0;
g
n
to
f0;
g
n
.
Ho
w
ev
er
suc
h
a
matrix
can
also
stand
for
a
bilinear
function
f
M
:
f0;
g
n

f0;
g
n
!
f0;
g,
(where
f
M
(x;
y
)
=
x
>
M
y
).
F
or
matrices
of
the
form
a
a
>
the
op
eration
b
ecomes
v
ery
simple
:
f
(a
a
>
)
(x
;
y
)
=
x
>
(a
a
>
)y
=
(x
>
a)(a
>
y
)
=<
x;
a
><
a
;
y
>=
A(x)A(y
)
(The
same
result
can
also
b
e
dev
elop
ed
in
the
more
tec
hnical
w
a
y
of
op
ening
the
summations).
Our
access
to
A()
enables
us
to
ev
aluate
the
bilinear
function
represen
ted
b
y
a
a
>
.
W
e
can
also
ev
aluate
the
bilinear
function
represen
ted
b
y
(
ij
)
since
:

..
DISTINGUISHING
A
NICE
ORA
CLE
FR
OM
A
VER
Y
UGL
Y
ONE

f
(
ij
)
(x
;
y
)
=
x
>
(
ij
)y
=
n
P
i;j
=

ij
x
i
y
j
.
So
in
order
to
ev
aluate
f
(
ij
)
(x
;
y
)
w
e
only
ha
v
e
to
feed
b
ij
=
x
i
y
j
in
to

.
Once
again
w
e
will
use
our
self
correcting
tec
hnique,
this
time
the
structure
of
the
queries
do
es
not
stand
out
as
in
the
the
case
of
A().
Nonetheless,
the
distribution
of
queries
is
not
uniform
(for
example
there
is
a
sk
ew
to
w
ards
0s
as
it
is
enough
that
one
of
the
co
ordinates
x
i
or
y
j
is
0
so
that
b
ij
will
b
e
0).
It
seems
reasonable
to
test
whether
(
ij
)
=
a
a
>
b
y
testing
whether
the
bilinear
functions
rep-
resen
ted
b
y
the
t
w
o
matrices
are
the
same,
and
do
that
b
y
sampling.
The
idea
is
to
sample
random
v
ectors
x;
y

f0;
g
n
and
c
hec
k
if
the
functions
agree
on
their
v
alue.
Structure
test
for
(
ij
):

Select
at
random
x;
y

R
f0;
g
n
.

Ev
aluate
A(x)
and
A(y
)
b
y
querying
with
co
ecien
ts
corresp
onding
to
a
matrix
U
where
x
(resp.
y
)
is
the
diagonal
i.e.
U
ii
=
x
i
and
the
rest
of
the
co
ecien
ts
are
0s.
(The
queries
should
b
e
presen
ted
using
self
correction).

Compute
f
(a
a
>
)
(x;
y
)
=
A(x
)

A(y
)

Query
f
(
ij
)
(x
;
y
)
b
y
querying

with
fx
i
y
j
g
n
i;j
=
as
the
query
bits.
(Again
self
correction
m
ust
b
e
used).

Accept
if
f
(a
a
>
)
(x;
y
)
=
f
(
ij
)
(x;
y
),
Reject
otherwise.
W
e
are
left
to
pro
v
e
that
if
the
matrices
(
ij
)
and
a
a
>
dier
then
if
w
e
sample
w
e
will
get
dieren
t
results
of
the
bilinear
functions
with
reasonably
high
probabilit
y
.
Giv
en
t
w
o
dieren
t
matrices
M
;
N
w
e
w
an
t
to
b
ound
from
b
elo
w
the
probabilit
y
that
for
t
w
o
random
v
ectors
x;
y
the
bilinear
functions
f
M
(x
;
y
)
and
f
N
(x;
y
)
agree.
The
question
is
when
x
>
M
y
=
x
>
N
y
?
Clearly
this
is
equiv
alen
t
to
x
>
(M
 N
)y
=
0.
Supp
ose
w
e
c
ho
ose
y
rst,
if
(M
 N
)y
=
0
then
whatev
er
x
w
e
c
ho
ose
the
result
x
>
0
will
alw
a
ys
b
e
0
and
do
es
not
dep
end
on
x.
If
on
the
other
hand
(M
 N
)y
=
0
then
it
migh
t
b
e
the
case
that
x
>
((M
 N
)y
)
=
0
dep
ending
on
the
c
hoice
of
x.
W
e
will
analyze
the
probabilities
for
these
t
w
o
ev
en
ts
separately
.
T
o
analyze
the
probabilit
y
of
c
ho
osing
a
y
s.t.
(M
 N
)y
=
0
,
denote
the
column
dimension
of
(M
 N
)
b
y
d.
There
exist
d
linearly
indep
enden
t
columns
i

;
:
:
:
;
i
d
of
(M
 N
)
assume
without
loss
of
generalit
y
these
are
the
last
d
columns
n
 d
+
;
:
:
:
;
n.
Also
assume
the
c
hoice
of
y
is
made
b
y
tossing
coins
for
its
co
ordinates'
v
alues
one
b
y
one
and
that
the
coins
for
the
last
co
ordinates
n
 d
+
;
:
:
:
;
n
are
tossed
last.
Lets
lo
ok
at
the
situation
just
b
efore
the
last
d
coins
are
tossed
(the
rest
of
the
coins
ha
v
e
already
b
een
c
hosen).
The
v
alue
of
(M
 N
)y
will
b
e
the
sum
of
columns
corresp
onding
to
co
ordinates
of
v
alue
,
ho
w
ev
er
at
our
stage
not
all
the
co
ordinates
v
alues
ha
v
e
b
een
c
hosen.
A
t
this
stage
w
e
ma
y
lo
ok
of
at
the
last
d
columns'
\con
tribution"
to
the
nal
sum
((M
 N
)y
)
as
a
random
v
ariable.
Denote
b
y
(M
 N
)
#j
the
j
th
column
in
(M
 N
)
and
b
y
v
r
and
the
random
v
ariable
n
P
k
=n d+
y
k

(M
 N
)
#k
.
(The
sum
of
columns
corresp
onding
to
co
ordinates
of
v
alue

out
of
the
last
d
co
ordinates).
The
rest
of
the
co
ordinates
\con
tribution"
has
already
b
een
set.
Denote
b
y
v
set
the
v
ector
n d
P
k
=
y
k

(M
 N
)
#k
(The
con
tribution
of
the
rest
of
the
co
ordinates).
Clearly
(M
 N
)y
=
v
set
+
v
r
and
i.e.
(M
 N
)y
=
0
i
v
set
=
 v
r
and
.
The
question
is
can
this
happ
en
and
at
what
probabilit
y
?
First
note
that
this
can
happ
en
-
otherwise
v
set
is
indep
enden
t


LECTURE
.
NP
IN
PCP[POL
Y,O()]
of
columns
i

;
:
:
:
;
i
d
whic
h
means
the
columns
dimension
is
bigger
then
d.
Second
notice
that
since
columns
i

;
:
:
:
;
i
d
are
indep
enden
t
there
is
only
one
linear
com
bination
of
them
that
equals
v
set
.
This
means
that
there
is
only
one
result
of
the
coin
tosses
for
co
ordinates
i

;
:
:
:
;
i
d
that
will
mak
e
v
r
and
equal
v
set
,
i.e.
the
probabilit
y
is

 d
=

 dim(M
 N
)
.
Assuming
(M
 N
)y
=
0
what
is
the
probabilit
y
that
x
>
((M
 N
)y
)
=
0
?
The
question
is
giv
en
a
xed
v
ector
that
is
not
0
what
is
the
probabilit
y
that
its
inner
pro
duct
with
a
randomly
c
hosen
v
ector
is
0
?
By
exactly
the
same
reasoning
used
ab
o
v
e
(and
in
the
random
sum
argumen
t
in
Section
),
the
probabilit
y
is


.
(T
oss
coins
for
the
random
v
ector
co
ordinates
one
b
y
one
and
lo
ok
at
the
coin
tossed
for
a
co
ordinate
whic
h
is
not
0
in
(M
 N
)y
,
there
is
alw
a
ys
a
result
of
the
coin
ip
that
w
ould
bring
the
inner
pro
duct
to
0).
Ov
erall
the
t
w
o
v
ectors
ha
v
e
the
same
image
if
either
(M
 N
)y
=
0
(with
probabilit
y
b
ounded
b
y


)
or
if
x
>
((M
 N
)y
)
=
0
(with
probabilit
y


).
Therefor
the
probabilit
y
that
the
t
w
o
v
ectors
dier
is
at
least


.
T
o
nish
the
probabilit
y
\b
o
ok
k
eeping"
w
e
m
ust
tak
e
in
to
accoun
t
the
probabilit
y
of
getting
wrong
answ
ers
from
the
oracle.
The
test
itself
has
a
failure
probabilit
y
of
0.
.
The
test
uses
three
queries
(A(x
);
A(y
)
and
f
(
ij
)
(x
;
y
)
)
but
using
self
correction
eac
h
of
those
b
ecomes
t
w
o
queries.
W
e
get
a
probabilit
y
of
0.0
of
getting
an
arbitrary
answ
er
leading
us
to
probabilit
y
of
0.
for
the
test
to
fail.
Rep
eating
the
test
0
indep
enden
t
times
w
e
get
a
failure
probabilit
y
of
0:
0



and
the
probabilit
y
of
failure
in
either
the
linearit
y
testing
or
structure
test
is
b
ounded
b
y
0:
+
0:0

0:.
The
goal
of
detecting
\bad
oracles"
with
probabilit
y
greater
than
0.
is
ac
hiev
ed.
..
Gluing
it
all
together
Basicly
our
v
erication
of
a
pro
of
consists
of
three
stages:
.
linearit
y
testing
:
a
phase
in
whic
h
if
the
oracle

is
0.0
far
from
linear
w
e
ha
v
e
a
c
hance
of
0.		
to
catc
h
it
(and
reject).
T
o
implemen
t
this
stage
w
e
use
the
linearit
y
tester
A

(0:0).
(Distance
parameter
set
to
0.0).
.
structure
test
:
assuming
the
oracle

is
close
to
linear
w
e
test
that
it
enco
des
an
assignmen
t
using
the
\structure
test".
T
o
b
o
ost
up
probabilit
y
w
e
rep
eat
the
test
0
indep
enden
t
times.
A
t
the
end
of
this
stage
w
e
detect
bad
oracles
with
probabilit
y
greater
than
0.
.
satisabilit
y
test
:
assuming
\go
o
d
oracle"
w
e
use
the
\random
summation"
test
to
v
erify
that
the
assignmen
t
enco
ded
b
y
the
oracle
is
indeed
satisfying.
(This
test
is
rep
eated
t
wice).
Bibliographic
Notes
This
lecture
is
mostly
based
on
[],
where
N
P

P
C
P
[p
oly
;
O
()]
is
pro
v
en
(as
a
step
to
w
ards
pro
ving
N
P

P
C
P
[log
;
O
()]).
The
linearit
y
tester
is
due
to
[],
but
the
main
analysis
presen
ted
here
follo
ws
[].
F
or
further
studies
of
this
linearit
y
tester
see
[].
.
S.
Arora,
C.
Lund,
R.
Mot
w
ani,
M.
Sudan
and
M.
Szegedy
.
Pro
of
V
erication
and
In
tractabil-
it
y
of
Appro
ximation
Problems.
JA
CM,
V
ol.
,
pages
0{,
		.
Preliminary
v
ersion
in
r
d
F
OCS,
		.

..
DISTINGUISHING
A
NICE
ORA
CLE
FR
OM
A
VER
Y
UGL
Y
ONE
	
.
M.
Bellare,
D.
Copp
ersmith,
J.
Hastad,
M.
Kiwi
and
M.
Sudan.
Linearit
y
testing
in
c
har-
acteristic
t
w
o.
IEEE
T
r
ansactions
on
Information
The
ory,
V
ol.
,
No.
,
No
v
em
b
er
		,
pages
{	.
.
M.
Bellare,
S.
Goldw
asser,
C.
Lund
and
A.
Russell.
Ecien
t
probabilistically
c
hec
k
able
pro
ofs
and
applications
to
appro
ximation.
In
th
STOC,
pages
	{0,
		.
.
M.
Blum,
M.
Lub
y
and
R.
Rubinfeld.
Self-T
esting/Correcting
with
Applications
to
Numerical
Problems.
JCSS,
V
ol.
,
No.
,
pages
	{	,
		.
Preliminary
v
ersion
in
nd
STOC,
		0.
App
endix
A:
Linear
functions
are
far
apart
In
this
section
w
e
in
tend
to
pro
v
e
that
linear
functions
are
far
apart.
In
addition
to
the
natural
in
terest
suc
h
a
result
ma
y
in
v
ok
e,
w
e
hop
e
the
result
ma
y
shed
some
ligh
t
as
to
wh
y
linear
functions
are
go
o
d
candidates
for
P
C
P
pro
of
systems.
When
lo
oking
for
a
P
C
P
pro
of
system
it
is
clear
that
the
pro
ofs
m
ust
b
e
robust
in
the
sense
that
giv
en
a
go
o
d
pro
of

a
small
c
hange
eecting
a
constan
t
n
um
b
er
of
bits
m
ust
yield
another
go
o
d
pro
of


,
this
is
so
since
the
probabilit
y
of
the
v
erier's
to
detect
suc
h
a
small
c
hange
is
negligible.
This
means
that


m
ust
carry
\the
same
information"
as

(at
least
with
resp
ect
to
the
information
the
v
erier
can
\read"
from
it).
This
form
ulation
has
a
v
ery
strong
scen
t
of
error
correcting
co
des,
w
e
do
kno
w
that
error
correcting
co
des
ha
v
e
this
prop
ert
y
.
In
the
case
of
a
P
C
P
pro
of
system
for
a
language
in
N
P
w
e
ev
en
ha
v
e
a
natural
candidate
for
the
information
to
b
e
co
ded
namely
the
witness.
One
should
note
that
our
discussion
is
just
a
plausibilit
y
argumen
t
as
there
are
ma
jor
dierences
b
et
w
een
a
P
C
P
v
erier
and
an
error
correcting
deco
der.
On
one
hand
the
P
C
P
v
erier
do
es
not
ha
v
e
to
deco
de
at
all.
The
v
erier's
task
is
to
b
e
con
vinced
that
there
exists
an
information
w
ord
with
some
desired
prop
erties
(the
witness).
On
the
other
hand
the
v
erier
is
signican
tly
w
eak
er
(computationally)
then
an
ecien
t
error
correcting
deco
der,
since
it
ma
y
only
lo
ok
at
part
of
the
co
de
w
ord
(pro
of
).
If
the
reader
is
con
vinced
that
the
error
correcting
approac
h
is
a
go
o
d
one
to
b
egin
with,
note
that
in
addition
to
b
eing
an
error
correcting
co
de
linear
functions
ha
v
e
another
desired
prop
ert
y
:
w
e
can
use
self
correction
in
a
natural
w
a
y
since
L(x)
=
L(x
+
r
)
 L(r
).
Prop
osition
..
If
f
;
g
:
f0;
g
m
!
f0;
g
ar
e
b
oth
line
ar
and
f
=
g
then
the
distanc
e
of
f
fr
om
g
is
exactly


Pro
of:
Note
that
(f
 g
)
is
also
linear,
clearly
f
(x)
=
g
(x
)
i
(f
 g
)(x
)
=
0.
All
w
e
ha
v
e
left
to
pro
v
e
is
that
for
ev
ery
linear
function
h
=
0
it
holds
that
Pr
x
[h(x
)
=
0]
=


.
Denote
h(x)
=
m
P
i=
a
h
i
x
i
.
Since
h
=
0
there
exists
an
a
h
i
that
do
es
not
equal
0.
Assume
the
bits
of
x
are
c
hosen
one
b
y
one
(x

is
c
hosen
b
efore
x

and
so
on),
denote
b
y
l
the
last
i
for
whic
h
a
h
i
is
not
0
(i.e.
a
l
=

and
for
all
j
>
l
a
j
=
0).
Clearly
h(x
)
=
(
l
 
P
i=
a
h
i
x
i
)
+
x
l
.
If
l
 
P
i=
a
h
i
x
i
=
0
then
with
probabilit
y


w
e
c
ho
ose
x
l
=

and
w
e
get
h(x
)
=
.
If
on
the
other
hand
l
 
P
i=
a
h
i
x
i
=

then
with
probabilit
y


w
e
c
ho
ose
x
l
=
0
and
again
w
e
get
h(x
)
=
.

0
LECTURE
.
NP
IN
PCP[POL
Y,O()]
Corollary
.
If
a
function
f
:
f0;
g
m
!
f0;
g
is
at
distanc
e
less
than


fr
om
line
ar
then
ther
e
exists
a
unique
line
ar
function
L
f
s.t.
f
is
at
distanc
e
less
then


fr
om
L
f
.
Pro
of:
Otherwise
b
y
triangle
inequalit
y
w
e
w
ould
get
t
w
o
linear
functions
at
distance
smaller
than


App
endix
B:
The
linearit
y
test
for
functions
far
from
linear
Recall
that
our
ob
jectiv
e
is
to
b
ound
the
probabilit
y
of
failure
of
one
iteration
of
the
basic
pro
cedure
T
in
case
of
functions
far
from
linear.
The
result
w
e
need
is
that
giv
en
a
function
f
:
f0;
g
m
!
f0;
g
whic
h
is
at
distance
at
least


from
linear
the
probabilit
y
that
f
(x
+
y
)
=
f
(x)
+
f
(y
)
for
randomly
c
hosen
x;
y
is
bigger
than
some
constan
t
c.
Denote
b
y

the
probabilit
y
of
failing
one
iteration
of
T
(i.e.

def
=
Pr
x;y
[f
(x
+
y
)
=
f
(x)
+
f
(y
)]).
In
tuitiv
ely
if
the
probabilit
y
of
c
ho
osing
x
and
y
s.t.
f
(x)
+
f
(y
)
=
f
(x
+
y
),
is
v
ery
big
than
there
m
ust
b
e
some
linear
function
L
whic
h
agree
with
f
on
\a
lot"
of
p
oin
ts
so
f
cannot
b
e
\to
o
far"
from
linear.
The
problem
is
that
it
is
not
clear
ho
w
to
nd
this
L
or
ev
en
those
p
oin
ts
on
whic
h
L
and
f
agree.
Since
a
linear
function
b
eha
vior
on
the
en
tire
space
f0;
g
n
is
xed
b
y
its
b
eha
vior
on
an
y
n
indep
enden
t
v
ectors,
w
e
should
someho
w
see
ho
w
\most
of
the
p
oin
ts
w
ould
lik
e
f
to
b
eha
v
e".
F
ormalizing
this
in
tuition
is
not
as
hard
as
it
ma
y
seem.
Giv
en
a
p
oin
t
a
w
e
w
ould
lik
e
to
nd
ho
w
\most
of
the
p
oin
ts
w
an
t
f
to
b
eha
v
e
on
a",
the
natural
w
a
y
to
dene
that,
is
that
the
p
oin
t
x
\w
ould
lik
e"
f
(a)
to
tak
e
a
v
alue
that
w
ould
mak
e
f
(a)
=
f
(x
+
a)
 f
(x)
true.
Denition
.
(L
f
()):
F
or
every
a

f0;
g
n
dene
L
f
(a)
to
b
e
either
0
or

s.t.
Pr
x
R
f0;g
n
[L
f
(a)
=
f
(x
+
a)
 f
(x)]



(In
c
ases
wer
e
L
f
(a)
might
b
e
either
0
or

(pr
ob
ability


)
dene
L
f
(a)
arbitr
arily.)
What
w
e
w
ould
lik
e
to
see
no
w
is
that
L
f
is
indeed
linear,
and
that
L
f
is
reasonably
close
to
f
(dep
ending
on

the
probabilit
y
to
fail
the
basic
pro
cedure
T
).
W
e
w
ould
indeed
pro
v
e
those
claims
but
b
efore
em
barking
on
that
task
w
e
ha
v
e
to
pro
v
e
a
tec
hnical
result.
By
denition
Pr
x;a
[f
(x)
+
f
(x
+
a)
=
L
f
(a)]


 
therefor
b
y
an
a
v
eraging
argumen
t
L
f
(a)
b
eha
v
es
nice
\on
the
a
v
erage
a"
(i.e.
for
most
a's
Pr
x
[f
(x)
+
f
(x
+
a)
=
L
f
(a)]


 
).
Ho
w
ev
er
there
migh
t
ha
v
e
b
een
a
few
bad
p
oin
ts
b
on
whic
h
Pr
x
[f
(x)
+
f
(x
+
b)
=
L
f
(b)]
=


.
W
e
w
ould
lik
e
to
sho
w
L
f
b
eha
v
es
nicely
on
all
the
p
oin
ts.
Claim
..
F
or
al
l
p
oints
a

f0;
g
n
it
holds
that:
Pr
x
R
f0;g
n
[f
(x)
+
f
(x
+
a)
=
L
f
(a)]


 
.
Pro
of:
Lo
ok
at
t
w
o
p
oin
ts
x;
y
c
hosen
at
random,
what
is
the
probabilit
y
that
\they
v
ote
the
same
for
L
f
(a)"
i.e.
f
(x)
+
f
(x
+
a)
=
f
(y
)
+
f
(y
+
a)
?
Clearly
if
w
e
denote
p
=
Pr
x
[f
(x)
+
f
(x
+
a)
=
L
f
(a)]
then
Pr
x;y
[f
(x)
+
f
(x
+
a)
=
f
(y
)
+
f
(y
+
a)]
=
p

+
(
 p)

(x
and
y
migh
t
b
oth
go
with
L
f
(a)
or
against
it).
F
rom
Another
p
ersp
ectiv
e:
Pr
x;y
[f
(x)
+
f
(x
+
a)
=
f
(y
)
+
f
(y
+
a)]
=
=
Pr
x;y
[f
(x)
+
f
(x
+
a)
+
f
(y
)
+
f
(y
+
a)
=
0]
=
=
Pr
x;y
[f
(x)
+
f
(y
+
a)
+
f
(x
+
y
+
a)
+
f
(y
)
+
f
(x
+
a)
+
f
(x
+
y
+
a)
=
0]


Pr
x;y
[(f
(x)
+
f
(y
+
a)
+
f
(x
+
y
+
a)
=
0)
^
(f
(y
)
+
f
(x
+
a)
+
f
(x
+
y
+
a)
=
0)]
=

..
DISTINGUISHING
A
NICE
ORA
CLE
FR
OM
A
VER
Y
UGL
Y
ONE

=

 Pr
x;y
[(f
(x)
+
f
(y
+
a)
+
f
(x
+
y
+
a)
=
0)
_
(f
(y
)
+
f
(x
+
a)
+
f
(x
+
y
+
a)
=
0)]



 
The
last
inequalit
y
is
true
since
all
x;
y
;
a
+
x;
a
+
y
are
uniformly
distributed
(also
dep
enden
t),
from
the
denition
of

and
using
the
union
b
ound.
W
e
got
that
p

+
(
 p)



 
.
Simple
manipulation
brings
us
to
:

 p
+
p



 
(
)
p(p
 )

 
(
)
p(
 p)


.
Note
that
since
L
f
(a)
v
alue
(0
or
)
w
as
dened
to
maximize
p
=
Pr
[f
(x)
+
f
(x
+
a)
=
L
f
(a)],
p
is
alw
a
ys
bigger
or
equal
to


.
So


(
 p)

p(
 p)


and
from
that
p


 
follo
ws.
Claim
..
If
the
failur
e
pr
ob
ability

is
smal
ler
then


then
the
function
L
f
()
is
line
ar.
Pro
of:
Giv
en
an
y
a;
b

f0;
g
n
w
e
ha
v
e
to
pro
v
e
L
f
(a
+
b)
=
L
f
(a)
+
L
f
(b).
The
strategy
here
is
simple,
w
e
will
pro
v
e
(b
y
the
probabilistic
metho
d)
the
existence
of
\go
o
d
in
termediate"
p
oin
ts
in
the
sense
that
L
f
b
eha
v
es
nicely
on
these
p
oin
ts
and
this
forces
L
f
to
b
e
linear.
Supp
ose
there
exists
x;
y
s.t.
the
follo
wing
ev
en
ts
happ
en
sim
ultaneously
:
E
:
L
f
(a
+
b)
=
f
(a
+
b
+
x
+
y
)
+
f
(x
+
y
).
E
:
L
f
(b)
=
f
(a
+
b
+
x
+
y
)
+
f
(a
+
x
+
y
).
E
:
L
f
(a)
=
f
(a
+
x
+
y
)
+
f
(x
+
y
).
Then
:
L
f
(a
+
b)
=
f
(a
+
b
+
x
+
y
)
+
f
(x
+
y
)
=
L
f
(b)
+
f
(a
+
x
+
y
)
+
f
(x
+
y
)
=
L
f
(b)
+
L
f
(a)
.
T
o
pro
v
e
the
existence
of
these
p
oin
ts
c
ho
ose
x;
y
at
random.
By
Claim
.
the
probabilit
y
that
eac
h
of
the
ev
en
ts
E,
E,
E
will
not
happ
en
is
smaller
or
equal

,
so
the
probabilit
y
that
one
of
these
will
not
happ
en
is
smaller
or
equal
to

.
Since

<


the
probabilit
y
that
some
of
those
ev
en
ts
do
not
happ
en
is
smaller
then
,
i.e.
there
exists
x
and
y
for
whic
h
the
ev
en
ts
E,
E,
E
happ
en
and
therefor
L
f
is
linear
with
regard
to
a;
b.
Claim
..
The
function
L
f
()
is
at
most
at
distanc
e

fr
om
f
.
Pro
of:
Basicly
w
e
sho
w
that
since
L
f
is
dened
as
\ho
w
f
should
b
eha
v
e
if
it
w
an
ts
to
b
e
linear"
then
if
f
(a)
=
L
f
(a)
then
\f
is
not
linear
on
a"
since
f
is
close
to
linear
it
m
ust
b
e
close
to
L
f
.
Denote
b
y
p
the
probabilit
y
that
f
agrees
with
L
f
i.e.
Pr
x
[f
(x)
=
L
f
(x)]
=
p.
(Notice
that
the
distance
b
et
w
een
f
and
L
f
equals

 p
b
y
denition).
Cho
ose
t
w
o
v
ectors
x;
y
and
denote
b
y
E
the
ev
en
t
\f
(x
+
y
)
=
L
f
(x
+
y
)"
and
b
y
F
the
ev
en
t
\f
(x)
+
f
(y
)
=
f
(x
+
y
)".
Pr
x;y
[F
^
E
c
]
=
Pr
x;y
[(f
(x)
+
f
(y
)
=
f
(x
+
y
))
^
(f
(x
+
y
)
=
L
f
(x
+
y
))]
=
=
Pr
x;y
[(f
(x
+
y
+
y
)
+
f
(y
)
=
f
(x
+
y
))
^
(f
(x
+
y
)
=
L
f
(x
+
y
))]
Clearly
whenev
er
f
(x
+
y
+
y
)
+
f
(y
)
=
f
(x
+
y
)
and
f
(x
+
y
)
=
L
f
(x
+
y
)
it
holds
that
f
(x
+
y
+
y
)
+
f
(y
)
=
L
f
(x
+
y
)
therefor
:
Pr
x;y
[F
^
E
c
]

Pr
x;y
[f
((x
+
y
)
+
y
)
+
f
(y
)
=
L
f
(x
+
y
)]


Where
the
last
inequalit
y
is
b
y
claim
.
No
w
notice
Pr
[F
]
=
(
 
)
and
Pr
[E
]
=
p
b
y
denition.
Lo
oking
at
Pr
[F
]
from
another
p
ersp
ectiv
e:
Pr
[F
]
=
Pr
[F
^
E
]
+
Pr[F
^
E
c
]

Pr
[E
]
+
Pr[F
^
E
c
]

p
+



LECTURE
.
NP
IN
PCP[POL
Y,O()]
Comparing
the
t
w
o
ev
aluations
of
Pr[F
]
w
e
get

 

p
+

i.e.
p


 
.
Since
the
distance
b
et
w
een
f
and
L
f
is

 p
our
result
is
that
f
's
distance
from
L
f
is
smaller
or
equal
to

T
o
conclude
if

,
the
probabilit
y
to
fail
T
,
is
smaller
then


then
f
is
at
distance
at
most


from
linear
and
this
means
:
Corollary
.
If
f
is
at
distanc
e
bigger
than


fr
om
line
ar
then
the
pr
ob
ability
to
fail
one
iter
ation
of
the
b
asic
pr
o
c
e
dur
e
T
is
bigger
or
e
qual
to


.
Oded's
Note:
The
ab
o
v
e
analysis
is
not
the
b
est
p
ossible.
One
ma
y
sho
w
that
if
f
is
at
distance
bigger
than
=
from
linear
then
T
rejects
with
probabilit
y
at
least
=	.

Lecture
	
Dtime
vs
Dspace
Notes
tak
en
b
y
T
amar
Seeman
and
Reub
en
Sumner
Summary:
In
this
lecture,
w
e
pro
v
e
that
D
time(t())

D
space(t()=
log
t()).
That
is,
w
e
sho
w
ho
w
to
sim
ulate
an
y
giv
en
deterministic
m
ulti-tap
e
T
uring
Mac
hine
(TM)
of
time
complexit
y
t,
using
a
deterministic
TM
of
space
complexit
y
t=
log
t.
A
main
ingredian
t
in
the
sim
ulation
is
the
analysis
of
a
p
ebble
game
on
directed
b
ounded-degree
graphs.
	.
In
tro
duction
W
e
b
egin
b
y
dening
Dtime(t)
and
Dspace(t).
Denition
	.
(Dtime):
D
time(t())
is
the
set
of
languages
L
for
which
ther
e
exists
a
multi-tap
e
deterministic
TM
M
which
de
cides
whether
or
not
x

L
in
less
than
t(jxj)
T
uring
machine
steps.
Denition
	.
(Dspace):
D
space(s())
is
the
set
of
languages
L
for
which
ther
e
exists
a
multi-
tap
e
deterministic
TM
which
de
cides
whether
or
not
x

L
while
never
going
to
the
right
of
c
el
l
s(jxj)
on
any
of
its
tap
es.
Since
an
y
T
uring
mac
hine
step
can
mo
v
e
the
head(s)
b
y
at
most
one
p
osition,
it
is
immediately
clear
that
D
time(t())

D
space(t()).
F
urthermore
NP
is
easily
in
D
space(p())
for
some
p
olyno-
mial
p,
but
is
not
b
eliev
ed
to
b
e
in
D
time(q
())
for
an
y
p
olynomial
q
.
Th
us
it
seems
that
space
is
m
uc
h
more
p
o
w
erful
than
time.
In
this
lecture
w
e
will
further
rene
this
in
tuition
b
y
pro
ving
that
D
time(t())

D
space(t()=
log
t()).
It
follo
ws
that
D
time(t())
is
a
strict
subset
of
D
space(t()),
since
it
has
already
b
een
sho
wn
that
D
space(o(t))
is
a
strict
subset
of
D
space(t).
Note:
The
m
ulti-tap
e
TM
consists
of
one
read-only
,
bi-directional
input
tap
e,
one
optional
write-only
output
tap
e
(in
the
case
of
a
mac
hine
to
decide
a
language
w
e
will
not
include
suc
h
a
tap
e
and
determine
acceptance
based
on
the
exact
state
that
the
T
uring
mac
hine
terminates
in)
together
with
a
xed
n
um
b
er
of
bi-directional
read/write
w
ork
tap
es.
The
n
um
b
er
of
w
ork
tap
es
is
irrelev
an
t
for
D
space(),
though,
since
a
TM
M
with
k
w
ork
tap
es
can
b
e
sim
ulated
b
y
a
TM
M
0
with
just
a
single
w
ork
tap
e
and
the
same
amoun
t
of
space.
This
is
done
b
y
transforming
k
w
ork
tap
es
in
to
one
w
ork
tap
e
with
k
trac
ks.
This
transformation
then
sim
ulates
the
w
ork
of
the
original
T
uring
mac
hine
b
y
using
p
olynomially
more
time,
but
the
same
amoun
t
of
space.
Ho
w
ev
er,
in
the
case
of
D
time()
the
n
um
b
er
of
tap
es
do
es
matter.



LECTURE
	.
DTIME
VS
DSP
A
CE
	.
Main
Result
Theorem
	.
If
t()
is
c
onstructible
in
sp
ac
e
t()=
log
t()
and
t()
is
at
le
ast
line
ar,
then
D
time(t())

D
space(t()=
log
t()).
In
order
to
mak
e
the
pro
of
of
this
theorem
as
readable
as
p
ossible
w
e
state
some
results
(without
pro
of
)
in
place
they
are
needed
so
that
their
motiv
ation
is
clear,
but
w
e
pro
v
e
them
only
later
so
as
not
to
disturb
the
o
w
of
the
pro
of.
Pro
of:
Let
L
b
e
a
language
accepted
b
y
a
TM
M
in
D
time(t()).
Let
x
b
e
an
input
and
t
=
t(jxj).
Divide
eac
h
tap
e
in
to
t
=
blo
c
ks
of
t
=
cells.
(This
ensures
that
(#
blo
c
ks)


t,
a
necessary
feature.)
Similarly
,
partition
time
in
to
p
erio
ds
of
t
=
steps.
During
a
particular
p
erio
d,
M
visits
at
most
t
w
o
blo
c
ks
of
space
on
eac
h
tap
e,
since
the
n
um
b
er
of
steps
M
can
tak
e
do
es
not
exceed
the
size
of
a
single
blo
c
k.
Lemma
	..
(Canonical
Computation
Lemma):
Without
loss
of
gener
ality
such
a
machine
moves
b
etwe
en
blo
ck
b
oundaries
only
on
the
very
last
move
of
a
time
p
erio
d.
This
holds
with
at
most
a
line
ar
incr
e
ase
in
the
amount
of
time
use
d,
and
a
c
onstant
numb
er
of
additional
tap
es.
The
pro
of
is
p
ostp
oned
to
the
next
section.
Therefore
without
loss
of
generalit
y
w
e
assume
that
in
eac
h
time
p
erio
d
our
mac
hine
sta
ys
within
the
same
blo
c
k
on
eac
h
w
ork
tap
e.
Our
goal
no
w
is
to
compute
the
nal
state
of
the
mac
hine,
whic
h
will
indicate
whether
or
not
our
input
is
in
the
language.
W
e
therefore
ha
v
e
to
someho
w
construct
the
blo
c
ks
that
w
e
are
in
at
the
nal
time
p
erio
d
while
nev
er
storing
the
complete
state
of
the
w
ork
tap
es
of
the
mac
hine
since
these
w
ould
p
oten
tially
exceed
our
space
b
ound.
T
o
do
so,
w
e
establish
a
dep
endency
graph
b
et
w
een
dieren
t
states
and
nd
a
sucien
tly
ecien
t
metho
d
of
ev
aluating
the
graph
no
de
corresp
onding
to
the
nal
mac
hine
state.
W
e
in
tro
duce
some
notation
to
describ
e
the
computation
of
the
mac
hine.
h
i
(j
)
is
the
blo
c
k
lo
cation
of
the
i
th
tap
e
head
during
the
j
th
p
erio
d.
h
i
(j
)

n
;
:
:
:
;
t
=
o
.
h(j
)
is
the
v
ector
h

(j
);
h

(j
);
:
:
:
;
h
k
(j
)
for
a
k
-tap
e
mac
hine.
c
i
(j
)
is
the
con
ten
t
of
blo
c
k
h
i
(j
)
on
the
i
th
tap
e,
together
with
the
T
uring
mac
hine
state
at
the
end
of
the
p
erio
d
and
head
p
osition
on
tap
e
i.
c
i
(j
)

f0;
g
t
=

O
()

f0;
g
O
(log
t)
.

c(j
)
is
the
v
ector
c

(j
);
c

(j
);
:
:
:
;
c
k
(j
).
l
i
(j
)
is
the
last
p
erio
d
where
w
e
visited
blo
c
k
h
i
(j
)
on
tap
e
i.
This
is
max
fj
0
<
j
suc
h
that
h
i
(j
0
)
=
h
i
(j
)g.
In
order
to
compute
c(j
)
w
e
will
need
to
kno
w:

c(j
 )
to
determine
what
state
to
start
our
computation
in.

c

(l

(j
));
:
:
:
;
c
k
(l
k
(j
))
so
that
w
e
kno
w
the
starting
con
ten
t
of
the
blo
c
ks
w
e
are
w
orking
with.

W
e
assume
that

=
f0;
g,
where

is
the
tap
e
alphab
et

	..
MAIN
RESUL
T

Figure
	.:
An
in
teresting
p
ebbling
problem
It
is
immediately
clear
then
that
w
e
need
at
most
k
+

blo
c
ks
from
past
stages
of
computation
to
compute
c(j
).
Dene
a
directed
graph
G
=
(V
;
E
)
where
V
=
f;
:
:
:
;
t
=
g
and
E
=
f(j
 ;
j
)jj
>
g[
f(l
i
(j
);
j
)g.
So
v
ertex
i
represen
ts
kno
wledge
of
c(i).
There
is
an
edge
(j
 )
!
j
since
c(j
)
dep
ends
on
c(j
 ).
Similarly
there
is
an
edge
l
i
(j
)
!
j
since
c(j
)
dep
ends
on
c
i
(l
i
(j
))
for
all
i.
Hence
this
graph
rep-
resen
ts
exactly
the
dep
endency
relationship
that
w
e
ha
v
e
just
describ
ed.
Consider
the
example
in
Figure
	..
The
most
ob
vious
w
a
y
to
reac
h
no
de

w
ould
b
e

calculate
state

from
the
starting
state

calculate
state

from
state

and
erase
state


calculate
state

from
state

and
k
eep
state


calculate
state

from
state

and
k
eep
state

as
w
ell
as
still
k
eeping
state


calculate
state

from
states

and

and
erase
states

and


calculate
state

from
states

and

and
erase
states

and

Notice
that
when
calculating
state

w
e
had
in
memory
states
,
and
,
a
total
of
three
prior
states.
W
e
can
do
b
etter.

calculate
state

from
the
starting
state

calculate
state

from
state

and
erase
state


calculate
state

from
state

and
er
ase
state


calculate
state

from
state

and
k
eep
state


calculate
state

from
states

and

and
then
erase
b
oth
states

and


calculate
state

from
the
starting
state
(for
the
second
time!)

calculate
state

from
state

and
erase
state


calculate
state

from
states

and

and
then
erase
b
oth
This
second
calculation
required
t
w
o
more
steps
to
compute
but
no
w
w
e
nev
er
needed
to
remem
b
er
more
than
t
w
o
previous
states,
rather
than
three.
It
is
exactly
this
tradeo
of
time
v
ersus
space
whic
h
enables
us
to
ac
hiev
e
our
goal.
In
general
on
a
directed
acyclic
graph
of
b
ounded
degree
w
e
can
pla
y
a
p
ebble
game.
The
rules
of
the
game
are
simple:
.
A
p
ebble
ma
y
b
e
placed
on
an
y
no
de
all
of
whose
paren
ts
ha
v
e
p
ebbles.
.
A
p
ebble
ma
y
b
e
remo
v
ed
from
an
y
no
de.
.
A
p
ebble
ma
y
b
e
placed
on
an
y
no
de
ha
ving
no
paren
ts.
This
is
a
sp
ecial
case
of
the
rst
rule.


LECTURE
	.
DTIME
VS
DSP
A
CE
The
goal
of
the
p
ebble
game
is
to
p
ebble
a
giv
en
target
no
de
while
minimizing
the
n
um
b
er
of
p
ebbles
used
at
an
y
one
time.
This
game
corresp
onds
to
calculations
of
our
T
uring
mac
hine.
A
p
ebble
on
v
ertex
j
in
the
game
corresp
onds
to
a
sa
v
ed
v
alue
for
c(j
).
When
the
game
tells
us
to
p
ebble
no
de
j
then
w
e
can
reco
v
er
the
con
ten
ts
of
c(j
)
as
follo
ws:
.
Load
con
ten
ts
of
c

(l

(j
));
:
:
:
;
c
k
(l
k
(j
))
from
storage.
Since
there
w
as
an
edge
from
l
i
(j
)
!
j
in
the
graph,
Rule

guaran
tees
that
no
de
l
i
(j
)
has
a
p
ebble
on
it.
Since
l
i
(j
)
has
a
p
ebble
in
the
p
ebble
game,
then
in
our
computation
w
e
ha
v
e
c(l
i
(j
)
and
therefore
c
i
(l
i
(j
))
in
storage.
.
Load
the
starting
state
and
head
p
osition
for
p
erio
d
j
from
c(j
 ),
again
guaran
teed
to
b
e
a
v
ailable
b
y
the
p
ebble
game.
.
Based
on
all
the
ab
o
v
e,
it
is
easy
to
reconstruct
c(j
).
Note
that
in
order
to
determine
the
answ
er
of
the
original
T
uring
mac
hine,
it
suces
to
reconstruct
c(t
=
).
Our
aim
is
to
do
so
using
O
(t=
log
t)
space,
whic
h
can
b
e
reduced
to
p
ebbling
the
target
no
de
t
=
using
t
=
=
log
t
p
ebbles.
W
e
rst
state
the
follo
wing
general
result
regarding
p
ebbling:
Theorem
	.
(P
ebble
Game
Theorem):
F
or
any
xe
d
d,
any
dir
e
cte
d
acyclic
gr
aph
G
=
(V
;
E
)
with
in-de
gr
e
e
b
ound
d,
and
any
v

V
,
we
c
an
p
ebble
v
while
never
using
mor
e
than
O
(jV
j=
log
jV
j)
p
ebbles
simultane
ously.
The
pro
of
is
p
ostp
oned
to
the
next
section.
Notice,
ho
w
ev
er,
that
the
theorem
do
es
not
state
an
ything
ab
out
ho
w
ecien
t
it
is
to
compute
this
p
ebbling.
Using
the
ab
o
v
e
result,
w
e
sim
ulate
the
mac
hine
M
on
input
x
as
follo
ws.
.
Compute
t
=
t(jxj)
.
Lo
op
o
v
er
all
p
ossible
guesses
of
h();
:
:
:
;
h(t
=
).
F
or
eac
h
one
do:
(a)
Compute
and
store
the
dep
endency
graph.
(b)
Execute
a
p
ebbling
strategy
to
reac
h
no
de
t
=
on
the
graph
as
follo
ws:
i.
Determine
next
p
ebble
mo
v
e
ii.
if
the
mo
v
e
is
\Remo
v
e
i"
then
erase
c(i)
iii.
if
the
mo
v
e
is
\P
ebble
i"
then
set
the
mac
hine
to
the
initial
state
for
p
erio
d
i
b
y
loading
the
con
ten
ts
of
the
w
ork
tap
e
from
storage.
Calculate
c(i).
V
erify
that
in
step

w
e
did
guess
h(i
+
)
correctly
.
If
not,
then
ab
ort
the
whole
calculation
so
far
and
return
to
the
next
guess
in
step
.
Otherwise
sa
v
e
c(i)
for
future
use.
(c)
Ha
ving
just
executed
\P
ebble
t
=
",
terminate
and
accept
or
reject
according
to
the
original
T
uring
mac
hine.
W
e
need
to
sho
w
that
all
of
the
ab
o
v
e
computation
can
b
e
p
erformed
within
our
space
b
ound.

Step

can
b
e
p
erformed
within
our
space
b
ound
b
y
our
h
yp
othesis
regarding
the
(space
constructabilit
y
of
the)
function
t().

In
step
a
w
e
store
a
graph
G
=
(V
;
E
)
where
jV
j
=
t
=
and
jE
j

jV
j

(k
+
).
A
simple
list
represen
tation
of
the
graph
will
therefore
tak
e
(k
+
)

t
=

log

(t
=
)

t=
log
t
space.

	..
ADDITIONAL
PR
OOFS

R
ev
(b
i
(j
 ))
R
ev
(b
i
(j
 ))
R
ev
(b
i
(j
))
:
:
:
b
i
(j
 )
b
i
(j
)
b
i
(j
+
)
:
:
:
R
ev
(b
i
(j
))
R
ev
(b
i
(j
+
))
R
ev
(b
i
(j
+
))
Figure
	.:
The
new
w
ork
tap
e

Step
(b)ii
actually
frees
space
rather
than
using
space.

Step
(b)iii
needs
space
only
for
k
blo
c
ks
copied
from
storage,
and
space
to
store
the
result.
Since
our
p
ebble
game
guaran
tees
that
w
e
will
nev
er
need
more
than
t
=
=
log
t
=
p
ebbles,
w
e
will
nev
er
need
more
than
(t
=
=
log
t
=
)

(k
+
)

t
=
=
O
(t=
log
t)
space,
our
space
b
ound.
So
aside
from
step
(b)i
all
of
step
b
can
b
e
calculated
within
our
space
b
ound.
W
e
need
to
describ
e
no
w
ho
w
to
p
erform
step
(b)i.
Consider
a
non-deterministic
algorithm.
W
e
set
a
coun
ter
to
b
e
the
maxim
um
n
um
b
er
of
p
ossible
p
ebble
mo
v
es
to
reac
h
our
target.
Since
it
nev
er
mak
es
sense
to
return
to
an
earlier
p
ebbling
conguration
of
the
graph,
w
e
can
b
ound
the
n
um
b
er
of
steps
b
y

t
=
.
Suc
h
a
coun
ter
will
only
require
t
=
space,
whic
h
is
within
our
b
ound.
W
e
no
w
non-deterministically
mak
e
our
c
hoice
of
v
alid
p
ebbling
mo
v
e
and
decremen
t
the
coun
ter.
W
e
accept
if
w
e
p
ebble
the
target
v
ertex,
reject
if
the
coun
ter
reac
hes
its
limit
rst.
The
dominan
t
space
required
b
y
the
routine
is
therefore
the
space
required
to
store
a
w
orking
cop
y
of
the
graph,
O
(t
=

log
t).
In
Lecture

w
e
pro
v
ed
that
N
space(s)

D
space(s

).
Therefore,
the
ab
o
v
e
non-deterministic
subprogram
whic
h
runs
in
N
space(t
=

log
t)
can
b
e
con
v
erted
to
a
deterministic
subprogram
running
in
D
space(t
=

log

t

t=
log
t).
	.
Additional
Pro
ofs
W
e
no
w
pro
v
e
t
w
o
results
stated
without
pro
of
in
the
previous
section.
	..
Pro
of
of
Lemma
	..
(Canonical
Computation
Lemma)
Our
new
mac
hine
M
0
whic
h
sim
ulates
the
op
eration
of
M
w
orks
as
follo
ws.
Firstly
eac
h
w
ork
tap
e
is
replaced
b
y
a
three-trac
k
w
ork
tap
e.
Three
additional
w
ork
tap
es
are
added:

one
uses
a
unary
alphab
et
and
is
used
as
a
coun
ter

the
second
is
a
k
-trac
k
\cop
y"
tap
e

the
last
is
a
k
-trac
k
binary
\p
osition"
tap
e
After
calculating
the
blo
c
k
b
ound
t
=
,
store
it
in
unary
on
the
coun
ter
tap
e.
No
w
mark
the
w
ork
tap
es
in
to
blo
c
ks
of
length
t
=
b
y
putting
a
sp
ecial
end-of-blo
c
k
mark
er
where
needed
(eac
h
blo
c
k
will
th
us
b
e
one
cell
larger
to
accommo
date
the
end
of
cell
mark
er).
Let
b
i
(j
)
and
b
0
i
(j
)
denote
the
con
ten
ts
of
the
j
th
blo
c
k
of
tap
e
i
in
the
original
mac
hine
and
new
mac
hine
resp
ectiv
ely
.
Then
b
0
i
(j
)
is
the
tuple
(R
ev
er
se(b
i
(j
 ));
b
i
(j
);
R
ev
er
se(b
i
(j
+
))).
Here
R
ev
er
se
means
rev
ersing
the
order
of
the
tap
e
cells.
When
sim
ulating
the
computation
w
e
start
b
y
w
orking
on
the
middle
trac
k.
If
w
e
see
the
end-of-blo
c
k
mark
er
when
going
left,
then
w
e
start
using
the
rst
trac
k
instead
and
rev
erse
the
directions
of
eac
h
head
mo
v
e
(on
this
trac
k).
Similarly
,
if
w
e
go
o
the
end
of
the
blo
c
k


LECTURE
	.
DTIME
VS
DSP
A
CE
while
going
righ
t
w
e
switc
h
to
using
the
last
trac
k
and
again
rev
erse
the
direction
of
head
mo
v
es.
Throughout
w
e
k
eep
mo
ving
the
head
on
the
coun
ter
tap
e
un
til
it
indicates
that
w
e
ha
v
e
reac
hed
the
end
of
the
p
erio
d.
A
t
the
end
of
the
p
erio
d
w
e
ha
v
e
to
do
some
housek
eeping.
First
w
e
sa
v
e
the
state,
either
within
the
state
itself
or
on
a
sp
ecial
tap
e
somewhere;
either
w
a
y
this
is
only
a
constan
t
amoun
t
of
information.
W
e
store
the
head
p
osition
within
eac
h
blo
c
k
on
the
\p
osition"
tap
e
as
follo
ws.
Scan
left
along
all
w
ork
tap
es
not
mo
ving
past
the
b
eginning
of
the
blo
c
k.
F
or
all
w
ork
tap
es
not
at
the
b
eginning
of
the
blo
c
k
write
a

in
the
corresp
onding
trac
k
of
the
\p
osition"
tap
e,
and
for
all
others
write
a
0.
Con
tin
ue
in
this
w
a
y
un
til
ev
ery
head
p
osition
is
at
the
start
of
blo
c
k
for
all
the
w
ork
tap
es;
the
n
um
b
er
of
's
on
trac
k
i
of
the
\p
osition"
tap
e
will
th
us
equal
the
head
p
osition,
within
the
blo
c
k,
on
w
ork
tap
e
i.
This
tak
es
one
additional
time
p
erio
d.
Consider
Figure
	..
A
t
this
p
oin
t
only
the
cen
ter
blo
c
k
is
up
to
date;
the
v
alues
for
b
i
(j
 )
and
b
i
(j
)
in
the
left
blo
c
k
are
`stale'
since
w
e
ma
y
ha
v
e
mo
died
them
in
our
last
time
p
erio
d.
Similarly
the
v
alues
for
b
i
(j
)
and
b
i
(j
+
)
in
the
righ
t
blo
c
k
are
also
`stale'.
W
e
up
date
these
stale
v
alues
b
y
scanning
bac
k
and
forth
o
v
er
all
three
cells
and
using
the
\cop
y"
tap
e
as
a
temp
orary
area
to
store
the
con
ten
ts
of
one
blo
c
k
(all
three
trac
ks)
at
a
time.
Altogether
this
pro
cess
requires
ab
out

time
p
erio
ds.
Finally
w
e
return
the
heads
to
their
correct
p
ositions,
but
this
time
if
they
w
ere
w
orking
on
the
rst
trac
k
of
the
w
orking
blo
c
k
w
e
place
them
in
the
corresp
onding
cell
in
the
previous
blo
c
k,
and
if
they
w
ere
in
the
last
trac
k
then
w
e
place
them
in
the
next
blo
c
k.
This
ma
y
require
an
additional
time
p
erio
d
or
t
w
o.
Altogether
this
sim
ulation
of
a
single
time
p
erio
d
has
cost
us
a
little
bit
of
extra
space
(more
w
ork
tap
es)
and
a
constan
t
factor
increase
of
ab
out

in
the
n
um
b
er
of
p
erio
ds.
	..
Pro
of
of
Theorem
	.
(P
ebble
Game
Theorem)
Denote
b
y
R
d
(p)
the
minim
um
n
um
b
er
of
edges
in
a
di-graph
of
in-degree
b
ound
d
whic
h
requires
p
p
ebbles
(that
is,
at
least
one
no
de
in
the
graph
needs
p
p
ebbles).
W
e
will
rst
sho
w
that
R
d
(p)
=

(p
log
p)
and
then
that
this
fact
implies
our
theorem.
Consider
a
graph
G
=
(V
;
E
)
with
the
minimal
p
ossible
n
um
b
er
of
edges
requiring
p
p
ebbles.
Let
V

b
e
the
set
of
v
ertices
whic
h
can
b
e
p
ebbled
using
at
most
p=
p
ebbles.
Let
G

b
e
the
subgraph
induced
b
y
V

ha
ving
edge
set
E

=
E
\
(V


V

).
Let
V

=
V
 V

and
let
G

b
e
the
subgraph
induced
b
y
V

and
ha
ving
edge
set
E

=
E
\
(V


V

).
Let
A
=
E
 E

 E

b
e
all
edges
b
et
w
een
the
t
w
o
subgraphs.
Notice
that
there
cannot
b
e
an
y
edge
from
V

to
V

since
then
the
v
ertex
at
the
head
of
the
edge
(in
V

)
w
ould
require
at
least
as
man
y
p
ebbles
as
the
v
ertex
at
its
tail,
whic
h
is
more
than
p=.
There
exists
v

V

whic
h
requires
p=
 d
p
ebbles
in
G

Assume
not.
Then
w
e
sho
w
that
w
e
can
p
ebble
an
y
no
de
in
G
with
few
er
than
p
p
ebbles,
con-
tradicting
the
h
yp
othesis
regarding
G.
F
or
an
y
no
de
v

V

w
e
ha
v
e
assumed
that
in
G

w
e
can
p
ebble
it
with
<
p=
 d
p
ebbles.
In
v
ok
e
the
strategy
to
p
ebble
v
in
the
graph
G

on
the
original
graph
G.
Whenev
er
w
e
w
an
t
to
p
ebble
a
v
ertex
u

G

that
has
a
paren
t
in
G

,
bring
a
p
ebble
to
the
paren
t
in
G

,
using

p=
p
ebbles.
Since
some
u
migh
t
ha
v
e
as
man
y
as
d
paren
ts
in
G

,
w
e
actually
use
as
man
y
as
d
 
+
p=
p
ebbles
when
p
ebbling
the
d
th
paren
t
in
G

(using
d
 
p
ebbles
to
co
v
er
the
rst
d
 
paren
ts
and
p=
to
do
the
actual
p
ebbling).
Once
all
the
paren
ts
in
G

are
p
ebbled,
p
ebble
u
and
then
lift
the
p
ebbles
on
all
the
v
ertices
in
G

.
Th
us
w
e
can
p
ebble

	..
ADDITIONAL
PR
OOFS
	
our
target
v
using
at
most
(p=
 d)
+
(p=
+
d
 )
=
p
 
p
ebbles.
Since
w
e
can
also
p
ebble
an
y
v

V

with
at
most
p=
<
p
w
e
can
p
ebble
an
y
v

V
with
few
er
than
p
p
ebbles,
a
con
tradiction
to
our
original
c
hoice
of
G.
If
v
with
in-degree
k
requires
m
p
ebbles
then
it
has
a
paren
t
needing
m
 k
W
e
sho
w
that
in
general,
for
an
y
G
=
(V
;
E
)
where
G
is
an
acyclic
di-graph,
an
y
no
de
v

V
requiring
m
p
ebbles,
with
an
in-degree
of
k
,
has
a
paren
t
u
requiring
at
least
m
 k
+

p
ebbles.
Supp
ose
to
the
con
trary
that
eac
h
paren
t
needs
m
 k
or
few
er
p
ebbles.
Then
simply
bring
a
p
ebble
to
eac
h
of
them
in
arbitrary
order,
eac
h
time
remo
ving
all
p
ebbles
not
on
paren
ts
of
v
.
When
bringing
a
p
ebble
to
the
i
th
paren
t
w
e
ha
v
e
i
 
p
ebbles
co
v
ering
the
other
paren
ts
and
use
at
most
m
 k
p
ebbles
for
the
p
ebbling
itself.
Th
us
at
an
y
time
w
e
use
at
most
m
 k
+
i
 
p
ebbles.
Ov
er
all
paren
ts
then,
the
maxim
um
n
um
b
er
of
p
ebbles
that
w
e
use
is
m
 k
+
k
 
=
m
 
whic
h
ma
y
o
ccur
only
when
p
ebbling
the
k
th
paren
t.
No
w,
ho
w
ev
er,
w
e
ha
v
e
k
p
ebbles
on
the
paren
ts
of
v
and
w
e
can
p
ebble
v
itself
ha
ving
used
at
most
m
 
p
ebbles,
a
con
tradiction.
There
exists
u

G

whic
h
requires
p=
 d
p
ebbles
(in
b
oth
G
and
G

)
Consider
an
y
v

G

.
Rep
eatedly
replace
v
b
y
a
paren
t
in
G

un
til
y
ou
can
go
no
further.
Since
G
is
acyclic
this
is
guaran
teed
to
stop.
When
it
stops,
since
the
new
v
requires,
b
y
virtue
of
b
eing
in
G

,
more
than
p=
>
0
p
ebbles,
v
has
at
least
one
paren
t
in
G

(and
no
paren
ts
in
G

).
Let
m
>
p=
b
e
the
n
um
b
er
of
p
ebbles
needed
to
p
ebble
v
and
let
k
b
e
the
in-degree
of
v
in
the
original
graph
G.
By
the
ab
o
v
e
claim
v
has
a
paren
t
whic
h
requires
m
 k
+

>
p=
 k
+

p
ebbles.
F
uthermore,
since
k

d
w
e
see
that
v
has
a
paren
t
requiring
at
least
p=
 d
+

p
ebbles.
Therefore
(as
v
has
no
paren
ts
in
G

)
there
is
a
v
ertex
u

G

requiring
at
least
p=
 d
+

>
p=
 d
p
ebbles.
jE

j
+
jAj

R
d
(p=
 d)
+
p
d
Since
G

requires
at
least
p=
 d
p
ebbles,
jE

j

R
d
(p=
 d).
If
jAj

p


p
d
then
w
e
are
done.
Otherwise
jAj
<
p

.
W
e
will
ignore
jAj
and
sho
w
that
jE

j

R
d
(p=
 d)
+
p
d
.
P
ebble
eac
h
of
the
<
p=
v
ertices
in
V

with
c
hildren
in
V

in
succession.
Indep
enden
tly
p
ebbling
eac
h
w
ould
require
at
most
p=
p
ebbles.
By
p
ebbling
one
at
a
time
w
e
can
p
ebble
them
all
using
at
most
p=
 
+
p=
=
p=
 
p
ebbles.
When
done
w
e
are
left
with
less
than
p=
p
ebbles
on
the
graph,
lea
ving
more
than
p=
p
ebbles
free.
Since
w
e
kno
w
that
there
m
ust
exist
a
v

G

requiring
p
p
ebbles
in
G
then
it
m
ust
require
at
least
p=
p
ebbles
in
G

.
Consider
a
v
ertex
v
with
out-
degree
of
0
whic
h
requires
p=
p
ebbles
in
G

,
and
remo
v
e
it.
As
pro
v
en
in
section
	..
the
resulting
graph
m
ust
still
require
at
least
p=
 d
p
ebbles.
Rep
eating
this
pro
cess
i
times,
w
e
ha
v
e
a
graph
requiring
p=
 i

d
p
ebbles
with
at
least
i
few
er
edges.
So
for
i
=
p
d
times,
w
e
ha
v
e
a
graph
requiring
at
least
p=
 p=
=
p=
p
ebbles
with
p
d
few
er
edges.
This
subgraph
will
ha
v
e
at
least
R
d
(p=)

R
d
(p=
 d)
edges.
T
ogether
with
the
p
d
that
w
e
remo
v
ed
w
e
see
that
jE

j
+
jAj

jE

j

R
d
(p=
 d)
+
p
d
as
required.
Putting
it
together
So
jE
j
=
jE

j
+
jE

j
+
jAj.
By
Section
	..
w
e
ha
v
e
jE

j

R
d
(p=
 d).
By
Section
	..
w
e
ha
v
e
jE

j
+
jAj

R
d
(p=
 d)
+
p
d
.
Therefore
R
d
(p)
=
jE
j

R
d
(p=
 d)
+
p
d
,
where
the
equalit
y
is
due
to
the
h
yp
othesis
that
G
has
minim
um
n
um
b
er
of
edges
among
grap
ohs
requiring
p
p
ebbles.

0
LECTURE
	.
DTIME
VS
DSP
A
CE
T
o
solv
e
the
recurrence
notice
that
R
d
(p)

R
d
(
p

 d)
+
p
d

R
d
(
p

 d)
+
p
d
F
or
an
y
i
w
e
get
R
d
(
p

i
 d)

R
d
(
p

i
 d

 d)
+
p

i
 d
d
=
R
d
(
p

i+
 d)
+
p

i
 d
d
So
R
d
(p
 d)


i
R
d
(
p

i
 d)
+
i 
X
j
=0

j

p

j
 d

d
=

i
R
d
(
p

i
 d)
+
i 
X
j
=0
p
 
j
+
d
d
Setting
i
=
log

(p=d)
w
e
get
R
d
(p)

R
d
(p
 d)


i
R
d
(0)
+
log

(p=d) 
X
j
=0
p
 
j
+
d
d

log

(p=d) 
X
j
=0
p
 
log

(p=d)
d
d
=
log

(p=d)
p
 p
d
d
d
=
log

(p=d)
p=
d
=

(p
log
p)
So,
for
some
constan
t
c
>
0,
R
d
(p)

c

p
log
p.
No
w
consider
our
original
question
of
ho
w
man
y
p
ebbles
one
needs
to
p
ebble
a
graph
of
n
v
ertices.
With
p
=
k
n=
log
n
p
ebbles,
w
e
can
certainly
p
ebble
all
graphs
with
less
than
c

p
log
p
edges.
No
w,
cp
log
p
=
c
k
n
log
n
log
k
n
log
n
=
c
k
n
log
n
(log
k
+
log
n
 log
log
n)
>
ck
n


 log
log
n
log
n

>
ck
n=
for
all
sucien
tly
large
n
suc
h
that
log
log
n
log
n
<
=.
Letting
k
=
d
c
,
w
e
can
p
ebble
all
graphs
with
less
than
cn


d
c
=
dn
edges,
using
p
=
k
n=l
og
n
p
ebbles.
Since
an
y
graph
on
n
v
ertices,
with
in-degree
b
ound
of
d,
has
less
than
dn
edges,
it
ma
y
b
e
p
ebbled
b
y
O
(dn=
log
n)
p
ebbles.
Since
d
is
a
constan
t,
the
theorem
follo
ws.
Bibliographic
Notes
This
lecture
is
based
on
[].
.
J.E.
Hop
croft,
W.
P
aul,
and
L.
V
alian
t.
On
time
v
ersus
space.
J.
of
A
CM,
V
ol.
,
No.
,
pages
{,
	.

Lecture
0
Circuit
Depth
and
Space
Complexit
y
Notes
tak
en
b
y
V
ered
Rosen
and
Alon
Rosen
Summary:
In
this
lecture
w
e
study
some
of
the
relations
b
et
w
een
Bo
olean
circuits
and
T
uring
mac
hines.
W
e
dene
the
complexit
y
classes
N
C
and
AC
,
compare
their
computational
p
o
w
er,
and
p
oin
t
out
the
p
ossible
connection
b
et
w
een
unif
or
m-N
C
and
\ecien
t"
parallel
computation.
W
e
conclude
the
discussion
b
y
establishing
a
strong
connection
b
et
w
een
space
complexit
y
and
depth
of
circuits
with
b
ounded
fan-in.
0.
Bo
olean
Circuits
Lo
osely
sp
eaking,
a
Bo
ole
an
Cir
cuit
is
a
directed
acyclic
graph
with
three
t
yp
es
of
lab
eled
v
ertices:
inputs,
outputs,
and
gates.
The
inputs
are
sources
in
the
graph
(i.e.
v
ertices
with
in-degree
0),
and
are
lab
eled
with
either
Bo
olean
v
ariables
or
constan
t
Bo
olean
v
alues.
The
outputs
are
sinks
in
the
graph
(i.e.
v
ertices
with
out-degree
0).
The
gates
are
v
ertices
with
in-degree
k
>
0,
whic
h
are
lab
eled
with
Bo
olean
functions
on
k
inputs.
W
e
refer
to
the
in-degree
of
a
v
ertex
as
its
fan-in
and
its
out-degree
as
its
fan-out.
A
general
denition
of
Bo
olean
circuits
w
ould
allo
w
the
lab
eling
of
gates
with
arbitrary
Bo
olean
functions.
W
e
restrict
our
atten
tion
to
circuits
whose
gates
are
lab
eled
with
the
b
o
olean
functions
AND,
OR,
and
NOT
(denoted
V
;
W
;
:
resp
ectiv
ely).
0..
The
Denition
Denition
0.
(Bo
olean
Circuit):
A
Bo
olean
Circuit
is
a
dir
e
cte
d
acyclic
gr
aph
with
lab
ele
d
vertic
es:

The
input
vertic
es,
lab
ele
d
with
a
variable
x
i
or
with
a
c
onstant
(0
or
),
and
have
fan-in
0.

The
gate
vertic
es,
have
fan-in
k
>
0
and
ar
e
lab
ele
d
with
one
of
the
b
o
ole
an
functions
V
;
W
;
:
on
k
inputs
(in
the
c
ase
that
the
lab
el
is
:,
the
fan-in
k
is
r
estricte
d
to
b
e
).

The
output
vertic
es,
lab
ele
d
'output',
and
have
fan-out
0.
Given
an
assignment


f0;
g
m
to
the
variables
x

;
:::;
x
m
,
C
(
)
wil
l
denote
the
value
of
the
cir
cuit's
output.
The
value
is
dene
d
in
the
natur
al
manner,
by
setting
the
value
of
e
ach
vertex
ac
c
or
ding
to
the
b
o
ole
an
op
er
ation
it
is
lab
ele
d
with.
F
or
example,
if
a
vertex
is
lab
ele
d
V
and
the
vertic
es
with
a
dir
e
cte
d
e
dge
to
it
have
values
a
and
b,
then
the
vertex
has
value
a
V
b.
We
denote
by
siz
e(C
)
the
numb
er
of
gates
in
a
cir
cuit
C
,
and
by
depth(C
)
the
maximum
distanc
e
fr
om
an
input
to
an
output
(i.e.
the
longest
dir
e
cte
d
p
ath
in
the
gr
aph).



LECTURE
0.
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY
A
circuit
is
said
to
ha
v
e
b
ounde
d
fan-in
if
there
exists
an
a-priori
upp
er
b
ound
on
the
fan-in
of
its
AND
and
OR
gates
(NOT
gates,
m
ust
ha
v
e
fan-in

an
yw
a
y).
If
there
is
no
a-priori
b
ound
on
the
fan-in
(other
than
the
size
of
the
circuit),
the
circuit
is
said
to
ha
v
e
unb
ounde
d
fan-in.
0..
Some
Observ
ations

W
e
ha
v
e
already
seen
ho
w
to
construct
a
circuit
whic
h
sim
ulates
the
run
of
a
T
uring
mac
hine
M
on
some
input
x

f0;
g
n
(see
the
pro
of
of
Co
ok's
Theorem
,
Lecture
).
Using
this
construction
the
resulting
circuit
will
b
e
of
size
quadratic
in
T
M
(n)
(the
running
time
of
M
on
input
length
n),
and
of
depth
b
ounded
b
y
T
M
(n).

Circuits
ma
y
b
e
organized
in
to
disjoin
t
la
y
ers
of
gates,
where
eac
h
la
y
er
consists
of
gates
ha
ving
equal
distance
from
the
input
v
ertices.
Once
presen
ted
this
w
a
y
,
a
circuit
ma
y
b
e
though
t
of
as
capturing
a
certain
notion
of
parallel
computation.
W
e
could
asso
ciate
eac
h
path
starting
from
an
input
v
ertex
with
a
computation
p
erformed
b
y
a
single
pro
cessor.
Note
that
all
suc
h
computations
can
b
e
p
erformed
concurren
tly
.
View
ed
in
this
w
a
y
,
the
circuit's
depth
corresp
onds
to
parallel
time,
whereas
the
size
corresp
onds
to
the
total
amoun
t
of
parallel
w
ork.

An
y
b
ounded
fan-in
circuit
can
b
e
transformed
in
to
a
circuit
whose
gates
ha
v
e
fan-in

while
pa
ying
only
a
constan
t
factor
in
its
depth
and
size.
A
gate
with
constan
t
fan-in
c,
can
b
e
con
v
erted
in
to
a
binary
tree
of
gates
of
the
same
t
yp
e
whic
h
computes
the
same
function
as
the
original
gate.
Since
c
is
a
constan
t
so
will
b
e
the
tree's
size
and
depth.
W
e
can
therefore
assume
without
loss
of
generalit
y
that
all
gates
in
b
ounded
fan-in
circuits
ha
v
e
fan-in
.
Note
that
the
same
transformation
in
the
case
of
un
b
ounded
fan-in
will
increase
the
depth
b
y
a
factor
logarithmic
in
the
size
of
the
circuit.

An
y
circuit
can
b
e
mo
died
in
suc
h
a
w
a
y
that
all
negations
(i.e.
:
gates)
in
it
app
ear
only
in
the
input
la
y
er.
Using
De-Morgan's
la
ws
(that
is,
:
W
x
i
=
V
:x
i
),
eac
h
W
gate
follo
w
ed
b
y
a
negation
can
b
e
transformed
in
to
an
V
gate
whose
inputs
are
negations
of
the
original
inputs
(the
same
argumen
t
applies
symmetrically
for
V
gates).
This
w
a
y
,
w
e
can
propagate
all
negations
in
the
circuit
to
w
ards
the
input
la
y
er
without
c
hanging
the
v
alue
of
the
circuit's
output,
and
without
c
hanging
its
depth.
Th
us,
without
loss
of
generalit
y
,
all
\in
ternal"
gates
in
a
Bo
olean
circuit
are
lab
eled
with
V
or
W
.
When
measuring
the
depth
of
the
circuit,
negations
will
b
e
coun
ted
as
a
single
la
y
er.

In
an
un
b
ounded
fan-in
circuit,
t
w
o
consecutiv
e
V
(resp.
W
)
gates
ha
ving
iden
tical
lab
els
can
b
e
merged
in
to
a
single
gate
with
the
same
lab
el
without
c
hanging
the
v
alue
of
the
circuit's
output.
W
e
can
therefore
assume
that
un
b
ounded
fan-in
circuits
are
of
the
sp
ecial
form
where
all
V
and
W
gates
are
organized
in
to
alternating
la
y
ers
with
edges
only
b
et
w
een
adjacen
t
la
y
ers.
Note,
ho
w
ev
er,
that
the
ab
o
v
e
argumen
t
do
es
not
necessarily
w
ork
for
b
ounded
fan-in
circuits
since
the
merging
op
eration
migh
t
cause
a
blo
w-up
in
the
fan-in
of
the
resulting
gates
whic
h
will
exceed
the
sp
ecied
b
ound.
0..
F
amilies
of
Circuits
Ev
en
though
a
circuit
ma
y
ha
v
e
arbitrarily
man
y
output
v
ertices
w
e
will
fo
cus
on
circuits
whic
h
ha
v
e
only
one
output
v
ertex
(unless
otherwise
sp
ecied).
Suc
h
circuits
can
b
e
used
in
a
natural
w
a
y

0..
SMALL-DEPTH
CIR
CUITS

to
dene
a
language
(subset
of
f0;
g

).
Since
w
e
are
in
terested
in
deciding
instances
of
arbitrary
size,
w
e
consider
families
of
Bo
olean
circuits
with
one
dieren
t
circuit
for
eac
h
input
size.
Denition
0.
(F
amily
of
Circuits):
A
language
L

f0;
g

is
said
to
b
e
de
cide
d
by
a
family
of
circuits,
fC
n
g
,
when
C
n
takes
n
variables
as
inputs,
if
and
only
if
for
every
n,
C
n
(x)
=

L
(x)
for
al
l
x

f0;
g
n
.
Giv
en
a
family
of
circuits,
w
e
migh
t
b
e
in
terested
in
measuring
the
gro
wth-rate
of
the
size
and
depth
of
its
circuits
(as
a
function
of
n).
This
ma
y
b
e
useful
when
trying
to
connect
circuit
complexit
y
with
some
other
abstract
mo
del
of
computation.
Denition
0.
(Depth
and
Size
of
F
amily)
L
et
D
and
S
b
e
sets
of
inte
ger
functions
(N
!
N
),
we
say
that
a
family
of
cir
cuits
fC
n
g
,
has
depth
D
and
size
S
if
for
al
l
n,
depth(C
n
)

d(n)
and
siz
e(C
n
)

s(n)
for
some
d()

D
and
s()

S
.
If
w
e
wish
to
correlate
the
size
and
depth
of
a
family
fC
n
g
that
decides
a
language
L
with
its
T
uring
mac
hine
complexit
y
,
it
is
necessary
to
in
tro
duce
some
notion
of
uniformity.
Otherwise,
w
e
could
construct
a
family
of
circuits
whic
h
decides
a
non-recursiv
e
language
(see
Lecture
).
The
notion
of
uniformit
y
whic
h
w
e
will
mak
e
use
of
is
lo
gsp
ac
e
uniformity.
Informally
,
w
e
require
that
a
description
of
a
circuit
can
b
e
obtained
b
y
in
v
oking
a
T
uring
mac
hine
using
space
whic
h
is
logarithmic
in
the
length
of
its
output
(i.e.
the
circuit's
description).
A
description
of
a
circuit
(denoted
desc(C
n
)),
is
a
list
of
its
gates,
where
for
eac
h
gate
w
e
sp
ecify
its
t
yp
e
and
its
list
of
predecessors.
Note
that
the
length
of
desc(C
n
)
is
at
most
quadratic
in
siz
e(C
n
).
Denition
0.
(logspace
uniformit
y):
A
family
of
cir
cuits,
f
C
n
g,
is
c
al
le
d
logspace
uniform
if
ther
e
exists
a
deterministic
T
uring
machine,
M
,
such
that
for
every
n,
M
(
n
)
=
desc(C
n
)
while
using
sp
ac
e
which
is
lo
garithmic
in
the
length
of
desc(C
n
).
The
reason
w
e
require
M
to
w
ork
in
space
whic
h
is
logarithmic
in
its
output
length
(rather
than
its
input
length)
lies
in
the
fact
that
siz
e(C
n
)
(and
therefore
the
length
of
desc(C
n
))
migh
t
b
e
sup
er-p
olynomial
in
n.
The
problem
is
that
the
description
of
a
circuit
with
sup
er-p
olynomial
size,
c
annot
b
e
pro
duced
b
y
T
uring
mac
hines
w
orking
with
space
whic
h
is
logarithmic
in
n
(the
input
size),
and
therefore
suc
h
a
circuit
(one
with
sup
er-p
olynomial
size)
w
ould
ha
v
e
b
een
o
v
erlo
ok
ed
b
y
a
denition
using
the
input
length
as
parameter.
Note
that
if
w
e
restrict
ourselv
es
to
circuits
with
p
olynomial
size,
then
the
ab
o
v
e
remark
do
es
not
apply
,
and
it
is
sucien
t
to
require
that
M
is
a
deterministic
logspace
T
uring
mac
hine.
Based
on
the
ab
o
v
e
denitions,
the
class
P
=pol
y
is
the
class
of
all
languages
for
whic
h
there
exists
a
family
of
circuits
ha
ving
p
olynomial
depth
and
p
olynomial
size
(see
Lecture
).
W
e
ha
v
e
already
seen
ho
w
to
use
the
transformation
from
T
uring
mac
hines
in
to
circuits
in
order
to
pro
v
e
that
unif
or
m-P
=pol
y
con
tains
(and
in
fact
equals)
P
.
W
e
note
that
the
ab
o
v
e
transformation
can
b
e
p
erformed
in
logarithmic
space
as
w
ell.
This
means
that
l
og
space-unif
or
m-P
=pol
y
also
equals
P
.
In
the
sequel,
w
e
c
ho
ose
logspace
uniformit
y
to
b
e
our
preferred
notion
of
uniformit
y
.
0.
Small-depth
circuits
In
this
section
w
e
consider
p
olynomial
size
circuits
whose
depth
is
considerably
smaller
than
n,
the
n
um
b
er
of
inputs.
By
depth
considerably
smaller
than
n
w
e
refer
to
p
oly-lo
garithmic
depth,
that
is,
b
ounded
b
y
O
(log
k
n)
for
some
k

0.
W
e
are
in
terested
in
separating
the
cases
of
un
b
ounded
and
b
ounded
fan-in,
sp
ecically
,
w
e
will
in
v
estigate
the
relation
b
et
w
een
t
w
o
main
complexit
y
classes.
As
w
e
will
see,
these
classes
will
ev
en
tually
turn
out
to
b
e
subsets
of
P
whic
h
capture
the
notion
of
what
is
\ecien
tly"
computable
b
y
parallel
algorithms.


LECTURE
0.
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY
0..
The
Classes
N
C
and
AC
The
complexit
y
class
N
C
is
dened
as
the
class
of
languages
that
can
b
e
decided
b
y
families
of
b
ounded
fan-in
circuits
with
p
olynomial
size,
and
p
oly-logarithmic
depth
(in
the
n
um
b
er
of
inputs
to
the
circuits).
The
actual
denition
of
N
C
in
tro
duces
an
hierarc
h
y
of
classes
decided
b
y
circuit
families
with
increasing
depth.
Denition
0.
(N
C
):
F
or
k

0,
dene
N
C
k
to
b
e
the
class
of
languages
that
c
an
b
e
de
cide
d
by
families
of
cir
cuits
with
b
ounded
fan-in,
p
olynomial
size,
and
depth
O
(log
k
n).
Dene
N
C
to
b
e
the
union
of
al
l
N
C
k
's
(i.e.
N
C
def
=
S
k
N
C
k
).
A
natural
question
w
ould
b
e
to
ask
what
happ
ens
to
the
computational
p
o
w
er
of
the
ab
o
v
e
circuits
if
w
e
remo
v
e
the
b
ound
on
the
fan-in.
F
or
instance,
it
is
easy
to
see
that
the
decision
problem
\is
the
input
string


f0;
g
n
made
up
of
all
's?"
(the
AND
function)
can
b
e
solv
ed
b
y
a
depth-
un
b
ounded
fan-in
circuit,
whereas
in
a
b
ounded
fan-in
circuit
this
problem
w
ould
require
depth
at
least

(log
n),
just
so
all
the
input
bits
could
ee
ct
the
output
gate.
The
classes
based
on
the
un
b
ounded
fan-in
mo
del
(namely
AC
)
are
dened
analogously
to
the
classes
in
the
N
C
hierarc
h
y
.
Denition
0.
(AC
):
F
or
k

0,
dene
AC
k
to
b
e
the
class
of
languages
that
c
an
b
e
de
cide
d
by
families
of
cir
cuits
with
un
b
ounded
fan-in,
p
olynomial
size,
and
depth
O
(log
k
n).
Dene
AC
to
b
e
the
union
of
al
l
AC
k
's
(i.e.
AC
def
=
S
k
AC
k
).
As
w
e
will
see
in
the
sequel,
AC
equals
N
C
.
Note
ho
w
ev
er,
that
suc
h
a
result
do
es
not
necessarily
rule
out
a
separation
b
et
w
een
the
computational
p
o
w
er
of
circuits
with
b
ounded
and
resp
ectiv
ely
un
b
ounded
fan-in.
W
e
ha
v
e
already
seen
that
the
class
N
C
0
of
languages
decided
b
y
constan
t
depth
circuits
with
b
ounded
fan-in,
is
strictly
con
tained
in
AC
0
,
its
un
b
ounded
fan-in
v
ersion.
W
e
are
therefore
motiv
ated
to
lo
ok
deep
er
in
to
the
N
C
k
and
AC
k
hierarc
hies
and
try
to
gain
a
b
etter
understanding
of
the
relation
b
et
w
een
their
corresp
onding
lev
els.
Theorem
0.
F
or
al
l
k

0,
N
C
k

AC
k

N
C
k
+
Pro
of:
The
rst
inclusion
is
trivial,
w
e
turn
directly
to
pro
v
e
the
second
inclusion.
Since
an
y
gate
in
an
un
b
ounded
fan-in
circuit
of
size
p
oly
(n)
can
ha
v
e
fan-in
at
most
p
oly
(n),
eac
h
suc
h
gate
can
b
e
con
v
erted
in
to
a
tree
of
gates
of
the
same
t
yp
e
with
fan-in
,
suc
h
that
the
output
gate
of
the
tree
computes
the
same
function
as
the
original
gate
(since
this
transformation
can
b
e
p
erformed
in
logarithmic
space,
the
claim
will
hold
b
oth
for
the
uniform
and
the
non
uniform
settings).
The
resulting
depth
of
the
tree
will
b
e
log
(p
oly
(n))
=
O
(log
n).
By
applying
this
transformation
to
eac
h
gate
in
an
un
b
ounded
fan-in
circuit
of
depth
O
(log
k
n)
w
e
obtain
a
b
ounded
fan-in
circuit
deciding
the
same
language
as
the
original
one.
The
ab
o
v
e
transformation
will
cost
us
only
a
logarithmic
factor
in
the
depth
and
a
p
olynomial
factor
in
the
size
(i.e.
the
depth
of
the
resulting
circuit
will
b
e
O
(log
k
+
n),
and
the
size
will
remain
p
olynomial
in
n).
Th
us,
an
y
language
in
AC
k
is
also
in
N
C
k
+
.
Corollary
0.
AC
=
N
C

0..
SMALL-DEPTH
CIR
CUITS

In
ligh
t
of
Theorem
0.,
it
migh
t
b
e
in
teresting
to
ask
ho
w
far
do
es
the
N
C
(resp.
AC
)
hierarc
h
y
extend.
Is
it
innite,
or
do
es
it
collapse
to
some
lev
el?
Ev
en
if
a
collapse
seems
unlik
ely
,
at
presen
t
no
argumen
t
is
kno
wn
to
rule
out
this
option.
One
thing
whic
h
is
kno
wn
at
presen
t,
is
that
AC
0
is
strictly
con
tained
in
N
C

.
Theorem
0.	
AC
0

N
C

(strictly
c
ontaine
d).
Note
that
unif
or
m-N
C
is
con
tained
in
P
.
Theorem
0.	
implies
that
unif
or
m-AC
0
is
strictly
con
tained
in
P
.
An
in
teresting
op
en
question
is
to
establish
what
is
the
exact
relationship
b
et
w
een
unif
or
m-N
C
and
P
,
it
is
not
curren
tly
kno
wn
whether
b
oth
classes
are
equal
or
not
(analogously
to
the
P
vs.
N
P
problem).
As
a
matter
of
fact,
it
is
not
ev
en
kno
wn
ho
w
to
separate
unif
or
m-N
C

from
N
P
.
0..
Sk
etc
h
of
the
pro
of
of
AC
0

N
C

W
e
pro
v
e
the
Theorem
b
y
sho
wing
that
the
decision
problem
\do
es
the
input
string
ha
v
e
an
ev
en
n
um
b
er
of
's?"
can
b
e
solv
ed
in
N
C

but
cannot
b
e
solv
ed
in
AC
0
.
In
this
con
text
it
will
b
e
more
con
v
enien
t
to
view
circuits
as
computing
functions
rather
than
deciding
languages.
Denition
0.0
(P
arit
y):
L
et
x

f0;
g
n
,
the
function
Parity
is
dene
d
as:
P
ar
ity
(x

;
:
:
:
;
x
n
)
def
=
n
X
i=
x
i
(mod
)
Claim
0..
P
ar
ity

N
C

.
Pro
of:
W
e
construct
a
circuit
computing
P
ar
ity
,
using
a
binary
tree
of
logarithmic
depth
where
eac
h
gate
is
a
xor
gate.
Eac
h
xor
gate
can
then
b
e
replaced
b
y
the
com
bination
of

legal
gates:
a

b
=
(a
^
:b)
_
(:a
^
b).
This
transformation
increases
the
depth
of
the
circuit
b
y
a
factor
of
,
and
the
size
of
the
circuit
b
y
a
factor
of
.
Consequen
tly
,
P
ar
ity
is
computed
b
y
circuits
of
logarithmic
depth
and
p
olynomial
size,
and
is
th
us
in
N
C

.
Claim
0..
P
ar
ity

AC
0
.
In
order
to
pro
v
e
claim
0..
w
e
sho
w
that
ev
ery
constan
t
depth
circuit
computing
P
ar
ity
m
ust
ha
v
e
sub-exp
onen
tial
size
(and
therefore
P
ar
ity
cannot
b
elong
to
AC
0
),
more
precisely:
Theorem
0.
F
or
every
c
onstant
d,
a
cir
cuit
c
omputing
P
ar
ity
on
n
inputs
with
depth
d,
must
have
size
exp(
(n

d 
)).
Pro
of:
(sk
etc
h):
The
Theorem
is
pro
v
en
b
y
induction
on
d
and
pro
ceeds
as
follo
ws:
.
Pro
v
e
that
parit
y
circuits
of
depth

m
ust
b
e
of
large
size.
.
Pro
v
e
that
depth
d
parit
y
circuits
of
small
size
can
b
e
con
v
erted
to
depth
d
 
parit
y
circuits
of
small
size
(th
us
con
tradicting
the
induction
h
yp
othesis).


LECTURE
0.
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY
The
rst
step
is
relativ
ely
easy
,
whereas
the
second
step
(whic
h
is
the
heart
of
the
Theorem)
is
b
y
far
more
complicated.
W
e
will
therefore
giv
e
only
the
outline
of
it.
Base
case
(d
=
):
Without
loss
of
generalit
y
,
w
e
assume
that
the
circuit
giv
en
to
us
is
an
OR
of
ANDs
(in
case
it
is
an
AND
of
ORs
the
follo
wing
argumen
ts
apply
symmetrically).
Then,
an
y
AND
gate
ev
aluating
to
\"
determines
the
v
alue
of
the
circuit.
Consider
no
w
an
AND
gate
(an
y
gate
in
the
in
termediate
la
y
er).
Note
that
all
the
v
ariables
x

;
:
:
:
;
x
n
m
ust
b
e
connected
to
that
gate.
Otherwise,
assume
there
exists
an
i
suc
h
that
there
is
no
edge
going
from
x
i
(and
from
:x
i
)
in
to
the
gate:
Then,
tak
e
an
assignmen
t
to
the
v
ariables
going
in
to
the
gate,
causing
it
to
ev
aluate
to
\"
(w
e
assume
the
gate
is
not
degenerate).
Under
this
assignmen
t,
the
circuit
will
output
\",
regardless
of
the
v
alue
of
x
i
,
whic
h
is
imp
ossible.
Due
to
that,
w
e
can
asso
ciate
eac
h
AND
gate
with
a
certain
assignmen
t
to
the
n
v
ariables
(deter-
mined
b
y
the
literals
connected
to
that
gate).
W
e
can
sa
y
that
this
gate
will
ev
aluate
to
\"
i
the
v
ariables
will
ha
v
e
this
assignmen
t.
This
argumen
t
sho
ws,
that
there
m
ust
b
e
at
least

n 
AND
gates.
Otherwise,
there
exists
an
assignmen
t

to
x

;
:
:
:
;
x
n
,
suc
h
that
P
ar
ity
(
)
=
,
of
whic
h
there
is
no
AND
gate
iden
tied
with

.
This
means
that
the
circuit
ev
aluated
on
the
assignmen
t

will
output
\0",
in
con
tradiction.
The
induction
step:
The
basic
idea
of
this
step
lies
in
a
lemma
pro
v
en
b
y
Hastad.
The
lemma
states
that
giv
en
a
depth

circuit,
sa
y
an
AND
of
ORs,
then
if
one
giv
es
random
v
alues
to
a
randomly
selected
subset
of
the
v
ariables,
then
it
is
p
ossible
to
write
the
resulting
induced
circuit
as
an
OR
of
relativ
ely
few
ANDs
with
v
ery
high
probabilit
y
.
W
e
no
w
outline
ho
w
use
this
lemma
in
order
to
carry
on
the
induction
step:
Giv
en
a
circuit
of
depth
d
computing
parit
y
,
w
e
assign
random
v
alues
to
some
of
its
inputs
(only
a
large
randomly
c
hosen
subset
of
the
inputs
are
preset,
the
rest
sta
y
as
v
ariables).
Consequen
tly
,
w
e
obtain
a
simplied
circuit
that
w
orks
on
few
er
v
ariables.
This
circuit
will
still
compute
parit
y
(or
the
negation
of
parit
y)
of
the
remaining
v
ariables.
By
virtue
of
the
lemma,
it
is
p
ossible
to
in
terc
hange
t
w
o
adjacen
t
lev
els
(the
ones
closest
to
the
input
la
y
er)
of
ANDs
and
ORs.
Then,
b
y
merging
the
t
w
o
no
w
adjacen
t
lev
els
with
the
same
connectiv
e,
w
e
decrease
the
depth
of
the
circuit
to
d
 .
This
is
done
without
increasing
signican
tly
the
size
of
the
circuit.
Let
us
no
w
mak
e
formal
what
w
e
mean
b
y
randomly
xing
some
v
ariables.
Denition
0.
(random
restriction):
A
random
restriction
with
a
p
ar
ameter

to
the
variables
x

;
:
:
:
;
x
n
,
tr
e
ats
every
variable
x
i
(indep
endently)
in
the
fol
lowing
way:
x
i
=

>
<
>
:
w.p.
 

set
x
i
 
0
w.p.
 

set
x
i
 

w.p.

l
eav
e
it
\alive"
Observ
e
that
the
exp
ected
n
um
b
er
of
v
ariables
remaining
is
m
=
n.
Ob
viously
,
the
smaller

is
the
more
w
e
can
simplify
our
circuit,
but
on
the
other
hand
w
e
ha
v
e
few
er
remaining
v
ariables.
In
order
to
con
tradict
the
induction
h
yp
othesis,
w
e
w
ould
lik
e
the
size
of
the
transformed
circuit
(after
doing
a
random
restriction
and
decreasing
its
depth
b
y
)
to
b
e
smaller
than
exp(o(m

d 
)).
It
will
suce
to
require
that
n

d 
<
m

d 
,
or
alternativ
ely
,
m
>
n

n
 
d 
.
Th
us,
a
wise
c
hoice
of
the
parameter
,
w
ould
b
e

=
n
 
d 
.

0..
SMALL-DEPTH
CIR
CUITS

0..
N
C
and
P
arallel
Computation
W
e
no
w
turn
to
briey
examine
the
connection
b
et
w
een
the
complexit
y
class
unif
or
m-N
C
and
parallel
computation.
In
particular,
w
e
consider
the
connection
to
the
parallel
random-access
mac
hine
(PRAM),
whic
h
can
b
e
view
ed
as
a
parallel
v
ersion
of
the
RAM.
A
RAM
is
a
computing
device
(pro
cessor),
consisting
of
a
program,
a
nite
set
of
registers,
and
an
innite
memory
(whose
cells
are
of
the
same
size
as
the
registers).
The
program
is
a
nite
sequence
of
instructions
whic
h
are
executed
sequen
tially
,
where
at
the
execution
of
eac
h
instruction
the
pro
cessor
reads
and
writes
v
alues
to
some
of
its
registers
or
memory
cells,
as
required
b
y
the
instruction.
The
PRAM
consists
of
sev
eral
indep
enden
t
sequen
tial
pro
cessors
(i.e.
RAMs),
eac
h
ha
ving
its
o
wn
set
of
priv
ate
registers.
In
addition
there
is
an
innite
shared
memory
accessible
b
y
all
pro
cessors.
In
one
unit
time
(of
a
clo
c
k
common
to
all
pro
cessors),
eac
h
pro
cessor
executes
a
single
instruction,
during
whic
h
it
can
read
and
write
to
its
priv
ate
set
of
registers,
or
to
the
shared
memory
cells.
PRAMs
can
b
e
classied
according
to
restrictions
on
global
memory
access.
An
Exclusiv
e-
Read
Exclusiv
e-W
rite
(EREW)
PRAM
forbids
sim
ultaneous
access
to
the
same
memory
cell
b
y
dieren
t
pro
cessors.
A
Concurren
t-Read
Exclusiv
e-W
rite
(CREW)
PRAM
allo
ws
sim
ultaneous
reads
but
no
sim
ultaneous
writes,
and
a
Concurren
t-Read
Concurren
t-W
rite
(CR
CW)
PRAM
allo
ws
sim
ultaneous
reads
and
writes
(in
this
case
one
has
to
dene
ho
w
concurren
t
writes
are
treated).
Despite
this
v
ariet
y
of
dieren
t
PRAM
mo
dels,
it
turns
out
that
they
do
not
dier
v
ery
widely
in
their
computational
p
o
w
er.
In
designing
algorithms
for
parallel
mac
hines
w
e
ob
viously
w
an
t
to
minimize
the
time
required
to
p
erform
the
concurren
t
computation.
In
particular,
w
e
w
ould
lik
e
our
parallel
algorithms
to
b
e
dr
amatic
al
ly
faster
than
our
sequen
tial
ones.
Impro
v
emen
ts
in
the
running
time
w
ould
b
e
considered
dramatic
if
w
e
could
ac
hiev
e
an
exp
onen
tial
drop
in
the
time
required
to
solv
e
a
problem,
sa
y
from
p
olynomial
to
logarithmic
(or
at
least
p
oly-logarithmic).
Denote
b
y
PRAM(t();
p())
the
class
of
languages
decidable
b
y
a
PRAM
mac
hine
w
orking
in
parallel
time
t()
and
using
p()
pro
cessors,
then
w
e
ha
v
e
the
follo
wing:
Theorem
0.
unif
or
m-N
C
=
PRAM(p
olylog,p
oly)
Hence,
the
complexit
y
class
unif
or
m-N
C
captures
the
notion
of
what
is
\ecien
tly"
com-
putable
b
y
PRAM
mac
hines
(just
as
the
class
P
captures
the
notion
of
what
is
\ecien
tly"
com-
putable
b
y
RAM
mac
hines,
whic
h
are
equiv
alen
t
to
T
uring
mac
hines).
Note
ho
w
ev
er,
that
the
PRAM
cannot
b
e
considered
a
ph
ysically
realizable
mo
del,
since,
as
the
n
um
b
er
of
pro
cessors
and
the
size
of
the
global
memory
scales
up,
it
quic
kly
b
ecomes
imp
ossible
to
pro
vide
a
constan
t-length
data
path
from
an
y
pro
cessor
to
an
y
memory
cell.
Nev
ertheless,
the
PRAM
has
pro
v
ed
to
b
e
an
extremely
useful
v
ehicle
for
studying
the
logical
structure
of
parallel
computation.
Algorithms
dev
elop
ed
for
other,
more
realistic
mo
dels,
are
often
based
on
algorithms
originally
designed
for
the
PRAM.
Moreo
v
er,
a
transformation
from
the
PRAM
mo
del
in
to
some
other
more
realistic
mo
del
will
cost
us
only
a
logarithmic
factor
in
the
parallel
complexit
y
.
Finally
,
w
e
w
ould
lik
e
to
note
that
the
analogy
of
N
C
to
parallel
computation
has
some
asp
ects
missing.
First
of
all,
it
ignores
the
issue
of
the
comm
unication
b
et
w
een
pro
cessors.
As
a
matter
of
fact,
it
seems
that
in
practice
this
is
the
o
v
ershado
wing
problem
to
mak
e
large
scale
parallel
computation
ecien
t
(note
that
the
PRAM
mo
del
implicitly
o
v
ercomes
the
comm
unication
issue
since
t
w
o
pro
cessors
can
comm
unicate
in
O
()
just
b
y
writing
a
message
on
the
memory).
Another
asp
ect
missing
is
the
fact
that
the
division
of
N
C
in
to
sub
classes
based
on
running
times
seems
to
obscure
the
real
b
ottlenec
k
for
parallel
computing,
whic
h
is
the
n
um
b
er
of
pro
cessors
required.
An
algorithm
that
requires
n
pro
cessors
and
l
og

n
running
time
is
lik
ely
to
b
e
far
more
useful
than


LECTURE
0.
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY
one
that
requires
n

pro
cessors
and
tak
es
log
n,
but
the
latter
is
in
N
C

,
the
more
restrictiv
e
(and
hence
presumably
b
etter)
class.
0.
On
Circuit
Depth
and
Space
Complexit
y
In
this
section
w
e
p
oin
t
out
a
strong
connection
b
et
w
een
depth
of
circuits
with
b
ounded
fan-in
and
space
complexit
y
.
It
turns
out
that
circuit
depth
and
space
complexit
y
are
p
olynomial
ly
r
elate
d.
In
particular,
w
e
are
able
to
pro
v
e
that
L
(and
in
fact
N
L)
falls
within
N
C
.
F
or
the
sak
e
of
generalit
y
,
w
e
in
tro
duce
t
w
o
families
of
complexit
y
classes,
whic
h
can
b
e
though
t
of
as
a
generalized
v
ersion
of
N
C
.
F
rom
no
w
on,
w
e
assume
that
all
circuits
are
uniform.
Denition
0.
(D
E
P
T
H
=S
I
Z
E
):
L
et
d;
s
b
e
inte
ger
functions.
Dene
D
E
P
T
H
=S
I
Z
E
(d();
s())
to
b
e
the
class
of
al
l
languages
that
c
an
b
e
de
cide
d
by
a
uniform
family
of
b
ounded
fan-in
cir
cuits
with
depth
d()
and
size
s().
In
particular,
if
w
e
denote
b
y
p
oly
the
set
of
all
in
teger
functions
b
ounded
b
y
a
p
olynomial
and
b
y
p
olylog
the
set
of
all
in
teger
functions
b
ounded
b
y
a
p
oly-logarithmic
function
(e.g.
f

p
olylog
i
f
(n)
=
O
(log
k
n)
for
some
k

0),
then
using
the
ab
o
v
e
notation,
the
class
N
C
can
b
e
view
ed
as
D
E
P
T
H
=S
I
Z
E
(p
olylog;
p
oly
).
Denition
0.
(D
E
P
T
H
):
L
et
d
b
e
an
inte
ger
function.
Dene
D
E
P
T
H
(d())
to
b
e
the
class
of
al
l
languages
that
c
an
b
e
de
cide
d
by
a
uniform
family
of
b
ounded
fan-in
cir
cuits
with
depth
d().
Clearly
,
N
C

D
E
P
T
H
(p
olylog).
Note
that
the
size
of
circuits
deciding
the
languages
in
D
E
P
T
H
(d())
is
not
limited,
except
for
the
natural
upp
er
b
ound
of

d()
(due
to
the
b
ounded
fan-in)

.
Ho
w
ev
er,
circuits
deciding
languages
in
N
C
are
of
p
olynomial
size.
Therefore,
the
class
D
E
P
T
H
(p
olylog)
con
tains
languages
whic
h
p
oten
tially
do
not
b
elong
to
N
C
.
W
e
are
no
w
ready
to
connect
circuit
depth
and
space
complexit
y
.
Theorem
0.
F
or
every
inte
ger
function
s()
which
is
at
le
ast
lo
garithmic,
N
S
P
AC
E
(s)

D
E
P
T
H
=S
I
Z
E
(O
(s

);

O
(s)
)
Pro
of:
Giv
en
a
non-deterministic
s(n)-space
T
uring
mac
hine
M
,
w
e
construct
a
uniform
family
of
circuits,
fC
n
g,
of
depth
O
(s

)
and
size

O
(s)
suc
h
that
for
ev
ery
x

f0;
g

,
C
jxj
(x)
=
M
(x).
Consider
the
conguration
graph,
G
M
;x
,
of
M
on
input
x
(see
Lecture
).
Recall
that
the
v
ertices
of
the
graph
are
all
the
p
ossible
congurations
of
M
on
input
x,
and
a
directed
edge
leads
from
one
conguration
to
another
if
and
only
if
they
are
p
ossible
consecutiv
e
congurations
of
a
computation
on
x.
In
order
to
decide
whether
M
accepts
x
it
is
enough
to
c
hec
k
whether
there
exists
a
directed
path
in
G
M
;x
leading
from
the
initial
conguration
v
ertex
to
the
accepting
conguration
v
ertex.
The
problem
of
deciding,
giv
en
a
graph
and
t
w
o
of
its
v
ertices,
whether
there
exists
a
directed
path
b
et
w
een
them,
is
called
the
directed
connectivit
y
problem
(denoted
C
O
N
N
,
see
also
Lecture
).
It
turns
out
that
C
O
N
N
can
b
e
decided
b
y
circuits
with
p
oly-logarithmic
depth.
More
precisely:
Claim
0..
C
O
N
N

N
C


Th
us,
an
alternativ
e
w
a
y
to
dene
D
E
P
T
H
(d())
w
ould
b
e
D
E
P
T
H=S
I
Z
E
(d();

d()
).

0..
ON
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY
	
Pro
of:
Let
G
b
e
a
directed
graph
on
n
v
ertices
and
let
A
b
e
the
adjacency
matrix
corresp
onding
to
it;
that
is,
A
is
a
b
o
olean
matrix
of
size
n

n,
and
A
i;j
=

i
there
is
a
directed
edge
from
v
ertex
v
i
to
v
ertex
v
j
in
G.
No
w,
let
B
b
e
A
+
I
,
i.e.
add
to
A
all
the
self
lo
ops.
Consider
no
w
the
b
o
olean
pro
duct
of
B
with
itself,
dened
as
B

i;j
=
n
_
k
=
(B
i;k
^
B
k
;j
)
(0.)
The
resulting
matrix
will
satisfy
that
B

i;j
=

if
and
only
if
there
is
a
directed
path
of
length

or
less
from
v
i
to
v
j
in
G.
Similarly
,
B

's
en
tries
will
denote
the
existence
of
paths
in
G
of
length
up
to
,
and
so
on.
Using
log
n
b
o
olean
m
ultiplications
w
e
can
compute
the
matrix
B
n
,
whic
h
is
the
adjacency
matrix
of
the
transitiv
e
closure
of
A
-
con
taining
the
answ
ers
to
all
the
p
ossible
connectivit
y
questions
(for
ev
ery
pair
of
v
ertices
in
G).
Squaring
a
matrix
of
size
n

n
can
b
e
done
in
AC
0
(see
Equation
0.)
and
therefore
in
N
C

.
Hence,
computing
B
n
can
b
e
done
via
rep
eated
squaring
in
N
C

.
The
circuit
w
e
build
is
a
comp
osition
of
t
w
o
circuits.
The
rst
circuit
tak
es
as
input
some
x

f0;
g
n
,
a
description
of
M
(whic
h
is
of
a
constan
t
length)
and
outputs
the
adjacency
matrix
of
G
M
;x
.
The
second
circuit
tak
es
as
input
the
adjacency
matrix
of
G
M
;x
and
decides
whether
there
exists
a
directed
path
from
the
initial
conguration
v
ertex
to
the
accepting
conguration
v
ertex
(i.e.
decides
C
O
N
N
on
G
M
;x
).
W
e
start
b
y
constructing
the
rst
circuit.
Giv
en
x
and
the
description
of
M
w
e
generate
all
the
p
ossible
congurations
of
M
on
x
(there
are

O
(s)
suc
h
congurations,
eac
h
is
represen
ted
b
y
O
(s)
bits).
Then,
for
eac
h
pair
of
congurations
w
e
can
decide
if
there
should
b
e
a
directed
edge
b
et
w
een
them
(i.e.
whether
these
are
p
ossible
consecutiv
e
congurations).
This
is
done
basically
b
y
comparing
the
con
ten
ts
of
the
w
ork
tap
e
in
the
t
w
o
congurations,
and
requires
depth
O
(log
s)
(note
that
the
size
of
the
resulting
circuit
will
b
e

O
(s)
).
As
for
the
second
circuit,
since
G
M
;x
is
of
size

O
(s)
,
w
e
ha
v
e
(b
y
Claim
0..)
that
C
O
N
N
can
b
e
decided
on
G
M
;x
in
depth
O
(s

)
and
size

O
(s)
.
Ov
erall,
w
e
obtain
a
circuit
C
n
of
depth
O
(s

)
and
of
size

O
(s)
,
suc
h
that
C
n
(x)
=
M
(x).
Corollary
0.
N
L

N
C

Pro
of:
T
ak
e
s(n)
=
log
n,
and
get
that
N
L
can
b
e
decided
b
y
circuits
of
p
olynomial
size
and
of
depth
O
(log

n).
Using
the
fact
that
D
E
P
T
H
=S
I
Z
E
(O
(s

);

O
(s)
)
is
con
tained
in
D
E
P
T
H
(O
(s

)),
w
e
can
conclude:
Corollary
0.
F
or
every
inte
ger
function
s()
which
is
at
le
ast
lo
garithmic,
N
S
P
AC
E
(s)

D
E
P
T
H
(O
(s

))
W
e
are
no
w
ready
to
establish
a
rev
erse
connection
b
et
w
een
circuit
depth
and
space
complexit
y
.
This
is
done
b
y
pro
ving
a
result
whic
h
is
close
to
b
eing
a
con
v
erse
to
Corollary
0.
(giv
en
that
for
ev
ery
function
s(),
D
S
P
AC
E
(s)

N
S
P
AC
E
(s)).
Theorem
0.	
F
or
every
function
d()
which
is
at
le
ast
lo
garithmic,
D
E
P
T
H
(d)

D
S
P
AC
E
(d)

0
LECTURE
0.
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY
Pro
of:
Giv
en
a
uniform
family
of
circuits
fC
n
g
of
depth
d(n),
w
e
construct
a
deterministic
d(n)-
space
T
uring
mac
hine
M
suc
h
that
for
ev
ery
x

f0;
g

,
M
(x)
=
C
jxj
(x).
Our
algorithm
will
b
e
the
comp
osition
of
t
w
o
algorithms,
eac
h
using
d(n)
space.
The
follo
wing
lemma
states
that
the
ab
o
v
e
comp
osition
will
giv
e
us
a
d(n)-space
algorithm,
as
required
(this
is
a
sp
ecial
case
of
the
Comp
osition
lemma
-
the
searc
h
v
ersion,
see
Lecture
).
Lemma
0..
L
et
M

and
M

b
e
two
s(n)-sp
ac
e
T
uring
machines.
Then,
ther
e
exists
an
s(n)-
sp
ac
e
T
uring
machine
M,
that
on
input
x
outputs
M

(M

(x)).
Our
algorithm
is
(giv
en
input
x

f0;
g
n
):
.
Obtain
a
description
of
C
n
.
.
Ev
aluate
C
n
(x).
A
description
of
a
circuit
is
a
list
of
its
gates,
where
for
eac
h
gate
w
e
sp
ecify
its
t
yp
e
and
its
list
of
predecessors.
Note
that
the
length
of
the
description
of
C
n
migh
t
b
e
exp
onen
tial
in
d(n)
(since
the
n
um
b
er
of
gates
in
C
n
migh
t
b
e
exp
onen
tial
in
d(n)),
therefore
w
e
m
ust
use
Lemma
0...
The
follo
wing
claims
establish
the
Theorem:
Claim
0..
A
description
of
C
n
c
an
b
e
gener
ate
d
using
O
(d(n))
sp
ac
e.
Claim
0..
Cir
cuit
evaluation
for
b
ounde
d
fan-in
cir
cuits
c
an
b
e
solve
d
in
sp
ac
e=O(cir
cuit
depth).
Pro
of:
(of
Claim
0..)
By
the
uniformit
y
of
fC
n
g
,
there
exists
a
deterministic
mac
hine
M
suc
h
that
M
(
n
)
=
desc(C
n
),
while
using
log
(jdesc(C
n
)j)
space.
Since
jdesc(C
n
)j


O
(d(n))
,
w
e
get
that
M
uses
O
(d(n))
space,
as
required.
Pro
of:
(of
Claim
0..)
Giv
en
a
circuit
C
of
depth
d
and
an
input
x,
w
e
w
an
t
to
compute
C
(x).
Our
implemen
tation
will
b
e
recursiv
e.
A
natural
approac
h
w
ould
b
e
to
use
the
follo
wing
algorithm:
Denote
b
y
V
ALU
E
(C
x
;
v
)
the
v
alue
of
v
ertex
v
in
the
circuit
C
when
assigning
x
to
its
inputs,
where
v
is
enco
ded
in
binary
(i.e.
using
log
(siz
e(C
))
=
O
(d)
bits).
Note
that
V
ALU
E
(C
x
;'output')
is
equal
to
the
desired
v
alue,
C
(x).
The
follo
wing
pro
cedure
obtains
V
ALU
E
(C
x
;
v
):
.
If
v
is
a
leaf
then
return
the
v
alue
assigned
to
it.
Otherwise,
let
u
and
w
b
e
v
's
predecessors
and
op
b
e
v
's
lab
el.
.
Compute
recursiv
ely

 
V
ALU
E
(C
x
;
u)
and

 
V
ALU
E
(C
x
;
w
).
.
Return

op

.
Notice
that
Step

in
the
algorithm
requires
t
w
o
recursion
calls.
Since
w
e
are
going
only
one
lev
el
do
wn
eac
h
recursion
call,
one
ma
y
hastily
conclude
that
the
space
consumed
b
y
the
algorithm
is

O
(d)
.
Remem
b
er
ho
w
ev
er,
that
w
e
are
dealing
with
space,
this
means
that
the
space
consumed
in
the
rst
recursion
call
can
b
e
reused
b
y
the
second
one,
and
therefore
the
actual
space
consumption
is
O
(d

)
(since
there
are
O
(d)
lev
els
in
the
recursion,
and
on
eac
h
lev
el
w
e
need
to
memorize
the
v
ertex
name
whic
h
is
of
length
O
(d)).
This
is
still
not
go
o
d
enough,
remem
b
er
that
our
goal
is
to
design
an
algorithm
w
orking
in
space
O
(d).
This
will
b
e
done
b
y
represen
ting
the
v
ertices
of
C
in
a
dieren
t
manner:
Eac
h
v
ertex
will
b
e
sp
ecied
b
y
a
path
reac
hing
it
from
the
output
v
ertex.
The
output
v
ertex
will
b
e
represen
ted

0..
ON
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY

b
y
the
empt
y
string
.
Its
left
predecessor
will
b
e
represen
ted
b
y
\0",
and
its
righ
t
predecessor
b
y
\",
the
left
predecessor
of
\"
will
b
e
represen
ted
b
y
\0",
and
so
on.
Since
there
migh
t
b
e
sev
eral
paths
reac
hing
a
v
ertex,
it
migh
t
ha
v
e
m
ultiple
names
assigned
to
it,
but
this
will
not
b
other
us.
Consequen
tly
,
eac
h
v
ertex
is
represen
ted
b
y
a
bit
string
of
length
O
(d).
Moreo
v
er,
obtaining
from
a
v
ertex
name
its
predecessor's
or
successor's
name
is
done
simply
b
y
concatenation
of
a
bit
or
deletion
of
the
last
bit.
The
follo
wing
pro
cedure
computes
V
ALU
E
(C
x
;
path
)
in
O
(d)
space:
.
Chec
k
whether
path
denes
a
leaf.
If
it
do
es,
then
return
the
v
alue
assigned
to
it.
Otherwise,
let
op
b
e
the
lab
el
of
the
corresp
onding
v
ertex.
.
Compute
recursiv
ely

 
V
ALU
E
(C
x
;
path

0)
and

 
V
ALU
E
(C
x
;
path

).
.
Return

op

.
When
computing
this
pro
cedure,
C
x
will
b
e
written
on
the
input
tap
e
and
path
will
b
e
written
on
the
w
ork
tap
e.
A
t
eac
h
lev
el
of
the
recursion,
path
determines
precisely
all
the
previous
recursion
lev
els,
so
the
space
consumption
of
this
algorithm
is
O
(d),
as
required.
Corollary
0.0
N
C


L

N
L

N
C

Pro
of:
The
rst
inclusion
follo
ws
from
Theorem
0.	,
the
second
inclusion
is
trivial
and
the
third
inclusion
is
just
Corollary
0..
Bibliographic
Notes
This
lecture
is
mostly
based
on
[].
F
or
wider
p
ersp
ectiv
e
see
Co
ok's
old
surv
ey
[].
.
A.
Boro
din.
On
relating
time
and
space
to
size
and
depth.
SIAM
J.
on
Computing,
V
ol.
,
No.
,
pages
{,
	.
.
S.A.
Co
ok.
A
taxonom
y
of
problems
with
fast
parallel
algorithms.
Information
and
Contr
ol,
V
ol.
,
pages
{,
	.


LECTURE
0.
CIR
CUIT
DEPTH
AND
SP
A
CE
COMPLEXITY

Lecture

Comm
unication
Complexit
y
Lecture
giv
en
b
y
Ran
Raz
Notes
tak
en
b
y
Amiel
F
erman
and
Noam
Sadot
Summary:
This
lecture
deals
with
Comm
unication
Complexit
y
,
whic
h
is
the
analysis
of
the
amoun
t
of
information
that
needs
to
b
e
comm
unicated
b
y
t
w
o
parties
whic
h
are
in
terested
in
reac
hing
a
common
computational
goal.
W
e
start
with
some
basic
deni-
tions
and
simple
examples.
W
e
con
tin
ue
to
consider
b
oth
deterministic
and
probabilistic
mo
dels
for
the
problem,
and
then
w
e
dev
elop
a
com
binatorial
to
ol
to
help
us
with
the
pro
ofs
of
lo
w
er
b
ounds
for
comm
unication
problems.
W
e
conclude
b
y
pro
ving
a
proba-
bilistic
linear
comm
unication
complexit
y
lo
w
er
b
ound
for
the
problem
of
computing
the
inner
pro
duct
of
t
w
o
v
ectors
where
initially
eac
h
part
y
holds
one
v
ector.
.
In
tro
duction
The
comm
unication
problem
arises
when
t
w
o
or
more
parties
(i.e.
pro
cesses,
systems
etcetera)
need
to
carry
out
a
task
whic
h
could
not
b
e
carried
out
alone
b
y
eac
h
of
them
b
ecause
of
lac
k
of
information.
Th
us,
in
order
to
ac
hiev
e
some
common
goal,
dened
as
a
function
of
their
inputs,
the
parties
need
to
comm
unicate.
Often,
the
form
ulation
of
a
problem
as
a
comm
unication
problem
serv
es
merely
as
a
con
v
enien
t
abstraction;
for
example,
a
task
that
needs
to
share
information
b
et
w
een
dieeren
t
parts
of
the
same
CPU
could
b
e
form
ulated
as
suc
h.
Comm
unication
complexit
y
is
concerned
with
anala
ysing
the
amoun
t
of
information
that
m
ust
b
e
comm
unicated
b
et
w
een
the
dieren
t
parties
in
order
to
correctly
p
erform
the
in
tended
task.
.
Basic
mo
del
and
some
examples
In
order
to
in
v
estigate
the
general
problem
of
comm
unication
w
e
state
a
few
simplied
assumptions
on
our
mo
del:
.
There
are
only
t
w
o
parties
(called
pla
y
er

and
pla
y
er
)
.
Eac
h
part
y
has
unlimited
computing
p
o
w
er
and
w
e
are
only
concerned
with
the
comm
unica-
tion
complexit
y
.
The
task
is
a
computation
of
a
predened
function
of
the
input



LECTURE
.
COMMUNICA
TION
COMPLEXITY
As
w
e
shall
see,
this
mo
del
is
ric
h
enough
to
study
some
non-trivial
and
in
teresting
asp
ects
of
comm
unication
complexit
y
.
The
input
domains
of
pla
y
er

and
pla
y
er

are
the
(nite)
sets
X
and
Y
resp
ectiv
ely
.
The
t
w
o
pla
y
ers
start
with
inputs
x

X
and
y

Y
,
and
their
task
is
to
compute
some
predened
function
f
(x;
y
).
A
t
eac
h
step,
the
comm
unication
proto
col
sp
ecies
whic
h
bit
is
sen
t
b
y
one
of
the
pla
y
ers
(alternately),
and
this
is
based
on
information
comm
unicated
so
far
as
w
ell
as
on
the
initial
inputs
of
the
pla
y
ers.
Let
us
see
a
few
examples:
.
Equalit
y
F
unction
(denoted
E
Q):
The
function
f
(x;
y
)
is
dened
as:
f
(x;
y
)
=

if
x
=
y
f
(x;
y
)
=
0
if
x
=
y
That
is,
the
t
w
o
pla
y
ers
are
in
terested
to
kno
w
wheather
their
initial
inputs
are
equal.
.
Disjoin
tness:
The
inputs
are
subsets:
x;
y

f;
:
:
:
;
ng
f
(x;
y
)
=

i
x
\
y
=
;
.
Inner
Pro
duct
(denoted
I
P
):
The
inputs
are:
x;
y

f0;
g
n
f
(x;
y
)
=
P
n
i
x
i

y
i
mo
d

.
Deterministic
v
ersus
Probabilistic
Complexit
y
W
e
b
egin
with
some
denitions:
Denition
.
A
deterministic
pr
oto
c
ol
P
with
domain
X

Y
and
with
r
ange
Z
(wher
e
X
and
Y
ar
e
the
input
domains
of
player

and
player

r
esp
e
ctively
and
Z
is
the
domain
of
the
function
f
)
is
dene
d
as
a
deterministic
algorithm
in
which
at
e
ach
step
it
sp
e
cies
a
bit
to
b
e
sent
fr
om
one
of
the
players
to
the
other.
The
output
of
the
algorithm,
denote
d
P
(x;
y
)
(on
inputs
x
and
y),
is
the
output
of
e
ach
of
the
players
at
the
end
of
the
pr
oto
c
ol
and
it
is
r
e
quir
e
d
that:
x

X
;
y

Y
:
P
(x;
y
)
=
f
(x;
y
)
Denition
.
The
c
ommunic
ation
c
omplexity
of
a
deterministic
pr
oto
c
ol
is
the
worst
c
ase
num-
b
er
of
bits
sent
by
the
pr
oto
c
ol
for
some
inputs.
Denition
.
The
c
ommunic
ation
c
omplexity
of
a
function
f
is
the
minimum
c
omplexity
of
al
l
deterministic
pr
oto
c
ols
which
c
ompute
f
.
This
is
denote
d
by
C
C
(f
).
A
natural
relaxation
of
the
ab
o
v
e
dened
deterministic
proto
col
w
ould
b
e
to
allo
w
eac
h
pla
y
er
to
toss
coins
during
his
computation.
This
means
that
eac
h
pla
y
er
has
an
access
to
a
random
string
and
the
proto
col
that
is
carried
out
dep
ends
on
this
string.
The
w
a
y
to
form
ulate
this
is
to
determine
a
distribution

from
whic
h
the
random
strings
eac
h
pla
y
er
uses
are
sampled
uniformly
,
once
the
strings
are
c
hosen,
the
proto
col
that
is
carried
out
b
y
eac
h
of
the
pla
y
ers
is
completely

..
EQUALITY
REVISITED
AND
THE
INPUT
MA
TRIX

deterministic.
W
e
consider
the
Mon
te-Carlo
mo
del,
that
is,
the
proto
col
should
b
e
correct
for
a
large
fraction
of
the
strings
in
.
Note
that
the
ab
o
v
e
description
of
a
randomized
proto
col
implicitly
allo
ws
for
t
w
o
kinds
of
p
ossibilities:
one
in
whic
h
the
string
that
is
initially
sampled
from

is
common
to
b
oth
pla
y
ers,
and
the
other
is
that
eac
h
pla
y
er
initially
samples
his
o
wn
priv
ate
string
so
that
the
string
sampled
b
y
one
pla
y
er
is
not
visible
to
the
other
pla
y
er.
These
t
w
o
p
ossibilites
are
called
resp
ectiv
ely
the
public
and
the
priv
ate
mo
del.
Ho
w
are
these
t
w
o
mo
dels
related
?
First
of
all,
it
is
clear
that
an
y
priv
ate
proto
col
can
b
e
sim
ulated
as
a
public
proto
col:
the
strings
sampled
priv
ately
b
y
eac
h
user
are
concatenated
and
serv
e
as
the
public
string.
It
turns
out
that
a
w
eak
er
reduction
exists
in
the
other
direction:
an
y
public
proto
col
can
b
e
sim
ulated
as
a
priv
ate
proto
col
with
a
small
increase
in
the
error
and
an
additiv
e
of
O
(log
n)
bits
of
comm
unication;
the
idea
of
the
pro
of
is
to
sho
w
that
an
y
public
proto
col
can
b
e
transformed
to
a
proto
col
whic
h
uses
the
same
amoun
t
of
comm
unication
bits
but
only
O
(log
n
+
log

 
)
random
bits
with
an
increase
of

in
the
error.
Next,
eac
h
pla
y
er
can
sample
a
string
of
that
length
and
send
it
to
the
other
pla
y
er
th
us
causing
an
increase
of
O
(log
n
+
log

 
)
in
the
comm
unication
complexit
y
and
of

in
the
error.
In
view
of
these
results
w
e
shall
conne
ourselv
es
to
the
public
mo
del.
Denition
.
A
r
andomize
d
pr
oto
c
ol
P
is
dene
d
as
an
algorithm
which
initial
ly
samples
uni-
formly
a
string
fr
om
some
distribution

and
then
c
arries
on
exactly
as
in
the
deterministic
c
ase.
The
sample
d
string
is
c
ommon
to
b
oth
player,
i.e.
-
this
is
the
public
mo
del.
It
is
r
e
quir
e
d
that
an

-
err
or
pr
oto
c
ol
wil
l
satisfy:
x

X
;
y

Y
P
r
r

R

[P
(x;
y
)
=
f
(x;
y
)]


 
Note
that
in
a
randomized
proto
col,
the
n
um
b
er
of
bits
comm
unicated
ma
y
v
ary
for
the
same
input
(due
to
dieren
t
random
strings).
Hence
the
comm
unication
complexit
y
is
dened
with
resp
ect
to
the
strings
sampled
from
.
One
can
dene
the
comm
unication
complexit
y
of
a
proto
col,
view
ed
as
a
random
v
ariable
(with
resp
ect
to
the
distribution
,
and
the
w
orst
p
ossible
input),
in
the
a
v
erage
case.
Ho
w
ev
er
w
e
prefer
the
somewhat
stronger
w
orst-case
b
eha
viour:
Denition
.
The
c
ommunic
ation
c
omplexity
of
a
r
andomize
d
pr
oto
c
ol
P
on
input
(x;
y
)
is
the
maximum
numb
er
of
bits
c
ommunic
ate
d
in
the
pr
oto
c
ol
for
any
choic
e
of
initial
r
andom
strings
of
e
ach
player.
The
c
ommunic
ation
c
omplexity
of
a
r
andomize
d
pr
oto
c
ol
P
is
the
maximum
c
ommu-
nic
ation
c
omplexity
of
P
over
al
l
p
ossible
inputs
(x;
y
)
Denition
.
The
c
ommunic
ation
c
omplexity
of
a
function
f
c
ompute
d
with
err
or
pr
ob
ability
,
denote
d
C
C

(f
)
is
the
minimum
c
ommunic
ation
c
omplexity
of
a
pr
oto
c
ol
P
which
c
omputes
f
with
err
or
pr
ob
ability
.
In
Lecture

w
e
ha
v
e
actually
considered
a
priv
ate
case
of
the
ab
o
v
e
denition,
one
in
whic
h
there
is
no
error;
i.e.,
C
C
0
(f
).
.
Equalit
y
revisited
and
the
Input
Matrix
Recall
the
Equalit
y
F
unction
dened
in
Section
,
in
whic
h
b
oth
pla
y
ers
wish
to
kno
w
wheather
their
inputs
(whic
h
are
n-bit
strings)
are
equal
(i.e.
whether
x
=
y
).
Let
us
rst
presen
t
a
randomized
proto
col
whic
h
computes
E
Q
with
a
constan
t
error
probabilit
y
and
a
constan
t
comm
unication
complexit
y:
Proto
col
for
pla
y
er
i
(i
=
,)
(input

=
x
and
input

=
y
):


LECTURE
.
COMMUNICA
TION
COMPLEXITY
.
sample
uniformly
an
n-bit
string
r
(this
r
is
common
to
b
oth
pla
y
ers
-
public
mo
del)
.
compute
<
input
i
;
r
>

(the
inner
pro
duct
of
input
i
and
r
mo
d
)
.
send
the
pro
duct
computed
and
receiv
e
the
pro
duct
computed
b
y
the
other
pla
y
er
(single
bit)
.
if
the
t
w
o
bits
are
equal
then
output

else
output
0
If
the
inputs
are
equal,
i.e.
x
=
y
,
then
clearly
<
x;
r
>

=<
y
;
r
>

for
all
r
-s
and
th
us
eac
h
pla
y
er
will
receiv
e
and
send
the
same
bit
and
will
decide
.
Ho
w
ev
er,
if
x
=
y
,
then
for
a
string
r
sampled
uniformly
w
e
ha
v
e
that
<
x;
r
>

=<
y
;
r
>

with
probabilit
y
exactly
one
half.
Th
us,
the
error
probabilit
y
of
a
single
iteration
of
the
ab
o
v
e
proto
col
is
exactly
one
half.
Since
at
eac
h
iteration
w
e
sample
a
random
string
r
indep
endan
tly
from
other
iterations
w
e
get
that
after
carrying
out
the
proto
col
for
exactly
C
times
the
error
probabilit
y
is
exactly

 C
.
F
urthermore,
since
the
n
um
b
er
of
bits
comm
unicated
in
eac
h
iteration
is
constan
t
(exactly
t
w
o
bits),
w
e
get
that
after
C
iterations
of
the
ab
o
v
e
proto
col,
the
comm
unication
complexit
y
is
O
(C
).
Hence,
if
C
is
a
constan
t,
w
e
get
b
oth
a
constan
t
error
probabilit
y
and
a
constan
t
comm
unication
complexit
y
(
 c
and
O
()
resp
ectiv
ely
for
a
constan
t
c).
Ho
w
ev
er,
if
w
e
c
ho
ose
C
to
b
e
equal
to
l
og
(n)
then
the
error
probabilit
y
and
the
comm
unication
complexit
y
will
b
e,
resp
ectiv
ely
,

n
and
O
(l
og
(n)).
W
e
no
w
presen
t
an
alternativ
e
proto
col
for
solving
E
Q
that
also
ac
hiev
es
an
error
probabilt
y
of

n
and
comm
unication
complexit
y
of
O
(l
og
(n)).
In
terestingly
,
this
proto
col
is
(already)
in
the
priv
ate
mo
del:
W
e
presen
t
b
oth
(n-bit
strings)
inputs
as
the
co
ecien
ts
of
p
olynomials
o
v
er
GF
(p)
where
p
is
an
arbitrary
xed
prime
b
et
w
een
n

and
n

(results
in
n
um
b
er
theory
guaran
tee
the
existence
of
suc
h
a
prime).
So,
b
oth
inputs
ma
y
b
e
view
ed
as:
input
of
pla
y
er
:
A(x)
=
n 
X
i=0
a
i

x
i
mo
d
p
input
of
pla
y
er
:
B
(x)
=
n 
X
i=0
b
i

x
i
mo
d
p
Proto
col
for
pla
y
er

(F
or
pla
y
er
:
just
rev
erse
A
and
B
)
.
c
ho
ose
uniformly
a
n
um
b
er
t
in
GF
(p)
.
compute
A(t)
.
send
b
oth
t
and
A(t)
to
the
other
pla
y
er
.
receiv
e
s
and
B
(s)
from
other
pla
y
er
.
if
A(s)
=
S
then
decide

else
decide
0
Clearly
,
if
the
inputs
are
equal
then
so
are
the
p
olynomials
and
th
us
necessarily
A(t)
=
B
(t)
for
ev
ery
t

GF
(p).
If
ho
w
ev
er,
A
=
B
,
then
these
p
olynomials
ha
v
e
at
most
n
 
p
oin
ts
on
whic
h
they
agree
(i.e.
t-s
for
whic
h
A(t)
=
B
(t))
since
their
dierence
is
a
p
olynomial
of
degree

..
EQUALITY
REVISITED
AND
THE
INPUT
MA
TRIX

n
 
whic
h
can
ha
v
e
at
most
n
 
ro
ots.
So
the
probabilit
y
of
error
in
this
case
is
n 
p

n
n

=

n
.
Notice
that
since
t
and
B
(t)
are
O
(l
g
n)
bits
long,
w
e
ma
y
conclude
that
C
C

n
(E
Q)
=
O
(l
g
n).
Pro
ofs
of
lo
w
er
b
ounds
whic
h
relate
to
certain
families
of
algorithms
usually
necessitate
a
formalization
that
could
express
in
a
non-trivial
w
a
y
the
underlying
structure.
In
our
case,
a
com
binatorial
view
pro
v
es
to
b
e
eectiv
e:
(Recall
that
the
input
domains
of
b
oth
parties
are
denoted
X
and
Y
)
w
e
ma
y
view
the
proto
col
as
a
pro
cess
whic
h
in
eac
h
of
its
steps
partitions
the
input
space
X

Y
in
to
disjoin
t
sets
suc
h
that
at
step
t
eac
h
set
includes
exactly
all
input
pairs
whic
h
according
to
their
rst
t
bits
cause
the
proto
col
to
\act"
the
same
(i.e.,
comm
unicate
exactly
the
same
messages
during
the
algorithm).
In
tuitiv
ely
,
eac
h
set
at
the
end
of
this
partioning
pro
cess
is
comprised
of
exactly
all
pairs
of
inputs
that
\cause"
the
proto
col
to
reac
h
the
same
conclusion
(i.e.,
compute
the
same
output).
A
nice
w
a
y
to
visualize
this
is
to
use
a
matrix:
eac
h
ro
w
corresp
onds
to
a
y

Y
and
eac
h
column
corresp
onds
to
a
x

X
.
The
v
alue
of
the
matrix
in
p
osition
(i;
j
)
is
simply
f
(i;
j
),
where
f
is
the
function
b
oth
parties
need
to
compute.
This
matrix
is
called
the
Input
Matrix.
Since
the
Input
Matrix
is
just
another
w
a
y
to
describ
e
the
function
f
,w
e
ma
y
c
ho
ose
to
talk
ab
out
the
comm
unication
complexit
y
of
an
Input
Matrix
A
-
den
to
ed
C
C
(A)
instead
of
the
comm
unication
complexit
y
of
the
corresp
onding
function
f
.
F
or
example,
the
matrix
corresp
onding
to
the
Equalit
y
F
unction
is
the
inden
tit
y
matrix
(since
the
output
of
the
Equalit
y
F
unction
m
ust
b
e

i
the
inputs
are
of
the
form
(i;
i)
for
eac
h
input
pair).
The
ab
o
v
e
men
tioned
partioning
pro
cess
can
no
w
b
e
view
ed
as
a
partioning
of
the
matrix
in
to
sets
of
matrix
elemen
ts.
It
turns
out
that
these
sets
ha
v
e
a
sp
ecial
structure,
namely
rectangles.
F
ormally
,
w
e
dene
Denition
.
A
rectangle
in
X

Y
is
a
subset
R

X

Y
such
that
R
=
A

B
for
some
A

X
and
B

Y
.
(Note
that
elemen
ts
of
the
rectangle,
as
dened
ab
o
v
e,
need
not
b
e
adjacen
t
in
the
input
matrix.)
Ho
w
ev
er,
in
order
to
relate
our
discussion
to
this
denition
w
e
need
an
alternativ
e
c
haracteri-
zation
of
rectangles
giv
en
in
the
next
prop
osition:
Prop
osition
..
R

X

Y
is
a
r
e
ctangle
i
(x

;
y

)

R
and
(x

;
y

)

R
)
(x

;
y

)

R
Pro
of:
)
If
R
=
A

B
is
a
rectangle
then
from
(x

;
y

)

R
w
e
get
that
x


A
and
from
(x

;
y

)

B
w
e
get
that
y


B
and
so
w
e
get
that
(x

;
y

)

A

B
=
R
.
(
W
e
dene
the
sets
A
=
fxj	y
s:t:
(x;
y
)

R
g
and
B
=
fy
j	x
s:t:
(x;
y
)

R
g.
On
the
one
hand
it
is
clear
that
R

A

B
(directly
from
A
and
B
's
denition).
On
the
other
hand,
supp
ose
(x;
y
)

A

B
.
Then
since
x

A
there
is
a
y
0
suc
h
that
(x;
y
0
)

R
and
similiarly
there
is
an
x
0
suc
h
that
(x
0
;
y
)

R
from
this,
according
to
the
assumption
w
e
ha
v
e
that
(x;
y
)

R
.
W
e
shall
no
w
sho
w
that
the
sets
of
matrix
elemen
ts
partitioned
b
y
the
proto
col
in
the
sense
describ
ed
ab
o
v
e
actually
form
a
partition
of
the
matrix
in
to
rectangles:
Supp
ose
b
oth
pairs
of
inputs
(x

;
y

)
and
(x

;
y

)
cause
the
proto
col
to
exc
hange
the
same
sequence
of
messages.
Since
the
rst
pla
y
er
(with
input
x

)
cannot
distinguish
at
eac
h
step
b
et
w
een
(x

;
y

)
and
(x

;
y

)
(he
computes
a
function
of
x

and
the
messages
so
far
in
an
y
case)
then
he
will
comm
unicate
the
same
message
to
pla
y
er

in
b
oth
cases.
Similarly
,
pla
y
er

cannot
distinguish
at
eac
h
step
b
et
w
een
(x

;
y

)
and
(x

;
y

)
and
will
act
the
same
in
b
oth
cases.
W
e
sho
w
ed
that
if
the
proto
col
acts
the
same
on
inputs
(x

;
y

)
and
(x

;
y

)
then
it
will
act
the
same
on
input
(x

;
y

),
whic
h,
using
prop
osition
..
establishes
the
fact
that
the
set
of
inputs
on
whic
h
the
pro
cto
col
b
eha
v
es
the
same
is
a
rectangle.


LECTURE
.
COMMUNICA
TION
COMPLEXITY
Since
the
comm
unication
is
the
same
during
the
proto
col
for
the
pair
of
inputs
(x

;
y

)
and
(x

;
y

)
(and
for
the
pairs
of
inputs
in
the
rectangle
dened
b
y
them,
as
w
as
explained
in
the
last
paragraph)
then
the
protcol's
output
m
ust
b
e
the
same
for
these
pairs,
and
this
implies
that
the
v
alue
of
the
f
function
m
ust
b
e
the
same
to
o.
Th
us,
a
deterministic
proto
col
partitions
the
Input
Matrix
in
to
rectangles
whose
elemen
ts
are
iden
tical,
that
is,
the
proto
col
computes
the
same
output
for
eac
h
pair
of
inputs
in
the
rectangle.
W
e
sa
y
that
a
deterministic
proto
col
partitions
the
Input
Matrix
in
to
rectangles
of
mono
c
hromatic
rectangles
(where
color
is
iden
tied
with
the
input
matrix
v
alue).
Since
at
eac
h
step
the
proto
col
partitions
the
Input
Matrix
in
to
t
w
o
(usually
not
equal
in
size)
parts
w
e
ha
v
e
the
follo
wing:
F
act
..
A
deterministic
pr
oto
c
ol
P
of
c
ommunic
ation
c
omplexity
k
p
artitions
the
Input
Matrix
into
at
most

k
mono
chr
omatic
r
e
ctangles
Recalling
the
fact
that
the
Input
Matrix
of
a
proto
col
to
the
equalit
y
problem
is
the
iden
tit
y
matrix,
then
since
the
smallest
mono
c
hromatic
rectangle
that
con
tains
eac
h
en
try
of

in
the
matrix
is
the
singleton
matrix
whic
h
con
tains
exactly
one
elemen
t
and
since
the
matrix
is
of
size

n


n
(for
inputs
of
size
n),
w
e
get
that
ev
ery
proto
col
for
the
equalit
y
problem
m
ust
ha
v
e
partitioned
the
Input
Matrix
in
to
at
least

n
+

mono
c
hromatic
rectangles
(
n
for
the
's
and
at
least

for
the
zeros).
Th
us,
from
F
act
..
and
from
the
trivial
proto
col
for
solving
E
Q
in
whic
h
pla
y
er

sends
its
input
to
the
pla
y
er

and
pla
y
er

sends
to
pla
y
er

the
bit

i
the
inputs
are
equal
(n
+

bits
of
comm
unication),
w
e
get
the
follo
wing
corollary:
Corollary
.
C
C
(E
Q)
=
n
+

.
Rank
Lo
w
er
Bound
Using
the
notion
of
an
Input
Matrix
dev
elop
ed
in
the
previous
section,
w
e
no
w
state
and
pro
v
e
a
useful
theorem
regarding
the
lo
w
er
b
ound
of
comm
unication
complexit
y:
Theorem
.	
L
et
A
b
e
an
Input
Matrix
for
a
c
ertain
function
f
,
then
C
C
(A)

l
og

(r
A
)
wher
e
r
A
is
the
r
ank
of
A
over
any
xe
d
eld
F
Pro
of:
The
pro
of
is
b
y
induction
on
C
C
(A).
Induction
Base:
If
C
C
(A)
=
0
then
this
means
that
b
oth
sides
w
ere
able
to
compute
the
function
f
without
an
y
comm
unication.
This
means
that
for
ev
ery
pair
of
inputs,
b
oth
sides
compute
a
constan
t
function
whic
h
could
b
e
either
f
(x;
y
)
=

or
f
(x;
y
)
=
0
for
all
x
and
y
.
This
implies
that
A
m
ust
b
e
the
all
0-s
or
the
all
-s
matrix,
and
so,
b
y
denition
r
A

0;

.
Th
us,
indeed,
C
C
(A)

l
og

(r
A
)
as
required.
Induction
Step:
Supp
ose
the
claim
is
true
for
C
C
(A)

n
 
and
w
e
shall
pro
v
e
the
claim
for
C
C
(A)
=
n.
Consider
the
rst
bit
sen
t:
This
bit
actually
partitions
A
in
to
t
w
o
matrices
A
0
and
A

suc
h
that
the
rest
of
the
proto
col
can
b
e
seen
as
a
protcol
that
relates
to
only
one
of
these
matrices.
Since
the
maximal
comm
unication
complexit
y
needed
for
b
oth
matrices
cannot
surpass
n
 
(otherwise
C
C
(A)
could
not
ha
v
e
b
een
equal
to
n),
w
e
get
the
follo
wing
equation:
C
C
(A)


+
M
axfC
C
(A
0
);
C
C
(A

)g


+
M
axfl
og

(r
A
0
);
l
og

(r
A

)g
(.)
where
the
second
inequalit
y
is
b
y
the
induction
h
yp
othesis.
No
w,
since
r
A

r
A
0
+
r
A

,
w
e
ha
v
e
that
r
A



M
axfr
A
0
;
r
A

g.
Put
dieren
tly
,
w
e
ha
v
e
M
axfl
og

(r
A
0
);
l
og

(r
A

)g

l
og

(r
A
)
 .
Com
bining
this
with
Eq.
(.)
w
e
get
that
C
C
(A)


+
l
og

(r
A
)
 
=
l
og

(r
A
).

..
INNER-PR
ODUCT
LO
WER
BOUND
	
Applying
this
theorem
to
the
Input
Matrix
of
the
equalit
y
problem
(the
iden
tit
y
matrix),
w
e
easily
get
the
lo
w
er
b
ound
C
C
(E
Q)

n.
A
linear
lo
w
er
b
ound
for
the
deterministic
comm
unication
complexit
y
of
the
Inner
Pro
duct
problem
can
also
b
e
ac
hiev
ed
b
y
applying
this
theorem.
In
the
next
section
w
e'll
see
a
linear
lo
w
er
b
ound
on
the
randomized
comm
unication
complexit
y
of
the
Inner
Pro
duct
function.
.
Inner-Pro
duct
lo
w
er
b
ound
Recalling
Inner-Pro
duct
problem
from
section
.,
w
e
pro
v
e
the
follo
wing
result:
Theorem
.0
C
C

(I
P
)
=

(n)
T
o
simplify
the
pro
of,
and
the
mathematical
tec
hniques
needed
for
the
pro
of,
w
e
will
assume
that
0
<

<
(


 
),
for
arbitarary
small

>
0.
T
o
pro
v
e
the
ab
o
v
e
theorem,
w
e
assume
that
there
is
a
probabilistic
comm
unication
proto
col
P
in
the
public
coin
mo
del
using
random
string
R
that
uses
less
than
n
comm
unication
bits,
and
w
e
will
sho
w
a
con
tradiction.
By
denition,
w
e
kno
w
that
Pr
R
[P
R
(x;
y
)
=
f
(x;
y
)]


 
for
ev
ery
string
x
and
y
.
Since
it
is
true
for
eac
h
pair
of
strings,
the
follo
wing
prop
ert
y
is
true:
Pr
(x;y
);R
[P
R
(x;
y
)
=
f
(x;
y
)]


 
in
whic
h
the
probabilit
y
measure
o
v
er
(x;
y
)
is
tak
en
as
the
uniform
probabilit
y
o
v
er
all
suc
h
pairs.
Changing
the
order
of
the
probabilit
y
measures,
w
e
obtain:
Pr
R;(x;y
)
[P
R
(x;
y
)
=
f
(x;
y
)]


 
And
since

<
,
w
e
conclude
that
there
is
a
xed
random
string
r
that
for
the
deterministic
proto
col
induced
b
y
P
r
,
the
follo
wing
prop
ert
y
exists:
Pr
(x;y
)
[P
r
(x;
y
)
=
f
(x;
y
)]


 
By
this
metho
d,
w
e
pro
duce
a
deterministic
proto
col
on
whic
h
w
e
can
w
ork
no
w
and
pro
v
e
lo
w
er
b
ound.
In
con
trast
to
the
previous
section,
the
proto
col
P
r
should
w
ork
w
ell
only
for
most
of
the
inputs,
but
not
necessarily
for
all
of
them.
Pro
ving
lo
w
er
b
ound
on
this
deterministic
proto
col
P
r
will
immediately
giv
e
the
lo
w
er
b
ound
of
the
original
randomized
proto
col
P
.
The
metho
d
for
pro
ving
the
lo
w
er
b
ound
is
to
sho
w
that
there
are
not
\big"
enough
rectangles
that
are
not
balanced
in
the
input
matrix
after
n
time,
and
hence
to
conclude
that
w
e
need
more
than
that
time.
The
follo
wing
denition
will
b
e
helpful:
Denition
.
A
r
e
ctangle
U

V

f0;
g
n

f0;
g
n
is
big
if
its
size
satises
jU

V
j


n( )
.
Otherwise,
the
r
e
ctangle
is
small.
Note
that
there
m
ust
b
e
at
least
one
big
rectangle
in
the
ab
o
v
e
matrix.
Otherwise,
using
F
act
..
and
the
fact
that
w
e
ha
v
e
at
most

n
rectangles
(for
at
most
n
comm
uniaction),
w
e
infer
that
the
size
of
the
en
tire
matrix
is
not
more
than

n


n( )
=

n( 

)
<

n
whic
h
leads
to
a
con
tradiction.

	0
LECTURE
.
COMMUNICA
TION
COMPLEXITY
Claim
..
If
P
r
works
in
n
time
then
ther
e
exists
a
big
r
e
ctangle
U

V

f0;
g
n

f0;
g
n
such
that
f
(x;
y
)
is
the
same
for
at
le
ast

 
fr
action
of
the
elements
in
the
r
e
ctangle
U

V
.
Pro
of:
T
o
pro
v
e
the
claim,
w
e
recall
that
Pr
(x;y
)
[P
r
(x;
y
)
=
f
(x;
y
)]


 .
In
other
w
ords,
at
most

fraction
of
the
elemen
ts
in
the
matrix
do
not
satisfy
P
r
(x;
y
)
=
f
(x;
y
).
In
addition,
after
at
most
n
time,
w
e
will
ha
v
e
a
partition
of
the
matrix
in
to
at
most

n
rectangles.
By
Denition
.,
eac
h
small
rectangle
has
size
less
than

n( )
,
and
so
the
n
um
b
er
of
the
elemen
ts
that
b
elong
to
small
rectangles
is
less
than

n


n( )
=

n( 

)
<

n 
,
and
so
big
rectangles
con
tain
more
than
half
of
the
matrix
elemen
ts.
Th
us,
if
all
big
rectangles
ha
v
e
more
than

error,
the
total
error
due
only
to
these
rectangles
w
ould
b
e
more
than
,
whic
h
leads
to
a
con
tradiction.
The
claim
follo
ws.
By
using
this
claim,
w
e
x
a
big
rectangle
R
that
satises
the
conditions
of
the
previous
claim.
Without
loss
of
generalit
y
,
w
e
can
assume
that
the
ma
jorit
y
of
this
rectangle
is
0.
If
it
is
not
0,
w
e
just
switc
h
ev
ery
elemen
t
in
the
matrix.
Let
us
denote
b
y
B
n
the

n


n
input
matrix
for
Inner
Pro
duct
problem,
whic
h
lo
oks
lik
e
that:
B
n
=

x

y
mo
d


(x;y
)f0;g
n
f0;g
n
The
inner
elemen
ts
are
scalar
pro
ducts
on
the
eld
GF
().
The
matrix
B
n
con
tains
t
w
o
t
yp
es
of
elemen
ts:
zero
es
and
ones.
By
switc
hing
eac
h
0
in
to

and
eac
h

in
to
 ,
w
e
get
a
new
matrix
H
n
(whic
h
is
a
v
ersion
of
Hadamard
matrix)
that
lo
oks
lik
e
that:
H
n
=

( )
xy

(x;y
)f0;g
n
f0;g
n
This
matrix
has
the
follo
wing
prop
ert
y:
Claim
..
H
n
is
an
ortho
gonal
matrix
over
the
r
e
als.
Pro
of:
W
e
will
pro
v
e
that
eac
h
t
w
o
ro
ws
in
the
matrix
H
n
are
orthogonal.
Let
r
x
b
e
a
ro
w
corresp
onds
to
a
string
x
and
r
z
b
e
a
ro
w
corresp
onds
to
a
string
z
=
x.
The
scalar
pro
duct
b
et
w
een
these
t
w
o
ro
ws
is
X
y
f0;g
n
( )
xy

( )
z
y
=
X
y
f0;g
n
( )
y
where

=
x

z
.
Since
x
=
z
there
is
an
index
j

f;
:
:
:
;
ng
suc
h
that

j
=
,
then
the
previous
expression
is
equal
X
y
f0;g
n
( )
P
i=j
y
i

i
+y
j
(.)
Let
us
denote
b
y
y
0
=
y

:
:
:
y
j
 
y
j
+
:
:
:
y
n
,
then
w
e
can
write
(.)
as:
X
y
0
X
y
j
( )
P
i=j
y
i

i
+y
j
=
X
y
0
( )
P
i=j
y
i

i
X
y
j
( )
y
j

..
INNER-PR
ODUCT
LO
WER
BOUND
	
Clearly
,
X
y
j
f0;g
( )
y
j
=
 
+

=
0
whic
h
pro
v
es
the
claim.
W
e
ha
v
e

n
ro
ws
and
columns
in
the
matrix
H
n
.
Let
us
en
umerate
the
ro
ws
b
y
r
i
,
for
i
=
0;
;
:
:
:
;

n
 .
Then
b
y
the
previous
claim,
w
e
ha
v
e
the
follo
wing
prop
erties,
where
here

denotes
inner
pro
duct
o
v
er
the
reals:
.
r
i

r
j
=
0
for
i
=
j
.
r
i

r
i
=
kr
i
k

=

n
for
i
=
0;
;
:
:
:
;

n
 .
This
follo
ws
easily
from
the
fact
that
the
absolute
v
alue
of
eac
h
elemen
t
in
H
n
is
.
Th
us,
the
ro
ws
in
the
matrix
dene
an
orthogonal
base
o
v
er
the
reals.
The
follo
wing
denition
will
b
e
helpful
in
the
construction
of
the
pro
of
of
Theorem
.0:
Denition
.
(discrep
ency):
The
discrep
ency
of
a
r
e
ctangle
U

V
is
dene
d
as
D
(U

V
)
=






X
(x;y
)U
V
( )
f
(x;y
)






Let
R
0
b
e
a
big
rectangle
(of
small
error)
as
guaran
teed
b
y
Claim
...
Supp
ose
without
loss
of
generalit
y
that
R
0
has
a
ma
jorit
y
of
zeros
(i.e.,
at
least

 
fraction
of
0's).
Recall
that
the
size
of
R
0
is
at
least

n( )
.
Th
us,
R
0
has
a
big
discrep
ency;
that
is,
D
(R
0
)

(
 
 )


n( )
=
(
 )


n( )
(.)
On
the
other
hand,
w
e
ha
v
e
an
upp
er
b
ound
on
the
discrep
ency
of
an
y
rectangle
(an
in
particular
of
R
0
):
Lemma
..
The
discr
ep
ency
of
any
r
e
ctangle
R
is
b
ounde
d
fr
om
ab
ove
by

 n



n
Pro
of:
Let
us
denote
R
=
U

V
.
The
matrix
H
n
has
the
prop
ert
y
that
eac
h
bit
b
in
B
n
c
hanges
in
to
( )
b
in
H
n
.
Let
us
consider
the
follo
wing
c
haractaristic
v
ector
I
U
:
f0;
g
n
!
f0;
g
n
that
is
dened
in
the
follo
wing
w
a
y:
I
U
(x)
=


if
x

U
0
otherwise
Observ
e
that
I
U

r
j
is
exactly
the
n
um
b
er
of
's
min
us
the
n
um
b
er
(-)'s
in
r
j
,
so
D
(U

V
)
=






X
j
V
I
U

r
j







X
j
V
jI
U

r
j
j

X
j
f0;g
n
jI
U

r
j
j
where
b
oth
inequalities
are
trivial.
Using
Cauc
h
y-Sc
h
w
artz
inequalit
y
(for
the
second
line),
w
e
obtain
D
(R
)

X
j
f0;g
n


jI
U

r
j
j

s

n

X
j
f0;g
n
jI
U

r
j
j

(.)

	
LECTURE
.
COMMUNICA
TION
COMPLEXITY
Recalling
that
H
n
is
an
orthogonal
matrix,
and
the
norm
of
eac
h
ro
w
is
p

n
,
w
e
denote
^
r
j
=

p

n
r
j
whic
h
dene
an
orthonormal
base.
With
this
notation,
Eq.
(.)
can
b
e
written
as:
s

n

X
j
f0;g
n
jI
U

p

n
^
r
j
j

=
s

n


n

X
j
f0;g
n
jI
U

^
r
j
j

=

n

s
X
j
f0;g
n
jI
U

^
r
j
j

Since
f
^
r
j
g
j
=0;;:::
;
n
 
is
an
orthonormal
base,
the
square
ro
ot
ab
o
v
e
is
merely
the
norm
of
I
U
(as
the
norm
is
in
v
arian
t
o
v
er
all
orthonormal
bases).
Ho
w
ev
er,
lo
oking
at
the
\standard"
(p
oin
t-wise)
base,
w
e
ha
v
e
that
the
norm
of
I
U
is
p
jU
j

p

n
(since
eac
h
elemen
t
in
the
v
ector
I
U
is
0
or
).
T
o
conclude,
w
e
got
that:
D
(R
)


n

p

n
=

n

(.)
whic
h
pro
v
es
the
lemma.
W
e
no
w
deriv
e
a
con
tradiction
b
y
con
trasting
the
upp
er
and
lo
w
er
b
ounds
pro
vided
for
R
0
.
By
Eq.
(.),
w
e
got
that:
D
(R
0
)

(
 )


n( )
whic
h
is
greater
than

n=
for
an
y
0
<

<


and
all
sucien
tly
large
n's
(since
for
suc
h

the
exp
onen
t
is
strictly
bigger
than
n=
whic
h
for
sucien
tly
big
n's
comp
ensates
for
the
small
p
ositiv
e
factor

 ).
In
con
trast,
Lemma
..
applies
also
to
R
0
and
implies
that
D
(R
0
)


n=
,
in
con
tradiction
to
the
ab
o
v
e
b
ound
(of
D
(R
0
)
>

n=
).
T
o
conclude,
w
e
sho
w
ed
a
con
tradiction
to
our
initial
h
yp
othesis
that
the
comminication
com-
plexit
y
is
lo
w
er
than
n.
Theorem
.0
th
us
follo
ws.
Bibliographic
Notes
F
or
further
discussion
of
Comm
unication
Complexit
y
see
the
textb
o
ok
[].
Sp
ecically
,
this
lecture
corresp
onds
to
Chapters

and
.
Comm
unication
Complexit
y
w
as
rst
dened
and
studied
b
y
Y
ao
[],
who
also
in
tro
duced
the
\rectangle-based"
pro
of
tec
hnique.
The
rank
lo
w
er
b
ound
w
as
suggested
in
[].
The
lo
w
er
b
ound
on
the
comm
unication
complexit
y
of
the
Inner
Pro
duct
function
is
due
to
[].
.
B.
Chor
and
O.
Goldreic
h.
Un
biased
Bits
F
rom
Sources
of
W
eak
Randomness
and
Probabilis-
tic
Comm
unication
Complexit
y
.
SIAM
J.
Comp.,
V
ol.
,
No.
,
April
	,
pp.
0{.
.
E.
Kushilevitz
and
N.
Nisan.
Communic
ation
Complexity,
Cam
bridge
Univ
ersit
y
Press,
		.
.
K.
Mehlhorn
and
E.
Sc
hmidt.
Las-V
egas
is
b
etter
than
Determinism
in
VLSI
and
Distributed
Computing.
In
Pr
o
c.
of
th
STOC,
pp.
0-,
	.
.
A.C.
Y
ao.
Some
Complexit
y
Questions
Related
to
Distributiv
e
Computing.
In
Pr
o
c.
of
th
STOC,
pp.
0	-,
		.

Lecture

Monotone
Circuit
Depth
and
Comm
unication
Complexit
y
Lecture
giv
en
b
y
Ran
Raz
Notes
tak
en
b
y
Y
ael
T
auman
and
Y
oa
v
Ro
deh
Summary:
One
of
the
main
goals
of
studying
circuit
complexit
y
is
to
pro
v
e
lo
w
er
b
ounds
on
the
size
and
depth
of
circuits
computing
sp
ecic
functions.
Since
studying
the
general
mo
del
ga
v
e
few
results,
w
e
will
concen
trate
on
monotone
circuits.
The
main
result
is
a
tigh
t
non
trivial
b
ound
on
the
monotone
circuit
depth
of
st-C
onnectiv
ity
.
This
is
pro
v
ed
via
a
series
of
reductions,
the
rst
of
whic
h
is
of
signican
t
imp
ortance:
A
connection
b
et
w
een
circuit
depth
and
comm
unication
complexit
y
.
W
e
then
get
a
com-
m
unication
game
and
pro
ceed
to
reduce
in
to
other
suc
h
games,
un
til
reac
hing
the
game
F
O
R
K
,
and
the
conclusion
that
pro
ving
a
lo
w
er
b
ound
on
its
comm
unication
complex-
it
y
will
giv
e
a
matc
hing
lo
w
er
b
ound
on
the
monotone
circuit
depth
of
st-C
onnectiv
ity
.
.
In
tro
duction
T
uring
mac
hines
are
abstract
mo
dels
used
to
capture
our
concept
of
computation.
Ho
w
ev
er,
w
e
tend
to
miss
some
complexit
y
prop
erties
of
functions
when
examining
them
from
the
T
uring
mac
hine
p
oin
t
of
view.
One
suc
h
cen
tral
prop
ert
y
of
a
function,
is
ho
w
ecien
tly
it
can
b
e
run
in
parallel.
This
prop
ert
y
is
b
est
observ
ed
when
w
e
use
the
circuit
mo
del
|
b
y
the
depth
of
the
circuit
realizing
the
function.
Another
motiv
ation
for
prefering
the
circuit
mo
del
to
the
T
uring
mac
hine
mo
del,
is
the
hop
e
that
using
adv
anced
com
binatorial
metho
ds
will
more
easily
giv
e
lo
w
er
b
ounds
to
the
size
of
circuits,
and
hence
to
the
running
time
of
T
uring
mac
hines.
Recall
that
w
e
need
only
examine
circuits
made
up
of
N
O
T
(:)
O
R
(_)
and
AN
D
(^)
gates,
an
y
other
gate
can
b
e
sim
ulated
with
constan
t
blo
wup
in
the
size
and
depth
of
the
circuit.
W
e
ma
y
also
assume
all
the
N
O
T
gates
are
at
the
leaf
lev
el
b
ecause
using
De-Morgan
rewrite
rules,
w
e
do
not
increase
the
depth
of
the
circuit
at
all,
and
ma
y
increase
its
size
b
y
a
constan
t
factor
of

at
most.
In
this
lecture
w
e
will
only
discuss
b
ounded
fan-in
circuits,
and
therefore
ma
y
assume
all
gates
to
b
e
of
fan-in

(except
N
O
T
).
As
alw
a
ys,
our
goal
is
to
nd
(or
at
least
pro
v
e
the
existence
of
)
hard
functions.
In
the
con
text
of
circuit
complexit
y
w
e
measure
hardness
b
y
t
w
o
parameters:
Denition
.
(Depth,
Size):
Given
f
:
f0;
g
n
!
f0;
g,
we
dene:
	

	LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY
.
D
epth(f
)
def
=
The
minimum
depth
of
a
cir
cuit
c
omputing
f
,
wher
e
the
depth
of
a
cir
cuit
is
the
maximum
distanc
e
fr
om
an
input
le
af
to
the
output
(when
the
cir
cuit
is
viewe
d
as
a
dir
e
cte
d
acyclic
gr
aph).
.
S
iz
e(f
)
def
=
The
minimum
size
of
a
cir
cuit
c
omputing
f
,
wher
e
the
size
of
a
cir
cuit
is
the
numb
er
of
gates
it
c
ontains.
Note
that
these
quan
tities
do
not
necessarily
correlate:
A
circuit
that
computes
f
and
has
size
S
iz
e(f
)
and
depth
D
epth(f
)
ma
y
not
exist.
In
other
w
ords,
it
is
p
ossible
that
ev
ery
circuit
of
minimal
size
do
es
not
ac
hiev
e
minimal
depth.
W
e
will
rst
pro
v
e
the
existence
of
hard
functions:
..
Hard
F
unctions
Exist
There
are
no
explicit
families
of
functions
that
are
pro
v
ed
to
need
large
size
circuits,
but
using
coun
ting
argumen
ts
w
e
can
easily
pro
v
e
the
existence
of
functions
with
size
that
is
exp
onen
tial
in
the
size
of
their
input.
Prop
osition
..
F
or
lar
ge
enough
n,
ther
e
exists
a
function
f
:
f0;
g
n
!
f0;
g
s.t.
S
iz
e(f
)
>

n
n

.
Pro
of:
First
easy
observ
ation
is
that
the
n
um
b
er
of
functions
ff
j
f
:
f0;
g
n
!
f0;
gg
is
exactly


n
,
since
eac
h
suc
h
function
can
b
e
represen
ted
as
a
f0;
g
v
ector
of
length

n
.
W
e
will
no
w
upp
er
b
ound
the
n
um
b
er
of
circuits
of
size
s.
The
w
a
y
w
e
approac
h
the
problem
is
b
y
adding
one
gate
at
a
time,
starting
from
the
inputs.
A
t
rst
w
e
ha
v
e
n
inputs
|
the
v
ariables
and
their
negations.
Eac
h
gate
w
e
add,
is
either
an
O
R
or
an
AN
D
gate,
and
its
t
w
o
inputs
can
b
e
c
hosen
from
an
y
of
the
original
inputs
or
from
the
outputs
of
the
gates
w
e
already
ha
v
e.
Therefore,
for
the
rst
gate
w
e
ha
v
e
 n


c
hoices
for
the
the
inputs
and
another
c
hoice
b
et
w
een
O
R
and
AN
D
.
F
or
the
second
gate
w
e
ha
v
e
exactly
the
same,
except
no
w
the
n
um
b
er
of
inputs
to
c
ho
ose
from
is
increased
b
y
one.
Th
us,
the
n
um
b
er
of
circuits
of
size
s
is
b
ounded
b
y:
s 
Y
i=0


 
n
+
i

!
<
s 
Y
i=0


(n
+
i)


<
s 
Y
i=0
(n
+
s)

=
(n
+
s)
s
=

slog
(n+s)
W
e
wish
to
pro
v
e
that
the
n
um
b
er
of
circuits
of
size
s
=

n
n

is
strictly
less
than
the
n
um
b
er
of
functions
on
n
v
ariables,
and
hence
pro
v
e
that
there
are
functions
that
need
circuits
of
size
larger
than
s.
F
or
this
w
e
need
to
pro
v
e:

s
log
(n+s)
<


n
m
s
log
(n
+
s)
<

n
Whic
h
is
ob
viously
true
for
s
<

n
n

,
since
for
large
enough
n:
s
log
(n
+
s)
<



n
n

log
(
n
)
=

n

(

n
)
<

n
If
w
e
examine
the
pro
of
carefully
,
w
e
can
see
that
actually
most
functions
need
a
large
circuit.
Th
us
it
w
ould
seem
that
it
should
b
e
easy
to
nd
suc
h
a
hard
function.
Ho
w
ev
er,
to
the
shame
of
all

..
MONOTONE
CIR
CUITS
	
in
v
olv
ed,
the
b
est
kno
wn
lo
w
er
b
ounds
for
size
and
depth
of
\explicitly
giv
en"
functions
(actually
families
of
functions)
are:
S
iz
e

n
D
epth


log
(n)
W
e
therefore
fo
cus
on
w
eak
er
mo
dels
of
computation:
..
Bounded
Depth
Circuits
The
rst
mo
del
w
e
consider
is
that
of
b
ounded
depth
circuits.
There
are
t
w
o
deviations
from
the
standard
mo
del.
The
rst
is
that
w
e
articially
b
ound
the
depth
of
the
circuit,
and
only
consider
the
size
of
the
circuit
as
a
parameter
for
complexit
y
.
This
immediately
implies
the
other
dierence
from
the
standard
mo
del:
W
e
do
not
b
ound
the
fan-in
of
gates.
This
is
b
ecause
otherwise,
if
w
e
b
ound
the
depth
to
b
e
a
constan
t
d,
w
e
automatically
b
ound
the
size
to
b
e
less
than

d
whic
h
is
also
a
constan
t.
This
mak
es
the
mo
del
unin
teresting,
therefore
w
e
allo
w
un
b
ounded
fan-in.
Notice
that
an
y
function
can
b
e
computed
b
y
a
depth

circuit
(not
coun
ting
N
O
T
's)
b
y
transforming
the
function's
truth
table
in
to
an
O
R
of
man
y
AN
D
's.
Ho
w
ev
er,
this
construction
giv
es
exp
onen
tial
size
circuits.
Sev
eral
results
w
ere
reac
hed
for
this
mo
del
(see
Lecture
0),
but
w
e
will
fo
cus
on
a
dieren
t
mo
del
in
this
lecture.
.
Monotone
Circuits
Monotone
circuits
is
the
mo
del
w
e
consider
next,
and
throughout
the
rest
of
this
lecture.
Monotone
circuits
are
dened
in
the
same
w
a
y
as
usual
circuits
except
w
e
do
not
allo
w
the
usage
of
N
O
T
gates.
It
seems
in
tuitiv
e
that
monotone
circuits
cannot
calculate
an
y
function,
b
ecause
there
is
no
w
a
y
to
sim
ulate
a
N
O
T
gate
using
AN
D
and
O
R
gates.
W
e
will
form
ulate
and
pro
v
e
a
c
haracterization
of
the
functions
that
can
b
e
computed
using
monotone
circuits:
Denition
.
(Monotone
F
unction):
f
:
f0;
g
n
!
f0;
g
is
a
monotone
function
if
for
every
x;
y

f0;
g
n
,
x

y
implies
f
(x)

f
(y
).
Wher
e
the
p
artial
or
der

on
f0;
g
n
is
the
hamming
or
der,
i.e.,
(x

;
:
:
:
;
x
n
)

(y

;
:
:
:
;
y
n
)
if
and
only
if
for
every


i

n
we
have
x
i

y
i
.
Remark:
The
hamming
partial
order
can
b
e
though
t
of
as
the
con
tainmen
t
order
b
et
w
een
sets,
where
a
v
ector
x

f0;
g
n
corresp
onds
to
the
set
S
x
=
fi
j
x
i
=
g.
Then:
x

y
if
and
only
if
S
x

S
y
.
An
example
of
a
monotone
function
is
C
LI
QU
E
n;k
:
f0;
g
(
n

)
!
f0;
g.
The
domain
of
the
function
C
LI
QU
E
n;k
is
the
set
of
graphs
on
n
v
ertices
f;
:
:
:
ng.
A
graph
is
represen
ted
b
y
assignmen
ts
to
the
 n


v
ariables
x
i;j
,
where
for
ev
ery
pair
i;
j

f;
:
:
:
ng,
x
i;j
=

i
(i;
j
)
is
an
edge
in
the
graph.
C
LI
QU
E
n;k
is

on
a
graph
if
and
only
if
the
graph
has
a
clique
of
size
k
.
Clearly
,
C
LI
QU
E
n;k
is
a
monotone
function,
b
ecause
when
our
ordering
is
in
terpreted
as
the
con
tainmen
t
ordering
b
et
w
een
the
sets
of
edges
in
a
graph,
then
if
a
graph
G
con
tains
a
clique
of
size
k
,
an
y
other
graph
con
taining
the
edges
of
G
will
also
con
tain
the
same
clique.
Theorem
.
A
function
f
:
f0;
g
n
!
f0;
g
is
monotone
if
and
only
if
it
c
an
b
e
c
ompute
d
by
a
monotone
cir
cuit.
Pro
of:

	LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY

(=
))
W
e
will
build
a
monotone
circuit
that
computes
f
:
F
or
ev
ery

s.t.
f
()
=

w
e
dene:


(x)
=
^

i
=
x
i
W
e
also
dene:
(x)
=
_
f
()=


(x)
It
is
clear
that

can
b
e
realized
as
a
monotone
circuit.
No
w
w
e
claim
that

=
f
.
.
F
or
ev
ery

s.t.
f
()
=
,
w
e
ha
v
e


()
=

and
therefore
()
=
.
.
If
(x)
=
,
then
there
is
an

s.t.,


(x)
=

and
thereb
y
f
()
=
.
The
fact
that


(x)
=

means
that
x


b
y
the
denition
of


.
No
w,
from
the
monotonicit
y
of
f
w
e
conclude
that
f
(x)

f
()
=
,
meaning
f
(x)
=
.

((
=)
The
functions
AN
D
and
O
R
and
the
pro
jection
function
p
i
(x

;
:
:
:
;
x
n
)
=
x
i
are
all
monotone.
W
e
will
no
w
sho
w
that
comp
osition
of
monotone
functions
forms
a
monotone
function,
and
therefore
conclude
that
ev
ery
monotone
circuit
computes
a
monotone
function.
Let
g
:
f0;
g
n
!
f0;
g
b
e
a
monotone
function.
Let
f

;
:
:
:
;
f
n
:
f0;
g
N
!
f0;
g
b
e
also
monotone.
W
e
claim
that
G
:
f0;
g
N
!
f0;
g
dene
b
y:
G(x)
=
g
(f

(x);
:
:
:
;
f
n
(x))
is
also
monotone.
If
x

y
then
from
the
monotonicit
y
of
f

;
:
:
:
;
f
n
,
w
e
ha
v
e
that
for
all
i:
f
i
(x)

f
i
(y
).
In
other
w
ords:
(f

(x);
:
:
:
f
n
(x))

(f

(y
);
:
:
:
f
n
(y
))
No
w,
from
the
monotonicit
y
of
g
,
w
e
ha
v
e:
G(x)
=
g
(f

(x);
:
:
:
;
f
n
(x))

g
(f

(y
);
:
:
:
;
f
n
(y
))
=
G(y
)
W
e
mak
e
analogous
denitions
for
complexit
y
in
monotone
circuits:
Denition
.
(M
on-S
iz
e
,
M
on-D
epth):
Given
a
monotone
function
f
:
f0;
g
n
!
f0;
g,
we
dene:
.
M
on-S
iz
e(f
)
def
=
The
minimum
size
of
a
monotone
cir
cuit
c
omputing
f
.
.
M
on-D
epth(f
)
def
=
The
minimum
depth
of
a
monotone
cir
cuit
c
omputing
f
.
Ob
viously
for
ev
ery
monotone
function
f
,
M
on-S
iz
e
(f
)

S
iz
e(f
),
and
M
on-D
epth(f
)

D
epth(f
).
In
fact
there
are
functions
for
whic
h
these
inequalities
are
strict.
W
e
will
not
pro
v
e
this
result
here.
Unlik
e
the
general
circuit
mo
del,
sev
eral
lo
w
er
b
ounds
w
ere
pro
v
ed
for
the
monotone
case.
F
or
example,
it
is
kno
wn
that
for
large
enough
n
and
sp
ecic
k
(dep
ending
on
n):
M
on-S
iz
e
(C
LI
QU
E
n;k
)
=

(


n
)
M
on-D
epth
(C
LI
QU
E
n;k
)
=

(n)
>F
rom
no
w
on
w
e
shall
concen
trate
on
pro
ving
a
lo
w
er
b
ound
on
st-C
onnectiv
ity
:

..
COMMUNICA
TION
COMPLEXITY
AND
CIR
CUIT
DEPTH
	
Denition
.
(st-C
onnectiv
ity
):
Given
a
dir
e
cte
d
gr
aph
G
on
n
no
des,
two
of
which
ar
e
marke
d
as
s
and
t,
st-C
onnectiv
ity
(G)
=

if
and
only
if
ther
e
is
a
dir
e
cte
d
p
ath
fr
om
s
to
t
in
G.
Ob
viously
st-C
onnectiv
ity
is
a
monotone
function
since
if
w
e
add
edges
w
e
cannot
disconnect
an
existing
path
from
s
to
t.
Theorem
.
M
on-D
epth(st-C
onnectiv
ity
)
=
(log

(n))
In
a
previous
lecture,
w
e
pro
v
ed
that
st-C
onnectiv
ity
is
in
N
C

.
This
w
e
pro
v
ed
b
y
construct-
ing
a
circuit
that
p
erforms
O
(log
(n))
successiv
e
b
o
olean
matrix
m
ultiplications.
Notice
that
the
op
eration
of
m
ultiplying
b
o
olean
matrices
is
a
monotone
op
eration
(it
uses
only
AN
D
and
O
R
gates).
Therefore,
the
circuit
constructed
for
st-C
onnectiv
ity
is
actually
monotone.
If
w
e
dene
M
on-N
C
i
to
b
e
the
natural
monotone
analog
of
N
C
i
,
then
st-C
onnectiv
ity
is
in
M
on-N
C

.
Also,
from
the
ab
o
v
e
theorem
st-C
onnectiv
ity
is
not
in
M
on-N
C

.
This
giv
es
us:
Corollary
.
M
on-N
C

=
M
on-N
C

An
analogous
result
in
the
non-monotone
case
is
b
eliev
ed
to
b
e
true,
y
et
no
pro
of
is
kno
wn.
W
e
will
pro
ceed
b
y
reducing
the
question
of
monotone
depth
to
a
question
in
comm
unication
complexit
y
.
.
Comm
unication
Complexit
y
and
Circuit
Depth
There
is
an
in
teresting
connection
b
et
w
een
circuit
depth
and
comm
unication
complexit
y
whic
h
will
assist
us
when
pro
ving
our
main
theorem.
Since
the
connection
itself
is
in
teresting,
w
e
will
pro
v
e
it
for
general
circuits.
First
some
denitions:
Denition
.
Given
f
:
f0;
g
n
!
f0;
g
we
dene
a
c
ommunic
ation
game
G
f
:

Player

gets
x

f0;
g
n
,
s.t.
f
(x)
=
.

Player

gets
y

f0;
g
n
,
s.t.
f
(y
)
=
0.
Their
go
al
is
to
nd
a
c
o
or
dinate
i
s.t.
x
i
=
y
i
.
Notice
that
this
game
is
not
exactly
a
comm
unication
game
in
the
sense
w
e
dened
in
the
previous
lecture,
since
the
t
w
o
pla
y
ers
do
not
compute
a
function,
but
rather
a
relation.
W
e
denote
the
comm
unication
complexit
y
of
a
game
G
b
y
C
C
(G).
The
connection
b
et
w
een
our
complexit
y
measures
is:
Lemma
..
C
C
(G
f
)
=
D
epth(f
)
Pro
of:
.
First
w
e'll
sho
w
C
C
(G
f
)

D
epth(f
).
Giv
en
a
circuit
C
that
calculates
f
,
w
e
will
describ
e
a
proto
col
for
the
game
G
f
.
The
pro
of
will
pro
ceed
b
y
induction
on
the
depth
of
the
circuit
C
.

base
case:
D
epth(f
)
=
0.
In
this
case,
f
is
simply
the
function
x
i
or
:x
i
,
for
some
i.
Therefore
there
is
no
need
for
comm
unication,
since
i
is
a
co
ordinate
in
whic
h
x
and
y
alw
a
ys
dier.

	LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY

Induction
step:
W
e
lo
ok
at
the
top
gate
of
C
:
Assume
C
=
C

^
C

,
then
D
epth(C
)
=

+
max
fD
epth(C

);
D
epth(C

)g
+
D
epth(C

);
D
epth(C

)

D
epth(C
)
 
Denote
b
y
f

and
f

the
functions
that
C

and
C

calculate
resp
ectiv
ely
.
By
the
induction
h
yp
othesis:
C
C
(G
f

);
C
C
(G
f

)

D
epth(C
)
 
W
e
kno
w
that
f
(x)
=

and
f
(y
)
=
0,
therefore:
f

(x)
=
f

(x)
=

f

(y
)
=
0
or
f

(y
)
=
0
No
w,
as
the
rst
step
in
the
proto
col,
pla
y
er

sends
a
bit
sp
ecifying
whic
h
of
the
functions
f

or
f

is
zero
on
y
.
Assume
pla
y
er

sen
t
.
In
this
case
they
b
oth
kno
w:
f

(y
)
=
0
f

(x)
=

And
no
w
the
game
has
turned
in
to
the
game
G
f

.
This
w
e
can
solv
e
(using
our
induction
h
yp
othesis)
with
comm
unication
complexit
y
C
C
(G
f

)

D
epth(f

).
If
pla
y
er

sen
t

w
e
w
ould
use
the
proto
col
for
G
f

.
W
e
needed
just
one
more
bit
of
comm
unication.
Therefore
our
proto
col
will
ha
v
e
comm
unication
complexit
y
of:
C
C
(G
f
)


+
max
fC
C
(G
f

);
C
C
(G
f

)g


+
max
fD
epth(f

);
D
epth(f

)g
=

+
(D
epth(f
)
 )
=
D
epth(f
)
W
e
pro
v
ed
this
for
the
case
where
C
=
C

^
C

.
The
case
where
C
=
C

_
C

is
pro
v
ed
in
the
same
w
a
y
,
exp
ect
pla
y
er

is
the
one
to
send
the
rst
bit
(indicating
if
f

(x)
=

or
f

(x)
=
).
.
No
w
w
e'll
sho
w
the
other
direction:
C
C
(G
f
)

D
epth(f
).
F
or
this
w
e'll
dene
a
more
general
sort
of
comm
unication
game
based
on
t
w
o
non-in
tersecting
sets:
A;
B

f0;
g
n
:

Pla
y
er

gets
x

A

Pla
y
er

gets
y

B

Their
goal
is
to
nd
a
co
ordinate
i
s.t.
x
i
=
y
i
.
W
e'll
denote
this
game
b
y
G
A;B
.
Using
the
new
denition
G
f
equals
G
f
 
();f
 
(0)
.
W
e
will
pro
v
e
the
follo
wing
claim:
Claim
..
If
C
C
(G
A;B
)
=
d
then
ther
e
is
a
function
f
:
f0;
g
n
!
f0;
g
that
satises:

f
(A)
=

(i.e.,
f
(x)
=

for
every
x

A).

f
(B
)
=
0

D
epth(f
)

d

..
THE
MONOTONE
CASE
		
In
the
case
of
G
f
,
the
function
w
e
get
b
y
the
claim
m
ust
b
e
f
itself,
and
w
e
get
that
it
satises
D
epth(f
)

C
C
(G
f
),
pro
ving
our
lemma.
Pro
of:
(claim)
By
induction
on
d
=
C
C
(G
A;B
)

Base
case:
d
=
0,
meaning
there
is
no
comm
unication,
so
there
is
a
co
ordinate
i
in
whic
h
all
of
A
is
dieren
t
then
all
of
B
,
and
so
the
function
f
()
=

i
or
the
function
f
()
=
:
i
will
satisfy
the
requiremen
ts
dep
ending
on
whether
the
co
ordinate
i
is

or
0
in
A.

Induction
step:
W
e
ha
v
e
a
proto
col
for
the
game
G
A;B
of
comm
unication
complexit
y
d.
First
assume
pla
y
er

sends
the
rst
bit
in
the
proto
col.
This
bit
partitions
the
set
A
in
to
t
w
o
disjoin
t
sets
A
=
A
0
[
A

,
or
in
other
w
ords,
this
bit
turns
our
game
in
to
one
of
the
follo
wing
games
(dep
ending
on
the
bit
sen
t):
G
A
0
;B
or
G
A

;B
.
Eac
h
one
of
these
has
comm
unication
complexit
y
of
at
most
d
 
simply
b
y
con
tin
uing
the
proto
col
of
G
A;B
after
the
rst
bit
is
already
sen
t.
No
w,
b
y
the
induction
h
yp
othesis
w
e
ha
v
e
t
w
o
functions
f
0
and
f

that
satisfy:
{
f
0
(A
0
)
=

and
f

(A

)
=
.
{
f
0
(B
)
=
f

(B
)
=
0
{
D
epth(f
0
);
D
epth(f

)

d
 
W
e
dene
f
=
f
0
_
f

.
Then:
{
f
(A)
=
f
0
(A)
_
f

(A)
=
,
b
ecause
f
0
is

on
A
0
and
f

is

on
A

.
{
f
(B
)
=
f
0
(B
)
_
f

(B
)
=
0
{
D
epth(f
)
=

+
max
fD
epth(f
0
);
D
epth(f

)g

d
So
f
is
exactly
what
w
e
w
an
ted.
If
pla
y
er

sends
the
rst
bit,
he
partitions
B
in
to
t
w
o
disjoin
t
sets
B
=
B
0
[
B

,
and
turns
the
game
in
to
G
A;B
0
or
G
A;B

.
By
the
induction
h
yp
othesis
w
e
ha
v
e
t
w
o
functions
corresp
onding
to
the
t
w
o
games,
g
0
and
g

,
so
that:
g
0
(A)
=
g

(A)
=

g
0
(B
0
)
=
g

(B

)
=
0
W
e
dene
g
def
=
g
0
^
g

.
This
g
satises:
{
g
(A)
=
g
0
(A)
^
g

(A)
=
.
{
g
(B
)
=
g
0
(B
)
^
g

(B
)
=
0
(b
ecause
g
0
is
0
on
B
0
,
and
g

is
0
on
B

).
.
The
Monotone
Case
Let
us
remem
b
er
that
our
goal
w
as
to
pro
v
e
tigh
t
b
ounds
on
the
monotone
depth
of
st-C
onnectiv
ity
.
Therefore
w
e
will
dene
an
analogue
game
for
monotone
functions,
that
will
giv
e
us
a
lemma
of
the
same
a
v
or
as
the
last.

00LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY
..
The
Analogous
Game
and
Connection
Denition
.	
(Monotone
game):
Given
a
monotone
f
:
f0;
g
n
!
f0;
g
we
dene
a
c
ommu-
nic
ation
game
M
f
:

Player

gets
x

f0;
g
n
,
s.t.
f
(x)
=
.

Player

gets
y

f0;
g
n
,
s.t.
f
(y
)
=
0.
Their
go
al
is
to
nd
a
c
o
or
dinate
i
s.t.
x
i
>
y
i
,
i.e.
x
i
=

and
y
i
=
0
We
denote
this
kind
of
a
game
a
monotone
game.
The
game
is
exactly
the
same
as
G
f
,
except
f
is
monotone,
and
the
goal
is
more
sp
ecic;
i.e.,
the
goal
is
to
a
nd
a
co
ordinate
i
where
not
only
x
i
=
y
i
but
also
x
i
>
y
i
.
Notice
that
the
goal
is
alw
a
ys
ac
hiev
able,
b
ecause
if
there
is
no
suc
h
i,
then
y
is
at
least
as
large
as
x
in
ev
ery
co
ordinate.
This
means
that
y

x,
but
this
con
tradicts
the
fact
that
f
is
monotone
and
f
(x)
=
,
f
(y
)
=
0.
Our
corresp
onding
lemma
for
the
monotone
case
is:
Lemma
..
C
C
(M
f
)
=
M
on-D
epth
(f
)
Pro
of:
The
pro
of
is
similar
to
the
non-monotone
case:
.
When
building
the
proto
col
from
a
giv
en
circuit:

Base
case:
since
f
is
monotone,
if
the
depth
is
0,
w
e
ha
v
e
that
f
()
=

i
and
there-
fore
it
m
ust
b
e
the
case
that
x
i
=

and
y
i
=
0.
Hence,
again
there
is
no
need
for
comm
unication,
and
the
answ
er
is
i
(after
all
x
i
>
y
i
).

Induction
step:
In
the
induction
step,
the
top
gate
separates
our
circuit
in
to
t
w
o
sub-
circuits.
The
proto
col
then
uses
one
comm
unication
bit
to
decide
whic
h
of
the
t
w
o
games
corresp
onding
to
the
t
w
o
sub-circuits
to
solv
e.
Since
the
sub-circuits
are
monotone,
b
y
the
induction
h
yp
othesis
they
eac
h
ha
v
e
a
proto
col
to
solv
e
their
matc
hing
monotone
game.
This
solv
es
the
monotone
game
corresp
onding
to
the
whole
circuit,
since
the
sub-games
are
monotone,
and
therefore
the
co
ordinate
i
found
satises
x
i
>
y
i
.
.
When
building
the
circuit
from
a
giv
en
proto
col:

Base
case:
if
there
is
no
comm
unication,
b
oth
pla
y
ers
already
kno
w
a
co
ordinate
i
in
whic
h
x
i
>
y
i
,
hence
our
circuit
w
ould
simply
b
e
f
()
=

i
,
whic
h
is
monotone
and
of
depth
0.

Induction
step:
Eac
h
comm
unication
bit
splits
our
game
in
to
t
w
o
sub-games
of
smaller
comm
unication.
Notice
that
if
the
original
game
w
as
a
monotone
game,
so
are
the
t
w
o
sub-games.
By
the
induction
h
yp
othesis,
the
circuits
for
these
games
are
monotone.
No
w,
since
w
e
only
add
AN
D
and
O
R
gates,
the
circuit
built
is
monotone.

..
THE
MONOTONE
CASE
0
..
An
Equiv
alen
t
Restricted
Game
Let
us
dene
a
more
restricted
game
than
the
one
in
Denition
.	,
that
will
b
e
easier
to
w
ork
with.
First
some
denitions
regarding
monotone
functions:
Denition
.0
(min
term,
maxterm):
L
et
f
:
f0;
g
n
!
f0;
g
b
e
a
monotone
function.

A
min
term
of
f
is
x

f0;
g
n
s.t.
f
(x)
=

and
for
every
x
0
<
x
we
have
f
(x
0
)
=
0.

A
maxterm
of
f
is
y

f0;
g
n
s.t.
f
(y
)
=
0
and
for
every
y
0
>
y
we
have
f
(y
0
)
=
.
F
or
example,
for
st-C
onnectiv
ity
:
h
h


h


h
h
h
h
h




"
"
"
"
"
"
"
"
"
"
`
`
`
`
`
`
`
`
`
`
X
X
X
X
X
X
X
X
X
X










E
E
E
E
E
E
E
E
@
@
@
@
@
@
@
@
@
@
maxter
m
minter
m
s
t
s
t
Figure
.:
A
maxterm
and
min
term
example
for
st-C
onnectiv
ity
.

The
set
of
min
terms
is
the
set
of
graphs
that
con
tain
only
a
simple
(do
es
not
in
tersect
itself
)
directed
path
from
s
to
t:
.
If
a
graph
G
is
a
min
term,
then
it
m
ust
con
tain
a
simple
path
from
s
to
t,
and
it
cannot
con
tain
an
y
other
edge.
This
is
b
ecause
st-C
onnectiv
ity
(G)
=
,
therefore
there
is
a
simple
path
P
from
s
to
t
in
G.
G
cannot
con
tain
an
y
other
edges,
b
ecause
then
P
<
G
(in
the
edge
con
tainmen
t
order),
but
st-C
onnectiv
ity
(P
)
=
,
con
tradicting
the
fact
that
G
is
a
min
term.
.
Ev
ery
G
that
is
a
simple
path
from
s
to
t
is
a
min
term,
b
ecause
st-C
onnectiv
ity
(G)
=

and
ev
ery
edge
w
e
drop
will
disconnect
s
from
t,
therefore
it
is
minimal.

The
set
of
maxterms
for
st-C
onnectiv
ity
is
the
set
of
graphs
G
s.t.
G's
set
of
v
ertices
can
b
e
partitioned
in
to
t
w
o
disjoin
t
parts
S
and
T
that
satisfy:
.
s

S
and
t

T
.
.
G
con
tains
all
p
ossible
directed
edges
except
those
from
S
to
T
This
is
indeed
the
set
of
maxterms
for
st-C
onnectiv
ity
:
.
If
G
is
a
maxterm
then
let
S
b
e
the
set
of
v
ertices
that
are
reac
hable
from
s
in
G.
Set
T
to
b
e
all
other
v
ertices.
t

T
b
ecause
one
cannot
reac
h
t
from
s
in
G,
since

0LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY
st-C
onnectiv
ity
(G)
=
0.
Also,
G
m
ust
con
tain
all
edges
except
those
from
S
to
T
,
otherwise
w
e
can
add
the
missing
edges
and
still
lea
v
e
t
unconnected
from
s.
There
are
no
edges
from
S
to
T
b
y
the
denition
of
S
as
the
connected
comp
onen
t
reac
hable
from
s.
.
If
G
satises
b
oth
criteria,
then
ev
ery
path
starting
from
s
in
G
will
remain
in
S
and
therefore
will
not
reac
h
t
so
st-C
onnectiv
ity
(G)
=
0.
Ev
ery
edge
w
e
add
to
G
will
connect
S
to
T
and
since
S
and
T
are
strongly
connected
it
will
create
a
path
b
et
w
een
s
and
t.
Another
w
a
y
to
view
a
maxterm
of
st-C
onnectiv
ity
,
is
that
the
partition
is
dened
b
y
a
coloring
of
the
v
ertices
b
y
t
w
o
colors
0
and
,
where
s
is
colored
0
and
t
is
colored
.
The
set
of
v
ertices
colored
0
is
S
,
and
those
colored

are
T
.
W
e
will
no
w
use
maxterms
and
min
terms
to
dene
a
new
comm
unication
game:
Denition
.
(
^
M
f
):
Given
a
monotone
f
:
f0;
g
n
!
f0;
g
we
dene
a
c
ommunic
ation
game
^
M
f
:

Player

gets
x

f0;
g
n
,
s.t.
x
is
a
minterm
of
f
(in
p
articular
f
(x)
=
).

Player

gets
y

f0;
g
n
,
s.t.
y
is
a
maxterm
of
f
(in
p
articular
f
(y
)
=
0).
Their
go
al
is
to
nd
a
c
o
or
dinate
i
s.t.
x
i
>
y
i
,
i.e.
x
i
=

and
y
i
=
0.
Notice
that
^
M
f
is
a
restriction
of
M
f
to
a
smaller
set
of
inputs,
therefore
the
proto
col
that
will
solv
e
M
f
will
also
solv
e
^
M
f
.
Hence
C
C
(
^
M
f
)

C
C
(M
f
).
In
fact,
the
comm
unication
complexit
y
of
the
t
w
o
games
is
exactly
the
same:
Prop
osition
..
C
C
(
^
M
f
)
=
C
C
(M
f
)
Pro
of:
What
is
left
to
pro
v
e
is
that:
C
C
(
^
M
f
)

C
C
(M
f
).
Giv
en
a
proto
col
for
^
M
f
w
e
construct
a
proto
col
for
M
f
of
the
same
comm
unication
complexit
y
.
.
Pla
y
er

has
x
s.t.
f
(x)
=
.
He
no
w
nds
a
minimal
x
0
s.t.
x
0

x
but
f
(x
0
)
=
.
This
is
done
b
y
successiv
ely
c
hanging
co
ordinates
in
x
from

to
0,
while
c
hec
king
that
f
(x
0
)
still
equals
.
This
w
a
y
,
ev
en
tually
,
he
will
get
x
0
that
is
a
min
term.
.
In
the
same
manner
pla
y
er

nds
a
maxterm
y
0

y
.
The
pla
y
ers
no
w
pro
ceed
according
to
the
proto
col
for
^
M
f
on
inputs
x
0
and
y
0
.
Since
x
0
is
a
min
term,
and
y
0
is
a
maxterm,
the
proto
col
will
giv
e
a
co
ordinate
i
in
whic
h:
x
0
i
=

=
)
x
i
=

b
ecause
x
0

x
y
0
i
=
0
=
)
y
i
=
0
b
ecause
y
0

y
The
comm
unication
complexit
y
is
exactly
the
same,
since
w
e
used
the
same
proto
col
except
for
a
prepro
cessing
stage
that
do
es
not
cost
us
in
comm
unication
bits.
Com
bining
our
last
results
w
e
get:
Corollary
.
Given
a
monotone
function
f
:
f0;
g
n
!
f0;
g:
M
on-D
epth
(f
)
=
C
C
(
^
M
f
)

..
TW
O
MORE
GAMES
0
.
Tw
o
More
Games
As
w
e
ha
v
e
seen,
when
examining
b
ounds
on
the
monotone
depth
of
st-C
onnectiv
ity
,
w
e
need
only
examine
the
comm
unication
complexit
y
of
the
follo
wing
game
denoted
K
W
(for
Karc
hmer
and
Wigderson)
whic
h
is
simply
a
dieren
t
form
ulation
of
^
M
st-C
onnectiv
ity
:
Giv
en
n
no
des
and
t
w
o
sp
ecial
no
des
s
and
t,

Pla
y
er

gets
a
directed
path
from
s
to
t.

Pla
y
er

gets
a
coloring
C
of
the
no
des
b
y
0
and
,
s.t.
C
(s)
=
0
and
C
(t)
=
.

The
goal
is
to
nd
an
edge
(v
;
w
)
on
pla
y
er
's
path
s.t.
C
(v
)
=
0
and
C
(w
)
=
.
First
w
e
will
use
this
form
ulation
to
sho
w
an
O
(log

(n))
upp
er
b
ound
on
M
on-D
epth(st-C
onnectiv
ity
)
using
a
proto
col
for
K
W
with
comm
unication
complexit
y
O
(log

(n)):
Prop
osition
..
C
C
(K
W
)
=
O
(log

(n))
Pro
of:
The
proto
col
will
sim
ulate
binary
searc
h
on
the
input
path
of
pla
y
er
.
In
eac
h
step,
w
e
reduce
the
length
of
the
path
b
y
a
factor
of
,
while
k
eeping
the
in
v
arian
t
that
the
color
of
the
rst
v
ertex
in
the
path
is
0,
and
the
color
of
the
last
is
.
This
is
of
course
true
in
the
b
eginning
since
C
(s)
=
0
and
C
(t)
=
.
The
base
case,
is
that
the
path
has
only
one
edge,
and
in
this
case
w
e
are
done,
since
our
in
v
arian
t
guaran
tees
that
this
edge
is
colored
as
w
e
w
an
t.
No
w,
pla
y
er

sends
pla
y
er

this
edge.
The
comm
unication
cost
is
O
(log
(n)).
If
the
path
is
longer,
pla
y
er

asks
pla
y
er

the
color
of
the
middle
v
ertex
in
the
path.
This
costs
log
(n)
+

bits
of
comm
unication
|
the
name
of
the
middle
v
ertex
sen
t
from
pla
y
er

to
pla
y
er

tak
es
log
(n)
bits,
and
pla
y
er
's
answ
er
costs
one
more
bit.
If
the
color
is
,
the
rst
half
of
the
path
satises
our
in
v
arian
t,
since
the
rst
v
ertex
is
colored
0,
and
no
w
the
last
will
b
e
colored
.
If
the
color
is
0,
w
e
tak
e
the
second
half
of
the
path.
In
an
y
case,
w
e
cut
the
length
of
the
path
b
y

with
comm
unication
cost
O
(log
(n)).
Since
the
length
of
the
original
path
is
at
most
n,
w
e
need
O
(log
(n))
steps
un
til
w
e
reac
h
a
path
of
length
.
All
in
all,
w
e
ha
v
e
comm
unication
complexit
y
of
O
(log

(n)).
W
e
will
no
w
direct
our
eorts
to
w
ards
pro
ving
a
lo
w
er
b
ound
of

(log

(n))
for
D
epth(st-C
onnectiv
ity
)
via
a
lo
w
er
b
ound
for
K
W
.
F
or
this
w
e
will
con
tin
ue
to
y
et
another
reduction
in
to
a
dieren
t
com-
m
unication
game
called
F
O
R
K
:
Denition
.
(F
O
R
K
):
Given
n
=
l

w
vertic
es
and
thr
e
e
sp
e
cial
vertic
es
s,
t

,
and
t

,
wher
e
the
n
vertic
es
ar
e
p
artitione
d
into
l
layers
L

;
:
:
:
;
L
l
,
and
e
ach
layer
c
ontains
w
vertic
es:

Player

gets
a
se
quenc
e
of
vertic
es
(x
0
;
x

;
:
:
:
;
x
l
;
x
l
+
),
wher
e
for
al
l


i

l
:
x
i

L
i
,
and
x
0
=
s,
x
l
+
=
t

.

Player

gets
a
se
quenc
e
of
vertic
es
(y
0
;
y

;
:
:
:
;
y
l
;
y
l
+
),
wher
e
for
al
l


i

l
:
y
i

L
i
,
and
y
0
=
s,
y
l
+
=
t

.

Their
go
al
is
to
nd
an
i
such
that
x
i
=
y
i
and
x
i+
=
y
i+
.

0LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h


h
h













X
X
X
X
X
X
X
X
H
H
H
s
w
t

t

l
:
:
:
Figure
.:
Pla
y
er
's
sequence
is
solid,
pla
y
er
's
is
dotted,
and
the
fork
p
oin
t
is
mark
ed
with
an
x.
Ob
viously
,
suc
h
an
i
alw
a
ys
exists,
since
the
sequences
start
at
the
same
v
ertex
(s),
and
end
in
dieren
t
v
ertices
(t

=
t

),
therefore
there
m
ust
b
e
a
fork
p
oin
t.
Note:
The
sequences
the
pla
y
ers
get
can
b
e
though
t
of
as
an
elemen
t
in
f;
:
:
:
;
w
g
l
.
Since
the
start
v
ertex
is
set
to
b
e
s,
and
the
end
v
ertices
for
b
oth
pla
y
ers
are
also
set
to
b
e
t

and
t

(dep
ending
on
the
pla
y
er).
This
game
is
somewhat
easier
to
deal
with
then
K
W
,
b
ecause
of
the
symmetry
b
et
w
een
the
pla
y
ers.
W
e
will
sho
w
that
this
new
game
needs
no
more
comm
unication
than
K
W
,
and
therefore,
pro
ving
a
lo
w
er
b
ound
on
its
comm
unication
complexit
y
suces.
Prop
osition
..
C
C
(F
O
R
K
)

C
C
(K
W
)
Pro
of:
Assuming
w
e
ha
v
e
a
proto
col
for
K
W
,
w
e
will
sho
w
a
proto
col
for
F
O
R
K
whic
h
uses
the
same
amoun
t
of
comm
unication.
Actually
,
as
in
the
pro
of
of
Prop
osition
..,
all
that
the
pla
y
ers
ha
v
e
to
do
is
some
prepro
cessing,
and
the
proto
col
itself
do
es
not
c
hange.
Recall
that
in
the
game
K
W
pla
y
er

has
a
directed
path
b
et
w
een
t
w
o
sp
ecial
v
ertices
s
and
t,
that
go
es
through
a
set
of
regular
v
ertices.
Pla
y
er

has
a
coloring
of
all
v
ertices
b
y
0
and
,
where
s
is
colored
0,
and
t
is
colored
.
T
o
use
the
proto
col
for
K
W
,
the
pla
y
ers
need
to
turn
their
instance
of
F
O
R
K
in
to
an
instance
of
K
W
.

W
e
dene
the
v
ertex
s
in
F
O
R
K
to
b
e
s
in
K
W
.

W
e
dene
the
v
ertex
t

to
b
e
t.

All
other
v
ertices
are
regular
v
ertices.

The
path
of
pla
y
er

remains
exactly
the
same
|
it
is
indeed
b
et
w
een
s
and
t(=
t

).

The
coloring
of
pla
y
er

is:
a
v
ertex
is
colored
0
if
and
only
if
it
is
in
his
input
path
of
v
ertices
|
note
that
s
is
colored
0,
since
it
is
the
rst
v
ertex
in
his
sequence,
and
t(=
t

)
is
colored

b
ecause
it
is
not
on
this
path
(whic
h
go
es
from
s
to
t

).
After
this
prepro
cessing
w
e
use
the
proto
col
for
K
W
to
get
an
edge
(u;
v
)
that
is
on
pla
y
er
's
path,
where
u
is
colored
0,
and
v
is
colored
.
This
means,
that
u
is
on
pla
y
er
's
path,
b
ecause
it
is
colored
0,
and
v
is
not,
b
ecause
it
is
colored
.

..
TW
O
MORE
GAMES
0
Hence,
u
is
exactly
the
kind
of
fork
p
oin
t
w
e
w
ere
lo
oking
for,
since
it's
in
b
oth
pla
y
ers
path
and
its
successor
is
dieren
t
in
the
t
w
o
paths.
In
the
next
lecture
w
e
will
pro
v
e
that
C
C
(F
O
R
K
)
=

(log
(l
)
log
(w
))
Setting
l
=
w
=
p
n
,
our
main
theorem
follo
ws:
C
C
(st-C
onnectiv
ity
)
=
(log

(n))
Bibliographic
Notes
This
lecture
is
based
mainly
on
[].
Sp
ecically
,
the
connection
b
et
w
een
circuit
depth
and
the
corresp
onding
comm
unication
complexit
y
problem
w
as
made
there.
The
curren
t
analysis
of
the
comm
unication
complexit
y
problem
corresp
onding
to
s-t-Connectivit
y
,
via
reduction
to
the
game
F
ORK,
is
due
to
[]
and
is
simpler
than
the
original
analysis
(as
in
[]).
.
M.
Gringi
and
M.
Sipser.
Monotone
Separation
of
Logarithmic
Space
from
Logarithmic
Depth.
JCSS,
V
ol.
0,
pages
{,
		.
.
M.
Karc
hmer
and
A.
Wigderson.
Monotone
Circuits
for
Connectivit
y
Require
Sup
er-Logarithmic
Depth.
SIAM
J.
on
Disc.
Math.,
V
ol.
,
No.
,
pages
{,
		0.

0LECTURE
.
MONOTONE
CIR
CUIT
DEPTH
AND
COMMUNICA
TION
COMPLEXITY

Lecture

The
F
ORK
Game
Lecture
giv
en
b
y
Ran
Raz
Notes
tak
en
b
y
Dana
Fisman
and
Nir
Piterman
Summary:
W
e
analyze
the
game
f
ork
that
w
as
in
tro
duced
in
the
previous
lecture.
W
e
giv
e
tigh
t
lo
w
er
and
upp
er
b
ounds
on
the
comm
unication
needed
in
a
proto
col
solving
f
ork.
This
completes
the
pro
of
of
the
lo
w
er
b
ound
on
the
depth
of
monotone
circuits
computing
the
function
st-Connectivit
y
.
.
In
tro
duction
W
e
ha
v
e
seen
in
the
previous
lecture
the
connection
b
et
w
een
circuit
depth
and
comm
unication
complexit
y:
W
e
compared
the
depth
of
a
circuit
computing
a
function
f
to
the
n
um
b
er
of
bits
transferred
b
et
w
een
t
w
o
pla
y
ers
pla
ying
a
comm
unication
game
G
f
.
W
e
sa
w
that
giv
en
a
comm
u-
nication
proto
col
solving
G
f
using
comm
unication
of
c-bits,
w
e
can
construc
t
a
circuit
computing
f
whose
depth
is
c.
On
the
other
hand,
giv
en
a
c-depth
circuit
computing
f
w
e
can
plan
a
com-
m
unication
proto
col
for
G
f
whic
h
uses
c
bits.
Th
us,
w
e
established
that
the
b
est
comm
unication
proto
col
for
solving
G
f
uses
t
he
same
amoun
t
of
comm
unication
bits
as
the
depth
of
the
b
est
circuit
computing
f
.
tion
proto
col.
The
plan
w
as
to
use
comm
unication
complexit
y
to
pro
v
e
upp
er
and
lo
w
er
b
ounds
on
the
depths
of
circuits.
F
ailing
to
reac
h
satisfactory
results
using
this
mo
del,
the
w
eak
er
mo
del
of
monotone
functions
and
monotone
circuits
w
as
in
tro
duced.
Since
a
mono
tone
circuit
is
in
particular
a
circuit
but
not
the
other
w
a
y
around,
pro
ving
lo
w
er
b
ounds
on
monotone
circuits
do
es
not
pro
v
e
the
same
lo
w
er
b
ounds
in
the
unrestricted
mo
del.
All
the
same,
our
goal
w
as
to
sho
w
a
(tigh
t)
lo
w
er
b
ound
for
the
depth
of
monot
one
circuits
computing
st-Connectivit
y
.
The
plan
w
as
to
ac
hiev
e
the
result
b
y
conducting
a
series
of
reductions.
The
rst
used
the
connection
b
et
w
een
circuit
depth
and
comm
unication
complexit
y
to
reduce
the
question
to
a
comm
unication
proto
col.
Then,
v
arious
reductions
b
et
w
een
sev
eral
kinds
of
games
led
us
to
the
f
ork
game
(see
Denition
.).
Based
on
a
lo
w
er
b
ound
for
the
comm
unication
needed
in
f
ork,
the
lo
w
er
b
ound
for
st-Connectivit
y
of

(l
og

n)
w
as
pro
v
en.
The
purp
ose
of
this
lecture
is
to
pro
v
e
the
lo
w
er
b
ound
of
the
f
ork
game.
W
e
will
giv
e
a
complete
analysis
of
the
f
ork
game
sho
wing
that
the
comm
uni-
cation
b
et
w
een
the
t
w
o
pla
y
ers
is
(l
og

w

l
og

l
),
th
us
supplying
the
missing
link
in
the
pro
of
of
the
lo
w
er
b
ound
on
the
depth
of
st-Connectivit
y
.
0

0
LECTURE
.
THE
F
ORK
GAME
.
The
f
ork
game
{
recalling
the
denition
F
ork
is
a
game
b
et
w
een
t
w
o
pla
y
ers.
Eac
h
pla
y
er
gets
a
path
from
a
predened
set
of
p
ossible
paths.
Both
paths
start
at
the
same
p
oin
t
but
ha
v
e
dieren
t
end
p
oin
ts.
Hence,
at
least
one
fork
p
oin
t
in
whic
h
the
t
w
o
paths
separate,
m
ust
exist.
The
pla
y
ers'
goal
is
to
nd
suc
h
a
fork
p
oin
t.
More
formally
,
w
e
recall
the
denition
of
the
f
ork
game
giv
en
in
previous
lecture.
Denition
.
(f
ork):
Given
n
=
l

w
vertic
es
and
thr
e
e
sp
e
cial
vertic
es
s,
t

,
and
t

,
wher
e
the
n
vertic
es
ar
e
divide
d
into
l
layers
l

;
:
:
:
;
l
l
,
and
e
ach
layer
c
ontains
w
vertic
es:

Pla
y
er
I
gets
a
se
quenc
e
of
vertic
es
(x

;
x

;
:
:
:
;
x
l
),
wher
e
for
al
l


i

l
:
x
i

l
i
.
F
or
simplicity
of
notation
we
assume
that
Pla
y
er
I
is
given
two
mor
e
c
o
or
dinates:
x
0
=
s,
x
l
+
=
t

.

Pla
y
er
I
I
gets
a
se
quenc
e
of
vertic
es
(y

;
y

;
:
:
:
;
y
l
),
wher
e
for
al
l


i

l
:
y
i

l
i
.
A
gain
we
c
onsider
y
0
=
s,
y
l
+
=
t

.

Their
go
al
is
to
nd
a
c
o
or
dinate
i
such
that
x
i
=
y
i
and
x
i+
=
y
i+
.
In
order
to
stress
the
fact
that
the
inputs
are
elemen
ts
of
[w
]
l
,
where
[w
]
=
f;
:::;
w
g,
w
e
sligh
tly
mo
died
the
denition,
excluding
the
constan
t
p
oin
ts
s;
t

and
t

from
the
input
sequences
giv
en
to
the
pla
y
ers.
The
follo
wing
theorem
states
the
main
result
of
this
lecture,
giving
the
b
ounds
on
the
comm
u-
nication
complexit
y
of
solving
the
f
ork
game.
Theorem
.
The
c
ommunic
ation
c
omplexity
of
the
f
ork
game
is
(l
og

w

l
og

l
).
W
e
rst
sho
w
an
upp
er
b
ound
on
the
comm
unication
complexit
y
of
the
game.
The
upp
er
b
ound
is
giv
en
here
only
for
the
completeness
of
the
discussion
concerning
the
f
ork
problem.
.
An
upp
er
b
ound
for
the
f
ork
game
The
pro
of
giv
en
here
is
v
ery
similar
to
the
pro
of
of
the
upp
er
b
ound
for
the
K
W
game
(see
previous
lecture).
W
e
basically
p
erform
a
binary
searc
h
on
the
path
to
nd
the
f
ork
p
oin
t.
Prop
osition
..
The
c
ommunic
ation
c
omplexity
of
the
f
ork
game
is
O
(l
og

w

l
og

l
).
Pro
of:
F
or
the
sak
e
of
simplicit
y
,
the
follo
wing
notation
is
in
tro
duced.
W
e
denote
F
a;b
as
the
f
ork
game
where
the
inputs
are
of
the
form
x
=
(x
a
;
x
a+
;
:::;
x
b
)
and
y
=
(y
a
;
y
a+
;
:::;
y
b
).
Lik
e
in
the
general
f
ork
gam
e,
w
e
consider
x
and
y
as
ha
ving
t
w
o
more
co
ordinates.
An
(a
 )-co
ordinate
suc
h
that
x
a 
=
y
a 
=
s
is
the
origin
p
oin
t
of
the
paths.
A
(b
+
)-co
ordinate
suc
h
that
x
b+
=
t

and
y
b+
=
t

are
the
endp
oin
ts
of
the
paths.
First
notice
that
if
the
length
of
the
paths
is
only
one,
i.e.,
a
=
b,
then
the
problem
can
b
e
solv
ed
using
l
og

w
bits,
ob
eying
the
follo
wing
proto
col:

Pla
y
er
I
sends
its
input
(this
requires
l
og

w
bits).

Pla
y
er
I
I
replies
with

if
he
has
the
same
co
ordinate
and
with
0
otherwise
(one
bit).
If
they
ha
v
e
the
same
co
ordinate,
the
fork
p
oin
t
is
found
in
that
co
ordinate
and
then
the
paths
separate
to
t

and
t

.
Otherwise
the
fork
is
the
p
oin
t
of
origin
s
=
x
a 
=
y
a 
.
If
the
length
of
the
paths
is
larger
than
,
i.e.,
a
<
b,
then
the
problem
can
b
e
reduced
to
half
using
l
og

w
bits.
(F
or
simplicit
y
,
w
e
assume
that
a+b

is
an
in
teger):

..
A
LO
WER
BOUND
F
OR
THE
F
ORK
GAME
0	

Pla
y
er
I
sends
its
middle
la
y
er
no
de:
x
(a+b)

(this
requires
l
og

w
bits).

Pla
y
er
I
I
c
hec
ks
if
they
ha
v
e
the
same
middle
no
de:
y
a+b

=
x
a+b

.
If
so
he
sends

otherwise
he
sends
0
(one
bit).
If
the
pla
y
ers
ha
v
e
the
same
middle
p
oin
t
(i.e.,
Pla
y
er
I
sen
t
),
then
there
has
to
b
e
a
fork
p
oin
t
b
et
w
een
the
middle
and
the
end.
Th
us
the
game
is
reduced
to
F
a+b

+;b
.
On
the
other
hand,
if
the
middle
p
oin
ts
dier,
then
there
has
to
b
e
a
fork
p
oin
t
b
et
w
een
the
start
and
the
middle.
Th
us
the
game
is
reduced
to
F
a;
a+b

 
.
Note
that
there
is
no
p
oin
t
in
including
the
middle
la
y
er
itself
in
the
range
of
the
reduced
game.
The
reason
b
eing
that
in
the
rst
case,
the
m
utual
p
oin
t
in
la
y
er
a+b

is
actually
the
new
origin
p
oin
t,
while
in
the
second
case,
the
t
w
o
p
oin
ts
in
la
y
er
a+b

are
actually
the
new
end
p
oin
ts.
Therefore
f
ork
(i.e.
F
;l
using
this
notation)
is
solv
ed
in
l
og

l
iterations
of
the
proto
col,
requiring
transmission
of
O
(l
og

w

l
og

l
)
bits.
Our
goal
no
w
is
to
sho
w
that
this
upp
er
b
ound
is
tigh
t.
.
A
lo
w
er
b
ound
for
the
f
ork
game
In
order
to
sho
w
the
lo
w
er
b
ound
w
e
consider
games
that
w
ork
only
for
a
subset
of
the
p
ossible
inputs.
W
e
p
erform
some
inductiv
e
pro
cess
in
whic
h
w
e
establish
the
connection
b
et
w
een
games
with
dieren
t
sets.
Tw
o
kinds
of
transformations
are
considered:
.
Giv
en
a
proto
col
that
w
orks
for
some
set,
w
e
devise
a
proto
col
that
w
orks
for
smaller
densit
y
sets
with
one
less
bit
of
comm
unication.
.
Giv
en
a
proto
col
that
w
orks
for
some
set,
w
e
con
v
ert
it
in
to
a
proto
col
that
w
orks
for
sets
of
higher
densit
y
but
shorter
paths
using
the
same
amoun
t
of
comm
unication.
When
adapting
the
giv
en
proto
col
in
to
a
new
one,
some
hea
vy
computations
are
in
v
olv
ed.
Ho
w
ev
er,
recall
that
the
only
parameter
considered
during
the
application
of
the
proto
col
is
the
n
um
b
er
of
bits
transmitted.
Th
us,
an
y
computation
done
in
the
preparati
on
of
the
proto
col
and
an
y
computation
done
lo
cally
b
y
either
side
is
not
tak
en
in
to
accoun
t.
..
Denitions
W
e
rst
dene
a
subgame
of
f
ork
that
w
orks
only
on
a
subset
of
the
p
ossible
inputs.
Giv
en
a
subset
S

[w
]
l
w
e
consider
the
game
f
ork
S
where
Pla
y
er
I
gets
an
input
x

S
and
Pla
y
er
I
I
gets
an
input
er
l
iney

S
and
they
ha
v
e
the
same
goal
of
nding
the
fork
p
oin
t
b
et
w
een
the
t
w
o
paths.
W
e
will
w
ork
only
with
subsets
of
[w
]
j
,
where
the
p
o
w
er
j
(the
length
of
the
path)
will
c
hange
from
time
to
time.
W
e
call
w
the
width
of
the
game.
Throughout
the
discussion
the
width
w
of
the
game
will
b
e
constan
t
and
w
e
frequen
tly
ignore
i
t.
As
explained
ab
o
v
e,
the
densit
y
of
the
set
will
b
e
an
imp
ortan
t
parameter
in
the
transformations:
Denition
.
(densit
y):
The
densit
y
of
a
set
S

[w
]
l
is
dene
d
as
(S
)
=
jS
j
w
l
.
Giv
en
a
proto
col
solving
a
partial
f
ork
game,
w
e
ask
ourselv
es
what
is
the
densit
y
of
the
sets
that
this
proto
col
w
orks
on.
W
e
are
in
terested
in
the
minimal
proto
col
that
will
w
ork
for
some
set
of
densit
y
:

0
LECTURE
.
THE
F
ORK
GAME
Denition
.
((;
l
)
 pr
otocol
):
A
c
ommunic
ation
pr
oto
c
ol
wil
l
b
e
c
al
le
d
an
(;
l
)-p
roto
col
if
it
works
for
some
set
S
of
p
aths
of
length
l
with
density
(S
)
=
.
Denition
.
(C
C
(;
l
)):
L
et
C
C
(;
l
)
denote
the
smal
lest
c
ommunic
ation
c
omplexity
of
an
(;
l
)
 pr
otocol
.
Using
this
terminology
,
since
there
is
just
one
set
of
densit
y
,
a
proto
col
for
f
ork
is
just
a
(;
l
)
 pr
otocol
and
so
w
e
are
in
terested
in
C
C
(;
l
).
..
Reducing
the
densit
y
The
follo
wing
lemma
enables
the
rst
kind
of
transformation
discussed
ab
o
v
e.
Giv
en
a
proto
col
that
w
orks
for
a
set
of
a
certain
densit
y
w
e
adapt
it
to
a
proto
col
that
w
orks
for
a
subset
of
half
that
densit
y
.
The
new
proto
col
will
w
ork
with
one
less
bit
o
f
comm
unication
than
the
original
proto
col.
Lemma
..
If
ther
e
exists
an
(;
l
)
 pr
otocol
which
works
with
c
bits
and
c
>
0,
then
ther
e
is
also
an
(


;
l
)
 pr
otocol
that
works
with
c
 
bits.
By
the
ab
o
v
e
lemma,
the
b
est
proto
col
for
an

densit
y
set
will
require
at
least
one
more
bit
than
the
b
est
proto
col
for
an


densit
y
set.
Th
us,
w
e
ha
v
e
Corollary
.
If
C
C
(;
l
)
is
non-zer
o
then
C
C
(;
l
)
is
gr
e
ater
than
C
C
(


;
l
)
by
at
le
ast
one;
C
C
(;
l
)

C
C
(


;
l
)
+

Pro
of:
The
pro
of
can
b
e
generalized
for
an
y
comm
unication
proto
col.
As
explained
in
detail
in
Lecture
,
w
e
use
the
fact
that
an
y
comm
unication
proto
col
can
b
e
view
ed
as
the
t
w
o
parties
observing
a
matrix.
The
matrix'
ro
ws
corresp
ond
to
the
p
ossible
inputs
of
Pla
y
er
I
and
its
columns
corresp
ond
to
the
p
ossible
inputs
of
Pla
y
er
I
I.
The
en
tries
of
the
matrix
are
the
desired
results
of
the
proto
col.
P
assing
one
bit
b
et
w
een
the
t
w
o
pla
y
ers
partitions
this
matrix
in
to
t
w
o
rectangles
(horizon
tally
if
the
rst
bit
is
sen
t
b
y
Pla
y
er
I
and
v
ertically
if
it
is
sen
t
b
y
Pla
y
er
I
I).
Let
P
b
e
a
non-zero
comm
unication
(;
l
)
 pr
otocol
for
the
set
S
.
Assume
without
loss
of
generalit
y
that
Pla
y
er
I
sends
the
rst
bit
in
the
proto
col
P
.
Let
S
0

S
b
e
those
inputs
in
S
for
whic
h
Pla
y
er
I
sends
0
as
the
f
irst
bit,
and
similarly
let
S


S
b
e
those
inputs
in
S
for
whic
h
Pla
y
er
I
sends

as
the
rst
bit.
Since
S
0
[
S

=
S
,
either
S
0
or
S

con
tains
at
least
half
the
inputs
in
S
.
Assuming
that
this
is
S
0
then
(S
0
)

(S
)

.
Let
P
0
b
e
a
proto
col
that
w
orks
lik
e
P
but
without
sending
the
rst
bit,
while
the
pla
y
ers
assume
that
the
v
alue
of
this
bit
is
0.
Ob
viously
P
0
w
orks
if
Pla
y
er
I
gets
inputs
from
S
0
and
Pla
y
er
I
I
gets
inputs
fr
om
S
.
In
particular
it
w
orks
when
b
oth
inputs
are
from
the
subset
S
0
.
W
e
conclude
that
P
0
is
an
(


;
l
)
 pr
otocol
that
w
orks
with
one
bit
less
than
P
.
The
case
where
S

is
larger
than
S
0
is
iden
tical.
In
order
to
apply
Lemma
..
the
proto
col
m
ust
cause
at
least
one
bit
to
b
e
sen
t
b
y
some
pla
y
er.
Therefore,
w
e
w
ould
lik
e
to
iden
tify
the
sets
for
whic
h
a
proto
col
consumes
some
comm
unication.
W
e
sho
w
that
for
large
enough
sets,
whose
densit
y
is
more
than

w
,
an
y
proto
col
indeed
uses
non-zero
comm
unication.
Lemma
..
A
ny
pr
oto
c
ol
for
a
set
whose
density
is
lar
ger
than

w
r
e
quir
es
c
ommunic
ation
of
at
le
ast
one
bit.

..
A
LO
WER
BOUND
F
OR
THE
F
ORK
GAME

Corollary
.
F
or
every
l
and
every

>

w
;
C
C
(;
l
)
>
0.
Pro
of:
W
e
will
sho
w
that
if
P
is
a
proto
col
that
w
orks
with
no
comm
unication
at
all
and
solv
es
f
ork
S
then
the
densit
y
of
S
is
at
most

w
.
Since
no
information
is
passed
b
et
w
een
the
pla
y
ers,
an
y
of
the
pla
y
ers
m
ust
establish
his
result
dep
ending
only
on
his
input.
It
could
b
e
the
case
where
b
oth
pla
y
ers
get
the
same
path
x
=
y
.
In
this
case
the
only
fork
p
oin
t
is
in
the
last
la
y
er
x
l
=
y
l
(with
x
l
+
=
t

=
t

=
y
l
+
).
So
Pla
y
er
I
m
ust
alw
a
ys
giv
e
the
last
la
y
er
as
the
fork
p
oin
t.
If
some
y

S
has
a
dieren
t
p
oin
t
in
the
last
la
y
er
(i.e.
y
l
=
x
l
),
the
proto
col
will
giv
e
a
wrong
answ
er.
Hence
for
all
the
paths
in
S
the
last
p
oin
t
in
the
path
has
to
b
e
j
for
some
j
b
et
w
een

and
w
.
So
the
set
S
is
in
fact
a
subset
of
[w
]
l
 

fj
g
and
therefore
its
densit
y
is
not
greater
than
w
l 
w
l
=

w
.
Note
that
using
these
t
w
o
lemmas
w
e
can
get
a
lo
w
er
b
ound
of

(l
og

w
)
bits
for
f
ork.
As
long
as
the
densit
y
is
greater
than

w
,
Lemma
..
guaran
tees
that
w
e
can
apply
Lemma
...
Rep
eating
Lemma
..
less
than
l
og

w
times
w
e
kno
w
that
the
densit
y
has
not
decreased
b
ey
ond

w
and
so
Lemma
..
can
b
e
applied
again.
C
C
(;
l
)

C
C
(


;
l
)
+


C
C
(


;
l
)
+


:::

C
C
(

w
;
l
)
+
l
og

w
Considering
our
aim
is
to
use
this
b
ound
in
connection
with
b
o
olean
circuits,
the
b
ound
C
C
(;
l
)
=

(l
og

w
)
is
insignican
t.
This
is
the
case,
since
in
order
to
read
an
input
of
length
w
w
e
m
ust
use
a
circuit
of
at
least
l
og

w
depth
an
yho
w.
..
Reducing
the
length
In
order
to
reac
h
the
desired
b
ound
w
e
need
to
manipulate
the
length
of
the
paths
as
w
ell.
The
main
to
ol
will
b
e
the
follo
wing
'amplication'
lemma
that
allo
ws
us,
using
an
(;
l
)
 pr
otocol
,
to
construct
another
proto
col
that
w
orks
on
a
set
of
short
er
paths
(of
length
l

)
but
whose
densit
y
is
larger
than
.
Lemma
..
L
et



w
.
If
ther
e
exists
an
(;
l
)
 pr
otocol
for
f
ork
S
,
that
uses
c
bits
of
c
ommunic
ation,
then
ther
e
is
also
an
(
p


;
l

)
 pr
otocol
that
uses
the
same
numb
er
of
bits.
F
or
's
in
the
range

w
<

<


using
this
lemma
increases
the
densit
y
of
the
set.
Since:

w
<

<


=
)
p


>

The
pro
of
of
the
lemma
uses
the
follo
wing
tec
hnical
claim:
Claim
..
Consider
an
n

n
matrix
of
0-.
Denote
by

the
fr
action
of
one-entries
in
the
matrix
and
by

i
the
fr
action
of
one-entries
in
the
i
th
r
ow.
We
say
that
a
r
ow
i
is
dense
if

i



.
One
of
the
fol
lowing
two
c
ases
hold:
.
ther
e
is
some
r
ow
i
with

i

q


.
the
numb
er
of
dense
r
ows
is
at
le
ast
q



n
Pro
of:
In
tuitiv
ely
,
the
claim
sa
ys
that
either
one
of
the
ro
ws
is
'v
ery
dense'
or
there
are
a
lot
of
dense
ro
ws.
Supp
ose
b
y
con
tradiction
that
the
t
w
o
cases
do
not
hold.
Let
us
calculate
the
densit
y
of
one-
en
tries
in
the
en
tire
matrix.
Since
Case

do
es
not
hold
there
are
less
than
q



n
dense
ro
ws.
Since


LECTURE
.
THE
F
ORK
GAME
Case

do
es
not
hold
eac
h
of
them
has
less
than
q



n
one-en
tries.
Hence
the
fraction
of
one-en
tries
in
all
the
dense
ro
ws
is
less
than
q



q


=


.
Non-dense
ro
ws
con
tain
less
than



n
one-en
tries
and
there
are
at
most
n
non-dense
ro
ws,
hence
the
fraction
of
one-en
tries
in
the
non-dense
ro
ws
is
less
than


.
Th
us,
the
total
fraction
of
one-en
tries
is
less
than


+


=
,
con
tradicting
the
assumed
fraction
of
one-en
tries
in
the
matrix.
Pro
of:
(Of
Lemma
..)
Giv
en
an
(;
l
)
 pr
otocol
w
e
w
ould
lik
e
to
sho
w
an
(
p


;
l

)
 pr
otocol
.
Let
P
b
e
an
(;
l
)
 pr
otocol
.
Assume
that
it
w
orks
for
the
set
S
of
paths
in
[w
]
l
whose
densit
y
is
(S
)
=
.
W
e
view
the
paths
in
S
as
t
w
o
concatenated
paths
of
half
the
length.
Giv
en
(s

;
:::;
s
l
)
a
path
in
S
w
e
denote
(s

;
:::;
s
l
)
b
y
a

b
where
a
=
(s

;
:::;
s
l

);
b
=
(s
l

+
;
:::;
s
l
).
F
or
an
y
a

[w
]
l

w
e
denote
b
y
S
uf
f
ix(a)
the
set
of
p
ossible
suxes
b
for
a,
that
form
a
path
in
S
:
S
uf
f
ix(a)
=
fb

[w
]
l

j
a

b

S
g
Consider
a
matrix
whose
ro
ws
and
columns
corresp
ond
to
paths
in
[w
]
l

.
An
en
try
of
the
matrix
(a;
b)
is

if
the
path
a

b
is
in
S
and
0
otherwise.
Th
us,
the
fraction
of
one-en
tries
in
the
matrix
is

(the
densit
y
of
S
).
Applying
Claim
..
to
the
matrix,
w
e
get
that
this
matrix
satises
either
()
or
().
Either
there
exists
a
prex
of
a
path
in
S
that
has
a
lot
of
suxes,
or
there
exist
man
y
prexes
of
paths
in
S
that
ha
v
e
quite
a
lot
of
suxes.
In
the
rst
case
w
e
use
the
set
of
suxes
as
the
new
set
for
whic
h
w
e
build
a
new
proto
col,
while
in
the
second
case
w
e
use
the
set
of
'hea
vy'
prexes
as
the
new
set.
In
b
oth
cases
w
e
adapt
the
proto
col
P
to
w
ork
for
the
new
set
of
half
length
paths.
Details
follo
w:
.
In
case
there
is
a
prex
of
a
path
a
with
at
least
q



w
l

suxes,
w
e
let
S
0
=
S
uf
f
ix(a)
b
e
the
set
of
suxes
of
that
prex,
and
the
new
proto
col
P
0
will
w
ork
as
follo
wing:
Pla
y
er
I
gets
an
input
x
in
S
0
,
he
concatenates
it
to
a
forming
the
path
a

x
in
S
.
In
a
similar
w
a
y
Pla
y
er
I
I
forms
the
path
a

y
.
No
w
they
ha
v
e
paths
in
S
and
can
apply
the
proto
col
P
to
nd
the
fork
p
oin
t.
Since
the
paths
coincide
in
their
rst
halv
es,
the
fork
p
oin
t
m
ust
b
e
in
the
second
half.
Note
that
if
the
rst
co
ordinate
in
x
and
y
is
dieren
t,
the
fork
in
the
whole
path
can
b
e
found
in
the
last
co
ordinate
of
a.
This
is
the
case
where
for
S
0
the
fork
w
as
found
in
the
p
oin
t
of
origin
of
the
paths,
s.
.
In
case
there
are
man
y
(i.e.
at
least
q



w
l

)
prexes
of
paths
with
at
least


suxes
w
e
tak
e
S
0
to
b
e
the
set
of
all
the
prexes
of
'dense'
paths;
that
is,
S
0
=
fx
:
jS
uf
f
ix(x)j




w
l

g.
W
e
ha
v
e
jS
0
j

q



w
l

.
F
or
eac
h
p
ossible
input
x
in
S
0
w
e
will
try
and
build
t
w
o
suxes
b

(x)
and
b

(x)
suc
h
that
for
an
y
t
w
o
inputs
x
and
y
the
suxes
b

(x)
and
b

(y
)
will
not
coincide.
In
this
case
a
fork
found
b
et
w
een
x

b

(x)
and
y

b

(y
)
m
ust
b
e
in
the
rst
half
of
the
path,
since
the
second
half
is
ensured
not
to
coincide
in
an
y
p
oin
t.
W
e
suggest
a
metho
d
for
building
the
suxes
(for
simplicit
y
w
e
assume
that
w
is
ev
en):
F
or
eac
h
la
y
er
l

+
;
:::;
l
w
e
color
half
the
no
des
in
the
la
y
er
in
orange
and
the
other
half
in
purple.
If
for
ev
ery
x,
the
sux
b

(x)
will
b
e
colored
orange
and
the
sux
b

(x)
will
b
e
colored
purple
the
goal
will
b
e
fullled.
W
e
wi
ll
sho
w
that
suc
h
a
coloring
exists
b
y
sho
wing
that
the
probabilit
y
for
suc
h
a
coloring
o
v
er
all
random
colorings
is
p
ositiv
e.

..
A
LO
WER
BOUND
F
OR
THE
F
ORK
GAME

Claim
..
Ther
e
exists
a
c
oloring
of
al
l
no
des
such
that
for
most
a

S
0
ther
e
ar
e
suxes
b

(a)
and
b

(a)
such
that:

a

b

(a)

S
and
a

b

(a)

S

al
l
no
des
in
b

(a)
ar
e
c
olor
e
d
or
ange

al
l
no
des
in
b

(a)
ar
e
c
olor
e
d
purple
W
e
prop
ose
the
follo
wing
w
a
y
to
color
the
v
ertices
in
la
y
ers
l

+
;
:::;
l
:

Cho
ose
randomly
w

paths
r

;
:::;
r
w

in
[w
]
l

and
color
all
the
v
ertices
app
earing
in
them
in
orange.

In
la
y
ers
where
the
paths
co
v
ered
less
than
w

of
the
v
ertices
c
ho
ose
randomly
orange
v
ertices
to
ac
hiev
e
w

v
ertices
colored
in
orange.

Color
the
rest
of
the
v
ertices
in
purple.
By
symmetry
it
is
apparen
t
that
this
metho
d
for
coloring
pro
duces
the
same
distribution
as
uniformly
c
ho
osing
w

no
des
out
of
the
w
p
ossible
no
des
in
eac
h
la
y
er
(coloring
them
in
orange
and
the
rest
in
purple).
W
e
shall
sho
w
that
for
an
y
a
in
S
0
there
is
a
high
probabilit
y
that
t
w
o
suc
h
suxes
(i.e.,
one
colored
orange
and
one
colored
purple)
can
b
e
found.
Dene
S
00
as
the
set
of
all
prexes
a
in
S
0
that
ha
v
e
t
w
o
suc
h
suxes.
The
exp
ected
size
of
S
00
will
b
e
close
to
the
size
of
S
0
.
Therefore
exists
a
coloring
that
induces
suc
h
a
set
S
00
,
whose
size
is
v
ery
close
to
the
size
of
S
0
.
F
or
a
path
a

S
0
,
since
the
densit
y
of
suxes
of
a
is
at
least


,
the
probabilit
y
that
for
some
i,
the
random
path
r
i
(c
hosen
in
the
ab
o
v
e
pro
cess)
is
not
a
sux
of
a
is
at
most
(
 

).
Since
these
paths
are
c
hosen
indep
enden
tly
and

>

w
,
the
probabilit
y
that
none
of
the
w

paths
is
a
sux
of
a
is
at
most
0:0.
This
is
b
ecause
P
r
ob[r
i
=

S
uf
f
ix(a)]

(
 

)
P
r
ob[i;
r
i
=

S
uf
f
ix(a)]

(
 

)
w

<
(
 
w
)
w


e
 

0:0	
There
is
no
dierence
b
et
w
een
the
probabilit
y
that
the
rst
sux
(i.e.
the
one
colored
orange)
do
es
not
exist
and
the
probabilit
y
that
the
second
sux
(i.e.
the
one
colored
purple)
do
es
not
exist.
Hence,
w
e
can
use
union
b
ound
to
determine
that
the
probabilit
y
of
either
b

(a)
or
b

(a)
not
to
exist,
is
smaller
than
0:.
Therefore,
the
exp
ected
size
of
S
00
is
at
least
0:	
of
the
size
of
S
0
.
F
ormally:
Dene
for
ev
ery
elemen
t
a
in
S
0
a
random
v
ariable
X
a
=
(

if
a
has
t
w
o
suc
h
suxes
0
otherwise
Ob
viously
jS
00
j
=

aS
0
X
a
hence:
E
(jS
00
j)
=
E
(
aS
0
X
a
)
=

aS
0
E
(X
a
)
=
jS
0
j

E
(X
a
)

0:	

jS
0
j
So
w
e
ha
v
e
a
set
S
00
of
densit
y
at
least
0:	

q



w
l

>
p



w
l

suc
h
that
for
an
y
path
a
of
length
l

in
S
00
w
e
ha
v
e
t
w
o
suxes
b

(a)
and
b

(a)
colored
orange
and
purple,
resp
ectiv
ely
.


LECTURE
.
THE
F
ORK
GAME
Pla
y
er
I
will
get
an
input
x
in
[w
]
l

and
concatenate
b

(x)
to
it,
creating
x

b

(x),
a
path
in
S
.
Pla
y
er
I
I,
getting
y
,
will
create
in
a
similar
w
a
y
y

b

(y
).
The
t
w
o
pla
y
ers
will
run
the
proto
col
P
and
nd
the
desired
fork
p
oin
t.
Note
that
if
the
last
co
ordinate
in
x
and
y
is
equal,
the
fork
can
b
e
found
there.
This
is
legitimate
in
S
00
b
ecause
the
end
p
oin
ts
of
the
paths
t

and
t

are
dieren
t.
W
e
ha
v
e
seen
that
in
b
oth
cases
w
e
w
ere
able
to
nd
subsets
of
[w
]
l

of
densit
y
at
least
p


for
whic
h
w
e
adapted
the
proto
col
P
.
Therefore
w
e
ha
v
e
pro
v
en:
C
C
(;
l
)

C
C
(
p


;
l

)
..
Applying
the
lemmas
to
get
the
lo
w
er
b
ound
W
e
sho
w
that
an
y
proto
col
for
solving
the
f
ork
game
uses
at
least

(l
og

w

l
og

l
)
bits
of
comm
u-
nication
b
y
emplo
ying
Lemmas
..
and
...
F
rom
Lemma
..
w
e
kno
w
that
during
all
the
transformat
ions,
w
e
do
not
nd
a
zero-bit
proto
col.
F
or
the
simplicit
y
of
calculations
w
e
assume
that
l
is
a
p
o
w
er
of
.
W
e
start
b
y
reducing
f
ork
to
an
y
subset
of
densit
y

p
w
.
C
C
(;
l
)

C
C
(

p
w
;
l
)
Using
Lemma
..
b
y
a
series
of

(l
og

w
)
transformations
(all
allo
w
ed
b
y
Lemma
..)
w
e
get:
C
C
(

p
w
;
l
)

C
C
(

w
;
l
)
+

(l
og

w
)
Using
Lemma
..,
w
e
ha
v
e
C
C
(

w
;
l
)

C
C
(

p
w
;
l

)
and
so
C
C
(

p
w
;
l
)

C
C
(

p
w
;
l

)
+
!
(l
og

w
)
Iterate
the
last
t
w
o
steps
l
og

l
times,
getting
C
C
(;
l
)
=

(l
og

l

l
og

w
)
W
e
conclude
that
the
comm
unication
complexit
y
of
the
f
ork
game
is
(l
og

l

l
og

w
).
Bibliographic
Notes
This
lecture
is
based
on
[].
.
M.
Gringi
and
M.
Sipser.
Monotone
Separation
of
Logarithmic
Space
from
Logarithmic
Depth.
JCSS,
V
ol.
0,
pages
{,
		.

Lecture

Av
erage
Case
Complexit
y
Notes
tak
en
b
y
Tzvik
a
Hartman
and
Hillel
Kugler
Summary:
W
e
in
tro
duce
a
theory
of
a
v
erage
case
complexit
y
.
W
e
dene
the
notion
of
a
distribution
function
and
the
classes
P-computable
and
P-samplable
of
distributions.
W
e
pro
v
e
that
P-computable

P-samplable
(strict
con
tainmen
t).
The
class
DistNP
,
whic
h
is
the
distributional
analogue
of
NP,
is
dened.
W
e
in
tro
duce
the
denition
of
p
olynomial
on
the
a
v
erage
and
discuss
the
w
eaknesses
of
an
alternativ
e
denition.
The
notion
of
reductions
b
et
w
een
distributional
problems
is
presen
ted.
Finally
,
w
e
pro
v
e
the
existence
of
a
problem
that
is
complete
for
DistNP
.
.
In
tro
duction
T
raditionally
,
in
theoretical
computer
science,
the
emphasis
of
the
researc
h
is
on
the
w
orst
case
complexit
y
of
problems.
Ho
w
ev
er,
one
ma
y
think
that
the
more
natural
(and
practical)
w
a
y
to
measure
the
complexit
y
of
a
problem
is
b
y
considering
its
a
v
erage
case
complexit
y
.
Man
y
imp
ortan
t
problems
w
ere
found
to
b
e
NP-complete
and
there
is
little
hop
e
to
solv
e
them
ecien
tly
in
the
w
orst
case.
In
these
cases
it
w
ould
b
e
useful
if
w
e
could
dev
elop
algorithms
that
solv
e
them
ecien
tly
on
the
a
v
erage.
This
is
the
main
motiv
ation
for
the
theory
of
a
v
erage
case
complexit
y
.
When
discussing
the
a
v
erage
case
complexit
y
of
a
problem
w
e
m
ust
sp
ecify
the
distribution
from
whic
h
the
instances
of
the
problem
are
tak
en.
It
is
p
ossible
that
the
same
problem
is
ecien
tly
solv
able
on
the
a
v
erage
with
resp
ect
to
one
distribution,
but
hard
on
the
a
v
erage
with
resp
ect
to
another.
One
ma
y
think
that
it
is
enough
to
consider
the
most
natural
distribution
-
the
uniform
one.
Ho
w
ev
er,
in
man
y
cases
it
is
more
realistic
to
assume
settings
in
whic
h
some
instances
are
more
probable
than
others
(e.g.
some
graph
problems
are
most
in
teresting
on
dense
graphs,
hence,
the
uniform
distribution
is
not
relev
an
t).
It
is
in
teresting
to
compare
the
a
v
erage
case
complexit
y
theory
with
cryptograph
y
theory
,
since
they
deal
with
similar
issues.
One
dierence
b
et
w
een
the
t
w
o
theories
is
that
in
cryptograph
y
w
e
deal
with
problems
that
are
dicult
on
the
a
v
erage,
while
in
the
a
v
erage
case
theory
w
e
try
to
nd
problems
that
are
easy
on
the
a
v
erage.
Another
dierence
is
that
in
cryptograph
y
w
e
need
problems
for
whic
h
it
is
p
ossible
to
generate
ecien
tly
instance-solution
pairs
suc
h
that
solving
the
problem
giv
en
only
the
instance
is
hard.
In
con
trast,
this
prop
ert
y
is
not
required
in
a
v
erage
case
complexit
y
theory
.



LECTURE
.
A
VERA
GE
CASE
COMPLEXITY
.
Denitions
..
Distributions
W
e
no
w
in
tro
duce
the
notion
of
a
distribution
function.
W
e
assume
a
canonical
order
of
the
binary
strings
(e.g.
the
standard
lexicographic
order).
The
notation
x
<
y
means
that
the
string
x
precedes
y
in
this
order
and
x
 
denotes
the
immediate
predecessor
of
x.
Denition
.
(Probabilit
y
Distribution
F
unction)
A
distribution
function

:
f0;
g

!
[0;
]
is
a
non-negativ
e
and
non-decreasing
function
(i.e.,
(0)

0;
(x)

(y
)
for
eac
h
x
<
y
)
from
strings
to
the
unit
in
terv
al
[0;
]
whic
h
con
v
erges
to
one
(i.e.,
l
im
x!
(x)
=
).
The
density
function
asso
ciated
with
the
distribution
function

is
denoted
b
y

0
and
dened
b
y

0
(0)
=
(0)
and

0
(x)
=
(x)
 (x
 )
for
ev
ery
x
>
0.
Clearly
,
(x)
=
P
y
x

0
(y
).
Notice
that
w
e
dened
a
single
distribution
on
all
inputs
of
all
sizes,
rather
than
ensem
bles
of
nite
distributions
(eac
h
ranging
o
v
er
xed
length
strings).
This
mak
es
the
denition
robust
under
dieren
t
represen
tations.
An
imp
ortan
t
example
is
the
uniform
distribution
function
dened
b
y

0
u
def
=

jxj



 jxj
.
This
densit
y
function
con
v
erges
to
some
constan
t
dieren
t
than
.
A
minor
mo
dication,
dening

0
u
def
=

jxj(jx+j)


 jxj
,
settles
this
problem:
X
xf0;g


0
u
(x)
=
X
xf0;g


jxj

jxj
+



 jxj
=
X
nN
X
xf0;g
n
(

n
 
n
+

)


 n
=
X
nN
(

n
 
n
+

)
=

W
e
will
use
a
notation
for
the
probabilit
y
mass
of
a
string
relativ
e
to
all
strings
of
equal
size:

0
n
(x)
def
=

0
(x)
P
jy
j=jxj

0
(y
)
.
..
Distributional
Problems
Av
erage
case
complexit
y
is
meaningful
only
if
w
e
asso
ciate
a
problem
with
a
sp
ecic
distribution
of
its
instances.
W
e
will
consider
only
decision
problems.
Similar
form
ulations
for
searc
h
problems
can
b
e
easily
deriv
ed.
Denition
.
(Distributional
Problem)
A
distributional
de
cision
pr
oblem
is
a
pair
(D
;
),
where
D
:
f0;
g

!
f0;
g
and

:
f0;
g

!
[0;
]
is
a
distribution
function.
..
Distributional
Classes
Before
dening
classes
of
distributional
problems
w
e
should
consider
classes
of
distributions.
It
is
imp
ortan
t
to
restrict
the
distributions,
otherwise,
the
whole
theory
collapses
to
the
w
orst
case
complexit
y
theory
(b
y
c
ho
osing
distributions
that
put
all
the
probabilit
y
mass
on
the
w
orst
cases).
W
e
will
consider
only
\simple"
distributions
in
a
computational
sense.
Denition
.
(P-samplable)
A
distribution

is
in
the
class
P-samplable
if
there
is
a
probabilis-
tic
T
uring
mac
hine
that
gets
no
input
and
outputs
a
binary
string
x
with
probabilit
y

0
(x),
while
running
in
time
p
olynomial
in
jxj.

..
DEFINITIONS

Denition
.
(P-computable)
A
distribution

is
in
the
class
P-c
omputable
if
there
is
a
deter-
ministic
p
olynomial
time
T
uring
mac
hine
that
on
input
x
outputs
the
binary
expansion
of
(x).
In
teresting
distributions
m
ust
put
noticeable
probabilit
y
mass
on
long
strings
(i.e.,
at
least

pol
y
(n)
on
strings
of
length
n).
Consider
to
the
con
trary
the
densit
y
function

0
(x)
def
=

 jxj
.
An
algorithm
of
exp
onen
tial
running
time,
t(x)
=

jxj
,
will
b
e
considered
to
ha
v
e
constan
t
on
the
a
v
erage
running
time
with
resp
ect
to
this
distribution
(since
P
x

0
(x)

t(jxj)
=
P
n

 n
=
).
In
tuitiv
ely
,
this
distribution
do
es
not
mak
e
sense
since
usually
the
long
instances
are
the
dicult
ones.
By
assigning
negligible
probabilit
y
to
these
long
instances,
w
e
can
articially
mak
e
the
a
v
erage
running
time
of
the
algorithm
small,
ev
en
though
the
algorithm
is
not
ecien
t.
Consider,
for
example,
an
extreme
case
in
whic
h
all
instances
of
size
greater
than
some
constan
t
ha
v
e
zero
probabilit
y
.
In
this
case,
ev
ery
algorithm
has
a
v
erage
constan
t
running
time.
W
e
no
w
sho
w
that
the
uniform
distribution
is
P-computable.
F
or
ev
ery
x,

u
(x)
=
X
y
x

0
u
(y
)
=
X
y
x
jy
j<jxj

0
u
(y
)
+
X
y
x
jy
j=jxj

0
u
(y
)
=


 
jxj

+

N
x


jxj(jxj
+
)


 jxj

where
N
x
def
=
jfy

f0;
g
jxj
:
y

xgj
=

+
P
jxj
i=

i 

x
i
,
where
x
=
x
n



x

.
Ob
viously
,
this
expression
can
b
e
computed
in
time
p
olynomial
in
the
length
of
x.
Prop
osition
..
P-c
omputable

P-samplable
(strict
c
ontainment
assuming
#P
=
P)
Pro
of:
W
e
pro
v
e
the
prop
osition
in
t
w
o
steps:
.
Claim
..
F
or
every
distribution
,
if


P-c
omputable
then


P-samplable.
Pro
of:
Let

b
e
a
distribution
that
is
P-computable.
W
e
describ
e
an
algorithm
that
samples
strings
according
to
the
distribution
,
assuming
that
w
e
can
compute

in
p
olynomial
time.
In
tuitiv
ely
,
w
e
can
view
the
algorithm
as
pic
king
a
random
real
n
um
b
er
r
in
[0;
)
and
c
hec
king
whic
h
string
x

f0;
g

satises
(x
 )
<
r

(x).
Unfortunately
,
this
is
not
p
ossible
since
r
has
innite
precision.
T
o
o
v
ercome
this
problem,
w
e
select
randomly
in
eac
h
step
one
bit
of
the
expansion
(of
r
)
and
stop
when
w
e
are
guaran
teed
that
there
is
a
unique
x
satisfying
the
requiremen
t
ab
o
v
e
(i.e.,
ev
ery
p
ossible
extension
will
yield
the
same
x).
The
algorithm
is
an
iterativ
e
pro
cedure
with
a
stopping
condition.
In
eac
h
iteration
w
e
select
one
bit.
W
e
view
the
bits
selected
so
far
as
the
binary
expansion
of
a
truncated
real
n
um
b
er,
denoted
b
y
t.
No
w
w
e
nd
the
smallest
n
suc
h
that
(
n
)

t.
By
p
erforming
a
binary
searc
h
o
v
er
all
binary
strings
of
length
n
w
e
nd
the
greatest
x

f0;
g
n
suc
h
that
(x)
<
t.
A
t
this
p
oin
t
w
e
c
hec
k
if
(x
+
)

t
+

 l
,
where
l
is
the
length
of
the
binary
expansion
of
t.
If
so,
w
e
halt
and
output
x
+
.
Otherwise,
w
e
con
tin
ue
to
the
next
iteration.
Ob
viously
,
this
can
b
e
implemen
ted
in
p
olynomial
time.
.
Claim
..
Ther
e
exists
a
distribution
which
is
P-samplable
but
not
P-c
omputable,
under
the
assumption
#P
=
P.
Pro
of:
Let
R

f0;
g


f0;
g

b
e
an
NP
relation
(w
e
assume
for
simplicit
y
that
R
con
tains
only
pairs
of
strings
with
the
equal
lengths).
W
e
dene
a
distribution
function:

0
(x



y
)
def
=
(
0
if
R
(x;
y
)
=


jxj

jy
j


jxj

jy
j
otherwise


LECTURE
.
A
VERA
GE
CASE
COMPLEXITY
for
ev
ery
x;
y

f0;
g

of
equal
length
and


f0;
g
.
The
fact
that
this
distribution
function
con
v
erges
to
a
constan
t
can
b
e
easily
v
eried
using
the
fact
that
for
ev
ery
x;
y
exactly
one
of
the
p
ossible
v
alues
of

giv
es
a
non-zero
probabilit
y
.
W
e
no
w
sho
w
that

is
P-samplable.
The
algorithm
samples
uniformly
a
string
of
length
n,
denoted
x

y
,
where
jxj
=
jy
j
=
n
(recall
that
the
uniform
distribution
is
P-computable
and
th
us
P-samplable).
Next,

is
dened
as
R
(x;
y
),
whic
h
is
an
NP-relation
and
therefore
can
b
e
computed
ecien
tly
.
The
algorithm
outputs
x



y
.
The
next
step
is
to
sho
w
that
if

is
P-computable
then
#P
problems
can
b
e
solv
ed
in
p
olynomial
time.
The
n
um
b
er
of
NP
witnesses
for
x
is
giv
en
b
y
the
follo
wing
expression:
(x




n
)
 (x

0


n
)

n


n
=
(
(x




n
)
 (x

0


n
))

n



n
The
n
umerator
is
the
probabilit
y
mass
of
all
strings
that
end
with
an
NPwitness
for
x.
By
normalizing
w
e
get
the
actual
n
um
b
er
of
witnesses,
th
us
solving
the
#P
problem.
The
prop
osition
follo
ws
directly
from
the
t
w
o
claims.
In
the
sequel
w
e
will
fo
cus
on
the
P-computable
class.
..
Distributional-NP
W
e
no
w
dene
the
a
v
erage-case
analogue
to
the
class
NP:
Denition
.
(The
class
DistNP)
A
distributional
problem
(D
;
)
b
elongs
to
the
class
DistNP
if
D
is
an
NP-predicate
and

is
P-computable.
DistNP
is
also
denoted
hN
P
;
P
 computabl
ei.
The
class
hN
P
;
P
 sampl
abl
ei
is
dened
similarly
.
..
Av
erage
P
olynomial
Time
The
follo
wing
denition
ma
y
seem
obscure
at
rst
glance.
In
the
app
endix
w
e
discuss
the
w
eaknesses
of
alternativ
e
naiv
e
form
ulation.
Denition
.
(P
olynomial
on
the
Av
erage)
A
problem
D
is
p
olynomial
on
the
aver
age
with
resp
ect
to
a
distribution

if
there
exists
an
algorithm
A
that
solv
es
D
in
time
t
A
()
and
there
exists
a
constan
t

>
0
suc
h
that
X
xf0;g


0
(x)

t
A
(x)

jxj
<

A
necessary
prop
ert
y
that
a
v
alid
denition
should
ha
v
e
is
that
a
function
that
is
p
olynomial
in
the
w
orst
case
should
b
e
p
olynomial
on
the
a
v
erage.
Assume
that
x
d
b
ounds
the
running-time
of
the
problem
and
let

=

d+
.
This
function
is
p
olynomial
on
the
a
v
erage
(with
resp
ect
to
an
y
)
according
to
Denition
:
X
xf0;g


0
(x)

t
A
(x)

jxj

X
xf0;g


0
(x)

jxj
d
d+
jxj

X
xf0;g


0
(x)
=

<


..
DISTNP-COMPLETENESS
	
W
e
will
no
w
try
to
giv
e
some
in
tuition
for
Denition
.
A
natural
denition
for
the
notion
of
a
function
f
()
that
is
\constan
t
on
the
a
v
erage"
with
resp
ect
to
the
distribution

is
requiring
X
xf0;g


0
(x)

f
(x)
<

Using
this
denition,
g
()
is
called
\linear
on
the
a
v
erage"
if
g
(x)
=
O
(f
(x)

jxj)
where
f
()
is
constan
t
on
the
a
v
erage.
This
implies
X
xf0;g


0
(x)
g
(x)
jxj
<

A
natural
extension
of
this
denition
for
the
case
of
p
olynomial
on
the
a
v
erage
yields
Denition
.
..
Reductions
W
e
no
w
in
tro
duce
the
denition
of
a
reduction
of
one
distributional
problem
to
another.
In
the
w
orst
case
reductions,
the
t
w
o
requiremen
ts
are
eciency
and
v
alidit
y
.
In
the
distributional
case
w
e
also
require
that
the
reduction
\preserv
e"
the
probabilit
y
distribution.
The
purp
ose
of
the
last
requiremen
t
is
to
ensure
that
the
reduction
do
es
not
map
v
ery
lik
ely
instances
of
the
rst
problem
to
rare
instances
of
the
second
problem.
Otherwise,
ha
ving
a
p
olynomial
time
on
the
a
v
erage
algorithm
for
the
second
distributional
problem
do
es
not
necessary
yield
suc
h
an
algorithm
for
the
rst
distributional
problem.
This
requiremen
t
is
captured
b
y
the
domination
condition.
Denition
.
(Av
erage
Case
Reduction)
W
e
sa
y
that
the
distributional
problem
(D

;


)
re-
duces
to
(D

;


)
(denote
(D

;


)
/
(D

;


)
)
if
there
exists
a
p
olynomial
time
computable
function
f
suc
h
that
.
validity:
x

D

i
f
(x)

D

.
.
domination:
There
exists
a
constan
t
c
>
0
suc
h
that
for
ev
ery
y

f0;
g

,
X
xf
 
(y
)

0

(x)

jy
j
c


0

(y
)
The
follo
wing
prop
osition
sho
ws
that
the
reduction
dened
ab
o
v
e
is
adequate:
Prop
osition
..
If
(D

;


)
/
(D

;


)
and
(D

;


)
is
p
olynomial
on
the
aver
age
then
so
is
(D

;


).
See
pro
of
in
App
endix
B.
.
DistNP-completeness
In
this
section
w
e
state
t
w
o
theorems
regarding
DistNP-complete
problems
and
pro
v
e
the
rst
one.
Theorem
.
Ther
e
exists
a
DistNP-c
omplete
pr
oblem.
Theorem
.	
Every
pr
oblem
c
omplete
for
DistNP
is
also
c
omplete
for
hN
P
;
P
 sampabl
ei.

0
LECTURE
.
A
VERA
GE
CASE
COMPLEXITY
Pro
of:
(of
rst
theorem)
W
e
rst
dene
a
distributional
v
ersion
of
the
Bounde
d
Halting
problem
(for
a
discussion
on
the
Bounde
d
Halting
problem
see
Lecture
.).
W
e
then
sho
w
that
it
is
in
DistNP-complete.
Denition
.0
(Distributional
Bounded
Halting)
.
De
cision:
B
H
(M
;
x;

k
)
=

i
there
exists
a
computation
of
the
non-deterministic
mac
hine
M
on
input
x
whic
h
halts
within
k
steps.
.
Distribution:

0
B
H
(M
;
x;

k
)
def
=

jM
j



jM
j


jxj



jxj


k

Pro
ving
completeness
results
for
distributional
problems
is
more
complicated
than
usual.
The
dicult
y
is
that
w
e
ha
v
e
to
reduce
all
DistNP
problems
with
dieren
t
distributions
to
one
sin-
gle
distributional
problem
with
a
sp
ecic
distribution.
In
the
w
orst
case
v
ersion
w
e
used
the
reduction
x
!
(M
D
;
x;

P
D
(jxj)
),
where
D
is
the
NP
problem
w
e
w
an
t
to
reduce
and
M
D
is
the
non-deterministic
mac
hine
that
solv
es
D
in
time
P
D
(n)
on
inputs
of
length
n
(see
Lecture
.).
A
rst
attempt
is
to
use
exactly
this
reduction.
This
reduction
is
v
alid
for
ev
ery
DistNP
problem,
but
for
some
distributions
it
violates
the
domination
condition.
Consider
for
example
distributional
problems
in
whic
h
the
distribution
of
(innitely
man
y)
strings
is
m
uc
h
higher
than
the
distribu-
tion
assigned
to
them
b
y
the
uniform
distribution.
In
suc
h
cases,
the
standard
reduction
maps
an
instance
x
ha
ving
probabilit
y
mass

0
(x)


 jxj
to
a
triple
(M
D
;
x;

P
D
(jxj)
)
with
m
uc
h
ligh
ter
probabilit
y
mass
(recall
that

0
B
H
(M
D
;
x;

P
D
(jxj)
)
<

 jxj
).
Th
us
the
domination
condition
is
not
satised.
The
essence
of
the
problem
is
that

0
B
H
giv
es
lo
w
probabilit
y
to
long
strings,
whereas
an
arbitrary
distribution
can
giv
e
them
high
probabilit
y
.
T
o
o
v
ercome
this
problem,
w
e
will
map
long
strings
with
high
probabilit
y
to
short
strings,
whic
h
get
high
probabilit
y
in

0
B
H
.
W
e
will
use
an
enco
ding
of
strings
whic
h
maps
a
string
x
in
to
a
co
de
of
length
b
ounded
ab
o
v
e
b
y
log



0
(x)
.
W
e
will
use
the
follo
wing
tec
hnical
co
ding
lemma:
Lemma
..
(Co
ding
Lemma):
L
et

b
e
a
p
olynomial-time
c
omputable
distribution
function
(i.e.,


P
 computabl
e).
Then
ther
e
exists
a
c
o
ding
function
C

satisfying
the
fol
lowing
thr
e
e
c
onditions:
.
Ecien
t
enco
ding:
The
function
C

is
c
omputable
in
p
olynomial
time.
.
Unique
deco
ding:
The
function
C

is
one-to-one.
.
Compression:
F
or
every
x
jC

(x)j


+
minfjxj;
log



0
(x)
g
Pro
of:
The
function
C

is
dened
as
follo
ws:
C

(x)
def
=
(
0

x
if

0
(x)


 jxj


z
otherwise
where
z
is
the
longest
common
prex
of
the
binary
expansions
of
(x
 )
and
(x)
(e.g.,
if
(00)
=
0:0000
and
(0)
=
0:00
then
C

(0)
=
0).
The
in
tuition
b
ehind
this

..
DISTNP-COMPLETENESS

denition
is
that
w
e
w
an
t
to
nd
uniquely
a
p
oin
t
with
certain
precision
in
the
in
terv
al
b
et
w
een
(x
 )
and
(x).
As
the
length
of
the
in
terv
al
gro
ws,
the
precision
needed
is
lo
w
er.
Recall
that
the
length
of
the
in
terv
al
is
exactly
the
v
alue
of

0
(x),
implying
the
short
enco
ding
of
high
probabilit
y
strings.
W
e
no
w
sho
w
that
C

(x)
satises
the
conditions
of
the
lemma:
.
Ecient
enc
o
ding:
The
eciency
of
the
enco
ding
follo
ws
from
the
fact
that

is
a
p
olynomial
time
computable
function.
.
Unique
de
c
o
ding:
In
the
rst
case,
in
whic
h
C

(x)
=
0

x,
the
unique
deco
ding
is
ob
vious.
In
the
second
case,
in
whic
h
C

(x)
=


z
,
since
the
in
terv
als
are
disjoin
t
and
(x
 )
<
0:z


(x),
ev
ery
z
determines
uniquely
the
enco
ded
x,
and
the
unique
deco
ding
conditions
follo
ws.
.
Compr
ession:
In
the
rst
case,
in
whic
h

0
(x)


 jxj
,
jC

(x)j
=

+
jxj


+
log



0
(x)
.
In
the
second
case,
in
whic
h

0
(x)
>

 jxj
,
let
l
=
jz
j
and
z




z
l
b
e
the
binary
represen
tation
of
z
.
Then,

0
(x)
=
(x)
 (x
 )

0
@
l
X
i=

 i
z
i
+
pol
y
(jxj)
X
i=l
+

 i

A
 l
X
i=

 i
z
i
<

 jz
j
So
jz
j

log



0
(x)
,
and
the
compression
condition
follo
ws.
No
w
w
e
use
the
Co
ding
Lemma
to
complete
the
pro
of
of
the
theorem.
W
e
dene
the
follo
wing
reduction
of
(D
;
)
to
(B
H
;

B
H
):
x
!
(M
D
;
;
C

(x);

P
D
;
(jxj)
)
where
M
D
;
is
a
non-deterministic
mac
hine
that
on
input
y
guesses
non-deterministically
x
suc
h
that
C

(x)
=
y
(notice
that
the
non-determinism
allo
ws
us
not
to
require
ecien
t
deco
ding),
and
then
runs
M
D
on
x.
The
p
olynomial
P
D
;
(n)
is
dened
as
P
D
(n)
+
P
C
(n)
+
n,
where
P
D
(n)
is
a
p
olynomial
b
ounding
the
running
time
of
M
D
on
acceptable
inputs
of
length
n
and
P
C
(n)
is
a
p
olynomial
b
ounding
the
running
time
of
the
enco
ding
algorithm.
It
remains
to
sho
w
that
this
reduction
satises
the
three
requiremen
ts.
.
Eciency:
The
description
of
M
D
;
is
of
xed
length
and
b
y
the
co
ding
lemma
C

is
com-
putable
b
y
p
olynomial
time.
Therefore,
the
reduction
is
ecien
t.
.
V
alidity:
By
construction
of
M
D
;
it
follo
ws
that
D
(x)
=

if
and
only
if
there
exists
a
computation
of
mac
hine
M
D
;
that
on
input
C

(x)
halts
with
output

within
P
D
;
(jxj)
steps.
.
Domination:
Notice
that
it
suces
to
consider
instances
of
Bounde
d
Halting
whic
h
ha
v
e
a
preimage
under
the
reduction.
Since
the
co
ding
is
one-to-one,
eac
h
suc
h
image
has
a
unique
preimage.
By
the
denition
of

B
H
,

0
B
H
(M
D
;
;
C

(x);

P
D
;
(jxj)
)
=
c


P
D
;
(jxj)



jC

(x)j



jc

(x)j


LECTURE
.
A
VERA
GE
CASE
COMPLEXITY
where
c
=

jM
D
;
j


jM
D
;
j
is
a
constan
t
indep
enden
t
of
x.
By
the
compression
requiremen
t
of
the
co
ding
lemma,

0
(x)




 jC

(x)j
Hence,

0
B
H
(M
D
;
;
C

(x);

P
D
;
(jxj)
)

c


P
D
;
(jxj)



jC

(x)j



0
(x)

>
c


P
D
;
(jxj)


jC

(x)j



0
(x)
Therefore,
the
reduction
satises
the
requiremen
ts
and
the
distributional
v
ersion
of
the
Bounde
d
Halting
problem
is
DistNP-complete.
Bibliographic
Notes
The
theory
of
a
v
erage-case
complexit
y
w
as
initiated
b
y
Levin
[].
Theorem
.	
is
due
to
[].
The
lecture
w
as
based
on
the
exp
osition
in
[].
F
or
further
in
v
estigations
see
[].
.
S.
Ben-Da
vid,
B.
Chor,
O.
Goldreic
h,
and
M.
Lub
y
.
On
the
Theory
of
Av
erage
Case
Com-
plexit
y
.
Journal
of
Computer
and
system
Scienc
es,
V
ol.
,
No.
,
April
		,
pp.
	{	.
.
O.
Goldreic
h.
Notes
on
Levin's
Theory
of
Av
erage-Case
Complexit
y
.
ECCC,
TR	-0,
Dec.
		.
.
R.
Impagliazzo
and
L.A.
Levin.
No
Better
W
a
ys
to
Generate
Hard
NP
Instances
than
Pic
king
Uniformly
at
Random.
In
Pr
o
c.
of
the
st
F
OCS,
		0,
pp.
{.
.
L.A.
Levin.
Av
erage
Case
Complete
Problems.
SIAM
Jour.
of
Computing,
V
ol.
,
pages
{,
	.
App
endix
A
:
F
ailure
of
a
naiv
e
form
ulation
W
e
no
w
discuss
an
alternativ
e
denition
of
the
notion
of
p
olynomial
on
the
a
v
erage
whic
h
seems
more
natural:
Denition
.
(Naiv
e
F
orm
ulation
of
P
olynomial
on
the
Av
erage)
A
problem
D
is
p
olynomial
on
the
aver
age
with
resp
ect
to
a
distribution

if
there
exists
an
algorithm
A
that
solv
es
D
in
time
t
A
()
and
there
exists
a
constan
t
c
>
0
suc
h
that
for
ev
ery
n
X
xf0;g
n

0
n
(x)

t
A
(x)
<
n
c
There
are
three
main
problems
with
this
naiv
e
denition:
.
This
denition
is
v
ery
dep
enden
t
on
the
particular
enco
ding
of
the
problem
instance.
In
this
denition,
the
a
v
erage
is
tak
en
o
v
er
all
instances
of
equal
length.
Changing
the
enco
ding
of
the
problem
instances
do
es
not
preserv
e
the
partition
of
instances
according
to
their
length
and
hence
do
es
not
preserv
e
the
a
v
erage
running
time
of
the
algorithm.

..
DISTNP-COMPLETENESS

.
This
denition
is
not
robust
under
functional
comp
osition
of
algorithms.
Namely
,
if
distri-
butional
problem
A
can
b
e
solv
ed
in
a
v
erage
p
olynomial
time
giv
en
access
to
an
oracle
for
distributional
problem
B
and
B
can
b
e
solv
ed
in
a
v
erage
p
olynomial
time,
then
it
do
es
not
follo
w
that
A
can
b
e
solv
ed
in
a
v
erage
p
olynomial
time.
.
This
denition
is
not
mac
hine
indep
enden
t,
i.e.,
an
algorithm
can
b
e
p
olynomial
on
the
a
v
erage
in
one
reasonable
computational
mo
del,
but
hard
on
the
a
v
erage
in
another
(e.g.
the
sim
ulation
of
a
t
w
o-tap
e
T
uring
mac
hine
on
a
one-tap
e
T
uring
mac
hine).
The
t
w
o
last
problems
stem
from
the
fact
that
the
denition
is
not
closed
under
application
of
p
olynomials.
F
or
example,
consider
a
function
t(x)
dened
as
follo
ws:
t(x)
def
=
(

n
if
x
=

n
n

otherwise
This
function
is
clearly
p
olynomial
on
the
a
v
erage
with
resp
ect
to
the
uniform
distribution
(i.e.,
for
ev
ery
x

f0;
g
n
;

0
n
(x)
=

 n
).
This
is
true
since
X
xf0;g
n

0
n
(x)

t(x)
=

 n


n
+
(
 
 n
)

n

<
n

+

On
the
other
hand,
X
xf0;g
n

0
n
(x)

t

(x)
=

 n


n
+
(
 
 n
)

n

>

n
whic
h
implies
that
the
function
t

(x)
is
not
p
olynomial
on
the
a
v
erage.
This
problem
do
es
not
o
ccur
in
Denition

since
if
t(x)
is
p
olynomial
on
the
a
v
erage
with
the
constan
t

then
t

(x)
is
p
olynomial
on
the
a
v
erage
with
the
constan
t


.
App
endix
B
:
Pro
of
Sk
etc
h
of
Prop
osition
..
In
this
app
endix
w
e
sk
etc
h
the
pro
of
of
prop
osition
..
W
e
rst
restate
the
prop
osition:
Prop
osition
.
:
If
(D

;


)
/
(D

;


)
and
(D

;


)
is
p
olynomial
on
the
a
v
erage
then
so
is
(D

;


).
The
formal
pro
of
of
the
prop
osition
has
man
y
tec
hnical
details,
so
it
will
b
e
only
sk
etc
hed.
As
a
w
arm-up,
w
e
will
rst
pro
v
e
the
prop
osition
under
some
simplifying
assumptions.
T
o
our
b
elief,
this
giv
es
in
tuition
to
the
full
pro
of.
The
simplifying
assumptions,
regarding
the
denition
of
a
reduction,
are:
.
There
exists
a
constan
t
c

suc
h
that
for
ev
ery
x,
jf
(x)j

c


jxj,
where
f
is
the
reduction
function.
.
Strong
domination
condition:
There
exists
a
constan
t
c

suc
h
that
for
ev
ery
y

f0;
g

,
X
xf
 
(y
)

0

(x)

c



0

(y
)


LECTURE
.
A
VERA
GE
CASE
COMPLEXITY
Pro
of:
(of
simplie
d
version)
The
distributional
problem
(D

;


)
is
p
olynomial
on
the
a
v
erage
implies
that
there
exists
an
algorithm
A

and
a
constan
t


>
0
suc
h
that
()
X
xf0;g


0

(x)

t
A

(x)


jxj
<

W
e
need
to
pro
v
e
that
(D

;


)
is
p
olynomial
on
the
a
v
erage,
i.e.
that
there
exists
an
algorithm
A

and
a
constan
t


>
0
suc
h
that
()
X
xf0;g


0

(x)

t
A

(x)


jxj
<

The
algorithm
A

,
when
giv
en
x,
applies
the
reduction
f
on
x
and
then
applies
A

on
f
(x).
Therefore,
t
A

(x)
=
t
f
(x)
+
t
A

(f
(x)),
where
t
f
denotes
the
time
required
to
compute
f
.
F
or
the
sak
e
of
simplicit
y
w
e
ignore
t
f
(x)
and
assume
t
A

(x)
=
t
A

(f
(x)).
T
aking


def
=


w
e
obtain:
X
xf0;g


0

(x)

t
A

(x)


jxj
=
X
y
f0;g

X
xf
 
(y
)

0

(x)

t
A

(y
)


jxj

c


X
y
f0;g

t
A

(y
)


jy
j
X
xf
 
(y
)

0

(x)

c


c


X
y
f0;g

t
A

(y
)




0

(y
)
jy
j
<

The
rst
inequalit
y
uses
Assumption
,
and
the
second
uses
the
simplied
domination
condition
(Assumption
).
Sk
etc
h
of
full
pro
of:
First
w
e
explain
ho
w
to
deal
with
the
tec
hnical
problem
that
arises
when
considering
the
running
time
of
the
reduction,
t
f
(x),
in
the
running
time
of
algorithm
A

.
This
tec
hnical
problem
arises
man
y
times
in
the
pro
of
and
is
solv
ed
as
describ
ed
b
elo
w.
The
fact
that
for
ev
ery


,
(a
+
b)


a

+
b

is
used
to
b
ound
expression
()
b
y
t
w
o
sums.
The
rst
sum
con
tains
the
factor
t
f
(x)
and
can
b
e
easily
b
ounded
b
y
c
ho
osing
the
appropriate
.
The
second
sum
con
v
erges
as
sho
wn
in
the
pro
of
of
the
simplied
v
ersion.
In
order
to
pro
v
e
that
expression
()
con
v
erges
w
e
partition
the
sum
in
expression
()
in
to
t
w
o
sums
and
sho
w
separately
that
eac
h
sum
con
v
erges.
F
ormally
,
y

f0;
g

is
called
a
b
ad
y
if

0

(y
)

t
A

(y
)



jy
j
and
is
called
a
go
o
d
y
otherwise.
W
e
partition
the
x's
according
to
the
\go
o
dness"
of
their
images
under
f
.
This
induces
the
follo
wing
partition
in
to
t
w
o
sums.
Denote
B
=
X
bad
y
X
xf
 
(y
)

0

(x)

t
A

(y
)


jxj
G
=
X
g
ood
y
X
xf
 
(y
)

0

(x)

t
A

(y
)


jxj
Then
X
xf0;g


0

(x)

t
A

(x)


jxj
=
X
y
X
xf
 
(y
)

0

(x)

t
A

(y
)


jxj
=
B
+
G

..
DISTNP-COMPLETENESS

The
in
tuition
b
ehind
this
partition
is
that
eac
h
bad
y
con
tributes
large
w
eigh
t
(at
least
one)
to
the
sum
in
expression
().
The
fact
that
expression
()
con
v
erges
implies
that
there
is
a
nite
n
um
b
er
of
bad
y
's.
This
is
used
to
sho
w
that
B
con
v
erges.
F
or
the
second
sum
w
e
again
partition
it
in
to
t
w
o
sums,
G

and
G

.
The
rst
sum,
G

,
consists
of
go
o
d
y
's
for
whic
h
t
A

(y
)
is
b
ounded
b
y
p(jxj)
for
ev
ery
x

f
 
(x)
and
some
p
olynomial
p
(that
dep
ends
on


and
the
domination
constan
t
c).
This
sum
can
b
e
b
ounded
b
y
c
ho
osing
an


so
that
t
A

(y
)


<
jxj
for
an
y
x

f
 
(y
).
That
is,
X
y
:t
A

(y
)<min
xf
 
(y
)
fp(jxj)g
X
xf
 
(y
)

0

(x)

t
A

(y
)


jxj
<
X
x

0

(x)
=

The
second
sum,
G

,
consists
of
the
rest
of
the
go
o
d
y
's.
F
or
eac
h
y
in
this
sum,
w
e
ha
v
e
t
A

(y
)

q
(y
)
def
=
min
xf
 
(y
)
fp(jxj)g.
Note
that
q
gro
ws
at
least
as
a
p
o
w
er
of
p
(dep
ending
on
the
relation
of
jf
(x)j
to
jxj);
that
is
for
some

>
0
w
e
ha
v
e
q
(y
)

p(jy
j

).
By
a
suitable
c
hoice
of
p
and


,
w
e
ha
v
e
jy
j
c

t
A

(y
)


=
jy
j
c

t
A

(y
)


p(jy
j

)


 

<
t
A

(y
)


jy
j
Th
us,
X
y
:t
A

(y
)q
(y
)
X
xf
 
(y
)

0

(x)

t
A

(y
)


jxj

X
y
:t
A

(y
)q
(y
)
jy
j
c

0

(y
)

t
A

(y
)



X
y
:t
A

(y
)q
(y
)

0

(y
)

t
A

(y
)


jy
j
whic
h
con
v
erges
b
eing
a
partial
sum
of
().
Finally
,
w
e
c
ho
ose


to
b
e
the
minim
um
of
all
the


's
used
in
the
pro
of
to
obtain
the
con
v
ergence
of
the
original
sum.


LECTURE
.
A
VERA
GE
CASE
COMPLEXITY

Lecture

Computational
Learning
Theory
Lecture
giv
en
b
y
Dana
Ron
Notes
tak
en
b
y
Oded
Lac
hish
and
Eli
P
orat
Summary:
W
e
dene
a
mo
del
of
learning
kno
wn
as
probably
appro
ximately
correct
(P
A
C)
learning.
W
e
dene
ecien
t
P
A
C
learning,
and
presen
t
sev
eral
ecien
t
P
A
C
learning
algorithms.
W
e
pro
v
e
the
Occam's
Razor
Theorem,
whic
h
reduces
the
P
A
C
learning
problem
to
the
problem
of
nding
a
succinct
represen
tation
for
the
v
alues
of
a
large
n
um
b
er
of
giv
en
lab
eled
examples.
.
T
o
w
ards
a
denition
of
Computational
learning
Learning
is
a
notion
w
e
are
familiar
with
from
ev
ery
da
y
life.
When
em
barking
on
the
task
of
imp
orting
this
notion
in
to
computer
science,
the
rst
natural
step
is
to
op
en
a
dictionary
and
nd
an
exact
meaning
for
it.
A
natural
meaning
that
can
b
e
found
is
\gaining
kno
wledge
through
exp
erience".
With
this
meaning
in
mind,
w
e
set
on
the
task
of
dening
a
formal
computer
science
mo
del
for
learning.
In
order
to
get
a
clue
to
what
the
mo
del
should
lo
ok
lik
e,
it
is
w
orth
while
to
examine
a
real
life
setting.
Learning
to
diagnose
a
new
disease.
A
medical
do
ctor
learns
the
symptoms
for
diagnosing
a
new
disease
b
y
dra
wing
a
n
um
b
er
of
les,
from
a
le
arc
hiv
e.
Eac
h
le
con
tains
a
list
of
the
patien
t's
parameters
suc
h
as
w
eigh
t,
b
o
dy
temp
erature,
age,
etc.,
and
a
lab
el
indicating
the
diagnosis;
that
is,
whether
the
p
erson
has
this
sp
ecic
disease.
The
n
um
b
er
of
examples
the
do
ctor
has
dra
wn
dep
ends
on
ho
w
accurate
he
w
an
ted
his
list
of
symptoms
to
b
e.
Using
these
les
he
concludes
a
list
of
symptoms
for
diagnosing
the
new
disease.
In
order
to
c
hec
k
the
accuracy
of
symptoms
he
concluded,
he
dra
ws
a
lab
eled
le,
uses
the
list
of
symptoms
to
reac
h
a
diagnosis,
and
nally
whether
the
diagnosis
obtained
b
y
him
matc
hes
the
true
diagnosis
pro
vided
in
the
le
(i.e.,
the
lab
el
of
the
le).
Using
this
setting
w
e
can
phrase
the
pro
cess
of
learning
as
follo
ws:
In
order
to
learn
an
unkno
wn
sub
ject
(disease)
from
a
family
of
sub
jects
(family
of
diseases)
the
learner
(do
ctor)
receiv
es
a
measure
of
accuracy
,
with
this
measure
in
mind
he
dra
ws
a
n
um
b
er
of
lab
eled
examples
(les)
from
the
set
of
all
p
ossible
examples
(arc
hiv
e
of
les),
lab
eled
with
resp
ect
to
sub
ject.
Using
the
examples
he
reac
hes
a
rule
(list
of
symptoms)
for
lab
eling
examples.
He
c
hec
ks
his
rule
accuracy
,
b
y
dra
wing
an
example,
and
comparing
its
lab
el
to
the
lab
el
computed
b
y
the
rule
for
this
example.



LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
But
something
is
still
lac
king
in
the
ab
o
v
e
mo
del.
W
e
demonstate
this
b
y
using
the
ab
o
v
e
setting
(of
a
do
ctor
learning
to
diagnose
a
new
disease):
Can
a
do
ctor
learn
a
new
disease
that
o
ccurs
in
a
certain
group
of
p
eople,
when
he
do
es
not
see
a
le
of
a
p
erson
from
this
sp
ecic
group?
Of
course
w
e
cannot
hop
e
for
this
to
happ
en.
Th
us,
the
missing
comp
onen
t
in
the
ab
o
v
e
mo
del
is
that
the
do
ctor's
abilit
y
to
diagnose
will
b
e
tested
against
the
same
distribution
of
examples
(les)
based
on
whic
h
he
has
learned
to
diagnose.
Th
us,
if
in
the
learning
stage
the
do
ctor
didn't
dra
w
an
y
le
of
a
p
erson
from
a
sp
ecic
group,
then
c
hances
are
that
he
w
on't
b
e
ask
ed
to
diagnose
suc
h
a
p
erson
later
(i.e.,
the
the
accuracy
of
symptoms
he
concluded
will
not
b
e
c
hec
k
ed
for
suc
h
a
p
erson).
W
e
stress
that
w
e
do
not
require
the
learner
(do
ctor)
to
ha
v
e
an
y
kno
wledge
regarding
the
distribution
of
examples.
The
comp
onen
ts
of
the
learning
pro
cess:
W
e
call
the
ob
ject
that
conducts
the
learning
pro
cess,
the
le
arner.
In
this
lecture
the
learner
is
an
algorithm.
The
ob
jects
on
whic
h
the
learning
is
done
are
called
instanc
es.
The
sp
ecics
of
the
instances
do
not
in
terest
us.
W
e
are
only
in
terested
in
an
abstract
represen
tation
of
the
parameters
c
haracterizing
it.
W
e
represen
t
an
instance
b
y
a
v
ector
dened
as
follo
ws:
a
v
alue
is
assigned
to
eac
h
parameter
of
the
instance,
this
v
ector
is
the
represen
tation
of
these
v
alues.

X
=
S
n
X
n
Instance
space
{
the
set
of
all
p
ossible
v
alues
of
instance
represen
tation.

D
:
X
n
 !
[0;
]
Underlying
distribution
{
the
(unkno
wn)
probabilit
y
distribution
o
v
er
example
space.
where
n
is
the
n
um
b
er
of
parameters
c
haracterizing
an
instance.
In
this
lecture
the
instance
space
will
b
e
either
f0;
g
n
or
R
n
.
The
learning
is
done
with
resp
ect
to
the
distribution
D
,
from
whic
h
w
e
obtain
indep
enden
t
samples.
W
e
denote
b
y
x

D
an
example
x
dra
wn
from
X
n
according
to
distribution
D
.
The
sub
ject
and
aim
of
our
learning
is
a
lab
eling
of
the
instance
space,
where
b
y
lab
eling
w
e
mean
a
correlation
b
et
w
een
the
instance
space
and
the
a
set
v
alues.
In
this
lecture
w
e
use
the
set
f0;
g
(or
an
y
set
isomorphic
to
it)
as
p
ossible
lab
eling
v
alues.
Eac
h
instance
is
lab
eled
b
y
suc
h
a
v
alue,
and
so
the
lab
eling
(of
all
instances)
is
a
function
from
X
to
f0;
g.
Suc
h
a
lab
eling
is
called
a
c
onc
ept.
W
e
use
the
follo
wing
notations

F
=
S
n
F
n
concept
class
{
family
of
concepts
w
e
ma
y
need
to
learn.
This
family
is
a
subset
of
the
set
of
all
Bo
olean
functions
dened
o
v
er
X
.

f

F
n
target
concept
{
the
concept
whic
h
is
the
sub
ject
of
the
learning.
where
n
app
ears
in
notations
since
a
concept
is
a
function
o
v
er
the
instance
space
X
n
.
The
rule
that
is
the
output
of
the
learner
is
also
a
lab
eling
of
the
instance
space,
therefore
it
is
also
a
Bo
olean
function.
W
e
call
this
function
the
hyp
othesis.
The
nal
comp
onen
t
missing
is
a
measure
of
accuracy
of
the
h
yp
othesis.
W
e
actually
consider
the
complemen
tary
measure;
that
is
the
error
of
the
h
yp
othesis.
The
latter
is
merely
the
probabilit
y
that
for
an
instance
dra
wn
according
to
D
,
whic
h
is
the
v
ery
distribution
used
in
the
learning
pro
cess,
the
h
yp
othesis
agrees
with
the
target
concept.
That
is,
Denition
.
(h
yp
othesis
error):
er
r
f
;D
(h)
is
the
pr
ob
ability
that
the
hyp
othesis
h
disagr
e
es
with
the
tar
get
function
f
on
an
instanc
e
x
dr
awn
fr
om
X
n
ac
c
or
ding
to
D
.
That
is,
er
r
f
;D
(h)
def
=
Pr
xD
[h(x)
=
f
(x)]

..
PR
OBABL
Y
APPR
O
XIMA
TEL
Y
CORRECT
(P
AC
)
LEARNING
	
                                                                                          














                                                                                          














target function
hypotheses
1

X
Figure
.:
The
target
function
and
h
yp
othesis
with
their
dierence
shaded
.
Probably
Appro
ximately
Correct
(P
AC
)
Learning
In
this
mo
del
the
algorithm
is
only
giv
en
partial
information
on
the
target
concept,
that
is,
a
\small"
n
um
b
er
of
examples
dra
wn
indep
enden
tly
b
y
a
giv
en
distribution.
Since
the
examples
are
c
hosen
indep
enden
tly
b
y
a
distribution,
it
could
b
e
the
case
that
the
algorithm
w
as
giv
en
the
same
example
all
the
time.
Therefore
it
is
unconceiv
able
to
exp
ect
the
algorithm
to
giv
e
an
h
yp
othesis
h
whic
h
is
fully
equiv
alen
t
to
the
target
function
f
.
A
realistic
view
to
the
problem
w
ould
b
e
to
exp
ect
with
a
giv
en
probabilit
y
that
the
algorithm
will
supply
an
h
yp
othesis
h
that
is
an
appro
ximation
of
the
target
function
f
with
resp
ect
to
the
underlying
distribution.
The
follo
wing
denitions
capture
these
qualities:
Denition
.
(P
AC
learning
algorithm):
A
n
algorithm
A
is
c
al
le
d
a
P
AC
lea
rning
algo
rithm
fo
r
a
concept
class
F
if
the
fol
lowing
holds:
F
or
every
n,
for
every
function
f

F
n
,
for
every
distribution
D
:
X
n
 !
[0;
],
for
every
err
or
p
ar
ameter
0
<

<

and
for
every
c
ondenc
e
p
ar
ameter
0
<

<
,
given
p
ar
ameters
n;
;

and
a
set
of
f
-lab
ele
d
examples,
f<
x
i
;
f
(x
i
)
>g,
wher
e
x
i
is
dr
awn
indep
endently
under
the
distribution
D
,
the
algorithm
outputs
a
hyp
othesis
h
such
that:
Pr
[er
r
f
;D
(h)

]


 
wher
e
the
pr
ob
ability
is
taken
over
the
choic
e
of
examples,
as
wel
l
as
over
the
internal
c
oin
tosses
of
the
algorithm.
Note
h
=
A(n;
;

;
f<
x
i
;
f
(x
i
)
>g).
Once
w
e
established
a
formal
mo
del,
a
natural
step
w
ould
b
e
to
inquire
up
on
follo
wing
questions:
.
What
is
an
ecien
t
P
AC
learning
algorithm?
.
Ho
w
man
y
examples
do
w
e
need
for
P
AC
learning
a
concept
from
a
concept
class?
In
order
to
deal
with
these
questions
w
e
dene
complexit
y
measures
for
the
P
A
C
learning
mo
del.
A
natural
complexit
y
measure
is
the
n
um
b
er
of
examples
needed:
Denition
.
(sample
complexit
y):
The
sample
complexit
y
of
a
P
A
C
algorithm
is
the
numb
er
of
examples
it
utilizes,
as
a
function
of
n,

and

.

0
LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
The
classical
measure
of
running
time
of
an
algorithm
is
also
applicable
for
this
mo
del.
The
running
time
complexit
y
is
necessarily
larger
then
the
sample
complexit
y
.
In
some
cases
it
app
ears
to
b
e
substain
tially
larger.
Oded's
Note:
That
is,
under
reasonable
complexit
y
assumptions
(e.g.,
existence
of
one-
w
a
y
functions),
there
are
concept
classes
that
can
b
e
P
A
C
learned
using
p
oly
(n;


;
log


)
examples,
but
cannot
b
e
P
A
C
learned
in
p
oly
(n;


;


)-time.
(In
analogy
to
other
con-
texts,
w
e
should
exp
ect
complexities
to
b
e
logarithmic
in
=
.)
Using
these
complexit
y
measures
w
e
can
answ
er
the
rst
question.
W
e
sa
y
that
a
P
AC
learning
algorithm
is
ecien
t
if
it
runs
in
time
p
olynomial
in
n,


,


,
and
siz
e(f
),
where
the
size
of
f

F
n
is
usually
p
olynomial
in
n.
In
the
rest
of
this
lecture,
w
e
fo
cus
on
the
second
question
(i.e.,
consider
only
the
sample
complexit
y
of
P
A
C
algorithms).
Learning
axis-aligned
rectangles
o
v
er
[0;
]

.
W
e
wish
to
design
an
ecien
t
P
AC
learning
algorithm
for
axis-aligned
rectangles
o
v
er
[0;
]

.
Let
us
rst
explicitly
cast
the
problem
according
to
the
P
AC
learning
mo
del.

An
instance
is
a
p
oin
t
in
[0;
]

,
w
e
represen
t
it
b
y
its
x,y
axis
co
ordinates,
that
is
<
x;
y
>.

The
instance
space
is
X
=
fall
<
x;
y
>
represen
tations
of
p
oin
ts
in
[0;
]

g.

A
concept
is
an
axis-aligned
rectangle
in
[0;
]

,
represen
ted
b
y
four
p
oin
ts
<
x
min
;
y
min
;
x
min
;
y
max
>
suc
h
that:
x
min
;
x
max
;
y
min
;
y
max

[0;
]
x
min

x
max
and
y
min

y
max
An
instance
<
x;
y
>
is
in
the
axis
aligned
rectangle
f
represen
ted
b
y
<
x
min
;
y
min
;
x
min
;
y
max
>
if:
x
min

x

x
max
and
y
min

y

y
max
If
<
x;
y
>
is
in
this
f
,
w
e
lab
el
it
b
y
a
\+"
(i.e.,
f
(<
x;
y
>)
=
+),
otherwise
w
e
lab
el
it
b
y
a
\-".

The
concept
class
is
F
=
fall
axis-aligned
rectangle
in
[0;
]

g.
The
target
concept
is
an
axis
aligned
rectangle
f

F
.

Finally
,
as
usual,
the
underlying
distribution
is
denoted
D
,
the
error
parameter
is
0
<

<

and
the
condence
parameter
is
0
<

<
.
F
or
the
sak
e
of
clarit
y
w
e
will
refer
to
the
target
concept
b
y
target
rectangle,
and
to
axis-aligned
rectangles
as
rectangles.
W
e
use
the
follo
wing
notations:

W
D
(g
)
is
the
w
eigh
t
assigned
b
y
distribution
D
to
the
rectangle
g
.
That
is,
W
D
(g
)
def
=
Pr
sD
[g
(s)
=
+]

S
is
a
random
v
ariable
represen
ting
the
set
of
example
p
oin
ts.
Eac
h
elemen
t
in
S
is
dra
wn
according
to
D
.
W
e
let
S
+
denote
the
subset
of
S
lab
eled
\+".
W
e
stress
that
all
probabilities
are
tak
en
o
v
er
the
c
hoice
of
examples.

..
PR
OBABL
Y
APPR
O
XIMA
TEL
Y
CORRECT
(P
AC
)
LEARNING

Claim
..
The
fol
lowing
algorithm
is
an
ecient
P
AC
le
arning
algorithm
for
axis-aligne
d
r
e
ctangles
over
[0;
]

.
If
S
+
is
empty
then
output
the
empty
r
e
ctangle
as
hyp
othesis.
Otherwise,
output
the
r
e
ctangle
r
epr
esente
d
by
<
~
x
min
;
~
y
min
;
~
x
max
;
~
y
max
>,
wher
e
~
x
min
=
the
minimal
x-axis
c
o
or
dinate
in
S
+
.
~
y
min
=
the
minimal
y-axis
c
o
or
dinate
in
S
+
.
~
x
max
=
the
maximal
x-axis
c
o
or
dinate
in
S
+
.
~
y
max
=
the
maximal
y-axis
c
o
or
dinate
in
S
+
.
Pro
of:
Let
f
b
e
the
target
rectangle
and
g
b
e
the
h
yp
othesis
output
b
y
the
algorith.
Note
that
the
h
yp
othesis
rectangle
is
alw
a
ys
con
tained
in
the
target
rectangle
(since
the
b
orders
of
the
former
are
determined
b
y
p
ositiv
e
examples).
W
e
partition
the
pro
of
in
to
t
w
o
cases:
.
W
D
(f
)
>

.
W
D
(f
)


W
e
start
with
the
pro
of
of
the
rst
case.
Giv
en
a
target
rectangle
let
us
dra
w
the
follo
wing
auxiliary
construction:
W
e
co
v
er
the
upp
er
side
of
the
target
rectangle
with
a
line,
w
e
push
this
line
to
w
ards
the
opp
osite
side,
un
til
w
e
get
a
rectangle
A

,
suc
h
that
W
D
(A

)
=


.
Oded's
Note:
W
e
assume
that
an
adequate
stopping
p
oin
t
exists;
that
is,
that
the
distribution
is
suc
h
that
some
rectangle
has
w
eigh
t
smaller
than
=
whereas
a
sligh
tly
bigger
rectangle
has
w
eigh
t
=.
Clearly
,
an
appro
ximation
of
this
assumption
is
go
o
d
enough,
but
ev
en
suc
h
an
appro
ximation
is
not
guaran
teed
to
exist.
This
issue
is
dealt
with
in
an
app
endex.
W
e
rep
eat
this
pro
cess
for
all
other
sides
of
the
residual
target
rectangle
(see
Fig.
.).
W
e
get
rectangle
A

,A

,A

suc
h
that
W
D
(A

)
=
W
D
(A

)
=
W
D
(A

)
=


.
(It
should
b
e
stressed,
that
w
e
assumed
that
this
pro
cess
can
b
e
done,
whic
h
is
not
necessarily
the
case.
W
e
will
deal
with
this
problem
in
app
endix.)
Let
us
lo
ok
at
the
part
of
the
f
,
that
is
not
co
v
ered
b
y
A

,A

,A

and
A

,
it
is
a
rectangle
w
e
call
it
B
.
According
to
auxiliary
construction:
W
D
(f
)
 W
D
(B
)
=
W
D
(A

)
+
W
D
(A

)
+
W
D
(A

)
+
W
D
(A

)
=




=

If
h
yp
othesis
rectangle
h
con
tains
B
(i.e.,
h

B
)
then
(using
h

f
)
er
r
f
;D
(h)
=
Pr
sD
[s

f
and
s

h]

Pr
sD
[s

f
and
s

B
]
=
W
D
(f
)
 W
D
(B
)
=

F
urthermore,
according
to
algorithm,
if
there
is
an
example
in
eac
h
of
the
rectangle
A

,
A

,
A

and
A

,
then
B

h.
Th
us,
w
e
merely
b
ound
the
n
um
b
er
of
examples,
denoted
m,
suc
h
that
the
probabilit
y
for
this
ev
en
t
not
to
o
ccur,
is
less
then

.
F
or
an
y
i

f;
;
;
g,
the
probabilit
y
that
no
example
resides
in
A
i
is
(
 W
D
(A
i
))
m
=
(
 

)
m
.
Th
us,
Pr
[er
r
f
;D
(h)
>
]

Pr
[	i
no
sample
in
A
i
]





 


m


LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
A1
A2
A3
A4
1
1
0
Target rectangle
Figure
.:
T
arget
rectangle
in
b
old,
and
rectangle
A

;
A

;
A

;
A

eac
h
of
w
eigh
t


So
w
e
need
to
set
m
so
that


(
 (=))
m


.
Using
the
inequalit
y

 


e
 

,
the
condition
simplies
to
(e
 

)
m


,
whic
h
solv
es
to
m

(=)

ln(=
).
W
e
no
w
turn
to
the
second
case,
where
W
D
(f
)

:
Using
h

f
,
w
e
ha
v
e
(for
ev
ery
sequence
of
examples
used
b
y
the
algorithm):
er
r
f
;D
(h)
=
Pr
sD
[s

f
and
s

h]

Pr
sD
[s

f
]
=
W
D
(f
)


Th
us,
in
this
case,
Pr
[er
r
f
;D
(h)

]
=
.
Note
all
op
erations
in
algorithm
dep
end
linearly
on
the
size
of
the
sample,
whic
h
in
turn
dep
ends
linearly
on


and
l
n


.
Therefore
the
algorithm
is
an
ecien
t
P
AC
learning
algorithm.
.
Occam's
Razor
Occam's
Razor
is
based
on
a
principle
stated
b
y
William
of
Occam.
W
e
in
terpret
Occam's
principle
as
follo
ws:
learning
can
b
e
ac
hiev
ed
b
y
nding
a
succinct
represen
tation
for
the
lab
els
of
large
n
um
b
er
of
examples.
By
a
succinct
represen
tation,
w
e
mean
that
the
size
of
the
represen
tation
is
sublinear
in
the
n
um
b
er
of
examples.
This
reduces
the
problem
of
learning
to
the
problem
of
nding
a
h
yp
othesis,
consisten
t
with
ev
ery
giv
en
example.
W
e
sa
y
an
algorithm
is
an
Oc
c
am's
A
lgorithm,
if
it
outputs
a
succinct
h
yp
othesis
consisten
t
with
ev
ery
giv
en
example.
Theorem
.
(Occam's
Razor
{
basic
v
ersion):
L
et
F
n
b
e
a
nite
c
onc
ept
class.
L
et
A
b
e
an
algorithm
such
that
for
every
n,
for
every
f

F
n
and
for
every
numb
er
of
examples
lab
ele
d
by
f
,
the
algorithm
outputs
an
hyp
othesis
h

F
n
,
that
is
c
onsistent
with
al
l
the
given
examples.
Then
for
any
distribution
D
,
for
any
err
or
p
ar
ameter
0
<

<

and
for
any
c
ondenc
e
p
ar
ameter
0
<

<

if
the
numb
er
of
examples
dr
awn
indep
endently
by
D
is
lar
ger
or
e
qual


(log


+
log
jF
n
j),
then
Pr
[er
r
f
;D
(h)

]


 

..
OCCAM'S
RAZOR

wher
e
h
is
the
hyp
othesis
output
by
the
algorithm,
and
the
pr
ob
ability
is
taken
over
the
choic
e
of
examples
as
wel
l
as
over
the
internal
c
oin
tosses
of
the
algorithm.
Pro
of:
W
e
use
the
follo
wing
notations:

S
=<
a
j
;
f
(a
j
)
>
j
=:::m
is
the
set
of
lab
eled
examples.

C
F
(S
)
is
the
set
of
h
yp
otheses
in
F
that
are
consisten
t
on
all
examples
in
S
.
That
is,
C
F
(S
)
def
=
fh

F
:
h(a
i
)
=
f
(a
i
)for
i
=
;
:::;
m
g

B
D
;
(f
)
is
the
set
of
h
yp
otheses
in
F
that
ha
v
e
a
probabilit
y
of
error
larger
than
.
That
is,
B
D
;
(f
)
def
=
fh

F
:
er
r
f
;D
(h)
>
g
Note
that
the
algorithm
alw
a
ys
outputs
an
h
yp
othesis
h

C
F
(S
).
W
e
will
upp
er
b
ound
the
probabilit
y
that
this
h
yp
othesis
is
in
B
D
;
(f
).
Actually
,
w
e
will
upp
er
b
ound
the
probabilit
y
that
an
y
h
yp
othesis
in
C
F
(S
)
remains
in
B
D
;
(f
).
Pr
S
[there
exists
h

B
D
;
(f
)
suc
h
that
h

C
F
(S
)]

X
hB
D
;
(f
)
Pr
S
[h

C
F
(S
)
]
(.)
Prop
osition
..
F
or
any
h

B
D
;
(f
),
Pr
S
[h

C
F
(S
)]


jF
j
Pro
of:
F
or
an
y
h
yp
othesis
in
B
D
;
the
probabilit
y
that
the
h
yp
othesis
will
b
e
in
C
F
(S
)
is
the
probabilit
y
that
all
giv
en
example
p
oin
ts
landed
in
the
agreemen
t
area.
By
using
denition
of
B
D
;
(f
)
the
probabilit
y
that
a
single
example
p
oin
t
landed
in
the
agreemen
t
area
is
at
most

 .
Therefore
for
an
y
xed
h

B
D
;
(f
):
Pr
S
[h

C
F
(S
)]

(
 )
jS
j

e
 jS
j
Using
the
assumption
on
jS
j:
jS
j

log
(


jF
j)
Th
us,
e
 jS
j


jF
j
and
the
prop
osition
follo
ws.
Com
bining
Eq.
(.)
and
Prop
osition
..,
the
theorem
follo
ws.
It
is
easy
to
observ
e,
that
an
algorithm
satisfying
this
theorem,
is
a
P
AC
learning
algorithm.
Also
note
that
since
the
h
yp
othesis
is
tak
en
from
the
concept
class
it
is
necessarily
succinct.
Learning
monomials.
The
concept
class
F
n
of
monomials
is
a
family
of
b
o
olean
expressions,
o
v
er
literals
corresp
onding
to
n
v
ariables
x

;
x

;
:::;
x
n
,
dened
as
follo
ws:
F
n
=
ff
:
f
=
l

^
l

^
:::
^
l
t
g
Where
for
eac
h


i

t


LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
l
i

fx
j
;
x
j
g
n
j
=
W
e
seek
an
ecien
t
P
AC
learning
algorithm
for
the
concept
class
of
monomials.
A
instance
a

X
n
is
in
terpreted
as
an
assignmen
t
to
the
v
ariables
x

;
x

;
:::;
x
n
.
W
e
us
the
notation
a
i
for
the
v
alue
of
the
i'th
bit
of
a.
W
e
denote
the
set
of
giv
en
examples
b
y:
S
=
f<
a
j
;
f
(a
j
)
>g
j
=;:::;m
where
eac
h
a
j
=
a
j

;
:::;
a
j
n
.
W
e
call
an
example
ne
gative
if
it
is
lab
eled
b
y
0,
otherwise
w
e
call
it
p
ositive.
Before
stating
the
algorithm,
let
us
see
what
information
can
w
e
conclude
ab
out
the
target
concept,
from
a
giv
en
example.
A
p
ositiv
e
example
consists
of
an
assignmen
t
to
the
target
concept,
suc
h
that
ev
ery
the
literal
in
it
ev
aluates
to
.
A
negativ
e
example
consists
of
an
assignmen
t
to
the
target
concept,
suc
h
that
there
exists
a
literal
in
it
that
ev
aluates
to
0.
Th
us
negativ
e
examples
con
v
ey
m
uc
h
less
information
then
p
ositiv
e
examples,
and
the
information
they
con
v
ey
is
not
trivial
to
use.
Our
algorithm
uses
only
p
ositiv
e
examples.
Claim
..
Given
at
le
ast


(n
+
log


)
examples,
the
fol
lowing
algorithm
is
a
P
AC
le
arning
algorithm.
.
initialize
h
=
x

^
x

^
x

^
x

^
:::
^
x
n
^
x
n
.
for
j
=
:::m,
if
f
(a
j
)
=
`+
0
then
do

for
e
ach
i

f:::ng
if
a
j
i
=

r
emove
x
i
fr
om
h

for
e
ach
i

f:::ng
if
a
j
i
=
0
r
emove
x
i
fr
om
h
Pro
of:
W
e
use
the
follo
wing
notations:
By
S
=
f<
a
j
;
f
(a
j
)
>g
j
=;:::;m
w
e
denote
the
set
of
all
samples.
By
h
j
w
e
denote
the
expression
h
after
j
iterations;
that
is,
after
pro
cessing
the
examples
a

;
:::;
a
j
.
W
e
rst
sho
w
that
the
nal
h
yp
othesis
h
is
consisten
t
with
all
examples.
Using
induction
on
j
,
w
e
sho
w
that
h
j
is
consisten
t
with
the
rst
j
examples
and
that
h
j
includes
all
literals
in
f
(i.e.,
h
j

f
).
The
induction
basis
(j
=
0)
holds
trivially
(as
h
0
is
as
initialized
in
Step
).
In
the
induction
step,
supp
ose
that
h
j

f
is
consisten
t
with
the
rst
j
examples,
and
consider
what
happ
ens
in
the
j
+

st
iteration.
W
e
consider
t
w
o
cases
Case
:
the
j
+

st
example
is
p
ositiv
e.
In
this
case
h
j
+

h
j
and
h
j
+

f
(since
the
only
literals
omitted
from
h
j
are
those
that
cannot
b
e
in
f
).

Using
h
j
+

h
j
(i.e.,
h
j
(a)
=

implies
h
j
+
(a)
=
),
w
e
ha
v
e
h
j
+
(a
i
)
=
h
j
(a
i
)
=
f
(a
i
)
for
ev
ery
i
=
;
:::;
j
satisfying
f
(a
i
)
=
.

Using
h
j
+

f
(i.e.,
f
(a)
=
0
implies
h
j
+
(a)
=
0),
w
e
ha
v
e
h
j
+
(a
i
)
=
f
(a
i
)
for
ev
ery
i
=
;
:::;
j
satisfying
f
(a
i
)
=
0.

Finally
,
b
y
the
op
eration
of
the
curren
t
in
teraction,
h
j
+

^
n
i=
l
i
,
where
l
i
=
x
i
if
a
j
+
i
=
,
and
l
i
=
x
i
otherwise.
Th
us,
h
j
+
(a
j
+
)
=
.
It
follo
ws
that
h
j
+
(a
i
)
=
f
(a
i
)
holds
for
i
=
;
:::;
j;
j
+
.
Case
:
the
j
+

st
example
is
negativ
e.
In
this
case
h
j
+
=
h
j
and
so
h
j
+

f
.
Also,
since
f
(a
j
+
)
=
0,
it
follo
ws
that
h
j
+
(a
j
+
)
=
0.
Th
us,
h
j
+
(a
i
)
=
f
(a
i
)
holds
for
i
=
;
:::;
j;
j
+
.

..
OCCAM'S
RAZOR

Let
us
compute
the
cardinalit
y
of
F
n
.
The
cardinalit
y
of
F
n
is
b
ounded
b
y

n
,
since
eac
h
literal
can
either
not
app
ear
in
the
monomial
or
app
ear
in
monomial,
and
the
n
um
b
er
of
literals
is
n.
Th
us,
log

jF
n
j

n,
and
the
claim
follo
ws
b
y
applying
Occam's
Razor.
Is
this
v
ersion
of
Occam's
Razor
p
o
w
erful
enough?
The
follo
wing
example
sho
ws
that
this
curren
t
v
ersion
is
somewhat
limited.
Learning
a
-term
D
N
F
.
The
concept
class
F
n
of
-term
D
N
F
's
is
a
family
of
b
o
olean
ex-
pressions,
o
v
er
v
ariables
x

;
x

;
:::;
x
n
,
dened
as
follo
ws:
F
n
=
ff
:
f
=
M

^
M

^
M

where
M
;
M
;
M

are
monomials
o
v
er
x

;
x

;
:::;
x
n
g
The
learning
task
seems
similar
to
the
previous
task
of
learning
monomials.
Ho
w
ev
er,
the
problem
of
nding
a
consisten
t
-term
DNF
seems
in
tractable.
Sp
ecically:
Claim
..
The
pr
oblem
of
nding
a
-term
D
N
F
that
is
c
onsistent
with
a
given
set
of
examples
is
N
P
-c
omplete.
The
pro
of
is
via
a
reduction
to
-colorabilit
y
and
is
omitted.
Actually
,
the
dicult
y
is
not
due
to
Occam
algorithms
only
.
It
rather
holds
with
resp
ect
to
an
y
P
A
C
learning
algorithm
that
alw
a
ys
outputs
h
yp
otheses
in
the
concept
class
(in
our
case
a
-term
D
N
F
).
Recall
that
in
our
denition
of
P
A
C
algorithms
w
e
did
not
insist
that
when
learning
a
target
concept
from
F
n
the
algorithm
m
ust
output
a
h
yp
othesis
in
F
n
.
Ho
w
ev
er,
w
e
did
mak
e
this
condition
when
dening
an
Occam
algorithm.
Claim
..
If
N
P
is
not
c
ontaine
d
in
B
P
P
then
no
pr
ob
abilistic
p
olynomial-time
that
outputs
a
hyp
othesis
in
-term
DNF
c
an
le
arn
the
class
of
-term
D
N
F
formulae.
Pro
of:
W
e
sho
w
a
randomized
p
olynomial-time
reduction
of
the
problem
of
nding
a
-term
D
N
F
that
is
consisten
t
with
a
giv
en
set
of
examples
to
the
problem
of
P
A
C
learning
-term
D
N
F
's
via
suc
h
h
yp
otheses.
Let
L
b
e
a
P
A
C
learning
algorithm
of
the
latter
form,
and
supp
ose
w
e
are
giv
en
a
set
of
m
instances,
denoted
S
,
lab
eled
b
y
some
-term
D
N
F
,
denoted
f
.
W
e
in
v
ok
e
algorithm
L
on
input
parameters

=
=m
and

=
=,
and
feed
it
with
a
sequence
of
(lab
eled)
examples
uniformly
distributed
in
S
.
(This
sequence
ma
y
con
tain
rep
etirtions.)
Th
us,
L
is
running
with
distribution
D
whic
h
is
uniform
on
S
,
and
outputs
a
-term
D
N
F
h
yp
othesis
h
satisfying
Pr

er
r
f
;D
(h)


m




Since
eac
h
s

S
has
probabilit
y
mass
=m,
it
follo
ws
that
Pr[	s

S
s.t.
h(s)
=
f
(s)]



Th
us,
with
probabilit
y
=
the
h
yp
othesis
h
is
consisten
t
with
f
on
S
.
In
v
oking
Claim
..,
the
curren
t
claim
follo
ws.
Discussion:
What
the
last
claim
sa
ys
is
that
if
w
e
insist
that
the
learning
algorithm
outputs
a
h
yp
othesis
in
the
concept
class
b
eing
learned
(as
w
e
do
in
case
of
Occam's
Razor)
then
w
e
cannot
learn
-term
DNF
form
ulae.
In
the
next
section,
w
e
shall
see
that
the
latter
class
can
b
e
learned
if
w
e
allo
w
the
h
yp
othesis
class
to
b
e
dieren
t.


LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
.
Generalized
denition
of
P
AC
learning
algorithm
Oded's
Note:
This
section
w
as
drastically
revised
b
y
me.
In
accordance
with
the
ab
o
v
e
discussion
w
e
dene
explicitly
the
notion
of
learning
one
concept
class
with
a
p
ossibly
dieren
t
class
of
h
yp
otheses,
whic
h
t
ypically
is
a
sup
erset
of
the
concept
class.
Denition
.
(P
AC
learning,
revisited):
L
et
F
=
[
n
F
n
and
H
=
[
n
H
n
so
that
F
n

H
n
ar
e
classes
of
functions
mapping
X
n
to
f0;
g.
We
say
that
algorithm
A
P
AC
lea
rns
the
concept
class
F
using
the
hyp
othesis
class
H
if
the
fol
lowing
holds:
F
or
every
n,
for
every
function
f

F
n
,
for
every
distribution
D
:
X
n
 !
[0;
],
for
every
err
or
p
ar
ameter
0
<

<

and
for
every
c
ondenc
e
p
ar
ameter
0
<

<
,
given
p
ar
ameters
n;
;

and
a
set
of
f
-lab
ele
d
examples,
f<
x
i
;
f
(x
i
)
>g,
wher
e
x
i
is
dr
awn
indep
endently
under
the
distribution
D
,
the
algorithm
outputs
a
hyp
othesis
h

H
n
such
that:
Pr
[er
r
f
;D
(h)

]


 
wher
e
the
pr
ob
ability
is
taken
over
the
choic
e
of
examples,
as
wel
l
as
over
the
internal
c
oin
tosses
of
the
algorithm.
That
is,
the
only
c
hange
relativ
e
to
Denition
.
is
the
condition
that
the
output
h
yp
othesis
h
b
elongs
to
H
n
.
In
case
H
=
F
w
e
sa
y
that
the
algorithm
is
a
p
rop
er
lea
rning
algorithm
for
F
.
In
con
trast
to
the
negativ
e
results
in
the
previous
section,
w
e
sho
w
that
the
class
-term
DNF
can
b
e
ecien
tly
learned
using
the
h
yp
othesis
class
CNF.
This
statemen
t
is
pro
v
en
via
a
reduction
of
this
learning
task
to
the
task
of
learning
monomials
(already
solv
ed
ecien
tly
ab
o
v
e).
T
o
presen
t
this
reduction,
w
e
rst
dene
what
w
e
mean
in
general
b
y
reduction
among
learning
tasks.
..
Reductions
among
learning
tasks
Reductions
are
a
p
o
w
erful
to
ol
common
in
computer
science
mo
dels.
It
is
only
natural
to
dene
suc
h
notion
for
the
mo
del
of
P
AC
learning.
Denition
.
We
say
that
the
c
onc
ept
class
F
over
the
instanc
e
sp
ac
e
X
,
is
P
AC
-r
e
ducible
to
the
c
onc
ept
class
F
0
over
the
instanc
e
X
0
if
for
some
p
olynomial
p

ther
e
exists
a
p
olynomial-time
c
omputable
mapping
G
fr
om
X
to
X
0
such
that
for
every
n
and
for
every
x

X
n
,
G(x)

X
p(n)
.

ther
e
exists
a
p
olynomial
q
()
such
that
for
every
c
onc
ept
f

F
n
,
ther
e
exists
a
c
onc
ept
f
0

F
0
p(n)
such
that
siz
e(c
0
)

q
(siz
e(c))
and
for
every
x

X
n
,
f
(x)
=
f
0
(G(x)).
Note
that
the
second
item
do
es
not
require
an
ecien
t
transformation
b
et
w
een
f
and
f
0
.
In
fact,
for
prop
er
learning
(b
y
ecien
t
algorithms),
an
ecien
t
transformation
from
F
0
to
F
m
ust
b
e
required.
Theorem
.
L
et
F
and
F
0
b
e
c
onc
ept
classes.
If
F
is
P
AC
-r
e
ducible
to
F
0
,
and
F
0
is
eciently
P
AC
le
arnable,
then
F
is
eciently
P
AC
le
arnable.
Pro
of:
Giv
en
an
ecien
t
P
AC
learning
algorithm
L
0
.
W
e
use
L
0
to
learn
a
an
unkno
wn
target
concept
f

F
n
as
follo
ws:
Giv
en
an
example
lab
eled
<
x;
f
(x)
>,
where
x

X
n
,
w
e
compute
an
example
lab
eled
<
G(x);
f
(x)
>,
where
G(x)

X
0
n
,
and
supply
it
to
L
0
.
The
original
examples
are
c
hosen
b
y
a
distribution
D
on
X
,
and
so
the
reduced
examples
G(x)
(computed
b
y
us)
are

..
GENERALIZED
DEFINITION
OF
P
AC
LEARNING
ALGORITHM

dra
wn
b
y
distribution
D
0
on
X
0
induced
b
y
ditribution
D
.
Also,
let
f
0
b
e
the
function
asso
ciated
b
y
Item

to
f
(i.e.,
f
0
(G(x))
=
f
(x)).
Algorithm
L
0
will
output
an
h
yp
othesis
h
0
suc
h
that:
Pr
[er
r
f
0
;D
0
(h
0
)

]


 
where
the
probabilit
y
dep
ends
on
a
sample
of
D
0
.
W
e
tak
e
the
comp
osition
of
h
0
and
G
to
b
e
our
h
yp
othesis
h
for
f
(i.e.,
h(x)
=
h
0
(G(x))).
W
e
need
to
ev
aluate
Pr
[er
r
f
;D
(h)

],
where
h
=
h
0

G
and
h
0
is
the
output
of
L
0
.
W
e
rst
observ
e
that
er
r
f
;D
(h)
=
Pr
xD
[h(x)
=
f
(x)]
=
Pr
xD
[h
0
(G(x))
=
f
0
(G(x))]
=
Pr
x
0
D
0
[h
0
(x
0
)
=
f
0
(x
0
)]
=
er
r
f
0
;D
0
(h
0
)
and
so
Pr[er
r
f
;D
(h)

]


 
,
as
required.
Learning
a
-term
D
N
F
,
revisited.
Recall
that,
assumping
N
P
is
not
con
tained
in
B
P
P
,
it
is
infeasible
to
pr
op
erly
learn
-term
DNF's
(i.e.,
learn
this
class
b
y
h
yp
otheses
in
it).
In
con
trast,
w
e
no
w
sho
w
that
it
is
feasible
to
learn
-term
DNF's
b
y
CNF
h
yp
otheses.
Actually
,
w
e
sho
w
that
it
is
feasible
to
(prop
erly)
learn
the
class
of
CNF
(whic
h
con
tain
via
a
easy
reduction
all
-term
DNF's).

This
is
sho
wn
b
y
reducting
the
learning
of
-CNF's
to
the
learning
of
monomials.
Claim
..
L
e
arning
-CNF's
is
r
e
ducible
to
le
arning
monomials.
F
urthermor
e,
ther
e
exists
an
ecient
algorithm
for
prop
erly
le
arning
-CNF.
Pro
of:
W
e
dene
the
follo
wing
transformation
G
from
-CNF
instance
space
X
n
,
to
the
monomial
instance
space
X
0
k
,
where
k
=


 n



.
F
or
eac
h
of
the
p
ossible
k
clauses,
w
e
asso
ciate
a
distinct
v
ariable,
and
the
transformation
from
CNF
to
monomials
just
maps
the
set
of
clauses
in
the
CNF
in
to
the
set
of
v
ariables.
(Indeed,
w
e
reduce
to
learning
monotone
monomials.)
The
transformation
G
do
es
the
analogous
thing;
that
is,
it
maps
truth
assignmen
ts
to
the
n
NCF
v
ariables
on
to
truth
assignmen
ts
to
the
k
monomial
v
araibles
in
the
natural
w
a
y
(i.e.,
a
monomial
v
ariable
represen
ting
a
p
ossible
clause
is
assigned
the
v
alue
to
whic
h
this
clause
ev
aluates
under
the
assignmen
t
to
the
CNF
v
ariables).
This
transformation
satises
the
conditions
of
Denition
..
T
o
sho
w
the
furthermore-part,
observ
e
that
the
h
yp
othesis
constructed
b
y
the
reduction-
algorithm
(giv
en
in
the
pro
of
of
Theorem
.)
can
b
e
readily
put
in
CNF
with
resp
ect
to
the
space
X
n
.
..
Generalized
forms
of
Occam's
Razor
W
e
ha
v
e
seen
that
it
is
w
orth
while
to
use
a
h
yp
othesis
from
a
wider
class
of
functions
(than
merely
from
the
concept
class).
A
staigh
tfo
w
ard
generalization
of
Occam's
Razor
to
a
case
where
the
algorithm
outputs
a
h
yp
othesis
from
a
predetermined
h
yp
othesis
class
follo
ws.
Theorem
.
(Occam's
Razor
{
generalization
to
predetermined
h
yp
othesis
class):
L
et
F
n

H
n
b
e
nite
c
onc
ept
classes.
L
et
A
b
e
an
algorithm
such
that
for
every
n,
for
every
f

F
n
and
for
every

Applying
the
distributional
la
w
to
a
-term
DNF
o
v
er
n
v
ariables,
w
e
obtain
a
CNF
with
at
most
(n)

clauses.


LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
numb
er
of
examples
lab
ele
d
by
f
,
the
algorithm
outputs
an
hyp
othesis
h

H
n
,
that
is
c
onsistent
with
al
l
the
given
examples.
Then
for
any
distribution
D
,
for
any
err
or
p
ar
ameter
0
<

<

and
for
any
c
ondenc
e
p
ar
ameter
0
<

<

if
the
numb
er
of
examples
dr
awn
indep
endently
by
D
is
lar
ger
or
e
qual


(log


+
log
jH
n
j),
then
Pr
[er
r
f
;D
(h)

]


 
The
pro
of
is
b
y
a
straigh
tforw
ard
adaptation
of
the
pro
of
giv
en
to
Theorem
.
(whic
h
is
indeed
a
sp
ecial
case
obtained
b
y
setting
H
n
=
F
n
).
A
wider
generalization
is
obtained
b
y
not
determining
a
priori
the
h
yp
othesis
class.
In
suc
h
case,
w
e
need
some
other
w
a
y
to
b
ound
the
h
yp
othesis
from
merely
recording
all
examples.
This
is
done
b
y
requiring
that
the
h
yp
othesis's
length
is
strictly
shorter
than
the
n
um
b
er
of
examples
seen.
This
leads
to
the
follo
wing
form
ulation.
Denition
.	
L
et

and

<

b
e
c
onstants.
We
say
that
L
is
an
(;

)-Occam's
algo
rithm
for
F
if
given
m
examples,
L
outputs
a
hyp
othesis
h
such
that
h
is
c
onsistent
with
every
given
example
and
siz
e(h)

(n

siz
e(f
))

m

Theorem
.0
F
or
every

and

<
,
an
(;

)-Oc
c
am's
algorithm
c
an
b
e
turne
d
into
a
P
A
C
le
arning
algorithm
by
running
it
on
O
(
 

(p
oly
(n

siz
e(f
))
+
log
(=
))
examples.
Pro
of:
Firstly
,
w
e
generalize
Theorem
.
to
a
setting
in
whic
h
the
Occam
algorithm
ma
y
uses
dieren
t
h
yp
othesis
classes
for
dieren
t
n
um
b
er
of
examples.
Sp
ecically
,
supp
ose
that
when
seeing
m
examples
lab
elled
b
y
f

F
n
,
the
algorithm
outputs
a
h
yp
othesis
in
H
n;m
.
Then
suc
h
an
algorithm
is
a
P
A
C
learner
pro
vided
m


 

(log
(=
)
+
log
jH
n;m
j)
;
where

and

are
the
error
and
condence
parameters
(giv
en
to
the
P
A
C
v
ersion).
No
w,
the
conditions
of
the
curren
t
theorem
pro
vide
suc
h
a
\dynamic"
h
yp
othesis
class,
H
n;m
,
and
the
upp
er
b
ound
on
the
length
of
h
yp
othesis
guaran
tees
that
log
jH
n;m
j

(n

siz
e(f
))

m

So,
since

<
,
for
sucien
tly
large
m
=
p
oly(n

siz
e(f
))=,
w
e
ha
v
e
m

(n

siz
e(f
))

m

.
The
theorem
follo
ws.
.
The
(V
C)
V
apnik-Cherv
onenkis
Dimension
In
all
v
ersions
of
Occam's
Razor
discussed
ab
o
v
e,
w
e
assumed
that
the
h
yp
othesis
class
is
nite.
In
general
this
is
not
necessarily
the
case.
F
or
example,
the
natural
h
yp
othesis
class
for
learning
axis-aligned
rectangles
o
v
er
[0;
]

is
the
innite
set
of
all
p
ossible
axis-aligned
rectangles.
Therefore
w
e
can
not
apply
Occam's
Razor
to
this
problem.
W
e
w
ould
lik
e
a
to
ol
with
similar
a
v
or
for
the
case
of
innite
h
yp
othesis
classes.
The
rst
step
for
ac
hieving
this
goal,
is
nding
a
parameter
of
nite
v
alue
that
c
haracterizing
also
innite
classes.

..
THE
(V
C)
V
APNIK-CHER
V
ONENKIS
DIMENSION
	
Denition
.
(shattering
a
set):
A
nite
subset
S
of
instanc
e
sp
ac
e
X
n
is
shattered
b
y
a
family
of
functions
F
n
if
for
every
S
0

S
ther
e
exists
a
function
f

F
n
such
that
f
(x)
=
(

x

S
0
0
x

S
nS
0
That
is
for
S
=
fx

;
:::;
x
m
g
and
for
ev
ery


f0;
g
m
there
exists
a
function
f

F
,
suc
h
that
for
an
y
i
=
f:::mg,
f
(x
i
)
=

i
,
where

i
is
the
i
th
bit
of
.
Denition
.
(V
C
dimen
tion):
The
V
C
dimention
of
a
set
of
functions
F
,
denote
d
V
C-dim
(F
),
is
the
maximal
inte
ger
d
such
that
ther
e
exists
a
set
S
of
c
ar
dinality
d
that
is
shatter
e
d
by
F
.
..
An
example:
V
C
dimension
of
axis
aligned
rectangles
The
follo
wing
example
demonstrates
the
computation
of
V
C
dimension
of
a
family
of
functions.
Prop
osition
..
V
C-dim(axis-aligne
d
r
e
ctangles)
=

Pro
of:
W
e
start
b
y
exhibiting
a
set
of
four
p
oin
ts
in
[0;
]

that
can
b
e
shattered.
F
or
ev
ery
lab
eling
of
the
set
of
four
p
oin
ts
exhibited
in
Fig.
.,
w
e
can
nd
a
rectangle
that
induces
suc
h
lab
eling.
Figure
.:
F
our
p
oin
t
that
in
[0;
]

,
that
can
b
e
shattered
.
In
case
all
p
oin
ts
are
lab
eled
\+",
w
e
tak
e
the
rectangle
[0;
]

itself.
.
In
case
all
p
oin
ts
are
lab
eled
\-",
w
e
tak
e
the
empt
y
rectangle.
.
F
or
the
case
three
p
oin
ts
are
lab
eled
\-"
and
one
p
oin
t
\+",
tak
e
a
\small"
rectangle
that
co
v
ers
only
the
p
oin
t
lab
eled
\+".
.
F
or
the
case
three
p
oin
ts
are
lab
eled
\+"
and
one
p
oin
t\-",
w
e
tak
e
rectangles
as
can
b
e
seen
in
Fig.
..
.
F
or
the
case
t
w
o
p
oin
ts
are
lab
eled
\-"
and
t
w
o
p
oin
ts
\+",
w
e
tak
e
rectangles
as
can
b
e
seen
in
Fig.
..
(The
gure
sho
ws
only

out
of
the

sub
cases;
the
other
t
w
o
are
easier.)

0
LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
+
+
+
+
+
-
+
+
-
+
+
+
+
-
_
+
Figure
.:
rectangle
co
v
ering
the
three
p
oin
ts
lab
eled
\+"
+
-
-
+
+
+
-
-
+
+
-
-
+
+
-
-
Figure
.:
rectangle
co
v
ering
the
t
w
o
p
oin
ts
lab
eled
\+"
It
remains
to
pro
v
e
that
for
ev
ery
set
of
v
e
p
oin
ts
in
[0;
]

,
there
exists
is
a
lab
eling,
suc
h
that
it
can
not
b
e
induced
b
y
an
y
rectangle
in
[0;
]

.
By
the
pro
of
of
Claim
..
(the
learning
algorithm
for
axis
aligned
rectangles),
for
ev
ery
set
of
v
e
p
oin
ts
in
[0;
]

,
at
most
four
of
them
determine
the
minimal
rectangle
that
con
tains
the
whole
set.
Then
no
rectangle
is
consisten
t
with
the
lab
eling
that
assigns
these

b
oundary
p
oin
ts
`+'
and
assigns
the
remaining
p
oin
t
(whic
h
m
ust
reside
inside
an
y
rectangle
co
ving
these
p
oin
ts)
a
`-'.
..
General
b
ounds
V
C
dimension
is
link
ed
to
the
sample
complexit
y
of
P
AC
learning
via
the
follo
wing
t
w
o
theorems:
Theorem
.
(upp
er
b
ound
on
sample
complexit
y):
L
et
H
,
F
b
e
function
classes
such
that
F

H
.
L
et
A
b
e
a
algorithm
that
given
a
lab
ele
d
sample
always
outputs
a
hyp
othesis
h

H
c
onsistent
with
the
sample.
Then
ther
e
exists
a
c
onstant
c
0
,
such
that
for
any
tar
get
function
f

F
,
for
any
underlying
distribution
D
,
any
err
or
p
ar
ameter
0
<

<

and
any
c
ondenc
e
p
ar
ameter
0
<

<
,
if
the
numb
er
of
examples
is
gr
e
ater
or
e
qual:
c
0



V
C-dim(H
)

log


+
log



then
with
pr
ob
ability
gr
e
ater
then

 
we
get
er
r
D
;f
(h)


wher
e
h
is
the
hyp
othesis
output
by
A.

..
THE
(V
C)
V
APNIK-CHER
V
ONENKIS
DIMENSION

Theorem
.
(lo
w
er
b
ound
on
sample
complexit
y):
A
ny
P
AC
le
arning
algorithm
for
le
arning
a
c
onc
ept
class
F
such
that
d
=
V
C-dim(F
)
r
e
quir
es
m
=

(



(log


+
d))
examples.
W
e
note
that
there
is
a
gap
of
factor
log


b
et
w
een
the
b
ounds.
The
pro
of
of
the
these
theorems
is
complex.
Instead
w
e
pro
v
e
a
sligh
tly
w
eak
er
lo
w
er
b
ound
theorem:
Theorem
.
(lo
w
er
b
ound,
sligh
tly
w
eak
er
form):
A
ny
P
AC
le
arning
algorithm
for
le
arning
a
c
onc
ept
class
F
such
that
d
=
V
C-dim(F
)
r
e
quir
es
r
e
quir
es

(
d

)
examples
in
the
worst
c
ase.
Oded's
Note:
The
pro
of
w
as
revised
b
y
me.
Pro
of:
Since
the
V
C
dimension
is
d,
there
exists
a
set
of
d
p
oin
ts
shattered
b
y
F
.
W
e
denote
this
set
b
y
S
=
fe

;
:::;
e
d
g.
Since
S
is
shattered
b
y
F
,
for
eac
h
p
ossible
lab
eling
of
S
there
exists
a
function
f

F
whic
h
is
consisten
t
with
this
lab
eling.
Let
us
denote
b
y
f


F
a
function
consisten
t
with
the
lab
eling

=






d

f0;
g
d
;
i.e.,
f

(e
i
)
=

i
for
eac
h
i
=
;
:::;
d.
Let
F
0
=
ff

:


f0;
g
d
g

F
.
W
e
consider
error
parameter




,
and
an
arbitrary
condence
parameter
0
<

<


.
W
e
start
b
y
pro
ving
a
b
ound
of

(d).
T
o
w
ards
this
goal,
w
e
dene
the
underlying
distribution
D
to
b
e
uniform
o
v
er
S
;
that
is,
ev
ery
p
oin
t
in
S
is
assigned
probabilit
y

d
,
and
all
other
instances
are
assigned
zero
probabilit
y
.
Let
us
assume,
in
con
trary
to
the
claimed
b
ound,
that
d

examples
suce.
Under
this
assumption
in
the
b
est
case,
d

dieren
t
examples
where
dra
wn.
W
e
denote
this
set
b
y
S

,
and
let
S

def
=
S
n
S

.
In
tutiv
ely
,
the
algforithm
only
got
the
lab
eling
of
S

and
so
there
is
no
w
a
y
it
can
distinguish
b
et
w
een
the

d jS

j
functions
in
F
0
consisten
t
with
S

.
F
ormally
,
w
e
consider
a
target
function
f
c
hosen
uniformly
in
F
0
.
The
algorithm
then
obtain
a
sample
S

lab
eled
b
y
f
,
and
outputs
a
h
yp
othesis
h.
W
e
are
in
terested
in
the
b
eha
vior
of
the
random
v
ariable
er
r
f
;D
(h).
Recall
that
this
random
v
araible
is
dened
o
v
er
the
probabilit
y
space
dened
b
y
the
uniform
c
hoice
of
f

F
0
,
the
uniform
c
hoice
of
d=
p
oin
ts
in
S
(yielding
S

),
and
additional
coin
tosses
the
learning
algorithm
ma
y
do.
Let
us
rev
erse
the
\natural"
order
of
the
randomization,
and
consider
what
happ
ens
when
S

is
selected
rst,
and
f

F
0
is
selected
next.
F
urthermore,
w
e
select
f
in
t
w
o
phases:
rst
w
e
select
at
random
the
v
alue
of
f
on
S

,
and
w
e
p
ostp
one
for
later
the
random
c
hoice
of
the
v
alue
of
f
on
S

.
(T
ogether,
the
v
alues
of
f
on
S

and
S

will
determine
a
unique
f

F
0
,
whic
h
will
b
e
uniformly
distributed.)
Ho
w
ev
er,
the
learning
algorithm
is
oblivious
of
the
latter
c
hoices
(i.e.,
of
the
v
alues
of
f
on
S

),
and
so
the
h
yp
othesis
h
is
sto
c
hastically
indep
enden
t
of
the
latter.
So
the
full
order
of
ev
en
ts
w
e
consider
is:
.
d
p
oin
ts
are
selected
uniformly
in
S
,
determining
S

and
S

.
.
one
assigns
uniformly
Bo
olean
v
alues
to
the
p
oin
ts
in
S

and
presen
ts
these
to
the
learning
algorithm.
.
the
learning
algorithm
outputs
a
h
yp
othesis
h.
.
one
assigns
uniformly
Bo
olean
v
alues
to
the
p
oin
ts
in
S

,
th
us
determining
a
unique
function
f

F
0
.
Since
f
is
determined
on
S

only
after
h
is
xed,
and
its
v
alues
on
S

are
uniformly
distributed
in
f0;
g,
the
exp
ected
v
alue
of
er
r
f
;D
(h)
is
at
least



jS

j
jS
j




d=
d
=




LECTURE
.
COMPUT
A
TIONAL
LEARNING
THEOR
Y
Th
us,
there
exists
f

F
0

F
so
that
with
probabilit
y
at
least
=,
er
r
f
;D
(h)



This
establishes
a
lo
w
er
b
ound
of
d=
on
the
sample
complexit
y
,
for
an
y


=
(and


=).
In
order
to
pro
v
e
the
stronger
b
ound,
stated
in
the
theorem,
w
e
mo
dify
the
distribution
D
as
follo
ws.
Fixing
an
arbitrary
elemen
t
e


S
,
w
e
let
D
assign
it
probabilit
y

 ,
and
assign
eac
h
other
elemen
t
of
S
probabilit
y
=(d
 ).
(Again,
only
p
oin
ts
in
S
are
assigned
non-zero
probabilit
y
.)
If
w
e
tak
e
a
sample
of
m
p
oin
ts
from
D
,
w
e
exp
ect
only
m
p
oin
ts
to
b
e
dieren
t
than
e

.
Th
us,
if
m
<
(d
 )=(0)
then
with
v
ery
high
probabilit
y
only
0m
<
(d
 )=
p
oin
ts
will
b
e
dieren
t
than
e

.
Applying
an
argumen
t
as
ab
o
v
e,
w
e
conclude
that
our
error
with
resp
ect
to
D
is
exp
ected
to
b
e
the
probabilit
y
assigned
to
p
oin
ts
not
seen
b
y
the
algorithm
times
one
half;
that
is,

d
 



d
 




=



Th
us,
there
exists
f

F
0

F
so
that
with
probabilit
y
at
least
=,
er
r
f
;D
(h)

.
This
establishes
a
lo
w
er
b
ound
of
(d
 )=(0)
on
the
sample
complexit
y
,
for
an
y


=
(and


=).
Bibliographic
Notes
The
P
A
C
learning
mo
del
w
as
in
tro
duced
b
y
V
alian
t
[].
Occam's
Razor
Theorem
is
due
to
[].
The
in
terested
reader
is
referred
to
a
textb
o
ok
b
y
Kearns
and
V
azirani
[].
.
A.
Blumer,
A.
Ehrenfeuc
h
t,
D.
Haussler,
and
M.K.
W
arm
uth.
Occam's
Razor.
Information
Pr
o
c
essing
L
etters,
V
ol.

(),
pages
{0,
April
	.
.
M.J.
Kearns
and
U.V.
V
azirani.
A
n
Intr
o
duction
to
Computational
L
e
arning
The
ory.
MIT
Press,
		.
.
L.G.
V
alian
t.
A
Theory
of
the
Learnable.
Communic
ations
of
the
A
CM,
V
ol.

(),
pages
{,
No
v
em
b
er
	.
App
endix:
Filling-up
gaps
for
the
pro
of
of
Claim
..
Oded's
Note:
The
gap
left
op
en
in
the
pro
of
w
as
the
assumption
that
w
e
can
slides
the
b
order
of
the
A
i
's
so
that
the
w
eigh
t
of
eac
h
of
them
is
exactly
=.
Firstly
,
w
e
rep
eat
the
commen
t
b
y
whic
h
it
is
not
essen
tial
to
ha
v
e
these
rectanges
ha
v
e
w
eigh
t
exactly
=,
and
it
suces
to
ha
v
e
eac
h
of
the
A
i
's
ha
v
e
w
eigh
t
().
But
still
it
ma
y
b
e
that
there
is
a
probabilit
y
mass
of

()
residing
on
one
single
axis-allinged
line.
This
problem
can
b
e
resolv
ed
is
sev
eral
w
a
ys.
F
or
example,
one
ma
y
p
ertub
all
p
oin
ts
at
random,
and
argue
that
the
p
erformance
of
the
algorithm
cannot
b
e
impro
v
ed
(a
pro
of
is
indeed
called
for!).
Alternativ
ely
,
one
ma
y
argue
sep
erately
for
these
patalogical
cases
of
single
lines
ha
ving
probabilit
y
mass
of

().

Lecture

Relativization
Notes
tak
en
b
y
Leia
P
assoni
Summary:
In
this
lecture
w
e
deal
with
relativization
of
complexit
y
classes.
In
partic-
ular,
w
e
discuss
the
role
of
relativization
with
resp
ect
to
the
P
?
=
N
P
question;
that
is,
w
e
shall
see
that
for
some
oracle
A,
P
A
=
N
P
A
whereas
for
another
A
P
A
=
N
P
A
.
Ho
w
ev
er,
it
also
holds
that
I
P
A
=
P
S
P
AC
E
A
for
a
random
A,
whereas
I
P
=
P
S
P
AC
E
Oded's
Note:
The
study
of
relativization
is
motiv
ated
b
y
the
b
elief
that
relativized
results
indicate
limitations
of
pro
of
tec
hniques
whic
h
ma
y
b
e
applied
in
the
real
(unrel-
ativized)
w
orld.
In
the
conclusion
section
w
e
explain
wh
y
w
e
do
not
share
this
b
elief.
In
a
n
utshell,
whereas
it
is
useful
to
refer
to
pro
of
tec
hniques
when
discussing
and
unifying
kno
wn
results,
it
is
misleading
to
refer
to
a
\pro
of
tec
hnique"
as
if
it
w
ere
a
domain
with
w
ell-dened
b
oundaries
(and
sp
eculate
on
whic
h
results
are
b
ey
ond
suc
h
b
oundaries).
In
con
trast,
w
e
see
b
enet
in
an
attempt
to
dene
framew
orks
for
pro
ving
certain
results,
and
discuss
prop
erties
of
pro
ofs
within
suc
h
w
ell-dened
framew
orks
(e.g.,
pro
ofs
of
certain
w
ell-dened
prop
erties
cannot
pro
v
e
a
particular
result
or
results
of
certain
w
ell-dened
form).
In
fact,
our
original
in
ten
tion
w
as
to
presen
t
suc
h
a
framew
ork,
called
Natural
Pro
ofs,
in
this
lecture.
This
in
ten
tion
w
as
abandoned
since
w
e
did
not
see
circuit
size
lo
w
er
b
ounds
in
the
course,
and
suc
h
pro
ofs
are
the
con
text
of
Natural
Pro
ofs.
.
Relativization
of
Complexit
y
Classes
W
e
ha
v
e
already
men
tioned
the
use
of
p
olynomial
time
mac
hines
that
ha
v
e
access
to
some
oracle
in
order
to
dene
the
P
olynomial
Hierarc
h
y
(P
H
).
Giv
en
an
y
t
w
o
comp
exit
y
classes
C

and
C

,
w
ere
C

is
the
class
of
oracles,
it
is
not
alw
a
ys
p
ossible
to
ha
v
e
a
natural
notion
of
what
is
the
class
C
C


{
it
is
not
necessarily
the
case
that
for
ev
ery
complexit
y
class
C

w
e
can
dene
its
r
elativization
to
C

.
There
are
some
conditions
under
whic
h
suc
h
a
relativization
can
b
e
done.
One
of
those
condi-
tions
seems
to
b
e
that
the
complexit
y
class
C

has
to
b
e
dened
in
terms
of
some
t
yp
e
of
mac
hines
{
that
is,
for
an
y
language
L
in
C

there
exists
a
mac
hine
M
L
suc
h
that
L
=
L(M
L
).
F
urthermore,
the
denition
of
M
L
has
to
b
e
extendable
to
the
denition
of
or
acle
machine.



LECTURE
.
RELA
TIVIZA
TION
Oracle
Mac
hines.
W
e
consider
three
t
yp
es
of
oracle
mac
hines:
Denition
.
A
(deterministic/nondeterministic/pr
ob
abilistic)
p
olynomial
time
oracle
mac
hine
is
a
machine
with
a
distinguishe
d
work
tap
e
{
the
query
tap
e,
wher
e
the
machine
may
make
or
acle
queries.
These
queries
ar
e
answer
e
d
by
a
function
c
al
le
d
the
oracle.
Some
notations:
M
f
(x)
denotes
a
computation
of
M
on
input
x
when
giv
en
access
to
oracle
f
.
When
the
mac
hine
writes
on
the
oracle
tap
e
a
query
q
(and
in
v
ok
es
the
oracle),
then
the
tap
e's
con
ten
ts
q
is
replaced
b
y
f
(q
).
W
e
also
write
M
A
(x),
where
the
oracle
A
is
a
language.
In
this
case
M
is
giv
en
access
to
the
c
haracteristic
function
of
the
language,

A
.
.
The
P
?
=
N
P
question
Relativized
T
rying
to
solv
e
the
P
=
N
P
conjecture,
the
corresp
onding
relativized
conjecture
has
b
een
in
v
esti-
gated,
that
is,
determining
whether
P
A
=
N
P
A
or
P
A
=
N
P
A
for
some
oracle
A.
Th
us,
w
e
ask
whic
h
of
the
follo
wing
t
w
o
p
ossibilities
holds:

There
exists
an
oracle
A
suc
h
that
P
A
=
N
P
A

There
exists
an
oracle
A
suc
h
that
P
A
=
N
P
A
Recall
that
P
A
def
=
fL
:
	
deterministic
p
oly
 time
M
:
L(M
A
)
=
Lg
P
A
is
then
a
class
of
languages:
from
a
coun
table
set
of
mac
hines
M
w
e
get
a
coun
table
set
of
oracle
mac
hines
M
A
that
run
in
p
olynomial
time,
eac
h
ha
ving
oracle
access
to
A.
Note
that
if
A
=

or
A
=
f0;
g

,
then
P
A
=
P
.
In
fact,
if
the
answ
ers
giv
en
to
queries
to
the
oracle
are
all
\y
es"
or
all
\no",
then
an
y
language
in
P
A
can
b
e
accepted
b
y
a
deterministic
p
olynomial
time
mac
hine,
just
committing
the
oracle
questins
and
asw
ers.
Also,
A

P
)
P
A
=
P
:
in
this
case
a
P
mac
hine
can
sim
ulate
the
access
to
the
oracle
in
p
olynomial
time.
Therefore,
it
is
w
orth
while
to
use
oracles
mac
hines
when
the
oracle
A
denes
a
language
whic
h
is
\complex".
In
the
same
w
a
y
N
P
A
can
b
e
dened:
N
P
A
def
=
fL
:
	
nondeterministic
p
oly
 time
M
:
L(M
A
)
=
Lg
It
could
seem
that
if
	A
:
P
A
=
N
P
A
then
P
=
N
P
,
but
this
is
not
the
case.
Of
course
it
w
ould
b
e
correct
to
assert
that
if
A
P
A
=
N
P
A
then
P
=
N
P
,
b
ecause
in
this
case
it
w
ould
b
e
p
ossible
to
consider
a
trivial
oracle
lik
e
A
=

or
A
=
f0;
g

.
Going
bac
k
to
the
P
?
=
N
P
question,
a
p
ossible
reason
for
in
v
estigating
its
relativized
v
ersion
is
the
hop
e
that
a
result
obtained
for
a
class
of
oracles
w
ould
shed
ligh
t
on
the
unrelativized
question.
The
t
w
o
follo
wing
theorems
sho
w
that
this
hop
e
fails.
Theorem
.
A
n
or
acle
A
exists
such
that
P
A
=
N
P
A

..
THE
P
?
=
N
P
QUESTION
RELA
TIVIZED

Pro
of:
It
is
ob
vious
that
A
P
A

N
P
A
.
It
is
left
to
sho
w
that
	A
:
N
P
A

P
A
.
The
idea
of
the
pro
of
is
to
c
ho
ose
as
oracle
A
a
P
S
P
AC
E
-complete
language,
that
is,
a
language
so
\p
o
w
erful"
that
no
more
adv
an
tage
is
left
for
a
nondeterministic
mac
hine
o
v
er
a
deterministic
one
if
they
b
oth
ha
v
e
access
to
oracle
A.
W
e
rst
sho
w
that
N
P
P
S
P
AC
E

P
S
P
AC
E
.
Let
A
b
e
a
P
S
P
AC
E
language
and
let
L

N
P
A
.
Then
there
exists
a
nondeterministic
p
oly-time
mac
hine
M
suc
h
that
L
=
L(M
A
).
Equiv
alen
tly
,
there
exists
a
deterministic
p
oly-time
mac
hine

M
suc
h
that
x

L;
	y
and
p
olynomial
p

:
jy
j

p

(jxj)
and
suc
h
that

M
A
(x;
y
)
accepts.
It
is
p
ossible
then
to
deriv
e
a
deterministic
mac
hine
M
0
suc
h
that
M
0
(x)
accepts
using
at
most
p

(jxj)
cells
of
space.
Mac
hine
M
0
cycles
throughout
all
p
ossible
y
's
of
length

p

(jxj),
sim
ulating

M
A
(x;
y
)
for
eac
h
suc
h
y
.
In
this
sim
ulation,
when

M
queries
the
oracle
on
a
w
ord
w
,
mac
hine
M
0
sim
ulates
the
oracle
b
y
determining
whether
w

A.
Note
that
jw
j

p

(jxj),
where
p

(jxj)
is
the
p
olynomial
b
ound
on
the
n
um
b
er
of
steps
in
whic
h

M
accepts
the
input
x.
Since
A

P
S
P
AC
E
it
follo
ws
that
determining
w

A
can
b
e
done
in
space
pol
y
(jw
j)
=
pol
y
(jxj).
So,
M
0
can
decide
L

N
P
A
using
in
all
p(jxj)
space
cells
for
some
p
olynomial
p.
W
e
conclude
that
for
an
y
A

P
S
P
AC
E
,
N
P
A

P
S
P
AC
E
.
(.)
W
e
no
w
sho
w
that
for
some
A

P
S
P
AC
E
(sp
ecically
,
an
y
P
S
P
AC
E
-complete
A
will
do),
P
S
P
AC
E

P
A
.
Consider
a
P
S
P
AC
E
-complete
language,
A.
F
or
an
y

L

P
S
P
AC
E
,
consider
the
Co
ok-reduction
of

L
to
A;
that
is,
a
p
olynomial-time
oracle
mac
hine
R
that
giv
en
access
to
oracle
A
decides

L.
But
this
means
that

L
=
L(R
A
)

P
A
(whic
h
also
equals
P
P
S
P
AC
E
).
Th
us,
for
some
A

P
S
P
AC
E
,
P
S
P
AC
E

P
A
.
(.)
Com
bining
Equations
(.)
and
(.),
w
e
conclude
that
there
exists
an
oracle
A
suc
h
that
N
P
A

P
S
P
AC
E

P
A
and
the
theorem
follo
ws.
In
spite
of
the
result
of
Theorem
.,
the
oracle
A
can
b
e
c
hosen
in
suc
h
a
w
a
y
that
the
t
w
o
relativized
classes
P
A
and
N
P
A
are
dieren
t.
Theorem
.
A
n
or
acle
A
exists
such
that
P
A
=
N
P
A
Pro
of:
Since
P
A

N
P
A
,
w
e
w
an
t
to
nd
a
language
L
suc
h
that
L

N
P
A
n
P
A
,
that
is,
a
language
that
separates
the
t
w
o
classes.
In
this
case
the
idea-tec
hnique
for
getting
the
separation
is
to
dene
the
language
using
the
oracle
A.
Th
us,
for
ev
ery
oracle
A
w
e
dene
L
A
def
=
f
n
:
	w

f0;
g
n
:
w

Ag
What
is
p
eculiar
ab
out
language
L
A
is
that
it
describ
es
whether
A
\
f0;
g
n
is
a
nonempt
y
set.
It
can
b
e
seen
that
L
A

N
P
A
,
b
y
sho
wing
a
nondeterministic
mac
hine
with
access
to
A
that
accepts
L
A
:
On
input
x

f0;
g
n
:
.
if
x
=

n
,
then
reject


LECTURE
.
RELA
TIVIZA
TION
.
otherwise
guess
w

f0;
g
n
.
accept
i
w

A
If
x
=

n

L
A
,
there
exists
a
computation
of
the
mac
hine
that
accepts
{
the
computation
that
succeeds
in
guessing
a
prop
er
w
whic
h
is
c
hec
k
ed
to
b
e
in
A
b
y
an
oracle
query
to
A.
Otherwise,
if

n
=

L
A
,
there
is
no
w

f0;
g
n
so
that
w

A,
and
therefore
the
mac
hine
can
nev
er
accept;
so
in
this
case,
no
matter
what
guess
it
mak
es,
the
mac
hine
alw
a
ys
rejects.
No
w
w
e
ha
v
e
to
sho
w
the
existence
of
an
oracle
A
so
that
no
deterministic
p
oly-time
mac
hine
can
accept
L
A
.
W
e
rst
sho
w
the
follo
wing
Claim
..
F
or
every
deterministic
p
olynomial
time
M
,
ther
e
exists
A
so
that
L
A
=
L(M
A
)
Pro
of:
W
e
are
giv
en
mac
hine
M
and
p(),
the
p
olynomial
upp
er
b
ound
on
the
running
time
of
M
and
on
the
n
um
b
er
of
p
ossible
queries
M
can
ask
to
the
oracle.
W
e
then
c
ho
ose
n

suc
h
that
n

=
minfm
:

m
>
p(m)g,
and
w
e
consider
the
computation
of
M
A
on
input

n

.
The
purp
ose
is
to
dene
A
in
suc
h
a
w
a
y
that
M
will
err
in
its
decision
whether

n


A
or
not.
Denition
of
A:
w
e
consider
three
cases
regarding
the
length
n
=
jq
j
of
a
query
q
:
n
<
n

,
n
=
n

,
n
>
n

.

If
n
<
n

,
w
e
can
for
instance
assume
w
e
ha
v
e
already
answ
ered
\0"
to
all

n
p
ossible
queries
during
computations
with
inputs
of
length
less
than
n

.
In
this
case,
on
input

n

w
e
answ
er
consisten
tly
\0".

If
n
=
n

,
then
w
e
use
the
fact

n
>
p(n),
whic
h
implies
that
not
all
n-bit
strings
are
queried.
T
o
all
(up
to
p(n))
queries
on
input

n

answ
er
\0".
What
ab
out
the
queries
whic
h
M
didn't
ask?
W
e
will
dene
A
in
suc
h
a
w
a
y
that
{
If
M
A
(
n

)
accepts
then
let
A
[
f0;
g
n

=
.
So
w
e
don't
put
in
A
an
y
w
ord
of
length
n

,
i.e.
w
e
answ
er
\0"
to
all
remaining
queries;
{
If
M
A
(
n

)
rejects
then
let
A
[
f0;
g
n

=
f

w
g,
where

w
is
one
of
the
w
ords
of
length
n

that
ha
v
e
not
b
een
queried
b
y
M
.
W
e
stress
that
M
A
(
n

)
do
es
not
dep
end
on
whether

w

A
(since

w
is
not
queried
in
this
compuation),
but

n

L
A
do
es
dep
end
on
whether

w

A.

If
n
>
n

.
Lik
e
b
efore,
answ
er
\0".
But
in
this
case
w
e
are
committing
the
oracle
answ
ers
on
longer
strings.
An
yw
a
y
,
this
is
not
a
problem
b
ecause
an
y
n
>
n

has
to
b
e
n

p(n

),
otherwise
M
w
ouldn't
ha
v
e
the
time
to
write
the
query
.
Th
us,
giv
en
n

there
is
a
nite
n
um
b
er
of
p
ossible
lengths
of
queries
n
:
n
>
n

,
so
w
e
can
carry
on
at
this
stage
the
construction
for
these
longer
n's.
(This
extra
discussion
is
only
imp
ortan
t
for
the
extension
b
elo
w.)
Oracle
A
is
th
us
dened
in
suc
h
a
w
a
y
that,
on
input

n

mac
hine
M
A
cannot
decide
whether

n

is
in
L
A
or
not.
An
yw
a
y
,
Claim
..
is
not
enough
to
pro
v
e
Theorem
.,
since
the
oracle
A
in
the
claim
is
p
ossibly
dieren
t
for
an
y
M
.
W
e
need
to
state
a
stronger
claim.
That
is
	A
:

deterministic
p
olynomial-time
M
,
L
A
=
L(M
A
)

..
THE
P
?
=
N
P
QUESTION
RELA
TIVIZED

This
claim
is
pro
v
able
b
y
extending
the
same
argumen
t
used
in
Claim
...
The
idea
is
as
follo
ws:
rst
w
e
en
umerate
all
deterministic
p
oly-time
mac
hines
M

;
M

;
:
:
:
;
M
i
;
:
:
:
#
#
#
n

;
n

;
:
:
:
;
n
i
;
:
:
:
and
to
eac
h
mac
hine
M
i
w
e
asso
ciate
a
\large
enough"
in
teger
n
i
.
The
oracle
A
is
built
in
stages
in
suc
h
a
w
a
y
that,
at
an
y
step
w
e
deal
with
a
p
ortion
A(i
 )

f0;
g
n
i 
of
the
oracle.
Then
A
=
[
i>0
A(i)
and
M
A
i
(
n
i
)
=
M
A(i )
i
(
n
i
).
The
oracle
A
is
suc
h
that,
at
an
y
step
i,
M
A
i
errs
in
decision
on
input

n
i
.
Dene
A(i)
=
fw
ords
that
ha
v
e
b
een
placed
in
A
after
step
ig.
W
e
let
A(0)
=

and
n
0
=
0.
Consider
mac
hine
M

and
its
p
olynomial
b
ound
p

().
Cho
ose
n

suc
h
that
n

=
minfm
:

m
>
p

(m)g
A
t
this
p
oin
t
A
=
A(0)
=
.
T
o
build
A()
w
e
follo
w
the
same
pro
cedure
used
in
Claim
..;
in
particular

if
M
A

(
n

)
accepts
then
A()
=
A(0);

if
M
A

(
n

)
rejects
then
A()
=
A(0)
[
fw
n

g,
where
w
n

is
one
of
the
w
ords
of
length
n

that
ha
v
e
not
b
een
queried
b
y
M

.
A
t
stage
i,
consider
mac
hine
M
i
and
p
i
().
Cho
ose
n
i
suc
h
that
n
i
=
minfm
:

m
>
p
i
(m)
and
(j
<
i);
m
>
p
j
(n
j
)g
Curren
tly
A
equals
A(i
 );
so
when
on
input

n
i
mac
hine
M
i
asks
queries
of
length
less
than
n
i
,
the
an
w
ers
ha
v
e
to
b
e
consisten
t
with
what
has
previously
dened
to
b
e
in
the
oracle.
Lik
e
b
efore,
w
e
then
build
A(i)
suc
h
that

if
M
A
i
(
n
i
)
accepts
then
A(i)
=
A(i
 );

if
M
A
i
(
n
i
)
rejects
then
A(i)
=
A(i
 )
[
fw
n
i
g,
where
w
n
i
is
one
of
the
w
ords
of
length
n
i
that
ha
v
e
not
b
een
queried
b
y
M
i
.
The
w
ord
w
n
i
p
ossibly
added
to
A
at
this
stage
is
one
of
those
not
queried
b
y
M
i
on
input

n
i
;
moreo
v
er,
w
ords
p
ossibly
added
in
successiv
e
stages
ha
v
e
length
greater
than
p
j
(n
j
)
for
ev
ery
j
<
i.
This
ensures
that
M
A(i)
j
(
n
j
)
=
M
A(j
 )
j
(
n
j
)
for
ev
ery
j
<
i.
The
oracle
A
is
then
built
in
suc
h
a
w
a
y
that
no
p
olynomial
time
mac
hine
M
with
oracle
A
exists
suc
h
that
L(M
A
)
=
L
A
{
ev
ery
M
A
i
fails
deciding
on
input

n
i
.
It
is
imp
ortan
t
to
note
that
this
construction
w
orks
only
on
a
sp
ecic
p
ortion
of
the
oracle
A
at
ev
ery
stage,
k
eeping
all
other
p
ortions
of
A
un
touc
hed.
Considering
the
results
of
Theorem
.
and
Theorem
.,
w
e
can
understand
that
the
in
v
es-
tigation
of
the
relativized
P
=
N
P
conjecture
cannot
p
ossibly
help
in
pro
ving
or
dispro
ving
the
unrelativized
one.
What
instead
should
b
e
tak
en
from
these
results
is
the
fact
that
the
P
=
N
P
conjecture
should
b
e
in
v
estigated
with
pr
o
of
te
chniques
that
do
not
r
elativize,
that
is,
pro
of
thec
h-
niques
that
cannot
b
e
extended
to
oracles
{
so
the
sim
ulation
and
diagonalization
thec
hniques
used
in
the
ab
o
v
e
theorems
do
not
seem
to
b
e
adequate
for
in
v
estigating
the
conjecture.


LECTURE
.
RELA
TIVIZA
TION
.
Relativization
with
a
Random
Oracle
Theorem
.
can
b
e
someho
w
generalized;
in
fact
it
is
p
ossible
to
pro
v
e
that
P
r
ob
A
[P
A
=
N
P
A
]
=

(.)
The
probabilily
is
tak
en
o
v
er
all
oracles
A;
Eq.
(.)
means
that
for
almost
all
oracles
A,
P
=
N
P
holds.
Since
P
A

B
P
P
A
for
all
A,
Eq.
(.)
is
a
consequence
of
the
follo
wing
Theorem
.
F
or
a
r
andom
or
acle
A,
N
P
A

B
P
P
A
,
that
is,
P
r
ob
A
[N
P
A

B
P
P
A
]
=

Pro
of:
W
e
ha
v
e
to
nd
a
language
L
suc
h
that
L

N
P
A
but
L
=

B
P
P
A
with
probabilit
y
.
Again,
the
idea
is
to
dene
L
A
dep
ending
on
the
oracle
A:
L
A
def
=
f
n
:
	w

f0;
g
n
:
u

C
n
;
w
u

Ag
Here
C
n
is
dened
to
b
e
an
y
canonical
set
of
n
strings
of
length
n.
F
or
instance,
w
e
can
tak
e
C
n
to
b
e
the
set
of
all
strings
of
Hamming
w
eigh
t
:
C
n
def
=
f0
:
:
:
0;
00
:
:
:
0;
:
:
:
;
0
:
:
:
0g.
Oded's
Note:
The
idea
b
ehind
the
use
of
C
n
is
to
asso
ciate
with
eac
h
w

f0;
g
n
a
coin
with
o
dds

 n
of
b
eing

(and
b
eing
zero
otherwise).
If
our
oracle
w
ere
to
b
e
c
hosen
so
that
eac
h
string
w

f0;
g
n
is
in
it
with
probabilit
y

 n
then
the
extra
complication
of
using
C
n
w
ould
not
ha
v
e
b
een
needed.
Ho
w
ev
er,
the
oracle
is
c
hosen
so
that
eac
h
string
is
equally
lik
ely
to
b
e
or
not
b
e
in
it,
and
so
w
e
need
to
implemen
t
a
coin
with
o
dds

 n
of
b
eing

b
y
using
an
un
biased
coin.
That's
exactly
what
the
ab
o
v
e
construction
do
es.
The
aim
of
all
this
is
to
get
to
a
situation
that

n

L
A
and
\
n

L
A
via
a
single
witness
w
"
are
ab
out
as
lik
ely
,
and
are
also
quite
lik
ely
.
Again,
it
can
b
e
seen
that
L
A

N
P
A
b
y
sho
wing
a
nondeterministic
mac
hine
with
access
to
A
that
generates
L
A
:
On
input
x

f0;
g
n
:
.
if
x
=

n
,
then
reject
.
otherwise
guess
w

f0;
g
n
.
accept
i
u

C
n
;
w
u

A
Here
the
mac
hine
mak
es
n
queries
to
A
and
accepts
i
all
of
them
are
an
w
ered
\y
es"
b
y
the
oracle.
No
w
for
a
randomly
c
hosen
A
it
can
b
e
sho
wn
that
P
r
ob
A
[
n
=

L
A
]
=
P
r
ob
A
[
	w
:
u

C
n
;
w
u

A]
=
=e
=
constan
t
(.)
In
fact,
if
w
and
u
are
xed,
then
P
r
ob
A
[w
u

A]
=
=
b
ecause
A
is
c
hosen
at
random,
th
us
ev
ery
query
has
probabilit
y
=
to
b
e
in
A
or
not.
If
only
w
is
xed
then
P
r
ob
A
[u

C
n
;
w
u

A]
=

 n
since
all
n
ev
en
ts
[w
u

A]
are
indep
enden
t.
So
for
this
xed
w
,
P
r
ob
A
[:u

C
n
;
w
u

A]
=

 P
r
ob
A
[u

C
n
;
w
u
=

A]
=

 
 n
.

..
RELA
TIVIZA
TION
WITH
A
RANDOM
ORA
CLE
	
Again,
there
are

n
p
ossible
w
and
all
ev
en
ts
are
indep
enden
t,
th
us
P
r
ob
A
[
n
=

L
A
]
=
P
r
ob
A
[w
;
:u

C
n
;
w
u

A]
=
(
 
 n
)

n
=
=e
=
constan
t.
In
a
similar
w
a
y
it
can
also
b
e
sho
wn
that
P
r
ob
A
[	!w

f0;
g
n
:
u

C
n
;
w
u

A]

=e
=
constan
t
(.)
Language
L
A
w
as
then
dened
in
suc
h
a
w
a
y
that

with
constan
t
probabilit
y
,

n
=

A

with
constan
t
probabilit
y
,
there
exists
a
unique
witness
for

n

A
i.e.
	!w
:
u

C
n
;
w
u

A
In
tuitiv
ely
,
there
is
therefore
a
v
ery
small
probabilit
y
for
an
y
probabilistic
p
olynomial
time
mac
hine
to
come
across
the
unique
w
so
that
u

C
n
;
w
u

A.
In
fact
the
follo
wing
can
b
e
pro
v
en:
Claim
..
L
et
M
b
e
a
pr
ob
abilistic
p
olynomial
time
or
acle
machine.
Then,
for
suciently
lar
ge
n
P
r
ob
A
[M
A
(
n
)
=

L
A
(
n
)]
<
0:
R
emarks:
What
w
e
actually
w
an
t
to
ac
hiev
e
is
P
r
ob
A
[L(M
A
)
=
L
A
]
=
0;
b
y
Claim
:
w
e
can
mak
e
the
probabilit
y
that
M
A
decides
correctly
on
all
inputs

n
(i.e.
for
all
n)
v
anish
exp
onen
tially
with
the
n
um
b
er
of
n's
w
e
consider.
Moreo
v
er,
w
e
w
an
t
Claim
:
to
hold
for
an
y
p
ossible
M
,
that
is
P
r
ob
A
[B
P
P
A

L
A
]
=
0.
This
extension
can
b
e
carried
out
similarly
to
what
w
as
done
in
Theorem
..
Pro
of:
W
e
c
ho
ose
n
large
enough
so
that
the
running
time
of
M
A
(
n
)
whic
h
equals
pol
y
(n)


n
.
No
w
consider
the
case
in
whic
h
on
input

n
mac
hine
M
mak
es
queries
of
length
n
(other
cases
can
b
e
carried
as
it
has
b
een
done
in
Theorem
.).
W
e
ev
aluate
P
r
ob
A
[M
A
(
n
)
=

L
A
(
n
)]
=
P
no
+
P
uni
+
P
rest
;
where
P
no
def
=
P
r
ob
A
[
L
A
(
n
)
=
0]

P
r
ob
A
[M
A
(
n
)
=
0j
L
A
(
n
)
=
0]
P
uni
def
=
P
r
ob
A
[	!w
:
u

C
n
;
w
u

A]

P
r
ob
A
[M
A
(
n
)
=
j	!w
:
u

C
n
;
w
u

A]
P
rest
def
=
P
r
ob
A
[
L
A
(
n
)
=

and
	w
0
=
w
:
u

C
n
;
w
u

A
&
w
0
u

A]
(That
is,
P
no
represen
ts
the
probabilit
y
that
M
is
correct
on
an
input
not
in
L
A
,
P
uni
the
probabilit
y
M
is
correct
on
an
input
with
a
unique
witness,
and
P
rest
the
that
M
is
correct
on
an
input
with
sev
eral
witnesses.)
Lo
oking
at
the
v
arious
probabilities,
w
e
ha
v
e:
.
By
Eq.
(.),
P
r
ob
A
[
L
A
(
n
)
=
0]
=
=e,
.
By
Eq.
(.),
P
r
ob
A
[	!w
:
u

C
n
;
w
u

A]

=e.
So
P
rest

P
r
ob
A
[	w
0
=
w
:
u

C
n
;
w
u

A
&
w
0
u

A]


 =e
(.)
It
is
p
ossible
to
see
that
the
dierence
b
et
w
een
the
t
w
o
probabilities

0
LECTURE
.
RELA
TIVIZA
TION
a)
p

def
=
P
r
ob
A
[M
A
(
n
)
=
j
L
A
(
n
)
=
0]
b)
p

def
=
P
r
ob
A
[M
A
(
n
)
=
j	!w
:
u

C
n
;
w
u

A]
is
v
ery
small.
In
fact,
w
e
can
dene
the
follo
wing
oracle:
A
(w
0
)
def
=

randomly
c
ho
ose
A
so
that

n
=

L
A
meaning
w

f0;
g
n
	u

C
n
:
w
u
=

A

only
on
w
0
mo
dify
A
so
that
u

C
n
;
w
0
u

A
Then
it
can
b
e
seen
that
the
probabilit
y
{
tak
en
o
v
er
a
random
A
and
a
random
selceted
w
of
length
n
b')
P
r
ob
A;w
f0;g
n
[M
A
(w
)
(
n
)
=
]
=
p

In
fact,
consider
the
pro
cess
of
selecting
A
at
random
suc
h
that
there
exists
no
w
for
whic
h
u

C
n
;
w
u

A.
Then
select
w
0
at
random
and
mo
dify
A
so
that
only
in
w
0
u

C
n
;
w
0
u

A.
This
pro
cess
is
the
same
as
selecting
a
random
A
conditioned
to
the
existence
of
a
unique
w
suc
h
that
u

C
n
;
w
u

A.
So
the
set
of
oracles
A
(w
)
considered
in
b')
has
the
same
distribution
as
the
set
of
random
oracles
conditioned
to
the
existence
of
a
unique
w
for
whic
h
u

C
n
;
w
u

A,
that
is
considered
in
b).
Probabilities
a)
and
b')
are
then
v
ery
close;
in
fact
in
b'),
when
the
mac
hine
asks
the
rst
query
,
there
is
a
c
hance
only
of

 n
for
the
mac
hine
to
come
across
the
unique
w
suc
h
that
u

C
n
;
w
u

A.
(On
all
other
w
's
the
t
w
o
oracles
b
eha
v
e
the
same.)
Asking
the
second
query
,
there
is
a
c
hance
only
of
=(
n
 )
for
the
mac
hine
to
get
dieren
t
answ
ers
to
the
query
,
and
so
on.
Since
the
total
n
um
b
er
of
p
ossible
queries
in
n
is
p(n)
for
some
p
olynomial
p,
the
dierence
p

 p

is
appro
ximately
p(n)

n


for
the
appropriate
n
that
has
b
een
c
hosen.
The
fact
that
these
t
w
o
probabilities
are
so
close
means
that
there
is
a
v
ery
small
c
hance
that
the
mac
hine
can
notice
the
mo
dication
that
has
b
een
in
tro
duced.
Th
us,
since
P
r
ob
A
[M
A
(
n
)
=
0j
L
A
(
n
)
=
0]
=

 P
r
ob
A
[M
A
(
n
)
=
j
L
A
(
n
)
=
0]
=

 p

,
w
e
see
that
P
r
ob
A
[M
A
(
n
)
=

L
A
(
n
)]
=
P
no
+
P
uni
+
P
rest

P
r
ob
A
[
L
A
(
n
)
=
0]

(
 p

)
+
P
r
ob
A
[	!w
:
u

C
n
;
w
u

A]

p

+
(
 e
 
)

e
 

(
 p

)
+
e
 

p

+

 e
 
=
e
 

(p

 p

)
+

 e
 
Since
the
term
e
 
(p

 p

)
<
p(n)=
n
is
negligible,
w
e
get
P
r
ob
A
[M
A
(
n
)
=

L
A
(
n
)]


 e
 
<
0:
and
the
claim
follo
ws.
This
also
completes
the
pro
of
of
Theorem
..
The
reason
for
in
v
estigating
ho
w
P
and
N
P
b
eha
v
e
if
relativized
to
a
random
oracle
is
the
follo
wing.
The
statemen
ts
in
Theorems
.
and
.
only
mean
that
b
oth
P
=
N
P
and
P
=
N
P
are

..
CONCLUSIONS

supp
orted
b
y
at
le
ast
one
oracle.
This
cannot
p
ossibly
giv
e
evidence
to
w
ards,
sa
y
,
P
=
N
P
.
But
then,
what
Theorem
.
sa
ys
is
that,
considering
all
p
ossible
oracles,
only
a
negligible
fraction
of
them
supp
orts
P
=
N
P
.
So
it
seem
plausible
that
suc
h
an
assertion
supp
orts
the
P
=
N
P
conjecture.
The
underlying
reasoning,
if
v
alid,
should
extend
to
an
y
pair
of
complexit
y
classes.
It
is
kno
wn
as
the
R
andom
Or
acle
Hyp
othesis
and
asserts
that
t
w
o
complexit
y
classes
are
dieren
t
if
and
only
if
they
dier
under
relativization
to
a
random
oracle.
Unfortunately
,
the
Random
Oracle
Hyp
othesis
turns
out
to
b
e
false.
F
or
example,
the
follo
wing
can
b
e
pro
v
en:
Theorem
.
F
or
a
r
andom
or
acle
A,
coN
P
A

I
P
A
,
that
is,
P
r
ob
A
[co
N
P
A

I
P
A
]
=

Since
it
is
kno
wn
that
co
N
P

I
P
,
Theorem
.
indicates
that
the
Random
Oracle
Hyp
othesis
fails.
Th
us,
the
random-relativization
separation
result
of
Theorem
.
cannot
sa
y
an
ything
ab
out
separation
of
the
unrelativized
classes.
.
Conclusions
Oded's
Note:
This
section
w
as
written
b
y
me.
Although
m
y
main
motiv
ation
for
giving
this
lecture
w
as
to
presen
t
these
views,
and
although
I
did
express
them
in
class,
little
of
them
found
their
w
a
y
to
the
ab
o
v
e
notes.
(Unfortunately
,
in
general,
studen
ts
tend
to
fo
cus
on
tec
hnical
material
and
attac
h
less
imp
ortance
to
conceptual
discussions.
A
t
times
ev
en
high-lev
el
discussions
of
pro
of
ideas
are
giv
es
less
atten
tion
than
some
v
ery
lo
w-lev
el
details.)
The
study
of
relativized
complexit
y
classes
w
as
initiated
with
the
hop
e
that
it
ma
y
shed
some
ligh
t
on
the
unrelativized
classes.
Our
o
wn
opinion
is
that
this
hop
e
is
highly
unjustied.
Relativized
results
as
predictors
of
non-relativized
results.
The
most
naiv
e
hop
e
regarding
relativized
complexit
y
classes
is
that
the
relations
whic
h
hold
in
some
relativized
w
orld
are
exactly
those
holding
in
the
real
(unrelativized)
w
orld.
As
sho
wn
ab
o
v
e,
this
cannot
p
ossibly
b
e
true
since
dieren
t
oracles
ma
y
lead
to
dieren
t
relativized
results.
That
is,
there
ma
y
b
e
a
relativized
w
orld
(i.e.,
an
oracle)
in
whic
h
some
relation
holds
and
another
in
whic
h
the
relation
do
es
not
hold;
whereas
the
same
situation
can
not
p
ossibly
hold
in
the
(single)
real
w
orld.
Conicting
relativizations
cannot
o
ccur
when
one
considers
relativization
results
whic
h
hold
for
almost
all
orcales
(i.e.,
relativ
e
to
a
random
oracle).
A
new
hop
e
p
ostulated
b
y
the
\Random
Oracle
Conjecture"
w
as
that
relations
whic
h
hold
in
almost
al
l
relativized
w
orld
are
exactly
those
holding
in
the
real
(unrelativized)
w
orld.
Ho
w
ev
er,
this
conjecture
has
b
een
refuted;
w
e
ha
v
e
men
tioned
ab
o
v
e
one
suc
h
refutation
(i.e.,
co
N
P

I
P
and
y
et
coN
P
A

I
P
A
for
almost
all
oracles
A).
W
e
men
tion
that
not
only
that
w
e
ha
v
e
results
whic
h
con
tradict
the
ab
o
v
e
naiv
e
hop
es,
but
also
the
w
a
y
these
results
are
pro
v
en
seems
to
indicate
that
the
structure
of
computation
of
an
oracle
mac
hine
has
little
to
do
with
the
computation
of
an
analogous
regular
mac
hine.
Add
to
this
our
initial
commen
t
b
y
whic
h
not
an
y
class
can
b
e
relativized
(i.e.,
relativization
requires
that
the
class
b
e
dened
in
terms
of
some
t
yp
e
of
mac
hines
and
that
these
mac
hine
extend
naturally
{
at
least
in
a
syn
taxtic
sense
{
to
orcale
mac
hines).
W
e
stress
again
that
suc
h
extensions,
ev
en
when
they
seem
syn
taxtically
natural,
ma
y
totally
disrupt
the
\seman
tics"
asso
ciated
with
the
bare
mac
hine.
Our
conclusion
is
that
relativized
results
are
p
o
or
predictors
of
non-relativized
results.
The
adv
o
cates
of
relativization
are
certainly
a
w
are
of
the
ab
o
v
e,
and
their
curren
t
adv
o
cacy
is
based
on
the
b
elief
that
relativized
results
indicate
limitations
of
pro
of
tec
hniques
whic
h
ma
y
b
e
applied
in
the
real
(unrelativized)
w
orld.
That
is
{


LECTURE
.
RELA
TIVIZA
TION
Relativized
results
as
indicating
limitations
of
pro
of
tec
hniques.
The
thesis
(of
the
ab
o
v
e
adv
o
cates)
is
that
a
relativized
result
asserting
some
relation
indicates
that
the
con
trary
relation
c
annot
b
e
pr
oven
in
the
real
(unrelativized)
w
orld
via
\a
pro
of
whic
h
relativizes".
This
thesis
is
based
on
the
hidden
assumption
that
there
is
a
natural
(generic)
w
a
y
of
extending
a
real
(unrela-
tivized)
pro
of
in
to
a
v
ersion
whic
h
refers
to
the
corresp
onding
oracle
classes.
The
thesis
suggests
that
one
should
consider
the
question
of
whether
suc
h
a
natural
exten
tion
is
indeed
v
alid
in
the
curren
t
case.
Supp
ose
that
some
result
fails
relativ
e
to
some
oracle,
then
the
thesis
asserts
that
an
y
pro
of
for
whic
h
the
natural
relativization
remains
v
alid
will
fail
to
pro
v
e
the
real
(unrelativized)
result
(since
it
w
ould
ha
v
e
pro
v
en
the
same
result
also
relativ
e
to
an
y
oracle,
whereas
the
relativized
result
asserts
the
existence
of
an
oracle
for
whic
h
the
result
do
es
not
hold).
Our
ob
jection
to
the
ab
o
v
e
thesis
is
based
on
the
ob
jection
to
the
assumption
that
suc
h
a
natural
(generic)
w
a
y
of
extending
a
real
(unrelativized)
pro
of
in
to
a
v
ersion
whic
h
refers
to
the
\corresp
onding
oracle
classes"
exists.
Indeed
certain
kno
wn
pro
ofs
(e.g.,
the
time
hierarc
h
y)
ex-
tend
naturally
to
the
relativized
w
orld,
but
w
e
don't
see
a
generic
pro
cedure
of
transforming
real
(unrelativized)
pro
ofs
in
to
relativized
ones.
(Recall
our
w
arning
that
not
all
complexit
y
classes
ha
v
e
a
relativized
v
ersion.)
Without
suc
h
generic
pro
cedure
of
transforming
unrelativized
pro
ofs
in
to
relativized
ones,
it
is
not
clear
what
is
suggested
b
y
the
ab
o
v
e
thesis.
F
urthermore,
one
can
transform
an
y
pro
of
whic
h
is
kno
wn
to
\relativize"
in
to
a
pro
of
(of
the
same
statemen
t)
that
do
es
not
\relativize"
(via
the
pro
cess
witnessing
the
former
\relativization").
Of
course
this
transfor-
mation
can
b
e
undone
b
y
a
new
transformation,
but
the
latter
can
not
b
e
considered
generic
in
an
y
natural
sense.
What
is
left
is
a
suggestion
to
use
relativized
results
as
a
to
ol
for
testing
the
viabilit
y
of
certain
approac
hes
for
pro
ving
real
(unrelativized)
results.
That
is
{
Relativized
results
as
a
debugging
to
ol.
Supp
ose
one
has
a
v
ague
idea
on
ho
w
to
pro
v
e
some
result
regarding
the
real
w
orld.
F
urther
supp
ose
that
one
is
not
sure
whether
this
idea
can
b
e
carried
out.
Then
the
suggestion
is
to
ask
whether
the
same
idea
seems
to
apply
also
to
the
relativized
w
orld
and
whether
a
con
trary
relativization
result
is
kno
wn.
The
thesis
is
that
if
the
answ
er
to
b
oth
questions
is
p
ossitiv
e
then
one
should
giv
e-up
hop
e
that
the
idea
can
w
ork
(in
the
real
w
orld).
W
e
ob
ject
ev
en
to
this
minimalistic
suggestion.
If
one
has
a
v
ague
idea
whic
h
ma
y
or
ma
y
not
w
ork
in
the
real
w
ork,
p
ossibly
dep
ending
on
unsp
ecied
details,
then
wh
y
should
one
b
eliev
e
in
the
v
alidit
y
of
statemen
ts
inferred
from
applying
a
v
ague
pro
cedure
(i.e.,
\relativization")
to
this
v
ague
idea?
Indeed,
in
some
cases
the
(relativized)
indication
obtained
(as
suggested
ab
o
v
e)
ma
y
sa
v
e
time
whic
h
could
ha
v
e
b
een
w
asted
pursuing
an
unfruitful
idea,
but
in
some
cases
it
ma
y
cause
to
abandon
a
fruitful
idea.
W
e
see
no
reason
to
b
eliev
e
that
the
probabilit
y
of
the
rst
ev
en
t
is
greater
than
that
of
the
second,
and
p
oin
t
out
that
the
pa
y-o
(b
enet/damage)
seems
to
b
e
m
uc
h
greater
in
the
second.
Bibliographic
Notes
The
study
of
relativized
complexit
y
classes
w
as
initiated
in
[],
whic
h
sho
ws
that
the
P
vs
NP
question
has
con
tradicting
relativizations.
This
came
as
a
surprise
since
in
recursiv
e
function
theory
,
the
origin
of
the
relativization
paradigm,
all
standard
results
do
hold
in
all
relativized
w
orlds.
Th
us,
relativization
w
as
tak
en
as
an
indication
that
the
main
tec
hnique
of
recursiv
e
function
theory
{

..
CONCLUSIONS

that
is
diagonalization
{
can
not
settle
questions
suc
h
as
P
vs
NP
.

The
\curse
of
con
tradicting
relativizations"
w
as
eliminated
in
[]
that
considers
the
measure
of
oracles
(or
relativized
w
orlds)
for
whic
h
certain
relations
hold.
The
authors
also
p
ostulated
the
Random
Oracle
Hyp
othesis
that
essen
tially
states
that
structural
relationships
whic
h
hold
in
almost
all
oracle
w
orlds
also
hold
in
the
unrelativized
case
(i.e.,
the
real
w
orld).
Sev
eral
refutations
of
the
Random
Oracle
Hyp
othesis
are
kno
wn:
F
or
example,
co
N
P

I
P
(see
Lecture

or
[]),
whereas
this
fails
in
almost
all
oracle
w
orlds
(see
[]).
A
dieren
t
approac
h
to
w
ards
in
v
estigation
of
the
limitation
of
certain
pro
ofs
w
as
tak
en
in
[].
The
latter
w
ork
denes
a
framew
ork
for
pro
ving
circuit
size
lo
w
er
b
ounds
and
certain
features,
called
natur
al,
of
pro
ofs
within
this
framew
ork.
It
then
sho
ws
that
the
existence
of
natural
pro
ofs
con
tradicts
the
existence
of
corresp
onding
pseudorandom
functions,
whic
h
in
turn
implies
that
the
existence
of
pseudorandom
functions
cannot
b
e
pro
v
en
via
natural
pro
ofs.

.
T.
Bak
er,
J.
Gill,
and
R.
Solo
v
a
y
.
Relativizations
of
the
P
=?
NP
question.
SIAM
Journal
on
Computing,
V
ol.

(),
pages
{,
Decem
b
er
	.
.
C.
Bennett
and
J.
Gill.
Relativ
e
to
a
random
oracle
A,
P
A
=
N
P
A
=
co
N
P
A
with
probabilit
y
.
SIAM
Journal
on
Computing,
V
ol.
0
(),
pages
	{,
F
ebruary
	.
.
R.
Chang,
B.
Chor,
O.
Goldreic
h,
J.
Hartmanis,
J.
H

astad,
D.
Ranjan,
and
P
.
Rohatgi.
The
Random
Oracle
Hyp
othesis
is
F
alse.
JCSS,
V
ol.
	
(),
pages
{	,
		.
.
C.
Lund,
L.
F
ortno
w,
H.
Karlo,
and
N.
Nisan.
Algebraic
Metho
ds
for
In
teractiv
e
Pro
of
Systems.
JA
CM,
V
ol.
	
(),
pages
	{,
		.
.
A.R.
Razb
oro
v
and
S.
Rudic
h.
Natural
Pro
ofs.
JCSS,
V
ol.

(),
pages
{,
		.

W
e
consider
the
latter
sen
tence
to
b
e
on
the
v
erge
on
non-sense:
It
is
not
clear
what
is
mean
t
b
y
sa
ying
that
a
pro
of
tec
hnique
cannot
settle
a
certain
question.
In
con
trast,
sa
ying
that
a
pro
of
with
certain
w
ell-dened
prop
erties
cannot
settle
questions
of
certain
prop
erties
ma
y
b
e
clear
and
useful.
That
is,
whereas
the
notion
of
a
pro
of
tec
hnique
(i.e.,
a
pro
of
whic
h
uses
a
certain
tec
hnique)
is
undened,
one
ma
y
dene
prop
erties
of
pro
ofs.
An
inspiring
case
in
whic
h
the
latter
w
as
done
is
the
form
ulation
and
study
of
Natural
Pro
ofs
[].

Our
original
in
ten
tion
w
as
to
discuss
also
Natural
Pro
ofs
in
the
lecture.
Ho
w
ev
er,
since
the
studen
ts
did
not
see
circuit
size
lo
w
er
b
ounds
in
this
course,
the
former
plan
w
as
abandoned.


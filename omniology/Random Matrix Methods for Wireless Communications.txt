
This page intentionally left blank

Random Matrix Methods for Wireless Communications
Blending theoretical results with practical applications, this book provides an
introduction to random matrix theory and shows how it can be used to tackle a variety of
problems in wireless communications. The Stieltjes transform method, free probabil-
ity theory, combinatoric approaches, deterministic equivalents, and spectral analysis
methods for statistical inference are all covered from a unique engineering perspective.
Detailed mathematical derivations are presented throughout, with thorough explana-
tions of the key results and all fundamental lemmas required for the readers to derive
similar calculus on their own. These core theoretical concepts are then applied to a wide
range of real-world problems in signal processing and wireless communications, includ-
ing performance analysis of CDMA, MIMO, and multi-cell networks, as well as signal
detection and estimation in cognitive radio networks. The rigorous yet intuitive style
helps demonstrate to students and researchers alike how to choose the correct approach
for obtaining mathematically accurate results.
Romain Couillet is an Assistant Professor at the Chair on System Sciences and the
Energy Challenge at Sup´elec, France. Previously he was an Algorithm Development
Engineer for ST-Ericsson, and he received his PhD from Sup´elec in 2010.
M´erouane Debbah is a Professor at Sup´elec, where he holds the Alcatel-Lucent Chair
on Flexible Radio. He is the recipient of several awards, including the 2007 General
Symposium IEEE Globecom best paper award and the Wi-Opt 2009 best paper award.


Random Matrix Methods for
Wireless Communications
Romain Couillet and M ´erouane Debbah
´Ecole Sup´erieure d’´Electricit´e, Gif sur Yvette, France

C A M B R I D G E U N I V E R S I T Y P R E S S
Cambridge, New York, Melbourne, Madrid, Cape Town,
Singapore, S˜ao Paulo, Delhi, Tokyo, Mexico City
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9781107011632
c⃝Cambridge University Press 2011
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2011
Printed in the United Kingdom at the University Press, Cambridge
A catalogue record for this publication is available from the British Library
Library of Congress Cataloguing in Publication data
Couillet, Romain, 1983–
Random matrix methods for wireless communications / Romain Couillet, Merouane Debbah.
p.
cm.
Includes bibliographical references and index.
ISBN 978-1-107-01163-2 (hardback)
1. Wireless communication systems – Mathematics.
2. Matrix analytic methods.
I. Debbah, Merouane, 1975–
II. Title.
TK5103.2.C68 2011
621.38401′51–dc23
2011013189
ISBN 978-1-107-01163-2 Hardback
Cambridge University Press has no responsibility for the persistence or
accuracy of URLs for external or third-party internet websites referred to
in this publication, and does not guarantee that any content on such
websites is, or will remain, accurate or appropriate.

v
To my family,
– Romain Couillet
To my parents,
– M´erouane Debbah


Contents
Preface
page xiii
Acknowledgments
xv
Acronyms
xvi
Notation
xviii
1
Introduction
1
1.1
Motivation
1
1.2
History and book outline
6
Part I Theoretical aspects
15
2
Random matrices
17
2.1
Small dimensional random matrices
17
2.1.1
Deﬁnitions and notations
17
2.1.2
Wishart matrices
19
2.2
Large dimensional random matrices
29
2.2.1
Why go to inﬁnity?
29
2.2.2
Limit spectral distributions
30
3
The Stieltjes transform method
35
3.1
Deﬁnitions and overview
35
3.2
The Mar˘cenko–Pastur law
42
3.2.1
Proof of the Mar˘cenko–Pastur law
44
3.2.2
Truncation, centralization, and rescaling
54
3.3
Stieltjes transform for advanced models
57
3.4
Tonelli theorem
61
3.5
Central limit theorems
63
4
Free probability theory
71
4.1
Introduction to free probability theory
72
4.2
R- and S-transforms
75
4.3
Free probability and random matrices
77
4.4
Free probability for Gaussian matrices
84

viii
Contents
4.5
Free probability for Haar matrices
87
5
Combinatoric approaches
95
5.1
The method of moments
95
5.2
Free moments and cumulants
98
5.3
Generalization to more structured matrices
105
5.4
Free moments in small dimensional matrices
108
5.5
Rectangular free probability
109
5.6
Methodology
111
6
Deterministic equivalents
113
6.1
Introduction to deterministic equivalents
113
6.2
Techniques for deterministic equivalents
115
6.2.1
Bai and Silverstein method
115
6.2.2
Gaussian method
139
6.2.3
Information plus noise models
145
6.2.4
Models involving Haar matrices
153
6.3
A central limit theorem
175
7
Spectrum analysis
179
7.1
Sample covariance matrix
180
7.1.1
No eigenvalues outside the support
180
7.1.2
Exact spectrum separation
183
7.1.3
Asymptotic spectrum analysis
186
7.2
Information plus noise model
192
7.2.1
Exact separation
192
7.2.2
Asymptotic spectrum analysis
195
8
Eigen-inference
199
8.1
G-estimation
199
8.1.1
Girko G-estimators
199
8.1.2
G-estimation of population eigenvalues and eigenvectors
201
8.1.3
Central limit for G-estimators
213
8.2
Moment deconvolution approach
218
9
Extreme eigenvalues
223
9.1
Spiked models
223
9.1.1
Perturbed sample covariance matrix
224
9.1.2
Perturbed random matrices with invariance properties
228
9.2
Distribution of extreme eigenvalues
230
9.2.1
Introduction to the method of orthogonal polynomials
230
9.2.2
Limiting laws of the extreme eigenvalues
233
9.3
Random matrix theory and eigenvectors
237

Contents
ix
10
Summary and partial conclusions
243
Part II Applications to wireless communications
249
11
Introduction to applications in telecommunications
251
11.1 Historical account of major results
251
11.1.1 Rate performance of multi-dimensional systems
252
11.1.2 Detection and estimation in large dimensional systems
256
11.1.3 Random matrices and ﬂexible radio
259
12
System performance of CDMA technologies
263
12.1 Introduction
263
12.2 Performance of random CDMA technologies
264
12.2.1 Random CDMA in uplink frequency ﬂat channels
264
12.2.2 Random CDMA in uplink frequency selective channels
273
12.2.3 Random CDMA in downlink frequency selective channels
281
12.3 Performance of orthogonal CDMA technologies
284
12.3.1 Orthogonal CDMA in uplink frequency ﬂat channels
285
12.3.2 Orthogonal CDMA in uplink frequency selective channels
285
12.3.3 Orthogonal CDMA in downlink frequency selective channels
286
13
Performance of multiple antenna systems
293
13.1 Quasi-static MIMO fading channels
293
13.2 Time-varying Rayleigh channels
295
13.2.1 Small dimensional analysis
296
13.2.2 Large dimensional analysis
297
13.2.3 Outage capacity
298
13.3 Correlated frequency ﬂat fading channels
300
13.3.1 Communication in strongly correlated channels
305
13.3.2 Ergodic capacity in strongly correlated channels
309
13.3.3 Ergodic capacity in weakly correlated channels
311
13.3.4 Capacity maximizing precoder
312
13.4 Rician ﬂat fading channels
316
13.4.1 Quasi-static mutual information and ergodic capacity
316
13.4.2 Capacity maximizing power allocation
318
13.4.3 Outage mutual information
320
13.5 Frequency selective channels
322
13.5.1 Ergodic capacity
324
13.5.2 Capacity maximizing power allocation
325
13.6 Transceiver design
328
13.6.1 Channel matrix model with i.i.d. entries
331
13.6.2 Channel matrix model with generalized variance proﬁle
332

x
Contents
14
Rate performance in multiple access and broadcast channels
335
14.1 Broadcast channels with linear precoders
336
14.1.1 System model
339
14.1.2 Deterministic equivalent of the SINR
341
14.1.3 Optimal regularized zero-forcing precoding
348
14.1.4 Zero-forcing precoding
349
14.1.5 Applications
353
14.2 Rate region of MIMO multiple access channels
355
14.2.1 MAC rate region in quasi-static channels
357
14.2.2 Ergodic MAC rate region
360
14.2.3 Multi-user uplink sum rate capacity
364
15
Performance of multi-cellular and relay networks
369
15.1 Performance of multi-cell networks
369
15.1.1 Two-cell network
373
15.1.2 Wyner model
376
15.2 Multi-hop communications
378
15.2.1 Multi-hop model
379
15.2.2 Mutual information
382
15.2.3 Large dimensional analysis
382
15.2.4 Optimal transmission strategy
388
16
Detection
393
16.1 Cognitive radios and sensor networks
393
16.2 System model
396
16.3 Neyman–Pearson criterion
399
16.3.1 Known signal and noise variances
400
16.3.2 Unknown signal and noise variances
406
16.3.3 Unknown number of sources
407
16.4 Alternative signal sensing approaches
412
16.4.1 Condition number method
413
16.4.2 Generalized likelihood ratio test
414
16.4.3 Test power and error exponents
416
17
Estimation
421
17.1 Directions of arrival
422
17.1.1 System model
422
17.1.2 The MUSIC approach
423
17.1.3 Large dimensional eigen-inference
425
17.1.4 The correlated signal case
429
17.2 Blind multi-source localization
432
17.2.1 System model
434
17.2.2 Small dimensional inference
436

Contents
xi
17.2.3 Conventional large dimensional approach
438
17.2.4 Free deconvolution approach
440
17.2.5 Analytic method
447
17.2.6 Joint estimation of number of users, antennas and powers
469
17.2.7 Performance analysis
471
18
System modeling
477
18.1 Introduction to Bayesian channel modeling
478
18.2 Channel modeling under environmental uncertainty
480
18.2.1 Channel energy constraints
481
18.2.2 Spatial correlation models
484
19
Perspectives
501
19.1 From asymptotic results to ﬁnite dimensional studies
501
19.2 The replica method
505
19.3 Towards time-varying random matrices
506
20
Conclusion
511
References
515
Index
537


Preface
More than sixty years have passed since the 1948 landmark paper of Shannon
providing the capacity of a single antenna point-to-point communication channel.
The method was based on information theory and led to a revolution in the ﬁeld,
especially on how communication systems were designed. The tools then showed
their limits when we wanted to extend the analysis and design to the multi-
terminal multiple antenna case, which is the basis of the wireless revolution
since the nineties. Indeed, in the design of these networks, engineers frequently
stumble on the scalability problem. In other words, as the number of nodes or
bandwidth increase, problems become harder to solve and the determination of
the precise achievable rate region becomes an intractable problem. Moreover,
engineering insight progressively disappears and we can only rely on heavy
simulations with all their caveats and limitations. However, when the system
is suﬃciently large, we may hope that a macroscopic view could provide a more
useful abstraction of the network. The properties of the new macroscopic model
nonetheless need to account for microscopic considerations, e.g. fading, mobility,
etc. We may then sacriﬁce some structural details of the microscopic view but the
macroscopic view will preserve suﬃcient information to allow for a meaningful
network optimization solution and the derivation of insightful results in a wide
range of settings.
Recently, a number of research groups around the world have taken
this approach and have shown how tools borrowed from physical and
mathematical frameworks, e.g. percolation theory, continuum models, game
theory, electrostatics, mean ﬁeld theory, stochastic geometry, just to name a
few, can capture most of the complexity of dense random networks in order to
unveil some relevant features on network-wide behavior.
The following book falls within this trend and aims to provide a comprehensive
understanding on how random matrix theory can model the complexity of the
interaction between wireless devices. It has been more than ﬁfteen years since
random matrix theory was successfully introduced into the ﬁeld of wireless
communications to analyze CDMA and MIMO systems. One of the useful
features, especially of the large dimensional random matrix theory approach,
is its ability to predict, under certain conditions, the behavior of the empirical
eigenvalue distribution of products and sums of matrices. The results are striking
in terms of accuracy compared to simulations with reasonable matrix sizes, and

xiv
Preface
the theory has been shown to be an eﬃcient tool to predict the behavior of
wireless systems with only few meaningful parameters. Random matrix theory
is also increasingly making its way into the statistical signal processing ﬁeld
with the generalization of detection and inference methods, e.g. array processing,
hypothesis tests, parameter estimation, etc., to the multi-variate case. This comes
as a small revolution in modern signal processing as legacy estimators, such as the
MUSIC method, become increasingly obsolete and unadapted to large sensing
arrays with few observations.
The authors are conﬁdent and have no doubt on the usefulness of the tool for
the engineering community in the upcoming years, especially as networks become
denser. They also think that random matrix theory should become sooner or later
a major tool for electrical engineers, taught at the graduate level in universities.
Indeed, engineering education programs of the twentieth century were mostly
focused on the Fourier transform theory due to the omnipresence of frequency
spectrum. The twenty-ﬁrst century engineers know by now that space is the next
frontier due to the omnipresence of spatial spectrum modes, which refocuses the
programs towards a Stieltjes transform theory.
We sincerely hope that this book will inspire students, teachers, and engineers,
and answer their present and future problems.
Romain Couillet and M´erouane Debbah

Acknowledgments
This book is the fruit of many years of the authors’ involvement in the ﬁeld
of random matrix theory for wireless communications. This topic, which has
gained increasing interest in the last decade, was brought to light in the
telecommunication community in particular through the work of Stephen Hanly,
Ralf M¨uller, Shlomo Shamai, Emre Telatar, David Tse, Antonia Tulino, and
Sergio Verd´u, among others. It then rapidly grew into a joint research framework
gathering both telecommunication engineers and mathematicians, among which
Zhidong Bai, Vyacheslav L. Girko, Leonid Pastur, and Jack W. Silverstein.
The authors are especially indebted to Prof. Silverstein for the agreeable
time spent discussing random matrix matters. Prof. Silverstein has a very
insightful approach to random matrices, which it was a delight to share with
him. The general point of view taken in this book is mostly inﬂuenced by Prof.
Silverstein’s methodology. The authors are also grateful to the many colleagues
working in this ﬁeld whose knowledge and wisdom about applied random matrix
theory contributed signiﬁcantly to its current popularity and elegance. This book
gathers many of their results and intends above all to deliver to the readers this
simpliﬁed approach to applied random matrix theory. The colleagues involved in
long and exciting discussions as well as collaborative works are Florent Benaych-
Georges, Pascal Bianchi, Laura Cottatellucci, Maxime Guillaud, Walid Hachem,
Philippe Loubaton, Myl`ene Ma¨ıda, Xavier Mestre, Aris Moustakas, Ralf M¨uller,
Jamal Najim, and Øyvind Ryan.
Regarding the book manuscript itself, the authors would also like to sincerely
thank the anonymous reviewers for their wise comments which contributed to
improve substantially the overall quality of the ﬁnal book and more importantly
the few people who dedicated a long time to thoroughly review the successive
drafts and who often came up with inspiring remarks. Among the latter are
David Gregoratti, Jakob Hoydis, Xavier Mestre, and Sebastian Wagner.
The success of this book relies in a large part on these people.
Romain Couillet and M´erouane Debbah

Acronyms
AWGN
additive white Gaussian noise
BC
broadcast channel
BPSK
binary pulse shift keying
CDMA
code division multiple access
CI
channel inversion
CSI
channel state information
CSIR
channel state information at receiver
CSIT
channel state information at transmitter
d.f.
distribution function
DPC
dirty paper coding
e.s.d.
empirical spectral distribution
FAR
false alarm rate
GLRT
generalized likelihood ratio test
GOE
Gaussian orthogonal ensemble
GSE
Gaussian symplectic ensemble
GUE
Gaussian unitary ensemble
i.i.d.
independent and identically distributed
l.s.d.
limit spectral distribution
MAC
multiple access channel
MF
matched-ﬁlter
MIMO
multiple input multiple output
MISO
multiple input single output
ML
maximum likelihood
LMMSE
linear minimum mean square error
MMSE
minimum mean square error
MMSE-SIC
MMSE and successive interference cancellation
MSE
mean square error
MUSIC
multiple signal classiﬁcation
NMSE
normalized mean square error
OFDM
orthogonal frequency division multiplexing

Acronyms
xvii
OFDMA
orthogonal frequency division multiple access
p.d.f.
probability density function
QAM
quadrature amplitude modulation
QPSK
quadrature pulse shift keying
ROC
receiver operating characteristic
RZF
regularized zero-forcing
SINR
signal-to-interference plus noise ratio
SISO
single input single output
SNR
signal-to-noise ratio
TDMA
time division multiple access
ZF
zero-forcing

Notation
Linear algebra
X
Matrix
IN
Identity matrix of size N × N
Xij
Entry (i, j) of matrix X (unless otherwise stated)
(X)ij
Entry (i, j) of matrix X
[X]ij
Entry (i, j) of matrix X
{f(i, j)}i,j
Matrix with (i, j) entry f(i, j)
(Xij)i,j
Matrix with (i, j) entry Xij
x
Vector (column by default)
x∗
Vector of the complex conjugates of the entries of x
xi
Entry i of vector x
F X
Empirical spectral distribution of the Hermitian X
XT
Transpose of X
XH
Hermitian transpose of X
tr X
Trace of X
det X
Determinant of X
rank(X)
Rank of X
∆(X)
Vandermonde determinant of X
∥X∥
Spectral norm of the Hermitian matrix X
diag(x1, . . . , xn)
Diagonal matrix with (i, i) entry xi
ker(A)
Null space of the matrix A, ker(A) = {x, Ax = 0}
span(A)
Subspace generated by the columns of the matrix A
Real and complex analysis
N
The space of natural numbers
R
The space of real numbers
C
The space of complex numbers
A∗
The space A \ {0}
x+
Right-limit of the real x
x−
Left-limit of the real x

Notation
xix
(x)+
For x ∈R, max(x, 0)
sgn(x)
Sign of the real x
ℜ[z]
Real part of z
ℑ[z]
Imaginary part of z
z∗
Complex conjugate of z
i
Square root of −1 with positive imaginary part
f ′(x)
First derivative of the function f
f ′′(x)
Second derivative of the function f
f ′′′(x)
Third derivative of the function f
f (p)(x)
Derivative of order p of the function f
∥f∥
Norm of a function ∥f∥= supx |f(x)|
1A(x)
Indicator function of the set A
1A(x) = 1 if x ∈A, 1A(x) = 0 otherwise
δ(x)
Dirac delta function, δ(x) = 1{0}(x)
∆(x|A)
Convex indicator function
∆(x|A) = 0 if x ∈A, ∆(x|A) = ∞otherwise
Supp(F)
Support of the distribution function F
x1, x2, . . .
Series of general term xn
xn →ℓ
Simple convergence of the series x1, x2, . . . to ℓ
xn = o(yn)
Upon existence, xn/yn →0 as n →∞
xn = O(yn)
There exists K, such that xn ≤Kyn for all n
n/N →c
As n →∞and N →∞, n/N →c
W(z)
Lambert-W function satisfying W(z)eW(z) = z
Ai(x)
Airy function
Γ(x)
Gamma function, Γ(n) = (n −1)! for n integer
Probability theory
(Ω, F, P)
Probability space Ωwith σ-ﬁeld F and measure P
PX(x)
Density of the random variable X
pX(x)
Density of the scalar random variable X
P(Xi)(x)
Unordered density of the random variable X1, . . . , XN
P ≥
(Xi)(x)
Ordered density of the random variable X1 ≥. . . ≥XN
P ≤
(Xi)(x)
Ordered density of the random variable X1 ≤. . . ≤XN
µX
Probability measure of X, µX(A) = P(X(A))
µX
Probability distribution of the eigenvalues of X
µ∞
X
Probability distribution associated with the l.s.d. of X
PX(x)
Density of the random variable X, PX(x)dx = µX(dx)
FX(x)
Distribution function of X (real), FX(x) = µX((−∞, x])
E[X]
Expectation of X, E[X] =
R
ΩX(ω)dω

xx
Notation
E[f(X)]
Expectation of f(X), E[f(X)] =
R
Ωf(X(ω))dω
var(X)
Variance of X, var(X) = E[X2] −E[X]2
X ∼L
X is a random variable with density L
N(µ, Σ)
Real Gaussian distribution of mean µ and covariance Σ
CN(µ, Σ)
Complex Gaussian distribution of mean µ and covariance Σ
WN(n, R)
Real zero mean Wishart distribution with n degrees of freedom
and covariance R
CWN(n, R)
Complex zero mean Wishart distribution with n degrees of freedom
and covariance R
Q(x)
Gaussian Q-function, Q(x) = P(X > x), X ∼N(0, 1)
F +
Tracy–Widom distribution function
F −
Conjugate Tracy–Widom d.f., F −(x) = 1 −F +(−x)
xn
a.s.
−→ℓ
Almost sure convergence of the series x1, x2, . . . to ℓ
Fn ⇒F
Weak convergence of the d.f. series F1, F2, . . . to F
Xn ⇒X
Weak convergence of the series X1, X2, . . . to the random X
Random Matrix Theory
mF (z)
Stieltjes transform of the function F
mX(z)
Stieltjes transform of the eigenvalue distribution of X
VF (z)
Shannon transform of the function F
VX(z)
Shannon transform of the eigenvalue distribution of X
RF (z)
R transform of the function F
RX(z)
R transform of the eigenvalue distribution of X
SF (z)
S transform of the function F
SX(z)
S transform of the eigenvalue distribution of X
ηF (z)
η-transform of the function F
ηX(z)
η-transform of the eigenvalue distribution of X
ψF (z)
ψ-transform of the function F
ψX(z)
ψ-transform of the eigenvalue distribution of X
µ ⊞ν
Additive free convolution of µ and ν
µ ⊟ν
Additive free deconvolution of µ and ν
µ ⊠ν
Multiplicative free convolution of µ and ν
µ  ν
Multiplicative free deconvolution of µ and ν
Topology
Ac
Complementary of the set A
#A
Cardinality of the discrete set A
A ⊕B
Direct sum of the spaces A and B
⊕1≤i≤nAi
Direct sum of the spaces Ai, 1 ≤i ≤n

Notation
xxi
⟨x, A⟩
Norm of the orthogonal projection of x on the space A
Miscellaneous
x ≜y
x is deﬁned as y
sgn(σ)
Signature (or parity) of the permutation σ, sgn(σ) ∈{−1, 1}


1
Introduction
1.1
Motivation
We initiate the book with a classical example, which exhibits both the non-
obvious behavior of large dimensional random matrices and the motivation
behind their study.
Consider a random sequence x1, . . . , xn of n independent and identically
distributed (i.i.d.) observations of a given random process. The classical law of
large numbers states that the sequence x1, x1+x2
2
, . . ., with nth term 1
n
Pn
k=1 xk
tends almost surely to the deterministic value E[x1], the expectation of this
process, as n tends to inﬁnity. Denote (Ω, F, P) the probability space that
generates the inﬁnite sequence x1, x2, . . .. For a given realization ω ∈Ω, we
will denote x1(ω), x2(ω), . . . the realization of the random sequence x1, x2, . . ..
We recall that almost sure convergence means that there exists A ⊂Ω, with
P(A) = 1, such that, for ω ∈A
1
n
n
X
k=1
xk(ω) →E[x1] ≜
Z
Ω
x1(w)dw.
We also remind brieﬂy that the notation (Ω, F, P) designates the triplet
composed of the space of random realizations Ω, i.e. in our case ω ∈Ωis the
realization of a series x1(ω), x2(ω), . . ., F is a σ-ﬁeld on Ω, which can be seen
as the space of the measurable events on Ω, e.g. the space B = {x1(ω) > 0} ∈F
is such an event, and P is a probability measure on F, i.e. P is a function that
assigns to every event in F a probability.
This law of large numbers is fundamental in the sense that it provides a
deterministic feature for a process ruled by ‘chance’ (or more precisely, ruled by a
deterministic process, the precise nature of which the observer is unaware). This
allows the observer to be able to retrieve deterministic information from random
variables based on any observed random sequence (within a space of probability
one). If, for instance, x1, . . . , xn are successive samples of a stationary zero mean
white noise waveform x(t), i.e. E[x(t)x(t −τ)] = σ2δ(t), it is the usual signal
processing problem to estimate the power σ2 = E[|x1|2] of the noise process; the

2
1. Introduction
empirical variance σ2
n, i.e.
σ2
n = 1
n
n
X
i=1
|xi|2
is a classical estimate of σ2 which, according to the law of large numbers, is such
that σ2
n →σ2 almost surely when n →∞. It is often said that σ2
n is a consistent
estimator of σ2 as it is asymptotically and almost surely equal to σ2. To avoid
confusion with the two-dimensional case treated next, we will say instead that
σ2
n is an n-consistent estimator of σ2, as it is asymptotically accurate as n grows
large. Obviously, we are never provided with an inﬁnitely long observation time
window, so that n is usually large but ﬁnite, and therefore σ2
n is merely an
approximation of σ2.
With the emergence of multiple antenna systems, channel spreading codes,
sensor networks, etc., signal processing problems have become more and more
concerned with vectorial inputs rather than scalar inputs. For a sequence of
n i.i.d. random vectors, the law of large numbers still applies. For instance,
for x1, x2, . . . ∈CN randomly drawn from a given N-variate zero mean random
process
Rn = 1
n
n
X
i=1
xixH
i →R ≜E[x1xH
1 ]
(1.1)
almost surely as n →∞, where the convergence is considered for any matrix
norm, i.e. ∥R −Rn∥→0 on a set of probability one. The matrix Rn is often
referred to as the empirical covariance matrix or as the sample covariance matrix,
as it is computed from observed vector samples. We will use this last phrase
throughout the book. Following the same semantic ﬁeld, the matrix R will be
referred to as the population covariance matrix, as it characterizes the innate
nature of all stochastic vectors xi from the overall population of such vectors.
The empirical Rn is again an n-consistent estimator of R of x1 and, as before, as
n is taken very large for N ﬁxed, Rn is a good approximation of R in the sense of
the aforementioned matrix norm. However, in practical applications, it might be
that the number of available snapshots xk is indeed very large but not extremely
large compared to the vector size N. This situation arises in diverse application
ﬁelds, such as biology, ﬁnance, and, of course, wireless communications. If this is
the case, as will become obvious in the following examples and against intuition,
the diﬀerence ∥R −Rn∥can be far from zero even for large n.
Since the DNA of many organisms have now been entirely sequenced, biologists
and evolutionary biologists are interested in the correlations between genes, e.g.:
How does the presence of a given gene (or gene sequence) in an organism impact
the probability of the presence of another given gene? Does the activation of a
given gene come along with the activation of several other genes? To be able
to study the joint correlation between a large population of the several ten
thousands of human genes, call this number N, we need a large sample of genome

1.1. Motivation
3
sequences extracted from human beings, call the number of such samples n. It is
therefore typical that the N × n matrix of the n gene sequence samples does not
have many more columns than rows, or, worse, may even have more rows than
columns. We see already that, in this case, the sample covariance matrix Rn is
necessarily rank-deﬁcient (of maximum rank N −n), while R has all the chances
to be full rank. Therefore, Rn is obviously no longer a good approximation of
R, even if n were very large in the ﬁrst place, since the eigenvalues of Rn and
R diﬀer by at least N −n terms.
In the ﬁeld of ﬁnance, the interest of statisticians lies in the interactions
between assets in the market and the joint time evolution of their stock market
indices. The vectors x1, . . . , xn here may be representative of n months of market
index evolution of N diﬀerent brands of a given product, say soda, the ith entry of
the column vector xk being the evolution of the market index of soda i in month
k. Obviously, this case diﬀers from the independent vector case presented up to
now since the evolution at month k + 1 is somewhat correlated to the evolution
at previous month k, but let us assume for simplicity that the month evolution
is at least an uncorrelated process (which does not imply independence). Similar
to the gene case for biologists, it often turns out that the N × n matrix under
study contains few columns compared to the number of rows, although both
dimensions are typically large compared to 1. Of speciﬁc importance to traders
is the largest eigenvalue of the population covariance matrix R of (a centered
and normalized version of) the random process x1, which is an indicator of the
maximal risk against investment returns taken by a trader who constitutes a
portfolio from these assets. From the biology example above, it has become clear
that the eigenvalues of Rn may be a very inaccurate estimate of those of R;
thus, Rn cannot be relied on to estimate the largest eigenvalue and hence the
trading risk. The case of wireless communications will be thoroughly detailed in
Part II, and a ﬁrst motivation is given in the next paragraph.
Returning to the initial sample covariance matrix model, we have already
mentioned that in the scalar case the strong law of large numbers ensures that
it suﬃces for n to be quite large compared to 1 for σ2
n to be a good estimator
for σ2. In the case where data samples are vectors, if n is large compared to 1,
whatever N, then the (i, j) entry Rn,ij of Rn is a good estimator of the (i, j)
entry Rij of R. This might (mis)lead us to assume that as n is much greater
than one, Rn ≃R in some sense. However, if both N and n are large compared
to 1 but n is not large compared to N, then the peculiar thing happens: the
eigenvalue distribution of Rn (see this as an histogram of the eigenvalues) in
general converges, but does not converge to the eigenvalue distribution of R.
This has already been pointed out in the degenerated case N > n, for which
Rn has N −n null eigenvalues, while R could be of full rank. This behavior is
evidenced in Figure 1.1 in which we consider x1 ∼CN(0, IN) and then R = IN,
for N = 500, n = 2000. In that case, notice that Rn converges point-wise to IN

4
1. Introduction
0
0.5
1
1.5
2
2.5
3
zero
0.2
0.4
0.6
0.8
Eigenvalues of Rn
Density
Empirical eigenvalues
Mar˘cenko–Pastur law
Figure 1.1 Histogram of the eigenvalues of Rn = 1
n
Pn
k=1 xkxH
k, xk ∈CN, for
n = 2000, N = 500.
when n is large, as Rn,ij, the entry (i, j) of Rn, is given by:
Rn,ij = 1
n
n
X
k=1
xikx∗
jk
which is close to one if i = j and close to zero if i ̸= j. This is obviously
irrespective of N, which is not involved in the calculus here. However, the
eigenvalues of Rn do not converge to a single mass in 1 but are spread around 1.
This apparent contradiction is due to the fact that N grows along with n but n/N
is never large. We say in that case that, while Rn is an n-consistent estimator
of R, it is not an (n, N)-consistent estimator of R. The seemingly paradoxical
behavior of the eigenvalues of Rn, while Rn converges point-wise to IN, lies in
fact in the rate convergence of the entries of Rn towards the entries of IN. Due
to central limit arguments for the sample mean of scalar i.i.d. random variables,
Rn,ij −E[Rn,ij] is of order O(1/√n). When determining the eigenvalues of Rn,
the deviations around the means are negligible when n is large and N ﬁxed.
However, for N and n both large, these residual deviations of the entries of Rn
(their number is N 2) are no longer negligible and the eigenvalue distribution of
Rn is not a single mass in 1. In some sense, we can see Rn as a matrix close to
the identity but whose entries all contain some small residual “energy,” which
becomes relevant as much of such small energy is cumulated.
This observation has very important consequences, which motivate the need
for singling out the study of large empirical covariance matrices and more
generally of large random Hermitian matrices as a unique ﬁeld of mathematics.
Wireless communications may be the one research ﬁeld in which large matrices
have started to play a fundamental role. Indeed, current and more importantly

1.1. Motivation
5
future wireless communication systems are multi-dimensional in several respects
(spatial with antennas, temporal with random codes, cellular-wise with large
number of users, multiple cooperative network nodes, etc.) and random in other
respects (time-varying fading channels, noisy communications, etc.). The study of
the behavior of large wireless communication systems therefore calls for advanced
mathematical tools that can easily deal with large dimensional random matrices.
Consider for instance a multiple input multiple output (MIMO) complex channel
matrix H ∈CN×n between an n-antenna transmitter and an N-antenna receiver,
the entries of which are independent and complex Gaussian with zero mean and
variance 1/n. If uniform power allocation across the antennas is used at the
transmit antenna array and the additive channel noise is white and Gaussian,
the achievable transmission rates over this channel are all rates less than the
channel mutual information
I(σ2) = E

log2 det

IN + 1
σ2 HHH

(1.2)
where σ−2 denotes now the signal-to-noise ratio (SNR) at the receiver and the
expectation is taken over the realizations of the random channel H, varying
according to the Gaussian distribution. Now note that HHH = Pn
i=1 hihH
i
with hi ∈CN the ith column of H, h1, . . . , hn being i.i.d. random vectors.
The matrix HHH can then be seen as the sample covariance matrix of some
hypothetical random N-variate variable √nh1. From our previous discussion,
denoting HHH = UΛUH the spectral decomposition of HHH, we have:
I(σ2) = E

log2 det

IN + 1
σ2 Λ

= E
" N
X
i=1
log2

1 + λi
σ2
#
(1.3)
with λ1, . . . , λN the eigenvalues of HHH, which again are not all close to one,
even for n and N large. The achievable transmission rates are then explicitly
dependent on the eigenvalue distribution of HHH. More generally, it will be
shown in Chapters 12–15 that random matrix theory provides a powerful
framework, with multiple methods, to analyze the achievable transmission rates
and rate regions of a large range of multi-dimensional setups (MIMO, CDMA,
multi-user transmissions, MAC/BC channels, etc.) and to derive the capacity-
achieving signal covariance matrices for some of these systems, i.e. determine
the non-negative deﬁnite matrix P ∈CN×N, which, under some trace constraint
tr P ≤P, maximizes the expression
I(σ2; P) = E

log det

IN + 1
σ2 HPHH

for numerous fading channel models for H.

6
1. Introduction
1.2
History and book outline
The present book is divided into two parts: a ﬁrst part on the theoretical
fundamentals of random matrix theory, and a second part on the applications
of random matrix theory to the ﬁeld of wireless communications. The ﬁrst part
will give a rather broad, although not exhaustive, overview of fundamental and
recent results concerning random matrices. However, the main purpose of this
part goes beyond a listing of important theorems. Instead, it aims on the one
hand at providing the reader with a large, yet incomplete, range of techniques
to handle problems dealing with random matrices, and on the other hand at
developing sketches of proofs of the most important results in order to provide
further intuition to the reader. Part II will be more practical as it will apply most
of the results derived in Part I to problems in wireless communications, such
as system performance analysis, signal sensing, parameter estimation, receiver
design, channel modeling, etc. Every application will be commented on with
regard to the theoretical results developed in Part I, for the reader to have a
clear understanding of the reasons why the practical results hold, of their main
limitations, and of the questions left open. Before moving on to Part I, in the
following we introduce in detail the objectives of both parts through a brief
historical account of eighty years of random matrix theory.
The origin of the study of random matrices is usually said to date back
to 1928 with the pioneering work of the statistician John Wishart [Wishart,
1928]. Wishart was interested in the behavior of sample covariance matrices of
i.i.d. random vector processes x1, . . . , xn ∈CN, in the form of the matrix Rn
previously introduced
Rn = 1
n
n
X
i=1
xixH
i .
(1.4)
Wishart provided an expression of the joint probability distribution of
the entries of such a matrix when its column vector entries are themselves
independent and have an identical standard complex Gaussian distribution, i.e.
xij ∼CN(0, 1). These normalized matrices with i.i.d. standard Gaussian entries
are now called Wishart matrices. Wishart matrices were thereafter generalized
and extensively studied. Today there exists in fact a large pool of properties
on the joint distribution of the eigenvalues, the distribution of the extremes
eigenvalues, the distribution of the ratios between extreme eigenvalues, etc.
The ﬁrst asymptotic considerations, i.e. the ﬁrst results on matrices of
asymptotically large dimensions, appeared with the work of the physician Eugene
Wigner [Wigner, 1955] on nuclear physics, who considered (properly scaled)
symmetric matrices with independent entries uniformly distributed in {1, −1}
and proved the convergence of the marginal probability distribution of its
eigenvalues towards the deterministic semi-circle law, as the dimension of the
matrix grows to inﬁnity. Hermitian n × n matrices with independent upper-

1.2. History and book outline
7
−3
−2
−1
0
1
2
3
zero
0.1
0.2
0.3
0.4
Eigenvalues
Density
Empirical eigenvalue distribution
Semi-circle law
Figure 1.2 Histogram of the eigenvalues of a Wigner matrix and the semi-circle law,
for n = 500.
triangular entries of zero mean and variance 1/n are now referred to as Wigner
matrices. The empirical eigenvalues of a large Wigner matrix and the semi-
circle law are illustrated in Figure 1.2, for a matrix of size n = 500. From
this time on, inﬁnite size random matrices have drawn increasing attention
in many domains of physics [Mehta, 2004] (nuclear physics [Dyson, 1962a],
statistical mechanics, etc.), ﬁnance [Laloux et al., 2000], evolutionary biology
[Arnold et al., 1994], etc. The ﬁrst accounts of work on large dimensional random
matrices for wireless communications are attributed to Tse and Hanly [Tse and
Hanly, 1999] on the performance of large multi-user linear receivers, Verd´u and
Shamai [Verd´u and Shamai, 1999] on the capacity of code division multiple
access (CDMA) systems, among others. The pioneering work of Telatar [Telatar,
1995] on the transmission rates achievable with multiple antennas, paralleled by
Foschini [Foschini and Gans, 1998], is on the contrary a particular example of
the use of small dimensional random matrices for capacity considerations. In
its ﬁnal version of 1999, the article also mentions asymptotic laws for capacity
[Telatar, 1999]. We will see in Chapter 13 that, while Telatar’s original proof
of the capacity growth rate for increasing number of antennas in a multiple
antenna setup is somewhat painstaking, large random matrix theory provides a
straightforward result. In Chapter 2, we will explore some of the aforementioned
results on random matrices of small dimension, which will be shown to be diﬃcult
to manipulate for simply structured matrices and rather intractable to extend
to more structured matrices.
The methods used for random matrix-based calculus are mainly segmented
into: (i) the analytical methods, which treat asymptotic eigenvalue distributions
of large matrices in a comprehensive framework of analytical tools, among

8
1. Introduction
which the important Stieltjes transform, and (ii) the moment-based methods,
which establish results on the successive moments of the asymptotic eigenvalues
probability distribution.1 The analytical framework allows us to solve a large
range of problems in wireless communications such as those related to capacity
evaluation in both random and orthogonal CDMA networks and in large
MIMO systems, but also to address questions such as signal sensing in large
networks or statistical inference, i.e. estimation of network parameters. These
analytic methods are mostly used when the random matrices under consideration
are sample covariance matrices, doubly correlated i.i.d. matrices, information
plus noise matrices (to be deﬁned later), isometric matrices or the sum and
products of such matrices. They are generally preferred over the alternative
moment-based methods since they consider the eigenvalue distribution of large
dimensional random matrices as the central object of study, while the moment
approach is dedicated to the speciﬁc study of the successive moments of the
distribution. Note in particular that not all distributions have moments of
all orders, and for those that do have moments of all orders, not all are
uniquely deﬁned by the series of their moments. However, in some cases of
very structured matrices whose entries are non-trivially correlated, as in the
example of Vandermonde matrices [Ryan and Debbah, 2009], the moment-based
methods convey a more accessible treatment. Both analytical and moment-based
methods are not completely disconnected from one another as they share a
common denominator when it comes to dealing with unitarily invariant random
matrices, such as standard Gaussian or Haar matrices, i.e. unitarily invariant
unitary matrices. This common denominator, namely the ﬁeld of free probability
theory, bridges the analytical tools to the moment-based methods via derivatives
of the Stieltjes transform, the R-transform, and the S-transform. The latter can
be expressed in power series with coeﬃcients intimately linked to moments and
cumulants of the underlying random matrix eigenvalue distributions. The free
probability tool, due to Voiculescu [Voiculescu et al., 1992], was not initially
meant to deal speciﬁcally with random matrices but with more abstract non-
commutative algebras, large dimensional random matrices being a particular case
of such algebras. The extension of classical probability theory to free probability
provides interesting and often surprising results, such as a strong equivalence
between some classical probability distributions, e.g. Poisson, Gaussian, and the
asymptotic probability distribution of the eigenvalues of some random matrix
models, e.g. Wishart matrices and Wigner matrices. Some classical probability
tools, such as the characteristic function, are also extensible through analytic
tools of random matrix theory.
1 Since the terminology method of moments is already dedicated to the speciﬁc technique
which aims at constructing a distribution function from its moments (under the condition
that the moments uniquely determine the distribution), see, e.g. Section 30 of [Billingsley,
1995], we will carefully avoid referring to any random matrix technique based on moments
as the method of moments.

1.2. History and book outline
9
The division between analytical and moment-based methods, with free
probability theory lying in between, can be seen from another point of view.
It will turn out that the analytical methods, and most particularly the Stieltjes
transform approach, take full advantage of the independence between the entries
of large dimensional random matrices. As for moment-based methods, from a free
probability point of view, they take full advantage of the invariance properties
of large dimensional matrices, such as the invariance by the left or right product
with unitary matrices. The theory of orthogonal polynomials follows the same
pattern, as it beneﬁts from the fact that the eigenvalue distribution of unitarily
invariant random matrices can be studied regardless of the (uniform) eigenvector
distribution. In this book, we will see that the Stieltjes transform approach can
solve most problems involving random matrices with invariance properties as
well. This makes this distinction between random matrices with independent
entries and random matrices with invariance properties not so obvious to us.
For this reason, we will keep distinguishing between the analytical approaches
that deal with the eigenvalue distribution as the central object of concern
and the moment-based approaches that are only concerned with successive
moments. We will also brieﬂy introduce the rather old theory of orthogonal
polynomials which has received much interest lately regarding the study of
limiting laws of largest eigenvalues of random matrices but which requires
signiﬁcant additional mathematical eﬀort for proper usage, while applications
to wireless communications are to this day rather limited, although in constant
expansion. We will therefore mostly state the important results from this ﬁeld,
particularly in terms of limit theorems of extreme eigenvalues, see Chapter 9,
without development of the corresponding proofs.
In Chapter 3, Chapter 4, and Chapter 5, we will introduce the analytical
and moment-based methods, as well as notions of free probability theory, which
are fundamental to understand the important concept of asymptotic freeness
for random matrices. We will also provide in these chapters a sketch of the
proof of the convergence of the eigenvalue distribution of the Wishart and
Wigner matrices to the Mar˘cenko–Pastur law, depicted in Figure 1.1, and the
semi-circle law, depicted in Figure 1.2, using the Stieltjes transform and the
method of moments, respectively. Generic methods to determine (almost sure)
limiting distributions of the eigenvalues of large dimensional random matrices,
as well as other functionals of such large matrices (e.g. log determinant), will
be reviewed in detail in these chapters. Chapter 6 will discuss the alternative
methods used when the empirical eigenvalue distribution of large random
matrices do not necessarily converge when the dimensions increase: in that
case, in place of limit distributions, we will introduce the so-called deterministic
equivalents, which provide deterministic approximations of functionals of random
matrices of ﬁnite size. These approximations are (almost surely) asymptotically
accurate as the matrix dimensions grow to inﬁnity, making them consistent
with the methods developed in Chapter 3. In addition to limiting eigenvalue
distributions and deterministic equivalents, in Chapter 3 and Chapter 6, central

10
1. Introduction
limit theorems that extend the convergence theorems to a higher precision
order will be introduced. These central limit theorems constitute a ﬁrst step
into a more thorough analysis of the asymptotic deviations of the spectrum
around its almost sure limit or around its deterministic equivalent. Chapter 7
will discuss advanced results on the spectrum of both the sample covariance
matrix model and the information plus noise model, which have been extensively
studied and for which many results have been provided in the literature, such
as the proof of the asymptotic absence of eigenvalues outside the support
of the limiting distribution. Beyond the purely mathematical convenience of
such a result, being able to characterize where the eigenvalues, and especially
the extreme eigenvalues, are expected to lie is of fundamental importance to
perform hypothesis testing decisions and in statistical inference. In particular,
the characterization of the spectrum of sample covariance matrices will be used
to retrieve information on functionals of the population covariance matrix from
the observed sample covariance matrix, or functionals of the signal space matrix
from the observed information plus noise matrix. Such methods will be referred to
as eigen-inference techniques and are developed in Chapter 8. The ﬁrst part will
then conclude with Chapter 9, which extends the analysis of Section 7.1 to the
expression of the limiting distributions of the extreme eigenvalues. We will also
introduce in this chapter the spiked models, which have recently received a lot of
attention for their many practical implications. These objects are necessary tools
for signal sensing in large dimensional networks, which are currently of major
interest with regard to the recent incentive for cognitive radios. In Chapter 10,
the essential results of Part I will ﬁnally be summarized and rediscussed with
respect to their applications to the ﬁeld of wireless communications.
The second part of this book is dedicated to the application of the diﬀerent
methods described in the ﬁrst chapter to diﬀerent problems in wireless
communications. As already mentioned, the ﬁrst applications of random matrix
theory to wireless communications are exclusively related to asymptotic system
performance analysis, and especially channel capacity considerations. The idea
of considering asymptotically large matrix approximations was initially linked
to studies in CDMA communications, where both the number of users and the
length of the spreading codes are potentially very large [Li et al., 2004; Tse and
Hanly, 1999; Tse and Verd´u, 2000; Tse and Zeitouni, 2000; Zaidel et al., 2001]. It
then occurred to researchers that large matrix approximations work rather well
when the size of the eﬀective matrix under study is not so large, e.g. for matrices
of size 8 × 8 or even 4 × 4 (in the case of random unitary matrices, simulations
suggest that approximations for matrices of size 2 × 2 are even acceptable). This
motivated further studies in systems where the number of relevant parameters
is moderately large. In particular, studies of MIMO communications [Chuah
et al., 2002; Hachem et al., 2008b; Mestre et al., 2003; Moustakas and Simon,
2005; M¨uller, 2002], designs of multi-user receivers [Honig and Xiao, 2001;
M¨uller and Verd´u, 2001], multi-cell communications [Abdallah and Debbah,
2004; Couillet et al., 2011a; Peacock et al., 2008], multiple access channels and

1.2. History and book outline
11
broadcast channels [Couillet et al., 2011a; Wagner et al., 2011] started to be
considered. More recent topics featuring large decentralized systems, such as
game theory-based cognitive radios [Meshkati et al., 2005] and ad-hoc and mesh
networks [Fawaz et al., 2011; Levˆeque and Telatar, 2005] were also treated using
similar random matrix theoretical tools. The initial fundamental reason for the
attractiveness of asymptotic results of random matrix theory for the study of
system performance lies in the intimate link between the Stieltjes transform and
the information-theoretic expression of mutual information. It is only recently,
thanks to a new wave of theoretical results, that the wireless communication
community has realized that many more questions can be addressed than just
mutual information and system performance evaluations.
In Chapter 11, we will start with an introduction to the important results
of random matrix theory for wireless communications and their connections to
the methods detailed in the ﬁrst part of this book. In Chapters 12–15, we will
present the latest results concerning achievable rates for a wide range of wireless
models, mostly taken from the aforementioned references.
From an even more practical, implementation-oriented point of view, we
will also introduce some ideas, rather scattered in the literature, which allow
engineers to reduce the computational burden of transceivers by anticipating
complex calculus and also to reduce the feedback load within communication
networks by beneﬁting from deterministic approximations brought by large
dimensional matrix analysis. The former will be introduced in Chapter 13,
where we discuss means to reduce the computational diﬃculty of implementing
large minimum mean square error (MMSE) CDMA or MIMO receivers, which
require to perform real-time matrix inversions, from basis expansion models.
Besides, it will be seen that in a multiple antenna uplink cellular network, the
ergodic capacity maximizing precoding matrices of all users, computed at the
central base station, can be fed back to the users under the form of partial
information contained within a few bits (the number of which do not scale with
the number of transmit antennas), which are suﬃcient for every user to compute
their own transmit covariance matrix. These ideas contribute to the cognitive
radio incentive for distributed intelligence within large dimensional networks.
Of major interest these days are also the questions of statistical estimation and
detection in large decentralized networks or cognitive radio networks, spurred
especially by the cognitive radio framework, a trendy example of which is the
femto-cell incentive [Calin et al., 2010; Claussen et al., 2008]. Take for example
the problem of radar detection using a large antenna array composed of N
sensors. Each signal arising at the sensor array originates from a ﬁnite number K
of sources positioned at speciﬁc angles with respect to the sensor support. The
population covariance matrix R ∈CN×N of the N-dimensional sample vectors is
composed of K distinct eigenvalues, with multiplicity linked to the size of every
individual detected object, and possibly some null eigenvalues if N is large.
Gathering n successive samples of the N-dimensional vectors, we can construct

12
1. Introduction
a sample covariance matrix Rn as in (1.1). We might then be interested in
retrieving the individual eigenvalues of R, which translate the distance with
respect to each object, or retrieving the multiplicity of every eigenvalue, which
is then an indicator of the size of the detected objects, or even detecting the
angles of arrival of the incoming waveforms, which is a further indication of the
geographical position of the objects. The multiple signal classiﬁcation (MUSIC)
estimator [Schmidt, 1986] has long been considered eﬃcient for such a treatment
and was indeed proved n-consistent. However, with N and n of similar order
of magnitude, i.e. if we wish to increase the number of sensors, the MUSIC
algorithm is largely biased [Mestre, 2008a]. Random matrices considerations,
again based on the Stieltjes transform, were recently used to arrive at alternative
(N, n)-consistent solutions [Karoui, 2008; Mestre, 2008b; Rao et al., 2008].
These methods of eigenvalue and eigenvector retrieval are referred to as eigen-
inference methods. In the case of a cognitive radio network, say a femto-cell
network, every femto-cell, potentially composed of a large number of sensors,
must be capable of discovering its environment, i.e. detecting the presence of
surrounding users in communication, in order to exploit spectrum opportunities,
i.e. unoccupied transmission bandwidth, unused by the surrounding licensed
networks. One of the ﬁrst objectives of a femto-cell is therefore to evaluate
the number of surrounding users and to infer their individual transmit powers.
The study of the power estimation capabilities of femto-cells is performed both
throughout analytical and moment-based methods in [Couillet et al., 2011c; Rao
and Edelman, 2008; Ryan and Debbah, 2007a]. Statistical eigen-inference are also
used to address hypothesis testing problems, such as signal sensing. Consider that
an array of N sensors captures n samples of the incoming waveform and generates
the empirical covariance matrix RN. The question is whether RN indicates the
presence of a signal issued by a source or only indicates the presence of noise.
It is natural to consider that, if the histogram of the eigenvalues of RN is close
to that of Figure 1.1, this evidences the presence of noise but the absence of a
transmitted signal. A large range of methods have been investigated, in diﬀerent
scenarios such as multi-source detection, multiple antenna signal sensing, known
or unknown signal-to-noise ratio (SNR), etc. to come up with (N, n)-consistent
detection tests [Bianchi et al., 2011; Cardoso et al., 2008]. These methods are
here of particular interest when the system dimensions are extremely large to
ensure low rates of false positives (declaring pure noise to be a transmitted signal)
and of false negatives (missing the detection of a transmitted signal). When a
small number of sensors are used, small dimensional random matrix models,
however more involved, may be required [Couillet and Debbah, 2010a]. Note
ﬁnally that, along with inference methods for signal sensing, small dimensional
random matrices are also used in the new ﬁeld of Bayesian channel modeling
[Guillaud et al., 2007], which will be introduced in Chapter 18 and thoroughly
detailed.

1.2. History and book outline
13
Chapter 16, Chapter 17 and Chapter 18 discuss the solutions brought by
the ﬁeld of asymptotically large and small random matrices to the problems
of estimation, detection, and system modeling, respectively. Chapter 19 will
then discuss the perspectives and challenges envisioned for the future of random
matrices in topics related to wireless communications. Finally, in Chapter 20, we
draw the conclusions of this book.


Part I
Theoretical aspects


2
Random matrices
It is often assumed that random matrices is a ﬁeld of mathematics which treats
matrix models as if matrices were of inﬁnite size and which then approximate
functionals of realistic ﬁnite size models using asymptotic results. We wish ﬁrst
to insist on the fact that random matrices are necessarily of ﬁnite size, so we do
not depart from conventional linear algebra. We start this chapter by introducing
initial considerations and exact results on ﬁnite size random matrices. We will see
later that, for some matrix models, it is then interesting to study the features of
some random matrices with large dimensions. More precisely, we will see that the
eigenvalue distribution function F BN of some N × N random Hermitian matrices
BN converge in distribution (often almost surely so) to some deterministic limit
F when N grows to inﬁnity. The results obtained for F can then be turned into
approximative results for F BN , and therefore help to provide approximations
of some functionals of BN. Even if it might seem simpler for some to think of
F as the eigenvalue distribution of an inﬁnite size matrix, this does not make
much sense in mathematical terms, and we will never deal with such objects as
inﬁnite size matrices, but only with sequences of ﬁnite dimensional matrices of
increasing size.
2.1
Small dimensional random matrices
We start with a formal deﬁnition of a random matrix and introduce some
notations.
2.1.1
Deﬁnitions and notations
Deﬁnition 2.1. An N × n matrix X is said to be a random matrix if it is a
matrix-valued random variable on some probability space (Ω, F, P) with entries
in some measurable space (R, G), where F is a σ-ﬁeld on Ωwith probability
measure P and G is a σ-ﬁeld on R. As per conventional notations, we denote
X(ω) the realization of the variable X at point ω ∈Ω.
The rigorous introduction of the probability space (Ω, F, P) is only necessary
here for some details on proofs given in this chapter. When not mentioning either

18
2. Random matrices
Ωor the σ-ﬁeld F in the rest of this book, it will be clear what implicit probability
space is being referred to. In general, though, this formalism is unimportant.
Also, unless necessary for clarity, we will in the following equally refer to X as
the random variable and as its random realization X(ω) for ω ∈Ω. The space
R will often be taken to be either R or C, i.e. for all ω ∈Ω, X(ω) ∈RN×n or
X(ω) ∈CN×n.
We deﬁne the probability distribution of X to be µX, the joint probability
distribution of the entries of X, such that, for A ∈GN×n
µX(A) ≜P({ω, X(ω) ∈A}).
In most practical cases, µX will have a probability density function (p.d.f.) with
respect to whatever measure on RN×n, which we will denote PX, i.e. for dY ∈G
an elementary volume around Y ∈RN×n
PX(Y)dY ≜µX(dY) = P({ω, X(ω) ∈dY}).
In order to diﬀerentiate the probability distribution function of real random
variables X from that of multivariate entities, we will often denote pX ≜PX in
lowercase characters. For vector-valued real random variables (X1, . . . , XN), we
further denote P(X1,...,XN)(x1, . . . , xN) or P(Xi)(x1, . . . , xN) the density of the
unordered values X1, . . . , XN P ≤
(X1,...,XN)(x1, . . . , xN) or P ≤
(Xi)(x1, . . . , xN) the
density of the non-decreasing values X1 ≤. . . ≤XN and P ≥
(X1,...,XN)(x1, . . . , xN)
or P ≥
(Xi)(x1, . . . , xN) the density of the non-increasing values X1 ≥. . . ≥XN.
The (cumulative) distribution function (d.f.) of a real random variable will
often be denoted by the letters F, G, or H, e.g. for x ∈R
F(x) ≜µX((−∞, x])
denotes the d.f. of X.
We will in particular often consider the marginal probability distribution
function of the eigenvalues of random Hermitian matrices X. Unless otherwise
stated, the d.f. of the real eigenvalues of X will be denoted F X.
Remark 2.1. As mentioned earlier, in most applications, the probability space
Ωneeds not be deﬁned but may have interesting interpretations. In wireless
communications, if H ∈Cnr×nt is a random MIMO channel between an nt-
antenna transmitter and an nr-antenna receiver, Ωcan be seen as the space
of “possible environments,” the elements ω of which being all valid snapshots of
the physical world at a given instant. The random value H(ω) is therefore the
realization of an instantaneous nr × nt multiple antenna propagation channel.
We will say that a sequence F1, F2, . . . of real-supported distribution functions
converge weakly to the function F, if, for x ∈R a continuity point of F
lim
n→∞Fn(x) = F(x).

2.1. Small dimensional random matrices
19
This will be denoted as
Fn ⇒F.
We will also say that a sequence x1, x2, . . . of random variables converges
almost surely to the constant x if
P(lim
n xn = x) = P({ω, lim
n xn(ω) = x}) = 1.
The underlying probability space (Ω, F, P) here is assumed to be the space that
generates the sequences x1(ω), x2(ω), . . . (and not the individual entries), for
ω ∈Ω. This will be denoted
xn
a.s.
−→x.
We will in particular be interested in the almost sure weak convergence of
distribution functions, i.e. in proving that, for some sequence F1, F2, . . . of d.f.
with weak limit F, there exists a space A ∈F, such that P(A) = 1 and, for all
x ∈R, ω ∈A implies
lim
n Fn(x; ω) = F(x)
with F1(·; ω), F2(·; ω), . . . one realization of the d.f.-valued random sequence
F1, F2, . . ..
Although this will rarely be used, we also mention the notation for convergence
in probability of a sequence x1, x2, . . . to x. A sequence x1, x2, . . . is said to
converge in probability to x if, for all ε > 0
lim
n P(|xn −x| > ε) = 0.
2.1.2
Wishart matrices
In the following section, we provide elementary results on the distribution of
random matrices with Gaussian entries and of their eigenvalue distributions.
As mentioned in the Introduction, the very ﬁrst random matrix considerations
date back to 1928 [Wishart, 1928], with the expression of the p.d.f. of random
matrices XXH, for X ∈CN×n with columns x1, . . . , xn ∼CN(0, R). Such a
matrix is called a (central) Wishart matrix.
Deﬁnition 2.2. The N × N random matrix XXH is a (real or complex) central
Wishart matrix with n degrees of freedom and covariance matrix R if the columns
of the N × n matrix X are zero mean independent (real or complex) Gaussian
vectors with covariance matrix R. This is denoted
XXH = XXT ∼WN(n, R)
for real Wishart matrices and
XXH ∼CWN(n, R)

20
2. Random matrices
for complex Wishart matrices.
Deﬁning the Gram matrix associated with any complex matrix X as being the
matrix XXH, XXH ∼CWN(n, R) is by deﬁnition the Gram matrix of a matrix
with Gaussian i.i.d. columns of zero mean and covariance R. When R = IN, it
is usual to refer to X as a standard Gaussian matrix.
The interest of Wishart matrices lies primarily in the following remark.
Remark 2.2. Let x1, . . . , xn ∈CN be n independent samples of the random
process x1 ∼CN(0, R). Then, denoting X = [x1, . . . , xn]
n
X
i=1
xixH
i = XXH.
For this reason, the random matrix Rn ≜1
nXXH is often referred to as an
(empirical) sample covariance matrix associated with the random process x1.
This is to be contrasted with the population covariance matrix R, whose relation
to the Rn was already evidenced as non-trivial, when n and N are of the same
order of magnitude. Of particular importance is the case when R = IN. In this
situation, XXH, sometimes referred to as a zero (or null) Wishart matrix, is
proportional to the sample covariance matrix of a white Gaussian process. The
zero (or null) terminology is due to the signal processing problem of hypothesis
testing, in which we have to decide whether the observed X emerges from a white
noise process or from an information plus noise process. The noise hypothesis is
often referred to as the null hypothesis.
Wishart provides us with the p.d.f. of Wishart matrices W in the space of
non-negative deﬁnite matrices with elementary volume dW, as follows.
Theorem 2.1 ([Wishart, 1928]). The p.d.f. of the complex Wishart matrix
XXH ∼CWN(n, R), X ∈CN×n, in the space of N × N non-negative deﬁnite
complex matrices, for n ≥N, is
PXXH(B) =
π−1
2 N(N−1)
det Rn QN
i=1(n −i)!
e−tr(R−1B) det Bn−N.
(2.1)
Note in particular that, for N = 1, this is the distribution of a real random
variable X with 2X chi-square-distributed with 2n degrees of freedom.
Proof. Since X is a Gaussian matrix of size N × n with zero mean and covariance
R, we know that
PX(A)dX =
1
πNn det Rn e−tr(R−1AAH)dX
which is the probability of an elementary volume dX around point A in the
space of N × n complex matrices with measure dX ≜Q
i,j dXij, with Xij the
entry (i, j) of X. Now, to derive the probability PXXH(B)d(XXH), it suﬃces to

2.1. Small dimensional random matrices
21
operate a variable change between X and XXH from the space of N × n complex
matrices to the space of N × N non-negative deﬁnite complex matrices. This is
obtained by Wishart in [Wishart, 1928], who shows that
dX = π−1
2 N(N−1)+Nn
QN
i=1(n −i)!
det(XXH)n−Nd(XXH)
and therefore:
PXXH(B)d(XXH) =
π−1
2 N(N−1)
det Rn QN
i=1(n −i)!
e−tr(R−1B) det Bn−Nd(XXH)
which is the intended formula.
In
[Ratnarajah
and
Vaillancourt,
2005,
Theorem
3],
Ratnarajah
and
Vaillancourt extend the result from Wishart to the case of singular matrices,
i.e. when n < N. This is given as follows.
Theorem 2.2 ([Ratnarajah and Vaillancourt, 2005]). The p.d.f. of the complex
Wishart matrix XXH ∼CW N(n, R), X ∈CN×n, in the space of N × N non-
negative deﬁnite complex matrices of rank n, for n < N, is
PXXH(B) = π−1
2 N(N−1)+n(n−N)
det Rn Qn
i=1(n −i)!e−tr(R−1B) det Λn−N
with Λ ∈Cn×n the diagonal matrix of the positive eigenvalues of B.
As already noticed from Equation (1.3) of the mutual information of MIMO
communication channels and from the brief introduction of eigenvalue-based
methods for detection and estimation, the center of interest of random matrices
often lies in the distribution of their eigenvalues. For null Wishart matrices, notice
that PXXH(B) = PXXH(UBUH), for any unitary N × N matrix U.1 Otherwise
stated, the eigenvectors of the random variable RN are uniformly distributed
over the space U(N) of unitary N × N matrices. As such, the eigenvectors do
not carry relevant information, and PXXH(B) is only a function of the eigenvalues
of B.
The joint p.d.f. of the eigenvalues of zero Wishart matrices were studied
simultaneously in 1939 by diﬀerent authors [Fisher, 1939; Girshick, 1939; Hsu,
1939; Roy, 1939]. The main two results are summarized in the following.
Theorem 2.3. Let the entries of X ∈CN×n be i.i.d. Gaussian with zero mean
and unit variance. Denote m = min(n, N) and M = max(n, N). The joint p.d.f.
P ≥
(λi) of the positive ordered eigenvalues λ1 ≥. . . ≥λN of the zero Wishart matrix
1 We recall that a unitary matrix U ∈CN×N is such that UUH = UHU = IN.

22
2. Random matrices
XXH is given by:
P ≥
(λi)(λ1, . . . , λN) = e−Pm
i=1 λi
m
Y
i=1
λM−m
i
(m −i)!(M −i)!∆(Λ)2
where, for a Hermitian non-negative m × m matrix Λ,2 ∆(Λ) denotes the
Vandermonde determinant of its eigenvalues λ1, . . . , λm
∆(Λ) ≜
Y
1≤i<j≤m
(λj −λi).
The marginal p.d.f. pλ (≜Pλ) of the unordered eigenvalues is
pλ(λ) = 1
m
m−1
X
k=0
k!
(k + M −n)![LM−m
k
(λ)]2λM−me−λ
where Lk
n are the Laguerre polynomials deﬁned as
Lk
n(λ) =
eλ
k!λn
dk
dλk (e−λλn+k).
The generalized case of (non-zero) central Wishart matrices is more involved
since it requires advanced tools of multivariate analysis, such as the fundamental
Harish–Chandra integral [Chandra, 1957]. We will mention the result of Harish–
Chandra, which is at the core of major results in signal sensing and channel
modeling presented in Chapter 16 and Chapter 18, respectively.
Theorem 2.4 ([Chandra, 1957]). For non-singular N × N positive deﬁnite
Hermitian matrices A and B of respective eigenvalues a1, . . . , aN and b1, . . . , bN,
such that, for all i ̸= j, ai ̸= aj and bi ̸= bj, we have:
Z
U∈U(N)
eκ tr(AUBUH)dU =
 N−1
Y
i=1
i!
!
κ
1
2 N(N−1) det
 {e−bjai}1≤i,j≤N

∆(A)∆(B)
where, for any bivariate function f, {f(i, j)}1≤i,j≤N denotes the N × N matrix
of (i, j) entry f(i, j), U(N) is the space of N × N unitary matrices, and dU
denotes the invariant measure on U(N) normalized to make the total measure
unity.
The remark that dU is a normalized measure on U(N) arises from the fact
that, contrary to spaces such as RN, U(N) is compact. As such, it can be
attached a measure dU such that
R
U(N) dU = V for V the volume of U(N) under
this measure. We arbitrarily take V = 1 here. In other publications, e.g., [Hiai
and Petz, 2006], the authors choose other normalizations. As for the invariant
measure terminology (called Haar measure in subsequent sections), it refers to
2 We will respect the convention that x (be it a scalar or a Hermitian matrix) is non-negative
if x ≥0, while x is positive if x > 0.

2.1. Small dimensional random matrices
23
the measure such that dU is a constant (taken to equal 1 here). In the rest of
this section, we take this normalization.
Theorem 2.4 enables the calculus of the marginal joint-eigenvalue distribution
of (non-zero) central Wishart matrices [Itzykson and Zuber, 2006] given as:
Theorem 2.5 (Section 8.7 of [James, 1964]). Let the columns of X ∈CN×n be
i.i.d. zero mean Gaussian with positive deﬁnite covariance R, and n ≥N. The
joint p.d.f. P ≥
(λi) of the ordered positive eigenvalues λ1 ≥. . . ≥λN of the central
Wishart matrix XXH reads:
P ≥
(λi)(λ1, . . . , λN) = (−1)
1
2 N(N−1) det({e
−λi
rj }1≤i,j≤N)
det Rn
∆(Λ)
∆(R−1)
N
Y
j=1
λn−N
j
(n −j)!
with r1 > . . . > rN > 0 the eigenvalues of R and Λ = diag(λ1, . . . , λN).
Proof. The idea behind the proof is ﬁrst to move from the space of XXH non-
negative deﬁnite matrices to the product space of matrices (U, Λ), U being
unitary and Λ diagonal with positive entries. For this, it suﬃces to notice from
a Jacobian calculus that
d(XXH) = π
1
2 N(N−1)
QN
i=1 i!
N
Y
i<j
(λj −λi)2dΛdU
with XXH = UΛUH.3 It then suﬃces to integrate out the matrices U for ﬁxed
Λ from the probability distribution PXXH. This requires the Harish–Chandra
formula, Theorem 2.4.
Noticing that the eigenvalue labeling is in fact irrelevant, we obtain the joint
unordered eigenvalue distribution of central Wishart matrices as follows.
Theorem 2.6 ([James, 1964]). Let the columns of X ∈CN×n be i.i.d. zero mean
Gaussian with positive deﬁnite covariance R and n ≥N. The joint p.d.f. P(λi)
of the unordered positive eigenvalues λ1, . . . , λN of the central Wishart matrix
XXH, reads:
P(λi)(λ1, . . . , λN) = (−1)
1
2 N(N−1) 1
N!
det({e
−λi
rj }1≤i,j≤N)
det Rn
∆(Λ)
∆(R−1)
N
Y
j=1
λn−N
j
(n −j)!
with r1 > . . . > rN > 0 the eigenvalues of R and Λ = diag(λ1, . . . , λN).
Note that it is necessary here that the eigenvalues of R be distinct. This
is because the general expression of Theorem 2.6 involves integration over the
3 It is often found in the literature that d(XXH) = QN
i<j(λj −λi)2dΛdU. This results from
another normalization of the invariant measure on U(N).

24
2. Random matrices
unitary space which expresses in closed-form in this scenario thanks to the
Harish–Chandra formula, Theorem 2.4.
A similar expression for the singular case when n < N is provided in
[Ratnarajah and Vaillancourt, 2005, Theorem 4], but the ﬁnal expression is less
convenient to express in closed-form as it features hypergeometric functions,
which we introduce in the following. Other extensions of the central Wishart
p.d.f. to non-central Wishart matrices, i.e. originating from a matrix with non-
centered Gaussian entries, have also been considered, leading to results on the
joint-entry distribution [Anderson, 1946], eigenvalue distribution [Jin et al.,
2008], largest and lowest eigenvalue marginal distribution, condition number
distribution [Ratnarajah et al., 2005a], etc. The results mentioned so far are of
considerable importance when matrices of small dimension have to be considered
in concrete applications. We presently introduce the result of [Ratnarajah and
Vaillancourt, 2005, Theorem 4] that generalizes Theorem 2.6 and the result
that concerns non-central complex Wishart matrices [James, 1964], extended
in [Ratnarajah et al., 2005b]. For this, we need to introduce a few deﬁnitions.
Deﬁnition 2.3. For κ = (k1, . . . , kN) with k1 + . . . + kN = k for some (k, N),
we denote for x ∈C
[x]κ =
N
Y
i=1
(x −i + 1)ki
where (u)k = u(u + 1) . . . (u + k −1). We also denote, for X ∈CN×N with
complex eigenvalues λ1, . . . , λN
Cκ(X) = χ[κ](1)χ[κ](X)
the complex zonal polynomial of X, where χ[κ](1) is given by:
χ[κ](1) = k!
QN
i<j(ki −kj −i −j)
QN
i=1(ki + N −i)!
and χ[κ](X) reads:
χ[κ](X) =
det

{λkj+N−j
i
}i,j

det

{λN−j
i
}i,j

.
The hypergeometric function of a complex matrix X ∈CN×N is then deﬁned by
pFq(a1, . . . , ap; b1, . . . , bp; X) =
∞
X
k=0
X
κ
[a1]κ . . . [ap]κ
[b1]κ . . . [bp]κ
Cκ(X)
k!
where a1, . . . , ap, b1, . . . , bp ∈C, P
κ is a summation over all partitions κ of k
elements into N and [x]κ = QN
i=1(x −i + 1)ki.

2.1. Small dimensional random matrices
25
We then deﬁne the hypergeometric function of two complex matrices of the
same dimension X, Y ∈CN×N to be
pFq(X, Y) =
Z
U(N)
pFq(XUYUH)dU.
We have in particular that
0F0(X) = etr X
1F0(a; X) = det(IN −X)−a.
From this observation, we remark that the Harish–Chandra formula, Theorem
2.4, actually says that
0F0(A, B) =
Z
U∈U(N)
0F0(AUBUH)dU =
 N−1
Y
i=1
i!
!
det
 {e−bjai}1≤i,j≤N

∆(A)∆(B)
as long as the eigenvalues of A and B are all distinct. In particular, Theorem 2.6
can be rewritten in its full generality, i.e. without the constraint of distinctness
of the eigenvalues of R, when replacing the determinant expressions by the
associated hypergeometric function.
Further generalizations of hypergeometric functions of multiple Hermitian
matrices, non-necessarily of the same dimensions, exist. In particular, for
A1, . . . , Ap all symmetric with Ak ∈CNk×Nk, we mention the deﬁnition, see,
e.g., [Hanlen and Grant, 2003]
0F0(A1, . . . , Ap) =
∞
X
k=0
X
κ
 
Cκ(A1)
k!
p
Y
i=2
Cκ(Ai)
Cκ(IN)
!
(2.2)
with N = maxk{Nk} and the partitions κ have dimension mink{Nk}. For
A ∈CN×N and B ∈Cn×n, n ≤N, we therefore have the general expression of
0F0(A, B) as
0F0(A, B) =
∞
X
k=0
X
κ
Cκ(A)Cκ(B)
k!Cκ(IN)
.
(2.3)
Sometimes, the notation
0F (n)
0
is used where the superscript (n) indicates
that the partitions κ are n-dimensional. For a deeper introduction to zonal
polynomials and matrix-valued hypergeometric functions, see, e.g., [Muirhead,
1982].
The extension of Theorem 2.6 to the singular case is as follows.
Theorem 2.7 (Theorem 4 of [Ratnarajah and Vaillancourt, 2005]). Let the
columns of X ∈CN×n be i.i.d. zero mean Gaussian with non-negative deﬁnite
covariance R and n < N. The joint p.d.f. P(λi) of the unordered positive

26
2. Random matrices
eigenvalues λ1, . . . , λn of the central Wishart matrix XXH, reads:
P(λi)(λ1, . . . , λN) = ∆(Λ)2
0F0(−R−1, Λ)
n
Y
j=1
λN−n
j
(n −j)!(N −j)!
where Λ = diag(λ1, . . . , λn).
Note here that R−1 ∈CN×N and Λ ∈Cn×n, n < N, so that 0F0(−R−1, Λ)
does not take the closed-form conveyed by the Harish–Chandra theorem,
Theorem 2.4, but can be evaluated from Equation (2.3).
The result from James on non-central Wishart matrices is as follows.
Theorem 2.8 ([James, 1964]). If X ∈CN×n is a complex Gaussian matrix with
mean M and invertible covariance Σ = 1
nE[(X −M)(X −M)H], then XXH is
distributed as
PXXH(B) = π−1
2 N(N−1)
QN
i=1(n −i)!
det(B)n−N
det(Σ)n e−tr Σ−1(MMH+B)
0F1(n; Σ−1MMHΣ−1B).
Also, the density of the unordered eigenvalues λ1, . . . , λN
of Σ−1XXH
expresses as a function of the unordered eigenvalues m1, . . . , mN of Σ−1MMH
as
P(λi)(λ1, . . . , λN) = ∆(Σ−1MMH)2e−PN
i=1(mi+λi)
N
Y
i=1
λn−N
i
(n −i)!(N −i)!
if mi ̸= mj for all i ̸= j.
We complete this section by the introduction of an identity that allows us
to extend results such as Theorem 2.6 to the case when an eigenvalue of R
has multiplicity greater than one. As can be observed in the expression of
the eigenvalue density in Theorem 2.6, equating population eigenvalues leads
in general to bringing numerators and denominators to zero. To overcome this
diﬃculty when some eigenvalues have multiplicities larger than one, we have the
following result which naturally extends [Simon et al., 2006, Lemma 6].
Theorem 2.9 ([Couillet and Guillaud, 2011; Simon et al., 2006]). Let f1, . . . , fN
be a family of inﬁnitely diﬀerentiable functions and let x1, . . . , xN ∈R. Denote
R(x1, . . . , xN) ≜
det

{fi(xj)}i,j

Q
i<j(xj −xi) .

2.1. Small dimensional random matrices
27
Then, for N1, . . . , Np such that N1 + . . . + Np = N and for y1, . . . , yp ∈R
distinct
lim
x1,...,xN1→y1
...
xN−Np+1,...,xN→yp
R(x1, . . . , xN)
=
det
h
fi(y1), f ′
i(y1), . . . , f (N1−1)
i
(y1), . . . , fi(yp), f ′
i(yp), . . . , f (Np−1)
i
(yp)
i
Q
1≤i<j≤p(yj −yi)NiNj Qp
l=1
QNl−1
j=1 j!
.
This observation will be useful when deriving results in Chapter 16 in the
context of signal detection, where one of the population eigenvalues, equal to σ2,
the variance of the additive noise, has a multiplicity in general much larger than
one. This result is obtained from successive iterations of L’Hospital rule, which
we remind below and which will be useful for other purposes in the book.
Theorem 2.10. Let f(x) and g(x) be two real-valued continuous functions,
diﬀerentiable in a neighborhood of a, such that
lim
x→a f(x) = lim
x→a g(x) = 0
or
lim
x→a |f(x)| = lim
x→a |g(x)| = ∞
and such that
lim
x→a
f ′(x)
g′(x) = l.
Then we have:
lim
x→a
f(x)
g(x) = l.
Theorem 2.9 is in particular obtained by taking for instance
R(x1, . . . , xN) = R(y1 + ε, . . . , y1 + N1ε, . . . , yp + ε, . . . , yp + Npε)
and using L’Hospital rule in the limit ε →0. Indeed, in the simplest scenario of
a convergence of x1, x2 to y1, we have
lim
x1,x2→y1 R(x1, . . . , xN)
= lim
ε→0
det [fi(y1 + ε), fi(y1 + 2ε), fi(x3), . . . , fi(xN)]
ε Q
i>j>2(xi −xj) QN
i=3(xi −y1 −ε)(xi −y1 −2ε)

28
2. Random matrices
which, after application of L’Hospital rule, equals (upon existence of the limit),
to the limiting ratio between the numerator
X
σ∈SN
sgn(σ)
"
f ′
σ1(y1 + ε)fσ2(y1 + 2ε)
N
Y
i=3
fσi(xi)
+2fσ1(y1 + ε)f ′
σ2(y1 + 2ε)
N
Y
i=3
fσi(xi)
#
and the denominator
Y
i>j>2
(xi −xj)
N
Y
i=3
(xi −y1 −ε)(xi −y1 −2ε)
+ ε

Y
i>j>2
(xi −xj)
N
Y
i=3
(xi −y1 −ε)(xi −y1 −2ε)


′
as ε →0, with SN the set of permutations of {1, . . . , N} whose elements σ =
(σ1, . . . , σN) have signature (or parity) sgn(σ). Calling σ′ the permutation such
that σ′
1 = σ2, σ′
2 = σ1 and σ′
i = σi for i ≥3, taking the limit when ε →0, we can
reorder the sum in the numerator as
X
(σ,σ′)⊂SN
sgn(σ)
"
f ′
σ1(y1)fσ2(y1)
N
Y
i=3
fσi(xi) + 2fσ1(y1)f ′
σ2(y1)
N
Y
i=3
fσi(xi)
#
+ sgn(σ′)
"
f ′
σ′
1(y1)fσ′
2(y1)
N
Y
i=3
fσ′
i(xi) + 2fσ′
1(y1)f ′
σ′
2(y1)
N
Y
i=3
fσ′
i(xi)
#
.
But from the deﬁnition of σ′, we have sgn(σ′) = −sgn(σ) and then this becomes
X
(σ,σ′)⊂SN
sgn(σ)
"
−f ′
σ1(y1)fσ2(y1)
N
Y
i=3
fσi(xi) + fσ1(y1)f ′
σ2(y1)
N
Y
i=3
fσi(xi)
#
or equivalently
X
σ∈SN
sgn(σ)fσ1(y1)f ′
σ2(y1)
N
Y
i=3
fσi(xi)
the determinant of the expected numerator. As for the denominator, it clearly
converges also to the expected result, which ﬁnally proves Theorem 2.9 in this
simple case.
As exempliﬁed by the last results given in Theorem 2.7 and Theorem 2.8, the
mathematical machinery required to obtain relevant results, already for such a
simple case of correlated non-centered Gaussian matrices, is extremely involved.
This is the main reason for the increasing attractiveness of large dimensional
random matrices, which, as is discussed in the following, provide surprisingly
simple results in comparison.

2.2. Large dimensional random matrices
29
2.2
Large dimensional random matrices
2.2.1
Why go to inﬁnity?
When random matrix problems relate to rather large dimensional matrices, it is
often convenient to mentally assume that these matrices have in fact extremely
large dimensions in order to blindly apply the limiting results for large matrices
to the eﬀective ﬁnite-case scenario. It turns out that this approximate technique
is often stunningly precise and can be applied to approximate scenarios where the
eﬀective matrix under consideration is not larger than 8 × 8, and even sometimes
4 × 4 and 2 × 2. Before delving into the core of large dimensional random matrix
theory, let us explain further what we mean by “assuming extremely large size
and applying results to the ﬁnite size case.” As was already evidenced, we are
largely interested in the eigenvalue structure of random matrices and in particular
in the marginal density of their eigenvalues.
Consider an N × N (non-necessarily random) Hermitian matrix TN. Deﬁne
its empirical spectral distribution (e.s.d.) F TN to be the distribution function of
the eigenvalues of TN, i.e. for x ∈R
F TN (x) = 1
N
N
X
j=1
1{x,λj≤x}(x)
where λ1, . . . , λN are the eigenvalues of TN.4 For such problems as signal source
detection from a multi-dimensional sensor (e.g. scanning multiple frequencies),
TN might be a diagonal matrix with K distinct eigenvalues, of multiplicity N/K
each, representing the power transmitted by each source on N/K independent
frequency bands. Assuming a random signal matrix SN ∈CN×n with i.i.d.
entries is transmitted by the joint-source (composed of the concatenation of
N-dimensional signal vectors transmitted in n successive time instants), and
N, n are both large, then a rather simple model is to assume that the sensor
observes a matrix YN = T
1
2
NSN from which it needs to infer the K distinct
eigenvalues of TN. This model is in fact unrealistic, as it should consider other
factors such as the additive thermal noise, but we will stick here to this easier
model for the sake of simplicity. Inferring the K eigenvalues of TN is a hard
problem in ﬁnite size random matrix theory, for which the maximum likelihood
method boils down to a K-dimensional space search. If, however, we consider the
hypothetical series {YpN = TpNSpN, p ∈N} with SpN ∈CpN×pn whose entries
follow the same distribution as those of SN, and TpN = TN ⊗Ip, with ⊗the
Kronecker product, then the system has just been made larger without impacting
its structural ingredients. Indeed, we have just made the problem larger by
growing N into pN and n into pn. The most important fact to observe here is that
4 The Hermitian property is fundamental to ensure that all eigenvalues of TN belong to the real
line. However, the extension of the e.s.d. to non-Hermitian matrices is sometimes required;
for a deﬁnition, see (1.2.2) of [Bai and Silverstein, 2009].

30
2. Random matrices
the distribution function of TN has not been aﬀected, i.e. F TpN = F TN , for each
p. It turns out, as will be detailed in Chapter 16, that when p →∞, F YpNYH
pN has
a deterministic weak limit and there exist computationally eﬃcient techniques
to retrieve the exact K distinct eigenvalues of TN, from the non-random limit of
the e.s.d. limp→∞F YpNYH
pN ). Going back to the ﬁnite (but large enough) N case,
we can apply the same deterministic techniques to the random F YNYH
N instead
of limp→∞F YpNYH
pN to obtain a good approximation of the eigenvalues of TN.
These approximated values are consistent with a proportional growth of n and
N, as they are almost surely exact when N and n tend to inﬁnity with positive
limiting ratio, and are therefore (n, N)-consistent.
This is basically how most random matrix results work: (i) we artiﬁcially
let both n, N dimensions grow to inﬁnity with constant ratio, (ii) very often,
assuming large dimensions asymptotically leads to deterministic expressions, i.e.
independent of the realization ω (at least for ω in a subset of Ωof probability one)
which are simpler to manipulate, and (iii) we can then apply the deterministic
results obtained in (ii) to the ﬁnite dimensional stochastic observation ω at
hand and usually have a good approximation of the small dimensional matrix
behavior. The fact that small dimensional matrices enjoy similar properties
as their large dimensional counterparts makes the above approach extremely
attractive, notably to wireless communication engineers, who often deal with
not-so-large matrix models.
2.2.2
Limit spectral distributions
Let us now focus on large dimensional random matrices and abandon for now
the practical applications, which are discussed in Part II. The relevant aspect
of some classes of large N × N Hermitian random matrices XN is that their
(random) e.s.d. F XN converges, as N →∞, towards a non-random distribution
F. This function F, if it exists, will be called the limit spectral distribution (l.s.d.)
of XN. Weak convergence of F XN to F, i.e. for all x where F is continuous,
F XN (x) −F(x) →0, is often suﬃcient to obtain relevant results; this is denoted
F XN ⇒F.
In most cases, though, the weak convergence of F XN to F will only be true on
a set of matrices XN = XN(ω) of measure one. This will be mentioned with the
phrase F XN ⇒F almost surely.
We detail in the following the best-known examples of such convergence. The
ﬁrst result on limit spectral distributions is due to Wigner [Wigner, 1955, 1958],
who establishes the convergence of the eigenvalue distribution of a particular
case of the now-called Wigner matrices. In its generalized form [Arnold, 1967,
1971; Bai and Silverstein, 2009], this is:

2.2. Large dimensional random matrices
31
Theorem 2.11 (Theorem 2.5 and Theorem 2.9 of [Bai and Silverstein, 2009]).
Consider an N × N Hermitian matrix XN, with independent entries
1
√
N XN,ij
such that E[XN,ij] = 0, E[|XN,ij|2] = 1 and there exists ε such that the XN,ij
have a moment of order 2 + ε. Then F XN ⇒F almost surely, where F has
density f deﬁned as
f(x) = 1
2π
p
(4 −x2)+.
(2.4)
Moreover, if the XN,ij are identically distributed, the result holds without the
need for existence of a moment of order 2 + ε.
The l.s.d. F is the semi-circle law,5 depicted in Figure 1.2. The sketch of a
proof of the semi-circle law, based on the method of moments (Section 30 of
[Billingsley, 1995]) is presented in Section 5.1.
This result was then followed by limiting results on other types of matrices,
such as the full circle law for non-symmetric random matrices, which is a largely
more involved problem, starting with the fact that the eigenvalues of such
matrices are no longer restricted to the real axis [Hwang, 1986; Mehta, 2004].
Although Girko was the ﬁrst to provide a proof of this result, the most general
result is due to Bai in 1997 [Bai, 1997].
Theorem 2.12. Let XN ∈CN×N have i.i.d. entries
1
√
N XN,ij, 1 ≤i, j ≤N,
such that XN,11 has zero mean, unit variance and ﬁnite sixth order moment.
Additionally, assume that the joint distribution of the real and imaginary parts
of
1
√
N XN,11 has bounded density. Then, with probability one, the e.s.d. of XN
tends to the uniform distribution on the unit complex disc. This distribution is
referred to as the circular law, or full circle law.
The circular law is depicted in Figure 2.1.
Theorem 2.11 and Theorem 2.12 and the aforementioned results have
important consequences in nuclear physics and statistical mechanics, largely
documented in [Mehta, 2004]. In wireless communications, though, Theorem 2.11
and Theorem 2.12 are not the most fundamental and widely used results. Instead,
in the wireless communications ﬁeld, we are often interested in sample covariance
matrices or even more general matrices such as i.i.d. matrices multiplied both
on the left and on the right by deterministic matrices, or i.i.d. matrices with
a variance proﬁle, i.e. with independent entries of zero mean but diﬀerence
variances. Those matrices are treated in problems of detection, estimation and
capacity evaluation which we will further discuss in Part II. The best known
result with a large range of applications in telecommunications is the convergence
of the e.s.d. of the Gram matrix of a random matrix with i.i.d. entries of zero
5 Note that the semi-circle law sometimes refers, instead of F, to the density f, which is the
“semi-circle”-shaped function.

32
2. Random matrices
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
Eigenvalues (real part)
Eigenvalues (imaginary part)
Empirical eigenvalue distribution
Circular law
Figure 2.1 Eigenvalues of XN =

1
√
N X(N)
ij

ij with X(N)
ij
i.i.d. standard Gaussian, for
N = 500, against the circular law.
mean and normalized variance. This result is due to Mar˘cenko and Pastur
[Mar˘cenko and Pastur, 1967], so that the limiting e.s.d. of the Gram matrix
is called the Mar˘cenko–Pastur law. The result unfolds as follows.
Theorem 2.13. Consider a matrix X ∈CN×n with i.i.d. entries

1
√nXN,ij

,
such that XN,11 has zero mean and unit variance. As n, N →∞with N
n →c ∈
(0, ∞), the e.s.d. of Rn = XXH converges weakly and almost surely to a non-
random distribution function Fc with density fc given by:
fc(x) = (1 −c−1)+δ(x) +
1
2πcx
p
(x −a)+(b −x)+
(2.5)
where a = (1 −√c)2, b = (1 + √c)2 and δ(x) = 1{0}(x).
Note that, similar to the notation introduced previously, Rn is the sample
covariance matrix associated with the random vector (XN,11, . . . , XN,N1)T, with
population covariance matrix IN. The d.f. Fc is named the Mar˘cenko–Pastur law
with limiting ratio c.6 This is depicted in Figure 2.2 for diﬀerent values of the
limiting ratio c. Notice in particular that, as is expected from the discussion in the
Preface, when c tends to be small and approaches zero, the Mar˘cenko–Pastur law
reduces to a single mass in 1. A proof of Theorem 2.13, which follows a Stieltjes
6 Similarly as with the semi-circle law, the Mar˘cenko–Pastur law can also refer to the density
fc of Fc.

2.2. Large dimensional random matrices
33
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
1.2
x
Density fc(x)
c = 0.1
c = 0.2
c = 0.5
Figure 2.2 Mar˘cenko–Pastur law for diﬀerent limit ratios c = lim N/n.
transform-based method, is proposed in Section 3.2. Since the Mar˘cenko–Pastur
law has bounded support, it has moments of all orders, which are explicitly given
in the following result.
Theorem 2.14. Let Fc be the Mar˘cenko–Pastur law with ratio c and with density
fc given by (2.5). The successive moments M1, M2, . . . of Fc are given, for all
integer k, by
Mk = 1
k
k−1
X
i=0
k
i
 k
i + 1

ci.
Note that further generalizations of Theorem 2.13 have been provided in the
literature, the most general form of which comes as follows.
Theorem 2.15 (Theorem 3.10 of [Bai and Silverstein, 2009]). Consider a matrix
X ∈CN×n with entries

1
√nXN,ij

, independent for all i, j, n and such that
XN,ij has zero mean, unit variance, and ﬁnite 2 + ε order moment (ε being
independent of i, j, n). Then, as n, N →∞with
N
n →c ∈(0, ∞), the e.s.d. of
Rn = XXH converges almost surely to the Mar˘cenko–Pastur law Fc, with density
given by (2.5).
This last result goes beyond the initial Mar˘cenko–Pastur identity as it does not
assume identically distributed entries in X. However, the result would no longer
stand if the variances of the entries were diﬀerent, or were not independent. These
scenarios are of relevance in wireless communications to model multi-dimensional
channels with correlation or with a variance proﬁle, for instance.

34
2. Random matrices
The above theorems characterize the eigenvalue distribution of Gram matrices
of N × n matrices with i.i.d. entries. In terms of eigenvector distribution, though,
not much is known. We know that the eigenvectors of a Wishart matrix, i.e. a
matrix XXH as above, where the entries are constrained to be Gaussian, are
uniformly distributed on the unit sphere. That is, the eigenvectors do not point
to any privileged direction. This fact is true for all ﬁnite dimension N. Now,
obviously, this result does not stand for all ﬁnite N for random matrices with i.i.d.
but non-Gaussian entries. Still, we clearly feel that in some sense the eigenvectors
of XXH must be “isotropically distributed in the limit.” This is stated precisely
for the real case under the following theorem, due to Silverstein [Silverstein, 1979,
1981, 1984].
Theorem 2.16. Let X ∈RN×n be random with i.i.d. real-valued entries of zero
mean and all moments uniformly bounded. Denote U = [u1, . . . , uN] ∈RN×N,
with uj the eigenvector of the jth largest eigenvalue of XXT. Additionally, denote
x ∈RN an arbitrary vector with unit Euclidean norm and y = (y1, . . . , yN)T the
random vector deﬁned by
y = Ux.
Then, as N, n →∞with limiting ratio N/n →c, 0 < c ≤1, for all t ∈[0, 1]
⌊tN⌋
X
k=1
y2
k
a.s.
−→t
where ⌊x⌋is the greatest integer smaller than x.
This result indicates some sort of uniformity in the distribution of the
eigenvectors of XXT. In [Silverstein, 1986], Silverstein extends Theorem 2.16 into
a limit theorem of the ﬂuctuations of the random process P⌊tN⌋
k=1 y2
k, the weak limit
being a Brownian bridge. Apart from these results, though, not much more is
known about eigenvectors of random matrices. This subject has however recently
gained some more interest and recent results will be given later in Chapter 8.
In the following chapters, we introduce the important tools known to this day
to characterize the l.s.d. of a large range of random matrix classes. We start
with the Stieltjes transform and provide a thorough proof of the Mar˘cenko–
Pastur law via the Stieltjes transform method. This proof will allow the reader
to have a clear view on the building blocks required for deriving most results of
large dimensional random matrix theory for wireless communications.

3
The Stieltjes transform method
This chapter is the ﬁrst of three chapters dedicated to providing the reader
with an overview of the most important tools used in problems related to large
random matrices. These tools will form a strong basis for the reader to be able
to appreciate the extensions discussed in the more technical Chapters 6–9. We
ﬁrst visit in this chapter the main results proved via the Stieltjes transform, to
be deﬁned subsequently. The Stieltjes transform tool is at ﬁrst not very intuitive
and not as simple as the moment-based methods developed later. For this
reason, we start with a step-by-step proof of the Mar˘cenko–Pastur law, Theorem
2.13, for large dimensional matrices with i.i.d. entries, before we can address
more elaborate random matrix models with non-independent or not identically
distributed entries. We will then introduce the Stieltjes transform related tools
that are the R-transform and the S-transform, which bear interesting properties
related to moments of the e.s.d. of some random matrix models. These R-
transform and the S-transform, along with the free probability theory from which
they originate, are the fundamental link between the Stieltjes transform and the
moment-based methods. These are discussed thoroughly in Chapters 4–5.
3.1
Deﬁnitions and overview
To be able to handle the powerful tools that are the Stieltjes transform and
moment-based methods, we start with some prior deﬁnitions of the Stieltjes
transform and other related functionals, which will be often used in the
subsequent chapters.
We ﬁrst introduce the Stieltjes transform.
Deﬁnition 3.1. Let F be a real-valued bounded measurable function over R.
Then the Stieltjes transform mF (z)of F,1 for z ∈Supp(F)c, the complex space
1 We borrow here the notation m to a large number of contributions from Bai, Silverstein et
al. In other works, the notation s or S for the Stieltjes transform is used. However, in this
work, the notation S will be reserved to the S-transform to be deﬁned later.

36
3. The Stieltjes transform method
complementary to the support of F,2 is deﬁned as
mF (z) ≜
Z ∞
−∞
1
λ −z dF(λ).
(3.1)
For all F which admits a Stieltjes transform, the inverse transformation exists
and formulates as follows.
Theorem 3.1. If x is a continuity points of F, then:
F(x) = 1
π lim
y→0+
Z x
−∞
ℑ[mF (x + iy)] dx.
(3.2)
Proof. Since the function t 7→
1
t−(x+iy) is continuous and tends to zero as |t| →
∞, it has uniformly bounded norm on the support of F and we can then apply
Tonelli’s theorem, Theorem 3.16, and write
1
π
Z b
a
ℑ[mF (x + iy)] dx
= 1
π
Z b
a
Z
y
(t −x)2 + y2 dF(t)dx
= 1
π
Z Z b
a
y
(t −x)2 + y2 dxdF(t)
= 1
π
Z 
tan−1
b −t
y

−tan−1
a −t
y

dF(t).
As y →0+, this tends to
R
1[a,b](t)dF(t) = F(b) −F(a).
In all practical applications considered in this book, F will be a distribution
function. Therefore, there exists an intimate link between distribution functions
and their Stieltjes transforms. More precisely, if F1 and F2 are two distribution
functions (therefore right-continuous by deﬁnition, see, e.g. Section 14 of
[Billingsley, 1995]) that have the same Stieltjes transform, then F1 and F2
coincide everywhere and the converse is true. As a consequence, mF uniquely
determines F and vice-versa. It will turn out that, while working on the
distribution functions of the empirical eigenvalues of large random matrices is
often a tedious task, the approach via Stieltjes transforms simpliﬁes greatly the
study. The initial intuition behind the Stieltjes transform approach for random
2 We recall that the support Supp(F ) of a d.f. F with density f is the closure of the set
{x ∈R, f(x) > 0}.

3.1. Deﬁnitions and overview
37
matrices lies in the following remark. For a Hermitian matrix X ∈CN×N
mF X(z) =
Z
1
λ −z dF X(λ)
= 1
N tr (Λ −zIN)−1
= 1
N tr (X −zIN)−1
in which we denoted Λ the diagonal matrix of eigenvalues of X. Working with
the Stieltjes transform of F X then boils down to working with the matrix
(X −zIN)−1, and more speciﬁcally the sum of its diagonal entries. From matrix
inversion lemmas and several fundamental matrix identities, it is then rather
simple to derive limits of traces
1
N tr (X −zIN)−1, as N grows large, and
therefore to derive a limit of the Stieltjes transform of F X. For instance, in
the case of large sample covariance matrices Rn, we will see that it is rather
easy to show that mF Rn tends almost surely to a function m, which is itself
the Stieltjes transform of a distribution function F. Thanks to Theorem 3.10,
this will prove that F Rn ⇒F almost surely. For notational simplicity, we may
denote mX ≜mF X the Stieltjes transform of the e.s.d. of the Hermitian matrix
X and call mX the Stieltjes transform of X.
An identity of particular interest is the relation between the Stieltjes transform
of AB and BA when AB is Hermitian.
Lemma 3.1. Let A ∈CN×n, B ∈Cn×N, such that AB is Hermitian. Then, for
z ∈C \ R
n
N mF BA(z) = mF AB(z) + N −n
N
1
z .
Also, for X ∈CN×n and for z ∈C \ R+, we have:
n
N mF XHX(z) = mF XXH (z) + N −n
N
1
z .
The identity follows directly from the fact that both AB and BA have the
same eigenvalues except for additional zero eigenvalues for the larger matrix.
Hence, say n ≥N, the larger matrix has N eigenvalues being the same as the
eigenvalues of the smaller matrix, plus additional (n −N) eigenvalues equal to
zero. Each one of the latter leads to the addition of a term 1/(0 −z) = −1/z,
hence the identity.
Also, we have the following trivial relation.
Lemma 3.2. Let X ∈CN×N be Hermitian and a be a non-zero real. Then, for
z ∈C \ R
maX(az) = 1
amX(z).

38
3. The Stieltjes transform method
This unfolds by noticing that
maX(az) =
Z
1
at −az dF X(t) = 1
a
Z
1
t −z dF X(t).
For practical calculus in the derivations of the subsequent chapters, we need
to introduce the following important properties for the Stieltjes transform of
distribution functions, see, e.g., [Hachem et al., 2007].
Theorem 3.2. Let mF be the Stieltjes transform of a distribution function F,
then:
• mF is analytic over C+,
• if z ∈C+, then mF (z) ∈C+,
• if z ∈C+, |mF (z)| ≤
1
ℑ[z] and ℑ[1/mF (z)] ≤−ℑ[z],
• if F(0−) = 0,3 then mF is analytic over C \ R+. Moreover, z ∈C+ implies
zmF (z) ∈C+ and we have the inequalities
|mF (z)| ≤





1
|ℑ[z]|
, z ∈C \ R
1
|z|
, z < 0
1
dist(z,R+) , z ∈C \ R+
with dist the Euclidean distance.
Conversely, if m is a function analytical on C+ such that m(z) ∈C+ if z ∈C+
and
lim
y→∞−iy m(iy) = 1
(3.3)
then m is the Stieltjes transform of a distribution function F given by
F(b) −F(a) = lim
y→0
1
π
Z b
a
ℑ[m(x + iy)]dx.
If, moreover, zm(z) ∈C+ for z ∈C+, then F(0−) = 0, in which case m has an
analytic continuation on C \ R+.
The ﬁrst inequalities will often be used when providing limiting results for
some large dimensional random matrix models involving the Stieltjes transform.
The converse results will be more rarely used, restricted mainly to some technical
points in the proof of advanced results. Note that, if the limit in Equation (3.3)
is ﬁnite but diﬀerent from 1, then m(z) is said to be the Stieltjes transform of a
ﬁnite measure on R+.
An interesting corollary of Theorem 3.2, which will often be reused in technical
proofs, is the following.
3 We will denote F(0−) and F(0+) the limit of F(x) when x tends to zero from below or from
above, respectively.

3.1. Deﬁnitions and overview
39
Corollary 3.1. Let t > 0 and mF (z) be the Stieltjes transform of a distribution
function F. Then, for z ∈C+

1
1 + tmF (z)
 ≤|z|
ℑ[z].
Proof. It suﬃces to realize here, from the properties of the Stieltjes transform
given in the converse of Theorem 3.2, that
−1
z(1 + tmF (z))
is the Stieltjes transform of some distribution function. It therefore unfolds, from
the Stieltjes transform inequalities of Theorem 3.2, that

−1
z(1 + tmF (z))
 ≤
1
ℑ[z].
A further corollary of Corollary 3.1 then reads:
Corollary 3.2. Let x ∈CN, t > 0 and A ∈CN×N be Hermitian non-negative
deﬁnite. Then, for z ∈C+

1
1 + txH (A −zIN)−1 x
 ≤|z|
ℑ[z].
Proof. This unfolds by writing A = UHΛU, the spectral decomposition of A,
with Λ = diag(λ1, . . . , λN) ∈CN×N diagonal and U ∈CN×N. Denoting y = Ux
and y = (y1, . . . , yN)T, we have:
xH (A −zIN)−1 x =
X
i
|yi|2
λi −z .
Under this notation, it is clear that the function f(z) = xH (A −zIN)−1 x maps
C+ to C+ and that limy→∞−iyf(iy) = P
j |yj|2 > 0. Therefore, up to a positive
scaling factor, f(z) is the Stieltjes transform of a probability measure. We can
therefore use Corollary 3.2, which completes the proof.
In wireless communications, we are often interested in calculating the data
transmission rate achievable on a multi-dimensional N × n communication
channel H. We are therefore often led to evaluate functions in the form of (1.2).
It turns out that the Stieltjes transform is directly connected to this expression
of mutual information through the so-called Shannon transform, initially coined
by Tulino and Verd´u.
Deﬁnition 3.2 (Section 2.3.3 of [Tulino and Verd´u, 2004]). Let F be a
probability distribution deﬁned on R+. The Shannon transform VF of F is

40
3. The Stieltjes transform method
deﬁned, for x ∈R+, as
VF (x) ≜
Z ∞
0
log(1 + xλ)dF(λ).
(3.4)
The Shannon transform of F is related to its Stieltjes transform mF through
the expression
VF (x) =
Z ∞
1
x
1
t −mF (−t)

dt =
Z x
0
1
t −1
t2 mF

−1
t

dt.
(3.5)
The expression in brackets in (1.2) is N times the right-hand side of (3.4) if F
is chosen to be the e.s.d. of HHH. To evaluate (1.2), it is therefore suﬃcient to
evaluate the Stieltjes transform of F. This is the very starting point of capacity
evaluations using the Stieltjes transform.
Another important characteristic of the Stieltjes transform, both from a
theoretical and a practical point of view, is its relationship to moments of the
underlying distribution. We have in particular the following result.
Theorem 3.3. If F has compact support included in [a, b], 0 < a < b < ∞, then,
for z ∈C \ R such that |z| > b, mF (z) can be expanded in Laurent series as
mF (z) = −1
z
∞
X
k=0
Mk
zk
(3.6)
where
Mk =
Z ∞
−∞
λkdF(λ)
is the kth order moment of F.
By successive diﬀerentiations of zmF (−1/z), we can recover the series of
moments M1, M2, . . . of F; the Stieltjes transform is then a moment generating
function (at least) for compactly supported probability distributions. If F is the
e.s.d. of the Hermitian matrix X, then Mk is also called the kth order moment of
X. The above result provides therefore a link between the Stieltjes transform of
X and the moments of X. The moments of random Hermitian matrices are in fact
of practical interest whenever direct usage of Stieltjes transform-based methods
are too diﬃcult. Chapter 5 is dedicated to an account of these moment-based
considerations.
Before concluding this section, we introduce a few additional tools, all derived
from the Stieltjes transform, which have fundamental properties regarding the
moments of Hermitian matrices. From a theoretical point of view, they help
bridge classical probability theory to free probability theory [Hiai and Petz, 2006;
Voiculescu et al., 1992], to be introduced in Chapter 4. For a slightly more
exhaustive account of the most commonly used functionals of e.s.d. of large

3.1. Deﬁnitions and overview
41
dimensional random matrices and their main properties, refer to [Tulino and
Verd´u, 2004].
We consider ﬁrst the R-transform, deﬁned as follows.
Deﬁnition 3.3. Let F be a distribution function on R and let mF be its Stieltjes
transform, then the R-transform of F, denoted RF , is such that
mF (RF (z) + z−1) = −z
(3.7)
or equivalently
mF (z) =
1
RF (−mF (z)) −z .
(3.8)
If F is associated with a probability measure µ, then Rµ will also denote the
R-transform of F. Also, if F is the e.s.d. of the Hermitian matrix X, then RF
will be called the R-transform of X. The importance of the R-transform lies in
Theorem 4.6, to be introduced in Chapter 4. Roughly speaking, the R-transform
is the random matrix equivalent to the characteristic function of scalar random
variables in the sense that, under appropriate conditions (independence is not
suﬃcient) on the Hermitian random matrices A and B, the R-transform of the
l.s.d. of A + B is the sum of the R-transforms of the l.s.d. of A and of the l.s.d.
of B (upon existence of these limits).
Similar to the R-transform, which has additive moment properties, we deﬁne
the S-transform, which has product moment properties.
Deﬁnition 3.4. Let F be a distribution function on R and let mF be its Stieltjes
transform, then the S-transform of F, denoted SF , satisﬁes
mF
 z + 1
zSF (z)

= −zSF (z).
If F has probability measure µ, then Sµ denotes also the S-transform of F.
Under suitable conditions, the S-transform of the l.s.d. of a matrix product AB
is the product of the S-transforms of the l.s.d. of A and the l.s.d. of B.
Both R- and S-transforms are particularly useful when dealing with matrix
models involving unitary matrices; in particular, they will be used in Chapter 12
to evaluate the capacity of networks using orthogonal CDMA communications.
We also introduce two alternative forms of the Stieltjes transform, namely
the η-transform and the ψ-transform, which are sometimes preferred over the
Stieltjes transform because they turn out to be more convenient for readability
in certain derivations, especially those derivations involving the R- and S-
transform. The η-transform is deﬁned as follows.

42
3. The Stieltjes transform method
Deﬁnition 3.5. For F a distribution function with support in R+, we deﬁne
the η-transform of F, denoted ηF , to be the function deﬁned for z ∈C \ R−as
ηF (z) ≜
Z
1
1 + ztdF(t).
As such, the η-transform can be expressed as a function of the Stieltjes
transform as
ηF (z) = 1
z mF

−1
z

.
The ψ-transform is deﬁned similarly in the following.
Deﬁnition 3.6. For F a distribution function with support in R+, we deﬁne
the ψ-transform of F, denoted ψF , to be the function deﬁned for z ∈C \ R+ as
ψF (z) ≜
Z
zt
1 −ztdF(t).
Therefore, the ψ-transform can be written as a function of the Stieltjes
transform as
ψF (z) = −1 −1
z mF
1
z

.
These tools are obviously totally equivalent and are used in place of the
Stieltjes transform only in order to simplify long derivations.
The next section introduces the proof of one of the pioneering fundamental
results in large dimensional random matrix theory. This proof will demonstrate
the power of the Stieltjes transform tool.
3.2
The Mar˘cenko–Pastur law
As already mentioned, the Stieltjes transform was used by Mar˘cenko and Pastur
in [Mar˘cenko and Pastur, 1967] to derive the Mar˘cenko–Pastur law of large
dimensional Gram matrices of random matrices with i.i.d. entries. We start with
some reminders before providing the essential steps of the proof of the Mar˘cenko–
Pastur law and providing the complete proof.
We recall, from Theorem 3.1, that studying the Stieltjes transform of a
Hermitian matrix X is equivalent to studying the distribution function F X of
the eigenvalues of X. The celebrated result that triggered the now extensive use
of the Stieltjes transform is due to Mar˘cenko and Pastur [Mar˘cenko and Pastur,
1967] on the limiting distribution of the e.s.d. of sample covariance matrices with
identity population covariance matrix. Although many diﬀerent proofs exist by
now for this result, some using diﬀerent approaches than the Stieltjes transform
method, we will focus here on what we will later refer to as the the Mar˘cenko–
Pastur method. This method is both simple and pedagogical for it uses the

3.2. The Mar˘cenko–Pastur law
43
building blocks of the analytical aspect of large dimensional random matrix
theory in an elegant manner. We give in the following ﬁrst the key steps and
then the precise steps of this derivation. This proof will serve as grounds for the
derivations of further results, which utilize mostly the same approach, and also
the same lemmas and identities. In Chapter 6, we will discuss the drawbacks
of the method as it fails to generalize to some more involved matrix models,
especially when the e.s.d. of the large matrix under study does not converge.
Less intuitive but more powerful methods will then be proposed, among which
the Bai and Silverstein approach and the Gaussian methods.
We recall that the result we want to prove is the following. Let X ∈CN×n be
a matrix with i.i.d. entries

1
√nXN,ij

, such that XN,11 has zero mean and unit
variance. As n, N →∞with N
n →c ∈(0, ∞), the e.s.d. of Rn ≜XXH converges
almost surely to a non-random distribution function Fc with density fc given by:
fc(x) = (1 −c−1)+δ(x) +
1
2πcx
p
(x −a)+(b −x)+
where a = (1 −√c)2, b = (1 + √c)2 and δ(x) = 1{0}(x).
Before providing the extended proof, let us outline the general derivation. The
idea behind the proof is to study the Stieltjes transform
1
N tr(XXH −zIN)−1 of
F XXH instead of F XXH itself, and more precisely to study the diagonal entries
of the matrix (XXH −zIN)−1, often called the resolvent of X. The main steps
consists of the following.
• It will ﬁrst be observed, through algebraic manipulations, involving matrix
inversion lemmas, that the diagonal entry (1, 1) of (XXH −zIN)−1 can be
written as a function of a quadratic form yH(YHY −zIn)−1y, with y the ﬁrst
column of XH and YH deﬁned as XH with column y removed. Precisely, we
will have

(XXH −zIN)−1
11 =
1
−z −zyH(YHY −zIn)−1y.
• Due to the important trace lemma, which we will introduce and prove
below, the quadratic form yH(YHY −zIn)−1y can then be shown to be
asymptotically very close to 1
n tr(YHY −zIn)−1 (asymptotically meaning here
for increasingly large N and almost surely). This is:
yH(YHY −zIn)−1y ≃1
n tr(YHY −zIn)−1
where we non-rigorously use the symbol “≃” to mean “almost surely equal in
the large N limit.”
• Another lemma, the rank-1 perturbation lemma, will state that a perturbation
of rank 1 of the matrix YHY, e.g. the addition of the rank-1 matrix yyH to
YHY, does not aﬀect the value of 1
n tr(YHY −zIn)−1 in the large dimensional
limit. In particular, 1
n tr(YHY −zIn)−1 can be approximated in the large n

44
3. The Stieltjes transform method
limit by 1
n tr(XHX −zIn)−1. With our non-rigorous formalism, this is:
1
n tr(YHY −zIn)−1 ≃1
n tr(XHX −zIn)−1
an expression which is now independent of y, and in fact independent of the
choice of the column of XH, which is initially taken out. The same derivation
therefore holds true for any diagonal entry of (XXH −zIN)−1.
• But then, we know that 1
n tr(XXH −zIN)−1 can be written as a function of
1
n tr(XHX −zIn)−1 from Lemma 3.1. This is:
1
n tr(XHX −zIn)−1 = 1
n tr(XXH −zIN)−1 + N −n
n
1
z .
• It follows that each diagonal entry of (XXH −zIN)−1 can be written as a
function of 1
n tr(XXH −zIN)−1 itself. By summing up all N diagonal elements
and averaging by 1/N, we end up with an approximated relation between
1
N tr(XXH −zIN)−1 and itself
1
N tr
 XXH −zIN
−1 ≃
1
1 −N
n −z −z N
n
1
N tr (XXH −zIN)−1
which is asymptotically exact almost surely.
• Since this appears to be a second order polynomial in
1
N tr
 XXH −zIN
−1,
this can be solved and we end up with an expression of the limiting Stieltjes
transform of XXH. From Theorem 3.1, we ﬁnally ﬁnd the explicit form of the
l.s.d. of F XXH, i.e. the Mar˘cenko–Pastur law.
We now proceed to the thorough proof of Theorem 2.13. For this, we restrict
the entries of the random matrix X to have ﬁnite eighth order moment. The
extension to the general case can be handled in several ways. We will mention
after the proof the ideas behind a powerful technique known as the truncation,
centralization, and rescaling method, due to Bai and Silverstein [Silverstein and
Bai, 1995], which allows us to work with truncated random variables, i.e. random
variables on a bounded support (therefore having moments of all orders), in place
of the actual random variables. It will be shown why working with the truncated
variables is equivalent to working with the variables themselves and why the
general version of Theorem 2.13 unfolds.
3.2.1
Proof of the Mar˘cenko–Pastur law
We wish this proof to be complete in the sense that all notions of random matrix
theory and probability theory tools are thoroughly detailed. As such, the proof
contains many embedded lemmas, with sometimes further embedded results. The
reader may skip most of these secondary results for ease of read. Those lemmas
are nonetheless essential to the understanding of the basics of random matrix
theory using the Stieltjes transform and deserve, as such, a lengthy explanation.

3.2. The Mar˘cenko–Pastur law
45
Let X ∈CN×n be a random matrix with i.i.d. entries of zero mean, variance
1/n, and ﬁnite eighth order moment, and denote RN = XXH. We start by
singling out the ﬁrst row yH ∈C1×n of X, and we write
X ≜
yH
Y

.
Now, for z ∈C+, we have
(RN −zIN)−1 =
yHy −z
yHYH
Yy
YYH −zIN−1
−1
(3.9)
the trace of which is the Stieltjes transform of F RN . Our interest being to
compute this Stieltjes transform, and then the sum of the diagonal elements
of the matrix in (3.9), we start by considering the entry (1, 1). For this, we need
a classical matrix inversion lemma
Lemma 3.3. Let A ∈CN×N, D ∈Cn×n be invertible, and B ∈CN×n, C ∈
Cn×N. Then we have:
A B
C D
−1
=

(A −BD−1C)−1
−A−1B(D −CA−1B)−1
−(A −BD−1C)−1CA−1
(D −CA−1B)−1

.
(3.10)
We apply Lemma 3.3 to the block matrix (3.9) to obtain the upper left entry

−z + yH 
IN −YH  YYH −zIN−1
−1 Y

y
−1
.
From the relation IN −AN(IN + BNAN)−1BN = (IN + ANBN)−1, this
further expresses as
 −z −zyH(YHY −zIn)−1y
−1 .
We then have
h
(RN −zIN)−1i
11 =
1
−z −zyH(YHY −zIn)−1y.
(3.11)
To go further, we need an additional result, proved initially in [Bai and
Silverstein, 1998], that we formulate in the following theorem.
Theorem 3.4. Let A1, A2, . . ., with AN ∈CN×N, be a series of matrices with
uniformly bounded spectral norm. Let x1, x2, . . ., with xN ∈CN, be random
vectors with i.i.d. entries of zero mean, variance 1/N, and eighth order moment
of order O(1/N 4), independent of AN. Then
xH
NANxN −1
N tr AN
a.s.
−→0
(3.12)
as N →∞.

46
3. The Stieltjes transform method
We mention besides that, in the case where the quantities involved are real
and not complex, the entries of xN have fourth order moment of order O(1/N 2)
and AN has l.s.d. A, a central limit of the variations of xT
NANxN −1
N tr AN is
proved in [Tse and Zeitouni, 2000], i.e.
√
N

xT
NANxN −1
N tr AN

⇒Z
(3.13)
as N →∞,4 with Z ∼N(0, v), for some variance v depending on the l.s.d. F A
of AN and on the fourth moment of the entries of xN as follows.
v = 2
Z
t2dF A(t) + (E[x4
11] −3)
Z
tdF A(t)
2
.
Intuitively, taking for granted that xH
NANxN does have a limit, this limit must
coincide with the limit of E[xH
NANxN]. But for ﬁnite N
E[xH
NANxN] =
N
X
i=1
N
X
j=1
AN,ijE[x∗
N,ixN,j]
= 1
N
N
X
i=1
N
X
j=1
AN,ijδj
i
= 1
N tr AN
which is the expected result. We hereafter provide a rigorous proof of the
almost sure limit, which has the strong advantage to introduce very classical
probability theoretic tools which will be of constant use in the detailed proofs of
the important results of this book. This proof may be skipped in order not to
disrupt the ﬂow of the proof of the Mar˘cenko–Pastur law.
Proof. We start by introducing the following two fundamental results of
probability theory. The ﬁrst result is known as the Markov inequality.
Theorem 3.5 (Markov Inequality, (5.31) of [Billingsley, 1995]). For X a real
random variable, α > 0, we have for all integer k
P({ω, |X(ω)| ≥α}) ≤1
αk E

|X|k
.
The second result is the ﬁrst Borel–Cantelli lemma.
Theorem 3.6 (First Borel–Cantelli Lemma, Theorem 4.3 in [Billingsley, 1995]).
Let {AN} be F-sets of Ω. If P
N P(AN) < ∞, then P(lim supN AN) = 0. When
AN has a limit, this implies that P(limN AN) = 0.
The symbol lim supN AN stands for the set T
k≥0
S
n≥k An. An element ω ∈Ω
belongs to lim supN AN if, for all integer k, there exists N ≥k such that ω ∈AN,
4 The notation XN ⇒X for XN and X random variables, with distribution function FN and
F, respectively, is equivalent to FN ⇒F.

3.2. The Mar˘cenko–Pastur law
47
i.e. ω ∈lim supN AN if ω belongs to inﬁnitely many sets AN. Informally, an event
AN such that P(lim supN AN) = 0 is an event that, with probability one, does
not happen inﬁnitely often (denoted i.o.). The set lim supN AN is sometimes
written AN i.o.
The technique to prove Theorem 3.4 consists in ﬁnding an integer k such that
E
"xH
NANxN −1
N tr AN

k#
≤fN
(3.14)
where fN is constant independent of both AN and xN, such that P
N fN < ∞.
Then, for some ε > 0, from the Markov inequality, we have that
P({ω, YN(ω) ≥ε}) ≤1
εk E

Y k
N

(3.15)
with YN ≜
xH
NANxN −(1/N) tr AN
. Since the right-hand side of (3.15) is
summable, it follows from the ﬁrst Borel–Cantelli lemma, Theorem 3.6, that
P({ω, YN(ω) ≥ε i.o.}) = 0.
Since ε > 0 was arbitrary, the above is true for all rational ε > 0. Because the
countable union of sets of probability zero is still a set of probability zero (see
[Billingsley, 1995]), we ﬁnally have that
P


[
(p,q)∈(N∗)2

ω, YN(ω) ≥p
q i.o.

= 0.
The complementary of the set in parentheses above satisﬁes: for all (p, q)
there exists N0(ω) such that, for all N ≥N0(ω), |YN(ω)| ≤p
q . This set has
probability one, and therefore YN has limit zero with probability one, i.e.
xH
NANxN −1
N tr AN
a.s.
−→0. It therefore suﬃces to ﬁnd an integer k such that
(3.14) is satisﬁed.
For k = 4, expanding xH
NANxN as a sum of terms xH
i xjAi,j and distinguishing
the cases when i = j or i ̸= j, we have:
E
"xH
NANxN −1
N tr AN

4#
≤
8
N 4

E
" N
X
i=1
AN,ii(|xN,i|2 −1)
#4
+ E

X
i̸=j
AN,ijx∗
N,ixN,j


4

where the inequality comes from: |x + y|k ≤(|x| + |y|)k ≤2k−1(|x|k + |y|k). The
latter arises from the H¨older’s inequality, which states that, for p, q > 0 such
that 1/p + 1/q = 1, applied to two sets x1, . . . , xN and y1, . . . , yN [Billingsley,

48
3. The Stieltjes transform method
1995]
N
X
n=1
|xnyn| ≤
 N
X
n=1
|xn|p
!1/p  N
X
n=1
|yn|q
!1/q
taking N = 2, x1 = x, x2 = y, y1 = y2 = 1 and p = 4, we have immediately the
result. Since the xi have ﬁnite eighth order moment (and therefore ﬁnite kth
order moment of all k ≤8) and that AN has uniformly bounded norm, all the
terms in the ﬁrst sum are ﬁnite. Now, expanding the sum as a 4-fold sum, the
number of terms that are non-identically zeros is of order O(N 2). The second
sum is treated identically, with an order of O(N 2) non-identically null terms.
Therefore, along with the factor 1/N 4 in front, there exists K > 0 independent
of N, such that the sum is less than K/N 2. This is summable and we have proved
Theorem 3.4, when the xk have ﬁnite eighth order moment.
Remark 3.1. Before carrying on the proof of the Mar˘cenko–Pastur law, we take
the opportunity of the introduction of the trace lemma to mention the following
two additional results. The ﬁrst result is an extension of the trace lemma to the
characterization of xHAy for independent x, y vectors.
Theorem 3.7. For A1, A2, . . ., with AN ∈CN×N with uniformly bounded spectral
norm, x1, x2, . . . and y1, y2, . . . two series of i.i.d. variables such that xN ∈CN
and yN ∈CN have zero mean, variance 1/N, and fourth order moment of order
O(1/N 2), we have:
xH
NANyN
a.s.
−→0.
Proof. The above unfolds simply by noticing that E|xH
NANyN|4 ≤c/N 2 for some
constant c. We give below the precise derivation for this rather easy case.
E
hxH
NANyN
4i
= E


X
i1,...,i4
j1,...,j4
x∗
i1xi2x∗
i3xi4yj1y∗
j2yj3y∗
j4Ai1,j1A∗
i2,j2Ai3,j3A∗
i4,j4

.
If one of the xik or yjk appears an odd number of times in one of the terms of
the sum, then the expectation of this term is zero. We therefore only account
for terms xik and yjk that appear two or four times. If i1 = i2 = i3 = i4 and
j1 = j2 = j3 = j4, then:
E

x∗
i1xi2x∗
i3xi4yj1y∗
j2yj3y∗
j4Ai1,j1A∗
i2,j2Ai3,j3A∗
i4,j4

=
1
N 4 |Ai1,j1|4E[|x1|4]E[|y1|4]
= O(1/N 4).
Since there are as many as N 2 such conﬁgurations of i1, . . . , i4 and j1, . . . , j4,
these terms contribute to an order O(1/N 2) in the end. If i1 = i2 = i3 = i4 and

3.2. The Mar˘cenko–Pastur law
49
j1 = j3 ̸= j2 = j4, then:
E

x∗
i1xi2x∗
i3xi4yj1y∗
j2yj3y∗
j4Ai1,j1A∗
i2,j2Ai3,j3A∗
i4,j4

=
1
N 4 |Ai1,j1|2|Ai1,j3|2E[|x1|4]
= O(1/N 4).
Noticing that P
i1,j1,j3 |Ai1,j1|2|Ai1,j3|2 = P
i1,j1 |Ai1,j1|2(P
j3 |Ai1,j3|2), and that
P
i,j |Ai,j|2 = tr ANAH
N = O(N) from the bounded norm condition on AN, we
ﬁnally have that the sum over all possible i1 and j1 ̸= j3 is of order O(1/N 2). The
same is true for the combination j1 = j2 = j3 = j4 and i1 = i3 ̸= i2 = i4, which
results in a term of order O(1/N 2). It remains the case when i1 = i3 ̸= i2 = i4
and j1 = j3 ̸= j2 = j4. This leads to the terms
E

x∗
i1xi2x∗
i3xi4yj1y∗
j2yj3y∗
j4Ai1,j1A∗
i2,j2Ai3,j3A∗
i4,j4

=
1
N 4 |Ai1,j1|2|Ai2,j3|2
= O(1/N 4).
Noticing that P
i1,i3,j1,j3 |Ai1,j1|2|Ai2,j3|2 = P
i1,j1 |Ai1,j1|2(P
i3,j3 |Ai1,j3|2) from
the same argument as above, we have that the last term is also of order O(1/N 2).
Therefore, the total expected sum is of order O(1/N 2). The Markov inequality
and the Borel–Cantelli lemma give the ﬁnal result.
The second result is a generalized version of the fourth order moment
inequality that led to the proof of the trace lemma above, when the entries
of xN = (xN,1, . . . , xN,N)T have moments of all orders. This result unfolds from
the same combinatorics calculus as presented in the proof of Theorem 3.7.
Theorem 3.8 (Lemma B.26 of [Bai and Silverstein, 2009]). Under the conditions
of Theorem 3.4, if for all N, k, E[|
√
NxN,k|m] ≤νm, then, for all p ≥1
E
xH
NANxN −1
N tr AN

p
≤Cp
N p
h
(ν4 tr(AAH))
p
2 + ν2p tr(AAH)
p
2
i
for Cp a constant depending only on p.
Returning to the proof of the Mar˘cenko–Pastur law, in (3.11), y ∈Cn is
extracted from X, which has independent columns, so that y is independent
of Y. Besides, y has i.i.d. entries of variance 1/n. For large n, we therefore have

h
(RN −zIN)−1i
11 −
1
−z −z 1
n tr(YHY −zIn)−1

a.s.
−→0
where the convergence is ensured by verifying that the denominator of the
diﬀerence has imaginary part uniformly away from zero.
We feel at this point that, for large n, the normalized trace 1
n tr(YHY −zIn)−1
should not be much diﬀerent from
1
n tr(XHX −zIn)−1, since the diﬀerence
between both matrices here is merely the rank-1 matrix yyH. This is formalized
in the following second theorem.

50
3. The Stieltjes transform method
Theorem 3.9 ([Silverstein and Bai, 1995]). For z ∈C \ R+, we have the
following quadratic form identities.
(i) Let z ∈C \ R, A ∈CN×N, B ∈CN×N with B Hermitian, and v ∈CN. Then
tr
 (B −zIN)−1 −(B + vvH −zIN)−1
A
 ≤∥A∥
|ℑ[z]|
with ∥A∥the spectral norm of A.
(ii) Moreover, if B is non-negative deﬁnite, for z ∈R−
tr
 (B −zIN)−1 −(B + vvH −zIN)−1
A
 ≤∥A∥
|z| .
This theorem can be further reﬁned for z ∈R+. Generally speaking, it is
important to take z away from the support of the eigenvalues of B and B + vvH
to make sure that both matrices remain invertible. With z purely complex, the
position of the eigenvalues of B and B + vvH on the real line does not matter,
and similarly for B non-negative deﬁnite and real z < 0, the position of the
eigenvalues of B does not matter.
In the present situation, A = In and therefore, irrespective of the actual
properties of y (it might even be a degenerated vector with large Euclidean
norm), we have
1
n tr(YHY −zIn)−1 −mF XHX(z)
= 1
n tr(YHY −zIn)−1 −1
n tr(XHX −zIn)−1
= 1
n tr(YHY −zIn)−1 −1
n tr(YHY + yyH −zIn)−1
→0
and therefore:
h
(RN −zIN)−1i
11 −
1
−z −zmF XHX(z)
a.s.
−→0.
By the deﬁnition of the Stieltjes transform, since the non-zero eigenvalues of
RN = XXH and XHX are the same, we have from Lemma 3.1
mF XHX(z) = N
n mF RN (z) + N −n
n
1
z
(3.16)
which leads to
h
(RN −zIN)−1i
11 −
1
1 −N
n −z −z N
n mF RN (z)
a.s.
−→0.
(3.17)
The second term in the diﬀerence is independent on the initial choice of the
entry (1, 1) in (RN −zIN)−1. Due to the symmetric structure of X, the result
is also true for all diagonal entries (i, i), i = 1, . . . , N. Summing them up and

3.2. The Mar˘cenko–Pastur law
51
averaging, we conclude that5
mF RN (z) −
1
1 −N
n −z −z N
n mF RN (z)
a.s.
−→0.
(3.18)
Take R1, R2, . . . a particular sequence for which (3.18) is veriﬁed (such
sequences lie in a space of probability one). Since mF RN (z) ≤1/ℑ[z] from
Theorem 3.2, the sequence mF R1(z), mF R2(z), . . . is uniformly bounded in a
compact set. Consider now any subsequence mF
Rψ(1)(z), mF
Rψ(2)(z), . . .; along
this subsequence, (3.18) is still valid. Since mF
Rψ(n) is uniformly bounded
from above, we can select a further subsequence mF
Rφ(ψ(1)), mF
Rφ(ψ(2)), . . . of
mF
Rψ(1)(z), mF
Rψ(2)(z), . . . which converges (this is an immediate consequence
of the Bolzano–Weierstrass theorem). Its limit, call it m(z; φ, ψ) is still a Stieltjes
transform, as can be veriﬁed from Theorem 3.2, and is one solution of the implicit
equation in m
m =
1
1 −c −z −zcm.
(3.19)
The form of the implicit Equation (3.19) is often the best we can obtain more
involved models than i.i.d. X matrices. It will indeed often turn out that no
explicit equation for the limiting Stieltjes transform mF (which we have not yet
proved exist) will be available. Additional tools will then be required to ensure
that (3.19) admits either (i) a unique scalar solution, when seen as an equation
in the dummy variable m, or (ii) a unique functional solution, when seen as an
equation in the dummy function-variable m of z. The diﬀerence is technically
important for practical applications. Indeed, if we need to recover the Stieltjes
transform of a d.f. F (for instance to evaluate its associated Shannon transform),
it is important to know whether a classical ﬁxed-point algorithm is expected to
converge to solutions of (3.19) other than the desired solution. This will be
discussed further later in this section.
For the problem at hand, though, (3.19) can be rewritten as the second order
polynomial m(1 −c −z −zcm) = 0 in the variable m, a unique root of which
is the Stieltjes transform of a distribution function taken at z. This limit is
of course independent of the choice of φ and ψ. Therefore, any subsequence of
mF R1(z), mF R2(z), . . . admits a further subsequence, which converges to some
value mF (z), which is the Stieltjes transform of some distribution function
F. Therefore, from the semi-converse of the Bolzano–Weierstrass theorem,
mF R1(z), mF R2(z), . . . converges to mF (z). The latter is given explicitly by
mF (z) = 1 −c
2cz −1
2c −
p
(1 −c −z)2 −4cz
2cz
(3.20)
5 We use here the fact that the intersection of countably many sets of probability one on which
the result holds is itself of probability one.

52
3. The Stieltjes transform method
where the branch of
p
(1 −c −z)2 −4cz is chosen such that mF (z) ∈C+ for
z ∈C+, mF (z) ∈C−for z ∈C−and mF (z) > 0 for z < 0.6
Using the inverse-Stieltjes transform formula (3.2), we then verify that mF RN
has a limit mF , the Stieltjes transform of the Mar˘cenko–Pastur law, with density
F ′(x) = (1 −c−1)+δ(x) +
1
2πcx
p
(x −a)+(b −x)+
where a = (1 −√c)2 and b = (1 + √c)2. The term
1
2x
p
(x −a)+(b −x)+ is
obtained by computing limy→0 mF (x + iy), and taking its imaginary part. The
coeﬃcient c is then retrieved from the fact that we know ﬁrst how many zero
eigenvalues should be added and second that the density should integrate to 1.
To prove that the almost sure convergence of mF RN (z)
a.s.
−→mF (z) induces the
weak convergence F RN ⇒F with probability one, we ﬁnally need the following
theorem.
Theorem 3.10 (Theorem B.9 of [Bai and Silverstein, 2009]). Let {FN} be a set
of bounded real functions such that limx→−∞FN(x) = 0. Then, for all z ∈C+
lim
N→∞mFN (z) = mF (z)
if and only if there exists F such that limx→−∞F(x) = 0 and |FN(x) −F(x)| →0
for all x ∈R.
Proof. For z ∈C+, the function f : (x, z) 7→
1
z−x
is continuous and tends
to zero when |x| →∞. Therefore, |FN(x) −F(x)| →0 for all x ∈R implies
that
R
1
z−xd(FN −F)(x) →0. Conversely, from the inverse Stieltjes transform
formula (3.2), for a, b continuity points of F, |(FN(b) −FN(a)) −(F(b) −
F(a))| ≤1
π limy→0+ R b
a ℑ|(mN −m)(x + iy)|dx, which tends to zero as N grows
large. Therefore, we have:
|FN(x) −F(x)| ≤|(FN(x) −F(x)) −(FN(a) −F(a))| + |FN(a) −F(a)|
which tends to zero as we take, e.g. a = N and N →∞(since both FN(a) →1
and F(a) →1).
The sure convergence of the Stieltjes transform mFN to mF therefore ensures
that FN ⇒F and conversely. This is the one theorem, along with the inversion
formula (3.2), that fully justiﬁes the usage of the Stieltjes transform to study the
convergence of probability measures.
Back to the proof of the Mar˘cenko–Pastur law, we have up to now proved that
mF RN (z) −mF (z)
a.s.
−→0
6 We use a minus sign here in front of
√
(1−c−z)2−4cz
2cz
for coherence with the principal square
root branch when z < 0.

3.2. The Mar˘cenko–Pastur law
53
for some initially ﬁxed z ∈C+. That is, there exists a subspace Cz ∈F, with
(Ω, F, P) the probability space generating the series R1, R2, . . ., such that
P(Cz) = 1 for which ω ∈Cz implies that mF RN (ω)(z) −mF (z) →0. Since we
want to prove the almost sure weak convergence of F RN to the Mar˘cenko–Pastur
law, we need to prove that the convergence of the Stieltjes transform holds for
all z ∈C \ R+ on a common space of probability one. We then need to show
that there exists C ∈F, with P(C) = 1 such that, for all z ∈C+, ω ∈C implies
that mF RN (ω)(z) −mF (z) →0. This requires to use Vitali’s convergence theorem
[Titchmarsh, 1939] on a countable subset of C+ (or alternatively, Montel’s
theorem).
Theorem 3.11. Let f1, f2, . . . be a sequence of functions, analytic on a region
D ⊂C, such that |fn(z)| ≤M uniformly on n and z ∈D. Further, assume that
fn(zj) converges for a countable set z1, z2, . . . ∈D having a limit point inside D.
Then fn(z) converges uniformly in any region bounded by a contour interior to
D. This limit is furthermore an analytic function of z.
The convergence mF RN (z) −mF (z)
a.s.
−→0 is valid for any z inside a bounded
region of C+. Take countably many z1, z2, . . . having a limit point in some
compact region of C+. For each i, we have mF RN (ω)(zi) −mF (zi)
a.s.
−→0 for
ω ∈Czi, some set with P(Czi) = 1. The set C = S
i Czi ∈F over which the
convergence holds for all zi has probability one, as the countable union of sets
of probability one. As a consequence, for ω ∈C, mF RN (ω)(ω)(zi) −mF (zi) →0
for any i. From Vitali’s convergence theorem, since mF RN (z) −mF (z) is clearly
an analytic function of z, this holds true uniformly in all sets interior to regions
where mF RN (z) and mF (z) are uniformly bounded, i.e. in all regions that exclude
the real positive half-line. From Theorem 3.10, F RN(ω)(x) −F(x) →0 for all
x ∈R and for all ω ∈C, so that, for ω ∈C, F RN(ω)(x) −F(x) →0. This ensures
that F RN ⇒F almost surely. This proves the almost sure weak convergence of
the e.s.d. of RN to the Marc˘enko-Pastur law.
The reason why the proof above constrains the entries of X to have ﬁnite
eighth order moment is due to the trace lemma, Theorem 3.4, which is only
proved to work with this eighth order moment assumption. We give below a
generalization of Theorem 3.4 when the random variables under consideration
are uniformly bounded in some sense, no longer requiring the ﬁnite eighth order
moment assumption. We will then present the truncation, centralization, and
rescaling steps, which allow us to prove the general version of Theorem 2.13.
These steps consist in replacing the variables Xij by truncated versions of these
variables, i.e. replacing Xij by zero whenever |Xij| exceeds some predeﬁned
threshold. The centralization and centering steps are then used to recenter the
modiﬁed Xij around its mean and to preserve its variance. The main objective
here is to replace variables, which may not have bounded moments, by modiﬁed
versions of these variables that have moments of all orders. The surprising result
is that it is asymptotically equivalent to consider Xij or their altered versions;

54
3. The Stieltjes transform method
as a consequence, in many practical situations, it is unnecessary to make any
assumptions on the existence of any moments of order higher than 2 for Xij. We
will subsequently show how this operates and why it is suﬃcient to work with
supportly compacted random Xij variables.
3.2.2
Truncation, centralization, and rescaling
A convenient trace lemma for truncated variables can be given as follows.
Theorem 3.12. Let {A1, A2, . . .}, AN ∈CN×N, be a series of matrices of
growing sizes and {x1, x2, . . .}, xN ∈CN, be random vectors with i.i.d. entries
bounded by N −1
2 log N, with zero mean and variance 1/N, independent of AN.
Then
E
"xH
NANxN −1
N tr AN

6#
≤K ∥AN∥6 log12 N
N 3
for some constant K independent of N.
The sixth order moment here is upper bounded by a bound on the values of
the entries of the xk instead of a bound on the moments. This alleviates the
consideration of the existence of any moment on the entries. Note that a similar
result for the fourth order moment also exists that is in general suﬃcient for
practical purposes, but, since going to higher order moments is now immaterial,
this result is slightly stronger. The proof of Theorem 3.12 unfolds from the same
manipulations as for Theorem 3.4.
Obviously, applying the Markov inequality, Theorem 3.5, and the Borel–
Cantelli lemma, Theorem 3.6, the result above implies the almost sure
convergence of the diﬀerence xH
NANxN −1
N tr AN to zero. A second obvious
remark is that, if the elements of
√
NxN are bounded by some constant C
instead of log(N), the result still holds true.
The following explanations follow precisely Section 3.1.3 in [Bai and
Silverstein, 2009]. We start by the introduction of two important lemmas.
Lemma 3.4 (Corollary A.41 in [Bai and Silverstein, 2009]). For A ∈CN×n and
B ∈CN×n
L4 
F AAH, F BBH
≤2
N tr(AAH + BBH) 1
N tr([A −B][A −B]H)
where L(F, G) is the L´evy distance between F and G, given by:
L(F, G) ≜inf {ε, ∀x ∈R, F(x −ε) −ε ≤G(x) ≤F(x + ε) + ε} .
The L´evy distance can be thought of as the length of the side of the largest
square that can ﬁt between the functions F and G. Of importance to us presently

3.2. The Mar˘cenko–Pastur law
55
is the property that, for a sequence F1, F2, . . . of d.f., L(FN, F) →0 implies the
weak convergence of FN to F.
The second lemma is a rank inequality.
Lemma 3.5 (Theorem A.44 in [Bai and Silverstein, 2009]). For A ∈CN×n and
B ∈CN×n
F AAH −F BBH ≤1
N rank(A −B)
with ∥f∥≜supx |f(x)|.
We take the opportunity of the introduction of this rank inequality to mention
also the following useful result.
Lemma 3.6 (Lemma 2.21 and Lemma 2.23 in [Tulino and Verd´u, 2004]). For
A, B ∈CN×N Hermitian
F A −F B ≤1
N rank(A −B).
Also, denoting λX
1 ≤. . . ≤λX
N the ordered eigenvalues of the Hermitian X
1
N
N
X
i=1
(λA
i −λB
i )2 ≤1
N tr(A −B)2.
Returning to our original problem, let C be a ﬁxed positive real number. The
truncation step consists ﬁrst of generating a matrix ˆX with entries
1
√n ˆXN,ij
deﬁned as a function of the entries
1
√
N XN,ij of X as follows.
ˆXN,ij = XN,ij1{|XN,ij|<C}(XN,ij).
This is the truncation step that cuts oﬀthe tail of the distribution of X11.
Now, since the distribution of X11 is not necessarily centered around its mean,
we recenter it as follows. We create a further matrix ˜X with entries
1
√
N ˜XN,ij
such that
˜XN,ij = ˆXN,ij −E[ ˆXN,ij].
(3.21)
This is the centralization step.
The remaining problem is that the random variable X11 has lost through this
process some of its weight in the cut tails. So we need to further rescale the
resulting variable. For this, we create the variable ¯X, with entries ¯XN,ij deﬁned
as
¯XN,ij =
1
σ(C)
˜XN,ij
with σ(C) deﬁned as
σ(C)2 = E[| ˜XN,ij|].

56
3. The Stieltjes transform method
The idea now is to show that the limiting distribution of F RN is the same
whether we use the i.i.d. entries XN,ij or their truncated, centered, and rescaled
versions ¯XN,ij. If this is so, it is equivalent to work with the ¯XN,ij or with XN,ij
in order to derive the Mar˘cenko–Pastur law, with the strong advantage that in
the truncation process above no moment assumption was required. Therefore,
if we can prove that F RN converges to the Mar˘cenko–Pastur law with XN,ij
replaced by ¯XN,ij, then we prove the convergence of F RN for all distributions
of XN,ij without any moment constraint of higher order than 2. This last result
is straightforward as it simply requires to go through every step of the proof of
the Mar˘cenko–Pastur law and replace every call to the trace lemma, Theorem
3.4, by the updated trace lemma, Theorem 3.12. The remainder of this section
is dedicated to proving that the limiting spectrum of RN remains the same if
the XN,ij are replaced by ¯XN,ij.
We have from Lemma 3.4
L4 
F XXH, F
ˆX ˆXH
≤2

1
Nn
X
i,j
|XN,ij|2 + | ˆXN,ij|2



1
Nn
X
i,j
|XN,ij −ˆXN,ij|2


≤4

1
Nn
X
i,j
|XN,ij|2



1
Nn
X
i,j
|XN,ij|21{|XN,ij|>C}(XN,ij)

.
From the law of large numbers, both terms in the right-hand side tend to their
means almost surely, i.e.

1
Nn
X
i,j
|XN,ij|2



1
Nn
X
i,j
|XN,ij|21{|XN,ij|>C}(XN,ij)


a.s.
−→E

|XN,11|21{|XN,11|>C}(XN,ij)

.
Notice that this goes to zero as C grows large (since the second order moment
of XN,11 exists). Now, from Lemma 3.5 and Equation (3.21), we also have
F
ˆX ˆXH −F
˜X ˜XH ≤1
N rank(E[ ˆX]) = 1
N .
This is a direct consequence of the fact that the entries are i.i.d. and therefore
E[ ˆXN,ij] = E[ ˆXN,11], entailing that the matrix composed of the E[ ˆXN,ij] has unit
rank. The right-hand side of the inequality goes to zero as N grows large.
Finally, from Lemma 3.4 again
L4 
F
¯X ¯XH −F
˜X ˜XH
≤2

1 + σ(C)2
Nn
X
i,j
| ˜XN,ij|2



1 −σ(C)2
nN
X
i,j
| ˜XN,ij|2


the right-hand side of which converges to 2(1 −σ(C)4) N
n almost surely as N
grows large. Notice again that σ(C) converges to 1 as C grows large.
At this point, we go over the proof of the Mar˘cenko–Pastur law but with ¯X in
place of X. The derivations unfold identically but, thanks to Theorem 3.12, we

3.3. Stieltjes transform for advanced models
57
nowhere need any moment assumption further than the existence of the second
order moment of the entries of X. We then have that, almost surely
F
¯X ¯XH ⇒F.
But since the constant C, which deﬁnes ¯X, was arbitrary from the very
beginning, it can be set as large as we want. For x ∈R and ε > 0, we can take
C large enough to ensure that
lim sup
N
|F XXH(x) −F
¯X ¯XH(x)| < ε
for a given realization of X. Therefore
lim sup
N
|F XXH(x) −F(x)| < ε.
Since ε is arbitrary, we ﬁnally have
F XXH ⇒F
this event having probability one. This is our ﬁnal result.
The proof of the Mar˘cenko–Pastur law, with or without truncation steps,
can be applied to a large range of models involving random matrices with i.i.d.
entries. The ﬁrst known extension of the Mar˘cenko–Pastur law concerns the
l.s.d. of a certain class of random matrices, which contains in particular the
N × N sample covariance matrices Rn of the type (1.4), where the vector samples
xi ∈CN have covariance matrix R. This is presented in the subsequent section.
3.3
Stieltjes transform for advanced models
We recall that, if N is ﬁxed and n grows large, then we have the almost sure
convergence F Rn ⇒F R of the e.s.d. of the sample covariance matrix Rn ∈
CN×N originating from n observations towards the population covariance matrix
R. This is a consequence of the law of large numbers in classical probability
theory. This is not so if both n and N grow large with limit ratio n/N →c, such
that 0 < c < ∞. In this situation, we have the following result instead.
Theorem 3.13 ([Silverstein and Bai, 1995]). Let BN = AN + XH
NTNXN,
where XN =

1
√nXN,ij

i,j ∈CN×n with the XN,ij independent with zero mean,
unit variance, and ﬁnite moment of order 2 + ε for some ε > 0 (ε is independent
of N, i, j), TN ∈CN×N diagonal with real entries and whose e.s.d. F TN
converges weakly and almost surely to F T , and AN ∈Cn×n Hermitian whose
e.s.d. F TN converges weakly and almost surely to F A, N/n tends to c, with
0 < c < ∞as n, N grow large. Then, the e.s.d. of BN converges weakly and
almost surely to F B such that, for z ∈C+, mF B(z) is the unique solution with

58
3. The Stieltjes transform method
positive imaginary part of
mF B(z) = mF A

z −c
Z
t
1 + tmF B(z)dF T (t)

.
(3.22)
Moreover, if the XN has identically distributed entries, then the result holds
without requiring that a moment of order 2 + ε exists.
Remark 3.2. In [Bai and Silverstein, 2009] it is precisely shown that the non-
i.i.d. case holds if the random variables XN,ij meet a Lindeberg-like condition
[Billingsley, 1995]. The existence of moments of order 2 + ε implies that the
XN,ij meet the condition, hence the result. The Lindeberg-like condition states
exactly here that, for any ε > 0
1
N 2
X
ij
E
h
|XN,ij|2 · 1{|XN,ij|≥ε
√
N}(XN,ij)
i
→0.
These conditions merely impose that the distribution of XN,ij, for all pairs (i, j),
has light tails, i.e. large values have suﬃciently low probability.
In the case of Gaussian XN, TN can be taken Hermitian non-diagonal. This
is because the joint distribution of XN is in this particular case invariant by
right-unitary product, i.e. XNUN has the same joint entry distribution as XN
for any unitary matrix UN ∈CN×N. Therefore, TN can be replaced by any
Hermitian matrix UNTNUH
N for UN unitary. As previously anticipated for this
simple extension of the Mar˘cenko–Pastur law, mF B does not have a closed-form
expression.
The particular case when AN = 0 is interesting in many respects. In this case,
(3.22) becomes
mF (z) = −

z −c
Z
t
1 + tmF (z)dF T (t)
−1
(3.23)
where we denoted F ≜F B. This special notation will often be used in Section
7.1 to diﬀerentiate the l.s.d. F of the matrix T
1
2
NXNXH
NT
1
2
N from the l.s.d. F
of the reversed Gram matrix XH
NTNXN. Note indeed that, similar to (3.16),
the Stieltjes transform mF of the l.s.d. F of XH
NTNXN is linked to the Stieltjes
transform mF of the l.s.d. F of T
1
2
NXNXH
NT
1
2
N through
mF (z) = cmF (z) + (c −1)1
z
(3.24)
and then we also have access to a characterization of F, which is the asymptotic
eigenvalue distribution of the sample covariance matrix model introduced earlier
in (1.4), when the columns x′
1, . . . , x′
n of X′
N = √nT
1
2
NXN form a sequence of
independent vectors with zero mean and covariance matrix E[x′
1x′H
1 ] = TN, with
T
1
2
N a Hermitian square root of TN. Note however that, contrary to the strict
deﬁnition of the sample covariance matrix model, we do not impose identical
distributions of the vectors of X′
N here, but only identically mean and covariance.

3.3. Stieltjes transform for advanced models
59
In addition to the uniqueness of the pair (z, mF (z)) in the set {z ∈
C+, mF (z) ∈C+} solution of (3.23), an inverse formula for the Stieltjes
transform can be written in closed-form, i.e. we can deﬁne a function zF (m)
on {m ∈C+, zF (m) ∈C+}, such that
zF (m) = −1
m + c
Z
t
1 + tmdF T (t).
(3.25)
This will turn out to be extremely useful to characterize the spectrum of F.
More on this topic is discussed in Section 7.1 of Chapter 7. From a wireless
communication point of view, even if this is yet far from obvious, (3.25) is
the essential ingredient to derive in particular (N, n)-consistent estimates of
the diagonal entries with large multiplicities of the diagonal matrix P for the
channel matrix models Y = P
1
2 X ∈CN×n and also Y = HP
1
2 X + W ∈CN×n.
The latter, in which H, X and W have independent entries, can be used to model
the n sampled vectorial data Y = [y1, . . . , yn] received at an array of N sensors
originating from K sources with respective powers P1, . . . , PK gathered in the
diagonal entries of P. In this model, H ∈CN×K, X ∈CK×n and W ∈CN×n
may denote, respectively, the concatenated K channel vectors (in columns),
the concatenated K transmit data (in rows), and the additive noise vectors,
respectively. It is too early at this stage to provide any insight on the reason why
(3.25) is so fundamental here. More on this subject will be successively discussed
in Chapter 7 and Chapter 17.
We do not prove Theorem 3.13 in this section, which is a special case of
Theorem 6.1 in Chapter 6, for which we will provide an extended sketch of the
proof.
Theorem 3.13 was further extended by diﬀerent authors for matrix models
when either XN has i.i.d. non-centered elements [Dozier and Silverstein, 2007a],
XN has a variance proﬁle, i.e. with independent entries of diﬀerent variances,
and centered [Girko, 1990] or non-centered entries [Hachem et al., 2007], XN is a
sum of Gaussian matrices with separable variance proﬁle [Dupuy and Loubaton,
2009], BN is a sum of such matrices [Couillet et al., 2011a; Peacock et al., 2008],
etc. We will present ﬁrst the two best known results of the previous list, which
have now been largely generalized in the contributions just mentioned. The ﬁrst
result of importance is due to Girko [Girko, 1990] on XN matrices with centered
i.i.d. entries with a variance proﬁle.
Theorem 3.14. Let the complex N × n random matrix XN be composed of
independent entries

1
√nXN,ij

, such that XN,ij has zero mean, variance σ2
N,ij,
and the σN,ijXN,ij are identically distributed. Further, assume that the σ2
N,ij
are uniformly bounded. Assume that the σ2
N,ij converge, as N grows large, to a
bounded limit density pσ2(x, y), (x, y) ∈[0, 1)2, as n, N →∞, n/N →c. That is,
deﬁning pN,σ2(x, y) as
pN,σ2(x, y) ≜σ2
N,ij

60
3. The Stieltjes transform method
for i−1
N ≤x <
i
N and y−1
N
≤y <
j
N , pN,σ2(x, y) →pσ2(x, y), as N, n →∞. Then
the e.s.d. of BN = XNXH
N converges weakly and almost surely to a distribution
function F whose Stieltjes transform is given by:
mF (z) =
Z 1
0
u(x, z)dx
and u(x, z) satisﬁes the ﬁxed-point equation
u(x, z) =
"
−z +
Z c
0
pσ2(x, y)dy
1 +
R 1
0 u(x′, z)pσ2(x′, y)dx′
#−1
the solution of which being unique in the class of functions u(x, z) ≥0, analytic
for ℑ[z] > 0 and continuous on the plan section {(x, y), x ∈[0, 1]}.
Girko’s proof is however still not well understood.7 In Chapter 6, a more
generic form of Theorem 3.14 and a sketch of the proof are provided. This result
is fundamental for the analysis of the capacity of MIMO Gaussian channels with
a variance proﬁle. In a single-user setup, the usual Kronecker channel model is
a particular case of such a model, referred to as a channel model with separable
variance proﬁle. That is, σ2
ij can be written in this case as a separable product ritj
with r1, . . . , rN the eigenvalues of the receive correlation matrix and t1, . . . , tn
the eigenvalues of the transmit correlation matrix. However, the requirement
that the variance proﬁle {σ2
N,ij} converges to a limit density pσ2(x, y) for large
N is often an unpractical assumption. More useful and more general results,
e.g., [Hachem et al., 2007], that do not require the existence of a limit will be
discussed in Chapter 6, when introducing the so-called deterministic equivalents
for mF XN (z).
The second result of importance in wireless communications deals with the
information plus noise models, as follows.
Theorem 3.15 ([Dozier and Silverstein, 2007a]). Let XN be N × n with i.i.d.
entries of zero mean and unit variance, AN be N × n independent of XN such
that F
1
n ANAH
N ⇒H almost surely. Let also σ be a positive integer and denote
BN = 1
n(AN + σXN)(AN + σXN)H.
Then, for n, N →∞with n/N →c > 0, F BN ⇒F almost surely, where F is a
non-random distribution function whose Stieltjes transform mF (z), for z ∈C+,
is the unique solution of
mF (z) =
Z
dH(t)
t
1+σ2cmF (z) −(1 + σ2cmF (z))z + σ2(1 −c)
7 As Bai puts it [Bai and Silverstein, 2009]: “his proofs have puzzled many who attempt to
understand, without success, Girko’s arguments.”

3.4. Tonelli theorem
61
such that mF (z) ∈C+ and zmF (z) ∈C+.
It is rather clear why the model BN is referred to as information plus noise.
In practical applications, this result is used in various contexts, such as the
evaluation of the MIMO capacity with imperfect channel state information
[Vallet and Loubaton, 2009], or the capacity of MIMO channels with line-
of-sight components [Hachem et al., 2007]. Both results were generalized in
[Hachem et al., 2007] for the case of non-centered matrices with i.i.d. entries
with a variance proﬁle, of particular appeal in wireless communications since
it models completely the so-called Rician channels, i.e. non-centered Rayleigh
fading channels with a variance proﬁle. The works [Couillet et al., 2011a; Dupuy
and Loubaton, 2009; Hachem et al., 2007] will be further discussed in Chapters
13–14 as they provide asymptotic expressions of the capacity of very general
wireless models in MIMO point-to-point channels, MIMO frequency selective
Rayleigh fading channels, as well as the rate regions of multiple access channels
and broadcast channels. The technical tools required to prove the latter no
longer rely on the Mar˘cenko–Pastur approach, although the latter can be used to
provide an insight on the expected results. The main limitations of the Mar˘cenko–
Pastur approach will be evidenced when proving one of the aforementioned
results, namely the result of [Couillet et al., 2011a], in Chapter 6.
Before introducing an important central limit theorem, we make a small
digression about the Tonelli theorem, also known as Fubini theorem. This result
is of interest when we want to extend results that are known to hold for matrix
models involving deterministic matrices converging weakly to some l.s.d. to the
case when those matrices are now random, converging almost surely to some
l.s.d.
3.4
Tonelli theorem
The Tonelli theorem for probability spaces can be stated as follows.
Theorem 3.16 (Theorem 18.3 in [Billingsley, 1995]). If (Ω, F, P) and (Ω′, F′, P ′)
are two probability spaces, then for f an integrable function with respect to the
product measure Q on F × F′
Z
Ω×Ω′ f(x, y)Q(d(x, y)) =
Z
Ω
Z
Ω′ f(x, y)P ′(dy)

P(dx)
and
Z
Ω×Ω′ f(x, y)Q(d(x, y)) =
Z
Ω′
Z
Ω
f(x, y)P(dy)

P ′(dx).
Moreover, the existence of one of the right-hand side values ensures the
integrability of f with respect to the product measure Q.

62
3. The Stieltjes transform method
As an application, we mention that Theorem 3.13 was originally stated in
[Silverstein and Bai, 1995] for deterministic TN matrices with l.s.d. F T (and, as
a matter of fact, under the i.i.d. assumption for the entries of XN). In Theorem
3.13, though, we mentioned that F TN is random and converges to F T only in
the almost sure sense. In the following, assuming only the result from [Silverstein
and Bai, 1995] is known, i.e. denoting BN = AN + XH
NTNXN, F BN converges
almost surely to F for TN deterministic having l.s.d. F T , we wish to prove the
more general Theorem 3.13 in the i.i.d. case, i.e. F BN converges almost surely
to F for TN random having almost sure l.s.d. F T . This result is exactly due to
Theorem 3.16.
Indeed, call (X, X, PX) the probability space that engenders the series
X1, X2, . . ., of i.i.d. matrices and (T, T, PT ) the probability space that engenders
the random series T1, T2, . . .. Further denote (X × T, X × T, PX×T ) the product
space. Since BN is determined by XN and TN (assume AN deterministic), we
can write every possible sequence B1, B2, . . . = B1(x, t), B2(x, t), . . . for some
(x, t) ∈X × T. Therefore, what we need to prove here is that the space
A ≜
n
(x, t) ∈X × T, F BN(x,t) converges to F Bo
has probability one in the product space X × T. From the Tonelli theorem, it
follows that
P(A) =
Z
A
PX×T (d(x, t))
=
Z
X×T
1A(x, t)PX×T (d(x, t))
=
Z
T
Z
X
1A(x, t)PX(dx)

PT (dt).
Let t0 ∈T be a realization of T1, T2, . . . = T1(t0), T2(t0), . . . such that F TN(t0)
converges to F T . Since t0 is such that F TN(t0) converges to F T , we can apply
Theorem 3.13. Namely, the set of x such that (x, t0) ∈A has measure one, and
therefore, for this t0
Z
X
1A(x, t0)PX(dx) = 1.
Call B ⊂T the space of all t such that F TN(t) converges weakly to F T . We then
have
P(A) =
Z
B
PT (dt) +
Z
T \B
Z
X
1A(x, t)PX(dx)

PT (dt).
Since F TN converges to F T with probability one, PT (B) = 1, and therefore
P(A) ≥1, which is what we needed to show.
This trick is interesting as we can quite often reuse existing results stated for
deterministic models, and extend them to stochastic models with almost sure
statements. It will be further reused in particular to provide a generalization of

3.5. Central limit theorems
63
the trace lemma, Theorem 3.4, and the rank-1 perturbation lemma, Theorem
3.9, in the case when the matrices involved in both results do not have
bounded spectral norm but only almost sure bounded spectral norm for all large
dimensions. These generalizations are required to study zero-forcing precoders
in multiple input single output broadcast channels, see Section 14.1.
This closes this parenthesis on the Tonelli theorem. We return now to further
considerations of asymptotic laws of large dimensional matrices, and to the study
of (central) limit theorems.
3.5
Central limit theorems
Due to the intimate relation between the Stieltjes and Shannon transforms (3.5),
it is now obvious that the capacity of large dimensional communication channels
can be approximated using deterministic limits of the Stieltjes transform.
For ﬁnite dimensional systems, this however only provides a rather rough
approximation of quasi-static channel capacities or alternatively a rather
accurate approximation of ergodic channel capacities. No information about
outage capacities is accessible to this point, since the variations of the
deterministic limit F of the Stieltjes transform F XN of some matrix XN ∈CN×N
under study are unknown. To this end, we need to study more precisely the
ﬂuctuations of the random quantity
rN [mF XN (z) −mF (z)]
for some rate rN, increasing with N. For XN a sample covariance matrix, it turns
out that under some further assumptions on the moments of the distribution of
the random entries of XN, the random variable N [mF XN (z) −mF (z)] has a
central limit. This central limit generalizes to any well-behaved functional of
XN.
The ﬁrst central limit result for functionals of large dimensional random
matrices is due to Bai and Silverstein for the covariance matrix model, as follows.
Theorem 3.17 ([Bai and Silverstein, 2004]). Let XN =

1
√nXN,ij

ij ∈CN×n
have i.i.d. entries, such that XN,11 has zero mean, unit variance, and ﬁnite fourth
order moment. Let TN ∈CN×N be non-random Hermitian non-negative deﬁnite
with uniformly bounded spectral norm (with respect to N) for which we assume
that F TN ⇒H, as N →∞. We denote τ1 ≥. . . ≥τN the eigenvalues of TN.
Consider the random matrix
BN = T
1
2
NXNXH
NT
1
2
N
as well as
BN = XH
NTNXN.

64
3. The Stieltjes transform method
We know from Theorem 3.13 that F BN ⇒F for some distribution function F,
as N, n →∞with limit ratio c = limN N/n. Denoting FN this limit distribution
if the series F T1, F T2, . . . were to converge to H = F TN , let
GN ≜N

F BN −FN

.
Consider now k functions f1, . . . , fk deﬁned on R that are analytic on the
segment

lim inf
N τN1(0,1)(c)(1 −√c)2, lim sup
n τ11(0,1)(c)(1 + √c)2

.
Then, if (i) XN,11 is real, TN is real and E[(XN,11)4] = 3, or (ii) if XN,11 is
complex, E[(XN,11)2] = 0 and E[(XN,11)4] = 2, then the random vector
Z
f1(x)dGN(x), . . . ,
Z
fk(x)dGN(x)

converges
weakly
to
a
Gaussian
vector
(Xf1, . . . , Xfk)
with
means
(E[Xf1], . . . , E[Xfk]) and covariance matrix Cov(Xf, Xg), (f, g) ∈{f1, . . . , fk}2,
such that, in case (i)
E[Xf] = −1
2πi
I
f(z)
c
R
m(z)3t2(1 + tm)−3dH(t)
(1 −c
R
m(z)2t2(1 + tm(z))−2dH(t))2 dz
and
Cov(Xf, Xg) = −1
2πi
I I
f(z1)g(z2)
(m(z1) −m(z2))2 m′(z1)m′(z2)dz1dz2
while in case (ii) E[Xf] = 0 and
Cov(Xf, Xg) = −1
4πi
I I
f(z1)g(z2)
(m(z1) −m(z2))2 m′(z1)m′(z2)dz1dz2
(3.26)
for any couple (f, g) ∈{f1, . . . , fk}2, and for m(z) the Stieltjes transform of the
l.s.d. of BN. The integration contours are positively deﬁned with winding number
one and enclose the support of F.
The mean and covariance expressions of Theorem 3.17 are not easily
exploitable except for some speciﬁc f1, . . . , fk functions for which the integrals
(or an expression of the covariances) can be explicitly computed. For instance,
when f1, . . . , fk are taken to be fi(x) = xi and TN = IN, Theorem 3.17 gives
the central limit of the joint moments of the distribution N[F BN −F] with F
the Mar˘cenko–Pastur law.
Corollary 3.3 ([Bai and Silverstein, 2004]). Under the conditions of Theorem
3.17 with H = 1[1,∞), in case (ii), denote vN the vector of jth entry
(vN)j = tr(Bj
N) −NMj

3.5. Central limit theorems
65
where Mj is the limiting jth order moment of the Mar˘cenko–Pastur law, as
N →∞. Then, as N, n →∞with limit ratio N/n →c
vN ⇒v ∼CN(0, Q)
where the (i, j)th entry of Q is
Qi,j = ci+j
i−1
X
k1=0
j
X
k2=0
 i
k1
 j
k2
 1 −c
c
k1+k2
×
i−k1
X
l=1
l
2i −1 −k1 −l
i −1
2j −1 −k2 + l
j −1

.
We will see in Chapter 5 that, in the Gaussian case, combinatorial moment-
based approaches can also be used to derive the above result in a much faster
way. In case (ii), for Gaussian XN, the ﬁrst coeﬃcients of Q can be alternatively
evaluated as a function of the limiting free cumulants C1, C2, . . . of BN [Rao
et al., 2008], which will be introduced in Section 5.2. Explicitly, we have:
Q11 = C2 −C2
1
Q21 = −4C1C2 + 2C3
1 + 2C3
Q22 = 16C2
1C2 −6C2
2 −6C4
1 −8C1C3 + 4C4
(3.27)
with Ck deﬁned as a function of the moments Mk ≜limN
1
N tr Bk
N through
Equation (5.3).
Obtaining central limits requires somewhat elaborate tools, involving the
theory of martingales in particular, see Section 35 of [Billingsley, 1995]. An
important result for wireless communications, for which we will provide a sketch
of the proof using martingales, is the central limit for the log determinant of
Wishart matrices. In its full form, this is:
Theorem 3.18. Let XN ∈CN×n have i.i.d. entries

1
√nXN,ij

, such that XN,11
has zero mean, unit variance, and ﬁnite fourth order moment. Denote BN =
XNXH
N ∈CN×N. We know that, as N, n →∞with N/n →c, F BN ⇒F almost
surely with F the Mar˘cenko–Pastur law with ratio c. Then the Shannon transform
VBN (x) ≜
R
log(1 + xt)dF BN (t) of BN satisﬁes
N (VBN (x) −E [VBN (x)]) ⇒X ∼N(0, Θ2)
with
Θ2 = −log

1 −
cmF (−1/x)
(1 + cmF (−1/x))2

+ κ
cmF (−1/x)
(1 + cmF (−1/x))2
κ = E(XN,11)4 −2 and mF (z) the Stieltjes transform of the Mar˘cenko–Pastur
law (3.20).
Note that in the case where XN,11 is Gaussian, κ = 0.

66
3. The Stieltjes transform method
We hereafter provide a sketch of the proof of Theorem 3.18, along with a short
introduction to martingale theory.
Proof. Martingale theory requires notions of conditional probability.
Deﬁnition 3.7 ((33.8) of [Billingsley, 1995]). Let (Ω, F, P) be a probability
space, and X be a random variable on this probability space. Let G be a σ-
ﬁeld in F. For A ∈F, we denote P[A∥G] the random variable with realizations
P[A∥G]ω, ω ∈Ω, which is measurable G, integrable, and such that, for all G ∈G
Z
G
P[A∥G]dP = P(A ∩G).
If G is the ﬁnite set of the unions of the F-sets B1, . . . , BK, then, for ω ∈
Bi, P(Bi)P[A∥G]ω = P(A ∩Bi), i.e. P[A∥G]ω = P(A|Bi), consistently with the
usual deﬁnition of conditional probability. We can therefore see P[A∥G] as the
probability of the event A when the result of the experiment G is known. As
such, the σ-ﬁeld G ⊂F can be seen as an information ﬁlter, in the sense that it
brings a rougher vision of the events of Ω. Of interest to us is the extension of
this information ﬁltering under the form of a so-called ﬁltration F1 ⊂F2 ⊂. . .
for a given sequence of σ-ﬁeld F1, F2, . . . ⊂F.
Deﬁnition 3.8. Consider a probability space (Ω, F, P) and a ﬁltration F1 ⊂
F2 ⊂. . . with, for each i, Fi ⊂F. Deﬁne X1, X2, . . . a sequence of random
variables such that Xi is measurable Fi and integrable. Then X1, X2, . . . is a
martingale with respect to F1, F2, . . . if, with probability one
E [Xn+1∥Fn] = Xn
where E[X∥G] is the conditional expectation deﬁned, for G ∈G as
Z
G
E[X∥G]dP =
Z
G
XdP.
If X1, X2, . . . and X′
1, X′
2, . . . are both martingales with respect to F1, F2, . . .,
then X1 −X′
1, X2 −X′
2, . . . is a martingale diﬀerence relative to F1, F2, . . .. More
generally, we have the following deﬁnition.
Deﬁnition 3.9. If Z1, Z2, . . . is such that Zi is measurable Fi and integrable,
for a ﬁltration F1, F2, . . ., and satisﬁes
E[Zn+1∥Fn] = 0
then Z1, Z2, . . . is a martingale diﬀerence with respect to F1, F2, . . ..
Informally, this implies that, when the experiment Fn is known before
experiment Fn+1 has been realized, the expected observation Xn+1 at time
n + 1 is exactly equal to the observation Xn at time n. Note that, by taking
F = FN = FN+1 = . . ., the ﬁltration can be limited to a ﬁnite sequence.
The link to random matrix theory is the following: consider a random
matrix XN ∈CN×n with independent columns x1, . . . , xn ∈CN. The columns

3.5. Central limit theorems
67
xk(ω), . . . , xn(ω) of XN(ω), ω ∈Ω, can be thought of as the result of the
experiment Fn−k+1 that consists in unveiling successive matrix columns from
the last to the ﬁrst: F1 unveils xn(ω), F2 unveils xn−1(ω), etc. This results in
a ﬁltration F0 ⊂F1 ⊂. . . ⊂Fn, with F0 = {∅, Ω}. The reason why considering
martingales will be helpful to the current proof takes the form of Theorem 3.19,
introduced later, which provides a central limit result for sums of martingale
diﬀerences. Back to the hypotheses of Theorem 3.18, consider the above ﬁltration
built from the column space of XN and denote
αn,j ≜En−j+1

log det
 IN + xXNXH
N

−En−j

log det
 IN + xXNXH
N

where En−j+1[X] stands for E[X∥Fn−j+1] for X measurable Fn−j+1. For all j,
the αn,j satisfy
En−j+1 [αn,j] = 0
and therefore αn,n, . . . , α1,n is a martingale diﬀerence relative to the ﬁltration
F1 ⊂. . . ⊂Fn. Note now that the variable of interest, namely
βn ≜log det
 IN + xXNXH
N

−E

log det
 IN + xXNXH
N

satisﬁes
βn ≜
n
X
j=1
(αn,j −αn,j+1) .
Denoting X(j) ≜[x1, . . . , xj−1, xj+1, . . . , xn], notice that
En−j+1
h
log det

IN + xX(j)XH
(j)
i
= En−j
h
log det

IN + xX(j)XH
(j)
i
and therefore, adding and subtracting En−j+1
h
log det

IN + xX(j)XH
(j)
i
βn = En−j
log det

IN + xX(j)XH
(j)

log det
 IN + xXNXH
N
 −
n
X
j=1
En−j+1
log det

IN + xX(j)XH
(j)

log det
 IN + xXNXH
N

= En−j
log det

In−1 + xXH
(j)X(j)

log det
 In + xXH
NXN

−
n
X
j=1
En−j+1
log det

In−1 + xXH
(j)X(j)

log det
 In + xXH
NXN

=
n
X
j=1
En−j+1 log
 
1 + xH
j

X(j)XH
(j) + 1
xIN
−1
xj
!
−En−j log
 
1 + xH
j

X(j)XH
(j) + 1
xIN
−1
xj
!
where the last equality comes ﬁrst from the expression of a matrix inverse as a
function of determinant and cofactors
"
XH
NXN + 1
xIn
−1#
jj
=
det

XH
(j)X(j) + 1
xIn−1

det
 XH
NXN + 1
xIn


68
3. The Stieltjes transform method
and second by remembering from (3.11) that
"
XH
NXN + 1
xIn
−1#
jj
=
x
1 + xH
j (X(j)XH
(j) + 1
xIN)−1xj
.
Using now the fact that
En−j+1 log
 
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1!
= En−j log
 
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1!
then adding and subtracting
log(1) = log



1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1



this is further equal to
βn =
n
X
j=1
En−j+1 log



1 + xH
j

X(j)XH
(j) + 1
xIN
−1
xj
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1



−En−j log



1 + xH
j

X(j)XH
(j) + 1
xIN
−1
xj
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1



=
n
X
j=1
En−j+1
n−j
log



xH
j

X(j)XH
(j) + 1
xIN
−1
xj −1
n tr

X(j)XH
(j) + 1
xIN
−1
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1



=
n
X
j=1
γj
with En−j+1
n−j
X = En−j+1X −En−jX, and with
γj = En−j+1
n−j
log (1 + Aj)
≜En−j+1
n−j
log



xH
j

X(j)XH
(j) + 1
xIN
−1
xj −1
n tr

X(j)XH
(j) + 1
xIN
−1
1 + 1
n tr

X(j)XH
(j) + 1
xIN
−1


.
The sequence γn, . . . , γ1 is still a martingale diﬀerence with respect to the
ﬁltration F1 ⊂. . . ⊂Fn.
We now introduce the fundamental theorem, which justiﬁes the previous steps
that led to express βn as a sum of martingale diﬀerences.

3.5. Central limit theorems
69
Theorem 3.19 (Theorem 35.12 of [Billingsley, 1995]). Let δ1, . . . , δn be a
sequence of martingale diﬀerences with respect to the ﬁltration F1 ⊂. . . ⊂Fn.
Assume there exists Θ2 such that
n
X
j=1
E

δ2
j ∥Fj

→Θ2
in probability, as n →∞. Moreover assume that, for all ε > 0
n
X
j=1
E

δ2
j 1{|δj|≥ε}

→0.
Then
n
X
j=1
δj ⇒X ∼N(0, Θ2).
It therefore suﬃces here to prove that the Lindeberg-like condition is satisﬁed
and to determine Θ2. For simplicity, we only carry out this second step here.
Noticing that Aj is close to zero by the trace lemma, Theorem 3.4
n
X
j=1
En−jγ2
j ≃
n
X
j=1
En−j (En−j+1Aj −En−jAj)2
≃
n
X
j=1
En−j (En−j+1Aj)2
where we use the symbol ‘≃’ to denote that the diﬀerence in the terms on either
side tends to zero for large N and where
En−j (En−j+1Aj)2
≃
En−j

xH
j
 XNXH
N + 1
xIN
−1 xj −1
n tr

X(j)XH
(j) + 1
xIN
−12

1 + 1
n tr
 XNXH
N + 1
xIN
−12
.
Further calculus shows that the term in the numerator expands as
En−j
 
xH
j

XNXH
N + 1
xIN
−1
xj −1
n tr

X(j)XH
(j) + 1
xIN
−1!2
≃1
n2
 
tr

En−jX(j)XH
(j) + 1
xIN
−2
+ κ
N
X
i=1
En−j
"
X(j)XH
(j) + 1
xIN
−1#
ii
!
where the term κ appears, and where the term
1
n2 tr

En−jX(j)XH
(j) + 1
xIN
−2
further develops into
1
n2 tr

En−jX(j)XH
(j) + 1
xIN
−2
≃
1
n tr E
 XNXH
N −zIN
−1
1 −n−j−1
n
1
n tr E(XNXH
N−zIN)
−1

1+ 1
n tr E(XNXH
N−zIN)
−12
.

70
3. The Stieltjes transform method
Expressing the limits as a function of mF , we ﬁnally obtain
n
X
j=1
En−j(En−j+1Aj)2 →
1
(1 + cmF (−1/x))2
Z 1
0
cmF (−1/x)2
1 −(1 −y)
cmF (−1/x)
(1+cmF (−1/x))2
dy
+ κ
cmF (−1/x)2
(1 + mF (−1/x))2
from which we fall back on the expected result.
This completes this ﬁrst chapter on limiting distribution functions of some
large dimensional random matrices and the Stieltjes transform method. In the
course of this chapter, the Stieltjes transform has been illustrated to be a powerful
tool for the study of the limiting spectrum distribution of large dimensional
random matrices. A large variety of practical applications in the realm of
wireless communications will be further developed in Part II. However, while
i.i.d. matrices, with diﬀerent ﬂavors (with variance proﬁle, non-centered, etc.),
and Haar matrices have been extensively studied, no analytic result concerning
more structured matrices used in wireless communications, such as Vandermonde
and Hankel matrices, has been found so far. In this case, some moment-based
approaches are able to ﬁll in the gap by providing results on all successive
moments (when they exist). In the subsequent chapter, we will ﬁrst introduce
the basics of free probability theory, which encompasses a very diﬀerent theory of
large dimensional random matrices than discussed so far and which provides
a rather comprehensive framework for moment-based approaches, which will
then be extended in a further chapter. We will come back to the Stieltjes
transform methods for determining limiting spectral distributions in Chapter
6 in which we discuss more elaborated techniques than the Mar˘cenko–Pastur
method introduced here.

4
Free probability theory
In this chapter, we introduce free probability theory, a diﬀerent approach to
the already introduced tools for random matrix theory. Free probability will be
shown to provide a very eﬃcient framework to study limiting distributions of
some models of large dimensional random matrices with symmetric features.
Although to this day it does not overcome the techniques introduced earlier and
can only be applied to very few random matrix models, this approach has the
strong advantage of often being faster at determining the l.s.d. for these models.
In particular, some results are derived in a few lines of calculus in the following,
which are generalized in Chapter 6 using more advanced tools.
It is in general a diﬃcult problem to deduce the eigenvalue distribution of the
sum or the product of two generic Hermitian matrices A, B as a function of
the eigenvalue distributions of A and B. In fact, it is often impossible as the
eigenvectors of A and B intervene in the expression of the eigenvalues of their
sum or product. In Section 3.2, we provided a formula, Theorem 3.13, linking
the Stieltjes transform of the l.s.d. of the sum or the product of a deterministic
matrix (AN or TN) and a matrix XNXH
N, where XN has i.i.d. entries, to the
Stieltjes transform of the l.s.d. of AN or TN. In this section, we will see that
Theorem 3.13 can be derived in a few lines of calculus under certain symmetry
assumptions on XN. More general results will unfold from this type of calculus
under these symmetry constraints on XN.
The approach originates from the work of Voiculescu [Voiculescu et al.,
1992] on a very diﬀerent subject. Voiculescu was initially interested in the
mathematical description of a theory of probability on non-commutative
algebras, called free probability theory. The random variables here are elements of
a non-commutative probability space (A, φ), with A a non-commutative algebra
and φ a given linear functional. The algebra of Hermitian random matrices is
a particular case of such a probability space, for which the random variables,
i.e. the random matrices, do not commute with respect to the matrix product.
We introduce hereafter the basics of free probability theory that are required to
understand the link between algebras of non-commutative random variables and
random matrices.

72
4. Free probability theory
4.1
Introduction to free probability theory
We ﬁrst deﬁne non-commutative probability spaces.
Deﬁnition 4.1. A non-commutative probability space is a couple (A, φ) where
A is a non-commutative unital algebra, that is an algebra over C having a unit
denoted by 1, and φ : A →C is a linear functional such that φ(1) = 1.
When the functional φ satisﬁes φ(ab) = φ(ba), it is also called a trace. As will
appear below, the role of φ can be compared to the role of the expectation in
classical probability theory.
Deﬁnition 4.2. Let (A, φ) be a non-commutative probability space. In the
context of free probability, a random variable is an element a of A. We call
the distribution of a the linear functional ρa on C[X], the algebra of complex
polynomials in one variable, deﬁned by
ρa : C[X] →
C
P
7→φ (P(a)) .
The distribution of a non-commutative random variable a is characterized by
its moments, which are deﬁned to be the sequence φ(a), φ(a2), . . ., the successive
images by ρa of the normalized monomials of C[X]. The distribution of a non-
commutative random variable can often be associated with a real probability
measure µa in the sense that
φ(ak) =
Z
R
tkdµa(t)
for each k ∈N. In this case, the moments of all orders of µa are of course ﬁnite.
In free probability theory, it is more conventional to use probability distributions
µ instead of distribution functions F (we recall that F(x) = µ(−∞, x] if F is the
d.f. associated with the measure µ on R), so we will keep these notations in the
present section.
Consider the algebra AN of N × N random matrices whose entries are deﬁned
on some common probability space (meant in the classical sense) and have all
their moments ﬁnite. In what follows, we refer to X ∈AN as a random matrix
in the sense of Deﬁnition 2.1, and not as a particular realization X(ω), ω ∈Ω,
of X. A non-commutative probability space of random variables is obtained by
associating to AN the functional τN given by:
τN(X) = 1
N E (tr X) = 1
N
N
X
i=1
E[Xii]
(4.1)
with Xij the (random variable) entry (i, j) of X. This is obviously a trace in the
free probability sense. This space will be denoted by (AN, τN).

4.1. Introduction to free probability theory
73
Suppose X is a random Hermitian matrix with real random eigenvalues
λ1, . . . , λN. The distribution ρX of X is deﬁned by the fact that its action on
each monomial Xk of C[X] is given by
ρX(Xk) = τN(Xk) = 1
N
N
X
i=1
E[λk
i ].
This distribution is of course associated with the probability measure µX
deﬁned, for all bounded continuous f, by
Z
f(t)dµX(t) = 1
N
N
X
i=1
E[f(λk
i )].
The notion of distribution introduced in Deﬁnition 4.2 is subsequently
generalized to the case of multiple random variables. Let a1 and a2 be two
random variables in a non-commutative probability space (A, φ). Consider
non-commutative monomials in two indeterminate variables of the form
Xk1
i1 Xk2
i2 . . . Xkn
in , where for all j, ij ∈{1, 2} , kj ≥1 and ij ̸= ij+1. The algebra
C⟨X1, X2⟩of non-commutative polynomials with two indeterminate variables is
deﬁned as the linear span of the space containing 1 and the non-commutative
monomials. The joint distribution of a1 and a2 is then deﬁned as the linear
functional on C⟨X1, X2⟩satisfying
ρ :
C⟨X1, X2⟩
→
C
Xk1
i1 Xk2
i2 . . . Xkn
in 7→ρ(Xk1
i1 Xk2
i2 . . . Xkn
in ) = φ(ak1
i1 ak2
i2 . . . akn
in ).
More
generally,
denote
by
C⟨Xi | i ∈{1, . . . , I}⟩
the
algebra
of
non-
commutative polynomials in I variables, which is the linear span of 1 and the non-
commutative monomials of the form Xk1
i1 Xk2
i2 . . . Xkn
in , where kj ≥1 and i1 ̸= i2,
i2 ̸= i3, . . ., in−1 ̸= in are smaller than or equal to I. The joint distribution of
the random variables a1, . . . , aI in (A, φ) is the linear functional
ρ : C⟨Xi | i ∈{1, . . . , I}⟩−→
C
Xk1
i1 Xk2
i2 . . . Xkn
in
7−→ρ(Xk1
i1 Xk2
i2 . . . Xkn
in ) = φ(ak1
i1 ak2
i2 . . . akn
in ) .
In short, the joint distribution of the non-commutative random variables
a1, . . . , aI is completely speciﬁed by their joint moments. We now introduce the
important notion of freeness, which will be seen as the free probability equivalent
to the notion of independence in classical probability theory.
Deﬁnition 4.3. Let (A, φ) be a non-commutative probability space. A family
{A1, . . . , AI} of unital subalgebras of A is free if φ(a1a2 . . . an) = 0 for all n-uples
(a1, . . . , an) satisfying
1. aj ∈Aij for some ij ≤I and i1 ̸= i2, i2 ̸= i3, . . ., in−1 ̸= in.
2. φ(aj) = 0 for all j ∈{1, . . . , n}.

74
4. Free probability theory
A family of subsets of A is free if the family of unital subalgebras generated by
each one of them is free. Random variables {a1, . . . , an} are free if the family of
subsets {{a1}, . . . , {an}} is free.
Note that in the statement of condition 1, only two successive random variables
in the argument of φ(a1a2 . . . an) belong to two diﬀerent subalgebras. This
condition does not forbid the fact that, for instance, i1 = i3. Note in particular
that, if a1 and a2 belong to two diﬀerent free algebras, then φ(a1a2a1a2) = 0
whenever φ(a1) = φ(a2) = 0. This relation cannot of course hold if a1 and a2
are two real-valued independent random variables and if φ coincides with the
classical mathematical expectation operator. Therefore freeness, often referred
to as a free probability equivalent to independence, cannot be considered as a
non-commutative generalization of independence because algebras generated by
independent random variables in the classical sense are not necessarily free.
Let us make a simple computation involving freeness. Let A1 and A2 be two
free subalgebras in A. Any two elements a1 and a2 of A1 and A2, respectively,
can be written as ai = φ(ai)1 + a′
i (1 is here the unit of A), so φ(a′
i) = 0. Now
φ(a1a2) = φ ((φ(a1)1 + a′
1) (φ(a2)1 + a′
2)) = φ(a1)φ(a2).
In other words, the expectations of two free random variables factorize.
By decomposing a random variable ai into φ(ai)1 + a′
i, the principle of this
computation can be generalized to the case of more than two random variables
and to the case of higher order moments, and we can check that non
commutativity plays a central role there.
Theorem 4.1 ([Biane, 2003]). Let A1, . . . , AI be free subalgebras in (A, φ) and
let {a1, . . . , an} ⊂A be such that, for all j ∈{1, . . . , n}, aj ∈Aij, 1 ≤ij ≤I. Let
Π be the partition of {1, . . . , n} associated with the equivalence relation j ≡k ⇔
ij = ik, i.e. the random variables aj are gathered together according to the free
algebras to which they belong. For each partition π of {1, . . . , n}, let
φπ =
Y
{j1,...,jr}∈π
j1<...<jr
φ(aj1 . . . ajr).
There exists universal coeﬃcients c(π, Π) such that
φ(a1 . . . an) =
X
π≤Π
c(π, Π)φπ
where “π ≤Π” stands for “π is ﬁner than Π,” i.e. every element of π is a subset
of an element of Π.
The main consequence of this result is that, given a family of free algebras
A1, . . . , AI in A, only restrictions of φ to the algebras Ai are needed to compute
φ(a1 . . . an) for any a1, . . . , an ∈A such that, for all j ∈{1, . . . , n}, we have aj ∈
Aij, 1 ≤ij ≤I. The problem of computing explicitly the universal coeﬃcients

4.2. R- and S-transforms
75
c(π, Π) has been solved using a combinatorial approach and is addressed in
Section 5.2.
Let µ and ν be two compactly supported probability measures on [0, ∞).
Then, from [Hiai and Petz, 2006], it always exists two free random variables a1
and a2 in some non-commutative probability space (A, φ) having distributions
µ and ν, respectively. We can see that the distributions of the random variables
a1 + a2 and a1a2 depend only on µ and on ν. The reason for this is the
following: Deﬁnition 4.2 states that the distributions of a1 + a2 and a1a2 are
fully characterized by the moments φ ((a1 + a2)n) and φ ((a1a2)n), respectively.
To compute these moments, we just need the restriction of φ to the algebras
generated by {a1} and {a2}, according to Theorem 4.1. In other words,
φ ((a1 + a2)n) and φ ((a1a2)n) depend on the moments of a1 and a2 only.
As such, the distributions of a1 + a2 and a1a2 can, respectively, be associated
with probability measures called free additive convolution and free multiplicative
convolution of the distributions µ and ν of these variables. The free additive
convolution of µ and ν is denoted µ ⊞ν, while the free multiplicative convolution
of µ and ν is denoted µ ⊠ν. Both µ ⊞ν and µ ⊠ν are compactly supported on
[0, ∞), see, e.g. page 30 of [Voiculescu et al., 1992]. Also, both additive and
multiplicative free convolutions are commutative, e.g. µ ⊞ν = ν ⊞µ, and the
moments of µ ⊞ν and µ ⊠ν are related in a universal manner to the moments
of µ and to those of ν. We similarly denote µ ⊟ν the free additive deconvolution,
which is such that, if η = µ ⊞ν, then µ = η ⊟ν and ν = η ⊟µ, and µ  ν the
free multiplicative deconvolution which is such that, if η = ν ⊠ν, then µ = η  ν
and ν = η  µ.
In the following, we express the fundamental link between the operations µ ⊞ν
and µ ⊠ν and the R- and S-transforms, respectively.
4.2
R- and S-transforms
The R-transform, introduced in Deﬁnition 3.3 from the point of view of its
connection to the Stieltjes transform, fully characterizes µ ⊞ν as a function of
µ and ν. This is given by the following result.
Theorem 4.2. Let µ and ν be compactly supported probability measures of R.
Deﬁne the R-transform Rµ of the probability distribution µ by the formal series
Rµ(z) ≜
X
k≥1
Ckzk−1
where the C1, C2, . . . are iteratively evaluated from
Mn =
X
π∈NC(n)
Y
V ∈π
C|V |
(4.2)

76
4. Free probability theory
with
Mn ≜
Z
xnµ(dx)
the moment of order k of µ and NC(n) the set of non-crossing partitions of
{1, . . . , n}. Then, we have that
Rµ⊞ν(z) = Rµ(z) + Rν(z).
If X is a non-commutative random variable with probability measure µ, RX
will also denote the R-transform of µ.
More is said in Section 5.2 about NC(n), which plays an important role in
combinatorial approaches for free probability theory, in a similar way as P(n),
the set of partitions of {1, . . . , n}, which plays a fundamental role in classical
probability theory.
Similarly, the S-transform introduced in Deﬁnition 3.4 allows us to turn
multiplicative free convolution into a mere multiplication of power series. In
the context of free probability, the S-transform is precisely deﬁned through the
following result.
Theorem 4.3. Given a probability measure µ on R with compact support, let
ψµ(z) be the formal power series deﬁned by
ψµ(z) =
X
k≥1
zk
Z
tkdµ(t) =
Z
zt
1 −ztdµ(t).
(4.3)
Let χµ be the unique function analytic in a neighborhood of zero, satisfying
χµ(ψµ(z)) = z
for |z| small enough. Let also
Sµ(z) = χµ(z)1 + z
z
.
The function Sµ is called the S-transform of µ, introduced in Deﬁnition 3.4.
Moreover the S-transform Sµ⊠ν of µ ⊠ν satisﬁes
Sµ⊠ν = SµSν.
Similar to the R-transform, if X is a non-commutative random variable with
probability measure µ, SX will also denote the S-transform of µ.
Remark 4.1. We mention additionally that, as in classical probability theory, a
limit theorem for free random variables exists, and is given as follows.
Theorem 4.4 ([Bercovici and Pata, 1996]). Let A1, A2, . . . be a sequence of free
random variables on the non-commutative probability space (A, φ), such that, for
all i, φ(Ai) = 0, φ(A2
i ) = 1 and for all k, supi |φ(Ak
i )| < ∞. We then have, as

4.3. Free probability and random matrices
77
N →∞
1
√
N
(A1 + . . . + AN) ⇒A
where A is a random variable whose distribution has R-transform RA(z) = z.
This distribution is the semi-circle law of Figure 1.2, with density deﬁned in
(2.4).
The semi-circle law is then the free probability equivalent of the Gaussian
distribution.
Now that the basis of free probability theory has been laid down, we move to
the application of free probability theory to large dimensional random matrix
theory, which is the core interest of this chapter.
4.3
Free probability and random matrices
Voiculescu discovered very important relations between free probability theory
and random matrix theory. Non-diagonal random Hermitian matrices are clearly
non-commutative random variables. In [Voiculescu, 1991], it is shown that certain
independent matrix models exhibit asymptotic free relations.
Deﬁnition 4.4. Let {XN,1, . . . , XN,I} be a family of random N × N matrices
belonging to the non-commutative probability space (AN, τN) with τN deﬁned
in (4.1). The joint distribution has a limit distribution ρ on C⟨Xi|i ∈{1, . . . , I}⟩
as N →∞if
ρ(Xk1
i1 . . . Xkn
in ) = lim
N→∞τN(Xk1
N,i1 . . . Xkn
N,in)
exists for any non-commutative monomial in C⟨Xi|i ∈{1, . . . , I}⟩.
Consider the particular case where I = 1 (we replace XN,1 by XN to simplify
the notations) and assume XN has real eigenvalues and that the distribution of
XN has a limit distribution ρ. Then, for each k ≥0
ρ(Xk) = lim
N→∞
Z
tkdµXN (t)
(4.4)
where µXN is the measure associated with the distribution of XN (seen as a
random variable in the free probability framework and not as a random variable
in the classical random matrix framework).
Remark 4.2. If ρ is associated with a compactly supported probability measure
µ, the convergence of the moments of µXN to the moments of µ expressed by
(4.4) implies the weak convergence of the sequence µX1, µX2, . . . to µ, i.e.
Z
f(t)dµ(t) = lim
N→∞
Z
f(t)dµXN (t)

78
4. Free probability theory
for each continuous bounded function f(t), from Theorem 30.2 of [Billingsley,
1995]. This is a particular case of the method of moments, see Section 5.1, which
allows us to determine the l.s.d. of a random matrix model from the successive
limiting moments of the e.s.d.
We now deﬁne asymptotic freeness, which is the extension of the notion of
freeness for the algebras of Hermitian random matrices.
Deﬁnition 4.5. The family {XN,1, . . . , XN,I} of random matrices in (AN, τN)
is said to be asymptotically free if the following two conditions are satisﬁed:
1. For every integer i ∈{1, . . . , I}, XN,i has a limit distribution on C[Xi].
2. For every family {i1, . . . , in} ⊂{1, . . . , I} with i1 ̸= i2, . . . , in−1 ̸= in, and
for every family of polynomials {P1, . . . , Pn} in one indeterminate variable
satisfying
lim
N→∞τN
 Pj(XN,ij)

= 0, j ∈{1, . . . , n}
(4.5)
we have:
lim
N→∞τN


n
Y
j=1
Pj(XN,ij)

= 0.
(4.6)
The conditions 1 and 2 are together equivalent to the following two conditions:
the family {XN,1, . . . , XN,I} has a joint limit distribution on C⟨Xi|i ∈{1, . . . , I}⟩
that we denote ρ and the family of algebras {C[X1], . . . , C[XI]} is free in the non-
commutative probability space (C⟨Xi|i ∈{1, . . . , I}⟩, ρ).
The type of asymptotic freeness introduced by Hiai and Petz in [Hiai and
Petz, 2006] is also useful because it deals with almost sure convergence under the
normalized classical matrix traces instead of convergence under the functionals
τN. Following [Hiai and Petz, 2006], the family {XN,1, . . . , XN,I} in (AN, τN) is
said to have a limit ρ almost everywhere if
ρ(Xk1
i1 . . . Xkn
in ) = lim
N→∞
1
N tr(Xk1
N,i1 . . . Xkn
N,in)
almost surely for any non-commutative monomial in C⟨Xi | i ∈{1, . . . , I}⟩. In
the case where N = 1 and XN has real eigenvalues, if the almost sure limit
distribution of XN is associated with a compactly supported probability measure
µ, this condition means that
Z
f(t)dµ(t) = lim
N→∞
1
N
N
X
i=1
f(λi,N)
almost surely for each continuous bounded function f(t). In other words, the
e.s.d. of XN converges weakly and almost surely to the d.f. associated with the
measure µ.

4.3. Free probability and random matrices
79
The family {XN,1, . . . , XN,I} in (AN, τN) is said to be asymptotically free
almost everywhere if, for every i ∈{1, . . . , I}, XN,i has a non-random limit
distribution on C[Xi] almost everywhere and if the condition 2 above is satisﬁed
with the operator τN(·) replaced by
1
N tr(·) in (4.5) and (4.6), where the limits
in these equations are understood to hold almost surely. These conditions imply
in particular that {XN,1, . . . , XN,I} has a non-random limit distribution almost
everywhere on C⟨Xi|i ∈{1, . . . , I}⟩.
Along with Remark 4.2, we can adapt Theorem 4.3 to the algebra of Hermitian
random matrices. In this case, µ and ν are the e.s.d. of two random matrices X
and Y, and the equality Sµ⊠ν = SµSν is understood to hold in the almost sure
sense. The same is true for Theorem 4.2.
It is important however to remember that the (almost everywhere) asymptotic
freeness condition only applies to a limited range of random matrices. Remember
that, for AN ∈CN×N deterministic with l.s.d. A and BN = XNXH
N with l.s.d.
B and XN ∈CN×n with i.i.d. entries of zero mean and variance 1/n, we saw in
Theorem 3.13 that the l.s.d. of XNANXH
N and therefore the l.s.d. of ANBN are
functions of A and B alone. It is however not obvious, and maybe not true at
all, under the mere i.i.d. conditions on the entries of BN that AN and BN are
asymptotically free. Free probability theory therefore does not necessarily embed
all the results derived in Chapter 3. The exact condition for which the l.s.d. of
ANBN is only a function of A and B is in fact unknown. Free probability theory
therefore provides a partial answer to this problem by introducing a suﬃcient
condition for this property to occur, which is the (almost everywhere) asymptotic
freeness condition. In fact, not so many families of random matrices are known to
be asymptotically free. Of importance to applications in Part II is the following
asymptotic freeness result.
Theorem 4.5 (Theorem 4.3.11 of [Hiai and Petz, 2006]). Let {XN,1, . . . , XN,I}
be a family of N × N complex bi-unitarily invariant random matrices, i.e. whose
joint distribution is invariant both by left- and right-unitary products, and let
{DN,1, . . . , DN,J} be a family of non-random diagonal matrices. Suppose that, as
N tends to inﬁnity, the e.s.d. of XN,iXH
N,i and DN,jDH
N,j converge in distribution
and almost surely to compactly supported d.f. Then the family

{XN,i}i∈{1,...,I}, {XH
N,i}i∈{1,...,I}, {DN,j}j∈{1,...,J}, {DH
N,j}j∈{1,...,J}
	
is asymptotically free almost everywhere as N →∞.
Theorem 4.5 allows us in particular to derive the (almost sure) l.s.d. of the
following random matrices:
• sums and products of random (non-necessarily square) Gaussian matrices;
• sums and products of random Gaussian or unitary matrices and deterministic
diagonal matrices;

80
4. Free probability theory
• the sum or product of a Hermitian AN ∈CN×N by XNBNXH
N ∈CN×N, with
BN ∈Cn×n Hermitian and XN ∈CN×n Gaussian or originating from columns
of a unitary matrix.
Theorem 4.5 can be used for sums and products of non-square Gaussian matrices
since the DN,j matrices can be taken such that their ﬁrst (N −n) diagonal entries
are ones and their last n entries are zeros. It can then be used to derive the l.s.d.
of AN + XNBNXH
N or ANXNBNXH
N for non-diagonal Hermitian AN and BN,
since the result holds if XN is replaced by UNXNVN, with UN the stacked
columns of eigenvectors for AN and VN the stacked columns of eigenvectors for
BN. Basically, and roughly speaking, for any couple (X, Y) of matrices, with
mutually independent entries and such that at least one of these matrices is
unitarily invariant by left and right product, then X and Y are asymptotically
free almost everywhere. Intuitively, almost everywhere asymptotic freeness of
the couple (X, Y) means that the random variables X and Y have independent
entries and that their respective eigenspaces are asymptotically “disconnected,”
as their dimensions grow, in the sense that the eigenvectors of each random
matrix are distributed in a maximally uncorrelated way. In the case where one
of the matrices has isotropically distributed eigenvectors, necessarily it will be
asymptotically free almost everywhere with respect to any other independent
matrix, be it random unitarily invariant or deterministic. On the opposite, two
deterministic matrices cannot be free as their eigenvectors point in deterministic
and therefore “correlated” directions.
Unitarily invariant unitary matrices, often called Haar matrices, are an
important class of matrices, along with Gaussian matrices, for which free
probability conveys quite a few results regarding their summation or product to
Gaussian matrices, deterministic matrices, etc. Haar matrices can be constructed
and deﬁned in the following deﬁnition-theorem.
Deﬁnition 4.6. Let X ∈CN×N be a random matrix with independent Gaussian
entries of zero mean and unit variance. Then the matrix W ∈CN×N, deﬁned as
W = X
 XHX
−1
2
is uniformly distributed on the space U(N) of N × N complex unitary matrices.
This random matrix is called a Haar matrix. Moreover, as N grows large, the
e.s.d. F W of W converges almost surely to the uniform distribution on the
complex unit circle.
For the applications to come, we especially need the following two corollaries
of Theorem 4.5.
Corollary 4.1. Let {T1, . . . , TK} be N × N Hermitian random matrices,
and let {W1, . . . , WK} be Haar distributed independent from all Tk. Assume
that the empirical eigenvalue distributions of all Tk converge almost surely

4.3. Free probability and random matrices
81
toward compactly supported probability distributions. Then, the random matrices
W1T1WH
1 , . . . , WKTKWH
K are asymptotically free almost surely as N →∞.
Corollary 4.2 (Proposition 4.3.9 of [Hiai and Petz, 2006]). Let AN and TN
be N × N Hermitian random matrices, and let WN be a Haar distributed
unitary random matrix independent from AN and TN. Assume that the empirical
eigenvalue distributions of AN and of TN converge almost surely toward
compactly supported probability distributions. Then, the random matrices AN
and WNTNWH
N are asymptotically free almost surely as N →∞.
The above propositions, along with the R- and S-transforms of Theorem 4.2
and Theorem 4.3, will be used to derive the l.s.d. of some random matrix models
for which the Stieltjes transform approach is more painful to use.
The R-transform and S-transform introduced in the deﬁnition-theorem,
Theorem 4.2, and the deﬁnition-theorem, Theorem 4.3, respectively, can be
indeed redeﬁned in terms of transforms of the l.s.d. of random matrices, as they
were already quickly introduced in Deﬁnition 3.3 and Deﬁnition 3.4. In this case,
Theorem 4.2 extends to the following result.
Theorem 4.6. Let AN ∈CN×N and BN ∈CN×N be two random matrices.
If AN and BN are asymptotically free almost everywhere and have respective
(almost sure) asymptotic eigenvalue probability distribution µA and µB, then
AN + BN has an asymptotic eigenvalue distribution µ, which is such that, if RA
and RB are the respective R-transforms of the l.s.d. of AN and BN
RA+B(z) = RA(z) + RB(z)
almost surely, with RA+B
the R-transform of the asymptotic eigenvalue
distribution of AN + BN. The distribution µ is often denoted µA+B, and we
write
µA+B = µA ⊞µB.
From the deﬁnition of the R-transform, we in particular have the following
useful property.
Lemma 4.1. Let X ∈CN×N be some Hermitian matrix and a ∈R. Then
RaX(z) = aRX(az).
Also, if X is random with limiting l.s.d. F, in the large N limit, the R-transform
RF(a) of the l.s.d. F(a) of the random matrix aX, for some a > 0, satisﬁes
RF(a)(z) = aRF (az).

82
4. Free probability theory
This is immediate from Lemma 3.2 and Deﬁnition 3.3. Indeed, applying the
deﬁnition of the R-transform at point az, we have:
mX

RX(az) + 1
az

= −az
while amaX(ay) = mX(y) from Lemma 3.2. Together, this is therefore:
amaX

aRX(az) + 1
z

= −az.
Removing the a on each side, we have that aRX(az) is the R-transform associated
with the Stieltjes transform of aX, from Deﬁnition 3.3 again.
Similarly, Theorem 4.3 for the S-transform extends to the following result.
Theorem 4.7. Let AN ∈CN×N and BN ∈CN×N be two random matrices.
If AN and BN are asymptotically free almost everywhere and have respective
(almost sure) asymptotic eigenvalue distribution µA and µB, then ANBN has
an asymptotic eigenvalue distribution µ, which is such that, if SA and SB are
the respective S-transforms of the l.s.d. of AN and BN
SAB(z) = SA(z)SB(z)
almost surely, with SAB the S-transform of the l.s.d. of ANBN. The distribution
µ is often denoted µAB and we write
µAB = µA ⊠µB.
An equivalent scaling result is also valid for the S-transform. Precisely, we
have the following lemma.
Lemma 4.2. Let X ∈CN×N be some Hermitian matrix and let a be a non-zero
real. Then
SaX(z) = 1
aSX(z).
If X is random with limiting l.s.d. F, then the S-transform SF(a) of the l.s.d.
F(a) of the random matrix aX satisﬁes
SF(a)(z) = 1
aSF (z).
This unfolds here from noticing that, by the (deﬁnition-)Theorem 4.3
SaX(z) = 1 + z
z
ψ−1
aX(z)
with ψaX the ψ-transform of F aX. Now, by deﬁnition of the ψ-transform
ψaX(z) =
Z
azt
1 −aztdF X(t) = ψX(az)

4.3. Free probability and random matrices
83
and therefore:
ψ−1
aX(z) = 1
aψ−1
X (z)
which gives the result.
Of interest to practical applications of the S-transform is also the following
lemma.
Lemma 4.3. Let A ∈CN×n and B ∈Cn×N such that AB is Hermitian non-
negative. Then, for z ∈C \ R+
SAB(z) = z + 1
z + n
N
SBA
N
n z

.
If AB is random with l.s.d. and BA has l.s.d. F as N, n →∞with N/n →c,
0 < c < ∞, then we also have
SF (z) = z + 1
z + 1
c
SF (cz) .
This result can be proved starting from the deﬁnition of the S-transform,
Deﬁnition 3.4, as follows. Notice ﬁrst from Deﬁnition 3.4 that
ψAB(z) = −1 −1
z mAB(z−1)
= n
N

−1 −1
z mBA(z−1)

= n
N ψBA(z).
Now, by deﬁnition
ψAB

z
1 + z SAB(z)

= z
from which:
ψBA

z
1 + z SAB(z)

= N
n z.
Taking ψ−1
BA (with respect to composition) on each side, this leads to
SAB(z) = 1 + z
z
ψ−1
BA
N
n z

=
N
n + N
n z
1 + N
n z
1 + N
n z
N
n z
ψ−1
BA
N
n z

=
N
n + N
n z
1 + N
n z SBA
N
n z

which is the ﬁnal result.

84
4. Free probability theory
In the next section, we apply the above results on the R- and S-transforms to
random matrix models involving Gaussian matrices, before considering models
involving Haar matrices in the subsequent section.
4.4
Free probability for Gaussian matrices
In the following, we will demonstrate in a few lines the result of Theorem 3.13 on
the limiting distribution of BN = AN + XH
NTNXN in the case when TN = IN,
AN ∈Cn×n has uniformly bounded spectral norm and XN ∈CN×n has Gaussian
entries of zero mean and variance 1/n. To begin with, we introduce some classical
results on the R- and S-transforms of classical laws in random matrix theory.
Theorem 4.8. The semi-circle law and Mar˘cenko–Pastur law have the following
properties.
(i) The R-transform RFc(z) of the Mar˘cenko–Pastur law Fc with ratio c, i.e. the
almost sure l.s.d. of XNXH
N, XN ∈CN×n, with i.i.d. entries of zero mean and
variance 1/n, as N/n →c, whose density fc is given by (2.5), reads:
RFc(z) =
1
1 −cz
and the S-transform SFc(z) of Fc reads:
SFc(z) =
1
1 + cz .
Similarly, the R-transform RF c(z) of the complementary Mar˘cenko–Pastur
law F c, i.e. the almost sure l.s.d. of XH
NXN, reads:
RF c(z) =
c
1 −z
and the S-transform SF c(z) of F c is given by:
SF c(z) =
1
c + z .
(ii) The R-transform RF (z) of the semi-circle law F, with density f given by
(2.4), reads:
RF (z) = z.
If the entries of XN are Gaussian distributed, from Corollary 4.2, AN and
XH
NXN are asymptotically free almost everywhere. Therefore, from Theorem
4.6, almost surely, the R-transform RB of the l.s.d. of BN satisﬁes
RB(z) = RA(z) + RF c(z)

4.4. Free probability for Gaussian matrices
85
with A the l.s.d. of AN. From the earlier Deﬁnition 3.3, Equation (3.8), of the
R-transform
mB(z) =
1
RB (−mB(z)) −z
=
1
RA (−mB(z)) +
c
1+mB(z) −z
almost surely. This is equivalent to
RA (−mB(z)) +
1
−mB(z) = z −
c
1 + mB(z)
almost surely.
From Deﬁnition 3.3, Equation (3.7), taking the Stieltjes transform of A on
both sides
mB(z) = mA

z −
c
1 + mB(z)

which is consistent with Theorem 3.13. Slightly more work is required to
generalize this result to TN diﬀerent from IN. This study is carried out in Section
4.5 for the case where XN is a Haar matrix instead.
We present now one of the important results known so far concerning Gaussian-
based models of deep interest for applications in wireless communications. In
[Ryan and Debbah, 2007b], Ryan and Debbah provide indeed a free probability
expressions of the free convolution and free deconvolution for the information
plus noise model, summarized as follows.
Theorem 4.9 ([Ryan and Debbah, 2007b]). Let XN ∈CN×n be a random
matrix with i.i.d. Gaussian entries of zero mean and unit variance, and RN
a (non-necessarily random) matrix such that the eigenvalue distribution of
RN = 1
nRNRH
N converges weakly and almost surely to the compactly supported
probability distribution µR, as n, N →∞with limit ratio N/n →c > 0. Then the
eigenvalue distribution of
BN = 1
n (RN + σXN) (RN + σXN)H
converges weakly and almost surely to the compact supported measure µB such
that
µB = ((µR  µc) ⊞δσ2) ⊠µc
(4.7)
with µc the probability distribution with distribution function the Mar˘cenko–
Pastur law and δσ2 the probability distribution of a single mass in σ2 (with kth
order moment σ2k). Equation (4.7) is the free convolution of the information
plus noise model. This can be reverted as
µR = ((µB ⊠µc) ⊟δσ2)  µc
which is the free deconvolution of the information plus noise model.

86
4. Free probability theory
To understand the importance of the result above, remember for instance that
we have already shown that matrix models of the type BN = AN + XNTNXH
N
or BN = (AN + XN)(AN + XN)H, where XN has i.i.d. (possibly Gaussian)
entries, can be treated by Stieltjes transform methods (Theorem 3.13 and
Theorem 3.15), which provide a complete description of the l.s.d. of BN, through
its Stieltjes transform. However, while this powerful method is capable of treating
the very loose case of XN with i.i.d. entries, there does not yet exist a unifying
framework that allows us to derive easily the limit of the Stieltjes transform for
more involved models in which successive sums, products and information plus
noise models of i.i.d. matrices are taken. In the Stieltjes transform approach,
every new model must be dedicated a thorough analysis that consists in (i)
deriving an implicit equation for the Stieltjes transform of the l.s.d., (ii) ensuring
that the implicit equation has a unique solution, (iii) studying the convergence
of ﬁxed-point algorithms that solve the implicit equations. Consider for instance
the model
BN =

HT
1
2 X + σW
 
HT
1
2 X + σW
H
(4.8)
where H, X, and W are independent and all have i.i.d. Gaussian entries, T is
deterministic and σ > 0. This model arises in wireless communications when a
Gaussian signal process T
1
2 X ∈Cnt×L is sent from an nt-antenna transmitter
during L sampling instants through a MIMO nr × nt channel H ∈Cnr×nt with
additive white Gaussian noise W ∈Cnr×L with entries of variance σ2. The
matrix X is assumed to have i.i.d. Gaussian entries and therefore the nt-
dimensional signal vector has transmit covariance matrix T ∈Cnt×nt. In the
framework of free probability, the model (4.8) is not diﬃcult to treat, since it
relies on elementary convolution and deconvolution operation. Precisely, it is the
succession of the multiplicative free convolution of the l.s.d. of X and T
1
2 and the
information plus noise-free convolution of the l.s.d. of T
1
2 X and σW. As we can
already guess from the deﬁnitions of the free probability framework and as we
will see in detail in Chapter 5, this problem can also be treated from elementary
combinatorics computations based on the successive moments of the d.f. under
study; these computations will be shown in particular to be easily implemented
on a modern computer. The analysis of model (4.8) by the Stieltjes transform
approach is more diﬃcult and has been treated in [Couillet et al., 2011c], in which
it is proved that mF (z), z ∈C+, the Stieltjes transform of the l.s.d. F of BN, is
the unique solution with positive imaginary part of a given implicit equation, see
Theorem 17.5. However, computer trials suggest that the region of convergence of
the ﬁxed-point algorithm solving the implicit equation for mF (z) has a very small
radius, which is very unsatisfactory in practice, therefore giving more interest to
the combinatorial approach. Moreover, the complete derivation of the limiting
spectrum of BN requires a similar mathematical machinery as in the proof of
the Mar˘cenko–Pastur, this being requested for every new model. These are two
strong arguments that motivate further investigations on automated numerical

4.5. Free probability for Haar matrices
87
methods based on combinatoric calculus. Nonetheless, the Stieltjes transform
approach is able to treat the case when X has i.i.d. non-necessarily Gaussian
entries, e.g. M-QAM, M-PSK modulated signals, which are not proved to satisfy
the requirement demanded in the free probability approach. Moreover, as will
become clear in Part II, the simplicity and the ﬂexibility of the combinatoric
methods are rarely any match for the accuracy and eﬃciency of the Stieltjes
transform approach.
In the following section, we derive further examples of applications of Theorem
4.6 and Theorem 4.7 to the free probability framework introduced in this section
for Haar random matrices.
4.5
Free probability for Haar matrices
We start with the introduction of a very simple model, from which the application
of the R-transform and the S-transform are rather straightforward. Consider
the matrix model BN = WNTNWH
N + AN, where AN ∈CN×N, TN ∈Cn×n
and WN ∈CN×n is a Haar random matrix. Note that this model is the Haar
equivalent of Theorem 3.13.
From Corollary 4.2, the matrices AN and WNTNWH
N are asymptotically free
almost everywhere and therefore we can consider retrieving the l.s.d. of BN as
a function of those of AN and WNTNWH
N using the R-transform.
For the problem at hand, denoting F the l.s.d. of BN, as N →∞
RF (z) = RT (z) + RA(z)
almost surely, with T the l.s.d. of TN and A the l.s.d. of AN. Indeed, WN being
unitary, the successive traces of powers of WNTNWH
N are also the successive
traces of powers of TN, so that the distribution of the free random variable
WNTNWH
N is also the distribution of TN.
Consider the special case when both TN and AN have l.s.d. T with density
T ′(x) = 1
2δ0(x) + 1
2δ1(x), then:
mT (z) = 1
2
2z −1
z(z −1)
which, from (3.8), leads to
RF (z) = z −1 −
√
z2 + 1
z
whose Stieltjes transform can be computed into the form
mF (z) = −
1
p
z(z −2)
using (3.7). We ﬁnally obtain the l.s.d. using (3.2)
F(x) = 1[0,2](x) 1
π arcsin(x −1)

88
4. Free probability theory
−0.5
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
Eigenvalues
Density
Empirical eigenvalue distribution
Arcsinus law
Figure 4.1 Histogram of the eigenvalues of WNTNWH
N + AN and the arcsinus law,
for N = 1000.
which is the arcsinus law, depicted in Figure 4.1.
The same kind of reasoning can be applied to the product of asymptotically
free random variables, using the S-transform in place of the R-transform.
For instance, the e.s.d. of BN = WNTNWH
NAN admits an almost sure l.s.d.
F satisfying
SF (z) = ST (z)SA(z)
a.s.
−→0
as N →∞, with T and A the respective l.s.d. of TN and AN.
From the R- and S-transform tools deﬁned and used above, the l.s.d. of more
elaborate matrix models involving unitary matrices can be evaluated [Couillet
and Debbah, 2009; Hachem, 2008]. From the above Corollary 4.1, for Haar
Wk matrices and deterministic Tk matrices with limiting compactly supported
d.f., the family {W1T1WH
1 , . . . , WKTKWH
K} is asymptotically free almost
everywhere. Moreover, for D a Hermitian matrix with compactly supported
limited d.f., the family {WkTkWH
k , D} is asymptotically free almost everywhere,
due to Corollary 4.2. This allows us to perform seamlessly the operations of S-
and R-transforms of Theorem 4.6 and Theorem 4.7, respectively, for all matrix
models involving random WkTkWH
k and deterministic D matrices. We hereafter
provide three applications of such large dimensional matrix models.
On the R-transform side, we have the following result.
Theorem 4.10 ([Couillet and Debbah, 2009]). Let T1, . . . , TK ∈CN×N be
diagonal non-negative deﬁnite matrices and W1, . . . , WK, Wk ∈CN×N, be
independent Haar matrices. Denote Tk the l.s.d. of Tk. Finally, let BN ∈CN×N

4.5. Free probability for Haar matrices
89
denote the matrix
BN =
K
X
k=1
WkTkWH
k .
Then, as N grows large, the e.s.d. of BN tends to F whose η-transform ηF is
given by:
ηF (x) =
 
1 + x
K
X
k=1
βk(x)
!−1
where the functions βk(x), k ∈{1, . . . , K}, satisfy the K ﬁxed-point equations
βk(x) =
Z
t
 
1 + x
t −βk(x)
1 + x PK
i=1 βi(x)
!−1
dTk(t).
(4.9)
Also, the Shannon transform VF (x) of F is given by:
VF (x) = log
 
1 + x
K
X
k=1
βk(x)
!
+
K
X
k=1
Z
log(1 + xη(x)[t −βk(x)])dTk(t).
(4.10)
This result can be used to characterize the performance of multi-cellular
orthogonal CDMA communications in frequency ﬂat channels, see, e.g., [Couillet
and Debbah, 2009; Peacock et al., 2008].
Proof. From the R-transform Deﬁnition 3.3 and the η-transform Deﬁnition 3.5,
for a given distribution function G, we have:
RG(−xηG(x)) = −1
x

1 −
1
ηG(x)

(4.11)
ηG

−
1
RG(x) + 1
x

= xRG(x) + 1.
(4.12)
Denoting RF the R-transform of F and Rk the R-transform of the l.s.d. of
WkTkWH
k , we have from Equation (4.12) that
xRk(x) + 1 =
Z
1
1 −
t
Rk(x)+ 1
x
dTk(t)
which is equivalent to
Rk(x) = 1
x
Z
t
Rk(x) + 1
x −tdTk(t).
Evaluating the above in −xηF (x) and denoting βk(x) = Rk(−xηF (x)), we have:
βk(x) = 1
x
Z
t
1 −xηF (x)βk(x) + xηF (x)tdTk(t).

90
4. Free probability theory
Remember now from Corollary 4.1 that the matrices WkTkWH
k
are
asymptotically free. We can therefore use the fact that
RF (−xηF (x)) =
K
X
k=1
Rk(−xηF (x)) =
K
X
k=1
βk(x)
from Theorem 4.6. From Equation (4.11), we ﬁnally have
K
X
k=1
βk(x) = −1
x

1 −
1
ηF (x)

which is the expected result.
To obtain (4.10), notice from Equation (3.4) that the Shannon transform can
be expressed as
VF (x) =
Z x
0
1
t (1 −ηF (t)) dt.
We therefore seek an integral form for the η-transform. For this, the strategy
is to diﬀerentiate logarithm expressions of the ηF (x) and βk(x) expressions
obtained above and seek for a link between the derivatives. It often turns out that
this strategy leads to a compact expression involving only the aforementioned
logarithm expressions.
Note ﬁrst that
1
x(1 −ηF (x)) = 1
x

1 −
 
1 + x
K
X
k=1
βk(x)
!−1
=
K
X
k=1
βk(x)ηF (x).
Also note from (4.9) that
1 −xηF (x)βk(x) =
Z
1 −xηF (x)βk(x)
1 −xηF (x)βk(x) + xηF (x)tdTk(t)
and therefore that
1 =
Z
1
1 −xηF (x)βk(x) + xηF (x)tdTk(t).
(4.13)
Now, for any k, the derivative along x of
Ck(x) ≜
Z
log(1 −xηF (x)βk(x) + xηF (x)t)dTk(t)
is
C′
k(x)
=
Z [−ηF (x)βk(x) −xη′
F (x)βk(x) −xηF (x)β′
k(x)] + [ηF (x) + xη′
F (x)]t
1 −xηF (x)βk(x) + xηF (x)t
dTk(t).

4.5. Free probability for Haar matrices
91
Recalling now (4.13) and (4.9), this yields
C′
k(x)
= −ηF (x)βk(x) −xη′
F (x)βk(x) −xηF (x)β′
k(x) + (ηF (x) + xη′
F (x))βk(x)
= −xηF (x)β′
k(x).
We also have
 
log
 
1 + x
K
X
k=1
βk(x)
!!′
= ηF (x)
K
X
k=1
βk(x) + x
K
X
k=1
ηF (x)β′
k(x).
Adding this last expression to PK
k=1 C′
k(x), we end up with the desired
PK
k=1 βk(x)ηF (x). Verifying that VF (0) = 0, we ﬁnally obtain (4.10).
Note that the strategy employed to obtain the Shannon transform is a very
general approach that is very eﬀective for a large range of models. In the case of
the sum of Gram matrices of doubly correlated i.i.d. matrices, the Gram matrix of
sums of doubly correlated i.i.d. matrices or matrices with a variance proﬁle, this
approach was identically used to derive expressions for the Shannon transform,
see further Chapter 6.
As for S-transform related derivations, we have the following result.
Theorem 4.11 ([Hachem, 2008]). Let D ∈CN×N and T ∈CN×N be diagonal
non-negative matrices, and W ∈CN×N be a Haar matrix. Denote D and T the
respective l.s.d. of D and T. Denote BN the matrix
BN = D
1
2 WTWHD
1
2 .
Then, as N grows large, the e.s.d. of BN converges to F whose η-transform
ηF (x) satisﬁes
ηF (x) =
Z
(xγ(x)t + 1)−1 dD(t)
γ(x) =
Z
t (ηF (x) + xδ(x)t)−1 dT(t)
δ(x) =
Z
t (xγ(x)t + 1)−1 dD(t)
and whose Shannon transform VF (x) satisﬁes
VF (x) =
Z
log (1 + xγ(x)t) dD(t) +
Z
log(ηF (x) + xδ(x)t)dT(t).
This last result is proved similarly to Theorem 4.10, using the S-transform
identity
SF (x) = SD(x)ST (x)
(4.14)

92
4. Free probability theory
emerging from the fact that D and WTWH are asymptotically free random
matrices (from Corollary 4.2) and that the e.s.d. of WTWH is the e.s.d. of the
matrix T.
Proof. The proof requires the introduction of the functional ζ(z) deﬁned as
ζ(z) ≜
ψ(z)
1 + ψ(z)
with ψ(z) introduced in Deﬁnition (3.6). It is shown in [Voiculescu, 1987] that ζ
is analytical on C+. We denote its analytical inverse ζ−1. From the deﬁnition of
the S-transform as a function of ψ, we ﬁnally have S(z) = 1
zζ−1(z).
From the almost sure asymptotic freeness of D and WTWH, and from (4.14),
we have:
ζ−1
F (z) = ζ−1
D (z)ST (z).
Hence, replacing z by ζF (−z)
−z = ζ−1
D (ζF (−z))ST (ζF (−z))
which, according to the deﬁnition of ζF , gives
ζ−1
D

ψF (−z)
1 + ψF (−z)

=
−z
ST

ψF (−z)
1+ψF (−z)
.
Taking ψD on both sides, this is:
ψF (−z) = ψD


−z
ST

ψF (−z)
1+ψF (−z)



(4.15)
since ζ−1
D maps
x
1+x to ψ−1
D (x), and hence
ψF (−z)
1+ψF (−z) to ψ−1
D (ψF (−z)).
Denoting
γ(z) =
1
ST

ψF (−z)
1+ψF (−z)

this is
ψF (−z) = ψD (−zγ(z)) .
Computing
ψF
1+ψF and taking ζ−1
T
of the resulting expression, we obtain
ζ−1
T

ψF (−z)
1 + ψF (−z)

=
1
γ(z)
ψF (−z)
1 + ψF (−z).
Taking ψT on both sides, this is ﬁnally
ψF (−z) = ψT

1
γ(z)
ψF (−z)
1 + ψF (−z)

.
(4.16)

4.5. Free probability for Haar matrices
93
To fall back on the system of equations, notice that ηF (z) = 1 −ψF (−z).
Equation (4.15) becomes
ηF (z) = ηD(zγ(z))
while Equation (4.16) leads to
γ(z) =
Z
t
ηF (z) −ψD(−zγ(z))
γ(z)
t
dT(t)
which are exactly the desired results with δ(z) deﬁned as
δ(z) = ψD(−zγ(z))
−zγ(z)
.
Putting together Theorem 4.10 and Theorem 4.11, we can show the following
most general result.
Theorem 4.12. Let D ∈CN×N, T1, . . . , TK ∈CN×N be diagonal non-negative
deﬁnite matrices, and Wk ∈CN×N, k ∈{1, . . . , K}, be independent Haar
matrices. Denote D, Tk the l.s.d. of the matrices D and Tk, respectively. Finally,
denote BN ∈CN×N the matrix
BN =
K
X
k=1
D
1
2 WkTkWH
k D
1
2 .
Then, as N grows large, the e.s.d. of BN tends to the F whose η-transform ηF
is given by:
ηF (x) =
 
1 + xδ(x)
K
X
k=1
γk(x)
!−1
where γ1, . . . , γK and δ are solutions to the system of equations
δ(x) =
Z
t
 
1 + xt
K
X
k=1
γk(x)
!−1
dD(t)
γk(x) =
Z
t (1 −xδ(x)γk(x) + xδ(x)t)−1 dTk(t).
Also, the Shannon transform VF (x) is given by:
VF (x) =
Z
log
 
1 + tx
K
X
k=1
γk(x)
!
dD(t)
+
K
X
k=1
Z
log (1 −xδ(x)γk(x) + xδ(x)t) dTk(t).

94
4. Free probability theory
To the best of our knowledge, this last result is as far as free probability
reasoning can go. In particular, if the products D
1
2 WiT
1
2
i
were replaced by
the more general D
1
2
i WiT
1
2
i , then it is impossible to use free probability
results any longer, as the family {D
1
2
i WiTiWH
i D
1
2
i , i ∈{1, . . . , K}} is no longer
asymptotically free almost everywhere. To deal with this model, which is
fundamental to the study of multi-cellular frequency selective orthogonal CDMA
systems, we will show in Section 6.2.4 that the Stieltjes transform approach is
our only asset. The Stieltjes transform approach is however much less direct and
requires more work than the free probability framework, although it is much
more ﬂexible.
In this chapter, we introduced free probability theory and its applications to
large dimensional random matrices through a deﬁnition involving joint moments
of spectral distributions with compact supports. Apart from the above analytical
results based on the R-transform and S-transform, the common use of free
probability theory for random matrices is related to the study of successive
moments of distributions, which is in fact more appealing when the models
under study are less tractable through analytic approaches. This is the subject
of the following chapter, which introduces the combinatoric moment methods
and which goes beyond the strict free probability framework to deal with more
structured random matrix models enjoying some symmetry properties. To this
day, these models have not been addressed through any analytical method but,
due to their symmetric structure, combinatorics calculus can be performed.

5
Combinatoric approaches
We start the discussion regarding moment-based approaches by mentioning the
well-known method of moments, which is a tool aimed at retrieving distribution
functions based on moments from a classical probability theory point of view.
This method of moments will be opposed to the moment-based methods or
moment methods which is the center of interest of this chapter since it
targets speciﬁcally combinatoric calculus in non-commutative algebras of random
matrices.
5.1
The method of moments
In this section, we will discuss the technique known as the method of moments
to derive a probability distribution function from its moments, see, e.g., [Bai
and Silverstein, 2009; Billingsley, 1995]. When we are able to infer the limiting
spectral distribution of some matrix model, although not able to prove it through
classical analytic approaches, it is possible under certain conditions to derive
all successive limiting moments of the distribution instead. Under Carleman’s
condition, to be introduced hereafter, these moments uniquely determine the
limiting distribution.
We start by introducing Carleman’s condition.
Theorem 5.1. Let F be a distribution function, and denote M1, M2, . . . its
sequence of moments which are assumed to be all ﬁnite. If the condition
∞
X
k=1
M
−1
2k
2k
= ∞
(5.1)
is fulﬁlled, then F is uniquely determined by the sequence M1, M2, . . ..
Therefore, if we only have access to the moments M1, M2, . . . of some
distribution F and that Carleman’s condition is met, then F is uniquely
determined by its moments.
In the speciﬁc case where M1, M2, . . . are the moments of the l.s.d. of large
Hermitian matrices, we will need the following moment convergence theorem.

96
5. Combinatoric approaches
Theorem 5.2 (Lemma 12.1 and Lemma 12.3 of [Bai and Silverstein, 2009]).
Let F1, F2, . . . be a sequence of distribution functions, such that, for each n, Fn
has ﬁnite moment Mn,k of order k for all k, with Mn,k →Mk < ∞as n →∞.
Assume additionally that the sequence of moments M1, M2, . . . fulﬁlls Carleman’s
condition (5.1). Then Fn converges to the unique distribution function F with
moments Mk.
To prove that the e.s.d. of a given sequence of Hermitian random matrices
tends to a limit distribution function, all that is needed to show is the convergence
of the empirical moments to limiting moments that meet Carleman’s condition.
It then suﬃces to match these moments to some previously inferred l.s.d., or to
try to determine this l.s.d. directly from the moments. In the following, we give
the main steps of the proof of the semi-circle law of Theorem 2.11, which we
presently recall.
Consider a Hermitian matrix XN ∈CN×N, with independent entries
1
√
N XN,ij
such that E[XN,ij] = 0, E[|XN,ij|2] = 1 and there exists ε such that the XN,ij
have a moment of order 2 + ε. Then F XN ⇒F almost surely, where F has density
f deﬁned as
f(x) = 1
2π
p
(4 −x2)+.
Moreover, if the XN,ij are identically distributed, the result holds without the
need for the existence of a moment of order 2 + ε.
Proof of Theorem 2.11. We wish to show that, on a space A ⊂Ωof probability
one, the empirical moments MN,1(ω), MN,2(ω), . . ., ω ∈A, of the e.s.d. of a
random Wigner matrix with i.i.d. entries converge to the moments M1, M2, . . .
of the semi-circle law, and that these moments satisfy Carleman’s condition. The
moments of the semi-circle law are computed as
M2k+1 = 1
2π
Z 2
−2
x2k+1p
4 −x2dx = 0
M2k = 1
2π
Z 2
−2
x2kp
4 −x2dx =
1
k + 1
2k
k

without diﬃculty, by the change of variable x = 2√y. The value of M2k is the
kth Catalan number. Using Stirling’s approximation formula, for k large
M2k
−1
2k = (k + 1)
1
2k
(k!)
1
k
((2k)!)
1
2k ∼(k + 1)
1
2k 1
2
(
√
2πk)
1
k
(
√
4πk)
1
2k
which tends to
1
2. Therefore, there exists k0 > 0, such that k ≥k0 implies
M
−1
2k
2k
> 1
4, which ensures that the series P
k M2k
−1
2k diverges, and Carleman’s
condition is satisﬁed.
The rest of this proof follows from [Anderson et al., 2006, 2010; Bai and
Silverstein, 2009], where more details are found. The idea is ﬁrst to use

5.1. The method of moments
97
truncation, centralization, and rescaling steps as described in Section 3.2 so to
move from random entries of the Wigner matrix with no moment constraint
to random entries with a distribution that admits moments of all orders. By
showing that it is equivalent to prove the convergence of F XN for either XN,ij
or their truncated versions, we can now work with variables with moments
of all orders. We need to show that E[MN,k] ≜
R
MN,k(ω)dP(ω) →Mk and
P
N E[(MN,k −Mk)2] < ∞. This will be suﬃcient to ensure the almost sure
convergence of the empirical moments to the moments of the semi-circle law,
by applying the Markov inequality, Theorem 3.5, and the ﬁrst Borel–Cantelli
lemma, Theorem 3.6.
To determine the limit of E[MN,k], we resort to combinatorial and particularly
graph theoretical tools. It is required indeed, when developing the trace of Xk
N
E[MN,k] = 1
N E[tr(Xk
N)]
= 1
N
X
p∈{1,...,N}k
EXN,p1p2XN,p2p3 . . . XN,pkp1
with pi the ith entry of p, to be able to determine which moments of the XN,ij
are non-negligible in the limit. These graph theoretical tools are developed in
detail in [Bai and Silverstein, 2009] and [Anderson et al., 2006]. What arises in
particular is that all odd moments vanish, while it can be shown that, for the
even moments
E[MN,2k] = (1 + O(1/N)) 1
N
k−1
X
j=0
E[MN,2j]E[MN,2(k−j−1)]
which establishes an asymptotic recursive equation for E[MN,2k]. For k = 1, we
have E[MN,2] →1. Finally, by comparing the recursive formula to the deﬁnition
of the Catalan numbers Ck, given later in (5.5), we obtain the required result
EMN,2k →Ck.
The previous proof provided the successive moments of the Wigner semi-circle
law, recalled in the following theorem.
Theorem 5.3. Let F be the semi-circle law distribution function with density
f given by
f(x) = 1
2π
p
(4 −x2)+.

98
5. Combinatoric approaches
This distribution is compactly supported and therefore has successive moments
M1, M2, . . ., given by:
M2k+1 = 0,
M2k =
1
k + 1
2k
k

.
This general approach can be applied to prove the convergence of the e.s.d. of
a random matrix model to a given l.s.d. However, this requires to know a priori
the limit distribution sought for. Also, it assumes the existence of moments
of all orders, which might already be a stringent assumption, but truncation,
centralization, and rescaling steps can be performed prior to using the method
of moments to alleviate this condition as was performed above. To the best of
our knowledge, for more general random matrix models than Wigner or Wishart
matrices, the method of moments leads to rather involved calculus, from which
not much can be inferred. We therefore close the method of moments parenthesis
here and will never mention it again.
We now move to the moment methods originating from the free probability
framework, which are concerned speciﬁcally with random matrix models. We
start with an introduction of the free moments and free cumulants of random
non-commutative variables and random matrices.
5.2
Free moments and cumulants
Remember that we mentioned that Theorem 4.1 is a major result since it allows
us to derive the successive moments of the l.s.d. of products of free random
matrices from the moments of the l.s.d. of the individual random matrices. For
instance, for free random variables A and B, we have:
φ(AB) = φ(A)φ(B)
φ(ABAB) = φ(A2)φ(B)2 + φ(A)2φ(B2) −φ(A)2φ(B)2
φ(AB2A) = φ(A2)φ(B2).
Translated in terms of traces of random matrices, we can compute the so-called
free moments, i.e. the moments of the (almost sure) l.s.d., of sums and products
of random matrices as a function of the free moments of the operands.
It is then possible to derive the limiting spectrum of the sum of asymptotically
free random matrices AN and BN from the sequences of the free moments of the
l.s.d. of AN + BN. Theorem 4.2 has already shown that AN + BN is connected
to AN and BN through their respective R-transforms. Denote A and B two non-
commutative random variables, with d.f. the l.s.d. of AN and BN, respectively.

5.2. Free moments and cumulants
99
The formal R-transform series of A can be expressed as
RA(z) =
∞
X
k=1
Ckzk−1
(5.2)
where Ck is called the kth order free cumulant of A. From Theorem 4.2, we
therefore have
RA+B(z) =
∞
X
k=1
[Ck(A) + Ck(B)]zk−1
with Ck(A) and Ck(B) the respective free cumulants of the l.s.d. of AN and
BN. In the same way as cumulants of independent random variables add up in
classical probability theory, free cumulants of free random variables add up in
free probability theory. This summarizes into the following result from Voiculescu
[Voiculescu, 1986].
Theorem 5.4. Let µA and µB be compactly supported probability distributions,
with respective free cumulants C1(A), C2(A), . . . and C1(B), C2(B), . . .. Then the
free cumulants C1(A + B), C2(A + B), . . . of the distribution µA+B ≜µA ⊞µB
satisfy
Ck(A + B) = Ck(A) + Ck(B).
We recall that, in reference to classical probability theory, the binary additive
operation ‘µA ⊞µB’ is called the free additive convolution of the distributions
µA and µB. We equivalently deﬁned the binary operation ‘µA ⊟µB’ as the free
additive deconvolution of µB from µA.
The cumulants of a given distribution µA, with bounded support, can be
computed recursively using Theorem 4.2. Equating coeﬃcients of the terms in
zk allows us to derive the kth order free cumulant Ck of A as a function of the
ﬁrst k free moments M1, . . . , Mk of A. In particular, the ﬁrst three cumulants
read
C1 = M1
C2 = M2 −M 2
1
C3 = M3 −3M1M2 + 2M 3
1 .
It is therefore possible to evaluate explicitly the successive moments of the
probability distribution µA+B = µA ⊞µB of the sum of the random free variables
A and B from the successive moments of µA and µB. The method comes as
follows: given µA and µB, (i) compute the moments and then the cumulants
Ck(A) of A and Ck(B) of B, (ii) compute the sum Ck(A + B) = Ck(A) + Ck(B),
(iii) from the cumulants Ck(A + B), retrieve the corresponding moments of
A + B, from which the distribution µA+B, assumed of compact support, can
be found. This approach can be conducted in a combinatorial way, using non-
crossing partitions. The method is provided by Speicher [Speicher, 1998], which

100
5. Combinatoric approaches
simpliﬁes many free probability calculus introduced by Voiculescu. In particular,
the method described above allows us to recover the moments of A + B from the
cumulants of A and B in the following result.
Theorem 5.5. Let A and B be free random variables with respective cumulants
{Ck(A)} and {Ck(B)}. The nth order free moment Mn(A + B) of the random
variable A + B reads:
Mn(A + B) =
X
π∈NC(n)
Y
V ∈π
 C|V |(A) + C|V |(B)

with NC(n) the set of non-crossing partitions of {1, . . . , n} and |V | the cardinality
of the subset V in the non-crossing partition partition π.
Setting
B = 0,
we
fall
back
on
the
relation
between
the
moments
M1(A), M2(A), . . . and the cumulants C1(A), C2(A), . . . of A
Mn(A) =
X
π∈NC(n)
Y
V ∈π
C|V |(A)
(5.3)
introduced in Theorem 4.2.
Remark 5.1. Note that, since the limiting distribution functions under study are
compactly supported, from Theorem 3.3, the Stieltjes transform mF A of the d.f.
F A associated with µA can be written, for z ∈C+ in the convergence region of
the series
mF A(z) = −
∞
X
k=0
Mkz−k−1
where Mk is the kth order moment of F A, i.e. the kth order free moment of
A. There therefore exists a strong link between the Stieltjes transform of the
compactly supported distribution function F A and the free moments of the non-
commutative random variable A.
Contrary to the Stieltjes transform approach, though, the combinatorial
method requires to compute all successive cumulants, or a suﬃcient number
of them, to better estimate the underlying distribution. Remember though that,
for compactly supported distributions, Mk/k! vanishes fast for large k and then
an estimate of only a few ﬁrst moments might be good enough. This is in
particular convenient when the studied distribution µ consists of K masses. If so,
assuming the respective weights of each mass are known, the ﬁrst K cumulants
are suﬃcient to evaluate the full distribution function. Indeed, in the special
case of evenly distributed masses, given M1, . . . , MK the ﬁrst K moments of the
distribution, µ(x) = 1
K
PK
i=1 δ(x −λi) where λ1, . . . , λK are the K roots of the
polynomial
XK −Π1XK−1 + Π2XK−2 −. . . + (−1)KΠK

5.2. Free moments and cumulants
101
where Π1, . . . , Πn
are the elementary symmetric polynomials, recursively
computed from the Newton–Girard formula
(−1)KKΠK +
K
X
i=1
(−1)K+iMiΠK−i = 0.
(5.4)
See [S´eroul, 2000] for more details on the Newton–Girard formula.
Similar relations hold for the product of free random variables. In terms of
moments and non-crossing partitions, the result is provided by Nica and Speicher
[Nica and Speicher, 1996] in the following theorem.
Theorem 5.6. Let A and B be free random variables with respective free
cumulants {Ck(A)} and {Ck(B)}. Then the nth order free moment Mn(AB)
of the random variable AB reads:
Mn(AB) =
X
(π1,π2)∈NC(n)
Y
V1∈π1
V2∈π2
C|V1|(A)C|V2|(B).
This formula enables the computation of all free moments of the distribution of
AB from the free moments of µA and µB. We recall that the product distribution
is denoted µA ⊠µB and is called multiplicative free convolution. Reverting the
polynomial formulas in the free cumulants enables also the computation of the
free moments of µA from the free moments of µAB and µB. This therefore allows
us to recover the distribution of the multiplicative free deconvolution µAB  µB.
Before debating the important results of free moments and cumulants for
wireless communications, we shortly introduce non-crossing partitions and the
relations between partitions in classical probability theory and non-crossing
partitions in free probability theory.
In (classical) probability theory, we have the following relation between the
cumulants cn and the moments mn of a given probability distribution
mn =
X
π∈P(n)
Y
V ∈π
c|V |
where P(n) is the set of partitions of {1, . . . , n}. For instance, P(3) is
composed of the ﬁve sets {{1, 2, 3}}, {{1, 2}, {3}}, {{1, 3}, {2}}, {{2, 3}, {1}} and
{{1}, {2}, {3}}}. The cardinality of P(n) is called the Bell number Bn, recursively
deﬁned by
B0
= 1
Bn+1 = Pn
k=0
 n
k

Bk.
From the example above, B3 = 5.
We recall for instance that the cumulant c1 = m1 is the distribution mean,
c2 = m2 −m2
1 is the variance, c3 is known as the skewness, and c4 is the kurtosis.

102
5. Combinatoric approaches
Free probability theory provides the similar formula (5.3), where the sum is
not taken over the partitions of {1, . . . , n} but over the non-crossing partitions
of {1, . . . , n}. Non-crossing partitions are deﬁned as follows.
Deﬁnition 5.1. Consider the set {1, . . . , n}. The partition π ∈P(n) is said to be
non-crossing if there does not exist a < b < c < d elements of {1, . . . , n} (ordered
modulo n), such that both {a, c} ∈π and {b, d} ∈π.
Otherwise stated, this means that in a circular graph of the elements of
{1, . . . , n} where the elements V1, . . . , V|V | of a given π ∈P(n) are represented
by |V | polygons, with polygon k connecting the elements of Vk and such that
the edges of polygon i and polygon j, i ̸= j, never cross. This is depicted in
Figure 5.1 in the case of π = {{1, 3, 4}, {2}, {5, 6, 7}, {8}}, for n = 8. The number
of such non-crossing partitions, i.e. the cardinality of NC(n), is known as the
Catalan number, denoted Cn, which was seen incidentally to be connected to the
moments of the semi-circle law, Theorem 5.3. This is summarized and proved in
the following.
Theorem 5.7 ([Anderson et al., 2006]). The cardinality Cn of NC(n), for n ≥1,
satisﬁes the recursion equation
C1 = 1,
Cn =
n
X
k=1
Cn−kCk−1
and is explicitly given by:
Cn =
1
n + 1
2n
n

.
(5.5)
We provide below the proof of this result, which is rather short and intuitive.
Proof. Let π ∈NC(n) and denote j the smallest element connected to 1 with
j = 1 if {1} ∈π, e.g. j = 3 in Figure 5.1. Then necessarily both sets {1, . . . , j −1}
and {j + 1, . . . , n} are non-crossing and, for ﬁxed link (1, j), the number of non-
crossing partitions in {1, . . . , n} is the product between the number of non-
crossing partitions in the sets {1, . . . , j −1}, i.e. Cj−1, and {j + 1, . . . , n}, i.e.
Cn−j. We then have the relation
Cn =
n
X
j=1
Cj−1Cn−j
as expected, along with the obvious fact that C1 = 1. By recursion calculus, it
is then easy to see that the expression (5.5) satisﬁes this recursive equality.
We now return to practical applications of the combinatorial moment
framework. All results mentioned so far are indeed of practical use for problems

5.2. Free moments and cumulants
103
1
2
3
4
5
6
7
8
Figure 5.1 Non-crossing partition π = {{1, 3, 4}, {2}, {5, 6, 7}, {8}} of NC(8).
related to large dimensional random matrices in wireless communication settings.
Roughly speaking, we can now characterize the limiting eigenvalue distributions
for sums and products of matrices involving random Gaussian matrices, random
unitary matrices, deterministic Hermitian matrices, etc. based on combinatorial
calculus of their successive free moments. This is particularly suitable when the
full limiting distribution is not required but only a few moments are needed,
and when the random matrix model involves a large number of such matrices.
The authors believe that all derivations handled by free probability theory can
be performed using the Stieltjes transform method, although this requires more
work. In particular, random matrix models involving unitary matrices are far
more easily handled using free probability approaches than Stieltjes transform
tools, as will be demonstrated in the subsequent sections. In contrast, the
application range of free probability methods is seriously limited by the need
for eigenvalue distributions to be compactly supported probability measures
and more importantly by the need for the random matrices under study to be
unitarily invariant (or more exactly asymptotically free).
Let us for instance apply the moment-cumulant relations as an application
of Theorem 4.9. Denote Bk the kth order moment of µB and Rk the kth order
moment of µR. Equation (4.7) provides a relation between µB and µR under
the form of successive free convolution or deconvolution operations involving
in particular the Mar˘cenko–Pastur law. From the moments of the Mar˘cenko–
Pastur law, Theorem 2.14, and the above free addition and free product theorems,
Theorem 5.5 and Theorem 5.6, we can then obtain polynomial relations between
the Bk and the Rk. Following this procedure, Theorem 4.9 entails
B1 = R1 + 1,
B2 = R2 + (2 + 2c)R1 + (1 + c),
B3 = R3 + (3 + 3c)R2 + 3cR2
1 + (3 + 9c + 3c2 + 3)R1 + (1 + 3c + c2)
(5.6)

104
5. Combinatoric approaches
the subsequent Bk being long expressions that can be derived by computer
software. As a matter of fact, moment-cumulant relations can be applied to
any type of random matrix model involving sums, products, and information
plus noise models of asymptotically free random matrices, the proper calculus
being programmable on modern computers. The moment framework that arises
from free probability theory therefore allows for very direct derivations of the
successive moments of involved random matrix models. In this sense, this is much
more convenient than the Stieltjes transform framework which requires involved
mathematical tools to be deployed for every new matrix model.
In [Rao and Edelman, 2008], in an attempt to fully exploit the above remark,
Rao and Edelman developed a systematic computation framework for a class of
random matrices, including special cases of (i) information plus noise models,
(ii) products and sums of Wishart matrices within themselves, or (iii) products
and sums of Wishart matrices with deterministic matrices. This class is deﬁned
by the authors as the class of algebraic random matrices. Roughly speaking, this
class gathers all random Hermitian matrices X with l.s.d. F for which there
exists a bivariate complex-valued polynomial L(x, y) satisfying
L(z, mF (z)) = 0.
The class of algebraic random matrices is large enough to cover many practical
applications in wireless communications. However, it does not include even the
most basic model XNTNXH
N, where XN has i.i.d. Gaussian entries and the
l.s.d. H of TN has a connected component. If H is a discrete sum of masses in
λk, k = 1, . . . , K, then XNTNXH
N is algebraic. Indeed, from Theorem 3.13, the
Stieltjes transform m(z) of the l.s.d. of XNTNXH
N satisﬁes
m(z)
 
c 1
K
K
X
k=1
λk
1 + λkm(z) −z
!
= 1
which, after multiplication on both sides by Q
k(1 + λkm(z)), leads to a bivariate
polynomial expression in (z, m(z)). This is not true in general when H has a
continuous support. Note also that several computer codes for evaluating free
moments of algebraic random matrices are provided in [Rao et al., 2008].
As repeatedly mentioned, the free probability framework is very limited in
its application scope as it is only applied to large dimensional random matrices
with unitarily invariant properties. Nonetheless, the important combinatorial
machinery coming along with free probability can be eﬃciently reused to extend
the initial results on Gaussian and Haar matrices to more structured types of
matrices that enjoy other symmetry properties. The next chapter introduces the
main results obtained for these extended methods in which new partition sets
appear.

5.3. Generalization to more structured matrices
105
5.3
Generalization to more structured matrices
In wireless communications, research focuses mainly on Gaussian and Haar
random matrices, but not only. Other types of random matrices, more structured,
are desirable to study. This is especially the case of random Vandermonde
matrices, deﬁned as follows.
Deﬁnition 5.2. The Vandermonde matrix V ∈CN×n generated from the vector
(α1, . . . , αn)T is the matrix with (i, j)th entry Vij = αj−1
i
V =





1
1
. . .
1
α1
α2
. . .
αn
...
...
. . .
...
αN−1
1
αN−1
2
. . . αN−1
n




.
A random Vandermonde matrix is a normalized (by
1
√
N ) Vandermonde matrix
whose generating vector (α1, . . . , αn)T is a random vector.
In [Ryan and Debbah, 2009], the authors derive the successive moments of the
e.s.d. of matrix models involving Vandermonde matrices with generating vector
entries drawn uniformly and independently from the complex unit circle. The
main result is as follows.
Theorem 5.8 ([Ryan and Debbah, 2009]). Let D1, . . . , DL be L diagonal
matrices of size n × n such that Di has an almost sure l.s.d. as n →∞, for all
i. Let V ∈CN×n be a random Vandermonde matrix with generators α1, . . . , αn
drawn independently and uniformly from the unit complex circle. Call α a random
variable on [0, 2π) distributed as α1. For ρ ∈P(L), the set of partitions of
{1, . . . , L}, deﬁne Kρ,α,N as
Kρ,α,N = N |ρ|−L−1
Z
[0,2π)|ρ|
L
Y
k=1
1 −ejN(αb(k−1)−αb(k))
1 −ej(αb(k−1)−αb(k))
|ρ|
Y
i=1
dαi
with b(k) the index of the set of ρ containing k (since the αi are i.i.d., the set
indexing in ρ is arbitrary). If the limit
Kρ,α = lim
N→∞Kρ,α,N
exists, then it is called a Vandermonde mixed moment expansion coeﬃcient. If
it exists for all ρ ∈P(L), then we have, as N, n →∞with n/N →c, 0 < c < ∞
1
ntr
 L
Y
i=1
DiVHV
!
→
X
ρ∈P(L)
Kρ,αc|ρ|−1Dρ

106
5. Combinatoric approaches
almost surely, where, for ρ = {ρ1, . . . , ρK}, we denote
Dρk = lim
n→∞
1
ntr

Y
i∈ρk
Di


and
Dρ =
K
Y
k=1
Dρk.
Contrary to the previous results presented for Gaussian random matrices,
the extent of knowledge on the analytical approaches of random matrix theory
so far does not enable us to determine the l.s.d. of random Vandermonde
matrices in a closed-form. Only the aforementioned free probability approach
is known to tackle this problem at this time. Note also that the support of
the l.s.d. of such random Vandermonde matrices is not compact. It is therefore
a priori uncertain whether the l.s.d. of such matrices can be determined by
the limiting moments. For this, we need to verify that Carleman’s condition,
Theorem 5.1, is met. This has in fact been shown by Tucci and Whiting in
[Tucci and Whiting, 2010]. For wireless communication purposes, it is possible
to evaluate the capacity of a random Vandermonde channel model under the form
of a series of moments. Such a channel arises whenever signals emerging from
n sources impinge on an N-fold linear antenna array in line-of-sight. Assuming
the sources are suﬃciently far from the sensing array, the signals emerging from
one particular source are received with equal amplitude by each antenna but
with phases rotated proportionally to the diﬀerence of the optical path lengths,
i.e. proportionally to both the antenna index in the array and the sinus of
the incoming angle. Therefore, calling di the power of signal source i at the
antenna array and V the Vandermonde matrix with generating vector the n
phases of the incoming signals, the matrix VD, D = diag(d1, . . . , dn), models
the aforementioned communication channel.
Since the moments Mk of the (almost sure) l.s.d. of VDVH are only dependent
on polynomial expressions of the moments D1, . . . , Dk of the l.s.d. of D (in the
notations of Theorem 5.8, D1 = . . . = DL so that Dk ≜D{1,...,k}), it is possible
to recover the Dk from the Mk and hence obtain an estimate of Dk from the
large dimensional observation VDVH. Assuming a small number of sources, the
estimates of D1, D2, . . . provide a further estimate of the respective distance of
the signal sources. In [Ryan and Debbah, 2009], the ﬁrst moments for this setup
are provided. Denoting ¯
Mk ≜cMk and ¯Dk ≜cDk, we have the relations
¯
M1 = ¯D1
¯
M2 = ¯D2 + ¯D2
1
¯
M3 = ¯D3 + 3 ¯D2 ¯D1 + ¯D3
1
¯
M4 = ¯D4 + 4 ¯D3 ¯D1 + 8
3
¯D2
2 + 6 ¯D2 ¯D2
1 + ¯D4
1

5.3. Generalization to more structured matrices
107
¯
M5 = ¯D5 + 5 ¯D4 ¯D1 + 25
3
¯D3 ¯D2 + 10 ¯D3 ¯D2
1 + 40
3
¯D2
2 ¯D1 + 10 ¯D2 ¯D3
1 + ¯D5
1
from which ¯D1, ¯D2, . . . can be written as a function of ¯
M1, ¯
M2, . . ..
The successive moments of other structured matrices can be studied similarly
for random Toeplitz or Hankel matrices. The moment calculus can in fact be
directly derived from the moment calculus of Vandermonde matrices [Ryan and
Debbah, 2011]. We have in particular the following results.
Theorem 5.9. Let XN ∈RN×N be the Toeplitz matrix given by:
XN =
1
√
N











X0
X1
X2 . . . XN−2 XN−1
X1
X0
X1
XN−2
X2
X1
X0
...
...
...
...
X2
XN−2
X0
X1
XN−1 XN−2 . . . X2
X1
X0











with X0, X1, . . . real independent Gaussian with zero mean and unit variance.
Then the moment Mk of order k of the l.s.d. of XN is given by Mk = 0 for k
odd and the ﬁrst even moments are given by:
M2 = 1
M4 = 8
3
M6 = 11
M8 = 1435
24 .
Remember for instance that slow fading frequency selective channels can be
modeled by Toeplitz matrices. When the matrices are Wiener-class [Gray, 2006],
i.e. when the series formed of the elements of the ﬁrst row is asymptotically
summable (this being in particular true when a ﬁnite number of elements are
non-zero), it is often possible to replace the Toeplitz matrices by circulant
matrices without modifying the l.s.d., see Theorem 12.1. Since circulant matrices
are diagonalizable in the Fourier basis, their study is simpler, so that this
assumption is often considered. However, in many practical cases, the Wiener-
class assumption does not hold so results on the l.s.d. of Toeplitz matrices are
of major importance.
A similar result holds for Hankel matrices.

108
5. Combinatoric approaches
Theorem 5.10. Let XN ∈RN×N be the Hankel matrix deﬁned by
XN =
1
√
N











X0
X1 X2
. . .
XN−2 XN−1
X1
X2 X3
XN
X2
X3 X4
...
...
...
...
X2N−2
XN−2
X2N−2 X2N−1
XN−1 XN . . . X2N−2 X2N−1
X2N











with X0, X1, . . . real independent Gaussian with zero mean and unit variance.
Then the free moment Mk of order k of XN is given by Mk = 0 for k odd and
the ﬁrst even moments are given by:
M2 = 1
M4 = 8
3
M6 = 14
M8 = 100.
In general, the exact features that random matrices must fulﬁll for results
such as Theorem 5.8 to be easily derived are not yet fully understood. It seems
however that any random matrix X whose joint entry probability distribution is
invariant by left product with permutation matrices enters the same scheme as
random Vandermonde, Toeplitz and Hankel matrices, i.e. the free moments of
matrix products of the type QL
k=1
 DkXXH
can be derived from the moments
of D1, . . . , DL. This is a very new, yet immature, ﬁeld of research.
As already stated in Chapter 4, free probabilistic tools can also be used in place
of classical random theoretical tools to derive successive moments of probability
distributions of large random matrices. We will mention here an additional usage
of moment-based approaches on the results from free probability theory described
previously that allows us to obtain exact results on the expected eigenvalue
distribution of small dimensional random matrices, instead of the almost sure
l.s.d. of random matrices.
5.4
Free moments in small dimensional matrices
Thanks to the unitary invariance property of Gaussian matrices, standard
combinatorics tools such as non-crossing partitions allow us to further generalize
moment results obtained asymptotically, as the matrix dimensions grow large,
to exact results on the moments of the expected e.s.d. of matrices for all ﬁxed
dimensions. We have in particular the following theorem for small dimensional
Wishart and deterministic matrix products.

5.5. Rectangular free probability
109
Theorem 5.11 ([Masucci et al., 2011]). Let XN ∈CN×n have i.i.d. standard
Gaussian entries and TN be a (deterministic) N × N matrix. For any positive
integer p, we have:
E
 1
N tr

TN
1
nXNXH
N
p
=
X
π∈P(p)
nk(ˆπ)−pN l(ˆπ)−1Tˆπ|odd
(5.7)
where ˆπ ∈P(2p) is the permutation such that
 ˆπ(2j −1) = 2π−1(j),
j ∈{1, 2, . . . , p}
ˆπ(2j)
= 2π(j) −1,
j ∈{1, 2, . . . , p} .
Every such ˆπ is attached the equivalence relation ∼ˆπ, deﬁned as
j ∼ˆπ ˆπ(j) + 1.
In (5.7), ˆπ|odd is the set consisting in the equivalence classes/blocks of ˆπ which
are contained within the odd numbers, k(ρ) is the number of blocks in ρ consisting
of only even numbers, l(ρ) is the number of blocks in ρ consisting of only odd
numbers, Tρ = Qk
i=1
1
N tr

T|ρi|
N

whenever ρ = {ρ1, . . . , ρk} is a partition with
blocks ρi, and |ρi| is the number of elements in ρi.
An information plus noise equivalent to Theorem 4.9 is also provided in
[Masucci et al., 2011] which requires further considerations of set partitions.
Both results arise from a generic diagrammatic framework to compute successive
moments of matrices invariant by row or column permutations.
We complete this introduction on extended combinatorics tools with recent
advances in the study of rectangular random matrices from a free probability
approach.
5.5
Rectangular free probability
A recent extension of free probability theory for Hermitian random matrices to
the most general rectangular matrices has been proposed by Benaych-Georges
[Benaych-Georges, 2009]. The quantity of interest is no longer the empirical
distribution of the eigenvalues of square Hermitian matrices but the symmetrized
singular law of rectangular matrices.
Deﬁnition 5.3. Let M ∈CN×n be a rectangular random matrix on (Ω, F, P).
The singular law µ of M is the uniform distribution of its singular values
s1, . . . , smin(n,N). The kth order moment Mk of µ is deﬁned as
Mk =
1
min(n, N) tr
 MMH k
2 .
The symmetrized singular law of M is the probability distribution ˜µ such that,
for any Borel set A ∈F, ˜µ(A) = 1
2(µ(A) + µ(−A)).

110
5. Combinatoric approaches
We have similar results for rectangular matrices as for Hermitian matrices. In
particular, we deﬁne a rectangular additive free convolution operator ‘⊞c’, with
c = limN N/n, which satisﬁes the following.
Theorem 5.12. Let M1, M2 be independent bi-unitarily invariant N × n
matrices whose symmetrized singular laws converge, respectively, to the measures
µ1 and µ2, as n, N grow to inﬁnity with limit ratio N/n →c. Then the
symmetrized singular law of M1 + M2 converges to a symmetric probability
measure, dependent on µ1, µ2, and c only, and which we denote µ1 ⊞c µ2.
The rectangular additive free convolution can be computed explicitly from an
equivalent rectangular R-transform which has the same property as in the square
case of summing cumulants of convolution of symmetrized singular laws.
Theorem 5.13. For a given symmetric distribution µ, denote
Rµ,c(z) ≜
∞
X
n=1
C2n,c(µ)zn
where C2n,c(µ) are the rectangular free cumulants of µ with ratio c, linked to the
free moments Mn(µ) of µ by
Mn(µ) =
X
π∈NC′(2n)
ce(π) Y
V ∈π
C|V |,c(µ)
where NC′(2n) is the subset of non-crossing partitions of {1, . . . , 2n} with all
blocks of even cardinality and e(π) is the number of blocks of π with even
cardinality.
Then, for two distributions µ1 and µ2, we have:
Rµ1⊞cµ2,c(z) = Rµ1,c(z) + Rµ2,c(z).
This is as far as we will go with rectangular random matrices, which is also a
very new ﬁeld of research in mathematics, with still few applications to wireless
communications; see [Gregoratti et al., 2010] for an example in the context of
relay networks with unitary precoders.
To conclude the last three theoretical chapters, we recollect the diﬀerent
techniques introduced so far in a short conclusion on the methodology to adopt
when addressing problems of random matrix theory. The methods to consider
heavily depend on the application sought for, on the time we have to invest on
the study, and obviously on the feasibility of every individual problem.

5.6. Methodology
111
5.6
Methodology
It is fundamental to understand why we would address a question regarding some
random matrix model from the analytical or the moments approach.
Say our intention is to study the e.s.d. F XN of a given random Hermitian
matrix XN ∈CN×N. Using the analytical methods, F XN will be treated as a
system parameter and will be shown to satisfy some classical analytic properties,
such as: F XN has a weak limit F, the Stieltjes transform of F is solution of some
implicit equation, etc. The moment-based methods will focus on establishing
results on the successive moments M1, M2, . . . of F (or E[F XN ]) when they exist,
such as: Mk is linked to the moments M ′
i, i = 1, . . . , k, of another distribution F ′,
Mk vanishes for k > 2 for growing dimensions of XN, etc. Both types of methods
will therefore ultimately give mutually consistent results. However, the choice of
a particular method over the other is often motivated by the following aspects.
1. Mathematical attractiveness. Both methods involve totally diﬀerent
mathematical tools and it often turns out that one method is preferable
over the other in this respect. In particular, moment-based methods, while
leading to tedious combinatorial computations, are very attractive due to their
mechanical and simple way of working. In contrast, the analytical methods are
not so ﬂexible in some respects and are not yet able to solve many problems
already addressed by diﬀerent moment-based methods. For instance, in
Section 5.3, we introduced results on large dimensional random Vandermonde,
Toeplitz, Hankel matrices and bi-unitarily invariant rectangular matrices,
which analytical methods are far from being able to provide. However, the
converse also holds: some random matrix models can be studied by the
Stieltjes transform approach, while moment-based methods are unusable. This
is in fact the case for all random matrix models involving matrices with
independent entries that are often not unitarily invariant. It might also turn
out that the moment-based methods are of no use for the evaluation of certain
functionals of F. For instance, the fact that the series expansion of log(1 + x)
has convergence radius 1 implies that the moments of F do not allow us to
estimate
R
log(1 + xλ)dF(λ) for large x. The immediate practical consequence
is that most capacity expressions cannot be evaluated from the moments of
F.
2. Application context. The most important drawback of the moment-based
methods lies in their results consisting of a series of properties concerning
the individual or joint free moments. If we are interested in studying the
limiting distribution F of the e.s.d. of a given random matrix XN ∈CN×N,
two cases generally occur: (i) F is a step function with K discontinuities, i.e.
F has K distinct eigenvalues with large multiplicities, in which case results on
the ﬁrst M1, . . . , MK may be suﬃcient to obtain (or estimate) F completely
(especially if the multiplicities are a priori known), although this estimate
is likely to perform poorly if only K moments are used, and (ii) F is a non-

112
5. Combinatoric approaches
trivial function, in which case all moments are in general needed to accurately
evaluate it. In case (ii), moment-based methods are not desirable because a
large number of moments need to be estimated (this often goes along with high
computational complexity) and because the moment estimates are themselves
correlated according to some non-trivial joint probability distribution, which is
even more computationally complex to evaluate. Typically, a small error in the
estimate of the moment of order 1 propagates into a larger error in the estimate
of the moment of order 2, which itself propagates forward into higher moment
estimates. These errors need to be tracked precisely to optimally exploit the
successive estimates. Analytical methods, if numerically solvable, are much
more appealing in this case. In general, the result of these methods expresses
as an approximation of mF by another Stieltjes transform, the approximation
being asymptotically accurate as the system dimensions grow large. The
analytical approach will especially be shown often to rely on computationally
inexpensive ﬁxed-point algorithms with proven convergence, see, e.g. Chapters
12–15, while moment-based methods require involved combinatorial calculus
when a large number of moments has to be taken into account.
3. Interpretation purpose. In general, analytical methods provide very
compact expressions, from which the typical behavior of the relevant problem
parameters can be understood. The example of the optimality of the water-
ﬁlling algorithm in the capacity maximization problem is a typical case where
this phenomenon appears, see Chapters 13–14. On the moment-based method
side, results appear in the form of lengthy combinatorial calculus, from which
physical interpretation is not always possible.
This concludes the set of three chapters on the limiting results of large
dimensional random matrices, using the Stieltjes transform, free probability
theory, and related methods using moments. The next chapter will be
dedicated to further extensions of the Stieltjes transform approach, which are
of fundamental use in the applicative context of wireless communications. The
ﬁrst of these approaches extends the limiting results of Theorems 3.13, 3.14,
3.15, 4.10, 4.11, etc., to the case where the e.s.d. of the underlying matrices does
not necessarily converge and allows us to provide accurate approximations of the
empirical Stieltjes transform for all ﬁnite matrix dimensions. This will be shown
to have crucial consequences from a practical point of view, in particular for the
performance study of large dimensional wireless communication systems.

6
Deterministic equivalents
6.1
Introduction to deterministic equivalents
The ﬁrst applications of random matrix theory to the ﬁeld of wireless
communications, e.g., [Tse and Hanly, 1999; Tse and Verd´u, 2000; Verd´u and
Shamai, 1999], originally dealt with the limiting behavior of some simple random
matrix models. In particular, these results are attractive as these limiting
behaviors only depend on the limiting eigenvalue distribution of the deterministic
matrices of the model. This is in fact the case of all the results we have derived
and introduced so far; for instance, Theorem 3.13 unveils the limiting behavior of
the e.s.d. of BN = AN + XH
NTNXN when both e.s.d. of AN and TN converge
toward given deterministic distribution functions and XN is random with i.i.d.
entries. However, for practical applications, it might turn out that:
(i) the e.s.d. of AN or TN do not necessarily converge to a limiting distribution;
(ii) even if the e.s.d. of the deterministic matrices in the model do all converge to
their respective l.s.d., the e.s.d. of the output matrix BN might not converge.
This is of course not the case in Theorem 3.13, but we will show that this may
happen for more involved models, e.g. the models treated by [Couillet et al.,
2011a] and [Hachem et al., 2007].
Let us introduce a simple scenario for which the e.s.d. of the random matrix
does not converge. This example is borrowed from [Hachem et al., 2007]. Deﬁne
XN ∈C2N×2N as
XN =
X′
N 0
0
0

(6.1)
with the entries of X′
N being i.i.d. with zero mean and variance
1
N . Consider in
addition the matrix TN ∈C2N×2N deﬁned as
TN =







IN 0
0 0

, N even
0 0
0 IN

, N odd.
(6.2)
Then,
taking
BN = (TN + XN)(TN + XN)H,
F B2N
and
F B2N+1
both
converge weakly towards limit distributions, as N →∞, but those distributions

114
6. Deterministic equivalents
0
1
2
3
4
5
0
1
2
3
4
5
Eigenvalues
Density
0
1
2
3
4
5
0
1
2
3
4
5
Eigenvalues
Density
Figure 6.1 Histogram of the eigenvalues of BN = (TN + XN)(TN + XN)H modeled in
(6.1)–(6.2), for N = 1000 (top) and N = 1001 (bottom).
diﬀer. Indeed, for N even, half of the spectrum of BN is formed of zeros, while
for N odd, half of the spectrum of BN is formed of ones, the rest of the spectrum
being a weighted version of the Mar˘cenko–Pastur law. And therefore there does
not exist a limit to F BN , while F XNXH
N tends to the uniformly weighted sum of
the Mar˘cenko–Pastur law and a mass in zero, and F TNTH
N tends to the uniformly
weighted sum of two masses in zero and one. This is depicted in Figure 6.1.
In such situations, there is therefore no longer any interest in looking at
the asymptotic behavior of e.s.d. Instead, we will be interested in ﬁnding
deterministic equivalents for the underlying model.

6.2. Techniques for deterministic equivalents
115
Deﬁnition 6.1. Consider a series of Hermitian random matrices B1, B2, . . .,
with BN ∈CN×N and a series f1, f2, . . . of functionals of 1 × 1, 2 × 2, . . .
matrices. A deterministic equivalent of BN for the functional fN is a series
B◦
1, B◦
2, . . . where B◦
N ∈CN×N, of deterministic matrices, such that
lim
N→∞fN(BN) −fN(B◦
N) →0
where the convergence will often be with probability one. Note that fN(B◦
N)
does not need to have a limit as N →∞. We will similarly call gN ≜fN(B◦
N)
the deterministic equivalent of fN(BN), i.e. the deterministic series g1, g2, . . .
such that fN(BN) −gN →0 in some sense.
We will often take fN to be the normalized trace of (BN −zIN)−1, i.e. the
Stieltjes transform of F BN . When fN(B◦
N) does not have a limit, the Mar˘cenko–
Pastur method, developed in Section 3.2, will fail. This is because, at some point,
all the entries of the underlying matrices will have to be taken into account and
not only the diagonal entries, as in the proof we provided in Section 3.2. However,
the Mar˘cenko–Pastur method can be tweaked adequately into a technique that
can cope with deterministic equivalents. In the following, we ﬁrst introduce this
technique, which we will call the Bai and Silverstein technique, and then discuss
an alternative technique, known as the Gaussian method, which is particularly
suited to random matrix models with Gaussian entries. Hereafter, we detail these
methods by successively proving two (similar) results of importance in wireless
communications, see further Chapters 13–14.
6.2
Techniques for deterministic equivalents
6.2.1
Bai and Silverstein method
We ﬁrst introduce a deterministic equivalent for the model
BN =
K
X
k=1
R
1
2
k XkTkXH
k R
1
2
k + A
where the K matrices Xk have i.i.d. entries for each k, mutually independent for
diﬀerent k, and the matrices T1, . . . , TK, R1, . . . , RK and A are ‘bounded’ in
some sense to be deﬁned later. This is more general than the model of Theorem
3.13 in several respects:
(i) left product matrices Rk, 1 ≤k ≤K, have been introduced. As an exercise,
it can already be veriﬁed that a l.s.d. for the model R
1
2
1 X1T1XH
1 R
1
2
1 + A may
not exist even if F R1 and F A both converge vaguely to deterministic limits,
unless some severe additional constraint is put on the eigenvectors of R1 and
A, e.g. R1 and A are codiagonalizable. This suggests that the Mar˘cenko–
Pastur method will fail to treat this model;

116
6. Deterministic equivalents
(ii) a sum of K such models is considered (K does not grow along with N here);
(iii) the e.s.d. of the (possibly random) matrices Tk and Rk are not required to
converge.
While the result to be introduced hereafter is very likely to hold for X1, . . . , XK
with non-identically distributed entries (as long as they have common mean and
variance and some higher order moment condition), we only present here the
result where these entries are identically distributed, which is less general than
the conditions of Theorem 3.13.
Theorem 6.1 ([Couillet et al., 2011a]). Let K be some positive integer. For
some integer N, let
BN =
K
X
k=1
R
1
2
k XkTkXH
k R
1
2
k + A
be an N × N matrix with the following hypotheses, for all k ∈{1, . . . , K}
1. Xk =

1
√nk Xk,ij

∈CN×nk is such that the Xk,ij are identically distributed
for all N, i, j, independent for each ﬁxed N, and E|Xk,11 −EXk,11|2 = 1;
2. R
1
2
k ∈CN×N is a Hermitian non-negative deﬁnite square root of the non-
negative deﬁnite Hermitian matrix Rk;
3. Tk = diag(τk,1, . . . , τk,nk) ∈Cnk×nk, nk ∈N∗, is diagonal with τk,i ≥0;
4. the sequences F T1, F T2, . . . and F R1, F R2, . . . are tight, i.e. for all ε > 0, there
exists M > 0 such that 1 −F Tk(M) < ε and 1 −F Rk(M) < ε for all nk, N;
5. A ∈CN×N is Hermitian non-negative deﬁnite;
6. denoting ck = N/nk, for all k, there exist 0 < a < b < ∞for which
a ≤lim inf
N ck ≤lim sup
N
ck ≤b.
(6.3)
Then, as all N and nk grow large, with ratio ck, for z ∈C \ R+, the Stieltjes
transform mBN (z) of BN satisﬁes
mBN (z) −mN(z)
a.s.
−→0
(6.4)
where
mN(z) = 1
N tr
 
A +
K
X
k=1
Z
τkdF Tk(τk)
1 + ckτkeN,k(z)Rk −zIN
!−1
(6.5)
and the set of functions eN,1(z), . . . , eN,K(z) forms the unique solution to the K
equations
eN,i(z) = 1
N tr Ri
 
A +
K
X
k=1
Z
τkdF Tk(τk)
1 + ckτkeN,k(z)Rk −zIN
!−1
(6.6)
such that sgn(ℑ[eN,i(z)]) = sgn(ℑ[z]), if z ∈C \ R, and eN,i(z) > 0 if z is real
negative.

6.2. Techniques for deterministic equivalents
117
Moreover, for any ε > 0, the convergence of Equation (6.4) is uniform over
any region of C bounded by a contour interior to
C \ ({z : |z| ≤ε} ∪{z = x + iv : x > 0, |v| ≤ε}) .
For all N, the function mN is the Stieltjes transform of a distribution function
FN, and
F BN −FN ⇒0
almost surely as N →∞.
In [Couillet et al., 2011a], Theorem 6.1 is completed by the following result.
Theorem
6.2.
Under
the
conditions
of
Theorem
6.1,
the
scalars
eN,1(z), . . . , eN,K(z) are also explicitly given by:
eN,i(z) = lim
t→∞et
N,i(z)
where, for all i, e0
N,i(z) = −1/z and, for t ≥1
et
N,i(z) = 1
N tr Ri

A +
K
X
j=1
Z
τjdF Tj(τj)
1 + cjτjet−1
N,j(z)Rj −zIN


−1
.
This result, which ensures the convergence of the classical ﬁxed-point
algorithm for an adequate initial condition, is of fundamental importance for
practical purposes as it ensures that the eN,1(z), . . . , eN,K(z) can be determined
numerically in a deterministic way. Since the proof of Theorem 6.2 relies heavily
on the proof of Theorem 6.1, we will prove Theorem 6.2 later.
Several remarks are in order before we prove Theorem 6.1. We have given
much detail on the conditions for Theorem 6.1 to hold. We hereafter discuss the
implications of these conditions. Condition 1 requires that the Xk,ij be identically
distributed across N, i, j, but not necessarily across k. Note that the identical
distribution condition could be further released under additional mild conditions
(such as all entries must have a moment of order 2 + ε, for some ε > 0), see
Theorem 3.13. Condition 4 introduces tightness requirements on the e.s.d. of Rk
and Tk. Tightness can be seen as the probabilistic equivalent to boundedness
for deterministic variables. Tightness ensures here that no mass of the F Rk and
F Tk escapes to inﬁnity as n grows large. Condition 6 is more general than the
requirement that ck has a limit as it allows ck, for all k, to wander between two
positive values.
From a practical point of view, R
1
2
KXkT
1
2
k will often be used to model a
multiple antenna N × nk channel with i.i.d. entries with transmit and receive
correlations. From the assumptions of Theorem 6.1, the correlation matrices Rk
and Tk are only required to be ‘bounded’ in the sense of tightness of their e.s.d.
This means that, as the number of antennas grows, the eigenvalues of Rk and Tk

118
6. Deterministic equivalents
can only blow up with increasingly low probability. If we increase the number N
of antennas on a bounded three-dimensional space, then the rough tendency is
for the eigenvalues of Tk and Rk to be all small except for a few of them, which
grow large but have a probability of order O(1/N), see, e.g., [Pollock et al., 2003].
In that context, Theorem 6.1 holds, i.e. for N →∞, F BN −FN ⇒0.
It is also important to remark that the matrices Tk are constrained to be
diagonal. This is unimportant when the matrices Xk are assumed Gaussian in
practical applications, as the Xk, being bi-unitarily invariant, can be multiplied
on the right by any deterministic unitary matrix without altering the ﬁnal
result. This limitation is linked to the technique used for proving Theorem
6.1. For mathematical completion, though, it would be convenient for the
matrices Tk to be unconstrained. We mention that Zhang and Bai [Zhang, 2006]
derive the limiting spectral distribution of the model BN = R
1
2
1 X1T1XH
1 R
1
2
1 for
unconstrained Hermitian T1, using a diﬀerent approach than that presented
below.
For practical applications, it will be easier in the following to write (6.6) in a
more symmetric way. This is discussed in the following remark.
Remark 6.1. In the particular case where A = 0, the K implicit Equations (6.6)
can be developed into the 2K linked equations
eN,i(z) = 1
N tr Ri
 
−z
"
IN +
K
X
k=1
¯ek(z)Rk
#!−1
¯eN,i(z) = 1
ni
tr Ti (−z [Ini + cieN,i(z)Ti])−1
(6.7)
whose symmetric aspect is both more readable and more useful for practical
reasons that will be evidenced later in Chapters 13–14. As a consequence, mN(z)
in (6.5) becomes
mN(z) = 1
N tr
 
−z
"
IN +
K
X
k=1
¯eN,k(z)Rk
#!−1
.
In the literature and, as a matter of fact, in some deterministic equivalents
presented later in this chapter, the variables eN,i(z) may be normalized by
1
ni
instead of
1
N in order to avoid carrying the factor ci in front of eN,i(z) in the
second ﬁxed-point equation of (6.7). In the application chapters, Chapters 12–15,
depending on the situation, either one or the other convention will be taken.
We present hereafter the general techniques, based on the Stieltjes transform,
to prove Theorem 6.1 and other similar results introduced in this section.
As opposed to the proof of the Mar˘cenko–Pastur law, we cannot prove that
that there exists a space of probability one over which mBN (z) →m(z) for
all z ∈C \ R+, for a certain limiting function m. Instead, we prove that there
exists a space of probability one over which mBN (z) −mN(z) →0 for all z, for
a certain series of Stieltjes transforms m1(z), m2(z), . . .. There are in general

6.2. Techniques for deterministic equivalents
119
two main approaches to prove this convergence. The ﬁrst option is a point-
wise approach that consists in proving the convergence for all z in a compact
subspace of C \ R+ having a limit point. Invoking Vitali’s convergence theorem,
similar to the proof of the Mar˘cenko–Pastur law, we then prove the convergence
for all z ∈C \ R+. In the coming proof, we will take z ∈C+. In the proof of
Theorem 6.17, we will take z real negative. The second option is a functional
approach in which the objects under study are not mBN (z) and mN(z) taken
at a precise point z ∈C \ R+ but rather mBN (z) and mN(z) seen as functions
lying in the space of Stieltjes transforms of distribution functions with support
on R+. The convergence mBN (z) −mN(z)
a.s.
−→0 is in this case functional and
Vitali’s convergence theorem is not called for. This is the approach followed in,
e.g., [Hachem et al., 2007]. The latter is not detailed in this book.
The ﬁrst step of the general proof, for either option, consists in determining
mN(z). For this, similar to the Mar˘cenko–Pastur proof, we develop the expression
of mBN (z), seeking for a limiting result of the kind
mBN (z) −hN(mBN (z); z)
a.s.
−→0
for some deterministic function hN, possibly depending on N. Such an expression
allows us to infer the nature of a deterministic approximation mN(z) of mBN (z)
as a particular solution of the equation in m
m −hN(m; z) = 0.
(6.8)
This equation rarely has a unique point-wise solution, i.e. for every z, but often
has a unique functional solution z →mN(z) that is the Stieltjes transform of a
distribution function. If the point-wise approach is followed, a unique point-wise
solution of (6.8) can often be narrowed down to a certain subspace of C for z lying
in some other subspace of C. In Theorem 6.1, there exists a single solution in C+
when z ∈C+, a single solution in C−when z ∈C−, and a single positive solution
when z is real negative. Standard holomorphicity arguments on the function
mN(z) then ensure that z →mN(z) is the unique Stieltjes transform satisfying
hN(mN(z); z) = mN(z). When using the functional approach, this fact tends to
be proved more directly. In the coming proof of Theorem 6.1, we will prove point-
wise uniqueness by assuming, as per standard techniques, the alleged existence
of two distinct solutions and prove a contradiction. An alternative approach is
to prove that the ﬁxed-point algorithm
m0 ∈D
mt+1 = hN(mt; z), t ≥0
always converges to mN(z), where D is taken to be either R−, C+ or C−. This
approach, when valid (in some involved cases, convergence may not always arise),
is doubly interesting as it allows both (i) to prove point-wise uniqueness for z
taken in some subset of C \ R+, leading to uniqueness of the Stieltjes transform
using again holomorphicity arguments, and (ii) to provide an explicit algorithm

120
6. Deterministic equivalents
to compute mN(z) for z ∈D, which is in particular of interest for practical
applications when z = −σ2 < 0. In the proof of Theorem 6.1, we will introduce
both results for completion. In the proof of Theorem 6.17, we will directly proceed
to proving the convergence of the ﬁxed-point algorithm for z real negative.
When the uniqueness of the Stieltjes transform mN(z) has been made clear,
the last step is to prove that, in the large N limit
mBN (z) −mN(z)
a.s.
−→0.
This step is not so immediate. To this point, we indeed only know that
mBN (z) −hN(mBN (z); z)
a.s.
−→0 and mN(z) −hN(mN(z); z) = 0. This does not
imply immediately that mBN (z) −mN(z)
a.s.
−→0. If there are several point-wise
solutions to m −hN(m; z) = 0, we need to verify that mN(z) was chosen to be
the one that will eventually satisfy mBN (z) −mN(z)
a.s.
−→0. This will conclude
the proof.
We now provide the speciﬁc proof of Theorem 6.1. In order to determine the
above function hN, we ﬁrst develop the Mar˘cenko–Pastur method (for simplicity
for K = 2 and A = 0). We will realize that this method fails unless all Rk and
A are constrained to be co-diagonalizable. To cope with this limitation, we will
introduce the more powerful Bai and Silverstein method, whose idea is to guess
along the derivations the suitable form of hN. In fact, as we will shortly realize,
the problem is slightly more diﬃcult here as we will not be able to ﬁnd such
a function hN (which may actually not exist at all in the ﬁrst place). We will
however be able to ﬁnd functions fN,i such that, for each i
eBN,i(z) −fN,i(eBN,1(z), . . . , eBN,K(z); z)
a.s.
−→0
where eBN,i(z) ≜1
N tr Ri(BN −zIN)−1. We will then look for a function eN,i(z)
that satisﬁes
eN,i(z) = fN,i(eN,1(z), . . . , eN,K(z); z).
From there, it will be easy to determine a further function gN such that
mBN (z) −gN(eBN,1(z), . . . , eBN,K(z); z)
a.s.
−→0
and
mN(z) −gN(eN,1(z), . . . , eN,K(z); z) = 0.
We will therefore have ﬁnally
mBN (z) −mN(z)
a.s.
−→0.
Proof of Theorem 6.1. In order to have a ﬁrst insight on what the deterministic
equivalent mN of mBN may look like, the Mar˘cenko–Pastur method will be
applied with the (strong) additional assumption that A and all Rk, 1 ≤k ≤K,
are diagonal and that the e.s.d. F Tk, F Rk converge for all k as N grows large.
In this scenario, mBN has a limit when N →∞and the method, however more
tedious than in the proof of the Mar˘cenko–Pastur law, leads naturally to mN.

6.2. Techniques for deterministic equivalents
121
Consider the case when K = 2, A = 0 for simplicity and denote Hk =
R
1
2
k XkT
1
2
k . Following similar steps as in the proof of the Mar˘cenko–Pastur law,
we start with matrix inversion lemmas
 H1HH
1 + H2HH
2 −zIN
−1
11
=
"
−z −z[hH
1 hH
2 ]
UH
1
UH
2

[U1U2] −zIn1+n2
−1 h1
h2
#−1
with the deﬁnition HH
i = [hiUH
i ]. Using the block matrix inversion lemma, the
inner inversed matrix in this expression can be decomposed into four submatrices.
The upper-left n1 × n1 submatrix reads:
 −zUH
1 (U2UH
2 −zIN−1)−1U1 −zIn1
−1
while, for the second block diagonal entry, it suﬃces to revert all ones in twos and
vice-versa. Taking the limits, using Theorem 3.4 and Theorem 3.9, we observe
that the two oﬀ-diagonal submatrices will not play a role, and we ﬁnally have
 H1HH
1 + H2HH
2 −zIN
−1
11
≃

−z −zr11
1
n1
tr T1
 −zHH
1 (H2HH
2 −zIN)−1H1 −zIn1
−1
−zr21
1
n2
tr T2
 −zHH
2 (H1HH
1 −zIN)−1H1 −zIn2
−1−1
where
the
symbol
“≃”
denotes
some
kind
of
yet
unknown
large
N
convergence and where we denoted rij
the jth diagonal entry of Ri.
Observe
that
we
can
proceed
to
a
similar
derivation
for
the
matrix
T1
 −zHH
1 (H2HH
2 −zIN)−1H1 −zIn1
−1 that now appears. Denoting now Hi =
[˜hi ˜Ui], we have indeed
h
T1
 −zHH
1 (H2HH
2 −zIN)−1H1 −zIn1
−1i
11
= τ11

−z −z˜hH
1

˜U1 ˜UH
1 + H2HH
2 −zIN
−1 ˜h1
−1
≃τ11

−z −zc1τ11
1
N tr R1
 H1HH
1 + H2HH
2 −zIN
−1−1
with τij the jth diagonal entry of Ti. The limiting result here arises from the
trace lemma, Theorem 3.4 along with the rank-1 perturbation lemma, Theorem
3.9. The same result holds when changing ones in twos.
We now denote by ei and ¯ei the (almost sure) limits of the random quantities
eBN,i = 1
N tr Ri
 H1HH
1 + H2HH
2 −zIN
−1
and
¯eBN,i = 1
N tr Ti
 −zHH
1 (H2HH
2 −zIN)−1H1 −zIn1
−1

122
6. Deterministic equivalents
respectively, as F Ti and F Ri converge in the large N limit. These limits exist
here since we forced R1 and R2 to be co-diagonalizable. We ﬁnd
ei = lim
N→∞
1
N tr Ri (−z¯eBN,iR1 −z¯eBN,iR2 −zIN)−1
¯ei = lim
N→∞
1
N tr Ti (−zcieBN,iTi −zIni)−1
where the type of convergence is left to be determined. From this short calculus,
we can infer the form of (6.7).
This derivation obviously only provides a hint on the deterministic equivalent
for mN(z). It also provides the aforementioned observation that mN(z) is not
itself solution of a ﬁxed-point equation, although eN,1(z), . . . , eN,K(z) are. To
prove Theorem 6.1, irrespective of the conditions imposed on R1, . . . , RK,
T1, . . . , TK and A, we will successively go through four steps, given below.
For readability, we consider the case K = 1 and discard the useless indexes.
The generalization to K ≥1 is rather simple for most of the steps but requires
cumbersome additional calculus for some particular aspects. These pieces of
calculus are not interesting here, the reader being invited to refer to [Couillet
et al., 2011a] for more details. The four-step procedure is detailed below.
• Step 1. We ﬁrst seek a function fN, such that, for z ∈C+
eBN (z) −fN(eBN (z); z)
a.s.
−→0
as N →∞, where eBN (z) = 1
N tr R(BN −zIN)−1. This function fN was
already inferred by the Mar˘cenko–Pastur approach. Now, we will make this
step rigorous by using the Bai and Silverstein approach, as is done in,
e.g., [Dozier and Silverstein, 2007a; Silverstein and Bai, 1995]. Basically, the
function fN will be found using an inference procedure. That is, starting
from a very general form of fN, i.e. fN = 1
N tr RD−1 for some matrix D ∈
CN×N (not yet written as a function of z or eBN (z)), we will evaluate the
diﬀerence eBN (z) −fN and progressively discover which matrix D will make
this diﬀerence increasingly small for large N.
• Step 2. For ﬁxed N, we prove the existence of a solution to the implicit
equation in the dummy variable e
fN(e; z) = e.
(6.9)
This is often performed by proving the existence of a sequence eN,1, eN,2, . . .,
lying in a compact space such that fN(eN,k; z) −eN,k converges to zero, in
which case there exists at least one converging subsequence of eN,1, eN,2, . . .,
whose limit eN satisﬁes (6.9).
• Step 3. Still for ﬁxed N, we prove the uniqueness of the solution of (6.9)
lying in some speciﬁc space and we call this solution eN(z). This is classically
performed by assuming the existence of a second distinct solution and by
exhibiting a contradiction.

6.2. Techniques for deterministic equivalents
123
• Step 4. We ﬁnally prove that
eBN (z) −eN(z)
a.s.
−→0
and, similarly, that
mBN (z) −mN(z)
a.s.
−→0
as N →∞, with mN(z) ≜gN(eN(z); z) for some function gN.
At ﬁrst, following the works of Bai and Silverstein, a truncation, centralization,
and rescaling step is required to replace the matrices X, R, and T by truncated
versions ˆX, ˆR, and ˆT, respectively, such that the entries of ˆX have zero mean,
∥ˆX∥≤k log(N), for some constant k, ∥ˆR∥≤log(N) and ∥ˆT∥≤log(N). Similar
to the truncation steps presented in Section 3.2.2, it is shown in [Couillet et al.,
2011a] that these truncations do not restrict the generality of the ﬁnal result for
{F T} and {F R} forming tight sequences, that is:
F
ˆR
1
2 ˆX ˆT ˆXH ˆR
1
2 −F R
1
2 XTXHR
1
2 ⇒0
almost surely, as N grows large. Therefore, we can from now on work with these
truncated matrices. We recall that the main interest of this procedure is to be
able to derive a deterministic equivalent (or l.s.d.) of the underlying random
matrix model without the need for any moment assumption on the entries of X,
by replacing the entries of X by truncated random variables that have moments
of all orders. Here, the interest is in fact two-fold, since, in addition to truncating
the entries of X, also the entries of T and R are truncated in order to be able
to prove results for matrices T and R that in reality have eigenvalues growing
very large but that will be assumed to have entries bounded by log(N). For
readability in the following, we rename X, T, and R the truncated matrices.
Remark 6.2. Alternatively, expected values can be used to discard the stochastic
character. This introduces an additional convergence step, which is the approach
followed by Hachem, Najim, and Loubaton in several publications, e.g., [Hachem
et al., 2007] and [Dupuy and Loubaton, 2009]. This additional step consists in
ﬁrst proving the almost sure weak convergence of F BN −GN to zero, for GN
some auxiliary deterministic distribution (such as GN = E[F BN ]), before proving
the convergence GN −FN ⇒0.
Step 1. First convergence step
We start with the introduction of two fundamental identities.
Lemma 6.1 (Resolvent identity). For invertible A and B matrices, we have the
identity
A−1 −B−1 = −A−1(A −B)B−1.

124
6. Deterministic equivalents
This can be veriﬁed easily by multiplying both sides on the left by A and on
the right by B (the resulting equality being equivalent to Lemma 6.1 for A and
B invertible).
Lemma 6.2 (A matrix inversion lemma, (2.2) in [Silverstein and Bai, 1995]).
Let A ∈CN×N be Hermitian invertible, then, for any vector x ∈CN and any
scalar τ ∈C, such that A + τxxH is invertible
xH(A + τxxH)−1 =
xHA−1
1 + τxHA−1x.
This is veriﬁed by multiplying both sides by A + τxxH from the right.
Lemma 6.1 is often referred to as the resolvent identity, since it will be
mainly used to take the diﬀerence between matrices of type (X −zIN)−1 and
(Y −zIN)−1, which we remind are called the resolvent matrices of X and Y,
respectively.
The fundamental idea of the approach by Bai and Silverstein is to guess the
deterministic equivalent of mBN (z) by writing it under the form 1
N tr D−1 at ﬁrst,
where D needs to be determined. This will be performed by taking the diﬀerence
mBN (z) −1
N tr D−1 and, along the lines of calculus, successively determining the
good properties D must satisfy so that the diﬀerence tends to zero almost surely.
We then start by taking z ∈C+ and D ∈CN×N some invertible matrix whose
normalized trace would ideally be close to mBN (z) = 1
N tr(BN −zIN)−1. We
then write
D−1 −(BN −zIN)−1 = D−1(A + R
1
2 XTXHR
1
2 −zIN −D)(BN −zIN)−1
(6.10)
using Lemma 6.1.
Notice here that, since BN is Hermitian non-negative deﬁnite, and z ∈C+, the
term (BN −zIN)−1 has uniformly bounded spectral norm (bounded by 1/ℑ[z]).
Since D−1 is desired to be close to (BN −zIN)−1, the same property should also
hold for D−1. In order for the normalized trace of (6.10) to be small, we need
therefore to focus exclusively on the inner diﬀerence on the right-hand side. It
seems then interesting at this point to write D ≜A −zIN + pNR for pN left to
be deﬁned. This leads to
D−1 −(BN −zIN)−1
= D−1R
1
2  XTXH
R
1
2 (BN −zIN)−1 −pND−1R(BN −zIN)−1
= D−1
n
X
j=1
τjR
1
2 xjxH
j R
1
2 (BN −zIN)−1 −pND−1R(BN −zIN)−1
where in the second equality we used the fact that XTXH = Pn
j=1 τjxjxH
j , with
xj ∈CN the jth column of X and τj the jth diagonal element of T. Denoting
B(j) = BN −τjR
1
2 xjxH
j R
1
2 , i.e. BN with column j removed, and using Lemma

6.2. Techniques for deterministic equivalents
125
6.2 for the matrix B(j), we have:
D−1 −(BN −zIN)−1
=
n
X
j=1
τj
D−1R
1
2 xjxH
j R
1
2 (B(j) −zIN)−1
1 + τjxHR
1
2 (B(j) −zIN)−1R
1
2 xj
−pND−1R(BN −zIN)−1.
Taking the trace on each side, and recalling that, for a vector x and a matrix
A, tr(AxxH) = tr(xHAx) = xHAx, this becomes
1
N tr D−1 −1
N tr(BN −zIN)−1
= 1
N
n
X
j=1
τj
xH
j R
1
2 (B(j) −zIN)−1D−1R
1
2 xj
1 + τjxHR
1
2 (B(j) −zIN)−1R
1
2 xj
−pN
1
N tr R(BN −zIN)−1D−1
(6.11)
where quadratic forms of the type xHAx appear.
Remembering the trace lemma, Theorem 3.4, which can a priori be applied to
the terms xH
j R
1
2 (B(j) −zIN)−1D−1R
1
2 xj since xj is independent of the matrix
R
1
2 (B(j) −zIN)−1D−1R
1
2 , we notice that by setting
pN = 1
n
n
X
j=1
τj
1 + τjc 1
N tr R(BN −zIN)−1 .
Equation (6.11) becomes
1
N tr D−1 −1
N tr(BN −zIN)−1
= 1
N
n
X
j=1
τj
"
xH
j R
1
2 (B(j) −zIN)−1D−1R
1
2 xj
1 + τjxHR
1
2 (B(j) −zIN)−1R
1
2 xj
−
1
n tr R(BN −zIN)−1D−1
1 + cτj 1
N tr R(BN −zIN)−1
#
(6.12)
which is suspected to converge to zero as N grows large, since both the
numerators and the denominators converge to one another. Let us assume for
the time being that the diﬀerence eﬀectively goes to zero almost surely. Equation
(6.12) implies
1
N tr(BN −zIN)−1 −1
N tr

A + 1
n
n
X
j=1
τjR
1 + τjc 1
N tr R(BN −zIN)−1 −zIN


−1
a.s.
−→0
which determines mBN (z) = 1
N tr(BN −zIN)−1 as a function of the trace
1
N tr R(BN −zIN)−1, and not as a function of itself. This is the observation
made earlier when we obtained a ﬁrst hint on the form of mN(z) using the
Mar˘cenko–Pastur method, according to which we cannot ﬁnd a function fN
such that mBN (z) −fN(mBN (z), z)
a.s.
−→0. Instead, running the same steps as

126
6. Deterministic equivalents
above, it is rather easy now to observe that
1
N tr RD−1 −1
N tr R(BN −zIN)−1
= 1
N
n
X
j=1
τj
"
xH
j R
1
2 (B(j) −zIN)−1RD−1R
1
2 xj
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
−
1
n tr R(BN −zIN)−1RD−1
1 + τj c
N tr R(BN −zIN)−1
#
where ∥R∥≤log N. Then, denoting eBN (z) ≜1
N tr R(BN −zIN)−1, we suspect
to have also
eBN (z) −1
N tr R

A + 1
n
n
X
j=1
τj
1 + τjceBN (z)R −zIN


−1
a.s.
−→0
and
mBN (z) −1
N tr

A + 1
n
n
X
j=1
τj
1 + τjceBN (z)R −zIN


−1
a.s.
−→0
which is exactly what was required, i.e. eBN (z) −fN(eBN (z); z)
a.s.
−→0 with
fN(e; z) = 1
N tr R

A + 1
n
n
X
j=1
τj
1 + τjceR −zIN


−1
and mBN (z) −gN(eBN (z); z)
a.s.
−→0 with
gN(e; z) = 1
N tr

A + 1
n
n
X
j=1
τj
1 + τjceR −zIN


−1
.
We now prove that the right-hand side of (6.12) converges to zero almost
surely. This rather technical part justiﬁes the use of the truncation steps and
is the major diﬀerence between the works of Bai and Silverstein [Dozier and
Silverstein, 2007a; Silverstein and Bai, 1995] and the works of Hachem et al.
[Hachem et al., 2007]. We ﬁrst deﬁne
wN ≜
n
X
j=1
τj
N
"
xH
j R
1
2 (B(j) −zIN)−1RD−1R
1
2 xj
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
−
1
n tr R(BN −zIN)−1RD−1
1 + τj c
N tr R(BN −zIN)−1
#
which we then divide into four terms, in order to successively prove the
convergence of the numerators and the denominators. Write
wN = 1
N
n
X
j=1
τj
 d1
j + d2
j + d3
j + d4
j


6.2. Techniques for deterministic equivalents
127
where
d1
j = xH
j R
1
2 (B(j) −zIN)−1RD−1R
1
2 xj
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
−
xH
j R
1
2 (B(j) −zIN)−1RD−1
(j)R
1
2 xj
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
d2
j =
xH
j R
1
2 (B(j) −zIN)−1RD−1
(j)R
1
2 xj
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
−
1
n tr R(B(j) −zIN)−1RD−1
(j)
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
d3
j =
1
n tr R(B(j) −zIN)−1RD−1
(j)
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
−
1
n tr R(BN −zIN)−1RD−1
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
d4
j =
1
n tr R(BN −zIN)−1RD−1
1 + τjxH
j R
1
2 (B(j) −zIN)−1R
1
2 xj
−
1
n tr R(BN −zIN)−1RD−1
1 + cτjeBN
where we introduced D(j) = A + 1
n
Pn
k=1
τk
1+τkceB(j)(z)R −zIN, i.e. D with
eBN (z) replaced by eB(j)(z). Under these notations, it is simple to show that
wN
a.s.
−→0 since every term dk
j can be shown to go fast to zero.
One of the diﬃculties in proving that the dk
j tends to zero at a suﬃciently fast
rate lies in providing inequalities for the quadratic terms of the type yH(A −
zIN)−1y present in the denominators. For this, we use Corollary 3.2, which
states that, for any non-negative deﬁnite matrix A, y ∈CN and for z ∈C+

1
1 + τjyH(A −zIN)−1y
 ≤|z|
ℑ[z].
(6.13)
Also, we need to ensure that D−1 and D−1
(j) have uniformly bounded spectral
norm. This unfolds from the following lemma.
Lemma 6.3 (Lemma 8 of [Couillet et al., 2011a]). Let D = A + iB + ivIN,
with A ∈CN×N Hermitian, B ∈CN×N Hermitian non-negative and v > 0. Then
∥D∥≤v−1.
Proof. Noticing that DDH = (A + iB)(A −iB) + v2IN + 2vB, the smallest
eigenvalue of DDH is greater than or equal to v2 and therefore ∥D−1∥≤v−1.
At this step, we need to invoke the generalized trace lemma, Theorem 3.12.
From Theorem 3.12, (6.13), Lemma 6.3 and the inequalities due to the truncation
steps, we can then show that
τj|d1
j| ≤∥xj∥2 c log7 N|z|3
Nℑ[z]7
τj|d2
j| ≤
log N
xH
j R
1
2 (B(j) −zIN)−1RD−1
(j)R
1
2 xj −1
n tr R(B(j) −zIN)−1RD−1
(j)

ℑ[z]|z|−1
τj|d3
j| ≤|z| log3 N
ℑ[z]N

1
ℑ[z]2 + c|z|2 log3 N
ℑ[z]6

τj|d4
j| ≤
log4 N
xH
j R
1
2 (B(j) −zIN)−1R
1
2 xj −1
n tr R(B(j) −zIN)−1 + log N
Nℑ[z]

ℑ[z]3|z|−1
.

128
6. Deterministic equivalents
Applying the trace lemma for truncated variables, Theorem 3.12, and classical
inequalities, there exists ¯K > 0 such that we have simultaneously
E|∥xj∥2 −1|6 ≤
¯K log12 N
N 3
and
E|xH
j R
1
2 (B(j) −zIN)−1RD−1
(j)R
1
2 xj −1
n tr R(B(j) −zIN)−1RD−1
(j)|6
≤
¯K log24 N
N 3ℑ[z]12
and
E|xH
j R
1
2 (B(j) −zIN)−1R
1
2 xj −1
n tr R
1
2 (B(j) −zIN)−1R
1
2 |6
≤
¯K log18 N
N 3ℑ[z]6 .
All three moments above, when summed over the n indexes j and multiplied by
any power of log N, are summable. Applying the Markov inequality, Theorem
3.5, the Borel–Cantelli lemma, Theorem 3.6, and the line of arguments used
in the proof of the Mar˘cenko–Pastur law, we conclude that, for any k > 0,
logk N maxj≤n τjdj
a.s.
−→0 as N →∞, and therefore:
eBN (z) −fN(eBN (z); z)
a.s.
−→0
mBN (z) −gN(eBN (z); z)
a.s.
−→0.
This convergence result is similar to that of Theorem (3.22), although in the
latter each side of the minus sign converges, when the eigenvalue distributions
of the deterministic matrices in the model converge. In the present case, even if
the series {F T} and {F R} converge, it is not necessarily true that either eBN (z)
or fN(eBN (z), z) converges.
We wish to go further here by showing that, for all ﬁnite N, fN(e; z) = e
has a solution (Step 2), that this solution is unique in some space (Step 3)
and that, denoting eN(z) this solution, eN(z) −eBN (z)
a.s.
−→0 (Step 4). This will
imply naturally that mN(z) ≜gN(eN(z); z) satisﬁes mBN (z) −mN(z)
a.s.
−→0, for
all z ∈C+. Vitali’s convergence theorem, Theorem 3.11, will conclude the proof
by showing that mBN (z) −mN(z)
a.s.
−→0 for all z outside the positive real half-
line.
Step 2. Existence of a solution
We now show that the implicit equation e = fN(e; z) in the dummy variable e has
a solution for each ﬁnite N. For this, we use a special trick that consists in growing
the matrices dimensions asymptotically large while maintaining the deterministic
components untouched, i.e. while maintaining F R and F T the same. The idea is
to ﬁx N and consider for all j > 0 the matrices T[j] = T ⊗Ij ∈Cjn×jn, R[j] =

6.2. Techniques for deterministic equivalents
129
R ⊗Ij ∈CjN×jN and A[j] = A ⊗Ij ∈CjN×jN. For a given x
f[j](x; z) ≜
1
jN tr R[j]

A[j] +
Z τdF T[j](τ)
1 + cτx R[j] −zINj
−1
which is constant whatever j and equal to fN(x; z). Deﬁning
B[j] = A[j] + R
1
2
[j]XT[j]XHR
1
2
[j]
for X ∈CNj×nj with i.i.d. entries of zero mean and variance 1/(nj)
eB[j](z) =
1
jN tr R[j](A[j] + R
1
2
[j]XT[j]XHR
1
2
[j] −zINj)−1.
With the notations of Step 1, wNj →0 as j →∞, for all sequences B[1], B[2], . . .
in a set of probability one. Take such a sequence. Noticing that both eB[j](z)
and the integrand
τ
1+cτeB[j](z) of f[j](x, z) are uniformly bounded for ﬁxed N
and growing j, there exists a subsequence of eB[1], eB[2], . . . over which they both
converge, when j →∞, to some limits e and τ(1 + cτe)−1, respectively. But since
wjN →0 for this realization of eB[1], eB[2], . . ., for growing j, we have that e =
limj f[j](e, z). But we also have that, for all j, f[j](e, z) = fN(e, z). We therefore
conclude that e = fN(e, z) and we have found a solution.
Step 3. Uniqueness of a solution
Uniqueness is shown classically by considering two hypothetical solutions e ∈C+
and e ∈C+ to (6.6) and by showing then that e −e = γ(e −e), where |γ| must
be shown to be less than one. Indeed, taking the diﬀerence e −e, we have with
the resolvent identity
e −e = 1
N tr RD−1
e
−1
N tr RD−1
e
= 1
N tr RD−1
e
Z
cτ 2(e −e)dF T(τ)
(1 + cτe)(1 + cτe)

RD−1
e
in which De and De are the matrix D with eBN (z) replaced by e and e,
respectively. This leads to the expression of γ as follows.
γ =
Z
cτ 2
(1 + cτe)(1 + cτe)dF T(τ) 1
N tr D−1
e RD−1
e R.
Applying the Cauchy–Schwarz inequality to the diagonal elements of
1
N D−1
e R
R
√cτ
1+cτedF T(τ) and of
1
N D−1
e R
R
√cτ
1+cτedF T(τ), we then have
|γ| ≤
sZ
cτ 2dF T(τ)
|1 + cτe|2N tr D−1
e R(DHe )−1R
sZ
cτ 2dF T(τ)
|1 + cτe|2N tr D−1
e R(De
H)−1R
≜√α√α.
We now proceed to a parallel computation of ℑ[e] and ℑ[e] in the hope
of retrieving both expressions in the right-hand side of the above equation.

130
6. Deterministic equivalents
Introducing the product (DH
e )−1DH
e in the trace, we ﬁrst write e under the form
e = 1
N tr

D−1
e R(DH
e )−1

A +
Z
τ
1 + cτe∗dF T(τ)

R −z∗IN

.
(6.14)
Taking the imaginary part, this is:
ℑ[e] = 1
N tr

D−1
e R(DH
e )−1
Z
cτ 2ℑ[e]
|1 + cτe|2 dF T(τ)

R + ℑ[z]IN

= ℑ[e]α + ℑ[z]β
where
β ≜1
N tr D−1
e R(DH
e )−1
is positive whenever R ̸= 0, and similarly ℑ[e] = αℑ[e] + ℑ[z]β, β > 0 with
β ≜1
N tr D−1
e R(DH
e )−1.
Notice also that
α = αℑ[e]
ℑ[e] =
αℑ[e]
αℑ[e] + βℑ[z] < 1
and
α = αℑ[e]
ℑ[e] =
αℑ[e]
αℑ[e] + βℑ[z] < 1.
As a consequence
|γ| ≤√α√α =
s
ℑ[e]α
ℑ[e]α + ℑ[z]β
s
ℑ[e]α
ℑ[e]α + ℑ[z]β < 1
as requested. The case R = 0 is easy to verify.
Remark 6.3. Note that this uniqueness argument is slightly more technical when
K > 1. In this case, uniqueness of the vector e1, . . . , eK (under the notations of
Theorem 6.1) needs be proved. Denoting e ≜(e1, . . . , eK)T, this requires to show
that, for two solutions e and e of the implicit equation, (e −e) = Γ(e −e), where
Γ has spectral radius less than one. To this end, a possible approach is to show
that |Γij| ≤α
1
2
ijα
1
2
ij, for αij and αij deﬁned similar as in Step 3. Then, applying
some classical matrix lemmas (Theorem 8.1.18 of [Horn and Johnson, 1985] and
Lemma 5.7.9 of [Horn and Johnson, 1991]), the previous inequality implies that
∥Γ∥≤∥(α
1
2
ijα
1
2
ij)ij∥
where (α
1
2
ijα
1
2
ij)ij is the matrix with (i, j) entry α
1
2
ijα
1
2
ij and the norm is the matrix
spectral norm. We further have that
∥(α
1
2
ijα
1
2
ij)ij∥≤∥A∥
1
2 ∥A∥
1
2

6.2. Techniques for deterministic equivalents
131
where A and A are now matrices with (i, j) entry αij and αij, respectively.
The multi-dimensional problem therefore boils down to proving that ∥A∥< 1
and ∥A∥< 1. This unfolds from yet another classical matrix lemma (Theorem
2.1 of [Seneta, 1981]), which states in our current situation that, if we have the
vectorial relation
ℑ[e] = Aℑ[e] + ℑ[z]b
with ℑ[e] and b vectors of positive entries and ℑ[z] > 0, then ∥A∥< 1. The above
relation generalizes, without much diﬃculty, the relation ℑ[e] = ℑ[e]α + ℑ[z]β
obtained above.
Step 4. Final convergence step
We ﬁnally need to show that eN −eBN (z)
a.s.
−→0. This is performed using a
similar argument as for uniqueness, i.e. eN −eBN (z) = γ(eN −eBN (z)) + wN,
where wN →0 as N →∞and |γ| < 1; this is true for any eBN (z) taken from
a space of probability one such that wN →0. The major diﬃculty compared to
the previous proof is to control precisely wN.
The details are as follows. We will show that, for any ℓ> 0, almost surely
lim
N→∞logℓN(eBN −eN) = 0.
(6.15)
Let αN, βN be the values as above for which ℑ[eN] = ℑ[eN]αN + ℑ[z]βN. Using
truncation inequalities
ℑ[eN]αN
βN
≤ℑ[eN]c log N
Z
τ 2
|1 + cτeN|2 dF T(τ)
= −log Nℑ
Z
τ
1 + cτeN
dF T(τ)

≤log2 N|z|ℑ[z]−1.
Therefore
αN =
ℑ[eN]αN
ℑ[eN]αN + ℑ[z]βN
=
ℑ[eN] αN
βN
ℑ[z] + ℑ[eN] αN
βN
≤
log2 N|z|
ℑ[z]2 + log2 N|z|.
(6.16)
We also have
eBN (z) = 1
N tr D−1R −wN.

132
6. Deterministic equivalents
We write as in Step 3
ℑ[eBN ]
= 1
N tr

D−1R(DH)−1
Z
cτ 2ℑ[eBN ]
|1 + cτeBN |2 dF T(τ)

R + ℑ[z]IN

−ℑ[wN]
≜ℑ[eBN ]αBN + ℑ[z]βBN −ℑ[wN].
Similarly to Step 3, we have eBN −eN = γ(eBN −eN) + wN, where now
|γ| ≤√αBN
√αN.
Fix an ℓ> 0 and consider a realization of BN for which wN logℓ′ N →0, where
ℓ′ = max(ℓ+ 1, 4) and N large enough so that
|wN| ≤
ℑ[z]3
4c|z|2 log3 N .
(6.17)
As opposed to Step 2, the term ℑ[z]βBN −ℑ[wN] can be negative. The idea is to
verify that in both scenarios where ℑ[z]βBN −ℑ[wN] is positive and uniformly
away from zero, or is not, the conclusion |γ| < 1 holds. First suppose βBN ≤
ℑ[z]2
4c|z|2 log3 N . Then by the truncation inequalities, we get
αBN ≤cℑ[z]−2|z|2 log3 NβBN ≤1
4
which implies |γ| ≤1
2. Otherwise we get from (6.16) and (6.17)
|γ| ≤√αN
s
ℑ[eBN ]αBN
ℑ[eBN ]αBN + ℑ[z]βBN −ℑ[wN]
≤
s
log N|z|
ℑ[z]2 + log N|z|.
Therefore, for all N large
logℓN|eBN −eN| ≤
(logℓN)wN
1 −

log2 N|z|
ℑ[v]2+log2 N|z|
 1
2
≤2ℑ[z]−2(ℑ[z]2 + log2 N|z|)(logℓN)wN
→0
as N →∞, and (6.15) follows. Once more, the multi-dimensional case is much
more technical; see [Couillet et al., 2011a] for details.
We ﬁnally show
mBN −mN
a.s.
−→0
(6.18)
as N →∞. Since mBN = 1
N tr D−1
N −˜wN (for some ˜wN deﬁned similar to wN),
we have
mBN −mN = γ(eBN −eN) −˜wN

6.2. Techniques for deterministic equivalents
133
where now
γ =
Z
cτ 2
(1 + cτeBN )(1 + cτeN)dF T(τ) 1
N tr D−1RD−1
N .
From the truncation inequalities, we obtain |γ| ≤c|z|2ℑ[z]−4 log3 N. From (6.15)
and the fact that logℓN ˜wN
a.s.
−→0, we ﬁnally have (6.18).
In the proof of Theorem 6.17, we will use another technique for this last
convergence part, which, instead of controlling precisely the behavior of wN,
consists in proving the convergence on a subset of C \ R+ that does not
meet strong diﬃculties. Using Vitali’s convergence theorem, we then prove the
convergence for all z ∈C \ R+. This approach is usually much simpler and is in
general preferred.
Returning to the original non-truncated assumptions on X, T, and R, for each
of a countably inﬁnite collection of z with positive imaginary part, possessing a
limit point with positive imaginary part, we have (6.18). Therefore, by Vitali’s
convergence theorem, Theorem 3.11, and similar arguments as for the proof of
the Mar˘cenko–Pastur law, for any ε > 0, we have exactly that with probability
one mBN (z) −mN(z)
a.s.
−→0 uniformly in any region of C bounded by a contour
interior to
C \ ({z : |z| ≤ε} ∪{z = x + iv : x > 0, |v| ≤ε}) .
(6.19)
This completes the proof of Theorem 6.1.
The previous proof is lengthy and technical, when it comes to precisely working
out the inequalities based on the truncation steps. Nonetheless, in spite of these
diﬃculties, the line of reasoning in this example can be generalized to more exotic
models, which we will introduce also in this section. Moreover, we will brieﬂy
introduce alternative techniques of proof, such as the Gaussian method, which
will turn out to be based on similar approaches, most particularly for Step 2 and
Step 3.
We now prove Theorem 6.2, which we recall provides a deterministic way to
recover the unique solution vector eN,1(z), . . . , eN,K(z) of the implicit Equation
(6.6). The arguments of the proof are again very classical and can be reproduced
for diﬀerent random matrix models.
Proof of Theorem 6.2. The convergence of the ﬁxed-point algorithm follows the
same line of proof as the uniqueness (Step 2) of Theorem 6.1. For simplicity, we
consider also here that K = 1. First assume ℑ[z] > 0. If we consider the diﬀerence
et+1
N
−et
N, instead of e −e, the same development as in the previous proof leads
to
et+1
N
−et
N = γt(et
N −et−1
N )
(6.20)
for t ≥1, with γt deﬁned by
γt =
Z
cτ 2
(1 + cτet−1
N )(1 + cτet
N)dF T(τ) 1
N tr D−1
t−1RD−1
t R
(6.21)

134
6. Deterministic equivalents
where Dt is deﬁned as D with eBN (z) replaced by et
N(z). From the Cauchy–
Schwarz inequality and the diﬀerent truncation bounds on the Dt, R, and T
matrices, we have:
γt ≤|z|2c
ℑ[z]4
log4 N
N
.
(6.22)
This entails
 et+1
N
−et
N

< ¯K |z|2c
ℑ[z]4
log4 N
N
 et
N −et−1
N

(6.23)
for some constant ¯K.
Let 0 < ε < 1, and take now a countable set z1, z2, . . . possessing a limit point,
such that
¯K |zk|2c
ℑ[zk]4
log4 N
N
< 1 −ε
for all zk (this is possible by letting ℑ[zk] > 0 be large enough). On this countable
set, the sequences e1
N, e2
N, . . . are therefore Cauchy sequences on CK: they all
converge. Since the et
N are holomorphic functions of z and bounded on every
compact set included in C \ R+, from Vitali’s convergence theorem, Theorem
3.11, et
N converges on such compact sets.
From the fact that we forced the initialization step to be e0
N = −1/z, e0
N is
the Stieltjes transform of a distribution function at point z. It now suﬃces to
verify that, if et
N = et
N(z) is the Stieltjes transform of a distribution function at
point z, then so is et+1
N . From Theorem 3.2, this requires to ensure that: (i) z ∈
C+ and et
N(z) ∈C+ implies et+1
N (z) ∈C+, (ii) z ∈C+ and zet
N(z) ∈C+ implies
zet+1
N (z) ∈C+, and (iii) limy→∞−yet
N(iy) < ∞implies that limy→∞−yet
N(iy) <
∞. These properties follow directly from the deﬁnition of et
N. It is not diﬃcult
to show also that the limit of et
N is a Stieltjes transform and that it is solution
to (6.6) when K = 1. From the uniqueness of the Stieltjes transform, solution
to (6.6) (this follows from the point-wise uniqueness on C+ and the fact that
the Stieltjes transform is holomorphic on all compact sets of C \ R+), we then
have that et
N converges for all j and z ∈C \ R+, if e0
N is initialized at a Stieltjes
transform. The choice e0
N = −1/z follows this rule and the ﬁxed-point algorithm
converges to the correct solution.
This concludes the proof of Theorem 6.2.
From Theorem 6.1, we now wish to provide deterministic equivalents for other
functionals of the eigenvalues of BN than the Stieltjes transform. In particular,
we wish to prove that
Z
f(x)d(F BN −FN)(x)
a.s.
−→0
for some function f. This is valid for all bounded continuous f from the
dominated convergence theorem, which we recall presently.

6.2. Techniques for deterministic equivalents
135
Theorem 6.3 (Theorem 16.4 in [Billingsley, 1995]). Let fN(x) be a sequence of
real measurable functions converging point-wise to the measurable function f(x),
and such that |fN(x)| ≤g(x) for some measurable function g(x) with
R
g(x)dx <
∞. Then, as N →∞
Z
fN(x)dx →
Z
f(x)dx.
In particular, if FN ⇒F, the FN and F being d.f., for any continuous bounded
function h(x)
Z
h(x)dFN(x) →
Z
h(x)dF(x).
However, for application purposes, such as the calculus of MIMO capacity, see
Chapter 13, we would like in particular to take f to be the logarithm function.
Proving such convergence results is not at all straightforward since f is here
unbounded and because F BN may not have bounded support for all large N.
This requires additional tools which will be brieﬂy evoked here and which will
be introduced in detail in Chapter 7.
We have the following result [Couillet et al., 2011a].
Theorem 6.4. Let x be some positive real number and f be some continuous
function on the positive half-line. Let BN be a random Hermitian matrix as
deﬁned in Theorem 6.1 with the following additional assumptions.
1. There exists α > 0 and a sequence rN, such that, for all N
max
1≤k≤K max(λTk
rN+1, λRk
rN+1) ≤α
where λX
1 ≥. . . ≥λX
N denote the ordered eigenvalues of the N × N matrix X.
2. Denoting bN an upper-bound on the spectral norm of the Tk and Rk, k ∈
{1, . . . , K}, and β some real, such that β > K(b/a)(1 + √a)2 (with a and b
such that a < lim infN ck ≤lim supN ck < b for all k), then aN = b2
Nβ satisﬁes
rNf (aN) = o(N).
(6.24)
Then, for large N, nk
Z
f(x)dF BN (x) −
Z
f(x)dFN(x)
a.s.
−→0
with FN deﬁned in Theorem 6.1.
In particular, if f(x) = log(x), under the assumption that (6.24) is fulﬁlled,
we have the following corollary.

136
6. Deterministic equivalents
Corollary 6.1. For A = 0, under the conditions of Theorem 6.4 with f(t) =
log(1 + xt), the Shannon transform VBN of BN, deﬁned for positive x as
VBN (x) =
Z ∞
0
log(1 + xλ)dF BN (λ)
= 1
N log det (IN + xBN)
(6.25)
satisﬁes
VBN (x) −VN(x)
a.s.
−→0
where VN(x) is deﬁned as
VN(x) = 1
N log det
 
IN + x
K
X
k=1
Rk
Z
τkdF Tk(τk)
1 + ckeN,k(−1/x)τk
!
+
K
X
k=1
1
ck
Z
log (1 + ckeN,k(−1/x)τk) dF Tk(τk)
+ 1
xmN(−1/x) −1
with mN and eN,k deﬁned by (6.5) and (6.6), respectively.
Again, it is more convenient, for readability and for the sake of practical
applications in Chapters 12–15 to remark that
VN(x) = 1
N log det
 
IN +
K
X
k=1
¯eN,k(−1/x)Rk
!
+
K
X
k=1
1
N log det (Ink + ckeN,k(−1/x)Tk)
−1
x
K
X
k=1
¯eN,k(−1/x)eN,k(−1/x)
(6.26)
with ¯eN,k deﬁned in (6.7).
Observe that the constraint
max
1≤k≤K max(λTk
rN+1, λRk
rN+1) ≤α
is in general not strong, as the F Tk and the F Rk are already known to form
tight sequences as N grows large. Therefore, it is expected that only o(N) largest
eigenvalues of the Tk and Rk grow large. Here, we impose only a slightly stronger
constraint that does not allow for the smallest eigenvalues to exceed a constant
α. For practical applications, we will see in Chapter 13 that this constraint is met
for all usual channel models, even those exhibiting strong correlation patterns
(such as densely packed three-dimensional antenna arrays).

6.2. Techniques for deterministic equivalents
137
Proof of Theorem 6.4 and Corollary 6.1. The only problem in translating the
weak convergence of the distribution function F BN −FN in Theorem 6.1 to
the convergence of
R
fd[F BN −FN] in Theorem 6.4 is that we must ensure
that f behaves nicely. If f were bounded, no restriction in the hypothesis of
Theorem 6.1 would be necessary and the weak convergence of F BN −FN to zero
gives the result. However, as we are particularly interested in the unbounded,
though slowly increasing, logarithm function, this no longer holds. In essence, the
proof consists ﬁrst in taking a realization B1, B2, . . . for which the convergence
F BN −FN ⇒0 is satisﬁed. Then we divide the real positive half-line in two
sets [0, d] and (d, ∞), with d an upper bound on the 2KrNth largest eigenvalue
of BN for all large N, which we assume for the moment does exist. For any
continuous f, the convergence result is ensured on the compact [0, d]; if the largest
eigenvalue λ1 of BN is moreover such that 2KrNf(λ1) = o(N), the integration
over (d, ∞) for the measure dF BN is of order o(1), which is negligible in the ﬁnal
result for large N. Moreover, since FN(d) −F BN (d) →0, we also have that, for
all large N, 1 −FN(d) =
R ∞
d dFN ≤2KrN/N, which tends to zero. This ﬁnally
proves the convergence of
R
fd[F BN −FN]. The major diﬃculty here lies in
proving that there exists such a bound on the 2KrNth largest eigenvalue of BN.
The essential argument that validates the result is the asymptotic absence of
eigenvalues outside the support of the sample covariance matrix. This is a result
of utmost importance (here, we cannot do without it) which will be presented
later in Section 7.1. It can be exactly proved that, almost surely, the largest
eigenvalue of XkXH
k is uniformly bounded by any constant C > (1 +
√
b)2 for
all large N, almost surely. In order to use the assumptions of Theorem 6.4, we
ﬁnally need to introduce the following eigenvalue inequality lemma.
Lemma 6.4 ([Fan, 1951]). Consider a rectangular matrix A and let sA
i
denote
the ith largest singular value of A, with sA
i = 0 whenever i > rank(A). Let m, n
be arbitrary non-negative integers. Then for A, B rectangular of the same size
sA+B
m+n+1 ≤sA
m+1 + sB
n+1
and for A, B rectangular for which AB is deﬁned
sAB
m+n+1 ≤sA
m+1sB
n+1.
As a corollary, for any integer r ≥0 and rectangular matrices A1, . . . , AK, all
of the same size
sA1+...+AK
Kr+1
≤sA1
r+1 + . . . + sAK
r+1.
Since λTk
i
and λRk
i
are bounded by α for i ≥rN + 1 and that ∥XkXH
k ∥is
bounded by C, we have from Lemma 6.4 that the 2KrNth largest eigenvalue of
BN is uniformly bounded by CKα2. We can then take d any positive real, such
that d > CKα2, which is what we needed to show, up to some ﬁne tuning on
the ﬁnal bound.

138
6. Deterministic equivalents
As for the explicit form of
R
log(1 + xt)dFN(t) given in (6.26), it results
from a similar calculus as in Theorem 4.10. Precisely, we expect the Shannon
transform to be somehow linked to
1
N log det

IN + PK
k=1 ¯eN,k(−z)Rk

and
1
N log det (Ink + ckeN,k(−z)Tk). We then need to ﬁnd a connection between the
derivatives of these functions along z and 1
z −mN(−z), i.e. the derivative of the
Shannon transform. Notice that
1
z −mN(−z) = 1
N

(zIN)−1 −
 
z
"
IN +
K
X
k=1
¯eN,kRk
#!−1

=
K
X
k=1
¯eN,k(−z)eN,k(−z).
Since the Shannon transform VN(x) satisﬁes VN(x) =
R ∞
1/x[w−1 −mN(−w)]dw,
we need to ﬁnd an integral form for PK
k=1 ¯eN,k(−z)eN,k(−z). Notice now that
d
dz
1
N log det
 
IN +
K
X
k=1
¯eN,k(−z)Rk
!
= −z
K
X
k=1
eN,k(−z)¯e′
N,k(−z)
d
dz
1
N log det (Ink + ckeN,k(−z)Tk) = −ze′
N,k(−z)¯eN,k(−z)
and
d
dz
 
z
K
X
k=1
¯eN,k(−z)eN,k(−z)
!
=
K
X
k=1
¯eN,k(−z)eN,k(−z)
−z
K
X
k=1
 ¯e′
N,k(−z)eN,k(−z) + ¯eN,k(−z)e′
N,k(−z)

.
Combining the last three equations, we have:
K
X
k=1
¯eN,k(−z)eN,k(−z)
= d
dz
"
−1
N log det
 
IN +
K
X
k=1
¯eN,k(−z)Rk
!
−
K
X
k=1
1
N log det (Ink + ckeN,k(−z)Tk) + z
K
X
k=1
¯eN,k(−z)eN,k(−z)
#
which after integration leads to
Z ∞
z
 1
w −mN(−w)

dw
= 1
N log det
 
IN +
K
X
k=1
¯eN,k(−z)Rk
!

6.2. Techniques for deterministic equivalents
139
+
K
X
k=1
1
N log det (Ink + ckeN,k(−z)Tk) −z
K
X
k=1
¯eN,k(−z)eN,k(−z)
which is exactly the right-hand side of (6.26) for z = −1/x.
Theorem 6.4 and Corollary 6.1 have obvious direct applications in wireless
communications since the Shannon transform VBN deﬁned above is the per-
dimension capacity of the multi-dimensional channel, whose model is given
by PK
k=1 R
1
2
k XkT
1
2
k . This is the typical model used for evaluating the rate
region of a narrowband multiple antenna multiple access channel. This topic
is discussed and extended in Chapter 14, e.g. to the question of ﬁnding the
transmit covariance matrix that maximizes the deterministic equivalent (hence
the asymptotic capacity).
6.2.2
Gaussian method
The second result that we present is very similar in nature to Theorem 6.1 but
instead of considering sums of matrices of the type
BN =
K
X
k=1
R
1
2
k XkTkXH
k R
1
2
k
we treat the question of matrices of the type
BN =
 K
X
k=1
R
1
2
k XkT
1
2
k
!  K
X
k=1
R
1
2
k XkT
1
2
k
!H
.
To obtain a deterministic equivalent for this model, the same technique as before
could be used. Instead, we develop an alternative method, known as the Gaussian
method, when the Xk have Gaussian i.i.d. entries, for which fast convergence rates
of the functional of the mean e.s.d. can be proved.
Theorem 6.5 ([Dupuy and Loubaton, 2009]). Let K be some positive integer.
For two positive integers N, n, denote
BN =
 X
k=1
R
1
2
k XkT
1
2
k
!  X
k=1
R
1
2
k XkT
1
2
k
!H
where the notations are the same as in Theorem 6.1, with the additional
assumptions that n1 = . . . = nK = n, the random matrix Xk ∈CN×nk
has
independent Gaussian entries (of zero mean and variance 1/n) and the spectral
norms ∥Rk∥and ∥Tk∥are uniformly bounded with N. Note additionally that,
from the unitarily invariance of Xk, Tk is not restricted to be diagonal. Then,
denoting as above mBN the Stieltjes transform of BN, we have
N (E[mBN (z)] −mN(z)) = O (1/N)

140
6. Deterministic equivalents
with mN deﬁned, for z ∈C \ R+, as
mN(z) = 1
N tr
 
−z
"
IN +
K
X
k=1
¯eN,k(z)Rk
#!−1
where (¯eN,1, . . . , ¯eN,K) is the unique solution of
eN,i(z) = 1
n tr Ri
 
−z
"
IN +
K
X
k=1
¯eN,k(z)Rk
#!−1
¯eN,i(z) = 1
n tr Ti
 
−z
"
In +
K
X
k=1
eN,k(z)Tk
#!−1
(6.27)
all with positive imaginary part if z ∈C+, negative imaginary part if z ∈C−,
and positive if z < 0.
Remark 6.4. Note that, due to the Gaussian assumption on the entries of
Xk, the convergence result N (E[mBN (z)] −mN(z)) →0 is both (i) looser than
the convergence result mBN (z) −mN(z)
a.s.
−→0 of Theorem 6.1 in that it is
only shown to converge in expectation, and (ii) stronger in the sense that a
convergence rate of O(1/N) of the Stieltjes transform is ensured. Obviously,
Theorem 6.1 also implies E[mBN (z)] −mN(z) →0. In fact, while this was not
explicitly mentioned, a convergence rate of 1/(log(N)p), for all p > 0, is ensured
in the proof of Theorem 6.1. The main applicative consequence is that, while the
conditions of Theorem 6.1 allow us to deal with instantaneous or quasi-static
channel models Hk = R
1
2
k XkT
1
2
k , the conditions of Theorem 6.5 are only valid
from an ergodic point of view. However, while Theorem 6.1 can only deal with the
per-antenna capacity of a quasi-static (or ergodic) MIMO channel, Theorem 6.5
can deal with the total ergodic capacity of MIMO channels, see further Theorem
6.8.
Of course, while this has not been explicitly proved in the literature, it is
to be expected that Theorem 6.5 holds also under the looser assumptions and
conclusions of Theorem 6.1 and conversely.
The proof of Theorem 6.5 needs the introduction of new tools, gathered
together into the so-called Gaussian method. Basically, the Gaussian method
relies on two main ingredients:
• an integration by parts formula, borrowed from mathematical physics [Glimm
and Jaﬀe, 1981]
Theorem 6.6. Let x = [x1, . . . , xN]T ∼CN(0, R) be a complex Gaussian
random
vector
and
f(x) ≜f(x1, . . . , xN, x∗
i, . . . , x∗
N)
be
a
continuously
diﬀerentiable functional, the derivatives of which are all polynomially bounded.

6.2. Techniques for deterministic equivalents
141
We then have the integration by parts formula
E[xkf(x)] =
N
X
i=1
rkiE
∂f(x)
∂x∗
i

with rki the entry (k, i) of R.
This relation will be used to derive directly the deterministic equivalent, which
substitutes to the ‘guess-work’ step of the proof of Theorem 6.1. Note in
particular that it requires us to use all entries of R here and not simply its
eigenvalues. This generalizes the Mar˘cenko–Pastur method that only handled
diagonal entries. However, as already mentioned, the introduction of the
expectation in front of xkf(x) cannot be avoided;
• the Nash–Poincar´e inequality
Theorem 6.7 ([Pastur, 1999]). Let x and f be as in Theorem 6.6, and let
∇zf = [∂f/∂z1, . . . , ∂f/∂zN]T. Then, we have the following Nash–Poincar´e
inequality
var(f(x)) ≤E

∇xf(x)TR(∇xf(x))∗
+ E

(∇x∗f(x))HR∇x∗f(x)

.
This result will be used to bound the deviations of the random matrices under
consideration.
For more details on Gaussian methods, see [Hachem et al., 2008a]. We now
give the main steps of the proof of Theorem 6.5.
Proof of Theorem 6.5. We ﬁrst consider E(BN −zIN)−1. Noting that −z(BN −
zIN)−1 = IN −(BN −zIN)−1BN, we apply the integration by parts, Theorem
6.6, in order to evaluate the matrix
E

(BN −zIN)−1BN

.
To this end, we wish to characterize every entry
E
 (BN −zIN)−1BN

aa′

=
X
1≤k,¯k≤K
E
h
(BN −zIN)−1R
1
2
k (XkT
1
2
k R
1
2
¯k )(X¯kT
1
2
¯k )H
aa′
i
.
This is however not so simple and does not lead immediately to a nice form
enabling us to use the Gaussian entries of the Xk as the inputs of Theorem 6.6.
Instead, we will consider the multivariate expression
E
h
(BN −zIN)−1
ab (R
1
2
k XkT
1
2
k )cd(R
1
2
¯k X¯kT
1
2
¯k )H
ea′
i
for some k, ¯k ∈{1, . . . , K} and given a, a′, b, c, d, e. This enables us to somehow
unfold easily the matrix products before we set b = c and d = e, and simplify the
management of the Gaussian variables. This being said, we take the vector x of
Theorem 6.6 to be the vector whose entries are denoted
xk,c,d ≜x(k−1)Nn+(c−1)N+d = (R
1
2
k XkT
1
2
k )cd

142
6. Deterministic equivalents
for all k, c, d. This is therefore a vector of total dimension KNn that collects
the entries of all Xk and accounts for the (Kronecker-type) correlation proﬁle
due to Rk and Tk. The functional f(x) = fa,b(x) of Theorem 6.6 is taken to be
the KNn-dimensional vector y(a,b) with entry
y(a,b)
¯k,a′,e ≜y(a,b)
(¯k−1)Nn+(a′−1)N+e = (BN −zIN)−1
ab (R
1
2
¯k X¯kT
1
2
¯k )H
ea′
for all ¯k, e, a′. This expression depends on x through (BN −zIN)−1
ab and through
x∗
¯k,a′,e = (R
1
2
¯k X¯kT
1
2
¯k )H
ea′.
We therefore no longer take b = c or d = e as matrix products would require.
This trick allows us to apply seamlessly the integration by parts formula.
Applying Theorem 6.6, we have that the entry (¯k −1)Nn + (a′ −1)N + e of
E[xk,c,dfa,b(x)], i.e. E[xk,c,dy(a,b)
¯k,a′,e], is given by:
E[(BN −zIN)−1
ab (R
1
2
k XkT
1
2
k )cd(R
1
2
¯k X¯kT
1
2
¯k )H
ea′]
=
X
k′,c′,d′
E

xk,c,dx∗
k′,c′,d′

E


∂

(BN −zIN)−1
ab x∗
¯k,a′,e

∂x∗
k′,c′,d′


for all choices of a, b, c, d, e, a′. At this point, we need to proceed to cumbersome
calculus, that eventually leads to a nice form when setting b = c and d = e.
This gives an expression of E
h
(BN −zIN)−1R
1
2
k XkT
1
2
k R
1
2
¯k X¯kT
1
2
¯k

aa′
i
, which
is then summed over all couples k, ¯k to obtain
E
 (BN −zIN)−1BN

aa′

= −z
K
X
k=1
¯eBN,k(z)E
 (BN −zIN)−1Rk

aa′

+ wN,aa′
where we deﬁned
¯eBN,k(z) ≜1
n tr Tk
 
−z
"
In +
K
X
k=1
eBN,kTk
#!−1
eBN,k(z) ≜E
 1
N tr Rk(BN −zIN)−1

and wN,aa′ is a residual term that must be shown to be going to zero at a certain
rate for increasing N. Using again the formula −z(BN −zIN)−1 = IN −(BN −
zIN)−1BN, this entails
E

(BN −zIN)−1
= −1
z
 
IN +
K
X
k=1
¯eBN,k(z)Rk
!−1
[IN + WN]
with WN the matrix of (a, a′) entry wN,aa′. Showing that WN is negligible with
summable entries as N →∞is then solved using the Nash–Poincar´e inequality,
Theorem 6.7, which again leads to cumbersome but doable calculus.
The second main step consists in considering the system (6.27) (the uniqueness
of the solution of which is treated as for Theorem 6.1) and showing that, for any

6.2. Techniques for deterministic equivalents
143
uniformly bounded matrix E
E

tr E(BN −zIN)−1
= tr E(−z[IN +
K
X
k=1
¯eN,k(z)Rk])−1 + O
 1
N

from which N(E[eBN,k(z)] −eN,k(z)) = O(1/N) (for E = Rk) and ﬁnally
N(E[mBN (z)] −mN(z)) = O(1/N) (for E = IN). This is performed in a similar
way as in the proof for Theorem 6.1, with the additional results coming from the
Nash–Poincar´e inequality.
The Gaussian method, while requiring more intensive calculus, allows us to
unfold naturally the deterministic equivalent under study for all types of matrix
combinations involving Gaussian matrices. It might as well be used as a tool
to infer the deterministic equivalent of more involved models for which such
deterministic equivalents are not obvious to ‘guess’ or for which the Mar˘cenko–
Pastur method for diagonal matrices cannot be used. For the latest results
derived from this technique, refer to, e.g., [Hachem et al., 2008a; Khorunzhy
et al., 1996; Pastur, 1999]. It is believed that Haar matrices can be treated using
the same tools, to the eﬀort of more involved computations but, to the best of
our knowledge, there exists no reference of such a work, yet.
In the same way as we derived the expression of the Shannon transform of the
model BN of Theorem 6.1 in Corollary 6.1, we have the following result for BN
in Theorem 6.5.
Theorem 6.8 ([Dupuy and Loubaton, 2010]). Let BN ∈CN×N be deﬁned as in
Theorem 6.5. Then the Shannon transform VBN of BN satisﬁes
N(E[VBN (x)] −VN(x)) = O(1/N)
where VN(x) is deﬁned, for x > 0, as
VN(x) = 1
N log det
 
IN +
K
X
k=1
¯eN,k(−1/x)Rk
!
+ 1
N log det
 
In +
K
X
k=1
eN,k(−1/x)Tk
!
−n
N
1
x
K
X
k=1
¯eN,k(−1/x)eN,k(−1/x).
(6.28)
Note that the expressions of (6.26) and (6.28) are very similar, apart from the
position of a summation symbol.
Both Theorem 6.1 and Theorem 6.5 can then be compiled into an even more
general result, as follows. This is however not a corollary of Theorem 6.1 and
Theorem 6.5, since the complete proof must be derived from the beginning.

144
6. Deterministic equivalents
Theorem 6.9. For k = 1, . . . , K, denote Hk ∈CN×nk the random matrix such
that, for a given positive Lk
Hk =
Lk
X
l=1
R
1
2
k,lXk,lT
1
2
k,l
for R
1
2
k,l a Hermitian non-negative square root of the Hermitian non-negative
Rk,l ∈CN×N, T
1
2
k,l a Hermitian non-negative square root of the Hermitian non-
negative Tk,l ∈Cnk×nk and Xk,l ∈CN×nk with Gaussian i.i.d. entries of zero
mean and variance 1/nk. All Rk,l and Tk,l are uniformly bounded with respect
to N, nk. Denote also for all k, ck = N/nk.
Call mBN (z) the Stieltjes transform of BN = PK
k=1 HkHH
k , i.e. for z ∈C \ R+
mBN (z) = 1
N tr
 K
X
k=1
HkHH
k −zIN
!−1
.
We then have
N (E[mBN (z)] −mN(z)) →0
where mN(z) is deﬁned as
mN(z) = 1
N tr
 
−z
" K
X
k=1
Lk
X
l=1
eN;k,l(z)Rk,l + IN
#!−1
and eN;k,l solves the ﬁxed-point equations
eN;k,l(z) = 1
nk
tr Tk,l
 
−z
" Lk
X
l′=1
¯eN;k,l′(z)Tk,l′ + Ink
#!−1
¯eN;k,l(z) = 1
nk
tr Rk,l

−z


K
X
k′=1
Lk′
X
l′=1
eN;k′,l′(z)Rk′,l′ + IN




−1
.
We also have that the Shannon transform VBN (x) of BN satisﬁes
N (E[VBN (x)] −VN(x)) →0
where
VN(x) = 1
N log det
 K
X
k=1
Lk
X
l=1
eN;k,l(−1/x)Rk,l + IN
!
+
K
X
k=1
1
N log det
 Lk
X
l=1
¯eN;k,l(−1/x)Tk,l + Ink
!
−1
x
K
X
k=1
nk
N
Lk
X
l=1
eN;k,l(−1/x)¯eN;k,l(−1/x).

6.2. Techniques for deterministic equivalents
145
For practical applications, this formula provides the whole picture for the
ergodic rate region of large MIMO multiple access channels, with K multiple
antenna users, user k being equipped with nk antennas, when the diﬀerent
channels into consideration are frequency selective with Lk taps for user k, slow
fading in time, and for each tap modeled as Kronecker with receive and transmit
correlation Rk,l and Tk,l, respectively.
We now move to another type of deterministic equivalents, when the entries
of the matrix X are not necessarily of zero mean and have possibly diﬀerent
variances.
6.2.3
Information plus noise models
In Section 3.2, we introduced an important limiting Stieltjes transform result,
Theorem 3.14, for the Gram matrix of a random i.i.d. matrix X ∈CN×n with a
variance proﬁle {σ2
ij/n}, 1 ≤i ≤N and 1 ≤j ≤n. One hypothesis of Girkos’s
law is that the proﬁle {σij} converges to a density σ(x, y) in the sense that
σij −
Z
i
N
i−1
N
Z
j
n
j−1
n
σ(x, y)dxdy →0.
It will turn out in practical applications that such an assumption is in general
unusable. Typically, suppose that σij is the channel fading between antenna i
and antenna j, respectively, at the transmitter and receiver of a multiple antenna
channel. As one grows N and n simultaneously, there is no reason for the σij
to converge in any sense to a density σ(x, y). In the following, we therefore
rewrite Theorem 3.14 in terms of deterministic equivalents without the need for
any assumption of convergence. This result is in fact a corollary of the very
general Theorem 6.14, presented later in this section, although the deterministic
equivalent is written in a slightly diﬀerent form. A sketch of the proof using the
Bai and Silverstein approach is also provided.
Theorem 6.10. Let XN ∈CN×n have independent entries xij with zero mean,
variance σ2
ij/n and 4 + ε moment of order O(1/N 2+ε/2), for some ε. Assume
that the σij are deterministic and uniformly bounded, over n, N. Then, as N, n
grow large with ratio cn ≜N/n such that 0 < lim infn cn ≤lim supn cn < ∞, the
e.s.d. F BN of BN = XNXH
N satisﬁes
F BN −FN ⇒0
almost surely, where FN is the distribution function of Stieltjes transform mN(z),
z ∈C \ R+, given by:
mN(z) = 1
N
N
X
k=1
1
1
n
Pn
i=1 σ2
ki
1
1+eN,i(z) −z

146
6. Deterministic equivalents
where eN,1(z), . . . , eN,n(z) form the unique solution of
eN,j(z) = 1
n
N
X
k=1
σ2
kj
1
n
Pn
i=1 σ2
ki
1
1+eN,i(z) −z
(6.29)
such that all eN,j(z) are Stieltjes transforms of a distribution function.
The reason why point-wise uniqueness of the eN,j(z) is not provided here is due
to the approach of the proof of uniqueness followed by Hachem et al. [Hachem
et al., 2007] which is a functional proof of uniqueness of the Stieltjes transforms
that the applications z 7→eN,i(z) deﬁne. This does not mean that point-wise
uniqueness does not hold but this is as far as this theorem goes.
Theorem 6.10 can then be written is a more compact and symmetric form by
rewriting eN,j(z) in (6.29) as
eN,j(z) = −1
z
1
n
N
X
k=1
σ2
kj
1 + ¯eN,k(z)
¯eN,k(z) = −1
z
1
n
n
X
i=1
σ2
ki
1 + eN,i(z).
(6.30)
In this case, mN(z) is simply
mN(z) = −1
z
1
N
N
X
k=1
1
1 + ¯eN,k(z).
Note that this version of Girko’s law, Theorem 3.14, is both more general
in the assumptions made, and more explicit. We readily see in this result that
ﬁxed-point algorithms, if they converge at all, allow us to recover the 2n coupled
Equations (6.30), from which mN(z) is then explicit.
For the sake of understanding and to further justify the strength of the
techniques introduced so far, we provide hereafter the ﬁrst steps of the proof
using the Bai and Silverstein technique. A complete proof can be found as a
particular case of [Hachem et al., 2007; Wagner et al., 2011].
Proof. Instead of studying mN(z), let us consider the more general eAN (z), a
deterministic equivalent for
1
N tr AN
 XNXH
N −zIN
−1 .
Using Bai and Silverstein approach, we introduce F ∈CN×N some matrix yet
to be deﬁned, and compute
eAN (z) = 1
N tr AN (F −zIN)−1 .

6.2. Techniques for deterministic equivalents
147
Using the resolvent identity, Lemma 6.1, and writing XNXH
N = Pn
i=1 xixH
i ,
we have:
1
N tr AN
 XNXH
N −zIN
−1 −1
N tr AN (F −zIN)−1
= 1
N tr AN
 XNXH
N −zIN
−1 F (F −zIN)−1
−1
N
n
X
i=1
tr AN
 XNXH
N −zIN
−1 xixH
i (F −zIN)−1
from which we then express the second term on the right-hand side under the
form of sums for i ∈{1, . . . , N} of xH
i (F −zIN)−1 AN
 XNXH
N −zIN
−1 xi and
we use Lemma 6.2 on the matrix
 XNXH
N −zIN
−1 to obtain
1
N tr AN
 XNXH
N −zIN
−1 −1
N tr AN (F −zIN)−1
= 1
N tr AN
 XNXH
N −zIN
−1 F (F −zIN)−1
−1
N
n
X
i=1
xH
i (F −zIN)−1 AN

X(i)XH
(i) −zIN
−1
xi
1 + xH
i

X(i)XH
(i) −zIN
−1
xi
(6.31)
with X(i) = [x1, . . . , xi−1, xi+1, . . . , xn].
Under this form, xi and

X(i)XH
(i) −zIN
−1
have independent entries.
However, xi does not have identically distributed entries, so that Theorem 3.4
cannot be straightforwardly applied. We therefore deﬁne yi ∈CN as
xi = Σiyi
with Σi ∈CN×N a diagonal matrix with kth diagonal entry equal to σki, and yi
has identically distributed entries of zero mean and variance 1/n. Replacing all
occurrences of xi in (6.31) by Σiyi, we have:
1
N tr AN
 XNXH
N −zIN
−1 −1
N tr AN (F −zIN)−1
= 1
N tr AN
 XNXH
N −zIN
−1 F (F −zIN)−1
−1
N
n
X
i=1
yH
i Σi (F −zIN)−1 AN

X(i)XH
(i) −zIN
−1
Σiyi
1 + yH
i Σi

X(i)XH
(i) −zIN
−1
Σiyi
.
(6.32)
Applying the trace lemma, Theorem 3.4, the quadratic terms of the form
yH
i Yyi are close to 1
n tr Y. Therefore, in order for (6.32) to converge to zero, F
ought to take the form
F = 1
n
n
X
i=1
1
1 + eBN,i(z)Σ2
i

148
6. Deterministic equivalents
with
eBN,i(z) = 1
n tr Σ2
i
 XNXH
N −zIN
−1 .
We therefore infer that eN,i(z) takes the form
eN,i(z) = 1
n
N
X
k=1
σ2
ki
1
n
Pn
i=1 σ2
ki
1
1+eN,i(z) −z
by setting AN = Σ2
i .
From this point on, the result unfolds by showing the almost sure convergence
towards zero of the diﬀerence eN,i(z) −1
n tr Σ2
i
 XNXH
N −zIN
−1 and the
functional uniqueness of the implicit equation for the eN,i(z).
The symmetric expressions (6.30) make it easy to derive also a deterministic
equivalent of the Shannon transform.
Theorem 6.11. Let BN be deﬁned as in Theorem 6.10 and let x > 0. Then, as
N, n grow large with uniformly bounded ratio cn = N/n, the Shannon transform
VBN (x) of BN, deﬁned as
VBN (x) ≜1
N log det (IN + xBN)
satisﬁes
E[VBN (x)] −VN(x) →0
where VN(x) is given by:
VN(x) = 1
N
N
X
k=1
log

1 + ¯eN,k(−1
x)

+ 1
N
n
X
i=1
log

1 + eN,i(−1
x)

−
x
nN
X
1≤k≤N
1≤i≤n
σ2
ki
 1 + ¯eN,k(−1
x)
  1 + eN,i(−1
x)
.
It is worth pointing out here that the Shannon transform convergence result
is only stated in the mean sense and not, as was the case in Theorem 6.4, in the
almost sure sense. Remember indeed that the convergence result of Theorem 6.4
depends strongly on the fact that the empirical matrix BN can be proved to have
bounded spectral norm for all large N, almost surely. This is a consequence of
spectral norm inequalities and of Theorem 7.1. However, it is not known whether
Theorem 7.1 holds true for matrices with a variance proﬁle and the derivation
of Theorem 6.4 can therefore not be reproduced straightforwardly.
It is in fact not diﬃcult to show the convergence of the Shannon transform in
the mean via a simple dominated convergence argument. Indeed, remembering

6.2. Techniques for deterministic equivalents
149
the Shannon transform deﬁnition, Deﬁnition 3.2, we have:
E[VBN (x)] −VN(x) =
Z ∞
1
x
1
t −E[mBN (−t)]

dt −
Z ∞
1
x
1
t −mN(−t)

dt
(6.33)
for which we in particular have

1
t −E[mBN (−t)]

−
1
t −mN(−t)

≤

1
t −E[mBN (−t)]
 +

1
t −mN(−t)

=

Z 1
t −
1
λ + t

E[dF BN (λ)]
 +

Z 1
t −
1
λ + t

dFN(λ)

≤1
t2
Z
λE[dF BN (λ)] + 1
t2
Z
λdFN(λ).
It is now easy to prove from standard expectation calculus that both integrals
above are upper-bound by lim supN supi ∥Ri∥< ∞. Writing Equation (6.33)
under the form of a single integral, we have that the integrand tends to zero
as N →∞and is summable over the integration parameter t. Therefore, from
the dominated convergence theorem, Theorem 6.3, E[VBN (x)] −VN(x) →0.
Note now that, in the proof of Theorem 6.10, there is no actual need for the
matrices Σk to be diagonal. Also, there is no huge diﬃculty added by considering
the matrix XNXH
N + AN, instead of XNXH
N for any deterministic AN. As such,
Theorem 6.10 can be further generalized as follows.
Theorem 6.12 ([Wagner et al., 2011]). Let XN ∈CN×n have independent
columns xi = Hiyi, where yi ∈CNi has i.i.d. entries of zero mean, variance
1/n, and 4 + ε moment of order O(1/n2+ε/2), and Hi ∈CN×Ni are such that
Ri ≜HiHH
i
has uniformly bounded spectral norm over n, N. Let also AN ∈
CN×N be Hermitian non-negative and denote BN = XNXH
N + AN. Then, as N,
N1, . . . , Nn, and n grow large with ratios ci ≜Ni/n and c0 ≜N/n satisfying 0 <
lim infn ci ≤lim supn ci < ∞for 0 ≤i ≤n, we have that, for all non-negative
Hermitian matrix CN ∈CN×N with uniformly bounded spectral norm
1
n tr CN (BN −zIN)−1 −1
n tr CN
 
1
n
n
X
i=1
1
1 + eN,i(z)Ri + AN −zIN
!−1
a.s.
−→0
where eN,1(z), . . . , eN,n(z) form the unique functional solution of
eN,j(z) = 1
n tr Rj
 
1
n
n
X
i=1
1
1 + eN,i(z)Ri + AN −zIN
!−1
(6.34)
such that all eN,j(z) are Stieltjes transforms of a non-negative ﬁnite measure on
R+. Moreover, (eN,1(z), . . . , eN,n(z)) is given by eN,i(z) = limk→∞e(k)
N,i(z), where

150
6. Deterministic equivalents
e(0)
N,i = −1/z and, for k ≥0
e(k+1)
N,j
(z) = 1
n tr Rj
 
1
n
n
X
i=1
1
1 + e(k)
N,i(z)
Ri + AN −zIN
!−1
.
Also, for x > 0, the Shannon transform VBN (x) of BN, deﬁned as
VBN (x) ≜1
N log det (IN + xBN)
satisﬁes
E[VBN (x)] −VN(x) →0
where VN(x) is given by:
VN(x) = 1
N log det
 
IN + x
"
1
n
n
X
i=1
1
1 + eN,i(−1
x)Ri + AN
#!
+ 1
N
n
X
i=1
log

1 + eN,i(−1
x)

−1
N
n
X
i=1
eN,i(−1
x)
1 + eN,i(−1
x).
Remark 6.5. Consider the identically distributed entries x1, . . . , xn in Theorem
6.12, and take n1, . . . , nK to be K integers such that P
i ni = n. Deﬁne
˜R1, . . . , ˜RK ∈CN×N to be K non-negative deﬁnite matrices with uniformly
bounded spectral norm and T1 ∈Cn1×n1, . . . , TK ∈CnK×nK to be K diagonal
matrices with positive entries, Tk = diag(tk1, . . . , tknk). Denote Rk = ˜Rjtji,
k ∈{1, . . . , n}, with j the smallest integer such that k −(n1 + . . . + nj−1) > 0,
n0 = 0, and i = k −(n1 + . . . + nj−1). Under these conditions and notations, up
to some hypothesis restrictions, Theorem 6.12 with Hi = ˜R
1
2
i also generalizes
Theorem 6.1 applied to the sum of K Gram matrices with left correlation matrix
˜R1, . . . , ˜RK and right correlation matrices T1, . . . , TK.
From Theorem 6.12, taking AN = 0, we also immediately have that the
distribution function FN with Stieltjes transform
mN(z) = 1
N tr
 
1
n
n
X
i=1
1
1 + eN,i(z)Ri −zIN
!−1
(6.35)
where
eN,j(z) = 1
n tr Rj
 
1
n
n
X
i=1
1
1 + eN,i(z)Ri −zIN
!−1
(6.36)
is a deterministic equivalent for F XNXH
N . An interesting result with application
in low complex ﬁlter design, see Section 13.6 of Chapter 13, is the description in
closed-form of the successive moments of the distribution function FN.

6.2. Techniques for deterministic equivalents
151
Theorem 6.13 ([Hoydis et al., 2011c]). Let FN be the d.f. associated with the
Stieltjes transform mN(z) deﬁned by (6.35) with eN,i(z) given by (6.36). Further
denote MN,0, MN,1, . . . the successive moments of FN
MN,p =
Z
xpdFN(x).
Then, these moments are explicitly given by:
MN,p = (−1)p
p!
1
N tr Tp
with T0, T1, . . . deﬁned iteratively from the following set of recursive equations
initialized with T0 = IN, fk,0 = −1 and δk,0 = 1
n tr Rk for k ∈{1, . . . , n}
Tp+1 =
p
X
i=0
i
X
j=0
p
i
i
j

Tp−iQi−j+1Tj
Qp+1 = p + 1
n
n
X
k=1
fk,pRk
fk,p+1 =
p
X
i=0
i
X
j=0
p
i
i
j

(p −i + 1)fk,jfk,i−jδk,p−i
δk,p+1 = 1
n tr RkTp+1.
Moreover, with BN = XNXH
N, XN being deﬁned in Theorem 6.12, we have for
all integer p
Z
xpE[dF BN (x)] −MN,p →0
as N, n →∞.
Note that a similar result was established from a combinatorics approach in [Li
et al., 2004] which took the form of involved sums over non-crossing partitions,
when all Rk matrices are Toeplitz and of Wiener class [Gray, 2006]. The proof
of the almost sure convergence of
R
xpdF BN (x) to MN,p, claimed in [Li et al.,
2004], would require proving that the support BN is almost surely uniformly
bounded from above for all large N. However, this fact is unknown to this day
so that convergence in the mean can be ensured, while almost sure convergence
can only be conjectured. It holds true in particular when the family {R1, . . . , Rn}
is extracted from a ﬁnite set.
Proof. Note that FN is necessarily compactly supported as the ∥Ri∥are
uniformly bounded and that the eN,i(z) are non-negative for z < 0. Reminding
then that the Stieltjes transform mN of FN can be written in that case under the
form of a moment generating function by (3.6), the expression of the successive
moments unfolds from successive diﬀerentiations of −zmN(−1/z), taken in

152
6. Deterministic equivalents
z = 0. The convergence of the diﬀerence of moments is then a direct consequence
of the dominated convergence theorem, Theorem 6.3.
Another generalization of Theorem 6.10 is found in [Hachem et al., 2007],
where XN still has a variance proﬁle but has non-zero mean. The result in the
latter is more involved and expresses as follows.
Theorem 6.14. Let XN = YN + AN ∈CN×n be a random matrix where YN
has independent entries yij with zero mean, variance σ2
ij/N and ﬁnite 4 + ε
moment of order O(1/N 2+ε/2), and AN is a deterministic matrix. Denote
Σj ∈CN×N the diagonal matrix with ith diagonal entry σij and ¯Σi ∈Cn×n the
diagonal matrix with jth diagonal entry σij. Suppose moreover that the columns
of AN have uniformly bounded Euclidean norm and that the σij are uniformly
bounded, with respect to N and n. Then, as N, n grow large with ratio cN = N/n,
such that 0 < lim infN cN ≤lim supN cN < ∞, the e.s.d. F BN of BN ≜XNXH
N
satisﬁes
F BN −FN ⇒0
almost surely, with FN the distribution function with Stieltjes transform mN(z),
z ∈C \ R+, given by:
mN(z) = 1
N tr
 Ψ−1 −zAN ¯ΨAT
N
−1
where Ψ ∈CN×N is diagonal with ith entry ψi(z), ¯Ψ ∈Cn×n is diagonal with jth
entry ¯ψj(z), with ψi(z) and ¯ψj(z), 1 ≤i ≤N, 1 ≤j ≤n, the unique solutions of
ψi(z) = −1
z

1 + 1
n tr ¯Σ2
i
  ¯Ψ−1 −zAT
NΨAN
−1−1
¯ψj(z) = −1
z

1 + 1
n tr Σ2
j
 Ψ−1 −zAN ¯ΨAT
N
−1−1
which are Stieltjes transforms of distribution functions.
Besides, for x = −1
z > 0, let VBN (x) = 1
N log det
 IN + xXNXH
N

be the
Shannon transform of BN. Then
E[VBN (x)] −VN(x) →0
as N, n grow large, where VN(x) is deﬁned by
VN(x) = 1
N log det

xΨ−1 + A ¯ΨAT
+ 1
N log det
 x ¯Ψ−1
−1
x
1
nN
X
i,j
σ2
ijti¯tj
with ti the ith diagonal entry of the diagonal matrix
 Ψ−1 + xAN ¯ΨAT
N
−1 and
¯tj the jth diagonal entry of the diagonal matrix
  ¯Ψ−1 + xAT
NΨAN
−1.
Remark 6.6. In [Hachem et al., 2008b], it is shown in particular that, if the
entries of YN are Gaussian distributed, then the diﬀerence between the Stieltjes

6.2. Techniques for deterministic equivalents
153
transform of E[F BN ] and its deterministic equivalent, as well as the diﬀerence
between the Shannon transform of E[F BN ] and its deterministic equivalent
converge to zero at rate O(1/N 2).
6.2.4
Models involving Haar matrices
As evidenced in the previous section, Hermitian random matrices with i.i.d.
entries or originating from general sums or products of such matrices are
convenient to study using Stieltjes transform-based methods. This is essentially
due to the trace lemma, Theorem 3.4, which provides an almost sure limit
to xH(XXH −xxH −zIN)−1x with x one of the independent columns of the
random matrix X. Such results can actually be found for more structured random
matrices, such as the random bi-unitarily invariant unitary N × N matrices. We
recall from Deﬁnition 4.6 that these random matrices are often referred to as
Haar matrices or isometric matrices. Among the known properties of interest
here of Haar matrices [Petz and R´eﬀy, 2004], we have the following trace lemma
[Chaufray et al., 2004; Debbah et al., 2003a], equivalent to Theorem 3.4 for i.i.d.
random matrices.
Theorem 6.15. Let W be n < N columns of an N × N Haar matrix and
suppose w is a column of W. Let BN be an N × N random matrix, which is
a function of all columns of W except w. Then, assuming that, for growing N,
c = supn n/N < 1 and B = supN ∥BN∥< ∞, we have:
E
"wHBNw −
1
N −n tr(ΠBN)

4#
≤C
N 2
(6.37)
where Π = IN −WWH + wwH and C is a constant which depends only on
B and c. If supN ∥BN∥< ∞, by the Markov inequality, Theorem 3.5, and the
Borel–Cantelli lemma, Theorem 3.6, this entails
wHBNw −
1
N −n tr(ΠBN)
a.s.
−→0.
(6.38)
Proof. We provide here an intuitive, yet non-rigorous, sketch of the proof. Let
U ∈CN×(n−1) be n −1 columns of a unitary matrix. We can write all unit-norm
vectors w in the space orthogonal to the space spanned by the columns of U
as w =
Πx
∥Πx∥, where Π = IN −UUH is the projector on the space orthogonal
to UUH (and thus ΠΠ = Π) and x is a Gaussian vector with zero mean and
covariance matrix E[xxH] = IN independent of U. This makes w uniformly
distributed in its space. Also, the vector x is independent of Π by construction.
We therefore have from Theorem 3.4 and for N large
wHBNw = 1
N xHΠBNΠx
N
∥Πx∥2 ≃1
N tr (ΠBN)
N
∥Πx∥2 .

154
6. Deterministic equivalents
where the symbol “≃” stands for some approximation in the large N limit. Notice
then that Πx is, up to a basis change, a vector composed of N −n + 1 i.i.d.
standard Gaussian entries and n −1 zeros. Hence ∥Πx∥2
N−n →1. Deﬁning now W
such that WWH −wwH = UUH, the reasoning remains valid, and this entails
(6.38).
Since BN in Theorem 6.15 is assumed of uniformly bounded spectral norm,
wHBNw is uniformly bounded also. Hence, if N, n grow large with ratio n/N
uniformly away from one, the term
1
N−nwHBNw tends to zero. This therefore
entails the following corollary, which can be seen as a rank-1 perturbation of
Theorem 6.15.
Corollary 6.2. Let W and BN be deﬁned as in Theorem 6.15, with N and n
such that lim supn
n
N < 1. Then, as N, n grow large, for w any column of W
wHBNw −
1
N −n tr BN
 IN −WWH a.s.
−→0.
Corollary 6.2 only diﬀers from Theorem 6.15 by the fact that the projector Π
is changed into IN −WWH.
Also, when BN is independent of W, we fall back on the same result as for
the i.i.d. case.
Corollary 6.3. Let W be deﬁned as in Theorem 6.15, and let A ∈CN×N be
independent of Wand have uniformly bounded spectral norm. Then, as N grows
large, for w any column of W, we have:
wHAw −1
N tr A
a.s.
−→0.
Theorem 6.15 is the basis for establishing deterministic equivalents involving
isometric matrices. In the following, we introduce a result, based on Silverstein
and Bai’s approach, which generalizes Theorems 4.10, 4.11, and 4.12 to the case
when the Wi matrices are multiplied on the left by diﬀerent non-necessarily co-
diagonalizable matrices. These models are the basis for studying the properties
of multi-user or multi-cellular communications both involving unitary precoders
and taking into account the frequency selectivity of the channel. From a
mathematical point of view, there exists no simple way to study such models
using tools extracted solely from free probability theory. In particular, it is
interesting to note that in [Peacock et al., 2008], the authors already generalized
Theorem 4.12 to the case where the left-product matrices are diﬀerent but co-
diagonalizable. To do so, the authors relied on tools from free probability as
the basic instruments and then need some extra matrix manipulation to derive
their limiting result, in a sort of hybrid method between free probability and
analytical approach. In the results to come, though, no mention will be made to

6.2. Techniques for deterministic equivalents
155
free probability theory, as the result can be derived autonomously from the tools
developed in this section.
The following results are taken from [Couillet et al., 2011b], where detailed
proofs can be found. We start by introducing the fundamental equations.
Theorem 6.16 ([Couillet et al., 2011b]). For i ∈{1, . . . , K}, let Ti ∈Cni×ni be
Hermitian diagonal and let Hi ∈CN×Ni. Deﬁne Ri ≜HiHH
i ∈CN×N, ci = ni
Ni
and ¯ci = Ni
N . Then the following system of equations in (¯e1(z), . . . , ¯eK(z)):
¯ei(z) = 1
N tr Ti (ei(z)Ti + [¯ci −ei(z)¯ei(z)]Ini)−1
ei(z) = 1
N tr Ri


K
X
j=1
¯ej(z)Rj −zIN


−1
(6.39)
has a unique solution (¯e1(z), . . . , ¯eK(z)) ∈C(C, C) satisfying (e1(z), . . . , eK(z)) ∈
S(R+)K and, for z real negative, 0 ≤ei(z) < ci¯ci/¯ei(z) for all i. Moreover, for
each real negative z
¯ei(z) = lim
t→∞¯e(t)
i (z)
where ¯e(t)
i (z) is the unique solution of
¯e(t)
i (z) = 1
N tr Ti

e(t)
i (z)Ti + [¯ci −e(t)
i (z)¯e(t)
i (z)]Ini
−1
within the interval [0, ci¯ci/e(t)
i (z)), e(0)
i (z) can take any positive value and e(t)
i (z)
is recursively deﬁned by
e(t)
i (z) = 1
N tr Ri


K
X
j=1
¯e(t−1)
j
(z)Rj −zIN


−1
.
We then have the following theorem on a deterministic equivalent for the e.s.d.
of the model BN = PK
k=1 HiWiTiWH
i HH
i .
Theorem 6.17 ([Couillet et al., 2011b]). For i ∈{1, . . . , K}, let Ti ∈Cni×ni
be a Hermitian non-negative matrix with spectral norm bounded uniformly along
ni and Wi ∈CNi×ni be ni ≤Ni columns of a unitary Haar distributed random
matrix. Consider Hi ∈CN×Ni a random matrix such that Ri ≜HiHH
i ∈CN×N
has uniformly bounded spectral norm along N, almost surely. Deﬁne ci = ni
Ni and
¯ci = Ni
N and denote
BN =
K
X
i=1
HiWiTiWH
i HH
i .
Then, as N, N1, . . . , NK, n1, . . . , nK grow to inﬁnity with ratios ¯ci satisfying
0 < lim inf ¯ci ≤lim sup ¯ci < ∞and 0 ≤ci ≤1 for all i, the following limit holds

156
6. Deterministic equivalents
true almost surely
F BN −FN ⇒0
where FN is the distribution function with Stieltjes transform mN(z) deﬁned by
mN(z) = 1
N tr
 K
X
i=1
¯ei(z)Ri −zIN
!−1
where (¯e1(z), . . . , ¯eK(z)) are given by Theorem 6.16.
Consider the case when, for each i, ¯ci = 1 and Hi = R
1
2
i
for some square
Hermitian non-negative square root R
1
2
i of Ri. We observe that the system of
Equations (6.39) is very similar to the system of Equations (6.7) established for
the case of i.i.d. random matrices. The noticeable diﬀerence here is the addition
of the extra term −ei¯ei in the expression of ¯ei. Without this term, we fall back on
the i.i.d. case. Notice also that the case K = 1 corresponds exactly to Theorem
4.11, which was treated for c1 = 1.
Another point worth commenting on here is that, when z < 0, the ﬁxed-point
algorithm to determine ei can be initialized at any positive value, while the ﬁxed-
point algorithm to determine ¯ei must be initialized properly. If not, it is possible
that ¯e(t)
i
diverges. Also, if we naively run the ﬁxed-point algorithm jointly over
ei and ¯ei, we may end up not converging to the correct solution at all. Based on
experience, this case arises sometimes if no particular care is taken.
We hereafter provide both a sketch of the proof and a rather extensive
derivation, which explains how (6.39) is derived and how uniqueness is proved.
We will only treat the case where, for all i, lim sup ci < 1, ¯ci = 1, Hi = R
1
2
i and
the Ri are deterministic with uniformly bounded spectral norm in order both to
simplify notations and for the derivations to be close in nature to those proposed
in the proof of Theorem 6.1. The case where in particular lim sup ci = 1 for a
certain i only demands some additional technicalities, which are not necessary
here. Nonetheless, note that, for practical applications, all these hypotheses are
essential, as unitary precoding systems such as code division or space division
multiple access systems, e.g. CDMA and SDMA, may require square unitary
precoding matrices (hence ci = 1) and may involve rectangular multiple antenna
channel matrices Hi; these channels being modeled as Gaussian i.i.d.-based
matrices with almost surely bounded spectral norm. The proof follows the
derivation in [Couillet et al., 2011b], where a detailed derivation can be found.
The main steps of the proof are similar to those developed for the proof
of Theorem 6.1. In order to propose diﬀerent approaches than in previous
derivations, we will work almost exclusively with real negative z, instead of
z with positive imaginary part. We will also provide a shorter proof of the
ﬁnal convergence step mBN (z) −mN(z)
a.s.
−→0, relying on restrictions of the
domain of z along with arguments from Vitali’s convergence theorem. These
approaches are valid here because upper bounds on the spectral norms of Ri and

6.2. Techniques for deterministic equivalents
157
Ti are considered, which was not the case for Theorem 6.1. Apart from these
technical considerations, the main noticeable diﬀerence between the deterministic
equivalent approaches proposed for matrices with independent entries and for
Haar matrices lies in the ﬁrst convergence step, which is much more intricate.
Proof. We ﬁrst provide a sketch of the proof for better understanding, which will
enhance the aforementioned main novelty. As usual, we wish to prove that there
exists a matrix F = PK
i=1 ¯fiRi, such that, for all non-negative A with ∥A∥< ∞
1
N tr A (BN −zIN)−1 −1
N tr A (F −zIN)−1 a.s.
−→0.
Contrary to classical deterministic equivalent approaches for random matrices
with i.i.d. entries, ﬁnding a deterministic equivalent for
1
N tr A (BN −zIN)−1
is not straightforward. The reason is that during the derivation, terms such
as
1
N−ni tr
 IN −WiWH
i

A
1
2 (BN −zIN)−1 A
1
2 , with the
 IN −WiWH
i

preﬁx
will naturally appear, as a result of applying the trace lemma, Theorem 6.15,
that will be required to be controlled. We proceed as follows.
• We ﬁrst denote for all i, δi ≜
1
N−ni tr
 IN −WiWH
i

R
1
2
i (BN −zIN)−1 R
1
2
i
some auxiliary variable. Then, using the same techniques as in the proof of
Theorem 6.1, denoting further fi ≜1
N tr Ri (BN −zIN)−1, we prove
fi −1
N tr Ri (G −zIN)−1 a.s.
−→0
with G = PK
j=1 ¯gjRj and
¯gi =
1
1 −ci + 1
N
Pni
l=1
1
1+tilδi
1
N
ni
X
l=1
til
1 + tilδi
where ti1, . . . , tini are the eigenvalues of Ti. Noticing additionally that
(1 −ci)δi −fi + 1
N
ni
X
l=1
δi
1 + tilδi
a.s.
−→0
we have a ﬁrst hint on a ﬁrst deterministic equivalent for fi. Precisely, we
expect to obtain the set of fundamental equations
∆i =
1
1 −ci
"
ei −1
N
ni
X
l=1
∆i
1 + til∆i
#
ei = 1
N tr Ri


K
X
j=1
1
1 −cj + 1
N
Pnj
l=1
1
1+tjl∆j
1
N
nj
X
l=1
tjl
1 + tjl∆j
Rj −zIN


−1
.
• The expressions of ¯gi and their deterministic equivalents are however not very
convenient under this form. It is then shown that
¯gi −1
N
ni
X
l=1
til
1 + tilfi −fi¯gi
= ¯gi −1
N tr Ti (fiTi + [1 −fi¯gi]Ini)−1 a.s.
−→0

158
6. Deterministic equivalents
which induces the 2K-equation system
fi −1
N tr Ri


K
X
j=1
¯gjRj −zIN


−1
a.s.
−→0
¯gi −1
N tr Ti (¯giTi + [1 −fi¯gi])−1 a.s.
−→0.
• These relations are suﬃcient to infer the deterministic equivalent but will be
made more attractive for further considerations by introducing F = PK
i=1 ¯fiRi
and proving that
fi −1
N tr Ri


K
X
j=1
¯fjRj −zIN


−1
a.s.
−→0
¯fi −1
N tr Ti
  ¯fiTi + [1 −fi ¯fi]
−1 = 0
where, for z < 0, ¯fi lies in [0, ci/fi) and is now uniquely determined by fi.
In particular, this step provides an explicit expression ¯fi as a function of fi,
which will be translated into an explicit expression of ¯ei as a function of ei.
This is the very technical part of the proof. We then prove the existence and
uniqueness of a solution to the ﬁxed-point equation
ei −1
N tr Ri


K
X
j=1
¯ejRj −zIN


−1
= 0
¯ei −1
N tr Ti (¯eiTi + [1 −ei¯ei])−1 = 0
for all ﬁnite N, z real negative, and for ¯ei ∈[0, ci/fi]. Here, instead of following
the approach of the proof of uniqueness for the fundamental equations of
Theorem 6.1, we use a property of so-called standard functions. We will show
precisely that the vector application h = (h1, . . . , hK) with
hi : (x1, . . . , xK) 7→1
N tr Ri


K
X
j=1
¯xjRj −zIN


−1
where ¯xi is the unique solution to
¯xi = 1
N tr Ti (¯xiTi + [1 −xi¯xi])−1
lying in [0, ci/xi), is a standard function. It will unfold that the ﬁxed-point
equation in (e1, . . . , eK) has a unique solution with positive entries and that this
solution can be determined as the limiting iteration of a classical ﬁxed-point
algorithm.
The last step proves that the unique solution (e1, . . . , eN) is such that
ei −fi
a.s.
−→0

6.2. Techniques for deterministic equivalents
159
which is solved by arguments borrowed from the work of Hachem et al. [Hachem
et al., 2007], using a restriction on the deﬁnition domain of z, which simpliﬁes
greatly the calculus.
We now turn to the precise proof. We use again the Bai and Silverstein steps:
the convergence fi −1
N tr Ri
PK
j=1 ¯fjRj −zIN
−1 a.s.
−→0 in a ﬁrst step, the
existence and uniqueness of a solution to ei = 1
N tr Ri
PK
j=1 ¯ejRj −zIN
−1
in a second, and the convergence ei −fi
a.s.
−→0 in a third. Although precise
control of the random variables involved needs be carried out, as is detailed
in [Couillet et al., 2011b], we hereafter elude most technical parts for simplicity
and understanding.
Step 1: First convergence step
In this section, we take z < 0, until further notice. Let us ﬁrst introduce
the
following
parameters.
We
will
denote
T = maxi{lim sup ∥Ti∥},
R =
maxi{lim sup ∥Ri∥} and c = maxi{lim sup ci}.
We start with classical deterministic equivalent techniques. Let A ∈CN×N be
a Hermitian non-negative deﬁnite matrix with spectral norm uniformly bounded
by A. Taking G = PK
j=1 ¯gjRj, with ¯g1, . . . , ¯gK left undeﬁned for the moment,
we have:
1
N tr A(BN −zIN)−1 −1
N tr A(G −zIN)−1
= 1
N tr
"
A(BN −zIN)−1
K
X
i=1
R
1
2
i
 −WiTiWH
i + ¯giIN

R
1
2
i (G −zIN)−1
#
=
K
X
i=1
¯gi
1
N tr A(BN −zIN)−1Ri(G −zIN)−1
−1
N
K
X
i=1
ni
X
l=1
tilwH
ilR
1
2
i (G −zIN)−1A(BN −zIN)−1R
1
2
i wil
=
K
X
i=1
¯gi
1
N tr A(BN −zIN)−1Ri(G −zIN)−1
−1
N
K
X
i=1
ni
X
l=1
tilwH
ilR
1
2
i (G −zIN)−1A(B(i,l) −zIN)−1R
1
2
i wil
1 + tilwH
ilR
1
2
i (B(i,l) −zIN)−1R
1
2
i wil
,
(6.40)
with ti1, . . . , tini the eigenvalues of Ti.
The
quadratic
forms
wH
ilR
1
2
i (G −zIN)−1A(B(i,l) −zIN)−1R
1
2
i wil
and
wH
ilR
1
2
i (B(i,l) −zIN)−1R
1
2
i wil are not asymptotically close to the trace of the
inner matrix, as in the i.i.d. case, but to the trace of the inner matrix multiplied
by (IN −WiWH
i ). This complicates the calculus. In the following, we will
therefore study the following stochastic quantities, namely the random variables
δi, βi and fi, introduced below.

160
6. Deterministic equivalents
For every i ∈{1, . . . , K}, denote
δi ≜
1
N −ni
tr
 IN −WiWH
i

R
1
2
i (BN −zIN)−1 R
1
2
i
fi ≜1
N tr Ri (BN −zIN)−1
both being clearly non-negative. We may already recognize that fi is a key
quantity for the subsequent derivations, as it will be shown to be asymptotically
close to ei, the central parameter of our deterministic equivalent.
Writing Wi = [wi,1, . . . , wi,ni] and WiWH
i = Pni
l=1 wilwH
il, we have from
standard calculus and the matrix inversion lemma, Lemma 6.2, that
(1 −ci)δi = fi −1
N
ni
X
l=1
wH
ilR
1
2
i (BN −zIN)−1 R
1
2
i wil
= fi −1
N
ni
X
l=1
wH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil
1 + tilwH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil
(6.41)
with B(i,l) = BN −tilR
1
2
i wilwH
ilR
1
2
i .
Since z < 0, δi ≥0, so that
1
1+tilδi is well deﬁned. We recognize already
from Theorem 6.15 that each quadratic term wH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil is
asymptotically close to δi. By adding the term
1
N
Pni
l=1
δi
1+tilδi on both sides,
(6.41) can further be rewritten
(1 −ci)δi −fi + 1
N
ni
X
l=1
δi
1 + tilδi
= 1
N
ni
X
l=1


δi
1 + tilδi
−
wH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil
1 + tilwH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil

.
We now apply the trace lemma, Theorem 6.15, which ensures that
E


(1 −ci)δi −fi + 1
N
ni
X
l=1
δi
1 + tilδi

4
= O
 1
N 2

.
(6.42)
We do not provide the precise derivations of the fourth order moment inequalities
here and in all the equations that follow, our main purpose being concentrated
on the fundamental steps of the proof. Precise calculus and upper bounds can
be found in [Couillet et al., 2011b]. This is our ﬁrst relation that links δi to
fi = 1
N tr Ri (BN −zIN)−1.
Introducing now an additional A(G −zIN)−1 matrix in the argument of the
trace of δi, with G, A ∈CN×N any non-negative deﬁnite matrices, ∥A∥≤A, we
denote
βi ≜
1
N −ni
tr
 IN −WiWH
i

R
1
2
i (G −zIN)−1 A (BN −zIN)−1 R
1
2
i .

6.2. Techniques for deterministic equivalents
161
We then proceed similarly as for δi by showing
βi =
1
N −ni
tr R
1
2
i (G −zIN)−1 A (BN −zIN)−1 R
1
2
i
−
1
N −ni
ni
X
l=1
wH
ilR
1
2
i (G −zIN)−1 A
 B(i,l) −zIN
−1 R
1
2
i wil
1 + tilwH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil
from which we have:
1
N −ni
tr R
1
2
i (G −zIN)−1 A (BN −zIN)−1 R
1
2
i −
1
N −ni
ni
X
l=1
βi
1 + tilδi
−βi
=
1
N −ni
ni
X
l=1

wH
ilR
1
2
i (G −zIN)−1 A
 B(i,l) −zIN
−1 R
1
2
i wil
1 + tilwH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil
−
βi
1 + tilδi

.
Since numerators and denominators converge again to one another, we can
show from Theorem 6.15 again that
E



wH
ilR
1
2
i (G −zIN)−1 A
 B(i,l) −zIN
−1 R
1
2
i wil
1 + tilwH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil
−
βi
1 + tilδi

4
= O
 1
N 2

.
(6.43)
Hence
E



1
N tr Ri (G −zIN)−1 A (BN −zIN)−1 −βi
 
1 −ci + 1
N
ni
X
l=1
1
1 + tilδi
!
4

= O
 1
N 2

.
(6.44)
This
provides
us
with
the
second
relation
that
links
βi
to
1
N tr R
1
2
i (G −zIN)−1 A (BN −zIN)−1 R
1
2
i .
That
is,
we
have
expressed
both δi and βi as a function of the traces
1
N tr R
1
2
i (BN −zIN)−1 R
1
2
i
and
1
N tr R
1
2
i (G −zIN)−1 A (BN −zIN)−1 R
1
2
i , which are more conventional to
work with.
We are now in position to determine adequate expressions for ¯g1, . . . , ¯gK.
From the fact that wH
ilR
1
2
i (B(i,l) −zIN)−1R
1
2
i wil is asymptotically close to δi
and that wH
ilR
1
2
i (G −zIN)−1A(B(i,l) −zIN)−1R
1
2
i wil is asymptotically close to
βi, we choose, based on (6.44) especially
¯gi =
1
1 −ci + 1
N
Pni
l=1
1
1+tilδi
1
N
ni
X
l=1
til
1 + tilδi
.

162
6. Deterministic equivalents
We then have
1
N tr A(BN −zIN)−1 −1
N tr A(G −zIN)−1
=
K
X
i=1
1
N
Pni
l=1
til
1+tilδi
1
N tr Ri (G −zIN)−1 A (BN −zIN)−1
1 −ci + 1
N
Pni
l=1
1
1+tilδi
−1
N
K
X
i=1
ni
X
l=1
tilwH
ilR
1
2
i (G −zIN)−1A(B(i,l) −zIN)−1R
1
2
i wil
1 + tilwH
ilR
1
2
i (B(i,l) −zIN)−1R
1
2
i wil
=
K
X
i=1
1
N
ni
X
l=1
til
"
1
N tr Ri (G −zIN)−1 A (BN −zIN)−1
(1 −ci + 1
N
Pni
l′=1
1
1+ti,l′δi )(1 + tilδi)
−wH
ilR
1
2
i (G −zIN)−1A(B(i,l) −zIN)−1R
1
2
i wil
1 + tilwH
ilR
1
2
i (B(i,l) −zIN)−1R
1
2
i wil

.
To show that this last diﬀerence tends to zero, notice that 1 + tilδi ≥1 and
1 −ci ≤1 −ci + 1
N
ni
X
l=1
1
1 + tilδi
≤1
which ensure that we can divide the term in the expectation in the left-hand
side of (6.44) by 1 + tilδi and 1 −ci + 1
N
Pni
l=1
1
1+tilδi without risking altering
the order of convergence. This results in
E



βi
1 + tilδi
−
1
N tr R
1
2
i (G −zIN)−1 A (BN −zIN)−1 R
1
2
i

1 −ci + 1
N
Pni
l=1
1
1+tilδi

(1 + tilδi)

4
= O
 1
N 2

.
(6.45)
From (6.43) and (6.45), we ﬁnally have that
E



1
N tr Ri (G −zIN)−1 A (BN −zIN)−1

1 −ci + 1
N
Pni
l=1
1
1+tilδi

(1 + tilδi)
−
wH
ilR
1
2
i (G −zIN)−1 A
 B(i,l) −zIN
−1 R
1
2
i wil
1 + tilwH
ilR
1
2
i
 B(i,l) −zIN
−1 R
1
2
i wil

4
= O
 1
N 2

(6.46)
from which we obtain
E
"
1
N tr A(BN −zIN)−1 −1
N tr A(G −zIN)−1

4#
= O
 1
N 2

.
(6.47)
This provides us with a ﬁrst interesting result, from which we could infer
a deterministic equivalent of mBN (z), which would be written as a function
of deterministic equivalents of δi and deterministic equivalents of fi, for i =
{1, . . . , K}. However this form is impractical to work with and we need to go
further in the study of ¯gi.

6.2. Techniques for deterministic equivalents
163
Observe that ¯gi can be written under the form
¯gi = 1
N
ni
X
l=1
til
(1 −ci + 1
N
Pni
l=1
1
1+tilδi ) + tilδi(1 −ci + 1
N
Pni
l=1
1
1+tilδi ).
We will study the denominator of the above expression and show that it can be
synthesized into a much more attractive form.
From (6.42), we ﬁrst have
E


fi −δi
 
1 −ci + 1
N
ni
X
l=1
1
1 + tilδi
!
4
= O
 1
N 2

.
Noticing that
1 −¯giδi
 
1 −ci + 1
N
ni
X
l=1
1
1 + tilδi
!
= 1 −ci + 1
N
ni
X
l=1
1
1 + tilδi
we therefore also have
E


(1 −¯gifi) −
 
1 −ci + 1
N
ni
X
l=1
1
1 + tilδi
!
4
= O
 1
N 2

.
The two relations above lead to
E


¯gi −1
N
ni
X
l=1
til
tilfi + 1 −fi¯gi

4

= E



1
N
ni
X
l=1
til
til [fi −δiκi] + [1 −fi¯gi −κi]
[κi + tilδiκi] [tilfi + 1 −fi¯gi]

4

(6.48)
where we denoted κi ≜1 −ci + 1
N
Pni
l=1
1
1+tilδi .
Again, all diﬀerences in the numerator converge to zero at a rate O(1/N 2).
However, the denominator presents now the term tilfi + 1 −fi¯gi, which must
be controlled and ensured to be away from zero. For this, we can notice that
¯gi ≤T/(1 −c) by deﬁnition, while fi ≤R/|z|, also by deﬁnition. It is therefore
possible, by taking z < 0 suﬃciently small, to ensure that 1 −fi¯gi > 0. We
therefore from now on assume that such z are considered.
Equation (6.48) becomes in this case
E


¯gi −1
N
ni
X
l=1
til
tilfi + 1 −fi¯gi

4
= O
 1
N 2

.
We are now ready to introduce the matrix F. Consider
F =
K
X
i=1
¯fiRi,

164
6. Deterministic equivalents
with ¯fi deﬁned as the unique solution to the equation in x
x = 1
N
ni
X
l=1
til
1 −fix + fitil
(6.49)
within the interval 0 ≤x < ci/fi. To prove the uniqueness of the solution within
this interval, note simply that
ci
fi
≥1
N
ni
X
l=1
til
1 −fi(ci/fi) + fitil
0 ≤1
N
ni
X
l=1
til
1 −fi · 0 + fitil
and that the function x 7→1
N
Pni
l=1
til
1−fix+fitil is convex. Hence the uniqueness
of the solution in [0, ci/fi]. We also show that this solution is an attractor of the
ﬁxed-point algorithm, when correctly initialized. Indeed, let x0, x1, . . . be deﬁned
by
xn+1 = 1
N
ni
X
l=1
til
1 −fixn + fitil
with x0 ∈[0, ci/fi]. Then, xn ∈[0, ci/fi] implies 1 −fixn + fitil ≥1 −ci +
fitil > fitil and therefore fixn+1 ≤ci, so x0, x1, . . . are all contained in [0, ci/fi].
Now observe that
xn+1 −xn = 1
N
ni
X
l=1
fi(xn −xn−1)
(1 + tilfi −fixn)(1 + tilfi −fixn−1)
so that the diﬀerences xn+1 −xn and xn −xn−1 have the same sign. The
sequence x0, x1, . . . is therefore monotonic and bounded: it converges. Calling
x∞this limit, we have:
x∞= 1
N
ni
X
l=1
til
1 + tilfi −fix∞
as required.
To ﬁnally prove that
1
N tr A(BN −zIN)−1 −1
N tr A(F −zIN)−1 a.s.
−→0, we
want now to show that ¯gi −¯fi tends to zero at a suﬃciently fast rate. For this,
we write
E
h¯gi −¯fi
4i
≤8E


¯gi −1
N
ni
X
l=1
til
tilfi + 1 −fi¯gi

4

+ 8E



1
N
ni
X
l=1
til
tilfi + 1 −fi¯gi
−1
N
ni
X
l=1
til
tilfi + 1 −fi ¯fi

4


6.2. Techniques for deterministic equivalents
165
= 8E


¯gi −1
N
ni
X
l=1
til
tilfi + 1 −fi¯gi

4

+ E

¯gi −¯fi
4

1
N
ni
X
l=1
tilfi
(tilfi + 1 −fi ¯fi)(tilfi + 1 −fi¯gi)

4
.
(6.50)
We only need to ensure now that the coeﬃcient multiplying
¯gi −¯fi
 in the
right-hand side term is uniformly smaller than one. This unfolds again from
noticing that the numerator can be made very small, with the denominator kept
away from zero, for suﬃciently small z < 0. For these z, we can therefore prove
that
E
h¯gi −¯fi
4i
= O
 1
N 2

.
It is important to notice that this holds essentially because we took ¯fi to be the
unique solution of (6.49) lying in the interval [0, ci/fi). The other solution (that
happens to equal 1/fi for ci = 1) does not satisfy this fourth moment inequality.
Finally, we can proceed to proving the deterministic equivalent relations.
1
N tr A (G −zIN)−1 −1
N tr A (F −zIN)−1
=
K
X
i=1
1
N
ni
X
l=1
til
"
1
N tr RiA (G −zIN)−1 (F −zIN)−1
(1 −ci + 1
N
Pni
l′=1
1
1+ti,l′δi )(1 + tilδi)
−
1
N tr RiA (G −zIN)−1 (F −zIN)−1
1 −fi ¯fi + tilfi
#
=
K
X
i=1
1
N
ni
X
l=1
til
" 
1
(1 −ci + 1
N
Pni
l′=1
1
1+ti,l′δi )(1 + tilδi) −
1
1 −fi¯gi + tilfi
!
+

1
1 −fi¯gi + tilfi
−
1
1 −fi ¯fi + tilfi
 1
N tr RiA (G −zIN)−1 (F −zIN)−1 .
The ﬁrst diﬀerence in brackets is already known to be small from previous
considerations on the relations between ¯gi and δi. As for the second diﬀerence,
it also goes to zero fast as E[|¯gi −¯fi|4] is summable. We therefore have
E
"
1
N tr A (G −zIN)−1 −1
N tr A (F −zIN)−1

4#
= O
 1
N 2

.
Together with (6.47), we ﬁnally have
E
"
1
N tr A (BN −zIN)−1 −1
N tr A (F −zIN)−1

4#
= O
 1
N 2

.

166
6. Deterministic equivalents
Applying the Markov inequality, Theorem 3.5, and the Borel–Cantelli lemma,
Theorem 3.6, this entails
1
N tr A (BN −zIN)−1 −1
N tr A (F −zIN)−1 a.s.
−→0
(6.51)
as N grows large. This holds however to this point for a restricted set of negative
z. But now, from the Vitali convergence theorem, Theorem 3.11, and the fact
that
1
N tr A (BN −zIN)−1 and
1
N tr A (F −zIN)−1 are uniformly bounded on
all closed subset of C not containing the positive real half-line, we have that the
convergence (6.51) holds true for all z ∈C \ R+, and that this convergence is
uniform on all closed subsets of C \ R+.
Applying the result for A = Rj, this is in particular
fj −1
N tr Rj
 K
X
i=1
¯fiRi −zIN
!−1
a.s.
−→0
where we recall that ¯fi is the unique solution to
x = 1
N
ni
X
i=1
til
1 −fix + tilfi
within the set [0, ci/fi).
For A = IN, this says that
mBN (z) −1
N tr Rj
 K
X
i=1
¯fiRi −zIN
!−1
a.s.
−→0
which proves the sought convergence of the Stieltjes transform. We now move to
proving the existence and uniqueness of the set (e1, . . . , eK) = (e1(z), . . . , eK(z)).
Step 2: Existence and uniqueness
The existence step unfolds similarly as in the proof of Theorem 6.1. It suﬃces
to consider the matrices T[p],i ∈Cnip and R[p],i ∈CNp for all i deﬁned as the
Kronecker products T[p],i ≜Ti ⊗Ip, R[p],i ≜Ri ⊗Ip, which have, respectively,
the d.f. F Ti and F Ri for all p. Similar to the i.i.d. case, it is easy to see that ei
is unchanged by substituting the T[p],i and R[p],i to the Ti and Ri, respectively.
Denoting in the same way f[p],i the equivalent of fi for T[p],i and R[p],i, from the
convergence result of Step 1, we can choose f[1],i, f[2],i, . . . a sequence of the set
of probability one where convergence is ensured as p grows large (N and the ni
are kept ﬁxed). This sequence is uniformly bounded (by R/|z|) in C \ R+, and
therefore we can extract a converging subsequence out of it. The limit over this
subsequence satisﬁes the ﬁxed-point equation, which therefore proves existence.
It is easy to see that the limit is also the Stieltjes transform of a ﬁnite measure
on R+ by verifying the conditions of Theorem 3.2.
We will prove uniqueness of positive solutions e1, . . . , eK > 0 for z < 0 and
the convergence of the classical ﬁxed-point algorithm to these values. We ﬁrst

6.2. Techniques for deterministic equivalents
167
introduce some notations and useful identities. Notice that, similar to Step 1
with the δi terms, we can deﬁne, for any pair of variables xi and ¯xi, with ¯xi
deﬁned as the solution y to y = 1
N
Pni
l=1
til
1+xjtil−xjy such that 0 ≤y < cj/xj, the
auxiliary variables ∆1, . . . , ∆K, with the properties
xi = ∆i
 
1 −ci + 1
N
ni
X
l=1
1
1 + til∆i
!
= ∆i
 
1 −1
N
ni
X
l=1
til∆i
1 + til∆i
!
and
1 −xi¯xi = 1 −ci + 1
N
ni
X
l=1
1
1 + til∆i
= 1 −1
N
ni
X
l=1
til∆i
1 + til∆i
.
The uniqueness of the mapping between the xi and ∆i can be proved. In fact,
it turns out that ∆i is a monotonically increasing function of xi with ∆i = 0 for
xi = 0.
We take the opportunity of the above deﬁnitions to notice that, for xi > x′
i
and ¯x′
i, ∆′
i deﬁned similarly as ¯xi and ∆i
xi¯xi −x′
i¯x′
i = 1
N
ni
X
l=1
til(∆i −∆′
i)
(1 + til∆i)(1 + til∆′
i) > 0
(6.52)
whenever Ti ̸= 0. Therefore xi¯xi is a growing function of xi (or equivalently of
∆i). This will turn out a useful remark later.
We are now in position to prove the step of uniqueness. Deﬁne, for i ∈
{1, . . . , K}, the functions
hi : (x1, . . . , xK) 7→1
N tr Ri


K
X
j=1
¯xjRj −zIN


−1
with ¯xj the unique solution of the equation in y
y = 1
N
nj
X
l=1
tjl
1 + xjtjl −xjy
(6.53)
such that 0 ≤y ≤cj/xj.
We will prove in the following that the multivariate function h = (h1, . . . , hK)
is a standard function, deﬁned in [Yates, 1995], as follows.
Deﬁnition 6.2. A function h(x1, . . . , xK) ∈RK, h = (h1, . . . , hK), is said to be
a standard function or a standard interference function if it fulﬁlls the following
conditions
1. Positivity: for all j, if x1, . . . , xK > 0 then hj(x1, . . . , xK) > 0,
2. Monotonicity:
if
x1 > x′
1, . . . , xK > x′
K,
then
hj(x1, . . . , xK) >
hj(x′
1, . . . , x′
K), for all j,
3. Scalability: for all α > 1 and j, αhj(x1, . . . , xK) > hj(αx1, . . . , αxK).

168
6. Deterministic equivalents
The important result regarding standard functions [Yates, 1995] is given as
follows.
Theorem 6.18. If a K-variate function h(x1, . . . , xK) is standard and there
exists (x1, . . . , xK) such that, for all j, xj ≥hj(x1, . . . , xK), then the ﬁxed-point
algorithm that consists in setting
x(t+1)
j
= hj(x(t)
1 , . . . , x(t)
K )
for t ≥1 and for any initial values x(0)
1 , . . . , x(0)
K > 0 converges to the unique
jointly positive solution of the system of K equations
xj = hj(x1, . . . , xK)
with j ∈{1, . . . , K}.
Proof. The proof of the uniqueness unfolds easily from the standard function
assumptions. Take (x1, . . . , xK) and (x′
1, . . . , x′
K) two sets of supposedly distinct
all positive solutions. Then there exists j such that xj < x′
j, αxj = x′
j, and αxi ≥
x′
i for i ̸= j. From monotonicity and scalability, it follows that
x′
j = hj(x′
1, . . . , x′
K) ≤hj(αx1, . . . , αxK) < αhj(x1, . . . , xK) = αxj
a contradiction. The convergence of the ﬁxed-point algorithm from any point
(x1, . . . , xK) unfolds from similar arguments, see [Yates, 1995] for more details.
Therefore, by showing that h ≜(h1, . . . , hK) is standard, we will prove that the
classical ﬁxed-point algorithm converges to the unique set of positive solutions
e1, . . . , eK, when z < 0.
The positivity condition is straightforward as ¯xi is positive for xi positive and
therefore hj(x1, . . . , xK) is always positive whenever x1, . . . , xK are.
The scalability is also rather direct. Let α > 1, then:
αhj(x1, . . . , xK) −hj(αx1, . . . , αxK)
= 1
N tr Rj
 K
X
k=1
¯xk
α Rk −z
αIN
!−1
−1
N tr Rj
 K
X
k=1
¯x(a)
k Rk −zIN
!−1
where we denoted ¯x(a)
j
the unique solution to (6.53) with xj replaced by αxj,
within the set [0, cj/(αxj)). Since αxi > xi, from the property (6.52), we have
αxk¯x(α)
k
> xk¯xk or equivalently ¯x(a)
k
−¯xk
α > 0. We now deﬁne the two matrices
A ≜Pk
k=1
¯xk
α Rk −z
αIN and A(α) ≜Pk
k=1 ¯x(α)
k Rk −zIN. For any vector a ∈
CN
aH 
A −A(α)
a =
K
X
k=1
 ¯xk
α −¯x(α)
k

aHRka + z

1 −1
α

aHa ≤0

6.2. Techniques for deterministic equivalents
169
since z < 0, 1 −1
α > 0 and
¯xk
α −¯x(α)
k
< 0. Therefore A −A(α) is non-positive
deﬁnite. Now, from [Horn and Johnson, 1985, Corollary 7.7.4], this implies that
A−1 −(A(α))−1 is non-negative deﬁnite. Writing
1
N tr Rj

A−1 −(A(α))−1
= 1
N
N
X
i=1
rH
j,i

A−1 −(A(α))−1
rj,i
with rj,i the ith column of Rj, this ensures αhj(x1, . . . , xK) > hj(αx1, . . . , αxK).
The monotonicity requires some more lines of calculus. This unfolds from
considering ¯xi as a function of ∆i, by verifying that
d
d∆i ¯xi is negative.
d
d∆i
¯xi = 1
∆2
i
 
1 −
1
1 −1
N
Pni
l=1
til∆i
1+til∆i
!
+ 1
∆2
i



1
N
Pni
l=1
til∆i
(1+til∆i)2

1 −1
N
Pni
l=1
til∆i
1+til∆i
2



=
−1
N
Pni
l=1
til∆i
1+til∆i

1 −1
N
Pni
l=1
til∆i
1+til∆i

+ 1
N
Pni
l=1
til∆i
(1+til∆i)2
∆2
i

1 −1
N
Pni
l=1
til∆i
1+til∆i
2
=

1
N
Pni
l=1
til∆i
1+til∆i
2
−1
N
Pni
l=1
til∆i
1+til∆i + 1
N
Pni
l=1
til∆i
(1+til∆i)2
∆2
i

1 −1
N
Pni
l=1
til∆i
1+til∆i
2
=

1
N
Pni
l=1
til∆i
1+til∆i
2
−1
N
Pni
l=1
(til∆i)2
(1+til∆i)2
∆2
i

1 −1
N
Pni
l=1
til∆i
1+til∆i
2
.
From the Cauchy–Schwarz inequality, we have:
 ni
X
l=1
1
N
til∆i
1 + til∆i
!2
≤
ni
X
l=1
1
N 2
ni
X
l=1
(til∆i)2
(1 + til∆i)2
= ci
1
N
ni
X
l=1
(til∆i)2
(1 + til∆i)2
< 1
N
ni
X
l=1
(til∆i)2
(1 + til∆i)2
which is suﬃcient to conclude that
d
d∆i ¯xi < 0. Since ∆i is an increasing function
of xi, we have that ¯xi is a decreasing function of xi, i.e.
d
dxi ¯xi < 0. This being
said, using the same line of reasoning as for scalability, we ﬁnally have that, for
two sets x1, . . . , xK and x′
1, . . . , x′
K of positive values such that xj > x′
j
hj(x1, . . . , xK) −h(x′
1, . . . , x′
K)
= 1
N tr Rj


 K
X
k=1
¯xkRk −zIN
!−1
−
 K
X
k=1
¯x′
kRk −zIN
!−1
> 0

170
6. Deterministic equivalents
with ¯x′
j deﬁned equivalently as ¯xj, and where the terms (¯x′
k −¯xk) are all positive
due to negativity of
d
dxi ¯xi. This proves the monotonicity condition.
We ﬁnally have from Theorem 6.18 that (e1, . . . , eK) is uniquely deﬁned
and that the classical ﬁxed-point algorithm converges to this solution from
any initialization point (remember that, at each step of the algorithm, the set
¯e1, . . . , ¯eK must be evaluated, possibly thanks to a further ﬁxed-point algorithm).
Consider
now
two
sets
of
Stieltjes
transforms
(e1(z), . . . , eK(z))
and
(e′
1(z), . . . , e′
K(z)), z ∈C \ R+, functional solutions of the ﬁxed-point Equation
(6.39). Since ei(z) −e′
i(z) = 0 for all i and for all z < 0, and ei(z) −e′
i(z) is
holomorphic on C \ R+ as the diﬀerence of Stieltjes transforms, by analytic
continuation (see, e.g., [Rudin, 1986]), ei(z) −e′
i(z) = 0 over C \ R+. This
therefore proves, in addition to point-wise uniqueness on the negative half-
line, the uniqueness of the Stieltjes transform solution of the functional implicit
equation and deﬁned over C \ R+.
We ﬁnally complete the proof by showing that the stochastic fi and the
deterministic ei are asymptotically close to one another as N grows large.
Step 3: Convergence of ei −fi
For this step, we follow the approach in [Hachem et al., 2007]. Denote
εi
N ≜fi −1
N tr Ri
 K
X
k=1
¯fkRk −zIN
!−1
and recall the deﬁnitions of fi, ei, ¯fi and ¯ei:
fi = 1
N tr Ri (BN −zIN)−1
ei = 1
N tr Ri


K
X
j−1
¯ejRj −zIN


−1
¯fi = 1
N
ni
X
l=1
ti,l
1 −fi ¯fi + ti,lfi
,
¯fi ∈[0, ci/fi]
¯ei = 1
N
ni
X
l=1
ti,l
1 −ei¯ei + ti,lei
,
¯ei ∈[0, ci/ei].
From the deﬁnitions above, we have the following set of inequalities
fi ≤R
|z|,
ei ≤R
|z|,
¯fi ≤
T
1 −ci
,
¯ei ≤
T
1 −ci
.
(6.54)
We will show in the sequel that
ei −fi
a.s.
−→0
(6.55)

6.2. Techniques for deterministic equivalents
171
for all i ∈{1, . . . , N}. Write the following diﬀerences
fi −ei =
K
X
j=1
(¯ej −¯fj) 1
N tr Ri
" K
X
k=1
¯ekRk −zIN
#−1
Rj
" K
X
k=1
¯fkRk −zIN
#−1
+ εi
N
¯ei −¯fi = 1
N
ni
X
l=1
t2
i,l(fi −ei) −ti,l

fi ¯fi −ei¯ei

(1 + ti,lei −¯eiei)(1 + ti,lfi −¯fifi)
and
fi ¯fi −ei¯ei = ¯fi(fi −ei) + ei( ¯fi −¯ei).
For notational convenience, we deﬁne the following values
α ≜sup
i
E

|fi −ei|4
¯α ≜sup
i
E

| ¯fi −¯ei|4
.
It is thus suﬃcient to show that α is summable to prove (6.55). By applying
(6.54) to the absolute of the ﬁrst diﬀerence, we obtain
|fi −ei| ≤KR2
|z|2 sup
i
| ¯fi −¯ei| + sup
i
|εi
N|
and hence
α ≤8K4R8
|z|8
¯α + 8C
N 2
(6.56)
for some constant C > 0 such that E[| supi εi
N|4] ≤C/N 2. This is possible since
E[| supi εi
N|4] ≤8K supi E[|εi
N|4] and E[|εi
N|4] has been proved to be of order
O(1/N 2). Similarly, we have for the third diﬀerence
|fi ¯fi −ei¯ei| ≤| ¯fi||fi −ei| + |ei|| ¯fi −¯ei|
≤
T
1 −c sup
i
|fi −ei| + R
|z| sup
i
| ¯fi −¯ei|
with c an upper bound on maxi lim supn ci, known to be inferior to one. This
result can be used to upper bound the second diﬀerence term, which writes
| ¯fi −¯ei| ≤
1
(1 −c)2

T 2 sup
i
|fi −ei| + T|fi ¯fi −ei¯ei|

≤
1
(1 −c)2

T 2 sup
i
|fi −ei| + T

T
1 −c sup
i
|fi −ei| + R
|z| sup
i
| ¯fi −¯ei|

= T 2(2 −c)
(1 −c)3 sup
i
|fi −ei| +
RT
|z|(1 −c)2 sup
i
| ¯fi −¯ei|.
Hence
¯α ≤8T 8(2 −c)4
(1 −c)12
α +
8R4T 4
|z|4(1 −c)8 ¯α.
(6.57)

172
6. Deterministic equivalents
For a suitable z, satisfying |z| >
2RT
(1−c)2 , we have
8R4T 4
|z|4(1−c)8 < 1/2 and, thus,
moving all terms proportional to α on the left
¯α < 16T 8(2 −c)4
(1 −c)12
α.
Plugging this result into (6.56) yields
α ≤128K4R8T 8(2 −c)4
|z|8(1 −c)12
α + 8C
N 2 .
Take
0 < ε < 1.
It
is
easy
to
check
that,
for
|z| >
1281/8RT√
K(2−c)
(1−c)3/2(1−ε)1/8 ,
128K4R8T 8(2−c)4
|z|8(1−c)12
< 1 −ε and thus
α < 8C
εN 2 .
(6.58)
Since C does not depend on N, α is clearly summable which, along with the
Markov inequality and the Borel–Cantelli lemma, concludes the proof.
Finally, taking the same steps as above, we also have
E
h
|mBN (z) −mN(z)|4i
≤8C
εN 2
for some |z| large enough. The same conclusion therefore holds: for these
z, mBN (z) −mN(z)
a.s.
−→0. From Vitali convergence theorem, since fi and ei
are uniformly bounded on all closed sets of C \ R+, we ﬁnally have that the
convergence is true for all z ∈C \ R+. The almost sure convergence of the
Stieltjes transform implies the almost sure weak convergence of F BN −FN to
zero, which is our ﬁnal result.
As a (not immediate) corollary of the proof above, we have the following result,
important for application purposes, see Section 12.2.
Theorem 6.19. Under the assumptions of Theorem 6.17 with Ti diagonal for
all i, denoting wij the jth column of Wi, tij the jth diagonal entry of Ti, and
z ∈C \ R+
wH
ijHH
i
 BN −tijHiwijwH
ijHH
i −zIN
−1 Hiwij −
ei(z)
¯ci −ei(z)¯ei(z)
a.s.
−→0. (6.59)
where ei(z) and ¯ei(z) are deﬁned in Theorem 6.17.
Similar to the i.i.d. case, a deterministic equivalent for the Shannon transform
can be derived. This is given by the following proposition.
Theorem 6.20. Under the assumptions of Theorem 6.17 with z = −1/x, for
x > 0, denoting
VBN (x) = 1
N log det (xBN + IN)

6.2. Techniques for deterministic equivalents
173
the Shannon transform of BN, we have:
VBN (x) −VN(x)
a.s.
−→0
where
VN(x) = 1
N log det
 
IN + x
K
X
i=1
¯eiRi
!
+
K
X
i=1
1
N log det ([¯ci −ei¯ei]Ini + eiTi)
+
K
X
i=1
[(1 −ci) log(¯ci −ei¯ei) −¯ci log(¯ci)] .
(6.60)
The proof for the deterministic equivalent of the Shannon transform follows
from similar considerations as for the i.i.d. case, see Theorem 6.4 and Corollary
6.1, and is detailed below.
Proof. For the proof of Theorem 6.20, we again take ¯ci = 1, Ri deterministic of
bounded spectral norm for simplicity and we assume ci ≤1 from the beginning,
the trace lemma, Theorem 6.15, being unused here. First note that the system of
Equations (6.39) is unchanged if we extend the Ti matrices into N × N diagonal
matrices ﬁlled with N −ni zero eigenvalues. Therefore, we can assume that all
Ti have size N × N, although we restrict the F Ti to have a mass 1 −ci in zero.
Since this does not alter the Equations (6.39), we have in particular ¯ei < 1/ei.
This being said, (6.60) now needs to be rewritten
VN(x) = 1
N log det
 
IN + x
K
X
i=1
¯eiRi
!
+
K
X
i=1
1
N log det ([1 −ei¯ei]IN + eiTi) .
Calling V the function
V : (x1, . . . , xK, ¯x1, . . . , ¯xK, x) 7→1
N log det
 
IN + x
K
X
i=1
¯xiRi
!
+
K
X
i=1
1
N log det ([1 −xi¯xi]IN + xiTi)
we have:
∂V
∂xi
(e1, . . . , eK, ¯e1, . . . , ¯eK, x) = ¯ei −¯ei
1
N
N
X
l=1
1
1 −ei¯ei + eitil
∂V
∂¯xi
(e1, . . . , eK, ¯e1, . . . , ¯eK, x) = ei −ei
1
N
N
X
l=1
1
1 −ei¯ei + eitil
.
Noticing now that
1 = 1
N
N
X
l=1
1 −ei¯ei + eitil
1 −ei¯ei + eitil
= (1 −ei¯ei) 1
N
N
X
l=1
1
1 −ei¯ei + eitil
+ ei¯ei

174
6. Deterministic equivalents
we have:
(1 −ei¯ei)
 
1 −1
N
N
X
l=1
1
1 −ei¯ei + eitil
!
= 0.
But we also know that 0 ≤¯ei < 1/ei and therefore 1 −ei¯ei > 0. This entails
1
N
N
X
l=1
1
1 −ei¯ei + eitil
= 1.
(6.61)
From (6.61), we conclude that
∂V
∂xi
(e1, . . . , eK, ¯e1, . . . , ¯eK, x) = 0
∂V
∂¯xi
(e1, . . . , eK, ¯e1, . . . , ¯eK, x) = 0.
We therefore have that
d
dxVN(x) =
K
X
i=1
∂V
∂ei
∂ei
∂x + ∂V
∂¯ei
∂¯ei
∂x

+ ∂V
∂x
= ∂V
∂x
=
K
X
i=1
¯ei
1
N tr Ri

IN + x
K
X
j=1
¯ejRj


−1
= 1
x −1
x2
1
N tr

1
xIN +
K
X
j=1
¯ejRj


−1
.
Therefore, along with the fact that VN(0) = 0, we have:
VN(x) =
Z x
0
1
t −1
t2 mN

−1
t

dt
and therefore VN(x) is the Shannon transform of FN, according to Deﬁnition
3.2.
In order to prove the almost sure convergence VBN (x) −VN(x)
a.s.
−→0, we need
simply to notice that the support of the eigenvalues of BN is bounded. Indeed, the
non-zero eigenvalues of WiWH
i have unit modulus and therefore ∥BN∥≤KTR.
Similarly, the support of FN is the support of the eigenvalues of PK
i=1 ¯eiRi,
which are bounded by KTR as well.
As a consequence, for B1, B2, . . . a realization for which F BN −FN ⇒0, we
have, from the dominated convergence theorem, Theorem 6.3
Z ∞
0
log (1 + xt) d[F BN −FN](t) →0.
Hence the almost sure convergence.

6.3. A central limit theorem
175
Applications of the above results are found in various telecommunication
systems employing random isometric precoders, such as random CDMA, SDMA
[Couillet et al., 2011b]. A speciﬁc application to assess the optimal number
of stream transmissions in multi-antenna interference channels is in particular
provided in [Hoydis et al., 2011a], where an extension of Theorem 6.17 to
correlated i.i.d. channel matrices Hi is provided. It is worth mentioning that
the approach followed in [Hoydis et al., 2011a] to prove this extension relies
on an “inclusion” of the deterministic equivalent of Theorem 6.12 into the
deterministic equivalent of Theorem 6.17. The ﬁnal result takes a surprisingly
simple expression and the proof of existence, uniqueness, and convergence of
the implicit equations obtained do not require much eﬀort. This “deterministic
equivalent of a deterministic equivalent” approach is very natural and is expected
to lead to very simple results even for intricate communication models; recall e.g.
Theorem 6.9.
We conclude this chapter on deterministic equivalents by a central limit
theorem for the Shannon transform of the non-centered random matrix with
variance proﬁle of Theorem 6.14.
6.3
A central limit theorem
Central limit theorems are also demanded for more general models than the
sample covariance matrix of Theorem 3.17. In wireless communications, it
is particularly interesting to study the limiting distribution of the Shannon
transform of doubly correlated random matrices, e.g. to mimic Kronecker models,
or even more generally matrices of i.i.d. entries with a variance proﬁle. Indeed,
the later allows us to study, in addition to the large dimensional ergodic capacity
of Rician MIMO channels, as provided by Theorem 6.14, the large dimensional
outage mutual information of such channels. In [Hachem et al., 2008b], Hachem
et al. provide the central limit theorem for the Shannon transform of this model.
Theorem 6.21 ([Hachem et al., 2008b]). Let YN be N × n whose (i, j)th entry
is given by:
YN,ij = σij(n)
√n XN,ij
with {σij(n)}ij uniformly bounded with respect to n, and XN,ij is the (i, j)th
entry of an N × n matrix XN with i.i.d. entries of zero mean, unit variance, and
ﬁnite eighth order moment. Denote BN = YNYH
N. We then have, as N, n →∞
with limit ratio c = limN N/n, that the Shannon transform
VBN (x) ≜1
N log det(IN + xBN)

176
6. Deterministic equivalents
of BN satisﬁes
N
θn
(VBN (x) −E[VBN (x)]) ⇒X ∼N(0, 1)
with
θ2
n = −log det(In −Jn) + κ tr(Jn)
κ = E[(XN,11)4] −3E[(XN,11)3] for real XN,11, κ = E[|XN,11|4] −2E[|XN,11|2]
for complex XN,11, and Jn the matrix with (i, j)th entry
Jn,ij = 1
n
1
n
PN
k=1 σ2
ki(n)σ2
kj(n)tk(−1/x)2

1 + 1
n
PN
k=1 σ2
ki(n)tk(−1/x)
2
with ti(z) such that (t1(z), . . . , tN(z)) is the unique Stieltjes transform vector
solution of
ti(z) =

−z + 1
n
n
X
j=1
σ2
ij(n)
1 + 1
n
PN
l=1 σ2
lj(n)tl(z)


−1
.
Observe that the matrix Jn is in fact the Jacobian matrix associated with the
fundamental equations in the eN,i(z), deﬁned in the implicit relations (6.29) of
Theorem 6.10 as
eN,i(z) = 1
n
N
X
k=1
σ2
ki(n)tk(z) = 1
n
N
X
k=1
σ2
ki(n)
1
−z + 1
n
Pn
l=1
σ2
kl(n)
1+eN,l(z)
.
Indeed, for all eN,k(−1/x) ﬁxed but eN,j(−1/x), we have:
∂
∂eN,j(−1/x)

1
n
N
X
k=1
σ2
ki(n)
1
1
x + 1
n
Pn
l=1
σ2
kl(n)
1+eN,l(−1/x)


= 1
n
N
X
k=1
σ2
ki(n)
1
nσ2
kj(n)
(1 + eN,j(−1/x))2
1

1
x + 1
n
Pn
l=1
σ2
kl(n)
1+eN,l(−1/x)
2
= 1
n
N
X
k=1
1
nσ2
ki(n)σ2
kj(n)tk(−1/x)2
(1 + eN,j(−1/x))2
= Jn,ji.
So far, this observation seems to generalize to all central limits derived
for random matrix models with independent entries. This is however only an
intriguing but yet unproven fact.
Similar to Theorem 3.17, [Hachem et al., 2008b] provides more than an
asymptotic central limit theorem for the Shannon transform of the information
plus noise model VBN −E[VBN ], but also the ﬂuctuations for N large of the
diﬀerence between VBN and its deterministic equivalent VN, provided in [Hachem

6.3. A central limit theorem
177
et al., 2007]. In the case where XN has Gaussian entries, this takes a very compact
expression.
Theorem 6.22. Under the conditions of Theorem 6.21 with the additional
assumption that the entries of XN are complex Gaussian, we have:
N
p
−log det (In −Jn)
(VBN (x) −VN(x)) ⇒X ∼N(0, 1)
where VN is deﬁned as
VN(x) = 1
N
N
X
i=1
log

x
ti(−1/x)

+ 1
N
n
X
j=1
log
 
1 + 1
n
N
X
l=1
σ2
lj(n)tl(−1/x)
!
−
1
Nn
X
1≤i≤N
1≤j≤n
σ2
ij(n)ti(−1/x)
1 + 1
n
PN
l=1 σ2
lj(n)tl(−1/x)
with t1, . . . , tN and Jn deﬁned as in Theorem 6.21.
The generalization to distributions of the entries of XN with a non-zero
kurtosis κ introduces an additional bias term corresponding to the limiting
variations of N(E[VBN (x)] −VN(x)). This converges instead to zero in the
Gaussian case or, as a matter of fact, in the case of any distribution with null
kurtosis.
This concludes this short section on central limit theorems for deterministic
equivalents.
This also closes this chapter on the classical techniques used for deterministic
equivalents, when there exists no limit to the e.s.d. of the random matrix under
study. Those deterministic equivalents are seen today as one of the most powerful
tools to evaluate the performance of large wireless communication systems
encompassing multiple antennas, multiple users, multiple cells, random codes,
fast fading channels, etc. which are studied with scrutiny in Part II. In order
to study complicated system models involving e.g. doubly-scattering channels,
multi-hop channels, random precoders in random channels, etc., the current trend
is to study nested deterministic equivalents; that is, deterministic equivalents
that account for the stochasticity of multiple independent random matrices, see
e.g. Hoydis et al. [2011a,b].
In the following, we turn to a rather diﬀerent subject and study more deeply
the limiting spectra of the sample covariance matrix model and of the information
plus noise model. For these, much more than limiting spectral densities is
known. It has especially been proved that, under some mild conditions, the
extreme eigenvalues for both models do not escape the support of the l.s.d.
and that a precise characterization of the position of some eigenvalues can be
determined. Some additional study will characterize precisely the links between
the population covariance matrix (or the information matrix) and the sample
covariance matrix (or the information plus noise matrix), which are fundamental

178
6. Deterministic equivalents
to address the questions of inverse problems and more precisely statistical eigen-
inference for large dimensional random matrix models. These questions are at
the core of the very recent signal processing tools, which enable novel signal
sensing techniques and (N, n)-consistent estimation procedures adapted to large
dimensional networks.

7
Spectrum analysis
In this chapter, we further study the spectra of the important random matrix
models for wireless communications that are the sample covariance matrix and
the information plus noise models. It has already been shown in Chapter 3
that, as the e.s.d. of the population covariance matrix (or of the information
matrix) converges, the e.s.d. of the sample covariance matrix (or the information
plus noise matrix) converges almost surely. The limiting d.f. can then be fully
characterized as a function of the l.s.d. of the population covariance matrix (or of
the information matrix). It is however not convenient to invert the problem and
to describe the l.s.d. of the population covariance matrix (or of the information
matrix) as a function of the l.s.d. of the observed matrices. The answer to this
inverse problem is provided in Chapter 8, which however requires some eﬀort
to be fully accessible. The development of the tools necessary for the statistical
eigen-inference methods of Chapter 8 is one of the motivations of the current
chapter.
The starting motivation, initiated by the work of Silverstein and Choi
[Silverstein and Choi, 1995], which resulted in the important Theorem 7.4
(accompanied later by an important corollary, due to Mestre [Mestre, 2008a],
Theorem 7.5), was to characterize the l.s.d. of the sample covariance matrix in
closed-form. Remember that, up to this point, we can only characterize the l.s.d.
F of a sample covariance matrix through the expression of its Stieltjes transform,
as the unique solution mF (z) of some ﬁxed-point equation for all z ∈C \ R+.
To obtain an explicit expression of F, it therefore suﬃces to use the inverse
Stieltjes transform formula (3.2). However, this suggests having a closer look at
the limiting behavior of mF (z) as z approaches the positive real half-line, about
which we do not know much yet. Therefore, up to this point in our analysis, it
is impossible to describe the support of the l.s.d., apart from rough estimations
based on the expression of ℑ[mF (z)], for z = x + iy, y being small. It is also
not convenient to depict F ′(x): the solution is to take z = x + iy, with y small
and x spanning from zero to inﬁnity, and to draw the curve z 7→1
πℑ[mF (z)] for
such z. In the following, we will show that, as z tends to x > 0, mF (z) has a
limit which can be characterized in two diﬀerent ways, depending on whether x
belongs to the support of F or not. In any case, this limit is also characterized as
the solution to an implicit equation, although particular care must be taken as to
which of the multiple solutions of this implicit equation needs to be considered.

180
7. Spectrum analysis
Before we detail this advanced spectrum characterization, we provide a
diﬀerent set of results, fundamental to the validation of the eigen-inference
methods proposed in Chapter 8. These results, namely the asymptotic absence
of eigenvalues outside the support of F, Theorem 7.1, and the exact separation
of the support into disjoints clusters, Theorem 7.2, are once more due to Bai
and Silverstein [Bai and Silverstein, 1998, 1999]. Their object is the analysis, on
top of the characterization of F, of the behavior of the particular eigenvalues
of the e.s.d. of the sample covariance matrix as the dimensions grow large.
It is fundamental to understand here, and this will be reminded again in the
next section, that the convergence of the e.s.d. toward F, as the matrix size
N grows large, does not imply the convergence of the largest eigenvalue of
the sample covariance matrix towards the right edge of the support. Indeed,
the largest eigenvalues, having weight 1/N in the spectrum, do not contribute
asymptotically to the support of F. As such, it may well be found outside the
support of F for all ﬁnite N, without invalidating Theorem 3.13. This particular
case in the Mar˘cenko–Pastur model where eigenvalues are found outside the
support almost surely when the entries of the random i.i.d. matrix XN in
Theorem 3.13, TN = IN, have inﬁnite fourth order moment. In this scenario,
it is even proved in [Silverstein et al., 1988] that the largest eigenvalue grows
without bound as the system dimensions grow to inﬁnity, while all the mass
of the l.s.d. is asymptotically kept in the support; if the fourth order moment
is ﬁnite. Under ﬁnite fourth moment assumption though [Bai and Silverstein,
1998; Yin et al., 1988], the important result to be detailed below is that no
eigenvalue is to be found outside the limiting support and that the eigenvalues
are found where they ought to be. This last statement is in fact slightly erroneous
and will be adequately corrected when discussing the spiked models that lead
some eigenvalues to leave the limiting support. To be more precise, when the
moment of order four of the entries of XN exists, we can characterize exactly
the subsets of R+ where no eigenvalue is asymptotically found, almost surely.
Further discussions on the extreme eigenvalues of sample covariance matrices are
provided in Chapter 9, where (non-central) limiting theorems for the distribution
of these eigenvalues are provided.
7.1
Sample covariance matrix
7.1.1
No eigenvalues outside the support
As observed in the previous sections, most early results of random matrix theory
dealt with the limiting behavior of e.s.d. For instance, the Mar˘cenko–Pastur
law ensures that the e.s.d. of the sample covariance matrix RN of vectors with
i.i.d. entries of zero mean and unit variance converges almost surely towards
a limit distribution function F. However, the Mar˘cenko–Pastur law does not
say anything about the behavior of any speciﬁc eigenvalue, say for instance the

7.1. Sample covariance matrix
181
extreme lowest and largest eigenvalues λmin and λmax of RN. It is relevant in
particular to wonder whether λmin and λmax can be asymptotically found outside
the support of F. Indeed, if all eigenvalues but the extreme two are in the support
of F, then the l.s.d. of RN is still F, which is still consistent with the Mar˘cenko–
Pastur law. It turns out that this is not the case in general. Under some mild
assumption on the entries of the sample covariance matrix, no eigenvalue is found
outside the support. We speciﬁcally have the following theorem.
Theorem 7.1 ([Bai and Silverstein, 1998; Yin et al., 1988]). Let the matrix
XN =

1
√nXN,ij

∈CN×n have i.i.d. entries, such that XN,11 has zero mean,
unit variance, and ﬁnite fourth order moment. Let TN ∈CN×N be non-random,
with uniformly bounded spectral norm ∥TN∥, whose e.s.d. F TN converge weakly
to H. From Theorem 3.13, the e.s.d. of BN = T
1
2
NXNXH
NT
1
2
N ∈CN×N converges
weakly and almost surely towards some distribution function F, as N, n go
to inﬁnity with ratio cN = N/n →c, 0 < c < ∞. Similarly, the e.s.d. of BN =
XH
NTNXN ∈Cn×n converges towards F given by:
F(x) = cF(x) + (1 −c)1[0,∞)(x).
Denote F N the distribution with Stieltjes transform mF N (z), which is solution,
for z ∈C+, of the following equation in m
m = −

z −N
n
Z
τ
1 + τmdF TN (τ)
−1
(7.1)
and deﬁne FN the d.f. such that
F N(x) = N
n FN(x) +

1 −N
n

1[0,∞)(x).
Let N0 ∈N, and choose an interval [a, b], a, b ∈(0, ∞], lying in an open interval
outside the union of the supports of F and FN for all N ≥N0. For ω ∈Ω,
the random space generating the series X1, X2, . . ., denote LN(ω) the set of
eigenvalues of BN(ω). Then
P({ω, LN(ω) ∩[a, b] ̸= ∅i.o.}) = 0.
This means concretely that, given a segment [a, b] outside the union of the
supports of F and FN0, FN0+1, . . ., for all series B1(ω), B2(ω), . . ., with ω in
some set of probability one, there exists M(ω) such that, for all N ≥M(ω),
there will be no eigenvalue of BN(ω) in [a, b]. By deﬁnition, FK is the l.s.d.
of an hypothetical BN with H = F TK. The necessity to consider the supports
of FN0, FN0+1, . . . is essential when a few eigenvalues of TN are isolated and
eventually contribute with probability zero to the l.s.d. H. Indeed, it is rather
intuitive that, if the largest eigenvalue of TN is large compared to the rest,
at least one eigenvalue of BN will also be large compared to the rest (take
n ≫N to be convinced). Theorem 7.1 states exactly here that there will be

182
7. Spectrum analysis
neither any eigenvalue outside the support of the main mass of F BN , nor any
eigenvalue around the largest one. Those models in which some eigenvalues of TN
are isolated are referred to as spiked models. These are thoroughly discussed in
Chapter 9. In wireless communications and modern signal processing, Theorem
7.1 is of key importance for signal sensing and hypothesis testing methods since it
allows us to verify whether the eigenvalues empirically found in sample covariance
matrix spectra originate either from noise contributions or from signal sources.
In the simple case where signals sensed at an antenna array originate either from
white noise or from a coherent signal source impaired by white noise, this can be
performed by simply verifying if the extreme eigenvalue of the sample covariance
matrix is inside or outside the support of the Mar˘cenko–Pastur law (Figure 1.1);
see further Chapter 16.
We give hereafter a sketch of the proof, which again only involves the Stieltjes
transform.
Proof. Surprisingly, the proof unfolds from a mere (though non-trivial)
reﬁnement of the Stieltjes transform relation proved in Theorem 3.13. Let FN
be deﬁned as above and let mN be its Stieltjes transform. It is possible to show
that, for z = x + ivN, with vN = N −1/68
sup
x∈[a,b]
|mBN (z) −mN(z)| = o
 1
N vN

almost surely. This result is in fact also true when ℑ[z] equals
√
2vN,
√
3vN, . . .
or
√
34vN. Note that this reﬁnes the known statement that the diﬀerence is of
order o(1). We take this property, which requires more than ten pages of calculus,
for granted. We now have that
max
1≤k≤34 sup
x∈[a,b]
mBN (x + ik
1
2 vN) −mN(x + ik
1
2 vN)
 = o(v67
N )
almost surely. Expanding the Stieltjes transforms and considering only the
imaginary parts, we obtain
max
1≤k≤34 sup
x∈[a,b]

Z d(F BN (λ) −FN(λ))
(x −λ)2 + kv2
N
 = o(v66
N )
almost surely. Taking successive diﬀerences over the 34 values of k, we end up
with
sup
x∈[a,b]

Z (v2
N)33d(F BN (λ) −FN(λ))
Q34
k=1((x −λ)2 + kv2
N)
 = o(v66
N )
(7.2)
almost surely, from which the term v66
N simpliﬁes on both sides. Consider now
a′ < a and b′ > b such that [a′, b′] is outside the support of F. We then divide

7.1. Sample covariance matrix
183
(7.2) into two terms, as (remember that 1/N = v68
N )
sup
x∈[a,b]

Z 1R+\[a′,b′](λ)d(F BN (λ) −FN(λ))
Q34
k=1((x −λ)2 + kv2
N)
+
X
λj∈[a′,b′]
v68
N
Q34
k=1((x −λj)2 + kv2
N)

= o(1)
almost surely. Assume now that, for a subsequence φ(1), φ(2), . . . of 1, 2, . . ., there
always exists at least one eigenvalue of Bφ(N) in [a, b]. Then, for x taken equal
to this eigenvalue, one term of the discrete sum above (whose summands are
all non-negative) is exactly 1/34!, which is uniformly bounded away from zero.
This implies that the integral must also be bounded away from zero. However
the integrand of the integral is clearly uniformly bounded on [a′, b′] and, from
Theorem 3.13, F BN −F ⇒0. Therefore the integral tends to zero as N →∞.
This is a contradiction. Therefore, the probability that there is an eigenvalue
of BN in [a, b] inﬁnitely often is null. Now, from [Yin et al., 1988], the largest
eigenvalue of 1
nXNXH
N is almost surely asymptotically bounded. Therefore, since
∥TN∥is also bounded by hypothesis, the theorem applies also to b = ∞.
Note that the ﬁniteness of the fourth order moment of the entries XN,ij is
fundamental for the validity of Theorem 7.1. It is indeed proved in [Yin et al.,
1988] and [Silverstein et al., 1988] that:
• if the entries XN,ij have ﬁnite fourth order moment, with probability one, the
largest eigenvalue of XNXH
N tends to the edge (1 + √c)2, c = limN N/n of
the support of the Mar˘cenko–Pastur law, which is an immediate corollary of
Theorem 7.1 with TN = IN;
• if the entries XN,ij do not have a ﬁnite fourth order moment then, with
probability one, the limit superior of the largest eigenvalue of XNXH
N is
inﬁnite, i.e. with probability one, for all A > 0, there exists N such that the
largest eigenvalue of XNXH
N is larger than A. It is therefore important never
to forget the underlying assumption made on the tails of the distribution of
the entries in XN.
We now move to an extension of Theorem 7.1.
7.1.2
Exact spectrum separation
Now assume that the e.s.d. of TN converges to the distribution function of,
say, three evenly weighted masses in λ1, λ2, and λ3. For not-too-large ratios
cN = N/n, it is observed that the support of F is divided into up to three
clusters of eigenvalues. In particular, when n becomes large while N is kept
ﬁxed, the clusters consist of three punctual masses in λ1, λ2, and λ3, as required
by classical probability theory. This is illustrated in Figure 7.1 in the case of a
three-fold clustered and a two-fold clustered support of F. The reason why we
observe sometimes three and sometimes less clusters is linked to the spreading

184
7. Spectrum analysis
of each cluster due to the limiting ratio c; the smaller c, the thinner the clusters,
as already observed in the simple case of the Mar˘cenko–Pastur law, Figure 2.2.
Considering Theorem 7.1, it is tempting to assume that, in addition to each
cluster of F being composed of one third of the total spectrum mass, each cluster
of BN contains exactly one third of the eigenvalues of BN. However, Theorem
7.1 only ensures that no eigenvalue is found outside the support of F for all
N larger than a given M, and does not say how the eigenvalues of BN are
distributed in the various clusters. The answer to this question is provided in
[Bai and Silverstein, 1999] in which the exact separation properties of the l.s.d.
of such matrices BN is discussed.
Theorem 7.2 ([Bai and Silverstein, 1999]). Assume the hypothesis of Theorem
7.1 with TN non-negative deﬁnite. Consider similarly 0 < a < b < ∞such that
[a, b] lies in an open interval outside the support of F and FN for all large N.
Denote additionally λk and τk the kth eigenvalues of BN and TN in decreasing
order, respectively. Then we have:
1. If c(1 −H(0)) > 1, then the smallest value x0 in the support of F is positive
and λN →x0 almost surely, as N →∞.
2. If c(1 −H(0)) ≤1, or c(1 −H(0)) > 1 but [a, b] is not contained in [0, x0],
then1
P(λiN > b, λiN+1 < a for all large N) = 1
where iN is the unique integer such that
τiN > −1/mF (b),
τiN+1 < −1/mF (a).
Theorem 7.2 ensures in particular the exact separation of the spectrum when
τ1, . . . , τN take values in a ﬁnite set. Consider for instance the ﬁrst plot in
Figure 7.1 and an interval [a, b] comprised between the second and third clusters.
What Theorem 7.2 claims is that, if iN and iN + 1 are the indexes of the right
and left eigenvalues when F BN jumps from one cluster to the next, and N is
large enough, then there is an associated jump from the corresponding iNth and
(iN + 1)th eigenvalues of TN (for instance, at the position of the discontinuity
from eigenvalue 7 to eigenvalue 3).
This bears some importance for signal detection. Indeed, consider the problem
of the transmission of information plus noise. Given the dimension p of the signal
1 The expression “P (AN for all large N) = 1” is used in place of “there exists B ⊂Ω, with
P(B) = 1, such that, for ω ∈B, there exists N0(ω) for which N > N0(ω) implies ω ∈AN.” It
is particularly important to note that “for all large N” is somewhat misleading as it does not
indicate the existence of a universal N0 such that N > N0 implies ω ∈AN for all ω ∈B, but
rather the existence of an N0(ω) for each such ω. Here, AN = {ω, λiN (ω) > b, λiN+1(ω) < a}
and the space Ωis the generator of the series B1(ω), B2(ω), . . ..

7.1. Sample covariance matrix
185
1
3
7
0
0.2
0.4
0.6
Eigenvalues
Density
Empirical eigenvalue distribution
Limit law (from Theorem 3.13)
1
3
4
0
0.2
0.4
0.6
Eigenvalues
Density
Empirical eigenvalue distribution
Limit law (from Theorem 3.13)
Figure 7.1 Histogram of the eigenvalues of BN = T
1
2
NXNXH
NT
1
2
N, N = 300, n = 3000,
with TN diagonal composed of three evenly weighted masses in (i) 1, 3, and 7 on top,
(ii) 1, 3, and 4 on the bottom.
space and n −p of the noise space, for large c, Theorem 7.2 allows us to isolate
the eigenvalues corresponding to the signal space from those corresponding to
the noise space. If both eigenvalue spaces are isolated in two distinct clusters,
then we can exactly determine the dimension of each space and infer, e.g. the
number of transmitting entities. The next question that then naturally arises is
to determine for which values of c = limN n/N the support of F separates into
1, 2, or more clusters.

186
7. Spectrum analysis
7.1.3
Asymptotic spectrum analysis
For better understanding in the following, we will take the convention that the
(hypothetical) single mass at zero in the spectrum of F is not considered as
a ‘cluster’. We will number the successive clusters from left to right, from one
to KF with KF the number of clusters in F, and we will denote kF the cluster
generated by the population eigenvalue tk, to be introduced shortly. For instance,
if two sample eigenvalues ti and ti+1 ̸= ti generate a unique cluster in F (as in the
bottom graph in Figure 7.1, where t2 = 3 and t3 = 4 generate the same cluster),
then iF = (i + 1)F ). The results to come will provide a unique way to deﬁne
kF mathematically and not only visually. To this end, we need to study in more
depth the properties of the limiting spectrum F of the sample covariance matrix.
Remember ﬁrst that, for the model BN = XH
NTNXN ∈Cn×n of l.s.d. F, where
XN ∈CN×n has i.i.d. entries of zero mean and variance 1/n, TN has l.s.d. H
and N/n →c, mF (z), z ∈C+, Equation (3.22) has an inverse formula, given by:
zF (m) = −1
m + c
Z
t
1 + tmdH(t)
(7.3)
for m ∈C+. The equation zF (m) = z ∈C+ has a unique solution m with positive
imaginary part and this solution equals mF (z) by Theorem 3.13. Of course, BN
and BN only diﬀer from |N −n| zero eigenvalues, so it is equivalent to study the
l.s.d. of BN or that of BN. The link between their respective Stieltjes transforms
is given by:
mF (z) = cmF (z) + (c −1)1
z
from (3.16). Since F turns out to be simpler to study, we will focus on BN
instead of the sample covariance matrix BN itself.
Now, according to the Stieltjes inversion formula (3.2), for every continuity
points a, b of F
F(b) −F(a) = lim
y→0+
1
π
Z b
a
ℑ[mF (x + iy)]dx.
To determine the distribution F, and therefore the distribution F, we must
determine the limit of mF (z) as z ∈C+ tends to x ∈R∗. It can in fact be shown
that this limit exists.
Theorem 7.3 ([Silverstein and Choi, 1995]). Let BN ∈Cn×n be deﬁned as
previously, with almost sure l.s.d. F. Then, for x ∈R∗
lim
z→x
z∈C+
mF (z) ≜m◦(x)
(7.4)
exists and the function m◦is continuous on R∗. For x in the support of F, the
density f(x) ≜F ′(x) equals
1
πℑ[m◦(x)]. Moreover, f is analytic for all x ∈R∗
such that f(x) > 0.

7.1. Sample covariance matrix
187
The study of m◦makes it therefore possible to describe the complete support
SF of F as well as the limiting density f. Since SF equals SF but for an additional
mass in zero, this is equivalent to determining the support of SF . Choi and
Silverstein provided an accurate description of the function m◦, as follows.
Theorem 7.4 ([Silverstein and Choi, 1995]). Let B = {m | m ̸= 0, −1/m ∈Sc
H},
with Sc
H the complementary of SH, and xF be the function deﬁned on B by
xF (m) = −1
m + c
Z
t
1 + tmdH(t).
(7.5)
For x ∈R∗, we can determine the limit m◦(x) of mF (z) as z →x, z ∈C+, along
the following rules:
1. If x ∈SF , then m◦(x) is the unique solution in B with positive imaginary part
of the equation x = xF (m) in the dummy variable m.
2. If x ∈Sc
F , then m◦(x) is the unique real solution in B of the equation x =
xF (m) in the dummy variable m such that x′
F (m0) > 0. Conversely, for m ∈
B, if x′
F (m) > 0, then xF (m) ∈Sc
F .
From rule 1, along with Theorem 7.3, we can evaluate for every x > 0 the
limiting density f(x), hence F(x), by ﬁnding the complex solution with positive
imaginary part of x = xF (m).
Rule 2 makes it simple to determine analytically the exact support of F. It
indeed suﬃces to draw xF (m) for −1/m ∈Sc
H. Whenever xF is increasing on
an interval I, xF (I) is outside SF . The support SF of F, and therefore of F
(modulo the mass in zero), is then deﬁned exactly by the complementary set
SF = R \
[
a,b∈R
a<b
n
xF ((a, b)) | ∀m ∈(a, b), x′
F (m) > 0
o
.
This is depicted in Figure 7.2 in the case when H is composed of three evenly
weighted masses t1, t2, t3 in {1, 3, 5} or {1, 3, 10} and c = 1/10. Notice that, in
the case where t3 = 10, F is divided into three clusters, while, when t3 = 5, F is
divided into only two clusters, which is due to the fact that xF is non-increasing
in the interval (−1/3, −1/5). For applicative purposes, we will see in Chapter 17
that it might be essential that the consecutive clusters be disjoint. This is one
reason why Theorem 7.6 is so important.
We do not provide a rigorous proof of Theorem 7.4. In fact, while thoroughly
proved in 1995, this result was already intuited by Mar˘cenko and Pastur in
1967 [Mar˘cenko and Pastur, 1967]. The fact that xF (m) increases outside the
spectrum of F and is not increasing elsewhere is indeed very intuitive, and is not
actually limited to the sample covariance matrix case. Observe indeed that, for
any F, and any x0 ∈R∗outside the support of F, mF (x0) is clearly well deﬁned

188
7. Spectrum analysis
−1
−1
3
−1
10zero
1
3
10
m−
1
m+
1
m1
m
xF (m)
xF (m)
Support of F
−1
−1
3 −1
5
zero
1
3
5
m
xF (m)
xF (m)
Support of F
Figure 7.2 xF (m) for m real, TN diagonal composed of three evenly weighted masses
in 1, 3, and 10 (top) and 1, 3, and 5 (bottom), c = 1/10 in both cases. Local extrema
are marked in circles, inﬂexion points are marked in squares. The support of F can be
read on the right vertical axises.
and
m′
F (x0) =
Z
1
(λ −x0)2 dF(λ) > 0.
Therefore mF (x) is continuous and increasing on an open neighborhood of x0.
This implies that it is locally a one-to-one mapping on this neighborhood and
therefore admits an inverse xF (m), which is also continuous and increasing. This
explains why xF (m) increases when its image is outside the spectrum of F.
Now, if for some real m0, xF (m0) is continuous and increasing, then it is locally
invertible and its inverse ought to be mF (x), continuous and increasing, in which

7.1. Sample covariance matrix
189
case x is outside the spectrum of F. Obviously, this reasoning is far from being
a proof (at least the converse requires much more work).
From Figure 7.2 and Theorem 7.4, we now observe that, when the e.s.d. of
population matrix is composed of a few masses, x′
F (m) = 0 has exactly 2KF
solutions with KF the number of clusters in F. Denote these roots in increasing
order m−
1 < m+
1 ≤m−
2 < m+
2 < . . . ≤m−
KF < m+
KF . Each pair (m−
j , m+
j ) is such
that xF ([m−
j , m+
j ]) is the jth cluster in F. We therefore have a way to determine
the support of the asymptotic spectrum through the function x′
F . This is
presented in the following result.
Theorem 7.5 ([Couillet et al., 2011c; Mestre, 2008a]). Let BN ∈CN×N be
deﬁned as in Theorem 7.1. Then the support SF of the l.s.d. F of BN is
SF =
KF
[
j=1
[x−
j , x+
j ]
where x−
1 , x+
1 , . . . , x−
KF , x+
KF are deﬁned by
x−
j = −1
m−
j
+
K
X
r=1
cr
tr
1 + trm−
j
x+
j = −1
m−
j
+
K
X
r=1
cr
tr
1 + trm+
j
with m−
1 < m+
1 ≤m−
2 < m+
2 ≤. . . ≤m−
KF < m+
KF
the 2KF (possibly counted
with multiplicity) real roots of the equation in m
K
X
r=1
cr
t2
rm2
(1 + trm2)2 = 1.
Note further from Figure 7.2 that, while x′
F (m) might not have roots on some
intervals (−1/tk−1, −1/tk), it always has a unique inﬂexion point there. This is
proved in [Couillet et al., 2011c] by observing that x′′
F (m) = 0 is equivalent to
K
X
r=1
cr
t3
rm3
(1 + trm)3 −1 = 0
the left-hand side of which has always positive derivative and shows asymptotes
in the neighborhood of tr; hence the existence of a unique inﬂexion point on
every interval (−1/tk−1, −1/tk), for 1 ≤k ≤K, with convention t0 = 0+. When
xF increases on an interval (−1/tk−1, −1/tk), it must have its inﬂexion point
in a point of positive derivative (from the concavity change induced by the
asymptotes). Therefore, to verify that cluster kF is disjoint from clusters (k −1)F
and (k + 1)F (when they exist), it suﬃces to verify that the (k −1)th and kth
roots mk−1 and mk of x′′
F (m) are such that x′
F (mk−1) > 0 and x′
F (mk) > 0. From
this observation, we therefore have the following result.

190
7. Spectrum analysis
Theorem 7.6 ([Couillet et al., 2011c; Mestre, 2008b]). Let BN be deﬁned
as in Theorem 7.1, with TN = diag(τ1, . . . , τN) ∈RN×N, diagonal containing
K distinct eigenvalues 0 < t1 < . . . < tK, for some ﬁxed K. Denote Nk the
multiplicity of the kth largest distinct eigenvalue (assuming ordering of the τi,
we may then have τ1 = . . . = τN1 = t1, . . . , τN−NK+1 = . . . = τN = tK). Assume
also that, for all 1 ≤r ≤K, Nr/n →cr > 0, and N/n →c, with 0 < c < ∞.
Then the cluster kF associated with the eigenvalue tk in the l.s.d. F of BN is
distinct from the clusters (k −1)F and (k + 1)F (when they exist), associated
with tk−1 and tk+1 in F, respectively, if and only if
K
X
r=1
cr
t2
rm2
k
(1 + trm2
k)2 < 1
K
X
r=1
cr
t2
rm2
k+1
(1 + trm2
k+1)2 < 1
(7.6)
where m1, . . . , mK are such that mK+1 = 0 and m1 < m2 < . . . < mK are the K
solutions of the equation in m
K
X
r=1
cr
t3
rm3
(1 + trm)3 = 1.
For k = 1, this condition ensures 1F = 2F −1. For k = K, this ensures KF =
(K −1)F + 1. For 1 < k < K, this ensures (k −1)F + 1 = kF = (k + 1)F −1.
Remark now that the conditions of Equation (7.6) are left unchanged if all
t1, . . . , tK are scaled by a common constant. Indeed, if tj becomes αtj for all
j, then m1, . . . , mK become m1/α, . . . , mK/α and the scaling eﬀects cancel out
in Equation (7.6). Therefore, in the case K = 2, the separability condition only
depends on the ratios c1, c2 and on t1/t2. If c1 = c2 = c/2, then we can depict the
plot of the critical ratio 1/c as a function of t1/t2 for which cluster separability
happens. This is depicted in Figure 7.3. Since 1/c is the limit of the ratio n/N,
Figure 7.3 determines, for a ﬁxed observation size N, the limiting number of
samples per observation size required to achieve cluster separability. Observe
how steeply the plot of 1/c increases when t1 gets close to t2; this suggests
that the tools to be presented later that require this cluster separability will
be very ineﬃcient when it comes to separate close sources (the deﬁnition of
‘closeness’ depending on each speciﬁc study, e.g. close directions of signal arrivals
in radar applications, close transmit powers in signal sensing, etc.). Figure 7.4
depicts the regions of separability of all clusters in the case K = 3, for ﬁxed
c = 0.1, c1 = c2 = c3, as a function of the ratios t3/t1 and t2/t1. Observe that the
triplets (1, 3, 7) and (1, 3, 10) are well inside the separability region as suggested,
respectively, by Figure 7.1 (top) and Figure 7.2 (top); on the contrary, notice that
the triplets (1, 3, 4) and (1, 3, 5) are outside the separability region, conﬁrming
then the observations of Figure 7.1 (bottom) and Figure 7.2 (bottom).

7.1. Sample covariance matrix
191
0
0.2
0.4
0.6
0.8
0
20
40
60
80
100
cluster separability region
t1/t2
1/c
Figure 7.3 Limiting ratio c to ensure separability of (t1, t2), t1 ≤t2, K = 2, c1 = c2.
0
1
3
5
10
0
1
3
5
10
t2/t1
t3/t1
Figure 7.4 Subset of (t1, t2, t3) that satisfy cluster separability condition, c1 = c2 = c3,
c = 0.1, in crosshatched pattern.
After establishing these primary results for the sample covariance matrix
models, we now move to the information plus noise model. According to the
previous remark borrowed from Mar˘cenko and Pastur in [Mar˘cenko and Pastur,
1967], we infer that it will still be the case that the Stieltjes transform mF (x),
extended to the real axis, has a local inverse xF (m), which is continuous
and increasing, and that the range where xF (m) increases is exactly the
complementary to the support of F. This statement will be shown to be

192
7. Spectrum analysis
somewhat correct. The main diﬀerence with the sample covariance matrix
model is that there does not exist an explicit inverse xF (m), as in (7.5) and
therefore mF (x) may have various inverses xF (m) for diﬀerent subsets in the
complementary of the support of F.
7.2
Information plus noise model
The asymptotic absence of eigenvalues outside the support of unconstrained
information plus noise matrices (when the e.s.d. of the information matrix
converges), i.e. with i.i.d. noise matrix components, is still at the stage of
conjecture. While promising developments are being currently carried out, there
exists to this day no proof of this fact, let alone a proof of the exact separation
of information plus noise clusters. Nonetheless, in the particular case where the
noise matrix is Gaussian, the two results have been recently proved [Vallet et al.,
2010]. Those results are given hereafter.
7.2.1
Exact separation
We recall that an information plus noise matrix BN is deﬁned by
BN = 1
n(AN + σXN)(AN + σXN)H
(7.7)
where AN is deterministic, representing the deterministic signal, XN is random
and represents the noise matrix, and σ > 0.
We start by introducing the theorem which states that, for all large N, no
eigenvalue is found outside the asymptotic spectrum of the information plus
noise model.
Theorem 7.7. Let BN be deﬁned as in (7.7), with AN ∈CN×n such that HN ≜
F
1
n ANAH
N ⇒H and supN ∥1
nANAH
N∥< ∞, XN ∈CN×n with entries XN,ij
independent for all i, j, N, Gaussian with zero mean and unit variance. Further
denote cN = N/n and assume cN →c, positive and ﬁnite. From Theorem 3.15,
we know that F BN converges almost surely to a limit distribution F with Stieltjes
transform mF (z) solution of the equation in m
m
1 + σ2cNm = mH
 z(1 + σ2cNm)2 −σ2(1 −cN)(1 + σ2cNm)

(7.8)
this solution being unique for z ∈C+, m ∈C+ and ℑ[zm] ≥0. Denote now
mN(z) this solution when mH is replaced by mHN and FN the distribution
function with Stieltjes transform mN(z).
Let N0 ∈N, and choose an interval [a, b] outside the union of the supports of
F and FN for all N ≥N0. For ω ∈Ω, the probability space generating the noise

7.2. Information plus noise model
193
sequences X1, X2, . . ., denote LN(ω) the set of eigenvalues of BN(ω). Then
P(ω, LN(ω) ∩[a, b] ̸= ∅i.o.) = 0.
The next theorem ensures that the repartition of the eigenvalues in the
consecutive clusters is exactly as expected.
Theorem 7.8 ([Vallet et al., 2010]). Let BN be as in Theorem 7.7. Let a < b be
such that [a, b] lies outside the support of F. Denote λk and ak the kth eigenvalues
smallest of BN and 1
nANAH
N, respectively. Then we have:
1. If c(1 −H(0)) > 1, then the smallest eigenvalue x0 of the support of F is
positive and λN →x0 almost surely, as N →∞.
2. If c(1 −H(0)) ≤1, or c(1 −H(0)) > 1 but [a, b] is not contained in [0, x0],
then:
P(λiN > b, λiN+1 < a for all large N) = 1
where iN is the unique integer such that
τiN > −1/mF (b)
τiN+1 < −1/mF (a).
We provide hereafter a sketch of the proofs of both Theorem 7.7 and Theorem
7.8 where considerations of complex integration play a fundamental role. In the
following chapter, Chapter 8, we introduce in detail the methods of complex
integration for random matrix theory and particularly for statistical inference.
Proof of Theorem 7.7 and Theorem 7.8. As already mentioned, these results are
only known to hold for the Gaussian case for the time being. The way these
results are achieved is similar to the way Theorem 7.1 and Theorem 7.2 were
obtained, although the techniques are radically diﬀerent. Indeed, somewhat
similarly to Theorem 7.1, the ﬁrst objective is to show that the diﬀerence
mN(z) −E[mBN (z)] between the deterministic equivalent mN(z) of the empirical
Stieltjes transform mBN (z) and E[mBN (z)] goes to zero at a suﬃciently fast rate.
In the Gaussian case, this rate is of order O(1/N 2). Remember from Theorem 6.5
that such a convergence rate was already observed for doubly correlated Gaussian
models and allowed us to ensure that N(mN(z) −E[mBN (z)]) →0. Using the
fact, established precisely in Chapter 8, that, for holomorphic functions f and a
distribution function G
Z
f(x)dG(x) = −1
2πi
I
f(z)mG(z)dz
on a positively oriented contour encircling the support of F, we can infer the
recent result from [Haagerup et al., 2006]
E
Z
f(x)[F BN −FN](dx)

= O
 1
N 2

.

194
7. Spectrum analysis
Take f any inﬁnitely diﬀerentiable function that is identically one on [a, b] ⊂R
and identically zero outside (a −ε, b + ε) for some small positive ε, such that
(a −ε, b + ε) is outside the support of F. From the convergence rate above, we
ﬁrst have.
E
" N
X
k=1
λk1(a−ε,b+ε)(λk)
#
= N(FN(b) −FN(a)) + O
 1
N

and therefore, for large N, we have in expectation the correct mass of eigenvalues
in (a −ε, b + ε). But we obviously want more than that: i.e., we want to
determine the asymptotic exact number of these eigenvalues. Using the Nash–
Poincar´e inequality, Theorem 6.7, we can in fact show that, for this choice of
f
E
"Z
f(x)[F BN −FN](dx)
2#
= O
 1
N 4

.
This is enough to prove, thanks to the Markov inequality, Theorem 3.5, that
P

Z
f(x)[F BN −FN](dx)
 >
1
N
4
3

< K
N
4
3
for some constant K. From there, the Borel–Cantelli lemma, Theorem 3.6,
ensures that the above event is inﬁnitely often true with probability zero; i.e.
the event

N
X
k=1
λk1(a−ε,b+ε)(λk) −N(FN(b) −FN(a))
 > K
N
1
3
is inﬁnitely often true with probability zero. Therefore, with probability one,
there exists N0 such that, for N > N0 there is no eigenvalue in (a −ε, b + ε).
This proves the ﬁrst result.
Take now [a, b] not necessarily outside the support of F and ε such that (a −
ε, a) ∪(b, b + ε) is outside the support of F. Then, repeating the same procedure
as above but to characterize now

N
X
k=1
λk1[a,b](λk) −N(FN(b) −FN(a))

we ﬁnd that this term equals

N
X
k=1
λk1(a−ε,b+ε)(λk) −N(FN(b) −FN(a))

almost surely in the large N limit since there is asymptotically no eigenvalue in
(a −ε, a) ∪(b, b + ε). This now says that the asymptotic number of eigenvalues
in [a, b] is N(FN(b) −FN(a)) almost surely. The fact that the indexes of these
eigenvalues are those expected is obvious. If it were not the case, then we can
always ﬁnd an interval on the left or on the right of [a, b] which does not contain

7.2. Information plus noise model
195
1
3
4
0
0.1
0.2
0.3
0.4
0.5
Eigenvalues
Density
Empirical eigenvalue distribution
Limit law (from Theorem 3.15)
1
3
10
0
0.1
0.2
0.3
0.4
Eigenvalues
Density
Empirical eigenvalue distribution
Limit law (from Theorem 3.15)
Figure 7.5 Empirical and limit eigenvalue distribution of the information plus noise
model BN = 1
n(AN + σXN)(AN + σXN)H, N = 300, n = 3000 (c = 1/10), F
1
N AN AH
N
has three evenly weighted masses at 1, 3, 4 (top) and 1, 3, 10 (bottom).
the right amount of eigenvalues, which is contradictory from this proof. This
completes the proof of both results.
7.2.2
Asymptotic spectrum analysis
A similar spectrum analysis as in the case of sample covariance matrices when
the population covariance matrix has a ﬁnite number of distinct eigenvalues can
be performed for the information plus noise model. As discussed previously, the

196
7. Spectrum analysis
extension of mF (z) to the real positive half-line is locally invertible and increasing
when outside the support of F. The semi-converse is again true: if xF (m) is an
inverse function for mF (x) continuous with positive derivative, then its image is
outside the support of F. However here, xF (m) is not necessarily unique, as will
be conﬁrmed by simulations. Let us ﬁrst state the main result.
Theorem
7.9
([Dozier
and
Silverstein,
2007b]).
Let
BN = 1
n(AN +
σXN)(AN + σXN)H, with AN ∈CN×n such that HN ≜F
1
n ANAH
N ⇒H and
supN ∥1
nANAH
N∥< ∞, XN = (XN,ij) ∈CN×n with XN,ij independent for all
i, j, N with zero mean and unit variance (we release here the non-necessary
Gaussian hypothesis). Denote SF and SH the supports of F and H, respectively.
Take (h1, h2) ⊂Sc
H. Then there is a unique interval (mF,1, mF,2) ⊂(−1
σ2c, ∞)
such that the function
m 7→
m
1 + σ2cm
maps
(mF,1, mF,2)
to
(mH,1, mH,2) ⊂(−∞,
1
σ2c),
where
we
introduced
(mH,1, mH,2) = mH((h1, h2)). On (h1, h2), mH
is invertible, and then we
can deﬁne
xF (m) = 1
b2 m−1
H
 1
σ2c

1 −1
b

+ 1
b σ2(1 −c)
with b = 1 + σ2cm.
Then:
1. if for m ∈(mF,1, mF,2), x(m) ∈Sc
F , then x′(m) > 0;
2. if x′
F (m) > 0 for b ∈(mF,1, mF,2), then xF (m) ∈Sc
F and m = mF (xF (m)).
Similar to the sample covariance matrix case, Theorem 7.9 gives readily a
way to determine the support of F: for m varying in (mF,1, mF,2), whenever
xF (m) increases, its image is outside the support of F. The support of F is
therefore the complementary set to the union of all such intervals. We must
nonetheless be aware that the deﬁnition of xF (m) is actually linked to the choice
of the interval (h1, h2) ⊂Sc
H. In Theorem 7.4, we had a unique explicit inverse
for xF (m) as a function of m, whatever the choice of the pre-image of mH
(the Stieltjes transform of the l.s.d. of the population covariance matrix); this
statement no longer holds here.
In fact, if SH is subdivided into KH clusters, we can expect at most KH + 1
diﬀerent local inverses for xF (m) as m varies along R. This is in fact exactly
what is observed. Figure 7.6 depicts the situation when H is composed of three
evenly weighted masses in (1, 3, 4), then (1, 3, 10). Observe that KH + 1 diﬀerent
inverses exist that have the aforementioned behavior.
Now, also similar to the sample covariance matrix model, a lot more can be
said in the case where H is composed of a ﬁnite number of masses. The exact

7.2. Information plus noise model
197
−4
−2
0
2
4
0
5
10
15
m
xF (m)
xF (m)
Support of F
−4
−2
0
2
4
−2
0
2
4
6
8
m
xF (m)
xF (m)
Support of F
Figure 7.6 Information plus noise model, xF (m) for m real, F
1
N AN AH
N ⇒H, where H
has three evenly weighted masses in 1, 3, and 10 (top) and 1, 3, and 4 (bottom),
c = 1/10, σ = 0.1 in both cases. The support of F can be read on the central vertical
axises.
determination of the boundary of F can be determined. The result is summarized
as follows.
Theorem 7.10 ([Vallet et al., 2010]). Let BN be deﬁned as in Theorem 7.9,
where F
1
n ANAH
N = H is composed of K eigenvalues h1, . . . , hK (we implicitly
assume N takes only values consistent with F
1
n ANAH
N = H). Let φ be the function
on R \ {h1, . . . , hK} deﬁned by
φ(w) = w(1 −σ2cmH(w))2 + (1 −c)σ2(1 −σ2cmH(w)).

198
7. Spectrum analysis
Then φ(w) has 2KF , KF ≤K, local maxima, such that 1 −σ2cmH(w) > 0 and
φ(w) > 0. We denote these maxima w−
1 , w+
1 , w−
2 , w+
2 , . . . , w−
KF , w+
KF in the order
w−
1 < 0 < w+
1 ≤w−
2 < w+
2 ≤. . . ≤w−
KF < w+
KF .
Furthermore, denoting x−
k = φ(w−
k ) and x+
k = φ(w+
k ), we have:
0 < x−
1 < x+
1 ≤x−
2 < x+
2 ≤. . . ≤x−
KF < x+
KF .
The support SF of F is the union of the compact sets [x−
k , x+
k ], k ∈{1, . . . , KF }
SF =
KF
[
k=1
[x−
k , x+
k ].
Note that this alternative approach, via the function φ(w), allows us to give
a deterministic expression of the subsets [x−
k , x+
k ] without the need to explicitly
invert mH in K + 1 diﬀerent inverses, which is more convenient.
A cluster separability condition can also be established, based on the results
of Theorem 7.10. Namely, we say that the cluster in F corresponding to the
eigenvalue hk is disjoint from the neighboring clusters if there exists kF ∈
{1, . . . , KF } such that
hk−1 < w−
kF < hk < w+
kF < hk+1
with convention h0 = 0, hK+1 = ∞, and we say that kF is the cluster associated
with hk in F.
This concludes this chapter on spectral analysis of the sample covariance
matrix and the information plus noise models. As mentioned in the Introduction
of this chapter, these results will be applied to solve eigen-inference problems,
i.e. inverse problems concerning the eigenvalue or eigenvector structure of the
underlying matrix models. We will then move to the last chapter, Chapter 9,
of the theoretical part, which is concerned with limiting results on the extreme
eigenvalues for both the sample covariance matrix and information plus noise
models. These results will push further the theorems of exact separation by
establishing the limiting distributions of the extreme eigenvalues (although solely
in the Gaussian case) and also some properties on the corresponding eigenvectors.

8
Eigen-inference
In the introductory chapter of this book, we mentioned that the sample
covariance matrix Rn ∈CN×N obtained from n independent samples of a random
process x ∈CN is a consistent estimate of R ≜E[xxH] as n →∞for N ﬁxed,
in the sense that, for any given matrix norm ∥Rn −R∥→0 as n →∞(the
convergence being almost sure under mild assumptions). As such, Rn was
referred to as an n-consistent estimator of R. However, it was then shown by
means of the Mar˘cenko–Pastur law that Rn is not an (n, N)-consistent estimator
for R in the sense that, as both (n, N) grow large with ratio bounded away from
zero and ∞, the spectral norm of the matrix diﬀerence stays often away from
zero. We then provided an explicit expression for the asymptotic l.s.d. of Rn in
this case. However, in most estimation problems, we are actually interested in
knowing R itself, and not Rn (or its limit). That is, we are more interested in
the inverse problem of ﬁnding R given Rn, rather than in the direct problem of
ﬁnding the l.s.d. of Rn given R.
8.1
G-estimation
8.1.1
Girko G-estimators
The ﬁrst well-known examples of (n, N)-consistent estimators were provided by
Girko, see, e.g., [Girko], who derived more than ﬁfty (n, N)-consistent estimators
for various functionals of random matrices. Those estimators are called G-
estimators after Girko’s name, and are numbered in sequence as G1, G2, etc.
The G1, G3, and G4 estimators may be rather useful in the context of wireless
communications and are given hereafter. The ﬁrst estimator G1 is a consistent
estimator of the log determinant of the population covariance matrix R, also
referred to as the generalized variance.
Theorem 8.1. Let x1, . . . , xn ∈RN be n i.i.d. realizations of a given random
process with covariance E[(x1 −E[x1])(x1 −E[x1])H] = R and n > N. Denote
Rn the sample covariance matrix deﬁned as
Rn ≜1
n
n
X
i=1
(xi −ˆx)(xi −ˆx)H

200
8. Eigen-inference
where ˆx ≜1
n
PN
i=1 xi. Deﬁne G1 the functional
G1(Rn) = α−1
n
"
log det(Rn) + log
n(n −1)N
(n −N) QN
k=1(n −k)
#
with αn any sequence such that α−2
n log(n/(n −N)) →0. We then have
G1(Rn) −α−1
n log det(R) →0
in probability.
The G3 estimator deals with the inverse covariance matrix. The result here is
surprisingly simple.
Theorem 8.2. Let R ∈RN×N invertible and Rn ∈RN×N be deﬁned as in
Theorem 8.1. Deﬁne G3 as the function
G3(Rn) =

1 −N
n

R−1
n .
Then, for a ∈RN, b ∈RN of uniformly bounded norm, we have:
aTG3(Rn)b −aTR−1b →0
in probability.
The G4 estimator is a consistent estimator of the second order moment of the
population covariance matrix, in a sample covariance matrix model. This unfolds
from Theorem 3.13 and is given in the following.
Theorem 8.3. Let R ∈RN×N and Rn ∈RN×N be deﬁned as in Theorem 8.1.
Deﬁne G4 the function
G4(Rn) = 1
N tr R2
n −
1
nN (tr Rn)2 .
Then
G4(Rn) −1
N tr R2 →0
in probability.
This last result is compliant with the free probability estimator, for less
stringent hypotheses on Rn.
It is then possible to derive some functionals of R based on the observation Rn.
Note in particular that the multiplicative free deconvolution operation presented
in Chapters 4 and 5 allows us to obtain (n, N)-consistent estimates of the
successive moments of the eigenvalue distribution of R as a function of the
moments of the e.s.d. of Rn. Those can therefore be seen as G-estimators of the
moments of the l.s.d. of R. Now, we may be interested in an even more diﬃcult

8.1. G-estimation
201
inverse problem: provide an (n, N)-consistent estimator of every eigenvalue in R.
In the case where R has eigenvalues with large multiplicities, this problem has
been recently solved by Mestre in [Mestre, 2008b]. The following section presents
this recent G-estimation result and details the mathematical approach used by
Mestre to determine this estimator.
8.1.2
G-estimation of population eigenvalues and eigenvectors
For ease of read, we come back to the notations of Section 7.1. In this case, we
have the following result
Theorem 8.4 ([Mestre, 2008b]). Let BN = T
1
2
NXNXH
NT
1
2
N ∈CN×N be deﬁned
as in Theorem 7.6, i.e. TN has K distinct eigenvalues t1 < . . . < tK with
multiplicities N1, . . . , NK, respectively, for all r, Nr/n →cr, 0 < cr < ∞.
Further denote λ1 ≤. . . ≤λN the eigenvalues of BN and λ = (λ1, . . . , λN)T. Let
k ∈{1, . . . , K} and deﬁne
ˆtk = n
Nk
X
m∈Nk
(λm −µm)
(8.1)
with Nk = {Pk−1
j=1 Nj + 1, . . . , Pk
j=1 Nj} and µ1 ≤. . . ≤µN
are the ordered
eigenvalues of the matrix diag(λ) −1
n
√
λ
√
λ
T.
Then, if condition (7.6) of Theorem 7.6 is fulﬁlled for k, i.e. cluster kF in F
is mapped to tk only, we have:
ˆtk −tk →0
almost surely as N, n →∞, N/n →c, 0 < c < ∞.
The performance of the estimator of Theorem 8.4 is demonstrated in Figure 8.1
for K = 3, t1 = 1, t2 = 3, t3 = 10 in the cases when N = 6, n = 18, and N = 30,
n = 90. Remember from Figure 7.4 that the set (1, 3, 10) fulﬁlls condition (7.6),
so that Theorem 8.4 is valid. Observe how accurate the G-estimates of the tk are
already for very small dimensions. We will see both in the current chapter and
in Chapter 17 that, under the assumption that a cluster separability condition
is met (here, condition (7.6)), this method largely outperforms the moment-
based approach that consists in deriving consistent estimates for the ﬁrst order
moments and inferring tk from these moments. Note already that the naive
approach that would consist in taking the mean of the eigenvalues inside each
cluster (bottom of Figure 8.1) shows a potentially large bias in the estimated
eigenvalue, although the estimator variance seems to be smaller than with the
consistent G-estimator.
The reason why condition (7.6) must be fulﬁlled is far from obvious but will
become evident once we understand the proof of Theorem 8.4. Before proceeding,

202
8. Eigen-inference
1
3
10
0
0.2
0.4
0.6
0.8
1
Estimated tk
Density
G-estimator
1
3
10
0
2
4
6
8
10
Estimated tk
Density
G-estimator
Cluster mean
Figure 8.1 G-estimation of t1, t2, t3 in the model BN = T
1
2
NXNXH
NT
1
2
N, for
N1/N = N2/N = N3/N = 1/3 ,N/n = 1/10, for 100 000 simulation runs; Top N = 6,
n = 18, bottom N = 30, n = 90.
we need to introduce some notions of complex analysis, and its link to the Stieltjes
transform.
We ﬁrst consider the fundamental theorem of complex analysis [Rudin, 1986].
Theorem 8.5 (Cauchy’s integral formula.). Suppose C is a closed positively
oriented path (i.e. counter-clockwise oriented) in an open subset U ⊂C with
winding number one (i.e. describing a 360◦rotation).

8.1. G-estimation
203
(i) If z ∈U is contained in the surface described by C, then for any f holomorphic
on U
1
2πi
I
C
f(ω)
ω −z dω = f(z).
(8.2)
If the contour C is negatively oriented, then the right-hand side becomes −f(z).
(ii) If z ∈U is outside the surface described by C, then:
1
2πi
I
C
f(ω)
ω −z dω = 0.
(8.3)
Note that this second result is compliant with the fact that, for f continuous,
deﬁned on the real axis, the integral of f along a closed contour C ⊂R (i.e. a
contour that would go from a to b and backwards from b to a) is null.
Consider f some complex holomorphic function on U ⊂C, H a distribution
function, and denote G the functional
G(f) =
Z
f(z)dH(z).
From Theorem 8.5, we then have, for a negatively oriented closed path C
enclosing the support of H and with winding number one
G(f) =
1
2πi
Z I
C
f(ω)
z −ω dωdH(z)
=
1
2πi
I
C
Z
f(ω)
z −ω dH(z)dω
=
1
2πi
I
C
f(ω)mH(ω)dω
(8.4)
the integral inversion being valid since f(ω)/(z −ω) is bounded for ω ∈C. Note
that the sign inversion due to the negative contour orientation is compensated
by the sign reversal of (ω −z) in the denominator.
If dH is a sum of ﬁnite or countable masses and we are interested in evaluating
f(tk), with tk the value of the kth mass with weight lk, then on a negatively
oriented contour Ck enclosing tk and excluding tj, j ̸= k
lkf(tk) =
1
2πi
I
Ck
f(ω)mH(ω)dω.
(8.5)
This last expression is particularly convenient when we have access to tk only
through an expression of the Stieltjes transform of H.
Now, in terms of random matrices, for the sample covariance matrix model
BN = T
1
2
NXNXH
NT
1
2
N, we have already noticed that the l.s.d. F of BN (or
equivalently the l.s.d. F of BN = XH
NTNXN) can be rewritten under the form
(3.23), which can be further rewritten
c
mF (z)mH

−
1
mF (z)

= −zmF (z) + (c −1)
(8.6)

204
8. Eigen-inference
where H (previously denoted F T ) is the l.s.d. of TN. Note that it is allowed to
evaluate mH in −1/mF (z) for z ∈C+ since −1/mF (z) ∈C+.
As a consequence, if we only have access to F BN (from the observation of
BN), then the only link between the observation of BN and H is obtained by (i)
the fact that F BN ⇒F almost surely and (ii) the fact that F and H are related
through (8.6). Evaluating a functional f of the eigenvalue tk of TN is then made
possible by (8.5). The relations (8.5) and (8.6) are the essential ingredients behind
the proof of Theorem 8.4, which we detail below.
Proof of Theorem 8.4. We have from Equation (8.5) that, for any continuous f
and for any negatively oriented contour Ck that circles around tk but none of the
tj for j ̸= k, f(tk) can be written under the form
Nk
N f(tk) =
1
2πi
I
Ck
f(ω)mH(ω)dω
=
1
2πi
I
Ck
1
N
K
X
r=1
Nr
f(ω)
tr −ω dω
with H the limit F TN ⇒H. This provides a link between f(tk) for all continuous
f and the Stieltjes transform mH(z).
Letting f(x) = x and taking the limit N →∞, Nk/N →ck/c, with c ≜c1 +
. . . + cK the limit of N/n, we have:
ck
c tk =
1
2πi
I
Ck
ωmH(ω)dω.
(8.7)
We now want to express mH as a function of mF , the Stieltjes transform of
the l.s.d. F of BN. For this, we have the two relations (3.24), i.e.
mF (z) = cmF (z) + (c −1)1
z
and (8.6) with F T = H, i.e.
c
mF (z)mH

−
1
mF (z)

= −zmF (z) + (c −1).
Together, those two equations give the simpler expression
mH

−
1
mF (z)

= −zmF (z)mF (z).
(8.8)
Applying the variable change ω = −1/mF (z) in (8.7), we end up with
ck
c tk =
1
2πi
I
CF ,k
z
m′
F (z)
mF (z)c + 1 −c
c
mF (z)′
m2
F (z) dz
= 1
c
1
2πi
I
CF ,k
z
m′
F (z)
mF (z)dz,
(8.9)

8.1. G-estimation
205
where CF ,k is the (well-deﬁned) preimage of Ck by −1/mF . The second equality
(8.9) comes from the fact that the second term in the previous relation is the
derivative of (c −1)/(cmF (z)), which therefore integrates to zero on a closed
path, as per classical real or complex integration rules [Rudin, 1986]. Obviously,
since z ∈C+ is equivalent to −1/mF (z) ∈C+ (the same being true if C+ is
replaced by C−), CF ,k is clearly continuous and of non-zero imaginary part
whenever ℑ[z] ̸= 0. Now, we must be careful about the exact choice of CF ,k.
Since k is assumed to satisfy the separability conditions of Theorem 7.6, the
cluster kF associated with k in F is distinct from the clusters (k −1)F and
(k + 1)F (whenever they exist). Let us then pick x(l)
F
and x(r)
F
two real values
such that
x+
(k−1)F < x(l)
F < x−
kF < x+
kF < x(r)
F
< x−
(k+1)F
with {x−
1 , x+
1 , . . . , x−
KF , x+
KF } the support boundary of F, as deﬁned in Theorem
7.5. That is, we take a point x(l)
F right on the left side of cluster kF and a point
x(r)
F
right on the right side of cluster kF . Now remember Theorem 7.4 and Figure
7.2; for x(l)
F as deﬁned previously, mF (z) has a limit m(l) ∈R as z →x(l)
F , z ∈C+,
and a limit m(r) ∈R as z →x(r)
F , z ∈C+, those two limits verifying
tk−1 < x(l) < tk < x(r) < tk+1
(8.10)
with x(l) ≜−1/m(l) and x(r) ≜−1/m(r).
This is the most important outcome of our integration process. Let us choose
CF ,k to be any continuously diﬀerentiable contour surrounding cluster kF such
that CF ,k crosses the real axis in only two points, namely x(l)
F
and x(r)
F . Since
−1/mF (C+) ⊂C+ and −1/mF (C−) ⊂C−, Ck does not cross the real axis
whenever CF ,k is purely complex and is obviously continuously diﬀerentiable
there; now Ck crosses the real axis in x(l) and x(r), and is in fact continuous
there. Because of (8.10), we then have that Ck is (at least) continuous and
piecewise continuously diﬀerentiable and encloses only tk. This is what is required
to ensure the validity of (8.9). In Figure 8.2, we consider the case where TN is
formed of three evenly weighted eigenvalues t1 = 1, t2 = 3 and t3 = 10, and we
depict the contours Ck, preimages of CF ,k, k ∈{1, 2, 3}, circular contours around
the clusters kF such that they cross the real line in the positions x(l)
F and x(r)
F ,
corresponding to the inﬂexion points of xF (m) (and an arbitrary large value for
the extreme right point).
The diﬃcult part of the proof is completed. The rest will unfold more naturally.
We start by considering the following expression
ˆtk ≜
1
2πi
n
Nk
I
CF ,k
z
m′
BN (z)
mBN (z)dz
=
1
2πi
n
Nk
I
CF ,k
z
1
n
Pn
i=1
1
(λi−z)2
1
n
Pn
i=1
1
λi−z
dz,
(8.11)

206
8. Eigen-inference
1
3
10
−0.25
−0.2
−0.15
−0.1
−5 · 10−2
0
5 · 10−2
0.1
0.15
0.2
0.25
ℜ(z)
ℑ(z)
C1
C2
C3
Figure 8.2 Integration contours Ck, k ∈{1, 2, 3}, preimage of CF ,k by −1/mF , for CF ,k
a circular contour around cluster kF , when TN composed of three distinct entries,
t1 = 1, t2 = 3, t3 = 10, N1 = N2 = N3, N/n = 1/10.
where we recall that BN ≜XH
NTNXN and, if n ≥N, λN+1 = . . . = λn = 0.
The value ˆtk can be viewed as the empirical counterpart of tk. Now, we
know from Theorem 3.13 that mBN (z)
a.s.
−→mF (z) and mBN (z)
a.s.
−→mF (z). It
is not diﬃcult to verify, from the fact that mF is holomorphic, that the same
convergence holds for the successive derivatives.
At this point, we need the two fundamental results that are Theorem 7.1 and
Theorem 7.2. We know that, for all matrices BN in a set of probability one, all
the eigenvalues of BN are contained in the support of F for all large N, and
that the eigenvalues of BN contained in cluster kF are exactly {λi, i ∈Nk} for
these large N. Take such a BN. For all large N, mBN (z) is uniformly bounded
over N and z ∈CF ,k, since CF ,k is away from the support of F. The integrand
in the right-hand side of (8.11) is then uniformly bounded for all large N and
for all z ∈CF ,k. By the dominated convergence theorem, Theorem 6.3, we then
have that ˆtk −tk
a.s.
−→0.
It then remains to prove that ˆtk takes the form (8.1). This is performed by
residue calculus [Rudin, 1986], i.e. by determining the poles in the expanded
expression of ˆtk (when developing mBN (z) in its full expression).
For this, we open a short parenthesis to introduce the basic rules of complex
integration, required here. First, we need to deﬁne poles and residues.
Deﬁnition 8.1. Let γ be a continuous, piecewise continuously diﬀerentiable
contour on C. If f is holomorphic inside γ but on a, i.e.
lim
z→a |f(z)| = ∞

8.1. G-estimation
207
then a is a pole of f. We then deﬁne the order of a as being the smallest integer
k such that
lim
z→a |(z −a)kf(z)| < ∞.
The residue Res(f, a) of f in a, is then deﬁned as
Res(f, a) ≜lim
z→a
dk−1
dzk−1

(z −a)kf(z)

.
This being deﬁned, we have the following fundamental result of complex
analysis.
Theorem 8.6. Let γ be a continuous, piecewise continuously diﬀerentiable
positively oriented contour on C. For f holomorphic inside γ but on a discrete
number of points, we have that
1
2πi
I
γ
f(z)dz =
X
a pole of f
Res(f, a).
If γ is negatively oriented, then the right-hand side term must be multiplied by
−1.
The calculus procedure of residues is then as follows:
1. we ﬁrst determine the poles of f lying inside the surface formed by γ,
2. we then determine the order of each pole,
3. we ﬁnally compute the residues of f at the poles and evaluate the integral as
the sum of all residues.
In our problem, the poles of the integrand of (8.11) are found to be λ1, . . . , λN
(indeed, the integrand of (8.11) behaves like O(1/(λi −z)) for z ≃λi) and
µ1, . . . , µN, the N real roots of the equation in µ, mBN (µ) = 0 (indeed, the
denominator of the integrand cancels for z = µi, while the numerator is non-
zero). Since CF ,k encloses only those values λi such that i ∈Nk, the other poles
are discarded. Noticing now that mBN (µ) →±∞as µ →λi, we deduce that
µ1 < λ1 < µ2 < . . . < µN < λN, and therefore we have that µi, i ∈Nk are all in
CF ,k but maybe for µj, j = min Nk. It can in fact be shown that µj is also in
CF ,k. To notice this last remaining fact, observe simply that
1
2πi
I
Ck
1
ω dω = 0
since zero is not contained in the contour Ck. Applying the variable change
ω = −1/mF (z) as previously, this gives
I
CF ,k
m′
F (z)
m2
F (z)dz = 0.
(8.12)

208
8. Eigen-inference
From the same reasoning as above, with the dominated convergence theorem
argument, Theorem 6.3, we have that, for suﬃciently large N and almost surely

I
CF ,k
m′
BN (z)
m2
BN (z)dz
 < 1
2.
(8.13)
We now proceed to residue calculus in order to compute the integral in the
left-hand side of (8.13). Following the above procedure, notice that the poles
of (8.12) are the λi and the µi that lie inside the integration contour CF ,k, all
of order one with residues equal to −1 and 1, respectively. These residues are
obtained using in particular L’Hospital rule, Theorem 2.10, as detailed below
for the ﬁnal calculus. Therefore, (8.12) equals the number of such λi minus the
number of such µi (remember that the integration contour is negatively oriented,
so we need to reverse the signs). We however already know that this diﬀerence,
for large N, equals either zero or one, since only the position of the leftmost µi is
unknown yet. But since the integral is asymptotically less than 1/2, this implies
that it is identically zero, and therefore the leftmost µi (indexed by min Nk) also
lies inside the integration contour.
We have therefore precisely characterized Nk. We can now evaluate (8.11). This
calls again for residue calculus, the steps of which are detailed below. Denoting
f(z) = z
m′
BN (z)
mBN (z),
we ﬁnd that λi (inside CF ,k) is a pole of order 1 with residue
lim
z→λi(z −λi)f(z) = −λi
which is straightforwardly obtained from the fact that f(z) ∼
1
λi−z as z ∼λi.
Also µi (inside CF ,k) is a pole of order 1 with residue
lim
z→µi(z −µi)f(z) = µi
which is obtained using L’Hospital rule: upon existence of a limit, we indeed
have
lim
z→µi(z −µi)f(z) = lim
z→µi
d
dz
h
(z −µi)zm′
BN (z)
i
d
dz

mBN (z)

which expands as
lim
z→µi(z −µi)f(z) = lim
z→µi
zm′
BN (z) + z(z −µi)m′′
BN (z) + (z −µi)m′
BN (z)
m′
BN (z)
.
Notice now that |m′
BN (z)| is positive and uniformly bounded by 1/ε2 for
mini{|λi −z|} > ε. Therefore, the ratio is always well deﬁned and, for z →µi
with µi poven away from all λi, we ﬁnally have
lim
z→µi(z −µi)f(z) = lim
z→µi z = µi.

8.1. G-estimation
209
Since the integration contour is chosen to be negatively oriented, it must be
kept in mind that the signs of the residues need be inverted in the ﬁnal relation.
It now remains to verify that µ1, . . . , µN are also the eigenvalues of diag(λ) −
1
n
√
λ
√
λ
T. This is immediate from the following lemma.
Lemma 8.1 ([Couillet et al., 2011c],[Gregoratti and Mestre, 2009]). Let A ∈
CN×N be diagonal with entries λ1, . . . , λN and y ∈CN. Then the eigenvalues of
(A −yy∗) are the N real solutions in x of
N
X
i=1
y2
i
λi −x = 1.
Proof. Let A ∈CN×N be a Hermitian matrix and y ∈CN. If µ is an eigenvalue
of (A −yy∗) with eigenvector x, we have the equivalent relations
(A −yy∗)x = µx,
(A −µIN)x = y∗xy,
x = y∗x(A −µIN)−1y,
y∗x = y∗xy∗(A −µIN)−1y,
1 = y∗(A −µIN)−1y.
Take A diagonal with entries λ1, . . . , λN, we then have
N
X
i=1
y2
i
λi −µ = 1.
Taking A = diag(λ) and yi = √λi/√n, we have the expected result. This
completes the proof of Theorem 8.4.
Other G-estimators can be derived from this technique. In particular, note
that, for x, y ∈CN given vectors, and TN = PK
k=1 tkUkUH
k ∈CN×N the spectral
distribution of TN in Theorem 8.4, we have from residue calculus
xHUkUH
k y =
1
2πi
I
Ck
xH(TN −zIN)−1ydz
with Ck a negatively oriented contour enclosing tk, but none of the ti, i ̸= k.
From this remark, using similar derivations as above for the quadratic form
xH(TN −zIN)−1y instead of the Stieltjes transform 1
N tr(TN −zIN)−1, we then
have the following result.
Theorem 8.7 ([Mestre, 2008b]). Let BN be deﬁned as in Theorem 8.4,
and denote BN = PN
k=1 λkbkbH
k , bH
k bi = δi
k, the spectral decomposition of BN.
Similarly, denote TN = PK
k=1 tkUkUH
k , UH
k Uk = Ink, with Uk ∈CN×Nk the
eigenspace associated with tk. For given vectors x, y ∈CN, denote
u(k; x, y) ≜xHUkUH
k y.

210
8. Eigen-inference
Then we have:
ˆu(k; x, y) −u(k; x, y)
a.s.
−→0
as N, n →∞with ratio cN = N/n →c, where
ˆu(k; x, y) ≜
N
X
i=1
θk(i)xHbkbH
k y
and θk(i) is deﬁned by
θi(k) =
 −φk(i)
, i /∈Nk
1 + ψk(i) , i ∈Nk
with
φk(i) =
X
r∈Nk

λr
λi −λr
−
µr
λi −µr

ψk(i) =
X
r/∈Nk

λr
λi −λr
−
µr
λi −µr

and Nk, µ1, . . . , µN deﬁned as in Theorem 8.4.
This result will be shown to be appealing in problems of direction of arrival
(DoA) detection, see Chapter 17.
We complete this section with modiﬁed versions of Girko’s G-1 estimator,
Theorem 8.1, which are obtained from similar sample covariance arguments as
above. The ﬁrst result is merely a generalization of the convergence in probability
of Theorem 8.1 to almost sure convergence.
Theorem 8.8 (Theorem 1 in [Kammoun et al., 2011]). Deﬁne the matrix YN =
TNXN +
1
√xWN ∈CN×M
for x > 0, with XN ∈Cn×M
and WN ∈CN×M
random matrices with independent entries of zero mean, unit variance and ﬁnite
2 + ε order moment for some ε > 0 and TN ∈CN×n deterministic such that
TNTH
N has uniformly bounded spectral norm along growing N. Assume that the
e.s.d. of TNTH
N converges weakly to H as N →∞. Denote BN =
1
M YNYH
N.
Then, as N, n, M →∞, with M
N →c > 1 and N
n →c0
1
N log det
 IN + xTNTH
N

−
 1
N log det (xBN) + M −N
N
log
M −N
M

+ 1

a.s.
−→0.
Under this setting, the G-estimator is exactly the estimator of the Shannon
transform of TNTH
N at point x or equivalently of the capacity of a deterministic
multiple antenna link TN
under additive noise variance 1/x from the
observations of the data vectors y1, . . . , yM such that Y = [y1, . . . , yM]. A simple

8.1. G-estimation
211
way to derive this result is to use the Shannon transform relation of Equation
(3.5)
1
N log det(IN + xTNTH
N) =
Z x
0
1
t −1
t2 mH

−1
t

dt
(8.14)
along with the fact, similar to Equation (8.8), that, for z ∈C \ R+
mH

−
1
mF (z) −1
x

= −zmF (z)mF (z)
with F the l.s.d. of BN =
1
M YNYH
N and F the l.s.d. of
1
M YH
NYN. The change of
variable t = (1/mF (u) + 1/x)−1 in Equation (8.14) allows us to write VTNTH
N (x)
as a function of mF from which we obtain directly the above estimator.
The second result introduces an additional deterministic matrix RN, which in
an applicative sensing context can be used to infer the achievable communication
rate over a channel under unknown interference pattern. We precisely have the
following.
Theorem 8.9 (Theorem 2 in [Kammoun et al., 2011]). Deﬁne the matrix
YN = TNXN +
1
√xWN ∈CN×M for x > 0 where XN ∈Cn×M, and WN ∈
CN×M are random matrices with Gaussian independent entries of zero mean
and unit variance, TN ∈Cn×n is deterministic with uniformly bounded spectral
norm for which the e.s.d. of TNTH
N converges weakly, and let RN ∈CN×N be
a deterministic non-negative Hermitian matrix. Then, as N, n, M →∞with
1 < lim inf M/N ≤lim sup M/N < ∞and 0 < lim inf N/n ≤lim sup N/n < ∞,
we have:
1
N log det
 IN + x

RN + TNTH
N

−
 1
N log det (x[BN + yNRN]) + M −N
N
log(yN) + M
N (1 −yN)

a.s.
−→0.
with yN the unique positive solution of the equation in y
y = 1
M tr yRN (yRN + BN)−1 + M −N
M
.
(8.15)
This result is particularly useful in a rate inference scenario when RN = HHH
for some multiple antenna channel matrix H but unknown colored interference
TNxk +
1
√xwk. Theorem 8.9 along with Theorem 8.8 allow for a consistent
estimation of the capacity of the MIMO channel H based on M successive
observations of noise-only signals (or the residual terms after data decoding).
The proof of Theorem 8.9 arises ﬁrst from the fact that, for given y and RN,
a deterministic equivalent for
1
N log det (yRN + BN)

212
8. Eigen-inference
was derived in [Vallet and Loubaton, 2009] and expresses as
1
N log det (yRN + BN)
−
"
1
N log det

yRN + TNTH
N + xIN
1 + κ(y)

+ M
N log(1 + κ(y)) −
M
N κ(y)
1 + κ(y)
#
a.s.
−→0
with κ(y) the unique positive solution for y > 0 of
κ(y) = 1
M tr[TNTH
N + xIN]

yRN +
1
1 + κ(y)[TNTH
N + xIN]
−1
.
This last result is obtained rather directly using deterministic equivalent
methods detailed in Chapter 6. Now, we observe that, if y =
1
1+κ(y), the term
1
N log det(yRN + y[TNTH
N + xIN]) appears, which is very close to what we need.
Observing that this has a unique solution, asymptotically close to the unique
solution of Equation (8.15), we easily infer the ﬁnal result. More details are
given in [Kammoun et al., 2011].
Similar results are also available beyond the restricted case of sample
covariance matrices, in particular for the information plus noise models. We
mention especially the information plus noise equivalent to Theorem 8.7,
provided in [Vallet et al., 2010], whose study is based on Theorem 7.10.
Theorem 8.10. Let BN be deﬁned as in Theorem 7.8, where we assume
that F
1
n ANAH
N = H for all N of practical interest, i.e. we assume F
1
n ANAH
N
is composed of K
masses in h1 < . . . < hK
with respective multiplicities
N1, . . . , NK. Further suppose that h1 = 0 and let Π be the associated eigenspace
of h1 (the kernel of
1
nANAH
N). Denote BN = PN
k=1 λkukuH
k
the spectral
decomposition of BN, with uH
k uj = δj
k, and denote
π(x) ≜xHΠx.
Then, we have that
π(x) −ˆπ(x)
a.s.
−→0
where ˆπ(x) is deﬁned as
ˆπ(x) ≜
N
X
k=1
βkxHukuH
k x
with βk deﬁned as
βk = 1 + σ2
N
N
X
l=N−N1+1
1
λl −λk
+ 2σ2
N
N
X
l=N−N1+1
λk
(λk −λl)2
−σ2(1 −c)
"
N
X
l=N−N1+1
1
λl −λk
−
N
X
l=N−N1+1
1
µl −λk
#
, 1 ≤k ≤N −N1

8.1. G-estimation
213
βk = 1 + σ2
N
N−N1
X
l=1
1
λl −λk
+ 2σ2
N
N−N1
X
l=1
λk
(λk −λl)2
−σ2(1 −c)
"N−N1
X
l=1
1
λk −µl
−
N−N1
X
l=1
1
λk −λl
#
, N −N1 + 1 ≤k ≤N
with µ1, . . . , µN the N real roots of mBN (x) = −1/σ2.
We do not further develop information plus noise model considerations in this
section and move now to second order statistics for G-estimators.
8.1.3
Central limit for G-estimators
The G-estimators derived above are consistent with increasingly large system
dimensions but are applied to systems of ﬁnite, sometimes small, dimensions.
This implies some inevitable inaccuracy in the successive estimates, as observed
for instance in Figure 8.1. For application purposes, it is fundamental to be
able to assess the quality of these estimates. In mathematical terms, this
implies computing statistics of the estimates. Various central limit theorems
for estimators of functionals of sample covariance matrix can be found in the
literature, notably in Girko’s work, see, e.g., [Girko], where central limits for
G-estimators are provided.
This section introduces instead a recent result on the limiting distribution of
n(ˆtk −tk) where tk and ˆtk are deﬁned in Theorem 8.4 as the entries of TN for
the sample covariance matrix BN = T
1
2
NXNXH
NT
1
2
N ∈CN×N, XN ∈CN×n with
entries
1
√nXij, i.i.d., such that X11 has zero mean, unit variance, and fourth order
moment E[|X11|4] = 2 and TN ∈CN×N with distinct eigenvalues t1 < . . . < tK
of multiplicity N1, . . . , NK, respectively. Speciﬁcally, we will show that the vector
(n(ˆtk −tk))1≤k≤K is asymptotically Gaussian with zero mean and a covariance
which we will evaluate, as N →∞.
The ﬁnal result, due to Yao, unfolds as follows.
Theorem 8.11 ([Yao et al., 2011]). Let BN be deﬁned as in Theorem 8.4
with E[|XN,ij|4] = 2 and Ni/N = ci + o(1/N) for all i, 0 < ci < ∞. Denote
I ⊂{1, . . . , K} the set of indexes k such that k satisﬁes the separability condition
of Theorem 7.6. Then, for every set J = {j1, . . . , jp} ⊂I, as N, n grow large
 n(ˆtk −tk)

k∈J ⇒X
with X a Gaussian p-dimensional vector with zero mean and covariance ΘJ with
(k, k′) entry ΘJ
k,k′ deﬁned as
ΘJ
k,k′ ≜−
1
4π2c2cicj
I
Cjk
I
Cjk′

m′(z1)m′(z2)
(m(z1) −m(z2))2 −
1
(z1 −z2)2

dz1dz2
m(z1)m(z2)
(8.16)

214
8. Eigen-inference
1
3
10
0
1
2
3
Estimates
Density
Histogram of the ˆtk
Theoretical limiting distribution
Figure 8.3 Comparison of empirical against theoretical variances for the estimator of
Theorem 8.4, based on Theorem 8.11, K = 3, t1 = 1, t2 = 3, t3 = 10,
N1 = N2 = N3 = 20, n = 600.
where the contour Ck encloses the limiting support of the eigenvalues of BN
indexed by Nk = {Pk−1
j=1 Nj + 1, . . . , Pk
j=1 Nj}, only, i.e. the cluster kF
as
deﬁned in Theorem 7.6. Moreover
ˆΘJ
k,k′ −ΘJ
k,k′
a.s.
−→0
as N, n →∞, where ˆΘJ
jk,jk′ is deﬁned by
ˆΘJ
k,k′ ≜
n2
NkNk′


X
(i,j)∈Njk ×Njk′
−1
(µi −µj)2m′
BN (µi)m′
BN (µj)
+ δkk′
X
i∈Nk
 
m′′′
BN (µi)
6m′
BN (µi)3 −m′′
BN (µi)2
4m′
BN (µi)4
!#
(8.17)
with the quantities µ1, . . . , µN deﬁned as in Theorem 8.4.
In Figure 8.3, the performance of Theorem 8.11 is evaluated against 10 000
Monte Carlo simulations of a scenario of three users, with t1 = 1, t2 = 3, t3 =
10, N1 = N2 = N3 = 20, N = 60, and n = 600. It appears that the limiting
distribution is very accurate for these values of N, n. Further simulations to
obtain empirical estimates ˆΘJ
k,k of ΘJ
k,k suggest that ˆΘk,k is an accurate estimator
as well.
We provide hereafter a sketch of the proof of Theorem 8.11.
Proof. The idea of the proof relies on the following remarks:
• from Theorem 3.17, well-behaved functionals of BN have a Gaussian limit;

8.1. G-estimation
215
• then, from the proof of Theorem 8.4 and in particular Equation (8.9), the
estimator ˆtk of tk expresses as an integral function of the Stieltjes transform
of F BN and of its derivative. Applying Theorem 3.17, the variations of
N(mBN (z) −mF (z)) and of N(m′
BN (z) −m′
F (z)), with F the l.s.d. of BN
can be proved to be asymptotically Gaussian;
• from
there,
the
Gaussian
limits
of
both
N(mBN (z) −mF (z))
and
N(m′
BN (z) −m′
F (z)) can be further extended to the ﬂuctuations of the
integrand in the expression of ˆtk in Equation (8.11), using the so-called delta
method, to be introduced subsequently;
• ﬁnal tightness arguments then ensure that the limiting Gaussian ﬂuctuations
of the integrand propagate to the ﬂuctuations of the integral, i.e. to n(ˆtk −tk).
This is the general framework of the proof, for which we provide a sketch
hereafter.
Following Theorem 3.17, denote N(F BN −FN) the diﬀerence between the
e.s.d. of BN and the l.s.d. of BN modiﬁed in such a way that F TN replaces the
limiting law of T1, T2, . . . and Ni/n replaces ci. Then, for a family f1, . . . , fp of
functions holomorphic on R+, the vector

n
Z
fi(x)d(F BN −FN)(x)

1≤i≤p
(8.18)
converges to a Gaussian random variable with zero mean and covariance V with
(i, j) entry Vij given by:
Vij = −
1
4π2c2
I I
fi(z1)fj(z2)vij(z1, z2)dz1dz2
with
vij(z1, z2) =
m′(z1)m′(z2)
(m(z1) −m(z2))2 −
1
(z1 −z2)2
where the integration is over positively oriented contours that circle around the
intersection of the supports of FN for all large N. If we ensure a suﬃciently
fast convergence of the spectral law of TN and of Ni/n, then we can replace
FN by F, the almost sure l.s.d. of BN, in (8.18). This explains the assumption
Ni/n = ci + o(1/N).
Now, consider Equation (8.9), where tk is expressed under the form of a
complex integral of the Stieltjes transform mF (z) of F and of its derivative m′
F (z)
(we remind that F is the l.s.d. of BN = XH
NTNXN) over a contour CF ,k that
encloses kF only. Since the functions (x −z)−1 and (x −z)−2 at any point z of the
integration contour CF ,k are holomorphic on R+, we can apply straightforwardly
Theorem 3.17 to ensure that any vector with entries n(mBN (zi) −mF (zi)) and
n(m′
BN (zi) −m′
F (zi)), for any ﬁnite set of zi away from the support of F, is
asymptotically Gaussian with zero mean and a certain covariance. Then, notice

216
8. Eigen-inference
that
n
"
z
m′
BN (zi)
mBN (zi) −z
m′
F (zi)
mF (zi)
#
= n
"
zim′
BN (zi)mF (zi) −zim′
F (zi)mBN (zi)
mBN (zi)mF (zi)
#
which we would like to express in terms of the diﬀerences n(mBN (zi) −mF (zi))
and n(m′
BN (zi) −m′
F (zi)).
To this end, we apply Slutsky’s lemma, given as follows.
Theorem 8.12 ([Van der Vaart, 2000]). Let X1, X2, . . . be a sequence of random
variables converging weakly to a random variable X and Y1, Y2, . . . converging in
probability to a constant c. Then, as n →∞
YnXn ⇒cX.
Applying Theorem 8.12 ﬁrst to the variables Yn = mBN (z)
a.s.
−→mF (z) and
Xn = n(mBN (z) −mF (z)), and then to the variables Yn = m′
BN (z)
a.s.
−→m′
F (z)
and Xn = n(m′
BN (z) −m′
F (z)), we have rather immediately that
n
"
zim′
BN (zi)mF (zi) −zim′
F (zi)mBN (zi)
mBN (zi)mF (zi)
#
⇒zi
m′
F (zi)X −mF (zi)Y
mF (zi)2
with X and Y two random variables such that n[mBN (zi) −mF (zi)] ⇒X and
n[m′
BN (zi) −m′
F (zi)] ⇒Y . This last form can be rewritten
f(X, Y ) = f(X −0, Y −0) = zi
m′
F (zi)
mF (zi)2 X −zi
mF (zi)
mF (zi)2 Y
where f is therefore a linear function in (X, Y ), diﬀerentiable at (0, 0). In order
to pursue, we then introduce the fundamental tool required in this proof, the
delta method. The delta method allows us to transfer Gaussian behavior from a
random variable to a functional of it, according to the following theorem.
Theorem 8.13. Let X1, X2, . . . ∈Rn be a random sequence such that
an(Xn −µ) ⇒X ∼N(0, V)
for
some
sequence
a1, a2, . . . ↑∞.
Then
for
f : Rn →RN,
a
function
diﬀerentiable at µ
an(f(Xn) −f(µ)) ⇒J(f)X
with J(f) the Jacobian matrix of f.
Using the delta method on the variables X and Y for diﬀerent zi, and applied
to the function f, we have that the vector
 
n
"
zi
m′
BN (zi)
mBN (zi) −zi
m′
F (zi)
mF (zi)
#!
1≤i≤p
i.e. the deviation of p points of the integrands in (8.11), is asymptotically
Gaussian.

8.1. G-estimation
217
In order to propagate the Gaussian limit of the deviations in the integrands
of (8.11) to the deviations in ˆtk itself, it suﬃces to study the behavior of the
sum of Gaussian variables over the integration contour CF ,k. Since the integral
can be written as the limit of a ﬁnite Riemann sum and that a ﬁnite Riemann
sum of Gaussian random variable is still Gaussian, it suﬃces to ensure that the
ﬁnite Riemann sum is still Gaussian in the limit. This requires an additional
ingredient: the tightness of the sequences
n
 
z
m′
BN (z)
mBN (z) −z
m′
F (z)
mF (z)
!
for growing n and for all z in the contour, see [Billingsley, 1968, Theorem 13.1].
This naturally unfolds from a direct application of [Billingsley, 1968, Theorem
13.2], following a similar proof as in [Bai and Silverstein, 2004], and we have
proven the Gaussian limit of vectors (n(ˆtk −tk))k∈J.
The last step of the proof is the calculus of the covariance of the Gaussian
limit. This requires to evaluate for all k, k′
n2E
I
Cjk
I
Cjk′
"
zkm′
BN (zk)
mBN (zk)
−
zkm′
F (zk)
mF (zk)
# "
zk′m′
BN (zk′)
mBN (zk′)
−
zk′m′
F (zk′)
mF (zk′)
#
dzkdzk′.
Integrations by parts simplify the result and lead to (8.16). In order to obtain
(8.17), residue calculus is ﬁnally performed similar to the proof of Theorem
8.4.
Note that the proof relies primarily on the central limit theorem of Bai and
Silverstein, Theorem 3.17. In particular, for other models more involved than
the sample covariance matrix model, the Gaussian limit of the deviations of
functionals of the e.s.d. of BN must be proven in order both to prove asymptotic
central limit of the estimator and even to derive the asymptotic variance of
the estimator. This calls for a generalization of Bai and Silverstein central limit
theorem to advanced random matrix models, see examples of such models in the
context of statistical inference for cognitive radios in Chapter 17.
This recent incentive for eigen-inference based on the Stieltjes transform is
therefore strongly constrained by the limited amount of central limit theorems
available today. As an alternative to the Stieltjes transform method for statistical
inference, we have already mentioned that free probability and methods derived
from moments can perform similar inference, based on consistent estimation of
the moments only. From the information on the estimated moments, assuming
that these moments alone describe the l.s.d., an estimate of the functional under
study can be determined. These moment approaches can well substitute Stieltjes
transform methods when (i) the model under study is too involved to proceed to
complex integration or (ii) when the analytic approach fails, as in the case when
clusters mapped to population eigenvalues are not disjoint. Note in particular
that the Stieltjes transform method requires exact separation properties, which
to this day is known only for very few models.

218
8. Eigen-inference
8.2
Moment deconvolution approach
Remember that the free probability framework allows us to evaluate the
successive moments of compactly supported l.s.d. of products, sums, and
information plus noise models of asymptotically free random matrices based
on the moments of the individual random matrix l.s.d. The operation that
evaluates the moments of the output random matrices from the moments of
the input deterministic matrices was called free convolution. It was also shown
that the moments of an input matrix can be retrieved from those of the resulting
output matrix and the other operands (this assumes large matrix dimensions):
the associated operation was called free deconvolution. From combinatorics
on non-crossing partitions, we stated that it is rather easy to automatize the
calculus of free convolved and deconvolved moments. We therefore have already
a straightforward way to perform eigen-inference on the successive moments
of the l.s.d. of the population covariance matrix from the observed sample
covariance matrix, or on the moments of the l.s.d. of the information matrix
from the observed information plus noise matrix and so on. Since the l.s.d. are
compactly supported, the moments determine the distribution and therefore
can be used to perform eigen-inference on various functionals of the l.s.d.
However, since moments are unbounded functionals of the eigenvalue spectrum,
the moment estimates are usually very inaccurate, more particularly so for
high order moments. When estimating the eigenvalues of population covariance
matrices TN, as in Theorem 8.4, moment approaches can be derived although
rather impractical, as we will presently see. This is the main drawback of this
approach, which, although much more simple and systematic than the previously
introduced methods, is fundamentally inaccurate in practice.
Consider again the inference on the individual eigenvalues of TN in the model
BN = T
1
2
NXNXH
NT
1
2
N ∈CN×N of Theorem 8.4. For simplicity, we assume that
the K distinct eigenvalues of TN have the same mass; this fact being known to the
experimenter. Since TN has K distinct positive eigenvalues t1 < t2 < . . . < tK,
we have already mentioned that we can recover these eigenvalues from the ﬁrst
K moments of the l.s.d. H of TN, recovered from the ﬁrst K (free deconlvolved)
moments of the l.s.d. F of BN. Those moments are the K roots of the Newton–
Girard polynomial (5.2), computed from the moments of H. A naive approach
might therefore consist in estimating the moments of TN by free deconvolution
of the e.s.d. of BN = BN(ω), for N ﬁnite, and then solving the Newton–Girard
polynomial for the estimated moments. We provide hereafter an example for the
case K = 3.
From the method described in Section 5.2, we obtain that the moments
B1, B2, B3 of F are given as a function of the moments T1, T2, T3 of H, as
B1 = T1,
B2 = T2 + cT 2
1 ,

8.2. Moment deconvolution approach
219
B3 = T3 + 3cT2T1 + c2T 3
1
with c = limn→∞N/n. Note that, from the extension to ﬁnite size random
matrices presented in Section 5.4, we could consider the expected e.s.d. of BN
in place of the l.s.d. of BN, in which case B1 and B2 remain the same, and B3
becomes
B3 = (1 + n−2)T3 + 3cT2T1 + c2T 3
1 .
We can then obtain an expression of the Tk by reverting the above equations,
as
T1 = B1,
T2 = B2 −cB2
1,
T3 = (1 + n−2)−1  B3 −3cB2B1 + 2c2B3
1

.
(8.19)
By deconvolving the empirical moments ˆBk ≜1
N tr Bk
N, 1 ≤k ≤3, of BN with
the method above, we obtain estimates ˆT1, ˆT2, ˆT3 of the moments T1, T2, T3, in
place of T1, T2, T3 themselves. We then obtain estimates ˆt1, ˆt2, ˆt3 of t1, t2, t3 by
solving the system of equations
ˆT1 = 1
3
 ˆt1 + ˆt2 + ˆt3

,
ˆT2 = 1
3
 ˆt2
1 + ˆt2
2 + ˆt2
3

,
ˆT3 = 1
3
 ˆt3
1 + ˆt3
2 + ˆt3
3

.
We recover the Newton–Girard polynomial by computing the successive
elementary symmetric polynomials Π1, Π2, Π3, using (5.4)
Π1 = 3 ˆT1,
Π2 = −1
2
ˆT2 + 9
2
ˆT 2
1 ,
Π3 = ˆT3 −15
2
ˆT2 ˆT1 + 27
2
ˆT 3
1 .
The three roots of the equation
X3 −Π1X2 + Π2X −Π3 = 0
are the estimates ˆt1, ˆt2, ˆt3 of t1, t2, t3.
However, this method has several major practical drawbacks.
• Inverting the Newton–Girard equation does not ensure that the solutions are
all real, since the estimator is not constrained to be real. When running the
previous algorithm for not too large N, a large portion of the estimated
eigenvalues are indeed returned as purely complex. When this happens, it
is diﬃcult to decide what to do with the algorithm output. The G-estimator
of Theorem 8.4 on the opposite, being an averaged sum of the eigenvalues of
non-negative deﬁnite, necessarily provides real positive estimates;

220
8. Eigen-inference
• from the system of Equations (8.19), it turns out that the kth order moment
Tk is determined by the moments B1, . . . , Bk. As a consequence, when
substituting ˆBi to Bi, 1 ≤i ≤k, in (8.19), a small diﬀerence between the
limiting Bi and the empirical ˆBi entails possibly large estimation errors in all
Tk, k ≥i. This engenders a snowball eﬀect on the resulting estimates ˆTk for
larger k, this eﬀect being increased by the intrinsic growing error between Bi
and ˆBi for growing i.
On the positive side, while the G-estimators based on complex analysis are to
this day not capable of coping with situations when successive clusters overlap,
the moment approach is immune against such situations. The performance of the
moment technique against the G-estimator proposed in Section 8.1.2 is provided
in Figure 8.4 for the same scenario as in Figure 8.1. We can observe that, although
asymptotically unbiased (as H is uniquely determined by its ﬁrst free moments),
the moment-based estimator performs very inaccurately compared to the G-
estimator of Theorem 8.4. Note that in the case N = 30, n = 90, some of the
estimates were purely complex and were discarded; running simulations for the
scenario N = 6, n = 18, as in Figure 8.1 leads to even worse results, most of which
being purely complex. Now, the limitations of this approach can be corrected by
paying more attention on the aforementioned snowball eﬀect for the moment
estimates. In particular, thanks to Theorem 3.17, we know that, for the sample
covariance matrix model under study, the k-multivariate random variable

N
Z
xd[F −F BN ](x) , N
Z
x2d[F −F BN ](x) , . . . , N
Z
xkd[F −F BN ](x)
T
has a central limit with covariance matrix given in Corollary 3.3. As a
consequence, an alternative estimate ˆt(k)
ML ≜(ˆt(k)
ML,1, . . . , ˆt(k)
ML,K)T of the K masses
in H is the maximum likelihood (ML) estimate for (t1, . . . , tK)T based on the
observation of k successive moments of BN. This is given by:
ˆt(k)
ML = arg min
t (ˆb −b(t))TQ(t)−1(ˆb −b(t)) + log det Q(t)
where t = (T1, . . . , Tk)T, ˆb = ( ˆB1, . . . , ˆBk)T, b(t) = (B1, . . . , Bk) assuming Tk =
1
K
PK
i=1 tk
i , and Q(t) is obtained as in (3.27); see [Masucci et al., 2011; Rao
et al., 2008] for more details. This method is however computationally expensive
in this form, since all vectors t must be tested, for which every time Q(t)
has to be evaluated. Suboptimal methods are usually envisioned to reduce the
computational complexity of the ML estimate down to a reasonable level. In
Chapter 17, such methods will be discussed for more elaborate models.

8.2. Moment deconvolution approach
221
1
3
10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Estimated tk
Density
Moment-based estimator
G-estimator, Theorem 8.4
1
3
10
0
2
4
6
Estimated tk
Density
Moment-based estimator
G-estimator, Theorem 8.4
Figure 8.4 Estimation of t1, t2, t3 in the model BN = T
1
2
NXNXH
NT
1
2
N based on ﬁrst
three empirical moments of BN and Newton–Girard inversion, for
N1/N = N2/N = N3/N = 1/3 ,N/n = 1/10, for 100 000 simulation runs; Top N = 30,
n = 90, bottom N = 90, n = 270. Comparison is made against the G-estimator of
Theorem 8.4.


9
Extreme eigenvalues
This last chapter of Part I introduces very recent mathematical advances of deep
interest to the ﬁeld of wireless communications, related to the limiting behavior
of the extreme eigenvalues and of their corresponding eigenvectors. Again, the
main objects which have been extensively studied in this respect are derivatives
of the sample covariance matrix and of the information plus noise matrix.
This chapter will be divided into two sections, whose results emerge from
two very diﬀerent random matrix approaches. The ﬁrst results, about the
limiting extreme eigenvalues of the spiked models, unfold from the previous exact
separation results described in Chapter 7. It will in particular be proved that
in a sample covariance matrix model, when all population eigenvalues are equal
but for the few largest ones, the l.s.d. of the sample covariance matrix is still
the Mar˘cenko–Pastur law, but a few eigenvalues may now be found outside the
support of the l.s.d. The second set of results concerns mostly random matrix
models with Gaussian entries, for which limiting results on the behavior of
extreme eigenvalues are available. These results use very diﬀerent approaches
than those proposed so far, namely the theory of orthogonal polynomials and
determinantal representations. This subject, which requires many additional
tools, is brieﬂy introduced in this chapter. For more information about these
tools, see, e.g. the tutorial [Johnstone, 2006] or the book [Mehta, 2004].
We start this section with the spiked models.
9.1
Spiked models
We ﬁrst discuss the sample covariance matrix model, which can be seen under
the spiked model assumption as a perturbed sample covariance matrix with
identity population covariance matrix. We will then move to a diﬀerent set
of models, using free probability arguments in random matrix models with
rotational invariance properties.

224
9. Extreme eigenvalues
9.1.1
Perturbed sample covariance matrix
Let us consider the so-called spiked models for the sample covariance matrix
model BN = T
1
2
NXNXH
NTN, with XN ∈CN×n random with i.i.d. entries of
zero mean and variance 1/n, which arise whenever the e.s.d. of the population
covariance matrix TN ∈CN×N contains a few outlying eigenvalues. What we
mean by “a few outlying eigenvalues” is described in the following. Assume the
e.s.d. of the series of matrices T1, T2, . . . converges weakly to some d.f. H and
denote τ1, . . . , τN the eigenvalues of TN. Consider now M integers k1, . . . , kM.
Consider also a set α1, . . . , αM of non-negative reals taken outside the union of
the sets {τ1, . . . , τN} for all N. Then the e.s.d. of the series ¯T1, ¯T2, . . . of diagonal
matrices given by:
¯TN = diag(α1, . . . , α1
|
{z
}
k1
, . . . , αM, . . . , αM
|
{z
}
kM
, τ1, . . . , τN−PM
i=1 ki)
also converges to H as N →∞, with M and the αk kept ﬁxed. Indeed, the ﬁnitely
many eigenvalues α1, . . . , αM of ﬁnite multiplicities will have null measure in
the asymptotic set of eigenvalues of ¯TN when N →∞. These eigenvalues will
however lie outside the support of H.
The question that now arises is whether those α1, . . . , αK will induce the
presence of some eigenvalues of
¯BN ≜¯T
1
2
NXNXH
N ¯T
1
2
N outside the limiting
support of the l.s.d. ¯F of ¯BN. First, it is clear that ¯F = F. Indeed, from Theorem
3.13, F is uniquely determined by H, and therefore the limiting distribution
¯F of ¯BN is nothing but F itself. If there are eigenvalues found outside the
support of F, they asymptotically contribute with no mass. These isolated
eigenvalues will then be referred to as spikes in the following, as they will be
outlying asymptotically zero weight eigenvalues. It is important at this point
to remind that the existence of eigenvalues outside the support of F is not
at all in contradiction with Theorem 7.1. Indeed, Theorem 7.1 precisely states
that (with probability one), for all large N, there is no eigenvalue of ¯BN in a
segment [a, b] contained both in the complementary of the support of F and in
the complementary of the supports of ¯FN, determined by the solutions of (7.1),
for all large N. In the case where τj = 1 for all j ≥PM
i=1 ki, ¯F(x) = limN ¯FN(x)
is the Mar˘cenko–Pastur law, which does not exclude ¯FN from containing large
eigenvalues of mass ki/N →0; therefore, it is possible for ¯BN to asymptotically
have eigenvalues outside the support of F.
The importance of spiked models in wireless communications arises when
performing signal sensing, where the (hypothetical) signal space has small
dimension compared to the noise space. In this case, we might wish to be able
to decide on the presence of a signal based on the spectrum of the sample
covariance matrix ¯BN. Typically, if an eigenvalue is found outside the predicted
noise spectrum of ¯BN, then this must indicate the presence of a signal bearing
informative data, while if all the eigenvalues are inside the limiting support, then

9.1. Spiked models
225
this should indicate the absence of such a signal. More on this is discussed in
detail in Chapter 16.
However, as we will show in the sequel, it might not always be true that a
spike in ¯TN results in a spike in ¯BN found outside the support of F, in the sense
that the support of ¯FN may “hide” the spike in some sense. This is especially
true when the size of the main clusters of eigenvalues (linked to the ratio N/n)
is large enough to “absorb” the spike of ¯BN that would have resulted from the
population spike of ¯TN. In this case, for signal detection purposes, whether a
signal bearing informative data is present or not, there is no way to decide on
the presence of this signal by simply looking at the asymptotic spectrum. The
condition for decidability when TN = IN is given in the following result.
Theorem 9.1 ([Baik and Silverstein, 2006]). Let ¯BN = ¯T
1
2
NXNXH
N ¯T
1
2
N, where
XN ∈CN×n has i.i.d. entries of zero mean, variance 1/n, and fourth order
moment of order O(1/n2), and ¯TN ∈RN×N is diagonal given by:
¯TN = diag(α1, . . . , α1
|
{z
}
k1
, . . . , αM, . . . , αM
|
{z
}
kM
, 1, . . . , 1
| {z }
N−PM
i=1 ki
)
with α1 > . . . > αM > 0 for some M. We denote here c = limN N/n. Call M0 =
#{j, αj > 1 + √c}. For c < 1, take also M1 to be such that M −M1 = #{j, αj <
1 −√c}. Denote additionally λ1, . . . , λN the eigenvalues of ¯BN, ordered as λ1 ≥
. . . ≥λN. We then have
• for 1 ≤j ≤M0, 1 ≤i ≤kj
λk1+...+kj−1+i
a.s.
−→αj +
cαj
αj −1
• for the other eigenvalues, we must discriminate upon c
– if c < 1
* for M1 + 1 ≤j ≤M, 1 ≤i ≤kj
λN−kj−...−kM+i
a.s.
−→αj +
cαj
αj −1
* for the indexes of eigenvalues of ¯TN inside [1 −√c, 1 + √c]
λk1+...+kM0+1
a.s.
−→(1 + √c)2
λN−kM1+1−...−kM
a.s.
−→(1 −√c)2
– if c > 1
λn
a.s.
−→(1 −√c)2
λn+1 = . . . = λN = 0
– if c = 1
λmin(n,N)
a.s.
−→0.

226
9. Extreme eigenvalues
Therefore, when c is large enough, the segment [max(0, 1 −√c), 1 + √c] will
contain some of the largest eigenvalues of ¯TN (those closest to one). If this occurs
for a given αk, the corresponding eigenvalues of ¯BN will be “attracted” by the
left or right end of the support of the l.s.d. of ¯BN. If c < 1, small population
spikes αk in ¯TN may generate spikes of ¯BN in the interval (0, (1 −√c)2); when
c > 1, though, the αk smaller than one will result in null eigenvalues of ¯BN.
Remember from the exact separation theorem, Theorem 7.2, that there is a
correspondence between the eigenvalues of ¯TN and those of ¯BN inside each
cluster. Since exactly {α1, . . . , αM0} are above 1 + √c then, asymptotically,
exactly k1 + . . . + kM0 eigenvalues will lie on the right-end side of the support
of the Mar˘cenko–Pastur law. This is depicted in Figure 9.1 where we consider
M = 2 spikes α1 = 2 and α2 = 3, both of multiplicity k1 = k2 = 2. We illustrate
the decidability condition depending on c by considering ﬁrst c = 1/3, in which
case 1 + √c ≃1.57 < α1 < α2 and then we expect two spikes of ¯BN at position
α1 + cα1(α1 −1)−1 ≃2.67 and two spikes of ¯BN at position α2 + cα2(α2 −
2)−1 = 3.5. We then move c to c = 5/4 for which α1 < 1 + √c ≃2.12 < α2; we
therefore expect only the two eigenvalues associated with α2 at position α2 +
cα2(α2 −2)−1 ≃4.88 to lie outside the spectrum of F. This is approximately
what is observed.
The fact that spikes are non-discernible for large c leads to a seemingly
paradoxical situation. Consider indeed that the sample space if ﬁxed to n samples
while the population space of dimension N increases, so that we increase the
collection of input data to improve the quality of the experiment. In the context
of signal sensing, if we rely only on a global analysis of the empirical eigenvalues
of the input covariance matrix to declare that “if eigenvalues are found outside
the support, a signal is detected,” then we are better oﬀlimiting N to a minimal
value and therefore we are better oﬀwith a mediocre quality of the experiment;
otherwise the decidability threshold is severely impacted. This point is critical
and it is essential to understand that the problem here lies in the non-suitability
of the decision criterion (that consists just in looking at the eigenvalues outside
or inside the support) rather than in the intrinsic non-decidable nature of the
problem, which for ﬁnite N is not true. If N is large and such that there is no
spike outside the support of F while ¯TN does have spikes, then we will need to
look more closely into the tail of the Mar˘cenko–Pastur law, which, for ﬁxed N,
contains more than the usual amount of eigenvalues; however, we will see that
even this strategy is bound to fail, for very large N. In this case, we may have
to resort to studying the joint eigenvalue distribution of ¯BN, which contains
the full information. In Chapter 16, we will present a scheme for signal sensing,
which aims at providing an optimal sensing decision, based on the complete joint
eigenvalue distribution of the input signals, instead of assuming large dimensional
assumptions. In these scenarios, the rule of thumb that suggests that small
dimensional systems are well approximated by large dimensional analysis now
fails, and a signiﬁcant advantage is provided by the small dimensional analysis.

9.1. Spiked models
227
α1 +
cα1
α1−1, α2 +
cα2
α2−1
0
0.2
0.4
0.6
0.8
Eigenvalues
Density
Mar˘cenko–Pastur law, c = 1/3
Empirical eigenvalues
α2 +
cα2
α2−1
0
0.2
0.4
0.6
0.8
1
1.2
Eigenvalues
Density
Mar˘cenko–Pastur law, c = 5/4
Empirical eigenvalues
Figure 9.1 Eigenvalues of ¯BN = ¯T
1
2
NXNXN
H ¯T
1
2
N, where ¯TN is a diagonal of ones but
for the ﬁrst four entries set to {3, 3, 2, 2}. On top, N = 500, n = 1500. One the
bottom, N = 500, n = 400. Theoretical limit eigenvalues of ¯BN are stressed.
Another way of observing practically when the e.s.d. at hand is close to the
Mar˘cenko–Pastur law F is to plot the empirical eigenvalues against the quantiles
F −1( k−1/2
N
) for k = 1, . . . , N. This is depicted in Figure 9.2, for the case c = 1/3
with the same set of population spikes {2, 2, 3, 3} in ¯TN as before. We observe
again the presence of four outlying eigenvalues in the e.s.d. of ¯BN.
We subsequently move to a diﬀerent type of results, dealing with the
characterization of the extreme eigenvalues and eigenvectors of some perturbed
unitarily invariant matrices. These recent results are due to the work of Benaych-
Georges and Rao [Benaych-Georges and Rao, 2011].

228
9. Extreme eigenvalues
(1 −√c)2
(1 + √c)2
(1 −√c)2
(1 + √c)2
Eigenvalues
Quantiles
Empirical eigenvalues
y = x
Figure 9.2 Eigenvalues of B′
N = T′
N
1
2 X′
NX′
N
HT′
N
1
2 , where T′
N is a diagonal of ones
but for the ﬁrst four entries set to {3, 3, 2, 2}, against the quantiles of the
Mar˘cenko–Pastur law, N = 500, n = 15 000, c = 1/3.
9.1.2
Perturbed random matrices with invariance properties
In [Benaych-Georges and Rao, 2011], the authors consider perturbations of
unitarily invariant random matrices (or random matrices with unitarily invariant
perturbation). What is meant by perturbation is either the addition of a small
rank matrix to a large dimensional random matrix, or the product of a large
dimensional random matrix by a perturbed identity matrix in the sense just
described. Thanks to the unitarily invariance property of either of the two
matrices, we obtain the following very general results.
Theorem 9.2 ([Benaych-Georges and Rao, 2011]). Let XN ∈CN×N be a
Hermitian random matrix with ordered eigenvalues λN
1 ≥. . . ≥λN
N for which
we assume that the e.s.d. F XN converges almost surely toward F with compact
support with inﬁmum a and supremum b, such that λN
1
a.s.
−→b and λN
N
a.s.
−→a.
Consider also a perturbation matrix AN of rank r, with ordered non-zero
eigenvalues aN
1 ≥. . . ≥aN
r . Denote s the integer such that as > 0 > as+1. We
further assume that either XN or AN (or both) are bi-unitarily invariant. Denote
YN the matrix deﬁned as
YN = XN + AN
with ordered eigenvalues νN
1 ≥. . . ≥νN
N . Then, as N grows large, for i ≥1
νN
i
a.s.
−→
 −m−1
F (1/ai) , if 1 ≤i ≤r and 1/ai < −mF (b+)
b
, otherwise

9.1. Spiked models
229
where the inverse is with respect to composition. Also, for i ≥0
νN
n−i
a.s.
−→
 −m−1
F (1/ar−i) , if i < r −s and 1/ai > −mF (a−)
a
, otherwise.
We also have the same result for multiplicative matrix perturbations, as
follows.
Theorem 9.3. Let XN and AN be deﬁned as in Theorem 9.2. Denote ZN the
matrix
ZN = XN (IN + AN)
with ordered eigenvalues µN
1 ≥. . . ≥µN
N. Then, as N grows large, for i ≥1
µN
i
a.s.
−→
 ψ−1
F (ai) , if 1 ≤i ≤s and 1/ai < ψF (b+)
b
, otherwise
and, for i ≥0
µN
n−r+i
a.s.
−→
ψ−1
F (ai) , if i < r −s and 1/ai > ψF (a−)
a
, otherwise
where ψF is the ψ-transform of F, deﬁned in (4.3) as
ψF (z) =
Z
t
z −tdF(t) = −1 −1
z mF
1
z

.
This result in particular encompasses the case when XN = WNWH
N, with
WN ﬁlled with i.i.d. Gaussian entries, perturbed in the sense of Theorem 9.1. In
this sense, this result generalizes Theorem 9.1 for unitarily invariant matrices,
although it does not encompass the general i.i.d. case of Theorem 9.1. Recent
extensions of the above results on the second order ﬂuctuations of the extreme
eigenvalues can be found in [Benaych-Georges et al., 2010].
As noticed above, the study of extreme eigenvalues carries some importance
in problems of detection of signals embedded in white noise, but not only. Fields
such as speech recognition, statistical learning, or ﬁnance also have interests in
extreme eigenvalues of covariance matrices. For the particular case of ﬁnance, see,
e.g., [Laloux et al., 2000; Plerous et al., 2002], consider XN is the N × n matrix
in which each row stands for a market product, while every column stands for a
time period, say a month, as already presented in Chapter 1. The (i, j)th entry of
XN contains the evolution of the market index for product i in time period j. If
all time-product evolutions are independent in the sense that the evolution of the
value of product A for a given month does not impact the evolution of the value of
product B, then it is expected that the rows of XN are statistically independent.
Also, if the time scale is chosen such that the evolution of the price of product
A over a given time period is roughly uncorrelated with its evolution on the
subsequent time period, then the columns will also be statistically independent.

230
9. Extreme eigenvalues
Therefore, after proper centralization and normalization of the entries (to ensure
constant variance), it is expected that, if N, n are large, the empirical eigenvalue
distribution of XNXH
N follows the Mar˘cenko–Pastur law. If not, i.e. if some
eigenvalues are found outside the support, then there exist some non-trivial
correlation patterns in XN. The largest eigenvalue here allows the trader to
anticipate the largest possible gain to be made if he aligns his portfolio on
the corresponding eigenvector. Also, as is the basic rule in ﬁnance, the largest
eigenvalue gives a rough idea of the largest possible risk of investment in the
market (the more gain we expect, the more risky).
Studies about the limiting distribution of the largest eigenvalue are carried out
in the subsequent section, in the speciﬁc case where the random matrix under
study has Gaussian entries.
9.2
Distribution of extreme eigenvalues
In practical applications, since the stochastic matrix observations have sometimes
small dimensions, it is fundamental to study the statistical variations of the
extreme eigenvalues. Indeed, let us consider the case of the observation of a
sample covariance matrix, upon which the experimenter would like to decide
whether the population eigenvalue matrix is either an identity matrix or a
perturbed identity matrix. A ﬁrst idea is then to decide whether the observed
largest eigenvalue is inside or outside the support of the Mar˘cenko–Pastur law.
However, due to the ﬁnite dimensionality of the matrix, there is a non-zero
probability for the largest observed eigenvalue to lie outside the support. Further
information on the statistical distribution of the largest eigenvalue of sample
covariance matrices is then required, to be able to design adequate hypothesis
tests. We will come back to these practical considerations in Chapter 16.
The study of the second order statistics, i.e. limit theorems on the largest
eigenvalues, took oﬀin the mid-nineties initiated by the work of Tracy and
Widom [Tracy and Widom, 1996] on N × N (Wigner) Hermitian matrices with
i.i.d. Gaussian entries above the diagonal. Before introducing the results from
Tracy and Widom, though, we hereafter provide some notions of orthogonal
polynomials in order to understand how these results are derived. This
introduction is based on the tutorial [Fyodorov, 2005].
9.2.1
Introduction to the method of orthogonal polynomials
To study the behavior of some particular eigenvalue of a random matrix, it
is required to study its marginal distribution. Calling P ≤
(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N)
the joint density of the ordered eigenvalues λN
1 ≤. . . ≤λN
N of some Hermitian

9.2. Distribution of extreme eigenvalues
231
random matrix XN ∈CN×N, the largest eigenvalue λN
N has density
PλN
N (λN
N) =
Z
λN
1
. . .
Z
λN
N−1
P ≤
(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N)dλN
1 . . . dλN
N.
(9.1)
In the case where the order of the eigenvalues is irrelevant, we have that
P(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N) = 1
N! P ≤
(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N)
with P(λN
1 ,...,λN
N) the density of the unordered eigenvalues.
From now on, the eigenvalue indexes 1, . . . , N are considered to be just labels
instead of ordering indexes. From the above equality, it is equivalent, and as will
turn out actually simpler, to study the unordered eigenvalue distribution rather
than the ordered eigenvalue distribution. In the particular case of a zero Wishart
matrix with n ≥N degrees of freedom, this property holds and we have from
Theorem 2.3 that
P(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N) = e−PN
i=1 λN
i
N
Y
i=1
(λN
i )n−N
(n −i)!i!
Y
i<j
(λN
i −λN
j )2.
Similarly, we have for Gaussian Wigner matrices [Tulino and Verd´u, 2004], i.e.
Wigner matrices with upper-diagonal entries complex standard Gaussian and
diagonal entries real standard Gaussian
P(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N) =
1
(2π)
N
2 e−PN
i=1(λN
i )2
N
Y
i=1
1
i!
Y
i<j
(λN
i −λN
j )2.
(9.2)
The
problem
now
is
to
be
able
to
compute
the
multi-dimensional
marginalization for either of the above distributions, or for more involved
distributions. We concentrate on the simpler Gaussian Wigner case in what
follows.
To be able to handle the marginalization procedure, we will use the reproducing
kernel property, given below which can be found in [Deift, 2000].
Theorem 9.4. Let Kn ∈Cn×n with (i, j) entry Kij = f(xi, xj) for some
complex-valued function f of two real variables and a real vector x = (x1, . . . , xn).
The function f is said to satisfy the reproducing kernel property with respect to
a real measure µ if
Z
f(x, y)f(y, z)dµ(y) = f(x, z).
Under this condition, we have that
Z
det Kndµ(xn) = (q −(n −1)) det Kn−1
(9.3)
with
q =
Z
f(x, x)dµ(x).

232
9. Extreme eigenvalues
The above property is interesting in the sense that, if such a reproducing
kernel property can be exhibited, then we can successively iterate (9.3) in order
to perform marginalization calculus such as in (9.1).
By working on the expression of the eigenvalue distribution of Gaussian Wigner
matrices (9.2), it is possible to write P(λN
1 ,...,λN
N) under the form
P(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N) = C det
n
e−1
2 (λN
j )2πi−1(λN
j )
o
1≤i,j≤N
2
for any set of polynomials (π0, . . . , πN−1) with πk of degree k and leading
coeﬃcient 1, and for some normalizing constant C. A proof of this fact stems
from similar arguments as for the proof of Lemma 16.1, namely that the matrix
above can be written under the form of the product of a diagonal matrix with
entries e
1
2 (λN
j )2 and a matrix with polynomial entries πi(xj), the determinant
of which is proportional to the product of e
P
j(λN
j )2 times the Vandermonde
determinant Q
i<j(λN
i −λN
j ).
Now, since we have the freedom to take any set of polynomials (π0, . . . , πN−1)
with leading coeﬃcient 1, we choose a set of orthogonal polynomials with respect
to the weighting coeﬃcient e−x2, i.e. we deﬁne (π0, . . . , πN−1) to be such that
Z
e−x2πi(x)πj(x)dx = δj
i .
Denoting now KN ∈CN×N the matrix with (i, j) entry
Kij = kN(λN
i , λN
j ) ≜
N−1
X
k=0
h
e−1
2 (λN
i )2πk(λN
i )
i h
e−1
2 (λN
j )2πk(λN
j )
i
we observe easily, from the fact that det(A2) = det(ATA), that
P(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N) = C det KN.
From the construction of KN, through the orthogonality of the polynomials
π0(x), . . . , πN−1(x), we have that
Z
kN(x, y)kN(y, z)dy = kN(x, y)
and the function kN has the reproducing kernel property.
This ensures that
Z
. . .
Z
det KNdxk+1 . . . dxN = (N −k)! det Kk
where the term (N −k)! follows from the computation of
R
kn(x, x)dx for n ∈
{k + 1, . . . , N}.
To ﬁnally compute the probability distribution of the largest eigenvalue, note
that the probability that it is greater than ξ is complementary to the probability
that there is no eigenvalue in B = (ξ, ∞). The latter, called the hole probability,

9.2. Distribution of extreme eigenvalues
233
expresses as
QN(B) =
Z
. . .
Z
P(λN
1 ,...,λN
N)(λN
1 , . . . , λN
N)
N
Y
k=1
(1 −1B(λN
k ))dλN
1 . . . dλN
N.
From the above discussion, expanding the product term, this can be shown to
express as
QN(B) =
N
X
i=0
(−1)i 1
i!
Z
B
. . .
Z
B
det Ki dλN
1 . . . dλN
i .
This last expression is in fact a Fredholm determinant, denoted det(I −KN),
where KN is called an integral operator with kernel kN acting on square
integrable functions on B. These Fredholm determinants are well-studied objects,
and it is in particular possible to derive the limiting behavior of QN(B) as N
grows large, which leads presently to the complementary of the Tracy–Widom
distribution, and to results such as Theorem 9.5.
Before completing this short introduction, we also mention that alternative
contributions such as the recent work of Tucci [Tucci, 2010] establish expressions
for functionals of eigenvalue distributions, without resorting to the orthogonal
polynomial machinery. In [Tucci, 2010], Tucci provides in particular a closed-form
formula for the quantity
Z
f(t)dF XNTNXH
N (t)
for XN ∈CN×n a random matrix with Gaussian entries of zero mean and unit
variance and TN a deterministic matrix, given under the form of the determinant
of a matrix with entries given in an integral form of f and the eigenvalues of
TN. This form is not convenient in its full expression, although it constitutes a
ﬁrst step towards the generalization of the average spectral analysis of Gaussian
random matrices. Incidentally, Tucci provides a novel integral expression of the
ergodic capacity of a point-to-point 2 × 2 Rayleigh fading MIMO channel.
Further information on the tools above can be found in the early book from
Mehta [Mehta, 2004], the very clear tutorial from Fyodorov [Fyodorov, 2005], and
the course notes from Guionnet [Guionnet, 2006], among others. In the following,
we introduce the main results concerning limit laws of extreme eigenvalues known
to this day.
9.2.2
Limiting laws of the extreme eigenvalues
The major result on the limiting density of extreme eigenvalues is due to Tracy
and Widom. It comes as follows.
Theorem 9.5 ([Tracy and Widom, 1996]). Let XN ∈CN×N be Hermitian
with independent Gaussian oﬀ-diagonal entries of zero mean and variance 1/N.

234
9. Extreme eigenvalues
Denote λ−
N and λ+
N the smallest and largest eigenvalues of XN, respectively.
Then, as N →∞
N
2
3  λ+
N −2

⇒X+ ∼F +
N
2
3  λ−
N + 2

⇒X−∼F −
where F + is the Tracy–Widom law given by:
F +(t) = exp

−
Z ∞
t
(x −t)2q2(x)dx

(9.4)
with q the Painlev´e II function that solves the diﬀerential equation
q′′(x) = xq(x) + 2q3(x)
q(x) ∼x→∞Ai(x)
in which Ai(x) is the Airy function, and F −is deﬁned as
F −(x) ≜1 −F +(−x).
This theorem is in fact extended in [Tracy and Widom, 1996] to a more
general class of matrix spaces, including the space of real symmetric matrices
and that of quaternion-valued symmetric matrices, with Gaussian i.i.d. entries.
Those are therefore all special cases of Wigner matrices. The space of Gaussian
real symmetric matrices is referred to as the Gaussian orthogonal ensemble
(denoted GOE), that of complex Gaussian Hermitian matrices is referred to as
the Gaussian unitary ensemble (GUE), and that of quaternion-valued symmetric
Gaussian matrices is referred to as the Gaussian symplectic ensemble (GSE). The
seemingly strange “orthogonal” and “unitary” denominations arise from deeper
considerations on these ensembles, involving orthogonal polynomials, see, e.g.,
[Faraut, 2006] for details.
It was later shown [Bianchi et al., 2010] that the random variables λ+
N and λ−
N
are asymptotically independent, giving therefore a simple description of their
ratio, the condition number of XN.
Theorem 9.6 ([Bianchi et al., 2010]). Under the assumptions of Theorem 9.5

N
2
3  λ+
N −2

, N
3
2  λ−
N + 2

⇒(X+, X−)
where X+ and X−are independent random variables with respective distributions
F + and F −. The random variable λ+
N/λ−
N satisﬁes
N
2
3
λ+
N
λ−
N
+ 1

⇒−1
2
 X+ + X−
.
The result of interest to our study of extreme eigenvalues of Wishart and
perturbed Wishart matrices was proposed later on by Johansson for the
largest eigenvalue in the complex case [Johansson, 2000], followed by Johnstone

9.2. Distribution of extreme eigenvalues
235
[Johnstone, 2001] for the largest eigenvalue in the real case, while it took ten
years before Feldheim and Sodin provided a proof of the result on the smallest
eigenvalue in both real and complex cases [Feldheim and Sodin, 2010]. We only
mention here the complex case.
Theorem 9.7 ([Feldheim and Sodin, 2010; Johansson, 2000]). Let XN ∈CN×n
be a random matrix with i.i.d. Gaussian entries of zero mean and variance 1/n.
Denoting λ+
N and λ−
N the largest and smallest eigenvalues of XNXH
N, respectively,
we have:
N
2
3 λ+
N −(1 + √c)2
(1 + √c)
4
3 √c
⇒X ∼F +
N
2
3 λ−
N −(1 −√c)2
−(1 −√c)
4
3 √c
⇒X ∼F +
as N, n →∞with c = limN N/n < 1 and F + the Tracy–Widom distribution
deﬁned in Theorem 9.5. Moreover, the convergence result for λ+
N holds also for
c ≥1.
The empirical against theoretical distributions of the largest eigenvalues of
XNXH
N are depicted in Figure 9.3, for N = 500, c = 1/3.
Observe that the Tracy–Widom law is largely weighted on the negative half
line. This means that the largest eigenvalue of XNXH
N has a strong tendency
to lie much inside the support of the l.s.d. rather than outside. For the same
scenario N = 500, c = 1/3, we now depict in Figure 9.4 the Tracy–Widom law
against the empirical distribution of the largest eigenvalue of T
1
2
NXNXH
NT
1
2
N in
the case where TN ∈RN×N is diagonal composed of all ones but for T11 = 1.5.
From Theorem 7.2, no eigenvalue is found outside the asymptotic spectrum of
the Mar˘cenko–Pastur law. Figure 9.4 suggests that the largest eigenvalue of
T
1
2
NXNXH
NT
1
2
N does not converge to the Tracy–Widom law since it shows a much
heavier tail in the positive side; this is however not true asymptotically. The
asymptotic limiting distribution of the largest eigenvalue of T
1
2
NXNXH
NT
1
2
N is
still the Tracy–Widom law, but the convergence towards the second order limit
arises at a seemingly much slower rate. This is proved in the following theorem.
To appreciate the convergence towards the Tracy–Widom law, N must then be
taken much larger.
Theorem 9.8 ([Baik et al., 2005]). Let XN ∈CN×n have i.i.d. Gaussian
entries of zero mean and variance 1/n and TN = diag(τ1, . . . , τN) ∈RN×N.
Assume, for some ﬁxed r and k, τr+1 = . . . = τN = 1 and τ1 = . . . = τk while
τk+1, . . . , τr lie in a compact subset of (0, τ1). Assume further that the ratio N/n
is constant, equal to c < 1 as N, n grow. Denoting λ+
N the largest eigenvalue of
T
1
2
NXNXH
NT
1
2
N, we have:

236
9. Extreme eigenvalues
• If τ1 < 1 + √c
N
2
3 λ+
N −(1 + √c)2
(1 + √c)
4
3 √c
⇒X+ ∼F +
with F + the Tracy–Widom distribution.
• If τ1 > 1 + √c

τ 2
1 −
τ 2
1 c
(τ1 −1)2
 1
2
n
1
2

λ+
N −

τ1 +
τ1c
τ1 −1

⇒Xk ∼Gk
with Gk the distribution function of the largest eigenvalue of the k × k GUE,
given by:
Gk(x) = 1
Zk
Z x
−∞
· · ·
Z x
−∞
Y
1≤i<j≤k
|ξi −ξj|2
k
Y
i=1
e−1
2 ξ2
i dξ1 . . . dξk.
with Zk a normalization constant. In particular, G1(x) is the Gaussian
distribution function.
The result on spiked eigenvalues was recently extended by Bai and Yao
[Bai and Yao, 2008a] to the analysis of the ﬂuctuations of the smallest
eigenvalues, when c < 1. Precisely, taking now τk+1, . . . , τr in a compact subset
of (τ1, ∞), if τ1 < 1 −√c, the smallest eigenvalue λ−
N of T
1
2
NXNXH
NT
1
2
N follows
the distribution Gk after identical centering and scaling as in Theorem 9.8. A
further generalization is provided in [Bai and Yao, 2008b] for the case where the
weak limit of the e.s.d. is not restricted to the Mar˘cenko–Pastur law.
The corollary of Theorem 9.8 is that, if the largest population eigenvalue (i.e.
the largest population spike) is not large enough for any eigenvalue of the sample
covariance matrix to escape the support of the Mar˘cenko–Pastur law, whatever
its multiplicity k, then the asymptotic distribution of the largest eigenvalue is
the Tracy–Widom law. This conﬁrms that the behavior observed in Figure 9.4
has not reached its asymptotic limit. Theorem 9.8 goes further by stating that, if
on the contrary τ1 is larger than the transition limit 1 + √c, where an eigenvalue
will be found outside the support of the Mar˘cenko–Pastur law, then, if k = 1,
the largest eigenvalue in T
1
2
NXNXH
NT
1
2
N has a central limit with convergence rate
O(n
1
2 ) instead of the Tracy–Widom rate O(n
2
3 ) when τ1 < 1 + √c. This sudden
convergence rate change is referred to by the author in [Baik et al., 2005] as a
phase transition. The case τ1 = 1 + √c is also treated in [Baik et al., 2005] that
shows that λ+
N converges in distribution to yet another law Fk, depending on
the multiplicity k of τ1, with rate O(n
2
3 ); the law Fk does not have a simple
expression though. Since the case τ1 = 1 + √c is highly improbable for practical
applications, this case was deliberately discarded from Theorem 9.8.
On the other hand, this nice result is yet another disappointment for
signal detection applications. Remember that one of our initial motivations
to investigate further the asymptotic distribution of the largest eigenvalue of

9.3. Random matrix theory and eigenvectors
237
−4
−2
0
2
4
0
0.1
0.2
0.3
0.4
0.5
Centered-scaled largest eigenvalue of XXH
Density
Empirical eigenvalues
Tracy–Widom law F +
Figure 9.3 Density of N
2
3 c−1
2 (1 + √c)−4
3 
λ+
N −(1 + √c)2
against the Tracy–Widom
law for N = 500, n = 1500, c = 1/3, for the covariance matrix model XXH of
Theorem 9.6. Empirical distribution taken over 10 000 Monte-Carlo simulations.
T
1
2
NXNXH
NT
1
2
N was the inability to visually determine the presence of a spike
τ1 < 1 + √c from the asymptotic spectrum of T
1
2
NXNXH
NT
1
2
N. Now, it turns out
that even the distribution of the largest eigenvalue in that case is asymptotically
the same as that when TN = IN. There is therefore not much left to be done in
the asymptotic regime to perform signal detection under the detection threshold
1 + √c. In that case, we may resort to further limit orders, or derive exact
expressions of the largest eigenvalue distribution. Similar considerations are
addressed in Chapter 16.
We also mention that, in the real case XN ∈RN×n, if τ1 > 1 + √c has
multiplicity one, Paul proves that the limiting distribution of λ+
N −

τ1 +
τ1c
τ1−1

is still Gaussian but with variance double that of the complex case, i.e.
2
n

τ 2
1 −
τ 2
1 c
(τ1−1)2

[Paul, 2007]. Theorem 9.8 is also extended in [Karoui, 2007]
to more general Gaussian sample covariance matrix models, where it is proved
that under some conditions on the population covariance matrix, for any integer
k ﬁxed, the largest k eigenvalues of the sample covariance matrix have a Tracy–
Widom distribution with the same scaling factor but diﬀerent centering and
scaling coeﬃcients.
9.3
Random matrix theory and eigenvectors
Fewer results are known relative to the limiting distribution of the largest
eigenvectors. We mention in the following the limiting distribution of the largest

238
9. Extreme eigenvalues
−4
−2
0
2
4
0
0.1
0.2
0.3
0.4
0.5
Centered-scaled largest eigenvalue of XXH
Density
Empirical eigenvalues
Tracy–Widom law F +
Figure 9.4 Distribution of N
2
3 c−1
2 (1 + √c)−4
3 
λ+
N −(1 + √c)2
against the
Tracy–Widom law for N = 500, n = 1500, c = 1/3, for the covariance matrix model
T
1
2 XXHT
1
2 with T diagonal with all entries 1 but for T11 = 1.5. Empirical
distribution taken over 10 000 Monte-Carlo simulations.
eigenvector in the spiked model for Gaussian sample covariance matrices, i.e.
normalized Wishart matrices with population covariance matrix composed of
eigenvalues that are all ones but for a few larger eigenvalues. This is given in the
following.
Theorem 9.9 ([Paul, 2007]). Let XN ∈RN×n have i.i.d. real Gaussian entries
of zero mean and variance 1/n and ¯TN ∈RN×N be deﬁned as
¯TN = diag(α1, . . . , α1
|
{z
}
k1
, . . . , αM, . . . , αM
|
{z
}
kM
, 1, . . . , 1
| {z }
N−PM
i=1 ki
)
with α1 > . . . > αM > 0 for some positive integer M. Then, as n, N →∞with
limit ratio N/n →c, 0 < c < 1, for all i ∈{k1 + . . . + kj−1 + 1, . . . , k1 + . . . +
kj−1 + kj}, the eigenvector pi associated with the ith largest eigenvalue λi of
¯T
1
2
NXNXH
N ¯T
1
2
N satisﬁes
pT
i eN,i
2 a.s.
−→



1−
c
(αj −1)2
1+
c
αj −1
, if αj > 1 + √c
0
, otherwise
where eN,i ∈RN denotes the vector with all zeros but a one in position i.
Also, if αj > 1 + √c has multiplicity one, denoting k ≜PM
l=1 kl, we write pi =
(pT
A,i, pT
B,i)T, with pA,i ∈Rk the vector of the ﬁrst k coordinates and pB,i ∈Rk
the vector of the last N −k coordinates. We further take the convention that the

9.3. Random matrix theory and eigenvectors
239
coordinate i of pi is non-negative. Then we have, for i = k1 + . . . + kj−1 + 1, as
N, n →∞with N/n −c = o(1/√n)
(i) the vector pA,i satisﬁes
√n
 pA,i
∥pA,i∥−eM,i

⇒X
where X is an M-variate Gaussian vector with zero mean and covariance Σj
given by:
Σj =

1 −
c
(αj −1)2
−1 X
1≤l≤M
l̸=j
αlαj
(αl −αj)2 eM,leT
M,l
(ii) the vector pB,i/∥pB,i∥is uniformly distributed on the unit sphere of dimension
N −k −1 and is independent of pA,i.
Note that (ii) is valid for all ﬁnite dimensions. As a matter of fact, (ii) is valid
for any i ∈{1, . . . , min(n, N)}. Theorem 9.9 is important as it states in essence
that only some of the eigenvectors corresponding to the largest eigenvalues of a
perturbed Wishart matrix carry information. Obviously, as αj tends to 1 + √c,
the almost sure limit of
pT
i eN,i
 tends to zero, while the variance of the second
order statistics tends to inﬁnity, meaning that increasingly less information can
be retrieved as αj →1 + √c.
In Figure 9.5, the situation of a single population spike α1 = α with
multiplicity one is considered. The matrix dimensions N and n are taken to
be such that N/n = 1/3, and N ∈{100, 200, 400}. We compare the averaged
empirical projections
pT
i eN,i
 against Theorem 9.9. That is, we evaluate the
averaged absolute value of the ﬁrst entry in the eigenvector matrix UN in
the spectral decomposition of ¯T
1
2
NXNXH
N ¯T
1
2
N = UN diag(λ1, . . . , λN)UH
N. We
observe that the convergence rate of the limiting projection is very slow.
Therefore, although nothing can be said asymptotically on the eigenvectors of a
spiked model, when α < 1 + √c, there exists a large range of values of N and n
for which this is not so.
We also mention the recent result from Benaych-Georges and Rao [Benaych-
Georges and Rao, 2011] which, in addition to providing limiting positions for
the eigenvalues of some unitarily invariant perturbed random matrix models,
Theorem 9.2, provides projection results `a la Paul. The main result is as follows.
Theorem 9.10. Let XN ∈CN×N be a Hermitian random matrix with ordered
eigenvalues λ1 ≥. . . ≥λN. We assume that the e.s.d. F XN converges weakly and
almost surely toward F with compact support with inﬁmum a and supremum b,
such that λ1
a.s.
−→b and λN
a.s.
−→a. Consider also a perturbation Hermitian matrix
AN of rank r, with ordered non-zero eigenvalues a1 ≥. . . ≥ar. Finally, assume
that either XN or AN (or both) are bi-unitarily invariant. Denote YN the matrix

240
9. Extreme eigenvalues
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
Population spike value α
Averaged |UN,11|
Simulation, N = 100
Simulation, N = 200
Simulation, N = 400
Limiting |UN,11|
Figure 9.5 Averaged absolute ﬁrst entry |UN,11| of the eigenvector corresponding to
the largest eigenvalue in ¯T
1
2
NXNXH
N ¯T
1
2
N = U diag(λ1, . . . , λN)UH, with XN ﬁlled with
i.i.d. Gaussian entries CN(0, 1/n) and ¯TN ∈RN×N diagonal with all entries one but
for the ﬁrst entry equal to α, N/n = 1/3, for varying N.
deﬁned as
YN = XN + AN
with order eigenvalues ν1 ≥. . . ≥νN. For i ∈{1, . . . , r} such that 1/ai ∈
(−mF (a−), −mF (b+)), call zi = νi if ai > 0 or zi = νN−r+i if ai < 0, and vi
an eigenvector associated with zi in the spectral decomposition of YN. As N
grows large, we have:
(i)
⟨vi, ker(aiIN −AN)⟩2 a.s.
−→
1
a2
i m′
F

−
1
m−1
F (1/ai)

(ii)
⟨vi, ⊕j̸=iker(ajIN −AN)⟩2 a.s.
−→0
where the notation ker(X) denotes the kernel or nullspace of X, i.e. the
space of vectors y such that Xy = 0 and ⟨x, A⟩is the norm of the orthogonal
projection of x on A.
A similar result for multiplicative matrix perturbations is also available.

9.3. Random matrix theory and eigenvectors
241
Theorem 9.11. Let XN and AN be deﬁned as in Theorem 9.10. Denote ZN
the matrix
ZN = XN (IN + AN)
with ordered eigenvalues µ1 ≥. . . ≥µN. For i ∈{1, . . . , r} such that 1/ai ∈
(ψ−1
F (a−), ψ−1
F (b+)) with ψF the ψ-transform of F (see Deﬁnition 3.6), call
zi = νi if ai > 0 or zi = νN−r+i if ai < 0, and vi an eigenvector associated with
zi in the spectral decomposition of ZN. Then, as N grows large, for i ≥1:
(i)
⟨vi, ker(aiIN −AN)⟩2 a.s.
−→−
1
a2
i ψ−1
F (1/ai)ψ′
F
 ψ−1
F (1/ai)

+ ai
(ii)
⟨vi, ⊕j̸=iker(ajIN −AN)⟩2 a.s.
−→0.
The results above have recently been extended by Couillet and Hachem
[Couillet and Hachem, 2011], who provide a central limit theorem for the joint
ﬂuctuations of the spiky sample eigenvalues and eigenvector projections, for the
product perturbation model of Theorem 9.11. These results are particularly
interesting in the applicative context of local failure localization in large
dimensional systems, e.g. sensor failure or sudden parameter change in large
sensor networks, or link failure in a large interconnected graphs. The underlying
idea is that a local failure may change the network topology, modeled though the
covariance matrix of successive nodal observations, by a small rank perturbation.
The perturbation matrix is a signature of the failure which is often easier to
identify from its eigenvector properties than from its eigenvalues, particularly so
in homogeneous networks where each failure leads to similar amplitudes of the
extreme eigenvalues.
This completes this short section on extreme eigenvectors. Many more results
are expected to be available on this subject in the near future. Before moving to
the application part, we summarize the ﬁrst part of this book and the important
results introduced so far.


10
Summary and partial conclusions
In this ﬁrst part, we started by introducing random matrices as nothing more
than a multi-dimensional random variable characterized by the joint probability
distribution of its entries. We then observed that the marginal distribution of
the eigenvalues of such matrices often carries a lot of information and may even
determine the complete stochastic behavior of the random matrix. For instance,
we pointed out that the marginal eigenvalue distribution of the Gram matrix
HHH associated with the multiple antenna channel H ∈CN×n is a suﬃcient
statistic for the characterization of the maximum achievable rate. However, we
then realized that, for channels H more structured than standard Gaussian i.i.d.,
it is very diﬃcult to provide a general characterization of the marginal empirical
eigenvalue distribution of HHH.
It was then shown that the eigenvalue distribution of certain classes of random
matrices converges weakly to a deterministic limiting distribution function, as
the matrix dimensions grow to inﬁnity, the convergence being proved in the
almost sure sense. These classes of matrices encompass in particular Wigner
matrices, sample covariance matrices, models of the type AN + XNTNXH
N when
the e.s.d. of TN and AN have an almost sure limit and Gram matrices XNXH
N
where XN has independent entries with a variance proﬁle that has a joint
limiting distribution function. The assumptions made on the distribution of the
entries of such matrices were in general very mild, while for small dimensional
characterization exact results can usually only be derived for matrices with
Gaussian independent or loosely correlated entries. While there exist now several
diﬀerent proofs for the major results introduced earlier (e.g. the Mar˘cenko–
Pastur law has now been proved from several diﬀerent techniques using moments
[Bai and Silverstein, 2009], the Stieltjes transform [Mar˘cenko and Pastur, 1967],
the characteristic function [Faraut, 2006], etc.), we have shown that the Stieltjes
transform approach is a very handy tool to determine the limit distribution (and
limiting second order statistics) of such matrix models. This is especially suited
to the spectrum characterization of matrices with independent entries.
When the matrices involved in the aforementioned models are no longer
matrices with i.i.d. entries, but are more structured, the Stieltjes transform
approach is more diﬃcult to implement (although still possible according to some
recent results introduced earlier). The Stieltjes transform is sometimes better
substituted for by simpler tools, namely the R- and S-transforms, when the

244
10. Summary and partial conclusions
matrices have some nice symmetric structure. It is at this point that we bridged
random matrix theory and free probability theory. The latter provides diﬀerent
tools to study sums and products of a certain class of large dimensional random
matrices. This class of random matrices is characterized by the asymptotic
freeness property, which is linked to rotational invariance properties. From
a practical point of view, asymptotic freeness arises only for random matrix
models based on Haar matrices and on Gaussian random matrices. We also
introduced the free probability theory from a moment-cumulant viewpoint, which
was illustrated to be a very convenient tool to study the l.s.d. of involved
random matrix models through their successive free moments. In particular,
we showed that, through combinatorics calculus that can be automated on a
modern computer, the successive moments of the l.s.d. of (potentially involved)
sums, products, and information plus noise models of asymptotically free random
matrices can be easily derived. This bears some advantages compared to the
Stieltjes transform approach for which case-by-case treatment of every random
matrix model must be performed.
From the basic combinatorial grounds of free probability theory, it was then
shown that we can go one step further into the study of more structured
random matrices. Speciﬁcally, it was shown that, by exploiting softer invariance
structures of some random matrices, such as the left permutation invariance
of some types of random matrices, it is possible to derive expressions of
the successive moments of more involved random matrices, such as random
Vandermonde matrices. Moreover, the rotational invariance of these matrix
models allows us to extend expressions of the moments of the l.s.d. to expressions
of the moments of the expected e.s.d. for all ﬁnite dimensions. This allows us to
reﬁne the moment estimates when dealing with small dimensional matrices. This
extrapolation of free probability however is still in its infancy, and is expected
to produce a larger number of results in the coming years.
However, the moment-based methods, despite their inherent simplicity,
suﬀer from several shortcomings. First, apart from the very recent results on
Vandermonde random matrices, which can be seen as a noticeable exception,
moment-based approaches are only useful in practice for dealing with Gaussian
and Haar matrices. This is extremely restrictive compared to the models treated
with the analytical Stieltjes transform approach, which encompass to this day
matrix models based on random matrices with independent entries (sample
covariance matrices, matrices with a variance proﬁle, doubly correlated sums of
such matrices, etc.) as well as models based on Haar matrices. Secondly, providing
an expression of successive moments to approximate a distribution function
assumes that the distribution function under study is uniquely characterized by
its moments, and more importantly that these moments do exist. If the former
assumption fails to be satisﬁed, the computed moments are mostly unusable; if
the latter assumption fails, then these moments cannot even be computed. Due
to these important limitations, we dedicated most of Part I to a deep study of
the Stieltjes transform tool and of the Stieltjes transform-based methods used

245
to determine the l.s.d. of involved random matrix models, rather than moments
methods.
To be complete, we must mention that recent considerations, mostly spurred
by Pastur, suggest that, for most classical matrix models discussed so far, it
is possible to prove that the l.s.d. of matrix models with independent entries
or with Gaussian independent entries are asymptotically the same. This can be
proved by using the Gaussian method, introduced in Chapter 6, along with
a generalized integration by parts formula and Nash–Poincar´e inequality for
generic matrices with independent entries. Thanks to this method, Pastur also
shows that central limit theorems for matrices with independent entries can be
recovered from central limit theorems for Gaussian matrices (again accessible
through the Gaussian method). Since the latter is much more convenient and
much more powerful, as it relies on appreciable properties of the Gaussian
distribution, this last approach may adequately replace in the future the Stieltjes
transform method, which is sometimes rather diﬃcult to handle. The tool that
allows for an extension of the results obtained for matrices with Gaussian entries
to unconstrained random matrices with independent entries is referred to as the
interpolation trick, see, e.g., [Lytova and Pastur, 2009]. Incidentally, for simple
random matrix models, such as XNXH
N, where √nXN ∈CN×n has i.i.d. entries
of zero mean, unit variance, and some order four cumulant κ, but not only, it
can be shown that the variance of the central limit for linear statistics of the
eigenvalues can be written under the form σ2
Gauss + κσ2, where σ2
Gauss is the
variance of the central limit for the Gaussian case and σ2 is some additional
parameter (remember that κ = 0 in the Gaussian case).
Regarding random matrix models, for which not only the eigenvalue but also
the eigenvector distribution plays a role, we further reﬁned the concept of l.s.d.
to the concept of deterministic equivalents of the e.s.d. These deterministic
equivalents are ﬁrst motivated by the fact that there might not exist a l.s.d.
in the ﬁrst place and that the deterministic matrices involved in the models
(such as the side correlation matrices Rk and Tk in Theorem 6.1) may not have
a l.s.d. as they grow large. Also, even if there is a limiting d.f. F, it is rather
inconvenient that two diﬀerent series of e.s.d. F B1, F B2, . . . and F ˜B1, F ˜B2, . . .,
both converging to the same l.s.d. F, are attributed the same deterministic
approximation. Instead, we introduced the concept of deterministic equivalents
which provide a speciﬁc approximate d.f. FN for F BN such that, as N grows
large, FN −F BN ⇒0 almost surely; this way, FN can approximate F BN more
closely than would F and the limiting result would still be valid even if F BN
does not have a limit d.f.. We then introduced diﬀerent methodologies (i) to
determine, through its Stieltjes transform mN, the deterministic equivalent FN,
(ii) to show that the ﬁxed-point equation to which mN(z) is a solution admits a
unique solution on some restriction of the complex plane, and (iii) to show that
mN(z) −mF BN (z)
a.s.
−→0 and therefore FN −F BN ⇒0 almost surely. This leads
to a much more involved work than just showing that there exists a l.s.d. to BN,
but it is necessary to ensure the stability of the applications derived from these

246
10. Summary and partial conclusions
results. Deterministic equivalents were presented for various models, such as the
sum of doubly correlated Gram matrices or general Rician models. For further
application purposes, we also derived deterministic equivalents for the Shannon
transform of these models.
We then used the Stieltjes transform tool to dig deeper into the study of
the empirical eigenvalue distribution of some matrix models, and especially the
empirical eigenvalue distribution of sample covariance matrices and information
plus noise matrices. While it is already known that the e.s.d. of a sample
covariance matrix has a limit d.f. whenever the e.s.d. of the population covariance
matrix has a limit, we showed that more can be said about the e.s.d. We observed
ﬁrst that, as the matrix dimensions grow large, no eigenvalue is found outside
the support of the limiting d.f. with probability one (under mild assumptions).
Then we observed that, when the l.s.d. is formed of a ﬁnite union of compact
sets, as the matrix dimensions grow large, the number of eigenvalues found in
every set (called a cluster) is exactly what we would expect. In particular, if
the population covariance matrix in the sample covariance matrix model (or the
information matrix in the information plus noise matrix model) is formed of a
ﬁnite number K of distinct eigenvalues, with respective multiplicities N1, . . . , NK
(each multiplicity growing with the system dimensions), then the number of
eigenvalues found in every cluster of the e.s.d. exactly matches each Nk or
an exact sum of consecutive Nk with high probability. Many properties for
sample covariance matrix models were then presented: determination of the exact
limiting support, condition for cluster separability, etc. The same types of results
were presented for the information plus noise matrix models. However, in this
particular case, only exact separation for the Gaussian case has been established
so far.
For both sample covariance matrix and information plus noise matrix
models, eigen-inference, i.e. inverse problems based on the matrix eigenstructure,
was performed to provide consistent estimates for some functionals of the
population eigenvalues. Those estimators, that are consistent in the sense of being
asymptotically unbiased as both matrix dimensions grow large with comparable
sizes, were named G-estimators after Girko who calculated a large number of
such consistent estimators.
We then introduced models of sample covariance matrices whose population
covariance matrix has a ﬁnite number of distinct eigenvalues, but for which
some of the eigenvalues have ﬁnite multiplicity, not growing with the system
dimensions. These models were referred to as spiked models, as these eigenvalues
with small multiplicity have an outlying behavior. For these types of matrices,
the previous analysis no longer holds and speciﬁc study was made. It was
shown that these models exhibit a so-called ‘phase transition’ eﬀect in the sense
that, if a population eigenvalue with small multiplicity is greater than a given
threshold, then eigenvalues of the sample covariance matrix will be found with
high probability outside the support of the l.s.d., the number of which being
equal to the multiplicity of the population eigenvalue. On the contrary, if the

247
population eigenvalue is smaller than this threshold, then no sample eigenvalue
is found outside the support of the l.s.d., again with high probability, and it was
shown that the outlying population eigenvalue was then mapped to a sample
eigenvalue that converges to the right edge of the spectrum. It was therefore
concluded that, from the observation of the l.s.d. (or in practice, from the
observation of the e.s.d. for very large matrices), it is impossible to infer on
the presence of an outlying eigenvalue in the population covariance matrix. We
then dug deeper again to study the behavior of the right edge of the Mar˘cenko–
Pastur law and more speciﬁcally the behavior of the largest eigenvalue in the
e.s.d. of normalized Wishart matrices.
The study of the largest eigenvalue requires diﬀerent tools than the
Stieltjes transform method, among which large deviation analysis, orthogonal
polynomials, etc. We observed a peculiar behavior of the largest eigenvalue of
the e.s.d. of uncorrelated normalized Wishart matrices, which does not have a
classical central limit with convergence rate O(N
1
2 ) but a Tracy–Widom limit
with convergence rate O(N
2
3 ). It was then shown that spiked models in which the
isolated population eigenvalues are below the aforementioned critical threshold
do not depart from this asymptotic behavior, i.e. the largest eigenvalue converges
in distribution to the Tracy–Widom law, although the matrix size must be
noticeably larger than in the non-spiked model for the asymptotic behavior
to match the empirical distribution. When the largest isolated population
eigenvalue is larger than the threshold and of multiplicity one, then it has a
central limit with rate O(N
1
2 ).
If the results given in Part I have not all been introduced in view of practical
wireless communications applications, at least most of them have. Some of
them have in fact been designed especially for telecommunication purposes,
e.g. Theorems 5.8, 6.4, 6.5, etc. We show in Part II that the techniques
presented in the present part can be used for a large variety of problems
in wireless communications, aside from the obvious applications to MIMO
capacity, MAC and BC rate regions, signal sensing already mentioned. Of
particular interest will be the adaption of deterministic equivalent methods
to the performance characterization of linearly precoded broadcast channels
with transmit correlation, general user path-loss pattern, imperfect channel
state information, channel quantization errors, etc. Very compact deterministic
equivalent expressions will be obtained, where exact results are mathematically
intractable. This is to say the level of details and the ﬂexibility that random
matrix theory has reached to cope with more and more realistic system
models. Interesting blind direction of arrival and distance estimators will also
be presented that use and extend the analysis of the l.s.d. of sample covariance
matrix models developed in Section 7.1 and Chapter 8. These examples translate
the aptitude of random matrix theory to deal today with more general questions
than merely capacity evaluation.


Part II
Applications to wireless
communications


11
Introduction to applications in
telecommunications
In the preface of [Bai and Silverstein, 2009], Silverstein and Bai provide a
table of the number of scientiﬁc publications in the domain of random matrix
theory for ten-year periods. The table reveals that the number of publications
roughly doubled from one period to the next, with an impressive total of
more than twelve hundred publications for the 1995-2004 period. This trend
is partly due to the mathematical tools developed over the years that allow
for more and more possibilities for matrix model analysis. The major reason
though is related to the increasing complexity of the system models employed in
many ﬁelds of physics which demand low complexity analysis. We have already
mentioned in the introductory chapter that nuclear physics, biology, ﬁnance,
and telecommunications are among the ﬁelds in which the system complexity
involved in the daily work of engineers is growing at a rapid pace. The second
part of this book is entirely devoted to wireless communications and to some
related signal processing topics. The reader must nonetheless be aware that
many models developed here can be adapted to other ﬁelds of research, the
typical example of such models being the sample covariance matrix model.
In the following section, we provide a brief historical account of the
publications in wireless communications dealing with random matrices (from
both small and large dimensional viewpoints), from the earlier results in ideal
transmission channels down to recent reﬁned examples reﬂecting more realistic
communication environments. It will appear in particular to the reader that, in
the latest works, the hypotheses made on channel conditions are precise enough
to take into account (sometimes simultaneously) multi-user transmissions, very
general channel models with both transmit and receive correlations, Rician
models, imperfect channel state information at the sources and the receivers,
integration of linear precoders and decoders, inter-cell interference, etc.
11.1
Historical account of major results
It is often mentioned that Tse and Hanly [Tse and Hanly, 1999] initiated the
ﬁrst contribution of random matrix theory to information theory. We would
like to insist, following our point that random matrix theory deals both with
large dimensional random matrices and small dimensional random matrices, that

252
11. Introduction to applications in telecommunications
the important work from Telatar on the multiplexing gain of multiple antenna
communications [Telatar, 1995, 1999] was also part of this multi-dimensional
system analysis trend of the late nineties. From this time on, the interest in
random matrix theory grew vividly, to such an extent that more and more
research laboratories dedicated their time to various applications of random
matrix theory in large systems. The major driver for this dramatic increase
of work in applied random matrix theory is the recent growth of all system
dimensions in recent telecommunication systems. The now ten-year-old story
of random matrices for wireless communications started with the performance
study of code division multiple access.
11.1.1
Rate performance of multi-dimensional systems
The ﬁrst large dimensional system which was approached by asymptotic analysis
is the code division multiple access (CDMA) technology that came along with
the third generation of mobile phone communications. We remind that CDMA
succeeded the time division multiple access (TDMA) technology used for the
second generation of mobile phone communications. In a network with TDMA
resource sharing policy, users are successively allocated an exclusive amount
of time to exchange data with the access points. Due to the established ﬁxed
pattern of time division, one of the major issues of the standard was then
that each user could only be allocated a unique time slot, while at the same
time a very strict maximal number of users could be accepted by a given
access point, regardless of the users’ requests in terms of quality of service.
In an eﬀort to increase the number of users for a given access point, while
dynamically balancing the quality of service oﬀered to each terminal, the CDMA
system was selected for the subsequent mobile phone generation. In a CDMA
system, each user is allocated a (usually long) spreading code that is made
roughly orthogonal to the other users’ codes, in such a way that all users can
simultaneously receive data while experiencing a limited amount of interference
from concurrent communications, due to code orthogonality. Equivalently, in the
uplink, the users can simultaneously transmit orthogonal streams that can be
decoded free of interference at the receiver. Since the spreading codes are rarely
fully orthogonal (unless orthogonal codes such as Hadamard codes are used),
the more users served by an access point, the more the interference and then the
less the quality of service; but at no time is a user rejected for lack of available
resource (unless an excessive number of users wishes to access the network).
While the achievable transmission data rate for TDMA systems is rather easy to
evaluate, the capacity for CDMA networks depends on the precoding strategy
applied. One strategy is to build purely orthogonal codes so that all users do
not interfere with each other; we refer to this precoding policy as orthogonal
CDMA. This has the strong advantage of making decoding easy at the receiver
and discards the so-called near-far eﬀect that leads non-orthogonal users that
transmit much power to interfere with more than other users that transmit less

11.1. Historical account of major results
253
power. However, this is not better than TDMA in terms of achievable rates, is
more demanding in terms of time synchronization (i.e. the codes of two users not
properly synchronous are no longer orthogonal), and suﬀers signiﬁcantly from the
frequency selectivity of the transmission medium (which induces convolutions of
the orthogonal codes, breaking then the orthogonality to some extent). For all
these reasons, it is often more sensible to use random i.i.d. codes. This second
precoding policy is called random CDMA. In practice, codes may originate
from random vector generators tailored so to mitigate inter-user interference;
these are called pseudo-random CDMA codes. Now the problem is to evaluate
the communication rates achieved by such precoders. Indeed, in the orthogonal
CDMA approach, assuming frequency ﬂat channel conditions for all users and
channel stability over a large number of successive symbol periods, the rates
achieved in the uplink (from user terminals to access points) are maximal when
the orthogonal codes are as long as the number of users N, and we have the
system capacity Corth(σ2) for a noise power σ2 given by:
Corth(σ2) = 1
N log det

IN + 1
σ2 WHHHWH

where W ∈CN×N is the unitary matrix whose columns are the CDMA codes
and H = diag(h1, . . . , hN) is the diagonal matrix of the channel gains of the users
1, . . . , N. By the property det(I + AB) = det(I + BA) for matrices A, B such
that both AB and BA are square matrices and the fact that W is unitary, this
reduces to
Corth(σ2) = 1
N log det

IN + 1
σ2 HHH

= 1
N
N
X
i=1
log

1 + |hi|2
σ2

.
This justiﬁes our previous statement on the equivalence between TDMA and
CDMA rate performance. When it comes to evaluate the capacity Crand(σ2) of
random CDMA systems, under the same conditions, we have:
Crand(σ2) = 1
N log det

IN + 1
σ2 XHHHXH

with X ∈CN×N the matrix whose columns are the users’ random codes. The
result here is no longer trivial and appeals to random matrix analysis. Since the
number of users attached to a given access point is usually assumed large, we
can use results from large dimensional random matrix theory. The analysis of
such systems in the large dimensional regime was performed by Shamai, Tse,
and Verd´u in [Tse and Verd´u, 2000; Verd´u and Shamai, 1999].
These capacity expressions may however not be realistic achievable rates in
practice, in the sense that they imply non-linear processing at the receiving
access points (e.g. decoding based on successive interference cancellation). For
complexity reasons, such non-linear processing might not be feasible, so that
linear precoders or decoders are often preferred. For random CDMA codes,
the capacity achieved by linear decoders in CDMA systems such as matched-

254
11. Introduction to applications in telecommunications
ﬁlters, linear minimum mean square error (LMMSE) decoders, etc. have been
extensively studied from the earlier work of Tse and Hanly [Tse and Hanly, 1999]
in frequency ﬂat channels, Evans and Tse [Evans and Tse, 2000] in frequency
selective channels, Li and Tulino for reduced-rank LMMSE decoders [Li et al.,
2004], Verd´u and Shamai [Verd´u and Shamai, 1999] for several receivers in
frequency ﬂat channels, etc.
While the single-cell orthogonal CDMA capacity does not require the random
matrix machinery, the study of random CDMA systems is more diﬃcult.
Paradoxically, when it comes to linear decoders, the tendency is opposite, as
the performance study is in general more involved for the orthogonal case than
for the random i.i.d. case. The ﬁrst substantial work on the performance of linear
orthogonal precoded systems arises in 2003 with Debbah and Hachem [Debbah
et al., 2003a] on the performance of MMSE decoders in orthogonal CDMA
frequency ﬂat fading channels. The subsequent work of Chaufray and Hachem
[Chaufray et al., 2004] deals with more realistic sub-optimum MMSE receivers
when only partial channel state information is available at the receiver. This
comes as a ﬁrst attempt to consider more realistic transmission conditions. Both
aforementioned works have the particularity of providing genuine mathematical
results that were at the time not part of the available random matrix literature.
As it will often turn out in the intricate models introduced hereafter, the
rapid involvement of the wireless communication community in random matrices
came along with a fast exhaustion of all the “plug-and-play” results available
in the mathematical literature. As a consequence, most results discussed
hereafter involving non-trivial channel models often require a deep mathematical
introduction, which fully justiﬁes Part I of the present book.
In the same vein as linear systems, linear precoders and decoders for multi-
user detection have been given a lot of attention in the past ten years, with the
increasing demand for the study of practical scenarios involving a large number
of users. The motivation for using random matrix theory here is the increase in
the number of users, as well as the increase in the number of antennas used for
multi-user transmissions and decoding. Among the notable works in this domain,
we mention the analysis of Tse and Zeitouni [Tse and Zeitouni, 2000] on linear
multi-user receivers for random CDMA systems. The recent work by Wagner and
Couillet [Wagner et al., 2011] derives deterministic equivalents for the sum rate in
multi-user broadcast channels with linear precoding and imperfect channel state
information at the transmitter. The sum rate expressions obtained in [Wagner
et al., 2011] provide diﬀerent system characterizations. In particular, the optimal
training time for channel estimation in quasi-static channels can be evaluated,
the optimal cell coverage or number of users to be served can be assessed, etc. We
come back to the performance of such linear systems in Chapter 12 and Chapter
14.
One of the major contributions to the analysis of multi-dimensional system
performance concerns the capacity derivation of multiple antenna technologies

11.1. Historical account of major results
255
when several transmit and receive antennas are assumed. The derivation from
Telatar [Telatar, 1999] of the capacity of frequency ﬂat point-to-point multiple
antenna transmissions with Gaussian i.i.d. channel matrix involves complicated
small dimensional random matrix calculus. The generalization of Telatar’s
calculus to non-Gaussian or correlated Gaussian models is even more involved
and cannot be treated in general. Large dimensional random matrices help
deriving large dimensional capacity expressions for these more exotic channel
models. The ﬁrst results for correlated channels are due to Chuah and Tse [Chuah
et al., 2002], Mestre and Fonollosa [Mestre et al., 2003], and Tulino and Verd´u
[Tulino and Verd´u, 2005]. In the last mentioned article, an implicit expression for
the asymptotic ergodic capacity of Kronecker channel models is derived, which
does not require integral calculus. We recall that a Kronecker-modeled channel
is deﬁned as a matrix H = R
1
2 XT
1
2 ∈Cnr×nt, where R
1
2 ∈Cnr×nr and T
1
2 ∈
Cnt×nt are deterministic non-negative Hermitian matrices, and X ∈Cnr×nt has
Gaussian independent entries of zero mean and normalized variance. This is
somewhat less general than similar models where X has non-necessarily Gaussian
entries. We must therefore make the distinction between the Kronecker model
and the doubly correlated i.i.d. matrix model, when necessary. Along with the
Kronecker model of [Tulino and Verd´u, 2005], the capacity of the very general
Rician channel model H = A + X is derived in [Hachem et al., 2007], where
A ∈Cnr×nt is deterministic and X ∈nr × nt has entries Xij = σijYij, where
the elements Yij are i.i.d. (non-necessarily Gaussian) and the factors σ2
ij form
the deterministic variance proﬁle.
These results on point-to-point communications in frequency ﬂat channels
are further generalized to multi-user communications, to frequency selective
communications, and to the most general multi-user frequency selective
communications, in doubly correlated channels. The frequency selective channel
results are successively due to Moustakas and Simon [Moustakas and Simon,
2007], who conjecture the capacity of Kronecker multi-path channels using
the replica method brieﬂy discussed in Section 19.2, followed by Dupuy and
Loubaton, who prove those earlier results and additionally determine the
capacity achieving signal precoding matrix in [Dupuy and Loubaton, 2010]. On
the multi-user side, using tools from free probability theory, Peacock and Honig
[Peacock et al., 2008] derive the limit capacity of multi-user communications.
Their analysis is based on the assumptions that the number of antennas per user
grows large and that all user channels are modeled as Kronecker, such that the
left correlations matrices are co-diagonalizable. In a parallel work Couillet et al.
[Couillet et al., 2011a] relax the constraints on the channels of [Peacock et al.,
2008] and provide a deterministic equivalent for the points in the rate region of
multi-user multiple access and broadcast channels corresponding to deterministic
precoders, with general doubly correlated channels. Moreover, [Couillet et al.,
2011a] provides an expression of the ergodic capacity maximizing precoding
matrix for each user in the multiple access uplink channel and therefore provides
an expression for the boundary of the ergodic multiple access rate region. Both

256
11. Introduction to applications in telecommunications
results can then be combined to derive a deterministic equivalent of the rate
region for multi-user communications on frequency selective Kronecker channels.
To establish results on outage capacity for multiple antenna transmissions, we
need to go beyond the expression of deterministic equivalents of the ergodic
capacity and study limiting results on the capacity around the mean. Most
results are central limits. We mention in particular the important result from
Hachem and Najim [Hachem et al., 2008b] on a central limit theorem for the
capacity of Rician multiple antenna channels, which follows from their previous
work in [Hachem et al., 2007].
The previous multi-user multiple antenna considerations may then be extended
to multi-cellular systems. In [Zaidel et al., 2001], the achievable rates in multi-
cellular CDMA networks are studied in Wyner’s inﬁnite linear cell array model
[Wyner, 1994]. Abdallah and Debbah [Abdallah and Debbah, 2004] provide
conditions on network planning to improve system capacity of a CDMA network
with matched-ﬁlter decoding, assuming an inﬁnite number of cells in the network.
Circular cell array models are studied by Hoydis et al. [Hoydis et al., 2011d],
where the optimal number of serving base stations per user is derived in a multi-
cell multiple antenna channel model with ﬁnite channel coherence time. The
ﬁnite channel coherence duration parameter involves a limitation of the gain of
multi-cell cooperation due to the time required for synchronization (linked to
the fundamental diversity-multiplexing trade-oﬀ[Tse and Zheng, 2003]), hence
a non-trivial rate optimum. Limiting results on the sum capacity in large cellular
multiple antenna networks are also studied in [Aktas et al., 2006] and [Huh et al.,
2010].
Networks with relay and ad-hoc networks are also studied using tools from
random matrix theory. We mention in particular the work of Levˆeque and Telatar
[Levˆeque and Telatar, 2005] who derive scaling laws for large ad-hoc networks.
On the relay network side Fawaz et al. [Fawaz et al., 2011] analyze the asymptotic
capacity of relay networks in which relays perform decode and forward. Game
theoretical aspects of large dimensional systems were also investigated in light
of the results from random matrix theory, among which is the work of Bonneau
et al. [Bonneau et al., 2007] on power allocation in large CDMA networks.
Chapters 13–15 develop the subject of large multi-user and multiple antenna
channels in detail.
11.1.2
Detection and estimation in large dimensional systems
One of the recent hot topics in applied random matrix theory for wireless
communications deals with signal detection and source separation capabilities
in large dimensional systems. The sudden interest in the late nineties for
signal detection using large matrices was spurred simultaneously by the recent
mathematical developments and by the recent need for detection capabilities in
large dimensional networks. The mathematical milestone might well be the early
work of Geman in 1980 [Geman, 1980] who proved that the largest eigenvalue of

11.1. Historical account of major results
257
the e.s.d. of XXH, where X has properly normalized central i.i.d. entries with
some assumption on the moments, converges almost surely to the right edge of
the Mar˘cenko–Pastur law. This work was followed by generalizations with less
constrained assumptions and then by the work of Tracy and Widom [Tracy and
Widom, 1996] on the limit distribution of the largest eigenvalue for this model
when X is Gaussian.
On the application side, when source detection is to be performed with
the help of a single sensing device, i.e. a single antenna, the optimal decision
criterion is given by the Neyman–Pearson test, which was originally derived by
Urkowitz for Gaussian channels [Urkowitz, 1967]. This method was then reﬁned
to more realistic channel models in, e.g., [Kostylev, 2002; Simon et al., 2003].
The Neyman–Pearson test assumes the occurrence of two possible events H0
and H1 with respective probability p0 and p1 = 1 −p0. The event H0 is called
the null hypothesis, which in our context corresponds to the case where only
noise is received at the sensing device, while H1 is the complementary event,
which corresponds to the case where a source is emitting. Upon reception of a
sequence of n symbols gathered in the vector y = (y1, . . . , yn)T at the sensing
device, the Neyman–Pearson criterion states that H1 must be decided if
P(H1|y)
P(H0|y) = P(y|H1)p1
P(y|H0)p0
> γ
for a given threshold γ. If this condition is not met, H0 must be decided. In
the Gaussian channel context, the approach is simple as it merely consists in
computing the empirical received power, i.e. the averaged square amplitude
1
n
P
i |yi|2, and deciding the presence or the absence of a signal source based
on whether the empirical received power is greater or less than the threshold
γ. Due to the Gaussian assumption, it is then easy to derive the probability
of false negatives, i.e. the probability of missing the presence of a source, or of
false positives, i.e. the probability of declaring the presence of a source when
there is none, which are classical criteria to evaluate the performance of a source
detector. Observe already that estimating these performances requires to know
the statistics of the yi or, in the large n hypothesis, to know limiting results on
P
i |yi|2.
The generalization to multi-dimensional data y1, . . . , yn ∈CN could consist in
summing the total empirical power received across the N sensors, i.e. Pn
i=1 ∥yi∥2,
and then comparing this value to some threshold. This is what is usually
done for arrays of N sensors, as it achieves an N-fold increase of performance.
Calling Y = [y1, . . . , yn] ∈CN×n, the empirical power reduces to the normalized
trace of YYH. However simple and widely used, see, e.g., [Meshkati et al.,
2005], this solution may not always be Neyman–Pearson optimal. Couillet and
Debbah [Couillet and Debbah, 2010a] derive the Neyman–Pearson optimal
detector in ﬁnitely large multiple antenna channels. Their work assumes that
both transmit symbols and channel fading links are Gaussian, which are shown
to be optimal assumptions in the maximum-entropy principle sense [Jaynes,

258
11. Introduction to applications in telecommunications
1957a,b] when limited prior information is known about the communication
channel. The derived Neyman–Pearson criterion for this model turns out to be a
rather complex formula involving all eigenvalues of YYH and not only their
sum. Moreover, under the realistic assumption that the signal-to-noise ratio
is not known a priori, the decision criterion expresses under a non-convenient
integral form. This is presented in detail in Section 16.3. Simpler solutions are
however sought for which may beneﬁt from asymptotic considerations on the
eigenvalue distribution of YYH. A ﬁrst approach, initially formulated by Zeng
and Liang [Zeng and Liang, 2009] and accurately studied by Cardoso and Debbah
[Cardoso et al., 2008], consists in considering the ratio of the largest to the
lowest eigenvalue in YYH and deciding the presence of a signal when this ratio
is more than some threshold. This approach is simple and does not assume
any prior knowledge on the signal-to-noise ratio. A further approach, actually
more powerful, consists in replacing the diﬃcult Neyman–Pearson test by the
suboptimal though simpler generalized likelihood ratio test. This is performed
by Bianchi et al. [Bianchi et al., 2011]. These methods are studied in detail in
Section 16.4.
Another non-trivial problem of large matrices with both dimensions growing
at a similar rate is the consistent estimation of functionals of eigenvalues and
eigenvectors. As already largely introduced in Chapter 8, in many disciplines,
we are interested in estimating, e.g. population covariance matrices, eigenvalues
of such matrices, etc., from the observation of the empirical sample covariance
matrix. With both number of samples n and sample size N growing large at the
same rate, it is rather obvious that no good estimate for the whole population
covariance matrix can be relied on. However, consistent estimates of functionals
of the eigenvalues and eigenvectors have been shown to be possible to obtain. In
the context of wireless communications, sample covariance matrices can be used
to model the downlink of multiple antenna Gaussian channels with directions
of arrivals. Using subspace methods, the directions of arrival can be evaluated
as a functional of the eigenvectors of the population null space, i.e. the space
orthogonal to the space spanned by the transmit vectors. Being able to evaluate
these directions of arrival makes it possible to characterize the position of a
source. In [Mestre, 2008a], Mestre proves that the classical n-consistent approach
MUSIC method from Schmidt [Schmidt, 1986] to estimate the directions of
arrival is largely biased when (n, N) grow large at a similar rate. Mestre then
oﬀers an alternative estimator in [Mestre, 2008b] which unfolds directly from
Theorem 8.4, under separability assumption of the clusters in the l.s.d. of
a sample covariance matrix. This method is shown to be (n, N)-consistent,
while showing slightly larger estimate variances. A similar direction of arrival
estimation is performed by Vallet et al. [Vallet et al., 2010] in the more realistic
scenario where the received data vectors are still deterministic but unknown to
the sensors. This approach uses an information plus noise model in place of the
sample covariance matrix model. The problem of detecting multiple multiple

11.1. Historical account of major results
259
antenna signal sources and estimating the respective distances or powers of each
source based on the data collected from an array of sensors enters the same
category of problems. Couillet and Silverstein [Couillet et al., 2011c] derive an
(n, N)-consistent estimator to this purpose. This estimator assumes simultaneous
transmissions over i.i.d. fading channels and is suﬃciently general to allow the
transmitted signals to originate from diﬀerent symbol constellations. The blind
estimation by an array of sensors of the distances to sources is particularly suited
to the recently introduced self-conﬁgurable femto-cells. In a few words, femto-
cells are required to co-exist with neighboring networks, by reusing spectrum
opportunities left available by these networks. The blind detection of adjacent
licensed users therefore allows for a dynamical update of the femto-cell coverage
area that ensures a minimum interference to the licensed network. See, e.g.,
[Calin et al., 2010; Claussen et al., 2008] for an introduction to femto-cells and
[Chandrasekhar et al., 2009] for an understanding of the need for femto-cells to
evaluate the distance to neighboring users.
The aforementioned methods, although very eﬃcient, are however often
constrained by strong requirements that restrict their usage. Suboptimal
methods that do not fall into these shortcomings are then sought for. Among
them, we mention inversion methods based on convex optimization, an example
of which being the inversion algorithm for the sample covariance matrix derived
by El Karoui [Karoui, 2008], and moment-based approaches, as derived by
Couillet and Debbah [Couillet and Debbah, 2008], Rao and Edelman [Rao
et al., 2008], Ryan and Debbah [Ryan and Debbah, 2007a], etc. These methods
are usually largely more complex and are no match both in complexity and
performance for the analytical approaches mentioned before. However, they are
much more reliable as they are not constrained by strong prior assumptions on
asymptotic spectrum separability. These diﬀerent problems are discussed at large
in Chapter 17.
In the following section, we open a short parenthesis to introduce the currently
active ﬁeld of cognitive radios, which provides numerous open problems related
to signal sensing, estimation, and optimization in possibly large dimensional
networks.
11.1.3
Random matrices and ﬂexible radio
The ﬁeld of cognitive radios, also referred to as ﬂexible radios, has known an
increasing interest in the past ten years, spurred by the concept of software
deﬁned radios, coined by Mitola [Mitola III and Maguire Jr, 1999]. Software
deﬁned radios are reconﬁgurable telecommunication service providers that are
meant to dynamically adapt to the client demand. That is, in order to increase
the total throughput of multi-protocol communications in a given geographical
area [Akyildiz et al., 2006; Tian and Giannakis, 2006], software deﬁned radios will
provide various protocol services to satisfy all users in a cell. For instance, cellular
phone users in a cognitive radio network will be able to browse the Internet

260
11. Introduction to applications in telecommunications
seamlessly through either WiFi, WiMAX, LTE, or 3G protocols, depending on
the available spectral resources. This idea, although not yet fully accepted in
the industrial world, attempts to eﬃciently reuse the largely unused (we might
say “wasted”) bandwidth, which is of fundamental importance in days when no
more room is left in the electromagnetic frequency spectrum for future high-speed
technologies, see, e.g., [Tandra et al., 2009].
The initial concept of software deﬁned radios has then grown into the
more general idea of intelligent, ﬂexible, and self-reconﬁgurable radios [Hur
et al., 2006]. Such radios are composed of smart systems at all places of the
communication networks, from the macroscopic infrastructure that spans across
thousands of kilometers and that must be globally optimized in some sense,
down to the microscopic local in-house networks that must ensure a high quality
of service to local users with minimum harm to neighboring communications.
From the macroscopic viewpoint, the network must harmonize a large number
of users at the network and MAC layers. These aspects may be studied in
large game-theoretical frameworks, which demand for large dimensional analysis.
Random matrices, but also mean ﬁeld theory (see, e.g., [Bordenave et al., 2005;
Buchegger and Le Boudec, 2005; Sharma et al., 2006]), are common tools to
analyze such large decentralized networks. The random matrix approach allows
us to characterize deterministic behaviors in large decentralized games. The
study of games with a large number of users invokes concepts such as that
of Wardrop equilibrium, [Haurie and Marcotte, 1985; Wardrop, 1952]. The ﬁrst
applications to large wireless communication networks mixing both large random
matrix analysis and games are due to Bonneau et al. [Bonneau et al., 2007,
2008] who study the equilibrium of distributed games for power allocation in
the uplink of large CDMA networks. The addition of random matrix tools to
the game theoretic settings of such large CDMA networks allows the players in
the game, i.e. the CDMA users, to derive deterministic approximations of their
functions of merit or cost functions.
On the microscopic side, individual terminals are now required to be smart
in the sense that they ought to be able to concur for available spectral
resources in decentralized networks. The interest for the decentralized approach
typically appears in large networks of extremely mobile users where centralized
resource allocation is computationally prohibitive. In such a scenario, mobile
users must be able to (i) understand their environment and its evolution by
developing sensing abilities (this phase of the communication process is often
called exploration) and (ii) take optimal decisions concerning resource sharing
(this phase is referred to as exploitation). These rather novel constraints on the
mobile terminals demand for an original technologically disruptive framework
for future mobile communications. Part of the exploration requirements for
smart terminals is the ability to detect on-going communications in the available
frequency resources. This point has already been discussed and is thoroughly
dealt with in Chapter 16. Now, another requirement for smart terminals, actually
prior to signal sensing, is that of channel modeling. Given some prior information

11.1. Historical account of major results
261
on the environment, such as the geographical location or the number of usually
surrounding base stations, the terminal must be able to derive a rough model
for the expected communication channel; in this way, sensing procedures will be
made faster and more accurate. This requirement again calls for a new framework
for channel modeling. This is introduced in Chapter 18, where, upon statistical
prior information known to the user terminal, various a priori channel probability
distributions are derived. This chapter uses extensively the works from Guillaud,
M¨uller, and Debbah [Debbah and M¨uller, 2005; Guillaud et al., 2007] and relies
primarily on small dimensional random matrix tools.
The chapter ordering somewhat parallels the ordering of Part I, by
chronologically introducing models that only require limit distribution theorems,
then models that require the introduction of deterministic equivalents and
ﬁnally detection criteria that rely either on extreme eigenvalue distributions or
on asymptotic spectrum considerations and eigen-inference methods. The next
chapter deals with system performance evaluation in CDMA communication
systems, that is the most documented subject to this day, gathering more than
ten years of progress in wireless communications.


12
System performance of CDMA
technologies
12.1
Introduction
The following four chapters are dedicated to the analysis of the rate at which
information can be reliably transmitted over a physical channel characterized
by its space, time, and frequency dimensions. We will often consider situations
where one or several of these dimensions can be considered large (and complex)
in some sense. Notably, the space dimension will be said to be large when
multiple transmit sources or receive sensors are used, which will be the case
of multiple antenna, multi-user, or multi-cell communications; the complexity
of these channels arises here from the joint statistical behavior of all point-to-
point channel links. The time dimension can be said to be large and complex
when the inputs to the physical channel exhibit a correlated behavior, such as
when signal precoders are used. It may be argued here that the communication
channel itself is not complex, but for simpliﬁed information-theoretic treatment,
the transmission channel in such a case is assumed to be the virtual medium
formed by the ensemble precoder and physical channel. Finally, the frequency
dimension exhibits complexity when the channel shows ﬂuctuations in the
frequency domain, i.e. frequency selectivity, which typically arises as multi-path
reﬂections come into play in the context of wireless communications or when
multi-modal transmissions are used in ﬁber optics.
To respect historical progress in the use of random matrix theory in wireless
communications, we should start with the work of Telatar [Telatar, 1995] on
multiple antenna communication channels. However, asymptotic considerations
were not immediately applied to multiple antenna systems and, as a matter
of fact, not directly applied to evaluate mutual information through Shannon
transform formulas. Instead, the ﬁrst applications of large dimensional random
matrix theory [Biglieri et al., 2000; Shamai and Verd´u, 2001; Tse and
Hanly, 1999; Verd´u and Shamai, 1999] were motivated by the similarity
between expressions of the signal-to-interference plus noise ratio (SINR) in
CDMA precoded transmissions and the Stieltjes transform of some distribution
functions. We therefore start by considering the performance of such CDMA
systems either with random or unitary codes.

264
12. System performance of CDMA technologies
12.2
Performance of random CDMA technologies
We brieﬂy recall that code division multiple access consists in the allocation
of (potentially large) orthogonal or quasi-orthogonal codes w1, . . . , wK ∈CN to
a series of K users competing for spectral resource access. Every transmitted
symbol, either in the downlink (from the access point to the users) or in the
uplink (from the users to the access point), is then modulated by the code of the
intended user. By making the codes quasi-orthogonal in the time domain, i.e.
in the downlink, by ensuring wH
i wj ≃0 for i ̸= j, user k receives its dedicated
message though a classical matched-ﬁlter (i.e. by multiplying the input signal by
wH
k ), while being minimally interfered with by the messages intended for other
users. In the uplink, the access point can easily recover the message from all
users by matched-ﬁltering successively the input signal by wH
1 , . . . , wH
K. Matched-
ﬁltering is however only optimal (from an output SINR viewpoint) when the
codes are perfectly orthogonal and when the communication channel does not
break orthogonality, i.e. frequency ﬂat channels. A reﬁned ﬁlter in that case is
the minimum mean square error (MMSE) decoder, which is optimal in terms
of SINR experienced at the receiver. Ideally, though, regardless of the code
being used, the sum rate optimal solution consists either in the uplink or in
the downlink in proceeding to joint decoding at the receiver. Since this requires
a lot of information about all user channel conditions, these optimal ﬁlters are
never considered in the downlink. The performance of all aforementioned ﬁlters
are presented in the following under more and more constraining communication
scenarios.
We ﬁrst start with uplink CDMA communications before proceeding to the
downlink scenario, and consider random i.i.d. CDMA codes before studying
orthogonal CDMA codes.
12.2.1
Random CDMA in uplink frequency ﬂat channels
We consider the uplink of a random code division multiple access transmission,
with K users sending simultaneously their signals to a unique access point
or base station, as in, e.g., [Biglieri et al., 2001; Grant and Alexander, 1998;
Madhow and Honig, 1994; M¨uller, 2001; Rapajic and Popescu, 2000; Schramm
and M¨uller, 1999]. Denote N the length of each CDMA spreading code. User
k, k ∈{1, . . . , K}, has code wk ∈CN, which has i.i.d. entries of zero mean
and variance 1/N. At time l, user k transmits the Gaussian symbol s(l)
k . The
channel from user k to the base station is assumed to be non-selective in the
frequency domain, constant over the spreading code length, and is written as
the product hk
√Pk of a fast varying parameter hk, and a long-term parameter
√Pk accounting for the power transmitted by user k and the shadowing eﬀect.
We refer for simplicity to Pk as the power of user k. At time l, we therefore have

12.2. Performance of random CDMA technologies
265
the transmission model
y(l) =
K
X
k=1
hkwk
p
Pks(l)
k + n(l)
(12.1)
where n(l) ∈CN denotes the additive Gaussian noise vector with entries of zero
mean and variance σ2, and y(l) ∈CN is the signal vector received at the base
station.
The expression (12.1) can be written in the more compact form
y(l) = WHP
1
2 s(l) + n(l)
with s(l) = [s(l)
1 , . . . , s(l)
K ]T ∈CK a Gaussian vector of zero mean and covariance
E[s(l)s(l)H] = IK, W = [w1, . . . , wK] ∈CN×K, P ∈CK×K is diagonal with kth
entry Pk, and H ∈CK×K is diagonal with kth entry hk.
In the following, we will successively study the general expressions of the
deterministic equivalents for the capacity achieved by the matched-ﬁlter, the
minimum-mean square error ﬁlter and the optimal decoder. Then, we apply our
results to the particular cases of the additive white Gaussian noise (AWGN)
channel and the Rayleigh fading channels. The AWGN channel will correspond
to the channel for which hk = 1 for all k. The Rayleigh channel is the channel for
which |hk| has Rayleigh distribution with real and imaginary parts of variance
1/2, and therefore |hk|2 is χ2
2-distributed with density p(|hk|2) = e−|hk|2.
12.2.1.1 Matched-ﬁlter
We ﬁrst assume that, at the base station, a matched-ﬁlter is applied to the
received signal. That is, the base station takes the product wH
k y(l) to retrieve
the data transmitted by user k. In this case, the signal power after matched-
ﬁltering reads:
Pk|hk|2|wH
k wk|2
while the interference plus noise power reads:
wH
k
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

wk
from which the signal-to-interference ratio γ(MF)
k
relative to the data of user k is
γ(MF)
k
=
Pk|hk|2|wH
k wk|2
wH
k
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

wk
.
Clearly, from the law of large numbers, as N →∞, wH
k wk
a.s.
−→1. Also, from
Theorem 3.4, for large N, K such that 0 < lim infN K/N ≤lim supN K/N < ∞,
we expect to have
wH
k
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

wk
−1
N tr
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

→0

266
12. System performance of CDMA technologies
almost surely. However, the conditions of Theorem 3.4 impose that the inner
matrix term
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

be uniformly bounded
in
spectral
norm.
This
is
not
the
case
here
since
there
is
a
non-
zero probability for the largest eigenvalue of WHPHHWH to grow large.
Nonetheless, Theorem 3.4 can be generalized to the case where the inner
matrix
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

only has almost surely bounded
spectral norm from a simple application of Tonelli’s theorem. Lemma 14.2
provides the exact statement and the proof of this result, which is even
more critical for applications in multi-user MIMO communications, see
Section 14.1. From Theorem 7.1, the almost sure uniform bounded norm of
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

is valid here for uniformly bounded
HPHH. Precautions must however be taken for unbounded HHH, such as for
Rayleigh channels.
Now
1
N tr
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

−1
N tr
 WHPHHWH + σ2IN

= −1
N Pk|hk|2wkwH
k
→0
almost surely. Together, this leads to
wH
k
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

wk −1
N tr
 WHPHHWH + σ2IN

→0
almost surely. The second term on the left-hand side can be divided into σ2 and
the normalized trace of WHPHHWH. The trace can be further rewritten
1
N tr
 WHPHHWH
= 1
N
K
X
i=1
Pi|hi|2wH
i wi.
Assume additionally that the entries of wi have ﬁnite eighth order moment.
Then, from the trace lemma, Theorem 3.4
1
N tr
 WHPHHWH
−1
N
K
X
i=1
Pi|hi|2 a.s.
−→0.
We ﬁnally have that the signal-to-interference plus noise ratio γ(MF)
k
for the
signal of user k after the matched-ﬁltering process satisﬁes
γ(MF)
k
−
Pk|hk|2
1
N
PK
i=1 Pi|hi|2 + σ2
a.s.
−→0.
(12.2)
If P1 = . . . = PK ≜P, K/N →c, and the |hi| are now Rayleigh distributed,
the denominator of the deterministic equivalent in (12.2) converges to
σ2 + c
Z
Pte−tdt = σ2 + Pc.

12.2. Performance of random CDMA technologies
267
If the simpler case when P1 = . . . = PK ≜P, K/N →c, and hi = 1 for all i,
we have simply
γ(MF)
k
a.s.
−→
P
Pc + σ2 .
The spectral eﬃciency associated with the matched-ﬁlter, denoted CMF(σ2),
is the maximum number of bits that can be reliably transmitted per second
and per Hertz of the transmission bandwidth. In the case of large dimensions,
the interference being Gaussian, the spectral eﬃciency in bits/s/Hz is well
approximated by
CMF(σ2) = 1
N
K
X
k=1
log2

1 + γ(MF)
k

.
From the discussion above, we therefore have, for N, K large, that
CMF(σ2) −1
N
K
X
k=1
log2
 
1 +
Pk|hk|2
1
N
PK
i=1 Pi|hi|2 + σ2
!
a.s.
−→0
and therefore we have exhibited a deterministic equivalent of the spectral
eﬃciency of the matched-ﬁlter for all deterministic channels.
When the |hi| arise from a Rayleigh distribution, all the Pi equal P and
K/N →c, we therefore infer that
CMF(σ2)
a.s.
−→c
Z
log2

1 +
Pt
Pc + σ2

e−tdt
= −c log2(e)e
P c+σ2
P
Ei

−Pc + σ2
P

with Ei(x) the exponential integral function
Ei(x) = −
Z ∞
−x
1
t e−tdt.
To obtain this expression, particular care must be taken, since, as mentioned
previously, it is hazardous to take HHH non-uniformly bounded in spectral norm.
However, if the queue of the Rayleigh distribution is truncated at C > 0, a mere
application of the dominated convergence theorem, Theorem 6.3, ensures the
convergence. Growing C large leads to the result.
In the AWGN case, where hi = 1, Pi = P for all i and K/N →c, this is instead
CMF(σ2)
a.s.
−→c log2

1 +
P
Pc + σ2

.
12.2.1.2 MMSE receiver
We assume now that the base station performs the more elaborate minimum
mean square error (MMSE) decoding. The signal y(l) received at the base station

268
12. System performance of CDMA technologies
at time l is now ﬁltered by multiplying it by the MMSE decoder, as
p
Pkh∗
kwH
k
 WHPHHWH + σ2IN
−1 y(l).
In this scenario, the signal-to-interference plus noise ratio γ(MMSE)
k
relative to
the signal of user k is slightly more involved to obtain. The signal power is

Pk|hk|2wH
k
 X
1≤i≤K
Pi|hi|2wiwH
i + σ2IN
!−1
wk


2
while the interference power is
Pk|hk|2 X
j̸=k
Pj|hj|2

wH
k
 X
i
Pi|hi|2wiwH
i + σ2IN
!−1
wj

2
+ σ2Pk|hk|2wH
k
 X
i
Pi|hi|2wiwH
i + σ2IN
!−2
wk.
Working on the interference power, by writing
X
j̸=k
Pj|hj|2wjwH
j =

X
1≤j≤K
Pj|hj|2wjwH
j + σ2IN

−σ2IN −Pk|hk|2wkwH
k
we obtain for the interference power
Pk|hk|2wH
k
 X
i
Pi|hi|2wiwH
i + σ2IN
!−1
wk
−

Pk|hk|2wH
k
 X
i
Pi|hi|2wiwH
i + σ2IN
!−1
wk


2
.
This simpliﬁes the expression of the ratio of signal power against interference
power as
γ(MMSE)
k
=
Pk|hk|2wH
k
 P
1≤i≤K Pi|hi|2wiwH
i + σ2IN
−1 wk
1 −Pk|hk|2wH
k
 P
1≤i≤K Pi|hi|2wiwH
i + σ2IN
−1 wk
.
Applying the matrix inversion lemma, Lemma 6.2, on √Pkhkwk, we ﬁnally
have the compact form of the MMSE SINR
γ(MMSE)
k
= Pk|hk|2wH
k




X
1≤i≤K
i̸=k
Pi|hi|2wiwH
i + σ2IN




−1
wk.

12.2. Performance of random CDMA technologies
269
Now, from Theorem 3.4, as N and K grow large with ratio K/N such that
0 < lim infN K/N ≤lim supN K/N < ∞
wH
k




X
1≤i≤K
i̸=k
Pi|hi|2wiwH
i + σ2IN




−1
wk
−1
N tr
 WHPHHWH −Pi|hi|2wiwH
i + σ2IN
−1 a.s.
−→0.
From Theorem 3.9, we also have that
1
N tr
 WHPHHWH −Pi|hi|2wiwH
i + σ2IN
−1
−1
N tr
 WHPHHWH + σ2IN
−1 →0
where the convergence is sure. Together, the last two equations entail
wH
k

X
i̸=k
Pi|hi|2wiwH
i + σ2IN


−1
wk −1
N tr
 WHPHHWH + σ2IN
−1 a.s.
−→0.
This last expression is the Stieltjes transform mWHPHHWH(−σ2) of the e.s.d.
of WHPHHWH evaluated at −σ2 < 0. Notice that the Stieltjes transform here
is independent of the choice of k. Now, if the e.s.d. of HPHH for all N form
a tight sequence, since W has i.i.d. entries of zero mean and variance 1/N, we
are in the conditions of Theorem 6.1. The Stieltjes transform mWHPHHWH(−σ2)
therefore satisﬁes
mWHPHHWH(−σ2) −mN(−σ2)
a.s.
−→0
where mN(−σ2) is the unique positive solution of the equation in m (see Theorem
3.13, under diﬀerent hypotheses)
m =
 1
N tr HPHH  mHPHH + IK
−1 + σ2
−1
.
(12.3)
We then have that the signal-to-interference plus noise ratio for the signal
originating from user k is close to
Pk|hk|2mN(−σ2)
for all large N, K, where mN(−σ2) is the unique positive solution to
m =
"
σ2 + 1
N
X
1≤i≤K
Pi|hi|2
1 + mPi|hi|2
#−1
.
(12.4)
In the Rayleigh case, the e.s.d. of HHH is also Rayleigh distributed, so that
{F HHH} (indexed by N) forms a tight sequence, which implies the tightness of
{F HPHH}. As a consequence, mN(−σ2) has a deterministic almost sure limit

270
12. System performance of CDMA technologies
m(−σ2) as N, K →∞, K/N →c, which is the unique positive solution to the
equation in m
m =

σ2 + c
Z
Pt
1 + Ptme−tdt
−1
.
(12.5)
The corresponding spectral eﬃciency CMMSE(σ2) of the MMSE receiver, for
noise variance equal to σ2, takes the form
CMMSE(σ2) = 1
N
K
X
k=1
log2

1 + Pk|hk|2wH
k

X
i̸=k
Pi|hi|2wiwH
i + σ2IN


−1
wk

.
For increasing N, K, K/N →c, P1 = . . . = PK = P, and |hi| Rayleigh
distributed for every i, we have:
CMMSE(σ2)
a.s.
−→c
Z
log2
 1 + Ptm(−σ2)

e−tdt
which can be veriﬁed once more by an adequate truncation of the queue of the
Rayleigh distributed at C > 0 and then taking C →∞.
In the AWGN scenario, (12.4) becomes
m =
1 + mP
σ2 + cP + mPσ2
which can be expressed as a second order polynomial in m, whose unique positive
solution m(−σ2) reads:
m(−σ2) = −(σ2 + (c −1)P) +
p
(σ2 + (c −1)P)2 + 4Pσ2
2Pσ2
(12.6)
which is therefore the almost sure limit of mN(−σ2), for N, K →∞. The spectral
eﬃciency therefore has the deterministic limit
CMMSE(σ2)
a.s.
−→c log2
 
1 + −(σ2 + (c −1)P) +
p
(σ2 + (c −1)P)2 + 4Pσ2
2σ2
!
.
12.2.1.3 Optimal receiver
The optimal receiver jointly decodes the data streams from all users. Its spectral
eﬃciency Copt(σ2) is simply
Copt(σ2) = 1
N log det

IN + 1
σ2 WHPHHWH

.

12.2. Performance of random CDMA technologies
271
A straightforward application of Corollary 6.1 leads to the limiting result
Copt(σ2) −log2
 
1 +
1
σ2N
K
X
k=1
Pk|hk|2
1 + cPk|hk|2mN(−σ2)
!
−1
N
K
X
k=1
log2
 1 + cPk|hk|2mN(−σ2)

−log2(e)
 σ2mN(−σ2) −1
 a.s.
−→0.
for P1|h1|2, . . . , PK|hK|2 uniformly bounded across N, where mN(−σ2) is the
unique positive solution of (12.4).
In the case of growing system dimensions and Rayleigh fading on all links,
with similar arguments as previously, we have the almost sure convergence
Copt(σ2)
a.s.
−→log2

1 + c
σ2
Z
Pt
1 + Ptm(−σ2)e−tdt

+ c
Z
log2
 1 + Ptm(−σ2)

e−tdt
+ log2(e)
 σ2m(−σ2) −1

.
with m(−σ2) deﬁned here as the unique positive solution to (12.5).
In the AWGN scenario, this is instead
Copt(σ2)
a.s.
−→log2

1 +
c
σ2(1 + Pm(−σ2))

+ c log2
 1 + Pm(−σ2)

+ log2(e)
 σ2m(−σ2) −1

.
with m(−σ2) deﬁned here by (12.6).
In Figure 12.1 and Figure 12.3, a comparison is made between the matched-
ﬁlter, the MMSE ﬁlter, and the optimum decoder, for K = 16 users and N = 32
chips per CDMA code, for diﬀerent SNR values, for the scenarios of AWGN
channels and Rayleigh fading channels, respectively. In these graphs, theoretical
expressions are compared against Monte Carlo simulations, the average and
standard deviation of which are displayed. We observe, already for these small
values of K and N, that the deterministic equivalents for the matched-ﬁlter, the
minimum mean square error ﬁlter, and the optimal decoder fall within (plus or
minus) one standard deviation of the empirical spectral eﬃciency.
In Figure 12.2 and Figure 12.4, the analysis of the spectral eﬃciency for
diﬀerent limiting ratios c = lim K/N is performed for these very decoders and
for AWGN and Rayleigh fading channels, respectively. The SNR is set to 10 dB.
Note that the rate achieved by the optimal decoder grows unbounded as c grows
large. This emerges naturally from the fact that the longer the CDMA codes, the
more important the data redundancy. The matched-ﬁlter also beneﬁts from short
code length. On the opposite, the MMSE decoder beneﬁts only from moderate
ratios K/N and especially suﬀers from large K/N. This can be interpreted from

272
12. System performance of CDMA technologies
−5
0
5
10
15
20
25
30
0
1
2
3
4
5
SNR [dB]
Spectral eﬃciency [bits/s/Hz]
MF, det. eq.
MF, simulation
MMSE, det. eq.
MMSE, simulation
Optimal, det. eq.
Optimal, simulation
Figure 12.1 Spectral eﬃciency of random CDMA decoders, AWGN channels.
Comparison between simulations and deterministic equivalents (det. eq.), for the
matched-ﬁlter, the MMSE decoder, and the optimal decoder, K = 16 users, N = 32
chips per code. Rayleigh channels. Error bars indicate two standard deviations.
0
1
2
3
4
0
1
2
3
4
5
c
Spectral eﬃciency [bits/s/Hz]
MF
MMSE
Optimal
Figure 12.2 Spectral eﬃciency of random CDMA decoders, for diﬀerent asymptotic
ratios c = lim K/N, SNR=10 dB, AWGN channels. Deterministic equivalents for the
matched-ﬁlter, the MMSE decoder, and the optimal decoder. Rayleigh channels.
the fact that, for N small compared to K, every user data stream at the output
of the MMSE ﬁlter is strongly interfered with by inter-code interference from
other users. The SINR for every stream is therefore very impacted, to the extent
that the spectral eﬃciency of the MMSE decoder is signiﬁcantly reduced.

12.2. Performance of random CDMA technologies
273
−5
0
5
10
15
20
25
30
0
1
2
3
4
5
SNR [dB]
Spectral eﬃciency [bits/s/Hz]
MF, det. eq.
MF, simulation
MMSE, det. eq.
MMSE, simulation
Optimal, det. eq.
Optimal, simulation
Figure 12.3 Spectral eﬃciency of random CDMA decoders, Rayleigh fading channels.
Comparison between simulations and deterministic equivalents (det. eq.), for the
matched-ﬁlter, the MMSE decoder, and the optimal decoder, K = 16 users, N = 32
chips per code. Rayleigh channels. Error bars indicate two standard deviations.
0
1
2
3
4
0
1
2
3
4
5
c
Spectral eﬃciency [bits/s/Hz]
MF
MMSE
Optimal
Figure 12.4 Spectral eﬃciency of random CDMA decoders, for diﬀerent asymptotic
ratios c = lim K/N, SNR=10 dB, Rayleigh fading channels. Deterministic equivalents
for the matched-ﬁlter, the MMSE decoder, and the optimal decoder.
12.2.2
Random CDMA in uplink frequency selective channels
We consider the same transmission model as in Section 12.2.1, but the
transmission channels are now frequency selective, i.e. contain multiple paths. We
then replace the ﬂat channel fading hk coeﬃcients in (12.1) by the convolution

274
12. System performance of CDMA technologies
Toeplitz matrix H(0)
k
∈CN×N, constant over time and given by:
H(0)
k
≜













hk,0
0
· · ·
· · ·
· · ·
0
...
...
...
· · ·
...
...
hk,Lk−1
...
hk0
...
...
...
0
hk,Lk−1
...
hk,0
...
...
...
...
...
...
...
0
0
· · ·
0 hk,Lk−1 · · · hk,0













where the coeﬃcient hk,l stands for the lth path of the multi-path channel H(0)
k ,
and the number of relevant such paths is supposed equal to Lk. We assume the
hk,l uniformly bounded over both k and l. In addition, because of multi-path,
inter-symbol interference arises between the symbols s(l−1)
k
transmitted at time
l −1 and the symbols s(l)
k . Under these conditions, (12.1) has now an additional
contribution due to the interference from the previously sent symbol and we
ﬁnally have
y(l) =
K
X
k=1
H(0)
k wk
p
Pks(l)
k +
K
X
k=1
H(1)
k wk
p
Pks(l−1)
k
+ n(l)
(12.7)
where H(1)
k
is deﬁned by
H(1)
k
≜













0 · · ·
0 hk,Lk−1 · · ·
hk,1
... ... ...
...
...
...
... ... ...
...
... hk,Lk−1
...
...
...
...
...
0
... ... · · ·
...
...
...
0 · · · · · ·
· · ·
· · ·
0













.
If maxk(Lk) is small compared to N, as N grows large, it is rather intuitive
that the term due to inter-symbol interference can be neglected, when estimating
the performances of the CDMA decoders. This is because the matrices H(1)
k
are
ﬁlled with zeros but in the upper 1
2Lk(Lk −1) elements. This number is of order
o(N) and the elements are uniformly bounded with increasing N. Informally, the
following therefore ought to be somewhat correct for large N and K
y(l) ≃
K
X
k=1
H(0)
k wk
p
Pks(l)
k + n(l).
We could now work with this model and evaluate the performance of the
diﬀerent decoding modes, which will feature in this case the Gram matrix of
a random matrix with independent columns wk left-multiplied by H(0)
k
for k ∈
{1, . . . , K}. Therefore, it will be possible to invoke Theorem 6.12 to compute

12.2. Performance of random CDMA technologies
275
in particular the performance of the matched-ﬁlter, MMSE and optimal uplink
decoders. Nonetheless, the resulting ﬁnal expressions will be given as a function
of the matrices H(0)
k , rather than as a function of the entries hk,j. Instead, we will
continue the successive model approximations by modifying further H(0)
k
into a
more convenient matrix form, asymptotically equivalent to H(0)
k .
From the same line of reasoning as previously, we can indeed guess that ﬁlling
the triangle of 1
2Lk(Lk −1) upper right entries of H(0)
k
with bounded elements
should not alter the ﬁnal result in the large N, K limit. We may therefore replace
H(0)
k
by H ≜H(0)
k
+ H(1)
k , leading to
y(l) ≃
K
X
k=1
Hkwk
p
Pks(l)
k + n(l).
(12.8)
The right-hand side of (12.8) is more interesting to study, as Hk is a circulant
matrix
Hk ≜













hk,0
0
· · · hk,Lk−1 · · ·
hk,1
...
...
...
...
...
...
hk,Lk−1
...
hk,0
...
... hk,Lk−1
0
hk,Lk−1
...
hk,0
...
...
...
...
...
...
...
0
0
· · ·
0
hk,Lk−1 · · ·
hk,0













which can be written under the form Hk = FH
NDkFN, with FN the discrete
Fourier transform matrix of order N
with entries FN,ab = e−2πi (a−1)(b−1)
N
.
Moreover, the diagonal entries of Dk are the discrete Fourier transform
coeﬃcients of the ﬁrst column of Hk [Gray, 2006], i.e. with dab ≜Da,bb
dab =
La−1
X
n=0
ha,ne−2πi bn
N .
All Hk matrices are therefore diagonalizable in a common eigenvector basis
and we have that the right-hand side of (12.8), multiplied on the left by FN,
reads:
z(l) ≜
K
X
k=1
Dk ˜wk
p
Pks(l)
k + ˜n(l)
(12.9)
with ˜wk ≜FNwk and ˜n(l) ≜FNn(l).
In order to simplify the problem, we now need to make the strong assumption
that the vectors wk have independent Gaussian entries. This ensures that ˜wk
also has i.i.d. entries (incidentally Gaussian).
We wish to study the performance of linear decoders for the model (12.7).
To prove that it is equivalent to work with (12.7) or with (12.9), as N, K grow
large, we need to prove that the diﬀerence between the ﬁgure of merit (say

276
12. System performance of CDMA technologies
here, the SINR) for the model y(l) and the ﬁgure of merit for the model z(l)
is asymptotically almost surely zero. This can be proved, see, e.g., [Chaufray
et al., 2004], using Szeg¨o’s theorem, the Markov inequality, Theorem 3.5, and
the Borel–Cantelli lemma, Theorem 3.6, in a similar way as in the proof of
Theorem 3.4 for instance. For this condition to hold, it suﬃces that L/N →0 as
N grows large with L an upper bound on L1, . . . , LK for all K large, that the
Hk matrices are bounded in spectral norm, and that there exists a < b such that
0 < a < Pk < b < ∞for all k, uniformly on K.
Indeed, the condition on the equivalence of Toeplitz and circulant matrices is
formulated rigorously by Szeg¨o’s theorem, given below.
Theorem 12.1 (Theorem 4.2 of [Gray, 2006]). Let . . . , t−2, t−1, t0, t1, t2, . . . be
a summable sequence of real numbers, i.e. such that
∞
X
k=−∞
|tk| < ∞.
Denote
TN ∈CN×N
the
Toeplitz
matrix
with
kth
column
the
vector
(t−k+1, . . . , tN−k)T. Then, denoting τN,1, . . . , τN,N the eigenvalues of TN, for
any positive s
lim
N→∞
1
N
N−1
X
k=0
τ s
N,k = 1
2π
Z 2π
0
f(λ)dλ
with
f(λ) ≜
∞
X
k=−∞
tkeiλ
the Fourier transform of . . . , t−2, t−1, t0, t1, t2, . . .. In particular, if tk = 0 for k <
0 and k > K for some constant K, then the series is ﬁnite and then absolutely
summable, and the l.s.d. of TN is the l.s.d. of the circulant matrices with ﬁrst
column (t0, . . . , tK−1)T.
From now on, we claim that model (12.9) is equivalent to model (12.7) for the
studies to come, in the sense that we can work either with (12.9) or with (12.7)
and will end up with the same asymptotic performance results, but for a set of
w1, . . . , wK of probability one. Equation (12.9) can be written more compactly
as
z(l) = XP
1
2 s + ˜n(l)
where the (i, j)th entry Xij of X ∈CN×K has zero mean, E[|Xij|2] = |dji|2 and
the elements Xij/dji are identically distributed. This is the situation of a channel
model with variance proﬁle. This type of model was ﬁrst studied in Theorem 3.14,
when the matrix of the dij has a limiting spectrum, and then in Theorem 6.14
in terms of a deterministic equivalent, in a more general case.

12.2. Performance of random CDMA technologies
277
We consider successively the performance of the matched-ﬁlter, the MMSE
decoder, and the optimal receiver in the frequency selective case.
12.2.2.1 Matched-ﬁlter
The matched-ﬁlter here consists, for user k, in ﬁltering z(l) by the kth column
of X as xH
k z(l). From the previous derivation, we have that the SINR γ(MF)
k
at
the output of the matched-ﬁlter reads:
γ(MF)
k
=
Pk|xH
k xk|2
xH
k
 XPXH −PkxkxH
k + σ2IN

xk
.
(12.10)
The xk are deﬁned as xk = Dk ˜wk, where ˜wk has i.i.d. entries of zero mean
and variance 1/N, independent of the Dk. The trace lemma therefore ensures
that
˜wH
k DH
k Dk ˜wk −1
N tr DH
k Dk
a.s.
−→0
where the trace can be rewritten
1
N tr DH
k Dk = 1
N
N
X
i=1
|dk,i|2.
As for the denominator of (12.10), notice that the inner matrix has entries
independent of xk, which we would ideally like to be of almost sure uniformly
bounded spectral norm, so that the trace lemma, Lemma 14.2, can operate as
before, i.e.
xH
k
 XPXH −PkxkxH
k + σ2IN

xk = ˜wH
k DH
k
 XPXH −PkxkxH
k + σ2IN

Dk ˜wk
would satisfy
xH
k
 XPXH −PkxkxH
k + σ2IN

xk −1
N tr DkDH
k
 XPXH + σ2IN
 a.s.
−→0.
However, although extensive Monte Carlo simulations suggest that ∥XPXH∥
indeed is uniformly bounded almost surely, it is not proved to this day that this
holds true. For the rest of this section, we therefore mainly conjecture this result.
From the deﬁnition of X, we then have
1
N tr DkDH
k
 XPXH + σ2IN

−1
N 2
N
X
n=1
|dk,n|2
"
σ2 +
X
1≤i≤K
Pi|di,n|2
#
a.s.
−→0.
(12.11)
And we ﬁnally have
γ(MF)
k
−
Pk

1
N
PN
n=1 |dk,n|22
1
N 2
PN
n=1
PK
i=1 Pi|dk,n|2|di,n|2 + σ2 1
N
PN
n=1 |dk,n|2
a.s.
−→0
where the deterministic equivalent is now clearly dependent on k.

278
12. System performance of CDMA technologies
The spectral eﬃciency CMF(σ2) in that case satisﬁes
CMF(σ2)
−1
N
K
X
k=1
log2


1 +
Pk

1
N
PN
n=1 |dk,n|22
1
N 2
PN
n=1
PK
i=1 Pi|dk,n|2|di,n|2 + σ2 1
N
PN
n=1 |dk,n|2



a.s.
−→0.
Contrary to previous sections, we will not derive any limiting result when the
random h1, . . . , hL variables have a given distribution. Indeed, in general, for
growing N, there does not exist a limit to CMF(σ2). Instead, we can assume, as
is often done, that the doubly inﬁnite array {|dk,n|2, k ≥1, n ≥1} converges, in
the sense that there exists a function p(x, y) for (x, y) ∈[0, 1] × [0, 1] such that
dk,n −
Z
k
N
k−1
N
Z
n
N
n−1
N
p(x, y)dxdy →0.
(12.12)
This is convenient to obtain a deterministic limit of CMF(σ2). This is the
assumption taken for instance in [Tulino et al., 2005] in the case of multi-carrier
random CDMA transmissions. Assume then that the dk,n are such that they
converge to p(x, y), in the sense of (12.12). The value p(x, y)dxdy represents the
square of the absolute channel fading coeﬃcient experienced by dx terminal users
indexed by the integers k such that x −dx/2 < k/K < x + dx/2, at normalized
frequency indexes n such that y −dy/2 < n/N < y + dy/2. Therefore, we will
denote p(x, y) = |h(x, y)|2 for any convenient h(x, y). We will further denote P(x)
a function such that user k satisfying x −dx/2 < k/K < x + dx/2 consumes
power P(x) −dP(x) < Pk < P(x) + dP(x).
In this scenario, we have that the matched-ﬁlter spectral eﬃciency satisﬁes
CMF(σ2)
a.s.
−→
c
Z 1
0
log2


1 +
P(κ)
R 1
0 |h(κ, f)|2df
2
c
R 1
0
R 1
0 P(κ′)|h(κ, f)|2|h(κ′, f)|2dfdκ′ + σ2 R 1
0 |h(κ, f)|2df


dκ.
12.2.2.2 MMSE decoder
We now consider the SINR minimizer MMSE decoder, that, in our context, ﬁlters
the receiver input z(l) as
PkxH
k
 XPXH + σ2IN
−1 z(l).
The SINR γ(MMSE)
k
relative to user k reads here
γ(MMSE)
k
= PkxH
k




X
1≤i≤K
i̸=k
PixixH
i + σ2IN




−1
xk.

12.2. Performance of random CDMA technologies
279
The inner part of the inverse matrix is independent of the entries of xk.
Since xk = Dk ˜wk with ˜wk a vector of i.i.d. entries with variance 1/N, we have,
similarly as before, that
γ(MMSE)
k
−Pk
N tr DkDH
k
 XPXH + σ2IN
−1 a.s.
−→0
as N, n grow large. Now notice that XP
1
2 is still a matrix with independent
entries and variance proﬁle {Pjσ2
ij}, 1 ≤i ≤N, 1 ≤j ≤n. From Theorem 6.10,
it turns out that the trace on the left-hand side satisﬁes
Pk
N tr DkDH
k
 XPXH + σ2IN
−1 −ek(−σ2)
a.s.
−→0
where ek(z), z ∈C \ R+, is deﬁned as the unique Stieltjes transform that satisﬁes
ek(z) = −1
z
1
N
N
X
n=1
Pk|dkn|2
1 + K
N ¯en(z)
where ¯en(z) is given by:
¯en(z) = −1
z
1
K
K
X
i=1
Pi|din|2
1 + ei(z).
Note that the factor K/N is placed in the denominator of the term ek(z) here
instead of the denominator of the term ¯ek(z), contrary to the initial statement
of Theorem 6.10. This is due to the fact that X is here an N × K matrix with
entries of variance |dij|2/N and not |dij|2/K. Particular care must therefore be
taken here when propagating the term K/N in the formula of Theorem 6.10.
We conclude that the spectral eﬃciency C(MMSE)(σ2) for the MMSE decoder
in that case satisﬁes
C(MMSE)(σ2) −1
N
K
X
k=1
log2
 1 + ek(−σ2)
 a.s.
−→0.
Similar to the MF case, there does not exist a straightforward limit to
C(MMSE)(σ2) for practical distributions of h1, . . . , hL. However, if the user-
frequency channel decay |dk,n|2 converges to a density |h(κ, f)|2 for users indexed
by k ≃κK and for normalized frequencies n ≃fN, and that users within dκ of κ
have power P(κ), then C(MMSE)(σ2) has a deterministic almost sure limit, given
by:
C(MMSE)(σ2)
a.s.
−→c
Z 1
0
log2 (1 + e(κ)) dκ
for e(κ) the function deﬁned as a solution to the diﬀerential equations
e(κ) = 1
σ2
Z 1
0
P(κ)|h(κ, f)|2
1 + c¯e(f)
df
¯e(f) = 1
σ2
Z 1
0
P(κ)|h(κ, f)|2
1 + e(κ)
dκ.
(12.13)

280
12. System performance of CDMA technologies
12.2.2.3 Optimal decoder
The spectral eﬃciency Copt(σ2) of the frequency selective uplink CDMA reads:
Copt(σ2) = 1
N log det

IN + 1
σ2 XPXH

.
A deterministic equivalent for Copt(σ2) is then provided by extending
Theorem 6.11 with the conjectured asymptotic boundedness of XPXH by a
straightforward application of the dominated convergence theorem, Theorem 6.3.
Namely, we have:
Copt(σ2)−
"
1
N
N
X
n=1
log2

1 + K
N ¯en(−σ2)

+ 1
N
K
X
k=1
log2
 1 + ek(−σ2)

−log2(e)
σ2
1
N 2
X
1≤n≤N
1≤k≤K
|dkn|2
 1 + K
N ¯en(−σ2)

(1 + ek(−σ2))


a.s.
−→0
where the ¯en(−σ2) and ek(−σ2) are deﬁned as in the previous MMSE case. The
conjectured result is however known to hold in expectation by applying directly
the result of Theorem 6.11.
As for the linear decoders, if |dk,n|2 has a density limit |h(κ, f)|2, then Copt(σ2)
converges almost surely as follows.
Copt(σ2)
a.s.
−→
Z 1
0
log2 (1 + c¯e(f)) df + c
Z 1
0
log2 (1 + e(κ)) dκ
−log2(e)
σ2
c
Z 1
0
Z 1
0
|h(κ, f)|2
(1 + c¯e(f)) (1 + e(κ))dκdf

where the functions e and ¯e are solutions of (12.13).
In Figure 12.5, we compare the performance of random CDMA detectors as
a function of the channel frequency selectivity, for diﬀerent ratios K/N. We
successively assume that the channel is Rayleigh fading, of length L = 1, L = 2,
and L = 8, the coeﬃcients hl being i.i.d. Gaussian of variance 1/L. For simulation
purposes, we consider a single realization, i.e. we do not average realizations, of
the channel condition for an N = 512 random CDMA transmission. We observe
various behaviors of the detectors against frequency selectivity. In particular, the
optimal decoder clearly beneﬁts from channel diversity. Nonetheless, although
this is not represented here, no further gain is obtained for L > 8; therefore,
there exists a diversity threshold above which the uplink data rate does not
increase. The matched-ﬁlter follows the same trend, as it beneﬁts as well from
frequency diversity. The case of the MMSE decoder is more intriguing, as the
latter beneﬁts from frequency selectivity only for ratios K/N lower than one, i.e.
for less users than code length, and suﬀers from channel frequency selectivity for
K/N ≪1. In our application example, K/N ≤1 is the most likely assumption,
in order for the CDMA codes to be almost orthogonal.

12.2. Performance of random CDMA technologies
281
0
1
2
3
4
0
1
2
3
4
5
K/N
Spectral eﬃciency [bits/s/Hz]
MF
MMSE
Optimal
Figure 12.5 Spectral eﬃciency of random CDMA decoders, for diﬀerent ratios K/N,
SNR=10 dB, Rayleigh frequency selective fading channels. Deterministic equivalents
for the matched-ﬁlter, the MMSE decoder, and the optimal decoder; N = 512, L = 1
in dashed lines, L = 4 in dotted lines, L = 8 in plain lines.
12.2.3
Random CDMA in downlink frequency selective channels
We now consider the downlink CDMA setting, where the base station issues data
for the K terminals. Instead of characterizing the complete rate region of the
broadcast channel, we focus on the capacity achieved by a speciﬁc terminal k ∈
{1, . . . , K}. In the case of frequency selective transmissions, the communication
model can be written at time l as
y(l)
k
=
p
PkH(0)
k Ws(l) +
p
PkH(1)
k Ws(l−1) + n(l)
k
(12.14)
with y(l)
k
∈CN the N-chip signal received by terminal k at time l, W =
[w1, . . . , wK] ∈CN×K, where wi is now the downlink random CDMA code
intended for user i, s(l) = [s(l)
1 , . . . , s(l)
K ]T ∈CK with s(l)
i
the signal intended for
user i at time l, Pk is the mean transmit power of user k, H(0)
k
∈CN×N is
the Topelitz matrix corresponding to the frequency selective channel from the
base station to user k, H(1)
k
∈CN×N the upper-triangular matrix that takes into
account the inter-symbol interference, and n(l)
k ∈CN is the N-dimensional noise
received by terminal k at time l.
Similar to the uplink case, we can consider for (12.14) the approximated model
y(l)
k
≃
p
PkHkWs(l) + n(l)
k
(12.15)
where Hk ∈CN×K is the circulant matrix equivalent to H(0)
k . It can indeed be
shown, see, e.g., [Debbah et al., 2003a], that the asymptotic SINR sought for are
the same in both models. We study here the two receive linear decoders that are
the matched-ﬁlter and the MMSE decoder. The optimal joint decoding strategy

282
12. System performance of CDMA technologies
is rather awkward in the downlink, as it requires highly ineﬃcient computational
loads at the user terminals. This will not be treated.
12.2.3.1 Matched-ﬁlter
Similar to the uplink approach, the matched-ﬁlter in the downlink consists for
user k to ﬁlter the input y(l)
k
by its dedicated code convoluted by the channel
wH
k HH
k . The SINR γ(MF)
k
for user k is then simply given by:
γ(MF)
k
=
Pk
wH
k HH
k Hkwk
2
wH
k HH
k
 PkHkWWHHH
k −PkHkwkwH
k HH
k + σ2IN

Hkwk
.
Using similar tools as for the uplink case, we straightforwardly have that
γ(MF)
k
−1
N 2
Pk
PN
n=1 |dk,n|22
σ2 1
N
PN
n=1 |dk,n|2 + K−1
N2 Pk
PN
n=1 |dk,n|4
a.s.
−→0
(12.16)
where dk,n is deﬁned as above by
dk,n ≜
Lk−1
X
l=0
hk,le−2πi nl
N .
From this expression, we then have that the sum rate CMF achieved by the
broadcast channel satisﬁes
CMF(σ2) −1
N
K
X
k=1
log2


1 +
Pk

1
N
PN
n=1 |dk,n|22
σ2 1
N
PN
n=1 |dk,n|2 + K−1
N2 Pk
PN
n=1 |dk,n|4



a.s.
−→0.
When P1, . . . , PK have a limiting density P(κ), the |dk,n| have a limiting
density |h(κ, f)|, and K/N →c, then asymptotically
CMF(σ2)
a.s.
−→c
Z 1
0
log2

1 + P(κ)
R 1
0 |h(κ, f)|2df
2
σ2 R 1
0 |h(κ, f)|2df + c
R 1
0 |h(κ, f)|4df
dκ

.
12.2.3.2 MMSE decoder
For
the
more
advanced
MMSE
decoder,
i.e.
the
linear
decoder
that
consists
in
retrieving
the
transmit
symbols
from
the
product
wH
k HH
k
 PkHkWWHHH
k + σ2IN
−1 y(l)
k ,
the
SINR
γ(MMSE)
k
for
the
data
intended for user k is explicitly given by:
γ(MMSE)
k
= PkwH
k HH
k
 PkHkWWHHH
k −PkHkwkwH
k HH
k + σ2IN
−1 Hkwk.
As in the uplink case, the central matrix is independent of wk and therefore the
trace lemma ensures that the right-hand side expression is close to the normalized
trace of the central matrix, which is its Stieltjes transform at point −σ2. We
therefore use again the deterministic equivalent of Theorem 6.1 to obtain
γ(MMSE)
k
−ek(−σ2)
a.s.
−→0

12.2. Performance of random CDMA technologies
283
where ek(−σ2) is the unique real positive solution to the equation in e
e = 1
N
N
X
n=1
Pk|dk,n|2
1
1+e
K−1
N Pk|dk,n|2 + σ2 .
(12.17)
The resulting achievable sum rate CMMSE for the MMSE decoded broadcast
channel is then such that
CMMSE(σ2) −1
N
K
X
k=1
log2
 1 + ek(−σ2)
 a.s.
−→0.
When the channel has a limiting space-frequency power density |h(κ, f)|2
and the users within dκ of κ have inverse path loss P(κ), the sum rate has
a deterministic limit given by:
CMMSE(σ2)
a.s.
−→c
Z 1
0
log2 (1 + e(κ))
where the function e(κ) satisﬁes
e(κ) =
Z 1
0
P(κ)|h(κ, f)|2
c
1+e(κ)P(κ)|h(κ, f)|2 + σ2 df.
Before moving to the study of orthogonal CDMA transmissions, let us recall
[Poor and Verd´u, 1997] that the multiple access interference incurred by the non-
orthogonal users can be considered roughly Gaussian in the large dimensional
system limit. As a consequence, the bit error rate BER induced, e.g. by the
MMSE decoder for user k and for QPSK modulation, is of order
BER ≃Q
q
γ(MMSE)
k

with Q the Gaussian Q-function, deﬁned by
Q(x) ≜
1
√
2π
Z ∞
x
e−t2
2 dt.
We can therefore give an approximation of the average bit error rate in the
downlink decoding. This is provided in Figure 12.6, where it can be seen that, for
small ratios K/N, the deterministic approximation of the bit error rate is very
accurate even for not too large K and N. In contrast, for larger K/N ratios,
large K, N are demanded for the deterministic approximation to be accurate.
Note additionally that asymptotic gaussianity of the SINR at the output of
the MMSE receiver for all cases above can be also proved, although this is
not detailed here, see, e.g., [Guo et al., 2002; Tse and Zeitouni, 2000]. Also,
extensions of the above results to the multiple antenna case were studied in [Bai
and Silverstein, 2007; Hanly and Tse, 2001] as well as to asynchronous random
CDMA in [Cottatellucci et al., 2010a,b; Hwang, 2007; Mantravadi and Veeravalli,
2002].

284
12. System performance of CDMA technologies
−5
0
5
10
15
10−4
10−3
10−2
10−1
100
SNR [dB]
Bit error rate
Sim., K = 16, N = 32
Sim., K = 64, N = 128
Det. eq., c = 1/2
Sim., K = 32, N = 32
Sim., K = 128, N = 128
Det. eq., c = 1
Figure 12.6 Bit error rate achieved by random CDMA decoders in the downlink,
AWGN channel. Comparison between simulations (sim.) and deterministic equivalents
(det. eq.) for the MMSE decoder, K = 16 users, N = 32 chips per code.
12.3
Performance of orthogonal CDMA technologies
The
initial
incentive
for
using
CDMA
schemes
in
multi-user
wireless
communications is based on the idea that orthogonality between users can be
brought about by codes, instead of separating user transmissions by using time
division or frequency division multiplexing. This is all the more convenient when
the codes are perfectly orthogonal and the communication channel is frequency
ﬂat. In this scenario, the signals received either in the uplink by the base station
or in the downlink by the users are perfectly orthogonal. If the channel is
frequency selective, the code orthogonality is lost so that orthogonal codes are
not much better than random codes, as will clearly appear in the following.
Moreover, it is important to recall that orthogonality is preserved on the sole
condition that all codes are sent and received simultaneously. In the uplink, this
imposes all users to be synchronous in their transmissions and that the delay
incurred by the diﬀerence of wave travel distance between the user closest and
the user furthest to the base station is small enough.
For all these reasons, in days when CDMA technologies are no longer used
exclusively for communications over narrowband channels, the question is posed
of whether orthogonal CDMA is preferable to random CDMA. This section
provides the orthogonal version of the results derived in the previous section
for i.i.d. codes. It will be shown that orthogonal codes perform always better
than random codes in terms of achievable rates, although the diﬀerence becomes
marginal as the channel frequency selectivity increases.

12.3. Performance of orthogonal CDMA technologies
285
Similar to the previous section, we start by the study of orthogonal CDMA in
the uplink.
12.3.1
Orthogonal CDMA in uplink frequency ﬂat channels
We start with the transmission model (12.1) where now w1, . . . , wK are K ≤N
columns of a Haar matrix, and W = [w1, . . . , wK] is such that WHW = IK. We
deﬁne H and P as before.
Consider the matched-ﬁlter for which the SINR ¯γ(MF)
k
for user k is given by:
¯γ(MF)
k
=
Pk|hk|2wH
k wk
wH
k
 WHPHHWH −Pk|hk|2wkwH
k + σ2IN

wk
= Pk|hk|2wH
k wk
σ2
which unfolds from the code orthogonality.
Since we have, for all N
wH
i wj = δj
i
with δj
i the Kronecker delta, we have simply
¯γ(MF)
k
= Pk|hk|2
σ2
and the achievable sum rate ¯C(MF) satisﬁes
¯C(MF) = 1
N
K
X
k=1
log2

1 + Pk|hk|2
σ2

.
This is the best we can get as this also corresponds to the capacity of both
the MMSE and the optimal joint decoder.
12.3.2
Orthogonal CDMA in uplink frequency selective channels
We now move to the frequency selective model (12.8).
12.3.2.1 Matched-ﬁlter
The SINR ¯γ(MF)
k
for the signal originating from user k reads as before
¯γ(MF)
k
=
Pk
xH
k xk
2
xH
k
 XPXH −PkxkxH
k + σ2IN

xk
where xk = DkFNwk, Dk being diagonal deﬁned as previously, and FN is the
Fourier transform matrix. Since W is formed of columns of a Haar matrix, FNW
is still Haar distributed.

286
12. System performance of CDMA technologies
From the trace lemma, Corollary 6.3, we have again that the numerator
satisﬁes
wH
k FH
NDH
k DkFNwk −1
N tr DH
k Dk
a.s.
−→0
as long as the inner matrix has almost surely uniformly bounded spectral norm.
The second term in the left-hand side is simply
1
N
PN
n=1 |dkn|2 with dkn the nth
diagonal entry of Dk. To handle the denominator, we need the following result.
Lemma 12.1 ([Bonneau et al., 2005]). Let W = [w1, . . . , wK] ∈CN×K, K ≤N,
be K columns of a Haar random unitary matrix, and A ∈CN×K be independent
of W and have uniformly bounded spectral norm. Denote X = [x1, . . . , xK] the
matrix with (i, j)th entry wijaij. Then, for k ∈{1, . . . , K}
xH
k XXHxk −

1
N 2
N
X
n=1
X
j̸=k
|ank|2|anj|2 −1
N 3
X
j̸=k

N
X
n=1
akna∗
jn

2
a.s.
−→0.
Remark 12.1. Compared to the case where wi has i.i.d. entries (see, e.g., (12.11)),
observe that the i.i.d. and Haar cases only diﬀer by the additional second term
in brackets. Observe also that wH
k XXHwk is necessarily asymptotically smaller
when W is Haar than if W has i.i.d. entries. Therefore, the interference term at
the output of the matched-ﬁlter is asymptotically smaller and the resulting SINR
larger. Note also that the almost sure convergence is easy to verify compared to
the (only conjectured) i.i.d. counterpart, since the eigenvalues of unitary matrices
are all of unit norm.
Applying Lemma 12.1 to the problem at hand, we ﬁnally have that the
diﬀerence between ¯γ(MF)
k
and
Pk

1
N
PN
n=1 |dk,n|22
P
i̸=k
PN
n=1
Pi
N2 |dk,n|2|di,n|2 −P
i̸=k
Pi
N3
PN
n=1 dk,nd∗
i,n

2
+ σ2
N
PN
n=1 |dk,n|2
is asymptotically equal to zero in the large N, K limit, almost surely. The
resulting deterministic equivalent for the capacity unfolds directly.
To this day, a convenient deterministic equivalent for the capacity of the
MMSE decoder in the frequency selective class has not been proposed, although
non-convenient forms can be obtained using similar derivations as in the ﬁrst
steps of the proof of Theorem 6.17. This is because the communication model
involves random Haar matrices with a variance proﬁle, which are more involved
to study.
12.3.3
Orthogonal CDMA in downlink frequency selective channels
We now consider the downlink model (12.15) and, again, turn W into K columns
of an N × N Haar matrix.

12.3. Performance of orthogonal CDMA technologies
287
12.3.3.1 Matched-ﬁlter
The SINR ¯γ(MF)
k
for user k at the output of the matched-ﬁlter is given by:
¯γ(MF)
k
=
Pk
wH
k HH
k Hkwk
2
wH
k HH
k
 PkHkWWHHH
k −PkHkwkwH
k HH
k + σ2IN

Hkwk
.
Since Hk is a circulant matrix and W is unitarily invariant, by writing
FNHkFH
N = Dk and ˜wk = FNwk, ˜
W = [ ˜w1, . . . , ˜wK] ( ˜
W is still unitary and
unitarily invariant), Dk is diagonal and the SINR reads:
¯γ(MF)
k
=
Pk
 ˜wH
k DH
k Dk ˜wk
2
˜wH
k DH
k

PkDk ˜
W ˜
WHDH
k −PkDk ˜wk ˜wH
k DH
k + σ2IN

Dk ˜wk
.
The numerator is as usual such that
˜wH
k DH
k Dk ˜wk −1
N
N
X
i=1
|dk,n|2 a.s.
−→0.
As for the denominator, we invoke once more Lemma 12.1 with a variance
proﬁle a2
ij with aij constant over j. We therefore obtain
˜wH
k DH
k

Dk ˜
W ˜
WHDH
k −Dk ˜wk ˜wH
k DH
k

Dk ˜wk
−

K −1
N 2
N
X
n=1
|dk,n|4 −K −1
N 3
 N
X
n=1
|dk,n|2
!2
a.s.
−→0.
A deterministic equivalent for the SINR therefore unfolds as
¯γ(MF)
k
−
Pk

1
N
PN
i=1 |dk,n|22
Pk(K−1)
N 2
PN
n=1 |dk,n|4 −Pk(K−1)
N3
PN
n=1 |dk,n|2
2
+ σ2
N
PN
i=1 |dk,n|2
a.s.
−→0.
Compared to Equation (12.16), observe that the term in the denominator
is necessarily inferior to the term in the denominator of the deterministic
equivalent of the SINR in (12.16), while the numerator is unchanged. As a
consequence, at least asymptotically, the performance of the orthogonal CDMA
transmission is better than that of the random CDMA scheme. Notice also, from
the boundedness assumption on the |dn,k|, that
K −1
N 3
 N
X
n=1
|dk,n|2
!2
≤K −1
N
max
n
|dk,n|4
the right-hand side of which is of order O(K/N). The diﬀerence between the
i.i.d. and orthogonal CDMA performance is then marginal for small K.

288
12. System performance of CDMA technologies
This expression has an explicit limit if |dk,n| and Pk have limiting densities
|h(κ, f)| and P(κ), respectively, and K/N →c. Precisely, we have:
¯γ(MF)
k
a.s.
−→
P(κ)
R 1
0 |h(κ, f)|2df
2
P(κ)c
R 1
0 |h(κ, f)|4df −P(κ)c
R 1
0

i
R 1
0
PN
n=1 |h(κ, f)|2df
2
+ σ2 R 1
0 |h(κ, f)|2df
.
The resulting limit of the deterministic equivalent of the capacity ¯C(MF)
unfolds directly.
12.3.3.2 MMSE decoder
The MMSE decoder leads to the SINR ¯γ(MMSE)
k
of the form
¯γ(MMSE)
k
= PkwH
k HH
k
 PkHkWWHHH
k −PkHkwkwH
k HH
k + σ2IN
−1 Hkwk.
A direct application of Theorem 6.19 when the sum of Gram matrices is taken
over a single term leads to
¯γ(MMSE)
k
−
Pkek
1 −ek¯ek
where ek and ¯ek satisfy the implicit equation
¯ek = K
N
Pk
1 + Pkek −¯ekek
ek = 1
N
N
X
n=1
|dk,n|2
¯ek|dk,n|2 + σ2 .
In [Debbah et al., 2003a,b], a free probability approach is used to derive the
above deterministic equivalent. The precise result of [Debbah et al., 2003a] is
that
¯γ(MMSE)
k
−ηk
a.s.
−→0
where ηk is the unique positive solution to
ηk
ηk + 1 = 1
N
N
X
n=1
Pk|dn,k|2
cPk|dn,k|2 + σ2(1 −c)ηk + σ2 .
It can be shown that both expressions are consistent, i.e. that ηk =
Pkek
1−ek¯ek , by
writing
Pkek
1 −ek¯ek

Pkek
1 −ek¯ek
+ 1
−1
= 1
N
N
X
n=1
Pk|dn,k|2
cPk|dn,k|2 + σ2(1 −ek¯ek + Pkek)
= 1
N
N
X
n=1
Pk|dn,k|2
cPk|dn,k|2 + σ2(1 + (1 −c) Pkek
1−ek¯ek )

12.3. Performance of orthogonal CDMA technologies
289
where the second equality comes from the observation that
1 −(1 −c)
Pkek
1 −ek¯ek
= (1 −ek¯ek + Pkek) −cPkek
1 −ek¯ek
= (1 −ek¯ek + Pkek) −ek¯ek(1 −ek¯ek + Pkek)
1 −ek¯ek
= 1 −ek¯ek + Pkek.
This notation in terms of ηk is rather convenient, as it can be directly compared
to the i.i.d. CDMA scenario of Equation (12.17). In both cases, denote ηk the
deterministic equivalent for the SINR of user k. In the i.i.d. case, we found out
that the solution for
ηk
ηk + 1 = 1
N
N
X
n=1
Pk|dk,n|2
cPk|dk,n|2 + σ2(1 + ηk)
(12.18)
is such a deterministic equivalent, while in the orthogonal case, we now have
ηk
ηk + 1 = 1
N
N
X
n=1
Pk|dn,k|2
cPk|dn,k|2 + σ2(1 + ηk) −cηkσ2 .
(12.19)
Since
ηk
ηk+1 = 1 −
1
ηk+1, clearly ηk is larger in the orthogonal CDMA setting
than in its i.i.d. counterpart. Notice that 0 ≤ηk ≤
1
σ2 in both cases, and ηk is
therefore uniformly bounded. As a consequence, as K/N →0,
ηk
ηk+1 in both the
orthogonal and the i.i.d. scenarios converge to the same value, which is similar
to the matched-ﬁlter case.
We then have the following expression of a deterministic equivalent for
¯C(MMSE)
¯C(MMSE) −1
N
K
X
k=1
log2 (1 + ηk)
a.s.
−→0.
If |dk,n| and Pk have limiting densities |h(κ, f)| and P(κ), respectively, and
K/N →c, this leads to
¯C(MMSE)
a.s.
−→c
Z 1
0
log2 (1 + P(κ)η(κ)) dκ
where the function η(κ) is solution to
η(κ)
η(κ) + 1 =
Z
P(κ)|h(κ, f)|2
cP(κ)|h(κ, f)|2 + σ2 (1 + (1 −c)η(κ))df.
In what follows, we compare the performance of the random CDMA and
orthogonal CDMA in the downlink. We will speciﬁcally evidence the gain brought
by orthogonal CDMA precoders when the channel frequency selectivity is not
too strong. In Figure 12.7, we consider the case when the multi-path channel is
composed of a single tap, and we depict the spectral eﬃciency for both orthogonal
and i.i.d. codes as the ratio K/N varies (note that K/N ≤1 necessarily in the

290
12. System performance of CDMA technologies
0
0.5
1
1.5
2
0
1
2
3
K/N
Spectral eﬃciency [bits/s/Hz]
MMSE, i.i.d.
MMSE, orth.
MF, i.i.d.
MF, orth.
Figure 12.7 Spectral eﬃciency of random and orthogonal CDMA decoders, for
diﬀerent ratios K/N, K = 512, SNR=10 dB, Rayleigh frequency selective fading
channels L = 1, in the downlink.
orthogonal case). That is, we consider that the orthogonal codes received by the
users are perfectly orthogonal. The SNR is set to 10 dB, and the number of
receivers taken for simulation is K = 512. In this case, we observe indeed that
both the matched-ﬁlter and the MMSE ﬁlter for the orthogonal codes perform the
same (as no inter-code interference is present), while the linear ﬁlters for the i.i.d.
codes are highly suboptimal, as predicted. Then, in Figure 12.8, we consider the
scenario of an L = 8-tap multi-path channel, which now shows a large advantage
of the MMSE ﬁlter compared to the matched-ﬁlter, both for i.i.d. codes and
for orthogonal codes. Nonetheless, in spite of the channel convolution eﬀect, the
spectral eﬃciency achieved by orthogonal codes is still largely superior to that
achieved by random codes. This is diﬀerent from the uplink case, where diﬀerent
channels aﬀect the diﬀerent codes. Here, from the point of view of the receiver,
the channel convolution eﬀect aﬀects identically all user codes, therefore limiting
the orthogonality reduction due to frequency selectivity and thus impacting the
spectral eﬃciency in a limited manner.
This completes the chapter on CDMA technologies. We now move to the study
of multiple antenna communications when the number of antennas on either
communication side is large.

12.3. Performance of orthogonal CDMA technologies
291
0
0.5
1
1.5
2
0
1
2
3
K/N
Spectral eﬃciency [bits/s/Hz]
MMSE, i.i.d.
MMSE, orth.
MF, i.i.d.
MF, orth.
Figure 12.8 Spectral eﬃciency of random and orthogonal CDMA decoders, for
diﬀerent ratios K/N, K = 512, SNR=10 dB, Rayleigh frequency selective fading
channels, L = 8, in the downlink.


13
Performance of multiple antenna
systems
In this section, we study the second most investigated application of random
matrix theory to wireless communications, namely multiple antenna systems,
ﬁrst introduced and motivated by the pioneering works of Telatar [Telatar,
1995] and Foschini [Foschini and Gans, 1998]. While large dimensional system
analysis is easily defensible in CDMA networks, which typically allow for a large
number of users with large orthogonal or random codes, it is not so for multiple
antenna communications. Indeed, when it comes to applying approximated
results provided by random matrix theory analysis, we expect that the typical
system dimensions are of order ten to a thousand. However, for multiple input
multiple output (MIMO) setups, the system dimensions can be of order 4, or
even 2. Asymptotic results for such systems are then of minor interest. However,
it will turn out in some speciﬁc scenarios that the diﬀerence between the
ergodic capacity for multiple antenna systems and their respective deterministic
equivalents is sometimes of order O(1/N), N being the typical system dimension.
The per-receive antenna rate, which is of interest for studying the cost and gain of
bringing additional antennas on ﬁnite size devices, can therefore be approximated
within O(1/N 2). This is a rather convenient rate, even for small N. In fact, as will
be observed through simulations, the accuracy of the deterministic equivalents
is often even better.
13.1
Quasi-static MIMO fading channels
We hereafter recall the foundations of multiple antenna communications. We ﬁrst
assume a simple point-to-point communication between a transmitter equipped
with nt antennas and a receiver equipped with nr antennas. The communication
channel is assumed linear, frequency ﬂat, and is modeled at any instant by the
matrix H ∈Cnr×nt, with (i, j) entry hij. At time t, the transmitter emits the
data vector x(t) ∈Cnt through H, which is corrupted by additive white Gaussian
noise σn(t) ∈Cnr with entries of variance σ2 and received as y(t) ∈Cnr. We
therefore have the classical linear transmission model
y(t) = Hx(t) + σn(t).

294
13. Performance of multiple antenna systems
Assuming H constant over a long time period, compared to the transmission
period of a given data block, and that H is perfectly known to the transmitter,
the capacity C(nr,nt) of the ﬂat fading MIMO point-to-point Gaussian channel
is given by:
C(nr,nt)(σ2) = max
P
tr P≤P
I(nr,nt)(σ2; P)
with I(nr,nt)(σ2; P) the mutual information
I(nr,nt)(σ2; P) ≜log2 det

Inr + 1
σ2 HPHH

(13.1)
where P ∈Cnt×nt is the covariance matrix P ≜E[x(t)x(t)H] of the transmitted
data and P is the maximal power allowed for transmission. If the channel is
indeed constant, C(nr,nt) is determined by ﬁnding the matrix P under trace
constraint tr P ≤P such that the log determinant is maximized. That is, we
need to determine P such that
det

Inr + 1
σ2 HPHH

= det

Int + 1
σ2 HHHP

= det

Int + 1
σ2 PHHH

is maximal. Since HHH is Hermitian and non-negative deﬁnite, we can
write HHH = UΛUH with Λ = diag(l1, . . . , lnt) diagonal non-negative and U ∈
Cnt×nt unitary. The precoding matrix P can then be rewritten P = UHQU, with
tr Q = P, and Q is still Hermitian non-negative deﬁnite. Denote qij the entry
(i, j) of Q. The maximization problem under constraint is therefore equivalent
to maximizing
det

Int + 1
σ2 Λ
1
2 QΛ
1
2

under constraint tr Q = P. From Hadamard inequality [Telatar, 1999], we have
that
det

Int + 1
σ2 Λ
1
2 QΛ
1
2

≤
min(nt,nr)
Y
i=1

1 + 1
σ2 liqii

(13.2)
with equality if and only if Q is diagonal. For given qii values for all i, the
Hadamard inequality implies that the maximizing Q must be diagonal, and
therefore the eigenvectors of the capacity maximizing P must be aligned to
those of H. Maximizing the right-hand side of (13.2) under trace constraint is
then easily shown through Lagrange multipliers to be found by the water-ﬁlling
algorithm as
qii =

µ −σ2
li
+
where µ is such that Pmin(nt,nr)
i=1
qii = P, and qii = 0 for i > min(nt, nr).

13.2. Time-varying Rayleigh channels
295
The capacity C(nr,nt) found above, when H is constant over a long time
(suﬃciently long to be considered inﬁnite) and H is known at the transmitter
side, will be further referred to as the quasi-static channel capacity. That is,
it concerns the scenario when H is a fading channel static over a long time,
compared to the data transmission duration.
13.2
Time-varying Rayleigh channels
However, in mobile communications, it is often the case that H is varying
fast, and often too fast for the transmitter to get to know the transmission
environment perfectly prior to transmission. For simplicity here, we assume
that some channel information is nonetheless emitted by the transmitter in the
direction of the receiver prior to proper communication so that the receiver is at
all times fully aware of H. We also assume that the feedback eﬀort is negligible in
terms of consumed bit rate (think of it as being performed on an adjacent control
channel). In this case, the computation of the mutual information between the
transmitter and the receiver therefore assumes that the exact value of H is
unknown to the transmitter, although the joint probability distribution PH(H)
of H is known (or at least that some statistical information about H has been
gathered). The computation of Shannon’s capacity C(nr,nt)
ergodic in this case reads:
C(nr,nt)
ergodic (σ2) ≜max
P
tr P≤P
Z
log2 det

Inr + 1
σ2 HPHH

dPH(H)
which is the maximization over P of the expectation over H of the mutual
information I(nr,nt)(σ2; P) given in (13.1).
This capacity is usually referred to as the ergodic capacity. Indeed, we assume
that H is drawn from an ergodic process, that is a process whose probability
distribution can be deduced from successive observations. Determining the exact
value of C(nr,nt)
ergodic in this case is more involved as an integral has to be solved and
maximized over P.
We now recall the early result from Telatar [Telatar, 1995, 1999] on the ergodic
capacity of multiple antenna ﬂat Rayleigh fading channels. Telatar assumes the
now well spread i.i.d. Gaussian model, i.e. H has Gaussian i.i.d. entries of zero
mean and variance 1/nt. This assumption amounts to assuming that the physical
channel between the transmitter side and the receiver side is ﬁlled with numerous
scatterers and that there exists no line-of-sight component. The choice of letting
the entries of H have variances proportional to 1/nt changes the power constraint
into
1
nt tr P ≤P, which will turn out to be often more convenient for practical
calculus.

296
13. Performance of multiple antenna systems
13.2.1
Small dimensional analysis
When H is i.i.d. Gaussian, it is unitarily invariant so that the ergodic capacity for
the channel HU, for U ∈Cnt×nt unitary, is identical to the ergodic capacity for
H itself. The optimal precoding matrix P can therefore be considered diagonal
(non-negative deﬁnite) with no generality restriction. Denote Π(nt) the set of
permutation matrices of size nt × nt, whose cardinality is (nt!). Note [Telatar,
1999], by the concavity of log2 det(Int +
1
σ2 HPHH) seen as a function of P, that
the matrix Q ≜
1
nt!
P
Π∈Π(nt) ΠPΠH is such that
log2 det

Int + 1
σ2 HQHH

≥1
nt!
X
Π∈Π(nt)
log2 det

Int + 1
σ2 HΠPΠHHH

= log2 det

Int + 1
σ2 HPHH

.
This follows from Jensen’s inequality. Since P was arbitrary, Q maximizes the
capacity. But now notice that Q is, by construction, necessarily a multiple of the
identity matrix. With the power constraint, we therefore have Q = Int.
It therefore remains to evaluate the ergodic capacity as
C(nr,nt)
ergodic (σ2) =
Z
log2 det

Inr + 1
σ2 HHH

dPH(H)
where PH is the density of an (nr × nt)-variate Gaussian variable with entries
of zero mean and variance 1/nt. To this purpose, we ﬁrst diagonalize HHH and
write
C(nr,nt)
ergodic (σ2) = nr
Z ∞
0
log2

1 +
λ
ntσ2

pλ(λ)dλ
where pλ is the marginal eigenvalue distribution of the null Wishart matrix
ntHHH. Remember now that this is exactly stated in Theorem 2.3. Hence, we
have that
C(nr,nt)
ergodic (σ2) =
Z ∞
0
log2

1 +
λ
ntσ2
 m−1
X
k=0
nrk!
(k + n −m)![Ln−m
k
(λ)]2λn−me−λdλ
(13.3)
with m = min(nr, nt), n = max(nr, nt), and Lj
i are the Laguerre polynomials.
This important result is however diﬃcult to generalize to more involved
channel conditions, e.g. by introducing side correlations or a variance proﬁle to
the channel matrix. We will therefore quickly move to large dimensional analysis,
where many results can be found to approximate the capacity of point-to-point
MIMO communications in various channel conditions, through deterministic
equivalents.

13.2. Time-varying Rayleigh channels
297
13.2.2
Large dimensional analysis
From a large dimensional point of view, the ergodic capacity evaluation for the
Rayleigh i.i.d. channel is a simple application of the Mar˘cenko–Pastur law. We
have that the per-receive antenna capacity
1
nr C(nr,nt)
ergodic satisﬁes
1
nr
C(nr,nt)
ergodic (σ2) →
Z ∞
0
log2

1 + x
σ2
 p
(x −(1 −√c)2)((1 + √c −x)2 −x)
2πcx
dx
as (nt, nr) grow large with asymptotic ratio nr/nt →c, 0 < c < ∞. This result
was already available in Telatar’s pioneering article [Telatar, 1999]. In fact, we
even have that the quasi-static mutual information for P = Int converges almost
surely to the right-hand side value. Since the channels H for which this is not
the case lie in a space of zero measure, this implies that the convergence holds
surely in expectation. Now, an explicit expression for the above integral can
be derived. It suﬃces here to apply, e.g. Theorem 6.1 or Theorem 6.8 to the
extremely simple case where the channel matrix has no correlation. In that case,
the equations leading to the deterministic equivalent for the Stieltjes transform
are explicit and we ﬁnally have the more interesting result
1
nr
C(nr,nt)
ergodic (σ2)
−
"
log2
 
1 +
1
σ2(1 + nr
nt δ)
!
+ nt
nr
log2

1 + nr
nt
δ

+ log2(e)

σ2δ −1

#
→0
where δ is the positive solution to
δ =
 
1
1 + nr
nt δ + σ2
!−1
which is explicitly given by:
δ = 1
2

1
σ2

1 −nt
nr

−nt
nr
+
s 1
σ2

1 −nt
nr

−nt
nr
2
+ 4 nt
nrσ2

.
Also, since H has Gaussian entries, by invoking, e.g. Theorem 6.8, it is known
that the convergence is as fast as O(1/n2
t). Therefore, we also have that
C(nr,nt)
ergodic (σ2)
−
"
nr log2
 
1 +
1
σ2(1 + nr
nt δ)
!
+ nt log2

1 + nr
nt
δ

+ nr log2(e)

σ2δ −1

#
= O(1/nt).
(13.4)
In Table 13.1, we evaluate the absolute diﬀerence between C(nr,nt)(σ2) (from
Equation (13.3)) and its deterministic equivalent (given in Equation (13.4)),
relative to C(nr,nt)(σ2), for (nt, nr) ∈{1, . . . , 8}2. The SNR is 10 dB. We observe
that, even for very small values of nt and nr, the relative diﬀerence does not

298
13. Performance of multiple antenna systems
nr, nt
1
2
3
4
5
6
7
8
1
0.0630
0.0129
0.0051
0.0027
0.0016
0.0011
0.0008
0.0006
2
0.0116
0.0185
0.0072
0.0035
0.0020
0.0013
0.0009
0.0007
3
0.0039
0.0072
0.0080
0.0044
0.0025
0.0016
0.0011
0.0008
4
0.0019
0.0032
0.0046
0.0044
0.0029
0.0019
0.0013
0.0009
5
0.0011
0.0017
0.0025
0.0030
0.0028
0.0020
0.0014
0.0010
6
0.0007
0.0010
0.0015
0.0019
0.0021
0.0019
0.0015
0.0011
7
0.0005
0.0007
0.0009
0.0012
0.0015
0.0015
0.0014
0.0011
8
0.0003
0.0005
0.0006
0.0008
0.0010
0.0012
0.0012
0.0011
Table 13.1. Relative diﬀerence between true ergodic nr × nt MIMO capacity and
associated deterministic equivalent.
exceed 6% and is of order 0.5% for nt and nr of order 4. This simple example
motivates the use of large dimensional analysis to approximate the real capacity
of even small dimensional systems. We will see in this chapter that this trend
can be extended to more general models, although particular care has to be
taken for some degenerated cases. It is especially of interest to determine the
transmit precoders that maximize the capacity of MIMO communications under
strong antenna correlations at both communication ends. It will be shown that
the capacity in this corner case can still be approximated using deterministic
equivalents, although fast convergence of the deterministic equivalents cannot
be ensured and the resulting estimators can therefore be very inaccurate. In
this case, theory requires that very large system dimensions be assumed to
obtain acceptable results. Nonetheless, simulations still suggest, apart from very
degenerated models, where, e.g. both transmit and receive sides have rank-1
correlation proﬁles, that the deterministic equivalents are still very accurate for
small system dimensions.
Note additionally that random matrix theory, in addition to providing
consistent estimates for the ergodic capacity, also ensures that, with probability
one, as the system dimension grows large, the instantaneous mutual information
of a given realization of a Rayleigh distributed channel is within o(1) of the
deterministic equivalent (this assumes however that only statistical channel
state information is available at the transmitter). This unveils some sort of
deterministic behavior for the achievable data rates as the number of antennas
grows, which can be thought of as a channel hardening eﬀect [Hochwald et al.,
2004], i.e. as the system dimensions grow large, the variance of the quasi-static
capacity is signiﬁcantly reduced.
13.2.3
Outage capacity
For practical ﬁnite dimensional quasi-static Rayleigh fading channel realizations,
whose realizations are unknown beforehand, the value of the ergodic capacity,

13.2. Time-varying Rayleigh channels
299
that can be only seen as an a priori “expected” capacity, is not a proper measure
of the truly achievable transmission data rate. In fact, if the Rayleigh fading
channel realization is unknown, the largest rate to which we can ensure data is
transmitted reliably is in fact null. Indeed, for every given positive transmission
rate, there exists a non-zero probability that the channel realization has a lesser
capacity. As this statement is obviously not convenient, it is often preferable to
consider the so-called q-outage capacity deﬁned as the largest transmission data
rate that is achievable at least a fraction q of the time. That is, for a random
channel H with realizations H(ω) and instantaneous capacity C(nt,nr)(σ2; ω),
ω ∈Ω, the q-outage capacity C(nt,nr)
outage (σ2; q) is deﬁned as
C(nt,nr)
outage (σ2; q) = sup
R≥0
n
P
n
ω, C(nt,nr)(σ2; ω) > R
o
≤q
o
= sup
R≥0
n
P

C(nt,nr)(σ2) > R
o
(13.5)
with C(nt,nr)(σ2) seen here as a random variable of H.
It is often diﬃcult to characterize fully the outage capacity under perfect
channel knowledge at the transmitter, since the transmit precoding policy is
diﬀerent for each channel realization. In the following, we will in general refer
to the outage capacity as the outage rate obtained under deterministic (often
uniform) power allocation at the transmitter. In this scenario, we have instead
the outage mutual information I(nt,nr)
outage (σ2; P; q), for a speciﬁc precoding matrix
P
I(nt,nr)
outage (σ2; P; q) = sup
R≥0
n
P

I(nt,nr)(σ2; P) > R

≤q
o
.
(13.6)
To determine the outage capacity of a given communication channel, it suﬃces
to be able to determine the complete probability distribution of C(nt,nr)(σ2) for
varying H, if we consider the outage capacity deﬁnition (13.5), or the probability
distribution of I(nt,nr)(σ2; P) for a given precoder P if we consider the deﬁnition
(13.6). For the latter, with P = Int, for the Rayleigh fading MIMO channel
H ∈Cnr×nt, it suﬃces to describe the distribution of
log2

IN + 1
σ2 HHH

=
nr
X
k=1
log2

1 + λk
σ2

for the random variables H, with λ1, . . . , λnr the eigenvalues of HHH.
Interestingly, as both nt
and nr
grow large, the distribution of the
zero mean random variable I(nt,nr)(σ2; P) −E[I(nt,nr)(σ2; P)] turns out to
be asymptotically Gaussian [Kamath et al., 2002]. This result is in fact a
straightforward consequence of the central limit Theorem 3.17. Indeed, consider
Theorem 3.17 in the case when the central TN matrix is identity. Taking the
uniform precoder P = Int and letting f1 be deﬁned as f1(x) = log2
 1 + x
σ2

,
clearly continuous on some closed set around the limiting support of the

300
13. Performance of multiple antenna systems
−4
−2
0
2
4
0
1 · 10−2
2 · 10−2
3 · 10−2
4 · 10−2
Centered capacity
Density
Simulation
Central limit
Figure 13.1 Simulated I(nt,nr)(σ2; Int) −E[I(nt,nr)(σ2; Int)] against central limit,
σ2 = −10 dB.
Mar˘cenko–Pastur law F, we obtain that
nr
Z
log2

1 + λ
σ2
 
dF HHH(λ) −dF(λ)

⇒X
with X a random real Gaussian variable with zero mean and variance computed
from (3.26) as being equal to
E

X2
= −log

1 −σ4
16c
 r
(1 + √c)2
σ2
+ 1 −
r
(1 −√c)2
σ2
+ 1
!4
.
This result is also a direct application of Theorem 3.18 for Gaussian distributed
random entries of XN.
13.3
Correlated frequency ﬂat fading channels
Although the above analysis has the advantage to predict the potential
capacity gains brought by multiple antenna communications, i.i.d. Rayleigh
fading channel links are usually too strong an assumption to model practical
communication channels. In particular, the multiplexing gain of order min(nr, nt)
announced by Telatar [Telatar, 1999] and Foschini [Foschini and Gans, 1998]
relies on the often unrealistic supposition that the channel links are frequency
ﬂat and have a multi-variate independent zero mean Gaussian distribution. In
practical communication channels, this model faces strong limitations. We look
at these limitations from a receiver point of view, although the same reasoning
can be performed on the transmitter side.

13.3. Correlated frequency ﬂat fading channels
301
0
2
4
6
8
10
12
14
0
0.2
0.4
0.6
0.8
1
Achievable rate
Distribution function
SISO, det. eq.
SISO, sim.
MIMO 2 × 2, det. eq.
MIMO 2 × 2, sim.
MIMO 4 × 4, det. eq.
MIMO 4 × 4, sim.
Figure 13.2 Distribution function of C(nt,nr)(σ2), σ2 = 0.1, for diﬀerent values of
nt, nr, and comparison against deterministic equivalents.
• To ensure conditional independence (with respect to the transmitted data)
of the waveforms received by two distinct antennas but emerging from a
single source, the propagation environments must be assumed decorrelated
in some sense. Roughly speaking, two incoming waveforms can be stated
independent if they propagate along diﬀerent paths in the communication
medium. This physically constrains the distance between receive antennas to
be of an order larger than the transmission wavelength. Introducing a speciﬁc
model of channel correlation, it can be in particular shown that increasing
the number of antennas to inﬁnity on ﬁnite size devices leads to a physically
fundamental rate saturation, see, e.g., [Couillet et al., 2008; Pollock et al.,
2003]. To model more realistic channel matrices, statistical correlation between
transmit antennas and receive antennas must therefore be taken into account.
The most largely spread channel model which accounts for both transmit and
receive signal correlations is the Kronecker channel model. This model assumes
that the transmitted signals are ﬁrst emitted from a correlated source (e.g.
close transmit antennas, privileged direction of wave transmission, etc.), then
propagate through a largely scattered environment, which acts as a random
i.i.d. linear ﬁlter decorrelated from transmit and receive parts, to be ﬁnally
received on a correlated antenna array. Again, the correlation at the receiver
is due either to the fact that receive antennas are so close, or that the solid
angle of direction of arrival is so thin, that all incoming signals are essentially
the same on all antennas. Note that this model, although largely spread, has
been criticized and claimed unrealistic to some extent by ﬁeld measurements
in, e.g., [Ozcelik et al., 2003; Weichselberger et al., 2006].

302
13. Performance of multiple antenna systems
• The i.i.d. Gaussian channel matrix model also assumes that the propagation
paths from a given transmit antenna to a given receive antenna have an
average fading gain, which is independent of the selected antenna pair. This
is a natural assumption for long-distance transmissions over a communication
medium with a large number of scatterers. For ﬁxed communication systems
with smaller distances, this does not take into account the speciﬁc impact
of the inter-antenna distance. A more general channel matrix model in this
sense is to let the channel matrix entries be independent Gaussian entries with
diﬀerent variances, though, i.e. with a variance proﬁle.
• Both previous generalization models still suﬀer from the lack of line-of-
sight components in the channel. Indeed, line-of-sight components cannot be
considered Gaussian, or, for that matter, random in the short-term, but are
rather modeled as deterministic components with possibly a varying phase
rotation angle. A more adequate model, that however assumes decorrelated
transmissions over the random channel components, consists in summing a
deterministic matrix, standing for the line-of-sight component, and a random
matrix with Gaussian entries and a variance proﬁle. To account for the
relative importance of the line-of-sight component and the random part,
both deterministic and random matrices are scaled accordingly. This model is
referred to as the Rician model.
• Although this section only deals with frequency ﬂat fading channels, we
recall that wideband communication channels, i.e. communication channels
that span over a large range of frequencies (the adjective “large” qualifying
the fact that the transmission bandwidth is several times larger than the
channel coherence bandwidth) induce frequency dependent channel matrices.
Therefore, in wideband transmissions, channel models cannot be simply
represented as a single matrix H ∈Cnr×nt at any given instant, but rather as
a matrix-valued continuous function H(f) ∈Cnr×nt, with f ∈[−W/2, W/2]
the communication bandwidth. This motivates in particular communication
schemes such as OFDM, which practically exploit these frequency properties.
This is however not the subject of the current section, which will be given
deeper considerations in Section 13.5.
In this section, we discuss the case of communication channels with correlation
patterns both at the transmitter and the receiver and a very scattered medium
in between; i.e. we consider here the Kronecker channel model. The results to
be introduced were initially derived in, e.g., [Debbah and M¨uller, 2003] using
tools from free probability theory. We will see in Chapter 18 that the Kronecker
model has deep information-theoretic grounds, in the sense that it constitutes
the least informative channel model when statistical correlation matrices at both
communication sides are a priori known by the system modeler.
Letting H ∈Cnr×nt be a narrowband Kronecker communication channel
matrix, we will write
H = R
1
2 XT
1
2

13.3. Correlated frequency ﬂat fading channels
303
where X ∈Cnr×nt is a random matrix with independent Gaussian entries of
zero mean and variance 1/nt, which models the rich scattering environment,
R
1
2 ∈Cnr×nr is a non-negative deﬁnite Hermitian square root of the non-negative
deﬁnite receive correlation matrix R, and T
1
2 ∈Cnt×nt is a non-negative deﬁnite
Hermitian square root of the non-negative deﬁnite receive correlation matrix T.
Note that the fact that X has Gaussian i.i.d. entries allows us to assume without
loss of generality that both R and T are diagonal matrices (a remark which no
longer holds in multi-user scenarios). The achievable quasi-static bit rate C(nt,nr)
for the additive white Gaussian noise channel under this medium ﬁltering matrix
model reads:
C(nt,nr)(σ2) =
sup
P
1
nt tr P≤P
I(nr,nt)(σ2; P)
where we deﬁne as before the mutual information I(nr,nt)(σ2; P) as
I(nt,nr)(σ2; P) ≜log2 det

Inr + 1
σ2 HPHH

= log2 det

Inr + 1
σ2 R
1
2 XT
1
2 PT
1
2 XHR
1
2

where σ2 is the variance of the individual i.i.d. receive noise vector entries.
The corresponding ergodic capacity takes the same form but with an additional
expectation in front of the log determinant, which is taken over the random X
matrices.
Evaluating the ergodic capacity and the corresponding optimal covariance
matrix in closed-form is however rather involved. This has been partially solved
in [Hanlen and Grant, 2003] where an exact expression of the ergodic mutual
information is given as follows.
Theorem 13.1 (Theorem 2 in [Hanlen and Grant, 2003]). Let H = R
1
2 XT
1
2 ,
with R ∈Cnr×nr and T ∈Cnt×nt deterministic and X ∈Cnr×nt random with
i.i.d. Gaussian entries of zero mean and variance 1/nt. Then, denoting
I(nt,nr)(σ2; P) = log2 det

Inr + 1
σ2 HPHH

we have:
E
h
I(nt,nr)(σ2; P)
i
=
det(R)−nr det(TP)−nt
Qm
i=1(M −i)! Qm
i=1(m −i)!
Z
{Λ>0}
0F0
 −R−1, Λ, T−1
×
m
Y
i=1
λM−m
i
m
Y
i<j
(λi −λj)2
m
X
i=1
log2

1 +
λi
σ2nt

dΛ
with m = min(nr, nt), M = max(nr, nt), Λ = diag(λ1, . . . , λm), 0F0 deﬁned in
Equation (2.2) and the integral is taken over the set of m-dimensional vectors
with positive entries.

304
13. Performance of multiple antenna systems
Moreover, assuming R and T diagonal without loss of generality, the matrix
P that maximizes E[I(nt,nr)(σ2; P)] under constraint
1
nt tr P ≤P is the diagonal
matrix P⋆= diag(p⋆
1, . . . , p⋆
nt) such that, for all k ∈{1, . . . , nt}
 E

(Int + HHHP⋆)−1HHH

kk = µ,
if p⋆
k > 0
 E

(Int + HHHP⋆)−1HHH

kk < µ,
if p⋆
k = 0
for some µ set to satisfy the power constraint
1
nt tr P⋆≤P.
These results, although exact, are diﬃcult to use in practice. We will see in
the next section that large dimensional random matrix analysis brings several
interesting features to the study of MIMO Kronecker channels.
• It ﬁrst allows us to provide a deterministic equivalent for the quasi-
static mutual information of typical channels, assuming channel independent
transmit data precoding. The informal “typical” adjective here suggests that
such channels belong to a high probability subset of all possible channel
realizations. Indeed, we have already discussed the fact that, for non-
deterministic channel models, the capacity is ill-deﬁned and is actually zero
in the current Kronecker scenario. Instead, we will say that we provide
a deterministic equivalent for the achievable rate of all highly probable
deterministic channels, with deterministic transmit precoding. The reason why
transmit precoding cannot be optimized will be made clear.
• It then allows us to obtain a deterministic equivalent of the ergodic capacity
of correlated MIMO communications. As we will see, the transmit covariance
matrix which maximizes the deterministic equivalent can be derived. The
mutual information evaluated at this precoder can further be proved to be
asymptotically close to the exact ergodic capacity. The main advantage of
this approach is that, compared to the results of Theorem 13.1, deterministic
equivalents provide much simpler and more elegant solutions to the ergodic
capacity characterization.
Nonetheless, before going further, we address some key limitations of the
Kronecker channel model. The major shortcoming of the Kronecker model lies in
the assumption of a rich scattered environment. To ensure that a large number
of antennas can be used on either communication side, the number of such
scatterers must be of an order larger than the product between the number
of transmit and received antennas, so as to generate diverse propagation paths
for all transmit–receive antenna pairs. This assumption is typically not met
in an outdoor environment. Also, no line-of-sight component is allowed for in
the Kronecker model, nor does there exist a correlation between transmit and
receive antennas. This is also restrictive in short range communications with few
propagation paths.
To provide a deterministic equivalent for the capacity of the Kronecker channel
model, we will use a simpliﬁed version of Corollary 6.1 of Theorem 6.4 and

13.3. Correlated frequency ﬂat fading channels
305
Theorem 6.8. Both deterministic equivalents will obviously be consistent and
in fact equal, although the theorems rely on diﬀerent underlying assumptions.
Thanks to these assumptions, diﬀerent conclusions will be drawn in terms of the
applicability to speciﬁc channel conditions. Note that these results were initially
derived in [Tulino et al., 2003] using Girko’s result, Theorem 3.14, and were also
derived later using tools borrowed from physics in [Sengupta and Mitra, 2006].
13.3.1
Communication in strongly correlated channels
We ﬁrst recall Corollary 6.1 in the case of a single transmitter, i.e. K = 1.
Assume, as above, a Gaussian channel with channel matrix H and additive
noise variance σ2. Let H = R
1
2 XT
1
2 ∈Cnr×nt, with X ∈Cnr×nt composed of
Gaussian i.i.d. entries of zero mean and variance 1/nt, R
1
2 ∈Cnr×nr and
T
1
2 ∈Cnt×nt be Hermitian non-negative deﬁnite, such that F R and F T form
a tight sequence, as the dimensions nr, nr grow large, and the sequence nt/nr
is uniformly bounded from below, away from a > 0, and from above, away from
b < ∞. Also assume that there exists α > 0 and a sequence snr, such that, for
all nr
max(tsnr +1, rsnr +1) ≤α
where ri and ti denote the ith ordered eigenvalue of R and T, respectively, and,
denoting bnr an upper-bound on the spectral norm of T and R and β some real,
such that β > (b/a)(1 + √a)2, assume that anr = b2
nrβ satisﬁes
snr log2(1 + anrσ−2) = o(nr).
(13.7)
Then, for large nr, nt, the Shannon transform of HHH, given by:
VHHH(σ−2) = 1
nr
log2 det(Inr + 1
σ2 HHH)
satisﬁes
VHHH(σ−2) −Vnr(σ−2)
a.s.
−→0
where Vnr(σ−2) satisﬁes
Vnr(σ−2) = 1
nr
nr
X
k=1
log2
 1 + ¯δrk

+ 1
nr
nt
X
k=1
log2

1 + nr
nt
δtk

−σ2 log2(e)δ¯δ
with δ and ¯δ the unique positive solutions to the ﬁxed-point equations
δ =
1
σ2nr
nr
X
k=1
rk
1 + ¯δrk
¯δ =
1
σ2nt
nt
X
k=1
tk
1 + nr
nt δtk
.

306
13. Performance of multiple antenna systems
This result provides a deterministic equivalent for the Shannon transform of
HHH, which is, in our information-theoretic context, a deterministic equivalent
for the per-receive antenna mutual information between the multiple antenna
transmitter and the multiple antenna receiver when uniform power allocation is
used across transmit antennas. We need now to understand the extent of the
applicability of the previous result, which consists in understanding exactly the
underlying assumptions made on T and R. Indeed, it is of particular importance
to be able to study the capacity of MIMO systems when the correlation matrices
T and R have very ill-conditioned proﬁles. It may seem in particular that the
deterministic equivalents may not be valid if T and R are composed of a few very
large eigenvalues and all remaining eigenvalues are close to zero. This intuition
turns out not to be correct.
Remember that tightness, which is commonly deﬁned as a probability theory
notion, qualiﬁes a sequence of distribution functions F1, F2, . . . (let us assume
F(x) = 0 for x < 0), such that, for all ε > 0, there exists M > 0, such that
Fk(M) > 1 −ε
for all k. This is often thought of as the probability theory equivalent to
boundedness. Indeed, a sequence x1, x2, . . . of real positive scalars is bounded
if there exists M such that xk < M for all k. Here, the parameter ε allows for
some event leakage towards inﬁnity, although the probability of such events is
increasingly small. Therefore, no positive mass can leak to inﬁnity. In our setting,
F T and F R do not really form tight probability distributions in the classical
probabilistic meaning of boundedness as they are deterministic distribution
functions rather than random distribution functions. This does not however aﬀect
the mathematical derivations to come.
Since T and R are correlation matrices, for a proper deﬁnition of the signal-
to-noise ratio 1/σ2, we assume that they are constrained by
1
nt tr T = 1 and
1
nr tr R = 1. That is, we do not allow the power transmitted or received to grow
as nt, nr grow. The trace constraint is set to one for obvious convenience. Note
that it is classical to let all diagonal entries of T and R equal one, as we generally
assume that every individual antenna on either communication side has the same
physical properties. This assumption would no longer be valid under a channel
with variance proﬁle, for which the fading link hij between transmit antenna i
and receive antenna j has diﬀerent variances for diﬀerent (i, j) pairs.
Observe that, because of the constraint
1
nr tr R = 1, the sequence {F R}
(for growing nr) is necessarily tight. Indeed, given ε > 0, take M = 2/ε;
nr[1 −F R(M)] is the number of eigenvalues in R larger than 2/ε, which is
necessarily smaller than or equal to nrε/2 from the trace constraint, leading
to 1 −F R(M) ≤ε/2 and then F R(M) ≥1 −ε/2 > 1 −ε. The same naturally
holds for matrix T. Now the condition regarding the smallest eigenvalues of R
and T (those less than α) requires a stronger assumption on the correlation
matrices. Under the trace constraint, this requires that there exists α > 0, such
that the number of eigenvalues in R greater than α is of order o(nr/ log nr). This

13.3. Correlated frequency ﬂat fading channels
307
may not always be the case, as we presently show with a counter-example. Take
nr = 2p + 1 and the eigenvalues of R to be
2p−1, p, . . . , p
| {z }
2p−1
p
, 0, . . . , 0
| {z }
2p−2p−1
p
.
The largest eigenvalue is of order nr so that anr is of order n2
r, and the
number snr of eigenvalues larger than any α > 0 for nr large is of order
2p−1
p
∼
nr
log(nr). Therefore snr log(1 + anr/x) = O(nr) here. Nonetheless, most
conventional models for R and T, even when showing strong correlation
properties, satisfy the assumptions of Equation (13.7). We mention in particular
the following examples:
• if all R and T have uniformly bounded spectral norm, then there exists α > 0
such that all eigenvalues of R and T are less then α for all nr. This implies
snr = 0 for all nr and therefore the condition is trivially satisﬁed. Our model
is therefore compatible with loosely correlated antenna structures;
• when antennas are on the opposite densely packed on a volume limited device,
the correlation matrices R and T tend to be asymptotically of ﬁnite rank, see,
e.g., [Pollock et al., 2003] in the case of a dense circular array. That is, for any
given α > 0, the number snr of eigenvalues greater than α is ﬁnite for all large
nr, while anr is of order nr2. This implies snr log(1 + anr/x) = O(log nr) =
o(nr).
• for one-, two- or three-dimensional antenna arrays with neighbors separated
by half the wavelength as discussed by Moustakas et al. in [Moustakas et al.,
2000], the correlation ﬁgure corresponds to O(nr) eigenvalues of order of
magnitude O(1) for one-dimensional arrays, O(√nr) large eigenvalues of order
O(√nr) for two-dimensional arrays or O(nr
2
3 ) large eigenvalues of order
O(nr
1
3 ) for three-dimensional arrays, the remaining eigenvalues being close
to zero. In the p-dimensional scenario, we can approximate snr by nr
p−1
p
and
anr by nr
2
p , and we have:
snr log(1 + anr/x) ∼nr
p−1
p log nr = o(nr).
As a consequence, a wide scope of antenna correlation models enter our
deterministic equivalent framework, which comes again at the price of a slower
theoretical convergence of the diﬀerence VHHH(σ−2) −Vnr(σ−2).
When T is changed into T
1
2 PT
1
2 , P ∈Cnt×nt standing for the transmit power
policy with constraint 1
nt tr P ≤1 to accept power allocation in the system model,
it is still valid that {F T
1
2 PT
1
2 } forms a tight sequence and that the condition
on the smallest eigenvalues of T
1
2 PT
1
2 is fulﬁlled for all matrices satisfying the
mild assumption (13.7). Indeed, let T satisfy the trace constraint, then for ε > 0
such that nrε ∈N, we can choose M such that we have 1 −F T(
√
M) < ε/2
and 1 −F P(
√
M) < ε/2 for all nt; since the smallest ntε/2 + 1 eigenvalues of
both T and P are less than
√
M, at least the smallest ntε + 1 eigenvalues of

308
13. Performance of multiple antenna systems
TP are less than M, hence 1 −F TP(M) < ε and {F TP} is tight. Once again,
the condition on the smallest eigenvalues can be satisﬁed for a vast majority of
T
1
2 PT
1
2 matrices from the same argument.
The analysis above makes it possible to provide deterministic equivalents for
the per-antenna mutual information of even strongly correlated antenna patterns.
Assuming a quasi-static channel with imposed deterministic power allocation
policy P at the transmitter (assume, e.g. short time data transmission over a
typical quasi-static channel such that the transmitter does not have the luxury to
estimate the propagation environment and chooses P in a deterministic manner),
the mutual information I(nt,nr)(σ2; P) for this precoder satisﬁes
1
nr
I(nt,nr)(σ2; P) −
 1
nr
log2 det

Int + nr
nt
δT
1
2 PT
1
2

+ 1
nr
nr
X
k=1
log2
 1 + ¯δrk

−σ2 log2(e)δ¯δ
#
a.s.
−→0
with δ and ¯δ the unique positive solutions to
δ =
1
σ2nr
nr
X
k=1
rk
1 + ¯δrk
,
¯δ =
1
σ2nt
tr T
1
2 PT
1
2

Int + nr
nt
δT
1
2 PT
1
2

and this is valid for all (but a restricted set of) choices of T, P, and R matrices.
This property of letting T and R have eigenvalues of order O(nr) is crucial to
model the communication properties of very correlated antenna arrays, although
extreme care must be taken in the degenerated case when both T and R have
very few large eigenvalues and the remainder of their eigenvalues are close to
zero. In such a scenario, extensive simulations suggest that the convergence of the
deterministic equivalent is extremely slow, to the point that even the per-antenna
capacity of a 1000 × 1000 MIMO channel is not well approximated by the
deterministic equivalent. Also, due to strong correlation, it may turn out that the
true per-antenna capacity decreases rather fast to zero, with growing nr, nt, while
the diﬀerence between true per-antenna capacity and deterministic equivalent is
only slowly decreasing. This may lead to the very unpleasant consequence that
the relative diﬀerence grows to inﬁnity while the eﬀective diﬀerence goes to zero,
but at a slow rate. On the other hand, extensive simulations also suggest that
when only one of T and R is very ill-conditioned, the other having not too
few large eigenvalues, deterministic equivalents are very accurate even for small
dimensions.
When it comes to determining the optimal transmit data precoding matrix,
we seek for the optimal matrix P, H being fully known at the transmitter, such
that the quasi-static mutual information is maximized. We might think that
determining the precoding matrix which maximizes the deterministic equivalent
of the quasi-static mutual information can provide at least an insight into the

13.3. Correlated frequency ﬂat fading channels
309
quasi-static capacity. This is however an incorrect reasoning, as the optimal
precoding matrix, and therefore the system capacity, depend explicitly on the
entries of X. But then, the assumptions of Theorem 6.4 clearly state that the
correlation matrices are deterministic or, as could be shown, are random but
at least independent of the entries of X. The deterministic equivalent of the
mutual information can therefore not be extended to a deterministic equivalent
of the quasi-static capacity. As an intuitive example, consider an i.i.d. Gaussian
channel H. The eigenvalue distribution of HHH is, with high probability, close to
the Mar˘cenko–Pastur for suﬃciently large dimensions. From our ﬁnite dimension
analysis of the multiple antenna quasi-static capacity, water-ﬁlling over the
eigenvalues of the Mar˘cenko–Pastur law must be applied to maximize the
capacity, so that strong communication modes receive much more power than
strongly faded modes. However, it is clear, by symmetry, that the deterministic
equivalent of the quasi-static mutual information is maximized under equal power
allocation, which leads to a smaller rate.
13.3.2
Ergodic capacity in strongly correlated channels
Assuming T and R satisfy the conditions of Theorem 6.4, the space of matrices
X over which the deterministic equivalent of the per-antenna quasi-static mutual
information is an asymptotically accurate estimator has probability one. As a
consequence, by integrating the per-antenna capacity over the space of Rayleigh
matrices X and applying straightforwardly the dominated convergence theorem,
Theorem 6.3, for all deterministic precoders P, we have that the per-antenna
ergodic mutual information is well approximated by the deterministic equivalent
of the per-antenna quasi-static mutual information for this precoder.
It is now of particular interest to provide a deterministic equivalent for the
per-antenna ergodic capacity, i.e. for unconstrained choice of a deterministic
transmit data precoding matrix. Contrary to the quasi-static scenario, as the
matrix X is unknown to the transmitter, the optimal precoding matrix is now
chosen independently of X. As a consequence, it seems possible to provide a
deterministic equivalent for
1
nr
C(nt,nr)
ergodic (σ2) ≜
sup
P
1
nt tr P=1
1
nr
E[I(nt,nr)(σ2; P)]
with
I(nt,nr)(σ2; P) ≜log2 det

Inr + 1
σ2 HPHH

.
This is however not so obvious as P is allowed here to span over all matrices
with constrained trace, which, as we saw, is a larger set than the set of matrices
that satisfy the constraint (13.7) on their smallest eigenvalues. If we consider in
the argument of the supremum only precoding matrices satisfying the assumption

310
13. Performance of multiple antenna systems
(13.7), the per-antenna unconstrained ergodic capacity reads:
1
nr
C(nt,nr)
ergodic (σ2) =
sup
P
1
nt tr P=1
qsnr +1≤α
1
nr
E[I(nt,nr)(σ2; P)]
for some constant α > 0, with q1, . . . , qnt the eigenvalues of T
1
2 PT
1
2 and
s1, s2, . . . any sequence which satisﬁes (13.7) (with T replaced by T
1
2 PT
1
2 ).
We will assume in what follows that (13.7) holds for all precoding matrices
considered. It is then possible to provide a deterministic equivalent for
1
nr C(nt,nr)
ergodic (σ2) as it is possible to provide a deterministic equivalent for the right-
hand side term for every deterministic P. In particular, assume P⋆is a precoder
that achieves
1
nr C(nt,nr)
ergodic and P◦is a precoder that maximizes its deterministic
equivalent. We will denote
1
nr I(nt,nr)◦(σ2; P) the value of the deterministic
equivalent of the mutual information for precoder P. In particular
1
nr
I(nt,nr)◦(σ2; P) = 1
nr
nr
X
k=1
log2
 1 + ¯δ(P)rk

+ 1
nr
log2 det
h
Int + δ(P)T
1
2 PT
1
2
i
−σ2 log2(e)δ(P)¯δ(P)
with δ(P) and ¯δ(P) the unique positive solutions to the equations in (δ, ¯δ)
δ =
1
σ2nr
nr
X
k=1
rk
1 + ¯δrk
,
¯δ =
1
σ2nt
tr T
1
2 PT
1
2

Int + nr
nt
δT
1
2 PT
1
2

.
(13.8)
We then have
1
nr
C(nt,nr)
ergodic (σ2) −1
nr
I(nt,nr)◦(σ2; P◦)
= 1
nr
E[I(nt,nr)(σ2; P⋆)] −1
nr
I(nt,nr)◦(σ2; P◦)
= 1
nr

E[I(nt,nr)(σ2; P⋆)] −I(nt,nr)◦(σ2, P⋆)

+ 1
nr

I(nt,nr)◦(σ2; P⋆) −I(nt,nr)◦(σ2; P◦)

= 1
nr

E[I(nt,nr)(σ2; P⋆)] −E[I(nt,nr)(σ2, P◦)]

+ 1
nr

E[I(nt,nr)(σ2, P◦)] −I(nt,nr)◦(σ2; P◦)

.
In the second equality, as nt, nr grow large, the ﬁrst term goes to zero,
while the second is clearly negative by deﬁnition of P◦, so that asymptotically
1
nr (C(nt,nr)
ergodic (σ2) −I(nt,nr)◦(σ2; P◦)) is negative. In the third equality, the ﬁrst
term is clearly positive by deﬁnition of P⋆while the second term goes to
zero, so that asymptotically
1
nr (C(nt,nr)
ergodic (σ2) −I(nt,nr)◦(σ2; P◦)) is positive.

13.3. Correlated frequency ﬂat fading channels
311
Therefore
1
nr (C(nt,nr)
ergodic (σ2) −I(nt,nr)◦(σ2; P◦))
a.s.
−→0, and the maximum value of
the deterministic equivalent provides a deterministic equivalent for the ergodic
capacity. This however does not say yet whether P◦is close to the optimal
precoder P⋆itself. To verify this fact, we merely need to see that both the ergodic
capacity and its deterministic equivalent, seen as functions of the precoder P, are
strictly concave functions, so that they both have a unique maximum; this can
be proved without diﬃculty and therefore P⋆coincides with P◦asymptotically.
The analysis above has the strong advantage to be valid for all practical
channel conditions that follow the Kronecker model, even strongly correlated
channels. However, it also comes along with some limitations. The strongest
limitation is that the rate of convergence of the deterministic equivalent of the
per-antenna capacity is only ensured to be of order o(1). This indicates that an
approximation of the capacity falls within o(nt), which might be good enough
(although not very satisfying) if the capacity scales as O(nt). In the particular
case where an increase in the number of antennas on both communication ends
increases correlation in some sense, the capacity no longer scales linearly with
nt, and the deterministic equivalent is of no practical use. The main two reasons
why only o(1) convergence could be proved is that the proof of Theorem 6.1 is
(i) not restricted to Gaussian entries for X, and (ii) assumes tight {F T}, {F R}
sequences. These two assumptions are shown, through truncation steps in the
proof, to be equivalent to assuming that T, R, and X have entries bounded by
log(nt), therefore growing to inﬁnity, but not too fast.
In the following, the same result is discussed but under tighter assumptions on
the T, R, and X matrices. It will then be shown that a much faster convergence
rate of the deterministic equivalent for the per-antenna capacity can be derived.
13.3.3
Ergodic capacity in weakly correlated channels
We now turn to the analysis provided in [Dupuy and Loubaton, 2009, 2010;
Moustakas and Simon, 2007], leading in particular to Theorem 6.8. Obviously,
the deterministic equivalent for the case when K = 1 is the same as the one from
Theorem 6.4 for K = 1. However, the underlying conditions are now slightly
diﬀerent. In particular, large eigenvalues in R and T are no longer allowed as
we now assume that both R and T have uniformly bounded spectral norm.
Also, the proof derived in Theorem 6.8 explicitly takes into account the fact that
X has Gaussian entries. In this scenario, it is shown that the ergodic capacity
itself E[I(nt,nr)], and not simply the per-antenna ergodic mutual information, is
well approximated by the above deterministic equivalent. Constraining uniform
power allocation across the transmit antennas, we have:
E[I(nt,nr)(σ2; Int)]
−
" nr
X
k=1
log2
 1 + ¯δrk

+
nt
X
k=1
log2

1 + nr
nt
δtk

−nrσ2 log2(e)δ¯δ
#
= O(1/nr).

312
13. Performance of multiple antenna systems
The capacity maximizing transmit precoder P is shown similarly as before
to coincide asymptotically with the transmit precoder P that maximizes the
deterministic equivalent. We need however to constrain the matrices P to
lie within a set of (nt × nt) Hermitian non-negative matrices with uniformly
bounded spectral norm. Nonetheless, in their thorough analysis of the Rician
model [Dumont et al., 2010; Hachem et al., 2007, 2008b], which we will discuss
in the following section, Hachem et al. go much further and show explicitly that,
under the assumption of uniformly bounded spectral norms for T and R, the
assumption that P lies within a set of matrices with uniformly bounded spectral
norm is justiﬁed. The proof of this result is however somewhat cumbersome and
is not further discussed.
In the following section, we turn to the proper evaluation of the capacity
maximizing transmit precoder.
13.3.4
Capacity maximizing precoder
As was presented in the above sections, the capacity maximizing precoder P⋆is
close to P◦, the precoder that maximizes the deterministic equivalent of the true
capacity, for large system dimensions. We will therefore determine P◦instead of
P⋆. Note nonetheless that [Hanlen and Grant, 2003] proves that the Gaussian
input distribution is still optimal in this correlated setting and provides an
iterative water-ﬁlling algorithm to obtain P⋆, although the formulas involved
are highly non-trivial. Since we do not deal with asymptotic considerations here,
there is no need to restrict the deﬁnition domain of the non-negative Hermitian
P matrices.
By deﬁnition
P◦
= arg max
P
" nr
X
k=1
log2
 1 + ¯δrk

+ log2 det

Int + nr
nt
δT
1
2 PT
1
2

−nrσ2 log2(e)δ¯δ
#
where δ and ¯δ are here function of P, deﬁned as the unique positive solutions to
(13.8).
In order to simplify the diﬀerentiation of the deterministic equivalent along P,
which is made diﬃcult due to the interconnection between δ, ¯δ, and P, we will
use the diﬀerentiation chain rule. For this, we ﬁrst denote V the function
V : (∆, ¯∆, P) 7→
nr
X
k=1
log2
 1 + ¯∆rk

+ log2 det

Int + nr
nt
∆T
1
2 PT
1
2

−nrσ2 log2(e)∆¯∆.
That is, we deﬁne V as a function of the independent dummy parameters ∆,
¯∆, and P. The function V is therefore a deterministic equivalent of the capacity
only for restricted choices of ∆, ¯∆, and P, i.e. ∆= δ, ¯∆= ¯δ that satisfy the

13.3. Correlated frequency ﬂat fading channels
313
implicit Equations (13.8). We have that
∂V
∂∆(∆, ¯∆, P) = log2(e)
"
nr
nt
tr T
1
2 PT
1
2

Int + nr
nt
∆T
1
2 PT
1
2
−1
−nrσ2 ¯∆
#
∂V
∂¯∆(∆, ¯∆, P) = log2(e)
" nr
X
k=1
rk
1 + ¯∆rk
−nrσ2∆
#
.
Observe now that, for δ and ¯δ the solutions of (13.8) (for any given P), we
have also by deﬁnition that
nr
nt
tr T
1
2 PT
1
2

Int + nr
nt
δT
1
2 PT
1
2
−1
−nrσ2¯δ = 0
nr
X
k=1
rk
1 + ¯δrk
−nrσ2δ = 0.
As a consequence
∂V
∂∆(δ, ¯δ, P) = 0
∂V
∂¯∆(δ, ¯δ, P) = 0
and then:
∂V
∂∆(δ, ¯δ, P)∂∆
∂P(δ, ¯δ, P) + ∂V
∂¯∆(δ, ¯δ, P)∂¯∆
∂P(δ, ¯δ, P) + ∂V
∂P(δ, ¯δ, P) = ∂V
∂P(δ, ¯δ, P)
as all ﬁrst terms vanish. But, from the diﬀerentiation chain rule, this expression
coincides with the derivative of the deterministic equivalent of the mutual
information along P. For P = P◦, this derivative is zero by deﬁnition of P◦.
Therefore, setting the derivative along P of the deterministic equivalent of the
mutual information to zero is equivalent to writing
∂V
∂P(δ(P◦), ¯δ(P◦), P◦) = 0
for (δ(P◦), ¯δ(P◦)) the solution of (13.8) with P = P◦. This is equivalent to
∂
∂P log2 det

Int + δ(P◦)T
1
2 PT
1
2

= 0
which reduces to a water-ﬁlling problem for given δ(P◦). Denoting T = UΛUH
the spectral decomposition of T for some unitary matrix U, P◦is deﬁned as
P◦= UQ◦UH, with Q◦diagonal with ith diagonal entry q◦
i given by:
q◦
i =

µ −
1
δ(P◦)ti
+
µ being set so that
1
nt
Pnt
k=1 q◦
k = P.
Obviously, the question is now to determine δ(P◦). For this, the iterative
water-ﬁlling algorithm of Table 13.2 is proposed.

314
13. Performance of multiple antenna systems
Deﬁne η > 0 the convergence threshold and l ≥0 the iteration step.
At step l = 0, for k ∈{1, . . . , nt}, set q0
k = P
while maxk{|ql
k −ql−1
k
|} > η do
Deﬁne (δl+1, ¯δl+1) as the unique pair of positive solutions to (13.8)
for P = UQlUH, Ql = diag(ql
1, . . . , ql
nt)
for i ∈{1 . . . , nt} do
Set ql+1
i
=

µ −
1
cel+1ti
+
, with µ such that
1
nt tr Ql = P
end for
assign l ←l + 1
end while
Table 13.2. Iterative water-ﬁlling algorithm for the Kronecker channel model.
In [Dumont et al., 2010], it is shown that, if the iterative water-ﬁlling algorithm
does converge, then the iterated Ql matrices of the algorithm in Table 13.2
necessarily converge towards Q◦. To this day, though, no proof of the absolute
or conditional convergence of this water-ﬁlling algorithm has been provided.
To conclude this section on correlated MIMO transmissions, we present
simulation results for R and T modeled as a Jakes’ correlation matrix, i.e. the
(i, j)th entry of R or T equals J0(2πdij/λ), with λ the transmission wavelength
and dij the distance between antenna i and antenna j (on either of the two
communication sides). We assume the antennas are distributed along a horizontal
linear array and numbered in order (say, from left to right), so that T and R
are simply Toeplitz matrices based on the vector (1, J0(2πd/λ), . . . , J0(2π(nt −
1)d/λ), with d the distance between neighboring antennas. The eigenvalues of R
and T for nt = 4 are provided in Table 13.3 for diﬀerent ratios d/λ. Jakes’ model
arises from the assumption that the antenna array under study transmits or
receives waveforms of wavelength λ isotropically in the three-dimensional space,
which is a satisfying assumption under no additional channel constraint.
In Figure 13.3, we depict simulation results of the MIMO mutual information
with equal power allocation at the transmitter as well as the MIMO capacity (i.e.
with optimal power allocation), and compare these results to the deterministic
equivalents derived in this section. We assume 4 × 4 MIMO communication with
inter-antenna spacing d = 0.1λ and d = λ. It turns out that, even for strongly
correlated channels, the mutual information with uniform power allocation is
very well approximated for all SNR values, while a slight mismatch appears for
the optimal power allocation policy. Although this is not appearing in this graph,
we mention that the mismatch gets more acute for higher SNR values. This is
consistent with the results derived in this section.
From an information-theoretic point of view, observe that the gains achieved
by optimal power allocation in weakly correlated channels are very marginal,
as the water-ﬁlling algorithm distributes power almost evenly on the channel
eigenmodes, while the gains brought about by optimal power allocation in

13.3. Correlated frequency ﬂat fading channels
315
Correlation factor
Eigenvalues of T, R
d = 0.1λ
0.0
0.0
0.1
3.9
d = λ
0.3
1.0
1.4
1.5
d = 10λ
0.8
1.0
1.1
1.1
Table
13.3. Eigenvalues
of
correlation
matrices
for
nt = nr = 4,
under
diﬀerent
correlations.
−15
−10
−5
0
5
10
15
20
0
5
10
15
20
25
SNR [dB]
Achievable rate
d
λ = 0.1, uni., sim.
d
λ = 0.1, uni., det.
d
λ = 0.1, opt., sim.
d
λ = 0.1, opt., det.
d
λ = 1, uni., sim.
d
λ = 1, uni., det.
d
λ = 1, opt., sim.
d
λ = 1, opt., det.
Figure 13.3 Ergodic capacity from simulation (sim.) and deterministic equivalent
(det.) of the Jakes’ correlated 4 × 4 MIMO, for SNR varying from −15 dB to 20 dB,
for diﬀerent values of d
λ, time-varying channels, uniform (uni.) and optimal (opt.)
power allocation.
strongly correlated channels are much more relevant, as the water-ﬁlling
algorithm manages to pour much of the power onto the stronger eigenmodes.
Also, the diﬀerence between the mutual information for uniform and optimal
power allocations reduces as the SNR grows. This is due to the fact that, for
high SNR, the contribution to the capacity of every channel mode become
sensibly the same, and therefore, from concavity arguments, it is optimal to
evenly distribute the available transmit power along these modes. In contrast,
for low SNR, log(1 + |hk|2σ−2) is close to |hk|2σ−2, where |hk|2 denotes the kth
eigenvalue of HHH, and therefore it makes sense to pour more power on the
larger |hk|2 eigenvalues. Also notice, as described in [Goldsmith et al., 2003]
that, for low SNR, statistical correlation is in fact beneﬁcial, as the capacity is
larger than in the weak correlation case. This is here due to the fact that strong
correlation comes along with high power modes (see Table 13.3), in which all the
available power can be poured to increase the transmission bit rate.

316
13. Performance of multiple antenna systems
13.4
Rician ﬂat fading channels
Note that in the previous section, although we used Theorem 6.4 and Theorem
6.8 that provide in their general form deterministic equivalents that are functions
of the eigenvectors of the underlying random matrices, all results derived so far
are only functions of the eigenvalues of the correlation matrices T and R. That
is, we could have considered T and R diagonal. It is therefore equivalent for
the point-to-point MIMO analysis to consider doubly correlated transmissions
or uncorrelated transmissions with weighted powers across the antennas. As
such, the large system analysis via deterministic equivalents of doubly correlated
MIMO communications is the same as that of the MIMO communication over
the channel H with independent Gaussian entries of zero mean and variance
proﬁle {σ2
ij/nt}, where σij = √tirj with ti the ith diagonal entry (or eigenvalue)
of T and rj the jth diagonal entry of R. When the variance proﬁle can be written
under this product form of the variance of rows and columns, we say that H has
a separable variance proﬁle.
As a consequence, the study of point-to-point MIMO channels with a general
(non-necessarily separable) variance proﬁle and entries of non-necessary zero
mean completely generalizes the previous study. Such channels are known as
Rician channels. The asymptotic analysis of these Rician channels is the objective
of this section, which relies completely on the three important contributions
of Hachem, Loubaton, Dumont, and Najim that are [Dumont et al., 2010;
Hachem et al., 2007, 2008b]. Note that early studies assuming channel matrices
with entries of non-zero mean and unit variance were already provided in, e.g.,
[Moustakas and Simon, 2003] in the multiple input single output (MISO) case
and [Cottatellucci and Debbah, 2004a,b] in the MIMO case.
Before moving to the study of the ergodic capacity, i.e. the study of the
results from [Hachem et al., 2007], it is important to remind that Rician channel
models do not generalize Kronecker channel models. What we stated before is
exactly that, for the capacity analysis of point-to-point MIMO communications,
Kronecker channels can be substituted by Rician channels with zero mean and
separable variance proﬁles. However, we will see in Chapter 14 that, for multi-
user communications, this remark does not hold in general, unless all users have
identical or at least co-diagonalizable channel correlation matrices. The coming
analysis of Rician channel models is therefore not suﬃcient to treat the most
general multi-user communications, which will be addressed in Chapter 14.
13.4.1
Quasi-static mutual information and ergodic capacity
Consider the point-to-point communication between an nt-antenna transmitter
and an nr-antenna receiver. The communication channel is denoted by the
random matrix H ∈Cnr×nt which is modeled as Rician, i.e. the entries of H
are Gaussian, independent and the (i, j)th entry hij has mean aij and variance

13.4. Rician ﬂat fading channels
317
σ2
ij/nt, for 1 ≤i ≤nr and 1 ≤j ≤nt. We further denote A ∈Cnr×nt the matrix
with (i, j)th entry aij, Σj ∈Rnr×nr the diagonal matrix of ith entry σij, and
¯Σi ∈Rnt×nt the diagonal matrix of jth entry σij. Assume, as in the previous
sections, that the ﬁltered transmit signal is corrupted by additive white Gaussian
noise of variance σ2 on all receive antennas. Here we assume that the σij are
uniformly bounded with respect to the system dimensions nt and nr.
Assuming equal power allocation at the transmitter, the mutual information
for the per-antenna quasi-static case I(nt,nr)(σ2; Int) reads:
I(nt,nr)(σ2; Int) = log2 det

Inr + 1
σ2 HHH

= log2 det

Inr + 1
σ2

R
1
2 XT
1
2 + A
 
R
1
2 XT
1
2 + A
H
.
From Theorem 6.14, we have that
1
nt
I(nt,nr)(σ2; Int) −
 1
nr
log2 det
 1
σ2 Ψ−1 + A ¯ΨAT

+ 1
nr
log2 det
 1
σ2 ¯Ψ−1

−log2(e)σ2
ntnr
X
i,j
σ2
ijvi¯vj

a.s.
−→0
(13.9)
where we deﬁned Ψ ∈Cnr×nr the diagonal matrix with ith entry ψi, ¯Ψ ∈Cnr×nr
the diagonal matrix with jth entry ¯ψj, with ψi and ¯ψj, 1 ≤i ≤nr, 1 ≤j ≤nt,
the unique solutions of
ψi = 1
σ2

1 + 1
nr
tr ¯Σ2
i
  ¯Ψ−1 + σ2ATΨA
−1−1
¯ψj = 1
σ2

1 + 1
nr
tr Σ2
j
 Ψ−1 + σ2A ¯ΨAT−1−1
which are Stieltjes transforms of distribution functions, while vi and ¯vj are
deﬁned as the ith diagonal entry of
  ¯Ψ−1 + σ2ATΨA
−1 and the jth diagonal
entry of
 Ψ−1 + σ2A ¯ΨAT−1, respectively.
Also, due to the Gaussian assumption (the general statement is based on
some moment assumptions), [Hachem et al., 2008b] ensures that the ergodic
mutual information I(nt,nr)(σ2; Int) under uniform transmit power allocation
has deterministic equivalent
E[I(nt,nr)(σ2; Int)]
−

log2 det
 1
σ2 Ψ−1 + A ¯ΨAT

+ log2 det
 1
σ2 ¯Ψ−1

−σ2 log2(e)
nt
X
i,j
σ2
ijvi¯vj


= O(1/nt).
(13.10)
As we have already mentioned in the beginning of this section, Equations (13.9)
and (13.10) can be veriﬁed to completely generalize the uncorrelated channel

318
13. Performance of multiple antenna systems
case, the Kronecker channel case, and the case of a channel with separable
variance proﬁle.
13.4.2
Capacity maximizing power allocation
In [Dumont et al., 2010], the authors assume the above Rician channel model,
however restricted to a separable variance proﬁle, i.e. for all pairs (i, j), σij can
be written as a product ritj. We then slightly alter the previous model setting
by considering the channel model
H = R
1
2 XT
1
2 P
1
2 + AP
1
2
with R ∈Rnr×nr diagonal with ith diagonal entry ri, T ∈Rnt×nt diagonal with
jth diagonal entry tj, and P Hermitian non-negative, the transmit precoding
matrix. Note, as previously mentioned, that taking R and T to be diagonal does
not restrict generality. Indeed, the mutual information for this channel model
reads:
I(nt,nr)(σ2; P)
= log2 det

Inr + 1
σ2

R
1
2 XT
1
2 P
1
2 + AP
1
2
 
R
1
2 XT
1
2 P
1
2 + AP
1
2
H
= log2 det

Inr + 1
σ2

UR
1
2 UHXVT
1
2 VHVP
1
2 VH + UAVHVP
1
2 VH
×

UR
1
2 UHXVT
1
2 VHVP
1
2 VH + UAVHVP
1
2 VHH
for any unitary matrices U ∈Cnt×nt and V ∈Cnr×nr. The matrix X being
Gaussian, its distribution is not altered by the left- and right-unitary products
by UH and V, respectively. Also, when addressing the power allocation problem,
optimization can be carried out equally on P or VPVH. Therefore, instead of the
diagonal R and T covariance matrices, we could have considered the Kronecker
channel with non-necessarily diagonal correlation matrices URUH at the receiver
and VTVH at the transmitter and the line-of-sight component matrix UAVH.
The ergodic capacity optimizing power allocation matrix, under power
constraint
1
nt tr P ≤P, is then shown in [Dumont et al., 2010] to be determined
as the solution of an iterative water-ﬁlling algorithm. Speciﬁcally, call P◦the
optimal power allocation policy for the deterministic equivalent of the ergodic
capacity. Using a similar approach as for the Kronecker channel case, it is shown
that maximizing the deterministic equivalent of I(nt,nr)(σ2; P) is equivalent to
maximizing the function
V : (δ, ¯δ, P) 7→log2 det
 Int + PG(δ, ¯δ)

+ log2 det
 Inr + ¯δR

−log2(e)ntσ2δ¯δ
over P such that
1
nt tr P = P, where G is the deterministic matrix
G = δT + 1
σ2 AT  Inr + ¯δR
−1 A
(13.11)

13.4. Rician ﬂat fading channels
319
Deﬁne η > 0 the convergence threshold and l ≥0 the iteration step.
At step l = 0, for k ∈{1, . . . , nt}, set q0
k = P. At step l ≥1,
while maxk{|ql
k −ql−1
k
|} > η do
Deﬁne (δl+1, ¯δl+1) as the unique pair of positive solutions to
(13.12) for P = WQlWH, Ql = diag(ql
1, . . . , ql
nt) and W the matrix
such that G, given in (13.11) with δ = δl, ¯δ = ¯δl has spectral
decomposition G = WΛWH, Λ = diag(λ1, . . . , λnt)
for i ∈{1 . . . , nt} do
Set ql+1
i
=

µ −1
λi
+
, with µ such that
1
nt tr Ql = P
end for
assign l ←l + 1
end while
Table 13.4. Iterative water-ﬁlling algorithm for the Rician channel model with separable
variance proﬁle.
in the particular case when δ = δ(P◦) and ¯δ = ¯δ(P◦), the latter two scalars being
the unique solutions to the ﬁxed-point equation
δ =
1
σ2nt
tr R

(Inr + ¯δR) + 1
σ2 AP◦1
2 (Int + δP◦1
2 TP◦1
2 )−1P◦1
2 AT
−1
¯δ =
1
σ2nt
tr P◦1
2 TP◦1
2

(Int + δP◦1
2 TP◦1
2 ) + 1
σ2 P◦1
2 AT(Inr + ¯δR)−1AP◦1
2
−1
(13.12)
which are Stieltjes transforms of distribution functions.
Similar to the Kronecker case, the iterative water-ﬁlling algorithm of Table
13.4 solves the power-optimization problem.
It is common to consider a parameter κ, the Rician factor, that accounts for
the relative importance of the line-of-sight component A relative to the varying
random part R
1
2 XT
1
2 . To incorporate the Rician factor in the present model,
we assume that
1
nt tr AAT =
κ
κ+1 and that R is constrained to satisfy
1
nr tr R =
1
κ+1. Therefore, the larger the parameter κ the more important the line-of-sight
contribution. In Figure 13.4, we consider the 4 × 4 MIMO Rician channel where
A has entries equal to
q
1
nt
κ
κ+1, and T, R are modeled as Jakes’ correlation
matrices for a linear antenna array with distance d between consecutive antennas.
The transmission wavelength is denoted λ. We wish to study the relative impact
of the line-of-sight component A on the ergodic capacity, and on the power
allocation policy. Therefore, we consider ﬁrst mildly correlated antennas at both
communication ends with inter-antenna distance d = λ. We give the channel
performance both for κ = 1 and for κ = 100.
We observe again, already for the 4 × 4 MIMO case, a very accurate match
between the approximated and simulated ergodic capacity expressions. We ﬁrst

320
13. Performance of multiple antenna systems
−15
−10
−5
0
5
10
15
20
0
5
10
15
20
SNR [dB]
Achievable rate
κ = 1, uni., sim.
κ = 1, uni., det.
κ = 1, opt., sim.
κ = 1, opt., det.
κ = 100, uni., sim.
κ = 100, uni., det.
κ = 100, opt., sim.
κ = 100, opt., det.
Figure 13.4 Ergodic capacity from simulation (sim.) and deterministic equivalent
(det.) of the Jakes’ correlated 4 × 4 MIMO model with line-of-sight component, linear
antenna arrays with d
λ = 1 are considered at both communication ends. The SNR
varies from −15 dB to 20 dB, and the Rician factor κ is chosen to be κ = 1 and
κ = 100. Uniform (uni.) and optimal (opt.) power allocations are considered.
see that the capacity for κ small is larger than the capacity for κ large at high
SNR, which is due to the limited multiplexing gain oﬀered by the strongly line-
of-sight channel. Note also that, for κ = 1, there is little room for high capacity
gain by proceeding to optimal power allocation, while important gains can be
achieved when κ = 100. For asymptotically large κ, the optimal power allocation
policy requires that all power be poured on the unique non-zero eigenmode of the
channel matrix. From the trace constraint on the precoding matrix, this entails
a SNR gain of up to log10(nt) ≃6 dB (already observed in the zero dB-10 dB
region). For low SNR regimes, it is therefore again preferable to seek correlation,
embodied here by the line-of-sight component. In contrast, for medium to high
SNR regimes, correlation is better avoided to fully beneﬁt from the channel
multiplexing gain. The latter is always equal to min(nt, nr) for all ﬁnite κ,
although extremely large SNR conditions might be necessary for this gain to
be observable.
13.4.3
Outage mutual information
We complete this section by the results from [Hachem et al., 2008b] on a central
limit theorem for the capacity of the Rician channel H, in the particular case
when the channel coeﬃcients hij have zero mean, i.e. when A = 0. Note that the
simple one-sided correlation case was treated earlier in [Debbah and R. M¨uller,

13.4. Rician ﬂat fading channels
321
2003], based on a direct application of the central limit Theorem 3.17. Further,
assume either of the following situations:
• the variance proﬁle {σ2
ij/nt} is separable in the sense that σij = √ritj and
the transmitter precodes its signals with matrix P. In that case, we denote
T = diag(t1, . . . , tnt) and {σ′2
ij/nt} the variance proﬁle with σ′
ij = i
q
rit′
j with
t′
1, . . . , t′
nt the eigenvalues of T
1
2 PT
1
2 .
• both T and the precoding matrix P = diag(p1, . . . , pnt) are diagonal. In this
case, σ′
ij =
q
rit′
j, where t′
j = σ2
i,jpj.
The result is summarized in Theorem 6.21, which states under the current
channel conditions that, for a deterministic precoder P satisfying one of the
above conditions, the mutual information I(nt,nr)(σ2; P) asymptotically varies
around its mean E[I(nt,nr)(σ2; P)] as a zero mean Gaussian random variable
with, for each nr, a variance close to θ2
nr, deﬁned as
θ2
nr = −log det (Int −Jnt)
where Jnt is the matrix with (i, j)th entry Jnt
ij , deﬁned as
Jnt
ij = 1
nt
1
nt
Pnr
k=1 σ′2
kiσ′2
kjt2
k

1 + 1
nt
Pnr
k=1 σ′
ki
2tk
2
with t1, . . . , tnr, deﬁned as the unique solutions of
ti =

σ2 + 1
nt
nt
X
j=1
σ′2
ij
1 + 1
nt
Pnr
l=1 σ′2
ljtl


−1
which are Stieltjes transforms of distribution functions when seen as functions
of the variable −σ2.
We therefore deduce a theoretical approximation of the outage mutual
information I(nt,nr)
outage (σ2; P; q), for large nt and nr and deterministic precoder
P, deﬁned by
I(nt,nr)
outage (σ2; P; q) = sup
R≥0
n
P

I(nt,nr)(σ2; P) > R

≤q
o
with I(nt,nr)(σ2; P) the quasi-static mutual information for the deterministic
precoder P. This is:
I(nt,nr)
outage (σ2; P; q) ≃Q−1
 q
θnr

with Q(x) the Gaussian Q-function, where the inverse is taken with respect to
the conjugation.
In Figure 13.5, we provide the curves of the theoretical distribution function
of the mutual information for the precoder P = Int. The assumptions taken in
Figure 13.5 are those of a Kronecker channel with transmit correlation matrices

322
13. Performance of multiple antenna systems
0
2
4
6
8
10
12
14
0
0.2
0.4
0.6
0.8
1
Achievable rate
Distribution function
d
λ = 0.1
d
λ = 1
d
λ = 10
Figure 13.5 Deterministic equivalent of the outage capacity for the Jakes’ correlated
4 × 4 MIMO channel model, linear antenna arrays with d
λ = 0.1, d
λ = 1 and d
λ = 10.
The SNR is set to 10 dB. Uniform power allocation is considered.
R ∈Cnr×nr and T ∈Cnt×nt modeled along Jakes’ correlation model for a linear
antenna array with inter-antenna distance d (both at the transmit and receive
ends) and transmission wavelength λ. We depict the outage performance of the
MIMO Rician channel for diﬀerent values of d/λ.
This
concludes
this
section
on
the
performance
of
point-to-point
communications over MIMO Rician channels. We recall that the most general
Rician setup generalizes all previously mentioned models. Nonetheless, the
subject has not been fully covered as neither the optimal power allocation
policy for a Rician channel with non-separable variance proﬁle, nor the outage
performance of a Rician channel with line-of-sight component have been
addressed.
In what follows, we generalize multiple antenna point-to-point communications
towards another direction, by introducing frequency selectivity. This will provide
a ﬁrst example of a channel model for which the MIMO Rician study developed in
this section is not suﬃcient to provide a theoretical analysis of the communication
performance.
13.5
Frequency selective channels
Due to the additional data processing eﬀort required by multiple antenna
communications compared to single antenna transmissions and to the non-
negligible correlation arising in multiple antenna systems, MIMO technologies are
mostly used as a solution to further increase the achievable data rate of existing

13.5. Frequency selective channels
323
wireless broadband communication networks, but are not used as a substitute for
large bandwidth communications. Therefore, practical MIMO communication
networks usually come along with large transmission bandwidths. It is therefore
often unrealistic to assume narrowband MIMO transmission as we have done
up to now. This section is dedicated to frequency selective transmissions, for
which the channel coherence bandwidth is assumed smaller than the typical
transmission bandwidth, or equivalently for which strong multi-path components
convey signal energy.
Consider an L-path MIMO channel between an nt-antenna transmitter and
an nr-antenna receiver, modeled as a sequence of L matrices {H1, . . . , HL},
Hl ∈Cnr×nt. That is, each link from transmit antenna i and receive antenna j is a
multi-path scalar channel with ordered path gains given by (h1(j, i), . . . , hL(j, i)).
In the frequency domain, the channel transfer matrix is modeled as a random
matrix process H(f) ∈Cnr×nt, for every frequency f ∈[−W/2, W/2], deﬁned as
H(f) =
L
X
k=1
Hke−2πik f
W
with W the two-sided baseband communication bandwidth. The resulting
(frequency normalized) quasi-static capacity C(nt,nr) of the frequency selective
channel H(f), for a communication in additive white Gaussian noise and under
power constraint P, reads:
C(nt,nr)(σ2) = sup
{P(f)}
1
W
Z W/2
−W/2
log2 det

Inr + 1
σ2 H(f)P(f)HH(f)

df
with P(f) ∈Cnt×nt, f ∈[−W/2, W/2], a matrix-valued function modeling the
precoding matrix to be applied at all frequencies, with maximum mean power P
(per Hertz) and therefore submitted to the power constraint
1
ntW
Z
f
tr P(f)df ≤P.
(13.13)
According to the deﬁnition of H(f), the capacity C(nt,nr)(σ2) also reads:
C(nt,nr)(σ2) = sup
{P(f)}
1
W
Z W/2
−W/2
log2 det
 
Inr + 1
σ2
" L
X
k=1
Hke−2πik f
W
#
P(f)
" L
X
k=1
HH
k e2πik f
W
#!
df
under the trace constraint (13.13) on P(f).
We then assume that every channel Hk, k ∈{1, . . . , L}, is modeled as a
Kronecker channel, with non-negative deﬁnite left-correlation matrix Rk ∈
Cnr×nr and non-negative deﬁnite right-correlation matrix Tk ∈Cnt×nt. That
is, Hk can be expressed as
Hk = R
1
2
k XkT
1
2
k

324
13. Performance of multiple antenna systems
where Xk ∈Cnr×nt has i.i.d. Gaussian entries of zero mean and variance 1/nt.
We also impose that the matrices Tk and Rk, for all k, have uniformly bounded
spectral norms and that the Xk are independent.
Although this is not strictly proven in the literature (Theorem 6.8), we infer
that, for all ﬁxed f and all deterministic choices of {P(f)} (with possibly
some mild assumptions on the extreme eigenvalues), it is possible to provide
a deterministic equivalent of the random quantity
1
nr
log2 det
 
Inr + 1
σ2
" L
X
k=1
Hke−2πik f
W
#
P(f)
" L
X
k=1
HH
k e2πik f
W
#!
using the implicit equations of Theorem 6.8. The latter indeed only provides
the convergence of such a deterministic equivalent in the mean. Integrating this
deterministic equivalent over f ∈[−W/2, W/2] (and possibly averaging over W)
would then lead to a straightforward deterministic equivalent for the per-receive-
antenna quasi-static capacity (or its per-frequency version). Note that Theorem
6.4 and Theorem 6.8 are very similar in nature, so that the latter must be
extensible to the quasi-static case, using tools from the proof of the former.
Similar to previous sections, it will however not be possible to derive the matrix
process {P(f)} which maximizes the capacity, as was performed for instance in
[Tse and Hanly, 1998] in the single antenna (multi-user) case. We mention that
[Scaglione, 2002] provides an explicit expression of the characteristic function of
the above mutual information in the small dimensional setting.
13.5.1
Ergodic capacity
For the technical reasons explained above and also because this is a more telling
measure of performance, we only consider the ergodic capacity of the frequency
selective MIMO channel. Note that this frequency selective ergodic capacity
C(nt,nr)
ergodic (σ2) reads:
C(nt,nr)
ergodic (σ2)
= sup
{P(f)}
1
W
Z W/2
−W/2
E
"
log2 det
 
Inr + 1
σ2
" L
X
k=1
Hk
#
P(f)
" L
X
k=1
HH
k
#!#
df
= sup
P(0)
E
"
log2 det
 
Inr + 1
σ2
" L
X
k=1
Hk
#
P(0)
" L
X
k=1
HH
k
#!#
where in the second equality we discarded the terms e−2πik f
W since it is equivalent
to take the expectation over Xk or over Xke−2πik f
W , for all f ∈[−W/2, W/2]
(since both matrices have the same joint Gaussian entry distribution). Therefore,
on average, all frequencies are alike and the current problem reduces to ﬁnding
a deterministic equivalent for the single frequency case. Also, it is obvious from
convexity arguments that there is no reason to distribute the power P unevenly
along the diﬀerent frequencies. Therefore, the power optimization can be simply

13.5. Frequency selective channels
325
operated over a single frequency and the supremum can be taken over the single
precoding matrix P(0). The new power constraint is therefore:
1
nt
tr P(0) ≤P.
For ease of read, from now on, we denote P ≜P(0).
For all deterministic choices of precoding matrices P, the ergodic mutual
information EI(nt,nr)(σ2; P) has a deterministic equivalent, given by Theorem
6.8, such that
E[I(nt,nr)(σ2; P)]−
"
log2 det
 
Inr +
L
X
k=1
¯δkRk
!
+ log2 det
 
Int +
L
X
k=1
δkT
1
2
k PT
1
2
k
!
−nr log2(e)σ2
L
X
k=1
¯δkδk
#
= O
 1
N

where ¯δk and δk, k ∈{1, . . . , L}, are deﬁned as the unique positive solutions of
δi =
1
nrσ2 tr Ri
 
Inr +
L
X
k=1
¯δkRk
!
¯δi =
1
nrσ2 tr T
1
2
i PT
1
2
i
 
Int +
L
X
k=1
δkT
1
2
k PT
1
2
k
!
.
(13.14)
13.5.2
Capacity maximizing power allocation
Based on the standard methods evoked so far, the authors in [Dupuy and
Loubaton, 2010] prove that the optimal power allocation strategy is to perform
a standard water-ﬁlling procedure on the matrix
L
X
k=1
δk(P◦)Tk
where we deﬁne P◦as the precoding matrix that maximizes the deterministic
equivalent of the ergodic mutual information, and we denote δk(P◦) the (unique
positive) solution of the system of Equations (13.14) when P = P◦.
Denote UΛUH the spectral decomposition of PL
k=1 δk(P◦)Tk, with U unitary
and Λ a diagonal matrix with diagonal entries λ1, . . . , λnt. We have that P◦
asymptotically well approximates the ergodic mutual information maximizing
precoder, and is given by:
P◦= UQ◦UH
where Q◦is diagonal with diagonal entries q◦
1, . . . , q◦
nt deﬁned by
q◦
k =

µ −1
λk
+

326
13. Performance of multiple antenna systems
Deﬁne η > 0 the convergence threshold and l ≥0 the iteration step.
At step l = 0, for k ∈{1, . . . , nt}, set q0
k = P. At step l ≥1,
while maxk{|ql
k −ql−1
k
|} > η do
Deﬁne (δl+1, ¯δl+1) as the unique pair of positive solutions to
(13.14) for P = UQlUH, Ql = diag(ql
1, . . . , ql
nt) and U the matrix
such that PL
k=1 δl
kTk has spectral decomposition UΛUH, Λ =
diag(λ1, . . . , λnt)
for i ∈{1 . . . , nt} do
Set ql+1
i
=

µ −1
λi
+
, with µ such that
1
nt tr Ql = P
end for
assign l ←l + 1
end while
Table 13.5. Iterative water-ﬁlling algorithm for the frequency selective channel.
where µ is set such that
1
nt
Pnt
k=1 q◦
k = P. Similar to the previous sections, it is
shown in [Dupuy and Loubaton, 2010] that, upon convergence, the precoder P
in the iterative water-ﬁlling algorithm of Table 13.5 converges to P◦.
For simulations, we consider a signal correlation model, which we will often
use in the next chapter. Since all Rk and Tk matrices, k ∈{1, . . . , L}, account
for the speciﬁc correlation pattern for every individual delayed propagation
path on the antenna array, taking a standard Jakes’ model for the Rk and Tk
matrices would lead to T1 = . . . = TL and R1 = . . . = RL. This is due to the fact
that Jakes’ model only accounts for inter-antenna distance, which is a physical
device parameter independent of the propagation path. A further generalization
of Jakes’ model consists in taking into consideration the solid angle of energy
departure or arrival. We will refer to this model as the generalized Jakes’ model.
Precisely, and for simplicity, we assume that the signal energy arrives (or departs)
uniformly from angles in the vertical direction, but from a restricted angle in the
horizontal direction, spanning from a minimum angle θmin to a maximum angle
θmax. Assuming linear antenna arrays at both communication ends and following
the derivation of Jakes’ model, the (n, m)th entry of, say, matrix R1 is given by:
R1nm =
Z θ(R1)
max
θ(R1)
min
exp

2πi|n −m|dR
λ cos(θ)

dθ
where dR is the distance between consecutive receive antennas, λ the signal
wavelength, θ(R1)
min the minimum horizontal angle of energy arrival, and θ(R1)
max the
maximum angle of energy arrival.
In Figure 13.6, we consider a two-path 4 × 4 frequency selective MIMO
channel. We assume that antenna j, on either communication side, emits or
receives data from a solid angle spanning the horizontal plane from the minimum
angle θ(Rj)
min = θ(Tj)
min = (j −1) π
L to the maximum angle θ(Rj)
max = θ(Tj)
max = ∆+ (j −

13.5. Frequency selective channels
327
−15
−10
−5
0
5
10
15
20
0
5
10
15
20
SNR [dB]
Achievable rate
∆= π
6 , sim., uni.
∆= π
6 , det., uni.
∆= π
6 , sim., opt.
∆= π
6 , det., opt.
∆= 2π, sim., uni.
∆= 2π, det., uni.
∆= 2π, sim., opt.
∆= 2π, det., opt.
Figure 13.6 Ergodic capacity of the frequency selective 4 × 4 MIMO channel. Linear
antenna arrays on each side, with correlation matrices modeled according to the
generalized Jakes’ model. Angle spreads in the horizontal direction set to ∆= 2π or
∆= π/6. Comparison between simulations (sim.) and deterministic equivalent (det.),
for uniform power allocation (uni.) and optimal power allocation (opt.).
1) π
L. We also assume an inter-antenna distance of dR = λ at the receiver side and
dT = 0.1λ at the transmitter side. We take successively ∆= 2π and ∆= π
6 . We
observe that the achievable rate is heavily impacted by the choice of a restricted
angle of aperture at both transmission sides. This is because transmit and receive
correlations increase with smaller antenna aperture, an eﬀect which it is therefore
essential to take into account.
We complete this chapter on single-user multiple antenna communications
with more applied considerations on suboptimal transmitter and receiver design.
From a practical point of view, be it for CDMA or MIMO technologies, achieving
channel capacity requires in general heavy computational methods, the cost of
which may be prohibitive for small communication devices. In place for optimal
precoders and decoders, we have already seen several instances of linear precoders
and decoders, such as the matched-ﬁlter and the MMSE ﬁlter. The subject of
the next section is to study intermediary precoders, performing better than the
matched-ﬁlter, less than the MMSE ﬁlter, but with adjustable complexity that
can be made simple and eﬃcient thanks to large dimensional random matrix
results.

328
13. Performance of multiple antenna systems
13.6
Transceiver design
In this last section, we depart from the previous capacity analysis introduced in
this chapter to move to a very practical application of large dimensional random
matrix results. The application we deal with targets the complexity reduction
of some linear precoder or decoder designs. Precisely, we will propose successive
approximations of MMSE ﬁlters with low complexity. We recall indeed that
MMSE ﬁlters demand the inversion of a potential large dimensional matrix,
the latter depending on the possibly fast changing communication channel. For
instance, we introduced in the previous chapter linear MMSE CDMA decoders
which are designed based on both the spreading code (usually constant over the
whole communication) and the multi-path channel gains (possibly varying fast
with time). If both the number of users and the number of chips per code are
large, inverting the decoding matrix every channel coherence time imposes a large
computational burden on the decoder, which might be intolerable for practical
purposes. This problem would usually and unfortunately be solved by turning
to less complex decoders, such as the classical matched-ﬁlter. Thanks to large
dimensional random matrix theory, we will realize that most of the complexity
involved in large matrix inverses can be fairly reduced by writing the matrix
inverse as a ﬁnite weighted sum of matrices and by approximating the weights
in this sum (which carry most of the computational burden) by deterministic
equivalents.
We will address in this section the question of optimal low complex MMSE
decoder design. The results mentioned below are initially due to M¨uller and
Verd´u in [M¨uller and Verd´u, 2001] and are further developed in the work of
Cottatellucci et al. in, e.g., [Cottatellucci and M¨uller, 2002, 2005; Cottatellucci
et al., 2004], Loubaton et al. [Loubaton and Hachem, 2003], and Hoydis et al.
[Hoydis et al., 2011c].
Consider the following communication channel
y = Hx + σw
with x = [x1, . . . , xn]T ∈Cn some transmitted vectorial data of dimension n,
assumed to have zero mean and covariance matrix In, w ∈CN is an additive
Gaussian noise vector of zero mean and covariance IN, y = [y1, . . . , yN]T ∈CN
is the received signal, and H ∈CN×n is the multi-dimensional communication
channel.
Under these model assumptions, irrespective of the communication scenario
under study (e.g. MIMO, CDMA), the minimum mean square error decoder
output ˆx for the vector x reads:
ˆx =
 HHH + σ2In
−1 HHy
(13.15)
= HH  HHH + σ2IN
−1 y.

13.6. Transceiver design
329
For practical applications, recovering ˆx therefore requires the inversion of
the potentially large
 HHH + σ2IN
−1 matrix. This inverted matrix has to be
evaluated every time the channel matrix H changes. It unfolds that a high
computational eﬀort is required at the receiver to numerically evaluate such
matrices. In some situations, where the computational burden at the receiver
is an important constraint (e.g. impacting directly the battery consumption in
cellular phones), this eﬀort might be unbearable and we may have to resort to
low complexity and less eﬃcient detectors, such as the matched-ﬁlter HH.
Now, from the Cayley-Hamilton theorem, any matrix is a (matrix-valued)
root of its characteristic polynomial. That is, denoting P(x) the characteristic
polynomial of HHH + σ2In, i.e.
P(x) = det
 HHH + σ2In −xIn

it is clear that P(HHH + σ2In) = 0. Since the determinant above can be written
as a polynomial of x of maximum degree n, P(x) expresses as
P(x) =
n
X
i=0
aixi
(13.16)
for some coeﬃcients a0, . . . , an to determine.
From (13.16), we then have
0 = P(HHH + σ2In) =
n
X
i=0
ai
 HHH + σ2In
i
from which
−a0 =
n
X
i=1
ai
 HHH + σ2In
i .
Multiplying both sides by (HHH + σ2In)−1, this becomes
 HHH + σ2In
−1 = −
n
X
i=1
ai
a0
 HHH + σ2In
i−1
= −
n
X
i=1
ai
a0
i−1
X
j=0
i −1
j

σ2(i−j−1)  HHH
i−1
and therefore (13.15) can be rewritten under the form
ˆx =
n−1
X
i=0
bi
 HHH
i HHy
with bi−1 = −ai
a0
Pi−1
j=0
 i−1
j

σ2(i−j−1).
Obviously, the eﬀort required to compute (HHH + σ2In)−1 is equivalent to
the eﬀort required to compute the above sum. Nonetheless, it will appear that
the bi above can be expressed as a function of the trace of successive powers of

330
13. Performance of multiple antenna systems
HHH + σ2In, which, in some cases, can be very well approximated thanks to
random matrix theory approaches, prior to communication.
Nonetheless, even if b0, . . . , bn−1 can be easily approximated, to evaluate ˆx, we
still have to take successive powers of HHH, which may still be computationally
intense. On the other hand, the matched-ﬁlter performance may be so weak
that it cannot be used either. Instead, it is always possible to use an estimator
that both performs better than the matched-ﬁlter and is less computationally
demanding than the MMSE ﬁlter. The idea here consists in substituting the
MMSE or MF ﬁlters by a polynomial ﬁlter of order m, with m ≤n −1. The
most natural decoder that comes to mind is a polynomially truncated version of
the above decoder, i.e.
m−1
X
i=0
bi
 HHH
i HH
for some m ≤n.
As often, though, sharp truncations do not lead to very eﬃcient results.
Instead, we may consider the polynomial precoder of order m in the variables
HHH, which minimizes the mean square decoding error, among all such
polynomial precoders. That is, we now seek for a precoder
m−1
X
i=0
b(m)
i
 HHH
i HH
for some coeﬃcients b(m)
k
deﬁned by
b(m) = (b(m)
0
, . . . , b(m)
m−1) = arg
min
(β0,...,βm−1) E


x −
m
X
i=0
βi
 HHH
i HHy

2
.
Note that the b(n)
k
= bk are the coeﬃcients of the MMSE decoder, while b(1)
1
is
the coeﬃcient of the matched-ﬁlter. Obviously, for given m, m′ with m < m′
arg
min
(β0,...,βm′−1) E


x −
m′
X
i=0
βi
 HHH
i HHy

2

≤arg
min
(β0,...,βm−1) E


x −
m
X
i=0
βi
 HHH
i HHy

2
.
This is because equality is already achieved by taking βm = . . . = βm′−1 = 0 in
the left-hand side argument. Therefore, with m ranging from 1 to n, we move
gradually from the simplest and inexpensive matched-ﬁlter to the most elaborate
but computationally demanding MMSE ﬁlter.
The above weights b(m) are actually known and are provided explicitly by
Moshavi et al. in [Moshavi et al., 1996]. These are given by:
b(m) = Φ−1φ

13.6. Transceiver design
331
where Φ ∈Cm×m and φ ∈C(m) depend only on the trace of the successive powers
of HHH. Denoting Φij the (i, j)th entry of Φ and φi the ith entry of φ, we
explicitly have
Φij = 1
n tr
 HHH
i+j + σ2 1
n tr
 HHH
i+j−1
φi = 1
n tr
 HHH
i .
But then, from all limiting results on large dimensional random matrices
introduced in Part I, either under the analytical Stieltjes transform approach
or under the free probability approach, it is possible to approximate the entries
of Φ and φ and to obtain deterministic equivalents for b(m)
0
, . . . , b(m)
m−1, for a large
set of random matrix models for H.
13.6.1
Channel matrix model with i.i.d. entries
Typically, for H with independent entries of zero mean, variance 1/N, and ﬁnite
2 + ε order moment, for some positive ε, from Theorem 2.14
1
n tr
 HHH
i a.s.
−→1
i
i−1
X
k=0
i
k

i
k + 1

ck
as N, n →∞with n/N →c.
In place for the optimal MSE minimizing truncated polynomial decoders, we
may then use the order m detector
m−1
X
i=0
˜b(m)
i
 HHH
i HH
where ˜b(m) = (˜b(m)
0
, . . . ,˜b(m)
m−1) is deﬁned as
˜b(m) = ˜Φ−1 ˜φ
where the entries of ˜Φ and ˜φ are the almost sure limit of Φ and φ, respectively,
as N, n grow to inﬁnity with limiting ratio n/N →c.
These suboptimal weights are provided, up to a scaling factor over all ˜b(m)
i
, in
Table 13.6.
Obviously, the partial MMSE detectors derived from asymptotic results diﬀer
from the exact partial MMSE detectors. They signiﬁcantly diﬀer if N, n are
not large, therefore impacting the decoding performance. In Figure 13.7 and
Figure 13.8, we depict the simulated bit error rate performance of partial MMSE
detectors, using the weights b(m) deﬁned in this section, along with the bit error
rate performance of the suboptimal detectors with weights ˜b(m). Comparison
is made between both approaches, when N = n = 4 or N = n = 64 and H has
independent Gaussian entries of zero mean and variance 1/N. Observe that, for
these small values of N and n, the large dimensional approximations ˜b(m) of b(m)
are far from accurate. Note in particular that the approximated MMSE detector

332
13. Performance of multiple antenna systems
N = 1
˜b(1)
0
= 1
N = 2
˜b(2)
0
= σ2 −2(1 + c)
˜b(2)
1
= 1
N = 3
˜b(3)
0
= 3(1 + c2) −3σ2(1 + c) + σ4 + 4c
˜b(3)
1
= σ2 −3(1 + c)
˜b(3)
2
= 1
N = 4
˜b(4)
0
= 6σ2(1 + c2) + 9σ2c −4(1 + c3) −4σ4(1 + c) + σ6 −6c(1 + c)
˜b(4)
1
= −6(1 + c2) + 4σ2(1 + c) −σ4 −9c
˜b(4)
2
= σ2 −4(1 + c)
˜b(4)
3
= 1
Table 13.6. Deterministic equivalents of the weights for the (MSE optimal) partial MMSE
ﬁlters.
−5
0
5
10
15
20
25
30
−35
−30
−25
−20
−15
SNR [dB]
bit error rate [dB]
b1
˜b1
b2
˜b2
b3
˜b3
b4
˜b4
Figure 13.7 Bit error rate performance of partial MMSE ﬁlters, for exact weights b(m)
and approximated weights ˜b(m), H ∈CN×n has i.i.d. Gaussian entries, N = n = 4.
(m = 4) is extremely badly approximated for these small values of N and n. For
higher N and n, the decoders based on the approximated ˜b(m) perform very
accurately for small m, as observed in Figure 13.8.
13.6.2
Channel matrix model with generalized variance proﬁle
In the scenario when H ∈CN×n can be written under the very general form
H = [h1, . . . , hn], with hi = R
1
2
i xi, with Ri ∈CN×N and xi ∈CN, with i.i.d.
entries of zero mean and variance 1/n, we have from Theorem 6.13, for every m,

13.6. Transceiver design
333
−5
0
5
10
15
20
25
30
−50
−40
−30
−20
SNR [dB]
bit error rate [dB]
b2
˜b2
b4
˜b4
b8
˜b8
b64
Figure 13.8 Bit error rate performance of partial MMSE ﬁlters, for exact weights b(m)
and approximated weights ˜b(m), H ∈CN×n has i.i.d. Gaussian entries, N = n = 64.
that ˜b(m) = ˜Φ−1 ˜φ, where
˜Φi,j = Mi+j + σ2Mi+j−1
φi = Mi
where the Mi are deﬁned recursively in Theorem 6.13.
Other polynomial detector models for, e.g. downlink CDMA frequency selected
channels, have been studied in [Hachem, 2004]. This concludes this chapter on
point-to-point, or single-user, MIMO communications. In the following chapter,
we extend some of the previous results to the scenario of multiple users possibly
communicating with multiple antennas.


14
Rate performance in multiple
access and broadcast channels
In this chapter, we consider both multiple access channels (MAC), which assume
a certain number of users competing for the access to (i.e. to transmit data to) a
single resource, and broadcast channels (BC), which assume the opposite scenario
where a single transmitter multicasts data to multiple receivers.
The performance of multi-user communications can no longer be assessed from
a single capacity parameter, as was the case for point-to-point communications.
In a K-user MAC, we must evaluate what vectors (R1, . . . , RK) of rates, Ri being
the data rate transmitted by user i, are achievable, in the sense that simultaneous
reliable decoding of all data streams is possible at the receiver. Now, similar
to single-user communications, where all rates R less than the capacity C are
achievable and therefore deﬁne a rate set R = {R, R ≤C}, for the multiple access
channel, we deﬁne the multi-dimensional MAC rate region as the set RMAC of
all vectors (R1, . . . , RK) such that reliable decoding is possible at the receiver
if users 1, . . . , K transmit, respectively, at rate R1, . . . , RK. Similarly, for the
broadcast channel, we deﬁne the BC rate region RBC as the (closed) set of all
vectors (R1, . . . , RK), Ri being now the information data rate received by user
i, such that every user can reliably decode its data. We further deﬁne the rate
region boundary, either in the MAC or BC case, as the topological boundary of
the rate region. These rate regions can be deﬁned either in the quasi-static or in
the ergodic sense. That is, the rate regions may assume perfect or only statistical
channel state information at the transmitters. This is particularly convenient
in the MAC, where in general perfect channel state information of all users’
channel links is hardly accessible to each transmitter and where imperfect channel
state information does not dramatically impact the achievable rates. In contrast,
imperfect channel state information at the transmitters in the BC results in
suboptimal beamforming strategies and thus high interference at all receivers,
therefore reducing the rates achievable under perfect channel knowledge.
In the MAC, either with perfect or imperfect channel information at the
transmitter, it is known that the boundary of the rate region can be achieved if
the receiver performs MMSE decoding and successive interference cancellation
(MMSE-SIC) of the input data streams. That is, the receiver decodes the
strongest signal ﬁrst using MMSE decoding, removes the signal contribution
from the input data, then decodes the second to strongest signal, etc. until
decoding the weakest signal. As for the BC with perfect channel information

336
14. Rate performance in multiple access and broadcast channels
at the transmitter, it took researchers a long time to ﬁgure out a precoding
strategy which achieves the boundary of the BC rate region; note here that
the major processing eﬀort is shifted to the transmitter. The main results were
found almost simultaneously in the following articles [Caire and Shamai, 2003;
Viswanath et al., 2003; Viswanathan and Venkatesan, 2003; Weingarten et al.,
2006; Yu and Cioﬃ, 2004]. One of these boundary achieving codes (and for
that matter, the only one we know of so far) is the so-called dirty-paper coding
(DPC) algorithm [Costa, 1983]. The strategy of the DPC algorithm is to encode
the data sequentially at the transmission in such a way that the interference
created at every receiver and treated as noise by the latter allows for reliable
data decoding. The approach is sometimes referred to as successive encoding,
in duality reference with the successive decoding approach for the MAC. The
DPC precoder is therefore non-linear and is to this day too complex to be
implemented in practical communication systems. However, it has been shown
in the information theory literature, see, e.g., [Caire and Shamai, 2003; Peel
et al., 2005; Wiesel et al., 2008; Yoo and Goldsmith, 2006] that suboptimal linear
precoders can achieve a large portion of the BC rate region while featuring low
computational complexity. Thus, much research has recently focused on linear
precoding strategies.
It is often not convenient though to derive complete achievable rate regions,
especially for communications with a large number of users. Instead, sum rate
capacity is often considered as a relevant performance metric, which corresponds
to the maximally achievable sum R1 + . . . + RK, with (R1, . . . , RK) elements of
the achievable MAC or BC rate regions. Other metrics are sometimes used in
place of the sum rate capacity, which allow for more user fairness. In particular,
maximizing the minimum rate is a strategy that avoids leaving users with bad
channel conditions with zero rate, therefore improving fairness among users. In
this chapter, we will however only discuss sum rate maximization.
The next section is dedicated to the evaluation, through deterministic
equivalents or large dimensional system limits, of the rate region or sum rate
of MAC, BC, linearly precoded BC, etc. We assume a multi-user communication
wireless network composed of K users, either transmitters (in the MAC) or
receivers (in the BC), communicating with a multiple antenna access point or
base station. User k, k ∈{1, . . . , K} is equipped with nk antennas, while the
access point is equipped with N antennas. Since users and base stations are
either transmitting or receiving data, we no longer use the notations nt and nr
to avoid confusion.
We ﬁrst consider the case of linearly precoded broadcast channels.
14.1
Broadcast channels with linear precoders
In this section, we consider the downlink communication of the N-antenna base
station towards K single antenna receivers, i.e. nk = 1 for all k ∈{1, . . . , K},

14.1. Broadcast channels with linear precoders
337
which is a common assumption in current broadcast channels, although studies
regarding multiple antenna receivers have also been addressed, see, e.g.,
[Christensen et al., 2008]. We further assume N ≥K. In some situations, we may
need to further restrict this condition to cN ≜N/K > 1 + ε for some ε > 0 (as we
will need to grow N and K large with speciﬁc rates). We denote by hH
k ∈C1×N
the MISO channel from the base station to user k. At time t, denoting s(t)
k
the
signal intended to user k of zero mean and unit variance, σw(t)
k
an additive white
Gaussian noise with zero mean and variance σ2, and y(t)
k
the signal received by
user k, the transmission model reads:
y(t)
k
= hH
k
K
X
j=1
gjs(t)
j
+ σw(t)
k
where gj ∈CN
denotes the linear vector precoder, also referred to as
beamforming vector or beamformer, of user j. Gathering the transmit data into
a vector s(t) = (s(t)
1 , . . . , s(t)
K )T ∈CK, the additive thermal noise into a vector
w(t) = (w(t)
1 , . . . , w(t)
K )T ∈CK, the data received at the antenna array into a
vector y(t) = (y(t)
1 , . . . , y(t)
K )T ∈CK, the beamforming vectors into a matrix G =
[g1, . . . , gK] ∈CN×K, and the channel vectors into a matrix H = [h1, . . . , hK]H ∈
CK×N, we have the compact transmission model
y(t) = HGs(t) + σw(t)
where G must satisfy the power constraint
tr(E[Gs(t)s(t)HGH]) = tr(GGH) ≤P
(14.1)
assuming E[s(t)s(t)H] = IK, for some available transmit power P at the base
station.
When necessary, we will denote z(t) ∈CN the vector
z(t) ≜Gs(t)
of data transmitted from the antenna array of the base station at time t.
Due to its practical and analytical simplicity, this linear precoding model is
very attractive. Most research in linear precoders has focused to this day both
on analyzing the performance of some ad-hoc precoders and on determining the
optimal linear precoders. Optimality is often taken with respect to sum rate
maximization (or sometimes, for fairness reasons, with respect to maximization
of the minimum user rate). In general, though, the rate maximizing linear
precoder has no explicit form. Several iterative algorithms have been proposed
in [Christensen et al., 2008; Shi et al., 2008] to come up with a sum rate optimal
precoder, but no global convergence has yet been proved. Still, these iterative
algorithms have a high computational complexity which motivates the use of
further suboptimal linear transmit ﬁlters, by imposing more structure into the
ﬁlter design.

338
14. Rate performance in multiple access and broadcast channels
In order to maximize the achievable sum rate, a ﬁrst straightforward technique
is to precode the transmit data by the inverse, or Moore–Penrose pseudo-inverse,
of the channel matrix H. This scheme is usually referred to as channel inversion
(CI) or zero-forcing (ZF) precoding [Caire and Shamai, 2003]. That is, the ZF
precoder Gzf reads:
Gzf =
ξ
√
N
HH(HHH)−1
where ξ is set to fulﬁll some transmit power constraint (14.1).
The
authors
in
[Hochwald
and
Vishwanath,
2002;
Viswanathan
and
Venkatesan, 2003] carry out a large system analysis assuming that the number of
transmit antennas N at the base station as well as the number of users K grow
large while their ratio cN = N/K remains bounded. It is shown in [Hochwald
and Vishwanath, 2002] that, for cN > 1 + ε, uniformly on N, the achievable
sum rate for ZF precoding has a multiplexing gain of K, which is identical to
the optimal DPC-achieving multiplexing gain. The work in [Peel et al., 2005]
extends the analysis in [Hochwald and Vishwanath, 2002] to the case K = N
and shows that the sum rate of ZF saturates as K grows large. The authors in
[Peel et al., 2005] counter this problem by introducing a regularization term α
into the inverse of the channel matrix. The precoder obtained is referred to as
regularized zero-forcing (RZF) or regularized channel inversion, is denoted Grzf,
and is given explicitly by
Grzf =
ξ
√
N
HH(HHH + αIN)−1
(14.2)
with ξ deﬁned again to satisfy some transmit power constraint.
Under the assumption of large K and for any unitarily invariant channel
distribution, [Peel et al., 2005] derives the regularization term α⋆that maximizes
the signal-to-interference plus noise ratio. It has been observed that the optimal
RZF precoder proposed in [Peel et al., 2005] is very similar to the transmit ﬁlters
derived under the minimum mean square error (MMSE) criterion at every user,
i.e. with α = σ2 [Joham et al., 2002], and become identical in the large K limit.
Based on the tools developed in Part I, we provide in this section deterministic
approximations of the achievable sum rate under ZF and RZF precoding, and
determine the optimal α⋆parameter in terms of the sum rate as well as other
interesting optimization measures discussed below. We also give an overview
of the potential applications of random matrix theory for linearly precoded
broadcast channels, under the most general channel conditions. In particular,
consider that:
• the signal transmitted at the base station antenna array is correlated. That
is, for every user k, k ∈{1, . . . , K}, hk can be written under the form
hk = T
1
2
k xk

14.1. Broadcast channels with linear precoders
339
where xk ∈CN has i.i.d. Gaussian entries of zero mean and variance 1/N, and
T
1
2
k is a non-negative deﬁnite square root of the transmit correlation matrix
Tk ∈CN×N with respect to user k;
• the diﬀerent users are assumed to have individual long-term path-losses
r1, . . . , rK. This allows us to further model hk as
hk = √rkT
1
2
k xk.
The letter ‘r’ is chosen to indicate the channel fading seen at the receiver;
• the channel state information (CSI) at the transmitter side is assumed
imperfect. That is, H is not completely known at the transmitter. Only
an estimate ˆH is supposed to be available at the transmitter. We model
ˆH = [ˆh1, . . . , ˆhK]H with
ˆhj =
q
1 −τ 2
j hj + τj ˜hj
with τj some parameter roughly indicating the accuracy of the jth channel
estimate, and ˜H = [˜h1, . . . , ˜hK]H as being the random matrix of channel errors
with properties to be deﬁned later. Note that a similar imperfect channel
state analysis framework for the single-user MIMO case has been introduced
in [Hassibi and Hochwald, 2003].
These channel conditions are rather realistic and include as particular cases the
initial results found in the aforementioned literature contributions and others,
e.g., [Ding et al., 2007; Hochwald and Vishwanath, 2002; Jindal, 2006; Peel
et al., 2005]. An illustration of the linearly precoded MISO (or vector) broadcast
channel is provided in Figure 14.1. The following development is heavily based
on the work [Wagner et al., 2011].
14.1.1
System model
Consider the transmission model described above. For simplicity here, we will
assume that T1 = . . . = TK ≜T, i.e. the correlation at the base station of all
users’ channel vectors is identical. Field measurements [Kaltenberger et al., 2009]
suggest that this assumption is too strong and not fully realistic. As will be
further discussed in subsequent sections, signal correlation at the transmitter
does not only arise from close antenna spacing, but also from the diﬀerent
solid angles of signal departure. It could be argued though that the scenario
where all users experience equal transmit covariance matrices represents a worst
case scenario, as it reduces multi-user diversity. If not fully realistic, the current
assumption on T is therefore still an interesting hypothesis.
Similar to [Chuah et al., 2002; Shi et al., 2008; Tulino and Verd´u, 2005], we
therefore denote
H = R
1
2 XT
1
2

340
14. Rate performance in multiple access and broadcast channels
Figure 14.1 Linearly precoded vector broadcast channel, composed of K users and a
base station. Each user is mono-antenna, and the base station is equipped with N
antennas. The channel between the base station and user k is hk = √rkT
1
2
k xk, an
approximation ˆhk of which is available at the base station. The linear precoder is
denoted G and is based on the channel matrix estimate ˆH = [ˆh1, . . . , ˆhK]H.
where X ∈CK×N has i.i.d. entries of zero mean and variance 1/N, T ∈CN×N is
the non-negative deﬁnite correlation matrix at the transmitter with eigenvalues
t1, . . . , tN, ordered as t1 ≤. . . ≤tN, and R = diag(r1, . . . , rK), with entries
ordered as r1 ≤. . . ≤rK, contains the user channel gains, i.e. the inverse user
path losses. We further assume that there exist a−, a+ > 0 such that
a−< t1 ≤tN < a+,
(14.3)
a−< r1 ≤rK < a+
(14.4)
uniformly on N and K. That is, (14.3) assumes that the correlation between
transmit antennas does not increase as the number of antennas increases. For
practical systems, this is equivalent to requesting that neighboring antennas are
spaced suﬃciently apart. Equation (14.4) assumes that the users are not too close
to the base station but also not too far away. This is a realistic assumption, as
distant users would be served by neighboring base stations. Those requirements,
although rather realistic, are obviously not mandatory for practical systems.
However, they are required for the mathematical derivations of the present study.
Further note that the Kronecker model assumes that the receivers are spaced
suﬃciently apart and are therefore spatially uncorrelated, an assumption which
could also be argued against in some speciﬁc scenarios.
Besides, we suppose that only ˆH ∈CK×N, an imperfect estimate of the true
channel matrix H, is available at the transmitter. The channel gain matrix R as
well as the transmit correlation T are assumed to be slowly varying compared
to the fast fading component X and are assumed to be perfectly known to the
transmitter. We model ˆH as
ˆH = R
1
2 ˆXT
1
2
(14.5)

14.1. Broadcast channels with linear precoders
341
with imperfect short-term statistics ˆX = [ˆx1, . . . , ˆxK]H given by:
ˆxk =
q
1 −τ 2
kxk + τkqk
where we deﬁned Q = [q1, . . . , qK]H ∈CK×N the matrix of channel estimation
errors containing i.i.d. entries of zero mean and variance 1/N, and τk ∈[0, 1]
the distortion in the channel estimate hk of user k. We assume that the τk are
perfectly known at the transmitter. However, as shown in [Dabbagh and Love,
2008], an approximated knowledge of τk will not lead to a severe performance
degradation of the system. Furthermore, we suppose that X and Q are mutually
independent as well as independent of the symbol vector s(t) and noise vector
w(t). A similar model for the case of imperfect channel state information at the
transmitter has been used in, e.g., [Dabbagh and Love, 2008; Hutter et al., 2000;
Yoo and Goldsmith, 2006].
We then deﬁne the average SNR ρ as ρ ≜P/σ2. The received symbol y(t)
k
of
user k at time t is given by:
y(t)
k
= hH
k gks(t)
k +
X
1≤i≤K
i̸=k
hH
k gis(t)
i
+ σw(t)
k
where we recall that hH
k ∈CN denotes the kth row of H.
The SINR γk of user k reads:
γk =
|hH
k gk|2
PN
j=1
j̸=k
|hH
k gj|2 + σ2
N
.
(14.6)
The system sum rate Rsum is deﬁned as
Rsum =
K
X
k=1
log2 (1 + γk)
evaluated in bits/s/Hz.
The objective of the next section is to provide a deterministic equivalent for
the γk under RZF precoding.
14.1.2
Deterministic equivalent of the SINR
A deterministic equivalent for the SINR under RZF precoding is given as follows.
Theorem 14.1. Let γrzf,k be the SINR of user k under RZF precoding, i.e. γrzf,k
is given by (14.6), with G given by (14.6) and ξ such that the power constraint
(14.1) is fulﬁlled. Then
γrzf,k −¯γrzf,k
a.s.
−→0

342
14. Rate performance in multiple access and broadcast channels
as N, K →∞, such that 1 ≤N/K ≤C for some C > 1, and where ¯γrzf,k is given
by:
¯γrzf,k =
r2
k(1 −τ 2
k) ¯m2
rk ¯Υ (1 −τ 2
k [1 −(1 + rk ¯m)2]) +
¯Ψ
ρ (1 + rk ¯m)2
with
¯m = 1
N tr T (αIN + φT)−1 ,
(14.7)
¯Ψ =
1
N tr R (IK + ¯mR)−2 1
N tr T (αIN + φT)−2
1 −1
N tr R2 (IK + ¯mR)−2 1
N tr T2 (αIN + φT)−2
(14.8)
¯Υ =
1
N tr R (IK + ¯mR)−2 1
N tr T2 (αIN + φT)−2
1 −1
N tr R2 (IK + ¯mR)−2 1
N tr T2 (αIN + φT)−2
(14.9)
where φ is the unique real positive solution of
φ = 1
N tr R

IK + R 1
N tr T (αIN + φT)−1
−1
.
(14.10)
Moreover, deﬁne φ0 = 1/α, and for k ≥1
φk = 1
N tr R

IK + R 1
K tr T (αIN + φk−1T)−1
−1
.
Then φ = limk→∞φk.
We subsequently prove the above result by providing deterministic equivalents
for ξ, then for the power of the signal of interest (numerator of the SINR) and
ﬁnally for the interference power (denominator of the SINR). Throughout this
proof, we will mainly use Theorem 6.12.
For convenience, we will constantly use the notation mBN,QN (z) for a random
matrix BN ∈CN×N and a deterministic matrix QN ∈CN×N to represent
mBN,QN (z) ≜1
N tr QN (BN −zIN)−1 .
Also, we will denote mBN (z) ≜mBN,IN (z). The character m is chosen here
because all mBN,QN (z) considered in the following will turn out to be Stieltjes
transforms of ﬁnite measures on R+.
From the sum power constraint (14.1), we obtain
ξ2 =
P
1
N tr

ˆHH ˆH

ˆHH ˆH + αIN
−2 =
P
m ˆHH ˆH(−α) −αm′
ˆHH ˆH(−α) ≜P
Ψ
where the last equation follows from the decomposition
ˆHH ˆH( ˆHH ˆH + αIM)−2 = ( ˆHH ˆH + αIN)−1 −α( ˆHH ˆH + αIN)−2

14.1. Broadcast channels with linear precoders
343
and we deﬁne
Ψ ≜m ˆHH ˆH(−α) −αm′
ˆHH ˆH(−α).
The received symbol y(t)
k
of user k at time t is given by:
y(t)
k
= ξhH
k ˆ
Wˆhks(t)
k + ξ
K
X
i=1
i̸=k
hH
k ˆ
Wˆhis(t)
i
+ σw(t)
k
where ˆ
W ≜( ˆHH ˆH + αIN)−1 and ˆhH
k is the kth row of ˆH. The SINR γrzf,k of
user k can be written under the form
γrzf,k =
|hH
k ˆ
Wˆhk|2
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk + 1
ρΨ
where ˆHH
(k) = [ˆh1, . . . , ˆhk−1, ˆhk+1, . . . , ˆhK] ∈CN×(K−1). Note that this SINR
expression implicitly assumes that the receiver is perfectly aware of both the
vector channel hk and the phase of hH
k ˆ
Wˆhk which rotates the transmitted signal.
This requires to assume the existence of a dedicated training sequence for the
receivers.
We will proceed by successively deriving deterministic equivalent expressions
for Ψ, for the signal power |hH
k ˆ
Wˆhk|2 and for the power of the interference
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk.
We ﬁrst consider the power regularization term Ψ. From Theorem 6.12,
m ˆHH ˆH(−α) is close to ¯m ˆHH ˆH(−α) given as:
¯m ˆHH ˆH(−α) = 1
N tr (αIN + φT)−1
(14.11)
where φ is deﬁned in (14.10).
Remark now, and for further purposes, that ¯m in (14.7) is a deterministic
equivalent of the Stieltjes transform mA(z) of
A ≜ˆXHR ˆX + αT−1
evaluated at z = 0, i.e.
mA(0) −¯m
a.s.
−→0.
(14.12)
Note that it is uncommon to evaluate Stieltjes transforms at z = 0. This is
valid here, since we assumed in (14.3) that 1/tN > 1/a+ and then the smallest
eigenvalue of A is strictly greater than 1/(2a+) > 0, uniformly on N. Therefore,
mA(0) is well deﬁned.
Since the deterministic equivalent of the Stieltjes transform of ˆHH ˆH is itself the
Stieltjes transform of a probability distribution, it is an analytic function outside
the real positive half-line. The dominated convergence theorem, Theorem 6.3,
ensures that the derivative ¯m′
ˆHH ˆH(z) of ¯m ˆHH ˆH(z) is a deterministic equivalent of

344
14. Rate performance in multiple access and broadcast channels
m′
ˆHH ˆH(z), i.e.
m′
ˆHH ˆH(z) −¯m′
ˆHH ˆH(z)
a.s.
−→0.
After diﬀerentiation of (14.11) and standard algebraic manipulations, we
obtain
Ψ −¯Ψ
a.s.
−→0
as N, K →∞, where
¯Ψ ≜¯m ˆHH ˆH(−α) −α ¯m′
ˆHH ˆH(−α)
which is explicitly given by (14.8).
We now derive a deterministic equivalent for the signal power.
Applying Lemma 6.2 to ˆhH
k ˆ
W = ˆhH
k ( ˆHH
(k) ˆH(k) + αIN + ˆhkˆhH
k )−1, we have:
ˆhH
k ˆ
Whk =
ˆhH
k

ˆHH
(k) ˆH(k) + αIN
−1
hk
1 + ˆhH
k

ˆHH
(k) ˆH(k) + αIN
−1 ˆhk
.
Together with ˆhH
k = √rk
p
1 −τ 2
kxH
k + τkqH
k

T
1
2 , we obtain
ˆhH
k ˆ
Whk =
p
1 −τ 2
krkxH
k A−1
(k)xk
1 + rkˆxH
k A−1
(k)ˆxk
+
τkrkqH
k A−1
(k)xk
1 + rkˆxH
k A−1
(k)ˆxk
with A(k) = ˆXH
(k)R(k) ˆX(k) + αT−1 for ˆXH
(k) = [ˆx1, . . . , ˆxk−1, ˆxk+1, . . . , ˆxK], ˆxn
being the nth row of ˆX, and R(k) = diag(r1, . . . , rk−1, rk+1 . . . rK). Since both
ˆxk and xk have i.i.d. entries of variance 1/N and are independent of A(k), while
A(k) has uniformly bounded spectral norm since t1 > a−, we invoke the trace
lemma, Theorem 3.4, and obtain
xH
k A−1
(k)xk −1
N tr A−1
(k)
a.s.
−→0
ˆxH
k A−1
(k)ˆxk −1
N tr A−1
(k)
a.s.
−→0.
Similarly, as qk and xk are independent, from Theorem 3.7
qH
k A−1
(k)xk
a.s.
−→0.
Consequently, since (1 + rkˆxH
k A−1
(k)ˆxk) is bounded away from zero, we obtain
ˆhH
k ˆ
Whk −
q
1 −τ 2
k
rk 1
N tr A−1
(k)
1 + rk 1
N tr A−1
(k)
a.s.
−→0.
(14.13)

14.1. Broadcast channels with linear precoders
345
To move from A(k) to A in the previous equation, we will invoke the rank-1
perturbation lemma, Theorem 3.9. First, rewrite A−1
(k) as
A−1
(k) =

ˆXH
(k)R(k) ˆX(k) + αT−1−1
=

ˆXH
(k)R(k) ˆX(k) + αT−1 −α 1
2a+
IN

+ α 1
2a+
IN
−1
.
Since 1/tN > 1/a+ uniformly on N, notice that the matrix in brackets on the
right-hand side is still non-negative deﬁnite. Thus we can apply the rank-1
perturbation lemma to this matrix and the scalar α
1
2a+ > 0, and we obtain
1
N tr

ˆXH
(k)R(k) ˆX(k) + αT−1−1
−1
N tr

ˆXH
(k)R(k) ˆX(k) + rkˆxkˆxH
k + αT−1−1
a.s.
−→0.
Therefore
1
N tr A−1
(k) −1
N tr A−1 →0
(14.14)
where we remind that A = ˆXHR ˆX + αT−1.
Thus, (14.14) and (14.12) together imply
1
N tr A−1
(k) −¯m
a.s.
−→0.
(14.15)
Finally, (14.13) takes the form
ˆhH
k ˆ
Whk −
q
1 −τ 2
k
rk ¯m
1 + rk ¯m
a.s.
−→0.
This establishes a deterministic equivalent for the square root of the signal
power, which directly gives a deterministic equivalent for the signal power by
taking the square of each term in the diﬀerence. Alternatively, similar to the
more elaborated proofs of, e.g. Theorem 6.1, we could have worked directly on
successive order moments of the diﬀerence between the signal power and its
deterministic equivalent in order to apply the Markov inequality and the Borel–
Cantelli lemma. A convenient result in this approach, that can come in handy
for similar calculus, is given as follows [Hoydis et al., 2010, Lemma 3].
Theorem 14.2. Let p be an integer, x1, x2, . . ., with xN ∈CN, be a sequence
of random vectors with i.i.d. entries of zero mean, variance 1/N, and 12(p −
1) moment of order O(1/N 6(p−1)), and let A1, A2, . . ., with AN ∈CN×N, be
matrices independent of xN. Then there exists Cp, a constant function of p only
such that
E
"(xH
NAxN)p −
 1
N tr AN
p
3#
≤
1
N
3
2 Cp∥AN∥3p.

346
14. Rate performance in multiple access and broadcast channels
This implies in particular that, if ∥AN∥is uniformly bounded
(xH
NAxN)p −
 1
N tr AN
p
a.s.
−→0.
We ﬁnally address the more involved question of the interference power.
With ˆ
W = T−1
2 A−1T−1
2 , the interference power can be written as
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk = rkxH
k A−1 ˆXH
(k)R(k) ˆX(k)A−1xk.
(14.16)
Denote c0 = (1 −τ 2
k)rk, c1 = τ 2
krk and c2 = τk
p
1 −τ 2
krk, then:
A = A(k) + c0xkxH
k + c1qkqH
k + c2xkqH
k + c2qkxH
k .
In order to eliminate the dependence between xk and A in (14.16), we rewrite
(14.16) as
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk
= rkxH
k A−1
(k) ˆXH
(k)R(k) ˆX(k)A−1xk + rkxH
k
h
A−1 −A−1
(k)
i
ˆXH
(k)R(k) ˆX(k)A−1xk.
(14.17)
Applying the resolvent identity, Lemma 6.1, to the term in brackets in (14.17)
and together with A−1
(k) ˆXH
(k)R(k) ˆX(k) = IN −αA−1
(k)T−1, (14.17) takes the form
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk = rkxH
k A−1xk −αrkxH
k A−1
(k)T−1A−1xk
−c0rkxH
k A−1xk

xH
k A−1xk −αxH
k A−1
(k)T−1A−1xk

−c1rkxH
k A−1qk

qH
k A−1xk −αqH
k A−1
(k)T−1A−1xk

−c2rkxH
k A−1xk

qH
k A−1xk −αqH
k A−1
(k)T−1A−1xk

−c2rkxH
k A−1qk

xH
k A−1xk −αxH
k A−1
(k)T−1A−1xk

.
(14.18)
To ﬁnd a deterministic equivalent for all of all the terms in (14.18), we need
the following lemma, which is an extension of Theorem 3.4.
Lemma 14.1. Let UN, VN ∈CN×N be invertible and of uniformly bounded
spectral norm. Let xN, yN ∈CN have i.i.d. complex entries of zero mean,
variance 1/N, and ﬁnite eighth order moment and be mutually independent as
well as independent of UN, VN. Deﬁne c0, c1, c2 > 0 such that c0c1 −c2
2 ≥0 and
let u ≜1
N tr V−1
N and u′ ≜1
N tr UNV−1
N . Then we have, as N →∞
xH
NUN
 VN + c0xNxH
N + c1yNyH
N + c2xNyH
N + c2yNxH
N
−1 xN
−
u′(1 + c1u)
(c0c1 −c2
2)u2 + (c0 + c1)u + 1
a.s.
−→0.

14.1. Broadcast channels with linear precoders
347
Furthermore
xH
NUN
 VN + c0xNxH
N + c1yNyH
N + c2xNyH
N + c2yNxH
N
−1 yN
−
−c2uu′
(c0c1 −c2
2)u2 + (c0 + c1)u + 1
a.s.
−→0.
The proof of Lemma 14.1 is just a matter of algebraic considerations, fully
detailed in [Wagner et al., 2011].
Denote u ≜1
N tr A−1
(k) and u′ ≜1
N tr T−1A−2
(k). Note that, in the present
scenario, c0c1 = c2
2 and thus (c0c1 −c2
2)u2 + (c0 + c1)u + 1 reduces to 1 + rku.
Applying Lemma 14.1 to each of the terms in (14.18), we obtain
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk −
rk(1 + c1u)(u −αu′)
1 + rku
−rkc0u(u −αu′)
(1 + rku)2

a.s.
−→0
(14.19)
where the ﬁrst term in brackets stems from the ﬁrst line in (14.18) and the second
term in brackets arises from the last four lines in (14.18). Replacing c0 and c1 by
(1 −τ 2
k)rk and τ 2
krk, respectively, and after some algebraic manipulation, (14.19)
takes the form
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk −rk(u −αu′)

1 −τ 2
k
 1 −(1 + rku)2
(1 + rku)2
a.s.
−→0.
Since a rank-1 perturbation has no impact on 1
N tr A−1 for N →∞, we also have
u −mA(0)
a.s.
−→0
u′ −mA2,T−1(0)
a.s.
−→0.
Note again that mA2,T−1(0) is well deﬁned since t1 > a−is bounded away from
zero. Denote
Υ = mA(0) −αm′
A,T−1(0)
and observe that
mA2,T−1(0) = m′
AT−1(0).
Furthermore, we have mA2,T−1(0) −¯m′
A,T−1(0)
a.s.
−→0, where
¯m′
A,T−1(0) is
obtained from Theorem 6.12. Similar to the derivations of ¯Ψ, we then obtain
Υ −¯Υ
a.s.
−→0, as N, K grow large with ¯Υ given by:
¯Υ = ¯mA(0) −α ¯m′
A,T−1(0)
whose explicit form is given by (14.9). Substituting u and u′ by their respective
deterministic equivalent expressions ¯m and ¯m′
AT−1(0), respectively, we obtain
hH
k ˆ
W ˆHH
(k) ˆH(k) ˆ
Whk −rk ¯Υ(1 −τ 2
k[1 −(1 + rk ¯m)2])
(1 + rk ¯m)2
a.s.
−→0.

348
14. Rate performance in multiple access and broadcast channels
This completes the ﬁrst part of the proof. Now, the convergence of the sequence
φ1, φ2, . . . to φ is a direct consequence of a trivial extension of the last statement
in Theorem 6.12.
The next question we address is the precise characterization of the sum rate
maximizing α parameter, call it α⋆. Indeed, contrary to the MMSE precoding
approach, that would consist in setting α = σ2, due to inter-user interference,
it is not clear which α parameter is optimal in the sum rate viewpoint. Since
this is a diﬃcult problem, instead we determine here ¯α⋆, the parameter α which
maximizes the deterministic equivalent of the sum rate. This parameter can be
shown to asymptotically well approximate the optimal sum rate maximizing α.
As it turns out, though, ¯α⋆cannot be expressed explicitly and is the only solution
to an implicit equation of the system parameters. However, in more symmetric
scenarios, ¯α⋆takes an explicit form. This is discussed subsequently.
14.1.3
Optimal regularized zero-forcing precoding
Deﬁne ¯Rrzf to be
¯Rrzf =
K
X
k=1
log2 (1 + ¯γrzf,k)
where the parameter α, implicitly present in the term ¯γrzf,k is chosen to be the
positive real that maximizes ¯Rrzf, i.e. α = ¯α⋆, with ¯α⋆deﬁned as
¯α⋆= arg max
α>0
¯Rrzf.
(14.20)
Under the channel model with imperfect CSIT (14.5), ¯α⋆is a positive solution
of the implicit equation
K
X
k=1
∂¯γrzf,k
∂α
1
1 + ¯γrzf,k
= 0.
The implicit equation above is not convex in α so that the solution needs to
be computed via a one-dimensional line search. The RZF precoder with optimal
regularization parameter ¯α⋆will be called RZF-O in the remainder of this section.
For homogeneous networks, i.e. R = IK, the user channels hk are statistically
equivalent and it is reasonable to assume that the distortions τ 2
k of the CSIT ˆhk
are identical for all users, i.e. τ1 = . . . = τK ≜τ. Under the additional assumption
of uncorrelated transmit antennas, i.e. T = IN, the solution to (14.20) has a
closed-form and leads to the asymptotically optimal precoder given by:
¯α⋆=
1 + τ 2ρ
1 −τ 2
 K
N
1
ρ.
To prove this, observe that ¯Ψ = ¯Υ and
¯m = ¯m ˆXH ˆX(−α) is the Stieltjes
transform of the Mar˘cenko–Pastur law at z = −α. The SINR and its derivative
then take an explicit form, whose extrema can be computed.

14.1. Broadcast channels with linear precoders
349
Note that, for τ > 0 and at asymptotically high SNR, the above regularization
term ¯α⋆satisﬁes
lim
ρ→∞¯α⋆=
τ 2
1 −τ 2
K
N .
Thus, for asymptotically high SNR, RZF-O does not converge to the ZF
precoding, in the sense that ¯α⋆does not converge to zero. This is to be opposed
to the case when perfect channel state information at the transmitter is assumed
where α⋆tends to zero.
Under this scenario and with α = ¯α⋆, the SINR ¯γrzf,k is now independent of
the index k and takes now the simpliﬁed form
¯γrzf,k = ω
2 ρ
N
K −1

+ χ
2 −1
2
where ω and χ are given by:
ω = 1 −τ 2
1 + τ 2ρ
χ =
sN
K −1
2
ω2ρ2 + 2
N
K + 1

ωρ + 1.
Further comments on the above are provided in [Wagner et al., 2011]. We
now move to ZF precoding, whose performance can be studied by having the
regularization parameter of RZF tend to zero.
14.1.4
Zero-forcing precoding
For α = 0, the RZF precoding matrix reduces to the ZF precoding matrix Gzf,
which we recall is deﬁned as
Gzf =
ξ
√
N
ˆHH 
ˆH ˆHH−1
where ξ is a scaling factor to fulﬁll the power constraint (14.1).
To derive a deterministic equivalent of the SINR of ZF precoding, we cannot
assume N = K and apply the same techniques as for RZF, since by removing
a row of ˆH, the matrix ˆHH ˆH becomes singular. Therefore, we adopt a diﬀerent
strategy and derive a deterministic equivalent ¯γzf,k for the SINR of ZF for user k
under the additional assumption that, uniformly on K, N, N
K > 1 + ε, for some
ε > 0. The approximated SINR ¯γzf,k is then given by:
¯γzf,k = lim
α→0 ¯γrzf,k.
The result is summarized in the following theorem.

350
14. Rate performance in multiple access and broadcast channels
Theorem 14.3. Let N/K > 1 + ε for some ε > 0 and ¯γzf,k be the SINR of user
k for the ZF precoder. Then
γzf,k −¯γzf,k
a.s.
−→0
as N, K →∞, with uniformly bounded ratio, where ¯γzf,k is given by:
¯γzf,k =
1 −τ 2
k
rkτ 2
k ¯Υ +
¯Ψ
ρ
(14.21)
with
¯Ψ = 1
φ
1
N tr R−1
¯Υ =
ψ
N
K φ2 −ψ
1
K tr R−1
ψ = 1
N tr T2

IN + K
N
1
φT
−2
(14.22)
where φ is the unique solution of
φ = 1
N tr T

IN + K
N
1
φT
−1
.
(14.23)
Moreover, φ = limk→∞φk, where φ0 = 1 and, for k ≥1
φk = 1
N tr T

IN + K
N
1
φk−1
T
−1
.
Note, that by Jensen’s inequality ψ ≥φ2 with equality if T = IN. In the
simpler case when T = IN, R = IK and τ1 = . . . = τK ≜τ, we have the following
deterministic equivalent for the SINR under ZF precoding.
¯γzf,k = 1 −τ 2
τ 2 + 1
ρ
N
K −1

.
We hereafter prove Theorem 14.3.
Recall the terms in the SINR of RZF that depend on α, i.e. mA, Ψ, and Υ
mA = 1
N tr TF
Ψ = 1
N tr
 F −αF2
Υ = 1
N tr T
 F −αF2
where we introduced the notation
F =

ˆHH ˆH + αIN
−1
.

14.1. Broadcast channels with linear precoders
351
In order to take the limit α →0 of Ψ and Υ, we apply the resolvent lemma,
Lemma 6.1, to F and we obtain
F −αF2 = ˆHH 
ˆH ˆHH + αIN
−2 ˆH.
Since ˆH ˆHH is non-singular with probability one for all large N, K, we can take
the limit α →0 of Ψ and Υ in the RZF case, for all such ˆH ˆHH and large enough
N, K. This redeﬁnes Ψ and Υ as
Ψ = 1
N tr

ˆH ˆHH−1
Υ = 1
N tr ˆHT ˆHH 
ˆH ˆHH−2
.
Note that it is necessary to assume that N/K > 1 + ε uniformly for some ε > 0
to ensure that the maximum eigenvalue of matrix ( ˆH ˆHH)−1 is uniformly bounded
for all large N. Since mA grows with α decreasing to zero as O(1/α), we have:
γzf,k = lim
α→0 γrzf,k =
1 −τ 2
k
rkτ 2
kΥ + Ψ
ρ
.
(14.24)
Now we derive deterministic equivalents ¯Ψ and ¯Υ for Ψ and Υ, respectively.
Theorem 6.12 can be directly applied to ﬁnd ¯Ψ as
¯Ψ = 1
N
1
φ tr R−1
where φ is deﬁned in (14.23).
To determine ¯Υ, note that we can diagonalize T as T = U diag(t1, . . . , tN)UH,
where U is a unitary matrix, and still have i.i.d. elements in the kth column ˆx′
k
of ˆXU. Denoting C = ˆH ˆHH, C(k) = ˆH[k] ˆHH
[k] −tkR
1
2 ˆx′
kˆx′H
k R
1
2 and applying the
usual matrix inversion lemma twice, we can write
Υ = 1
N
N
X
k=1
t2
k
ˆx′H
k R
1
2 C−2
(k)R
1
2 ˆx′
k
(1 + tkˆx′H
k R
1
2 C−1
(k)R
1
2 ˆx′
k)2 .
Notice here that C−1
(k) does not have uniformly bounded spectral norm. The
trace lemma, Theorem 3.4, can therefore not be applied straightforwardly here.
However, since the ratio N/K is taken uniformly greater than 1 + ε for some
ε > 0, C−1
(k) has almost surely bounded spectral norm for all large N (this unfolds
from Theorem 7.1 and from standard matrix norm inequalities, reminding us that
0 < a < tk, rk < b < ∞). This is in fact suﬃcient for a similar trace lemma to
hold.
Lemma 14.2. Let A1, A2, . . ., with AN ∈CN×N, be a series of random matrices
generated by the probability space (Ω, F, P) such that, for ω ∈A ⊂Ω, with
P(A) = 1, ∥AN(ω)∥< K(ω) < ∞, uniformly on N. Let x1, x2, . . . be random
vectors of i.i.d. entries such that the entries of xN ∈CN have zero mean,

352
14. Rate performance in multiple access and broadcast channels
variance 1/N, and eighth order moment of order O(1/N 4), independent of AN.
Then
xH
NANxN −1
N tr AN
a.s.
−→0
as N →∞.
Proof. The proof unfolds from a direct application of the Tonelli theorem,
Theorem 3.16. Denoting (X, X, PX) the probability space that generates the
series x1, x2, . . ., we have that, for every ω ∈A (i.e. for every realization
A1(ω), A2(ω), . . . with ω ∈A), the trace lemma, Theorem 3.4, holds true. Now,
from Theorem 3.16, the space B of couples (x, ω) ∈X × Ωfor which the trace
lemma holds satisﬁes
Z
X×Ω
1B(x, ω)dPX×Ω(x, ω) =
Z
Ω
Z
X
1B(x, ω)dPX(x)dP(ω).
If ω ∈A, then 1B(x, ω) = 1 on a subset of X of probability one. The inner integral
therefore equals one whenever ω ∈A. As for the outer integral, since P(A) = 1,
it also equals one, and the result is proved.
Moreover, the rank-1 perturbation lemma, Theorem 3.9, no longer ensures
that the normalized trace of C−1
(k) and C−1 are asymptotically equal. In fact,
following the same line of argument as above, we also have a generalized rank-1
perturbation lemma, which now holds only almost surely.
Lemma
14.3. Let A1, A2, . . ., with AN ∈CN×N, be deterministic with
uniformly bounded spectral norm and B1, B2, . . ., with BN ∈CN×N, be random
Hermitian, with eigenvalues λBN
1
≤. . . ≤λBN
N
such that, with probability one,
there exist ε > 0 for which λBN
1
> ε for all large N. Then for v ∈CN
1
N tr ANB−1
N −1
N tr AN(BN + vvH)−1 a.s.
−→0
as N →∞, where B−1
N and (BN + vvH)−1 exist with probability one.
Proof. The proof unfolds similarly as above, with some particular care to be
taken. Call B the set of probability one in question and take ω ∈B. The smallest
eigenvalue of BN(ω) is greater than ε(ω) for all large N. Therefore, for such an
N, ﬁrst of all BN(ω) and BN(ω) + vvH are invertible and, taking z = −ε(ω)/2,
we can write
1
N tr ANBN(ω)−1 = 1
N tr AN

BN(ω) −ε(ω)
2
IN

+ ε(ω)
2
IN
−1

14.1. Broadcast channels with linear precoders
353
and
1
N tr AN(BN(ω) + vvH)−1
= 1
N tr AN

BN(ω) + vvH −ε(ω)
2
IN

+ ε(ω)
2
IN
−1
.
With these notations, BN(ω) −ε(ω)
2 IN and BN(ω) + vvH −ε(ω)
2 IN are still non-
negative deﬁnite for all N. Therefore, the rank-1 perturbation lemma, Theorem
3.9, can be applied for this ω. But then, from the Tonelli theorem, in the space
that generates (B1, B2, . . .), the subspace where the rank-1 perturbation lemma
applies has probability one, which is what needed to be proved.
Applying the above trace lemma and rank-1 perturbation lemma, we obtain
Υ −1
N tr RC−2 1
N
N
X
k=1
t2
k
(1 + tk 1
N tr RC−1)2
a.s.
−→0.
To determine a deterministic equivalent ˆmC,Λ(0) for mC,Λ(0) = 1
K tr RC−1,
we apply Theorem 6.12 again (noticing that there is once more no continuity
issue in point z = 0). For
1
K tr RC−2, we have:
1
K tr RC−2 = mC2,R(z) = m′
C,R(0).
The derivative of ¯mC,R(0) being a deterministic equivalent of m′
C,R(0), we
have ¯Υ, given as:
¯Υ =
ψ
N
K φ2 −ψ
1
K tr R−1
where ψ and φ are deﬁned in (14.22) and (14.23), respectively. Finally, we obtain
(14.21) by substituting Ψ and Υ in (14.24) by their respective deterministic
equivalents, which completes the proof.
The above results allow for interesting characterizations of the linearly
precoded broadcast channels. Some of these are provided below.
14.1.5
Applications
An interesting characterization of the performance of RZF-O derived above
for imperfect channel state information at the transmitter is to evaluate the
diﬀerence ∆R between the sum rate achieved under RZF-O and the sum
rate achieved when perfect channel information is assumed. For N = K, from
the deterministic equivalents obtained above, this is close to ¯∆R which, in
homogeneous channel conditions, takes the following convenient form
¯∆R = log2
 1 + √1 + 4ρ
1 + √1 + 4ρω


354
14. Rate performance in multiple access and broadcast channels
with
ω = 1 −τ 2
1 + τ 2ρ.
An interesting question is to determine how much feedback is required in the
uplink for the sum rate between the perfect CSIT and imperfect CSIT case to be
no larger than log2 b. That is, how much distortion τ 2 is maximally allowed to
ensure a rate gap of log2 b. Under the above simpliﬁed channel conditions, with
N = K
τ 2 = 1 + 4ρ −ω2
b2
3 + ω2
b2
1
ρ.
In particular, τ 2 ≃b2 −1 in the large ρ regime. This further allows us to evaluate
the optimal number of training bits required to maintain a given rate loss. Then,
taking into account the additional rate loss incurred by the process of channel
training, the optimal sum rate under the channel training constraint can be made
explicit. This is further discussed in [Wagner et al., 2011].
A second question of interest lies in the optimal ratio N/K between the number
of transmit antennas and the number of users that maximizes the sum rate per
transmit antenna. This allows us to determine a ﬁgure of merit for the eﬃciency
of every single antenna. In the uncorrelated channel conditions above, from the
deterministic equivalents, we ﬁnd the optimal ratio c to be given by
c =
 
1 −
τ 2 + 1
ρ
1 −τ 2
! 

1 +
1
W

1
e
h
1−τ 2
τ 2+ 1
ρ −1
i



with W(x) the Lamber-W function, deﬁned as the solution to W(x) = xeW(x),
unique for x ∈[−1
e, ∞).
In Figure 14.2, the sum rate performance of RZF and ZF under imperfect
channel state information at the transmitter are depicted and compared. We
assume here that N = K, that the users are uniformly distributed around the
transmitter, and that there exists no channel correlation at the transmitter.
The channel estimation distortion equals τ 2 = 0.1. Observe that a signiﬁcant
performance gain is achieved when the imperfect channel estimation parameter
is taken into account, while, as expected, the performance of both RZF assuming
perfect channel information and ZF converge asymptotically to the same limit.
In Figure 14.3, the performance of ZF precoding under diﬀerent channel
assumptions are compared against their respective deterministic equivalents.
The exact channel conditions assumed here follow from those introduced in
[Wagner et al., 2011] and are omitted here. We only mention that homogeneous
channel conditions, i.e. with T = IN and R = IK, are compared against the
case when correlation at the transmitter emerges from a compact array of
antennas on a limited volume, with inter-antenna spacing equal to half the
transmit wavelength, and the case when users have diﬀerent path losses, following

14.2. Rate region of MIMO multiple access channels
355
0
5
10
15
20
25
30
0
10
20
30
ρ [dB]
sum rate [bits/s/Hz]
RZF-O
RZF-CDU
ZF
Figure 14.2 Ergodic sum rate of regularized zero-forcing with optimal α (RZF-O), α
taken as if τ = 0, i.e. for channel distortion unaware transmitter (RZF-CDU) and
zero-forcing (ZF), T = IN, R = IK, N = K, τ 2 = 0.1.
the COST231 Hata urban propagation model. We also take τ1 = . . . = τK ≜τ,
with τ = 0 or τ 2 = 0.1, alternatively. The number of users is K = 16 and the
number of transmit antennas N = 32. The lines drawn are the deterministic
equivalents, while the dots and error bars are the averaged sum rate evaluated
from simulations and the standard deviation, respectively. Observe that the
deterministic equivalents, already for these not too large system dimensions,
fall within one standard deviation of the simulated sum rates and are in most
situations very close approximations of the mean sum rates.
Similar considerations of optimal training time in large networks, but in the
context of multi-cell uplink models are also provided in [Hoydis et al., 2011d],
also relying on deterministic equivalent techniques. This concludes this section on
linearly precoded broadcast channels. In the subsequent section, we address the
information-theoretic, although less practical, question of the characterization of
the overall rate region of the dual multiple access channel.
14.2
Rate region of MIMO multiple access channels
We consider the generic model of an N-antenna access point (or base station)
communicating with K users. User k, k ∈{1, . . . , K}, is equipped with nk
antennas. Contrary to the study developed in Section 14.1, we do not
restrict receiver k to be equipped with a single antenna. To establish large

356
14. Rate performance in multiple access and broadcast channels
0
5
10
15
20
25
30
0
50
100
150
ρ [dB]
sum rate [bits/s/Hz]
T = IN, R = IK, τ = 0
T = IN, R ̸= IK, τ = 0
T ̸= IN, R ̸= IK, τ = 0
T = IN, R = IK, τ 2 = 0.1
T ̸= IN, R ̸= IK, τ 2 = 0.1
Figure 14.3 Sum rate of ZF precoding, with N = 32, K = 16, under diﬀerent channel
conditions. Uncorrelated transmit antennas (T = IN) or volume limited transmit
device with inter-antenna spacing of half the wavelength (T ̸= IN), equal path losses
(R = IK) or path losses based on modiﬁed COST231 Hata urban model (T ̸= IK),
τ = 0 or τ 2 = 0.1. Simulation results are indicated by circle marks with error bars
indicating one standard deviation in each direction.
dimensional matrix results, we will consider here that N and n ≜PK
i=1 nk are
commensurable. That is, we will consider a large system analysis where:
• either a large number K of users, each equipped with few antennas,
communicate with an access point, equipped with a large number N of
antennas;
• either a small number K of users, each equipped with a large number of
antennas, communicate with an access point, equipped also with a large
number of antennas.
The channel between the base station and user k is modeled by the matrix
Hk ∈Cnk×N. In the previous section, where nk = 1 for all k, Hk was denoted by
hH
k , the Hermitian sign being chosen for readability to avoid working with row
vectors. With similar notations as in Section 14.1, the downlink channel model
for user k at time t reads:
y(t)
k
= Hkz(t) + σw(t)
k
where z(t) ∈CN is the transmit data vector and the receive symbols y(t)
k
∈Cnk
and additive Gaussian noise w(t)
k
∈Cnk are now vector valued. Contrary to the
linear precoding approach, the relation between the eﬀectively transmitted z(t)
and the intended symbols s(t) = (s(t)
1 , . . . , s(t)
K )T is not necessarily linear. We also

14.2. Rate region of MIMO multiple access channels
357
Figure 14.4 Multiple access MIMO channel, composed of K users and a base station.
User k is equipped with nk antennas, and the base station with N antennas. The
channel between user k and the base station is HH
k = T
1
2
k XH
kRH
k.
denote P the transmit covariance matrix P = E[z(t)z(t)H], assumed independent
of t, which satisﬁes the power constraint
1
N tr P = P.
Denoting equivalently z(t)
k
∈Cnk the signal transmitted in the uplink (MAC)
by user k, such that E[z(t)
k z(t)H
k
] = Pk,
1
nk tr Pk ≤Pk, y(t) and w(t) the signal
and noise received by the base station, respectively, and assuming perfect channel
reciprocity in the downlink and the uplink, we have the uplink transmission
model
y(t) =
K
X
k=1
HH
k z(t)
k + w(t).
(14.25)
Similar to the linearly precoded case, we assume that Hk, k ∈{1, . . . , K}, is
modeled as Kronecker, i.e.
Hk ≜R
1
2
k XkT
1
2
k
(14.26)
where Xk ∈Cnk×N has i.i.d. Gaussian entries of zero mean and variance 1/nk,
Tk ∈CN×N is the Hermitian non-negative deﬁnite channel correlation matrix
at the base station with respect to user k, and Rk ∈Cnk×nk is the Hermitian
non-negative deﬁnite channel correlation matrix at user k.
In the following, we study the MAC rate regions for quasi-static channels. We
will then consider the ergodic rate region for time-varying MAC. An illustrative
representation of a cellular uplink MAC channel as introduced above is provided
in Figure 14.4.
14.2.1
MAC rate region in quasi-static channels
We start by assuming that the channels H1, . . . , HK are random realizations of
the Kronecker channel model (14.26), considered constant over the observation
period. The MIMO MAC rate region CMAC(P1, . . . , PK; HH) for the quasi-static

358
14. Rate performance in multiple access and broadcast channels
model (14.25), under respective transmit power constraints P1, . . . , PK for users
1 to K and channel HH ≜[HH
1 . . . HH
K], reads [Tse and Viswanath, 2005]
CMAC(P1, . . . , PK; HH)
=
[
1
ni tr(Pi)≤Pi
Pi≥0
i=1,...,K
(
r;
X
i∈S
ri ≤log2 det
 
IN + 1
σ2
X
i∈S
HH
i PiHi
!
, ∀S ⊂{1, . . . , K}
)
(14.27)
where r = (r1, . . . , rK) ∈[0, ∞)K and Pi ≥0 stands for Pi non-negative deﬁnite.
That is, the set of achievable rate vectors (r1, . . . , rK) is such that the sum of
the rates of any subset S = {i1, . . . , i|S|} is less than a classical log determinant
expression for all possible precoders Pi1, . . . , Pi|S|.
Consider such a subset S = {i1, . . . , i|S|} of {1, . . . , K} and a set Pi1, . . . , Pi|S|
of deterministic precoders, i.e. precoders chosen independently of the particular
realizations of the H1, . . . , HK matrices (although possibly taken as a function
of the Rk and Tk correlation matrices).
At this point, depending on the underlying model assumptions, it is possible
to apply either Corollary 6.1 of Theorem 6.4 or Remark 6.5 of Theorem 6.12.
Although we mentioned in Remark 6.5 that it is highly probable that both
theorems hold for the most general model hypotheses, we presently state which
result can be applied to which situation based on the mathematical results
available in the literature.
• If the Rk and Tk matrices are only constrained to have uniformly bounded
normalized trace and the number of users K is small compared to the number
of antennas nk per user and the number of antennas at the transmitter, then
Corollary 6.1 of Theorem 6.4 states (under some mild additional assumptions
recalled in Chapter 13) that the per-antenna normalized log determinant
expressions of (14.27) can be given a deterministic equivalent. This case allows
us to assume very correlated antenna arrays, which is rather convenient for
practical purposes.
• If the Rk and Tk matrices have uniformly bounded spectral norm as their
dimensions grow, i.e. in practice if the largest eigenvalues of Rk or Tk are
much smaller than the matrix size, and the total number of user antennas n ≜
PK
k=1 nk is of the same order as the number N of antennas at the base station,
then the hypotheses of Theorem 6.12 hold and a deterministic equivalent for
the total capacity can be provided.
The ﬁrst setting is more general in the sense that more general antenna
correlation proﬁles can be used, while the second case is more general in the
sense that the users’ antennas can be distributed in a more heterogeneous way.
Since we wish ﬁrst to determine the rate region of MAC, though, we need, for
all subset S ⊂{1, . . . , K}, that N and P
i∈S ni grow large simultaneously. This

14.2. Rate region of MIMO multiple access channels
359
imposes in particular that all ni grow large simultaneously with N. Later we will
determine the achievable sum rate, for which only the subset S = {1, . . . , K} will
be considered. For the latter, Theorem 6.12 will be more interesting, as it can
assume a large number of users with few antennas, which is a far more realistic
assumption in practice.
For rate region analysis, consider then that all ni are of the same order of
dimension as N and that K is small in comparison. From Theorem 6.4, we have
immediately that
1
N log2 det
 
IN + 1
σ2
X
i∈S
HH
i PiHi
!
−
"
1
N log2 det
 
IN +
X
k∈S
¯ekTk
!
−log2(e)σ2 X
k∈S
¯ekek + 1
N
X
k∈S
log2 det

Ink + ckekR
1
2
k PkR
1
2
k
#
a.s.
−→0
where ck ≜N/nk and ei1, . . . , ei|S|, ¯ei1, . . . , ¯ei|S| are the only positive solutions to
ei =
1
σ2N tr Ti
 
IN +
X
k∈S
¯ekTk
!−1
¯ei =
1
σ2ni
tr R
1
2
i PiR
1
2
i

Ini + cieiR
1
2
i PiR
1
2
i
−1
.
(14.28)
This therefore provides a deterministic equivalent for the points in the rate
region corresponding to deterministic power allocation strategies, i.e. power
allocation strategies that do not depend on the Xk matrices. That is, not all
points in the rate region can be associated with a deterministic equivalent
(especially not the points on the rate region boundary) but only those points
for which a deterministic power allocation is assumed.
Note now that we can similarly provide a deterministic equivalent to every
point in the rate region of the quasi-static broadcast channel corresponding to
a deterministic power allocation policy. As recalled earlier, the boundaries of
the rate region CBC(P; H) of the broadcast channel have been recently shown
[Weingarten et al., 2006] to be achieved by dirty paper coding (DPC). For a
transmit power constraint P over the compound channel H, it is shown by MAC-
BC duality that [Viswanath et al., 2003]
CBC(P; H) =
[
P1,...,PK
PK
k=1 Pk≤P
CMAC(P1, . . . , PK; HH).
Therefore, from the deterministic equivalent formula above, we can also
determine a portion of the BC rate region: that portion corresponding to the
deterministic precoders. However, note that this last result has a rather limited
interest. Indeed, channel-independent precoders in quasi-static BC inherently
perform poorly compared to precoders adapted to the propagation channel, such
as the optimal DPC precoder or the linear ZF and RZF precoders. This is because
BC communications come along with potentially strong inter-user interference,

360
14. Rate performance in multiple access and broadcast channels
which is only mitigated through adequate beamforming strategies. Deterministic
precoders are incapable of providing eﬃcient inter-user interference reduction
and are therefore rarely considered in the literature.
Simulation results are provided in Figure 14.5 in which we assume a two-user
MAC scenario. Each user is equipped with n1 = n2 antennas, where n1 = 8 or
n1 = 16, while the base station is equipped with N = n1 = n2 antennas. The
antenna array is linear with inter-antenna distance dR/λ set to 0.5 or 0.1 at the
users, and dT/λ = 10 at the base station. We further assume that the eﬀectively
transmitted energy propagates from a solid angle of π/6 on either communication
side, with diﬀerent propagation directions, and therefore consider the generalized
Jakes’ model for the Rk and Tk matrices. Speciﬁcally, we assume that user 2
sees the signal arriving at angle zero rad, and user 1 sees the signal arriving at
angle π rad. We further assume uniform power allocation at the transmission.
From the ﬁgure, we observe that the deterministic equivalent plot is centered
somewhat around the mean value of the rates achieved for diﬀerent channel
realizations. As such, it provides a rather rough estimate of the instantaneous
multiple access mutual information. It is nonetheless necessary to have at least 16
antennas on either side for the deterministic equivalent to be eﬀectively useful. In
terms of information-theoretical observations, note that a large proportion of the
achievable rates is lost by increasing the antenna correlation. Also, as already
observed in the single-user MIMO case, increasing the number of antennas in
strongly correlated channels reduces the eﬃciency of every individual antenna.
As largely discussed above, it is in fact of limited interest to study the
performance of quasi-static MAC and BC channels through large dimensional
analysis, in a similar way to the single-user case, in the sense that optimal power
allocation cannot be performed and the deterministic equivalent only provides a
rough estimate of the eﬀective rates achieved with high probability for a small
number of antennas. When it comes to ergodic mutual information, though,
similar to the point-to-point MIMO scenario, large system analysis can provide
optimal power allocation policies and very accurate capacity approximations for
small system dimensions.
14.2.2
Ergodic MAC rate region
Consider now the situation where the K channels are changing too fast for the
users to be able to adapt adequately their transmit powers, while having constant
Kronecker-type statistics. In this case, the MAC rate region is deﬁned as
C(ergodic)
MAC
(P1, . . . , PK; HH)
=
[
{Pi}
(
r,
X
i∈S
ri ≤E
"
log2 det
 
IN + 1
σ2
X
i∈S
HH
i PiHi
!#
, ∀S ⊂{1, . . . , K}
)

14.2. Rate region of MIMO multiple access channels
361
0
0.5
1
1.5
2
2.5
0
1
2
3
4
Per-antenna rate of user 1 [bits/s/Hz]
Per-antenna rate of user 2 [bits/s/Hz]
0
0.5
1
1.5
2
2.5
0
1
2
3
4
Per-antenna rate of user 1 [bits/s/Hz]
Per-antenna rate of user 2 [bits/s/Hz]
Figure 14.5 (Per-antenna) rate of two-user ﬂat fading MAC, equal power allocation,
for N = 8 (top), N = 16 (bottom) antennas at the base station, n1 = n2 = N
antennas at the transmitters, uniform linear antenna arrays, antenna spacing
dR
λ = 0.5 (dashed) and dR
λ = 0.1 (solid) at the transmitters, dT
λ = 10 at the base
station, SNR = 20 dB. Deterministic equivalents are given in thick dark lines.
where the union is taken over all Pi non-negative deﬁnite such that
1
ni
tr(Pi) ≤Pi.

362
14. Rate performance in multiple access and broadcast channels
We can again recall Theorem 6.4 to derive a deterministic equivalent for the
ergodic mutual information for all deterministic Pi1, . . . , Pi|S| precoders, as
E
"
1
N log2 det
 
IN + 1
σ2
X
i∈S
HH
i PiHi
!#
−
"
1
N log2 det
 
IN +
X
k∈S
¯ekTk
!
+ 1
N
X
k∈S
log2 det

Ink + ckekR
1
2
k PkR
1
2
k

−log2(e)σ2 X
k∈S
¯ekek
#
→0
for growing N, ni1, . . . , ni|S|. Now it is of interest to determine the optimal
deterministic equivalent precoders. That is, for every subset S = {i1, . . . , i|S|},
we wish to determine the precoding vector PS
i1, . . . , PS
i|S| which maximizes the
deterministic equivalent. This study is performed in [Couillet et al., 2011a].
Similar to the single-user MIMO case, it suﬃces to notice that maximizing
the deterministic equivalent over Pi1, . . . , Pi|S| is equivalent to maximizing the
expression
X
k∈S
log2 det

Ink + ckeS
kR
1
2
k PkR
1
2
k

over Pi1, . . . , Pi|S|, where (eS
i1, . . . , eS
i|S|, ¯eS
i1, . . . , ¯eS
i|S|) are ﬁxed, equal to the unique
solution with positive entries of (14.28) when Pi = PS
i for all i ∈S. To observe
this, we essentially need to observe that the derivative of the function
V : (Pi1, . . . , Pi|S|, ∆i1, . . . , ∆i|S|, ¯∆i1, . . . , ¯∆i|S|) 7→1
N log2 det
 
IN +
X
k∈S
¯∆kTk
!
+ 1
N
X
k∈S
log2 det

Ink + ck∆kR
1
2
k PkR
1
2
k

−log2(e)σ2 X
k∈S
¯∆k∆k
along any ∆k or ¯∆k is zero when ∆i = ei and ¯∆i = ¯ei. This unfolds from
∂V
∂¯∆k
(Pi1, . . . , Pi|S|, ei1, . . . , ei|S|, ¯ei1, . . . , ¯ei|S|)
= log2(e)

1
N tr


 
IN +
X
i∈S
¯eiTi
!−1
Tk

−σ2ek


∂V
∂∆k
(Pi1, . . . , Pi|S|, ei1, . . . , ei|S|, ¯ei1, . . . , ¯ei|S|)
= log2(e)
ck
N tr

I + ckekR
1
2
i PiR
1
2
i
−1
R
1
2
k PkR
1
2
k

−σ2¯ek

both being null according to (14.28).
By the diﬀerentiation chain rule, the maximization of the log determinants
over every Pi is therefore equivalent to the maximization of every term
log2 det

Ink + ckeS
kR
1
2
k PkR
1
2
k


14.2. Rate region of MIMO multiple access channels
363
Deﬁne η > 0 the convergence threshold and l ≥0 the iteration step.
At step l = 0, for k ∈S, i ∈{1, . . . , nk}, set q0
k,i = Pk. At step l ≥1,
while maxk,i{|ql
k,i −ql−1
k,i |} > η do
For k ∈S, deﬁne (el+1
k
, ¯el+1
k
) as the unique pair of positive
solutions to (14.28) with, for all j ∈S, Pj = UjQl
jUH
j , Ql
j =
diag(ql
j,1, . . . , ql
j,nj) and Uj the matrix such that Rj has spectral
decomposition UjΛjUH
j , Λj = diag(rj,1, . . . , rj,nj)
for i ∈{1 . . . , nk} do
Set ql+1
k,i =

µk −
1
ckel+1
k
rk,i
+
, with µk such that
1
nk tr Ql
k = Pk
end for
assign l ←l + 1
end while
Table 14.1. Iterative water-ﬁlling algorithm for the determination of the MIMO MAC
ergodic rate region boundary.
(remember that the power constraints over the Pi are independent). The
maximum of the deterministic equivalent for the MAC ergodic mutual
information is then found to be V evaluated at eS
k, ¯eS
k and PS
k for all k ∈S. It
unfolds that the capacity maximizing precoding matrices are given by a water-
ﬁlling solution, as
PS
k = UkQS
kUH
k
where Uk ∈Cnk×nk is the eigenvector matrix of the spectral decomposition of
Rk as Rk = Uk diag(rk,1, . . . , rk,nk)UH
k , and QS
k is a diagonal matrix with ith
diagonal entry qS
ki given by:
qS
ki =

µk −
1
ckeS
krk,i
+
µk being set so that
1
nk
Pnk
i=1 qS
ki = Pk, the maximum power allowed for user
k. As usual, this can be determined from an iterative water-ﬁlling algorithm,
provided that the latter converges. This is given in Table 14.1.
The performance of uniform and optimal power allocation strategies in the
uplink ergodic MAC channel is provided in Figure 14.6 and Figure 14.7. As in
the quasi-static case, the system comprises two users with n1 = n2 antennas,
identical distance 0.5λ between consecutive antennas placed in linear arrays,
and angle spread of energy arrival of π/2. User 1 sees the signal from an angle of
zero rad, while user 1 sees the signal from an angle of π rad. In Figure 14.6, we
observe, as was true already for the point-to-point MIMO case, that deterministic
equivalents approximate very well the actual ergodic mutual information, for
dimensions greater than or equal to four. It is then observed in Figure 14.7 that
much data throughput can be gained by using optimal precoders at the user

364
14. Rate performance in multiple access and broadcast channels
0
2
4
6
8
10
12
14
16
0
5
10
15
20
25
Per antenna rate of user 1 [bits/s/Hz]
Per-antenna rate of user 2 [bits/s/Hz]
N = 2, simulation
N = 2, det. eq.
N = 4, simulation
N = 4, det. eq.
N = 8, simulation
N = 8, det. eq.
Figure 14.6 Ergodic rate region of two-user MAC, uniform power allocation, for
N = 2, N = 4, and N = 8, n1 = n2 = N, uniform linear array model, antenna spacing
at the users dR
λ = 0.5, at the base station dT
λ = 10. Comparison between simulations
and deterministic equivalents (det. eq.).
terminals, especially on the rates of strongly correlated users. Notice also that
in all previous performance plots, depending on the direction of energy arrival,
a large diﬀerence in throughput can be achieved. This is more acute than in
the single-user case where the resulting capacity is observed to be only slightly
reduced by diﬀerent propagation angles. Here, it seems that some users can either
beneﬁt or suﬀer greatly from the conditions met by other users.
We now turn to the speciﬁc study of the achievable ergodic sum rate.
14.2.3
Multi-user uplink sum rate capacity
As recalled earlier, when it comes to sum rate capacity, we only need to provide
a deterministic equivalent for the log determinant expression
E
"
log2 det
 
IN + 1
σ2
K
X
i=1
HH
i PiHi
!#
for all deterministic P1, . . . , PK. Obviously, this problem is even easier to
treat than the previous case, as only one subset S of {1, . . . , K}, namely
S = {1, . . . , K}, has to be considered. As a consequence, the large dimensional
constraint is just that both n = PK
i=1 ni and N are large and of similar
dimension. This does no longer restrict any individual ni to be large and of
similar amplitude as N. Therefore, Theorem 6.12 will be used instead of Theorem
6.4.

14.2. Rate region of MIMO multiple access channels
365
0
5
10
15
20
0
10
20
30
Per antenna rate of user 1 [bits/s/Hz]
Per-antenna rate of user 2 [bits/s/Hz]
N = 2, uniform
N = 2, optimal
N = 4, uniform
N = 4, optimal
N = 8, uniform
N = 8, optimal
Figure 14.7 Deterministic equivalents for the ergodic rate region of two-user MAC,
uniform power allocation against optimal power allocation, for N = 2, N = 4, and
N = 8, n1 = n2 = N, uniform linear array model, antenna spacing at the users
dR
λ = 0.5, at the base station dT
λ = 10.
We will treat a somewhat diﬀerent problem in this section, which assumes that,
instead of a per-user power constraint, all users can share a total energy budget
to be used in the uplink. We also assume that the conditions of Theorem 6.12
now hold and we can consider, as in Section 14.1, that a large number of mono-
antenna users share the channel to access a unique base station equipped with
a large number of antennas. In this case, the deterministic equivalent developed
in the previous section still holds true. Under transmit sum-power budget P,
the power optimization problem does no longer take the form of a few optimal
matrices but of a large number of scalars to be appropriately shared among
the users. Recalling the notations of Section 14.1 and assuming perfect channel
reciprocity, the ergodic MAC sum rate capacity Cergodic(σ2) reads:
Cergodic(σ2) =
max
p1,...,pK
PK
i=1 pi=P
E
"
log2 det
 
IN + 1
σ2
K
X
i=1
pihihH
i
!#
(14.29)
with hi = T
1
2
i xi ∈CN, xi ∈CN having i.i.d. Gaussian entries of zero mean and
variance 1/N. Remember that Ti stems for both the correlation at the base
station and the path-loss component. In the MAC channel, Ti is now a receive
correlation matrix.
The right-hand side of (14.29) is asymptotically maximized for pk = p⋆
k such
that p⋆
k −p◦
k →0 as the system dimensions grow large, where p◦
k is given by:
p◦
k =

µ −K
N
1
e◦
k
+

366
14. Rate performance in multiple access and broadcast channels
for all k ∈{1, . . . , K}, where µ is set to satisfy PK
i=1 pi = P, and where e◦
k is such
that (e◦
1, . . . , e◦
K) is the only vector of positive entries solution of the implicit
equations in (e1, . . . , eK)
ei = 1
N tr Ti
 
1
N
K
X
k=1
p◦
k
1 + cKp◦
kek
Tk + σ2IN
!−1
with cK ≜N/K. Again, the p◦
k can be determined by an iterative water-ﬁlling
algorithm.
We ﬁnally have that the deterministic equivalent for the capacity Cergodic(σ2)
is given by:
1
N Cergodic(σ2)−
"
1
N log2 det
 
σ2 1
N
K
X
k=1
1
1 + cKe◦
k
Tk + IN
!
+ 1
N
K
X
k=1
log2 (1 + cKe◦
kp◦
k) −log2(e) 1
K
K
X
k=1
p◦
ke◦
k
1 + cKe◦
kp◦
k
#
→0.
In Figure 14.8, the performance of the optimal power allocation scheme is
depicted for correlation conditions similar to the previous scenarios, i.e. with
correlation patterns at the base station accounting for inter-antenna distance
and propagation direction, and diﬀerent path loss exponents for the diﬀerent
users. It turns out numerically that, as in the single-user MIMO case, for low
SNR, it is beneﬁcial to reduce the number of transmitting users and allocate
most of the available power to a few users, while this tendency fades away for
higher SNR.
This closes this chapter on multi-user communications in both single antenna
and multiple antenna regimes. In the next chapter, we extend the single-cell
multiple antenna and multi-user framework to multi-cellular communications
and relay communications.

14.2. Rate region of MIMO multiple access channels
367
−15
−10
−5
0
5
10
15
20
0
2
4
6
8
10
12
SNR [dB]
Achievable rate
Sim., uni.
Det. eq., uni.
Sim., opt.
Det. eq., opt.
Figure 14.8 Ergodic MAC sum rate for an N = 4-antenna receiver and K = 4
mono-antenna transmitters under sum power constraint. Every user transmit signal
has diﬀerent correlation patterns at the receiver, and diﬀerent path losses.
Deterministic equivalents (det. eq.) against simulation (sim.), with uniform (uni.) or
optimal (opt.) power allocation.


15
Performance of multi-cellular and
relay networks
In this chapter, we move from single-cell considerations, with a central entity,
e.g. access point or base station, to the wider multi-user multi-cell network
point of view or the multi-hop relay point of view. For the former, we now
consider that communications are performed in a cellular environment over a
given shared communication resource (e.g. same frequency band, overlaying cell
coverage), with possible inter-cell interference. This is a more realistic assumption
to model practical communication networks than the isolated single-cell case with
additive white Gaussian noise. Here, not only AWGN is aﬀecting the diﬀerent
actors in the network but also cross-talk between adjacent base stations or cell-
edge users. For the latter, we do not assume any cellular planning but merely
consider multi-hop communications between relays which is of reduced use in
commercial communication standards but of high interest to, e.g., ad-hoc military
applications.
We start with the multi-cell viewpoint.
15.1
Performance of multi-cell networks
We consider the model of a multi-cellular network with overlaying coverage
regions. In every cell, each user has a dedicated communication resource,
supposed to be orthogonal to any other user’s resource. For instance, we may
assume that orthogonal frequency division multiple access (OFDMA) is used in
every cell, so that users are orthogonally separated both in time and frequency.
It is well-known that a major drawback of such networks is their being strongly
interference limited since users located at cell edges may experience much
interference from signals transmitted in adjacent cells. To mitigate or cancel
this problem, a few solutions are considered, such as:
• ban the usage of certain frequencies in individual cells. This technique is
referred to as spatial frequency reuse and consists precisely in making sure
that two adjacent cells never use the same frequency band. This has the
strong advantage that cross-talk in the same communication bandwidth can
only come from remote cells. Nonetheless, this is a strong sacriﬁce in terms of
available communication resources;

370
15. Performance of multi-cellular and relay networks
• create directional transmit signal beams. That is, at least in the downlink
scenario, base stations use additional antennas to precode the transmit data
in such a way that the information dedicated to the in-cell users is in the
main lobe of a propagation beam, while the out-cell users are outside these
lobes. This solution does not cancel adjacent cell interference but mitigates
it strongly, although this requires additional antennas at the transmitter and
more involved data processing.
An obvious solution to totally discard interference is to allow the base
stations to cooperate. Precisely, if all network base stations are connected via
a high rate backbone and are able to process simultaneously all uplink and
downlink transmissions, then it is possible to remove the inter-cell interference
completely by considering a single large cell instead, which is composed of
multiple cooperative base stations. In this case, the network can be simply
considered as a regular multi-access or broadcast channel, as those studied in
Chapter 14. Nonetheless, while this idea of cooperative base stations has now
made its way to standardization even for large range communications, e.g. in
the 3GPP-LTE Advanced standard [Sesia et al., 2009], this approach is both
diﬃcult to establish and of actually limited performance gain. Concerning the
diﬃculty to put the system in place, we ﬁrst mention the need to have a high
rate backbone common to all base stations and a central processing unit so fast
that joint decoding of all users can be performed under the network delay-limited
constraints. As for the performance gain limitation, this mainly arises from the
fact that, as the eﬀective cell size increases, the central processing unit must be
at all times aware of all communication channels and all intended data of all
users. This imposes a large amount of synchronization and channel estimation
information to be fed back to the central unit as the network size grows large.
This therefore assumes that much time is spent by the network learning from its
own structure. In fast mobile communication networks, where communication
channels are changing fast, this is intolerable as too little time is then spent on
eﬀective communications.
This section is dedicated to the study of the intrinsic potential gains brought
by cooperative cellular networks. As cooperative networks become large, large
dimensional matrix theory can be used with high accuracy and allows us to
obtain deterministic approximations of the system performance from which
optimization can be conducted, such as determining the capacity maximizing
feedback time, the capacity maximizing number of cooperative cells, etc. For
simplicity, though, we assume perfect channel knowledge in the following in order
to derive the potential gains obtained by unrealistic genie-aided multi-cellular
cooperation.
Let us consider a K-cell network sharing a narrowband frequency resource,
used by exactly one user per cell. Therefore, we can assume here that each cell
is composed of a single-user. We call user k the user of cell k, for k ∈{1, . . . , K}.
We can consider two interference limited scenarios, namely:

15.1. Performance of multi-cell networks
371
• the downlink scenario, where multiple base stations concurrently use the
resource of all users, generating inter-cell interference to all users;
• the uplink scenario, where every base station suﬀers from inter-cell interference
generated by the data transmitted by users in other cells.
We will consider here only the uplink scenario. We assume that base station
k is equipped with Nk transmit antennas, and that user k is equipped with
nk receive antennas. Similar to previous chapters, we denote n ≜PK
k=1 nk
and N ≜PK
k=1 Nk. We also denote Hk,i ∈CNi×nk the uplink multiple antenna
channel between user k and base station i. We consider that Hk,i is modeled as
a Kronecker channel, under the following form
Hk,i = R
1
2
k,iXk,iT
1
2
k,i
for non-negative Rk,i with eigenvalues rk,1, . . . , rk,N, non-negative Tk,i with
eigenvalues tk,i,1, . . . , tk,i,nk, and Xk,i with i.i.d. Gaussian entries of zero mean
and variance 1/nk. As usual, we can assume the Tk,i diagonal without loss of
generality.
Assume that a given subset C ⊂{1, . . . , K} of cells cooperates and is interfered
with by the remaining users from the complementary set Cc = {1, . . . , K} \ C.
Denote NC = P
k∈C Nk, NCc = P
k∈Cc Nk, and Hk,C ∈CNC×nk the channel
[HH
k,i1, . . . , HH
k,i|C|]H, with {i1, . . . , i|C|} = C. The channel Hk,C is the joint channel
between user k and the cooperative large base station composed of the |C|
subordinate base stations indexed by elements of C. Assuming uniform power
allocation across the antennas of the transmit users, the ergodic sum rate
E[I(C; σ2)] of the multiple access channel between the |C| users and the
cooperative base stations, considered as a unique receiver, under AWGN of
variance σ2 reads:
E[I(C; σ2)]
= E

log2 det

INC +
"X
k∈C
Hk,CHH
k,C
# "
σ2INC +
X
k∈Cc
Hk,CHH
k,C
#−1



= E
"
log2 det
 
INC + 1
σ2
K
X
k=1
Hk,CHH
k,C
!#
−E
"
log2 det
 
INC + 1
σ2
X
k∈Cc
Hk,CHH
k,C
!#
.
Under this second form, it unfolds that a deterministic equivalent of E[I(C; σ2)]
can be found as the diﬀerence of two deterministic equivalents, similar to Chapter

372
15. Performance of multi-cellular and relay networks
14. Indeed, the jth column hk,C,j ∈CNC of Hk,C can be written under the form
hk,C,j =



hk,i1,j
...
hk,i|C|,j


=




√tk,i1,jR
1
2
k,i1 · · ·
0
...
...
...
0
· · · ptk,i|C|,jR
1
2
k,i|C|







xk,i1,j
...
xk,i|C|,j



with xk,i,j the jth column of Xk,i, which has independent Gaussian entries of
zero mean and variance 1/nk. We will denote ¯Rk,C,j the block diagonal matrix
¯Rk,C,j ≜



tk,i1,jRk,i1 · · ·
0
...
...
...
0
· · · tk,i|C|,jRk,i|C|


.
With these notations, the random matrix Hk,CHH
k,C therefore follows the
model of Theorem 6.12 with correlation matrices ¯Rk,C,j for all j ∈{1, . . . , nk}
and the normalized ergodic sum rate
1
NC
E[I(C; σ2)] = E

1
NC
log2 det

INC + 1
σ2
K
X
k=1
nk
X
jk=1
hk,C,jkhH
k,C,jk




−E

1
NC
log2 det

INC + 1
σ2
X
k∈Cc
nk
X
jk=1
hk,C,jkhH
k,C,jk




has a deterministic equivalent that can be split into the diﬀerence of the following
expression (i)
1
NC
log2 det

σ2 1
n
K
X
k=1
nk
X
jk=1
1
1 + cNeK
k,jk
¯Rk,C,jk + I


+ 1
NC
K
X
k=1
nk
X
jk=1
log2
 1 + cNeK
k,jk

−1
n
K
X
k=1
log2(e)
nk
X
jk=1
eK
k,jk
1 + cNek,jk
with cN ≜N/n and eK
k,jk, K ≜{1, . . . , K}, deﬁned as the only all positive
solutions of
eK
k,jk =
1
NC
tr ¯Rk,C,jk

1
n
K
X
k′=1
nk′
X
jk′=1
1
1 + cNeK
k′,jk′
¯Rk′,C,jk′ + σ2I


−1
(15.1)
and of this second term (ii)
1
NC
log2 det

σ2 1
n
X
k∈Cc
nk
X
jk=1
1
1 + cNeCc
k,jk
¯Rk,C,jk + I


+ 1
NC
X
k∈Cc
nk
X
jk=1
log2
 1 + cNeCc
k,jk

−1
n
X
k∈Cc
nk
X
jk=1
log2(e)
eCc
k,jk
1 + cNeCc
k,jk

15.1. Performance of multi-cell networks
373
with eCc
k,jk the only all positive solutions of
eCc
k,jk =
1
NC
tr ¯Rk,C,jk

1
n
X
k′∈Cc
nk′
X
jk′=1
1
1 + cNeCc
k′,jk′
¯Rk′,C,jk′ + σ2I


−1
.
Although these expressions are all the more general under our model
assumptions, simpliﬁcations are required to obtain tractable expressions. We
will consider two situations. First, that of a two-cell network with diﬀerent
interference powers, and second that of a large network with mono-antenna
devices. The former allows us to discuss the interest of cooperation in a simple
two-cell network as a function of the diﬀerent channel conditions aﬀecting
capacity: overall interference level, number of available antennas, transmit and
receive correlation, etc. The latter allows us to determine how many base stations
are required to be connected in order for the overall system capacity not to
be dramatically impaired. On top of these discussions, the problem of optimal
channel state information feedback time can also be considered when the channel
coherence time varies. It is then possible, for practical networks, to have a rough
overview on the optimal number of base stations to inter-connect under mobility
constraints. This last point is not presented here.
15.1.1
Two-cell network
In this ﬁrst example, we consider a two-cell network for which we provide the
theoretical sum rate capacities achieved when:
• both cells cooperate in the sense that they proceed to joint decoding of the
users operating at the same frequency;
• cells do not cooperate and are therefore interference limited.
When base stations do not cooperate, it is a classical assumption that the
channels from user k to base station k, k ∈{1, 2}, are not shared among the
adjacent cells. An optimal power allocation policy can therefore not be clearly
deﬁned. However, in the cooperative case, power allocation can be clearly
performed, under either sum power constraint or individual power constraint
for each user. We will consider the latter, more realistic, choice. Also, we make
the strong necessary assumption that, at the transmitter side, the eﬀective energy
is transmitted isotropically, i.e. there is no privileged direction of propagation.
This assumption is in fact less restrictive when the mobile transmitter is located
in a very scattered environment, such as in the house or in the street while the
receive base station is located far away. This assumption would however not hold
for base stations, which are typically located in a poorly scattered environment.
Following Jakes’ model for the transmit correlation Tk,i matrices, we will
therefore assume that Tk,1 = Tk,2 ≜Tk, k ∈{1, 2}. We are now in the situation
of the previous study for K = {1, 2}. The subset C ⊂K of simultaneously decoded

374
15. Performance of multi-cellular and relay networks
users is alternatively {1} and {2} in the non-cooperative case, and {1, 2} in the
cooperative case.
The real diﬃculty lies in determining the optimal power allocation policy for
the cooperative case. Denoting P◦
k ∈Cnk×nk this optimal power allocation policy
for user k in the cooperative case, we have in fact trivially that P◦
k = UkQ◦
kUH
k
with Uk the eigenvector basis of Tk, and Q◦
k = diag(q◦
k,1, . . . , q◦
k,nk), with q◦
k,i
given by:
q◦
k,i =
 
µk −
1
1 + cNeK◦
k,jk
!+
where µk is set to satisfy
1
nk
Pnk
i=1 q◦
k,i = Pk, Pk being the power allowed for
transmission to user k, and eK◦
k,jk is the solution to the ﬁxed-point Equation
(15.1) in eK
k,jk, with eK
k,jk replaced by eK
k,jkp◦
k,j (present now in the expression
of ¯Rk,K,j). The only diﬀerence compared to the multi-user mono-antenna MAC
model is that multiple power constraints are imposed. This is nonetheless not
hard to solve as the maximization is equivalent to the maximization of
X
k∈K
nk
X
jk=1
log2
 1 + cNeK
k,jkp◦
k,j

.
As usual, the entries of Pk can be determined from an appropriate iterative
water-ﬁlling algorithm. In the following, we provide performance results for
the two-cell network. In Figure 15.1 and Figure 15.2, we consider the case
of two users, equipped with four antennas each, and two base stations, also
equipped with four antennas each. In Figure 15.1, both transmit and receive
sides are loosely correlated, i.e. the inter-antenna spacing is large and isotropic
transmissions are assumed. In Figure 15.2, the same scenario is considered,
although, at the transmitting users, strong signal correlation is present (the
distance between successive antennas is taken to be half the wavelength). We
observe that strong correlation at the transmitter side reduces the achievable
optimal sum rate, although in this case optimal power allocation policy helps
to recovere part of the lost capacity, notably for low SNR regimes. However, for
strongly correlated transmitters, it turns out that the performance of single-user
decoding in every cell is not strongly aﬀected and is even beneﬁcial for high
SNR. This is mostly due to the uniform power allocation policy adopted in the
interference limited case, which could be greatly improved.
In the following, we address the problem of the uplink communication in a large
dimensional network, somewhat following the Wyner model [Wyner, 1994] with
one user per cell and only one transmitting antenna on either communication
device.

15.1. Performance of multi-cell networks
375
−15
−10
−5
0
5
10
15
20
0
10
20
30
40
50
SNR [dB]
Achievable rate
Coop., uni.
Coop., opt.
Non-coop.
Figure 15.1 Sum rate capacity of two-cell network with two users per cell. Comparison
between cooperative MAC scenario (coop.) for uniform (uni.) and optimal (opt.)
power allocation, and interference limited scenario (non-coop.). Loosely correlated
signals at both communication ends. Interference power 0.5.
−15
−10
−5
0
5
10
15
20
0
10
20
30
40
SNR [dB]
Achievable rate
Coop., uni.
Coop., opt.
Non-coop.
Figure 15.2 Sum rate capacity of two-cell network with two users per cell. Comparison
between cooperative MAC scenario (coop.) for uniform (uni.) and optimal (opt.)
power allocation, and interference limited scenario (non-coop.). Loosely correlated
signals at the receiver end, strongly correlated signals at the transmission end.
Interference power 0.5.

376
15. Performance of multi-cellular and relay networks
Figure 15.3 Wyner model of a three-cell network, with cooperation via a central
backbone. Every cell site contains a single-user per spectral resource.
15.1.2
Wyner model
This section follows the ideas of, e.g., [Hoydis et al., 2011d; Levy and Shamai,
2009; Somekh et al., 2004, 2007]. We consider a K-cell network with one user
per cell (again, this describes the case when a single resource is analyzed) and
one antenna per user. We further assume that the base stations of all cells are
connected via an inﬁnite capacity backbone, so that inter-cell communications
are possible at a high transmission rate. The link hik between user k and base
station i is assumed to be random Gaussian with zero mean and variance
aik/K, with aik the long-term path loss exponent. This scenario is depicted
in Figure 15.3. Denoting A ∈CK×K the matrix with (i, k)th entry aik and
H = [h1, . . . , hK] ∈CK×K the matrix with (i, k)th entry hik, the achievable sum
rate per cell site C under perfect channel state information at the connected
base stations, unit transmit power for all users, additive white Gaussian noise of
variance σ2, is
C(σ2) = 1
K log2 det
 
IK + 1
σ2
K
X
k=1
hkhH
k
!
= 1
K log2 det

IK + 1
σ2 HHH

.
This is a classical expression with H having a variance proﬁle, for which a
deterministic equivalent is obtained from Theorem 6.12 for instance.
C(σ2) −

2
K
K
X
k=1
log2 (1 + ek) −log2(e)
1
σ2K2
K
X
i,k=1
ak,i
(1 + ei)(1 + ek)

a.s.
−→0
where e1, . . . , eK are the only all positive solutions to
ei = 1
σ2
1
K
K
X
k=1
ak,i
1 + ek
.
(15.2)

15.1. Performance of multi-cell networks
377
We consider the (admittedly unrealistic) situation when the cells are uniformly
distributed on a linear array, following the Wyner model [Wyner, 1994]. The cells
are numbered in order, following the linear structure. The path loss from user k
to cell n has variance α|n−k|, for some ﬁxed parameter α. As such, A is explicitly
given by:
A =







1
α α2 · · · αK−1
α
1
α · · · αK−2
...
... ... ...
...
αK−2 · · · α
1
α
αK−1 · · · α2 α
1







.
In the case where α is much smaller than one, we can approximate the Toeplitz
matrix A by a matrix with all terms αn replaced by zeros, for all n greater than
some integer L, of order O(K) but strictly less than K/2. In this case, the
resulting Toeplitz matrix is a band matrix, which in turn can be approximated
for large dimensions by an equivalent circulant matrix ¯A, whose entries are the
same as A in the 2L main diagonals, and which is made circulant by ﬁlling the
upper right and lower left matrix corners accordingly. This must nonetheless
be performed with extreme care, following the conditions of Szeg¨o’s theorem,
Theorem 12.1. The matrix ¯A, in addition to being circulant, has the property to
have all sums in rows and columns equal. This matrix is referred to as a doubly
regular variance proﬁle matrix. In this particular case, it is shown in [Tulino and
Verd´u, 2005] that the asymptotic eigenvalue distribution of the matrix
1
1
K
PK
k=1 ¯aki
HHH
with ¯aki the entry (k, i) of ¯A, is the Mar˘cenko–Pastur law. To observe this fact,
it suﬃces to notice that the system of Equations (15.2), with aij replaced by ¯aij,
is solved for e1 = . . . = eK = f, with f deﬁned as the only Stieltjes transform
solution to
f = 1
σ2
1
K
K
X
k=1
¯ak,i
1 + f
for all i. This is easily veriﬁed as PK
k=1 ¯ak,i is constant irrespective of the choice
of the row i. Now, f is also the solution of (15.2) if all ¯aki were taken to be
identical, all equal to
1
K
PK
k=1 ¯aki for any i, in which case H would be a matrix
with i.i.d. entries of zero mean and variance
1
K
PK
k=1 ¯aki. Hence the result.
We therefore ﬁnally have, analogously to the uncorrelated antenna case in
Chapter 13, that C can be approximated explicitly as
C(σ2) −

log2

1 + δ + s
σ2

+ log2(e)
σ2
s δ −1

a.s.
−→0

378
15. Performance of multi-cellular and relay networks
−5
0
5
10
15
20
25
30
0
1
2
3
4
SNR [dB]
Achievable rate
α = 0.5
α = 0.2
α = 0.1
Figure 15.4 Multi-cell site capacity performance for K = 64 cell sites distributed on a
linear array. Path loss decay parameter α = 0.5, α = 0.2, α = 0.1.
where δ is given by:
δ = 1
2
"r
1 + 4s
σ2 −1
#
and where we denoted s ≜1
K
PK
k=1 ¯ak1.
In Figure 15.4, we provide the deterministic equivalents for the capacity C for
diﬀerent values of α, and for K = 64. The observed gain in capacity achieved by
inter-cell cooperation when cells are rather close encourages a shift towards inter-
cell cooperation, although in this particular example we did not consider at all the
inherent problems of such communications: base station synchronization, cost of
simultaneous data decoding, and more importantly feedback cost necessary for
the joint base station to learn the various (possibly fast varying) communication
channels.
In the next section, we turn to the analysis of multi-hop multiple antenna
communications assuming the number of antennas per relay is large.
15.2
Multi-hop communications
Relay communications are often seen as a practical solution to extend the
coverage of multi-cellular networks to remote areas. In this context, the main
interest is focused on two-hop relaying, where the relays are forwarding directly
the information from the source to the receiver. In the simple scenario where the
relay only ampliﬁes and forwards the data of the source, the receiver captures
identical signals, with possibly some delay, so that the overall system can be

15.2. Multi-hop communications
379
roughly modeled as a multiple antenna channel. However, information theory
requires that more intelligent processing be done at the relay than enhancing and
forwarding both received signal and noise. In particular, decoding, re-encoding,
and forwarding the information at the relay is a better approach in terms
of achievable rate, but is much more diﬃcult to analyze. This becomes even
more diﬃcult as the number of simultaneously transmitting relays increases.
Simple models are therefore often called for when studying these scenarios. Large
dimensional random matrix theory is particularly helpful here. Increasing the
number of relays in the multi-hop model has diﬀerent applications, especially for
ad-hoc networks, e.g. intended for military ﬁeld operations.
We will study in this section the scenario of multiple hops in a relay network
assuming a linear array composed of a source, K −1 relays, and a destination.
Each hop between relay pairs or source to ﬁrst relay will be assumed noise-free for
simplicity, while the last hop will experience additive noise. Each communication
entity will be assumed to be equipped with a large number of antennas and to
receive signals only from the last backward relay or source; we therefore assume
the simple scenario where distant relays do not interfere with each other. At
each hop, the communication strategy is to re-encode the receive data by a
linear precoder in a hybrid manner between the amplify and forward and the
decode and forward strategies. The study of the achievable rates in this setup will
therefore naturally call for the analysis of successive independent channel matrix
products. Therefore, tools such as the S-transform, and in particular Theorem
4.7, will be extremely useful here.
The following relies heavily on the work of M¨uller [M¨uller, 2002] on the capacity
of large dimensional product channels and on the work of Fawaz et al. [Fawaz
et al., 2011].
15.2.1
Multi-hop model
Consider a multi-hop relaying system with a source, K −1 relays, and a
destination. The source is equipped with N0 antennas, the destination with
NK antennas and the kth relay level with Nk antennas. We assume that the
noise power is negligible at all relays, while at the destination, at all times l, an
additive Gaussian noise vector w(l) ∈CNK with zero mean and covariance σ2INK
is received. In eﬀect, the simplifying assumption of noise-free relays is made to
have a white aggregate noise at the destination and consequently more tractable
derivations. Note that several works have implicitly used a similar noise-free relay
assumption by assuming that the noise at the destination of a multiple antenna
multi-hop relay network is white. In [Yang and Belﬁore, 2008], the authors prove
that in an amplify and forward multi-hop relay system the resulting colored
noise at the destination can be well approximated by white noise in the high
SNR regime. In terms of practical relevance, the mutual information expression
derived in the case of noise-free relays can be seen as an upper-bound for the
case of noisy relays.

380
15. Performance of multi-cellular and relay networks
We further assume that the channel matrix Hk at hop k ∈{1, . . . , K} (hop
k is from relay k −1 to relay k, with relay zero the source and relay K the
destination) is a random realization of a Kronecker channel model
Hk = R
1
2
k XkT
1
2
k
where Tk ∈CNk−1 and Rk ∈CNk are the transmit and receive correlation
matrices, respectively, Xk ∈CNk×Nk−1 has i.i.d. Gaussian entries of zero mean
and variance ak/Nk−1, where ak = d−β
k
represents the long-term path loss
attenuation with β and dk the path loss exponent and the length of the kth
hop, respectively.
Note that, by adapting the correlation matrix structures, the Kronecker model
can be used to model relay-clustering. Given a total number of antennas Nk at
relaying level k, instead of considering that the relaying level consists of a single
relay equipped with Nk antennas, we can consider that a relaying level contains
rk relays equipped with (Nk/rk) antennas each. Clustering has a direct impact
on the structure of correlation matrices; when the Nk antennas at level k are
distributed among several relays, correlation matrices become block diagonal
matrices, whose blocks represent the correlation between antennas at a relay,
while antennas at diﬀerent relays suﬃciently separated in space are supposed
uncorrelated. If each relaying level k contains Nk relays equipped with a single
antenna each, we fall back to the case of uncorrelated fading with correlation
matrices equal to identity.
Within one channel coherence block, the signal transmitted by the N0 source
antennas at time l is given by the vector s(l)
0
= P0y(l−1)
0
, where P0 is the source
precoding matrix and y(l)
0
is a random vector with zero mean and covariance
E[y(l)
0 y(l)H
0
] = IN0. This implies E[s(l)
0 s(l)H
0
] = P0PH
0 . Assuming that relays work
in full-duplex mode, at time l the relay at level k uses a precoding matrix Pk to
linearly precode its received signal y(l−1)
k
= Hks(l−1)
k−1 and to form its transmitted
signal (we remind that these channels are noise-free)
s(l)
k = Pky(l−1)
k
.
The precoding matrices Pk, k ∈{0, . . . , K −1}, at source and relays are
moreover subject to the per-node power constraints
1
Nk
tr(E[s(l)
k s(l)H
k
]) ≤Pk.
(15.3)
This scenario is depicted in Figure 15.5.
It should be noticed that choosing diagonal precoding matrices would reduce
the above scheme to the simpler amplify and forward relaying strategy. Also, the
proposed linear precoding relaying technique is adapted for high SNR regimes,
but not for low SNR regimes. In the low SNR regime, the performance of
the relay system is imposed by the noise ﬁgure and linear precoding performs
poorly because power is wasted on forwarding noise; other relaying strategies
such as decode and forward are more appropriate in this case, see, e.g., [Fawaz

15.2. Multi-hop communications
381
Figure 15.5 Linear relay network.
and M´edard, 2010; Maric and Yates, 2010]. On the contrary, in the high SNR
regime, linear precoding techniques such as amplify and forward perform well
[Borade et al., 2007; Maric et al., 2010]. Finally, from a practical point of
view, limited channel knowledge and simple linear precoding techniques at
relays are particularly relevant for systems where relays have limited processing
capabilities.
The signal received at the destination at time l is given by:
y(l)
K = HKPK−1HK−1PK−2 . . . H2P1H1P0y(l−K)
0
+ w(l)
= GKy(l−K)
0
+ w(l)
where the end-to-end equivalent channel GK is given by:
GK ≜HKPK−1HK−1PK−2 . . . H2P1H1P0
= R
1
2
KXKT
1
2
KPK−1R
1
2
K−1XK−1T
1
2
K−1PK−2 . . . R
1
2
2 X2T
1
2
2 P1R
1
2
1 X1T
1
2
1 P0.
For clarity in what follows, let us introduce the notations
M0 = T
1
2
1 P0
Mk = T
1
2
k+1PkR
1
2
k , k ∈{1, . . . , K −1}
MK = R
1
2
K.
Then GK can be rewritten as
GK = MKXKMK−1XK−1 . . . M2X2M1X1M0.
In what follows, we will always assume that the destination knows perfectly
the channel GK at all times in order to decode the source data. We may
additionally assume that the source and the relays have statistical channel state

382
15. Performance of multi-cellular and relay networks
information about the backward and forward channels, i.e. relay k knows the
receive correlation matrix Rk and the transmit correlation matrix Tk+1.
15.2.2
Mutual information
Consider the channel realization GK in one channel coherence block. Under the
assumption that the destination knows GK perfectly, the instantaneous end-
to-end mutual information between the channel input y0 and channel output
(yK, GK) in this channel coherence block is the same as the mutual information
of a multiple antenna channel given by Telatar [Telatar, 1999] as in Chapter 13
by
log det

INK + 1
σ2 GKGH
K

.
The end-to-end mutual information averaged over multiple channel realizations
is in turn given by:
E(Xk)

log det

INK + 1
σ2 GKGH
K

where the expectation is taken over the joint realization of the K −1 random
matrices Xk, k ∈{1, . . . , K −1}.
As in Chapter 13, we will not be able to optimize the instantaneous mutual
information, but will rather focus on optimizing the average mutual information,
when the diﬀerent relays have at least statistical information about the channels
Hk. Under adequate assumptions on the various channel state information known
at the relays, we will therefore try to ﬁnd the precoders Pk that maximize the
end-to-end mutual information subject to power constraints (15.3). This will give
us the end-to-end average mutual information
C(σ2) ≜
sup
{Pk}
1
Nk tr(E[sksH
k])≤Pk
E(Xk)

log det

INK + 1
σ2 GKGH
K

.
Note that the average mutual information above does not necessarily represent
the channel capacity in the Shannon sense here. In the next section, we will derive
a limiting result for the mutual information using tools from free probability
theory.
15.2.3
Large dimensional analysis
In this section, we consider the instantaneous mutual information per source
antenna between the source and the destination and derive its asymptotic value
as N0, N1, . . . , NK grow large at a similar rate. We obtain the following result.
Theorem 15.1. For the system described above, and under the assumption that
the destination knows GK at all times, that Nk
NK →ck, 0 < ck < ∞, and MH
k Mk

15.2. Multi-hop communications
383
has an almost sure l.s.d. Fk and has uniformly bounded spectral norm along
growing Nk, for all k, then the normalized (per source antenna) instantaneous
mutual information
I ≜1
N0
log det

INK + 1
σ2 GKGH
K

converges almost surely to
I∞=
K
X
k=0
ci
c0
Z
log

1 + 1
σ2
ak+1
ck
hK
k t

dFk(t) −K σ2
c0
K
Y
k=0
hk
where aK+1 = 1 by convention and h0, h1, . . . , hK are the solutions of
K
Y
j=0
hj =
Z
ak+1hK
k t
1 +
1
σ2
ak+1
ck hK
k tdFk(t).
We give hereafter the main steps of the proof.
Proof. The proof of Theorem 15.1 consists of the following steps:
• Step 1. We ﬁrst obtain an expression for the limiting S-transform SG(z)
of GKGH
K using the fact that the matrix GK is composed of products of
asymptotically almost everywhere free matrices.
• Step 2. We then use SG(z) to determine the limiting ψ-transform ψG(z) of
GKGH
K, which we recall is closely related to the Stieltjes transform of GKGH
K,
see Deﬁnition 3.6.
• Step 3. We ﬁnally use the relation between the ψ-transform and the Shannon
transform to complete the derivation.
Step 1.
We show ﬁrst the following result. As all Nk →∞with the same rate, the S-
transform of GKGH
K converges almost surely to SG(z), given by:
SG(z) = SFK(z)
K
Y
k=1
ck−1
ak
1
(z + ck−1)SFk−1
 z
ck−1

.
(15.4)
The proof is done by induction. First, we prove the case K = 1. Note that
G1GH
1 = M1X1M0MH
0 XH
1 MH
1
and therefore, denoting systematically S∞
Z (z) the limiting almost sure S-
transform of the random Hermitian matrix Z as the dimensions grow to inﬁnity
SG(z) = S∞
X1M0MH
0XH
1MH
1M1(z)
thanks to the S-transform matrix exchange identity, Lemma 4.3. Then, from the
asymptotic freeness almost everywhere of Wishart matrices and deterministic

384
15. Performance of multi-cellular and relay networks
matrices, Theorem 4.5, and the S-transform product relation, Theorem 4.7, we
further have
SG(z) = S∞
X1M0MH
0XH
1(z)SF1(z).
Using again Lemma 4.3, we exchange the order in matrix X1M0MH
0 XH
1 to obtain
SG(z) = z + 1
z + N0
N1
S∞
M0MH
0XH
1X1

z N1
N0

SF1(z).
A second application of Theorem 4.7 gives
SG(z) = z + 1
z + N0
N1
SF0

z N1
N0

S∞
XH
1X1

z N1
N0

SF1(z)
where we recognize the S-transform of (a scaled version of) the Mar˘cenko–Pastur
law. Applying both Lemma 4.2 and Theorem 4.8, we obtain
SG(z) = z + 1
z + N0
N1
SF0

z N1
N0
 1
a1
1
z N1
N0 + N1
N0
SF1(z)
and ﬁnally
SG(z) = SF1(z) c0
a1
1
z + c0
SF0
 z
c0

which proves the case K = 1.
Now, we need to prove that, if the result holds for K = q, it also holds for
K = q + 1. Note that
Gq+1GH
q+1 = Mq+1Xq+1MqXq . . . M1X1M0MH
0 XH
1 MH
1 . . . XH
q MH
q XH
q+1MH
q+1.
Therefore
SGq+1GH
q+1(z) = SXq+1Mq...MHqXH
q+1MH
q+1Mq+1(z).
We use once more the same approach and theorems as above to obtain
successively, with K = q + 1
SG(z) = S∞
Xq+1...XH
q+1(z)SMH
q+1Mq+1(z)
=
z + 1
z +
Nq
Nq+1
S∞
Mq...MHqXH
q+1Xq+1

z Nq+1
Nq

SFq+1(z)
=
z + 1
z +
Nq
Nq+1
SFq

z Nq+1
Nq

S∞
XH
q+1Xq+1

z Nq+1
Nq

SFq+1(z).

15.2. Multi-hop communications
385
As above, developing the expression of the S-transform of the Mar˘cenko–
Pastur law, we then obtain
SG(z) =
z + 1
z +
Nq
Nq+1
SFq

z Nq+1
Nq
 

q
Y
i=1
1
ai
Ni−1
Nq
1
z Nq+1
Nq
+ Ni−1
Nq
SFi−1

z Nq+1
Ni−1


×
1
aq+1
1
Nq+1
Nq
+ z Nq+1
Nq
SFq+1(z)
which further simpliﬁes as
SG(z) =
z + 1
z +
Nq
Nq+1
SFq+1(z)
1
aq+1
Nq
Nq+1
1
z + 1SFq

z Nq+1
Nq

×
q
Y
i=1
Ni−1
Nq+1
ai
1
z + Ni−1
Nq+1
SFi−1

z Nq+1
Ni−1

= SFq+1(z)
q+1
Y
i=1
1
ai
Ni−1
Nq+1
1
z + Ni−1
Nq+1
SFi−1

z Nq+1
Ni−1

= SFq+1(z)
q+1
Y
i=1
ci−1
ai
1
(z + ci−1)SFi−1
 z
ci−1

which is the intended result.
Step 2.
We now prove the following. Denoting aK+1 = 1, for s ∈C \ R+, we have:
s(ψG(s))K =
K
Y
i=0
ci
ai+1
ψ−1
Fi
ψG(s)
ci

.
(15.5)
This unfolds ﬁrst from (15.4), by multiplying each side by z/(z + 1)
z
z + 1SG(z) =
z
z + 1SFK(z)
K
Y
k=1
ck−1
ak
1
(z + ck−1)SFk−1
 z
ck−1

where the last right-hand side term can be rewritten
SFk−1
 z
ck−1

=
1 +
z
ck−1
z
ck−1
z
ck−1
1 +
z
ck−1
SFk−1
 z
ck−1

.
Using the free probability deﬁnition of the S-transform in relation to the ψ-
transform, Theorem 4.3, we obtain
ψ−1
G (z) = 1
zK ψ−1
FK(z)
K
Y
i=1
ci−1
ai
ψ−1
Fi−1
 z
ci−1


386
15. Performance of multi-cellular and relay networks
or equivalently
ψ−1
G (z) = 1
zK
K
Y
i=0
ci
ai+1
ψ−1
Fi
 z
ci

.
Substituting z = ψG(s) gives the result.
Step 3.
We subsequently show that, as N0, N1, . . . , NK go to inﬁnity, the derivative of
the almost surely limiting instantaneous mutual information I∞is given by:
dI∞
d(σ−2) = 1
c0
K
Y
i=0
hi
where h0, h1, . . . , hK are the solutions to the following
K
Y
j=0
hj = ci
Z
hK
i t
ci
ai+1 +
1
σ2 hK
i tdFi(t).
First, we note that
I = 1
N0
log det

INK + 1
σ2 GKGH
K

a.s.
−→1
c0
Z
log

1 + 1
σ2 t

dG(t)
the convergence result being valid here because the largest eigenvalue of GKGH
K
is almost surely bounded as the system dimensions grow large. This is due to the
fact that the deterministic matrices MkMH
k are uniformly bounded in spectral
norm and that the largest eigenvalue of XkXH
k is almost surely bounded for
all Nk by Theorem 7.1; therefore, the largest eigenvalue of GKGH
K, which is
smaller than or equal to the product of all these largest eigenvalues is almost
surely uniformly bounded. The dominated convergence theorem, Theorem 6.3,
then ensures the convergence.
Now, we also have the relation
dI∞
d(σ−2) = 1
c0
Z
t
1 +
1
σ2 tdFG(t)
= −σ2
c0
ψG(−σ−2).
(15.6)
Let us denote
τ = ψG(−σ−2)
gi = ψ−1
Fi
 t
ci

.
From (15.6), we have:
τ = −c0
σ2
dI∞
d(σ−2).

15.2. Multi-hop communications
387
Substituting s = −σ−2 in (15.5) and using τ and gi above, it follows that
−τ K
σ2 =
K
Y
i=0
ci
ai+1
gi.
Using the deﬁnition of the ψ-transform, this is ﬁnally
τ = ci
Z
git
1 −gitdFi(t).
These last three equations together give

−1
σ2
K+1 
c0
dI∞
d(σ−2)
K
=
K
Y
i=0
ci
ai+1
gi
and
−1
σ2

c0
dI∞
d(σ−2)

= ci
Z
git
1 −gitdFi(t).
Deﬁning now
hi ≜
 ci
ai+1
 1
K  −giσ2 1
K
we have
c0
dI∞
d(σ−2) =
K
Y
i=0
hi.
(15.7)
Using again these last three equations, we obtain
−1
σ2
K
Y
j=0
hj = ci
Z
−1
σ2 hK
i
ai+1
ci t
1 −(−1
σ2 )hK
i
ai+1
ci tdFi(t)
or equivalently
N
Y
j=0
hj = ci
Z
hK
i t
ci
ai+1 +
1
σ2 hK
i tdFi(t).
This, along with Equation (15.7), gives the result.
We ﬁnally need to prove that the ﬁnal result is indeed a pre-derivative of
dI∞/dσ−2 that also veriﬁes limσ2→∞I∞= 0, i.e. the mutual information is zero
for null SNR. This unfolds, as usual, by diﬀerentiating the ﬁnal result. In

388
15. Performance of multi-cellular and relay networks
particular, we successively obtain
K
X
i=0
ci
Z t
 hK
i + K
σ2 hK−1
i
h′
i

ci
ai+1 (1 + ai+1
σ2ci hK
i t) dFi(t) −K
K
Y
i=0
hi −K
σ2


K
X
i=0
h′
i
Y
j̸=i
hj


=
K
X
i=0
ci
Z
thK
i dFi(t)
ci
ai+1 +
1
σ2 hK
i t + K
σ2
K
X
i=0
h′
i
hi
ci
Z
thK
i dFi(t)
ci
ai+1 +
1
σ2 hK
i t −K
K
Y
i=0
hi
−K
σ2


K
X
i=0
h′
i
hi
K
Y
j=0
hj


=
K
X
i=0
K
Y
j=0
hj + K
σ2


K
X
i=0
h′
i
hi
K
Y
j=0
hj

−K
K
Y
i=0
hi −K
σ2


K
X
i=0
h′
i
hi
K
Y
j=0
hj


= (K + 1)
K
Y
j=0
hj −K
K
Y
j=0
hj
=
K
Y
j=0
hj
where h′
i ≜
dhi
d(σ−2). This completes the proof.
Theorem 15.1 holds as usual for any arbitrary set of precoding matrices Pk,
k ∈{0, . . . , K −1} such that MH
k Mk has uniformly bounded spectral norm.
15.2.4
Optimal transmission strategy
In this section, we analyze the optimal linear precoding strategies Pk, k ∈
{0, . . . , K −1}, at the source and the relays that allow us to maximize the average
mutual information. We characterize the optimal transmit directions determined
by the singular vectors of the precoding matrices at source and relays, for a
system with ﬁnite dimensions N0, . . . , NK before considering large dimensional
limits.
The main result of this section is given by the following theorem.
Theorem 15.2. For each i ∈{1, . . . , K}, denote Ti = Ut,iΛt,iUH
t,i and Ri =
Ur,iΛr,iUH
r,i the spectral decompositions of the correlation matrices Ti and
Ri, where Ut,i and Ur,i are unitary and Λt,i and Λr,i are diagonal, with
their respective eigenvalues ordered in non-increasing order. Then, assuming the
destination knows GK at all times and that the source and intermediary relays
have local statistical information about the backward and forward channels, the
optimal linear precoding matrices that maximize the average mutual information

15.2. Multi-hop communications
389
under power constraints (15.3) can be written as
P0 = Ut,1ΛP0
Pi = Ut,i+1ΛPiUH
r,i, i ∈{1, . . . , K −1}
where ΛPi are diagonal matrices with non-negative real diagonal elements.
We do not provide here the proof of Theorem 15.2 that can be found in [Fawaz
et al., 2011, Appendix C]. Theorem 15.2 indicates that the power maximization
can then be divided into two phases: the alignment of the eigenvectors on the
one hand and the search for the optimal eigenvalues (entries of Λt,i and Λr,i) on
the other hand.
We now apply the above result to two speciﬁc multi-hop communication
scenarios. In these scenarios, a multi-hop multiple antenna system as above
is considered and the asymptotic mutual information is developed in the
uncorrelated and exponential correlation cases, respectively.
15.2.4.1 Uncorrelated multi-hop MIMO
In this example, we consider an uncorrelated multi-hop MIMO system, i.e. all
correlation matrices are equal to the identity matrix.
Before analyzing this scenario, we mention the following ﬁnite dimensional
result, which can be proved rather easily by induction on i.
tr(E[sisH
i ]) = ai tr(PiRiPH
i )
i−1
Y
k=0
ak
Nk
tr(Tk+1PkPH
k ).
(15.8)
By Theorem 15.2, in the uncorrelated case (Rk and Tk taken identity) the
optimal precoding matrices should be diagonal. Assuming equal power allocation
at source and relays, the precoding matrices are of the form Pk = αkINk,
where αk is real positive and chosen to satisfy the power constraints. Using
the expression (15.8), it can be shown by induction on k that the coeﬃcients αk
in the uncorrelated case are necessarily given by:
α0 =
p
P0
αi =
s
Pi
aiPi−1
, i ∈{1, . . . , K −1}
αK = 1.
(15.9)
Then the asymptotic mutual information for the uncorrelated multi-hop
MIMO system with equal power allocation is given by:
I∞=
K
X
i=0
ci
c0
log

1 + hK
i ai+1α2
i
σ2ci

−K σ2
c0
K
Y
i=0
hi
(15.10)

390
15. Performance of multi-cellular and relay networks
where h0, h1, . . . , hK are the solutions of the system of K + 1 multivariate
polynomial equations
K
Y
j=0
hj =
hK
i α2
i ai+1
1 + hK
i ai+1α2
i
σ2ci
.
15.2.4.2 Exponentially correlated multi-hop MIMO
In this second example, the asymptotic mutual information is developed in the
case of exponential correlation matrices and precoding matrices with singular
vectors as in Theorem 15.2. Similar to Jakes’ model, exponential correlation
matrices are a common model of correlation, particularly for uniform linear
antenna array, see, e.g., [Loyka, 2001; Martin and Ottersten, 2004; Oestges et al.,
2008].
We assume that the relay at level i is equipped with a uniform linear antenna
array of length Li, characterized by its antenna spacing li = Li/Ni and its
characteristic distances ∆t,i and ∆r,i proportional to transmit and receive spatial
coherences, respectively. Then the receive and transmit correlation matrices at
relaying level i can, respectively, be modeled by the following Hermitian Toeplitz
matrices
Ri =










1
ri r2
i . . . rNi−1
i
ri
1 ... ...
...
r2
i
... ... ...
r2
i
...
... ... 1
ri
rNi−1
i
. . . r2
i
ri
1










and
Ti+1 =










1
ti+1 t2
i+1 . . . tNi−1
i+1
ti+1
1
...
...
...
t2
i+1
...
...
...
t2
i+1
...
...
...
1
ti+1
tNi−1
i+1
. . . t2
i+1 ti+1
1










where the antenna correlation at receive and transmit sides read ri = e
−
li
∆r,i and
ti+1 = e
−
li
∆t,i , respectively. It can be veriﬁed that these Toeplitz matrices are of
Wiener-class thanks to Szeg¨o’s theorem, Theorem 12.1. We therefore know also
from Theorem 12.1 the limiting eigenvalue distribution of those deterministic
matrices as their dimensions grow large. We further assume here equal power
allocation over the optimal directions, i.e. the singular values of Pi are chosen
to be all equal: ΛPi = αiINi, where αi is real positive and chosen to satisfy the
power constraint (15.3). Equal power allocation may not be the optimal power
allocation scheme, but it is considered in this example for simplicity.

15.2. Multi-hop communications
391
Using the power constraint expression for general correlation models (15.8) and
considering precoding matrices Pi = UH
r,i(αiINi)Ut,i+1, with Ur,i unitary such
that Ri = Ur,iΛr,iUH
r,i with Λr,i diagonal and Ut,i such that Ti = Ut,iΛt,iUH
t,i
with Λt,i diagonal, following the conditions of Theorem 15.2 with equal singular
values αi, we can show by induction on i that the coeﬃcients αi respecting the
power constraints for any correlation model are now given by:
α0 =
p
P0
αi =
s
Pi
aiPi−1
tr(Λr,i−1)
tr(Λr,i)
ki
tr(Λt,iΛr,i−1), i ∈{1, . . . , K −1}
αK = 1.
Applying the exponential correlation model to the above relations and making
the dimensions of the system grow large, it can be shown that in the asymptotic
regime, the αi respecting the power constraint for the exponentially correlated
system converge to the same value, given in (15.9), as for the uncorrelated system.
It can then be shown that the asymptotic mutual information in this scenario is
given by:
I∞=
K
X
i=0
ci
c0π2
Z ∞
t=−∞
Z ∞
u=−∞
g(t, u; h)dtdu −K σ2
c0
K
Y
i=0
hi
where, denoting h = (h0, . . . , hK)
g(t, u; h) ≜
1
1 + t2
1
1 + u2 log
 
1 + ρr,iρt,i+1
hK
i ai+1α2
i
σ2ci
(1 + t2)
(ρ2
r,i + t2)
(1 + u2)
(ρ2
t,i+1 + u2)
!
and h0, h1, . . . , hK are the solutions of the system
K
Y
j=0
hj = 2
π
hK
i ai+1α2
i
q
ρr,iρt,i+1 + hK
i ai+1α2
i
σ2ci
q
1
ρr,iρt,i+1 + hK
i ai+1α2
i
σ2ci
F
π
2 , √mi

with F(θ, x) the incomplete elliptic integral of the ﬁrst kind given by:
F(θ, x) =
Z θ
0
1
p
1 −x2 sin2(t)
dt
and, for all i ∈{0, K}
ρr,i = 1 −ri
1 + ri
ρt,i+1 = 1 −ti+1
1 + ti+1
mi = 1 −

ρt,i+1
ρr,i
+ hK
i ai+1α2
i
σ2ci
 
ρr,i
ρt,i+1 + hK
i ai+1α2
i
σ2ci


1
ρr,iρt,i+1 + hK
i ai+1α2
i
σ2ci
 
ρr,iρt,i+1 + hK
i ai+1α2
i
σ2ci


392
15. Performance of multi-cellular and relay networks
with the convention r0 = tN+1 = 0. The details of these results can be found in
[Fawaz et al., 2011].
This concludes this chapter on the performance of multi-cellular and relay
communication systems. In the subsequent chapters, the signal processing
problems of source detection, separation, and statistical inference for large
dimensional systems are addressed.

16
Detection
In this chapter, we now address a quite diﬀerent problem than the performance
evaluation of data transmissions in large dimensional communication channel
models. The present chapter, along with Chapter 17, deals with practical signal
processing techniques to solve problems involving (possibly large dimensional)
random matrix models. Speciﬁcally in this chapter, we will ﬁrst address the
question of signal sensing using multi-dimensional sensor arrays.
16.1
Cognitive radios and sensor networks
A renewed motivation for large dimensional signal sensing has been recently
triggered by the cognitive radio incentive, which, according to some, may be
thought of as the next information-theoretic revolution after the original work
of Shannon [Shannon, 1948] and the introduction of multiple antenna systems
by Foshini [Foschini and Gans, 1998] and Telatar [Telatar, 1999]. In addition
to the theoretical expression of the point-to-point noisy channel capacity in
[Shannon, 1948], Shannon made us realize that, in order to achieve high
rate of information transfer, increasing the transmission bandwidth is largely
preferred over increasing the transmission power. Therefore, to ensure high
rate communications with a ﬁnite power budget, we have to consider frequency
multiplexing. This constituted the ﬁrst and most important revolution in modern
telecommunications and most notably wireless communications, which led today
to an almost complete saturation of all possible transmission frequencies. By
“all possible,” we mean those frequencies that can eﬃciently carry information
(high frequencies tend to be rapidly attenuated when propagating in the
atmosphere) and be adequately processed by analog and digital devices
(again, high frequency carriers require expensive and sometimes even physically
infeasible radio front-ends). Foschini and Telatar brought forward the idea of
multiplexing the information, not only through orthogonal frequency bands, but
also in the space domain, by using spatially orthogonal propagation paths in
multi-dimensional channels. As we saw in the previous chapter, though, this
assumed orthogonality only holds for fairly unrealistic communication channels
(very scattered propagation environments ﬁlled with objects the size of the
transmission wavelength, very distant transmit and receive antennas, etc.). Also,

394
16. Detection
the multiple antenna multiplexing gain is only apparent for high signal-to-noise
ratios, which is inconsistent with most contemporary interference limited cellular
networks. We also discussed in Chapter 15 the impracticality of large cooperative
networks which require a huge load of channel state information feedback. The
cognitive radio incentive, initiated with the work of Mitola [Mitola III and
Maguire Jr, 1999], follows the same idea of communication resource harvesting.
That is, cognitive radios intend to communicate not by exploiting the over-used
frequency domain, or by exploiting the over-used space domain, but by exploiting
so-called spectrum holes, jointly in time, space, and frequency. The basic idea is
that, while the time, frequency, and spatial domains are over-used in the sense
that telecommunication service providers have already bought all frequencies and
have already placed base stations, access points, and relays to cover most areas,
the eﬀectively delivered communication service is largely discontinuous. That
is, the telecommunication networks do not operate constantly in all frequency
bands, at all times, and in all places at the maximum of their deliverable
capacities. Multiple situations typically arise.
• A licensed network is under-used over a given period of time. This is typically
the case during night-time, when little large range telecommunication service
is provided.
• The frequency band exploited by a network is left free of use or the
delivered content is not of interest to potential receivers. This is the case
of broadcast television, whose frequency multiplexed channels are not all used
simultaneously at any given space location.
• A licensed network is not used locally. This arises whenever no close user is
found in a given space area, where a licensed network is operating.
• A licensed network is used simultaneously on all resource dimensions, but the
users’ service request induces transmission rates below the channel capacity.
This arises when for instance a wideband CDMA network is used for a single-
user voice call. The single-user is clearly capable, in the downlink, of decoding
the CDMA stream with few errors, even if it were slightly interfered with
by overlaying communications, since the CDMA code redundancy induces
resistence against interfering with data streams.
The concept of cognitive radios covers a very large framework, not clearly
uniﬁed to this day, though, which intends to reuse spectrum left-overs (or holes),
four examples of which were given above. A cognitive radio network can be
described as an autonomous network overlaying one or many existing legacy
networks. While the established networks have dedicated bandwidths and spatial
planning to operate, cognitive radios are not using any licensed resource, be it in
space, time, or frequency. Cognitive radios are however free to use any licensed
spectrum, as long as by doing so they do not dramatically interfere with the
licensed networks. That is, they are able to reuse the spectrum left unused by so-
called primary networks whenever possible, while generating minimum harm to

16.1. Cognitive radios and sensor networks
395
the on-going communications. Considering the four examples above, a cognitive
radio network, also called secondary network, could operate, respectively:
• on a given time–frequency–space resource when no communication is found
to take place in the licensed frequencies for a certain amount of time;
• if the delivered data content is not locally used, by overlaying the (now
interfering) network transmissions;
• if the delivered data content is intended for some user but the cognitive radio
is aware that this user is suﬃciently far away, by again overlaying the locally
unused spectrum;
• by intentionally interfering with the established network but using a
suﬃciently low transmit power that still allows the licensed user to decode
its own data.
Now,
what
makes
the
secondary
network
cognitive
is
that
all
the
aforementioned ways of action require constant awareness of the operations
taking place in the licensed networks. Indeed, as it is an absolute necessity not to
interfere with the licensed users, some sort of dynamic monitoring, or information
feedback, is required for the secondary network to abide by the rules. Since
secondary networks are assumed to minimally impact on the networks in place,
it is a conventional assumption to consider that the licensed networks do not pro-
actively deliver network information to the cognitive radio. It is even conventional
to assume that the licensed networks are completely oblivious of the existence of
potential interferers. Therefore, legacy telecommunication networks need not be
restructured in order to face the interference of the new secondary networks. As a
consequence, all the burden is placed on the cognitive radio to learn about its own
environment. This is relatively easy when dealing with surrounding base stations
and other ﬁxed transmitters, as much data can be exploited in the long-term,
but this is not so for mobile users. Service providers sometimes do not transmit
at all (apart from pilot data), in which case secondary networks can detect a
spectrum hole and exploit it. However, the real gain of cognitive radios does not
come solely from beneﬁting from completely unused access points, but rather
from beneﬁting from overlaying on-going communications while not aﬀecting
the licensed users. A classical example is that of a mobile phone network, which
can cover an area as large as a few kilometers. In day-time, it is uncommon for
a given base station never to be in use (for CDMA transmissions, remember
that this means that the whole spectrum is then used at once), but it is also
uncommon that the users communicating with this base station are always
located close to a secondary network. A cognitive radio can always overlay the
data transmitted by an operating base station if the user, located somewhere in
a large area, is not found to be anywhere close to the cognitive network. For in-
house cognitive radios, such as femto-cells in closed access (see, e.g., [Calin et al.,
2010; Chandrasekhar et al., 2009; Claussen et al., 2008], it can even be assumed
that overlaying communication can take place almost continuously, as long as no

396
16. Detection
user inside the house or in neighboring houses establishes a communication with
this network.
The question of whether an active user is to be found in the vicinity of a
cognitive radio is therefore of prior importance to establish reliable overlaying
communications in cognitive radios. For that, a cognitive radio needs to be able
to sense neighboring users in active transmissions. This can be performed by
simple energy detection, as in the original work from Urkowitz [Urkowitz, 1967].
However, energy detection is meant for single antenna transmitters and receivers
under additive white Gaussian noise conditions and does therefore not take
into account the possibility of joint processing at the sensor network level in
MIMO fading channel conditions. In this chapter, we will investigate the various
approaches brought by random matrix theory to perform signal detection as
reliably as possible. We will ﬁrst investigate the generalization of the Urkowitz
approach to multiple sources and multiple receivers under a small dimensional
random matrix approach. The rather involved result we will present will then
motivate large dimensional random matrix analysis. Most notably, approaches
that require minimum a priori knowledge of the environment will be studied from
a large dimensional perspective. Indeed, it must be assumed that the cognitive
radio is completely unaware even of the expected received signal-to-noise ratio
in a given frequency band, as it exactly intends to decide whether only noise or
informative signals are received within this band.
Before getting into random matrix applications, let us model the signal sensing
problem.
16.2
System model
We consider a communication network composed of K transmitting sources, e.g.
this can be either a K-antenna transmitter or K single antenna (not necessarily
uncorrelated) information sources, and a receiver composed of N sensors, be they
the uncorrelated antennas of a single terminal or a mesh of scattered sensors,
similar to the system model exploited in, e.g., [Cabric et al., 2006; Ghasemi
and Sousa, 2005, 2007; Mishra et al., 2006; Sun et al., 2007a,b; Wang et al.,
2010; Zhang and Letaief, 2008]. To enhance the multiple antenna (MIMO) model
analogy, the set of sources and the set of sensors will be collectively referred to
as the transmitter and the receiver, respectively. The communication channel
between the transmitter and the receiver is modeled by the matrix H ∈CN×K,
with (i, j)th entry hij. If at time l the transmitter emits data, those are denoted
by the K-dimensional vector x(l) = (x(l)
1 , . . . , x(l)
K )T ∈CK and are assumed
independent across time. The additive white Gaussian noise at the receiver is
modeled, at time l, by the vector σw(l) = σ(w(l)
1 , . . . , w(l)
N )T ∈CN, where σ2
denotes the variance of the noise vector entries, again assumed independent
across time. Without generality restriction, we consider in the following zero

16.2. System model
397
Figure 16.1 A cognitive radio network under hypothesis H0, i.e. no close user is
transmitting during the exploration period.
mean and unit variance of the entries of both w(l) and x(l), i.e. E[|w(l)
i |2] = 1,
E[|x(l)
i |2] = 1 for all i. We then denote y(l) = (y(l)
1 , . . . , y(l)
N )T the N-dimensional
data received at time l. Assuming the channel coherence time is at least as long as
M sampling periods, we ﬁnally denote Y = [y(1), . . . , y(M)] ∈CN×M the matrix
of the concatenated receive i.i.d. vectors.
Depending on whether the transmitter emits informative signals, we consider
the following hypotheses.
• H0. Only background noise is received.
• H1. Informative signals plus background noise are received.
Both scenarios of cognitive radio networks under hypotheses H0 and H1 are
depicted in Figure 16.1 and Figure 16.2, respectively. Figure 16.1 illustrates the
case when users neighboring the secondary network are not transmitting, while
Figure 16.2 illustrates the opposite situation when a neighboring user is found
to transmit in the frequency resource under exploration.
Therefore, under condition H0, we have the model
Y = σW
with W = [w(1), . . . , w(M)] ∈CN×M and under condition H1
Y =
 H σIN
  X
W

(16.1)
with X = [x(1), . . . , x(M)] ∈CN×M.
Under this hypothesis, we further denote Σ the covariance matrix of y(1)
Σ = E[y(1)y(1)H] = HHH + σ2IN = UGUH

398
16. Detection
Figure 16.2 A cognitive radio network under hypothesis H1, i.e. at least one close user
is transmitting during the exploration period.
where
G = diag
 ν1 + σ2, . . . , νN + σ2
∈RN×N,
with
{ν1, . . . , νN}
the
eigenvalues of HHH and U ∈CN×N a certain unitary matrix.
The receiver is entitled to decide whether the primary users are transmitting
informative signals or not. That is, the receiver is required to test the hypothesis
H0 against the hypothesis H1. The receiver is however considered to have
very limited information about the transmission channel and is in particular
not necessarily aware of the exact number K of sources and of the signal-to-
noise ratio. For this reason, following the maximum entropy principle [Jaynes,
1957a,b], we seek a probabilistic model for the unknown variables, which is
both (i) consistent with the little accessible prior information available to
the sensor network and (ii) has maximal entropy over the set of densities
that validate (i). Maximum entropy considerations, which we do not develop
here, are further discussed in Chapter 18, as they are at the core of the
channel models developed in this chapter. We therefore admit for the time
being that the entropy maximizing probability distribution of a random vector,
the knowledge about which is limited to its population covariance matrix, is
a multivariate Gaussian distribution with zero mean and covariance matrix
the known population covariance matrix. If the population covariance matrix
is unknown but is known to be of unit trace, then the entropy maximizing
distribution is now multivariate independent Gaussian with zero mean and
normalized identity covariance matrix. Therefore, if the channel matrix H is only
known to satisfy, as is often the case in the short-term, E[ 1
N tr HHH] = E, with
E the total power carried through the channel, the maximum entropy principle
states that the entries hij should be modeled as independent and all Gaussian
distributed with zero mean and variance E/K. For the same reason, both noise
w(l)
i
and signal x(l)
i
entries are taken independent Gaussian with zero mean and

16.3. Neyman–Pearson criterion
399
variance E[|w(l)
i |2] = 1, E[|x(l)
i |2] = 1. Obviously, the above scalings depend on
the deﬁnition of the signal-to-noise ratio.
Now that the model is properly deﬁned, we turn to the question of testing
hypothesis H0 against hypothesis H1. The idea is to decide, based on the
available prior information and upon observation of Y, whether H0 is more
likely than H1. Instead of exploiting structural features of the signal, such as
cyclostationarity as in, e.g., [Enserink and Cochran, 1994; Gardner, 1991; Kim
and Shin, 2008], we consider here the optimal Neyman–Pearson decision test.
This is what we study ﬁrst in the following (Section 16.3) under diﬀerent prior
information on all relevant system parameters. We will realize that the optimal
Neyman–Pearson test, be it explicitly derivable for the model under study, leads
nonetheless to very involved formulations, which cannot ﬂexibly be extended to
more involved system models. We will therefore turn to simpler suboptimal tests,
whose behavior can be controlled based on large dimensional analysis. This is
dealt with in Section 16.4.
16.3
Neyman–Pearson criterion
The Neyman–Pearson criterion [Poor, 1994; Vantrees, 1968] for the receiver to
establish whether an informative signal was transmitted is based on the ratio
C(Y) = PH1|Y(Y)
PH0|Y(Y)
(16.2)
where, following the conventions of Chapter 2, PHi|Y(Y) is the probability of the
event Hi conditioned on the observation Y. For a given receive space–time matrix
Y, if C(Y) > 1, then the odds are that an informative signal was transmitted,
while if C(Y) < 1, it is more likely that no informative signal was transmitted
and therefore only background noise was captured. To ensure a low probability
of false alarms (or false positives), i.e. the probability of declaring a pure noise
sample to carry an informative signal, a certain threshold ξ is generally set such
that, when C(Y) > ξ, the receiver declares an informative signal was sent, while
when C(Y) < ξ, the receiver declares that no informative signal was sent. The
question of what ratio ξ to be set to ensure a given maximally acceptable false
alarm rate will not be treated in the following. We will however provide an
explicit expression of (16.2) for the aforementioned model, and will compare its
performance to that achieved by classical detectors. The results provided in this
section are borrowed from [Couillet and Debbah, 2010a].
Thanks to Bayes’ rule, (16.2) becomes
C(Y) = PH1 · PY|H1(Y)
PH0 · PY|H0(Y)
with PHi the a priori probability for hypothesis Hi to hold. We suppose that no
side information allows the receiver to consider that H1 is more or less probable

400
16. Detection
than H0, and therefore set PH1 = PH0 = 1
2, so that
C(Y) = PY|H1(Y)
PY|H0(Y)
(16.3)
reduces to a maximum likelihood ratio.
In the next section, we derive closed-form expressions for C(Y) under the
hypotheses that the values of K and the SNR, that we deﬁne as 1/σ2, are either
perfectly or only partially known at the receiver.
16.3.1
Known signal and noise variances
16.3.1.1 Derivation of PY|Hi in the SIMO case
We ﬁrst analyze the situation where the noise power σ2 and the number K of
signal sources are known to the receiver. We also assume in this ﬁrst scenario that
K = 1. Since it is a common assumption that the number of available samples at
the receiver is larger than the number of sensors, we further consider that M > N
and N ≥2 (the case N = 1 is already known to be solved by the classical energy
detector [Kostylev, 2002]).
Likelihood under H0.
In this ﬁrst scenario, the noise entries w(l)
i
are Gaussian and independent. The
probability density of Y, that can be seen as a random vector with NM entries,
is then an NM multivariate uncorrelated complex Gaussian with covariance
matrix σ2INM
PY|H0(Y) =
1
(πσ2)NM e−1
σ2 tr YYH.
(16.4)
Denoting λ = (λ1, . . . , λN)T the eigenvalues of YYH, (16.4) only depends on
PN
i=1 λi as follows.
PY|H0(Y) =
1
(πσ2)NM e−1
σ2
PN
i=1 λi.
Likelihood under H1.
Under the information plus noise hypothesis H1, the problem is more involved.
The entries of the channel matrix H were previously modeled as jointly
uncorrelated Gaussian, with E[|hij|2] = E/K. From now on, for simplicity, we
take E = 1 without loss of generality. Therefore, since here K = 1, H ∈CN×1
and Σ = HHH + σ2IN has N −1 eigenvalues g2 = . . . = gN equal to σ2 and
another distinct eigenvalue g1 = ν1 + σ2 = (PN
i=1 |hi1|2) + σ2. Since the |hi1|2
are the sum of two Gaussian independent variables of zero mean and variance 1
2
(the real and imaginary parts of hij), 2(g1 −σ2) is a χ2
2N distribution. Hence,

16.3. Neyman–Pearson criterion
401
the unordered eigenvalue distribution of Σ, deﬁned on [σ2, ∞)N, reads:
PG(G) = 1
N (g1 −σ2)N−1 e−(g1−σ2)
(N −1)!
N
Y
i=2
δ(gi −σ2).
From the model H1, Y is distributed as correlated Gaussian, as follows.
PY|Σ,I1(Y, Σ) =
1
πMN det(G)M e−tr(YYHUG−1UH)
where Ik denotes the prior information at the receiver “H1 and K = k.” This
additional notation is very conventional for Bayesian probabilists, as it helps
remind that all derived probability expressions are the outcomes of a so-called
plausible reasoning based on the prior information available at the system
modeler.
Since the channel H is unknown, we need to integrate out all possible channels
for the transmission model under H1 over the probability space of N × K
matrices with Gaussian i.i.d. distribution. From the unitarily invariance of
Gaussian i.i.d. random matrices, this is equivalent to integrating out all possible
covariance matrices Σ over the space of such non-negative deﬁnite Hermitian
matrices, as follows.
PY|H1(Y) =
Z
Σ
PY|Σ,H1(Y, Σ)PΣ(Σ)dΣ.
Eventually, after complete integration calculus given in the proof below,
the Neyman–Pearson decision ratio (16.2) for the single input multiple output
channel takes an explicit expression, given by the following theorem.
Theorem 16.1. The Neyman–Pearson test ratio CY(Y) for the presence of an
informative signal when the receiver knows K = 1, the signal power E = 1, and
the noise power σ2, reads:
CY(Y) = 1
N
N
X
l=1
σ2(N+M−1)eσ2+
λl
σ2
QN
i=1
i̸=l(λl −λi)
JN−M−1(σ2, λl),
(16.5)
with λ1, . . . , λN the eigenvalues of YYH and where
Jk(x, y) ≜
Z ∞
x
tke−t−y
t dt = 2y
k+1
2 K−k−1(2√y) −
Z x
0
tke−t−y
t dt.
(16.6)
The proof of Theorem 16.1 is provided below. Among the interesting features of
(16.5), note that the Neyman–Pearson test does only depend on the eigenvalues
of YYH. This suggests that the eigenvectors of YYH do not provide any
information regarding the presence of an informative signal. The essential reason
is that, both under H0 and H1, the eigenvectors of Y are isotropically distributed
on the unit N-dimensional complex sphere due to the Gaussian assumptions
made here. As such, a given realization of the eigenvectors of Y does indeed not

402
16. Detection
carry any relevant information to the hypothesis test. The Gaussian assumption
for H brought by the maximum entropy principle, or as a matter of fact for
any unitarily invariant distribution assumption for H, is therefore essential here.
Note however that (16.5) is not reduced to a function of the sum P
i λi of the
eigenvalues, as suggested by the classical energy detector.
On the practical side, note that the integral Jk(x, y) does not take a closed-
form expression, but for x = 0, see, e.g., pp. 561 of [Gradshteyn and Ryzhik,
2000]. This is rather inconvenient for practical purposes, since Jk(x, y) must
either be evaluated every time, or be tabulated. It is also diﬃcult to get
any insight on the performance of such a detector for diﬀerent values of σ2,
N, and K. We provide hereafter a proof of Theorem 16.1, in which classical
multi-dimensional integration techniques are required. In particular, the tools
introduced in Section 2.1, such as the important Harish–Chandra formula,
Theorem 2.4, will be shown to be key ingredients of the derivation.
Proof. We start by writing the probability PY|I1(Y) as the marginal probability
of PY,Σ,I1 after integration along all possible Σ. This is:
PY|I1(Y) =
Z
S(σ2)
PY|Σ,I1(Y, Σ)PΣ(Σ)dΣ
with S(σ2) ⊂CN×N the cone of positive deﬁnite complex matrices with smallest
N −1 eigenvalues equal to σ2.
We now consider the one-to-one mapping
B : (U(N)/T) × (σ2, ∞) →S(σ2)
(U, g1) 7→Σ = UGUH
where
G =
g1
0
0 σ2IN−1

and where U(N)/T is the space of unitary matrices of N × N with ﬁrst column
composed of real positive entries. More information on this mapping is provided
in [Hiai and Petz, 2006], which is reused later in Chapter 18. From variable
change calculus, see, e.g., [Billingsley, 1995], we have
PY|I1(Y)
=
Z
(U(N)/T )×(σ2,∞)
PY|B(U,g1)(Y, U, g1)PB(U,g1)(U, g1) det(J(B))dUdg1
with J(B) the Jacobian matrix of B.
Notice now that Σ −σ2IN = HHH is a Wishart matrix. The density of its
entries is therefore invariant by left- and right-product by unitary matrices.
The eigenvectors of Σ are as a consequence uniformly distributed over U(N),
the space of complex unitary matrices of size N × N. Moreover, the eigenvalue
distribution of Σ is independent of the matrix U. From these observations, we

16.3. Neyman–Pearson criterion
403
conclude that the joint density
P(U,g1)(U, g1) = PB(U,g1)(U, g1) det(J(B))
can be written under the product form
P(U,g1)(U, g1) = PU(U)Pg1(g1).
As in Chapter 2, we assume that dU is the Haar measure with density
PU(U) = 1. We can therefore write
PY|I1(Y) =
Z
U(N)×(σ2,∞)
PY|Σ,H1(Y, Σ)Pg1(g1)dUdg1.
The latter can further be equated to
PY|I1(Y) =
Z
U(N)×(σ2,∞)
e−tr(YYHUG−1UH)
πNM det(G)M
(g1 −σ2)N−1 e−(g1−σ2)
N!
dUdg1.
The next step is to use the Harish–Chandra identity provided in Theorem
2.4. Denoting ∆(Z) the Vandermonde determinant of matrix Z ∈CN×N with
eigenvalues z1 ≤. . . ≤zN
∆(Z) ≜
Y
i>j
(zi −zj)
(16.7)
the likelihood PY|I1(Y) further develops as
PY|I1(Y) =
lim
g2,...,gN→σ2
eσ2(−1)
N(N−1)
2
QN−1
j=1 j!
πMNσ2M(N−1)N!
×
Z +∞
σ2
(g1 −σ2)N−1e−g1 1
gM
1
det

e
−λi
gj

∆(YYH)∆(G−1)dg1.
Now, noticing that ∆(G−1) = (−1)N(N+3)/2
∆(G)
det(G)N−1 , this is also
PY|I1(Y)
=
lim
g2,...,gN→σ2
π−MNeσ2 QN−1
j=1 j!
N!σ2(N−1)(M−N+1)
Z +∞
σ2
(g1 −σ2)N−1e−g1
gM−N+1
1
det

e
−λi
gj

∆(YYH)∆(G) dg1
in which we remind that λ1, . . . , λN are the eigenvalues of YYH. Note the trick
of replacing the known values of g2, . . . , gN by limits of scalars converging to
these known values, which dodges the problem of improper ratios. To derive the
explicit limits, we then proceed as follows.
Denoting y = (γ1, . . . , γN−1, γN) = (g2, . . . , gN, g1) and deﬁning the functions
f(xi, γj) ≜e
−xi
γj
fi(γj) ≜f(xi, γj)

404
16. Detection
we then have from Theorem 2.9
lim
g2,...,gN→σ2
det



e
−λi
gj

1≤i≤N
1≤j≤N


∆(YYH)∆(G)
=
lim
γ1,...,γN−1→σ2
γM→g1
(−1)N−1 det

{fi(λj)}i,j

∆(YYH)∆(G)
= (−1)N−1 det

fi(σ2), f ′
i(σ2), . . . , f (N−2)(σ2), fi(g1)

Q
i<j(λi −λj)(g1 −σ2)N−1 QN−2
j=1 j!
.
The change of variables led to a switch of one column and explains the
(−1)N−1 factor appearing when computing the resulting determinant. The
partial derivatives of f along the second variable is
 ∂
∂γk f

k≥1
(a, b) =
k
X
m=1
(−1)k+m
bm+k
m
k
 (k −1)!
(m −1)!ame−a
b
≜κk(a, b)e−a
b .
Back to the full expression of PY|H1(Y), we then have
PY|I1(Y)
= eσ2σ2(N−1)(N−M−1)
NπMN
×
Z +∞
σ2
(−1)N−1gN−M−1
1
e−g1 det

fi(σ2), f ′
i(σ2), . . . , f (N−2)(σ2), fi(g1)

Q
i<j(λi −λj)
dg1
=
eσ2σ2(N−1)(N−M−1)
NπMN Q
i<j(λi −λj)
×
Z +∞
σ2
(−1)N−1gN−M−1
1
e−g1 det


e−x1
σ2
...
e−xN
σ2

κj(λi, σ2)e−λi
σ2

1≤i≤N
1≤j≤N−2
e−λ1
g1
...
e−λN
g1

dg1.
Before going further, we need the following result, often required in the calculus
of marginal eigenvalue distributions for Gaussian matrices.
Lemma 16.1. For any family {a1, . . . , aN} ∈RN, N ≥2, and for any b ∈R∗
det


1
... (κj(ai, b)) 1≤i≤N
1≤j≤N−1
1

=
1
bN(N−1)
Y
i<j
(aj −ai).
This identity follows from the observation that column k of the matrix
above is a polynomial of order k. Since summations of linear combinations of
the columns do not aﬀect the determinant, each polynomial can be replaced

16.3. Neyman–Pearson criterion
405
by the monomial of highest order, i.e. b−2(k−1)ak
i in row i. Extracting the
product 1 · b−2 · · · b−2(N−1) = b−(N−1)N from the determinant, what remains is
the determinant of a Vandermonde matrix based on the vector a1, . . . , aN.
By factorizing every row of the matrix by e−λi
σ2 and developing the determinant
on the last column, we obtain
PY|I1(Y)
=
eσ2σ2(N−1)(N−M−1)
NπMN Q
i<j(λi −λj)
×
Z +∞
σ2
gN−M−1
1
e−g1−
PN
i=1
λi
σ2
N
X
l=1
(−1)2N+l−1e
−λl

1
g1 −1
σ2

σ2(N−1)(N−2)
Y
i<j
i̸=l
j̸=l
(λi −λj)dg1
=
eσ2−1
σ2
PN
i=1 λi
NπMNσ2(N−1)(M−1)
N
X
l=1
(−1)l−1
Z +∞
σ2
gN−M−1
1
e−g1e
−λl

1
g1 −1
σ2

Q
i<l(λi −λl) Q
i>l(λl −λi)dg1
=
eσ2−1
σ2
PN
i=1 λi
NπMNσ2(N−1)(M−1)
N
X
l=1
e
λl
σ2
QN
i=1
i̸=l(λl −λi)
Z +∞
σ2
gN−M−1
1
e
−

g1+
λl
g1

dg1
which ﬁnally gives
PY|I1(Y) =
eσ2−1
σ2
PN
i=1 λi
NπMNσ2(N−1)(M−1)
N
X
l=1
e
λl
σ2
QN
i=1
i̸=l(λl −λi)
JN−M−1(σ2, λl)
where
Jk(x, y) =
Z +∞
x
tke−t−y
t dt = 2y
k+1
2 K−k−1(2√y) −
Z x
0
tke−t−y
t dt
and Kn denotes the modiﬁed Bessel function of the second kind.
We now turn to the more general case where K ≥1, which unfolds similarly.
16.3.1.2 Multi-source case
In the generalized multi-source conﬁguration, where K ≥1, the likelihood PY|H0
remains unchanged and therefore the previous expression for K = 1 is still
correct. For the subsequent derivations, we only treat the situation where K ≤N
but the case K > N is a rather similar extension.
In this scenario, H ∈CN×K is now a random matrix (instead of a vector) with
i.i.d. zero mean Gaussian entries. The variance of every row is E[PK
j=1 |hij|2] = 1.
Therefore KHHH is distributed as a null Wishart matrix. Hence, observing that
Σ −σ2IN is the diagonal matrix of the eigenvalues of HHH
Σ = U · diag(ν1 + σ2, . . . , νK + σ2, σ2, . . . , σ2) · UH
(16.8)

406
16. Detection
for some unitary matrix U ∈CN×N and with ν1, . . . , νK the eigenvalues of HHH,
the unordered eigenvalue density of G unfolds from Theorem 2.3
PG(G) = (N −K)!KKN
N!
K
Y
i=1
e−K PK
i=1(gi−σ2) (gi −σ2)N−K
+
(K −i)!(N −i)!
K
Y
i<j
(gi −gj)2.
(16.9)
From the Equations (16.8) and (16.9) above, it is possible to extend Theorem
16.1 to the multi-source scenario, using similar techniques as for the proof of
Theorem 16.1, which we do not further develop here, but can be found in [Couillet
and Debbah, 2010a]. This extended result is provided below.
Theorem 16.2. The Neyman–Pearson test ratio CY(Y) for the presence of
informative signals when the receiver perfectly knows the number K (K ≤N) of
signal sources, the source power E = 1, and the noise power σ2, reads:
CY(Y) = σ2K(N+M−K)(N −K)!eK2σ2
N!K(K−1−2M)K/2 QK−1
j=1 j!
X
a⊂[1,N]
e
PK
i=1
λai
σ2
Y
ai
Y
j̸=a1
...
j̸=ai
(λai −λj)
×
X
b∈P(K)
(−1)sgn(b)+K
K
Y
l=1
JN−M−2+bl(Kσ2, Kλal)
with P(K) the ensemble of permutations of {1, . . . , K}, b = (b1, . . . , bK) and
sgn(b) the signature of the permutation b. The function Jk is deﬁned as in
Theorem 16.1.
Observe again that CY(Y) is a function of the empirical eigenvalues λ1, . . . , λN
of YYH only. In the following, we extend the current signal detector to the more
realistic situations where K, E, and σ2 are not a priori known to the receiver.
16.3.2
Unknown signal and noise variances
Eﬃcient signal detection when the noise variance is unknown is highly desirable
[Tandra and Sahai, 2005]. Indeed, as recalled earlier, if the noise and signal
variances were exactly known, some prior noise detection mechanism would be
required. The diﬃculty here is handily avoided thanks to ad-hoc methods that are
asymptotically independent of the noise variance, as in, e.g., [Cardoso et al., 2008;
Zeng and Liang, 2009], or more theoretical, although suboptimal, approaches
as in [Bianchi et al., 2011], which will be discussed when dealing with large
dimensional random matrix considerations.
In the following, we consider the general case when the knowledge about the
signal and noise variances can range from a total absence of information to a
perfect knowledge, and will represent this knowledge under the form of a prior

16.3. Neyman–Pearson criterion
407
probability distribution, as per classical Bayesian derivation. It might happen
in particular that the receiver has no knowledge whatsoever on the values of
the noise power and the expected signal power, but obviously knows that these
powers are positive values. When such a situation arises, the unknown parameter
must be assigned a so-called uninformative prior, such as the widely spread
Jeﬀreys prior [Jeﬀreys, 1946]. Assigning uninformative priors of variables deﬁned
in a continuum is however, still to this day, a controverted issue of the maximum
entropy principle [Caticha, 2001]. The classical uninformative priors considered
in the literature are (i) the uniform prior, i.e. every two positive values for
the signal/noise power are equi-probable, which experiences problems of scaling
invariance thoroughly discussed in [Jaynes, 2003], and (ii) the aforementioned
Jeﬀreys prior [Jeﬀreys, 1946], i.e. the prior distribution for the variance parameter
σ2 takes the form σ−2β for any deterministic choice of positive β, which is
invariant under scaling but is not fully attractive as it requires a subjective
choice of β.
In the case where the signal power E is known to be contained between E−
and E+ (for the time being, we had considered E = 1), and the noise power σ2
is known at least to be bounded by σ2
−and σ2
+, we will consider the “desirable”
assumption of uniform prior
PE(E) =
1
E+ −E−
Pσ2(σ2) =
1
σ2
+ −σ2
−
.
Denoting I′
k the event “H1, K = k, E−≤E ≤E+ and σ2
−≤σ2 ≤σ2
+,” this
leads to the updated decisions of the form
CY(Y) =
1
E+ −E−
R σ2
+
σ2
−
R E+
E−PY|σ2,I′
K(Y, σ2, E)dσ2dE
R σ2
+
σ2
−PY|σ2,H0(Y, σ2)dσ2
(16.10)
where PY|σ2,I′
K(Y, σ2, E) is obtained as previously by assuming a transmit power
E, instead of 1. Precisely, it suﬃces to consider the density of EY with σ2
changed into σ2/E.
The computational diﬃculty raised by the integrals Jk(x, y) does not allow
for any satisfying closed-form expression for (16.10) so that only numerical
integrations can be performed at this point.
16.3.3
Unknown number of sources
In practical cases, the number of transmitting sources is only known to be ﬁnite
and discrete. If only an upper bound Kmax on K is known, a uniform prior
is assigned to K. The probability distribution of Y under hypothesis I0 ≜“σ2

408
16. Detection
known, 1 ≤K ≤Kmax unknown,” reads:
PY|I0(Y) =
Kmax
X
i=1
PY|“K=i”,I0(Y) · P“K=i”|I0
=
1
Kmax
Kmax
X
i=1
PY|“K=i”,I0(Y)
which does not meet any computational diﬃculty.
Assuming again equal probability for the hypotheses H0 and H1, this leads to
the decision ratio
CY(Y) =
1
Kmax
Kmax
X
i=1
PY|“K=i”,I0(Y)
PY|H0(Y)
.
Note now that it is possible to make a decision test on the number of sources
itself in a rather straightforward extension of the previous formula. Indeed, given
a space–time matrix realization Y, the probability for the number of transmit
antennas to be i is from Bayes’ rule
P“K=i”|Y(Y)
=
PY|“K=i”(Y)P“K=i”
PKmax
j=0
PY|“K=j”(Y)P“K=j”
=



PY|H0(Y)
h
PY|H0(Y) +
1
Kmax
PKmax
j=1
PY|“K=j”(Y)
i−1
, i = 0
1
Kmax PY|“K=i”(Y)
h
PY|H0(Y) +
1
Kmax
PKmax
j=1
PY|“K=j”(Y)
i−1
, i ≥1
where all the quantities of interest here were derived in previous sections. The
multiple hypothesis test on K is then based on a comparison of the odds O(“K =
i”) for the events “K = i,” for all i ∈{0, . . . , Kmax}. Under Bayesian terminology,
we remind that the odds for the event “K = i” are deﬁned as
O(“K = i”) =
P“K=i”|Y(Y)
PKmax
j=0
j̸=i
P“K=j”|Y(Y)
.
In the current scenario, these odds express as
O(“K = i”)
=



PY|H0(Y)
h
1
Kmax
PKmax
j=1
PY|“K=j”(Y)
i−1
, i = 0
1
Kmax PY|“K=i”(Y)
h
PY|H0(Y) +
1
Kmax
P
j̸=i PY|“K=j”(Y)
i−1
, i ≥1.
We now provide a few simulation results that conﬁrm the optimality of
the Neyman–Pearson test for the channel model under study, i.e. with i.i.d.
Gaussian channel, signal, and noise. We also provide simulation results when
these assumptions are not met, in particular when a line-of-sight component is
present in the channel and when the signal samples are drawn from a quadrature
phase shift-keying (QPSK) constellation.

16.3. Neyman–Pearson criterion
409
First, we provide in Figure 16.3 the simulated plots of the false alarm
and correct detection rates obtained for the Neyman–Pearson test derived in
Theorem 16.1 when K = 1, with respect to the decision threshold above which
correct detection is claimed. To avoid trivial scenarios, we consider a rather low
SNR of −3 dB, and N = 4 receivers capturing only M = 8 signal instances.
The channel conditions are assumed to match the conditions required by the
maximum entropy model, i.e. channel, signal, and noise are all i.i.d. Gaussian.
Note that such conditions are desirable when fast decisions are demanded.
In a cognitive radio setup, secondary networks are expected to be capable of
very fast and reliable signal detection, in order to be able to optimally exploit
spectrum opportunities [Hoyhtya et al., 2007]. This is often referred to as the
exploration versus exploitation trade-oﬀ, which balances the time spent exploring
for available resources with high enough detection reliability and the time spent
exploiting the available spectrum resources. Observe in Figure 16.3 that the
false alarm rate curve shows a steep drop around CY(Y) = 1 (or zero dB).
This however comes along with a drop, although not so steep, of the correct
detection rate. A classical way to assess the performance of various detection
tests is to evaluate how much correct detection rate is achieved for a given
ﬁxed tolerable false alarm rate. Comparison of correct detection rates for given
false alarm rates is obtained in the so-called receiver operating characteristic
(ROC) curve. The ROC curve of the Neyman–Pearson test against that of the
energy detector is provided in Figure 16.4 under the channel model conditions,
for N = 4, M = 8, and σ2 = −3 dBm as above, with the transmit power E equal
to zero dBm, this last information being either perfectly known or only known
to belong to [−10 dBm, 10 dBm]. We only focus on a section of the curve which
corresponds to low false alarm rates (FAR), which is a classical assumption. We
recall that the energy detector consists in summing up λ1 to λN, the eigenvalues
of YYH (or equivalently taking the trace of YYH) and comparing it against some
deterministic threshold. The larger the sum the more we expect the presence of an
informative signal in the received signal. Observe that the Neyman–Pearson test
is eﬀectively superior in correct detection rate than the legacy energy detector,
with up to 10% detection gain for low false alarm rates.
We then test the robustness of the Neyman–Pearson test by altering the
eﬀective transmit channel model. We speciﬁcally consider that a line-of-sight
component of amplitude one fourth of the mean channel energy is present. This
is modeled by letting the eﬀective channel matrix H be H =
√
1 −α2Z + αA,
where Z ∈CN×K has i.i.d. Gaussian entries of variance 1/K and A ∈CN×K
has all entries equal to 1/K. This is depicted in Figure 16.5 with α2 = 1
4. We
observe once more that the Neyman–Pearson test performs better than the
power detector, especially at low SNR. It therefore appears to be quite robust to
alterations in the system model such as the existence of a line-of-sight component,
although this was obviously not a design purpose.
In Figure 16.6, we now vary the SNR range, and evaluate the correct detection
rates under diﬀerent false alarm rate constraints, for the Gaussian i.i.d. signal

410
16. Detection
−20
0
20
40
60
80
100
120
140
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
CY(Y) [dB]
Correct detection/False alarm rates
False alarm rate
Correct detection rate
Figure 16.3 Neyman–Pearson test performance in single-source scenario. Correct
detection rates and false alarm rates for K = 1, N = 4, M = 8, SNR = −3 dB.
and channel model. This graph conﬁrms the previous observation that the
stronger the false alarm request, the more eﬃcient the Neyman–Pearson test
comparatively with the energy detection approach. Note in particular that as
much as 10% of correct detection can again be gained in the low SNR regime
and for a tolerable FAR of 10−3.
Finally, in Figure 16.7, we provide the ROC curve performance for the multi-
source scheme, when K ranges from K = 1 to K = 3, still under the Gaussian
i.i.d. system model. We observe notably that, as the number of sources increases,
the energy detector closes in the performance gap observed in the single source
case. This arises both from a performance decrease of the Neyman–Pearson test,
which can be interpreted from the fact that the more the unknown variables
(there are more unknown channel links) the less reliable the noise-versus-
information comparative test, and from a performance increase of the power
detector, which can be interpreted as a channel hardening eﬀect (the more the
channel links the less the received signal variance).
A more interesting problem though is to assume that the noise variance σ2 is
not a priori known at the receiver end since the receiver is entitled to determine
whether noise or informative signals are received without knowing the noise
statistics in the ﬁrst place. We have already seen that the Neyman–Pearson test
approach leads to a multi-dimensional integral form, which is diﬃcult to further
simplify. Practical systems however call for low complex implementation [Cabric
et al., 2004]. We therefore turn to alternative approaches bearing ideas in the
large dimensional random matrix ﬁeld to cover this particularly interesting case.
It will turn out that very simple tests can be determined for the scenario where
the noise variance is not known to the receiver, and theoretical derivations of the

16.3. Neyman–Pearson criterion
411
1 · 10−3
5 · 10−3
1 · 10−2
2 · 10−2
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
False alarm rate
Correct detection rate
Neyman–Pearson test
Neyman–Pearson test (E unknown)
Energy detector
Figure 16.4 ROC curve for single-source detection, K = 1, N = 4, M = 8,
SNR = −3 dB, FAR range of practical interest, with signal power E = 0 dBm, either
known or unknown at the receiver.
1 · 10−3
5 · 10−3
1 · 10−2
2 · 10−2
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
False alarm rate
Correct detection rate
Neyman–Pearson test
Energy detector
Figure 16.5 ROC curve for single-source detection, K = 1, N = 4, M = 8,
SNR = −3 dB, FAR range of practical interest, under Rician channel with
line-of-sight component of amplitude 1/4 and QPSK modulated input signals.
correct detection rate against the false alarm rate can be performed. This is the
subject of the subsequent section.

412
16. Detection
−4
−2
0
2
4
6
8
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SNR [dB]
Correct detection rates
Neyman–Pearson (10−3)
Energy detector (10−3)
Neyman–Pearson (10−2)
Energy detector (10−2)
Neyman–Pearson (10−1)
Energy detector (10−1)
Figure 16.6 Correct detection rates under diﬀerent FAR constraints (in parentheses in
the legend) and for diﬀerent SNR levels, K = 1, N = 4, M = 8.
1 · 10−3
5 · 10−3
1 · 10−2
2 · 10−2
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
False alarm rate
Correct detection rate
Neyman–Pearson (K = 3)
Energy detector (K = 3)
Neyman–Pearson (K = 2)
Energy detector (K = 2)
Neyman–Pearson (K = 1)
Energy detector (K = 1)
Figure 16.7 ROC curve for MIMO transmission, K = 1 to K = 3, N = 4, M = 8,
SNR = −3 dB. FAR range of practical interest.
16.4
Alternative signal sensing approaches
The major results of interest in the large dimensional random matrix ﬁeld for
signal detection are those regarding the position of extreme eigenvalues of a
sample covariance matrix. The ﬁrst idea we will discuss, namely the condition
number test, arises from the simple observation that, under hypothesis H0,

16.4. Alternative signal sensing approaches
413
not only should the empirical eigenvalue distribution of YYH be close to the
Mar˘cenko–Pastur law, but also should the largest eigenvalue of YYH be close to
the rightmost end of the Mar˘cenko–Pastur law support, as both the number of
sensors and the number of available time samples grow large. If an informative
signal is present in the observation Y, we expect instead the largest eigenvalue of
YYH to be found suﬃciently far away from the Mar˘cenko–Pastur law support.
The methods proposed below therefore heavily rely on Bai and Silverstein’s
Theorem 7.1 and its various extensions and corollaries, e.g. Theorem 9.8.
16.4.1
Condition number method
The ﬁrst method we introduce is an ad-hoc approach based on the observation
that in the large dimensional regime, as both N and M grow large, the ratio
between the largest and the smallest eigenvalue of
1
M YYH, often referred to as
the condition number of
1
M YYH, converges almost surely to a deterministic
value. Ordering the eigenvalues λ1, . . . , λN of YYH as λ1 ≥. . . ≥λN, under
hypothesis H0, this convergence reads:
λ1
λN
a.s.
−→σ2 (1 + √c)2
σ2 (1 −√c)2 = (1 + √c)2
(1 −√c)2
with c deﬁned as the limiting ratio c ≜limN→∞N/M. This is an immediate
consequence of Theorem 7.1 and Theorem 9.8. This ratio is seen no longer to
depend on the speciﬁc value of the noise variance σ2. Under hypothesis H1,
notice that the model (16.1) is related to a spiked model, as the population
covariance matrix of
1
M YYH is formed of N −K eigenvalues equal to σ2 and K
other eigenvalues strictly superior to σ2 and all diﬀerent with probability one. In
the particular case when K = 1, all eigenvalues of E[y(1)y(1)H] equal σ2 but the
largest which equals σ2 + PN
i=1 |hi1|2. As previously, call g1 ≜σ2 + PN
i=1 |hi1|2.
Similar to the previous section, let us consider the K = 1 scenario. We still
assume that M > N, i.e. that more time samples are collected than there are
sensors. From Theorem 9.1, we then have that, as M and N grow large with
limiting ratio c ≜lim N
M and such that g1
σ2 −1 →ρ, if ρ > √c
λ1
M
a.s.
−→(1 + ρ)

1 + c
ρ

≜λsp
and
λN
M
a.s.
−→σ2  1 −√c
2
while if ρ < √c
λ1
M
a.s.
−→σ2  1 + √c
2

414
16. Detection
and
λN
M
a.s.
−→σ2  1 −√c
2 .
Thus, under the condition that M is large enough to ensure that g1 > 1 + √c,
it is asymptotically possible to detect the presence of informative signals, without
explicit knowledge of σ2. To this end, we may compare the ratio λ1/λN to the
value
1 + √c
1 −√c
2
corresponding to the asymptotically expected ratio under H0. This deﬁnes a new
test, rather empirical, which consists in considering a threshold around the ratio

1+√c
1−√c
2
and of deciding for hypothesis H1 whenever λ1/λN exceeds this value,
or H0 otherwise.
The condition number approach is interesting, although it is totally empirical.
In the following section, we will derive the generalized likelihood ratio test
(GLRT). Although suboptimal from a Bayesian point of view, this test will be
shown through simulations to perform much more accurately than the present
condition number test and in fact very close to the optimal Bayesian test. It will
in particular appear that the intuitive choice of λ1/λN as a decision variable was
not so appropriate and that the appropriate choice (at least, the choice that is
appropriate in the GLRT approach) is in fact λ1/( 1
N tr(YYH)).
16.4.2
Generalized likelihood ratio test
As we concluded in Section 16.3, it is rather diﬃcult to exploit the ﬁnal formula
obtained in Theorem 16.1, let alone its generalized form of Theorem 16.2. This is
the reason why a diﬀerent approach is taken in this section. Instead of considering
the optimal Neyman–Pearson test, which is nothing more than a likelihood ratio
test when PH0 = PH1, we consider the suboptimal generalized likelihood ratio
test, which is based on the calculus of the ratio CGLRT(Y) below
CGLRT(Y) = supH,σ2 PY|H,σ2,H1(Y)
supσ2 PY|σ2,H0(Y)
.
This test diﬀers from the likelihood ratio test (or Neyman–Pearson test) by the
introduction of the supH,σ2 in the numerator and the supσ2 in the denominator.
That is, among all possible H and σ2 that are tested against the observation Y,
we consider only the most probable (H, σ2) pair in the calculus of the numerator
and the most probable σ2 in the calculus of the denominator. This is a rather
appropriate approach whenever Y carries much information about the possible
H and σ2, but a rather hazardous one when a large extent of (H, σ2) pairs
can account for the observation Y, most of these being discarded by taking the
supremum.

16.4. Alternative signal sensing approaches
415
The explicit calculus of CGLRT(Y) is rather classical and not new. It is
particularly based on, e.g., [Anderson, 1963] and [Wax and Kailath, 1985]. The
complete calculus, which we do not recall here, leads to the following result.
Theorem 16.3. Call TM the ratio
TM =
λ1
1
N tr YYH .
Then the generalized likelihood ratio CGLRT(Y) is given by:
CGLRT(Y) =

1 −1
N
(1−N)M
T −M
M

1 −TM
N
(1−N)M
.
The function
φN : t 7→

1 −1
N
(1−N)M
t−M

1 −t
N
(1−N)M
turns out to be increasing on (1, N). Since TM lies in this interval with probability
one, CGLRT(Y) > ξN is equivalent to TM > φ−1
N (ξN), with φ−1
N the local inverse of
φN on (1, N). Therefore, setting CGLRT(Y) above a threshold ξN is equivalent to
setting the ratio TM of the largest eigenvalue to the trace of YYH above φ−1
N (ξN).
Theorem 16.3 therefore provides a close alternative approach to the condition
number criterion, which we remind was linked on the ratio of the largest to the
smallest eigenvalues of YYH instead.
For practical purposes, it is fundamental to be able to set adequately the
decision threshold over which H1 is preferred to H0. To this end, we need to
be able to derive, e.g., for all desired false alarm rates α ∈(0, 1), the threshold
γM such that φ−1
N (γM) = α. Then, for such γM, the so-called power of the test,
i.e. the correct detection rate for ﬁxed false alarm rate, needs be evaluated to
assess the performance of the test. It turns out that the probability of missing
the detection of H1 (i.e. the complementary to the power of the test) can be
shown to be approximately exponential with growing M. That is:
PTM|H1(x) ≃e−MI(x)
where, in large deviation theory [Dembo and Zeitouni, 2009], the function I(x) is
called the rate function associated with TM. An interesting ﬁgure of performance
to characterize the probability of missing detection under H1 is therefore given
by the asymptotic value
I∞(α) ≜lim
M→∞−1
M log
Z γM
0
PTM|H1(x)dx
which depends on α through the fact that φ−1
N (γM) = α.

416
16. Detection
16.4.3
Test power and error exponents
As was mentioned earlier, the explicit computation of a decision threshold and
of error exponents for the optimal Neyman–Pearson test is prohibitive due to
the expression taken by CY, not to mention the expression when the signal-
to-noise ratio is a priori unknown. However, we will presently show that, for
the GLRT approach it is possible to derive both the optimal threshold and
the error exponent for increasingly large system dimensions N and M (growing
simultaneously large). For the condition number approach, the optimal threshold
cannot be derived due to an eigenvalue independence assumption, which is not
proved to this day, although it is possible to derive an expression for the error
exponent for this threshold. In fact, it will turn out that the error exponents are
independent of the choice of the detection threshold.
We ﬁrst consider the GLRT test. The optimal threshold γM for a ﬁxed false
alarm rate α can be expressed as follows.
Theorem 16.4. For ﬁxed false alarm rate α ∈(0, 1), the power of the generalized
likelihood ratio test is maximum for the threshold
γM =
 
1 +
r
N
M
!2
+ bM
M
2
3 ζM
for some sequence ζM such that ζM converges to ( ¯F +)−1(α), with
¯F + the
complementary Tracy–Widom law deﬁned as
¯F +(x) = 1 −F +(x) and where
bM ≜

1 +
q
N
M
 4
3   N
M
−1
6 .
Moreover the false alarm rate of the GLRT with threshold
γM =
 
1 +
r
N
M
!2
+ bM
M
2
3 ( ¯F +)−1(α)
converges to α, and more generally
Z ∞
γ
PTM|H0(x) −¯F +

M
2
3 (γ −(1 +
q
N
M )2)
bM

→0.
Let us assume σ = 1 without loss of generality. To prove this result, we ﬁrst
need to remember from Theorem 9.5 that the largest eigenvalue λ1 of YYH is
related to the Tracy–Widom distribution as follows.
M
2
3
 λ1
M −
 1 + N
M
2
bM
!
⇒X+
as N, M grow large, where X+ is distributed according to the Tracy–Widom
distribution F +. At the same time,
1
MN tr YYH a.s.
−→1. Therefore, from the

16.4. Alternative signal sensing approaches
417
Slutsky theorem, Theorem 8.12, we have that
˜TM ≜M
2
3
 
TM −
 1 + N
M
2
bM
!
⇒X+.
Denoting FM(x) ≜
R ∞
x P ˜TM|H0(x)dx, the distribution function of ˜TM under H0,
it can in fact be shown that the convergence of FM towards F + is in fact uniform
over R. By deﬁnition of α, we have that
1 −FM
 
M
2
3
 
γM −
 1 + N
M
2
bM
!!
= α.
From the convergence of FM to F +, we then have that
¯F +
 
M
2
3
 
γM −
 1 + N
M
2
bM
!!
→α
from which, taking the (continuous) inverse of ¯F +, we have the ﬁrst identity. The
remaining expressions are merely due to the fact that FM( ˜TM) −F +( ˜TM) →0.
Deriving a similar threshold for the condition number test requires to prove the
asymptotic independence of λ1 and λN. This has been shown for the Gaussian
unitary ensemble, Theorem 9.6, i.e. for the extreme eigenvalues of the semi-circle
law, but to this day not for the extreme eigenvalues of the Mar˘cenko–Pastur law.
After setting the decision threshold, it is of interest to evaluate the
corresponding theoretical power test, i.e. the probability of correct detection. To
this end, instead of an explicit expression of the theoretical test power, we assume
large system dimensions and use tools from the theory of large deviations. Those
tools come along with mathematical requirements, though, which are outside
the scope of this book. We will therefore only state the main conclusions. For
details, the reader is referred to [Bianchi et al., 2011]. As stated in [Bianchi et al.,
2011], as the system dimensions grow large, it is sensible for us to reduce the
acceptable false alarm rate. Instead of a mere expression of the test power for
a given false alarm, we may be interested in the error exponent curve, which
is the set of points (a, b), such that there exists sequences α1, α2, . . . such that
limM −1
M log αM = a and I∞(α) = b. We ﬁrst address the case of the GLRT.
Theorem 16.5. Assume, under H1, that PN
k=1
|hk|2
σ2
converges to ρ as N, M
grow large, and that N/M →c. Then, for any α ∈(0, 1), I∞(α) is well deﬁned
and reads:
I∞(α) =
 Iρ
 (1 + √c)2
, if ρ > √c
0
, otherwise

418
16. Detection
where Iρ(x) is deﬁned as
Iρ(x) = x −λsp
1 + ρ
−(1 −c) log
 x
λsp

−c(V +(x) −V +(λsp)) + ∆(x|[(1 + √c)2, ∞))
with ∆(x|A) the function equal to zero is x ∈A or to inﬁnity otherwise, and
V +(x) the function deﬁned by
V +(x) = log(x) + 1
c log(1 + cm(x)) + log(1 + m(x)) + xm(x)m(x)
with m(x) = cm(x) −1−c
x
and m(x) the Stieltjes transform of the Mar˘cenko–
Pastur law with ratio c
m(z) = (1 −z −c) +
p
(1 −z −c)2 −4cz
2cz
(the branch of the square-root being such that m is a Stieltjes transform).
Moreover, the error exponent curve is described by the set of points

(I0(x), Iρ(x)), x ∈
 (1 + √c)2, λsp
	
with I0 deﬁned as
I0(x) = x −(1 + √c)2 −(1 −c) log

x
(1 + √c)2

−c[V (x) −V ((1 + √c)2)] + ∆(x|[(1 + √c)2, ∞)).
A similar result is obtained for the condition number test, namely:
Theorem 16.6. For any false alarm rate α ∈(0, 1) and for each limiting ρ, the
error exponent of the condition number test coincides with the error exponent for
the GLRT. Moreover, the error exponent curve is given by the set of points

(J0(x), Jρ(x)), x ∈
(1 + √c)2
(1 −√c)2 ,
λsp
(1 −√c)2

with
Jρ(x) = inf

Iρ(x1) + I−(x2), x1
x2
= x

J0(x) = inf

I0(x1) + I−(x2), x1
x2
= x

where I−is deﬁned as
I−(x) = x −(1 −√c)2 −(1 −c) log

x
(1 −√c)2

−2c[V −(x) −V − (1 −√c)2
] + ∆(x|(0, (1 −√c)2])

16.4. Alternative signal sensing approaches
419
and V −is given by:
V −(x) = log(x) + 1
c log(1 + cm(x)) + log(−1 −m(x)) + xm(x)m(x).
The ROC curve performance of the optimal Neyman–Pearson test with
unknown SNR is compared against the condition number test and the GLRT
in Figure 16.8. For the Neyman–Pearson test, we remind that the unknown SNR
parameter must be integrated out, which assumes the need for a prior probability
distribution for σ2. We provide in Figure 16.8 two classical approaches, namely
uniform distribution and Jeﬀreys prior with coeﬃcient β = 1, i.e. Pσ2(σ2) =
1
σ2 .
The simulation conditions are as before with K = 1 transmit source, N = 4
receive sensors, M = 8 samples. The SNR is now set to zero dB in order to have
non-trivial correct detection values. Observe that, as expected, the Neyman–
Pearson test outperforms both the GLRT and condition number tests, either
for uniform or Jeﬀreys prior. More surprising is the fact that the generalized
likelihood ratio test largely outperforms the condition number test and performs
rather close to the optimal Neyman–Pearson test. Therefore, the choice of the
ratio between the largest eigenvalue and the normalized trace of the sample
covariance matrix as a test comparison criterion is much more appropriate than
the ratio between the largest eigenvalue and the smallest eigenvalue. Given
the numerical complexity involved by the explicit computation of the Neyman–
Pearson test, the GLRT can be considered as an interesting suboptimal substitute
for this test when the signal and noise powers are a priori unknown.
It is also mentioned in [Bianchi et al., 2011] that the error exponent curve of
the GLRT dominates that of the condition number test in the sense that, for
each (a, b) in the error exponent curve of the condition number test, there exists
b′ > b such that (a, b′) is in the error exponent curve of the GLRT. Therefore,
from the above theorems, at least asymptotically, the GLRT always outperforms
the condition number test. The practical simulations conﬁrm this observation.
In this section, we mainly used Theorem 7.1 and Theorem 9.1 that basically
state that, for large dimensional sample covariance matrices with population
covariance eigenvalues converging to a single mass in 1, the largest eigenvalue
is asymptotically found at the edge of the support of the Mar˘cenko–Pastur
law or outside this support, depending on whether a spike is found among
the population eigenvalues. This allowed us to proceed to hypothesis tests
discriminating both models with or without a spiked population eigenvalue. In
the next section, we go further by considering more involved random matrix
models for which not only hypothesis testing is performed but also statistical
inference. That is, we will proceed to the estimation of system parameters using
eigen-inference methods. This chapter will therefore require the tools developed
in Chapter 7 and Chapter 8 of Part I.

420
16. Detection
1 · 10−3
5 · 10−3
1 · 10−2
2 · 10−2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
False alarm rate
Correct detection rate
N-P, Jeﬀreys
N-P, uniform
Condition number
GLRT
Figure 16.8 ROC curve for a priori unknown σ2 of the Neyman–Pearson test (N-P),
condition number method and GLRT, K = 1, N = 4, M = 8, SNR = 0 dB. For the
Neyman–Pearson test, both uniform and Jeﬀreys prior, with exponent β = 1, are
provided.

17
Estimation
In this chapter, we consider the consistent estimation of system parameters
involving random matrices with large dimensions. When it comes to estimation
or statistical inference in signal processing, there often exists a large number of
diﬀerent methods proposed in the literature, most of which are usually based on
a reference, simple, and robust method which has various limitations such as the
Urkowitz’s power detector [Urkowitz, 1967] that only assumes the additive white
Gaussian noise (AWGN) model, or the multiple signal classiﬁcation (MUSIC)
algorithm [Schmidt, 1986] of Schmidt that suﬀers from undecidability issues
when the signal to noise ratio reaches a critically low value. When performing
statistical inference based on a limited number of large dimensional vector
inputs, the main limitation is due to the fact that those legacy estimators are
usually built under the assumption that the number of available observations is
extremely large compared to the number of system parameters to identify. In
modern signal processing applications, especially for large sensor networks, the
estimators receive as inputs the M stacked N-dimensional observation vectors
Y = [y(1), . . . , y(M)] ∈CN×M of some observation vectors y(m) ∈CN at time m,
M and N being of similar size, or even sometimes M being much smaller than
N. Novel estimators that can cope with this large population size limitation
are therefore required in place of the historical estimators. In this chapter,
we introduce such (N, M)-consistent estimators, which we recall are estimators
which are asymptotically unbiased when both N and M grow large at a similar
rate.
Since the signiﬁcant advances in this ﬁeld of research are rather new, only two
main examples will be treated here. The ﬁrst example is that of the consistent
estimation of direction of arrivals (DoA) in linear sensor arrays (such as radars)
[Kay, 1993; Scharf, 1991] when the number of sensors is of similar dimension
as the number of available observations. The major works in this direction
are [Mestre and Lagunas, 2008] and [Vallet et al., 2010] for almost identical
situations, involving nonetheless diﬀerent system models. We will then move
to the question of blind user sensing and power estimation. Speciﬁcally, we will
consider a sensor array receiving simultaneous transmissions from multiple signal
emitters, the objective being to estimate both the number of transmitters and
the power used by each one of those. The latter is mainly based on [Couillet
et al., 2011c].

422
17. Estimation
Figure 17.1 Two-user line-of-sight transmissions with diﬀerent angles of arrival, θ1 and
θ2.
17.1
Directions of arrival
In this section, we consider the problem of a sensor array impinged by multiple
signals, each one of which comes from a given direction. This is depicted in
Figure 17.1, where two signals transmitted simultaneously by two terminal users
(positioned far away from the receiving end) are received with angles θ1 and θ2
at the sensor array. The objective here is to detect both the number of signal
sources and the direction of arrival from each of these signals. This has natural
applications in radar detection for instance, where multiple targets need to be
localized. In general, thanks to the diversity oﬀered by the sensor array, and the
phase shifts in the signals impacting every antenna, it is possible to determine
the angle of signal arrival from basic geometrical optics. In the following, we
will recall the classical so-called multiple signal classiﬁcation estimator (MUSIC)
[Schmidt, 1986], which is suited for large streams of data and small dimensional
sensor array as it can be proved to be a consistent estimator in this setting.
However, it can be proved that the MUSIC technique is not consistent with
increasing dimensions of both the number of sensors and the number of samples.
To cope with this problem, a G-estimator is proposed, essentially based on
Theorem 8.7. This recent estimator, developed in [Mestre and Lagunas, 2008] by
Mestre and Lagunas, is based on the concept of G-estimation and is referred to
as G-MUSIC. We ﬁrst introduce the system model under consideration.
17.1.1
System model
We consider the communication setup between K signal sources (that would
be, in the radar context, the reﬂected waveforms from detected targets) and N

17.1. Directions of arrival
423
receive sensors, N > K. Denote x(t)
k
the signal issued by source k at time t. The
received signals at time t, corrupted by the additive white Gaussian noise vector
σw(t) ∈CN with E[w(t)w(t′)] = δt,t′IN, are gathered into the vector y(t) ∈CN.
We assume that the channel between the sources and the sensors creates only
phase rotations, that essentially depend on the antenna array geometry. Other
parameters such as known scattering eﬀects might be taken into account as well.
To be all the more general, we assume that the channel steering eﬀect on signal
x(t)
k
for sensor i is modeled through the time invariant function si(θ) for θ = θk.
As such, we characterize the transmission model at time t as
y(t) =
K
X
k=1
s(θk)x(t)
k + σw(t)
(17.1)
where s(θk) = [s1(θk), . . . , sN(θk)]T. For simplicity, we assume that the vectors
s(θk) have unit Euclidean norm.
Suppose for the time being that x(t) = [x(t)
1 , . . . , x(t)
K ]T ∈CK are i.i.d. along
the time domain t and have zero mean and covariance matrix P ∈CK×K. This
assumption, which is not necessarily natural, will be discarded in Section 17.1.4.
The vectors y(t) are sampled M times, with M of the same order of magnitude
as N, and are gathered into the matrix Y = [y(1), . . . , y(M)] ∈CN×M. From the
assumptions above, the columns of Y have zero mean and covariance R, given
by:
R = S(Θ)PS(Θ)H + σ2IN
where S(Θ) = [s(θ1), . . . , s(θK)] ∈CN×K.
The DoA detection question amounts to estimating θ1, . . . , θK based on Y,
knowing the steering vector function s(θ) = [s1(θ), . . . , sN(θ)]T for all θ. To this
end, not only eigenvalues of
1
M YYH but also eigenvectors are necessary. This is
why we will resort to the G-estimators introduced in Section 17.1.3. Before that,
we discuss the classical subspace methods and the MUSIC approach.
17.1.2
The MUSIC approach
We
denote
λ1 ≤. . . ≤λN
the
eigenvalues
of
R
and
e1, . . . , eN
their
corresponding eigenvectors. Similarly, we denote ˆλ1 ≤. . . ≤ˆλN the eigenvalues
of RN ≜
1
M YYH, with respective eigenvectors ˆe1, . . . , ˆeN. If some eigenvalue has
multiplicity greater than one, the set of corresponding eigenvectors is taken to be
any orthonormal basis of the associated eigenspace. From the assumption that
the number of sensors N is greater than the number of transmit sources K, the
last N −K eigenvalues of R equal σ2 and we can represent R under the form
R =
 EW ES
 σ2IN−K
0
0
ΛS
 EH
W
EH
S


424
17. Estimation
with ΛS = diag(λN−K+1, . . . , λN), ES = [eN−K+1, . . . , eN] the so-called signal
space and EW = [e1, . . . , eN−K] the so-called noise space.
The basic idea of the subspace approach, which is at the core of the MUSIC
method, is to observe that any vector lying in the signal space is orthogonal to
the noise space. This leads in particular to
EH
W s(θk) = 0
for all k ∈{1, . . . , K}, which is equivalent to
η(θk) ≜s(θk)EW EH
W s(θk) = 0.
The idea behind the MUSIC approach is simple in that it suggests, according
to the large M-dimension approach, that the covariance matrix R is well
approximated by RN as M grows to inﬁnity. Therefore, denoting ˆEW =
[ˆe1, . . . , ˆeN−K] the eigenvector space corresponding to the smallest eigenvalues of
RN, the MUSIC estimator consists in retrieving the arguments θ which minimize
the function
ˆη(θ) ≜s(θ)H ˆEW ˆEH
W s(θ).
Notice that it may not be possible for ˆη(θ) to be zero for any θ, so that by
looking for minima in η(θ), we are not necessarily looking for roots. This approach
is originally due to Schmidt in [Schmidt, 1986]. However, the ﬁnite number of
available samples strongly aﬀects the eﬃciency of the MUSIC algorithm. In order
to come up with more eﬃcient approaches, the subspace approach was further
reﬁned by taking into account the fact that, in addition to be orthogonal to
the noise space, s(θk) is aligned to the signal space S(Θ)PS(Θ)H. One of the
known examples is the so-called SSMUSIC approach due to McCloud and Scharf
[McCloud and Scharf, 2002]. The approach considered in the SSMUSIC method
is now to determine the local minima of the function
ˆηSS(θ) ≜
s(θ)H ˆEW ˆEH
W s(θ)
s(θ)H ˆES

ˆΛS −ˆσ2IK
−1 ˆEH
Ss(θ)
where the denominator comes from
 S(Θ)PS(Θ)H−1 = ES
 ΛS −σ2IK
−1 EH
S
(when S(Θ)PS(Θ)H
is not invertible, the same remark holds with the
inverse sign replaced by the Moore–Penrose pseudo-inverse sign), with ˆΛS =
diag(ˆλN−K+1, . . . , ˆλN) and ˆσ2 =
1
N−K
PN−K
k=1 ˆλk. The SSMUSIC technique was
proved to outperform the MUSIC approach for ﬁnite M, as it has a higher
resolution power to distinguish close angles of arrival [McCloud and Scharf, 2002].
However, even though it is proved to be better, this last approach is still not
(N, M)-consistent. This fact, which we do not prove here, is the point made
by Mestre in [Mestre, 2008a] and [Mestre and Lagunas, 2008]. Instead of this
classical large dimensional M approach, we will assume that both N and M
grow large at similar pace while K is kept constant, so that we can use the
results from Chapter 8.

17.1. Directions of arrival
425
17.1.3
Large dimensional eigen-inference
The improved MUSIC estimator unfolds from a trivial application of Theorem
8.7. The cost function u introduced in Theorem 8.7 is simply replaced by the
subspace cost function η(θ), deﬁned by
η(θk) = s(θk)EW EH
W s(θk).
We therefore have the following improved MUSIC estimator, called by the
authors in [Mestre and Lagunas, 2008] the G-MUSIC estimator.
Theorem 17.1 ([Mestre and Lagunas, 2008]). Under the above conditions, we
have:
η(θ) −¯η(θ)
a.s.
−→0
as N, M grow large with limiting ratio satisfying 0 < lim N/M < ∞, where
¯η(θ) = s(θ)H
 N
X
n=1
φ(n)ˆenˆeH
n
!
s(θ)
with φ(n) deﬁned as
φ(n) =



1 + PN
k=N−K+1

ˆλk
ˆλn−ˆλk −
ˆµk
ˆλn−ˆµk

, n ≤N −K
−PN−K
k=1

ˆλk
ˆλn−ˆλk −
ˆµk
ˆλn−ˆµk

, n > N −K
and with µ1 ≤. . . ≤µN the eigenvalues of diag(ˆλ) −
1
M
p
ˆλ
p
ˆλ
T
, where we
denoted ˆλ = (ˆλ1, . . . , ˆλN)T.
This derives naturally from Theorem 8.7 by noticing that the noise space EW
is the space of the smallest eigenvalue of R with multiplicity N −K, which is
mapped to the space of the smallest N −K eigenvalues of the empirical RN to
derive the consistent estimate.
It is also possible to derive an (N, M)-consistent estimate for the improved
SSMUSIC method. The latter will be referred to as G-SSMUSIC This unfolds
from a similar application of Theorem 8.7 and is given in [Mestre and Lagunas,
2008] under the following form.
Theorem 17.2 ([Mestre and Lagunas, 2008]). Under the above conditions
ηSS(θ) −¯ηSS(θ)
a.s.
−→0
as N, M grow large with ratio uniformly bounded away from zero and inﬁnity,
where
¯ηSS(θ) =
¯η(θ)
ε¯η(θ) + ¯χ(θ)

426
17. Estimation
for any ε ≥0 that guarantees that the denominator does not vanish for all θ,
where ¯η(θ) is given in Theorem 17.1 and ¯χ(θ) is given by:
¯χ(θ) = s(θ)H
 N
X
n=1
ψ(n)ˆenˆeH
n
!
s(θ)
with ψ(n) deﬁned as
ψ(n) =



1
ˆσ2
PN
k=N−K+1

ˆλk
ˆλn−ˆλk −
ˆνk
ˆλn−ˆνk

, n ≤N −K
1
ˆσ2
PN−K
k=0
ˆνk
ˆλn−ˆνk −PN−K
k=1
ˆλk
ˆλn−ˆλk

, n > N −K
with µ1 ≤. . . ≤µN the eigenvalues of diag(ˆλ) −
1
M
p
ˆλ
p
ˆλ
T
, ˆν0 ≤. . . ≤ˆνN the
solutions of the equation in ν
ν = ˆσ2
 
1 −1
M
N
X
k=1
ˆλk
ˆλk −ν
!
and ˆσ2 is an (N, M)-consistent estimator for σ2, given here by
ˆσ2 =
M
N −K
N−K
X
k=1

ˆλk −ˆµk

.
We hereafter provide one-shot realizations of the cost functions ¯η(θ) and
¯ηSS(θ) for the diﬀerent DoA estimation methods proposed above. We take the
assumptions that K = 3 signal sources are emitting and that an array of N = 20
sensors is used to perform the statistical inference, that samples M = 150 times
the incoming waveform. The angles of arrival are 10◦, 35◦, and 37◦, while the
SNR is set to 10 dB. This situation is particularly interesting as two incoming
waveforms are found with very close DoA. From the discussion above, we
therefore hope that SSMUSIC would better resolve the two close angles. In
fact, we will see that the G-estimators that are G-MUSIC and G-SSMUSIC
are even more capable of discriminating between close angles. Figure 17.2 and
Figure 17.3 provide the comparative performance plots of the MUSIC against
G-MUSIC approaches, for θ ranging from −45◦to 45◦in Figure 17.2 and for
θ varying from −33◦to −38◦in Figure 17.3. Observe that, while the MUSIC
approach is not able to resolve the two close DoA, the G-MUSIC technique
clearly isolates two minima of ¯η(θ) around 35◦and 37◦. Apart from that, both
performance plots look alike. Similarly, the SSMUSIC and G-SSMUSIC cost
functions for the same random realization as in Figure 17.2 and Figure 17.3
are provided in Figure 17.4 and Figure 17.5. Observe here that the SSMUSIC
estimator is able to resolve both angles, although it is clearly not as eﬃcient as
the G-SSMUSIC estimator. Performance ﬁgures in terms of mean square error
are found in [Mestre and Lagunas, 2008]. It is observed in particular by the
authors that the improved estimators still do not solve the inherent problem
of both MUSIC and SS-MUSIC estimators, which is that both perform very

17.1. Directions of arrival
427
-10
35 37
−30
−25
−20
−15
−10
−5
0
5
10
angle [deg]
Cost function [dB]
MUSIC
G-MUSIC
Figure 17.2 MUSIC against G-MUSIC for DoA detection of K = 3 signal sources,
N = 20 sensors, M = 150 samples, SNR of 10 dB. Angles of arrival of 10◦, 35◦, and
37◦.
35
37
−30
−28
−26
−24
−22
−20
−18
−16
angle [deg]
Cost function [dB]
MUSIC
G-MUSIC
Figure 17.3 MUSIC against G-MUSIC for DoA detection of K = 3 signal sources,
N = 20 sensors, M = 150 samples, SNR of 10 dB. Angles of arrival of 10◦, 35◦, and
37◦.
badly in the low SNR regime. Nevertheless, the improved G-estimators manage
to repel to a lower level the SNR limit for which performance decays signiﬁcantly.
The same performance behavior will also be observed in Section 17.2, where the
performance of blind multi-source power estimators is discussed.

428
17. Estimation
-10
35 37
−30
−20
−10
0
10
20
30
angle [deg]
Cost function [dB]
SSMUSIC
G-SSMUSIC
Figure 17.4 SSMUSIC against G-SSMUSIC for DoA detection of K = 3 signal sources,
N = 20 sensors, M = 150 samples, SNR of 10 dB. Angles of arrival of 10◦, 35◦, and
37◦.
35
37
−28
−26
−24
−22
−20
−18
−16
angle [deg]
Cost function [dB]
SSMUSIC
G-SSMUSIC
Figure 17.5 SSMUSIC against G-SSMUSIC for DoA detection of K = 3 signal sources,
N = 20 sensors, M = 150 samples, SNR of 10 dB. Angles of arrival of 10◦, 35◦, and
37◦.
Further work has been done on the DoA topic, especially in the case where,
instead of i.i.d. samples, the sensors receive correlated data. These data can be
assumed not to be known to the sensors, so that no speciﬁc random model can
be applied. This is discussed in the next section.

17.1. Directions of arrival
429
17.1.4
The correlated signal case
We recall that in the previous section, we explicitly assumed that the vector of
transmit signals are independent for successive samples, have zero mean, and
have the same covariance matrix. The present section is merely an extension of
Section 17.1.3 to the even more restrictive case when the transmit data structure
is unknown to the receiving sensors. In this case, the sample covariance matrix
model assumed in Section 17.1.3, which allowed us to use Theorem 8.7, is no
longer available. This section mainly recalls the results of [Vallet et al., 2010].
Remark 17.1. It must be noted that the authors of [Vallet et al., 2010], instead
of mentioning that the signal source is random with unknown distribution, state
that the source is deterministic but unknown to the sensors. We prefer to say that
the source, being unknown to the receiver, is therefore random from the point
of view of the receiver, and not deterministic. However, to be able to use the
tools hereafter, we will need to assume that, although unknown to the receiver,
the particular realization of the random incoming data satisﬁes some important
boundedness assumptions, known to the receiver.
The model (17.1) is still valid, i.e. the receive data vector y(t) at time instant
t reads:
y(t) =
K
X
k=1
s(θk)x(t)
k + σw(t)
where the vector x(t) = [x(t)
1 , . . . , x(t)
K ]T ∈CK is no longer i.i.d. along the time
index t. We therefore collect the M vector samples into the random matrix
X = [x(1), . . . , x(M)] ∈CK×M and we obtain the receive model
Y = SX + σW
with W = [w(1), . . . , w(M)] ∈CN×M a Gaussian matrix and S ∈CN×K the
matrix with columns the K steering vectors, deﬁned as previously. In Section
17.1.3, X was of the form P
1
2 Z with Z ∈CK×M ﬁlled with i.i.d. entries of
zero mean and unit variance, and W was naturally ﬁlled with i.i.d. entries of
zero mean and unit variance, so that Y took the form of a sample covariance
matrix. This is no longer valid in this new scenario. The matrix Y can instead
be considered as an information plus noise matrix, if we take the additional
assumption that we can ensure ∥SXXHS∥uniformly bounded for all matrix
sizes. Denoting EW the noise subspace of R ≜
1
M SXXHS, our objective is now
to estimate the cost function
η(θ) = s(θ)HEW EH
W s(θ).
Again, the traditional MUSIC approach replaces the noise subspace EW by
the empirical subspace ˆEW composed of the eigenvectors corresponding to the
N −K smallest eigenvalues of
1
M YYH. The resulting cost function therefore

430
17. Estimation
reads:
ˆη(θ) = s(θ)H ˆEW ˆEH
W s(θ)
and the MUSIC algorithm consists once more in ﬁnding the K deepest minima
of ˆη(θ). Assuming this time that the noise variance σ2 is known, as per the
assumptions of [Vallet et al., 2010], the improved MUSIC approach, call it once
more the G-MUSIC technique, now for the information plus noise model, derives
directly from Theorem 8.10.
Theorem 17.3 ([Vallet et al., 2010]). As N, M grow to inﬁnity with limiting
ratio satisfying 0 < lim N/M < ∞
η(θ) −¯η(θ)
a.s.
−→0
for all θ, where
¯η(θ) = s(θ)H
 N
X
k=1
φ(k)ˆekˆeH
k
!
s(θ)
with φ(k) deﬁned as
φ(k) =













1 + σ2
M
PN
i=N−K+1
1
ˆλi−ˆλk + 2σ2
M
PN
i=N−K+1
ˆλk
ˆλi−ˆλk
+ σ2(M−N)
M
PN
i=N−K+1

1
ˆλi−ˆλk −
1
ˆµi−ˆλk

, k ≤N −K
σ2
M
PN−K
i=1
1
ˆλi−ˆλk −2σ2
M
PN−K
i=1
ˆλk
ˆλi−ˆλk
+ σ2(M−N)
M
PN−K
i=1

1
ˆλi−ˆλk −
1
ˆµi−ˆλk

, k > N −K
and with ˆµ1 ≤. . . ≤ˆµN the N roots of the equation in µ
1 + σ2
M
N
X
k=1
1
ˆλk −µ
.
The performance of the G-MUSIC approach which assumes the information
plus noise model against the previous G-MUSIC technique is compared for the
one-shot random transmission in Figure 17.6. We observe that both estimators
detect very accurately both directions of arrival. Incidentally, the information
plus noise G-MUSIC shows slightly deeper minima than the sample covariance
matrix G-MUSIC. This is only an outcome of the one-shot observation at hand
and does not aﬀect the average performance of both approaches, as shown more
precisely in [Vallet et al., 2010].
Note that Theorem 17.3 only proves the asymptotic consistency for the
function ¯η(θ). The consistency of the angle estimator itself, which is the result
of interest for practical applications, is derived in [Vallet et al., 2011a]. The
ﬂuctuations of the estimator are then provided in [Mestre et al., 2011]. We also
mention that the same authors also proposed a G-MUSIC alternative relying on
an additive spike model approach. Both limit and ﬂuctuations are derived also
for this technique, which turns out to perform worse than the approach presented

17.1. Directions of arrival
431
35
37
−32
−30
−28
−26
−24
−22
−20
−18
angle [deg]
Cost function [dB]
i.i.d. G-MUSIC
General G-MUSIC
Figure 17.6 G-MUSIC tailored to i.i.d. samples (i.i.d. G-MUSIC) against unconditional
G-MUSIC (General G-MUSIC) for DoA detection of K = 3 signal sources, N = 20
sensors, M = 150 samples, SNR of 10 dB. Angles of arrival of 10◦, 35◦, and 37◦.
in this section. The initial results can be found in [Hachem et al., 2011; Vallet
et al., 2011b].
This completes this section on DoA localization. We take the opportunity of
the information plus noise study above to mention that similar G-estimators have
been derived by the same authors for system models involving the information
plus noise scenario. In particular, in [Vallet and Loubaton, 2009], a consistent
estimator for the capacity of a MIMO channels H under additive white noise
of variance σ2, i.e. log det(IN + σ−2HHH), is derived based on successive
observations y(t) = Hs(t) + σw(t) with known pilot sequence s(1), s(2), . . . and
with additive standard Gaussian noise vector w(t).
In the following, we consider a similar inference problem, relative to the
localization of multiple transmit sources, not from an angular point of view, but
rather from a distance point of view. The main idea now is to consider a statistical
model where hidden eigenvalues must be recovered that give information on the
power transmitted by distinct signal sources. Similar to the DoA estimation,
the problem of resolving transmissions of close power will be raised for which a
complete analysis of the conditions of source resolution is performed. As such, we
will treat the following section in more detail than the current DoA estimation
section.

432
17. Estimation
17.2
Blind multi-source localization
In Chapter 16, we considered the setup of a simultaneous multi-source signal
transmission on the same spectral resource, impacting on an array of sensors
which is expected to answer the binary question: is a signal being transmitted
by these sources? In this section, we consider again this multi-source transmission
scheme, but we wish now to know more. The model is slightly generalized as we
let the concurrent transmissions imping the sensor array with diﬀerent power
levels, i.e. transmitters are localized at diﬀerent distances from the sensor array
and may also be using diﬀerent transmit signal powers. Moreover, we now let
the transmitters be equipped with more than a single antenna. The question we
now wish to answer is more advanced than a mere signal sensing decision. We
desire to collect the following information:
• the number of simultaneous transmissions, i.e. the number of active users;
• the power of each individual transmitter;
• the number of antennas of each transmitter.
The relative importance of the above pieces of information to the sensor array
depends on the problem at hand. We will mainly discuss the problem of user
localization in a cognitive radio setting. In the introduction of Chapter 16, we
mentioned that cognitive radios, whose objective is to reuse licensed spectrum
holes, basically work on a two-step mechanism as they successively need to
explore the available spectrum for transmission opportunities and to exploit the
spectrum found unused. Through the dual hypothesis test analyzed in Chapter 16
(presence or absence of on-going transmissions), a secondary network is capable
of deciding with more or less accuracy whether a given spectral resource is free
of use. It is however rather unusual that a spectrum resource be completely left
unused within a suﬃciently large network coverage area. Typically, a secondary
network will sense that no transmission is on-going in a close neighborhood, as
it may sense only very low power signals coming from remote transmitters. This
situation will then be associated with the no-transmission H0 hypothesis and
exploitation of the spectral resource under study will then be declared possible.
How to optimally exploit the spectrum holes depends then on the maximally
acceptable secondary transmit coverage area that lets the primary transmissions
be free of interference. This question is however not fully answered by the dual
hypothesis test.
This is where the question of estimating the power of on-going transmissions
is of utmost importance. Obtaining a rough estimate of the total or mean power
used by primary transmitters is a ﬁrst step towards assessing the acceptable
secondary transmit coverage area. But this is not the whole story. Indeed, if
the secondary network is only able to state that a signal of cumulated power
P is received, then the secondary network will dynamically adapt its transmit
coverage area as follows:

17.2. Blind multi-source localization
433
• Assuming the sensed data are due to primary uplink transmissions by mobile
users to a ﬁxed network, the primary uplink frequency band will be reused in
such a way that no primary user emitting with power P is interfered with by
any transmission from the secondary network;
• if P is above a certain threshold, the cognitive radio will decide that
neighboring primary cell sites are in use by primary users. Therefore, also
downlink transmissions are not to be interfered with, so that the downlink
spectrum is not considered a spectrum hole.
If the secondary network is able to do more than just overall power estimation,
namely if it is capable of estimating both the number of concurrent simultaneous
transmissions in a given spectral resource, call this number K, and the power
of each individual source, call them P1, . . . , PK for source 1 to K, respectively,
with P1 ≤. . . ≤PK, then the secondary network can adapt its coverage area in
a more accurate way as follows.
• Since the strongest transmitter has power PK, the secondary cell coverage
area can be set such that the primary user with power PK is not interfered
with. This will automatically induce that the other primary users are not
interfered with (if it is further assumed that no power control is performed
by the primary users). As an immediate consequence, the primary uplink
transmission will be stated as reusable if PK is not too large. Also, if PK
is so little that no primary user is expected to use primary downlink data
sent by neighboring cells, also the downlink spectrum will be reused. In the
case where multiple transmissions happen simultaneously, this strategy will
turn out to be much more eﬃcient than the estimation of the overall transmit
power P ≜PK
k=1 Pk.
• Also, by measuring the transmit powers of multiple primary users within
multiple distant secondary networks, information can be shared (via low speed
links) among these networks so as to eventually pinpoint the precise location
of the users. This brings even more information about the occupancy (and
therefore the spectrum reusability) of each primary cell site. Moreover, it will
turn out that most methods presented below show a strong limitation when
it comes to resolving diﬀerent users transmitting with almost equal power.
Quite often, it is diﬃcult to discriminate between the scenario of a single-user
transmitting with power P or multiple transmitters with similar transmit
powers, the sum of which being equal to P. Communications between distant
secondary networks can therefore bring more information on the number of
users with almost equal power. This eventually leads to the same performance
gain as given in the previous point when it comes for the cognitive network
to decide on the maximally acceptable coverage area.
• We also mentioned the need for estimating the number of transmit antennas
per user. In fact, in the eigen-inference techniques presented below, the ability
to estimate the number of antennas per user is an aftermath of the estimation
algorithms. The interest of estimating the number of eigenvalues is linked

434
17. Estimation
to the previous point, concerning the diﬃculty to diﬀerentiate between one
user transmitting with strong power or many users transmitting with almost
equal powers. If it is observed that power P is used by a device equipped
with many more antennas than the communication protocol allows, then this
should indicate to the sensors the presence of multiple transmitters with close
transmit powers instead of a unique transmitter. This again allows for more
precise inference on the system parameters.
Note from the discussion above that estimating PK is in fact more important
to the secondary network than estimating P1, as PK can by itself already provide
a major piece of information concerning the largest coverage radius for secondary
transmissions. When the additive noise variance is large, or when the number
of available sensors is too small, inferring the smallest transmit powers is rather
diﬃcult. This is one of the reasons why eigen-inference methods that are capable
of estimating a particular Pk are preferred over methods that jointly estimate
the power distribution with masses in P1, . . . , PK.
We hereafter introduce the general communication model discussed in the rest
of this section. We will then derive eigen-inference techniques based on either
exact small dimensional approaches, asymptotic free deconvolution approaches
(as presented in Section 8.2), or the more involved but more eﬃcient Stieltjes
transform methods, relying on the theorems derived in Section 8.1.2.
17.2.1
System model
Consider a wireless (primary) network in which K entities are transmitting
data simultaneously on the same frequency resource. Transmitter k ∈{1, . . . , K}
has transmission power Pk and is equipped with nk antennas. We denote
n ≜PK
k=1 nk the total number of transmit antennas within the primary network.
Consider also a secondary network composed of a total of N, N > n, sensing
devices (either N single antenna devices or multiple devices equipped with a
total of N antennas); we refer to the N sensors collectively as the receiver. This
scenario relates in particular to the conﬁguration depicted in Figure 17.7.
To ensure that every sensor in the secondary network, e.g. in a closed-access
femto-cell [Claussen et al., 2008], roughly captures the same amount of energy
from a given transmitter, we need to assume that all distances between a given
transmitter and the individual sensors are alike. This is a realistic assumption for
instance for an in-house femto-cell network, where all sensors lie in a restricted
space and transmitters are found far away from the sensors. Denote Hk ∈CN×nk
the channel matrix between transmitter k and the receiver. We assume that
the entries of
√
NHk are i.i.d. with zero mean, unit variance, and ﬁnite fourth
order moment. At time instant m, transmitter k emits the signal x(m)
k
∈Cnk,
with entries assumed to be independent, independent along m, k, identically
distributed along m, and all have zero mean, unit variance, and ﬁnite fourth
order moment (the x(m)
k
need not be identically distributed along k). Assume

17.2. Blind multi-source localization
435
Figure 17.7 A cognitive radio network.
further that at time instant m the receive signal is impaired by additive white
noise with entries of zero mean, variance σ2, and ﬁnite fourth order moment on
every sensor; we denote σw(m) ∈CN the receive noise vector where the entries
of w(m)
k
have unit variance. At time m, the receiver therefore senses the signal
y(m) ∈CN deﬁned as
y(m) =
K
X
k=1
p
PkHkx(m)
k
+ σw(m).
Assuming the channel fading coeﬃcients are constant over at least M
consecutive sampling periods, by concatenating M successive signal realizations
into Y = [y(1), . . . , y(M)] ∈CN×M, we have:
Y =
K
X
k=1
p
PkHkXk + σW
where Xk = [x(1)
k , . . . , x(M)
k
] ∈Cnk×M and W = [w(1), . . . , w(M)] ∈CN×M. This
can be further rewritten as
Y = HP
1
2 X + σW
(17.2)
where P ∈Rn×n is diagonal with ﬁrst n1 entries P1, subsequent n2 entries P2, etc.
and last nK entries PK, H = [H1, . . . , HK] ∈CN×n and X = [XT
1 , . . . , XT
K]T ∈
Cn×M. By convention, we assume P1 ≤. . . ≤PK.
Remark 17.2. The statement that
√
NH, X and W have independent entries of
ﬁnite fourth order moment is meant to provide as loose assumptions as possible
on the channel, signal, and noise properties. In the simulations carried out later
in this section, the entries of H, W are taken Gaussian. Nonetheless, according
to our assumptions, the entries of X need not be identically distributed, but
may originate from a maximum of K distinct distributions. This translates
the realistic assumption that diﬀerent data sources may use diﬀerent symbol

436
17. Estimation
constellations (e.g. M-QAM, M-PSK); the ﬁnite fourth moment assumption
is obviously veriﬁed for ﬁnite constellations. These assumptions though are
suﬃcient requirements for the analysis performed later in Section 17.2.5. Section
17.2.2 and Section 17.2.4, in contrast, will require much stronger assumptions on
the system model, namely that the random matrices
√
NH, X, and W under
consideration are Gaussian with i.i.d. entries. The reason is these sections are
based methods that require the involved matrices to be unitarily invariant.
Our objective is to infer the values of the powers P1, . . . , PK from the
realization of a single random matrix Y. This is successively performed
from diﬀerent approaches in the following sections. We will start with small
dimensional optimal maximum-likelihood and minimum mean square error
estimators, using similar derivations as in Chapter 16. Due to the computational
complexity of the method, we then consider large dimensional approaches. The
ﬁst of those is the conventional approach that assumes n small, N much larger
than n, and M much larger than N. This will lead to a simple although
largely biased estimation algorithm when tested in practical small dimensional
scenarios. This algorithm will be corrected ﬁrst by using moment approaches
and speciﬁcally free deconvolution approaches, although this approach requires
strong system assumptions and will be proved not to be very eﬃcient, both with
respect to performance and to computational eﬀort. Finally, the latter will be
further improved using Stieltjes transform approaches in the same spirit as in
Section 17.1.
17.2.2
Small dimensional inference
This ﬁrst approach consists in evaluating the exact distribution of the powers
P1, . . . , PK given the observations Y, modeled in (17.2), when H, X, and W are
assumed Gaussian. Noticing that we can write Y under the unitarily invariant
form
Y =

HP
1
2 σIN
  X
W

(17.3)
the derivation unfolds similarly as that proposed in Chapter 16 with the
noticeable exception that the matrix NHPHH has now a correlated Wishart
distribution instead of an uncorrelated Wishart distribution. This makes the
calculus somewhat more involved. We do not provide the successive steps of the
full derivations that mimic those of Chapter 16 and that can be found in detail
in [Couillet and Guillaud, 2011]. The ﬁnal result is given as follows.
Theorem 17.4 ([Couillet and Guillaud, 2011]). Assume P1, . . . , PK are all
diﬀerent and have multiplicity n1 = . . . = nK = 1, hence n = K. Then, denoting

17.2. Blind multi-source localization
437
λ = (λ1, . . . , λN) the eigenvalues of
1
M YYH, PY|P1,...,PK(Y) reads:
PY|P1,...,PK(Y) =
C(−1)Nn+1eNσ2 Pn
i=1
1
Pi
σ2(N−n)(M−n) Qn
i=1 P M−n+1
i
∆(P)
X
a∈SN
n
(−1)|a|sgn(a)e−M
σ2 |λ[¯a]|
× ∆(diag(λ[¯a]))
∆(diag(λ))
X
b∈Sn
sgn(b)
n
Y
i=1
JN−M−1
Nσ2
Pbi
, NMλai
Pbi

where SN
n is the set of all permutations of n-subsets of {1, . . . , N}, Sn = Sn
n,
|x| = P
i xi, ¯x is the complementary of the set x, x[a] is the restriction of x to
the indexes stored in the vector a, ∆(X) is the Vandermonde determinant of the
matrix X, the constant C is given by:
C =
1
πNMn!
N n(M−n−1
2 )
M n(N−n+1
2 )
and Jk(x, y) is the integral form deﬁned in (16.6) by
Jk(x, y) = 2y
k+1
2 K−k−1(2√y) −
Z x
0
uke−u−y
u du.
The generalization of Theorem 17.4 to powers P1, . . . , PK of multiplicities
greater than one is obtained by exploiting Theorem 2.9. The ﬁnal result however
takes a more involved form which we do not provide here.
From Theorem 17.4, we can derive the maximum likelihood (ML) estimator
ˆP
(ML) = ˆP (ML)
1
, . . . , ˆP (ML)
K
of the joint (P1, . . . , PK) vector as
ˆP
(ML) = arg
max
P1,...,PK PY|P1,...,PK(Y)
or
the
minimum
mean
square
error
(MMSE)
estimator
ˆP
(MMSE) =
ˆP (MMSE)
1
, . . . , ˆP (MMSE)
K
as
ˆP
(MMSE) =
Z
[0,∞)K(P1, . . . , PK)PP1,...,PK|Y(P1, . . . , PK)dP1 . . . dPK
with
PP1,...,PK|Y(Y)PY(Y) = PY|P1,...,PK(Y)PP1,...,PK(P1, . . . , PK).
Under
uniform a priori distribution of the powers, this is simply
ˆP
(MMSE) =
R
[0,∞)K(P1, . . . , PK)PY|P1,...,PK(P1, . . . , PK)dP1 . . . dPK
R
[0,∞)K PY|P1,...,PK(P1, . . . , PK)dP1 . . . dPK
.
However, both approaches are computationally complex, the complexity
scaling exponentially with N in particular, and require multi-dimensional line
searches on ﬁne grids for proper evaluation, which also do not scale nicely with
growing K. We will therefore no longer consider this optimal approach, which
is only useful in providing lower bounds on the inference performance for small
system dimensions. We now turn directly to alternative estimators using large
dimensional random matrix theory.

438
17. Estimation
17.2.3
Conventional large dimensional approach
The classical large dimensional approach assumes numerous sensors in order to
have much diversity in the observation vectors, as well as an even larger number
of observations so as to create an averaging eﬀect on the incoming random data.
In this situation, let us consider the system model (17.3) and now denote λ1 ≤
. . . ≤λN the ordered eigenvalues of
1
M YYH (the non-zero eigenvalues of which
are almost surely diﬀerent).
Appending Y ∈CN×M into the larger matrix Y ∈C(N+n)×M
Y =
HP
1
2 σIN
0
0
  X
W

we recognize that, conditional on H,
1
M YYH is a sample covariance matrix, for
which the population covariance matrix is
T ≜
HPHH + σ2IN 0
0
0

and the random matrix
 X
W

has independent (non-necessarily identically distributed) entries of zero mean
and unit variance. The population covariance matrix T, whose upper left entries
also form a matrix unitarily equivalent to a sample covariance matrix, clearly has
an almost sure l.s.d. as N grows large for ﬁxed or slowly growing n. Extending
Theorem 3.13 and Theorem 9.1 to c = 0 and applying them twice (once for the
population covariance matrix T and once for
1
M YYH), we ﬁnally have that, as
M, N, n →∞with M/N →∞and N/n →∞, the distribution of the largest n
eigenvalues of
1
M YYH is asymptotically almost surely composed of a mass σ2 +
P1 of weight lim n1/n, a mass σ2 + P2 of weight lim n2/n, etc. and a mass σ2 +
PK of weight lim nK/n. As for the distribution of the smallest N −n eigenvalues
of
1
M YYH, it converges to a single mass in σ2.
If σ2 is a priori known, a rather trivial estimator of Pk is then given by:
1
nk
X
i∈Nk
(λi −σ2)
where we denoted Nk = {N −PK
j=k nj + 1, . . . , N −PK
j=k+1 nj} and we recall
that λ1 ≤. . . ≤λN are the ordered eigenvalues of
1
M YYH.
This means in practice that PK is asymptotically well approximated by
the averaged value of the nK largest eigenvalues of
1
M YYH, PK−1 is well
approximated by the averaged value of the nK−1 eigenvalues before that, etc.
This also assumes that σ2 is perfectly known at the receiver. If it were not,
observe that the averaged value of the N −n smallest eigenvalues of
1
M YYH is
a consistent estimate for σ2. This therefore leads to the second estimator ˆP ∞
k for

17.2. Blind multi-source localization
439
Pk, that will constitute our reference estimator
ˆP ∞
k = 1
nk
X
i∈Nk
 λi −ˆσ2
where
ˆσ2 =
1
N −n
N−n
X
i=1
λi.
Note that the estimation of Pk only relies on nk contiguous eigenvalues
of
1
M YYH, which suggests that the other eigenvalues are asymptotically
uncorrelated from these. It will turn out that the improved (n, N, M)-consistent
estimator does take into account all eigenvalues for each k, in a certain manner.
As a reference example, we assume the scenario of three simultaneous
transmissions with transmit powers P1, P2, and P3 equal to 1/16, 1/4, and
1, respectively. We assume that each user possesses four transmit antennas, i.e.
K = 3 and n1 = n2 = n3 = 4. The receiver is an array of N = 24 sensors, that
samples as many as 128 independent (and identically distributed) observations.
The SNR is set to 20 dB. In this reference scenario, we assume that K, n1, n2,
n3 are known. The question of estimating these values will be discussed later in
Section 17.2.6. In Figure 17.8 and Figure 17.9, the performance of the estimator
ˆP ∞
k
for k ranging from one to three is evaluated, for 1000 random realizations
of Gaussian channels H, Gaussian additive noise W and QPSK modulated user
transmissions X. This is gathered in Figure 17.8 under the form of an histogram
of the estimated ˆP ∞
k
in linear scale and in Figure 17.9 under the form of the
distribution function of the marginal distribution of the ˆP ∞
k in logarithmic scale.
While our analysis ensures consistency of the ˆP ∞
k
estimates for extremely large
M and very large N, we observe that, for not-too-large system dimensions, the
ˆP ∞
k
are very biased estimates of the true Pk powers. In particular here, both
P1 and P2 are largely underestimated overall, while P3 is clearly overestimated.
Since the system dimensions under study are rather realistic in practical cognitive
(secondary) networks, i.e. the number of sensors is not assumed extremely large
and the number of observation samples is such that the exploration phase is
short, this means that the estimator ˆP ∞
k
is inappropriate to applications in
cognitive radios. These performance ﬁgures naturally call for improved estimates.
In particular, it will turn out that estimates that take into account the facts that
M is not much larger than N and that N is not signiﬁcantly larger than n will
provide unbiased estimates in the large dimensional setting, which will be seen
through simulations to be very accurate even for small system dimensions.
We start with a moment approach, which recalls the free probability and
moment-based eigen-inference methods detailed in Section 8.2 of Chapter 8.

440
17. Estimation
1
16
1
4
1
0
5
10
15
20
25
Estimated ˆP ∞
k
Density
Figure 17.8 Histogram of the ˆP ∞
k for k ∈{1, 2, 3}, P1 = 1/16, P2 = 1/4, P3 = 1,
n1 = n2 = n3 = 4 antennas per user, N = 24 sensors, M = 128 samples and
SNR = 20 dB.
1
16
1
4
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
ˆP ∞
k
Distribution function
Figure 17.9 Distribution function of the estimator ˆP ∞
k for k ∈{1, 2, 3}, P1 = 1/16,
P2 = 1/4, P3 = 1, n1 = n2 = n3 = 4 antennas per user, N = 24 sensors, M = 128
samples and SNR = 20 dB. Optimum estimator shown in dashed lines.
17.2.4
Free deconvolution approach
To be able to proceed with free deconvolution, similar to Section 17.2.2, we will
need to take some further assumptions on the system model at hand. Precisely,
we will require the random matrices H, W, and X to be ﬁlled with Gaussian

17.2. Blind multi-source localization
441
i.i.d. entries. That is, we no longer allow for arbitrary modulated transmissions
such as QPSK or QAM (at least from a theoretical point of view). This is a
key assumption to ensure asymptotic freeness of products and sums of random
matrices. For the sake of diversity in the methods developed in this chapter, we
no longer consider the compact model (17.3) for Y but instead we will see Y
as an information plus noise matrix whose information matrix is random but
has an almost surely l.s.d. We also assume that, as the system dimensions grow
large, we have M/N →c, for k ∈{1, . . . , K}, N/nk →ck and N/n →c0, with
0 < c, c0, c1, . . . , cK < ∞.
We can write
BN ≜1
M YYH = 1
M

HP
1
2 X + σW
 
HP
1
2 X + σW
H
which is such that, as M, N, and n grow to inﬁnity with positive limiting ratios,
the e.s.d. of HP
1
2 XXHP
1
2 HH converges weakly and almost surely to a limiting
distribution function. This is ensured by iterating twice Theorem 3.13: a ﬁrst
time on the sample covariance matrix P
1
2 HHHP
1
2 with population covariance
matrix P (which we assume converges towards a l.s.d. composed of K masses
in P1, . . . , PK) and a second time on the (conditional) sample covariance matrix
HP
1
2 XXHP
1
2 HH with population covariance matrix HPHH, that was proved in
the ﬁrst step to have an almost sure l.s.d.
In what follows, we will denote for readability µ∞
Z the probability distribution
associated with the l.s.d. of the Hermitian random matrix Z. We can now refer
to Theorem 4.9, which ensures under the system model above that the limiting
distribution µ∞
BN of BN (in the free probability terminology) can be expressed as
a function of the limiting distribution µ∞
1
M HP
1
2 XXHP
1
2 HH of
1
M HP
1
2 XXHP
1
2 HH.
Using the free probability operators, this reads:
µ∞
1
M HP
1
2 XXHP
1
2 HH =

µ∞
BN  µ 1
c

⊟δσ2

⊠µ 1
c
where µ 1
c is the probability distribution of a random variable with distribution
function the Mar˘cenko–Pastur law with ratio
1
c
and δσ2 the probability
distribution of a single mass in σ2. Remember that the above formula (through
the convolution operators) translates by deﬁnition the fact that all moments of
the left-hand side can be computed iteratively from the moments of the terms
in the right-hand side. Since all eigenvalue distributions under study satisfy
Carleman’s condition, Theorem 5.1, this is equivalent to saying that the l.s.d. of
1
M HP
1
2 XXHP
1
2 HH is entirely deﬁned through the l.s.d. of µ∞
BN , a fact which is
obvious from Theorem 3.13.
Instead of describing step by step the link between the moments of the
l.s.d. of BN and the moments of the l.s.d. of the deconvolved matrices, we
perform deconvolution remembering that automated algorithms can provide us
eﬀortlessly with the ﬁnal relations between the moments of the l.s.d. of BN and
the moments of the l.s.d. of P, i.e. all the sums 1
n
PK
k=1 nkP m
k , for all integers
m.

442
17. Estimation
Before
we
move
to
the
second
deconvolution
step,
we
rewrite
1
M HP
1
2 XXHP
1
2 HH under the form of a product of a scaled zero Wishart
matrix with another matrix. This is:
µ∞
1
M P
1
2 HHHP
1
2 XXH = c0µ∞
1
M HP
1
2 XXHP
1
2 HH + (1 −c0) δ0
with µ∞
1
M P
1
2 HHHP
1
2 XXH the l.s.d. of
1
M P
1
2 HHHP
1
2 XXH.
In terms of moments, this introduces a scaling factor c0 to all successive
moments of the limiting distribution. Under this form, we can proceed to the
second deconvolution step, which writes
µ∞
P
1
2 HHHP
1
2 = µ∞
1
M P
1
2 HHHP
1
2 XXH  µ
1
cc0 ,
with µ∞
P
1
2 HHHP
1
2 the l.s.d. of P
1
2 HHHP
1
2 .
Note that the scaling factor
1
M disappeared due to the fact that X has entries
of unit variance. With the same line of reasoning as before, we then write the
resulting matrix under the form of a matrix product containing a Wishart matrix
as a second factor. The step is rather immediate here as no additional mass in
zero is required to be added
µ∞
PHHH = µ∞
P
1
2 HHHP
1
2
with µ∞
PHHH the l.s.d. of PHHH.
The ﬁnal deconvolution step consists in removing the eﬀect of the scaled
Wishart matrix HHH. Incidentally, since HH has N columns and has entries
of variance 1/N, we ﬁnally have the simple expression
µ∞
P = µ∞
PHHH  µ 1
c0
where µ∞
P is the probability distribution associated with the l.s.d. of P, i.e. µ∞
P
is the probability distribution of a random variable with K masses in P1, . . . , PK
with respective weights
c
c1 , . . . ,
c
cK . This completes the free deconvolution steps.
It is therefore possible, going algebraically or numerically through the successive
deconvolution steps, to express all moments of the l.s.d. of P as a function of the
moments of the almost sure l.s.d. of BN.
Remember now from Section 5.3 that it is possible to generalize further the
concept of free deconvolution to ﬁnite dimensional random matrices, if we replace
µ∞
BN , µ∞
P and the intermediate probability distributions introduced so far by the
probability distributions of the averaged e.s.d., instead of the l.s.d. That is, for
any random matrix X ∈CN×N with compactly supported eigenvalue distribution
for all N, similar to Chapter 4, we deﬁne µN
X as the probability distribution
with mth order moment E
R
xmdF X(x)

. Substituting the µ∞
X by the µN
X
in the derivations above and changing the deﬁnitions of the free convolution
operators accordingly, see, e.g., [Masucci et al., 2011], we can ﬁnally derive the
combinatorial expressions that link the moments of the eigenvalue distribution

17.2. Blind multi-source localization
443
of BN (seen here as a random matrix and not as a particular realization) to
1
K
PK
k=1 P m
k , for all integer m.
We will not go into excruciating details as to how this expresses theoretically
and will merely state the ﬁrst three moment relations, whose output was
generated automatically by a computer software, see, e.g., [Koev; Ryan, 2009a,b].
Denote
pm ≜
K
X
k=1
nk
n P m
k
and
bm ≜1
N E [tr Bm
N]
where the expectation is taken over the joint realization of the random matrices
H, X, and W. In the case where n1 = . . . = nK, the ﬁrst moments pm and bm
relate together as
b1 = N −1np1 + 1
b2 =
 N −2M −1n + N −1n

p2 +
 N −2n2 + N −1M −1n2
p2
1
+
 2N −1n + 2M −1n

p1 +
 1 + NM −1
b3 =
 3N −3M −2n + N −3n + 6N −2M −1n + N −1M −2n + N −1n

p3
+
 6N −3M −1n2 + 6N −2M −2n2 + 3N −2n2 + 3N −1M −1n2
p2p1
+
 N −3M −2n3 + N −3n3 + 3N −2M −1n3 + N −1M −2n3
p3
1
+
 6N −2M −1n + 6N −1M −2n + 3N −1n + 3M −1n

p2
+
 3N −2M −2n2 + 3N −2n2 + 9N −1M −1n2 + 3M −2n2
p2
1
+
 3N −1M −2n + 3N −1n + 9M −1n + 3NM −2n

p1.
(17.4)
As a consequence, if L instances of the random matrices Y(ω1), . . . , Y(ωL) are
available, and L grows large, then asymptotically the averaged moments of the
e.s.d. of the
1
M Y(ωi)Y(ωi)H converge to the moments b1, b2, . . .. This however
requires that multiple realizations of Y matrices are indeed available, and that
changes the conditions of the problem in the ﬁrst place. Nonetheless, if eﬀectively
only one such matrix Y is available, it is possible to handily use this multi-
instance approach by breaking down Y into several parts of smaller column
dimension. That is, Y can be rewritten under the form Y = [Y1, . . . , YL],
where Yi ∈CN×(M/L), for some L which divides M. Note importantly that this
approach is totally empirical and not equivalent to L independent realizations
of the random Y matrix for all Yi, since the channel matrix H is kept identical
for all realizations.
If large dimensions are assumed, then the terms that go to zero in the
above relations must be discarded. Two diﬀerent approaches can then be taken
to use the moment approach. Either we assume large dimensions and keeps
the realization of Y as is, or we may rewrite Y under the form of multiple
submatrices and use the approximated averaged relations. In either case, the

444
17. Estimation
relations between consecutive moments must be dealt with carefully. We develop
hereafter two estimation methods, already introduced in Chapter 8, namely the
fast but inaccurate Newton–Girard approach and a computationally expensive
method, which we will abusively call the maximum-likelihood approach.
17.2.4.1 Newton–Girard method
Let us work in the small system dimension regime and consider the scenario
where the realization of Y is divided into L submatrices Y1, . . . , YL. The
Newton–Girard approach consists in taking, for m ∈{1, . . . , K}, the estimate
ˆbm =
1
NL
L
X
l=1
tr(YlYH
l )m
of bm for m = 1, 2, . . .. Remember that those estimates are not L-consistent,
since the random realization of H is the same for all Yl (unless the time
between successive observations of Yi is long enough for the independence of
the successive H matrices to hold). From ˆbm, we may then successively take
estimates of p1, . . . , pm by simply recursively inverting the formulas (17.4), with
bm replaced by ˆbm. These estimates are denoted ˆp1, . . . , ˆpK.
Newton–Girard formulas, see Section 5.2, allow us to recover estimates
ˆP (mom)
1
, . . . , ˆP (mom)
K
of the transmit powers P1, . . . , PK by inverting the relations
K
X
k=1
nk
n

ˆP (mom)
k
m
= ˆpm
for m ∈{1, . . . , K}. For instance, in the case of our example where K = 3,
n1 = n2 = n3, we have that ˆP (mom)
1
, ˆP (mom)
2
, and ˆP (mom)
3
are the roots of the
polynomial in X

−9
2 ˆp3
1 + 9
2 ˆp1ˆp2 −ˆp3

X2 +
9
2 ˆp1 −3
2 ˆp2

X + 1 = 0.
This method is simple and does not require a lot of computational resources,
but fails in accuracy for several reasons discussed in Section 5.2 and which
we presently recall. First, the free deconvolution approach aims at providing
consistent estimates of the successive moments of the e.s.d. of P, in order to
obtain a good estimate on the e.s.d. of P, while our current objective is rather to
estimate some or all entries of P instead. Second, the Newton–Girard approach
does not take into account the fact that the estimated random moments ˆbm, and
consequently the moments ˆpm, do not all have the same variance around their
means. Inverting the moment relations linking the bm to the pm by replacing
the moments by their estimates assumes implicitly that all estimated moments
equally well approximate the true values, which is in fact far from being correct.
Finally, the roots of the polynomial that lead to the estimates ˆP (mom)
k
are not
ensured to be non-negative and worse not ensured to be real. Post-processing
is then required to deal with such estimates. In the simulations below, we will

17.2. Blind multi-source localization
445
1
16
1
4
1
0
5
10
15
20
Estimated ˆP (mom)
k
Density
Figure 17.10 Histogram of the ˆP (mom)
k
for k ∈{1, 2, 3}, P1 = 1/16, P2 = 1/4, P3 = 1,
n1 = n2 = n3 = 4 antennas per user, N = 24 sensors, M = 128 samples, and
SNR = 20 dB.
simply discard the realizations leading to purely complex or real negative values,
altering therefore the ﬁnal result.
Figure
17.10
and
Figure
17.11
provide
the
performance
of
the
free
deconvolution approach with Newton–Girard inversion for the same system
model as before, i.e. three simultaneous transmissions, each user being equipped
with four antennas, N = 24 sensors, M = 128 samples, and the SNR is 20 dB.
We consider the case where L = 1, which is observed to perform overall similarly
as the case where L is taken larger (with M scaled accordingly), with some
minor diﬀerences. Notice that, although we now moved from a nested M- and
N-consistent estimator to an (N, n, M)-consistent estimator of P1, . . . , PK, the
contestable Newton–Girard inversion has very awkward side eﬀects, both in
terms of bias for small dimensions, especially for the smallest powers, and in
terms of variance of the estimate, which is also no match for the variance of the
previous estimator when it comes to estimating very low powers. In terms of
absolute mean square error, the conventional approach is therefore still better
here. More elaborate post-processing methods are thus demanded to cope with
the issues of the Newton–Girard inversion. This is what the subsequent section
is devoted to.
17.2.4.2 ML and MMSE methods
The idea is now to consider the distribution of the ˆbm moment estimates and
to take the estimated powers ( ˆP (mom,ML)
1
, . . . , ˆP (mom,ML)
K
) as the K-tuple that
maximizes some reward function of the joint variable ˆb1, . . . ,ˆbT , for some integer
T. A classical approach is to consider as a reward function the maximum

446
17. Estimation
1
16
1
4
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
ˆP (mom)
k
Distribution function
Figure 17.11 Distribution function of the estimator ˆP (mom)
k
for k ∈{1, 2, 3}, P1 = 1/16,
P2 = 1/4, P3 = 1, n1 = n2 = n3 = 4 antennas per user, N = 24 sensors, M = 128,
samples and SNR = 20 dB. Optimum estimator shown in dashed lines.
likelihood of ˆb1, . . . ,ˆbT given ( ˆP (mom,ML)
1
, . . . , ˆP (mom,ML)
K
) or the minimum mean
square error in the estimation of ˆb1, . . . ,ˆbT . This however implies that the
joint probability distribution of ˆb1, . . . ,ˆbT is known. Reminding that BN can
be written under the form of a covariance matrix with population covariance
matrix whose e.s.d. converges almost surely to a limit distribution function,
we are tempted to use Theorem 3.17 and to state that, for all ﬁnite T, the
vector N(ˆb1 −E[ˆb1], . . . ,ˆbT −E[ˆbT ]) converges in distribution and almost surely
towards a T-variate Gaussian random variable. However, the assumptions of
Theorem 3.17 do not let the sample covariance matrix be random, so that the
central limit of the vector N(ˆb1 −E[ˆb1], . . . ,ˆbT −E[ˆbT ]) cannot be stated but
only conjectured. For the rest of the coming derivation, we will assume that the
result does hold, as it may likely be the case. We therefore need to compute
the covariance matrix of the vector N(ˆb1 −E[ˆb1], . . . ,ˆbT −E[ˆbT ]), i.e. we need to
compute all cross-moments
CN
ij (P1, . . . , PK) ≜N 2E
h
ˆbi −E[ˆbi]
 
ˆbj −E[ˆbj]
i
for 1 ≤i, j ≤T, for some integer T, where the exponent N reminds the
system dimension. Call CN(P1, . . . , PK) ∈CT ×T
the matrix with entries
CN
ij (P1, . . . , PK). As N grows large, CN(P1, . . . , PK) converges point-wise to
some matrix, which we denote C∞(P1, . . . , PK). The central limit theorem,
be it valid in our situation, would therefore state that, asymptotically, the
vector N(ˆb1 −E[ˆb1], . . . ,ˆbT −E[ˆbT ]) is jointly Gaussian with zero mean and
covariance matrix C∞(P1, . . . , PK). We therefore determine the estimate vector

17.2. Blind multi-source localization
447
P (mom,ML) ≜( ˆP (mom,ML)
1
, . . . , ˆP (mom,ML)
K
) as
P (mom,ML) = arg
inf
( ˜
P1,..., ˜
PK)
˜
Pi≥0
(ˆb −E[ˆb])CN( ˜P1, . . . , ˜PK)−1(ˆb −E[ˆb])T
where ˆb = (ˆb1, . . . ,ˆbT )T and the expectations E[ˆb] are conditioned with respect
to ( ˜P1, . . . , ˜PK) and can therefore be computed explicitly as described in the
previous sections.
Similar to the maximum likelihood vector estimator P (mom,ML), we can deﬁne
the vector estimator P (mom,MMSE) ≜( ˆP (mom,MMSE)
1
, . . . , ˆP (mom,MMSE)
K
), which
realizes the minimum mean square error of P1, . . . , PK as
P (mom,MMSE)
= 1
Z
Z
˜
P d ˜
P =( ˜
P1,..., ˜
PK)T
˜
Pm≥0
˜P
det CN( ˜P1, . . . , ˜PK)
e−(ˆb−E[ˆb])CN( ˜
P1,..., ˜
PK)−1(ˆb−E[ˆb])T
with Z a normalization factor.
Practically speaking, the problem is two-fold. First, either in the maximum-
likelihood or in the minimum mean square error approach, a multi-dimensional
line search is required. This is obviously extremely expensive compared to the
Newton–Girard method. Simpliﬁcation methods, such as iterative algorithms can
be thought of, although they still require a lot of computations and are rarely
ensured to converge to the global extremum sought for. The second problem
is that the on-line or oﬀ-line computation of CN(P1, . . . , PK) is also extremely
tedious. Note that the matrix CN(P1, . . . , PK) depends on the parameters K, M,
N, n1, . . . , nK, P1, . . . , PK and σ2, and is of size T × T. Based on combinatorial
approaches, recent work has led to the possibility of a partly on-line, partly
oﬀ-line computation of such matrices. Typically, it is intolerable that tables of
CN(P1, . . . , PK) be kept in memory for multiple values of K, M, N, n1, . . . , nK,
σ2 and P1, . . . , PK. It is nonetheless acceptable that reference matrices be kept in
memory so to fast compute on-line CN(P1, . . . , PK) for all K, M, N, n1, . . . , nK,
σ2 and P1, . . . , PK.
17.2.5
Analytic method
We ﬁnally introduce in this section the method based on G-estimation, which
will be seen to have numerous advantages compared to the previous methods,
although it is slightly handicapped by a cluster separability condition. The
approach relies heavily on the recent techniques from Mestre, established in
[Mestre, 2008b] and discussed at length in Chapter 8. This demands much more
work than the combinatorial and rather automatic moment free deconvolution
approach. Nevertheless, it appears that this approach can somewhat be
reproduced for diﬀerent models, as long as exact separation theorems, such as
Theorem 7.2 or Theorem 7.8, are available.

448
17. Estimation
The main strategy is the following:
• We ﬁrst need to study the asymptotic spectrum of BN ≜
1
M YYH, as all
system dimensions (N, n, M) grow large (remember that K is ﬁxed). For
this, we will proceed by
– determining the almost sure l.s.d. of BN as all system dimensions grow
large with ﬁnite limiting ratio. Practically, this will allow us to connect the
asymptotic spectrum of BN to the spectrum of P;
– studying the exact separation of the eigenvalues of BN in clusters of
eigenvalues. This is necessary ﬁrst to determine whether the coming step
of complex integration is possible and second to determine a well-chosen
integration contour for the estimation of every Pk.
• Then, we will write Pk under the form of the complex integral of a functional
of the spectrum of P over this well-chosen contour. Since the spectrum of P
can be linked to that of BN (at least asymptotically) through the previous
step, a change of variable will allow us to rewrite Pk under the form of an
integral of some functional of the l.s.d. of BN. This point is the key step
in our derivation, where Pk is now connected to the observation matrix Y
(although only in an asymptotic way).
• Finally, the estimate ˆPk of Pk will be computed from the previous step by
replacing the l.s.d. of BN by its e.s.d., i.e. by the truly observed eigenvalues
of
1
M YYH in the expression relating Pk to the l.s.d. of BN.
We therefore divide this section into three subsections that analyze successively
the almost sure l.s.d. of BN, then the conditions for cluster separation, and ﬁnally
the actual calculus of the power estimator.
17.2.5.1 Limiting spectrum of BN
In this section, we prove the following result.
Theorem 17.5. Let BN =
1
M YYH, with Y deﬁned as in (17.2). Then, for M,
N, n growing large with limit ratios M/N →c, N/nk →ck, 0 < c, c1, . . . , cK <
∞, the e.s.d. F BN of BN converges almost surely to the distribution function F,
whose Stieltjes transform mF (z) satisﬁes, for z ∈C+
mF (z) = cmF (z) + (c −1)1
z
(17.5)
where mF (z) is the unique solution with positive imaginary part of the implicit
equation in mF
1
mF
= −σ2 + 1
f −
K
X
k=1
1
ck
Pk
1 + Pkf
(17.6)
in which we denoted f the value
f = (1 −c)mF −czm2
F .

17.2. Blind multi-source localization
449
The rest of this section is dedicated to the proof of Theorem 17.5. First
remember that the matrix Y in (17.2) can be extended into the larger sample
covariance matrix Y ∈C(N+n)×M (conditionally on H)
Y =
HP
1
2 σIN
0
0
  X
W

.
From Theorem 3.13, since H has independent entries with ﬁnite fourth order
moment, we have that the e.s.d. of HPHH converges weakly and almost surely to
a limit distribution G as N, n1, . . . , nK →∞with N/nk →ck > 0. For z ∈C+,
the Stieltjes transform mG(z) of G is the unique solution with positive imaginary
part of the equation in mG
z = −1
mG
+
K
X
k=1
1
ck
Pk
1 + PkmG
.
(17.7)
The almost sure convergence of the e.s.d. of HPHH ensures the almost sure
convergence of the e.s.d. of
  HPHH+σ2IN 0
0
0

. Since mG(z) evaluated at z ∈C+ is
the Stieltjes transform of the l.s.d. of HPHH + σ2IN evaluated at z + σ2, adding
n zero eigenvalues, we ﬁnally have that the e.s.d. of
  HPHH+σ2IN 0
0
0

tends almost
surely to a distribution H whose Stieltjes transform mH(z) satisﬁes
mH(z) =
c0
1 + c0
mG(z −σ2) −
1
1 + c0
1
z
(17.8)
for z ∈C+ and c0 the limit of N/n, i.e. c0 = (c−1
1
+ . . . + c−1
K )−1.
As a consequence, the sample covariance matrix
1
M YYH has a population
covariance matrix which is not deterministic but whose e.s.d. has an almost sure
limit H for increasing dimensions. Since X and W have entries with ﬁnite fourth
order moment, we can again apply Theorem 3.13 and we have that the e.s.d. of
BN ≜
1
M YHY converges almost surely to the limit F whose Stieltjes transform
mF (z) is the unique solution in C+ of the equation in mF
z = −1
mF
+ 1
c

1 + 1
c0
 Z
t
1 + tmF
dH(t)
= −1
mF
+
1 + 1
c0
cmF

1 −
1
mF
mH

−1
mF

(17.9)
for all z ∈C+.
For z ∈C+, mF (z) ∈C+. Therefore −1/mF (z) ∈C+ and we can evaluate
(17.8) at −1/mF (z). Combining (17.8) and (17.9), we then have
z = −1
c
1
mF (z)2 mG

−
1
mF (z) −σ2

+
1
c −1

1
mF (z)
(17.10)

450
17. Estimation
where, according to (17.7), mG(−1/mF (z) −σ2) satisﬁes
1
mF (z) = −σ2 +
1
mG(−
1
mF (z) −σ2) −
K
X
k=1
1
ck
Pk
1 + PkmG(−
1
mF (z) −σ2).
(17.11)
Together with (17.10), denoting f(z) = mG(−
1
mF (z) −σ2) = (1 −c)mF (z) −
czmF (z)2, this is exactly (17.6).
Since the eigenvalues of the matrices BN and BN only diﬀer by M −N zeros,
we also have that the Stieltjes transform mF (z) of the l.s.d. of BN satisﬁes
mF (z) = cmF (z) + (c −1)1
z .
(17.12)
This completes the proof of Theorem 17.5. For further usage, notice here that
(17.12) provides a simpliﬁed expression for mG(−1/mF (z) −σ2). Indeed, we
have:
mG(−1/mF (z) −σ2) = −zmF (z)mF (z).
(17.13)
Therefore, the support of the (almost sure) l.s.d. F of BN can be evaluated as
follows: for any z ∈C+, mF (z) is given by (17.5), in which mF (z) solves (17.6);
the inverse Stieltjes transform formula (3.2) then allows us to evaluate F from
mF (z), for values of z spanning over the set {z = x + iy, x > 0} and y small. This
is depicted in Figure 17.12, where P has three distinct values P1 = 1, P2 = 3,
P3 = 10 and n1 = n2 = n3, N/n = 10, M/N = 10, σ2 = 0.1, as well as in Figure
17.13 for the same setup but with P3 = 5.
Two remarks on Figure 17.12 and Figure 17.13 are of fundamental importance
to the following. Similar to the study carried out in Chapter 7, it appears that
the asymptotic l.s.d. F of BN is compactly supported and divided into up to
K + 1 disjoint compact intervals, which we further refer to as clusters. Each
cluster can be mapped onto one or many values in the set {σ2, P1, . . . , PK}. For
instance, in Figure 17.13, the ﬁrst cluster is mapped to σ2, the second cluster to
P1, and the third cluster to the set {P2, P3}. Depending on the ratios c and c0
and on the particular values taken by P1, . . . , PK and σ2, these clusters are either
disjoint compact intervals, as in Figure 17.12, or they may overlap to generate
larger compact intervals, as in Figure 17.13. As is in fact required by the law
of large numbers, for increasing c and c0, the asymptotic spectrum tends to
be divided into thinner and thinner clusters. The inference technique proposed
hereafter relies on the separability of the clusters associated with each Pi and
to σ2. Precisely, to be able to derive a consistent estimate of the transmitted
power Pk, the cluster associated with Pk in F, number it cluster kF , must be
distinct from the neighboring clusters (k −1)F and (k + 1)F , associated with
Pk−1 and Pk+1, respectively (when they exist), and also distinct from cluster 1
in F associated with σ2. As such, in the scenario of Figure 17.13, our method
will be able to provide a consistent estimate for P1, but (so far) will not succeed

17.2. Blind multi-source localization
451
0.1 1
3
10
0
0.025
0.05
0.075
0.1
Estimated powers
Density
Asymptotic spectrum
Empirical eigenvalues
Figure 17.12 Empirical and asymptotic eigenvalue distribution of
1
M YYH when P has
three distinct entries P1 = 1, P2 = 3, P3 = 10, n1 = n2 = n3, c0 = 10, c = 10,
σ2 = 0.1. Empirical test: n = 60.
0.1
1
3
5
0
0.025
0.05
0.075
0.1
Estimated powers
Density
Asymptotic spectrum
Empirical eigenvalues
Figure 17.13 Empirical and asymptotic eigenvalue distribution of
1
M YYH when P has
three distinct entries P1 = 1, P2 = 3, P3 = 5, n1 = n2 = n3, c0 = 10, c = 10, σ2 = 0.1.
Empirical test: n = 60.
in providing a consistent estimate for either P2 or P3, since 2F = 3F . We will see
that a consistent estimate for (P2 + P3)/2 is accessible though. Secondly, notice
that the empirical eigenvalues of BN are all inside the asymptotic clusters and,
most importantly, in the case where cluster kF is distinct from either cluster 1,

452
17. Estimation
(k −1)F or (k + 1)F , observe that the number of eigenvalues in cluster kF is
exactly nk. This is what we referred to as exact separation in Chapter 7. The
exact separation for the current model originates from a direct application of
the exact separation for the sample covariance matrix of Theorem 7.2 and is
provided below in Theorem 17.7 for more generic model assumptions than in
Theorem 7.2. This is further discussed in the subsequent sections.
17.2.5.2 Condition for separability
In the following, we are interested in estimating consistently the power Pk for
a given ﬁxed k ∈{1, . . . , K}. We recall that consistency means here that, as
all system dimensions grow large with ﬁnite asymptotic ratios, the diﬀerence
ˆPk −Pk between the estimate ˆPk of Pk and Pk itself converges to zero with
probability one. As previously mentioned, we will show by construction in the
subsequent section that such an estimate is only achievable if the cluster mapped
to Pk in F is disjoint from all other clusters. The purpose of the present section
is to provide suﬃcient conditions for cluster separability. To ensure that cluster
kF (associated with Pk in F) is distinct from cluster 1 (associated with σ2) and
clusters iF , i ̸= k (associated with all other Pi), we assume now and for the rest
of this section that the following conditions are fulﬁlled:
(i) k satisﬁes Assumption 17.1, given as follows.
Assumption 17.1.
K
X
r=1
1
cr
(PrmG,k)2
(1 + PrmG,k)2 < 1,
(17.14)
K
X
r=1
1
cr
(PrmG,k+1)2
(1 + PrmG,k+1)2 < 1
(17.15)
with mG,1, . . . , mG,K the K real solutions to the equation in mG
K
X
r=1
1
cr
(PrmG)3
(1 + PrmG)3 = 1
(17.16)
with the convention mG,K+1 = 0, and
(ii) k satisﬁes Assumption 17.2 as follows.
Assumption 17.2. Denoting, for j ∈{1, . . . , K}
jG ≜# {i ≤j | i satisﬁes Assumption 17.1}
(17.17)
we have the two conditions
1 −c0
c0
(σ2mF ,kG)2
(1 + σ2mF ,kG)2 +
kG−1
X
r=1
1
cr
(x+
G,r + σ2)2m2
F ,kG
(1 + (x+
G,r + σ2)mF ,kG)2
+
KG
X
r=kG
1
cr
(x−
G,r + σ2)2m2
F ,kG
(1 + (x−
G,r + σ2)mF ,kG)2 < c

17.2. Blind multi-source localization
453
and
1 −c0
c0
(σ2mF ,kG+1)2
(1 + σ2mF ,kG+1)2 +
kG
X
r=1
1
cr
(x+
G,r + σ2)2m2
F ,kG+1
(1 + (x+
G,r + σ2)mF ,kG+1)2
+
KG
X
r=kG+1
1
cr
(x−
G,r + σ2)2m2
F ,kG+1
(1 + (x−
G,r + σ2)mF ,kG+1)2 < c
where x−
G,i, x+
G,i, i ∈{1, . . . , KG}, are deﬁned by
x−
G,i = −
1
m−
G,i
+
K
X
r=1
1
cr
Pr
1 + Prm−
G,i
(17.18)
x+
G,i = −
1
m+
G,i
+
K
X
r=1
1
cr
Pr
1 + Prm+
G,i
,
(17.19)
with m−
G,1, m+
G,1, . . . , m−
G,KG, m+
G,KG the 2KG real roots of (17.14) and mF ,j,
j ∈{1, . . . , KG + 1}, the jth real root (in increasing order) of the equation in
mF
1 −c0
c0
(σ2mF )3
(1 + σ2mF )3 +
j−1
X
r=1
1
cr
(x+
G,r + σ2)3m3
F
(1 + (x+
G,r + σ2)mF )3
+
KG
X
r=j
1
cr
(x−
G,r + σ2)3m3
F
(1 + (x−
G,r + σ2)mF )3 = c.
Although diﬃcult to fathom at this point, the above assumptions will be
clariﬁed later. We give here a short intuitive explanation of the role of every
condition. Assumption 17.1 is a necessary and suﬃcient condition for cluster
kG, that we deﬁne as the cluster associated with Pk in G (the l.s.d. of HPHH),
to be distinct from the clusters (k −1)G and (k + 1)G, associated with Pk−1
and Pk+1 in G, respectively. Note that we implicitly assume a unique mapping
between the Pi and clusters in G; this statement will be made more rigorous
in subsequent sections. Assumption 17.1 only deals with the inner HPHH
covariance matrix properties and ensures speciﬁcally that the powers to be
estimated diﬀer suﬃciently from one another for our method to be able to resolve
them.
Assumption 17.2 deals with the complete BN matrix model. It is however a
non-necessary but suﬃcient condition for cluster kF , associated with Pk in F,
to be distinct from clusters (k −1)F , (k + 1)F , and 1 (cluster 1 being associated
with σ2). The exact necessary and suﬃcient condition will be stated further in the
next sections; however, the latter is not exploitable in practice and Assumption
17.2 will be shown to be an appropriate substitute. Assumption 17.2 is concerned
with the value of c necessary to avoid:
(i) cluster kG (associated with Pk in G) to further overlap the clusters kG −1
and kG + 1 associated with Pk−1 and Pk+1,

454
17. Estimation
−15
−10
−5
0
5
10
15
20
0
20
40
60
80
100
cluster separability region
c = 10
SNR≜
1
σ2 [dB]
c
P3 separability limit
P2 separability limit
P1 separability limit
Figure 17.14 Limiting ratio c as a function of σ2 to ensure consistent estimation of
P1 = 1, P2 = 3 and P3 = 10, c0 = 10, c1 = c2 = c3.
(ii) cluster 1 associated with σ2 in F to merge with cluster kF .
As will become evident in the next sections, when σ2 is large, the tendency
is for the cluster associated with σ2 to become large and overlap the clusters
associated with P1, then P2, etc. To counter this eﬀect, we must increase c, i.e.
take more signal samples. Figure 17.14 depicts the critical ratio c that satisﬁes
Assumption 17.2 as a function of σ2, in the case K = 3, (P1, P2, P3) = (1, 3, 10),
c0 = 10, c1 = c2 = c3. Notice that, in the case c = 10, below σ2 ≃1, it is possible
to separate all clusters, which is compliant with Figure 17.12 where σ2 = 0.1.
As a consequence, under the assumption (partly proved later) that our
proposed method can only perform consistent power estimation when the cluster
separability conditions are met, we have two ﬁrst conclusions:
• if we want to increase the sensitivity of the estimator, i.e. to be able to separate
two sources of close transmit powers, we need to increase the number of sensors
(by increasing c0);
• if we want to detect and reliably estimate power sources in a noise-limited
environment, we need to increase the number of sensed samples (by increasing
c).
In the subsequent section, we study the properties of the asymptotic spectrum
of HPHH and BN in more detail. These properties will lead to an explanation
for Assumptions 17.1 and 17.2. Under those assumptions, we will then derive the
Stieltjes transform-based power estimator.

17.2. Blind multi-source localization
455
17.2.5.3 Multi-source power inference
In the following, we ﬁnally prove the main result of this section, which provides
the G-estimator ˆP1, . . . , ˆPK of the transmit powers P1, . . . , PK.
Theorem 17.6. Let BN ∈CN×N be deﬁned as BN =
1
M YYH with Y deﬁned
as in (17.2), and λ = (λ1, . . . , λN), λ1 ≤. . . ≤λN, be the vector of the ordered
eigenvalues of BN. Further, assume that the limiting ratios c0, c1, . . . , cK, c and
P are such that Assumptions 17.1 and 17.2 are fulﬁlled for some k ∈{1, . . . , K}.
Then, as N, n, M grow large, we have:
ˆPk −Pk
a.s.
−→0
where the estimate ˆPk is given by:
• if M ̸= N
ˆPk =
NM
nk(M −N)
X
i∈Nk
(ηi −µi)
• if M = N
ˆPk =
N
nk(N −n)
X
i∈Nk


N
X
j=1
ηi
(λj −ηi)2


−1
in which Nk = {N −PK
i=k ni + 1, . . . , N −PK
i=k+1 ni}, η1 ≤. . . ≤ηN are the
ordered eigenvalues of the matrix diag(λ) −1
N
√
λ
√
λ
T and µ1 ≤. . . ≤µN are
the ordered eigenvalues of the matrix diag(λ) −
1
M
√
λ
√
λ
T.
Remark 17.3. We immediately notice that, if N < n, the powers P1, . . . , Pl, with l
the largest integer such that N −PK
i=l ni < 0, cannot be estimated since clusters
may be empty. The case N ≤n turns out to be of no practical interest as clusters
always merge and no consistent estimate of either Pi can be described.
The approach pursued to prove Theorem 17.6 relies strongly on the original
idea of [Mestre, 2008a] which was detailed for the case of sample covariance
matrices in Section 8.1.2 of Chapter 8. From Cauchy’s integration formula,
Theorem 8.5
Pk = ck
1
2πi
I
Ck
1
ck
ω
Pk −ω dω
= ck
1
2πi
I
Ck
K
X
r=1
1
cr
ω
Pr −ω dω
(17.20)
for any negatively oriented contour Ck ⊂C, such that Pk is contained in the
surface described by the contour, while for every i ̸= k, Pi is outside this surface.
The strategy unfolds as follows: we ﬁrst propose a convenient integration contour
Ck which is parametrized by a function of the Stieltjes transform mF (z) of the

456
17. Estimation
−1
−1
3
−1
100
1
3
10
m−
G,1
m+
G,1
mG,1
mG
xG(mG)
xG(mG)
Support of G
Figure 17.15 xG(mG) for mG real, P diagonal composed of three evenly weighted
masses in 1, 3 and 10. Local extrema are marked in circles, inﬂexion points are
marked in squares.
l.s.d. of BN; this is the technical part of the proof. We then proceed to a variable
change in (17.20) to express Pk as a function of mF (z). We evaluate the complex
integral resulting from replacing the limiting mF (z) in (17.20) by its empirical
counterpart mBN (z) = 1
N tr(BN −zIN)−1. This new integral, whose value we
name ˆPk, is shown to be almost surely equal to Pk in the large N limit. It then
suﬃces to evaluate ˆPk, which is just a matter of residue calculus.
We start by determining the integration contour Ck. For this, we ﬁrst need to
study the distributions G and F in more detail, following the study carried out
in Chapter 7.
Properties of G and F.
First consider the matrix HPHH, and let the function xG(mG) be deﬁned, for
scalars mG ∈R \ {0, −1/P1, . . . , −1/PK}, by
xG(mG) = −1
mG
+
K
X
r=1
1
cr
Pr
1 + PrmG
.
(17.21)
The function xG(mG) is depicted in Figure 17.15 and Figure 17.16 for the
cases where c0 = 10, c1 = c2 = c3 and (P1, P2, P3) equal (1, 3, 10) and (1, 3, 5),
respectively. As expected by Theorem 7.4, xG(mG) is increasing for mG such that
xG(mG) is outside the support of G. Note now that the function xG presents
asymptotes in the positions −1/P1, . . . , −1/PK
lim
mG↓(−1/Pi) xG(mG) = ∞
lim
mG↑(−1/Pi) xG(mG) = −∞

17.2. Blind multi-source localization
457
−1
−1
3 −1
5
zero
1
3
5
mG
xG(mG)
xG(mG)
Support of G
Figure 17.16 xG(mG) for mG real, P diagonal composed of three evenly weighted
masses in 1, 3 and 5. Local extrema are marked in circles, inﬂexion points are marked
in squares.
−1
−1
3
−1
10zero
0.1
1
3
10
mF
xH(mF )
xH(mF )
Support of F
Support of −1/H
Figure 17.17 xF (mF ) for mF real, σ2 = 0.1, c = c0 = 10, P diagonal composed of
three evenly weighted masses in 1, 3 and 10. The support of F is read on the right
vertical axis.
and that xG(mG) →0+ as mG →−∞. Note also that, on its restriction to the
set where it is non-decreasing, xG is increasing. To prove this, let mG and m⋆
G be
two distinct points such that xG(mG) > 0 and xG(m⋆
G) > 0, and m⋆
G < mG < 0.

458
17. Estimation
We indeed have1
xG(mG) −xG(m⋆
G) = mG −m⋆
G
mGm⋆
G
"
1 −
K
X
r=1
1
cr
P 2
r
(Pr +
1
mG )(Pr +
1
m⋆
G )
#
.
(17.22)
Noticing that, for Pi > 0
"
Pi
Pi +
1
mG
−
Pi
Pi +
1
m⋆
G
#2
=
P 2
i
(Pi +
1
mG )2 +
P 2
i
(Pi +
1
m⋆
G )2 −2
P 2
i
(Pi +
1
mG )(Pi +
1
m⋆
G )
> 0
we have, after taking the opposite and the sum over i ∈{1, . . . , K} and adding
2 on both sides
 
1 −
K
X
r=1
1
cr
P 2
r
(Pr +
1
mG )2
!
+
 
1 −
K
X
r=1
1
cr
P 2
r
(Pr +
1
m⋆
G )2
!
< 2 −2
K
X
r=1
1
cr
P 2
r
(Pr +
1
mG )(Pr +
1
m⋆
G ).
Since we also have
x′
G(mG) =
1
m2
G
"
1 −
K
X
r=1
1
cr
P 2
r
(Pr +
1
mG )2
#
≥0
x′
G(m⋆
G) =
1
(m⋆
G)2
"
1 −
K
X
r=1
1
cr
P 2
r
(Pr +
1
m⋆
G )2
#
≥0
we conclude that the term in brackets in (17.22) is positive and then that
xG(mG) −xG(m⋆
G) > 0. Hence xG is increasing on its restriction to the set where
it is non-decreasing.
Notice also that xG, both in Figure 17.15 and Figure 17.16, has exactly
one inﬂexion point on each open set (−1/Pi−1, −1/Pi), for i ∈{1, . . . , K}, with
convention P0 = 0+. This is proved by noticing that x′′
G(mG) = 0 is equivalent
to
K
X
r=1
1
cr
P 3
r m3
G
(1 + PrmG)3 −1 = 0.
(17.23)
Now, the left-hand side of (17.23) has derivative along mG
3
K
X
r=1
1
cr
P 3
r m2
G
(1 + PrmG)4
(17.24)
which is always positive. Notice that the left-hand side of (17.23) has asymptotes
for mG = −1/Pi for all i ∈{1, . . . , K} and has limits 0 as mG →0 and 1/c0 −1
1 This proof is borrowed from the proof of [Mestre, 2008b], with diﬀerent notations.

17.2. Blind multi-source localization
459
as mG →−∞. If c0 > 1, Equation (17.23) (and then x′′
G(mG) = 0) therefore
has a unique solution in (−1/Pi−1, −1/Pi) for all i ∈{1, . . . , K}. When xG is
increasing somewhere on (−1/Pi−1, −1/Pi), the inﬂexion point, i.e. the solution
of x′′
G(mG) = 0 in (−1/Pi−1, −1/Pi), is necessarily found in the region where xG
increases. If c0 ≤1, the leftmost inﬂexion point may not exist.
From the discussion above and from Theorem 7.4 (and its corollaries discussed
in Section 7.1.3), it is clear that the support of G is divided into KG ≤K compact
subsets [x−
G,i, x+
G,i], i ∈{1, . . . , KG}. Also, if c0 > 1, G has an additional mass in
zero of probability G(0) −G(0−) = (c0 −1)/c0; this mass will not be counted as
a cluster in G. Observe that every Pi can be uniquely mapped to a corresponding
subset [x−
G,j, x+
G,j] in the following fashion. The power P1 is mapped onto the ﬁrst
cluster in G; we then have 1G = 1. Then the power P2 is either mapped onto
the second cluster in G if xG increases in the subset (−1/P1, −1/P2), which is
equivalent to saying that x′
G(mG,2) > 0 for mG,2 the only solution to x′′
G(mG) = 0
in (−1/P1, −1/P2); in this case, we have 2G = 2 and the clusters associated with
P1 and P2 in G are distinct. Otherwise, if x′
G(mG,2) ≤0, P2 is mapped onto the
ﬁrst cluster in F; in this case, 2G = 1. The latter scenario visually corresponds
to the case when P1 and P2 engender “overlapping clusters.” More generally, Pj,
j ∈{1, . . . , K}, is uniquely mapped onto the cluster jG such that
jG = # {i ≤j | min[x′
G(mG,i), x′
G(mG,i+1)] > 0}
with convention mG,K+1 = 0, which is exactly
jG = # {i ≤j | i satisﬁes Assumption 17.1}
when c0 > 1. If c0 ≤1, mG,1, the zero of x′′
G in (−∞, −1/P1) may not exist. If
c0 < 1, we claim that P1 cannot be evaluated (as was already observed in Remark
17.3). The special case when c0 = 1 would require a restatement of Assumption
17.1 to handle the special case of P1; this will however not be done, as it will
turn out that Assumption 17.2 is violated for P1 if σ2 > 0, which we assume.
In the particular case of the power Pk of interest in Theorem 17.6, because of
Assumption 17.1, x′
G(mG,k) > 0. Therefore the index kG of the cluster associated
with Pk in G satisﬁes kG = (k −1)G + 1 (with convention 0G = 0). Also, from
Assumption 17.1, x′
G(mG,k+1) > 0. Therefore (k + 1)G = kG + 1. In that case,
we have that Pk is the only power mapped to cluster kG in G, and then we have
the required cluster separability condition.
We now proceed to the study of F, the almost sure limit spectrum distribution
of BN. In the same way as previously, we have that the support of F is fully
determined by the function xF (mF ), deﬁned for mF real, such that −1/mF lies
outside the support of H, by
xF (mF ) = −1
mF
+ 1 + c0
cc0
Z
t
1 + tmF
dH(t).
Figure 17.17 depicts the function xF in the system conditions already used in
Figure 17.12, i.e. K = 3, P1 = 1, P2 = 3, P3 = 10, c1 = c2 = c3, c0 = 10, c = 10,

460
17. Estimation
σ2 = 0.1. Figure 17.17 has the peculiar behavior that it does not have asymptotes
as in Figure 17.15 where the population eigenvalue distribution was discrete. As
a consequence, our previous derivations cannot be straightforwardly adapted to
derive the spectrum separability condition. If c0 > 1, note also, although it is
not appearing in the abscissa range of Figure 17.17, that there exist asymptotes
in the position mF = −1/σ2. This is due to the fact that G(0) −G(0−) > 0, and
therefore H(σ2) −H((σ2)−) > 0. We assume c0 > 1 until further notice.
Applying a second time Theorem 7.4, the support of F is complementary to
the set of real non-negative x such that x = xF (mF ) and x′
F (mF ) > 0 for a
certain real mF , with x′
F (mF ) given by:
x′
F (mF ) =
1
m2
F
−1 + c0
cc0
Z
t2
(1 + tmF )2 dH(t).
Reminding that H(t) =
c0
c0+1G(t −σ2) +
1
1+c0 δ(t), this can be rewritten
x′
F (mF ) =
1
m2
F
−1
c
Z
t2
(1 + tmF )2 dG(t −σ2).
(17.25)
It is still true that xF (mF ), restricted to the set of mF where x′
F (mF ) ≥0, is
increasing. As a consequence, it is still true also that each cluster of H can be
mapped to a unique cluster of F. It is then possible to iteratively map the power
Pk onto cluster kG in G, as previously described, and to further map cluster kG in
G (which is also cluster kG in H) onto a unique cluster kF in F (or equivalently
in F).
Therefore, a necessary and suﬃcient condition for the separability of the cluster
associated with Pk in F reads:
Assumption 17.3. There exist two distinct real values m(l)
F ,kG < m(r)
F ,kG such that:
1. x′
F (m(l)
F ,kG) > 0, x′
F (m(r)
F ,kG) > 0
2. there exist m(l)
G,k, m(r)
G,k ∈R such that
xG(m(l)
G,k) = −
1
m(l)
F ,kG
−σ2
xG(m(r)
G,k) = −
1
m(r)
F ,kG
−σ2
that satisfy:
a. x′
G(m(l)
G,k) > 0, x′
G(m(r)
G,k) > 0
b. and
Pk−1 < −
1
m(l)
G,k
< Pk < −
1
m(r)
G,k
< Pk+1
(17.26)
with the convention P0 = 0+, PK+1 = ∞.

17.2. Blind multi-source localization
461
Assumption 17.3 states ﬁrst that cluster kG in G is distinct from clusters (k −
1)G and (k + 1)G (Item 2b), which is equivalent to Assumption 17.1, and second
that m(l)
F ,kG ≜−1/(xG(m(l)
G,kG) + σ2) and m(r)
F ,kG ≜−1/(xG(m(r)
G,kG) + σ2) (which
lie on either side of cluster kG in H) have respective images x(l)
kF ≜xF (m(l)
F ,kG)
and x(r)
kF ≜xF (m(r)
F ,kG) by xF , such that x′
F (m(l)
F ,kG) > 0 and x′
F (m(r)
F ,kG) > 0, i.e.
x(l)
kF and x(r)
kF lie outside the support of F, on either side of cluster kF .
However, Assumption 17.3, be it a necessary and suﬃcient condition for the
separability of cluster kF , is diﬃcult to exploit in practice. Indeed, it is not
satisfactory to require the veriﬁcation of the existence of such m(l)
F ,kG and m(r)
F ,kG.
More importantly, the computation of xF requires to know H, which is only fully
accessible through the non-convenient inverse Stieltjes transform formula
H(x) = 1
π lim
y→0
Z x
−∞
mH(t + iy)dt.
(17.27)
Instead of Assumption 17.3, we derive here a suﬃcient condition for cluster
separability in F, which can be explicitly veriﬁed without resorting to involved
Stieltjes transform inversion formulas. Note from the clustering of G into KG
clusters plus a mass at zero that (17.25) becomes
x′
F (mF ) =
1
m2
F
−1
c
KG
X
r=1
Z x+
G,r
x−
G,r
t2
(1 + tmF )2 dG(t −σ2) −c0 −1
cc0
σ4
(1 + σ2mF )2
(17.28)
where we remind that [x−
G,i, x+
G,i] is the support of cluster i in G, i.e.
x−
G,1, x+
G,1, . . . , x−
G,KG, x+
G,KG are the images by xG of the 2KG real solutions
to x′
G(mG) = 0.
Observe now that the function −t2/(1 + tmF )2, found in the integrals of
(17.28), has derivative along t

−
t2
(1 + tmF )2
′
= −
2t
(1 + tmF )4 (1 + tmF )
and is therefore strictly increasing when mF < −1/t and strictly decreasing
when mF > −1/t. For mF ∈(−1/(x+
G,i + σ2), −1/(x−
G,i+1 + σ2)), we then have
the inequality
x′
F (mF ) ≥
1
m2
F
−1
c
 
i
X
r=1
(x+
G,r + σ2)2
(1 + (x+
G,r + σ2)mF )2
+
KG
X
r=i+1
(x−
G,r + σ2)2
(1 + (x−
G,r + σ2)mF )2 + c0 −1
c0
σ4
(1 + σ2mF )2
!
.
(17.29)
Denote fi(mF ) the right-hand side of (17.29). Through the inequality (17.29),
we then fall back on a ﬁnite sum expression as in the previous study of the
support of G. In that case, we can exhibit a suﬃcient condition to ensure the
separability of cluster kF from the neighboring clusters. Speciﬁcally, we only need

462
17. Estimation
to verify that fkG−1(mF ,kG) > 0, with mF ,kG the single solution to f ′
kG−1(mF ) =
0 in the set (−1/(x+
G,kG−1 + σ2), −1/(x−
G,kG + σ2)), and fkG(mF ,kG+1) > 0,
with mF ,kG+1 the unique solution to f ′
kG(mF ) = 0 in the set (−1/(x+
G,kG +
σ2), −1/(x−
G,kG+1 + σ2)). This is exactly what Assumption 17.2 states.
Remember now that we assumed in this section c0 > 1. If c0 ≤1, then zero is
in the support of H, and therefore the leftmost cluster in F, i.e. that attached to
σ2, is necessarily merged with that of P1. This already discards the possibility of
spectrum separation for P1 and therefore P1 cannot be estimated. It is therefore
not necessary to update Assumption 17.1 for the particular case of P1 when
c0 = 1.
Finally, Assumptions 17.1 and 17.2 ensure that (k −1)F < kF < (k + 1)F ,
kF ̸= 1, and there exists a constructive way to derive the mapping k 7→kF . We
are now in position to determine the contour Ck.
Determination of Ck.
From Assumption 17.2 and Theorem 7.4, there exist x(l)
kF and x(r)
kF outside the
support of F, on either side of cluster kF , such that mF (z) has limits m(l)
F ,kG ≜
m◦
F (x(l)
kF ) and m(r)
F ,kG ≜m◦
F (x(r)
kF ), as z →x(l)
kF and z →x(r)
kF , respectively, with
m◦
F the analytic extension of mF in the points x(l)
kF ∈R and x(r)
kF ∈R. These
limits m(l)
F ,kG and m(r)
F ,kG are on either side of cluster kG in the support of −1/H,
and therefore −1/m(l)
F ,kG −σ2 and −1/m(l)
F ,kG −σ2 are on either side of cluster
kG in the support of G.
Consider any continuously diﬀerentiable complex path ΓF,k with endpoints
x(l)
kF and x(r)
kF , and interior points of positive imaginary part. We deﬁne the
contour CF,k as the union of ΓF,k oriented from x(l)
kF to x(r)
kF and its complex
conjugate Γ∗
F,k oriented backwards from x(r)
kF to x(l)
kF . The contour CF,k is clearly
continuous and piecewise continuously diﬀerentiable. Also, the support of cluster
kF in F is completely inside CF,k, while the supports of the neighboring clusters
are away from CF,k. The support of cluster kG in H is then inside −1/mF (CF,k),2
and therefore the support of cluster kG in G is inside CG,k ≜−1/mF (CF,k) −σ2.
Since mF is continuously diﬀerentiable on C \ R (it is in fact holomorphic
there [Silverstein and Choi, 1995]) and has limits in x(l)
kF and x(r)
kF , CG,k is also
continuous and piecewise continuously diﬀerentiable. Going one more step in
this process, we ﬁnally have that Pk is inside the contour Ck ≜−1/mG(CG,k),
while Pi, for all i ̸= k, is outside Ck. Since mG is also holomorphic on C \ R and
has limits in −1/m◦
F (x(l)
kF ) −σ2 and −1/m◦
F (x(r)
kF ) −σ2, Ck is a continuous and
piecewise continuously diﬀerentiable complex path, which is suﬃcient to perform
complex integration [Rudin, 1986].
2 We slightly abuse notations here and should instead say that the support of cluster kG in H
is inside the contour described by the image by −1/mF of the restriction to C+ and C−of
CF,k, continuously extended to R in the points −1/m(l)
F ,kG and −1/m(r)
F ,kG.

17.2. Blind multi-source localization
463
Figure
17.18
depicts
the
contours
C1, C2, C3
originating
from
circular
integration contours CF,k of diameter [x(l)
kF , x(r)
kF ], k ∈{1, 2, 3}, for the case
of Figure 17.12. The points x(l)
kF and x(r)
kF for kF ∈{1, 2, 3} are taken to be
x(l)
kF = xF (mF ,kG), x(r)
kF = xF (mF ,kG+1), with mF ,i the real root of f ′
i(mF )
in (−1/(x+
G,i−1 + σ2), −1/(x−
G,i + σ2)) when i ∈{1, 2, 3}, and we take the
convention mG,4 = −1/(15 + σ2).
Recall now that Pk was deﬁned as
Pk = ck
1
2πi
I
Ck
K
X
r=1
1
cr
ω
Pr −ω dω.
With the variable change ω = −1/mG(t), this becomes
Pk = ck
2πi
I
CG,k
K
X
r=1
1
cr
−1
1 + PrmG(t)
m′
G(t)
mG(t)2 dt
= ck
2πi
I
CG,k
"
mG(t)
K
X
r=1
1
cr
Pr
1 + PrmG(t) −
K
X
r=1
1
cr
#
m′
G(t)
mG(t)2 dt
= ck
2πi
I
CG,k
 
mG(t)
"
−
1
mG(t) +
K
X
r=1
1
cr
Pr
1 + PrmG(t)
#
+ c0 −1
c0
!
m′
G(t)
mG(t)2 dt.
From Equation (17.7), this simpliﬁes into
Pk = ck
c0
1
2πi
I
CG,k
(c0tmG(t) + c0 −1) m′
G(t)
mG(t)2 dt.
(17.30)
Using (17.10) and proceeding to the change of variable t = −1/mF (z) −σ2,
(17.30) becomes
Pk = ck
2πi
I
CF,k
 1 + σ2mF (z)

"
−
1
zmF (z) −
m′
F (z)
mF (z)2 −
m′
F (z)
mF (z)mF (z)
#
dz.
(17.31)
This whole process of variable changes allows us to describe Pk as a function of
mF (z), the Stieltjes transform of the almost sure limiting spectral distribution
of BN, as N →∞. It then remains to exhibit a relation between Pk and the
e.s.d. of BN for ﬁnite N. This is to what the subsequent section is dedicated.
17.2.5.4 Evaluation of ˆPk
Let us now deﬁne mBN (z) and mBN (z) as the Stieltjes transforms of the
empirical eigenvalue distributions of BN and BN, respectively, i.e.
mBN (z) = 1
N
N
X
i=1
1
λi −z
(17.32)

464
17. Estimation
1
3
10
−6
−4
−2
0
2
4
6
Real part of Ck, k = 1, 2, 3
Imaginary part of Ck
Figure 17.18 (Negatively oriented) integration contours C1, C2 and C3, for c = 10,
c0 = 10, P1 = 1, P2 = 3, P3 = 10.
and
mBN (z) = N
M mBN (z) −M −N
M
1
z .
Instead of going further with (17.31), deﬁne ˆPk, the “empirical counterpart”
of Pk, as
ˆPk = n
nk
1
2πi
I
CF,k
N
n
 1 + σ2mBN (z)

×
"
−
1
zmBN (z) −
m′
BN (z)
mBN (z)2 −
m′
BN (z)
mBN (z)mBN (z)
#
dz. (17.33)
The integrand can then be expanded into several terms, for which residue
calculus can easily be performed. Denote ﬁrst η1, . . . , ηN the N real roots of
mBN (z) = 0 and µ1, . . . , µN the N real roots of mBN (z) = 0. We identify three
sets of possible poles for the aforementioned terms: (i) the set {λ1, . . . , λN} ∩
[x(l)
kF , x(r)
kF ], (ii) the set {η1, . . . , ηN} ∩[x(l)
kF , x(r)
kF ], and (iii) the set {µ1, . . . , µN} ∩
[x(l)
kF , x(r)
kF ]. For M ̸= N, the full calculus leads to
ˆPk =
NM
nk(M −N)


X
1≤i≤N
x(l)
kF ≤ηi≤x(r)
kF
ηi −
X
1≤i≤N
x(l)
kF ≤µi≤x(r)
kF
µi



17.2. Blind multi-source localization
465
+ N
nk


X
1≤i≤N
x(l)
kF ≤ηi≤x(r)
kF
σ2 −
X
1≤i≤N
x(l)
kF ≤λi≤x(r)
kF
σ2 +
X
1≤i≤N
x(l)
kF ≤µi≤x(r)
kF
σ2 −
X
1≤i≤N
x(l)
kF ≤λi≤x(r)
kF
σ2

.
(17.34)
We know from Theorem 17.5 that mBN (z)
a.s.
−→mF (z) and mBN (z)
a.s.
−→mF (z)
as N →∞. Observing that the integrand in (17.33) is uniformly bounded on
the compact CF,k, the dominated convergence theorem, Theorem 6.3, ensures
ˆPk
a.s.
−→Pk.
To go further, we now need to determine which of λ1, . . . , λN, η1, . . . , ηN and
µ1, . . . , µN lie inside CF,k. This requires a result of eigenvalue exact separation
that extends Theorem 7.1 [Bai and Silverstein, 1998] and Theorem 7.2 [Bai and
Silverstein, 1999], as follows.
Theorem
17.7. Let Bn = 1
nT
1
2nXnXH
nT
1
2n ∈Cp×p, where we assume the
following conditions:
1. Xn ∈Cp×n has entries xij, 1 ≤i ≤p, 1 ≤j ≤n, extracted from a doubly
inﬁnite array {xij} of independent variables, with zero mean and unit
variance.
2. There exist K and a random variable X with ﬁnite fourth order moment such
that, for any x > 0
1
n1n2
X
i≤n1,j≤n2
P(|xij| > x) ≤KP(|X| > x)
(17.35)
for any n1, n2.
3. There is a positive function ψ(x) ↑∞as x →∞, and M > 0, such that
max
ij
E[|x2
ij|ψ(|xij|)] ≤M.
(17.36)
4. p = p(n) with cn = p/n →c > 0 as n →∞.
5. For each n, Tn ∈Cp×p is Hermitian non-negative deﬁnite, independent of
{xij}, satisfying Hn ≜F Tn ⇒H, H a non-random probability distribution
function, almost surely. T
1
2n is any Hermitian square root of Tn.
6. The spectral norm ∥Tn∥of Tn is uniformly bounded in n almost surely.
7. Let a, b > 0, non-random, be such that, with probability one, [a, b] lies in an
open interval outside the support of F cn,Hn for all large n, with F y,G deﬁned
to be the almost sure l.s.d. of 1
nXH
nTnXn when H = G and c = y.
Denote λY
1 ≥. . . ≥λY
p the ordered eigenvalues of the Hermitian matrix Y ∈
Cp×p. Then, we have that:
1. P(no eigenvalues of Bn appear in [a, b] for all large n) = 1.
2. If c(1 −H(0)) > 1, then x0, the smallest value in the support of F c,H, is
positive, and with probability one, λBn
n
→x0 as n →∞.

466
17. Estimation
3. If c(1 −H(0)) ≤1, or c(1 −H(0)) > 1 but [a, b] is not contained in [0, x0],
then mF c,H(a) < mF c,H(b) < 0. Almost surely, there exists, for all n large, an
index in ≥0 such that λTn
in > −1/mF c,H(b) and λTn
in+1 > −1/mF c,H(a) and we
have:
P(λBn
in > b and λBn
in+1 < a for all large n) = 1.
Theorem 17.7 is proved in [Couillet et al., 2011c]. This result is more general
than Theorem 7.2, but the assumptions are so involved that we preferred to state
Theorem 7.2 in Chapter 7 in its original form with i.i.d. entries in matrix Xn.
To apply Theorem 17.7 to BN in our scenario, we need to ensure all
assumptions are met. Only Items 2–6 need particular attention. In our scenario,
the matrix Xn of Theorem 17.7 is ( X
W ), while Tn is T ≜
  HPHH+σ2IN 0
0
0

. The
latter has been proved to have almost sure l.s.d. H, so that Item 5 is veriﬁed.
Also, from Theorem 7.1 upon which Theorem 17.7 is based, there exists a subset
of probability one in the probability space that engenders the T over which, for n
large enough, T has no eigenvalues in any closed set strictly outside the support
of H; this ensures Item 6. Now, by construction, X and W have independent
entries of zero mean, unit variance, fourth order moment and are composed
of at most K + 1 distinct distributions, irrespective of M. Denote X1, . . . , Xd,
d ≤K + 1, d random variables distributed as those distinct distributions. Letting
X = |X1| + . . . + |Xd|, we have that
1
n1n2
X
i≤n1,j≤n2
P(|zij| > x) ≤P
 d
X
i=1
|Xi| > x
!
= P(|X| > x)
where zij is the (i, j)th entry of ( X
W ). Since all Xi have ﬁnite order four moments,
so does X, and Item 2 is veriﬁed. From the same argument, Item 3 follows with
φ(x) = x2. Theorem 17.7 can then be applied to BN.
The corollary of Theorem 17.7 applied to BN is that, with probability
one, for N suﬃciently large, there will be no eigenvalue of BN (or BN)
outside the support of F, and the number of eigenvalues inside cluster kF
is exactly nk. Since CF,k encloses cluster kF and is away from the other
clusters, {λ1, . . . , λN} ∩[x(l)
kF , x(r)
kF ] = {λi, i ∈Nk} almost surely, for all large N.
Also, for any i ∈{1, . . . , N}, it is easy to see from (17.32) that mBN (z) →∞
when z ↑λi and mBN (z) →−∞when z ↓λi. Therefore mBN (z) has at least
one root in each interval (λi−1, λi), with λ0 = 0, hence µ1 < λ1 < µ2 < . . . <
µN < λN. This implies that, if k0 is the index such that CF,k contains exactly
λk0, . . . , λk0+(nk−1), then CF,k also contains {µk0+1, . . . , µk0+(nk−1)}. The same
result holds for ηk0+1, . . . , ηk0+(nk−1). When the indexes exist, due to cluster
separability, ηk0−1 and µk0−1 belong, for N large, to cluster kF −1. We are then
left with determining whether µk0 and ηk0 are asymptotically found inside CF,k.

17.2. Blind multi-source localization
467
For this, we use the same approach as in Chapter 8 by noticing that, since
zero is not included in Ck, we have:
1
2πi
I
Ck
1
ω dω = 0.
Performing the same changes of variables as previously, we have:
I
CF,k
−mF (z)mF (z) −zm′
F (z)mF (z) −zmF (z)m′
F (z)
z2mF (z)2mF (z)2
dz = 0.
(17.37)
For N large, the dominated convergence theorem, Theorem 6.3, ensures again
that the left-hand side of the (17.37) is close to
I
CF,k
−mBN (z)mBN (z) −zm′
BN (z)mBN (z) −zmBN (z)m′
BN (z)
z2mBN (z)2mBN (z)2
dz.
(17.38)
The residue calculus of (17.38) then leads to
X
1≤i≤N
λi∈[x(l)
kF ,x(r)
kF ]
2 −
X
1≤i≤N
ηi∈[x(l)
kF ,x(r)
kF ]
1 −
X
1≤i≤N
µi∈[x(l)
kF ,x(r)
kF ]
1
a.s.
−→0.
(17.39)
Since the cardinalities of {i, ηi ∈[x(l)
kF , x(r)
kF ]} and {i, µi ∈[x(l)
kF , x(r)
kF ]} are at
most nk, (17.39) is satisﬁed only if both cardinalities equal nk in the limit. As a
consequence, µk0 ∈[x(l)
kF , x(r)
kF ] and ηk0 ∈[x(l)
kF , x(r)
kF ] for all N large, almost surely.
For N large, N ̸= M, this allows us to simplify (17.34) into
ˆPk =
NM
nk(M −N)
X
1≤i≤N
λi∈Nk
(ηi −µi)
(17.40)
with probability one. The same reasoning holds for M = N. This is our ﬁnal
relation. It now remains to show that the ηi and the µi are the eigenvalues of
diag(λ) −1
N
√
λ
√
λ
T and diag(λ) −
1
M
√
λ
√
λ
T, respectively. But this is merely
an application of Lemma 8.1.
This concludes the elaborate proof of Theorem 17.6. We now turn to the proper
evaluation of this last power inference method, for the two system models studied
so far. The ﬁrst system model, Scenario (a), has the following characteristics:
K = 3 sources, P1 = 1, P2 = 3, and P3 = 10, N = 60 sensors, M = 600 samples,
and n1 = n2 = n3 = 2 antennas per transmit source, while for the second system
model, Scenario (b): K = 3 sources, P1 = 1/16, P2 = 1/4, N = 24 sensors, M =
128 samples, and n1 = n2 = n3 = 4 antennas per transmit source. The histogram
and distribution function of the estimated powers for Scenario (b) are depicted
in Figure 17.19 and Figure 17.20. Observe that this last estimator seems rather
unbiased and very precise for all three powers under study.
In all previous approaches to the problem of power inference, we have assumed
to this point that the number of simultaneous transmissions is known and that
the number of antennas used by every transmitter is known. In the moment
deconvolution approach, this has to be assumed either when inverting the

468
17. Estimation
1
16
1
4
1
0
5
10
15
Estimated ˆPk
Density
Figure 17.19 Histogram of the ˆPk for k ∈{1, 2, 3}, P1 = 1/16, P2 = 1/4, P3 = 1,
n1 = n2 = n3 = 4 antennas per user, N = 24 sensors, M = 128 samples, and
SNR = 20 dB.
1
16
1
4
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
ˆP ∞
k
Distribution function
ˆP ∞
k
ˆPk
Figure 17.20 Distribution function of the estimator ˆPk for k ∈{1, 2, 3}, P1 = 1/16,
P2 = 1/4, P3 = 1, n1 = n2 = n3 = 4 antennas per user, N = 24 sensors, M = 128
samples, and SNR = 20 dB. Optimum estimator shown in dashed lines.
Newton–Girard formulas or when ﬁnding the maximum likelihood or minimum
mean square error estimator for the transmit powers. As for the Stieltjes
transform approach, this is required to determine which eigenvalues actually form
a cluster. The same remark holds for the nest N- and M-consistent approach. It is

17.2. Blind multi-source localization
469
therefore of prior importance to be ﬁrst able to detect the number of simultaneous
transmissions and the number of antennas per user. In the following, we will see
that this is possible using ad-hoc tricks, although in most practical cases, more
theoretical methods are required that are yet to be investigated.
17.2.6
Joint estimation of number of users, antennas and powers
It is obvious that the less is a priori known to the estimator, the less reliable
estimation of the system parameters is possible. We discuss hereafter the
problems linked to the absence of knowledge of some system parameters as well
as what this entails from a cognitive radio point of view. Some further comments
on the way to use the above estimators are also made.
• If both the number of transmit sources and the number of antennas per source
are known prior to signal sensing, then all aforementioned methods will give
more or less accurate estimates of the transmit powers. The accuracy depends
in that case on whether transmit sources are suﬃciently distinct from one
another (depending on the cluster separability condition for Theorem 17.6)
and on the eﬃciency of the algorithm used. From a cognitive radio viewpoint,
that would mean that the secondary network is aware of the number of users
exploiting a resource and of the number of antennas per user. It is in fact not
necessary to know exactly how many users are currently transmitting, but only
the maximum number of such users, as the sensing array would then always
detect the maximum amount of users, some transmitting with null power. The
assumption that the cognitive radio is aware of this maximal number of users
per resource is therefore tenable. The assumption that the number of transmit
antennas is known also makes sense if the primary communication protocols is
known not to allow multiple antenna transmissions for instance. Note however
that the overall performance in that case is rather degraded by the fact that
single antenna transmissions do not provide much channel diversity. If this is
so, it is reasonable for the sensing array to acquire more samples for diﬀerent
realizations of channel H, which would take more time, or to be composed of
numerous sensors, which might not be a realistic assumption.
• If the number of users is unknown, as discussed in the previous point, this
might not be a dramatic issue on practical grounds if we can at least assume
a maximal number of simultaneous transmissions. Typically, though, in a
wideband CDMA network, a large number of users may simultaneously occupy
a given frequency resource. If a cognitive radio is to operate on this frequency
resource, it must then cope with the fact that a very large number of user
transmit powers may need be estimated. Nonetheless, and rather fortunately,
it is fairly untypical that all transmit users are found at the same location,
close to the secondary network. The most remote users would in that case be
hidden by thermal noise and the secondary network would then only need to
deal with the closest users. Anyhow, if ever a large number of users is to be

470
17. Estimation
found in the neighborhood of a cognitive radio, it is very unlikely that the
frequency resource be reusable at all.
• If now the number of antennas per user is unknown, then more elaborate
methods are demanded since this parameter is essential to all algorithms.
Indeed, for both classical and Stieltjes transform approaches, we need to be
able to distribute the empirical eigenvalues of
1
M YYH in several clusters, one
for each source, the size of each cluster matching the number of antennas used
by the transmitter. The same holds true for the exact inference method or
the moment approach that both assume known power multiplicities. Among
the methods to cope with this issue, we present below an ad-hoc suboptimal
approach. We ﬁrst assume for readability that we know the number K of
transmit sources (taken large enough to cover all possible hypotheses), some
having possibly a null number of transmit antenna. The approach consists in
the following steps:
1. we ﬁrst identify a set of plausible hypotheses for n1, . . . , nK. This can be
performed by inferring clusters based on the spacing between consecutive
eigenvalues: if the distance between neighboring eigenvalues is more than a
threshold, then we add an entry for a possible cluster separation in the list
of all possible positions of cluster separation. From this list, we create all
possible K-dimensional vectors of eigenvalue clusters. Obviously, the choice
of the threshold is critical to reduce the number of hypotheses to be tested;
2. for each K-dimensional vector with assumed numbers of antennas
ˆn1, . . . , ˆnK, we use Theorem 17.6 in order to obtain estimates of the
ˆP1, . . . , ˆPK (some being possibly null);
3. based on these estimates, we compare the e.s.d. F BN of BN to ˆF deﬁned
as the l.s.d. of the matrix model ˆY = HˆPX + W with ˆP the diagonal
matrix composed of ˆn1 entries equal to ˆP1, ˆn2 entries equal to ˆP2, etc.
up to ˆnK entries equal to ˆPK. The l.s.d. ˆF is obtained from Theorem
17.5. The comparison can be performed based on diﬀerent metrics. In the
simulations carried hereafter, we consider as a metric the mean absolute
diﬀerence between the Stieltjes transform of F BN and of ˆF on the segment
[−1, −0.1].
A more elaborate approach would consist in analyzing the second order
statistics of F BN , and therefore determining decision rules, such as hypothesis
tests for every possible set (K, n1, . . . , nK).
Note that, when the number of antennas per user is unknown to the receiver
and clusters can be clearly identiﬁed, another problem still occurs. Indeed, even
if the clusters are perfectly disjoint, to this point in our study, the receiver has
no choice but to assume that the cluster separability condition is always met and
therefore that exactly as many users as visible clusters are indeed transmitting.
If the condition is in fact not met, say the empirical eigenvalues corresponding to
the p power values Pi, . . . , Pi+(p−1) are merged into a single cluster, i.e. with the

17.2. Blind multi-source localization
471
notations of Section 17.2.5 iF = . . . = (i + p −1)F , then applying the methods
described above leads to an estimator of their mean P0 =
1
n0
Pp−1
k=0 ni+kPi+k with
n0 = ni + . . . + ni+(p−1) (since the integration contour encloses all power values
or the link between moments and P1, . . . , PK takes into account the assumed
eigenvalue multiplicities), instead of an estimator of their individual values.
In this case, the receiver can therefore only declare that a given estimate ˆP0
corresponds either to a single transmit source with dimension n0 or to multiple
transmit sources of cumulated dimension n0 with average transmit power P0,
well approximated by ˆP0. For practical blind detection purposes in cognitive
radios, this leads the secondary network to infer a number of transmit entities
that is less than the eﬀective number of transmitters. In general, this would not
have serious consequences on the decisions made by the secondary network but
this might at least reduce the capabilities of the secondary network to optimally
overlay the licensed spectrum. To go past this limitation, current investigations
are performed to allow multiple eigenvalue estimations within a given cluster of
eigenvalues. This can be performed by studying the second order statistics of the
estimated powers.
17.2.7
Performance analysis
17.2.7.1 Method comparison
We ﬁrst compare the conventional method against the Stieltjes transform
approach for Scenario (a). Under the hypotheses of this scenario, the ratios
c and c0 equal 10, leading therefore the conventional detector to be close to
unbiased. We therefore suspect that the normalized mean square error (NMSE)
performance in the estimation of the powers for both detectors is alike. This
is described in Figure 17.21, which suggests as predicted that in the high
SNR regime (when cluster separability is reached) the conventional estimator
performs similar to the Stieltjes transform method. However, it appears that a
3 dB gain is achieved by the Stieltjes transform method around the position
where cluster separability is no longer satisﬁed. This is due to the fact that,
when subsequent clusters tend to merge as σ2 increases, the Stieltjes transform
method manages to track the position of the powers Pk while the conventional
method keeps assuming each Pk is located at the center of gravity of cluster
kF . This observation is very similar to that made in [Mestre and Lagunas, 2008]
where the improved G-MUSIC estimator pushes further the SNR limit where
the performance of the classical MUSIC estimator starts to decay signiﬁcantly.
We now consider Scenario (b). We ﬁrst compare the performance of the
conventional, Stieltjes transform and moment estimators for a SNR of 20 dB.
In order to compare techniques with similar computational complexity, we use
Newton–Girard inversion formulas to retrieve the powers from the estimated
moments. Figure 17.22 depicts the distribution function of the estimated powers
in logarithmic scale. The Stieltjes transform method appears here to be very
precise and seemingly unbiased. In contrast, the conventional method, with a

472
17. Estimation
−15
−10
−5
0
5
10
15
20
−20
−10
0
10
SNR [dB]
Normalized mean square error [dB]
ˆ
P1
ˆ
P2
ˆ
P3
ˆ
P ∞
1
ˆ
P ∞
2
ˆ
P ∞
3
Figure 17.21 Normalized mean square error of individual powers ˆP1, ˆP2, ˆP3,
P1 = 1, P2 = 3, P3 = 10, n1/n = n2/n = n3/n = 1/3 ,n/N = N/M = 1/10, n = 6.
Comparison between the conventional and Stieltjes transform approaches.
slightly smaller variance shows a large bias as was anticipated. As for the moment
method, it shows rather accurate performance for the stronger estimated power,
but proves very inaccurate for smaller powers. This follows from the inherent
shortcomings of the moment method. The performance of the estimator ˆP ′
k will
be commented on later.
We then focus on the estimate of the larger power P3 and take now the SNR
to range from −15 to 30 dB under the same conditions as previously and for the
same estimators. The NMSE for the estimators of P3 is depicted in Figure 17.23.
The curve marked with squares will be commented on in the next section. As
already observed in Figure 17.22, in the high SNR regime, the Stieltjes transform
estimator outperforms both alternative methods. We also notice the SNR gain
achieved by the Stieltjes transform approach with respect to the conventional
method in the low SNR regime, as already observed in Figure 17.21. However,
it now turns out that in this low SNR regime, the moment method is gaining
ground and outperforms both cluster-based methods. This is due to the cluster
separability condition, which is not a requirement for the moment approach.
This indicates that much can be gained by the Stieltjes transform method in the
low SNR regime if a more precise treatment of overlapping clusters is taken into
account.
17.2.7.2 Joint estimation of K, nk, Pk
So far, we have assumed that the number of users K and the numbers of antennas
per user n1, . . . , nK were perfectly known. As discussed previously, this may
not be a strong assumption if it is known in advance how many antennas are

17.2. Blind multi-source localization
473
1
16
1
4
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Powers
Distribution function
ˆPk
ˆP ∞
k
ˆP (mom)
k
ˆP ′
k
Perfect estimate
Figure 17.22 Distribution function of the estimators ˆP ∞
k , ˆPk, ˆP ′
k and ˆP (mom)
k
for
k ∈{1, 2, 3}, P1 = 1/16, P2 = 1/4, P3 = 1, n1 = n2 = n3 = 4 antennas per user,
N = 24 sensors, M = 128 samples and SNR = 20 dB. Optimum estimator shown in
dotted line.
systematically used by every source or if another mechanism, such as in [Chung
et al., 2007], can provide this information. Nonetheless, these are in general
strong assumptions. Based on the ad-hoc method described above, we therefore
provide the performance of our novel Stieltjes transform method in the high
SNR regime when only n is known; this assumption is less stringent since in
the medium to high SNR regime we can easily decide which eigenvalues of
BN belong to the cluster associated with σ2 and which eigenvalues do not.
We denote ˆP ′
k the estimator of Pk when K and n1, . . . , nK are unknown. We
assume for this estimator that all possible combinations of 1 to 3 clusters can
be generated from the n = 6 observed eigenvalues in Scenario (a) and that all
possible combinations of 1 to 3 clusters with even cluster size can be generated
from the n = 12 eigenvalues of BN in Scenario (b). For Scenario (a), the NMSE
performance of the estimators ˆPk and ˆP ′
k is proposed in Figure 17.24 for the
SNR ranging from 5 dB to 30 dB. For Scenario (b), the distribution function
of the inferred ˆP ′
k is depicted in Figure 17.22, while the NMSE performance for
the inference of P3 is proposed in Figure 17.23; these are both compared against
the conventional, moment, and Stieltjes transform estimators. We also indicate
in Table 17.1 the percentage of correct estimations of the triplet (n1, n2, n3)
for both Scenario (a) and Scenario (b). In Scenario (a), this amounts to 12
such triplets that satisfy nk ≥0, n1 + n2 + n3 = 6, while, in Scenario (b), this
corresponds to 16 triplets that satisfy nk ∈2N, n1 + n2 + n3 = 12. Observe that
the noise variance, assumed to be known a priori in this case, plays an important
role with respect to the statistical inference of the nk. In Scenario (a), for a

474
17. Estimation
−5
0
5
10
15
20
25
30
−20
−15
−10
−5
0
SNR [dB]
Normalized mean square error [dB]
ˆ
P3
ˆ
P ∞
3
ˆ
P (mom)
3
ˆ
P ′
3
Figure 17.23 Normalized mean square error of largest estimated power P3,
P1 = 1/16, P2 = 1/4, P3 = 1, n1 = n2 = n3 = 4 ,N = 24, M = 128. Comparison
between conventional, moment, and Stieltjes transform approaches.
SNR
RCI (a)
RCI (b)
5 dB
0.8473
0.1339
10 dB
0.9026
0.4798
15 dB
0.9872
0.4819
20 dB
0.9910
0.5122
25 dB
0.9892
0.5455
30 dB
0.9923
0.5490
Table 17.1. Rate of correct inference (RCI) of the triplet (n1, n2, n3) for scenarios (a) and
(b).
SNR greater than 15 dB, the correct hypothesis for the nk is almost always
taken and the performance of the estimator is similar to that of the optimal
estimator. In Scenario (b), the detection of the exact cluster separation is less
accurate and the performance for the inference of P3 saturates at high SNR to
−16 dB of NMSE, against −19 dB when the exact cluster separation is known.
It therefore seems that, in the high SNR regime, the performance of the Stieltjes
transform detector is loosely aﬀected by the absence of knowledge about the
cluster separation. This statement is also conﬁrmed by the distribution function
of ˆP ′
k in Figure 17.22, which still outperforms the conventional and moment
methods. We underline again here that this is merely the result of an ad-hoc
approach; this performance could be greatly improved if, e.g. more was known
about the second order statistics of F BN .

17.2. Blind multi-source localization
475
5
10
15
20
25
30
−22
−20
−18
−16
−14
−12
−10
SNR [dB]
Normalized mean square error [dB]
ˆ
P1
ˆ
P2
ˆ
P3
ˆ
P ′
1
ˆ
P ′
2
ˆ
P ′
3
Figure 17.24 Normalized mean square error of individual powers ˆP1, ˆP2, ˆP3 and ˆP ′
1,
ˆP ′
2, ˆP ′
3, P1 = 1, P2 = 3, P3 = 10, n1/n = n2/n = n3/n = 1/3 ,n/N = N/M = 1/10,
n = 6, 10 000 simulation runs.
This concludes the present chapter on eigen-inference methods using large
dimensional random matrices. Note that, to obtain the above estimators, a strong
mathematical eﬀort was put into the macroscopic analysis of the asymptotic
spectra for rather involved random matrix models as well as in the microscopic
analysis of the behavior of individual eigenvalues and eigenvectors. We believe
that much more emphasis will be cast in the near future on G-estimation for
other signal processing and wireless communication issues. The main limitation
today to further develop multi-dimensional consistent estimators is that only
few models have been carefully studied. In particular, we mentioned repeatedly
in Part I the sample covariance matrix model and the spiked model, for which
we have a large number of results concerning exact spectrum separation, limit
distributions of the extreme eigenvalues in the Gaussian case, etc. When it comes
to slightly more elaborate models, such as the information plus noise model,
even the result on exact separation is yet unproved in the general i.i.d. case
(with obviously some moment assumptions). Only the exact separation result
for the information plus noise model with uncorrelated Gaussian noise is known.
Nothing is yet known about limit distribution of the extreme eigenvalues. There
is therefore a wide range of new results to come along with the deeper study of
such random matrix models.
In the next chapter, we will deal with a more speciﬁc application of random
matrices to the problem of channel modeling. This problem enters a much wider
framework of statistical inference based on Bayesian probability theory and the
maximum entropy principle [Jaynes, 1957a,b]. The introduction of the Neyman–

476
17. Estimation
Pearson test developed in Chapter 16 unfolds in particular from this Bayesian
framework.

18
System modeling
Channel modeling is a fundamental ﬁeld of research, as all communication
models that were used so far in this book as well as in the whole mobile
communication literature are based on such models. These models are by
deﬁnition meant to provide an adequate representation of real practical
communication environments. As such, i.i.d. Gaussian channel models are meant
to represent the most scattered environment possible where multiple waveforms
reﬂecting on a large number of scattering objects add up non-coherently and
independently on the receive antennas. From this point of view, the Gaussian
assumption is due to a loose application of the law of large numbers. Due to
the mobility of the communication devices in the propagation environment, the
statistical disposition of scatterers is rather uniform, hence the i.i.d. property.
This is basically the physical arguments for using Gaussian i.i.d. models,
along with conﬁrmation by ﬁeld measurements. An alternative explanation for
Gaussian channel models will be provided in this chapter, which accounts for
a priori information available at the system modeler, rather than for physical
interpretations.
However, the complexity of propagation environments call for more involved
models. This is why the Kronecker model comes into play to adequately model
correlations arising at either communication end, supposedly independent from
one another. This is also why the Rician model is of common use, as it can take
into account possible line-of-sight components in the channel, which can often
be assumed of constant fading value for a rather long time period, compared
to the fast varying scattering part of the channel. Those are channels usually
considered in the scientiﬁc literature for their overall simplicity, but whose
accuracy is somewhat disputed [Ozcelik et al., 2003]. Nonetheless, the literature
is also full of alternative models, mostly based on ﬁeld tests. After ﬁfty years
of modern telecommunications, it is still unclear which models to value, which
models are relevant and for what reasons. This chapter intends ﬁrst to propose a
joint information-theoretic framework for telecommunications that encompasses
many topics such as channel modeling, source sensing, parameter estimation,
etc. from a common probability-theoretic basis. This enables channel modeling
to be seen, no longer as a remote empirical ﬁeld of research, but rather as a
component of a larger information-theoretic framework. More details about the
current literature on channel modeling can be found in [Almers et al., 2007]. The

478
18. System modeling
role of random matrix theory in this ﬁeld will become clear when we address the
question of ﬁnding the maximum entropy distribution of random matrix models.
Before we proceed with the channel modeling problem, explored through
several articles by Debbah, M¨uller, and Guillaud mainly [de Lacerda Neto et al.,
2006; Debbah and M¨uller, 2005; Guillaud et al., 2006], we need to quickly recall
the theoretical foundation of Bayesian probability theory and the maximum
entropy principle.
18.1
Introduction to Bayesian channel modeling
The ultimate objective of the ﬁeld of channel modeling is to provide us
with probabilistic models that are consistent with the randomness observed in
actual communication channels. It therefore apparently makes sense to probe
realistic channels in order to infer a general model by “sampling” the observed
randomness. It also makes sense to come up with simple mathematical models
that (i) select only the essential features of the channels, (ii) build whatever
simple probability distribution around them, and (iii) can be compared to
actual channel statistics to see if the unknown random part of the eﬀective
channel is well approximated by the model. In our opinion, though, these widely
spread approaches suﬀer from a severe misconception of the word randomness.
From a frequentist probability point of view, randomness reﬂects the unseizable
character that makes particular events diﬀerent every time they are observed. For
instance, we may think of a coin toss as being a physical action ruled by chance, as
if it were obeying no law of nature. This is however conceivably absurd when we
consider the same toss coin played in slow motion in such a way that the observer
can actually compute from the observed initial rotation speed, height, and other
parameters, such as air friction, the exact outcome of the experiment in advance.
Playing a toss coin in slow motion removes its random part. A more striking
example, that does not require to modify the conditions of the experiment, is
that of consecutive withdrawals of balls concealed in identical boxes with same
initial content. Assume that we know prior to the ﬁrst withdrawal from the
ﬁrst box that some balls are red, the others being blue. From the frequentist
conception of randomness, the probability of drawing a red ball at ﬁrst, second,
or third withdrawal (the nth withdrawal is done from the nth box on the line) is
identical since the boxes are all identical in content and the successive physical
events are the same. But this is clearly untrue. Without any need for advanced
mathematics, if we are told that the ﬁrst box contains red and blue balls (but not
the exact number), we must infer in total honesty that the probability of getting
a blue ball is somewhere around one half. However, if after a hundred successive
withdrawals, all selected balls turned out to be red, we would reasonably think
that the probability for the hundred-ﬁrst ball withdrawn to be blue is much lower
than ﬁrst anticipated.

18.1. Introduction to Bayesian channel modeling
479
From the above examples, it is more sensitive to see randomness, not as the
result of unruled chance, but rather as the result of a lack of knowledge. The
more we know about an event, the more conﬁdence we have about the outcome
of this event. This is the basis of Bayesian probability theory. The conﬁdence
factor here is what we wish to call the probability of the event. This conception
of probabilities is completely anchored in our everyday life where our decisions
are based on our knowledge and appreciation of the probability of possible events.
In the same way, communication channels, be they often called the environment,
as if we did not have any control over them, can be reduced to the knowledge
we have about them. If we know the communication channels at all times, i.e.
channels for which we often coin the phrase “perfect channel state information,”
then the channel is no longer conceived as random. If we do not have perfect
channel state information at all times, though, then the channel is random in
the sense that it is one realization of all possible channels consistent with the
reduced knowledge we have on this channel.
Modeling channels under imperfect state information therefore consists in
providing a probability distribution for all such channels consistent with this
restricted information. From a conﬁdence point of view, this means we must
assign to each potential channel realization a degree of conﬁdence in such a
realization. This degree of conﬁdence must be computed by taking into account
only the information available, and by discarding as much as possible all
supplementary unwanted hypotheses. It has been proved, successively by Cox
[Cox, 1946], Shannon [Shannon, 1948], Jaynes [Jaynes, 1957a,b], and Shore and
Johnson [Shore and Johnson, 1980] that, under a reasonable axiomatic deﬁnition
of the information-theoretic key notion of ignorance (preferably referred to as
information content by Shannon), the most non-committal way to attribute
degrees of conﬁdence of possible realizations of an event is to assign to it
the probability distribution that has maximal entropy, among all probability
distributions consistent with the prior knowledge. That is, the process of
assigning degrees of conﬁdence to a parameter x, given some information I,
consists in the following recipe:
• Among all probability distributions for x, discard those that are inconsistent
with I. For instance, if I contains information about the statistical mean of x,
then all probability distributions that have diﬀerent means must be discarded;
• among the remaining set SI of such probability distributions, select the one
which maximizes the entropy, i.e. calling p⋆this probability distribution, we
have:
p⋆≜arg max
p∈SI
Z
p(x) log(p(x))dx.
This principle is referred to as the maximum entropy principle [Jaynes, 2003],
which is widely spread in the signal processing community [Bretthorst, 1987], in
econometrics [Zellner, 1971], engineering [Kapur, 1989], and in general science
[Brillouin, 1962] in spite of several century-old philosophical divisions inside the

480
18. System modeling
community. Since such philosophical debates are nowhere near the target of this
section, let alone the subject of this book, we claim from now on and without
further justiﬁcation that the maximum entropy principle is the most reliable tool
to build statistical models for parameters regarding which limited information is
available.
In the next section, we address the channel modeling question through a
Bayesian and maximum entropy point of view.
18.2
Channel modeling under environmental uncertainty
Fast varying communication channels are systems for which a full parametrical
description is lacking to the observer. Since these channels are changing too fast
in time to be adequately tracked by communication devices without incurring
too much information feedback, only limited inexpensive information is usually
collected. For instance, the most trivial quantities that we can collect without
eﬀort on the successive channels is their empirical mean and their empirical
variance (or covariance matrix in the case of multiple antenna communications).
Assuming channel stationarity (at least in the wide sense), it is then possible
to propose a consistent non-committal model for the channel at hand, by
following the steps of the maximum entropy principle. In this particular case
where mean and variance are known, it can in fact be shown that the channel
model under consideration is exactly the Gaussian matrix channel. Some classical
channel models such as the correlated Gaussian model and the Kronecker model
were recovered in [Debbah and M¨uller, 2005] and [Guillaud et al., 2006], while
some new channel maximum entropy-consistent models were also derived. In
the following, we detail the methodology used to come up with these models
when statistical knowledge is available to the system modeler, similar to the
ideas developed in [Franceschetti et al., 2003]. In [Debbah and M¨uller, 2005],
the maximum entropy principle is used also when deterministic knowledge is
available at the system modeler. Both problems lead to mathematically diﬀerent
approaches, problems of the former type being rather easy to treat, while
problems of latter type are usually not tractable. We only deal in this chapter
with problems when statistical information about the channel is available.
Let us consider the multiple antenna wireless channel with nt transmit and
nr receive antennas. We assume narrowband transmission so that the MIMO
communication channels to be modeled are non-frequency selective. Let the
complex scalar coeﬃcient hi,j denote the channel attenuation between the
transmit antenna j and the receive antenna i, j ∈{1, . . . , nt}, i ∈{1, . . . , nr}.
Let H(t) denote the nr × nt channel matrix at time instant t. We recall the
general model for a time-varying ﬂat-fading channel with additive noise
y(t) = H(t)x(t) + w(t)
(18.1)

18.2. Channel modeling under environmental uncertainty
481
where the noise vector w(t) ∈Cnr at time t is modeled as a complex circularly
symmetric Gaussian random variable with i.i.d. coeﬃcients (in compliance with
maximum entropy requirements) and x(t) ∈Cnt denotes the transmit data vector
at time t. In the following, we focus on the derivation of the fading characteristics
of H(t). When we are not concerned with the time-related properties of H(t), we
will drop the time index t, and refer to the channel realization H or equivalently
to its vectorized notation h ≜vec(H) = (h1,1, . . . , hnr,1, h1,2, . . . , hnr,nt)T. Let
us also denote N ≜nrnt and map the antenna indices into {1 . . . , N}; that is,
h = (h1, . . . , hN)T.
18.2.1
Channel energy constraints
18.2.1.1 Average channel energy constraint
In this section, we recall the results of [Debbah and M¨uller, 2005] where an
entropy-maximizing probability distribution is derived for the case where the
average energy carried through a MIMO channel is known deterministically.
This probability distribution is obtained by maximizing the entropy
Z
CN −log(PH(H))PH(H)dH
under the only assumption that the channel has a ﬁnite average energy NE0
and the normalization constraint associated with the deﬁnition of a probability
density, i.e.
Z
CN ∥H∥2
F PH(H)dH = NE0
(18.2)
with ∥H∥F the matrix Frobenius norm and
Z
CN PH(H)dH = 1.
This is achieved through the method of Lagrange multipliers, by writing
L(PH) = −
Z
CN log(PH(H))PH(H)dH + β

1 −
Z
CN PH(H)dH

+ γ

NE0 −
Z
CN ||H||2
F PH(H)dH

where we introduce the scalar Lagrange coeﬃcients β and γ, and by taking the
functional derivative [Fomin and Gelfand, 2000] with respect to PH equal to zero
δL(PH)
δPH
= 0.
This functional derivative takes the form of an integral over H, which is in
particular identically null if the integrand is null for all H. We pick this one
solution and therefore write
−log(PH(H)) −1 −β −γ ∥H∥2
F = 0

482
18. System modeling
for all H.
The latter equation yields
PH(H) = exp

−(β + 1) −γ ∥H∥2
F

and the normalization of this distribution according to (18.2) ﬁnally allows us
to compute the coeﬃcients β and γ. Observing in particular that β = −1 and
γ =
1
E0 are consistent with the initial constraints, the ﬁnal distribution is given
by:
PH|E0(H) =
1
(πE0)N exp
 
−
N
X
i=1
|hi|2
E0
!
.
(18.3)
Interestingly, the distribution deﬁned by (18.3) corresponds to a complex
Gaussian random variable with independent fading coeﬃcients, although neither
Gaussianity nor independence were among the initial constraints. Via the
maximum entropy principle, these properties are the consequence of the
ignorance of the modeler regarding any constraint other than the total average
energy NE0.
18.2.1.2 Probabilistic average channel energy constraint
Let us now introduce a new model for situations where the channel model deﬁned
in the previous section applies locally in time but where E0 cannot be expected
to be constant, e.g. due to short-term shadowing. Therefore, let us replace E0 in
(18.3) by the random quantity E known only through its p.d.f. PE(E). In this
case, the p.d.f. of the channel H can be obtained by marginalizing over E, as
follows.
PH(H) =
Z ∞
0
PH,E(H, E)dE =
Z ∞
0
PH|E(H)PE(E)dE.
(18.4)
In order to determine the probability distribution PE, let us ﬁnd the maximum
entropy distribution under the constraints:
• 0 ≤E ≤Emax, where Emax represents an absolute constraint on the power
carried through the channel;
• the mean E0 ≜
R Emax
0
EPE(E)dE is known.
Applying once more the Lagrange multiplier method, we introduce the scalar
unknowns β and γ, and maximize the functional
L(PE) = −
Z Emax
0
log(PE(E))PE(E)dE + β
Z Emax
0
EPE(E)dE −E0

+ γ
Z Emax
0
PE(E)dE −1

.
Equating the derivative to zero
∂L(PE)
∂PE
= 0

18.2. Channel modeling under environmental uncertainty
483
and picking the solution corresponding to taking all integrand terms of the
resulting integral to be identically zero yields
PE(E) = exp (βE −1 + γ)
and the Lagrange multipliers are ﬁnally eliminated by solving the normalization
equations
Z Emax
0
E exp (βE −1 + γ) dE = E0
Z Emax
0
exp (βE −1 + γ) dE = 1.
The Lagrangian multiplier β < 0 is then the solution to the implicit equation
EmaxeβEmax −
 1
β + E0
  eβEmax −1

= 0
(18.5)
and ﬁnally PE is obtained as the truncated exponential law
PE(E) =
β
exp(βEmax) −1eβE
for 0 ≤E ≤Emax and PE(E) = 0 elsewhere. Note that taking Emax = ∞in
(18.5) yields β = −1
E0 and the classical exponential law
PE(E) = E0e−E
E0 .
The ﬁnal maximum entropy model for PH is then:
PH(H) =
Z Emax
0
1
(πE)N
β exp(βE)
exp(βEmax) −1 exp
 
−
N
X
i=1
|hi|2
E
!
dE.
18.2.1.3 Application to the single antenna channel
In order to illustrate the diﬀerence between the two situations presented so far,
let us investigate the single input single output (SISO) case nt = nr = 1 where
the channel is represented by a single complex scalar h. Furthermore, since
the distribution is circularly symmetric, it is more convenient to consider the
distribution of r ≜|h|. After the change of variables h ≜r(cos θ + i sin θ), and
marginalization over θ, (18.3) becomes
Pr(r) = 2r
E0
e−r2
E0
(18.6)
whereas (18.4) yields
Pr(r) =
Z Emax
0
β
eβEmax −1
2r
E eβE−r2
E dE.
(18.7)
Note that the integral always exists since β < 0. Figure 18.1 depicts the
p.d.f. of r under known energy constraint ((18.6), with E0 = 1) and the known
energy distribution constraint ((18.7) is computed numerically, for Emax = 2 and

484
18. System modeling
0
1
2
3
4
0
0.2
0.4
0.6
0.8
1
Channel gain
Density
E known
Emax = 2
Emax = ∞
Figure 18.1 Amplitude distribution of the maximum entropy SISO channel models, for
E0 = 1, Emax ∈{1, 2, ∞}.
Emax = ∞, taking E0 = 1). Figure 18.2 depicts the distribution function of the
corresponding instantaneous mutual information CSISO(r) ≜log2(1 +
1
σ2 r2) for
a signal-to-noise ratio
1
σ2 of 15 dB. The lowest range of the d.f. is of particular
interest for wireless communications since it indicates the probability of a channel
outage for a given transmission rate. The curves clearly show that the models
corresponding to the unknown energy have a lower outage probability than the
Gaussian channel model.
We now consider the more involved scenario of channels with known spatial
correlation at either communication end. We will provide the proofs in their
complete versions as they are instrumental to the general manipulation of
Jacobian determinants, marginal eigenvalue distribution for small dimensional
matrices, etc. and as they provide a few interesting tools to deal with matrix
models with unitary invariance.
18.2.2
Spatial correlation models
In this section, we will incorporate several states of knowledge about the spatial
correlation characteristics of the channel in the framework of maximum entropy
modeling. We ﬁrst study the case where the correlation matrix is deterministic
and subsequently extend the result to an unknown covariance matrix.
18.2.2.1 Deterministic knowledge of the correlation matrix
In this section, we establish the maximum entropy distribution of H under
the assumption that the covariance matrix Q ≜
R
CN hhHPH|Q(H)dH is known,
where Q is a N × N complex Hermitian matrix. Each component of the

18.2. Channel modeling under environmental uncertainty
485
0
1
2
3
4
5
6
7
0
0.2
0.4
0.6
0.8
1
CSISO(r) [bits/s/Hz]
Distribution function
E known
Emax = 2
Emax = ∞
Figure 18.2 Mutual information distribution for maximum entropy SISO channel
models, when E0 = 1, Emax ∈{1, 2, ∞}, SNR of 15 dB.
covariance constraint represents an independent linear constraint of the form
Z
CN hah∗
bPH|Q(H)dH = qa,b
for (a, b) ∈{1, . . . , N}2. Note that this constraint makes any previous energy
constraint redundant since
R
CN ∥H∥2
F PH|Q(H)dH = tr Q. Proceeding along the
lines of the method exposed previously, we introduce N 2 Lagrange coeﬃcients
αa,b, and maximize
L(PH|Q) =
Z
CN −log(PH|Q(H))PH|Q(H)dH + β

1 −
Z
CN PH|Q(H)dH

+
X
a∈{1,...,N}
b∈{1,...,N}
αa,b
Z
CN hah∗
bPH|Q(H)dH −qa,b

.
Denoting A ∈CN×N the matrix with (a, b) entry αa,b and equating the
derivative of the Lagrangian to zero, one solution satisﬁes
−log(PH|Q(H)) −1 −β −hTAh∗= 0.
(18.8)
Therefore we take
PH|Q(H) = exp
 −(β + 1) −hTAh∗
which leads, after elimination of the Lagrange coeﬃcients through proper
normalization, to
PH|Q(H, Q) =
1
det(πQ) exp
 −hHQ−1h

.
(18.9)

486
18. System modeling
Again, the maximum entropy principle yields a Gaussian distribution,
although of course its components are not independent anymore.
18.2.2.2 Knowledge of the existence of a correlation matrix
It was shown in the previous sections that in the absence of information on
space correlation the maximum entropy modeling yields i.i.d. coeﬃcients for the
channel matrix and therefore an identity covariance matrix. We now consider the
case where the covariance is known to be a parameter of interest but is not known
deterministically. Again, we will proceed in two steps, ﬁrst seeking a probability
distribution function for the covariance matrix Q, and then marginalizing the
channel distribution over Q.
Density of the correlation matrix.
We
ﬁrst
establish
the
distribution
of
Q,
under
the
energy
constraint
R
tr(Q)PQ(Q)dQ = NE0, by maximizing the functional
L(PQ) =
Z
S
−log(PQ(Q))PQ(Q)dQ + β
Z
S
PQ(Q)dQ −1

+ γ
Z
S
tr(Q)PQ(Q)dQ −NE0

.
(18.10)
Due to their structure, covariance matrices are restricted to the space S of
N × N non-negative deﬁnite complex Hermitian matrices. Therefore, let us
perform the variable change to the eigenvalues and eigenvectors space as was
performed in Chapter 2 and in more detail in Chapter 16. Speciﬁcally, denote
Λ ≜diag(λ1 . . . λN) the diagonal matrix containing the eigenvalues λ1, . . . , λN
of Q and let U be the unitary matrix containing the associated eigenvectors,
such that Q = UΛUH.
We use the mapping between the space of complex N × N self-adjoint matrices
(of which S is a subspace) and U(N)/T × RN
≤, where U(N)/T denotes the space
of unitary N × N matrices with ﬁrst row composed of real non-negative entries,
and RN
≤is the space of real N-tuples with non-decreasing components (see
Lemma 4.4.6 of [Hiai and Petz, 2006]). The positive semideﬁnite property of
the covariance matrices further restricts the components of Λ to non-negative
values, and therefore S maps into U(N)/T × R+
≤
N.
Let us now deﬁne the function F over U(N)/T × R+
≤
N as
F(U, Λ) ≜PQ(UΛUH)
where U ∈U(N)/T and the ordered vector of diagonal entries of Λ lies in R+
≤
N.
According to this mapping, (18.10) becomes
L(F) =
Z
U(N)/T ×R+N
≤
−log(F(U, Λ))F(U, Λ)K(Λ)dUdΛ

18.2. Channel modeling under environmental uncertainty
487
+ β
"Z
U(N)/T ×R+N
≤
F(U, Λ)K(Λ)dUdΛ −1
#
+ γ
"Z
U(N)/T ×R+N
≤
 N
X
i=1
λi
!
F(U, Λ)K(Λ)dUdΛ −NE0
#
(18.11)
where we introduced the corresponding Jacobian
K(Λ) ≜πN(N−1)/2
QN
j=1 j!
Y
i<j
(λi −λj)2
and used tr Q = tr Λ = PN
i=1 λi. Maximizing the entropy of the distribution PQ
by taking ∂L(F )
∂F
= 0 and equating all entries in the integrand to zero yields
−K(Λ) −K(Λ) log(F(U, Λ)) + βK(Λ) + γ
 N
X
i=1
λi
!
K(Λ) = 0.
Since K(Λ) ̸= 0 except on a set of measure zero, this is equivalent to
F(U, Λ) = eβ−1+γ PN
i=1 λi.
(18.12)
Note that the distribution F(U, Λ)K(Λ) does not explicitly depend on U. This
indicates that U is uniformly distributed, with constant density PU = (2π)N
over U(N)/T. Therefore, the joint density can be factored under the form
F(U, Λ)K(Λ) = PUPΛ(Λ) where the distribution of the eigenvalues over R+
≤
N
is
PΛ(Λ) = eβ−1
PU
eγ P
i=1...N λi πN(N−1)/2
QN
j=1 j!
Y
i<j
(λi −λj)2.
(18.13)
At this point, notice that the form of (18.13) indicates that the order of the
eigenvalues is immaterial. Therefore, for the sake of simplicity, we will now work
with the p.d.f. P ′
Λ(Λ) of the joint distribution of the unordered eigenvalues,
deﬁned over R+N. Note that its restriction to the set of the ordered eigenvalues
is proportional to PΛ(Λ). More precisely
P ′
Λ(Λ) = Ceγ P
i=1...N λi Y
i<j
(λi −λj)2
(18.14)
where the value
C = eβ−1
PU
πN(N−1)/2
N! QN
j=1 j!
can be determined by solving the normalization equation for the probability
distribution P ′
Λ
Z
R+N P ′
Λ(Λ)dΛ = 1

488
18. System modeling
where we used the change of variables xi = −γλi and the Selberg integral (see
(17.6.5) of [Mehta, 2004]). Furthermore,
d log(C)
d(−γ) = N2
−γ = NE0, and we ﬁnally
obtain the ﬁnal expression of the eigenvalue distribution
P ′
Λ(Λ) =
 N
E0
N2
N
Y
n=1
1
n!(n −1)!e−N
E0
P
i=1...N λi Y
i<j
(λi −λj)2.
(18.15)
In order to obtain the ﬁnal distribution of Q, ﬁrst note that since the
order of the eigenvalues is immaterial, the restriction of U to U(N)/T is
not necessary, and Q is distributed as UΛUH where the distribution of Λ
is given by (18.15) and U is a Haar matrix. Furthermore, note that (18.15)
is a particular case of the density of the eigenvalues of a complex Wishart
matrix, described in Chapter 2. We recall that the complex N × N Wishart
matrix with K degrees of freedom and covariance Σ, denoted CWN(K, Σ), is
the matrix A = BBH where B is a N × K matrix whose columns are complex
independent Gaussian vectors with covariance Σ. Indeed, (18.15) describes the
unordered eigenvalue density of a CWN(N, E0
N IN) matrix. Taking into account
the isotropic property of the distribution of U, we can conclude that Q itself is
also a CWN(N, E0
N IN) Wishart matrix. A similar result with a slightly diﬀerent
constraint was obtained by Adhikari in [Adhikari, 2006] where it is shown that
the entropy-maximizing distribution of a positive deﬁnite matrix with known
mean G follows a Wishart distribution with N + 1 degrees of freedom, more
precisely the CWN(N + 1,
G
N+1) distribution.
The isotropic property of the obtained Wishart distribution is a consequence
of the fact that no spatial constraints were imposed on the correlation. The
energy constraint imposed through the trace only aﬀects the distribution of the
eigenvalues of Q.
We highlight the fact that the result is directly applicable to the case where the
channel correlation is known to be separable between transmitter and receiver.
In this case, the full correlation matrix Q is known to be the Kronecker product
of the transmit Qt and receive Qr correlation matrices. This channel model
is therefore the channel with separable variance proﬁle, or equivalently the
Kronecker model in the MIMO case. The stochastic nature of Qt and Qr is barely
mentioned in the literature, since the correlation matrices are usually assumed
to be measurable quantities associated with a particular antenna array shape
and propagation environment. However, in situations where these are not known
(for instance, if the array shape is not known at the time of the channel code
design, or if the properties of the scattering environment cannot be determined),
but the Kronecker model is assumed to hold, the above analysis suggests that
the maximum entropy choice for the distribution of Qt and Qr is independent,
complex Wishart distributions with, respectively, nt and nr degrees of freedom.
A Kronecker channel representation is provided in Figure 18.3.

18.2. Channel modeling under environmental uncertainty
489
Figure 18.3 MIMO Kronecker channel representation, with Qt ∈Cnt×nt. the transmit
covariance matrix, Qr ∈Cnr×nr the receive correlation matrix and X ∈Cnr×nt the
i.i.d. Gaussian scattering matrix.
Marginalization over Q.
The complete distribution of the correlated channel can be obtained by
marginalizing out Q, using its distribution as established in the previous
paragraph. The distribution of H is obtained through
PH(H) =
Z
S
PH|Q(H, Q)PQ(Q)dQ =
Z
U(N)×R+N PH|Q(H, U, Λ)P ′
Λ(Λ)dUdΛ.
(18.16)
Let us rewrite the conditional probability density of (18.9) as
PH|Q(h, U, Λ) =
1
πN det(Λ)e−hHUΛ−1UHh =
1
πN det(Λ)e−tr(hhHUΛ−1UH).
(18.17)
Using this expression in (18.16), we obtain
PH(H) =
1
πN
Z
R+N
Z
U(N)
e−tr(hhHUΛ−1UH)dU det(Λ)−1P ′
Λ(Λ)dΛ.
(18.18)
Now, similar to the proof of Theorem 16.1, let det(f(i, j)) denote the
determinant of a matrix with the (i, j)th element given by an arbitrary function
f(i, j). Let A be a Hermitian matrix which has its Nth eigenvalue AN equal
to hHh, and the others A1, . . . , AN−1 are arbitrary, positive values that will
eventually be set to zero. Letting
I(H, A1, . . . , AN−1) =
1
πN
Z
R+N
Z
U(N)
e−tr(AUΛ−1UH)PUdU det(Λ)−1P ′
Λ(Λ)dΛ
the probability PH(H) can be determined as the limit distribution when the ﬁrst
N −1 eigenvalues of A go to zero
PH(H) =
lim
A1,...,AN−1→0 I(H, A1, . . . , AN−1).

490
18. System modeling
Applying the Harish–Chandra integral of Theorem 2.4 for the now non-singular
matrix A to integrate over U yields
I(H, A1, . . . , AN−1)
= (−1)
N(N−1)
2
πN
 N−1
Y
n=1
n!
! Z
R+N
det

e
−Ai
λj

∆(A)∆(Λ−1) det(Λ)−1P ′
Λ(Λ)dΛ
=
1
πN
 N−1
Y
n=1
n!
! Z
R+N
det

e
−Ai
λj

det(Λ)N−2
∆(A)∆(Λ)
P ′
Λ(Λ)dΛ
= C
πN
 N−1
Y
n=1
n!
! Z
R+N
det

e
−Ai
λj

det(Λ)N−2∆(Λ)
∆(A)
e−N
E0 tr(Λ)dΛ
where we used the identity ∆(Λ−1) = det( 1
λi
j−1) = (−1)N(N+3)/2
∆(Λ)
det(Λ)N−1 .
Then, we decompose the determinant product using the classical expansion
formula. That is, for an arbitrary N × N matrix X = (Xij)
det(X) =
X
a∈SN
sgn(a)
N
Y
n=1
Xn,an = 1
N!
X
a,b∈SN
sgn(a)sgn(b)
N
Y
n=1
Xan,bn
where a = (a1, . . . , aN), SN denotes the set of all permutations of {1, . . . , N},
and sgn(a) is the signature of the permutation a. Using the ﬁrst form of the
determinant expansion, we obtain
∆(Λ) det

e
−Ai
λj

= det(λj−1
i
) det(e−
Aj
λi )
(18.19)
=
X
a,b∈S2
N
sgn(a)sgn(b)
N
Y
n=1
λan−1
n
e−
Abn
λn .
Note that in (18.19) we used the invariance of the second determinant by
transposition in order to simplify subsequent derivations. Therefore
I(H, A1, . . . , AN−1)
=
C
πN∆(A)
 N−1
Y
n=1
n!
!
X
a,b∈SN
sgn(a)sgn(b)
N
Y
n=1
Z
R+ λN+an−3
n
e−
Abn
λn e−N
E0 λndλn
= CN!
πN
 N−1
Y
n=1
n!
!
det(fi(Aj))
∆(A)
where we let
fi(x) =
Z
R+ tN+i−3e−x
t e−N
E0 tdt

18.2. Channel modeling under environmental uncertainty
491
and we recognize the second form of the determinant expansion. In order to
obtain the limit as A1, . . . AN−1 go to zero, similar to the proof of Theorem 16.1,
we apply Theorem 2.9 with p = 1, N1 = N −1 and y1 = 0 since A has only one
non-zero eigenvalue. This yields
PH(H) =
lim
A1,A2,...,AN−1→0 I(H, A1, . . . , AN−1)
= (−γ)N2
πNxN−1
N
N−1
Y
n=1
[n!(n −1)!]−1 det
h
fi(0), f ′
i(0), . . . , f (N−2)
i
(0), fi(xN)
i
.
(18.20)
At this point, it becomes obvious from (18.20) that the probability of H
depends only on its norm (recall that xN = hHh by deﬁnition of A). The
distribution of h is isotropic, and is completely determined by the p.d.f. PhHh(x)
of having h such that hHh = x.
Thus, for given x, h is uniformly distributed over SN−1(x) =

h, hHh = x
	
, the
complex hypersphere of radius √x centered on zero. Its volume is VN(x) = πNxN
N! ,
and its surface is SN(x) = dVN(x)
dx
= πNxN−1
(N−1)! . Therefore, we can write the p.d.f.
of x as
PhHh(x) =
Z
SN−1(x)
PH(h)dh
= (−γ)N2
(N −1)!
N−1
Y
n=1
[n!(n −1)!]−1 det
h
fi(0), f ′
i(0), . . . , f (N−2)
i
(0), fi(x)
i
.
In order to simplify the expression of the successive derivatives of fi, it
is useful to identify the Bessel K-function, Section 8.432 of [Gradshteyn and
Ryzhik, 2000], and to replace it by its inﬁnite sum expansion, Section 8.446 of
[Gradshteyn and Ryzhik, 2000].
fi(x) = 2
r x
−γ
i+N−2
Ki+N−2(2√−γx)
= (−γ)−i−N+2
"i+N−3
X
k=0
(−1)k (i + N −3 −k)!
k!
(−γx)k + (−1)i+N−1
×
+∞
X
k=0
(−γx)i+N−2+k
k!(i + N −2 + k)! (log(−γx) −ψ(k + 1) −ψ(i + N −1 + k))
#
.
Note that there is only one term in the sum with a non-zero pth derivative at
zero. Therefore, the pth derivative of fi at zero is simply (for 0 ≤p ≤N −2)
f (p)
i
(0) = (−1)−i−Nγp−i−N+2(i + N −3 −p)!.
(18.21)

492
18. System modeling
Let us bring the last column to become the ﬁrst, and expand the resulting
determinant along its ﬁrst column
det
h
f (0)
i
(0), . . . , f (N−2)
i
(0), fi(x)
i
= (−1)N−1 det
h
fi(x), f (0)
i
(0), . . . , f (N−2)
i
(0)
i
= (−1)N−1
N
X
n=1
(−1)1+nfn(x) det
h
˜f (0)
i,n (0), . . . , ˜f (N−2)
i,n
(0)
i
where ˜f (p)
i,n (0) is the N −1 dimensional column obtained by removing the nth
element from f (p)
i
(0). Factorizing the (−1)pγp−i−N+2 in the expression of f (p)
i
(0)
out of the determinant yields
det
h
˜f (0)
i,n (0), . . . , ˜f (N−2)
i,n
(0)
i
= (−1)n−N(N+1)/2γn−N2+N−1 det(G(n))
where the N −1 dimensional matrix G(n) has (l, k) entry
G(n)
l,k = Γ(q(n)
l
+ N −k −1)
where Γ(i) = (i −1)! for i positive integer, and
q(n)
l
=
 l
, l ≤n −1,
l + 1 , l ≥n.
Using the fact that Γ(q(n)
l
+ i) = q(n)
l
Γ(q(n)
l
+ i −1) + (i −1)Γ(q(n)
l
+ i −1),
note that the kth column of G(n) is
G(n)
l,k = q(n)
l
Γ(q(n)
l
+ N −k −2) + (N −k −2)G(n)
l,k+1.
Since the second term is proportional to the (k + 1)th column, it can be
omitted without changing the value of the determinant. Applying this property
to the ﬁrst N −2 pairs of consecutive columns and repeating this process again
to the ﬁrst N −2, . . . , 1 pairs of columns, we obtain
det(G(n)) = det
h
Γ(q(n)
l
+ N −2), . . . , Γ(q(n)
l
+ 2), Γ(q(n)
l
+ 1), Γ(q(n)
l
)
i
= det
h
q(n)
l
Γ(q(n)
l
+ N −3), . . . , q(n)
l
Γ(q(n)
l
+ 1), q(n)
l
Γ(q(n)
l
), Γ(q(n)
l
)
i
= det

q(n)
l
N−1−kΓ(q(n)
l
)

=
QN
i=1 Γ(i)
Γ(n)
det

q(n)
l
N−1−k
=
QN
i=1 Γ(i)
Γ(n)
(−1)
1
2 (N−1)(N−2) det

q(n)
l
k−1
where the last two equalities are obtained, respectively, by factoring out the
Γ(q(n)
l
) factors (common to all terms on the lth row) and inverting the order
of the columns in order to get a proper Vandermonde structure. Finally, the

18.2. Channel modeling under environmental uncertainty
493
determinant can be computed as
det

q(n)
l
k−1
=
Y
1≤j<i≤N−1

q(n)
i
−q(n)
j

=
n−2
Y
i=1
i!
N−1
Y
i=n
i!
(i −n + 1)!
N−1
Y
i=n+1
(i −n)!
=
QN−1
i=1 i!
(n −1)!(N −n)!.
Wrapping up the above derivations, we obtain successively
det(G(n)) =
 N−1
Y
i=1
i!
!2
(−1)(N−1)(N−2)/2
1
[(n −1)!]2 (N −n)!
then:
det
h
˜f (0)
i,n (0), . . . , ˜f (N−2)
i,n
(0)
i
=
 N−1
Y
i=1
i!
!2
(−1)n+1γn−N2+N−1
[(n −1)!]2 (N −n)!
which gives
det
h
f (0)
i
(0), . . . , f (N−2)
i
(0), fi(x)
i
=
N
X
n=1
(−1)1−Nfn(x)
 N−1
Y
i=1
i!
!2
γn−N2+N−1
[(n −1)!]2 (N −n)!
.
Finally, we have:
PhHh(x) = −
N
X
n=1
fn(x)
γN+n−1
[(n −1)!]2 (N −n)!
where γ = −N
E0 . This leads to the maximum entropy distribution for H, given
by:
PH(H) = −
1
πN(hHh)N−1
N
X
n=1
fn(hHh) γN+n−1(N −1)!
[(n −1)!]2 (N −n)!
.
The corresponding p.d.f. is shown in Figure 18.4, as well as the p.d.f. of the
instantaneous power of a Gaussian i.i.d. channel of the same size and mean power.
As expected, the energy distribution of the proposed model is more spread out
than the energy of a Gaussian i.i.d. channel.
Figure 18.5 shows the d.f. curves of the instantaneous mutual information
achieved over the channel described in (18.1) by these two channel models. The
proposed model diﬀers in particular in the tails of the distribution: for instance,
the 1% outage capacity is reduced from 8 to 7 bits/s/Hz with respect to the
Gaussian i.i.d. model.

494
18. System modeling
0
10
20
30
40
50
0.02
0.04
0.06
0.08
0.1
Channel gain
Density
Gaussian i.i.d.
Correlated channel
Figure 18.4 Amplitude distribution of the maximum entropy 4 × 4 MIMO channel
models, with known identity correlation (Gaussian i.i.d.) or unknown correlation.
6
8
10
12
14
0
0.2
0.4
0.6
0.8
1
Mutual information [bits/s/Hz]
Distribution function
Gaussian i.i.d.
Correlated channel
Figure 18.5 Mutual information distribution of the maximum entropy 4 × 4 MIMO
channel models, with known identity correlation (Gaussian i.i.d.) or unknown
correlation, SNR of 10 dB.
18.2.2.3 Limited-rank covariance matrix
In this section, we address the situation where the modeler takes into account
the existence of a covariance matrix of rank L < N (we assume that L is known).
Such a situation arises in particular when the communication channel is a priori
known not to oﬀer numerous degrees of freedom, or when the MIMO antennas on

18.2. Channel modeling under environmental uncertainty
495
Figure 18.6 MIMO Kronecker channel representation with limited number of
scatterers in the propagation environment, with Qt ∈Cnt×nt. the (possibly
rank-limited) transmit covariance matrix, Qr ∈Cnr×nr the (possibly rank-limited)
receive correlation matrix and X ∈Cnr×nt the i.i.d. Gaussian scattering matrix.
either communication side are known to be close enough for correlation to arise.
Figure 18.6 depicts a Kronecker channel environment with limited diversity.
As in the full-rank case, we will use the spectral decomposition Q = UΛUH
of the covariance matrix, with Λ = diag(λ1, . . . , λL, 0, . . . , 0). Let us denote
ΛL = diag(λ1, . . . , λL). The maximum entropy probability density of Q with
the extra rank constraint is unsurprisingly similar to that derived previously,
with the diﬀerence that all the energy is carried by the ﬁrst L eigenvalues, i.e.
U is uniformly distributed over U(N), while
PΛL(ΛL) =
 L2
NE0
L2
L
Y
n=1
1
n!(n −1)!e−
L2
NE0
P
i=1...L λi
Y
i<j≤L
(λi −λj)2. (18.22)
However, the deﬁnition of the conditional probability density PH|Q(h, U, Λ)
in (18.9) does not hold when Q is not full rank. The channel vector h becomes
a degenerate Gaussian random variable. Its projection onto the L-dimensional
subspace associated with the non-zero eigenvalues of Q follows a Gaussian law,
whereas the probability of h being outside this subspace is zero. The conditional
probability in (18.17) must therefore be rewritten as
PH|Q(h, U, ΛL) = 1span(U[L])(h)
1
πL QL
i=1 λi
e−hHU[L]Λ−1
L U[L]
Hh
(18.23)
where U[L] denotes the N × L matrix obtained by truncating the last N −L
columns of U. The indicator function ensures that PH|Q(h, U, Λ) is zero for h
outside of the column span of U[L].
We need now to marginalize U and Λ in order to obtain the p.d.f. of h.
PH(h) =
Z
U(N)×R+L PH|Q(h, U, ΛL)PΛL(ΛL)dUdΛL.

496
18. System modeling
However, the expression of PH|Q(h, U, ΛL) does not lend itself directly to the
marginalization described in the previous sections, since the zero eigenvalues of Q
complicate the analysis. This can be avoided by performing the marginalization
of the covariance in an L-dimensional subspace. In order to see this, consider an
L × L unitary matrix BL and note that the N × N block matrix
B =
BL
0
0 IN−L

is unitary as well. Since the uniform distribution over U(N) is unitarily invariant,
UB is uniformly distributed over U(N) and for any BL ∈U(L) we have:
PH(h) =
Z
U(N)×R+L PH|Q(h, UB, ΛL)PΛL(ΛL)dUdΛL.
Furthermore, since
R
U(L) dBL = 1
PH(h) =
Z
U(L)
Z
U(N)×R+L PH|Q(h, UB, ΛL)PΛL(ΛL)dUdΛLdBL
=
Z
U∈U(N)
1span(U[L])(h)Pk(U[L]
Hh)dU
(18.24)
where (18.24) is obtained by letting k = U[L]
Hh and
Pk(k) =
Z
U(L)×R+L
1
πL QL
i=1 λi
e−kHBLΛ−1
L BH
LkPΛL(ΛL)dBLdΛL.
(18.25)
We can then exploit the similarity of (18.25) and (18.18) and, by the same
reasoning as in previous sections, conclude directly that k is isotropically
distributed in U(L) and that its p.d.f. depends only on its Frobenius norm,
following
Pk(k) =
1
SL(kHk)P (L)
x
(kHk)
where
P (L)
x
(x) = 2
x
L
X
i=1

−L
r
x
NE0
L+i
Ki+L−2

2L
r
x
NE0

1
[(i −1)!]2 (L −i)!
.
Finally, note that hHh = kHk, and that the marginalization over the random
rotation that transforms k into h in (18.24) preserves the isotropic property of
the distribution. Therefore
PH(h) =
1
SN(hHh)P (L)
x
(hHh).
Examples of the corresponding p.d.f. for L ∈{1, 2, 4, 8, 12, 16} are represented
in Figure 18.7 for a 4 × 4 channel, together with the p.d.f. of the instantaneous
power of a Gaussian i.i.d. channel of the same size and mean power. As expected,
the energy distribution of the proposed maximum entropy model is more spread
out than the energy of a Gaussian i.i.d. channel.

18.2. Channel modeling under environmental uncertainty
497
0
10
20
30
40
50
0.02
0.04
0.06
0.08
0.1
Channel gain
Density
Gaussian
L = 1
L = 2
L = 4
L = 8
L = 12
L = 16
Figure 18.7 Amplitude distribution of the maximum entropy 4 × 4 MIMO channel
models, for E0 = 1, and limited degrees of freedom L ∈{1, 2, 4, 8, 12, 16}.
The d.f. of the mutual information achieved over the limited-rank (L < 16)
and full rank (L = 16) covariance maximum entropy channel at a signal-to-noise
ratio of 10 dB is depicted in Figure 18.8 for various ranks L, together with
the Gaussian i.i.d. channel. As already mentioned, the proposed model diﬀers
especially in the tails of the distribution. In particular, the outage capacity for
low outage probability is greatly reduced with respect to the Gaussian i.i.d.
channel model.
18.2.2.4 Discussion
It is important to understand the reason why maximum entropy channels are
designed. It is of interest to characterize ergodic and outage capacities when very
limited information is known about the channel as this can provide a ﬁgure of
what mean or minimum transmission rates can be expected in a channel that is
known to have limited degrees of freedom. Typically, MIMO communication
channels with small devices embedded with multiple antennas tend to have
strong correlation. Measuring correlation by a simple scalar number is however
rather diﬃcult and can be done through many approaches. Measuring correlation
through the number of degrees of freedom left in the channel is one of those. The
study above therefore helps us anticipate the outage performance of multiple
antenna communications in more or less scattered environments.
Another very interesting feature of maximum entropy channel models is that
they can be plugged into problems such as source sensing when the channel
environment is known to enjoy some speciﬁc features. For instance, remember
that in Chapter 16 we derived Neyman–Pearson tests based on the maximum
entropy principle, in the sense that we assumed the communication channel

498
18. System modeling
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
Mutual information [bits/s/Hz]
Distribution function
Gaussian
L = 1
L = 2
L = 4
L = 8
L = 12
L = 16
Figure 18.8 Mutual information distribution of the maximum entropy 4 × 4 MIMO
channel models, with known identity correlation (Gaussian i.i.d.) or unknown
correlation, SNR of 15 dB.
was only known to have signal-to-noise ratio 1/σ2, in which case we considered
a Gaussian i.i.d. channel model (the choice of which is now conﬁrmed by the
analysis above). We then assumed the SNR was imperfectly known, so that
we obtained an integral form over possible σ2 of the Neyman–Pearson test.
Alternatively, we could have assumed from the beginning that the channel
variance was imperfectly known and used the expressions derived above of
the distribution of the channel variance. Consistency of the maximum entropy
principle, detailed in [Jaynes, 2003], would then ensure identical results at the
end. Now, in the case when further information is known, such as the channel
degrees of freedom are limited (for instance when a sensor network with multiple
close antennas scans a low frequency resource), adaptive sensing strategies can be
put in place that account for the expected channel correlation. In such scenarios,
Neyman–Pearson tests can be more adequately designed than when assuming
Gaussian i.i.d. propagation channels.
We believe that the maximum entropy approach, often used in signal
processing questions, while less explored in wireless communications, can provide
interesting solutions to problems dealing with too many unknown variables.
Instead of relying on various ad-hoc approaches, the maximum entropy principle
manages to provide an information-theoretic optimum solution to a large range of
problems. It was in particular noticed in [Couillet et al., 2010] that conventional
minimum mean square error channel estimators enter the framework of maximum
entropy channel estimation, when only the number of propagation paths in the
frequency selective channel is a priori known. Then, for unknown channel delay
spread, extensions of the classical minimum mean square error approach can

18.2. Channel modeling under environmental uncertainty
499
be designed, whose increased complexity can then be further reduced based on
suboptimal algorithms. This is the basic approach of maximum entropy solutions,
which seek for optimal solutions prior to providing suboptimal implementations,
instead of using simpliﬁed suboptimal models in the ﬁrst place. Similarly, a
maximum entropy optimal data-aided coarse frequency oﬀset estimator for
orthogonal frequency division multiplexing protocols is provided in [Couillet and
Debbah, 2010b], along with a suboptimal iterative algorithm.
This completes this chapter on maximum entropy channel modeling.


19
Perspectives
Before concluding this book, we will brieﬂy discuss open questions in random
matrix theory. We will describe current research topics, results that are still
unknown and that would be worth looking into. We also introduce brieﬂy the
replica methods, an alternative to the Stieltjes transform, and free probability
methods proposed in this book which have been gaining a lot of interest lately,
and will conclude with possible extensions of random matrix theory to a more
elaborate time-varying (stochastic) mathematical framework.
19.1
From asymptotic results to ﬁnite dimensional studies
First, we recall the recent advances in random matrix theory, already introduced
in this book, and which will be studied further in the near future. In Part I,
we ﬁrst explored limiting spectral distributions of some simple random matrix
models. For instance, we characterized the l.s.d. of the sample covariance matrix
model when the e.s.d. of the population covariance matrix converges weakly to
some distribution function. We then observed that, for more practical system
models, such as the sum of matrices with independent entries and left- and
right-correlations, there may not exist a limiting spectral distribution, even
when all deterministic matrices in the model do have a l.s.d. This led us to
consider the deterministic equivalent approach instead of the l.s.d. approach.
These deterministic equivalents have an outstanding advantage over l.s.d. and
can be considered a major breakthrough for applied random matrix theory for
the following reasons.
• They no longer require the existence of converging series of deterministic
matrices in the model. For instance, in the sample covariance matrix case
applied to the characterization of the sum rate capacity of a MISO broadcast
channel with N users, we can freely assume that the distances of the users
to the base station (which are modeled in the entries of the population
covariance matrix) take any given values for all ﬁnite N. There is therefore no
need to assume the existence of an unrealistic converging user-to-base station
distance distribution as the number of users grows. This alleviates basic system
modeling issues.

502
19. Perspectives
• More importantly, deterministic equivalents provide an approximation of the
performance of such systems for all ﬁnite N, and not as N tends to inﬁnity.
Based on the previous example, we can imagine the case of a cellular MISO
broadcast channel with users being successively connected to or disconnected
from the base station. In this scenario, the analyzes based on l.s.d. or
deterministic equivalents diﬀer as follows.
– with l.s.d. considerations, the sum rate for all ﬁnite N can be approximated
by a single value corresponding to some functional of the l.s.d. of the
sample covariance matrix when the population covariance matrix models
the scenario of an increasingly high user density. The approximation here
lies therefore in the fact that the reality does not ﬁt the asymptotic model;
– with deterministic equivalents, it is possible to derive an approximation of
the system performance for every N, whatever the position of the active
users. Therefore, even if the large N asymptotic performance (when all
users are connected and their number grows to inﬁnity) leads to a unique
expression, the performances for all conﬁgurations of N users lead to various
results. The inaccuracy here does not lie in the system model but rather in
the inexactness of the ﬁnite N approximation, which is more convenient.
• Remember ﬁnally that, for more involved system models, limiting spectral
distributions may not exist at all, and therefore the l.s.d. approach cannot be
used any longer. This led quite a few authors to assume very unrealistic system
models in order for a l.s.d. to exist, so to obtain exploitable results. From the
theory of deterministic equivalents developed in Part I, this is unnecessary.
We wanted to insist on the considerations above a second time since random
matrix theory applications to wireless communications are suﬀering from the
false impression that the models designed assume an inﬁnite number of antennas,
an inﬁnite number of users, etc. and that these models are so unrealistic that
the results obtained are worthless. We hope that the reader now has a clear
understanding that such criticism, totally acceptable ten years ago, are no longer
justiﬁed. In Chapters 13–14, we derived approximated expressions of the capacity
of multiple antenna systems (single-user MIMO, MIMO MAC, MIMO BC)
and observed that the theoretical curves are indiscernible from the simulated
curves, sometimes for N as small as 4. In this book, when dealing with practical
applications, we systematically and purposely replaced most asymptotic results
found in the literature by deterministic equivalents and replaced any mention of
the term asymptotic or the phrase inﬁnite size matrices by phrases such as for
all ﬁnite N or accurate as N grows large.
We believe that much more is to be done regarding deterministic equivalents
for more involved system models than those presented in this book. Such models
are in particular demanded for the understanding of future cognitive radio
networks as well as small-cell networks which will present more elaborate system
conditions, such as cooperating base stations, short-range communications with
numerous propagation paths, involved interference patterns, intense control

19.1. From asymptotic results to ﬁnite dimensional studies
503
data exchange, limited real-time channel state information, etc. With all these
parameters taken into account, it is virtually impossible to assume large
dimensional scenarios of converging matrix models. Deterministic equivalents
can instead provide very precise system performance characterizations. It is
important also to realize that, while the system models under study in the
application chapters, Chapters 12–15, were sometimes very intricate, questions
such as capacity optimization often resulted in very elegant and compact forms
and come along with simple iterative algorithms, often with ensured convergence.
It is therefore to be believed that even more complex models can still be
provided with simple optimizations. It is important again to recall at this
point that accurate mathematical derivations are fundamental to ensure in
particular that the capacity maximizing algorithms do converge surely. We
also mentioned that second order statistics of the performance of such models
can also be well approximated in the large dimensional regime, with simple
forms involving Jacobian matrices of the fundamental equations appearing
systematically. Second order statistics provide further information on the outage
capacity of these systems but also on the overall reliability of the deterministic
equivalents. In heterogeneous primary-secondary networks where low probability
of interference is a key problem to be considered, simple expressions of the second
order statistics are of dramatic importance. As a consequence, in the near future,
considerable eﬀort will still need to be cast on deterministic equivalents and
central limit theorems. Given the manpower demanded to treat the vastness of
the recent small cell incentive, a systematic simpliﬁcation of classical random
matrix methods presented in this book will become a research priority.
From a more technical point of view, we also insist on the fact that
the existence of a trace lemma for some matrix models is often suﬃcient
to establish deterministic equivalents for involved system models. This was
recently exempliﬁed by the case of Haar matrices for which the trace lemma,
Theorem 6.15, allows us to determine the approximated capacity of multi-cellular
orthogonal CDMA setups with multi-path channels based on the Stieltjes and
Shannon transforms provided in Theorem 6.17. Future multi-station short-range
communication models with strong line-of-sight components may surely demand
more exotic channel models than the simple models based on i.i.d. random
matrices. We think in particular of Euclidean matrices [Bordenave, 2008] that
can be used to model random grids of access points. If trace lemmas can be found
for these models, it is likely that results similar to the i.i.d. case will be derived,
for which systematic optimization methods and statistical analysis will have to
be generated.
The methods for deterministic equivalents presented in this book therefore
only pave the way for much more complex system model characterizations.
Nonetheless, we also noticed that many random matrix models, more structured,
are still beyond analytical reach, although combinatoric moment approaches are
ﬁlling the gap. In particular, the characterization of the limiting spectrum of
some random Vandermonde matrix models has known an increasing interest since

504
19. Perspectives
tools from combinatorics can be used to eﬃciently derive all successive moments
of the mean e.s.d. for all ﬁnite dimensions. As long as the random matrices
under study exhibit symmetric structures, in the sense of rotational invariance,
moments of the mean e.s.d. and of the l.s.d. can be usually characterized. For
a complete distribution function to be analyzed, though, more advanced tools
are demanded. As reminded earlier, the existence of a trace lemma makes the
Stieltjes transform approach usable. However, for the random Vandermonde
matrices, such a trace lemma is beyond one’s knowledge to this day. Instead, it
may be that other types of matrix transforms or possibly the Gaussian method,
introduced brieﬂy in Section 6.2 and which relies to some extent on moment
calculus, can be used eﬃciently for these involved random matrix models. The
Gaussian tools in particular, which are gaining more and more interest these
days, may open new routes to future breakthroughs in the characterization of
spectral distribution of structured matrices with invariance properties. Instead
of a trace lemma, this approach essentially requires an appropriate integration
by parts formula, Theorem 6.6, and a Nash–Poincar´e inequality, Theorem 6.7,
adapted to the structure of the random matrices under study.
In terms of estimation and detection, the increasing interest for cognitive radio
networks, sensor networks with non-necessarily regular topology, and the need
to share control information across large networks demand for more and more
complex parameter estimators. Since these network structures typically have
several large dimensions of similar sizes, the random matrix-based inference
techniques developed in Chapter 8 are expected to be further developed in
the future. From a purely mathematical viewpoint, these techniques are based
on advanced results of limiting exact eigenvalue separation and central limit
theorems. Such results are however known only to hold for a limited number
of random matrix models and will need to be further extended in the future.
Moreover, similar to central limit theorems for the deterministic equivalents of
some functionals of the eigenvalues, central limit theorems for G-estimators are
fundamental to characterize the sensitivity and the reliability of the parameter
estimates. For practical cognitive radio and sensor network applications, such
results will be importantly needed.
From a small dimensional viewpoint, we observed in Chapter 9 with the
extreme eigenvalue distribution of spiked models, in Chapter 16 with detection
tests for multi-dimensional systems, and in Chapter 18 with multi-dimensional
channel modeling that small dimensional random matrix analysis is still a hot
topic, which also demands for further generalizations. In fact, while random
matrix theory started oﬀwith the study of small dimensional random matrices
with the work of Wishart, it quickly moved to asymptotic considerations that
have reached their glory in the past ten years, while today the study of the
extreme eigenvalues and spiked models call for a revisit of the ﬁnite size
considerations. We mentioned many times that an important ﬁeld of random
matrices, involving orthogonal polynomials and Fredholm determinants, has
been providing numerous key results very recently for the characterization

19.2. The replica method
505
of limiting distributions and asymptotic independence of extreme eigenvalues.
This topic, which originates from rather old works on the inner symmetry
of unitarily invariant random matrices, e.g., [James, 1964], is still being
thoroughly investigated these days. The tools required to study such models
are very diﬀerent from those proposed here and call for deeper mathematical
considerations. A systematic simpliﬁcation of these methods which should also
generalize to more challenging random matrix models is also the key for
future usage of these rather diﬃcult tools in signal processing and wireless
communications.
As we mention the democratization of some mathematical tools for random
matrices, this book being a strong eﬀort in this direction, we discuss brieﬂy
hereafter the method of statistical mechanics known as the replica trick or replica
method which has not been mentioned so far but which has been generating lately
an important wave of results in terms of l.s.d. and deterministic equivalents.
Instead of introducing the speciﬁcs of this tool, which has the major drawback
of relying on non-mathematically rigorous methods, we discuss its interaction
with the methods used in this book.
19.2
The replica method
In addition to the approaches treated in Part I to determine deterministic
equivalents, another approach, known as the replica method, is gaining ground
in the wireless communication community. In a similar way as deterministic
equivalent derivations based on the ‘guessing’ part of the Bai and Silverstein
approach, this technique provides in general a ﬁrst rapid hint on the expected
solutions. The replica method does however not come along with appropriate
mathematical tools to prove the accuracy of the derived solutions. More precisely,
the replica derivations assume several mathematical properties of continuity and
limit-integral interchange, which are assumed valid at ﬁrst (in order to obtain
the hypothetical solutions) but which are very challenging to prove. This tool
therefore has to be used with extreme care. For a short introduction to the replica
method, see, e.g., the appendix of [M¨uller, 2003].
The replica method is an approach borrowed from physics and especially from
the ﬁeld of statistical mechanics, see, e.g., [M´ezard et al., 1987; Nishimori, 2001].
It was then extensively used in the ﬁeld of wireless communications, starting with
the work of Tanaka [Tanaka, 2002] on maximum-likelihood CDMA detectors. The
asymptotic behavior of numerous classical detectors were then derived using
the replica method, e.g., [Guo and Verd´u, 2002]. The replica method is used
in statistical physics to evaluate thermodynamical entropies and free energy.
In a wireless communication context, those are closely linked to the mutual
information, i.e. the diﬀerence of receive signal and source signal entropies
[Shannon, 1948], in the sense that free energy and mutual information only
diﬀer from an additive constant and a scalar factor. Replica methods have in

506
19. Perspectives
particular proved to be very useful when determining central limit theorems
for the e.s.d. of involved random matrix models. While classical deterministic
equivalents and Stieltjes transform approaches used to fail to derive nice closed-
form formulas for the asymptotic covariance matrices of functionals of the e.s.d.,
the replica method often conjectured that these covariance matrices take the form
of Jacobian matrices. So far, all conjectures of the replica method going in this
Jacobian direction turned out to be exact. For instance, the proof of Theorem
6.21 relies on martingale theory, similar to the proof of Theorem 3.18. With these
tools alone, the limiting variance of the central limit often takes an unpleasant
form, which is not obvious to relate to a Jacobian, although it eﬀectively is
asymptotically equivalent to a Jacobian. In this respect, replica methods have
turned out to be extremely useful. However, some examples of calculus where
the replica method fails have also been identiﬁed. Today, mathematicians are
progressively trying to raise necessary and suﬃcient conditions for the replica
method to be valid. That is, situations where the critical points of the replica
derivations are valid or not are progressively being identiﬁed.
So far, however, the validity conditions are not suﬃcient for limiting laws and
deterministic equivalents to be accurately proved using this method. We therefore
see the replica method as a good opportunity for engineers and researchers to
easily come up with would-be results that can then be accurately proved using
classical random matrix techniques. We do not develop further this technique,
though, as it requires the introduction of several additional tools, and we leave
it to the reader to refer to alternative introductory articles.
We complete this chapter with the introduction of some ideas on the
generalization of random matrix theory to continuous time random matrix theory
and possible applications to wireless communications.
19.3
Towards time-varying random matrices
In addition to characterizing the capacity of wireless channels, it has always been
of interest in wireless communications to study their time evolution. We have
to this point gone successively through the following large dimensional network
characterizations:
• the capacity, sum rate, or rate regions, that allow us to anticipate either the
averaged achievable rate of quasi-static channels or the exact achievable rate
of long coded sequences over very fast varying channels;
• the outage capacity, sum rate, or rate regions, which constitute a quality of
service parameter relative to the rates achievable with high probability.
Now, since communication channels vary with time, starting from a given
deterministic channel realization, it is possible to anticipate the rate evolution
of this channel. Indeed, similar to the averaging eﬀect arising when the
matrix dimensions grow large, that turn random eigenvalue distributions into

19.3. Towards time-varying random matrices
507
asymptotically deterministic quantities, solutions to implicit equations, the
behavior of the time-varying random eigenvalues of a deterministic matrix
aﬀected by a Wiener process (better known as Brownian motion) can be
deterministically characterized as the solution of diﬀerential equations. Although
no publication related to these time-varying aspects for wireless communications
has been produced so far (mostly because the tools are not mature enough), it
is to be believed that random matrix theory for wireless communications may
move on a more or less long-term basis towards random matrix process theory
for wireless communications. Nonetheless, these random matrix processes are
nothing new and have been the interest of several generations of mathematicians.
We hereafter introduce brieﬂy the fundamental ideas, borrowed from a tutorial
by Guionnet [Guionnet, 2006]. The initial interest of Guionnet is to derive the
limiting spectral distribution of a non-central Wigner matrix with Gaussian
entries, based on stochastic calculus. The result we will present here provides,
under the form of the solution of a diﬀerential equation, the limiting eigenvalue
distribution of such a random matrix aﬀected by Brownian noise at all times
t > 0.
We brieﬂy introduce the notion of a Wigner matrix-valued Wiener process. A
Wiener process Wt is deﬁned as a stochastic process with the following properties:
• W0 = 0
• Wt is a random variable, almost surely continuous over t
• for s, t > 0, Wt −Ws is Gaussian with zero mean and variance t −s
• for s1 ≤t1 < s2 ≤t2, Wt1 −Ws1 is independent of Wt2 −Ws2.
This deﬁnition allows for the generation of random processes with independent
increments. That is, if Wt is seen as the trajectory of a moving particle, the
Wiener process assumptions ensure that the increment of the trajectory between
two time instants is independent of the increments observed between any two
instants in the past. This will be suitable in wireless communications to model
the evolution of an unpredictable time-varying process such as the evolution of
a time-varying channel matrix from a deterministically known matrix and an
additional random time-varying innovation term; the later being conventionally
modeled as Gaussian at time t = 1.
Instead of considering channel matrices, though, we restrict this introduction
to Wigner matrices. We deﬁne the Wigner matrix-valued Wiener process as the
time-varying matrix XN(t) ∈CN×N with (m, n) entry XN,mn(t) given by:
XN,mn(t) =
(
1
√
2N (Wm,n(t) + iW ′
m,n(t)) , m < n
1
√
N Wm,m(t)
, m = n
where Wm,n(t) and W ′
m,n(t) are independent Wiener processes. As such, from the
above deﬁnition, XN(1) is a Gaussian Wigner matrix. We then deﬁne YN(t) ∈
CN×N as
YN(t) = YN(0) + XN(t)

508
19. Perspectives
for some deterministic Hermitian matrix YN(0) ∈CN×N.
We recognize that at time t = 1, YN(1) = YN(0) + XN(1) is a Gaussian
Wigner matrix with Gaussian independent entries of mean given by the entries
of YN(0) and variance 1/N. The current question though is to analyze the time
evolution of the eigenvalue distribution of YN(t).
Denote λN(t) = (λN
1 (t), . . . , λN
N(t)) the set of random eigenvalues of YN(t) and
F YN(t) the e.s.d. of YN at time t. The following result, due to Dyson [Dyson,
1962b], characterizes the time-varying e.s.d. F YN(t) as the solution of a stochastic
diﬀerential equation.
Theorem 19.1. Let λN(0) be such that λN
1 (0) < . . . < λN
N(0). Then F YN(t) is
the unique (weak) solution of the stochastic diﬀerential system
dλN
i (t) =
1
√
N
dV i
t + 1
N
X
j̸=i
1
λN
i (t) −λN
j (t)dt
with initial condition λN(0), such that λN
1 (t) < . . . < λN
N(t), where (V 1
t , . . . , V N
t )
is an N-dimensional Wiener process.
This characterizes the distribution of λN(t) for all ﬁnite N. A large dimensional
limit for such processes is then characterized by the following result, [Guionnet,
2006, Lemma 12.5].
Theorem 19.2. Let λN(0) ∈RN such that F YN(0) converges weakly towards
F0 as N tends to inﬁnity. Further, assume that
sup
N
Z
log(1 + x2)dF YN(0)(x) < ∞.
Then, for all T > 0, the measure-valued process (F YN(t), t ∈[0, T]) converges
almost surely in the set of distribution function-valued continuous functions
deﬁned on [0, T] towards (Ft, t ∈[0, T]), such that, for all z ∈C \ R
mFt(z) = mF0(z) +
Z t
0
mFs(z)m′
Fs(z)ds
where the derivative of the Stieltjes transform is taken along z.
This result generalizes the free additive convolution to time continuous
processes. What this exactly states is that, as N grows large, F YN(t) converges
almost surely to some d.f. Ft, which is continuous along the time variable t.
This indicates that, for large N, the eigenvalues of the time-evolving random
matrix YN(t) follow a trajectory, whose Stieltjes transform satisﬁes the above
diﬀerential equation.
It is to be believed that such time-varying considerations, along with the recent
growing interest in mean ﬁeld and mean ﬁeld game theories [Bordenave et al.,
2005; Buchegger and Le Boudec, 2005; Le Boudec et al., 2007; Saraﬁjanovic and

19.3. Towards time-varying random matrices
509
Le Boudec, 2005; Sharma et al., 2006], may lead to the opening up of a new ﬁeld of
research in wireless communications. Indeed, mean ﬁeld theory is dealing with the
characterization of large dimensional time-varying systems for which asymptotic
behavior are found to be solutions of stochastic diﬀerential equations. Mean ﬁeld
theory is in particular used in game-theoretic settings where numerous players,
whose space distribution enjoys some symmetric structure, compete under some
cost function constraint. Such characterizations are suitable for the study of the
medium access control for future large dimensional networks, where the adjective
large qualiﬁes the number of users in the network. The time-varying aspects
developed above may well turn in the end into a characterization of the time
evolution of the physical layer for future large dimensional networks, where the
adjective large now characterizes, e.g. the number of transmit antennas at the
base station in a cellular broadcast channel or the number of users in a CDMA
cell.
The possibility to study the time evolution of large dimensional networks, be
it from a physical layer, medium access control layer, or network layer point of
view, provides much more information than discrete time analysis in the sense
that:
• as already mentioned, the classical static analysis brought by random matrix
theory in wireless communications only allows us to anticipate the average
performance and outage performance of a given communication system. That
is, irrespective of the time instant t, quality of service ﬁgures such as averaged
or minimally ensured data delivery rate can be derived. Starting from a data
rate R0 at time t0, it is however not possible to anticipate the averaged rate
or minimally ensured rate Rt at time t > t0 (unless t is large enough for the
knowledge of R0 to become irrelevant). This can be performed by continuous
time analysis though;
• dynamic system analysis also allows us to anticipate probabilities of chaotic
situations such as system failure after a time t > t0, with t0 some initial time
when the system is under control. We mention speciﬁcally the scenario of
automatized systems with control, such as recent smart energy distribution
networks in which information feedback is necessary to set the system as a
whole in equilibrium. However, in wireless communications, as in any other
communication system, feeding information back comes at a price and is
preferably limited. This is why being able to anticipate the (possibly erratic)
evolution of a system in free motion is necessary so to be able to decide when
control has to take place;
• along the same line of thought as in the previous point, it is also important
for system designers to take into account and anticipate the consequences
of mobility within a large dimensional network. Indeed, in a network
where users’ mobility is governed by some time-varying stochastic process,
the achievable transmission data rates depend strongly on channel state
information exchanges within the network. In the discrete time random matrix

510
19. Perspectives
framework, deterministic considerations of the necessary information feedback
is assumed, irrespective of the users’ mobility (i.e. irrespective of the frequency
to which information would eﬀectively need to be fed back). The introduction
of dynamics and user mobility distributions would allow for a disruptive study
of such networks from a physical layer point of view. We mention for instance
the long-standing problem of multi-cellular cooperation for future wireless
communication networks, which takes time to appear in telecommunication
standards since it is still unclear whether the extra feedback cost involved (i.e.
the channel fading information of all base station-to-user links required to all
base stations) is detrimental to the cooperation gain in terms of peak data
rate. This demands the performance analysis of a typical network with users
in motion according to some stochastic behavior, which again is not accessible
to this day in the restrictive framework of random matrix theory.
This concludes this short chapter on the envisioned perspectives for random
matrix theory applications in wireless communication networks. We end this
book with a general conclusion of Part I and Part II.

20
Conclusion
Throughout this book, we tried to propose an up-to-date vision of the
fundamental applications of random matrix theory to wireless communications.
“Up-to-date” refers to the time when these lines were written. At the pace which
the random matrix ﬁeld evolves these days, the current book will be largely
outdated when published. This is one of the two fundamental reasons why we
thoroughly introduced the methods used to derive most of the results known to
this day, as these technical approaches will take more time to be replaced by
more powerful tools. The other, more important, reason why such an emphasis
was made on these techniques, is that the wireless communication community
is moving fast and is in perpetual need for new random matrix models for
which mathematical research has no answer yet. Quite often, such an answer
does not exist because it is either of no apparent interest to mathematicians or
simply because too many of these problems are listed that cannot all be solved
in a reasonable amount of time. But very often also, these problems can be
directly addressed by non-mathematical experts. We desired this book to be both
accessible in the sense that fast solutions to classical problems can be derived
by wireless communication engineers and rigorous in some of the proofs so that
precise proof techniques be known to whomever desires to derive mathematically
sound information-theoretic results.
An important outcome of the current acceleration of the breakthroughs made
in random matrix theory for wireless communications is the generalization of
non-mathematically accurate methods, such as the replica method, introduced
brieﬂy in Chapter 19. From our point of view, thanks to the techniques developed
in this book, it is also fairly simple to derive deterministic equivalents for the
very same problems addressed by the replica approach. Nonetheless, the replica
method is a very handy tool for fast results that may take time to obtain using
conventional methods. This is conﬁrmed for instance by the work of Moustakas
et al. [Moustakas and Simon, 2007] on frequency selective MIMO channels, later
proved accurate by Dupuy and Loubaton in an unpublished work, or by the work
of Taricco [Taricco, 2008] on MIMO Rician models, largely generalized by the
work of Hachem et al. [Hachem et al., 2008b], or again by the work of Simon
et al. [Simon et al., 2006] generalized and accurately proved by Couillet et al.
[Couillet et al., 2011a] and further extended in [Wagner et al., 2011] by Wagner et
al.. As reminded in Chapter 19, replica methods also provide results that are not

512
20. Conclusion
at all immediate using conventional tools, and constitute, as such, an important
tool to be further developed. However, we intend this book to reinforce the idea
to the reader that the diﬃcult Stieltjes transform and Gaussian method tools
of yesterday are now clearly understood and have moved to a simple and very
accessible framework, no longer exclusive to a few mathematicians among the
research community. We recall in particular that the deterministic equivalent
method we referred to as Bai and Silverstein’s approach in this book is rather
simple and only requires practice. Once deterministic equivalents are inferred
via the “guess-work” technique, accurate proofs can then be performed, which
are usually based on very classical techniques. Moreover, as most results end up
being solutions of implicit equations, it is important for practical purposes to
be able to prove solution uniqueness and if possible sure convergence of some
ﬁxed-point algorithm to the solution. One of the reasons comes as follows: in the
situation where we have to estimate some key system parameters (such as the
optimal transmit covariance matrix in MIMO communications) and that these
parameters are one of the solutions to an implicit equation, if sure convergence
of some iterative algorithm towards this speciﬁc solution is proved, then the
stability of the system under consideration is ensured.
Another source of debate within the random matrix community is the question
of whether free probabilistic tools or more conventional random matrix tools
must be used to solve problems dealing with large dimensional random matrices.
Some time ago, problems related to Haar matrices were all approached using free
probability tools since the R- and S-transforms are rather convenient for dealing
with sums or products of these types of matrices. Nonetheless, as an equivalent
trace lemma for Haar random matrices, Theorem 6.15, exists, it is also possible to
treat models involving Haar matrices with the same tools as those used for i.i.d.
matrices, see, e.g., Theorem 6.17. Moreover, free probability relies on stringent
assumptions on the matrices involved in sums and products, starting with
the eigenvalue boundedness assumption that can be somewhat extended using
more conventional random matrix techniques. There is therefore no fundamental
reason to prefer the exclusive usage of free probability theory, as the same
derivations and much more are accessible through classical random matrix
theory. Nonetheless, it is usually simpler, when possible, to exploit directly
free probability theorems for involved sums and products of asymptotically
free matrix families than to resort to complete random matrix derivations and
convergence proofs, see, e.g., Section 15.2 on multi-hop communications. Since
both ﬁelds are not orthogonal, it is therefore possible to use results from both of
them to come up fast with results on more involved matrix models.
Regarding the latest contributions on signal sensing and parameter estimation,
the studies provided in this book showed that, while important limitations
(linked to spectrum clustering) restrict the use of recent Stieltjes transform-based
techniques, these tools perform outstandingly better than any other moment-
based approach and obviously better than original algorithms that only assume
one large system dimension. Obtaining Stieltjes transform estimators requires

513
work, but is not so diﬃcult once the relation between the l.s.d. of the observed
matrix model and the e.s.d. of the hidden parameter matrix is found. Proving
that the estimator is indeed correct requires to ensure that exact separation
of the eigenvalues in the observed matrix holds true. Nonetheless, as recalled
many times, proving exact separation, already for the simple information plus
noise model, is a very involved problem. Obtaining exact separation for more
involved models is therefore an open question, still under investigation and still
rather exclusive to pure mathematicians. To this day, for these complex models,
intellectual honesty requires to step back to less precise combinatoric moment-
based methods, which are also much easier to derive, and, as we have seen, can
be often automatically obtained by computer software.
Moment approaches are also the only access we have to even more involved
random matrix models, such as random Vandermonde or random Toeplitz
matrices. Parameter estimation can then be performed for models involving
Vandermonde matrices using the inaccurate moment approach, as no alternative
technique is available yet. Moment approaches also have the property to be
able to provide the exact moments of some functionals of small dimensional
random matrices on average, as well as exact covariance matrices of the successive
moments. Nonetheless, we also saw that functionals of random matrices as large
as 4 × 4 matrices are often very accurately deterministically approximated using
methods that assume large dimensions. The interest of tools that assume small
dimensions is therefore often very limited.
Small dimensional random matrix theory need not be discarded, though, as
exempliﬁed in Chapter 16 and Chapter 18, where important results on multi-
source detection and more generally optimum statistical inference through the
maximum entropy principle were introduced. Even if such approaches often lead
to very involved expressions, from which sometimes not much can be said, they
always provide upper-bounds on alternative approaches which are fundamental
to assess the performance of such alternative suboptimal methods. However,
small dimensional techniques are very often restricted to simple problems that are
very symmetrical in the sense that the matrices involved need to have pleasant
invariance properties. The increasing complexity of large dimensional systems
comes however in contradiction with this simplicity requirement. It is therefore
believed that small dimensional random matrix theory will leave more and more
room for the much better performing large dimensional random matrix theory
for all applications.
Finally, we mention that the most recent ﬁeld of study, for which new
results appear at an increasing pace, is that of the limiting distribution and
the large deviation of smallest and largest eigenvalues for diﬀerent types of
models, asymptotic independence within the spectrum of large dimensional
matrices, etc. These new ideas, stated in this book under the form of a
series of important results rely on powerful tools, which necessitate a lengthy
mathematical introduction to Fredholm determinants or operator theory, which
we brieﬂy provided in Chapter 19. It is believed that the time will come when

514
20. Conclusion
these tools will be made simpler and more accessible so that non-specialists can
also beneﬁt from these important results in the medium to long-term.
To conclude, we wish to insist once more that random matrix theory, which
was ten years ago still in its infancy with techniques only exploitable by
mathematicians of the ﬁeld, has now become more popular, is better understood,
and provides wireless telecommunication researchers with a large pool of useful
and accessible tools. We now enter an era where the initial results on system
performance evaluation are commonplace and where thrilling results have now
to do with statistical inference in large dimensional inverse problems involving
possibly time-varying random matrix processes.

References
T. B. Abdallah and M. Debbah. Downlink CDMA: to cell or not to cell. In
12th European Signal Processing Conference (EUSIPCO’04), pages 197–200,
Vienna, Austria, September 2004.
S. Adhikari.
A non-parametric approach for uncertainty quantiﬁcation in
elastodynamics. In Proceedings of the 47th AIAA/ASME/ASCE/AHS/ASC
Structures, Structural Dynamics and Materials Conference, 2006.
D. Aktas, M. N. Bacha, J. S. Evans, and S. V. Hanly. Scaling results on the
sum capacity of cellular networks with MIMO links. IEEE Transactions on
Information Theory, 52(7):3264–3274, 2006.
I.
F.
Akyildiz,
W.
Y.
Lee,
M.
C.
Vuran,
and
S.
Mohanty.
Next
generation/dynamic spectrum access/cognitive radio wireless networks: a
survey. Computer Networks Journal, 50(13):2127–2159, 2006.
P. Almers, E. Bonek, A. Burr, N. Czink, M. Debbah, V. Degli-Esposti,
H. Hofstetter, P. Kyosti, D. Laurenson, and G. Matz et al. Survey of channel
and radio propagation models for wireless MIMO systems. EURASIP Journal
on Wireless Communications and Networking, 2007.
G. W. Anderson, A. Guionnet, and O. Zeitouni.
Lecture notes on random
matrices, 2006. www.mathematik.uni-muenchen.de/~lerdos/SS09/Random/
randommatrix.pdf. SAMSI, Lecture Notes.
G. W. Anderson, A. Guionnet, and O. Zeitouni.
An introduction to random
matrices. Cambridge University Press, 2010. ISBN 0521194520.
T. W. Anderson. The non-central Wishart distribution and certain problems of
multivariate statistics. The Annals of Mathematical Statistics, 17(4):409–431,
1946.
T. W. Anderson. Asymptotic theory for principal component analysis. Annals
of Mathematical Statistics, 34(1):122–148, March 1963.
L. Arnold. On the asymptotic distribution of the eigenvalues of random matrices.
Journal of Mathematics and Analytic Applications, 20:262–268, 1967.
L. Arnold. On Wigner’s semi-circle law for the eigenvalues of random matrices.
Probability Theory and Related Fields, 19(3):191–198, September 1971.
L. Arnold, V. M. Gundlach, and L. Demetrius.
Evolutionary formalism for
products of positive random matrices. The Annals of Applied Probability, 4
(3):859–901, 1994.
Z. D. Bai. Circular law. The Annals of Probability, 25(1):494–529, 1997.

516
References
Z. D. Bai and J. W. Silverstein.
No eigenvalues outside the support of the
limiting spectral distribution of large dimensional sample covariance matrices.
The Annals of Probability, 26(1):316–345, January 1998.
Z. D. Bai and J. W. Silverstein.
Exact separation of eigenvalues of large
dimensional sample covariance matrices.
The Annals of Probability, 27(3):
1536–1555, 1999.
Z. D. Bai and J. W. Silverstein.
CLT of linear spectral statistics of large
dimensional sample covariance matrices. The Annals of Probability, 32(1A):
553–605, 2004.
Z. D. Bai and J. W. Silverstein. On the signal-to-interference-ratio of CDMA
systems in wireless communications. Annals of Applied Probability, 17(1):81–
101, 2007.
Z. D. Bai and J. W. Silverstein. Spectral analysis of large dimensional random
matrices. Springer Series in Statistics, New York, NY, USA, second edition,
2009.
Z. D. Bai and J. F. Yao.
Central limit theorems for eigenvalues in a
spiked population model.
Annales de lInstitut Henri Poincar´e-Probabilit´es
et Statistiques, 44(3):447–474, 2008a.
Z. D. Bai and J. F. Yao. Limit theorems for sample eigenvalues in a generalized
spiked population model. 2008b. http://arxiv.org/abs/0806.1141.
J. Baik and J. W. Silverstein. Eigenvalues of large sample covariance matrices of
spiked population models. Journal of Multivariate Analysis, 97(6):1382–1408,
2006.
J. Baik, G. Ben Arous, and S. P´ech´e. Phase transition of the largest eigenvalue
for non-null complex sample covariance matrices. The Annals of Probability,
33(5):1643–1697, 2005.
F. Benaych-Georges. Rectangular random matrices, related free entropy and free
Fisher’s information. Journal of Operator Theory, 62(2):371–419, 2009.
F. Benaych-Georges and R. Rao. The eigenvalues and eigenvectors of ﬁnite, low
rank perturbations of large random matrices. Advances in Mathematics, 227
(1):494–521, 2011. ISSN 0001-8708.
F. Benaych-Georges, A. Guionnet, and M. Maida. Fluctuations of the extreme
eigenvalues of ﬁnite rank deformations of random matrices.
2010.
http:
//arxiv.org/abs/1009.0145.
H. Bercovici and V. Pata. The law of large numbers for free identically distributed
random variables. The Annals of Probability, 24(1):453–465, 1996.
P. Bianchi, M. Debbah, and J. Najim. Asymptotic independence in the spectrum
of the Gaussian Unitary Ensemble. Electronic Communications in Probability,
15:376–395, 2010. http://arxiv.org/abs/0811.0979.
P. Bianchi, J. Najim, M. Maida, and M. Debbah. Performance of some eigen-
based hypothesis tests for collaborative sensing.
IEEE Transactions on
Information Theory, 57(4):2400–2419, 2011.
P.
Biane.
Free
probability
for
probabilists.
Quantum
Probability
Communications, 11:55–71, 2003.

References
517
E. Biglieri, G. Caire, and G. Tarico. CDMA system design through asymptotic
analysis. IEEE Transactions on Communications, 48:1882–1896, November
2000.
E. Biglieri, G. Caire, G. Taricco, and E. Viterbo. How fading aﬀects CDMA: an
asymptotic analysis with linear receivers. IEEE Journal on Selected Areas in
Communications (Wireless Series), 19(2):191–201, 2001.
P. Billingsley. Convergence of Probability Measures. John Wiley and Sons, Inc.,
Hoboken, NJ, 1968.
P. Billingsley. Probability and Measure. John Wiley and Sons, Inc., Hoboken,
NJ, third edition, 1995.
N. Bonneau, E. Altman, M. Debbah, and G. Caire.
When to synchronize
in uplink CDMA.
In Proceedings of IEEE International Symposium on
Information Theory (ISIT’05), pages 337–341, 2005.
N. Bonneau, M. Debbah, and E. Altman.
Wardrop equilibrium in CDMA
networks.
In Workshop on Resource Allocation in Wireless Networks,
Limassol, Cyprus, 2007.
N. Bonneau, M. Debbah, E. Altman, and A. Hjørungnes. Non-atomic games for
multi-user systems. IEEE Journal on Selected Areas in Communications, 26
(7):1047–1058, 2008.
S. Borade, L. Zheng, and R. Gallager. Amplify-and-forward in wireless relay
networks: rate, diversity, and network size. IEEE Transactions on Information
Theory, 53(10):3302–3318, 2007.
C. Bordenave. Eigenvalues of euclidean random matrices. Random Structures
and Algorithms, 33(4):515–532, 2008.
C. Bordenave, D. McDonald, and A. Prouti`ere. Random multi-access algorithms,
a mean ﬁeld analysis. In Proceedings of IEEE Annual Allerton Conference on
Communication, Control, and Computing (Allerton’05), Allerton, IL, USA,
2005.
G. L. Bretthorst. Bayesian spectrum analysis and parameter estimation. PhD
thesis, Washington University, St. Louis, 1987.
L. Brillouin.
Science and Information Theory.
Academic Press, New York,
second edition, 1962.
S. Buchegger and J. Y. Le Boudec.
Self-policing mobile ad-hoc networks by
reputation systems. IEEE Communication Magazine, 43(7):101–107, 2005.
D. Cabric, S. M. Mishra, and R. W. Brodersen.
Implementation issues in
spectrum sensing for cognitive radios.
In Proceedings of IEEE Conference
Record of the Asilomar Conference on Signals, Systems, and Computers
(ASILOMAR’04), pages 772–776, Paciﬁc Grove, CA, USA, 2004.
D. Cabric, A. Tkachenko, and R. W. Brodersen. Spectrum sensing measurements
of pilot, energy and collaborative detection. In IEEE Military Communications
Conference, October 2006.
G. Caire and S. Shamai.
On the achievable throughput of a multiantenna
Gaussian broadcast channel. IEEE Transactions on Information Theory, 49
(7):1691–1706, 2003.

518
References
D. Calin, H. Claussen, and H. Uzunalioglu. On femto deployment architectures
and macrocell oﬄoading beneﬁts in joint macro-femto deployments.
IEEE
Transactions on Communications, 48(1):26–32, January 2010.
L. S. Cardoso, M. Debbah, P. Bianchi, and J. Najim. Cooperative spectrum
sensing using random matrix theory.
In IEEE Pervasive Computing
(ISWPC’08), pages 334–338, Santorini, Greece, May 2008.
A. Caticha. Maximum entropy, ﬂuctuations and priors. Maximum Entropy and
Bayesian Methods in Science and Engineering, 568:94–106, 2001.
H. Chandra.
Diﬀerential operators on a semi-simple Lie algebra.
American
Journal of Mathematics, 79:87–120, 1957.
V. Chandrasekhar, M. Kountouris, and J. G. Andrews.
Coverage in multi-
antenna two-tier networks. IEEE Transactions on Wireless Communications,
8(10):5314–5327, 2009.
J. M. Chaufray, W. Hachem, and P. Loubaton. Asymptotic analysis of optimum
and sub-optimum CDMA downlink MMSE receivers. IEEE Transactions on
Information Theory, 50(11):2620–2638, 2004.
S. S. Christensen, R. Agarwal, E. D. Carvalho, and J. M. Cioﬃ. Weighted sum-
rate maximization using weighted MMSE for MIMO-BC beamforming design,
Part I. IEEE Transactions on Wireless Communications, 7(12):4792–4799,
2008.
C. N. Chuah, D. N. C. Tse, J. M. Kahn, and R. A. Valenzuela. Capacity scaling
in MIMO wireless systems under correlated fading. IEEE Transactions on
Information Theory, 48(3):637–650, March 2002.
P. Chung, J. B¨ohme, C. Mecklenbra¨uker, and A. Hero. Detection of the number
of signals using the Benjamini-Hochberg procedure. IEEE Transactions on
Signal Processing, 55(6):2497–2508, 2007.
H. Claussen, L. T. Ho, and L. G. Samuel. An overview of the femtocell concept.
Bell Labs Technical Journal, 13(1):221–245, May 2008.
M. H. M. Costa. Writing on dirty paper. IEEE Transactions on Information
Theory, 29(3):439–441, 1983.
L. Cottatellucci and M. Debbah. The eﬀect of line of sight on the asymptotic
capacity of MIMO systems. In Proceedings of IEEE International Symposium
on Information Theory (ISIT’04), page 542, Chicago, USA, July 2004a.
L. Cottatellucci and M. Debbah. On the capacity of MIMO Rice channels. In
Proceedings of IEEE Annual Allerton Conference on Communication, Control,
and Computing (Allerton’04), Allerton, IL, USA, October 2004b.
L. Cottatellucci and R. M¨uller. Asymptotic design and analysis of multistage
detectors with unequal powers. In Proceedings of IEEE Information Theory
Workshop (ITW’02), pages 167–170, Bangalore, India, 2002.
L. Cottatellucci and R. M¨uller. A systematic approach to multistage detectors
in multipath fading channels. IEEE Transactions on Information Theory, 51
(9):3146–3158, 2005.
L. Cottatellucci, R. M¨uller, and M. Debbah. Asymptotic design and analysis
of linear detectors for asynchronous CDMA systems. In Proceedings of IEEE

References
519
International Symposium on Information Theory (ISIT’04), Chicago, IL, USA,
2004.
L. Cottatellucci, R. M¨uller, and M. Debbah.
Asynchronous CDMA systems
with random spreading – Part I: Fundamental limits. IEEE Transactions on
Information Theory, 56(4):1477–1497, 2010a.
L. Cottatellucci, R. M¨uller, and M. Debbah.
Asynchronous CDMA systems
with random spreading – Part II: Design criteria.
IEEE Transactions on
Information Theory, 56(4):1498–1520, 2010b.
R. Couillet and M. Debbah.
Free deconvolution for OFDM multicell SNR
detection.
In Proceedings of IEEE International Symposium on Personal,
Indoor and Mobile Radio Communications (PIMRC’08), Cannes, France,
2008.
R. Couillet and M. Debbah.
Uplink capacity of self-organizing clustered
orthogonal CDMA networks in ﬂat fading channels. In Proceedings of IEEE
Information Theory Workshop, Taormina, Sicily, 2009.
R. Couillet and M. Debbah. A Bayesian framework for collaborative multi-source
signal detection. IEEE Transactions on Signal Processing, 58(10):5186–5195,
October 2010a.
R. Couillet and M. Debbah. Information theoretic approach to synchronization:
the OFDM carrier frequency oﬀset example. In Sixth Advanced International
Conference on Telecommunications (AICT), Barcelona, Spain, 2010b.
R. Couillet and M. Guillaud. Performance of statistical inference methods for
the energy estimation of multiple sources. In Proceedings of IEEE Workshop
on Statistical Signal Processing (SSP’11), Nice, France, 2011. To appear.
R. Couillet and W. Hachem. Local failure detection and diagnosis in large sensor
networks. IEEE Transactions on Information Theory, 2011. Submitted for
publication.
R. Couillet, S. Wagner, M. Debbah, and A. Silva.
The space frontier:
physical limits of multiple antenna information transfer.
In Workshop on
Interdisciplinary Systems Approach in Performance Evaluation and Design of
Computer and Communication Systems (Inter-Perf’08), Athens, Greece, 2008.
R. Couillet, A. Ancora, and M. Debbah.
Bayesian foundations of channel
estimation for smart radios. Advances in Electronics and Telecommunications,
1(1):41–49, 2010.
R. Couillet, M. Debbah, and J. W. Silverstein. A deterministic equivalent for
the analysis of correlated MIMO multiple access channels. IEEE Transactions
on Information Theory, 57(6):3493–3514, June 2011a.
R. Couillet, J. Hoydis, and M. Debbah. Deterministic equivalents for the analysis
of unitary precoded systems.
IEEE Transactions on Information Theory,
2011b. ttp://arxiv.org/abs/1011.3717. Submitted for publication.
R. Couillet, J. W. Silverstein, Z. D. Bai, and M. Debbah. Eigen-inference for
energy estimation of multiple sources.
IEEE Transactions on Information
Theory, 57(4):2420–2439, 2011c.

520
References
R. T. Cox. Probability, frequency and reasonable expectation. American Journal
of Physics, 14(1):1–13, 1946.
A. D. Dabbagh and D. J. Love.
Multiple antenna MMSE based downlink
precoding with quantized feedback or channel mismatch. IEEE Transactions
on Communications, 56(11):1859–1868, 2008.
R. de Lacerda Neto, A. Menouni Hayar, M. Debbah, and B. H. Fleury.
A
maximum entropy approach to ultra-wideband channel modelling. Proceedings
of the 31st IEEE International Conference on Acoustics, Speech, and Signal
Processing, 2006.
M. Debbah and R. M¨uller. Impact of the power of the steering directions on the
asymptotic capacity of MIMO channels. In Proceedings of IEEE International
Symposium on Signal Processing and Information Technology (ISSPIT’03),
Darmstadt, Germany, December 2003.
M. Debbah and R. M¨uller.
MIMO channel modelling and the principle of
maximum entropy. IEEE Transactions on Information Theory, 51(5):1667–
1690, 2005.
M. Debbah and R. M¨uller.
Capacity complying MIMO channel models.
In Proceedings of IEEE Conference Record of the Asilomar Conference on
Signals, Systems, and Computers (ASILOMAR’03), Paciﬁc Grove, CA, USA,
November 2003.
M. Debbah, W. Hachem, P. Loubaton, and M. de Courville. MMSE analysis
of certain large isometric random precoded systems. IEEE Transactions on
Information Theory, 49(5):1293–1311, May 2003a.
M. Debbah, P. Loubaton, and M. de Courville. The spectral eﬃciency of linear
precoders. In Proceedings of IEEE Information Theory Workshop (ITW’03),
pages 90–93, Paris, France, March 2003b.
P. Deift.
Orthogonal Polynomials and Random Matrices: a Riemann-Hilbert
Approach. New York University Courant Institute of Mathematical Sciences,
New York, NY, USA, 2000.
A. Dembo and O. Zeitouni.
Large Deviations Techniques and applications.
Springer Verlag, 2009.
P. Ding, D. J. Love, and M. D. Zoltowski. Multiple antenna broadcast channels
with shape feedback and limited feedback, Part I.
IEEE Transactions on
Signal Processing, 55(7):3417–3428, 2007.
B. Dozier and J. W. Silverstein. On the empirical distribution of eigenvalues
of large dimensional information plus noise-type matrices.
Journal of
Multivariate Analysis, 98(4):678–694, 2007a.
B. Dozier and J. W. Silverstein. Analysis of the limiting spectral distribution
of large dimensional information-plus-noise type matrices.
Journal of
Multivariate Analysis, 98(6):1099–1122, 2007b.
J. Dumont, W. Hachem, S. Lasaulce, P. Loubaton, and J. Najim.
On
the capacity achieving covariance matrix for Rician MIMO channels: an
asymptotic approach. IEEE Transactions on Information Theory, 56(3):1048–
1069, 2010.

References
521
F. Dupuy and P. Loubaton. Mutual information of frequency selective MIMO
systems: an asymptotic approach, 2009. http://www-syscom.univ-mlv.fr/
~fdupuy/publications.php.
F. Dupuy and P. Loubaton. On the capacity achieving covariance matrix for
frequency selective MIMO channels using the asymptotic approach.
IEEE
Transactions on Information Theory, 2010. http://arxiv.org/abs/1001.
3102. To appear.
F. J. Dyson. Statistical theory of the energy levels of complex systems, Part II.
Journal of Mathematical Physics, 3:157–165, January 1962a.
F. J. Dyson. A Brownian-motion model for the eigenvalues of a random matrix.
Journal of Mathematical Physics, 3:1191–1198, 1962b.
S. Enserink and D. Cochran. A cyclostationary feature detector. In Proceedings
of IEEE Conference Record of the Asilomar Conference on Signals, Systems,
and Computers (ASILOMAR’94), pages 806–810, Paciﬁc Grove, CA, USA,
1994.
J. Evans and D. N. C. Tse. Large system performance of linear multiuser receivers
in multipath fading channels. IEEE Transactions on Information Theory, 46
(6):2059–2078, 2000.
K. Fan. Maximum properties and inequalities for the eigenvalues of completely
continuous operators. Proceedings of the National Academy of Sciences of the
United States of America, 37(11):760–766, 1951.
J. Faraut. Random matrices and orthogonal polynomials. Lecture Notes, CIMPA
School of Merida, 2006. www.math.jussieu.fr/~faraut/Merida.Notes.pdf.
N. Fawaz and M. M´edard. On the non-coherent wideband multipath fading relay
channel.
In Proceedings of IEEE International Symposium on Information
Theory (ISIT’10), pages 679–683, Austin, Texas, USA, 2010.
N. Fawaz, K. Zariﬁ, M. Debbah, and D. Gesbert.
Asymptotic capacity and
optimal precoding in MIMO multi-hop relay networks. IEEE Transactions on
Information Theory, 57(4):2050–2069, 2011. ISSN 0018-9448.
O. N. Feldheim and S. Sodin. A universality result for the smallest eigenvalues
of certain sample covariance matrices. Geometric And Functional Analysis, 20
(1):88–123, 2010. ISSN 1016-443X.
R. A. Fisher. The sampling distribution of some statistics obtained from non-
linear equations. The Annals of Eugenics, 9:238–249, 1939.
S. V. Fomin and I. M. Gelfand. Calculus of Variations. Prentice Hall, 2000.
G. J. Foschini and M. J. Gans.
On limits of wireless communications in
a fading environment when using multiple antennas.
Wireless Personal
Communications, 6(3):311–335, March 1998.
M. Franceschetti, S. Marano, and F. Palmieri.
The role of entropy in wave
propagation. In Proceedings of IEEE International Symposium on Information
Theory (ISIT’03), Yokohama, Japan, July 2003.
Y. V. Fyodorov. Introduction to the random matrix theory: Gaussian unitary
ensemble and beyond.
Recent Perspectives in Random Matrix Theory and
Number Theory, 322:31–78, 2005.

522
References
W. A. Gardner. Exploitation of spectral redundancy in cyclostationary signals.
IEEE Signal Processing Magazine, 8(2):14–36, 1991.
S. Geman. A limit theorem for the norm of random matrices. The Annals of
Probability, 8(2):252–261, 1980.
A. Ghasemi and E. S. Sousa. Collaborative spectrum sensing for opportunistic
access in fading environments.
In IEEE Proceedings of the International
Symposium on Dynamic Spectrum Access Networks, pages 131–136, 2005.
A. Ghasemi and E. S. Sousa.
Spectrum sensing in cognitive radio networks:
the cooperation-processing tradeoﬀ.
Wireless Communications and Mobile
Computing, 7(9):1049–1060, 2007.
V.
L.
Girko.
Ten
years
of
general
statistical
analysis.
www.
general-statistical-analysis.girko.freewebspace.com/chapter14.
pdf.
V. L. Girko.
Theory of Random Determinants.
Kluwer, Kluwer Academic
Publishers, Dordrecht, The Netherlands, 1990.
M. A. Girshick. On the sampling theory of roots of determinantal equations.
The Annals of Math. Statistics, 10:203–204, 1939.
J. Glimm and A. Jaﬀe. Quantum Physics. Springer, New York, NY, USA, 1981.
A. Goldsmith, S. A. Jafar, N. Jindal, and S. Vishwanath. Capacity limits of
MIMO channels. IEEE Journal on Selected Areas in Communications, 21(5):
684–702, 2003.
I. S. Gradshteyn and I. M. Ryzhik.
Table of Integrals, Series and Products.
Academic Press, sixth edition, 2000.
A. J. Grant and P. D. Alexander. Random sequence multisets for synchronous
code-division multiple-access channels.
IEEE Transactions on Information
Theory, 44(7):2832–2836, November 1998.
R. M. Gray. Toeplitz and circulant matrices: a review. Foundations and Trends
in Communications and Information Theory, 2(3), 2006.
D. Gregoratti and X. Mestre. Random DS/CDMA for the amplify and forward
relay channel. IEEE Transactions on Wireless Communications, 8(2):1017–
1027, 2009.
D. Gregoratti, W. Hachem, and X. Mestre.
Randomized isometric linear-
dispersion space-time block coding for the DF relay channel.
IEEE
Transactions on Signal Processing, 2010. Submitted for publication.
M. Guillaud, M. Debbah, and A. L. Moustakas.
Maximum entropy MIMO
wireless channel models, 2006. http://arxiv.org/abs/cs.IT/0612101.
M. Guillaud, M. Debbah, and A. L. Moustakas. Modeling the multiple-antenna
wireless channel using maximum entropy methods. In International Workshop
on Bayesian Inference and Maximum Entropy Methods in Science and
Engineering (MaxEnt’07), pages 435–442, Saratoga Springs, NY, November
2007.
A. Guionnet.
Large random matrices: lectures on macroscopic asymptotics.
´Ecole d’´Et´e de Probabilit´es de Saint-Flour XXXVI-2006, 2006. www.umpa.
ens-lyon.fr/~aguionne/cours.pdf.

References
523
D. Guo and S. Verd´u.
Multiuser detection and statistical mechanics.
Communications, Information and Network Security, pages 229–277, 2002.
D. Guo, S. Verd´u, and L. K. Rasmussen.
Asymptotic normality of linear
multiuser receiver outputs.
IEEE Transactions on Information Theory, 48
(12):3080–3095, December 2002.
U. Haagerup, H. Schultz, and S. Thorbjørnsen. A random matrix approach to
the lack of projections in Cred*(F2). Advances in Mathematics, 204(1):1–83,
2006.
W. Hachem. Simple polynomial MMSE receivers for CDMA transmissions on
frequency selective channels. IEEE Transactions on Information Theory, pages
164–172, January 2004.
W. Hachem. An expression for
R
log(t/σ2 + 1)µ ⊞˜µ(dt), 2008. unpublished.
W. Hachem, P. Loubaton, and J. Najim. Deterministic equivalents for certain
functionals of large random matrices. Annals of Applied Probability, 17(3):
875–930, 2007.
W. Hachem, O. Khorunzhy, P. Loubaton, J. Najim, and L. A. Pastur. A new
approach for capacity analysis of large dimensional multi-antenna channels.
IEEE Transactions on Information Theory, 54(9), 2008a.
W. Hachem, P. Loubaton, and J. Najim.
A CLT for information theoretic
statistics of Gram random matrices with a given variance proﬁle. The Annals
of Probability, 18(6):2071–2130, December 2008b.
W. Hachem, P. Loubaton, X. Mestre, J. Najim, and P. Vallet.
A subspace
estimator of ﬁnite rank perturbations of large random matrices. Journal on
Multivariate Analysis, 2011. Submitted for publication.
L. Hanlen and A. Grant.
Capacity analysis of correlated MIMO channels.
In Proceedings of IEEE International Symposium on Information Theory
(ISIT’03), Yokohama, Japan, July 2003.
S. V. Hanly and D. N. C. Tse.
Resource pooling and eﬀective bandwidths
in CDMA networks with multiuser receivers and spatial diversity.
IEEE
Transactions on Information Theory, pages 1328–1351, May 2001.
B. Hassibi and B. M. Hochwald. How Much Training is Needed in Multiple-
Antenna Wireless Links. IEEE Transactions on Information Theory, 49(4):
951–963, April 2003.
A. Haurie and P. Marcotte. On the relationship between Nash–Cournot and
Wardrop equilibria. Networks, 15(1):295–308, 1985.
F. Hiai and D. Petz. The Semicircle Law, Free Random Variables and Entropy
- Mathematical Surveys and Monographs No. 77.
American Mathematical
Society, Providence, RI, USA, 2006.
B. Hochwald and S. Vishwanath.
Space-time multiple access: Linear growth
in the sum rate.
Proceedings of IEEE Annual Allerton Conference on
Communication, Control, and Computing (Allerton’02), 2002.
B. M. Hochwald, T. L. Marzetta, and V. Tarokh.
Multiple-antenna channel
hardening and its implications for rate feedback and scheduling.
IEEE
Transactions on Information Theory, 50(9):1893–1909, 2004.

524
References
M. L. Honig and W. Xiao.
Performance of reduced-rank linear interference
suppression.
IEEE Transactions on Information Theory, 47(5):1928–1946,
May 2001.
R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press,
1985.
R. A. Horn and C. R. Johnson. Topics in Matrix Analysis. Cambridge University
Press, 1991.
J. Hoydis, M. Kobayashi, and M. Debbah. Asymptotic performance of linear
receivers in network MIMO. In Proceedings of IEEE Conference Record of the
Asilomar Conference on Signals, Systems, and Computers (ASILOMAR’10),
Paciﬁc Grove, CA, USA, November 2010.
J. Hoydis, R. Couillet, and M. Debbah. Random beamforming over correlated
fading channels. IEEE Transactions on Information Theory, 2011a. Submitted
for publication.
J. Hoydis, R. Couillet, and M. Debbah.
Asymptotic analysis of double-
scattering channels. In Proceedings of IEEE Conference Record of the Asilomar
Conference on Signals, Systems, and Computers (ASILOMAR’11), Paciﬁc
Grove, CA, USA, 2011b.
J. Hoydis, M. Debbah, and M. Kobayashi. Asymptotic moments for interference
mitigation in correlated fading channels. In Proceedings of IEEE International
Symposium on Information Theory (ISIT’11), Saint Petersburg, Russia,
August 2011c. http://arxiv.org/abs/1104.4911.
J. Hoydis, M. Kobayashi, and M. Debbah. Optimal channel training in uplink
network mimo systems. IEEE Transactions on Signal Processing, 59(6), June
2011d.
M. Hoyhtya, A. Hekkala, and A. Mammela. Spectrum awareness: techniques
and challenges for active spectrum sensing. Springer Cognitive Networks, 3:
353–372, April 2007.
P. L. Hsu. On the distribution of roots of certain determinantal equations. The
Annals of Eugenics, 9:250–258, 1939.
H. Huh, G. Caire, S. H. Moon, and I. Lee. Multi-cell MIMO downlink with
fairness criteria: the large-system limit. In Proceedings of IEEE International
Symposium on Information Theory (ISIT’10), pages 2058–2062, June 2010.
Y. Hur, J. Park, W. Woo, K. Lim, C. H. Lee, H. S. Kim, and J. Laskar. A
wideband analog multi-resolution spectrum sensing (MRSS) technique for
cognitive radio (CR) systems. In IEEE International Symposium on Circuits
and Systems (ISCAS’06), page 4, Island of Kos, Greece, 2006.
A. A. Hutter, E. Carvalho, and J. M. Cioﬃ. On the impact of channel estimation
for multiple antenna diversity reception in mobile OFDM systems. Proceedings
of IEEE Conference Record of the Asilomar Conference on Signals, Systems,
and Computers (Asimolar’00), 2, 2000.
C. Hwang. A brief survey on the spectral radius and the spectral distribution of
large dimensional random matrices with i.i.d. entries. Random Matrices and
Their Applications, 50:145–152, 1986.

References
525
C. Hwang. Eigenvalue distribution of correlation matrix in asynchronous CDMA
with inﬁnite observation window width. In Proceedings of IEEE International
Symposium on Information Theory (ISIT’07), Nice, France, June 2007.
C. Itzykson and J. B. Zuber. Quantum Field Theory. Dover Publications, 2006.
A. T. James.
Distributions of matrix variates and latent roots derived from
normal samples. The Annals of Mathematical Statistics, 35(2):475–501, 1964.
E. T. Jaynes. Information theory and statistical mechanics, Part I. Physical
Review, 106(2):620–630, 1957a.
E. T. Jaynes. Information theory and statistical mechanics, Part II. Physical
Review, 108(2):171–190, 1957b.
E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University
Press, 2003.
H. Jeﬀreys. An invariant form for the prior probability in estimation problems.
Proceedings of the Royal Society of London. Series A, Mathematical and
Physical Sciences, 186(1007):453–461, 1946.
S. Jin, M. R. McKay, X. Gao, and I. B. Collings.
MIMO multichannel
beamforming: SER and outage using new eigenvalue distributions of complex
noncentral Wishart matrices. IEEE Transactions on Communications, 56(3):
424–434, 2008. http://arxiv.org/abs/0611007.
N. Jindal.
MIMO broadcast channels with ﬁnite-rate feedback.
IEEE
Transactions on Information Theory, 52(11):5045–5060, 2006.
M. Joham, K. Kusume, M. H. Gzara, W. Utschick, and J. A. Nossek. Transmit
Wiener ﬁlter for the downlink of TDD DS-CDMA systems. Proceedings of
ISSSTA 2002, 1:9–13, 2002.
K. Johansson. Shape ﬂuctuations and random matrices. Communications of
Mathematical Physics, 209:437–476, 2000.
I. M. Johnstone.
On the distribution of the largest eigenvalue in principal
components analysis. Annals of Statistics, 99(2):295–327, 2001.
I. M. Johnstone. High dimensional statistical inference and random matrices. In
International Congress of Mathematicians I, pages 307–333, Z¨urich, Germany,
2006. European Mathematical Society.
F. Kaltenberger, M. Kountouris, D. Gesbert, and R. Knopp.
On the trade-
oﬀbetween feedback and capacity in measured MU-MIMO channels. IEEE
Transactions on Wireless Communications, 8(9):4866–4875, 2009.
M. A. Kamath, B. L Hughes, and Y. Xinying.
Gaussian approximations for
the capacity of MIMO Rayleigh fading channels.
In Proceedings of IEEE
Conference Record of the Asilomar Conference on Signals, Systems, and
Computers (ASILOMAR’02), pages 614–618, Paciﬁc Grove, CA, USA, 2002.
A. Kammoun, R. Couillet, J. Najim, and M. Debbah. Performance of capacity
inference methods under colored interference, 2011. Submitted for publication.
J. N. Kapur. Maximum Entropy Models in Science and Engineering. John Wiley
and Sons, Inc., New York, 1989.
N. El Karoui. Tracy-Widom limit for the largest eigenvalue of a large class of
complex sample covariance matrices. The Annals of Probability, 35(2):663–714,

526
References
2007.
N. El Karoui. Spectrum estimation for large dimensional covariance matrices
using random matrix theory. Annals of Statistics, 36(6):2757–2790, December
2008.
S. M. Kay. Fundamentals of Statistical Signal Processing: Estimation Theory.
Prentice-Hall, Englewood Cliﬀs, NJ, USA, 1993.
A. M. Khorunzhy, B. A. Khoruzhenko, and L. A. Pastur.
On asymptotic
properties of large random matrices with independent entries.
Journal of
Mathematical Physics, 37(10):5033–5061, 1996.
H. Kim and K. G. Shin. In-band spectrum sensing in cognitive radio networks:
energy detection or feature detection? In ACM Internatioanl Conference on
Mobile Computing and Networking, pages 14–25, San Francisco, CA, USA,
September 2008.
P. Koev. Random matrix statistics toolbox. http://math.mit.edu/~plamen/
software/rmsref.html.
V. I. Kostylev.
Energy detection of a signal with random amplitude.
In
Proceedings of IEEE International Conference on Communications (ICC’02),
pages 1606–1610, New York, NY, USA, 2002.
L. Laloux, P. Cizeau, M. Potters, and J. P. Bouchaud. Random matrix theory
and ﬁnancial correlations. International Journal of Theoretical and Applied
Finance, 3(3):391–397, July 2000.
J. Y. Le Boudec, D. McDonald, and J. Mundinger.
A generic mean ﬁeld
convergence result for systems of interacting objects.
In International
Conference on the Quantitative Evaluation of Systems (QEST’07), Budapest,
Hungary, 2007.
O. Levˆeque and I. E. Telatar.
Information-theoretic upper bounds on the
capacity of large extended ad hoc wireless networks. IEEE Transactions on
Information Theory, 51(3):858–865, March 2005.
N. Levy and S. Shamai. Clustered local decoding for Wyner-type cellular models.
In Proceedings of IEEE Information Theory and Applications Workshop
(ITA’09), pages 318–322, San Diego, CA, USA, 2009.
L. Li, A. M. Tulino, and S. Verd´u. Design of reduced-rank MMSE multiuser
detectors using random matrix methods. IEEE Transactions on Information
Theory, 50(6):986–1008, June 2004.
P. Loubaton and W. Hachem. Asymptotic analysis of reduced rank wiener ﬁlters.
In Proceedings of IEEE Information Theory Workshop (ITW’03), pages 328–
331, Paris, France, 2003.
S. Loyka.
Channel capacity of MIMO architecture using the exponential
correlation matrix. IEEE Communication Letters, 5(9):1350–1359, September
2001.
A. Lytova and L. A. Pastur.
Central Limit Theorem for linear eigenvalue
statistics of random matrices with independent entries.
The Annals of
Probability, 37(5):1778–1840, 2009.

References
527
U. Madhow and M. L. Honig. MMSE interference suppression for direct-sequence
spread-spectrum CDMA.
IEEE Transactions on Communications, 42(12):
3178–3188, December 1994.
A. Mantravadi and V. V. Veeravalli.
Mmse detection in asynchronous cdma
systems: An equivalence result. IEEE Transactions on Information Theory,
48(12):3128–3137, December 2002.
I. Maric and R. D. Yates.
Bandwidth and power allocation for cooperative
strategies in Gaussian relay networks.
IEEE Transactions on Information
Theory, 56(4):1880–1889, 2010.
I. Maric, A. Goldsmith, and M. M´edard. Analog network coding in the high-SNR
regime. In IEEE Wireless Network Coding Conference (WiNC’10), pages 1–6,
Boston, MA, USA, 2010.
C. Martin and B. Ottersten. Asymptotic eigenvalue distributions and capacity
for MIMO channels under correlated fading. IEEE Transactions on Wireless
Communications, 3(4):1350–1359, July 2004.
V. A. Mar˘cenko and L. A. Pastur. Distributions of eigenvalues for some sets of
random matrices. Math USSR-Sbornik, 1(4):457–483, April 1967.
A. Masucci, Ø. Ryan, S. Yang, and M. Debbah. Finite dimensional statistical
inference. IEEE Transactions on Information Theory, 57(4):2457–2473, 2011.
ISSN 0018-9448.
M. L. McCloud and L. L. Scharf.
A new subspace identiﬁcation algorithm
for high-resolution DOA estimation.
IEEE Transactions on Antennas and
Propagation, 50(10):1382–1390, 2002.
M. L. Mehta. Random Matrices. ELSEVIER, San Diego, CA, USA, ﬁrst edition,
2004.
F. Meshkati, H. V. Poor, S. C. Schwartz, and N. B. Mandayam.
An
energy-eﬃcient approach to power control and receiver design in wireless
data networks.
IEEE Transactions on Communications, 53(11):1885–1894,
November 2005.
X. Mestre. On the asymptotic behavior of the sample estimates of eigenvalues
and eigenvectors of covariance matrices.
IEEE Transactions on Signal
Processing, 56(11):5353–5368, November 2008a.
X. Mestre. Improved estimation of eigenvalues of covariance matrices and their
associated subspaces using their sample estimates.
IEEE Transactions on
Information Theory, 54(11):5113–5129, November 2008b.
X. Mestre and M. Lagunas. Modiﬁed subspace algorithms for DoA estimation
with large arrays. IEEE Transactions on Signal Processing, 56(2):598–614,
February 2008.
X. Mestre, J. R. Fonollosa, and A. Pag`es-Zamora. Capacity of MIMO channels:
asymptotic evaluation under correlated fading.
IEEE Journal on Selected
Areas in Communications, 21(5):829–838, June 2003.
X. Mestre, P. Vallet, W. Hachem, and P. Loubaton.
Asymptotic analysis of
a consistent subspace estimator for observations of increasing dimension. In
Proceedings of IEEE Workshop on Statistical Signal Processing (SSP’11), Nice,

528
References
France, 2011.
M. M´ezard, G. Parisi, and M. Virasoro. Spin Glass Theory and Beyond. World
scientiﬁc Singapore, 1987. ISBN 9971501155.
S. M. Mishra, A. Sahai, and R. Brodersen. Cooperative sensing among cognitive
radios. In Proceedings of IEEE International Conference on Communications
(ICC’06), pages 1658–1663, Istanbul, Turkey, 2006.
J. Mitola III and G. Q. Maguire Jr. Cognitive radio: making software radios
more personal. IEEE Personal Communication Magazine, 6(4):13–18, 1999.
S. Moshavi, E. G. Kanterakis, and D. L. Schilling. Multistage linear receivers for
DS-CDMA systems. International Journal of Wireless Information Networks,
3(1):1–17, 1996.
A. L. Moustakas and S. H. Simon.
Optimizing multiple-input single output
(MISO) communication with general Gaussian channels: nontrivial covariance
and non-zero mean. IEEE Transactions on Information Theory, 49(10):2770–
2780, October 2003.
A. L. Moustakas and S. H. Simon.
Random matrix theory of multi-antenna
communications: the Rician channel. Journal of Physics A: Mathematical and
General, 38(49):10859–10872, November 2005.
A. L. Moustakas and S. H. Simon. On the outage capacity of correlated multiple-
path MIMO channels.
IEEE Transactions on Information Theory, 53(11):
3887–3903, 2007.
A. L. Moustakas, H. U. Baranger, L. Balents, A. M. Sengupta, and S. H. Simon.
Communication through a diﬀusive medium: Coherence and capacity. Science,
287:287–290, 2000.
R. J. Muirhead. Aspects of Multivariate Statistical Theory. Wiley Online Library,
1982.
R. M¨uller. Multiuser receivers for randomly spread signals: fundamental limits
with and without decision-feedback.
IEEE Transactions on Information
Theory, 47(1):268–283, January 2001.
R. M¨uller. A random matrix model of communication via antenna arrays. IEEE
Transactions on Information Theory, 48(9):2495–2506, September 2002.
R. M¨uller. On the asymptotic eigenvalue distribution of concatenated vector-
valued fading channels.
IEEE Transactions on Information Theory, 48(7):
2086–2091, July 2002.
R. M¨uller. Channel capacity and minimum probability of error in large dual
antenna array systems with binary modulation. IEEE Transactions on Signal
Processing, 51(11):2821–2828, 2003.
R. M¨uller and S. Verd´u.
Design and analysis of low-complexity interference
mitigation on vector channels.
IEEE Journal on Selected Areas in
Communications, 19(8):1429–1441, August 2001.
A. Nica and R. Speicher.
On the multiplication of free N-tuples of
noncommutative random variables. American Journal of Mathematics, 118:
799–837, 1996.

References
529
H. Nishimori. Statistical Physics of Spin Glasses and Information Processing:
An Introduction. Clarendon Press, Gloucestershire, UK, July 2001.
C. Oestges, B. Clerckx, M. Guillaud, and M. Debbah. Dual-polarized wireless
communications: from propagation models to system performance evaluation.
IEEE Transactions on Wireless Communications, 7(10):4019–4031, October
2008.
H. Ozcelik, M. Herdin, W. Weichselberger, J. Wallace, and E. Bonek. Deﬁciencies
of Kronecker MIMO radio channel model. Electronics Letters, 39(16):1209–
1210, 2003.
L. A. Pastur. A simple approach to global regime of random matrix theory. In
Mathematical Results in Statistical Mechanics, pages 429–454. World Scientiﬁc
Publishing, 1999.
D. Paul. Asymptotics of sample eigenstructure for a large dimensional spiked
covariance model. Statistica Sinica, 17(4):1617, 2007.
M. J. M. Peacock, I. B. Collings, and M. L. Honig. Eigenvalue distributions
of sums and products of large random matrices via incremental matrix
expansions.
IEEE Transactions on Information Theory, 54(5):2123–2138,
2008.
C. B. Peel, B. M. Hochwald, and A. L. Swindlehurst. A vector-perturbation
technique for near-capacity multiantenna multiuser communication, Part I:
channel inversion and regularization. IEEE Transactions on Communications,
53(1):195–202, 2005.
D. Petz and J. R´eﬀy. On asymptotics of large Haar distributed unitary matrices.
Periodica Mathematica Hungarica, 49(1):103–117, September 2004.
V. Plerous, P. Gopikrishnan, B. Rosenow, L. Amaral, T. Guhr, and H. Stanley.
Random matrix approach to cross correlations in ﬁnancial data. Phys. Rev.
E, 65(6), June 2002.
T. S. Pollock, T. D. Abhayapala, and R. A. Kennedy.
Antenna saturation
eﬀects on dense array MIMO capacity. In Proceedings of IEEE International
Conference on Communications (ICC’03), pages 2301–2305, Anchorage,
Alaska, 2003.
H. V. Poor. An Introduction to Signal Detection and Estimation. Springer, 1994.
H. V. Poor and S. Verd´u. Probability of error in MMSE multiuser detection.
IEEE Transactions on Information Theory, 43(3):858–871, 1997.
N. R. Rao and A. Edelman.
The polynomial method for random matrices.
Foundations of Computational Mathematics, 8(6):649–702, December 2008.
N. R. Rao, J. A. Mingo, R. Speicher, and A. Edelman. Statistical eigen-inference
from large Wishart matrices. Annals of Statistics, 36(6):2850–2885, December
2008.
P. Rapajic and D. Popescu. Information capacity of random signature multiple-
input multiple output channel. IEEE Transactions on Communications, 48
(8):1245–1248, August 2000.
T. Ratnarajah and R. Vaillancourt.
Complex singular Wishart matrices and
applications. Computers and Mathematics with Applications, 50(3–4):399–411,

530
References
2005.
T. Ratnarajah, R. Vaillancourt, and M. Alvo.
Eigenvalues and condition
numbers of complex random matrices.
SIAM Journal on Matrix Analysis
and Applications, 26(2):441–456, 2005a.
T. Ratnarajah, R. Vaillancourt, and M. Alvo. Complex random matrices and
Rician channel capacity. Problems of Information Transmission, 41(1):1–22,
2005b.
S. N. Roy.
p-statistics or some generalizations in the analysis of variance
appropriate to multi-variate problems.
Sankhya: The Indian Journal of
Statistics, 4:381–396, 1939.
W. Rudin.
Real and Complex Analysis.
McGraw-Hill Series in Higher
Mathematics, third edition, May 1986.
Ø. Ryan. Tools for convolution with ﬁnite Gaussian matrices, 2009a. http:
//folk.uio.no/oyvindry/finitegaussian/.
Ø. Ryan. Documentation for the random matrix library, 2009b. http://folk.
uio.no/oyvindry/rmt/doc.pdf.
Ø. Ryan and M. Debbah. Free deconvolution for signal processing applications.
In Proceedings of IEEE International Symposium on Information Theory
(ISIT’07), pages 1846–1850, Nice, France, June 2007a.
Ø. Ryan and M. Debbah. Multiplicative free convolution and information plus
noise type matrices, 2007b. http://arxiv.org/abs/math/0702342.
Ø. Ryan and M. Debbah. Asymptotic behavior of random Vandermonde matrices
with entries on the unit circle. IEEE Transactions on Information Theory, 55
(7):3115–3148, July 2009.
Ø. Ryan and M. Debbah. Convolution operations arising from Vandermonde
matrices. IEEE Transactions on Information Theory, 2011. http://arxiv.
org/abs/0910.4624. To appear.
S. Saraﬁjanovic and J. Y. Le Boudec. An artiﬁcial immune system approach
with secondary response for misbehavior detection in mobile ad-hoc networks.
IEEE Transactions on Neural Networks, Special Issue on Adaptive Learning
Systems in Communication Networks, 16(5):1076–1087, 2005.
A. Scaglione. Statistical analysis of the capacity of MIMO frequency selective
Rayleigh fading channels with arbitrary number of inputs and outputs.
In Proceedings of IEEE International Symposium on Information Theory
(ISIT’02), page 278, Lausanne, Switzerland, July 2002.
L. Scharf. Statistical Signal Processing: Detection, Estimation and Time-Series
Analysis. Addison-Wesley, Boston, MA, USA, 1991.
R. Schmidt. Multiple emitter location and signal parameter estimation. IEEE
Transactions on Antennas and Propagation, 34(3):276–280, 1986.
P. Schramm and R. M¨uller. Spectral eﬃciency of CDMA systems with linear
MMSE interference suppression. IEEE Transactions on Communications, 47
(5):722–731, May 1999.
E. Seneta. Non-negative Matrices and Markov Chains. Springer Verlag, New
York, second edition, 1981.

References
531
A. M. Sengupta and P. P. Mitra.
Capacity of multivariate channels with
multiplicative noise: random matrix techniques and large-N expansions for
full transfer matrices.
Journal of Statistical Physics, 125(5-6):1223–1242,
December 2006.
R. S´eroul. Programming for Mathematicians. Springer Universitext, New York,
NY, USA, February 2000.
S. Sesia, I. Touﬁk, and M. Baker. LTE, The UMTS Long Term Evolution: From
Theory to Practice. John Wiley and Sons, Inc., 2009.
S. Shamai and S. Verd´u. The impact of frequency-ﬂat fading on the spectral
eﬃciency of CDMA. IEEE Transactions on Information Theory, 47(4):1302–
1327, 2001.
C. Shannon. A mathematical theory of communication. Bell System Technical
Journal, 27:379–423, 1948.
G. Sharma, A. Ganesh, and P. Key.
Performance analysis of random access
scheduling schemes.
In Proceedings of IEEE International Conference on
Computer Communications (INFOCOM’06), Barcelona, Spain, April 2006.
S. Shi, M. Schubert, and H. Boche.
Rate optimization for multiuser MIMO
systems with linear processing, Part II.
IEEE Transactions on Signal
Processing, 56(8):4020–4030, 2008.
J. Shore and R. Johnson. Axiomatic derivation of the principle of maximum
entropy and the principle of minimum cross-entropy. IEEE Transactions on
Information Theory, 26(1):26–37, 1980.
J. W. Silverstein. On the randomness of eigenvectors generated from networks
with random topologies. SIAM Journal on Applied Mathematics, 37(2):235–
245, 1979.
J. W. Silverstein. Describing the behavior of eigenvectors of random matrices
using sequences of measures on orthogonal groups.
SIAM Journal on
Mathematical Analysis, 12(2):274–281, 1981.
J. W. Silverstein. Some limit theorems on the eigenvectors of large dimensional
sample covariance matrices. Journal of Multivariate Analysis, 15(3):295–324,
1984.
J. W. Silverstein.
Eigenvalues and eigenvectors of large dimensional sample
covariance matrices. Random Matrices and their Applications, pages 153–159,
1986.
J. W. Silverstein and Z. D. Bai. On the empirical distribution of eigenvalues of a
class of large dimensional random matrices. Journal of Multivariate Analysis,
54(2):175–192, 1995.
J. W. Silverstein and S. Choi. Analysis of the limiting spectral distribution of
large dimensional random matrices. Journal of Multivariate Analysis, 54(2):
295–309, 1995.
J. W. Silverstein, Z. D. Bai, and Y. Q. Yin. A note on the largest eigenvalue of a
large dimensional sample covariance matrix. Journal of Multivariate Analysis,
26(2):166–168, 1988.

532
References
M. K. Simon, F. F. Digham, and M. S. Alouini. On the energy detection of
unknown signals over fading channels. In Proceedings of IEEE International
Conference on Communications (ICC’03), Anchorage, Alaska, 2003.
S. H. Simon, A. L. Moustakas, and L. Marinelli.
Capacity and character
expansions: Moment generating function and other exact results for MIMO
correlated channels. IEEE Transactions on Information Theory, 52(12):5336–
5351, 2006.
O. Somekh, B. J. Zaidel, and S. Shamai. Spectral eﬃciency of joint multiple
cell-sites processors for randomly spread DS-CDMA. In Proceedings of IEEE
International Symposium on Information Theory (ISIT’04), Chicago, CA,
USA, July 2004.
O. Somekh, B. M. Zaidel, and S. Shamai. Sum rate characterization of joint
multiple cell-site processing. IEEE Transactions on Information Theory, 53
(12):4473–4497, 2007.
R. Speicher.
Combinatorial theory of the free product with amalgamation
and operator-valued free probability theory.
Memoirs of the American
Mathematical Society, 627:1–88, 1998.
C. Sun, W. Zhang, and K. B. Letaief.
Cooperative spectrum sensing for
cognitive radios under bandwidth constraints. In Proceedings of IEEE Wireless
Communications & Networking Conference (WCNC’07), pages 1–5, Hong
Kong, 2007a.
C. Sun, W. Zhang, and K. B. Letaief. Cluster-based cooperative spectrum sensing
in cognitive radio systems. In Proceedings of IEEE International Conference
on Communications (ICC’07), pages 2511–2515, Glasgow, Scotland, 2007b.
T. Tanaka. A statistical-mechanics approach to large-system analysis of CDMA
multiuser detectors. IEEE Transactions on Information Theory, 48(11):2888–
2910, 2002.
R. Tandra and A. Sahai.
Fundamental limits on detection in low SNR
under noise uncertainty. In International Conference on Wireless Networks,
Communications and Mobile Computing, pages 464–469, 2005.
R. Tandra, M. Mishra, and A. Sahai. What is a spectrum hole and what does it
take to recognize one? Proceedings of the IEEE, 97(5):824–848, 2009.
G. Taricco. Asymptotic mutual information statistics of separately correlated
Rician fading MIMO channels. IEEE Transactions on Information Theory, 54
(8):3490–3504, 2008.
I. E. Telatar. Capacity of multi-antenna Gaussian channels. Bell Labs, Technical
Memorandum, pages 585–595, 1995.
I. E. Telatar.
Capacity of multi-antenna Gaussian channels.
European
Transactions on Telecommunications, 10(6):585–595, February 1999.
Z. Tian and G. B. Giannakis.
A wavelet approach to wideband spectrum
sensing for cognitive radios. In International Conference on Cognitive Radio
Oriented Wireless Networks and Communications (CROWCOM’06), pages 1–
5, Mykonos Island, Greece, 2006.

References
533
E. C. Titchmarsh. The Theory of Functions. Oxford University Press, New York,
NY, USA, 1939.
C. A. Tracy and H. Widom. On orthogonal and symplectic matrix ensembles.
Communications in Mathematical Physics, 177(3):727–754, 1996.
D. N. C. Tse and S. V. Hanly.
Multiaccess fading channels. I. Polymatroid
structure, optimal resource allocation and throughput capacities.
IEEE
Transactions on Information Theory, 44(7):2796–2815, 1998.
D. N. C. Tse and S. V. Hanly. Linear multiuser receivers: eﬀective interference,
eﬀective bandwidth and user capacity. IEEE Transactions on Information
Theory, 45(2):641–657, February 1999.
D. N. C. Tse and S. Verd´u. Optimum asymptotic multiuser eﬃciency of randomly
spread CDMA. IEEE Transactions on Information Theory, 46(7):2718–2722,
July 2000.
D. N. C. Tse and P. Viswanath.
Fundamentals of Wireless Communication.
Cambridge University Press, Cambridge, UK, 2005.
D. N. C. Tse and O. Zeitouni. Linear multiuser receivers in random environments.
IEEE Transactions on Information Theory, 46(1):171–188, January 2000.
D. N. C. Tse and L. Zheng. Diversity and multiplexing: a fundamental tradeoﬀ
in multiple-antenna channels. IEEE Transactions on Information Theory, 49
(5):1073–1096, 2003.
G. H. Tucci.
A Note on Averages over Random Matrix Ensembles.
IEEE
Transactions on Information Theory, 2010. http://arxiv.org/abs/0910.
0575. Submitted for publication.
G. H. Tucci and P. A. Whiting.
Eigenvalue results for large scale random
Vandermonde matrices with unit complex entries.
IEEE Transactions on
Information Theory, 2010. To appear.
A. M. Tulino and S. Verd´u. Random matrix theory and wireless communications.
Foundations and Trends in Communications and Information Theory, 1(1),
2004.
A. M. Tulino and S. Verd´u. Impact of antenna correlation on the capacity of
multiantenna channels.
IEEE Transactions on Information Theory, 51(7):
2491–2509, 2005.
A. M. Tulino, S. Verd´u, and A. Lozano. Capacity of antenna arrays with space,
polarization and pattern diversity. In Proceedings of IEEE Information Theory
Workshop (ITW’03), pages 324–327, Paris, France, 2003.
A. M. Tulino, L. Li, and S. Verd´u. Spectral eﬃciency of multicarrier CDMA.
IEEE Transactions on Information Theory, 51(2):479–505, 2005.
H. Urkowitz. Energy detection of unknown deterministic signals. Proceedings of
the IEEE, 55(4):523–531, 1967.
P. Vallet and P. Loubaton.
A G-estimator of the MIMO channel ergodic
capacity. In Proceedings of IEEE International Symposium on Information
Theory (ISIT’09), pages 774–778, Seoul, Korea, June 2009.
P. Vallet, P. Loubaton, and X. Mestre.
Improved subspace estimation for
multivariate observations of high dimension: the deterministic signals case.

534
References
IEEE Transactions on Information Theory, 2010. http://arxiv.org/abs/
1002.3234. Submitted for publication.
P. Vallet, W. Hachem, P. Loubaton, X. Mestre, and J. Najim. On the consistency
of the G-MUSIC DoA estimator.
In Proceedings of IEEE Workshop on
Statistical Signal Processing (SSP’11), Nice, France, 2011a.
P. Vallet, W. Hachem, P. Loubaton, X. Mestre, and J. Najim. An improved
music algorithm based on low-rank perturbation of large random matrices.
In Proceedings of IEEE Workshop on Statistical Signal Processing (SSP’11),
Nice, France, 2011b.
A. W. Van der Vaart. Asymptotic Statistics. Cambridge University Press, New
York, 2000.
H. L. Vantrees. Detection, Estimation and Modulation Theory. Wiley and Sons,
1968.
S. Verd´u and S. Shamai. Spectral eﬃciency of CDMA with random spreading.
IEEE Transactions on Information Theory, 45(2):622–640, February 1999.
S. Viswanath, N. Jindal, and A. Goldsmith. Duality, achievable rates, and sum-
rate capacity of Gaussian MIMO broadcsat channels. IEEE Transactions on
Information Theory, 49(10):2658–2668, 2003.
H. Viswanathan and S. Venkatesan. Asymptotics of sum rate for dirty paper
coding and beamforming in multiple-antenna broadcast channels. Proceedings
of IEEE Annual Allerton Conference on Communication, Control, and
Computing (Allerton’03), 41(2):1064–1073, 2003.
D. Voiculescu. Addition of certain non-commuting random variables. Journal of
functional analysis, 66(3):323–346, 1986.
D. Voiculescu. Multiplication of certain non-commuting random variables. J.
Operator Theory, 18:223–235, 1987.
D. Voiculescu. Limit laws for random matrices and free products. Inventiones
Mathematicae, 104(1):201–220, December 1991.
D. Voiculescu, K. J. Dykema, and A. Nica. Free random variables. American
Mathematical Society, 1992.
S. Wagner, R. Couillet, M. Debbah, and D. T. M. Slock. Large system analysis
of linear precoding in MISO broadcast channels with limited feedback. IEEE
Transactions on Information Theory, 2011. http://arxiv.org/abs/0906.
3682. Submitted for publication.
B. Wang, K. J. Liu, and T. Clancy. Evolutionary cooperative spectrum sensing
game: how to collaborate?
IEEE Transactions on Communications, 58(3):
890–900, March 2010. ISSN 0090-6778.
J. G. Wardrop. Road paper: some theoretical aspects of road traﬃc research.
ICE Proceedings, Engineering Divisions, 1(3):325–362, 1952.
M. Wax and T. Kailath. Detection of signals by information theoretic criteria.
IEEE Transactions on Signal, Speech and Signal Processing, 33(2):387–392,
1985.
W. Weichselberger, M. Herdin, H. ¨Ozcelik, and E. Bonek. A stochastic MIMO
channel model with joint correlation of both link ends. IEEE Transactions on

References
535
Wireless Communications, 5(1):90–100, 2006. ISSN 1536-1276.
H. Weingarten, Y. Steinberg, and S. Shamai. The capacity region of the Gaussian
multiple-input multiple-output broadcast channel.
IEEE Transactions on
Information Theory, 52(9):3936–3964, 2006.
A. Wiesel, Y. C. Eldar, and S. Shamai. Zero-forcing precoding and generalized
inverses. IEEE Transactions on Signal Processing, 56(9):4409–4418, 2008.
E. Wigner. Characteristic vectors of bordered matrices with inﬁnite dimensions.
The Annals of Mathematics, 62(3):548–564, November 1955.
E. Wigner. On the distribution of roots of certain symmetric matrices. The
Annals of Mathematics, 67(2):325–327, March 1958.
J. Wishart. The generalized product moment distribution in samples from a
normal multivariate population. Biometrika, 20(1-2):32–52, December 1928.
A. D. Wyner. Shannon-theoretic approach to a Gaussian cellular multiple access
channel. IEEE Transactions on Information Theory, 40(6):1713–1727, 1994.
S. Yang and J. C. Belﬁore. Diversity of MIMO multihop relay channels. IEEE
Transactions on Information Theory, 2008. http://arxiv.org/abs/0708.
0386. Submitted for publication.
J. Yao, R. Couillet, J. Najim, E. Moulines, and M. Debbah. CLT for eigen-
inference methods in cognitive radios. In Proceedings of IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP’11), Prague,
Czech Republic, 2011. To appear.
R. D. Yates. A framework for uplink power control in cellular radio systems.
IEEE Journal on Selected Areas in Communications, 13(7):1341–1347, 1995.
Y. Q. Yin, Z. D. Bai, and P. R. Krishnaiah. On the limit of the largest eigenvalue
of the large dimensional sample covariance matrix. Probability Theory and
Related Fields, 78(4):509–521, 1988.
T. Yoo and A. Goldsmith.
On the optimality of multiantenna broadcast
scheduling using zero-forcing beamforming. IEEE Journal on Selected Areas
in Communications, 24(3):528–541, 2006.
W. Yu and J. M. Cioﬃ. Sum capacity of Gaussian vector broadcast channels.
IEEE Transactions on Information Theory, 50(9):1875–1892, 2004.
B. M. Zaidel, S. Shamai, and S. Verd´u. Multicell uplink spectral eﬃciency of
coded DS-CDMA with random signatures. IEEE Journal on Selected Areas
in Communications, 19(8):1556–1569, August 2001.
A. Zellner. An Introduction to Bayesian Inference in Econometrics. John Wiley
and Sons, Inc., New York, second edition, 1971.
Y. Zeng and Y. C. Liang. Eigenvalue based spectrum sensing algorithms for
cognitive radio.
IEEE Transactions on Communications, 57(6):1784–1793,
2009.
L. Zhang. Spectral analysis of large dimensional random matrices. PhD thesis,
National University of Singapore, 2006.
W. Zhang and K. B. Letaief.
Cooperative spectrum sensing with transmit
and relay diversity in cognitive networks.
IEEE Transactions on Wireless

536
References
Communications, 7(12):4761–4766, December 2008.

Index
almost sure convergence, 19
distribution function, 19
arcsinus law, 87
asymptotic freeness, 78
Bai and Silverstein method, 115
Bayesian probability theory, 478
Bell number, 101
Borel–Cantelli lemma, 46
broadcast channel, 335
linear precoders, 336
Brownian motion, 507
Dyson, 508
capacity maximizing precoder, 5
frequency selective channels, 325
Rayleigh model, 296
Rice model, 318
Carleman’s condition, 95
Catalan number, 102
Cauchy integral formula, 202
CDMA
orthogonal, 284
random, 264
central limit theorem, 63, 213
martingale diﬀerence, 69
variance proﬁle, 175
channel modeling, 477
correlated channel, 484
rank-limited channel, 494
circular law, 31
CLT, see central limit theorem
cognitive radio, 393
complex analysis
pole, 206
residue, 206
residue calculus, 207
complex zonal polynomial, 24
conditional probability, 66
consistent estimator, 2
convergence in probability, 19
correlated channel, 484
correlation proﬁle, 149
cumulant
classical cumulant, 101
free cumulant, 99
moment to cumulant, 100
cumulative distribution function, see
distribution function
decoder design, 328
delta method, 216
detection, 393
condition number criterion, 413
error exponent, 416
GLRT criterion, 414
Neyman–Pearson criterion, 399
test power, 416
deterministic equivalent, 114
Haar matrix, 153
information plus noise, 152
variance proﬁle, 145
distribution function, 18
dominated convergence theorem, 135
Dyson Brownian motion, 508
eigen-inference, see G-estimation
eigenvector
central limit theorem, 238
limiting distribution, 238
elementary symmetric polynomials, 100
empirical spectral distribution, 29
ergodic capacity, 296
frequency selective channels, 324
Rayleigh model, 295
Rice model, 316
e.s.d., 29
estimation, 421
DoA, 422
G-MUSIC, 425
MUSIC, 423
G-MUSIC, 429
power estimation, 432
free probability, 440
G-estimation, 447
η-transform, 41
exact separation, 184, 193
537

538
Index
femto-cell, 11
ﬁxed-point algorithm, 117
Fredholm determinant, 233
free family, 73
free moments, 98
free probability theory, 72
additive convolution, 75
additive deconvolution, 75
additive free convolution, 100
asymptotic freeness, 78
free cumulant, 99
free moments, 98
information plus noise, 85
limit distribution, 77
multiplicative convolution, 75
multiplicative deconvolution, 75
multiplicative free convolution, 101
random matrices, 77
rectangular free cumulants, 110
rectangular free probability, 109
frequency selective channel, 322
full circle law, 31
G-estimation, 199, 421
G-MUSIC, 425
Gaussian method, 139
Haar matrix, 80, 87, 153
Harish–Chandra formula, 22, 25
H¨older’s inequality, 47
l’Hostipal rule, 27, 208
hypergeometric function, 24
imperfect CSI, 339
information plus noise, 152
deterministic equivalent, 145
exact separation, 193
free convolution, 85
limiting support, 197
no eigenvalue outside the support, 192
spectrum analysis, 192
information plus noise matrix
l.s.d., 60
integration by parts, 140
isometric matrix, see Haar matrix
Jacobian matrix, 176
Jakes’ model, 326
Kronecker model, 339, 488
Laguerre polynomial, 22, 296
law of large numbers, 1
limit distribution, 77
limit spectral distribution, 30
limiting support
information plus noise, 197
sample covariance matrix, 189
linear precoders, 336
l.s.d., 30
MAC
ergodic capacity, 360
quasi-static channel, 357
rate region, 355
Mar˘cenko–Pastur law, 4
Mar˘cenko–Pastur law, 32
central limit theorem, 65
moments, 33
proof, 44
R-transform, 84
S-transform, 84
Markov inequality, 46
martingale, 66
diﬀerence, 66
matched-ﬁlter, 265, 277, 282, 285
matrix inversion lemma
block inversion, 45
Silverstein’s inversion lemma, 124
maximum entropy principle, 478
method of moments, 78, 95
MIMO
BC, 335
linear precoders, 336
MAC, 335
ergodic capacity, 360
quasi-static channel, 357
sum rate, 364
Rayleigh channel, 481
single-user, 293
ergodic capacity, 309, 311
frequency selective channel, 322
optimal precoder, 312
outage capacity, 298
quasi-static capacity, 293, 305
Rice model, 316
ML, 445
MMSE, 445
MMSE decoder, 278, 282
MMSE receiver, 267
moment
classical moment, 101
convergence theorem, 95
free moment, 98
method of moments, 78, 95
moment to cumulant, 100
Montel’s theorem, see Vitali’s theorem
multi-cell network, 369
multiple access channel, 335
MUSIC, 423
mutual information, 5
Nash–Poincar´e inequality, 141
Newton–Girard formula, 101, 444

Index
539
Neyman–Pearson test, 399
MIMO, 406
SIMO, 401
no eigenvalue outside the support, 181, 192
non-commutative probability space, 72
non-crossing partitions, 99, 101, 103
orthogonal polynomials, 232
outage capacity
Rayleigh model, 298
Rice model, 320
pole, 206
population covariance matrix, 2
probability density function, 18
probability distribution, 18
ψ-transform, 42
quasi-static capacity, 293
Rice model, 316
quasi-static channel, 357
random matrix, 17
random Vandermonde matrix, 105
rank inequality, 55
rank-1 perturbation lemma, 50, 352
rectangular free cumulants, 110
rectangular free probability, 109
regularized zero-forcing, 341
relay network, 369
replica method, 505
replica trick, 505
reproducing kernel, 231
residue, 206
residue calculus, 207
resolvent identity, 123
R-transform, 41
deﬁnition, 75
properties, 75
sample covariance matrix, 2
eigen-inference
central limit theorem, 213
exact separation, 184
G-estimation, 201
limiting support, 189
l.s.d., 58
no eigenvalue outside the support, 181
spectrum analysis, 180
semi-circle law, 7, 30
moments, 97
proof, 96
R-transform, 84
Shannon transform, 39
singular law, 109
Slutsky’s lemma, 216
spectrum analysis, 179
sample covariance matrix, 180
spectrum separability, 190
standard function, 167
standard interference function, 167
Stieltjes transform, 35
inverse, 36
properties, 38, 52
stochastic diﬀerential equation, 508
S-transform, 41
deﬁnition, 76
properties, 76
symmetrized singular law, 109
system modeling, 477
Szeg¨o’s theorem, 276
time-varying random matrices, 506
Tonelli theorem, 61, 352
trace lemma, 45, 49, 54, 345, 346, 351
central limit theorem, 46
Haar matrix, 153
transceiver design, 328
truncation, centralization, rescaling, 54
Vandermonde matrix, 105
variance proﬁle
correlation proﬁle, 149
deterministic equivalent, 145
Shannon transform, 148
Vitali’s theorem, 53
water-ﬁlling, 293
iterative, 313, 319, 325, 363
weak convergence, 18
Wiener process, 507
Wigner matrix, 7, 30
Wishart matrix, 19
Wyner model, 376
zero-forcing, 349
regularized, 341
zonal polynomial, 24





© 2015 Cengage Learning
ALL RIGHTS RESERVED. No part of this work covered by the copyright 
herein may be reproduced, transmitted, stored, or used in any form 
or by any means graphic, electronic, or mechanical, including but not 
limited to photocopying, recording, scanning, digitizing, taping, web 
distribution, information networks, or information storage and retrieval 
systems, except as permitted under Section 107 or 108 of the 1976 
United States Copyright Act, without the prior written permission of 
the publisher.
Microsoft Excel® is a registered trademark of Microsoft Corporation. 
© 2015 Microsoft.
Library of Congress Control Number: 2013944175
ISBN-13: 978-1-285-18727-3
ISBN-10: 1-285-18727-X
Cengage Learning  
200 First Stamford Place, 4th Floor 
Stamford, CT 06902 
USA
Cengage Learning is a leading provider of customized learning solutions 
with office locations around the globe, including Singapore, the United 
Kingdom, Australia, Mexico, Brazil, and Japan. Locate your local office at:  
www.cengage.com/global
Cengage Learning products are represented in Canada by  
Nelson Education, Ltd.
To learn more about Cengage Learning Solutions, visit  
www.cengage.com
Purchase any of our products at your local college store or at our  
preferred online store www.cengagebrain.com
Essentials of Business Analytics, 
First Edition
Camm, Cochran, Fry, Ohlmann, Anderson, 
Sweeney, Williams
Senior Vice President, Global Product 
 Manager, Higher Education: Jack W. Calhoun
Vice President, General Manager, Social 
 Science & Qualitative Business: Erin Joyner 
Product Director: Joe Sabatino
Content Developer: Maggie Kubale
Product Assistant: Brad Sullender
Content Project Manager: Cliff Kallemeyn
Media Developer: Chris Valentine
Manufacturing Planner: Ron Montgomery
Marketing Director: Natalie King
Marketing Manager:  Heather Mooney
Associate Market Development Manager: 
Roy Rosa
Production Service: MPS Limited
Sr. Art Director: Stacy Jenkins Shirley
Internal Designer: Mike Stratton/ 
Chris Miller
Cover Designer: Craig Ramsdell
Cover Image: © Kutlayev Dmitry/
Shutterstock
Sr. Rights Acquisitions Specialist: John Hill
For product information and technology assistance, contact us at  
Cengage Learning Customer & Sales Support, 1-800-354-9706
For permission to use material from this text or product,  
submit all requests online at www.cengage.com/permissions  
Further permissions questions can be emailed to 
permissionrequest@cengage.com
Printed in the United States of America
1 2 3 4 5 6 7 17 16 15 14 13

Brief Contents
 About the Authors xiv
 Preface xvii
Chapter 1 
Introduction 1
Chapter 2 
Descriptive Statistics 15
Chapter 3 
Data Visualization 70
Chapter 4 
Linear Regression 123
Chapter 5 
Time Series Analysis and Forecasting 202
Chapter 6 
Data Mining 251
Chapter 7 
Spreadsheet Models 320
Chapter 8 
Linear Optimization Models 352
Chapter 9 
Integer Linear Optimization Models 405
Chapter 10 Nonlinear Optimization Models 448
Chapter 11 Monte Carlo Simulation 485
Chapter 12 Decision Analysis 550
Appendix A Basics of Excel 609
Appendix B Data Management and Microsoft Access 621
Appendix C Answers to Even-Numbered Exercises (online)
 References 659
 Index 661

Contents
About the Authors xiv
Preface xvii
Chapter 1 
Introduction  1
1.1  Decision Making 4
1.2  Business Analytics Defined 5
1.3  A Categorization of Analytical Methods and Models 5
Descriptive Analytics 5
Predictive Analytics 6
Prescriptive Analytics 6
Analytics in Action:  Procter & Gamble Uses Business Analytics  
to Redesign its Supply Chain 7
1.4  Big Data 8
1.5  Business Analytics in Practice 9
Financial Analytics 9
Human Resource (HR) Analytics 10
Marketing Analytics 10
Health Care Analytics 10
Supply Chain Analytics 11
Analytics for Government and Nonprofits 11
Sports Analytics 12
Web Analytics 12
Summary 13
Glossary 13
Chapter 2 
Descriptive Statistics  15
Analytics in Action: U.S. Census Bureau 16
2.1  Overview of Using Data:  Definitions and Goals 16
2.2  Types of Data 17
Population and Sample Data 17
Quantitative and Categorical Data 18
Cross-Sectional and Time Series Data 18
Sources of Data 18
2.3  Modifying Data in Excel 21
Sorting and Filtering Data in Excel 21
Conditional Formatting of Data in Excel 23

vi 
Contents
2.4  Creating Distributions from Data 25
Frequency Distributions for Categorical Data 25
Relative Frequency and Percent Frequency Distributions 27
Frequency Distributions for Quantitative Data 28
Histograms 31
Cumulative Distributions 34
2.5  Measures of Location 35
Mean (Arithmetic Mean) 35
Median 36
Mode 37
Geometric Mean 38
2.6  Measures of Variability 40
Range 41
Variance 41
Standard Deviation 43
Coefficient of Variation 44
2.7  Analyzing Distributions 44
Percentiles 44
Quartiles 45
z-scores 46
Empirical Rule 48
Identifying Outliers 48
Box Plots 49
2.8  Measures of Association Between Two  Variables 51
Scatter Charts 51
Covariance 52
Correlation Coefficient 55
Summary 57
Glossary 57
Problems 58
Case:  Heavenly Chocolates Web Site  Transactions 66
Appendix:  Creating Box Plots in XLMiner 67
Chapter 3 
Data Visualization  70
Analytics in Action:  Cincinnati Zoo & Botanical Garden 71
3.1  Overview of Data Visualization 73
Effective Design Techniques 73
3.2  Tables 75
Table Design Principles 77
Crosstabulation 79
PivotTables in Excel 80
3.3  Charts 85
Scatter Charts 85
Line Charts 87

 
Contents 
vii
Bar Charts and Column Charts 90
A Note on Pie Charts and 3-D Charts 93
Bubble Charts 93
Heat Maps 95
Additional Charts for Multiple Variables 97
PivotCharts in Excel 101
3.4  Advanced Data Visualization 102
Advanced Charts 103
Geographic Information Systems Charts 104
3.5  Data Dashboards 105
Principles of Effective Data Dashboards 106
Applications of Data Dashboards 106
Summary 108
Glossary 109
Problems 110
Case Problem:  All-Time Movie Box Office Data 118
Appendix:  Creating a Scatter Chart Matrix and a  Parallel Coordinates  
Plot with XLMiner 119
Chapter 4 
Linear Regression  123
Analytics in Action:  Alliance Data Systems 124
4.1  The Simple Linear Regression Model 125
Regression Model and Regression Equation 125
Estimated Regression Equation 126
4.2  Least Squares Method 127
Least Squares Estimates of the Regression Parameters 129
Using Excel’s Chart Tools to Compute the Estimated Regression 
Equation 132
4.3  Assessing the Fit of the Simple Linear  Regression Model 133
The Sums of Squares 134
The Coefficient of Determination 136
Using Excel’s Chart Tools to Compute the Coefficient of 
Determination 137
4.4  The Multiple Regression Model 138
Regression Model and Regression Equation 138
Estimated Multiple Regression Equation 138
Least Squares Method and Multiple Regression 139
Butler Trucking Company and Multiple Regression  140
Using Excel’s Regression Tool to Develop the Estimated Multiple 
Regression Equation 140
4.5  Inference and Regression 143
Conditions Necessary for Valid Inference in the Least Squares 
Regression Model 144
Testing for an Overall Regression Relationship 148

viii 
Contents
Testing Individual Regression Parameters 150
Addressing Nonsignificant Independent Variables 153
Multicollinearity 154
Inference and Very Large Samples 156
4.6  Categorical Independent Variables 161
Butler Trucking Company and Rush Hour 161
Interpreting the Parameters 162
More Complex Categorical Variables 164
4.7  Modeling Nonlinear Relationships 165
Quadratic Regression Models 167
Piecewise Linear Regression Models 170
Interaction Between Independent Variables 173
4.8  Model Fitting 177
Variable Selection Procedures 177
Overfitting 179
Summary 180
Glossary 180
Problems 182
Case Problem: Alumni Giving 197
Appendix: Using XLMiner for Regression 198
Chapter 5 
Time Series Analysis and Forecasting  202
Analytics in Action:  Forecasting Demand for a Broad Line  
of Office Products 203
5.1  Time Series Patterns 205
Horizontal Pattern 205
Trend Pattern 207
Seasonal Pattern 209
Trend and Seasonal Pattern 209
Cyclical Pattern 211
Identifying Time Series Patterns 212
5.2  Forecast Accuracy 212
5.3  Moving Averages and Exponential Smoothing 217
Moving Averages 217
Forecast Accuracy 221
Exponential Smoothing 221
Forecast Accuracy 224
5.4  Using Regression Analysis for Forecasting 226
Linear Trend Projection 226
Seasonality 228
Seasonality Without Trend 228
Seasonality with Trend 230
Using Regression Analysis as a Causal Forecasting Method 231

 
Contents 
ix
Combining Causal Variables with Trend and Seasonality  
Effects 235
Considerations in Using Regression in Forecasting 235
5.5  Determining the Best Forecasting Model to Use 236
Summary 237
Glossary 237
Problems 238
Case Problem: Forecasting Food and Beverage Sales 246
Appendix: Using XLMiner for Forecasting 247
Chapter 6 
Data Mining  251
Analytics in Action:  Online Retailers Using Predictive Analytics  
to Cater to Customers 252
6.1  Data Sampling 253
6.2  Data Preparation 254
Treatment of Missing Data 254
Identification of Outliers and Erroneous Data 254
Variable Representation 254
6.3  Unsupervised Learning 255
Cluster Analysis 256
Association Rules 265
6.4  Supervised Learning 269
Partitioning Data 269
Classification Accuracy 273
Prediction Accuracy 277
k-Nearest Neighbors 277
Classification and Regression Trees 283
Logistic Regression 299
Summary 308
Glossary 309
Problems 311
Case Problem: Grey Code Corporation 319
Chapter 7 
Spreadsheet Models  320
Analytics in Action:  Procter and Gamble Sets Inventory Targets Using 
Spreadsheet Models 321
7.1  Building Good Spreadsheet Models 322
Influence Diagrams 322
Building a Mathematical Model 322
Spreadsheet Design and Implementing the Model  
in a Spreadsheet 324

x 
Contents
7.2  What-If Analysis 327
Data Tables 327
Goal Seek 331
7.3  Some Useful Excel Functions for Modeling 332
SUM and SUMPRODUCT 332
IF and COUNTIF 333
VLOOKUP 337
7.4  Auditing Spreadsheet Models 339
Trace Precedents and Dependents 339
Show Formulas 340
Evaluate Formulas 340
Error Checking 341
Watch Window 342
Summary 343
Glossary 343
Problems 344
Case Problem: Retirement Plan 350
Chapter 8 
Linear Optimization Models  352
Analytics in Action:  Timber Harvesting Model at MeadWestvaco 
Corporation 353
8.1  A Simple Maximization Problem 354
Problem Formulation 355
Mathematical Model for the Par, Inc. Problem 357
8.2  Solving the Par, Inc. Problem 358
The Geometry of the Par, Inc. Problem 358
Solving Linear Programs with Excel Solver 360
8.3  A Simple Minimization Problem 364
Problem Formulation 364
Solution for the M&D Chemicals Problem 365
8.4  Special Cases of Linear Program Outcomes 367
Alternative Optimal Solutions 367
Infeasibility 368
Unbounded 370
8.5  Sensitivity Analysis 372
Interpreting Excel Solver Sensitivity Report 372
8.6   General Linear Programming Notation and More Examples 374
Investment Portfolio Selection 375
Transportation Planning 378
Advertising Campaign Planning  381
8.7   Generating an Alternative Optimal Solution for a Linear Program 386
Summary 388
Glossary 389

 
Contents 
xi
Problems 390
Case Problem: Investment Strategy 398
Appendix:  Solving Linear Optimization Models  Using Analytic Solver 
Platform 399
Chapter 9 
Integer Linear Optimization Models  405
Analytics in Action:  Optimizing the Transport of Oil Rig Crews 406
9.1  Types of Integer Linear Optimization Models 406
9.2  Eastborne Realty, An Example of Integer  Optimization  407
The Geometry of Linear All-Integer Optimization 408
9.3  Solving Integer Optimization Problems with Excel Solver 410
A Cautionary Note About Sensitivity Analysis 414
9.4  Applications Involving Binary Variables 415
Capital Budgeting 415
Fixed Cost 416
Bank Location 420
Product Design and Market Share Optimization 424
9.5  Modeling Flexibility Provided by Binary Variables 426
Multiple-Choice and Mutually Exclusive Constraints 427
k out of n Alternatives Constraint 427
Conditional and Corequisite Constraints 427
9.6  Generating Alternatives in Binary Optimization 428
Summary 430
Glossary 430
Problems 431
Case Problem: Applecore Children’s Clothing 441
Appendix:  Solving Integer Linear Optimization Problems Using Analytic 
Solver Platform 442
Chapter 10 Nonlinear Optimization Models  448
Analytics in Action: Intercontinental Hotels Optimizes Retail Pricing 449
10.1  A Production Application: Par, Inc. Revisited 449
An Unconstrained Problem 450
A Constrained Problem 450
Solving Nonlinear Optimization Models Using Excel Solver 453
Sensitivity Analysis and Shadow Prices in Nonlinear Models 454
10.2  Local and Global Optima 455
Overcoming Local Optima with Excel Solver 457
10.3  A Location Problem 459
10.4  Markowitz Portfolio Model 461
10.5  Forecasting Adoption of a New Product 465

xii 
Contents
Summary 469
Glossary 470
Problems 470
Case Problem:  Portfolio Optimization with Transaction Costs 477
Appendix:  Solving Nonlinear Optimization Problems with Analytic 
Solver Platform 480
Chapter 11 Monte Carlo Simulation  485
Analytics in Action: Reducing Patient Infections in the ICU 486
11.1  What-If Analysis 487
The Sanotronics Problem 487
Base-Case Scenario 487
Worst-Case Scenario 488
Best-Case Scenario 488
11.2  Simulation Modeling with Native Excel  Functions 488
Use of Probability Distributions to Represent  
Random Variables 489
Generating Values for Random Variables with Excel 491
Executing Simulation Trials with Excel 495
Measuring and Analyzing Simulation Output 495
11.3  Simulation Modeling with Analytic Solver Platform 498
The Land Shark Problem 499
Spreadsheet Model for Land Shark 499
Generating Values for Land Shark’s Random Variables 500
Tracking Output Measures for Land Shark 503
Executing Simulation Trials and Analyzing Output for Land Shark 504
The Zappos Problem 506
Spreadsheet Model for Zappos 507
Modeling Random Variables for Zappos 510
Tracking Output Measures for Zappos 515
Executing Simulation Trials and Analyzing Output for Zappos 517
11.4  Simulation Optimization 518
11.5  Simulation Considerations 524
Verification and Validation 524
Advantages and Disadvantages of Using Simulation 524
Summary 525
Glossary 526
Problems 527
Case Problem: Four Corners 536
Appendix 11.1:  Incorporating Dependence Between  
Random Variables 537
Appendix 11.2:  Probability Distributions for Random Variables 545

 
Contents 
xiii
Chapter 12 Decision Analysis  550
Analytics in Action:  Phytopharm’s New Product Research and 
Development 551
12.1  Problem Formulation 552
Payoff Tables 553
Decision Trees 553
12.2  Decision Analysis Without Probabilities 554
Optimistic Approach 554
Conservative Approach 555
Minimax Regret Approach 555
12.3  Decision Analysis with Probabilities 557
Expected Value Approach 557
Risk Analysis 559
Sensitivity Analysis 560
12.4  Decision Analysis with Sample Information 561
Expected Value of Sample Information 566
Expected Value of Perfect Information 567
12.5  Computing Branch Probabilities with Bayes’ Theorem 568
12.6  Utility Theory 571
Utility and Decision Analysis 573
Utility Functions 577
Exponential Utility Function 580
Summary 581
Glossary 582
Problems 584
Case Problem:  Property Purchase Strategy 595
Appendix:  Using Analytic Solver Platform to  Create  Decision Trees 596
Appendix A Basics of Excel  609
Appendix B Data Management and Microsoft Access  621
Appendix C Answers to Even-Numbered Exercises (online)
 References 659
 Index 661

Jeffrey D. Camm Jeffrey D. Camm is Professor of Quantitative Analysis, Head of the 
Department of Operations, Business Analytics, and Information Systems and College of 
Business Research Fellow in the Carl H. Lindner College of Business at the University of 
Cincinnati. Born in Cincinnati, Ohio, he holds a B.S. from Xavier University and a Ph.D. 
from Clemson University. He has been at the University of Cincinnati since 1984, and has 
been a visiting scholar at Stanford University and a visiting professor of business adminis-
tration at the Tuck School of Business at Dartmouth College.
Dr. Camm has published over 30 papers in the general area of optimization applied 
to problems in operations management. He has published his research in Science, Man-
agement Science, Operations Research, Interfaces and other professional journals. At the 
University of Cincinnati, he was named the Dornoff Fellow of Teaching Excellence and 
he was the 2006 recipient of the INFORMS Prize for the Teaching of Operations  Research 
Practice. A firm believer in practicing what he preaches, he has served as an operations 
 research consultant to numerous companies and government agencies. From 2005 to 
2010 he served as editor-in-chief of Interfaces, and is currently on the editorial board of 
INFORMS Transactions on Education.
James J. Cochran James J. Cochran is the Bank of Ruston Barnes, Thompson, &  Thurmon 
Endowed Research Professor of Quantitative Analysis at Louisiana Tech  University. Born 
in Dayton, Ohio, he holds a B.S., an M.S., and an M.B.A. from Wright State University 
and a Ph.D. from the University of Cincinnati. He has been at  Louisiana Tech University 
since 2000, and has been a visiting scholar at Stanford University,  Universidad de Talca, 
and the University of South Africa.
Professor Cochran has published over two dozen papers in the development and ap-
plication of operations research and statistical methods. He has published his research in 
Management Science, The American Statistician, Communications in Statistics—Theory 
and Methods, European Journal of Operational Research, Journal of Combinatorial 
 Optimization, and other professional journals. He was the 2008 recipient of the INFORMS 
Prize for the Teaching of Operations Research Practice and the 2010 recipient of the Mu 
Sigma Rho Statistical Education Award. Professor Cochran was elected to the International 
Statistics Institute in 2005 and named a Fellow of the American Statistical Association 
in 2011. A strong advocate for effective operations research and statistics education as a 
means of improving the quality of applications to real problems, Professor Cochran has 
organized and chaired teaching effectiveness workshops in Montevideo, Uruguay; Cape 
Town, South Africa; Cartagena, Colombia; Jaipur, India; Buenos Aires, Argentina; and 
Nairobi, Kenya. He has served as an operations research consultant to numerous compa-
nies and not-for-profit organizations. He currently serves as editor-in-chief of INFORMS 
Transactions on Education and is on the editorial board of Interfaces, the Journal of the 
Chilean Institute of Operations Research, and ORiON.
Michael J. Fry Michael J. Fry is Associate Professor of Operations, Business Analyt-
ics, and Information Systems in the Carl H. Lindner College of Business at the University 
of Cincinnati. Born in Killeen, Texas, he earned a B.S. from Texas A&M University, and 
M.S.E. and Ph.D. degrees from the University of Michigan. He has been at the University of 
Cincinnati since 2002, and he has been a visiting professor at The Johnson School at  Cornell 
University and the Sauder School of Business at the University of British Columbia.
About the Authors

 
About the Authors 
xv
Professor Fry has published over a dozen research papers in journals such as Op-
erations Research, M&SOM, Transportation Science, Naval Research Logistics, and In-
terfaces. His research interests are in applying quantitative management methods to the 
areas of  supply chain analytics, sports analytics, and public-policy operations. He has 
worked with many different organizations for his research, including Dell, Inc., Copeland 
Corporation,  Starbucks Coffee Company, the Cincinnati Fire Department, the State of Ohio 
 Election Commission, the Cincinnati Bengals, and the Cincinnati Zoo. In 2008, he was 
named a finalist for the Daniel H. Wagner Prize for Excellence in Operations Research 
Practice, and he has been recognized for both his research and teaching excellence at the 
University of Cincinnati.
Jeffrey W. Ohlmann Jeffrey W. Ohlmann is Associate Professor of Management 
 Sciences in the Tippie College of Business at the University of Iowa. Born in Valentine, 
 Nebraska, he earned a B.S. from the University of Nebraska, and M.S. and Ph.D. degrees 
from the University of Michigan. He has been at the University of Iowa since 2003.
Professor Ohlmann’s research on the modeling and solution of decision-making prob-
lems has produced over a dozen research papers in journals such as Mathematics of Opera-
tions Research, INFORMS Journal on Computing, Transportation Science, and Interfaces. 
He has collaborated with companies such as Transfreight, LeanCor, Cargill, the Hamilton 
County Board of Elections, and the Cincinnati Bengals. Due to the relevance of his work to 
industry, he was bestowed the George B. Dantzig Dissertation Award and was recognized 
as a finalist for the Daniel H. Wagner Prize for Excellence in Operations Research Practice.
David R. Anderson David R. Anderson is Professor Emeritus of Quantitative Analysis 
in the Carl H. Lindner College of Business at the University of Cincinnati. Born in Grand 
Forks, North Dakota, he earned his B.S., M.S., and Ph.D. degrees from Purdue University.
Professor Anderson has served as Head of the Department of Quantitative Analysis and 
Operations Management and as Associate Dean of the College of Business Administration. 
In addition, he was the coordinator of the College’s first Executive Program.
At the University of Cincinnati, Professor Anderson has taught introductory statistics 
for business students as well as graduate-level courses in regression analysis, multivariate 
analysis, and management science. He has also taught statistical courses at the  Department 
of Labor in Washington, D.C. He has been honored with nominations and awards for 
 excellence in teaching and excellence in service to student organizations.
Professor Anderson has coauthored 10 textbooks in the areas of statistics, management 
science, linear programming, and production and operations management. He is an active 
consultant in the field of sampling and statistical methods.
Dennis J. Sweeney Dennis J. Sweeney is Professor Emeritus of Quantitative  Analysis 
and Founder of the Center for Productivity Improvement at the University of Cincinnati. 
Born in Des Moines, Iowa, he earned a B.S.B.A. degree from Drake University and his 
M.B.A. and D.B.A. degrees from Indiana University, where he was an NDEA  Fellow. 
 During 1978–1979, Professor Sweeney worked in the management science group at 
Procter & Gamble; during 1981–1982, he was a visiting professor at Duke University. 
Professor Sweeney served as Head of the Department of Quantitative Analysis and as 
 Associate Dean of the College of Business Administration at the University of Cincinnati.
Professor Sweeney has published more than 30 articles and monographs in the areas 
of management science and statistics. The National Science Foundation, IBM, Procter & 
Gamble, Federated Department Stores, Kroger, and Cincinnati Gas & Electric have funded 
his research, which has been published in Management Science, Operations Research, 
Mathematical Programming, Decision Sciences, and other journals.
Professor Sweeney has coauthored 10 textbooks in the areas of statistics, management 
science, linear programming, and production and operations management.

xvi 
About the Authors
Thomas A. Williams Thomas A. Williams is Professor Emeritus of Management  Science 
in the College of Business at Rochester Institute of Technology. Born in Elmira, New York, 
he earned his B.S. degree at Clarkson University. He did his graduate work at Rensselaer 
Polytechnic Institute, where he received his M.S. and Ph.D. degrees.
Before joining the College of Business at RIT, Professor Williams served for seven 
years as a faculty member in the College of Business Administration at the University of 
Cincinnati, where he developed the undergraduate program in Information Systems and 
then served as its coordinator. At RIT he was the first chairman of the Decision Sciences 
Department. He teaches courses in management science and statistics, as well as graduate 
courses in regression and decision analysis.
Professor Williams is the coauthor of 11 textbooks in the areas of management sci-
ence, statistics, production and operations management, and mathematics. He has been a 
 consultant for numerous Fortune 500 companies and has worked on projects ranging from 
the use of data analysis to the development of large-scale regression models.

Preface
Essentials of Business Analytics is designed to introduce the concept of business analytics 
to undergraduate and graduate students. This textbook contains one of the first collections 
of materials that are essential to the growing field of business analytics. In Chapter 1 we 
present an overview of business analytics and our approach to the material in this text-
book. In simple terms, business analytics helps business professionals make better deci-
sions based on data. We discuss models for summarizing, visualizing, and understanding 
useful information from historical data in Chapters 2 through 6. Chapter 7 covers the use of 
spreadsheets for examining data and building decision models. In Chapters 8 through 10 we 
discuss optimization models to help decision makers choose the best decision based on the 
available data. Chapter 10 presents material that some may consider more advanced forms 
of optimization (nonlinear optimization models), although these models are extremely use-
ful and widely applicable to many business situations. In any case, some instructors may 
choose to omit covering Chapter 10. In Chapter 11 we introduce the concept of simulation 
models for understanding the effect of uncertainty on decisions. Chapter 12 is an overview 
of decision analysis approaches for incorporating a decision maker’s views about risk into 
decision making. In Appendix A we present optional material for students who need to 
learn the basics of using Microsoft Excel. The use of databases and manipulating data in 
Microsoft Access is discussed in Appendix B.
This textbook can be used by students who have previously taken a course on basic 
statistical methods as well as students who have not had a prior course in statistics. This 
textbook introduces basic statistical concepts in enough detail to support their use in busi-
ness analytics tools. For the student who has not had a prior statistics course, these concepts 
are sufficient to prepare the student for more advanced business analytics methods. For 
students who have had a previous statistics class, the material will provide a good review. 
All statistical concepts contained in this textbook are presented from a business analytics 
perspective using practical business examples. For those instructors who wish to skip the 
introductory statistics material, Chapters 2 and 4 can be considered optional.
 
 
Features and Pedagogy
The style and format of this textbook is based on the other classic textbooks written by the 
Anderson, Sweeney, and Williams (ASW) team. Some of the specific features that we use 
in this textbook are listed below.
● Integration of Microsoft Excel: Excel has been thoroughly integrated throughout 
this textbook. For many methodologies, we provide instructions for how to perform 
calculations both by hand and with Excel. In other cases where realistic models 
are practical only with the use of a spreadsheet, we focus on the use of Excel to 
describe the methods to be used.
● Use of Excel 2013: The material presented for Excel in this textbook is fully com-
patible with Excel 2013. In most cases, Excel 2013 can be considered a relatively 
minor update from previous Excel versions as it relates to business analytics. 
However, the data visualization abilities of Excel have been greatly enhanced in 
Excel 2013. It is much easier to create, modify and analyze charts in Excel 2013. 

xviii 
Preface
Recognizing that many students and instructors may not have access to Excel 2013 
at this time, we also provide instructions for using previous versions of Excel when-
ever possible.
● Use of Analytics Solver Platform and XLMiner: This textbook incorporates the 
use of two very powerful Microsoft Excel Add-ins: Analytics Solver Platform and 
XLMiner, both created by Frontline Systems. Analytics Solver Platform provides 
additional optimization and simulation features for Excel. XLMiner incorporates 
sophisticated data mining algorithms into Excel and allows for additional data vi-
sualization and data exploration. In most chapters we place the use of Analytics 
Solver Platform and XLMiner in the chapter appendix so that the instructor can 
choose whether or not to cover this material. However, because these tools are es-
sential to performing simulation and data mining methods, we integrate XLMiner 
throughout Chapter 6 on data mining and we utilize Analytics Solver Platform in 
Sections 11.3 and 11.4 for simulation.
● Notes and Comments: At the end of many sections, we provide Notes and Com-
ments to give the student additional insights about the methods presented in that 
section. These insights include comments on the limitations of the presented meth-
ods, recommendations for applications, and other matters. Additionally, margin 
notes are used throughout the textbook to provide additional insights and tips re-
lated to the specific material being discussed.
● Analytics in Action: Each chapter contains an Analytics in Action article. These 
articles present interesting examples of the use of business analytics in practice. 
The examples are drawn from many different organizations in a variety of areas 
including healthcare, finance, manufacturing, marketing, and others.
● WEBfiles: All data sets used as examples and in student exercises are also provided 
online as files available for download by the student. The names of the WEBfiles 
are called out in margin notes throughout the textbook.
● Problems and Cases: With the exception of Chapter 1, each chapter contains more 
than 20 problems to help the student master the material presented in that chapter. 
The problems vary in difficulty and most relate to specific examples of the use of 
business analytics in practice. Answers to even-numbered problems are provided 
in an online supplement for student access. With the exception of  Chapter 1, each 
chapter also includes an in-depth case study that connects many of the different 
methods introduced in the chapter. The case studies are designed to be more open-
ended than the chapter problems, but enough detail is provided to give the student 
some direction in solving the cases.
Matthew D. Bailey  
 Bucknell University
Phillip Beaver  
Daniels College of  Business 
 University of Denver
M. Khurrum S. Bhutta  
Ohio University
Q B. Chung  
Villanova University
Elizabeth A. Denny  
 University of Kentucky
Mike Taein Eom  
 University of Portland
Acknowledgements
We would like to acknowledge the work of our reviewers, who provided comments and 
suggestions for improvement of this text. Thanks to:

 
Preface 
xix
We are indebted to our product director Joe Sabatino and our product manager, 
Aaron  Arnsparger; our marketing director, Natalie King, our marketing manager, Heather 
Mooney, and our associate marketing development manager, Roy Rosa; our content de-
veloper, Maggie Kubale; our senior content project manager, Cliff Kallemeyn; our media 
developer, Chris Valentine; and others at Cengage Learning for their counsel and support 
during the preparation of this text.
Jeffrey D. Camm 
James J. Cochran 
Michael J. Fry 
Jeffrey W. Ohlmann 
David R. Anderson 
Dennis J. Sweeney 
Thomas A. Williams
Yvette Njan Essounga  
 Fayetteville State University
Lawrence V. Fulton  
Texas State University
James F. Hoelscher  
 Lincoln Memorial University
Eric Huggins  
Fort Lewis College
Faizul Huq  
Ohio University
Marco Lam  
York College of Pennsylvania
Ram Pakath  
University of Kentucky
Susan Palocsay  
James Madison University
Dothan Truong  
Embry-Riddle Aeronautical University
Kai Wang  
Wake Technical  Community College

Introduction
CONTENTS
1.1 
 DECISION MAKING
1.2 
 BUSINESS ANALYTICS 
 DEFINED
1.3 
 A CATEGORIZATION OF 
ANALYTICAL METHODS 
AND MODELS
Descriptive Analytics
Predictive Analytics
Prescriptive Analytics
1.4 
BIG DATA
1.5 
 BUSINESS ANALYTICS 
IN PRACTICE
Financial Analytics
Human Resource (HR) Analytics
Marketing Analytics
Health Care Analytics
Supply Chain Analytics
Analytics for Government and 
Nonprofits
Sports Analytics
Web Analytics
CHAPTER 1

2 
Chapter 1 Introduction
You apply for a loan for the first time. How does the bank assess the riskiness of the loan 
it might make to you? How does Amazon.com know which books and other products to 
recommend to you when you log in to their Web site? How do airlines determine what price 
to quote to you when you are shopping for a plane ticket? How can doctors better diagnose 
and treat you when you are ill or injured?
Even though you are applying for a loan for the first time, millions of people around the 
world have applied for loans. Many of these loan recipients have paid back their loans in 
full and on time, but some of them have not. The bank wants to know whether you are more 
like those who have paid back their loans or more like those who defaulted. By comparing 
your credit history, financial situation, and other factors to the vast database of previous 
loan recipients, the bank can effectively assess how likely you are to default on a loan.
Similarly, Amazon.com has access to millions of previous purchases made by custom-
ers on its Web site. Amazon.com examines your previous purchases, the products you have 
viewed, and any product recommendations you have provided. Amazon.com then searches 
through its huge database for customers who are similar to you in terms of product pur-
chases, recommendations, and interests. Once similar customers have been identified, their 
purchases form the basis of the recommendations given to you.
Prices for airline tickets are frequently updated. The price quoted to you for a flight 
between New York and San Francisco today could be very different from the price quoted 
tomorrow. These changes happen because airlines use a pricing strategy known as revenue 
management. Revenue management works by examining vast amounts of data on past air-
line customer purchases and using these data to forecast future purchases. These forecasts 
are then fed into sophisticated optimization algorithms that determine the optimal price 
to charge for a particular flight and when to change that price. Revenue management has 
resulted in substantial increases in airline revenues.
Finally, consider the case of being evaluated by a doctor for a potentially serious 
medical issue. Hundreds of medical papers may describe research studies done on patients 
facing similar diagnoses and thousands of data points exist on their outcomes. However, 
it is extremely unlikely that your doctor has read every one of these research papers or is 
aware of all previous patient outcomes. Instead of relying only on her medical training and 
knowledge gained from her limited set of previous patients, wouldn’t it be better for your 
doctor to have access to the expertise and patient history of thousands of doctors around 
the world?
In 2007, a group of IBM computer scientists initiated a project to develop a new deci-
sion technology to help in answering these types of questions. That technology is called 
Watson, named after the founder of IBM, Thomas J. Watson. The team at IBM focused on 
one aim: how the vast amounts of data now available on the Internet can be used to make 
more data-driven, smarter decisions.
Watson became a household name in 2011, when it famously won the television game 
show, Jeopardy! Since that proof of concept in 2011, IBM has reached agreements with 
the health insurance provider WellPoint, the financial services company Citibank, and 
Memorial Sloan-Kettering Cancer Center to apply Watson to the decision problems that 
they face.
Watson is a system of computing hardware, high-speed data processing, and analytical 
algorithms that are combined to make data-based recommendations. As more and more 
data are collected, Watson has the capability to learn over time. In simple terms, accord-
ing to IBM, Watson gathers hundreds of thousands of possible solutions from a huge data 
bank, evaluates them using analytical techniques, and proposes only the best solutions for 
consideration. Watson provides not just a single solution, but a range of good solutions with 
a confidence level for each.
For example, at WellPoint’s Virginia data center, to the delight of doctors and patients, 
Watson is already being used to speed up the approval of medical procedures. Citibank is 

 
Chapter 1 Introduction 
3
beginning to explore how to use Watson to better serve its customers, and Sloan-Kettering 
is launching a pilot study to assess the effectiveness of Watson in assisting with the diag-
nosis and treatment of patients.1
This book is concerned with data-driven decision making and the use of analytical 
approaches in the decision-making process. Three developments spurred recent explo-
sive growth in the use of analytical methods in business applications. First, technological 
advances, such as improved point-of-sale scanner technology and the collection of data 
through e-commerce, Internet social networks, and data generated from personal electronic 
devices, produce incredible amounts of data for businesses. Naturally, businesses want to 
use these data to improve the efficiency and profitability of their operations, better under-
stand their customers, price their products more effectively, and gain a competitive advan-
tage. Second, ongoing research has resulted in numerous methodological developments, 
including advances in computational approaches to effectively handle and explore massive 
amounts of data, faster algorithms for optimization and simulation, and more effective 
approaches for visualizing data. Third, these methodological developments were paired 
with an explosion in computing power and storage capability. Better computing hardware, 
parallel computing, and, more recently, cloud computing (the remote use of hardware and 
software over the Internet) have enabled businesses to solve big problems faster and more 
accurately than ever before.
In summary, the availability of massive amounts of data, improvements in analytic 
methodologies, and substantial increases in computing power have all come together to 
result in a dramatic upsurge in the use of analytical methods in business and a reliance on 
the  discipline that is the focus of this text: business analytics. Figure 1.1, a graph generated 
by Google Trends, displays the search volume for the word analytics from 2004 to 2013 
(projected) on a percentage basis from the peak. The figure clearly illustrates the recent 
increase in interest in analytics.
Business analytics is a crucial area of study for students looking to enhance their em-
ployment prospects. By 2018, it is predicted that there will be a shortage of more than 
1.5 million business managers with adequate training in analytics in the United States 
1“IBM’s Watson Is Learning Its Way to Saving Lives,” Fastcompany Web site, December 8, 2012.
FIGURE 1.1   GOOGLE TRENDS GRAPH OF SEARCHES ON THE TERM AnAlytics
The number 100 represents the peak search volume
2005
22
44
66
88
110
2007
2009
2011
2013
Interest over time

4 
Chapter 1 Introduction
alone.2 As stated in the Preface, the purpose of this text is to provide students with a sound 
conceptual understanding of the role that business analytics plays in the decision-making 
process. To reinforce the applications orientation of the text and to provide a better under-
standing of the variety of applications in which analytical methods have been used suc-
cessfully, Analytics in Action articles are presented throughout the book. Each Analytics 
in Action article summarizes an application of analytical methods in practice. For example, 
the first Analytics in Action, Procter & Gamble Uses Business Analytics to Redesign its 
supply chain (later in this chapter) describes how analytics was used to drive efficiency in 
Procter & Gamble’s North American supply chain.
Decision Making
It is the responsibility of managers to plan, coordinate, organize, and lead their organiza-
tions to better performance. Ultimately, managers’ responsibilities require that they make 
strategic, tactical, or operational decisions. Strategic decisions involve higher-level issues 
concerned with the overall direction of the organization; these decisions define the orga-
nization’s overall goals and aspirations for the future. Strategic decisions are usually the 
domain of higher-level executives and have a time horizon of three to five years. Tactical 
decisions concern how the organization should achieve the goals and objectives set by its 
strategy, and they are usually the responsibility of midlevel management. Tactical decisions 
usually span a year and thus are revisited annually or even every six months. Operational 
decisions affect how the firm is run from day to day; they are the domain of operations 
managers, who are the closest to the customer.
Consider the case of the Thoroughbred Running Company (TRC). Historically, TRC 
had been a catalog-based retail seller of running shoes and apparel. TRC sales revenue 
grew quickly as it changed its emphasis from catalog-based sales to Internet-based sales. 
Recently, TRC decided that it should also establish retail stores in the malls and downtown 
areas of major cities. This is a strategic decision that will take the firm in a new direc-
tion that it hopes will complement its Internet-based strategy. TRC middle managers will 
therefore have to make a variety of tactical decisions in support of this strategic decision, 
including how many new stores to open this year, where to open these new stores, how 
many distribution centers will be needed to support the new stores, and where to locate 
these distribution centers. Operations managers in the stores will need to make day-to-day 
decisions regarding, for instance, how many pairs of each model and size of shoes to order 
from the distribution centers and how to schedule their sales personnel.
Regardless of the level within the firm, decision making can be defined as the follow-
ing process:
1. Identify and define the problem
2. Determine the criteria that will be used to evaluate alternative solutions
3. Determine the set of alternative solutions
4. Evaluate the alternatives
5. Choose an alternative
Step 1 of decision making, identifying and defining the problem, is the most critical. Only 
if the problem is well-defined, with clear metrics of success or failure (step 2), can a proper 
approach for solving the problem (steps 3 and 4) be devised. Decision making concludes 
with the choice of an alternative (step 5).
1.1
if i were given one hour 
to save the planet, i would 
spend 59 minutes defining 
the problem and one minute 
resolving it.
—Albert Einstein
2J. Manyika et al., “Big Data: The Next Frontier for Innovation, Competition and Productivity,” McKinsey Global Institute 
Report, 2011.

There are a number of approaches to making decisions: tradition (“We’ve always 
done it this way”), intuition (“gut feeling”), and rules of thumb (“As the restaurant owner, 
I schedule twice the number of waiters and cooks on holidays”). The power of each of 
these approaches should not be underestimated. Managerial experience and intuition are 
valuable inputs to making decisions, but what if relevant data were available to help us 
make more informed decisions? With the vast amounts of data now generated and stored 
electronically, it is estimated that the amount of data stored by businesses more than 
doubles every two years. How can managers convert these data into knowledge that they 
can use to be more efficient and effective in managing their businesses?
Business Analytics Defined
What makes decision making difficult and challenging? Uncertainty is probably the number 
one challenge. If we knew how much the demand will be for our product, we could do a 
much better job of planning and scheduling production. If we knew exactly how long each 
step in a project will take to be completed, we could better predict the project’s cost and 
completion date. If we knew how stocks will perform, investing would be a lot easier.
Another factor that makes decision making difficult is that we often face such an enor-
mous number of alternatives that we cannot evaluate them all. What is the best combina-
tion of stocks to help me meet my financial objectives? What is the best product line for a 
company that wants to maximize its market share? How should an airline price its tickets 
so as to maximize revenue?
Business analytics is the scientific process of transforming data into insight for making 
better decisions.3 Business analytics is used for data-driven or fact-based decision making, 
which is often seen as more objective than other alternatives for decision making.
As we shall see, the tools of business analytics can aid decision making by creating 
 insights from data, by improving our ability to more accurately forecast for planning, by 
helping us quantify risk, and by yielding better alternatives through analysis and opti-
mization. Indeed, a recent study based on a large sample of firms that was conducted by 
researchers at MIT’s Sloan School of Management and the University of Pennsylvania, 
concluded that firms guided by data-driven decision making have higher productivity and 
market value and increased output and profitability.4
A Categorization of Analytical Methods 
and Models
Business analytics can involve anything from simple reports to the most advanced optimi-
zation techniques (methods for finding the best course of action). Analytics is generally 
thought to comprise three broad categories of techniques: descriptive analytics, predictive 
analytics, and prescriptive analytics.
Descriptive Analytics
Descriptive analytics encompasses the set of techniques that describes what has happened 
in the past. Examples are data queries, reports, descriptive statistics, data visualization 
1.2
some firms and industries 
use the simpler term, 
analytics. Analytics is often 
thought of as a broader 
 category than business 
analytics,  encompassing 
the use of analytical 
techniques in the sciences 
and engineering as well. 
in this text, we use business 
analytics and analytics 
synonymously.
1.3
3 We adopt the definition of analytics developed by the Institute for Operations Research and the Management Sciences 
(INFORMS).
4E. Brynjolfsson, L. M. Hitt, and H. H. Kim, “Strength in Numbers: How Does Data-Driven Decisionmaking  Affect Firm Perfor-
mance?” (April 18, 2013). Available at SSRN http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1819486.
 
1.3 A Categorization of Analytical Methods and Models 
5

6 
Chapter 1 Introduction
including data dashboards, some data-mining techniques, and basic what-if spreadsheet 
models.
A data query is a request for information with certain characteristics from a database. 
For example, a query to a manufacturing plant’s database might be for all records of ship-
ments to a particular distribution center during the month of March. This query provides 
descriptive information about these shipments: the number of shipments, how much was in-
cluded in each shipment, the date each shipment was sent, and so on. A report summarizing 
relevant historical information for management might be conveyed by the use of descriptive 
statistics (means, measures of variation, etc.) and data visualization tools (tables, charts, and 
maps). Simple descriptive statistics and data visualization techniques can be used to find 
patterns or relationships in a large database.
Data dashboards are collections of tables, charts, maps, and summary statistics that 
are updated as new data become available. Dashboards are used to help management moni-
tor specific aspects of the company’s performance related to their decision-making respon-
sibilities. For corporate-level managers, daily data dashboards might summarize sales by 
region, current inventory levels, and other company-wide metrics; front-line managers may 
view dashboards that contain metrics related to staffing levels, local inventory levels, and 
short-term sales forecasts.
Predictive Analytics
Predictive analytics consists of techniques that use models constructed from past data to 
predict the future or ascertain the impact of one variable on another. For example, past data 
on product sales may be used to construct a mathematical model to predict future sales, 
which can factor in the product’s growth trajectory and seasonality based on past patterns. 
A packaged food manufacturer may use point-of-sale scanner data from retail outlets to 
help in estimating the lift in unit sales due to coupons or sales events. Survey data and past 
purchase behavior may be used to help predict the market share of a new product. All of 
these are applications of predictive analytics.
Linear regression, time series analysis, some data-mining techniques, and simulation, 
often referred to as risk analysis, all fall under the banner of predictive analytics. We 
discuss all of these techniques in greater detail later in this text.
Data mining, techniques used to find patterns or relationships among elements of the 
data in a large database, is often used in predictive analytics. For example, a large grocery 
store chain might be interested in developing a new targeted marketing campaign that of-
fers a discount coupon on potato chips. By studying historical point-of-sale data, the store 
may be able to use data mining to predict which customers are the most likely to respond 
to an offer on discounted chips by purchasing higher-margin items such as beer or soft 
drinks in addition to the chips, thus increasing the store’s overall revenue.
Simulation involves the use of probability and statistics to construct a computer model 
to study the impact of uncertainty on a decision. For example, banks often use simulation to 
model investment and default risk in order to stress test financial models. Simulation is also 
often used in the pharmaceutical industry to assess the risk of introducing a new drug.
Prescriptive Analytics
Prescriptive analytics differ from descriptive or predictive analytics in that prescriptive 
 analytics indicate a best course of action to take; that is, the output of a prescriptive model 
is a best decision. The airline industry’s use of revenue management is an example of a 
prescriptive analytics. Airlines use past purchasing data as inputs into a model that recom-
mends the best pricing strategy across all flights for maximizing revenue.
Other examples of prescriptive analytics are portfolio models in finance, supply net-
work design models in operations, and price markdown models in retailing. Portfolio 
Appendix B at the end of 
this book describes how 
to use Microsoft Access to 
conduct data queries. 

models use historical investment return data to determine the mix of investments that 
yield the highest expected return while controlling or limiting exposure to risk. Sup-
ply network design models provide the cost-minimizing plant and distribution center 
locations subject to meeting the customer service requirements. Given historical data, 
retail price markdown models yield revenue-maximizing discount levels and the timing 
of discount offers when goods have not sold as planned. All of these models are known 
as optimization models, that is, models that give the best decision subject to constraints 
of the situation.
Another type of modeling in the prescriptive analytics category is simulation 
 optimization, which combines the use of probability and statistics to model uncertainty 
with optimization techniques to find good decisions in highly complex and highly un-
certain settings. Finally, the techniques of decision analysis can be used to develop an 
optimal strategy when a decision maker is faced with several decision alternatives and 
an uncertain set of future events. Decision analysis also employs utility theory, which 
assigns values to outcomes based on the decision maker’s attitude toward risk, loss, and 
other factors.
In this text we cover all three areas of business analytics: descriptive, predictive, and 
prescriptive. Table 1.1 shows how the chapters cover the three categories.
 
1.3 A Categorization of Analytical Methods and Models 
7
Consumer goods giant Procter & Gamble (P&G), the 
maker of such well-known brands as Tide, Olay, Crest, 
Bounty, and Pampers, sells its products in over 180 
countries around the world. Supply chain coordination 
and efficiency are critical to the company’s profitabil-
ity. After many years of acquisitions and growth, P&G 
embarked on a effort known as Strengthening Global 
 Effectiveness. A major piece of that effort was the North 
American Supply Chain Study, whose purpose was to 
make the supply chain in North America as efficient as 
possible, while ensuring that customer service require-
ments were met.
A team of P&G analysts and managers partnered 
with a group of analytics faculty at the University of 
Cincinnati to create a system to help managers redesign 
PROctER & GAMBlE UsEs BUsinEss AnAlytics tO REDEsiGn its sUPPly cHAin5
ANALYTICS  in  ACTION
5J. Camm, T. Chorman, F. Dill, J. Evans, D. Sweeney, and G. Wegryn, 
“Blending OR/MS, Judgment and GIS: Restructuring P&G’s Supply 
Chain,” Interfaces 27, no. 1 (1997): 83–97.
Chapter
Title
Descriptive
Predictive
Prescriptive
 1
Introduction
●
●
●
 2
Descriptive Statistics
●
 3
Data Visualization
●
 4
Linear Regression
●
●
 5
Time Series Analysis & Forecasting
●
 6
Data Mining
●
●
 7
Spreadsheet Models
●
 8
Linear Optimization Models
●
 9
Integer Linear Optimization Models
●
10
Nonlinear Optimization Models
●
11
Monte Carlo Simulation
●
●
12
Decision Analysis
●
TABLE 1.1  COVERAGE OF BUSINESS ANALYTICS TOPICS IN THIS TEXT

8 
Chapter 1 Introduction
Big Data
Like the explosion of interest in analytics, interest in what is known as big data has recently 
increased dramatically. Big data is simply a set of data that cannot be managed, processed, 
or analyzed with commonly available software in a reasonable amount of time. Walmart 
handles over one million purchase transactions per hour. Facebook processes more than 
250 million picture uploads per day. Five billion cell-phone owners around the world gen-
erate vast amounts of data by calling, texting, tweeting and browsing the web on a daily 
basis.6 As Google CEO Eric Schmidt has noted,7 the amount of data currently created every 
48 hours is equivalent to the entire amount of data created from the dawn of civilization 
until the year 2003. Perhaps it is not surprising that 90 percent of the data in the world today 
has been created in the last two years.8
Businesses are interested in understanding and using data to gain a competitive ad-
vantage. Although big data represents opportunities, it also presents analytical challenges 
from a processing point of view and consequently has itself led to an increase in the use of 
analytics. More companies are hiring data scientists who know how to process and analyze 
massive amounts of data. However, it is important to understand that in some sense big 
data issues are a subset of analytics and that many very valuable applications of analytics 
do not involve big data.
1.4
8“Bringing Big Data to the Enterprise,” IBM Website. Available at http://www-01.ibm.com/software/data/bigdata/, 
 retrieved December 1, 2012.
6 SAS White Paper, “Big Data Meets Big Data Analytics,” SAS Institute, 2012.
7E. Schmidt, Panel discussion at Technomy Conference, Lake Tahoe, CA, August 4, 2010.
the supply effort in North America. The fundamental 
questions to be answered were: (1) Which plants should 
make which product families? (2) Where should the dis-
tribution centers be located? (3) Which plants should 
serve which distribution centers? (4) Which custom-
ers should be served by each distribution center? The 
team’s approach utilized all three categories of business 
analytics: descriptive, predictive, and prescriptive.
At the start of the study, data had to be collected 
from all aspects of the supply chain. These included 
demand by product family, fixed and variable pro-
duction costs by plant, and freight costs and handling 
charges at the distribution centers. Data queries and 
descriptive statistics were utilized to acquire and bet-
ter understand the current supply chain data. Data vi-
sualization, in the form of a geographic information 
system, allowed the proposed solutions to be displayed 
on a map for more intuitive interpretation by manage-
ment. Because the supply chain had to be redesigned 
for the future, predictive analytics was used to fore-
cast product family demand by three-digit zip code for 
ten years into the future. This future demand was then 
input, along with projected freight and other relevant 
costs, into an interactive optimization model, that min-
imized cost subject to service constraints. The suite of 
analytical models was aggregated into a single system 
that could be run quickly on a laptop computer. P&G 
product category managers made over a thousand runs 
of the system before reaching consensus on a small 
set of alternative designs. Each proposed design in this 
selected set was then subjected to a risk analysis using 
computer simulation, ultimately leading to a single go-
forward design.
The chosen redesign of the supply chain was imple-
mented over time and led to a documented savings in ex-
cess of $250 million per year in P&G’s North American 
supply chain. The system of models was later utilized 
to streamline the supply chains in Europe and Asia, and 
P&G has become a world leader in the use of analytics 
in supply chain management.

Business Analytics in Practice
Business analytics involves tools as simple as reports and graphs, as well as some that are 
as sophisticated as optimization, data mining, and simulation. In practice, companies that 
apply analytics often follow a trajectory similar to that shown in Figure 1.2. Organizations 
start with basic analytics in the lower left. As they realize the advantages of these analytic 
techniques, they often progress to more sophisticated techniques in an effort to reap the 
 derived competitive advantage. Predictive and prescriptive analytics are sometimes there-
fore referred to as advanced analytics. Not all companies reach that level of usage, but 
those that embrace analytics as a competitive strategy often do.
Analytics has been applied in virtually all sectors of business and government. Organi-
zations such as Procter & Gamble, IBM, UPS, Netflix, Amazon.com, Google, the Internal 
Revenue Service, and General Electric have embraced analytics to solve important prob-
lems or to achieve competitive advantage. In this section, we briefly discuss some of the 
types of applications of analytics by application area.
Financial Analytics
Applications of analytics in finance are numerous and pervasive. Predictive models are used 
to forecast future financial performance, to assess the risk of investment portfolios and proj-
ects, and to construct financial instruments such as derivatives. Prescriptive models are used 
to construct optimal portfolios of investments, to allocate assets, and to create optimal capital 
budgeting plans. For example, GE Asset Management uses optimization models to decide 
how to invest its own cash received from insurance policies and other financial products, as 
well as the cash of its clients such as Genworth Financial. The estimated benefit from the 
optimization models was $75 million over a five-year period.9 Simulation is also often used 
to assess risk in the financial sector; one example is the deployment by Hypo Real Estate 
International of simulation models to successfully manage commercial real estate risk.10
1.5
9 L. C. Chalermkraivuth et al., “GE Asset Management, Genworth Financial, and GE Insurance Use a Sequential-Linear 
 Programming Algorithm to Optimize Portfolios,” Interfaces 35, no. 5 (September–October 2005): 370–80.
10Y. Jafry, C. Marrison, and U. Umkehrer-Neudeck, “Hypo International Strengthens Risk Management with a Large-Scale, 
Secure Spreadsheet-Management Framework,” Interfaces 38, no. 4 (July–August 2008): 281–88.
Degree of Complexity
Standard Reporting
Data Query
Data Visualization
Descriptive Statistics
Data Mining
Forecasting
Predictive Modeling
Simulation
Decision Analysis
Optimization
Competitive Advantage
Prescriptive
Predictive
Descriptive
FIGURE 1.2   THE SPECTRUM OF BUSINESS ANALYTICS
source: Adapted from SAS.
 
1.5 Business Analytics in Practice 
9

10 
Chapter 1 Introduction
Human Resource (HR) Analytics
A relatively new area of application for analytics is the management of an organization’s 
human resources (HR). The HR function is charged with ensuring that the organization 
(1) has the mix of skill sets necessary to meet its needs, (2) is hiring the highest-quality 
talent and providing an environment that retains it, and (3) achieves its organizational 
diversity goals. Sears Holding Corporation (SHC), owners of retailers Kmart and Sears, 
Roebuck and Company, has created an HR analytics team inside its corporate HR function. 
The team uses descriptive and predictive analytics to support employee hiring and to track 
and influence retention.11
Marketing Analytics
Marketing is one of the fastest growing areas for the application of analytics. A better 
understanding of consumer behavior through the use of scanner data and data generated 
from social media has led to an increased interest in marketing analytics. As a result, de-
scriptive, predictive, and prescriptive analytics are all heavily used in marketing. A better 
understanding of consumer behavior through analytics leads to the better use of advertis-
ing budgets, more effective pricing strategies, improved forecasting of demand, improved 
product line management, and increased customer satisfaction and loyalty. For example, 
each year, NBC-Universal uses a predictive model to help support its annual up-front 
market. The upfront market is a period in late May when each television network sells 
the majority of its on-air advertising for the upcoming television season. Over 200 NBC 
sales and finance personnel use the results of the forecasting model to support pricing and 
sales decisions.12
In another example of high-impact marketing analytics, automobile manufacturer 
Chrysler teamed with J. D. Power and Associates to develop an innovate set of predic-
tive models to support its pricing decisions for automobiles. These models help Chrysler 
to better understand the ramifications of proposed pricing structures (a combination of 
manufacturer’s suggested retail price, interest rate offers, and rebates) and, as a result, to 
improve its pricing decisions. The models have generated an estimated annual savings of 
$500 million.13
Figure 1.3 shows the Google Trends graph for Marketing, Financial, and HR Analytics. 
While interest in each of these three areas of business is increasing, the graph clearly shows 
the pronounced increase in the interest in marketing analytics.
Health Care Analytics
The use of analytics in health care is on the increase because of pressure to simultaneously 
control cost and provide more effective treatment. Descriptive, predictive, and prescriptive 
analytics are used to improve patient, staff, and facility scheduling; patient flow; purchas-
ing; and inventory control. A study by McKinsey Global Institute (MGI) and McKinsey & 
Company14 estimates that the health care system in the United States could save more than 
$300 billion per year by better utilizing analytics; these savings are approximately the 
equivalent of the entire gross domestic product of countries such as Finland, Singapore, 
and Ireland.
13J. Silva-Risso et al., “Chrysler and J. D. Power: Pioneering Scientific Price Customization in the Automobile Industry,” 
Interfaces 38, no. 1 (January–February 2008): 26–39.
14J. Manyika et al., “Big Data: The Next Frontier for Innovation, Competition and Productivity,” McKinsey Global Institute 
Report, 2011.
11 T. H. Davenport, ed., Enterprise Analytics (Upper Saddle River, NJ: Pearson Education Inc., 2013).
12S. Bollapragada et al., “NBC-Universal Uses a Novel Qualitative Forecasting Technique to Predict Advertising Demand,” 
Interfaces 38, no. 2 (March–April 2008): 103–11.

The use of prescriptive analytics for diagnosis and treatment is relatively new, but it may 
prove to be the most important application of analytics in health care. For example, working 
with the Georgia Institute of Technology, Memorial Sloan-Kettering Cancer  Center devel-
oped a real-time prescriptive model to determine the optimal placement of  radioactive seeds 
for the treatment of prostate cancer.15 Using the new model, 20–30 percent fewer seeds are 
needed, resulting in a faster and less invasive procedure. 
Supply Chain Analytics
One of the earliest applications of analytics was in logistics and supply chain management. 
The core service of companies such as UPS and FedEx is the efficient delivery of goods, and 
analytics has long been used to achieve efficiency. The optimal sorting of goods, vehicle 
and staff scheduling, and vehicle routing are all key to profitability for logistics companies 
such as UPS, FedEx, and others like them.
Companies can benefit from better inventory and processing control and more effi-
cient supply chains. Analytic tools used in this area span the entire spectrum of analyt-
ics. For example, the women’s apparel manufacturer Bernard Claus, Inc., has successfully 
used  descriptive analytics to present the status of its supply chain to managers visually.16 
ConAgra Foods uses predictive and prescriptive analytics to better plan capacity utiliza-
tion by incorporating the inherent uncertainty in commodities pricing. ConAgra realized a 
100 percent return on their investment in analytics in under three months17—an unheard of 
result for a major technology investment.
Analytics for Government and Nonprofits
Government agencies and other nonprofits have used analytics to drive out inefficiencies 
and increase the effectiveness and accountability of programs. Indeed, much of advanced 
analytics has its roots in the U.S. and English military dating back to World War II. Today, 
the use of analytics in government is becoming pervasive in everything from elections to 
FIGURE 1.3   GOOGLE TRENDS FOR MARKETING, FINANCIAL, AND HUMAN RESOURCE 
 ANALYTICS, 2004–2012
17“ConAgra Mills: Up-to-the-Minute Insights Drive Smarter Selling Decisions and Big Improvements in Capacity Utilization,” 
IBM Smarter Planet Leadership Series. Available at: http://www.ibm.com/smarterplanet/us/en/leadership/conagra/, 
 retrieved December 1, 2012.
15 E. Lee and M. Zaider, “Operations Research Advances Cancer Therapeutics,” Interfaces 38, no. 1 (January–February 
2008): 5–25.
16T. H. Davenport, ed., Enterprise Analytics (Upper Saddle River, NJ: Pearson Education Inc., 2013).
 
1.5 Business Analytics in Practice 
11
The number 100 represents the peak search volume
Marketing analytics
Financial analytics
HR analytics
Average
20
60
40
80
100
2006
2007
2005
2008
2009
2010
2011
2012
Interest over time
3
3
3

12 
Chapter 1 Introduction
tax collection. For example, the New York State Department has worked with IBM to use 
prescriptive analytics in the development of a more effective approach to tax collection. The 
result was an increase in collections from delinquent payers of $83 million over two years.18 
The U.S. Internal Revenue Service has used data mining to identify patterns that distinguish 
questionable annual personal income tax filings. In one application, the IRS combines its 
data on individual taxpayers with data, received from banks, on mortgage payments made 
by those taxpayers. When taxpayers report a mortgage payment that is unrealistically high 
relative to their reported taxable income, they are flagged as possible underreporters of tax-
able income. The filing is then further scrutinized and may trigger an audit.
Likewise, nonprofit agencies have used analytics to ensure their effectiveness and 
 accountability to their donors and clients. Catholic Relief Services (CRS) is the official 
international humanitarian agency of the U.S. Catholic community. The CRS mission is to 
provide relief for the victims of both natural and human-made disasters and to help people 
in need around the world through its health, educational, and agricultural programs. CRS 
uses an analytical spreadsheet model to assist in the allocation of its annual budget based 
on the impact that its various relief efforts and programs will have in different countries.19
Sports Analytics
The use of analytics in sports has gained considerable notoriety since 2003 when renowned 
author Michael Lewis published Moneyball, the story of how the Oakland Athletics used 
an analytical approach to player evaluation in order to assemble a competitive team with 
a limited budget. The use of analytics for player evaluation and on-field strategy is now 
common, especially in professional sports. Examples are professional sports teams that use 
analytics to assess players for the amateur drafts20 and to decide how much to offer players 
in contract negotiations; professional motorcycle racing teams that use sophisticated opti-
mization for gearbox design to gain competitive advantage21; and teams that use analytics 
to assist with on-field decisions such as which pitchers to use in various games of a Major 
League Baseball playoff series.
The use of analytics for off-the-field business decisions is also increasing rapidly. En-
suring customer satisfaction is important for any company, and fans are the customers of 
sports teams. Following the lead of Marriott Hotels, which created the Courtyard by Marriott, 
“designed by business travelers for business travelers,” the Cleveland Indians professional 
baseball team used a type of predictive modeling known as conjoint analysis to design its 
premium seating offerings at Progressive Field based on fan survey data. Using prescriptive 
analytics, franchises across several major sports dynamically adjust ticket prices throughout 
the season to reflect the relative attractiveness and potential demand for each game.
Web Analytics
Web analytics is the analysis of online activity, which includes, but is not limited to, visits to 
Web sites and social media sites such as Facebook and LinkedIn. Web analytics obviously 
has huge implications for promoting and selling products and services via the Internet. 
Leading companies apply descriptive and advanced analytics to data collected in online 
experiments to determine the best way to configure Web sites, position ads, and utilize so-
cial networks for the promotion of products and services. Online experimentation involves 
20N. Streib, S. J. Young, and J. Sokol, “A Major League Baseball Team Uses Operations Research to Improve Draft 
 Preparation,” Interfaces 42, no. 2 (March–April 2012): 119–30.
21J. Amoros, L. F. Escudero, J. F. Monge, J. V. Segura, and O. Reinoso. “TEAM ASPAR Uses Binary Optimization to Obtain 
Optimal Gearbox Ratios in Motorcycle Racing,” Interfaces 42, no. 2 (March–April 2012): 191–98.
18 G. Miller et al., “Tax Collection Optimization for New York State,” Interfaces 42, no. 1 (January–February 2013): 74–84.
19I. Gamvros, R. Nidel, and S. Raghavan, “Investment Analysis and Budget Allocation at Catholic Relief Services,” Interfaces 
36. no. 5 (September–October 2006): 400–406.

exposing various subgroups to different versions of a Web site and tracking the results. Be-
cause of the massive pool of Internet users, experiments can be conducted without risking 
the disruption of the overall business of the company. Such experiments are proving to be 
invaluable because they enable the company to use trial-and-error in determining statisti-
cally what makes a difference in their Web site traffic and sales.
Summary
This introductory chapter began with a discussion of decision making. Decision making can 
be defined as the following process: (1) identify and define the problem; (2) determine the 
criteria that will be used to evaluate alternative solutions; (3) determine the set of alterna-
tive solutions; (4) evaluate the alternatives; and (5) choose an alternative. Decisions may be 
strategic (high-level, concerned with the overall direction of the firm), tactical (midlevel, 
concerned with how to achieve the strategic goals of the firm), or operational (day-to-day 
decisions that must be made to run the company).
Uncertainty and an overwhelming number of alternatives are two key factors that make 
decision making difficult. Business analytics approaches can assist by identifying and miti-
gating uncertainty and by prescribing the best course of action from a very large number 
of alternatives. In short, business analytics can help us make better informed decisions.
There are three categories of analytics: descriptive, predictive, and prescriptive. De-
scriptive analytics describes what has happened and includes tools such as reports, data 
visualization, data dashboards, descriptive statistics, and some data-mining techniques. 
Predictive analytics consists of techniques that use past data to predict future events and 
include regression, data mining, forecasting, and simulation. Prescriptive analytics uses 
input data to determine a best course of action. This class of analytical techniques includes 
simulation, decision analysis, and optimization. Descriptive and predictive analytics can 
help us better understand the uncertainty and risk associated with our decision alternatives. 
Predictive and prescriptive analytics, also often referred to as advanced analytics, can help 
us make the best decision when facing a myriad of alternatives.
Big data is a set of data that cannot be managed, processed, or analyzed with commonly 
available software in a reasonable amount of time. The increasing prevalence of big data is 
leading to an increase in the use of analytics. The Internet, retail scanners, and cell phones 
are making huge amounts of data available to companies, and these companies want to 
better understand these data. Business analytics is helping them understand these data and 
use them to make better decisions.
We concluded this chapter with a discussion of various application areas of analytics. 
Our discussion focused on financial analytics, human resource analytics, marketing analyt-
ics, health care analytics, supply chain analytics, analytics for government and nonprofit 
organizations, sports analytics, and Web analytics. However, the use of analytics is rapidly 
spreading to other sectors, industries, and functional areas of organizations. Each remaining 
chapter in this text will provide a real-world vignette in which business analytics is applied 
to a problem faced by a real organization.
Glossary
Strategic decision A decision that involves higher-level issues and that is concerned with 
the overall direction of the organization, defining the overarching goals and aspirations for 
the organization’s future.
Tactical decision A decision concerned with how the organization should achieve the goals 
and objectives set by its strategy. 
 
Glossary 
13

14 
Chapter 1 Introduction
Operational decision A decision concerned with how the organization is run from day 
to day.
Business analytics The scientific process of transforming data into insight for making 
better decisions.
Descriptive analytics Analytical tools that describe what has happened. 
Data query A request for information with certain characteristics from a database.
Data dashboard A collection of tables, charts, and maps to help management monitor 
selected aspects of the company’s performance.
Predictive analytics Techniques that use models constructed from past data to predict the 
future or to ascertain the impact of one variable on another.
Data mining Techniques used to find patterns or relationships among elements of the data 
in a large database.
Simulation The use of probability and statistics to construct a computer model to study the 
impact of uncertainty on the decision at hand.
Prescriptive analytics Techniques that take input data and yield a best course of action.
Optimization model A mathematical model that gives the best decision, subject to the 
situation’s constraints.
Simulation optimization The use of probability and statistics to model uncertainty, com-
bined with optimization techniques, to find good decisions in highly complex and highly 
uncertain settings.
Decision analysis A technique used to develop an optimal strategy when a decision maker 
is faced with several decision alternatives and an uncertain set of future events.
Utility theory The study of the total worth or relative desirability of a particular outcome 
that reflects the decision maker’s attitude toward a collection of factors such as profit, loss, 
and risk.
Big data A set of data that cannot be managed, processed, or analyzed with commonly 
available software in a reasonable amount of time.
Advanced analytics Predictive and prescriptive analytics.

Descriptive Statistics
CONTENTS
2.1 
 OVERVIEW OF USING DATA: 
DEFINITIONS AND GOALS
2.2 
TYPES OF DATA
Population and Sample Data
Quantitative and Categorical 
Data
Cross-Sectional and Time Series 
Data
Sources of Data
2.3 
MODIFYING DATA IN EXCEL
Sorting and Filtering Data in 
Excel
Conditional Formatting of Data 
in Excel
2.4   CREATING DISTRIBUTIONS 
FROM DATA
Frequency Distributions for 
Categorical Data
Relative Frequency and Percent 
Frequency Distributions
Frequency Distributions for 
Quantitative Data
Histograms
Cumulative Distributions
2.5 
MEASURES OF LOCATION
Mean (Arithmetic Mean)
Median
Mode
Geometric Mean
2.6 
MEASURES OF VARIABILITY
Range
Variance
Standard Deviation
Coefficient of Variation
2.7 
 ANALYZING 
 DISTRIBUTIONS
Percentiles
Quartiles
z-scores
Empirical Rule
Identifying Outliers
Box Plots
2.8 
 MEASURES OF 
 ASSOCIATION BETWEEN 
TWO VARIABLES
Scatter Charts
Covariance
Correlation Coefficient
APPENDIX:  CREATING BOX PLOTS 
IN XLMINER
CHAPTER 2

16 
Chapter 2 Descriptive Statistics
Overview of Using Data: 
 Definitions and Goals
Data are the facts and figures collected, analyzed, and summarized for presentation and 
interpretation. Table 2.1 shows a data set containing 2013 information for stocks in the Dow 
Jones Industrial Index (or simply The Dow). The Dow is tracked by many financial advisors 
and investors as an indication of the state of the overall financial markets and the economy 
in the United States. The share prices for the 30 companies listed in Table 2.1 are the basis 
for computing The Dow Jones Industrial Average (DJI), which is tracked continuously by 
virtually every financial publication.
A characteristic or a quantity of interest that can take on different values is known as 
a variable; for the data in Table 2.1, the variables are Symbol, Industry, Share Price, and 
Volume. An observation is a set of values corresponding to a set of variables; each row in 
Table 2.1 corresponds to an observation.
Practically every problem (and opportunity) that an organization (or individual) faces 
is concerned with the impact of the possible values of relevant variables on the business 
outcome. Thus, we are concerned with how the value of a variable can vary; variation is 
the difference in a variable measured over observations (time customers items etc )
2.1
The Bureau of the Census is part of the United States 
Department of Commerce and is more commonly 
known as the U.S. Census Bureau. The U.S. Census Bu-
reau collects data related to the population and economy 
of the United States using a variety of methods and for 
many purposes. These data are essential to many gov-
ernment and business decisions.
Probably the best known data collected by the 
U.S. Census Bureau is the decennial census, which is 
an  effort to count the total U.S. population. Collecting 
these data is a huge undertaking involving mailings, 
door-to-door visits, and other methods. The decennial 
census collects categorical data such as the sex and race 
of the respondents, as well as quantitative data such as 
the number of people living in the household. The data 
collected in the decennial census are used to determine 
the number of U.S. House of Representatives assigned 
to each state, the number of Electoral College votes ap-
portioned to each state, and how federal government 
funding is divided among communities.
The U.S. Census Bureau also administers the Current 
Population Survey (CPS). The CPS is a cross-sectional 
monthly survey of a sample of 60,000 households used 
to estimate employment and unemployment rates in dif-
ferent geographic areas. The CPS has been administered 
since 1940, so an extensive time series of employment 
and unemployment data now exists. These data drive 
government policies such as job assistance programs. The 
estimated unemployment rates are watched closely as an 
overall indicator of the health of the U.S. economy.
The data collected by the U.S. Census Bureau are 
also very useful to businesses. Retailers use data on pop-
ulation changes in different areas to plan new store open-
ings. Mail-order catalog companies use the demographic 
data when designing targeted marketing campaigns. In 
many cases, businesses combine the data collected by 
the U.S. Census Bureau with their own data on customer 
behavior to plan strategies and to identify potential cus-
tomers. The U.S. Census Bureau is one of the most im-
portant providers of data used in business analytics.
In this chapter, we first explain the need to collect and 
analyze data and identify some common sources of data. 
Then we discuss the types of data that you may encoun-
ter in practice and present several numerical measures 
for summarizing data. We cover some common ways of 
manipulating and summarizing data using spreadsheets. 
We then develop numerical summary measures for data 
sets consisting of a single variable. When a data set con-
tains more than one variable, the same numerical mea-
sures can be computed separately for each variable. In 
the two-variable case, we also develop measures of the 
relationship between the variables.
U.S. CenSUS BUreaU
ANALYTICS  in  ACTION

 
2.2 Types of Data 
17
Table 2.1  DATA FOR DOW JONES INDUSTRIAL INDEX COMPANIES
Company
Symbol
Industry
Share Price ($)
Volume
Alcoa
AA
Manufacturing
  8.03
8,360,181
American Express
AXP
Financial
 66.83
5,020,965
Boeing
BA
Manufacturing
 87.82
3,377,781
Bank of America
BAC
Financial
 11.67
85,564,239
Caterpillar
CAT
Manufacturing
 80.60
4,418,069
Cisco Systems
CSCO
Technology
 20.47
37,824,927
Chevron Corporation
CVX
Chemical, Oil, and Gas
116.21
4,331,463
DuPont
DD
Chemical, Oil, and Gas
 48.97
5,610,522
Walt Disney
DIS
Entertainment
 61.28
5,893,711
General Electric
GE
Conglomerate
 21.81
74,030,249
The Home Depot
HD
Retail
 74.04
5,627,195
Hewlett-Packard
HPQ
Technology
 19.68
20,229,367
IBM
IBM
Technology
190.29
13,890,330
Intel
INTC
Technology
 22.38
33,303,641
Johnson & Johnson
JNJ
Pharmaceuticals
 84.04
6,094,620
JPMorgan Chase
JPM
Banking
 47.28
12,334,210
Coca-Cola
KO
Food and Drink
 42.60
8,916,978
McDonald’s
MCD
Food and Drink
 99.94
5,571,538
3M
MMM
Conglomerate
105.79
1,850,264
Merck
MRK
Pharmaceuticals
 47.18
6,601,636
Microsoft
MSFT
Technology
 29.77
76,918,154
Pfizer
PFE
Pharmaceuticals
 30.91
16,917,714
Procter & Gamble
PG
Consumer Goods
 81.42
7,894,506
AT&T
T
Telecommunications
 38.28
14,762,872
Travelers
TRV
Insurance
 84.79
1,284,813
UnitedHealth Group Inc.
UNH
Healthcare
 59.94
3,730,520
United Technologies  
 Corporation
UTX
Conglomerate
 92.92
2,466,956
Verizon Communications
VZ
Telecommunications
 52.04
9,643,848
Wal-Mart
WMT
Retail
 78.07
4,766,959
ExxonMobil
XOM
Chemical, Oil, and Gas
 87.02
9,446,864
The role of descriptive analytics is to collect and analyze data to gain a better under-
standing of variation and its impact on the business setting. The values of some variables 
are under direct control of the decision maker (these are often called decision variables, as 
discussed in Chapters 8, 9, and 10). The values of other variables may fluctuate with uncer-
tainty due to factors outside the direct control of the decision maker. In general, a quantity 
whose values are not known with certainty is called a random variable, or uncertain 
variable. When we collect data, we are gathering past observed values, or realizations of a 
variable. By collecting these past realizations of one or more variables, our goal is to learn 
more about the variation of a particular business situation.
Types of Data
Population and Sample Data
Data can be categorized in several ways based on how they are collected and the type 
collected. In many cases, it is not feasible to collect data from the population of all 
elements of interest. In such instances, we collect data from a subset of the population 
known as a sample For example with the thousands of publicly traded companies in
2.2

18 
Chapter 2 Descriptive Statistics
the United States, tracking and analyzing all of these stocks every day would be too 
time consuming and expensive. The Dow represents a sample of 30 stocks of large 
public companies based in the United States, and it is often interpreted to represent 
the larger population of all publicly traded companies. It is very important to collect 
sample data that are representative of the population data so that generalizations can be 
made from them. In most cases (although not true of the Dow), a representative sample 
can be gathered by random sampling of the population data. Dealing with popula-
tions and samples can introduce subtle differences in how we calculate and interpret 
summary statistics. In almost all practical applications of business analytics, we will 
be dealing with sample data.
Quantitative and Categorical Data
Data are considered quantitative data if numeric and arithmetic operations, such as addi-
tion, subtraction, multiplication, and division, can be performed on them. For instance, we 
can sum the values for Volume in the Dow data in Table 2.1 to calculate a total volume of 
all shares traded by companies included in the Dow. If arithmetic operations cannot be per-
formed on the data, they are considered categorical data. We can summarize categorical 
data by counting the number of observations or computing the proportions of observations 
in each category. For instance, the data in the Industry column in Table 2.1 are categori-
cal. We can count the number of companies in the Dow that are in the telecommunications 
industry. Table 2.1 shows two companies in the telecommunications industry: AT&T and 
Verizon Communications. However, we cannot perform arithmetic operations on the data 
in the Industry column. 
Cross-Sectional and Time Series Data
For statistical analysis, it is important to distinguish between cross-sectional data and 
time series data. Cross-sectional data are collected from several entities at the same, 
or  approximately the same, point in time. The data in Table 2.1 are cross-sectional be-
cause they describe the 30 companies that comprise the Dow at the same point in time 
(April 2013). Time series data are collected over several time periods. Graphs of time 
series data are frequently found in business and economic publications. Such graphs help 
analysts  understand what happened in the past, identify trends over time, and project future 
levels for the time series. For example, the graph of the time series in Figure 2.1 shows the 
DJI value from February 2002 to April 2013. The figure illustrates that the DJI was near 
10,000 in 2002 and climbed to above 14,000 in 2007. However, the financial crisis in 2008 
led to a significant decline in the DJI to between 6000 and 7000 by 2009. Since 2009, the 
DJI has been generally increasing and topped 14,000 in April 2013.
Sources of Data
Data necessary to analyze a business problem or opportunity can often be obtained with 
an appropriate study; such statistical studies can be classified as either experimental or 
observational. In an experimental study, a variable of interest is first identified. Then one 
or more other variables are identified and controlled or manipulated so that data can be 
obtained about how they influence the variable of interest. For example, if a pharmaceuti-
cal firm is interested in conducting an experiment to learn about how a new drug affects 
blood pressure, then blood pressure is the variable of interest in the study. The dosage level 
of the new drug is another variable that is hoped to have a causal effect on blood pressure. 
To obtain data about the effect of the new drug, researchers select a sample of individuals. 
The dosage level of the new drug is controlled as different groups of individuals are given 

 
2.2 Types of Data 
19
different dosage levels. Before and after the study, data on blood pressure are collected for 
each group. Statistical analysis of these experimental data can help determine how the new 
drug affects blood pressure.
nonexperimental, or observational, studies make no attempt to control the variables of 
interest. A survey is perhaps the most common type of observational study. For instance, 
in a personal interview survey, research questions are first identified. Then a question-
naire is designed and administered to a sample of individuals. Some restaurants use ob-
servational studies to obtain data about customer opinions on the quality of food, quality 
of service, atmosphere, and so on. A customer opinion questionnaire used by Chops City 
Grill in Naples, Florida, is shown in Figure 2.2. Note that the customers who fill out the 
questionnaire are asked to provide ratings for 12 variables, including overall experience, 
the greeting by hostess, the table visit by the manager, overall service, and so on. The 
response categories of excellent, good, average, fair, and poor provide categorical data 
that enable Chops City Grill management to maintain high standards for the restaurant’s 
food and service.
In some cases, the data needed for a particular application already exist from an 
 experimental or observational study already conducted. For example, companies maintain 
a variety of databases about their employees, customers, and business operations. Data 
on employee salaries, ages, and years of experience can usually be obtained from internal 
personnel records. Other internal records contain data on sales, advertising expenditures, 
distribution costs, inventory levels, and production quantities. Most companies also main-
tain detailed data about their customers.
Anyone who wants to use data and statistical analysis as aids to decision making must 
be aware of the time and cost required to obtain the data. The use of existing data sources 
is desirable when data must be obtained in a relatively short period of time. If important 
data are not readily available from a reliable existing source, the additional time and cost 
involved in obtaining the data must be taken into account. In all cases, the decision maker 
should consider the potential contribution of the statistical analysis to the decision-making 
process. In Chapter 12 we discuss methods for determining the value of additional informa-
tion that can be provided by collecting data. The cost of data acquisition and the subsequent 
statistical analysis should not exceed the savings generated by using the information to 
make a better decision.
FIGURe 2.1   DOW JONES INDEX VALUES SINCE 2002
5,0002002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
15,000
12,000
11,000
10,000
9,000
8,000
7,000
6,000
13,000
14,000
DJI Value

20 
Chapter 2 Descriptive Statistics
FIGURe 2.2   CUSTOMER OPINION QUESTIONNAIRE USED BY CHOPS CITY 
GRILL RESTAURANT
NOTES AND COMMENTS
1.  Organizations that specialize in collecting and 
maintaining data make available substantial 
amounts of business and economic data. Com-
panies can access these external data sources 
through leasing arrangements or by purchase. 
Dun & Bradstreet, Bloomberg, and Dow Jones 
& Company are three firms that provide ex-
tensive business database services to clients. 
Nielsen and SymphonyIRI Group, Inc., have 
built successful businesses collecting and pro-
cessing data that they sell to advertisers and 
product manufacturers. Data are also avail-
able from a variety of industry associations and 
 special-interest organizations.
2.  Government agencies are another important 
source of existing data. For instance, the Web 
site data.gov was launched by the U.S. govern-
ment in 2009 to make it easier for the public to 
access data collected by the U.S. federal govern-
ment. The data.gov Web site includes hundreds 
of thousands of data sets from a variety of U.S. 
federal departments and agencies. In general, 
the Internet is an important source of data and 
statistical information. One can obtain access to 
stock quotes, meal prices at restaurants, salary 
data, and a wide array of other information sim-
ply by means of an Internet search.
Date: ____________ 
Server Name: ____________
Our customers are our top priority. Please take a moment to ﬁll out our 
survey card, so we can better serve your needs. You may return this card to the front 
desk or return by mail. Thank you!
SERVICE SURVEY 
Excellent 
Good 
Average 
Fair 
Poor
Overall Experience 
❑ 
❑ 
❑ 
❑ 
❑
Greeting by Hostess 
❑ 
❑ 
❑ 
❑ 
❑
Manager (Table Visit) 
❑ 
❑ 
❑ 
❑ 
❑
Overall Service 
❑ 
❑ 
❑ 
❑ 
❑
Professionalism 
❑ 
❑ 
❑ 
❑ 
❑
Menu Knowledge 
❑ 
❑ 
❑ 
❑ 
❑
Friendliness 
❑ 
❑ 
❑ 
❑ 
❑
Wine Selection 
❑ 
❑ 
❑ 
❑ 
❑
Menu Selection 
❑ 
❑ 
❑ 
❑ 
❑
Food Quality 
❑ 
❑ 
❑ 
❑ 
❑
Food Presentation 
❑ 
❑ 
❑ 
❑ 
❑
Value for $ Spent 
❑ 
❑ 
❑ 
❑ 
❑
What comments could you give us to improve our restaurant?
Thank you, we appreciate your comments. —The staff of Chops City Grill.

 
2.3 Modifying Data in Excel 
21
Modifying Data in Excel
Projects often involve so much data that it is difficult to analyze all of the data at once. In 
this section, we examine methods for summarizing and manipulating data using Excel to 
make the data more manageable and to develop insights.
Sorting and Filtering Data in Excel
Excel contains many useful features for sorting and filtering data so that one can more easily 
identify patterns. Table 2.2 contains data on the top 20 selling automobiles in the United 
States in March 2011. The table shows the model and manufacturer of each automobile as 
well as the sales for the model in March 2011 and March 2010.
Figure 2.3 shows the data from Table 2.2 entered into an Excel spreadsheet, and the 
percent change in sales for each model from March 2010 to March 2011 has been calcu-
lated. This is done by entering the formula 5(D2-E2)/E2 in cell F2 and then copying the 
contents of this cell to cells F3 to F20. (We cannot calculate the percent change in sales for 
the Ford Fiesta because it was not being sold in March 2010.)
Suppose that we want to sort these automobiles by March 2010 sales instead of by 
March 2011 sales. To do this, we use Excel’s Sort function, as shown in the following steps.
Step 1. Select cells A1:F21
Step 2. Click the DATA tab in the Ribbon
Step 3. Click Sort in the Sort & Filter group
Step 4. Select the check box for My data has headers
Step 5. In the first Sort by dropdown menu, select Sales (March 2010)
Step 6. In the Order dropdown menu, select Largest to Smallest (see Figure 2.4)
Step 7. Click OK
2.3
Table 2.2  TOP 20 SELLING AUTOMOBILES IN UNITED STATES IN MARCH 2011
Rank  
(by March 
2011 Sales) 
Manufacturer
Model
Sales  
(March 2011)
Sales  
(March 2010)
 1
Honda
Accord
33616
29120
 2
Nissan
Altima
32289
24649
 3
Toyota
Camry
31464
36251
 4
Honda
Civic
31213
22463
 5
Toyota
Corolla/Matrix
30234
29623
 6
Ford
Fusion
27566
22773
 7
Hyundai
Sonata
22894
18935
 8
Hyundai
Elantra
19255
 8225
 9
Toyota
Prius
18605
11786
10
Chevrolet
Cruze/Cobalt
18101
10316
11
Chevrolet
Impala
18063
15594
12
Nissan
Sentra
17851
 8721
13
Ford
Focus
17178
19500
14
Volkswagon
Jetta
16969
 9196
15
Chevrolet
Malibu
15551
17750
16
Mazda
3
12467
11353
17
Nissan
Versa
11075
13811
18
Subaru
Outback
10498
 7619
19
Kia
Soul
10028
 5106
20
Ford
Fiesta
 9787
    0
Source: Manufacturers and Automotive News Data Center
file
WEB
Top20Cars

22 
Chapter 2 Descriptive Statistics
FIGURe 2.3   TOP 20 SELLING AUTOMOBILES DATA ENTERED INTO  EXCEL WITH 
 PERCENT CHANGE IN SALES FROM 2010
A
B
C
D
E
F
Rank (by March
2011 Sales)
Manufacturer
Model
Sales (March
2011)
Sales (March
2010)
Percent Change in
Sales from 2010
15.4%
31.0%
–13.2%
39.0%
2.1%
21.0%
20.9%
134.1%
57.9%
75.5%
15.8%
104.7%
–11.9%
84.5%
–12.4%
9.8%
–19.8%
37.8%
96.4%
-----
29120
24649
36251
22463
29623
22773
18935
8225
11786
10316
15594
8721
19500
9196
17750
11353
13811
7619
0
5106
33616
32289
31464
31213
30234
27566
22894
19255
18605
18101
18063
17851
17178
16969
15551
12467
11075
10498
10028
9787
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Honda
Nissan
Toyota
Honda
Toyota
Ford
Hyundai
Hyundai
Toyota
Chevrolet
Chevrolet
Nissan
Ford
Volkswagon
Chevrolet
Mazda
Nissan
Subaru
Kia
Ford
Accord
Altima
Camry
Civic
Corolla/Matrix
Fusion
Sonata
Elantra
Prius
Cruze/Cobalt
Impala
Sentra
Focus
Jetta
Malibu
3
Versa
Outback
Soul
Fiesta
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
file
WEB
Top20CarsPercent
FIGURe 2.4   USING EXCEL’S SORT FUNCTION TO SORT THE TOP SELLING AUTOMOBILES DATA
A
B
C
D
E
F
G
Rank (by March
2011 Sales)
Manufacturer
Model
Sales (March
2011)
Sales (March
2010)
Percent Change in
Sales from 2010
15.4%
31.0%
–13.2%
39.0%
2.1%
21.0%
20.9%
134.1%
57.9%
75.5%
15.8%
104.7%
–11.9%
84.5%
–12.4%
9.8%
–19.8%
37.8%
96.4%
-----
29120
24649
36251
22463
29623
22773
18935
8225
11786
10316
15594
8721
19500
9196
17750
11353
13811
7619
0
5106
33616
32289
31464
31213
30234
27566
22894
19255
18605
18101
18063
17851
17178
16969
15551
12467
11075
10498
10028
9787
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Honda
Nissan
Toyota
Honda
Toyota
Ford
Hyundai
Hyundai
Toyota
Chevrolet
Chevrolet
Nissan
Ford
Volkswagon
Chevrolet
Mazda
Nissan
Subaru
Kia
Ford
Accord
Altima
Camry
Civic
Corolla/Matrix
Fusion
Sonata
Elantra
Prius
Cruze/Cobalt
Impala
Sentra
Focus
Jetta
Malibu
3
Versa
Outback
Soul
Fiesta
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

 
2.3 Modifying Data in Excel 
23
The result of using Excel’s Sort function for the March 2010 data is shown in  Figure 2.5. 
Now we can easily see that, although the Honda Accord was the best-selling automobile 
in March 2011, both the Toyota Camry and the Toyota Corolla/Matrix outsold the Honda 
 Accord in March 2010. Note that while we sorted on Sales (March 2010), which is in 
 column E, the data in all other columns are adjusted accordingly.
Now let’s suppose that we are interested only in seeing the sales of models made by 
Toyota. We can do this using Excel’s Filter function:
Step 1. Select cells A1:F21
Step 2. Click the DATA tab in the Ribbon
Step 3. Click Filter in the Sort & Filter group
Step 4. Click on the Filter Arrow 
 in column B, next to Manufacturer
Step 5.  Select only the check box for Toyota. You can easily deselect all choices by 
unchecking (Select All)
The result is a display of only the data for models made by Toyota (see Figure 2.6). We 
now see that of the 20 top-selling models in March 2011, Toyota made three of them. We 
can further filter the data by choosing the down arrows in the other columns. We can make 
all data visible again by clicking on the down arrow in column B and checking (Select All) 
or by clicking Filter in the Sort & Filter Group again from the DATA tab.
Conditional Formatting of Data in Excel
Conditional formatting in Excel can make it easy to identify data that satisfy certain condi-
tions in a data set. For instance, suppose that we wanted to quickly identify the automobile 
FIGURe 2.5   TOP SELLING AUTOMOBILES DATA SORTED BY SALES IN MARCH 2010 SALES
A
B
C
D
E
F
Rank (by March
2011 Sales)
Manufacturer
Model
Sales (March
2011)
Sales (March
2010)
Percent Change in
Sales from 2010
Camry
–13.2%
2.1%
15.4%
31.0%
21.0%
39.0%
–11.9%
20.9%
–12.4%
15.8%
–19.8%
57.9%
9.8%
75.5%
84.5%
104.7%
134.1%
37.8%
96.4%
-----
36251
29623
29120
24649
22773
22463
19500
18935
17750
15594
13811
11786
11353
10316
9196
8721
8225
7619
0
5106
31464
30234
33616
32289
27566
31213
17178
22894
15551
18063
11075
18605
12467
18101
16969
17851
19255
10498
10028
9787
Corolla/Matrix
Accord
Altima
Fusion
3
5
1
2
6
4
Civic
13
Focus
7
Sonata
15
Malibu
11
Impala
17
Versa
9
Prius
16
3
10
Cruze/Cobalt
14
Jetta
12
Sentra
8
Elantra
18
Outback
19
Soul
20
Toyota
Toyota
Honda
Nissan
Ford
Honda
Ford
Hyundai
Chevrolet
Chevrolet
Nissan
Toyota
Mazda
Chevrolet
Volkswagon
Nissan
Hyundai
Subaru
Kia
Ford
Fiesta
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

24 
Chapter 2 Descriptive Statistics
models in Table 2.2 for which sales had decreased from March 2010 to March 2011. We 
can quickly highlight these models:
Step 1. Starting with the original data shown in Figure 2.3, select cells F1:F21
Step 2. Click on the HOME tab in the Ribbon
Step 3. Click Conditional Formatting in the Styles group
Step 4. Select Highlight Cells Rules, and click Less Than from the dropdown menu
Step 5. Enter 0% in the Format cells that are LESS THAN: box
Step 6. Click OK
The results are shown in Figure 2.7. Here we see that the models with decreasing sales 
(Toyota Camry, Ford Focus, Chevrolet Malibu, and Nissan Versa) are now clearly visible. 
Note that Excel’s Conditional Formatting function offers tremendous flexibility. Instead of 
a new feature in excel 
2013 is the Quick Analysis 
 button 
 . This  button 
 appears just outside the 
 bottom right corner of 
selected cells  whenever 
you select multiple cells. 
 Clicking on the Quick 
 Analysis button allows 
shortcuts for  Conditional 
 Formatting, adding Data 
Bars, and other operations.
FIGURe 2.6   TOP SELLING AUTOMOBILES DATA FILTERED TO SHOW ONLY  AUTOMOBILES 
 MANUFACTURED BY TOYOTA
A
B
C
D
E
F
Rank (by March
2011 Sales)
Model
Sales (March
2011)
Sales (March
2010)
Percent Change in
Sales from 2010
Toyota
31464
36251
29623
–13.2%
2.1%
57.9%
11786
30234
18605
Camry
Corolla/Matrix
Prius
3
5
Toyota
9
Toyota
1
2
3
13
Manufacturer
FIGURe 2.7   USING CONDITIONAL FORMATTING IN EXCEL TO HIGHLIGHT  AUTOMOBILES WITH 
DECLINING SALES FROM MARCH 2010
A
B
C
D
E
F
Rank (by March
2011 Sales)
Manufacturer
Model
Sales (March
2011)
Sales (March
2010)
Percent Change in
Sales from 2010
Honda
Accord
33616
29120
15.4%
24649
31.0%
36251
–13.2%
22463
39.0%
29623
2.1%
22773
21.0%
18935
20.9%
8225
134.1%
11786
57.9%
10316
75.5%
15594
15.8%
8721
104.7%
19500
–11.9%
9196
84.5%
17750
–12.4%
11353
9.8%
13811
–19.8%
7619
37.8%
96.4%
0
-----
32289
31464
31213
30234
27566
22894
19255
18605
18101
18063
17851
17178
16969
15551
12467
11075
10498
10028
5106
9787
Altima
Camry
Civic
Corolla/Matrix
1
2
Nissan
3
Toyota
4
Honda
5
Toyota
6
Ford
Fusion
7
Hyundai
Sonata
8
Hyundai
Elantra
9
Toyota
Prius
10
Chevrolet
Cruze/Cobalt
11
Chevrolet
Impala
12
Nissan
Sentra
13
Ford
Focus
14
Volkswagon
Jetta
15
Chevrolet
Malibu
16
Mazda
3
17
Nissan
Versa
18
Subaru
Outback
19
Kia
Soul
20
Ford
Fiesta
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

 
2.4 Creating Distributions from Data 
25
highlighting only models with decreasing sales, we could instead choose Data Bars from 
the Conditional Formatting dropdown menu in the Styles Group of the HOME tab in 
the Ribbon. The result of using the Blue Data Bar Gradient Fill 
 option is shown in 
Figure 2.8. Data bars are essentially a bar chart input into the cells that show the magnitude 
of the cell values. The width of the bars in this display are comparable to the values of the 
variable for which the bars have been drawn; a value of 20 creates a bar twice as wide as 
that for a value of 10. Negative values are shown to the left side of the axis; positive values 
are shown to the right. Cells with negative values are shaded in a color different from that 
of cells with positive values. Again, we can easily see which models had decreasing sales, 
but Data Bars also provide us with a visual representation of the magnitude of the change 
in sales. Many other Conditional Formatting options are available in Excel.
Creating Distributions from Data
Distributions help summarize many characteristics of a data set by describing how often 
certain values for a variable appear in a data set. Distributions can be created for both 
 categorical and quantitative data, and they assist the analyst in gauging variation.
Frequency Distributions for Categorical Data
It is often useful to create a frequency distribution for a data set. A frequency  distribution 
is a summary of data that shows the number (frequency) of observations in each of sev-
eral nonoverlapping classes, typically referred to as bins, when dealing with distributions. 
Bar charts and other graphi-
cal presentations will be cov-
ered in detail in  Chapter 3. 
We will see other uses for 
Conditional Formatting in 
excel in Chapter 3.
2.4
Bins for categorical data are 
also referred to as classes.
FIGURe 2.8   USING CONDITIONAL FORMATTING IN EXCEL TO GENERATE DATA BARS FOR THE 
TOP SELLING AUTOMOBILES DATA
A
B
C
D
E
F
Rank (by March
2011 Sales)
Manufacturer
Model
Sales (March
2011)
Sales (March
2010)
Percent Change in
Sales from 2010
Honda
Accord
33616
29120
15.4%
24649
31.0%
36251
–13.2%
22463
39.0%
29623
2.1%
22773
21.0%
18935
20.9%
8225
134.1%
11786
57.9%
10316
75.5%
15594
15.8%
8721
104.7%
19500
–11.9%
9196
84.5%
17750
–12.4%
11353
9.8%
13811
–19.8%
7619
37.8%
96.4%
0
-----
32289
31464
31213
30234
27566
22894
19255
18605
18101
18063
17851
17178
16969
15551
12467
11075
10498
10028
5106
9787
Altima
Camry
Civic
Corolla/Matrix
1
2
Nissan
3
Toyota
4
Honda
5
Toyota
6
Ford
Fusion
7
Hyundai
Sonata
8
Hyundai
Elantra
9
Toyota
Prius
10
Chevrolet
Cruze/Cobalt
11
Chevrolet
Impala
12
Nissan
Sentra
13
Ford
Focus
14
Volkswagon
Jetta
15
Chevrolet
Malibu
16
Mazda
3
17
Nissan
Versa
18
Subaru
Outback
19
Kia
Soul
20
Ford
Fiesta
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

26 
Chapter 2 Descriptive Statistics
 Consider the data in Table 2.3, taken from a sample of 50 soft drink purchases. Each pur-
chase is for one of five popular soft drinks, which define the five bins: Coca-Cola, Diet 
Coke, Dr. Pepper, Pepsi, and Sprite.
To develop a frequency distribution for these data, we count the number of times each 
soft drink appears in Table 2.3. Coca-Cola appears 19 times, Diet Coke appears 8 times, 
Dr.  Pepper appears 5 times, Pepsi appears 13 times, and Sprite appears 5 times. These 
counts are summarized in the frequency distribution in Table 2.4. This frequency distribu-
tion provides a summary of how the 50 soft drink purchases are distributed across the five 
soft drinks. This summary offers more insight than the original data shown in Table 2.3. 
The frequency distribution shows that Coca-Cola is the leader, Pepsi is second, Diet Coke 
is third, and Sprite and Dr. Pepper are tied for fourth. The frequency distribution thus sum-
marizes information about the popularity of the five soft drinks.
We can use Excel to calculate the frequency of categorical observations occurring in a data 
set using the COUNTIF function. Figure 2.9 shows the sample of 50 soft drink purchases in 
an Excel spreadsheet. Column D contains the five different soft drink categories as the bins. 
In cell E2, we enter the formula 5COUNTIF($A$2:$B$26, D2), where A2:B26 is the range 
for the sample data, and D2 is the bin (Coca-Cola) that we are trying to match. The COUNTIF 
function in Excel counts the number of times a certain value appears in the indicated range. 
In this case we want to count the number of times Coca-Cola appears in the sample data. The 
result is a value of 19 in cell E2, indicating that Coca-Cola appears 19 times in the sample data. 
We can copy the formula from cell E2 to cells E3 to E6 to get frequency counts for Diet Coke, 
See appendix a for more 
information on absolute 
verus relative references 
in excel.
 
Coca-Cola 
Sprite 
Pepsi
 
Diet Coke 
Coca-Cola 
Coca-Cola
 
Pepsi 
Diet Coke 
Coca-Cola
 
Diet Coke 
Coca-Cola 
Coca-Cola
 
Coca-Cola 
Diet Coke 
Pepsi
 
Coca-Cola 
Coca-Cola 
Dr. Pepper
 
Dr. Pepper 
Sprite 
Coca-Cola
 
Diet Coke 
Pepsi 
Diet Coke
 
Pepsi 
Coca-Cola 
Pepsi
 
Pepsi 
Coca-Cola 
Pepsi
 
Coca-Cola 
Coca-Cola 
Pepsi
 
Dr. Pepper 
Pepsi 
Pepsi
 
Sprite 
Coca-Cola 
Coca-Cola
 
Coca-Cola 
Sprite 
Dr. Pepper
 
Diet Coke 
Dr. Pepper 
Pepsi
 
Coca-Cola 
Pepsi 
Sprite
 
Coca-Cola 
Diet Coke 
Table 2.3  DATA FROM A SAMPLE OF 50 SOFT DRINK PURCHASES
file
WEB
SoftDrinks
 
Soft Drink 
Frequency
 
Coca-Cola 
19
 
Diet Coke 
 8
 
Dr. Pepper 
 5
 
Pepsi 
13
 
Sprite 
 5
 
  Total 
50
Table 2.4  FREQUENCY DISTRIBUTION OF SOFT DRINK  PURCHASES

 
2.4 Creating Distributions from Data 
27
Pepsi, Dr. Pepper, and Sprite. By using the absolute reference $A$2:$B$26 in our formula, 
Excel always searches the same sample data for the values we want when we copy the formula.
Relative Frequency and Percent Frequency Distributions
A frequency distribution shows the number (frequency) of items in each of several non-
overlapping bins. However, we are often interested in the proportion, or percentage, of 
items in each bin. The relative frequency of a bin equals the fraction or proportion of items 
belonging to a class. For a data set with n observations, the relative frequency of each bin 
can be determined as follows:
Relative frequency of a bin 5 Frequency of the bin
n
A relative frequency distribution is a tabular summary of data showing the relative fre-
quency for each bin. A percent frequency distribution summarizes the percent frequency of 
the data for each bin. Table 2.5 shows a relative frequency distribution and a percent frequency 
distribution for the soft drink data. Table 2.4 shows that the relative frequency for Coca-Cola is 
19/50 5 0.38, the relative frequency for Diet Coke is 8/50 5 0.16, and so on. From the percent 
frequency distribution, we see that 38 percent of the purchases were Coca-Cola, 16 percent 
of the purchases were Diet Coke, and so on. We can also note that 38 percent + 26 percent + 
16 percent 5 80 percent of the purchases were the top three soft drinks.
The percent frequency of a 
bin is the relative frequency 
multiplied by 100.
FIGURe 2.9   CREATING A FREQUENCY DISTRIBUTION FOR SOFT DRINKS 
DATA IN EXCEL
A
B
C
Bins
D
E
1
Sample Data
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
Coca-Cola
Diet Coke
Pepsi
Diet Coke
Coca-Cola
Coca-Cola
Dr. Pepper
Diet Coke
Pepsi
Pepsi
Coca-Cola
Dr. Pepper
Sprite
Coca-Cola
Diet Coke
Coca-Cola
Coca-Cola
Diet Coke
Coca-Cola
Coca-Cola
Coca-Cola
Sprite
Coca-Cola
Coca-Cola
Diet Coke
Coca-Cola
Diet Coke
Pepsi
Dr. Pepper
Sprite
19
8
5
13
5
Coca-Cola
Sprite
Pepsi
Coca-Cola
Pepsi
Sprite
Dr. Pepper
Pepsi
Diet Coke
Pepsi
Coca-Cola
Coca-Cola
Diet Coke
Pepsi
Pepsi
Pepsi
Coca-Cola
Dr. Pepper
Sprite
Coca-Cola
Coca-Cola
Pepsi
Dr. Pepper
Pepsi
Pepsi

28 
Chapter 2 Descriptive Statistics
A percent frequency distribution can be used to provide estimates of the relative likeli-
hoods of different values of a random variable. So, by constructing a percent frequency 
distribution from observations of a random variable, we can estimate the probability dis-
tribution that characterizes its variability. For example, the volume of soft drinks sold by 
a concession stand at an upcoming concert may not be known with certainty. However, if 
the data used to construct Table 2.5 are representative of the concession stand’s customer 
population, then the concession stand manager can use this information to determine the 
appropriate volume of each type of soft drink.
Frequency Distributions for Quantitative Data
We can also create frequency distributions for quantitative data, but we must be more 
careful in defining the nonoverlapping bins to be used in the frequency distribution. For 
example, consider the quantitative data in Table 2.6. These data show the time in days re-
quired to complete year-end audits for a sample of 20 clients of Sanderson and Clifford, a 
small public accounting firm. The three steps necessary to define the classes for a frequency 
distribution with quantitative data are:
 1. Determine the number of nonoverlapping bins.
 2. Determine the width of each bin.
 3. Determine the bin limits.
Let us demonstrate these steps by developing a frequency distribution for the audit time 
data in Table 2.6.
Number of bins Bins are formed by specifying the ranges used to group the data. As a 
general guideline, we recommend using between 5 and 20 bins. For a small number of data 
items, as few as five or six bins may be used to summarize the data. For a larger number 
of data items, more bins are usually required. The goal is to use enough bins to show the 
variation in the data, but not so many classes that some contain only a few data items. 
Because the number of data items in Table 2.6 is relatively small (n 5 20), we chose to 
develop a frequency distribution with five bins.
Table 2.6  YEAR-END AUDIT TIMES (DAYS)
12
15
20
22
14
14
15
27
21
18
19
18
22
33
16
18
17
23
28
13
Table 2.5   RELATIVE FREQUENCY AND PERCENT FREQUENCY DISTRIBUTIONS 
OF SOFT DRINK PURCHASES
Soft Drink
Relative Frequency
Percent Frequency (%)
Coca-Cola
0.38
38
Diet Coke
0.16
16
Dr. Pepper
0.10
10
Pepsi
0.26
26
Sprite
0.10
 10
 Total
1.00
100

 
2.4 Creating Distributions from Data 
29
Width of the bins Second, choose a width for the bins. As a general guideline, we 
recommend that the width be the same for each bin. Thus the choices of the number of 
bins and the width of bins are not independent decisions. A larger number of bins means 
a smaller bin width and vice versa. To determine an approximate bin width, we begin by 
identifying the largest and smallest data values. Then, with the desired number of bins 
specified, we can use the following expression to determine the approximate bin width.
APPROXIMATE BIN WIDTH
Largest data value 2 smallest data value
number of bins
 
(2.1)
The approximate bin width given by equation (2.1) can be rounded to a more conve-
nient value based on the preference of the person developing the frequency distribution. For 
example, an approximate bin width of 9.28 might be rounded to 10 simply because 10 is a 
more convenient bin width to use in presenting a frequency distribution.
For the data involving the year-end audit times, the largest data value is 33, and the 
smallest data value is 12. Because we decided to summarize the data with five classes, us-
ing equation (2.1) provides an approximate bin width of (33 2 12)/5 5 4.2. We therefore 
decided to round up and use a bin width of five days in the frequency distribution.
In practice, the number of bins and the appropriate class width are determined by trial 
and error. Once a possible number of bins is chosen, equation (2.1) is used to find the 
approximate class width. The process can be repeated for a different number of bins. Ulti-
mately, the analyst uses judgment to determine the combination of the number of bins and 
bin width that provides the best frequency distribution for summarizing the data.
For the audit time data in Table 2.6, after deciding to use five bins, each with a width 
of five days, the next task is to specify the bin limits for each of the classes.
Bin limits Bin limits must be chosen so that each data item belongs to one and only one 
class. The lower bin limit identifies the smallest possible data value assigned to the bin. The 
upper bin limit identifies the largest possible data value assigned to the class. In developing 
frequency distributions for qualitative data, we did not need to specify bin limits because 
each data item naturally fell into a separate bin. But with quantitative data, such as the audit 
times in Table 2.6, bin limits are necessary to determine where each data value belongs.
Using the audit time data in Table 2.6, we selected 10 days as the lower bin limit and 
14 days as the upper bin limit for the first class. This bin is denoted 10–14 in Table 2.7. 
The smallest data value, 12, is included in the 10–14 bin. We then selected 15 days as the 
lower bin limit and 19 days as the upper bin limit of the next class. We continued defining 
the lower and upper bin limits to obtain a total of five classes: 10–14, 15–19, 20–24, 25–29, 
and 30–34. The largest data value, 33, is included in the 30–34 bin. The difference between 
although an audit time of 
12 days is actually the small-
est observation in our data, 
we have chosen a lower bin 
limit of 10 simply for conve-
nience. The lowest bin limit 
should include the smallest 
observation, and the highest 
bin limit should include the 
largest observation.
Table 2.7   FREQUENCY, RELATIVE FREQUENCY, AND PERCENT  FREQUENCY 
 DISTRIBUTIONS FOR THE AUDIT TIME DATA
Audit Times 
(days)
Frequency
Relative 
Frequency
Percent 
Frequency
10–14
15–19
20–24
25–29
30–34 
4
8
5
2
1
0.20
0.40
0.25
0.10
0.05
20
40
25
10
 5
file
WEB
AuditData

30 
Chapter 2 Descriptive Statistics
the upper bin limits of adjacent bins is the bin width. Using the first two upper bin limits of 
14 and 19, we see that the bin width is 19 2 14 5 5.
With the number of bins, bin width, and bin limits determined, a frequency distribution 
can be obtained by counting the number of data values belonging to each bin. For example, 
the data in Table 2.6 show that four values—12, 14, 14, and 13—belong to the 10–14 bin. 
Thus, the frequency for the 10–14 bin is 4. Continuing this counting process for the 15–19, 
20–24, 25–29, and 30–34 bins provides the frequency distribution in Table 2.7. Using this 
frequency distribution, we can observe that:
● 
 The most frequently occurring audit times are in the bin of 15–19 days. Eight of the 
20 audit times are in this bin.
● 
Only one audit required 30 or more days.
Other conclusions are possible, depending on the interests of the person viewing the fre-
quency distribution. The value of a frequency distribution is that it provides insights about 
the data that are not easily obtained by viewing the data in their original unorganized form. 
Table 2.7 also shows the relative frequency distribution and percent frequency distribu-
tion for the audit time data. Note that 0.40 of the audits, or 40 percent, required from 15 to 
19 days. Only 0.05 of the audits, or 5 percent, required 30 or more days. Again, additional 
interpretations and insights can be obtained by using Table 2.7.
Frequency distributions for quantitative data can also be created using Excel. Fig-
ure 2.10 shows the data from Table 2.6 entered into an Excel Worksheet. The sample of 
20 audit times is contained in cells A2:D6. The upper limits of the defined bins are in cells 
We define the relative 
frequency and percent 
frequency distributions 
for quantitative data in 
the same manner as for 
 qualitative data.
FIGURe 2.10   USING EXCEL TO GENERATE A FREQUENCY DISTRIBUTION FOR AUDIT TIMES DATA
A
B
C
D
Bin
Frequency
Year-End Audit Times (in Days)
12
1
2
3
4
5
6
15
20
22
14
7
8
9
10
11
12
13
14
14
19
24
29
34
14
15
27
21
18
19
18
22
33
16
18
17
23
28
13
=FREQUENCY(A2:D6,A10:A14)
=FREQUENCY(A2:D6,A10:A14)
=FREQUENCY(A2:D6,A10:A14)
=FREQUENCY(A2:D6,A10:A14)
=FREQUENCY(A2:D6,A10:A14)
A
B
C
D
Bin
Frequency
Year-End Audit Times (in Days)
12
1
2
3
4
5
6
15
20
22
14
7
8
9
10
11
12
13
14
14
19
24
29
34
14
15
27
21
18
19
18
22
33
16
18
17
23
28
13
4
8
5
2
1

 
2.4 Creating Distributions from Data 
31
A10:A14. We can use the FREQUENCY function in Excel to count the number of observa-
tions in each bin:
Step 1. Select cells B10:B14
Step 2.  Enter the formula 5FREQUENCY(A2:D6, A10:A14). The range A2:D6 
 defines the data set, and the range A10:A14 defines the bins
Step 3. Press CTRL+SHIFT+ENTER
Excel will then fill in the values for the number of observations in each bin in cells B10 
through B14 because these were the cells selected in Step 1 above (see Figure 2.10).
Histograms
A common graphical presentation of quantitative data is a histogram. This graphical sum-
mary can be prepared for data previously summarized in either a frequency, a relative 
frequency, or a percent frequency distribution. A histogram is constructed by placing the 
variable of interest on the horizontal axis and the selected frequency measure (absolute 
frequency, relative frequency, or percent frequency) on the vertical axis. The frequency 
measure of each class is shown by drawing a rectangle whose base is determined by the 
class limits on the horizontal axis and whose height is the corresponding frequency measure.
Figure 2.11 is a histogram for the audit time data. Note that the class with the greatest 
frequency is shown by the rectangle appearing above the class of 15–19 days. The height of 
the rectangle shows that the frequency of this class is 8. A histogram for the relative or percent 
frequency distribution of these data would look the same as the histogram in Figure 2.11 with 
the exception that the vertical axis would be labeled with relative or percent frequency values.
Histograms can be created in Excel using the Data Analysis ToolPak. We will use the 
sample of 20 year-end audit times and the bins defined in Table 2.7 to create a histogram 
using the Data Analysis ToolPak. As before, we begin with an Excel Worksheet where the 
sample of 20 audit times is contained in cells A2:D6, and the upper limits of the bins defined 
in Table 2.7 are in cells A10:A14 (see Figure 2.10).
Step 1. Click the DATA tab in the Ribbon
Step 2. Click Data Analysis in the Analysis group
Step 3.  When the Data Analysis dialog box opens, choose Histogram from the list of 
Analysis Tools, and click OK
 
In the Input Range: box, enter a2:D6
 
In the Bin Range: box, enter a10:a14
Pressing 
CTRL+SHIFT+ENTER 
in excel indicates that the 
function should return an 
array of values. 
If Data analysis does not 
appear in your analysis 
Group, then you will have 
to include the Data  analysis 
ToolPak add-In. To do so, 
click on the FILE tab and 
choose Options. When the 
Excel Options dialog box 
opens, click Add-Ins. at the 
bottom of the Excel Options 
dialog box, where it says 
Manage: Excel Add-ins, 
click Go…. Select the check 
box for Analysis ToolPak, 
and click OK.
FIGURe 2.11  HISTOGRAM FOR THE AUDIT TIME DATA
2
4
6
8
Frequency
10–14
15–19
20–24
25–29
30–34
Audit Time (days)
7
5
3
1

32 
Chapter 2 Descriptive Statistics
 
Under Output Options:, select New Worksheet Ply:
 
Select the check box for Chart Output (see Figure 2.12)
 
Click OK
The histogram created by Excel for these data is shown in Figure 2.13. We have modi-
fied the bin ranges in column A by typing the values shown in Figure 2.13 into cells A2:A6 
so that the chart created by Excel shows both the lower and upper limits for each bin. We 
have also removed the gaps between the columns in the histogram in Excel to match the 
traditional format of histograms. To remove the gaps between the columns in the Histogram 
created by Excel, follow these steps:
Step 1. Right-click on one of the columns in the histogram
 
Select Format Data Series…
Step 2.  When the Format Data Series pane opens, click the Series Options 
 button, 
 
Set the Gap Width to 0%
One of the most important uses of a histogram is to provide information about the shape, 
or form, of a distribution. Skewness, or the lack of symmetry, is an important characteristic 
of the shape of a distribution. Figure 2.14 contains four histograms constructed from rela-
tive frequency distributions that exhibit different patterns of skewness. Panel A shows the 
FIGURe 2.12   CREATING A HISTOGRAM FOR THE AUDIT TIME DATA USING DATA ANALYSIS 
TOOLPAK IN EXCEL
A
B
C
D
E
F
G
H
I
Year-End Audit Times (in Days)
14
19
18
17
23
28
13
18
22
33
16
15
27
21
18
12
15
20
22
14
Bin
14
19
24
29
34
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
A
B
C
D
E
F
G
H
I
Year-End Audit Times (in Days)
14
19
18
17
23
28
13
18
22
33
16
15
27
21
18
12
15
20
22
14
Bin
14
19
24
29
34
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

 
2.4 Creating Distributions from Data 
33
Panel A: Moderately Skewed Left
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Panel C: Symmetric
0.3
0.25
0.2
0.15
0.1
0.05
0
Panel B: Moderately Skewed Right
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
Panel D: Highly Skewed Right
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
FIGURe 2.14   HISTOGRAMS SHOWING DISTRIBUTIONS WITH DIFFERENT LEVELS OF SKEWNESS
FIGURe 2.13   COMPLETED HISTOGRAM FOR THE AUDIT TIME DATA USING DATA ANALYSIS 
TOOLPAK IN EXCEL
A
B
C
D
E
F
G
H
I
J
Bin
Frequency
4
10–14
15–19
8
20–24
5
25–29
2
30–34
More
0
1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
10–14
15–19
20–24
Bin
Histogram
9
8
7
6
5
4
3
2
1
0
Frequency
25–29
30–34
Frequency
More

34 
Chapter 2 Descriptive Statistics
histogram for a set of data moderately skewed to the left. A histogram is said to be skewed 
to the left if its tail extends farther to the left than to the right. This histogram is typical for 
exam scores, with no scores above 100 percent, most of the scores above 70 percent, and 
only a few really low scores. 
Panel B shows the histogram for a set of data moderately skewed to the right. A his-
togram is said to be skewed to the right if its tail extends farther to the right than to the 
left. An example of this type of histogram would be for data such as housing prices; a few 
expensive houses create the skewness in the right tail.
Panel C shows a symmetric histogram. In a symmetric histogram, the left tail mirrors the 
shape of the right tail. Histograms for data found in applications are never perfectly symmet-
ric, but the histogram for many applications may be roughly symmetric. Data for SAT scores, 
the heights and weights of people, and so on lead to histograms that are roughly symmetric.
Panel D shows a histogram highly skewed to the right. This histogram was constructed 
from data on the amount of customer purchases over one day at a women’s apparel store. 
Data from applications in business and economics often lead to histograms that are skewed 
to the right. For instance, data on housing prices, salaries, purchase amounts, and so on often 
result in histograms skewed to the right.
Cumulative Distributions
A variation of the frequency distribution that provides another tabular summary of quantita-
tive data is the  cumulative frequency distribution, which uses the number of classes, class 
widths, and class limits developed for the frequency distribution. However, rather than showing 
the frequency of each class, the cumulative frequency distribution shows the number of data 
items with values less than or equal to the upper class limit of each class. The first two columns 
of Table 2.8 provide the cumulative frequency distribution for the audit time data.
To understand how the cumulative frequencies are determined, consider the class with 
the description “Less than or equal to 24.” The cumulative frequency for this class is simply 
the sum of the frequencies for all classes with data values less than or equal to 24. For the 
frequency distribution in Table 2.7, the sum of the frequencies for classes 10–14, 15–19, 
and 20–24 indicates that 4 1 8 1 5 5 17 data values are less than or equal to 24. Hence, the 
cumulative frequency for this class is 17. In addition, the cumulative frequency distribution 
in Table 2.8 shows that four audits were completed in 14 days or less and that 19 audits 
were completed in 29 days or less.
As a final point, a cumulative relative frequency distribution shows the proportion of 
data items, and a cumulative percent frequency distribution shows the percentage of data 
items with values less than or equal to the upper limit of each class. The cumulative rela-
tive frequency distribution can be computed either by summing the relative frequencies in 
the relative frequency distribution or by dividing the cumulative frequencies by the total 
Table 2.8   CUMULATIVE FREQUENCY, CUMULATIVE RELATIVE FREQUENCY, 
AND CUMULATIVE PERCENT FREQUENCY DISTRIBUTIONS FOR 
THE AUDIT TIME DATA
Audit Time (days)
Cumulative 
Frequency
Cumulative  
Relative Frequency
Cumulative  
Percent Frequency
Less than or equal to 14
Less than or equal to 19
Less than or equal to 24
Less than or equal to 29
Less than or equal to 34
 4
12
17
19
20
0.20
0.60
0.85
0.95
1.00
 20
 60
 85
 95
100

 
2.5 Measures of Location 
35
number of items. Using the latter approach, we found the cumulative relative frequencies 
in column 3 of Table 2.8 by dividing the cumulative frequencies in column 2 by the total 
number of items (n 5 20). The cumulative percent frequencies were again computed by 
multiplying the relative frequencies by 100. The cumulative relative and percent frequency 
distributions show that 0.85 of the audits, or 85 percent, were completed in 24 days or less, 
0.95 of the audits, or 95 percent, were completed in 29 days or less, and so on.
NOTES AND COMMENTS
Distributions are often used when discussing 
 concepts related to probability and simulation 
because they are used to describe uncertainty. In 
Chapter 11 we will revisit distributions when we 
introduce simulation models.
Measures of Location
Mean (Arithmetic Mean)
The most commonly used measure of location is the mean (arithmetic mean), or average 
value, for a variable. The mean provides a measure of central location for the data. If the 
data are for a sample (typically the case), the mean is denoted by x. The sample mean is a 
point estimate of the (typically unknown) population mean for the variable of interest. If the 
data for the entire population are available, the population mean is computed in the same 
manner, but denoted by the Greek letter m.
In statistical formulas, it is customary to denote the value of variable x for the first 
observation by x1, the value of variable x for the second observation by x2, and so on. In 
general, the value of variable x for the ith observation is denoted by xi. For a sample with n 
observations, the formula for the sample mean is as follows.
SAMPLE MEAN
x 5 gxi
n
5 x1 1 x2 1 c1 xn
n
 
(2.2)
To illustrate the computation of a sample mean, suppose a sample of home sales is taken for 
a suburb of Cincinnati, Ohio. Table 2.9 shows the collected data. The mean home selling 
price for the sample of 12 home sales is
 
x 5 gxi
n
5 x1 1 x2 1 c1 x12
12
 
5 138,000 1 254,000 1 c1 456,250
12
 
5 2,639,250
12
5 219,937.50
The mean can be found in Excel using the AVERAGE function. Figure 2.15 shows the 
Home Sales data from Table 2.9 in an Excel spreadsheet. The value for the mean in cell E2 
is calculated using the formula 5AVERAGE(B2:B13).
2.5
If the data set is not a 
sample, but is the entire 
population with n observa-
tions, the population mean 
is computed directly by: 
μ 5 g xi
n .

36 
Chapter 2 Descriptive Statistics
Table 2.9   DATA ON HOME SALES IN CINCINNATI, OHIO, SUBURB
Home Sale
Selling Price ($)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
138,000
254,000
186,000
257,500
108,000
254,000
138,000
298,000
199,500
208,000
142,000
456,250
file
WEB
HomeSales
Median
The median, another measure of central location, is the value in the middle when the data 
are arranged in ascending order (smallest to largest value). With an odd number of obser-
vations, the median is the middle value. An even number of observations has no single 
FIGURe 2.15   CALCULATING THE MEAN, MEDIAN, AND MODES FOR THE HOME SALES DATA 
 USING EXCEL
A
B
C
D
E
Home Sale
Mean: =AVERAGE(B2:B13)
Median: =MEDIAN(B2:B13)
Mode 1: =MODE.MULT(B2:B13)
Mode 2: =MODE.MULT(B2:B13)
Selling Price ($)
138,000
254,000
186,000
257,500
108,000
254,000
138,000
298,000
199,500
208,000
142,000
1
2
3
4
5
6
7
8
9
10
11
12
456,250
1
2
3
4
5
6
7
8
9
10
11
12
13
A
B
C
D
E
Mean: $ 219,937.50
Median: $ 203,750.00
Mode 1: $ 138,000.00
Mode 2: $ 254,000.00
Selling Price ($)
138,000
254,000
186,000
257,500
108,000
254,000
138,000
298,000
199,500
208,000
142,000
Home Sale
1
2
3
4
5
6
7
8
9
10
11
12
456,250
1
2
3
4
5
6
7
8
9
10
11
12
13

 
2.5 Measures of Location 
37
middle value. In this case, we follow convention and define the median as the average of 
the values for the middle two observations. 
Let us apply this definition to compute the median class size for a sample of five college 
classes. Arranging the data in ascending order provides the following list:
32 42 46 46 54
Because n 5 5 is odd, the median is the middle value. Thus the median class size is 
46 students. Even though this data set contains two observations with values of 46, each 
observation is treated separately when we arrange the data in ascending order.
Suppose we also compute the median value for the 12 home sales in Table 2.9. We first 
arrange the data in ascending order.
108,000 138,000 138,000 142,000 186,000 199,500 208,000 254,000 254,000 257,500 298,000 456,250
Middle Two Values
Because n 5 12 is even, the median is the average of the middle two values: 199,500 and 208,000.
Median 5 199,500 1 208,000
2
5 203,750
The median of a data set can be found in Excel using the function MEDIAN. In Fig-
ure 2.15, the value for the median in cell E3 is found using the formula 5MEDIAN(B2:B13).
Although the mean is the more commonly used measure of central location, in some 
situations the median is preferred. The mean is influenced by extremely small and large 
data values. Notice that the median is smaller than the mean in Figure 2.15. This is because 
the one large value of $456,250 in our data set inflates the mean but does not have the same 
effect on the median. Notice also that the median would remain unchanged if we replaced 
the $456,250 with a sales price of $1.5 million. In this case, the median selling price would 
remain $203,750, but the mean would increase to $306,916.67. If you were looking to 
buy a home in this suburb, the median gives a better indication of the central selling price 
of the homes there. We can generalize, saying that whenever a data set contains extreme 
values or is severely skewed, the median is often the preferred measure of central location.
Mode
A third measure of location, the mode, is the value that occurs most frequently in a data set. 
To illustrate the identification of the mode, consider the sample of five class sizes.
32 42 46 46 54
The only value that occurs more than once is 46. Because this value, occurring with a fre-
quency of 2, has the greatest frequency, it is the mode. To find the mode for a data set with 
only one most often occurring value in Excel, we use the MODE.SNGL function.
Occasionally the greatest frequency occurs at two or more different values, in which 
case more than one mode exists. If data contain at least two modes, we say that they are 
multimodal. A special case of multimodal data occurs when the data contain exactly two 
modes; in such cases we say that the data are bimodal. In multimodal cases when there 
are more than two modes, the mode is almost never reported because listing three or more 
modes is not particularly helpful in describing a location for the data. Also, if no value in 
the data occurs more than once, we say the data have no mode.
The Excel MODE.SNGL function will return only a single most-often-occurring value. 
For multimodal distributions, we must use the MODE.MULT command in Excel to return 
more than one mode. For example, two home selling prices occur twice in Table 2.9: 
In versions of excel prior 
to excel 2010, the mode for 
a data set is found using 
the function MODe. This 
is equivalent to the newer 
command of MODe.SnGL. 
More recent versions of 
excel will also accept the 
function MODe.

38 
Chapter 2 Descriptive Statistics
$138,000 and $254,000. Hence, these data are bimodal. To find both of the modes in Excel, 
we take these steps:
Step 1. Select cells E4 and E5
Step 2. Enter the formula 5MODE.MULT(B2:B13)
Step 3. Press CTRL+SHIFT+ENTER
Excel enters the values for both modes of this data set in cells E4 and E5: $138,000 and 
$254,000.
Geometric Mean
The geometric mean is a measure of location that is calculated by finding the nth root of the 
product of n values. The general formula for the sample geometric mean, denoted xg, follows.
SAMPLE GEOMETRIC MEAN
xg 5
n"(x1) (x2) c (xn) 5 3 (x1) (x2) c (xn) 41/n 
(2.3)
The geometric mean is often used in analyzing growth rates in financial data. In these types 
of situations, the arithmetic mean or average value will provide misleading results.
To illustrate the use of the geometric mean, consider Table 2.10 which shows the per-
centage annual returns, or growth rates, for a mutual fund over the past ten years. Suppose 
we want to compute how much $100 invested in the fund at the beginning of year 1 would 
be worth at the end of year 10. We start by computing the balance in the fund at the end of 
year 1. Because the percentage annual return for year 1 was 222.1 percent, the balance in 
the fund at the end of year 1 would be:
$100 2 0.221($100) 5 $100(1 2 0.221) 5 $100(0.779) 5 $77.90
We refer to 0.779 as the growth factor for year 1 in Table 2.10. We can compute the bal-
ance at the end of year 1 by multiplying the value invested in the fund at the beginning of 
year 1 by the growth factor for year 1: $100(0.779) 5 $77.90.
The balance in the fund at the end of year 1, $77.90, now becomes the beginning bal-
ance in year 2. So, with a percentage annual return for year 2 of 28.7 percent, the balance 
at the end of year 2 would be:
$77.90 1 0.287($77.90) 5 $77.90(1 1 0.287) 5 $77.90(1.287) 5 $100.26
We must press 
CTRL+SHIFT+ENTER 
because the MODe.MULT 
formula returns an array 
of values.
The geometric mean for 
a population is computed 
similarly but is defined 
as mg to denote that it is 
computed using the entire 
population.
The growth factor for each 
year is 1 plus 0.01 times 
the percentage return. a 
growth factor less than 1 
indicates negative growth, 
whereas a growth factor 
greater than 1 indicates 
positive growth. The 
growth factor cannot be 
less than zero.
Table 2.10   PERCENTAGE ANNUAL RETURNS AND GROWTH FACTORS FOR 
THE MUTUAL FUND DATA
Year
Return (%)
Growth Factor
1
2
3
4
5
6
7
8
9
10
222.1
28.7
10.9
4.9
15.8
5.5
237.0
26.5
15.1
2.1
0.779
1.287
1.109
1.049
1.158
1.055
0.630
1.265
1.151
1.021
file
WEB
MutualFundsReturns

 
2.5 Measures of Location 
39
Note that 1.287 is the growth factor for year 2. By substituting $100(0.779) for $77.90, we 
see that the balance in the fund at the end of year 2 is:
$100(0.779)(1.287) 5 $100.26
In other words, the balance at the end of year 2 is just the initial investment at the beginning 
of year 1 times the product of the first two growth factors. This result can be generalized 
to show that the balance at the end of year 10 is the initial investment times the product of 
all ten growth factors.
$100[(0.779)(1.287)(1.109)(1.049)(1.158)(1.055)(0.630)(1.265)(1.151)(1.021)]  
 5 $100(1.335) 5 $133.45
So a $100 investment in the fund at the beginning of year 1 would be worth $133.45 at the end of 
year 10. Note that the product of the ten growth factors is 1.335. Thus, we can compute the balance 
at the end of year 10 for any amount of money invested at the beginning of year 1 by multiplying 
the value of the initial investment by 1.335. For instance, an initial investment of $2500 at the be-
ginning of year 1 would be worth $2500(1.335), or approximately $3337.50, at the end of year 10.
What was the mean percentage annual return or mean rate of growth for this invest-
ment over the ten-year period? The geometric mean of the ten growth factors can be used to 
answer this question. Because the product of the ten growth factors is 1.335, the geometric 
mean is the tenth root of 1.335, or:
xg 5 10"1.335 5 1.029
The geometric mean tells us that annual returns grew at an average annual rate of 
(1.029 2 1)100, or 2.9 percent. In other words, with an average annual growth rate of 2.9 per-
cent, a $100 investment in the fund at the beginning of year 1 would grow to $100(1.029)10 5  
$133.09 at the end of ten years. We can use Excel to calculate the geometric mean for the data 
in Table 2.10 by using the function GEOMEAN. In Figure 2.16, the value for the geometric 
mean in cell C13 is found using the formula 5GEOMEAN(C2:C11).
It is important to understand that the arithmetic mean of the percentage annual returns 
does not provide the mean annual growth rate for this investment. The sum of the ten per-
centage annual returns in Table 2.10 is 50.4. Thus, the arithmetic mean of the ten percent-
age returns is 50.4y10 5 5.04 percent. A salesperson might try to convince you to invest 
The growth factor over the 
first two years is the prod-
uct of the growth factors for 
years 1 and 2. This can be 
generalized to any number 
of consecutive periods.
FIGURe 2.16   CALCULATING THE GEOMETRIC MEAN FOR THE MUTUAL 
FUND DATA USING EXCEL
A
1
2
3
4
5
6
7
8
9
10
1
–22.1
0.779
1.287
1.109
1.049
1.158
1.055
0.630
1.265
1.151
1.021
1.029
28.7
10.9
4.9
15.8
5.5
–37.0
26.5
15.1
2.1
2
3
4
5
6
7
8
9
10
11
12
13
14
B
C
D
Year
Return (%)
Growth Factor
Geometric Mean:

40 
Chapter 2 Descriptive Statistics
in this fund by stating that the mean annual percentage return was 5.04 percent. Such a 
statement is not only misleading, it is inaccurate. A mean annual percentage return of 5.04 
percent corresponds to an average growth factor of 1.0504. So, if the average growth factor 
were really 1.0504, $100 invested in the fund at the beginning of year 1 would have grown 
to $100(1.0504)10 5 $163.51 at the end of ten years. But, using the ten annual percentage 
returns in Table 2.10, we showed that an initial $100 investment is worth $133.09 at the 
end of ten years. The salesperson’s claim that the mean annual percentage return is 5.04 
percent grossly overstates the true growth for this mutual fund. The problem is that the 
arithmetic mean is appropriate only for an additive process. For a multiplicative process, 
such as applications involving growth rates, the geometric mean is the appropriate measure 
of location.
While the application of the geometric mean to problems in finance, investments, and 
banking is particularly common, the geometric mean should be applied any time you want to 
determine the mean rate of change over several successive periods. Other common applica-
tions include changes in the populations of species, crop yields, pollution levels, and birth and 
death rates. The geometric mean can also be applied to changes that occur over any number of 
successive periods of any length. In addition to annual changes, the geometric mean is often 
applied to find the mean rate of change over quarters, months, weeks, and even days.
Measures of Variability
In addition to measures of location, it is often desirable to consider measures of variability, 
or dispersion. For example, suppose that you are a considering two financial funds. Both 
funds require a $1000 annual investment. Table 2.11 shows the annual payouts for Fund A 
and Fund B for $1000 investments over the past 20 years. Fund A has paid out exactly 
2.6
Table 2.11  ANNUAL PAYOUTS FOR TWO DIFFERENT INVESTMENT FUNDS
Year
Fund A ($)
Fund B ($)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
1100
700
2500
1200
1550
1300
800
300
1600
1500
350
460
890
1050
800
1150
1200
1800
100
1750
1000
Mean
1100
1100

 
2.6 Measures of Variability 
41
$1100 each year for an initial $1000 investment. Fund B has had many different payouts, 
but the mean payout over the previous 20 years is also $1100. But would you consider the 
payouts of Fund A and Fund B to be equivalent? Clearly, the answer is no. The difference 
between the two funds is due to variability.
Figure 2.17 shows a histogram for the payouts received from Funds A and B. Although 
the mean payout is the same for both funds, their histograms differ in that the payouts asso-
ciated with Fund B have greater variability. Sometimes the payouts are considerably larger 
than the mean, and sometimes they are considerably smaller. In this section, we present 
several different ways to measure variability.
Range
The simplest measure of variability is the range. The range can be found by subtracting 
the smallest value from the largest value in a data set. Let us return to the home sales data 
set to demonstrate the calculation of range. Refer to the data from home sales prices in 
Table 2.9. The largest home sales price is $456,250, and the smallest is $108,000. The range 
is $456,250 2 $108,000 5 $348,250.
Although the range is the easiest of the measures of variability to compute, it is seldom 
used as the only measure. The reason is that the range is based on only two of the obser-
vations and thus is highly influenced by extreme values. If, for example, we replace the 
selling price of $456,250 with $1.5 million, the range would be $1,500,000 2 $108,000 5 
$1,392,000. This large value for the range would not be especially descriptive of the variabil-
ity in the data because 11 of the 12 home selling prices are between $108,000 and $298,000.
The range can be calculated in Excel using the MAX and MIN functions. The range 
value in cell E7 of Figure 2.18 calculates the range using the formula 5MAX(B2:B13) 2 
MIN(B2:B13). This subtracts the smallest value in the range B2:B13 from the largest value 
in the range B2:B13.
Variance
The variance is a measure of variability that utilizes all the data. The variance is based on 
the deviation about the mean, which is the difference between the value of each observation 
(xi) and the mean. For a sample, a deviation of an observation about the mean is written 
(xi 2 x¯ ). In the computation of the variance, the deviations about the mean are squared.
FIGURe 2.17  HISTOGRAMS FOR PAYOUTS OF LAST 20 YEARS FROM FUND A AND FUND B
0
Fund A Payouts ($)
15
10
5
20
Frequency
1100
0
Fund B Payouts ($)
15
10
5
20
Frequency
0–500
501–1000
1001–1500
1501–2000
2001–2500

42 
Chapter 2 Descriptive Statistics
In most statistical applications, the data being analyzed are for a sample. When we 
compute a sample variance, we are often interested in using it to estimate the population 
variance, s2. Although a detailed explanation is beyond the scope of this text, for a random 
sample, it can be shown that, if the sum of the squared deviations about the sample mean 
is divided by n 2 1, and not n, the resulting sample variance provides an unbiased estimate 
of the population variance.1
For this reason, the sample variance, denoted by s2, is defined as follows:
SAMPLE VARIANCE
 
s2 5 g (xi 2 x) 2
n 2 1
 
(2.4)
If the data are for a 
population, the population 
variance, s2, can be com-
puted directly (rather than 
estimated by the sample 
 variance). For a popula-
tion of n observations and 
with m denoting the popu-
lation mean, population 
variance is computed by  
s2 5 g ( xi 2 μ) 2
n
.
To illustrate the computation of the sample variance, we will use the data on class size 
from page 37 for the sample of five college classes. A summary of the data, including the 
1Unbiased means that if we take a large number of independent random samples of the same size from the population and calcu-
late the sample variance for each sample, the average of these sample variances will tend to be equal to the population variance.
FIGURe 2.18  CALCULATING VARIABILITY MEASURES FOR THE HOME SALES DATA IN EXCEL
1
2
3
4
5
6
7
8
9
10
11
12
13
1
2
3
4
5
6
7
8
9
10
11
12
138,000
Mean: $ 219,937.50
$ 203,750.00
$ 138,000.00
$ 254,000.00
$ 348,250.00
9037501420
$   95,065.77
43.22%
$ 305,912.50
Median:
Mode 1:
Mode 2:
Range:
Variance:
Standard Deviation:
Coefﬁcient of Variation:
85th Percentile:
254,000
186,000
257,500
108,000
254,000
138,000
298,000
199,500
208,000
142,000
456,250
A
B
Home Sale Selling Price ($)
C
D
E
1
2
3
4
5
6
7
8
9
10
11
12
13
1
2
3
4
5
6
7
8
9
10
11
12
138000
Mean: =AVERAGE(B2:B13)
=MEDIAN(B2:B13)
=MODE.MULT(B2:B13)
=MODE.MULT(B2:B13)
=MAX(B2:B13)-MIN(B2:B13)
=VAR.S(B2:B13)
=STDEV.S(B2:B13)
=E9/E2
=PERCENTILE.EXC(B2:B13,0.85)
Median:
Mode 1:
Mode 2:
Range:
Variance:
Standard Deviation:
Coefﬁcient of Variation:
85th Percentile:
254000
186000
257500
108000
254000
138000
298000
199500
208000
142000
456250
A
B
Home Sale Selling Price ($)
C
D
E

 
2.6 Measures of Variability 
43
computation of the deviations about the mean and the squared deviations about the mean, 
is shown in Table 2.12. The sum of squared deviations about the mean is ∑(xi 2 x )2 5 256. 
Hence, with n 2 1 5 4, the sample variance is:
s2 5 g (xi 2 x) 2
n 2 1
5 256
4
5 64
Note that the units of variance are squared. For instance, the sample variance for our 
calculation is s2 5 64 (students)2. In Excel, you can find the variance for sample data using 
the VAR.S function. Figure 2.18 shows the data for home sales examined in the previous 
section. The variance in cell E8 is calculated using the formula 5VAR.S(B2:B13). Excel 
calculates the variance of the sample of 12 home sales to be 9,037,501,420.
Standard Deviation
The standard deviation is defined to be the positive square root of the variance. We use s 
to denote the sample standard deviation and s to denote the population standard deviation. 
The sample standard deviation, s, is a point estimate of the population standard deviation, 
s, and is derived from the sample variance in the following way:
SAMPLE STANDARD DEVIATION
s 5 "s2 
(2.5)
The sample variance for the sample of class sizes in five college classes is s2 5 64. Thus, 
the sample standard deviation is s 5 "64 5 8.
Recall that the units associated with the variance are squared and that it is difficult 
to interpret the meaning of squared units. Because the standard deviation is the square 
root of the variance, the units of the variance, (students)2 in our example, are converted 
to students in the standard deviation. In other words, the standard deviation is measured 
in the same units as the original data. For this reason, the standard deviation is more eas-
ily compared to the mean and other statistics that are measured in the same units as the 
original data.
Figure 2.18 shows the Excel calculation for the sample standard deviation of the home 
sales data, which can be calculated using Excel’s STDEV.S function. The sample standard 
deviation in cell E9 is calculated using the formula 5STDEV.S(B2:B13). Excel calculated 
the sample standard deviation for the home sales to be $95,065.77.
Versions of excel prior to 
excel 2010 use the function 
Var to calculate sample 
variance. It is equivalent to 
5Var.S in later versions of 
excel and is still accepted 
as a function in more recent 
versions of excel. If the data 
are a population, excel 
2010 and excel 2013 can 
also calculate the population 
variance using the function 
5Var.P.
If the data are a population, 
the population standard 
 deviation s is obtained by 
taking the positive square 
root of the population 
 variance: σ 5 "σ2.
Table 2.12   COMPUTATION OF DEVIATIONS AND SQUARED DEVIATIONS ABOUT THE 
MEAN FOR THE CLASS SIZE DATA
Number of 
Students in 
Class (xi)
Mean 
Class Size 
(x–)
Deviation 
About the 
Mean (xi 2 x–)
Squared 
 Deviation About 
the Mean (xi 2 x–)2
46
54
42
46
32
44
44
44
44
44
2
10
22
2
212
4
100
4
4
144
0
∑(xi 2 x¯ )
    256
∑(xi 2 x¯ )2

44 
Chapter 2 Descriptive Statistics
Coefficient of Variation
In some situations we may be interested in a descriptive statistic that indicates how large 
the standard deviation is relative to the mean. This measure is called the coefficient of 
 variation and is usually expressed as a percentage.
COEFFICIENT OF VARIATION
aStandard deviation
Mean
3 100b% 
(2.6)
For the class size data on page 37, we found a sample mean of 44 and a sample standard 
deviation of 8. The coefficient of variation is (8y44 3 100) 5 18.2 percent. In words, the coef-
ficient of variation tells us that the sample standard deviation is 18.2 percent of the value of the 
sample mean. The coefficient of variation for the home sales data is shown in Figure 2.18. It 
is calculated in cell E11 using the formula 5E9/E2, which divides the standard deviation by 
the mean. The coefficient of variation for the home sales data is 43.22 percent. In general, the 
coefficient of variation is a useful statistic for comparing the relative variability of different 
variables, each with different standard deviations and different means.
Analyzing Distributions
In Section 2.4 we demonstrated how to create frequency, relative, and cumulative distri-
butions for data sets. Distributions are very useful for interpreting and analyzing data. A 
distribution describes the overall variability of the observed values of a variable. In this 
section we introduce additional ways of analyzing distributions.
Percentiles
A percentile is the value of a variable at which a specified (approximate) percentage of observa-
tions are below that value. The pth percentile tells us the point in the data where approximately 
p percent of the observations have values less than the pth percentile; hence, approximately 
(100 – p) percent of the observations have values greater than the pth percentile.
Colleges and universities frequently report admission test scores in terms of percentiles. 
For instance, suppose an applicant obtains a raw score of 54 on the verbal portion of an 
admission test. How this student performed in relation to other students taking the same 
test may not be readily apparent. However, if the raw score of 54 corresponds to the 70th 
percentile, we know that approximately 70 percent of the students scored lower than this 
individual, and approximately 30 percent of the students scored higher.
The following procedure can be used to compute the pth percentile:
 1. Arrange the data in ascending order (smallest to largest value).
 2. Compute k 5 (n 1 1) 3 p.
 3.  Divide k into its integer component, i, and its decimal component, d. (For example, 
k 5 13.25 would result in i 5 13 and d 5 0.25.)
a.  If d 5 0 (there is no decimal component for k), find the kth largest value in the 
data set. This is the pth percentile.
b.  If d . 0, the percentile is between the values in positions i and i 11 in the sorted 
data. To find this percentile, we must interpolate between these two values.
 i.  Calculate the difference between the values in positions i and i 11 in the sorted 
data set. We define this difference between the two values as m.
Versions of excel prior to 
excel 2010 use the function 
STDeV to calculate sample 
standard deviation. The 
function STDeV is equivalent 
to the function STDeV.S 
in more recent versions of 
 excel. excel 2010 and  
excel 2013 can also 
 calculate the population 
standard deviation using the 
function STDeV.P.
2.7
There is no agreed-upon 
standard definition of a 
percentile. Different sources 
may give slightly different 
methods for calculating a 
percentile. However, all 
methods should result in 
similar insights, particularly 
for large data sets.

 
2.7 Analyzing Distributions 
45
 ii. Multiply this difference by d:
t 5 m 3 d.
 iii. To find the pth percentile, add t to the value in position i of the sorted data.
As an illustration, let us determine the 85th percentile for the home sales data in Table 2.9.
 1. Arrange the data in ascending order.
108,000 138,000 138,000 142,000 186,000 199,500  
208,000 254,000 254,000 257,500 298,000 456,250
 2. Compute k 5 (n 1 1) 3 p 5 (12 1 1) 3 0.85 5 11.05.
 3. Dividing 11.05 into the integer and decimal components gives us i 5 11 and d 5 0.05.
 
 Because d . 0, we must interpolate between the values in the 11th and 12th positions 
in our sorted data. The value in the 11th position is 298,000, and the value in the 12th 
position is 456,250.
  i. m 5 456,250 2 298,000 5 158,250.
 ii. t 5 m 3 d 5 158,250 3 0.05 5 7912.5.
iii. pth percentile 5 298,000 1 7912.5 5 305,912.5.
Therefore, $305,912.50 represents the 85th percentile of the home sales data.
The pth percentile can also be calculated in Excel using the function PERCENTILE.
EXC. Figure 2.18 shows the Excel calculation for the 85th percentile of the home sales data. 
The value in cell E13 is calculated using the formula 5PERCENTILE.EXC(B2:B13,0.85); 
B2:B13 defines the data set for which we are calculating a percentile, and 0.85 defines the 
percentile of interest.
Quartiles
It is often desirable to divide data into four parts, with each part containing approximately 
one-fourth, or 25 percent, of the observations. These division points are referred to as the 
quartiles and are defined as:
Q1 5 first quartile, or 25th percentile
Q2 5 second quartile, or 50th percentile (also the median)
Q3 5 third quartile, or 75th percentile.
To demonstrate quartiles, the home sales data are again arranged in ascending order. 
108,000 138,000 138,000 142,000 186,000 199,500  
208,000 254,000 254,000 257,500 298,000 456,250
We already identified Q2, the second quartile (median) as 203,750. To find Q1 and Q3, we 
must find the 25th and 75th percentiles.
For Q1,
 1. The data are arranged in ascending order, as previously done.
 2. Compute k 5 (n 1 1) 3 p 5 (12 1 1) 3 0.25 5 3.25.
 3.  Dividing 3.25 into the integer and decimal components gives us i 5 3 and d 5 0.25. 
Because d . 0, we must interpolate between the values in the 3rd and 4th positions 
in our sorted data. The value in the 3rd position is 138,000, and the value in the 4th 
position is 142,000.
 i. m 5 142,000 2 138,000 5 4000.
 ii. t 5 m 3 d 5 4000 3 0.25 5 1000.
iii. pth percentile 5 138,000 1 1000 5 139,000.
Therefore, the 25th percentile is 139,000. Similar calculations for the 75th percentile result 
in 75th percentile 5 256,625. 
The median corresponds 
to the 50th percentile of 
the data.
Versions of excel prior to 
excel 2010 use the function 
PerCenTILe to calculate 
a percentile. However, this 
excel function can result in 
counterintuitive percentile 
calculations for small data 
sets. although this function 
is still accepted in later 
versions of excel, we do not 
recommend the use of Per-
CenTILe to calculate a per-
centile; instead, we suggest 
the use of the excel function 
PerCenTILe.eXC.

46 
Chapter 2 Descriptive Statistics
The quartiles divide the home sales data into four parts, with each part containing 
25 percent of the observations.
108,000 
138,000 
138,000
142,000 
186,000 
199,500
208,000 
254,000 
254,000
257,500 
298,000 
456,250
Q1 5 139,000
Q2 5 203,750
Q3 5 256,625
The difference between the third and first quartiles is often referred to as the  interquartile 
range, or IQR. For the home sales data, IQR 5 Q3 2 Q1 5 256,625 2 139,000 5 117,625. 
Because it excludes the smallest and largest 25 percent of values in the data, the IQR is a 
useful measure of variation for data that have extreme values or are badly skewed.
A quartile can be computed in Excel using the function QUARTILE.EXC. Figure 2.18 
shows the calculations for first, second, and third quartiles for the home sales data. The 
formula used in cell E15 is 5QUARTILE.EXC(B2:B13,1). The range B2:B13 defines the 
data set, and 1 indicates that we want to compute the 1st quartile. Cells E16 and E17 use 
similar formulas to compute the second and third quartiles.
z-scores
A z-score allows us to measure the relative location of a value in the data set. More spe-
cifically, a z-score helps us determine how far a particular value is from the mean relative 
to the data set’s standard deviation. Suppose we have a sample of n observations, with 
the values denoted by x1, x2, . . . , xn. In addition, assume that the sample mean, x¯, and 
the sample standard deviation, s, are already computed. Associated with each value, xi, 
is another value called its z-score. Equation (2.7) shows how the z-score is computed for 
each xi:
z-SCORE
 
zi 5 xi 2 x
s
 
(2.7)
where
 
zi 5 the z-score for xi
 
x¯ 5 the sample mean
 
s 5 the sample standard deviation
The z-score is often called the standardized value. The z-score, zi, can be interpreted as 
the number of standard deviations, xi, is from the mean. For example, z1 5 1.2 indicates that 
x1 is 1.2 standard deviations greater than the sample mean. Similarly, z2 5 20.5 indicates 
that x2 is 0.5, or 1/2, standard deviation less than the sample mean. A z-score greater than 
zero occurs for observations with a value greater than the mean, and a z-score less than zero 
occurs for observations with a value less than the mean. A z-score of zero indicates that the 
value of the observation is equal to the mean.
The z-scores for the class size data are computed in Table 2.13. Recall the previously 
computed sample mean, x 5 44, and sample standard deviation, s 5 8. The z-score of 
21.50 for the fifth observation shows that it is farthest from the mean; it is 1.50 standard 
deviations below the mean.
The z-score can be calculated in Excel using the function STANDARDIZE. Fig-
ure 2.19 demonstrates the use of the STANDARDIZE function to compute z-scores for 
Versions of excel prior to 
excel 2010 used the func-
tion QUarTILe to compute 
quartiles. Similar to the old 
function PerCenTILe, this 
outdated excel function can 
produce odd results. There-
fore, we do not recommend 
the use of QUarTILe to 
compute quartiles; instead, 
one should use the function 
QUarTILe.eXC.

 
2.7 Analyzing Distributions 
47
the home sales data. To calculate the z-scores, we must provide the mean and standard 
deviation for the data set in the arguments of the STANDARDIZE function. For instance, 
the z-score in cell C2 is calculated with the formula 5STANDARDIZE(B2, $B$15, 
$B$16), where cell B15 contains the mean of the home sales data and cell B16 contains 
the standard deviation of the home sales data. We can then copy and paste this formula 
into cells C3:C13.
notice that we use $B$15 
and $B$16 in the excel 
STanDarDIZe function 
here. This provides abso-
lute references to cells B15 
and B16 so that, when we 
copy this formula to other 
cells, the formula always 
references cells B15 and 
B16. absolute and relative 
references in excel are 
described in more detail in 
appendix a.
Number of 
Students in 
Class (xi)
Deviation  
About the Mean
(xi 2 x¯)
z-Score
axi 2 x
s
b
46
   2
   2/8 5    .25
54
  10
  10/8 5   1.25
42
 22
 22/8 5  2.25
46
   2
   2/8 5    .25
32
212
212/8 5 21.50
Table 2.13  z-SCORES FOR THE CLASS SIZE DATA
FIGURe 2.19  CALCULATING z-SCORES FOR THE HOME SALES DATA IN EXCEL
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
2
3
4
5
6
7
8
9
10
11
12
138000
=STANDARDIZE(B2,$B$15,$B$16)
=STANDARDIZE(B3,$B$15,$B$16)
=STANDARDIZE(B4,$B$15,$B$16)
=STANDARDIZE(B5,$B$15,$B$16)
=STANDARDIZE(B6,$B$15,$B$16)
=STANDARDIZE(B7,$B$15,$B$16)
=STANDARDIZE(B8,$B$15,$B$16)
=STANDARDIZE(B9,$B$15,$B$16)
=STANDARDIZE(B10,$B$15,$B$16)
=STANDARDIZE(B11,$B$15,$B$16)
=STANDARDIZE(B12,$B$15,$B$16)
=STANDARDIZE(B13,$B$15,$B$16)
Mean: =AVERAGE(B2:B13)
=STDEV.S(B2:B13)
Standard Deviation:
254000
186000
257500
108000
254000
138000
298000
199500
208000
142000
456250
Selling Price ($)
z-Score
Home Sale
A
B
C
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
2
3
4
5
6
7
8
9
10
11
12
138,000
–0.862
0.358
–0.357
0.395
–1.177
0.358
–0.862
0.821
–0.215
–0.126
–0.820
2.486
Mean: $
$
Standard Deviation:
254,000
186,000
257,500
108,000
254,000
138,000
298,000
199,500
208,000
142,000
456,250
219,937.50
95,065.77
Selling Price ($)
z-Score
Home Sale
A
B
C

48 
Chapter 2 Descriptive Statistics
Empirical Rule
When the distribution of data exhibit a symmetric bell-shaped distribution, as shown in 
Figure 2.20, the empirical rule can be used to determine the percentage of data values that 
are within a specified number of standard deviations of the mean. Many, but not all, distri-
butions of data found in practice exhibit a symmetric bell-shaped distribution.
EMPIRICAL RULE
For data having a bell-shaped distribution:
 Approximately 68 percent of the data values will be within one standard 
 deviation of the mean.
 Approximately 95 percent of the data values will be within two standard 
 deviations of the mean.
 Almost all of the data values will be within three standard deviations of the 
mean.
The height of adult males in the United States has a bell-shaped distribution similar to 
that shown in Figure 2.20 with a mean of approximately 69.5 inches and standard deviation 
of approximately 3 inches. Using the empirical rule, we can draw the following conclusions.
 
 Approximately 68 percent of adult males in the United States have heights between 
69.5 2 3 5 66.5 and 69.5 1 3 5 72.5 inches.
 
 Approximately 95 percent of adult males in the United States have heights between 
63.5 and 75.5 inches.
 
 Almost all adult males in the United States have heights between 60.5 and 78.5 inches.
Identifying Outliers
Sometimes a data set will have one or more observations with unusually large or unusually small 
values. These extreme values are called outliers. Experienced statisticians take steps to identify 
outliers and then review each one carefully. An outlier may be a data value that has been incor-
rectly recorded; if so, it can be corrected before further analysis. An outlier may also be from an 
observation that doesn’t belong to the population we are studying and was incorrectly included 
FIGURe 2.20   A SYMMETRIC BELL-SHAPED DISTRIBUTION

 
2.7 Analyzing Distributions 
49
in the data set; if so, it can be removed. Finally, an outlier may be an unusual data value that has 
been recorded correctly and is a member of the population we are studying. In such cases, the 
observation should remain.
Standardized values (z-scores) can be used to identify outliers. Recall that the empiri-
cal rule allows us to conclude that for data with a bell-shaped distribution, almost all the 
data values will be within three standard deviations of the mean. Hence, in using z-scores 
to identify outliers, we recommend treating any data value with a z-score less than 23 or 
greater than 13 as an outlier. Such data values can then be reviewed to determine their 
accuracy and whether they belong in the data set.
Box Plots
A box plot is a graphical summary of the distribution of data. A box plot is developed from 
the quartiles for a data set. Figure 2.21 is a box plot for the home sales data. Here are the 
steps used to construct the box plot:
 1.  A box is drawn with the ends of the box located at the first and third quartiles. For 
the home sales data, Q1 5 139,000 and Q3 5 256,625. This box contains the middle 
50 percent of the data.
 2.  A vertical line is drawn in the box at the location of the median (203,750 for the home 
sales data).
 3.  By using the interquartile range, IQR 5 Q3 2 Q1, limits are located. The limits for 
the box plot are 1.5(IQR) below Q1 and 1.5(IQR) above Q3. For the home sales data, 
IQR 5 Q3 2 Q1 5 256,625 2 139,000 5 117,625. Thus, the limits are 139,000 2 
1.5(117,625) 5 237,437.5 and 256,625 1 1.5(117,625) 5 433,062.5. Data outside these 
limits are considered outliers.
 4.  The dashed lines in Figure 2.21 are called whiskers. The whiskers are drawn from the 
ends of the box to the smallest and largest values inside the limits computed in step 
3. Thus, the whiskers end at home sales values of 108,000 and 298,000.
 5.  Finally, the location of each outlier is shown with an asterisk (*). In Figure 2.21, we see 
one outlier, 456,250.
Box plots are also very useful for comparing different data sets. For instance, if we want 
to compare home sales from several different communities, we could create box plots for re-
cent home sales in each community. An example of such box plots is shown in Figure 2.22.
Clearly we would not expect 
a home sales price less 
than 0, so we could also 
define the lower limit here 
to be $0.
0
Q1
Median
Whisker
Outlier
Q3
100,000
200,000
Price ($)
300,000
*
400,000
500,000
IQR
FIGURe 2.21   BOX PLOT FOR THE HOME SALES DATA

50 
Chapter 2 Descriptive Statistics
What can we learn from these box plots? The most expensive houses appear to be 
in Shadyside and the cheapest houses in Hamilton. The median home selling price in 
Groton is about the same as the median home selling price in Irving. However, home 
sales prices in Irving have much greater variability. Homes appear to be selling in 
Irving for many different prices, from very low to very high. Home selling prices have 
the least variation in Groton and Hamilton. Unusually expensive home sales (relative 
to the respective distribution of home sales vales) have occurred in Fairview, Groton, 
and Irving, which appear as outliers. Groton is the only location with a low outlier, but 
note that most homes sell for very similar prices in Groton, so the selling price does 
not have to be too far from the median to be considered an outlier.
Note that box plots use a different definition of an outlier because the distribution of 
the data in a box plot is not assumed to follow a bell-shaped curve. However, the interpreta-
tion is the same. The outliers in a box plot are extreme values that should be investigated 
to ensure data accuracy.
The excel add-In XLMiner 
can generate multiple box 
plots to compare different 
data sets. We demonstrate 
the use of XLMiner to 
create a box plot in the 
appendix at the end of this 
chapter.
NOTES AND COMMENTS
1.  The empirical rule applies only to distributions 
that have an approximately bell-shaped distri-
bution. For distributions that do not have a bell-
shaped distribution, one can use  Chebyshev’s 
Theorem to make statements about the pro-
portion of data values that must be within a 
specified number of standard deviations of the 
mean. Chebyshev’s Theorem states that at least 
(1 2 1
z2) of the data values must be within z 
standard deviations of the mean, where z is any 
value greater than 1.
2.  There is no easy way to automatically generate 
box plots in standard Excel, but the XLMiner 
Add-In allows one to do so easily. We explain 
how to generate a box plot with XLMiner in the 
chapter appendix.
100,000
Fairview
Shadyside
Groton
Irving
Hamilton
200,000
300,000
400,000
500,000
Selling Price ($)
FIGURe 2.22   BOX PLOTS COMPARING HOME SALE PRICES IN DIFFERENT 
COMMUNITIES
Box plots can be drawn 
horizontally or verti-
cally. Figure 2.21 shows 
a horizontal box plot, and 
Figure 2.22 shows vertical 
box plots.

 
2.8 Measures of Association Between Two  Variables 
51
Measures of Association Between Two  Variables
Thus far we have examined numerical methods used to summarize the data for one variable at a 
time. Often a manager or decision maker is interested in the relationship between two variables. 
In this section, we present covariance and correlation as descriptive measures of the relationship 
between two variables. To illustrate these concepts, we consider the case of the sales manager of 
Queensland Amusement Park, who is in charge of ordering bottled water for purchase by park 
customers. The sales manager believes that daily bottled water sales in the summer are related to 
the outdoor temperature. Table 2.14 shows data for high temperatures and bottled water sales for 14 
summer days. The data have been sorted by high temperature from lowest value to highest value.
Scatter Charts
A scatter chart is a useful graph for analyzing the relationship between two variables. 
Figure 2.23 shows a scatter chart for sales of bottled water versus the high temperature 
2.8
0
High Temperature (˚F)
30
25
20
15
10
5
76
78
80
82
84
86
88
90
92
94
35
Sales (cases)
FIGURe 2.23   CHART SHOWING THE POSITIVE LINEAR RELATION BETWEEN 
SALES AND HIGH TEMPERATURES
High  
Temperature (°F)
Bottled Water 
Sales (cases)
78
23
79
22
80
24
80
22
82
24
83
26
85
27
86
25
87
28
87
26
88
29
88
30
90
31
92
31
Table 2.14   DATA FOR BOTTLED WATER SALES AT QUEENSLAND 
 AMUSEMENT PARK FOR A SAMPLE OF 14 SUMMER DAYS

52 
Chapter 2 Descriptive Statistics
experienced over 14 days. The scatter chart in the figure suggests that higher daily high 
temperatures are associated with higher bottled water sales. This is an example of a 
positive relationship because when one variable (high temperature) increases, the other 
variable (sales of bottled water) generally also increases. The scatter chart also suggests 
that a straight line could be used as an approximation for the relationship between high 
temperature and sales of bottled water. We will cover the creation of scatter charts in 
more detail in Chapter 3.
Covariance
Covariance is a descriptive measure of the linear association between two variables. For 
a sample of size n with the observations (x1, y1), (x2, y2), and so on, the sample covariance 
is defined as follows:
SAMPLE COVARIANCE
 
sxy 5 g (xi 2 x) (yi 2 y)
n 2 1
 
(2.8)
This formula pairs each xi with a yi. We then sum the products obtained by multiplying the 
deviation of each xi from its sample mean (xi 2 x) by the deviation of the corresponding 
yi from its sample mean (yi 2 y); this sum is then divided by n 2 1.
To measure the strength of the linear relationship between the high temperature x and 
the sales of bottled water y at Queensland, we use equation (2.8) to compute the sample co-
variance. The calculations in Table 2.15 show the computation g (xi 2 x) (yi 2 y). Note 
that for our calculations, x 5 84.6 and y 5 26.3.
xi
yi
xi 2 x
yi 2 y
(xi 2 x) (yi 2 y )
78
23
26.6
23.3
21.78
79
22
25.6
24.3
24.08
80
24
24.6
22.3
10.58
80
22
24.6
24.3
19.78
82
24
22.6
22.3
5.98
83
26
21.6
20.3
0.48
85
27
0.4
0.7
0.28
86
25
1.4
21.3
21.82
87
28
2.4
1.7
4.08
87
26
2.4
20.3
20.72
88
29
3.4
2.7
9.18
88
30
3.4
3.7
12.58
90
31
5.4
4.7
25.38
  92
 31
7.4
  4.7
  34.78
Totals
1185
368
0.6
20.2
166.42
x 5 84.6
y 5 26.3
sxy 5 g (xi 2 x) (yi 2 y)
n 2 1
5 166.42
14 2 1 5 12.8
Table 2.15   SAMPLE VARIANCE CALCULATIONS FOR DAILY HIGH 
 TEMPERATURE AND BOTTLED WATER SALES AT QUEENSLAND 
AMUSEMENT PARK

 
2.8 Measures of Association Between Two  Variables 
53
The covariance calculated in Table 2.15 is sxy 5 12.8. Because the covariance is greater 
than 0, it indicates a positive relationship between the high temperature and sales of bottled 
water. This verifies the relationship we saw in the scatter chart in Figure 2.23 that as the 
high temperature for a day increases, sales of bottled water generally increase. If the covari-
ance is near 0, then the x and y variables are not linearly related. If the covariance is less 
than 0, then the x and y variables are negatively related, which means that as x increases, y 
generally decreases.
The sample covariance can also be calculated in Excel using the COVARIANCE.S 
function. Figure 2.24 shows the data from Table 2.14 entered into an Excel Worksheet. 
The covariance is calculated in cell B17 using the formula 5COVARIANCE.S(A2:A15, 
B2:B15). The A2:A15 defines the range for the x variable (high temperature), and B2:B15 
defines the range for the y variable (sales of bottled water). Figure 2.25 demonstrates several 
possible scatter charts and their associated covariance values.
One problem with using covariance is that the magnitude of the covariance value is difficult 
to interpret. Larger sxy values do not necessarily mean a stronger linear relationship because 
the units of covariance depend on the units of x and y. For example, suppose we are interested 
file
WEB
BottledWater
If data consist of a popula-
tion of n observations, the 
population covariance  
sxy is computed by: 
σxy 5 g ( xi 2 μx) g ( yi 2 μy)
n
 . 
note that this equation is 
similar to equation (2.8), but 
uses population parameters 
instead of sample estimates 
(and divides by n instead of 
n – 1 for technical reasons be-
yond the scope of this book).
A
B
High Temperature
(degrees F)
Bottled Water Sales (cases)
87
78
79
80
80
82
83
85
86
87
88
88
90
92
26
23
22
24
22
24
26
27
25
28
29
30
31
31
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Covariance:
Correlation Coefficient:
5COVARIANCE.S(A2:A15,B2:B15)
5CORREL(A2:A15,B2:B15)
17
18
A
B
High Temperature
(degrees F)
Bottled Water
Sales (cases)
87
78
79
80
80
82
83
85
86
87
88
88
90
92
26
23
22
24
22
24
26
27
25
28
29
30
31
31
12.80
0.93
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Covariance:
Correlation Coefficient:
17
18
FIGURe 2.24   CALCULATING COVARIANCE AND CORRELATION COEFFICIENT FOR BOTTLED 
 WATER SALES USING EXCEL

54 
Chapter 2 Descriptive Statistics
y
x
sxy Positive:
(x and y are positively
linearly related)
y
x
sxy Approximately 0:
(x and y are not
linearly related)
sxy  Negative:
(x and y are negatively
linearly related)
y
x
FIGURe 2.25   SCATTER DIAGRAMS AND ASSOCIATED COVARIANCE VALUES 
FOR DIFFERENT VARIABLE RELATIONSHIPS
Versions of excel prior to 
excel 2010 use the func-
tion COVar to calculate 
covariance. This function 
calculates the covariance 
for a population, and it is 
equivalent to the function 
COVarIanCe.P in later 
versions of excel.
in the relationship between height x and weight y for individuals. Clearly the strength of the 
relationship should be the same whether we measure height in feet or inches. Measuring the 
height in inches, however, gives us much larger numerical values for (xi 2 x) than when we 
measure height in feet. Thus, with height measured in inches, we would obtain a larger value for 

 
2.8 Measures of Association Between Two  Variables 
55
the numerator g (xi 2 x) (yi 2 y) in equation (2.8)—and hence a larger covariance—when in 
fact the relationship does not change.
Correlation Coefficient
The correlation coefficient measures the relationship between two variables, and, unlike 
covariance, the relationship between two variables is not affected by the units of measure-
ment for x and y. For sample data, the correlation coefficient is defined as follows.
SAMPLE CORRELATION COEFFICIENT
 
rxy 5
sxy
sxsy
 
(2.9)
where
 
rxy 5 sample correlation coefficient
 
sxy 5 sample covariance
 
sx 5 sample standard deviation of x
 
sy 5 sample standard deviation of y
If data are a population, 
the population correlation 
coefficient is computed 
by rxy 5 
σxy
σxσy . note that 
this is similar to equation 
(2.9) but uses population 
 parameters instead of 
sample estimates.
The sample correlation coefficient is computed by dividing the sample covariance by the 
product of the sample standard deviation of x and the sample standard deviation of y. This 
scales the correlation coefficient so that it will always take values between 21 and 11.
Let us now compute the sample correlation coefficient for bottled water sales at 
Queensland Amusement Park. Recall that we calculated sxy 5 12.8 using equation (2.8). 
Using data in  Table 2.14, we can compute sample standard deviations for x and y.
 
sx 5 Å
g (xi 2 x) 2
n 2 1
5 4.36
 
sy 5 Å
g (yi 2 y) 2
n 2 1
5 3.15
The sample correlation coefficient is computed from equation (2.9) as follows:
rxy 5
sxy
sxsy
5
12.8
(4.36) (3.15) 5 0.93
The correlation coefficient can take only values between –1 and 11. Correlation 
coefficient values near 0 indicate no linear relationship between the x and y variables. 
Correlation coefficients greater than 0 indicate a positive linear relationship between 
the x and y variables. The closer the correlation coefficient is to 11, the closer the x and 
y values are to forming a straight line that trends upward to the right (positive slope). 
Correlation coefficients less than 0 indicate a negative linear relationship between the 
x and y variables. The closer the correlation coefficient is to –1, the closer the x and 
y values are to forming a straight line with negative slope. Because rxy 5 0.93 for the 
bottled water, we know that there is a very strong linear relationship between these two 
variables. As we can see in Figure 2.23, one could draw a straight line that would be 
very close to all of the data points in the scatter chart.

56 
Chapter 2 Descriptive Statistics
NOTES AND COMMENTS
1.  The correlation coefficient discussed in this 
chapter was developed by Karl Pearson and 
is sometimes referred to as Pearson product 
moment correlation coefficient. It is appro-
priate for use only with two quantitative 
variables. A variety of alternatives, such as 
the Spearman rank-correlation coefficient, 
exist to measure the association of categori-
cal variables.
2.  Correlation measures only the association between 
two variables. A large positive or large negative 
correlation coefficient does not indicate that a 
change in the value of one of the two variables 
causes a change in the value of the other variable.
Because the correlation coefficient defined here measures only the strength of the linear 
relationship between two quantitative variables, it is possible for the correlation coefficient 
to be near zero, suggesting no linear relationship, when the relationship between the two 
variables is nonlinear. For example, the scatter diagram in Figure 2.26 shows the relation-
ship between the amount spent by a small retail store for environmental control (heating 
and cooling) and the daily high outside temperature over 100 days.
The sample correlation coefficient for these data is rxy 5 20.007 and indicates 
that there is no linear relationship between the two variables. However, Figure 2.26 
provides strong visual evidence of a nonlinear relationship. That is, we can see that 
as the daily high outside temperature increases, the money spent on environmental 
control first decreases as less heating is required and then increases as greater cooling 
is required.
We can compute correlation coefficients using the Excel function CORREL. The cor-
relation coefficient in Figure 2.24 is computed in cell B18 for the sales of bottled water 
using the formula 5CORREL(A2:A15, B2:B15) where A2:A15 defines the range for the x 
variable and B2:B15 defines the range for the y variable.
Outside Temperature (˚F)
0
20
40
60
80
100
$1,600
$1,400
$1,200
$1,000
$800
$600
$400
$200
$0
Dollars Spent on Environmental Control
FIGURe 2.26   EXAMPLE OF NONLINEAR RELATIONSHIP PRODUCING A 
 CORRELATION COEFFICIENT NEAR ZERO

 
Glossary 
57
Summary
In this chapter we have provided an introduction to descriptive statistics that can be used to sum-
marize data. We began by explaining the need for data collection, defining the types of data one 
may encounter, and providing a few commonly used sources for finding data. We presented several 
useful functions for modifying data in Excel, such as sorting and filtering to aid in data analysis.
We introduced the concept of a distribution and explained how to generate frequency, rela-
tive, percent, and cumulative distributions for data. We also presented histograms as a way to 
visualize the distribution of data. We then introduced measures of location for a distribution 
of data such as mean, median, mode, and geometric mean, as well as measures of variability 
such as range, variance, standard deviation, coefficient of variation, and interquartile range. 
We presented additional measures for analyzing a distribution of data including percentiles, 
quartiles, and z-scores. We showed that box plots are effective for visualizing a distribution.
Finally, we discussed measures of association between two variables. Scatter plots 
allow one to visualize the relationship between variables. Covariance and the correlation 
coefficient summarize the linear relationship between variables into a single number.
Glossary
Data The facts and figures collected, analyzed, and summarized for presentation and 
 interpretation.
Variable A characteristic or quantity of interest that can take on different values.
Observation A set of values corresponding to a set of variables.
Variation Differences in values of a variable over observations.
Random (uncertain) variable A quantity whose values are not known with certainty.
Population The set of all elements of interest in a particular study.
Sample A subset of the population.
Random sampling The act of collecting a sample that ensures that (1) each element se-
lected comes from the same population and (2) each element is selected  independently.
Quantitative data Data where numerical values are used to indicate magnitude, such as 
how many or how much. Arithmetic operations such as addition, subtraction, and multipli-
cation can be performed on quantitative data.
Categorical data Data where categories of like items are identified by labels or names. 
Arithmetic operations cannot be performed on categorical data.
Cross-sectional data Data collected at the same or approximately the same point in time.
Time series data Data that are collected over a period of time (minutes, hours, days, 
months, years, etc.).
Frequency distribution A tabular summary of data showing the number (frequency) of 
data values in each of several nonoverlapping bins.
Bins The nonoverlapping groupings of data used to create a frequency distribution. Bins 
for categorical data are also known as classes.
Relative frequency distribution A tabular summary of data showing the fraction or pro-
portion of data values in each of several nonoverlapping bins.
Percent frequency distribution A tabular summary of data showing the percentage of data 
values in each of several nonoverlapping bins.
Histogram A graphical presentation of a frequency distribution, relative frequency dis-
tribution, or percent frequency distribution of quantitative data constructed by placing the 
bin intervals on the horizontal axis and the frequencies, relative frequencies, or percent 
frequencies on the vertical axis.
Skewness A measure of the lack of symmetry in a distribution.
Cumulative frequency distribution A tabular summary of quantitative data showing the 
number of data values that are less than or equal to the upper class limit of each bin.

58 
Chapter 2 Descriptive Statistics
Mean (Arithmetic Mean) A measure of central location computed by summing the data 
values and dividing by the number of observations.
Median A measure of central location provided by the value in the middle when the data 
are arranged in ascending order.
Mode A measure of location, defined as the value that occurs with greatest frequency.
Geometric Mean A measure of location that is calculated by finding the nth root of the 
product of n values.
Growth Factor The percentage increase of a value over a period of time is calculated using the 
formula (1 – growth factor). A growth factor less than 1 indicates negative growth, whereas a 
growth factor greater than 1 indicates positive growth. The growth factor cannot be less than zero.
Range A measure of variability, defined to be the largest value minus the smallest value.
Variance A measure of variability based on the squared deviations of the data values about 
the mean.
Standard deviation A measure of variability computed by taking the positive square root 
of the variance.
Coefficient of variation A measure of relative variability computed by dividing the stan-
dard deviation by the mean and multiplying by 100.
Percentile A value such that approximately p percent of the observations have values less 
than the pth percentile; hence, approximately (100p) percent of the observations have val-
ues greater than the pth percentile. The 50th percentile is the median.
Quartile The 25th, 50th, and 75th percentiles, referred to as the first quartile, the second 
quartile (median), and third quartile, respectively. The quartiles can be used to divide a data 
set into four parts, with each part containing approximately 25 percent of the data.
Interquartile range The difference between the third and first quartiles.
z-score A value computed by dividing the deviation about the mean (xi 2 x) by the stan-
dard deviation s. A z-score is referred to as a standardized value and denotes the number of 
standard deviations that xi is from the mean.
Empirical rule A rule that can be used to compute the percentage of data values that must 
be within one, two, and three standard deviations of the mean for data that exhibit a bell-
shaped distribution.
Outlier An unusually large or unusually small data value.
Box plot A graphical summary of data based on the quartiles of a distribution.
Scatter chart A graphical presentation of the relationship between two quantitative variables. 
One variable is shown on the horizontal axis, and the other variable is shown on the vertical axis.
Covariance A measure of linear association between two variables. Positive values indi-
cate a positive relationship; negative values indicate a negative relationship.
Correlation coefficient A standardized measure of linear association between two vari-
ables that takes on values between 21 and 11. Values near 21 indicate a strong negative 
linear relationship, values near 11 indicate a strong positive linear relationship, and values 
near zero indicate the lack of a linear relationship.
Problems
 1.  A Wall Street Journal subscriber survey asked 46 questions about subscriber character-
istics and interests. State whether each of the following questions provides categorical or 
quantitative data.
a. What is your age?
b. Are you male or female?
c.  When did you first start reading the WSJ? High school, college, early career, midca-
reer, late career, or retirement?
d. How long have you been in your present job or position?
e.  What type of vehicle are you considering for your next purchase? Nine response 
 categories include sedan, sports car, SUV, minivan, and so on.

 
Problems 
59
 2.  The following table contains a partial list of countries, the continents on which they are 
located, and their respective gross domestic products (GDP) in U.S. dollars. A list of 
125 countries and their GDPs is contained in the file GDPlist. 
Country
Continent
GDP  
(millions of US$) 
Afghanistan
Asia
18,181 
Albania
Europe
12,847 
Algeria
Africa
190,709 
Angola
Africa
100,948 
Argentina
South America
447,644 
Australia
Oceania
1,488,221 
Austria
Europe
419,243 
Azerbaijan
Europe
62,321 
Bahrain
Asia
26,108 
Bangladesh
Asia
113,032 
Belarus
Europe
55,483 
Belgium
Europe
513,396 
Bolivia
South America
24,604 
Bosnia and Herzegovina
Europe
17,965 
Botswana
Africa
17,570
a.  Sort the countries in GDPlist from largest to smallest GDP. What are the top ten 
countries according to GDP?
b.  Filter the countries to display only the countries located in Africa. What are the top 
five countries located in Africa according to GDP?
c. What are the top five countries by GDP that are located in Europe?
 3.  Ohio Logistics manages the logistical activities for firms by matching companies that 
need products shipped with carriers that can provide the best rates and best service for the 
companies. Ohio Logistics is very concerned that it uses carriers that get their customers’ 
material delivered on time, so it carefully monitors its carriers’ on-time percentage of de-
liveries. The following table contains a list of the carriers used by Ohio Logistics and the 
corresponding on-time percentages for the current and previous year.
Carrier
Previous Year On-Time 
Percentage (%)
Current Year On-Time 
Percentage (%)
Blue Box Shipping
88.4
94.8
Cheetah LLC
89.3
91.8
Granite State Carriers
81.8
87.6
Honsin Limited
74.2
80.1
Jones Brothers
68.9
82.8
Minuteman Company
91.0
84.2
Rapid Response
78.8
70.9
Smith Logistics
84.3
88.7
Super Freight
92.1
86.8
a.  Sort the carriers in descending order by their current year’s on-time percentage. Which 
carrier is providing the best service in the current year? Which carrier is providing the 
worst service in the current year?
b.  Calculate the change in on-time percentage from the previous to the current year for 
each carrier. Use Excel’s conditional formatting to highlight the carriers whose on-
time percentage decreased from the previous year to the current year.
c. 
 Use Excel’s conditional formatting tool to create data bars for the change in on-time 
percentage from the previous year to the current year for each carrier calculated in part b.
d. Which carriers should Ohio Logistics try to use in the future? Why?
file
WEB
GDPlist 
file
WEB
Carriers

60 
Chapter 2 Descriptive Statistics
 4. A partial relative frequency distribution is given.
Class
Relative Frequency
A
0.22
B
0.18
C
0.40
D
a. What is the relative frequency of class D?
b. The total sample size is 200. What is the frequency of class D?
c. Show the frequency distribution.
d. Show the percent frequency distribution.
 5.  In a recent report, the top five syndicated television programs were The Big Bang Theory 
(BBT), Judge Judy (JJ), Wheel of Fortune (WoF), Jeopardy (Jep), and Two and a Half 
Men (THM). The preferred shows for a sample of 50 viewers are shown in the following 
table:
WoF
Jep
JJ
Jep
BBT
THM
WoF
BBT
BBT
BBT
Jep
BBT
WoF
WoF
WoF
WoF
THM
BBT
THM
WoF
BBT
JJ
JJ
Jep
BBT
BBT
BBT
JJ
JJ
Jep
JJ
WoF
THM
WoF
WoF
THM
BBT
WoF
JJ
JJ
Jep
BBT
WoF
Jep
Jep
WoF
THM
BBT
BBT
Jep
a. Are these data categorical or quantitative?
b. Provide frequency and percent frequency distributions.
c.  On the basis of the sample, which television show has the largest viewing audience? 
Which one has the second largest?
 6.  In a study of how chief executive officers (CEOs) spend their days, it was found that 
CEOs spend an average of about 18 hours per week in meetings, not including conference 
calls, business meals, and public events. Shown here are the times spent per week in meet-
ings (hours) for a sample of 25 CEOs:
14
15
18
23
15
19
20
13
15
23
23
21
15
20
21
16
15
18
18
19
19
22
23
21
12
a.  What is the least amount of time a CEO spent per week on meetings in this sample? 
The highest?
b.  Use a class width of 2 hours to prepare a frequency distribution and a percent fre-
quency distribution for the data.
c. Prepare a histogram and comment on the shape of the distribution.
 7.  Consumer complaints are frequently reported to the Better Business Bureau. Industries 
with the most complaints to the Better Business Bureau are often banks, cable and satellite 
file
WEB
TVshows
file
WEB
CEOtime

 
Problems 
61
television companies, collection agencies, cellular phone providers, and new car dealer-
ships. The results for a sample of 200 complaints are in the file BBB.
a. Show the frequency and percent frequency of complaints by industry.
b. Which industry had the highest number of complaints?
c. Comment on the percentage frequency distribution for complaints.
 8.  Reports have found that many U.S. adults would rather live in a different type of com-
munity than where they are living. A national survey of 2260 adults asked: “Where do 
you live now?” and “What do you consider to be the ideal community?” Response options 
were City (C), Suburb (S), Small Town (T), or Rural (R). A representative portion of this 
survey for a sample of 100 respondents is as follows.
Where do you live now?
S T R C R R T C S T C S C S T
S S C S S T T C C S T C S T C
T R S S T C S C T C T C T C R
C C R T C S S T S C C C R S C
S S C C S C R T T T C R T C R
C T R R C T C C R T T R S R T
T S S S S S C C R T
What do you consider to be the ideal community?
S C R R R S T S S T T S C S T
C C R T R S T T S S C C T T S
S R C S C C S C R C T S R R R
C T S T T T R R S C C R R S S
S T C T T C R T T T C T T R R
C S R T C T C C T T T R C R T
T C S S C S T S S R
a. Provide a percent frequency distribution and a histogram for each question.
b. Where are most adults living now?
c. Where do most adults consider the ideal community?
d.  What changes in living areas would you expect to see if people moved from where 
they currently live to their ideal community?
 9. Consider the following data.
14
24
18
22
19
18
16
22
24
17
15
16
19
23
24
16
16
26
21
16
20
22
16
12
24
23
19
25
20
25
21
19
21
25
23
24
22
19
20
20
a.  Develop a frequency distribution using classes of 12–14, 15–17, 18–20, 21–23, and 
24–26.
b.  Develop a relative frequency distribution and a percent frequency distribution using 
the classes in part a.
file
WEB
BBB
file
WEB
Communities
file
WEB
Frequency 

62 
Chapter 2 Descriptive Statistics
10. Consider the following frequency distribution.
Class
Frequency
10–19
10
20–29
14
30–39
17
40–49
 7
50–59
 2
 
Construct a cumulative frequency distribution.
11.  The owner of an automobile repair shop studied the waiting times for customers who ar-
rive at the shop for an oil change. The following data with waiting times in minutes were 
collected over a 1-month period.
2 5 10 12 4 4 5 17 11 8 9 8 12 21 6 8 7 13 18 3
 
Using classes of 0–4, 5–9, and so on, show:
a. The frequency distribution.
b. The relative frequency distribution.
c. The cumulative frequency distribution.
d. The cumulative relative frequency distribution.
e. The proportion of customers needing an oil change who wait 9 minutes or less.
12.  Approximately 1.65 million high school students take the Scholastic Aptitude Test (SAT) 
each year, and nearly 80 percent of the college and universities without open admissions 
policies use SAT scores in making admission decisions. The current version of the SAT 
includes three parts: reading comprehension, mathematics, and writing. A perfect com-
bined score for all three parts is 2400. A sample of SAT scores for the combined three-part 
SAT are as follows:
1665
1525
1355
1645
1780
1275
2135
1280
1060
1585
1650
1560
1150
1485
1990
1590
1880
1420
1755
1375
1475
1680
1440
1260
1730
1490
1560
940
1390
1175
a.  Show a frequency distribution and histogram. Begin with the first bin starting at 800, 
and use a bin width of 200.
b. Comment on the shape of the distribution.
c.  What other observations can be made about the SAT scores based on the tabular and 
graphical summaries?
13. Consider a sample with data values of 10, 20, 12, 17, and 16.
a. Compute the mean and median.
b.  Consider a sample with data values 10, 20, 12, 17, 16, and 12. How would you expect 
the mean and median for these sample data to compare to the mean and median for 
part a (higher, lower, or the same)? Compute the mean and median for the sample data 
10, 20, 12, 17, 16, and 12.
14.  Consider a sample with data values of 27, 25, 20, 15, 30, 34, 28, and 25. Compute the 20th, 
25th, 65th, and 75th percentiles.
15.  Consider a sample with data values of 53, 55, 70, 58, 64, 57, 53, 69, 57, 68, and 53. Com-
pute the mean, median, and mode.
16.  If an asset declines in value from $5,000 to $3,500 over 9 years, what is the mean annual 
growth rate in the asset’s value over these 9 years?
file
WEB
RepairShop
file
WEB
SAT

 
Problems 
63
17.  Suppose that you initially invested $10,000 in the Stivers mutual fund and $5,000 in the 
Trippi mutual fund. The value of each investment at the end of each subsequent year is 
provided in the table:
Year
Stivers ($)
Trippi ($)
1
11,000
5,600
2
12,000
6,300
3
13,000
6,900
4
14,000
7,600
5
15,000
8,500
6
16,000
9,200
7
17,000
9,900
8
18,000
10,600
 
Which of the two mutual funds performed better over this time period?
18.  The average time that Americans commute to work is 27.7 minutes (Sterling’s Best Places, 
April 13, 2012). The average commute times in minutes for 48 cities are as follows:
Albuquerque
23.3
Jacksonville
26.2
Phoenix
28.3
Atlanta
28.3
Kansas City
23.4
Pittsburgh
25.0
Austin
24.6
Las Vegas
28.4
Portland
26.4
Baltimore
32.1
Little Rock
20.1
Providence
23.6
Boston
31.7
Los Angeles
32.2
Richmond
23.4
Charlotte
25.8
Louisville
21.4
Sacramento
25.8
Chicago
38.1
Memphis
23.8
Salt Lake City
20.2
Cincinnati
24.9
Miami
30.7
San Antonio
26.1
Cleveland
26.8
Milwaukee
24.8
San Diego
24.8
Columbus
23.4
Minneapolis
23.6
San Francisco
32.6
Dallas
28.5
Nashville
25.3
San Jose
28.5
Denver
28.1
New Orleans
31.7
Seattle
27.3
Detroit
29.3
New York
43.8
St. Louis
26.8
El Paso
24.4
Oklahoma City
22.0
Tucson
24.0
Fresno
23.0
Orlando
27.1
Tulsa
20.1
Indianapolis
24.8
Philadelphia 
34.2
Washington, D.C.
32.8
a. What is the mean commute time for these 48 cities?
b. What is the median commute time for these 48 cities?
c. What is the mode for these 48 cities?
d. What is the variance and standard deviation of commute times for these 48 cities?
e. What is the third quartile of commute times for these 48 cities?
19.  Suppose that the average waiting time for a patient at a physician’s office is just over 
29 minutes. To address the issue of long patient wait times, some physician’s offices are us-
ing wait-tracking systems to notify patients of expected wait times. Patients can adjust their 
arrival times based on this information and spend less time in waiting rooms. The following 
data show wait times (in minutes) for a sample of patients at offices that do not have a wait-
tracking system and wait times for a sample of patients at offices with such systems.
Without Wait-Tracking  
System
With Wait-Tracking  
System
24
31
67
11
17
14
20
18
31
12
44
37
12
 9
23
13
16
12
37
15
file
WEB
CommuteTimes 
file
WEB
PatientWaits 

64 
Chapter 2 Descriptive Statistics
a.  What are the mean and median patient wait times for offices with a wait-tracking system? 
What are the mean and median patient wait times for offices without a wait-tracking system?
b.  What are the variance and standard deviation of patient wait times for offices with a 
wait-tracking system? What are the variance and standard deviation of patient wait 
times for visits to offices without a wait tracking system?
c. Create a box plot for patient wait times for offices without a wait-tracking system.
d. Create a box plot for patient wait times for offices with a wait-tracking system.
e.  Do offices with a wait-tracking system have shorter patient wait times than offices 
without a wait-tracking system? Explain.
20.  According to the National Education Association (NEA), teachers generally spend more 
than 40 hours each week working on instructional duties. The following data show the 
number of hours worked per week for a sample of 13 high school science teachers and a 
sample of 11 high school English teachers.
 
High school science teachers 53 56 54 54 55 58 49 61 54 54 52 53 54
 
High school English teachers 52 47 50 46 47 48 49 46 55 44 47
a.  What is the median number of hours worked per week for the sample of 13 high school 
science teachers?
b.  What is the median number of hours worked per week for the sample of 11 high school 
English teachers?
c. Create a box plot for the number of hours worked for high school science teachers.
d. Create a box plot for the number of hours worked for high school English teachers.
e. Comment on the differences between the box plots for science and English teachers.
21. Return to the waiting times given for the physician’s office in Problem 19.
a.  Considering only offices without a wait tracking system, what is the z-score for the 
tenth patient in the sample (wait time 5 37 minutes)?
b.  Considering only offices with a wait tracking system, what is the z-score for the sixth 
patient in the sample (wait time 5 37 minutes)? How does this z-score compare with 
the z-score you calculated for part a?
c.  Based on z-scores, do the data for offices without a wait tracking system contain any 
outliers? Based on z-scores, do the data for offices without a wait tracking system 
contain any outliers?
22.  The results of a national survey showed that on average, adults sleep 6.9 hours per night. 
Suppose that the standard deviation is 1.2 hours and that the number of hours of sleep fol-
lows a bell-shaped distribution.
a.  Use the empirical rule to calculate the percentage of individuals who sleep between 
4.5 and 9.3 hours per day.
b. What is the z-value for an adult who sleeps 8 hours per night?
c. What is the z-value for an adult who sleeps 6 hours per night?
23.  Suppose that the national average for the math portion of the College Board’s SAT is 515. 
The College Board periodically rescales the test scores such that the standard deviation is 
approximately 100. Answer the following questions using a bell-shaped distribution and 
the empirical rule for the math test scores.
a. What percentage of students have an SAT math score greater than 615?
b. What percentage of students have an SAT math score greater than 715?
c. What percentage of students have an SAT math score between 415 and 515?
d. What is the z-score for student with an SAT math score of 620?
e. What is the z-score for a student with an SAT math score of 405?
24. Five observations taken for two variables follow.
xi 
4
6
11
3
16
yi 
50
50
40
60
30
a. Develop a scatter diagram with x on the horizontal axis.
b.  What does the scatter diagram developed in part a indicate about the relationship 
file
WEB
Teachers
file
WEB
PatientWaits

 
Problems 
65
c. Compute and interpret the sample covariance.
d. Compute and interpret the sample correlation coefficient.
25.  The scatter chart in the following figure was created using sample data for profits and 
market capitalizations from a sample of firms in the Fortune 500.
0
Profits ($ millions)
Market Cap ($ millions)
0
4,000
8,000
12,000
16,000
200,000
160,000
120,000
80,000
40,000
a.  What does this scatter chart indicate about the relationship between profits and market 
capitalization? Discuss.
b.  The data used to produce this are contained in the file Fortune500. Calculate the co-
variance between profits and market capitalization. What does the covariance indicate 
about the relationship between profits and market capitalization? Discuss.
c.  Calculate the correlation coefficient between profits and market capitalization. What 
does the correlations coefficient indicate about the relationship between profits and 
market capitalization?
26.  The recent economic downturn resulted in the loss of jobs and an increase in delinquent 
loans for housing. In projecting where the real estate market was headed in the coming 
year, economists studied the relationship between the jobless rate and the percentage of 
delinquent loans. The expectation was that if the jobless rate continued to increase, there 
would also be an increase in the percentage of delinquent loans. The following data show 
the jobless rate and the delinquent loan percentage for 27 major real estate markets.
Metro Area
Jobless  
Rate (%)
Delinquent  
Loan (%)
Metro Area
Jobless  
Rate (%)
Delinquent  
Loan (%)
Atlanta
7.1
7.02
New York
6.2
5.78
Boston
5.2
5.31
Orange County
6.3
6.08
Charlotte
7.8
5.38
Orlando
7.0
10.05
Chicago
7.8
5.40
Philadelphia
6.2
4.75
Dallas
5.8
5.00
Phoenix
5.5
7.22
Denver
5.8
4.07
Portland
6.5
3.79
Detroit
9.3
6.53
Raleigh
6.0
3.62
Houston
5.7
5.57
Sacramento
8.3
9.24
Jacksonville
7.3
6.99
St. Louis
7.5
4.40
Las Vegas
7.6
11.12
San Diego
7.1
6.91
Los Angeles
8.2
7.56
San Francisco
6.8
5.57
Miami
7.1
12.11
Seattle
5.5
3.87
Minneapolis
6.3
4.39
Tampa
7.5
8.42
Nashville
6.6
4.78
Source: The Wall Street Journal, January 27, 2009.
a.  Compute the correlation coefficient. Is there a positive correlation between the jobless 
rate and the percentage of delinquent housing loans? What is your interpretation?
b.  Show a scatter diagram of the relationship between the jobless rate and the percentage 
f d li
h
i
l
file
WEB
Fortune500
file
WEB
JoblessRate

66 
Chapter 2 Descriptive Statistics
Case Heavenly Chocolates Web Site  Transactions
Heavenly Chocolates manufactures and sells quality chocolate products at its plant and 
retail store located in Saratoga Springs, New York. Two years ago, the company developed 
a Web site and began selling its products over the Internet. Web site sales have exceeded 
the company’s expectations, and management is now considering strategies to increase 
sales even further. To learn more about the Web site customers, a sample of 50 Heavenly 
Chocolate transactions was selected from the previous month’s sales. Data showing the 
day of the week each transaction was made, the type of browser the customer used, the 
time spent on the Web site, the number of Web site pages viewed, and the amount spent by 
each of the 50 customers are contained in the file named Shoppers. A portion of the data is 
shown in the table that follows:
Customer
Day
Browser
Time (min)
Pages 
Viewed
Amount 
Spent ($)
 1
Mon
Internet Explorer
12.0
4
54.52
 2
Wed
Other
19.5
6
94.90
 3
Mon
Internet Explorer
8.5
4
26.68
 4
Tue
Firefox
11.4
2
44.73
 5
Wed
Internet Explorer
11.3
4
66.27
 6
Sat
Firefox
10.5
6
67.80
 7
Sun
Internet Explorer
11.4
2
36.04
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
48
Fri
Internet Explorer
9.7
5
103.15
49
Mon
Other
7.3
6
52.15
50
Fri
Internet Explorer
13.4
3
98.75
Heavenly Chocolates would like to use the sample data to determine whether online 
shoppers who spend more time and view more pages also spend more money during their 
visit to the Web site. The company would also like to investigate the effect that the day of 
the week and the type of browser have on sales.
Managerial Report
Use the methods of descriptive statistics to learn about the customers who visit the Heav-
enly Chocolates Web site. Include the following in your report.
 1.  Graphical and numerical summaries for the length of time the shopper spends on the 
Web site, the number of pages viewed, and the mean amount spent per transaction. 
Discuss what you learn about Heavenly Chocolates’ online shoppers from these nu-
merical summaries.
 2.  Summarize the frequency, the total dollars spent, and the mean amount spent per 
transaction for each day of week. What observations can you make about Heavenly 
Chocolates’ business based on the day of the week? Discuss.
 3.  Summarize the frequency, the total dollars spent, and the mean amount spent per 
transaction for each type of browser. What observations can you make about Heavenly 
Chocolate’s business based on the type of browser? Discuss.
 4.  Develop a scatter diagram, and compute the sample correlation coefficient 
to explore the relationship between the time spent on the Web site and the 
file
WEB
HeavenlyChocolates

 dollar amount spent. Use the horizontal axis for the time spent on the Web site. 
 Discuss.
 5.  Develop a scatter diagram, and compute the sample correlation coefficient to 
explore the relationship between the number of Web site pages viewed and the 
amount spent. Use the horizontal axis for the number of Web site pages viewed. 
Discuss.
 6.  Develop a scatter diagram, and compute the sample correlation coefficient to explore 
the relationship between the time spent on the Web site and the number of pages viewed. 
Use the horizontal axis to represent the number of pages viewed. Discuss.
Appendix Creating Box Plots in XLMiner
XLMiner, an Add-in for Excel developed by Frontline Systems, can be used for basic statis-
tical analysis, data visualization, and data mining. In this chapter appendix, we demonstrate 
the use of XLMiner in making box plots. There are no easy methods for creating box plots 
in Excel without an Add-in. XLMiner makes it very easy to create box plots for single- and 
multiple-variable data sets.
We demonstrate the use of XLMiner to create a box plot with multiple variables 
using an expanded form of the home sales data illustrated in Figure 2.22. The data 
used in generating the box plots in this figure are entered into an Excel Worksheet. 
In addition to data on the location of the home and selling price, the Excel Worksheet 
contains data on the type of home sold (detached or condo) and on the size of the 
home in square feet. Rows 1–20 of this Excel Worksheet are shown in Figure 2.27. 
file
WEB
HomeSalesComparison 
A
B
C
D
E
F
G
H
I
J
K
L
Selling Price ($)
Size (ft2)
Location
302,000
265,000
280,000
220,000
2150
1890
1540
1790
Fairview
Fairview
Fairview
Fairview
Type
Detached
Detached
Detached
Detached
1
2
3
4
5
149,000
155,000
198,000
187,000
208,000
174,000
336,000
398,000
378,000
298,000
425,000
344,000
302,000
300,000
298,000
1500
1450
1700
1900
1800
1650
1750
1950
1780
1600
2250
1780
1750
1700
1540
Fairview
Fairview
Fairview
Fairview
Fairview
Fairview
Shadyside
Shadyside
Shadyside
Shadyside
Shadyside
Shadyside
Shadyside
Shadyside
Shadyside
Detached
Detached
Condo
Condo
Detached
Detached
Condo
Condo
Condo
Detached
Detached
Condo
Condo
Detached
Detached
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Bar Chart
Histogram
Parallel Coordinates
Scatterplot Matrix
Variable
Line Chart
Scatter Plot
BoxPlot
Boxplots help analyze how values are distributed across a range
FIGURe 2.27   USING XLMINER TO ANALYZE HOME SELLING PRICE COMPARISON DATA
 
Appendix Creating Box Plots in XLMiner 
67

68 
Chapter 2 Descriptive Statistics
To create box plots for the home sales data in Figure 2.22 using XLMiner, we use 
the following steps:
Step 1. Select a cell containing home sales data (any cell in the range A1:D51)
Step 2. Click the XLMINER tab in the Ribbon
Step 3. In the Data Analysis group, select Explore and click Chart Wizard
Step 4.  When the Chart Wizard New Chart dialog box opens, select Box Plot, and 
click Next
Step 5.  In the Y Axis Selection Dialog box, select Selling Price ($)  
Click Next
Step 6.  In the X Axis Selection Dialog box select Location 
Click Finish
The completed box plot appears in Figure 2.28.
Figure 2.28 shows that the most expensive houses appear to be in Shadyside and the 
cheapest houses in Hamilton. Home selling prices have the least variation in Groton and 
Hamilton, while there is a large amount of variation in home selling prices in Irving. The 
percentile and other statistical information shown in Figure 2.28 can be viewed by hovering 
the pointer over one of the box plots.
0
300,000
350,000
400,000
250,000
200,000
150,000
100,000
50,000
Fairview
Groton
Hamilton
Irving
Shadyside
450,000
Location
Selling Price ($)
75th Percentile: 223250
25th Percentile: 109750
Maximum: 
454000
Minimum: 
103000
Median: 
158000
Mean: 
200900
FIGURe 2.28   BOX PLOTS CREATED USING XLMINER FOR THE HOME SALES COMPARISON DATA

XLMiner provides several tools for exploring the data and interacting with the charts 
created. We can easily adjust the variables being displayed on both the horizontal and verti-
cal axes. To change these values, follow these steps:
Step 1. Click on Location below the horizontal axis
 
Change this value to Type
Step 2. Click on Selling Price ($) next to the vertical axis
 
Change this value to Size (ft2)
Step 3. Click No Panel next to Panel By:
 
Change this value to Location
This will now display box plots for Detached and Condo types of home sales versus the size 
of the homes sold for each location. This chart appears in Figure 2.29. Here we can make 
several interesting comparisons. Figure 2.29 shows that Hamilton tends to have smaller 
homes regardless of whether they are condos or detached homes and that detached homes 
tend to have more variation in size than condos in all locations.
In the Filters area of the box plot chart shown in Figure 2.29, we can adjust the data used 
in constructing the box plots. By unchecking the boxes next to the locations, we can delete 
all home sales in a particular location from the data used to create the box plots. We can also 
filter the data displayed within each box plot by adjusting the slider bar handles below Size 
(ft2) in the Filters area. For instance if we drag the left handle from 950 to 1200 (or by click-
ing on the value of 950 above the slider handle and replacing it with 1200), all sales of homes 
less than 1200 square feet in size will not be used in creating the box plots.
 
Appendix Creating Box Plots in XLMiner 
69
0
2500
2000
1500
1000
500
Condo
Detached
Type
Size (ft2)
0
2500
2000
1500
1000
500
Condo
Detached
Type
Size (ft2)
0
2500
2000
1500
1000
500
Condo
Detached
Type
Size (ft2)
0
2500
2000
1500
1000
500
Condo
Detached
Type
Size (ft2)
0
2500
2000
1500
1000
500
Condo
Detached
Type
Size (ft2)
Location: Fairview
Location: Groton
Location: Hamilton
Location: Shadyside
Location: Irving
FIGURe 2.29   MODIFIED BOX PLOTS CREATED IN XLMINER FOR THE HOME SALES  COMPARISON DATA

CHAPTER 3
Data Visualization
CONTENTS
3.1 
 OVERVIEW OF DATA 
 VISUALIZATION
Effective Design Techniques
3.2 
TABLES
Table Design Principles
Crosstabulation
PivotTables in Excel
3.3 
CHARTS
Scatter Charts
Line Charts
Bar Charts and Column Charts
A Note on Pie Charts and  
3-D Charts
Bubble Charts
Heat Maps
Additional Charts for Multiple 
Variables
PivotCharts in Excel
3.4 
 ADVANCED DATA 
 VISUALIZATION
Advanced Charts
Geographic Information Systems 
Charts
3.5 
DATA DASHBOARDS
Principles of Effective Data 
Dashboards
Applications of Data Dashboards
APPENDIX: 
 CREATING A 
S CATTER CHART 
MATRIX AND 
A PARALLEL 
 COORDINATES 
PLOT WITH 
 XLMINER

 
Analytics in Action 
71
The Cincinnati Zoo & Botanical Garden, located in 
 Cincinnati, Ohio, is the one of the oldest zoos in the 
United States. To improve decision making by becom-
ing more data-driven, management decided they needed 
to link the various facets of their business and provide 
nontechnical managers and executives with an intuitive 
way to better understand their data. A complicating fac-
tor is that when the zoo is busy, managers are expected 
to be on the grounds interacting with guests, checking on 
operations, and dealing with issues as they arise or an-
ticipating them. Therefore, being able to monitor what is 
happening in real time was a key factor in deciding what 
to do. Zoo management concluded that a data visualiza-
tion strategy was needed to address the problem
Because of its ease of use, real-time updating 
 capability, and iPad compatibility, the Cincinnati Zoo 
decided to implement its data visualization strategy 
using IBM’s Cognos advanced data visualization soft-
ware. Using this software, the Cincinnati Zoo developed 
the set of charts shown in Figure 3.1 (known as a data 
dashboard) to enable management to track the following 
key measures of performance:
●   Item analysis (sales volumes and sales dollars 
by location within the zoo)
●  Geoanalytics (using maps and displays of where 
the day’s visitors are spending their time at the 
zoo)
● 
Customer spending 
● 
Cashier sales performance
●  Sales and attendance data versus weather patterns
● 
 Performance of the zoo’s loyalty rewards program
An iPad mobile application was also developed to enable 
the zoo’s managers to be out on the grounds and still see 
and anticipate what is occurring in real time. The C incinnati 
CinCinnati Zoo & BotaniCal Garden  1
ANALYTICS  in  ACTION
1 The authors are indebted to John Lucas of the Cincinnati Zoo and 
 Botanical Garden for providing this application.
file
WEB
ZooDashboardColor
FIGURE 3.1   DATA DASHBOARD FOR THE CINCINNATI ZOO
Watering Hole
LaRosa’s
Skyline Safari
UDF
Safari Grill
Safari Ice Cream
Skyline Entry
Cantina
Funnel Cake
Zoo Cafe
Zoo Cafe Ice Cream
Dippin Dots Cart 1
Dippin Dots Cart 2
Outpost
$151,367.00
($981.00)
($153,329.00)
($305,677.00)
($458,025.00)
($610,373.00)
Watering Hole
9%
LaRosa’s
12%
Skyline
Safari
2%
Safari Grill
12%
Skyline Entry
4%
Zoo Cafe
27%
Zoo Cafe
Ice Cream
8%
Food
Revenue Amount
Revenue Amount
Actual Attendance
Zoo Shop
Food YTD
YTD
Revenue Amount
Per Cap
YTD
MTD
2011 Q1
2011/09/01
Jacket Fleece Men’s
Prior YTD
166,286
440,758
47,500
16,241
27,118
86,512
11,180
25,562
96
liate
R
d
o
o
F
Adult Admission
Membership Admission
Group Sales Tickets
Group Sales - Events
Comp Admission
School Admissions
Event Admission
Education Program Admissions
Online Tickets
YTD Change
YTD Growth
YTD
Sweat Rhino/Tree Ki
Giraffe Sitting 16*
Hoody Double Play Zo
Wolf Sitting 16* Pro
Subtotal (included)
Subtotal (included)
Total (Access Code Groups)
Food and Retail
917,037
160,115
422,060
41,963
18,050
29,387
75,283
11,891
20,538
2,514
868,459
–6,171
–18,698
–5,537
1,809
2,269
–11,229
711
–5,024
2,418
–48,578
Child Admission
95,784
86,658
–9,126
–9.53%
–3.71%
–4.24%
–11.66%
11.14%
8.37%
–12.98%
6.36%
–19.65%
2,518,75%
2,482.84%
Subtotal (included)
Product Name
Product Name
Bottled Pop
Sweat Rhino Behind B
Jacket Fleece Ladies
Giraffe Sitting 16*
Giraffe Sitting 16*
Giraffe Mini Flopsie
Penguin Emp Mini Flo
Wolf Sitting 16* Pro
Sweat Rhino Behind B
Backpack Logo Fold
$36.99
$79.98
$284.81
$46.99
$224.85
$673.62
$154.83
$350.46
$11,279.85
$842.89
$99.86
$29.99
$14.99
$224.85
$12.59
$8.09
Bottom
Page down
Top
Page up
$104.93
$9.99
$1,305.73
$1,223.68
$659.56
$789.42
$524.65
$4,503.04
$507.13
$580.47
$31,520.16
$2,600.49
$107.02
$111.97
$107.27
$89.94
$149.90
$94.00
$96.69
$119.92
$119.96
$90.93
$1,342.72
$1,303.66
$944.37
$836.41
$749.50
$5,176.66
$661.96
$930.93
$42,800.01
$3,443.38
$206.88
$141.96
$107.27
$104.93
$374.75
$106.59
$104.78
$224.85
$119.96
$100.92
4D Shop
Family Shop
2011/09/04
2011 Q2
2011 Q3
5.00
Food - Non-Memb
Food - Member
Retail - Non-Mem
Retail - Member
Food and Retail
4.00
3.00
2.00
1.00
0.00
0.00
2.50
5.00
7.50
10.00
12.50
15.00
17.50
20.00
0.00
1.00
2.00
3.00
4.00 5.00 6.00
7.00
8.00
9.00
10.00

72 
Chapter 3 Data Visualization
Zoo’s iPad application, shown in Figure 3.2, provides man-
agers with access to the following information:
●  Real-time attendance data, including what types 
of guests are coming to the zoo (members,  non-
members, school groups, and so on)
●  Real-time analysis showing which locations are 
busiest and which items are selling the fastest 
inside the zoo
●  Real-time geographical representation of where 
the zoo’s visitors live
Having access to the data shown in Figures 3.1 and 3.2 
allows the zoo managers to make better decisions on 
staffing levels, which items to stock based on weather 
and other conditions, and how to better target its adver-
tising based on geodemographics.
The impact that data visualization has had on the 
zoo has been substantial. Within the first year of use, the 
system has been directly responsible for revenue growth 
of over $500,000, increased visitation to the zoo, en-
hanced customer service, and reduced marketing costs. 
The first step in trying to interpret data is often to visualize the data in some way. Data 
visualization can be as simple as creating a summary table for the data, or it could require 
generating charts to help interpret, analyze, and learn from the data. Data visualization is 
very helpful for identifying data errors and for reducing the size of your data set by high-
lighting important relationships and trends in the data.
Data visualization is also important in conveying your analysis to others. Although busi-
ness analytics is about making better decisions, in many cases, the ultimate decision maker is 
not the same person analyzing the data. Therefore, the person analyzing the data has to make 
the analysis simple for others to understand. Proper data visualization techniques greatly 
improve the ability of the decision maker to interpret the analysis easily.
In this chapter we discuss some general concepts  related to data visualization to help 
you analyze data and convey your analysis to others. We cover specifics dealing with how to 
 design tables and charts, as well as the most commonly used charts, and present an overview of 
some more advanced charts We also introduce the concept of data dashboards and geographic
FIGURE 3.2   THE CINCINNATI ZOO iPAD DATA DASHBOARD
French Fries Basket
Krazy Kritter
Souvenir Drink
Slice of Pepp
Slice of Cheese
Icee
Cheeseburger Basket
Bottled Water
Medium Drink
Ice Cream cone
$0
$200
$400
$600
Revenue Amount
Top 10 Food Items
$800
$1,000
$1,200
ZOO ADVENTURE BUCKET
YooHoo & Friends cli
Tee Junior V-neck Pe
Tee Wild Bunch Boy W
Zebra Mini Flospie
Sweat Rhino /Tree Ki
Tee Zookeeper Girl
Tee Ladies Melt Hear
Tee Giraffe family V
MONKEY SITTING 16* P
$0
$50
$100
$150
Revenue Amount
Top 10 Retail Items
Top 5 attendance Cities - Ohio
CINCINNATI
WEST CHESTER
HAMILTON
MASON
LOVELAND
705
49
40
28
26
Membership Admission
Child Admission
Education Program
Admissions
Adult Admission
Group Sales Tickets
666
16
12
11
0
$200
$250
$300
30
IN
1072
OH
243
KY
4
WV
0
PA
file
WEB
ZooiPadColor

 
3.1 Overview of Data Visualization 
73
 information systems (GISs). Our detailed examples use Excel to generate tables and charts, 
and we discuss several software packages that can be used for advanced data visualization. The 
appendix to this chapter covers the use of XLMiner (an Excel Add-in) for data visualization.
Overview of Data Visualization
Decades of research studies in psychology and other fields show that the human mind can 
process visual images such as charts much faster than it can interpret rows of numbers. 
However, these same studies also show that the human mind has certain limitations in its 
ability to interpret visual images and that some images are better at conveying information 
than others. The goal of this chapter is to introduce some of the most common forms of 
visualizing data and demonstrate when these forms are appropriate.
Microsoft Excel is a ubiquitous tool in business for basic data visualization. Software tools 
such as Excel make it easy for anyone to create many standard examples of data visualization. 
However, as discussed in this chapter, the default settings for tables and charts created with 
Excel can be altered to increase clarity. New types of software that are dedicated to data visu-
alization have appeared recently. We focus our techniques on Excel in this chapter, but we also 
mention some of these more advanced software packages for specific data visualization uses.
Effective Design Techniques
One of the most helpful ideas for creating effective tables and charts for data visualization 
is the idea of the data-ink ratio, first described by Edward R. Tufte in 2001 in his book 
the Visual display of Quantitative information. The data-ink ratio measures the proportion 
of what Tufte terms “data-ink” to the total amount of ink used in a table or chart. Data-ink 
is the ink used in a table or chart that is necessary to convey the meaning of the data to 
the audience. Non-data-ink is ink used in a table or chart that serves no useful purpose in 
conveying the data to the audience. 
Let us consider the case of Gossamer Industries, a firm that produces fine silk clothing 
products. Gossamer is interested in tracking the sales of one of its most popular items, a 
particular style of women’s scarf. Table 3.1 and Figure 3.3 provide examples of a table and 
chart with low data-ink ratios used to display sales of this style of women’s scarf. The data 
used in this table and figure represent product sales by day. Both of these examples are simi-
lar to tables and charts generated with Excel using common default settings. In Table 3.1, 
most of the grid lines serve no useful purpose. Likewise, in Figure 3.3, the horizontal lines 
in the chart also add little additional information. In both cases, most of these lines can be 
3.1
TABLE 3.1   EXAMPLE OF A LOW DATA-INK RATIO TABLE
Scarf Sales By Day
Day
Sales
Day
Sales
 1
150
11
170
 2
170
12
160
 3
140
13
290
 4
150
14
200
 5
180
15
210
 6
180
16
110
 7
210
17
 90
 8
230
18
140
 9
140
19
150
10
200
20
230

74 
Chapter 3 Data Visualization
deleted without reducing the information conveyed. However, an important piece of infor-
mation is missing from Figure 3.3: labels for axes. Axes should always be labeled in a chart 
unless both the meaning and unit of measure are obvious.
Table 3.2 shows a modified table in which all grid lines have been deleted except for 
those around the title of the table. Deleting the grid lines in Table 3.1 increases the data-ink 
ratio because a larger proportion of the ink used in the table is used to convey the informa-
tion (the actual numbers). Similarly, deleting the unnecessary horizontal lines in Figure 3.4 
increases the data-ink ratio. Note that deleting these horizontal lines and removing (or 
reducing the size of ) the markers at each data point can make it more difficult to determine 
the exact values plotted in the chart. However, as we discuss later, a simple chart is not the 
most effective way of presenting data when the audience needs to know exact values. In 
these cases, it is better to use a table. 
In many cases, white space in a table or chart can improve readability. This principle 
is similar to the idea of increasing the data-ink ratio. Consider Table 3.2 and Figure 3.4. 
Removing the unnecessary lines has increased the white space, making it easier to read both 
the table and the chart. The fundamental idea in creating effective tables and charts is to 
make them as simple as possible in conveying information to the reader.
TABLE 3.2   INCREASING THE DATA-INK RATIO BY R EMOVING 
UNNECESSARY GRIDLINES
Scarf Sales By Day
Day
Sales
Day
Sales
1
2
3
4
5
6
7
8
9
10
150
170
140
150
180
180
210
230
140
200
11
12
13
14
15
16
17
18
19
20
170
160
290
200
210
110
90
140
150
230
FIGURE 3.3  EXAMPLE OF A LOW DATA-INK RATIO CHART
1
0
50
100
150
200
250
300
350
Scarf Sales by Day
2
3
4
5
6
7
8
9
10 11 12 13 14 15
Sales
16 17 18 19 20

 
3.2 Tables 
75
Tables
The first decision in displaying data is whether a table or a chart will be more effective. In 
general, charts can often convey information faster and easier to readers, but in some cases 
a table is more appropriate. Tables should be used when:
 1. The reader needs to refer to specific numerical values.
 2. The reader needs to make precise comparisons between different values and not just 
relative comparisons.
 3. The values being displayed have different units or very different magnitudes.
Consider when the accounting department of Gossamer Industries is summarizing the 
company’s annual data for completion of its federal tax forms. In this case, the specific 
numbers corresponding to revenues and expenses are important and not just the relative 
values. Therefore, these data should be presented in a table similar to Table 3.3.
Similarly, if it is important to know exactly by how much revenues exceed expenses 
each month, then this would also be better presented as a table rather than as a line chart, 
as seen in Figure 3.5. Notice that it is very difficult to determine the monthly revenues and 
costs in Figure 3.5. We could add these values using data labels, but they would clutter the 
figure. The preferred solution is to combine the chart with the table into a single figure, as 
in Figure 3.6, to allow the reader to easily see the monthly changes in revenues and costs 
while also being able to refer to the exact numerical values
3.2
NOTES AND COMMENTS
1.  Tables have been used to display data for 
more than a thousand years. However, charts 
are much more recent inventions. The famous 
seventeenth-century French mathematician, 
René Descartes, is credited with inventing the 
now familiar graph with horizontal and verti-
cal axes. William Playfair invented bar charts, 
line charts, and pie charts in the late eighteenth 
century, all of which we will discuss in this 
c hapter. More recently, individuals such as Wil-
liam Cleveland, Edward R. Tufte, and Stephen 
Few have introduced design techniques for both 
clarity and beauty in data visualization.
2.  Many of the default settings in Excel are not ideal 
for displaying data using tables and charts that 
communicate effectively. Before presenting Excel- 
generated tables and charts to others, it is worth the 
effort to remove unnecessary lines and labels.
1
0
50
100
150
200
250
300
350
Scarf Sales by Day
Sales (Units)
Day
3
5
7
9
11
13
15
17
19
FIGURE 3.4   INCREASING THE DATA-INK RATIO BY ADDING LABELS TO AXES AND 
REMOVING UNNECESSARY LINES AND LABELS 

76 
Chapter 3 Data Visualization
TABLE 3.3   TABLE SHOWING EXACT VALUES FOR COSTS AND REVENUES BY MONTH FOR 
 GOSSAMER INDUSTRIES
Month
1
2
3
4
5
6
Total
Costs ($)
Revenues ($)
48,123
64,124
56,458
66,128
64,125
67,125
52,158
48,178
54,718
51,785
50,985
55,687
326,567
353,027
0
10,000
0
20,000
30,000
40,000
50,000
60,000
70,000
80,000
1
2
3
Month
4
5
6
Revenues ($)
Costs ($)
FIGURE 3.5   LINE CHART OF MONTHLY COSTS AND REVENUES AT GOSSAMER 
 INDUSTRIES
© Cengage Learning 2015
0
10,000
0
20,000
30,000
40,000
50,000
60,000
70,000
80,000
1
2
3
4
5
6
Revenues ($)
Costs ($)
Month
1
48,123
56,458
64,125
52,158
54,718
50,985
326,567
64,124
66,128
67,125
48,178
51,785
55,687
353,027
2
3
4
5
6
Total
Month
Revenues ($)
Costs ($)
FIGURE 3.6   COMBINED LINE CHART AND TABLE FOR MONTHLY COSTS AND 
REVENUES AT GOSSAMER INDUSTRIES

 
3.2 Tables 
77
Now suppose that you wish to display data on revenues, costs, and head count for 
each month. Costs and revenues are measured in dollars ($), but head count is measured 
in number of employees. Although all these values can be displayed on a line chart using 
multiple vertical axes, this is generally not recommended. Because the values have widely 
different magnitudes (costs and revenues are in the tens of thousands, whereas headcount is 
approximately 10 each month), it would be difficult to interpret changes on a single chart. 
Therefore, a table similar to Table 3.4 is recommended.
Table Design Principles
In designing an effective table, keep in mind the data-ink ratio and avoid the use of un-
necessary ink in tables. In general, this means that we should avoid using vertical lines 
in a table unless they are necessary for clarity. Horizontal lines are generally necessary 
only for separating column titles from data values or when indicating that a calculation 
has taken place. Consider Figure 3.7, which compares several forms of a table displaying 
Gossamer’s costs and revenue data. Most people find Design D, with the fewest grid lines, 
easiest to read. In this table, grid lines are used only to separate the column headings from 
the data and to indicate that a calculation has occurred to generate the Profits row and the 
Total column.
In large tables, vertical lines or light shading can be useful to help the reader differenti-
ate the columns and rows. Table 3.5 breaks out the revenue data by location for nine cities 
and shows 12 months of revenue and cost data. In Table 3.5, every other column has been 
TABLE 3.4   TABLE DISPLAYING HEADCOUNT, COSTS, AND REVENUES AT GOSSAMER INDUSTRIES
Month
1
2
3
4
5
6
Total
Headcount
Costs ($)
Revenues ($)
8
48,123
64,124
9
56,458
66,128
10
64,125
67,125
9
52,158
48,178
9
54,718
51,785
9
50,985
55,687
326,567
353,027
Design A:
6
Total
5
4
3
2
1
Costs ($)
48,123
56,458
64,125
52,158
54,718
50,985
326,567
Revenues ($)
64,124
66,128
67,125
48,178
51,785
55,687
353,027
Proﬁts ($)
16,001
9,670
3,000
(3,980)
(2,933)
4,702
26,460
Month
Design B:
6
Total
5
4
3
2
1
Costs ($)
Revenues ($)
Proﬁts ($)
48,123
64,124
16,001
56,458
66,128
9,670
64,125
67,125
3,000
52,158
48,178
(3,980)
54,718
51,785
(2,933)
50,985
55,687
4,702
326,567
353,027
26,460
Month
Design C:
6
Total
5
4
3
2
1
Costs ($)
Revenues ($)
Proﬁts ($)
48,123
64,124
16,001
56,458
66,128
9,670
64,125
67,125
3,000
52,158
48,178
(3,980)
54,718
51,785
(2,933)
50,985
55,687
4,702
326,567
353,027
26,460
Month
Design D:
6
Total
5
4
3
2
1
Costs ($)
Revenues ($)
Proﬁts ($)
48,123
64,124
16,001
56,458
66,128
9,670
64,125
67,125
3,000
52,158
48,178
(3,980)
54,718
51,785
(2,933)
50,985
55,687
4,702
326,567
353,027
26,460
Month
FIGURE 3.7   COMPARING DIFFERENT TABLE DESIGNS


 
3.2 Tables 
79
lightly shaded. This helps the reader quickly scan the table to see which values c orrespond 
with each month. The horizontal line between the revenue for Academy and the Total row 
helps the reader differentiate the revenue data for each location and indicates that a calcu-
lation has taken place to generate the totals by month. If one wanted to highlight the dif-
ferences among locations, the shading could be done for every other row instead of every 
other column. 
Notice also the alignment of the text and numbers in Table 3.5. Columns of numerical 
values in a table should be right-aligned; that is, the final digit of each number should be 
aligned in the column. This makes it easy to see differences in the magnitude of values. If 
you are showing digits to the right of the decimal point, all values should include the same 
number of digits to the right of the decimal. Also, use only the number of digits that are 
necessary to convey the meaning in comparing the values; there is no need to include ad-
ditional digits if they are not meaningful for comparisons. In many business applications, 
we report financial values, in which case we often round to the nearest dollar or include 
two digits to the right of the decimal if such precision is necessary. Additional digits to the 
right of the decimal are usually unnecessary. For extremely large numbers, we may prefer 
to display data rounded to the nearest thousand, ten thousand, or even million. For instance, 
if we need to include, say, $3,457,982 and $10,124,390 in a table when exact dollar values 
are not necessary, we could write these as 3,458 and 10,124 and indicate that all values in 
the table are in units of $1000. 
It is generally best to left-align text values within a column in a table, as in the Rev-
enues by Location (the first) column of Table 3.5. In some cases, you may prefer to center 
text, but you should do this only if the text values are all approximately the same length. 
Otherwise, aligning the first letter of each data entry promotes readability. Column head-
ings should either match the alignment of the data in the columns or be centered over the 
values, as in Table 3.5.
Crosstabulation
A useful type of table for describing data of two variables is a crosstabulation, which 
provides a tabular summary of data for two variables. To illustrate, consider the following 
application based on data from Zagat’s Restaurant Review. Data on the quality rating, meal 
price, and the usual wait time for a table during peak hours were collected for a sample 
of 300 Los Angeles area restaurants. Table 3.6 shows the data for the first ten restaurants. 
Quality ratings are an example of categorical data, and meal prices are an example of 
quantitative data.
We depart from these 
guidelines in some figures 
and tables in this textbook 
to more closely match 
excel’s output.
types of data such 
as categorical and 
quantitative are discussed 
in Chapter 2.
TABLE 3.6   QUALITY RATING AND MEAL PRICE FOR 300 LOS ANGELES 
 RESTAURANTS
Restaurant
Quality Rating
Meal Price ($)
Wait Time (min)
1
2
3
4
5
6
7
8
9
10
Good
Very Good
Good
Excellent
Very Good
Good
Very Good
Very Good
Very Good
Good
18
22
28
38
33
28
19
11
23
13
5
6
1
74
6
5
11
9
13
1
file
WEB
Restaurant

80 
Chapter 3 Data Visualization
For now, we will limit our consideration to the quality rating and meal price variables. 
A crosstabulation of the data for quality rating and meal price data is shown in Table 3.7. 
The left and top margin labels define the classes for the two variables. In the left margin, 
the row labels (Good, Very Good, and Excellent) correspond to the three classes of the 
quality rating variable. In the top margin, the column labels ($10–19, $20–29, $30–39, and 
$40–49) correspond to the four classes (or bins) of the meal price variable. Each restau-
rant in the sample provides a quality rating and a meal price. Thus, each restaurant in the 
sample is associated with a cell appearing in one of the rows and one of the columns of 
the crosstabulation. For example, restaurant 5 is identified as having a very good quality 
rating and a meal price of $33. This restaurant belongs to the cell in row 2 and column 3. 
In constructing a crosstabulation, we simply count the number of restaurants that belong to 
each of the cells in the crosstabulation.
Table 3.7 shows that the greatest number of restaurants in the sample (64) have a very good 
rating and a meal price in the $20–29 range. Only two restaurants have an excellent rating and 
a meal price in the $10–19 range. Similar interpretations of the other frequencies can be made. 
In addition, note that the right and bottom margins of the crosstabulation give the frequency 
of quality rating and meal price separately. From the right margin, we see that data on qual-
ity ratings show 84 good restaurants, 150 very good restaurants, and 66 excellent restaurants. 
Similarly, the bottom margin shows the counts for the meal price variable. The value of 300 in 
the bottom right corner of the table indicates that 300 restaurants were included in this data set.
PivotTables in Excel
A crosstabulation in Microsoft Excel is known as a PivotTable. We will first look at a 
simple example of how Excel’s PivotTable is used to create a crosstabulation of the Zagat’s 
restaurant data shown previously. Figure 3.8 illustrates a portion of the data contained in 
the file restaurants; the data for the 300 restaurants in the sample have been entered into 
cells B2:D301. 
To create a PivotTable in Excel, we follow these steps:
Step 1.  Click the INSERT tab on the Ribbon
Step 2.  Click PivotTable in the Tables group
Step 3.  When the Create PivotTable dialog box appears:
 
 
Choose Select a Table or Range
 
 
Enter a1:d301 in the Table/Range: box
 
 
Select New Worksheet as the location for the PivotTable Report
 
 
Click OK
The resulting initial PivotTable Field List and PivotTable Report are shown in Figure 3.9.
file
WEB
Restaurant
TABLE 3.7   CROSSTABULATION OF QUALITY RATING AND MEAL PRICE FOR 
300 LOS ANGELES RESTAURANTS
Meal Price
Quality Rating
$10–19
$20–29
$30–39
$40–49
Total
Good
Very Good
Excellent
42
34
2
40
64
14
2
46
28
0
6
22
84
150
66
 Total
78
118
76
28
300

 
3.2 Tables 
81
FIGURE 3.8   EXCEL WORKSHEET CONTAINING RESTAURANT DATA
Wait Time (min)
Meal Price ($)
Quality Rating
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
5
6
1
74
6
5
11
9
13
1
18
7
18
46
0
3
3
36
7
3
10
14
27
80
9
Good
Very Good
Good
Excellent
Very Good
Good
Very Good
Very Good
Very Good
Good
Very Good
Very Good
Excellent
Excellent
Good
Good
Good
Excellent
Very Good
Good
Very Good
Very Good
Excellent
Excellent
Very Good
18
22
28
38
33
28
19
11
23
13
33
44
42
34
25
22
26
17
30
19
33
22
32
33
34
Restaurant
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
A
B
C
D
file
WEB
Restaurant
FIGURE 3.9   INITIAL PIVOTTABLE FIELD LIST AND PIVOTTABLE FIELD  REPORT FOR 
THE RESTAURANT DATA
A
B
C
D
E
F
G
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
PivotTable1
To build a report, choose ﬁelds
from the PivotTable Field List

82 
Chapter 3 Data Visualization
Each of the four columns in Figure 3.8 [Restaurant, Quality Rating, Meal Price ($), and 
Wait Time (min)] is considered a field by Excel. Fields may be chosen to represent rows, 
columns, or values in the body of the PivotTable Report. The following steps show how to 
use Excel’s PivotTable Field List to assign the Quality Rating field to the rows, the Meal 
Price ($) field to the columns, and the Restaurant field to the body of the PivotTable report.
Step 4.   In the PivotTable Fields area, go to Drag fields between areas below:
 
 
 Drag the Quality Rating field to the ROWS area
 
 
 Drag the Meal Price ($) field to the COLUMNS area
 
 
 Drag the Restaurant field to the VALUES area
Step 5.   Click on Sum of Restaurant in the VALUES area
Step 6.   Select Value Field Settings from the list of options
Step 7.   When the Value Field Settings dialog box appears:
 
 
 Under Summarize value field by, select Count
 
 
 Click OK
Figure 3.10 shows the completed PivotTable Field List and a portion of the PivotTable 
worksheet as it now appears.
To complete the PivotTable, we need to group the columns representing meal prices 
and place the row labels for quality rating in the proper order:
Step 8.   Right-click in cell B4 or any cell containing a meal price column label
Step 9.   Select Group from the list of options
Step 10.  When the Grouping dialog box appears:
 
 
 Enter 10 in the Starting at: box
 
 
 Enter 49 in the Ending at: box
in excel versions prior to 
excel 2013, Drag fields 
between areas below: is 
referred to as Choose 
Fields to add to report:. 
additionally, Rows and 
Columns are labeled as 
Row Labels and Column 
Labels, respectively.
FIGURE 3.10   COMPLETED PIVOTTABLE FIELD LIST AND A PORTION OF THE PIVOTTABLE REPORT 
FOR THE RESTAURANT DATA (COLUMNS H:AK ARE HIDDEN)
A
B
C D
E
F
G AL AM
AN
AO AP AQ AR
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
Count of Restaurant
Row Labels
10
6
1
7
4
4
8
3
3
6
3
1
2 2
66
5
9
2
6
8
4
1
5
84
150
300
2
1
3
111213 14 1547 48 Grand Total
Columns Labels
Very Good
Good
Excellent
Grand Total

 
3.2 Tables 
83
 
 
 Enter 10 in the By: box
 
 
 Click OK
Step 11.  Right-click on “Excellent” in cell A5
Step 12.  Select Move and click Move “Excellent” to End
The final PivotTable, shown in Figure 3.11,  provides the same information as the cross-
tabulation in Table 3.7. 
The values in Figure 3.11 can be interpreted as the frequencies of the data. For instance, 
row 8 provides the frequency distribution for the data over the quantitative variable of meal 
price. Seventy-eight restaurants have meal prices of $10 to $19. Column F provides the 
frequency distribution for the data over the categorical variable of quality. One hundred 
fifty restaurants have a quality rating of Very Good. We can also use a PivotTable to create 
percent frequency distributions, as shown in the following steps:
Step 1.  Click the Count of Restaurant in the VALUES area
Step 2.  Select Value Field Settings… from the list of options
Step 3.  When the Value Field Settings dialog box appears, click the tab for Show 
Values As
Step 4.  In the Show values as area, select % of Grand Total from the drop-down menu
 
 Click OK
Figure 3.12 displays the percent frequency distribution for the Restaurant data as a Pivot-
Table. The figure indicates that 50 percent of the restaurants are in the Very Good quality 
category and that 26 percent have meal prices between $10 and $19.
PivotTables in Excel are interactive, and they may be used to display statistics other 
than a simple count of items. As an illustration, we can easily modify the PivotTable in 
Figure 3.11 to display summary information on wait times instead of meal prices. 
FIGURE 3.11  FINAL PIVOTTABLE REPORT FOR THE RESTAURANT DATA
A
B
C
D
E
F
G H I
3
2
1
4
5
Good
6
Very Good
7
Excellent
8
9
10
11
12
13
21
15
16
16
17
18
19
20
Count of Restaurant Column Labels
Grand Total
Row Labels
10–19
20–29
30–39 40–49
Grand Total
42
34
2
78
40
64
14
118
2
46
28
76
84
150
66
300
6
22
28

84 
Chapter 3 Data Visualization
Step 1.  Click the Count of Restaurant field in the VALUES area
 
 
 Select Remove Field
Step 2.  Drag the Wait Time (min) to the VALUES area
Step 3.  Click on Sum of Wait Time (min) in the VALUES area
Step 4.  Select Value Field Settings… from the list of options
Step 5.  When the Value Field Settings dialog box appears:
 
 
Under Summarize value field by, select Average
 
 
Click Number Format
 
 
In the Category: area, select Number
 
 
Enter 1 for Decimal places:
 
 
Click OK
 
When the Value Field Settings dialog box reappears, click OK
The completed PivotTable appears in Figure 3.13. This PivotTable replaces the counts of 
restaurants with values for the average wait time for a table at a restaurant for each group-
ing of meal prices ($10–19, $20–29, $30–39, $40–49). For instance, cell B7 indicates that 
the average wait time for a table at an Excellent restaurant with a meal price of $10–$19 is 
25.5 minutes. Column F displays the total average wait times for tables in each quality rat-
ing category. We see that Excellent restaurants have the longest average waits of 35.2 min-
utes and that Good restaurants have average wait times of only 2.5 minutes. Finally, cell 
D7 shows us that the longest wait times can be expected at Excellent restaurants with meal 
prices in the $30–$39 range (34 minutes).
We can also examine only a portion of the data in a PivotTable using the Filter option in 
Excel. To Filter data in a PivotTable, click on the Filter Arrow 
 next to Row  Labels or 
Column Labels and then uncheck the values that you want to remove from the  PivotTable. 
For example, we could click on the arrow next to Row Labels and then uncheck the Good 
value to examine only Very Good and Excellent restaurants.
You can also filter data in 
a Pivottable by dragging 
the field that you want to 
filter the data on to the 
FILTERS area in the 
PivotTable Fields.
FIGURE 3.12   PERCENT FREQUENCY DISTRIBUTION AS A PIVOTTABLE FOR 
THE RESTAURANT DATA
A
B
C
D
E
F
G
3
2
1
4
5
Good
6
Very Good
7
Excellent
8
9
10
11
12
13
15
16
16
17
18
Count of Restaurant Column
Grand Total
Row Labels
Labels 10–19
20–29
30–39
40–49
Grand Total
14.00%
11.33%
0.67%
26.00%
13.33%
21.33%
4.67%
39.33%
0.67%
15.33%
9.33%
25.33%
0.00%
2.00%
7.33%
9.33%
28.00%
50.00%
22.00%
100.00%

 
3.3 Charts 
85
Charts
Charts (or graphs) are visual methods of displaying data. In this section, we introduce some 
of the most commonly used charts to display and analyze data including scatter charts, line 
charts, and bar charts. Excel is the most commonly used software package for creating 
simple charts. We explain how to use Excel to create scatter charts, line charts, sparklines, 
bar charts, bubble charts, and heat maps. In the chapter appendix, we demonstrate the use 
of the Excel Add-in XLMiner to create a scatter chart matrix and a parallel coordinates plot.
Scatter Charts
A scatter chart (introduced in Chapter 2) is a graphical presentation of the relationship 
between two quantitative variables. As an illustration, consider the advertising/sales rela-
tionship for an electronics store in San Francisco. On ten occasions during the past three 
months, the store used weekend television commercials to promote sales at its stores. The 
managers want to investigate whether a relationship exists between the number of com-
mercials shown and sales at the store the following week. Sample data for the ten weeks, 
with sales in hundreds of dollars, are shown in Table 3.8. 
We will use the data from Table 3.8 to create a scatter chart using Excel’s chart tools 
and the data in the file electronics:
Step 1.   Select cells B2:C11
Step 2.   Click the INSERT tab in the Ribbon
Step 3.   Click the Insert Scatter (X,Y) or Bubble Chart button 
 in the 
Charts group
Step 4.   When the list of scatter chart subtypes appears, click the Scatter button 
 
Step 5.   Click the DESIGN tab under the Chart Tools Ribbon 
3.3
the steps for modifying and 
formatting charts have been 
changed in excel 2013. 
the steps shown here apply 
to excel 2013. in excel 
versions prior to excel 
2013, most chart formatting 
options can be found in the 
Layout tab in the Chart 
Tools ribbon. in previous 
versions of excel, this is 
where you will find options 
for adding a Chart Title, 
Axis Titles, Data Labels, 
and so on.
file
WEB
Electronics
Hovering the pointer over 
the chart type buttons in 
excel 2013 will display the 
names of the buttons and 
short descriptions of the 
types of chart.
FIGURE 3.13   PIVOTTABLE REPORT FOR THE RESTAURANT DATA WITH AVERAGE  
WAIT TIMES ADDED
A
B
C
D
E
F
G
3
2
1
4
5
Good
6
Very Good
7
Excellent
8
9
10
11
12
13
15
16
16
17
Average of Wait Time (min) Column
Grand Total
Row Labels
Labels 10–19 20–29 30–39 40–49
Grand Total
2.6
12.6
25.5
7.6
2.5
12.6
29.1
11.1
0.5
12.0
34.0
19.8
10.0
32.3
27.5
2.5
12.3
32.1
13.9

86 
Chapter 3 Data Visualization
Step 6.   Click Add Chart Element in the Chart Layouts group
 
 
 Select Chart Title, and click Above Chart
 
 
  Click on the text box above the chart, and replace the text with Scatter  
 Chart for the San Francisco electronics Store
Step 7.   Click Add Chart Element in the Chart Layouts group
 
 
 Select Axis Title, and click Primary Vertical
 
 
  Click on the text box under the horizontal axis, and replace “Axis  
 Title” with number of Commercials
Step 8.   Click Add Chart Element in the Chart Layouts group
 
 
 Select Axis Title, and click Primary Horizontal
 
 
  Click on the text box next to the vertical axis, and replace “Axis Title”  
 with Sales ($100s)
Step 9.   Right-click on the one of the horizontal grid lines in the body of the chart, 
and click Delete
Step 10.  Right-click on the one of the vertical grid lines in the body of the chart, and 
click Delete
We can also use Excel to add a trendline to the scatter chart. A trendline is a line that 
provides an approximation of the relationship between the variables. To add a linear trend-
line using Excel, we use the following steps:
Step 1.  Right-click on one of the data points in the scatter chart, and select Add 
Trendline… 
Step 2.  When the Format Trendline task pane appears, select Linear under 
 TRENDLINE OPTIONS
Figure 3.14 shows the scatter chart and linear trendline created with Excel for the data 
in Table 3.8. The number of commercials (x) is shown on the horizontal axis, and sales (y) 
are shown on the vertical axis. For week 1, x 5 2 and y 5 50. A point is plotted on the 
scatter chart at those coordinates; similar points are plotted for the other nine weeks. Note 
that during two of the weeks, one commercial was shown, during two of the weeks, two 
commercials were shown, and so on.
The completed scatter chart in Figure 3.14 indicates a positive linear relationship (or 
positive correlation) between the number of commercials and sales: higher sales are associ-
ated with a higher number of commercials. The linear relationship is not perfect b ecause 
not all of the points are on a straight line. However, the general pattern of the points and 
the trendline suggest that the overall relationship is positive. From Chapter 2, we know 
Steps 9 and 10 are optional, 
but they improve the chart’s 
readability. We would want 
to retain the gridlines only 
if they were important for 
the reader to determine 
more precisely where data 
points are located relative 
to certain values on the 
horizontal and/or vertical 
axes. 
in excel 2013, Step 1 
should open the Format 
Trendline task pane. in 
previous versions of excel, 
Step 1 will open the Format 
Trendline dialog box where 
you can select Linear under 
Trend/ Regression Type.
Scatter charts are often 
referred to as scatter plots 
or scatter diagrams.
file
WEB
Electronics
TABLE 3.8   SAMPLE DATA FOR THE SAN FRANCISCO ELECTRONICS STORE
Week
Number of Commercials 
x
Sales ($100s) 
y
1
2
3
4
5
6
7
8
9
10
2
5
1
3
4
1
5
3
4
2
50
57
41
54
54
38
63
48
59
46

 
3.3 Charts 
87
that this implies that the covariance between sales and commercials is positive and that the 
correlation coefficient between these two variables is between 0 and 11.
Line Charts
Line charts are similar to scatter charts, but a line connects the points in the chart. Line 
charts are very useful for time series data collected over a period of time (minutes, hours, 
days, years, etc.). As an example, Kirkland Industries sells air compressors to  manufacturing 
companies. Table 3.9 contains total sales amounts (in $100s) for air compressors during 
each month in the most recent calendar year. Figure 3.15 displays a scatter chart and a line 
chart created in Excel for these sales data. The line chart connects the points of the scatter 
chart. The addition of lines between the points suggests continuity, and it is easier for the 
reader to interpret changes over time. 
a line chart for time series 
data is often called a time 
series plot.
FIGURE 3.14   SCATTER CHART FOR THE SAN FRANCISCO ELECTRONICS STORE
A
B
C
D
Week
1
1
2
3
4
5
6
7
8
9
10
3
4
5
6
7
8
9
10
No. of Commercials
2
5
1
3
4
5
3
4
2
Sales Volume
50
57
41
54
54
38
63
48
59
46
11
12
13
14
15
16
17
18
19
Scatter Chart for the San Francisco
Electronics Store
50
70
30
10
60
40
20
0 0
1
2
3
4
5
6
Number of Commercials
Sales ($100s)
2
1
E
F
G
H
I
J
K
L
TABLE 3.9   MONTHLY SALES DATA OF AIR COMPRESSORS  
AT KIRKLAND INDUSTRIES
Month
Sales ($100s)
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
150
145
185
195
170
125
210
175
160
120
115
120
file
WEB
Kirkland

88 
Chapter 3 Data Visualization
To create the line chart in Figure 3.15 in Excel, we follow these steps:
Step 1.  Select cells A2:B13
Step 2.  Click the INSERT tab on the Ribbon
Step 3.  Click the Insert Line Chart button 
 in the Charts group
Step 4.  When the list of line chart subtypes appears, click the Line with Markers 
button 
 under 2-D Line
 
 
 This creates a line chart for sales with a basic layout and minimum  
  formatting
Step 5.  Select the line chart that was just created to reveal the CHART TOOLS 
 Ribbon 
 
 
Click the DESIGN tab under the CHART TOOLS Ribbon 
Step 6.  Click Add Chart Element in the Chart Layouts group
 
 
Select Axis Title from the drop-down menu 
 
 
Click Primary Vertical
 
 
 Click on the text box next to the vertical axis, and replace “Axis Title”  
 with Sales ($100s)
Step 7.  Click Add Chart Element in the Chart Layouts group
 
 
Select Chart Title from the drop-down menu
 
 
Click Above Chart
 
 
 Click on the text box above the chart, and replace “Chart Title” with  
 line Chart for Monthly Sales data
Step 8.  Right-click on one of the horizontal lines in the chart, and click Delete
Line charts can also be used to graph multiple lines. Suppose we want to break out 
Kirkland’s sales data by region (North and South), as shown in Table 3.10. We can create a 
line chart in Excel that shows sales in both regions, as in Figure 3.16, by following similar 
steps but selecting cells A2:C14 in the file Kirklandregional before creating the line chart. 
Figure 3.16 shows an interesting pattern. Sales in both the North and South regions seemed 
to follow the same increasing/decreasing pattern until October. Starting in October, sales 
in the North continued to decrease while sales in the South increased. We would probably 
want to investigate any changes that occurred in the North region around October.
file
WEB
Kirkland
in versions of excel prior 
to excel 2013, you can 
add titles to axes and other 
chart labels by clicking on 
the Layout tab in the Chart 
Tools ribbon.
excel assumes that line 
charts will be used to graph 
only time series data. the 
line Chart tool in excel is 
the most intuitive for creat-
ing charts that include text 
entries for the horizontal 
axis (for example, the 
month labels of Jan, Feb, 
Mar, etc. for the monthly 
sales data in Figures 3.14 
and 3.15). When the 
horizontal axis represents 
numerical values (1, 2, 
3, etc.), then it is easiest 
to go to the Charts group 
under the INSERT tab in 
the ribbon, click the Insert 
Scatter (X,Y) or Bubble 
Chart button 
, and 
then choose the Scatter 
with Straight Lines and 
Markers button 
  .
FIGURE 3.15   SCATTER CHART AND LINE CHART FOR MONTHLY SALES DATA AT 
KIRKLAND INDUSTRIES
in the line chart 
in Figure 3.15, 
we have kept the 
markers at each 
data point. this 
is a matter of 
personal taste, 
but removing the 
markers tends to 
suggest that the 
data are continu-
ous when in fact 
we have only one 
data point per 
month.
Scatter Chart for Monthly Sales Data
0
200
150
100
50
Jan
Mar
Apr
Feb
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
250
Sales ($100s)
0
Line Chart for Monthly Sales Data
200
150
100
50
250
Sales ($100s)
Jan
Mar
Apr
Feb
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec

 
3.3 Charts 
89
A special type of line chart is a sparkline, which is a minimalist type of line chart that 
can be placed directly into a cell in Excel. Sparklines contain no axes; they display only the 
line for the data. Sparklines take up very little space, and they can be effectively used to 
provide information on overall trends for time series data. Figure 3.17 illustrates the use of 
sparklines in Excel for the regional sales data. To create a sparkline in Excel:
Step 1.  Click the INSERT tab on the Ribbon
Step 2.  Click Line in the Sparklines group
Step 3.  When the Create Sparklines dialog box opens,
 
 
Enter B3:B14 in the Data Range: box
 
 
Enter B15 in the Location Range: box
 
 
Click OK
Step 4.  Copy cell B15 to cell C15
file
WEB
KirklandRegional
TABLE 3.10   REGIONAL SALES DATA BY MONTH FOR AIR COMPRESSORS 
AT KIRKLAND INDUSTRIES
Sales ($100s)
Month
North
South
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
95
100
120
115
100
85
135
110
100
50
40
40
40
45
55
65
60
50
75
65
60
70
75
80
file
WEB
KirklandRegional
FIGURE 3.16   LINE CHART OF REGIONAL SALES DATA AT KIRKLAND 
INDUSTRIES
in the line chart in Fig-
ure 3.16, we have replaced 
excel’s default legend with 
text boxes labeling the lines 
corresponding to sales in 
the north and South. this 
can often make the chart 
look cleaner and easier to 
interpret.
0
20
40
60
80
100
120
140
Line Chart of Regional Sales Data
Jan
Feb Mar Apr May Jun
Jul
Aug Sep
Oct Nov Dec
South
North
160
Sales ($100s)

90 
Chapter 3 Data Visualization
The sparklines in cells B15 and C15 do not indicate the magnitude of sales in the North 
and South regions, but they do show the overall trend for these data. Sales in the North ap-
pear to be decreasing in recent time, and sales in the South appear to be increasing overall. 
Because sparklines are input directly into the cell in Excel, we can also type text directly 
into the same cell that will then be overlaid on the sparkline, or we can add shading to the 
cell, which will appear as the background. In Figure 3.17, we have shaded cells B15 and 
C15 to highlight the sparklines. As can be seen, sparklines provide an efficient and simple 
way to display basic information about a time series.
Bar Charts and Column Charts
Bar charts and column charts provide a graphical summary of categorical data. Bar charts 
use horizontal bars to display the magnitude of the quantitative variable. Column charts 
use vertical bars to display the magnitude of the quantitative variable. Bar and column 
charts are very helpful in making comparisons between categorical variables. Consider the 
regional supervisor who wants to examine the number of accounts being handled by each 
manager. Figure 3.18 shows a bar chart created in Excel displaying these data. To create 
this bar chart in Excel:
Step 1.  Select cells A2:B9
Step 2.  Click the INSERT tab on the Ribbon
Step 3.  Click the Insert Bar Chart button 
 in the Charts group
Step 4.  When the list of bar chart subtypes appears:
 
 
Click the Clustered Bar button 
 in the 2-D Bar section
Step 5.  Select the bar chart that was just created to reveal the CHART TOOLS ribbon 
 
 
Click the DESIGN tab under the CHART TOOLS Ribbon 
Step 6.  Click Add Chart Element in the Chart Layouts group
 
 
Select Axis Title from the drop-down menu
 
 
Click Primary Horizontal
 
 
 Click on the text box next to the vertical axis, and replace “Axis Title”  
 with accounts Managed
excel differentiates between 
bar and column charts 
based on whether the bars 
are horizontal or vertical, 
so we maintain these defini-
tions. However, in common 
usage, both of these types of 
charts may be referred to as 
bar charts.
file
WEB
AccountsManaged
FIGURE 3.17   SPARKLINES FOR THE REGIONAL SALES DATA AT KIRKLAND INDUSTRIES
A
B
C
D
E
F
G
H
I
Month
1
2
3
4
5
6
7
8
9
10
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
100
50
40
40
North
95
100
120
115
100
85
135
110
60
70
75
80
South
40
45
55
65
60
50
75
65
Sales ($100s)
11
12
13
14
15
Sales ($100s)

 
3.3 Charts 
91
Step 7.  Click Add Chart Element in the Chart Layouts group
 
 
Select Axis Title from the drop-down menu
 
 
Click Primary Vertical
 
 
 Click on the text box next to the vertical axis, and replace “Axis Title”  
 with Manager
Step 8.  Click Add Chart Element in the Chart Layouts group
 
 
Select Chart Title from the drop-down menu
 
 
Click Above Chart
 
 
 Click on the text box above the chart, and replace “Chart Title” with  
 Bar Chart of accounts Managed
Step 9.  Right-click on one of the vertical lines in the chart, and click Delete
From Figure 3.18 we can see that Gentry manages the greatest number of accounts and 
Williams the fewest. We can make this bar chart even easier to read by ordering the results 
by the number of accounts managed. We can do this with the following steps:
Step 1.  Select cells A1:B9
Step 2.  Right-click any of the cells A1:B9
 
 
Choose Sort
 
 
Click Custom Sort
Step 3.  When the Sort dialog box appears:
 
 
Make sure that the check box for My data has headers is checked
 
 
Choose Accounts Managed in the Sort by box under Column
 
 
Choose Smallest to Largest under Order
 
Click OK
In the completed bar chart in Excel, shown in Figure 3.19, we can easily compare 
the relative number of accounts managed for all managers. However, note that it is dif-
ficult to interpret from the bar chart exactly how many accounts are assigned to each 
FIGURE 3.18   BAR CHARTS FOR ACCOUNTS MANAGED DATA
A
B
C
D
E
F
G
H
I
J
Manager
Davis
Edwards
Francois
Gentry
Jones
Lopez
Smith
Williams
Accounts
Managed
1
2
24
11
28
37
15
29
21
6
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
0
Williams
Smith
Lopez
Jones
Gentry
Francois
Edwards
Davis
10
Accounts Managed
Bar Chart of Accounts Managed
Manager
20
30
40
file
WEB
AccountsManaged

92 
Chapter 3 Data Visualization
manager. If this information is necessary, these data are better presented as a table or 
by adding data labels to the bar chart, as in Figure 3.20, which is created in Excel using 
the following steps:
Step 1.  Select the bar chart just created to reveal the CHART TOOLS Ribbon
Step 2.  Click DESIGN tab in the CHART TOOLS Ribbon
FIGURE 3.19   SORTED BAR CHART FOR ACCOUNTS MANAGED DATA 
Manager
Williams
Edwards
Jones
Smith
Davis
Francois
Lopez
Gentry
6
11
15
21
24
28
29
37
A
B
C
D
E
F
G
H
I
J
Accounts
Managed
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
0
Lopez
Francois
Davis
Smith
Jones
Edwards
Williams
10
Accounts Managed
Bar Chart of Accounts Managed
Manager
20
30
40
Gentry
FIGURE 3.20   BAR CHART WITH DATA LABELS FOR ACCOUNTS MANAGED DATA
Manager
Williams
Edwards
Jones
Smith
Davis
Francois
Lopez
Gentry
6
11
15
21
24
28
29
37
A
B
C
D
E
F
G
H
I
J
Accounts
Managed
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
0
Lopez
Francois
Davis
Smith
Jones
Edwards
Williams
10
Accounts Managed
Bar Chart of Accounts Managed
Manager
20
30
40
Gentry
6
11
15
21
24
28
29
37

 
3.3 Charts 
93
Step 3.  Click Add Chart Element in the Chart Layouts group
 
 
Select Data Labels
 
Click Outside End
This adds labels of the number of accounts managed to the end of each bar so that the reader 
can easily look up exact values displayed in the bar chart.
A Note on Pie Charts and 3-D Charts
Pie charts are another common form of chart used to compare categorical data. However, 
many experts argue that pie charts are inferior to bar charts for comparing data. The pie 
chart in Figure 3.21 displays the data for the number of accounts managed in Figure 3.18. 
Visually, it is still relatively easy to see that Gentry has the greatest number of accounts and 
that Williams has the fewest. However, it is difficult to say whether Lopez or Francois has 
more accounts. Research has shown that people find it very difficult to perceive differences 
in area. Compare Figure 3.21 to Figure 3.19. Making visual comparisons is much easier in 
the bar chart than in the pie chart (particularly when using a limited number of colors for 
differentation). Therefore, we recommend against using pie charts in most situations and 
suggest bar charts for comparing categorical data instead.
Because of the difficulty in visually comparing area, many experts also recommend 
against the use of three-dimensional (3-D) charts in most settings. Excel makes it very 
easy to create 3-D bar, line, pie, and other types of charts. In most cases, however, the 3-D 
 effect simply adds unnecessary detail that does not help explain the data. As an alternative, 
consider the use of multiple lines on a line chart (instead of adding a z-axis), employing 
multiple charts, or creating bubble charts where the size of the bubble can represent the  
z-axis value. Never use a 3-D chart when a two-dimensional chart will suffice.
Bubble Charts
A bubble chart is a graphical means of visualizing three variables in a two-dimensional 
graph and is therefore sometimes a preferred alternative to a 3-D graph. Suppose that we 
want to compare the number of billionaires in various countries. Table 3.11 provides a 
sample of six countries, showing, for each country, the number of billionaires per 10 mil-
lion residents, the per capita income, and the total number of billionaires. We can create a 
bubble chart using Excel to further examine these data:
Step 1.  Select cells B2:D7
Step 2.  Click the INSERT tab on the Ribbon
alternatively, you can add 
data labels by right-
clicking on a bar in the 
chart and selecting Add 
Data Label.
file
WEB
Billionaires
FIGURE 3.21   PIE CHART OF ACCOUNTS MANAGED
Davis
Edwards
Francois
Gentry
Jones
Lopez
Smith
Williams

94 
Chapter 3 Data Visualization
Step 3.  In the Charts group, click Insert Scatter (X,Y) or Bubble Chart button 
 
 
In the Bubble subgroup, click the Bubble button 
Step 4.  Select the chart that was just created to reveal the CHART TOOLS  
ribbon
 
 
Click the DESIGN tab under the CHART TOOLS Ribbon
Step 5.  Click Add Chart Element in the Chart Layouts group
 
 
Choose Axis Title from the drop-down menu
 
 
Click Primary Horizontal
 
 
 Click on the text box under the horizontal axis, and replace “Axis Title”  
 with Billionaires per 10 Million residents
Step 6.  Click Add Chart Element in the Chart Layouts group
 
 
Choose Axis Title from the drop-down menu
 
 
Click Primary Vertical
 
 
 Click on the text box next to the vertical axis, and replace “Axis Title”  
 with Per Capita income
Step 7.  Click Add Chart Element in the Chart Layouts group
 
 
Choose Chart Title from the drop-down menu
 
 
Click Above Chart
 
 
 Click on the text box above the chart, and replace “Chart Title” with  
 Billionaires by Country
Step 8.  Click Add Chart Element in the Chart Layouts group
 
 
Choose Gridlines from the drop-down menu
 
 Deselect Primary Major Horizontal and Primary Major Vertical  
 to remove the gridlines from the bubble chart
Figure 3.22 shows the bubble chart that has been created by Excel. However, we can make 
this chart much more informative by taking a few additional steps to add the names of the 
countries: 
Step 9.   Click Add Chart Element in the Chart Layouts group
 
 
Choose Data Labels from the drop-down menu
 
 
Click More Data Label Options . . .
Step 10.  Click the Label Options icon 
 
 
 
 Under LABEL OPTIONS, select Value from Cells, and click the  
 S elect Range button
Step 11.  When the Data Label Range dialog box opens, select cells A2:A7 in the 
Worksheet. This will enter the value “5SheetName!$A$2:$A$7” into 
in excel 2013, Step 9 opens 
a task pane for Format 
Data Labels on the right-
hand side of the excel 
window.
TABLE 3.11   SAMPLE DATA ON BILLIONAIRES PER COUNTRY
Country
Billionaires per 
10M Residents
Per Capita  
Income
Number of  
Billionaires
United States
China
Russia
Mexico
Hong Kong
United Kingdom
13.2
0.9
7.1
0.4
51.0
5.3
48,300
 8,300
16,800
14,600
49,300
36,000
412
115
101
 40
 36
 33
file
WEB
Billionaires

 
3.3 Charts 
95
the Select Data Label Range box where “SheetName” is the name of 
the  active Worksheet
 
 
Click OK
Step 12.  In the Format Data Labels task pane, deselect Y Value in the LABEL 
OPTIONS area, and select Right under Label Position
The completed bubble chart in Figure 3.23 enables us to easily associate each country 
with the corresponding bubble. The figure indicates that Hong Kong has the most billion-
aires per 10 million residents but that the United States has many more billionaires overall 
(Hong Kong has a much smaller population than the United States). From the relative 
bubble sizes, we see that China and Russia also have many billionaires but that there are 
relatively few billionaires per 10 million residents in these countries and that these countries 
overall have low per capita incomes. The United Kingdom, United States, and Hong Kong 
all have much higher per capita incomes. Bubble charts can be very effective for comparing 
categorical variables on two different quantitative values.
Heat Maps
A heat map is a two-dimensional graphical representation of data that uses different shades 
of color to indicate magnitude. Figure 3.24 shows a heat map indicating the magnitude of 
changes for a metric called same-store sales, which are commonly used in the retail industry 
Steps 9–12 add the name 
of each country to the right 
of the appropriate bubble 
in the bubble chart. the 
process of adding text data 
labels to bubble charts is 
greatly improved in excel 
2013. in prior versions 
of excel, the task is quite 
time-consuming, so we do 
not describe the specific 
steps here.
FIGURE 3.22   BUBBLE CHART COMPARING BILLIONAIRES BY COUNTRY
A
B
C
D
F
E
Country
United Kingdom
Billionaires per 10M
Residents
Per Capita
 Income
Number of
Billionaires
United States
13.2
412
115
101
40
36
33
48300
8300
16800
14600
49300
36000
0.9
7.1
0.4
51
5.3
China
Russia
Mexico
Hong Kong
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
–10
0
10
60,000
50,000
40,000
30,000
20,000
10,000
Billionaires per 10 million Residents
Per Capita Income
Billionaires by Country
20
30
40
50
60

96 
Chapter 3 Data Visualization
to measure trends in sales. The cells shaded grey in Figure 3.24 (which are shaded red in the 
full color version) indicate declining same-store sales for the month, and cells shaded blue 
indicate increasing same-store sales for the month. Column N in Figure 3.24 also contains 
sparklines for the same-store sales data.
Figure 3.24 can be created in Excel by following these steps:
Step 1.  Select cells B2:M17
Step 2.  Click the HOME tab on the Ribbon
Step 3.  Click Conditional Formatting in the Styles group
 
Choose Color Scales and click on Blue–White–Red Color Scale
To add the sparklines in Column N, we use the following steps:
Step 4.  Select cell N2
Step 5.  Click the INSERT tab on the Ribbon
Step 6.  Click Line in the Sparklines group
Step 7.  When the Create Sparklines dialog box opens:
 
 
Enter B2:M2 in the Data Range: box
 
 
Enter n2 in the Location Range: box and click OK
Step 8.  Copy cell N2 to N3:N17
The heat map in Figure 3.24 helps the reader to easily identify trends and patterns. We 
can see that Austin has had positive increases throughout the year while Pittsburgh has
file
WEB
SameStoreSales
Both the heat map and the 
sparklines described here 
can also be created using 
the Quick Analysis button 
 in excel 2013. to 
display this button, select 
cells B2:M17. the Quick 
Analysis button will appear 
at the bottom right of the 
selected cells. Click the but-
ton to display options for 
heat maps, sparklines, and 
other data analysis tools
FIGURE 3.23   BUBBLE CHART COMPARING BILLIONAIRES BY COUNTRY WITH DATA 
LABELS ADDED
A
B
C
D
F
E
Country
United Kingdom
Billionaires per 10M
Residents
Per Capita
 Income
Number of
Billionaires
United States
13.2
412
115
101
40
36
33
48300
8300
16800
14600
49300
36000
0.9
7.1
0.4
51
5.3
China
Russia
Mexico
Hong Kong
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
–10
0
10
60,000
50,000
40,000
30,000
20,000
10,000
Billionaires per 10 million Residents
Per Capita Income
20
United States
Hong Kong
United Kingdom
Russia
China
30
40
50
60
Mexico
Billionaires by Country

 
3.3 Charts 
97
had consistently negative same-store sales results. Same-store sales at Cincinnati started 
the year negative but then became increasingly positive after May. In addition, we can 
differentiate between strong positive increases in Austin and less substantial positive in-
creases in Chicago by means of color shadings. A sales manager could use the heat map in 
Figure 3.24 to identify stores that may require intervention and stores that may be used as 
models. Heat maps can be used effectively to convey data over different areas, across time, 
or both, as seen here. 
Because heat maps depend strongly on the use of color to convey information, one 
must be careful to make sure that the colors can be easily differentiated and that they 
do not become overwhelming. To avoid problems with interpreting differences in color, 
we can add the sparklines in Column N of Figure 3.24. The sparklines clearly show the 
overall trend (increasing or decreasing) for each location. However, we cannot gauge dif-
ferences in the magnitudes of increases and decreases among locations using sparklines. 
The combination of a heat map and sparklines here is a particularly effective way to show 
both trend and magnitude.
Additional Charts for Multiple Variables
Figure 3.25 provides an alternative display for the regional sales data of air compressors 
for Kirkland Industries. The figure uses a stacked column chart to display the North and 
the South regional sales data previously shown in a line chart in Figure 3.16. We could 
also use a stacked bar chart to display the same data by using horizontal bars instead of 
vertical. To create the stacked column chart shown in Figure 3.25, we use the following 
steps:
Step 1. Select cells A3:C14
Step 2. Click the INSERT tab on the Ribbon
Step 3.  In the Charts group, click the Insert Column Chart button 
 
Click the Stacked Column button 
 under the 2-D Column
note that here we have not 
included the additional 
steps for formatting the 
chart in excel, but the steps 
are similar to those used to 
create the previous charts.
file
WEB
SameStoreSales 
Color
–3%
–1%
–2%
–2%
–1%
–2%
–1%
–3%
–4%
–3%
–1%
1%
2%
3%
5%
–5%
–6%
–8%
–11% –13% –11% –10%
–5%
–5%
–7%
–5%
–2%
–1%
–2%
5%
8%
12%
13%
1%
1%
8%
7%
7%
8%
5%
3%
3%
0%
1%
–4%
–1%
2%
14%
13%
17%
11%
12%
12%
Chicago
3%
2%
6%
7%
8%
5%
8%
10%
9%
8%
8%
5%
7%
8%
7%
7%
–6%
–7%
–3%
–9%
6%
8%
11%
10%
11%
13%
11%
15%
15%
16%
17%
14%
15%
16%
19%
18%
16%
18%
16%
–6%
–4%
–5%
–5%
–5%
–3%
–1%
–2%
–1%
–2%
–2%
–5%
–2%
–5%
–8%
–6%
–5%
–7%
–8%
6%
7%
8%
8%
–6%
–8%
–5%
–6%
–6%
7%
7%
7%
0%
–5%
–3%
–5%
–1%
–1%
0%
–2%
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
A
B
C
D
E
F
G
H
I
J
K
L
M
N
St. Louis
Phoenix
Albany
Austin
Cincinnati
San Francisco
Seattle
Atlanta
Miami
Minneapolis
Denver
Salt Lake City
Raleigh
Boston
Pittsburgh
4%
4%
4%
2%
–6%
4%
5%
–4%
2%
8%
5%
4%
–5%
4%
2%
4%
3%
3%
3%
4%
2%
1%
5%
5%
3%
0%
–1%
9%
5%
1%
–2%
0%
10%
9%
–1%
–4%
1%
9%
11%
1%
–6%
2%
7%
8%
2%
–5%
3%
6%
6%
2%
2%
3%
4%
5%
2%
4%
5%
2%
5%
4%
–6%
JAN
FEB
MAR
APR
MAY
JUN
JUL
AUG
SEP
OCT
NOV
DEC SPARKLINES
FIGURE 3.24   HEAT MAP AND SPARKLINES FOR SAME-STORE SALES DATA
the printed 
color of Fig-
ure 3.24 does 
not match the 
excel output. 
For a full-color 
version of the 
figure, please 
refer to file 
SameStore-
SalesColor.
file
WEB
KirklandRegional

98 
Chapter 3 Data Visualization
Stacked column and bar charts allow the reader to compare the relative values of quan-
titative variables for the same category in a bar chart. However, stacked column and bar 
charts suffer from the same difficulties as pie charts because the human eye has difficulty 
perceiving small differences in areas. As a result, experts often recommend against the use 
of stacked column bar charts for more than a couple of quantitative variables in each cat-
egory. An alternative chart for these same data is called a clustered column (or bar) chart. 
It is created in Excel following the same steps but selecting Clustered Column under the 
2-D Column in Step 4. Clustered column and bar charts are often superior to stacked col-
umn and bar charts for comparing quantitative variables, but they can become cluttered for 
more than a few quantitative variables per category. 
An alternative that is often preferred to both stacked and clustered charts, particularly 
when many quantitative variables need to be displayed, is to use multiple charts. For the 
regional sales data, we would include two column charts: one for sales in the North and 
one for sales in the South. For additional regions, we would simply add additional column 
charts. To facilitate comparisons between the data displayed in each chart, it is important 
to maintain consistent axes from one chart to another. The categorical variables should be 
listed in the same order in each chart, and the axis for the quantitative variable should have 
the same range. For instance, the vertical axis for both North and South sales starts at 0 and 
ends at 140. This makes it easy to see that, in most months, the North region has greater 
sales. Figure 3.26 compares the stacked, clustered, and multiple bar chart approaches for 
the regional sales data.
Figure 3.26 shows that the multiple column charts require considerably more space 
than the stacked and clustered column charts. However, when comparing many  quantitative 
variables, using multiple charts can often be superior even if each chart must be made 
smaller. Stacked column and bar charts should be used only when comparing a few quanti-
tative variables and when there are large differences in the relative values of the quantitative 
variables within the category.
An especially useful chart for displaying multiple variables is the scatter chart 
 matrix. Table 3.12 contains a partial listing of the data for each of New York City’s 55 
subboroughs (a designation of a community within New York City) on monthly median 
rent, percentage of college graduates, poverty rate, and mean travel time to work.  Suppose 
we want to examine the relationship between these different categorical v ariables. 
Clustered column (bar) 
charts are also referred 
to as side-by-side column 
(bar) charts.
Sales ($100s)
150
100
50
0
200
A
B
I
K
L
M
J
Willia
Sales ($100s)
95
100
120
115
100
85
135
110
100
50
40
40
40
45
55
65
60
50
75
65
60
70
75
80
1
2
Month
North
South
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
South
North
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
H
G
F
E
D
C
FIGURE 3.25   STACKED COLUMN CHART FOR REGIONAL SALES DATA FOR 
KIRKLAND INDUSTRIES

 
3.3 Charts 
99
 Figure 3.27 displays a scatter chart matrix (scatter plot matrix) for data related to rentals 
in New York City. 
A scatter chart matrix allows the reader to easily see the relationships among multiple 
variables. Each scatter chart in the matrix is created in the same manner as for creating a 
single scatter chart. Each column and row in the scatter chart matrix corresponds to one 
Stacked Column Chart:
Multiple Column Charts:
0
Clustered Column Chart:
80
100
120
140
60
20
Sales ($100s)
40
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
South
North
0
80
100
120
140
60
20
Sales ($100s)
40
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
South
0
80
100
120
140
60
20
Sales ($100s)
40
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
North
Sales ($100s)
150
100
50
0
200
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
South
North
FIGURE 3.26   COMPARING STACKED, CLUSTERED AND MULTIPLE COLUMN CHARTS FOR THE 
REGIONAL SALES DATA FOR KIRKLAND INDUSTRIES
TABLE 3.12   DATA FOR NEW YORK CITY SUBBOROUGHS
Area
Median Monthly 
Rent ($)
Percentage College 
Graduates (%)
Poverty 
Rate (%)
Travel 
Time (min)
Astoria
1106
36.8
15.9
35.4
Bay Ridge
1082
34.3
15.6
41.9
Bayside/Little Neck
1243
41.3
7.6
40.6
Bedford Stuyvesant
822
21.0
34.2
40.5
Bensonhurst
876
17.7
14.4
44.0
Borough Park
980
26.0
27.6
35.3
Brooklyn Heights/ 
 Fort Greene
1086
55.3
17.4
34.5
Brownsville/Ocean Hill
714
11.6
36.0
40.3
Bushwick
945
13.3
33.5
35.5
Central Harlem
665
30.6
27.1
25.0
Chelsea/Clinton/Midtown
1624
66.1
12.7
43.7
Coney Island
786
27.2
20.0
46.3
. ..
. ..
. ..
. ..
. ..
file
WEB
NYCityData

100 
Chapter 3 Data Visualization
categorical variable. For instance, row 1 and column 1 in Figure 3.27 correspond to the 
median monthly rent variable. Row 2 and column 2 correspond to the percentage of col-
lege graduates variable. Therefore, the scatter chart shown in the row 1, column 2 shows 
the relationship between median monthly rent (on the y-axis) and the percentage of college 
graduates (on the x-axis) in New York City subboroughs. The scatter chart shown in row 2, 
column 3 shows the relationship between the percentage of college graduates (on the y-axis) 
and poverty rate (on the x-axis).
Figure 3.27 allows us to infer several interesting findings. Because the points in the 
scatter chart in row 1, column 2 generally get higher moving from left to right, this tells 
us that subboroughs with higher percentages of college graduates appear to have higher 
median monthly rents. The scatter chart in row 1, column 3 indicates that subboroughs 
with higher poverty rates appear to have lower median monthly rents. The data in row 2, 
column 3 show that subboroughs with higher poverty rates tend to have lower percentages 
of college graduates. The scatter charts in column 4 show that the relationships between 
Column 1
Row 1
Row 2
Row 3
Row 4
Column 2
Column 3
Column 4
CollegeGraduates
MedianRent
PovertyRate
CommuteTime
MedianRent
CollegeGraduates
PovertyRate
CommuteTime
FIGURE 3.27   SCATTER CHART MATRIX FOR NEW YORK CITY RENT DATA
the scatter 
charts along 
the diagonal in 
a scatter chart 
matrix (for in-
stance, in row 1, 
column 1 and in 
row 2, column 2) 
display the rela-
tionship between 
a variable and 
itself. therefore, 
the points in 
these  scatter 
charts will 
always fall along 
a straight line 
at a 45-degree 
angle, as shown 
in Figure 3.27.

 
3.3 Charts 
101
the mean travel time and the other categorical variables are not as clear as relationships in 
other columns.
The scatter chart matrix is very useful in analyzing relationships among variables. Un-
fortunately, it is not possible to generate a scatter chart matrix using native Excel func-
tionality. In the appendix at the end of this chapter, we demonstrate how to create a scatter 
chart matrix similar to Figure 3.27 using the Excel Add-In XLMiner. Statistical software 
packages such as R, NCSS, and SAS can also be used to create these matrixes.
PivotCharts in Excel
To summarize and analyze data with both a crosstabulation and charting, Excel pairs 
 PivotCharts with PivotTables. Using the restaurant data introduced in Table 3.7 and 
 Figure 3.7, we can create a PivotChart by taking the following steps: 
Step 1.   Click the INSERT tab on the Ribbon
Step 2.   In the Charts group, choose PivotChart
Step 3.   When the Create PivotChart dialog box appears:
 
 
 Choose Select a Table or Range
 
 
 Enter a1:d301 in the Table/Range: box
 
 
 Choose New Worksheet as the location for the PivotTable Report
 
 
 Click OK
Step 4.   In the PivotChart Fields area, under Choose fields to add to report:
 
 
 Drag the Quality Rating field to the AXIS (CATEGORIES) area
 
 
 Drag the Meal Price ($) field to the LEGEND (SERIES) area
 
 
 Drag the Wait Time (min) field to the VALUES area
Step 5.   Click on Sum of Wait Time (min) in the Values area
Step 6.   Click Value Field Settings… from the list of options that appear
Step 7.   When the Value Field Settings dialog box appears:
 
 
 Under Summarize value field by, choose Average
 
 
 Click Number Format
 
 
 In the Category: box, choose Number
 
 
 Enter 1 for Decimal places:
 
 
 Click OK
 
 
 When the Value Field Settings dialog box reappears, click OK
Step 8.   Right-click in cell B2 or any cell containing a meal price column label
Step 9.   Select Group from the list of options that appears
Step 10.  When the Grouping dialog box appears:
 
 
 Enter 10 in the Starting at: box
 
 
 Enter 49 in the Ending at: box
 
 
 Enter 10 in the By: box
 
 
 Click OK
Step 11.  Right-click on “Excellent” in cell A5
Step 12.  Select Move and click Move “Excellent” to End
The completed PivotTable and PivotChart appear in Figure 3.28. The PivotChart is a 
clustered column chart whose column heights correspond to the average wait times and are 
clustered into the categorical groupings of Good, Very Good, and Excellent. The columns 
are shaded to differentiate the wait times at restaurants in the various meal price ranges. 
Figure 3.28 shows that Excellent restaurants have longer wait times than Good and Very 
Good restaurants. We also see that Excellent restaurants in the price range of $30–$39 
have the longest wait times. The PivotChart displays the same information as that of the 
PivotTable in Figure 3.13, but the column chart used here makes it easier to compare the 
restaurants based on quality rating and meal price.
file
WEB
Restaurant
in excel versions prior to 
excel 2013, PivotCharts 
can be created by clicking 
the Insert tab on the rib-
bon, clicking on the arrow 
under PivotTable from the 
Tables group, and then 
selecting PivotChart.

102 
Chapter 3 Data Visualization
NOTES AND COMMENTS
1.  A new feature in Excel 2013 is the inclusion 
of Chart Buttons to quickly modify and format 
charts. Three new buttons appear next to a chart 
whenever you click on it to make it active in Ex-
cel 2013. The 
 button is the Chart Elements 
button. Clicking it brings up a list of check boxes 
to quickly add and remove axes, axis titles, a 
chart title, data labels, trendlines, and more. The 
 button is the Chart Styles button, which al-
lows you to quickly choose from many prefor-
matted chart styles to change the look of your 
chart. The 
 button is the Chart Filter button, 
and it allows you to select the data to include in 
your chart. The Chart Filter button is very useful 
for performing additional data analysis.
2.  Color is frequently used to differentiate elements 
in a chart. However, be wary of the use of color to 
differentiate for several reasons: (1) Many people 
are colorblind and may not be able to differentiate 
colors. (2) Many charts are printed in black and 
white as handouts, which reduces or eliminates 
the impact of color. (3) The use of too many col-
ors in a chart can make the chart appear too busy 
and distract or even confuse the reader. In many 
cases, it is preferable to differentiate chart ele-
ments with dashed lines, patterns, or labels. 
3.  Histograms and box plots (discussed in Chap-
ter 2 in relation to analyzing distributions) are 
other effective data visualization tools for sum-
marizing the distribution of data.
Advanced Data Visualization
In this chapter, we have presented only some of the most basic ideas for using data visu-
alization effectively both to analyze data and to communicate data analysis to others. The 
charts discussed so far are the most commonly used ones and will suffice for most data vi-
sualization needs. However, many additional concepts, charts, and tools can be used to im-
prove your data visualization techniques. In this section we briefly mention some of them.
3.4
A
B
C
D
E
F
Average of Wait
Time(min)
Columns
Lables
Row Labels
10–19
Good
2.6
12.6
25.5
7.6
2.5
12.6
29.1
11.1
0.5
12.0
34.0
19.8
2.5
12.3
32.1
13.9
10.0
32.3
27.5
Very Good
Excellent
Grand Total
20–29 30–39 40–49
Grand
Total
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
40.0
35.0
30.0
25.0
20.0
15.0
10.0
5.0
0.0
Good
Very Good
Excellent
Average of Wait Time (min)
Quality Rating
Meal Price ($)
10–19
20–29
30–39
40–49
FIGURE 3.28   PIVOTTABLE AND PIVOTCHART FOR THE RESTAURANT DATA
like Pivottables, 
PivotCharts are 
interactive. You 
can use the ar-
rows on the axes 
and legend labels 
to change the 
categorical data 
being displayed. 
For example, you 
can click on the 
Quality Rating 
horizontal axis 
label and choose 
to look at only 
Very Good and 
excellent restau-
rants.

 
3.4 Advanced Data Visualization 
103
Advanced Charts
Although line charts, bar charts, scatter charts, and bubble charts suffice for most data visu-
alization applications, other charts can be very helpful in certain situations. Unfortunately, 
these advanced charts are not easily created with native Excel functionality. 
One type of helpful chart for examining data with more than two variables is the 
 parallel coordinates plot, which includes a different vertical axis for each variable. Each 
observation in the data set is represented by drawing a line on the parallel coordinates 
plot connecting each vertical axis. The height of the line on each vertical axis represents 
the value taken by that observation for the variable corresponding to the vertical axis. For 
instance, Figure 3.29 displays a parallel coordinates plot for a sample of Major League 
Baseball players. The figure contains data for ten players who play first base (1B) and ten 
players who play second base (2B). For each player, the leftmost vertical axis plots his 
total number of home runs (HR). The center vertical axis plots the player’s total number of 
stolen bases (SB), and the rightmost vertical axis plots the player’s batting average. Vari-
ous colors differentiate 1B players from 2B players (1B players are in the lighter color, 2B 
player in the darker color). 
We can make several interesting statements upon examining Figure 3.29. The sample 
of 1B players tend to hit lots of home runs (HR) but have very few stolen bases (SB). Con-
versely, the sample of 2B players steal more bases but generally have fewer HR, although 
some 2B players have many HR and many SB. Finally, 1B players tend to have higher bat-
ting averages (AVG) than 2B players. We may infer from Figure 3.29 that the traits of 1B 
players may be different from those of 2B players. In general, this statement is true. Players 
at 1B tend to be offensive stars who hit for power and average, whereas players at 2B are 
often faster and more agile in order to handle the defensive responsibilities of the position 
(traits that are not common in strong HR hitters). Parallel coordinates plots, in which you 
can differentiate categorical variable values using color as in Figure 3.29, can be very help-
ful in identifying common traits across multiple dimensions.
A treemap is useful for visualizing hierarchical data along multiple dimensions. Smart-
Money’s Map of the Market, shown in Figure 3.30, is a treemap for analyzing stock market 
0
HR
1
SB
0.222
AVG
0.338
1B
30
39
2B
FIGURE 3.29   PARALLEL COORDINATES PLOT FOR BASEBALL DATA
the appendix at 
the end of this 
chapter describes 
how to create a 
parallel coordi-
nates plot similar 
to the one shown 
in Figure 3.29 
using XlMiner.

104 
Chapter 3 Data Visualization
performance. In the Map of the Market, each rectangle represents a particular company 
(Apple, Inc. is highlighted in Figure 3.30). The shading of the rectangle represents the over-
all performance of the company’s stock over the previous 52 weeks. The Map of the Market 
is also divided into market sectors (Health Care, Financials, Oil & Gas, etc.). The size of 
each company’s rectangle provides information on the company’s market capitalization 
size relative to the market sector and the entire market. Figure 3.30 shows that Apple has 
a very large market capitalization relative to other firms in the Technology sector and that 
it has performed exceptionally well over the previous 52 weeks. An investor can use the 
treemap in Figure 3.30 to quickly get an idea of the performance of individual companies 
relative to other companies in their market sector as well as the performance of entire 
 market sectors relative to other sectors.
Geographic Information Systems Charts
Consider the case of the Cincinnati Zoo & Botanical Garden, which derives much of its 
revenue from selling annual memberships to customers. The Cincinnati Zoo would like to 
better understand where its current members are located. Figure 3.31 displays a map of the 
Cincinnati, Ohio, metropolitan area showing the relative concentrations of Cincinnati Zoo 
members. The more darkly shaded areas represent areas with a greater number of members. 
Figure 3.31 is an example of a geographic information system (GIS), which merges maps 
and statistics to present data collected over different geographies. Displaying geographic 
data on a map can often help in interpreting data and observing patterns.
The GIS chart in Figure 3.31 combines a heat map and a geographical map to help the 
reader analyze this data set. From the figure we can see that a high concentration of zoo 
members in a band to the northeast of the zoo that includes the cities of Mason and Hamilton 
(circled). Also, a high concentration of zoo members lies to the southwest of the zoo around 
the city of Florence. These observations could prompt the zoo manager to identify the 
characteristics that the populations of Mason Hamilton and Florence share to learn what
a GiS chart such as is 
shown in Figure 3.31 is 
an example of geoanalyt-
ics, the use of data by 
geographical area or some 
other form of spatial refer-
encing to generate insights.
FIGURE 3.30   SMARTMONEY’S MAP OF THE MARKET AS AN EXAMPLE  
OF A TREEMAP
file
WEB
MapOfMarketColor
the Map of the 
Market is based 
on work done by 
Professor Ben 
Shneiderman 
and students at 
the University of 
Maryland Human– 
Computer interac-
tion lab. a full 
color version of 
this treemap is 
provided in the file 
MapOfMarket-
Color.
 

 
3.5 Data Dashboards 
105
is leading them to be zoo members. If these characteristics can be identified, the  manager 
can then try to identify other nearby populations that share these characteristics as potential 
markets for increasing the number of zoo members. 
NOTES AND COMMENTS
1.  Microsoft MapPoint is a software tool for gen-
erating relatively simple GIS charts; ArcGIS is 
a powerful software tool produced by Esri that 
can be used in many GIS applications. 
 
2.  Spotfire, Tableau, QlikView, and JMP are ex-
amples of software that include advanced data 
visualization capabilities. Many Eyes is a Web 
site developed by IBM Research that allows us-
ers to upload data and create data visualizations. 
Data Dashboards
A data dashboard is a data visualization tool that illustrates multiple metrics and automat-
ically updates these metrics as new data become available. It is like an automobile’s dash-
board instrumentation that provides information on the vehicle’s current speed, fuel level, 
and engine temperature so that a driver can assess the current operating conditions and take
3.5
Columbia
Fayette
Highland
Center
Oldenburg
Delaware
Ripley
Milan
Versailles
Sparta Greendale
Dearborn
Manchester
Dillsboro
101
47012
47016
47035
45053
45013
Bitler
New Miami
Trenton
Franklin
Carlisle
Montgomery
Springboro
Greene
Caesar
Creek
Lake
Waynesville
Monroe
Middletown
Middletown
Mason
Mason
O H I O
Indian
Springs
Hamilton
Hamilton
45042
45036
45177
45113
45107
45152
45040
45069
45241
45044
Clinton
Lebanon
South Lebanon
Warren
Landen
Loveland
Montgomery
Woodlawn
Greenhills
Pleasant Run
Fairﬁeld
Groesbeck
Northbrook
Harrison
Bright
White Oak
Dent
Cheviot
Miami
Heights
Burlington
Crestview
Hills
Boone
Rising Sun
Ohio
Pleasant
Jefferson
Warsaw
Gallatin
Hebron
Lawrenceburg
Aurora
Villa
Hills
Delhi
Delhi
Cincinnati
Cincinnati
St.
Bernard
Lockland
Madeira
Mariemont
Newport
Norwood
Covington
Florence
Florence
Indian
Hill
Goshen
Mount Repose
Mulberry
Summerside
Batavia
Greenbush
Williamsburg Mount
Orab
Brown
Georgetown
Bethel
41006
41001
45255
45244
Forestville
Kenton
Piner
Crittenden
Alexandria
Independence
K E N T U C K Y
Cold Spring
Fort Mitchell
Edgewood
Amelia
New Richmond
Clermont
Village
Estates
Sherwood
Blanchester
45011
Oxford
Oxford
Preble
47060
229
47036
47006
I N D I A N A
52
27
63
50
73
42
127
128
126
4
50
1
9
56
27
52
125
275
71
74
76
71
Switzerland
Pendleton
Campbell
Norwood
Batesville
Union
Franklin
Cincinnati
Zoo
Cincinnati
Zoo
FIGURE 3.31   GIS CHART FOR CINCINNATI ZOO MEMBER DATA
file
WEB
CinciZooGISColor

106 
Chapter 3 Data Visualization
effective action. Similarly, a data dashboard provides the important metrics that manag-
ers need to quickly assess the performance of their organization and react  accordingly. In 
this section we provide guidelines for creating effective data dashboards and an example 
application. 
Principles of Effective Data Dashboards
In an automobile dashboard, values such as current speed, fuel level, and oil pressure are 
displayed to give the driver a quick overview of current operating characteristics. In a busi-
ness, the equivalent values are often indicative of the business’s current operating charac-
teristics, such as its financial position, the inventory on hand, customer service metrics, and 
the like. These values are typically known as key performance indicators (KPIs). A data 
dashboard should provide timely summary information on KPIs that are important to the 
user, and it should do so in a manner that informs rather than overwhelms its user.
Ideally, a data dashboard should present all KPIs as a single screen that a user can 
quickly scan to understand the business’s current state of operations. Rather than requiring 
the user to scroll vertically and horizontally to see the entire dashboard, it is better to create 
multiple dashboards so that each dashboard can be viewed on a single screen.
The KPIs displayed in the data dashboard should convey meaning to its user and be 
related to the decisions the user makes. For example, the data dashboard for a marketing 
manager may have KPIs related to current sales measures and sales by region, while the 
data dashboard for a CFO should provide information on the current financial standing of 
the company including cash on hand, current debt obligations, and so on. 
A data dashboard should call attention to unusual measures that may require atten-
tion, but not in an overwhelming way. Color should be used to call attention to specific 
values to differentiate categorical variables, but the use of color should be restrained. 
Too many different or too bright colors make the presentation distracting and difficult 
to read. 
Applications of Data Dashboards
To illustrate the use of a data dashboard in decision making, we discuss an application 
involving the Grogan Oil Company which has offices located in three cities in Texas: 
Austin (its headquarters), Houston, and Dallas. Grogan’s Information Technology (IT) 
call center, located in Austin, handles calls from employees regarding computer-related 
problems involving software, Internet, and e-mail issues. For example, if a Grogan em-
ployee in Dallas has a computer software problem, the employee can call the IT call 
center for assistance. 
The data dashboard shown in Figure 3.32, developed to monitor the performance 
of the call center, combines several displays to track the call center’s KPIs. The data 
presented are for the current shift, which started at 8:00 a.m. The line chart in the upper 
left-hand corner shows the call volume for each type of problem (Software, Internet, or 
E-mail) over time. This chart shows that call volume is heavier during the first few hours 
of the shift, that calls concerning e-mail issues appear to decrease over time, and that 
the volume of calls regarding software issues are highest at midmorning. A line chart is 
effective here because these are time series data and the line chart helps identify trends 
over time. 
The column chart in the upper right-hand corner of the dashboard shows the percentage 
of time that call center employees spent on each type of problem or were idle (not working 
on a call). Both the line chart and the column chart are important displays in determining 
optimal staffing levels. For instance, knowing the call mix and how stressed the system is, 
as measured by percentage of idle time, can help the IT manager make sure that enough call 
center employees are available with the right level of expertise. 
Key performance indicators 
are sometimes referred to 
as key performance metrics 
(KPMs).

 
3.5 Data Dashboards 
107
The clustered bar chart in the middle right of the dashboard shows the call volume by 
type of problem for each of Grogan’s offices. This allows the IT manager to quickly iden-
tify if there is a particular type of problem by location. For example, the office in Austin 
seems to be reporting a relatively high number of issues with e-mail. If the source of the 
problem can be identified quickly, then the problem might be resolved quickly for many 
users all at once. Also, note that a relatively high number of software problems are coming 
from the Dallas office. In this case, the Dallas office is installing new software, resulting in 
more calls to the IT call center. Having been alerted to this by the Dallas office last week, 
the IT manager knew that calls coming from the Dallas office would spike and proactively 
increased staffing levels to handle the expected increase in calls. 
For each unresolved case that was received more than 15 minutes ago, the bar chart 
shown in the middle left of the data dashboard displays the length of time for which each 
case has been unresolved. This chart enables Grogan to quickly monitor the key problem 
cases and decide whether additional resources may be needed to resolve them. The worst 
case, T57, has been unresolved for over 300 minutes and is actually left over from the 
previous shift. Finally, the chart in the bottom panel shows the length of time required for 
Grogan Oil
IT Call Center
Shift 1
22–Feb–12
12:44:00 PM
Time to Resolve a Case
Minutes
14
0
< 1
1–2
2–3
3–4
4–5
5–6
6–7
7–8
8–9
9–10
10–11
11–12
12–13
13–14
14–15
15–16
16–17
17–18
18–19
19–20
20–21
21–22
22–23
23–24
24–25
25–26
26–27
27–28
28–29
29–30
30–31
31–32
32+
Frequency
2
4
6
8
10
12
Number of Calls
Call Volume by City
Austin
Dallas
Houston
0
5
10
15
20
25
Unresolved Cases Beyond 15 Minutes
T57
W5
Case Number
W24
W59
0
100
200
Minutes
300
400
Call Volume
Hour
16
0
14
12
10
8
6
4
2
Email
Software
Internet
Email
Software
Internet
Email
Software
Internet
8:00
9:00
10:00
11:00
12:00
Time Breakdown This Shift
Hour
0%
10%
20%
30%
40%
50%
Email
Email
22%
Internet
Internet
18%
Software
Software 46%
Idle
Idle
14%
FIGURE 3.32   DATA DASHBOARD FOR THE GROGAN OIL INFORMATION TECHNOLOGY 
CALL CENTER

108 
Chapter 3 Data Visualization
resolved cases during the current shift. This chart is an example of a frequency distribution 
for quantitative data (discussed in Chapter 2).
Throughout the dashboard a consistent color coding scheme is used for problem type 
( E-mail, Software, and Internet). Because the Time to Resolve a Case chart is not broken 
down by problem type, dark grey shading is used so as not to confuse these values with 
a particular problem type. Other dashboard designs are certainly possible, and improve-
ments could certainly be made to the design shown in Figure 3.32. However, what is 
important is that information is clearly communicated so that managers can improve 
their decision making.
The Grogan Oil data dashboard presents data at the operational level, is updated in 
real time, and is used for operational decisions such as staffing levels. Data dashboards 
may also be used at the tactical and strategic levels of management. For example, a sales 
manager could monitor sales by salesperson, by region, by product, and by customer. 
This would alert the sales manager to changes in sales patterns. At the highest level, a 
more strategic dashboard would allow upper management to quickly assess the financial 
health of the company by monitoring more aggregate financial, service-level, and capacity-
utilization information.
NOTES AND COMMENTS
The creation of data dashboards in Excel generally 
requires the use of macros written using  Visual 
 Basic for Applications (VBA). The use of VBA 
is beyond the scope of this textbook, but VBA is 
a  powerful programming tool that can greatly in-
crease the capabilities of Excel for analytics, in-
cluding data visualization.
Summary
In this chapter we covered techniques and tools related to data visualization. We discussed 
several important techniques for enhancing visual presentation, such as improving the clarity 
of tables and charts by removing unnecessary lines and presenting numerical values only 
to the precision necessary for analysis. We explained that tables can be preferable to charts 
for data visualization when the user needs to know exact numerical values. We introduced 
crosstabulation as a form of a table for two variables and explained how to use Excel to 
create a PivotTable. 
We presented many charts in detail for data visualization, including scatter charts, line 
charts, bar and column charts, bubble charts, and heat maps. We explained that pie charts 
and three-dimensional charts are almost never preferred tools for data visualization and that 
bar (or column) charts are usually much more effective than pie charts. We also discussed 
several advanced data visualization charts, such as parallel coordinates plots, tree maps, and 
GIS charts. We introduced data dashboards as a data visualization tool that provides a sum-
mary of a firm’s operations in visual form to allow managers to quickly assess the current 
operating conditions and to aid decision making. 
Many other types of charts can be used for specific forms of data visualization, but we 
have covered the most popular and most useful ones. Data visualization is very important 
for helping someone analyze data and identify important relations and patterns. The effec-
tive design of tables and charts is also necessary to communicate data analysis to others. 
Tables and charts should be only as complicated as necessary to help the user understand 
the patterns and relationships in the data.

 
Glossary 
109
Glossary
Data-ink ratio The ratio of the amount of ink used in a table or chart that is necessary to 
convey information to the total amount of ink used in the table and chart. Ink used that is 
not necessary to convey information reduces the data-ink ratio.
Crosstabulation A tabular summary of data for two variables. The classes of one vari-
able are represented by the rows; the classes for the other variable are represented by the 
columns.
PivotTable A crosstabulation created in Excel that is interactive.
Chart A visual method for displaying data; also called a graph or a figure.
Scatter chart A graphical presentation of the relationship between two quantitative vari-
ables. One variable is shown on the horizontal axis, and the other variable is shown on the 
vertical axis.
Trendline  A line that provides an approximation of the relationship between variables in 
a chart.
Line chart A graphical presentation of time-series data in which the data points are con-
nected by a line.
Sparkline A special type of line chart that indicates the trend of data but not magnitude. A 
sparkline does not include axes or labels.
Bar chart A graphical presentation that uses horizontal bars to display the magnitude of a 
quantitative data. Each bar typically represents a class of a categorical variable.
Column chart A graphical presentation that uses vertical bars to display the magnitude of 
quantitative data. Each bar typically represents a class of a categorical variable.
Pie chart A graphical presentation used to compare categorical data. Because of difficulties 
in comparing relative areas on a pie chart, these charts are not recommended. Bar or column 
charts are generally superior to pie charts for comparing categorical data.
Bubble chart A graphical presentation used to visualize three variables in a two- dimensional 
graph. The two axes represent two variables, and the magnitude of the third variable is given 
by the size of the bubble.
Heat map A two-dimensional graphical presentation of data in which color shadings in-
dicate magnitudes.
Stacked column (bar) chart A special type of column (bar) chart in which multiple vari-
ables appear on the same bar.
Clustered column (bar) chart A special type of column (bar) chart in which multiple bars 
are clustered in the same class to compare multiple variables; also known as a side-by-side 
column (bar) chart.
Scatter chart matrix A graphical presentation that uses multiple scatter charts arranged as 
a matrix to illustrate the relationships among multiple variables.
PivotChart A graphical presentation created in Excel that functions similar to a PivotTable. 
Parallel coordinates plot A graphical presentation used to examine more than two vari-
ables in which each variable is represented by a different vertical axis. Each observation in 
a data set is plotted in a parallel coordinates plot by drawing a line between the values of 
each variable for the observation.
Treemap A graphical presentation that is useful for visualizing hierarchical data along mul-
tiple dimensions. A treemap groups data according to the classes of a categorical variable 
and uses rectangles whose size relates to the magnitude of a quantitative variable.
Geographic information system (GIS) A system that merges maps and statistics to  present 
data collected over different geographies.
Data dashboard A data visualization tool that updates in real time and gives multiple 
outputs. 
Key performance indicator (KPI) A metric that is crucial for understanding the current 
performance of an organization; also known as key performance metrics (KPMs).

110 
Chapter 3 Data Visualization
Problems
 1.  A sales manager is trying to determine appropriate sales performance bonuses for her team 
this year. The following table contains the data relevant to determining the bonuses, but it 
is not easy to read and interpret. Reformat the table to improve readability and to help the 
sales manager make her decisions on bonuses.
Salesperson
Total Sales  
($)
Average Performance 
Bonus Previous  
Years ($)
Customer 
Accounts
Years with 
Company
Smith, Michael
325000.78
12499.3452
124
14
Yu, Joe
13678.21
239.9434
9
7
Reeves, Bill
452359.19
21987.2462
175
21
Hamilton, Joshua
87423.91
7642.9011
28
3
Harper, Derek
87654.21
1250.1393
21
4
Quinn, Dorothy
234091.39
14567.9833
48
9
Graves, Lorrie
379401.94
27981.4432
121
12
Sun, Yi
31733.59
672.9111
7
1
Thompson, Nicole
127845.22
13322.9713
17
3
 2.  The following table shows an example of gross domestic product values for five countries 
from 2005 to 2010 in equivalent U.S. dollars ($). 
Gross Domestic Product (in US Dollars, $)
Country
2005
2006
2007
2008
2009
2010
Albania
7385937423
8105580293
9650128750
11592303225
10781921975
10569204154
Argentina
169725491092
198012474920
241037555661
301259040110
285070994754
339604450702
Australia
704453444387
758320889024
916931817944
982991358955
934168969952
1178776680167
Austria
272865358404
290682488352
336840690493
375777347214
344514388622
341440991770
Belgium
335571307765
355372712266
408482592257
451663134614
421433351959
416534140346
a. How could you improve the readability of this table?
b.  The file GdPyears contains sample data from the United Nations Statistics Division 
on 30 countries and their GDP values from 2005 to 2010 in U.S. dollars ($). Create a 
table that provides all these data for a user. Format the table to make it as easy to read 
as possible. 
 
 Hint: It is generally not important for the user to know GDP to an exact dollar figure. 
It is typical to present GDP values in millions or billions of dollars.
 3.  The following table provides monthly revenue values for Tedstar, Inc., a company that 
sells valves to large industrial firms. The monthly revenue data have been graphed using 
a line chart in the following figure.
Month
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
 Dec
Revenue ($)
145869
123576
143298
178505
186850
192850
134500
145286
154285
148523
139600
148235
file
WEB
SalesBonuses
file
WEB
GDPyears

 
Problems 
111
0
30000
40000
50000
20000
10000
60000
70000
80000
90000
10000
110000
120000
130000
140000
150000
160000
170000
180000
190000
200000
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
210000
Revenue ($)
Months
a. What are the problems with the layout and display of this line chart?
b.  Create a new line chart for the monthly revenue data at Tedstar, Inc. Format the chart 
to make it easy to read and interpret.
 4.  In the file MajorSalary, data have been collected from 111 College of Business graduates 
on their monthly starting salaries. The graduates include students majoring in manage-
ment, finance, accounting, information systems, and marketing. Create a PivotTable in 
Excel to display the number of graduates in each major and the average monthly starting 
salary for students in each major. 
a. Which major has the greatest number of graduates?
b.  Which major has the highest average starting monthly salary?
c.  Use the PivotTable to determine the major of the student with the highest overall start-
ing monthly salary. What is the major of the student with the lowest overall starting 
monthly salary?
 5.  entrepreneur magazine ranks franchises. Among the factors that the magazine uses in its 
rankings are growth rate, number of locations, start-up costs, and financial stability. A 
recent ranking listed the top 20 U.S. franchises and the number of locations as follows:
Franchise
Number of U.S. 
Locations
Franchise
Number of U.S. 
Locations
Hampton Inns
ampm
McDonald’s
7-Eleven Inc.
Supercuts
Days Inn
Vanguard Cleaning Systems
Servpro
Subway
Denny’s Inc.
1,864
3,183
32,805
37,496
2,130
1,877
2,155
1,572
34,871
1,668
Jan-Pro Franchising Intl. Inc.
Hardee’s
Pizza Hut Inc.
Kumon Math & Reading Centers
Dunkin’ Donuts
KFC Corp.
Jazzercise Inc.
Anytime Fitness
Matco Tools
Stratus Building Solutions
12,394
1,901
13,281
25,199
9,947
16,224
7,683
1,618
1,431
5,018
 
These data can be found in the file Franchises. Create a PivotTable to summarize these 
data using classes 0–9,999, 10,000–19,999, 20,000–29,999, 30,000–39,999 to answer the 
following questions. (Hint: Use Number of U.S. Locations as the COLUMNS, and use 
Count of Number of U.S. Locations as the VALUES in the PivotTable.)
a. How many franchises have between 0 and 9,999 locations? 
b. How many franchises have more than 30,000 locations?
file
WEB
Tedstar
file
WEB
MajorSalary
file
WEB
Franchises

112 
Chapter 3 Data Visualization
 6.  The file MutualFunds contains a data set with information for 45 mutual funds that are 
part of the Morningstar Funds 500. The data set includes the following five variables: 
 
 Fund type: The type of fund, labeled DE (Domestic Equity), IE (International Equity),  
 and FI (Fixed Income)
 
 net asset Value ($): The closing price per share
 
 Five-Year average return (%): The average annual return for the fund over the past  
 five years
 
 expense ratio (%): The percentage of assets deducted each fiscal year for fund  
 expenses
 
 Morningstar rank: The risk adjusted star rating for each fund; Morningstar ranks go  
 from a low of 1 Star to a high of 5 Stars.
a.  Prepare a PivotTable that gives the frequency count of the data by Fund Type (rows) 
and the five-year average annual return (columns). Use classes of 0–9.99, 10–19.99, 
20–29.99, 30–39.99, 40–49.99, and 50–59.99 for the Five-Year Average Return (%).
b.  What conclusions can you draw about the fund type and the average return over the 
past five years?
 7.  The file taxdata contains information from federal tax returns filed in 2007 for all coun-
ties in the United States (3,142 counties in total). Create a PivotTable in Excel to answer 
the following questions. The PivotTable should have State Abbreviation as Row Labels. 
The Values in the PivotTable should be the sum of adjusted gross income for each state.
a.  Sort the PivotTable data to display the states with the smallest sum of adjusted gross 
income on top and the largest on the bottom. Which state had the smallest sum of 
adjusted gross income? What is the total adjusted gross income for federal tax returns 
filed in this state with the smallest total adjusted gross income? (Hint: To sort data in 
a PivotTable in Excel, right-click any cell in the PivotTable that contains the data you 
want to sort, and select Sort.)
b.  Add the County Name to the Row Labels in the PivotTable. Sort the County Names by 
Sum of Adjusted Gross Income with the lowest values on top and the highest values 
on bottom. Filter the Row Labels so that only the state of Texas is displayed. Which 
county had the smallest sum of adjusted gross income in the state of Texas? Which 
county had the largest sum of adjusted gross income in the state of Texas?
c.  Click on Sum of Adjusted Gross Income in the Values area of the PivotTable in Ex-
cel. Click Value Field Settings . . . . Click the tab for Show Values As. In the Show 
values as box, choose % of Parent Row Total. Click OK. This displays the adjusted 
gross income reported by each county as a percentage of the total state adjusted gross 
income. Which county has the highest percentage adjusted gross income in the state 
of Texas? What is this percentage?
d.  Remove the filter on the Row Labels to display data for all states. What percentage of 
total adjusted gross income in the United States was provided by the state of New York?
 8.  The file FdiCBankFailures contains data on failures of federally insured banks between 
2000 and 2012. Create a PivotTable in Excel to answer the following questions. The Pivot-
Table should group the closing dates of the banks into yearly bins and display the counts 
of bank closures each year in columns of Excel. Row labels should include the bank 
locations and allow for grouping the locations into states or viewing by city. You should 
also sort the PivotTable so that the states with the greatest number of total bank failures 
between 2000 and 2012 appear at the top of the PivotTable.
a.  Which state had the greatest number of federally insured bank closings between 2000 
and 2012?
b.  How many bank closings occurred in the state of Nevada (NV) in 2010? In what cities 
did these bank closings occur?
c.  Use the PivotTable’s filter capability to view only bank closings in California (CA), 
Florida (FL), Texas (TX), and New York (NY) for the years 2009 through 2012. What 
is the total number of bank closings in these states between 2009 and 2012?
file
WEB
MutualFunds
note that excel may display 
the column headings as 
0–10, 10–20, 20–30, etc., 
but they should be inter-
preted as 0–9.99, 10–19.99, 
20–29.99, etc.
file
WEB
TaxData
file
WEB
FDICBankFailures

d.  Using the filtered PivotTable from part c, what city in Florida had the greatest number of 
bank closings between 2009 and 2012? How many bank closings occurred in this city?
e.  Create a PivotChart to display a column chart that shows the total number of bank 
closings in each year 2000 through 2012 in the state of Florida. Adjust the formatting 
of this column chart so that it best conveys the data. What does this column chart sug-
gest about bank closings between 2000 and 2012 in Florida? Discuss.
 
 Hint: You may have to switch the row and column labels in the PivotChart to get the 
best presentation for your PivotChart.
 9. The following 20 observations are for two quantitative variables, x and y. 
Observation
x
y
Observation
x
y
1
2
3
4
5
6
7
8
9
10
222
233
2
29
213
21
213
223
14
3
22
49
8
216
10
228
27
35
25
23
11
12
13
14
15
16
17
18
19
20
237
34
9
233
20
23
215
12
220
27
48
229
218
31
216
14
18
17
211
222
a. Create a scatter chart for these 20 observations. 
b.  Fit a linear trendline to the 20 observations. What can you say about the relationship 
between the two quantitative variables?
10.  The file Fortune500 contains data for profits and market capitalizations from a recent 
sample of firms in the Fortune 500
a.  Prepare a scatter diagram to show the relationship between the variables Market Capi-
talization and Profit where Market Capitalization is on the vertical axis and Profit is 
on the horizontal axis. Comment on any relationship between the variables.
b.  Create a trendline for the relationship between Market Capitalization and Profit. What 
does the trendline indicate about this relationship?
11.  The International Organization of Motor Vehicle Manufacturers (officially known as the 
Organisation Internationale des Constructeurs d’Automobiles, OICA) provides data on 
worldwide vehicle production by manufacturer. The following table shows vehicle pro-
duction numbers for four different manufacturers for five recent years. Data are in millions 
of vehicles.
Manufacturer
Year 1
Year 2
Year 3
Year 4
Year 5
Toyota
GM
Volkswagen
Hyundai
8.04
8.97
5.68
2.51
8.53
9.35
6.27
2.62
9.24
8.28
6.44
2.78
7.23
6.46
6.07
4.65
8.56
8.48
7.34
5.76
a.  Construct a line chart for the time series data for years 1 through 5 showing the number 
of vehicles manufactured by each automotive company. Show the time series for all 
four manufacturers on the same graph.
b.  What does the line chart indicate about vehicle production amounts between years 1 
through 5? Discuss.
c.  Construct a clustered bar chart showing vehicles produced by automobile manufac-
turer using the years 1 through 5 data. Represent the years of production along the 
horizontal axis, and cluster the production amounts for the four manufacturers in each 
year. Which company is the leading manufacturer in each year?
file
WEB
Scatter
file
WEB
Fortune500
file
WEB
AutoProduction
 
Problems 
113

114 
Chapter 3 Data Visualization
12.  The following table contains time series data for regular gasoline prices in the United 
States for 36 consecutive months:
Month
Price ($)
Month
Price ($)
Month
Price ($)
1
2
3
4
5
6
7
8
9
10
11
12
2.27
2.63
2.53
2.62
2.55
2.55
2.65
2.61
2.72
2.64
2.77
2.85
13
14
15
16
17
18
19
20
21
22
23
24
2.84
2.73
2.73
2.73
2.71
2.80
2.86
2.99
3.10
3.21
3.56
3.80
25
26
27
28
29
30
31
32
33
34
35
36
3.91
3.68
3.65
3.64
3.61
3.45
3.38
3.27
3.38
3.58
3.85
3.90
a.  Create a line chart for these time series data. What interpretations can you make about 
the average price per gallon of conventional regular gasoline over these 36 months?
b.  Fit a linear trendline to the data. What does the trendline indicate about the price of 
gasoline over these 36 months?
13.  The following table contains sales totals for the top six term life insurance salespeople at 
American Insurance. 
 
Salesperson
Contracts Sold
Harish
David
Kristina
Steven
Tim
Mona
24
41
19
23
53
39
a.  Create a column chart to display the information in the table above. Format the column 
chart to best display the data by adding axes labels, a chart title, etc.
b.  Sort the values in Excel so that the column chart is ordered from most contracts sold 
to fewest.
c.  Insert data labels to display the number of contracts sold for each salesperson above 
the columns in the column chart created in part a.
14.  The total number of term life insurance contracts sold in Problem 13 is 199. The following 
pie chart shows the percentages of contracts sold by each salesperson. 
19.6%
12.1%
20.6%
9.5%
11.6%
26.6%
Harish
David
Kristina
Steven
Tim
Mona
file
WEB
GasPrices

a. What are the problems with using a pie chart to display these data?
b.  What type of chart would be preferred for displaying the data in this pie chart?
c.  Use a different type of chart to display the percentage of contracts sold by each sales-
person that conveys the data better than the pie chart. Format the chart and add data 
labels to improve the chart’s readability.
15.  An automotive company is considering the introduction of a new model of sports car 
that will be available in two engine types: four cylinder and six cylinder. A sample of 
 customers who were interested in this new model were asked to indicate their preference 
for an engine type for the new model of automobile. The customers were also asked to 
indicate their preference for exterior color from four choices: red, black, green, and white. 
Consider the following data regarding the customer responses:
Four Cylinder
Six Cylinder
Red
Black
Green
White
143
200
321
420
857
800
679
580
a.  Construct a clustered column chart with exterior color as the horizontal variable.
b. What can we infer from the clustered bar chart in part a?
16. Consider the following survey results regarding smartphone ownership by age: 
Age Category
Smartphone (%)
Other Cell  
Phone (%)
No Cell  
Phone (%)
18–24
25–34
35–44
45–54
55–64
651
49
58
44
28
22
11
46
35
45
58
59
45
 5
 7
11
14
19
44
a.  Construct a stacked column chart to display the survey data on type of mobile phone 
ownership. Use Age Category as the variable on the horizontal axis.
b.  Construct a clustered column chart to display the survey data. Use Age Category as 
the variable on the horizontal axis.
c.  What can you infer about the relationship between age and smartphone ownership 
from the column charts in parts a and b? Which column chart (stacked or clustered) is 
best for interpreting this relationship? Why?
17.  The Northwest regional manager of Logan Outdoor Equipment Company has conducted a 
study to determine how her store managers are allocating their time. A study was undertaken 
over three weeks that collected the following data related to the percentage of time each store 
manager spent on the tasks of attending required meetings, preparing business reports, custom-
er interaction, and being idle. The results of the data collection appear in the following table:
Attending  
Required 
Meetings (%)
Tasks Preparing 
Business Reports 
(%)
Customer 
Interaction 
(%)
Idle (%)
Locations
Seattle
Portland
Bend
Missoula
Boise
Olympia
32
52
18
21
12
17
17
11
11
 6
14
12
37
24
52
43
64
54
14
13
19
30
10
17
file
WEB
NewAuto
file
WEB
Smartphone
file
WEB
Logan
 
Problems 
115

116 
Chapter 3 Data Visualization
a. Create a stacked bar chart with locations along the vertical axis. Reformat the bar chart 
to best display these data by adding axis labels, a chart title, and so on.
b. Create a clustered bar chart with locations along the vertical axis and clusters of tasks. 
Reformat the bar chart to best display these data by adding axis labels, a chart title, 
and the like.
c. Create multiple bar charts where each location becomes a single bar chart showing the 
percentage of time spent on tasks. Reformat the bar charts to best display these data 
by adding axes labels, a chart title, and so forth.
d. Which form of bar chart (stacked, clustered, or multiple) is preferable for these data? 
Why?
e. What can we infer about the differences among how store managers are allocating 
their time at the different locations?
18. The Ajax Company uses a portfolio approach to manage their research and development 
(R&D) projects. Ajax wants to keep a mix of projects to balance the expected return and 
risk profiles of their R&D activities. Consider the situation where Ajax has six R&D proj-
ects as characterized in the table. Each project is given an expected rate of return and a risk 
assessment, which is a value between 1 and 10 where 1 is the least risky and 10 is the most 
risky. Ajax would like to visualize their current R&D projects to keep track of the overall 
risk and return of their R&D portfolio.
Project
Expected Rate of 
Return (%)
Risk Estimate
Capital Invested  
(Millions $)
1
2
3
4
5
6
12.6
14.8
9.2
6.1
21.4
7.5
6.8
6.2
4.2
6.2
8.2
3.2
6.4
45.8
9.2
17.2
34.2
14.8
a. Create a bubble chart where the expected rate of return is along the horizontal axis, the 
risk estimate is on the vertical axis, and the size of the bubbles represents the amount 
of capital invested. Format this chart for best presentation by adding axes labels and 
labeling each bubble with the project number.
b. The efficient frontier of R&D projects represents the set of projects that have the high-
est expected rate of return for a given level of risk. In other words, any project that has 
a smaller expected rate of return for an equivalent, or higher, risk estimate cannot be 
on the efficient frontier. From the bubble chart in part a., what projects appear to be 
located on the efficient frontier?
19. Heat maps can be very useful for identifying missing data values in moderate to large data 
sets. The file Surveyresults contains the responses from a marketing survey: 108 individu-
als responded to the survey of 10 questions. Respondents provided answers of 1, 2, 3, 4, 
or 5 to each question, corresponding to the overall satisfaction on 10 different dimensions 
of quality. However, not all respondents answered every question. 
a. To find the missing data values, create a heat map in Excel that shades the empty 
cells a different color. Use Excel’s Conditional Formatting function to create this 
heat map. 
 
Hint: Click on Conditional Formatting in the Styles group in the HOME tab. Select 
Highlight Cells Rules and click More Rules . . . . Then enter Blanks in the Format 
only cells with: box. Choose a format for these blank cells that will make them obvi-
ously stand out.
b. For each question, which respondents did not provide answers? Which question has 
the highest nonresponse rate?
file
WEB
Ajax
file
WEB
SurveyResults

20. The following table shows monthly revenue for six different web development companies. 
Revenue ($)
Company
Jan
Feb
Mar
Apr
May
Jun
Blue Sky Media
Innovate Technologies
Timmler Company
Accelerate, Inc.
Allen and Davis, LLC
Smith Ventures
8,995
18,250
8,480
28,325
4,580
17,500
9,285
16,870
7,650
27,580
6,420
16,850
11,555
19,580
7,023
23,450
6,780
20,185
9,530
17,260
6,540
22,500
7,520
18,950
11,230
18,290
5,700
20,800
8,370
17,520
13,600
16,250
4,930
19,800
10,100
18,580
a. Use Excel to create sparklines for sales at each company. 
b. Which companies have generally decreasing revenues over the six months? Which 
company has exhibited the most consistent growth over the six months? Which com-
panies have revenues that are both increasing and decreasing over the six months?
c. Use Excel to create a heat map for the revenue of the six companies. Do you find the 
heat map or the sparklines to be better at communicating the trend of revenues over 
the six months for each company? Why?
21. Zeitler’s Department Stores sells its products online and through traditional brick-and-
mortar stores. The following parallel coordinates plot displays data from a sample of 
20 customers who purchased clothing from Zeitler’s either online or in-store. The data 
include variables for the customer’s age, annual income, and the distance from the cus-
tomer’s home to the nearest Zeitler’s store. According to the parallel coordinates plot, how 
are online customers differentiated from in-store customers?
23
Age
64
14
Annual Income
($ 000)
6
Distance from Nearest
Store (miles)
120
154
In-store
Online
22. The file Zeitlerselectronics contains data on customers who purchased electronic equip-
ment either online or in-store from Zeitler’s Department Stores. 
a. Create a parallel coordinates plot using XLMiner for these data. Include vertical axes 
for the customer’s age, annual income, and distance from nearest store. Color the lines 
by the type of purchase made by the customer (online or in-store).
b. How does this parallel coordinates plot compare to the one shown in Problem 21 for 
clothing purchases. Does the division between online and in-store purchasing habits 
for customer’s buying electronics equipment appear to be the same as for those cus-
tomers buying clothing?
c. Parallel coordinates plots are very useful for interacting with your data to perform 
analysis. Filter the parallel coordinates plot in XLMiner so that only customers whose 
home is more than 40 miles from the nearest store are displayed. What do you learn 
from the parallel coordinates plot about these customers?
file
WEB
WebDevelop
file
WEB
ZeitlersElectronics
 
Problems 
117

118 
Chapter 3 Data Visualization
23.  Aurora Radiological Services is a health care clinic that provides radiological imaging ser-
vices (such as MRIs, X-rays, and CAT scans) to patients. It is part of Front Range Medical 
Systems that operates clinics throughout the state of Colorado.
a. What type of key performance indicators and other information would be appropriate 
to display on a data dashboard to assist the Aurora clinic’s manager in making daily 
staffing decisions for the clinic?
b. What type of key performance indicators and other information would be appropriate 
to display on a data dashboard for the CEO of Front Range Medical Systems who 
oversees the operation of multiple radiological imaging clinics?
24. Bravman Clothing sells high-end clothing products online and through phone orders. 
 Bravman Clothing has taken a sample of 25 customers who placed orders by phone. The file 
Bravman contains data for each customer purchase, including the wait time the c ustomer 
experienced when he or she called, the customer’s purchase amount, the customer’s age, 
and the customer’s credit score. Bravman Clothing would like to analyze these data to try 
to learn more about their phone customers.
a. Use XLMiner to create a scatter chart matrix for these data. Include the variables wait 
time, purchase amount, customer age, and credit score.
b. What can you infer about the relationships between these variables from the scatter 
chart matrix?
 
Case Problem All-Time Movie Box Office Data
The motion picture industry is an extremely competitive business. Dozens of movie studios 
produce hundreds of movies each year, many of which cost hundreds of millions of dollars 
to produce and distribute. Some of these movies will go on to earn hundreds of millions of 
dollars in box office revenues, while others will earn much less than their production cost.
Data from fifty of the top box office receipt generating movies are provided in the file 
top50Movies. The following table shows the first ten movies contained in this data set. The 
categorical variables included in the data set for each movie are the rating and genre. Quanti-
tative variables for the movie’s release year, inflation- and  noninflation-adjusted box office 
receipts in the United States, budget, and the world box office receipts are also included.
Title
Year 
Released
U.S. Box  Office 
Receipts  
( inflation-adjusted 
millions $)
Rating
Genre
Budget  
(non inflation-
adjusted 
millions $)
World Box 
Office Receipts 
(non inflation-
adjusted 
millions $)
U.S. Box 
Office Receipts 
(non inflation-
adjusted 
millions $)
Gone with the  
 Wind
1939
1650 
G
Drama
 3 
391 
199 
Star Wars
1977
1426 
PG
Scifi/fantasy
 11 
798 
461 
the Sound of  
 Music
1965
1145 
G
Musical
—
163 
163 
e.t.
1982
1132 
PG
Scifi/fantasy
—
757 
435 
titanic
1997
1096 
PG-13
Drama
200 
2185 
659 
the ten  
  Commandments
1956
1053 
G
Drama
 14 
80 
 80 
Jaws
1975
1029 
PG
Action
 12 
471 
260 
doctor Zhivago
1965
973 
PG-13
Drama
 11 
112 
112 
the Jungle Book
1967
871 
G
Animated
—
206 
142 
Snow White and the  
 Seven dwarfs
1937
854 
G
Animated
 1 
185 
185 
file
WEB
Bravman
file
WEB
Top50Movies

Managerial Report
Use the data visualization methods presented in this chapter to explore these data and dis-
cover relationships between the variables. Include the following, in your report:
1.  Create a scatter chart to examine the relationship between the year released and 
the inflation-adjusted U.S. box office receipts. Include a trendline for this scatter 
chart. What does the scatter chart indicate about inflation-adjusted U.S. box office 
receipts over time for these top 50 movies? 
2. Create a scatter chart to examine the relationship between the budget and the 
 noninflation-adjusted world box office receipts. (note: You may have to adjust the data 
in Excel to ignore the missing budget data values to create your scatter chart. You can 
do this by first sorting the data using Budget and then creating a scatter chart using only 
the movies that include data for Budget.) What does this scatter chart indicate about the 
relationship between the movie’s budget and the world box office receipts? 
3. Create a frequency distribution, percent frequency distribution, and histogram for 
inflation-adjusted U.S. box office receipts. Use bin sizes of $100 million. Interpret 
the results. Do any data points appear to be outliers in this distribution?
4. Create a PivotTable for these data. Use the PivotTable to generate a crosstabulation 
for movie genre and rating. Determine which combinations of genre and rating 
are most represented in the top 50 movie data. Now filter the data to consider only 
movies released in 1980 or later. What combinations of genre and rating are most 
represented for movies after 1980? What does this indicate about how the prefer-
ences of moviegoers may have changed over time?
5. Use the PivotTable to display the average inflation-adjusted U.S. box office receipts 
for each genre–rating pair for all movies in the dataset. Interpret the results.
 
Appendix  Creating a Scatter Chart Matrix and  
a  Parallel Coordinates Plot with XlMiner 
The Excel Add-in XLMiner provides additional functionality for visualizing data in Excel. 
XLMiner allows the user to create several charts that are not available in Excel without an 
 add-in and allows for additional interactivity with many charts to aid in analyzing data. In 
this chapter appendix, we discuss two of the most useful charts for analyzing multivariate 
data that are available in XLMiner but not in Excel: the scatter chart matrix and the parallel 
coordinates plot.
Scatter Chart Matrix in XLMiner
To demonstrate the use of XLMiner to create a scatter chart matrix similar to Figure 3.27, 
we will use New York City subboroughs data.
Step 1.  Select any cell containing data (any cell in A2:E56)
Step 2.  Click the XLMINER tab in the Ribbon
Step 3.  Click Explore in the Data Analysis group
 
 
Click Chart Wizard
Step 4.  When the Chart Wizard dialog box appears, select Scatterplot Matrix
 
 
Click Next .
 
 
 In the Variable Selection Dialog, select Median Monthly Rent ($),  
  Percentage College Graduates (%), Poverty Rate (%), and  
Travel Time (min)
 
 
Click Finish
file
WEB
NYCityData
 
Appendix  Scatter Chart Matrix and Parallel Coordinates Plot in XLMiner 
119

120 
Chapter 3 Data Visualization
The completed scatter chart matrix produced by XLMiner is shown in Figure 3.33 
Compare this figure to Figure 3.27. We can infer the same relations between variables as in 
Figure 3.27. For example, median monthly rent increases as a percentage of college gradu-
ates increases. Median monthly rent decreases as the poverty rate increases. Notice that 
XLMiner provides a histogram (discussed in Chapter 2) along the diagonal of the scatter 
chart matrix and that each histogram shows the distribution of the data for the correspond-
ing variable. 
Charts produced using XLMiner are much more interactive than the standard charts 
produced in Excel. For the scatter chart matrix in Figure 3.33, we can easily change which 
variables are displayed by clicking the check boxes in the Filters area. We can also filter 
each variable by clicking and dragging the end points along the lines for each variable in 
the Filters box. This will change which data points are displayed in the scatter chart matrix. 
Only data points with values within the endpoints shown in the lines in the Filters area will 
be displayed.
Parallel Coordinates Plot in XLMiner
To demonstrate the use of XLMiner to create a parallel coordinates plot similar to Fig-
ure 3.29, we will use baseball player data.
Step 1.  Select any cell containing data (any cell in A1:D21)
Step 2.  Click the XLMINER tab in the Ribbon
file
WEB
BaseballData
0.3 Median Monthly Rent($)
Percentage College Graduates (%)
Poverty Rate (%)
Travel Time (min)
0.2
0.1
0
100
50
60
40
20
0
50
40
30
20
500
1000
1500
2000 0
20
40
60
80
0
10
20
30
40
50 20
30
40
50
0
FIGURE 3.33   SCATTER CHART MATRIX CREATED IN XLMINER FOR THE NEW YORK CITY DATA

Step 3.  Click Explore in the Data Analysis group
 
 
Click Chart Wizard
Step 4.  When the Chart Wizard dialog box appears, select Parallel Coordinates
 
 
Click Next .
Step 5.  In the Variable Selection Dialog, select HR, SB, AVG
 
 
Click Finish
Step 6.  When the Parallel Coordinates Plot opens, click Color by: in the top right 
corner of the chart
 
Select Position
The parallel coordinates plot created by XLMiner is shown in Figure 3.34. The lines 
corresponding to data for players at the 1B position are in the darker color; the lines corre-
sponding to data for players at the 2B position are in the lighter color. Like the scatter chart 
matrix, XLMiner allows for interactive analysis of data through the parallel coordinates 
plot; we can filter the positions shown by selecting and deselecting the check boxes for 
Position in the Filters area. We can also filter the quantitative variable values by using the 
slider bars for HR, SB, and AVG. To demonstrate this, adjust the left-hand slider bar handle 
for home runs (HR) to 20 (or just type 20 over the 0 on the left-hand side of HR). The result 
is shown in Figure 3.35: Only four players at 2B had more than 20 HRs. Interestingly, these 
four players also had more stolen bases (SB) than any 1B players, so these players were 
particularly good at combining speed and power.
 
Appendix  Scatter Chart Matrix and Parallel Coordinates Plot in XLMiner 
121
0
HR
1
SB
0.222
AVG
0.338
30
39
1B
2B
FIGURE 3.34   PARALLEL COORDINATES PLOT CREATED IN XLMINER FOR BASEBALL DATA

122 
Chapter 3 Data Visualization
0
HR
1
SB
0.222
AVG
0.338
30
39
1B
2B
FIGURE 3.35   PARALLEL COORDINATES PLOT FILTERED TO SHOW ONLY THOSE PLAYERS  
WITH 20 OR MORE HRS

Chapter 4
Linear regression
CONTENTS
4.1 
 THE SIMPLE LINEAR 
 REGRESSION MODEL
Regression Model and 
Regression Equation
Estimated Regression Equation
4.2 
LEAST SQUARES METHOD
Least Squares Estimates of the 
Regression Parameters
Using Excel’s Chart Tools 
to Compute the Estimated 
Regression Equation
4.3 
 ASSESSING THE FIT OF 
THE SIMPLE LINEAR 
 REGRESSION MODEL
The Sums of Squares
The Coefficient of Determination
Using Excel’s Chart Tools to 
Compute the Coefficient 
of Determination
4.4 
 THE MULTIPLE REGRESSION 
MODEL
Regression Model and 
Regression Equation
Estimated Multiple Regression 
Equation
Least Squares Method and 
Multiple Regression
Butler Trucking Company and 
Multiple Regression
Using Excel’s Regression Tool 
to Develop the Estimated 
Multiple Regression Equation
4.5 
 INFERENCE AND 
 REGRESSION
Conditions Necessary for Valid 
Inference in the Least Squares 
Regression Model
Testing for an Overall Regression 
Relationship
Testing Individual Regression 
Parameters
Addressing Nonsignificant 
Independent Variables
Multicollinearity
Inference and Very Large 
Samples
4.6 
 CATEGORICAL 
 INDEPENDENT VARIABLES
Butler Trucking Company and 
Rush Hour
Interpreting the Parameters
More Complex Categorical 
Variables
4.7 
 MODELING NONLINEAR 
RELATIONSHIPS
Quadratic Regression Models
Piecewise Linear Regression 
Models
Interaction Between Independent 
Variables
4.8 
MODEL FITTING
Variable Selection Procedures
Overfitting
 APPENDIX: 
 USING XLMINER 
FOR REGRESSION

124 
Chapter 4 Linear Regression
Alliance Data Systems (ADS) provides transaction 
processing, credit services, and marketing services for 
clients in the rapidly growing customer relationship 
management (CRM) industry. ADS clients are concen-
trated in four industries: retail, petroleum/convenience 
stores, utilities, and transportation. In 1983, Alliance 
began  offering end-to-end credit processing services 
to the retail, petroleum, and casual dining industries; 
today the company employs more than 6500 employ-
ees who provide services to clients around the world. 
Operating more than 140,000 point-of-sale terminals 
in the United States alone, ADS processes in excess of 
2.5 billion transactions annually. The company ranks 
second in the United States in private label credit ser-
vices by representing 49 private label programs with 
nearly 72 million cardholders. In 2001, ADS made an 
initial public offering and is now listed on the New 
York Stock Exchange.
As one of its marketing services, ADS designs 
 direct mail campaigns and promotions. With its data-
base containing information on the spending habits of 
more than 100 million consumers, ADS can target con-
sumers who are the most likely to benefit from a direct 
mail promotion. The Analytical Development Group 
uses regression analysis to build models that measure 
and predict the responsiveness of consumers to direct 
market campaigns. Some regression models predict the 
probability of purchase for individuals receiving a pro-
motion, and others predict the amount spent by consum-
ers who make purchases.
For one campaign, a retail store chain wanted to 
 attract new customers. To predict the effect of the 
campaign, ADS analysts selected a sample from the 
consumer database, sent the sampled individuals pro-
motional materials, and then collected transaction data 
on the consumers’ response. Sample data were col-
lected on the amount of purchase made by the consum-
ers responding to the campaign, as well as on a variety 
of  consumer-specific variables thought to be useful in 
predicting sales. The consumer-specific variable that 
contributed most to predicting the amount purchased 
was the total amount of credit purchases at related stores 
over the past 39 months. ADS analysts developed an 
 estimated regression equation relating the amount of 
purchase to the amount spent at related stores:
y^ 5 26.7 1 0.00205x
where
 
y^ 5 predicted amount of purchase
 
x 5 amount spent at related stores
Using this equation, we could predict that someone 
spending $10,000 over the past 39 months at  related 
stores would spend $47.20 when responding to the 
direct mail promotion. In this chapter, you will 
learn how to develop this type of estimated regres-
sion equation. The final model developed by ADS 
analysts also included several other variables that 
increased the predictive power of the preceding equa-
tion. Among these variables was the absence or pres-
ence of a bank credit card, estimated income, and the 
average amount spent per trip at a selected store. In 
this chapter, we will also learn how such additional 
variables can be incorporated into a multiple regres-
sion model.
ALLIANCE DATA SYSTEMS*
DALLAS, TEXAS
AnAlytics   in  Action
Alliance Data Systems analysts discuss use of a 
regression model to predict sales for a direct marketing 
campaign. © Courtesy of Alliance Data Systems.
*The authors are indebted to Philip Clemance, Director of Analytical 
 Development at Alliance Data Systems, for providing this Analytics in 
Action.

 
4.1 The Simple Linear Regression Model 
125
Managerial decisions are often based on the relationship between two or more variables. 
For example, after considering the relationship between advertising expenditures and sales, 
a marketing manager might attempt to predict sales for a given level of advertising expen-
ditures. In another case, a public utility might use the relationship between the daily high 
temperature and the demand for electricity to predict electricity usage on the basis of next 
month’s anticipated daily high temperatures. Sometimes a manager will rely on intuition 
to judge how two variables are related. However, if data can be obtained, a statistical pro-
cedure called regression analysis can be used to develop an equation showing how the 
variables are related.
In regression terminology, the variable being predicted is called the dependent vari-
able, or response, and the variables being used to predict the value of the dependent variable 
are called the independent variables, or predictor variables. For example, in analyzing the 
effect of advertising expenditures on sales, a marketing manager’s desire to predict sales 
would suggest making sales the dependent variable. Advertising expenditure would be the 
independent variable used to help predict sales. 
A regression analysis involving one independent variable and one dependent variable 
is referred to as a simple regression, and in statistical notation y denotes the dependent 
variable and x denotes the independent variable. A regression analysis for which any one 
unit change in the independent variable, x, is assumed to result in the same change in the 
dependent variable, y, is referred to as a linear regression. Regression analysis involving 
two or more independent variables is called multiple regression; multiple regression and 
cases involving curvilinear relationships are covered in later sections of this chapter.
the Simple Linear regression Model
Butler Trucking Company is an independent trucking company in southern California. A 
major portion of Butler’s business involves deliveries throughout its local area. To develop 
better work schedules, the managers want to estimate the total daily travel times for their 
drivers. The managers believe that the total daily travel times (denoted by y) are closely 
related to the number of miles traveled in making the daily deliveries (denoted by x). Using 
regression analysis, we can develop an equation showing how the dependent variable y is 
related to the independent variable x.
Regression Model and Regression Equation
In the Butler Trucking Company example, the population consists of all the driving assign-
ments that can be made by the company. For every driving assignment in the population, 
there is a value of x (miles traveled) and a corresponding value of y (travel time in hours). 
The equation that describes how y is related to x and an error term is called the regression 
model. The regression model used in simple linear regression follows:
SIMPLE LINEAR REGRESSION MODEL
 
y 5 b0 1 b1x 1 « 
(4.1)
b0 and b1 are characteristics of the population and so are referred to as the parameters of 
the model, and « (the Greek letter epsilon) is a random variable referred to as the error 
term. The error term accounts for the variability in y that cannot be explained by the linear 
relationship between x and y.
The statistical methods 
used in studying the rela-
tionship between two vari-
ables were first employed 
by Sir Francis Galton 
(1822–1911).  Galton 
found that the heights of 
the sons of unusually tall 
or unusually short fathers 
tend to move, or “regress,” 
toward the average height 
of the male population. 
Karl Pearson (1857–1936), 
a disciple of Galton, later 
confirmed this finding over 
a sample of 1078 pairs of 
fathers and sons.
4.1
A random variable is the 
outcome of a random 
experiment (such as the 
drawing of a random 
sample) and so represents 
an uncertain outcome.

126 
Chapter 4 Linear Regression
The equation that describes how the expected value of y, denoted E(y), is related to x is 
called the regression equation. The regression equation for simple linear regression follows:
 
E(y|x) 5 b0 1 b1x 
(4.2)
where E(y|x) is the expected value of y for a given value of x. The graph of the simple linear 
regression equation is a straight line; b0 is the y-intercept of the regression line, b1 is the 
slope, and E(y|x) is the mean or expected value of y for a given value of x.
Examples of possible regression lines are shown in Figure 4.1. The regression line 
in Panel A shows that the mean value of y is related positively to x, with larger values of  
E(y|x) associated with larger values of x. In Panel B, the mean value of y is related nega-
tively to x, with smaller values of E(y|x) associated with larger values of x. In Panel C, the 
mean value of y is not related to x; that is, E(y|x) is the same for every value of x.
Estimated Regression Equation
If the values of the population parameters b0 and b1 were known, we could use equa-
tion (4.2) to compute the mean value of y for a given value of x. In practice, the 
parameter values are not known and must be estimated using sample data. Sample 
statistics (denoted b0 and b1) are computed as estimates of the population parameters 
b0 and b1. Substituting the values of the sample statistics b0 and b1 for b0 and b1 in 
the regression equation, we obtain the estimated regression equation. The estimated 
regression equation for simple linear regression follows.
SIMPLE LINEAR REGRESSION ESTIMATED REGRESSION EQUATION
 
y^ 5 b0 1 b1x 
(4.3)
Figure 4.2 provides a summary of the estimation process for simple linear regression.
The graph of the estimated simple linear regression equation is called the estimated 
regression line; b0 is the estimated y-intercept, and b1 is the estimated slope. In the next 
section, we show how the least squares method can be used to compute the values of b0 and 
b1 in the estimated regression equation.
In general, y^ is the point estimator of E(y|x), the mean value of y for a given value of 
x. Thus, to estimate the mean or expected value of travel time for a driving assignment of 
75 miles, Butler trucking would substitute the value of 75 for x in equation (4.3). In some 
E(y|x) is often referred to 
as the conditional mean of 
y given the value of x.
FIGURE 4.1   POSSIBLE REGRESSION LINES IN SIMPLE LINEAR REGRESSION
E(y|x)
E(y|x)
E(y|x)
x
x
x
Regression line
Panel A: 
Positive Linear Relationship
Panel B: 
Negative Linear Relationship
Panel C: 
No Relationship
Regression line
Regression line

 
4.2 Least Squares Method 
127
cases, however, Butler Trucking may be more interested in predicting travel time for 
an upcoming driving assignment of a particular length. For example, suppose Butler 
Trucking would like to predict travel time for a new 75-mile driving assignment the 
company is considering. As it turns out, the best predictor of y for a given value of x is 
also provided by y^. Thus, to predict travel time for a new 75-mile driving assignment, 
Butler Trucking would also substitute the value of 75 for x in equation (4.3). The value 
of y^ provides both a point estimate of E(y|x) for a given value of x and a prediction of 
an individual value of y for a given value of x. In most cases, we will refer to y^ simply 
as the predicted value of y.
Least Squares Method
The least squares method is a procedure for using sample data to find the estimated 
 regression equation. To illustrate the least squares method, suppose data were collected 
from a sample of ten Butler Trucking Company driving assignments. For the ith observa-
tion or driving assignment in the sample, xi is the miles traveled and yi is the travel time 
(in hours). The values of xi and yi for the ten driving assignments in the sample are sum-
marized in Table 4.1. We see that driving assignment 1, with x1 5 100 and y1 5 9.3, is a 
driving assignment of 100 miles and a travel time of 9.3 hours. Driving assignment 2, with 
x2 5 50 and y2 5 4.8, is a driving assignment of 50 miles and a travel time of 4.8 hours. 
The shortest travel time is for driving assignment 5, which requires 50 miles with a travel 
time of 4.2 hours.
Figure 4.3 is a scatter chart of the data in Table 4.1. Miles traveled is shown on the 
horizontal axis, and travel time (in hours) is shown on the vertical axis. Scatter charts for 
regression analysis are constructed with the independent variable x on the horizontal axis 
and the dependent variable y on the vertical axis. The scatter chart enables us to observe 
A point estimator is a single 
value used as an estimate of 
the corresponding popula-
tion parameter.
4.2
FIGURE 4.2   THE ESTIMATION PROCESS IN SIMPLE LINEAR REGRESSION
E(y|x)
The estimation of b0 and 
b1 is a statistical process 
much like the estimation 
of the population mean, m, 
discussed in Chapter 2. b0 
and b1 are the unknown 
parameters of interest, and 
b0 and b1 are the sample 
statistics used to estimate 
the parameters.

128 
Chapter 4 Linear Regression
the data graphically and to draw preliminary conclusions about the possible relationship 
between the variables.
What preliminary conclusions can be drawn from Figure 4.3? Longer travel times 
 appear to coincide with more miles traveled. In addition, for these data, the relationship 
between the travel time and miles traveled appears to be approximated by a straight line; 
indeed, a positive linear relationship is indicated between x and y. We therefore choose 
the simple linear regression model to represent this relationship. Given that choice, our 
next task is to use the sample data in Table 4.1 to determine the values of b0 and b1 in the 
estimated simple linear regression equation. For the ith driving assignment, the estimated 
regression equation provides
 
y^i 5 b0 1 b1xi 
(4.4)
file
WEB
Butler
Driving  
Assignment i
x 5 Miles  
Traveled
y 5 Travel 
Time (hours)
1
2
3
4
5
6
7
8
9
10
100
 50
100
100
 50
 80
 75
 65
 90
 90
9.3
4.8
8.9
6.5
4.2
6.2
7.4
6.0
7.6
6.1
TablE 4.1   MILES TRAVELED AND TRAVEL TIME (IN HOURS) FOR TEN 
 BUTLER TRUCKING COMPANY DRIVING ASSIGNMENTS
FIGURE 4.3   SCATTER CHART OF MILES TRAVELED AND TRAVEL TIME IN 
HOURS FOR SAMPLE OF TEN BUTLER TRUCKING COMPANY 
DRIVING  ASSIGNMENTS
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x

 
4.2 Least Squares Method 
129
where
 
  y^i 5 predicted travel time (in hours) for the ith driving assignment
 
b0 5 the y-intercept of the estimated regression line
 
b1 5 the slope of the estimated regression line
 
  xi 5 miles traveled for the ith driving assignment
With yi denoting the observed (actual) travel time for driving assignment i and y^i in equa-
tion (4.4) representing the predicted travel time for driving assignment i, every driving 
assignment in the sample will have an observed travel time yi and a predicted travel time y^i . 
For the estimated regression line to provide a good fit to the data, the differences between 
the observed travel times yi and the predicted travel times y^i should be small.
The least squares method uses the sample data to provide the values of b0 and b1 that 
minimize the sum of the squares of the deviations between the observed values of the 
 dependent variable yi and the predicted values of the dependent variable y^i . The criterion 
for the least squares method is given by equation (4.5).
LEAST SQUARES METHOD EQUATION
 
mina
n
i51
(yi 2 y^i) 2 
(4.5)
where
 
yi 5 observed value of the dependent variable for the ith observation
 
y^i 5 predicted value of the dependent variable for the ith observation
 
 n 5 total number of observations
This is known as the least squares method for estimating the regression equation.
The error we make using the regression model to estimate the mean value of the depen-
dent variable for the ith observation is often written as ei 5 yi 2 y^i and is referred to as the 
ith residual. Using this notation, equation (4.5) can be rewritten as
 
mina
n
i51
ei
2 
and we say that we are finding the regression that minimizes the sum of squared errors.
Least Squares Estimates of the Regression Parameters
Differential calculus can be used to show that the values of b0 and b1 that minimize expres-
sion (4.5) are found by using equations (4.6) and (4.7).
SLOPE EQUATION
 
b1 5
a
n
i51
(xi 2 x) (yi 2 y)
a
n
i51
(xi 2 x) 2
 
(4.6)
y-INTERCEPT EQUATION
 
b0 5 y 2 b1x 
(4.7)

130 
Chapter 4 Linear Regression
where
 
xi 5 value of the independent variable for the ith observation
 
yi 5 value of the dependent variable for the ith observation
 
  x 5 mean value for the independent variable
 
  y 5 mean value for the dependent variable
 
 n 5 total number of observations
For the Butler Trucking Company data in Table 4.1, these equations yield an estimated slope 
of b15 0.0678 and a y-intercept of b0 5 1.2739. Thus, our estimated simple linear regression 
model is y^ 5 1.2739 1 0.0678x1 . Although equations (4.6) and (4.7) are not difficult to use, 
computer software such as Excel or XLMiner is generally used to calculate b1 and b0.
We interpret b1 and b0 as we would the y-intercept and slope of any straight line. The slope b1 
is the estimated change in the mean of the dependent variable y that is associated with a one unit 
increase in the independent variable x. For the Butler Trucking Company model, we therefore es-
timate that, if the length of a driving assignment were 1 unit (1 mile) longer, the mean travel time 
for that driving assignment would be 0.0678 units (0.0678 hours, or approximately 4 minutes) 
longer. The y-intercept b0 is the estimated value of the dependent variable y when the independent 
variable x is equal to 0. For the Butler Trucking Company model, we estimate that if the driving 
distance for a driving assignment was 0 units (0 miles), the mean travel time would be 1.2739 
units (1.2739 hours, or approximately 76 minutes). Can we find a plausible explanation for this? 
Perhaps the 76 minutes represent the time needed to prepare, load, and unload the vehicle, which 
is required for all trips regardless of distance and which therefore does not depend on the distance 
traveled. However, we must use caution: To estimate the travel time for a driving distance of 
0 miles, we have to extend the relationship we have found with simple linear regression well 
beyond the range of values for driving distance in our sample. Those sample values range from 
50 to 100 miles, and this range represents the only values of driving distance for which we have 
empirical evidence of the relationship between driving distance and our estimated travel time.
It is important to note that the regression model is valid only over the experimental 
region, which is the range of values of the independent variables in the data used to esti-
mate the model. Prediction of the value of the dependent variable outside the experimental 
region is called extrapolation and is risky. Because we have no empirical evidence that 
the relationship we have found holds true for values of x outside of the range of values of x 
in the data used to estimate the relationship, extrapolation is risky and should be avoided if 
possible. For VButler Trucking, this means that any prediction outside the travel time for a 
driving distance less than 50 miles or greater than 100 miles is not a reliable estimate, and 
so for this model the estimate of b0 is meaningless. However, if the experimental region for 
a regression problem includes zero, the y-intercept will have a meaningful interpretation.
We can now also use this model and our known values for miles traveled for a driving 
assignment (x) to estimate mean travel time in hours. For example, the first driving assign-
ment in Table 4.1 has a value for miles traveled of x 5 100. We estimate the mean travel 
time in hours for this driving assignment to be
 
y^i 5 1.2739 1 0.0678(100) 5 8.0539
Since the travel time for this driving assignment was 9.3 hours, this regression estimate 
would have resulted in a residual of
 
e1 5 y1 2 y^1 5 9.3 2 8.0539 5 1.2461
The simple linear regression model underestimated travel time for this driving assignment by 
1.2261 hours (approximately 74 minutes). Table 4.2 shows the predicted mean travel times, 
the residuals, and the squared residuals for all ten driving assignments in the sample data.
The estimated value of the 
y-intercept often results 
from extrapolation.

 
4.2 Least Squares Method 
131
Note in Table 4.2 that:
● 
 The sum of predicted values y^i is equal to the sum of the values of the dependent 
variable y.
● 
The sum of the residuals ei is 0.
● 
The sum of the squared residuals e2
i has been minimized.
These three points will always be true for a simple linear regression that is determined by 
equations (4.6) and (4.7). Figure 4.4 shows the simple linear regression line y^i 5 1.2739 1 
0.0678xi superimposed on the scatter chart for the Butler Trucking Company data in 
 Table 4.1. This figure, which also highlights the residuals for driving assignment 3 (e3) and 
driving assignment 5 (e5), shows that the regression model underpredicts travel time for 
some driving assignments (such as driving assignment 3) and overpredicts travel time for 
others (such as driving assignment 5), but in general appears to fit the data relatively well.
 Driving  
x 5 Miles 
y 5 Travel 
 Assignment i 
Traveled 
Time (hours) 
y^i 5 b0 1 b1xi 
ei 5 yi 2 y^i 
ei
2
 
 1 
100 
9.3 
8.0565 
1.2435 
1.5463
 
 2 
50 
4.8 
4.6652 
0.1348 
0.0182
 
 3 
100 
8.9 
8.0565 
0.8435 
0.7115
 
 4 
100 
6.5 
8.0565 
21.5565 
2.4227
 
 5 
50 
4.2 
4.6652 
20.4652 
0.2164
 
 6 
80 
6.2 
6.7000 
20.5000 
0.2500
 
 7 
75 
7.4 
6.3609 
1.0391 
1.0797
 
 8 
65 
6.0 
5.6826 
0.3174 
0.1007
 
 9 
90 
7.6 
7.3783 
0.2217 
0.0492
 
10 
90 
6.1 
7.3783 
21.2783 
1.6341
 
 
Totals 
67.0 
67.0000 
0.0000 
8.0288
TablE 4.2   PREDICTED TRAVEL TIME IN HOURS AND THE RESIDUALS FOR TEN 
 BUTLER TRUCKING COMPANY DRIVING ASSIGNMENTS
FIGURE 4.4   SCATTER CHART OF MILES TRAVELED AND TRAVEL TIME 
IN HOURS FOR BUTLER TRUCKING COMPANY DRIVING 
 ASSIGNMENTS WITH REGRESSION LINE SUPERIMPOSED
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x
e5
e3
y3
y3
^
5 1.2739 1 0.0678xi
yi^
y5
y5
^

132 
Chapter 4 Linear Regression
Note that if we square a residual, we obtain the area of a square with the length of each 
side equal to the absolute value of the residual. In other words, the square of the residual 
for driving assignment 4 (e4), 21.55652 5 2.4227, is the area of a square for which the 
length of each side is 1.5565. This relationship between the algebra and geometry of the 
least squares method provides interesting insight into what we are achieving by using this 
approach to fit a regression model.
In Figure 4.5, a vertical line is drawn from each point in the scatter chart to the linear 
regression line. Each of these lines represents the difference between the actual driving 
time and the driving time we predict using linear regression for one of the assignments in 
our data. The length of each line is equal to the absolute value of the residual for one of the 
driving assignments. When we square a residual, the resulting value is equal to the square 
that is formed using the vertical dashed line representing the residual in Figure 4.4 as one 
side of a square. Thus, when we find the linear regression model that minimizes the sum of 
squared errors for the Butler Trucking example, we are  positioning the regression line in 
the manner that minimizes the sum of the areas of the ten squares in Figure 4.5.
Using Excel’s Chart Tools to Compute the Estimated 
 Regression Equation
We can use Excel’s chart tools to compute the estimated regression equation on a scatter 
chart of the Butler Trucking Company data in Table 4.1. After constructing a scatter chart 
(as shown in Figure 4.3) with Excel’s chart tools, the following steps describe how to com-
pute the estimated regression equation using the data in the worksheet: 
Step 1. Right-click on any data point in the scatter chart and select Add Trendline . . .
Step 2. When the Format Trendline task pane appears:
 
Select Linear in the Trendline Options area
 
Select Display Equation on chart in the Trendline Options area
The worksheet displayed in Figure 4.6 shows the original data, scatter chart, estimated 
regression line, and estimated regression equation. Note that Excel uses y instead of y^ to 
denote the predicted value of the dependent variable and puts the regression equation into 
slope-intercept form whereas we use the intercept-slope form that is standard in statistics.
In versions of Excel prior to 
2013, Step 1 will open the 
Format Trendline dialog 
box where you can select 
Linear under Trend/ 
Regression Type.
FIGURE 4.5   A GEOMETRIC INTERPRETATION OF THE LEAST SQUARES  METHOD 
APPLIED TO THE BUTLER TRUCKING COMPANY EXAMPLE
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x
5 1.2739 1 0.0678xi
yi^

 
4.3 Assessing the Fit of the Simple Linear  Regression Model 
133
 assessing the Fit of the Simple Linear 
 regression Model
For the Butler Trucking Company example, we developed the estimated regression equation 
y^i 5 1.2739 1 0.0678xi to approximate the linear relationship between the miles traveled x 
and travel time in hours y. We now wish to assess how well the estimated regression equa-
tion fits the sample data. We begin by developing the intermediate calculations, referred to 
as sums of squares.
4.3
FIGURE 4.6   EXCEL SPREADSHEET WITH SCATTER CHART, ESTIMATED  REGRESSION LINE, AND 
ESTIMATED REGRESSION EQUATION FOR  BUTLER TRUCKING COMPANY
A
Assignment
Miles
Time
B
C
D
E
F
G
H
I
J
K
L
1
1
2
3
4
5
6
7
8
9
10
11
2
3
4
5
6
7
8
9
10
100
50
100
100
50
80
75
65
90
90
9.3
4.8
8.9
6.5
4.2
6.2
7.4
6.0
7.6
6.1
12
13
14
15
16
17
18
19
20
21
22
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x
y   5 0.0678x 1 1.2739
NOTES AND COMMENTS
 Equation 4.5 minimizes the sum of the squared 
deviations between the observed values of the de-
pendent variable yi and the predicted values of the 
dependent variable y^i . One alternative is to simply 
minimize the sum of the deviations between the 
observed values of the dependent variable yi and 
the predicted values of the dependent variable y^i . 
This is not a viable option because then negative 
deviations (observations for which the regression 
forecast exceeds the actual value) and positive 
deviations (observations for which the regression 
forecast is less than the actual value) offset each 
other. Another alternative is to minimize the sum 
of the absolute value of the deviations between the 
observed values of the dependent variable yi and 
the predicted values of the dependent variable y^i. It 
is possible to compute estimated regression param-
eters that minimize this sum of absolute value of 
the deviations, but this approach is more difficult 
than the least squares approach.

134 
Chapter 4 Linear Regression
The Sums of Squares
Recall that we found our estimated regression equation for the Butler Trucking Company 
example by minimizing the sum of squares of the residuals. This quantity, also known as 
the sum of squares due to error, is denoted by SSE.
SUM OF SQUARES DUE TO ERROR
 
SSE 5 a
n
i51
(yi 2 y^i) 2 
(4.8)
The value of SSE is a measure of the error in using the estimated regression equation to 
predict the values of the dependent variable in the sample.
We have already shown the calculations required to compute the sum of squares due to 
error for the Butler Trucking Company example in Table 4.2. The squared residual or error 
for each observation in the data is shown in the last column of that table. After computing and 
squaring the residuals for each driving assignment in the sample, we sum them to obtain SSE 
5 8.0288. Thus, SSE 5 8.0288 measures the error in using the estimated regression equa-
tion y^i 5 1.2739 1 0.0678xi to predict travel time for the driving assignments in the sample.
Now suppose we are asked to predict travel time in hours without knowing the miles 
traveled for a driving assignment. Without knowledge of any related variables, we would 
use the sample mean y as a predictor of travel time for any given driving assignment. To 
find y, we divide the sum of the actual driving times yi from Table 4.2 (67) by the number 
of observations n in the data (10); this yields y 5 6.7. 
Figure 4.7 provides insight on how well we would predict the values of yi in the Butler 
Trucking company example using y 5 6.7. From this figure, which again highlights the 
residuals for driving assignments 3 and 5, we can see that y tends to overpredict travel times 
for driving assignments that have relatively small values for miles traveled (such as driving 
assignment 5) and tends to underpredict travel times for driving assignments have relatively 
large values for miles traveled (such as driving assignment 3). 
In Table 4.3 we show the sum of squared deviations obtained by using the sample 
mean y 5 6.7 to predict the value of travel time in hours for each driving assignment in 
the sample. For the ith driving assignment in the sample, the difference yi 2 y provides a 
FIGURE 4.7   THE SAMPLE MEAN y AS A PREDICTOR OF TRAVEL TIME 
IN HOURS FOR BUTLER TRUCKING COMPANY
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x
y5 
y 
y5 2 y 
y3 2 y
y3

 
4.3 Assessing the Fit of the Simple Linear  Regression Model 
135
measure of the error involved in using y to predict travel time for the ith driving assign-
ment. The corresponding sum of squares, called the total sum of squares, is denoted SST.
TOTAL SUM OF SQUARES, SST
 
SST 5 a
n
i51
(yi 2 y) 2 
(4.9)
The sum at the bottom of the last column in Table 4.3 is the total sum of squares for Butler 
Trucking Company: SST 5 23.9.
Now we put it all together. In Figure 4.8 we show the estimated regression line y^i 5 
1.2739 1 0.0678xi and the line corresponding to y 5 6.7. Note that the points cluster more 
closely around the estimated regression line y^i 5 1.2739 1 0.0678xi than they do about the 
horizontal line y 5 6.7. For example, for the third driving assignment in the sample, we see 
FIGURE 4.8   DEVIATIONS ABOUT THE ESTIMATED REGRESSION LINE 
AND THE LINE y 5 y FOR THE THIRD BUTLER TRUCKING 
 COMPANY  DRIVING  ASSIGNMENT
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x
y   5 0.0678x 1 1.2739
y3 2 y
y3
y3
^
y
y3 2 y3
^
y3 2 y
^
TablE 4.3   CALCULATIONS FOR THE SUM OF SQUARES TOTAL FOR THE BUTLER 
TRUCKING SIMPLE LINEAR REGRESSION
Driving 
 Assignment i 
x 5 Miles  
Traveled
y 5 Travel  
Time (hours)
yi 2 y
(yi 2 y)2
1
2
3
4
5
6
7
8
9
10
100
50
100
100
50
80
75
65
90
90
  Totals
9.3
4.8
8.9
6.5
4.2
6.2
7.4
6.0
7.6
6.1
67.0
2.6
21.9
2.2
20.2
22.5
20.5
0.7
20.7
0.9
2.6
  0
6.76
3.61
4.84
0.04
6.25
0.25
0.49
0.49
0.81
6.76
23.9

136 
Chapter 4 Linear Regression
that the error is much larger when y 5 6.7 is used to predict y3 than when y^i 5 1.2739 1 
0.0678xi is used. We can think of SST as a measure of how well the observations cluster 
about the y line and SSE as a measure of how well the observations cluster about the y^ line.
To measure how much the y^ values on the estimated regression line deviate from y^, 
another sum of squares is computed. This sum of squares, called the sum of squares due to 
regression, is denoted SSR.
SUM OF SQUARES DUE TO REGRESSION, SSR
 
SSR 5 a
n
i51
(y^i 2 y) 2 
(4.10)
From the preceding discussion, we should expect that SST, SSR, and SSE are related. 
Indeed, the relationship among these three sums of squares is:
 
SST 5 SSR 1 SSE 
(4.11)
where
SST 5 total sum of squares
SSR 5 sum of squares due to regression
SSE 5 sum of squares due to error
Equation (4.11) shows that the total sum of squares can be partitioned into two compo-
nents, the sum of squares due to regression and the sum of squares due to error. Hence, if 
the values of any two of these sum of squares are known, the third sum of squares can be 
computed easily. For instance, in the Butler Trucking Company example, we already know 
that SSE 5 8.0288 and SST 5 23.9; therefore, solving for SSR in equation (4.11), we find 
that the sum of squares due to regression is
SSR 5 SST 2 SSE 5 23.9 2 8.0288 5 15.8712
The Coefficient of Determination
Now let us see how the three sums of squares, SST, SSR, and SSE, can be used to provide 
a measure of the goodness of fit for the estimated regression equation. The estimated re-
gression equation would provide a perfect fit if every value of the dependent variable yi 
happened to lie on the estimated regression line. In this case, yi 2 y^ would be zero for each 
observation, resulting in SSE 5 0. Because SST 5 SSR 1 SSE, we see that for a perfect 
fit SSR must equal SST, and the ratio (SSR/SST) must equal one. Poorer fits will result in 
larger values for SSE. Solving for SSE in equation (4.11), we see that SSE 5 SST 2 SSR. 
Hence, the largest value for SSE (and hence the poorest fit) occurs when SSR 5 0 and 
SSE 5 SST. The ratio SSR/SST, which will take values between zero and one, is used to 
evaluate the goodness of fit for the estimated regression equation. This ratio is called the 
coefficient of determination and is denoted by r 2.
COEFFICIENT OF DETERMINATION
 
r2 5 SSR
SST 
(4.12)
For the Butler Trucking Company example, the value of the coefficient of determination is
 
r2 5 SSR
SST 5 15.8712
23.9
5 0.6641
When we express the coefficient of determination as a percentage, r2 can be interpreted 
as the percentage of the total sum of squares that can be explained by using the estimated
In simple regression, r 2 
is often referred to as 
the simple coefficient of 
 determination. 
The coefficient of determi-
nation r 2 is the square of 
the correlation between the 
yi and y^ and 0 # r 2 # 1

 
4.3 Assessing the Fit of the Simple Linear  Regression Model 
137
regression equation. For Butler Trucking Company, we can conclude that 66.41 percent 
of the total sum of squares can be explained by using the estimated regression equation 
y^i 5 1.2739 1 0.0678xi to predict quarterly sales. In other words, 66.41 percent of the 
variability in the values of travel time in our sample can be explained by the linear rela-
tionship between the miles traveled and travel time.
Using Excel’s Chart Tools to Compute  
the Coefficient of Determination
In Section 4.1 we used Excel’s chart tools to construct a scatter chart and compute the es-
timated regression equation for the Butler Trucking Company data. We will now describe 
how to compute the coefficient of determination using the scatter chart in Figure 4.3.
Step 1. Right-click on any data point in the scatter chart and select Add Trendline. . . 
Step 2. When the Format Trendline task pane appears:
 
Select Display R-squared value on chart in the Trendline Options area
Figure 4.9 displays the scatter chart, the estimated regression equation, the graph of the 
estimated regression equation, and the coefficient of determination for the Butler Trucking 
Company data. We see that r2 5 0.6641.
Note that Excel uses R2 to 
represent the coefficient of 
determination.
NOTES AND COMMENTS
As a practical matter, for typical data in the social 
and behavioral sciences, values of r2 as low as 0.25 
are often considered useful. For data in the physi-
cal and life sciences, r2 values of 0.60 or greater are 
often found; in fact, in some cases, r2 values greater 
than 0.90 can be found. In business applications, r2 
values vary greatly, depending on the unique char-
acteristics of each application.
FIGURE 4.9   EXCEL SPREADSHEET WITH ORIGINAL DATA, SCATTER CHART,  ESTIMATED 
 REGRESSION LINE, ESTIMATED REGRESSION EQUATION, AND COEFFICIENT OF 
 DETERMINATION r 2 FOR BUTLER TRUCKING COMPANY
A
Assignment
Miles
Time
B
C
D
E
F
G
H
I
J
K
L
1
2
3
4
5
6
7
8
9
10
100
50
100
100
50
80
75
65
90
90
9.3
4.8
8.9
6.5
4.2
6.2
7.4
6.0
7.6
6.1
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
10
9
8
7
6
5
4
3
2
1
040
50
60
70
80
90
100
Travel Time (hours) - y
Miles Traveled - x
y   5 0.0678x 1 1.2739
R2 5 0.6641

138 
Chapter 4 Linear Regression
the Multiple regression Model
We now extend our discussion to the study of how a dependent variable y is related to two 
or more independent variables. 
Regression Model and Regression Equation
The concepts of a regression model and a regression equation introduced in the preceding 
sections are applicable in the multiple regression case. We will use q to denote the number 
of independent variables in the regression model. The equation that describes how the de-
pendent variable y is related to the independent variables x1, x2, . . . , xq and an error term 
is called the multiple regression model. We begin with the assumption that the multiple 
regression model takes the following form.
MULTIPLE REGRESSION MODEL
 
y 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq 1  
(4.13)
In the multiple regression model, b0, b1, b2, . . . , bq are the parameters and the error 
term  is a random variable. A close examination of this model reveals that y is a linear 
function of x1, x2, . . . , xq plus the error term . As in simple regression, the error term 
accounts for the variability in y that cannot be explained by the linear effect of the q 
independent variables. The interpretation of the y-intercept b0 in multiple regression is 
similar to the interpretation in simple regression; in a multiple regression model, b0 is 
the mean of the dependent variable y when all of the independent variables x1, x2, . . . , 
xq are equal to zero. On the other hand, the interpretation of the slope coefficients b1, 
b2, . . . , bq in a multiple regression model differ in a subtle but important way from 
the interpretation of the slope b1 in a simple regression model. In a multiple regression 
model the slope coefficient bj represents the change in the mean value of the dependent 
variable y that corresponds to a one unit increase in the independent variable xj, holding 
the values of all other independent variables in the model constant. Thus, in a multiple 
regression model, the slope coefficient b1 represents the change in the mean value of 
the dependent variable y that corresponds to a one-unit increase in the independent vari-
able x1, holding the values of x2, x3, . . . , xq constant. Similarly, the slope coefficient b2 
represents the change in the mean value of the dependent variable y that corresponds to 
a one-unit increase in the independent variable x2, holding the values of x1, x3, . . . , xq 
constant., and so on.
The multiple regression equation that describes how the mean value of y is related to 
x1, x2, . . . , xq is given in equation (4.14).
 
E( y | x1, x2, . . . , xq) 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq 
(4.14) 
Estimated Multiple Regression Equation
If the values of b0, b1, b2, . . . , bq were known, equation (4.13) could be used to compute 
the mean value of y at given values of x1, x2, . . . , xq. Unfortunately, these parameter values 
will not, in general, be known and so must be estimated from sample data. A simple random 
sample is used to compute sample statistics b0, b1, b2, . . . , bq that are then used as the point 
estimators of the parameters b0, b1, b2, . . . , bq. These sample statistics provide the follow-
ing estimated multiple regression equation.
4.4

 
4.4 The Multiple Regression Model 
139
ESTIMATED MULTIPLE REGRESSION EQUATION
 
y^ 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq 
(4.15)
where
b0, b1, b2, . . . , bq 5 the point estimates of b0, b1, b2, . . . , bq
y^ 5 estimated value of the dependent variable
Least Squares Method and Multiple Regression
As with simple linear regression, in multiple regression we wish to find a model that results 
in small errors over the sample data. We continue to use the least squares method to develop 
the estimated multiple regression equation; that is, we find b0, b1, b2, . . . , bq that satisfy
 
mina
n
i51
(yi 2 y^i) 2 5 mina
n
i51
e2
i
The estimation process for multiple regression is shown in Figure 4.10.
The estimated values of the dependent variable y are computed by substituting values 
of the independent variables x1, x2, . . . , xq into the estimated multiple regression equation.
y^ 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq
The least squares method uses sample data to provide the values of b0, b1, b2, . . . , bq that 
minimize the sum of squared residuals (the deviations between the observed values of the 
dependent variable yi and the estimated values of the dependent variable y^ ).
FIGURE 4.10   THE ESTIMATION PROCESS FOR MULTIPLE REGRESSION
Sample Data
x1
x2
xq
y
…
provide the estimates of
0, 1, 2,…,q 
b0, b1, b2,…,bq
Multiple Regression
Model
Multiple Regression Equation
y 5 0 1 1x1 1 2x2 1…1 qxq 1 e
unknown parameters.
E(y|x1, x2,…,xq) 5 0 1 1x1 1 2x2 1…1 qxq
0, 1, 2,…,q are 
Compute the Estimated
Multiple Regression
Equation
y  5 b0 1 b1x1 1 b2x2 1…1bqxq
b0, b1, b2,…,bq are
sample statistics.
ˆ

140 
Chapter 4 Linear Regression
Earlier in this chapter, we presented formulas in equations 4.6 and 4.7 for comput-
ing the least squares estimators b0 and b1 for the estimated simple linear regression 
equation y^ 5 b0 1 b1x1. With relatively small data sets, we were able to use those for-
mulas to compute b0 and b1 by means of manual calculations. In multiple regression, 
however, the presentation of the formulas for the regression coefficients b0, b1, b2 , . . . , 
bq involves the use of matrix algebra and is beyond the scope of this text. Therefore, 
in presenting multiple regression, we focus on how computer software packages can 
be used to obtain the estimated regression equation and other information. The empha-
sis will be on how to interpret a regression model rather than on how to compute the 
 parameter estimates for multiple regression models.
Butler Trucking Company and Multiple Regression 
As an illustration of multiple regression analysis, recall that a major portion of Butler 
Trucking Company’s business involves deliveries throughout its local area and that the 
managers want to estimate the total daily travel time for their drivers in order to develop 
better work schedules for the company’s drivers.
Initially the managers believed that the total daily travel time would be closely related 
to the number of miles traveled in making the daily deliveries. A simple random sample of 
ten driving assignments provided the data in Table 4.1 and in the scatter chart in Figure 4.3. 
After reviewing the scatter chart, the managers hypothesized that the simple linear regres-
sion model y^ 5 b0 1 b1x could be used to describe the relationship between the total travel 
time y and the number of miles traveled x. Using the scatter chart in Excel’s chart tools, 
we found that the estimated simple linear regression equation for our sample data is y^i 5 
1.2739 1 0.0678xi. With a coefficient of determination r 2 5 0.6641, the linear effect of 
the number of miles traveled explains 66.41 percent of the variability in travel time in the 
sample data, and so 33.59 percent of the variability in sample travel times remains unex-
plained. This result suggests to Butler’s managers that other factors may contribute to the 
travel times for driving assignments. The managers might want to consider adding one or 
more independent variables to the model to explain some of the remaining variability in 
the dependent variable.
In considering other independent variables for their model, the managers felt that the 
number of deliveries made on a driving assignment also contributes to the total travel time. 
To support the development of a multiple regression model that includes both the number 
of miles traveled and the number of deliveries, they augment their original data with infor-
mation on the number of deliveries for the ten driving assignments in the original data and 
they collect new observations over several ensuing weeks. The new data, which consist of 
300 observations, are provided in the file ButlerWithDeliveries. Note that we now refer to 
the independent variables miles traveled as x1 and the number of deliveries as x2.
Our multiple linear regression with two independent variables will take the form y^ 5 
b0 1 b1x1 1 b2x2. The SST, SSR, and SSE are again calculated using equations (4.9), (4.10), 
and (4.11), respectively. Thus, the coefficient of determination, which in multiple regres-
sion is denoted R2, is again calculated using equation (4.12). We will now use Excel’s 
Regression tool to calculate the values of the estimates b0, b1, b2, and R2.
Using Excel’s Regression Tool to Develop the Estimated 
Multiple Regression Equation
The following steps describe how to use Excel’s Regression tool to compute the estimated 
regression equation using the data in the worksheet.
Step 1. Click the DATA tab in the Ribbon
Step 2. Click Data Analysis in the Analysis group
file
WEB
ButlerWithDeliveries
In multiple regression, 
R2 is often referred to as 
the multiple coefficient of 
determination.
When using Excel’s 
 Regression tool, the data 
for the independent vari-
ables must be in adjacent 
columns or rows. Thus, you 
may have to rearrange the 
data in order to use Excel 
to run a particular multiple 
regression.

 
4.4 The Multiple Regression Model 
141
Step 3.  Select Regression from the list of Analysis Tools in the Data Analysis tools 
box (shown in Figure 4.11) and click OK
Step 4.  When the Regression dialog box appears (as shown in Figure 4.12):
 
Enter D1:D301 in the Input Y Range: box
 
Enter B1:C301 in the Input X Range: box
 
Select Labels
Selecting Labels tells Excel to use the names you have given to your variables in Row 1 
when displaying the regression model output.
Select Confidence Level:
Enter 99 in the Confidence Level: box
Select New Worksheet Ply:
Click OK
Selecting New Worksheet 
Ply: tells Excel to place the 
output of the regression 
analysis in a new work-
sheet. In the adjacent box, 
you can specify the name 
of the worksheet where the 
output is to be placed, or 
you can leave this blank 
and allow Excel to create a 
new worksheet to use as the 
destination for the results 
of this regression analysis 
(as we are doing here).
If Data Analysis does not 
appear in your Analysis 
group, you will have to 
load the Analysis ToolPak 
Add-in into Excel. To do so, 
click the FILE tab in the 
Ribbon, and click Options. 
When the Excel Options 
dialog box appears, click 
Add-Ins from the menu. 
Next to Manage:, select 
Excel Add-ins, and click 
Go. . . at the bottom of the 
dialog box. When the Add-
Ins dialog box appears, 
select Analysis ToolPak 
and click OK.
FIGURE 4.11  DATA ANALYSIS TOOLS BOX
FIGURE 4.12  REGRESSION DIALOG BOX

142 
Chapter 4 Linear Regression
In the Excel output shown in Figure 4.13, the label for the independent variable x1 is 
Miles (see cell A18), and the label for the independent variable x2 is Deliveries (see cell 
A19). The estimated regression equation is
 
y^ 5 0.1273 1 0.0672x1 1 0.6900x2 
(4.16)
We interpret this model in the following manner:
● 
 For a fixed number of deliveries, we estimate that the mean travel time will increase 
by 0.0672 hours when the distance traveled increases by 1 mile.
● 
 For a fixed distance traveled, we estimate that the mean travel time will increase by 
0.69 hours when the number of deliveries increases by 1 delivery.
The interpretation of the estimated y-intercept for this model (the expected mean travel time 
for a driving assignment with a distance traveled of 0 and no deliveries) does not make sense 
because it is the result of extrapolation.
This model has a multiple coefficient of determination of R2 5 0.8173. By adding the 
independent variable, number of deliveries, to our original simple linear regression, we now 
explain 81.73 percent of the variability in our sample values of the dependent variable, travel 
time. Since the simple linear regression with miles traveled as the sole independent variable 
explained 66.41 percent of the variability in our sample values of travel time, we can see that 
adding number of deliveries as an independent variable to our regression model resulted in 
explaining an additional 15.32 percent of the variability in our sample values of travel time. 
The addition of the number of deliveries to the model appears to have been worthwhile.
With two independent variables x1 and x2, we now generate a predicted value of y for 
every combination of values of x1 and x2. Thus, instead of a regression line, we now create 
a regression plane in three-dimensional space. Figure 4.14 provides the graph of the esti-
mated regression plane for the Butler Trucking Company example and shows the seventh 
driving assignment in the data. Note that (1) as the plane slopes upward to larger values 
The sum of squares due to 
error, SSE, cannot become 
larger (and generally will 
become smaller) when inde-
pendent variables are added 
to a regression model. 
Because SSR – SST 5  
SSE, the SSR cannot become 
smaller (and generally 
becomes larger) when an in-
dependent variable is added 
to a regression model. Thus, 
R2 5 SSR/SST can never 
decrease as independent 
variables are added to the 
regression model.
FIGURE 4.13   EXCEL SPREADSHEET WITH RESULTS FOR THE BUTLER TRUCKING COMPANY 
 MULTIPLE REGRESSION WITH MILES AND DELIVERIES AS INDEPENDENT VARIABLES
Regression Statistics
Multiple R
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0% Upper 99.0%
2
915.5160626 457.7580313
0.68884558
664.5292419
2.2419E-110
204.5871374
1120.1032
0.90407397
0.817349743
0.816119775
0.829967216
300
297
299
0.127337137
0.20520348 0.620540826
0.53537766 –0.276499931 0.531174204 –0.404649592
0.06081725
0.613465414
0.659323866
0.073546235
0.766531147
0.072013099
0.748095234
0.062350385
0.631901326
3.5398E-83
2.84826E-69
27.36551071
23.37308852
0.002454979
0.029521057
0.067181742
0.68999828
SS
MS
F
Signiﬁcance F
Regression
Residual
Total
Intercept
Miles
Deliveries
A
B
C
D
E
F
G
H
I
SUMMARY OUTPUT
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

 
4.5 Inference and Regression 
143
of total travel time (y) as either the number of miles traveled (x1) or the number of deliver-
ies (x2) increases and (2) the residual for a driving assignment when x1 5 x*
1 and x2 5 x*
2 
is the difference between the actual y value and the expected value of y when x1 5 x*
1 and 
x2 5 x*
2, or E( y | x*
1, x*
2). Note that in Figure 4.14, the observed value lies above the regres-
sion plane, indicating that the regression model underestimates the expected driving time 
for the seventh driving assignment.
NOTES AND COMMENTS
Although we use regression analysis to estimate re-
lationships between independent variables and the 
dependent variable, it does not provide information 
on whether these are cause-and- effect relation-
ships. The analyst can conclude that a cause-and-
effect relationship exists between an independent 
variable and the dependent only if there is a theo-
retical justification that the relationship is in fact 
causal. In the Butler Trucking Company multiple 
regression, through regression analysis we have 
found evidence of a relationship between distance 
traveled and travel time and evidence of a rela-
tionship between number of deliveries and travel 
time. Nonetheless, we cannot conclude from the 
regression model that changes in distance traveled 
x1 cause changes in travel time y, and we cannot 
conclude that changes in number of deliveries x2 
cause changes in travel time y. The appropriate-
ness of such cause-and-effect conclusions are left 
to supporting practical justification and to good 
judgment on the part of the analyst. Based on their 
practical experience, Butler Trucking’s managers 
felt that increases in distance traveled and number 
of deliveries were likely causes of increased travel 
time. However, it is important to realize that the re-
gression model itself provides no information about 
cause-and-effect relationships.
Inference and regression
The statistics b0, b1, b2, . . ., bq are point estimators of the population parameters b0, b1, 
b2, . . . , bq; that is, each of these q 1 1 estimates is a single value used as an estimate 
of the corresponding population parameter. By using the point estimators b0, b1, b2, . . . , bq 
as estimators of the population parameters b0, b1, b2, . . . , bq, we avoid the costs as-
sociated with taking a census of a population. Similarly, we use y^ as a point estimator of  
4.5
FIGURE 4.14   GRAPH OF THE REGRESSION EQUATION FOR MULTIPLE 
 REGRESSION ANALYSIS WITH TWO INDEPENDENT VARIABLES
y7
y7
8
6
4
2
3
4
50
Miles
Number of Deliveries
Delivery Time
75
100
e7
^

144 
Chapter 4 Linear Regression
E( y | x1, x2, . . . , xq). However, we must recognize that the savings in time and cost asso-
ciated with using point estimators based on sample data comes at a price; only through a 
census can the value of a parameter be determined with certainty. 
Samples do not replicate the population exactly. Different samples taken from the same 
population will result in different values of the point estimators b0, b1, b2, . . . , bq; that is, the 
point estimators have variability. If the values of a point estimator such as b0, b1, b2, . . . , bq 
change relatively little from sample to sample, the point estimator has low variability, and 
so the value of the point estimator that we calculate based on a random sample will likely 
be a reliable estimate of the population parameter. On the other hand, if the values of a point 
estimator change dramatically from sample to sample, the point estimator has high vari-
ability, and so the value of the point estimator that we calculate based on a random sample 
will likely be a less reliable estimate. How confident can we be in the estimates b0, b1, and 
b2 that we developed for the Butler Trucking multiple regression model? Do these estimates 
have little variation and so are relatively reliable, or do they have so much variation that 
they have little meaning? We address the variability in potential values of the estimators 
through use of statistical inference.
Statistical inference is the process of making estimates and drawing conclusions about 
one or more characteristics of a population (the value of one or more parameters) through 
the analysis of sample data drawn from the population. In regression, we commonly use 
inference to estimate and draw conclusions about:
● 
The regression parameters b0, b1, b2, . . . , bq.
● 
 The mean value and/or the predicted value of the dependent variable y for specific 
values of the independent variables x*
1, x*
2 , . . . , x*
q, E( y* | x*
1, x*
2 , . . . , x*
q).
In our discussion of inference and regression, we will consider both hypothesis  testing 
and interval estimation.
Conditions Necessary for Valid Inference in the Least 
Squares Regression Model
In conducting a regression analysis, we begin by making an assumption about the appropri-
ate model for the relationship between the dependent and independent variable(s). For the 
case of linear regression, the assumed multiple regression model is
 
y 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq1 
The least squares method is used to develop values for b1, b2, . . . , bq, the estimates of the 
model parameters b0, b1, b2, . . . , bp, respectively. The resulting estimated multiple regres-
sion equation is
 
y^ 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq
Although inference can provide greater understanding of the nature of relationships esti-
mated through regression analysis, our inferences are valid only if the error term  behaves 
in a certain way. Specifically, the validity of inferences in regression analysis depends on 
how well the following two conditions about the error term  are met:
 1.  For any given combination of values of the independent variables x1, x2, . . . , xq, the 
population of potential error terms  is normally distributed with a mean of 0 and a 
constant variance.
 2. The values of  are statistically independent.
The practical implication of normally distributed errors with a mean of zero and a 
 constant variation for any given combination of values of x1, x2, . . . , xq is that the regres-
sion estimates are unbiased (i.e., do not tend to over- or underpredict), possess consistent 
accuracy, and tend to err in small amounts rather than in large amounts. The first condi-
tion must be met for statistical inference in regression to be valid. The second condition is 
A normally distributed 
variable has values that are 
symmetric about the mean. 
Values close to the mean are 
expected to occur most fre-
quently, and values farther 
from the mean are expected 
to occur less frequently. This 
is what many refer to as the 
bell-shaped curve.

 
4.5 Inference and Regression 
145
 generally of concern only when we collect data from a single entity over several periods 
of time and must also be met for statistical inference in regression to be valid in these in-
stances. However, inferences in regression are generally reliable unless there are marked 
violations of these conditions.
Figure 4.15 illustrates these model conditions and their implications for a simple 
linear regression; note that in this graphical interpretation, the value of E(y|x) changes 
linearly according to the specific value of x considered, and so the mean error is zero 
at each value of x. However, regardless of the x value, the error term  and hence the 
dependent variable y are normally distributed, each with the same variance. The specific 
value of the error  at any particular point depends on whether the actual value of y is 
greater or less than E(y|x).
There are many sophisticated diagnostic procedures for detecting violations of these 
conditions, but simple scatter charts of the residuals and independent variables are an ex-
tremely effective method for assessing whether these conditions are violated. We should 
review the scatter chart for patterns in the residuals indicating that one or more of the con-
ditions have been violated. Ideally, the residuals will be consistently scattered around zero 
throughout the predicted values of the independent variable. (At any given value of x, the 
center of the residuals is approximately zero, the spread in the errors is similar to the spread 
in error for other values of x, and the errors are symmetrically distributed with values near 
zero occurring more frequently than values that differ greatly from zero.) This is shown in 
the example in Figure 4.16. A pattern in the residuals such as this gives us little reason to 
doubt the validity of inferences made on the regression that generated the residuals.
Keep in mind that we are 
also making an  assumption 
or hypothesis about the 
form of the relationship 
 between x and y. We  
assume that a straight line 
represented by b0 1 b1x is 
the basis for the relation-
ship between the variables. 
We must not lose sight 
of the fact that some other 
model, for instance  
y 5 b0 1 b1x1 1 b2x2 1 «,  
may actually provide a 
better representation for 
the underlying population 
relationship.
FIGURE 4.15   ILLUSTRATION OF THE CONDITIONS FOR VALID INFERENCE 
IN  REGRESSION
E(y) when
x = 30
x = 30
x = 20
x = 10
x = 0
Distribution of
y at x = 30
Distribution of
y at x = 20
Distribution of
y at x = 10
0
y
0 +
1x
E(y|x) =
x
Note: The y distributions have the
same shape at each x value.
E(y) when
x = 20
E(y) when
x = 10
E(y) when
 x = 0

146 
Chapter 4 Linear Regression
Although the residuals in Figure 4.16 show no discernible pattern, the residuals in the 
four panels of Figure 4.17 show distinct patterns, each of which suggests a violation of at 
least one of the regression model conditions. Figure 4.17 shows plots of residuals from 
four different regressions, each showing a different pattern. In panel a, the variation in the 
residuals e increases as the value of the independent variable x increases, suggesting that 
FIGURE 4.16   EXAMPLE OF AN IDEAL SCATTER CHART OF RESIDUALS AND 
 PREDICTED VALUES OF THE DEPENDENT VARIABLE
0
e
y^
FIGURE 4.17    EXAMPLES OF DIAGNOSTIC SCATTER CHARTS OF RESIDUALS FROM 
FOUR  REGRESSIONS 
0
(a)
(c)
(d)
(b)
x
e
0
x
e
e
0
t
0
x
e

 
4.5 Inference and Regression 
147
the residuals do not have a constant variance. In panel b, the residuals are positive for small 
and large values of the independent variable x but are negative for the remaining values of 
the independent variable. This pattern suggests that the linear relationships in the regres-
sion model underpredicts the value of dependent variable for small and large values of the 
independent variable and overpredicts the value of the dependent variable for intermediate 
values of the independent variable. In this case, the regression model does not adequately 
capture the relationship between the independent variable x and the dependent variable y. 
The residuals in panel c are not symmetrically distributed around 0; many of the negative 
residuals are relatively close to zero, while the relatively few positive residuals tend to be far 
from zero. This skewness suggests that the residuals are not normally distributed. Finally, 
the residuals in panel d are plotted over time t, which generally serves as an independent 
variable; that is, an observation is made at each of several (usually equally spaced) points 
in time. In this case, connected consecutive residuals allow us to see a distinct pattern 
across every set of four residuals; the second residual is consistently larger than the first 
and smaller than the third, whereas the fourth residual is consistently the smallest. This pat-
tern, which occurs consistently over each set of four consecutive residuals in the chart in 
panel d, suggests that the residuals generated by this model are not independent. A residual 
pattern such as this generally occurs when we have collected quarterly data and have not 
captured seasonal effects in the model. In each of these instances, any inferences based on 
our regression will likely not be reliable.
Frequently, the errors do not meet these conditions either because an important inde-
pendent variable has been omitted from the model or because the functional form of the 
model is inadequate to explain the relationships between the independent variables and the 
dependent variable. It is important to note that calculating the values of the estimates b0, b1, 
b2, . . . , bq does not require the errors to satisfy these conditions. However, the errors must 
satisfy these conditions in order for inferences (confidence intervals for predicted values 
of the dependent variable and confidence intervals and hypothesis tests of the regression 
parameters b0, b1, b2, . . . , bq) to be reliable.
You can generate scatter charts of the residuals against each independent variable in the 
model when using Excel’s Regression tool; to do so, select the Residual Plots option in the 
Residuals area of the Regression dialog box. Figure 4.18 shows residual plots produced 
by Excel for the Butler Trucking Company example for which the independent variables 
are miles (x1) and deliveries (x2).
The residuals at each value of miles appear to have a mean of zero, to have similar vari-
ances, and to be concentrated around zero. The residuals at each value of deliveries also ap-
pear to have a mean of zero, to have similar variances, and to be concentrated around zero. 
Although there appears to be a slight pattern in the residuals across values of deliveries, it 
is negligible and could conceivably be the result of random variation. Thus, this evidence 
provides little reason for concern over the validity of inferences about the regression model 
that we may perform.
A scatter chart of the residuals e against the predicted values of the dependent vari-
ables is also commonly used to assess whether the residuals of the regression model 
satisfy the conditions necessary for valid inference. We look for similar patterns in these 
scatter charts. You can generate scatter charts of the residuals against the predicted values 
of the dependent variable using Excel’s Regression tool; to do so, select the Residuals 
option in the Residuals area of the Regression dialog box (shown in Figure 4.12). This 
generates a table of predicted values of the dependent variable and residuals for the ob-
servations in the data; a partial list for the Butler Trucking multiple regression example 
is shown in Figure 4.19.
We can then use the Excel chart tool to create a scatter chart of these predicted values 
and residuals similar to the chart in Figure 4.20. The figure shows that the residuals at 
each predicted value of the dependent variable appear to have a mean of zero, to have 
similar variances, and to be concentrated around zero. This leads us to the same conclusion 
In simple linear regres-
sion, a plot of the residuals 
against the independent 
variable and a plot of 
the residuals against the 
predicted values of the 
dependent variable yield 
the same information.

148 
Chapter 4 Linear Regression
we reached when looking at the residuals plotted against the independent variables: The 
residuals provide little evidence that our regression model violates the conditions neces-
sary for reliable inference. We can trust the inferences that we may wish to perform on 
our regression model.
Testing for an Overall Regression Relationship
Once we ascertain that our regression model satisfies the conditions necessary for reli-
able inference reasonably well, we can begin testing hypotheses and building confidence 
intervals. We begin by testing a fundamental hypothesis: that the regression parameters 
b1, b2, . . . , bq are all equal to zero. If we do not reject this test, using regression to predict 
the mean value of the dependent variable for various values of the independent variables 
is no more effective than using the sample mean y of the dependent variable. If this is the 
case, we will use y as the predicted value of the dependent variable y for any values of the 
dependent variables x1, x2, . . . , xq.
In a linear regression equation, the mean or expected value of the dependent variable y 
is a linear function of independent variables of x1, x2, . . . , xq; that is, E( y | x1, x2, . . . , xq) 5 
b0 1 b1x1 1 b2x2 1 . . . 1 bq xq. If the values of b1, b2, . . . , bq are all zero, then E( y | x1, 
FIGURE 4.18   EXCEL RESIDUAL PLOTS FOR THE BUTLER TRUCKING 
 COMPANY MULTIPLE REGRESSION
2.5
2.0
1.5
1.0
0.5
Residuals
0
20
11
2
3
4
5
6
7
Miles
Deliveries
Deliveries Residual Plot
Miles Residual Plot
40
60
80
100
120
–0.5
–1.0
–1.5
–2.0
2.5
2.0
1.5
1.0
0.5
Residuals
0
–0.5
–1.0
–1.5
–2.0
Recall that in the 
Excel  output shown in 
 Figure 4.13, the label for 
the independent variable x1 
is Miles and the label for 
the independent variable x2 
is Deliveries.

 
4.5 Inference and Regression 
149
x2 . . . , xq) 5 b0 1 (0)x1 1 (0)x2 1 . . . 1 (0)xq 5 b0. In this case, the mean value of y does 
not depend on the values of x1, x2, . . . , xq; hence the independent variables x1, x2, . . . , xq and 
the dependent variable y are not linearly related, and an overall regression relationship is not 
present. Alternatively, if the value of at least one of b1, b2, . . . , bq is not equal to zero, an 
overall regression relationship is present. To test for an overall regression relationship, we 
FIGURE 4.19   TABLE OF THE FIRST SEVERAL PREDICTED VALUES y^ AND 
 RESIDUALS e GENERATED BY THE EXCEL REGRESSION TOOL
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
RESIDUAL OUTPUT
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
9.605504464
5.556419081
9.605504464
8.225507903
4.8664208
6.881873062
7.235932632
7.254143492
8.243688763
7.553690482
6.936415641
7.290505212
9.287776613
5.874146931
6.954596501
5.556419081
–0.305504464
–0.756419081
–0.705504464
–1.725507903
–0.6664208
–0.681873062
0.164037368
–1.254143492
–0.643688763
–1.453690482
0.063584359
–0.290505212
0.312223387
0.625853069
0.245403499
0.443580919
Observation
Predicted Time
Residuals
FIGURE 4.20  SCATTER CHART OF PREDICTED VALUES y^ AND RESIDUALS e
Predicted Values of the Dependent Variable
–2.0
–1.5
–1.0
–0.5
2
4
6
8
10
12
0
0
0.5
1.0
1.5
2.0
2.5
Residuals

150 
Chapter 4 Linear Regression
use sample data to test the hypothesis that the values of b1, b2, . . . , bq are all zero. To test 
this hypothesis we use an F test based on the F probability distribution. If the F test leads 
us to reject the hypothesis that the values of b1, b2, . . . , bq are all zero, we conclude that 
there is an overall regression relationship; otherwise, we conclude that there is no overall 
regression relationship. The test statistic generated by the sample data for this test is
 
F 5
SSR@q
SSE @1n 2 q 2 12  
(4.17)
where 
SSR and SSE 5 as defined by Equations 4.10 and 4.8
q 5 the number of independent variables in the regression model
n 5 the number of observations in the sample
The numerator of this test statistic is a measure of the variability in the dependent 
variable y that is explained by the independent variables x1, x2, . . . , xq, and the denomina-
tor is a measure of the variability in the dependent variable y that is not explained by the 
independent variables x1, x2, . . . , xq. We can see that larger values of F provide stronger 
evidence of an overall regression relationship. Statistical software will generally report a 
p-value for this test statistic; for a given value of F, the p-value represents the probability of 
collecting a sample of the same size from the same population that yields a larger F statistic 
given that the values of b1, b2, . . . , bq are all actually zero. Thus, smaller p-values indicate 
stronger evidence against the hypothesis that the values of b1, b2, . . . , bq are all zero (i.e., 
stronger evidence of an overall regression relationship). The hypothesis is rejected when 
the p-value is smaller than some predetermined value (usually 0.05 or 0.01) that is referred 
to as the level of significance.
The output of Excel’s Regression tool provides the results of the F test. Consider again 
Figure 4.13, which shows the multiple linear regression results for Butler Trucking with 
independent variables x1 (labeled Miles) and x2 (labeled Deliveries). The values of the 
SSR, SSE, and SST are contained in cells C12, C13, and C14, respectively; the degrees 
of freedom for each of these measures are in cells B12, B13, and B14, respectively; the 
values of the MSR and MSE are in cells D12 and D13; and the value of the F statistic and 
the corresponding p-value are in E12 and F12, respectively. Suppose that, for our Butler 
Trucking multiple regression, we want to test the hypothesis that the values of b1, b2, . . . , 
bq are all zero at a 0.05 level of significance. The Excel output in Figure 4.13 shows that 
the F statistic for our regression is 664.5292 and that the associated p-value is essentially 
zero (2.2419E-110, or 2.2419 with the decimal moved 110 places to the left). This p-value 
tells us that if the values of b1, b2, . . . , bq are all actually zero, the probability we could 
collect a random sample of 300 observations from the population of Butler Trucking driving 
 assignments that yields an F statistic greater than 664.5292 is practically zero. This p-value 
is sufficiently small to allow us to reject the hypothesis that no overall regression relation-
ship exists at the 0.05 level of significance. The p-value is small enough to justify rejecting 
the hypothesis that b1 5 b2 5 ∙ ∙ ∙ 5 bq 5 0 or the Butler Trucking multiple regression 
example at a 0.01 level of significance or even at a far smaller level of significance. We 
therefore conclude that there is an overall regression relationship.
Testing Individual Regression Parameters
If the F test leads us to conclude that an overall regression relationship exists, we may then 
wish to determine whether statistically significant relationships exist between the dependent 
variable y and each of the independent variables x1, x2, . . . , xq individually. Note that if a bj is 
zero, then the dependent variable y does not change when the independent variable xj changes, 
and there is no linear relationship between y and xj. Alternatively, if a bj is not zero, there is a 
linear relationship between the dependent variable y and the independent variable xj.
George W. Snedecor named 
the F test for Sir Ronald A. 
Fisher, who had initially 
developed the statistic in 
the 1920s.
SSRyq is commonly referred 
to as the mean square 
due to regression (MSR), 
and SSEy(n 2 q 2 1) is 
 commonly referred to as the 
mean square error (MSE). 
The degrees of freedom 
represent the number of 
independent units of 
information in a calculation 
and generally equal 
differences between the 
number of observations and 
the number of estimated 
parameters that are used in 
a calculation. For example, 
in Section 2.4, the equation 
for calculating the sample 
variance was given as 
s2 5 a
n
i51
1xi 2 x2 2@1n 2 12. 
Thus, sample variance has 
n – 1 degrees of freedom 
because n observations and 
one estimated parameter 
(x) are used in this 
calculation.

 
4.5 Inference and Regression 
151
We use a t test to test the hypothesis that a regression parameter bj is zero. The test 
statistic for this t test is
 
t 5
bj
sbj
 
(4.18)
where sbj is the estimated standard deviation of bj.
As the value of bj, the point estimate of bj, deviates from zero in either direction, the evi-
dence from our sample that the corresponding regression parameter bj is not zero increases. 
Thus, as the magnitude of t increases (as t deviates from zero in either direction), we are 
more likely to reject the hypothesis that the regression parameter bj is zero and so conclude 
that a relationship exists between the dependent variable y and the independent variable xj.
Statistical software will generally report a p-value for this test statistic; for a given value 
of t, this p-value represents the probability of collecting a sample of the same size from 
the same population that yields a larger t statistic given that the value of bj is actually zero. 
Thus, smaller p-values indicate stronger evidence against the hypothesis that the value of 
bj is zero (i.e., stronger evidence of a relationship between xj and y). As with the F test, this 
hypothesis is rejected when the corresponding p-value is smaller than some predetermined 
level of significance (usually 0.05 or 0.01).
In simple linear regression, the t test that is used to determine whether a statistically sig-
nificant relationship exists between the dependent variable y and the independent variable x 
and the F test of an overall regression relationship are mathematically identical tests of the 
same hypothesis and will yield identical p-values. Therefore, in simple linear regression, 
these tests are interchangeable and the F test is usually ignored.
The output of Excel’s Regression tool provides the results of the t tests for each regres-
sion parameter. Refer again to Figure 4.13, which shows the multiple linear regression 
results for Butler Trucking with independent variables x1 (labeled Miles) and x2 (labeled 
Deliveries). The values of the parameter estimates b0, b1, and b2 are located in cells B17, 
B18, and B19, respectively; the standard deviations sb0, sb1, and sb2 are contained in cells 
C17, C18, and C19, respectively; the values of the t statistics for the hypothesis tests are 
in cells D17, D18, and D19, respectively; and the corresponding p-values are in cells E17, 
E18, and E19, respectively.
Let’s use these results to test the hypothesis that b1 is zero. If we do not reject this 
hypothesis, we conclude that the mean value of y does not change when the value of x1 
changes, and so there is no relationship between driving time and miles traveled. We see in 
the Excel output in Figure 4.13 that the statistic for this test is 27.3655 and that the associ-
ated p-value is 3.5398E-83. This p-value tells us that if the value of b1 is actually zero, the 
probability we could collect a random sample of 300 observations from the population of 
Butler Trucking driving assignments that yields a t statistic with an absolute value greater 
than 27.3655 is practically zero. Such a small probability represents a highly unlikely sce-
nario; thus, the small p-value allows us to conclude that a relationship exists between 
driving time and miles traveled. (The p-value is small enough to justify rejecting the hy-
pothesis that b1 5 0 for the Butler Trucking multiple regression example at a 0.01 level of 
significance or even at a far smaller level of significance.) Thus, this p-value is sufficiently 
small to allow us to reject the hypothesis that there is no relationship between driving time 
and miles traveled at the 0.05 level of significance.
Similarly, we can test the hypothesis that b2 is zero. If we do not reject this hypothesis, 
we conclude that the mean value of y does not change when the value of x2 changes, and 
so there is no relationship between driving time and number of deliveries. We see in the 
Excel output in Figure 4.13 that the t statistic for this test is 23.3731 and that the associated 
p-value is 2.84826E-69. This p-value tells us that if the value of b2 is actually zero, the 
probability we could collect a random sample of 300 observations from the population of 
The standard deviation of 
bj is often referred to as the 
standard error of bj. Thus, 
sbj provides an estimate of 
the standard error of bj.

152 
Chapter 4 Linear Regression
Butler  Trucking driving assignments that yields a t statistic with an absolute value greater 
than 23.3731 is practically zero. This is highly unlikely, and so the p-value is sufficiently 
small to allow us to conclude that a relationship exists between driving time and number 
of deliveries. (The p-value is small enough to justify rejecting the hypothesis that b2 5 0 
for the Butler Trucking multiple regression example at a 0.01 level of significance (or even 
at a far smaller level of significance).) Thus, this p-value is sufficiently small to allow us 
to reject the hypothesis that there is no relationship between driving time and number of 
deliveries at the 0.05 level of significance.
Finally, we can test the hypothesis that b0 is zero in a similar fashion. If we do not reject 
this hypothesis, we conclude that the mean value of y is zero when the values of x1 and x2 
are both zero, and so there is no driving time when a driving assignment is 0 miles and has 
0 deliveries. We see in the Excel output that the t statistic for this test is 0.6205 and the as-
sociated p-value is 0.5358. This p-value tells us that if the value of b0 is actually zero, the 
probability we could collect a random sample of 300 observations from the population of 
Butler Trucking driving assignments that yields a t statistic with an absolute value greater 
than 0.6205 is 0.5358. Thus, we do not reject the hypothesis that mean driving time is zero 
when a driving assignment is 0 miles and has 0 deliveries. However, the range of values 
for the independent variable distance traveled for the Butler Trucking multiple regression 
is 40 to 100, and the range of values for the independent variable number of deliveries is 
1 to 6. Any prediction outside these ranges, such as the y-intercept for this model, is not a 
reliable estimate, and so a hypothesis test of b0 is meaningless for this model. However, if 
the experimental region for a regression problem includes the origin, a hypothesis test of 
b0 will be meaningful.
We can also test each of these hypotheses tests through confidence intervals. A 
 confidence interval is an estimate of a population parameter that provides an interval 
believed to contain the value of the parameter at some level of confidence. The level of con-
fidence, or confidence level, indicates how frequently interval estimates based on samples 
of the same size taken from the same population using identical sampling techniques will 
contain the true value of the parameter we are estimating. Thus, when building a 95 percent 
confidence interval, we can expect that if we took samples of the same size from the same 
population using identical sampling techniques, the corresponding interval estimates would 
contain the true value of the parameter we are estimating for 95 percent of the samples.
Although the confidence intervals for b0, b1, b2, . . . , bq convey information about 
the variation in the estimates b1, b2, . . . , bq that can be expected across repeated samples, 
they can also be used to test whether each of the regression parameters b0, b1, b2, . . . , 
bq is equal to zero in the following manner. To test that bj is zero (i.e., there is no linear 
relationship between xj and y) at some predetermined level of significance (say 0.05), first 
build a confidence interval at the (1 – 0.05)100 percent confidence level. If the resulting 
confidence interval does not contain zero, we conclude that bj differs from zero at the 
predetermined level of significance. Similarly, to test that b0 is zero (i.e., the value of the 
dependent variable is zero when all the independent variables x1, x2, . . . , xq are equal to zero) 
at some predetermined level of significance (say 0.05), first build a confidence interval at the 
(1 – 0.05)100 percent confidence level. If the resulting confidence interval does not contain 
zero, we conclude that b0 differs from zero at the predetermined level of significance.
While constructing individual confidence intervals for the regression parameters b0, 
b1, b2, . . . , bq is beyond the scope of this book, most software that is capable of regression 
analysis can also produce these confidence intervals. For example, the output of Excel’s 
Regression tool for Butler Trucking, given in Figure 4.13, provides confidence intervals for 
the b1 (the slope coefficient associated with the independent variable x1, labeled Miles) and 
b2 (the slope coefficient associated with the independent variable x2, labeled Deliveries), as 
well as the y-intercept b0. The 95 percent confidence intervals for b0, b1, and b2 are shown 
in cells F17–G17, F18–G18, and F19–G19, respectively; these 95 percent confidence 
The estimated value of the 
y-intercept often results 
from extrapolation.

 
4.5 Inference and Regression 
153
intervals are automatically generated. Neither of the 95 percent confidence intervals for  
b1 and b2 includes zero, so we can conclude that b1 and b2 each differ from zero at the 
0.05 level of significance. On the other hand, the 95 percent confidence interval for b0 does 
include zero, so we conclude that b0 does not differ from zero at the 0.05 level of signifi-
cance. Again note that, for the Butler Trucking example, the estimated y-intercept results 
from extrapolation, and so the confidence interval for b0 is meaningless. However, if the 
experimental region for a regression problem includes the origin, the confidence interval 
for b0 will be meaningful.
The Regression tool dialog box offers the user the opportunity to generate confidence 
intervals for b0, b1, and b2 at a confidence level other 95 percent. In this example, we 
chose to create 99 percent confidence intervals for b0, b1, and b2, which in Figure 4.13 
are given in cells H17–I17, H18–I18, and H19–I19, respectively. Neither of the 99 per-
cent confidence intervals for b1 and b2 includes zero, so we can conclude that b1 and b2 
each differs from zero at the 0.01 level of significance. On the other hand, the 99 percent 
confidence interval for b0 does include zero, so we conclude that b0 does not differ from 
zero at the 0.01 level of significance.
Addressing Nonsignificant Independent Variables
If the data do not support rejection of the hypothesis that a bj is zero, we conclude that 
there is no linear relationship between y and xj. This leads to the question of how to handle 
the corresponding independent variable. Do we use the model as originally formulated 
with the nonsignificant independent variable, or do we rerun the regression without the 
nonsignificant independent and use the new result? The approach to be taken depends on a 
number of factors, but ultimately whatever model we use should have a theoretical basis. If 
practical experience dictates that the nonsignificant independent variable has a relationship 
with the dependent variable, the independent variable should be left in the model. On the 
other hand, if the model sufficiently explains the dependent variable without the nonsig-
nificant independent variable, then we should consider rerunning the regression without the 
nonsignificant independent variable. Note that it is possible that the estimates of the other 
regression coefficients and their p-values may change considerably when we remove the 
nonsignificant independent variable from the model.
The appropriate treatment of the inclusion or exclusion of the y-intercept when b0 
is not statistically significant may require special consideration. For example, in the 
Butler Trucking multiple regression model, recall that the p-value for b0 is 0.5354, sug-
gesting that this estimate of b0 is not statistically significant. Should we remove the 
y-intercept from this model because it is not statistically significant? Excel provides 
functionality to remove the y-intercept from the model by selecting Constant is zero in 
Excel’s Regression tool. This will force the y-intercept to go through the origin (when 
the independent variables x1, x2, . . . , xq all equal zero, the estimated value of the de-
pendent variable will be zero). However, doing this can substantially alter the estimated 
slopes in the regression model and result in a less effective regression that yields less 
accurate predicted values of the dependent variable. Since the primary purpose of the 
regression model is to explain or predict values of the dependent variable for values of 
the independent variables that lie within the experimental region on which the model 
is based, regression through the origin should not be forced unless there are strong a 
priori reasons for believing that the dependent variable is equal to zero when the values 
of all independent variables in the model are equal to zero. A common business ex-
ample of regression through the origin is a model for which output in a labor-intensive 
production process is the dependent variable and hours of labor is the independent 
variable; because the production process is labor intense, we would expect no output 
when the value of labor hours is zero.

154 
Chapter 4 Linear Regression
Multicollinearity
We use the term independent variable in regression analysis to refer to any variable used 
to predict or explain the value of the dependent variable. The term does not mean, how-
ever, that the independent variables themselves are independent in any statistical sense. On 
the contrary, most independent variables in a multiple regression problem are correlated 
to some degree with one another. For example, in the Butler Trucking example involv-
ing the two independent variables x1 (miles traveled) and x2 (number of deliveries), we 
could compute the sample correlation coefficient rx1,x2 to determine the extent to which 
these two variables are related. Doing so yields rx1,x2 5 0.16. Thus, we find some degree of 
linear association between the two independent variables. In multiple regression analysis, 
 multicollinearity refers to the correlation among the independent variables.
To gain a better perspective of the potential problems of multicollinearity, let us consider 
a modification of the Butler Trucking example. Instead of x2 being the number of deliveries, 
let x2 denote the number of gallons of gasoline consumed. Clearly, x1 (the miles traveled) and 
x2 are now related; that is, we know that the number of gallons of gasoline used depends to a 
large extent on the number of miles traveled. Hence, we would conclude logically that x1 and 
x2 are highly correlated independent variables and that multicollinearity is present in the model. 
The data for this example are provided in the file ButlerWithGasConsumption.
Using Excel’s Regression tool, we obtain the results shown in Figure 4.21 for our multiple 
regression. The F test has a p-value of 4.09542E-43, so we have found an overall regression 
relationship at either the 0.05 or 0.01 level of significance. Furthermore, when we conduct a 
t test to determine whether b1 is equal to zero, we find a p-value of 3.1544E-07, and so we 
reject this hypothesis and conclude that travel time is related to miles traveled. On the other 
hand, when we conduct a t test to determine whether b2 is equal to zero, we find a p-value 
of 0.6588, and so we do not reject this hypothesis. Does this mean that travel time is not 
file
WEB
ButlerWithGasConsumption
FIGURE 4.21   EXCEL SPREADSHEET WITH RESULTS FOR THE BUTLER TRUCKING  COMPANY 
 MULTIPLE REGRESSION WITH MILES AND GASOLINE CONSUMPTION AS 
 INDEPENDENT VARIABLES
A
SUMMARY OUTPUT
Multiple R
0.69406354
0.481724198
0.478234125
1.398077545
300
2
2.493095385
0.074701825
–0.067506102
0.33669895
0.014274552
0.152707928
7.404523781
5.233216928
–0.442060235
1.36703E-12
3.15444E-07
0.658767336
1.830477398
269.7904079
1.954620822
138.0269794
4.09542E-43
0.046609743
–0.368032789
3.155713373
0.102793908
0.233020584
1.620208758
0.037695279
–0.463398955
3.365982013
0.111708371
0.328386751
539.5808158
580.5223842
1120.1032
297
299
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
Regression
Residual
Total
Intercept
Miles
Gasoline Consumption
Regression Statistics
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0%
Upper 99.0%
SS
MS
F
Signiﬁcance F
B
C
D
E
F
G
H
I
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

 
4.5 Inference and Regression 
155
related to gasoline consumption? Not necessarily. What it probably means in this instance 
is that, with x1 already in the model, x2 does not make a significant marginal contribution to 
predicting the value of y. This interpretation makes sense within the context of the Butler 
Trucking example; if we know the miles traveled, we do not gain much new information 
that would be useful in predicting driving time by also knowing the amount of gasoline 
consumed. We can see this in the scatter chart in Figure 4.22; miles traveled and gasoline 
consumed are strongly related.
Note that, even though we rejected the hypothesis that b1 is equal to zero for this model, 
the value of the t statistic is much smaller and the p-value substantially larger than in the 
multiple regression model that includes miles driven and number of deliveries as the inde-
pendent variables. The evidence against the hypothesis that b1 is equal to zero is weaker in 
the multiple regression that includes miles driven and gasoline consumed as the indepen-
dent variables because of the high correlation between these two independent variables.
To summarize, in t tests for the significance of individual parameters, the difficulty 
caused by multicollinearity is that it is possible to conclude that a parameter associated 
with one of the multicollinear independent variables is not significantly different from 
zero when the independent variable actually has a strong relationship with the dependent 
variable. This problem is avoided when there is little correlation among the independent 
variables.
 Statisticians have developed several tests for determining whether multicollinearity is 
high enough to cause problems. In addition to the initial understanding of the nature of the 
relationships between the various pairs of variables that we can gain through scatter charts 
such as the chart shown in Figure 4.22, correlations between pairs of independent variables 
can be used to identify potential problems. According to a common rule of thumb test, 
multicollinearity is a potential problem if the absolute value of the sample correlation coef-
ficient exceeds 0.7 for any two of the independent variables. As we learned in Chapter 2, 
we can place the Excel function 
5CORREL(B2:B301,C2:C301)
into any empty cell of the file ButlerWithGasConsumption to find that the correlation between 
Miles (in column B) and Gasoline Consumed (in column C) for the sample Butler Trucking 
If any estimated regression 
parameters b1, b2, . . . , bq or 
associated p-values change 
dramatically when a new in-
dependent variable is added 
to the model (or an existing 
independent variable is 
removed from the model), 
multicollinearity is likely 
present. Looking for changes 
such as these is sometimes 
used as a way to test for 
multicollinearity.
FIGURE 4.22   SCATTER CHART OF MILES AND GASOLINE CONSUMED FOR 
BUTLER TRUCKING COMPANY
Miles
0
20
40
60
80
100
120
12
0
2
4
6
8
10
Gasoline Consumption (gal)

156 
Chapter 4 Linear Regression
data is rMiles, Gasoline Consumed 5 0.9572, which supports the conclusion that Miles and Gasoline 
Consumed are multicollinear. Similarly, by placing the Excel function 
5CORREL(B2:B301,D2:D301) 
into any empty cell of the file ButlerWithGasConsumption shows that the correlation between 
Miles (in column B) and Deliveries (in column D) for the sample data is rMiles,  Deliveries 5 
0.0258. This supports the conclusion that Miles and Deliveries are not multicollinear. Other 
tests for multicollinearity are more advanced and beyond the scope of this text.
The primary consequence of multicollinearity is that it increases the variances and standard 
errors of the regression estimates of b0, b1, b2, . . . , bq and predicted values of the dependent 
variable, and so inference based on these estimates is less precise than it should be. This means 
that confidence intervals for b0, b1, b2, . . . , bq and predicted values of the dependent vari-
able are wider than they should be, and we are less likely to reject tests of hypotheses that an 
individual parameter b0, b1, b2, . . . , bq is equal to zero than we otherwise would be. Thus, 
multicollinearity leads us to conclude that independent variables x1, x2, . . . , xq are not related 
to the dependent variable y when they in fact are related. In addition, multicollinearity can 
result in confusing or misleading regression parameters b1, b2, . . . , bq. Therefore, if a primary 
objective of the regression analysis is inference (confidence intervals for predicted values of the 
dependent variable and confidence intervals and hypothesis tests of the regression parameters 
b0, b1, b2, . . . , bq), you should avoid including independent variables that are highly corre-
lated in the regression model, if possible. For example, when a pair of independent variables is 
highly correlated it is common to simply include only one of these independent variables in the 
regression model. When decision makers have reason to believe substantial multicollinearity 
is present and they choose to retain the highly correlated independent variables in the model, 
they must realize that separating the relationships between each of the individual independent 
variables and the dependent variable is difficult (and maybe impossible). On the other hand, 
multicollinearity does not affect the predictive capability of a regression model, so if the pri-
mary objective is prediction or forecasting, then multicollinearity is not a concern.
Inference and Very Large Samples
Consider the example of a credit card company that has a very large database of informa-
tion provided by its customers when the customers apply for credit cards. These customer 
records include information on the customer’s annual household income, number of years of 
post–high school education, and number of members of the customer’s household. In a sec-
ond database, the company has records of the credit card charges accrued by each customer 
over the past year. Because the company is interested in using annual household income, the 
number of years of post–high school education, and the number of members of the household 
reported by new applicants to predict the credit card charges that will be accrued by these ap-
plicants, a data analyst links these two databases to create one data set containing all relevant 
information for a sample of 5000 customers (these data are available in the file LargeCredit).
The company has decided to apply multiple regression to these data to develop a model 
for predicting annual credit card charges for its new applicants. The dependent variable in 
the model is credit card charges accrued by a customer in the data set over the past year 
(y); the independent variables are the customer’s annual household income (x1), number of 
members of the household (x2), and number of years of post–high school education (x3). 
Figure 4.23 provides Excel output for the multiple regression model estimated using the 
data set the company has created.
The model has a coefficient of determination of 0.3635 (see cell B5 in Figure 4.23), 
indicating that this model explains approximately 36 percent of the variation in credit card 
charges accrued by the customers in the sample over the past year and that the F test of an 
overall regression relationship is highly significant with an extremely small p-value (see cell 
F12 in Figure 4.23). The p-value for each test of the individual regression parameters is also 
file
WEB
LargeCredit

 
4.5 Inference and Regression 
157
very small (see cells E18 through E20), indicating that for each independent variable we can 
reject the hypothesis of no relationship with the dependent variable. The estimated slopes 
associated with the dependent variables are all highly significant. The model estimates that:
●  For a fixed number of members of the household and number of years of post–high 
school education, accrued credit card charges increase by $120.63 when a customer’s 
annual household income increases by $1,000. This is shown in cell B18 of Figure 4.23.
●  For a fixed annual household income and number of years of post–high school edu-
cation, accrued credit card charges increase by $533.85 when a customer’s house-
hold increases by one member. This is shown in cell B19 of Figure 4.23.
●  For a fixed annual household income and number of members of the household, 
accrued credit card charges decrease by $505.63 when a customer’s number of years 
of post–high school education increases by one year. This is shown in cell B20 of 
Figure 4.23.
Because the y-intercept is an obvious result of extrapolation (no customer in the data has val-
ues of zero for annual household income, number of members of the household, and number 
of years of post–high school education), the estimated regression parameter b0 is meaningless.
The small p-values associated with a model that is fit on an extremely large sample do 
not imply that an extremely large sample solves all problems. Because virtually all relation-
ships between independent variables and the dependent variable will be statistically signifi-
cant if the sample size is sufficiently large, inference can no longer be used to discriminate 
between meaningful and specious relationships. This is because the variability in potential 
values of an estimator bj of a regression parameter bj depends on two factors:
●  How closely the members of the population adhere to the relationship between xj 
and y that is implied by bj
● The size of the sample on which the value of the estimator bj is based
FIGURE 4.23   EXCEL OUTPUT FOR MULTIPLE REGRESSION FOR CREDIT CARD COMPANY  EXAMPLE
A
SUMMARY OUTPUT
Multiple R
0.602946393
0.363544353
0.363162174
4847.563495
5000
3
2051.638735
120.6315397
533.8460243
258.2118129
2.439500895
33.07739782
7.945564971
49.44927054
16.13929932
2.37056E-15
0
3.6874E-57
1545.430245
22353083859
23498871.84
951.2407238
0
115.8490472
468.9998058
2557.847226
125.4140323
598.6922428
1386.274984
114.3454003
448.6117306
2717.002486
126.9176792
619.080318
–505.632418
45.54182323 –11.10259498
2.60612E-28 –594.9143812 –416.3504547 –622.9852144 –388.2796215
67059251577
1.174E+11
1.8446E+11
4996
4999
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
Regression
Residual
Total
Intercept
Annual Income ($1000)
Household Size
Years of Post-High
School Education
Regression Statistics
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0%
Upper 99.0%
SS
MS
F
Signiﬁcance F
B
C
D
E
F
G
H
I
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

158 
Chapter 4 Linear Regression
Let us consider these factors one at a time. If values of xj and y consistently adhere to the 
relationship between xj and y that is implied by bj, there will be relatively little differ-
ence in potential values of bj that can be generated by different random samples. Under 
these circumstances, if we were to take many random samples and calculate the value of 
the estimator bj for each of these samples, the values of these estimates would generally 
differ little from the regression parameter by bj. However, if values of xj and y do not 
consistently adhere to the relationship between xj and y that is implied by bj, there is po-
tential for relatively large differences in the values of bj that can be generated by different 
random samples. Under these circumstances, if we were to take many random samples 
and calculate the value of the estimator bj for each of these samples, the values of these 
estimates would generally differ greatly from the regression parameter by bj.
As the sample size increases, the amount of information about the population that is 
contained in the sample also increases. Thus, for larger samples, there will generally be 
smaller differences in potential values of bj that can be generated by different random 
samples. If there is relatively little difference in potential values of bj that can be generated 
by different random samples, then there will generally also be relatively little difference 
between potential values of the estimator bj and the regression parameter by bj.
If the sample used to estimate the regression model is very large, then almost no dif-
ference in potential values of bj can be generated by different random samples, and so 
there will also be almost no difference between potential values of the estimator bj and 
the regression parameter by bj. Since we address the variability in potential values of our 
estimators through the use of statistical inference, and variability of our estimates bj es-
sentially disappears as the sample size grows very large, inference is of little use for esti-
mates generated from very large samples. Thus, we generally are not concerned with the 
conditions a regression model must satisfy in order for inference to be reliable when we 
use a very large sample to estimate our regression model inference. Multicollinearity, on 
the other hand, can result in confusing or misleading regression parameters b1, b2, . . . , bq 
and so is still a concern when we use a large data set to estimate a regression model.
How much does sample size matter? Table 4.4 provides the regression parameter es-
timates and the corresponding p-values for multiple regression models estimated on the 
first 50 observations, the second 50 observations, and so on for the LargeCredit data. Note 
that, even though the mean of the parameter estimates for the regressions based on 50 obser-
vations are similar to the parameter estimates based on the full sample of 5000 observations, 
the individual values of the estimated regression parameters in the regressions based on 
50 observations show a great deal of variation. In these ten regressions, the estimated values 
The phenomenon by which 
the value of an estimate 
generally becomes closer to 
the value of parameter be-
ing estimated as the sample 
size grows is called the Law 
of Large Numbers.
Observations
b0
p-value
b1
p-value
b2
p-value
b3
p-value
  1–50
 51–100
101–150
151–200
201–250
251–300
301–350
351–400
401–450
451–500
2805.182
894.407
22191.590
2294.023
8994.040
7265.471
2147.906
2504.532
1587.067
2315.945
0.7814
0.6796
0.4869
0.3445
0.0289
0.0234
0.5236
0.8380
0.5123
0.9048
154.488
125.343
155.187
114.734
103.378
73.207
117.500
118.926
81.532
148.860
1.45E-06
2.23E-07
3.56E-07
1.26E-04
6.89E-04
1.02E-02
1.88E-04
8.54E-07
5.06E-04
1.07E-05
234.664
822.675
674.961
297.011
2489.932
277.874
390.447
798.499
1267.041
1000.243
0.5489
0.0070
0.0501
0.3700
0.2270
0.8409
0.3053
0.0112
0.0004
0.0053
207.828
2355.585
225.309
2537.063
2375.601
2405.195
2374.799
45.259
2891.118
2974.791
0.6721
0.3553
0.9560
0.2205
0.5261
0.4060
0.4696
0.9209
0.0359
0.0420
Mean
1936.567
119.316
491.773
2368.637
TablE 4.4   REGRESSION PARAMETER ESTIMATES AND THE CORRESPONDING p-VALUES FOR TEN 
MULTIPLE REGRESSION MODELS, EACH ESTIMATED ON 50 OBSERVATIONS FROM THE 
LARGECREDIT DATA

 
4.5 Inference and Regression 
159
of b0 range from 22191.590 to 8994.040, the estimated values of b1 range from 73.207 to 
155.187, the estimated values of b2 range from 2489.932 to 1267.041, and the estimated 
 values of b3 range from 2974.791 to 207.828. This is reflected in the p-values corresponding 
to the parameter estimates in the regressions based on 50 observations, which are substan-
tially larger that the corresponding p-values in the regression based on 5000 observations. 
These results underscore the impact that a very large sample size can have on inference.
For another example, suppose the credit card company also has a separate database of 
information on shopping and lifestyle characteristics that it has collected from its custom-
ers in a recent Internet survey. The data analyst notes in the results in Figure 4.24 that the 
original regression model fails to explain almost 65 percent of the variation in credit card 
charges accrued by the customers in the data set. In an attempt to increase the variation 
in the dependent variable explained by the model, the data analyst decides to augment 
the original regression with a new independent variable, number of hours per week spent 
watching television (which we will designate as x4). After linking the databases so that all 
necessary information for each of the 5000 customers is in a single data set, the analyst runs 
the new multiple regression and achieves the following results.
Although the F test of an overall regression relationship is again highly significant with 
an extremely small p-value, the new model has a coefficient of determination of 0.3669 (see 
cell B5 in Figure 4.24), indicating the addition of number of hours per week spent watching 
television increased the explained variation in sample values of accrued credit card charges 
by less than 1 percent. The estimated regression parameters and associated p-values for 
annual household income, number of members of the household, and number of years of 
FIGURE 4.24   EXCEL OUTPUT FOR MULTIPLE REGRESSION FOR CREDIT CARD COMPANY 
 EXAMPLE AFTER ADDING NUMBER OF HOURS PER WEEK SPENT WATCHING 
 TELEVISION TO THE MODEL
A
SUMMARY OUTPUT
Multiple R
0.605753974
0.366937877
0.36643092
4835.106762
5000
4
1440.385909
120.4937794
538.2043625
283.3464635
2.433377775
33.00314865
5.083479398
49.51708715
16.30766713
3.84109E-07
0
2.72804E-58
884.9024443
16921304900
23378257.4
723.8052269
0
115.7232906
473.5037019
1995.869374
125.2642681
602.9050231
710.2547892
114.2234176
453.1613886
2170.51703
126.7641412
623.2473364
–509.7777354
45.43185836 –11.22071062
7.12888E-29 –598.8441236 –420.7113472 –626.8471819 –392.7082889
67685219598
1.16774E+11
1.8446E+11
4995
4999
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
Regression
Residual
Total
Intercept
Annual Income ($1000)
Household Size
Years of Post-High
School Education
Regression Statistics
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0%
Upper 99.0%
SS
MS
F
Signiﬁcance F
B
C
D
E
F
G
H
I
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
20.4413308
3.950382611
5.174519234
2.37441E-07
12.69684656
28.18581504
10.26192978
30.62073183
Hours Per Week
Watching Television
21

160 
Chapter 4 Linear Regression
post–high school education changed little after introducing number of hours per week spent 
watching television into the model.
The estimated regression parameter for number of hours per week spent watching tele-
vision is 20.44 (see cell B21 in Figure 4.24), suggesting a that 1-hour increase coincides 
with an increase of $20.44 in credit card charges accrued by each customer over the past 
year. The p-value associated with this estimate is 2.3744E-07 (see cell E21 in Figure 4.24), 
so we can reject the hypothesis that there is no relationship between the number of hours 
per week spent watching television and credit card charges accrued. However, when the 
model is based on a very large sample, almost all relationships will be significant whether 
they are real or not, and statistical significance does not necessarily imply that a relationship 
is meaningful or useful.
Is it reasonable to expect that the credit card charges accrued by a customer are related 
to the number of hours per week the consumer watches television? If not, the model that 
includes number of hours per week the consumer watches television as an independent vari-
able may provide inaccurate or unreliable predictions of the credit card charges that will be 
accrued by new customers, even though we have found a significant relationship between 
these two variables. If the model is to be used to predict future amounts of credit charges, 
then the usefulness of including the number of hours per week the consumer watches tele-
vision is best evaluated by measuring the accuracy of predictions for observations not 
included in the sample data used to construct the model. This use of out-of-sample data is 
common in data-mining applications and is covered in detail in Chapter 6.
NOTES AND COMMENTS
1.  Finding a significant relationship between an 
independent variable xj and a dependent vari-
able y in a linear regression does not enable 
us to conclude that the relationship is linear. 
We can state only that xj and y are related and 
that a linear relationship explains a statisti-
cally significant portion of the variability in 
y over the range of values for xj observed in 
the sample.
2.  Note that a review of the correlations of pairs 
of independent variables is not always suf-
ficient to entirely uncover multicollinearity. 
The problem is that sometimes one indepen-
dent variable is highly correlated with some 
combination of several other independent 
variables. If you suspect that one independent 
variable is highly correlated with a combina-
tion of several other independent variables, 
you can use multiple regression to assess 
whether the sample data support your suspi-
cion. Suppose that your original regression 
model includes the independent variables x1, 
x2, . . . , xq and that you suspect that x1 is 
highly correlated with a subset of the other 
independent variables x2, . . . , xq. Estimate 
the multiple linear regression for which x1 is 
now the dependent variable; the subset of the 
independent variables x2, . . . , xq that you sus-
pect are highly correlated with x1 are now the 
independent variables. The coefficient of de-
termination R2 for this regression provides an 
estimate of the strength of the relationship be-
tween x1 and the subset of the other indepen-
dent variables x2, . . . , xq that you suspect are 
highly correlated with x1. As a rule of thumb, 
if the coefficient of determination R2 for this 
regression exceeds 0.50, multicollinearity be-
tween x1 and the subset of the other indepen-
dent variables x2, . . . , xq is a concern.
3.  When working with a small number of obser-
vations, assessing the conditions necessary 
for inference to be valid in regression can be 
extremely difficult. Similarly, when working 
with a small number of observations, assess-
ing multicollinearity can also be difficult. 
Under these conditions we generally proceed 
with inference unless we find strong evidence 
of a violation of the conditions necessary for 
inference to be valid in regression or a strong 
multicollinearity.
4.  To determine the independent variables to be 
included in a regression model when working 
with an extremely large sample, one can parti-
tion the sample into a training set and a vali-
dation set. The training set is used to estimate 
the regression coefficients and the validation 
set is then used to estimate the accuracy of the 
model.

 
4.6 Categorical Independent Variables 
161
Categorical Independent Variables
Thus far, the examples we have considered have involved quantitative independent vari-
ables such as distance traveled and number of deliveries. In many situations, however, we 
must work with categorical independent variables such as gender (male, female), method 
of payment (cash, credit card, check), and so on. The purpose of this section is to show 
how categorical variables are handled in regression analysis. To illustrate the use and inter-
pretation of a categorical independent variable, we will again consider the Butler Trucking 
Company example.
Butler Trucking Company and Rush Hour
Several of Butler Trucking’s driving assignments require the driver to travel on a congested 
segment of a highway during the afternoon rush hour period. Management believes this 
factor may also contribute substantially to variability in travel times. How do we incorpo-
rate information on which driving assignments include travel on a congested segment of a 
highway during the afternoon rush hour period into a regression model?
The previous independent variables we have considered (such as miles traveled and num-
ber of deliveries) have been quantitative, but this new variable is categorical and will require 
us to define a new type of variable called a dummy variable. To incorporate a variable that in-
dicates whether a driving assignment included travel on this congested segment of a highway 
during the afternoon rush hour period into a model that currently includes the independent 
variables miles traveled (x1) and number of deliveries (x2), we define the following variable:
0 if an assignment did not include travel on the congested segment of highway 
during afternoon rush hour
1 if an assignment included travel on the congested segment of highway  
during afternoon rush hour
x3 5
Once a value of one is input for each of the driving assignments that included travel on a 
congested segment of a highway during the afternoon rush hour period and a value of zero is 
input for each of the remaining driving assignments in the sample data, the independent vari-
able x3 can be included in the model. The file ButlerHighway includes this dummy variable.
Will this dummy variable add valuable information to the current Butler Trucking 
 regression model? A review of the residuals produced by the current model may help us 
make an initial assessment. Using Excel chart tools discussed in Chapter 2, we can create 
a frequency distribution and a histogram of the residuals for driving assignments that in-
cluded travel on a congested segment of a highway during the afternoon rush hour period. 
We then create a frequency distribution and a histogram of the residuals for driving assign-
ments that did not include travel on a congested segment of a highway during the afternoon 
rush hour period. The two histograms are shown in Figure 4.25.
Recall that the residual for the ith observation is ei 5 yi 2 y^i , which is the difference between 
the observed and predicted values of the dependent variable. The histograms in Figure 4.25 
show that driving assignments that included travel on a congested segment of a highway during 
the afternoon rush hour period tend to have positive residuals, which means we are generally 
underpredicting the travel times for those driving assignments. Conversely, driving assignments 
that did not include travel on a congested segment of a highway during the afternoon rush hour 
period tend to have negative residuals, which means we are generally overpredicting the travel 
times for those driving assignments. These results suggest that the dummy variable could po-
tentially explain a substantial proportion of the variance in travel time that is unexplained by the 
current model, and so we proceed by adding the dummy variable x3 to the current Butler Truck-
ing multiple regression model. Using Excel’s Regression tool to develop the estimated regres-
sion equation, we obtained the Excel output in Figure 4.26. The estimated regression equation is
y^ 5 20 3302 1 0 0672x1 1 0 6735x2 1 0 9980x3
(4.19)
4.6
Dummy variables are 
sometimes referred to as 
indicator variables.
file
WEB
ButlerHighway

162 
Chapter 4 Linear Regression
Interpreting the Parameters
After checking to make sure this regression satisfies the conditions for inference and the 
model does not suffer from serious multicollinearity, we can consider inference on our 
results. The Excel output in Figure 4.26 shows a p-value of 5.7766E-138 associated with 
the F test (F 5 750.4558); this provides a strong indication of an overall regression rela-
tionship. The p-values for the t tests of miles traveled (p-value 5 4.7852E-105), number of 
FIGURE 4.25   HISTOGRAMS OF THE RESIDUALS FOR DRIVING ASSIGNMENTS THAT INCLUDED 
TRAVEL ON A CONGESTED SEGMENT OF A HIGHWAY DURING THE AFTERNOON 
RUSH HOUR PERIOD AND RESIDUALS FOR DRIVING ASSIGNMENTS THAT DID NOT
70
60
50
40
30
20
10
0
–1.0 –
–1.5
–0.5 –
–1.0
0.0 –
–0.5
0.0 –
0.5
0.5 –
1.0
1.0 –
1.5
> 1.5
Frequency
Residuals
Included Highway - Rush Hour Driving
< –1.5
70
60
50
40
30
20
10
0
< –1.5 –1.0 –
–1.5
–0.5 –
–1.0
0.0 –
–0.5
0.0 –
0.5
0.5 –
1.0
1.0 –
1.5
> 1.5
Frequency
Residuals
Did Not Include Highway - Rush Hour Driving
FIGURE 4.26   EXCEL DATA AND OUTPUT FOR BUTLER TRUCKING WITH MILES TRAVELED (x1), 
NUMBER OF DELIVERIES (x2), AND THE HIGHWAY RUSH HOUR DUMMY VARIABLE 
(x3) AS THE INDEPENDENT VARIABLES
A
SUMMARY OUTPUT
Multiple R
0.940107228
0.8838016
0.882623914
0.663106426
300
3
–0.330229304
0.067220302
0.67351584
0.167677925
0.00196142
0.023619993
–1.969426232
34.27125147
28.51465081
0.04983651
4.7852E-105
6.74797E-87
–0.66022126
329.9830003
0.439710132
750.455757
5.7766E–138
0.063360208
0.627031441
–0.000237349
0.071080397
0.720000239
–0.764941128
0.062135243
0.612280051
0.104482519
0.072305362
0.734751629
0.9980033
0.076706582
13.0106605
6.49817E-31
0.847043924
1.148962677
0.799138374
1.196868226
989.9490008
130.1541992
1120.1032
296
299
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
Regression
Residual
Total
Intercept
Miles
Deliveries
Highway
Regression Statistics
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0%
Upper 99.0%
SS
MS
F
Signiﬁcance F
B
C
D
E
F
G
H
I
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

 
4.6 Categorical Independent Variables 
163
deliveries (p-value 5 6.7480E-87), and the rush hour driving dummy variable (p-value 5 
6.4982E-31) are all extremely small, indicating that each of these independent variables has 
a statistical relationship with travel time. The model estimates that travel time increases by:
●  0.0672 hours for every increase of 1 mile traveled, holding constant the number of 
deliveries and whether the driving assignment route requires the driver to travel on 
the congested segment of a highway during the afternoon rush hour period.
●  0.6735 hours for every delivery, holding constant the number of miles traveled and 
whether the driving assignment route requires the driver to travel on the congested 
segment of a highway during the afternoon rush hour period.
●  0.9980 hours if the driving assignment route requires the driver to travel on the 
congested segment of a highway during the afternoon rush hour period, holding 
constant the number of miles traveled and the number of deliveries.
In addition, R2 5 0.8838 indicates that the regression model explains approximately 
88.4 percent of the variability in travel time for the driving assignments in the sample. 
Thus, equation (4.19) should prove helpful in estimating the travel time necessary for the 
various driving assignments.
To understand how to interpret the parameters b0, b1, b2, and b3 when a categorical 
variable is present, consider the case when x3 5 0 (the driving assignment did not include 
travel on the congested segment of highway during the afternoon rush hour period). Using 
E(y|x3 5 0) to denote the mean or expected value of travel time for driving assignments 
given no rush hour driving, we have
 
E(y|x3 5 0) 5 b0 1 b1x1 1 b2 x2 1 b3(0) 5 b0 1 b1x1 1 b2 x2 
(4.20)
Similarly, using E(y|x3 5 1) to denote the mean or expected value of travel time for driving 
assignments given rush hour driving, we have
 
E(y|x3 5 1) 5 b0 1 b1x1 1 b2x2 1 b3(1) 5 b0 1 b1x1 1 b2x2 1 b3 
(4.21)
 
5 (b0 1 b3) 1 b1x1 1 b2 x2
Comparing equations (4.20) and (4.21), we see that the mean travel time is a linear 
function of x1 and x2 for both driving assignments that include travel on the congested 
 segment of highway during the afternoon rush hour period and driving assignments 
that do not. b1 and b2, are the same in both equations, but the y-intercept differs. The y-
intercept is b0 in equation (4.20) and (b0 1 b3) in equation (4.21). The interpretation of 
b3 is that it is the difference between the mean travel time for driving assignments that 
include travel on the congested segment of highway during the afternoon rush hour pe-
riod and the mean travel time for driving assignments that do not. For the Butler Truck-
ing Company example, if b3 is positive, the mean travel time for driving assignments 
that include travel on the congested segment of highway during the afternoon rush hour 
period exceeds the mean travel time for driving assignments that do not; if b3 is nega-
tive, the mean travel time for driving assignments that include travel on the congested 
segment of highway during the afternoon rush hour period is less than the mean travel 
time for driving assignments that do not. Finally, if b3 5 0, there is no difference in the 
mean travel time for driving assignments that include travel on the congested segment 
of highway during the afternoon rush hour period and the mean travel time for driving 
assignments that do not.
Using the estimated multiple regression equation y^ 5 20.3302 1 0.0672x1 1 0.6735x2 1 
0.9980x3, we see that, when x3 5 0 (the driving assignment does not include travel on the 
congested segment of highway during the afternoon rush hour period),
 
y^ 5 20.3302 1 0.0672x1 1 0.6735x2 1 0.9980(0) 
(4.22)
 
5 20.3302 1 0.0672x1 1 0.6735x2

164 
Chapter 4 Linear Regression
and when x3 5 1 (the driving assignment includes travel on the congested segment of 
 highway during the afternoon rush hour period)
 
y^ 5 20.3302 1 0.0672x1 1 0.6735x2 1 0.9980(1) 
(4.23)
 
5 0.6678 1 0.0672x1 1 0.6735x2
In effect, the use of a dummy variable provides two estimated regression equations that 
can be used to predict the travel time: One corresponds to driving assignments that include 
travel on the congested segment of highway during the afternoon rush hour period, and one 
corresponds to driving assignments that do not include such travel. In addition, with b3 5 
0.9980, we learn that on average, driving assignments that include travel on the congested 
segment of highway during the afternoon rush hour period take 0.9980 hours (or approxi-
mately 60 minutes) longer than driving assignments that do not include such travel.
More Complex Categorical Variables
The categorical variable for the Butler Trucking Company example had two levels: (1) 
driving assignments that include travel on the congested segment of highway during the 
afternoon rush hour period and (2) driving assignments that do not. As a result, defining a 
dummy variable with a value of zero indicating a driving assignment that does not include 
travel on the congested segment of highway during the afternoon rush hour period and 
a value of one indicating a driving assignment that includes such travel was sufficient. 
However, when a categorical variable has more than two levels, care must be taken in both 
defining and interpreting the dummy variables. As we will show, if a categorical variable 
has k levels, k – 1 dummy variables are required, with each dummy variable corresponding 
to one of the levels of the categorical variable and coded as 0 or 1.
For example, suppose a manufacturer of vending machines organized the sales territories 
for a particular state into three regions: A, B, and C. The managers want to use regression 
analysis to help predict the number of vending machines sold per week. With the number of 
units sold as the dependent variable, they are considering several independent variables (the 
number of sales personnel, advertising expenditures, etc.). Suppose the managers believe 
sales region is also an important factor in predicting the number of units sold. Because sales 
region is a categorical variable with three levels (A, B, and C), we will need 3 2 1 5 2 dummy 
variables to represent the sales region. Each variable can be coded 0 or 1 as follows:
1 if sales Region B,
0 otherwise
x1 5
1 if sales Region C
0 otherwise
x2 5
With this definition, we have the following values of x1 and x2:
Region
x1
x2
A
B
C
0
1
0
0
0
1
The regression equation relating the expected value of the number of units sold, E( y|x1, x2), 
to the dummy variables are written as
E( y|x1, x2) 5 b0 1 b1x1 1 b2 x2
Observations corresponding to Sales Region A are coded x1 5 0, x2 5 0, and the corre-
sponding regression equation is
E( y|x1 5 0, x2 5 0) 5 E( y|Sales Region A) 5 b0 1 b1(0) 1 b2(0) 5 b0
Observations corresponding to Sales Region B are coded x1 5 1, x2 5 0, and the correspond-
ing regression equation is
E(y|x 5 1 x 5 0) 5 E(y|Sales Region B) 5 b 1 b (1) 1 b (0) 5 b 1 b

 
4.7 Modeling Nonlinear Relationships 
165
Observations corresponding to Sales Region C are coded x1 5 0, x2 5 1, and the correspond-
ing regression equation is
E( y|x1 5 0, x2 5 1) 5 E( y|Sales Region C) 5 b0 1 b1(0) 1 b2(1) 5 b0 1 b2
Thus, b0 is the mean or expected value of sales for Region A, b1 is the difference between 
the mean number of units sold in Region B and the mean number of units sold in Region A, 
and b2 is the difference between the mean number of units sold in Region C and the mean 
number of units sold in Region A.
Two dummy variables were required because sales region is a categorical variable with three 
levels. But the assignment of x1 5 0 and x2 5 0 to indicate Sales Region A, x1 5 1 and x2 5 0 
to indicate Sales Region B, and x1 5 0 and x2 5 1 to indicate Sales Region C was arbitrary. For 
example, we could have chosen to let x1 5 1 and x2 5 0 indicate Sales Region A, x1 5 0 and  
x2 5 0 indicate Sales Region B, and x1 5 0 and x2 5 1 indicate Sales Region C. In this case, b0 is 
the mean or expected value of sales for Region B, b1 is the difference between the mean number of 
units sold in Region A and the mean number of units sold in Region B, and b2 is the difference be-
tween the mean number of units sold in Region C and the mean number of units sold in Region B.
The important point to remember is that when a categorical variable has k levels, k – 1 
dummy variables are required in the multiple regression analysis. Thus, if the sales region 
example had a fourth region, labeled D, three dummy variables would be necessary. For 
example, these three dummy variables can be coded as follows.
1 if sales Region B,
0 otherwise
x1 5
1 if sales Region C,
0 otherwise
x2 5
1 if sales Region D
0 otherwise
x3 5
Dummy variables are often 
used to model seasonal 
effects in sales data. If 
the data are collected 
quarterly, we may use three 
dummy variables defined in 
the following manner:
1 if spring  ;
0 otherwise
x1 5
1 if summer ;
0 otherwise
x2 5
1 if fall     
0 otherwise
x3 5
NOTES AND COMMENTS
Detecting multicollinearity when a cat-
egorical variable is involved is difficult. 
The correlation coefficient that we used in 
Section 4.5 is appropriate only when assess-
ing the relationship between two quantitative 
variables. However, recall that if any esti-
mated regression parameters b1, b2, . . . , bq 
or associated p-values change dramatically 
when a new independent variable is added to 
the model (or an existing independent vari-
able is removed from the model), multicol-
linearity is likely present. We can use our 
understanding of these ramifications of mul-
ticollinearity to assess whether there is multi-
collinearity that involves a dummy variable. 
We estimate the regression model twice; 
once with the dummy variable included as 
an independent variable and once with the 
dummy variable omitted from the regression 
model. If we see relatively little change in the 
estimated regression parameters b1, b2, . . . , bq 
or associated p-values for the independent 
variables that have been included in both  
regression models, we can be confident there 
is not a strong multicollinearity involving the 
dummy variable.
Modeling Nonlinear relationships
Regression may be used to model more complex types of relationships. To illustrate, let us 
consider the problem facing Reynolds, Inc., a manufacturer of industrial scales and labora-
tory equipment. Managers at Reynolds want to investigate the relationship between length 
of employment of their salespeople and the number of electronic laboratory scales sold. The 
file Reynolds gives the number of scales sold by 15 randomly selected salespeople for the 
most recent sales period and the number of months each salesperson has been employed 
by the firm. Figure 4.27, the scatter chart for these data, indicates a possible curvilinear 
relationship between the length of time employed and the number of units sold.
Before considering how to develop a curvilinear relationship for Reynolds, let us consider 
the Excel output in Figure 4.28 for a simple linear regression; the estimated regression is
Sales 5 113 7453 1 2 3675 Months Employed
4.7
file
WEB
Reynolds

166 
Chapter 4 Linear Regression
Although the computer output shows that the relationship is significant (p-value 5 9.3954E-06  
in cell E18 of Figure 4.28 for the t test that b1 5 0) and that a linear relationship explains a high 
percentage of the variability in sales (r 2 5 0.7901 in cell B5), the pattern in the scatter chart 
of residuals against the predicted values of the dependent variable in Figure 4.29 suggests 
that a curvilinear relationship may provide a better fit to the data (the scatter chart of residu-
als against the independent variable months employed would also suggest that a curvilinear 
relationship may provide a better fit to the data). This becomes more apparent when we review 
a scatter chart of the residuals and predicted values of the dependent variable.
If we have a practical reason to suspect a curvilinear relationship between number of 
electronic laboratory scales sold by a salesperson and the number of months the salesperson 
has been employed, we may wish to consider an alternative to simple linear regression. For 
example, we may believe that a recently hired salesperson faces a learning curve but becomes 
increasingly more effective over time and that a salesperson who has been in a sales position
FIGURE 4.27  SCATTER CHART FOR THE REYNOLDS EXAMPLE
400
350
300
250
200
150
100
50
00
20
40
60
80
100
Scales Sold
Months Employed
FIGURE 4.28  EXCEL OUTPUT FOR THE REYNOLDS EXAMPLE SIMPLE LINEAR REGRESSION
A
SUMMARY OUTPUT
Multiple R
0.888897515
0.790138792
0.773995622
48.49087146
15
1
113.7452874
2.367463621
20.81345608
0.338396631
5.464987985
6.996120545
0.000108415
9.39543E-06
68.78054927
115089.1933
2351.364615
48.94570268
9.39543E–06
1.636402146
158.7100256
3.098525095
68.78054927
1.636402146
158.7100256
3.098525095
115089.1933
30567.74
145656.9333
13
14
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
Regression
Residual
Total
Intercept
Months Employed
Regression Statistics
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 95.0%
Upper 95.0%
SS
MS
F
Signiﬁcance F
B
C
D
E
F
G
H
I
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

 
4.7 Modeling Nonlinear Relationships 
167
FIGURE 4.29   SCATTER CHART OF THE RESIDUALS AND PREDICTED  VALUES 
OF THE DEPENDENT VARIABLE FOR THE REYNOLDS SIMPLE 
LINEAR REGRESSION
–80
–60
–40
–20
50
100
150
200
250
300
350
400
0
Residuals
Predicted Values
20
40
60
80
100
with Reynolds for a long time eventually becomes burned out and becomes increasingly less 
effective. If our regression model supports this theory, Reynolds management can use the 
model to identify the approximate point in employment when its salespeople begin to lose their 
effectiveness, and management can plan strategies to counteract salesperson burnout.
Quadratic Regression Models
To account for the curvilinear relationship between months employed and scales sold that 
is suggested by the scatter chart of residuals against the predicted values of the dependent 
variable, we could include the square of the number of months the salesperson has been 
employed in the model as a second independent variable.
 
y 5 b0 1 b1x1 1 b2 x1
2 1 e 
(4.24)
As we can see in Figure 4.30, quadratic regression models are flexible and are capable of 
representing a wide variety of nonlinear relationships between an independent variable and 
the dependent variable.
To develop an estimated regression equation corresponding to this model, referred to as 
a quadratic regression model, the statistical software package we are using needs the origi-
nal data as well as the square of the number of months the employee has been with the firm. 
Figure 4.31 shows the Excel spreadsheet that includes the square of the number of months 
the employee has been with the firm. To create the variable, which we will call MonthsSq, 
we create a new column and set each cell in that column equal to the square of the associated 
value of the variable Months. These values are shown in Column B of Figure 4.31.
The regression output for the model in equation (4.24) is shown in Figure 4.32. The 
estimated regression equation is
Sales 5 61.4299 1 5.8198 Months Employed 2 0.0310 MonthsSq
where MonthsSq is the square of the number of months the salesperson has been employed. 
Because the value of b1 (5.8198) is positive, and the value of b2 (20.0310) is negative, y^ will 
initially increase as the number of months the salesperson has been employed increases. 
As the value of the independent variable Months Employed increases its squared value

168 
Chapter 4 Linear Regression
increases more rapidly, and eventually y^ will decrease as the number of months the sales-
person has been employed increases.
The R2 of 0.9013 indicates that this regression model explains approximately 90.2 per-
cent of the variation in Scales Sold for our sample data. The lack of a distinct pattern in 
the scatter chart of residuals against the predicted values of the dependent variable (Fig-
ure 4.33) suggests that the quadratic model fits the data better than the simple linear regres-
sion in the Reynolds example (the scatter chart of residuals against the independent variable 
Months Employed would also lead us to this conclusion)
FIGURE 4.30   RELATIONSHIPS THAT CAN BE FIT WITH A QUADRATIC 
 REGRESSION MODEL
y
x
(b) 1 , 0, 2 . 0
(a) 1 . 0, 2 . 0
(c) 1 . 0, 2 , 0
(d) 1 , 0, 2 , 0
y
x
y
x
y
x
If b2 . 0 the function is bowl 
shaped relative to the x-axis, 
it is convex; if b2 , 0 the 
function is mound shaped 
relative to the x-axis, it is 
concave.
FIGURE 4.31   EXCEL DATA FOR THE REYNOLDS QUADRATIC  REGRESSION MODEL
A
B
C
Months Employed
MonthsSq
Scales Sold
1
2
3
4
5
6
7
41
106
76
100
22
12
85
111
40
51
0
12
6
56
19
1,681
11,236
5,776
10,000
484
144
7,225
12,321
1,600
2,601
0
144
36
3,136
361
275
296
317
376
162
150
367
308
189
235
83
112
67
325
189
8
9
10
11
12
13
14
15
16

 
4.7 Modeling Nonlinear Relationships 
169
Although it is difficult to assess from a sample as small as this whether the regression 
model satisfies the conditions necessary for reliable inference, we see no marked violations 
of these conditions, so we will proceed with hypothesis tests of the regression parameters 
b0, b1, and b2 for our quadratic regression model.
From the Excel output for the model in equation (4.24) provided in Figure 4.32, 
we see that at the 0.05 level of significance the output shows that the overall model is
FIGURE 4.32  EXCEL OUTPUT FOR THE REYNOLDS QUADRATIC REGRESSION MODEL
A
B
C
D
E
F
G
H
I
SUMMARY OUTPUT
Multiple R
R Square
Adjusted
R Square
Standard Error
Observations
Regression 
Residual
Total
ANOVA
0.949361402
0.901287072
0.884834917
34.61481184
15
2
12
14
61.42993467
5.819796648
20.031009589
Intercept
Months
Employed
MonthsSq
20.57433536
0.969766536
0.008436087
0.011363561
6.20497E-05
0.003172962
16.60230882
3.706856877
20.049390243
106.2575605
7.93273642
20.012628935
21.415187222
2.857606371
20.05677795
124.2750566
8.781986926
20.005241228
2.985755485
6.001234761
23.675826286
131278.711
14378.22238
145656.9333
65639.35548 54.78231208
9.25218E-07
1198.185199
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Regression Statistics
df
SS
MS
F
Signiﬁcance F
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0%
Upper 99.0%
FIGURE 4.33   SCATTER CHART OF THE RESIDUALS AND PREDICTED 
 VALUES OF THE DEPENDENT VARIABLE FOR THE REYNOLDS 
 QUADRATIC REGRESSION MODEL
60
40
20
0
–20
–40
–600
50
100
150
200
250
300
350
Residuals
Predicted Values

170 
Chapter 4 Linear Regression
significant (p-value for the F test is 9.2522E-07). We also note that the p-values corre-
sponding to the t statistics for Months Employed (6.2050E-05) and MonthsSq (0.0032) 
are both substantially less than 0.05, and hence we can conclude that adding MonthsSq 
to the model involving Months is significant. There is a nonlinear relationship between 
months and sales.
Note that if the estimated regression parameters b1 and b2 corresponding to the linear 
term x and the squared term x2 are of the same sign, the estimated value of the dependent 
variable is either increasing over the experimental range of x (when b1 . 0 and b2 . 0) 
or decreasing over the experimental range of x (when b1 , 0 and b2 , 0). If the estimated 
regression parameters b1 and b2 corresponding to the linear term x and the squared term x2 
have different signs, the estimated value of the dependent variable has a maximum over 
the experimental range of x (when b1 . 0 and b2 , 0) or a minimum over the experimental 
range of x (when b1 , 0 and b2 . 0). In these instances, we can find the estimated maxi-
mum or minimum over the experimental range of x by finding the value of x at which the 
estimated value of the dependent variable stops increasing and begins decreasing (when a 
maximum exists) or stops decreasing and begins increasing (when a minimum exists). For 
example, we estimate that when months employed increases by 1 from some value x to x 1 1, 
sales changes by
 
5.8198 [(x 1 1) 2 x] 2 0.0310 [(x 1 1)2 2 x2]
5 5.8198 (x 2 x 1 1) 2 0.0310 (x2 1 2x 1 1 2 x2)
5 5.8198 2 0.0310 (2x 1 1)
5 5.7888 2 0.0620x
That is, estimated Sales initially increases as Months Employed increases and then eventu-
ally decreases as Months Employed increases. Solving this result for x
 
5.7888 2 0.0620x 5 0
 
20.0620x 5 25.7888
 
x 5 25.7888
20.0620 5 93.3387
tells us that estimated maximum sales occurs at approximately 93 months (in about seven 
years and nine months). We can then find the estimated maximum value of the dependent 
variable Sales by substituting this value of x into the estimated regression equation:
Sales 5 61.58198 1 5.8198 (93.3387) 2 0.0310 (93.33872) 5 334.4909
At approximately 93 months, the maximum estimated sales of approximately 334 scales 
occurs.
Piecewise Linear Regression Models
As an alternative to a quadratic regression model, we can recognize that below some value 
of Months Employed, the relationship between Months Employed and Sales appears to be 
positive and linear, whereas the relationship between Months Employed and Sales appears to 
be negative and linear for the remaining observations. A piecewise linear regression model 
will allow us to fit these relationships as two linear regressions that are joined at the value of 
Months at which the relationship between Months Employed and Sales changes.
Our first step in fitting a piecewise linear regression model is to identify the value 
of the independent variable Months Employed at which the relationship between Months 
Employed and Sales changes; this point is called the knot, or breakpoint. Although theory 
should determine this value, analysts often use the sample data to aid in the identification 
of this point. Figure 4.34 provides the scatter chart for the Reynolds data with an indication 
In business analytics 
applications, polynomial 
regression models of higher 
than second or third order 
are rarely used.
A piecewise linear regres-
sion model is sometimes 
referred to as a segment 
regression or a spline 
model.

 
4.7 Modeling Nonlinear Relationships 
171
of the possible location of the knot, which we have denoted x (k). From this scatter chart, it 
appears the knot is at approximately 90 months.
Once we have decided on the location of the knot, we define a dummy variable that is 
equal to zero for any observation for which the value of Months Employed is less than or 
equal to the value of the knot, and equal to one for any observation for which the value of 
Months Employed is greater than the value of the knot:
 
0 if x1 # x(k)
1 if x1 . x(k)
xk 5
 
(4.25)
where
 
  x1 5 Months
 
x(k) 5 the value of the knot (90 months for the Reynolds example)
 
   xk 5 the knot dummy variable
We then fit the following regression model:
 
y 5 b0 1 b1x1 1 b21x1 2 x1k22xk 1 e 
(4.26)
The data and Excel output for the Reynolds piecewise linear regression model are 
provided in Figure 4.35. Because we placed the knot at x(k) 5 90, the estimated regression 
model is
 
y^ 5 87.2172 1 3.4094x1 2 7.8726(x1 2 90)xk
At the 0.05 level of significance, the output shows that the overall model is significant (p-
value for the F test is 4.1755E-07). Note also that the p-value corresponding to the t statistic 
for knot term (p-value 5 0.0014) is less than 0.05, and hence we can conclude that adding 
the knot to the model with Months Employed as the independent variable is significant.
But what does this model mean? For any value of Months less than or equal to 90, 
the knot term 7.8726(x1 2 90)xk is zero because the knot dummy variable xk 5 0, so the 
regression model is 
y^ 5 87.2172 1 3.4094x1
FIGURE 4.34   POSSIBLE POSITION OF KNOT x(k)
100
200
300
400
20
40
60
80
x(k) 100
120
Months Employed
Scales Sold
0

172 
Chapter 4 Linear Regression
For any value of Months Employed greater than 90, the knot term is 27.87(x1 2 90) be-
cause the knot dummy variable xk 5 1, so the regression model is 
y^  5 87.2172 1 3.4094x1 2 7.8726(x1 2 90)  
5 87.2172 2 7.8726(290) 1 (3.4094 2 7.8726)x1 5 795.7512 2 4.4632x1
Note that if Months Employed is equal to 90, both regressions yield the same value of y^:
 
y^ 5 87.2172 1 3.4094(90) 5 795.7512 2 4.4632(90) 5 394.06
So the two regression segments are joined at the knot.
FIGURE 4.35   DATA AND EXCEL OUTPUT FOR THE REYNOLDS PIECEWISE LINEAR REGRESSION 
MODEL
The variable Knot 
Dummy*Months is the 
product of the correspond-
ing values of Knot Dummy 
and the difference between 
Months Employed and 
the knot value, i.e., C2 5 
A2 * (B2 – 90) in this Excel 
spreadsheet.
A
B
C
D
E
F
G
H
I
Knot Dummy
Months
Employed
Knot
Dummy* Months
Scales
Sold
Multiple R
Regression
Residual
Total
Intercept
Months Employed
Knot Dummy* Months
SUMMARY OUTPUT
ANOVA
Regression Statistics
0.955796127
R Square
0.913546237
Adjusted R Square
0.899137276
32.3941739
15
2
12
14
Standard Error
Observations
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Lower 99.0% Upper 99.0%
Upper 95%
SS
MS
F
Signiﬁcance F
0
0
275
296
317
376
162
150
367
308
189
235
83
112
67
325
189
16
0
10
0
0
0
21
0
0
0
0
0
0
0
106
41
76
100
22
12
85
111
40
51
0
12
6
56
19
1
0
1
0
0
0
1
0
0
0
0
0
0
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
133064.3433
87.21724231
133.9841531
53.85825572
2.67220742
–12.01699634
120.5762289
4.146656538
–3.728110179
40.45033153
2.375895931
–13.68276572
4.442968028
–2.062340794
5.696517369
10.07632484
–4.138751508
15.31062519
0.338360666
1.902156543
9.9677E-05
3.2987E-07
0.00137388
3.409431979
–7.872553259
66532.17165
63.4012588
4.17545E-07
1049.382502
12592.59003
145656.9333

 
4.7 Modeling Nonlinear Relationships 
173
The interpretation of this model is similar to the interpretation of the quadratic regression 
model. A salesperson’s sales are expected to increase by 3409.4 electronic laboratory scales 
for each month of employment until the salesperson has been employed for 90 months. At 
that point the salesperson’s sales are expected to decrease by 4463.1 (because 3409.4 2 
7872.5 5 24463.1) electronic laboratory scales for each additional month of employment.
Should we use the quadratic regression model or the piecewise linear regression model? 
These models fit the data equally well, and both have reasonable interpretations, so we 
cannot differentiate between the models on either of these criteria. Thus, we must consider 
whether the abrupt change in the relationship between Sales and Months Employed that is 
suggested by the piecewise linear regression model captures the real relationship between 
Sales and Months Employed better than the smooth change in the relationship between 
Sales and Months Employed suggested by the quadratic model.
Interaction Between Independent Variables
Often the relationship between the dependent variable and one independent variable is 
different at various values of a second independent variable. When this occurs, it is called 
an interaction. If the original data set consists of observations for y and two independent 
variables x1 and x2, we can incorporate an x1x2 interaction into the multiple linear regression 
in the following manner.
 
y 5 b0 1 b1x1 1 b2 x2 1 b3 x1 x2 1 e 
(4.27)
To provide an illustration of interaction and what it means, let us consider the regression 
study conducted by Tyler Personal Care for one of its new shampoo products. Two factors 
believed to have the most influence on sales are unit selling price and advertising expenditure. 
To investigate the effects of these two variables on sales, prices of $2.00, $2.50, and $3.00 
were paired with advertising expenditures of $50,000 and $100,000 in 24 test markets.
The data collected by Tyler are provided in the file Tyler. Note that the sample mean 
sales corresponding to a price of $2.00 and an advertising expenditure of $50,000 is 
461,000 units and that the sample mean sales corresponding to a price of $2.00 and an 
advertising expenditure of $100,000 is 808,000 units. Hence, with price held constant 
at $2.00, the difference in mean sales between advertising expenditures of $50,000 and 
$100,000 is 808,000 2 461,000 5 347,000 units. When the price of the product is $2.50, 
the difference in mean sales between advertising expenditures of $50,000 and $100,000 is  
646,000 2 364,000 5 282,000 units. Finally, when the price is $3.00, the difference in mean 
sales between advertising expenditures of $50,000 and $100,000 is 375,000 2 332,000 5 
43,000 units. Clearly, the difference in mean sales between advertising expenditures of 
$50,000 and $100,000 depends on the price of the product. In other words, at higher sell-
ing prices, the effect of increased advertising expenditure diminishes. These observations 
provide evidence of interaction between the price and advertising expenditure variables.
To provide another perspective of interaction, Figure 4.36 shows the sample mean 
sales for the six price–advertising expenditure combinations. This graph also shows that 
the relationship between advertising expenditure and mean sales depends on the price of 
the product; we again see the effect of interaction.
When interaction between two variables is present, we cannot study the relationship 
between one independent variable and the dependent variable y independently of the other 
variable. In other words, meaningful conclusions can be developed only if we consider the 
joint relationship that both independent variables have with the dependent variable. To ac-
count for the effect of interaction, we use the regression model in equation (4.27) where
 
y 5 Unit Sales (1000s)
 
x1 5 Price ($)
 
x2 5 Advertising Expenditure ($1,000s)
Multiple knots can be used 
to fit complex piecewise 
linear regressions.
file
WEB
Tyler
The data for the independent 
variable Price is in column 
A, the independent variable 
Advertising Expenditures is in 
column B, and the dependent 
variable Sales is in column 
D of the Tyler file. We 
create the interaction vari-
able Price*Advertising in 
column C by first inserting 
a column between column B 
(Advertising Expenditures) 
and column C (Sales) in the 
Tyler file (so the values of 
the independent variables 
will be in contiguous col-
umns), entering the function 
5A2*B2 in cell C2, and 
then copying cell C2 into 
cells C3 through C25.

174 
Chapter 4 Linear Regression
Note that the regression model in equation (4.27) reflects Tyler’s belief that the number 
of units sold is related to selling price and advertising expenditure (accounted for by the 
b1x1 and b2x2 terms) and an interaction between the two variables (accounted for by the 
b3x1x2 term).
The Excel output corresponding to the interaction model for the Tyler Personal Care 
example is provided in Figure 4.37.
The resulting estimated regression equation is
 
Sales 5 2275.8333 1 175 Price 1 19.68 Advertising 2 6.08 Price*Advertising
Because the model is significant (the p-value for the F test is 9.2588E-17) and the p-value 
corresponding to the t test for Price*Advertising is 8.6772E-10, we conclude that interac-
tion is significant. Thus, the regression results show that the relationship between advertis-
ing expenditure and sales depends on the price (and vice versa).
Our initial review of these results may alarm us: How can price have a positive esti-
mated regression coefficient? With the exception of luxury goods, we expect sales to de-
crease as price increases. Although this result appears counterintuitive, we can make sense 
FIGURE 4.36   MEAN UNIT SALES (1000s) AS A FUNCTION OF SELLING PRICE 
AND ADVERTISING EXPENDITURES
400
500
700
Selling Price ($)
Mean Unit Sales (1,000s)
300
2.00
2.50
3.00
600
800
900
$50,000
$50,000
$50,000
$100,000
$100,000
$100,000
Difference
of
808 – 461
= 347
Difference
of
646 – 364
= 282
Difference
of
375 – 332
= 43

 
4.7 Modeling Nonlinear Relationships 
175
of this model if we work through the interpretation of the interaction. In other words, the 
relationship between the independent variable Price and the dependent variable Sales is dif-
ferent at various values of Advertising (and the relationship between the independent vari-
able Advertising and the dependent variable Sales is different at various values of Price).
It becomes easier to see how the predicted value of Sales depends on Price by using the 
estimated regression model to consider the effect when Price increases by $1:
Sales After Price Increase 5  2275.8333 1 175 (Price 1 1)  
1 19.68 Advertising 2 6.08 (Price 1 1) * Advertising
Thus,
Sales After Price Increase 2 Sales Before Price Increase 5 175 2 6.08 Advertising Expenditure
So the change in the predicted value of the dependent variable that occurs when the inde-
pendent variable Price increases by $1 depends on how much was spent on advertising.
Consider a concrete example. If Advertising Expenditures is $50,000 when price is 
$2.00, we estimate sales to be
Sales 5 2275.8333 1 175 (2) 1 19.68 (50) 2 6.08 (2) (50) 5 450.1667, or 450,167 units
At the same level of Advertising Expenditures ($50,000) when price is $3.00, we  estimate 
sales to be
Sales 5 2275.8333 1 175 (3) 1 19.68 (50) 2 6.08 (3) (50) 5 321.1667, or 321,167 units
So when Advertising Expenditures is $50,000, a change in price from $2.00 to $3.00 results 
in a 450,167 2 321,167 5 129,000-unit decrease in estimated sales. However, if Advertis-
ing Expenditures is $100,000 when price is $2.00, we estimate sales to be
Sales 5 2275.8333 1 175 (2) 1 19.68 (100) 2 6.08 (2) (100) 5 826.1667, or 826,167 units
FIGURE 4.37   EXCEL OUTPUT FOR THE TYLER PERSONAL CARE LINEAR REGRESSION MODEL 
WITH INTERACTION
A
SUMMARY OUTPUT
Multiple R
0.988993815
0.978108766
0.974825081
28.17386496
24
3
–275.8333333
175
112.8421033
44.54679188
–2.444418575
3.928453489
0.023898351
0.0008316
–511.2178361
236438.6667
793.7666667
297.8692
9.25881E-17
82.07702045
–40.44883053
267.9229796
–596.9074508
48.24924412
45.24078413
301.7507559
709316
15875.33333
725191.3333
20
23
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
Regression
Residual
Total
Intercept
Price
Regression Statistics
df
Coefﬁcients
Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0%
Upper 99.0%
SS
MS
F
Signiﬁcance F
B
C
D
E
F
G
H
I
19.68
–6.08
1.42735225
0.563477299
13.78776683
–10.79014187
1.1263E-11
8.67721E-10
16.70259538
–7.255393049
22.65740462
–4.904606951
15.61869796
–7.683284335
23.74130204
–4.476715665
Advertising Expenditure
($1,000s)
Price*Advertising
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

176 
Chapter 4 Linear Regression
At the same level of Advertising Expenditures ($100,000) when price is $3.00, we estimate 
sales to be
Sales 5 2275.8333 1 175 (3) 1 19.68 (100) 2 6.08 (3) (100) 5 393.1667, or 393,167 units
So when Advertising Expenditures is $100,000, a change in price from $2.00 to $3.00 
results in a 826,167 2 393,167 5 433,000-unit decrease in estimated sales. When Tyler 
spends more on advertising, its sales are more sensitive to changes in price. Perhaps at 
larger Advertising  Expenditures, Tyler attracts new customers who have been buying 
the product from another company and so are more aware of the prices charged for the 
product by Tyler’s  competitors.
There is a second and equally valid interpretation of the interaction; it tells us that the 
relationship between the independent variable Advertising Expenditure and the dependent 
variable Sales is different at various values of Price. Using the estimated regression model 
to consider the effect when Advertising Expenditure increases by $1,000:
Sales After Advertising Increase 5  2275.8333 1 175 Price 1 19.68 (Advertising 1 1)  
26.08 Price * (Advertising 1 1) 
Thus,
Sales After Advertising Increase 2 Sales Before Advertising Increase 5 19.68 2 6.08 Price
So the change in the predicted value of the dependent variable that occurs when the inde-
pendent variable Advertising Expenditure increases by $1000 depends on the price.
We return to our concrete example, but now we now hold Price constant and change 
Advertising Expenditure. If Price is $2.00 when Advertising Expenditures is $50,000, we 
estimate sales to be
Sales 5 2275.8333 1 175 (2) 1 19.68 (50) 2 6.08 (2) (50) 5 450.1667, or 450,167 units
At the same level of Price ($2.00) when Advertising Expenditures is $100,000, we estimate 
sales to be
Sales 5 2275.8333 1 175 (2) 1 19.68 (100) 2 6.08 (2) (100) 5 826.1667, or 826,167 units
So when Price is $2.00, a change in Advertising Expenditures from $50,000 to $100,000 
results in a 826,167 2 450,167 5 376,000-unit increase in estimated sales. However, if 
Price is $3.00 when  Advertising Expenditures is 50,000, we estimate sales to be
Sales 5 2275.8333 1 175 (3) 1 19.68 (50) 2 6.08 (3) (50) 5 321.1667, or 321,167 units
At the same level of Price ($3.00) when Advertising Expenditures is $100,000, we estimate 
sales to be
Sales 5 2275.8333 1 175 (3) 1 19.68 (100) 2 6.08 (3) (100) 5 393.1667, or 393,167 units
So when Advertising Expenditures is $100,000, a change in price from $2.00 to $3.00 
results in a 393,167 2 321,167 5 72,000-unit increase in estimated sales. When the price 
of Tyler’s product is high, its sales are less sensitive to changes in advertising expenditure. 
Perhaps as Tyler increases its price, it must advertise more to convince potential customers 
that its product is a good value.
Note that we can combine a quadratic effect with interaction to produce a second-order 
polynomial model with interaction between the two independent variables. The model 
 obtained is
 
y 5 b0 1 b1x1 1 b2 x2 1 b3 x1
2
 1 b4 x2
2
 1 b5 x1x2 1 e 
(4.28)
This model provides a great deal of flexibility in capturing nonlinear effects.

 
4.8 Model Fitting 
177
Model Fitting
Finding an effective regression model can be a challenging task. Although we rely on theory 
to guide us, often we are faced with a large number of potential independent variables from 
which to choose. In this section we discuss common algorithms for building a regression 
model and the potential hazards of these and other similar algorithms.
Variable Selection Procedures
When there are many independent variables to consider, special procedures are sometimes 
employed to select the independent variables to include in the regression model. These vari-
able selection procedures include stepwise regression, the forward selection procedure, the 
4.8
NOTES AND COMMENTS
1.  Just as a dummy variable can be used to allow 
for different y-intercepts for the two groups 
represented by the dummy, we can use an in-
teraction between a dummy variable and a 
quantitative independent variable to allow for 
different relationships between independent 
and dependent variables for the two groups 
represented by the dummy. Consider the Butler 
Trucking example: Travel time is the dependent 
variable y, miles traveled and number of deliv-
eries are the quantitative independent variables 
x1 and x2, and the dummy variable x3 differenti-
ates between driving assignments that included 
travel on a congested segment of a highway and 
driving assignments that did not. If we believe 
that the relationship between miles traveled and 
travel time differs for driving assignments that 
included travel on a congested segment of a 
highway and those that did not, we could create 
a new variable that is the interaction between 
miles traveled and the dummy variable (x4 5 
x1*x3) and estimate the following model:
y^ 5 b0 1 b1x1 1 b2 x2 1 b3 x4
If a driving assignment does not include travel 
on a congested segment of a highway, x4 5 
x1*x3 5 x1*(0) 5 0 and the regression model is
y^ 5 b0 1 b1x1 1 b2 x2 
If a driving assignment does include travel on a 
congested segment of a highway, x4 5 x1*x3 5 
x1*(1) 5 x1 and the regression model is
y^  5 b0 1 b1x1 1 b2 x2 1 b3 x1(1)  
5 b0 1 (b1 1 b3)x1 1 b2x2
So in this regression model b1 is the estimate 
of the relationship between miles traveled and 
travel time for driving assignments that do not 
include travel on a congested segment of a high-
way, and b1 1 b3 is the estimate of the relation-
ship between miles traveled and travel time for 
driving assignments that do include travel on a 
congested segment of a highway.
2.  Multicollinearity can be divided into two types. 
Data-based multicollinearity occurs when sepa-
rate independent variables that are related are 
included in the model, whereas structural multi-
collinearity occurs when a new independent 
variable is created by taking a function of one or 
more existing independent variables. If we use 
ratings that consumers give on bread’s aroma 
and taste as independent variables in a model for 
which the dependent variable is the overall rat-
ing of the bread, the multicollinearity that would 
exist between the aroma and taste ratings is an 
example of data-based multicollinearity. If we 
build a quadratic model for which the indepen-
dent variables are ratings that consumers give 
on bread’s aroma and the square of the ratings 
that consumers give on bread’s aroma, the mul-
ticollinearity that would exist is an example of 
structural multicollinearity.
3.  Structural multicollinearity occurs naturally in 
polynomial regression models and regression 
models with interactions. You can greatly re-
duce the structural multicollinearity in a poly-
nomial regression by centering the independent 
variable x (using x 2 x in place of x). In a re-
gression model with interaction, you can greatly 
reduce the structural multicollinearity by cen-
tering both independent variables that interact. 
However, quadratic regression models and re-
gression models with interactions are frequently 
used only for prediction; in these instances cen-
tering independent variables is not necessary 
because we are not concerned with inference.

178 
Chapter 4 Linear Regression
sequential replacement procedure, and best-subsets procedure. Given a data set with several 
possible independent variables, we can use these procedures to identify which independent 
variables provide a model that best satisfies some criterion. The first four procedures are 
iterative; at each step of the procedure a single independent variable is added or removed 
and the new model is evaluated. The process continues until a stopping criterion indicates 
that the procedure cannot find a superior model. The last procedure (best-subsets) is not a 
one-variable-at-a-time procedure; it evaluates regression models involving different sub-
sets of the independent variables.
The forward selection procedure begins with none of the independent variables un-
der consideration included in the regression model. The analyst establishes a criterion 
for allowing independent variables to enter the model (such as the independent variable 
j with the smallest p-value associated with the test of the hypothesis bj 5 0, subject to 
some predetermined maximum p-value for which a potential independent variable will 
be allowed to enter the model). In the first step of the procedure, the independent vari-
able that best satisfies the criterion is added to the model. In each subsequent step, the 
remaining independent variables not in the current model are evaluated, and the one that 
best satisfies the criterion is added to the model. The procedure stops when there are no 
independent variables not currently in the model that meet the criterion for being added 
to the regression model.
The backward procedure begins with all of the independent variables under consid-
eration included in the regression model. The analyst establishes a criterion for allowing 
independent variables to remain in the model (such as the largest p-value associated 
with the test of the hypothesis bj 5 0, subject to some predetermined minimum p-value 
for which a potential independent variable will be allowed to remain in the model). In 
the first step of the procedure, the independent variable that violates this criterion to the 
greatest degree is removed from the model. In each subsequent step, the independent 
variables in the current model are evaluated, and the one that violates this criterion to 
the greatest degree is removed from the model. The procedure stops when there are no 
independent variables currently in the model that violate the criterion for remaining in 
the regression model.
Similar to the forward selection procedure, the stepwise procedure begins with none 
of the independent variables under consideration included in the regression model. The 
analyst establishes both a criterion for allowing independent variables to enter the model 
and a criterion for allowing independent variables to remain in the model. In the first step 
of the procedure, the independent variable that best satisfies the criterion for entering the 
model is added. Each subsequent step involves two intermediate steps. First, the remain-
ing independent variables not in the current the model are evaluated, and the one that best 
satisfies the criterion for entering is added to the model. Then the independent variables in 
the current the model are evaluated, and the one that violates the criterion for remaining 
in the model to the greatest degree is removed. The procedure stops when no independent 
variables not currently in the model meet the criterion for being added to the regression 
model, and no independent variables currently in the model violate the criterion for remain-
ing in the regression model.
In the best-subsets procedure, simple linear regressions for each of the independent 
variables under consideration are generated, and then the multiple regressions with all 
combinations of two independent variables under consideration are generated, and so on. 
Once a regression has been generated for every possible subset of the independent variables 
under consideration has been generated, an output that provides some criteria for selecting 
from these regression models is produced for all models generated.
Although these algorithms are potentially useful when dealing with a large num-
ber of potential independent variables, they do not necessarily provide useful models. 
Once the procedure terminates, you should deliberate whether the combination of 
The stepwise procedure re-
quires that the criterion for 
an independent variable to 
enter the regression model 
is more difficult to satisfy 
than the criterion for an 
independent variable to be 
removed from the regres-
sion model. This require-
ment prevents the same 
independent variable from 
exiting and then reentering 
the regression model in the 
same step.
In addition to these vari-
ables procedures, XLMiner 
also provides a sequential 
replacement algorithm in 
which, for a given number 
of independent variables, 
individual independent 
variables are sequentially 
replaced and replacements 
that improve performance 
are retained.

 
4.8 Model Fitting 
179
 independent variables included in the final regression model makes sense from a prac-
tical standpoint and consider whether you can create a more useful regression model 
with more meaningful interpretation through the addition or removal of independent 
variables. Use your own judgment and intuition about your data to refine the results 
of these algorithms.
Overfitting
The objective in building a regression model (or any other type of mathematical model) is 
to provide the simplest accurate representation of the population. A model that is relatively 
simple will be easy to understand, interpret, and use, and a model that accurately represents 
the population will yield meaningful results.
When we base a model on sample data, we must be wary. Sample data generally do 
not perfectly represent the population from which they are drawn; if we attempt to fit a 
model too closely to the sample data, we risk capturing behavior that is idiosyncratic to the 
sample data rather than representative of the population. When the model is too closely fit 
to sample data and as a result does not accurately reflect the population, the model is said 
to have been overfit.
Overfitting generally results from creating an overly complex model to explain id-
iosyncrasies in the sample data. In regression analysis, this often results from the use 
of complex functional forms or independent variables that do not have meaningful re-
lationships with the dependent variable. If a model is overfit to the sample data, it will 
perform better on the sample data used to fit the model than it will on other data from 
the population. Thus, an overfit model can be misleading about its predictive capability 
and its interpretation.
Overfitting is a difficult problem to detect and avoid. How does one avoid overfitting 
a model?
●  Use only independent variables that you expect to have real and meaningful rela-
tionships with the dependent variable.
●  Use complex models, such as quadratic models and piecewise linear regression 
models, only when you have a reasonable expectation that such complexity pro-
vides a more accurate depiction of what you are modeling.
●  Do not let software dictate your model. Use iterative modeling procedures, such as 
the stepwise and best-subsets procedures, only for guidance and not to generate your 
final model. Use your own judgment and intuition about your data and what you are 
modeling to refine your model.
●  If you have access to a sufficient quantity of data, assess your model on data 
other than the sample data that were used to generate the model (this is referred 
to as cross-validation). In such cases, it is recommended to divide the original 
sample data into training and validation sets. The training set is the data set used 
to build the candidate models that appear to make practical sense. The valida-
tion set is the set of data used to compare model performances and ultimately 
pick a model for predicting values of the dependent variable. For example, we 
might randomly select half of the data for use in developing regression models. 
We could use these data as our training set to estimate a model or a collection 
of models that appear to perform well. Finally, we use the remaining half of the 
data as a validation set to assess and compare the models’ performances and 
ultimately select the model that minimizes some measure of overall error when 
applied to the validation set.
Observing these guidelines will reduce the risk of overfitting, but one must always be wary 
of the potential for overfitting when interpreting and assessing a model.
The principle of using the 
simplest meaningful model 
possible without sacrificing 
accuracy is referred to as 
Ockham’s razor, the law 
of parsimony, or the law of 
economy.
Validation data sets are 
sometimes referred to as 
holdout samples. XLMiner 
allows the user to easily 
 divide data sets into train-
ing and validation sets for 
use with regression models.

180 
Chapter 4 Linear Regression
Summary
In this chapter we showed how regression analysis can be used to determine how a depen-
dent variable y is related to an independent variable x. In simple linear regression, the re-
gression model is y 5 b0 1 b1x1 1 . The simple linear regression equation E( y|x) 5 b0 1 
b1x1 describes how the mean or expected value of y is related to x. We use sample data and 
the least squares method to develop the estimated regression equation y^ 5 b0 1 b1x1. In 
effect, b0 and b1 are the sample statistics used to estimate the unknown model parameters. 
The coefficient of determination r2 was presented as a measure of the goodness of fit for 
the estimated regression equation; it can be interpreted as the proportion of the variation in 
the sample values of the dependent variable y that can be explained by the estimated regres-
sion equation. We then extended our discussion to include multiple independent variables 
and reviewed how to use Excel to find the estimated multiple regression equation y^ 5 b0 1 
b1x1 1 b2x2 1 . . . 1 bq xq, and we considered the interpretations of the parameter estimates 
in multiple regression and the ramifications of multicollinearity.
The assumptions related to the regression model and its associated error term  were 
discussed. We introduced the F test for determining whether there is a statistically signifi-
cant overall relationship between the dependent variable and the set of all independent vari-
ables, and we reviewed the t test for determining whether there is a statistically significant 
relationship between the dependent variable and an individual independent variable given 
the other independent variables in the regression model. We showed how to use Excel to 
develop confidence interval estimates of the regression parameters b0, b1, . . . , bq, and we 
discussed the special case of inference with very large samples.
We showed how to incorporate categorical independent variables into a regression 
model through the use of dummy variables, and we discussed a variety of ways to use 
multiple regression to fit nonlinear relationships between independent variables and the 
dependent variable. We concluded with a discussion of various automated procedures for 
selecting independent variables to include in a regression model and consideration of the 
problem of overfitting a regression model.
Glossary
Regression analysis A statistical procedure used to develop an equation showing how the 
variables are related.
Dependent variable The variable that is being predicted or explained. It is denoted by y 
and is often referred to as the response.
Independent variable(s) The variable (or variables) used for predicting or explaining val-
ues of the dependent variable. It is denoted by x and is often referred to as the predictor 
variable.
Simple regression Regression analysis involving one dependent variable and one inde-
pendent variable.
Linear regression Regression analysis in which relationships between the independent 
variables and the dependent variable are approximated by a straight line.
Multiple regression Regression analysis involving one dependent variable and more than 
one independent variable.
Regression model The equation that describes how the dependent variable y is related to an inde-
pendent variable x and an error term; the simple linear regression model is y 5 b0 1 b1x 1 , and 
the multiple linear regression model is y 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq 1 .
Parameter A measurable factor that defines a characteristic of a population, process, 
or system.

 
Glossary 
181
Random variable The outcome of a random experiment (such as the drawing of a random 
sample) and so represents an uncertain outcome.
Regression equation The equation that describes how the expected value of y for a 
given value of x, denoted E( y|x), is related to x. The simple linear regression equation is 
E( y|x) 5 b0 1 b1x and the multiple linear regression equation is E( y|x) 5 b0 1 b1x1 1 
b2x2 1 . . . 1 bq xq.
Estimated regression equation The estimate of the regression equation developed 
from sample data by using the least squares method. The estimated simple linear re-
gression equation is y^ 5 b0 1 b1x, and the estimated multiple linear regression equa-
tion is y^ 5 b0 1 b1x1 1 b2x2 1 . . . 1 bq xq.
Point estimator A single value used as an estimate of the corresponding population 
 parameter.
Least squares method A procedure for using sample data to find the estimated regression 
equation.
Residual The difference between the observed value of the dependent variable and the 
value predicted using the estimated regression equation; for the ith observation, the ith 
residual is yi 2 y^i.
Experimental region The range of values for the independent variables x1, x2, . . . , xq for 
the data that are used to estimate the regression model.
Extrapolation Prediction of the mean value of the dependent variable y for values of the 
independent variables x1, x2, . . . , xq that are outside the experimental range.
Coefficient of determination A measure of the goodness of fit of the estimated regression 
equation. It can be interpreted as the proportion of the variability in the dependent variable 
y that is explained by the estimated regression equation.
Statistical inference The process of making estimates and drawing conclusions about one 
or more characteristics of a population (the value of one or more parameters) through analy-
sis of sample data drawn from the population.
Hypothesis testing The process of making conjecture about the value of a population 
parameter, collecting sample data that can be used to assess this conjecture, measuring the 
strength of the evidence against the conjecture that is provided by the sample, and using 
these results to draw a conclusion about the conjecture.
Interval estimation The use of sample data to calculate a range of values that is believed 
to include the unknown value of a population parameter.
F test Statistical test based on the F probability distribution that can be used to test the 
hypothesis that the values of b1, b2, . . . , bp are all zero; if this hypothesis is rejected, we 
conclude that there is an overall regression relationship.
p-value The probability that a random sample of the same size collected from the same 
population using the same procedure will yield stronger evidence against a hypothesis than 
the evidence in the sample in the sample data given that the hypothesis is actually true.
t test Statistical test based on the Student’s t probability distribution that can be used to 
test the hypothesis that a regression parameter bj is zero; if this hypothesis is rejected, we 
conclude that there is a regression relationship between the jth independent variable and 
the dependent variable.
Confidence interval An estimate of a population parameter that provides an interval 
 believed to contain the value of the parameter at some level of confidence.
Confidence level An indication of how frequently interval estimates based on samples 
of the same size taken from the same population using identical sampling techniques will 
contain the true value of the parameter we are estimating.
Multicollinearity The degree of correlation among independent variables in a regression 
model.
Dummy variable A variable used to model the effect of categorical independent variables 
in a regression model; generally takes only the value zero or one.

182 
Chapter 4 Linear Regression
Quadratic regression model Regression model in which a nonlinear relationship between 
the independent and dependent variables is fit by including the independent variable and 
the square of the independent variable in the model: y^ 5 b0 1 b1x1 1 b2 x2
1; also referred to 
as a second-order polynomial model.
Piecewise linear regression model Regression model in which one linear relationship be-
tween the independent and dependent variables is fit for values of the independent variable 
below a prespecified value of the independent variable, a different linear relationship be-
tween the independent and dependent variables is fit for values of the independent variable 
above the prespecified value of the independent variable, and the two regressions have the 
same estimated value of the dependent variable (i.e., are joined) at the prespecified value 
of the independent variable.
Knot The prespecified value of the independent variable at which its relationship with 
the dependent variable changes in a piecewise linear regression model; also called the 
breakpoint or the joint.
Interaction The relationship between the dependent variable and one independent variable 
is different at different values of a second independent variable.
Overfitting Fitting a model too closely to sample data, resulting in a model that does not 
accurately reflect the population.
Training set The data set used to build the candidate models.
Validation set The data set used to compare model forecasts and ultimately pick a model 
for predicting values of the dependent variable.
Problems
 1.  Bicycling World, a magazine devoted to cycling, reviews hundreds of bicycles throughout 
the year. Its Road-Race category contains reviews of bicycles used by riders primarily in-
terested in racing. One of the most important factors in selecting a bicycle for racing is its 
weight. The following data show the weight (pounds) and price ($) for ten racing bicycles 
reviewed by the magazine:
Model
Weight (lbs)
Price ($)
Fierro 7B
HX 5000
Durbin Ultralight
Schmidt
WSilton Advanced
bicyclette vélo
Supremo Team
XTC Racer
D’Onofrio Pro
Americana #6
17.9
16.2
15.0
16.0
17.3
13.2
16.3
17.2
17.7
14.2
2200
6350
8470
6300
4100
8700
6100
2680
3500
8100
a. 
Develop a scatter chart with weight as the independent variable. What does the scatter 
chart indicate about the relationship between the weight and price of these bicycles?
b. Use the data to develop an estimated regression equation that could be used to estimate 
the price for a bicycle, given its weight. What is the estimated regression model?
c. 
Test whether each of the regression parameters b0 and b1 is equal to zero at a 0.05 
level of significance. What are the correct interpretations of the estimated regression 
parameters? Are these interpretations reasonable?
d. How much of the variation in the prices of the bicycles in the sample does the regres-
sion model you estimated in part b explain?
file
WEB
BicyclingWorld

 
Problems 
183
e. 
The manufacturers of the D’Onofrio Pro plan to introduce the 15-pound D’Onofrio 
Elite bicycle later this year. Use the regression model you estimated in part a to predict 
the price of the D’Ononfrio Elite.
 2. In a manufacturing process the assembly line speed (feet per minute) was thought to af-
fect the number of defective parts found during the inspection process. To test this theory, 
managers devised a situation in which the same batch of parts was inspected visually at a 
variety of line speeds. They collected the following data:
Line Speed  
(ft/min)
Number of Defective  
Parts Found
20
20
40
30
60
40
21
19
15
16
14
17
a. 
Develop a scatter chart with line speed as the independent variable. What does the 
scatter chart indicate about the relationship between line speed and the number of 
defective parts found?
b. Use the data to develop an estimated regression equation that could be used to predict 
the number of defective parts found, given the line speed. What is the estimated regres-
sion model?
c. 
Test whether each of the regression parameters b0 and b1 is equal to zero at a 0.01 
level of significance. What are the correct interpretations of the estimated regression 
parameters? Are these interpretations reasonable?
d. How much of the variation in the number of defective parts found for the sample data 
does the model you estimated in part b explain?
 3. Jensen Tire & Auto is deciding whether to purchase a maintenance contract for its new 
computer wheel alignment and balancing machine. Managers feel that maintenance ex-
pense should be related to usage, and they collected the following information on weekly 
usage (hours) and annual maintenance expense (in hundreds of dollars).
Weekly Usage 
(hours)
Annual Maintenance 
Expense ($100s)
13
10
20
28
32
17
24
31
40
38
17.0
22.0
30.0
37.0
47.0
30.5
32.5
39.0
51.5
40.0
a. 
Develop a scatter chart with weekly usage hours as the independent variable. What 
does the scatter chart indicate about the relationship between weekly usage and annual 
maintenance expense?
b. Use the data to develop an estimated regression equation that could be used to predict 
the annual maintenance expense for a given number of hours of weekly usage. What 
is the estimated regression model?
file
WEB
LineSpeed
file
WEB
Jensen

184 
Chapter 4 Linear Regression
c. 
Test whether each of the regression parameters b0 and b1 is equal to zero at a 0.05 
level of significance. What are the correct interpretations of the estimated regression 
parameters? Are these interpretations reasonable?
d. How much of the variation in the sample values of annual maintenance expense does 
the model you estimated in part b explain?
e. 
If the maintenance contract costs $3,000 per year, would you recommend purchasing 
it? Why or why not?
 4. A sociologist was hired by a large city hospital to investigate the relationship between the 
number of unauthorized days that employees are absent per year and the distance (miles) 
between home and work for the employees. A sample of 10 employees was chosen, and 
the following data were collected.
Distance to Work  
(miles)
Number of 
Days Absent
 1
 3
 4
 6
 8
10
12
14
14
18
8
5
8
7
6
3
5
2
4
2
a. 
Develop a scatter chart for these data. Does a linear relationship appear reasonable? 
Explain.
b. 
Use the data to develop an estimated regression equation that could be used to predict the 
number of days absent given the distance to work. What is the estimated regression model?
c. 
What is the 99 percent confidence interval for the regression parameter b1? Based on 
this interval, what conclusion can you make about the hypotheses that the regression 
parameter b1 is equal to zero?
d. What is the 99 percent confidence interval for the regression parameter b0? Based on 
this interval, what conclusion can you make about the hypotheses that the regression 
parameter b0 is equal to zero?
e. 
How much of the variation in the sample values of number of days absent does the 
model you estimated in part b explain?
 5. The regional transit authority for a major metropolitan area wants to determine whether 
there is a relationship between the age of a bus and the annual maintenance cost. A sample 
of ten buses resulted in the following data:
Age of Bus (years)
Annual Maintenance Cost ($)
1
2
2
2
2
3
4
4
5
5
350
370
480
520
590
550
750
800
790
950
file
WEB
Absent
file
WEB
AgeCost

a. 
Develop a scatter chart for these data. What does the scatter chart indicate about the 
relationship between age of a bus and the annual maintenance cost?
b. Use the data to develop an estimated regression equation that could be used to predict 
the annual maintenance cost given the age of the bus. What is the estimated regression 
model?
c. 
Test whether each of the regression parameters b0 and b1 is equal to zero at a 0.05 
level of significance. What are the correct interpretations of the estimated regression 
parameters? Are these interpretations reasonable?
d. How much of the variation in the sample values of annual maintenance cost does the 
model you estimated in part b explain?
e. 
What do you predict the annual maintenance cost to be for a 3.5-year-old bus?
 6. A marketing professor at Givens College is interested in the relationship between hours 
spent studying and total points earned in a course. Data collected on 156 students who 
took the course last semester are provided in the file MktHrsPts.
a. 
Develop a scatter chart for these data. What does the scatter chart indicate about the 
relationship between total points earned and hours spent studying?
b. Develop an estimated regression equation showing how total points earned is related 
to hours spent studying. What is the estimated regression model?
c. 
Test whether each of the regression parameters b0 and b1 is equal to zero at a 0.01 
level of significance. What are the correct interpretations of the estimated regression 
parameters? Are these interpretations reasonable?
d. How much of the variation in the sample values of total point earned does the model 
you estimated in part b explain?
e. 
Mark Sweeney spent 95 hours studying. Use the regression model you estimated in 
part b to predict the total points Mark earned.
 7. The Dow Jones Industrial Average (DJIA) and the Standard & Poor’s 500 (S&P 500) 
indexes are used as measures of overall movement in the stock market. The DJIA is based 
on the price movements of 30 large companies; the S&P 500 is an index composed of 500 
stocks. Some say the S&P 500 is a better measure of stock market performance because it 
is broader based. The closing price for the DJIA and the S&P 500 for 15 weeks, beginning 
with January 6, 2012, follow (Barron’s Web site, April 17, 2012).
Date
DJIA
S&P
January 6
January 13
January 20
January 27
February 3
February 10
February 17
February 24
March 2
March 9
March 16
March 23
March 30
April 5
April 13
12360
12422
12720
12660
12862
12801
12950
12983
12978
12922
13233
13081
13212
13060
12850
1278
1289
1315
1316
1345
1343
1362
1366
1370
1371
1404
1397
1408
1398
1370
a. 
Develop a scatter chart for these data with DJIA as the independent variable. What 
does the scatter chart indicate about the relationship between DJIA and S&P 500?
b. Develop an estimated regression equation showing how S&P 500 is related to DJIA. 
What is the estimated regression model?
file
WEB
MktHrsPts
file
WEB
DJIAS&P500
 
Problems 
185

186 
Chapter 4 Linear Regression
c. 
What is the 95 percent confidence interval for the regression parameter b1? Based on 
this interval, what conclusion can you make about the hypotheses that the regression 
parameter b1 is equal to zero?
d. What is the 95 percent confidence interval for the regression parameter b0? Based on 
this interval, what conclusion can you make about the hypotheses that the regression 
parameter b0 is equal to zero?
e. 
How much of the variation in the sample values of S&P 500 does the model estimated 
in part b explain?
f. 
Suppose that the closing price for the DJIA is 13,500. Estimate the closing price for 
the S&P 500.
g. Should we be concerned that the DJIA value of 13,500 used to predict the S&P 500 
value in part f is beyond the range of the DJIA used to develop the estimated regression 
equation?
 8. The Toyota Camry is one of the best-selling cars in North America. The cost of a previ-
ously owned Camry depends on many factors, including the model year, mileage, and 
condition. To investigate the relationship between the car’s mileage and the sales price for 
Camrys, the following data show the mileage and sale price for 19 sales (PriceHub Web 
site, February 24, 2012).
Miles (1000s)
Price ($1,000s)
22
29
36
47
63
77
73
87
92
101
110
28
59
68
68
91
42
65
110
16.2
16.0
13.8
11.5
12.5
12.9
11.2
13.0
11.8
10.8
 8.3
12.5
11.1
15.0
12.2
13.0
15.6
12.7
 8.3
a. 
Develop a scatter chart for these data with miles as the independent variable. What 
does the scatter chart indicate about the relationship between price and miles?
b. Develop an estimated regression equation showing how price is related to miles. What 
is the estimated regression model?
c. 
Test whether each of the regression parameters b0 and b1 is equal to zero at a 0.01 
level of significance. What are the correct interpretations of the estimated regression 
parameters? Are these interpretations reasonable?
d. How much of the variation in the sample values of price does the model estimated in 
part b explain?
e. 
For the model estimated in part b, calculate the predicted price and residual for each 
automobile in the data. Identify the two automobiles that were the biggest bargains.
f. 
Suppose that you are considering purchasing a previously owned Camry that has been 
driven 60,000 miles. Use the estimated regression equation developed in part b to 
predict the price for this car. Is this the price you would offer the seller?
file
WEB
Camry

 9.  Dixie Showtime Movie Theaters, Inc., owns and operates a chain of cinemas in several 
markets in the southern United States. The owners would like to estimate weekly gross 
revenue as a function of advertising expenditures. Data for a sample of eight markets for 
a recent week follow:
Market
Weekly Gross 
Revenue ($100s)
Television 
Advertising 
($100s)
Newspaper 
Advertising 
($100s)
Mobile
Shreveport
Jackson
Birmingham
Little Rock
Biloxi
New Orleans
Baton Rouge
101.3
51.9
74.8
126.2
137.8
101.4
237.8
219.6
5.0
3.0
4.0
4.3
3.6
3.5
5.0
6.9
1.5
3.0
1.5
4.3
4.0
2.3
8.4
5.8
a. 
 Develop an estimated regression equation with the amount of television advertising 
as the independent variable. Test for a significant relationship between television ad-
vertising and weekly gross revenue at the 0.05 level of significance. What is the inter-
pretation of this relationship?
b. How much of the variation in the sample values of weekly gross revenue does the 
model in part a explain?
c. 
Develop an estimated regression equation with both television advertising and news-
paper advertising as the independent variables. Is the overall regression statistically 
significant at the 0.05 level of significance? If so, then test whether each of the regres-
sion parameters b0, b1, and b2 is equal to zero at a 0.05 level of significance. What are 
the correct interpretations of the estimated regression parameters? Are these interpre-
tations reasonable?
d. How much of the variation in the sample values of weekly gross revenue does the 
model in part c explain?
e. 
Given the results in parts a and c, what should your next step be? Explain.
f. 
What are the managerial implications of these results?
10. Resorts & Spas, a magazine devoted to upscale vacations and accommodations, pub-
lished its Reader’s Choice List of the top 20 independent beachfront boutique hotels in the 
world. The data shown are the scores received by these hotels based on the results from 
 Resorts & Spas’ annual Readers’ Choice Survey. Each score represents the percentage of 
respondents who rated a hotel as excellent or very good on one of three criteria (comfort, 
amenities, and in-house dining). An overall score was also reported and used to rank the 
hotels. The highest ranked hotel, the Muri Beach Odyssey, has an overall score of 94.3, 
the  highest component of which is 97.7 for in-house dining.
Hotel
Overall
Comfort
Amenities
In-House Dining
Muri Beach Odyssey
Pattaya Resort
Sojourner’s Respite
Spa Carribe
Penang Resort and Spa
Mokihana Ho-  kele
Theo’s of Cape Town
Cap d’Agde Resort
Spirit of Mykonos
Turismo del Mar
94.3
92.9
92.8
91.2
90.4
90.2
90.1
89.8
89.3
89.1
94.5
96.6
99.9
88.5
95.0
92.4
95.9
92.5
94.6
90.5
90.8
84.1
100.0
94.7
87.8
82.0
86.2
92.5
85.8
83.2
97.7
96.6
88.4
97.0
91.1
98.7
91.9
88.8
90.7
90.4
(continued )
file
WEB
DixieShowtime
 
Problems 
187
file
WEB
BeachFrontHotels

188 
Chapter 4 Linear Regression
Hotel
Overall
Comfort
Amenities
In-House Dining
Hotel Iguana 
Sidi Abdel Rahman Palace
Sainte-Maxime Quarters
Rotorua Inn
Club Lapu-Lapu
Terracina Retreat
Hacienda Punta Barco
Rendezvous Kolocep
Cabo de Gata Vista
Sanya Deluxe
 89.1
89.0
88.6
87.1
87.1
86.5
86.1
86.0
86.0
85.1
90.8
93.0
92.5
93.0
90.9
94.3
95.4
94.8
92.0
93.4
81.9
93.0
78.2
91.6
74.9
78.0
77.3
76.4
72.2
77.3
88.5
89.6
91.2
73.5
89.6
91.5
90.8
91.4
89.2
91.8
a. 
Determine the estimated multiple linear regression equation that can be used to predict 
the overall score given the scores for comfort, amenities, and in-house dining.
b. Use the F test to determine the overall significance of the regression relationship. What 
is the conclusion at the 0.01 level of significance?
c. 
Use the t test to determine the significance of each independent variable. What is the 
conclusion for each test at the 0.01 level of significance?
d. Remove all independent variables that are not significant at the 0.01 level of signifi-
cance from the estimated regression equation. What is your recommended estimated 
regression equation?
11. The American Association of Individual Investors (AAII) On-Line Discount Broker Sur-
vey polls members on their experiences with electronic trades handled by discount brokers. 
As part of the survey, members were asked to rate their satisfaction with the trade price and 
the speed of execution, as well as provide an overall satisfaction rating. Possible responses 
(scores) were no opinion (0), unsatisfied (1), somewhat satisfied (2), satisfied (3), and very 
satisfied (4). For each broker, summary scores were computed by computing a weighted 
average of the scores provided by each respondent. A portion the survey results follow 
(AAII Web site, February 7, 2012).
Brokerage
Satisfaction with 
Trade Price
Satisfaction with 
Speed of Execution
Overall Satisfaction 
with Electronic Trades
Scottrade, Inc.
3.4
3.4
3.5
Charles Schwab
3.2
3.3
3.4
Fidelity Brokerage  
 Services
3.1
3.4
3.9
TD Ameritrade
2.9
3.6
3.7
E*Trade Financial
2.9
3.2
2.9
(Not listed)
2.5
3.2
2.7
Vanguard Brokerage  
 Services
2.6
3.8
2.8
USAA Brokerage  
 Services
2.4
3.8
3.6
Thinkorswim
2.6
2.6
2.6
Wells Fargo  
  Investments
2.3
2.7
2.3
Interactive Brokers
3.7
4.0
4.0
Zecco.com
2.5
2.5
2.5
Firstrade Securities
3.0
3.0
4.0
Banc of America  
 Investment Services
4.0
1.0
2.0
a. 
Develop an estimated regression equation using trade price and speed of execution to 
predict overall satisfaction with the broker. Interpret the coefficient of determination.
file
WEB
Broker

b. Use the F test to determine the overall significance of the relationship. What is your 
conclusion at the 0.05 level of significance? Use the t test to determine the signifi-
cance of each independent variable? What are your conclusions at the 0.05 level of 
 significance?
c. 
Interpret the estimated regression parameters. Are the relationships indicated by these 
estimates what you would expect?
d. Finger Lakes Investments has developed a new electronic trading system and would 
like to predict overall customer satisfaction assuming they can provide satisfactory 
service levels (3) for both trade price and speed of execution. Use the estimated repres-
sion equation developed in part a to predict overall satisfaction level for Finger Lakes 
Investments if they can achieve these performance levels.
e. 
What concerns (if any) do you have with regard to the possible responses the respon-
dents could select on the survey.
12. The National Football League (NFL) records a variety of performance data for individuals 
and teams. To investigate the importance of passing on the percentage of games won by 
a team, the following data show the conference (Conf), average number of passing yards 
per attempt (Yds/Att), the number of interceptions thrown per attempt (Int/Att), and the 
percentage of games won (Win%) for a random sample of 16 NFL teams for the 2011 
season (NFL Web site, February 12, 2012).
Team
Conf
Yds/Att
Int/Att
Win%
Arizona Cardinals
Atlanta Falcons
Carolina Panthers
Cincinnati Bengals
Detroit Lions
Green Bay Packers
Houston Texans
Indianapolis Colts
Jacksonville Jaguars
Minnesota Vikings
New England Patriots
New Orleans Saints
Oakland Raiders
San Francisco 49ers
Tennessee Titans
Washington Redskins
NFC
NFC
NFC
AFC
NFC
NFC
AFC
AFC
AFC
NFC
AFC
NFC
AFC
NFC
AFC
NFC
6.5
7.1
7.4
6.2
7.2
8.9
7.5
5.6
4.6
5.8
8.3
8.1
7.6
6.5
6.7
6.4
0.042
0.022
0.033
0.026
0.024
0.014
0.019
0.026
0.032
0.033
0.020
0.021
0.044
0.011
0.024
0.041
50.0
62.5
37.5
56.3
62.5
93.8
62.5
12.5
31.3
18.8
81.3
81.3
50.0
81.3
56.3
31.3
a. 
Develop the estimated regression equation that could be used to predict the percentage 
of games won, given the average number of passing yards per attempt. What propor-
tion of variation in the sample values of proportion of games won does this model 
explain?
b. Develop the estimated regression equation that could be used to predict the percentage 
of games won, given the number of interceptions thrown per attempt. What proportion 
of variation in the sample values of proportion of games won does this model explain?
c. 
Develop the estimated regression equation that could be used to predict the percentage 
of games won, given the average number of passing yards per attempt and the number 
of interceptions thrown per attempt. What proportion of variation in the sample values 
of proportion of games won does this model explain?
d. The average number of passing yards per attempt for the Kansas City Chiefs dur-
ing the 2011 season was 6.2, and the team’s number of interceptions thrown per at-
tempt was 0.036. Use the estimated regression equation developed in part c to predict 
the percentage of games won by the Kansas City Chiefs during the 2011 season.  
file
WEB
NFLPassing
 
Problems 
189

190 
Chapter 4 Linear Regression
Compare your prediction to the actual percentage of games won by the Kansas City 
Chiefs. Note: For the 2011 season, the Kansas City Chiefs record was 7 wins and 9 
losses.
e. 
Did the estimated regression equation that uses only the average number of passing 
yards per attempt as the independent variable to predict the percentage of games won 
provide a good fit?
13. Johnson Filtration, Inc., provides maintenance service for water filtration systems through-
out southern Florida. Customers contact Johnson with requests for maintenance service on 
their water filtration systems. To estimate the service time and the service cost, Johnson’s 
managers want to predict the repair time necessary for each maintenance request. Hence, 
repair time in hours is the dependent variable. Repair time is believed to be related to 
three factors: the number of months since the last maintenance service, the type of repair 
problem (mechanical or electrical), and the repairperson who performs the repair (Donna 
Newton or Bob Jones). Data for a sample of ten service calls are reported in the following 
table:
Repair Time 
in Hours
Months Since 
Last Service
Type of Repair
Repairperson
2.9
3.0
4.8
1.8
2.9
4.9
4.2
4.8
4.4
4.5
2
6
8
3
2
7
9
8
4
6
Electrical
Mechanical
Electrical
Mechanical
Electrical
Electrical
Mechanical
Mechanical
Electrical
Electrical
Donna Newton
Donna Newton
Bob Jones
Donna Newton
Donna Newton
Bob Jones
Bob Jones
Bob Jones
Bob Jones
Donna Newton
a. 
Develop the simple linear regression equation to predict repair time given the number 
of months since the last maintenance service, and use the results to test the hypothesis 
that no relationship exists between repair time and the number of months since the last 
maintenance service at the 0.05 level of significance. What is the interpretation of this 
relationship? What does the coefficient of determination tell you about this model?
b. Using the simple linear regression model developed in part a, calculate the predicted 
repair time and residual for each of the ten repairs in the data. Sort the data by residual 
(so the data are in ascending order by value of the residual). Do you see any pattern in 
the residuals for the two types of repair? Do you see any pattern in the residuals for the 
two repairpersons? Do these results suggest any potential modifications to your simple 
linear regression model? Now create a scatter chart with months since last service 
on the x-axis and repair time in hours on the y-axis for which the points representing 
electrical and mechanical repairs are shown in different shapes and/or colors. Create 
a similar scatter chart of months since last service and repair time in hours for which 
the points representing Bob Jones and Donna Newton repairs are shown in different 
shapes and/or colors, Do these charts and the results of your residual analysis suggest 
the same potential modifications to your simple linear regression model?
c. 
Create a new dummy variable that is equal to zero if the type of repair is mechanical 
and one if the type of repair is electrical. Develop the multiple regression equation to 
predict repair time, given the number of months since the last maintenance service and 
the type of repair. What are the interpretations of the estimated regression parameters? 
What does the coefficient of determination tell you about this model?
d. Create a new dummy variable that is equal to zero if the repairperson is Bob Jones and 
one if the repairperson is Donna Newton. Develop the multiple regression equation to 
predict repair time, given the number of months since the last maintenance service and 
file
WEB
Repair

the repairperson. What are the interpretations of the estimated regression parameters? 
What does the coefficient of determination tell you about this model?
e. 
Develop the multiple regression equation to predict repair time, given the number of 
months since the last maintenance service, the type of repair, and the repairperson. 
What are the interpretations of the estimated regression parameters? What does the 
coefficient of determination tell you about this model?
f. 
Which of these models would you use? Why?
14. A study investigated the relationship between audit delay (the length of time from a compa-
ny’s fiscal year-end to the date of the auditor’s report) and variables that describe the client 
and the auditor. Some of the independent variables that were included in this study follow:
 
Industry 
 A dummy variable coded 1 if the firm was an industrial company or 0 if the 
firm was a bank, savings and loan, or insurance company.
 
Public 
 A dummy variable coded 1 if the company was traded on an organized ex-
change or over the counter; otherwise coded 0.
 
Quality 
 A measure of overall quality of internal controls, as judged by the auditor, on 
a five-point scale ranging from “virtually none” (1) to “excellent” (5).
 
Finished 
 A measure ranging from 1 to 4, as judged by the auditor, where 1 indicates 
“all work performed subsequent to year-end” and 4 indicates “most work per-
formed prior to year-end.”
 
A sample of 40 companies provided the following data:
Delay (Days)
Industry
Public
Quality
Finished
62
45
54
71
91
62
61
69
80
52
47
65
60
81
73
89
71
76
68
68
86
76
67
57
55
54
69
82
94
74
0
0
0
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
1
0
1
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
1
0
0
0
0
1
3
3
2
1
1
4
3
5
1
5
3
2
1
1
2
2
5
2
1
5
2
3
2
4
3
5
3
5
1
5
1
3
2
2
1
4
2
2
1
3
2
3
3
2
2
1
4
2
2
2
2
1
3
2
2
2
3
1
1
2
(continued )
file
WEB
Audit
 
Problems 
191

192 
Chapter 4 Linear Regression
Delay (Days)
Industry
Public
Quality
Finished
75
69
71
79
80
91
92
46
72
85
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
1
0
0
4
2
4
5
1
4
1
4
5
5
3
2
4
2
4
1
4
3
2
1
a. 
Develop the estimated regression equation using all of the independent variables in-
cluded in the data.
b. Test for an overall regression relationship at the 0.05 level of significance. Is there a 
significant regression relationship?
c. 
How much of the variation in the sample values of delay does this estimated regression 
equation explain? What other independent variables could you include in this regres-
sion model to improve the fit?
d. Test the relationship between each independent variable and the dependent variable 
at the 0.05 level of significance, and interpret the relationship between each of the 
independent variables and the dependent variable.
e. 
On the basis of your observations about the relationships between the dependent vari-
able Delay and the independent variables Quality and Finished, suggest an alternative 
regression equation to the model developed in part a to explain as much of the vari-
ability in Delay as possible.
15. The U.S. Department of Energy’s Fuel Economy Guide provides fuel efficiency data for 
cars and trucks. A portion of the data for 311 compact, midsized, and large cars follows. 
The Class column identifies the size of the car; Compact, Midsize, or Large. The Dis-
placement column shows the engine’s displacement in liters. The FuelType column shows 
whether the car uses premium (P) or regular (R) fuel, and the HwyMPG column shows the 
fuel efficiency rating for highway driving in terms of miles per gallon. The complete data 
set is contained in the file FuelData:
Car
Class
Displacement
FuelType
HwyMPG
 1
 2
 3
 . . .
161
162
 . . .
310
Compact
Compact
Compact
  .
  .
  .
Midsize
Midsize
  .
  .
  .
Large
3.1
3.1
3.0
...
2.4
2.0
...
3.0
P
P
P
...
R
P
...
R
25
25
25
...
30
29
...
25
a. 
Develop an estimated regression equation that can be used to predict the fuel ef-
ficiency for highway driving given the engine’s displacement. Test for significance 
using the 0.05 level of significance. How much of the variation in the sample values 
of HwyMPG does this estimated regression equation explain?
file
WEB
FuelData

b. Create a scatter chart with HwyMPG on the y-axis and displacement on the x-axis 
for which the points representing compact, midsize, and large automobiles are shown 
in different shapes and/or colors. What does this chart suggest about the relationship 
between the class of automobile (compact, midsize, and large) and HwyMPG?
c. Now consider the addition of the dummy variables ClassMidsize and ClassLarge 
to the simple linear regression model in part a. The value of ClassMidsize is 1 if 
the car is a midsize car and 0 otherwise; the value of ClassLarge is 1 if the car is a 
large car and 0 otherwise. Thus, for a compact car, the value of ClassMidsize and 
the value of ClassLarge are both 0. Develop the estimated regression equation that 
can be used to predict the fuel efficiency for highway driving, given the engine’s 
displacement and the dummy variables ClassMidsize and ClassLarge. How much 
of the variation in the sample values of HwyMPG does this estimated regression 
equation explain?
d. Use significance level of 0.05 to determine whether the dummy variables added to the 
model in part c are significant.
e. Consider the addition of the dummy variable FuelPremium, where the value of 
FuelPremium is 1 if the car uses premium fuel and 0 if the car uses regular fuel. 
Develop the estimated regression equation that can be used to predict the fuel ef-
ficiency for highway driving given the engine’s displacement, the dummy variables 
ClassMidsize and ClassLarge, and the dummy variable FuelPremium. How much 
of the variation in the sample values of HwyMPG does this estimated regression 
equation explain?
f. 
For the estimated regression equation developed in part e, test for the significance of 
an overall regression relationship and relationships between each of the independent 
variables and the dependent variable using the 0.05 level of significance for each 
test.
16. A highway department is studying the relationship between traffic flow and speed during 
rush hour on Highway 193. The data in the file TrafficFlow were collected on Highway 
193 during 100 recent rush hours.
a. 
Develop a scatter chart for these data. What does the scatter chart indicate about the 
relationship between vehicle speed and traffic flow?
b. Develop an estimated simple linear regression equation for the data. How much varia-
tion in the sample values of traffic flow is explained by this regression model? Use 
a 0.05 level of significance to test the relationship between vehicle speed and traffic 
flow. What is the interpretation of this relationship? 
c. 
Develop an estimated quadratic regression equation for the data. How much variation 
in the sample values of traffic flow does this regression model explain? Is the overall 
regression relationship significant at a 0.05 level of significance? If so, then test the 
relationship between each of the independent variables and the dependent variable at a 
0.05 level of significance. How would you interpret this model? Is this model superior 
to the model you developed in part b?
d. As an alternative to fitting a second-order model, fit a model using a piecewise linear 
regression with a single knot. What value of vehicle speed appears to be a good point 
for the placement of the knot? Does the estimated piecewise linear regression provide 
a better fit than the estimated quadratic regression developed in part c? Explain.
e. 
Separate the data into two sets such that one data set contains the observations of ve-
hicle speed less than the value of the knot from part d and the other data set contains 
the observations of vehicle speed greater than or equal to the value of the knot from 
part d. Then fit a simple linear regression equation to each data set. How does this 
pair of regression equations compare to the single piecewise linear regression with 
the single knot from part d? In particular, compare predicted values of traffic flow for 
values of the speed slightly above and slightly below the knot value from part d.
f. 
What other independent variables could you include in your regression model to 
 explain more variation in traffic flow?
file
WEB
TrafficFlow
 
Problems 
193

194 
Chapter 4 Linear Regression
17. A sample containing years to maturity and yield (percent) for 40 corporate bonds are con-
tained in the data file named CorporateBonds (Barron’s, April 2, 2012).
a. 
Develop a scatter chart of the data using years to maturity as the independent variable. 
Does a simple linear regression model appear to be appropriate?
b. Develop an estimated quadratic regression equation with years to maturity and squared 
values of years to maturity as the independent variables. How much variation in the 
sample values of yield does this regression model explain? Is the overall regression 
relationship significant at a 0.05 level of significance? If so, then test the relationship 
between each of the independent variables and the dependent variable at a 0.05 level 
of significance. How would you interpret this model?
c. 
Create a plot of the linear and quadratic regression lines overlaid on the scatter chart 
of years to maturity and yield. Does this helps you better understand the difference in 
how the quadratic regression model and a simple linear regression model fit the sample 
data? Which model does this chart suggest provides a superior fit to the sample data?
d. What other independent variables could you include in your regression model to ex-
plain more variation in yield?
18. In 2011, home prices and mortgage rates fell so far that in a number of cities the monthly 
cost of owning a home was less expensive than renting. The following data show the av-
erage asking rent for ten markets and the monthly mortgage on the median priced home 
(including taxes and insurance) for ten cites where the average monthly mortgage pay-
ment was less than the average asking rent (The Wall Street Journal, November 26–27, 
2011).
City 
Rent ($)
Mortgage ($)
Atlanta
Chicago
Detroit
Jacksonville
Las Vegas
Miami
Minneapolis
Orlando
Phoenix
St. Louis
840
1062
823
779
796
1071
953
851
762
723
539
1002
626
711
655
977
776
695
651
654
a. 
Develop a scatter chart for these data, treating the average asking rent as the indepen-
dent variable. Does a simple linear regression model appear to be appropriate?
b. Use a simple linear regression model to develop an estimated regression equation to 
predict the monthly mortgage on the median priced home given the average asking 
rent. Construct a plot of the residuals against the independent variable rent. Based on 
this residual plot, does a simple linear regression model appear to be appropriate?
c. 
Using a quadratic regression model, develop an estimated regression equation to pre-
dict the monthly mortgage on the median-priced home, given the average asking rent.
d. Do you prefer the estimated regression equation developed in part a or part c? Create 
a plot of the linear and quadratic regression lines overlaid on the scatter chart of the 
monthly mortgage on the median-priced home and the average asking rent to help you 
assess the two regression equations. Explain your conclusions.
19. A recent ten-year study conducted by a research team at the Great Falls Medical School 
was conducted to assess how age, systolic blood pressure, and smoking relate to the risk of 
strokes. Assume that the following data are from a portion of this study. Risk is interpreted 
as the probability (times 100) that the patient will have a stroke over the next ten-year 
period. For the smoking variable, define a dummy variable with 1 indicating a smoker and 
0 indicating a nonsmoker.
file
WEB
CorporateBonds
file
WEB
RentMortgage

Risk
Age
Systolic Blood Pressure
Smoker
12
24
13
56
28
51
18
31
37
15
22
36
15
48
15
36
 8
34
 3
37
57
67
58
86
59
76
56
78
80
78
71
70
67
77
60
82
66
80
62
59
152
163
155
177
196
189
155
120
135
 98
152
173
135
209
199
119
166
125
117
207
No
No
No
Yes
No
Yes
Yes
No
Yes
No
No
Yes
Yes
Yes
No
Yes
No
Yes
No
Yes
a. 
Develop an estimated multiple regression equation that relates risk of a stroke to the 
person’s age, systolic blood pressure, and whether the person is a smoker.
b. Is smoking a significant factor in the risk of a stroke? Explain. Use a 0.05 level of 
significance.
c. 
What is the probability of a stroke over the next ten years for Art Speen, a 68-year-
old smoker who has systolic blood pressure of 175? What action might the physician 
recommend for this patient?
d. Can you think of any other factors that could be included in the model as independent 
variables?
20. The Scholastic Aptitude Test (or SAT) is a standardized college entrance test that is used 
by colleges and universities as a means for making admission decisions. The critical read-
ing and mathematics components of the SAT are reported on a scale from 200 to 800. 
Several universities believe these scores are strong predictors of an incoming student’s 
potential success, and they use these scores as important inputs when making admission 
decisions on potential freshman. The file RugglesCollege contains freshman year GPA 
and the critical reading and mathematics SAT scores for a random sample of two hundred 
students who recently completed their freshman year at Ruggles College.
a. 
Develop an estimated multiple regression equation that includes critical reading and 
mathematics SAT scores as independent variables. How much variation in freshman 
GPA does this model explain? Is the overall regression statistically significant at the 
0.05 level of significance? If so, then test whether each of the regression parameters b0, 
b1, and b2 is equal to zero at a 0.05 level of significance. What are the correct interpre-
tations of the estimated regression parameters? Are these interpretations reasonable?
b. 
Using the multiple linear regression model you developed in part a, what is the predicted 
freshman GPA of Bobby Engle, a student who has been admitted to Ruggles College 
with a 660 SAT score on critical reading and at a 630 SAT score on mathematics?
c. 
The Ruggles College Director of Admissions believes that the relationship between a 
student’s scores on the critical reading component of the SAT and the student’s fresh-
man GPA varies with the student’s score on the mathematics component of the SAT. 
Develop an estimated multiple regression equation that includes critical reading and 
mathematics SAT scores and their interaction as independent variables. How much 
file
WEB
Stroke
file
WEB
RugglesCollege
 
Problems 
195

196 
Chapter 4 Linear Regression
variation in freshman GPA does this model explain? Is the overall regression statisti-
cally significant at the 0.05 level of significance? If so, then test whether each of the 
regression parameters b0, b1, b2, and b3 is equal to zero at a 0.05 level of significance. 
What are the correct interpretations of the estimated regression parameters? Do these 
results support the conjecture made by the Ruggles College Director of Admissions?
d. Do you prefer the estimated regression model developed in part a or part c? Explain.
e. 
Can you think of any other factors that could be included in the model as independent 
variables?
21. Consider again the example introduced in Section 4.5 of a credit card company that has 
a database of information provided by its customers when the customers apply for credit 
cards. An analyst has created a multiple regression model for which the dependent vari-
able in the model is credit card charges accrued by a customer in the data set over the past 
year (y), and the independent variables are the customer’s annual household income (x1), 
number of members of the household (x2), and number of years of post–high school edu-
cation (x3). Figure 4.23 provides Excel output for a multiple regression model estimated 
using a data set the company created.
a. Estimate the corresponding simple linear regression with the customer’s annual 
household income as the independent variable and credit card charges accrued by 
a customer over the past year as the dependent variable. Interpret the estimated 
relationship between the customer’s annual household income and credit card 
charges accrued over the past year. How much variation in credit card charges 
accrued by a customer over the past year does this simple linear regression model 
explain?
b. Estimate the corresponding simple linear regression with the number of members in 
the customer’s household as the independent variable and credit card charges accrued 
by a customer over the past year as the dependent variable. Interpret the estimated 
relationship between the number of members in the customer’s household and credit 
card charges accrued over the past year. How much variation in credit card charges 
accrued by a customer over the past year does this simple linear regression model 
explain?
c. 
Estimate the corresponding simple linear regression with the customer’s number 
of years of post–high school education as the independent variable and credit card 
charges accrued by a customer over the past year as the dependent variable. Inter-
pret the estimated relationship between the customer’s number of years of post–high 
school education and credit card charges accrued over the past year. How much varia-
tion in credit card charges accrued by a customer over the past year does this simple 
linear regression model explain?
d. Recall the multiple regression in Figure 4.23 with credit card charges accrued by a 
customer over the past year as the dependent variable and customer’s annual house-
hold income (x1), number of members of the household (x2), and number of years of 
post–high school education (x3) as the independent variables. Do the estimated slopes 
differ substantially from the corresponding slopes that were estimated using simple 
linear regression in parts a, b, and c? What does this tell you about multicollinearity 
in the multiple regression model in Figure 4.23?
e. 
Add the coefficients of determination for the simple linear regression in parts a, b, and 
c, and compare the result to the coefficient of determination for the multiple regression 
model in Figure 4.23. What does this tell you about multicollinearity in the multiple 
regression model in Figure 4.23?
f. 
Add age, a dummy variable for gender, and a dummy variable for whether a customer 
has exceeded his or her credit limit in the past 12 months as independent variables to 
the multiple regression model in Figure 4.23. Code the dummy variable for gender as 
1 if the customer’s gender is female and 0 if male, and code the dummy variable for 
whether a customer has exceeded his or her credit limit in the past 12 months as 1 if 
the customer has exceeded his or her credit limit in the past 12 months and 0 otherwise. 
Do these variables substantially improve the fit of your model?
file
WEB
ExtendedLargeCredit

 
Case Problem Alumni Giving 
197
 
Case Problem alumni Giving
Alumni donations are an important source of revenue for colleges and universities. If admin-
istrators could determine the factors that could lead to increases in the percentage of alumni 
who make a donation, they might be able to implement policies that could lead to increased 
revenues. Research shows that students who are more satisfied with their contact with teachers 
are more likely to graduate. As a result, one might suspect that smaller class sizes and lower stu-
dent/faculty ratios might lead to a higher percentage of satisfied graduates, which in turn might 
lead to increases in the percentage of alumni who make a donation. The following table shows 
data for 48 national universities. The Graduation Rate column is the percentage of students who 
initially enrolled at the university and graduated. The % of Classes Under 20 column shows the 
percentages of classes offered with fewer than 20 students. The Student/Faculty Ratio column 
is the number of students enrolled divided by the total number of faculty. Finally, the Alumni 
Giving Rate column is the percentage of alumni who made a donation to the university.
State
Graduation 
Rate
% of 
Classes 
Under 20
Student/
Faculty 
Ratio
Alumni 
Giving 
Rate
Boston College
MA
85
39
13
25
Brandeis University
MA
79
68
8
33
Brown University
RI
93
60
8
40
California Institute of 
Technology
CA
85
65
3
46
Carnegie Mellon University
PA
75
67
10
28
Case Western Reserve Univ.
OH
72
52
8
31
College of William and Mary
VA
89
45
12
27
Columbia University
NY
90
69
7
31
Cornell University
NY
91
72
13
35
Dartmouth College
NH
94
61
10
53
Duke University
NC
92
68
8
45
Emory University
GA
84
65
7
37
Georgetown University
DC
91
54
10
29
Harvard University
MA
97
73
8
46
Johns Hopkins University
MD
89
64
9
27
Lehigh University
PA
81
55
11
40
Massachusetts Institute of 
Technology
MA
92
65
6
44
New York University
NY
72
63
13
13
Northwestern University
IL
90
66
8
30
Pennsylvania State Univ.
PA
80
32
19
21
Princeton University
NJ
95
68
5
67
Rice University
TX
92
62
8
40
Stanford University
CA
92
69
7
34
Tufts University
MA
87
67
9
29
Tulane University
LA
72
56
12
17
University of California–
Berkeley
CA
83
58
17
18
University of California–Davis
CA
74
32
19
7
University of California–Irvine
CA
74
42
20
9
University of California– 
Los Angeles
CA
78
41
18
13
University of California– 
San Diego
CA
80
48
19
8
(continued )
file
WEB
AlumniGiving

198 
Chapter 4 Linear Regression
State
Graduation 
Rate
% of 
Classes 
Under 20
Student/
Faculty 
Ratio
Alumni 
Giving 
Rate
University of California– 
Santa Barbara
CA
70
45
20
12
University of Chicago
IL
84
65
 4
36
University of Florida
FL
67
31
23
19
University of Illinois– 
Urbana Champaign
IL
77
29
15
23
University of Michigan– 
Ann Arbor
MI
83
51
15
13
University of North Carolina–
Chapel Hill
NC
82
40
16
26
University of Notre Dame
IN
94
53
13
49
University of Pennsylvania
PA
90
65
 7
41
University of Rochester
NY
76
63
10
23
University of Southern 
California
CA
70
53
13
22
University of Texas–Austin
TX
66
39
21
13
University of Virginia
VA
92
44
13
28
University of Washington
WA
70
37
12
12
University of Wisconsin–
Madison
WI
73
37
13
13
Vanderbilt University
TN
82
68
 9
31
Wake Forest University
NC
82
59
11
38
Washington University–St. Louis
MO
86
73
 7
33
Yale University
CT
94
77
 7
50
Managerial Report
 1. Use methods of descriptive statistics to summarize the data.
 2. Develop an estimated simple linear regression model that can be used to predict the 
alumni giving rate, given the graduation rate. Discuss your findings.
 3. Develop an estimated multiple linear regression model that could be used to predict 
the alumni giving rate using the Graduation Rate, % of Classes Under 20, and Student/
Faculty Ratio as independent variables. Discuss your findings.
 4. Based on the results in parts 2 and 3, do you believe another regression model may be 
more appropriate? Estimate this model, and discuss your results.
 5. What conclusions and recommendations can you derive from your analysis? What 
universities are achieving a substantially higher alumni giving rate than would be 
expected, given their Graduation Rate, % of Classes Under 20, and Student/Faculty 
Ratio? What universities are achieving a substantially lower alumni giving rate than 
would be expected, given their Graduation Rate, % of Classes Under 20, and Student/
Faculty Ratio? What other independent variables could be included in the model?
 
Appendix  Using XLMiner for regression
To show how XLMiner can be used for regression analysis, we again develop a multiple 
regression model for the Butler Trucking Company example. The dependent variable is 
Travel Time (y), and the independent variables are Miles (x1) and Deliveries (x2) are shown 
in Figure 4.38. Values for Miles are in cells B2:B301, values for Deliveries are in C2:C301, 
and values of Travel Time are in D2:D301.

 
Appendix  Using XLMiner for Regression 
199
Use the following steps to estimate a multiple regression model for these data in 
XLMiner:
Step 1. Select any cell in the range of data (any cell in A1:D301)
Step 2. Click the XLMINER tab in the Ribbon
Step 3.  Click Predict in the Data Mining group, and select Multiple Linear 
 Regression (Figure 4.38)
Step 4.  When the Multiple Linear Regression–Step 1 of 2 dialog box appears 
( Figure 4.39):
 
 Select Miles in the Variables in input data box, and click the $ button  
  to move this variable into the Input variables box (this is what 
XLMiner calls the independent variables)
 
 Select Deliveries in the Variables in input data box, and click the $  
 button to move this variable into the Input variables box
 
 Select Time in the Variables in input data box, and click the . button  
  to move this variable into the Output variable box (this is what 
XLMiner calls the dependent variables)
 
Click Next .
Step 5.  When the Multiple Linear Regression–Step 2 of 2 dialog box appears 
( Figure 4.40):
 
Select Summary report and click Finish
The results are shown in Figure 4.41; they are found in the MLR_Output1 worksheet. The 
estimated regression equation is y^ 5 0.1273 1 0.0672x1 1 0.6900x2 (XLMiner refers to the 
y-intercept as the Constant term), and this model has a multiple coefficient of determination 
of R2 5 0.8173 (referred to in the XLMiner output as Multiple R-squared).
A
B
Assignment
Miles
Deliveries
Time
1
2
1
2
3
4
5
6
7
8
9
10
100.0
50.0
100.0
100.0
50.0
80.0
75.0
65.0
90.0
90.0
4.0
3.0
4.0
2.0
2.0
2.0
3.0
4.0
3.0
2.0
9.3
4.8
8.9
6.5
4.2
6.2
7.4
6.0
7.6
6.1
3
4
5
6
7
8
9
10
11
C
D
E
F
J
FIGURE 4.38   XLMINER TAB AND DATA MINING GROUP OPTIONS

200 
Chapter 4 Linear Regression
FIGURE 4.40   MULTIPLE LINEAR REGRESSION–STEP 2 OF 2 DIALOG BOX
FIGURE 4.39   MULTIPLE LINEAR REGRESSION–STEP 1 OF 2 DIALOG BOX

Note that the Multiple Linear Regression–Step 2 of 2 dialog box in Figure 4.40 pro-
vides several options for generating additional output. For example, selecting Fitted values 
under Output options on training data generates a worksheet with predicted values of 
the dependent variable for every driving assignment in the data, selecting ANOVA Table 
under Output options on training data generates an F test of the overall regression rela-
tionship, and selecting Unstandardized under Residuals generates a worksheet with the 
residuals for every driving assignment in the data.
 
Appendix  Using XLMiner for Regression 
201
FIGURE 4.41   XLMINER OUTPUT FOR THE BUTLER TRUCKING COMPANY MULTIPLE REGRESSION
A
B
C
D
E
F
G
H
I
J
K
L
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
32
31
33
34
35
36
37
38
39
40
41
42
Training data used for building the model
# Records in the training data
# Input Variables
2
Miles
Deliveries
Time
Yes
Input variables
Output variables
Constant term present
['ButlerWithDeliveries.xlsx']'Data'!$A$2: $D$301
300
Inputs
Variables
Output options chosen
The Regression Model
Training Data scoring - Summary Report
Elapsed Time
Summary report of scoring on training data
Input variables
Total sum of 
squared
errors
Overall (secs)
RMS
Error
Average
Error
Coefﬁcient
Std. Error
p-value
SS
Constant term
0.12733707
0.06718174
0.68999827
Residual df
297
0.817349737
0.8299672
204.5871429
Multiple R-squared
Std. Dev. estimate
Residual SS
15916.99609
539.1988525
376.3171997
0.53537786
0
0
0.20520349
0.00245498
0.02952106
Miles
Deliveries
204.5871374 0.825806954 2.48267E-07
7.00
Data

Time Series Analysis 
and Forecasting
CONTENTS
5.1 
TIME SERIES PATTERNS
Horizontal Pattern
Trend Pattern
Seasonal Pattern
Trend and Seasonal Pattern
Cyclical Pattern
Identifying Time Series 
Patterns
5.2 
FORECAST ACCURACY
5.3 
MOVING AVERAGES 
AND EXPONENTIAL 
SMOOTHING
Moving Averages
Forecast Accuracy
Exponential Smoothing
Forecast Accuracy
5.4 
USING REGRESSION 
 ANALYSIS FOR  FORECASTING
Linear Trend Projection
Seasonality
Seasonality Without Trend
Seasonality with Trend
Using Regression Analysis as a 
Causal Forecasting Method
Combining Causal Variables with 
Trend and Seasonality Effects
Considerations in Using 
Regression in Forecasting
5.5 
DETERMINING THE BEST 
FORECASTING MODEL 
TO USE
APPENDIX:  USING XLMINER 
FOR FORECASTING
CHAPTER 5

ACCO Brands Corporation is one of the world’s largest 
suppliers of branded office and consumer products and 
print finishing solutions. The company’s widely recog-
nized brands include AT-A-GLANCE®, Day-Timer®, 
Five Star®, GBC®, Hilroy®, Kensington®, Marbig®, 
Mead®, NOBO, Quartet®, Rexel, Swingline®, Tilibra®, 
Wilson Jones®, and many others.
Because it produces and markets a wide array of prod-
ucts with a myriad of demand characteristics, ACCO Brands 
relies heavily on sales forecasts in planning its manufactur-
ing, distribution, and marketing activities. By viewing its 
relationship in terms of a supply chain, ACCO Brands and 
its customers (which are generally retail chains) establish 
close collaborative relationships and consider each other to 
be valued partners. As a result, ACCO Brands’ custom-
ers share valuable information and data that serve as inputs 
into ACCO Brands’ forecasting process.
In her role as a forecasting manager for ACCO 
Brands, Vanessa Baker appreciates the importance of 
this additional information. “We do separate forecasts of 
demand for each major customer,” said Baker, “and we 
generally use twenty-four to thirty-six months of history 
to generate monthly forecasts twelve to eighteen months 
into the future. While trends are important, several of our 
major product lines, including school, planning and orga-
nizing, and decorative calendars, are heavily seasonal, and 
seasonal sales make up the bulk of our annual volume.”
Daniel Marks, one of several account-level strategic 
forecast managers for ACCO Brands, adds, 
The supply chain process includes the total lead time 
from identifying opportunities to making or procuring 
the product to getting the product on the shelves to align 
with the forecasted demand; this can potentially take 
several months, so the accuracy of our forecasts is criti-
cal throughout each step of the supply chain. Adding to 
this challenge is the risk of obsolescence. We sell many 
dated items, such as planners and calendars, which have 
a natural, built-in obsolescence. In addition, many of 
our products feature designs that are fashion-conscious 
or contain pop culture images, and these products can 
also become obsolete very quickly as tastes and popu-
larity change. An overly optimistic forecast for these 
products can be very costly, but an overly pessimis-
tic forecast can result in lost sales potential and give 
our competitors an opportunity to take market share 
from us.
In addition to looking at trends, seasonal compo-
nents, and cyclical patterns, Baker and Marks must con-
tend with several other factors. Baker notes, “We have 
to adjust our forecasts for upcoming promotions by our 
customers.” Marks agrees and adds:
We also have to go beyond just forecasting consumer 
demand; we must consider the retailer’s specific needs 
in our order forecasts, such as what type of display will 
be used and how many units of a product must be on 
display to satisfy their presentation requirements. Cur-
rent inventory is another factor—if a customer is car-
rying either too much or too little inventory, that will 
affect their future orders, and we need to reflect that 
in our forecasts. Will the product have a short life be-
cause it is tied to a cultural fad? What are the retailer’s 
marketing and markdown strategies? Our knowledge 
of the environments in which our supply chain partners 
are competing helps us to forecast demand more accu-
rately, and that reduces waste and makes our custom-
ers, as well as ACCO Brands, far more profitable.
FORECASTING DEMAND FOR A BROAD LINE OF OFFICE PRODUCTS*
AnAlytics  in  Action
*The authors are indebted to Vanessa Baker and Daniel Marks of ACCO 
Brands for providing input for this Analytics in Action.
The purpose of this chapter is to provide an introduction to time series analysis and forecast-
ing. Suppose we are asked to provide quarterly forecasts of sales for one of our company’s 
products over the coming one-year period. Production schedules, raw materials purchasing, 
inventory policies, marketing plans, and cash flows will all be affected by the quarterly fore-
casts we provide. Consequently, poor forecasts may result in poor planning and increased 
costs for the company. How should we go about providing the quarterly sales forecasts? 
Good judgment, intuition, and an awareness of the state of the economy may give us a rough 
idea, or feeling, of what is likely to happen in the future, but converting that feeling into a 
number that can be used as next year’s sales forecast is challenging.
 
Analytics in Action 
203

204 
Chapter 5 Time Series Analysis and Forecasting
Forecasting methods can be classified as qualitative or quantitative. Qualitative meth-
ods generally involve the use of expert judgment to develop forecasts. Such methods are 
appropriate when historical data on the variable being forecast are either unavailable or not 
applicable. Quantitative forecasting methods can be used when (1) past information about 
the variable being forecast is available, (2) the information can be quantified, and (3) it is 
reasonable to assume that past is prologue (i.e. the pattern of the past will continue into 
the future). We will focus exclusively on quantitative forecasting methods in this chapter.
If the historical data are restricted to past values of the variable to be forecast, the fore-
casting procedure is called a time series method and the historical data are referred to as 
time series. The objective of time series analysis is to uncover a pattern in the time series 
and then extrapolate the pattern into the future; the forecast is based solely on past values 
of the variable and/or on past forecast errors.
Causal or exploratory forecasting methods are based on the assumption that the variable 
we are forecasting has a cause-effect relationship with one or more other variables. These 
methods help explain how the value of one variable impacts the value of another. For instance, 
the sales volume for many products is influenced by advertising expenditures, so regression 
analysis may be used to develop an equation showing how these two variables are related. 
Then, once the advertising budget is set for the next period, we could substitute this value 
into the equation to develop a prediction or forecast of the sales volume for that period. Note 
that if a time series method was used to develop the forecast, advertising expenditures would 
not be considered; that is, a time series method would base the forecast solely on past sales.
Modern data-collection technologies have enabled individuals, businesses, and govern-
ment agencies to collect vast amounts of data that may be used for causal forecasting. For 
example, supermarket scanners allow retailers to collect point-of-sale data that can then 
be used to help aid in planning sales, coupon targeting, and other marketing and planning 
efforts. These data can help answer important questions like which products tend to be pur-
chased together? One of the techniques used to answer such questions is regression analy-
sis. In this chapter we discuss the use of regression analysis as a causal forecasting method.
In Section 5.1 we discuss the various kinds of time series that a forecaster might be 
faced with in practice. These include a constant or horizontal pattern, a trend, a seasonal 
pattern, both a trend and a seasonal pattern, and a cyclical pattern. To build a quantitative 
forecasting model it is also necessary to have a measurement of forecast accuracy. Different 
measurements of forecast accuracy, as well as their respective advantages and disadvan-
tages, are discussed in Section 5.2. In Section 5.3 we consider the simplest case, which is 
a horizontal or constant pattern. For this pattern, we develop the classical moving average, 
weighted moving average, and exponential smoothing models. Many time series have a 
trend, and taking this trend into account is important; in Section 5.4 we provide regression 
models for finding the best model parameters when a linear trend is present, when the data 
show a seasonal pattern, or when the variable to be predicted has a causal relationship with 
other variables. Finally, in Section 5.5 we discuss considerations to be made when deter-
mining the best forecasting model to use.
A forecast is simply a pre-
diction of what will happen 
in the future. Managers 
must accept that regardless 
of the technique used, they 
will not be able to develop 
perfect forecasts.
NOTES AND COMMENTS
Virtually all large companies today rely on enter-
prise resource planning (ERP) software to aid their 
planning and operations. These software systems 
help the business run smoothly by collecting and 
efficiently storing company data, enabling it to be 
shared company-wide for planning at all levels: 
strategically, tactically, and operationally. Most 
ERP systems include a forecasting module to help 
plan for the future. SAP, one of the most widely 
used ERP systems includes a forecasting compo-
nent. This module allows the user to select from a 
number of forecasting techniques and/or have the 
system find a “best” model. The various forecasting 
methods and ways to measure the quality of a fore-
casting model discussed in this chapter are routinely 
available in software that supports forecasting.

 
5.1 Time Series Patterns 
205
Time Series Patterns
A time series is a sequence of observations on a variable measured at successive points in 
time or over successive periods of time. The measurements may be taken every hour, day, 
week, month, year, or any other regular interval. The pattern of the data is an important 
factor in understanding how the time series has behaved in the past. If such behavior can 
be expected to continue in the future, we can use it to guide us in selecting an appropriate 
forecasting method.
To identify the underlying pattern in the data, a useful first step is to construct a time 
series plot, which is a graphical presentation of the relationship between time and the time 
series variable; time is represented on the horizontal axis and values of the time series vari-
able are shown on the vertical axis. Let us first review some of the common types of data 
patterns that can be identified in a time series plot.
Horizontal Pattern
A horizontal pattern exists when the data fluctuate randomly around a constant mean over 
time. To illustrate a time series with a horizontal pattern, consider the 12 weeks of data in 
Table 5.1. These data show the number of gallons of gasoline (in 1000s) sold by a gasoline 
distributor in Bennington, Vermont, over the past 12 weeks. The average value or mean 
for this time series is 19.25 or 19,250 gallons per week. Figure 5.1 shows a time series 
plot for these data. Note how the data fluctuate around the sample mean of 19,250 gallons. 
Although random variability is present, we would say that these data follow a horizontal 
pattern.
The term stationary time series is used to denote a time series whose statistical proper-
ties are independent of time. In particular this means that
 1. The process generating the data has a constant mean.
 2. The variability of the time series is constant over time.
A time series plot for a stationary time series will always exhibit a horizontal pattern with 
random fluctuations. However, simply observing a horizontal pattern is not sufficient evi-
dence to conclude that the time series is stationary. More advanced texts on forecasting 
discuss procedures for determining whether a time series is stationary and provide methods 
for transforming a nonstationary time series into a stationary series.
5.1
We limit our discussion to 
time series for which the 
values of the series are 
recorded at equal intervals. 
Cases in which the observa-
tions are made at unequal 
intervals are beyond the 
scope of this text.
In Chapter 2 we discussed 
line charts, which are often 
used to graph time series.
file
WEB
Gasoline
For a formal definition of 
stationarity, see K. Ord 
and R. Fildes, Principles 
of Business Forecasting 
(Mason, OH: Cengage 
Learning, 2012), p. 155.
Week
Sales  
(1000s of gallons)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
17
21
19
23
18
16
20
18
22
20
15
22
TABLE 5.1  GASOLINE SALES TIME SERIES

206 
Chapter 5 Time Series Analysis and Forecasting
Changes in business conditions often result in a time series with a horizontal pat-
tern that shifts to a new level at some point in time. For instance, suppose the gasoline 
distributor signs a contract with the Vermont State Police to provide gasoline for state 
police cars located in southern Vermont beginning in week 13. With this new contract, 
the distributor naturally expects to see a substantial increase in weekly sales starting 
in week 13. Table 5.2 shows the number of gallons of gasoline sold for the original 
time series and the ten weeks after signing the new contract. Figure 5.2 shows the cor-
responding time series plot. Note the increased level of the time series beginning in 
week 13. This change in the level of the time series makes it more difficult to choose 
an appropriate forecasting method. Selecting a forecasting method that adapts well to 
FIGURE 5.1  GASOLINE SALES TIME SERIES PLOT
Sales (1000s of gallons)
Week
0
1
2
3
4
5
6
7
8
9
10
11
12
25
20
15
0
5
10
© Cengage Learning 2015
TABLE 5.2   GASOLINE SALES TIME SERIES AFTER OBTAINING THE 
 CONTRACT WITH THE VERMONT STATE POLICE
file
WEB
GasolineRevised
Week
Sales  
(1000s of gallons)
Week
Sales  
(1000s of gallons)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
17
21
19
23
18
16
20
18
22
20
15
12
13
14
15
16
17
18
19
20
21
22
22
31
34
31
33
28
32
30
29
34
33

 
5.1 Time Series Patterns 
207
changes in the level of a time series is an important consideration in many practical 
applications.
Trend Pattern
Although time series data generally exhibit random fluctuations, a time series may also 
show gradual shifts or movements to relatively higher or lower values over a  longer period 
of time. If a time series plot exhibits this type of behavior, we say that a trend pattern 
exists. A trend is usually the result of long-term factors such as population increases or 
decreases, shifting demographic characteristics of the population, improving technology, 
changes in the competitive landscape, and/or changes in consumer  preferences.
To illustrate a time series with a linear trend pattern, consider the time series 
of bicycle sales for a particular manufacturer over the past ten years, as shown in 
Table 5.3 and  Figure 5.3. Note that 21,600 bicycles were sold in year 1, 22,900 were 
FIGURE 5.2   GASOLINE SALES TIME SERIES PLOT AFTER OBTAINING THE 
CONTRACT WITH THE VERMONT STATE POLICE
10
00
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
40
35
30
20
5
25
15
Sales (1000s of gallons)
Week
TABLE 5.3  BICYCLE SALES TIME SERIES
file
WEB
Bicycle
Year
Sales (1000s)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
21.6
22.9
25.5
21.9
23.9
27.5
31.5
29.7
28.6
31.4

208 
Chapter 5 Time Series Analysis and Forecasting
sold in year 2, and so on. In year ten, the most recent year, 31,400 bicycles were sold. 
Visual inspection of the time series plot shows some up-and-down movement over 
the past ten years, but the time series seems also to have a systematically increasing 
or upward trend.
The trend for the bicycle sales time series appears to be linear and increasing over 
time, but sometimes a trend can be described better by other types of patterns. For in-
stance, the data in Table 5.4 and the corresponding time series plot in Figure 5.4 show the 
sales revenue for a cholesterol drug since the company won FDA approval for the drug 
ten years ago. The time series increases in a nonlinear fashion; that is, the rate of change 
of revenue does not increase by a constant amount from one year to the next. In fact, 
the revenue appears to be growing in an exponential fashion. Exponential relationships 
FIGURE 5.3  BICYCLE SALES TIME SERIES PLOT
Sales (1000s)
Year
0
1
2
3
4
5
6
7
8
9
34
10
20
22
24
26
28
30
32
TABLE 5.4  CHOLESTEROL DRUG REVENUE TIME SERIES
file
WEB
Cholesterol
Year
Revenue ($ millions)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
23.1
21.3
27.4
34.6
33.8
43.2
59.5
64.4
74.2
99.3

 
5.1 Time Series Patterns 
209
such as this are appropriate when the percentage change from one period to the next is 
relatively constant.
Seasonal Pattern
The trend of a time series can be identified by analyzing movements in historical data over 
multiple time periods. Seasonal patterns are recognized by observing recurring patterns 
over successive periods of time. For example, a manufacturer of swimming pools expects 
low sales activity in the fall and winter months, with peak sales in the spring and summer 
months to occur every year. Manufacturers of snow removal equipment and heavy clothing, 
however, expect the opposite yearly pattern. Not surprisingly, the pattern for a time series 
plot that exhibits a recurring pattern over a one-year period due to seasonal influences is 
called a seasonal pattern. Although we generally think of seasonal movement in a time 
series as occurring within one year, time series data can also exhibit seasonal patterns of 
less than one year in duration. For example, daily traffic volume shows within-the-day “sea-
sonal” behavior, with peak levels occurring during rush hours, moderate flow during the 
rest of the day and early evening, and light flow from midnight to early morning. Another 
example of an industry with sales that exhibit easily discernible seasonal patterns within a 
day is the restaurant industry.
As an example of a seasonal pattern, consider the number of umbrellas sold at a 
clothing store over the past five years. Table 5.5 shows the time series and Figure 5.5 
shows the corresponding time series plot. The time series plot does not indicate a long-
term trend in sales. In fact, unless you look carefully at the data, you might conclude 
that the data follow a horizontal pattern with random fluctuation. However, closer 
inspection of the fluctuations in the time series plot reveals a systematic pattern in the 
data that occurs within each year. Specifically, the first and third quarters have moder-
ate sales, the second quarter has the highest sales, and the fourth quarter tends to have 
the lowest sales volume. Thus, we would conclude that a quarterly seasonal pattern is 
present.
Trend and Seasonal Pattern
Some time series include both a trend and a seasonal pattern. For instance, the data in 
Table 5.6 and the corresponding time series plot in Figure 5.6 show quarterly smartphone 
FIGURE 5.4  CHOLESTEROL DRUG REVENUE TIMES SERIES PLOT ($ MILLIONS)
Revenue
Year
0
1
2
3
4
5
6
7
8
9
20
80
10
120
100
60
40
0

210 
Chapter 5 Time Series Analysis and Forecasting
sales for a particular manufacturer over the past four years. Clearly an increasing trend is 
present. However, Figure 5.6 also indicates that sales are lowest in the second quarter of 
each year and highest in quarters 3 and 4. Thus, we conclude that a seasonal pattern also ex-
ists for smartphone sales. In such cases we need to use a forecasting method that is capable 
of dealing with both trend and seasonality.
TABLE 5.5  UMBRELLA SALES TIME SERIES
file
WEB
Umbrella
Year
Quarter
Sales
1
2
3
4
5
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
125
153
106
 88
118
161
133
102
138
144
113
 80
109
137
125
109
130
165
128
 96
FIGURE 5.5  UMBRELLA SALES TIME SERIES PLOT
20
0
2
4
6
8
1
3
5
7
9
10 11 12 13 14 15 16 17 18 19 20
Time Period
180
60
80
100
120
140
160
40
Sales

 
5.1 Time Series Patterns 
211
Cyclical Pattern
A cyclical pattern exists if the time series plot shows an alternating sequence of points 
below and above the trendline that lasts for more than one year. Many economic time 
series exhibit cyclical behavior with regular runs of observations below and above the 
trendline. Often the cyclical component of a time series is due to multiyear business cy-
cles. For  example, periods of moderate inflation followed by periods of rapid inflation can  
TABLE 5.6  QUARTERLY SMARTPHONE SALES TIME SERIES
file
WEB
SmartPhoneSales
Year
Quarter
Sales ($1000s)
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
4.8
4.1
6.0
6.5
5.8
5.2
6.8
7.4
6.0
5.6
7.5
7.8
6.3
5.9
8.0
8.4
FIGURE 5.6  QUARTERLY SMARTPHONE SALES TIME SERIES PLOT
0.0
1.0
2.0
3.0
4.0
5.0
6.0
7.0
8.0
9.0
0
2
4
6
8
10
12
14
16
18
Period
Quarterly Smartphone Sales ($1000s)

212 
Chapter 5 Time Series Analysis and Forecasting
lead to a time series that alternates below and above a generally increasing trendline (e.g., 
a time series for housing costs). Business cycles are extremely difficult, if not impossible, 
to forecast. As a result, cyclical effects are often combined with long-term trend effects and 
referred to as trend-cycle effects. In this chapter we do not deal with cyclical effects that 
may be present in the time series.
Identifying Time Series Patterns
The underlying pattern in the time series is an important factor in selecting a forecast-
ing method. Thus, a time series plot should be one of the first analytic tools employed 
when trying to determine which forecasting method to use. If we see a horizontal pat-
tern, then we need to select a method appropriate for this type of pattern. Similarly, if 
we observe a trend in the data, then we need to use a forecasting method that is capable 
of handling a trend effectively. In the next section we discuss methods for assessing 
forecast accuracy. We then consider forecasting models that can be used in situations 
for which the underlying pattern is horizontal; in other words, no trend or seasonal ef-
fects are present. We then consider methods appropriate when trend and/or seasonality 
are present in the data.
Forecast Accuracy
In this section we begin by developing forecasts for the gasoline time series shown in 
Table 5.1 using the simplest of all the forecasting methods. We use the most recent 
week’s sales volume as the forecast for the next week. For instance, the distributor sold 
17 thousand gallons of gasoline in week 1; this value is used as the forecast for week 2. 
Next, we use 21, the actual value of sales in week 2, as the forecast for week 3, and so on. 
The forecasts obtained for the historical data using this method are shown in Table 5.7  
in the Forecast column. Because of its simplicity, this method is often referred to as a 
naïve forecasting method.
5.2
Week
Time 
Series 
Value
Forecast
Forecast 
Error
Absolute Value 
of Forecast 
Error
Squared 
Forecast 
Error
Percentage 
Error
Absolute Value 
of Percentage 
Error
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
17
21
19
23
18
16
20
18
22
20
15
22
17
21
19
23
18
16
20
18
22
20
  15  
Totals
4
22
4
25
22
4
22
4
22
25
  7
5
 4
 2
 4
 5
 2
 4
 2
 4
 2
 5
 7
41
 16
  4
 16
 25
  4
 16
  4
 16
  4
 25
 49
179
19.05
210.53
17.39
227.78
212.50
20.00
211.11
18.18
210.00
233.33
  31.82
1.19
 19.05
 10.53
 17.39
 27.78
 12.50
 20.00
 11.11
 18.18
 10.00
 33.33
 31.82
211.69
TABLE 5.7   COMPUTING FORECASTS AND MEASURES OF FORECAST ACCURACY USING THE MOST 
RECENT VALUE AS THE FORECAST FOR THE NEXT PERIOD

 
5.2 Forecast Accuracy 
213
How accurate are the forecasts obtained using this naïve forecasting method? To 
answer this question, we will introduce several measures of forecast accuracy. These 
measures are used to determine how well a particular forecasting method is able to 
reproduce the time series data that are already available. By selecting the method that 
is most accurate for the data already known, we hope to increase the likelihood that we 
will obtain more accurate forecasts for future time periods. The key concept associated 
with measuring forecast accuracy is forecast error. If we denote yt and y^t as the actual 
and forecasted values of the time series for period t, respectively, the forecasting error 
for period t is
FORECAST ERROR
 
et 5 yt 2 y^t 
(5.1)
That is, the forecast error for time period t is the difference between the actual and the 
forecasted values for period t.
For instance, because the distributor actually sold 21 thousand gallons of gasoline in 
week 2, and the forecast, using the sales volume in week 1, was 17 thousand gallons, the 
forecast error in week 2 is
 
Forecast Error in week 2 5 e2 5 y2 2 y^2 5 21 2 17 5 4
A positive error such as this indicates that the forecasting method underestimated the actual 
value of sales for the associated period. Next we use 21, the actual value of sales in week 2, 
as the forecast for week 3. Since the actual value of sales in week 3 is 19, the forecast error 
for week 3 is e3 5 19 2 21 5 22. In this case, the negative forecast error indicates the 
forecast overestimated the actual value for week 3. Thus, the forecast error may be positive 
or negative, depending on whether the forecast is too low or too high. A complete summary 
of the forecast errors for this naïve forecasting method is shown in Table 5.7 in the Forecast 
Error column. It is important to note that because we are using a past value of the time series 
to produce a forecast for period t, we do not have sufficient data to produce a naïve forecast 
for the first week of this time series.
A simple measure of forecast accuracy is the mean or average of the forecast errors. 
If we have n periods in our time series and k is the number of periods at the beginning 
of the time series for which we cannot produce a naïve forecast, the mean forecast error 
(MFE) is
MEAN FORECAST ERROR (MFE)
 
MFE 5
a
n
t5k11
et
n 2 k  
(5.2)
Table 5.7 shows that the sum of the forecast errors for the gasoline sales time series is 
5; thus, the mean or average error is 5/11 5 0.45. Because we do not have sufficient 
data to produce a naïve forecast for the first week of this time series, we must adjust 
our calculations in both the numerator and denominator accordingly. This is common in 
forecasting; we often use k past periods from the time series to produce forecasts, and 

214 
Chapter 5 Time Series Analysis and Forecasting
so we frequently cannot produce forecasts for the first k periods. In those instances the 
summation in the numerator starts at the first value of t for which we have produced a 
forecast (so we begin the summation at t 5 k 1 1), and the denominator (which is the 
number of periods in our time series for which we are able to produce a forecast) will also 
reflect these circumstances. In the gasoline example, although the time series consists of 
n 5 12 values, to compute the mean error we divided the sum of the forecast errors by 
11 because there are only 11 forecast errors (we cannot generate forecast sales for the 
first week using this naïve forecasting method). 
Also note that in the gasoline time series, the mean forecast error is positive, implying 
that the method is generally underforecasting; in other words, the observed values tend to 
be greater than the forecasted values. Because positive and negative forecast errors tend to 
offset one another, the mean error is likely to be small; thus, the mean error is not a very 
useful measure of forecast accuracy.
The mean absolute error (MAE) is a measure of forecast accuracy that avoids the 
problem of positive and negative forecast errors offsetting one another. As you might ex-
pect given its name, MAE is the average of the absolute values of the forecast errors:
MEAN ABSOLUTE ERROR (MAE)
 
MAE 5
a
n
t5k11
0 et 0
n 2 k  
(5.3)
This is also referred to as the mean absolute deviation (MAD). Table 5.7 shows that the sum 
of the absolute values of the forecast errors is 41; thus
 
MAE 5 average of the absolute value of the forecast errors 5 41
11 5 3.73
Another measure that avoids the problem of positive and negative errors offsetting each 
other is obtained by computing the average of the squared forecast errors. This measure of 
forecast accuracy is referred to as the mean squared error (MSE):
MEAN SQUARED ERROR (MSE)
 
MSE 5
a
n
t5k11
e2
t
n 2 k  
(5.4)
From Table 5.7, the sum of the squared errors is 179; hence,
 
MSE 5 average of the square of the forecast errors 5 179
11 5 16.27
The size of MAE or MSE depends upon the scale of the data. As a result, it is difficult to 
make comparisons for different time intervals (such as comparing a method of forecasting 

 
5.2 Forecast Accuracy 
215
monthly gasoline sales to a method of forecasting weekly sales) or to make comparisons 
across different time series (such as monthly sales of gasoline and monthly sales of oil fil-
ters). To make comparisons such as these we need to work with relative or percentage error 
measures. The mean absolute percentage error (MAPE) is such a measure. To calculate 
MAPE we use the formula:
MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
 
MAPE 5
a
n
t5k11
` aet
ytb100`
n 2 k
 
(5.5)
Table 5.7 shows that the sum of the absolute values of the percentage errors is
 
a
12
t5111
` aet
ytb100` 5 211.69
Thus the MAPE, which is the average of the absolute value of percentage forecast  errors, is
 
211.69
11
5 19.24%
In summary, using the naïve (most recent observation) forecasting method, we obtain 
the following measures of forecast accuracy:
 
MAE 5 3.73
 
MSE 5 16.27
 
MAPE 5 19.24%
These measures of forecast accuracy simply measure how well the forecasting method is 
able to forecast historical values of the time series. Now, suppose we want to forecast sales 
for a future time period, such as week 13. The forecast for week 13 is 22, the actual value of 
the time series in week 12. Is this an accurate estimate of sales for week 13? Unfortunately 
there is no way to address the issue of accuracy associated with forecasts for future time 
periods. However, if we select a forecasting method that works well for the historical data, 
and we have reason to believe the historical pattern will continue into the future, we should 
obtain forecasts that will ultimately be shown to be accurate.
Before closing this section, let us consider another method for forecasting the gasoline 
sales time series in Table 5.1. Suppose we use the average of all the historical data avail-
able as the forecast for the next period. We begin by developing a forecast for week 2. 
Because there is only one historical value available prior to week 2, the forecast for week 2 
is just the time series value in week 1; thus, the forecast for week 2 is 17 thousand gallons 
of gasoline. To compute the forecast for week 3, we take the average of the sales values in 
weeks 1 and 2. Thus,
 
y^3 5 17 1 21
2
5 19
Similarly, the forecast for week 4 is
 
y^4 5 17 1 21 1 19
3
5 19

216 
Chapter 5 Time Series Analysis and Forecasting
The forecasts obtained using this method for the gasoline time series are shown in Table 5.8 
in the Forecast column. Using the results shown in Table 5.8, we obtain the following values 
of MAE, MSE, and MAPE:
 
 MAE 5 28.81
11
5 2.44
 
 MSE 5 89.07
11
5 8.10
 
 MAPE 5 141.34
11
5 12.85%
We can now compare the accuracy of the two forecasting methods we have considered 
in this section by comparing the values of MAE, MSE, and MAPE for each method.
Naïve Method
Average of Past Values
MAE
MSE
MAPE
 3.73
16.27
19.24%
 2.44
 8.10
12.85%
For each of these measures, the average of past values provides more accurate forecasts for 
the next period than using the most recent observation.
Evaluating different forecasts based on historical accuracy is only helpful if historical 
patterns continue in to the future. As we note in Section 5.1, the 12 observations of Table 5.1 
comprise a stationary time series. In Section 5.1 we also mentioned that changes in business 
conditions often result in a time series that is not stationary. We discussed a situation in which 
the gasoline distributor signed a contract with the Vermont State Police to provide gasoline 
for state police cars located in southern Vermont. Table 5.2 shows the number of gallons of 
gasoline sold for the original time series and the ten weeks after signing the new contract, 
and Figure 5.2 shows the corresponding time series plot. Note the change in level in week 13 
for the resulting time series. When a shift to a new level such as this occurs, it takes several 
periods for the forecasting method that uses the average of all the historical data to adjust to 
the new level of the time series. However, in this case the simple naïve method adjusts very 
rapidly to the change in level because it uses only the most recent observation as the forecast
Week
Time 
Series 
Value
Forecast
Forecast 
Error
Absolute Value 
of Forecast 
Error
Squared 
Forecast 
Error
Percentage 
Error
Absolute Value 
of Percentage 
Error
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
17
21
19
23
18
16
20
18
22
20
15
22
17.00
19.00
19.00
20.00
19.60
19.00
19.14
19.00
19.33
19.40
19.00
Totals
4.00
0.00
4.00
22.00
23.60
1.00
21.14
3.00
0.67
24.40
  3.00
4.52
 4.00
 0.00
 4.00
 2.00
 3.60
 1.00
 1.14
 3.00
 0.67
 4.40
 3.00
26.81
16.00
 0.00
16.00
 4.00
12.96
 1.00
 1.31
 9.00
 0.44
19.36
 9.00
89.07
19.05
0.00
17.39
211.11
222.50
5.00
26.35
13.64
3.33
229.33
  13.64
2.75
 19.05
  0.00
 17.39
 11.11
 22.50
  5.00
  6.35
 13.64
  3.33
 29.33
 13.64
141.34
TABLE 5.8   COMPUTING FORECASTS AND MEASURES OF FORECAST ACCURACY USING THE 
 AVERAGE OF ALL THE HISTORICAL DATA AS THE FORECAST FOR THE NEXT PERIOD

 
5.3 Moving Averages and Exponential Smoothing 
217
Measures of forecast accuracy are important factors in comparing different forecast-
ing methods, but we have to be careful to not rely too heavily upon them. Good judgment 
and knowledge about business conditions that might affect the value of the variable to be 
forecast also have to be considered carefully when selecting a method. Historical forecast 
accuracy is not the sole consideration, especially if the pattern exhibited by the time series 
is likely to change in the future.
In the next section, we will introduce more sophisticated methods for developing fore-
casts for a time series that exhibits a horizontal pattern. Using the measures of forecast 
accuracy developed here, we will be able to assess whether such methods provide more 
accurate forecasts than we obtained using the simple approaches illustrated in this section. 
The methods that we will introduce also have the advantage that they adapt well to situa-
tions in which the time series changes to a new level. The ability of a forecasting method 
to adapt quickly to changes in level is an important consideration, especially in short-term 
forecasting situations.
Moving Averages and Exponential Smoothing
In this section we discuss two forecasting methods that are appropriate for a time series 
with a horizontal pattern: moving averages and exponential smoothing. These methods are 
capable of adapting well to changes in the level of a horizontal pattern such as what we saw 
with the extended gasoline sales time series (Table 5.2 and Figure 5.2). However, without 
modification they are not appropriate when considerable trend, cyclical, or seasonal effects 
are present. Because the objective of each of these methods is to smooth out random fluc-
tuations in the time series, they are referred to as smoothing methods. These methods are 
easy to use and generally provide a high level of accuracy for short-range forecasts, such 
as a forecast for the next time period.
Moving Averages
The moving averages method uses the average of the most recent k data values in the 
time series as the forecast for the next period. Mathematically, a moving average forecast 
of order k is 
MOVING AVERAGE FORECAST
 
 y^t11 5 a (most recent k data values)
k
5
a
t
i5t2k11
yi
k
 
 5 yt2k11 1 c1 yt21 1 yt
k
 
(5.6)
where
 y^t11 5 forecast of the time series for period t 1 1
 
yt 5 actual value of the time series in period t
 
k 5 number of periods of time series data used to generate the forecast
The term moving is used because every time a new observation becomes available 
for the time series, it replaces the oldest observation in the equation and a new average is 
computed. Thus, the periods over which the average is calculated change, or move, with 
each ensuing period
5.3

218 
Chapter 5 Time Series Analysis and Forecasting
To illustrate the moving averages method, let us return to the original 12 weeks of 
gasoline sales data in Table 5.1 and Figure 5.1. The time series plot in Figure 5.1 indicates 
that the gasoline sales time series has a horizontal pattern. Thus, the smoothing methods of 
this section are applicable.
To use moving averages to forecast a time series, we must first select the order k, or the 
number of time series values to be included in the moving average. If only the most recent 
values of the time series are considered relevant, a small value of k is preferred. If a greater 
number of past values are considered relevant, then we generally opt for a larger value of k. 
As previously mentioned, a time series with a horizontal pattern can shift to a new level over 
time. A moving average will adapt to the new level of the series and continue to provide 
good forecasts in k periods. Thus a smaller value of k will track shifts in a time series more 
quickly (the naïve approach discussed earlier is actually a moving average for k 5 1). On the 
other hand, larger values of k will be more effective in smoothing out random fluctuations. 
Thus, managerial judgment based on an understanding of the behavior of a time series is 
helpful in choosing an appropriate value of k.
To illustrate how moving averages can be used to forecast gasoline sales, we will use a 
three-week moving average (k 5 3). We begin by computing the forecast of sales in week 
4 using the average of the time series values in weeks 1 to 3.
 
y^4 5 average for weeks 1 to 3 5 17 1 21 1 19
3
5 19
Thus, the moving average forecast of sales in week 4 is 19 or 19,000 gallons of gaso-
line. Because the actual value observed in week 4 is 23, the forecast error in week 4 is  
e4 5 23 2 19 5 4.
We next compute the forecast of sales in week 5 by averaging the time series values 
in weeks 2–4.
 
y^5 5 average for weeks 2 to 4 5 21 1 19 1 23
3
5 21
Hence, the forecast of sales in week 5 is 21 and the error associated with this forecast is 
e5 5 18 2 21 5 23. A complete summary of the three-week moving average forecasts for 
the gasoline sales time series is provided in Table 5.9. Figure 5.7 shows the original time 
Week
Time 
Series 
Value
Forecast
Forecast 
Error
Absolute Value 
of Forecast 
Error
Squared 
Forecast 
Error
Percentage 
Error
Absolute Value 
of Percentage 
Error
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
17
21
19
23
18
16
20
18
22
20
15
22
19
21
20
19
18
18
20
20
19
Totals
  4
23
24
  1
  0
  4
  0
25
  3
  0
 4
 3
 4
 1
 0
 4
 0
 5
 3
24
16
 9
16
 1
 0
16
 0
25
 9
92
  17.39
216.67
225.00
   5.00
   0.00
  18.18
   0.00
233.33
  13.64
220.79
 17.39
 16.67
 25.00
  5.00
  0.00
 18.18
  0.00
 33.33
 13.64
129.21
TABLE 5.9  SUMMARY OF THREE-WEEK MOVING AVERAGE CALCULATIONS

 
5.3 Moving Averages and Exponential Smoothing 
219
series plot and the three-week moving average forecasts. Note how the graph of the mov-
ing average forecasts has tended to smooth out the random fluctuations in the time series.
To forecast sales in week 13, the next time period in the future, we simply compute the 
average of the time series values in weeks 10, 11, and 12.
 
y^13 5 average for weeks 10 to 12 5 20 1 15 1 22
3
5 19
Thus, the forecast for week 13 is 19, or 19,000 gallons of gasoline.
To show how Excel can be used to develop forecasts using the moving averages method, 
we develop a forecast for the gasoline sales time series in Table 5.1 and Figure 5.1. We as-
sume that the user has entered the week into rows 2 through 13 of column A and the sales 
data for the 12 weeks into worksheet rows 2 through 13 of column B.
The following steps can be used to produce a three-week moving average:
Step 1. Click the DATA tab in the Ribbon
Step 2. Click Data Analysis in the Analysis group
Step 3. When the Data Analysis dialog box appears (Figure 5.8), select Moving 
 Average and click OK
Step 4.  When the Moving Average dialog box appears (Figure 5.9):
Enter B2:B13 in the Input Range: box
Enter 3 in the Interval: box
Enter C3 in the Output Range: box
Click OK
Once you have completed this step, the three-week moving average forecasts will ap-
pear in column C of the worksheet as shown in Figure 5.10. Note that forecasts for 
periods of other lengths can be computed easily by entering a different value in the 
Interval: box.
file
WEB
Gasoline
If Data Analysis does not 
appear in your Analysis 
group, you will have to 
load the Analysis ToolPak 
Add-in into Excel. To do so, 
click the FILE tab in the 
Ribbon and click Options. 
When the Excel Options 
dialog box appears, click 
Add-Ins from the menu. 
Next to Manage:, select 
Excel Add-ins and click 
Go . . . at the bottom of the 
dialog box. When the Add-
Ins dialog box appears, 
select Analysis ToolPak 
and click OK.
FIGURE 5.7   GASOLINE SALES TIME SERIES PLOT AND THREE-WEEK 
 MOVING AVERAGE FORECASTS
Sales (1000s of gallons)
Week
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
25
20
15
10
5
0
Three-week moving
average forecasts

220 
Chapter 5 Time Series Analysis and Forecasting
FIGURE 5.8  DATA ANALYSIS DIALOG BOX
FIGURE 5.9  MOVING AVERAGE DIALOG BOX
FIGURE 5.10   EXCEL OUTPUT FOR MOVING AVERAGE FORECAST FOR 
 GASOLINE DATA
A
1
17
#N/A
#N/A
19
21
20
19
18
18
20
20
19
19
21
19
23
18
16
20
18
22
15
22
20
2
3
4
5
6
7
8
9
10
11
12
13
1
2
3
4
5
6
7
8
9
10
11
12
13
14
B
C
Week Sales (1000s of gallons)

 
5.3 Moving Averages and Exponential Smoothing 
221
Forecast Accuracy
In Section 5.2 we discussed three measures of forecast accuracy: mean absolute error 
(MAE), mean squared error (MSE), and mean absolute percentage error (MAPE). Using 
the three-week moving average calculations in Table 5.9, the values for these three mea-
sures of forecast accuracy are
 
 MAE 5
a
n
t5k11
0 et 0
n 2 k
5 24
9 5 2.67
 
 MSE 5
a
n
t5k11
e2
t
n 2 k 5 92
9 5 10.22
 
 MAPE 5
a
n
t5k11
` aet
ytb100`
n 2 k
5 129.21
9
5 14.36%
In Section 5.2 we showed that using the most recent observation as the forecast for the 
next week (a moving average of order k 5 1) resulted in values of MAE 5 3.73, MSE 5 
16.27, and MAPE 5 19.24 percent. Thus, in each case the three-week moving average ap-
proach has provided more accurate forecasts than simply using the most recent observation 
as the forecast. Also note how we have revised the formulas for the MAE, MSE, and MAPE 
to reflect that our use of a three-week moving average leaves us with insufficient data to 
generate forecasts for the first three weeks of our time series.
To determine whether a moving average with a different order k can provide more 
accurate forecasts, we recommend using trial and error to determine the value of k that 
minimizes the MSE. For the gasoline sales time series, it can be shown that the minimum 
value of MSE corresponds to a moving average of order k 5 6 with MSE 5 6.79. If we 
are willing to assume that the order of the moving average that is best for the historical 
data will also be best for future values of the time series, the most accurate moving aver-
age forecasts of gasoline sales can be obtained using a moving average of order k 5 6.
Exponential Smoothing
Exponential smoothing uses a weighted average of past time series values as a forecast. 
The exponential smoothing model is 
EXPONENTIAL SMOOTHING FORECAST:
 
y^t11 5 αyt 1 (1 2 α)y^t 
(5.7)
where
y^t11 5 forecast of the time series for period t 1 1
 yt 5 actual value of the time series in period t
 y^t 5 forecast of the time series for period t
 α 5 smoothing constant (0 # a # 1)
Equation (5.7) shows that the forecast for period t 1 1 is a weighted average of the 
actual value in period t and the forecast for period t. The weight given to the actual value 
in period t is the smoothing constant a, and the weight given to the forecast in period t is  
1 2 a. It turns out that the exponential smoothing forecast for any period is actually a 
weighted average of all the previous actual values of the time series. Let us illustrate by 
working with a time series involving only three periods of data: y1, y2, and y3.
If a large amount of data 
are available to build the 
forecast models, we suggest 
dividing the data into train-
ing and validation sets, and 
then determining the best 
value of k as the value that 
minimizes the MSE for the 
validation set. We discuss 
the use of training and vali-
dation sets in more detail in 
Section 5.5.

222 
Chapter 5 Time Series Analysis and Forecasting
To initiate the calculations, we let y^1 equal the actual value of the time series in period 1; 
that is, y^1 5 y1. Hence, the forecast for period 2 is
 
y^2 5 αy1 1 (1 2 α)y^1
 
 5 αy1 1 (1 2 α)y1
 
 5 y1
We see that the exponential smoothing forecast for period 2 is equal to the actual value of 
the time series in period 1.
The forecast for period 3 is
 
y^3 5 αy2 1 (1 2 α)y^2 5 αy2 1 (1 2 α)y1
Finally, substituting this expression for y^3 into the expression for y^4, we obtain
 
y^4 5 αy3 1 (1 2 α)y^3
 
 5 αy3 1 (1 2 α) (αy2 1 (1 2 α)y1)
 
 5 αy3 1 α(1 2 α)y2 1 (1 2 α) 2y1
We now see that y^4 is a weighted average of the first three time series values. The sum of 
the coefficients, or weights, for y1, y2, and y3 equals 1. A similar argument can be made 
to show that, in general, any forecast y^t11 is a weighted average of all the t previous time 
series values.
Despite the fact that exponential smoothing provides a forecast that is a weighted aver-
age of all past observations, all past data do not need to be retained to compute the forecast 
for the next period. In fact, equation (5.7) shows that once the value for the smoothing con-
stant a is selected, only two pieces of information are needed to compute the forecast for 
period t 1 1: yt, the actual value of the time series in period t; and y^t, the forecast for period t.
To illustrate the exponential smoothing approach to forecasting, let us again consider 
the gasoline sales time series in Table 5.1 and Figure 5.1. As indicated previously, to 
initialize the calculations we set the exponential smoothing forecast for period 2 equal 
to the actual value of the time series in period 1. Thus, with y1 5 17, we set y^2 5 17 to 
initiate the computations. Referring to the time series data in Table 5.1, we find an actual 
time series value in period 2 of y2 5 21. Thus, in period 2 we have a forecast error of 
e2 5 21 2 17 5 4.
Continuing with the exponential smoothing computations using a smoothing constant 
of a 5 0.2, we obtain the following forecast for period 3:
 
y^3 5 0.2y2 1 0.8y^2 5 0.2(21) 1 0.8(17) 5 17.8
Once the actual time series value in period 3, y3 5 19, is known, we can generate a forecast 
for period 4 as follows.
 
y^4 5 0.2y3 1 0.8y^3 5 0.2(19) 1 0.8(17.8) 5 18.04
Continuing the exponential smoothing calculations, we obtain the weekly forecast val-
ues shown in Table 5.10. Note that we have not shown an exponential smoothing forecast or 
a forecast error for week 1 because no forecast was made (we used actual sales for week 1 
as the forecasted sales for week 2 to initialize the exponential smoothing process). For 
week 12, we have y12 5 22 and y^12 5 18.48. We can we use this information to generate a 
forecast for week 13.
 
y^13 5 0.2y12 1 0.8y^12 5 0.2(22) 1 0.8(18.48) 5 19.18
Thus, the exponential smoothing forecast of the amount sold in week 13 is 19.18, or 
19,180 gallons of gasoline. With this forecast, the firm can make plans and decisions 
 accordingly.

 
5.3 Moving Averages and Exponential Smoothing 
223
Figure 5.11 shows the time series plot of the actual and forecast time series values. 
Note in particular how the forecasts smooth out the irregular or random fluctuations in the 
time series.
To show how Excel can be used for exponential smoothing, we again develop a fore-
cast for the gasoline sales time series in Table 5.1 and Figure 5.1. We use Gasoline file 
which has the week in rows 2 through 13 of column A and the sales data for the 12 weeks  
file
WEB
Gasoline
Week
Time Series 
Value
Forecast
Forecast 
Error
Squared Forecast 
 Error
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
17
21
19
23
18
16
20
18
22
20
15
22
17.00
17.80
18.04
19.03
18.83
18.26
18.61
18.49
19.19
19.35
18.48
Totals
  4.00
  1.20
  4.96
21.03
22.83
  1.74
20.61
  3.51
  0.81
24.35
  3.52
 10.92
16.00
 1.44
24.60
 1.06
 8.01
 3.03
 0.37
12.32
 0.66
18.92
12.39
98.80
TABLE 5.10   SUMMARY OF THE EXPONENTIAL SMOOTHING FORECASTS 
AND FORECAST ERRORS FOR THE GASOLINE SALES TIME 
S ERIES WITH SMOOTHING CONSTANT a 5 0.2
FIGURE 5.11   ACTUAL AND FORECAST GASOLINE TIME SERIES WITH 
SMOOTHING CONSTANT a 5 0.2
Sales (1000s of gallons)
Week
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
25
20
15
10
5
0
Forecast time series
with a 5 0.2
Actual time
series

224 
Chapter 5 Time Series Analysis and Forecasting
in rows 2 through 13 of column B. We use a 5 0.2. The following steps can be used to 
produce a forecast.
Step 1. Click the DATA tab in the Ribbon
Step 2. Click Data Analysis in the Analysis group
Step 3. When the Data Analysis dialog box appears (Figure 5.12), select Exponential 
Smoothing and click OK
Step 4. When the Exponential Smoothing dialog box appears (Figure 5.13):
Enter B2:B13 in the Input Range: box
Enter 0.8 in the Damping factor: box
Enter C2 in the Output Range: box
Click OK
Once you have completed this step, the exponential smoothing forecasts will appear in 
column C of the worksheet as shown in Figure 5.14. Note that the value we entered in the 
Damping factor: box is 1 2 a; forecasts for other smoothing constants can be computed 
easily by entering a different value for 1 2 a in the Damping factor: box.
Forecast Accuracy
In the preceding exponential smoothing calculations, we used a smoothing constant of 
a 5 0.2. Although any value of a between 0 and 1 is  acceptable, some values will yield 
FIGURE 5.12   DATA ANALYSIS DIALOG BOX
FIGURE 5.13  EXPONENTIAL SMOOTHING DIALOG BOX

 
5.3 Moving Averages and Exponential Smoothing 
225
more accurate forecasts than others. Insight into choosing a good value for a can be 
obtained by rewriting the basic exponential smoothing model as follows:
 
y^t11 5 αyt 1 (1 2 α)y^t
 
 5 αyt 1 y^t 2 αy^t
 
 5 y^t 1 α(yt 2 y^t) 5 y^t 1 αet
Thus, the new forecast y^t11 is equal to the previous forecast y^t plus an adjustment, which is 
the smoothing constant a times the most recent forecast error, et 5 yt 2 y^t. In other words, 
the forecast in period t 1 1 is obtained by adjusting the forecast in period t by a fraction of 
the forecast error from period t. If the time series contains substantial random variability, 
a small value of the smoothing constant is preferred. The reason for this choice is that if 
much of the forecast error is due to random variability, we do not want to overreact and 
adjust the forecasts too quickly. For a time series with relatively little random variability, 
a forecast error is more likely to represent a real change in the level of the series. Thus, 
larger values of the smoothing constant provide the advantage of quickly adjusting the 
forecasts to changes in the time series, thereby allowing the forecasts to react more quickly 
to changing conditions.
The criterion we will use to determine a desirable value for the smoothing constant 
a is the same as that proposed for determining the order or number of periods of data to 
 include in the moving averages calculation; that is, we choose the value of a that minimizes 
the MSE. A summary of the MSE calculations for the exponential smoothing forecast of 
gasoline sales with a 5 0.2 is shown in Table 5.10. Note that there is one less squared error 
term than the number of time periods; this is because we had no past values with which to 
make a forecast for period 1. The value of the sum of squared forecast errors is 98.80; hence 
MSE 5 98.80/11 5 8.98. Would a different value of a provide better results in terms of a 
lower MSE value? Trial and error is often used to determine whether a different smooth-
ing constant a can provide more accurate forecasts, but we can avoid trial and error and 
determine the value of a that minimizes MSE through the use of nonlinear optimization. 
Nonlinear optimization is discussed in Chapter 10.
Similar to our note  related 
to moving averages, 
if enough data are avail-
able then a should be 
 chosen to minimize the 
MSE of the validation set.
FIGURE 5.14   EXCEL OUTPUT FOR EXPONENTIAL SMOOTHING FORECAST 
FOR GASOLINE DATA
A
1
2
3
4
5
6
7
8
9
10
1
17
#N/A
17
17.8
18.04
19.032
18.8256
18.2605
18.6084
18.4867
19.1894
19.3515
18.4812
21
19
23
18
16
20
18
22
15
22
20
2
3
4
5
6
7
8
9
10
11
12
11
12
13
B
C
Week Sales (1,000s of gallons)

226 
Chapter 5 Time Series Analysis and Forecasting
Using Regression Analysis for Forecasting
As we saw in Chapter 4, regression analysis is a statistical technique that can be used to 
develop a mathematical equation showing how variables are related. In regression termi-
nology, the variable that is being predicted is called the dependent, or response, variable, 
and the variable or variables being used to predict the value of the dependent variable 
are called the independent, or predictor, variables. Regression analysis involving one 
independent variable and one dependent variable for which the relationship between the 
variables is approximated by a straight line is called simple linear regression. Regression 
analysis involving two or more independent variables is called multiple regression analy-
sis. In this section we will show how to use regression analysis to develop forecasts for 
a time series that has a trend, a seasonal pattern, and both a trend and a seasonal pattern. 
We will also show how to use regression analysis to develop forecast models that include 
causal variables.
Linear Trend Projection
We now consider forecasting methods that are appropriate for time series that exhibit 
trend patterns and show how regression analysis can be used to forecast a time series 
with a linear trend. In Section 5.1 we used the bicycle sales time series in Table 5.3 
and Figure 5.3 to illustrate a time series with a trend pattern. Let us now use this time 
series to illustrate how regression analysis can be used to forecast a time series with 
a linear trend. Although the time series plot in Figure 5.3 shows some up-and-down 
movement over the past ten years, we might agree that the linear trendline shown in 
Figure 5.3  provides a reasonable approximation of the long-run movement in the series. 
We can use regression analysis to develop such a linear trendline for the bicycle sales 
time series.
Because simple linear regression analysis yields the linear relationship between the 
independent variable and the dependent variable that minimizes the MSE, we can use this 
approach to find a best-fitting line to a set of data that exhibits a linear trend. In finding a 
linear trend, the variable to be forecasted (y, the actual value of the time series period t) is 
the dependent variable and the trend variable (time period t) is the independent variable. 
We will use the following notation for our linear trendline.
 
y^t 5 b0 1 b1t 
(5.8)
5.4
NOTES AND COMMENTS
1. Spreadsheet packages are effective tools for 
implementing exponential smoothing. With 
the time series data and the forecasting for-
mulas in a spreadsheet such as the one shown 
in Table  5.10, you can use the MAE, MSE, 
and MAPE to evaluate different values of the 
smoothing constant a.
2. Moving averages and exponential smooth-
ing provide the foundation for much of time 
series analysis, and many more sophisticated 
refinements of these methods have been de-
veloped. These include but are not limited to 
weighted moving averages, double moving 
averages, Brown’s method for double ex-
ponential smoothing, and triple exponential 
smoothing. Students who are interested in 
these and other more advanced methods for 
time series analysis are encouraged to read 
about them in the references listed at the end 
of this textbook.

 
5.4 Using Regression Analysis for Forecasting 
227
where
 y^t 5 forecast of sales in period t
 t 5 time period
b0 5 the y-intercept of the linear trendline 
b1 5 the slope of the linear trendline
In equation (5.8), the time variable begins at t 5 1 corresponding to the first time series 
observation (year 1 for the bicycle sales time series) and continues until t 5 n corresponding 
to the most recent time series observation (year 10 for the bicycle sales time series). Thus, 
for the bicycle sales time series t 5 1 corresponds to the oldest time series value, and t 5 10 
corresponds to the most recent year.
Excel can be used to compute the estimated intercept b0 and slope b1. The Excel output 
for a regression analysis of the bicycle sales data are provided in Figure 5.15.
We see in this output that the estimated intercept b0 is 20.4 (shown in cell B17) and the 
estimated slope b1 is 1.1 (shown in cell B18). Thus
 
y^t 5 20.4 1 1.1t 
(5.9)
is the regression equation for the linear trend component for the bicycle sales time series. 
The slope of 1.1 in this trend equation indicates that over the past ten years the firm has 
experienced an average growth in sales of about 1100 units per year. If we assume that the 
past ten-year trend in sales is a good indicator for the future, we can use equation (5.9) to 
project the trend component of the time series. For example, substituting t 5 11 into equa-
tion (5.9) yields next year’s trend projection, y^11:
 
y^11 5 20.4 1 1.1(11) 5 32.5
Thus, the linear trend model yields a sales forecast of 32,500 bicycles for the next year.
FIGURE 5.15   EXCEL SIMPLE LINEAR REGRESSION OUTPUT FOR TRENDLINE MODEL FOR BICYCLE 
SALES DATA
Regression Statistics
Multiple R
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
df
Coefﬁcients Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0% Upper 99.0%
1
99.825
99.825
3.8375
26.01302932
0.000929509
30.7
130.525
0.874526167
0.764796016
0.735395518
1.958953802
10
8
9
20.4
1.338220211 15.24412786 3.39989E-07
17.31405866 23.48594134 15.90975286
0.376331148
24.89024714
1.823668852
1.59734448
0.60265552
0.000929509
5.100296983
0.215673715
1.1
SS
MS
F
Signiﬁcance F
Regression
Residual
Total
Intercept
Year
A
B
C
D
E
F
G
H
I
SUMMARY OUTPUT
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

228 
Chapter 5 Time Series Analysis and Forecasting
We can also use the trendline to forecast sales farther into the future. Using equa-
tion (5.9), we develop annual forecasts of bicycle sales for two and three years into the 
future as follows:
 
y^12 5 20.4 1 1.1(12) 5 33.6
 
y^13 5 20.4 1 1.1(13) 5 34.7
The forecasted value increases by 1100 bicycles in each year.
Note that in this example we are not using past values of the time series to produce 
forecasts, so we can produce a forecast for each period of the time series; that is, k 5 0 in 
equations (5.3), (5.4), and (5.5) to calculate the MAE, MSE, or MAPE.
We can also use more complex regression models to fit nonlinear trends. For example, 
if we also include t2 and t3 as independent variables in our model, the estimated regression 
equation would become
 
y^t 5 b0 1 b1t 1 b2t2 1 b3t3
This model provides a forecast of a time series with curvilinear characteristics over time.
Another type of regression-based forecasting model occurs whenever all the indepen-
dent variables are previous values of the same time series. For example, if the time series 
values are denoted y1, y2, . . . , yn, we might try to find an estimated regression equation 
relating yt to the most recent time series values, yt21, yt22, and so on. If we use the actual 
values of the time series for the three most recent periods as independent variables, the 
estimated regression equation would be
 
y^t 5 b0 1 b1yt21 1 b2yt22 1 b3yt23
Regression models such as this in which the independent variables are previous values of 
the time series are referred to as autoregressive models.
Seasonality
To the extent that seasonality exists, we need to incorporate it into our forecasting models to 
ensure accurate forecasts. We begin the section by considering a seasonal time series with 
no trend and then discuss how to model seasonality with a linear trend.
Seasonality Without Trend
Let us consider again the data from Table 5.5, the number of umbrellas sold at a clothing 
store over the past five years. As we see in the time series plot provided in Figure 5.5, the 
data do not suggest any long-term trend in sales. In fact, unless you look carefully at the 
data, you might conclude that the data follow a horizontal pattern with random fluctuation 
and that single exponential smoothing could be used to forecast sales. However, closer 
inspection of the time series plot reveals a pattern in the fluctuations. The first and third 
quarters have moderate sales, the second quarter the highest sales, and the fourth quarter 
tends to be the lowest quarter in terms of sales volume. Thus, we conclude that a quarterly 
seasonal pattern is present.
We can model a time series with a seasonal pattern by treating the season as a dummy 
variable. As indicated in Chapter 4, categorical variables are data used to categorize 
observations of data, and k 2 1 dummy variables are required to model a categorical 
variable that has k levels. Thus, we need three dummy variables to model four seasons. 
For instance, in the umbrella sales time series, the quarter to which each observation 
corresponds is treated as a season; it is a categorical variable with four levels: quarter 1, 
quarter 2, quarter 3, and quarter 4. Thus, to model the seasonal effects in the umbrella 
Because autoregressive 
models typically violate the 
conditions necessary for 
inference in least squares 
regression, one must be 
careful if testing hypotheses 
or estimating confidence 
intervals in autoregressive 
models. There are special 
methods for constructing 
autoregressive models, but 
they are beyond the scope 
of this book.

 
5.4 Using Regression Analysis for Forecasting 
229
time series we need 4 2 1 5 3 dummy variables. The three dummy variables can be 
coded as follows:
 
Qtr1t 5 e1 if period t is a quarter 1
0 otherwise
 
Qtr2t 5 e1 if period t is a quarter 2
0 otherwise
 
Qtr3t 5 e1 if period t is a quarter 3
0 otherwise
Using y^t to denote the forecasted value of sales for period t, the general form of the equation 
relating the number of umbrellas sold to the quarter the sales take place follows:
 
y^t 5 b0 1 b1Qtr1t 1 b2Qtr2t 1 b3Qtr3t 
(5.10)
Note that the fourth quarter will be denoted by setting all three dummy variables to 0.  Table 5.11 
shows the umbrella sales time series with the coded values of the dummy variables shown. We 
can use a multiple linear regression model to find the values of b0, b1, b2, and b3 that minimize 
the sum of squared errors. For this regression model, yt is the dependent variable, and the quar-
terly dummy variables Qtr1t, Qtr2t, and Qtr3t are the independent variables.
Using the data in Table 5.11 and regression analysis, we obtain the following equation:
 
y^t 5 95.0 1 29.0Qtr1t 1 57.0Qtr2t 1 26.0Qtr3t 
(5.11) 
We can use equation (5.11) to forecast sales of every quarter for next year:
 
 Quarter 1: Sales 5 95.0 1 29.0(1) 1 57.0(0) 1 26.0(0) 5 124
 
 Quarter 2: Sales 5 95.0 1 29.0(0) 1 57.0(1) 1 26.0(0) 5 152
 
 Quarter 3: Sales 5 95.0 1 29.0(0) 1 57.0(0) 1 26.0(1) 5 121
 
Quarter 4: Sales 5 95.0 1 29.0(0) 1 57.0(0) 1 26.0(0) 5 95
Period
Year
Quarter
Qtr1
Qtr2
Qtr3
Sales
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
1
2
3
4
5
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
125
153
106
 88
118
161
133
102
138
144
113
 80
109
137
125
109
130
165
128
 96
TABLE 5.11  UMBRELLA SALES TIME SERIES WITH DUMMY VARIABLES

230 
Chapter 5 Time Series Analysis and Forecasting
It is interesting to note that we could have obtained the quarterly forecasts for next year 
by simply computing the average number of umbrellas sold in each quarter. Nonetheless, 
for more complex problem situations, such as dealing with a time series that has both trend 
and seasonal effects, this simple averaging approach will not work.
Seasonality with Trend
We now consider situations for which the time series contains both seasonal effects and 
a linear trend by showing how to forecast the quarterly sales of smartphones introduced 
in Section 5.1. The data for the smartphone time series are shown in Table 5.6. The time 
series plot in Figure 5.6 indicates that sales are lowest in the second quarter of each year 
and increase in quarters 3 and 4. Thus, we conclude that a seasonal pattern exists for smart-
phone sales. However, the time series also has an upward linear trend that will need to be 
accounted for in order to develop accurate forecasts of quarterly sales. This is easily done 
by combining the dummy variable approach for handling seasonality with the approach for 
handling a linear trend discussed earlier in this section.
The general form of the regression equation for modeling both the quarterly seasonal 
effects and the linear trend in the smartphone time series is
 
y^t 5 b0 1 b1Qtr1t 1 b2Qtr2t 1 b3Qtr3t 1 b4t 
(5.12)
where
 y^t 5 forecast of sales in period t
Qtr1t 5 1 if time period t corresponds to the first quarter of the year; 0, otherwise
Qtr2t 5 1 if time period t corresponds to the second quarter of the year; 0, otherwise
Qtr3t 5 1 if time period t corresponds to the third quarter of the year; 0, otherwise
 t 5 time period (quarter)
For this regression model yt is the dependent variable and the quarterly dummy variables 
Qtr1t  , Qtr2t  , and Qtr3t and the time period t are the independent variables.
Table 5.12 shows the revised smartphone sales time series that includes the coded 
values of the dummy variables and the time period t. Using the data in Table 5.12 with the 
regression model that includes both the seasonal and trend components, we obtain the fol-
lowing equation that minimizes our sum of squared errors:
 
y^t 5 6.07 2 1.36Qtr1t 2 2.03Qtr2t 2 0.304Qtr3t 1 0.146t 
(5.13)
TABLE 5.12   SMARTPHONE SALES TIME SERIES WITH DUMMY VARIABLES 
AND TIME PERIOD
Period
Year
Quarter
Qtr1
Qtr2
Qtr3
Sales (1000s)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
4.8
4.1
6.0
6.5
5.8
5.2
6.8
7.4
6.0
5.6
7.5
7.8
6.3
5.9
8.0
8.4

 
5.4 Using Regression Analysis for Forecasting 
231
We can now use equation (5.13) to forecast quarterly sales for next year. Next year is year 
5 for the smartphone sales time series, that is, time periods 17, 18, 19, and 20.
Forecast for time period 17 (quarter 1 in year 5)
 
y^17 5 6.07 2 1.36(1) 2 2.03(0) 2 0.304(0) 1 0.146(17) 5 7.19
Forecast for time period 18 (quarter 2 in year 5)
 
y^18 5 6.07 2 1.36(0) 2 2.03(1) 2 0.304(0) 1 0.146(18) 5 6.67
Forecast for time period 19 (quarter 3 in year 5)
 
y^19 5 6.07 2 1.36(0) 2 2.03(0) 2 0.304(1) 1 0.146(19) 5 8.54
Forecast for time period 20 (quarter 4 in year 5)
 
y^20 5 6.07 2 1.36(0) 2 2.03(0) 2 0.304(0) 1 0.146(20) 5 8.99
Thus, accounting for the seasonal effects and the linear trend in smartphone sales, the esti-
mates of quarterly sales in year 5 are 7190, 6670, 8540, and 8990.
The dummy variables in the equation actually provide four equations, one for each quar-
ter. For instance, if time period t corresponds to quarter 1, the estimate of quarterly sales is
 
Quarter 1: Sales 5 6.07 2 1.36(1) 2 2.03(0) 2 0.304(0) 1 0.146t 5 4.71 1 0.146t
Similarly, if time period t corresponds to quarters 2, 3, and 4, the estimates of quarterly 
sales are:
 
Quarter 2: Sales 5 6.07 2 1.36(0) 2 2.03(1) 2 0.304(0) 1 0.146t 5 4.04 1 0.146t
 
Quarter 3: Sales 5 6.07 2 1.36(0) 2 2.03(0) 2 0.304(1) 1 0.146t 5 5.77 1 0.146t
 
Quarter 4: Sales 5 6.07 2 1.36(0) 2 2.03(0) 2 0.304(0) 1 0.146t 5 6.07 1 0.146t
The slope of the trendline for each quarterly forecast equation is 0.146, indicating a con-
sistent growth in sales of about 146 phones per quarter. The only difference in the four 
equations is that they have different intercepts.
In the smartphone sales example, we showed how dummy variables can be used to ac-
count for the quarterly seasonal effects in the time series. Because there were four levels for 
the categorical variable season, three dummy variables were required. However, many busi-
nesses use monthly rather than quarterly forecasts. For monthly data, season is a categorical 
variable with 12 levels, and thus 12 2 1 5 11 dummy variables are required to capture 
monthly seasonal effects. For example, the 11 dummy variables could be coded as follows:
 
 Month1t 5 e1 if period t is January
0 otherwise
 
 Month2t 5 e1 if period t is February
0 otherwise
 
(  
 
 Month11t 5 e1 if period t is November
0 otherwise
Other than this change, the approach for handling seasonality remains the same. Time series 
data collected at other intervals can be handled in a similar manner.
Using Regression Analysis as a Causal  
Forecasting Method
The methods discussed for estimating linear trends and seasonal effects make use of pat-
terns in historical values of the variable to be forecast; these methods are classified as 

232 
Chapter 5 Time Series Analysis and Forecasting
time series methods because they rely on past values of the variable to be forecast when 
developing the model. However, the relationship of the variable to be forecast with other 
variables may also be used to develop a forecasting model. Generally such models include 
only variables that are believed to cause changes in the variable to be forecast, such as
● 
 Advertising expenditures when sales is to be forecast.
● 
 The mortgage rate when new housing construction is to be forecast.
● 
 Grade point average when starting salaries for recent college graduates is to be 
forecast.
● 
 The price of a product when the demand of the product is to be forecast.
● 
 The value of the Dow Jones Industrial Average when the value of an individual 
stock is to be forecast.
● 
 Daily high temperature when electricity usage is to be forecast.
Because these variables are used as independent variables when we believe they cause 
changes in the value of the dependent variable, forecasting models that include such vari-
ables as independent variables are referred to as causal models. It is important to note here 
that the forecasting model provides evidence only of association between an independent 
variable and the variable to be forecast. The model does not provide evidence of a causal 
relationship between an independent variable and the variable to be forecast, and the con-
clusion that a causal relationship exists must be based on practical experience.
To illustrate how regression analysis is used as a causal forecasting method, we con-
sider the sales forecasting problem faced by Armand’s Pizza Parlors, a chain of Italian res-
taurants doing business in a five-state area. Historically, the most successful locations have 
been near college campuses. The managers believe that quarterly sales for these restaurants 
(denoted by y) are related positively to the size of the student population (denoted by x); 
that is, restaurants near campuses with a large population tend to generate more sales than 
those located near campuses with a small population.
Using regression analysis we can develop an equation showing how the dependent vari-
able y is related to the independent variable x. This equation can then be used to forecast 
quarterly sales for restaurants located near college campuses given the size of the student 
population. This is particularly helpful for forecasting sales of new restaurant locations. 
For instance, suppose that management wants to forecast sales for a new restaurant that it 
is considering opening near a college campus. Because no historical data are available on 
sales for a new restaurant, Armand’s cannot use time series data to develop the forecast. 
However, as we will now illustrate, regression analysis can still be used to forecast quarterly 
sales for this new location.
To develop the equation relating quarterly sales to the size of the student popula-
tion, Armand’s collected data from a sample of ten of its restaurants located near college 
campuses. These data are summarized in Table 5.13. For example, restaurant 1, with 
y 5 58 and x 5 2, had $58,000 in quarterly sales and is located near a campus with 2000 
students. Figure 5.16 shows a scatter chart of the data presented in Table 5.13, with the 
size of the student population is shown on the horizontal axis and quarterly sales shown 
on the vertical axis.
What preliminary conclusions can we draw from Figure 5.16? Sales appear to be higher 
at locations near campuses with larger student populations. Also, it appears that the relation-
ship between the two variables can be approximated by a straight line. In Figure 5.17 we 
can draw a straight line through the data that appears to provide a good linear approxima-
tion of the relationship between the variables. Observe that the relationship is not perfect. 
Indeed, few, if any, of the data fall exactly on the line. However, if we can develop the 
mathematical expression for this line, we may be able to use it to forecast the value of y 
corresponding to each possible value of x. The resulting equation of the line is called the 
estimated regression equation.

 
5.4 Using Regression Analysis for Forecasting 
233
Using the least-squares method of estimation, the estimated regression equation is
 
y^i 5 b0 1 b1xi 
(5.14)
where
 y^i 5 estimated value of the dependent variable (quarterly sales) for the ith observation
b0 5 intercept of the estimated regression equation
b1 5 slope of the estimated regression equation
 xi 5 value of the independent variable (student population) for the ith observation
Restaurant
Student Population (1000s)
Quarterly Sales ($1000s)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
 2
 6
 8
 8
12
16
20
20
22
26
 58
105
 88
118
117
137
157
169
149
202
TABLE 5.13   STUDENT POPULATION AND QUARTERLY SALES DATA FOR TEN 
ARMAND’S PIZZA PARLORS
file
WEB
Armand’s
FIGURE 5.16   SCATTER CHART OF STUDENT POPULATION AND QUARTERLY 
SALES FOR ARMAND’S PIZZA PARLORS
20
40
60
80
100
120
140
160
180
200
220
Quarterly Sales ($1000s) - y
6
0
2
4
14
8
10
12
22
16
18
20
24
26
Student Population (1000s) - x

234 
Chapter 5 Time Series Analysis and Forecasting
The Excel output for a simple linear regression analysis of the Armand’s Pizza data is 
provided in Figure 5.18.
We see in this output that the estimated intercept b0 is 60 and the estimated slope b1 
is 5. Thus, the estimated regression equation is
 
y^i 5 60 1 5xi
The slope of the estimated regression equation (b1 5 5) is positive, implying that, as student 
population increases, quarterly sales increase. In fact, we can conclude (because sales are 
measured in thousands of dollars and student population in thousands) that an increase in 
the student population of 1000 is associated with an increase of $5000 in expected quar-
terly sales; that is, quarterly sales are expected to increase by $5 per student. The estimated 
y-intercept b0 tells us that if the student population for the location of an Armand’s pizza 
parlor was 0 students, we would expect sales of $60,000.
If we believe that the least squares estimated regression equation adequately describes the 
relationship between x and y, using the estimated regression equation to forecast the value of y 
for a given value of x seems reasonable. For example, if we wanted to forecast quarterly sales 
for a new restaurant to be located near a campus with 16,000 students, we would compute
 
y^ 5 60 1 5(16)
 
 5 140
Hence, we would forecast quarterly sales of $140,000.
The sales forecasting problem facing Armand’s Pizza Parlors illustrates how simple lin-
ear regression analysis can be used to develop forecasts when a causal variable is available.
Note that the values of the 
independent variable range 
from 2000 to 26,000; thus, 
as discussed in Chapter 4,  
the y-intercept in such 
cases is an extrapolation 
of the regression line and 
must be interpreted with 
caution.
FIGURE 5.17   GRAPH OF THE ESTIMATED REGRESSION EQUATION FOR AR-
MAND’S PIZZA PARLORS: y 5 60 1 5x
20
40
60
80
100
120
140
160
180
200
220
Quarterly Sales ($1000s) - y
6
0
2
4
14
8
10
12
22
16
18
20
24
26
Student Population (1000s) - x
y = 60 + 5x
 y intercept
 b0 = 60
Slope b1 = 5
^

 
5.4 Using Regression Analysis for Forecasting 
235
Combining Causal Variables with Trend  
and Seasonality Effects
Regression models are very flexible and can incorporate both causal variables and time 
series effects. Suppose we had a time series of several years of quarterly sales data and 
advertising expenditures for a single Armand’s restaurant. If we suspected that sales 
were related to the causal variable advertising expenditures and that sales showed trend 
and seasonal effects, we could incorporate each into a single model by combining the 
approaches we have outlined. If we believe the effect of advertising is not immediate, 
we might also try to find a relationship between sales in period t and advertising in the 
previous period, t 2 1.
Multiple regression analysis also can be applied in these situations if additional data for 
other independent variables are available. For example, suppose that the management of 
Armand’s Pizza Parlors also believes that the number of competitors near the college cam-
pus is related to quarterly sales. Intuitively, management believes that restaurants located 
near campuses with fewer competitors generate more sales revenue than those located near 
campuses with more competitors. With additional data, multiple regression analysis could 
be used to develop an equation relating quarterly sales to the size of the student population 
and the number of competitors.
Considerations in Using Regression in Forecasting
Although regression analysis allows for the estimation of complex forecasting mod-
els, we must be cautious about using such models and guard against the potential for 
overfitting our model to the sample data. Spyros Makridakis, a noted forecasting ex-
pert, conducted research showing that simple techniques usually outperform more com-
plex procedures for short-term forecasting. Using a more sophisticated and expensive 
Refer to Section 4 of 
Chapter 4 for  instructions 
on how to use Excel’s 
 Regression or to the 
Chapter 4 appendix for 
instructions on how to 
use XLMiner to estimate 
 regression models.
The value of an indepen-
dent variable from the prior 
period is referred to as a 
lagged variable.
FIGURE 5.18   EXCEL SIMPLE LINEAR REGRESSION OUTPUT FOR ARMAND’S PIZZA PARLORS
Regression Statistics
Multiple R
R Square
Adjusted R Square
Standard Error
Observations
ANOVA
df
Coefﬁcients Standard Error
t Stat
P-value
Lower 95%
Upper 95%
Lower 99.0% Upper 99.0%
1
14200
14200
191.25
74.24836601
2.54887E-05
1530
15730
0.950122955
0.90273363
0.890575334
13.82931669
10
8
9
60
9.22603481 6.503335532 0.000187444
38.72472558 81.27527442 29.04307968
3.052985371
90.95692032
6.947014629
6.338094038
3.661905962
2.54887E-05
8.616749156
0.580265238
5
SS
MS
F
Signiﬁcance F
Regression
Residual
Total
Intercept
Student Population (1,000s)
A
B
C
D
E
F
G
H
I
SUMMARY OUTPUT
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

236 
Chapter 5 Time Series Analysis and Forecasting
 procedure will not guarantee better forecasts. However, many research studies, including 
those done by Makridakis, have also shown that quantitative forecasting models such 
as those presented in this chapter commonly outperform qualitative forecasts made by 
“experts.” Thus, there is good reason to use quantitative forecasting methods whenever 
data are available.
Whether a regression approach provides a good forecast depends largely on how well 
we are able to identify and obtain data for independent variables that are closely related to 
the time series. Generally, during the development of an estimated regression equation, we 
will want to consider many possible sets of independent variables. Thus, part of the regres-
sion analysis procedure should focus on the selection of the set of independent variables 
that provides the best forecasting model.
NOTES AND COMMENTS
Several other methods for estimating trend and 
seasonal effects exist. These include but are not 
limited to Holt-Winters seasonal smoothing and 
Holt-Winters multiplicative method. Although 
each of the methods we have considered treats 
the trend and seasonal effects (and the cyclical 
effect, if included in the model) as additive, some 
methods treat the effects as  multiplicative (that 
is, seasonal effects are more pronounced when 
the trend effect is large). We refer students who 
are interested in these and other more advanced 
methods for time series analysis to the references 
listed at the end of this textbook.
Determining the Best Forecasting  
Model to Use
Given the variety of forecasting models and approaches, the obvious question is, “For 
a given forecasting study, how does one choose an appropriate model?” As discussed 
throughout this text, it is always a good idea to get descriptive statistics on the data and 
graph the data so that it can be visually inspected. In the case of times series data, a visual 
inspection can indicate whether seasonality appears to be a factor and whether a linear or 
nonlinear trend seems to exist. For causal modeling, scatter charts can indicate whether 
strong linear or nonlinear relationships exist between the independent and dependent vari-
ables. If certain relationships appear totally random, this may lead you to exclude these 
variables from the model.
As in regression analysis, you may be working with large data sets when generating 
a forecasting model. In such cases, it is recommended to divide your data into training 
and validation sets. For example, you might have five years of monthly data available 
to produce a time series forecast. You could use the first three years of data as a training 
set to estimate a model or a collection of models that appear to provide good forecasts. 
You might develop exponential smoothing models and regression models for the train-
ing set. You could then use the last two years as a validation set to assess and compare 
the models’ performances. Based on the errors produced by the different models for 
the validation set, you could ultimately pick the model that minimizes some forecast 
error measure, such as MAE, MSE or MAPE. However, you must exercise caution in 
using the older portion of a time series for the training set and the more recent portion 
of the time series as the validation set; if the behavior of the time series has changed 
recently, the older portion of the time series may no longer show patterns similar to the 
5.5

 
Glossary 
237
more recent values of the time series, and a forecasting model based on such data will 
not perform well.
Some software packages try many different forecasting models on time series data 
(those included in this chapter and more) and report back optimal model parameters and 
error measures for each model tested. Although some of these software packages will even 
automatically select the best model to use, ultimately the user should decide which model 
to use going forward based on a combination of the software output and the user’s manage-
rial knowledge.
Summary
This chapter provided an introduction to the basic methods of time series analysis and fore-
casting. First, we showed that to explain the behavior of a time series, it is often helpful to 
graph the time series and identify whether trend, seasonal, and/or cyclical components are 
present in the time series. The methods we have discussed are based on assumptions about 
which of these components are present in the time series.
We discussed how smoothing methods can be used to forecast a time series that exhibits 
no significant trend, seasonal, or cyclical effect. The moving averages approach consists of 
computing an average of past data values and then using that average as the forecast for the 
next period. In the exponential smoothing method, a weighted average of past time series 
values is used to compute a forecast.
For time series that have only a long-term trend, we showed how regression analysis 
could be used to make trend projections. For time series with seasonal influences, we showed 
how to incorporate the seasonality for more accurate forecasts. We described how regression 
analysis can be used to develop causal forecasting models that relate values of the variable 
to be forecast (the dependent variable) to other independent variables that are believed to 
explain (cause) the behavior of the dependent variable. Finally, we have  provided guidance 
on how to select an appropriate model from the models discussed in this chapter.
Glossary
Forecast A prediction of future values of a time series.
Time series A set of observations on a variable measured at successive points in time or 
over successive periods of time.
Stationary time series A time series whose statistical properties are independent of time.
Trend The long-run shift or movement in the time series observable over several periods 
of time.
Seasonal pattern The component of the time series that shows a periodic pattern over one 
year or less.
Cyclical pattern The component of the time series that results in periodic above-trend and 
below-trend behavior of the time series lasting more than one year.
Forecast error The amount by which the forecasted value y^t differs from the observed 
value yt and y^t, denoted et 5 yt 2 y^t.
Mean absolute error (MAE) A measure of forecasting accuracy; the average of the values 
of the forecast errors.
Mean squared error (MSE) A measure of the accuracy of a forecasting method; this 
measure is the average of the sum of the squared differences between the forecast values 
and the actual time series values.

238 
Chapter 5 Time Series Analysis and Forecasting
Mean absolute percentage error (MAPE) A measure of the accuracy of a forecasting 
method; the average of the absolute values of the errors as a percentage of the correspond-
ing forecast values.
Moving averages method A method of forecasting or smoothing a time series that uses the 
average of the most recent n data values in the time series as the forecast for the next period.
Exponential smoothing A forecasting technique that uses a weighted average of past time 
series values as the forecast.
Smoothing constant A parameter of the exponential smoothing model that provides 
the weight given to the most recent time series value in the calculation of the forecast 
value.
Autoregressive model A regression model in which a regression relationship based on past 
time series values is used to predict the future time series values.
Causal models Forecasting methods that relate a time series to other variables that are 
believed to explain or cause its behavior.
Problems
 1. Consider the following time series data:
Week
1
2
3
4
5
6
Value
18
13
16
11
17
14
 
 Using the naïve method (most recent value) as the forecast for the next week, compute the 
following measures of forecast accuracy:
a. Mean absolute error
b. Mean squared error
c. Mean absolute percentage error
d. What is the forecast for week 7?
 2.  Refer to the time series data in Problem 1. Using the average of all the historical data as a 
forecast for the next period, compute the following measures of forecast accuracy:
a. Mean absolute error
b. Mean squared error
c. Mean absolute percentage error
d. What is the forecast for week 7?
 3.  Problems 1 and 2 used different forecasting methods. Which method appears to provide 
the more accurate forecasts for the historical data? Explain.
 4. Consider the following time series data:
Month
1
2
3
4
5
6
7
Value
24
13
20
12
19
23
15
a.  Compute MSE using the most recent value as the forecast for the next period. What 
is the forecast for month 8?
b.  Compute MSE using the average of all the data available as the forecast for the next 
period. What is the forecast for month 8?
c.  Which method appears to provide the better forecast?
 5. Consider the following time series data:
Week
1
2
3
4
5
6
Value
18
13
16
11
17
14

 
Problems 
239
a. Construct a time series plot. What type of pattern exists in the data?
b.  Develop a three-week moving average for this time series. Compute MSE and a fore-
cast for week 7.
c.  Use a 5 0.2 to compute the exponential smoothing values for the time series. Com-
pute MSE and a forecast for week 7.
d.  Compare the three-week moving average forecast with the exponential smoothing 
forecast using a 5 0.2. Which appears to provide the better forecast based on MSE? 
Explain.
e.  Use trial and error to find a value of the exponential smoothing coefficient a that 
results in a smaller MSE than what you calculated for a 5 0.2.
 6. Consider the following time series data:
Month
1
2
3
4
5
6
7
Value
24
13
20
12
19
23
15
a. Construct a time series plot. What type of pattern exists in the data?
b.  Develop a three-week moving average for this time series. Compute MSE and a fore-
cast for week 8.
c.  Use a 5 0.2 to compute the exponential smoothing values for the time series. Com-
pute MSE and a forecast for week 8.
d.  Compare the three-week moving average forecast with the exponential smooth-
ing forecast using a 5 0.2. Which appears to provide the better forecast based on 
MSE?
e.  Use trial and error to find a value of the exponential smoothing coefficient a that 
results in a smaller MSE than what you calculated for a 5 0.2.
 7. Refer to the gasoline sales time series data in Table 5.1.
a. Compute four-week and five-week moving averages for the time series.
b.  Compute the MSE for the four-week and five-week moving average forecasts.
c.  What appears to be the best number of weeks of past data (three, four, or five) to use in 
the moving average computation? Recall that MSE for the three-week moving average 
is 10.22.
 8.  With the gasoline time series data from Table 5.1, show the exponential smoothing fore-
casts using a 5 0.1.
a.  Applying the MSE measure of forecast accuracy, would you prefer a smoothing con-
stant of a 5 0.1 or a 5 0.2 for the gasoline sales time series?
b. Are the results the same if you apply MAE as the measure of accuracy?
c. What are the results if MAPE is used?
 9.  With a smoothing constant of a 5 0.2, equation (5.7) shows that the forecast for week 13 
of the gasoline sales data from Table 5.1 is given by y^13 5 0.2y12 1 0.8y^12. However, the 
forecast for week 12 is given by y^12 5 0.2y11 1 0.8y^11. Thus, we could combine these two 
results to show that the forecast for week 13 can be written
y^13 5 0.2y12 1 0.8(0.2y11 1 0.8y^11) 5 0.2y12 1 0.16y11 1 0.64y^11
a.  Making use of the fact that y^11 5 0.2y10 1 0.8y^10 (and similarly for y^10 and y^9), con-
tinue to expand the expression for y^13 until it is written in terms of the past data values 
y12, y11, y10, y9, y8, and the forecast for period 8, y^8.
b.  Refer to the coefficients or weights for the past values y12, y11, y10, y9, y8. What obser-
vation can you make about how exponential smoothing weights past data values in 
arriving at new forecasts? Compare this weighting pattern with the weighting pattern 
of the moving averages method.
10.  United Dairies, Inc., supplies milk to several independent grocers throughout Dade 
C ounty, Florida. Managers at United Dairies want to develop a forecast of the number of 
half gallons of milk sold per week. Sales data for the past 12 weeks are:
file
WEB
Gasoline
file
WEB
Gasoline

240 
Chapter 5 Time Series Analysis and Forecasting
Week
Sales
Week
Sales
1
2
3
4
5
6
2750
3100
3250
2800
2900
3050
 7
 8
 9
10
11
12
3300
3100
2950
3000
3200
3150
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use exponential smoothing with a 5 0.4 to develop a forecast of demand for week 13. 
What is the resulting MSE?
11.  For the Hawkins Company, the monthly percentages of all shipments received on time 
over the past 12 months are 80, 82, 84, 83, 83, 84, 85, 84, 82, 83, 84, and 83.
a. Construct a time series plot. What type of pattern exists in the data?
b.  Compare a three-month moving average forecast with an exponential smoothing fore-
cast for a 5 0.2. Which provides the better forecasts using MSE as the measure of 
model accuracy?
c. What is the forecast for next month?
12. Corporate triple A bond interest rates for 12 consecutive months follow.
9.5 9.3 9.4 9.6 9.8 9.7 9.8 10.5 9.9 9.7 9.6 9.6
a. Construct a time series plot. What type of pattern exists in the data?
b.  Develop three-month and four-month moving averages for this time series. Does the 
three-month or the four-month moving average provide the better forecasts based on 
MSE? Explain.
c. What is the moving average forecast for the next month?
13.  The values of Alabama building contracts (in millions of dollars) for a 12-month period 
follow:
240 350 230 260 280 320 220 310 240 310 240 230
a. Construct a time series plot. What type of pattern exists in the data?
b.  Compare a three-month moving average forecast with an exponential smoothing fore-
cast. Use a 5 0.2. Which provides the better forecasts based on MSE?
c.  What is the forecast for the next month using exponential smoothing with a 5 0.2?
14. The following time series shows the sales of a particular product over the past 12 months.
Month
Sales
Month
Sales
1
2
3
4
5
6
105
135
120
105
 90
120
 7
 8
 9
10
11
12
145
140
100
 80
100
110
a. Construct a time series plot. What type of pattern exists in the data?
b. Use a 5 0.3 to compute the exponential smoothing values for the time series.
c.  Use trial and error to find a value of the exponential smoothing coefficient a that 
results in a relatively small MSE.
15. Ten weeks of data on the Commodity Futures Index are:
7.35 7.40 7.55 7.56 7.60 7.52 7.52 7.70 7.62 7.55
file
WEB
UnitedDairies
file
WEB
Hawkins
file
WEB
TripleABond
file
WEB
Alabama
file
WEB
MonthlySales
file
WEB
Commodity Futures

 
Problems 
241
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use trial and error to find a value of the exponential smoothing coefficient a that 
results in a relatively small MSE.
16. The following table reports the percentage of stocks in a portfolio for nine quarters:
Quarter
Stock (%)
Year 1, qtr 1
Year 1, qtr 2
Year 1, qtr 3
Year 1, qtr 4
Year 2, qtr 1
Year 2, qtr 2
Year 2, qtr 3
Year 2, qtr 4
Year 3, qtr 1
29.8
31.0
29.9
30.1
32.2
31.5
32.0
31.9
30.0
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use trial and error to find a value of the exponential smoothing coefficient a that 
results in a relatively small MSE.
c.  Using the exponential smoothing model you developed in part b, what is the forecast 
of the percentage of stocks in a typical portfolio for the second quarter of year 3?
17. Consider the following time series:
t
1
2
3
4
5
yt
6
11
9
14
15
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use simple linear regression analysis to find the parameters for the line that minimizes 
MSE for this time series.
c. What is the forecast for t 5 6?
18. Consider the following time series.
t
1
2
3
4
5
6
7
yt
120
110
100
96
94
92
88
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use simple linear regression analysis to find the parameters for the line that minimizes 
MSE for this time series.
c. What is the forecast for t 5 8?
19.  Because of high tuition costs at state and private universities, enrollments at community 
colleges have increased dramatically in recent years. The following data show the enroll-
ment for Jefferson Community College for the nine most recent years:
Year
Period (t)
Enrollment (1000s)
1
2
3
4
5
6
7
8
9
1
2
3
4
5
6
7
8
9
 6.5
 8.1
 8.4
10.2
12.5
13.3
13.7
17.2
18.1
file
WEB
Portfolio
file
WEB
Jefferson

242 
Chapter 5 Time Series Analysis and Forecasting
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use simple linear regression analysis to find the parameters for the line that minimizes 
MSE for this time series.
c.  What is the forecast for year 10?
20.  The Seneca Children’s Fund (SCC) is a local charity that runs a summer camp for disad-
vantaged children. The fund’s board of directors has been working very hard over recent 
years to decrease the amount of overhead expenses, a major factor in how charities are 
rated by independent agencies. The following data show the percentage of the money SCC 
has raised that was spent on administrative and fund-raising expenses over the last seven 
years.
Period (t)
Expense (%)
1
2
3
4
5
6
7
13.9
12.2
10.5
10.4
11.5
10.0
 8.5
a.  Construct a time series plot. What type of pattern exists in the data?
b.  Use simple linear regression analysis to find the parameters for the line that minimizes 
MSE for this time series.
c.  Forecast the percentage of administrative expenses for year 8.
d.  If SCC can maintain its current trend in reducing administrative expenses, how long 
will it take SCC to achieve a level of 5 percent or less?
21.  The president of a small manufacturing firm is concerned about the continual increase 
in manufacturing costs over the past several years. The following figures provide a time 
series of the cost per unit for the firm’s leading product over the past eight years:
Year
Cost/Unit ($)
Year
Cost/Unit ($)
1
2
3
4
20.00
24.50
28.20
27.50
5
6
7
8
26.60
30.00
31.00
36.00
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use simple linear regression analysis to find the parameters for the line that minimizes 
MSE for this time series.
c. What is the average cost increase that the firm has been realizing per year?
d. Compute an estimate of the cost/unit for next year.
22. Consider the following time series:
Quarter
Year 1
Year 2
Year 3
1
2
3
4
71
49
58
78
68
41
60
81
62
51
53
72
a.  Construct a time series plot. What type of pattern exists in the data? Is there an indica-
tion of a seasonal pattern?
file
WEB
Seneca
file
WEB
ManufacturingCosts

 
Problems 
243
b.  Use a multiple linear regression model with dummy variables as follows to develop an 
equation to account for seasonal effects in the data. Qtr1 5 1 if quarter 1, 0 otherwise; 
Qtr2 5 1 if quarter 2, 0 otherwise; Qtr3 5 1 if quarter 3, 0 otherwise.
c. Compute the quarterly forecasts for next year.
23. Consider the following time series data:
Quarter
Year 1
Year 2
Year 3
1
2
3
4
4
2
3
5
6
3
5
7
7
6
6
8
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use a multiple regression model with dummy variables as follows to develop an equa-
tion to account for seasonal effects in the data. Qtr1 5 1 if quarter 1, 0 otherwise; 
Qtr2 5 1 if quarter 2, 0 otherwise; Qtr3 5 1 if quarter 3, 0 otherwise.
c.  Compute the quarterly forecasts for next year based on the model you developed in 
part b.
d.  Use a multiple regression model to develop an equation to account for trend and sea-
sonal effects in the data. Use the dummy variables you developed in part b to capture 
seasonal effects and create a variable t such that t 5 1 for quarter 1 in year 1, t 5 2 for 
quarter 2 in year 1, . . . t 5 12 for quarter 4 in year 3.
e.  Compute the quarterly forecasts for next year based on the model you developed in 
part d.
f. 
 Is the model you developed in part b or the model you developed in part d more effec-
tive? Justify your answer.
24.  The quarterly sales data (number of copies sold) for a college textbook over the past three 
years follow:
Quarter
Year 1
Year 2
Year 3
1
2
3
4
1690
 940
2625
2500
1800
 900
2900
2360
1850
1100
2930
2615
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use a regression model with dummy variables as follows to develop an equation to 
account for seasonal effects in the data. Qtr1 5 1 if quarter 1, 0 otherwise; Qtr2 5 1 
if quarter 2, 0 otherwise; Qtr3 5 1 if quarter 3, 0 otherwise.
c.  Based on the model you developed in part b, compute the quarterly forecasts for next 
year.
d.  Let t 5 1 to refer to the observation in quarter 1 of year 1; t 5 2 to refer to the obser-
vation in quarter 2 of year 1; . . . ; and t 5 12 to refer to the observation in quarter 4 
of year 3. Using the dummy variables defined in part b and t, develop an equation to 
account for seasonal effects and any linear trend in the time series.
e.  Based upon the seasonal effects in the data and linear trend, compute the quarterly 
forecasts for next year.
f. 
 Is the model you developed in part b or the model you developed in part d more effec-
tive? Justify your answer.
25.  Air pollution control specialists in southern California monitor the amount of ozone, car-
bon dioxide, and nitrogen dioxide in the air on an hourly basis. The hourly time series data 
exhibit seasonality, with the levels of pollutants showing patterns that vary over the hours 
file
WEB
TextbookSales

244 
Chapter 5 Time Series Analysis and Forecasting
in the day. On July 15, 16, and 17, the following levels of nitrogen dioxide were observed 
for the 12 hours from 6:00 a.m. to 6:00 p.m.:
July 15
July 16
July 17
25
28
35
28
30
42
35
35
45
50
48
70
60
60
72
60
65
75
40
50
60
35
40
45
30
35
40
25
25
25
25
20
25
20
20
25
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use a multiple linear regression model with dummy variables as follows to develop 
an equation to account for seasonal effects in the data:
 
 Hour1 5 1 if the reading was made between 6:00 a.m. and 7:00a.m.; 0 otherwise;
 
 Hour2 5 1 if the reading was made between 7:00 a.m. and 8:00 a.m.; 0 otherwise;
 
 .
 
 .
 
 .
 
 Hour11 5 1 if the reading was made between 4:00 p.m. and 5:00 p.m., 0 otherwise.
 
 Note that when the values of the 11 dummy variables are equal to 0, the observation 
corresponds to the 5:00 p.m. to 6:00 p.m. hour.
c.  Using the equation developed in part b, compute estimates of the levels of nitrogen 
dioxide for July 18.
d.  Let t 5 1 to refer to the observation in hour 1 on July 15; t 5 2 to refer to the observation 
in hour 2 of July 15; . . . ; and t 5 36 to refer to the observation in hour 12 of July 17.  
Using the dummy variables defined in part b and ts, develop an equation to account 
for seasonal effects and any linear trend in the time series.
e.  Based upon the seasonal effects in the data and linear trend estimated in part d, com-
pute estimates of the levels of nitrogen dioxide for July 18.
f. 
 Is the model you developed in part b or the model you developed in part d more effec-
tive? Justify your answer.
26.  South Shore Construction builds permanent docks and seawalls along the southern shore 
of Long Island, New York. Although the firm has been in business only five years, rev-
enue has increased from $308,000 in the first year of operation to $1,084,000 in the most 
recent year. The following data show the quarterly sales revenue in thousands of dollars:
Quarter
Year 1
Year 2
Year 3
Year 4
Year 5
1
2
3
4
 20
100
175
 13
 37
136
245
 26
 75
155
326
 48
 92
202
384
 82
176
282
445
181
a. Construct a time series plot. What type of pattern exists in the data?
b.  Use a multiple regression model with dummy variables as follows to develop an equa-
tion to account for seasonal effects in the data. Qtr1 5 1 if quarter 1, 0 otherwise; 
Qtr2 5 1 if quarter 2, 0 otherwise; Qtr3 5 1 if quarter 3, 0 otherwise.
c.  Based on the model you developed in part b, compute estimates of quarterly sales for 
year 6.
d.  Let Period 5 1 refer to the observation in quarter 1 of year 1; Period 5 2 refer to the ob-
servation in quarter 2 of year 1; . . . and Period 5 20 refer to the observation in quarter 4 
of year 5. Using the dummy variables defined in part b and the variable Period, develop 
an equation to account for seasonal effects and any linear trend in the time series.
e.  Based upon the seasonal effects in the data and linear trend estimated in part c, com-
pute estimates of quarterly sales for year 6.
f. 
 Is the model you developed in part b or the model you developed in part d more effec-
tive? Justify your answer.
file
WEB
Pollution
file
WEB
SouthShore

 
Problems 
245
27.  Hogs & Dawgs is an ice cream parlor on the border of north-central Louisiana and south-
ern Arkansas that serves 43 flavors of ice creams, sherbets, frozen yogurts, and sorbets. 
During the summer Hogs & Dawgs is open from 1:00 p.m. to 10:00 p.m. on Monday 
through Saturday, and the owner believes that sales change systematically from hour to 
hour throughout the day. She also believes her sales increase as the outdoor temperature 
increases. Hourly sales and the outside temperature at the start of each hour for the last 
week are provided in the WEBfile IceCreamSales.
a.  Construct a time series plot of hourly sales and a scatter plot of outdoor temperature 
and hourly sales. What types of relationships exist in the data?
b.  Use a simple regression model with outside temperature as the causal variable to 
develop an equation to account for the relationship between outside temperature and 
hourly sales in the data. Based on this model, compute an estimate of hourly sales for 
today from 2:00 p.m. to 3:00 p.m. if the temperature at 2:00 p.m. is 93°.
c.  Use a multiple linear regression model with the causal variable outside temperature 
and dummy variables as follows to develop an equation to account for both seasonal 
effects and the relationship between outside temperature and hourly sales in the data 
in the data:
 
 Hour1 5 1 if the sales were recorded between 1:00 p.m. and 2:00 p.m., 0 otherwise;
 
 Hour2 5 1 if the sales were recorded between 2:00 p.m. and 3:00 p.m., 0 otherwise;
 
 .
 
 .
 
 .
 
 Hour8 5 1 if the sales were recorded between 8:00 p.m. and 9:00 p.m., 0 otherwise.
 
 Note that when the values of the 8 dummy variables are equal to 0, the observation 
corresponds to the 9:00-to-10:00-p.m. hour.
 
 
Based on this model, compute an estimate of hourly sales for today from 2:00 p.m. 
to 3:00 p.m. if the temperature at 2:00 p.m. is 93°.
d.  Is the model you developed in part b or the model you developed in part c more effec-
tive? Justify your answer.
28.  Donna Nickles manages a gasoline station on the corner of Bristol Avenue and Harpst Street 
in Arcata, California. Her station is a franchise, and the parent company calls her station 
every day at midnight to give her the prices for various grades of gasoline for the upcoming 
day. Over the past eight weeks Donna has recorded the price and gallon sales of regular grade 
gasoline at her station as well as the price of regular grade gasoline charged by her competi-
tor across the street. She is curious about the sensitivity of her sales to the price of regular 
grade gasoline she charges and the price of regular gasoline charged by her competitor across 
the street. She also wonders whether her sales differ systematically by day of the week and 
whether her station has experienced a trend in sales over the past eight weeks. The data col-
lected by Donna for each day of the past eight weeks are provided in the WEBfile GasStation.
a.  Construct a time series plot of daily sales, a scatter plot of the price Donna charges 
for a gallon of regular grade gasoline and daily sales at Donna’s station, and a scatter 
plot of the price Donna’s competitor charges for a gallon of regular grade gasoline and 
daily sales at Donna’s station. What types of relationships exist in the data?
b.  Use a multiple regression model with the price Donna charges for a gallon of regular 
grade gasoline and the price Donna’s competitor charges for a gallon of regular grade 
gasoline as causal variables to develop an equation to account for the relationships be-
tween these prices and Donna’s daily sales in the data. Based on this model, compute an 
estimate of sales for a day on which Donna is charging $3.50 for a gallon for regular grade 
gasoline and her competitor is charging $3.45 for a gallon of regular grade gasoline.
c.  Use a multiple linear regression model with the trend and dummy variables as follows 
to develop an equation to account for both trend and seasonal effects in the data:
 
 Monday 5 1 if the sales were recorded on a Monday, 0 otherwise;
 
 Tuesday 5 1 if the sales were recorded on a Tuesday, 0 otherwise;
 
 .
 
 .
 
 .
Saturday 5 1 if the sales were recorded on a Saturday, 0 otherwise;
file
WEB
IceCreamSales
file
WEB
GasStation

246 
Chapter 5 Time Series Analysis and Forecasting
 
 Note that when the values of the six dummy variables are equal to 0, the observation 
corresponds to Sunday.
 
 
Based on this model, compute an estimate of sales for Tuesday of the first week 
after Donna collected her data.
d.  Use a multiple regression model with the price Donna charges for a gallon of regular 
grade gasoline and the price Donna’s competitor charges for a gallon of regular grade 
gasoline as causal variables and the trend and dummy variables from part c to create 
an equation to account for the relationships between these prices and daily sales as 
well as the trend and seasonal effects in the data. Based on this model, compute an 
estimate of sales for Tuesday of the first week after Donna collected her data a day if 
Donna is charging $3.50 for a gallon for regular grade gasoline and her competitor is 
charging $3.45 for a gallon of regular grade gasoline.
e.  Which of the three models you developed in parts b, c, and d is most effective? Justify 
your answer.
Forecasting Food and Beverage Sales
The Vintage Restaurant, on Captiva Island near Fort Myers, Florida, is owned and oper-
ated by Karen Payne. The restaurant just completed its third year of operation. During 
that time, Karen sought to establish a reputation for the restaurant as a high-quality dining 
establishment that specializes in fresh seafood. Through the efforts of Karen and her staff, 
her restaurant has become one of the best and fastest-growing restaurants on the island.
To better plan for future growth of the restaurant, Karen needs to develop a system that 
will enable her to forecast food and beverage sales by month for up to one year in advance. 
The following table shows the value of food and beverage sales ($1,000s) for the first three 
years of operation:
Month
First Year
Second Year
Third Year
January
February
March
April
May
June
July
August
September
October
November
December
242
235
232
178
184
140
145
152
110
130
152
206
263
238
247
193
193
149
157
161
122
130
167
230
282
255
265
205
210
160
166
174
126
148
173
235
Managerial Report
Perform an analysis of the sales data for the Vintage Restaurant. Prepare a report for Karen 
that summarizes your findings, forecasts, and recommendations. Include the following:
 1. A time series plot. Comment on the underlying pattern in the time series.
 2. Using the dummy variable approach, forecast sales for January through December of 
the fourth year.
How would you explain this model to Karen?
Assume that January sales for the fourth year turn out to be $295,000. What was your 
forecast error? If this error is large, Karen may be puzzled about the difference between 
your forecast and the actual sales value. What can you do to resolve her uncertainty about 
the forecasting procedure?
Case Problem
file
WEB
Vintage

 
Appendix Using XLMiner for Forecasting 
247
Using XLMiner for Forecasting
To show how XLMiner can be used for forecasting, we again develop a three-week moving 
average for the original 12 weeks of gasoline sales data in Table 5.1 and Figure 5.1. Values 
for Time are in cells A2:A13 and Sales are in cells B2:B13.
Step 1. Select any cell in the range of data (and cell in A1:B13)
Step 2. Click the XLMINER tab in the Ribbon (Figure 5.19)
Step 3. In the Time Series group, click Smoothing and select Moving Average
Step 4. When the Moving Average Smoothing dialog box appears (Figure 5.20):
Verify that the checkbox for First Row contains headers is selected
Select Week in the Variables in input data box and click the $ button 
to move this variable into the Time Variable box (or verify that Week 
is in the Time Variable box)
Appendix
file
WEB
Gasoline
FIGURE 5.19  XLMINER TAB IN THE RIBBON IN EXCEL
FIGURE 5.20  XLMINER MOVING AVERAGE SMOOTHING DIALOG BOX

248 
Chapter 5 Time Series Analysis and Forecasting
Select Sales (1000s of gallons) in the Variables in input data box, and 
click the $ button to move this variable into the Selected variable 
box (or verify that Sales (1000s of gallons) is in the Selected vari-
able box)
Select the checkbox for Give Forecast in the Output Options area
Enter 3 in the Interval box
In the Forecast Options area, enter 1 in the #forecasts box to generate a 
forecast one period into the future
Click OK
The results are shown in Figure 5.21; they are found in the MASmoothingOutput1 
 worksheet. For this model the MAPE 514.36 percent, the MAD (or MAE) 5 2.67, and 
the MSE 5 10.22. The forecast for the next week is 19.
XLMINER denotes the 
mean absolute deviation as 
the MAD.
FIGURE 5.21   XLMINER OUTPUT FOR THE GASOLINE SALES DATA THREE PERIOD MOVING 
AVERAGE FORECAST
6
7
8
9
10
11
12
13
14
Actual
Forecast
A
XLMiner : Time Series - Moving Average Smoothing
Inputs
Fitted Model
Error Measures (Training)
Forecast
Elapsed Time
B
C
D
E
F
G
H
I
J
K
L
M
Date: 23-Jan-2013 17:04:26
(Ver:
12.5.2P)
N
O
P
Q
1
2
3
4
5
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
Data
12
Data!$A$2:$B$13
Sales (1000s of gallons)
Parameters/Options
MAPE
Week
Forecast
LCI
UCI
Forecast1
19
13.5730057 24.4269943
14.3566096
2.66666667
10.2222222
MAD
MSE
Number of seasons
Week
1
2
3
•
•
•
4
5
6
7
8
9
10
11
12
Actual
17
21
19
23
18
16
20
18
22
20
15
22
Forecast
19
21
20
19
18
18
20
20
19
•
•
•
4
–3
–4
1
0
4
0
–5
3
Residual
3
N.A.
N.A
Yes
1
Overall (secs)
Week
Sales (1,000s of gallons)
Time Plot of Actual Vs Forecast
(Training Data)
1
0
5
10
15
20
25
2
3
4
5
6
7
8
9
Output Navigator
Inputs
Elapsed Time
ErrorMeasures
(Training)
Fitted
Model
Forecast
Error
Measures(Validation)
Data
MASmoothingOutput1
1.00
# Records in input data
Input data
Selected Variable
Interval
Season length
Forecast
#Forecasts

 
Appendix Using XLMiner for Forecasting 
249
To fit an exponential smoothing model with a 5 0.2 for the original 12 weeks of gaso-
line sales data in Table 5.1 and Figure 5.1, we follow the steps below. Recall again that 
values for Time are in cells A1:A13 and Sales are in cells B1:B13.
Step 1. Select any cell in the range of data (any cell in A1:B13)
Step 2. Click the XLMINER tab in the Ribbon
Step 3. In the Time Series group, select Smoothing and select Exponential
Step 4. When the Exponential Smoothing dialog box appears (Figure 5.22):
Verify that the check box for First Row contains headers is selected
Select Week in the Variables in input data box and click the $ button 
to move this variable into the Time Variable box
Enter 0.2 in the Level (Alpha) box
Select the check box for Give Forecast in the Output options area
Enter 1 in the #forecasts box
Click OK
The results are shown in Figure 5.23; they are found in the ExponentialOutput1 work-
sheet. For this model the MAPE 513.40 percent, the MAD (or MAE) 5 2.60, and the 
MSE 5 8.98.22. The forecast for the next week is 19.18.
XLMiner can also be used to compute any of the regression models discussed in this 
chapter. Refer to Chapter 4 for instructions on how to use these tools.
file
WEB
Gasoline
FIGURE 5.22  XLMINER EXPONENTIAL SMOOTHING DIALOG BOX

250 
Chapter 5 Time Series Analysis and Forecasting
FIGURE 5.23   XLMINER OUTPUT FOR THE GASOLINE SALES DATA EXPONENTIAL SMOOTHING 
FORECAST WITH a 5 0.2
A
XLMiner : Time Series - Exponential Smoothing
Inputs
Forecast
Elapsed Time
B
C
D
E
F
G
H
I
J
K
L
M
Date: 23-Dec-2012 17:23:30
(Ver:
4.0.2P)
N
O
P
Q
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
Data
# Records in input data
Input data
Selected Variable
12
Data!$A$2:$B$13
Sales (1000s of gallons)
Parameters/Options
Alpha (Level)
Beta (Trend)
Gamma (Seasonality)
Season length
Number of seasons
Forecast
#Forecasts
0.2
N.A.
N.A.
N.A.
N.A
Yes
1
Week
Forecast
LCI
UCI
Forecast1 19.184955 13.560844
24.809066
Overall (secs)
Fitted Model
Error Measures (Training)
MAPE
13.402425
2.5963391
8.9822307
MAD
MSE
Week
1
2
3
4
5
6
7
8
9
10
11
12
Actual
17
21
19
23
18
16
20
18
22
20
15
22
Forecast
•
17
17.8
18.04
19.032
18.8256
18.26048
18.608384
18.486707
19.189366
19.351493
18.481194
4
1.2
4.96
–1.032
–2.8256
1.73952
–0.608384
3.5132928
0.8106342
–4.3514926
3.5188059
•
Residual
Output Navigator
Inputs
Elapsed
Time
Error
Measures(Training)
Fitted
Model
Forecast
Error
Measures(Validation)
Data
1.00
ExponentialOutput1
Week
Sales (1,000s of gallons)
Time Plot of Actual Vs Forecast
(Training Data)
1
0
5
10
15
20
25
2
3
4
5
6
7
8
9
10 11
Actual
Forecast

Data Mining
CONTENTS
6.1 
DATA SAMPLING
6.2 
DATA PREPARATION
Treatment of Missing Data
Identification of Outliers and 
Erroneous Data
Variable Representation
6.3 
UNSUPERVISED LEARNING
Cluster Analysis
Association Rules
6.4 
SUPERVISED LEARNING
Partitioning Data
Classification Accuracy
Prediction Accuracy
k-Nearest Neighbors
Classification and Regression 
Trees
Logistic Regression
Chapter 6

252 
Chapter 6 Data Mining
Over the past few decades, technological advances have led to a dramatic increase in the 
amount of recorded data. The use of smartphones, radio-frequency identification (RFID) 
tags, electronic sensors, credit cards, and the Internet has facilitated the collection of data 
from phone conversations, e-mails, business transactions, product and customer tracking, 
business transactions, and Web page browsing. The increase in the use of data-mining tech-
niques in business has been caused largely by three events: the explosion in the amount of 
data being produced and electronically tracked, the ability to electronically warehouse these 
data, and the affordability of computer power to analyze the data. In this chapter, we discuss 
the analysis of large quantities of data in order to gain insight on customers, to uncover pat-
terns to improve business processes, and to establish new business rules to guide managers.
We define an observation as the set of recorded values of variables associated with a 
single entity. An observation is often displayed as a row of values in a spreadsheet or da-
tabase in which the columns correspond to the variables. For example, in direct marketing 
data, an observation may correspond to a customer and contain information regarding her 
response to an e-mail advertisement and demographic characteristics.
Data-mining approaches can be separated into two categories: supervised learning and 
unsupervised learning. In a supervised learning approach, the goal is to predict an outcome 
based on a set of variables (features). Linear regression is a well-known supervised learning 
approach from classical statistics, in which observations of a quantitative outcome (the de-
pendent y-variable) and one or more corresponding features (the independent x-variables) 
are used to create an equation for estimating y values. In other words, in supervised learn-
ing, the outcome variable “supervises” or guides the process of learning how to predict 
future outcomes. In this chapter, we focus on supervised learning methods for prediction 
and classification. A prediction task requires the estimation of the value for a continuous 
outcome (e.g., sales revenue). A classification task requires the identification of the value 
for a categorical outcome (e.g., loan default or no loan default).
In contrast, unsupervised learning methods do not attempt to predict an output value 
but are rather used to detect patterns and relationships in the data. In this chapter, we con-
sider the unsupervised learning tasks of clustering observations and developing association 
rules between items in an observation.
Whether we are using a supervised or unsupervised learning approach, the data-mining 
process comprises the following steps:
 1. Data Sampling: Extract a sample of data that is relevant to the business problem  under 
consideration
Although they might not see their customers face to face, 
online retailers are getting to know their patrons in order 
to tailor the offerings on their virtual shelves. By mining 
Web-browsing data collected in cookies—files that Web 
sites use to track people’s Web-browsing behavior— 
online retailers identify trends that can be used to im-
prove customer satisfaction and boost online sales.
For example, Orbitz, an online travel agency, books 
flights, hotels, car rentals, cruises, and other travel 
 activities for its customers. Tracking its patrons’ online 
 activities, Orbitz discovered that people who use  Apple’s 
Mac computers spend as much as 30 percent more per 
night on hotels. Orbitz’ analytics team has uncovered 
other factors that affect purchase behavior, including 
how the shopper arrived at the Orbitz site (directly or 
referred from another site), previous booking history on 
Orbitz, and the shopper’s geographic location. Orbitz 
can act on this and other information gleaned from the 
vast amount of Web data to differentiate the recommen-
dations for hotels, car rentals, flight bookings, and other 
purchases.
ONLINE RETAILERS USING PREDICTIVE ANALyTICS TO CATER TO CUSTOMERS*
ANALYTICS  in  Action
*“On Orbitz, Mac Users Steered to Pricier Hotels,” Wall Street Journal, 
June 26, 2012.

 
6.1 Data Sampling 
253
 2. Data Preparation: Manipulate the data to put it in a form suitable for formal modeling.
 3. Model Construction: Apply the appropriate data-mining technique (regression, clas-
sification trees, k-means) to accomplish the desired data-mining task (prediction, 
 classification, clustering, etc.).
 4. Model Assessment: Evaluate models by comparing performance on appropriate data sets.
Data Sampling
When a business problem is identified, data on relevant variables must be obtained for 
analysis. Although access to large amounts of data offers the possibility of unlocking in-
sight and improving decision making, it comes with the risk of drowning in a sea of data. 
Data repositories with millions of observations over hundreds of measured variables are 
now common. If the volume of relevant data is extremely large (thousands of observations 
or more), it is unnecessary (and computationally difficult) to use all the data for detailed 
analysis. When dealing with large volumes of data, it is best practice to extract a repre-
sentative sample for analysis. A sample is representative if the analyst can make the same 
conclusions from it as from the entire population of data.
There are no definite rules to determine the size of the sample. The sample of data must 
be large enough to contain significant information, yet small enough to be manipulated 
quickly. The best advice is perhaps to use enough data to eliminate any doubt about whether 
the sample size is sufficient; data-mining algorithms typically are more effective given 
more data. If we are investigating a rare event, such as click-throughs on an advertisement 
posted on a Web site, the sample should be large enough to ensure several hundreds to thou-
sands of observations that correspond to click-throughs. In other words, if the  click-through 
rate is only 1 percent, then a representative sample would need to be approximately 50,000 
observations in order to have about 500 observations corresponding to situations in which 
a person clicked on an ad.
When obtaining a representative sample, it is also important not to carelessly discard 
variables from consideration. It is generally best to include as many variables as possible in 
the sample. After exploring the data with descriptive statistics and visualization, the analyst 
can eliminate variables that are not of interest.
We utilize the XLMiner 
add-in for Excel to demon-
strate data-mining concepts 
and methods.
6.1
NOTES AND COMMENTS
XLMiner provides functionality to create data 
sets for data-mining analysis by sampling data 
from the larger volume residing in an Excel work-
sheet or a database (MS-Access, SQL Server, or 
Oracle). In the XLMINER tab in the Ribbon, the 
Data Analysis group contains the Sampling icon, 
which allows the user to choose the appropriate 
source, Sample from Worksheet or Sample 
from Database.
After the source of the sample is selected (work-
sheet or database), XLMiner offers several Sam-
pling Options in its Sampling dialog box. Users 
can specify a Desired sample size, and random 
samples can be generated by varying the random 
seed in the box next to Set seed. XLMiner sup-
ports Simple random sampling with or without 
replacement. In simple random sampling  without 
replacement, each observation is equally likely to 
be selected for the sample, and an observation can 
be selected for the sample only once. If Sample 
with replacement is selected, each observation 
is equally likely to be picked for the sample, and 
an observation can be inserted more than once 
into the sample; one use of this approach to artifi-
cially generate a larger sample when the number 
of observations observed is not large enough for 
the analysis desired. XLMiner also provides an 
option to execute Stratified random sampling, 
which allows the user to control the number of 
observations in the sample with certain values of 
a specified variable, called the stratum variable. 
One use of stratified sampling is to ensure that 
rare events of interest are  adequately represented 
in the sample.

254 
Chapter 6 Data Mining
Data preparation
The data in a data set are often said to be “dirty” and “raw” before they have been pre-
processed to put them into a form that is best suited for a data-mining algorithm. Data 
preparation makes heavy use of the descriptive statistics and data visualization methods 
described in Chapters 2 and 3 to gain an understanding of the data. Common tasks include 
treating missing data, identifying erroneous data and outliers, and defining the appropriate 
way to represent variables.
Treatment of Missing Data
It is common to have observations with missing values for one or more variables. The pri-
mary options for addressing missing data are (1) to discard observations with any missing 
values, (2) to discard any variable with missing values, (3) to fill in missing entries with 
estimated values, or (4) to apply a data-mining algorithm (such as classification and regres-
sion trees) that can handle missing values.
If the number of observations with missing values is small, throwing out these incom-
plete observations may be a reasonable option. However, the values may not be missing at 
random and there may be a reason for the missing variable measurement. For example, in 
health care data, an observation corresponding to a patient visit may be missing the results 
of a diagnostic procedure if the doctor deemed that the patient was too sick. In this case, 
throwing out all patient observations without measurements of this diagnostic procedure 
may bias the sample by removing a disproportionate number of extremely sick patients. In 
this case, the absence of a variable measurement actually provides the additional informa-
tion that the patient was too sick for the diagnostic procedure. This information could be 
helpful in understanding a phenomenon.
If a variable is missing measurements for a large number of observations, removing this 
variable from consideration may be an option. In particular, if the variable to be dropped 
is highly correlated with another variable that is known for a majority of observations, the 
loss of information may be minimal.
Another option is to fill in missing values with estimates. Convenient choices include 
replacing the missing entries for a variable with the variable’s mode, mean, or median. 
Imputing values in this manner is truly valid only if variable values are missing at random; 
otherwise we may be introducing misleading information into the data. If missing values 
are particularly troublesome, it may be possible to build a model to predict a variable with 
missing values and then to use these predictions in place of the missing entries.
Identification of Outliers and Erroneous Data
Examining the variables in the data set by means of summary statistics, histograms, 
PivotTables, scatter plots, and other tools can uncover data quality issues and outliers. For 
example, negative values for sales may result from a data entry error or may actually denote a 
missing value. Closer examination of outliers may reveal an error or a need for further investi-
gation to determine whether the observation is relevant to the current analysis. A conservative 
approach is to create two data sets, one with and one without out liers, and then construct a 
model on both data sets. If a model’s implications depend on the inclusion or exclusion of 
outliers, then one should spend additional time to track down the cause of the outliers.
Variable Representation
In many data-mining applications, the number of variables for which data is recorded may 
be prohibitive to analyze. In many data-mining applications the analyst may have to first 
identify variables that can be safely omitted from further analysis before proceeding with 
a data-mining technique. Dimension reduction is the process of removing variables from 
the analysis without losing any crucial information. One simple method for reducing the
6.2
XLMiner provides a 
 Missing Data Handling 
procedure under Trans-
form in the Data Analysis 
group.

 
6.3 Unsupervised Learning 
255
number of variables is to examine pairwise correlations to detect variables or groups of vari-
ables that may supply similar information. Such variables can be aggregated or removed to 
allow more parsimonious model development.
A critical part of data mining is determining how to represent the measurements of the 
variables and which variables to consider. The treatment of categorical variables is particularly 
important. Typically, it is best to encode categorical variables with 0–1 dummy variables. Con-
sider a data set that contains a variable Language to track the language preference of callers to 
a call center. The variable Language with the possible values of English, German, and Spanish 
would be replaced with three binary variables called English, German, and Spanish. An entry 
of German would be captured using a 0 for the English dummy variable, a 1 for the German 
dummy variable and a 0 for the Spanish dummy variable. Using 0–1 dummy variables to en-
code categorical variables with many different categories results in a large number of variables. 
In these cases, the use of PivotTables is helpful in identifying categories that are similar and 
can possibly be combined to reduce the number of 0–1 dummy variables. For example, some 
categorical variables (zip code, product model number) may have many possible categories 
such that, for the purpose of model building, there is no substantive difference between multiple 
categories, and therefore the number of categories may be reduced by combining categories.
Often data sets contain variables that, considered separately, are not particularly in-
sightful but that, when combined as ratios, may represent important relationships. Financial 
data supplying information on stock price and company earnings may be as useful as the 
derived variable representing the price/earnings (PE) ratio. A variable tabulating the dollars 
spent by a household on groceries may not be interesting because this value may depend on 
the size of the household. Instead, considering the proportion of total household spending 
on groceries may be more informative.
XLMiner provides a Trans-
form Categorical proce-
dure under Transform in 
the Data Analysis group. 
This procedure provides 
options to create dummy 
variables, create ordinal 
category scores, and reduce 
categories by combining 
them into similar groups.
NOTES AND COMMENTS
1. In some cases, it may be desirable to transform 
a continuous variable into categories. For ex-
ample, if we wish to apply a classification ap-
proach to a situation with a continuous outcome 
variable, we will need to first categorize the 
values of the outcome variable. If a variable 
has a skewed distribution, it may be helpful to 
categorize the values into quantiles. XLMiner 
provides a Bin Continuous Data procedure 
under Transform in the Data Analysis group. 
However, in general, we advise caution when 
transforming continuous variables into catego-
ries because this causes a loss of information (a 
continuous variable’s category is less informa-
tive than a specific numeric value) and increases 
the number of variables.
2. XLMiner provides functionality to apply a 
more sophisticated dimension reduction ap-
proach called principal components analysis. 
The Principal Components procedure can be 
found on the XLMINER tab under Transform 
in the Data Analysis group. Principal compo-
nents analysis creates a collection of metavari-
ables (components) that are weighted sums of 
the original variables. These components are 
uncorrelated with each other and often only a 
few of them are needed to convey the same in-
formation as the large set of original variables. 
In many cases, only one or two components are 
necessary to explain the majority of the vari-
ance in the original variables. Then, the analyst 
can continue to build a data-mining model using 
just a few of the most explanatory components 
rather than the entire set of original variables. 
Although principal components analysis can 
reduce the number of variables in this manner, 
it may be harder to explain the results of the 
model because the interpretation of a compo-
nent that is a linear combination of variables can 
be  unintuitive.
Unsupervised Learning
In this section, we discuss techniques in the area of data mining called unsupervised learn-
ing. In an unsupervised learning application, there is no outcome variable to predict; rather, 
the goal is to use the variable values to identify relationships between observations Without
6.3

256 
Chapter 6 Data Mining
an explicit outcome variable, there is no definite measure of accuracy. Instead, qualitative 
assessments, such as how well the results match expert judgment, are used to assess unsu-
pervised learning methods.
Cluster Analysis
The goal of clustering is to segment observations into similar groups based on the observed 
variables. Clustering can be employed during the data preparation step to identify variables 
or observations that can be aggregated or removed from consideration. Cluster analysis is 
commonly used in marketing to divide consumers into different homogeneous groups, a 
process known as market segmentation. Identifying different clusters of consumers allows 
a firm to tailor marketing strategies for each segment. Cluster analysis can also be used to 
identify outliers, which in a manufacturing setting may represent quality control problems 
and in financial transactions may represent fraudulent activity.
In this section, we consider the use of cluster analysis to assist a company called Know 
Thy Customer (KTC). KTC is a financial advising company that provides personalized 
financial advice to its clients. As a basis for developing this tailored advising, KTC would 
like to segment its customers into several groups (or clusters) so that the customers within a 
group are similar with respect to key characteristics and are dissimilar to customers that are 
not in the group. For each customer, KTC has an observation consisting of the age, gender, 
annual income, marital status, number of children, whether the customer has a car loan, and 
whether the customer has a home mortgage.
We present two clustering methods using a small sample of data from KTC. We first 
consider bottom-up hierarchical clustering that starts with each observation belonging 
to its own cluster and then sequentially merges the most similar clusters to create a series 
of nested clusters. The second method, k-means clustering, assigns each observation to 
one of k clusters in a manner such that the observations assigned to the same cluster are as 
similar as possible. Because both methods depend on how two observations are similar, we 
first discuss how to measure similarity between observations. 
Measuring similarity between observations The goal of cluster analysis is to 
group observations into clusters such that observations within a cluster are similar and 
observations in different clusters are dissimilar. Therefore, to formalize this process, we 
need explicit measurements of similarity or, conversely, dissimilarity. Some metrics track 
similarity between observations, and a clustering method using such a metric would seek 
to maximize the similarity between observations. Other metrics measure dissimilarity, or 
distance, between observations, and a clustering method using one of these metrics would 
seek to minimize the distance between observations in a cluster.
When observations include continuous variables, Euclidean distance is the 
most common method to measure dissimilarity between observations. Let observations 
u 5 (u1, u2, . . . , uq) and v 5 (v1, v2, . . . , vq) each comprise measurements of q variables. 
In the KTC example, each observation corresponds to a vector of measurements on 
seven customer variables, that is, (Age, Female, Income, Married, Children, Car Loan, 
Mortgage). For example, the observation u 5 (61, 0, 57881, 1, 2, 0, 0) corresponds to a 
61-year-old male with an annual income of $57,881, married with two children, but no 
car loan and no mortgage. The Euclidean distance between observations u and v is 
 
du,v 5 "(u1 2 v1) 2 1 (u2 2 v2) 2 1  c 1 (uq 2 vq) 2
Figure 6.1 depicts Euclidean distance for two observations consisting of two variable mea-
surements. Euclidean distance becomes smaller as a pair of observations become more 
similar with respect to their variable values. Euclidean distance is highly influenced by 

 
6.3 Unsupervised Learning 
257
the scale on which variables are measured. For example, the task of classifying customer 
observations based on income (most observations of which are in the tens of thousands of 
dollars) and age (measured in years and usually less than 100) will be dominated by the 
income variable due to the difference in magnitude of the measurements. Therefore, it is 
common to standardize the units of each variable j of each observation u; for example, uj, 
the value of variable j in observation u, is replaced with its z-score, zj.
The conversion to z-scores also makes it easier to identify outlier measurements, which 
can distort the Euclidean distance between observations. After conversion to z-scores, un-
equal weighting of variables can also be considered by multiplying the variables of each 
observation by a selected set of weights. For instance, after standardizing the units on 
customer observations so that income and age are expressed as their respective z-scores 
(instead of expressed in dollars and years), we can multiply the income z-scores by 2 if 
we wish to treat income with twice the importance of age. In other words, standardizing 
removes bias due to the difference in measurement units, and variable weighting allows the 
analyst to introduce appropriate bias based on the business context.
When clustering observations solely on the basis of categorical variables encoded as 
0–1 (or dummy variables), a better measure of similarity between two observations can be 
achieved by counting the number of variables with matching values. The simplest overlap 
measure is called the matching coefficient and is computed by
MATCHING COEFFICIENT
 
number of variables with matching value for observations u and v
total number of variables
One weakness of the matching coefficient is that if two observations both have a 0 entry 
for a categorical variable, this is counted as a sign of similarity between the two obser-
vations. However, matching 0 entries do not necessarily imply similarity. For instance, 
if the categorical variable is Own A Minivan, then a 0 entry in two different observa-
tions does not mean that these two people own the same type of car; it means only that 
neither owns a minivan. To avoid misstating similarity due to the absence of a feature, 
Refer to Chapter 2 for a 
discussion of z-scores.
Scaling and weighting 
variable values can be 
particularly helpful when 
clustering observations with 
respect to both continuous 
and categorical variables.
FIGURE 6.1  EUCLIDEAN DISTANCE
1st Variable
2nd Variable
v = (v1, v2)
u = (u1, u2)
duv

258 
Chapter 6 Data Mining
a similarity measure called Jaccard’s coefficient does not count matching zero entries 
and is computed by
JACCARD’S COEFFICIENT
number of variables with matching nonzero value for observations u and v
(total number of variables) 2 (number of variables with matching zero values for observations u and v)
Hierarchical clustering We consider a bottom-up hierarchical clustering approach that 
starts with each observation in its own cluster and then iteratively combines the two clusters 
that are the most similar into a single cluster. Each iteration corresponds to an increased 
level of aggregation by decreasing the number of distinct clusters. Hierarchical clustering 
determines the similarity of two clusters by considering the similarity between the observa-
tions composing either cluster. There are several methods for comparing observations in 
two clusters to obtain a cluster similarity measure. Figure 6.2 provides a two-dimensional 
depiction of the four methods we will discuss.
When using the single linkage clustering method, the similarity between two 
clusters is defined by the similarity of the pair of observations (one from each cluster) 
FIGURE 6.2  MEASURING SIMILARITy BETWEEN CLUSTERS
Complete Linkage, d1,6
Average Group Linkage, dc1,c2
9
Average Linkage,
d1,41d1,51d1,61d2,41d2,51d2,61d3,41d3,51d3,6
Single Linkage, d3,4
1
2
3
4
6
5
1
2
3
4
6
5
c1
c2
1
2
3
4
6
5
1
2
4
6
5
3

 
6.3 Unsupervised Learning 
259
that are the most similar. Thus, single linkage will consider two clusters to be close 
if an observation in one of the clusters is close to at least one observation in the other 
cluster. However, a cluster formed by merging two clusters that are close with respect 
to single linkage may also consist of pairs of observations that are very different. The 
reason is that there is no consideration of how different an observation may be from 
other observations in a cluster as long as it is similar to at least one observation in 
that cluster.
The complete linkage clustering method defines the similarity between two clusters as 
the similarity of the pair of observations (one from each cluster) that are the most different. 
Thus, complete linkage will consider two clusters to be close if their most different pair of 
observations are close. This method produces clusters such that all member observations 
of a cluster are relatively close to each other.
The single linkage and complete linkage methods define between-cluster similarity 
based on the single pair of observations in two different clusters that are most similar or 
least similar. In contrast, the average linkage clustering method defines the similarity 
between two clusters to be the average similarity computed over all pairs of observations 
between the two clusters. If cluster 1 consists of n1 observations and cluster 2 consists of 
n2 observations, the similarity of these clusters would be the average of n1 3 n2 similarity 
measures. This method produces clusters that are less dominated by the similarity between 
single pairs of observations.
Average group linkage uses the averaging concept of cluster centroids to define 
 between-cluster similarity. The centroid for cluster k, denoted ck, is found by calculating 
the average value for each variable across all observations in a cluster; that is, a centroid 
is the average observation of a cluster. The similarity between two clusters is then defined 
as the similarity of the centroids of the two clusters.
In addition to these four linkage measures, XLMiner also provides the option of 
 using Ward’s method to compute between-cluster similarity. For a pair of clusters under 
consideration for aggregation, Ward’s method computes dissimilarity as the sum of the 
squared differences in similarity between each individual observation in the union of the 
two clusters and the centroid of the resulting merged cluster. The process of aggregating 
observations into clusters and representing observations within a cluster with the centroid 
can be viewed as a loss of information in the sense that, unless the observations in a cluster 
are identical, the individual differences in these observations will not be captured by the 
cluster centroid. Hierarchical clustering using Ward’s method results in a sequence of 
clusters that minimizes this loss of information between the individual observation level 
and the cluster level.
Using XLMiner for hierarchical clustering KTC is interested in developing 
 customer segments based on the gender, marital status, and whether the customer is 
repaying a car loan and a mortgage. Using the file KTC-Small, the following steps, 
accompanied by Figure 6.3, demonstrate how to use XLMiner to construct hierarchi-
cal clusters. We base the clusters on a collection of 0–1 categorical variables (Female, 
Married, Car Loan, and Mortgage). We use Jaccard’s coefficient to measure similarity 
between observations and the average linkage clustering method to measure similarity 
between clusters.
Step 1. Select any cell in the range of the data
Step 2. Click the XLMINER tab in the Ribbon
Step 3. Click Cluster in the Data Analysis group
Step 4. Click Hierarchical Clustering
Step 5. When the Hierarchical Clustering—Step 1 of 3 dialog box appears:
 In the Data source area, confirm that the Worksheet:, Workbook:, and 
Data range: entries correspond to the appropriate data
file
WEB
KTC-Small
Typically clustering is 
executed on so-called 
raw data consisting of 
observations of variable 
measurements. However, in 
some cases, a precomputed 
distance matrix of pairwise 
dissimilarity between each 
pair of observations is used 
to cluster observations. For 
these cases, in the Hierar-
chical Clustering—Step 1 
of 3 dialog box, you should 
select Distance matrix for 
the Data type: in the Data 
source area

260 
Chapter 6 Data Mining
 In the Data source area, select Raw data from the drop down window 
next to Data type:
 In the Input data area, select the checkbox for Variable names in the 
first row
 In the Variables box of the Input data area, select the variables  
  Female, Married, Car Loan, and Mortage, and click the $ button  
 to populate the Selected variables box
Click Next .
Step 6. In the Hierarchical Clustering—Step 2 of 3 dialog box:
In the Similarity measure area, select Jaccard’s coefficients
In the Clustering method area, select Average linkage
Click Next .
Step 7. In the Hierarchical Clustering—Step 3 of 3 dialog box:
Select the checkboxes for Draw dendrogram and Show cluster 
 membership
In the box next to # Clusters, enter 2
Click Finish
This procedure produces a worksheet titled HC_Dendrogram1 that visually summarizes the 
clustering output with a dendrogram, as shown in Figure 6.4. A dendrogram is a chart that 
depicts the set of nested clusters resulting at each step of aggregation. The vertical axis on the 
dendrogram represents the dissimilarity (distance) between observations within the clusters, 
and the horizontal axis corresponds to the observation indexes. To interpret a dendrogram, 
visualize a horizontal line such as one of the black lines we have drawn across Figure 6.4. 
The bottom horizontal black line intersects with the vertical branches in the dendrogram four 
Double-clicking on the 
variable names Female, 
Married, Car Loan, and 
Mortage in the Variables 
box will also move these 
variables into the Selected 
variables box.
FIGURE 6.3   XLMINER STEPS FOR HIERARCHICAL CLUSTERING

 
6.3 Unsupervised Learning 
261
times; each intersection corresponds to a cluster containing the observations connected by 
the vertical branch that is intersected. The composition of these four clusters is
 Cluster 1: 516 5 single female with no car loan and no mortgage
 Cluster 2: 52, 14, 16, 26, 3, 9, 12, 24, 29, 13, 17, 18, 4, 5, 6, 11, 19, 28,
 15, 27, 8, 10, 20, 21, 22, 23, 306
5 married female or unmarried female with car loan or mortgage
 Cluster 3: 5256 5 single male with car loan and no mortgage
 Cluster 4: 576 5 single male with no car loan and no mortgage
These clusters segment KTC’s customers into four groups that could possibly indicate 
varying levels of responsibility—an important factor to consider when providing financial 
advice.
The nested construction of the hierarchical clusters allows KTC to identify different 
numbers of clusters and assess (often qualitatively) the implications. By sliding a horizontal 
line up or down the vertical axis of a dendrogram and observing the intersection of the hori-
zontal line with the vertical dendrogram branches, an analyst can extract varying numbers 
of clusters. Note that sliding up to the position of the top horizontal black line in Figure 6.4  
results in merging Cluster 1 with Cluster 2 into a single more dissimilar cluster. The verti-
cal distance between the points of agglomeration (e.g., four clusters to three clusters in 
Figure 6.4) is the “cost” of merging clusters in terms of decreased homogeneity within 
XLMiner’s hierarchical clus-
tering procedure produces 
two other worksheets, HC_
Output1 and HC_ Clusters1, 
which provide in table 
format the same information 
visually displayed in  
HC_Dendrogram1.  
HC_Output1 lists the 
 sequence in which clusters 
are aggregated and the 
increase in dissimilarity 
resulting from each merger. 
HC_Clusters1 provides 
a table showing each 
 observation’s final cluster. 
FIGURE 6.4   DENDROGRAM FOR KTC 
0
1
2 14 16 26 3
9 12 24 29 13 17 18 4
5
6 11 19 28 15 27 8 10 20 21 22 23 30 25 7
0.2
0.4
0.6
0.8
1.0
1.2
Dendrogram (Average linkage)
Observation
Distance

262 
Chapter 6 Data Mining
clusters. Thus, vertically elongated portions of the dendrogram represent mergers of more 
dissimilar clusters, and vertically compact portions of the dendrogram represent mergers of 
more similar clusters. A cluster’s durability (or strength) can be measured by the difference 
between the distance value at which a cluster is originally formed and the distance value at 
which it is merged with another cluster. Figure 6.4 shows that the singleton clusters com-
posed of {1} and {7}, respectively, are very durable clusters in this example because the 
vertical lines for these clusters are very long before they are merged with another cluster.
k-Means clustering In k-means clustering, the analyst must specify the number of clus-
ters, k. If the number of clusters, k, is not clearly established by the context of the business 
problem, the k-means clustering algorithm can be repeated for several values of k. Given a 
value of k, the k-means algorithm randomly partitions the observations into k clusters. After 
all observations have been assigned to a cluster, the resulting cluster centroids are calculated 
(these cluster centroids are the “means” of k-means clustering). Using the updated cluster 
centroids, all observations are reassigned to the cluster with the closest centroid (where 
 Euclidean distance is the standard metric). The algorithm repeats this process (calculate 
cluster centroid, assign observation to cluster with nearest centroid) until there is no change 
in the clusters or a specified maximum number of iterations is reached.
As an unsupervised learning technique, cluster analysis is not guided by any explicit 
measure of accuracy and thus the notion of a “good” clustering is subjective and is depen-
dent on what the analyst hopes the cluster analysis will uncover. Regardless, one can mea-
sure the strength of a cluster by comparing the average distance in a cluster to the distance 
between cluster centroids. One rule of thumb is that the ratio of between-cluster distance to 
within-cluster distance should exceed 1.0 for useful clusters.
To illustrate k-means clustering, we consider a 3-means clustering of a small sample 
of KTC’s customer data in the file KTC-Small. Figure 6.5 shows three clusters based on 
customer income and age. Cluster 1 is characterized by relatively older, higher-income 
customers (Cluster 1’s centroid is at [58, $47,729]), Cluster 2 is characterized by relatively 
younger, lower-income customers (Cluster 2’s centroid is at [37, $19,502]), and Cluster 3 
is characterized by relatively older, lower-income customers (Cluster 3’s centroid is at [57, 
$25,399]). As visually demonstrated in Figure 6.5, Table 6.1 shows that Cluster 3 is the 
smallest, most homogeneous cluster, whereas Cluster 2 is the largest, most heterogeneous 
cluster. To evaluate the strength of the clusters, we compare the average distances between 
clusters in Table 6.2 to the average distance within clusters in Table 6.1. Cluster 1 and 
If there is a wide disparity 
in cluster strength across 
a set of clusters, it may be 
possible to find a better 
clustering of the data by 
removing all members of 
the strong clusters and then 
continuing the clustering 
process on the remaining 
observations.
FIGURE 6.5   CLUSTERING OBSERVATIONS By AGE AND INCOME USING 
k-MEANS CLUSTERING WITH k 5 3
20
$10,000
$0
$20,000
$30,000
$40,000
$60,000
$50,000
30
40
Age (years)
Income
50
60
70
Cluster 1
Cluster 3
Cluster 2
Cluster centroids are depicted 
by circles in Figure 6.5.

 
6.3 Unsupervised Learning 
263
Cluster 2 are the most distinct from each other. Cluster 2 and Cluster 3 are the least distinct 
from each other. Comparing the distance between the Cluster 2 and Cluster 3 centroids 
(0.954) to the average distance between observations within Cluster 2 (0.996), suggests 
that there are observations within Cluster 2 that are more similar to those in Cluster 3 than 
to those in Cluster 2. Although qualitative considerations should take priority in evaluating 
the clusters, this blurred  distinction between Cluster 2 and Cluster 3 suggests investigating 
k 5 4 or perhaps clustering on additional or different variables.
Using XLMiner for k-means clustering KTC is interested in developing customer seg-
ments based on the age, income, and number of children. Using the file KTC-Small, the fol-
lowing steps and Figure 6.6 demonstrate how to execute k-means clustering with XLMiner.
Step 1. Select any cell in the range of the data
Step 2. Click the XLMINER tab in the Ribbon
Step 3. Click Cluster in the Data Analysis group
Step 4. Click k-Means Clustering
Step 5. When the k-Means Clustering—Step 1 of 3 dialog box appears:
In the Data source, confirm the Worksheet:, Workbook:, and Data 
range:  entries correspond to the appropriate data
 In the Input data area, select the checkbox for First row contains headers
In the Variables box of the Input data area, select the variables Age, 
 Income, and Children in the Variables in data source box, and click 
the $ button to populate the Input variables box
Click Next .
Step 6. In the k-Means Clustering—Step 2 of 3 dialog box:
Select the checkbox for Normalize input data
 In the Parameters area, enter 3 in the # Clusters box, and enter 50 in 
the # Iterations box
In the Options area, select Random Starts
 In the Start Options area, increase the number in the No. Of Starts 
box to 10
Click Next .
Step 7. In the k-Means Clustering—Step 3 of 3 dialog box, click Finish
# Iterations corresponds 
to the number of times that 
cluster centroids are recal-
culated and observations 
are reassigned to clusters. 
By choosing Random 
Starts and increasing No. 
Of Starts, the k-means 
algorithm is repeated on 
multiple randomly gener-
ated initial clusters and 
the best-found cluster set is 
reported. If the additional 
run time is not prohibitive, 
better clusters may result 
from a larger number of 
iterations and from a larger 
number of starts.
TABLE 6.1  AVERAGE DISTANCES WITHIN CLUSTERS
Number of Observations
Average Distance Between 
Observations in Cluster
Cluster 1
Cluster 2
Cluster 3
 8
17
 5
0.775
0.996
0.277
TABLE 6.2  DISTANCES BETWEEN CLUSTER CENTROIDS
Distance Between 
Cluster Centroids
Cluster 1
Cluster 2
Cluster 3
Cluster 1
Cluster 2
Cluster 3
    0
1.852
1.204
1.852
    0
0.954
1.204
0.954
0
file
WEB
 KTC-Small

264 
Chapter 6 Data Mining
This procedure produces a worksheet titled KM_Output1 (see Figure 6.7) that summarizes 
the procedure. Of particular interest on the KM_Output1 worksheet is the Cluster Centers 
information. As shown in Figure 6.7, clicking on the Cluster Centers link in the Output 
Navigator area at the top of the KM_Output1 worksheet brings information describing the 
clusters into view. The first table under Cluster centers in Figure 6.7 shows that Cluster 1  
consists of the youngest customers with large families and moderate incomes, Cluster 2 con-
sists of the low income customers who are in the midrange of age and have an average of one 
child, and Cluster 3 consists of the older customers with high incomes and no children. If 
KTC decides these clusters are appropriate, they can use them as a basis for creating financial 
advising plans based on the characteristics of each cluster.
The second table under Cluster centers in Figure 6.7 displays the between-clus-
ter distances between the three cluster centers. The Data summary table displays the 
within-cluster distances and reveals that all three clusters possess approximately the 
same amount of within-cluster similarity, with Cluster 3 being slightly more homoge-
neous. By comparing the between-cluster distance in the second table under Cluster 
centers to the within-cluster distance in Data summary table, we observe that the ob-
servations within clusters are much more similar than the observations between clusters. 
Cluster 1 and Cluster 3 are the most distinct cluster pair, Cluster 2 and Cluster 3 are the 
second most distinct cluster pair, and Cluster 1 and Cluster 2 are the least distinct from 
each other. By conducting k-means clusters for other values of k, we can evaluate how 
the choice of k affects the within-cluster and between-cluster distances and therefore the 
strength of the clustering.
Hierarchical clustering versus k-means clustering If you have a small data set 
(e g
less than 500 observations) and want to easily examine solutions with increasing
XLMiner’s k-means cluster-
ing procedure produces one 
other worksheet, KM_ 
Clusters1, which lists each 
observation’s assigned 
cluster in the Cluster id 
column as well as the dis-
tance from the observation 
to each cluster center that 
the observation was not 
assigned.
FIGURE 6.6   XLMINER STEPS FOR k-MEANS CLUSTERING

 
6.3 Unsupervised Learning 
265
numbers of clusters, you may want to use hierarchical clustering. Hierarchical clusters are 
also convenient if you want to observe how clusters are nested. If you know how many 
clusters you want and you have a larger data set (e.g., larger than 500 observations), you 
may choose to use k-means clustering. Recall that k-means clustering partitions the obser-
vations, which is appropriate if trying to summarize the data with k “average” observations 
that describe the data with the minimum amount of error. Because Euclidean distance is 
the standard metric for k-means clustering, it is generally not as appropriate for binary or 
ordinal data for which an “average” is not meaningful.
Association Rules
In marketing, analyzing consumer behavior can lead to insights regarding the location and 
promotion of products. Specifically, marketers are interested in examining transaction data on 
customer purchases to identify the products commonly purchased together. In this section, we 
discuss the development of if-then statements, called association rules, which convey the like-
lihood of certain items being purchased together. Although association rules are an important 
tool in market basket analysis, they are applicable to disciplines other than marketing. For 
example, association rules can assist medical researchers in understanding which treatments 
have been commonly prescribed to certain patient symptoms (and the resulting effects)
FIGURE 6.7   DISTANCE INFORMATION FOR k-MEANS CLUSTERS
A
Inputs
Elapsed Time
Cluster Centers
Predicted Clusters
B
C
D
E
F
G
H
I
J
K
XLMiner : k-Means Clustering
Cluster centers
Data summary
Output Navigator
1
2
3
4
5
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
Cluster
40
48
57
Age
30085.1
17546
50576.3
Income
3
1
0
Children
Distance between
cluster centers
0
2.19059424
3.47032888
Cluster-1
2.19059424
0
2.71823031
Cluster-2
3.47032888
2.71823031
0
Cluster-3
Cluster-1
Cluster-2
Cluster-3
Overall
Cluster
6
17
7
30
#Obs
1.081823
1.188126
0.991933
1.121087
Average
distance in
cluster
Cluster-1
Cluster-2
Cluster-3
Overall
Cluster
6
17
7
30
#Obs
6740.974093
4205.29711
7484.674481
5477.62056
Average
distance in
cluster
Data summary (In original coordinates)
Data Summ.
Random Starts Summary
Cluster-1
Cluster-2
Cluster-3
Cluster-1
Cluster-2
Cluster-3

266 
Chapter 6 Data Mining
Hy-Vee grocery store would like to gain insight into its customers’ purchase patterns 
to possibly improve its in-aisle product placement and cross-product promotions. Table 6.3 
contains a small sample of data where each transaction comprises the items purchased by a 
shopper in a single visit to a Hy-Vee. An example of an association rule from this data would 
be “if {bread, jelly}, then {peanut butter}” meaning that “if a transaction includes bread and 
jelly it also includes peanut butter.” The collection of items (or item set) corresponding to the 
if portion of the rule, {bread, jelly}, is called the antecedent. The item set corresponding to 
the then portion of the rule, {peanut butter}, is called the consequent. Typically, only asso-
ciation rules for which the consequent consists of a single item are considered because these 
are more actionable. Although the number of possible association rules can be overwhelm-
ing, we typically investigate only association rules that involve antecedent and consequent 
item sets that occur together frequently. To formalize the notion of “frequent,” we define the 
support count of an item set as the number of transactions in the data that include that item 
set. In Table 6.3, the support count of {bread, jelly} is 4. A rule-of-thumb is to consider only 
association rules with a support count of at least 20  percent of the total number of transactions.
The potential impact of an association rule is often governed by the number of transac-
tions it may affect, which is measured by computing the support count of the item set consist-
ing of the union of its antecedent and consequent. Investigating the rule “if {bread, jelly}, 
then {peanut butter}” from the Table 6.3, we see the support count of {bread, jelly, peanut 
butter} is 2. By only considering rules involving item sets with a support above a minimum 
level, inexplicable rules capturing random noise in the data can generally be avoided.
To help identify reliable association rules, we define the measure of confidence of a 
rule, which is computed as
CONFIDENCE
support of 5antecedent and consequent6
support of antecedent
This measure of confidence can be viewed as the conditional probability of the consequent 
item set occurs given that the antecedent item set occurs. A high value of confidence sug-
gests a rule in which the consequent is frequently true when the antecedent is true, but a 
high value of confidence can be misleading. For example, if the support of the consequent 
is high—that is, the item set corresponding to the then part is very frequent—then the con-
fidence of the association rule could be high even if there is little or no association between 
the items. In Table 6.3, the rule “if {cheese}, then {fruit}” has a confidence of 1.0 (or 100 
percent). This is misleading because {fruit} is a frequent item, the confidence of almost any 
rule with {fruit} as the consequent will have high confidence Therefore to evaluate the
If an item set is particularly 
valuable, then the minimum 
support used to filter rules 
is often lowered.
Support is also sometimes 
expressed as the percent-
age of total transactions 
containing an item set.
TABLE 6.3  SHOPPING CART TRANSACTIONS
Transaction
Shopping Cart
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
bread, peanut butter, milk, fruit, jelly 
bread, jelly, soda, potato chips, milk, fruit, vegetables, peanut butter
whipped cream, fruit, chocolate sauce, beer
steak, jelly, soda, potato chips, bread, fruit
jelly, soda, peanut butter, milk, fruit 
jelly, soda, potato chips, milk, bread, fruit
fruit, soda, potato chips, milk
fruit, soda, peanut butter, milk 
fruit, cheese, yogurt 
yogurt, vegetables, beer
The data in Table 6.3 are 
in item list format; that 
is, each transaction row 
corresponds to a list of item 
names. Alternatively, the 
data can be represented 
in binary matrix format, 
in which each row is a 
transaction record and 
the columns correspond 
to each distinct item. This 
is equivalent to encoding 
each item name with a 0–1 
dummy variable.

 
6.3 Unsupervised Learning 
267
efficiency of a rule, we compute the lift ratio of the rule by accounting for the frequency 
of the consequent:
LIFT RATIO
confidence
support of consequent/total number of transactions
A lift ratio greater than 1 suggests that there is some usefulness to the rule and that it is 
better at identifying cases when the consequent occurs than no rule at all. In other words, 
a lift ratio greater than 1 suggests that the level of association between the antecedent and 
consequent is higher than would be expected if these item sets were independent.
For the data in Table 6.3, the rule “if {bread, jelly}, then {peanut butter}” has con-
fidence 5 2/4 5 0.5 and a lift ratio 5 0.5/(4/10) 5 1.25. In other words, identifying a 
customer who purchased both bread and jelly as one who also purchased peanut butter is 
25 percent better than just guessing that a random customer purchased peanut butter.
The utility of a rule depends on both its support and its lift ratio. Although a high lift 
ratio suggests that the rule is very efficient at finding when the consequent occurs, if it has 
a very low support, the rule may not be as useful as another rule that has a lower lift ratio 
but affects a large number of transactions (as demonstrated by a high support).
Using XLMiner to develop association rules Using the file HyVee-Small, the fol-
lowing steps and Figure 6.8 demonstrate how to examine association rules using XLMiner.
Step 1. Select any cell in the range of the data
Step 2. Click the XLMINER tab in the Ribbon
Step 3. Click Associate in the Data Mining group
Step 4. Click Association Rules
Step 5. When the Assiciation Rule dialog box appears, in the Data source area, con-
firm that the Worksheet:, Workbook:, and Data range: entries correspond 
to the appropriate data
Adjusting the data by 
 aggregating items into 
more general categories 
(or splitting items into more 
specific categories) so that 
items occur in roughly the 
same number of transac-
tions often yields better 
association rules.
An association rule with a 
high lift ratio and low sup-
port may still be useful if 
the consequent represents a 
very valuable opportunity. 
file
WEB
HyVee-Small
FIGURE 6.8   XLMINER ASSOCIATION RULE DIALOG BOX

268 
Chapter 6 Data Mining
Step 6. In the Data source area, select the checkbox for First row contains headers
Step 7. In the Input data format area, select Data in binary matrix format
Step 8. In the Parameters area, enter 4 in the box next to Minimum support (# trans-
actions), and enter 50 in the box next to Minimum confidence (%)
Step 9. Click OK
The procedure generates a worksheet titled AssocRules_1, as illustrated in Figure 6.9. Rules 
satisfying the minimum support rule of 4 transactions (out of 10) and the minimum confi-
dence of 50 percent are displayed in decreasing order of lift ratio. By clicking on a row in 
the table of rules, XLMiner will display the interpretation of the corresponding rule in the 
FIGURE 6.9   XLMINER ASSOCIATION RULES OUTPUT
A
B
C
D
E
F
G
H
XLMiner : Association Rules
Rule No. Conf. %
Antecedent (a)
Data
Consequent (c)
Support(a) Support(c) Support(a : c) Lift Ratio
Bread, Fruit=>
Jelly
Bread=>
Fruit, Jelly
Bread=>
Jelly
Fruit, Jelly=>
Bread
Jelly=>
Bread, Fruit
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Peanut Butter=>
Milk
Peanut Butter=>
Fruit, Milk
Fruit, Peanut Butter=> Milk
Potato Chips=>
Soda
Fruit, Potato Chips=>
Soda
Potato Chips=>
Fruit, Soda
Milk=>
Peanut Butter
Milk=>
Fruit, Peanut Butter
Fruit, Soda=>
Potato Chips
Soda =>
Potato Chips
27
Soda =>
Fruit, Potato Chips
Fruit, Milk=>
Peanut Butter
Soda =>
Fruit, Milk
Fruit, Milk=>
Soda
Fruit, Soda =>
Milk
Milk=>
Soda
1
2
3
5
6
7
8
9
11
12
13
14
15
16
17
18
19
20
21
22
23
10
100
100
100
80
80
100
100
100
100
100
100
66.67
66.67
66.67
66.67
66.67
66.67
83.33
83.33
83.33
83.33
83.33 Milk=>
Fruit, Soda
4
4
4
5
5
4
4
4
4
4
4
6
6
6
6
6
6
6
6
6
6
6
22
18
19
20
21
23
24
25
26
28
29
30
31
32
33
34
35
36
37
38
Input Data
BinaryMatrixData!$A$1:$N$11
Data Format
Binary Matrix
Minimum Support
4
Minimum Confidence % 50
No. of Rules
52
Overall Time (secs)
2
Rule 4: If item(s) Jelly= is/are purchased, then this implies item(s) Bread is/are also purchased. This rule has confidence of 80%. 
Jelly=>
Bread
4
80
5
5
5
5
4
4
4
6
6
6
6
6
6
4
4
4
4
4
4
6
6
6
6
6
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
4
5
5
5
5
5
2
2
2
2
2
2
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.666667
1.388889
1.388889
1.388889
1.388889
1.388889

 
6.4 Supervised Learning 
269
range B11:F14. The top rules in Figure 6.9 suggest that bread, fruit, and jelly are commonly 
associated items. Perhaps HyVee could consider a promotion and/or product placement to 
leverage this perceived relationship.
Evaluating association rules Although explicit measures such as support, confidence, 
and lift ratio can help filter association rules, an association rule is ultimately judged 
on how actionable it is and how well it explains the relationship between item sets. For 
 example, Wal-Mart mined its transactional data to uncover strong evidence of the associa-
tion rule, “If a customer purchases a Barbie doll, then a customer also purchases a candy 
bar.” Wal-Mart could leverage this relationship in product placement decisions as well as 
in advertisements and promotions, perhaps by placing a high margin candy-bar display 
near the Barbie dolls. However, we must be aware that association rule mining often results 
in obvious relationships such as, “If a customer purchases hamburger, then a customer also 
purchases hamburger buns,” which may be true but provide no new insight. Association 
rules with a weak support measure often are inexplicable. For an association rule to be use-
ful, it must be well supported and explain an important previously unknown relationship. 
The support of an association rule can generally be improved by basing it on less specific 
antecedent and consequent item sets. Unfortunately, association rules based on less spe-
cific item sets tend to yield less insight.
Supervised Learning
In this section, we discuss techniques in the area of data mining called supervised learning. 
The goal of a supervised learning technique is to develop a model that predicts a value for 
a continuous outcome or classifies a categorical outcome. We begin by discussing how to 
partition the data set in order to appropriately evaluate future performance of the model. We 
then discuss performance measures for classification and prediction methods. We present 
three commonly used supervised learning methods: k-nearest neighbors, classification and 
regression trees, and logistic regression.
In this section, we consider an example from the financial services industry. Optiva 
Credit Union wants to better understand its personal lending process and its loan custom-
ers. The file Optiva contains over 40,000 loan customer observations with information 
on whether the customer defaulted on the loan, customer age, average checking account 
balance, whether the customer had a mortgage, customer’s job status, customer’s marital 
status, and customer’s level of education. We will use this data to demonstrate the use of 
supervised learning methods to classify customers who are likely to default and to predict 
the average customer balance in their respective bank accounts.
Partitioning Data
Consider a situation in which an analyst has relatively few data points from which to build 
a multiple regression model. To maintain the sample size necessary to obtain reliable esti-
mates of slope coefficients, an analyst may have no choice but to use the entire data set to 
build a model. Even if measures such as R2 and standard error of the estimate suggest that 
the resulting linear regression model may fit the data set well, these measures explain only 
how well the model fits data that it has “seen,” and the analyst has little idea how well this 
model will fit other “unobserved” data points.
Scarcity of data necessitates classical statistical techniques to determine the minimum re-
quired amount of sample data to collect and to draw inferences about the population through 
confidence intervals and hypothesis tests from the sample data. In contrast, data mining appli-
cations deal with an abundance of data that simplifies the process of assessing the accuracy of 
data-based estimates of variable effects. However, the wealth of data can provide the temptation 
6.4
file
WEB
Optiva
Multiple regression models 
are discussed in Chapter 4.

270 
Chapter 6 Data Mining
to overfit a model. Overfitting occurs when the analyst builds a model that does a great job of 
explaining the sample of data on which it is based but fails to accurately predict outside the 
sample data. We can use the abundance of data to guard against the potential for overfitting by 
decomposing the data set into three partitions: the training set, the validation set, and the test set.
The training set consists of the data used to build the candidate models. For example, a 
training set may be used to estimate the slope coefficients in a multiple regression model. We 
use measures of accuracy of these models on the training set to identify a promising initial 
subset of models. However, because the training set is the data used to build the models, it 
cannot be used to clearly identify the best model for predicting when applied to new data 
(data outside the training set). Therefore, the promising subset of models is then applied to 
the validation set to identify which model is the most accurate at predicting when applied to 
data that were not used to build the model. Depending on the data-mining method applied, 
the validation set can also be used to tune model parameters. If the validation set is used to 
identify a “best” model through either comparison with other models or model improvement, 
then the estimates of model accuracy are biased (we tend to overestimate accuracy). Thus, the 
final model should be applied to the test set in order to conservatively estimate this model’s 
effectiveness when applied to data that have not been used to build or select the model.
For example, suppose we have identified four models that fit the training set reasonably 
well. To evaluate how these models will predict when applied to new data, we apply them to the 
validation set. After identifying the best of the four models, we apply this “best” model to the 
test set in order to obtain an unbiased estimate of this model’s accuracy on future applications.
There are no definite rules for the size of the three partitions, but the training set is typically 
the largest. For prediction tasks, a rule of thumb is to have at least ten times as many observations 
as variables. For classification tasks, a rule of thumb is to have at least 6 3 m 3 q observations, 
where m is number of outcome categories and q is the number of variables. When we are inter-
ested in predicting a rare event, such as click-throughs on an advertisement posted on a Web site, 
it is recommended that the training set oversample the number of observations corresponding 
to the rare events to provide the data-mining algorithm sufficient data to “learn” about the rare 
events. For example, if we have 10,000 events, but only one for which the user clicked through 
an advertisement posted on a Web site, we would not have sufficient information to distinguish 
between users who do not click through and those who do. In these cases, the training set should 
contain equal or nearly equal numbers of observations corresponding to the different values 
of the outcome variable. Note that we do not oversample the validation set and test sets; these 
samples should be representative of the overall population so that accuracy measures evaluated 
on these data sets appropriately reflect future performance of the data-mining model.
Using XLMiner to partition data with oversampling In the Optiva file, we observe 
that only 1.8 percent of the customer observations correspond to a default. Thus the task of 
classifying loan customers as either default or no default involves a rare event. To provide 
a classification algorithm with sufficient information on loan defaults, we create a training 
set with 50 percent loan default observations. The validation set and test set are formed 
to have approximately 1.8 percent loan default observations in order to be representative 
of the overall population. The following steps and Figure 6.10 demonstrate this process.
 Step 1. Select any cell in the range of the data
 Step 2. Click the XLMINER tab in the Ribbon
 Step 3. Click Partition in the Data Mining group
 Step 4. Click Partition with Oversampling
 Step 5. When the Partition with oversampling dialog box appears, in the Data 
source area, confirm that the Worksheet:, Workbook:, and Data range: 
entries correspond to the appropriate data
 Step 6. In the Variables area, select the checkbox for First row contains headers
 Step 7. In the Variables box of the Variables area, select LoanDefault, 
 AverageBalance, Age, Entrepreneur, Unemployed, Married, Divorced, 
file
WEB
Optiva

 
6.4 Supervised Learning 
271
High School, and  College variables, and click the $ button to populate the 
Variables in the partitioned data box
 Step 8. Select LoanDefault in the Variables in the partitioned data box of the 
Variables area
 Step 9. Click the $ button to populate the Output variable: Choose one of the 
selected variables box
Step 10. In the Output options area, select 1 from the drop down menu of the Specify 
 Success class:
Step 11. In the Output options area, enter 50 in the Specify % Success in training 
set: box, and enter 40 in the Specify % validation data to be taken away as 
test data: box
Step 12. Click OK
Using XLMiner for standard partition of data To partition the data in the Optiva file 
for the purposes of predicting a customer’s average balance, we use XLMiner’s Standard 
Data Partition procedure. The following steps and Figure 6.11 demonstrate the process of 
partitioning a data set so that 23.15 percent of the observations compose the training set, 
Different random samples 
can be generated by 
changing the seed value 
via the Set seed box in the 
 Randomization options 
area.
FIGURE 6.10   XLMINER DATA PARTITION WITH OVERSAMPLING DIALOG BOX
This procedure creates a 
worksheet titled Data_ 
Partition1 containing a 
training set of 782 observa-
tions, a validation set of 
12,958 observations, and a 
test set of 8638 observa-
tions. We will use the 
data on this worksheet to 
demonstrate classification 
methods.

272 
Chapter 6 Data Mining
46.11 percent of the observations compose the validation set, and 30.74 percent of the 
 observations compose the test set.
 Step 1. Select any cell in the range of the data
 Step 2. Click the XLMINER tab in the Ribbon
 Step 3. Click Partition in the Data Mining group
 Step 4. Click Standard Partition
 Step 5. When the Standard Data Partition dialog box appears, in the Data source 
area, confirm that the Worksheet:, Workbook:, and Data range: entries 
correspond to the appropriate data
 Step 6. In the Variables area, select the checkbox for First row contains headers
 Step 7. In the Variables box of the Variables area, select AverageBalance, Age, 
 Entrepreneur, Unemployed, Married, Divorced, High School, and 
 College variables, and click the $ button to populate the Variables in the 
partitioned data box
Step 8 In the Partitioning options area select Pick up rows randomly
FIGURE 6.11   XLMINER STANDARD DATA PARTITION DIALOG BOX
This procedure  creates a 
worksheet titled Data_ 
Partition2 containing 
a training set of 9999 
observations, a validation 
set of 19,916 observations, 
and a test set of 13,278 
observations. We will use 
the data on this worksheet 
to demonstrate prediction 
methods.

 
6.4 Supervised Learning 
273
 Step 9. In the Partitioning percentages when picking up rows randomly area, 
select Specify percentages
Step 10. In the Partitioning percentages when picking up rows randomly area, 
enter 23.15 in the Training Set box, enter 46.11 in the Validation Set box, 
and enter 30.74 in the Test Set box
Step 11. Click OK
Classification Accuracy
In our treatment of classification problems, we restrict our attention to problems for which we 
want to classify observations into one of two possible classes (e.g., loan default or no default), 
but the concepts generally extend to cases with more than two classes. A natural way to evalu-
ate the performance of a classification method, or classifier, is to count the number of times 
that an observation is predicted to be in the wrong class. By counting the classification errors 
on a sufficiently large validation set and/or test set that is representative of the population, we 
will generate an accurate measure of the model’s classification performance.
Classification error is commonly displayed in a classification confusion matrix, which 
displays a model’s correct and incorrect classifications. Table 6.4 illustrates a classifica-
tion confusion matrix resulting from an attempt to classify the customer observations in a 
subset of data from the file Optiva. In this table, Class 1 5 loan default and Class 0 5 no 
default. The classification confusion matrix is a crosstabulation of the actual class of each 
observation and the predicted class of each observation. From the first row of the matrix 
in Table 6.4, we see that 146 observations corresponding to loan defaults were correctly 
identified as such, but another 89 actual loan defaults were classified as nondefault obser-
vations. From the second row, we observe that 5244 actual nondefault observations were 
incorrectly classified as loan defaults, whereas 7479 nondefaults were correctly identified.
Many measures of classification accuracy are based on the classification confusion 
matrix. The percentage of misclassified observations is expressed as the overall error rate 
and is computed as
OVERALL ERROR RATE
n10 1 n01
n11 1 n10 1 n01 1 n00
The overall error rate of the classification in Table 6.4 is (89 1 5244)y(146 1 89 1 
5244 1 7479) 5 41.2 percent.
Although overall error rate conveys an aggregate measure of misclassification, it counts 
misclassifying an actual Class 0 observation as a Class 1 observation (a false positive) 
the same as misclassifying an actual Class 1 observation as a Class 0 observation (a false 
negative). In many situations, the cost of making these two types of errors is not equivalent. 
For example, suppose we are classifying patient observations into two categories: Class 1 
is “cancer,” and Class 0 is “healthy.” The cost of incorrectly classifying a healthy patient 
observation as “cancer” will likely be limited to the expense (and stress) of additional test-
ing. The cost of incorrectly classifying a cancer patient observation as “healthy” may result 
in an indefinite delay in treatment of the cancer and premature death of the patient.
In this classification 
confusion matrix, n01 is the 
number of false positives, 
and n10 is the number of 
false negatives.
One minus the overall error 
rate is often referred to as 
the accuracy of the model.
TABLE 6.4  CLASSIFICATION CONFUSION MATRIX
Predicted Class
Actual Class
1
0
1
0
n11 5 146 
n01 5 5244
n10 5 89
n00 5 7479

274 
Chapter 6 Data Mining
To account for the assymetric costs in misclassification, we define error rate with 
 respect to the individual classes:
 
Class 1 error rate 5
n10
n11 1 n10
 
Class 0 error rate  5
n01
n01 1 n00
The Class 1 error rate of the classification in Table 6.4 is (89)y(146 1 89)  5 37.9 percent. The 
Class 0 error rate of the classification in Table 6.4 is (5244)y(5244 1 7479) 5 41.2 percent.
To understand the tradeoff between Class 1 error rate and Class 0 error rate, we must 
be aware of the criteria that classification algorithms generally employ to classify observa-
tions. Most classification algorithms first estimate an observation’s probability of Class 1 
membership and then classify the observation into Class 1 if this probability meets or ex-
ceeds a specified cutoff value (the default cutoff value is 0.5). The choice of cutoff value 
affects the type of classification error. As we decrease the cutoff value, more observations 
will be classified as Class 1, thereby increasing the likelihood that Class 1 observation will 
be correctly classified as a Class 1 observation; that is, Class 1 error will decrease. However, 
as a side effect, more Class 0 observations will be incorrectly classified as Class 1 observa-
tions; that is, Class 0 error will rise.
To demonstrate how the choice of cutoff value affects classification error, Table 6.5 
shows a list of 50 observations (11 of which are actual Class 1 members) and an estimated 
probability of Class 1 membership produced by the classification algorithm. Table 6.6 
One minus the Class 1 
 error rate is a measure of 
the ability of the model to 
correctly identify positive 
results and is often referred 
to as the sensitivity of 
the model. One minus 
the class 0 error rate is a 
measure of the ability of the 
model to correctly identify 
negative results and is often 
referred to as the specificity 
of the model.
TABLE 6.5  CLASSIFICATION PROBABILITIES
Actual 
Class
Probability 
of Class 1
Actual 
Class
Probability 
of Class 1
1
1
0
1
0
0
1
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1.00
1.00
1.00
1.00
1.00
0.90
0.90
0.88
0.88
0.88
0.87
0.87
0.87
0.86
0.86
0.86
0.86
0.85
0.84
0.84
0.83
0.68
0.67
0.67
0.67
0
0
1
0
0
0
0
0
1
0
1
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0.66
0.65
0.64
0.62
0.60
0.51
0.49
0.49
0.46
0.46
0.45
0.45
0.45
0.44
0.44
0.30
0.28
0.26
0.25
0.22
0.21
0.04
0.04
0.01
0.00

 
6.4 Supervised Learning 
275
shows classification confusion matrices and corresponding Class 1 error rates, Class 0 error 
rates, and overall error rates for cutoff values of 0.75, 0.5, and 0.25, respectively. As we 
decrease the cutoff value, more observations will be classified as Class 1, thereby increasing 
the likelihood that a Class 1 observation will be correctly classified as a Class 1 observation; 
that is, Class 1 error will decrease. However, as a side effect, more Class 0 observations 
will be incorrectly classified as Class 1 observations; that is, Class 0 error will rise. In other 
words, we can accurately identify more of the actual Class 1 observations by lowering the 
cutoff value, but we do so at a cost of misclassifying more actual Class 0 observations 
TABLE 6.6  CLASSIFICATION CONFUSION MATRICES AND ERROR RATES FOR VARIOUS CUTOFF VALUES
Cutoff Value 5 0.75
Predicted Class
Actual Class
1
0
1
0
n11 5 6 
n01 5 15
n10 5 5
n00 5 24
Actual Class
Number of Cases
Number of Errors
Error Rate (%)
1
0
Overall
n11 1 n10 5 11
n01 1 n00 5 39
n11 1 n10 1 n01 1 n00 5 50
n10 5 5
n01 5 15
n10 1 n01 5 20
45.45
38.46
40.00
Cutoff Value 5 0.50
Predicted Class
Actual Class
1
0
1
0
n11 5 7
n01 5 24
n10 5 4
n00 5 15
Actual Class
Number of Cases
Number of Errors
Error Rate (%)
1
0
Overall
11 
39
50
 4
24
28
36.36
61.54
56.00
Cutoff Value 5 0.25
Predicted Class
Actual Class
1
0
1
0
n11 5 10
n01 5 33
n10 5 1
n00 5 6
Actual Class
Number of Cases
Number of Errors
Error Rate (%)
1
0
Overall
11 
39
50
 1
33
34
 9.09
84.62
68.00

276 
Chapter 6 Data Mining
FIGURE 6.12  CLASSIFICATION ERROR RATES VERSUS CUTOFF VALUE
100
90
80
70
60
50
40
30
20
10
00
0.1
Error Rate (%)
0.2
0.3
0.4
0.5
Cutoff Value
0.6
0.7
0.8
0.9
1
Class 1 Error Rate
Class 0 Error Rate
Figure 6.12 was created 
using a data table that 
varied the cutoff value and 
tracked the Class 1 error 
rate and Class 0 error rate. 
For instructions on how 
to construct data tables in 
Excel, see Chapter 7.
as Class 1 observations. Figure 6.12 shows the Class 1 and Class 0 error rates for cutoff 
values ranging from 0 to 1. One common approach to handling the tradeoff between Class 1 
and Class 0 error is to set the cutoff value to minimize the Class 1 error rate subject to a 
threshold on the maximum Class 0 error rate. For example, a maximum Class 0 error rate of 
70 percent, a cutoff value of 0.45 minimizes the Class 1 error rate to a value of 20 percent.
As we have mentioned, identifying Class 1 members is often more important than 
identifying Class 0 members. One way to evaluate a classifier’s value is to compare its 
effectiveness at identifying Class 1 observations versus randomly guessing. To gauge a 
classifier’s value added, a cumulative lift chart compares the number of actual Class 1 
observations identified if considered in decreasing order of their estimated probability of 
being in Class 1 and compares this to the number of actual Class 1 observations identified if 
randomly  selected. The left panel of Figure 6.13 illustrates a cumulative lift chart. The point 
(10, 5) on the light blue curve means that if the 10 observations with the largest estimated 
probabilities of being in Class 1 were selected, 5 of these observations correspond to actual 
Class 1 members (refer to Table 6.5 to confirm). In contrast, the point (10, 2.2) on the dark 
blue diagonal line means that if 10 observations were randomly selected, an average of only 
(11/50) 3 10 5 2.2 of these observations would be Class 1 members. In other words, if 10 
observations were randomly selected from the 50 in Table 6.5, an average of 2.2 would be 
FIGURE 6.13  CUMULATIVE AND DECILE-WISE LIFT CHARTS
12
3
10
8
6
(10, 5)
(10, 2.2)
4
2
00
20
Cumulative
40
Number of Cases
Lift Chart
(Validation Data Set)
Decile-Wise Lift Chart
(Validation Data Set)
60
1
0
Decile Mean /Global
Mean
0.5
1
1.5
2
2.5
2
3
4
5
6
Deciles
7
8
9
10
Cumulative class 1
records when sorted
using predicted
values
Cumulative class 1
records using average

 
6.4 Supervised Learning 
277
actual Class 1 members. Visually, the better that the classifier is at identifying responders, 
the larger the vertical gap between points on the red diagonal line and the blue curve.
Another way to view how much better a classifier is at identifying Class 1 observations 
than random classification is to construct a decile-wise lift chart. For a decile-wise lift 
chart, observations are ordered in decreasing probability of Class 1 membership and then 
considered in 10 equal-sized groups. For the data in Table 6.5, the first decile group corre-
sponds to the 0.1 3 50 5 5 observations most likely to be in Class 1, the second decile group 
corresponds to the sixth through the tenth observations most likely to be in Class 1, and so 
on. For each of these decile groups, the decile-wise lift chart compares the number of actual 
Class 1 observations to the number of Class 1 responders in a randomly selected group of 
0.1 3 50 5 5 observations. In the first decile group (top 10 percent of observations most 
likely to be in Class 1) are three Class 1 observations. A random sample of 5 observations 
would be expected to have 5 3 (11/50) 5 1.1 observations in Class 1.Thus the first-decile 
lift of this classification is 3/1.1 5 2.73, which corresponds to the height of the first bar in 
the chart in the right panel of Figure 6.13. The height of the bars corresponds to the second 
through tenth deciles in a similar manner. Visually, the taller the bar in a decile-wise lift 
chart, the better the classifier is at identifying responders in the respective decile group.
The computation of lift charts is prominently used in direct marketing applications that 
seek to identify customers who are likely to respond to a direct mailing promotion. In these 
applications, it is common to have a fixed budget to mail only a fixed number of customers. 
Lift charts identify how much better a data-mining model does at identifying responders 
than a mailing to a random set of customers.
Prediction Accuracy
There are several ways to measure accuracy when estimating a continuous outcome vari-
able, but each of these measures is some function of the error in estimating an outcome 
for an observation i. Let ei be the error in estimating an outcome for observation i. Then 
ei 5 yi 2 y^i, where yi is the actual outcome for observation i, and y^i is the predicted out-
come for observation i. The measures provided as standard output from XLMiner are the 
average error 5 Sn
i51ei /n and the root mean squared error (RMSE) 5 "Sn
i51 e2
i /n. If 
the average error is negative, then the model tends to overpredict; if the average error is 
positive, the model tends to underpredict. The RMSE is similar to the standard error of the 
estimate for a regression model; it has the same units as the outcome variable predicted.
We note that applying these measures (or others) to the model’s predictions on the 
training set estimates the goodness-of-fit of the model, not the predictive accuracy. In 
 estimating future performance, we are most interested in applying the accuracy measures 
to the model’s predictions on the validation and test sets.
k-Nearest Neighbors
The k-nearest neighbors (k-NN) method can be used either to classify an outcome cat-
egory or predict a continuous outcome. To classify or predict an outcome of a new obser-
vation, k-NN uses the k most similar observations from the training set, where similarity is 
typically measured with Euclidean distance.
When k-NN is used as a classification method, a new observation is classified as Class 1 
if the percentage of its k nearest neighbors in Class 1 is greater than or equal to a speci-
fied cutoff value (the default value is 0.5 in XLMiner). When k-NN is used as a prediction 
method, a new observation’s outcome value is predicted to be the average of the outcome 
values of its k-nearest neighbors.
The value of k can plausibly range from 1 to n, the number of observations in the train-
ing set. If k 5 1, then the classification or prediction of a new observation is based solely 
on the single most similar observation from the training set. At the other extreme, if k 5 n, 
A decile is one of nine 
values that divide ordered 
data into ten equal parts. 
The deciles determine 
the values for 10%, 20%, 
30% . . . 90% of the data.
Chapter 5 discusses 
 additional accuracy 
measures such as mean 
 absolute error, mean abso-
lute percentage error, and 
mean squared error (which 
is the RMSE squared).
Lift charts analogous to 
those constructed for clas-
sification methods can also 
be applied to the continu-
ous outcomes treated by 
prediction methods. A lift 
chart for a continuous out-
come variable is relevant 
for evaluating a model’s 
effectiveness in identifying 
observations with the larg-
est values of the outcome 
variable. This is similar 
to the way a lift chart for 
a categorical outcome 
variable helps evaluate a 
model’s effectiveness in 
identifying observations 
that are most likely to be 
Class 1 members.

278 
Chapter 6 Data Mining
then the new observation’s class is naïvely assigned to the most common class in the train-
ing set, or analogously, the new observation’s prediction is set to the average outcome value 
over the entire training set. Typical values of k range from 1 to 20. The best value of k can 
be determined by building models over a typical range (k 5 1, . . . 20) and then selecting 
the k* that results in the smallest classification error. Note that the use of the validation set 
to identify k* in this manner implies that the method should be applied to a test set with this 
value of k to accurately estimate the anticipated error rate on future data.
Using XLMiner to classify with k-nearest neighbors XLMiner provides the capa-
bility to apply the k-nearest neighbors method for classifying a 0–1 categorical outcome. 
We apply this k-nearest neighbors method on the data partitioned with oversampling from 
Optiva to classify observations as either loan default (Class 1) or no default (Class 0). The 
following steps and Figure 6.14 demonstrate this process.
Step 1. Select any cell in the range of data in the Data_Partition1 worksheet
Step 2. Click the XLMINER tab in the Ribbon
Step 3. Click Classify in the Data Mining group
Step 4. Click k-Nearest Neighbors
Step 5. When the k-Nearest Neighbors Classification—Step 1 of 2 dialog box appears:
In the Data source area, confirm that the Worksheet: and Workbook: 
entries correspond to the appropriate data
In the Variables in input data box of the Variables area, select 
 AverageBalance, Age, Entrepreneur, Unemployed, Married, and 
Divorced, High School, and College variables, and click the $ but-
ton to populate the Input variables box
file
WEB
Optiva-Oversampled
FIGURE 6.14  XLMINER STEPS FOR k-NEAREST NEIGHBORS CLASSIFICATION
This tutorial demonstrates 
just a single pass through 
the procedure. The model-
ing process would typically 
involve rerunning the  
k-nearest neighbors method 
using different sets of input 
variables.

 
6.4 Supervised Learning 
279
In the Variables in input data box of the Variables area, select Loan-
Default, and click the . button to the left of the Output variable: box
In the Classes in the output variable area, select 1 from dropdown box 
next to Specify “Success” class (for Lift Chart):, and enter 0.5 in 
the Specify initial cutoff probability value for success box
Click Next .
Step 6. In the k-Nearest Neighbors Classification—Step 2 of 2 dialog box:
Select the checkbox for Normalize input data
Enter 20 in the Number of nearest neighbors (k): box
In the Scoring option area, select Score on best k between 1 and 
specified value
In the Score test data area, select the checkboxes for Detailed scoring 
and Lift charts
Click Finish
This procedure runs the k-nearest neighbors method for values of k ranging from 1 to 
20 on both the training set and validation set. The procedure generates a worksheet titled 
KNNC_Output1 that contains the overall error rate on the training set and validation set for 
In the Score new data area, 
XLMiner also provides the 
functionality to estimate the 
probabilities of Class 1  
membership and classify 
new observations for which 
the true classification is 
unknown.
FIGURE 6.15   CLASSIFICATION ERROR RATES FOR RANGE OF k VALUES FOR k-NEAREST  NEIGHBORS
A
B
C
D
E
F
G
H
I
J
1
2
3
4
5
6
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
XLMiner : k-Nearest Neighbors Classification
Validation error log for different k
Output Navigator
Inputs
Elapsed Time
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Database Score
New Score - Detailed Rep.
Validation error log
Prior Class Pr
Value of k
% Error
Training
% Error
Validation
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
0.26
23.53
23.15
28.13
27.62
32.61
30.56
31.33
32.48
33.25
33.38
35.17
34.91
36.32
36.57
37.47
36.83
36.70
37.21
40.03
42.48
64.51
44.03
58.38
45.54
58.90
47.52
57.78
47.87
57.75
48.48
57.15
49.19
57.61
49.16
56.90
50.25
57.32
50.56
58.56
Best k

280 
Chapter 6 Data Mining
various values of k. As Figure 6.15 shows, k 5 1 achieves the smallest overall error rate on 
the validation set. This suggests that Optiva classify a customer as “default or no default” 
based on the category of the most similar customer in the training set.
XLMiner applies k-nearest neighbors to the test set using the value of k that achieves 
the smallest overall error rate on the validation set (k 5 1 in this case). The KNNC_Out-
put1 worksheet contains the classification confusion matrices resulting from applying the 
k-nearest neighbors with k 5 1 to the training, validation, and test data. Figure 6.16 shows 
the classification confusion matrix for the test data and displays a Class 1 error rate of 44.87 
percent and a Class 0 error rate of 42.83 percent.
The error rate on the test data is more indicative of future accuracy than the error rates 
on the training data or validation data. The classification for all three sets (training, valida-
tion, and test) is based on the nearest neighbors in the training data, so the error rate on the 
training data is biased by using actual Class 1 observations rather than the estimated class 
of these observations. Furthermore, the error rate on the validation data is biased because it 
was used to identify the value of k that achieves the smallest overall error rate.
Using XLMiner to predict with k-nearest neighbors XLMiner provides the ca-
pability to apply the k-nearest neighbors method for prediction of a continuous outcome. 
We apply this k-nearest neighbors method to the file Optiva-Standard (which contains the 
standard-partitioned data from Optiva) to predict an observation’s average balance. The 
following steps and Figure 6.17 demonstrate this process.
Step 1. Select any cell on the Data_Partition1 worksheet
Step 2. Click the XLMINER tab in the Ribbon
XLMiner also produces 
a KNNC_TestLiftChart1 
worksheet that contains 
a cumulative lift chart 
and decile-wise lift chart 
to evaluate the k-nearest 
neighbor as well as a 
KNNC_TestScore1 
worksheet that shows the 
k-nearest neighbor classifi-
cation of each observation 
in the test set.
file
WEB
Optiva-Standard
FIGURE 6.16  CLASSIFICATION CONFUSION MATRIX FOR k-NEAREST NEIGHBORS
A
B
C
D
E
F
G
H
I
J
1
2
3
4
5
6
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
XLMiner : k-Nearest Neighbors Classification
Test Data scoring - Summary Report (for k=1)
Output Navigator
Classification Confusion Matrix
Predicted Class
Actual Class
Inputs
Elapsed Time
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Database Score
New Score - Detailed Rep.
Validation error log
Prior Class Pr
Cut off Prob. Val. for Success (Updatable)
(Updating the value here will NOT update value in detailed report)
0.5
Error Report
Class
Overall
# Cases
# Errors
% Error
42.87
1
86
3633
0
70
4849
1
0
1
0
8638
156
8482
3703
70
3633
44.87
42.83

 
6.4 Supervised Learning 
281
FIGURE 6.17  XLMINER STEPS FOR k-NEAREST NEIGHBORS PREDICTION
Step 3. Click Predict from the Data Mining group
Step 4. Click k-Nearest Neighbors
Step 5. When the k-Nearest Neighbors Prediction—Step 1 of 2 dialog box appears:
In the Data source area, confirm that the Worksheet:, and Workbook: 
entries correspond to the appropriate data
In the Variables in input data box of the Variables area, select Age, 
Entrepreneur, Unemployed, Married, Divorced, High School, 
and College variables, and click the $ button to populate the Input 
Variables box
Select AverageBalance in the Variables in input data box of the Variables 
area, and click the . button to the left of the Output variable: box
Click Next .
Step 6. In the k-Nearest Neighbors Prediction—Step 2 of 2 dialog box:
Select the checkbox for Normalize input data
Enter 20 in the Number of nearest neighbors (k): box
In the Scoring option area, select Score on best k between 1 and speci-
fied value
In the Score test data area, select the checkboxes for Detailed scoring 
and Lift charts
Click Finish
This procedure runs the k-nearest neighbors method for values of k ranging from 1 
to 20 on both the training set and validation set. The procedure generates a worksheet 
titled KNNP_Output1 that contains the root mean squared error on the training set and 
In the Score new data area, 
XLMiner also provides the 
functionality to predict the 
continuous outcomes for 
new observations for which 
the true outcome values are 
unknown.

282 
Chapter 6 Data Mining
validation set for various values of k. As Figure 6.18 shows, k 5 20 achieves the smallest 
root mean squared error on the validation set. This suggests that Optiva estimate a cus-
tomer’s average balance using the average balance of the 20 most similar customers in 
the training set.
XLMiner applies k-nearest neighbors to the test set using the value of k that 
achieves the smallest root mean squared error on the validation set (k 5 20 in this 
case). The KNNP_Output1 worksheet contains the root mean squared error and aver-
age error resulting from applying the k-nearest neighbors with k 5 20 to the training, 
validation, and test data. Figure 6.19 shows that the root mean squared error is con-
sistent across the training, validation, and test sets, so Optiva can expect that the root 
mean squared error will be approximately $4000 on new data. The average error of 
43.46 on the validation data suggests a slight tendency to underestimate the average 
balance in the validation data. The average error of 235.39 on the test data suggests 
a slight tendency to overestimate the average balance of observation in the test data. 
The difference in sign of these two average error estimates suggests that there is no 
systematic bias in the predictions.
XLMiner also generates a 
KNNP_TestScore1 work-
sheet that contains a listing 
of each test set observa-
tion’s predicted value and 
a KNNP_TestLiftChart1 
worksheet that contains the 
cumulative lift chart and 
decile-wise lift chart for the 
test set.
FIGURE 6.18  PREDICTION ERROR FOR RANGE OF k VALUES FOR k-NEAREST NEIGHBORS
A
B
C
D
E
F
G
H
I
J
1
2
3
4
5
6
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
XLMiner : k-Nearest Neighbors Prediction
Validation error log for different k
Output Navigator
Inputs
Elapsed Time
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Database Score
New Score - Detailed Rep.
Validation error log
Value of k
Training
RMS Error
Validation
RMS Error
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
3816.797434
4428.385507
4238.93618
4134.488978
4130.594147
4127.523779
4118.174511
4110.496342
4108.619817
4100.592738
4099.865693
4096.881914
4090.989157
4090.496585
4089.670732
4085.146428
4085.083246
4084.637422
4080.048361
4079.759744
4079.570785
Best k

 
6.4 Supervised Learning 
283
Classification and Regression Trees
Classification and regression trees (CART) successively partition a data set of observations 
into increasingly smaller and more homogeneous subsets. At each iteration of the CART 
method, a subset of observations is split into two new subsets based on the values of a single 
variable. The CART method can be thought of as a series of questions that successively 
narrow down observations into smaller and smaller groups of decreasing impurity. For 
classification trees, the impurity of a group of observations is based on the proportion of 
observations belonging to the same class (where the impurity 5 0 if all observations in a 
group are in the same class). For regression trees, the impurity of a group of observations 
is based on the variance of the outcome value for the observations in the group. After a final 
tree is constructed, the classification or prediction of a new observation is then based on the 
final partition to which the new observation belongs (based on the variable splitting rules).
To demonstrate CART, we consider an example involving Hawaiian Ham Inc. (HHI), 
a company that specializes in the development of software that filters out unwanted e-mail 
messages (often referred to as spam). HHI has collected data on 4601 e-mail messages. For 
each of these 4601 observations, the file HawaiianHam contains the following variables:
● 
The frequency of 48 different words (expressed as the percentage of words)
● 
The frequency of six characters (expressed as the percentage of characters)
file
WEB
HawaiianHam
FIGURE 6.19  PREDICTION ACCURACy FOR k-NEAREST NEIGHBORS
A
B
C
D
E
F
G
H
I
J
1
2
3
4
5
6
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
XLMiner : k-Nearest Neighbors Prediction
Training Data scoring - Summary Report (for k = 20)
Validation Data scoring - Summary Report (for k = 20)
Test Data scoring - Summary Report (for k = 20)
Output Navigator
Total sum
of squared
errors
RMS Error
Average
Error
Inputs
Elapsed Time
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Database Score
New Score - Detailed Rep.
Validation error log
1.45665E111 3816.797434 2.36024E-08
74
Total sum
of squared
errors
RMS Error
Average
Error
3.3146E111
4079.570785 43.46397713
Total sum
of squared
errors
RMS Error
Average
Error
2.01059E111
3891.304944 235.3929643

284 
Chapter 6 Data Mining
● 
The average length of the sequences of capital letters
● 
The longest sequence of capital letters
● 
The total number of sequences with capital letters
● 
Whether the e-mail was spam
HHI would like to use these variables to classify e-mail messages as either “spam” (Class 1) 
or “not spam” (Class 0).
Classifying a categorical outcome with a classification tree In this section, we 
use a small sample of data from HHI consisting of 46 observations and only two variables 
from HHI—percentage of the $ character (denoted Percent_$) and percentage of the ! 
character (Percent_!)—to explain how a classification tree categorizes observations. The 
results of a classification tree analysis can be graphically displayed in a tree that explains 
the process of classifying a new observation. The tree outlines the values of the variables 
that result in an observation falling into a particular partition.
Let us consider the classification tree in Figure 6.20. At each step, the CART method 
identifies the variable and the split of this variable that results in the least impurity in the 
two resulting categories. In Figure 6.20, the number within the circle (or node) represents 
the value on which the variable (whose name is listed below the node) is split. The first 
partition is formed by splitting observations into two groups, observations with Percent_$ 
, 0.056 and observations with Percent_$ . 0.056. The numbers on the left and right arc 
emanating from the node denote the number of observations in the Percent_$ , 0.056 and 
Percent_$ . 0.056 partitions, respectively. There are 28 observations containing less than 
0.056 percent of the character $ and 18 observations containing more than 0.056 percent 
FIGURE 6.20   CONSTRUCTION SEQUENCE OF BRANCHES IN A CLASSIFICATION TREE 
1
2
1
1
0
0
1
0
1
0
0.056
28
21
7
4
14
1
3
2
1
1
3
4
3
Percent_$
Percent_$
Percent_!
Percent_!
Percent_!
Percent_!
Percent_!
18
3
0.0615
5
0.1665
4
0.2665
6
0.099
2
0.0735
7
0.5605

 
6.4 Supervised Learning 
285
of the character $. The split on the variable Percent_$ at the value 0.056 is selected be-
cause it results in the two subsets of the original 46 observations with the least impurity. 
The splitting process is then repeated on these two newly created groups of observations 
in a manner that again results in an additional subset with the least impurity. In this tree, 
the second split is applied to the group of 28 observations with Percent_$ , 0.056 using 
the variable Percent_! which corresponds to the proportion of characters in an e-mail that 
are a !; 21 of the 28 observations in this subset have Percent_! , 0.0735, whereas 7 have 
Percent_! . 0.0735. After this second variable splitting, there are three total partitions of 
the original 46 observations. There are 21 observations with values of Percent_$ , 0.056 
and Percent_! , 0.0735, 7 observations with values of Percent_$ , 0.056 and Percent_! 
. 0.0735, and 18 observations with values of Percent_$ . 0.056. No further partitioning 
of the 21-observation group with values of Percent_$ , 0.056, and Percent_! , 0.0735 is 
necessary because this group consists entirely of Class 0 (nonspam) observations; that is, 
this group has zero impurity. The 7-observation group with values of Percent_$ , 0.056 
and Percent_! . 0.0735 and the 18-observation group with values of Percent_$ . 0.056 
are successively partitioned in the order as denoted by the boxed numbers in Figure 6.20 
until obtaining subsets with zero impurity.
For example, the group of 18 observations with Percent_$ . 0.056 is further split 
into two groups using the variable Percent_!; 4 of the 18 observations in this subset have 
Percent_! , 0.0615, wherease 14 have Percent_! . 0.0615. In other words, 4 observations 
have Percent_$ . 0.056 and Percent_! , 0.0615. This subset of 4 observations is further 
decomposed into 1 observation with Percent_$ , 0.1665 and and 3 observations with 
Percent_$ . 0.1665. At this point there is no further branching in this portion of the tree 
because corresponding subsets have zero impurity. In other words, the subset of 1 observa-
tion with 0.056 , Percent_$ , 0.1665 and Percent_! , 0.0615 is a Class 0 observation 
(nonspam), and the subset of 3 observations with Percent_$ . 0.1665 and Percent_! , 
0.0615 are Class 1 observations (spam). The recursive partitioning for the other branches 
in Figure 6.20 follows similar logic. Figure 6.21 illustrates the partitioning resulting from 
the sequence of variable splits. The rules defining a partition divide the variable space into 
rectangles.
As Figure 6.21 suggests, with enough variable splitting, it is possible to obtain parti-
tions on the training set such that each partition either contains Class 1 observations or 
Class 0 observations, but not both. In other words, enough decomposition results in a set 
of partitions with zero impurity, and there are no misclassifications of the training set by 
this full tree. However, this degree of partitioning is an example of extreme overfitting. 
Applying the same partitioning rules to the data in a validation set will typically result in 
a relatively large classification error. Thus, there is a need to prune the classification tree 
by removing the branches corresponding to weaker rules. Specifically, a series of pruned 
trees is obtained by removing partitions in the reverse order in which they were added. 
For example, sequentially pruning the full tree in Figure 6.20 would result in trees with 
one to six variable splits (decision nodes). Figure 6.22 illustrates the tree resulting from 
pruning the last variable splitting rule (Percent_! , 0.5605 or Percent_! . 0.5605) from 
Figure 6.20. By pruning this rule, we obtain a partition defined by Percent_$ , 0.056, 
Percent_! . 0.0735, and Percent_! . 0.2665 that contains three observations. Two of 
these observations are Class 1 (spam) and one is Class 0 (nonspam). As Figure 6.22 
shows, this pruned tree classifies observations in the partition defined by Percent_$ , 
0.056, Percent_! . 0.0735, and Percent_! . 0.2665 as Class 1 observations because 
the proportion of Class 1 observations in this partition—which we know is (2/3) from 
Figure 6.20—exceeds the default cutoff value of 0.5. Therefore, the classification error 
of this pruned tree on the training set is no longer zero. However, this pruned tree is less 
likely to be overfit to the training set and may classify the validation set more accurately 
than the full tree that perfectly classifies the training set.

286 
Chapter 6 Data Mining
FIGURE 6.21   GEOMETRIC ILLUSTRATION OF CLASSIFICATION TREE PARTITIONS
0.05
20.05
0.10
20.10
0.30
0.50
0.70
0.90
1.10
1.30
0.15
0.35
0.25
0.45
0.55
Percent_!
Percent_$
Spam
Not Spam
FIGURE 6.22  CLASSIFICATION TREE WITH ONE PRUNED BRANCH
2
1
3
5
4
6
2
1
1
0
1
0
1
0
0.056
28
21
7
4
14
1
3
1
3
4
3
Percent_$
Percent_$
Percent_!
Percent_!
Percent_!
Percent_!
18
0.0615
0.1665
0.2665
0.099
0.0735

 
6.4 Supervised Learning 
287
Pruned classification trees with as few as one decision node can be obtained by removing 
the variable splitting rules in the reverse order they were grown in the full tree of Figure 6.20. 
For each of these pruned trees, the classification of the observations in the partition affected 
by the pruned rule are classified as Class 1 if the proportion of Class 1 observations in the 
partition exceeds the cutoff value (default value is 0.5) and Class 0 otherwise. Table 6.7 
shows that the classification error on the training set decreases as more decision nodes 
splitting the observations into smaller partitions are added. However, while adding deci-
sion nodes at first decreases the classification error on the validation set, too many decision 
nodes overfits the classification tree to the training data and results in increased error on the 
validation set. Table 6.7 suggests that a classification tree partitioning observations into two 
subsets with a single rule (Percent_! , 0.5605 or Percent_! . 0.5605) is just as reliable at 
classifying the validation data as any other tree. As Figure 6.23 shows, this classification tree 
classifies  e-mails with ! accounting for less than 0.5605 percent of the characters as nonspam 
and e-mails with ! accounting for more than 0.5605 percent of the characters as spam, which 
results in a classification error of 20.9 percent on the validation set.
Using XLMiner to construct classification trees Using the XLMiner’s Standard 
 Partition procedure, we randomly partition the 4601 observations in the file  HawaiianHam 
so that 50 percent of the observations create a training set of 2300 observations, 30 per-
cent of the observations create a validation set of 1380 observations, and 20 percent of the 
observations create a test set of 921 observations. We apply the following steps (shown in 
Figure 6.24) to conduct a classification tree analysis on these data partitions.
Step 1. Select any cell in the range of data in the Data_Partition1 worksheet
Step 2. Click the XLMINER tab in the Ribbon
file
WEB
HawaiianHam-Standard
TABLE 6.7   CLASSIFICATION ERROR RATES ON SEQUENCE OF PRUNED 
TREES
Number of  
Decision Nodes
Percent Classification 
Error on Training Set
Percent Classification  
Error on Validation Set
0
1
2
3
4
5
6
7
46.5
8.7
8.7
8.7
6.5
4.4
2.17
0
39.4
20.9
20.9
20.9
20.9
21.3
21.3
21.6
FIGURE 6.23  BEST PRUNED CLASSIFICATION TREE
1
0
3452
1103
Percent_$
0.056

288 
Chapter 6 Data Mining
Step 3. Click Classify in the Data Mining group
Step 4. Click Classification Tree
Step 5. When the Classification Tree—Step 1 of 3 dialog box appears:
In the Data source area, confirm that the Worksheet: and Workbook: 
entries correspond to the appropriate data
In the Variables in input data box of the Variables area,  select 
 Percent_;, 
Percent_(, 
Percent_[, 
Percent_!, 
Percent_$, 
 Percent_%, AvgLengthAllCap, LongAllCap, and TotalAllCap, 
and click the $ button to populate the Input Variables box
Select Spam in the Variables in input data box of the Variables area, 
and click the . button to the left of the Output variable: box
In the Classes in the output variable area, select 1 from dropdown box 
next to Specify “Success” class (for Lift Chart): and enter 0.5 in the 
Specify initial cutoff probability value for success: box
Click Next .
Step 6. In the Classification Tree—Step 2 of 3 dialog box:
Select the checkbox for Normalize input data
Select the checkbox for Minimum #records in a terminal node, and 
enter 230 in the box
In the Prune tree using the validation data set area, select the check-
box for Prune tree
Click Next .
FIGURE 6.24  XLMINER STEPS FOR CLASSIFICATION TREES
XLMiner’s 
default value 
for the minimum 
number of 
observations in 
a terminal node 
is 10 percent of 
the training set. 
Setting this to 
a lower value 
may extend the 
run time of the 
classification 
tree procedure 
but may obtain a 
best pruned tree 
with improved 
classification 
error on the 
validation and 
test sets.

 
6.4 Supervised Learning 
289
Step 7. In the Classification Tree—Step 3 of 3 dialog box:
In the Trees area, increase the Maximum # levels to be displayed: box to 7
In the Trees area, select the checkboxes for Full tree (grown using 
training data) and Best pruned tree (pruned using validation data)
In the Score test data area, select the checkboxes for Detailed report 
and Lift charts
Click Finish
This procedure first constructs a full classification tree on the training data, that 
is, a tree that is successively partitioned by variable splitting rules until the resultant 
branches contain less than the minimum number of observations (230 observations 
in this example). Figure  6.25 displays this full tree which XLMiner provides in a 
In the Score new data area, 
XLMiner also provides the 
functionality to estimate 
the probabilities of Class 1 
membership and classify 
new observations for which 
the true classification is 
unknown.
FIGURE 6.25  FULL CLASSIFICATION TREE FOR HAWAIIAN HAM
0.0065
2.5875
3.55
0.0295
1224
563
513
Percent_!
Percent_$
1076
9.5
1.4185
0.116
576
648
LongestAllCaps
316
260
AvgLengthAllCaps
0
0
0
0
0
1
1
1
231
282
AvgLengthAllCaps
329
234
 AvgLengthAllCaps
285
363
Percent_(
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
XLMiner : Classification Tree – Full Tree (Using Training Data)
Back to Navigator
Place the cursor on a Decision Node to read the decision rule
Circle denotes a DECISION NODE
Rectangle denotes a TERMINAL NODE
2
1
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35

290 
Chapter 6 Data Mining
worksheet titled CT_FullTree1. This full tree is sequentially pruned to varying degrees 
to investigate overfitting the training data. XLMiner applies classification trees with 
between seven decision nodes (no pruning) and zero decision nodes (complete prun-
ing) to the validation set and provides the results in a worksheet titled CT_PruneLog1. 
Figure 6.26 displays the content of the CT_PruneLog1 worksheet, which indicates that 
the minimum classification error on the validation set is achieved by a three-decision 
node tree.
We note that in addition to a minimum error tree—which is the classification tree that 
achieves the minimum error on the validation set—XLMiner refers to a best pruned tree 
(see Figure 6.26). The best pruned tree is the smallest classification tree with a classifica-
tion error within one standard error of the classification error of the minimum error tree. 
By using the standard error in this manner, the best pruned tree accounts for any sampling 
error (the validation set is just a sample of the overall population). The best pruned tree will 
always be the same size or smaller than the minimum error tree.
The worksheet CT_PruneTree1 contains the best pruned tree as displayed in 
 Figure 6.27. Figure 6.27 illustrates that the best pruned tree uses the variables Percent_!, 
Percent_$, and AvgAllCaps to classify an observation as spam or not. To see how the 
best pruned tree classifies an observation, consider the classification of the test data in 
the CT_TestScore1 worksheet (Figure 6.28). Observation 2 (i.e., observation with Row 
id. 5 2) has values of:
Percent_;
Percent_(
Percent_[
Percent_!
Percent_$
Percent_%
AvgAllCap
LongAllCap
TotalAllCap
0
0.132
0
0.372
0.18
0.048
5.114
101
1028
Applying the first decision rule in the best pruned tree, we see that Observation 2 
falls into the category Percent_! . 0.0295. The next rule applies to Percent_$, and 
we see that Observation 2 falls into the category Percent_$ . 0.0065. There is no 
further partitioning, and because the proportion of observations in the training set with 
FIGURE 6.26  PRUNING LOG FOR CLASSIFICATION TREE
A
7
6
5
4
3
2
0
1
B
C
D
E
F
G
H
XLMiner : Classification Tree - Prune Log (Using Validation Data)
# Decision
Nodes
16.014493
16.014493
16.014493
16.014493
16.014493
22.028986
<-- Min. Err. & Best Pruned Tree
Std. Err.
0.00987232
40.652174
22.028986
% Error
Back to Navigator
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

 
6.4 Supervised Learning 
291
FIGURE 6.27  BEST PRUNED CLASSIFICATION TREE FOR HAWAIIAN HAM
0.0295
0.0065
719
661
0
1
1
0
345
316
Percent_$
Percent_!
197
148
AvgLengthAllCaps
2.5875
A
B
C
D
E
F
G
H
I
J
K
XLMiner: Classiﬁcation Tree - Best Pruned Tree (Using Validation Data)
Back to Navigator
Place the cursor on a Decision Node to read the decision rule
Circle denotes a DECISION NODE
Rectangle denotes a TERMINAL NODE
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Percent_! . 0.0295 and Percent_$ . 0.0065 exceeds the cutoff value of 0.5, the best 
pruned tree classifies Observation 2 as Class 1 (spam). As Figure 6.28 shows, this is 
indeed the correct classification for Observation 2. The overall classification accuracy 
for the best pruned tree on the test set can be found on the CT_Output1 worksheet as 
shown in Figure 6.29.
Predicting a continuous outcome with a regression tree A regression tree suc-
cessively partitions observations of the training set into smaller and smaller groups in a 
similar fashion to that of a classification tree. The only differences are how impurity of 
the parititions are measured and how a partition is used to estimate the outcome value of 
XLMiner also produces a 
worksheet titled CT_  
TestLiftChart1 that 
 contains a cumulative lift 
chart and decile-wise lift 
chart to evaluate a clas-
sification tree.

292 
Chapter 6 Data Mining
FIGURE 6.28  BEST PRUNED TREE CLASSIFICATION OF TEST DATA FOR HAWAIIAN HAM
Data range
0.5
Cut off Prob. Val. for Success (Updatable)
(Updating the value here will NOT update value in summary report)
Row Id.
2
7
8
13
22
24
25
28
33
40
46
59
61
70
79
80
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0.927874982
0.927874982
0.125817001
0.709402025
0.927874982
0.291792989
0.927874982
0.709402025
0.125817001
0.709402025
0.927874982
0.709402025
0.125817001
0.125817001
0.291792989
0.125817001
0.00
0.00
0.00
0.00
0.04
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.13
0.05
0.21
0.06
0.10
0.13
0.20
0.00
0.05
0.07
0.25
0.00
0.26
0.20
0.00
0.00
1
1
0
1
1
0
1
1
0
1
1
1
0
0
0
0
Back to Navigator
Predicted
Class
Actual
Class
Prob. for 1
(success)
AvgLength
AllCaps
Longest
AllCaps
Total
AllCaps
Percent_; Percent_( Percent_[
C
E
D
F
G
H
I
J
K
XLMiner : Classification Tree - Classification of Test Data (Using Best Pruned Tree)
1
3
5
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
L
M
N
O
0.37
0.16
0.00
0.79
0.25
0.67
0.39
0.37
0.00
0.75
1.32
1.50
0.00
0.00
0.13
0.00
0.05
0.00
0.00
0.00
0.06
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.18
0.05
0.00
0.00
0.05
0.00
0.20
0.00
0.09
0.00
0.07
0.00
0.00
0.10
0.00
0.34
Percent_! Percent_$ Percent_%
5.11
1.67
2.45
3.73
2.57
1.13
5.47
2.61
1.39
3.85
5.30
2.78
8.59
4.55
1.65
1.67
1028
112
49
261
2259
69
82
47
89
285
774
200
249
141
175
10
101
4
11
61
66
5
22
12
11
121
130
61
66
59
15
5
B
FIGURE 6.29  BEST PRUNED TREE’S CLASSIFICATION CONFUSION MATRIX ON TEST DATA
A
B
C
D
E
F
G
H
I
J
K
1
3
4
5
6
7
8
132
133
134
136
137
138
139
140
141
142
143
144
145
146
XLMiner : Classification Tree
Output Navigator
Inputs
Elapsed Time
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Database Score
New Score - Detailed Rep.
Prior Class Pr
Train Log
Full Tree Rules
Full Tree
Best Pruned Tree Rules
Best Pruned Tree
Minimum Error Tree Rules
Minimum Error Tree
User Specified Tree Rules
User Specified Tree
Prune Log
1
0
360
561
Cut off Prob. Val. for Success (Updatable)
(Updating the value here will NOT update value in detailed report)
0.5
Test Data scoring - Summary Report (Using Best Pruned Tree)
Classification Confusion Matrix
Predicted Class
Actual Class
Error Report
# Cases
# Errors
% Error
Class
1
0
Overall
1
251
42
0
109
519
921
109
42
151
30.28
7.49
16.40

 
6.4 Supervised Learning 
293
an observation lying in that partition. Instead of measuring impurity of a partition based 
on the proportion of observations in the same class as in a classification tree, a regression 
tree bases the impurity of a partition based on the variance of the outcome value for the 
observations in the group. After a final tree is constructed, the predicted outcome value 
of an observation is based on the mean outcome value of the partition into which the new 
observation belongs.
Using XLMiner to construct regression trees XLMiner provides the capability to ap-
ply a regression tree to predict a continuous outcome. We use the standard-partitioned data 
from the Optiva Credit Union problem to predict a customer’s average checking account 
balance. The following steps and Figure 6.30 demonstrate this process.
Step 1. Select any cell in the range of data in the Data_Partition1 worksheet
Step 2. Click the XLMINER tab in the Ribbon
Step 3. Click Predict in the Data Mining group
Step 4. Click Regression Tree
file
WEB
Optiva-Standard
FIGURE 6.30  XLMINER STEPS FOR REGRESSION TREES

294 
Chapter 6 Data Mining
Step 5. When the Regression Tree—Step 1 of 3 dialog box appears:
In the Data source area, confirm that the Worksheet: and Workbook: 
entries correspond to the appropriate data
In the Variables in input data box of the Variables area, select Age, 
Entrepreneur, Unemployed, Married, Divorced, High School, 
and College variable, and click the $ button to populate the Input 
Variables box
Select AverageBalance in the Variables in input data box of the 
Variables area, and click the . button to the left of the Output 
variable: box
Click Next .
Step 6. In the Regression Tree—Step 2 of 3 dialog box:
Select the checkbox for Normalize input data
Enter 100 in the box next to Maximum #splits for input variables
Enter 1000 in the box next to Maximum #observations in a terminal 
node
In the Scoring option area, select Using Best prune tree
Click Next .
Step 7. In the Regression Tree—Step 3 of 3 dialog box:
Increase the Maximum # levels to be displayed: box to 7
In the Trees area, select the checkboxes for Full tree (grown using 
training data) and Pruned tree (pruned using validation data)
In the Score test data area, select the checkboxes for Detailed report 
and Lift charts
Click Finish
This procedure first constructs a full regression tree on the training data, that is, a tree 
that is successively partitioned by variable splitting rules until the resultant branches con-
tain less than the specified minimum number of observations (1000 observations in this 
example). In this full tree (Figure 6.31), the number within the node represents the value 
on which the variable (whose name is listed below the node) is split. The first partition 
is formed by splitting observations into two groups, observations with Age , 55.5 and 
observations with Age . 55.5. The numbers on the left and right arc emanating from the 
node denote the number of observations in the Age , 55.5 and Age . 55.5 partitions, 
respectively. There are 8944 observations in which the customer is younger than 55.5 
years and 1055 observations in which the customer is older than 55.5 years. The group of 
observations with Age , 55.5 is further partitioned as shown in Figure 6.31. The group of 
observations with Age . 55.5 is not partitioned any further (because further partitioning 
would result in fewer than 1000 observations), and the regression tree estimates the aver-
age balance for observations in this partition as the mean of the average balance values for 
the observations in this partition, $3054.91.
To guard against overfitting, the full regression tree is pruned to varying degrees and 
tested on the validation set. In this case, regression trees with between six decision nodes 
(no pruning) and zero decision nodes (complete pruning) are applied to the validation set. 
As Figure 6.32 indicates, the minimum error on the validation set (as measured by the sum 
of squared error between the regression tree predictions and actual observation values) is 
achieved by a five-decision node tree. Figure 6.33 shows this tree, which differs from the 
full regression tree in Figure 6.31 by only one decision node.
XLMiner’s default value 
for the maximum number 
of splits for input variables 
is 100. XLMiner’s default 
value for the minimum 
number of observations in 
a terminal node is 1000. 
Increasing the  maximum 
number of splits or 
decreasing the mininum 
number of observations in 
a terminal node extends the 
run time of the regression 
tree procedure but may 
obtain a best pruned tree 
with improved error on the 
validation and test sets.

 
6.4 Supervised Learning 
295
FIGURE 6.31  FULL REGRESSION TREE FOR OPTIVA CREDIT UNION
55.5
0.5
3054.910142
8944
1055
Age
College
6102
2842
38.5
30.5
43.5
3123
2979
Age
1069
2054
Age
1154.469972
1367.054625
2025.926406
2502.112189
37.5
1636
1206
Age
1016
1963
Age
1562.496555
1697.734641
A
B
C
D
E
F
G
H
I
J
K
XLMiner : Regression Tree - Full Tree (Using Training Data)
Back to Navigator
Place the cursor on a Decision Node to read the decision rule
Circle denotes a DECISION NODE
Rectangle denotes a TERMINAL NODE
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
37
38
39
40
41
42
43
44
31
32
33
34
35
36

296 
Chapter 6 Data Mining
FIGURE 6.32  REGRESSION TREE PRUNING LOG
# Decision
Nodes
Cost
Complexity
Training
RSS
Validation
RSS
A
C
D
E
F
G
H
I
1
2
3
4
5
6
7
8
9
10
11
12
13
14
J
B
3853.318505
6
5
4
3
2
1
0
0
12244513.49
31773860.03
157419810
194668094.2
1117759497
1706943689
1.7272E111
1.72732E111
1.72764E111
1.72922E111
1.73116E111
1.74234E111
1.75941E111
2.95714E111
2.95714E111
2.95771E111
2.96152E111
2.96628E111
2.9899E111
2.99743E111
XLMiner : Regression Tree - Prune Log (Using Validation Data)
Back to Navigator
<-- Min. Err. & Best Pruned Tree
Std. Err.
We note that XLMiner refers to both a minimum error tree and a best pruned tree in 
Figure 6.32. The meanings of these regression trees are the same as described for classifi-
cation trees.
To see how the best pruned tree (Figure 6.33) predicts an outcome for an observation, 
consider the classification of the test data in the CT_TestScore1 worksheet (Figure 6.34). 
Observation 2 (i.e., the observation with Row id. 5 2) has values of:
Age
Entrepreneur
Unemployed
Married
Divorced
High School
College
46
0
0
1
0
0
0
Applying the first decision rule in the best pruned tree, we see that Observation 2 falls into 
the Age , 55.5. The next rule applies to the College variable, and we see that Observation 2 
falls into the College , 0.5. The next rule places Observation 2 in the Age . 38.5 partition. 
There is no further partitioning, and since the mean observation value of average balance 
for observations in the training set with Age , 55.5, College , 0.5, and Age . 38.5 is 
$1651.61, the best pruned tree predicts Observation 2’s average balance as $1651.61. As 
Figure 6.34 shows, Observation 2’s actual average balance is $358.80, resulting in an error 
of 2$1292.81.
The RT_Output1 worksheet (Figure 6.35) provides the prediction error of the 
best pruned tree on the training, validation, and test sets. Specifically, the root mean 
squared (RMS) errors of the best pruned tree on the validation set and test set are 
3853.74 and 3729.95, respectively. Using this best pruned tree that characterizes cus-
tomers based only on age and whether they attended colleage, Optiva can expect that 
the root mean squared error will be approximately $3800 when estimating the average 
balance of new customer data.
Reducing the minimum 
number of records required 
for a terminal node in 
XLMiner’s Regression Tree 
procedure may result in 
more accurate predictions 
at the expense of increased 
time to construct the tree.

 
6.4 Supervised Learning 
297
FIGURE 6.33  BEST PRUNED REGRESSION TREE FOR OPTIVA CREDIT UNION
Rectangle denotes a TERMINAL NODE
55.5
0.5
Age
College
Age
2143
17773
12250
5523
2371
3152
38.5
30.5
1154.4699
72
1367.0546
25
1651.6111
45
3054.9101
42
Age
Age
5796
6454
2127
4327
A
C
D
E
F
G
H
I
J
XLMiner : Regression Tree - Pruned Tree (Using Validation Data)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
46
45
K
B
Back to Navigator
Place the cursor on a Decision Node to read the decision rule
Circle denotes a DECISION NODE
37.5
2025.9264
06
2502.1121
89

298 
Chapter 6 Data Mining
FIGURE 6.34   BEST PRUNED TREE PREDICTION OF TEST DATA FOR OPTIVA CREDIT UNION
Row Id.
2
3
7
8
13
14
16
22
24
25
28
33
34
1651.61115
1651.61115
1367.05463
1651.61115
1367.05463
3054.91014
1651.61115
1651.61115
1651.61115
3054.91014
1367.05463
1651.61115
1651.61115
358.8
5291
176.8
52
12439.7
232.7
19.5
397.8
40.3
1.3
10.4
408.2
3.9
46
41
35
46
34
56
43
41
46
57
35
43
48
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
0
0
1
1
1
0
0
1
1
0
1
0
0
1
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
21647.71115
21292.81115
3639.38886
21190.25463
21599.61115
11072.6454
22822.21014
21632.11115
21253.81115
21611.31115
23053.61014
21356.65463
21243.41115
Predicted
Value
Actual
Value
Residual
Age
Entrepreneur Unemployed Married
Divorced
College
High
School
A
C
D
E
F
G
H
I
K
XLMiner : Regression Tree - Prediction of Test Data (Using Prune Tree)
1
3
5
6
7
8
9
10
11
12
13
14
15
16
17
18
J
M
L
B
Data range
Back to Navigator
FIGURE 6.35  PREDICTION ERROR OF REGRESSION TREES
Inputs
Elapsed Time
Prune Log
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Full Tree Rules
Full Tree
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Best Pruned Tree Rules
Best Pruned Tree
Minimum Error Tree
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Minimum Error Tree Rules
Database Score
New Score - Detailed Rep.
User Specified Tree
User Specified Tree Rules
RMS Error
Total sum of
squared
errors
Average
Error
5.90065E209
4156.169367
1.7272E111
A
C
D
E
F
G
H
I
J
XLMiner : Regression Tree
89
1
2
3
4
5
6
7
8
78
79
80
81
84
85
86
87
88
90
91
92
93
B
RMS Error
Total sum of
squared
errors
Average
Error
9.305103667
3853.737973
2.95778E111
RMS Error
Total sum of
squared
errors
Average
Error
247.80795004
3729.950705
1.84731E111
Output Navigator
Training Data scoring - Summary Report (Using Full Tree)
Validation Data scoring - Summary Report (Using Prune Tree)
Test Data scoring - Summary Report (Using Prune Tree)

 
6.4 Supervised Learning 
299
Logistic Regression
Similar to how multiple linear regression predicts a continuous outcome variable, y, with 
a collection of explanatory variables, x1, x2, . . . , xq, via the linear equation y^ 5 b0 1 b1x1 
1 ∙ ∙ ∙ 1 bqxq, logistic regression attempts to classify a categorical outcome (y 5 0 or 1) as 
a linear function of explanatory variables. However, directly trying to explain a categorical 
outcome via a linear function of the explanatory variables is not effective. To understand 
this, consider the task of predicting whether a movie wins the Best Picture Oscar Award. 
for Best Picture using information on the total number of Oscar nominations that a movie 
receives. Figure 6.36 shows a scatter chart of a sample of movie data found in the file 
Oscars-Small; each data point corresponds to the the total number of Oscar nominations 
that a movie received and whether the movie won the best picture award (1 5 movie won, 0 
5 movie lost). The line on Figure 6.36 corresponds to the simple linear regression fit. This 
linear function can be thought of as predicting the probability p of a movie winning the Best 
Picture Oscar Award via the equation p^ 5 20.4054 1 (0.0836 3 total number of Oscar 
nominations). As Figure 6.36 shows, a linear regression model fails to appropriately explain 
a categorical outcome variable. For fewer than five total Oscar nominations, this model 
predicts a negative probability of winning the best picture award. For more than 17 total 
Oscar nominations, this model would predict a probability greater than 1.0 of winning the 
best picture award. The residual plot in Figure 6.37 shows an unmistakable pattern of sys-
tematic misprediction, suggesting that the simple linear regression model is not appropriate.
Estimating the probability p with the linear function p^ 5 b0 1 b1x1 1 ∙ ∙ ∙ 1 bq xq does not 
fit well because, although p is a continuous measure, it is restricted to the range [0, 1]; that is, 
a probability cannot be less than zero or larger than one. Figure 6.38 shows an S-shaped curve 
that appears to better explain the relationship between the probability p of winning best picture 
and the total number of Oscar nominations. Instead of extending off to positive and negative 
infinity, the S-shaped curve flattens and never goes above one or below zero. We can achieve 
file
WEB
Oscars-Small
As Chapter 4 discusses, if a 
linear regression model is 
appropriate, the residuals 
should appear randomly 
dispersed with no discern-
ible pattern. 
FIGURE 6.36   SCATTER CHART AND SIMPLE LINEAR REGRESSION FIT FOR 
OSCARS EXAMPLE
20.4
20.2
0
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
14
12
Total Number of Oscar Nominations
Winner of Best Picture
y 5 0.0836x 2 0.4054
R2 5 0.2708

300 
Chapter 6 Data Mining
FIGURE 6.37   RESIDUALS FOR SIMPLE LINEAR REGRESSION ON OSCARS DATA
20.8
20.6
20.4
20.2
0
0.2
0.4
0.6
0.8
1.2
1.0
0
5
10
15
Oscar Nominations
Residuals
FIGURE 6.38   LOGISTIC S-CURVE IN OSCARS EXAMPLE
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
1.0
0.9
0
2
4
6
8
10
12
14
Total Number of Oscar Nominations
Winner of Best Picture
this S-shaped curve by estimating an appropriate function of the probability p of winning best 
picture with a linear function rather than directly estimating p with a linear function.
As a first step, we note that there is a measure related to probability known as odds, which 
is very prominent in gambling and epidemiology. If an estimate of the probability of an event 
is p^, then the equivalent odds measure is p^ / (1 2 p^). For example, if the probability of an 
event is p^ 5 0.5, then the odds measure would be p^/(1 2 p^) 5 2, meaning that the odds are 2 to 

 
6.4 Supervised Learning 
301
1 that the event will occur. The odds metric ranges between zero and positive infinity, so, by 
considering the odds measure rather than the probability p^, we eliminate the linear fit problem 
resulting from the upper bound on the probability p^. To eliminate the fit problem resulting 
from the remaining lower bound on p^ /(1 2 p^), we observe that the log odds, or logit, of an 
event, ln (p^/(1 2 p^)), ranges from negative infinity to positive infinity. Estimating the logit 
with a linear function results in the estimated logistic regression equation:
ESTIMATED LOGISTIC REGRESSION EQUATION
lna
p^
1 2 p^b 5 b0 1 b1x1 1 c1 bqxq 
(6.1)
Given a set of explanatory variables, a logistic regression algorithm determines values of 
b0, b1, . . . , bq that best estimate the log odds. Applying a logistic regression algorithm to 
the data in the file Oscars-Small results in estimates of b0 5 26.214 and b1 5 0.596; that 
is, the log odds of a movie winning the best picture award is given by
 
ln a
p
1 2 p^b 5 26.214 1 0.596 3 Total Number Of Oscar Nominations 
(6.2)
Unlike the coefficients in a multiple linear regression, the coefficients in a logistic regres-
sion do not have an intuitive interpretation. For example, b1 5 0.596 means that for every ad-
ditional Oscar nomination that a movie receives, its log odds of winning the best picture award 
increase by 0.596. In other words, the total number of Oscar nominations is linearly related 
to the log odds of winning the best picture award. Unfortunately, a change in the log odds of 
an event is not as easy as to interpret as a change in the probability of an event. Algebraically 
solving equation (6.1) for p^, we can express the relationship between the estimated probability 
of an event and the explanatory variables with an equation known as the logistic function:
LOGISTIC FUNCTION
p^ 5
1
1 1 e2(b01b1x11c1bqxq) 
(6.3)
For the Oscars-Small data, equation (6.3) is
 
p^ 5
1
1 1 e2(26.21410.5963Total Number Of Oscar Nominations) 
(6.4)
Plotting equation (6.4), we obtain the S-shaped curve in Figure 6.38. Clearly, the logistic 
regression fit implies a nonlinear relationship between the probability of winning the best 
picture award and the total number of Oscar nominations. The effect of increasing the 
total number of Oscar nominations on the probability of winning the best picture award 
depends on the original number of Oscar nominations. For instance, if the total number of 
Oscar nominations is four, an additional Oscar nomination increases the estimated prob-
ability of winning the best picture award from p^ 5 1/(1 1 e2(26.214 1 0.596 3 4)) 5 0.021 to  
p^ 5 1/(1 1 e2(26.214 1 0.596 3 5)) 5 0.038, an absolute increase of 0.017. But if the total num-
ber of Oscar nominations is eight, an additional Oscar nomination increases the estimated 
probability of winning the best picture award from p^ 5 1/(1 1 e2(26.214 1 0.596 3 8)) 5 0.191 
to p^ 5 1/(1 1 e2(26.214 1 0.596 3 9)) 5 0.299, an absolute increase of 0.108.
As with other classification methods, logistic regression classifies an observation by 
 using equation (6.3) to compute the probability of a new observation belonging to Class 1 
and then comparing this probability to a cutoff value. If the probability exceeds the cutoff 
value (default value of 0.5), the observation is classified as a Class 1 member. Table 6.8 

302 
Chapter 6 Data Mining
shows a subsample of the predicted probabilities computed via equation (6.3) and subse-
quent classification for this small subsample of movies.
The selection of variables to consider for a logistic regression model is similar to the 
approach in multiple linear regression. Especially when dealing with many variables, thor-
ough data exploration via descriptive statistics and data visualization is essential in narrow-
ing down viable candidates for explanatory variables. As with multiple linear regression, 
strong collinearity between any of the explanatory variables x1, x2, . . . , xq, can distort the 
estimation of the coefficients b0, b1, . . . , bq in equation (6.1). Therefore, the identification 
of pairs of explanatory variables that exhibit large amounts of dependence can assist the 
analyst in culling the set of variables to consider in the logistic regression model.
Using XLMiner to construct logistic regression models We demonstrate how 
XLMiner facilitates the construction of a logistic regression model by using the Optiva 
Credit Union problem of classifying customer observations as either a loan default (Class 1)  
or no default (Class 0). The following steps and Figure 6.39 demonstrate this process.
Step 1. Select any cell on the Data_Partition1 worksheet
Step 2. Click the XLMINER tab in the Ribbon
Step 3. Click Classify in the Data Mining group
Step 4. Click Logistic Regression
Step 5. When the Logistic Regression—Step 1 of 3 dialog box appears:
In the Data source area, confirm that the Worksheet: and Workbook: 
entries correspond to the appropriate data
In the Variables in input data box of the Variables area, select 
 AverageBalance, Age, Entrepreneur, Unemployed, Married, 
 Divorced, High School, and College, and click the $ button to 
 populate the Input Variables box
Select LoanDefault in the Variables in input data box of the Variables 
area, and click the . button to the left of the Output variable: box
In the Classes in the output variable area, select 1 from the dropdown 
box next to Specify “Success” class (necessary): and enter 0.5 in the 
Specify initial cutoff probability value for success: box
Click Next .
Step 6. In the Logistic Regression—Step 2 of 3 dialog box:
Click the Best subset . . . button. When the Best Subset dialog box 
appears:
Select the checkbox for Perform best subset selection
Increase the Maximum size of best subset: box to 8
Increase the Number of best subsets: box to 2
In the Selection procedure area, select Exhaustive search
Click OK
Click Next .
Step 7. In the Logistic Regression—Step 3 of 3 dialog box, click Finish
file
WEB
Optiva- 
Oversampled- 
NewPredict
TABLE 6.8   PREDICTED PROBABILITIES By LOGISTIC REGRESSION ON 
 OSCARS DATA
Total Number of 
Oscar Nominations
Actual 
Class
Predicted Probability 
of Winning
Predicted 
Class
14
11
10
 6
Winner
Loser
Loser
Winner
0.89
0.58
0.44
0.07
Winner
Winner
Loser
Loser

 
6.4 Supervised Learning 
303
Figure 6.40 displays the XLMiner output produced by the logistic regression procedure. 
The area titled The Regression Model in Figure 6.40 lists the statistical information for the 
logistic regression model using all of the selected explanatory variables. This information 
corresponds to the logistic regression fit of
 
 lna
p
1 2 p^b 5 0.4829 2 0.0006 3 Average Balance 1 c
 
 
1 0.2712 3 Divorced 1 c 2 0.3567 3 College
Although these coefficients do not have a direct intuitive interpretation, the sign of a coef-
ficient in the logistic regression is meaningful. For example, the negative coefficient of the 
AverageBalance variable means that, as the average balance of a customer increases, the 
probability of default decreases. Similarly, the positive coefficient of the binary Divorced 
variable means that a divorced customer is more likely to default than a nondivorced cus-
tomer. The p-value information reflects the statistical significance of each coefficient. Al-
though a logistic regression model used for predictive purposes should ultimately be judged 
by its classification error on the validation and test sets, the p-value information can provide 
some guidance of which models to evaluate further; that is, large p-values suggest that the 
corresponding variable may be less helpful in accurately classifying observations
FIGURE 6.39  XLMINER STEPS FOR LOGISTIC REGRESSION

304 
Chapter 6 Data Mining
In addition to fitting the logistic regression model with all the selected explanatory variables, 
XLMiner also provides summary measures on models with combinations of the variables. The 
Best subset selection in Figure 6.40 lists up to (maximum size of best subset 3 number of 
best subsets) 5 8 3 2 5 16 models. To guide selection among these models, typically there 
is preference for models with fewer coefficients (cells C67 through C81), with a  Mallow’s 
Cp statistic value (cells E67 through E81) approximately equal to the number of coefficients. 
Using these criteria to evaluate the models listed in Figure 6.40, we see that there appear to be 
several candidate models for consideration. For example, the model in row 69 with three coef-
ficients (the constant and the variables AverageBalance, Entrepreneur, and Divorced) as well 
as the model in row 71 with four coefficients (the constant and the variables AverageBalance, 
Entreprenuer, and Divorced) may be good candidates to examine more closely.
FIGURE 6.40  XLMINER LOGISTIC REGRESSION OUTPUT
Inputs
Elapsed Time
Prior Class Pr
Reg. Model
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Residuals
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Var. Covar. Matrix
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Collinearity Diagnostics
Database Score
New Score - Detailed Rep.
Subset selection
773
991.434875
50
8
0.08546152
Residual df
Residual Dev.
% Success in training data
# Iterations used
Multiple R-squared
A
C
D
E
F
G
H
I
J
XLMiner : Logistic Regression
The Regression Model
63
1
2
3
4
5
6
7
49
50
51
52
53
54
55
56
57
58
59
60
61
62
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
81
80
K
L
M
N
O
B
Output Navigator
#Coeffs
Cp
Probability
0.09639337
0.00000002
0.32933539
0.17991926
0.70313913
0.44754469
0.72259235
0.62093687
0.7820909
0.65887463
0.8059864
0.61725473
0.72484541
0.57169807
1
Model (Constant present in all models)
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
Constant
1
2
AverageBalance
Entrepreneur
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
AverageBalance
3
-
-
Entrepreneur
Divorced
Entrepreneur
Entrepreneur
Entrepreneur
Entrepreneur
Entrepreneur
Entrepreneur
Entrepreneur
Entrepreneur
Age
Entrepreneur
Age
7.16103363
47.9348526
3.91871667
5.89838982
1.99622273
3.7597959
3.07174802
3.63497663
4.07931423
4.60324621
5.43156433
5.96588612
7.12403774
7.32027531
8.99997616
RSS
784.145264
824.866333
778.909729
780.886841
774.99231
776.753601
774.069031
774.631531
773.077881
773.601135
772.430969
772.9646
772.12384
772.319824
771.999939
2
2
3
3
4
4
5
5
6
6
7
7
8
8
9
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
Choose Subset
4
-
-
-
-
Divorced
Married
Divorced
Married
Divorced
Married
Married
Unemployed
Entrepreneur
Unemployed
Entrepreneur
5
-
-
-
-
-
-
College
Divorced
High School
Divorced
Divorced
Divorced
Married
Married
Unemployed
6
-
-
-
-
-
-
-
-
College
College
High School
High School
Divorced
Divorced
Married
7
-
-
-
-
-
-
-
-
-
-
College
College
High School
High School
Divorced
8
-
-
-
-
-
-
-
-
-
-
-
-
College
College
High School
9
-
-
-
-
-
-
-
-
-
-
-
-
-
-
College
Best subset selection
Input variables
Coefficient
Std. Error
0.41260341
0.0000848
0.00911902
0.27580711
0.43580058
0.1928287
0.2686584
0.22733882
0.25835317
p-value
0.24180879
0
0.57168293
0.00821805
0.72484046
0.33656538
0.31275669
0.33162266
0.16735514
Odds
-
0.99942642
1.00517082
2.07290697
1.16579187
0.83085221
1.31153536
0.80194759
0.69996727
0.48294309
20.0005737
0.00515748
0.72895199
0.15340053
20.1853033
0.27119845
20.220712
20.3567217
Constant term
AverageBalance
Age
Entrepreneur
Unemployed
Married
Divorced
High School
College

 
6.4 Supervised Learning 
305
Clicking on Choose Subset in cell B71 of the LR_Output1 worksheet (see Figure 6.40)  
activates the XLMiner procedure to refit the logistic regression model with explana-
tory variables AverageBalance, Entrepreneur, and Divorced. The following steps and 
Figure 6.41 explain how to construct this logistic regression model and use it to predict 
the loan default probability of 30 new customers.
Step 1. Click Choose Subset in cell B71 of the LR_Output1 worksheet
Step 2. In the Logistic Regression—Step 1 of 3 dialog box, click Next .
Step 3. In the Logistic Regression—Step 2 of 3 dialog box, click Next .
Step 4. In the Logistic Regression—Step 3 of 3 dialog box:
In the Score test data area, select the checkboxes for Detailed report 
and Lift charts
In the Score new data area, select the checkbox for In worksheet
In the Match variables in the new range dialog box:
In the Data Source area, select the worksheet named NewData-
file
WEB
Optiva-Oversampled-NewPredict
FIGURE 6.41   XLMINER STEPS FOR REFITTING LOGISTIC REGRESSION MODEL AND USING IT TO 
PREDICT NEW DATA

306 
Chapter 6 Data Mining
To Predict from the dropdown menu next to Worksheet:
Enter $A$1:$J$31 in the Data Range: box
In the Variables area, select First row contains headers, and 
click Match variable(s) with same name(s)
Click OK
Click Finish
The preceeding steps produce a worksheet titled LR_Output2 that lists the classification 
confusion matrices for the logistic regression model with AverageBalance, Entrepreneur, 
and Divorced as the explanatory variables. Figure 6.42 displays the classification confu-
sion matrices for the validation and test sets. Using the cutoff value of 0.5, we observe 
that the logistic regression model has a Class 1 error rate of 17.87 percent and a Class 0 
error rate of 52.13 percent on the validation data; on the test data, the Class 1 error rate is 
XLMiner also  produces 
a LR_TestLiftChart 
worksheet that contains 
a cumulative lift chart 
and decile-wise lift chart 
to evaluate the logistic 
 regression classifier.
XLMiner also produces a 
LR_TestScore worksheet 
that shows the logistic 
regression model’s classifi-
cation of each observation 
in the test set.
FIGURE 6.42  CLASSIFICATION ERROR FOR LOGISTIC REGRESSION MODEL
Output Navigator
Inputs
Elapsed Time
Prior Class Pr
Reg. Model
Train. Score - Summary
Train. Score - Detailed Rep.
Training Lift Charts
Residuals
Valid. Score - Summary
Valid. Score - Detailed Rep.
Validation Lift Charts
Var. Covar. Matrix
Test Score - Summary
Test Score - Detailed Rep.
Test Lift Charts
Collinearity Diagnostics
Database Score
New Score - Detailed Rep.
Subset selection
0.5
Cut off Prob. Val. for Success (Updatable)
Classification Confusion Matrix
Predicted Class
Actual
1
0
(Updating the value here will NOT update value in detailed report)
C
D
E
F
G
H
I
J
XLMiner : Logistic Regression
Validation Data scoring - Summary Report
90
1
2
3
4
5
6
7
76
77
78
79
80
81
82
83
84
85
86
87
88
89
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
K
B
A
Error Report
# Cases
# Errors
% Error
42
235
1
Class
6632
12723
0
6674
17.87
52.13
51.50
12958
Overall
0.5
Cut off Prob. Val. for Success (Updatable)
Classification Confusion Matrix
Predicted Class
Actual
1
0
36
4073
1
120
4409
0
Error Report
# Cases
# Errors
% Error
1
Class
0
36
4409
4445
23.08
51.98
51.46
156
8482
8638
Overall
Test Data scoring - Summary Report
42
6091
0
193
6632
1

 
6.4 Supervised Learning 
307
23.08 percent, and the Class 0 error rate is 51.98 percent. Optiva can expect a Class 1 error 
rate of approximately 20 percent and a Class 0 error rate of approximately of 52 percent 
when using this model on new customer observations.
The preceding steps also produce a worksheet titled LR_NewScore2 that lists the logistic 
regression model’s classification of the 30 new customer observations in the NewDataTo-
Predict worksheet. Figure 6.43 displays the estimated probability of Class 1 membership 
(loan default) and the classification using the cutoff value of 0.5. For example, Observa-
tion 1 (i.e., the observation with Row id. 5 1) has an estimated probability 0.3785 of  
defaulting on a loan. Based on the cutoff value of 0.5, Observation 1 is categorized as 
Class 0 or as a nondefaulter on a loan.
FIGURE 6.43  CLASSIFICATION OF 30 NEW CUSTOMER OBSERVATIONS
Data range
0.5
Cut off Prob. Val. for Success (Updatable)
(Updating the value here will NOT update value in summary report)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
0.378527088
0.530278541
0.376113
0.593397552
0.484511484
0.678353192
0.071686462
0.317881666
0.211565187
0.499062694
0.006638452
0.541352148
0.196641277
0.509880331
0.388107141
0.58208246
0.533547431
0.573728099
0.47240446
0.561961974
0.577493069
0.242480111
0.545884003
0.047916623
0.512759841
0.571213334
0.647501136
0.662845784
0.487933851
0.512474608
20.49580469
0.12126254
20.50607963
0.37802866
20.06197389
0.74621401
22.56106763
20.76352396
21.31551657
20.00374923
25.00821584
0.16578728
21.4074202
0.03952647
20.45527576
0.33132798
0.13439163
0.29707818
20.11049444
0.24912846
0.31249059
21.1391301
0.18405384
22.98919013
0.05105045
0.28680324
0.60807315
0.67600192
20.04827397
0.04990879
1467
386
1485
695
707
50
5085
1936
2903
605
9372
308
3064
1288
1396
18
363
78
792
162
51
2594
276
5835
509
96
292
173
683
511
0
0
0
1
0
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
0
1
0
0
0
0
0
1
0
1
0
1
1
1
0
1
1
0
1
0
1
1
1
1
0
1
Back to Navigator
A
C
D
E
F
G
H
I
J
XLMiner : Logistic Regression - Classification of New Data
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
K
B
Row Id.
Predicted
Class
Prob. for 1
(success)
Log odds
Average
Balance
Entrepreneur
Divorced

308 
Chapter 6 Data Mining
Summary
We present an introduction to the concepts and techniques in data mining. Data mining is 
described as the application of statistics and high-speed computer algorithms to help un-
derstand large data sets. As the amount of data stored increases, there is a growing need to 
systematically put it in a form to be formally analyzed. After the data are prepared, we can 
then apply data-mining methods. In this chapter, we present common techniques in the areas 
of unsupervised learning and supervised learning. Unsupervised learning approaches, such as 
clustering and association rules, can be thought of as high-dimensional descriptive analytics 
because they do not have explicit measures of error and the results are evaluated subjectively. 
NOTES AND COMMENTS
1. In Step 2 of the XLMiner’s logistic regression 
procedure, there are several options for select-
ing variables in the model building process. 
Exhaustive search is the most comprehensive 
and considers every possible combination of 
the variables but it is typically appropriate only 
when dealing with fewer than ten explanatory 
variables. When dealing with many variables, 
exhaustive search may be too computation-
ally expensive because it requires construc-
tions hundreds of alternative models. In cases 
with a moderate number of variables, 10 to 20, 
backward selection is effective at eliminating 
the unhelpful variables. Backward selection 
begins with all possible variables and sequen-
tially removes the least useful variable (with 
respect to statistical significance). When deal-
ing with more than 20 variables, forward selec-
tion is often appropriate because it starts with 
an empty model and sequentially adds the most 
helpful variable as measured by statistical sig-
nificance. Other alternatives for selecting vari-
ables in a regression model include stepwise 
selection and sequential replacement. Stepwise 
selection starts with no variables, but at each 
step considers both the insertion and removal 
of a variable based on the F-statistics FIN and 
FOUT, respectively. To avoid a situation where 
a variable repeatedly exits and enters a model, 
FIN $ FOUT, with typical values of 6.5 and 3, 
respectively. Sequential replacement considers 
models with a fixed number of values by insert-
ing a new variable whenever one is removed.
2. XLMiner provides functionality for the Dis-
criminant Analysis classification procedure 
under Classify in the Data Mining group. 
Like logistic regression, discriminant analy-
sis assumes a functional form to describe the 
probability that an observation belongs to a 
class and then uses data to develop estimates 
of the parameters of the function. Specifically, 
P(observation i belongs to Class 1) 5 
ec1(i)
ec0(i) 1 ec1(i), 
where c0(i) and c1(i) are classification scores for 
Class 0 and Class 1 that are computed by the al-
gorithm. The strengths of discriminant analysis 
are its computational simplicity and its ability to 
provide estimates of the effect of each variable 
on the probability of class membership. How-
ever, although discriminant analysis is useful 
for small data sets, its performance is typically 
dominated by other classification methods.
3. XLMiner provides functionality for the Naïve 
Bayes classification procedure under Classify in 
the Data Mining group. The naïve Bayes method 
is based on Bayes’ theorem from classical statis-
tics. However, it is limited to using only categori-
cal predictor variables to classify an observation 
and requires a very large number of observations 
to be effective.
4. XLMiner provides functionality for neural net-
works for both classification and prediction. 
Neural networks are based on the biological 
model of brain activity. Well-structured neural 
networks have been shown to possess accurate 
classification and prediction performance in 
many application domains. However, a neural 
network is a “black box” method that does not 
provide any interpetable explanation to accom-
pany its classifications or predictions. Adjust-
ing the parameters to tune the neural network 
performance is largely a matter of trial and error 
guided by rules of thumb and user experience.
5. XLMiner provides functionality for multiple 
linear regression that greatly enhances the  basic 
regression capabilities provide by Excel’s Data 
Analysis toolpak. The Multiple Linear Re-
gression procedure is listed under Prediction 
in the Data Mining group. This functionality is 
described in the appendix to Chapter 4.

 
Glossary 
309
Supervised learning approaches aim to classify a categorical outcome or predict a continuous 
outcome as accurately as possible and are evaluated by their performance on the training, val-
idation, and test sets. Table 6.9 provides a comparative summary of the supervised learning 
approaches supported by XLMiner. We provide brief descriptions of discriminant analysis, 
the naive Bayes method, and neural networks in the preceding Notes and Comments section.
Glossary
Observation A set of observed values of variables associated with a single entity, often 
displayed as a row in a spreadsheet or database.
Supervised learning Category of data-mining techniques in which an algorithm learns how 
to predict or classify an outcome variable of interest.
Prediction Estimating the value of a continuous outcome variable.
Classification Estimating the value of a categorical outcome variable.
Unsupervised learning Category of data-mining techniques in which an algorithm  explains 
relationships without an outcome variable to guide the process.
Dimension reduction Process of reducing the number of variables to consider in a data-
mining approach.
Hierarchical clustering Process of agglomerating observations into a series of nested 
groups based on a measure of similarity.
k-means clustering Process of organizing observations into one of k groups based on a 
measure of similarity.
Euclidean distance Geometric measure of dissimilarity between observations based on 
Pythagorean Theorem.
Matching coefficient Measure of similarity between observations consisting solely of 
 categorical variables.
TABLE 6.9  OVERVIEW OF SUPERVISED LEARNING METHODS
Strengths
Weaknesses
k-NN
Simple
Requires large amounts of data relative 
to number of variables
Classification and  
regression trees
Provides easy-to-interpret business 
rules; can handle data sets with 
missing data
May miss interactions between variables 
because splits occur one at a time; 
 sensitive to changes in data entries
Multiple linear regression
Provides easy-to-interpret 
 relationship between dependent and 
independent variables
Assumes linear relationship between 
independent variables and a continuous 
dependent variable
Logistic  regression
Classification analog of the familiar 
 multiple regression modeling 
procedure
Coefficients not easily interpretatable in 
terms of effect on likelihood of outcome 
event
Discriminant analysis
Allows classification based on 
 interaction effects between variables
Assumes variables are normally 
 distributed with equal variance; 
 performance often dominated by other 
classification methods
Naïve Bayes
Simple and effective at classifying
Requires a large amount of data; 
 restricted to categorical variables
Neural networks
Flexible and often effective
Many difficult decisions to make when 
building the model; results cannot be 
easily explained (black box)

310 
Chapter 6 Data Mining
Jaccard’s coefficient Measure of similarity between observations consisting solely of 
 binary categorical variables that considers only matches of nonzero entries.
Single linkage Measure of calculating dissimilarity between clusters by considering only 
the two closest observations in the two clusters.
Complete linkage Measure of calculating dissimilarity between clusters by considering 
only the two most dissimilar observations in the two clusters.
Average linkage Measure of calculating dissimilarity between clusters by computing the 
average dissimilarity between every pair of observations between the two clusters.
Average group linkage Measure of calculating dissimilarity between clusters by consider-
ing the distance between the cluster centroids.
Ward’s method Procedure that partitions observations in a manner to obtain clusters with 
the least amount of information loss due to the aggregation.
Dendrogram A tree diagram used to illustrate the sequence of nested clusters produced by 
hierarchical clustering.
Association rules An if-then statement describing the relationship between item sets.
Market basket analysis Analysis of items frequently co-occuring in transactions (such as 
purchases).
Antecedent The item set corresponding to the if portion of an if-then association rule.
Consequent The item set corresponding to the then portion of an if-then association 
rule.
Support count The number of times that a collection of items occur together in a transac-
tion data set.
Confidence The conditional probability that the consequent of an association rule occurs 
given the antecedent occurs.
Lift ratio The ratio of the confidence of an association rule to the benchmark confidence.
Training set Data set used to build data mining model.
Validation set Data set used to estimate accuracy of candidate models on unseen data.
Test set Data set used to estimate accuracy of final model on unseen data. 
Classification confusion matrix A matrix showing the counts of actual versus predicted 
class values.
Overall error rate The percentage of observations misclassified by a model in a data set.
Accuracy Measure of classification success defined as 1 minus the overall error rate.
False positive An observation classified as part of a group with a characteristic when it 
actually does not have the characteristic.
False negative An observation classified as not having a characteristic when it actually 
does possess the characteristic.
Class 1 error rate The percentage of actual Class 1 observations misclassified by a model 
in a data set.
Class 0 error rate The percentage of Class 0 observations misclassified by a model in a 
data set.
Sensitivity Measure of how well Class 1 observations are identified defined as 1 minus the 
Class 1 error rate.
Specificity Measure of how well Class 0 observations are identified defined as 1 minus the 
Class 0 error rate.
Cutoff value The smallest value that the predicted probability of an observation can be for 
the observation to be classified as Class 1.
Cumulative lift chart A chart used to present how well a model performs at identifying 
observations most likely to be in Class 1 versus a random selection.
Decile-wise lift chart A chart used to present how well a model performs at identifying 
observations for each of the top k deciles most likely to be in Class 1 versus a random 
selection.
Average error The average difference between the actual values and the predicted values 
of observations in a data set.

 
Problems 
311
Root mean squared error (RMSE) A measure of the accuracy of a prediction method 
defined as the square root of the sum of squared deviations between the actual values and 
predicted values of observations.
k-nearest neighbors (k-NN) A classification method that classifies an observation based 
on the class of the k observations most similar or nearest to it.
Impurity Measure of the heterogeneity of observations in a classification tree.
Classification tree A tree that classifies a categorical outcome variable by splitting obser-
vations into groups via a sequence of hierarchical rules.
Regression tree A tree that predicts values of a continuous outcome variable by splitting 
observations into groups via a sequence of hierarchical rules.
Logistic regression A generalization of linear regression for predicting a categorical out-
come variable.
Problems
 1. The Football Bowl Subdivision (FBS) level of the National Collegiate Athletic Associa-
tion (NCAA) consists of over 100 schools. Most of these schools belong to one of several 
conferences, or collections of schools, that compete with each other on a regular basis 
in collegiate sports. Suppose the NCAA has commissioned a study that will propose the 
formation of conferences based on the similarities of the constituent schools. The file FBS 
contains data on schools belong to the Football Bowl Subdivision (FBS). Each row in this 
file contains information on a school. The variables include football stadium capacity, lati-
tude, longitude, athletic department revenue, endowment, and undergraduate enrollment.
a. Apply k-means clustering with k 5 10 using football stadium capacity, latitude, longi-
tude, endowment, and enrollment as variables. Be sure to Normalize input data, and 
specify 50 iterations and 10 random starts in Step 2 of the XLMiner k-Means Cluster-
ing procedure. Analyze the resultant clusters. What is the smallest cluster? What is the 
least dense cluster (as measured by the average distance in the cluster)? What makes 
the least dense cluster so diverse?
b. What problems do you see with the plan with defining the school membership of the 
10 conferences directly with the 10 clusters?
c. Repeat part a, but this time do not Normalize input data in Step 2 of the XLMiner 
k-Means Clustering procedure. Analyze the resultant clusters. Why do they differ from 
those in part a? Identify the dominating factor(s) in the formation of these new clusters.
 2.  Refer to the clustering problem involving the file FBS described in Problem 1. Apply hi-
erarchical clustering with 10 clusters using football stadium capacity, latitude, longitude, 
endowment, and enrollment as variables. Be sure to Normalize input data in Step 2 of the 
XLMiner Hierarchical Clustering procedure. Use Ward’s method as the clustering method.
a. Use a PivotTable on the data in the HC_Clusters1 worksheet to compute the cluster 
centers for the clusters in the hierarchical clustering.
b. Identify the cluster with the largest average football stadium capacity. Using all the 
variables, how would you characterize this cluster?
c. Examine the smallest cluster. What makes this cluster unique?
d. By examining the dendrogram on the HC_Dendrogram worksheet (as well as the sequence 
of clustering stages in HC_Output1), what number of clusters seems to be the most natural 
fit based on the distance? What is the increase in distance if 9 clusters were used instead 
of 10? (Use the HC_Output1 worksheet as well as the HC_Dendrogram worksheet.)
 3. Refer to the clustering problem involving the file FBS described in Problem 1.
a. Apply hierarchical clustering with 10 clusters using latitude and longitude as vari-
ables. Be sure to Normalize input data in Step 2 of the XLMiner Hierarchical Clus-
tering procedure, and specify single linkage as the clustering method. Analyze the 
resulting clusters by computing the cluster size and the geographical range that the 
cluster spans. It may be helpful to use a PivotTable on the data in the HC_Clusters 
file
WEB
FBS

312 
Chapter 6 Data Mining
worksheet generated by XLMiner. you can also visualize the clusters by creating a 
scatter plot with longitude as the x-variable and latitude as the y-variable.
b. Repeat part a using average linkage as the clustering method. Compare the clusters to 
the previous methods.
c. Repeat part a using Ward’s method as the clustering method. Compare the clusters to 
the previous methods.
d. Repeat part a using complete linkage as the clustering method. Compare the clusters 
to the previous methods.
e. Repeat part a using average group linkage as the clustering method. Compare the 
clusters to the previous methods.
 4.  From 1946 to 1990, the Big Ten Conference consisted of the University of Illinois,  Indiana 
University, University of Iowa, University of Michigan, Michigan State  University, 
 University of Minnesota, Northwestern University, Ohio State University, Purdue Uni-
versity, and University of Wisconsin. In 1990, the conference added Pennsylvania State 
University. In 2011, the conference added the University of Nebraska. Even more re-
cently, the University of Maryland and Rutgers University have been added to the confer-
ence with speculation of more schools being added in the future. The file BigTen contains 
much of the same information as the file FBS (see Problem 1’s description), except that 
the variable values for the original ten individual schools in the Big Ten Conference have 
been replaced with the respective averages of these variables over these ten schools.
 
  Apply hierarchical clustering with 2 clusters using football stadium capacity, latitude, 
longitude, endowment, and enrollment as variables. Be sure to Normalize input data in 
Step 2 of the XLMiner Hierarchical Clustering procedure. Use average group linkage as 
the clustering method. By referencing the HC_Output1 worksheet, which schools does the 
clustering suggest would have been the most appropriate to be the eleventh and twelfth 
schools in the Big Ten? The thirteenth and fourteenth? The fifteenth and sixteenth?
 5. Refer to the clustering problem involving the file FBS described in Problem 1. The NCAA has 
a preference for conferences consisting of similar schools with respect to their endowment, 
enrollment, and football stadium capacity, but these conferences must be in the same geo-
graphic region to reduce traveling costs. Take the following steps to address this desire. Apply 
k-means clustering using latitude and longitude as variables with k 5 3. Be sure to Normalize 
input data, and specify 50 iterations and 10 random starts in Step 2 of the XLMiner k-Means 
Clustering procedure. Then create one distinct data set for each of the three regional clusters. 
a. 
For the west cluster, apply hierarchical clustering with Ward’s method to form two clus-
ters using football stadium capacity, endowment, and enrollment as variables. Be sure 
to Normalize input data in Step 2 of the XLMiner Hierarchical Clustering procedure. 
 Using a PivotTable on the data in HC_Clusters1, report the characteristics of each cluster.
b. For the east cluster, apply hierarchical clustering with Ward’s method to form three 
clusters using football stadium capacity, endowment, and enrollment as variables. 
Be sure to Normalize input data in Step 2 of the XLMiner Hierarchical Clustering 
procedure. Using a PivotTable on the data in HC_Clusters1, report the characteristics 
of each cluster.
c. For the south cluster, apply hierarchical clustering with Ward’s method to form four 
clusters using football stadium capacity, endowment, and enrollment as variables. 
Be sure to Normalize input data in Step 2 of the XLMiner Hierarchical Clustering 
procedure. Using a PivotTable on the data in HC_Clusters1, report the characteristics 
of each cluster.
d. What problems do you see with the plan with defining the school membership of nine 
conferences directly with the nine total clusters formed from the regions? How could 
this approach be tweaked to solve this problem?
 6. Suppose that IBM employs a network of expert analytics consultants for various projects. 
To help it determine how to distribute its bonuses, IBM wants to form groups of employees 
with similar performance according to key performance metrics. Each observation (cor-
responding to an employee) in the file BigBlue consists of values for (1) UsageRate, which 
file
WEB
BigTen
file
WEB
BigBlue

 
Problems 
313
corresponds to the proportion of time that the employee has been actively working on high 
priority projects, (2) Recognition, which is the number of projects for which the employee 
was specifically requested, and (3) Leader, which is the number of projects on which the 
employee has served as project leader.
 
  Apply k-means clustering with for values of k 5 2, . . . ,7. Be sure to Normalize input 
data, and specify 50 iterations and 10 random starts in Step 2 of the XLMiner k-Means 
Clustering procedure. How many clusters do you recommend using to categorize the 
 employees? Why?
 7. Consider the case where Apple computer tracks online transactions at its iStore and is 
interested in learning about the purchase patterns of its customers in order to provide rec-
ommendations as a customer browses its Web site. A sample of the shopping cart data in 
binary matrix format resides in the file AppleCart. Each row indicates which iPad features 
and accessories a customer selected.
 
  Using a minimum support of 10 percent of the total number of transactions and a mini-
mum confidence of 50 percent, use XLMiner to generate a list of association rules.
a. Interpret what the rule with the largest lift ratio is saying about the relationship  between 
the antecedent item set and consequent item set.
b. Interpret the support count of the item set involved in the rule with the largest lift ratio.
c. Interpret the confidence of the rule with the largest lift ratio.
d. Interpret the lift ratio of the rule with the largest lift ratio.
e. Review the top 15 rules, and summarize what they suggest.
 8. Cookie Monster Inc. is a company that specializes in the development of software that 
tracks Web browsing history of individuals. A sample of browser histories is provided 
in the file CookieMonster. Using binary matrix format, the entry in row i and column j 
indicates whether Web site j was visited by user i.
 
  Using a minimum support of 800 transactions and a minimum confidence of 50 percent, 
use XLMiner to generate a list of association rules. Review the top 14 rules. What informa-
tion does this analysis provide Cookie Monster regarding the online behavior of individuals?
 9. A grocery store introducing items from Italy is interested in analyzing buying trends of 
new international items: prosciutto, pepperoni, risotto, and gelato.
a. Using a minimum support of 100 transactions and a minimum confidence of 50 per-
cent, use XLMiner to generate a list of association rules. How many rules satisfy this 
criterion?
b. Using a minimum support of 250 transactions and a minimum confidence of 50 per-
cent, use XLMiner to generate a list of association rules. How many rules satisfy this 
criterion? Why may the grocery store want to increase the minimum support required 
for their analysis? What is the risk of increasing the minimum support required?
c. Using the list of rules from part b, consider the rule with the largest lift ratio that in-
volves an Italian item. Interpret what this rule is saying about the relationship between 
the antecedent item set and consequent item set.
d. Interpret the support count of the item set involved in the rule with the largest lift ratio 
that involves an Italian item.
e. 
Interpret the confidence of the rule with the largest lift ratio that involves an Italian item.
f. 
Interpret the lift ratio of the rule with the largest lift ratio that involves an Italian item.
g. What insight can the grocery store obtain about its purchasers of the Italian fare?
 10. Campaign organizers for both the Republican and Democrat parties are interested in iden-
tifying individual undecided voters who would consider voting for their party in an upcom-
ing election. The file BlueOrRed contains data on a sample of voters with tracked variables 
including: whether or not they are undecided regarding their candidate preference, age, 
whether they own a home, gender, marital status, household size, income, years of educa-
tion, and whether they attend church.
 
  Partition the data into training (50 percent), validation (30 percent), and test (20 percent) 
sets. Classify the data using k-nearest neighbors with up to k 5 20. Use Age, Home-
Owner, Female, Married, HouseholdSize, Income, and Education as input variables and 
file
WEB
AppleCart
file
WEB
CookieMonster
file
WEB
GroceryStore
file
WEB
BlueOrRed

314 
Chapter 6 Data Mining
Undecided as the output variable. In Step 2 of XLMiner’s k-nearest neighbors Classifica-
tion procedure, be sure to Normalize input data and to Score on best k between 1 and 
specified value. Generate lift charts for both the validation data and test data.
a. For k 5 1, why is the overall rate equal to 0 percent on the training set? Why isn’t the 
overall rate equal to 0 percent on the validation set?
b. For the cutoff probability value 0.5, what value of k minimizes the overall error rate 
on the validation data? Explain the difference in the overall error rate on the training, 
validation, and test data.
c. Examine the decile-wise lift chart. What is the first decile lift on the test data? Interpret 
this value.
d. In the effort to identify undecided voters, a campaign is willing to accept an increase 
in the misclassification of decided voters as undecided if it can correctly classify more 
undecided voters. For cutoff probability values of 0.5, 0.4, 0.3, and 0.2, what are the 
corresponding Class 1 error rates and Class 0 error rates on the validation data?
 11. Refer to the scenario described in Problem 10 and the file BlueOrRed. Partition the data 
into training (50 percent), validation (30 percent), and test (20 percent) sets. Fit a clas-
sification tree  using Age, HomeOwner, Female, Married, HouseholdSize, Income, and 
Education as input variables and Undecided as the output variable. In Step 2 of XLMiner’s 
Classification Tree procedure, be sure to Normalize input data and to set the Minimum 
#records in a terminal node to 1. In Step 3 of XLMiner’s Classification Tree procedure, 
set the maximum number of levels to seven. Generate the Full tree, Best pruned tree, and 
Minimum error tree. Generate lift charts for both the validation data and the test data.
a. Interpret the set of rules implied by the best pruned tree that characterize undecided 
voters.
b. In the CT_Output1 sheet, why is the overall error rate of the full tree 0 percent? Explain 
why this is not necessarily an indication that the full tree should be used to classify 
future observations and the role of the best pruned tree.
c. For the default cutoff value of 0.5, what is the overall error rate, Class 1 error rate, and 
Class 0 error rate of the best pruned tree on the test data?
d. Examine the decile-wise lift chart for the best pruned tree on the test data. What is the 
first decile lift? Interpret this value.
 12. Refer to the scenario described in Problem 10 and the file BlueOrRed. Partition the data 
into training (50 percent), validation (30 percent), and test (20 percent) sets. Use logistic 
regression to classify observations as undecided (or decided) using Age, HomeOwner, Fe-
male, Married, HouseholdSize, Income, and Education as input variables and Undecided 
as the output variable. Perform an exhaustive-search best subset selection with the number 
of subsets equal to 2.
a. From the generated set of logistic regression models, select one that you believe is a 
good fit. Express the model as a mathematical equation relating the output variable to 
the input variables.
b. Increases in which variables increase the chance of a voter being undecided? Increases 
in which variables decrease the chance of a voter being decided?
c. Using the default cutoff value of 0.5 for your logistic regression model, what is the 
overall error rate on the test data?
d. Examine the decile-wise lift chart for your model on the test data. What is the first 
decile lift? Interpret this value.
 13. Telecommunications companies providing cell phone service are interested in customer 
retention. In particular, identifying customers who are about to churn (cancel their service) 
is potentially worth millions of dollars if the company can proactively address the reason 
that customer is considering cancellation and retain the customer. The WEBfile Cellphone 
contains customer data to be used to classify a customer as a churner or not.
 
  In XLMiner’s Partition with Oversampling procedure, partition the data so there is 
50 percent successes (churners) in the training set and 40 percent of the validation data 
is taken away as test data. Classify the data using k-nearest neighbors with up to k 5 20. 
file
WEB
Cellphone

 
Problems 
315
Use Churn as the output variable and all the other variables as input variables. In Step 2 
of XLMiner’s k-nearest neighbors Classification procedure, be sure to Normalize input 
data and to Score on best k between 1 and specified value. Generate lift charts for both 
the validation data and test data.
a. Why is partitioning with oversampling advised in this case?
b. For the cutoff probability value 0.5, what value of k minimizes the overall error rate 
on the validation data?
c. What is the overall error rate on the test data?
d. What are the Class 1 error rate and the Class 0 error rate on the test data?
e. Compute and interpret the sensitivity and specificity for the test data.
f. 
How many false positives and false negatives did the model commit on the test data? 
What percentage of predicted churners were false positives? What percentage of pre-
dicted nonchurners were false negatives?
g. Examine the decile-wise lift chart on the test data. What is the first decile lift on the 
test data? Interpret this value.
 14. Refer to the scenario described in Problem 13 and the file Cellphone. In XLMiner’s Par-
tition with Oversampling procedure, partition the data so there is 50 percent successes 
(churners) in the training set and 40 percent of the validation data is taken away as test 
data. Fit a classification tree using Churn as the output variable and all the other variables 
as input variables. In Step 2 of XLMiner’s Classification Tree procedure, be sure to Nor-
malize input data, and set the Minimum #records in a terminal node to 1. In Step 3 of 
XLMiner’s Classification Tree procedure, set the maximum number of levels to 7. Gener-
ate the Full tree, Best pruned tree, and Minimum error tree. Generate lift charts for 
both the validation data and test data.
a. Why is partitioning with oversampling advised in this case?
b. Interpret the set of rules implied by the best pruned tree that characterize churners.
c. In the CT_Output1 sheet, why is the overall error rate of the full tree 0 percent? Explain 
why this is not necessarily an indication that the full tree should be used to classify 
future observations and the role of the best pruned tree.
d. For the default cutoff value of 0.5, what are the overall error rate, Class 1 error rate, 
and Class 0 error rate of the best pruned tree on the test data?
e. Examine the decile-wise lift chart for the best pruned tree on the test data. What is the 
first decile lift? Interpret this value.
 15. Refer to the scenario described in Problem 13 and the file Cellphone. In XLMiner’s Partition 
with Oversampling procedure, partition the data so there is 50 percent successes (churners) 
in the training set and 40 percent of the validation data are taken away as test data. Construct 
a logistic regression model using Churn as the output variable and all the other variables as 
input variables. Perform an exhaustive-search best subset selection with the number of best 
subsets equal to 2. Generate lift charts for both the validation data and test data.
a. Why is partitioning with oversampling advised in this case?
b. From the generated set of logistic regression models, select one that you believe is a 
good fit. Express the model as a mathematical equation relating the output variable to 
the input variables. Do the relationships suggested by the model make sense? Try to 
explain them.
c. Using the default cutoff value of 0.5 for your logistic regression model, what is the 
overall error rate on the test data?
d. Examine the decile-wise lift chart for your model on the test data. What is the first 
decile lift? Interpret this value.
 16. A consumer advocacy agency, Equitable Ernest, is interested in providing a service in which 
an individual can estimate their own credit score (a continuous measure used by banks, insur-
ance companies, and other businesses when granting loans, quoting premiums, and issuing 
credit). The file CreditScore contains data on an individual’s credit score and other variables.
 
  Partition the data into training (50 percent), validation (30 percent), and test (20 percent) 
sets. Predict the individuals’ credit scores using k-nearest neighbors with up to k 5 20. Use 
file
WEB
CreditScore

316 
Chapter 6 Data Mining
CreditScore as the output variable and all the other variables as input variables. In Step 2 
of XLMiner’s k-Nearest Neighbors Prediction procedure, be sure to Normalize input data 
and to Score on best k between 1 and specified value. Generate a Detailed Scoring for 
all three sets of data.
a. 
For k 5 1, why is the root mean squared error greater than zero on the training set? Why 
would we expect the root mean squared error to be zero for k 5 1 on the training set?
b. What value of k minimizes the root mean squared error (RMSE) on the validation 
data?
c. How does the RMSE on the test set compare to the RMSE on the validation set?
d. What is the average error on the test set? Analyze the output in the KNNP_TestScore1 
worksheet, paying particular attention to the observations in which had the largest 
overprediction (large negative residuals) and the largest underprediction (large posi-
tive residuals). Explain what may be contributing to the inaccurate predictions and 
possible ways to improve the k-NN approach.
 17. Refer to the scenario described in Problem 16 and the file CreditScore. Partition the data  
into training (50 percent), validation (30 percent), and test (20 percent) sets. Predict the indi-
viduals’ credit scores using a regression tree. Use CreditScore as the output variable and all 
the other variables as input variables. In Step 2 of XLMiner’s Regression Tree procedure, 
be sure to Normalize input data, and specify Using Best prune tree as the scoring option. 
In Step 3 of XLMiner’s Regression Tree procedure, set the maximum number of levels to 7.  
Generate the Full tree, Best pruned tree, and Minimum error tree. Generate Detailed 
Scoring for all three sets of data.
a. What is the root mean squared error (RMSE) of the best pruned tree on the validation 
data and on the test data? Explain the difference and whether the magnitude of the 
difference is of concern in this case.
b. Interpret the set of rules implied by the best pruned tree and how these rules are used 
to predict an individual’s credit score.
c. Examine the best pruned tree in RT_PruneTree1 as well as the predicted credit scores 
and residuals for the test data in RT_TestScore1. Identify the weakness of this regres-
sion tree model. Explain what may be contributing to inaccurate predictions, and 
discuss possible ways to improve the regression tree approach.
d. Repeat the construction of a regression tree following the previous instructions, but in 
Step 2 of XLMiner’s Regression Tree procedure, set the Minimum #records in a ter-
minal node to 1. How does the RMSE of the best pruned tree on the test data compare 
to the analogous measure from part a? In terms of number of decision nodes, how does 
the size of the best pruned tree compare to the size of the best pruned tree from part a?
 18. Each year, the American Academy of Motion Picture Arts and Sciences recognizes excel-
lence in the film industry by honoring directors, actors, and writers with awards (called the 
Oscars) in different categories. The most notable of these awards is the Oscar for Best Pic-
ture. The Data worksheet in the file Oscars contains data on a sample of movies nominated 
for the Best Picture Oscar. The variables include total number of Oscar nominations across 
all award categories, number of Golden Globe awards won (the Golden Globe award show 
precedes the Oscars), whether the movie is a comedy, and whether the movie won the Best 
Picture Oscar.
 
  There is also a variable called ChronoPartition that specifies how to partition the data 
into training, validation, and test sets. The value t identifies observations that belong to 
the training set, the value v identifies observations that belong to the validation set, and the 
value s identifies observations that belong to the test set. Partition the data using  XLMiner’s 
Standard Partition procedure by selecting Use partition variable in the Partitioning 
options area and specifying the variable ChronoPartition.
 
  Construct a logistic regression model to classify winners of the Best Picture Oscar. Use 
Winner as the output variable and OscarNominations, GoldenGlobeWins, and Comedy as 
input variables. Perform an exhaustive-search best subset selection with the number of best 
subsets equal to 2.
file
WEB
Oscars

 
Problems 
317
a. From the generated set of logistic regression models, select one that you believe is a 
good fit. Express the model as a mathematical equation relating the output variable to 
the input variables. Do the relationships suggested by the model make sense? Try to 
explain the relationships.
b. Using the default cutoff value of 0.5 for your logistic regression model, what is the 
overall error rate on the validation data?
c. Note that each year there is only one winner of the Best Picture Oscar. Knowing 
this, what is wrong with classifying a movie as a winner or not using a cutoff value? 
Hint: Be sure to generate the Detailed Scoring for the validation data in Step 3  
of  XLMiner’s Logistic Regression procedure and then use the LR_ValidScore  
worksheet.
d. What is the best way to use the model to predict the annual winner? Out of the six 
years in the validation data, in how many years does the model correctly identify the 
winner?
e. Use your model to classify the 2011 nominees for Best Picture. In Step 3 of XLMiner’s 
Logistic Regression procedure, check the box next to In worksheet in the Score new 
data area. In the Match variable in the new range dialog box, (1) specify NewData 
in the Worksheet: field, (2) enter the cell range A1:E9 in the Data range: field, and 
(3) click Match variable(s) with same name(s). When completing the procedure, this 
will result in a LR_NewScore worksheet that contains the predicted probability that 
each 2011 nominee will win the Best Picture Oscar. What film did the model believe 
was the most likely to win the 2011 Best Picture Oscar? Was the model correct?
 19. As an intern with the local home builder’s association, you have been asked to analyze the 
state of the local housing market that has suffered during a recent economic crisis. you 
have been provided three data sets in the file HousingBubble. The Pre-Crisis worksheet 
contains information on 1978 single-family homes sold during the one-year period before 
the burst of the housing bubble. The Post-Crisis worksheet contains information on 1657 
single-family homes sold during the one-year period after the burst of the housing bubble. 
The NewDataToPredict worksheet contains information on homes currently for sale.
a. Consider the Pre-Crisis worksheet data. Partition the data into training (50 percent), 
validation (30 percent), and test (20 percent) sets. Predict the sale price using k-nearest 
neighbors with up to k 5 20. Use Price as the output variable and all the other variables 
as input variables. In Step 2 of XLMiner’s k-Nearest Neighbors Prediction procedure, 
be sure to Normalize input data and to Score on best k between 1 and specified 
value. Check the box next to In worksheet in the Score new data area. In the Match 
variables in the new range dialog box, (1) specify the NewDataToPredict worksheet 
in the Worksheet: field, (2) enter the cell range A1:P2001 in the Data range: field, 
and (3) click Match variable(s) with same name(s). Completing the procedure will 
result in a KNNP_NewScore worksheet that will contain the predicted sales price for 
each home in NewDataToPredict.
 
 i.  What value of k minimizes the root mean squared error (RMSE) on the validation 
data?
 
 ii. What is the RMSE on the validation data and test data?
 
iii.  What is the average error on the validation data and test data? What does this 
 suggest?
b. Repeat part a with the Post-Crisis worksheet data.
c. The KNNP_NewScore1 and KNNP_NewScore2 worksheets contain the sales price 
predictions for the 2000 homes in the NewDataToPredict using the precrisis and 
postcrisis data, respectively. For each of these 2000 homes, compare the two predic-
tions by computing the percentage change in predicted price between the precrisis 
and postcrisis models. Let percentage change 5 (postcrisis predicted price 2 precrisis 
predicted price)/precrisis predicted price. Summarize these percentage changes with 
a histogram. What is the average percentage change in predicted price between the 
precrisis and postcrisis model?
file
WEB
HousingBubble

318 
Chapter 6 Data Mining
 20. Refer to the scenario described in Problem 19 and the file HousingBubble.
a. For the following substeps, consider the Pre-Crisis worksheet data. Partition the data 
into training (50 percent), validation (30 percent), and test (20 percent) sets. Predict 
the sale price using a regression tree. Use Price as the output variable and all the other 
variables as input variables. In Step 2 of XLMiner’s Regression Tree procedure, be 
sure to Normalize input data, to set the Minimum #records in a terminal node to 
1, and to specify Using Best prune tree as the scoring option. In Step 3 of XLMiner’s 
Regression Tree procedure, set the maximum number of levels to 7. Generate the Full 
tree and Best pruned tree. Check the box next to In worksheet in the Score new 
data area. In the Match variable in the new range dialog box, (1) specify the New-
DataToPredict worksheet in the Worksheet: field, (2) enter the cell range A1:P2001 
in the Data range: field, and (3) click Match variable(s) with same name(s). When 
completing the procedure, this will result in a LR_NewScore1 worksheet that contains 
the predicted sales price for each home in NewDataToPredict.
 
 i.  In terms of number of decision nodes, compare the size of the full tree to the size 
of the best pruned tree.
 
 ii.  What is the root mean squared error (RMSE) of the best pruned tree on the valida-
tion data and on the test data?
 
iii.  What is the average error on the validation data and test data? What does this 
 suggest?
 
iv.  By examining the best pruned tree, what are the critical variables in predicting the 
price of a home?
b. Repeat part a with the Post-Crisis worksheet data.
c. The RT_NewScore1 and RT_NewScore2 worksheets contain the sales price predic-
tions for the 2000 homes in the NewDataToPredict using the precrisis and postcrisis 
data, respectively. For each of these 2000 homes, compare the two predictions by 
computing the percentage change in predicted price between the precrisis and postcri-
sis model. Let percentage change 5 (postcrisis predicted price 2 precrisis predicted  
price)yprecrisis predicted price. Summarize these percentage changes with a histo-
gram. What is the average percentage change in predicted price between the precrisis 
and postcrisis models?
 21. Refer to the scenario described in Problem 19 and the file HousingBubble.
a. Consider the Pre-Crisis worksheet data. Partition the data into training (50 percent), 
validation (30 percent), and test (20 percent) sets. Predict the sale price using multiple 
linear regression. Use Price as the output variable and all the other variables as input 
variables. To generate a pool of models to consider, execute the following steps. In 
Step 2 of XLMiner’s Multiple Linear Regression procedure, click the Best subset 
option. In the Best Subset dialog box, check the box next to Perform best subset 
selection, enter 16 in the box next to Maximum size of best subset:, enter 1 in the 
box next to Number of best subsets:, and check the box next to Exhaustive search. 
Once you have identified an acceptable model, rerun the Multiple Linear Regression 
procedure and in Step 2, check the box next to In worksheet in the Score new data 
area. In the Match variable in the new range dialog box, (1) specify the NewDataTo-
Predict worksheet in the Worksheet: field, (2) enter the cell range A1:P2001 in the 
Data range: field, and (3) click Match variable(s) with same name(s).
 
 i.  From the generated set of multiple linear regression models, select one that you 
believe is a good fit. Express the model as a mathematical equation relating the 
output variable to the input variables.
 
 ii.  For your model, what is the RMSE on the validation data and test data?
 
iii.  What is the average error on the validation data and test data? What does this  suggest?
b. Repeat part a with the Post-Crisis worksheet data.
c. The MLR_NewScore worksheets generated in parts a and b contain the sales price 
predictions for the 2000 homes in the NewDataToPredict using the pre-crisis and post-
crisis data, respectively. For each of these 2000 homes, compare the two predictions 
The XLMiner functionality 
for multiple linear regres-
sion mirrors that for logistic 
regression described in this 
chapter. Refer to Chapter 4  
for more information on 
multiple linear regression.

 
Case Problem  Grey Code Corporation 
319
by computing the percentage change in predicted price between the pre-crisis and 
postcrisis models. Let percentage change 5 (postcrisis predicted price 2 pre-crisis 
predicted price)/pre-crisis predicted price. Summarize these percentage changes with 
a histogram. What is the average percentage change in predicted price between the 
pre-crisis and postcrisis model?
Grey Code Corporation
Grey Code Corporation (GCC) is a media and marketing company involved in magazine 
and book publishing and television broadcasting. GCC’s portfolio of home and family 
magazines have been a long running strength, but the company has expanded to become a 
provider of a spectrum of services (market research, communications planning, Web site 
advertising, etc.) that can enhance its clients’ brands.
GCC’s relational database contains over a terabyte of data encompassing 75 million 
customers. GCC uses the data in its database to develop campaigns for new customer acqui-
sition, customer reactivation, and identification of cross-selling opportunities for products. 
For example, GCC will generate separate versions of a monthly issue of a magazine that 
will differ only by the advertisements the magazines contain. They will mail a subscribing 
customer the version with the print ads that the GCC database has determined will most 
interest that customer.
A problem facing GCC is how to boost the customer response rate to the renewal offers 
that it mails to its magazine subscribers. The industry response rate is about 2 percent, but 
GCC has historically had a higher response rate. GCC’s director of database marketing, 
Chris Grey, wants to ensure that GCC maintains its place as one of the top achievers in tar-
geted marketing. In one effort directed at maintaining this position, GCC is currently con-
sidering the development of a targeted marketing strategy for a hobby-based magazine. The 
file GCC contains 38 columns (each corresponding to a distinct variable) and over 40,000 
rows (each corresponding to a distinct customer). The Description worksheet contains a 
description of each variable in the file GCC. The Data worksheet contains the data to be 
used to train, validate, and test a classification method. The NewDataToPredict worksheet 
contains data on a new set of former subscribers to the hobby-based magazine whom GCC 
would like to classify as likely or not likely to respond to a targeted renewal offer.
Managerial Report
Play the role of Chris Gray and construct a classification model to identify customers who 
are likely to respond to a mailing. your report should include the following analyses:
 1. The data provided is still relatively raw. Prepare the date for data mining by address-
ing missing data entries and transforming variables. you may want to use XLMiner’s 
Missing Data Handling utility and Transform Categorical Data utility.
 2. Explore the data. Due to the large number of variables, you may want to identify means 
of reducing the dimensions of the data. In particular, analyze relationships between 
variables using correlation and pivot tables.
 3. Experiment with various classification methods and propose a final model for identify-
ing customers who will respond to the targeted marketing.
a. Provide a summary of the classification model that you select.
b. Provide a decile-wise lift chart and analysis of your model’s performance on the 
test set.
c. Provide a chart of the Class 1 and Class 0 error rates on the test set for various 
values of the cutoff probability.
d. Classify the customers in the NewDataToPredict worksheet.
Case Problem 
file
WEB
GCC
For more information on 
XLMiner functionality, 
refer to the Help button 
in the Tools group of the 
XLMINER tab.

Spreadsheet Models
CONTENTS
7.1 
BUILDING GOOD 
 SPREADSHEET MODELS
Influence Diagrams
Building a Mathematical Model
Spreadsheet Design and 
Implementing the Model in a 
Spreadsheet
7.2 
WHAT-IF ANALYSIS
Data Tables
Goal Seek
7.3 
SOME USEFUL EXCEL 
 FUNCTIONS FOR MODELING
SUM and SUMPRODUCT
IF and COUNTIF
VLOOKUP
7.4 
AUDITING SPREADSHEET 
MODELS 
Trace Precedents and Dependents
Show Formulas
Evaluate Formulas
Error Checking
Watch Window
Chapter 7

Numerous specialized software packages are available for descriptive, predictive, and pre-
scriptive business analytics. Because these software packages are specialized, they usually 
provide the user with numerous options and the capability to perform detailed analyses. 
However, they tend to be considerably more expensive than a spreadsheet package such as 
Excel. Also, specialized packages often require substantial user training. Because spread-
sheets are less expensive, often come preloaded on computers, and are fairly easy to use, 
they are without question the most used business analytics tool. Every day, millions of 
people around the world use risk analysis, inventory tracking and control, investment plan-
ning, breakeven analysis, and many other types of spreadsheet decision models. A well 
designed, well documented, and accurate spreadsheet model can be a very valuable tool in 
decision making.
Spreadsheet models are mathematical and logic-based models. Their strength is that 
they provide easy-to-use, sophisticated mathematical and logical functions, allowing for 
easy instantaneous recalculation for a change in model inputs. This is why spreadsheet 
models are often referred to as what-if models. What-if models allow you to answer ques-
tions like, “If per unit cost is $4, what is the impact on profit?” Changing data in a given 
cell has an impact not only on that cell but, also on any other cells containing a formula or 
function that uses that cell.
In this chapter we discuss principles for building reliable spreadsheet models. We be-
gin with a discussion of how to build a conceptual model of a decision problem, how to 
convert the conceptual model to a mathematical model, and how to implement the model 
in a spreadsheet. We introduce two analysis tools available in Excel, Data Tables and Goal 
Seek, and discuss some Excel functions that are useful for building spreadsheet models 
for decision making. Finally, we present how to audit a spreadsheet model to ensure its 
reliability.
If you have never used a 
spreadsheet or have not 
done so recently, we sug-
gest you first familiarize 
yourself with the material 
in Appendix A of this book. 
It provides basic informa-
tion that is fundamental to 
using Excel.
Procter & Gamble (P&G) is a Fortune 500 consumer 
goods company headquartered in Cincinnati, Ohio. P&G 
produces well-known brands such as Tide detergent, 
 Gillette razors, Swiffer cleaning products, and many 
other consumer goods. P&G is a global company and has 
been recognized for its excellence in business analytics, 
including supply chain analytics and market research.
With operations around the world, P&G must do 
its best to maintain inventory at levels that meet its 
high customer service requirements. A lack of on-hand 
inventory can result in a stockout of a product and an 
inability to meet customer demand. This not only re-
sults in lost revenue for an immediate sale but can also 
cause customers to switch permanently to a competing 
brand. On the other hand, excessive inventory forces 
P&G to invest cash in inventory when that money could 
be invested in other opportunities, such as research and 
development.
To ensure that the inventory of its products around 
the world are set at appropriate levels, P&G analytics 
personnel developed and deployed a series of spread-
sheet inventory models. These spreadsheets implement 
mathematical inventory models to tell business units 
when and how much to order to keep inventory levels 
where they need to be in order to maintain service and 
keep investment as low as possible.
The spreadsheet models were carefully designed to 
be easily understood by the users and easy to use and 
 interpret. Their users can also customize the spread-
sheets to their individual situations.
Over 70 percent of the P&G business units use these 
models, with a conservative estimate of a 10 percent re-
duction in inventory around the world. This equates to a 
cash savings of nearly $350 million.
PROCTER AND GAMBLE SETS INVENTORY TARGETS USING SPREADSHEET MODELS*
ANALYTICS  in  Action
*I. Farasyn, K. Perkoz, and W. Van de Velde, “Spreadsheet Model for 
Inventory Target Setting at Procter & Gamble, Interfaces 38, no. 4 (July–
August 2008): 241–250.
 
Analytics in Action 
321

322 
Chapter 7 Spreadsheet Models
Building Good Spreadsheet Models
Let us begin our discussion of spreadsheet models by considering the cost of producing a 
single product. The total cost of manufacturing a product can usually be defined as the sum 
of two costs: fixed cost and variable cost. Fixed cost is the portion of the total cost that does 
not depend on the production quantity; this cost remains the same no matter how much is 
produced. Variable cost, on the other hand, is the portion of the total cost that is dependent 
on and varies with the production quantity. To illustrate how cost models can be developed, 
we will consider a manufacturing problem faced by Nowlin Plastics.
Nowlin Plastics produces a line of cell phone covers. Nowlin’s best-selling cover is its 
Viper model, a slim but very durable black and gray plastic cover. The annual fixed cost 
for the Viper cover is $234,000. This fixed cost includes management time, advertising, 
and other costs that are incurred regardless of the number of units eventually produced. 
In addition, the total variable cost, including labor and material costs, is $2 for each unit 
produced.
Nowlin is considering outsourcing the production of some products for next year, 
including the Viper. Nowlin has a bid from an outside firm to produce the Viper for 
$3.50 per unit. Although it is more expensive per unit to outsource the Viper ($3.50 ver-
sus $2.00), the fixed cost can be avoided if Nowlin purchases rather than manufactures 
the product. The exact demand for Viper for next year is not yet known. Nowlin would 
like to compare the costs of manufacturing the Viper in-house to those of outsourcing 
its production to another firm, and management would like to do that for various pro-
duction quantities. Many manufacturers face this type of decision, which is known as a 
make-versus-buy decision.
Influence Diagrams
It is often useful to begin the modeling process with a conceptual model that shows the 
relationships between the various parts of the problem being modeled. The conceptual 
model helps in organizing the data requirements and provides a road map for eventually 
constructing a mathematical model. A conceptual model also provides a clear way to com-
municate the model to others. An influence diagram is a visual representation that shows 
which entities influence others in a model. Parts of the model are represented by circular or 
oval symbols called nodes, and arrows connecting the nodes show influence.
Figure 7.1 shows an influence diagram for Nowlin’s total cost of production for the 
Viper. Total manufacturing cost depends on fixed cost and variable cost, which in turn, 
depends on the variable cost per unit and the quantity required.
An expanded influence diagram that includes an outsourcing option is shown in Fig-
ure 7.2. Note that the influence diagram in Figure 7.1 is a subset of the influence diagram 
in Figure 7.2. Our method here—namely, to build an influence diagram for a portion 
of the problem and then expand it until the total problem is conceptually  modeled—is 
 usually a good way to proceed. This modular approach simplifies the process and reduces 
the likelihood of error. This is true not just for influence diagrams but for the construction 
of the mathematical and spreadsheet models as well. Next we turn our attention to using 
the influence diagram in Figure 7.2 to guide us in the construction of the mathematical 
model.
Building a Mathematical Model
The task now is to use the influence diagram to build a mathematical model. Let us first 
consider the cost of manufacturing the required units of the Viper. As the influence diagram 
shows, this cost is a function of the fixed cost, the variable cost per unit, and the quantity 
7.1

 
7.1 Building Good Spreadsheet Models 
323
FIGURE 7.1   AN INFLUENCE DIAGRAM FOR NOWLIN’S MANUFACTURING 
COST
Total
Manufacturing
Cost
Quantity
Required
Fixed
Cost
Variable
Cost
Variable
Cost per Unit
FIGURE 7.2   AN INFLUENCE DIAGRAM FOR COMPARING MANUFACTURING VERSUS 
 OUTSOURCING COST FOR NOWLIN PLASTICS
Difference in
Cost of
Manufacturing and
Outsourcing
Total
Manufacturing
Cost
Total
Outsource
Cost
Purchase
Cost per Unit
Quantity
Required
Variable
Cost per Unit
Variable
Cost
Fixed
Cost

324 
Chapter 7 Spreadsheet Models
required. In general, it is best to define notation for every node in the influence diagram. 
Let us define the following:
 
q 5 quantity (number of units) required
 
FC 5 the fixed cost of manufacturing
 
VC 5 the per-unit variable cost of manufacturing
 
TMC(q) 5 total cost to manufacture q units
The cost-volume model for producing q units of the Viper can then be written as  follows:
 
TMC(q) 5 FC 1 (VC 3 q) 
(7.1)
For the Viper, FC 5 $234,000 and VC 5 $2, so that equation (7.1) becomes
TMC(q) 5 $234,000 1 $2q
Once a quantity required (q) is established, equation (7.1), now populated with the 
data for the Viper, can be used to compute the total manufacturing cost. For example, 
the  decision to produce q 5 10,000 units would result in a total cost of TMC(10,000) 5 
$234,000 1 $2(10,000) 5 $254,000.
Similarly, a mathematical model for purchasing q units is as follows. Let P 5 the per-
unit purchase cost and TPC(q) 5 the total cost to outsource or purchase q units:
 
TPC(q) 5 Pq 
(7.2)
For the Viper, since P 5 $3.50, equation (7.2) becomes
TPC(q) 5 $3.5q
Thus, the total cost to outsource 10,000 units of the Viper is TPC(10,000) 5 3.5(10,000) 5 
$35,000.
We can now state mathematically the savings associated with outsourcing. Let S(q) 5 
the savings due to outsourcing, that is, the difference between the total cost of manufactur-
ing q units and the total cost of buying q units.
 
S(q) 5 TMC(q) 2 TPC(q) 
(7.3)
In summary, Nowlin’s decision problem is whether to manufacture or outsource the de-
mand for its Viper product next year. Because management does not yet know the required 
demand, the key question is, “For what quantities is it more cost-effective to outsource 
rather than produce the Viper?” Mathematically, this question is, “For what values of q 
is S(q) . 0?” Next we discuss a spreadsheet implementation of our conceptual and math-
ematical models that will help us answer this question.
Spreadsheet Design and Implementing  
the Model in a Spreadsheet
There are several guiding principles for how to build a spreadsheet so that it is easily used by 
others and the risk of error is mitigated. In this section, we discuss some of those principles 
and illustrate the design and construction of a spreadsheet model using the Nowlin Plastics 
make-versus-buy decision.
In the construction of a spreadsheet model, it is helpful to categorize its components. 
For the Nowlin Plastics problem, we have defined the following components (correspond-
ing to the nodes of the influence diagram in Figure 7.2):
 
q 5 number of units required
 
FC 5 the fixed cost of manufacturing
 
VC 5 the per-unit variable cost of manufacturing
 
TMC(q) 5 total cost to manufacture q units
 
P 5 the per-unit purchase cost 
 
TPC(q) 5 the total cost to purchase q units
S(q) 5 the savings from outsourcing q units

 
7.1 Building Good Spreadsheet Models 
325
Several points are in order. Some of these components are a function of other compo-
nents (TMC, TPC, and S), and some are not (q, FC, VC, and P). TMC, TPC, and S will 
be formulas involving other cells in the spreadsheet model, whereas q, FC, VC, and P 
will just be entries in the spreadsheet. Furthermore, the value we can control or choose is 
q. In our analysis, we seek the value of q, such that S(q) . 0; that is, the savings associ-
ated with outsourcing is positive. The number of Vipers to make or buy for next year is 
really a decision Nowlin gets to make. So we will treat q somewhat differently than we 
will FC, VC, and P in the spreadsheet model, and we refer to the quantity q as a decision 
variable. FC, VC, and P are measurable factors that define characteristics of the process 
we are modelling and so are uncontrollable inputs to the model, which we refer to as 
parameters of the model. 
Figure 7.3 shows a spreadsheet model for the Nowlin Plastics make-versus-buy 
 decision. 
Note that q, FC, VC, and 
P each are the beginning 
of a path in the influence 
diagram in Figure 7.2. In 
other words, they have no 
inward pointing arrows.
To display the formulas in a 
spreadsheet, hold down the 
Ctrl key and then press ,. 
To revert to the display of 
the numbers, repeat the 
process (hold down the Ctrl 
key and press ,).
FIGURE 7.3   NOWLIN PLASTICS MAKE-VERSUS-BUY SPREADSHEET MODEL
A
B
C
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
234000
Model
Quantity
2
Outsourcing Cost per Unit
3.5
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
19
16
6
10000
Total Cost to Produce
=B4+B11*B5
Total Cost to Outsource
=B7*B11
Savings due to Outsourcing
=B13–B15
A
B
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$234,000.00
Model
Quantity
$2.00
Outsourcing Cost per Unit
$3.50
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
19
16
6
10,000
Total Cost to Produce
$254,000.00
Total Cost to Outsource
$35,000.00
Savings due to Outsourcing
$219,000.00
file
WEB
Nowlin

326 
Chapter 7 Spreadsheet Models
 Column A is reserved for labels, including cell A1 where we have named the model 
“Nowlin Plastics.” The input parameters (FC, VC, and P) are placed in cells B4, B5, and B7, 
respectively. We offset P from FC and VC because it is for outsourcing. We have created 
a parameters section in the upper part of the sheet. Below the parameters section, we have 
created the Model section. The first entry in the Model section is the quantity q, which is 
the number of units of Viper produced or purchased in cell B11, and shaded it to signify 
that this is a decision variable. We have placed the formulas corresponding to equations 
(7.1)–(7.3) in cells B13, B15, and B17. Cell B13 corresponds to equation (7.1), cell B15 
to (7.2), and cell B17 to (7.3).
In cell B11 of Figure 7.3, we have set the value of q to 10,000 units. The model shows 
that the cost to manufacture 10,000 units is $254,000, the cost to purchase the 10,000 units 
is $35,000, and the savings from outsourcing is $219,000. We see that, at a quantity of 
10,000 units, it is better to incur the higher variable cost ($3.50 versus $2) than to manu-
facture and have to incur the additional fixed cost of $234,000. It will take a value of q 
larger than 10,000 units to make up the fixed cost incurred when Nowlin manufactures 
the product. At this point, we could increase the value of q, by placing a value higher than 
10,000 in cell B11 and see how much the savings in cell B17 decreases, doing this until the 
savings are close to zero. This is called a trial-and-error approach. Fortunately, Excel has 
what-if analysis tools that will help us use our model to further analyze the problem. We 
will discuss these what-if analysis tools in Section 7.2. Before doing so, let us first review 
what we have learned in constructing the Nowlin spreadsheet model.
The general principles of spreadsheet model design and construction are: 
● 
Separate the parameters from the model.
● 
Document the model, and use proper formatting and color as needed.
● 
Use simple formulas. 
Let us discuss the general merits of each of these points.
Separate the parameters from the model Separating the parameters from the model 
enables the user to update the model parameters without the risk of mistakenly creating an 
error in a formula. For this reason, it is good practice to have a parameters section at the top 
of the spreadsheet. A separate model section should contain all calculations. For a what-if 
model or an optimization model, some cells in the model section might also correspond 
to controllable inputs or decision variables (values that are not parameters or calculations 
but are the values we decide). The Nowlin model in Figure 7.3 is an example of this. The 
parameters section is in the upper part of the spreadsheet, followed by the model section, 
below which are the calculations and a decision cell (B11 for q in our model). Cell B11 is 
shaded gray to signify that it is a decision cell. 
Document the model and use proper formatting and color as needed A good 
spreadsheet model is well documented. Clear labels and proper formatting and alignment 
facilitate navigation and understanding. For example, if the values in a worksheet are cost, 
currency formatting should be used. Also, no cell with content should be unlabeled. A new 
user should be able to easily understand the model and its calculations. If color makes a 
model easier to understand and navigate, use it for cells and labels.
Use simple formulas Clear, simple formulas can reduce errors and make maintaining the 
spreadsheet easier. Long and complex calculations should be divided into several cells. This 
makes the formula easier to understand and easier to edit. Avoid using numbers in a formula 
(separate the data from the model). Instead, put the number in a cell in the parameters sec-
tion of your worksheet and refer to the cell location in the formula. Building the formula in 
this manner avoids having to edit the formula for a simple data change. For example, equa-
tion (7.3), the savings due to outsourcing, can be calculated as follows: S(q) 5 TMC(q) 2 
TPC(q) 5 FC 1 (VC)q 2 Pq 5 FC 1 (VC 2 P)q. Since VC 2 P 5 3.50 2 2 5 1.50, we 
As described in Appendix A,  
Excel formulas and func-
tions always begin with an 
equal sign.

 
7.2 What-If Analysis 
327
could have just entered the following formula in a single cell: 5234,000 21.50 * B11. This 
is a very bad idea because if any of the input data change, the formula must be edited. Fur-
thermore, the user would not know the values of VC and P, only that, for the current values, 
the difference is 1.50. The approach in Figure 7.3 is more transparent, simpler, lends itself 
better to analysis of changes in the parameters, and is less likely to contain errors.
NOTES AND COMMENTS
1. Some users of influence diagrams recommend 
using different symbols for the various types of 
model entities. For example, circles might de-
note known inputs, ovals might denote uncer-
tain inputs, rectangles might denote decisions 
or controllable inputs, triangles might denote 
calculations, and so forth.
2. The use of color in a spreadsheet model is an effec-
tive way to draw attention to a cell or set of cells. 
For example, we shaded cell B11 in Figure 7.3 to 
draw attention to the fact that q is a controllable 
input. However, avoid using too much color. Over-
doing it may overwhelm users and actually nega-
tively impact their ability to understand the model.
What-If analysis
Excel offers a number of tools to facilitate what-if analysis. In this section we introduce 
two such tools, Data Tables and Goal Seek. Both of these tools are designed to rid the user 
of the tedious manual trial-and-error approach to analysis. Let us see how these two tools 
can help us analyze Nowlin’s make-versus-buy decision.
Data Tables
An Excel Data Table quantifies the impact of changing the value of a specific input on 
an output of interest. Excel can generate either a one-way data table, which summarizes 
a single input’s impact on the output, or a two-way data table, which summarizes two 
inputs’ impact on the output.
Let us consider how savings due to outsourcing changes as the quantity of Vipers 
required changes. This should help us answer the question, “For which values of q is out-
sourcing more cost-effective?” A one-way data table changing the value of quantity and 
reporting savings due to outsourcing would be very useful. We will use the previously 
developed Nowlin spreadsheet for this analysis.
The first step in creating a one-way data table is to construct a sorted list of the values 
you would like to consider for the input. Let us investigate the quantity q over a range 
from 0 to 300,000 in increments of 25,000 units. Figure 7.4 shows we have entered these 
data in cells D5 through D17, with a column label in D4. This column of data is the set 
of values that Excel will use as inputs for q. Since the output of interest is savings due 
to outsourcing (located in cell B17), we have entered the formula 5B17 in cell E4. In 
general, set the cell to the right of the label to the cell location of the output variable of 
interest. Once the basic structure is in place, we invoke the Data Table tool using the 
following steps:
Step 1. Select cells D4:E17
Step 2. Click the DATA tab in the Ribbon
Step 3. Click What-If Analysis in the Data Tools group, and select Data Table
Step 4.  When the Data Table dialog box appears, enter B11 in the Column input 
cell: box
 
Click OK
7.2
Entering B11 in the Column 
input cell: box indicates 
that the column of data 
corresponds to different 
values of the input located 
in cell B11.

328 
Chapter 7 Spreadsheet Models
As shown in Figure 7.5, the table will be populated with the value of savings due to 
outsourcing for each value of quantity of Vipers in the table. For example, when q 5 25,000 
we see that S(25,000) 5 $196,500, and when q 5 250,000, S(250,000) 5 2$141,000. A 
negative value for savings due to outsourcing means that manufacturing is cheaper than 
outsourcing for that quantity.
FIGURE 7.4   THE INPUT FOR CONSTRUCTING A ONE-WAY DATA TABLE FOR NOWLIN PLASTICS
A
B
C
F
G
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$219,000.00
Model
Quantity
Outsourcing Cost per Unit
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6
Total Cost to Produce
Total Cost to Outsource
Savings due to Outsourcing
$234,000.00
$2.00
$3.50
10,000
$254,000.00
$35,000.00
$219,000.00
Quantity
0
50,000
25,000
75,000
100,000
125,000
175,000
225,000
275,000
150,000
200,000
250,000
300,000
D
E
FIGURE 7.5   RESULTS OF ONE-WAY DATA TABLE FOR NOWLIN PLASTICS
A
B
C
D
E
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$234,000.00
$219,000.00
Model
Quantity
$2.00
Outsourcing Cost per Unit
$3.50
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6
10,000
Total Cost to Produce
$254,000.00
Total Cost to Outsource
$35,000.00
Savings due to Outsourcing
$219,000.00
Quantity
0
50,000
25,000
75,000
100,000
125,000
175,000
225,000
275,000
150,000
200,000
250,000
300,000
$196,500
$234,000
$159,000
$121,500
$84,000
$9,000
–$66,000
–$141,000
$46,500
–$28,500
–$103,500
–$178,500
–$216,000
Nowlin Plastics

 
7.2 What-If Analysis 
329
We have learned something very valuable from this table. Not only have we quanti-
fied the savings due to outsourcing for a number of quantities, we know too that, for 
quantities of 150,000 units or less, outsourcing is cheaper than manufacturing and that, 
for quantities of 175,000 units or more, manufacturing is cheaper than outsourcing. 
Depending on Nowlin’s confidence in their demand forecast for the Viper product for 
next year, we have likely satisfactorily answered the make-versus-buy question. If, for 
example, management is highly confident that demand will be at least 200,000 units of 
Viper, then clearly they should manufacture the Viper rather than outsource. If manage-
ment believes that Viper demand next year will be close to 150,000 units, they might 
still decide to manufacture rather than outsource. At 150,000 units, the savings due to 
outsourcing is only $9,000. That might not justify outsourcing if quality assurance of 
the outsource firm is not at an acceptable level. We have provided management valuable 
information that they may use to decide whether to make or buy. Next we illustrate how 
to construct a two-way data table.
Suppose that Nowlin has now received five different bids on the per-unit cost for out-
sourcing the production of the Viper. Clearly, the lowest bid provides the greatest savings. 
However, the selection of the outsource firm—if Nowlin decides to outsource—will de-
pend on many factors, including reliability, quality, and on-time delivery. So it would be 
instructive to quantify the differences in savings for various quantities and bids. The five 
current bids are $2.89, $3.13, $3.50, $3.54, and $3.59. We may use the Excel Data Table 
to construct a two-way data table with quantity as a column and the five bids as a row, as 
shown in Figure 7.6.
In Figure 7.6, we have entered various quantities in cells D5 through D17, as in the 
one-way table. These correspond to cell B11 in our model. In cells E4 through I4, we 
have entered the bids. These correspond to B7, the outsourcing cost per unit. In cell D4, 
above the column input values and to the left of the row input values, we have entered 
FIGURE 7.6   THE INPUT FOR CONSTRUCTING A TWO-WAY DATA TABLE FOR NOWLIN PLASTICS
A
B
C
J
K
M
L
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$234,000.00
$2.89 $3.13 $3.50 $3.54
$3.59
Model
Quantity
$2.00
Outsourcing Cost per Unit
$3.50
10,000
Total Cost to Produce
$254,000.00
Total Cost to Outsource
$35,000.00
Savings due to Outsourcing
$219,000.00
$219,000.00
0
50,000
25,000
75,000
100,000
125,000
175,000
225,000
275,000
150,000
200,000
250,000
300,000
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
19
16
6
D
E
F
G
H
I

330 
Chapter 7 Spreadsheet Models
the formula 5B17, the location of the output of interest, in this case, savings due to 
 outsourcing. Once the table inputs have been entered into the spreadsheet, we perform 
the following steps to construct the two-way Data Table.
Step 1. Select cells D4:I17
Step 2. Click the DATA tab in the Ribbon
Step 3. Click What-If Analysis in the Data Tools group, and select Data Table
Step 4. When the Data Table dialog box appears:
Enter B7 in the Row input cell: box
Enter B11 in the Column input cell: box
Click OK
Figure 7.6 shows the selected cells and the Data Table dialog box. The results are 
shown in Figure 7.7.
We now have a table that shows the savings due to outsourcing for each combination 
of quantity and bid price. For example, for 75,000 Vipers at a cost of $3.13 per unit, the 
savings from buying versus manufacturing the units is $149,250. We can also see the range 
for the quantity for each bid price that results in a negative savings. For these quantities and 
bid combinations, it is better to manufacture than to outsource.
Using the Data Table allows us to quantify the savings due to outsourcing for the quan-
tities and bid prices specified. However, the table does not tell us the exact quantity where 
the transition occurs from outsourcing being cheaper to manufacturing being cheaper. For 
example, although it is clear from the table that, for a bid price of $3.50 the savings due 
to outsourcing goes from positive to negative at some quantity between 150,000 units and 
175,000 units, we know only that this transition occurs somewhere in that range. As we 
 illustrate next, the what-if analysis tool Goal Seek can tell us the precise quantity where 
this transition occurs.
FIGURE 7.7   RESULTS OF TWO-WAY DATA TABLE FOR NOWLIN PLASTICS
A
B
C
D
E
F
G
H
I
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$234,000.00
$2.89
Model
Quantity
$2.00
Outsourcing Cost per Unit
$3.50
10,000
Total Cost to Produce
$254,000.00
Total Cost to Outsource
$35,000.00
Savings due to Outsourcing
$219,000.00
$219,000.00
0
50,000
25,000
75,000
100,000
125,000
175,000
225,000
275,000
150,000
200,000
250,000
300,000
$234,000
$189,500
$211,750
$167,250
$145,000
$122,750
$78,250
$33,750
–$10,750
$100,500
$56,000
$11,500
–$33,000
$3.13
$234,000
$177,500
$205,750
$149,250
$121,000
$92,750
$36,250
–$20,250
–$76,750
$64,500
$8,000
–$48,500
–$105,000
$3.50
$234,000
$159,000
$196,500
$121,500
$84,000
$46,500
–$28,500
–$103,500
–$178,500
$9,000
–$66,000
–$141,000
–$216,000
$3.54
$234,000
$157,000
$195,500
$118,500
$80,000
$41,500
–$35,500
–$112,500
–$189,500
$3,000
–$74,000
–$151,000
–$228,000
$3.59
$234,000
$154,500
$194,250
$114,750
$75,000
$35,250
–$44,250
–$123,750
–$203,250
–$4,500
–$84,000
–$163,500
–$243,000
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6

 
7.2 What-If Analysis 
331
Goal Seek
Excel’s Goal Seek tool allows the user to determine the value of an input cell that will cause 
the value of a related output cell to equal some specified value (the goal). In the case of 
Nowlin Plastics, suppose we want to know the value of the quantity of Vipers where it be-
comes more cost effective to manufacture rather than outsource. For example, we see from 
the table in Figure 7.7 that, for a bid price of $3.50 and some quantity between 150,000 units 
and 175,000 units, savings due to outsourcing goes from positive to negative. Somewhere 
in this range of quantity, the savings due to outsourcing is zero, and that is the point where 
Nowlin would be indifferent to manufacturing and outsourcing. We may use Goal Seek to 
find the quantity of Vipers that satisfies the goal of zero savings due to outsourcing for a 
bid price of $3.50. The following steps describe how to use Goal Seek to find this point.
Step 1. Click the DATA tab in the Ribbon
Step 2. Click What-If Analysis in the Data Tools group, and select Goal Seek
Step 3. When the Goal Seek dialog box appears (Figure 7.8):
Enter B17 in the Set cell: box
Enter 0 in the To value: box
Enter B11 in the By changing cell: box
Click OK
Step 4. When the Goal Seek Status dialog box appears, click OK
The completed Goal Seek dialog box is shown in Figure 7.8.
The results from Goal Seek are shown in Figure 7.9. The savings due to outsourcing 
in cell B17 is zero, and the quantity in cell B11 has been set by Goal Seek to 156,000. 
When the annual quantity required is 156,000, it costs $564,000 either to manufacture the 
product or to purchase it. We have already seen that lower values of the quantity required 
favor outsourcing. Beyond the value of 156,000 units it becomes cheaper to manufacture 
the product.
FIGURE 7.8   GOAL SEEK DIALOG BOX FOR NOWLIN PLASTICS
A
B
C
E
D
F
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$234,000.00
Model
Quantity
$2.00
Outsourcing Cost per Unit
$3.50
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6
10,000
Total Cost to Produce
$254,000.00
Total Cost to Outsource
$35,000.00
Savings due to Outsourcing
$219,000.00

332 
Chapter 7 Spreadsheet Models
Some Useful excel Functions for Modeling
In this section we use several examples to introduce additional Excel functions that have 
proven useful in modeling decision problems. Many of these functions will be used in the 
chapters on optimization, simulation, and decision analysis.
SUM and SUMPRODUCT
Two very useful functions are SUM and SUMPRODUCT. The SUM function adds up all 
of the numbers in a range of cells. The SUMPRODUCT function returns the sum of the 
products of elements in a set of arrays. As we shall see in Chapter 8, SUMPRODUCT is 
very useful for linear optimization models.
7.3
FIGURE 7.9   RESULTS FROM GOAL SEEK FOR NOWLIN PLASTICS
A
B
C
E
D
F
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
$234,000.00
Model
Quantity
$2.00
Outsourcing Cost per Unit
$3.50
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6
156,000
Total Cost to Produce
$546,000.00
Total Cost to Outsource
$546,000.00
Savings due to Outsourcing
$0.00
NOTES AND COMMENTS
1. We emphasize the location of the reference 
to the desired output in a one-way versus a 
two-way Data Table. For a one-way table, the 
reference to the output cell location is placed 
in the cell above and to the right of the col-
umn of input data so that it is in the cell just 
to the right of the label of the column of input 
data. For a two-way table, the reference to 
the output cell location is placed above the 
column of input data and to the left of the row 
input data.
2. Notice that in Figures 7.5 and 7.7, the tables are 
formatted as currency. This must be done manu-
ally after the table is constructed using the options 
in the Number group under the HOME tab in the 
Ribbon. It also a good idea to label the rows and 
the columns of the table.
3. For very complex functions, Goal Seek might 
not converge to a stable solution. Trying sev-
eral different initial values (the actual value in 
the cell referenced in the By changing cell: 
box) when invoking Goal Seek may help.

 
7.3 Some Useful Excel Functions for Modeling 
333
Let us illustrate the use of SUM and SUMPRODUCT by considering a transporta-
tion problem faced by Foster Generators. This problem involves the transportation of a 
product from three plants to four distribution centers. Foster Generators operates plants in 
 Cleveland, Ohio; Bedford, Indiana; and York, Pennsylvania. Production capacities for the 
three plants over the next three-month planning period are known.
The firm distributes its generators through four regional distribution centers located in 
Boston, Massachusetts; Chicago, Illinois; St. Louis, Missouri; and Lexington, Kentucky. 
Foster has forecasted demand for the three-month period for each of the distribution cen-
ters. The per-unit shipping cost from each plant to each distribution center is also known. 
Management would like to determine how much of its production should be shipped from 
each plant to each distribution center.
A transportation analyst developed a what-if spreadsheet model to help Foster de-
velop a plan for how to ship its generators from the plants to the distribution centers to 
minimize cost. Of course, capacity at the plants must not be exceeded, and forecasted 
demand must be satisfied at each of the four distribution centers. The what-if model is 
shown in Figure 7.10.
The parameters section is rows 2 through 10. Cells B5 through E7 contain the per-
unit shipping cost from each origin (plant) to each destination (distribution center). For 
example, it costs $2.00 to ship one generator from Bedford to St. Louis. The plant capaci-
ties are given in cells F5 through F7, and the distribution center demands appear in cells 
B8 through E8.
The model is in rows 11 through 20. Trial values of shipment amounts from each plant 
to each distribution center appear in the shaded cells, B17 through E19. The total cost of 
shipping for this proposed plan is calculated in cell B13 using the SUMPRODUCT func-
tion. The general form of the SUMPRODUCT function is:
 
5SUMPRODUCT(array1, array2)
The function pairs each element of the first array with its counterpart in the second ar-
ray, multiplies the elements of the pairs together, and adds the results. In cell B13, 
5SUMPRODUCT(B5:E7,B17:E19) pairs the per-unit cost of shipping for each origin-
destination pair with the proposed shipping plan for that and adds their products:
 
B5*B17 1 C5*C17 1 D5*D17 1 E5*E17 1 B6*B18 1 . . . . . 1 E7*E19
In cells F17 through F19, the SUM function is used to add up the amounts shipped for 
each plant. The general form of the SUM function is
 
5SUM(range)
where range is a range of cells. For example, the function in cell F17 is 5SUM(B17:E17), 
which adds the values in B17, C17, D17, and E17: 5000 1 0 1 0 1 0 5 5000. The 
SUM function in cells B20 through E20 does the same for the amounts shipped to each 
 distribution center.
By comparing the amounts shipped from each plant to the capacity for that plant, we 
see that no plant violates its capacity. Likewise, by comparing the amounts shipped to 
each distribution center to the demand at that center, we see that all demands are met. The 
total shipping cost for the proposed plan is $54,500. Is this the lowest-cost plan? It is not 
clear. We will revisit the Foster Generators problem in Chapter 8, where we discuss linear 
optimization models.
IF and COUNTIF
Gambrell Manufacturing produces car stereos. Stereos are composed of a variety of com-
ponents that the company must carry in inventory to keep production running smoothly. 
The arrays that appear as 
arguments to the SUM-
PRODUCT function must 
be of the same dimension. 
For example, in the Foster 
Generator model, B5:E7 is 
an array of three rows and 
four columns. B17:E19 is 
an array of the same dimen-
sions.
Conditional formatting 
(which was discussed in 
Chapter 2) can be used to 
facilitate visual identifica-
tion of the violation of a 
plant’s capacity by altering 
the color of the amount 
shipped from a plant when 
the amount shipped exceeds 
the plant’s capacity.

334 
Chapter 7 Spreadsheet Models
However, because inventory can be a costly investment, Gambrell generally likes to keep its 
components inventory to a minimum. To help monitor and control its inventory, Gambrell 
uses an inventory policy known as an order-up-to policy.
The order-up-to policy is as follows. Whenever the inventory on hand drops below a 
certain level, enough units are ordered to return the inventory to that predetermined level. 
FIGURE 7.10   WHAT-IF MODEL FOR FOSTER GENERATORS
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
A
B
C
D
E
F
G
Foster Generators
5000
6000
2500
6
3
5
1500
7
2
4
2000
Parameters
Shipping Cost/Unit
Origin
Cleveland
Bedford
York
Demand
Model
Total Cost
Origin
Cleveland
Bedford
York
Total
Boston
Chicago
Destination
St. Louis
Lexington
Supply
Boston
Chicago
St. Louis
Lexington
Total
2
6
3
=SUMPRODUCT(B5:E7,B17:E19)
2
5
5
4000
6000
Destination
=SUM(B17:E17)
=SUM(B18:E18)
=SUM(B19:E19)
=SUM(C17:C19)
=SUM(B17:B19)
=SUM(D17:D19)
=SUM(E17:E19)
5000
1000
0
0
4000
0
0
1000
1000
0
0
1500
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
A
B
C
D
E
F
G
Foster Generators
5000
6000
2500
Parameters
Shipping Cost/Unit
Origin
Cleveland
Bedford
York
Demand
Model
Total Cost
Origin
Cleveland
Bedford
York
Total
Boston
Chicago
Destination
St. Louis
Lexington
Supply
Boston
5000
1000
0
0
4000
0
0
1000
1000
0
0
1500
Chicago
St. Louis
Lexington
Total
$2.00
$6.00
$3.00
$54,500.00
6000
$5.00
$5.00
$2.00
4000
$4.00
$2.00
$7.00
2000
$5.00
$3.00
$6.00
1500
Destination
5000
6000
2500
4000
6000
2000
1500

 
7.3 Some Useful Excel Functions for Modeling 
335
If the current number of units in inventory, denoted by H, drops below M units, enough 
inventory is ordered to get the level back up to M units. M is called the order-up-to point. 
Stated mathematically, if Q is the amount we order, then
 
Q 5 M2H
An inventory model for Gambrell Manufacturing appears in Figure 7.11. In the 
upper half of the worksheet, the component ID number, inventory on hand (H), order-
up-to point (M), and cost per unit are given for each of four components. Also given 
in this sheet is the fixed cost per order. The fixed cost is interpreted as follows: Each 
time a component is ordered, it costs Gambrell $120 to process the order. The fixed 
cost of $120 is incurred whenever an order is placed, regardless of how many units 
are ordered.
The model portion of the worksheet calculates the order quantity for each component. 
For example, for component 570, M 5 100 and H 5 5, so Q 5 M 2 H 5 100 2 5 5 95. 
For component 741, M 5 70 and H 5 70 and no units are ordered because the on-hand 
inventory of 70 units is equal to the order-up-to point of 70. The calculations are similar 
for the other two components.
Depending on the number of units ordered, Gambrell receives a discount on the cost 
per unit. If 50 or more units are ordered, there is a quantity discount of 10 percent on 
every unit purchased. For example, for component 741, the cost per unit is $4.50, and 
95 units are ordered. Because 95 exceeds the 50-unit requirement, there is a 10 percent 
discount, and the cost per unit is reduced to $4.50 2 0.1($4.50) 5 $4.50 2 $0.45 5 
$4.05. Not including the fixed cost, the cost of goods purchased is then $4.05(95) 5 
$384.75.
The Excel functions used to perform these calculations are shown in Figure 7.11 (for 
clarity, we show formulas for only the first three columns). The IF function is used to cal-
culate the purchase cost of goods for each component in row 17. The general form of the 
IF function is
 
5IF(condition, result if condition is true, result if condition is false)
For example, in cell B17 we have 5IF(B16 >5 $B$10, $B$11*B6, B6)*B16. This state-
ment says that, if the order quantity (cell B16) is greater than or equal to minimum amount 
required for a discount (cell B10), then the cost per unit is B11*B6 (there is a 10 percent 
discount, so the cost is 90 percent of the original cost); otherwise, there is no discount, 
and the cost per unit is the amount given in cell B6. The cost per unit computed by the IF 
function is then multiplied by the order quantity (B16) to obtain the total purchase cost of 
component 570. The purchase cost of goods for the other components are computed in a 
like manner.
The total cost in cell B23 is the sum of the total fixed ordering costs (B21) and the total 
cost of goods (B22). Because we place three orders (one each for components 570, 578, 
and 755), the fixed cost of the orders is 3*120 5 $360.
The COUNTIF function in cell B19 is used to count how many times we order. In par-
ticular, it counts the number of components having a positive order quantity. The general 
form of the COUNTIF function (which was discussed in Chapter 2 for creating frequency 
distributions) is
 
5COUNTIF(range, condition)
The range is the range to search for the condition. The condition is the test to be counted when 
satisfied. In the Gambrell model in Figure 7.11, cell B19 counts the number of cells that are 
greater than zero in the range of cells B16:E16 via the syntax 5COUNTIF(B16:E16, “.0”). 
Note that quotes are required for the condition with the COUNTIF function. In the model, 

336 
Chapter 7 Spreadsheet Models
because only cells B16, C16, and E16 are greater than zero, the COUNTIF function in cell 
B19 returns 3.
As we have seen, IF and COUNTIF are powerful functions that allow us to make cal-
culations based on a condition holding (or not). There are other such conditional functions 
available in Excel. In a problem at the end of this chapter, we ask you to investigate one 
such function, the SUMIF function. Another conditional function that is extremely useful 
in modeling is the VLOOKUP function, which is illustrated with an example in the next 
section.
file
WEB
Gambrell
FIGURE 7.11   GAMBRELL MANUFACTURING COMPONENT ORDERING MODEL
570
5
100
4.5
120
50
0.9
=B5–B4
=IF(B16 >= $B$10, $B$11*B6,B6)*B16
=COUNTIF(B16:E16,“>0”)
=B19*B8
=SUM(B17:E17)
=SUM(B21:B22)
578
30
55
12.5
=C5–C4
=IF(C16 >= $B$10, $B$11*C6,C6)*C16
Gambrell Manufacturing
Parameters
Component ID
Inventory On-Hand
Order-up-to Point
Cost per Unit
Fixed Cost per Order
=B3
=C3
Component ID
Order Quantity
Cost of Goods
Total Number of Orders
Total Fixed Costs
Total Cost of Goods
Total Cost
Minimum Order Size for Discount
Discounted to
Model
1
2
3
4
5
6
7
8
9
10
11
12
15
13
14
16
17
18
19
20
21
22
24
23
A
B
C
Gambrell Manufacturing
Parameters
Component ID
50
90%
578
25
$312.50
741
0
$0.00
755
28
$116.20
741
70
70
$3.26
755
17
45
$4.15
578
30
55
$12.50
Inventory On-Hand
Order-up-to Point
Cost per Unit
Fixed Cost per Order
Component ID
Order Quantity
Cost of Goods
Total Number of Orders
Total Fixed Costs
Total Cost of Goods
Total Cost
Minimum Order Size for Discount
Discounted to
Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
24
16
17
18
19
20
21
22
23
A
570
5
100
$4.50
$120
570
95
$384.75
3
$360.00
$813.45
$1,173.45
B
E
C
D
Notice the use of absolute 
references to B10 and B11 
in row 17. As discussed in 
Appendix A, this facilitated 
copying from cell B17 to 
cells C17, D17, and E17.

 
7.3 Some Useful Excel Functions for Modeling 
337
VLOOKUP
The director of sales at Granite Insurance needs to award bonuses to her sales force based 
on sales performance. There are 15 salespeople, each with his or her own territory. Based 
on the size and population of the territory, each salesperson has a sales target for the year.
The measure of performance for awarding bonuses is the percentage achieved above the 
sales target. Based on this metric, a salesperson is placed into one of five bonus bands and 
awarded bonus points. After all salespeople are placed in a band and awarded points, each is 
awarded a percentage of the bonus pool, based on the percentage of the total points awarded. 
The sales director has created a spreadsheet model to calculate the bonuses to be awarded. 
The spreadsheet model is shown in Figure 7.12 (note that we have hidden rows 19–28).
 As shown in cell E3 in Figure 7.12, the bonus pool is $250,000 for this year. The bonus 
bands are in cells A7:C11. In this table, column A gives the lower limit of the bonus band, 
column B the upper limit, and column C the bonus points awarded to anyone in that bonus 
band. For example, salespeople who achieve a 56 percent above their sales target would be 
awarded 15 bonus points.
As shown in Figure 7.12, the name and percentage above the target achieved for each 
salesperson appear below the bonus-band table in columns A and B. In column C, the 
VLOOKUP function is used to look in the bonus band table and automatically assign the 
number of bonus points to each salesperson.
The VLOOKUP function allows the user to pull a subset of data from a larger table of 
data based on some criterion. The general form of the VLOOKUP function is
 
5VLOOKUP(value, table, index, range)
where 
 
value 5 the value to search for in the first column of the table
 
table 5 the cell range containing the table
 
index 5 the column in the table containing the value to be returned
 
range 5  TRUE if looking for the first approximate match of value and FALSE 
if looking for an exact match of value (We will explain the difference 
between approximate and exact matches in a moment.)
VLOOKUP assumes that the first column of the table is sorted in  ascending order.
The VLOOKUP function for salesperson Choi in cell C18 is as follows:
 
5 VLOOKUP(B18,$A$7:$C$11,3,TRUE)
This function uses the percentage above target sales from cell B18 and searches the first 
column of the table defined by A7:C11. Because the range is set to TRUE indicating a 
search for the first approximate match, Excel  searches in the first column of the table from 
the top until it finds a number strictly greater than the value of B18. B18 is 44 percent, and 
the first value in the table in column A larger than 44 percent is in cell A9 (51 percent). It 
then backs up one row (to row 8). In other words, it finds the last value in the first column 
less than or equal to 44 percent. Because a 3 is in the third argument of the VLOOKUP 
function, it takes the element in row 8 of the third column of the table, which is 10 bonus 
points. In summary, the VLOOKUP with range set to TRUE takes the first argument and 
searches the first column of the table for the last row that is strictly less than the first argu-
ment. It then selects from that row the element in the column number of the third argument. 
Once all salespeople are awarded bonus points based on VLOOKUP and the bonus-
band table, the total number of bonus points awarded is given in cell C30 using the SUM 
function. Each person’s bonus points as a percentage of the total awarded is calculated in 
column D, and in column E each person is awarded that percentage of the bonus pool. As a 
check, cells D30 and E30 give the total percentages and dollar amounts awarded.
If the range in the  VLOOKUP 
function is FALSE, the 
only change is that Excel 
searches for an exact match 
of the first argument in the 
first column of the data.

338 
Chapter 7 Spreadsheet Models
Numerous mathematical, logical, and financial functions are available in Excel. In ad-
dition to those discussed here, we will introduce you to some other functions, as needed, in 
examples and end-of-chapter problems. Having already discussed principles for building 
good spreadsheet models and after having seen a variety of spreadsheet models, we turn 
now to how to audit Excel models to ensure model integrity.
FIGURE 7.12   GRANITE INSURANCE BONUS MODEL
A
B
C
E
D
Granite Insurance Bonus Awards
Parameters
Bonus Bands to be awarded for percentage above target sales.
Bonus Pool
250000
Lower
Upper 
Limit
Limit
1
0
0.11
0.51
0.8
0.1
0.5
0.79
0.99
100
=VLOOKUP(B15,$A$7:$C$11,3,TRUE)
=VLOOKUP(B16,$A$7:$C$11,3,TRUE)
=VLOOKUP(B17,$A$7:$C$11,3,TRUE)
=VLOOKUP(B18,$A$7:$C$11,3,TRUE)
=VLOOKUP(B29,$A$7:$C$11,3,TRUE)
=SUM(C15:C29)
=C15/$C$30
=C16/$C$30
=C17/$C$30
=C18/$C$30
=C29/$C$30
=SUM(D15:D29)
Total
Points
Bonus 
0
10
15
25
40
Model
Barth
Benson
Last Name
% Above Target Sales
Bonus Points
% of Pool
=D15*$E$3
=D16*$E$3
=D17*$E$3
=D18*$E$3
=D29*$E$3
=SUM(E15:E29)
Bonus Amount
Capel
Choi
1
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6
29
31
30
Ruebush
0.83
0
1.18
0.44
0.85
A
B
C
E
D
1
Granite Insurance Bonus Awards
Parameters
Bonus Bands to be awarded for percentage above target sales.
Bonus Pool
$250,000
Lower Limit
Upper Limit
100%
0%
11%
51%
80%
10%
50%
79%
99%
2
3
4
5
7
8
9
10
11
12
13
14
15
17
18
16
6
10000%
25
0
40
10
25
295
8.5%
0.0%
13.6%
3.4%
8.5%
100%
Total
Bonus Points
0
10
15
25
40
Model
Barth
Benson
Last Name
% Above Target Sales
Bonus Points
% of Pool
$21,186.44
$0.00
$33,898.31
$8,474.58
$21,186.44
$250,000.00
Bonus Amount
Capel
Choi
29
30
31
Ruebush
83%
0%
118%
44%
85%
file
WEB
Granite

 
7.4 Auditing Spreadsheet Models 
339
 auditing Spreadsheet Models
EXCEL contains a variety of tools to assist you in the development and debugging of spread-
sheet models. These tools are found in the Formula Auditing group of the FORMULAS 
tab, as shown in Figure 7.13. Let us review each of the tools available in this group.
Trace Precedents and Dependents
After selecting cells, the Trace Precedents button creates arrows pointing to the selected 
cell from cells that are part of the formula in that cell. The Trace Dependents button, on the 
other hand, shows arrows pointing from the selected cell to cells that depend on the selected 
cell. Both of the tools are excellent for quickly ascertaining how parts of a model are linked.
An example of Trace Precedents is shown in Figure 7.14. Here we have opened 
the Foster Generators Excel file, selected cell B13, and clicked the Trace Precedents 
7.4
FIGURE 7.13   THE FORMULA AUDITING GROUP
Watch
Window
Show Formulas
Error Checking
Evaluate Formula
Formula Auditing
Trace Precedents
Trace Dependents
Remove Arrows
15
FIGURE 7.14   TRACE PRECEDENTS FOR FOSTER GENERATOR
A
C
D
E
F
G
Foster Generators
Parameters
Shipping Cost/Unit
Origin
Cleveland
Bedford
York
Demand
Model
Total Cost
Origin
Cleveland
Bedford
York
Total
Boston
Chicago
Destination
St. Louis
Lexington
Supply
Boston
Chicago
St. Louis
Lexington
Total
$54,500.00
$2.00
$6.00
$3.00
6000
Destination
1
2
3
4
5
6
7
8
9
10
11
12
14
15
16
17
18
19
20
5000
6000
2500
5000
6000
2500
6000
5000
1000
0
$5.00
$5.00
$2.00
4000
4000
0
4000
0
$4.00
$2.00
$7.00
2000
2000
0
1000
1000
$5.00
$3.00
$6.00
1500
1500
0
0
1500
B
13
21
22

340 
Chapter 7 Spreadsheet Models
 button in the Formula Auditing group. Recall that the cost in cell B13 is calculated 
as the SUMPRODUCT of the per-unit shipping cost and units shipped. In Figure 7.14, 
to show this relationship, arrows are drawn to these areas of the spreadsheet to cell 
B13. These arrows may be removed by clicking on the Remove Arrows button in the 
 Auditing Tools group.
An example of Trace Dependents is shown in Figure 7.15. We have selected cell E18, 
the units shipped from Bedford to Lexington, and clicked on the Trace Dependents but-
ton in the Formula Auditing group. As shown in Figure 7.15, units shipped from Bedford 
to Lexington impacts the cost function in cell B13, the total units shipped from Bedford 
given in cell F18, as well as the total units shipped to Lexington in cell E20. These arrows 
may be removed by clicking on the Remove Arrows button in the Auditing Tools group.
Trace Precedents and Trace Dependents can highlight errors in copying and formula 
construction by showing that incorrect sections of the worksheet are referenced.
Show Formulas
The Show Formulas button does exactly that. To see the formulas in a worksheet, simply 
click on any cell in the worksheet and then click on Show Formulas. You will see the for-
mulas residing in that worksheet. To revert to hiding the formulas, click again on the Show 
Formulas button. As we have already seen in our examples in this chapter, the use of Show 
Formulas allows you to inspect each formula in detail in its cell location.
Evaluate Formulas
The Evaluate Formula button allows you to investigate the calculations of a cell in great 
detail. As an example, let us investigate cell B17 of the Gambrell Manufacturing model 
FIGURE 7.15   TRACE DEPENDENTS FOR THE FOSTER GENERATORS MODEL
Origin
Cleveland
Bedford
York
Total
16
17
18
19
20
21
A
B
C
D
E
F
G
Foster Generators
5000
6000
2500
Parameters
Shipping Cost/Unit
Origin
Cleveland
Bedford
York
Demand
Model
Total Cost
Boston
Chicago
Destination
St. Louis
Lexington
Supply
Boston
Chicago
St. Louis
Lexington
Total
$54,500.00
$2.00
$6.00
$3.00
6000
$5.00
$5.00
$2.00
4000
$4.00
$2.00
$7.00
2000
$5.00
$3.00
$6.00
1500
Destination
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
5000
6000
2500
0
4000
0
4000
5000
1000
0
6000
0
1000
1000
2000
0
0
1500
1500
22

 
7.4 Auditing Spreadsheet Models 
341
(Figure 7.11). Recall that we are calculating cost of goods based on whether there is a 
quantity discount. We follow these steps:
Step 1. Select cell B17
Step 2. Click the FORMULAS tab in the Ribbon
Step 3. Click the Evaluate Formula button in the Formula Auditing group
Step 4.  When the Evaluate Formula dialog box appears (Figure 7.16), click the 
 Evaluate button
Step 5. Repeat Step 4 until the formula has been completely evaluated
Step 6. Click Close
Figure 7.17 shows the Evaluate Formula dialog box for cell B17 in the Gambrell Manu-
facturing spreadsheet model after four clicks of the Evaluate button.
The Evaluate Formula tool provides an excellent means of identifying the exact loca-
tion of an error in a formula.
Error Checking
The Error Checking button provides an automatic means of checking for mathematical 
errors within formulas of a worksheet. Clicking on the Error Checking button causes 
Excel to check every formula in the sheet for calculation errors. If an error is found, the 
Error Checking dialog box appears. An example for a hypothetical division by zero error 
is shown in Figure 7.18. From this box, the formula can be edited, the calculation steps 
can be observed (as in the previous section on Evaluate Formulas), or help can be obtained 
FIGURE 7.16   THE EVALUATE FORMULA DIALOG BOX FOR GAMBRELL MANUFACTURING
1
2
3
4
5
7
8
9
10
11
12
13
14
15
18
16
6
19
20
21
22
24
23
A
C
D
E
F
G
H
I
J
Gambrell Manufacturing
Component ID
Parameters
Inventory On-Hand
Order Up to Point
Cost per Unit
570
578
741
755
5
30
70
17
100
570
95
$384.75
$360.00
$813.45
$1,173.45
3
55
70
45
$4.50
$120
$12.50
$3.26
$4.15
Minimum Order Size for Discount
Discounted to
50
90%
Fixed Cost per Order
Model
Component ID
Cost of Goods
Total Number of Orders
Order Quantity
Total Fixed Costs
Total Cost
Total Cost of Goods
B
17

342 
Chapter 7 Spreadsheet Models
through the Excel help function. The Error Checking procedure is particularly helpful for 
large models where not all cells of the model are visible.
Watch Window
The Watch Window, located in the Formula Auditing group, allows the user to observe the 
values of cells included in the Watch Window box list. This is useful for large models when 
not all of the model is observable on the screen or when multiple worksheets are used. The 
user can monitor how the listed cells change with a change in the model without searching 
through the worksheet or changing from one worksheet to another.
A Watch Window for the Gambrell Manufacturing model is shown in Figure 7.19. The 
following steps were used to add cell B17 to the watch list:
Step 1. Click the FORMULAS tab in the Ribbon
Step 2.  Click Watch Window in the Formula Auditing group to display the Watch 
Window
Step 3. Click Add Watch. . .
Step 4. Select the cell you would like to add to the watch list (in this case B17)
FIGURE 7.17   THE EVALUATE FORMULA DIALOG BOX FOR GAMBRELL 
MANUFACTURING CELL B17 AFTER FOUR CLICKS OF THE 
EVALUATE BUTTON
FIGURE 7.18   THE ERROR CHECKING DIALOG BOX FOR A DIVISION BY ZERO 
ERROR.

 
Glossary 
343
As shown in Figure 7.19, the list gives the workbook name, worksheet name, cell name 
(if used), cell location, cell value, and cell formula. To delete a cell from the watch list, 
click on the entry from the list, and then click on the Delete Watch button that appears in 
the upper part of the Watch Window.
The Watch Window, as shown in Figure 7.19, allows us to monitor the value of B17 as 
we make changes elsewhere in the worksheet. Furthermore, if we had other worksheets in 
this workbook, we could monitor changes to B17 of the worksheet even from these other 
worksheets. The Watch Window is observable regardless of where we are in any worksheet 
of a workbook.
Summary
In this chapter we discussed the principles of building good spreadsheet models, several 
what-if analysis tools, some useful Excel functions, and how to audit spreadsheet models. 
What-if spreadsheet models are important and popular analysis tools in and of themselves, 
but as we shall see in later chapters, they also serve as the basis for optimization and simu-
lation models.
We discussed how to use influence diagrams to structure a problem. Influence dia-
grams can serve as a guide to developing a mathematical model and implementing the 
model in a spreadsheet. We discussed the importance of separating the parameters from 
the model because it leads to simpler analysis and minimizes the risk of creating an error in 
a formula. In most cases, cell formulas should use cell references in their arguments rather 
 being “hardwired” with values. We also discussed the use of proper formatting and color 
to enhance the ease of use and understanding of a spreadsheet model.
We used examples to illustrate how Excel What-If Analysis tools Data Tables and Goal 
Seek can be used to perform detailed and efficient what-if analysis. We also discussed a 
number of Excel functions that are useful for business analytics. Finally, we discussed 
Excel Formula Auditing tools that may be used to debug and monitor spreadsheet models 
to ensure that they are error-free and accurate.
Glossary
What-if model A model designed to study the impact of changes in model inputs on model 
outputs.
Make-versus-buy decision A decision often faced by companies that have to decide 
whether they should manufacture a product or outsource its production to another firm.
FIGURE 7.19   THE WATCH WINDOW FOR CELL B17 OF THE GAMBRELL 
MANUFACTURING MODEL

344 
Chapter 7 Spreadsheet Models
Influence diagram A visual representation that shows which entities influence others in 
a model.
Parameter In a what-if model, a parameter refers to an uncontrollable model input. 
Decision variable A model input the decision maker can control.
Data Table An Excel tool that quantifies the impact of changing the value of a specific 
input on an output of interest.
One-Way Data Table An Excel Data Table that summarizes a single input’s impact on 
the output of interest.
Two-Way Data Table An Excel Data Table that summarizes two inputs’ impact on the 
output of interest.
Goal Seek An Excel tool that allows the user to determine the value for an input cell 
that will cause the value of a related output cell to equal some specified value, called 
the goal.
Problems
 1.  Cox Electric makes electronic components and has estimated the following for a new 
design of one of its products:
 
Fixed cost 5 $10,000
 
Material cost per unit 5 $0.15
 
Labor cost per unit 5 $0.10
 
Revenue per unit 5 $0.65
 
 These data are given in the file CoxElectric. Note that fixed cost is incurred regardless 
of the amount produced. Per-unit material and labor cost together make up the variable 
cost per unit. Assuming Cox Electric sells all that it produces, profit is calculated by sub-
tracting the fixed cost and total variable cost from total revenue.
a. Build an influence diagram that illustrates how to calculate profit.
b.  Using mathematical notation similar to that used for Nowlin Plastics, give a math-
ematical model for calculating profit.
c.  Implement your model from part b in Excel using the principles of good spreadsheet 
design.
d. If Cox Electric makes 12,000 units of the new product, what is the resulting profit? 
 2. Use the spreadsheet model constructed to answer Problem 1 to answer this problem. 
a.  Construct a one-way data table with production volume as the column input and profit 
as the output. Breakeven occurs when profit goes from a negative to a positive value, 
that is, breakeven is when total revenue 5 total cost, yielding a profit of zero. Vary 
production volume from 0 to 100,000 in increments of 10,000. In which interval of 
production volume does breakeven occur?
b.  Use Goal Seek to find the exact breakeven point. Assign Set cell: equal to the location 
of profit, To value: 5 0, and By changing cell: equal to the location of the production 
volume in your model.
 3.  Eastman Publishing Company is considering publishing an electronic textbook on spread-
sheet applications for business. The fixed cost of manuscript preparation, textbook design, 
and Web site construction is estimated to be $160,000. Variable processing costs are esti-
mated to be $6 per book. The publisher plans to sell access to the book for $46 each. 
a.  Build a spreadsheet model to calculate the profit/loss for a given demand. What profit 
can be anticipated with a demand of 3500 copies? 
file
WEB
CoxElectric

 
Problems 
345
b.  Use a data table to vary demand from 1000 to 6000 increments of 200 to assess the 
sensitivity of profit to demand. 
c.  Use Goal Seek to determine the access price per copy that the publisher must charge 
to break even with a demand of 3500 copies.
 4.  The University of Cincinnati Center for Business Analytics is an outreach center that col-
laborates with industry partners on applied research and continuing education in business 
analytics. One of the programs offered by the Center is a quarterly Business Intelligence 
Symposium. Each symposium features three speakers on the real-world use of analytics. 
Each corporate member of the center (there are currently ten) receives five free seats to 
each symposium. Nonmembers wishing to attend must pay $75 per person. Each attendee 
receives breakfast, lunch, and free parking. The following are the costs incurred for put-
ting on this event:
 
Rental cost for the auditorium 
$150
 
Registration processing 
$8.50 per person
 
Speaker costs 
3@$800 5 $2,400
 
Continental breakfast 
$4.00 per person
 
Lunch 
$7.00 per person
 
Parking 
$5.00 per person
a.  Build a spreadsheet model that calculates a profit or loss based on the number of 
nonmember registrants.
b.  Use Goal Seek to find the number of nonmember registrants that will make the event 
break even.
 5. Consider again the scenario described in Problem 4.
a.  The Center for Business Analytics is considering a refund policy for no-shows. No 
refund would be given for members who do not attend, but nonmembers who do 
not attend will be refunded 50 percent of the price. Extend the model you developed 
in Problem 4 for the Business Intelligence Symposium to account for the fact that, 
historically, 25 percent of members who registered do not show and 10 percent of 
registered nonmembers do not attend. The Center pays the caterer for breakfast and 
lunch based on the number of registrants (not the number of attendees). However, 
the Center pays for parking only for those who attend.What is the profit if each 
corporate member registers their full allotment of tickets and 127 nonmembers 
register? 
b.  Use a two-way data table to show how profit changes as a function of number 
of registered nonmembers and the no-show percentage of nonmembers. Vary 
the number of nonmember registrants from 80 to 160 in increments of 5 and 
the  percentage of nonmember no-shows from 10 to 30 percent in increments of 
2  percent.
 6.  Consider again Problem 3. Through a series of Web-based experiments, Eastman has cre-
ated a predictive model that estimates demand as a function of price. The predictive model 
is demand 5 4000 2 6p where p is the price of the e-book. 
a.  Update your spreadsheet model constructed for Problem 3 to take into account this 
demand function. 
b. Use Goal Seek to calculate the price that results in breakeven. 
c.  Use a data table that varies price from $50 to $400 in increments of $25 to find the 
price that maximizes profit.
 7.  Lindsay is 25 years old and has a new job in Web development. Lindsay wants to 
make sure she is financially sound in 30 years, so she plans to invest the same amount 
into a retirement account at the end of every year for the next 30 years. Construct a 

346 
Chapter 7 Spreadsheet Models
data table that will show Lindsay the balance of her retirement account for various 
levels of annual investment and return. Develop the two-way table for annual invest-
ment amounts of $5,000 to $20,000 in increments of $1,000 and for returns of 0 to 
12 percent in increments of 1 percent. Note that because Lindsay invests at the end of 
the year, there is no interest earned on that year’s contribution for the year in which 
she contributes.
 8.  Consider again Lindsay’s investment problem (Problem 7). The real value of Lind-
say’s account after 30 years of investing will depend on inflation over that period. In 
the Excel function 5NPV(rate, value1, value2, …), rate is called the discount rate, 
and value1, value2 etc. are incomes (positive) or expenditures (negative) over equal 
periods of time. Update your model previously developed in Problem 7 using the 
NPV function to get the net present value of Lindsay’s retirement fund. Construct a 
data table that shows the net present value of Lindsay’s retirement fund for various 
levels of return and inflation (discount rate). Use a data table to vary the return from 
0 to 12 percent in increments of 1 percent and the discount rate from 0 to 4 percent in 
increments of 1 percent to show the impact on the net present value. Hint: Calculate 
the total amount added to the account each year, and discount that stream of payments 
using the NPV function.
 9.  Newton Manufacturing produces scientific calculators. The models are N350, N450, and 
the N900. Newton has planned its distribution of these products around eight customer 
zones: Brazil, China, France, Malaysia, U.S. Northeast, U.S. Southeast, U.S. Midwest, 
and U.S. West. Data for the current quarter (volume to be shipped in thousands of units) 
for each product and each customer zone are given in the file Newton. Newton would like 
to know the total number of units going to each customer zone and also the total units of 
each product shipped. There are several ways to get this information from the data set. 
One way is to use the SUMIF function. 
 
 The SUMIF function extends the SUM function by allowing the user to add the values 
of cells meeting a logical condition. The general form of the function is
5SUMIF(test range, condition, range to be summed)
 
 The test range is an area to search to test the condition, and the range to be summed is 
the position of the data to be summed. So, for example, using the file Newton, we use the 
following function to get the total units sent to Malaysia:
5SUMIF(A3:A26,A3,C3:C26)
 
 Cell A3 contains the text “Malaysia”; A3:A26 is the range of customer zones; and 
C3:C26 are the volumes for each product for these customer zones. The SUMIF looks 
for matches of “Malaysia” in column A and, if a match is found, adds the volume to the 
total. Use the SUMIF function to get total volume by each zone and total volume by each 
product.
10.  Consider the transportation model in the file Williamson, which is very similar to the 
Foster Generators model discussed in the chapter. Williamson produces a single product 
and has plants in Atlanta, Lexington, Chicago, and Salt Lake City and warehouses in 
Portland, St. Paul, Las Vegas, Tuscon, and Cleveland. Each plant has a capacity, and each 
warehouse has a demand. Williamson would like to find a low-cost shipping plan. Mr. 
Williamson has reviewed the results and notices right away that the total cost is way out 
of line. Use the Formula Auditing Tools under the FORMULAS tab in Excel to find any 
errors in this model. Correct the errors. Hint: The model contains two errors. Be sure to 
check every formula.
file
WEB
Newton
file
WEB
Williamson

 
Problems 
347
11.  Professor Rao would like to accurately calculate the grades for the 58 students in his Op-
erations Planning and Scheduling class (OM 455). He has thus far constructed a spread-
sheet, part of which follows:
Section 001
Course Grading Scale Based on Course Average:
Last Name
Alt
Amini
Amoako
Apland
Bachman
Corder
Desi
Dransman
Duffuor
Finkel
Foster
28
38
Midterm
Score
70
95
82
45
68
91
87
60
80
97
90
Final
Score
56
91
80
78
45
98
74
80
93
98
91
Course
Average
Course
Grade
63.0
93.0
81.0
61.5
56.5
94.5
80.5
70.0
86.5
97.5
90.5
OM 455
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
A
B
C
D
E
Lower
Limit
Upper
Limit
Course
Grade
0
60
70
80
90
59
69
79
89
100
F
D
C
B
A
a.  The Course Average is calculated by weighting the Midterm Score and Final Score 
50 percent each. Use the VLOOKUP function with the table shown to generate the 
Course Grade for each student in cells E14 through E24.
b.  Use the COUNTIF function to determine the number of students receiving each letter 
grade.
12.  Richardson Ski Racing (RSR) sells equipment needed for downhill ski racing. One of 
RSR’s products is fencing used on downhill courses. The fence product comes in 150-foot 
rolls and sells for $215 per roll. However, RSR offers quantity discounts. The following 
table shows the price per roll depending on order size: 
Quantity Ordered
From 
To
Price per Roll
  1
 50
$215
 51
100
$195
101
200
$175
201
and up
$155
 
The file RSR contains 172 orders that have arrived for the coming six weeks.
a.  Use the VLOOKUP function with the preceding pricing table to determine the total 
revenue from these orders. 
b. Use the COUNTIF function to determine the number of orders in each price bin. 
file
WEB
OM455
file
WEB
RSR

348 
Chapter 7 Spreadsheet Models
13.  A put option in finance allows you to sell a share of stock at a given price in the future. There 
are different types of put options. A European put option allows you to sell a share of stock 
at a given price, called the exercise price, at a particular point in time after the purchase of 
the option. For example, suppose you purchase a six-month European put option for a share 
of stock with an exercise price of $26. If six months later, the stock price per share is $26 or 
more, the option has no value. If in six months the stock price is lower than $26 per share, 
then you can purchase the stock and immediately sell it at the higher exercise price of $26. If 
the price per share in six months is $22.50, you can purchase a share of the stock for $22.50 
and then use the put option to immediately sell the share for $26. Your profit would be the 
difference, $26 2 $22.50 5 $3.50 per share, less the cost of the option. If you paid $1.00 
per put option, then your profit would be $3.50 2 $1.00 5 $2.50 per share. 
a. Build a model to calculate the profit of this European put option. 
b.  Construct a data table that shows the profit per share for a share price in six months 
between $10 and $30 per share in increments of $1.00.
14.  Consider again Problem 13. The point of purchasing a European option is to limit the risk 
of a decrease in the per-share price of the stock. Suppose you purchased 200 shares of the 
stock at $28 per share and 75 six-month European put options with an exercise price of 
$26. Each put option costs $1. 
a.  Using data tables, construct a model that shows the value of the portfolio with options 
and without options for a share price in six months between $15 and $35 per share in 
increments of $1.00. 
b. Discuss the value of the portfolio with and without the European put options.
15.  The Camera Shop sells two popular models of digital cameras. The sales of these products 
are not independent; if the price of one increases, the sales of the other increases. In eco-
nomics, these two camera models are called substitutable products. The store wishes to 
establish a pricing policy to maximize revenue from these products. A study of price and 
sales data shows the following relationships between the quantity sold (N ) and price (P) 
of each model:
 
NA 5 195 2 0.6PA 1 0.25PB
 
NB 5 301 1 0.08PA 2 0.5PB
a. Construct a model for the total revenue and implement it on a spreadsheet.
b.  Develop a two-way data table to estimate the optimal prices for each product in order to 
maximize the total revenue. Vary each price from $250 to $500 in increments of $10.
16.  A few years back, Dave and Jana bought a new home. They borrowed $230,415 at an an-
nual fixed rate of 5.49 percent (15-year term) with monthly payments of $1,881.46. They 
just made their 25th payment, and the current balance on the loan is $208,555.87.
 
 Interest rates are at an all-time low, and Dave and Jana are thinking of refinancing to a 
new 15-year fixed loan. Their bank has made the following offer: 15-year term, 3.0 per-
cent, plus out-of-pocket costs of $2,937. The out-of-pocket costs must be paid in full at the 
time of refinancing.
 
 Build a spreadsheet model to evaluate this offer. The Excel function
5PMT(rate, nper, pv, fv, type)
 
 calculates the payment for a loan based on constant payments and a constant interest rate. 
The arguments of this function are
 
rate 5 the interest rate for the loan
 
nper 5 the total number of payments
 
pv 5 present value (the amount borrowed)
 
fv 5 future value [the desired cash balance after the last payment (usually 0)]
 
type 5 payment type (0 5 end of period, 1 5 beginning of the period)
 
 For example, for Dave and Jana’s original loan, there will be 180 payments (12*15 5 180), 
so we would use 5PMT(0.0549/12, 180, 230415,0,0) 5 $1,881.46. Note that  because 

 
Problems 
349
 payments are made monthly, the annual interest rate must be expressed as a monthly rate. 
Also, for payment calculations, we assume that the payment is made at the end of the 
month.
 
 The savings from refinancing occur over time, and therefore need to be discounted 
back to current dollars. The formula for converting K dollars saved t months from now to 
 current dollars is 
K
(1 1 r)t21
 
 where r is the monthly inflation rate. Assume that r 5 0.002 and that Dave and Jana make 
their payment at the end of each month.
 
 Use your model to calculate the savings in current dollars associated with the refi-
nanced loan versus staying with the original loan.
17.  Consider again the mortgage refinance problem faced by Dave and Jana in Problem 16. 
Assume that they have accepted the refinance offer of a 15-year loan at 3 percent interest 
rate with out-of-pocket expenses of $2,937. Recall that they are borrowing $208,555.87. 
Assume there is no prepayment penalty, so that any amount over the required payment 
is applied to the principle. Construct a model so that you can use Goal Seek to determine 
the monthly payment that will allow Dave and Jana to pay off the loan in 12 years. Do the 
same for 10 and 11 years. Which option for prepayment, if any, would you choose and 
why?
 
 Hint: Break each monthly payment up into interest and principal (the amount that is 
 deducted from the balance owed). Recall that the monthly interest that is charged is the 
monthly loan rate multiplied by the remaining loan balance.
18.  Floyd’s Bumpers has distribution centers in Lafayette, Indiana: Charlotte, North  Carolina; 
Los Angeles, California; Dallas, Texas; and Pittsburgh, Pennsylvania. Each distribution 
center carries all products sold. Floyd’s customers are auto repair shops and larger auto 
parts retail stores. You are asked to perform an analysis of the customer assignments to 
determine which of Floyd’s customers should be assigned to each distribution center. 
The rule for assigning customers to distribution centers is simple: A customer should be 
assigned to the closest center. The data file Floyds contains the distance from each of 
Floyd’s 1029 customers to each of the five distribution centers. Your task is to build a list 
that tells which distribution center should serve each customer. The following functions 
will be helpful:
5MIN(array)
 
 The MIN function returns the smallest value in a set of numbers. For example, if the 
range A1:A3 contains the values 6, 25, and 38, then the formula 5MIN(A1:A3) returns 
the number 6, because it is the smallest of the three numbers.
5MATCH(lookup_value, lookup_array, match type)
 
 The MATCH function searches for a specified item in a range of cells and returns the 
relative position of that item in the range. The  lookup_ value is the value to match, the 
lookup_array is the range of search, and match type indicates the type of match (use 0 for 
an exact match).
 
 For example, if the range A1:A3 contains the values 6, 25, and 38, then the formula 
5MATCH(25,A1:A3,0) returns the number 2, because 25 is the second item in the range.
 
5INDEX(array, column_num)
 
 The INDEX function returns the value of an element in a position of an array. For exam-
ple, if the range A1:A3 contains the values 6, 25, and 38, then the formula 5INDEX(A1: 
A3, 2) 5 25 because 25 is the value in the second position of the array A1:A3.
file
WEB
Floyds

350 
Chapter 7 Spreadsheet Models
 
 Hint: Create three new columns. In the first column, use the MIN function to calcu-
late the minimum distance for the customer in that row. In the second column use the 
MATCH function to find the position of the minimum distance. In the third column, use 
the position in the previous column with the INDEX function referencing the row of dis-
tribution center names to find the name of the distribution center that should service that 
customer.
19.  Refer to Problem 18. Floyd’s Bumpers pays a transportation company to ship its product 
in full truckloads to its customers. Therefore, the cost for shipping is a function of the dis-
tance traveled and a fuel surcharge (also on a per-mile basis). The cost per mile is $2.42, 
and the fuel surcharge is $0.56 per mile. The file FloydsMay contains data for shipments 
for the month of May (each record is simply the customer zip code for a given truckload 
shipment) as well as the distance table from the distribution centers to each customer. Use 
the MATCH and INDEX functions to retrieve the distance traveled for each shipment, and 
calculate the charge for each shipment. What is the total amount that Floyd’s Bumpers 
spends on these May shipments?
 
 Hint: The INDEX function may be used with a two-dimensional array: 5INDEX(array, 
row_num, column_num), where array is a matrix, row_num is the row number, and 
 column_num is the column position of the desired element of the matrix.
20.  An auto dealership is advertising that a new car with a sticker price of $35,208 is on 
sale for $25,995 if payment is made in full, or it can be financed at 0 percent interest for 
72 months with a monthly payment of $489. Note that 72 payments 3 $489 per pay-
ment 5 $35,208, which is the sticker price of the car. By allowing you to pay for the 
car in a series of payments (starting one month from now) rather than $25,995 now, the 
dealer is effectively loaning you $25,995. If you choose the 0 percent financing option, 
what is the effective interest rate that the auto dealership is earning on your loan? Hint: 
Discount the payments back to current dollars (see Problem 16 for a discussion of dis-
counting), and use Goal Seek to find the discount rate that makes the net present value 
of the payments 5 $25,995.
retirement plan
Tim is 37 years old and would like to establish a retirement plan. Develop a spreadsheet 
model that could be used to assist Tim with retirement planning. Your model should include 
the following input parameters:
Tim’s current age 5 37 years
Tim’s current total retirement savings 5 $259,000
Annual rate of return on retirement savings 5 4 percent
Tim’s current annual salary 5 $145,000
Tim’s expected annual percentage increase in salary 5 2 percent
Tim’s percentage of annual salary contributed to retirement 5 6 percent
Tim’s expected age of retirement 5 65
Tim’s expected annual expenses after retirement (current dollars) 5 $90,000
Rate of return on retirement savings after retirement 5 3 percent
Income tax rate postretirement 5 15 percent
Assume that Tim’s employer contributes 6% of Tim’s salary to his retirement fund. 
Tim can make an additional annual contribution to his retirement fund before taxes (tax 
free) up to a contribution of $16,000. Assume he contributes $6,000 per year. Also, assume 
an inflation rate of 2%.
file
WEB
FloydsMay
Case Problem

 
Case Problem Retirement Plan 
351
Managerial Report
Your spreadsheet model should provide the accumulated savings at the onset of retire-
ment as well as the age at which funds will be depleted (given assumptions on the input 
parameters).
As a feature of your spreadsheet model, build a data table to demonstrate the sensitiv-
ity of the age at which funds will be depleted to the retirement age and additional pre-tax 
contributions. Similarly, consider other factors you think might be important.
Develop a report for Tim outlining the factors that will have the greatest impact on his 
retirement.

Linear Optimization Models
CONTENTS
8.1 
 A SIMPLE MAXIMIZATION 
PROBLEM
Problem Formulation
Mathematical Model for the Par, 
Inc. Problem
8.2 
 SOLVING THE PAR, INC. 
PROBLEM
The Geometry of the Par, Inc. 
Problem
Solving Linear Programs with 
Excel Solver
8.3 
 A SIMPLE MINIMIZATION 
PROBLEM
Problem Formulation
Solution for the M&D Chemicals 
Problem
8.4 
 SPECIAL CASES OF LINEAR 
PROGRAM OUTCOMES
Alternative Optimal Solutions
Infeasibility
Unbounded
8.5 
 SENSITIVITY ANALYSIS
Interpreting Excel Solver 
Sensitivity Report
8.6 
 GENERAL LINEAR 
 PROGRAMMING NOTATION 
AND MORE EXAMPLES
Investment Portfolio Selection
Transportation Planning
Advertising Campaign 
Planning 
8.7 
 GENERATING AN ALTERNA-
TIVE OPTIMAL SOLUTION 
FOR A LINEAR PROGRAM
APPENDIX:  SOLVING LINEAR 
 OPTIMIZATION 
 MODELS USING 
ANALYTIC SOLVER 
PLATFORM
Chapter 8

 
Analytics in Action 
353
MeadWestvaco Corporation is a major producer of pre-
mium papers for periodicals, books, commercial printing, 
and business forms. The company also produces pulp and 
lumber; designs and manufactures packaging systems for 
beverage and other consumables markets; and is a world 
leader in the production of coated board and shipping 
containers. Quantitative analyses at MeadWestvaco are 
developed and implemented by the company’s Decision 
Analysis Department. The department assists decision 
makers by providing them with analytical tools as well as 
personal analysis and recommendations.
MeadWestvaco uses analytical models to assist 
with the long-range management of the company’s 
timberland. Through the use of large-scale linear 
programs, timber harvesting plans are developed to 
cover a substantial time horizon. These models consider 
wood market conditions, mill pulpwood requirements, 
harvesting capacities, and general forest management 
principles. Within these constraints, the model arrives 
at an optimal harvesting and purchasing schedule based 
on discounted cash flow. Alternative schedules reflect 
changes in the various assumptions concerning for-
est growth, wood availability, and general economic 
 conditions.
Business analytics is also used in the development 
of the inputs for the linear programming models. Timber 
prices and supplies, as well as mill requirements, must 
be forecast over the time horizon, and advanced sam-
pling techniques are used to evaluate land holdings and 
to project forest growth. The harvest schedule is then 
developed using a linear optimization model.
TIMBER HARVESTING MODEL AT MEADWESTVACO CORPORATION*
ANALYTICS  in  Action
*Based on information provided by Dr. Edward P. Winkofsky of 
 MeadWestvaco Corporation.
This chapter begins our discussion of prescriptive analytics and how optimization models 
can be used to support and improve managerial decision making. Optimization problems 
maximize or minimize some function, called the objective function, and usually have a 
set of restrictions known as constraints. Consider the following typical applications of 
optimization:
 1. A manufacturer wants to develop a production schedule and an inventory policy that 
will satisfy demand in future periods. Ideally, the schedule and policy will enable the 
company to satisfy demand and at the same time minimize the total production and 
inventory costs.
 2. A financial analyst must select an investment portfolio from a variety of stock and 
bond investment alternatives. The analyst would like to establish the portfolio that 
maximizes the return on investment.
 3. A marketing manager wants to determine how best to allocate a fixed advertising 
budget among alternative advertising media such as web, radio, television, newspaper, 
and magazine. The manager would like to determine the media mix that maximizes 
advertising effectiveness.
 4. A company has warehouses in a number of locations. Given specific customer de-
mands, the company would like to determine how much each warehouse should ship 
to each customer so that total transportation costs are minimized.
Each of these examples has a clear objective. In example 1, the manufacturer wants to 
minimize costs; in example 2, the financial analyst wants to maximize return on investment; 
in example 3, the marketing manager wants to maximize advertising effectiveness; and in 
example 4, the company wants to minimize total transportation costs.
Likewise, each problem has constraints that limit the degree to which the objective can 
be pursued. In example 1, the manufacturer is restricted by the constraints requiring product 
demand to be satisfied and limiting production capacity. The financial analyst’s portfolio 

354 
Chapter 8 Linear Optimization Models
problem is constrained by the total amount of investment funds available and the maximum 
amounts that can be invested in each stock or bond. The marketing manager’s media selec-
tion decision is constrained by a fixed advertising budget and the availability of the various 
media. In the transportation problem, the minimum-cost shipping schedule is constrained 
by the supply of product available at each warehouse.
Optimization models can be linear or nonlinear. We begin with linear optimiza-
tion models, also known as linear programs. Linear programming is a problem-solving 
approach developed to help managers make better decisions. Numerous applications 
of linear programming can be found in today’s competitive business environment. For 
instance, GE Capital uses linear programming to help determine optimal lease structur-
ing, and Marathon Oil Company uses linear programming for gasoline blending and to 
evaluate the economics of a new terminal or pipeline.
a Simple Maximization problem
Par, Inc. is a small manufacturer of golf equipment and supplies whose management has 
decided to move into the market for medium- and high-priced golf bags. Par’s distributor is 
enthusiastic about the new product line and has agreed to buy all the golf bags Par produces 
over the next three months.
After a thorough investigation of the steps involved in manufacturing a golf 
bag,  management determined that each golf bag produced will require the following 
 operations:
 1. Cutting and dyeing the material
 2. Sewing
 3. Finishing (inserting umbrella holder, club separators, etc.)
 4. Inspection and packaging
The director of manufacturing analyzed each of the operations and concluded that if the 
company produces a medium-priced standard model, each bag will require 7⁄10 hour in the 
cutting and dyeing department, 1⁄2 hour in the sewing department, 1 hour in the finishing 
department, and 1⁄10 hour in the inspection and packaging department. The more expensive 
deluxe model will require 1 hour for cutting and dyeing, 5⁄6 hour for sewing, 2⁄3 hour for 
finishing, and 1⁄4 hour for inspection and packaging. This production information is sum-
marized in Table 8.1.
Par’s production is constrained by a limited number of hours available in each depart-
ment. After studying departmental workload projections, the director of manufacturing 
estimates that 630 hours for cutting and dyeing, 600 hours for sewing, 708 hours for finish-
ing, and 135 hours for inspection and packaging will be available for the production of golf 
bags during the next three months.
Linear programming 
was initially referred to 
as “programming in a 
linear structure.” In 1948, 
Tjalling Koopmans sug-
gested to George Dantzig 
that the name was much 
too long: Koopman’s 
suggestion was to shorten 
it to linear programming. 
George Dantzig agreed, 
and the field we now know 
as linear programming was 
named.
8.1
TABLE 8.1  PRODUCTION REQUIREMENTS PER GOLF BAG
Production Time (hours)
Department
Standard Bag
Deluxe Bag
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
7⁄10
1⁄2
1
1⁄10
1
5⁄6
2⁄3
1⁄4

 
8.1 A Simple Maximization Problem 
355
The accounting department analyzed the production data, assigned all relevant vari-
able costs, and arrived at prices for both bags that will result in a profit contribution1 of 
$10 for every standard bag and $9 for every deluxe bag produced. Let us now develop a 
mathematical model of the Par, Inc. problem that can be used to determine the number of 
standard bags and the number of deluxe bags to produce in order to maximize total profit 
contribution.
Problem Formulation
Problem formulation, or modeling, is the process of translating the verbal statement of a 
problem into a mathematical statement. Formulating models is an art that can be mastered 
only with practice and experience. Even though every problem has some unique features, 
most problems also have common features. As a result, some general guidelines for optimi-
zation model formulation can be helpful, especially for beginners. We will illustrate these 
general guidelines by developing a mathematical model for Par, Inc.
Understand the problem thoroughly We selected the Par, Inc. problem to introduce 
linear programming because it is easy to understand. However, more complex problems 
will require much more effort to identify the items that need to be included in the model. 
In such cases, read the problem description to get a feel for what is involved. Taking notes 
will help you focus on the key issues and facts.
Describe the objective The objective is to maximize the total contribution to profit.
Describe each constraint Four constraints relate to the number of hours of manufactur-
ing time available; they restrict the number of standard bags and the number of deluxe bags 
that can be produced.
● 
Constraint 1: The number of hours of cutting and dyeing time used must be less 
than or equal to the number of hours of cutting and dyeing time available.
● 
Constraint 2: The number of hours of sewing time used must be less than or equal 
to the number of hours of sewing time available.
● 
Constraint 3: The number of hours of finishing time used must be less than or equal 
to the number of hours of finishing time available.
● 
Constraint 4: The number of hours of inspection and packaging time used must be 
less than or equal to the number of hours of inspection and packaging time available.
Define the decision variables The controllable inputs for Par, Inc. are (1) the number 
of standard bags produced and (2) the number of deluxe bags produced. Let
 
 S 5 number of standard bags
 
D 5 number of deluxe bags
In optimization terminology, S and D are referred to as the decision variables.
Write the objective in terms of the decision variables Par’s profit contribution 
comes from two sources: (1) the profit contribution made by producing S standard bags and 
(2) the profit contribution made by producing D deluxe bags. If Par makes $10 for every 
standard bag, the company will make $10S if S standard bags are produced. Also, if Par 
makes $9 for every deluxe bag, the company will make $9D if D deluxe bags are produced. 
Thus, we have
 
Total profit contribution 5 10S 1 9D
It is important to under-
stand that we are maximiz-
ing profit contribution, 
not profit. Overhead and 
other shared costs must be 
deducted before arriving at 
a profit figure.
1From an accounting perspective, profit contribution is more correctly described as the contribution margin per bag since 
overhead and other shared costs are not allocated.

356 
Chapter 8 Linear Optimization Models
Because the objective—maximize total profit contribution—is a function of the decision 
variables S and D, we refer to 10S 1 9D as the objective function. Using Max as an abbre-
viation for maximize, we write Par’s objective as follows:
 
Max 10S 1 9D
Write the constraints in terms of the decision variables
Constraint 1:
 
aHours of cutting and
dyeing time used b #  a Hours of cutting and
dyeing time availableb
Every standard bag Par produces will use 7⁄10 hour cutting and dyeing time; therefore, the 
total number of hours of cutting and dyeing time used in the manufacture of S standard 
bags is 7⁄10 S. In addition, because every deluxe bag produced uses 1 hour of cutting and 
dyeing time, the production of D deluxe bags will use 1D hours of cutting and dyeing time. 
Thus, the total cutting and dyeing time required for the production of S standard bags and 
D deluxe bags is given by
 
Total hours of cutting and dyeing time used 5 7⁄10 S 1 1D
The director of manufacturing stated that Par has at most 630 hours of cutting and dye-
ing time available. Therefore, the production combination we select must satisfy the 
 requirement
 
7⁄10 S 1 1D # 630 
(8.1)
Constraint 2:
 
aHours of sewing
time used
b #  aHours of sewing
time available b
From Table 8.1 we see that every standard bag manufactured will require 1⁄2 hour for sewing, 
and every deluxe bag will require 5⁄6 hour for sewing. Because 600 hours of sewing time 
are available, it follows that
 
1⁄2 S 1 5⁄6 D # 600 
(8.2)
Constraint 3:
 
aHours of finishing
time used
b #  aHours of finishing
time available
b
Every standard bag manufactured will require 1 hour for finishing, and every deluxe bag 
will require 2⁄3 hour for finishing. With 708 hours of finishing time available, it follows that
 
1S 1 2⁄3 D # 708 
(8.3)
Constraint 4:
 
aHours of inspection and
packaging time used b #  a Hours of inspection and
packaging time availableb
Every standard bag manufactured will require 1⁄10 hour for inspection and packaging, and 
every deluxe bag will require 1⁄4 hour for inspection and packaging. Because 135 hours of 
inspection and packaging time are available, it follows that
 
1⁄10 S 1 1⁄4 D # 135 
(8.4)
We have now specified the mathematical relationships for the constraints associated with 
the four departments. Have we forgotten any other constraints? Can Par produce a negative 
The units of measurement 
on the left-hand side of the 
constraint must match the 
units of measurement on 
the right-hand side.

 
8.1 A Simple Maximization Problem 
357
number of standard or deluxe bags? Clearly, the answer is no. Thus, to prevent the decision 
variables S and D from having negative values, two constraints must be added: 
 
S $ 0 and D $ 0 
(8.5)
These constraints ensure that the solution to the problem will contain only nonnegative 
values for the decision variables and are thus referred to as the nonnegativity constraints. 
Nonnegativity constraints are a general feature of many linear programming problems and 
may be written in the abbreviated form:
 
S, D $ 0
Mathematical Model for the Par, Inc. Problem
The mathematical statement, or mathematical formulation, of the Par, Inc. problem is now 
complete. We succeeded in translating the objective and constraints of the problem into 
a set of mathematical relationships, referred to as a mathematical model. The complete 
mathematical model for the Par, Inc. problem is as follows:
Max
10S 1
9D
subject to (s.t.)
7⁄10 S 1
1D # 630  Cutting and dyeing
1⁄2 S 1
5⁄6 D # 600  Sewing
1S 1
2⁄3 D # 708  Finishing
1⁄10 S 1
1⁄4 D # 135  Inspection and packaging
S, D $ 0
Our job now is to find the product mix (i.e., the combination of values for S and D) that 
satisfies all the constraints and at the same time yields a value for the objective function that 
is greater than or equal to the value given by any other feasible solution. Once these values 
are calculated, we will have found the optimal solution to the problem.
This mathematical model of the Par, Inc. problem is a linear programming model, or 
linear program, because the objective function and all constraint functions (the left-hand 
sides of the constraint inequalities) are linear functions of the decision variables.
Mathematical functions in which each variable appears in a separate term and is raised 
to the first power are called linear functions. The objective function (10S 1 9D) is linear 
because each decision variable appears in a separate term and has an exponent of 1. The 
amount of production time required in the cutting and dyeing department (7⁄10 S 1 1D) is also 
a linear function of the decision variables for the same reason. Similarly, the functions on 
the left-hand side of all the constraint inequalities (the constraint functions) are linear func-
tions. Thus, the mathematical formulation of this problem is referred to as a linear program.
Linear programming has 
nothing to do with comput-
er programming. The use 
of the word programming 
means “choosing a course 
of action.” Linear pro-
gramming involves choos-
ing a course of action when 
the mathematical model of 
the problem contains only 
linear functions.
NOTES AND COMMENTS
The three assumptions necessary for a linear 
programming model to be appropriate are pro-
portionality, additivity, and divisibility. Pro-
portionality means that the contribution to the 
objective function and the amount of resources 
used in each constraint are proportional to 
the value of each decision variable. Additivity 
means that the value of the objective function 
and the total resources used can be found by 
summing the objective function contribution 
and the resources used for all decision variables. 
Divisibility means that the decision variables 
are continuous. The divisibility assumption plus 
the nonnegativity constraints mean that decision 
variables can take on any value greater than or 
equal to zero.

358 
Chapter 8 Linear Optimization Models
Solving the par, Inc. problem
Now that we have modeled the Par, Inc. problem as a linear program, let us discuss how 
we might find the optimal solution. The optimal solution must be a feasible solution. A 
feasible solution is a setting of the decision variables that satisifies all of the constraints 
of the problem. The optimal solution also must have an objective function value as good 
as any other feasible solution. For a maximization problem like Par, Inc., this means that 
the solution must be feasible and achieve the highest objective function value of any fea-
sible solution. To solve a linear program then, we must search over the feasible region, 
which is the set of all feasible solutions, and find the solution that gives the best objective 
function value.
Because the Par, Inc. model has two decision variables, we are able to graph the feasible 
region. Let us discuss the geometry of the feasible region of the model. This will help us 
better understand linear programming and how we are able to solve much larger problems 
on the computer.
The Geometry of the Par, Inc. Problem
Recall that the feasible region is the set of points that satisfies all of the constraints of the 
problem. When we have only two decision variables and the functions of these variables 
are linear, they form lines in 2-space. If the constraints are inequalities, the constraint cuts 
the space into two, with the line and the area on one side of the line being the space that 
satisfies that constraint. These subregions are called half spaces. The intersection of these 
half spaces makes up the feasible region.
The feasible region for the Par, Inc. problem is shown in Figure 8.1. Notice that the 
horizontal axis corresponds to the value of S and the vertical axis to the value of D. The non-
negativity constraints define that the feasible region is in the area bounded by the horizontal 
and vertical axes. Each of the four constraints is graphed as equality (a line) and arrows 
show the direction of the half space that satisfies the inequality constraint. The intersection 
of the four half spaces in the area bounded by the axes is the shaded region and this is the 
feasible region for the Par, Inc. problem. Any point in the shaded region satisfies all four 
constraints of the problem and nonnegativity.
To solve the Par, Inc. problem, we must find the point in the feasible region that re-
sults in the highest possible objective function value. A contour line is a set of points on 
a map, all of which have the same elevation. Similar to the way contour lines are used in 
geography, we may define an objective function contour to be a set of points (in this case 
a line) that yield a fixed value of the objective function. By choosing a fixed value of the 
objective function, we may plot contour lines of the objective function over the feasible 
region (Figure 8.2). In this case as we move away from the origin we see higher values 
of the objective function and the highest such contour is 10S 1 9D 5 7668, after which 
we leave the feasible region. The highest value contour intersects the feasible region at a 
single point—point 3.
Of course, this geometric approach to solving a linear program is limited to problems 
with only two variables. What have we learned that can help us solve larger linear optimi-
zation problems?
Based on the geometry of Figure 8.2, to solve a linear optimization problem we 
only have to search over the extreme points of the feasible region to find an optimal 
solution. The extreme points are found where constraints intersect on the boundary of 
the feasible region. In Figure 8.2, points 1, 2, 3, 4, and 5 are the extreme points of the 
feasible region.
8.2

 
8.2 Solving the Par, Inc. Problem 
359
FigurE 8.1  FEASIBLE REGION FOR THE PAR, INC. PROBLEM
0
200
400
600
800
1000
1200
1400
Number of Standard Bags
200
400
600
800
1000
1200
C & D
Feasible
Region
I & P
Sewing
S
Finishing 
Number of Deluxe Bags
D
FigurE 8.2  THE OPTIMAL SOLUTION TO THE PAR, INC. PROBLEM
0
200
400
600
800
Number of Standard Bags
200
400
600
Number of Deluxe Bags
D
10S + 9D = 1800
10S + 9D = 3600
10S + 9D = 5400
Optimal Solution is Point 3:
S = 540 D = 252
Maximum Profit Line
10S + 9D = 7668
1
5
4
2
S
3

360 
Chapter 8 Linear Optimization Models
Because each extreme point lies at the intersection of two constraint lines, we may 
obtain the values of S and D by solving simultaneously as equalities, the pair of constraints 
that form the given point. The values of S and D and the objective function value at points 
1 through 5 are as follows:
Point
S
D
Profit 5 10S 1 9D
1
2
3
4
5
  0
708
540
300
  0
  0
  0
252
420
540
10(0)   1 9(0)   5 0
10(708) 1 9(0)   5 7080
10(540) 1 9(252) 5 7668
10(300) 1 9(420) 5 6780
10(0)   1 9(540) 5 4860
The highest profit is achieved at point 3. Therefore, the optimal plan is to produce 
540 Standard bags and 252 Deluxe bags, as shown in Figure 8.2.
It turns out that this approach of investigating only extreme points works well and gen-
eralizes for larger problems. The simplex algorithm, developed by George Dantzig, is quite 
effective at investigating extreme points in an intelligent way to find the optimal solution 
to even very large linear programs.
Excel Solver is software that utilizes Dantzig’s simplex algorithm to solve linear pro-
grams by systematically finding which set of constraints form the optimal extreme point of 
the feasible region. Once it finds an optimal solution, Solver then reports the optimal values 
of the decision variables and the optimal objective function value. Let us illustrate now how 
to use Excel Solver to find the optimal solution to the Par, Inc. problem.
Solving Linear Programs with Excel Solver
The first step in solving a linear optimization model in Excel is to construct the relevant 
what-if model. Using the principles for developing good spreadsheet models discussed in 
Chapter 7, a what-if model for optimization allows the user to try different values of the 
decision variables and see easily (a) whether that trial solution is feasible and (b) the value 
of the objective function for that trial solution.
Figure 8.3 shows a spreadsheet model for the Par, Inc. problem with a trial solution 
of one Standard bag and one Deluxe bag. Rows one through ten contain the parameters 
for the problem. Row 14 contains the decision variable cells: Cells B14 and C14 are the 
locations for the number of Standard and Deluxe bags to produce. Cell B16 calculates the 
objective function value for the trial solution by using the SUMPRODUCT function. The 
SUMPRODUCT function is very useful for linear problems. Recall how the SUMPROD-
UCT function works:
 5 SUMPRODUCT(B9:C9,$B$14:$C$14) 5 B9*B14 1 C9*C14 5 10(1) 1 9(1) 5 19
We likewise use the SUMPRODUCT function in cells B19:B22 to calculate the number 
of hours used in each of the four departments. The hours available are immediately to the 
right for each department. Hence we see that the current solution is feasible, since Hours 
Used do not exceed Hours Available in any department.
Once the what-if model is built, we need a way to convey to Excel Solver the structure 
of the linear optimization model. This is accomplished through the Excel Solver dialog 
box as follows:
Step 1. Click the DATA tab in the Ribbon
Step 2.  Click Solver in the Analysis Group
Note the use of  absolute 
referencing in the 
 SUMPRODUCT  function 
here. This facilitates 
 copying this function from 
cell B19 to cells B20:B22 in 
Figure 8.3.

 
8.2 Solving the Par, Inc. Problem 
361
FigurE 8.3  WHAT-IF SPREADSHEET MODEL FOR PAR, INC.
A
Par, Inc.
B
C
D
Parameters
Production Time (Hours)
Time Available
Standard
Hours
Deluxe
Standard
Deluxe
Operation
Hours Used
Hours Available
Operation
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Proﬁt Per Bag
=7/10
=5/10
1
=1/10
10
Bags Produced
1
=SUMPRODUCT(B9:C9,$B$14:$C$14)
=SUMPRODUCT(B5:C5,$B$14:$C$14)
=SUMPRODUCT(B6:C6,$B$14:$C$14)
=SUMPRODUCT(B7:C7,$B$14:$C$14)
=SUMPRODUCT(B8:C8,$B$14:$C$14)
=D5
=D6
=D7
=D8
1
1
=5/6
=2/3
=1/4
9
630
600
708
135
Model
Total Proﬁt
1
2
3
4
5
6
7
8
9
10
11
13
12
14
15
16
17
18
19
20
21
22
A
Par, Inc.
B
C
D
Parameters
Production Time (Hours)
Time Available
Standard
Hours
Deluxe
Standard
Deluxe
1
2
3
4
5
6
7
8
9
10
11
Operation
Hours Used
Hours Available
Operation
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Proﬁt Per Bag
Bags Produced
0.7
0.5
1
0.1
10
1.00
$19.00
1.7
1.33333
1.66667
0.35
630
600
708
135
1.00
1
0.83333
0.66667
0.25
9.00
630
600
708
135
Model
Total Proﬁt
13
12
14
15
16
17
18
19
20
21
22
file
WEB
Par

362 
Chapter 8 Linear Optimization Models
Step 3.  When the Solver dialog box appears (Figure 8.4):
Enter B16 in the Set Objective: box
Select Max for the To: option
Enter B14:C14 in the By Changing Variable Cells: box
Step 4. Click the Add button
When the Add Constraint dialog box appears:
Enter B19:B22 in the left-hand box under Cell Reference:
Select <5 from the dropdown button
Enter C19:C22 in the Constraint: box
Click OK.
Step 5. Select the checkbox for Make Unconstrained Variables Non-negative
Step 6.  From the drop-down menu for Select a Solving Method:, chose Simplex LP
Step 7. Click Solve
FigurE 8.4  SOLVER DIALOG BOX AND SOLUTION TO THE PAR, INC. PROBLEM
A
Par, Inc.
B
C
D
Parameters
Production Time (Hours) Time Available
Standard
Hours
Deluxe
Standard
Deluxe
Operation
Hours Used
Hours
Available
Operation
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Proﬁt Per Bag
0.7
0.5
1
0.1
10
Bags Produced
540.00
$7,668.00
630
480.00000
708.00000
117
630
600
708
135
252.00
1
0.83333
0.66667
0.25
9.00
630
600
708
135
Model
Total Proﬁt
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
23
22
24
25
26
27
28
29
30
20
21
E
F
G
H
I
J
K
L
M

 
8.2 Solving the Par, Inc. Problem 
363
FigurE 8.5  THE SOLVER ANSWER REPORT FOR THE PAR, INC. PROBLEM
A
B
C
D
Objective Cell (Max)
Name
Original 
Value
Final Value
Final Value
Integer
Constraints
Cell
$B$22 Inspection and Packaging Hours Used
$B$16 Total Proﬁt
$19.00
$7,668.00
Cell
Variable Cells
$B$14 Bags Produced Standard
$B$19 Cutting and Dyeing Hours Used
$C$14 Bags Produced Deluxe
Name
Original
Value
Formula
Status
Slack
Cell
Name
Cell Value
1.000
540.000
Contin
1.000
252.000
Contin
$B$20 Sewing Hours Used
$B$21 Finishing Hours Used
$B$22<=$C$22
$B$19<=$C$19
$B$20<=$C$20
117
630
480
708
$B$21<=$C$21
18
0
120
0
Not Binding
Binding
Not Binding
Binding
13
14
15
16
17
19
18
20
21
22
23
24
25
26
27
28
29
30
31
E
F
G
Step 8.  When the Solver Results dialog box appears:
Select Keep Solver Solution
In the Reports section, select Answer Report
Click OK
The completed Solver dialog box and solution for the Par, Inc. problem are shown in 
Figure 8.4. The optimal solution is to make 540 Standard bags and 252 Deluxe bags (see 
cells B14 and C14) for a profit of $7,688 (see cell B16). This corresponds to point 3 in 
Figure 8.2. Also note that, from cells B19:B22 compared to C19:C22, we use all cutting 
and dyeing time as well as all finishing time. This is, of course, consistent with what we 
have seen in Figures 8.1 and 8.2: The cutting, dyeing, and finishing constraints intersect to 
form point 3 in the graph.
The Excel Solver Answer Report appears in Figure 8.5. The Answer Report contains 
three sections: Objective Cell, Variable Cells, and Constraints. In addition to some other 
information, each section gives the cell location, name and value of the cell(s). The Objec-
tive Cell section indicates that the optimal (Final Value) of Total Profit is $7,668.00. In the 
Variable Cells section, the two far-right columns indicate the optimal values of the deci-
sion cells and whether or not the variables are required to be integer (here they are labeled 
“Contin” for continuous). Note that Solver generates a Name for a cell by concatenating 
the text to the left and above that cell.
The Constraints section gives the left-hand side value for each constraint (in this case 
the hours used), the formula showing the constraint relationship, the status (Binding of Not 
Binding), and the Slack value. A binding constraint is one that holds as an equality at the 
optimal solution. Geometrically, binding constraints intersect to form the optimal point. 
We see in Figure 8.5 that the cutting and dyeing, and finishing constraints are designated 
as binding, consistent with our geometric study of this problem.
The slack value for each less-than-or-equal-to constraint indicates the difference be-
tween the left-hand and right-hand values for a constraint. Of course, by definition, binding 
Variable cells that are 
 required to be integer will 
be discussed in Chapter 9.

364 
Chapter 8 Linear Optimization Models
a Simple Minimization problem
M&D Chemicals produces two products that are sold as raw materials to companies manu-
facturing bath soaps and laundry detergents. Based on an analysis of current inventory 
levels and potential demand for the coming month, M&D’s management specified that 
the combined production for products A and B must total at least 350 gallons. Separately, 
a major customer’s order for 125 gallons of product A must also be satisfied. Product A 
requires 2 hours of processing time per gallon, and product B requires 1 hour of processing 
time per gallon. For the coming month, 600 hours of processing time are available. M&D’s 
objective is to satisfy these requirements at a minimum total production cost. Production 
costs are $2 per gallon for product A and $3 per gallon for product B.
Problem Formulation
To find the minimum-cost production schedule, we will formulate the M&D Chemicals 
problem as a linear program. Following a procedure similar to the one used for the Par, Inc. 
problem, we first define the decision variables and the objective function for the problem. Let
 
A 5 number of gallons of product A to produce
 
B 5 number of gallons of product B to produce
With production costs at $2 per gallon for product A and $3 per gallon for product B, 
the objective function that corresponds to the minimization of the total production cost can 
be written as
 
Min 2A 1 3B
8.3
NOTES AND COMMENTS
1. Notice in the data section for the Par, Inc. 
spreadsheet shown in Figure 8.3, that we have 
entered fractions in cells C6: 5 5⁄6 and C7: 5 2⁄3. 
We do this to make sure we maintain accuracy 
because rounding these values could have an 
impact on our solution.
2. By selecting Make Unconstrained Variables 
Non-negative in the Solver dialog box, all deci-
sion variables are declared to be nonnegative.
3. Although we have shown the Answer Report 
and how to interpret it, we will usually show the 
solution to an optimization problem directly in the 
spreadsheet. A well designed spreadsheet that fol-
lows the principles discussed in Chapter 7 should 
make it easy for the user to interpret the optimal 
solution directly from the spreadsheet.
4. In addition to the Answer Report, Solver also 
allows you to generate two other reports. The 
Sensitivity Report will be discussed later in this 
chapter in section 8.5. The Limits Report gives 
information on the objective function value 
when variables are set to their limits.
constraints have zero slack. Consider for example the sewing department constraint. By 
adding a nonnegative slack variable, we can make the constraint equality:
1⁄2 S 1 5⁄6 D # 600
1⁄2 S 1 5⁄6 D 1 slacksewing 5 600
slacksewing 5 600 2 1⁄2(540) 1 5⁄6(252) 5 600 2 270 2 210 5 120
The slack value for the inspecting and packaging constraint is calculated in a similar way. 
For resource constraints like departmental hours available, the slack value gives the amount 
of unused resource, in this case, time measured in hours.

 
8.3 A Simple Minimization Problem 
365
Next consider the constraints placed on the M&D Chemicals problem. To satisfy the 
major customer’s demand for 125 gallons of product A, we know A must be at least 125. 
Thus, we write the constraint
 
1A $ 125
For the combined production for both products, which must total at least 350 gallons, we 
can write the constraint
 
1A 1 1B $ 350
Finally, for the limitation of 600 hours on available processing time, we add the constraint
 
2A 1 1B # 600
After adding the nonnegativity constraints (A, B $ 0), we arrive at the following linear 
program for the M&D Chemicals problem:
Min
2A 1 3B
s.t.
1A
$ 125  Demand for product A
1A 1 1B $ 350  Total production
2A 1 1B # 600  Processing time
A, B $ 0
Solution for the M&D Chemicals Problem
A spreadsheet model for the M&D Chemicals problem along with the Solver dialog box 
are shown in Figure 8.6. The complete linear programming model for the M&D Chemicals 
problem in Excel Solver is contained in the file M&DModel. We use the SUMPRODUCT 
function to calculate total cost in cell B16 and also to calculate total processing hours used 
in cell B23. The optimal solution, which is shown in the spreadsheet and in the Answer 
Report in Figure 8.7, is to make 250 gallons of product A and 100 gallons of product B for 
a total cost of $800. Both the total production constraint and the processing time  constraints 
are binding (350 gallons are provided, the same as required, and all 600 processing hours 
are used). The requirement that at least 125 gallons of Product A be produced is not binding. 
For greater-than-or-equal-to constraints, we can define a nonnegative variable called a sur-
plus variable. A surplus variable tells how much over the right-hand side the left-hand side 
of a greater-than-or-equal-to constraint is for a solution. A surplus variable is subtracted 
from the left-hand side of the constraint. For example:
1A $ 125
1A 2 surplusA 5 125
surplusA 5 1A 2 125 5 250 2 125 5 125
As was the case with less-than-or-equal-to constraints and slack variables, a positive value 
for a surplus variable indicates that the constraint is not binding.
file
WEB
M&DModel
NOTES AND COMMENTS
1. In the spreadsheet and Solver model for the 
M&D Chemicals problem, we separated 
the  greater-than-or-equal-to constraints and  
the less-than-or-equal-to constraints. This 
allows for easier entry of the constraints into 
the Add Constraint dialog box.
2. In the Excel Answer Report, both slack and sur-
plus variables are labeled “Slack”.

366 
Chapter 8 Linear Optimization Models
FigurE 8.6   SOLVER DIALOG BOX AND SOLUTION TO THE M&D CHEMICAL  PROBLEM
A
M&D Chemicals
D
Parameters
Time Available
Processing Time (hours)
Production Cost
Minimum Total Production
Product A Minimum
Product A
Total Production
Processing Time
B
Product A
Product A
Provided
Hours Used
C
Product B
Product B
Required
Hours Available
Unused Hours
600
600
0
2
$2.00
350
125
1
$3.00
Gallons Produced
250
$800.00
250
350
125
350
100
600
Model
Minimize Total Cost
1
2
3
4
5
6
7
8
11
10
9
13
12
14
15
16
17
18
19
23
22
24
25
26
27
28
29
30
20
21
file
WEB
M&DModel

 
8.4 Special Cases of Linear Program Outcomes 
367
Special Cases of Linear program Outcomes
In this section we discuss three special situations that can arise when we attempt to solve 
linear programming problems.
Alternative Optimal Solutions
From the discussion of the graphical solution procedure, we know that optimal solutions 
can be found at the extreme points of the feasible region. Now let us consider the special 
case in which the optimal objective function contour line coincides with one of the binding 
constraint lines on the boundary of the feasible region. We will see that this situation can 
lead to the case of alternative optimal solutions; in such cases, more than one solution 
provides the optimal value for the objective function.
To illustrate the case of alternative optimal solutions, we return to the Par, Inc. problem. 
However, let us assume that the profit for the standard golf bag (S) has been decreased to 
$6.30. The revised objective function becomes 6.3S 1 9D. The graphical solution of this 
problem is shown in Figure 8.8. Note that the optimal solution still occurs at an extreme 
point. In fact, it occurs at two extreme points: extreme point 4 (S 5 300, D 5 420) and 
extreme point 3 (S 5 540, D 5 252).
The objective function values at these two extreme points are identical; that is,
 
6.3S 1 9D 5 6.3(300) 1 9(420) 5 5670
and
 
6.3S 1 9D 5 6.3(540) 1 9(252) 5 5670
Furthermore, any point on the line connecting the two optimal extreme points also pro-
vides an optimal solution. For example, the solution point (S 5 420, D 5 336), which 
8.4
FigurE 8.7  THE SOLVER ANSWER REPORT FOR THE M&D CHEMICALS PROBLEM
Objective Cell (Min)
Name
Original Value
Final Value
Final Value
Integer
Constraints
Cell
$B$16 Minimize Total Cost
$0.00
$800.00
Cell
Variable Cells
$B$14 Gallons Produced Product A
$B$19 Product A Provided
$C$14 Gallons Produced Product B
Name
Original Value
Formula
Status
Slack
Cell
Name
Cell Value
0
250
Contin
0
100
Contin
$B$20 Total Production Provided
$B$23 Processing Time Hours Used
250
$B$19>=$C$19
350
$B$20>=$C$20
600
$B$23<=$C$23
Not Binding
Binding
Binding
125
0
0
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
30
28
29
A
B
C
D
E
F
G

368 
Chapter 8 Linear Optimization Models
is halfway between the two extreme points, also provides the optimal objective function 
value of
 
6.3S 1 9D 5 6.3(420) 1 9(336) 5 5670
A linear programming problem with alternative optimal solutions is generally a good 
situation for the manager or decision maker. It means that several combinations of the 
decision variables are optimal and that the manager can select the most desirable optimal 
solution. Unfortunately, determining whether a problem has alternative optimal solutions 
is not a simple matter. In Section 8.7, we discuss an approach for finding alternative 
optima.
Infeasibility
Infeasibility means that no solution to the linear programming problem satisfies all the 
constraints, including the nonnegativity conditions. Graphically, infeasibility means that 
a feasible region does not exist; that is, no points satisfy all the constraints and the non-
negativity conditions simultaneously. To illustrate this situation, let us look again at the 
problem faced by Par, Inc.
Suppose that management specified that at least 500 of the Standard bags and at least 
360 of the Deluxe bags must be manufactured. The graph of the solution region may now 
be constructed to reflect these new requirements (see Figure 8.9). The shaded area in 
the lower left-hand portion of the graph depicts those points satisfying the departmental 
constraints on the availability of time. The shaded area in the upper right-hand portion 
depicts those points satisfying the minimum production requirements of 500 Standard 
Problems with no feasible 
solution do arise in prac-
tice, most often because 
management’s expectations 
are too high or because too 
many restrictions have been 
placed on the problem.
FigurE 8.8   PAR, INC. PROBLEM WITH AN OBJECTIVE FUNCTION OF 
6.3S 1 9D (ALTERNATIVE OPTIMAL SOLUTIONS)
0
200
400
600
800
Number of Standard Bags
200
400
600
Number of Deluxe Bags
S
D
6.3S + 9D = 3780 
6.3S + 9D = 5670 
(300, 420)
(540, 252)
5
2
1
3
4

 
8.4 Special Cases of Linear Program Outcomes 
369
and 360 Deluxe bags. But no points satisfy both sets of constraints. Thus, we see that if 
management imposes these minimum production requirements, no feasible region exists 
for the problem.
How should we interpret infeasibility in terms of this current problem? First, we should 
tell management that, given the resources available (i.e., production time for cutting and 
dyeing, sewing, finishing, and inspection and packaging), it is not possible to make 500 
Standard bags and 360 Deluxe bags. Moreover, we can tell management exactly how much 
of each resource must be expended to make it possible to manufacture 500 Standard and 360 
Deluxe bags. Table 8.2 shows the minimum amounts of resources that must be available, 
the amounts currently available, and additional amounts that would be required to accom-
plish this level of production. Thus, we need 80 more hours for cutting and dyeing, 32 more 
FigurE 8.9   NO FEASIBLE REGION FOR THE PAR, INC. PROBLEM WITH 
 MINIMUM PRODUCTION REQUIREMENTS OF 500 STANDARD 
AND 360 DELUXE BAGS
0
200
400
600
800
Number of Standard Bags
200
400
600
Number of Deluxe Bags
S
D
Points Satisfying
Departmental
Constraints
Points Satisfying
Minimum Production
Requirements
Minimum D
Minimum S
TABLE 8.2   RESOURCES NEEDED TO MANUFACTURE 500 STANDARD BAGS AND 
360 DELUXE BAGS
Operation
Minimum Required 
 Resources  
(hours)
Available 
Resources 
(hours)
Additional 
 Resources 
Needed (hours)
Cutting and dyeing
Sewing
Finishing
Inspection and packaging
7⁄10(500) 1 1(360) 5 710
  1⁄2(500) 1 5⁄6(360) 5 550
   1(500) 1 2⁄3(360) 5 740
1⁄10(500) 1 1⁄4(360) 5 140
630
600
708
135
80
None
32
 5

370 
Chapter 8 Linear Optimization Models
hours for finishing, and 5 more hours for inspection and packaging to meet management’s 
minimum production requirements.
If after reviewing this information, management still wants to manufacture 500 Stan-
dard and 360 Deluxe bags, additional resources must be provided. Perhaps by hiring another 
person to work in the cutting and dyeing department, transferring a person from elsewhere 
in the plant to work part-time in the finishing department, or having the sewing people help 
out periodically with the inspection and packaging, the resource requirements can be met. 
As you can see, many possibilities are available for corrective management action, once we 
discover the lack of a feasible solution. The important thing to realize is that linear program-
ming analysis can help determine whether management’s plans are feasible. By analyzing 
the problem using linear programming, we are often able to point out infeasible conditions 
and initiate corrective action.
Whenever you attempt to solve a problem that is infeasible, Excel Solver will return 
a message in the Solver Results dialog box indicating that no feasible solutions exists. In 
this case you know that no solution to the linear programming problem will satisfy all 
constraints, including the nonnegativity conditions. Careful inspection of your formula-
tion is necessary to try to identify why the problem is infeasible. In some situations, the 
only reasonable approach is to drop one or more constraints and re-solve the problem. If 
you are able to find an optimal solution for this revised problem, you will know that the 
constraint(s) that were omitted, in conjunction with the others, are causing the problem to 
be infeasible.
Unbounded
The solution to a maximization linear programming problem is unbounded if the value 
of the solution may be made infinitely large without violating any of the constraints; for a 
minimization problem, the solution is unbounded if the value may be made infinitely small.
As an illustration, consider the following linear program with two decision variables, 
X and Y.
Max 20X 1 10Y
s.t.
1X
$ 2
1Y # 5
X, Y $ 0
In Figure 8.10 we graphed the feasible region associated with this problem. Note that 
we can indicate only part of the feasible region because the feasible region extends 
indefinitely in the direction of the X-axis. Looking at the objective function lines in 
Figure 8.10, we see that the solution to this problem may be made as large as we de-
sire. In other words, no matter which solution we pick, we will always be able to reach 
some feasible solution with a larger value. Thus, we say that the solution to this linear 
program is unbounded.
Whenever you attempt to solve an unbounded problem using Excel Solver, you will 
receive a message in the Solver Results dialog box telling you that the “Objective Cell 
values do not converge.” In linear programming models of real problems, the occurrence 
of an unbounded solution means that the problem has been improperly formulated. We 
know it is not possible to increase profits indefinitely. Therefore, we must conclude that if 
a profit maximization problem results in an unbounded solution, the mathematical model 
does not represent the real-world problem sufficiently. In many cases, this error is the result 
of inadvertently omitting a constraint during problem formulation.

 
8.4 Special Cases of Linear Program Outcomes 
371
NOTES AND COMMENTS
1. Infeasibility is independent of the objective 
function. It exists because the constraints are 
so restrictive that no feasible region for the 
linear programming model is possible. Thus, 
when you encounter infeasibility, making 
changes in the coefficients of the objective 
function will not help; the problem will re-
main infeasible.
2.  The occurrence of an unbounded solution 
is often the result of a missing constraint. 
However, a change in the objective function 
may cause a previously unbounded problem 
to become bounded with an optimal solution. 
For example, the graph in Figure 8.10 shows 
an unbounded solution for the objective func-
tion Max 20X 1 10Y. However, changing the 
objective function to Max 2 20X 2 10Y will 
provide the optimal solution X 5 2 and Y 5 0 
even though no changes have been made in the 
constraints.
FigurE 8.10  EXAMPLE OF AN UNBOUNDED PROBLEM
0
5
10
15
20
5
10
20
X
Y
15
Feasible Region
Objective function
increases without limit.
20X + 10Y = 80
20X + 10Y = 160
20X + 10Y = 240
The parameters for optimization models are often less than certain. In the next section, 
we discuss the sensitivity of the optimal solution to uncertainty in the model parameters. In 
addition to the optimal solution, Excel Solver can provide some useful information on the 
sensitivity of that solution to changes in the model parameters.

372 
Chapter 8 Linear Optimization Models
Sensitivity analysis
Sensitivity analysis is the study of how the changes in the input parameters of an optimiza-
tion model affect the optimal solution. Using sensitivity analysis, we can answer questions 
such as the following:
 1. How will a change in a coefficient of the objective function affect the optimal  solution?
 2. How will a change in the right-hand-side value for a constraint affect the optimal 
solution?
Because sensitivity analysis is concerned with how these changes affect the optimal 
solution, the analysis does not begin until the optimal solution to the original linear pro-
gramming problem has been obtained. For that reason, sensitivity analysis is often referred 
to as postoptimality analysis. Let us return to the M&D Chemicals problem as an example 
of how to interpret the sensitivity report provided by Excel Solver.
Interpreting Excel Solver Sensitivity Report
Recall the M&D Chemicals problem discussed in Section 8.3. We had defined the  following 
decision variables and model:
A 5 number of gallons of product A
B 5 number of gallons of product B
Min
2A 1 3B
s.t.
1A
$ 125  Demand for product A
1A 1 1B $ 350  Total production
2A 1 1B # 600  Processing time
A, B $ 0
We found that the optimal solution is A 5 250 and B 5 100 with objective function value 5 
2(250) 1 3(100) 5 $800. The first constraint is not binding, but the second and third con-
straints are binding because 1(250) 1 1(100) 5 350 and 2(250) 1 100 5 600. After running 
Excel Solver, we may generate the Sensitivity Report by selecting Sensitivity from the 
Reports section of the Solver Results dialog box and then selecting OK. The Sensitivity 
report for the M&D Chemicals problem appears in Figure 8.11. There are two sections in 
this report: one for decision variables (Variable Cells) and one for Constraints.
Let us begin by interpreting the Constraints section. The cell location of the left-
hand side of the constraint, the constraint name, and the value of the left-hand side of the  
constraint at optimality are given in the first three columns. The fourth column gives 
the shadow price for each constraint. The shadow price for a constraint is the change in 
the optimal objective function value if the right-hand side of that constraint is increased 
by one. Let us interpret each shadow price given in the report in Figure 8.11.
The first constraint is: 1A $ 125. This is a nonbinding constraint because 250 . 125. 
If we change the constraint to 1A $ 126, there will be no change in the objective func-
tion value. The reason for this is that the constraint will remain nonbinding at the optimal 
solution, because 1A 5 250 . 126. Hence the shadow price is zero. In fact, nonbinding 
constraints will always have a shadow price of zero.
The second constraint is binding and its shadow price is 4. The interpretation of 
the shadow price is as follows If we change the constraint from 1A 1 1B $ 350 to
8.5

 
8.5 Sensitivity Analysis 
373
1A 1 1B $ 351, the optimal objective function value will increase by $4; that is, the new 
optimal solution will have objective function value equal to $800 1 $4 5 $804.
The third constraint is also binding and has a shadow price of 21. The interpretation 
of the shadow price is as follows. If we change the constraint from 2A 1 1B # 600 to 
2A 1 1B # 601, the objective function value will decrease by $1, that is, the new optimal 
solution will have objective function value equal to $800 2 $1 5 $799.
Note that the shadow price for the second constraint is positive, but for the third con-
straint it is negative. Why is this? The sign of the shadow price depends on whether the 
problem is a maximization or a minimization and the type of constraint under consideration. 
The M&D Chemicals problem is a cost minimization problem. The second constraint is 
a greater-than-or-equal-to constraint. By increasing the right-hand side, we make the con-
straint even more restrictive. This results in an increase in cost. Contrast this with the third 
constraint. The third constraint is a less-than-or-equal-to constraint. By increasing the right-
hand side, we make more hours available. We have made the constraint less restrictive. 
Because we have made the constraint less restrictive, there are no more feasible solutions 
from which to choose. Therefore, cost drops by $1.
When observing shadow prices, the following general principle holds. Making a bind-
ing constraint more restrictive degrades or leaves unchanged the optimal objective function 
value, and making a binding constraint less restrictive improves or leaves unchanged the 
optimal objective function. We shall see several more examples of this later in this chapter. 
Also, shadow prices are symmetric; so the negative of the shadow price is the change in the 
objective function for a decrease of one in the right-hand side.
In Figure 8.11, the Allowable Increase and the Allowable Decrease are the allow-
able changes in the right-hand side for which the current shadow price remains valid. For 
example, because the allowable increase in the processing time is 100, if we increase the 
processing time hours by 50 to 600 1 50 5 650, we can say with certainty that the optimal 
objective function value will change by (21)50 5 250. Hence, we know that optimal 
 objective function value will be $800 2 $50 5 $750. If we increase the right-hand side 
of the processing time beyond the allowable increase of 100, we cannot predict what will 
happen. Likewise, if we decrease the right-hand side of the processing time constraint by 
50, we know the optimal objective function value will change by the negative of the shadow 
price: 2(21)50 5 50. Cost will increase by $50. If we change the right-hand side by more 
than the allowable increase or decrease the shadow price is no longer valid
Making a constraint more 
restrictive is often referred 
to as tightening the con-
straint. Making a constraint 
less restrictive is often 
referred to as relaxing, or 
loosening, the constraint.
FigurE 8.11  SOLVER SENSITIVITY REPORT FOR THE M&D CHEMICALS PROBLEM
A
B
C
D
Name
Final
Value
Final
Value
Cost
Reduced
Objective
Coefﬁcient
Allowable
Increase
Allowable
Decrease
Price
Shadow
Constraint
R.H. Side
Allowable
Increase
Allowable
Decrease
Variable Cells
250
100
1
1E + 30
250
350
125
125
600
0
0
0
4
–1
2
3
125
350
600
100
Cell
Cell
Constraints
$B$14 Gallons Produced Product A
$C$14 Gallons Produced Product B
$B$19 Product A Provided
$B$20 Total Production Provided
$B$23 Processing Time Hours Used
Name
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
E
F
G
1E + 30
1
1E + 30
50
125
H

374 
Chapter 8 Linear Optimization Models
Let us now turn to the Variable Cells section of the Sensitivity Report. As in the con-
straint section, the cell location, variable name and final (optimal) value for each variable is 
given. The fourth column is Reduced Cost. The reduced cost for a decision variable is the 
shadow price of the nonnegativity constraint for that variable. In other words, the reduced 
cost indicates the change in the optimal objective function value that results from changing 
the right-hand side of the nonnegativity constraint from 0 to 1.
In the fifth column of the report, the objective function coefficient for the variable is 
given. The Allowable Increase and Allowable Decrease indicate the change in the objective 
function coefficient for which the current optimal solution will remain optimal. The value 
1E 1 30 in the report is essentially infinity. So long as the cost of product A, is greater 
than or equal to negative infinity and less than or equal to 2 1 1 5 3, the current solution 
remains optimal. For example, if the cost of product A is really $2.50 per gallon, we do 
not need to re-solve the model. Because the increase in cost of $0.50 is less than the allow-
able increase of $1.00, the current solution of 250 gallons of product A and 100 gallons of 
product B remains optimal.
As we have seen, the Excel Solver Sensitivity Report can provide useful information 
on the sensitivity of the optimal solution to changes in the model input data. However, this 
type of classical sensitivity analysis is somewhat limited. Classical sensitivity analysis is 
based on the assumption that only one piece of input data has changed; it is assumed that 
all other parameters remain as stated in the original problem. In many cases, however, we 
are interested in what would happen if two or more pieces of input data are changed simul-
taneously. The easiest way to examine the effect of simultaneous changes is to make the 
changes and rerun the model. 
NOTES AND COMMENTS
We defined the reduced cost as the shadow 
price of the nonnegativity constraint for that 
variable. When there is a binding simple upper 
bound constraint for a variable, the reduced cost 
reported by Solver is the shadow price of that 
upper bound constraint. Likewise, if there is a 
binding nonzero lower bound for a variable, the 
reduced cost is the shadow price for that lower 
bound constraint. So to be more general, the re-
duced cost for a decision variable is the shadow 
price of the binding simple lower or upper bound 
constraint for that variable.
General Linear programming Notation 
and More examples
Earlier in this chapter we showed how to formulate linear programming models for the 
Par, Inc. and M&D Chemicals problems. To formulate a linear programming model of the 
Par, Inc. problem, we began by defining two decision variables: S 5 number of standard 
bags and D 5 number of deluxe bags. In the M&D Chemicals problem, the two decision 
variables were defined as A 5 number of gallons of product A and B 5 number of gallons 
of product B. We selected decision-variable names of S and D in the Par, Inc. problem and 
A and B in the M&D Chemicals problem to make it easier to recall what these decision vari-
ables represented in the problem. Although this approach works well for linear programs 
involving a small number of decision variables, it can become difficult when dealing with 
problems involving a large number of decision variables.
A more general notation that is often used for linear programs uses the letter x with 
a subscript. For instance, in the Par, Inc. problem, we could have defined the decision 
8.6

 
8.6 General Linear Programming Notation and More Examples 
375
variables as follows:
 
x1 5 number of standard bags
 
x2 5 number of deluxe bags
In the M&D Chemicals problem, the same variable names would be used, but their defini-
tions would change:
 
x1 5 number of gallons of product A
 
x2 5 number of gallons of product B
A disadvantage of using general notation for decision variables is that we are no longer 
able to easily identify what the decision variables actually represent in the mathematical 
model. However, the advantage of general notation is that formulating a mathematical model 
for a problem that involves a large number of decision variables is much easier. For instance, 
for a linear programming model with three decision variables, we would use variable names 
of x1, x2, and x3; for a problem with four decision variables, we would use variable names of 
x1, x2, x3, and x4, and so on. Clearly, if a problem involved 1000 decision variables, trying to 
identify 1000 unique names would be difficult. However, using the general linear program-
ming notation, the decision variables would be defined as x1, x2, x3, …., x1000.
Using this new general notation, the Par, Inc. model would be written:
Max
10x1 1 9x2
s.t.
7⁄10 x1 1 1x2 # 630  Cutting and dyeing
1⁄2 x1 1
5⁄6 x2 # 600  Sewing
1x1 1
2⁄3 x2 # 708  Finishing
1⁄10 x1 1
1⁄4 x2 # 135  Inspection and packaging
x1, x2 $ 0
In some of the examples that follow in this section and in Chapters 9 and 10, we will use 
this type of subscripted notation.
Investment Portfolio Selection
In finance, linear programming can be applied in problem situations involving capital bud-
geting, make-or-buy decisions, asset allocation, portfolio selection, financial planning, and 
many more. Next, we describe a portfolio selection problem.
Portfolio selection problems involve situations in which a financial manager must se-
lect specific investments—for example, stocks and bonds—from a variety of investment 
alternatives. Managers of mutual funds, credit unions, insurance companies, and banks 
frequently encounter this type of problem. The objective function for portfolio selection 
problems usually is maximization of expected return or minimization of risk. The con-
straints usually take the form of restrictions on the type of permissible investments, state 
laws, company policy, maximum permissible risk, and so on. Problems of this type have 
been formulated and solved using a variety of optimization techniques. In this section we 
formulate and solve a portfolio selection problem as a linear program.
Consider the case of Welte Mutual Funds, Inc., located in New York City. Welte just 
obtained $100,000 by converting industrial bonds to cash and is now looking for other 
investment opportunities for these funds. Based on Welte’s current investments, the firm’s 
top financial analyst recommends that all new investments be made in the oil industry, 
steel industry, or government bonds. Specifically, the analyst identified five investment 
opportunities and projected their annual rates of return. The investments and rates of return 
are shown in Table 8.3.

376 
Chapter 8 Linear Optimization Models
Management of Welte imposed the following investment guidelines:
 1. Neither industry (oil or steel) should receive more than $50,000.
 2. The amount invested in government bonds should be at least 25 percent of the steel 
industry investments.
 3. The investment in Pacific Oil, the high-return but high-risk investment, cannot be more 
than 60 percent of the total oil industry investment.
What portfolio recommendations—investments and amounts—should be made for the 
available $100,000? Given the objective of maximizing projected return subject to the 
budgetary and managerially imposed constraints, we can answer this question by formu-
lating and solving a linear programming model of the problem. The solution will provide 
 investment recommendations for the management of Welte Mutual Funds.
Let us define the following decision variables:
 
X1 5 dollars invested in Atlantic Oil
 
X2 5 dollars invested in Pacific Oil
 
X3 5 dollars invested in Midwest Steel
 
X4 5 dollars invested in Huber Steel
 
X5 5 dollars invested in government bonds
Using the projected rates of return shown in Table 8.3, we write the objective function for 
maximizing the total return for the portfolio as
 
Max    0.073X1 1 0.103X2 1 0.064X3 1 0.075X4 1 0.045X5
The constraint specifying investment of the available $100,000 is
 
X1 1 X2 1 X3 1 X4 1 X5 5 100,000
The requirements that neither the oil nor steel industry should receive more than $50,000 are
 
X1 1 X2 # 50,000
 
X3 1 X4 # 50,000
The requirement that the amount invested in government bonds be at least 25 percent of the 
steel industry investment is expressed as
 
X5 $ 0.25(X3 1 X4)
Finally, the constraint that Pacific Oil cannot be more than 60 percent of the total oil in-
dustry investment is
 
X2 # 0.60(X1 1 X2)
By adding the nonnegativity restrictions, we obtain the complete linear programming model 
for the Welte Mutual Funds investment problem:
TABLE 8.3  INVESTMENT OPPORTUNITIES FOR WELTE MUTUAL FUNDS
Investment
Projected Rate  
of Return  
(%)
Atlantic Oil
Pacific Oil
Midwest Steel
Huber Steel 
Government bonds
 7.3
10.3
 6.4
 7.5
 4.5

 
8.6 General Linear Programming Notation and More Examples 
377
Max
0.073 X1 1 0.103 X2 1 0.064 X3 1 0.075 X4 1 0.045 X5
s.t.
X1 1
X2 1
X3 1
X4 1
X5 5 100,000
Available funds
X1 1
X2
#  50,000
Oil industry maximum
X3 1
X4
#  50,000
Steel industry maximum
X5 $ 0.25 (X3 1 X4)
Government bonds minimum
X2 # 0.60 (X1 1 X2)
Pacific Oil restriction
X1, X2, X3, X4, X5 $ 0
The optimal solution to this linear program is shown in Figure 8.12. Note that the op-
timal solution indicates that the portfolio should be diversified among all the investment 
opportunities except Midwest Steel. The projected annual return for this portfolio is $8,000, 
which is an overall return of 8 percent. Except for the upper bound on the Steel investment, 
all constraints are binding.
FigurE 8.12  THE SOLUTION FOR THE WELTE MUTUAL FUNDS PROBLEM
Projected Rate of Return
A
B
C
D
E
F
Parameters
1
3
2
4
5
6
7
8
9
11
Investment
Amount Invested
Funds Invested
Max Total Return
Funds Available
Unused Funds
= C23–B23
Max Allowed
=E5
=E6
=E7
=E8*(B14+B15)
=E9*(B16+B17)
Total
Atlantic Oil
Paciﬁc Oil
Oil
Midwest Steel
Steel
Huber Steel
Gov’t Bonds
0.073
0.103
0.064
0.075
0.045
Investment
Atlantic Oil
Paciﬁc Oil
Paciﬁc Oil
Midwest Steel
Huber Steel
Gov’t Bonds
Gov’t Bonds
Funds Invested
=SUM(B14:B18) 
=SUM(B14:B15) 
=SUM(B16:B17) 
=B18
=B15
Funds Invested
Min Required
Model
13
14
15
16
17
18
19
20
22
21
23
25
24
26
27
28
30
29
31
100000
50000
50000
Oil Max
Steel Max
Paciﬁc Oil Max
Gov’t Bonds Min 0.25
0.6
20000
30000
40000
10000
0
Welte Mutual Funds Problem
=SUMPRODUCT(B5:B9, B14:B18) 
Available Funds
10
12
Projected Rate of
Return
A
B
C
D
E
Parameters
1
3
2
4
5
6
7
8
9
11
Investment
Amount Invested
Funds Invested
Max Total
Return
Funds Available
Unused Funds
$0.00
Max Allowed
$100,000.00
$50,000.00
$50,000.00
$30,000.00
$10,000.00
Total
Atlantic Oil
Paciﬁc Oil
Oil
Midwest Steel
Steel
Huber Steel
Gov’t Bonds
0.073
0.103
0.064
0.075
0.045
Investment
Atlantic Oil
Paciﬁc Oil
Paciﬁc Oil
Midwest Steel
Huber Steel
Gov’t Bonds
Gov’t Bonds
Funds Invested
Funds Invested
Min Required
Model
13
14
15
16
17
18
19
20
22
21
23
25
24
26
27
28
30
29
31
$100.000.00
$50.000.00
$50.000.00
0.25
0.6
Welte Mutual Funds Problem
$100,000.00
$50,000.00
$40,000.00
$10,000.00
$30,000.00
$20,000.00
$30,000.00
$40,000.00
$10,000.00
$0.00
$8,000.00
Oil Max
Steel Max
Paciﬁc Oil Max
Gov’t Bonds Min
Available Funds
10
12
file
WEB
Welte
Cengage Learning 2015

378 
Chapter 8 Linear Optimization Models
NOTES AND COMMENTS
1. The optimal solution to the Welte Mutual Funds 
problem indicates that $20,000 is to be spent 
on the Atlantic Oil stock. If Atlantic Oil sells 
for $75 per share, we would have to purchase 
exactly 2662⁄3 shares in order to spend exactly 
$20,000. The difficulty of purchasing fractional 
shares can be handled by purchasing the largest 
possible integer number of shares with the allot-
ted funds (e.g., 266 shares of Atlantic Oil). This 
approach guarantees that the budget constraint 
will not be violated. This approach, of course, 
introduces the possibility that the solution will 
no longer be optimal, but the danger is slight 
if a large number of securities are involved. In 
cases where the analyst believes that the deci-
sion variables must have integer values, the 
problem must be formulated as an integer linear 
programming model (the topic of Chapter 9).
2. Financial portfolio theory stresses obtaining a 
proper balance between risk and return. In the 
Welte problem, we explicitly considered return 
in the objective function. Risk is controlled 
by choosing constraints that ensure diversity 
among oil and steel stocks and a balance be-
tween government bonds and the steel industry 
investment. In Chapter 10, we discuss invest-
ment portfolio models that control risk as mea-
sured by the variance of returns on investment.
Transportation Planning
The transportation problem arises frequently in planning for the distribution of goods and 
services from several supply locations to several demand locations. Typically, the quantity 
of goods available at each supply location (origin) is limited, and the quantity of goods 
needed at each of several demand locations (destinations) is known. The usual objective 
in a transportation problem is to minimize the cost of shipping goods from the origins to 
the destinations.
Let us revisit the transportation problem faced by Foster Generators, discussed in Chap-
ter 7. This problem involves the transportation of a product from three plants to four dis-
tribution centers. Foster Generators operates plants in Cleveland, Ohio; Bedford, Indiana; 
and York, Pennsylvania. Production capacities over the next three-month planning period 
for one type of generator are as follows:
Origin
Plant
Three-Month  
Production Capacity  
(units)
1
2
3
Cleveland
Bedford
York
 5,000
 6,000
 2,500
Total 13,500
The firm distributes its generators through four regional distribution centers located in 
Boston, Massachusetts; Chicago, Illinois; St. Louis, Missouri; and Lexington, Kentucky; 
the three-month forecast of demand for the distribution centers is as follows:
Destination
Distribution 
Center
Three-Month  
Demand Forecast 
(units)
1
2
3
4
Boston
Chicago
St. Louis
Lexington
 6,000
 4,000
2,000
 1,500
Total 13,500

 
8.6 General Linear Programming Notation and More Examples 
379
Management would like to determine how much of its production should be shipped 
from each plant to each distribution center. Figure 8.13 shows graphically the 12 distribu-
tion routes Foster can use. Such a graph is called a network; the circles are referred to as 
nodes, and the lines connecting the nodes as arcs. Each origin and destination is represented 
by a node, and each possible shipping route is represented by an arc. The amount of the 
supply is written next to each origin node, and the amount of the demand is written next to 
each destination node. The goods shipped from the origins to the destinations represent the 
flow in the network. Note that the direction of flow (from origin to destination) is indicated 
by the arrows.
For Foster’s transportation problem, the objective is to determine the routes to be used 
and the quantity to be shipped via each route that will provide the minimum total transpor-
tation cost. The cost for each unit shipped on each route is given in Table 8.4 and is shown 
on each arc in Figure 8.13.
A linear programming model can be used to solve this transportation problem. We use 
double-subscripted decision variables, with x11 denoting the number of units shipped from 
origin 1 (Cleveland) to destination 1 (Boston), x12 denoting the number of units shipped 
from origin 1 (Cleveland) to destination 2 (Chicago), and so on. In general, the decision 
variables for a transportation problem having m origins and n destinations are written as 
follows:
 
xij 5 number of units shipped from origin i to destination j
where i 5 1, 2, . . . , m and j 5 1, 2, . . . , n
Because the objective of the transportation problem is to minimize the total transporta-
tion cost, we can use the cost data in Table 8.4 or on the arcs in Figure 8.13 to develop the 
following cost expressions:
 
Transportation costs for 
 
   units shipped from Cleveland 5 3x11 1 2x12 1 7x13 1 6x14
 
Transportation costs for 
 
   units shipped from Bedford  5 6x21 1 5x22 1 2x23 1 3x24
 
Transportation costs for 
 
   units shipped from York
 5 2x31 1 5x32 1 4x33 1 5x34
The sum of these expressions provides the objective function showing the total transporta-
tion cost for Foster Generators.
Transportation problems need constraints because each origin has a limited supply 
and each destination has a demand requirement. We consider the supply constraints first. 
The capacity at the Cleveland plant is 5000 units. With the total number of units shipped 
from the Cleveland plant expressed as x11 1 x12 1 x13 1 x14, the supply constraint for the 
Cleveland plant is
 
x11 1 x12 1 x13 1 x14 # 5000 Cleveland supply
TABLE 8.4   TRANSPORTATION COST PER UNIT FOR THE FOSTER 
 GENERATORS TRANSPORTATION PROBLEM ($)
Destination
Origin
Boston
Chicago
St. Louis
Lexington
Cleveland
Bedford
York
3
6
2
2
5
5
7
2
4
6
3
5

380 
Chapter 8 Linear Optimization Models
With three origins (plants), the Foster transportation problem has three supply constraints. 
Given the capacity of 6000 units at the Bedford plant and 2500 units at the York plant, the 
two additional supply constraints are
 
x21 1  x22 1 x23 1 x24 # 6000    Bedford supply 
 
x31 1  x32 1 x33 1 x34 # 2500    York supply 
With the four distribution centers as the destinations, four demand constraints are 
needed to ensure that destination demands will be satisfied:
 
x11 1 x21 1 x31 5 6000    Boston demand 
 
x12 1 x22 1 x32 5 4000    Chicago demand 
 
x13 1 x23 1 x33 5 2000    St. Louis demand 
 
x14 1 x24 1 x34 5 1500    Lexington demand 
FigurE 8.13   THE NETWORK REPRESENTATION OF THE FOSTER 
 GENERATORS TRANSPORTATION PROBLEM
Plants
(origin nodes)
Distribution Centers
(destination nodes)
Supplies
Distribution Routes
(arcs)
Demands
1500
2500
6000
5000
3 
York
1 
Cleveland
3
St. Louis
2 
Chicago
1 
Boston
4 
Lexington
2000
4000
6000
2 
Bedford
3
2
7
6
6
5
2
3
2
5
4
5
Transportation
Cost per Unit

 
8.6 General Linear Programming Notation and More Examples 
381
Combining the objective function and constraints into one model provides a 12- variable, 
7-constraint linear programming formulation of the Foster Generators transportation 
problem:
Min
3x11 1 2x12 1 7x13 1 6x14 1 6x21 1 5x22 1 2x23 1 3x24 1 2x31 1 5x32 1 4x33 1
5x34
s.t.
x11 1
x12 1
x13 1
x14
# 5000
x21 1
x22 1
x23 1
x24
# 6000
x31 1
x32 1
x33 1
x34 # 2500
x11
1
x21
1
x31
5 6000
x12
1
x22
1
x32
5 4000
x13
1
x23
1
x33
5 2000
x14
1
x24
1
x34 5 1500
xij $ 0  for i 5 1, 2, 3 and j 5 1, 2, 3, 4
Comparing the linear programming formulation to the network in Figure 8.13 leads to 
several observations. All the information needed for the linear programming formulation 
is on the network. Each node has one constraint, and each arc has one variable. The sum 
of the variables corresponding to arcs from an origin node must be less than or equal to the 
origin’s supply, and the sum of the variables corresponding to the arcs into a destination 
node must be equal to the destination’s demand.
A spreadsheet model and the solution to the Foster Generators problem (Figure 8.14) 
shows that the minimum total transportation cost is $39,500. The values for the decision 
variables show the optimal amounts to ship over each route. For example, 1000 units should 
be shipped from Cleveland to Boston, and 4000 units should be shipped from Cleveland to 
Chicago. Other values of the decision variables indicate the remaining shipping quantities 
and routes.
Advertising Campaign Planning 
Applications of linear programming to marketing are numerous. Advertising campaign 
planning, marketing mix, and market research are just a few areas of application. In this 
section we consider an advertising campaign planning application.
Advertising campaign planning applications of linear programming are designed to 
help marketing managers allocate a fixed advertising budget to various advertising media. 
Potential media include newspapers, magazines, radio, television, and direct mail. In these 
applications, the objective is to maximize reach, frequency, and quality of exposure. Re-
strictions on the allowable allocation usually arise during consideration of company policy, 
contract requirements, and media availability. In the application that follows, we illustrate 
how a media selection problem might be formulated and solved using a linear program-
ming model.
Relax-and-Enjoy Lake Development Corporation is developing a lakeside community 
at a privately owned lake. The primary market for the lakeside lots and homes includes all 
middle- and upper-income families within approximately 100 miles of the development. 
Relax-and-Enjoy employed the advertising firm of Boone, Phillips, and Jackson (BP&J) to 
design the promotional campaign.
After considering possible advertising media and the market to be covered, BP&J rec-
ommended that the first month’s advertising be restricted to five media. At the end of the 
month, BP&J will then reevaluate its strategy based on the month’s results. BP&J collected 
data on the number of potential customers reached, the cost per advertisement, the maxi-
mum number of times each medium is available, and the exposure quality rating for each of 

382 
Chapter 8 Linear Optimization Models
the five media. The quality rating is measured in terms of an exposure quality unit, a mea-
sure of the relative value of one advertisement in each of the media. This measure, based on 
BP&J’s experience in the advertising business, takes into account factors such as audience 
demographics (age, income, and education of the audience reached), image presented, and 
quality of the advertisement. The information collected is presented in Table 8.5.
Relax-and-Enjoy provided BP&J with an advertising budget of $30,000 for the first 
month’s campaign. In addition, Relax-and-Enjoy imposed the following restrictions on how 
BP&J may allocate these funds: At least 10 television commercials must be used, at least 
FigurE 8.14   SPREADSHEET MODEL AND SOLUTION FOR THE FOSTER GENERATOR PROBLEM
Foster Generators
A
B
C
D
E
F
Shipping Cost/Unit
Parameters
Origin
Boston
St. Louis
Lexington
Supply
St. Louis
Destination
Chicago
Destination
Chicago
Total
Cleveland
Bedford
York
Demand
5000
6000
2500
Total Cost
Origin
Cleveland
Bedford
York
Model
1
2
3
4
5
6
9
10
7
8
11
12
13
14
15
16
17
18
19
20
21
3
6
2
6000
1000
2500
2500
2
5
5
4000
4000
0
0
Lexington
Total
6
3
5
1500
0
1500
0
Boston
=SUM(B17:B19) 
=SUM(C17:C19) 
7
2
4
2000
0
2000
0
=SUM(D17:D19) 
=SUM(E17:E19) 
=SUM(B17:E17)
=SUM(B18:E18)
=SUM(B19:E19)
=SUMPRODUCT(B5:E7,B17:E19) 
Foster Generators
Shipping Cost/Unit
Parameters
Origin
Boston
St. Louis
Lexington
Supply
St. Louis
Destination
Chicago
Destination
Chicago
Total
Cleveland
Bedford
York
Demand
$5.00
$2.00
$5.00
4000
$7.00
$2.00
$4.00
2000
$6.00
$3.00
$5.00
1500
5000
6000
2500
Total Cost
Origin
Cleveland
Bedford
York
Model
Lexington
Total
Boston
$3.00
$6.00
$2.00
6000
1000
2500
2500
6000
4000
0
0
4000
0
2000
0
2000
0
1500
0
1500
5000
6000
2500
$39,500.00
A
B
C
D
E
F
G
1
2
3
4
5
6
9
10
7
8
11
12
13
14
15
16
17
18
19
20
21
file
WEB
Foster

 
8.6 General Linear Programming Notation and More Examples 
383
50,000 potential customers must be reached, and no more than $18,000 may be spent on 
television advertisements. What advertising media selection plan should be recommended?
The decision to be made is how many times to use each medium. We begin by defining 
the decision variables:
 
DTV 5 number of times daytime TV is used
 
 ETV 5 number of times evening TV is used
 
 DN 5 number of times daily newspaper is used
 
 SN 5 number of times Sunday newspaper is used
 
 R 5 number of times radio is used
The data on quality of exposure in Table 8.5 show that each daytime TV (DTV) adver-
tisement is rated at 65 exposure quality units. Thus, an advertising plan with DTV adver-
tisements will provide a total of 65DTV exposure quality units. Continuing with the data in 
Table 8.5, we find evening TV (ETV) rated at 90 exposure quality units, daily newspaper 
(DN) rated at 40 exposure quality units, Sunday newspaper (SN) rated at 60 exposure qual-
ity units, and radio (R) rated at 20 exposure quality units. With the objective of maximizing 
the total exposure quality units for the overall media selection plan, the objective function 
becomes
 
Max    65DTV 1 90ETV 1 40DN 1 60SN 1 20R        Exposure quality
We now formulate the constraints for the model from the information given:
Each medium has a maximum availability:
DTV
# 15
ETV
# 10
DN
# 25
SN
#
4
R # 30
A total of $30,000 is available for the media campaign:
 
1500DTV 1 3000ETV 1 400DN 1 1000SN 1 100R # 30,000
TABLE 8.5   ADVERTISING MEDIA ALTERNATIVES FOR THE RELAX-AND-ENJOY LAKE 
 DEVELOPMENT CORPORATION
Advertising Media
Number of  
Potential  
Customers  
Reached
Cost ($) per 
Advertisement
Maximum 
Times  
Available  
per Month*
Exposure 
Quality 
Units
1.
2.
3.
4.
5.
Daytime TV (1 min), station WKLA
Evening TV (30 sec), station WKLA
Daily newspaper (full page),  
The Morning Journal
Sunday newspaper magazine (1⁄2 page color), 
The Sunday Press
Radio, 8:00 a.m. or 5:00 p.m. news (30 sec), 
station KNOP
1000
2000
1500
2500
 300
1500
3000
 400
1000
 100
15
10
25
 4
30
65
90
40
60
20
*The maximum number of times the medium is available is either the maximum number of times the advertising medium  occurs 
(e.g., four Sundays per month) or the maximum number of times BP&J recommends that the medium is used.

384 
Chapter 8 Linear Optimization Models
At least 10 television commercials must be used:
 
DTV 1 ETV $ 10
At least 50,000 potential customers must be reached
 
1000DTV 1 2000ETV 1 1500DN 1 2500SN 1 300R $ 50,000
No more than $18,000 may be spent on television advertisements
 
1500DTV 1 3000ETV # 18,000
By adding the nonnegativity restrictions, we obtain the complete linear programming model 
for the Relax-and-Enjoy advertising campaign planning problem:
Max 65 DTV 1 90 ETV 1 40 DN 1 60 SN 1 20 R   Exposure quality
s.t.
DTV
#
15
Availability  
of media
ETV
#
10
DN
#
25
SN
#
4
R #
30
1500 DTV 1 3000 ETV 1
400 DN 1 1000 SN 1 100 R # 30,000
Budget
DTV 1
ETV
$
10
Television  
restrictions
1500 DTV 1 3000 ETV
# 18,000
1000 DTV 1 2000 ETV 1 1500 DN 1 2500 SN 1 300 R $ 50,000
Customers reached
DTV, ETV, DN, SN, R $
0
A spreadsheet model and the optimal solution to this linear programming model is shown 
in Figure 8.15.
The optimal solution calls for advertisements to be distributed among daytime TV, 
daily newspaper, Sunday newspaper, and radio. The maximum number of exposure quality 
units is 2370, and the total number of customers reached is 61,500.
Let us consider now the Sensitivity Report for the Relax-and-Enjoy advertising cam-
paign planning problem shown in Figure 8.16. We begin by interpreting the Constraints 
section.
Note that the overall budget constraint has a shadow price of 0.060. Therefore, a $1.00 
increase in the advertising budget will lead to an increase of 0.06 exposure quality units. 
The shadow price of 225 for the number of TV ads indicates that increasing the number of 
television commercials required by 1 will decrease the exposure quality of the advertising 
plan by 25 units. Alternatively, decreasing the number of television commercials by 1 will 
increase the exposure quality of the advertising plan by 25 units. Thus, Relax-and-Enjoy 
should consider reducing the requirement of having at least 10 television commercials.
Note that the availability-of-media constraints are not listed in the constraint section. 
These types of constraints, simple upper (or lower) bounds on a decision variable, are not 
listed in the report, just as nonnegativity constraints are not listed. There is information 
about these constraints in the variables section under reduced cost. Let us therefore turn our 
attention to the Variable Cells section of the report.
There are three nonzero reduced costs in Figure 8.16. Let us interpret each. The variable 
ETV, the number of evening TV ads, is currently at its lower bound of zero. Therefore the 
reduced cost of 265 is the shadow price of the nonnegativity constraint, which we interpret 
as follows. If we change the requirement that ETV $ 0 to ETV $ 1, exposure will drop 
by 65. Notice that for the other variables that have a nonzero reduced cost, DN and R, the 
number of daily newspaper ads and radio ads respectively are at their upper bounds of

 
8.6 General Linear Programming Notation and More Examples 
385
25 and 30. In these cases, the reduced cost is the shadow price of the upper bound constraint 
on each of these variables. For example, allowing 31 rather than only 30 radio ads will 
increase exposures by 14.
The allowable increase and decrease for the objective function coefficients are inter-
preted as discussed in Section 8.5. For example, as long as the number of exposures per ad 
for daytime TV does not increase by more than 25 or decrease by more than 65, the current 
plan shown in Figure 8 15 remains optimal
FigurE 8.15   A SPREADSHEET MODEL AND THE SOLUTION FOR THE RELAX-AND-ENJOY LAKE DE-
VELOPMENT CORPORATION PROBLEM
A
B
C
D
E
F
G
Parameters
DTV
ETV
DN
SN
R
Media
Cust Reach
Cost/Ad
Availability
Exp./Ad
1000
1500
15
65
2000
3000
10
90
1500
400
25
40
Min Cust Reach
Min TV Ads.
Max TV Budget
Budget
2500
1000
4
60
300
100
30
20
Ads Placed
Max Exposure
Reach
Num TV Ads
TV Budget
Budget
Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
Achieved
Used
Limit
=I5
=I6
=I17
=I18
=SUMPRODUCT(B8:F8,B13:F13)
=SUMPRODUCT(B5:F5,B13:F13)
=SUMPRODUCT(B6:C6,B13:C13)
=SUMPRODUCT(B6:F6,B13:F13)
=B13+C13
Min Required
H
10
0
25
2
30
Relax-and-Enjoy Lake Development Corporation
ETV
DN
SN
R
DTV
I
50000
10
18000
30000
A
B
C
D
E
F
G
Parameters
DTV
ETV
DN
SN
R
Media
Cust Reach
Cost/Ad
Availability
Exp./Ad
1,500
$400
25
40
Min Cust Reach
Min TV Ads.
Max TV Budget
Budget
50,000
10
$18,000
&30,000
2,500
$1,000
4
60
300
$100
30
20
Ads Placed
Max Exposure
Reach
Num TV Ads
TV Budget
Budget
Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
17
18
19
21
22
23
Achieved
Used
Limit
2,000
$3,000
10
90
50,000
10
$18,000
$30,000
Min Required
H
1,000
$1,500
15
65
61,500
$15,000
$30,000
10
2370
10
0
25
2
30
Relax-and-Enjoy Lake Development Corporation
ETV
DN
SN
R
DTV
I
16
20
file
WEB
Relax

386 
Chapter 8 Linear Optimization Models
FigurE 8.16   THE EXCEL SENSITIVITY REPORT FOR THE RELAX-AND-ENJOY LAKE 
DEVELOPMENT CORPORATION PROBLEM
A
B
C
D
Name
Final
Value
Reduced
Cost
Objective
Coefﬁcient
Allowable
Increase
Allowable
Decrease
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Variable Cells
Cell
$B$13
Ads Placed DTV
$C$13
Ads Placed ETV
$E$13
Ads Placed SN
$D$13
Ads Placed DN
$F$13
Ads Placed R
10
0
2
25
30
0
–65
0
16
14
65
90
60
40
20
25
65
40
1E + 30
1E + 30
65
1E + 30
16.6666667
16
14
$B$22
TV Budget Used
$B$23
Budget Used
20
21
22
E
F
G
H
Final
Value
Shadow
Price
Constraint
R.H. Side
Allowable
Decrease
Cell
Constraints
$B$18
Reach Achieved
$B$19
Num TV Ads Achieve
$15,000.00
$30,000.00
61500
10
0
0.06
0
–25
18000
30000
50000
10
1E + 30
2000
11500
1.33333333
3000
2000
1E + 30
1.33333333
Name
Allowable
Increase
NOTES AND COMMENTS
1. The media selection model required subjec-
tive evaluations of the exposure quality for 
the media alternatives. Marketing managers 
may have substantial data concerning expo-
sure quality, but the final coefficients used 
in the objective function may also include 
considerations based primarily on managerial 
 judgment. 
2. The media selection model presented in this sec-
tion uses exposure quality as the objective func-
tion and places a constraint on the number of 
customers reached. An alternative formulation 
of this problem would be to use the number of 
customers reached as the objective function and 
add a constraint indicating the minimum total 
exposure quality required for the media plan.
Generating an alternative Optimal Solution 
for a Linear program
The goal of business analytics is to provide information to management for improved 
 decision making. If a linear program has more than one optimal solution, as discussed in 
Section 8.4, it would be good for management to know this. There might be factors external 
to the model that make one optimal solution preferable to another. For example, in a port-
folio optimization problem, perhaps more than one strategy yields the maximum expected 
return. However, those strategies might be quite different in terms of their risk to the inves-
tor. Knowing the optimal alternatives and then assessing the risk of each, the investor could 
then pick the least risky alternative from the optimal solutions. In this section we discuss 
how to generate an alternative optimal solution if one exists.
8.7

 
8.7 Generating an Alternative Optimal Solution for a Linear Program 
387
Let us reconsider the Foster Generators transportation problem from the previous sec-
tion. If one exists, how might we generate an alternative optimal solution for this problem? 
From Figure 8.14 we know the following is an optimal solution:
 
x11 5 1000, x12 5 4000, x13 5 0, x14 5 0
 
x21 5 2500, x22 5 0, x23 5 2000, x24 5 1500
 
x31 5 2500, x32 5 0, x33 5 0, x34 5 0 
The optimal cost is $39,500. With this information, we may revise our previous model to 
try to find an alternative optimal solution. We know that any alternative solution must be 
feasible, so it must satisfy all of the constraints of the original model. Also, to be optimal, 
the solution must give a total cost of $39,500. We can enforce this by taking the objective 
function and making it a constraint equal to $39,500.
3x11 1 2x12 1 7x13 1 6x14 1 6x21 1 5x22 1 2x23 1 3x24 1 2x31 1 5x32 1 4x33 1 5x34 5 39,500
But, what should our objective function be for the revised problem? In the solution we 
previously found
 
x13 5 x14 5 x22 5 x32 5 x33 5 x34 5 0
If we maximize the sum of these variables and if the optimal objective function value of this 
revised problem is positive, we have found a different feasible solution that is also optimal. 
The revised model is
Max
x13 1
x14 1
x22 1
x32 1
x33 1
x34
s.t.
x11 1
x12 1
x13 1
x14
#
5,000
x21 1
x22 1
x23 1
x24
#
6,000
x31 1
x32 1
x33 1
x34 #
2,500
x11
1
x21
1
x31
5
6,000
x12
1
x22
1
x32
5
4,000
x13
1
x23
1
x33
5
2,000
x14
1
x24
1
x34 5
1,500
3x11 1 2x12 1 7x13 1 6x14 1 6x21 1 5x22 1 2x23 1 3x24 1 2x31 1 5x32 1 4x33 1 5x34 5 39,500
xij $ 0
  for i 5 1, 2, 3 and j 5 1, 2, 3, 4
The solution to this problem has objective function value 5 2500, indicating that the vari-
ables that were zero in the previous solution now add up to 2500. The new solution in 
shown in Table 8.6.
TABLE 8.6   AN ALTERNATIVE OPTIMAL SOLUTION TO THE FOSTER 
 GENERATORS TRANSPORTATION PROBLEM
Total Cost 5 $39,500
Amount Shipped
To:
Boston
Chicago
St. Louis
Lexington
Total
From:
Cleveland
3500
1500
   0
   0
5000
Bedford
   0
2500
2000
1500
6000
York
2500
   0
   0
   0
2500
Total
6000
4000
2000
1500

388 
Chapter 8 Linear Optimization Models
Comparing Figure 8.14 and Table 8.6, we see that in this new solution, Bedford ships 
2500 units to Chicago instead of Boston.
What types of issues might make management prefer one of these solutions over the 
other? Notice that the original solution has the Boston distribution center sourced from all 
three plants, whereas each of the other distribution centers is sourced by one plant. This 
would imply that the manager in the Boston distribution center has to deal with three dif-
ferent plant managers, whereas each of the other distribution center managers has only 
one plant manager. The Boston manager might feel disadvantaged, having to spend too 
much time coordinating among the plants. The alternative solution provides a more bal-
anced solution. Managers in Boston and Chicago each deal with two plants, and those in 
St. Louis and Lexington, which have lower total volumes, deal with only one plant. Because 
the alternative solution seems to be more equitable, it might be preferred. Recall that both 
solutions give a total cost of $39,500.
In summary, the general approach for trying to find an alternative optimal solution to 
a linear program is:
Step 1: Solve the linear program
Step 2: Make a new objective function to be maximized. It is the sum of those variables 
that were equal to zero in the solution from Step 1
Step 3:  Keep all the constraints from the original problem. Add a constraint that forces 
the original objective function to be equal to the optimal objective function 
value from Step 1
Step 4:  Solve the problem created in Steps 2 and 3. If the objective function value is 
positive, you have found an alternative optimal solution
Summary
We formulated linear programming models for the Par, Inc. maximization problem and 
the M&D Chemicals minimization problem. For the Par, Inc. problem, we showed how 
a graphical solution procedure could be used to solve a two-variable problem to help us 
better understand how the computer can solve large linear programs. We discussed how 
Excel Solver can be used to solve linear optimization problems. In formulating a linear 
programming model of the Par, Inc. and M&D problems, we developed a general definition 
of a linear program.
A linear program is a mathematical model with the following qualities:
 1. A linear objective function that is to be maximized or minimized
 2. A set of linear constraints
 3. Variables restricted to nonnegative values
Slack variables may be used to write less-than-or-equal-to constraints in equality form, 
and surplus variables may be used to write greater-than-or-equal-to constraints in equality 
form. The value of a slack variable can usually be interpreted as the amount of unused re-
source, whereas the value of a surplus variable indicates the amount over and above some 
stated minimum requirement. Binding constraints have zero slack or surplus.
If the solution to a linear program is infeasible or unbounded, no optimal solution to 
the problem can be found. In the case of infeasibility, no feasible solutions are possible. In 
the case of an unbounded solution, the objective function can be made infinitely large for 
a maximization problem and infinitely small for a minimization problem. In the case of 
alternative optimal solutions, two or more optimal extreme points exist.

 
Glossary 
389
We also discussed sensitivity analysis and the interpretation of the Sensitivity Report 
generated by Excel Solver and how the impact of changes in the objective function coef-
ficients and right-hand side values of constraints can be assessed. We showed how to write 
a mathematical model using general linear programming notation and presented three ad-
ditional examples of linear programming applications: portfolio selection, transportation 
planning, and media selection. Finally, we concluded the chapter with a procedure for 
finding an alternative optimal solution when one exists.
Glossary
Objective function The expression that defines the quantity to be maximized or minimized 
in a linear programming model.
Constraints Restrictions that limit the settings of the decision variables.
Problem formulation (modeling) The process of translating a verbal statement of a prob-
lem into a mathematical statement called the mathematical model.
Decision variable A controllable input for a linear programming model.
Nonnegativity constraints A set of constraints that requires all variables to be nonnegative.
Mathematical model A representation of a problem where the objective and all constraint 
conditions are described by mathematical expressions.
Linear programming model (linear program) A mathematical model with a linear objec-
tive function, a set of linear constraints, and nonnegative variables.
Linear function A mathematical function in which each variable appears in a separate term 
and is raised to the first power
Feasible solution A solution that satisfies all the constraints simultaneously.
Feasible region The set of all feasible solutions.
Extreme point Graphically speaking, extreme points are the feasible solution points oc-
curring at the vertices, or “corners,” of the feasible region. With two-variable problems, 
extreme points are determined by the intersection of the constraint lines.
Binding constraint A constraint that holds as an equality at the optimal solution.
Slack variable A variable added to the left-hand side of a less-than-or-equal-to constraint to 
convert the constraint into an equality. The value of this variable can usually be interpreted 
as the amount of unused resource.
Surplus variable A variable subtracted from the left-hand side of a greater-than-or-equal-
to constraint to convert the constraint into an equality. The value of this variable can usually 
be interpreted as the amount over and above some required minimum level.
Alternative optimal solutions The case in which more than one solution provides the 
optimal value for the objective function.
Infeasibility The situation in which no solution to the linear programming problem satisfies 
all the constraints.
Unbounded The situation in which the value of the solution may be made infinitely large in 
a maximization linear programming problem or infinitely small in a minimization problem 
without violating any of the constraints.
Sensitivity analysis The study of how changes in the input parameters of a linear program-
ming problem affect the optimal solution.
Objective function coefficient allowable increase (decrease) The allowable increase/
decrease of an objective function coefficient is the amount the coefficient may increase 
(decrease) without causing any change in the values of the decision variables in the optimal 
solution. The allowable increase/decrease for the objective function coefficients can be 
used to calculate the range of optimality.
Shadow price The change in the optimal objective function value per unit increase in the 
right-hand side of a constraint.

390 
Chapter 8 Linear Optimization Models
Reduced cost If a variable is at its lower bound of zero, the reduced cost is equal to the 
shadow price of the nonnegativity constraint for that variable. In general, if a variable is 
at its lower or upper bound, the reduced cost is the shadow price for that simple lower or 
upper bound constraint.
Right-hand side allowable increase (decrease) The allowable increase (decrease) of the 
right-hand side of a constraint is the amount the right-hand side may increase (decrease) 
without causing any change in the shadow price for that constraint. The allowable increase 
and decrease for the right-hand side can be used to calculate the range of feasibility for that 
constraint.
Problems
 1. Kelson Sporting Equipment, Inc., makes two types of baseball gloves: a regular model and 
a catcher’s model. The firm has 900 hours of production time available in its cutting and 
sewing department, 300 hours available in its finishing department, and 100 hours available 
in its packaging and shipping department. The production time requirements and the profit 
contribution per glove are given in the following table:
Production Time (hours)
Model
Cutting  
and Sewing
Finishing
Packaging  
and Shipping
Profit/Glove
Regular model
Catcher’s model
1
3⁄2
1⁄2
1⁄3
1⁄8
1⁄4
$5
$8
 
 Assuming that the company is interested in maximizing the total profit contribution, 
 answer the following:
a. What is the linear programming model for this problem?
b. Develop a spreadsheet model and find the optimal solution using Excel Solver. How 
many of each model should Kelson manufacture?
c. What is the total profit contribution Kelson can earn with the optimal production 
 quantities?
d. How many hours of production time will be scheduled in each department?
e. What is the slack time in each department?
 2. The Sea Wharf Restaurant would like to determine the best way to allocate a monthly 
advertising budget of $1000 between newspaper advertising and radio advertising. Man-
agement decided that at least 25 percent of the budget must be spent on each type of 
media and that the amount of money spent on local newspaper advertising must be at 
least twice the amount spent on radio advertising. A marketing consultant developed an 
index that measures audience exposure per dollar of advertising on a scale from 0 to 100, 
with higher values implying greater audience exposure. If the value of the index for local 
newspaper advertising is 50 and the value of the index for spot radio advertising is 80, 
how should the restaurant allocate its advertising budget to maximize the value of total 
audience  exposure?
a. Formulate a linear programming model that can be used to determine how the res-
taurant should allocate its advertising budget in order to maximize the value of total 
audience exposure.
b. Develop a spreadsheet model and solve the problem using Excel Solver.
 3. Blair & Rosen, Inc. (B&R) is a brokerage firm that specializes in investment portfolios 
designed to meet the specific risk tolerances of its clients. A client who contacted B&R 
this past week has a maximum of $50,000 to invest. B&R’s investment advisor decides 
to recommend a portfolio consisting of two investment funds: an Internet fund and a Blue 

 
Problems 
391
Chip fund. The Internet fund has a projected annual return of 12 percent, and the Blue 
Chip fund has a projected annual return of 9 percent. The investment advisor requires that 
at most $35,000 of the client’s funds should be invested in the Internet fund. B&R ser-
vices include a risk rating for each investment alternative. The Internet fund, which is the 
more risky of the two investment alternatives, has a risk rating of 6 per $1,000 invested. 
The Blue Chip fund has a risk rating of 4 per $1,000 invested. For example, if $10,000 is 
invested in each of the two investment funds, B&R’s risk rating for the portfolio would be 
6(10) 1 4(10) 5 100. Finally, B&R developed a questionnaire to measure each client’s 
risk tolerance. Based on the responses, each client is classified as a conservative, moder-
ate, or aggressive investor. Suppose that the questionnaire results classified the current 
client as a moderate investor. B&R recommends that a client who is a moderate investor 
limit his or her portfolio to a maximum risk rating of 240.
a. Formulate a linear programming model to find the best investment strategy for this 
client.
b. Build a spreadsheet model and solve the problem using Solver. What is the rec-
ommended investment portfolio for this client? What is the annual return for the 
 portfolio?
c. Suppose that a second client with $50,000 to invest has been classified as an aggres-
sive investor. B&R recommends that the maximum portfolio risk rating for an aggres-
sive investor is 320. What is the recommended investment portfolio for this aggressive 
investor?
d. Suppose that a third client with $50,000 to invest has been classified as a conservative 
investor. B&R recommends that the maximum portfolio risk rating for a conserva-
tive investor is 160. Develop the recommended investment portfolio for the conserva-
tive investor.
 4. Adirondack Savings Bank (ASB) has $1 million in new funds that must be allocated to 
home loans, personal loans, and automobile loans. The annual rates of return for the three 
types of loans are 7 percent for home loans, 12 percent for personal loans, and 9 percent 
for automobile loans. The bank’s planning committee has decided that at least 40 percent 
of the new funds must be allocated to home loans. In addition, the planning committee 
has specified that the amount allocated to personal loans cannot exceed 60 percent of the 
amount allocated to automobile loans.
a. Formulate a linear programming model that can be used to determine the amount of 
funds ASB should allocate to each type of loan to maximize the total annual return for 
the new funds.
b. How much should be allocated to each type of loan? What is the total annual return? 
What is the annual percentage return?
c. If the interest rate on home loans increases to 9 percent, would the amount allocated 
to each type of loan change? Explain.
d. Suppose the total amount of new funds available is increased by $10,000. What effect 
would this have on the total annual return? Explain.
e. Assume that ASB has the original $1 million in new funds available and that the 
planning committee has agreed to relax the requirement that at least 40 percent of the 
new funds must be allocated to home loans by 1 percent. How much would the annual 
return change? How much would the annual percentage return change?
 5. Round Tree Manor is a hotel that provides two types of rooms with three rental classes: 
Super Saver, Deluxe, and Business. The profit per night for each type of room and rental 
class is as follows:
Rental Class
Room
Super Saver
Deluxe
Business
Type I
Type II
$30
$20
$35 
$30
—
$40

392 
Chapter 8 Linear Optimization Models
 
 Type I rooms do not have wireless Internet access and are not available for the Business 
rental class. Round Tree’s management makes a forecast of the demand by rental class 
for each night in the future. A linear programming model developed to maximize profit 
is used to determine how many reservations to accept for each rental class. The demand 
forecast for a particular night is 130 rentals in the Super Saver class, 60 rentals in the 
Deluxe class, and 50 rentals in the Business class. Round Tree has 100 Type I rooms and 
120 Type II rooms.
a. Use linear programming to determine how many reservations to accept in each rental 
class and how the reservations should be allocated to room types. Is the demand by 
any rental class not satisfied? Explain.
b. How many reservations can be accommodated in each rental class?
c. Management is considering offering a free breakfast to anyone upgrading from a 
 Super Saver reservation to Deluxe class. If the cost of the breakfast to Round Tree is 
$5, should this incentive be offered?
d. With a little work, an unused office area could be converted to a rental room. If the 
conversion cost is the same for both types of rooms, would you recommend converting 
the office to a Type I or a Type II room? Why?
e. Could the linear programming model be modified to plan for the allocation of rental 
demand for the next night? What information would be needed and how would the 
model change?
 6. Industrial Designs has been awarded a contract to design a label for a new wine produced 
by Lake View Winery. The company estimates that 150 hours will be required to complete 
the project. The firm’s three graphic designers available for assignment to this project 
are Lisa, a senior designer and team leader; David, a senior designer; and Sarah, a junior 
designer. Because Lisa has worked on several projects for Lake View Winery, manage-
ment specified that Lisa must be assigned at least 40 percent of the total number of hours 
assigned to the two senior designers. To provide label designing experience for Sarah, the 
junior designer must be assigned at least 15 percent of the total project time. However, 
the number of hours assigned to Sarah must not exceed 25 percent of the total number of 
hours assigned to the two senior designers. Due to other project commitments, Lisa has 
a maximum of 50 hours available to work on this project. Hourly wage rates are $30 for 
Lisa, $25 for David, and $18 for Sarah.
a. Formulate a linear program that can be used to determine the number of hours each 
graphic designer should be assigned to the project to minimize total cost.
b. How many hours should each graphic designer be assigned to the project? What is the 
total cost?
c. Suppose Lisa could be assigned more than 50 hours. What effect would this have on 
the optimal solution? Explain.
d. If Sarah were not required to work a minimum number of hours on this project, would 
the optimal solution change? Explain.
 7. Vollmer Manufacturing makes three components for sale to refrigeration companies. The 
components are processed on two machines: a shaper and a grinder. The times (in min-
utes) required on each machine are as follows:
Machine
Component
Shaper
Grinder
1
2
3
6
4
4
4
5
2

 
Problems 
393
 
 The shaper is available for 120 hours, and the grinder is available for 110 hours. No more 
than 200 units of component 3 can be sold, but up to 1000 units of each of the other com-
ponents can be sold. In fact, the company already has orders for 600 units of component 1 
that must be satisfied. The profit contributions for components 1, 2, and 3 are $8, $6, and 
$9, respectively.
a. Formulate and solve for the recommended production quantities.
b. What are the objective coefficient ranges for the three components? Interpret these 
ranges for company management.
c. What are the right-hand-side ranges? Interpret these ranges for company management.
d. If more time could be made available on the grinder, how much would it be worth?
e. If more units of component 3 can be sold by reducing the sales price by $4, should the 
company reduce the price?
 8. PhotoTech, Inc., a manufacturer of rechargeable batteries for digital cameras, signed a 
contract with a digital photography company to produce three models of lithium-ion bat-
tery packs for a new line of digital cameras. The contract calls for the following:
Battery Pack
Production Quantity
PT-100
PT-200
PT-300
200,000
100,000
150,000
 
 PhotoTech can manufacture the battery packs at manufacturing plants located in the 
 Philippines and Mexico. The unit cost of the battery packs differs at the two plants because 
of differences in production equipment and wage rates. The unit costs for each battery pack 
at each manufacturing plant are as follows:
Plant
Product
Philippines
Mexico
PT-100
PT-200
PT-300
$0.95
$0.98
$1.34
$0.98
$1.06
$1.15
 
 The PT-100 and PT-200 battery packs are produced using similar production equipment 
available at both plants. However, each plant has a limited capacity for the total number of 
PT-100 and PT-200 battery packs produced. The combined PT-100 and PT-200 production 
capacities are 175,000 units at the Philippines plant and 160,000 units at the Mexico plant. 
The PT-300 production capacities are 75,000 units at the Philippines plant and 100,000 
units at the Mexico plant. The cost of shipping from the Philippines plant is $0.18 per unit, 
and the cost of shipping from the Mexico plant is $0.10 per unit.
a. Develop a linear program that PhotoTech can use to determine how many units of each 
battery pack to produce at each plant to minimize the total production and shipping 
cost associated with the new contract.
b. Solve the linear program developed in part a, to determine the optimal production plan.
c. Use sensitivity analysis to determine how much the production and/or shipping 
cost per unit would have to change to produce additional units of the PT-100 in the 
 Philippines plant.
d. Use sensitivity analysis to determine how much the production and/or shipping cost 
per unit would have to change to produce additional units of the PT-200 in the Mexico 
plant.

394 
Chapter 8 Linear Optimization Models
 9. The Westchester Chamber of Commerce periodically sponsors public service seminars 
and programs. Currently, promotional plans are under way for this year’s program. Ad-
vertising alternatives include television, radio, and newspaper. Audience estimates, costs, 
and maximum media usage limitations are as shown:
Constraint
Television
Radio
Newspaper
Audience per advertisement
Cost per advertisement
Maximum media usage
100,000
 $2,000
     10
18,000
  $300
    20
40,000
  $600
    10
 
 To ensure a balanced use of advertising media, radio advertisements must not exceed 
50 percent of the total number of advertisements authorized. In addition, television should 
account for at least 10 percent of the total number of advertisements authorized.
a. If the promotional budget is limited to $18,200, how many commercial messages 
should be run on each medium to maximize total audience contact? What is the al-
location of the budget among the three media, and what is the total audience reached?
b. By how much would audience contact increase if an extra $100 were allocated to the 
promotional budget?
 10. The management of Hartman Company is trying to determine the amount of each of two 
products to produce over the coming planning period. The following information concerns 
labor availability, labor utilization, and product profitability:
Labor-Hours Required 
(hours/unit)
Department
Product 1
Product 2
Hours Available
A
B
C
Profit contribution/unit
  1.00
  0.30
  0.20
$30.00
  0.35
  0.20
  0.50
$15.00
100
 36
 50
a. Develop a linear programming model of the Hartman Company problem. Solve the 
model to determine the optimal production quantities of products 1 and 2.
b. In computing the profit contribution per unit, management does not deduct labor costs 
because they are considered fixed for the upcoming planning period. However, sup-
pose that overtime can be scheduled in some of the departments. Which departments 
would you recommend scheduling for overtime? How much would you be willing to 
pay per hour of overtime in each department?
c. Suppose that 10, 6, and 8 hours of overtime may be scheduled in departments A, B, and 
C, respectively. The cost per hour of overtime is $18 in department A, $22.50 in de-
partment B, and $12 in department C. Formulate a linear programming model that can 
be used to determine the optimal production quantities if overtime is made available. 
What are the optimal production quantities, and what is the revised total contribution 
to profit? How much overtime do you recommend using in each department? What is 
the increase in the total contribution to profit if overtime is used?
 11. The employee credit union at State University is planning the allocation of funds for the 
coming year. The credit union makes four types of loans to its members. In addition, 
the credit union invests in risk-free securities to stabilize income. The various revenue-
producing investments, together with annual rates of return, are as follows:
Type of Loan/Investment
Annual Rate of Return (%)
Automobile loans
Furniture loans
Other secured loans
Signature loans
Risk-free securities
 8
10
11
12
 9

 
Problems 
395
 
 The credit union will have $2 million available for investment during the coming year. 
State laws and credit union policies impose the following restrictions on the composition 
of the loans and investments:
 
Risk-free securities may not exceed 30 percent of the total funds available for  investment.
 
Signature loans may not exceed 10 percent of the funds invested in all loans (automo-
bile, furniture, other secured, and signature loans).
 
Furniture loans plus other secured loans may not exceed the automobile loans.
 
Other secured loans plus signature loans may not exceed the funds invested in risk-free 
securities.
 
 How should the $2 million be allocated to each of the loan/investment alternatives to 
maximize total annual return? What is the projected total annual return?
 12. The Atlantic Seafood Company (ASC) is a buyer and distributor of seafood products that 
are sold to restaurants and specialty seafood outlets throughout the Northeast. ASC has a 
frozen storage facility in New York City that serves as the primary distribution point for 
all products. One of the ASC products is frozen large black tiger shrimp, which are sized 
at 16–20 pieces per pound. Each Saturday ASC can purchase more tiger shrimp or sell 
the tiger shrimp at the existing New York City warehouse market price. The ASC goal is 
to buy tiger shrimp at a low weekly price and sell it later at a higher price. ASC currently 
has 20,000 pounds of tiger shrimp in storage. Space is available to store a maximum of 
100,000 pounds of tiger shrimp each week. In addition, ASC developed the following 
estimates of tiger shrimp prices for the next four weeks:
Week
Price/lb.
1
2
3
4
$6.00
$6.20
$6.65
$5.55
 
 ASC would like to determine the optimal buying/storing/selling strategy for the next four 
weeks. The cost to store a pound of shrimp for one week is $0.15, and to account for un-
foreseen changes in supply or demand, management also indicated that 25,000 pounds of 
tiger shrimp must be in storage at the end of week 4. Determine the optimal buying/storing/
selling strategy for ASC. What is the projected four-week profit? (Hint: Define variables 
for buying, selling, and inventory held in each week. Then use a constraint to define the 
relationship between these: inventory from end of previous period 1 bought this period 2 
sold this period 5 inventory at end of this period. This type of constraint is referred to as 
an inventory balance constraint.)
 13. The Silver Star Bicycle Company will be manufacturing both men’s and women’s models 
for its Easy-Pedal bicycles during the next two months. Management wants to develop a 
production schedule indicating how many bicycles of each model should be produced in 
each month. Current demand forecasts call for 150 men’s and 125 women’s models to 
be shipped during the first month and 200 men’s and 150 women’s models to be shipped 
during the second month. Additional data are as follows:
Labor Requirements (hours)
Production
Model
Current
Inventory
Costs
Manufacturing
Assembly
Men’s
Women’s
$120
$90
2.0
1.6
1.5
1.0
20
30
 
 Last month the company used a total of 1000 hours of labor. The company’s labor relations 
policy will not allow the combined total hours of labor (manufacturing plus assembly)

396 
Chapter 8 Linear Optimization Models
to increase or decrease by more than 100 hours from month to month. In addition, the 
company charges monthly inventory at the rate of 2 percent of the production cost based 
on the inventory levels at the end of the month. The company would like to have at least 
25 units of each model in inventory at the end of the two months. (Hint: Define variables 
for production and inventory held in each period for each product. Then use a constraint to 
define the relationship between these: inventory from end of previous period 1 produced 
this period 2 demand this period 5 inventory at end of this period.)
a. Establish a production schedule that minimizes production and inventory costs and 
satisfies the labor-smoothing, demand, and inventory requirements. What inventories 
will be maintained and what are the monthly labor requirements?
b. If the company changed the constraints so that monthly labor increases and decreases 
could not exceed 50 hours, what would happen to the production schedule? How much 
will the cost increase? What would you recommend?
 14. The Clark County Sheriff’s Department schedules police officers for 8-hour shifts. The 
beginning times for the shifts are 8:00 a.m., noon, 4:00 p.m., 8:00 p.m., midnight, and 
4:00 a.m. An officer beginning a shift at one of these times works for the next 8 hours. 
During normal weekday operations, the number of officers needed varies depending on 
the time of day. The department staffing guidelines require the following minimum num-
ber of officers on duty:
Time of Day
Minimum Officers  
On Duty
8:00 a.m.–Noon
Noon–4:00 p.m.
4:00 p.m.–8:00 p.m.
8:00 p.m.–Midnight
Midnight–4:00 a.m.
4:00 a.m.–8:00 a.m.
 5
 6
10
 7
 4
 6
 
 Determine the number of police officers that should be scheduled to begin the 8-hour shifts 
at each of the six times (8:00 a.m., noon, 4:00 p.m., 8:00 p.m., midnight, and 4:00 a.m.) 
to minimize the total number of officers required. (Hint: Let x1 5 the number of officers 
beginning work at 8:00 a.m., x2 5 the number of officers beginning work at noon, and 
so on.)
 15. Bay Oil produces two types of fuel (regular and super) by mixing three ingredients. The 
major distinguishing feature of the two products is the octane level required. Regular fuel 
must have a minimum octane level of 90, whereas super must have a level of at least 100. 
The cost per barrel, octane levels, and available amounts (in barrels) for the upcoming 
two-week period appear in the following table, along with the maximum demand for each 
end product and the revenue generated per barrel:
Ingredient
Cost/bbl
Octane
Available (bbl)
1
2
3
$16.50
$14.00
$17.50
100
 87
110
110,000
350,000
300,000
Revenue/bbl
Max Demand (bbl)
Regular
Super
$18.50
$20.00
350,000
500,000
 
 Develop and solve a linear programming model to maximize contribution to profit. What 
is the optimal contribution to profit?

 
Problems 
397
 16. Consider the following network representation of a transportation problem: 
Supplies
Demands
10
20
30
Omaha
Kansas
City
Des
Moines
St.
Louis
15
25
Jefferson
City
14
16
7
8
10
5
 
 The supplies, demands, and transportation costs per unit are shown on the network. What 
is the optimal (cost minimizing) distribution plan?
 17. Refer to the transportation problem described in Problem 16. Use the procedure described 
in Section 8.7 to try to find an alternative optimal solution.
 18. Aggie Power Generation supplies electrical power to residential customers for many U.S. 
cities. Its main power generation plants are located in Los Angeles, Tulsa, and Seattle. The 
following table shows Aggie Power Generation’s major residential markets, the annual 
demand in each market (in Megawatts or MWs), and the cost to supply electricity to each 
market from each power generation plant (prices are in $/MW).
Distribution Costs ($/MW)
City
Los Angeles
Tulsa
Seattle
Demand (MWs)
Seattle
Portland
San Francisco
Boise
Reno
Bozeman
Laramie
Park City
Flagstaff
Durango
$356.25
$356.25
$178.13
$356.25
$237.50
$415.63
$356.25
$356.25
$178.13
$356.25
$593.75
$593.75
$475.00
$475.00
$475.00
$415.63
$415.63
$356.25
$475.00
$296.88
$ 59.38
$178.13
$296.88
$296.88
$356.25
$296.88
$356.25
$475.00
$593.75
$593.75
 950.00
 831.25
2375.00
 593.75
 950.00
 593.75
1187.50
 712.50
1187.50
1543.75
a. If there are no restrictions on the amount of power that can be supplied by any of the 
power plants, what is the optimal solution to this problem? Which cities should be 
supplied by which power plants? What is the total annual power distribution cost for 
this solution?
file
WEB
Aggie

398 
Chapter 8 Linear Optimization Models
b. If at most 4000 MWs of power can be supplied by any one of the power plants, what 
is the optimal solution? What is the annual increase in power distribution cost that 
results from adding these constraints to the original formulation?
 19. The Calhoun Textile Mill is in the process of deciding on a production schedule. It wishes 
to know how to weave the various fabrics it will produce during the coming quarter. The 
sales department has confirmed orders for each of the 15 fabrics produced by Calhoun. 
These demands are given in the following table. Also given in this table is the variable 
cost for each fabric. The mill operates continuously during the quarter: 13 weeks, 7 days 
a week, and 24 hours a day.
 
  There are two types of looms: dobbie and regular. Dobbie looms can be used to make 
all fabrics and are the only looms that can weave certain fabrics, such as plaids. The rate 
of production for each fabric on each type of loom is also given in the table. Note that if 
the production rate is zero, the fabric cannot be woven on that type of loom. Also, if a 
fabric can be woven on each type of loom, then the production rates are equal. Calhoun 
has 90 regular looms and 15 dobbie looms. For this problem, assume the time requirement 
to change over a loom from one fabric to another is negligible.
 
  Management would like to know how to allocate the looms to the fabrics and which 
fabrics to buy on the market so as to minimize the cost of meeting demand.
Fabric
Demand 
(yd)
Dobbie  
(yd/hr)
Regular 
(yd/hr)
Mill Cost  
($/yd)
Sub. Cost 
($/yd)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
 16,500
 52,000
 45,000
 22,000
 76,500
110,000
122,000
 62,000
  7,500
 69,000
 70,000
 82,000
 10,000
380,000
 62,000
4.653
4.653
4.653
4.653
5.194
3.809
4.185
5.232
5.232
5.232
3.733
4.185
4.439
5.232
4.185
 0.00
 0.00
 0.00
 0.00
5.194
3.809
4.185
5.232
5.232
5.232
3.733
4.185
4.439
5.232
4.185
0.6573
0.5550
0.6550
0.5542
0.6097
0.6153
0.6477
0.4880
0.5029
0.4351
0.6417
0.5675
0.4952
0.3128
0.5029
0.80
0.70
0.85
0.70
0.75
0.75
0.80
0.60
0.70
0.60
0.80
0.75
0.65
0.45
0.70
 20. Refer to the Calhoun Mills make versus buy problem described in Problem 19. Use the pro-
cedure described in Section 8.7 to try to find an alternative optimal solution. If you are suc-
cessful, discuss the differences in the solution you found versus that found in Problem 19.
Investment Strategy
J. D. Williams, Inc. is an investment advisory firm that manages more than $120 million 
in funds for its numerous clients. The company uses an asset allocation model that rec-
ommends the portion of each client’s portfolio to be invested in a growth stock fund, an 
income fund, and a money market fund. To maintain diversity in each client’s portfolio, 
the firm places limits on the percentage of each portfolio that may be invested in each of 
the three funds. General guidelines indicate that the amount invested in the growth fund 
must be between 20 and 40 percent of the total portfolio value. Similar percentages for the 
other two funds stipulate that between 20 and 50 percent of the total portfolio value must 
file
WEB
Calhoun
Case Problem

 
Appendix Solving Linear Optimization Models  Using Analytic Solver Platform 
399
be in the income fund and that at least 30 percent of the total portfolio value must be in the 
money market fund.
In addition, the company attempts to assess the risk tolerance of each client and adjust 
the portfolio to meet the needs of the individual investor. For example, Williams just con-
tracted with a new client who has $800,000 to invest. Based on an evaluation of the client’s 
risk tolerance, Williams assigned a maximum risk index of 0.05 for the client. The firm’s 
risk indicators show the risk of the growth fund at 0.10, the income fund at 0.07, and the 
money market fund at 0.01. An overall portfolio risk index is computed as a weighted aver-
age of the risk rating for the three funds, where the weights are the fraction of the client’s 
portfolio invested in each of the funds.
Additionally, Williams is currently forecasting annual yields of 18 percent for the 
growth fund, 12.5 percent for the income fund, and 7.5 percent for the money market fund. 
Based on the information provided, how should the new client be advised to allocate the 
$800,000 among the growth, income, and money market funds? Develop a linear program-
ming model that will provide the maximum yield for the portfolio. Use your model to 
develop a managerial report.
Managerial Report
 1. Recommend how much of the $800,000 should be invested in each of the three funds. 
What is the annual yield you anticipate for the investment recommendation?
 2. Assume that the client’s risk index could be increased to 0.055. How much would the 
yield increase, and how would the investment recommendation change?
 3. Refer again to the original situation where the client’s risk index was assessed to be 
0.05. How would your investment recommendation change if the annual yield for the 
growth fund were revised downward to 16 percent or even to 14 percent?
 4. Assume that the client expressed some concern about having too much money in the 
growth fund. How would the original recommendation change if the amount invested 
in the growth fund is not allowed to exceed the amount invested in the income fund?
 5. The asset allocation model you developed may be useful in modifying the portfolios for 
all of the firm’s clients whenever the anticipated yields for the three funds are periodi-
cally revised. What is your recommendation as to whether use of this model is possible?
Solving Linear Optimization Models 
 Using analytic Solver platform
In this appendix, we illustrate how to use Analytic Solver Platform (ASP) to solve linear 
programs in Excel. We assume that ASP has been installed.
Recall the M&D Chemicals problem. The linear optimization model we developed is
A 5 number of gallons of product A
B 5 number of gallons of product B
Min
2A 1 3B
s.t.
1A
$ 125  Demand for product A
1A 1 1B $ 350  Total production
2A 1 1B # 600  Processing time
A, B $ 0
The spreadsheet model is as shown in Figure 8.17.
Appendix

400 
Chapter 8 Linear Optimization Models
FigurE 8.17  EXCEL SPREADSHEET MODEL FOR M&D CHEMICALS
A
M&D Chemicals
B
C
D
Parameters
Product A
Product B
Product A
Product B
Time Available
600
1
1
2
3
4
5
6
7
8
9
10
11
Processing Time (hours)
Provided
Required
Production Cost
Minimum Total Production
Product A Minimum
Product A
Total Production
Processing Time
=C23–B23
2
2
350
125
3
Gallons Produced
250
=SUMPRODUCT(B4:C4,B14:C14)
=SUMPRODUCT(B5:C5,B14:C14)
=B14
=B14 + C14
Hours Used
=D4
=B8
=B7
Hours Available
Unused Hours
100
Model
Minimize Total Cost
12
13
14
15
16
17
18
19
20
24
21
23
22
A
M&D Chemicals
B
D
C
Parameters
Product A
Product B
Product A
Product B
Time Available
600
1
2
3
4
5
6
7
8
9
10
11
Processing Time (hours)
Provided
Required
Production Cost
Minimum Total Production
Minimize Total Cost
Product A
Total Production
Processing Time
600
0
2
$2.00
350
125
Gallons Produced
$800.00
250
350
Hours Used
Hours Available
Unused Hours
600
1
$3.00
125
350
Model
Minimize Total Cost
12
13
14
15
16
17
18
19
20
21
23
22
250
100
file
WEB
M&D

 
Appendix Solving Linear Optimization Models  Using Analytic Solver Platform 
401
Open the file M&D. To solve the M&D Chemicals problem using ASP, follow these steps:
Step 1. Click the ANALYTICS SOLVER PLATFORM tab in the Ribbon
Step 2.  When the Solver Options and Model Specifications task pane appears, click 
the  next to Optimization to expand the tree structure (If the task pane is not 
visible, click Model in the Model group to activate this pane.) 
Step 3.  When the optimization tree structure appears (Figure 8.18): 
Select Objective
Select cell B16
Click the Add button  in the Solver Options and Model Specifications 
task pane (Figure 8.18)
Select $B$16 (Max) under Objective
Right click and select Edit
When the Change Objective dialog box appears, select Min for To:
Click OK
Step 4. Under Variables, select Normal
Select cells B14:C14
Click the Add button  in the Solver Options and Model Specifications 
task pane
file
WEB
M&D
FigurE 8.18   THE OPTIMIZATION TREE STRUCTURE IN THE SOLVER 
 OPTIONS AND MODEL SPECIFICATIONS DIALOG BOX
Delete
Add
Analyze
Solve
Refresh

402 
Chapter 8 Linear Optimization Models
Step 5. Under Constraints, select Normal
Select cells B19:B20
Click the Add button  in the Solver Options and Model Specifications 
task pane
When the Add Constraint dialog box appears:
Select .5 from the drop-down button
Enter C19:C20 in the Constraint: area
Click OK
Step 6. Under Constraints, select Normal
Select cell B23
Click the Add button  in the Solver Options and Model Specifications 
task pane
When the Add Constraint dialog box appears:
Select ,5 from the drop-down button
Enter C23 in the Constraint: area
Click OK
 
The Solver Options and Model Specifications dialog box should now appear 
as shown in Figure 8.19.
FigurE 8.19   THE M&D CHEMCIAL MODEL IN THE SOLVER OPTIONS AND 
MODEL SPECIFICATIONS DIALOG BOX

 
Appendix Solving Linear Optimization Models  Using Analytic Solver Platform 
403
Step 7. Click the Engine tab in the Solver Options and Model Specifications task 
pane
Select the checkbox for Automatically Select Engine
In the General area click Assume Non-Negative
Select True from the drop-down menu
Step 8. Click the Model tab in the Solver Options and Model Specifications task 
pane
Step 9. To solve the problem, click the Solve button 
 in the Solver Options and 
Model Specifications task pane (Figure 8.18)
The solution is sent to the spreadsheet, and the output appears under the Output tab in the 
Solver Options and Model Specifications task pane (Figure 8.20). The output indicates 
that the optimal solution was found and that all constraints were satisfied. The solution 
should be the same as when we solved the M&D Chemicals problem using standard Excel 
Solver as shown in Figure 8.6. The complete linear programming model for the M&D 
Chemicals problem can be found in the file M&DModel.
Upon clicking the Solve 
button, the Guided Mode 
dialog box will appear 
if ASP’s Guided Mode is 
turned on. ASP’s Guided 
Mode assists the analyst 
through the optimiza-
tion process and can be 
turned off at the analyst’s 
 discretion.
file
WEB
M&DModel
FigurE 8.20   THE OUTPUT FROM THE OPTIMIZATION FROM ANALYTIC 
SOLVER PLATFORM

404 
Chapter 8 Linear Optimization Models
NOTES AND COMMENTS
1. The Solver Options and Model Specifications 
dialog box can be moved by clicking the banner 
at the top (the location of the title of the box), 
holding the mouse, and dragging. The box can 
be invoked or hidden by clicking on the Model 
button on the far left of the ASP Ribbon.
2. The default objective type is maximize. To 
change the objective function to minimize, se-
lect the objective function location in the opti-
mization tree, right-click the mouse, select Edit, 
and then select Min. Alternatively, select the 
objective function location in the optimization 
tree, click the Objective button in the Optimi-
zation Model group in the ASP Ribbon, select 
Min, and select Normal.
3. The objective, variables, and constraints can 
also be added to the model by clicking the 
 Decisions, Constraints, and Objective buttons 
respectively in the Optimization Model group 
of the ASP Ribbon.
4. The Refresh button  
 (Figure 8.18) should be 
used whenever a model has been changed (new 
constraints, variables, or other changes have 
been made).
5. To delete elements (variables, constraints, or 
objective) from the model, select the element 
from the optimization tree, and click the Delete 
button  
 (Figure 8.18). To keep a variable or 
constraint in the model but have it ignored for a 
given run, click the box next to that element (the 
green checkmark will disappear, indicating it is 
not part of the current model run).
6. The Analyze button  
 (Figure 8.18) provides 
an analysis of the model including number and 
types of variables and constraints. The report 
appears at the bottom of the Solver Options 
and Model Specifications dialog box.
7. To generate an Answer or Sensitivity Report af-
ter solving the problem (as discussed previously 
in this chapter), select Reports from the Analy-
sis group of the ASP Ribbon. Select Optimiza-
tion and then Answer for an Answer Report or 
Sensitivity for a Sensitivity Report.
8. ASP allows you to solve larger linear programs 
than Standard Excel Solver. Standard Solver is 
limited to 200 variables and 100 constraints. 
ASP is limited to 8000 variables and 8000 con-
straints for linear programs.

Integer Linear 
 Optimization Models
CONTENTS
9.1 
TYPES OF INTEGER LINEAR 
OPTIMIZATION MODELS
9.2 
EASTBORNE REALTY, AN 
EXAMPLE OF INTEGER 
 OPTIMIZATION
The Geometry of Linear All-
Integer Optimization 
9.3 
SOLVING INTEGER 
 OPTIMIZATION PROBLEMS 
WITH EXCEL SOLVER
A Cautionary Note About 
Sensitivity Analysis
9.4 
APPLICATIONS INVOLVING 
BINARY VARIABLES
Capital Budgeting
Fixed Cost
Bank Location
Product Design and Market Share 
Optimization
9.5 
MODELING FLEXIBILITY 
PROVIDED BY BINARY 
VARIABLES
Multiple-Choice and Mutually 
Exclusive Constraints
k out of n Alternatives Constraint
Conditional and Corequisite 
Constraints
9.6 
GENERATING 
 ALTERNATIVES IN BINARY 
OPTIMIZATION
APPENDIX:  SOLVING 
 INTEGER LINEAR 
 OPTIMIZATION 
PROBLEMS USING 
ANALYTIC SOLVER 
PLATFORM
Chapter 9

406 
Chapter 9 Integer Linear Optimization Models
In this chapter we discuss a class of problems that are modeled as linear programs with 
the additional requirement that one or more variables must be integer. Such problems are 
called integer linear programs.
The objective of this chapter is to provide an applications-oriented introduction to 
integer linear programming. First, in Section 9.1, we discuss the different types of integer 
linear programming models. In Section 9.2, we discuss an example, Eastborne Realty and 
the geometry of all-integer linear programs, and in Section 9.3, we show how to use Excel 
Solver to solve integer optimization problems. In Section 9.4, we discuss four applica-
tions of integer linear programming that make use of binary variables: capital budgeting, 
fixed cost, bank location, and market share optimization problems. In Section 9.5, we 
provide additional illustrations of the modeling flexibility provided by binary variables. 
In Section 9.6, we discuss ways to generate useful alternative solutions in integer linear 
optimization. Appendix 9.1 discusses how to use Analytic Solver Platform to solve integer 
linear optimization problems.
types of Integer Linear Optimization Models
The only difference between the problems in this chapter and the ones in Chapter 8 on 
linear programming is that one or more variables are required to be integer. If all variables 
are required to be integer, we have an all-integer linear program. The following is a two-
variable, all-integer linear programming model:
9.1
Petrobras, the largest corporation in Brazil, operates 
approximately 80 offshore oil production and explo-
ration platforms in the oil-rich Campos Basin. One 
of Petrobras’ biggest challenges is the planning of 
its logistics, including how to efficiently and safely 
transport nearly 1900 employees per day from its four 
mainland bases to the offshore platforms. Every day, 
planners must route and schedule the helicopters used 
to transport Petrobras employees from the mainland to 
the offshore locations and back to the mainland. This 
routing and scheduling problem is challenging be-
cause there are over a billion possible combinations of 
schedules and routes. 
Petrobras uses mixed integer linear optimization 
to solve its helicopter transport scheduling and rout-
ing problem. The objective function of the optimization 
model is a weighted function designed to ensure safety, 
minimize unmet demand, and minimize the cost of the 
transport of its crews. Because offshore landings are the 
riskiest part of the transport, the safety objective is met 
by minimizing the number of offshore landings required 
in the schedule. Numerous constraints must be met in 
planning these routes and schedule. These include limit-
ing the number of departures from a platform at certain 
times; ensuring no time conflicts for a given helicopter 
and pilot; ensuring proper breaks for pilots; and limit-
ing the number of flights per day for a given helicopter 
and routing restrictions. The decision variables include 
binary variables for assigning helicopters to flights and 
pilots to break times, as well as variables on the number 
of passengers per flight. 
Compared to the previously used manual approach 
to this problem, the new approach using the integer op-
timization model transports the same number of pas-
sengers but with 18 percent fewer offshore landings, 
8  percent less flight time, and a reduction in cost of 
14 percent. The annual cost savings is estimated to be 
approximately $24 million.
OPTIMIZING THE TRANSPORT OF OIL RIG CREWS*
AnAlytics  in  Action
*Based on F. Menezes et al., “Optimizing Helicopter Transport of Oil Rig 
Crews at Petrobras,” Interfaces 40. no. 5 (September–October 2010): 
408–416.

 
9.2 Eastborne Realty, An Example of Integer  Optimization  
407
 
Max 2x1 1 3x2
 
s.t.
 
3x1 1 3x2 # 12
 
2⁄3 x1 1 1x2 # 4
 
1x1 1 2x2 # 6
 
x1, x2 $ 0 and integer
If we drop the phrase and integer from the last line of this model, we have the familiar 
two-variable linear program. The linear program that results from dropping the integer 
requirements is called the linear programming relaxation, or LP Relaxation, of the integer 
linear program.
If some, but not necessarily all, variables are required to be integer, we have a mixed-
integer linear program. The following is a two-variable, mixed-integer linear program:
 
Max 3x1 1 4x2
 
s.t.
 
21x1 1 2x2 # 8
 
1x1 1 2x2 # 12
 
2x1 1 1x2 # 16
 
x1, x2 $ 0 and x2 integer
We obtain the LP Relaxation of this mixed-integer linear program by dropping the 
requirement that x2 be integer.
In some applications, the integer variables may take on only the values 0 or 1. Then 
we have a binary integer linear program. As we see later in the chapter, binary variables 
provide additional modeling capability. 
eastborne realty, an example 
of Integer  Optimization 
Eastborne Realty has $2 million available for the purchase of new rental property. After an 
initial screening, Eastborne reduced the investment alternatives to townhouses and apart-
ment buildings. Each townhouse can be purchased for $282,000, and five are available. 
Each apartment building can be purchased for $400,000, and the developer will construct 
as many buildings as Eastborne wants to purchase.
Eastborne’s property manager can devote up to 140 hours per month to these new prop-
erties; each townhouse is expected to require 4 hours per month, and each apartment building 
is expected to require 40 hours per month. The annual cash flow, after deducting mortgage 
payments and operating expenses, is estimated to be $10,000 per townhouse and $15,000 per 
apartment building. Eastborne’s owner would like to determine the number of townhouses 
and the number of apartment buildings to purchase to maximize annual cash flow.
We begin by defining the decision variables:
 
T 5 number of townhouses
 
A 5 number of apartment buildings
The objective function for cash flow (in thousands of dollars) is
 
Max 10T 1 15A
Three constraints must be satisfied:
282T 1 400A # 2000  Funds available ($1000s)
 
4T 1  40A #  140  Manager’s time (hours)
 
T        #     5  Townhouses available
9.2

408 
Chapter 9 Integer Linear Optimization Models
The variables T and A must be nonnegative. In addition, the purchase of a fractional 
number of townhouses and/or a fractional number of apartment buildings is unacceptable. 
Thus, T and A must be integer. The model for the Eastborne Realty problem is the following 
all-integer linear program:
 
Max 10T 1 15A
 
s.t.
 
282T 1 400A # 2000
 
4T 1  40A # 140
 
 T        # 5
 
T, A $ 0 and integer
The model for Eastborne Realty is a linear all-integer program. Next we discuss the geom-
etry of this model. 
The Geometry of Linear All-Integer Optimization
The geometry of the feasible region for the Eastborne Reality problem is shown in Fig-
ure 9.1. The shaded region is the feasible region of the LP Relaxation. The optimal linear 
programming solution is point b, which is T 5 2.479 townhouses and A 5 3.252 apart-
ment buildings. The optimal value of the objective function is 73.574, which indicates 
an annual cash flow of $73,574. Point b is formed by the intersection of the Managers 
Time constraint and the Available Funds constraint. Unfortunately, Eastborne cannot 
purchase fractional numbers of townhouses and apartment buildings; further analysis 
is necessary.
In many cases, a noninteger solution can be rounded to obtain an acceptable integer 
solution. For instance, a linear programming solution to a production scheduling problem 
might call for the production of 15,132.4 cases of breakfast cereal. The rounded integer 
solution of 15,132 cases would probably have minimal impact on the value of the objective 
function and the feasibility of the solution. Rounding would be a sensible approach. Indeed, 
whenever rounding has a minimal impact on the objective function and constraints, most 
managers find it acceptable. A near-optimal solution is fine.
However, rounding may not always be a good strategy. When the decision variables 
take on small values that have a major impact on the value of the objective function or 
feasibility, an optimal integer solution is needed. Let us return to the Eastborne Realty 
problem and examine the impact of rounding. The optimal solution to the LP Relaxation 
for Eastborne Realty resulted in T 5 2.479 townhouses and A 5 3.252 apartment build-
ings. Because each townhouse costs $282,000 and each apartment building costs $400,000, 
rounding to an integer solution can be expected to have a substantial economic impact on 
the problem.
Suppose that we round the solution to the LP Relaxation to obtain the integer solution 
T 5 2 and A 5 3, with an objective function value of 10(2) 1 15(3) 5 65. The annual cash 
flow of $65,000 is substantially less than the annual cash flow of $73,574 provided by the 
solution to the LP Relaxation. Do other rounding possibilities exist? Exploring other round-
ing alternatives shows that the integer solution T 5 3 and A 5 3 is infeasible because it re-
quires $282,000(3) 1 $400,000(3) 5 $3,738,000, which is more funds than the $2,000,000 
Eastborne has available. The rounded solution of T 5 2 and A 5 4 is also infeasible for 
the same reason. At this point, rounding has led to two townhouses and three apartment 
buildings with an annual cash flow of $65,000 as the best feasible integer solution to the 
problem. Unfortunately, we don’t know whether this solution is the best integer solution 
to the problem.

 
9.2 Eastborne Realty, An Example of Integer  Optimization  
409
Rounding to an integer solution is a trial-and-error approach. Each rounded solution 
must be evaluated for feasibility as well as for its impact on the value of the objective func-
tion. Even when a rounded solution is feasible, we do not have a guarantee that we have 
found the optimal integer solution. We will see shortly that the rounded solution (T 5 2 and 
A 5 3) is not optimal for Eastborne Realty.
What is the true feasible region for the Eastborne Realty problem? As shown in 
Figure 9.1, the feasible region is the set of integer points that lie within the feasible 
region of the LP Relaxation. There are 20 such feasible solutions (designated by blue 
dots in the figure). The region bounded by the dashed lines is known as the convex hull 
of the set of feasible integer solutions. The convex hull of a set of points is the smallest 
intersection of linear inequalities that contain the set of points. Notice that the convex 
hull in Figure 9.1 has integer extreme points (points d, e, f, g, h, and i). If we knew 
the convex hull, we could use linear programming to find the optimal integer corner 
point. Unfortunately, identifying the convex hull can be very time-consuming. This is 
somewhat counterintuitive because there are only 20 feasible solutions, but solving an 
integer optimization problem such as Eastborne Realty’s may require solving numerous 
linear programs to find the optimal integer solution. Therefore, an integer optimization 
problem can be much more time-consuming to solve than solving a linear program of 
comparable size.
It is true that the optimal solution to the integer program will be an extreme point of 
the convex hull, so one or more of the extreme points d, e, f, g, h, and i are optimal. The 
FIGURE 9.1   THE GEOMETRY OF THE EASTBORNE REALTY PROBLEM
1
2
3
4
5
6
T
A
a
f
g
h
c
i
d
e
b
Number of Townhouses 
Objective Function = 70
Available Funds
Constraint
1
2
3
4
5
6
Feasible Region 
Manager’s Time
Constraint
Number of Apartment Buildings
Townhouse
Availability
Constraint
Optimal Integer Solution
T = 4, A = 2
Note: Dots show the location of
          feasible integer solutions

410 
Chapter 9 Integer Linear Optimization Models
 objective function contour shown in Figure 9.1 with objective function value equal to 70 
shows that point h is the optimal solution. As a check, let us evaluate each of the corner 
points of the convex hull in Figure 9.1:
 
Point 
T 5 
A 5  
Annual Cash Flow ($000) 5 
 
d 
5 
0 
10(5) 1 15(0) 5 50
 
e 
0 
0 
10(0) 1 15(0) 5 0
 
f 
0 
3 
10(0) 1 15(3) 5 45
 
g 
2 
3 
10(2) 1 15(3) 5 65
 
h 
4 
2 
10(4) 1 15(2) 5 70 
 
i 
5 
1 
10(5) 1 15(1) 5 65
This confirms that the optimal integer solution occurs at point h, where T 5 4 town-
houses and A 5 2 apartment buildings. The objective function value is an annual cash 
flow of $70,000. This solution is substantially better than the best solution found by 
rounding: T 5 2, A 5 3 with an annual cash flow of $65,000. Thus, we see that rounding 
would not have been the best strategy for Eastborne Realty.
NOTES AND COMMENTS
1. An important observation can be made from the 
analysis of the Eastborne Realty problem. It has 
to do with the relationship between the value of 
the optimal integer solution and the value of the 
optimal solution to the LP Relaxation. For in-
teger linear programs involving maximization, 
the value of the optimal solution to the LP Re-
laxation provides an upper bound on the value 
of the optimal integer solution. This observation 
is valid for the Eastborne Realty problem. The 
value of the optimal integer solution is $70,000, 
and the value of the optimal solution to the LP 
Relaxation is $73,574. Thus, we know from the 
LP Relaxation solution that the upper bound for 
the value of the objective function is $73,574. 
For integer linear programs involving minimi-
zation, the value of the optimal solution to the 
LP Relaxation provides a lower bound on the 
value of the optimal integer solution.
2. The two popular approaches to solving integer 
linear optimization problems are branch and 
bound and cutting planes. Both solve a series 
of LP relaxations to arrive at an optimal inte-
ger solution. The branch-and-bound approach 
breaks the feasible region of the LP Relaxation 
into subregions until the subregions have inte-
ger solutions or it is determined that the solu-
tion cannot be in the subregion. Cutting plane 
approaches try to identify the convex hull by 
adding a series of new constraints that do not ex-
clude any feasible integer points. Indeed, most 
software for integer optimization, including Ex-
cel Solver, employ a combination of these two 
approaches.
Solving Integer Optimization 
problems with excel Solver
The worksheet formulation and solution for integer linear programs is similar to that for 
linear programming problems. Actually the worksheet formulation is exactly the same, but 
some additional information must be provided when setting up the Solver Parameters and 
Options dialog boxes. Constraints must be added in the Solver Parameters dialog box to 
9.3

 
9.3 Solving Integer Optimization Problems with Excel Solver 
411
identify the integer variables. In addition, the value for Tolerance in the Integer Options 
dialog box may need to be adjusted to obtain a solution.
Let us demonstrate the Excel solution of an integer linear program by showing how 
Excel Solver can be used to solve the Eastborne Realty problem. The worksheet with the 
optimal solution is shown in Figure 9.2. We will describe the key elements of the worksheet 
and how to obtain the solution, and then we will interpret the solution.
The parameters and descriptive labels appear in cells A1:G7 of the worksheet in Fig-
ure 9.2. The cells in the lower portion of the worksheet contain the information required 
FIGURE 9.2   EASTBORNE REALTY SPREADSHEET MODEL
Eastborne Realty Problem
A
B
C
D
E
F
G
Parameters
Price (000)
Townhouse
Apt. Bldg.
Apt. Bldgs.
Mgr. Time
Ann. Cash Flow ($000)
Funds Avl. ($000)
Mgr. Time Avl. (Hours)
Townhouses Avl.
282
4
10
400
40
15
Purchase Plan
Total Used
Total Available
4
2
Max Cash Flow ($000)
Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
=SUMPRODUCT(B7:C7,B14:C14)
Townhouses
2000
140
5
=G4
=G5
=G6
Funds ($000)
Funds (Hours)
=SUMPRODUCT(B4:C4,$B$14:$C$14)
=SUMPRODUCT(B5:C5,$B$14:$C$14)
=B14
Townhouses
Number of
Eastborne Realty Problem
A
B
C
D
E
F
G
H
Price (000)
Townhouse
Apt. Bldg.
Apt. Bldgs.
Mgr. Time
Ann. Cash Flow ($000)
Funds Avl. ($000)
Mgr. Time Avl. (Hours)
Townhouses Avl.
$400
40
$15
$2,000
140
5
Purchase Plan
Total Used
Total Available
2
Max Cash Flow ($000)
Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
$282
4
$10
4
$70
$2,000
140
5
Funds ($000)
Townhouses
Time (Hours)
$1,928
96
4
Townhouses
Number of
Parameters
file
WEB
Eastborne

412 
Chapter 9 Integer Linear Optimization Models
by the Excel Solver (decision variables, objective function, constraint left-hand sides, and 
constraint right-hand sides).
Decision variables 
Cells B14:C14 are reserved for the decision variables. 
Objective function 
 The formula 5SUMPRODUCT(B7:C7,B14:C14) has been 
placed into cell B17 to reflect the annual cash flow associated 
with the solution. 
Left-hand sides 
 The left-hand sides for the three constraints are placed into cells 
F15:F17.
 
 Cell F15 5SUMPRODUCT(B4:C4, $B$14:$C$14) (Copy to 
cell F16)
 
Cell F17 5B14
Right-hand sides 
 The right-hand sides for the three constraints are placed into 
cells G15:G17.
 
Cell G15 5G4 (Copy to cells G16:G17)
To solve the Eastborne Realty problem, we follow these steps:
Step 1. Click the DATA tab in the Ribbon
Step 2. In the Analysis group, click Solver
Step 3. When the Solver Parameters dialog box appears (Figure 9.3):
Enter B17 in the Set Objective: box
Select Max for the To: option
Enter B14:C14 in the By Changing Variable Cells: box
FIGURE 9.3   SOLVER PARAMETERS DIALOG BOX FOR EASTBORNE REALTY

 
9.3 Solving Integer Optimization Problems with Excel Solver 
413
Step 4. Click the Add button
When the Add Constraint dialog box appears:
Enter B14:C14 in the Cell Reference: box 
Select int from the drop-down menu
When int is selected, the term “integer” automatically appears 
in the Constraint: box. This constraint tells Solver that the 
 decision variables in cells B14 and C14 must be integer.
Step 5. Click the Add button
When the Add Constraint dialog box appears:
Enter F15:F17 in the Cell Reference: box
Select # from the drop-down menu
Enter G15:G17 in the Constraint: area
Click OK
Step 6.  Select the Make Unconstrained Variables Non-Negative option
Select Simplex LP from the Select a Solving Method: drop-down 
menu
Step 7. Click the Options button
Select the All Methods tab, and set the Integer Optimality (%): to 0, 
as shown in Figure 9.4. This ensures that we find the optimal integer 
solution.
Click OK to close the Options dialog box
Binary variables are 
identified with the bin 
designation in the Solver 
Parameters dialog box.
FIGURE 9.4   SOLVER OPTIONS DIALOG BOX

414 
Chapter 9 Integer Linear Optimization Models
Step 8. When the Solver Parameters dialog box reappears, click Solve
Step 9. When the Solver Results dialog box appears, select Answer in the Reports 
area and click OK
The completed linear integer optimization model for the Eastborne Realty problem is 
 contained in the file EastborneModel.
Figure 9.5 shows the Eastborne Realty Answer Report. The structure of the Answer 
Report from Excel Solver for integer programs is the same as that described in Chapter 8 
for linear programs. The first section gives information regarding the objective function. It 
shows that the objective function is located in cell B17 and that the optimal (Final Value) of 
the objective function is $70,000. The Variable Cells section gives the location, name, and 
original and optimal values (Final Value) of the decision variables, as well as an indication 
that the decision variables have been designated as integer. For the Eastborne problem, 
in Figure 9.5, we see that the optimal solution is to purchase four townhouses and two 
apartment buildings. Finally the Constraints section gives us detail on the status of each 
constraint at optimality. We see that none of the three constraints is binding and from the 
slack column, that we have $72,000 unused from budget and 44 unused hours, and that we 
are under the limit of 5 townhouses by 1.
As this example illustrates, and as we have seen in Figure 9.1, unlike in a linear pro-
gram, the solution to an integer program can be such that none of the constraints is binding 
at the optimal point.
A Cautionary Note About Sensitivity Analysis
The classical sensitivity analysis discussed in Chapter 8 for linear programs is not avail-
able for integer programs. Because of the discrete nature of integer optimization, it is 
not possible to easily calculate objective function coefficient ranges, shadow prices, and 
file
WEB
EastborneModel
FIGURE 9.5   EXCEL SOLVER ANSWER REPORT FOR THE EASTBORNE  REALTY PROBLEM
A
B
C
D
Objective Cell (Max)
Name
Original Value
Final Value
Final Value
Integer
Constraints
13
14
15
16
17
18
19
20
21
22
23
Cell
$B$14:$C$14=Integer
$B$17
Max Cash Flow ($000)
$0
$70
Cell
Variable Cells
$B$14
Purchase Plan Townhouses
Purchase Plan Apt. Bldgs.
$F$15
$C$14
Name
Original Value
Formula
Status
Slack
Cell
Name
Cell Value
0
4
Integer
0
2
Integer
$F$16
$F$17
Funds ($000) Total Used
Time (Hours) Total Used
Townhouses Total Used
$F$15<=$G$15
Not Binding
Not Binding
Not Binding
$F$16<=$G$16
$1,928
96
4
$F$17<=$G$17
72
44
1
24
25
26
27
28
29
30
31
E
F
G

 
9.4 Applications Involving Binary Variables 
415
right-hand-side ranges. However, this does not mean that the sensitivity analysis is not 
important for integer programs. Sensitivity analysis often is more crucial for integer linear 
programming problems than for linear programming problems. A small change in one of 
the coefficients in the constraints can cause a relatively large change in the value of the 
optimal solution. To understand why, consider the following integer programming model 
of a simple capital budgeting problem involving four projects and a budgetary constraint 
for a single time period:
 
Max 40x1 1 60x2 1 70x3 1 160x4
 
s.t.
 
16x1 1 35x2 1 45x3 1 85x4 # 100
 
x1, x2, x3, x4 5 0, 1
The optimal solution to this problem is x1 5 1, x2 5 1, x3 5 1, and x4 5 0, with an objective 
function value of $170. However, note that if the budget available is increased by $1 (from 
$100 to $101), the optimal solution changes to x1 5 1, x2 5 0, x3 5 0, and x4 5 1, with an 
objective function value of $200. In other words, one additional dollar in the budget would 
lead to a $30 increase in the return. Surely management, when faced with such a situation, 
would increase the budget by $1. Because of the extreme sensitivity of the value of the 
optimal solution to the constraint coefficients, practitioners usually recommend re-solving 
the integer linear program several times with variations in the coefficients before attempting 
to choose the best solution for implementation. 
Sensitivity reports are 
not available for integer 
optimization problems. To 
determine the sensitivity 
of the solution to changes 
in model inputs, you must 
change the data and 
 re-solve the problem.
NOTES AND COMMENTS
The time required to obtain an optimal solution can 
be highly variable for integer linear programs. If an 
optimal solution cannot be found within a reason-
able amount of time, the Integer Optimality (%) 
can be reset to 5 percent or some higher value so 
that the search procedure may stop when a near-
optimal solution (within the tolerance of being 
optimal) has been found. This can shorten the so-
lution time because, if the Integer Optimality (%) 
is set to 5 percent, Solver can stop when it knows 
it is within 5 percent of optimal rather than having 
to complete the search. In general, unless you are 
experiencing excessive run times, we recommend 
you set the Integer Optimality (%) to 0. 
applications Involving Binary Variables
Much of the modeling flexibility provided by integer linear programming is due to the use 
of binary variables. In many applications, binary variables provide selections or choices 
with the value of the variable equal to one if a corresponding activity is undertaken and 
equal to zero if the corresponding activity is not undertaken. The capital budgeting, fixed 
cost, distribution system design, bank location, and product design/market share applica-
tions presented in this section make use of binary variables.
Capital Budgeting
The Ice-Cold Refrigerator Company is considering investing in several projects that have 
varying capital requirements over the next four years. Faced with limited capital each year, 
management would like to select the most profitable projects that it can afford. The esti-
mated net present value for each project, the capital requirements, and the available capital 
over the four-year period are shown in Table 9.1.
9.4
The estimated net present 
value is the net cash flow 
discounted back to the 
beginning of year 1.

416 
Chapter 9 Integer Linear Optimization Models
Let us define four binary decision variables:
 
P 5 1 if the plant expansion project is accepted; 0 if rejected
 
W 5 1 if the warehouse expansion project is accepted; 0 if rejected
 
M 5 1 if the new machinery project is accepted; 0 if rejected
 
R 5 1 if the new product research project is accepted; 0 if rejected
In a capital budgeting problem, the company’s objective function is to maximize the net 
present value of the capital budgeting projects. This problem has four constraints: one for 
the funds available in each of the next four years.
A binary integer linear programming model with dollars in thousands is
 
Max 90P 1 40W 1 10M 1 37R
 
s.t.
 
15P 1 10W 1 10M 1 15R # 40 (Year 1 capital available)
 
20P 1 15W       1 10R # 50 (Year 2 capital available)
 
20P 1 20W       1 10R # 40 (Year 3 capital available)
 
15P 1  5W   1   4M 1 10R # 35 (Year 4 capital available)
 
  P, W, M, R 5 0, 1
The Ice-Cold spreadsheet model and Solver dialog box are shown in Figure 9.6. The SUM-
PRODUCT function is used to calculate the amount of capital used in each year as well as 
the net present value.
The Excel Solver Answer Report is shown in Figure 9.7. The optimal solution is P 5 1, 
W 5 1, M 5 1, R 5 0, with a total estimated net present value of $140,000. Thus, the com-
pany should fund the plant expansion, warehouse expansion, and new machinery projects. 
The new product research project should be put on hold unless additional capital funds 
become available. The values of the slack variables (Figure 9.7) show that the company will 
have $5,000 remaining in year 1, $15,000 remaining in year 2, and $11,000 remaining in 
year 4. Checking the capital requirements for the new product research project, we see that 
enough funds are available for this project in years 2 and 4. However, the company would 
have to find additional capital funds of $10,000 in year 1 and $10,000 in year 3 to fund the 
new product research project.
Fixed Cost
In many applications, the cost of production has two components: a setup cost, which is 
a fixed cost, and a variable cost, which is directly related to the production quantity. The 
use of binary variables makes including the setup cost possible in a model for a production 
application.
TABLE 9.1   PROJECT NET PRESENT VALUE, CAPITAL REQUIREMENTS, AND AVAILABLE CAPITAL 
FOR THE ICE-COLD REFRIGERATOR COMPANY
Project
Plant  
Expansion ($)
Warehouse 
Expansion ($)
New  
Machinery ($)
New Product  
Research ($)
Total  
Capital ($)
Present Value
Year 1 Cap Rqmt
Year 2 Cap Rqmt
Year 3 Cap Rqmt
Year 4 Cap Rqmt
90,000
15,000
20,000
20,000
15,000
40,000
10,000
15,000
20,000
 5,000
10,000
10,000
 4,000
37,000
15,000
10,000
10,000
10,000
Available ($)
40,000
50,000
40,000
35,000

 
9.4 Applications Involving Binary Variables 
417
As an example of a fixed-cost problem, consider the production problem faced by 
RMC Inc. Three raw materials are used to produce three products: a fuel additive, a solvent 
base, and a carpet cleaning fluid. The following decision variables are used:
 
F 5 tons of fuel additive produced
 
S 5 tons of solvent base produced
 
C 5 tons of carpet cleaning fluid produced
The profit contributions are $40 per ton for the fuel additive, $30 per ton for the solvent 
base, and $50 per ton for the carpet cleaning fluid. Each ton of fuel additive is a blend of 
FIGURE 9.6   ICE-COLD SPREADSHEET MODEL AND SOLVER DIALOG BOX
A
B
C
D
F
G
E
1
Ice-Cold Refrigerator
2
Parameters
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
Year 1
Year 2
Year 3
Financial Data ($1000s)
$40
$40
$35
$50
$15
$15
$20
$20
$10
$15
$10
$10
Capital
Available
Plant
Expansion
$5
$10
$15
$20
$40
$90
$37
Warehouse
Expansion
Machinery
New
$4
$10
$10
New Prod.
Research
Net Present Value
Plant
Expansion
Warehouse
Expansion
Machinery
New
New Prod.
Research
Investment Plan
Amount ($1000s)
Year 1 Capital
Year 2 Capital
Year 3 Capital
Year 4 Capital
Model
Net Present Value ($1000s)
$140.00
Spent
Available
$35
$35
27 Year 4
$24
$40
$40
$50
$35
$40
0
1
1
1
file
WEB
IceCold

418 
Chapter 9 Integer Linear Optimization Models
0.4 ton of material 1 and 0.6 ton of material 3. Each ton of solvent base requires 0.5 ton of 
material 1, 0.2 ton of material 2, and 0.3 ton of material 3. Each ton of carpet cleaning fluid 
is a blend of 0.6 ton of material 1, 0.1 ton of material 2, and 0.3 ton of material 3. RMC has 
20 tons of material 1, 5 tons of material 2, and 21 tons of material 3, and management is in-
terested in determining the optimal production quantities for the upcoming planning period.
A linear programming model of the RMC problem is
 
Max 40F 1 30S 1 50C
 
s.t.
 
0.4F 1 0.5S 1 0.6C # 20 Material 1
 
       0.2S 1 0.1C #  5 Material 2
 
0.6F 1 0.3S 1 0.3C # 21 Material 3
 
F, S, C $ 0
Using Excel Solver, we obtain an optimal solution consisting of 27.5 tons of fuel additive, 
0 tons of solvent base, and 15 tons of carpet cleaning fluid, with a value of $1,850.
This linear programming formulation of the RMC problem does not include a fixed cost 
for production setup of the products. Suppose that the following data are available concern-
ing the setup cost and the maximum production quantity for each of the three products:
 
 
 
Maximum  
 
Product 
Setup Cost ($) 
Production (tons)
 
Fuel additive 
200 
50 
 
Solvent base 
50 
25 
 
Carpet cleaning fluid 
400 
40 
FIGURE 9.7   ANSWER REPORT FOR ICE-COLD REFRIGERATOR
A
B
C
D
Objective Cell (Max)
Name
Original Value
Final Value
Final Value
Integer
13
14
15
16
17
18
19
20
21
22
23
Cell
$B$25
$D$16
Net Present Value ($1000s) Expansion
$0.00
$140.00
Cell
Variable Cells
$C$20
Investment Plan Plant Expansion
Investment Plan WH Expansion
Investment Plan Machinery
Investment Plan Research
Constraints
$D$20
$E$20
$F$20
Name
Original Value
Formula
Status
Slack
Name
Cell Value
0
1
Binary
0
1
Binary
0
1
Binary
0
0
Binary
Cell
$B$24
$B$27
$C$20:$F$20=Binary
$B$26
Year 1 Spent
Year 2 Spent
Year 3 Spent
Year 4 Spent
Not Binding
Not Binding
Binding
Not Binding
$35
$B$24<=$C$24
$35
$B$25<=$C$25
$40
$B$26<=$C$26
$24
$B$27<=$C$27
5
15
0
11
24
25
26
27
28
29
32
33
34
30
31
E
F
G

 
9.4 Applications Involving Binary Variables 
419
The modeling flexibility provided by binary variables can now be used to incorpo-
rate the fixed setup costs into the production model. The binary variables are defined as 
follows:
 
SF 5 1 if the fuel additive is produced; 0 if not
 
SS 5 1 if the solvent base is produced; 0 if not
 
SC 5 1 if the carpet cleaning fluid is produced; 0 if not
Using these setup variables, the total setup cost is
200SF 1 50SS 1 400SC
We can now rewrite the objective function to include the setup cost. Thus, the net profit 
objective function becomes
Max 40F 1 30S 1 50C 2 200SF 2 50SS 2 400SC
Next, we must write production capacity constraints so that, if a setup variable equals 0, 
production of the corresponding product is not permitted, and, if a setup variable equals 1, 
production is permitted up to the maximum quantity. For the fuel additive, we do so by add-
ing the following constraint:
F # 50SF
Note that, with this constraint present, production of the fuel additive is not permitted when 
SF 5 0. When SF 5 1, production of up to 50 tons of fuel additive is permitted. We can 
think of the setup variable as a switch. When it is off (SF 5 0), production is not permitted; 
when it is on (SF 5 1), production is permitted.
Similar production capacity constraints, using binary variables, are added for the sol-
vent base and carpet cleaning products:
 
S # 25SS
 
C # 40SC
In summary, we have the following fixed-cost model for the RMC problem with setups:
 
Max 40F 1 30S 1 50C 2 200SF 2 50SS 2 400SC
 
s.t.
 
0.4F 1 0.5S 1 0.6C # 20    Material 1
 
0.2S 1 0.1C # 5       Material 2
 
0.6F 1 0.3S 1 0.3C # 21    Material 3
 
F               # 50SF  Maximum Fuel Additive
 
S         # 25SS  Maximum Solvent Base
 
C # 40SC  Maximum Carpet Cleaning
 
F, S, C $ 0; SF, SS, SC 5 0 or 1
A spreadsheet model and Solver dialog box for the RMC problem are shown in Fig-
ure 9.8. The SUMPRODUCT function is used to calculate the material used, and cells 
D31, D32, and D33 contain the capacity multiplied by the appropriate binary variable 
(5B11*B22 in cell D31, 5C11*C22 in cell D32 and 5D11*D22 in cell D33).
The Excel Answer Report is shown in Figure 9.9. The optimal solution requires 25 tons 
of fuel additive and 20 tons of solvent base. The value of the objective function after de-
ducting the setup cost is $1,350. The setup cost for the fuel additive and the solvent base is 
$200 1 $50 5 $250. The optimal solution includes SC 5 0, which indicates that the more 
expensive $400 setup cost for the carpet cleaning fluid should be avoided. Thus, the carpet 
cleaning fluid is not produced.
The key to developing a fixed-cost model is the introduction of a binary variable for 
each fixed cost and the specification of an upper bound for the corresponding production 

420 
Chapter 9 Integer Linear Optimization Models
variable. For a production quantity x, a constraint of the form x # My can then be used to 
allow production when the setup variable y 5 1 and not to allow production when the setup 
variable y 5 0. The value of the maximum production quantity M should be large enough 
to allow for all reasonable levels of production, but choosing values of M excessively large 
will slow the solution procedure.
Bank Location
The long-range planning department for the Ohio Trust Company is considering expand-
ing its operation into a 20-county region in northeastern Ohio (Figure 9.10). Currently, 
Ohio Trust does not have a principal place of business in any of the 20 counties. Ac-
cording to the banking laws in Ohio, if a bank establishes a principal place of business 
FIGURE 9.8   RMC WITH SETUPS SPREADSHEET MODEL AND SOLVER DIALOG BOX
A
B
C
D
E
1 RMC
2 Parameters
3
4
5
6
7
8
9
10
11
13
12
14
15
16
17
18
19
20
21
28
29
30
31
32
33
22
23
24
25
26
Material Requirements (tons)
Fuel
Additive
$1,350.00
$50
25
0.2
0.3
$30
0.5
$400
40
0.1
0.3
$50
0.6
5
21
20
50
$200
0.6
$40
0.4
Solvent
Base
Fluid
Cleaning
Tons
Available
Materials
Tons Produced
Setup
Material 1
Material 2
Material 3
Proﬁt per Ton
Setup Cost
Capacity (Tons)
Model
Max Net Proﬁt
Tons Produced
Max Tons
Used
27
Available
Material 1
Cleaning
Solvent
Fuel
Material 2
Material 3
20
5
21
20
4
21
Max F
Max S
Max C
50
25
0
25
20
0.0
0.0
0
20.0
1
25.0
1
file
WEB
RMCSetup

 
9.4 Applications Involving Binary Variables 
421
FIGURE 9.9   ANSWER REPORT FOR RMC PRODUCTION PROBLEM
A
B
C
D
Objective Cell (Max)
Name
Original Value
Final Value
Final Value
Integer
13
14
15
16
17
18
19
20
21
22
23
Cell
$C$27
$C$17
Max Net Proﬁt
$0.00
$1,350.00
Cell
Variable Cells
$B$21
Constraints
$C$21
$D$21
$B$22
Name
Original Value
Formula
Status
Slack
Name
Cell Value
Contin
Contin
Contin
Binary
Tons Produced Fuel
Tons Produced Solvent
Tons Produced Cleaning
Setup Fuel
Setup Solvent
Setup Cleaning
$C$22
$D$22
Binary
0.0
0.0
0.0
0
0
0
25.0
20.0
0.0
1
1
0
Binary
Cell
$C$26
$C$31
$C$32
$C$28
$C$26<=$D$26
$C$27<=$D$27
$C$28<=$D$28
$C$31<=$D$31
$C$33
$B$22:$D$22=Binary
$C$33<=$D$33
Material 1 Used
Material 2 Used
Material 3 Used
Max F Tons Produced
Max C Tons Produced
Max S Tons Produced
Binding
Not Binding
Binding
Not Binding
Binding
Not Binding
$C$32<=$D$32
20
4
21
25
0.0
20
0
1
0
25
0
5
24
27
28
25
26
29
30
31
34
35
37
36
32
33
E
F
G
38
(PPB) in any county, branch banks can be established in that county and in any of the 
adjacent counties.  However, to establish a new principal place of business, Ohio Trust 
must either obtain approval for a new bank from the state’s superintendent of banks or 
purchase an existing bank.
Table 9.2 lists the 20 counties in the region and adjacent counties. For example, 
Ashtabula County is adjacent to Lake, Geauga, and Trumbull counties; Lake County is 
adjacent to Ashtabula, Cuyahoga, and Geauga counties; and so on.
As an initial step in its planning, Ohio Trust would like to determine the minimum 
number of PPBs necessary to do business throughout the 20-county region. A binary integer 
programming model can be used to solve this location problem for Ohio Trust. We define 
the variables as
xi 5 1 if a PBB is established in county i; 0 otherwise
To minimize the number of PPBs needed, we write the objective function as
Min x1 1 x2 1 . . . 1 x20
The bank may locate branches in a county if the county contains a PPB or is adjacent to 
another county with a PPB. Thus, the linear program will need one constraint for each 
county. For example, the constraint for Ashtabula County is
x1 1 x2 1 x12 1 x16 $ 1 Ashtabula

422 
Chapter 9 Integer Linear Optimization Models
Note that satisfaction of this constraint ensures that a PPB will be placed in Ashtabula 
County or in one or more of the adjacent counties. This constraint thus guarantees that Ohio 
Trust will be able to place branch banks in Ashtabula County.
The complete statement of the bank location problem is
Min
x1 1 x2 1
. . .
1 x20
s.t.
x1 1 x2
1 x12 1 x16
$ 1 Ashtabula
x1 1 x2 1 x3 1 x12
$ 1 Lake
x11 1 x14 1 x19 1 x20 $ 1 Carroll
xi 5 0,1 i 5 1, 2 , . . . , 20
We use Excel Solver to solve this 20-variable, 20-constraint problem formulation. In 
Figure 9.11, we show the optimal solution. The optimal solution calls for principal 
places of business in Ashland, Stark, and Geauga counties. With PPBs in these three 
counties, Ohio Trust can place branch banks in all 20 counties. Clearly the integer 
programming model could be enlarged to allow for expansion into a larger area or 
throughout the entire state.
In Problem 10, we ask you 
to solve this problem for the 
entire state of Ohio.
FIGURE 9.10   OHIO TRUST COUNTY MAP 
1
2
12
16
15
14
20
19
11
13
3
10
18
17
6
5
4
9
8
7
Lake 
Erie
Ohio
Pennsylvania
West 
Virginia
Counties
1. 
2. 
3. 
4. 
5.
Ashtabula 
Lake 
Cuyahoga 
Lorain 
Huron
6. 
7. 
8. 
9. 
10.
Richland 
Ashland 
Wayne 
Medina 
Summit
11. 
12. 
13. 
14. 
15.
Stark 
Geauga 
Portage 
Columbiana
Mahoning
16. 
17. 
18. 
19. 
20.
Trumbull 
Knox 
Holmes 
Tuscarawas
Carroll


424 
Chapter 9 Integer Linear Optimization Models
Product Design and Market Share Optimization
Conjoint analysis is a market research technique that can be used to learn how prospec-
tive buyers of a product value the product’s attributes. In this section, we will show how 
the results of conjoint analysis can be used in an integer programming model of a product 
design and market share optimization problem. We illustrate the approach by consider-
ing a problem facing Salem Foods, a major producer of frozen foods.
Salem Foods is planning to enter the frozen pizza market. Currently, two existing 
brands, Antonio’s and King’s, have the major share of the market. In trying to develop 
a sausage pizza that will capture a substantial share of the market, Salem determined 
that the four most important attributes when consumers purchase a frozen sausage pizza 
are crust, cheese, sauce, and sausage flavor. The crust attribute has two levels (thin and 
thick); the cheese attribute has two levels (mozzarella and blend); the sauce attribute has 
two levels (smooth and chunky); and the sausage flavor attribute has three levels (mild, 
medium, and hot).
In a typical conjoint analysis, a sample of consumers is asked to express their prefer-
ence for a product with chosen levels for the attributes. Then regression analysis is used to 
determine the part-worth for each of the attribute levels. In essence, the part-worth is the 
utility value that a consumer attaches to each level of each attribute. Provided part-worths 
from regression analysis, we will show how they can be used to determine the overall value 
a consumer attaches to a particular product.
Table 9.3 shows the part-worths for each level of each attribute provided by a 
sample of eight potential Salem customers who are currently buying either King’s or 
Antonio’s pizza. For consumer 1, the part-worths for the crust attribute are 11 for thin 
crust and 2 for thick crust, indicating a preference for thin crust. For the cheese attri-
bute, the part-worths are 6 for the mozzarella cheese and 7 for the cheese blend; thus, 
consumer 1 has a slight preference for the cheese blend. From the other part-worths, we 
see that consumer 1 shows a strong preference for the chunky sauce over the smooth 
sauce (17 to 3) and has a slight preference for the medium-flavored sausage. Note that 
consumer 2 shows a preference for the thin crust, the cheese blend, the chunky sauce, 
and mild-flavored sausage. The part-worths for the others consumers are interpreted 
similarly.
The part-worths can be used to determine the overall value (utility) that each consumer 
attaches to a particular type of pizza. For instance, consumer 1’s current favorite pizza 
is the Antonio’s brand, which has a thick crust, mozzarella cheese, chunky sauce, and 
medium-flavored sausage. We can determine consumer 1’s utility for this particular type 
TABLE 9.3  PART-WORTHS FOR THE SALEM FOODS PROBLEM
Crust
Cheese
Sauce
Sausage Flavor
Consumer
Thin
Thick
Mozzarella
Blend
Smooth
Chunky
Mild
Medium
Hot
1
2
3
4
5
6
7
8
11
11
 7
13
 2
12
 9
 5
 2
 7
 5
20
 8
17
19
 9
 6
15
 8
20
 6
11
12
 4
 7
17
14
17
11
 9
16
14
 3
16
16
17
30
 2
16
23
17
26
 7
14
20
30
25
16
26
14
29
25
15
22
30
16
27
 1
16
29
 5
12
23
30
 8
10
19
10
12
20
19
 3

 
9.4 Applications Involving Binary Variables 
425
of pizza using the part-worths in Table 9.3. For consumer 1, the part-worths are 2 for thick 
crust, 6 for mozzarella cheese, 17 for chunky sauce, and 27 for medium-flavored sausage. 
Thus, consumer 1’s utility for the Antonio’s brand pizza is 2 1 6 1 17 1 27 5 52. We 
can compute consumer 1’s utility for a King’s brand pizza similarly. The King’s brand 
pizza has a thin crust, a cheese blend, smooth sauce, and mild-flavored sausage. Because 
the part-worths for consumer 1 are 11 for thin crust, 7 for cheese blend, 3 for smooth 
sauce, and 26 for mild-flavored sausage, consumer 1’s utility for the King’s brand pizza is  
11 1 7 1 3 1 26 5 47. In general, each consumer’s utility for a particular type of pizza 
is the sum of the part-worths for the attributes of that type of pizza.
To be successful with its brand, Salem Foods realizes that it must entice consumers in 
the marketplace to switch from their current favorite brand of pizza to the Salem product. 
In other words, Salem must design a pizza (choose the type of crust, cheese, sauce, and 
sausage flavor) that will have the highest utility for a sufficient number of people to ensure 
sufficient sales to justify making the product. Assuming the sample of eight consumers 
in the current study is representative of the marketplace for frozen sausage pizza, we can 
formulate and solve an integer programming model that can help Salem come up with such 
a design. In marketing literature, the problem being solved is called the share of choice 
problem.
The decision variables are defined as follows:
 
lij 5 1 if Salem chooses level i for attribute j; 0 otherwise
 
yk 5 1 if consumer k chooses the Salem brand; 0 otherwise
The objective is to choose the levels of each attribute that will maximize the number of 
consumers preferring the Salem brand pizza. Because the number of consumers preferring 
the Salem brand pizza is just the sum of the yk variables, the objective function is
Max y1 1 y2 1 . . . 1 y8
One constraint is needed for each consumer in the sample. To illustrate how the con-
straints are formulated, let us consider the constraint corresponding to consumer 1. For 
consumer 1, the utility of a particular type of pizza can be expressed as the sum of the 
part-worths:
Utility for consumer 1 5 11l11 1 2l21 1 6l12 1 7l22 1 3l13 1 17l23 1 26l14 1 27l24 1 8l34
For consumer 1 to prefer the Salem pizza, the utility for the Salem pizza must be greater 
than the utility for consumer 1’s current favorite. Recall that consumer 1’s current favorite 
brand of pizza is Antonio’s, with a utility of 52. Thus, consumer 1 will purchase the Salem 
brand only if the levels of the attributes for the Salem brand are chosen such that
11l11 1 2l21 1 6l12 1 7l22 1 3l13 1 17l23 1 26l14 1 27l24 1 8l34 . 52
Given the definitions of the yk decision variables, we want y1 5 1 when the consumer pre-
fers the Salem brand and y1 5 0 when the consumer does not prefer the Salem brand. Thus, 
we write the constraint for consumer 1 as follows:
 
11l11 1 2l21 1 6l12 1 7l22 1 3l13 1 17l23 1 26l14 1 27l24 1 8l34 $ 1 1 52y1
With this constraint, y1 cannot equal 1 unless the utility for the Salem design (the left-hand 
side of the constraint) exceeds the utility for consumer 1’s current favorite by at least 1. 
Because the objective function is to maximize the sum of the yk variables, the optimization 
will seek a product design that will allow as many yk variables as possible to equal 1.
Utility values are discussed 
in more detail in Chapter 12.

426 
Chapter 9 Integer Linear Optimization Models
A similar constraint is written for each consumer in the sample. The coefficients for 
the lij variables in the utility functions are taken from Table 9.3, and the coefficients for 
the yk variables are obtained by computing the overall utility of the consumer’s current 
favorite brand of pizza. The following constraints correspond to the eight consumers in 
the study:
11l11 1
2l21 1
6l12 1
7l22 1
3l13 1 17l23 1 26l14 1 27l24 1
8l34 $ 1 1 52y1
11l11 1
7l21 1 15l12 1 17l22 1 16l13 1 26l23 1 14l14 1
1l24 1 10l34 $ 1 1 58y2
7l11 1
521 1
8l12 1 14l22 1 16l13 1
7l23 1 29l14 1 16l24 1 19l34 $ 1 1 66y3
13l11 1 20l21 1 20l12 1 17l22 1 17l13 1 14l23 1 25l14 1 29l24 1 10l34 $ 1 1 83y4
2l11 1
8l21 1
6l12 1 11l22 1 30l13 1 20l23 1 15l14 1
5l24 1 12l34 $ 1 1 58y5
12l11 1 17l21 1 11l12 1
9l22 1
2l13 1 30l23 1 22l14 1 12l24 1 20l34 $ 1 1 70y6
9l11 1 19l21 1 12l12 1 16l22 1 16l13 1 25l23 1 30l14 1 23l24 1 19l34 $ 1 1 79y7
5l11 1
9l21 1
4l12 1 14l22 1 23l13 1 16l23 1 16l14 1 30l24 1
3l34 $ 1 1 59y8
Four more constraints must be added, one for each attribute. These constraints are necessary 
to ensure that one and only one level is selected for each attribute. For attribute 1 (crust), 
we must add the constraint
 
l11 1 l21 5 1
Because l11 and l21 are both binary variables, this constraint requires that one of the two 
variables equals one, and the other equals zero. The following three constraints ensure that 
one and only one level is selected for each of the other three attributes:
 
l12 1 l22 5 1
 
l13 1 l23 5 1
 
l14 1 l24 1 l34 5 1
The data, model and solution for the Salem pizza problem may be found in the file Salem. 
The optimal solution to this 17-variable, 12-constraint integer linear program is l11 5 
l22 5 l23 5 l14 5 1 and y2 5 y5 5 y6 5 y7 5 1. The value of the optimal solution is 4, 
indicating that if Salem makes this type of pizza, it will be preferable to the current 
favorite for four of the eight consumers. With l21 5 l22 5 l23 5 l14 5 1, the pizza design 
that obtains the largest market share for Salem has a thin crust, a cheese blend, a chunky 
sauce, and mild-flavored sausage. Note also that with y2 5 y5 5 y6 5 y7 5 1, consumers 
2, 5, 6, and 7 will prefer the Salem pizza. This information may lead Salem to choose 
to market this type of pizza.
Modeling Flexibility provided 
by Binary Variables
In Section 9.4, we presented four applications involving binary integer variables. In this 
section, we continue the discussion of the use of binary integer variables in modeling. 
First, we show how binary integer variables can be used to model multiple-choice and 
mutually exclusive constraints. Then we show how binary integer variables can be used 
to model situations in which k projects out of a set of n projects must be selected, as well 
as situations in which the acceptance of one project is conditional on the acceptance of 
another project. 
Antonio’s brand is the 
current favorite pizza for 
consumers 1, 4, 6, 7, and 8. 
King’s brand is the current 
favorite pizza for consumers 
2, 3, and 5.
file
WEB
Salem
9.5

 
9.5 Modeling Flexibility Provided by Binary Variables 
427
Multiple-Choice and Mutually Exclusive Constraints
Recall the Ice-Cold Refrigerator capital budgeting problem introduced in Section 9.4. The 
decision variables were defined as
 
P 5 1 if the plant expansion project is accepted; 0 if rejected
 
W 5 1 if the warehouse expansion project is accepted; 0 if rejected
 
M 5 1 if the new machinery project is accepted; 0 if rejected
 
R 5 1 if the new product research project is accepted; 0 if rejected
Suppose that, instead of one warehouse expansion project, the Ice-Cold Refrigerator Com-
pany actually has three warehouse expansion projects under consideration. One of the ware-
houses must be expanded because of increasing product demand, but new demand is not 
sufficient to make expansion of more than one warehouse necessary. The following vari-
able definitions and multiple-choice constraint could be incorporated into the previous 
binary integer linear programming model to reflect this situation. Let
 
W1 5 1 if the original warehouse expansion project is accepted; 0 if rejected
 
W2 5 1 if the second warehouse expansion project is accepted; 0 if rejected
 
W3 5 1 if the third warehouse expansion project is accepted; 0 if rejected
The multiple-choice constraint reflecting the requirement that exactly one of these projects 
must be selected is
 
W1 1 W2 1 W3 5 1
If W1, W2, and W3 are allowed to assume only the values 0 or 1, then one and only one of 
these projects will be selected from among the three choices.
If the requirement that one warehouse must be expanded did not exist, the multiple-
choice constraint could be modified as follows:
 
W1 1 W2 1 W3 # 1
This modification allows for the case of no warehouse expansion (W1 5 W2 5 W3 5 0) but 
does not permit more than one warehouse to be expanded. This type of constraint is often 
called a mutually exclusive constraint.
k out of n Alternatives Constraint
An extension of the notion of a multiple-choice constraint can be used to model situations 
in which k out of a set of n projects must be selected—a k out of n alternatives constraint. 
Suppose that W1, W2, W3, W4, and W5 represent five potential warehouse expansion projects 
and that two of the five projects must be accepted. The constraint that satisfies this new 
requirement is
 
W1 1 W2 1 W3 1 W4 1 W5 5 2
If no more than two of the projects are to be selected, we would use the following less-than-
or-equal-to constraint:
 
W1 1 W2 1 W3 1 W4 1 W5 # 2
Again, each of these variables must be restricted to binary values.
Conditional and Corequisite Constraints
Sometimes the acceptance of one project is conditional on the acceptance of another. For 
example, suppose that for the Ice-Cold Refrigerator Company, the warehouse expansion 
project was conditional on the plant expansion project. In other words, management will 

428 
Chapter 9 Integer Linear Optimization Models
not consider expanding the warehouse unless the plant is expanded. With P representing 
plant expansion and W representing warehouse expansion, a conditional constraint could 
be introduced to enforce this requirement:
 
W # P
P and W must each be 0 or 1; when P is 0, W will be forced to 0. When P is 1, W is allowed 
to be 1; thus, both the plant and the warehouse can be expanded. However, we note that the 
preceding constraint does not force the warehouse expansion project (W) to be accepted if 
the plant expansion project (P) is accepted.
If the warehouse expansion project had to be accepted whenever the plant expansion 
project was, and vice versa, we would say that P and W represented corequisite constraint 
projects. To model such a situation, we simply write the preceding constraint as an equality:
 
W 5 P
The constraint forces P and W to take on the same value.
Generating alternatives in 
Binary Optimization
If alternative optimal solutions exist, it would be good for management to know this be-
cause some factors that make one alternative preferred over another might not be included 
in the model. Also, if the solution is a unique optimal solution, it would be good to know 
how much worse the second-best solution is than the unique optimal solution. If the second-
best solution is very close to optimal, it might be preferred over the true optimal solution 
because of factors outside the model.
As an example, let us reconsider the Ohio Trust location problem presented in 
Section 9.4. The solution for the minimum number of principle places of business (PPBs) is 
three. As shown in Figure 9.11, the solution is to place PBBs in county 7 (Ashland), county 
11 (Stark), and county 12 (Geauga). However, suppose when Ohio Trust tries to implement 
this solution, it is not possible to find a suitable location for a PPB in one of these three 
counties. Are there other alternative solutions of three counties, or is this a unique optimal 
solution? By adding a special constraint based on the current solution and then resolving 
the model, we may answer this question.
The current solution for Ohio Trust can be broken into two sets of variables; those 
that are set to one and those that are set to zero. Let the set O denote the set of variables 
set to one and the set Z those that are set to zero. For the Ohio Trust solution, these sets 
are as follows: 
Set O: x7, x11, x12
Set Z: x1, x2, x3, x4, x5, x6, x8, x9, x10, x13, x14, x15, x16, x17, x18, x19, x20
We may add the following constraint:
(Sum of variables in the set O) 2 (sum of variables in the set Z) # (number of 
variables in the set O) 2 1
which for our current solution is:
x7 1 x11 1 x12 2 x1 2 x2 2 x3 2 x4 2 x5 2 x6 2 x8 2 x9 2 x10 2 x13 2  
x14 2 x15 2 x16 2 x17 2 x18 2 x19 2 x20 # 3 2 1 5 2
This constraint has the very special property that it makes the current solution infea-
sible, but keeps feasible all other solutions that are feasible to the original problem. This 
9.6

 
9.6 Generating Alternatives in Binary Optimization 
429
constraint will force (at least) one of the variables in set O to change from one to zero or 
will force (at least) one of the variables in set Z to change from zero to one. 
When we append this new constraint to the original model, we obtain the solution 
displayed in Figure 9.12. Notice that the optimal objective function value has increased to 
four. This tells us that the solution we found in Section 9.4 with objective function value 
equal to 3 is a unique optimal solution. Any other feasible solution will require four or 
more PBBs to cover the entire 20-county region. So, if for any of the three counties in the 
original solution, we cannot find a suitable location for a PBB, the next best solution will 
require PBBs in four counties and the solution in Figure 9.10 is a second-best alternative. 
Note that if the optimal objective functions of the new problem with constraint added had 
been 3, we would have found an alternative optimal solution.
We can summarize the procedure for finding an alternative solution as follows:
Step 1: Solve the original problem
Step 2: Create two sets: 
O 5 the set of variables equal to one in Step 1 
Z 5 the set of variables equal to zero in Step 1
Step 3: Add the following constraint to the original problem, and solve
 
(Sum of variables in the set O) 2 (sum of variables in the set Z)  
 
 # (number of variables in the set O) 2 1 
(9.1)
FIGURE 9.12   A SECOND-BEST SOLUTION TO THE OHIO TRUST LOCATION 
PROBLEM
Lake 
Erie
Ohio
Pennsylvania
West 
Virginia
Counties
1. 
2. 
3. 
4. 
5.
Ashtabula 
Lake 
Cuyahoga 
Lorain 
Huron
6. 
7. 
8. 
9. 
10.
Richland 
Ashland 
Wayne 
Medina 
Summit
11. 
12. 
13. 
14. 
15.
Stark
Geauga 
Portage 
Columbiana
Mahoning
16. 
17. 
18. 
19. 
20.
Trumbull 
Knox 
Holmes 
Tuscarawas
Carroll
A principal place
of business
should be located
in these counties.
★
1
2
12
16
15
14
20
19
11
13
3
10
18
17
6
8
5
4
9
7
★
★
★
★

430 
Chapter 9 Integer Linear Optimization Models
If the objective function value in step 3 is equal to the objective function value of step 1, 
we have found an alternative optimal solution. If the objective function value of step 3 is 
inferior to that of step 1, we have found a next-best solution.
NOTES AND COMMENTS
1. The procedure just described can be applied 
iteratively. In other words, we can take the 
second-best solution found and create the 
equation (9.1) based on that solution to find 
the next-best solution. Note that we leave all 
previous constraints in the problem, includ-
ing the first constraint based on (9.1). The 
resulting solution could be a third-best solu-
tion or an alternative second-best solution. It 
turns out that there are numerous second-best 
solutions to the Ohio Trust problem using 
four PPBs. 
2. Applying equation (9.1) iteratively and finding 
that the objective function value does not deteri-
orate generates an alternative optimal solution. 
In fact applying equation (9.1) iteratively until 
the objective function changes ensures you have 
found all alternative optima. 
Summary
In this chapter we introduced the important extension of linear programming referred to as 
integer linear programming. The only difference between the integer linear programming 
problems discussed in this chapter and the linear programming problems studied in the 
previous chapter is that one or more of the variables must be integer. If all variables must 
be integer, we have an all-integer linear program. If some, but not necessarily all, vari-
ables must be integer, we have a mixed-integer linear program. Most integer programming 
 applications involve binary variables.
Studying integer linear programming is important for two major reasons. First, integer 
linear programming may be helpful when fractional values for the variables are not permit-
ted. Rounding a linear programming solution may not provide an optimal integer solution; 
methods for finding optimal integer solutions are needed when the economic consequences 
of rounding are substantial. A second reason for studying integer linear programming is the 
increased modeling flexibility provided through the use of binary variables. We showed how 
binary variables could be used to model important managerial considerations in capital bud-
geting, fixed cost, facility location, and product design/market share applications. We showed 
how to generate second-best solutions or alternative optima if they exist by adding a constraint 
based on those solutions. This is important for providing alternatives for management.
The number of applications of integer linear programming continues to grow rapidly, 
partly because of the availability of good integer linear programming software packages. As 
researchers develop solution procedures capable of solving larger integer linear programs 
and as computer speed increases, a continuation of the growth of integer programming 
 applications is expected.
Glossary
Integer linear program A linear program with the additional requirement that one or more 
of the variables must be integer.
All-integer linear program An integer linear program in which all variables are required 
to be integer.
LP Relaxation The linear program that results from dropping the integer requirements for 
the variables in an integer linear program.

 
Problems 
431
Mixed-integer linear program An integer linear program in which some, but not neces-
sarily all, variables are required to be integer.
Binary integer linear program An all-integer or mixed-integer linear program in which 
the integer variables are permitted to assume only the values 0 or 1. Also called binary 
integer program.
Convex hull The convex hull of a set of points is the smallest intersection of linear inequali-
ties that contain the set of points.
Capital budgeting problem A binary integer programming problem that involves choos-
ing which possible projects or activities provide the best investment return.
Fixed-cost problem A binary mixed-integer programming problem in which the binary 
variables represent whether an activity, such as a production run, is undertaken (variable 
5 1) or not (variable 5 0).
Location problem A binary integer programming problem in which the objective is to 
select the best locations to meet a stated objective. Variations of this problem (see the bank 
location problem in Section 9.4) are known as covering problems.
Conjoint analysis A market research technique that can be used to learn how prospective 
buyers of a product value the product’s attributes.
Product design and market share optimization problem Sometimes called the share of 
choice problem, the choice of a product design that maximizes the number of consumers 
preferring it.
Part-worth The utility value that a consumer attaches to each level of each attribute in a 
conjoint analysis model.
Multiple-choice constraint A constraint requiring that the sum of two or more binary 
variables equals one. Thus, any feasible solution makes a choice of which variable to set 
equal to one.
Mutually exclusive constraint A constraint requiring that the sum of two or more binary 
variables be less than or equal to one. Thus, if one of the variables equals one, the others 
must equal zero. However, all variables could equal zero.
k out of n alternatives constraint An extension of the multiple-choice constraint. This 
constraint requires that the sum of n binary variables equals k.
Conditional constraint A constraint involving binary variables that does not allow certain 
variables to equal one unless certain other variables are equal to one.
Corequisite constraint A constraint requiring that two binary variables be equal and that 
thus are both either in or out of the solution together.
Problems
 1.  STAR Co. provides paper to smaller companies with volumes that are not large enough 
to warrant dealing directly with the paper mill. STAR receives 100-feet-wide paper rolls 
from the mill and cuts the rolls into smaller rolls of widths 12, 15, and 30 feet. The de-
mands for these widths vary from week to week. The following cutting patterns have been 
established:
Pattern 
Number
12-ft
15-ft
30-ft
Trim Loss (ft)
1
2
3
4
5
0
0
8
2
7
6
0
0
1
1
0
3
0
2
0
10
10
 4
 1
 1 

432 
Chapter 9 Integer Linear Optimization Models
 
 Trim loss is the leftover paper from a pattern (for example, for pattern 4, 2(12) 1 1(15) 1  
2(30) 5 99 feet used results in 100 – 99 5 1 foot of trim loss). Demands this week are 
5670 12-foot rolls, 1680 15-foot rolls, and 3350 30-foot rolls. Develop an all-integer 
model that will determine how many 100-foot rolls to cut into each of the five patterns 
in order to meet demand and minimize trim loss (leftover paper from a pattern).
 2.  The following questions refer to a capital budgeting problem with six projects represented 
by binary variables x1, x2, x3, x4, x5, and x6.
a.  Write a constraint modeling a situation in which two of the projects 1, 3, 5, and 6 must 
be undertaken.
b.  Write a constraint modeling a situation in which, if project 3 or 5 is undertaken, they 
must both be undertaken.
c.  Write a constraint modeling a situation in which project 1 or 4 must be undertaken, 
but not both.
d.  Write constraints modeling a situation where project 4 cannot be undertaken unless 
projects 1 and 3 also are undertaken.
e.  Revise the requirement in part d to accommodate the case in which, when projects 1 
and 3 are undertaken, project 4 also must be undertaken.
 3.  Spencer Enterprises is attempting to choose among a series of new investment alterna-
tives. The potential investment alternatives, the net present value of the future stream of 
returns, the capital requirements, and the available capital funds over the next three years 
are summarized as follows:
Alternative
Net Present 
Value ($)
Capital Requirements ($)
Year 1
Year 2
Year 3
Limited warehouse expansion
Extensive warehouse expansion
Test market new product
Advertising campaign
Basic research
Purchase new equipment
Capital funds available
 4,000
 6,000
10,500
 4,000
 8,000
 3,000
 3,000
 2,500
 6,000
 2,000
 5,000
 1,000
10,500
1,000
3,500
4,000
1,500
1,000
 500
7,000
4,000
3,500
5,000
1,800
4,000
 900
8,750
a.  Develop and solve an integer programming model for maximizing the net present 
value.
b.  Assume that only one of the warehouse expansion projects can be implemented. 
 Modify your model from part a.
c.  Suppose that if test marketing of the new product is carried out, the advertising cam-
paign also must be conducted. Modify your formulation from part b to reflect this new 
situation.
 4.  Hawkins Manufacturing Company produces connecting rods for 4- and 6-cylinder auto-
mobile engines using the same production line. The cost required to set up the production 
line to produce the 4-cylinder connecting rods is $2,000, and the cost required to set up the 
production line for the 6-cylinder connecting rods is $3,500. Manufacturing costs are $15 
for each 4-cylinder connecting rod and $18 for each 6-cylinder connecting rod. There is no 
production on weekends, so on Friday the line is diassembled and cleaned. On Monday, 
the line must be set up to run whichever product will be produced that week. Once the line 
has been set up, the weekly production capacities are 6000 6-cylinder connecting rods and 
8000 4-cylinder connecting rods. Let
 
x4 5 the number of 4-cylinder connecting rods produced next week
 
x6 5 the number of 6-cylinder connecting rods produced next week
 
s4 5  1 if the production line is set up to produce the 4-cylinder connecting rods; 0 if  otherwise
 
s6 5  1 if the production line is set up to produce the 6-cylinder connecting rods; 0 if 
 otherwise

 
Problems 
433
a.  Using the decision variables x4 and s4, write a constraint that sets next week’s  maximum 
production of the 4-cylinder connecting rods to either 0 or 8000 units.
b.  Using the decision variables x6 and s6, write a constraint that sets next week’s  maximum 
production of the 6-cylinder connecting rods to either 0 or 6000 units.
c.  Write a constraint that requires that production be set up for exactly one of the two 
rods.
d. Write the cost function to be minimized.
 5.  Grave City is considering the relocation of several police substations to obtain better en-
forcement in high-crime areas. The locations under consideration together with the areas 
that can be covered from these locations are given in the following table:
Potential Locations  
for Substations
Areas Covered
A
B
C
D
E
F
G
1, 5, 7
1, 2, 5, 7
1, 3, 5
2, 4, 5
3, 4, 6
4, 5, 6
1, 5, 6, 7
a.  Formulate an integer programming model that could be used to find the minimum 
number of locations necessary to provide coverage to all areas.
b. Solve the problem in part a.
 6.  Hart Manufacturing makes three products. Each product requires manufacturing opera-
tions in three departments: A, B, and C. The labor-hour requirements, by department, are 
as follows:
Department
Product 1
Product 2
Product 3
A
B
C
1.50
2.00
0.25
3.00
1.00
0.25
2.00
2.50
0.25
 
 During the next production period the labor-hours available are 450 in department A, 350 
in department B, and 50 in department C. The profit contributions per unit are $25 for 
product 1, $28 for product 2, and $30 for product 3.
a. Formulate a linear programming model for maximizing total profit contribution.
b.  Solve the linear program formulated in part a. How much of each product should be 
produced, and what is the projected total profit contribution?
c.  After evaluating the solution obtained in part b, one of the production supervisors 
noted that production setup costs had not been taken into account. She noted that setup 
costs are $400 for product 1, $550 for product 2, and $600 for product 3. If the solution 
developed in part b is to be used, what is the total profit contribution after taking into 
account the setup costs?
d.  Management realized that the optimal product mix, taking setup costs into account, 
might be different from the one recommended in part b. Formulate a mixed-integer 
linear program that takes setup costs provided in part c into account. Management also 
stated that we should not consider making more than 175 units of product 1, 150 units 
of product 2, or 140 units of product 3.
e.  Solve the mixed-integer linear program formulated in part d. How much of each prod-
uct should be produced and what is the projected total profit contribution? Compare 
this profit contribution to that obtained in part c.

434 
Chapter 9 Integer Linear Optimization Models
 7.  Offhaus Manufacturing produces office supplies but outsources the delivery of its products 
to third-party carriers. Offhaus ships to 20 cities from its Dayton, Ohio, manufacturing facil-
ity and has asked a variety of carriers to bid on its business. Seven carriers have responded 
with bids. The resulting bids (in dollars per truckload) are shown in the table. For example, 
the table shows that carrier 1 bid on the business to cities 11–20. The right side of the table 
provides the number of truckloads scheduled for each destination in the next quarter. 
Bid
City 1
City 2
City 3
City 4
City 5
City 6
City 7
City 8
City 9
City 10
City 11
City 12
City 13
City 14
City 15
City 16
City 17
City 18
City 19
City 20
Number of Bids
City 1
City 2
City 3
City 4
City 5
City 6
City 7
City 8
City 9
City 10
City 11
City 12
City 13
City 14
City 15
City 16
City 17
City 18
City 19
City 20
30
10
20
40
10
10
12
25
25
33
11
29
12
24
10
10
23
25
12
10
S/Truckload
Demand
$724
$766
$741
$815
$904
$958
$925
$892
$927
$963
10
Carrier 1
$1,453
$1,534
$1,687
$1,523
$1,521
$2,100
$1,800
$1,134
$672
$800
10
Carrier 2
$723
$766
$745
$828
$880
$933
$929
$869
$969
$938
10
Carrier 3
$1,922
$1,432
$1,233
$610
$627
$721
$822
7
Carrier 4
$2,188
$2,602
$2,283
$2,617
$2,239
$1,571
$1,938
$1,416
$1,181
$669
$657
$682
$682
$745
$891
$891
$937
$829
$967
$955
20
Carrier 5
$1,666
$1,767
$1,857
$1,738
$1,771
5
Carrier 6
$1,790
$1,870
$1,855
$1,545
$2,050
$1,739
$1,150
$678
$706
$733
$733
$832
$914
$914
$984
$864
$1,008
$995
18
Carrier 7
Destination (Truckloads)
 
 Because dealing with too many carriers can be cumbersome, Offhaus would like to limit 
the number of carriers it uses to three. Also, for customer relationship reasons Offhaus 
wants each city to be assigned to only one carrier (that is, there is no splitting of the de-
mand to a given city across carriers).
a.  Develop a model that will yield the three selected carriers and the city-carrier assign-
ments that minimize the cost of shipping. Solve the model and report the solution.
b.  Offhaus is not sure whether three is the correct number of carriers to select. Run the 
model you developed in part a for allowable carriers varying from 1 up to 7. Based on 
results, how many carriers would you recommend and why?
 8.  The Martin-Beck Company operates a plant in St. Louis with an annual capacity of 30,000 
units. Product is shipped to regional distribution centers located in Boston, Atlanta, and 
Houston. Because of an anticipated increase in demand, Martin-Beck plans to increase ca-
pacity by constructing a new plant in one or more of the following cities: Detroit, Toledo, 
Denver, or Kansas City. The estimated annual fixed cost and the annual capacity for the 
four proposed plants are as follows:
Proposed Plant
Annual Fixed Cost
Annual Capacity
Detroit
Toledo
Denver
Kansas City
$175,000
$300,000
$375,000
$500,000
10,000
20,000
30,000
40,000
file
WEB
Offhaus

 
Problems 
435
 
 The company’s long-range planning group developed forecasts of the anticipated annual 
demand at the distribution centers as follows:
Distribution Center
Annual Demand
Boston
Atlanta
Houston
30,000
20,000
20,000
 
 The shipping cost per unit from each plant to each distribution center is as follows:
Distribution Centers
Plant Site
Boston
Atlanta
Houston
Detroit
Toledo
Denver
Kansas City
St. Louis
 5
 4
 9
10
 8
2
3
7
4
4
3
4
5
2
3
a.  Formulate a mixed-integer programming model that could be used to help Martin-Beck 
determine which new plant or plants to open in order to satisfy anticipated demand.
b.  Solve the model you formulated in part a. What is the optimal cost? What is the opti-
mal set of plants to open?
c.  Using equation (9.1), find a second-best solution. What is the increase in cost versus 
the best solution from part b?
 9.  Galaxy Cloud Services operates several data centers across the United States containing 
servers that store and process the data on the Internet. Suppose that Galaxy Cloud Services 
currently has five outdated data centers: one each in Michigan, Ohio, and California and 
two in New York. Management is considering increasing the capacity of these data centers 
to keep up with increasing demand. Each data center contains servers that are dedicated to 
Secure data and to Super Secure data. The cost to update each data center and the resulting 
increase in server capacity for each type of server are as follows:
Data Center
Cost  
($ millions)
Secure 
Servers
Super Secure 
Servers
Michigan
New York 1
New York 2
Ohio
California
2.5
3.5
3.5
4.0
2.0
50
80
40
90
20
30
40
80
60
30
 
 The projected needs are for a total increase in capacity of 90 Secure servers and 90 Super 
Secure servers. Management wants to determine which data centers to update to meet 
projected needs and, at the same time, minimize the total cost of the added capacity.
a.  Formulate a binary integer programming model that could be used to determine the 
optimal solution to the capacity increase question facing management.
b. Solve the model formulated in part a to provide a recommendation for management.
10.  CHB, Inc. is a bank holding company that is evaluating the potential for expanding into 
the state of Ohio. State law permits establishing branches in any county that is adjacent to 
a county in which a PPB (principal place of business) is located. The following map shows 
the State of Ohio. The file CHB contains an adjacency matrix with a one in the ith row and 

436 
Chapter 9 Integer Linear Optimization Models
jth column indicating that the counties represented by the ith row and the jth column share 
a border. A zero indicates that the two counties do not share a border. 
 
 Formulate and solve a linear binary model that will tell CHB the minimium number of PPB’s 
required and their location in order to allow CHB to put a branch in every county in Ohio.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
Ohio
11.  Consider again Problem 10. Use equation (9.1) to determine whether your solution to 
Problem 10 is unique. If your solution is not unique, use equation (9.1) iteratively to find 
all alternative optima. How many alternative optimal solutions are there?
12.  Consider again the CHB Inc. problem described in Problem 10. Suppose only a limted 
number of PPBs can be placed. CHB would like to place this limited number of PPBs in 
counties so that the allowable branches can reach the maximum possible population. The 
file CHBPop contains the county adjacency matrix described in Problem 10 as well as the 
population of each county. 
a.  Assume that only a fixed number of PPBs, denoted k, can be established. Formulate a 
linear binary integer program that will tell CHB Inc. where to locate the fixed number 
of PPBs in order to maximize the population reached. (Hint: Review the Ohio Trust 
formulation in Section 9.4. Introduce variable yi 5 1 if it is possible to establish a 
branch in county i, and yi 5 0 otherwise; that is, if county i is covered by a PPB, then 
the population can be counted as covered.).
b.  Suppose that two PPBs can be established. Where should they be located to maximize 
the population served? 
c.  Solve your model from part a for allowable number of PPBs ranging from 1 to 10. In 
other words, solve the model 10 times, k set to 1,2, . . . , 10. Record the population 
reached for each value of k. Graph the results of part b by plotting the population 
reached versus number of PPBs allowed. Based on their cost calculations, CHB con-
siders an additional PPB to be a fiscally prudent only if it increases the population 
reached by at least 500,000 people. Based on this graph, what is the number of PPBs 
you recommend to be implemented?
13.  The Northshore Bank is working to develop an efficient work schedule for full-time and 
part-time tellers. The schedule must provide for efficient operation of the bank including 
adequate customer service, employee breaks, and so on. On Fridays, the bank is open 
file
WEB
CHB
file
WEB
CHBPop

 
Problems 
437
from 9:00 a.m. to 7:00 p.m. The number of tellers necessary to provide adequate customer 
service during each hour of operation is summarized here:
Time
Number  
of Tellers
Time
Number  
of Tellers
9:00 a.m.–10:00 a.m.
10:00 a.m.–11:00 a.m.
11:00 a.m.–Noon
Noon–1:00 p.m.
1:00 p.m.–2:00 p.m.
 6
 4
 8
10
 9
2:00 p.m.–3:00 p.m.
3:00 p.m.–4:00 p.m.
4:00 p.m.–5:00 p.m.
5:00 p.m.–6:00 p.m.
6:00 p.m.–7:00 p.m.
6
4
7
6
6
 
 Each full-time employee starts on the hour and works a 4-hour shift, followed by a 1-hour 
break and then a 3-hour shift. Part-time employees work one 4-hour shift beginning on the 
hour. Considering salary and fringe benefits, full-time employees cost the bank $15 per hour 
($105 a day), and part-time employees cost the bank $8 per hour ($32 per day).
a.  Formulate an integer programming model that can be used to develop a schedule 
that will satisfy customer service needs at a minimum employee cost. (Hint: Let xi 5 
number of full-time employees coming on duty at the beginning of hour i and yi 5 
number of part-time employees coming on duty at the beginning of hour i.)
b. Solve the LP Relaxation of your model in part a.
c. Solve your model in part a for the optimal schedule of tellers. Comment on the solution.
d.  After reviewing the solution to part c, the bank manager realized that some additional 
requirements must be specified. Specifically, she wants to ensure that one full-time 
employee is on duty at all times and that there is a staff of at least five full-time em-
ployees. Revise your model to incorporate these additional requirements, and solve 
for the optimal solution.
14.  Burnside Marketing Research conducted a study for Barker Foods on several formulations 
for a new dry cereal. Three attributes were found to be most influential in determining 
which cereal had the best taste: ratio of wheat to corn in the cereal flake, type of sweetener 
(sugar, honey, or artificial), and the presence or absence of flavor bits. Seven children 
participated in taste tests and provided the following part-worths for the attributes (see 
Section 9.4 for a discussion of part-worths):
Wheat/Corn
Sweetener
Flavor Bits
Child
Low
High
Sugar
Honey
Artificial
Present
Absent
1
2
3
4
5
6
7
15
30
40
35
25
20
30
35
20
25
30
40
25
15
30
40
20
25
40
20
25
40
35
40
20
20
35
40
25
35
10
30
35
30
40
15
 8
 7
15
18
 9
20
 9
11
14
18
14
16
11
a.  Suppose the overall utility (sum of part-worths) of the current favorite cereal is 75 
for each child. What product design will maximize the share of choice for the seven 
children in the sample?
b.  Assume the overall utility of the current favorite cereal for children 1–4 is 70, and the 
overall utility of the current favorite cereal for children 5–7 is 80. What product design 
will maximize the share of choice for the seven children in the sample?
15.  The Bayside Art Gallery is considering installing a video camera security system to reduce 
its insurance premiums. A diagram of the eight display rooms that Bayside uses for exhibi-
tions is shown in the following figure; the openings between the rooms are numbered 1–13. A 
file
WEB
Burnside

438 
Chapter 9 Integer Linear Optimization Models
 security firm proposed that two-way cameras be installed at some room openings. Each cam-
era has the ability to monitor the two rooms between which the camera is located. For example, 
if a camera were located at opening number 4, rooms 1 and 4 would be covered; if a camera 
were located at opening 11, rooms 7 and 8 would be covered; and so on. Management decided 
not to locate a camera system at the entrance to the display rooms. The objective is to provide 
security coverage for all eight rooms using the minimum number of two-way cameras.
a.  Formulate a binary integer linear programming model that will enable Bayside’s man-
agement to determine the locations for the camera systems.
b.  Solve the model formulated in part a to determine how many two-way cameras to 
purchase and where they should be located.
c.  Suppose that management wants to provide additional security coverage for room 7. 
Specifically, management wants room 7 to be covered by two cameras. How would the 
model you formulated in part a have to change to accommodate this policy restriction?
d.  With the policy restriction specified in part c, determine how many two-way camera 
systems will need to be purchased and where they will be located.
Entrance
Room 
1
Room 
3
Room 
7
Room 
4
Room 
8
Room 
6
Room 
5
Room 
2
1
4
2
5
8
9
12
13
11
10
7
6
3
16.  The Delta Group is a management consulting firm specializing in the health care industry. 
A team is being formed to study possible new markets, and a linear programming model has 
been developed for selecting team members. However, one constraint the president imposed 

 
Problems 
439
is that the team size should be three, five, or seven members. The staff cannot figure out how 
to incorporate this requirement in the model. The current model requires that team members 
be selected from three departments and uses the following variable definitions:
 
x1 5 the number of employees selected from department 1
 
x2 5 the number of employees selected from department 2
 
x3 5 the number of employees selected from department 3
Show the staff how to write constraints that will ensure that the team will consist of three, 
five, or seven employees. The following integer variables should be helpful:
y1 5
1 if team size is 3 
0 otherwise
y2 5
1 if team size is 5 
0 otherwise
y3 5
1 if team size is 7 
0 otherwise
17.  Roedel Electronics produces tablet computer accessories, including integrated keyboard 
tablet stands that connect a keyboard to a tablet device and holds the device at a preferred 
angle for easy viewing and typing. Roedel produces two sizes of integrated keyboard tablet 
stands, a small and a large size. Each size uses the same keyboard attachment, but the 
stand consists of two different pieces, a top flap and a vertical stand that differ by size. 
Thus, a completed integrated keyboard tablet stand consists of three subassemblies that 
are manufactured by Roedel: a keyboard, a top flap, and a vertical stand. 
 Roedel’s sales forecast indicates that 7000 small integrated keyboard tablet stands and 
5000 large integrated keyboard tablet stands will be needed to satisfy demand during the 
upcoming Christmas season. Because only 500 hours of in-house manufacturing time 
are available, Roedel is considering purchasing some, or all, of the subassemblies from 
outside suppliers. If Roedel manufactures a subassembly in-house, it incurs a fixed setup 
cost as well as a variable manufacturing cost. The following table shows the setup cost, 
the manufacturing time per subassembly, the manufacturing cost per subassembly, and the 
cost to purchase each of the subassemblies from an outside supplier:
Subassembly
Setup  
Cost ($)
Manufacturing  
Time per Unit (min.)
Manufacturing  
Cost per Unit ($)
Purchase Cost  
per Unit ($)
Keyboard
Small top flap
Large top flap
Small vertical stand
Large vertical stand
1000
1200
1900
1500
1500
0.9
2.2
3.0
0.8
1.0
0.40
2.90
3.15
0.30
0.55
0.65
3.45
3.70
0.50
0.70
a.  Determine how many units of each subassembly Roedel should manufacture and how 
many units of each subassembly Roedel should purchase. What is the total manufac-
turing and purchase cost associated with your recommendation?
b.  Suppose Roedel is considering purchasing new machinery to produce large top flaps. 
For the new machinery, the setup cost is $3,000; the manufacturing time is 2.5 minutes 
per unit, and the manufacturing cost is $2.60 per unit. Assuming that the new ma-
chinery is purchased, determine how many units of each subassembly Roedel should 
manufacture and how many units of each subassembly Roedel should purchase. What 
is the total manufacturing and purchase cost associated with your recommendation? 
Do you think the new machinery should be purchased? Explain.
18.  Suppose that management of Valley Cinemas would like to investigate the potential of 
using a scheduling system for their chain of multiple-screen theaters. Valley selected a 
small two-screen movie theater for the pilot testing and would like to develop an integer 
programming model to help schedule the movies. Six movies are available. The first week 

440 
Chapter 9 Integer Linear Optimization Models
each movie is available, the last week each movie can be shown, and the maximum num-
ber of weeks that each movie can run are shown here:
Movie
First Week  
Available
Last Week  
Available
Max. Run  
(weeks)
1
2
3
4
5
6
1
1
1
2
3
3
2
3
1
4
6
5
2
2
1
2
3
3
 
 The overall viewing schedule for the theater is composed of the individual schedules for 
each of the six movies. For each movie, a schedule must be developed that specifies the 
week the movie starts and the number of consecutive weeks it will run. For instance, one 
possible schedule for movie 2 is for it to start in week 1 and run for two weeks. Theater 
policy requires that once a movie is started, it must be shown in consecutive weeks. It can-
not be stopped and restarted again. To represent the schedule possibilities for each movie, 
the following decision variables were developed:
yijw 5 1 if movie i is scheduled to start in week j and run for w weeks 
0 otherwise
 
 For example, x532 5 1 means that the schedule selected for movie 5 is to begin in week 3 and 
run for two weeks. For each movie, a separate variable is given for each possible schedule.
a.  Three schedules are associated with movie 1. List the variables that represent these 
schedules.
b. Write a constraint requiring that only one schedule be selected for movie 1.
c. Write a constraint requiring that only one schedule be selected for movie 5.
d.  What restricts the number of movies that can be shown in week 1? Write a constraint 
that restricts the number of movies selected for viewing in week 1.
e. Write a constraint that restricts the number of movies selected for viewing in week 3.
19.  East Coast Trucking provides service from Boston to Miami using regional offices lo-
cated in Boston, New York, Philadelphia, Baltimore, Washington, Richmond, Raleigh, 
Florence, Savannah, Jacksonville, and Tampa. The number of miles between the regional 
offices is provided in the following table:
New York Philadelphia Baltimore Washington Richmond Raleigh Florence Savannah Jacksonville Tampa Miami
Boston
New York
Philadelphia
Baltimore
Washington
Richmond
Raleigh
Florence
Savannah
Jacksonville
Tampa
211
320
109
424
213
104
459
248
139
 35
565
354
245
141
106
713
502
393
289
254
148
884
673
564
460
425
319
171
1056
 845
 736
 632
 597
 491
 343
 172
1196
 985
 876
 772
 737
 631
 483
 312
 140
1399
1188
1079
 975
 940
 834
 686
 515
 343
 203
1669
1458
1349
1245
1210
1104
 956
 785
 613
 473
 270
 
 The company’s expansion plans involve constructing service facilities in some of the cit-
ies where regional offices are located. Each regional office must be within 400 miles of 
a service facility. For instance, if a service facility is constructed in Richmond, it can 
provide service to regional offices located in New York, Philadelphia, Baltimore, Wash-
ington, Richmond, Raleigh, and Florence. Management would like to determine the mini-
mum number of service facilities needed and where they should be located.
a.  Formulate an integer linear program that can be used to determine the minimum num-
ber of service facilities needed and their locations
file
WEB
EastCoast

 
Case Problem Applecore Children’s Clothing 
441
b.  Solve the integer linear program formulated in part a. How many service facilities are 
required, and where should they be located?
c.  Suppose that each service facility can provide service only to regional offices within 
300 miles. Re-solve the integer linear program with the 300-mile requirement. How 
many service facilities are required and where should they be located?
20.  Dave has $100,000 to invest in 10 mutual fund alternatives with the following restrictions. 
For diversification, no more than $25,000 can be invested in any one fund. If a fund is 
chosen for investment, then at least $10,000 will be invested in it. No more than two of the 
funds can be pure growth funds, and at least one pure bond fund must be selected. The total 
amount invested in pure bond funds must be at least as much as the amount invested in pure 
growth funds. Using the following expected returns, formulate and solve a model that will 
determine the investment strategy that will maximize expected annual return. What assump-
tions have you made in your model? How often would you expect to run your model?
Fund
Type
Expected  
Return (%)
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
Growth
Growth
Growth
Growth
Growth & Income
Growth & Income
Growth & Income
Stock & Bond
Bond
Bond
6.70
7.65
7.55
7.45
7.50
6.45
7.05
6.90
5.20
5.90
applecore Children’s Clothing
Applecore Children’s Clothing is a retailer that sells high-end clothes for toddlers (ages 
1–3) primarily in shopping malls. Applecore also has a successful Internet-based sales divi-
sion. Recently Dave Walker, vice-president of the e-commerce division, has been given the 
directive to expand the company’s Internet sales. He commissioned a major study on the 
effectiveness of Internet ads placed on news Web sites. The results were favorable: Current 
patrons who purchased via the Internet and saw the ads on news Web sites spent more, on 
average, than did comparable Internet customers who did not see the ads. 
With this new information on Internet ads, Walker continued to investigate how new In-
ternet customers could most effectively be reached. One of these ideas involved strategically 
purchasing ads on news Web sites prior to and during the holiday season. To determine which 
news sites might be the most effective for ads, Walker conducted a follow-up study. An e-mail 
questionnaire was administered to a sample of 1200 current Internet customers to ascertain 
which of 30 news sites they regularly visit. The idea is that Web sites with high proportions 
of current customer visits would be viable sources of future customers of Applecore products. 
Walker would like to ascertain which news sites should be selected for ads. The prob-
lem is complicated because Walker does not want to count multiple exposures. So, if a 
respondent visits multiple sites with Applecore ads or visits a given site multiple times, that 
respondent should be counted as reached but not more than once. In other words, a customer 
is considered reached if he or she has visited at least one Web site with an Applecore ad. 
Data from the customer e-mail survey have begun to trickle in. Walker wants to de-
velop a prototype model based on the current survey results. So far, 53 surveys have been 
returned. To keep the prototype model manageable, Walker wants to proceed with model 
development using the data from the 53 returned surveys and using only the first ten news 
sites in the questionnaire. The costs of ads per week for the ten Web sites are given in the
Case Problem

442 
Chapter 9 Integer Linear Optimization Models
following table, and the budget is $10,000 per week. For each of the 53 responses received, 
which of the ten Web sites are regularly visited is given as shown below. For a given 
 customer–Web site pair, a one indicates that the customer regularly visits that Web site, 
and a zero indicates that the customer does not regularly visit that site. 
Data for Applecore Customer Visits to News Web sites (respondents 5–33 hidden).
Website
Website
$5.0
$8.0
$3.5
$5.5
$7.0
$4.5
$6.0
$5.0
$3.0
$2.2
Customer
Cost/Wk ($000)
2
3
4
34
1
1
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
1
1
0
0
0
1
1
0
0
0
0
0
0
0
0
1
0
1
0
1
0
0
0
0
2
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
1
0
0
0
0
0
1
0
1
3
0
0
0
0
0
0
1
1
1
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
4
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
5
0
0
1
1
0
1
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
0
0
1
6
0
0
1
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
7
0
0
0
0
0
0
0
0
0
1
1
1
0
0
0
0
1
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
9
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
10
1
2
3
4
5
6
7
8
9
10
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
Managerial Report:
 1.  Develop a model that will allow Applecore to maximize the number of customers 
reached for a budget of $10,000 for one week of promotion. 
 2.  Solve the model. What is the maximum number of customers reached for the $10,000 
budget? 
 3.  Perform a sensitivity analysis on the budget for values from $5,000 to $35,000 in 
increments of $5,000. Construct a graph of percentage reach versus budget. Is the 
additional increase in percentage reach monotonically decreasing as the budget al-
location increases? Why or why not? What is your recommended budget? Explain.
Solving Integer Linear Optimization problems 
Using analytic Solver platform
In this appendix, we illustrate how to use Analytic Solver Platform (ASP) to solve integer 
linear programs in Excel
file
WEB
Applecore
Appendix

 
Appendix Solving Integer Linear Optimization Problems Using Analytic Solver Platform 
443
Recall the Eastborne Realty integer programming model:
 
T 5 number of townhouses
 
A 5 number of apartment buildings
Max
10T 1
15A
s.t.
282T 1 400A # 2000
4T 1
40A # 140
T # 5
T, A $ 0, and integer
The spreadsheet model is shown in Figure 9.13. 
FIGURE 9.13   EASTBORNE REALTY SPREADSHEET MODEL
Eastborne Realty Problem
A
B
C
E
D
F
G
Parameters
Price ($000)
Townhouse
Apt. Bldg.
Mgr. Time
Ann. Cash Flow ($000)
282
4
10
40
400
15
2
Funds Avl. ($000)
Mgr. Time Avl. (Hours)
Townhouses Avl.
Purchase Plan
Max Cash Flow ($000)
Model
1
2
3
4
5
6
7
8
11
12
13
9
10
14
15
16
17
18
4
=SUMPRODUCT (B7:C7,B14:C14)
Funds ($000)
Time (Hours)
Townhouses
=SUMPRODUCT(B4:C4,$B$14:$C$14)
Total Used
=SUMPRODUCT(B5:C5,$B$14:$C$14)
=B14
Total Available
2000
140
5
=G4
=G5
=G6
Townhouses
Number of
Apt. Bldgs.
Eastborne Realty Problem
A
B
C
D
E
F
G
Parameters
Price ($000)
Townhouse
Apt. Bldg.
Mgr. Time
Ann. Cash Flow ($000)
$282
4
$10
40
$400
$15
2
Funds Avl. ($000)
Mgr. Time Avl. (Hours)
Townhouses Avl.
Purchase Plan
Max Cash Flow ($000)
Model
1
2
3
4
5
6
7
8
11
12
13
9
10
14
15
16
17
18
4
Funds ($000)
Time (Hours)
$70
Townhouses
Total Used
$1,928
96
4
Total Available
$2,000
140
5
$2,000
140
5
Townhouses
Number of
Apt. Bldgs.

444 
Chapter 9 Integer Linear Optimization Models
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Excel Ribbon
Step 2. When the Solver Options and Model Specifications task pane appears, select 
the  next to Optimization to expand the tree structure
Step 3.  When the optimization tree structure appears (see Figure 9.14):
Select Objective
Select cell B17
Click the Add button 
 in the Solver Options and Model 
 Specifications task pane (Figure 9.14)
Step 4. Under Variables, select Normal
Select cells B14:C14
Click the Add button 
 in the Solver Options and Model Specifica-
tions task pane
Step 5. Under Constraints, select Normal
Select cells F15:F17
Click the Add button 
 in the Solver Options and Model Specifica-
tions task pane
file
WEB
Eastborne
If the Solver Options and 
Model Specifications task 
pane is not visible, click 
Model in the Model group 
under the ANALYTIC 
SOLVER PLATFORM tab 
to activate this pane.
FIGURE 9.14   THE OPTIMIZATION TREE STRUCTURE IN THE SOLVER 
 OPTIONS AND MODEL SPECIFICATIONS DIALOG BOX
Delete
Add
Analyze
Solve
Refresh

 
Appendix Solving Integer Linear Optimization Problems Using Analytic Solver Platform 
445
When the Add Constraint dialog box appears:
Select ,5 from the drop-down button
Enter G15:G17 in the Constraint: area
Click OK
Step 6. Under Constraints, select Integers
Select cells B14:C14
Click the Add button 
 in the Solver Options and Model Specifica-
tions task pane
In the Add Constraint dialog box, from the drop down list select int
Click OK
The Solver Options and Model Specifications pane should now appear as shown in 
Figure 9.15. 
Step 7. Click the Engine tab in the Solver Options and Model Specifications task 
pane
Select the checkbox for Automatically Select Engine
In the General area, click Assume Non-Negative
Select True from the drop-down menu
FIGURE 9.15   THE EASTBORNE REALTY MODEL IN THE SOLVER OPTIONS 
AND MODEL SPECIFICATIONS DIALOG BOX

446 
Chapter 9 Integer Linear Optimization Models
FIGURE 9.16   THE OUTPUT FROM THE OPTIMIZATION FROM ANALYTIC 
SOLVER PLATFORM
Step 8. Click the Model tab in the Solver Options and Model Specifications task 
pane
Step 9. Click the Solve button  in the Solver Options and Model Specifications task 
pane (see Figure 9.14)
The solution is sent to the spreadsheet, and the output appears under the Output tab in 
the Solver Options and Model Specifications pane, as shown in Figure 9.16. The  output 
indicates that the optimal solution was found and that all constraints were satisfied. The 
solution should be same as when we solve the Eastborne Realty problem using standard 
Excel Solver as shown in Figure 9.2. The completed linear integer optimization model for 
the Eastborne Realty problem is contained in the file  EastborneModel.
Upon clicking the Solve 
button, the Guided Mode 
dialog box will appear 
if ASP’s Guided Mode is 
turned on. ASP’s Guided 
Mode assists the analyst 
through the optimiza-
tion process and can be 
turned off at the analyst’s 
 discretion.
file
WEB
EastborneModel
 

 
Appendix Solving Integer Linear Optimization Problems Using Analytic Solver Platform 
447
NOTES AND COMMENTS
1. To make a variable binary, under Constraints, 
select Integers. Select the variable cell location 
and then click the Add button. In the Add Con-
straint dialog box, select bin from the drop-
down list. Then click OK.
2. The Solver Options and Model Specifications 
pane can be moved by clicking the banner at the 
top (the location of the title of the box), holding 
the mouse, and dragging. The pane can be in-
voked or hidden by clicking on the Model but-
ton on the far left of the ASP Ribbon.
3. The default objective type is maximize. To 
change the objective function to minimize, se-
lect the objective function location in the opti-
mization tree, right click the mouse, select Edit, 
and then select Min. Alternatively, select the 
objective function location in the optimization 
tree, click the Objective button in the Optimi-
zation Model group in the ASP Ribbon, select 
Min, and select Normal. 
4. The objective, variables, and constraints can 
also be added to the model by clicking the De-
cisions, Constraints, and Objective buttons, 
respectively, in the Optimization Model group 
of the ASP Ribbon.
5. The Refresh button 
 (Figure 9.14) should be 
used whenever a model has been changed (new 
constraints, variables, or other changes have 
been made).
6. To delete elements (variables, constraints, or 
objective) from the model, select the element 
from the optimization tree, and click the Delete 
button 
 (Figure 9.14). To keep a variable or 
constraint in the model but have it ignored for a 
given run, click the box next to that element (the 
green check mark will disappear, indicating that 
it is not part of the current model run). 
7. The Analyze button 
 (Figure 9.14) provides 
an analysis of the model including number and 
types of variables and constraints. The report 
appears at the bottom of the Solver Options 
and Model Specifications pane.
8. To generate an Answer Report after solving the 
problem (as discussed previously in this chap-
ter), select Reports from the Analysis group 
of the ASP Ribbon. Select Optimization and 
then Answer. The Sensitivity Report available 
for linear programs is not available for integer 
programs. To determine whether changes in the 
input parameters have an impact on the solu-
tion, the model must be resolved with the new 
parameter values.
9. ASP allows you to solve larger linear integer 
programs than Standard Excel Solver. Stan-
dard Solver is limited to 200 variables and 
100 constraints; ASP is limited to 8000 vari-
ables and 8000 constraints for linear integer 
problems.

Nonlinear Optimization 
Models
CONTENTS
10.1 A PRODUCTION 
APPLICATION: PAR, INC. 
REVISITED
An Unconstrained Problem
A Constrained Problem
Solving Nonlinear Optimization 
Models Using Excel Solver
Sensitivity Analysis and Shadow 
Prices in Nonlinear Models
10.2 LOCAL AND GLOBAL  OPTIMA
Overcoming Local Optima with 
Excel Solver
10.3 A LOCATION PROBLEM
10.4 MARKOWITZ  PORTFOLIO 
MODEL
10.5 FORECASTING ADOPTION 
OF A NEW PRODUCT
APPENDIX:  SOLVING 
 NONLINEAR 
OPTIMIZATION 
 PROBLEMS WITH 
ANALYTIC SOLVER 
PLATFORM
Chapter 10

 
10.1 A Production Application: Par, Inc. Revisited 
449
Many business processes behave in a nonlinear manner. For example, the price of a bond 
is a nonlinear function of interest rates, and the price of a stock option is a nonlinear func-
tion of the price of the underlying stock. The marginal cost of production often decreases 
with the quantity produced, and the quantity demanded for a product is usually a nonlinear 
function of the price. These and many other nonlinear relationships are present in many 
business applications.
A nonlinear optimization problem is any optimization problem in which at least one 
term in the objective function or a constraint is nonlinear. In Section 10.1, we examine a 
production problem in which the objective function is a nonlinear function of the deci-
sion variables, similar to the Analytics in Action: Intercontinental Hotels Optimizes Retail 
Pricing. In Section 10.2, we discuss issues that make nonlinear optimization very different 
from linear optimization. Section 10.3 presents a nonlinear model for facility location. 
In Sec tion 10.4, we present the Nobel Prize–winning Markowitz model for managing the 
trade-off between risk and return in the construction of an investment portfolio. In Section 
10.5, we consider a well-known model that effectively forecasts sales or adoptions of a new 
product. The chapter appendix describes how to solve nonlinear optimization models using 
Analytic Solver Platform.
a production application: par, Inc. revisited
We introduce constrained and unconstrained nonlinear optimization problems by consider-
ing an extension of the Par, Inc. linear program introduced in Chapter 8. We first consider 
the case in which the relationship between price and quantity sold causes the objective 
function to be nonlinear. The resulting unconstrained nonlinear program is then solved. As 
10.1
InterContinental Hotel Group (IHG) owns, leases, 
or franchises over 4500 hotels in about 100 countries 
around the world. It offers in excess of 650,000 guest 
rooms, more than any other hotel. InterContinental 
 Hotels, Crowne Plaza Hotels and Resorts, Holiday Inn 
Hotels and Resorts, and Holiday Inn Express are some 
of InterContinental’s brands. 
Like airlines and rental car companies, hotels of-
fer a perishable good; that is, hotels have a limited time 
window in which to sell the product, after which the 
value perishes. For example, an empty seat on an air-
line flight is of no value, as is a hotel room that goes 
empty overnight. In dealing with perishable goods, how 
to price them in such a way as to maximize revenue is a 
challenge. Price the hotel room too high, and it will sit 
empty overnight and generate zero revenue. Price the 
hotel room too low, the hotel will be filled, but revenue 
likely will be lower than it could have been with higher 
pricing, even if fewer rooms were booked. Revenue 
management (RM) is a term used to describe analytical 
approaches to this pricing problem.
IHG developed a novel approach to the hotel room 
pricing problem that uses a nonlinear optimization  model 
to determine prices to charge for its rooms. Each day, 
IHG searches the Internet to acquire competitors’ prices. 
The competitors’ prices are factored into IHG’s pricing 
optimization model, which is run daily. The model is 
nonlinear because the objective function is to maximize 
contribution (revenue 2 cost), but both demand and rev-
enue are a function of the price variable. Over 2000 IHG 
hotels have started using this pricing model, and its use 
has led to increased revenue in excess of $145 million.
INTERCONTINENTAL HOTELS OPTIMIZES RETAIL PRICING*
ANALYTICS  in  Action
*Based on D. Kosuhik, J. A. Higbie, and C. Eister, “Retail Price Optimiza-
tion at InterContinental Hotels Group,” Interfaces 42, no. 1, (January–
February 2012): 45–57.

450 
Chapter 10 Nonlinear Optimization Models
we shall see, the unconstrained optimal solution does not satisfy the production constraints 
of the original problem. Adding the production constraints back into the problem allows 
us to show the formulation and solution of a constrained nonlinear optimization model.
An Unconstrained Problem
Let us consider a revision of the Par, Inc. problem discussed in Chapter 8. Recall that Par, 
Inc. decided to manufacture Standard and Deluxe golf bags. In formulating the linear pro-
gramming model for the Par, Inc. problem, we assumed that the company could sell all of 
the Standard and Deluxe bags it could produce. However, depending on the price of the 
golf bags, this assumption may not hold. An inverse relationship usually exists between 
price and demand. As price increases, the quantity demanded decreases. Let PS denote the 
price Par, Inc. charges for each Standard bag and PD denote the price for each Deluxe bag. 
 Assume that the demand for Standard bags, S, and the demand for Deluxe bags, D, are 
given by
 
 S 5 2250 2 15PS 
(10.1)
 
D 5 1500 2 5PD  
(10.2)
The revenue generated from Standard bags is the price of each Standard bag, PS, times 
the number of Standard bags sold, S. If the cost to produce a Standard bag is $70, then the 
cost to produce S Standard bags is 70S. Thus the profit contribution for producing and sell-
ing S Standard bags (revenue 2 cost) is
 
PS S 2 70S 5 (PS 2 70)S 
(10.3)
We can solve equation (10.1) for PS to show how the price of a Standard bag is related to 
the number of Standard bags sold: PS 5 150 2 (1/15)S. Substituting 150 2 (1/15)S for PS 
in equation (10.3), the profit contribution for standard bags is
 
(PS 2 70)S 5 [150 2 (1y15)S 2 70]S 5 80S 2 (1y15)S 2 
(10.4)
Suppose that the cost to produce each Deluxe golf bag is $150. Using the same logic 
we used to develop equation (10.4), the profit contribution for Deluxe bags is
 
(PD 2 150)D 5 [300 2 (1y5)D 2 150]D 5 150D 2 (1y5) D2
Total profit contribution is the sum of the profit contribution for Standard bags and the 
profit contribution for Deluxe bags. Thus, total profit contribution is written as
 
Total profit contribution 5 80S 2 (1y15) S2 1 150D 2 (1y5) D2 
(10.5)
Note that the two linear demand functions, equations (10.1) and (10.2), give a nonlinear to-
tal profit contribution function, equation (10.5). This function is an example of a quadratic 
function because the nonlinear terms have an exponent of 2 (S2 and D2).
Using Excel Solver, we find that the values of S and D that maximize the profit contri-
bution function are S 5 600 and D 5 375. The corresponding prices are $110 for Standard 
bags and $225 for Deluxe bags, and the profit contribution is $52,125. If all production 
constraints are also satisfied, these values provide the optimal solution for Par, Inc.
A Constrained Problem
In calculating the unconstrained optimal solution, we have ignored the production con-
straints discussed in Chapter 8. Recall that Par, Inc. has limited amounts of time available 
in each of four departments (cutting and dyeing, sewing, finishing, and inspection and 
packaging). We must enforce constraints that ensure that the amount of time used does not 
exceed the amount of time available in each of these departments. The problem that Par, 
Inc. must solve is to maximize the total profit contribution subject to all of the departmental 
Details of how to use Excel 
Solver for nonlinear optimi-
zation are discussed in the 
next section.

 
10.1 A Production Application: Par, Inc. Revisited 
451
labor hour constraints given in Chapter 8. The complete mathematical model for the Par, 
Inc. constrained nonlinear maximization problem follows.
Max
80S 2
1⁄15 S2 1 150D 2 1⁄5D2
s.t.
7⁄10S 1
1D # 630  Cutting and dyeing
1⁄2 S 1
5⁄6 D # 600  Sewing
1S 1
2⁄3 D # 708  Finishing
1⁄10 S 1
1⁄4 D # 135  Inspection and packaging
S, D $ 0
The feasible region for the original Par, Inc. problem, along with the unconstrained 
optimal solution point (600, 375), is shown in Figure 10.1. The unconstrained optimum of 
(600, 375) is obviously outside the feasible region.
This maximization problem is exactly the same as the Par, Inc. problem in Chapter 8 
except for the nonlinear objective function. The solution to this new constrained nonlinear 
maximization problem is shown in Figure 10.2.
In Figure 10.2 we see three profit contribution contour lines. Each point on the same 
contour line is a point of equal profit. Here, the contour lines show profit contributions of 
$45,000, $49,920.55, and $51,500. In the original Par, Inc. problem described in Chapter 2, 
the objective function is linear, and thus the profit contours are straight lines. However, for 
the Par, Inc. problem with a quadratic objective function, the profit contours are ellipses.
Because part of the $45,000 profit contour line cuts through the feasible region, we 
know an infinite number of combinations of Standard and Deluxe bags will yield a profit of 
$45,000. An infinite number of combinations of Standard and Deluxe bags also provide a 
Figure 10.1   THE PAR, INC. FEASIBLE REGION AND THE OPTIMAL SOLUTION 
FOR THE UNCONSTRAINED OPTIMIZATION PROBLEM
S
800
600
400
200
Number of Standard Bags
0
Number of Deluxe Bags
Feasible
Region
200
400
600
Unconstrained Optimum
(600, 375)
Proﬁt = $52,125
D

452 
Chapter 10 Nonlinear Optimization Models
profit of $51,500. However, none of the points on the $51,500 contour profit line are in the 
feasible region. As the contour lines move farther out from the unconstrained optimum of 
(600, 375) the profit contribution associated with each contour line decreases. The contour 
line representing a profit of $49,920.55 intersects the feasible region at a single point. With-
out showing all of the details in solving for this point, the point of intersection is 459.717 
Standard bags and 308.198 Deluxe bags. This solution provides the maximum possible 
profit. No contour line that has a profit contribution greater than $49,920.55 will intersect 
the feasible region. Because the contour lines are nonlinear, the contour line with the high-
est profit can touch the boundary of the feasible region at any point, not just an extreme 
point. In the Par, Inc. case, the optimal solution is on the cutting and dyeing constraint line 
partway between two extreme points.
It is also possible for the optimal solution to a nonlinear optimization problem to lie in 
the interior of the feasible region. For instance, if the right-hand sides of the constraints in 
the Par, Inc. problem were all increased by a sufficient amount, the feasible region would 
expand so that the optimal unconstrained solution point of (600, 375) with a profit contribu-
tion of $52,125 in Figure 10.2 would be in the interior of the feasible region.
Many linear optimization algorithms (e.g., the simplex method) optimize by examining 
only the extreme points and selecting the extreme point that gives the best solution value. 
As the solution to the constrained Par, Inc. nonlinear problem illustrates, such a method 
will not work in the nonlinear case because the optimal solution is generally not an extreme 
point solution. Hence, nonlinear optimization algorithms are more complex than linear op-
timization algorithms, and the details are beyond the scope of this text. Fortunately, we do 
not need to know how nonlinear algorithms work; we just need to know how to use them. 
Computer software such as Excel Solver and Analytic Solver Platform are available to solve 
nonlinear optimization problems. 
Figure 10.2   THE PAR, INC. FEASIBLE REGION WITH OBJECTIVE FUNCTION 
CONTOUR LINES
S
800
600
400
200
0
200
400
600
D
$45,000
Contour
$52,125
Optimal Solution
$49,920.55
$49,920.55
Contour
$51,500
Contour
Number of Standard Bags
Number of Deluxe Bags

 
10.1 A Production Application: Par, Inc. Revisited 
453
Next we discuss how to use Excel Solver to solve nonlinear optimization problems. In 
the chapter appendix, we discuss using Analytic Solver Platform to solve nonlinear opti-
mization problems.
Solving Nonlinear Optimization Models  
Using Excel Solver
We use the constrained nonlinear Par, Inc. problem to illustrate how to use Excel Solver 
to solve nonlinear optimization problems. The procedure for developing and entering the 
model in Excel is the same as for linear problems as discussed in Chapter 8, except that one 
or more of the functions is nonlinear. 
Figure 10.3 shows the Excel model and Solver dialog box for the nonlinear Par, Inc. 
problem. The SUMPRODUCT function is used in cells B19 through B22 to calculate 
the number of hours required in each department. The price function for Standard bags 
Figure 10.3   SPREADSHEET MODEL AND SOLVER DIALOG BOX FOR THE 
 NONLINEAR PAR, INC. PROBLEM.
A
Par, Inc.
B
C
D
Parameters
Production
Time (Hours)
Time
Available
Standard
Hours
Deluxe
Standard
Deluxe
1
2
3
4
5
6
7
8
9
10
11
Operation
Hours
Used
Hours
Available
Operation
Cutting and Dyeing
Sewing
Finishing
Inspection and
Packaging
Cutting and Dyeing
Sewing
Finishing
Inspection and
Packaging
Standard Bag
Price Function
Deluxe Bag
Price Function
Marginal Cost
0.7
0.5
1
0.1
$70.00
Bags Produced
459.717
$49,920.55
630.000
486.690
665.182
123.021
119.35
238.36
630
600
708
135
308.198
1
0.833
0.667
0.25
$150.00
630
600
708
135
Model
Total Proﬁt
12
13
14
15
16
17
18
19
23
22
24
25
26
20
21
E
F
G
H
I
J
file
WEB
ParNonlinear

454 
Chapter 10 Nonlinear Optimization Models
is entered in cell B25 as 5150-(1/15)*$B$14 and similarly for Deluxe bags in cell D26 
as 5300-(1/5)*$C$14. The objective function in cell B16 contains the formula 5(B25-
B9)*B141(B26-C9)*C14, which corresponds to (150 2 (1/15)S 2 70)S 1 (300 2 (1/5)D 2 
150)D. As previously shown, this is mathematically equivalent to equation (10.5) because 
(150 2 (1/15)S 2 70)S 1 (300 2 (1/5)D 2 150)D 5 80S 2 (1/15) S2 1 150D 2 (1/5) D2.
To invoke Solver, we follow these steps:
Step 1. Click the DATA tab in the Ribbon
Step 2. Click Solver in the Analysis group
Step 3. When the Solver Parameters dialog box appears:
Enter B16 into the Set Objective: box 
Step 4. Enter B14:C14 into the By Changing Variable Cells: box area
Step 5. Click the Add button
Enter B19:B22 in the Cell Reference: box
Select ≤ from the drop-down menu
Enter C19:C22 in the Constraint: box
Click OK
Step 6. Select the Make Unconstrained Variables Non-negative option
Step 7. For Select a Solving Method: select GRG Nonlinear from the drop-down 
menu
Step 8. Click Solve
Step 9. When the Solver Results dialog box appears, click OK
The complete model for the constrained nonlinear Par, Inc. problem is contained in the 
file ParNonlinearModel.
The Answer Report generated by Excel Solver has the same structure as that of linear 
programs. Rather than show the Answer Report here, we refer to the optimal values shown 
in the spreadsheet in Figure 10.3. The optimal value of the objective function is $49,920.55, 
and this is achieved by producing 459.717 Standard bags and 308.198 Deluxe bags. This is 
the optimal point shown geometrically in Figure 10.2. Also, comparing cells C19 through 
C22 with D19 through D22 shows that only the cutting and dyeing constraint is binding, 
which is consistent with Figure 10.2.
Sensitivity Analysis and Shadow Prices  
in Nonlinear Models
The Sensitivity Report for the nonlinear Par, Inc. problem is shown in Figure 10.4. As in the 
linear case, there are two sections: one for the variables and the other for constraints. The 
variables section gives the cell location, name, final (optimal) value, and reduced gradient 
for each variable. The reduced gradient is analogous to the reduced cost for linear models. 
It is essentially the shadow price of the nonnegativity constraint or more generally, the 
shadow price of a binding simple lower or upper bound on the decision variable.
The constraint section gives the cell location, name, and final value for the left hand side 
of each constraint. For the Par, Inc. problem, the final values are the amount of time in hours 
used in each of the four departments. The far right column gives the Lagrangian multi-
plier for each constraint. The Lagrangian multiplier is the shadow price for a constraint 
in a nonlinear problem. In other words, the Lagrangian multiplier is the rate of change of 
the objective function with respect to the right hand side of a constraint. For the Par, Inc. 
example, as we increase the number of hours available in the cutting and dyeing depart-
ment, we expect the profit to increase by $26.72 per hour. However, notice that no ranges 
are given for allowable right-hand side changes. This is because the allowable increase and 
decrease are essentially zero. Changing the right and side of a binding constraint by even a 
small amount will change the value of Lagrangian multiplier. Nonetheless, the Lagrangian 
multiplier does give an estimate of the importance of relieving a binding constraint
file
WEB
ParNonlinearModel

 
10.2 Local and Global Optima 
455
Figure 10.4   EXCEL SOLVER SENSITIVITY REPORT FOR THE NONLINEAR 
PAR, INC. PROBLEM
A
B
C
D
Reduced
Gradient
Constraints
$B$22
Cell
Variable Cells
$B$14
Bags Produced Standard
Bags Produced Deluxe
$B$19
$C$14
Name
Final Value
Lagrange
Multiplier
Cell
Name
Final Value
0
459.7166
308.19838
0
$B$20
$B$21
Cutting and Dyeing Hours Used
Sewing Hours Used
Finishing Hours Used
Inspection and Packaging Hours Used
0
26.720587
0
123.02126
630
486.69028
665.18219
0
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
E
F
Local and Global Optima
A feasible solution is a local optimum if no other feasible solution with a better objective 
function value is found in the immediate neighborhood. For example, for the constrained 
Par, Inc. problem, the local optimum corresponds to a local maximum; a point is a local 
maximum if no other feasible solution with a larger objective function value is in the im-
mediate neighborhood. Similarly, for a minimization problem, a point is a local minimum 
if no other feasible solution with a smaller objective function value is in the immediate 
neighborhood.
Nonlinear optimization problems can have multiple local optimal solutions, which 
means we are concerned with finding the best of the local optimal solutions. A feasible 
solution is a global optimum if no other feasible point with a better objective function value 
is found in the feasible region. In the case of a maximization problem, the global optimum 
corresponds to a global maximum. A point is a global maximum if no other point in the 
feasible region gives a strictly larger objective function value. For a minimization problem, 
a point is a global minimum if no other feasible point with a strictly smaller objective 
function value is in the feasible region. A global maximum is also a local maximum, and a 
global minimum is also a local minimum.
Nonlinear problems with multiple local optima are difficult to solve. But in many non-
linear applications, a single local optimal solution is also the global optimal solution. For 
such problems, we need to find only a local optimal solution. We will now present some of 
the more common classes of nonlinear problems of this type.
Consider the function f (X, Y) 5 2X 2 2 Y 2. The shape of this function is illustrated in 
Figure 10.5. A function that is bowl-shaped down is called a concave function. The maxi-
mum value for this particular function is 0, and the point (0, 0) gives the optimal value of 
0. The point (0, 0) is a local maximum; but it is also a global maximum because no point 
gives a larger function value In other words no values of X and Y result in an objective
10.2
The neighborhood of a 
solution is a mathematical 
concept that refers to the 
set of points within a rela-
tively close proximity of the 
solution. See Figure 10.7 
for a graphical example of 
local minimums and local 
maximums.
All global optimal solutions 
are local optimal solutions, 
but not all local optimal so-
lutions are global optimal 
solutions.

456 
Chapter 10 Nonlinear Optimization Models
function value greater than 0. Functions that are concave, such as f (X, Y) 5 2X2 2 Y2, have 
a single local maximum that is also a global maximum. This type of nonlinear problem is 
relatively easy to maximize.
The objective function for the nonlinear Par, Inc. problem is an example of a concave 
function.
 
80S 2 1/15 S2 1 150D 2 1/5D2
In general, if all the squared terms in a quadratic function have a negative coefficient and 
there are no cross-product terms, such as xy (or for the Par, Inc. problem, SD), then the 
function is a concave quadratic function. Thus, for the Par, Inc. problem, we are assured 
that the local maximum identified by Excel Solver in Figure 10.3 is the global maximum.
Let us now consider another type of function with a single local optimum that is also 
a global optimum. Consider the function f (X, Y) 5 X2 1 Y2. The shape of this function is 
illustrated in Figure 10.6. It is bowl-shaped up and called a convex function. The minimum 
value for this particular function is 0, and the point (0, 0) gives the minimum value of 0. 
The point (0, 0) is a local minimum and a global minimum because no values of X and Y 
give an objective function value less than 0. Convex functions, such as f (X, Y) 5 X2 1 Y2, 
have a single local minimum and are relatively easy to minimize.
For a concave function, we can be assured that if our computer software finds a local 
maximum, it has found a global maximum. Similarly, for a convex function, we know that 
if our computer software finds a local minimum, it has found a global minimum. However, 
some nonlinear functions have multiple local optima. For example, Figure 10.7 shows the 
graph of the following function over the feasible regions: 0 # X # 1, 0 # Y # 1:
 
f (X,Y) 5 X sin(5πX) 1 Y sin(5πY)
Figure 10.5  A CONCAVE FUNCTION f (X, Y) 5 2X2 2 Y2
–4
–2
0
2
4
X
–4
–2
0
2
4
Y
–40
–20
0
Z
Figure 10.6  A CONVEX FUNCTION f (X, Y) 5 X2 1 Y2
–4
–2
0
2
4
X
–4
–2
0
2
4
Y
0
20
40
Z

 
10.2 Local and Global Optima 
457
where sin is the trigonometric sine function, and p is approximately 3.1416. The hills and val-
leys in this graph show that this function has a number of local maximums and local minimums.
From a technical standpoint, functions with multiple local optima pose a serious chal-
lenge for optimization software; most nonlinear optimization software methods can get 
stuck and terminate at a local optimum. Unfortunately, many applications can be nonlinear 
with multiple local optima, and the objective function value for a local optima may be 
much worse than the objective function value for a global optimum. Developing algorithms 
capable of finding the global optimum is currently an active research area. 
Next we discuss a very practical approach to dealing with local maximums and local 
minimums when using Excel Solver for nonlinear problems.
Overcoming Local Optima with Excel Solver
How do you know when multiple local optima exist? The mathematical ways to determine this 
are beyond the scope of this text. From a practical point of view, if the solution obtained by 
optimization software depends on the starting point, then there are multiple local optima. Thus, 
when using Excel Solver, if the solution returned from Solver is different when starting from 
different values in the decision variable cells, then there are local optima. The converse is not 
necessarily true; that is, if the same solution is returned when starting from a different set of 
starting points, this does not necessarily mean that you have found the global optimal solution.
Let us consider the problem shown in Figure 10.7:
 
Max f (X,Y) 5 Xsin(5πX) 1 Y sin(5πY)
s.t.
 0 # X # 1
 0 # Y # 1
file
WEB
LocalOptima
Figure 10.7  A FUNCTION WITH LOCAL MAXIMA AND MINIMA
–1.5
0.1
0
0.2
0.3
0.4
0.5 0.6 0.7
0.8
0.9
1
2.0
0.5
0
–0.5
–1.0
1.0
1.5
f (X, Y)
X
Y
0.1
0
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

458 
Chapter 10 Nonlinear Optimization Models
Table 10.1 shows the results returned from Excel Solver for different starting points 
( values in the decision variable cells when Solver is invoked). In each of the five cases in 
 Table 10.1, Solver returns with the message, “Solver has converged to the current solution. 
All constraints are satisfied.”
Excel Solver does provide an option that allows you to increase the confidence that 
you have found a global optimal solution. Clicking Options on the Solver Parameters 
dialog box and then selecting the GRG Nonlinear tab results in the dialog box shown in 
Figure 10.8. Clicking the Use Multistart option in the Multistart section causes Solver 
TABLe 10.1   SOLUTIONS FROM EXCEL SOLVER FOR A PROBLEM WITH 
 MULTIPLE LOCAL OPTIMA
Starting Point
Solution Returned
X
Y
X
Y
Objective  
Function Value 
0.000
1.000
0.000
0.500
1.000
0.000
0.000
1.000
0.500
1.000
0.129
0.905
0.000
0.508
0.905
0.129
0.000
0.905
0.508
0.905
0.231
0.902
0.902
1.008
1.805
Figure 10.8  THE GRG NONLINEAR TAB IN SOLVER OPTIONS

 
10.3 A Location Problem 
459
to use multiple starting solutions and report the best solution found from all of the start-
ing points. The Population Size is the number of starting points used. Solver selects 
starting points randomly using the Random Seed (an integer value) such that the points 
are within the bounds specified. Although providing simple lower and upper bounds 
is not required (unless the Require Bounds on the Variables option is selected), the 
procedure is much more effective when bounds are provided. We recommend selecting 
the Require Bounds on the Variables checkbox and providing bounds before you use 
the Multistart option.
In Figure 10.8, randomly generated starting points will be used and simple bounds of 
0 and 1 have been specified as constraints in the Solver dialog box. The result reported by 
Solver is X 5 0.90447, Y5 0.90447, with objective function 5 1.804. The message pro-
vided by Solver is, “Solver converged in probability to a global solution.”
If the solution to a problem 
appears to depend on the 
starting values for the deci-
sion variables, we recom-
mend you use the Multistart 
option.
NOTES AND COMMENTS
1. The Multistart option works best with bounds 
specified on each decision variable. It is often 
easy to calculate effective upper and lower 
bounds for the decision variables. For example, 
if you have a linear less-than-or-equal-to con-
straint with positive coefficients, upper bounds 
can be a calculated by simply dividing the right-
hand side by the coefficient for each variable. 
Using the cutting and dyeing constraint from the 
Par, Inc. problem, 7⁄10 S 1 1D # 630, we can 
deduce the following upper bounds: S # 630/
(7/10) 5 900 and D # 630/1 5 630.
2. In addition to GRG Nonlinear, Excel Solver 
provides another solution method, Evolutionary 
Solver, to solve nonlinear problems with local 
optimal solutions. Evolutionary Solver is based 
on a method that searches for an optimal solution 
by iteratively adjusting a population of candidate 
solutions. In this text, we limit our discussion for 
nonlinear problems to GRG Nonlinear, which is 
based on more classical optimization techniques. 
However, Evolutionary Solver may be useful 
for more complex nonlinear models that involve 
 Excel functions such as VLOOKUP and IF.
a Location problem
Let us consider the case of LaRosa Machine Shop (LMS). LMS is studying where to locate 
its tool bin facility on the shop floor. The locations of the five production stations appear 
in Figure 10.9. In an attempt to be fair to the workers in each of the production stations, 
management has decided to try to find the position of the tool bin that would minimize the 
sum of the distances from the tool bin to the five production stations. We define the fol-
lowing decision variables:
 
 X 5 horizontal location of the tool bin
 
 Y 5 vertical location of the tool bin
We may measure the distance from a station to the tool bin located at (X, Y) by using 
 Euclidean (straight-line) distance. For example, the distance from fabrication located at the 
coordinates (1, 4) to the tool bin located at the coordinates (X, Y) is given by
 
"(X 2 1) 2 1 (Y 2 4) 2.
The unconstrained optimization problem is as follows:
 Min    ("(X 2 1) 2 1 (Y 2 4) 2 1 "(X 2 1) 2 1 (Y 2 2) 2 1 "(X 2 2.5) 2 1 (Y 2 2) 2
 1 "(X 2 3) 2 1 (Y 2 5) 2 1 "(X 2 4) 2 1 (Y 2 4) 2
 )
Note that we do not require that the variables X or Y be nonnegative. The optimal solution 
found by Excel Solver is X 5 2.230, Y 53.349. The solution is shown in Figure 10.10.
10.3
file
WEB
LaRosa

460 
Chapter 10 Nonlinear Optimization Models
Figure 10.9  DATA FOR THE LMS TOOL BIN LOCATION PROBLEM
1
2
3
Fabrication
Paint
Subassembly 1
Subassembly 2
4
5
6
1
0
2
3
4
5
6
X
Assembly
Fabrication
Paint
Subassembly 1
Subassembly 2
Assembly
1
1
2.5
3
4
4
2
2
5
4
X
Y
Station
Location
Y
Figure 10.10  SOLUTION TO THE LMS TOOL BIN LOCATION PROBLEM
1
2
3
Fabrication
Paint
Subassembly 1
Subassembly 2
Tool Bin
4
5
6
1
0
2
3
4
5
6
X
Assembly
Fabrication
Paint
Subassembly 1
Subassembly 2
Assembly
1
1
2.5
3
4
4
2
2
5
4
X
Y
Station
Location
Y

 
10.4 Markowitz Portfolio Model 
461
Location models are used extensively for determining the optimal locations for ev-
erything from drilling holes in computer circuit boards to locating distribution centers and 
retail stores in supply chains. A variety of different location models can be created by using 
different objective functions or by adding additional contraints on distances traveled. The 
exercises at the end of this chapter provide practice in creating several different forms of 
location models.
Markowitz portfolio Model
Harry Markowitz received the 1990 Nobel Prize for his ground-breaking work in portfolio 
optimization. The Markowitz mean-variance portfolio model is a classic application of 
nonlinear programming. In this section, we present the Markowitz mean-variance port-
folio model. Money management firms throughout the world use numerous variations of 
this basic model.
A key trade-off in financial planning is that between risk and return. For a chance to 
earn greater returns, the investor must also accept greater risk. In most portfolio optimiza-
tion models, the return used is the expected (or average) return of the possible outcomes, 
and the risk is some measure of variability in these possible outcomes. To illustrate the 
Markowitz portfolio model, let us consider the case of Hauck Investment Services.
Hauck Investment Services designs annuities, IRAs, 401(k) plans, and other invest-
ment vehicles for investors with a variety of risk tolerances. Hauck would like to develop 
a portfolio model that can be used to determine an optimal portfolio involving a mix of six 
mutual funds. Table 10.2 shows the annual return (percent) for five 1-year periods for the 
six mutual funds. Year 1 represents a year in which all mutual funds yield good returns. 
Year 2 is also a good year for most of the mutual funds. But year 3 is a bad year for the 
small-cap value fund, year 4 is a bad year for the intermediate-term bond fund, and year 5 
is a bad year for four of the six mutual funds.
It is not possible to predict the exact returns for any of the funds over the next 12 
months, but the portfolio managers at Hauck Financial Services think that the returns for the 
five years shown in Table 10.2 are scenarios that can be used to represent the possibilities 
for the next year. For the purpose of building portfolios for their clients, Hauck’s portfolio 
managers will choose a mix of these six mutual funds and assume that one of the five pos-
sible scenarios will describe the return over the next 12 months.
The portfolio construction problem is to determine how much of the portfolio to invest 
in each investment alternative. To determine the proportion of the portfolio that will be 
10.4
TABLe 10.2   MUTUAL FUND PERFORMANCES IN FIVE SELECTED YEARS 
(USED AS PLANNING SCENARIOS FOR THE NEXT 12 MONTHS)
Annual Return (%)
Mutual Fund
Year 1
Year 2
Year 3
Year 4
Year 5
Foreign Stock
Intermediate-Term Bond
Large-Cap Growth
Large-Cap Value
Small-Cap Growth
Small-Cap Value
10.06
17.64
32.41
32.36
33.44
24.56
13.12
 3.25
18.71
20.61
19.40
25.32
13.47
 7.51
33.28
12.93
 3.85
26.70
45.42
21.33
41.46
 7.06
58.68
 5.43
221.93
 7.36
223.26
25.37
29.02
17.31

462 
Chapter 10 Nonlinear Optimization Models
invested in each of the mutual funds we use the following decision variables:
 
 FS 5 proportion of portfolio invested in the foreign stock mutual fund
 
 IB 5 proportion of portfolio invested in the intermediate-term bond fund
 
LG 5 proportion of portfolio invested in the large-cap growth fund
 
 LV 5 proportion of portfolio invested in the large-cap value fund
 
 SG 5 proportion of portfolio invested in the small-cap growth fund
 
 SV 5 proportion of portfolio invested in the small-cap value fund
Because the sum of these proportions must equal one, we need the following constraint:
 
FS 1 IB 1 LG 1 LV 1 SG 1 SV 5 1
The other constraints are concerned with the return that the portfolio will earn under each 
of the planning scenarios in Table 10.2.
The portfolio return over the next 12 months depends on which of the possible scenarios 
(years 1 through 5) in Table 10.2 occurs. Let R1 denote the portfolio return if the scenario 
represented by year 1 occurs, R2 denote the portfolio return if the scenario represented by 
year 2 occurs, and so on. The portfolio returns for the five planning years (scenarios) are 
as follows:
Scenario 1 return:
 
R1 5 10.06FS 1 17.64IB 1 32.41LG 1 32.36LV 1 33.44SG 1 24.56SV
Scenario 2 return:
 
R2 5 13.12FS 1 3.25IB 1 18.71LG 1 20.61LV 1 19.40SG 1 25.32SV
Scenario 3 return:
 
R3 5 13.47FS 1 7.51IB 1 33.28LG 1 12.93LV 1 3.85SG 2 6.70SV
Scenario 4 return:
 
R4 5 45.42FS 2 1.33IB 1 41.46LG 1 7.06LV 1 58.68SG 1 5.43SV
Scenario 5 return:
 
R5 5 221.93FS 1 7.36IB 2 23.26LG 2 5.37LV 2 9.02SG 1 17.31SV
If ps is the probability of scenario s, among n possible scenarios, then the expected return 
for the portfolio is R where
 
R 5 a
n
s51
psRs 
(10.6)
If we assume that the five planning scenarios in the Hauck Financial Services model are 
equally likely to occur, then
 
R 5 a
5
s51
 1⁄5 Rs 5 1⁄5 a
5
s51
Rs
Measuring risk is a bit more difficult. Entire books are devoted to the topic of risk mea-
surement. The measure of risk most often associated with the Markowitz portfolio model 
is the variance of the portfolio’s return. If the expected return is defined by equation (10.6), 
the variance of the portfolio’s return is
 
Var 5 a
n
s51
ps(Rs 2 R) 2 
(10.7)

 
10.4 Markowitz Portfolio Model 
463
For the Hauck Financial Services example, the five planning scenarios are equally likely, thus
 
Var 5 a
n
s51
 1⁄5 (Rs 2 R) 2 5 1⁄5 a
n
s51
(Rs 2 R) 2
The portfolio variance is the average of the sum of the squares of the deviations from 
the mean value under each scenario. The larger this number, the more widely dispersed the 
scenario returns are about the average value. If the portfolio variance were equal to zero, 
then every scenario return Ri would be equal, and there would be no risk.
Two basic ways to formulate the Markowitz model are (1) to minimize the variance 
of the portfolio subject to a constraint on the expected return of the portfolio and (2) to 
maximize the expected return of the portfolio subject to a constraint on variance. Consider 
the first case. Assume that Hauck clients would like to construct a portfolio from the six 
mutual funds listed in Table 10.2 that will minimize their risk as measured by the portfolio 
variance. However, the clients also require the expected portfolio return to be at least 10 
percent. In our notation, the objective function is
 
Min 
1⁄5 a
n
s51
(Rs 2 R) 2
The constraint on expected portfolio return is R $ 10. The complete Markowitz model 
involves 12 variables and 8 constraints (excluding the nonnegativity constraints).
Min 1⁄5 a
5
s51
(Rs 2 R) 2
(10.8)
s.t.
10.06FS 1 17.64IB 1 32.41LG 1 32.36LV 1 33.44SG 1 24.56SV 5 R1
(10.9)
13.12FS 1
3.25IB 1 18.71LG 1 20.61LV 1 19.40SG 1 25.32SV 5 R2
(10.10)
13.47FS 1
7.51IB 1 33.28LG 1 12.93LV 1
3.85SG 2
6.70SV 5 R3
(10.11)
45.42FS 2
1.33IB 1 41.46LG 1
7.06LV 1 58.68SG 1
5.43SV 5 R4
(10.12)
221.93FS 1
7.36IB 2 23.26LG 2
5.37LV 2
9.02SG 1 17.31SV 5 R5
(10.13)
FS 1
IB 1
LG 1
LV 1
SG 1
SV 5 1
(10.14)
1⁄5 a
5
s51
Rs 5
R
(10.15)
R $ 10
(10.16)
FS, IB, LG, LV, SG, SV $ 0
(10.17)
The objective for the Markowitz model is to minimize portfolio variance. Equations 
(10.9) through (10.13) define the return for each scenario. Equation (10.14) requires all of 
the money to be invested in the mutual funds; this constraint is often called the unity con-
straint. Equation (10.15) defines R, which is the expected return of the portfolio. Equation 
(10.16) requires the portfolio return to be at least 10 percent. Finally, expression (10.17) 
requires a nonnegative investment in each Hauck mutual fund. Note that R1, R2, R3, R4 and 
R5, as well as R are not required to be nonnegative. It is possible that the return in a given 
scenario or the expected return of the portfolio is negative.
The solution for this model using a required return of at least 10 percent appears in 
Figure 10.11. The minimum value for the portfolio variance is 27.136. This solution implies 
that the clients will get an expected return of 10 percent (R 5 10.) and minimize their risk 
as measured by portfolio variance by investing approximately 16 percent of the portfolio 
in the foreign stock fund (FS 5 0.158), 53 percent in the intermediate bond fund (IB 5 
0.525), 4 percent in the large-cap growth fund (LG 5 0.042), and 27 percent in the small-
cap value fund (SV 5 0.274).

464 
Chapter 10 Nonlinear Optimization Models
Figure 10.11   SOLUTION FOR THE HAUCK MINIMUM VARIANCE PORTFOLIO WITH A REQUIRED 
RETURN OF AT LEAST 10 PERCENT
file
WEB
HauckMarkowitz
The Solver Parameters dialog box is also shown in Figure 10.11. Note that we have 
selected GRG Nonlinear as the method and we have not selected Make Unconstrained 
Variables Non-Negative. Instead we have entered as an explicit constraint set that B17 
through B22 must be $ 0.
The Markowitz portfolio model provides a convenient way for an investor to trade 
off risk versus return. In practice, this model is typically solved iteratively for dif-
ferent values of return. Figure 10.12 is a graph of the minimum portfolio variances 
Figure 10.12   AN EFFICIENT FRONTIER FOR THE MARKOWITZ PORTFOLIO MODEL
15
20
25
30
35
40
45
8
9
10
11
12
Required Return (%)
Portfolio Variance

 
10.5 Forecasting Adoption of a New Product 
465
NOTES AND COMMENTS
1. Notice that the solution given in Figure 10.11 
has more than 50 percent of the portfolio in-
vested in the intermediate-term bond fund. It 
may be unwise to let one asset contribute so 
heavily to the portfolio. Upper and lower bounds 
on the amount of an asset type in the portfolio 
can be easily modeled. Hence upper bounds are 
often placed on the percentage of the portfolio 
invested in a single asset. Likewise, it might be 
undesirable to include an extremely small quan-
tity of an asset in the portfolio. Thus, there may 
be constraints that require nonzero amounts of 
an asset to be at least a minimum percentage of 
the portfolio.
2. In the Hauck example, 100 percent of the avail-
able portfolio was invested in mutual funds. 
However, risk-averse investors often prefer to 
have some of their money in a so-called risk-free 
asset, such as U.S. Treasury bills. Thus, many 
portfolio optimization models allow funds to be 
invested in a risk-free asset.
3. In this section, portfolio variance was used to 
measure risk. However, variance, as it is de-
fined, counts deviations both above and below 
the mean. Most investors are happy with returns 
above the mean but wish to avoid returns below 
the mean. Hence, numerous portfolio models al-
low for flexible risk measures. A problem at the 
end of this chapter illustrates the use of alterna-
tive risk measures.
4. In practice, both brokers and mutual fund com-
panies adjust portfolios as new information be-
comes available. However, constantly adjusting a 
portfolio may lead to large transaction costs. The 
case problem at the end of this chapter requires 
you to develop a modification of the Markowitz 
portfolio selection problem to account for trans-
action costs.
Forecasting adoption of a New product
Forecasting new adoptions after a product introduction is an important marketing problem. 
In this section, we introduce a forecasting model developed by Frank Bass1 that has proven 
to be particularly effective in forecasting the adoption of innovative and new technologies 
in the marketplace. Nonlinear optimization is used to estimate the parameters of the Bass 
forecasting model. The model has three parameters that must be estimated.
 
m 5 the number of people estimated to eventually adopt the new product
A company introducing a new product is obviously interested in the value of parameter m.
 
q 5 the coefficient of imitation
Parameter q measures the likelihood of adoption due to a potential adopter being influenced 
by someone who has already adopted the product. It measures the word-of-mouth or social 
media effect influencing purchases.
 
p 5 the coefficient of innovation
10.5
1See Frank M. Bass, “A New Product Growth Model for Consumer Durables,” Management Science 15 (1969).
versus required expected returns as required expected return is varied from 8 percent 
to 12 percent in increments of 1 percent. In finance, this graph is called the efficient 
frontier. Each point on the efficient frontier is the minimum possible risk (measured 
by portfolio variance) for the given return. By looking at the graph of the efficient 
frontier, investors can select the mean-variance combination with which they are most 
comfortable.

466 
Chapter 10 Nonlinear Optimization Models
Parameter p measures the likelihood of adoption, assuming no influence from someone who 
has already purchased (adopted) the product. It is the likelihood of someone adopting the 
product due to her or his own interest in the innovation.
Using these parameters, let us now develop the forecasting model. Let Ct21 denote 
the number of people who have adopted the product through time t 2 1. Because m is the 
number of people estimated to eventually adopt the product, m 2 Ct21 is the number of 
potential adopters remaining at time t 2 1. We refer to the time interval between time t 2 
1 and time t as time period t. During period t, some percentage of the remaining number of 
potential adopters, m 2 Ct21, will adopt the product. This value depends on the likelihood 
of a new adoption.
Loosely speaking, the likelihood of a new adoption is the likelihood of adoption due 
to imitation plus the likelihood of adoption due to innovation. The likelihood of adop-
tion due to imitation is a function of the number of people who have already adopted 
the product. The larger the current pool of adopters, the greater their influence through 
word of mouth. Because Ct21ym is the fraction of the number of people estimated to 
adopt the product by time t 2 1, the likelihood of adoption due to imitation is computed 
by multiplying this fraction by q, the coefficient of imitation. Thus, the likelihood of 
adoption due to imitation is
 
q(Ct21 ym)
The likelihood of adoption due to innovation is simply p, the coefficient of innovation. 
Thus, the likelihood of adoption is
 
p 1 q(Ct21 ym)
Using the likelihood of adoption we can develop a forecast of the remaining number of 
potential customers that will adopt the product during time period t. Thus, Ft, the forecast 
of the number of new adopters during time period t, is
 
Ft 5 (p 1 q[Ct21 ym] ) ( m 2 Ct21) 
(10.18)
In developing a forecast of new adoptions in period t using the Bass model, the value 
of Ct21 will be known from past sales data. But we also need to know the values of the pa-
rameters to use in the model. Let us now see how nonlinear optimization is used to estimate 
the parameter values m, p, and q.
Consider Figure 10.13. This figure shows the graph of box office revenues (in $ mil-
lions) for two different films, an independent studio film and a summer blockbuster action 
movie, over the first 12 weeks after release. Strictly speaking, box office revenues for time 
period t are not the same as the number of adopters during time period t. However, the 
number of repeat customers is usually small, and box office revenues are a multiple of the 
number of moviegoers. The Bass forecasting model seems appropriate here.
These two films illustrate drastically different adoption patterns. Note that revenues for 
the independent studio film grow until the revenues peak in week four and then decline. For 
this film, much of the revenue is obviously due to word-of-mouth influence. In terms of 
the Bass model, the imitation factor dominates the innovation factor, and we expect q . p. 
However, for the summer blockbuster, revenues peak in week 1 and drop sharply afterward. 
The innovation factor dominates the imitation factor, and we expect q , p.
The forecasting model given in equation (10.18) can be incorporated into a nonlinear 
optimization problem to find the values of p, q, and m that give the best forecasts for a set 
of data. Assume that N periods of data are available. Let us denote the actual number of 
adopters (or a multiple of that number, such as sales) in period t as Ct for t 5 1, …, N. Then 
the forecast in each period and the corresponding forecast error Et is defined by
 
Ft 5 (p 1 q[Ct21ym])(m 2 Ct21) and Et 5 Ft 2 Ct
The Bass forecasting model 
given in equation (10.18) 
can be rigorously derived 
from statistical principles. 
Rather than providing 
such a derivation, we have 
emphasized the intuitive 
aspects of the model.

 
10.5 Forecasting Adoption of a New Product 
467
Notice that the forecast error is the difference between the forecast value Ft and the actual 
value Ct. It is common statistical practice to estimate the parameters p, q, and m by mini-
mizing the sum of squared errors.
Doing so for the Bass forecasting model leads to the following nonlinear optimization 
problem:
 
Min a
N
t51
E2
t 
(10.19)
 
s.t.
 
 
Ft 5 (p 1 q3Ct21/m4) (m 2 Ct21)         t 5 1, 2, . . . N 
(10.20)
 
Et 5 Ft 2 Ct 
 t 5 1, 2, . . . N 
(10.21)
Because equations (10.19) and (10.20) both contain nonlinear terms, this model is a non-
linear minimization problem.
The data in Table 10.3 provide the revenue and cumulative revenues for the inde-
pendent studio film in weeks 1–12. Using these data, the nonlinear model to estimate the 
parameters of the Bass forecasting model for the independent studio film follows:
 
Min    E1
2 1  E2
2 1 c1  E12
2
 
 s.t.    F1 5 (p)m
 
 F2 5 3p 1 q(0.10/m) 4 (m 2 0.10)
 
 F3 5 3p 1 q(3.10/m) 4 (m 2 3.10)
 
(
 
 F12 5 3p 1 q(34.85/m) 4 (m 2 34.85)
 
 E1 5 F1 2 0.10
 
 E2 5 F2 2 3.00
 
(
 
E12 5 F12 2 0.60
Note that the parameters of 
the Bass forecasting model 
are the decision variables 
in this nonlinear optimiza-
tion model.
Figure 10.13   WEEKLY BOX OFFICE REVENUES FOR AN INDEPENDENT  STUDIO FILM AND A 
 SUMMER BLOCKBUSTER MOVIE
1
2
3
4
5
6
7
8
1
0
2
3
4
5
6
7
8
9
10 11 12
Week
Independent Studio Film
Revenue ($ millions)
 
10
20
30
40
50
60
70
80
1
2
3
4
5
6
7
8
9
10 11 12
Week
Summer Blockbuster
Revenue ($ millions)
0

468 
Chapter 10 Nonlinear Optimization Models
The solutions to this nonlinear model and to a similar nonlinear model for the summer 
blockbuster are given in Table 10.4.
The optimal forecasting parameter values given in Table 10.4 are intuitively appealing 
and consistent with Figure 10.13. For the independent studio film, which has the largest 
revenues in week 4, the value of the imitation parameter q is 0.49; this value is substan-
tially larger than the innovation parameter p 5 0.074. The film picks up momentum over 
time due to favorable word of mouth. After week 4, revenues decline as more and more of 
the potential market for the film has already seen it. Contrast these data with the summer 
blockbuster movie, which has a negative value of 20.018 for the imitation parameter q 
and an innovation parameter p of 0.49. The greatest number of adoptions is in week 1, and 
new adoptions decline afterward. Obviously the word-of-mouth influence is not favorable.
In Figure 10.14, we show the forecast values based on the parameters in Table 10.4 
and the observed values in the same graph. The Bass forecasting model does a good job of 
tracking revenue for the independent small-studio film. For the summer blockbuster, the 
Bass model does an outstanding job; it is virtually impossible to distinguish the forecast 
line from the actual adoption line.
You may wonder what good a forecasting model is if we must wait until after the 
adoption cycle is complete to estimate the parameters. One way to use the Bass forecasting 
model for a new product is to assume that sales of the new product will behave in a way that 
is similar to a previous product for which p and q have been calculated and to subjectively 
estimate m, the potential market for the new product. For example, one might assume that 
box office receipts for movies next summer will behave similarly to box office receipts for 
movies last summer. Then the p and q used for next summer’s movies would be the p and 
q values calculated from the actual box office receipts last summer.
TABLe 10.3   BOX OFFICE REVENUES AND CUMULATIVE REVENUES IN 
$  MILLIONS FOR INDEPENDENT STUDIO FILM
Week
Revenues St
Cumulative Revenues Ct
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
0.10
3.00
5.20
7.00
5.25
4.90
3.00
2.40
1.90
1.30
0.80
0.60
 0.10
 3.10
 8.30
15.30
20.55
25.45
28.45
30.85
32.75
34.05
34.85
35.45
file
WEB
Bass
TABLe 10.4   OPTIMAL FORECAST PARAMETERS FOR INDEPENDENT STUDIO 
FILM AND SUMMER BLOCKBUSTER MOVIE
Parameter
Independent Studio Film
Summer Blockbuster
p
q
m
 0.074
 0.490
34.850
0.460
20.018
149.540

 
Summary 
469
A second approach is to wait until several periods of data for the new product are avail-
able. For example, if five periods of data are available, the sales data for these five periods 
could be used to forecast demand for period 6. Then, after six periods of sales are observed, 
a forecast for period 7 is made. This method is often called a rolling-horizon approach. 
Figure 10.14   FORECAST AND ACTUAL WEEKLY BOX OFFICE  REVENUES FOR INDEPENDENT 
 STUDIO FILM AND SUMMER  BLOCKBUSTER
1
2
3
4
5
Forecast values
Observed values
6
7
8
1
2
3
4
5
6
7
8
9
10 11 12
Week
Independent Studio Film
Revenue and Forecast ($ millions)
0
 
10
20
30
40
50
60
70
80
1
2
3
4
5
6
7
8
9
10 11 12
Week
Revenue and Forecast ($ millions)
Forecast values
Observed values
0
Summer Blockbuster
NOTES AND COMMENTS
The optimization model used to determine the pa-
rameter values for the Bass forecasting model is 
an example of a difficult nonlinear optimization 
problem. It is neither convex nor concave. For such 
models, local optima may give values that are much 
worse than the global optimum. We recommend 
using the Multistart option in Excel Solver when 
solving such  problems.
Summary
In this chapter we introduced nonlinear optimization models. A nonlinear optimization 
model is a model with at least one nonlinear term in either a constraint or the objective 
function. Because so many applications of business analytics involve nonlinear functions, 
allowing nonlinear terms greatly increases the number of important applications that can 
be modeled as an optimization problem. Numerous problems in portfolio optimization, 
option pricing, marketing, economics, facility location, forecasting, and scheduling lend 
themselves to nonlinear models.
Unfortunately, nonlinear optimization models are not as easy to solve as linear optimi-
zation models, or even integer linear optimization models. As a rule of thumb, if a problem 
can be modeled realistically as a linear or integer linear problem, then it is probably best 
to do so. Many nonlinear formulations have local optima that are not globally optimal. Be-
cause most nonlinear optimization software terminates with a local optimum, the solution 
returned by the software may not be the best solution available. However, as discussed in 
this chapter, numerous important classes of optimization problems, such as the Markowitz 

470 
Chapter 10 Nonlinear Optimization Models
portfolio models, are convex optimization problems. For a convex optimization problem, 
a local optimum is also the global optimum. Additionally, the development of software for 
solving (nonconvex) nonlinear optimization problems that find globally optimal solutions 
is proceeding at a rapid rate. When using Excel Solver for nonlinear optimization, we rec-
ommend using the Multistart option.
Glossary
Nonlinear optimization problem An optimization problem that contains at least one non-
linear term in the objective function or a constraint.
Quadratic function A nonlinear function with term to the power of two.
Reduced gradient Value associated with a variable in a nonlinear model that is analogous 
to the reduced cost in a linear model; the shadow price of a binding simple lower or upper 
bound on the decision variable.
Lagrangian multiplier The shadow price for a constraint in a nonlinear problem, that 
is, the rate of change of the objective function with respect to the right-hand side of a 
 constraint.
Local optimum A feasible solution is a local optimum if there are no other feasible solu-
tions with a better objective function value in the immediate neighborhood. A local opti-
mum may be either a local maximum or a local minimum.
Local maximum A feasible solution is a local maximum if there are no other feasible solu-
tions with a larger objective function value in the immediate neighborhood.
Local minimum A feasible solution is a local minimum if there are no other feasible solu-
tions with a smaller objective function value in the immediate neighborhood.
Global optimum A feasible solution is a global optimum if there are no other feasible 
points with a better objective function value in the entire feasible region. A global optimum 
may be either a global maximum or a global minimum.
Global maximum A feasible solution is a global maximum if there are no other feasible 
points with a larger objective function value in the entire feasible region. A global maxi-
mum is also a local maximum.
Global minimum A feasible solution is a global minimum if there are no other feasible 
points with a smaller objective function value in the entire feasible region. A global mini-
mum is also a local minimum.
Concave function A function that is bowl-shaped down: For example, the functions  
f(x) 5 25x2 2 5x and f(x, y) 5 2x2 2 11y2 are concave functions.
Convex function A function that is bowl-shaped up: For example, the functions f(x) 5 
x2 2 5x and f(x, y) 5 x2 1 5y2 are convex functions.
Markowitz mean-variance portfolio model A portfolio optimization model used to con-
struct a portfolio that minimizes risk subject to a constraint requiring a minimum level of 
return.
Efficient frontier A set of points defining the minimum possible risk (measured by port-
folio variance) for a set of return values.
Problems
 1. GreenLawns provides a lawn fertilizer and weed control service. The company is adding a 
special aeration treatment as a low-cost extra service option that it hopes will help attract 
new customers. Management is planning to promote this new service in two media: radio 
and direct-mail advertising. A media budget of $3,000 is available for this promotional 
campaign. Based on past experience in promoting its other services, GreenLawns has 

 
Problems 
471
obtained the following estimate of the relationship between sales and the amount spent on 
promotion in these two media.
 
S 5 22R2 2 10M2 2 8RM 1 18R 1 34M
 
 where
 
 S 5 total sales in thousands of dollars
 
 R 5 thousands of dollars spent on radio advertising
 
M 5 thousands of dollars spent on direct-mail advertising
 
 GreenLawns would like to develop a promotional strategy that will lead to maximum sales 
subject to the restriction provided by the media budget.
a. What is the value of sales if $2,000 is spent on radio advertising and $1,000 is spent 
on direct-mail advertising?
b. Formulate an optimization problem that can be solved to maximize sales subject to the 
media budget of spending no more than $3,000 on total advertising.
c. Determine the optimal amount to spend on radio and direct-mail advertising. How 
much in sales will be generated?
 2. The Cobb-Douglas production function is a classic model from economics used to model 
output as a function of capital and labor. It has the form
 
f(L,C) 5 c0 L c1C c2
 
 where c0, c1, and c2 are constants. The variable L represents the units of input of labor, and 
the variable C represents the units of input of capital.
a. In this example, assume c0 5 5, c1 5 0.25, and c2 5 0.75. Assume each unit of labor 
costs $25 and each unit of capital costs $75. With $75,000 available in the budget, 
develop an optimization model to determine how the budgeted amount should be al-
located between capital and labor in order to maximize output.
b. Find the optimal solution to the model you formulated in part a. (Hint: When using 
Excel Solver, use the Multistart option with bounds 0 # L # 3000 and 0 # C # 1000.)
 3. Let S represent the amount of steel produced (in tons). Steel production is related to the 
amount of labor used (L) and the amount of capital used (C) by the following  function:
 
S 5 20 L0.30 C 0.70
 
 In this formula L represents the units of labor input and C the units of capital input. Each 
unit of labor costs $50, and each unit of capital costs $100.
a. Formulate an optimization problem that will determine how much labor and capital 
are needed to produce 50,000 tons of steel at minimum cost.
b. Solve the optimization problem you formulated in part a. (Hint: When using Excel 
Solver, start with an initial L . 0 and C . 0.)
 4. The profit function for two products is:
 
Profit 5 23x1
2 1 42 x1 2 3x2
2 1 48x2 1 700
 
 where x1 represents units of production of product 1, and x2 represents units of production 
of product 2. 
 
 Producing one unit of product 1 requires 4 labor-hours, and producing one unit of product 
2 requires 6 labor-hours. Currently, 24 labor-hours are available. The cost of labor-hours 
is already factored into the profit function, but it is possible to schedule overtime at a pre-
mium of $5 per hour.
a. Formulate an optimization problem that can be used to find the optimal production 
quantity of products 1 and 2 and the optimal number of overtime hours to schedule.
b. Solve the optimization model you formulated in part a. How much should be produced 
and how many overtime hours should be scheduled?

472 
Chapter 10 Nonlinear Optimization Models
 5. Jim’s Camera shop sells two high-end cameras, the Sky Eagle and Horizon. The demand 
for these two cameras are as follows: DS 5 demand for the Sky Eagle, PS is the selling 
price of the Sky Eagle, DH is the demand for the Horizon, and PH is the selling price of the 
Horizon.
 
 DS 5 222 2 0.60PS 1 0.35PH
 
DH 5 270 1 0.10PS 2 0.64PH
 
 The store wishes to determine the selling price that maximizes revenue for these two prod-
ucts. Develop the revenue function for these two models, and find the revenue maximizing 
prices.
 6. Heller Manufacturing has two production facilities that manufacture baseball gloves. Pro-
duction costs at the two facilities differ because of varying labor rates, local property 
taxes, type of equipment, capacity, and so on. The Dayton plant has weekly costs that can 
be expressed as a function of the number of gloves produced:
 
TCD(X) 5 X2 2 X 1 5
 
 where X is the weekly production volume in thousands of units, and TCD(X) is the cost in 
thousands of dollars. The Hamilton plant’s weekly production costs are given by
 
TCH(Y) 5 Y2 1 2Y 1 3
 
 where Y is the weekly production volume in thousands of units, and TCH(Y) is the cost in 
thousands of dollars. Heller Manufacturing would like to produce 8000 gloves per week 
at the lowest possible cost.
a. Formulate a mathematical model that can be used to determine the optimal number of 
gloves to produce each week at each facility.
b. Solve the optimization model to determine the optimal number of gloves to produce 
at each facility.
 7. Many forecasting models use parameters that are estimated using nonlinear optimiza-
tion. A good example is the Bass model introduced in this chapter. Another example is 
the exponential smoothing forecasting model discussed in Chapter 5. The exponential 
smoothing model is common in practice and is described in further detail in Chapter 5. For 
instance, the basic exponential smoothing model for forecasting sales is
 
y^t11 5 α yt 1 (1 2 α)y^t
 
 where
 
  
y^t11 5 forecast of sales for period t 1 1
 
  
yt 5 actual sales for period t
 
  
y^t 5 forecast of sales for period t
 
  
α 5 smoothing constant, 0 # α # 1
 
 This model is used recursively; the forecast for time period t 1 1 is based on the forecast 
for period t, y^t, the observed value of sales in period t, yt, and the smoothing parameter α. 
The use of this model to forecast sales for 12 months is illustrated in the following table 
with the smoothing constant α 5 0.3. The forecast errors, yt 2 y^t, are calculated in the 
fourth column. The value of α is often chosen by minimizing the sum of squared forecast 
errors. The last column of the table shows the square of the forecast error and the sum of 
squared forecast errors.
 
  In using exponential smoothing models, one tries to choose the value of α that provides 
the best forecasts.
a. The file ExpSmooth contains the observed data shown here. Construct this table using 
the formula above. Note that we set the forecast in period 1 to the observed in period 1 
to get started (y^1 5 y1 5 17) then the formula above for y^t11 is used starting in pe-
riod 2. Make sure to have a single cell corresponding to α in your spreadsheet model. 
file
WEB
ExpSmooth

 
Problems 
473
After confirming the values in the table below with α 5 0.3, try different values of α 
to see if you can get a smaller sum of squared forecast errors.
b. Use Excel Solver to find the value of α that minimizes the sum of squared forecast errors.
Weet
(t)
Observed 
Value
(yt)
Forecast
(y^ t)
Forecast 
Error
(yt 2 y^ t)
Squared  
Forecast Error
(yt 2 y^ t)2
 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
17
21
19
23
18
16
20
18
22
20
15
22
17.00
17.00
18.20
18.44
19.81
19.27
18.29
18.80
18.56
19.59
19.71
18.30
0.00
4.00
0.80
4.56
21.81
23.27
1.71
20.80
3.44
0.41
24.71
3.70
0.00
16.00
0.64
20.79
3.27
10.66
2.94
0.64
11.83
0.17
22.23
13.69
SUM 5 102.86
 8. Andalus Furniture Company has two manufacturing plants, one at Aynor and another at 
Spartanburg. The cost in dollars of producing a kitchen chair at each of the two plants is 
given here. The cost of producing Q1 chairs at Aynor is:
 
75Q1 1 5Q1 21 100
 
 and the cost of producing Q2 kitchen chairs at Spartanburg is
 
25Q2 1 2.5Q2 21 150
 
 Andalus needs to manufacture a total of 40 kitchen chairs to meet an order just received. 
How many chairs should be made at Aynor, and how many should be made at Spartanburg 
in order to minimize total production cost?
 9. The economic order quantity (EOQ) model is a classical model used for controlling in-
ventory and satisfying demand. Costs included in the model are holding cost per unit, 
ordering cost, and the cost of goods ordered. The assumptions for that model are that only 
a single item is considered, that the entire quantity ordered arrives at one time, that the 
demand for the item is constant over time, and that no shortages are allowed.
 
  Suppose we relax the first assumption and allow for multiple items that are independent 
except for a budget restriction. The following model describes this situation:
 
 Let Dj 5 annual demand for item j
 
  
Cj 5 unit cost of item j
 
  
Sj 5 cost per order placed for item j
 
  
i 5 inventory carrying charge as a percentage of the cost per unit
 
  
B 5 the maximum amount of investment in goods 
 
  
N 5 number of items
 
 The decision variables are Qj, the amount of item j to order. The model is:
 
Minimize  a
N
j51
cCjDj 1
SjDj
Qj
1 iCj
Qj
2 d
 
s.t.
 
 
 a
N
j51
CjQj # B
 
 Qj $ 0 j 5 1, 2, . . . , N

474 
Chapter 10 Nonlinear Optimization Models
 
  In the objective function, the first term is the annual cost of goods, the second is the 
annual ordering cost (DjyQj is the number of orders), and the last term is the annual inven-
tory holding cost (Qjy2 is the average amount of inventory).
a. Set up a spreadsheet model and for the following data:
Item 1
Item 2
Item 3
Annual Demand
Item Cost
Order Cost
B 5 $20,000
i 5 0.20
2000
$100
$150
2000
$50
$135
1000
$80
$125
b. Solve the problem using Excel Solver. (Hint: For Solver to find a solution, you need 
to start with decision variable values that are greater than 0.)
 10. Phillips Inc. produces two distinct products, A and B. The products do not compete 
with each other in the marketplace; that is, neither cost, price, nor demand for one 
product will impact the demand for the other. Phillips’ analysts have collected data on 
the effects of advertising on profits. These data suggest that, although higher advertis-
ing correlates with higher profits, the marginal increase in profits diminishes at higher 
advertising levels, particularly for Product B. Analysts have estimated the following 
functions:
 
Annual profit for product A 5 1.2712 LN(XA) 1 17.414
 
Annual profit for product B 5 0.3970 LN(XB) 1 16.109
 
 where XA and XB are the advertising amount allocated to products A and B, respectively, 
in thousands of dollars, profit is in millions of dollars, and LN is the natural logarithm 
function. The advertising budget is $500,000, and management has dictated that at least 
$50,000 must be allocated to each of the two products.
 
 (Hint: To compute a natural logarithm for the value X in Excel, use the formula 5LN(X). 
For Solver to find an answer, you also need to start with decision variable values greater 
than 0 in this problem.)
a. Build an optimization model that will prescribe how Phillips should allocate its mar-
keting budget to maximize profit.
b. Solve the model you constructed in part a using Excel Solver.
 11. Let us consider again the data from the LaRosa tool bin location problem discussed in 
Section 10.3.
a. Suppose we know the average number of daily trips made to the tool bin from 
each production station. It seems as though we would want the tool bin closer to 
those stations with high numbers of average trips. Develop a new unconstrained 
model that minimizes the sum of the demand-weighted distance defined as the 
product of the demand (measured in number of trips) and the distance to the 
 station.
b. Solve the model you developed in part a. Comment on the differences between 
the unweighted distance solution given in Section10.3 and the demand-weighted 
solution.
 12. TN Communications provides cellular telephone services. The company is planning to 
expand into the Cincinnati area and is trying to determine the best location for its trans-
file
WEB
LaRosaDemand

 
Problems 
475
mission tower. The tower transmits over a radius of 10 miles. The locations that must be 
reached by this tower are shown in the following figure.
 
 
5
10
15
20
25
5
0
10
15
20
x
y
Hyde Park
Evendale
TN Locations
Covington
Florence
Florence
Covington
Hyde Park
Evendale
10
12
16
12
10
16
18
22
x
y
 
  TN Communications would like to find the tower location that reaches each of these 
cities and minimizes the sum of the distances to all locations from the new tower.
a. Formulate a model to find the optimal location.
b. Formulate and solve a model that minimizes the maximum distance from the transmis-
sion tower location to the city locations.
 13. The distance between two cities in the United States can be approximated by the following 
formula where lat1 and long1 are the latitude and longitude of city 1 and lat2 and long2 are 
the latitude and longitude of city 2:
 
69"(lat1 2 lat2)2 1 (long1 2 long2)2
 
 Ted’s daughter is getting married, and he is inviting relatives from 15 different locations 
in the United States. The file Wedding gives the longitude, latitude, and number of rela-
tives in each of the 15 locations. Ted would like to find the location to hold the wedding 
that minimizes the demand-weighted distance, where demand is the number of relatives at 
each location. Find the optimal location. (Hint: Notice that all longitude values given for 
this problem are negative. Make sure that you do not check the option for Make Uncon-
strained Variables Non-Negative in Solver.)
 14. Consider the stock return scenarios for Apple Computer (APPL), Advanced Micro 
 Devices (AMD), and Oracle Corporation (ORCL) shown in the following table: 
1
2
3
4
5
6
7
8
AAPL
AMD
ORCL
239.80
242.50
210.20
10.10
13.60
137.90
124.90
56.90
170.60
151.80
36.70
16.60
258.30
234.80
240.70
14.30
267.40
230.30
241.90
183.60
15.20
57.10
6.30
20.60
a. Develop the Markowitz portfolio model for these data with a required expected return 
of 25 percent. Assume that the eight scenarios are equally likely to occur.
b. Solve the model developed in part a.
c. Vary the required return in one percent increments from 25 percent to 30 percent, and 
plot the efficient frontier.
file
WEB
Wedding
file
WEB
StockReturn1

476 
Chapter 10 Nonlinear Optimization Models
 15. A second version of the Markowitz portfolio model maximizes expected return subject to 
a constraint that the variance of the portfolio must be less than or equal to some specified 
amount. Consider again the Hauck Financial Service data given in Section 10.4.
Annual Return (%)
Mutual Fund
Year 1
Year 2
Year 3
Year 4
Year 5
Foreign Stock
Intermediate-Term Bond
Large-Cap Growth
Large-Cap Value
Small-Cap Growth
Small-Cap Value
10.06
17.64
32.41
32.36
33.44
24.56
13.12
 3.25
18.71
20.61
19.40
25.32
13.47
 7.51
33.28
12.93
 3.85
26.70
45.42
21.33
41.46
 7.06
58.68
 5.43
221.93
7.36
223.26
25.37
29.02
17.31
a. Construct this version of the Markowitz model for a maximum variance of 30.
b. Solve the model developed in part a.
 16. Consider the following stock return data:
1
2
3
4
5
6
Stock 1
Stock 2
Stock 3
0.300
0.225
0.149
0.103
0.290
0.260
0.216
0.216
0.419
20.046
20.272
20.078
20.071
0.144
0.169
0.056
0.107
20.035
7
8
9
10
11
12
Stock 1
Stock 2
Stock 3
0.038
0.321
0.133
0.089
0.305
0.732
0.090
0.195
0.021
0.083
0.390
0.131
0.035
20.072
0.006
0.176
0.715
0.908
a. Construct the Markowitz portfolio model using a required expected return of 15 per-
cent. Assume that the 12 scenarios are equally likely to occur.
b. Solve the model using Excel Solver.
c. Solve the model for various values of required expected return and plot the efficient 
frontier.
 17. Let us consider again the investment data from Hauck Financial Services used in Section 
10.4 to illustrate the Markowitz portfolio model. The data follows, along with the return 
of the S&P 500 Index. Hauck would like to create a portfolio using the funds listed, so that 
the resulting portfolio matches the return of the S&P 500 index as closely as possible.
Mutual Fund
Year 1
Year 2
Year 3
Year 4
Year 5
Foreign Stock
Intermediate-Term Bond
Large-Cap Growth
Large-Cap Value
Small-Cap Growth
Small-Cap Value
S&P 500 Return
10.060
17.640
32.410
32.360
33.440
24.560
25.000
13.120
 3.250
18.710
20.610
19.400
25.320
20.000
13.470
7.510
33.280
12.930
3.850
26.700
8.000
45.420
21.330
41.460
7.060
58.680
5.430
30.000
221.930
7.360
223.260
25.370
29.020
17.310
210.000
a. Develop an optimization model that will give the fraction of the portfolio to invest 
in each of the funds so that the return of the resulting portfolio matches as closely as 
possible the return of the S&P 500 Index. (Hint: Minimize the sum of the squared 
deviations between the portfolio’s return and the S&P 500 Index return for each year 
in the data set.)
b. Solve the model developed in part a.
 18. As discussed in Section 10.4, the Markowitz Model uses the variance of the portfolio as 
the measure of risk However variance includes deviations both below and above the
file
WEB
HauckData
file
WEB
StockReturn2
file
WEB
Hauck500

 
Case Problem Portfolio Optimization with Transaction Costs 
477
mean return. Semivariance includes only deviations below the mean and is considered by 
many to be a better measure of risk.
a. Develop a model that minimizes semivariance for the Hauck Financial data given 
in the file HauckData with a required return of 10 percent. (Hint: Modify model 
(10.8)–(10.17). Define a variable ds for each scenario and let ds $ R 2 Rs with ds $ 0. 
Then make the objective function: Min 1⁄5 a
5
s51
ds
2.
b. Solve the model you developed in part a with a required expected return of 10 percent.
 19. Refer to Problem 15. Use the model developed there to construct an efficient frontier by 
varying the maximum allowable variance from 20 to 60 in increments of 5 and solving for 
the maximum return for each. Plot the efficient frontier and compare it to Figure 10.12.
 20. The weekly box office revenues (in $ millions) for the summer blockbuster movie dis-
cussed in Section 10.5 follow. Use these data in the Bass forecasting model given by 
equations (10.19)–(10.21) to estimate the parameters p, q, and m.
Week
Revenues
1
2
3
4
5
6
7
8
9
10
11
12
72.39
37.93
17.58
9.57
5.39
3.13
1.62
0.87
0.61
0.26
0.19
0.35
 
 The Bass forecasting model is a good example of a difficult-to-solve nonlinear program, 
and the answer you get may be a local optimum that is not nearly as good as the result 
given in Table 10.4. Solve the model using Excel Solver with the Multistart option, and 
see whether you can duplicate the results in Table 10.4.
portfolio Optimization with transaction Costs
Hauck Financial Services has a number of passive, buy-and-hold clients. For these clients, 
Hauck offers an investment account whereby clients agree to put their money into a portfolio 
of mutual funds that is rebalanced once a year. When the rebalancing occurs, Hauck determines 
the mix of mutual funds in each investor’s portfolio by solving an extension of the Markowitz 
portfolio model that incorporates transaction costs. Investors are charged a small transaction 
cost for the annual rebalancing of their portfolio. For simplicity, assume the following:
● 
At the beginning of the time period (in this case one year), the portfolio is rebalanced 
by buying and selling Hauck mutual funds.
● 
The transaction costs associated with buying and selling mutual funds are paid at the 
beginning of the period when the portfolio is rebalanced, which, in effect, reduces 
the amount of money available to reinvest.
● 
No further transactions are made until the end of the time period, at which point the 
new value of the portfolio is observed.
● 
The transaction cost is a linear function of the dollar amount of mutual funds bought 
or sold
file
WEB
HauckData
file
WEB
Blockbuster
Case Problem

478 
Chapter 10 Nonlinear Optimization Models
Jean Delgado is one of Hauck’s buy-and-hold clients. We briefly describe the model as 
it is used by Hauck for rebalancing her portfolio. The mix of mutual funds that are being 
considered for her portfolio are a foreign stock fund (FS), an intermediate-term bond fund 
(IB), a large-cap growth fund (LG), a large-cap value fund (LV), a small-cap growth fund 
(SG), and a small-cap value fund (SV). In the traditional Markowitz model, the variables 
are usually interpreted as the proportion of the portfolio invested in the asset represented 
by the variable. For example, FS is the proportion of the portfolio invested in the foreign 
stock fund. However, it is equally correct to interpret FS as the dollar amount invested in the 
foreign stock fund. Then FS 5 25,000 implies $25,000 is invested in the foreign stock fund. 
Based on these assumptions, the initial portfolio value must equal the amount of money 
spent on transaction costs plus the amount invested in all the assets after rebalancing; that is,
 
Initial portfolio value 5 amount invested in all assets after rebalancing
 
 
1 transaction costs
The extension of the Markowitz model that Hauck uses for rebalancing portfolios re-
quires a balance constraint for each mutual fund. This balance constraint is
 Amount invested in fund i 5 initial holding of fund i
 
 
1 amount of fund i purchased 2 amount of fund i sold
Using this balance constraint requires three additional variables for each fund: one for 
the amount invested prior to rebalancing, one for the amount sold, and one for the amount 
purchased. For instance, the balance constraint for the foreign stock fund is:
 
FS 5 FS_START 1 FS_BUY 2 FS_SELL
Jean Delgado has $100,000 in her account prior to the annual rebalancing, and she 
has specified a minimum acceptable return of 10 percent. Hauck plans to use the follow-
ing model to rebalance Ms. Delgado’s portfolio. The complete model with transaction 
costs is
Min 1⁄5 a
5
s51
(Rs 2 R) 2
s.t.
0.1006FS 1 0.1764IB 1 0.3241LG 1 0.3236LV 1 0.3344SG 1 0.2456SV 5 R1
0.1312FS 1 3.2500IB 1 0.1871LG 1 0.2061LV 1 0.1940SG 1 0.2532SV 5 R2
0.1347FS 1 0.0751IB 1 0.3328LG 1 0.1293LV 1
0.385SG 2 0.0670SV 5 R3
0.4542FS 2 0.0133IB 1 0.4146LG 1 0.0706LV 1 0.5868SG 1 0.0543SV 5 R4
20.2193FS 1 0.0736IB 2 0.2326LG 2 0.0537LV 2 0.0902SG 1 0.1731SV 5 R5
1⁄5 a
5
s51
Rs 5 R
R $ 10,000
FS 1 IB 1 LG 1 LV 1 SG 1 SV 1 TRANS_COST 5 100,000
FS_START 1 FS_BUY 2 FS_SELL 5 FS
IB_START 1 IB_BUY 2 IB_SELL 5 IB
LG_START 1 LG_BUY 2 LG_SELL 5 LG
LV_START 1 LV_BUY 2 LV_SELL 5 LV
SG_START 1 SG_BUY 2 SG_SELL 5 SG
SV START 1 SV BUY 2 SV SELL 5 SV

 
Case Problem Portfolio Optimization with Transaction Costs 
479
TRANS_FEE * (FS_BUY 1 FS_SELL 1 IB_BUY 1 IB_SELL 1
LG_BUY 1 LG_SELL 1 LV_BUY 1 LV_SELL 1 SG_BUY 1
SG_SELL 1 SV_BUY 1 SV_SELL) 5 TRANS_COST
FS_START 5 10,000
IB_START 5 10,000
LG_START 5 10,000
LV_START 5 40,000
SG_START 5 10,000
SV_START 5 20,000
TRANS_FEE 5 0.01
FS, IB, LG, LV, SG, SV $ 0
Notice that the transaction fee is set at 1 percent in the model (the last constraint) and that the 
transaction cost for buying and selling shares of the mutual funds is a linear function of the 
amount bought and sold. With this model, the transactions costs are deducted from the cli-
ent’s account at the time of rebalancing and thus reduce the amount of money invested. The 
solution for Ms. Delgado’s rebalancing problem is shown as part of the Managerial Report.
Managerial Report
Assume you are a newly employed financial analytics specialist hired by Hauck Financial 
Services. One of your first tasks is to review the portfolio rebalancing model in order to 
resolve a dispute with Jean Delgado. Ms. Delgado has had one of the Hauck passively man-
aged portfolios for the last five years and has complained that she is not getting the rate of 
return of 10 percent that she specified. After a review of her annual statements for the last 
five years, she feels that she is actually getting less than 10 percent on average.
 1. According to the following Model Solution, IB_BUY 5 $41,268.51. How much in 
transaction costs did Ms. Delgado pay for purchasing additional shares of the inter-
mediate-term bond fund?
 MODEL SOLUTION
Optimal
Objective Value
27219457.356
Variable
Value
Variable
Value
R1
R
R2
R3
R4
R5
FS
IB
LG
LV
SG
SV
TRANS_COST
FS_START
FS_BUY
FS_SELL
18953.280
10000.000
11569.210
5663.961
9693.921
4119.631
15026.860
51268.510
4939.312
0.000
0.000
27675.000
1090.311
10000.000
5026.863
0.000
IB_START
IB_BUY
IB_SELL
LG_START
LG_BUY
LG_SELL
LV_START
LV_BUY
LV_SELL
SG_START
SG_BUY
SG_SELL
SV_START
SV_BUY
SV_SELL
TRANS_FEE
10000.000
41268.510
0.000
10000.000
0.000
5060.688
40000.000
0.000
40000.000
10000.000
0.000
10000.000
20000.000
7675.004
0.000
0.010

480 
Chapter 10 Nonlinear Optimization Models
 2. Based on the Model Solution, what is the total transaction cost associated with rebal-
ancing Ms. Delgado’s portfolio?
 3. After paying transactions costs, how much did Ms. Delgado have invested in mutual 
funds after her portfolio was rebalanced?
 4. According to the Model Solution, IB 5 $51,268.51. How much can Ms. Delgado 
expect to have in the intermediate-term bond fund at the end of the year?
 5. According to the Model Solution, the expected return of the portfolio is $10,000. What 
is the expected dollar amount in Ms. Delgado’s portfolio at the end of the year? Can 
she expect to earn 10 percent on the $100,000 she had at the beginning of the year?
 6. It is now time to prepare a report to management to explain why Ms. Delgado did not 
earn 10 percent each year on her investment. Make a recommendation in terms of a 
revised portfolio model that can be used so that Jean Delgado can have an expected 
portfolio balance of $110,000 at the end of next year. Prepare a report that includes 
a modified optimization model that will give an expected return of 10 percent on the 
amount of money available at the beginning of the year before paying the transaction 
costs. Explain why the current model does not do this.
 7. Solve the formulation in part 6 for Jean Delgado. How does the portfolio composition 
differ from that of the Model Solution?
Solving Nonlinear Optimization problems 
with analytic Solver platform
In this appendix, we illustrate how to use Analytic Solver Platform (ASP) to solve nonlinear 
optimization problems in Excel. We assume that ASP has been installed.
In the Par, Inc. problem, the nonlinear optimization model we developed is
Let 
 S 5 the number of Standard bags to produce
  
D 5 the number of Deluxe bags to produce
Max
80S 2
1⁄15 S2 1 150D 2 1⁄5D2
s.t.
7⁄10S 1
1D # 630  Cutting and dyeing
1⁄2 S 1
5⁄6 D # 600  Sewing
1S 1
2⁄3 D # 708  Finishing
1⁄10 S 1
1⁄4 D # 135  Inspection and packaging
S, D $ 0
The spreadsheet model is shown in Figure 10.15.
Open the file ParNonlinear. To solve the Par, Inc. problem using ASP, follow these 
steps:
Step 1. Click the ANALYTICS SOLVER PLATFORM tab in the Ribbon
Step 2. When the Solver Options and Model Specifications task pane appears, select 
the  next to Optimization to expand the tree structure (If the task pane is not 
visible, click Model in the Model group to activate this pane.)
Step 3. When the optimization tree structure appears (Figure 10.16):
Select Objective
Select cell B16
Click the Add button  in the Solver Options and Model Specifications 
task pane (Figure 10.16)
Appendix
file
WEB
ParNonlinear

 
Appendix Solving Nonlinear Optimization Problems with Analytic Solver Platform 
481
FiGURE 10.15  EXCEL SPREADSHEET MODEL FOR PAR, INC.
A
Par, Inc.
B
C
D
Parameters
Production Time (Hours)
Standard
Time Available
(Hours)
Deluxe
Standard
Deluxe
Operation
Hours Used
Hours Available
Operation
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Marginal Cost
0.7
0.5
1
0.1
70
Bags Produced
459.716599481298
=(B25-B9)∗B14+(B26-C9)∗C14
=SUMPRODUCT(B5:C5,$B$14:$C$14)
=SUMPRODUCT(B6:C6,$B$14:$C$14)
=SUMPRODUCT(B7:C7,$B$14:$C$14)
=SUMPRODUCT(B8:C8,$B$14:$C$14)
=D5
=D6
=D7
=D8
308.198380121294
1
=5/6
=2/3
0.25
150
630
600
708
135
Model
Total Proﬁt
1
2
3
4
5
6
7
8
9
10
23
24
25
26
27
11
12
13
14
15
16
17
18
19
20
21
22
Standard Bag Price Function =150-(1/15)*$B$14
Deluxe Bag Price Function
=300-(1/5)*$C$14
A
Par, Inc.
B
C
D
Parameters
Production Time (Hours)
Time Available
Standard
(Hours)
Deluxe
Standard
Deluxe
1
2
3
4
5
6
7
8
9
25
26
11
12
10
Operation
Hours Used Hours Available
Operation
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Cutting and Dyeing
Sewing
Finishing
Inspection and Packaging
Marginal Cost
0.7
0.5
1
0.1
$70.00
Bags Produced
459.7166
$49,920.55
630.00
486.69
665.18
123.02
630
600
708
135
308.1984
1
0.833333333
0.666666667
0.25
$150.00
630
600
708
135
Model
Total Proﬁt
13
14
15
16
18
17
19
20
21
22
24
23
Standard Bag Price Function
Deluxe Bag Price Function
119.35
238.36

482 
Chapter 10 Nonlinear Optimization Models
Step 4. Under Variables, select Normal 
Select cells B14:C14
Click the Add button  in the Solver Options and Model Specifications 
task pane
Step 5. Under Constraints, select Normal
Select cells B19:B22
Click the Add button  in the Solver Options and Model Specifications 
task pane
When the Add Constraint dialog box appears:
Select <5 from the drop-down button
Enter C19:C22 in the Constraint area
Click OK
The Solver Options and Model Specifications dialog box should now appear as 
shown in Figure 10.17.
Figure 10.16   THE OPTIMIZATION TREE STRUCTURE IN THE SOLVER 
 OPTIONS AND MODEL SPECIFICATIONS DIALOG BOX
Solve
Analyze
Refresh
Delete
Add

 
Appendix Solving Nonlinear Optimization Problems with Analytic Solver Platform 
483
Step 6. Click the Engine tab in the Solver Options and Model Specifications task 
pane
Select the checkbox for Automatically Select Engine
In the General area click Assume Non-Negative
Select True from the drop-down menu
Step 7. Click the Model tab in the Solver Options and Model Specifications task 
pane
Step 8. To solve the problem, click the Solve button 
 in the Solver Options and 
Model Specifications task pane (Figure 10.16)
The solution is sent to the spreadsheet, and the output appears under the Output tab in 
the Solver Options and Model Specifications dialog box, as shown in Figure 10.18. The 
output indicates that the optimal solution was found and all constraints were satisfied. The 
solution should be the same as when we solved the problem using standard Excel Solver as 
shown in Figure 10.3. The complete model for the constrained nonlinear Par, Inc. problem 
is contained in the file ParNonlinearModel.
Upon clicking the Solve 
button, the Guided Mode 
dialog box willappear if 
ASP’s Guided Mode is 
turned on. ASP’s Guided 
Mode assists the analyst in 
the optimization process 
and can be turned off at the 
analyst’s discretion.
file
WEB
ParNonlinearModel
Figure 10.17   THE PAR, INC. MODEL IN THE SOLVER OPTIONS AND MODEL 
SPECIFICATIONS DIALOG BOX


Monte Carlo Simulation
CONTENTS
11.1 WHAT-IF ANALYSIS
The Sanotronics Problem
Base-Case Scenario
Worst-Case Scenario
Best-Case Scenario
11.2 SIMULATION  MODELING 
WITH NATIVE EXCEL 
 FUNCTIONS
Use of Probability Distributions 
to Represent Random 
Variables 
Generating Values for Random 
Variables with Excel
Executing Simulation Trials 
with Excel
Measuring and Analyzing 
Simulation Output
11.3 SIMULATION MODELING 
WITH ANALYTIC SOLVER 
PLATFORM
The Land Shark Problem
Spreadsheet Model for Land 
Shark
Generating Values for Land 
Shark’s Random Variables
Tracking Output Measures for 
Land Shark
Executing Simulation Trials and 
Analyzing Output for Land 
Shark
The Zappos Problem
Spreadsheet Model for Zappos
Modeling Random Variables for 
Zappos
Tracking Output Measures for 
Zappos
Executing Simulation Trials and 
Analyzing Output for Zappos
11.4 SIMULATION  
 OPTIMIZATION 
11.5  SIMULATION 
 CONSIDERATIONS
Verification and Validation
Advantages and Disadvantages 
of Using Simulation
APPENDIX 11.1  INCORPORATING 
DEPENDENCE 
BETWEEN 
RANDOM 
VARIABLES
APPENDIX 11.2  PROBABILITY 
DISTRIBUTIONS 
FOR RANDOM 
VARIABLES
Chapter 11

486 
Chapter 11 Monte Carlo Simulation
Every year in the United States, approximately two mil-
lion patients acquire an infection after being admitted 
to a hospital. More than 100,000 of these patients die 
as a result of their hospital-acquired infections. This 
problem is expected to worsen as pathogens continue to 
 develop greater resistance to antibiotics.
Two methods of decreasing the rate of hospital- 
acquired infections are (1) patient isolation and (2) greater 
adherence to hand-washing hygiene. If infected patients 
can be identified quickly, they can be quarantined to pre-
vent greater outbreaks. Furthermore, proper hand wash-
ing can greatly reduce the number of pathogens present on 
the skin and thereby lead to fewer infections. Yet previous 
studies have found that less than half of all health workers 
completely and correctly follow hand hygiene protocols.
A group of researchers used data from the intensive 
care unit (ICU) at Cook County Hospital in Chicago, 
Illinois, to create a simulation model of the movements 
of patients, health care workers, hospital visitors, and 
actual pathogens that lead to infections. The researchers 
were able to simulate both the creation of a new isola-
tion ward in the ICU and better hand hygiene habits. The 
simulation estimated rates of infection and impacts on 
hospital costs in each scenario.
The simulation showed that both patient isolation 
and better hand hygiene can greatly reduce infec-
tion rates. Improving hand hygiene is considerably 
 cheaper than building and maintaining additional 
quarantine facilities, but the researchers point out that 
even the best simulations do not consider the psycho-
logical responses of health care workers. The simu-
lation cannot detect why hand hygiene compliance 
is currently low, so improving adherence in practice 
could be challenging.
Reducing Patient infections in the icu*
AnAlytics  in  Action
*From R. Hagtvedt, P. Griffin, P. Keskinocak, and R. Roberts, “A Simu-
lation Model to Compare Strategies for the Reduction of Health-Care-
Associated Infections,” Interfaces 39, no. 3 (May–June 2009).
Uncertainty pervades decision-making in business, government, and our personal lives. 
This chapter introduces the use of Monte Carlo simulation to evaluate the impact of 
uncertainty on a decision. Simulation models have been successfully used in a variety 
of disciplines. Financial applications include investment planning, project selection, and 
 option pricing. Marketing applications include new product development and the timing of 
market entry for a product. Management applications include project management, inven-
tory ordering (especially important for seasonal products), capacity planning, and revenue 
management (prominent in the airline, hotel, and car rental industries). In each of these 
applications, uncertain quantities complicate the decision process.
As we will demonstrate, a spreadsheet simulation analysis requires a model foundation 
of logical formulas that correctly express the relationships between parameters and deci-
sions to generate outputs of interest. A simulation model extends the spreadsheet modeling 
approach of Chapter 7 by replacing the use of single values for parameters with a range of 
possible values. For example, a simple spreadsheet model may compute a clothing retailer’s 
profit, given values for the number of ski jackets ordered from the manufacturer and the 
number of ski jackets demanded by customers. A simulation analysis extends this model 
by replacing the single value used for ski jacket demand with a probability distribution 
of possible values of ski jacket demand. A probability distribution of ski jacket demand 
represents not only the range of possible values but also the relative likelihood of various 
levels of demand.
To evaluate a decision with a Monte Carlo simulation, an analyst identifies parameters 
that are not known with a high degree of certainty and treats these parameters as random, or 
uncertain, variables. The values for the random variables are randomly generated from the 
specified probability distributions. The simulation model uses the randomly generated values 
of the random variables and the relationships between parameters and decisions to compute 
Monte carlo  simulation 
originated during World 
War ii as part of the 
 Manhattan Project to 
develop nuclear  weapons. 
“Monte carlo” was 
selected as the code name 
for the classified method 
in reference to the famous 
Monte carlo casino in 
 Monaco and the uncertain-
ties inherent in gambling.

 
11.1 What-If Analysis 
487
the corresponding values of an output. Specifically, a simulation experiment produces a dis-
tribution of output values that correspond to the randomly generated values of the uncertain 
input variables. This probability distribution of the output values describes the range of pos-
sible outcomes, as well as the relative likelihood of each outcome. After reviewing the simula-
tion results, the analyst is often able to make decision recommendations for the controllable 
inputs that address not only the average output but also the variability of the output. 
What-If analysis
When making a decision in the presence of uncertainty, the decision maker should be inter-
ested not only in the average, or expected, outcome, but also in information regarding the 
range of possible outcomes. In particular, decision makers are interested risk analysis, that 
is, quantifying the likelihood and magnitude of an undesirable outcome. In this section, we 
show how to perform a basic risk analysis by considering a small set of what-if scenarios. 
The Sanotronics Problem
Sanotronics is a start-up company that manufactures medical devices for use in hospital 
clinics. Inspired by experiences with family members who have battled cancer,  Sanotronics’ 
founders have developed a prototype for a new device that limits health care workers’ 
 exposure to chemotherapy treatments while they are preparing, administering, and dispos-
ing of these hazardous medications. The new device features an innovative design and has 
the potential to capture a substantial share of the market. 
Sanotronics would like an analysis of the first-year profit potential for the device. 
Because of Sanotronics’ tight cash flow situation, management is particularly concerned 
about the potential for a loss. Sanotronics has identified the key parameters in determining 
first-year profit: selling price per unit (p), first-year administrative and advertising costs 
(ca), direct labor cost per unit (ci), parts cost per unit (cp), and first-year demand (d). After 
conducting market research and a financial analysis, Sanotronics estimates with a high 
level of certainty that the device’s selling price will be $249 per unit and that the first-year 
administrative and advertising costs will total $1,000,000.
Sanotronics is not certain about the values for the cost of direct labor, the cost of parts, 
and the first-year demand. At this stage of the planning process, Sanotronics’ base estimates 
of these inputs are $45 per unit for the direct labor cost, $90 per unit for the parts cost, and 
15,000 units for the first-year demand. 
Base-Case Scenario
Sanotronics first-year profit is computed by:
 
Profit 5 ( p 2 ci 2 cp) 3 d 2 ca 
(11.1)
Recall that Sanotronics is certain of a selling price of $249 per unit, and administrative 
and advertising costs total $1,000,000. Substituting these values into equation (11.1) yields
 
Profit 5 (249 2 ci 2 cp) 3 d 2 1,000,000 
(11.2)
Sanotronics’ base-case estimates of the direct labor cost per unit, the parts cost per unit, and 
first-year demand are $45, $90, and 15,000 units, respectively. These values constitute the 
base-case scenario for Sanotronics. Substituting these values into equation (11.2) yields 
the following profit projection:
 
Profit 5 (249 2 45 2 90)(15,000) 2 1,000,000 5 710,000
Thus, the base-case scenario leads to an anticipated profit of $710,000.
11.1

488 
Chapter 11 Monte Carlo Simulation
Although the base-case scenario looks appealing, Sanotronics is aware that the values 
of direct labor cost per unit, parts cost per unit, and first-year demand are uncertain, so the 
base-case scenario may not occur. To help Sanotronics gauge the impact of the uncertainty, 
the company may consider performing a what-if analysis. A what-if analysis involves 
considering alternative values for the random variables (direct labor cost, parts cost, and 
first-year demand) and computing the resulting value for the output (profit).
Sanotronics is interested in what happens if the estimates of the direct labor cost per 
unit, parts cost per unit, and first-year demand do not turn out to be as expected under the 
base-case scenario. For instance, suppose that Sanotronics believes that direct labor costs 
could range from $43 to $47 per unit, parts cost could range from $80 to $100 per unit, and 
first-year demand could range from 0 to 30,000 units. Using these ranges, what-if analysis 
can be used to evaluate a worst-case scenario and a best-case scenario.
Worst-Case Scenario
The worst-case for the direct labor cost is $47 (the highest value), the worst-case for the 
parts cost is $100 (the highest value), and the worst-case for demand is 0 units (the lowest 
value). Substituting these values into equation (11.2) leads to the following profit  projection:
 
Profit 5 (249 2 47 2 100)(0) 2 1,000,000 5 21,000,000
So, the worst-case scenario leads to a projected loss of $1,000,000.
Best-Case Scenario
The best-case value for the direct labor cost is $43 (the lowest value), for the parts cost it is 
$80 (the lowest value), and for demand it is 30,000 units (the highest value). Substituting 
these values into equation (11.2) leads to the following profit projection:
 
Profit 5 (249 2 43 2 80)(30,000) 2 1,000,000 5 2,780,000
So the best-case scenario leads to a projected profit of $2,780,000.
At this point, the what-if analysis provides the conclusion that profits may range from a 
loss of $1,000,000 to a profit of $2,780,000 with a base-case profit of $710,000. Although 
the base-case profit of $710,000 is possible, the what-if analysis indicates that either a sub-
stantial loss or a substantial profit is possible. Sanotronics can repeat this what-if analysis 
for other scenarios. However, simple what-if analyses do not indicate the likelihood of the 
various profit or loss values. In particular, we do not know anything about the probability of 
a loss. To conduct a more thorough evaluation of risk by obtaining insight on the  potential 
magnitude and probability of undesirable outcomes, we now turn to developing a spread-
sheet simulation model.
Simulation Modeling with 
Native excel  Functions
In this section, we show how to construct a simulation model and conduct a risk analysis 
using only native Excel functionality. The first step in constructing a spreadsheet simula-
tion model is to express the relationship between the inputs and the outputs with appropri-
ate formula logic. Figure 11.1 provides the formula and value views for the Sanotronics 
spreadsheet. Data on selling price per unit, administrative and advertising cost, direct labor 
cost per unit, parts cost per unit, and demand are in cells B4 to B8. The profit calculation, 
corresponding to equation (11.1), is expressed in cell B11 using appropriate cell references 
and formula logic. For the values shown in Figure 11.1, the spreadsheet model computes 
in chapter 7, we cover the 
use of data tables and 
goal seek in excel for 
what-if analysis. however, 
these methods do not indi-
cate the relative likelihood 
of the occurrence of differ-
ent scenarios. 
11.2
Later in the chapter we 
demonstrate how analytic 
solver Platform facilitates 
the construction of more 
complex simulation models.

 
11.2 Simulation Modeling with Native Excel  Functions 
489
profit for the base-case scenario. By changing one or more values for the input parameters, 
the spreadsheet model can be used to conduct a manual what-if analysis (e.g., the best-case 
and worst-case scenarios). 
Use of Probability Distributions 
to Represent Random Variables
Using the what-if approach to risk analysis, we manually select values for the random vari-
ables (direct labor cost per unit, parts cost per unit, and first-year demand), and then com-
pute the resulting profit. Instead of manually selecting the values for the random variables, 
a Monte Carlo simulation randomly generates values for the random variables so that the 
values used reflect what we might observe in practice. A probability distribution describes 
the possible values of a random variable and the relative likelihood of the random variable 
realizing these values. The analyst can use historical data and knowledge of the random 
variable (range, mean, mode, standard deviation) to specify the probability distribution for 
a random variable. As we describe in the following paragraphs, Sanotronics examined the 
random variables to identify probability distributions for the direct labor cost per unit, the 
parts cost per unit, and first-year demand. 
Based on recent wage rates and estimated processing requirements of the device, Sano-
tronics believes that the direct labor cost will range from $43 to $47 per unit and is described 
by the discrete probability distribution shown in Figure 11.2. Thus, we see that there is 0.1 
file
WEB
Sanotronics
FIGURE 11.1   EXCEL WORKSHEET FOR SANOTRONICS
A
B
Sanotronics
Selling Price per Unit
Administrative & Advertising Cost
Direct Labor Cost Per Unit
Parts Cost Per Unit
Demand
Model
Proﬁt
Parameters
249
1000000
45
90
15000
=((B4-B6-B7)*B8)-B5
1
2
3
4
5
6
7
8
9
10
11
12
A
B
Sanotronics
Selling Price per Unit
Administrative & Advertising Cost
Direct Labor Cost Per Unit
Parts Cost Per Unit
Demand
Model
Proﬁt
Parameters
$249.00
$1,000,000
$45.00
$90.00
15,000
$710,000.00
1
2
3
4
5
6
7
8
9
10
11

490 
Chapter 11 Monte Carlo Simulation
probability that the direct labor cost will be $43 per unit, a 0.2 probability that the direct 
labor cost will be $44 per unit, and so on. The highest probability, 0.4, is associated with 
a direct labor cost of $45 per unit. Because we have assumed that the direct labor cost per 
unit is best described by a discrete probability distribution, the direct labor cost per unit 
can take on only the values of $43, $44, $45, $46, or $47.
Sanotronics is relatively unsure of the parts cost because it  depends on many factors, 
including the general economy, the overall demand for parts, and the pricing policy of 
Sanotronics’ parts suppliers. Sanotronics is confident that the parts cost will be between $80 
and $100 per unit but is unsure as to whether any particular values between $80 and $100 
are more likely than others. Therefore, Sanotronics decides to describe the uncertainty in 
parts cost with a uniform probability distribution, as shown in Figure 11.3. Costs per unit 
between $80 and $100 are equally likely. A uniform probability distribution is an example 
FIGURE 11.2   PROBABILITY DISTRIBUTION FOR DIRECT LABOR COST 
PER UNIT
Direct Labor Cost Per Unit
Probability
0.35
0.30
0.25
0.45
0.40
0.20
0.15
0.10
0.05
0
$43
$44
$45
$46
$47
FIGURE 11.3   UNIFORM PROBABILITY DISTRIBUTION FOR PARTS COST 
PER UNIT
1
20
80
90
Parts Cost per Unit
100

 
11.2 Simulation Modeling with Native Excel  Functions 
491
of a continuous probability distribution, which means that the parts cost can take on any 
value between $80 and $100.
Based on sales of comparable medical devices, Sanotronics believes that first-year de-
mand is described by the normal probability distribution shown in Figure 11.4. The mean 
or expected value of first-year demand is 15,000 units. The standard deviation of 4500 units 
describes the variability in the first-year demand. The normal probability distribution is a 
continuous probability distribution in which any value is possible, but values extremely 
larger or smaller than the mean are increasingly unlikely.
Generating Values for Random Variables with Excel
To simulate the Sanotronics problem, we must generate values for the three random vari-
ables and compute the resulting profit. A set of values for the random variables is called a 
trial. Then we generate another trial, compute a second value for profit, and so on. We con-
tinue this process until we are satisfied that enough trials have been conducted to describe 
the probability distribution for profit. This process of generating the values of random 
variables and computing the value of the output are the essential components of Monte 
Carlo simulation.
In the Sanotronics model, representative values must be generated for the random vari-
ables corresponding to direct labor cost per unit, the parts cost per unit, and the first-year 
demand. To illustrate how to generate these values, we need to introduce the concept of 
computer-generated random numbers.
Computer-generated random numbers1 are randomly selected numbers from 0 up to, 
but not including, 1; this interval is denoted [0, 1). All values of the computer-generated 
random numbers are equally likely and so the values are uniformly distributed over the 
interval from 0 to 1. Computer-generated random numbers can be obtained using built-in 
functions available in computer simulation packages and spreadsheets. For example, plac-
ing the formula 5RAND() in a cell of an Excel worksheet will result in a random number 
between 0 and 1 being placed into that cell. 
one advantage of simula-
tion is that the analyst 
can adjust the probability 
distributions of the random 
variables to determine the 
impact of the assumptions 
about the shape of the 
uncertainty on the results 
(and ultimately the sensitiv-
ity of the decision to the 
distribution assumptions on 
the random variables).
FIGURE 11.4   NORMAL PROBABILITY DISTRIBUTION FOR FIRST-YEAR 
 DEMAND
15,000
Standard Deviation
  = 4500 units
Number of Units Sold
1Computer-generated random numbers are formally called pseudorandom numbers because they are generated through 
the use of mathematical formulas and therefore not technically random. The difference between random numbers and 
pseudorandom numbers is primarily philosophical, and we use the term random numbers regardless of whether they are 
generated by a computer.

492 
Chapter 11 Monte Carlo Simulation
Let us show how random numbers can be used to generate values corresponding to the 
probability distributions for the random variables in the Sanotronics example. We begin by 
showing how to generate a value for the direct labor cost per unit. The approach described 
is applicable for generating values from any discrete probability distribution. 
Table 11.1 illustrates the process of partitioning the interval from 0 to 1 into subinter-
vals so that the probability of generating a random number in a subinterval is equal to the 
probability of the corresponding direct labor cost. The interval of random numbers from 0 
up to but not including 0.1, [0, 0.1), is associated with a direct labor cost of $43; the interval 
of random numbers from 0.1 up to but not including 0.3, [0.1, 0.3), is associated with a 
direct labor cost of $44, and so on. With this assignment of random number intervals to the 
possible values of the direct labor cost, the probability of generating a random number in 
any interval is equal to the probability of obtaining the corresponding value for the direct 
labor cost. Thus, to select a value for the direct labor cost, we generate a random number 
between 0 and 1 using the RAND function in Excel. If the random number is at least 0.0 
but less than 0.1, we set the direct labor cost equal to $43. If the random number is at least 
0.1 but less than 0.3, we set the direct labor cost equal to $44, and so on.
Each trial of the simulation requires a value for the direct labor cost. Suppose that on the 
first trial the random number is 0.9109. From Table 11.1, because 0.9109 is in the interval 
[0.9, 1.0), the corresponding simulated value for the direct labor cost is $47 per unit. Sup-
pose that on the second trial the random number is 0.2841. From Table 11.1, the simulated 
value for the direct labor cost is $44 per unit.
Each trial in the simulation also requires a value of the parts cost and first-year demand. 
Let us now turn to the issue of generating values for the parts cost. The probability distribu-
tion for the parts cost per unit is the uniform distribution shown in Figure 11.3. Because 
this random variable has a different probability distribution than direct labor cost, we use 
random numbers in a slightly different way to generate simulated values for parts cost. To 
generate a value for a random variable characterized by a continuous uniform distribution, 
the following Excel formula is used:
Value of uniform random variable 
 
5 lower bound 1 (upper bound 2 lower bound) 3 RAND( ) 
(11.3)
For Sanotronics, the parts cost per unit is a uniformly distributed random variable with a 
lower bound of $80 and an upper bound of $100. Applying equation (11.3) leads to the 
following formula for generating the parts cost:
 
Parts cost 5 80 1 20 3 RAND( ) 
(11.4)
By closely examining equation (11.4), we can understand how it uses random numbers to 
generate uniformly distributed values for parts cost. The first term of equation (11.4) is 80 
because Sanotronics is assuming that the parts cost will never drop below $80 per unit. 
Since RAND is between 0 and 1, the second term, 20 3 RAND(), corresponds to how much 
TABLE 11.1  RANDOM NUMBER INTERVALS FOR GENERATING VALUE OF 
DIRECT LABOR COST PER UNIT
Direct Labor  
Cost Per Unit
Probability
Interval of  
Random Numbers
$43
$44
$45
$46
$47
0.1
0.2
0.4
0.2
0.1
[0.0, 0.1)
[0.1, 0.3)
[0.3, 0.7)
[0.7, 0.9)
[0.9, 1.0)

 
11.2 Simulation Modeling with Native Excel  Functions 
493
more than the lower bound the simulated value of parts cost is. Because RAND is equally 
likely to be any value between 0 and 1, the simulated value for the parts cost is equally likely 
to be between the lower bound (80 1 0 5 80) and the upper bound (80 1 20 5 100). For 
 example, suppose that a random number of 0.4576 is obtained. As illustrated by Figure 11.5,  
the value for the parts cost is
 
Parts cost 5 80 1 20 3 0.4576 5 80 1 9.15 5 89.15 per unit
Suppose that a random number of 0.5842 is generated on the next trial. The value for the 
parts cost is 
 
Parts cost 5 80 1 20 3 0.5842 5 80 1 11.68 5 91.68 per unit
With appropriate choices of the lower  and upper bounds, equation (11.3) can be used to 
generate values for any uniform probability distribution. 
Lastly, we need a procedure for generating the first-year demand from computer- 
generated random numbers. Because first-year demand is normally distributed with a mean 
of 15,000 units and a standard deviation of 4500 units (see Figure 11.4), we need a proce-
dure for generating random values from this normal probability distribution. 
Once again we will use random numbers between 0 and 1 to simulate values for first-
year demand. To generate a value for a random variable characterized by a normal distri-
bution with a specified mean and standard deviation, the following Excel formula is used:
Value of normal random variable 5 NORM.INV(RAND( ), mean, standard deviation) (11.5)
For Sanotronics, first-year demand is a normally distributed random variable with a mean 
of 15,000 and a standard deviation of 4500. Applying equation (11.5) leads to the following 
formula for generating the first-year demand:
 
Demand 5 NORM.INV(RAND( ), 15000, 4500) 
(11.6)
Suppose that the random number of 0.6026 is produced by the RAND function; 
 applying equation (11.6) then results in Demand 5 NORM.INV(0.6026, 15000, 4500) 5 
16,170 units. To understand how equation (11.6) uses random numbers to generate nor-
mally distributed values for first-year demand, we note that the Excel expression 5NORM.
INV(0.6026, 15000, 4500) provides the value for a normal distribution with a mean of 
15,000 and a standard deviation of 4500, such that 60.26 percent of the area under the nor-
mal curve is to the left of this value (Figure 11.6). Now suppose that the random number 
produced by the RAND function is 0.3551; applying equation (11.6) then results in Demand 
5 NORM.INV(0.3551, 15000, 4500) 5 13,328 units. Because half of this normal distribu-
tion lies below the mean of 15,000 and half lies above it, RAND values less than 0.5 result 
Versions of excel prior to 
excel 2010 do not recog-
nize the function noRM.
inV; in these earlier ver-
sions of excel, one can use 
the function noRMinV. 
the results will be the 
same.
With appropriate specifica-
tion of the mean and stan-
dard deviation, equation 
(11.5) can be used to gener-
ate values for any normal 
probability distribution. 
FIGURE 11.5   GENERATION OF VALUE FOR PARTS COST PER UNIT 
 CORRESPONDING TO RANDOM NUMBER 0.4576
1
20
80
89.15
Parts Cost per Unit
100
0.4576

494 
Chapter 11 Monte Carlo Simulation
in values of first-year demand below the average of 15,000 units, and RAND values above 
0.5 correspond to values of first-year demand above the average of 15,000 units.
Now that we know how to randomly generate values for the random variables ( direct 
labor cost, parts cost, first-year demand) from their respective probability distributions, 
we modify the spreadsheet by adding this information. The static values in Figure 11.1 
for these parameters in cells B6, B7, and B8 are replaced with cell formulas that will ran-
domly generate values whenever the spreadsheet is recalculated (as shown in Figure 11.7).  
FIGURE 11.6   GENERATION OF VALUE FOR FIRST-YEAR DEMAND 
 CORRESPONDING TO RANDOM NUMBER 0.6026
15,000 16,170
Standard Deviation
  = 4500 units
Number of Units Sold
.6026
FIGURE 11.7   FORMULA WORKSHEET FOR SANOTRONICS
A
C
D
E
F
Sanotronics
Parameters
Selling Price per Unit
Administrative &
Advertising Cost
Direct Labor
Cost per Unit
Parts Cost per Unit
Demand
Lower End of Interval
Model
Proﬁt
Direct Labor Cost
0
=B15
=B16
=B17
=B18
44
43
Cost per Unit Probability
Parts Cost
(Uniform Distribution)
249
1000000
=F14+(F15-F14)*RAND()
=VLOOKUP(RAND(),A15:C19,3,TRUE)
=NORM.INV(RAND(),F18,F19)
=((B4-B6-B7)*B8)-B5
Upper End of Interval
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
80
100
15000
4500
=D15+A15
=D16+A16
=D17+A17
=D18+A18
1
45
46
47
0.2
0.1
0.4
0.2
0.1
Demand
(Normal Distribution)
Lower Bound
Upper Bound
Mean
Standard Deviation
B

 
11.2 Simulation Modeling with Native Excel  Functions 
495
Corresponding to Table 11.1, cell B6 uses a random number generated by the RAND 
function and looks up the corresponding cost per unit by applying the VLOOKUP 
function to the table of intervals contained in cells A15:C19 (which corresponds to 
Table 11.1). Cell B7 executes equation (11.4) using references to the lower bound and 
upper bound of the uniform distribution of the parts cost in cells F14 and F15, respec-
tively.2 Cell B8 executes equation (11.6) using references to the mean and standard 
deviation of the normal distribution of the first-year demand in cells F18 and F19, 
respectively.3 
Executing Simulation Trials with Excel
Each trial in the simulation involves randomly generating values for the random vari-
ables (direct labor cost, parts cost, and first-year demand) and computing profit. To 
facilitate the execution of multiple simulation trials, we use Excel’s Data Table func-
tionality in an unorthodox, but effective, manner. To set up the spreadsheet for the 
execution of 1000 simulation trials, we structure a table as shown in cells A21 through 
E1021 in Figure 11.8. As Figure 11.8 shows, A22:A1021 numbers the 1000 simulation 
trials (rows 25 through 1019 are hidden). To populate the data table in cells A23 through 
E1021, we execute the following steps:
Step 1. Select cell range A22:E1021
Step 2. Click the DATA tab in the Ribbon
Step 3. Click What-If Analysis in the Data Tools group and select Data Table 
Step 4. When the Data Table dialog box appears, leave the Row input cell: box 
blank and enter any empty cell in the  spreadsheet (e.g., D1) into the Column 
input cell: box
Step 5. Click OK
Figure 11.9 shows the results of our simulation. After executing the simulation with 
the data table, each row in this table corresponds to a distinct simulation trial consisting of 
different values of the random variables. In trial 1 (row 20 in the spreadsheet), we see that 
the direct labor cost is $45 per unit, the parts cost is $86.29 per unit, and first-year demand 
is 19,976 units, resulting in profit of $1,351,439. In trial 2 (row 21 in the spreadsheet), we 
observe random variables of $45 for the direct labor cost, $81.02 for the parts cost, and 
14,910 for first-year demand. These values provide a simulated profit of $833,700 on the 
second simulation trial. Note that the values shown when you run your simulation will be 
different due to the random inputs.
Measuring and Analyzing Simulation Output
The analysis of the output observed over the set of simulation trials is a critical part of the 
simulation process. For the collection of simulation trials, it is helpful to compute descrip-
tive statistics such as sample average, sample standard deviation, minimum, maximum, 
for further description of 
the VLooKuP function, 
refer to chapter 7.
file
WEB
Sanotronics
these steps iteratively 
select the simulation trial 
number from the range 
a22 through a1021 and 
substitute it into the blank 
cell selected in step 4 (d1). 
this substitution has no 
bearing on the spreadsheet, 
but it forces excel to recal-
culate the spreadsheet each 
time, thereby generating 
new random numbers with 
the Rand functions in cells 
B6, B7, and B8.
Pressing the f9 key recal-
culates the spreadsheet, 
thereby generating a new 
set of simulation trials.
2Technically, random variables modeled with continuous probability distributions should be appropriately 
rounded to avoid modeling error. For example, the simulated values of parts cost per unit should be rounded to 
the nearest penny. To simplify exposition, we do not worry about the small amount of error that occurs in this 
case. To model these random variables more accurately, the formula in cell B7 should be 5ROUND(F121(F13-
F12)*RAND(),2).
3In addition to being a continuous distribution that technically requires rounding when applied to discrete phenomena (like 
units of medical device demand), the normal distribution also allows negative values. The probability of a negative value 
is quite small in the case of first-year demand, and we simply ignore the small amount of modeling error induced for the 
sake of simplicity. To model first-year demand more accurately, the formula in cell B8 should be 5MAX(ROUND(NORM.
INV(RAND(),F16,F17),0),0).

496 
Chapter 11 Monte Carlo Simulation
FIGURE 11.8   SETTING UP SANOTRONICS SPREADSHEET FOR MULTIPLE SIMULATION TRIALS
A
C
D
E
F
Sanotronics
Parameters
Selling Price per Unit
Administrative &
Advertising Cost
Direct Labor
Cost per Unit
Parts Cost per Unit
Demand
Lower End of Interval
Model
Proﬁt
Direct Labor Cost
Simulation Trial
Direct Labor Cost Per Unit
0
=B15
=B16
=B17
=B18
1
2
3
998
999
1000
Cost per Unit
Parts Cost per Unit
Probability
Parts Cost
(Uniform Distribution)
Demand
(Normal Distribution)
Proﬁt
80
100
249
1000000
=F14+(F15-F14)*RAND()
=VLOOKUP(RAND(),
A15:C19,3,TRUE)
=NORM.INV(RAND(),F18,F19)
=((B4-B6-B7)*B8)-B5
Upper End of Interval
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
20
21
22
23
24
1019
1020
1021
19
15000
4500
=D15+A15
=D16+A16
=D17+A17
=D18+A18
1
=B6
44
43
45
46
47
=B7
Demand
=B8
=B11
0.2
0.1
0.4
0.2
0.1
Lower Bound
Upper Bound
Mean
Standard Deviation
B
and sample proportion. To compute these statistics for the Sanotronics example, we use 
the following Excel functions:
Cell H22  5AVERAGE(E22:E1021)
 
Cell H23  5STDEV.S(E22:E1021)
 
Cell H24  5MIN(E22:E1021)
 
Cell H25  5MAX(E22:E1021)
 
Cell H26  5COUNTIF(E22:E1021,“,0”)/COUNT(E22:E1021)
Cell H26 computes the ratio of the number of trials whose profit is less than zero over the 
total number of trials. Note by changing the value of the second argument in the COUNTIF 
function, the probability that the profit is less than any specified value can be computed in 
cell H26.
As shown in Figure 11.9, we observe a mean profit of $717,663, standard deviation of 
$521,536, extremes ranging between 2$996,547 and $2,253,674, and a 0.078 estimated 
probability of a loss. 
excel versions prior 
to  excel 2010 do not 
 recognize the stdeV.s 
function; in these versions 
of excel, one can use the 
function stdeV. the 
results will be the same. 
simulation studies enable 
an objective estimate of the 
probability of a loss, which 
is an important aspect of 
risk analysis.

 
11.2 Simulation Modeling with Native Excel  Functions 
497
FIGURE 11.9   OUTPUT FROM SANOTRONICS SIMULATION
A
Sanotronics
B
C
D
Parameters
Selling Price
per Unit
Administrative &
Advertising Cost
Direct Labor
Cost Per Unit
Parts Cost
Per Unit
Demand
Simulation Trial
Direct
Labor Cost
Per Unit
Parts
Cost
Per Unit
Demand
Profit
Profit
Summary
Statistics
Mean
Bin
Frequency
Standard
Deviation
Minimum
Profit
Maximum
Profit
P(Profit < $)
$1,000,000
$249
$45
$86.29
19,976
$1,351,439
Lower End
of Interval
0.1
0.3
0.7
0.9
0.0
Upper End
of Interval
0.3
0.7
0.9
1.0
0.1
Cost per
Unit
$44
$45
$46
$47
$43
Probability
0.2
0.4
0.2
0.1
Mean
Lower
Bound
Upper
Bound
Parts Cost
(Uniform
Distribution)
Demand
(Normal
Distribution)
Standard
Deviation
0.1
$100
$80
4,500
15,000
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
1,000
$45
$45
$46
$45
$47
$45
$44
$47
$44
$45
$45
$45
$47
$44
$46
$45
$45
$45
$47
$43
$47
$45
$86.29
$81.02
$98.15
$92.29
$88.82
$95.98
$88.23
$96.20
$85.97
$89.62
$85.96
$92.06
$85.34
$80.05
$92.47
$81.61
$84.16
$93.07
$85.33
$83.58
$92.23
$92.87
19,976
14,910
18,570
12,561
6,844
15,337
18,723
17,589
19,967
14,056
11,204
11,150
11,880
24,733
10,933
17,453
13,205
15,809
9,422
13,599
17,168
22,467
$717,663
$521,536
–$996,547
$2,253,674
0.078
$0
–$1,500,000
–$1,250,000
–$1,000,000
–$750,000
–$500,000
–$250,000
$250,000
$500,000
$750,000
$1,000,000
$1,250,000
$1,500,000
$1,750,000
$2,000,000
$2,250,000
$2,500,000
$2,750,000
$3,000,000
0
0
0
3
4
22
49
113
151
188
193
122
79
47
20
7
2
0
0
0
$1,351,439
$833,700
$947,064
$403,085
–$225,345
$656,778
$1,186,276
$861,005
$1,376,760
$607,650
$322,448
$248,172
$385,901
$2,090,469
$208,447
$1,136,087
$582,483
$753,735
$99,247
$664,800
$884,578
$1,496,677
Model
Profit
Direct Labor
Cost
E
F
G
H
I
J
K
0
50
100
150
200
250
Profit Distribution
–$1,500,000
–$1,250,000
–$1,000,000
–$750,000
–$500,000
–$250,000
$0
$250,000
$500,000
$750,000
$1,000,000
$1,250,000
$1,500,000
$1,750,000
$2,000,000
$2,250,000
$2,500,000
$2,750,000
$3,000,000
1
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
20
19
23
22
24
25
26
27
28
29
30
21
31
35
34
36
37
38
39
40
1021
41
42
32
33

498 
Chapter 11 Monte Carlo Simulation
To visualize the distribution of profit on which these descriptive statistics are based, we 
create a histogram using the FREQUENCY function and a column chart. We note that the 
distribution of profit values is fairly symmetric, with a large number of values in the range 
of $250,000 to $1,250,000. The probability of a large loss or a large gain is small. Only 
7 trials out of 1000 resulted in a loss of more than $500,000, and only 9 trials resulted in a 
profit greater than $2,000,000. The bin with the largest number of values has profit ranging 
between $750,000 and $1,000,000.
In comparing the simulation approach to the manual what-if approach, we observe that 
much more information is obtained by using simulation. Recall from the what-if analysis 
in Section 11.1, we learned that the base-case scenario projected a profit of $710,000. The 
worst-case scenario projected a loss of $1,000,000, and the best-case scenario projected a 
profit of $2,591,000. From the 1000 trials of the simulation that have been run, we see that 
the worst- and best-case scenarios, although possible, are unlikely. Indeed, the advantage 
of simulation for risk analysis is the information it provides on the likely values of the out-
put. We now know the probability of a loss, how the profit values are distributed over their 
range, and what profit values are most likely.
The simulation results help Sanotronics’ management better understand the profit/loss 
potential of the new medical device. The 0.078 probability of a loss may be acceptable to 
management. On the other hand, Sanotronics might want to conduct further market research 
before deciding whether to introduce the product. In any case, the simulation results should 
be helpful in reaching an appropriate decision.
for a detailed description 
of the fReQuencY func-
tion and creating charts 
in excel, see chapters 2 
and 3. 
NOTES AND COMMENTS
1. In general, the value k of a random variable X 
corresponding to a computer-generated random 
number r between 0 and 1 is the smallest value 
k such that P(X # k) $ r. 
2. In the preceding section, we showed how to gen-
erate values for random variables from a generic 
discrete distribution, a uniform distribution, and 
an normal distribution. Generating values from 
normally distributed random variable required 
the use of NORM.INV and the RAND func-
tions. Similarly, the RAND function can be used 
with the Excel functions BETA.INV, BINOM.
INV, GAMMA.INV, and LOGNORM.INV to 
generate values for a random variable with a 
beta distribution, binomial distribution, gamma 
distribution, or lognormal distribution, respec-
tively. Using a different probablity distribution 
for a random variable simply changes the rela-
tive likelihood of the random variable realizing 
certain values. The choice of probability distri-
bution to use for a random variable should be 
based on historical data and knowledge of the 
analyst. We discuss other probability distribu-
tions in Appendix 11.2.
3. The exponential distribution is a common dis-
tribution for random variables corresponding to 
the interarrival times of customers, the service 
times of customers, and so forth. Although there 
is no direct Excel function, entering the formula 
5LN(RAND())*(-m) into a cell will generate a 
value for an exponential random variable with 
mean m.
4. The Excel function RANDBETWEEN generates 
integer values between lower and upper bounds 
(this corresponds to values from a discrete uni-
form distribution as described in Appendix 11.1).
Simulation Modeling with 
analytic Solver platform
Because simulation is one of the most widely used business analytics techniques, various 
software tools exist to facilitate simulation modeling. With two examples, we demonstrate 
the process of building a spreadsheet simulation model with the Excel add-in Analytic 
Solver Platform (ASP). This Excel add-in facilitates the generation of random values from a 
variety of probability distributions, eases the process of executing simulation trials (no need 
11.3

 
11.3 Simulation Modeling with Analytic Solver Platform 
499
for the data tables presented in the previous section), and offers a wide array of measures 
and charts describing the simulation output.
The Land Shark Problem
Land Shark is a real estate company that purchases properties that it develops and then 
resells. In the past, Land Shark has successfully acquired properties via first-price sealed-
bid auctions involving commercial and residential properties. In a first-price sealed-bid 
auction, each bidder submits a single concealed bid. The submitted bids are then compared, 
and the party with the highest bid wins the property and pays the bid amount. In case of a 
tie (a rare occurrence), a coin flip decides the winner.
Land Shark has been reviewing upcoming property auctions and has identified a com-
mercial property of interest. Land Shark estimates the value of this property to be $1,389,000. 
Using bidding data disclosed to the public, Land Shark has maintained a file summarizing 
previous auctions. Table 11.2 displays bid data on 13 recent auctions that Land Shark believes 
are similar to the upcoming property auction. To make the auctions comparable, Land Shark 
expresses the submitted bid amounts as a percentage of the respective property’s value (as 
estimated by Land Shark). As Table 11.2 shows, the number of bids submitted for any par-
ticular auction has ranged from two to eight. Land Shark is considering a bid of $1,250,000 
but would like to evaluate its chances of winning the upcoming auction with this bid. 
Spreadsheet Model for Land Shark
To evaluate Land Shark’s chances of winning the auction, we develop a simulation model 
for the auction. Our first step in modeling the upcoming property auction is to identify the 
input parameters and output measures. The next step is to develop a spreadsheet model that 
conveys the logical relationships between the input parameters and the output measures. 
Then we prepare the spreadsheet model for simulation analysis by replacing the static val-
ues of the input parameters that Land Shark does not know with certainty with probability 
distributions of possible values. 
The relevant input parameters for the upcoming auction are the estimated value of the 
property, the number of bidders, and the submitted bid amounts. The output that we are 
interested in is whether Land Shark wins the simulated auction given its specified amount 
and Land Shark’s net return. If Land Shark wins the auction, its return is computed as the 
TABLE 11.2  BID DATA ON COMMERCIAL PROPERTY AUCTIONS
Bid Amount (as a Fraction of Estimated Property Value)
Property 
Number
Bid 1
Bid 2
Bid 3
Bid 4
Bid 5
Bid 6
Bid 7
Bid 8
 1
0.830
0.797
0.833
0.878
0.839
0.843
 2
0.835
0.823
0.781
0.892
0.767
0.787
 3
0.763
0.862
0.814
0.895
 4
0.771
0.859
0.867
0.850
0.833
 5
0.836
0.898
0.831
0.897
0.831
0.657
0.846
 6
0.850
0.863
0.825
0.910
0.848
 7
0.890
0.820
0.874
0.877
0.818
 8
0.804
0.881
0.786
0.884
0.773
0.819
0.824
 9
0.819
0.851
0.786
0.896
0.784
0.792
10
0.860
0.756
0.876
0.887
0.866
11
0.880
0.834
0.831
0.871
0.857
0.759
12
0.810
0.870
13
0.887
0.716
0.817
 0.9
0.869
0.885
0.856
0.761

500 
Chapter 11 Monte Carlo Simulation
difference between the estimated value of the property and its bid amount. If Land Shark 
does not win the auction, its return is $0. 
Whether Land Shark wins the simulated auction can be determined by comparing Land 
Shark’s bid amount to the largest competitor bid amount. Based on the data in Table 11.2, 
we assume that the number of submitted bid amounts may range from two to eight. There-
fore, to determine the largest competitor bid amount, the spreadsheet model must be able 
to compute the maximum bid from a varying number of bids. 
Figure 11.10 shows the formula view and value view of the spreadsheet model. Cell B4 
contains the estimated value of the property (Land Shark is relatively certain of this value), 
cell B5 contains a value for the number of bidders (a random variable), and cells B8 through 
B15 contain values of eight possible competing bids expressed as percentages of the prop-
erty’s estimated value (also random variables). Cells C8 through C15 express these bids in 
dollars but account for the possibility that the number of bids may be lower than the eight pos-
sible listed in cells B8 through B15 using the IF function. For example, consider the formula 
in cell C8, 5IF(A8>$B$5,0,B8*$B$4). It compares the bid number in cell A8 to the number 
of bidders in cell B5 and if the bid number exceeds the number of bidders, a bid amount of 
$0 is calculated so that the bid is not considered. Otherwise, the bid amount is calculated by 
multiplying the bid percentage (cell B8) by the estimated value of the property (cell B4).
Cell B18 contains Land Shark’s bid amount (highlighted in gray to denote that this 
is a controllable decision). Cell B19 computes the largest competitor bid by taking the 
maximum value over the range C8:C15. Land Shark tracks two output measures: whether 
it wins the auction and the return from the auction. By comparing Land Shark’s bid amount 
in cell B18 to the largest competitor bid in cell B19, the logic 5IF(B18>B19,1,0) in cell 
B20 indicates that Land Shark wins the auction by returning the value of 1 and otherwise 
returning the value of 0 if Land Shark loses the auction. The value of 1 or 0 in Cell B20 
to denote a Land Shark win or loss allows the simulation model to count the number of 
times Land Shark wins the auction over a set of simulation trials. The formula in cell B21, 
5B20*(B4-B18), computes the return from the auction; if Land Shark wins the auction, 
the return is equal to the estimated value minus the bid amount, otherwise the return is zero 
because the value of cell B20 will be zero.
Generating Values for Land Shark’s Random Variables
ASP provides the analyst flexibility in characterizing the possible values of the random 
variables in a simulation model by providing a gallery of probability distributions. ASP 
uses the term uncertain variable to refer to a random variable. These terms are synonyms 
referring to an uncertain quantity in a simulation model. We next demonstrate the use of 
ASP to specify probability distributions for the Land Shark problem.
First consider the number of bidders. The number of bidders has ranged from two to 
eight over the past 13 auctions.  Unless Land Shark has reason to believe that there may be 
fewer than two bids on an upcoming auction, it is probably safe to assume that there will 
be a minimum of two competing bids. There has not been an auction with more than eight 
bidders, so eight is a reasonable assumption for the maximum number of competing bids 
unless Land Shark’s experience with the local real estate market suggests that more than 
eight competing bids is possible.
With only 13 data points, there is limited information on the relative likelihood of dif-
ferent values for the number of bidders in the range from two to eight. Due to this lack of 
information, Land Shark decides to use an integer uniform distribution in which the number 
of bidders is equally likely to be 2, 3, 4, 5, 6, 7, or 8. To implement this probability distribu-
tion in the Land Shark simulation model, we follow these steps:
Step 1. Select cell B5 in the Model worksheet (corresponding to the number of  bidders) 
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Distributions in the Simulation Model group
for a detailed discus-
sion of the if function in 
excel, see chapter 7; for a 
discussion of relative and 
absolute cell references, 
see appendix a. 
appendix 11.2 provides an 
overview of many of the 
various probability distri-
butions used in asP.
note that if Land shark 
believes an upcoming 
auction could have fewer 
than two competing bids 
(or more than eight), it can 
easily adjust the distribu-
tion describing the number 
of bidders to determine its 
effect on the simulation 
output. 
file
WEB
LandShark

 
11.3 Simulation Modeling with Analytic Solver Platform 
501
Step 4. Select Discrete and click IntUniform
Step 5. When the $B$5 dialog box appears (Figure 11.11), in the Parameters area:
Enter 2 in the box to the right of lower 
Enter 8 in the box to the right of upper 
Step 6 Click Save
instead of using 
the asP Ribbon 
buttons, the Solver 
Options and Model 
Specifications pane 
that appears on the 
right side of the 
spreadsheet can also 
be used to directly 
set up a simulation 
and/or optimization 
model.
Placing the mouse 
pointer over a cell 
that contains an asP 
probability distribu-
tion displays a min-
idialog box with a 
chart that illustrates 
the distribution in 
the cell. clicking the 
upper right-hand 
corner of this box 
will open a larger 
dialog box for this 
distribution
FIGURE 11.10   BASE SPREADSHEET MODEL FOR LAND SHARK
A
C
Land Shark
Parameters
Estimated Value
Number of Bidders
Bid Number
1
2
7
8
3
4
6
5
Model
Land Shark Bid Amount
Land Shark Win Auction?
Land Shark Return
Largest Competitor Bid
Bid (% of Estimated Value)
Bid Amount
1389000
4
=IF(A15>$B$5,0,B15*$B$4)
=IF(A14>$B$5,0,B14*$B$4)
=IF(A9>$B$5,0,B9*$B$4)
=IF(A10>$B$5,0,B10*$B$4)
=IF(A13>$B$5,0,B13*$B$4)
=IF(A12>$B$5,0,B12*$B$4)
=IF(A8>$B$5,0,B8*$B$4)
=IF(A11>$B$5,0,B11*$B$4)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
22
20
0.716
0.817
0.885
0.869
0.887
0.900
0.856
0.761
1230000
=IF(B18>B19,1,0)
=B20*(B4-B18)
=MAX(C8:C15)
B
A
C
Land Shark
Parameters
Estimated Value
Number of Bidders
Bid Number
1
2
7
8
3
4
6
5
Model
Land Shark Bid Amount
Land Shark Win Auction?
Land Shark Return
Largest Competitor Bid
$1,389,000
Bid (% of Estimated Value)
Bid Amount
4
$0
$0
$994,524
$1,134,813
$0
$0
$1,232,043
$1,250,100
0.716
0.817
0.885
0.869
0.887
0.900
0.856
0.761
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
0
$0
$1,230,000
$1,250,100
B
file
WEB
LandShark

502 
Chapter 11 Monte Carlo Simulation
Each competitor’s bid percentage is also a random variable. From the past 13 auc-
tions, Land Shark has gathered 72 observations of how competitors have bid (as a 
percentage of the estimated value). Land Shark believes that these 72 bid values are an 
accurate representation of distribution of future bids. Thus, we will simulate the bids 
for the upcoming auction by randomly selecting a value from one of these 72 bid values 
using the DisUniform distribution in ASP. When using the DisUniform distribution on 
sample data, we note that only values that exist in the data will be possible values for 
a simulation trial. Resampling empirical data is a good approach only when the data 
adequately represents the range of possible values and the distribution of values across 
this range.
The following steps demonstrate how to simulate values of competing bids by resam-
pling empirical data:
Step 1. Select cell B8 in the Model worksheet 
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Distributions in the Simulation Model group 
 
Select Custom, and click DisUniform
Step 4. When the $B$8 dialog box appears (Figure 11.12), enter data!$a$19:$a$90 
in the box to the right of values in the Parameters area
Step 5. Click Save
Step 6. Copy cell B8 and paste into cells B9 through B15
Note that the DisUniform distribution randomly resamples values from a list of  provided 
sample values. This does not mean the resulting values are uniformly distributed across a 
range but rather that the generated values will have an empirical distribution similar to the 
sample on which they are based. For example, Figure 11.12 illustrates that values between 
0.75 and 0.90 are far more likely than values less than 0.75 and greater than 0.90. This is dif-
ferent from a continuous uniform distribution (as shown in 11.3), in which values between 
a lower and upper bound are equally likely.
if the sample data does 
not adequately character-
ize the set of values for a 
random variable, it may 
be more appropriate to fit 
a distribution using the 
Fit procedure in the Tools 
group of the ANALYTIC 
SOLVER PLATFORM 
tab. We demonstrate fit-
ting a distribution for the 
 Zappos problem later in 
this chapter.
the argument 
Data!$A$19:$A$90 in 
step 4 tells asP to look 
in cells a19:a90 of the 
Data worksheet to find 
the  possible values for the 
random variable.
FIGURE 11.11   ENTERING AN INTEGER UNIFORM DISTRIBUTION
0.14
90.00%
5%
2
8
0.12
0.1
0.08
0.06
0.04
0.02
0
2
3
4
5
6
7
8
5%
the Distribution 
 Wizard option under the 
 Distributions button in the 
Simulation Model group of 
the ANALYTIC SOLVER 
PLATFORM tab guides 
the user through the steps 
of identifying an appropri-
ate probability distribution 
for a random variable.

 
11.3 Simulation Modeling with Analytic Solver Platform 
503
Tracking Output Measures for Land Shark
After defining all the random variables with ASP, we are ready to use ASP to track output 
measures. For Land Shark, one output measure of interest is whether Land Shark wins a simu-
lated auction. To record this output over the simulation trials, we apply the following steps: 
Step 1. Select cell B20 in the Model worksheet (corresponding to whether Land Shark 
won the auction) 
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Results in the Simulation Model group
Step 4. Select Output, and click In Cell
This procedure appends the formula in cell B20 with “1PsiOutput()”—which triggers ASP 
to record the cell’s value for each of the simulation trials. By collecting the value of an output 
measure for each of 10,000 trials, ASP can then create a distribution of the output measure. 
Repeating these steps for cell B21 (corresponding to Land Shark’s return on the auction) 
tracks the other output measure of interest, Land Shark’s return from a simulated auction. 
ASP also explicitly tracks statistics of the output measures and records them in a speci-
fied cell. For the Land Shark problem, we track the mean of the cell B20, which indicates 
whether Land Shark won the simulated auction using the following steps:
Step 1. Select cell B20 in the Model worksheet
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Results in the Simulation Model group
Select Statistic and click Mean
When the dialog bubble =PsiMean(B20) Please select the cell where 
the statistic will be placed appears, select cell B23
These steps instruct ASP to compute the mean of the value of cell B20 over the simula-
tion trials and place it in cell B23. Computing the mean of a function that is one if Land 
Shark wins the auction and zero if Land Shark loses the auction is equivalent to counting 
the number of trials in which Land Shark wins the simulated auction and dividing by the 
upon completion of these 
steps, excel will display 
“#n/a” in cell B23. asP 
will not populate the value 
for cell B23 until a simula-
tion is executed. We explain 
how to execute a simulation 
in the next section. 
FIGURE 11.12   RESAMPLING DATA TO SIMULATE COMPETITOR BIDS
90.00%
5.00%
0.897
0.759
0.012
0.010
0.008
0.006
0.004
0.002
0
0.650
0.700
0.750
0.800
0.850
0.900
5.00%

504 
Chapter 11 Monte Carlo Simulation
number of auctions; in other words, this mean (or expected value) corresponds to a prob-
ability estimate that Land Shark wins the upcoming auction. 
To track the mean of Land Shark’s return from the set of simulated auctions, we execute 
the following steps:
Step 1. Select cell B21 in the Model worksheet
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Results in the Simulation Model group
Select Statistic and click Mean
When the dialog bubble =PsiMean(B21) Please select the cell where 
the statistic will be placed appears, select cell B24
Figure 11.13 shows the formula view of the Land Shark simulation model after its construc-
tion via ASP.
Executing Simulation Trials and Analyzing Output  
for Land Shark
ASP provides many user options for executing a simulation. For the Land Shark problem, 
the following steps and Figure 11.14 illustrate how to set the number of trials to 10,000. 
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 2. Click the Options icon in the Options group
Step 3. Click the Simulation tab. In the General area, enter 10000 in the Trials per 
Simulation: box
Step 4. Click OK
upon completion of these 
steps, excel will display 
“#n/a” in cell B24. asP 
will not populate the value 
for cell B24 until a simula-
tion is executed. We explain 
how to execute in the next 
section.
clicking on the arrow 
under the Options icon 
opens a drop-down menu. 
clicking All Options from 
this menu is equivalent to 
clicking on the Options 
icon.
FIGURE 11.13   LAND SHARK SIMULATION MODEL
A
C
Land Shark
Parameters
Estimated Value
Number of Bidders
Bid Number
1
2
7
8
3
4
6
5
Model
Land Shark Bid Amount
Land Shark Win Auction?
Land Shark Return
P(Land Shark Wins Auction):
Expected Return:
Largest Competitor Bid
1389000
Bid (% of Estimated Value)
Bid Amount
=PsiIntUniform(2,8)
=IF(A15>$B$5,0,B15*$B$4)
=IF(A14>$B$5,0,B14*$B$4)
=IF(A9>$B$5,0,B9*$B$4)
=IF(A10>$B$5,0,B10*$B$4)
=IF(A13>$B$5,0,B13*$B$4)
=IF(A12>$B$5,0,B12*$B$4)
=IF(A8>$B$5,0,B8*$B$4)
=IF(A11>$B$5,0,B11*$B$4)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
=PsiDisUniform(Data!$A$19:$A$90)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
23
24
1230000
=IF(B18>B19,1,0) + PsiOutput()
=B20*(B4-B18) + PsiOutput()
=PsiMean(B21)
=PsiMean(B21)
=MAX(C8:C15)
B

 
11.3 Simulation Modeling with Analytic Solver Platform 
505
The following steps describe how to execute the set of 10,000 simulation trials, to 
analyze simulation output, and then to interactively (and near instantaneously) observe 
the effect of varying Land Shark’s bid amount (or any fixed value in the spreadsheet 
model).
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 2. Click the arrow under Simulate from the Solve Action group
 
From the drop-down menu that appears, select Interactive
Step 3. When the Chart Wizard dialog box appears (Figure 11.15), double-click 
on the $B$21 histogram in the top row to active the $B$21 dialog box 
(Figure 11.16)
Step 4. Select cell B18 and enter 1300000 (Figure 11.17)
This histogram for $B$21 displays the distribution of Land Shark’s return over the 
10,000 simulation trials. As Figure 11.16 illustrates, if Land Shark bids $1,230,000 on 
the property, it has a probability of approximately 0.4931 of winning the auction and an 
expected return of $78,403. Figure 11.17 shows that if Land Shark increases its bid amount 
to $1,300,000, its estimated probability of winning the auction increases to 1.00 and its 
 expected return increases to $89,000. 
Increasing the number of trials per simulation reduces the error of the estimates of the 
output. Unless the simulation model is extremely complex, it is recommended to use 10,000 
trials (the maximum allowed in the educational version of ASP). To get an idea of how 
much noise is in the output statistics, rerun the simulation, and observe how much output 
When there is more than 
one uncertain function in 
the model, the Chart  Wizard 
dialog box will appear and 
display histograms of up 
to six cells corresponding 
to uncertain functions and 
uncertain variables. 
Recall that when you run 
your simulation in the 
 Landshark file, the values 
you see will be different. 
this is to be expected when 
running a simulation. each 
time the simulation is run, 
the values may vary some-
what due to different random 
numbers being used.
When interactive simula-
tion in asP is activated, the 
spreadsheet will automati-
cally rerun the simulation to 
evaluate changes to any of 
the values in the spreadsheet. 
FIGURE 11.14   SIMULATION OPTIONS MENU

506 
Chapter 11 Monte Carlo Simulation
FIGURE 11.15   CHART WIZARD DIALOG BOX AFTER RUNNING LAND SHARK SIMULATION WITH 
ANALYTIC SOLVER PLATFORM
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
$B$11
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
$B$12
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
$B$13
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
$B$8
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
$B$9
0.6
0.7
0.8
0.9
1
0
0.05
0.1
0.15
0.2
$B$10
0
1
0
20000
40000
60000
$B$20
0
159000
0
20000
40000
60000
$B$21
2
0
5000
10000
15000
$B$5
3
4
5
6
7
8
statistics change. For example, in the Land Shark problem, the probability that Land Shark 
wins the auction when bidding $1,250,000 is consistently between 0.4900 and 0.5050, 
whereas the expected return is consistently between $77,500 and $78,500, suggesting that 
the 10,000 trials is sufficient for obtaining a precise estimate.
A complete version of the simulation model for the Land Shark problem can be found 
in the file LandsharkModel.
The Zappos Problem
Zappos is a retailer of women’s clothing, accessories, and beauty products. Suppose that 
the regional purchasing manager for Zappos is in the process of determining the order 
quantities for a new product line of women’s sleepwear for the upcoming holiday season. 
The new product line consists of four types of pajamas: cotton, flannel, silk, and velour. 
An initial order quantity of 10,000 of each type of pajama has been proposed, but we want 
to evaluate this decision with a simulation model. 
Pressing F9 in excel will 
rerun the specified number 
of simulation trials (10,000 
in our example).
file
WEB
LandSharkModel

 
11.3 Simulation Modeling with Analytic Solver Platform 
507
The decision of how much of each pajama type to purchase depends on the pricing and 
demand for each. For each pajama type, Table 11.3 lists the wholesale price (the price at 
which Zappos procures the pajama from its supplier), the retail price (the price at which 
Zappos sells the pajama during the holiday season), and the clearance price (the price at 
which Zappos liquidates the inventory remaining after the holiday season). To help estimate 
the demand for each pajama type, Zappos’ marketing department has gathered a repre-
sentative sample of past sales for products similar to the cotton and flannel pajama types. 
Representative data for the silk and velour pajamas are not available, so we must work with 
the marketing managers to gain insight on the demand for these products. 
Spreadsheet Model for Zappos
Our first step in modeling Zappos’ ordering decision is to identify the input parameters and 
output measures. The next step is to develop a spreadsheet model that conveys the logical 
relationships between the input parameters and the output measures. Then, we prepare the 
spreadsheet model for simulation analysis by replacing the static values of the input parame-
ters that Zappos does not know with certainty with probability distributions of possible values. 
The relevant input parameters for Zappos are the wholesale price, retail price, clearance 
price, order quantity, and the retail demand for each of the four pajama types. Total profit 
FIGURE 11.16   LAND SHARK SIMULATION OUTPUT
A
Land Shark
B
C
D
Parameters
Estimated Value
Land Shark Bid Amount
Number of Bidders
Bid Number
Bid (% of
Estimated Value)
Largest Competitor Bid
Land Shark Win Auction?
Land Shark Return
P(Land Shark Wins Auction):
Expected Return:
1
2
3
4
5
6
7
8
$1,389,000
8
0.787
0.797
0.819
0.763
0.896
0.887
0.856
$1,232,043
$1,188,984
Bid Amount
$1,182,039
0.851
$1,093,143
$1,107,033
$1,137,591
$1,059,807
$1,244,544
Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
23
22
24
25
26
20
21
E
F
G
H
I
J
K
L
M
$1,244,544
0
$0
0.4931
$78,403
$1,230,000
$0
$20 $40 $60
(Values in Thousands)
Simulation Results - $B$21
$80 $100 $120 $140 $160
0
1000
2000
Frequency
Relative Probability
3000
4000
5000
0.00
0.10
0.20
0.30
0.40
0.50

508 
Chapter 11 Monte Carlo Simulation
TABLE 11.3  PAjAMA PRICE INFORMATION
Wholesale Price
Retail Price
Clearance Price
Cotton Pajama
Flannel Pajama
Silk Pajama
Velour Pajama
$25
$25
$35
$30
$30
$40
$60
$55
$10
$10
$30
$20
FIGURE 11.17   OBSERVING THE IMPACT OF CHANGING BID AMOUNT WITH 
INTERACTIVE  SIMULATION
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
25
26
A
C
Land Shark
Parameters
Estimated Value
Number of Bidders
Bid Number
1
2
7
8
3
4
6
5
0.856
0.843
0.863
0.834
Model
Land Shark Bid Amount
Land Shark Win Auction?
Land Shark Return
Largest Competitor Bid
Bid (% of Estimated Value) Bid Amount
$1,389,000
5
0.823
0.881
$0
$0
$1,188,984
$1,170,927
$0
$1,158,426
$1,143,147
$1,223,709
0.881
0.862
P(Land Shark Win Auction):
Expected Return
$89,000
$1,300,000
$1,223,709
1
$89,000
1
B
D
E
F
G
$89
$89
$89
$89
(Values in Thousands)
Simulation Results - $B$21
$89
$89
$89
$89
$89
0
2000
4000
Frequency
Relative Probability
6000
8000
10000
0.00
0.20
0.40
0.60
0.80
1.00
$89
$89
is the output measure of interest that we want to compute for given values of the prices, 
order quantity, and demand. 
As a step toward computing total profit, we first consider how to compute the retail 
sales volume. The retail sales volume for a pajama type depends on the demand and the 
order quantity.
 
Retail sales 5 minimum {demand, order quantity} 
(11.7)
In other words, the amount of a pajama type sold at retail price is the minimum of the 
amount demanded and the amount available.

 
11.3 Simulation Modeling with Analytic Solver Platform 
509
Pajamas not sold at retail price during the selling season are sold at their respective 
clearance prices in the weeks after the holidays. The clearance sales volume for a pajama 
type is computed as:
 
Clearance sales 5 order quantity 2 retail sales 
(11.8)
Profit corresponding to the sales of a pajama type is computed from the retail sales and 
clearance sales by multiplying each by their corresponding profit margin. The profit margin 
for a retail sale is the retail price (pr) minus the wholesale price (pw). The profit margin for 
a clearance sale is the clearance price (pc) minus the wholesale price (pw). Thus, the profit 
resulting from a pajama type is:
 
Profit 5 retail sales 3 ( pr 2 pw) 1 clearance sales 3 ( pc 2 pw) 
(11.9)
Figure 11.18 shows the Excel formula logic of the spreadsheet model. Cells H5 through H8 
apply equation (11.7) to compute the retail sales volume for each pajama type. Cells I5 through 
I8 apply equation (11.8) to compute the clearance sales volume for each pajama type. Cells j5 
FIGURE 11.18   ZAPPOS SPREADSHEET MODEL
A
C
Zappos
Parameters
Cotton Pajama
Flannel Pajama
Silk Pajama
Velour Pajama
Values
Silk Pajama
Demand
Maximum value
Minimum value
Most likely value
Maximum value
10000
15000
5000
25000
20000
Wholesale
Price
30000
0
60
40
30
55
Clearance
Price
30
10
10
20
10000
10000
10000
=(H5*(C5-B5))
+(I5*(D5-B5))
=(H6*(C6-B6))
+(I6*(D6-B6))
=(H7*(C7-B7))
+(I7*(D7-B7))
=(H8*(C8-B8))
+(I8*(D8-B8))
=SUM(J5:J8)
=G5-H5
=G6-H6
=G7-H7
=G8-H8
=MIN
(E5,G5)
=MIN
(E6,G6)
=MIN
(E7,G7)
=MIN
(E8,G8)
Retail
Sales
Clearance
Sales
Total
Proﬁt:
Mean Total
Profit:
Proﬁt
Order
Quantity
Model
10000
Retail
Price
35
25
25
30
Demand
10000
15000
25000
5000
Likelihood
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
25
26
Velour Pajama
Demand
Minimum value
0.20
0.40
0.10
0.25
0.05
2500
10000
25000
B
D
E
F
G
H
I
J
file
WEB
Zappos

510 
Chapter 11 Monte Carlo Simulation
through j8 apply equation (11.9) to compute the profit corresponding to each pajama type. We 
compute the total profit across all pajama types with the Excel formula 5SUM(j5:j8) in cell 
j9; this is the output measure that Zappos wants to track. Cells G5 through G8 contain the order 
quantities (highlighted in gray to denote that these are controllable decisions).
Modeling Random Variables for Zappos
Cells E5 through E8 of Figure 11.18 contain values for the demand for each pajama type. The 
demand for each pajama type is a random variable whose value is not known with certainty 
at the time Zappos must determine their order quantities. In this section, we demonstrate 
several different ways to model the uncertainty in the demand for the four pajama types. 
The cotton and flannel pajamas are similar to other products that Zappos has sold in the 
past. The marketing department has compiled representative samples of cotton and flannel 
pajama demand, respectively. These data are provided in the worksheet labeled data in the 
Zappos file. The following steps demonstrate how to use ASP to fit a probability distribu-
tion to characterize the demand for cotton pajamas. 
Step 1. Select cells B2:B25 in the data worksheet
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 3. Click Fit in the Tools group
Step 4. When the Fit Options dialog box appears, click Fit (Figure 11.19)
At this point, a Fit Results dialog box appears (Figure 11.20). In the left pane of the 
 Fit  Results dialog box, ASP lists the distributions that can be fit to the data in order of 
FIGURE 11.19   FIT OPTIONS DIALOG BOX FOR COTTON PAjAMAS 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
25
26
A
C
Period
Cotton
2
1
14
13
12
11
10
9
8
7
6
5
4
3
16
17
15
19
20
21
24
22
18
23
13,311
21,311
23,058
18,969
23,841
22,553
19,315
25,218
17,279
22,882
21,710
17,994
17,573
15,571
18,333
25,657
22,497
21,443
21,659
17,334
20,018
20,409
16,013
14,191
Flannel
15,052
6,800
9,651
12,327
3,454
9,563
12,241
3,235
16,298
7,052
9,978
12,601
9,145
14,290
12,006
6,580
9,494
3,401
9,684
15,241
19,839
5,252
12,848
20,398
B
D
E
F
G
H

 
11.3 Simulation Modeling with Analytic Solver Platform 
511
 decreasing goodness-of-fit. ASP attempts to fit many distributions to the data, including 
several exotic distributions. To view the fit of a distribution, click the box next to the distri-
bution. For example, Figure 11.20 shows that, by clicking the box next to the Normal and 
BetaGen distributions, we can visually compare the normal and beta distributions to the 
statistically best-fitting minimum extreme value (MinExtreme) distribution. 
It is important to emphasize that the analyst should also take into consideration 
qualitative factors in selecting a distribution (particularly when fitting a distribution 
to a small sample of data). In this case, we may be concerned that the shape of the 
minimum extreme value distribution does not accurately capture the possibility of high 
cotton pajama demand. Cotton pajamas are the most popular type of pajamas. Although 
the actual demand for cotton pajamas is uncertain, we may believe that high levels of 
demand are just as likely as low levels of demand. By clicking on Beta Generalized in 
the right pane of the Fit Results dialog box, we can view statistical information about 
the fitted beta distribution. This fitted beta distribution has a minimum value of 12,431 
and a maximum value of 26,096. If we want to allow levels of demand outside this 
range, then the beta distribution is not appropriate (or should be adjusted). In this case, 
we believe that the fitted normal distribution appropriately captures the behavior of 
cotton pajama demand. Regardless of the choice, it is generally a good idea to test the 
impact of the choice of distribution on the output measures by running the simulation 
with various distributions for the random variables. To proceed with the selection of 
the normal distribution for cotton pajama demand, we deselect the other distributions 
so that Normal is the only distribution selected in the left pane of the Fit Results dia-
log box. To complete the process of modeling the cotton pajama demand as a random 
variable, follow these steps:
Step 5. In the left pane of the Fit Results dialog box, make sure that Normal is the 
only distribution selected
Step 6. Close the Fit Results dialog box by clicking the 
 in the upper right corner 
of the dialog box 
Step 7. Click Yes in response to the query Do you wish to accept the fitted 
 distribution? 
FIGURE 11.20   FIT RESULTS DIALOG BOX FOR COTTON PAjAMAS
15,000 16,000 17,000 18,000 19,000 20,000 21,000 22,000 23,000 24,000 25,000 26,000
0
1
2
3
4
5
6
Sample Data Frequency
Min Extreme
Normal
Beta Generalized

512 
Chapter 11 Monte Carlo Simulation
Step 8. Select cell E5 in the Model worksheet (This cell corresponds to the demand for 
cotton pajamas.) 
Step 9. When the $E$5 dialog box appears, click Save
To view the distribution of cotton pajama demand, double-click cell E5 to open the 
$E$5 dialog box. As Figure 11.21 illustrates, the right pane of this dialog box contains in-
formation on this random, or uncertain, variable. In particular, in the Control Parameters 
area, the Lower Cutoff and Upper Cutoff values specify that the normal distribution is 
unbounded (extremely large and small values are possible, although unlikely). To reflect 
the fact that negative values of cotton pajama demand are not possible, we enter 0 in the 
box to the right of Lower Cutoff and click Save. 
Similarly, we can fit a distribution for flannel pajama demand by following these steps: 
Step 1. Select cells C2:C25 in the data worksheet
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 3. Click Fit in the Tools group
Step 4. When the Fit Options dialog box opens, click Fit
At this point, a Fit Results dialog box appears (Figure 11.22). For flannel pajama 
 demand, the error function (Erf) distribution provides the best mathematical fit to the 
sample of 24 observations. However, upon closer inspection, one can see that the normal 
distribution provides almost an identical fit as the error function distribution (and is more 
familiar). As Figure 11.22 illustrates, we can visually compare the Normal distribution, 
Logistic distribution, and Rayleigh distribution by selecting the checkbox next to each of 
these distributions in the left pane of the Fit Results dialog box. Each of these distribu-
tions provides a similar fit. However, we discover, by clicking on the distribution names in 
the right pane of the Fit Results dialog box, that the Rayleigh distribution has a minimum 
value of 0 while the normal and logistic distributions have no lower bound. The Rayleigh 
distribution reflects a slightly higher chance of larger sales than the normal and logistic 
distributions. We select the Rayleigh distribution because flannel pajama demand cannot 
FIGURE 11.21   MODELING COTTON PAjAMA DEMAND WITH A NORMAL DISTRIBUTION
25.43%
17.761
(Values in Thousands)
(Values in Thousandths)
0.12
0.10
0.08
0.06
0.04
0.02
0
8
10
12
14
16
20
32
30
28
26
24
22
18
74.57%

 
11.3 Simulation Modeling with Analytic Solver Platform 
513
be negative, and the heavier right tail of the Rayleigh distribution more closely reflects 
Zappos’ belief that flannel pajama demand can achieve high levels. To proceed with the 
selection of the Rayleigh distribution for flannel pajama demand, we deselect the other 
distributions so that the Rayleigh distribution is the only one selected in the left pane of the 
Fit Results dialog box. To complete the process of modeling the cotton pajama demand as 
a random variable, we complete the following steps:
Step 5. In the left pane of the Fit Results dialog box, make sure that Rayleigh is the 
only distribution selected
Step 6. Close the Fit Results dialog box by cliking the 
 in upper right corner of the 
dialog box 
Step 7. Click Yes in response to the query Do you wish to accept the fitted distribution? 
Step 8. Select cell E6 in the Model worksheet (this cell corresponds to the demand for 
cotton pajamas) 
Step 9. Click Save
Because the silk pajama’s design is different from that of all other Zappos’ existing 
products, the marketing department does not believe that we should use past retail sales of 
other silk attire to estimate the demand distribution for the new silk pajama type. However, 
Zappos has conducted extensive marketing surveys to estimate likely demand scenarios for 
silk pajamas. Based on these marketing surveys, Zappos produced Table 11.4. Zappos also 
estimates a maximum silk pajama demand of 30,000 and a minimum possible demand of 
0. We use this information to construct a custom (continuous) general distribution for silk 
pajama demand with the following steps.
Step 1. Select cell E7 in the Model worksheet (corresponding to silk pajama demand) 
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 3. Click Distributions in the Simulation Model group
Select Custom and click General
FIGURE 11.22   COMPARING DISTRIBUTION FITS TO FLANNEL PAjAMA DEMAND
6,000
8,000
10,000
12,000
14,000
16,000
18,000
20,000
Sample Data Frequency
Normal
Logistic
Rayleigh
0
1
2
3
4
5

514 
Chapter 11 Monte Carlo Simulation
Step 4. When the $E$7 dialog box appears (Figure 11.23), in the Parameters area:
Enter B12 in the box to the right of min 
Enter B13 in the box to the right of max
Enter a16:a20 in the box to the right of values
Enter B16:B20 in the box to the right of weights
Step 5. Click Save
The velour pajama is a new product being introduced this season. Zappos’ current 
portfolio of products has nothing comparable that it could use to create a representative 
sample, and little information is known on potential demand for the product. However, 
a panel of Zappos’ most experienced marketing managers has estimated that the mini-
mum likely and maximum values of velour pajama demand are 2500 units 10 000 units
the velour pajama demand 
values are contained in 
the Model worksheet of 
the Zappos file in cells 
B24:B26
TABLE 11.4  ESTIMATES OF SILK PAjAMA DEMAND
Values
Likelihood
 5,000
10,000
15,000
20,000
25,000
 0.1
 0.4
 0.2
0.25
0.05
FIGURE 11.23   CUSTOM GENERAL DISTRIBUTION FOR SILK PAjAMA DEMAND
.00%
83.48%
16.52%
20.199
0
(Values in Thousands)
(Values in Thousandths)
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0
5
10
15
20
30
25

 
11.3 Simulation Modeling with Analytic Solver Platform 
515
and 25,000 units, respectively. We use these values to model the velour pajama demand 
with a triangular distribution with the following steps:
Step 1. Select cell E8 in the Model worksheet (corresponding to velour pajama demand) 
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Distributions in the Simulation Model group
Select Common and click Triangular
Step 4. In the Parameters area of the $E$8 dialog box (Figure 11.24):
Enter B24 in the box to the right of min 
Enter B25 in the box to the right of likely
Enter B26 in the box to the right of max
Step 5. Click Save
Tracking Output Measures for Zappos
After defining all the random variables with ASP, we are ready to use ASP to track output 
measures. For Zappos, the primary output measure of interest is the total profit summed 
across all four pajama types. To record this output over the simulation trials, we apply the 
following steps: 
Step 1. Select cell j9 in the Model worksheet (corresponding the total profit summed 
across pajama types) 
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Results in the Simulation Model group
Select Output and click In Cell
This procedure appends the formula in cell j9 with “1PsiOutput()”—which triggers the 
ASP to record the cell’s value for each of the simulation trials. By collecting the value of 
total profit for each of 10 000 trials ASP then can create a distribution of total profit
FIGURE 11.24   TRIANGULAR DISTRIBUTION FOR VELOUR PAjAMA DEMAND
.00%
71.23%
28.77%
15.146
2.5
(Values in Thousands)
(Values in Thousandths)
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0
5
10
15
20
25
0.08

516 
Chapter 11 Monte Carlo Simulation
ASP also allows us to explicitly track statistics of the output measures and record them in a 
specified cell. For the Zappos problem, we track the mean total profit using the following steps:
Step 1. Select cell j9 in the Model worksheet
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 3. Click Results in the Simulation Model group
Select Statistic and click Mean
Select cell j11
These steps instruct ASP to compute the mean of the value of cell j9 over the simulation 
trials and place it in cell j11. Figure 11.25 shows the formula view of the Zappos simula-
tion model after its construction via ASP. The complete simulation model for the Zappos 
problem is contained in the file ZapposModel.
on completion of these 
steps, excel will display 
“#n/a” in cell J11. asP 
will not populate the value 
for cell J11 until a simula-
tion is executed.
file
WEB
ZapposModel
FIGURE 11.25   ZAPPOS’ SIMULATION MODEL
A
Zappos
B
C
D
Parameters
Wholesale
Price
Clearance
Price
Demand
Order
Quantity
Retail
Sales
Clearance
Sales
Proﬁt
Model
Retail
Price
1
2
3
4
5
6
7
8
9
10
11
Cotton Pajama
Flannel Pajama
Silk Pajama
Velour Pajama
Minimum Value
Velour Pajama
Demand
Most likely value
Maximum value
Minimum Value
Maximum Value
Values
Likelihood
25
25
35
30
0
30000
10000
2500
25000
30
40
60
55
10
=PsiNormal(19922.
4583333333,3269.
99910778743,
PsiTruncate(0,1E+30))
=PsiRayleigh(6845.
40150927047,PsiShift
(2198.19603347544))
=PsiGeneral(B12,B13,
A16:A20,B16:B20)
=PsiTriangular
(B24,B25,B26)
=(H5*(C5-B5))+
(I5*(D5-B5))
=(H6*(C6-B6))+
(I6*(D6-B6))
=(H7*(C7-B7))+
(I7*(D7-B7))
=(H8*(C8-B8))+
(I8*(D8-B8))
=SUM(J5:J8)
+PsiOutput()
Total
Proﬁt:
Mean Total
Proﬁt:
=G5-H5
=G6-H6
=G7-H7
=G8-H8
=MIN(E5,G5)
=MIN(E6,G6)
=MIN(E7,G7)
=MIN(E8,G8)
10000
10000
10000
10000
10
30
20
Silk Pajama
Demand
0.10
0.25
0.05
0.40
0.20
20000
25000
5000
10000
15000
12
13
14
15
16
17
18
19
23
22
24
25
26
20
21
E
F
G
H
I
J
=PsiMean(J9)

 
11.3 Simulation Modeling with Analytic Solver Platform 
517
Executing Simulation Trials and 
Analyzing Output for Zappos
ASP provides many user options for executing a simulation. For the Zappos problem, the 
following steps (which are very similar to the steps used in the Land Shark simulation) 
illustrate how to set the number of trials to 10,000, execute a set of simulation trials, and 
analyze the output (the distribution of total profit). 
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 2. Click Options in the Options group
Step 3. Click the Simulation tab. In the General area, enter 10000 in the Trials per 
Simulation: box
Step 4. Click OK
Step 5. Click the arrow under Simulate in the Solve Action group
From the drop-down menu that appears, click Interactive
Step 6. When the $J$9 dialog box appears, enter 350 in the box to the right of Lower 
Cutoff in the Chart Statistics area (Figure 11.26)
Click Save
As Figure 11.26 illustrates, if Zappos orders 10,000 of each pajama type, the mean 
total profit is $603,660. In addition, the minimum profit over 10,000 simulation trials is 
$121,647 and the maximum profit is $700,000. By specifying a Lower Cutoff value of 
350, we observe that there is an estimated 0.984 probability that Zappos will make more 
than $350,000 (and hence a 0.016 probability that Zappos will make less than $350,000). At 
first glance, the shape of the total profit distribution is a bit surprising (with a large number 
of trials resulting in the maximum value of $700,000 profit). This maximum value occurs 
when demand exceeds the order quantity for all four pajama types and sells all the pajamas 
ordered; Zappos can never sell more pajamas than it orders! 
To see how the distribution of total profit is affected by the order quantity, we can 
 re-run the simulation for different order quantities. For example, if we order 20,000 of each 
entering a value for only 
the Lower Cutoff will 
compute the Likelihood 
that the output measure 
will be above the cutoff 
value. entering a value for 
only the Upper Cutoff will 
compute the Likelihood 
that the output measure will 
be below the cutoff value. 
entering values for both the 
Lower Cutoff and Upper 
Cutoff will compute the 
Likelihood that the output 
measure is between the 
cutoff values.
FIGURE 11.26   DISTRIBUTION OF ZAPPOS’ TOTAL PROFIT WITH ORDER QUANTITIES 5 10,000
0.30
350
0.25
0.20
0.15
0.10
Relative Probability
0.05
0
3000
2500
2000
1500
1000
Frequency
500
0
$700
98.40%
1.60%
$650
$600
$550
$500
$450
$400
(Values in Thousands)
Simulation Results - $J$9
$350
$300
$250
$200
$150

518 
Chapter 11 Monte Carlo Simulation
pajama type, we obtain the distribution of total profit in Figure 11.27. We now observe 
that the mean total profit is $625,989 which is an increase of $22,329 over the case when 
ordering 10,000 of each pajama type. The minimum profit over 10,000 simulation trials is 
–$317,079, and the maximum profit is $1,400,000. There is an estimated 0.8505 probability 
that Zappos will make more than $350,000 (and hence a 0.1495 probability that Zappos will 
make less than $350,000). This analysis indicates that doubling the order quantity increases 
the mean total profit and increases the upside (in terms of chance of larger profits) but also 
exposes Zappos to a larger chance of smaller profits. 
We see from Figure 11.27 that the distribution of total profit is bell-shaped. When order 
quantities are sufficiently high (so that they do not limit profit potential), low profit contribu-
tions from one pajama type are often offset by high profit contributions from another pajama 
type. The result of the offsetting profit contributions among pajama types is that moderate 
total profit amounts are more common than extremely low or high total profit amounts. 
Simulation Optimization
In Section 11.3, we demonstrated how to evaluate different order quantities for the Zappos 
problem by rerunning simulations for different values of the decision variables. In this section, 
we demonstrate how to use ASP to apply optimization techniques in spreadsheet simulation 
models with random variables. This approach is called simulation optimization. A simula-
tion optimization model is an optimization problem complicated by the presence of random 
variables. In general, ASP searches for optimal values of the decision variables by itera-
tively adjusting the values of the decision variables. For each set of decision variable values, 
ASP evaluates the quality of the solution over a set of simulation trials. Therefore, solving a 
simulation optimization model can be very computationally expensive and take a long time. 
Furthermore, ASP cannot guarantee an optimal solution to a simulation optimization model 
the resulting bell-shaped 
histogram in figure 11.23 
is the result of general 
statistical concept known as 
the central limit theorem. 
the central limit theorem 
states that the sum of inde-
pendent random variables 
can be approximated by a 
normal probability distri-
bution. the total profit is 
computed based on the sum 
of the uncertain profit from 
the four individual pajama 
types.
11.4
FIGURE 11.27   DISTRIBUTION OF ZAPPOS’ TOTAL PROFIT WITH ORDER QUANTITIES 5 20,000
0.12
350
0.10
0.08
0.06
0.04
Relative Probability
0.02
0.00
1200
1000
800
600
400
Frequency
200
0
$1,400
$1,200
85.05%
14.95%
$1,000
$800
$600
$400
$200
$0
–$200
(Values in Thousands)
Simulation Results - $J$9

 
11.4 Simulation Optimization 
519
because of sampling error resulting from the simulation trials and because the optimization 
problem may have multiple local optimal solutions due to nonlinear relationships. Therefore, 
it is helpful to solve the model multiple times and compare the solutions across runs.
For the Zappos problem, we want to determine the order quantities for the four types of 
pajamas that maximize the mean (expected) total profit. Although the forecasted demand 
looks promising, Zappos wants to be conservative and spend no more than $1,150,000 
on the wholesale procurement of the pajamas. As Figure 11.28 shows, we add the con-
straint on wholesale expenditures to the Zappos spreadsheet model by first entering 
Refer to chapter 10 for 
a detailed discussion of 
deterministic nonlinear 
optimization. 
file
WEB
ZapposSimOpt
FIGURE 11.28   SETTING UP ZAPPOS’ SIMULATION OPTIMIZATION
A
C
Zappos
Parameters
Cotton Pajama
Flannel Pajama
Silk Pajama
Values
Silk Pajama Demand
Maximum value
Minimum value
Expenditures
$1,150,000
Budget
$1,150,000
Most likely value
Maximum value
10,000
15,000
5,000
25,000
20,000
Wholesale Price
30,000
0
$35
$25
$25
Clearance Price
$30
$10
$10
Order Quantity
Retail Sales
Clearance Sales
Total Proﬁt:
Proﬁt
Model
10,000
6,696
10,000
0
3,304
0
$600,873
$250,000
$50,873
$50,000
$60
$40
$30
Retail Price
Demand
22,875
6,696
22,739
Likelihood
1
2
3
4
5
6
7
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
25
26
0.20
0.40
0.10
0.25
0.05
Velour Pajama Demand
Minimum value
2,500
10,000
25,000
B
D
E
G
H
I
J
F
10,000
10,000
10,000
Velour Pajama
$30
$20
10,000
0
$250,000
$55
13,580
8
10,000

520 
Chapter 11 Monte Carlo Simulation
the computation of the wholesale expenditures into cell G12 using the Excel formula 
5SUMPRODUCT(B5:B8,G5:G8). Then we enter the $1,150,000 budget into cell H12. 
The algorithms that ASP employs to solve simulation optimization models work best when 
bounds on the decision variable values are provided. For the Zappos problem, management 
wants to order at least 1000 units of each type of pajama. Furthermore, management does 
not want to order more than 20,000 units of any single type of pajama.
To enter the simulation optimization model into ASP, we execute the following steps:
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 2. Click Model in the Model group to display the Solver Options and Model 
Specifications pane on the right-hand side of the spreadsheet
Step 3.  In the Solver Options and Model Specifications pane, click the  to the left 
of Optimization to expand the optimization tree structure (Figure 11.29)
Click Objective
Select cell j9 (which contains the computation of total profit for a 
single scenario)
Click the Add button 
 in the Solver Options and Model Specifica-
tions pane
Double-click $J$9 (Max) listed under Objective
When the Change Objective dialog box appears (Figure 11.30), select 
Expected from the drop-down menu next to Set Cell:
Click OK
FIGURE 11.29   SOLVER OPTIONS AND MODEL SPECIFICATIONS PANE
Delete
Add
Analyze
Solve
Refresh

 
11.4 Simulation Optimization 
521
Step 4. Under Variables, click Normal 
Select the range G5:G8 (which contains the four order quantities)
Click the Add button 
 in the Solver Options and Model Specifica-
tions pane
Step 5. Under Constraints, click Normal 
Select cell G12
Click the Add button 
 in the Solver Options and Model Specifica-
tions pane
When the Add Constraint dialog box appears (Figure 11.31):
 Select ,5 from the drop-down button
 Enter h12 in the Constraint: area
 Click OK
Step 6. Under Constraints, select Normal
Select cells G5:G8
Click the Add button 
 in the Solver Options and Model Specifica-
tions pane
When the Add Constraint dialog box appears:
 Select ,5 from the drop-down button
 Enter 20000 in the Constraint: area
 Click Add
When the Add Constraint dialog box appears:
Enter g5:g8 in the Cell Reference: area
Select .5 from the drop-down button
Enter 1000 in the Constraint: area
Click OK
step 5 constrains the total 
expenditures (cell g12) 
to be less than or equal 
to the budget (cell h12). 
step 6 requires that all 
order quantities (cells 
g5:g8) are less than or 
equal to 20,000. step 7 
forces Zappos to order at 
least 1000 units of each 
type of pajama.
FIGURE 11.30   CHANGE OBjECTIVE DIALOG BOX
FIGURE 11.31   ADDING CONSTRAINT ON WHOLESALE EXPENDITURE

522 
Chapter 11 Monte Carlo Simulation
The pane on the right-hand side of Figure 11.28 shows the completed simulation op-
timization model. The complete simulation optimization model for the Zappos problem 
is contained in the file ZappossimoptModel. The following steps prepare settings for the 
simulation model for optimization and execute the simulation optimization: 
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 2. If the Solver Options and Model Specifications pane does not appear on the 
right-hand side of the spreadsheet, click Model in the Model group
Step 3. Click the Engine tab in the Solver Options and Model Specifications pane 
(Figure 11.32)
Select the Automatically Select Engine checkbox
Select Standard Evolutionary Engine from the drop-down menu at 
the top of the pane
In the General area, click Global Search and then select Genetic Al-
gorithm from the drop-down menu 
Step 4. Click the Output tab in the Solver Options and Model Specifications pane
Click the Solve button  to begin the simulation-optimization procedure
Figure 11.33 shows one possible solution consisting of order quantities of 1543  cotton 
pajamas, 8363 flannel pajamas, 15,000 silk pajamas, and 12,578 velour pajamas. By 
file
WEB
ZapposSimOptModel
for problems for which 
asP automatically selects 
the standard evolutionary 
engine, we recommend 
testing both the scat-
ter search and genetic 
algorithm options for the 
global search setting. 
depending on the problem, 
one method may be supe-
rior to the other. 
upon clicking the Solve 
button, the Guided Mode 
dialog box will appear 
if asP’s guided Mode is 
turned on. asP’s guided 
Mode assists the analyst in 
the optimization process 
and can be turned off at the 
analyst’s discretion. 
FIGURE 11.32   SETTINGS FOR SIMULATION OPTIMIZATION
Pressing the Esc key will 
allow the user to interrupt 
the optimization process.

 
11.4 Simulation Optimization 
523
 double-clicking cell j9, we can view the distribution of total profit resulting from these 
 order quantities. Figure 11.33 shows that these order quantities achieve an estimated aver-
age total profit of $636,392. At this solution, there is a 0.9584 probability of a total profit 
greater than $350,000 (and hence a 0.0416 probability of a profit less than $350,000). 
As explained, ASP (or any software) cannot generally guarantee obtaining the optimal 
solution to a simulation optimization model because this type of problem can be very 
difficult to solve. Furthermore, ASP may terminate with different best-found solutions 
when re-solving the model. Therefore, we recommend re-solving a simulation optimization 
model several times and comparing the solutions. For example, Table 11.5 shows results 
from five optimization runs on the Zappos problem. Although the values of the decision 
variables are not exactly the same, the estimated average total profit is nearly identical for 
each of the five solutions (varying by less than 0.01 percent). Furthermore, we observe that 
the relative values of the order quantities are stable across the optimization runs, so this 
analysis provides a clear guidance on the ordering decision. As long as its assumptions on 
the random variables are accurate, Zappos should be comfortable with an order of 1500 
cotton, 8500 flannel, 15,000 silk, and 12,500 velour pajamas.
FIGURE 11.33   ZAPPOS’ SIMULATION OPTIMIZATION OUTPUT
A
Zappos
B
C
D
Parameters
Model
Wholesale
Price
Clearance
Price
Order
Quantity
Retail
Sales
Clearance
Sales
Total Proﬁt:
Proﬁt
Demand
Retail
Price
Cotton Pajama
Flannel Pajama
Silk Pajama
Silk Pajama
Demand
Expenditures
Budget
Minimum value
$1,150,000
$1,150,000
$25
$25
$35
$30
0
30,000
$30
$40
$60
$55
$10
$10
$30
$20
15,704
5,536
19,585
6,805
1,543
5,536
15,000
6,805
0
2,827
0
5,773
$7,717
$40,637
$374,997
$112,387
$535,739
Minimum value
Most likely
value
Maximum value
Maximum value
Values
5,000
10,000
15,000
20,000
25,000
0.10
0.40
0.20
0.25
0.05
2,500
10,000
25,000
Likelihood
Velour Pajama
Velour Pajama
Demand
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
23
22
24
25
26
27
20
21
E
F
G
H
I
J
K
L
M
1,543
8,363
15,000
12,578
0.05
Relative Probability
0.00
(Values in Thousands)
$50 $100$150$200$250$300$350$400$450$500$550$600$650$700$750$800
0
500
Frequency
1000
1500
0.10
0.15
4.16%
350
95.84%
Simulation Results- $J$9

524 
Chapter 11 Monte Carlo Simulation
Simulation Considerations
Verification and Validation
An important aspect of any simulation study involves confirming that the simulation model 
accurately describes the real system. Inaccurate simulation models cannot be expected to 
provide worthwhile information. Thus, before using simulation results to draw conclusions 
about a real system, one must take steps to verify and validate the simulation model.
Verification is the process of determining that the computer procedure that performs 
the simulation calculations is logically correct. Verification is largely a debugging task to 
make sure that no errors are in the computer procedure that implements the simulation. 
In some cases, an analyst may compare computer results for a limited number of events 
with independent hand calculations. In other cases, tests may be performed to verify that 
the random variables are being generated correctly and that the output from the simulation 
model seems reasonable. The verification step is not complete until the user develops a high 
degree of confidence that the computer procedure is error free.
Validation is the process of ensuring that the simulation model provides an accurate 
representation of a real system. Validation requires an agreement among analysts and man-
agers that the logic and the assumptions used in the design of the simulation model accu-
rately reflect how the real system operates. The first phase of the validation process is done 
prior to or in conjunction with the development of the computer procedure for the simula-
tion process. Validation continues after the computer program has been developed, with 
the analyst reviewing the simulation output to see whether the simulation results closely 
approximate the performance of the real system. If possible, the output of the simulation 
model is compared to the output of an existing real system to make sure that the simulation 
output closely approximates the performance of the real system. If this form of validation 
is not possible, an analyst can experiment with the simulation model and have one or more 
individuals experienced with the operation of the real system review the simulation output 
to determine whether it is a reasonable approximation of what would be obtained with the 
real system under similar conditions.
Verification and validation are not tasks to be taken lightly. They are key steps in any 
simulation study and are necessary to ensure that decisions and conclusions based on the 
simulation results are appropriate for the real system.
Advantages and Disadvantages of Using Simulation
The primary advantages of simulation are that it is easy to understand and that the meth-
odology can be used to model and learn about the behavior of complex systems that would 
be difficult, if not impossible, to deal with analytically. Simulation models are flexible; 
they can be used to describe systems without requiring the assumptions that are often 
required by mathematical models. In general, the larger the number of random variables 
11.5
TABLE 11.5  RESULTS FROM MULTIPLE SIMULATION OPTIMIZATION RUNS
Run
1
2
3
4
5
Cotton
Flannel
Silk
Velour
Average Total Profit
1,543
8,363
15,000
12,578
$636,392
1,557
8,359
15,000
12,570
$636,398
1,529
8,364
15,009
12,578
$636,377
1,574
8,363
14,986
12,568
$636,388
1,563
8,367
14,990
12,571
$636,385

 
Summary 
525
a system has, the more likely that a simulation model will provide the best approach for 
studying the system. Another advantage of simulation is that a simulation model pro-
vides a convenient experimental laboratory for the real system. Changing assumptions 
or operating policies in the simulation model and rerunning it can provide results that 
help predict how such changes will affect the operation of the real system. Experimenting 
directly with a real system is often not feasible. Simulation models often warn against 
poor decision strategies by projecting disastrous outcomes such as system failures, large 
financial losses, and so on. 
Simulation is not without disadvantages. For complex systems, the process of develop-
ing, verifying, and validating a simulation model can be time-consuming and expensive 
(however, the process of developing the model generally leads to a better understanding 
of the system, which is an important benefit). Like all mathematical models, the analyst 
must be conscious of the assumptions of the model in order to understand its limitations. In 
 addition, each simulation run provides only a sample of how the real system will operate. As 
such, the summary of the simulation data provides only estimates or approximations about 
the real system. Nonetheless, the danger of obtaining poor solutions is greatly mitigated if 
the analyst exercises good judgment in developing the simulation model and follows proper 
verification and validation steps, and if the simulation process is run long enough under 
a wide variety of conditions so that the analyst has sufficient data to predict how the real 
system will operate.
Summary
Simulation is a method for learning about a real system by experimenting with a model that 
represents the system. Some of the reasons simulation is frequently used are:
 1. It can be used for a wide variety of practical problems.
 2.  The simulation approach is relatively easy to explain and understand. As a result, 
management confidence is increased, and the acceptance of the results is more easily 
obtained.
 3.  Spreadsheet packages now provide another alternative for model implementation, 
and third-party vendors have developed add-ins that expand the capabilities of the 
spreadsheet packages.
 4.  Computer software developers have produced simulation packages that make it easier 
to develop and implement simulation models for more complex problems.
In this chapter, we first showed how native Excel functions can be used to execute a 
simulation to evaluate risk involving the development of a new product (the Sanotronics 
device). Then we developed a simulation model using Analytic Solver Platform for the 
Land Shark Real Estate bid determination problem and the Zappos order quantity prob-
lem. These examples demonstrate how simulation software such as ASP facilitates more 
sophisticated simulation modeling. Next we used the Zappos problem to illustrate the 
power of combining simulation and optimization methods within Analytic Solver Plat-
form. This problem combines many of the methods used in business analytics, including 
data analysis using descriptive statistics and data visualization, optimization methods, 
and simulation models. 
Our approach throughout this chapter was to develop simulation models that contained 
both controllable inputs and random variables. Procedures were developed for randomly 
generating values for the random variables, the sequence of logical and mathematical 
 operations that describe the steps of the simulation process were modeled, and simulation 
results were obtained by running the simulation for a suitable number of trials. Simulation 
results were obtained and conclusions were drawn about the operation of the real system.

526 
Chapter 11 Monte Carlo Simulation
Summary of Steps for Conducting a Simulation Analysis
 1.  construct a spreadsheet model that computes output measures for given values of 
inputs. The foundation of a good simulation model is logic that correctly relates  input 
values to outputs. Audit the spreadsheet to assure that the cell formulas correctly 
evaluate the outputs over the entire range of possible input values. 
 2.  identify inputs that are uncertain, and specify probability distributions for these 
cells (rather than just static numbers). Note that not all inputs may have a degree 
of uncertainty sufficient to require modeling with a probability distribution. Other 
inputs may actually be decision variables, which are not random and should not be 
modeled with probability distributions; rather, these are values that the decision 
maker can control.
 3.  select one or more outputs to record over the simulation trials. Typical information 
recorded for an output includes a histogram of output values over all simulation trials 
and summary statistics such as the mean, standard deviation, maximum, minimum, 
percentile values.
 4.  execute the simulation for a specified number of trials. For most small to moderately 
sized simulation problems, we recommend the use of 10,000 trials. The amount of 
sampling error can be monitored by observing how much simulation output measures 
fluctuate across multiple simulation runs. 
 5.  analyze the outputs and interpret the implications on the decision-making process. In 
addition to estimates of the mean output, simulation allows us to construct a distribu-
tion of possible output values.
In this chapter, we have focused on Monte Carlo simulation consisting of indepen-
dent trials in which the results for one trial do not affect what happens in subsequent 
trials. Another style of simulation, called discrete-event simulation, involves trials 
that represent how a system evolves over time. One common application of discrete-
event simulation is the analysis of waiting lines. In a waiting line simulation, the 
random variables are the interarrival times of the customers and the service times 
of the servers, which together determine the waiting and completion times of the 
customers. Although it is possible to conduct small discrete-event simulations with 
a Monte Carlo simulation package such as Analytic Solver Platform, discrete-event 
simulation modeling is best conducted with special-purpose software such as Arena®, 
ProModel®, and GPSS®. These packages have built-in simulation clocks, simplified 
methods for generating random variables, and procedures for collecting and summa-
rizing the simulation output. 
Glossary
Monte Carlo simulation A simulation method that uses repeated random sampling to 
represent uncertainty in a model representing a real system and that computes the values 
of model outputs.
Probability distribution A description of the range and relative likelihood of possible 
values of an uncertain variable. 
Random variable (uncertain variable)  Input to a simulation model whose value is 
 uncertain and described by a probability distribution.
Controllable input Input to a simulation model that is selected by the decision maker.
Risk analysis The process of evaluating a decision in the face of uncertainty by quantifying 
the likelihood and magnitude of an undesirable outcome.

 
Problems 
527
Base-case scenario Determining output assuming the most likely values for the random 
variables of a model.
What-if analysis A trial-and-error approach to learning about the range of possible outputs 
for a model. Trial values are chosen for the model inputs (these are the what-ifs) and the 
value of the output(s) is computed.
Worst-case scenario  Determining the output assuming the worst values that can be 
 expected for the random variables of a model.
Best-case scenario Determining the output assuming the best values that can be expected 
for the random variables of a model.
Discrete probability distribution A probability distribution where the possible values for 
a random variable can take on only specified discrete values. 
Continuous probability distribution A probability distribution where the possible values 
for a random variable can take any value between two specified values. The specified values 
can include negative and positive infinity.
Simulation optimization The process of applying optimization techniques to identify 
 optimal (or near-optimal) values of the decision variables in a simulation model.
Verification The process of determining that a computer program implements a simulation 
model as it is intended.
Validation The process of determining that a simulation model provides an accurate rep-
resentation of a real system.
Discrete-event simulation A simulation method that describes how a system evolves over 
time by using events that occur at discrete points in time.
Correlation A measure of dependence between two random variables.
Monotonic relationship A unidirectional relationship between two quantities; either one 
quantity never increases as the other increases, or one quantity never decreases as the other 
increases. 
Stochastic library unit with relationship preserved (SLURP) A table of sample values 
where each column corresponds to a random variable and each row represents a set of 
 associated values for the random variables.
Problems
 1.  The management of Brinkley Corporation is interested in using simulation to estimate the 
profit per unit for a new product. The selling price for the product will be $45 per unit. 
Probability distributions for the purchase cost, the labor cost, and the transportation cost 
are estimated as follows:
Procurement  
Cost ($)
Probability
Labor 
Cost ($)
Probability
Transportation 
Cost ($)
Probability
10
11
12
0.25
0.45
0.30
20
22
24
25
0.10
0.25
0.35
0.30
3
5
0.75
0.25
a. Compute profit per unit for the base case, worst case, and best case.
b. Construct a simulation model to estimate the mean profit per unit. 
c.  Why is the simulation approach to risk analysis preferable to generating a variety of 
what-if scenarios?
d.  Management believes the project may not be sustainable if the profit per unit is less than 
$5. Use simulation to estimate the probability the profit per unit will be less than $5.
unless noted, these prob-
lems can be solved with 
either asP or with native 
excel functionality. We 
typically recommend the 
use of asP with 10,000 
trials per simulation. 

528 
Chapter 11 Monte Carlo Simulation
 2.  Using Analytic Solver Platform, develop a simulation model for the Sanotronics problem 
that was presented in Section 11.2 using native Excel functionality. 
a.  Obtain estimates for the mean profit, maximum profit, minimum profit, and standard 
deviation of profit.
b. What is your estimate of the probability of a loss? 
 3.  Grear Tire Company has produced a new tire with an estimated mean lifetime mileage of 
36,500 miles. Management also believes that the standard deviation is 5000 miles and that 
tire mileage is normally distributed. To promote the new tire, Grear has offered to refund 
some money if the tire fails to reach 30,000 miles before the tire needs to be replaced. 
Specifically, for tires with a lifetime below 30,000 miles, Grear will refund a customer $1 
per 100 miles short of 25,000. 
a. For each tire sold, what is the expected cost of the promotion?
b. What is the probability that Grear will refund more than $50 for a tire?
c.  What mileage should Grear set the promotion claim if it wants the expected cost to be $2?
 4.  To generate leads for new business, Gustin Investment Services offers free financial plan-
ning seminars at major hotels in Southwest Florida. Gustin conducts seminars for groups 
of 25 individuals. Each seminar costs Gustin $3,500 and the average first-year commis-
sion for each new account opened is $5,000. Gustin estimates that for each individual 
 attending the seminar, there is a 0.01 probability that he/she will open a new account. 
a.  Determine the equation for computing Gustin’s profit per seminar, given values of the 
relevant parameters.
b.  What type of random variable is the number of new accounts opened? (hint: Review 
Appendix 11.2 for descriptions of various types of probability distributions.)
c.  Construct a spreadsheet simulation model to analyze the profitability of Gustin’s 
seminars. Would you recommend that Gustin continue running the seminars?
d.  How large of an audience does Gustin need before a seminar’s expected profit is 
greater than zero?
 5.  Baseball’s World Series is a maximum of seven games, with the winner being the first 
team to win four games. Assume that the Atlanta Braves are playing the Minnesota Twins 
in the World Series and that the first two games are to be played in Atlanta, the next three 
games at the Twins’ ballpark, and the last two games, if necessary, back in Atlanta. Taking 
into account the projected starting pitchers for each game and the home field advantage, 
the probabilities of Atlanta winning each game are as follows:
Game
1
2
3
4
5
6
7
Probability of Win
0.60
0.55
0.48
0.45
0.48
0.55
0.50
a.  Set up a spreadsheet simulation model in which whether Atlanta wins each game is a 
random variable.
b. What is the probability that the Atlanta Braves win the World Series? 
c. What is the average number of games played regardless of winner?
 6.  The price of a share of a particular stock listed on the New York Stock Exchange is 
currently $39. The following probability distribution shows how the price per share is 
expected to change over a three-month period:
Stock Price  
Change ($)
Probability
22
21
0
11
12
13
14
0.05
0.10
0.25
0.20
0.20
0.10
0.10
file
WEB
 SanotronicsASP

 
Problems 
529
a.  Construct a spreadsheet simulation model that computes the value of the stock price in 
3 months, 6 months, 9 months, and 12 months under the assumption that the change in 
stock price over any 3-month period is independent of the change in stock price over 
any other 3-month period.
b.  With the current price of $39 per share, simulate the price per share for the next four 
3-month periods. What is the average stock price per share in 12 months? What is the 
standard deviation of the stock price in 12 months?
c.  Based on the model assumptions, what are the lowest and highest possible prices for this 
stock in 12 months? Based on your knowledge of the stock market, how valid do you think 
this is? Propose an alternative to modeling how stock prices evolve over 3-month periods.
 7.  The Iowa Energy are scheduled to play against the Maine Red Claws in an upcoming 
game in the National Basketball Association Developmental League (NBA-DL). Because 
a player in the NBA-DL is still developing his skills, the number of points he scores in 
a game can vary. Assume that each player’s point production can be represented as an 
integer uniform variable with the ranges provided in the following table: 
Player
Iowa Energy
Maine Red Claws
1
2
3
4
5
6
7
8
[5,20]
[7,20]
[5,10]
[10,40]
[6,20]
[3,10]
[2,5]
[2,4]
[7,12]
[15,20]
[10,20]
[15,30]
[5,10]
[1,20]
[1,4]
[2,4]
a. Develop a spreadsheet model that simulates the points scored by each team. 
b.  What are the average and standard deviation of points scored by the Iowa Energy? 
What is the shape of the distribution of points scored by the Iowa Energy?
c.  What is the average and standard deviation of points scored by the Maine Red Claws? 
What is the shape of the distribution of points scored by the Maine Red Claws?
d.  Let Point Differential 5 Iowa Energy points – Maine Red Claw points. What is the 
average point differential between the Iowa Energy and Maine Red Claws? What is 
the standard deviation in the point differential? What is the shape of the point differ-
ential distribution?
e.  What is the probability that the Iowa Energy scores more points than the Maine Red Claws?
f. 
 The coach of the Iowa Energy feels that they are the underdog and is considering 
a riskier game strategy. The effect of this strategy is that the range of each Energy 
player’s point production increases symmetrically so that the new range is [0, original 
upper bound 1 original lower bound]. For example, Energy player 1’s range with the 
risky strategy is [0,25]. How does the new strategy affect the average and standard 
deviation of the Energy point total? How does that affect the probability of the Iowa 
Energy scoring more points that the Maine Red Claws?
 8.  A project has four activities (A, B, C, and D) that must be performed sequentially. The prob-
ability distributions for the time required to complete each of the activities are as follows:
Activity
Activity Time 
(weeks)
Probability
A
 5
0.25
 6
0.35
 7
0.25
 8
0.15
(continued)

530 
Chapter 11 Monte Carlo Simulation
Activity
Activity Time 
(weeks)
Probability
B
 3
0.20
 5
0.55
 7
0.25
C
10
0.10
12
0.25
14
0.40
16
0.20
18
0.05
D
 8
0.60
10
0.40
a.  Construct a spreadsheet simulation model to estimate the average length of the project 
and the standard deviation of the project length. 
b.  What is the estimated probability that the project will be completed in 35 weeks or less?
 9.  Over the past year, a financial analyst has tracked the daily change in the price per share 
of common stock for a major oil company. Using ASP, develop a simulation model to 
analyze the stock price at the end of the next quarter. Assume 63 trading days and a current 
price per share of $51.60.
a.  Using the data in the datatofit worksheet of the dailystock file, fit a distribution to 
represent the daily change in stock price. Assume that the each day’s change in stock 
price is independent of every other day’s change in stock price. You can justify this 
assumption by estimating the correlation of consecutive days’ change in stock price 
using the command CORREL(B3:B313, B4:B314) and observing that this quantity is 
relatively small. 
b. What is the expected price per share at the end of the quarter?
c. What is the probability that the stock price will be below $26.55?
d.  The WhatReallyhappened worksheet of dailystock file contains the 63 values of the 
daily change in stock price that actually occurred during the quarter. (You can plug 
these 63 values into your model to confirm the calculations.) What does this reveal 
about the limitations of simulation modeling?
e.  Based on the observation that many distributions underestimate the possibility of 
 extreme values, some experts suggest the use of a “heavy-tailed” distribution to model 
the change in stock price. The Cauchy distribution is one such heavy-tailed distribution 
in which the likelihood of extreme daily changes in stock price are much more likely 
than with other distributions (such as the normal distribution). Fit a Cauchy distribution 
to the data in the datatofit worksheet and observe the impact on the analysis. hint: 
To avoid unrealistic swings in stock price, truncate the possible values from the Cauchy 
distribution by setting the lower cutoff and upper cutoff values to 21 and 1, respectively. 
10.  In preparing for the upcoming holiday season, Fresh Toy Company (FTC) designed a new 
doll called The Dougie that teaches children how to dance. The fixed cost to produce the 
doll is $100,000. The variable cost, which includes material, labor, and shipping costs, is 
$34 per doll. During the holiday selling season, FTC will sell the dolls for $42 each. If 
FTC overproduces the dolls, the excess dolls will be sold in january through a distributor 
who has agreed to pay FTC $10 per doll. Demand for new toys during the holiday sell-
ing season is extremely uncertain. Forecasts are for expected sales of 60,000 dolls with a 
standard deviation of 15,000. The normal probability distribution is assumed to be a good 
description of the demand. FTC has tentatively decided to produce 60,000 units (the same 
as average demand), but it wants to conduct an analysis regarding this production quantity 
before finalizing the decision. 
a.  Create a what-if spreadsheet model using formula that relate the values of production 
quantity, demand, sales, revenue from sales, amount of surplus, revenue from sales of 
file
WEB
DailyStock

 
Problems 
531
surplus, total cost, and net profit. What is the profit corresponding to average demand 
(60,000 units)?
b.  Modeling demand as a normal random variable with a mean of 60,000 and a standard 
deviation of 15,000, simulate the sales of The Dougie doll using a production quantity 
of 60,000 units. What is the estimate of the average profit associated with the produc-
tion quantity of 60,000 dolls? How does this compare to the profit corresponding to 
the average demand (as computed in part a)?
c.  Before making a final decision on the production quantity, management wants an 
analysis of a more aggressive 70,000-unit production quantity and a more conserva-
tive 50,000-unit production quantity. Run your simulation with these two production 
quantities. What is the mean profit associated with each? 
d.  Besides mean profit, what other factors should FTC consider in determining a produc-
tion quantity? Compare the three production quantities (50,000, 60,000. and 70,000) 
using all these factors. What trade-offs occur? What is your recommendation? 
11.  South Central Airlines (SCA) operates a commuter flight between Atlanta and Charlotte. 
The regional jet holds 50 passengers and currently SCA books only up to 50 reservations. 
Past data shows that SCA always sells all 50 reservations but that, on average, two passen-
gers do not show up. As a result, with 50 reservations, the flight is often being flown with 
empty seats. To capture additional profit, SCA is considering an overbooking strategy in 
which they would accept 52 reservations even though the airplane holds only 50 passen-
gers. SCA believes that it will be able to always book all 52 reservations. The probability 
distribution for the number of passengers showing up when 52 reservations are accepted 
is estimated as follows:
Passengers  
Showing Up
Probability
48
49
50
51
52
0.05
0.25
0.50
0.15
0.05
 
 SCA receives a marginal profit of $100 for each passenger who books a reservation 
( regardless whether they show up). The airline will also incur a cost for any passenger de-
nied seating on the flight. This cost covers added expenses of rescheduling the passenger 
as well as loss of goodwill, estimated to be $150 per passenger. Develop a spreadsheet 
simulation model for this overbooking system. Simulate the number of passengers show-
ing up for a flight.
a. What is the average net profit for each flight with the overbooking strategy? 
b.  What is the probability that the net profit with the overbooking strategy will be less 
than the net profit without overbooking (50*$100 5 $5,000)?
c.  Explain how your simulation model could be used to evaluate other overbooking 
levels such as 51, 53, and 54 and for recommending a best overbooking strategy. 
12.  The wedding date for a couple is quickly approaching, and the wedding planner must 
provide the caterer an estimate of how many people will attend the reception so that 
the appropriate quantity of food is prepared for the buffet. The following table contains 
 information on the number of RSVP’ed guests for the 145 invitations. Unfortunately, the 
number of guests does not always correspond to the number of RSVP’ed guests.
 
 Based on her experience, the wedding planner knows it is extremely rare for guests 
to attend a wedding if they affirmed that they will not be attending. Therefore, the wed-
ding planner will assume that no one from these 50 invitations will attend. The wedding 
planner estimates that the each of the 25 guests planning to come solo has a 75 percent 

532 
Chapter 11 Monte Carlo Simulation
chance of attending alone, a 20 percent chance of not attending, and a 5 percent chance of 
bringing a companion. For each of the 60 RSVPs who plan to bring a companion, there 
is a 90 percent chance that she or he will attend with a companion, a 5 percent chance of 
attending solo, and a 5 percent chance of not attending at all. For the 10 people who have 
not responded, the wedding planner assumes that there is an 80 percent chance that each 
will not attend, a 15 percent chance they will attend alone, and a 5 percent chance they will 
attend with a companion.
RSVP’ed Guests
Number  
of Invitations
0
1 
2
No response
50
25
60
10
a.  Assist the wedding planner by constructing a spreadsheet simulation model to deter-
mine the expected number of guests who will attend the reception. 
b.  To be accommodating hosts, the couple has instructed the wedding planner to use the 
Monte Carlo simulation model to determine X, the minimum number of guests for 
which the caterer should prepare the meal, so that there is at least a 90 percent chance 
that the actual attendance is less than or equal to X. What is the best estimate for the 
value of X?
13.  OuRx, a retail pharmacy chain, is faced with the decision of how much flu vaccine to 
order for the next flu season. OuRx has to place a single order for the flu vaccine several 
months before the beginning of the season because it takes four to five months for the 
supplier to create the vaccine. OuRx wants to more closely examine the ordering decision 
because, over the past few years, the company has ordered too much vaccine and too little. 
OuRx pays a wholesale price of $12 per dose to obtain the flu vaccine from the supplier 
and then sells the flu shot to their customers at a retail price of $20. Based on industry 
trends as feedback from their marketing managers, OuRx has generated a rough estimate 
of flu vaccine demand at their retail pharmacies. OuRx is confident that demand will range 
from 800,000 doses to 4,500,000 doses. The following table lists weights for demand val-
ues within this range. 
Demand
1,000,000
2,000,000
3,000,000
4,000,000
Weight
.05
.20
.50
.25
 
 Because OuRx earns a profit on flu shots that it sells and it can’t sell more than its supply, 
the appropriate profit computation depends on whether demand exceeds the order quantity 
or vice versa. Similarly, the number of lost sales and excess doses depends on whether 
demand exceeds the order quantity or vice versa. 
a.  Construct a spreadsheet model that computes net profit corresponding to a given level 
of demand and specified order quantity. Model demand as a random variable with 
ASP’s custom general distribution.
b.  Using simulation optimization, determine the order quantity that maximizes expected 
profit. What is the probability of running out of flu vaccine at this order quantity?
c.  How many doses does OuRx need to order so that the probability of running out of 
flu vaccine is only 25 percent? How much expected profit will OuRx lose if it orders 
this amount rather than the amount from part b?
14. Recall the Land Shark example within the chapter. 
a.  Use ASP to apply simulation optimization to determine Land Shark’s bid amount that 
would maximize its expected return on the upcoming auction.
b.  What is Land Shark’s probability of winning the auction at the bid amount from part a?
file
WEB
LandSharkModel

 
Problems 
533
15.  At a local university, the Student Commission on Programming and Entertainment 
(SCOPE) is preparing to host its first rock concert of the school year. To successfully 
produce this rock concert, SCOPE has to complete several activities. The following table 
lists information regarding each activity. An activity’s immediate predecessors are those 
activities that must be completed before the considered activity can begin. The table also 
lists duration estimates (in days) for each activity.
Activity
Immediate
Predecessors
Minimum 
Time
Likely 
Time
Maximum 
Time
A: Negotiate contract with selected musicians
B: Reserve site
C: Logistical arrangements for music group
D: Screen and hire security personnel 
E: Advertising and ticketing
F: Hire parking staff
G: Arrange concession sales
—
—
A
B
B, C
D
E
5
8
5
3
1
4
3
 6
12
 6
 3
 5
 7
 8
 9
15
 7
 3
 9
10
10
 
 The following network illustrates the precedence relationships in the SCOPE project. The 
project begins with activities A and B, which can start immediately (time 0) because they 
have no predecessors. On the other hand, activity E cannot be started until activities B and 
C are both completed. The project is not complete until all activities are completed. 
A
Start
Finish
C
E
B
D
F
G
a.  Using the PERT distribution in ASP to represent the duration of each activity, 
 construct a simulation model to compute the total time to complete the concert 
 preparations.
b.  What is the expected duration of the entire project? What is the standard deviation of 
the project duration?
c. What is the likelihood that the project will be complete in 23 days?
16.  Burger Dome is a fast-food restaurant currently appraising its customer service. In its 
current operation, an employee takes a customer’s order, tabulates the cost, receives 
payment from the customer, and then fills the order. Once the customer’s order is filled, 
the employee takes the order of the next customer waiting for service. Assume that 
time between each customer’s arrival is an exponential random variable with a mean of 
1.35 minutes. Assume that the time for the employee to complete the customer’s service 
is an exponential random variable with mean of 1 minute. Use the Burgerdome file to 
complete a simulation model for the waiting line at Burger Dome for a 14-hour workday. 
Note that you will need to use native Excel functionality to solve this problem because 
the educational version of ASP has a limit of 100 random variables. Recall the formula 
5LN(RAND())*(-m) generates a value for an exponential random variable with mean m. 
Using the summary statistics gathered at the bottom of the spreadsheet model, answer the 
following questions. 
a. What is the average wait time experienced by a customer? 
b. What is the longest wait time experienced by a customer? 
c. What is the probability that a customer waits more than 2 minutes?
d. Create a histogram depicting the wait time distribution.
e.  By pressing the F9 key to generate a new set of simulation trials, one can observe the 
variability in the summary statistics from simulation to simulation. Typically, this 
variability can be reduced by increasing the number of trials. Why is this approach 
not appropriate for this problem?
file
WEB
BurgerDome

534 
Chapter 11 Monte Carlo Simulation
17.  One advantage of simulation is that a simulation model can be altered easily to reflect a 
change in the assumptions. Refer to the Burger Dome analysis in Problem 16. Assume 
that the service time is more accurately described by a normal distribution with a mean 
of 1 minute and a standard deviation of 0.2 minute. This distribution has less variability 
than the exponential distribution originally used. What is the impact of this change on the 
output measures?
18.  Refer to the Burger Dome analysis in Problem 16. Burger Dome wants to consider the effect 
of hiring a second employee to serve customers (in parallel with the first employee). Use the 
Burgerdometwoservers file to complete a simulation model that accounts for the second 
employee. (hint: The time that a customer begins service will depend on the availability of 
employees.) What is the impact of this change on the output measures?
19.  Land Shark is investigating the sensitivity of its model to the assumptions it made on the 
random variables. In particular, Land Shark is interested in modeling how it generates its 
competitor’s bid percentages. 
a.  Rather than generating a competitor’s bid percentage by resampling from the 72 
 observed bid amounts, use ASP to fit an appropriate distribution using these 72 data 
points and rerun the simulation model. In addition to considering the fit of the distribu-
tion to the data, when selecting a distribution, keep in mind the range of bid amounts 
that would be reasonable.
b.  For Land Shark’s bid amount of $1,250,000, how does the estimate of its probability 
of winning the auction differ from the model developed in Section 11.3?
c.  Comment on the implications of modeling random variables and which approach you 
feel is more appropriate in this case.
20.  For this problem, use the Land Shark simulation model from Problem 19 that fits a distri-
bution to data in order to simulate competitor bid amounts. 
a.  Use ASP to apply simulation optimization to determine Land Shark’s bid amount 
(rounded to the nearest $1,000) that maximizes its expected return. To reduce the 
time to solve this model, you may want to reduce the number of trials per simulation 
to 1000. What is the probability that Land Shark wins the auction?
b.  If Land Shark bids $5,000 more than the amount in part a, what is the likelihood that it 
wins the auction? How much expected return does Land Shark  sacrifice by increasing 
its bid in this manner?
21.  Orange Tech (OT) is a software company providing a suite of programs that are essential 
to everyday business computing. OT has just enhanced its software and released a new 
version of its programs. For financial planning purposes, OT needs to forecast its revenue 
over the next few years. To begin this analysis, OT is considering one of its largest cus-
tomers. This customer always eventually upgrades to the newest software version, but 
the number of years that pass before the customer purchases an upgrade varies. Up to the 
year that the customer actually upgrades, assume there is a 0.50 probability that the cus-
tomer upgrades in any particular year. In other words, the upgrade year of the customer 
is a random variable. For guidance on an appropriate way to model upgrade year, refer to 
Appendix 11.1. Furthermore, the revenue that OT earns from the customer’s upgrade also 
varies (depending on the number of programs the customer decides to upgrade). Assume 
that the revenue from an upgrade obeys a normal distribution with a mean of $100,000 
and a standard deviation of $25,000. Using the template in the file orangetech, complete 
a simulation model that analyzes the net present value of the revenue from the customer 
upgrade. Use an annual discount rate of 10 percent. 
a. What is the average net present value that OT earns from this customer? 
b.  What is the standard deviation of net present value? How does this compare to the 
standard deviation of the revenue? Explain.
22.  To boost holiday sales, Ginsberg jewelry store is advertising the following promotion: “If 
more than 7 inches of snow fall in the first 7 days of the year (january 1 through january 7),  
purchases made between Thanksgiving and Christmas are free!”
file
WEB
BurgerDomeTwoServers
file
WEB
LandSharkModel
file
WEB
OrangeTech

 
Problems 
535
 
 Based on historical sales records as well as experience with past promotions, the store 
manager believes that the total holiday sales between Thanksgiving and Christmas could 
range anywhere between $200,000 and $400,000 but is unsure of anything more specific. 
Ginsberg has collected data on snowfall from December 17 to january 16 for the past 
several winters. 
a.  Construct a simulation model to assess potential refund amounts so that Ginsberg 
can evaluate the option of purchasing an insurance policy to cover potential losses. 
Be sure to account for the correlation in snowfall on consecutive days. Refer to 
 Appendix 11.2 for instructions on how to incorporate correlation using ASP. To aid 
the computation of correlation in consecutive days, the data in the file ginsberg has 
been organized so that one column contains the snowfall amount on the day cor-
responding to the Day column and another column contains snowfall amount on the 
following day. You may safely ignore any correlation between every second day, 
every third day, and so on.
b.  What is the probability that Ginsberg will have to refund sales?
c. What is the expected refund? Why is this a poor measure to use to assess risk?
d. What is the expected refund if snowfall exceeds 7 inches?
23.  A creative entrepreneur has created a novelty soap called jackpot. Inside each bar of jack-
pot soap is a rolled-up bill of U.S. currency. There are 1000 bars of soap in the initial of-
fering of the soap. Although the denomination of the bill inside a bar of soap is unknown, 
the distribution of bills in these first 1000 bars is given in the following table:
Bill  
Denomination
Number of Bills
$1
$5
$10
$20
$50
$100
Total
 520
 260
 130
  60
  29
   1
1000
 
 How many bars of soap does a customer have to buy so that, on average, she has pur-
chased two containing a $50 or $100 bill? (hint: Use the hypergeometric distribution in 
ASP to answer this question.)
24.  Refer to the jackpot soap scenario in Problem 23. After the sale of the original 1000 bars of 
soap, jackpot soap went viral, and the soap has become wildly popular. Production of the 
soap has been ramped up so that now millions of bars have been produced. However, the 
distribution of the bills in the soap obeys the same distribution as outlined in Problem 23.  
On average, how many bars of soap will a customer have to buy before purchasing three 
bars of soap each containing a bill of at least $20 value? Use the negative binomial distri-
bution in ASP to answer this question.
25.  Press Teag Worldwide (PTW) has investments around the world that generate revenue in 
the British pound, the New Zealand kiwi, and the japanese yen. At the end of each quar-
ter, PTW converts the revenue from these three international operations back into U.S. 
 dollars, exposing PTW to exchange rate risk. The current exchange rates are US$1.60 
per £1, US$0.82 per NZD$1, and US$0.01 per ¥1. PTW wants to construct a simulation 
model to assess its vulnerability to uncertain exchange rate fluctuations. The quarterly 
revenues generated in British pounds, New Zealand kiwis, and japanese yen are £100,000, 
NZD$250,000, and ¥10,000,000, respectively. 
a.  If exchange rates stay at their current values, what is the total quarterly revenue in U.S. 
dollars?
file
WEB
Ginsberg
file
WEB
ExchangeRates

536 
Chapter 11 Monte Carlo Simulation
b.  Following the instructions in Appendix 11.1, model the uncertainty in the quarterly 
changes of the exchange rates between U.S. dollars and British pounds, New Zealand 
kiwis, and japanese yen using a SLURP. 
c.  Use your simulation model to estimate the average total quarterly revenue in U.S. 
dollars. What is the probability that the total quarterly revenue will be lower than the 
answer in part a?
Four Corners
What will your portfolio be worth in 10 years? In 20 years? When you stop working? The 
Human Resources Department at Four Corners Corporation was asked to develop a finan-
cial planning model that would help employees address these questions. Tom Gifford was 
asked to lead this effort and decided to begin by developing a financial plan for himself. 
Tom has a degree in business and, at the age of 40, is making $85,000 per year. Through 
contributions to his company’s retirement program and the receipt of a small inheritance, 
Tom has accumulated a portfolio valued at $50,000. Tom plans to work 20 more years and 
hopes to accumulate a portfolio valued at $1,000,000. Can he do it?
Tom began with a few assumptions about his future salary, his new investment contri-
butions, and his portfolio growth rate. He assumed a 5 percent annual salary growth rate and 
plans to make new investment contributions at 6 percent of his salary. After some research 
on historical stock market performance, Tom decided that a 10 percent annual portfolio 
growth rate was reasonable. Using these assumptions, Tom developed the following Excel 
worksheet:
A
C
Four Corners
Age
Current Salary
Current Portfolio
Annual Investment Rate
Salary Growth Rate
Portfolio Growth Rate
40
$85,000
$50,000
6%
5%
10%
Year
Beginning Balance
3
4
5
2
1
$72,013
$85,118
$99,829
$60,355
$50,000
Salary
$93,713
$98,398
$103,318
$89,250
$85,000
New Investment
$5,623
$5,904
$6,199
$5,355
$5,100
Earnings
$7,482
$8,807
$10,293
$6,303
$5,255
Ending Balance
$85,118
$99,829
$116,321
$72,013
$60,355
Age
43
44
45
42
41
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
B
D
E
G
F
The worksheet provides a financial projection for the next five years. In computing 
the portfolio earnings for a given year, Tom assumed that his new investment contribution 
would occur evenly throughout the year, and thus half of the new investment could be 
included in the computation of the portfolio earnings for the year. From the worksheet, we 
see that, at age 45, Tom is projected to have a portfolio valued at $116,321.
Tom’s plan was to use this worksheet as a template to develop financial plans for the 
company’s employees. The data in the spreadsheet would be tailored for each employee, 
and rows would be added to it to reflect the employee’s planning horizon. After adding 
another 15 rows to the worksheet, Tom found that he could expect to have a portfolio of 
$772,722 after 20 years. Tom then took his results to show his boss, Kate Krystkowiak.
Case Problem

 
Appendix 11.1 Incorporating Dependence Between Random Variables 
537
Although Kate was pleased with Tom’s progress, she voiced several criticisms. One of 
the criticisms was the assumption of a constant annual salary growth rate. She noted that 
most employees experience some variation in the annual salary growth rate from year to 
year. In addition, she pointed out that the constant annual portfolio growth rate was unreal-
istic and that the actual growth rate would vary considerably from year to year. She further 
suggested that a simulation model for the portfolio projection might allow Tom to account 
for the random variability in the salary growth rate and the portfolio growth rate.
After some research, Tom and Kate decided to assume that the annual salary growth rate 
would vary from 0 to 5 percent and that a uniform probability distribution would provide a 
realistic approximation. Four Corner’s accountants suggested that the annual portfolio growth 
rate could be approximated by a normal probability distribution with a mean of 10 percent and 
a standard deviation of 5 percent. With this information, Tom set off to redesign his spread-
sheet so that it could be used by the company’s employees for financial planning.
Managerial Report
Play the role of Tom Gifford, and develop a simulation model for financial planning. Write 
a report for Tom’s boss and, at a minimum, include the following:
 1.  Without considering the random variability, extend the current worksheet to 20 years. 
Confirm that by using the constant annual salary growth rate and the constant annual 
portfolio growth rate, Tom can expect to have a 20-year portfolio of $772,722. What 
would Tom’s annual investment rate have to increase to in order for his portfolio to 
reach a 20-year, $1,000,000 goal? (hint: Use Goal Seek.)
 2.  Redesign the spreadsheet model to incorporate the random variability of the annual 
salary growth rate and the annual portfolio growth rate into a simulation model. As-
sume that Tom is willing to use the annual investment rate that predicted a 20-year, 
$1,000,000 portfolio in part 1. Show how to simulate Tom’s 20-year financial plan. 
Use results from the simulation model to comment on the uncertainty associated with 
Tom reaching the 20-year, $1,000,000 goal. 
 3.  What recommendations do you have for employees with a current profile similar to 
Tom’s after seeing the impact of the uncertainty in the annual salary growth rate and 
the annual portfolio growth rate?
 4.  Assume that Tom is willing to consider working 25 more years instead of 20 years. 
What is your assessment of this strategy if Tom’s goal is to have a portfolio worth 
$1,000,000?
 5.  Discuss how the financial planning model developed for Tom Gifford can be used as 
a template to develop a financial plan for any of the company’s employees. 
Incorporating Dependence 
Between random Variables
In the simulation models in this chapter, we have represented all random variables with 
independent probability distributions. In other words, the values of the random variables 
have been generated independently and have no dependence on each other. There are many 
situations in which the values of two or more random variables have strong interrelation-
ships. Consider two complementary products such as razors and replacement blades. Above- 
average sales of razors are often associated with above-average sales of replacement blades. 
In finance, the notion of diversification is based on investing in assets such that when one 
asset provides a below-average return, another asset often provides an above-average return. 
ASP provides two ways to model the dependence between random variables: (1) through 
the use of correlation to express the degree of dependence between each pair of random 
file
WEB
FourCorners
for a review of goal seek, 
refer to chapter 7. 
Appendix 11.1

538 
Chapter 11 Monte Carlo Simulation
variables, or (2) by directly resampling the data so that the values of the random variables 
occur in combination exactly as they do empirically in the data. We present examples of 
both approaches. 
Correlation
In Chapter 2, we presented the most common correlation measure, the Pearson product mo-
ment correlation coefficient. The Pearson product moment correlation coefficient ranges 
between 21 and 11. Values closer to 21 denote a stronger negative linear relationship 
between a pair of random variables. Values closer to 11 denote a stronger positive linear 
relationship between a pair of random variables. A correlation measure close to 0 denotes 
the absence of a linear relationship between a pair of random variables. 
ASP employs a slightly different measure of correlation called the Spearman rank 
correlation. Like the Pearson product moment correlation coefficient, the Spearman rank 
correlation coefficeint ranges between 21 and 11. However, Spearman rank correlation 
measures the strength of a monotonic relationship between two variables; this differs from 
Pearson product moment correlation which measures the strength of a linear relationship 
between two variables. 
The left panel of Figure 11.34 illustrates a negative monotonic relationship (Spearman 
rank correlation coefficient 5 21). The right panel of Figure 11.34 illustrates a positive 
monotonic relationship (Spearman rank correlation coefficient 5 11). On a scatter chart of 
two variables with a monotonic relationship, the slope of the lines connecting consecutive 
data points will never change sign, that is, the slopes will either be all less than or equal to 
zero, or all greater than or equal to zero.
Recall the Zappos problem from Section 11.3. Our original simulation model treats 
the demand for the four pajama products as independent random variables. However, it is 
likely that the demand for these products is not independent in reality. Suppose that market 
research suggests that cotton and flannel pajamas are often substitutes; that is, customers 
often buy one or the other, but rarely both. Based on the sample data on cotton and pajama 
demand in the data worksheet of the Zappos workbook, Figure 11.35 illustrates the evi-
dence of a negative relationship between cotton and flannel pajama demand.
To compute the Spearman rank correlation coefficient to measure the dependence be-
tween cotton and flannel pajama demand, we first find the rank of each observation of 
cotton pajama demand and the rank of each observation of flannel pajama demand using 
the Excel function RANK.AVG. Columns E and F in Figure 11.36 show the rank of each 
observed value of cotton pajama demand and flannel pajama demand, respectively. For 
asP’s use of the spearman 
rank correlation coef-
ficent allows it to generate 
interrelated values between 
pairs of random variables 
which have different prob-
ability distributions. 
When a value occurs more 
than once, the RanK.aVg 
function returns the aver-
age rank of the value.
FIGURE 11.34   MONOTONIC DECREASING RELATIONSHIP (LEFT PANE) AND MONOTONIC 
 INCREASING RELATIONSHIP (RIGHT PANE)
0
1
2
3
4
5
6
7
8
10
9
0
2
1
3
4
5
7
6
8
9
10
0
2
1
3
4
5
7
6
8
9
10
0
1
2
3
4
5
6
7
8
10
9

 
Appendix 11.1 Incorporating Dependence Between Random Variables 
539
FIGURE 11.35   DEPENDENCE BETWEEN COTTON PAjAMA DEMAND AND 
FLANNEL PAjAMA DEMAND
5000
0
10,000
15,000
20,000
25,000
30,000
Cotton Pajama Demand
0
25,000
20,000
15,000
10,000
5000
Flannel Pajama Demand
FIGURE 11.36   CALCULATING SPEARMAN RANK CORRELATION COEFFICIENT 
A
B
C
D
1
2
3
4
5
6
7
8
11
10
9
13
12
14
15
16
17
18
19
23
22
24
25
26
27
20
21
E
F
Season
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
Cotton
21311
13311
17573
17994
21710
22882
17279
25218
19315
22553
23841
18969
23058
22497
17334
21659
21443
20018
20409
18333
15571
25657
14191
16013
Pearson:
Flannel
6800
15052
9145
12601
9978
7052
16298
3235
12241
9563
3454
12327
9651
9494
15241
9684
3401
19839
5252
12006
14290
6580
20398
12848
Cotton Rank
=RANK_AVG(B2,$B$2:$B$25)
=RANK_AVG(B3,$B$2:$B$25)
=RANK_AVG(B4,$B$2:$B$25)
=RANK_AVG(B5,$B$2:$B$25)
=RANK_AVG(B6,$B$2:$B$25)
=RANK_AVG(B7,$B$2:$B$25)
=RANK_AVG(B8,$B$2:$B$25)
=RANK_AVG(B9,$B$2:$B$25)
=RANK_AVG(B10,$B$2:$B$25)
=RANK_AVG(B11,$B$2:$B$25)
=RANK_AVG(B12,$B$2:$B$25)
=RANK_AVG(B13,$B$2:$B$25)
=RANK_AVG(B14,$B$2:$B$25)
=RANK_AVG(B15,$B$2:$B$25)
=RANK_AVG(B16,$B$2:$B$25)
=RANK_AVG(B17,$B$2:$B$25)
=RANK_AVG(B18,$B$2:$B$25)
=RANK_AVG(B19,$B$2:$B$25)
=RANK_AVG(B20,$B$2:$B$25)
=RANK_AVG(B22,$B$2:$B$25)
=RANK_AVG(B21,$B$2:$B$25)
=RANK_AVG(B23,$B$2:$B$25)
=RANK_AVG(B25,$B$2:$B$25)
=RANK_AVG(B24,$B$2:$B$25)
=RANK_AVG(C2,$C$2:$C$25)
=RANK_AVG(C3,$C$2:$C$25)
=RANK_AVG(C4,$C$2:$C$25)
=RANK_AVG(C5,$C$2:$C$25)
=RANK_AVG(C6,$C$2:$C$25)
=RANK_AVG(C7,$C$2:$C$25)
=RANK_AVG(C8,$C$2:$C$25)
=RANK_AVG(C9,$C$2:$C$25)
=RANK_AVG(C10,$C$2:$C$25)
=RANK_AVG(C11,$C$2:$C$25)
=RANK_AVG(C12,$C$2:$C$25)
=RANK_AVG(C13,$C$2:$C$25)
=RANK_AVG(C14,$C$2:$C$25)
=RANK_AVG(C15,$C$2:$C$25)
=RANK_AVG(C16,$C$2:$C$25)
=RANK_AVG(C17,$C$2:$C$25)
=RANK_AVG(C18,$C$2:$C$25)
=RANK_AVG(C19,$C$2:$C$25)
=RANK_AVG(C20,$C$2:$C$25)
=RANK_AVG(C22,$C$2:$C$25)
=RANK_AVG(C21,$C$2:$C$25)
=RANK_AVG(C23,$C$2:$C$25)
=RANK_AVG(C25,$C$2:$C$25)
=RANK_AVG(C24,$C$2:$C$25)
=CORREL(B2:B25,C2:C25)
Spearman:
=CORREL(E2:E25,F2:F25)
Flannel Rank
A
B
C
D
1
2
3
4
5
6
7
8
11
10
9
13
12
14
15
16
17
18
19
23
22
24
25
26
27
20
21
E
F
Season
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
Cotton
21,311
13,311
17,573
17,994
21,710
22,882
17,279
25,218
19,315
22,553
23,841
18,969
23,058
22,497
17,334
21,659
21,443
20,018
20,409
18,333
15,571
25,657
14,191
16,013
Pearson:
Flannel
6,800
15,052
9,145
12,601
9,978
7,052
16,298
3,235
12,241
9,563
3,454
12,327
9,651
9,494
15,241
9,684
3,401
19,839
5,252
12,006
14,290
6,580
20,398
12,848
Cotton Rank
-0.7456
Spearman:
Flannel Rank
11
24
18
17
8
5
20
2
14
6
3
15
4
7
19
9
10
13
12
16
22
1
23
21
–0.7713
19
5
17
8
12
18
3
24
10
15
22
9
14
16
4
13
23
2
21
11
6
20
1
7

540 
Chapter 11 Monte Carlo Simulation
 example, cell E2 contains the formula 5RANK.AVG(B2, $B$2:$B$25) and returns the 
value of 11 to denote that demand of 21,311 cotton pajamas is the eleventh-largest observa-
tion in the list of 24 values.
The Spearman rank correlation coefficent is computed by applying the Excel function 
CORREL on the rank data for cotton and flannel pajamas. In Figure 11.36, we compute the 
Spearman rank correlation coefficient in cell F27 using the formula 5CORREL(E2:E25, 
F2:F25). We contrast this with the Pearson product moment correlation coefficient com-
puted in cell C27 by applying the CORREL function on the original demand observations 
(see the formula 5CORREL(B2:B25, C2:C25) in cell C27 of Figure 11.36). In this case, 
we see that the Pearson product moment correlation coefficient (–0.7456) is close to the 
value of the Spearman rank correlation coefficient (–0.7713), but this is not always the case. 
To model the dependence between random variables, ASP uses Spearman rank correlation. 
Although there are no data for the demand of silk and velour pajamas, Zappos believes 
that the demand will have a moderate negative correlation and wants to consider this in 
the simulation model by using an estimated correlation of –0.5. Further, Zappos believes 
that cotton pajama demand will have a slight positive correlation with both silk and velour 
pajamas and wants to use an estimate of 0.25 for the correlation between cotton and silk 
pajama demand, as well as between cotton and velour pajama demand. Zappos does not 
have a strong evidence regarding the interdependence between flannel pajamas and silk or 
velour pajamas, respectively. Based on intuition, Zappos will set the correlation between 
flannel and velour to 0.25 and the correlation between flannel and silk to zero. To set up 
a correlation matrix that captures these interrelationships, we execute the following steps:
Step 1. Starting from the Model worksheet in the file ZapposRank, click the  ANALYTIC 
SOLVER  PLATFORM tab in the Ribbon
Step 2. Click the Correlations icon in the Simulation Model group
Step 3. When the Create new correlation matrix dialog box opens, click the >> but-
ton to populate the Preview area
Step 4. In the Preview area of the Create new correlations matrix dialog box:
Click the matrix entry in the $E$5 row and $E$6 column and enter 20.77
Click the matrix entry in the $E$5 row and $E$7 column and enter 0.25
Click the matrix entry in the $E$5 row and $E$8 column and enter 0.25
Click the matrix entry in the $E$6 row and $E$7 column and enter 0
Click the matrix entry in the $E$6 row and $E$8 column and enter 0.25
Click the matrix entry in the $E$7 row and $E$8 column and enter 20.5
Step 5. Click Save
Step 6. When the dialog bubble Please select the location of the Matrix appears, 
select cell A28
When subjectively estimating correlation coefficients, it is possible to define values that 
are mathematically inconsistent. For example, if stock A’s return is positively correlated 
with stock B’s return, and stock B’s return is positively correlated with stock C’s return, then 
stock C’s return cannot be negatively correlated with stock A’s return. ASP validates and, if 
necessary, adjusts the correlation matrix to maintain mathematical consistency.  Continuing 
from the previous steps, we execute this validation by means of the following steps:
Step 7. In the Manage correlation matrices dialog box, click Validate
Step 8. Click Yes in the dialog box The Correlation Matrix is not Positive Definite. 
Do you want Analytic Solver Platform to help you fix the matrix?
Step 9. In the Make your matrix valid dialog box (see Figure 11.38)
Click the Largest Change button and click the matrix entry in the $E$6 
row and $E$7 column
Click the Moderate Change button and click the matrix entry in the 
$E$6 row and $E$8 column
file
WEB
ZapposRank
clicking on the arrow 
under the Correlations 
icon opens a drop-down 
menu. clicking Matrices 
from this menu is equiva-
lent to clicking on the 
 Correlations icon.
the entry in row i and col-
umn j in the lower diagonal 
of the correlation matrix 
in figure 11.37 displays 
a scatter chart of data for 
the two variables with the 
corresponding correla-
tion measure in row j and 
column i.
if correlation coefficients 
are defined in a manner 
that is mathematically 
consistent, the correlation 
matrix is said to be positive 
definite. after clicking 
Validate, if the correla-
tion matrix is not positive 
definite, asP will alert the 
analyst and offer to adjust 
the correlation coefficients 
so that the correlation 
 matrix is positive definite.


542 
Chapter 11 Monte Carlo Simulation
Step 10. Click Update Matrix
Step 11. Click Accept Update (Figure 11.39)
Step 12. Close Manage correlation matrices dialog box
In this case, Zappos’ qualitative estimates of correlation are not mathematically con-
sistent. As Figure 11.38 shows, we adjust this by instructing ASP which estimates we will 
allow it to adjust in order to maintain mathematical consistency. In this case, Zappos is 
not at all confident in its estimates of the correlation between flannel and silk pajamas and 
between flannel and velour pajamas. Figure 11.39 shows the correlation matrix after adjust-
ment, and we see that the changes are very minor, so we accept them without reservation.
The effect of modeling correlation can be analyzed by running the simulation with and 
without correlations by clicking on the drop-down arrow below the Correlations button 
in the Simulations Model group of the ANALYTIC SOLVER PLATFORM tab in the 
Ribbon and then checking (and unchecking) the box next to Use  Correlations. For the 
Zappos problem, running a simulation with the correlations (and order quantities of 10,000 
units of each pajama type), we obtain the distribution of total profit shown in Figure 11.40. 
Comparing Figure 11.26 to Figure 11.40, we see that considering the correlation between 
demands has a negligible effect when  ordering 10,000 units each pajama type. Repeating 
the simulation trials shows that there is a negligible difference in the simulation model with 
or without correlated demand. However, this will not always be the case, and we advise 
accounting for correlation between random variables when there is evidence that it exists.
Stochastic Library Unit with Relationships Preserved (SLURP)
In this section, we demonstrate how to make use of large amounts of data to capture the 
dependence between random variables in a simulation model. By directly resampling the 
FIGURE 11.39   ADjUSTED CORRELATION MATRIX

 
Appendix 11.1 Incorporating Dependence Between Random Variables 
543
data so that the values of the random variables occur in combination exactly as they have 
in the data, we can empirically preserve any relationship between the random variables. 
Analytic Solver Platform facilitates this resampling approach through a stochastic library 
unit with relationships preserved (SLURP). A SLURP is defined by a table of sample 
values where each column corresponds to a random variable and each row corresponds to 
a set of associated values for the random variables. One advantage of a SLURP is that, by 
resampling the data directly, a SLURP can capture nonmonotonic relationships that are 
missed using the correlation coefficient approach of the previous section. We demonstrate 
the use of a SLURP with an example.
Press Teag Worldwide (PTW) has investments around the world that generate revenue 
in foreign currencies, and therefore the company is extremely interested in exchange rates. 
In particular, PTW is exposed to currency exchange risk in Great Britain, New Zealand, 
and japan. PTW is interested in analyzing its exposure to the fluctuations in the currency 
exchange rates of the British pound, the New Zealand dollar (the kiwi), and the japanese 
yen. Typically, PTW converts at least some of its international earnings into U.S. dollars 
each quarter and would like to base its hedging strategy on sound analysis. 
To model the fluctuation in the exchange rate between the American dollar and British 
pound, PTW expresses the number of U.S. dollars ($) per British pound (£) one quarter 
from now by
Quarter end $ per £ 5 quarter begin $ per £ 1 (quarter begin $ per £ 3 quarterly rate change in $ per £) (11.10)
The number of U.S. dollars per kiwi and per japanese yen (¥) can be computed by equations 
analogous to equation (11.10). 
In equation (11.10), the quarterly rate change in U.S. dollars per British pound is a ran-
dom variable. Similarly, the quarterly rate change in U.S. dollars per kiwi and U.S. dollars 
per japanese yen also are random variables. Using a proprietary macroeconomic model and 
historical information, PTW has compiled data on the quarterly change in the value of the 
file
WEB
ExchangeRates
FIGURE 11.40   ZAPPOS TOTAL PROFIT DISTRIBUTION WITH CORRELATED  DEMAND
$300
0.00
0.05
0.10
Relative Probability
0.15
0.20
0.25
0.30
Simulation Results - $J$9
1.30%
98.70%
350
$350
$400
$450
(Values in Thousands)
$500
$550
$600
$650
$700
0
50
100
Frequency
150
200
250
300

544 
Chapter 11 Monte Carlo Simulation
dollar against these currencies (Figure 11.41). Realizing that the fluctuations in exchange 
rates are highly interdependent, PTW would like use these data to construct a SLURP. 
Figure 11.42 and the following steps illustrate the process of building a simulation model 
using a SLURP using the file exchangeRates:
Step 1. In the Model worksheet, enter the formula 5PsiSlurp(Data!A2:C901, 1) in 
cell B6
Step 2. Enter the formula 5PsiSlurp(Data!A2:C901, 2) in cell C6
Step 3. Enter the formula 5PsiSlurp(Data!A2:C901, 3) in cell D6
Step 4. Select cell B7
Step 5. Click the ANALYTIC SOLVER PLATFORM tab on the Ribbon
Step 6. Click the Results icon in the Simulation Model group
Select Output and click In Cell
Step 7. Repeat steps 4 through 6 for cells C7 and D7
Step 8. Click the arrow under Simulate from the Solve Action group
From the drop-down menu that appears, click Interactive
file
WEB
ExchangeRates
FIGURE 11.41   DATA FOR QUARTERLY CURRENCY RATE CHANGE
A
C
USD/GBP
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
21
20
22
24
23
25
26
–10.47%
–17.00%
1.10%
11.14%
4.61%
5.36%
1.95%
5.29%
–2.08%
–1.23%
–0.21%
–6.45%
–2.05%
–0.09%
–1.48%
–0.87%
–1.85%
–0.47%
1.99%
5.29%
0.20%
–0.26%
0.99%
–0.78%
–0.28%
1.02%
3.13%
2.15%
4.01%
USD/NZD
–2.55%
5.12%
–0.27%
–2.34%
–0.75%
0.55%
–1.59%
–2.07%
–1.89%
–2.83%
–3.18%
–1.07%
1.86%
2.94%
–0.86%
–1.24%
2.78%
1.39%
1.46%
1.97%
–1.00%
–1.14%
4.30%
0.24%
3.12%
2.13%
3.56%
3.37%
3.93%
USD/JPY
Data
Model
–4.72%
6.81%
11.23%
5.41%
–1.47%
4.38%
–2.41%
–3.20%
1.40%
0.77%
1.58%
1.65%
4.09%
2.86%
–10.15%
–7.39%
–4.03%
–1.61%
–1.37%
1.08%
–6.74%
–3.43%
13.81%
–2.41%
0.51%
4.21%
4.29%
0.20%
10.03%
B
28
29
30
27

 
Appendix 11.2 Probability Distributions for Random Variables 
545
Figure 11.42 displays the PTW spreadsheet model with the use of a SLURP for the 
quarterly change in the respective exchange rates in cells B6, C6, and E6. Cells B7, C7, 
and E7 contain the logic of equation (11.9) to compute the quarter end exchange rate for 
each currency. 
By clicking the right and left arrows in the Tools group of the ANALYTIC SOLVER 
PLATFORM tab in the Ribbon, we can see that each set of values that the SLURP gener-
ates for the random variables in cells B6, C6, and D6 correspond to a row of values in the 
data worksheet. For example, simulation trial 1 corresponds to the values 1.10 percent, 
–2.55 percent, and –4.72 percent; simulation trial 2 corresponds to the values 11.14 percent, 
5.12 percent, and 6.81 percent, respectively. With the SLURP in place to generate values of 
the random variables, PTW can now complete the simulation model to analyze its exposure 
to exchange rate fluctuations. 
probability Distributions for 
random Variables
Selecting the appropriate probability distribution to characterize a random variable in a 
simulation model can be a critical modeling decision. In this appendix, we review several 
of the distributions provided by Analytic Solver Platform in Excel. For each distribution, 
the parameters are the values required to completely specify the distribution. The range 
provides the minimum and maximum values that can be taken by a random variable that 
follows the given distribution. We also provide a short description of the overall shape and/
or common uses of the distribution.
Continuous Probability Distributions
Random variables that can be many possible values (even if the values are discrete) are 
often modeled with a continuous probability distribution. 
Appendix 11.2
FIGURE 11.42   FORMULA AND VALUE VIEW OF PRESS TEAGUE WORLDWIDE SIMULATION MODEL
A
C
Press Teag Worldwide
Quarter Begin Exchange Rate ($ per)
Quarterly Change
Quarter End Exchange Rate ($ per)
Quarterly Revenue
Parameters
1.6
=PsiSlurp(Data!A2:C901,1)
=B5+(B5*B6) + PsiOutput()
100000
=PsiSlurp(Data!A2:C901,2)
=C5+(C5*C6) + PsiOutput()
0.82
250000
=PsiSlurp(Data!A2:C901,3)
=D5+(D5*D6) + PsiOutput()
0.01
10000000
1
2
3
4
5
6
7
8
9
B
D
A
C
Press Teag Worldwide
Parameters
Quarter Begin Exchange Rate ($ per)
Quarterly Change
Quarter End Exchange Rate ($ per)
Quarterly Revenue
1
2
3
4
5
6
7
8
9
B
D
$1.60
1.10%
$1.62
£100,000
$0.82
–2.55%
$0.80
$250,000
$0.01
–4.72%
$0.01
¥10,000,000
£
Kiwi
¥
£
Kiwi
¥

546 
Chapter 11 Monte Carlo Simulation
Normal Distribution
Parameters: mean (m), stdev (s)
Range: 2` to 1`
Description: The normal distribution is a bell-shaped, symmetric distribution centered 
at its mean m. The normal distribution is often a good way to characterize a quantity 
that is the sum of many independent random variables.
Example: In human resource management, employee performance is often well 
represented by a normal distribution. Typically the performance of 68 percent of 
employees is within one standard deviation of the average performance, and the 
performance of 95 percent of the employees is within two standard deviations. Em-
ployees with exceptionally low or high performance are rare. 
Beta Distribution
Parameters (for BetaGen): shape1 (a), shape2 (b), min (a), max (b)
Range: a to b
Description: The beta distribution provides values between a and b and by adjusting a 
and b. The beta distribution has a very flexible shape and is useful in modeling an 
uncertain quantity that has a known minimum and maximum value.
Example: Setting a 5 0 and b 5 1, the beta distribution can be used to describe the 
likelihood of values for the true proportion of drivers in an age group who would 
favor one model of car over another. 
PERT Distribution
Parameters: min (a), likely (m), max (b)
Range: a to b
Description: The PERT distribution is a special case of the beta distribution in which 
the most likely value is specified instead of shape parameters. The PERT distribu-
tion is used specifically for project management applications.
Example: In estimating the time to complete an activity, a project manager often uses 
the PERT distribution by specifying the minimum time to complete the activity, 
the most likely time to complete an activity, and the maximum time it will take to 
complete the activity. 
Exponential Distribution
Parameters: mean (m)
Range: 0 to 1`
Description: The exponential distribution is characterized by a mean value equal to its 
standard deviation and a long right tail stretching from a mode value of 0.
Example: The time between events, such as customer arrivals or customer defaults on 
bill payment, are commonly modeled with an exponential distribution. An exponen-
tial random variable possesses the “memoryless” property: the probability that there 
will be 25 or more minutes between customer arrivals if 10 minutes have passed 
since the last customer arrival is the same as the probability that there will be more 
than 15 minutes until the next arrival if a customer just arrived. In other words, the 
probability of a customer arrival occurring in the next x minutes does not depend on 
how long it’s been since the last arrival. 
Triangular Distribution
Parameters: min (a), likely (m), max (b)
Range: a to b
Description: The triangular distribution is often used when little is known about a 
random variable besides its range, but it is thought to have a single mode. The dis-
tribution is shaped like a triangle with vertices at a m and b

 
Appendix 11.2 Probability Distributions for Random Variables 
547
Example: In corporate finance, a triangular distribution may be used to model a 
project’s revenue growth in a net present value analysis if the analyst can reliably 
 provide a minimum, most likely, and maximum estimates of growth.
Uniform Distribution
Parameters: min (a), max (b)
Range: a to b
Description: The uniform distribution is appropriate when a random variable is equally 
likely to be any value between a and b. When little is known about a phenomenon 
besides its minimum and maximum possible values, the uniform distribution can be 
a safe choice to model an uncertain quantity.
Example: A service technician making a house call may quote a 4-hour time window 
in which he will arrive. If the technician is equally likely to arrive any time during 
this time window, then the arrival time of the technician in this time window may 
be described with a uniform distribution.
Log-Normal Distribution
Parameters: mean (m), stdev (s)
Range: 0 to 1`
Description: The log-normal distribution is a unimodal distribution (like the normal 
distribution) that has a minimum value of 0 and a long right tail (unlike the normal 
distribution). The log-normal distribution is often a good way to characterize a 
quantity that is the product of many independent, positive random variables. The 
logarithm of a log-normally distributed random variable is normally distributed. 
Example: The income distribution of a population is often well described using a 
 log-normal distribution.
Custom General Distribution 
Parameters: min (a), max (b), and a set of values {v1, v2, v3, . . . , vk}and corresponding 
weights {w1, w2, w3, . . . , wk}
Range: a to b
Description: A custom general distribution can be used to shape a tailored distribution 
to model any continuous uncertain quantity. 
Example: The boom-or-bust nature of the revenue generated by a movie from a po-
larizing director may be described by a custom discrete distribution. The relevant 
values (in millions of dollars) are a minimum of a 5 0, a maximum of b 5 70, and 
other values of {5, 25, 45, 65} with respective weights of {0.6, 0.05, 0.05, 0.3}. This 
particular distribution is U-shaped, and extreme values are more likely than moder-
ate values—a phenomenon not well captured by other distributions. 
Discrete Distributions
Random variables that can be only a relatively small number of discrete values are often 
best modeled with a discrete distribution. The appropriate choice of discrete distribu-
tion relies on the specific situation. For discrete distributions, we provide the parameters 
required to specify the distribution, the possible values taken by a random variable that 
follows the distribution, and a short description of the distribution and example of a pos-
sible application.
Bernoulli Distribution
Parameters: prob (p)
Possible values: 0 (event doesn’t happen) or 1 (event successfully occurs)
Description: A Bernoulli random variable corresponds to whether an event success-
fully occurs given a probability p of successfully occurring

548 
Chapter 11 Monte Carlo Simulation
Example: Whether a particular stock increases in value over a defined length of time 
is a Bernoulli random variable. 
Binomial Distribution
Parameters: trials (n), prob (p)
Possible values: 0, 1, 2, . . . , n
Description: A binomial random variable corresponds to the number of times an event 
successfully occurs in n trials, and the probability of a success at each trial is p and 
independent of whether a success occurs on other trials. Note that for n 5 1, the 
binomial is equivalent to the Bernoulli distribution.
Example: In a portfolio of 20 similar stocks, each of which has the same probability 
of increasing in value of p 5 0.6, the total number of stocks that increase in value 
can be described by a binomial distribution with parameters n 5 20 and p 5 0.6. 
Hypergeometric Distribution 
Parameters: trials (n), suc (s), pop (n)
Possible values: max{0, n 1 s – n}, . . . , min{n, s}
Description: A hypergeometric random variable corresponds to the number of times 
an element labeled a success is selected out of n trials in the situation where there 
are n total elements, s of which are labeled a success and, once selected, cannot be 
selected again. Note that this is similar to the binomial distribution except that now 
the trials are dependent because removing the selected element changes the prob-
abilities of selecting an element labeled a success on subsequent trials.
Example: A certain company produces circuit boards to sell to computer manufac-
turers. Because of a quality defect in the manufacturing process, it is known that 
exactly 70 circuit boards out of a lot of 100 have been produced incorrectly and are 
faulty. If a company orders 40 circuit boards, the number of faulty circuit boards 
that the company will receive in their order is a hypergeometric random variable 
with n 5 40, s 5 70, and n 5 100. Note that, in this case, at least 10 circuit boards 
will be faulty, but no more than 40 (because at most 30 circuit boards are not faulty). 
Geometric Distribution
Parameters: prob (p)
Possible values: 0, 1, 2, . . . , 
Description: A geometric random variable corresponds to the number of times that an 
event fails to occur until the first trial that an event successfully occurs, given that 
the probability of an event successfully occurring at each trial is p.
Example: Consider the research and development (R&D) division of a large com-
pany. An R&D division may invest in several projects that fail before investing in 
a project succeeds. If each project has a probability of success of p, the number of 
projects that fail before a successful project occurs is a geometric random variable 
with parameter p.
Negative Binomial Distribution
Parameters: suc (s), prob (p)
Possible values: 0, 1, 2, . . . ,
Description: A negative binomial random variable corresponds to the number of times 
that an event fails to occur until an event successfully occurs s times, given that the 
probability of an event successfully occurring at each trial is p. Note that for s 5 1, 
the negative binomial is equivalent to the geometric distribution.
Example: The number of projects that fail in the R&D division before experiencing 
three successful projects is a negative binomial random variable.

 
Appendix 11.2 Probability Distributions for Random Variables 
549
Poisson Distribution
Parameters: mean (m)
Possible values: 0, 1, 2, . . . ,
Description: A Poisson random variable corresponds to the number of times that an 
event occurs within a specified period of time given that m is the average number of 
events within the specified period of time. 
Example: The number of patients arriving at a health care clinic in a hour can be mod-
eled with a Poisson random variable with m 5 5, if on average 5 customers arrive 
to the store in a hour.
Integer Uniform Distribution 
Parameters: lower (l), upper (u)
Possible values: l, l 1 1, l 1 2, . . . , u 2 2, u 2 1, u
Description: An integer uniform random variable assumes that the integer values 
 between l and u are equally likely. 
Example: The number of philanthropy volunteers from a class of 10 students may be 
an integer uniform variable with values 0, 1, 2, …, 10. 
Discrete Uniform Distribution 
Parameters: set of values {v1, v2, v3, . . . , vk}
Possible values: v1, v2, v3, . . . , vk
Description: A discrete uniform random variable is equally likely to be any of the 
specified set of values {v1, v2, v3, . . . , vk}. 
Example: Consider a game show that awards a contestant a cash prize from an envelope 
randomly selected from six possible envelopes. If the envelopes contain $1, $5, $10, 
$20, $50, and $100, respectively, then the prize is a discrete uniform random variable 
with values {1, 5, 10, 20, 50, 100}. 
Custom Discrete Distribution 
Parameters: set of values {v1, v2, v3, . . . , vk} and corresponding weights {w1, w2, 
w3, . . . , wk}
Possible values: v1, v2, v3, . . . , vk
Description: A custom discrete distribution can be used to create a tailored distribu-
tion to model a discrete, uncertain quantity. The value of a custom discrete random 
variable is equal to the value vi with probability wiy^k
j 5 1 wj. 
Example: The number of qualified proposals submitted for consideration of venture 
capital funding may be described by a custom discrete distribution with values of 
{2, 3, 4, 6} with respective weights of {0.2, 0.1, 0.35, 0.35}. 

Decision Analysis
CONTENTS
12.1 PROBLEM FORMULATION
Payoff Tables
Decision Trees
12.2 DECISION ANALYSIS 
 WITHOUT PROBABILITIES
Optimistic Approach
Conservative Approach
Minimax Regret Approach
12.3 DECISION ANALYSIS 
WITH PROBABILITIES
Expected Value Approach
Risk Analysis
Sensitivity Analysis
12.4  DECISION ANALYSIS WITH 
SAMPLE INFORMATION
Expected Value of Sample 
Information
Expected Value of Perfect 
Information
12.5 COMPUTINg BRANCH 
 PROBABILITIES WITH 
BAYES’ THEOREM
12.6 UTILITY THEORY
Utility and Decision Analysis
Utility Functions
Exponential Utility Function
APPENDIX:  USINg ANALYTIC 
SOLVER  PLATFORM 
TO CREATE 
 DECISION TREES
ChApter 12

 
Analytics in Action 
551
Ultimately, business analytics is about making better decisions. The tools and techniques we 
have introduced previously are designed to aid a decision maker in analyzing existing data, 
predicting future behavior, and recommending decisions. This chapter introduces a field 
known as decision analysis that can be used to develop an optimal strategy when a decision 
maker is faced with several decision alternatives and an uncertain or risk-filled pattern of 
future events. For example, by evaluating the different naming options and  understanding 
the potential sources of uncertainty, Procter & gamble used decision analysis techniques 
to help choose the best brand name when they introduced Crest White Strips.
Decision analysis techniques are used widely in many different settings. The Analyt-
ics in Action, Phytopharm’s New Product Research and Development, discusses the use 
of decision analysis to manage Phytopharm’s pipeline of pharmaceutical products, which 
have long development times and relatively high levels of uncertainty. Federal agencies in 
the United States have used decision analysis to evaluate the potential risks from terrorist 
attacks and to recommend counterterrorism strategies. The State of North Carolina used 
decision analysis in evaluating whether to implement a medical screening test to detect 
metabolic disorders in newborns. 
Even when a careful decision analysis has been conducted, the uncertain future events 
mean that the final outcome is not completely under the control of the decision maker. In 
some cases, the selected decision alternative may provide good or excellent results. In other 
cases, a relatively unlikely future event may occur, causing the selected decision alternative 
to provide only fair or even poor results. The risk associated with any decision alternative 
is a direct result of the uncertainty associated with the final outcome. A good decision 
analysis includes careful consideration of risk. Through risk analysis, the decision maker 
is provided with probability information about the favorable as well as the unfavorable 
outcomes that may occur.
As a pharmaceutical development and functional food 
company, Phytopharm’s primary revenue streams come 
from licensing agreements with larger companies.  After 
Phytopharm establishes proof of principle for a new 
product by successfully completing early clinical tri-
als, it seeks to reduce its risk by licensing the product 
to a large pharmaceutical or nutrition company that will 
 further develop and market it. 
There is substantial uncertainty regarding the future 
sales potential of early stage products; only one in ten 
of such products makes it to market, and only 30 per-
cent of these yield a healthy return. Phytopharm and its 
licensing partners would often initially propose very 
different terms of the licensing agreement. Therefore, 
Phytopharm employed a team of researchers to develop 
a flexible methodology for appraising a product’s poten-
tial and subsequently supporting the negotiation of the 
lump-sum payments for development milestones and 
royalties on eventual sales that comprise the licensing 
agreement.
Using computer simulation, the resulting decision 
analysis model allows Phytopharm to perform sensitiv-
ity analysis on estimates of development cost, the prob-
ability of successful Food and Drug Administration 
clearance, launch date, market size, market share, and 
patent expiry. In particular, a decision tree model allows 
Phytopharm and its licensing partner to mutually agree 
upon the number of development milestones. Depend-
ing on the status of the project at a milestone, the licens-
ing partner can opt to abandon the project or continue 
development. Laying out these sequential decisions in a 
decision tree allows Phytopharm to negotiate milestone 
payments and royalties that equitably split the project’s 
value between Phytopharm and its potential licensee.
PhytoPharm’s New Product research aNd develoPmeNt*
ANALYTICS  in  Action
*Based on P. Crama, B. De Ryck, Z. Degraeve, and W. Chong, “Research 
and Development Project Valuation and Licensing Negotiations at Phyto-
pharm plc,” Interfaces, 37 no. 5 (September–October 2007): 472–487. 

552 
Chapter 12 Decision Analysis
We begin the study of decision analysis by considering problems that involve reason-
ably few decision alternatives and reasonably few possible future events. Payoff tables and 
decision trees are introduced to provide a structure for the decision problem and to illustrate 
the fundamentals of decision analysis. Decision trees are used to analyze more complex 
problems and to identify an optimal sequence of decisions, referred to as an optimal deci-
sion strategy. Sensitivity analysis shows how changes in various aspects of the problem 
affect the recommended decision alternative. We discuss the use of Bayes’ Theorem for 
calculating the probabilities of future events and incorporating additional information about 
the decisions. We conclude this chapter with a discussion of utility and decision analysis 
that expands on different attitudes toward risk taken by decision makers.
problem Formulation
The first step in the decision analysis process is problem formulation. We begin with a 
verbal statement of the problem. We then identify the decision alternatives; the uncertain 
future events, referred to as chance events; and the outcomes associated with each com-
bination of decision alternative and chance event outcome. Let us begin by considering a 
construction project of the Pittsburgh Development Corporation.
Pittsburgh Development Corporation (PDC) purchased land that will be the site of a 
new luxury condominium complex. The location provides a spectacular view of downtown 
Pittsburgh and the golden Triangle, where the Allegheny and Monongahela Rivers meet 
to form the Ohio River. PDC plans to price the individual condominium units between 
$300,000 and $1,400,000.
PDC commissioned preliminary architectural drawings for three different projects: one 
with 30 condominiums, one with 60 condominiums, and one with 90 condominiums. The 
financial success of the project depends on the size of the condominium complex and the 
chance event concerning the demand for the condominiums. The statement of the PDC 
decision problem is to select the size of the new luxury condominium project that will lead 
to the largest profit given the uncertainty concerning the demand for the condominiums.
given the statement of the problem, it is clear that the decision is to select the best size 
for the condominium complex. PDC has the following three decision alternatives:
 
d1 5 a small complex with 30 condominiums
 
d2 5 a medium complex with 60 condominiums
 
d3 5 a large complex with 90 condominiums
A factor in selecting the best decision alternative is the uncertainty associated with the 
chance event concerning the demand for the condominiums. When asked about the possible 
demand for the condominiums, PDC’s president acknowledged a wide range of possibili-
ties but decided that it would be adequate to consider two possible chance event outcomes: 
a strong demand and a weak demand.
In decision analysis, the possible outcomes for a chance event are referred to as the 
states of nature. The states of nature are defined so that they are mutually exclusive (no 
more than one can occur) and collectively exhaustive (at least one must occur); thus one 
and only one of the possible states of nature will occur. For the PDC problem, the chance 
event concerning the demand for the condominiums has two states of nature:
 
s1 5 strong demand for the condominiums
 
s2 5 weak demand for the condominiums
Management must first select a decision alternative (complex size); then a state of nature 
follows (demand for the condominiums), and finally an outcome will occur. In this case, 
the outcome is PDC’s profit.
12.1

 
12.1 Problem Formulation 
553
Payoff Tables
given the three decision alternatives and the two states of nature, which complex size 
should PDC choose? To answer this question, PDC will need to know the outcome associ-
ated with each possible combination of decision alternative and state of nature. In decision 
analysis, we refer to the outcome resulting from a specific combination of a decision alter-
native and a state of nature as a payoff. A table showing payoffs for all combinations of 
decision alternatives and states of nature is a payoff table.
Because PDC wants to select the complex size that provides the largest profit, profit 
is used as the outcome. The payoff table with profits (in millions of dollars) is shown in 
Table 12.1. Note, for example, that if a medium complex is built and demand turns out to 
be strong, a profit of $14 million will be realized. We will use the notation vij to denote the 
payoff associated with decision alternative i and state of nature j. Using Table 12.1, v31 5 
20 indicates that a payoff of $20 million occurs if the decision is to build a large complex 
(d3) and the strong demand state of nature (s1) occurs. Similarly, v32 5 29 indicates a loss 
of $9 million if the decision is to build a large complex (d3) and the weak demand state of 
nature (s2) occurs.
Decision Trees
A decision tree provides a graphical representation of the decision-making process. Fig-
ure 12.1 presents a decision tree for the PDC problem. Note that the decision tree shows the 
natural or logical progression that will occur over time. First, PDC must make a decision 
regarding the size of the condominium complex (d1, d2, or d3). Then, after the decision is 
implemented, either state of nature s1 or s2 will occur. The number at each endpoint of the 
tree indicates the payoff associated with a particular sequence. For example, the topmost 
payoff of 8 indicates that an $8 million profit is anticipated if PDC constructs a small 
condominium complex (d1) and demand turns out to be strong (s1). The next payoff of 7 
indicates an anticipated profit of $7 million if PDC constructs a small condominium com-
plex (d1) and demand turns out to be weak (s2). Thus, the decision tree provides a graphical 
depiction of the sequences of decision alternatives and states of nature that provide the six 
possible payoffs for PDC.
The decision tree in Figure 12.1 shows four nodes, numbered 1–4. Nodes are used to 
represent decisions and chance events. Squares are used to depict decision nodes, circles 
are used to depict chance nodes. Thus, node 1 is a decision node, and nodes 2, 3, and 4 are 
chance nodes. The branches connect the nodes; those leaving the decision node correspond 
to the decision alternatives. The branches leaving each chance node correspond to the states 
of nature. The outcomes (payoffs) are shown at the end of the states-of-nature branches. 
We now turn to the question: How can the decision maker use the information in the payoff 
table or the decision tree to select the best decision alternative? Several  approaches may be 
used and are covered in the remaining sections of this chapter.
Payoffs can be expressed 
in terms of profit, cost, 
time, distance, or any other 
measure appropriate for 
the decision problem being 
analyzed.
TABLE 12.1   PAYOFF TABLE FOR THE PDC CONDOMINIUM PROJECT 
($  MILLIONS)
State of Nature
Decision Alternative
Strong Demand, s1
Weak Demand, s2
Small Complex, d1
Medium Complex, d2
Large Complex, d3
 8
14
20
  7
  5
29

554 
Chapter 12 Decision Analysis
Decision Analysis Without probabilities
In this section we consider approaches to decision analysis that do not require knowledge 
of the probabilities of the states of nature. These approaches are appropriate in situations in 
which a simple best-case and worst-case analysis is sufficient and where the decision maker 
has little confidence in his or her ability to assess the probabilities. Because different ap-
proaches sometimes lead to different decision recommendations, the decision maker must 
understand the approaches available and then select the specific approach that, according 
to the judgment of the decision maker, is the most appropriate.
Optimistic Approach
The optimistic approach evaluates each decision alternative in terms of the best payoff 
that can occur. The decision alternative that is recommended is the one that provides the 
best possible payoff. For a problem in which maximum profit is desired, as in the PDC 
problem, the optimistic approach would lead the decision maker to choose the alternative 
corresponding to the largest profit. For problems involving minimization, this approach 
leads to choosing the alternative with the smallest payoff.
12.2
For a maximization 
problem, the optimistic ap-
proach often is referred to 
as the maximax approach; 
for a minimization problem, 
the corresponding termi-
nology is minimin.
NOTES AND COMMENTS
1. The first step in solving a complex problem is to 
decompose the problem into a series of smaller 
subproblems. Decision trees provide a useful 
way to decompose a problem and illustrate the 
sequential nature of the decision process.
2. People often view the same problem from dif-
ferent perspectives. Thus, the discussion re-
garding the development of a decision tree may 
provide additional insight into the problem.
FigurE 12.1   DECISION TREE FOR THE PDC CONDOMINIUM PROJECT 
($  MILLIONS)
Weak (s2)
Strong (s1)
Weak (s2)
Strong (s1)
Weak (s2)
Strong (s1)
1
2
3
4
8
7
14
5
20
–9
Small (d1)
Medium (d2)
Large (d3)

 
12.2 Decision Analysis Without Probabilities 
555
To illustrate the optimistic approach, we use it to develop a recommendation for the 
PDC problem. First, we determine the maximum payoff for each decision alternative; then 
we select the decision alternative that provides the overall maximum payoff. These steps 
systematically identify the decision alternative that provides the largest possible profit. 
Table 12.2 illustrates these steps.
Because 20, corresponding to d3, is the largest payoff, the decision to construct the 
large condominium complex is the recommended decision alternative using the optimistic 
approach.
Conservative Approach
The conservative approach evaluates each decision alternative in terms of the worst pay-
off that can occur. The decision alternative recommended is the one that provides the best 
of the worst possible payoffs. For a problem in which the output measure is profit, as in 
the PDC problem, the conservative approach would lead the decision maker to choose 
the  alternative that maximizes the minimum possible profit that could be obtained. For 
problems involving minimization (for example, when the output measure is cost instead 
of profit), this approach identifies the alternative that will minimize the maximum payoff.
To illustrate the conservative approach, we use it to develop a recommendation for the 
PDC problem. First, we identify the minimum payoff for each of the decision alternatives; 
then we select the decision alternative that maximizes the minimum payoff. Table 12.3 
 illustrates these steps for the PDC problem.
Because 7, corresponding to d1, yields the maximum of the minimum payoffs, the deci-
sion alternative of a small condominium complex is recommended. This decision  approach 
is considered conservative because it identifies the worst possible payoffs and then recom-
mends the decision alternative that avoids the possibility of extremely “bad” payoffs. In 
the conservative approach, PDC is guaranteed a profit of at least $7 million. Although PDC 
may make more, it cannot make less than $7 million.
Minimax Regret Approach
In decision analysis, regret is the difference between the payoff associated with a particu-
lar decision alternative and the payoff associated with the decision that would yield the 
most desirable payoff for a given state of nature. Thus, regret represents how much potential 
For a maximization 
problem, the conservative 
approach often is referred 
to as the maximin approach; 
for a minimization problem 
the corresponding termi-
nology is minimax.
TABLE 12.2  MAXIMUM PAYOFF FOR EACH PDC DECISION ALTERNATIVE
Decision Alternative
Maximum Payoff
Small Complex, d1
Medium Complex, d2
Large Complex, d3
 8
14
20
Maximum of the maximum payoff values
TABLE 12.3  MINIMUM PAYOFF FOR EACH PDC DECISION ALTERNATIVE
Decision Alternative
Minimum Payoff 
($ Millions)
Small Complex, d1
Medium Complex, d2
Large Complex, d3
  7
  5
29
Maximum of the minimum payoff values

556 
Chapter 12 Decision Analysis
payoff one would forgo by selecting a particular decision alternative, given that a specific 
state of nature will occur. This is why regret is often referred to as opportunity loss.
As its name implies, under the minimax regret approach to decision analysis, one 
would choose the decision alternative that minimizes the maximum state of regret that could 
occur over all possible states of nature. This approach is neither purely optimistic nor purely 
conservative. Let us illustrate the minimax regret approach by showing how it can be used 
to select a decision alternative for the PDC problem.
Suppose that PDC constructs a small condominium complex (d1) and demand turns out 
to be strong (s1). Table 12.1 showed that the resulting profit for PDC would be $8 million. 
However, given that the strong demand state of nature (s1) has occurred, we realize that the 
 decision to construct a large condominium complex (d3), yielding a profit of $20 million, 
would have been the best decision. The difference between the payoff for the best decision 
alternative ($20 million) and the payoff for the decision to construct a small condominium 
complex ($8 million) is the regret or opportunity loss associated with decision alternative d1 
when state of nature s1 occurs; thus, for this case, the opportunity loss or regret is $20 million 2 
$8 million 5 $12 million. Similarly, if PDC makes the decision to construct a medium condo-
minium complex (d2) and the strong demand state of nature (s1) occurs, the opportunity loss, or 
regret, associated with d2 would be $20 million 2 $14 million 5 $6 million. Of course if PDC 
chooses to construct a large complex (d3) and demand is strong, they would have no regret.
In general, the following expression represents the opportunity loss, or regret:
REgRET (OPPORTUNITY LOSS)
 
rij 5 0 vj
  * 2 vij 0  
(12.1)
where
  rij 5 the regret associated with decision alternative di and state of nature sj
  vj
  * 5 the payoff value corresponding to the best decision for the state of nature sj
  vij 5 the payoff corresponding to decision alternative di and state of nature sj
Note the role of the absolute value in equation (12.1). For minimization problems, the 
best payoff, vj
*, is the smallest entry in column j. Because this value always is less than or 
equal to vij, the absolute value of the difference between vj
* and vij ensures that the regret 
is always the magnitude of the difference.
Using equation (12.1) and the payoffs in Table 12.1, we can compute the regret asso-
ciated with each combination of decision alternative di and state of nature sj. Because the 
PDC problem is a maximization problem, vj
* will be the largest entry in column j of the 
payoff table. Thus, to compute the regret, we simply subtract each entry in a column from 
the largest entry in the column. Table 12.4 shows the opportunity loss, or regret, table for 
the PDC problem.
TABLE 12.4   OPPORTUNITY LOSS, OR REgRET, TABLE FOR THE PDC 
 CONDOMINIUM PROJECT ($ MILLIONS)
State of Nature
Decision Alternative
Strong Demand s1
Weak Demand s2
Small Complex, d1
Medium Complex, d2
Large Complex, d3
12
 6
 0
 0
 2
16

 
12.3 Decision Analysis with Probabilities 
557
The next step in applying the minimax regret approach is to list the maximum regret 
for each decision alternative; Table 12.5 shows the results for the PDC problem. Selecting 
the decision alternative with the minimum of the maximum regret values—hence, the name 
minimax regret—yields the minimax regret decision. For the PDC problem, the alternative 
to construct the medium condominium complex, with a corresponding maximum regret of 
$6 million, is the recommended minimax regret decision.
Note that the three approaches discussed in this section provide different recommen-
dations, which in itself is not bad. It simply reflects the difference in decision-making 
philosophies that underlie the various approaches. Ultimately, the decision maker will 
have to choose the most appropriate approach and then make the final decision accord-
ingly. The main criticism of the approaches discussed in this section is that they do not 
consider any information about the probabilities of the various states of nature. In the 
next section, we discuss an approach that utilizes probability information in selecting a 
decision alternative.
Decision Analysis with probabilities
Expected Value Approach
In many decision-making situations, we can obtain probability assessments for the states 
of nature. When such probabilities are available, we can use the expected value approach 
to identify the best decision alternative. Let us first define the expected value of a decision 
alternative and then apply it to the PDC problem.
Let
 
 N 5 the number of states of nature
 
P(sj) 5 the probability of state of nature sj
Because one and only one of the N states of nature can occur, the probabilities must satisfy 
two conditions:
 
P(sj) $ 0    for all states of nature
 
a
N
j51
P(sj) 5 P(s1) 1 P(s2) 1 c1 P(sN) 5 1
The expected value (EV) of decision alternative di is defined as follows:
EXPECTED VALUE OF DECISION ALTERNATIVE di
 
EV(di) 5 a
N
j51
P(sj)vij 
(12.2)
12.3
TABLE 12.5  MAXIMUM REgRET FOR EACH PDC DECISION ALTERNATIVE
Decision Alternative
Maximum Regret  
($ millions)
Small Complex, d1
Medium Complex, d2
Large Complex, d3
12
 6
16
Minimum of the maximum regret

558 
Chapter 12 Decision Analysis
In words, the expected value of a decision alternative is the sum of weighted payoffs for 
the decision alternative. The weight for a payoff is the probability of the associated state 
of nature and therefore the probability that the payoff will occur. Let us return to the PDC 
problem to see how the expected value approach can be applied.
PDC is optimistic about the potential for the luxury high-rise condominium complex. 
Suppose that this optimism leads to an initial subjective probability assessment of 0.8 that 
demand will be strong (s1) and a corresponding probability of 0.2 that demand will be weak 
(s2). Thus, P(s1) 5 0.8 and P(s2) 5 0.2. Using the payoff values in Table 12.1 and equation 
(12.2), we compute the expected value for each of the three decision alternatives as follows:
 
EV(d1) 5 0.8(8) 1 0.2(7) 5 7.8
 
EV(d2) 5 0.8(14) 1 0.2(5) 5 12.2
 
EV(d3) 5 0.8(20) 1 0.2(29) 5 14.2
Thus, using the expected value approach, we find that the large condominium complex, 
with an expected value of $14.2 million, is the recommended decision.
The calculations required to identify the decision alternative with the best expected 
value can be conveniently carried out on a decision tree. Figure 12.2 shows the decision tree 
for the PDC problem with state-of-nature branch probabilities. Working backward through 
the decision tree, we first compute the expected value at each chance node. In other words, 
at each chance node, we weight each possible payoff by its probability of occurrence. By 
doing so, we obtain the expected values for nodes 2, 3, and 4, as shown in Figure 12.3.
Because the decision maker controls the branch leaving decision node 1 and because 
we are trying to maximize the expected profit, the best decision alternative at node 1 is d3. 
Thus, the decision tree analysis leads to a recommendation of d3, with an expected value 
of $14.2 million. Note that this recommendation is also obtained with the expected value 
approach in conjunction with the payoff table.
Other decision problems may be substantially more complex than the PDC problem, 
but if a reasonable number of decision alternatives and states of nature are present, you can 
Because expected values 
are often applied to 
monetary units (profits or 
losses), the expected value 
approach is often referred 
to as the expected monetary 
value (emv) approach.
computer packages 
are available to help in 
constructing more complex 
decision trees. In the chap-
ter appendix, we discuss 
the use of analytic solver 
Platform to create decision 
trees.
FigurE 12.2   PDC DECISION TREE WITH STATE-OF-NATURE BRANCH 
 PROBABILITIES
8
7
14
5
20
–9
Weak (s2)
Strong (s1)
Weak (s2)
Strong (s1)
Weak (s2)
Strong (s1)
Small (d1)
Medium (d2 )
Large (d3)
1
2
3
4
P(s1) = 0.8
P(s2) = 0.2
P(s1) = 0.8
P(s2) = 0.2
P(s1) = 0.8
P(s2) = 0.2

 
12.3 Decision Analysis with Probabilities 
559
use the decision tree approach outlined here. First, draw a decision tree consisting of deci-
sion nodes, chance nodes, and branches that describe the sequential nature of the problem. 
If you use the expected value approach, the next step is to determine the probabilities for 
each of the states of nature and compute the expected value at each chance node. Then select 
the decision branch leading to the chance node with the best expected value. The decision 
alternative associated with this branch is the recommended decision.
In practice, obtaining precise estimates of the probabilities for each state of nature is 
often impossible. In some cases where similar decisions have been made many times in 
the past, one may use historical data to estimate the probabilities for the different states of 
nature. However, often there are little, or no, historical data to guide the estimates of these 
probabilities. In these cases, we may have to rely on subjective estimates to determine the 
probabilities for the states of nature. When relying on subjective estimates, we often want 
to get more than one estimate because many studies have shown that even knowledgeable 
experts are often overly optimistic in their estimates. It is also particularly important when 
dealing with subjective probability estimates to perform risk analysis and sensitivity analy-
sis as we will explain.
Risk Analysis
Risk analysis helps the decision maker recognize the difference between the expected 
value of a decision alternative and the payoff that may actually occur. A decision alterna-
tive and a state of nature combine to generate the payoff associated with a decision. The 
risk profile for a decision alternative shows the possible payoffs along with their associated 
probabilities.
Let us demonstrate risk analysis and the construction of a risk profile by returning to the 
PDC condominium construction project. Using the expected value approach, we identified 
the large condominium complex (d3) as the best decision alternative. The expected value of 
$14.2 million for d3 is based on a 0.8 probability of obtaining a $20 million profit and a 0.2 
probability of obtaining a $9 million loss. The 0.8 probability for the $20 million payoff and 
the 0.2 probability for the 2$9 million payoff provide the risk profile for the large complex 
decision alternative. This risk profile is shown graphically in Figure 12.4.
Sometimes a review of the risk profile associated with an optimal decision alterna-
tive may cause the decision maker to choose another decision alternative even though the 
FigurE 12.3   APPLYINg THE EXPECTED VALUE APPROACH USINg A 
 DECISION TREE FOR THE PDC CONDOMINIUM PROJECT
Small (d1)
Medium (d2)
Large (d3)
1
2
3
4
EV(d1) = 0.8(8) + 0.2(7) = $7.8
EV(d2) = 0.8(14) + 0.2(5) = $12.2
EV(d3) = 0.8(20) + 0.2(–9) = $14.2

560 
Chapter 12 Decision Analysis
expected value of the other decision alternative is not as good. For example, the risk profile 
for the medium complex decision alternative (d2) shows a 0.8 probability for a $14 million 
payoff and a 0.2 probability for a $5 million payoff. Because no probability of a loss is 
associated with decision alternative d2, the medium complex decision alternative would be 
judged less risky than the large complex decision alternative. As a result, a decision maker 
might prefer the less risky medium complex decision alternative even though it has an 
 expected value of $2 million less than the large complex decision alternative.
Sensitivity Analysis
Sensitivity analysis can be used to determine how changes in the probabilities for the 
states of nature or changes in the payoffs affect the recommended decision alternative. In 
many cases, the probabilities for the states of nature and the payoffs are based on subjective 
 assessments. Sensitivity analysis helps the decision maker understand which of these inputs 
are critical to the choice of the best decision alternative. If a small change in the value of 
one of the inputs causes a change in the recommended decision alternative, the solution 
to the decision analysis problem is sensitive to that particular input. Extra effort and care 
should be taken to make sure the input value is as accurate as possible. On the other hand, 
if a modest-to-large change in the value of one of the inputs does not cause a change in 
the recommended decision alternative, the solution to the decision analysis problem is 
not sensitive to that particular input. No extra time or effort would be needed to refine the 
estimated input value.
One approach to sensitivity analysis is to select different values for the probabilities of 
the states of nature and the payoffs and then resolve the decision analysis problem. If the 
recommended decision alternative changes, we know that the solution is sensitive to the 
changes made. For example, suppose that in the PDC problem the probability for a strong 
demand is revised to 0.2 and the probability for a weak demand is revised to 0.8. Would the 
recommended decision alternative change? Using P(s1) 5 0.2, P(s2) 5 0.8, and equation 
(12.2), the revised expected values for the three decision alternatives are
 
EV(d1) 5 0.2(8) 1 0.8(7) 5 7.2
 
EV(d2) 5 0.2(14) 1 0.8(5) 5 6.8
 
EV(d3) 5 0.2(20) 1 0.8(29) 5 23.2
the drawback to the sen-
sitivity analysis approach 
given here is the numerous 
calculations required to 
evaluate the effect of sev-
eral possible changes in the 
state-of-nature probabili-
ties and/or payoff values. 
In the chapter appendix 
we demonstrate how to use 
analytic solver Platform 
and a data table in excel to 
easily generate sensitiv-
ity analysis for decision 
problems.
FigurE 12.4   RISk PROFILE FOR THE LARgE COMPLEX DECISION 
 ALTERNATIVE FOR THE PDC CONDOMINIUM PROJECT
1.0
.8
.6
.4
.2
–10
0
10
20
Proﬁt ($ millions)
Probability

 
12.4 Decision Analysis with Sample Information 
561
With these probability assessments, the recommended decision alternative is to construct a 
small condominium complex (d1), with an expected value of $7.2 million. The probability 
of strong demand is only 0.2, so constructing the large condominium complex (d3) is the 
least preferred alternative, with an expected value of 2$3.2 million (a loss).
Thus, when the probability of strong demand is large, PDC should build the large com-
plex; when the probability of strong demand is small, PDC should build the small complex. 
Obviously, we could continue to modify the probabilities of the states of nature and learn 
even more about how changes in the probabilities affect the recommended decision alterna-
tive. Sensitivity analysis calculations can also be made for the values of the payoffs. We can 
easily change the payoff values and resolve the problem to see if the best decision changes. 
Decision Analysis with Sample Information
Frequently, decision makers have the ability to collect additional information about the 
states of nature. It is worthwhile for the decision maker to consider the potential value 
of this additional information and how it can affect the decision analysis process. Most 
often, additional information is obtained through experiments designed to provide sample 
 information about the states of nature. Raw material sampling, product testing, and market 
research studies are examples of experiments (or studies) that may enable management to 
revise or update the state-of-nature probabilities. 
To analyze the potential benefit of additional information, we must first introduce a few 
additional terms related to decision analysis. The preliminary or prior probability assess-
ments for the states of nature that are the best probability values available prior to obtaining 
additional information. These revised probabilities after obtaining additional information 
are called posterior probabilities.
Let us return to the PDC problem and assume that management is considering a 6-month 
market research study designed to learn more about potential market acceptance of the PDC 
condominium project. Management anticipates that the market research study will provide 
one of the following two results:
 1. Favorable report: A substantial number of the individuals contacted express interest 
in purchasing a PDC condominium.
 2. unfavorable report: Very few of the individuals contacted express interest in pur-
chasing a PDC condominium.
The decision tree for the PDC problem with sample information shows the logical 
sequence for the decisions and the chance events in Figure 12.5. By introducing the pos-
sibility of conducting a market research study, the PDC problem becomes more complex. 
First, PDC’s management must decide whether the market research should be conducted. 
If it is conducted, PDC’s management must be prepared to make a decision about the size 
of the condominium project if the market research report is favorable and, possibly, a dif-
ferent decision about the size of the condominium project if the market research report is 
unfavorable. In Figure 12.5, the squares are decision nodes and the circles are chance nodes. 
At each decision node, the branch of the tree that is taken is based on the decision made. At 
each chance node, the branch of the tree that is taken is based on probability or chance. For 
example, decision node 1 shows that PDC must first make the decision of whether to con-
duct the market research study. If the market research study is undertaken, chance node 2 
indicates that both the favorable report branch and the unfavorable report branch are not 
under PDC’s control and will be determined by chance. Node 3 is a decision node, indicat-
ing that PDC must make the decision to construct the small, medium, or large complex if 
the market research report is favorable. Node 4 is a decision node showing that PDC must 
make the decision to construct the small, medium, or large complex if the market research 
12.4

562 
Chapter 12 Decision Analysis
report is unfavorable. Node 5 is a decision node indicating that PDC must make the decision 
to construct the small, medium, or large complex if the market research is not undertaken. 
Nodes 6 to 14 are chance nodes indicating that the strong demand or weak demand state-
of-nature branches will be determined by chance.
Analysis of the decision tree and the choice of an optimal strategy require that we know 
the branch probabilities corresponding to all chance nodes. PDC has developed the follow-
ing branch probabilities:
FigurE 12.5   THE PDC DECISION TREE INCLUDINg THE MARkET RESEARCH 
STUDY
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Unfavorable
Report
Market Research 
Study
No Market Research 
Study
Favorable
Report
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Small (d1)
Small (d1)
Small (d1)
Large (d3)
Large (d3)
Large (d3)
8
7
14
5
20
29
8
7
14
5
20
29
8
7
14
5
20
29
Medium (d2)
Medium (d2)
Medium (d2)

 
12.4 Decision Analysis with Sample Information 
563
If the market research study is undertaken:
 
 P(favorable report) 5 0.77
 
P(unfavorable report) 5 0.23
If the market research report is favorable, the posterior probabilities are:
 
P(strong demand given a favorable report) 5 0.94
 
 P(weak demand given a favorable report) 5 0.06
If the market research report is unfavorable, the posterior probabilities are:
 
P(strong demand given an unfavorable report) 5 0.35
 
 P(weak demand given an unfavorable report) 5 0.65
If the market research report is not undertaken, the prior probabilities are applicable:
 
P(strong demand) 5 0.80
 
 P(weak demand) 5 0.20
The branch probabilities are shown on the decision tree in Figure 12.6.
A decision strategy is a sequence of decisions and chance outcomes where the deci-
sions chosen depend on the yet to be determined outcomes of chance events. The approach 
used to determine the optimal decision strategy is based on a rollback of the expected values 
in the decision tree using the following steps:
 1. At chance nodes, compute the expected value by multiplying the payoff at the end of 
each branch by the corresponding branch probabilities.
 2. At decision nodes, select the decision branch that leads to the best expected value. 
This expected value becomes the expected value at the decision node.
Starting the rollback calculations by computing the expected values at chance nodes 
6–14 provides the following results:
EV(Node 6)
EV(Node 7)
EV(Node 8)
EV(Node 9)
EV(Node 10)
EV(Node 11)
EV(Node 12)
EV(Node 13)
EV(Node 14)
5
5
5
5
5
5
5
5
5
0.94(8) 1 0.06(7)
0.94(14) 1 0.06(5)
0.94(20) 1 0.06(29)
0.35(8) 1 0.65(7)
0.35(14) 1 0.65(5)
0.35(20) 1 0.65(29)
0.80(8) 1 0.20(7)
0.80(14) 1 0.20(5)
0.80(20) 1 0.20(29)
5
5
5
5
5
5
5
5
5
 7.94
13.46
18.26
 7.35
 8.15
 1.15
 7.80
12.20
14.20
Figure 12.7 shows the reduced decision tree after computing expected values at these 
chance nodes.
Next, move to decision nodes 3, 4, and 5. For each of these nodes, we select the decision 
alternative branch that leads to the best expected value. For example, at node 3 we have the 
choice of the small complex branch with EV(Node 6) 5 7.94, the medium complex branch 
with EV(Node 7) 5 13.46, and the large complex branch with EV(Node 8) 5 18.26. Thus, 
we select the large complex decision alternative branch and the expected value at node 3 
becomes EV(Node 3) 5 18.26.
For node 4, we select the best expected value from nodes 9, 10, and 11. The best deci-
sion alternative is the medium complex branch that provides EV(Node 4) 5 8.15. For node 
5, we select the best expected value from nodes 12, 13, and 14. The best decision alterna-
tive is the large complex branch that provides EV(Node 5) 5 14.20. Figure 12.8 shows the 
we explain in section 12.5 
how the branch probabili-
ties for P(Favorable report) 
and P(unfavorable report) 
can be developed.

564 
Chapter 12 Decision Analysis
reduced decision tree after choosing the best decisions at nodes 3, 4, and 5 and rolling back 
the expected values to these nodes.
The expected value at chance node 2 can now be computed as follows:
 
EV(Node 2) 5 0.77EV(Node 3) 1 0.23EV(Node 4)
 
 5 0.77(18.26) 1 0.23(8.15) 5 15.93
This calculation reduces the decision tree to one involving only the two decision branches 
from node 1 (see Figure 12.9).
FigurE 12.6  THE PDC DECISION TREE WITH BRANCH PROBABILITIES
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Unfavorable
Report 0.23
Market Research
Study
No Market Research 
Study
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Small (d1)
Small (d1)
Small (d1)
Large (d3)
Large (d3)
Large (d3)
8
7
14
5
20
29
8
7
14
5
20
29
8
7
14
5
20
29
Medium (d2)
Medium (d2)
Medium (d2)
0.94
0.94
0.06
0.06
0.94
0.35
0.06
0.65
0.35
0.65
0.35
0.65
0.80
0.20
0.80
0.20
0.20
0.80
Favorable 
Report 0.77

 
12.4 Decision Analysis with Sample Information 
565
FigurE 12.7   PDC DECISION TREE AFTER COMPUTINg EXPECTED VALUES 
AT CHANCE NODES 6–14
Unfavorable
Report 0.23
Market Research
Study
No Market Research 
Study
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Small (d1)
Small (d1)
Small (d1)
Large (d3)
Large (d3)
Large (d3)
Medium (d2)
Medium (d2)
Medium (d2)
EV = 7.94
EV = 13.46
EV = 18.26
EV = 7.35
EV = 8.15
EV = 1.15
EV = 7.80
EV = 12.20
EV = 14.20
Favorable
Report 0.77
FigurE 12.8   PDC DECISION TREE AFTER CHOOSINg BEST DECISIONS AT 
NODES 3, 4, AND 5
Unfavorable
Report 0.23
Market Research 
Study
No Market Research 
Study
1
2
3
4
5
EV(d3) = 18.26
EV(d2 ) = 8.15
EV(d3) = 14.20
Favorable 
Report 0.77

566 
Chapter 12 Decision Analysis
FigurE 12.9  PDC DECISION TREE REDUCED TO TWO DECISION BRANCHES
Market Research
Study
No Market Research 
Study
1
2
5
EV = 15.93
EV = 14.20
Finally, the decision can be made at decision node 1 by selecting the best expected val-
ues from nodes 2 and 5. This action leads to the decision alternative to conduct the market 
research study, which provides an overall expected value of 15.93.
The optimal decision for PDC is to conduct the market research study and then carry 
out the following decision strategy:
 
 If the market research is favorable, construct the large condominium complex.
 
 If the market research is unfavorable, construct the medium condominium complex.
The analysis of the PDC decision tree describes the methods that can be used to ana-
lyze more complex sequential decision problems. First, draw a decision tree consisting of 
decision and chance nodes and branches that describe the sequential nature of the problem. 
Determine the probabilities for all chance outcomes. Then, by working backward through 
the tree, compute expected values at all chance nodes and select the best decision branch 
at all decision nodes. The sequence of optimal decision branches determines the optimal 
decision strategy for the problem.
Expected Value of Sample Information
In the PDC problem, the market research study is the sample information used to determine 
the optimal decision strategy. The expected value associated with the market research study 
is 15.93. Previously, we showed that the best expected value if the market research study 
is not undertaken is 14.20. Thus, we can conclude that the difference, 15.93 2 14.20 5 
1.73, is the expected value of sample information (EVSI). In other words, conducting 
the market research study adds $1.73 million to the PDC expected value. In general, the 
expected value of sample information is as follows:
EXPECTED VALUE OF SAMPLE INFORMATION (EVSI)
 
EVSI 5 0 EVwSI 2 EVwoSI 0  
(12.3)
where
 
 EVSI 5 expected value of sample information
 
 EVwSI 5 expected value with sample information about the states of nature
 EVwoSI 5 expected value without sample information about the states of nature
the evsI 5 $1.73 million 
suggests Pdc should 
be willing to pay up to 
$1.73 million to conduct the 
market research study.

 
12.4 Decision Analysis with Sample Information 
567
Expected Value of Perfect Information
A special case of gaining additional information related to a decision problem is when the 
sample information provides perfect information on the states of nature. In other words, 
consider the case where the marketing study undertaken by PDC would determine exactly 
which state of nature will occur. Clearly, such a result is highly unlikely from a market-
ing study, but such an analysis provides a best case analysis of the benefit provided by 
the marketing study. If the investment required for the additional information exceeds the 
expected value of perfect information, then we would not want to invest in procuring the 
additional information.
To illustrate the calculation of expected value of perfect information, we return to the 
PDC decision. We assume for the moment that PDC could determine with certainty, prior 
to making a decision, which state of nature is going to occur. To make use of this perfect 
information, we will develop a decision strategy that PDC should follow once it knows 
which state of nature will occur. 
To help determine the decision strategy for PDC, we reproduced PDC’s payoff table 
as Table 12.6. Note that, if PDC knew for sure that state of nature s1 would occur, the best 
decision alternative would be d3, with a payoff of $20 million. Similarly, if PDC knew for 
sure that state of nature s2 would occur, the best decision alternative would be d1, with a 
payoff of $7 million. Thus, we can state PDC’s optimal decision strategy when the perfect 
information becomes available as follows:
 
 If s1, select d3 and receive a payoff of $20 million.
 
If s2, select d1 and receive a payoff of $7 million.
What is the expected value for this decision strategy? To compute the expected value 
with perfect information, we return to the original probabilities for the states of nature: 
P(s1) 5 0.8 and P(s2) 5 0.2. Thus, there is a 0.8 probability that the perfect information 
will  indicate state of nature s1, and the resulting decision alternative d3 will provide a 
$20 million profit. Similarly, with a 0.2 probability for state of nature s2, the optimal deci-
sion alternative d1 will provide a $7 million profit. Thus, from equation (12.2) the expected 
value of the decision strategy that uses perfect information is 0.8(20) 1 0.2(7) 5 17.4.
We refer to the expected value of $17.4 million as the expected value with perfect 
information (EVwPI).
Earlier in this section we showed that the recommended decision using the expected 
value approach is decision alternative d3, with an expected value of $14.2 million. Because 
this decision recommendation and expected value computation were made without the 
benefit of perfect information, $14.2 million is referred to as the expected value without 
perfect information (EVwoPI).
The expected value with perfect information is $17.4 million, and the expected 
value without perfect information is $14.2; therefore, the expected value of the perfect 
TABLE 12.6   PAYOFF TABLE FOR THE PDC CONDOMINIUM PROJECT 
($  MILLIONS)
State of Nature
Decision Alternative
Strong Demand s1
Weak Demand s2
Small Complex, d1
Medium Complex, d2
Large Complex, d3
 8
14
20
  7
  5
29
It would be worth $3.2 mil-
lion for Pdc to learn the 
level of market acceptance 
before selecting a decision 
alternative. this repre-
sents the maximum that 
Pdc should invest in any 
market research to provide 
additional information on 
the states of nature because 
no market research study 
can be expected to provide 
perfect information.

568 
Chapter 12 Decision Analysis
information (EVPI) is $17.4 2 $14.2 5 $3.2 million. In other words, $3.2 million repre-
sents the  additional expected value that can be obtained if perfect information were avail-
able about the states of nature.
In general, the expected value of perfect information (EVPI) is computed as follows:
EXPECTED VALUE OF PERFECT INFORMATION (EVPI)
 
EVPI 5 0 EVwPI 2 EVwoPI 0  
(12.4)
where
 
 EVPI 5 expected value of perfect information
 
 EVwPI 5 expected value with perfect information about the states of nature
 EVwoPI 5 expected value without perfect information about the states of nature
Computing Branch probabilities 
with Bayes’ theorem
In Section 12.4 the branch probabilities for the PDC decision tree chance nodes were pro-
vided in the problem description. No computations were required to determine these prob-
abilities. In this section, we show how Bayes’ theorem can be used to compute branch 
probabilities for decision trees.
The PDC decision tree is shown again in Figure 12.10. Let
 
 F 5 favorable market research report
 
u 5 unfavorable market research report
 
 s1 5 strong demand (state of nature 1)
 
 s2 5 Weak demand (state of nature 2)
At chance node 2, we need to know the branch probabilities P(F) and P(u). At chance 
nodes 6, 7, and 8, we need to know the branch probabilities P(s1 0 F), which is read as “the 
probability of state of nature 1 given a favorable market research report,” and P(s2 0 F), 
which is the probability of state of nature 2 given that the market research report was 
favorable. The notation 0  in P(s1 0 F) and P(s2 0 F) is read as “given” and indicates a con-
ditional probability because we are interested in the probability of a particular state 
of nature “conditioned” on the fact that we receive a favorable market report. P(s1 0 F) 
and P(s2 0 F) are referred to as posterior probabilities because they are conditional prob-
abilities based on the outcome of the sample information. At chance nodes 9, 10, and 11, 
we need to know the branch probabilities P(s1 0 u) and P(s2 0 u); note that these are also 
posterior probabilities, denoting the probabilities of the two states of nature given that the 
market research report is unfavorable. Finally, at chance nodes 12, 13, and 14, we need 
the probabilities for the states of nature, P(s1) and P(s2), if the market research study is 
not undertaken.
In performing the probability computations, we need to know PDC’s assessment of the 
probabilities for the two states of nature, P(s1) and P(s2), which are the prior probabilities 
as discussed earlier. In addition, we must know the conditional probability of the market 
research outcomes (the sample information) given each state of nature. For example, we 
need to know the conditional probability of a favorable market research report given that the 
state of nature is strong demand for the PDC project; note that this conditional probability 
12.5

 
12.5 Computing Branch Probabilities with Bayes’ Theorem 
569
of F given state of nature s1 is written P(F 0 s1). To carry out the probability calculations, we 
will need conditional probabilities for all sample outcomes given all states of nature, that is, 
P(F 0 s1), P(F 0 s2), P(u 0 s1), and P(u 0 s2). These conditional probabilities are assessments of 
the accuracy of the market research; they are often estimated using historical performance 
of previous market research reports. For example, P(F 0 s1) may be estimated via the histori-
cal frequency of strong demand being associated with a market research report that was 
FigurE 12.10  THE PDC DECISION TREE
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Strong (s1)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Weak (s2)
Unfavorable
Report P(U)
Market Research
Study
No Market Research 
Study
Favorable
Report P(F)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Small (d1)
Small (d1)
Small (d1)
Large (d3)
Large (d3)
Large (d3)
8
7
14
5
20
29
8
7
14
5
20
29
8
7
14
5
20
29
Medium (d2)
Medium (d2)
Medium (d2)
P(s1| F)
P(s1| F)
P(s2| F)
P(s2| F)
P(s1| F)
P(s1| U)
P(s2| F)
P(s2| U)
P(s1| U)
P(s2| U)
P(s1| U)
P(s2| U)
P(s1)
P(s2)
P(s1)
P(s2)
P(s2)
P(s1)

570 
Chapter 12 Decision Analysis
favorable. In the PDC problem we assume that the following assessments are available for 
these conditional probabilities:
Market Research
State of Nature
Favorable, F
Unfavorable, U
Strong Demand, s1
Weak Demand, s2
P(F 0 s1) 5 0.90
P(F 0 s2) 5 0.25
P(u 0 s1) 5 0.10
P(u 0 s2) 5 0.75
Note that the preceding probability assessments provide a reasonable degree of confi-
dence in the market research study. If the true state of nature is s1, the probability of a favor-
able market research report is 0.90, and the probability of an unfavorable market research 
report is 0.10. If the true state of nature is s2, the probability of a favorable market research 
report is 0.25, and the probability of an unfavorable market research report is 0.75. One rea-
son for a 0.25 probability of a potentially misleading favorable market research report for 
state of nature s2 is that when some potential buyers first hear about the new condominium 
project, their enthusiasm may lead them to overstate their real interest in it. A potential 
buyer’s initial favorable response can change quickly to a “no-thank–you” when later faced 
with the reality of signing a purchase contract and making a down payment.
In the following discussion, we present a tabular approach as a convenient method for 
carrying out the probability computations. The computations for the PDC problem based 
on a favorable market research report (F) are summarized in Table 12.7. The steps used to 
develop this table are as follows:
Step 1.  In column 1, enter the states of nature 
Step 2. In column 2, enter the prior probabilities for the states of nature
Step 3. In column 3, enter the conditional probabilities of a favorable market research 
report (F) given each state of nature
Step 4. In column 4, compute the joint probabilities by multiplying the prior prob-
ability values in column 2 by the corresponding conditional probability values 
in column 3
Step 5. Sum the joint probabilities in column 4 to obtain the probability of a favorable 
market research report, P(F)
Step 6. Divide each joint probability in column 4 by P(F) 5 0.77 to obtain the revised 
or posterior probabilities, P(s1 0 F) and P(s2 0 F)
Table 12.7 shows that the probability of obtaining a favorable market research report 
is P(F) 5 0.77. In addition, P(s1 0 F) 5 0.94 and P(s2 0 F) 5 0.06. In particular, note that a 
favorable market research report will prompt a revised or posterior probability of 0.94 that 
the market demand of the condominium will be strong, s1.
TABLE 12.7   BRANCH PROBABILITIES FOR THE PDC CONDOMINIUM PROJECT BASED 
ON A FAVORABLE MARkET RESEARCH REPORT
States of  
Nature  
sj
Prior  
Probabilities  
P(sj)
Conditional  
Probabilities  
P(F0 sj)
Joint  
Probabilities  
P(F " sj)
Posterior  
Probabilities  
P(sj0 F)
s1
s2
0.80
0.20
1.00
0.90
0.25
0.80 3 0.90 5 0.72
0.20 3 0.25 5 0.05
P(F) 5 0.77
0.72/0.77 5 0.94
0.05/0.77 5 0.06
1.00

 
12.6 Utility Theory 
571
The tabular probability computation procedure must be repeated for each possible sam-
ple information outcome. Table 12.8 shows the computations of the branch probabilities of 
the PDC problem based on an unfavorable market research report. Note that the probability 
of obtaining an unfavorable market research report is P(u) 5 0.23. If an unfavorable report 
is obtained, the posterior probability of a strong market demand, s1, is 0.35 and of a weak 
market demand, s2, is 0.65. The branch probabilities from Tables 12.7 and 12.8 were shown 
on the PDC decision tree in Figure 12.6.
The tabular method can be used directly to compute the branch probabilities in the deci-
sion tree. Alternatively, equation (12.5) provides a general formula for Bayes’ theorem for 
computing posterior probabilities.
BAYES’ THEOREM
 
P(ai 0 B) 5
P(B 0 ai)P(ai)
Sj P(B 0 aj) P(aj)  
(12.5)
To perform the Bayes’ theorem calculations for P(s1 0 u) with equation (12.5), we 
 replace B with u (unfavorable report) and ai with s1 in (12.5) so that we have,
 
 P(s1 0 u) 5
P(u 0 s1)P(s1)
Sj P(u 0 sj) P(sj)
 
 5
0.10 3 0.80
(0.10 3 0.80) 1 (0.20 3 0.75) 5 0.35
which provides the same value as the tabular approach used to generate the values in 
 Table 12.7.
The discussion in this section shows an underlying relationship between the probabili-
ties on the various branches in a decision tree. It would be inappropriate to assume differ-
ent prior probabilities, P(s1) and P(s2) without determining how these changes would alter 
P(F) and P(u), as well as the posterior probabilities P(s1 0 F), P(s2 0 F), P(s1 0 u), and P(s2 0 u).
Utility theory
The decision analysis situations presented so far in this chapter expressed outcomes (payoffs) 
in terms of monetary values. With probability information available about the outcomes of 
the chance events, we defined the optimal decision alternative as the one that provided the 
best expected value. However, in some situations the decision alternative with the best ex-
pected value may not be the preferred alternative. A decision maker may also wish to consider 
intangible factors such as risk, image, or other nonmonetary criteria in order to evaluate the 
12.6
TABLE 12.8   BRANCH PROBABILITIES FOR THE PDC CONDOMINIUM PROJECT BASED 
ON AN UNFAVORABLE MARkET RESEARCH REPORT
States of  
Nature  
sj
Prior  
Probabilities  
P(sj)
Conditional  
Probabilities  
P(U0 sj)
Joint  
Probabilities  
P(U d sj)
Posterior  
Probabilities  
P(sj0 U)
s1
s2
0.80
0.20
1.00
0.10
0.75
0.80 3 0.10 5 0.08
0.20 3 0.75 5 0.15
P(u) 5 0.23
0.08/0.23 5 0.35
0.15/0.23 5 0.65
1.00

572 
Chapter 12 Decision Analysis
decision alternatives. When monetary value does not necessarily lead to the most preferred 
decision, expressing the value (or worth) of a consequence in terms of its utility will permit 
the use of expected utility to identify the most desirable decision alternative. The discussion 
of utility and its application in decision analysis is presented in this section.
Utility is a measure of the total worth or relative desirability of a particular outcome; it 
reflects the decision maker’s attitude toward a collection of factors such as profit, loss, and 
risk. Researchers have found that as long as the monetary value of payoffs stays within a 
range that the decision maker considers reasonable, selecting the decision alternative with 
the best expected value usually leads to selection of the most preferred decision. However, 
when the payoffs are extreme, decision makers are often unsatisfied or uneasy with the 
decision that simply provides the best expected value.
As an example of a situation in which utility can help in selecting the best decision al-
ternative, let us consider the problem faced by Swofford, Inc., a relatively small real estate 
investment firm located in Atlanta, georgia. Swofford currently has two investment oppor-
tunities that require approximately the same cash outlay. The cash requirements necessary 
prohibit Swofford from making more than one investment at this time. Consequently, three 
possible decision alternatives may be considered.
The three decision alternatives, denoted d1, d2, and d3, are
 
d1 5 make investment A
 
d2 5 make investment B
 
d3 5 do not invest
The monetary payoffs associated with the investment opportunities depend on the in-
vestment decision and on the direction of the real estate market during the next six months 
(the chance event). Real estate prices will go up, remain stable, or go down. Thus the states 
of nature, denoted s1, s2, and s3, are
 
s1 5 real estate prices go up
 
s2 5 real estate prices remain stable
 
s3 5 real estate prices go down
Using the best information available, Swofford has estimated the profits, or payoffs, associ-
ated with each decision alternative and state-of-nature combination. The resulting payoff 
table is shown in Table 12.9.
The best estimate of the probability that real estate prices will go up is 0.3; the best 
estimate of the probability that prices will remain stable is 0.5; and the best estimate of the 
probability that prices will go down is 0.2. Thus the expected values for the three decision 
alternatives are
EV(d1)
EV(d2)
EV(d3)
5
5
5
0.3(30,000)
0.3(50,000)
0.3(0)
1
1
1
0.5(20,000)
0.5(220,000)
0.5(0)
1
1
1
0.2(250,000)
0.2(230,000)
0.2(0)
5
5
5
9000
211,000
0
TABLE 12.9  PAYOFF TABLE FOR SWOFFORD, INC.
State of Nature
Decision Alternative
Prices Go Up s1
Prices Stable s2
Prices Go Down s3
Investment A, d1
Investment B, d2
Do Not Invest, d3
$30,000
$50,000
0
$20,000
2$20,000
0
2$50,000
2$30,000
0

 
12.6 Utility Theory 
573
Using the expected value approach, the optimal decision is to select investment A with 
an expected value of $9,000. Is it really the best decision alternative? Let us consider some 
other relevant factors that relate to Swofford’s capability for absorbing the loss of $50,000 
if investment A is made and prices actually go down.
Actually, Swofford’s current financial position is weak. This condition is partly re-
flected in Swofford’s ability to make only one investment. More important, however, the 
firm’s president believes that, if the next investment results in a substantial loss, Swofford’s 
future will be in jeopardy. Although the expected value approach leads to a recommenda-
tion for d1, do you think the firm’s president would prefer this decision? We suspect that 
the president would select d2 or d3 to avoid the possibility of incurring a $50,000 loss. In 
fact, a reasonable conclusion is that, if a loss of even $30,000 could drive Swofford out of 
business, the president would select d3, believing that both investments A and B are too 
risky for Swofford’s current financial position.
The way we resolve Swofford’s dilemma is first to determine Swofford’s utility for 
the various outcomes. Recall that the utility of any outcome is the total worth of that 
outcome, taking into account all risks and consequences involved. If the utilities for the 
various consequences are assessed correctly, the decision alternative with the highest 
 expected utility is the most preferred, or best, alternative. We next show how to determine 
the utility of the outcomes so that the alternative with the highest expected utility can be 
identified.
Utility and Decision Analysis
The procedure we use to establish a utility for each of the payoffs in Swofford’s situation 
requires that we first assign a utility to the best and worst possible payoffs. Any values will 
work as long as the utility assigned to the best payoff is greater than the utility assigned 
to the worst payoff. In this case, $50,000 is the best payoff and 2$50,000 is the worst. 
 Suppose, then, that we arbitrarily make assignments to these two payoffs as follows:
 
Utility of 2$50,000 5 u(250,000) 5 0
 
     Utility of $50,000 5 u(50,000) 5 10
Let us now determine the utility associated with every other payoff.
Consider the process of establishing the utility of a payoff of $30,000. First we ask 
Swofford’s president to state a preference between a guaranteed $30,000 payoff and an op-
portunity to engage in the following lottery, or bet, for some probability of p that we select:
 
 lottery: Swofford obtains a payoff of $50,000 with probability p and a payoff of 
2$50,000 with probability (1 2 p).
Obviously, if p is very close to 1, Swofford’s president would prefer the lottery to the 
guaranteed payoff of $30,000 because the firm would virtually ensure itself a payoff of 
$50,000. If p is very close to 0, Swofford’s president would clearly prefer the guarantee 
of $30,000. In any event, as p increases continuously from 0 to 1, the preference for the 
guaranteed payoff of $30,000 decreases and at some point is equal to the preference for the 
lottery. At this value of p, Swofford’s president would have equal preference for the guar-
anteed payoff of $30,000 and the lottery; at greater values of p, Swofford’s president would 
prefer the lottery to the guaranteed $30,000 payoff. For example, let us assume that when 
p 5 0.95, Swofford’s president is indifferent between the guaranteed payoff of $30,000 and 
the lottery. For this value of p, we can compute the utility of a $30,000 payoff as follows:
 
u(30,000) 5 pu(50,000) 1 (1 2 p)u(250,000)
 
 5 0.95(10) 1  (0.05) (0)
 
 5 9.5
utility values of 0 and 1 
could have been selected 
here; we selected 0 and 10 
to avoid any possible con-
fusion between the utility 
value for a payoff and the 
probability p.
p is often referred to as the 
indifference probability.

574 
Chapter 12 Decision Analysis
Obviously, if we had started with a different assignment of utilities for a payoff of 
$50,000 and 2$50,000, the result would have been a different utility for $30,000. For 
 example, if we had started with an assignment of 100 for $50,000 and 10 for 2$50,000, 
the utility of a $30,000 payoff would be
 
u(30,000) 5 0.95(100) 1 0.05(10)
 
 5 95.0 1 0.5
 
 5 95.5
Hence, we must conclude that the utility assigned to each payoff is not unique but merely 
depends on the initial choice of utilities for the best and worst payoffs.
Before computing the utility for the other payoffs, let us consider the implication of 
Swofford’s president assigning a utility of 9.5 to a payoff of $30,000. Clearly, when p 5 
0.95, the expected value of the lottery is
 
EV(lottery) 5 0.95($50,000) 1 0.05(2$50,000)
 
 5 $47,500 2 $2,500
 
 5 $45,000
Although the expected value of the lottery when p 5 0.95 is $45,000, Swofford’s presi-
dent is indifferent between the lottery (and its associated risk) and a guaranteed payoff of 
$30,000. Thus, Swofford’s president is taking a conservative, or risk-avoiding, viewpoint. 
A decision maker who would choose a guaranteed payoff over a lottery with a superior 
expected payoff is a risk avoider (or is said to be risk averse). The president would rather 
have $30,000 for certain than risk anything greater than a 5 percent chance of incurring a 
loss of $50,000. In other words, the difference between the EV of $45,000 and the guaran-
teed payoff of $30,000 is the risk premium that Swofford’s president would be willing to 
pay to avoid the 5 percent chance of losing $50,000.
To compute the utility associated with a payoff of 2$20,000, we must ask Swofford’s 
president to state a preference between a guaranteed 2$20,000 payoff and an opportunity 
to engage again in the following lottery:
 
 lottery: Swofford obtains a payoff of $50,000 with probability p and a payoff of 
2$50,000 with probability (1 2 p).
Note that this lottery is exactly the same as the one we used to establish the utility of a 
payoff of $30,000 (in fact, we can use this lottery to establish the utility for any value in the 
Swofford payoff table). We need to determine the value of p that would make the president 
indifferent between a guaranteed payoff of 2$20,000 and the lottery. For example, we 
might begin by asking the president to choose between a certain loss of $20,000 and the 
lottery with a payoff of $50,000 with probability p 5 0.90 and a payoff of 2$50,000 with 
probability (1 2 p) 5 0.10. What answer do you think we would get? Surely, with this high 
probability of obtaining a payoff of $50,000, the president would elect the lottery. Next, we 
might ask whether p 5 0.85 would result in indifference between the loss of $20,000 for 
certain and the lottery. Again the president might prefer the lottery. Suppose that we con-
tinue until we get to p 5 0.55, at which point the president is indifferent between the payoff 
of 2$20,000 and the lottery. In other words, for any value of p less than 0.55, the president 
would take a loss of $20,000 for certain rather than risk the potential loss of $50,000 with 
the lottery; and for any value of p above 0.55, the president would choose the lottery. Thus, 
the utility assigned to a payoff of 2$20,000 is
 
u(2$20,000) 5 pu(50,000) 1 (1 2 p)u(2$50,000)
 
 5 0.55(10) 1 0.45(0)
 
 5 5.5
the difference between 
the expected value of the 
lottery and the guaranteed 
payoff can be viewed as the 
risk premium the decision 
maker is willing to pay.

 
12.6 Utility Theory 
575
Again let us assess the implication of this assignment by comparing it to the expected 
value approach. When p 5 0.55, the expected value of the lottery is
 
EV(lottery) 5 0.55($50,000) 1 0.45(2$50,000)
 
 5 $27,500 2 $22,500
 
 5 $5,000
Thus, Swofford’s president would just as soon absorb a certain loss of $20,000 as take 
the lottery and its associated risk, even though the expected value of the lottery is $5,000. 
Once again this preference demonstrates the conservative, or risk-avoiding, point of view 
of Swofford’s president.
In these two examples, we computed the utility for the payoffs of $30,000 and 2$20,000. 
We can determine the utility for any payoff m in a similar fashion. First, we must find the 
probability p for which the decision maker is indifferent between a guaranteed payoff of 
m and a lottery with a payoff of $50,000 with probability p and 2$50,000 with probability 
(1 2 p). The utility of m is then computed as follows:
 
u(m) 5 pu($50,000) 1 (1 2 p)u(2$50,000)
 
 5 p(10) 1 (1 2 p)0
 
 5 10p
Using this procedure we developed a utility for each of the remaining payoffs in Swofford’s 
problem. The results are presented in Table 12.10.
Now that we have determined the utility of each of the possible monetary values, we 
can write the original payoff table in terms of utility. Table 12.11 shows the utility for the 
various outcomes in the Swofford problem. The notation we use for the entries in the util-
ity table is uij, which denotes the utility associated with decision alternative di and state of 
nature sj. Using this notation, we see that u23 5 4.0.
We can now compute the expected utility (EU) of the utilities in Table 12.11 in a 
similar fashion as we computed expected value in Section 12.3. In other words, to identify 
TABLE 12.10   UTILITY OF MONETARY PAYOFFS FOR SWOFFORD, INC.
Monetary Value
Indifference Value of p
Utility
$50,000
30,000
20,000
0
220,000
230,000
250,000
Does not apply
0.95
0.90
0.75
0.55
0.40
Does not apply
10.0
 9.5
 9.0
 7.5
 5.5
 4.0
   0
TABLE 12.11  UTILITY TABLE FOR SWOFFORD, INC.
State of Nature
Decision Alternative
Prices Up s1
Prices Stable s2
Prices Down s3
Investment A, d1
Investment B, d2
Do Not Invest, d3
 9.5
10.0
 7.5
9.0
5.5
7.5
  0
4.0
7.5

576 
Chapter 12 Decision Analysis
an optimal decision alternative for Swofford, Inc., the expected utility approach requires 
the analyst to compute the expected utility for each decision alternative and then select 
the alternative yielding the highest expected utility. With N possible states of nature, the 
expected utility of a decision alternative di is given by
EXPECTED UTILITY (EU)
 
eu(di) 5 a
N
j51
P(sj)uij 
(12.6)
The expected utility for each of the decision alternatives in the Swofford problem is
 
EU(d1) 5 0.3(9.5) 1 0.5(9.0) 1 0.2(0) 5 7.35
 
EU(d2) 5 0.3(10) 1 0.5(5.5) 1 0.2(4.0) 5 6.55
 
EU(d3) 5 0.3(7.5) 1 0.5(7.5) 1 0.2(7.5) 5 7.50
Note that the optimal decision using the expected utility approach is d3, do not invest. 
The ranking of alternatives according to the president’s utility assignments and the associ-
ated monetary values are as follows:
Ranking of  
Decision Alternatives
Expected 
Utility
Expected 
Value
Do Not Invest
Investment A
Investment B
7.50
7.35
6.55
0
9000
21000
Note that, although investment A had the highest expected value of $9,000, the analysis 
indicates that Swofford should decline this investment. The rationale behind not selecting 
investment A is that the 0.20 probability of a $50,000 loss was considered to involve a 
serious risk by Swofford’s president. The seriousness of this risk and its associated impact 
on the company were not adequately reflected by the expected value of investment A. We 
assessed the utility for each payoff to assess this risk adequately.
The following steps state in general terms the procedure used to solve the Swofford, 
Inc., investment problem:
Step 1. Develop a payoff table using monetary values
Step 2. Identify the best and worst payoff values in the table and assign each a utility, 
with u(best payoff) . u(worst payoff)
Step 3. For every other monetary value m in the original payoff table, do the following 
to determine its utility:
a. Define the lottery such that there is a probability p of the best payoff 
and a probability (1 2 p) of the worst payoff
b. Determine the value of p such that the decision maker is indifferent 
between a guaranteed payoff of m and the lottery defined in step 3(a)
c. Calculate the utility of m as follows:
u(m) 5 pu(best payoff) 1 (1 2 p)u(worst payoff)
Step 4. Convert each monetary value in the payoff table to a utility
Step 5. Apply the expected utility approach to the utility table developed in step 4 and 
select the decision alternative with the highest expected utility
The procedure we described for determining the utility of monetary consequences can 
also be used to develop a utility measure for nonmonetary consequences. Assign the best 

 
12.6 Utility Theory 
577
consequence a utility of 10 and the worst a utility of 0. Then create a lottery with a prob-
ability of p for the best consequence and (1 2 p) for the worst consequence. For each of the 
other consequences, find the value of p that makes the decision maker indifferent between 
the lottery and the consequence. Then calculate the utility of the consequence in question 
as follows:
 
u(consequence) 5 pu(best consequence) 1 (1 2 p)u(worst consequence)
Utility Functions
Next we describe how different decision makers may approach risk in terms of their assess-
ment of utility. The financial position of Swofford, Inc., was such that the firm’s president 
evaluated investment opportunities from a conservative, or risk-avoiding, point of view. 
However, if the firm had a surplus of cash and a stable future, Swofford’s president might 
have been looking for investment alternatives that, although perhaps risky, contained a 
potential for substantial profit. That type of behavior would demonstrate that the president 
is a risk taker with respect to this decision.
A risk taker is a decision maker who would choose a lottery over a guaranteed payoff 
when the expected value of the lottery is inferior to the guaranteed payoff. In this section, 
we analyze the decision problem faced by Swofford from the point of view of a decision 
maker who would be classified as a risk taker. We then compare the conservative point of 
view of Swofford’s president (a risk avoider) with the behavior of a decision maker who 
is a risk taker.
For the decision problem facing Swofford, Inc., using the general procedure for devel-
oping utilities as discussed previously, a risk taker might express the utility for the various 
payoffs shown in Table 12.12. As before, u(50,000) 5 10 and u(250,000) 5 0. Note 
the difference in behavior reflected in Table 12.12 and Table 12.10. In other words, in 
determining the value of p at which the decision maker is indifferent between a guaranteed 
payoff of m and a lottery in which $50,000 is obtained with probability p and 2$50,000 
with probability (1 2 p), the risk taker is willing to accept a greater risk of incurring a loss 
of $50,000 in order to gain the opportunity to realize a profit of $50,000.
To help develop the utility table for the risk taker, we have reproduced the Swofford, 
Inc. payoff table in Table 12.13. Using these payoffs and the risk taker’s utilities given in 
Table 12.12, we can write the risk taker’s utility table as shown in Table 12.14. Using the 
state-of-nature probabilities P(s1) 5 0.3, P(s2) 5 0.5, and P(s3) 5 0.2, the expected utility 
for each decision alternative is
 
 EU(d1) 5 0.3(5.0) 1 0.5(4.0) 1 0.2(0) 5 3.50
 
 EU(d2) 5 0.3(10) 1 0.5(1.5) 1 0.2(1.0) 5 3.95
 
 EU(d3) 5 0.3(2.5) 1 0.5(2.5) 1 0.2(2.5) 5 2.50
TABLE 12.12   REVISED UTILITIES FOR SWOFFORD, INC., ASSUMINg A 
RISk TAkER
Monetary Value
Indifference Value of p
Utility
$50,000
30,000
20,000
0
220,000
230,000
250,000
Does not apply
0.50
0.40
0.25
0.15
0.10
Does not apply
10.0
 5.0
 4.0
 2.5
 1.5
 1.0
   0

578 
Chapter 12 Decision Analysis
What is the recommended decision? Perhaps somewhat to your surprise, the analysis 
recommends investment B, with the highest expected utility of 3.95. Recall that this invest-
ment has a 2$1,000 expected value. Why is it now the recommended decision? Remember 
that the decision maker in this revised problem is a risk taker. Thus, although the expected 
value of investment B is negative, utility analysis has shown that this decision maker is 
enough of a risk taker to prefer investment B and its potential for the $50,000 profit.
Ranking by the expected utilities generates the following order of preference of the 
decision alternatives for the risk taker and the associated expected values:
Ranking of  
Decision Alternatives
Expected 
Utility
Expected 
Value
Investment B
Investment A
Do Not Invest
3.95
3.50
2.50
2$1,000
$9,000
0
Comparing the utility analysis for a risk taker with the more conservative preferences 
of the president of Swofford, Inc., who is a risk avoider, we see that, even with the same 
decision problem, different attitudes toward risk can lead to different recommended deci-
sions. The utilities established by Swofford’s president indicated that the firm should not 
invest at this time, whereas the utilities established by the risk taker showed a preference for 
investment B. Note that both of these decisions differ from the best expected value decision, 
which was investment A.
We can obtain another perspective of the difference between behaviors of a risk avoider 
and a risk taker by developing a graph that depicts the relationship between monetary value 
and utility. We use the horizontal axis of the graph to represent monetary values and the 
vertical axis to represent the utility associated with each monetary value. Now, consider 
the data in Table 12.10, with a utility corresponding to each monetary value for the original 
Swofford, Inc., problem. These values can be plotted on a graph to produce the top curve 
in Figure 12.11. The resulting curve is the utility function for money for Swofford’s 
president. Recall that these points reflected the conservative, or risk-avoiding, nature of 
Swofford’s president. Hence, we refer to the top curve in Figure 12.11 as a utility function 
TABLE 12.13  PAYOFF TABLE FOR SWOFFORD, INC.
State of Nature
Decision Alternative
Prices Up s1
Prices Stable s2
Prices Down s3
Investment A, d1
Investment B, d2
Do Not Invest, d3
$30,000
$50,000
      0
$20,000
2$20,000
0
2$50,000
2$30,000
0
TABLE 12.14  UTILITY TABLE OF A RISk TAkER FOR SWOFFORD, INC.
State of Nature
Decision Alternative
Prices Up s1
Prices Stable s2
Prices Down s3
Investment A, d1
Investment B, d2
Do Not Invest, d3
 5.0
10.0
 2.5
4.0
1.5
2.5
  0
1.0
2.5

 
12.6 Utility Theory 
579
for a risk avoider. Using the data in Table 12.12 developed for a risk taker, we can plot 
these points to produce the bottom curve in Figure 12.8. The resulting curve depicts the 
utility function for a risk taker.
By looking at the utility functions in Figure 12.11, we can begin to generalize about 
the utility functions for risk avoiders and risk takers. Although the exact shape of the utility 
function will vary from one decision maker to another, we can see the general shape of these 
two types of utility functions. The utility function for a risk avoider shows a diminishing 
marginal return for money. For example, the increase in utility going from a monetary value 
of 2$30,000 to $0 is 7.5 2 4.0 5 3.5, whereas the increase in utility in going from $0 to 
$30,000 is only 9.5 2 7.5 5 2.0.
However, the utility function for a risk taker shows an increasing marginal return for 
money. For example, in Figure 12.8, the increase in utility for the risk taker in going from 
2$30,000 to $0 is 2.5 2 1.0 5 1.5, whereas the increase in utility in going from $0 to 
$30,000 for the risk taker is 5.0 2 2.5 5 2.5. Note also that in either case the utility func-
tion is always increasing; that is, more money leads to more utility. All utility functions 
possess this property.
We concluded that the utility function for a risk avoider shows a diminishing marginal 
return for money and that the utility function for a risk taker shows an increasing marginal 
return. When the marginal return for money is neither decreasing nor increasing but remains 
constant, the corresponding utility function describes the behavior of a decision maker who 
is neutral to risk. The following characteristics are associated with a risk-neutral decision 
maker:
 1. The utility function can be drawn as a straight line connecting the “best” and the 
“worst” points.
 2. The expected utility approach and the expected value approach applied to monetary 
payoffs result in the same action.
FigurE 12.11   UTILITY FUNCTION FOR MONEY FOR RISk-AVOIDER, RISk-
TAkER, AND RISk-NEUTRAL DECISION MAkERS
10
–50
Monetary Value ($1000s) 
Utility
–40 –30 –20 –10
  0
 10
 20
 30
 40
 50
Risk Neutral
Risk Avoider 
Risk Taker 
9
8
7
6
5
4
3
2
1
0

580 
Chapter 12 Decision Analysis
The straight, diagonal line in Figure 12.11 depicts the utility function of a risk-neutral deci-
sion maker using the Swofford, Inc., problem data. 
generally, when the payoffs for a particular decision-making problem fall into a reason-
able range—the best is not too good and the worst is not too bad—decision makers tend 
to express preferences in agreement with the expected value approach. Thus, we suggest 
asking the decision maker to consider the best and worst possible payoffs for a problem and 
assess their reasonableness. If the decision maker believes that they are in the reasonable 
range, the decision alternative with the best expected value can be used. However, if the 
payoffs appear unreasonably large or unreasonably small (for example, a huge loss) and if 
the decision maker believes that monetary values do not adequately reflect her or his true 
preferences for the payoffs, a utility analysis of the problem should be considered.
Exponential Utility Function
Having a decision maker provide enough indifference values to create a utility function can 
be time consuming. An alternative is to assume that the decision maker’s utility is defined 
by an exponential function. Figure 12.12 shows examples of different exponential utility 
functions. Note that all the exponential utility functions indicate that the decision maker is 
risk averse. The form of the exponential utility function is as follows.
EXPONENTIAL UTILITY FUNCTION
 
u(x) 5 1 2 e2x/r 
(12.7)
The r parameter in equation (12.7) represents the decision maker’s risk tolerance; it controls 
the shape of the exponential utility function. Larger r values create flatter exponential func-
tions, indicating that the decision maker is less risk averse (closer to risk neutral). Smaller 
r values indicate that the decision maker has less risk tolerance (is more risk averse). A 
common method to determine an approximate risk tolerance is to ask the decision maker to 
In equation (12.7), the 
number e ø 2.718282 . . . 
is a mathematical constant 
corresponding to the base 
of the natural logarithm. In 
excel, ex can be evaluated 
for any power x using the 
function eXP(x).
FigurE 12.12   EXPONENTIAL UTILITY FUNCTIONS WITH DIFFERENT RISk 
TOLERANCE (r) VALUES
1.0
0.8
0.2
0.4
0.6
0
–0.04
–0.02
–0.06
5
Utility, U(x)
10
R = 10
R = 20
R = 50
R = 100
15
20
25
x

 
Summary 
581
consider a scenario where he or she could win $r with probability 0.5 and lose $r/2 with 
probability 0.5. The r value to use in equation (12.7) is the largest $r for which the deci-
sion maker would accept this gamble. For instance, if the decision maker is comfortable 
accepting a gamble with a 50 percent chance of winning $2,000 and a 50 percent chance 
of losing $1,000, but not with a gamble with a 50 percent chance of winning $3,000 and 
a 50 percent chance of losing $1,500 then we would use r 5 $2,000 in equation (12.7). 
Determining the maximum gamble that a decision maker is willing to take and then using 
this value in the exponential utility function can be much less time-consuming than gen-
erating a complete table of indifference probabilities. One should remember that using an 
exponential utility function assumes that the decision maker is risk averse; however, this is 
often true in practice for business decisions.
NOTES AND COMMENTS
1. In the Swofford problem, we have been us-
ing a utility of 10 for the best payoff and 0 for 
the worst. We could have chosen any values 
as long as the utility associated with the best 
payoff exceeds the utility associated with the 
worst payoff. Alternatively, a utility of 1 can be 
associated with the best payoff and a utility of 0 
associated with the worst payoff. Had we made 
this choice, the utility for any monetary value 
m would have been the value of p at which 
the decision maker was indifferent between a 
guaranteed payoff of m and a lottery in which 
the best payoff is  obtained with probability p 
and the worst payoff is obtained with probabil-
ity (1 2 p). Thus, the utility for any monetary 
value would have been equal to the probability 
of earning the best payoff. Often this choice is 
made because of the ease in computation. We 
chose not to do so to emphasize the distinction 
between the utilities and the indifference prob-
abilities for the lottery.
2. Circumstances often dictate whether one acts as 
a risk avoider or a risk taker when making a de-
cision. For example, you may think of yourself 
as a risk avoider when faced with financial deci-
sions, but if you have ever purchased a lottery 
ticket, you have actually acted as a risk taker. 
For example, suppose you purchase a $1 lottery 
ticket for a simple lottery in which the object is 
to pick the six numbers that will be drawn from 
50 potential numbers. Also suppose that the 
winner (who correctly choses all six numbers 
that are drawn) will receive $1,000,000. There 
are 15,890,700 possible winning combinations, 
so your probability of winning is 1/15890700 5 
0.000000062929889809763 (i.e., very low) and 
the expected value of your ticket is 
1
15,890,700 ($1,000,000 2 $1) 1 a1 2
1
15,890,700b (2$1)
 5 2$0.93707
 
or about 2$0.94.
 
 If a lottery ticket has a negative expected 
value, why does anyone play? The answer is in 
utility; most people who play lotteries associ-
ate great utility with the possiblity of winning 
the $1,000,000 prize and relatively little utility 
with the $1 cost for a ticket, and so the expected 
value of the utility of the lottery ticket is positive 
even though the expected value of the ticket is 
 negative.
Summary
Decision analysis can be used to determine a recommended decision alternative or an op-
timal decision strategy when a decision maker is faced with an uncertain and risk-filled 
pattern of future events. The goal of decision analysis is to identify the best decision alterna-
tive or the optimal decision strategy, given information about the uncertain events and the 
possible consequences or payoffs. The “best” decision should consider the risk preference 
of the decision maker in evaluating outcomes.

582 
Chapter 12 Decision Analysis
We showed how payoff tables and decision trees could be used to structure a decision 
problem and describe the relationships among the decisions, the chance events, and the 
consequences. We presented three approaches to decision making without probabilities: the 
optimistic approach, the conservative approach, and the minimax regret approach. When 
probability assessments are provided for the states of nature, the expected value approach 
can be used to identify the recommended decision alternative or decision strategy.
Even though the expected value approach can be used to obtain a recommended deci-
sion alternative or optimal decision strategy, the payoff that actually occurs will usually 
have a value different from the expected value. A risk profile provides a probability dis-
tribution for the possible payoffs and can assist the decision maker in assessing the risks 
associated with different decision alternatives. Sensitivity analysis can be conducted to 
determine the effect changes in the probabilities for the states of nature and changes in the 
values of the payoffs have on the recommended decision alternative.
In cases where sample information about the chance events is available, a sequence of 
decisions has to be made. First we must decide whether to obtain the sample information. If 
the answer to this decision is yes, an optimal decision strategy based on the specific sample 
information must be developed. In this situation, decision trees and the expected value 
 approach can be used to determine the optimal decision strategy. 
Bayes’ theorem can be used to compute branch probabilities for decision trees. Bayes’ 
theorem updates a decision maker’s prior probabilities regarding the states of nature using 
sample information to compute revised posterior probabilities.
We showed how utility could be used in decision-making situations in which monetary 
value did not provide an adequate measure of the payoffs. Utility is a measure of the total 
worth of an outcome. As such, utility takes into account the decision maker’s assessment 
of all aspects of a consequence, including profit, loss, risk, and perhaps additional nonmon-
etary factors. The examples showed how the use of expected utility can lead to decision 
recommendations that differ from those based on expected value.
A decision maker’s judgment must be used to establish the utility for each consequence. 
We presented a step-by-step procedure to determine a decision maker’s utility for monetary 
payoffs. We also discussed how conservative, risk-avoiding decision makers assess utility 
differently from more aggressive, risk-taking decision makers. 
Glossary
Decision alternatives Options available to the decision maker.
Chance event An uncertain future event affecting the consequence, or payoff, associated 
with a decision.
Outcome The result obtained when a decision alternative is chosen and a chance event 
occurs. 
States of nature The possible outcomes for chance events that affect the payoff associated 
with a decision alternative.
Payoff A measure of the outcome of a decision such as profit, cost, or time. Each combina-
tion of a decision alternative and a state of nature has an associated payoff.
Payoff table A tabular representation of the payoffs for a decision problem.
Decision tree A graphical representation of the decision problem that shows the sequential 
nature of the decision-making process.
Node An intersection or junction point of a decision tree.
Decision nodes Nodes indicating points where a decision is made.
Chance nodes Nodes indicating points where an uncertain event will occur.
Branch Lines showing the alternatives from decision nodes and the outcomes from chance 
nodes.

 
Glossary 
583
Optimistic approach An approach to choosing a decision alternative without using prob-
abilities. For a maximization problem, it leads to choosing the decision alternative corre-
sponding to the largest payoff; for a minimization problem, it leads to choosing the decision 
alternative corresponding to the smallest payoff.
Conservative approach An approach to choosing a decision alternative without using 
probabilities. For a maximization problem, it leads to choosing the decision alternative 
that maximizes the minimum payoff; for a minimization problem, it leads to choosing the 
decision alternative that minimizes the maximum payoff.
Regret (opportunity loss) The amount of loss (lower profit or higher cost) from not mak-
ing the best decision for each state of nature.
Minimax regret approach An approach to choosing a decision alternative without using 
probabilities. For each alternative, the maximum regret is computed, which leads to choos-
ing the decision alternative that minimizes the maximum regret.
Expected value approach An approach to choosing a decision alternative based on the 
expected value of each decision alternative. The recommended decision alternative is the 
one that provides the best expected value.
Expected value (EV) For a chance node, the weighted average of the payoffs. The weights 
are the state-of-nature probabilities.
Risk analysis The study of the possible payoffs and probabilities associated with a decision 
alternative or a decision strategy in the face of uncertainty.
Risk profile The probability distribution of the possible payoffs associated with a decision 
alternative or decision strategy.
Sensitivity analysis The study of how changes in the probability assessments for the states 
of nature or changes in the payoffs affect the recommended decision alternative.
Sample information New information obtained through research or experimentation that 
enables an updating or revision of the state-of-nature probabilities.
Prior probabilities The probabilities of the states of nature prior to obtaining sample 
 information.
Posterior (revised) probabilities The probabilities of the states of nature after revising the 
prior probabilities based on sample information.
Decision strategy A strategy involving a sequence of decisions and chance outcomes to 
provide the optimal solution to a decision problem.
Expected value of sample information (EVSI) The difference between the expected value 
of an optimal strategy based on sample information and the “best” expected value without 
any sample information.
Perfect information A special case of sample information where the information tells the 
decision maker exactly which state of nature is going to occur.
Expected value of perfect information (EVPI) The difference between the expected 
value of an optimal strategy based on perfect information and the “best” expected value 
without any sample information.
Bayes’ theorem A theorem that enables the use of sample information to revise prior 
 probabilities.
Conditional probabilities The probability of one event, given the known outcome of a 
(possibly) related event.
Joint probabilities The probabilities of both sample information and a particular state of 
nature occurring simultaneously.
Utility A measure of the total worth of a consequence reflecting a decision maker’s attitude 
toward considerations such as profit, loss, and risk.
Risk avoider A decision maker who would choose a guaranteed payoff over a lottery with 
a better expected payoff.
Expected utility (EU) The weighted average of the utilities associated with a decision 
alternative. The weights are the state-of-nature probabilities.

584 
Chapter 12 Decision Analysis
Risk taker A decision maker who would choose a lottery over a better guaranteed payoff.
Utility function for money A curve that depicts the relationship between monetary value 
and utility.
Risk-neutral A decision maker who is neutral to risk. For this decision maker the deci-
sion alternative with the best expected value is identical to the alternative with the highest 
expected utility.
Problems
 1. The following payoff table shows profit for a decision analysis problem with two decision 
alternatives and three states of nature:
State of Nature
Decision Alternative
s1
s2
s3
d1
d2
250
100
100
100
25
75
a. Construct a decision tree for this problem.
b. If the decision maker knows nothing about the probabilities of the three states of 
nature, what is the recommended decision using the optimistic, conservative, and 
minimax regret approaches?
 2. Southland Corporation’s decision to produce a new line of recreational products resulted 
in the need to construct either a small plant or a large plant. The best selection of plant size 
depends on how the marketplace reacts to the new product line. To conduct an analysis, 
marketing management has decided to view the possible long-run demand as low, me-
dium, or high. The following payoff table shows the projected profit in millions of dollars:
Long-Run Demand
Plant Size
Low
Medium
High
Small
Large
150
 50
200
200
200
500
a. What is the decision to be made, and what is the chance event for Southland’s 
 problem?
b. Construct a decision tree.
c. Recommend a decision based on the use of the optimistic, conservative, and minimax 
regret approaches.
 3. Amy Lloyd is interested in leasing a new Honda and has contacted three automobile deal-
ers for pricing information. Each dealer offered Amy a closed-end 36-month lease with 
no down payment due at the time of signing. Each lease includes a monthly charge and a 
mileage allowance. Additional miles receive a surcharge on a per-mile basis. The monthly 
lease cost, the mileage allowance, and the cost for additional miles follow:
Dealer
Monthly Cost
Mileage Allowance
Cost per  
Additional Mile
Hepburn Honda
Midtown Motors
Hopkins Automotive
$299
$310
$325
36,000
45,000
54,000
$0.15
$0.20
$0.15

 
Problems 
585
 
 Amy decided to choose the lease option that will minimize her total 36-month cost. The 
difficulty is that Amy is not sure how many miles she will drive over the next three years. 
For purposes of this decision, she believes it is reasonable to assume that she will drive 
12,000 miles per year, 15,000 miles per year, or 18,000 miles per year. With this assump-
tion Amy estimated her total costs for the three lease options. For example, she figures that 
the Hepburn Honda lease will cost her 36($299) 1 $0.15(36,000 2 36,000) 5 $10,764 if 
she drives 12,000 miles per year, 36($299) 1 $0.15(45,000 2 36,000) 5 $12,114 if she 
drives 15,000 miles per year, or 36($299) 1 $0.15(54,000 2 36,000) 5 $13,464 if she 
drives 18,000 miles per year.
a. What is the decision, and what is the chance event?
b. Construct a payoff table for Amy’s problem.
c. If Amy has no idea which of the three mileage assumptions is most appropriate, what 
is the recommended decision (leasing option) using the optimistic, conservative, and 
minimax regret approaches?
d. Suppose that the probabilities that Amy drives 12,000, 15,000, and 18,000 miles per 
year are 0.5, 0.4, and 0.1, respectively. What option should Amy choose using the 
expected value approach?
e. Develop a risk profile for the decision selected in part d. What is the most likely cost, 
and what is its probability?
f. 
Suppose that, after further consideration, Amy concludes that the probabilities that she 
will drive 12,000, 15,000, and 18,000 miles per year are 0.3, 0.4, and 0.3, respectively. 
What decision should Amy make using the expected value approach?
  4. Investment advisors estimated the stock market returns for four market segments: com-
puters, financial, manufacturing, and pharmaceuticals. Annual return projections vary de-
pending on whether the general economic conditions are improving, stable, or declining. 
The anticipated annual return percentages for each market segment under each economic 
condition are as follows:
Economic Condition
Market Segment
Improving
Stable
Declining
Computers
Financial
Manufacturing
Pharmaceuticals
10
 8
 6
 6
2
5
4
5
24
23
22
21
a. Assume that an individual investor wants to select one market segment for a new 
investment. A forecast shows improving to declining economic conditions with the 
following probabilities: improving (0.2), stable (0.5), and declining (0.3). What is the 
preferred market segment for the investor, and what is the expected return percentage?
b. At a later date, a revised forecast shows a potential for an improvement in economic 
conditions. New probabilities are as follows: improving (0.4), stable (0.4), and declin-
ing (0.2). What is the preferred market segment for the investor based on these new 
probabilities? What is the expected return percentage?
 5. Hudson Corporation is considering three options for managing its data warehouse: continu-
ing with its own staff, hiring an outside vendor to do the managing, or using a combination 
of its own staff and an outside vendor. The cost of the operation depends on future demand. 
The annual cost of each option (in thousands of dollars) depends on demand as follows:
Demand
Staffing Options
High
Medium
Low
Own Staff
Outside Vendor
Combination
650
900
800
650
600
650
600
300
500

586 
Chapter 12 Decision Analysis
a. If the demand probabilities are 0.2, 0.5, and 0.3, which decision alternative will min-
imize the expected cost of the data warehouse? What is the expected annual cost 
 associated with that recommendation?
b. Construct a risk profile for the optimal decision in part a. What is the probability of 
the cost exceeding $700,000?
 6. The following payoff table shows the profit for a decision problem with two states of 
nature and two decision alternatives:
State of Nature
Decision Alternative
s1
s2
d1
d2
10
 4
1
3
a. Suppose P(s1) 5 0.2 and P(s2) 5 0.8. What is the best decision using the expected 
value approach?
b. Perform sensitivity analysis on the payoffs for decision alternative d1. Assume the 
probabilities are as given in part a, and find the range of payoffs under states of nature 
s1 and s2 that will keep the solution found in part a optimal. Is the solution more sensi-
tive to the payoff under state of nature s1 or s2?
 7. Myrtle Air Express decided to offer direct service from Cleveland to Myrtle Beach. Man-
agement must decide between a full-price service using the company’s new fleet of jet 
aircraft and a discount service using smaller-capacity commuter planes. It is clear that the 
best choice depends on the market reaction to the service Myrtle Air offers. Management 
developed estimates of the contribution to profit for each type of service based on two pos-
sible levels of demand for service to Myrtle Beach: strong and weak. The following table 
shows the estimated quarterly profits (in thousands of dollars):
Demand for Service
Service
Strong
Weak
Full Price
Discount
$960
$670
2$490
$320
a. What is the decision to be made, what is the chance event, and what is the consequence 
for this problem? How many decision alternatives are there? How many outcomes are 
there for the chance event?
b. If nothing is known about the probabilities of the chance outcomes, what is the recom-
mended decision using the optimistic, conservative, and minimax regret approaches?
c. Suppose that management of Myrtle Air Express believes that the probability of strong 
demand is 0.7 and the probability of weak demand is 0.3. Use the expected value ap-
proach to determine an optimal decision.
d. Suppose that the probability of strong demand is 0.8 and the probability of weak de-
mand is 0.2. What is the optimal decision using the expected value approach?
e. Use sensitivity analysis to determine the range of demand probabilities for which each 
of the decision alternatives has the largest expected value.
 8. Video Tech is considering marketing one of two new video games for the coming holiday 
season: Battle Pacific or Space Pirates. Battle Pacific is a unique game and appears to have 
no competition. Estimated profits (in thousands of dollars) under high, medium, and low 
demand are as follows:
Demand
Battle Pacific
High
Medium
Low
Profit
Probability
$1,000
0.2
$700
0.5
$300
0.3

 
Problems 
587
 
 Video Tech is optimistic about its Space Pirates game. However, the concern is that profit-
ability will be affected by a competitor’s introduction of a video game viewed as similar 
to Space Pirates. Estimated profits (in thousands of dollars) with and without competition 
are as follows:
Space Pirates
Demand
With Competition
High
Medium
Low
Profit
Probability
$800
0.3
$400
0.4
$200
0.3
Space Pirates
Demand
Without Competition
High
Medium
Low
Profit
Probability
$1,600
0.5
$800
0.3
$400
0.2
a. Develop a decision tree for the Video Tech problem.
b. For planning purposes, Video Tech believes there is a 0.6 probability that its com-
petitor will produce a new game similar to Space Pirates. given this probability of 
competition, the director of planning recommends marketing the Battle Pacific video 
game. Using expected value, what is your recommended decision?
c. Show a risk profile for your recommended decision.
d. Use sensitivity analysis to determine what the probability of competition for Space 
Pirates would have to be for you to change your recommended decision alternative.
 9. Seneca Hill Winery recently purchased land for the purpose of establishing a new vineyard. 
Management is considering two varieties of white grapes for the new vineyard: Chardonnay 
and Riesling. The Chardonnay grapes would be used to produce a dry Chardonnay wine, 
and the Riesling grapes would be used to produce a semidry Riesling wine. It takes ap-
proximately four years from the time of planting before new grapes can be harvested. This 
length of time creates a great deal of uncertainty concerning future demand and makes the 
decision about the type of grapes to plant difficult. Three possibilities are being considered: 
 Chardonnay grapes only; Riesling grapes only; and both Chardonnay and Riesling grapes. 
Seneca management decided that for planning purposes it would be adequate to consider 
only two demand possibilities for each type of wine: strong or weak. With two possibilities 
for each type of wine, it was necessary to assess four probabilities. With the help of some 
forecasts in industry publications, management made the following probability assessments:
Riesling Demand
Chardonnay Demand
Weak
Strong
Weak
Strong
0.05
0.25
0.50
0.20
 
 Revenue projections show an annual contribution to profit of $20,000 if Seneca Hill plants 
only Chardonnay grapes and demand is weak for Chardonnay wine, and $70,000 if  Seneca 
plants only Chardonnay grapes and demand is strong for Chardonnay wine. If Seneca 
plants only Riesling grapes, the annual profit projection is $25,000 if demand is weak for 
Riesling grapes and $45,000 if demand is strong for Riesling grapes. If Seneca plants both 
types of grapes, the annual profit projections are shown in the following table:
Riesling Demand
Chardonnay Demand
Weak
Strong
Weak
Strong
$22,000
$26,000
$40,000
$60,000

588 
Chapter 12 Decision Analysis
a. What is the decision to be made, what is the chance event, and what is the conse-
quence? Identify the alternatives for the decisions and the possible outcomes for the 
chance events.
b. Develop a decision tree.
c. Use the expected value approach to recommend which alternative Seneca Hill Winery 
should follow in order to maximize expected annual profit.
d. Suppose management is concerned about the probability assessments when demand 
for Chardonnay wine is strong. Some believe it is likely for Riesling demand to also 
be strong in this case. Suppose the probability of strong demand for Chardonnay 
and weak demand for Riesling is 0.05 and that the probability of strong demand for 
Chardonnay and strong demand for Riesling is 0.40. How does this change the recom-
mended decision? Assume that the probabilities when Chardonnay demand is weak 
are still 0.05 and 0.50.
e. Other members of the management team expect the Chardonnay market to become 
saturated at some point in the future, causing a fall in prices. Suppose that the annual 
profit projections fall to $50,000 when demand for Chardonnay is strong and only 
Chardonnay grapes are planted. Using the original probability assessments, determine 
how this change would affect the optimal decision.
 10. Hemmingway, Inc. is considering a $5 million research and development (R&D) project. 
Profit projections appear promising, but Hemmingway’s president is concerned because 
the probability that the R&D project will be successful is only 0.50. Furthermore, the pres-
ident knows that even if the project is successful, it will require that the company build 
a new production facility at a cost of $20 million in order to manufacture the product. If 
the facility is built, uncertainty remains about the demand and thus uncertainty about the 
profit that will be realized. Another option is that if the R&D project is successful, the 
company could sell the rights to the product for an estimated $25 million. Under this op-
tion, the company would not build the $20 million production facility.
 
  
The decision tree follows. The profit projection for each outcome is shown at the end of 
the branches. For example, the revenue projection for the high demand outcome is $59 mil-
lion. However, the cost of the R&D project ($5 million) and the cost of the production 
facility ($20 million) show the profit of this outcome to be $59 2 $5 2 $20 5 $34 million. 
Branch probabilities are also shown for the chance events.
Not Successful
0.5
Start R&D Project ($5 million)
Do Not Start the R&D Project
Successful
0.5
1
2
3
Building Facility ($20 million)
Sell Rights
34
20
10
20
25
0
Proﬁt ($ millions)
High Demand
0.5
Medium Demand
0.3
Low Demand
0.2
4

 
Problems 
589
a. Analyze the decision tree to determine whether the company should undertake the 
R&D project. If it does, and if the R&D project is successful, what should the company 
do? What is the expected value of your strategy?
b. What must the selling price be for the company to consider selling the rights to the 
product?
c. Develop a risk profile for the optimal strategy.
 11. Dante Development Corporation is considering bidding on a contract for a new office 
building complex. The following figure shows the decision tree prepared by one of 
Dante’s analysts. At node 1, the company must decide whether to bid on the contract. The 
cost of preparing the bid is $200,000. The upper branch from node 2 shows that the com-
pany has a 0.8 probability of winning the contract if it submits a bid. If the company wins 
the bid, it will have to pay $2 million to become a partner in the project. Node 3 shows that 
the company will then consider doing a market research study to forecast demand for the 
office units prior to beginning construction. The cost of this study is $150,000. Node 4 is 
a chance node showing the possible outcomes of the market research study.
 
  
Nodes 5, 6, and 7 are similar in that they are the decision nodes for Dante to either build 
the office complex or sell the rights in the project to another developer. The decision to 
build the complex will result in an income of $5 million if demand is high and $3 million if 
demand is moderate. If Dante chooses to sell its rights in the project to another developer, 
income from the sale is estimated to be $3.5 million. The probabilities shown at nodes 4, 
8, and 9 are based on the projected outcomes of the market research study.
Lose Contract
0.2
Bid
Do Not Bid
Win Contract
0.8
1
2
3
Market Research
No Market Research
Build Complex
Sell
6
1150
2650
650
1150
Build Complex
Sell
7
2800
800
1300
2200
0
Build Complex
Sell
5
2650
Proﬁt ($1000s)
650
Forecast High
0.6
Forecast Moderate
0.4
4
10
9
High Demand
0.85
Moderate Demand
0.15
High Demand
0.225
Moderate Demand
0.775
High Demand
0.6
Moderate Demand
0.4
8
a. Verify Dante’s profit projections shown at the ending branches of the decision tree by 
calculating the payoffs of $2,650,000 and $650,000 for first two outcomes.
b. What is the optimal decision strategy for Dante, and what is the expected profit for 
this project?
c. What would the cost of the market research study have to be before Dante would 
change its decision about the market research study?
d. Develop a risk profile for Dante.

590 
Chapter 12 Decision Analysis
 12. Embassy Publishing Company received a six-chapter manuscript for a new college text-
book. The editor of the college division is familiar with the manuscript and estimated a 
0.65 probability that the textbook will be successful. If successful, a profit of $750,000 
will be realized. If the company decides to publish the textbook and it is unsuccessful, a 
loss of $250,000 will occur.
 
  
Before making the decision to accept or reject the manuscript, the editor is considering 
sending the manuscript out for review. A review process provides either a favorable (F) 
or unfavorable (u) evaluation of the manuscript. Past experience with the review process 
suggests that probabilities P(F) 5 0.7 and P(u) 5 0.3 apply. Let s1 5 the textbook is suc-
cessful, and s2 5 the textbook is unsuccessful. The editor’s initial probabilities of s1 and 
s2 will be revised based on whether the review is favorable or unfavorable. The revised 
probabilities are as follows:
 
P(s1 0 F) 5 0.75   P(s1 0 u) 5 0.417
 
P(s2 0 F) 5 0.25    P(s2 0 u) 5 0.583
a. Construct a decision tree assuming that the company will first make the decision as to 
whether to send the manuscript out for review and then make the decision to accept or 
reject the manuscript.
b. Analyze the decision tree to determine the optimal decision strategy for the publishing 
company.
c. If the manuscript review costs $5,000, what is your recommendation?
d. What is the expected value of perfect information? What does this EVPI suggest for 
the company?
 13. The following profit payoff table was presented in Problem 1:
State of Nature
Decision Alternative
s1
s2
s3
d1
d2
250
100
100
100
25
75
 
 The probabilities for the states of nature are P(s1) 5 0.65, P(s2) 5 0.15, and P(s3) 5 0.20.
a. What is the optimal decision strategy if perfect information were available?
b. What is the expected value for the decision strategy developed in part a?
c. Using the expected value approach, what is the recommended decision without perfect 
information? What is its expected value?
d. What is the expected value of perfect information?
 14. The Lake Placid Town Council decided to build a new community center to be used for 
conventions, concerts, and other public events, but considerable controversy surrounds 
the appropriate size. Many influential citizens want a large center that would be a show-
case for the area. But the mayor feels that if demand does not support such a center, 
the community will lose a large amount of money. To provide structure for the decision 
process, the council narrowed the building alternatives to three sizes: small, medium, and 
large. Everybody agreed that the critical factor in choosing the best size is the number of 
people who will want to use the new facility. A regional planning consultant provided de-
mand estimates under three scenarios: worst case, base case, and best case. The worst-case 
scenario corresponds to a situation in which tourism drops substantially; the base-case 
scenario corresponds to a situation in which Lake Placid continues to attract visitors at 
current levels; and the best-case scenario corresponds to a substantial increase in tourism. 
The consultant has provided probability assessments of 0.10, 0.60, and 0.30 for the worst-
case, base-case, and best-case scenarios, respectively.
 
  
The town council suggested using net cash flow over a 5-year planning horizon as 
the criterion for deciding on the best size. The following projections of net cash flow 

 
Problems 
591
(in thousands of dollars) for a five-year planning horizon have been developed. All 
costs, including the consultant’s fee, have been included.
Demand Scenario
Center Size
Worst Case
Base Case
Best Case
Small
Medium
Large
  400
2250
2400
500
650
580
660
800
990
a. What decision should Lake Placid make using the expected value approach?
b. Construct risk profiles for the medium and large alternatives. given the mayor’s con-
cern over the possibility of losing money and the result of part a, which alternative 
would you recommend?
c. Compute the expected value of perfect information. Do you think it would be worth 
trying to obtain additional information concerning which scenario is likely to occur?
d. Suppose the probability of the worst-case scenario increases to 0.2, the probability of 
the base-case scenario decreases to 0.5, and the probability of the best-case scenario 
remains at 0.3. What effect, if any, would these changes have on the decision recom-
mendation?
e. The consultant has suggested that an expenditure of $150,000 on a promotional cam-
paign over the planning horizon will effectively reduce the probability of the worst-
case scenario to zero. If the campaign can be expected to also increase the probability 
of the best-case scenario to 0.4, is it a good investment?
 15. A real estate investor has the opportunity to purchase land currently zoned residential. If 
the county board approves a request to rezone the property as commercial within the next 
year, the investor will be able to lease the land to a large discount firm that wants to open 
a new store on the property. However, if the zoning change is not approved, the investor 
will have to sell the property at a loss. Profits (in thousands of dollars) are shown in the 
following payoff table:
State of Nature
Decision Alternative
Rezoning Approved 
s1
Rezoning Not Approved 
s2
Purchase, d1
Do not purchase, d2
600
  0
2200
    0
a. If the probability that the rezoning will be approved is 0.5, what decision is recom-
mended? What is the expected profit?
b. The investor can purchase an option to buy the land. Under the option, the investor 
maintains the rights to purchase the land anytime during the next three months while 
learning more about possible resistance to the rezoning proposal from area residents. 
Probabilities are as follows:
 
Let h 5 high resistance to rezoning
 
  
l 5 low resistance to rezoning
 
P(h) 5 0.55   P(s1 0 h) 5 0.18   P(s2 0 h) 5 0.82
 
 P(l) 5 0.45    P(s1 0 l) 5 0.89    P(s2 0 l) 5 0.11
 
What is the optimal decision strategy if the investor uses the option period to learn 
more about the resistance from area residents before making the purchase decision?
c. If the option will cost the investor an additional $10,000, should the investor purchase 
the option? Why or why not? What is the maximum that the investor should be willing 
to pay for the option?

592 
Chapter 12 Decision Analysis
 16. Suppose that you are given a decision situation with three possible states of nature: s1, s2, 
and s3. The prior probabilities are P(s1) 5 0.2, P(s2) 5 0.5, and P(s3) 5 0.3. With sample 
information I, P(I 0 s1) 5 0.1, P(I 0 s2) 5 0.05, and P(I 0 s3) 5 0.2. Compute the revised (or 
posterior) probabilities: P(s1 0 I), P(s2 0 I), and P(s3 0 I).
 17. To save on expenses, Rona and Jerry agreed to form a carpool for traveling to and from 
work. Rona prefers to use the somewhat longer but more consistent Queen City Avenue. 
Although Jerry prefers the quicker expressway, he agreed with Rona that they should take 
Queen City Avenue if the expressway has a traffic jam. The following payoff table pro-
vides the one-way time estimate in minutes for traveling to or from work:
State of Nature
Decision Alternative
Expressway Open 
s1
Expressway Jammed 
s2
Queen City Avenue, d1
Expressway, d2
30
25
30
45
 
 Based on their experience with traffic problems, Rona and Jerry agreed on a 0.15 prob-
ability that the expressway would be jammed.
 
  
In addition, they agreed that weather seemed to affect the traffic conditions on the 
expressway. Let
 
c 5 clear
 
o 5 overcast
 
r 5 rain
 
 The following conditional probabilities apply:
 
P(c 0 s1) 5 0.8   P(o 0 s1) 5 0.2   P(r 0 s1) 5 0.0
 
P(c 0 s2) 5 0.1   P(o 0 s2) 5 0.3   P(r 0 s2) 5 0.6
a. Use Bayes’ theorem for probability revision to compute the probability of each 
weather condition and the conditional probability of the expressway being open, s1, 
or jammed, s2, given each weather condition.
b. Show the decision tree for this problem.
c. What is the optimal decision strategy, and what is the expected travel time?
 18. The gorman Manufacturing Company must decide whether to manufacture a component 
part at its Milan, Michigan, plant or purchase the component part from a supplier. The 
resulting profit is dependent upon the demand for the product. The following payoff table 
shows the projected profit (in thousands of dollars):
State of Nature
Decision Alternative
Low Demand 
s1
Medium Demand 
s2
High Demand 
s3
Manufacture, d1
Purchase, d2
220
  10
40
45
100
 70
 
 The state-of-nature probabilities are P(s1) 5 0.35, P(s2) 5 0.35, and P(s3) 5 0.30.
a. Use a decision tree to recommend a decision.
b. Use EVPI to determine whether gorman should attempt to obtain a better estimate of 
demand.
c. A test market study of the potential demand for the product is expected to report either 
a favorable (F) or unfavorable (u) condition. The relevant conditional probabilities 
are as follows:

 
Problems 
593
 
P(F 0 s1) 5 0.10 P(u 0 s1) 5 0.90
 
P(F 0 s2) 5 0.40 P(u 0 s2) 5 0.60
 
P(F 0 s3) 5 0.60 P(u 0 s3) 5 0.40
 
What is the probability that the market research report will be favorable?
d. What is gorman’s optimal decision strategy?
e. What is the expected value of the market research information?
 19. A firm has three investment alternatives. Payoffs are in thousands of dollars.
Economic Conditions
Decision Alternative
Up  
s1
Stable 
s2
Down 
s3
Investment A, d1
Investment B, d2
Investment C, d3
Probabilities
100
 75
 50
0.40
25
50
50
0.30
 0
25
50
0.30
a. Using the expected value approach, which decision is preferred?
b. For the lottery having a payoff of $100,000 with probability p and $0 with probability 
(1 2 p), two decision makers expressed the following indifference probabilities. Find 
the most preferred decision for each decision maker using the expected utility approach.
Indifference Probability (p)
Profit
Decision Maker A
Decision Maker B
$75,000
$50,000
$25,000
0.80
0.60
0.30
0.60
0.30
0.15
c. Why don’t decision makers A and B select the same decision alternative?
 20. Alexander Industries is considering purchasing an insurance policy for its new office 
building in St. Louis, Missouri. The policy has an annual cost of $10,000. If Alexander In-
dustries doesn’t purchase the insurance and minor fire damage occurs, a cost of $100,000 
is anticipated; the cost if major or total destruction occurs is $200,000. The costs, includ-
ing the state-of-nature probabilities, are as follows:
Damage
Decision Alternative
None  
s1
Minor  
s2
Major  
s3
Purchase Insurance, d1
Do Not Purchase Insurance, d2
Probabilities
10,000
0
0.96
 10,000
100,000
0.03
 10,000
200,000
0.01
a. Using the expected value approach, what decision do you recommend?
b. What lottery would you use to assess utilities? (Note: Because the data are costs, the 
best payoff is $0.)
c. Assume that you found the following indifference probabilities for the lottery defined 
in part b. What decision would you recommend?
Cost
Indifference Probability
 10,000
100,000
p 5 0.99
p 5 0.60
d. Do you favor using expected value or expected utility for this decision problem? Why?

594 
Chapter 12 Decision Analysis
 21. In a certain state lottery, a lottery ticket costs $2. In terms of the decision to purchase or 
not to purchase a lottery ticket, suppose that the following payoff table applies:
State of Nature
Decision Alternatives
Win  
s1
Lose 
s2
Purchase Lottery Ticket, d1
Do Not Purchase Lottery Ticket, d2
300,000
0
22
  0
a. A realistic estimate of the chances of winning is 1 in 250,000. Use the expected value 
approach to recommend a decision.
b. If a particular decision maker assigns an indifference probability of 0.000001 to the $0 
payoff, would this individual purchase a lottery ticket? Use expected utility to justify 
your answer.
 22. Three decision makers have assessed utilities for the following decision problem  (payoff 
in dollars):
State of Nature
Decision Alternative
s1
s2
s3
d1
d2
20
80
50
100
220
2100
 
 The indifference probabilities are as follows:
Indifference Probability (p)
Payoff
Decision Maker A
Decision Maker B
Decision Maker C
100
80
50
20
220
2100
1.00
0.95
0.90
0.70
0.50
0.00
1.00
0.70
0.60
0.45
0.25
0.00
1.00
0.90
0.75
0.60
0.40
0.00
a. Plot the utility function for money for each decision maker.
b. Classify each decision maker as a risk avoider, a risk taker, or risk neutral.
c. For the payoff of 20, what is the premium that the risk avoider will pay to avoid risk? 
What is the premium that the risk taker will pay to have the opportunity of the high 
payoff?
 23. In Problem 22, if P(s1) 5 0.25, P(s2) 5 0.50, and P(s3) 5 0.25, find a recommended deci-
sion for each of the three decision makers. (Note: For the same decision problem, different 
utilities can lead to different decisions.)
 24. Translate the following monetary payoffs into utilities for a decision maker whose utility 
function is described by an exponential function with r 5 250: 2$200, 2$100, $0, $100, 
$200, $300, $400, $500.
 25. Consider a decision maker who is comfortable with an investment decision that has a 
50 percent chance of earning $25,000 and a 50 percent chance of losing $12,500, but not 
with any larger investments that have the same relative payoffs. 
a. Write the equation for the exponential function that approximates this decision  maker’s 
utility function.
b.  Plot the exponential utility function for this decision maker for x values between 
220,000 and 35,000. Is this decision maker risk seeking, risk neutral, or risk averse?

 
Case Problem Property Purchase Strategy 
595
c.  Suppose the decision maker decides that she would actually be willing to make an 
investment that has a 50 percent chance of earning $30,000 and a 50 percent chance 
of losing $15,000. Plot the exponential function that approximates this utility function 
and compare it to the utility function from part b. Is the decision maker becoming more 
risk seeking or more risk averse?
property purchase Strategy
glenn Foreman, president of Oceanview Development Corporation, is considering sub-
mitting a bid to purchase property that will be sold by sealed bid auction at a county tax 
foreclosure. glenn’s initial judgment is to submit a bid of $5 million. Based on his experi-
ence, glenn estimates that a bid of $5 million will have a 0.2 probability of being the high-
est bid and securing the property for Oceanview. The current date is June 1. Sealed bids 
for the property must be submitted by August 15. The winning bid will be announced on 
September 1.
If Oceanview submits the highest bid and obtains the property, the firm plans to build 
and sell a complex of luxury condominiums. However, a complicating factor is that the 
property is currently zoned for single-family residences only. glenn believes that a ref-
erendum could be placed on the voting ballot in time for the November election. Passage 
of the referendum would change the zoning of the property and permit construction of the 
condominiums.
The sealed-bid procedure requires the bid to be submitted with a certified check for 
10 percent of the amount bid. If the bid is rejected, the deposit is refunded. If the bid is ac-
cepted, the deposit is the down payment for the property. However, if the bid is accepted 
and the bidder does not follow through with the purchase and meet the remainder of the 
financial obligation within six months, the deposit will be forfeited. In this case, the county 
will offer the property to the next highest bidder.
To determine whether Oceanview should submit the $5 million bid, glenn conducted 
some preliminary analysis. This preliminary work provided an assessment of 0.3 for the 
probability that the referendum for a zoning change will be approved and resulted in the 
following estimates of the costs and revenues that will be incurred if the condominiums 
are built:
Costs and Revenue Estimates
Revenue from Condominium Sales 
Costs
Property
Construction Expenses
$15,000,000
$5,000,000
$8,000,000
If Oceanview obtains the property and the zoning change is rejected in November, 
glenn believes that the best option would be for the firm not to complete the purchase of 
the property. In this case, Oceanview would forfeit the 10 percent deposit that accompanied 
the bid.
Because the likelihood that the zoning referendum will be approved is such an impor-
tant factor in the decision process, glenn suggested that the firm hire a market research 
service to conduct a survey of voters. The survey would provide a better estimate of the 
likelihood that the referendum for a zoning change would be approved. The market research 
firm that Oceanview Development has worked with in the past has agreed to do the study 
for $15,000. The results of the study will be available August 1, so that Oceanview will 
have this information before the August 15 bid deadline. The results of the survey will be 
a prediction either that the zoning change will be approved or that the zoning change will 
Case Problem

596 
Chapter 12 Decision Analysis
be rejected. After considering the record of the market research service in previous studies 
conducted for Oceanview, glenn developed the following probability estimates concerning 
the accuracy of the market research information:
 
P(a 0 s1) 5 0.9 P(N 0 s1) 5 0.1
 
P(a 0 s2) 5 0.2 P(N 0 s2) 5 0.8
where
 
 a 5 prediction of zoning change approval
 
 N 5 prediction that zoning change will not be approved
 
s1 5 the zoning change is approved by the voters
 
s2 5 the zoning change is rejected by the voters
Managerial Report
Perform an analysis of the problem facing the Oceanview Development Corporation, and 
prepare a report that summarizes your findings and recommendations. Include the follow-
ing items in your report:
 1. A decision tree that shows the logical sequence of the decision problem
 2. A recommendation regarding what Oceanview should do if the market research infor-
mation is not available
 3. A decision strategy that Oceanview should follow if the market research is 
 conducted
 4. A recommendation as to whether Oceanview should employ the market research firm, 
along with the value of the information provided by the market research firm
Include the details of your analysis as an appendix to your report.
Using Analytic Solver platform 
to  Create  Decision trees
In this appendix, we describe how Analytic Solver Platform can be used to develop a deci-
sion tree for the PDC problem presented in Section 12.3. The decision tree for the PDC 
problem is shown in Figure 12.13.
Getting Started: An Initial Decision Tree
To build a decision tree for the PDC problem using Analytic Solver Platform, follow these 
steps in a blank workbook in Excel:
Step 1. Select cell A1
Step 2. Click the ANALYTIC SOLVER PLATFORM tab on the Ribbon
Step 3. Click Decision Tree in the Tools group
Select Node, and click Add Node
Step 4. When the Decision Tree dialog box appears, verify that Decision is selected 
for Node Type, and click OK
A decision tree with one decision node and two branches (initially labeled as “Decision 1” 
and “Decision 2”) appears, as shown in Figure 12.14.
Appendix

 
Appendix Using Analytic Solver Platform to  Create  Decision Trees 
597
Adding a Branch
The PDC problem has three decision alternatives (small, medium, and large condominium 
complexes), so we must add another decision branch to the tree.
Step 1. Select cell B5
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon
Step 3. Click Decision Tree in the Tools group 
Select Branch, and click Add Branch
Step 4. When the Decision Tree dialog box appears, verify that Decision is selected 
for Node Type, and click OK
FigurE 12.13   DECISION TREE FOR THE PDC CONDOMINIUM PROJECT 
 (PAYOFFS IN MILLIONS $)
8
7
14
5
20
–9
Weak (s2)
Strong (s1)
Weak (s2)
Strong (s1)
Weak (s2)
Strong (s1)
Small (d1)
Medium (d2 )
Large (d3)
1
2
3
4
FigurE 12.14   DECISION TREE WITH ONE DECISION NODE AND TWO 
BRANCHES CREATED WITH ANALYTIC SOLVER PLATFORM
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
Decision 1
Decision 2
1
0
0
0
0
0
0
0

598 
Chapter 12 Decision Analysis
A revised tree with three decision branches now appears in the Excel worksheet as shown 
in Figure 12.15.
Naming the Decision Alternatives
The decision alternatives can be named by selecting the cells containing the labels “Deci-
sion 1”, “Decision 2”, and “New Branch”, and then entering the corresponding PDC names 
small, medium, and large (cells D2, D7 and D12). After naming the alternatives, the PDC 
tree with three decision branches appears as shown in Figure 12.16.
FigurE 12.15   DECISION TREE FOR THE PDC PROBLEM WITH THREE 
BRANCHES CREATED WITH ANALYTIC SOLVER PLATFORM
Decision 1
Decision 2
New Branch
0
0
0
0
0
0
0
0
0
0
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
1
© Cengage Learning 2015
FigurE 12.16   DECISION TREE FOR THE PDC PROBLEM WITH RENAMED 
BRANCHES CREATED WITH ANALYTIC SOLVER PLATFORM
1
0
0
0
0
0
0
0
0
0
0
A
B
C
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Small
Medium
Large
D

 
Appendix Using Analytic Solver Platform to  Create  Decision Trees 
599
Adding Chance Nodes
The chance event for the PDC problem is the demand for the condominiums, which may 
be either strong or weak. Thus, a chance node with two branches must be added at the end 
of each decision alternative branch. To add a chance node with two branches to the top 
decision alternative branch:
Step 1. Select cell F3
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 3. Select Decision Tree from the Tools group
Select Node, and click Add Node
Step 4. When the Decision Tree dialog box appears, select Event/Chance in the Node 
Type area
Click OK
The tree now appears as shown in Figure 12.17.
We next select the cells containing “Event 1” and “Event 2” (cells H2 and H7) and re-
name them strong and weak to provide the proper names for the PDC states of nature. After 
doing so, we can copy the subtree for the chance node in cell F5 to the other two decision 
branches to complete the structure of the PDC decision tree as follows:
Step 1. Select cell F5
Step 2. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 3. Click Decision Tree in the Tools group
 
 
Select Node, and click Copy Node
Step 4. Select cell F13
FigurE 12.17   DECISION TREE FOR THE PDC PROBLEM WITH AN  ADDED 
CHANCE NODE CREATED WITH ANALYTIC SOLVER 
 PLATFORM
Small
Medium
Large
1
0
0
0
0
0
0
0
0
0
0
0
0
50%
Event 1
50%
Event 2
0
0
0
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
I
J K
L

600 
Chapter 12 Decision Analysis
Step 5. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 6. Click Decision Tree in the Tools group
Select Node, and click Paste Node
This copy-and-paste procedure places a chance node at the end of the Medium deci-
sion branch. Repeating the same copy-and-paste procedure for the Large decision branch 
completes the structure of the PDC decision tree, as shown in Figure 12.18.
Inserting Probabilities and Payoffs
We now insert probabilities and payoffs into the decision tree. In Figure 12.18, we see that 
an equal probability of 0.5 is assigned automatically to each of the chance outcomes. For 
PDC, the probability of strong demand is 0.8 and the probability of weak demand is 1 minus 
the probability of strong demand, 5 1 2 0.8 5 0.2. We can enter 0.8 into cell H1 and the 
formula 51 2 H1 into cell H6. We enter the formula 5H1 into cells H11 and H21, and we 
enter the formula 5H6 into cells H16 and H26. In this way, all probabilities will be updated 
correctly if we change the value in cell H1. 
FigurE 12.18   PDC DECISION TREE CREATED WITH ANALYTIC SOLVER 
PLATFORM
Small
Medium
Large
1
0
50%
Strong
50%
Weak
50%
Strong
50%
Weak
50%
Strong
50%
Weak
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
I
J K
L

 
Appendix Using Analytic Solver Platform to  Create  Decision Trees 
601
To insert the payoffs, we enter 8 in H4, 7 in cell H9, 14 in cell H14, 5 in cell H19, 20 
in cell H24, and 29 in cell H29. Note in Figure 12.19 that the payoffs also appear in the 
right-hand margin of the decision tree. The payoffs in the right margin are computed by a 
formula that adds the payoffs on all of the branches leading to the associated terminal node. 
For the PDC problem, no payoffs are associated with the decision alternatives branches, 
so we leave the default values of zero in cells D6, D16, and D26. The PDC decision tree 
is now complete. After inserting the PDC probabilities and payoffs, the PDC decision tree 
appears as shown in Figure 12.19.
Interpreting the Result
When probabilities and payoffs are inserted, Analytic Solver Platform automatically makes 
the rollback computations necessary to determine the optimal solution. Optimal decisions 
are identified by the number in the corresponding decision node. In the PDC decision tree 
in Figure 12.19, cell B15 contains the decision node. Note that a “3” appears in this node, 
which tells us that decision alternative branch 3 provides the optimal decision. We can also 
FigurE 12.19   PDC DECISION TREE WITH BRANCH PROBABILITIES AND 
 PAYOFFS CREATED WITH ANALYTIC SOLVER PLATFORM
Small
Medium
Large
3
14.2
80%
Strong
20%
Weak
80%
Strong
20%
Weak
80%
Strong
20%
Weak
7.8
0
8
14
20
–9
5
8
14
20
–9
5
7
7
12.2
0
14.2
–9
20
5
14
7
8
0
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
I
J K
L

602 
Chapter 12 Decision Analysis
easily identify the best decision using the Highlight function in Analytic Solver Platform. 
To highlight the best decision follow these steps:
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon 
Step 2. Click Decision Tree in the Tools group
Select Highlight, and click Highlight Best
Analytic Solver Platform highlights the best decision for the PDC problem. From 
Figure 12.20, we see that decision analysis recommends that PDC construct the Large 
condominium complex. The expected value of this decision appears at the beginning of 
the tree in cell A16. Thus, we see that the optimal expected value is $14.2 million. The 
expected values of the other decision alternatives are displayed at the end of the corre-
sponding decision branch. Thus, referring to cells E6 and E16, we see that the expected 
value of the Small complex is $7.8 million and the expected value of the Medium complex 
is $12.2 million.
Using software such as Analytic Solver Platform to develop decision trees allows for 
quick and easy sensitivity analysis. We can easily analyze the impact of changing branch 
FigurE 12.20   DECISION TREE FOR THE PDC PROBLEM WITH BEST DECISION 
HIgHLIgHTED CREATED WITH ANALYTIC SOLVER PLATFORM
Small
Medium
Large
14.2
8
8
8
7
7
7.8
0
80%
Strong
20%
Weak
20
20
–9
–9
14.2
0
80%
Strong
20%
Weak
14
14
5
5
12.2
0
80%
Strong
20%
Weak
7
14
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
I
J K
20
–9
20
21
22
23
24
29
25
26
27
28
5
3

 
Appendix Using Analytic Solver Platform to  Create  Decision Trees 
603
probabilities and payoffs by simply changing these values in Excel and observing the im-
pact on the optimal decision using Analytic Solver Platform. For instance, if we want to 
examine the impact of different values of Strong demand on our decision, we can change 
the value of cell H1 and see whether this changes the optimal decision. 
A convenient way to summarize the sensitivity of a decision to a particular parameter 
is to combine the decision tree from XLMiner with a Data Table in Excel. Suppose we 
want to evaluate the impact of different probabilities of strong demand over a wide range 
of possibilities. The Excel worksheet shown in Figure 12.21 demonstrates the use of a Data 
Table to perform this sensitivity analysis. To create this Data Table, we follow these steps 
once we have created the decision tree for this problem in XLMiner:
Step 1. Enter the values 0.0, 0.1, 0.2, etc. into cells M4 to M14, as shown in Fig-
ure 12.20, to represent the different scenarios for the probability of Strong 
demand
Step 2. Enter 5A16 into cell N3 to keep track of the optimal expected value in each 
scenario
FigurE 12.21   DECISION TREE AND DATA TABLE ILLUSTRATINg SENSITIVITY ANALYSIS FOR THE 
PDC PROBLEM CREATED WITH ANALYTIC SOLVER PLATFORM
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
I
J K
L
M
N
O
Small
Medium
Large
3
14.2
80%
Strong
20%
Weak
80%
Strong
20%
Weak
80%
Strong
20%
Weak
7.8
0
8
0.0
Large
Small
Small
Small
Medium
Medium
Medium
Medium
Medium
Large
Large
Large
14.2
7
7.1
7.2
7.7
8.6
9.5
10.4
11.3
14.2
17.1
20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
8
14
14
20
20
–9
–9
5
5
7
7
12.2
0
14.2
–9
20
5
14
7
8
0
Probability Strong
Demand
Expected
Value
Best
Decision 
Data Table

604 
Chapter 12 Decision Analysis
Step 3. Enter the formula 5CHOOSE(B15,“Small”,“Medium”,“Large”) into cell O3. 
This will return the best decision in each scenario
Step 4. Select cells M3:O14
Step 5. Click the DATA tab in the Ribbon
Step 6. Click What-If Analysis from the Data Tools group
Select Data Table…
Step 7. When the Data Table dialog box opens, enter 5H1 into the Column input 
cell: box
Click OK
Figure 12.21 shows the completed decision tree and Data Table. From Figure 12.21, we see 
that the best decision is to construct the Small complex if the probability of strong demand 
is 0, 0.1, or 0.2, the Medium complex if the probability is any value shown between 0.3 and 
0.7, and the Large complex if the probability of strong demand is 0.8 or greater. The Data 
Table also provides the expected values for these decisions in each scenario. Such sensitiv-
ity analysis can be greatly beneficial in demonstrating which values should be clarified, if 
possible, by procuring additional information.
Using the Exponential Utility Function  
in Analytic Solver Platform
By default, the decision trees created in Analytic Solver Platform use the expected value 
approach for calculating the best decisions. However, we can easily change this setting so 
that Analytic Solver Platform will use an exponential utility function to calculate utilities 
and determine the best decisions. To do this, we will modify the settings using the Solver 
Options and Model Specifications task pane of Analytic Solver Platform. To change the 
settings in a decision tree to use exponential utility functions, we use the following steps.
Step 1. Click the ANALYTIC SOLVER PLATFORM tab in the Ribbon to reveal 
the Solver Options and Model Specifications task pane
Step 2. In the Solver Options and Model Specifications task pane, click the Model 
tab
Select Decision Tree in the Solver Options and Model Specifications 
task pane (Figure 12.22)
Step 3. In the Decision Tree area at the bottom of the Solver Options and Model 
Specifications task pane, click Expected Values next to Certainty  Equivalent
 
 
Change this value to Exponential Utility Function
Step 4. We also must provide the risk tolerance value (r in equation 12.7) to be used 
in the exponential utility function. In the Decision Tree area at the bottom of 
the Solver Options and Model Specifications task pane, change the value 
next to Risk  Tolerance to 1
Figure 12.22 shows the completed decision tree using the exponential utility function. 
Step 4 indicates that we are using a value of $1 million as the r value in equation (12.7). 
We know that the units here are in millions of dollars because those are the units used by 
the values in our decision tree. Recall that a small risk tolerance (r value), relative to the 
payoff values in the decision tree, indicates that the decision maker is very risk averse. 
Once we make this change in Analytic Solver Platform, the decision tree calculations are 
done using utilities based on the exponential function rather than using the expected value 
method. 
the excel function 
choose chooses a value 
from a list of possibilities 
based on the index in the 
referenced cell. here, the 
choose function chooses 
“small” in cell o3 if the 
value in cell B15 is 1; it 
enters “medium” if the 
value in cell B15 is 2; it 
enters “large” if the value 
in cell B15 is 3.
By entering 5h1 in step 7, 
we tell excel to substitute 
the values of 0, 0.1, 0.2, 
and so on for the prob-
ability of strong demand 
and then return the related 
outputs. the use of data 
tables in excel is covered 
in more detail in chapter 7.
If the Solver Options and 
Model Specifications task 
pane is not visible, it can 
be activated by clicking the 
Model button in the Model 
group under the ANALYT-
IC SOLVER PLATFORM 
tab in the ribbon.

 
Appendix Using Analytic Solver Platform to  Create  Decision Trees 
605
FigurE 12.22   DECISION TREE IN ANALYTIC SOLVER PLATFORM USINg AN EXPONENTIAL 
 UTILITY FUNCTION WITH r 5 $1 MILLION
Small
Medium
Large
7.7046
0.9995
8
7.7046
7.0000
7.0653
7.1352
7.2103
7.2915
7.3799
7.4769
7.5843
7.7046
7.8414
20.0000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Small
Large
Data Table
Probability
Strong Demand
Expected
Value
Best
Decision
8
0.9997
0.9991
8
7
7
7.7046
0.9995
0
20
1.0000
20
–9
–8102.0839
–9
–7.3906
–1619.6168
0
14
1.0000
14
5
0.9933
5
6.6089
0.9987
0
80%
Strong
20%
Weak
80%
Strong
20%
Weak
80%
Strong
20%
Weak
7
14
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
I
M
N
O
L
J K
5
20
–9
20
21
22
23
24
29
25
26
27
28
30
1
In the Data Table in Figure 12.22, we see that the decision maker often prefers to build 
the Small complex to limit downside risk due to the decision maker being very risk averse. 
However, if we change the risk tolerance (r) to be $9 million, this means that the decision 
maker is less risk averse. Figure 12.23 shows the decision tree with an exponential utility 
function and r 5 $9 million; here we see that the decision maker is more likely to choose 
the Medium complex for many different probabilities of Strong demand as compared to the 
more risk averse decision maker shown in Figure 12.22. Figure 12.23 reflects a decision 
maker who is less risk averse and more willing to accept decisions that could have higher 
payoffs but that also have higher likelihoods of worse payoffs.
The complete decision tree and data table for the PDC problem is contained in the file 
Pdcmodel.
file
WEB
PDCModel

606 
Chapter 12 Decision Analysis
FigurE 12.23   DECISION TREE IN ANALYTIC SOLVER PLATFORM USINg AN EXPONENTIAL 
 UTILITY FUNCTION WITH r 5 $9 MILLION
Small
Medium
Large
2
11.3414
0.7164
8
11.3414
7.0000
7.0951
7.1913
7.2885
7.6234
8.4190
9.2918
10.2584
11.3414
12.5729
20.0000
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Medium
Small
Small
Small
Small
Medium
Medium
Medium
Medium
Medium
Medium
Large
Data Table
Probability
Strong Demand
Expected
Value
Best
Decision
8
8
7
7
7.7909
0.5792
0
20
20
–9
–1.7183
0.8916
0.4262
0.7889
0.5406
0.5889
–9
4.1533
0.3696
0
14
14
5
5
11.3414
0.7164
0
80%
Strong
20%
Weak
80%
Strong
20%
Weak
80%
Strong
20%
Weak
7
14
A
B
C
D
E
F G
H
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
I
M
N
O
L
J K
5
20
–9
20
21
22
23
24
29
25
26
27
28
30

APPENDIXES
APPENDIX A
Basics of Excel
APPENDIX B
Data Management and Microsoft Access


Appendix A  Basics of Excel
CONTENTS
A.1 USING MICROSOFT EXCEL
Basic Spreadsheet Workbook Operations
Creating, Saving, and Opening Files in Excel
A.2 SPREADSHEET BASICS
Cells, References, and Formulas in Excel
Finding the Right Excel Function
Colon Notation
Inserting a Function into a Worksheet Cell
Using Relative Versus Absolute Cell References
Using Microsoft Excel
When using Excel for modeling, the data and the model are displayed in a workbook, each 
of which contains a series of worksheets. Figure A.1 shows the layout of a blank workbook 
created in Excel 2013. The workbook is named Book1 and by default contains a worksheet 
named Sheet1. 
The wide bar located across the top of the workbook is referred to as the Ribbon. Tabs, 
located at the top of the Ribbon, contain groups of related commands. By default, nine tabs 
are included on the Ribbon in Excel: FILE, HOME, INSERT, PAGE LAYOUT, FOR-
MULAS, DATA, REVIEW, VIEW, and ADD-INS. Loading additional packages (such as 
Analytic Solver Platform or Acrobat) may create additional tabs. Each tab contains several 
groups of related commands. The FILE tab is used to Open, Save, and Print files as well as 
A.1
Depending on the settings 
for your particular instal-
lation of Excel, you may 
see additional worksheets 
labeled Sheet2, Sheet3, and 
so on.
Figure A.1  BLANk WORkBOOk IN EXCEL
Name of
Workbook
Cell A1 is
selected
Worksheet
tab
Ribbon

610 
Appendix A Basics of Excel
to change the Options being used by Excel and to load Add-ins. Note that the HOME tab is 
selected when a workbook is opened. Figure A.2 displays the seven groups located in the 
HOME tab: Clipboard, Font, Alignment, Number, Styles, Cells, and Editing. Commands 
are arranged within each group. 
For example, to change selected text to boldface, click the HOME tab and click the 
Bold button  in the Font group. The other tabs in the Ribbon are used to modify data in 
your spreadsheet or to perform analysis.
Figure A.3 illustrates the location of the FILE tab, the Quick Access Toolbar, and the 
Formula Bar. The Quick Access Toolbar allows you to quickly access commonly used 
workbook functions. 
Keyboard shortcut: press-
ing Ctrl-B will change 
the font of the text in the 
selected cell to bold. We in-
clude a full list of keyboard 
shortcuts for Excel at the 
end of this appendix.
Figure A.2   GROUPS ON THE HOME TAB IN THE RIBBON OF AN EXCEL WORkBOOk
HOME
tab
Clipboard
group
Alignment
group
Styles
group
Editing
group
Font
group
Number
group
Cells
group
Figure A.3   FILE TAB, QUICk ACCESS TOOLBAR, AND FORMULA BAR OF AN EXCEL WORkBOOk
FILE
tab
Name box
Insert Function
button
Formula
bar
Formula
box
Quick Access
Toolbar

 
A.1 Using Microsoft Excel 
611
For instance, the Quick Access Toolbar shown in Figure A.3 includes a Save button 
 
that can be used to save files without having to first click the FILE tab. To add or remove 
features on the Quick Access Toolbar, click the Customize Quick Access Toolbar button 
 on the Quick Access Toolbar.
The Formula Bar contains a Name box, the Insert Function button 
, and a Formula 
box. In Figure A.3, “A1” appears in the Name box because cell A1 is selected. You can 
select any other cell in the worksheet by using the mouse to move the cursor to another 
cell and clicking or by typing the new cell location in the name box and pressing the Enter 
key. The Formula box is used to display the formula in the currently selected cell. For in-
stance, if you had entered =A1+A2 into cell A3, whenever you select cell A3, the formula 
=A1+A2 will be shown in the Formula box. This feature makes it very easy to see and edit 
a formula in a cell. The Insert Function button allows you to quickly access all of the func-
tions available in Excel. Later, we show how to find and use a particular function with the 
Insert Function button.
Basic Spreadsheet Workbook Operations
To change the name of the current worksheet, we take the following steps:
Step 1. Right-click on the worksheet tab named Sheet1
Step 2. Select the Rename option
Step 3. Enter Nowlin to rename the worksheet and press Enter
You can create a copy of the newly renamed Nowlin worksheet by following these steps: 
Step 1. Right-click the worksheet tab named Nowlin
Step 2. Select the Move or Copy… option
Step 3. When the Move or Copy dialog box appears, select the checkbox for Create 
a Copy, and click OK
The name of the copied worksheet will appear as “Nowlin (2)”. You can then rename 
it, if desired, by following the steps outlined previously. Worksheets can also be moved to 
other workbooks or to a different position in the current workbook by using the Move or 
Copy option.
To create additional worksheets follow these steps:
Step 1. Right-click on the tab of any existing worksheet
Step 2. Select Insert…
Step 3. When the Insert dialog box appears, select Worksheet from the General 
area, and click OK
Worksheets can be deleted by right-clicking the worksheet tab and choosing Delete. 
After clicking Delete, a window may appear, warning you that any data appearing in the 
worksheet will be lost. Click Delete to confirm that you do want to delete the worksheet. 
Creating, Saving, and Opening Files in Excel
As an illustration of manually entering, saving, and opening a file, we will use the Nowlin 
Plastics make-versus-buy model from Chapter 7. The objective is to determine whether 
Nowlin should manufacture or outsource production for its Viper product next year. Now-
lin must pay a fixed cost of $234,000 and a variable cost per unit of $2 to manufacture the 
product. Nowlin can outsource production for $3.50 per unit. 
We begin by assuming that Excel is open and a blank worksheet is displayed. The 
Nowlin data can now be entered manually by simply typing the manufacturing fixed cost 
of $234,000, the variable cost of $2, and the outsourcing cost of $3.50 into the worksheet.
New worksheets can also 
be created in Excel 2013 
using the insert worksheet 
button 
 at the bottom of 
the screen.

612 
Appendix A Basics of Excel
We will place the data for the Nowlin example in the top portion of Sheet1 of the new 
workbook. First, we enter the label Nowlin Plastics in cell A1. To identify each of the three 
data values, we enter the label Manufacturing Fixed Cost in cell A4, the label Manufac-
turing Variable Cost per Unit in cell A5, and the label Outsourcing Cost per Unit in cell 
A7. Next, we enter the actual data into the corresponding cells in column B: the value of 
$234,000 in cell B4; the value of $2 in cell B5; and the value of $3.50 in cell B7. Figure A.4 
shows a portion of the worksheet we have just developed.
Before we begin the development of the model portion of the worksheet, we recom-
mend that you first save the current file; this will prevent you from having to reenter the 
data in case something happens that causes Excel to close. To save the workbook using the 
filename Nowlin, we perform the following steps:
Step 1. Click the FILE tab on the Ribbon
Step 2. Click Save in the list of options
Step 3. Select Computer under Save As, and click Browse 
Step 4. When the Save As dialog box appears
Select the location where you want to save the file
Enter the file name Nowlin in the File name: box
Click Save
Excel’s Save command is designed to save the file as an Excel workbook. As you work 
with and build models in Excel, you should follow the practice of periodically saving the 
file so that you will not lose any work. After you have saved your file for the first time, 
the Save command will overwrite the existing version of the file, and you will not have to 
perform Steps 3 and 4.
Sometimes you may want to create a copy of an existing file. For instance, suppose 
you change one or more of the data values and would like to save the modified file using 
the filename NowlinMod. The following steps show how to save the modified workbook 
using filename NowlinMod:
Step 1. Click the FILE tab in the Ribbon
Step 2. Click Save As in the list of options
Step 3. Select Computer under Save As, and click Browse 
Step 4. When the Save As dialog box appears:
Select the location where you want to save the file
Type the file name NowlinMod in the File name: box
Click Save
Once the NowlinMod workbook has been saved, you can continue to work with the 
file to perform whatever type of analysis is appropriate. When you are finished working 
Step 3 is only necessary 
for Excel 2013. In previous 
versions of Excel, you may 
skip to Step 4.
Keyboard shortcut: To save 
the file, press Ctrl-S.
Step 3 is only necessary 
in Excel 2013. In previous 
versions of  Excel, you may 
skip to Step 4.
Figure A.4  NOWLIN PLASTICS DATA
A
C
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
Outsourcing Cost per Unit
$234,000.00
$3.50
$2.00
1
2
3
4
5
6
7
8
B

 
A.2 Spreadsheet Basics 
613
with the file, simply click the close window button 
 located at the top right-hand corner 
of the Ribbon.
You can easily access a previously saved file at another point in time. For example, the 
following steps show how to open the previously saved Nowlin workbook:
Step 1. Click the FILE tab in the Ribbon
Step 2. Click Open in the list of options
Step 3. Select Computer under Open and click Browse
Step 4. When the Open dialog box appears:
Find the location where you previously saved the Nowlin file
Click on the filename Nowlin so that it appears in the File name: box
Click Open
Spreadsheet Basics
Cells, References, and Formulas in Excel
We begin by assuming that the Nowlin workbook is open again and that we would like to 
develop a model that can be used to compute the manufacturing and outsourcing cost given 
a production volume. We will use the bottom portion of the worksheet shown in Figure A.4 
to develop the model. The model will contain formulas that refer to the location of the data 
cells in the upper section of the worksheet. By putting the location of the data cells in the 
formula, we will build a model that can be easily updated with new data. 
We enter the label Model into cell A10 to provide a visual reminder that the bottom 
portion of this worksheet will contain the model. In cell A11, we enter the label Quantity. 
Next, we enter the labels Total Cost to Produce in cell A13, Total Cost to Outsource in cell 
A15, and Savings due to Outsourcing in cell A17. 
In cell B11 we enter 10000 to represent the quantity produced/outsourced by Nowlin 
Plastics. We will now enter formulas in cells B13, B15, and B17 that use the quantity in 
cell B11 to compute the values for production cost, outsourcing cost, and savings from 
outsourcing. The total cost to produce is the sum of the manufacturing fixed cost (cell B4) 
and the manufacturing variable cost. The manufacturing variable cost is the product of the 
production volume (cell B11) and the variable cost per unit (cell B5). Thus, the formula 
for total variable cost is B11*B5; to compute the value of total cost, we enter the formula 
=B4+B11*B5 in cell B13. Next, total cost to outsource is the product of the outsourcing 
cost per unit (cell B7) and the quantity (cell B11); this is computed by entering the formula 
=B7*B11 in cell B15. Finally, the savings due to outsourcing is computed by subtracting 
the cost of outsourcing (cell B15) from the production cost (cell B13). Thus, in cell B17 we 
enter the formula =B13-B15. Figure A5 shows the Excel worksheet values and formulas 
used for these calculations.
We can now compute the savings due to outsourcing by entering a value for the quantity 
to be manufactured or outsourced in cell B11. Figure A.5 shows the results after entering a 
value of 10,000 in cell B11. We see that a quantity of 10,000 units results in a production 
cost of $254,000 and outsourcing cost of 35,000. Thus, the savings due to outsourcing is 
$219,000.
Finding the Right Excel Function
Excel provides a variety of built-in formulas or functions for developing mathematical 
models. If we know which function is needed and how to use it, we can simply enter the 
function into the appropriate worksheet cell. However, if we are not sure which functions 
Step 3 is only necessary  
in Excel 2013. The file-
name Nowlin may also 
appear under the Recent 
Workbooks list in Excel to 
allow you to open it directly 
without navigating to where 
you saved the file.
A.2
To display all formulas in 
the cells of a worksheet, 
hold down the Ctrl key 
and then press the , key 
(usually located above the 
Tab key).

614 
Appendix A Basics of Excel
are available to accomplish a task or are not sure how to use a particular function, Excel 
can provide assistance.
To identify the functions available in Excel click the Insert Function button 
 on the 
formula bar; this opens the Insert Function dialog box shown in Figure A.6. The Search 
for a function: box at the top of the dialog box enables us to type a brief description for 
what we want to do. After doing so and clicking Go, Excel will search for and display, in 
the Select a function box, the functions that may accomplish our task. In many situations, 
however, we may want to browse through an entire category of functions to see what is 
available. For this task, the Or select a category: box is helpful. It contains a drop-down list 
of several categories of functions provided by Excel. Figure A.6 shows that we selected the 
Math & Trig category. As a result, Excel’s Math & Trig functions appear in alphabetical 
Figure A.5  NOWLIN PLASTICS DATA AND MODEL
A
C
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
Outsourcing Cost per Unit
234000
3.5
=B4+B11*B5
=B7*B11
=B13-B15
2
1
2
3
4
5
6
7
Model
Quantity
Total Cost to Produce
Total Cost to Outsource
9
10
11
12
13
14
Savings due to Outsourcing
15
16
17
18
8
B
10000
A
C
Nowlin Plastics
Parameters
Manufacturing Fixed Cost
Manufacturing Variable Cost per Unit
Outsourcing Cost per Unit
1
2
3
4
5
6
7
Model
Quantity
Total Cost to Produce
Total Cost to Outsource
9
10
11
12
13
14
Savings due to Outsourcing
15
16
17
18
8
B
$234,000.00
$3.50
$254,000.00
$35,000.00
$219,000.00
$2.00
10,000
file
WEB
Nowlin

 
A.2 Spreadsheet Basics 
615
order in the Select a function: area. We see the ABS function listed first, followed by the 
ACOS function, and so on.
Colon Notation
Although many functions, such as the ABS function, have a single argument, some Ex-
cel functions depend on arrays. Colon notation provides an efficient way to convey ar-
rays and matrices of cells to functions. The colon notation may be described as follows: 
B1:B5 means cell B1 “through” cell B5, namely the array of values stored in the locations 
(B1,B2,B3,B4,B5). Consider for example the following function =SUM(B1:B5). The sum 
function adds up the elements contained in the function’s argument. Hence, =SUM(B1:B5) 
evaluates the following formula:
 
=B1+B2+B3+B4+B5.
To illustrate the use of colon notation, we will consider the financial data for Nowlin 
Plastics contained in the file NowlinFinancial and shown in Figure A.7. Column A contains 
the name of each month, column B contains the revenue for each month, and column C 
contains cost data. In row 15, we compute the total revenues and costs for the year. To do 
this we first enter Total: in cell A15. Next, we enter the formula =SUM(B2:B13) in cell 
B15 and =SUM(C2:C13) in cell C15. This shows that the total revenues for the company 
are $39,319,000 and the total costs are $36,549,000.
Inserting a Function into a Worksheet Cell
Continuing with the Nowlin financial data, we will now show how to use the Insert Func-
tion and Function Arguments dialog boxes to select a function, develop its arguments, and 
The ABS function calcu-
lates the absolute value 
of a number. The ACOS 
function calculates the 
 arccosine of a number.
file
WEB
NowlinFinancial
Figure A.6  INSERT FUNCTION DIALOG BOX

616 
Appendix A Basics of Excel
insert the function into a worksheet cell. We wish to calculate the average monthly revenue 
and cost at Nowlin. To do so, we take the following steps.
Step 1. Select cell B17 in the file NowlinFinancial
Step 2. Click the Insert Function button 
.
Select Statistical in the Or select a category: box
Select AVERAGE from the Select a function: options
Step 3. When the Function Arguments dialog box appears:
Enter B2:B13 in the Number1 box
Click OK
Step 4. Repeat steps 1 through 3 for the cost data in column C
Figure A.7 shows that the average monthly revenue is $3,276,583 and the average monthly 
cost is $3 045 750
The Function Arguments 
dialog box contains a link 
Help on this function in 
case you need additional 
guidance on the use of 
a particular function in 
Excel.
Figure A.7  NOWLIN PLASTICS MONTHLY REVENUES AND COSTS
A
C
Month
February
January
March
April
June
July
August
May
=SUM(B2:B13)
=AVERAGE(B2:B13)
1
2
3
4
5
6
7
September
October
November
December
Total:
9
10
11
12
13
14
Average:
15
16
17
8
B
Revenue
2873000
3459000
3195000
3436000
2845000
2925000
3682000
3410000
3782000
3548000
3028000
3136000
Cost
2640000
3250000
3021000
3240000
2803000
=SUM(C2:C13)
=AVERAGE(C2:C13)
3015000
3150000
3185000
3237000
3196000
2815000
2997000
A
C
Month
February
January
March
April
June
July
August
May
1
2
3
4
5
6
7
September
October
November
December
Total:
9
10
11
12
13
14
Average:
15
16
17
8
B
Revenue
$39,319,000
 $  3,276,583
 $  2,873,000
 $  3,459,000
 $  3,195,000
 $  3,436,000
 $  2,845,000
 $  2,925,000
 $  3,682,000
 $  3,410,000
 $  3,782,000
 $  3,548,000
 $  3,028,000
 $  3,136,000
Cost
 $  2,640,000
 $  3,250,000
 $  3,021,000
 $  3,240,000
 $  2,803,000
$36,549,000
 $  3,045,750
 $  3,015,000
 $  3,150,000
 $  3,185,000
 $  3,237,000
 $  3,196,000
 $  2,815,000
 $  2,997,000
file
WEB
NowlinFinancial

 
A.2 Spreadsheet Basics 
617
Using Relative Versus Absolute Cell References
One of the most powerful abilities of spreadsheet software such as Excel is the ability to use rela-
tive references in formulas. Use of a relative reference allows the user enter a formula once into 
Excel and then copy and paste that formula to other places so that the formula will update with 
the correct data without having to retype the formula. We will demonstrate the use of relative ref-
erences in Excel by calculating the monthly profit at Nowlin Plastics using the following steps:
Step 1. Enter the label Profit in cell D1
Step 2. Enter the formula =B2-C2 in cell D2
Step 3. Copy the formula from cell D2 by selecting cell D2 and clicking Copy from 
the Clipboard group of the HOME tab
Step 4. Select cells D3:D13
Step 5. Paste the formula from cell D2 by clicking Paste from the Clipboard group 
of the HOME tab
The result of these steps is shown in Figure A.8 where we have calculated the profit for 
each month. Note that even though the only formula we entered was =B2-C2 in cell D2, 
file
WEB
NowlinFinancial
After completing Step 2, a 
shortcut to copying the for-
mula to the range D3:D13 
is to place the pointer in the 
bottom-right corner of cell 
D2 and then double-click.
Keyboard shortcut: You 
can copy in Excel by 
 pressing Ctrl-C. You can 
paste in Excel by pressing 
Ctrl-V.
Figure A.8  NOWLIN PLASTICS PROFIT CALCULATION
A
C
Month
February
January
March
April
June
July
August
May
=SUM(B2:B13)
=AVERAGE(B2:B13)
1
2
3
4
5
6
7
September
October
November
December
Total:
9
10
11
12
13
14
Average:
15
16
17
8
B
D
Revenue
2873000
3459000
3195000
3436000
2845000
2925000
3682000
3410000
3782000
3548000
3028000
3136000
Cost
2640000
3250000
3021000
3240000
2803000
=SUM(C2:C13)
=AVERAGE(C2:C13)
3015000
3150000
3185000
3237000
3196000
2815000
2997000
Proﬁt
=B3-C3
=B2-C2
=B4-C4
=B7-C7
=B13-C13
=B5-C5
=B6-C6
=B8-C8
=B9-C9
=B10-C10
=B12-C12
=B11-C11
A
C
Month
February
January
March
April
June
July
August
May
1
2
3
4
5
6
7
September
October
November
December
Total:
9
10
11
12
13
14
Average:
15
16
17
8
B
D
Revenue
$39,319,000
 $  3,276,583
 $  2,873,000
 $  3,459,000
 $  3,195,000
 $  3,436,000
 $  2,845,000
 $  2,925,000
 $  3,682,000
 $  3,410,000
 $  3,782,000
 $  3,548,000
 $  3,028,000
 $  3,136,000
Cost
Proﬁt
 $  2,640,000
 $  3,250,000
 $  3,021,000
 $  3,240,000
 $  2,803,000
$36,549,000
 $  3,045,750
 $  3,015,000
 $  3,150,000
 $  3,185,000
 $  3,237,000
 $  3,196,000
 $  2,815,000
 $  2,997,000
 $  233,000
 $  209,000
 $  174,000
 $  196,000
 $    42,000
 $ (90,000)
 $  532,000
 $  225,000
 $  545,000
 $  352,000
 $  213,000
 $  139,000

618 
Appendix A Basics of Excel
the formulas in cells D3 through D13 have been updated correctly to calculate the profit of 
each month using that month’s revenue and cost. This illustrates the use of relative refer-
ences in Excel.
In some situations, however, we do not want to use relative referencing in formulas. The 
alternative is to use an absolute reference, which we indicate to Excel by putting “$” before 
the row and/or column locations of the cell location. An absolute reference does not update 
to a new cell reference when the formula is copied to another location. We illustrate the use 
of an absolute reference by continuing to use the Nowlin financial data. Nowlin calculates 
an after-tax profit each month by multiplying its actual monthly profit by one minus its tax 
rate, which is currently estimated to be 30 percent. Cell B19 in Figure A.9 contains this tax 
rate. In column E, we calculate the after-tax profit for Nowlin in each month by using the 
following steps:
Step 1. Enter the label After-Tax Profit in cell E1
Step 2. Enter the formula =D2*(1-$B$19) in cell E2
Step 3. Copy the formula from cell E2 by selecting cell E2 and clicking Copy from 
the Clipboard group of the HOME tab
Step 4. Select cells E3:E13
Step 5. Paste the formula from cell E2 by clicking Paste from the Clipboard group of 
the HOME tab
Figure A.9 shows the after-tax profit in each month. By using $B$19 in the formula in 
cell E2, this forces Excel to always refer to cell $B$19, even if we copy and paste this 
formula somewhere else in our worksheet. Notice that D2 continues to be a relative ref-
erence and is updated to D3, D4, and so on when we copy this formula to cells E3, E4, 
etc., respectively.
Summary
In this appendix we have reviewed the basics of using Microsoft Excel. We have discussed 
the basic layout of Excel, file creation, saving, and editing as well as how to reference cells, 
use formulas, and use the copy-and-paste functions in an Excel worksheet. We have illus-
trated how to find and enter Excel functions and described the difference between relative 
and absolute cell references. In Chapter 7, we give a detailed treatment of how to create 
more advanced business analytics models in Excel. We conclude this appendix with a table 
(Table A.1) of commonly used keyboard shortcut keys in Excel. keyboard shortcut keys 
can save considerable time when entering data into Excel; most experienced Excel users 
prefer keyboard shortcuts to using drop-down menus in Excel.
Glossary
Workbook An Excel file that contains a series of worksheets.
Worksheet A single page in Excel containing a matrix of cells defined by their column and 
row locations in an Excel workbook.
Colon notation Notation used in an Excel worksheet to denote “through.” For example, 
=SUM(B1:B4) implies sum cells B1 through B4, or equivalently, B1+B2+B3+B4.
Relative reference The reference to a cell location in an Excel worksheet formula or func-
tion. This reference updates according to its relative position when copied.
Absolute reference The reference to a cell location in an Excel worksheet formula or func-
tion. This reference does not update according to its relative position when copied.
In some cases, one may 
want Excel to use relative 
referencing for either the 
column or row location and 
absolute referencing for 
the other. For instance, to 
force Excel to always refer 
to column A but use relative 
referencing for the row, 
one would enter =$A1 into, 
say, cell B1. If this formula 
is copied into cell C3, the 
updated formula would be 
=$A3 (whereas it would be 
updated to =B3 if relative 
referencing was used for 
both the column and row 
location).

 
Glossary 
619
Figure A.9   NOWLIN PLASTICS AFTER-TAX PROFIT CALCULATION ILLUSTRATING 
RELATIVE VERSUS ABSOLUTE REFERENCES
A
C
Month
February
January
March
April
June
July
August
May
=SUM(B2:B13)
=AVERAGE(B2:B13)
1
2
3
4
5
6
7
September
October
November
December
Total:
9
10
11
12
13
14
Average:
15
16
17
8
B
D
Revenue
2873000
3459000
3195000
3436000
2845000
2925000
3682000
3410000
3782000
3548000
3028000
3136000
Cost
=SUM(C2:C13)
=AVERAGE(C2:C13)
2640000
3250000
3021000
3240000
2803000
3015000
3150000
3185000
3237000
3196000
2815000
2997000
E
=D3*(1-$B$19)
=D2*(1-$B$19)
=D4*(1-$B$19)
=D7*(1-$B$19)
=D13*(1-$B$19)
=D5*(1-$B$19)
=D6*(1-$B$19)
=D8*(1-$B$19)
=D9*(1-$B$19)
=D10*(1-$B$19)
=D12*(1-$B$19)
=D11*(1-$B$19)
Tax Rate:
18
19
0.3
=B3-C3
=B2-C2
=B4-C4
=B7-C7
=B13-C13
=B5-C5
=B6-C6
=B8-C8
=B9-C9
=B10-C10
=B12-C12
=B11-C11
A
C
Month
February
January
March
April
June
July
August
May
1
2
3
4
5
6
7
September
October
November
December
Total:
9
10
11
12
13
14
Average:
15
16
Tax Rate:
30%
18
19
17
8
B
D
E
Revenue
$39,319,000
 $  3,276,583
 $  2,873,000
 $  3,459,000
 $  3,195,000
 $  3,436,000
 $  2,845,000
 $  2,925,000
 $  3,682,000
 $  3,410,000
 $  3,782,000
 $  3,548,000
 $  3,028,000
 $  3,136,000
Cost
 $  2,640,000
 $  3,250,000
 $  3,021,000
 $  3,240,000
 $  2,803,000
$36,549,000
 $  3,045,750
 $  3,015,000
 $  3,150,000
 $  3,185,000
 $  3,237,000
 $  3,196,000
 $  2,815,000
 $  2,997,000
 $  233,000
 $  209,000
 $  174,000
 $  196,000
 $    42,000
 $ (90,000)
 $  532,000
 $  225,000
 $  545,000
 $  352,000
 $  213,000
 $  139,000
 $  163,100
$  146,300
 $  121,800
 $  137,200
 $    29,400
 $ (63,000)
 $  372,400
 $  157,500
 $  381,500
 $  246,400
 $  149,100
 $    97,300
Proﬁt
After-Tax Proﬁt
Proﬁt
After-Tax Proﬁt
file
WEB
NowlinFinancialComplete

620 
Appendix A Basics of Excel
TAble A.1  kEYBOARD SHORTCUT kEYS IN EXCEL
Keyboard Shortcut Key
Task Description
Ctrl-S
Ctrl-C
Ctrl-V
Ctrl-F 
Ctrl-P
Ctrl-A
Ctrl-B
Ctrl-I
Ctrl-, (usually located 
above the Tab key)
Ctrl-v (down arrow key)
Ctrl-y (up arrow key)
Ctrl-u (right arrow key)
Ctrl-z (left arrow key)
Ctrl-Home
Ctrl-End 
Shift-v
Shift-y
Shift-u
Shift-z
Ctrl-Shift-v 
Ctrl-Shift-y 
Ctrl-Shift-u 
Ctrl-Shift-z 
Ctrl-Shift-Home 
Ctrl-Shift-End 
Ctrl-Spacebar
Shift-Spacebar
Save
Copy
Paste
Find (can be used to find text both within a cell and 
within a formula in Excel)
Print
Selects all cells in the current data region
Changes the selected text to/from bold font
Changes the selected text to/from italic font
Toggles between displaying values and formulas in the 
Worksheet
Moves to the bottom-most cell of the current data region
Moves to the top-most cell of the current data region
Moves to the right-most cell of the current data region
Moves to the left-most cell of the current data region
Moves to the top-right-most cell of the current data region
Moves to the bottom-left-most cell of the current data 
region
Selects the current cell and the cell below
Selects the current cell and the cell above
Selects the current cell and the cell to the right
Selects the current cell and the cell to the left
Selects all cells from the current cell to the bottom-most 
cell of the data region
Selects all cells from the current cell to the top-most cell 
of the data region
Selects all cells from the current cell to the right-most cell 
in the data region
Selects all cells from the current cell to the left-most cell 
in the data region
Selects all cells from the current cell to the top-left-most 
cell in the data region
Selects all cells from the current cell to the bottom-right-
most cell in the data region
Selects the entire current column
Selects the entire current row
A data region refers to all 
adjacent cells that contain 
data in an Excel worksheet.
Holding down the Ctrl key 
and clicking on multiple 
cells allows you to select 
multiple nonadjacent cells. 
Holding down the Shift key 
and clicking on two nonad-
jacent cells selects all cells 
between the two cells.

Appendix B   Data Management 
and Microsoft Access
CONTENTS
B.1 DATABASE BASICS AND MICROSOFT ACCESS
Considerations When Designing a Database
Creating a Database in Access
B.2 CREATING RELATIONSHIPS BETWEEN TABLES IN ACCESS
B.3 SORTING AND FILTERING RECORDS
B.4 QUERIES
Select Queries
Action Queries
Crosstab Queries
B.5 SAVING DATA TO EXTERNAL FILES
Data is the cornerstone of analytics; without accurate and timely data on relevant aspects 
of a business or organization, analytic techniques are useless, and the resulting analyses are 
meaningless (or worse yet, potentially misleading). The data used by organizations to make 
decisions are not static, but rather are dynamic and constantly changing, usually at a rapid 
pace. Every change or addition to a database represents a new opportunity to introduce er-
rors into the data, so it is important to be capable of searching for duplicate entries or entries 
with errors. Furthermore, related data may be stored in different locations to simplify data 
entry or increase security. Because an analysis frequently requires information from several 
sets of data, an analyst must be able to efficiently combine information from multiple data 
sets in a logical manner. In this appendix, we will review tools in Microsoft Access® that 
can be used for these purposes.
Database Basics and Microsoft Access
A database is a collection of logically related data that can be retrieved, manipulated, and 
updated to meet a user or organization’s needs. By providing centralized access to data 
efficiently and consistently, a database serves as an electronic warehouse of information 
on some specific aspect of an organization. A database allows for the systematic accumu-
lation, management, storage, retrieval, and analysis of the information it contains while 
reducing inaccuracies that routinely result from manual record keeping. Organizations of 
all sizes maintain databases that contain information about their customers, markets, sup-
pliers, and employees. Throughout this appendix, we will consider issues that arise in the 
creation and maintenance of a database for Stinson’s MicroBrew Distributor, a licensed 
regional independent distributor of beer and a member of the National Beer Wholesal-
ers Association. Stinson’s provides refrigerated storage, transportation, and delivery of 
premium beers produced by several local microbreweries, and so the company’s facilities 
include a state-of-the-art temperature-controlled warehouse and a fleet of temperature-
controlled trucks. Stinson’s also employs sales, receiving, warehousing/inventory, and 
delivery personnel. When making a delivery, Stinson’s monitors the retailer’s shelves, 
B.1

622 
Appendix B Data Management and Microsoft Access
taps, and keg lines to ensure the freshness and quality of the product. Because beer is 
perishable and because microbreweries often do not have the capacity to store, transport, 
and deliver large quantities of the products they produce, Stinson’s holds a critical posi-
tion in this supply chain.
Stinson’s needs to develop a faster, more efficient, and more accurate means of re-
cording, maintaining, and retrieving data related to various aspects of its business. The 
company’s management team has identified three broad key areas of data management: 
personnel (information on Stinson’s employees); supplier (information on purchases of 
beer made by Stinson’s from its suppliers); and retailer (information on sales to Stinson’s 
retail customers). We will use Microsoft Access 2013 in designing Stinson’s database. 
Access is a relational database management system (RDBMS), which is the most com-
monly used type of database system in business. Data in a relational database are stored 
in tables, which are the fundamental components of a database. A relational database 
allows the user to retrieve subsets of data from tables and retrieve and combine data that 
are stored in related tables.
In this section we will learn how to use Access to create a database and perform some 
basic database operations. Access is a database management system that is commonly 
used by businesses to manage databases. In Access, a database is defined as a collection of 
related objects that are saved as a single file. An object in Access can be a:
● 
Table; Data arrayed in rows and columns (similar to a worksheet in an Excel spread-
sheet) in which rows correspond to records (the individual units from which the 
data have been collected) and columns correspond to fields (the variables on which 
data have been collected from the records)
● 
Form; An object that is created from a table to simplify the process of entering data
● 
Query; A question posed by a user about the data in the database
● 
Report; Output from a table or a query that has been put into a specific prespecified 
format
In this appendix, we will focus on tables and queries. You can refer to a wide variety of 
books on database design to learn about forms, reports, and other database objects.
Tables are the foundation of an Access database. Each field in a table has a data type. 
The most commonly used are:
● 
Short Text; A field that contains words (such as the field Gender that may be used to 
record whether a Stinson’s employee is female or male); can contain no more than 
255 alphanumeric characters
● 
Long Text; A larger field that contains words and is generally used for recording 
lengthy descriptive entries (such as the field Notes on Special Circumstances for a 
Transaction that may be used to record detailed notes about unique aspects of spe-
cific transactions between Stinson’s and its retail customers); can contain no more 
than to 65,536 alphanumeric characters.
● 
Number:  Field that contains numerical values. There are several sizes of Number 
fields, which include: 
● 
Byte:  Stores whole numbers from 0 to 255
● 
Decimal:  Stores numbers from 21028 11 to 1028 2 1
● 
Integer:  Stores nonfractional numbers from 232,768 to 32,767
● 
Long Integer: Stores nonfractional numbers from 22,147,483,648 to 
2,147,483,647
● 
Single:  Stores numbers from 23.402823 3 1038 to 3.402823 3 1038 
● 
Double: Stores numbers from 21.79769313 3 10308 to 1.79769313 3 10308 
● 
Currency:  A field that contains monetary values (such as the field Transaction 
Amount that may be used to record payments for goods that have been ordered by 
Stinson’s retail customers)
In Access 2010, the Long 
Text field type is referred to 
as the Memo field type.

 
B.1 Database Basics and Microsoft Access 
623
● 
Yes/No:  A field that contains binary variables (such as the field Sunday Deliveries? 
that may be used to record whether Stinson’s retail customers accept deliveries on 
Sundays)
● 
Date/Time:  A field that contains dates and times (such as the field Date of Order 
that may be used to record the date of an order placed by Stinson’s with one of its 
suppliers)
Once you create a field and set its data type, you can set additional field properties. For 
example, for a numerical field you can define the data size to be Byte, Integer, Long Integer, 
Single, Double, Replication ID, or Decimal.
A database may consist of several tables that are maintained separately for a variety of 
reasons. We have already mentioned that Stinson’s maintains information on its person-
nel, its suppliers and orders, and its retail customers and sales. With regard to its retail 
customers, Stinson’s may maintain information on the company name, street address, 
city, state, zip code, telephone number, and e-mail address; the dates of orders placed and 
quantities ordered; and the dates of actual deliveries and quantities delivered. In this ex-
ample, we may consider establishing a table on Stinson’s retailer customers; in this table 
each record corresponds to a retail customer, and the fields include the retail customer’s 
company name, street address, city, state, zip code, telephone number, and e-mail address. 
Maintenance of this table is relatively simple; these data likely are not updated frequently 
for existing retail customers, and when Stinson’s begins selling to a new retail customer, 
it only has to establish a single new record containing the information for the new retail 
customer in each field.
Stinson’s may maintain other tables in this database. To track purchases made by its 
retail customers, the company may maintain a table of retail orders that includes the retail 
customer’s name and the dollar value, date, and number of kegs and cases of beer for each 
order received by Stinson’s. Because this table contains one record for each order placed 
with Stinson’s, this table must be updated much more frequently than the table of informa-
tion on Stinson’s retailer customers.
A user who submits a query is effectively asking a question about the information 
in one or more tables in a database. For example, suppose Stinson’s has determined it 
has surplus kegs of Fine Pembrook Ale in inventory and is concerned about potential 
spoilage. As a result, the Marketing Department decides to identify all retail custom-
ers who have ordered kegs of Fine Pembrook Ale during the previous three months 
so Stinson’s can call these retailers and offer them a discount price on additional kegs 
of this beer. A query could be designed to search the Retail Orders table for retail 
customers who meet this criterion. When the query is run, the output of the query 
provides the answer.
More complex queries may require data to be retrieved from multiple tables. For these 
queries, the tables must be connected by a join operation that links the records of the tables 
by their values in some common field. The common field serves as a bridge between the 
two tables, and the bridged tables are then treated by the query as a large single table com-
prising the fields of the original tables that have been joined. In designing a database for 
Stinson’s, we may include the customer ID as a field in both the table of retail customers 
and table of retail orders; values in the field customer ID would then provide the basis for 
linking records in these two tables. Thus, even though the table of retail orders does not 
contain the information on each of Stinson’s retail customers that is contained in the table 
of Stinson’s retail customers, if the database is well designed, the information in these two 
tables can easily be combined whenever necessary.
Each table in a database generally contains a primary key field that has a unique 
value for each record in the table. A primary key field is used to identify how records from 
several tables in a database are logically related. In our previous example, Customer ID is 
the primary key field for the table of Stinson’s retail customers. To facilitate the linking of 
A Replication ID field is 
used for storing a globally 
unique identifier to prevent 
duplication of an identifier 
(such as customer number) 
when multiple copies of the 
same database are in use in 
different locations.
In addition to answering a 
user’s questions about the 
data in one or more tables, 
a query can also be used 
to add a record to the end 
of a table, delete a record 
from a table, or change 
the values for one or more 
records in a table. These 
functions are accomplished 
through append, delete, and 
update queries. We discuss 
queries in more detail later 
in this appendix.

624 
Appendix B Data Management and Microsoft Access
records in the table of Stinson’s retail customers with logically related records in the table 
of retail orders, the two tables must share a primary key. Thus, a field for Customer ID 
may be included in the table of retail orders so that information in this table can be linked 
to information on each of Stinson’s retail customers; when a field is included in a table for 
the sole purpose of facilitating links with records from another table, the field is referred 
to as a foreign key field.
Considerations When Designing a Database
Before creating a new database, we should carefully consider the following issues:
● 
What is the purpose of this database?
● 
Who will use this database?
● 
What queries and reports do the users of this database need?
● 
What information or data (fields) will this database include?
● 
What tables must be created, and how will the fields be allocated to these tables?
● 
What are the relationships between these tables?
● 
What are the fields that will be used to link related tables?
● 
What forms does the organization need to create to support the use of this database?
The answers to these questions will enable us to efficiently create a more effective 
and useful database. Let us consider these issues within the context of designing Stinson’s 
database. Stinson’s has several reasons for developing and implementing a database. Quick 
access to reliable and current data will enable Stinson’s to monitor inventory and place 
orders from the microbreweries so that it can meet the demand of the retailers it supplies, 
while avoiding excess quantities and potential spoilage of inventory. These data can also be 
used to monitor the age of the product in inventory, which is a critical issue for a perishable 
product. Patterns in the orders of various beers placed by Stinson’s retail customers can be 
analyzed to determine forecasts of future demand. Employees’ salaries, federal and state tax 
withholding, vacation and sick days taken/remaining for the current year, and contributions 
to retirement funds can be tracked. Orders received from retail customers and Stinson’s 
deliveries can be better coordinated. In summary, Stinson’s can use a database to utilize in-
formation about its business in numerous ways that will potentially improve the efficiency 
and profitability of the company.
If we were to create a database for Stinson’s MicroBrew Distributor, who within the 
company might need to use information from the database? A quick review of Stinson’s 
reasons for developing and implementing a database provides the answer. Warehousing/
inventory can use the database to control inventory. Delivery can create efficient delivery 
routes for the drivers on a daily basis and assess the on-time performance of the delivery 
system. Receiving can anticipate and prepare to receive daily deliveries of microbrews. 
Human resources can administer payroll, taxes, and benefits. Marketing can identify and 
exploit potential sales opportunities.
By considering the users and uses for the database, we can make a preliminary deter-
mination of the queries and reports the users of this database will need and the data (fields) 
this database must include. At this point we can consider the tables to be created, how the 
fields will be allocated to the tables, and the potential relationships between these tables. 
We can see that we will need to incorporate data on:
● 
Each microbrewery for which Stinson’s distributes beer (Stinson’s suppliers).
● 
Each order placed with and delivery received from the microbreweries (Stinson’s 
supplies).
● 
Each retailer to which Stinson’s distributes beer (Stinson’s customers).
● 
Each order received from and delivery made to Stinson’s retail customers (Stinson’s 
sales).
● 
Each of Stinson’s employees (Stinson’s workforce).
For tables that do not 
include a primary key field, 
a unique identifier for each 
record in the table may be 
formed by combining two 
or more fields (if the com-
bination of these two fields 
will yield a unique value 
for each record that may be 
included in the table); the 
result is called a compound 
primary key and is used in 
the same way a primary key 
is used.

 
B.1 Database Basics and Microsoft Access 
625
As we design these tables and allocate fields to the tables we design, we must ensure that 
our database stores Stinson’s data in the correct formats and is capable of outputting the 
queries, forms, and reports that Stinson’s employees need. 
With these considerations in mind, we decide to begin with the following ten tables and 
associated fields in designing a database for Stinson’s MicroBrew Distributor:
● 
TblEmployees
● 
EmployeeID 
● Street Address
● 
EmpFirstName 
● City
● 
EmpLastName 
● State
● 
Gender 
● Zip Code
● 
DOB 
● Phone Number
● 
TblJobTitle
● 
Job ID 
● Job Title
● 
TblEmployHist
● 
EmployeeID 
● Job ID
● 
Start Date 
● Salary
● 
End Date 
● Hourly Rate
● 
TblBrewers 
● 
BrewerID 
● State
● 
Brewery Name 
● Zip Code
● 
Street Address 
● Phone Number
● 
City
● 
TblSOrders
● 
SOrder Number 
● EmployeeID
● 
BrewerID 
● Keg or Case?
● 
Date of SOrder 
● SQuantity Ordered
● 
TblSDeliveries
● 
SOrder Number 
● Date of SDelivery
● 
BrewerID 
● SQuantity Delivered
● 
EmployeeID
● 
TblPurchasePrices
● 
BrewerID 
● CasePurchasePrice
● 
KegPurchasePrice
● 
TblRetailers
● 
CustID 
● City
● 
Name 
● State
● 
Class 
● Zip Code
● 
Street Address 
● Phone Number
● 
TblROrders
● 
ROrder Number 
● Date of ROrder
● 
Name 
● Keg or Case?
● 
CustID 
● RQuantity Ordered
● 
BrewerID 
● Rush?
● 
TblRDeliveries
● 
CustID 
● EmployeeID
● 
Name 
● Date of RDelivery
● 
ROrder Number 
● RQuantity Delivered

626 
Appendix B Data Management and Microsoft Access
● 
TblSalesPrices
● 
BrewerID 
● CaseSalesPrice
● 
KegSalesPrice
Each table contains information about a particular aspect of Stinson’s business operations:
● 
TblEmployees: Information about each Stinson’s employee, primarily obtained 
when the employee is hired
● 
TblJobTitle: Information about each type of position held by Stinson’s employees
● 
TblEmployHist: Information about the employment history of each Stinson’s 
 employee
● 
TblBrewers: Information about each microbrewery that supplies Stinson’s with beer
● 
TblSOrders: Information about each order that Stinson’s has placed with the micro-
breweries that supply Stinson’s with beer
● 
TblSDeliveries: Information about each delivery that Stinson’s has received from 
the microbreweries that supply Stinson’s with beer
● 
TblPurchasePrices: Information about the price charged by each microbrewery that 
supplies Stinson’s with beer
● 
TblRetailers: Information about each retailer that Stinson’s supplies with beer
● 
TblROrders: Information about each order that Stinson’s has received from the 
retailers that Stinson’s supplies with beer
● 
TblRDeliveries: Information about each delivery that Stinson’s has made to the 
retailers that Stinson’s supplies with beer
● 
TblSalesPrices: Information about the price charged to retailers by Stinson’s for 
each of the microwbrews it distributes
The first three tables deal with personnel information, the next four tables deal with product 
supply/purchasing information, and the last four tables deal with demand/sales information. 
Figure B.1 shows how these tables are related.
The relationships among the tables define how they can be linked. For example, sup-
pose Stinson’s Shipping Manager needs information on the orders placed by Stinson’s retail 
customers that are to be filled tomorrow so that she can solve an optimization model that 
provides the optimal routes for Stinson’s delivery trucks. The Shipping Manager needs to 
generate a report that includes the amount of various beers ordered and the address of each 
retail customer that has placed an order. To do so, she can use the common field  CustID to 
link records from the TblRetailers. When the delivery has been made, the relevant infor-
mation is input into the TblRDeliveries table. If the Shipping Manager needs to generate a 
report of deliveries made by each driver for the past week, she can use the common field 
EmpoyeeID to link records from the TblEmployees table with related records from the 
TblRDeliveries table.
Once Stinson’s is satisfied that the planned database will provide the organization with 
the capability to collect and manage its data, and Stinson’s is also confident that the data-
base is capable of outputting the queries, forms, and reports that its employees need, we 
can proceed by using Access to create the new database. However, it is important to realize 
that it is unusual for a new database to meet all of the potential needs of its users. A well 
designed database allows for augmentation and revision when needs that the current data-
base does not meet are identified.
Creating a Database in Access
When you open Access 2013, the left pane provides links to databases you have recently 
opened as well as a means for opening existing database documents. The available docu-
ment templates are provided in the right pane; these preinstalled templates can be used to 
create new databases that utilize common formats. Because we are focusing on building a 
Note that the name of 
each table begins with the 
three letter designation 
Tbl; this is consistent with 
the Leszynski/Reddick 
guidelines, a common set 
of standards for naming 
database objects. 
Similar steps are used to 
create a new database 
in previous versions of 
 Microsoft Access.

 
B.1 Database Basics and Microsoft Access 
627
fairly generic database, we will use the Blank desktop database tool. We are now ready to 
create a new database by following these steps:
Step 1. Click the Blank desktop database icon (Figure B.2)
Step 2. When the Blank Desktop Database dialog box (Figure B.3) appears:
Enter the name of the new database in the File Name box (we will call 
our new database Stinsons.accdb)
Indicate the location where the new database will be saved by clicking the 
Browse button 
 (we will save the database called Stinsons.accdb in 
the folder C:\Users\Access 2013\Stinson Files)
Step 3. Click Create
This takes us to the Access Datasheet view. As shown in Figure B.4, the Datasheet view 
includes a Navigation Panel and Table Window. The Ribbon in the Datasheet view contains 
the FILE, HOME, CREATE, EXTERNAL DATA, DATABASE TOOLS, FIELDS, 
and TABLE tabs.
The Datasheet view provides the means for controlling the database. The groups and 
buttons of the Table Tools contextual tab are displayed across the top of this window. The 
Navigation Panel on the left side of the display lists all objects in the database. This provides a 
user with direct access to tables, reports, queries, forms, and so on that make up the currently 
Clicking the FILE tab 
in the Ribbon will allow 
the user to create new 
databases and access 
existing databases from the 
Datasheet view.
Figure B.1   TABLES AND RELATIONSHIPS FOR STINSON’S MICROBREW  DISTRIBUTOR DATABASE
TblEmployHist
EmployeeID
Start Date
End Date
Job ID
Salary
Hourly Rate
TblRDeliveries
TblEmployees
CustID
Name
ROrder Number
EmployeeID
Date of RDelivery
RQuantity Delivered
TblSDeliveries
SOrder Number
BrewerID
EmployeeID
Date of SDelivery
SQuantity Delivered
TblPurchasePrices
TblJobTitle
Job ID
Job Title
EmployeeID
EmpFirstName
EmpLastName
Gender
DOB
Street Address
City
State
Zip Code
Phone Number
TblRetailers
CustID
Name
Class
Street Address
City
State
Zip Code
Phone Number
TblROrders
ROrder Number
Name
CustID
BrewerID
Date of ROrder
Keg or Case?
RQuantity Ordered
Rush?
TblBrewers
BrewerID
Brewery Name
Street Address
City
State
Zip Code
Phone Number
TblSOrders
Personnel Information
Retailer Information
Supplier Information
BrewerID
KegPurchasePrice
CasePurchasePrice
SOrder Number
BrewerID
Date of SOrder
EmployeeID
Keg or Case?
SQuantity Ordered
TblSalesPrices
BrewerID
KegSalesPrice
CaseSalesPrice

628 
Appendix B Data Management and Microsoft Access
open database. On the right side of the display is the Table Window; the tab in the upper left 
corner of the Table Window shows the name of the current table (Table1 in Figure B.4). In the 
Table Window, we can enter data directly into the table or modify data in an existing table.
The first step in creating a new database is to create one or more tables. Because tables 
store the information contained in a database, they are the foundation of a database and 
must be created prior to the creation of any other objects in the database. There are two 
options for manually creating a table: We can enter data directly in Datasheet view, or we 
can design a table in Design view. We will create our first table, TblBrewers, by entering 
data directly in Datasheet view. You can review an example database comprising all of the 
file
WEB
Stinsons
Figure B.2  BLANK DESKTOP DATABASE ICON
Blank desktop database
Figure B.3  BLANK DESKTOP DATABASE DIALOG BOX
File Name
C:\Users\Access 2013\Stinson Files\
Stinsons.accdb
Create
Blank desktop database

 
B.1 Database Basics and Microsoft Access 
629
objects and relationships between the objects that we create throughout this appendix for 
the Stinson’s database by opening the file Stinsons. 
In Datasheet view the data are entered by field, one record at a time. In Figure B.1 
we see that the fields for TblBrewers are BrewerID, Brewery Name, Street Address, City, 
State, Zip Code, and Phone Number. From Stinson’s current filing system, we have been 
able to retrieve the following information on the breweries that supply Stinson’s.
We can enter these data into our new database in Datasheet view by following these 
steps:
Step 1. Enter the first record from Table B.1 into the first row of the Table Window 
in Access by entering a 3 in the top row next to (New), pressing the Tab key, 
entering Oak Creek Brewery in the next column, pressing the Tab key, enter-
ing 12 Appleton St in the next column, pressing the Tab key, entering Dayton 
in the next column, pressing the Tab key, etc.
Step 2. Enter the second record from Table B.1 by repeating Step 1 for the Gonzo 
 Microbrew data and entering these data into the second row of the Table 
 Window in Access
 
Continue entering data for the remaining microbreweries in this manner
The completed table in Access appears in Figure B.5.
Now that we have entered all of our information on the microbreweries that supply 
 Stinson’s, we need to save this table as an object in the Stinson’s database. We click on 
You can click the Help 
button  to find detailed 
instructions on creating a 
table (or using any other 
Access functionality).
When we enter 3 in Step 1, 
this establishes a new field 
with the generic name 
“Field1”, and generates 
a value for the ID column. 
Pressing Tab moves to the 
next field entry box for the 
same record.
Figure B.4  DATASHEET VIEW AND TABLE TOOLS CONTEXTUAL TAB
Navigation Panel
Table Window
TABLe B.1  RAW DATA FOR TABLE TBLBREWERS
BrewerID
Brewery Name
Street Address
City
State
Zip 
Code
Phone 
Number
3
6
4
9
7
2
Oak Creek Brewery
Gonzo Microbrew
McBride’s Pride
Fine Pembrook Ale
Midwest Fiddler Crab
Herman’s Killer Brew
12 Appleton St
1515 Main St
425 Mad River Rd
141 Dusselberg Ave
844 Far Hills Ave
912 Airline Dr
Dayton
Dayton
Miamisburg
Trotwood
Kettering
Fairborn
OH
OH
OH
OH
OH
OH
45455
45429
45459
45426
45453
45442
937-445-1212
937-278-2651
937-439-0123
937-837-8752
937-633-7183
937-878-2651

630 
Appendix B Data Management and Microsoft Access
the Save button 
 in the Quick Access Toolbar above the Ribbon, type the table name 
TblBrewers in the Save As dialog box that appears (as shown in Figure B.6), and click OK. 
The name in the Table Name tab on the Table Window now reads “TblBrewers.”
We can now use the Design view to provide meaningful names for our fields and 
specify each field’s properties. We switch to Design view by first clicking on the arrow 
below the View button 
 in the Views group of the Ribbon. This will open a pull-down 
menu with options for various views (recall that we are currently in the Datasheet view). 
Clicking on the Design View option opens the Design view for the current table as shown 
in Figure B.7. From the Design view we can define or edit the table’s fields and field prop-
erties as well as rearrange the order of the fields if we wish. The name of the current table 
is again identified in the Name Tab, and the Table Window is replaced with two sections; 
the Table Design Grid on top and the Field Properties Pane on the bottom of this window.
We can now replace the generic field names (Field1, Field2, etc.) in the column on the 
right side of the Table Design Grid of TblBrewers with the names we established from our 
original database design and then move to defining the field type for each field. To change 
the data type for a field in design view, we follow these steps:
Step 1. Click on the cell in the Data Type column (the middle column) in the Table 
Design Grid in the row of the field for which you want to change the data type
Step 2. Click on the down arrow 
 in the upper right-hand corner of the selected cell
Step 3. Define the data type for the field using the drop-down menu (Figure B.8)
Note that we can resize a 
column in this table by ei-
ther double-clicking on the 
right side of the header cell 
or grabbing and dragging 
the right side of the header 
cell for any field, just as 
we would in an Excel 
spreadsheet.
Note that Field Names used 
in Access cannot exceed 
64 characters, cannot 
begin with a space, and can 
include any combination 
of letters, numbers, spaces, 
and special characters 
except for a period (.), an 
accent grave (‘), an excla-
mation point (!), or square 
brackets ([ and ]).
Figure B.5   RECORDS FOR SIX MICROBREWERIES ENTERED INTO AN ACCESS 2013 TABLE
Figure B.6  SAVE AS DIALOG BOX

 
B.1 Database Basics and Microsoft Access 
631
Figure B.7   DESIGN VIEW FOR THE TABLE TBLBREWERS
Navigation Pane
Table Design Grid
Field Properties Pane
Figure B.8   CHANGING THE DATA TYPE FOR THE BREWERY NAME FIELD IN THE TABLE 
TBLBREWERS

632 
Appendix B Data Management and Microsoft Access
Notice that when you use this menu to define the data type for a field, the Field Proper-
ties pane changes to display the properties and restrictions of the selected data type. For 
example, the field Brewery Name is defined as Short Text; when any row of the Table 
Design Grid associated with this field is selected, the Field Property Pane shows the charac-
teristics associated with a field of data type Short Text, including a limit of 255 characters. 
If we thought we might eventually do business with a brewery that has a business name 
that exceeds 255 characters, we may decide to select the Long Text data type for this field 
(Figure B.8). However, selecting a data type that allows for greater capacity will increase 
the size of the database and should not be used unless necessary.
After defining the data type for each of the fields to be Short Text (although fields such 
as BrewerID, Zip Code, and Telephone Number are made up of numbers, we would not 
consider doing arithmetic operations on these cells, so we define these fields as Short Text), 
we can use the column labeled Description on the right side of the Table Design Grid to 
document the contents of each field. Here we may include:
● 
Brief descriptions of the fields (especially if our field names are not particularly 
meaningful or descriptive)
● 
Instructions for entering data into the fields (for example, we may indicate that a 
telephone number is entered area code-exchange-number)
● 
Indications of whether a field acts as a primary or foreign key
To change the primary key from the default field ID to BrewerID, we use the following 
steps:
Step 1. Click on any cell in the BrewerID row
Step 2. Click the FILE tab in the Ribbon
Step 3. Click the Primary Key icon in the Tools group
This changes the primary key from the ID field to the BrewerID field. We can now delete 
the ID field because it is no longer needed.
Step 4. Right-click any cell in the ID row and click Delete Rows (Figure B.9)
Click Yes when the dialog box appears to confirm that you want to delete 
this row
We have now created the table TblBrewers by entering the data in Datasheet view 
(Figure B.10) and (1) changed the name of each field, (2) identified the correct data type 
for each field, (3) revised properties for some fields, (4) added a description for each field, 
and (5) changed the key field to BrewerID in Design view. Alternatively, we could create 
a table in Design view. We first enter the field names, data types, and descriptions in the 
Table Design grid. After saving this table as TblSOrders, we then move to the Database 
Window, which now has defined fields, and enter the information in the appropriate cells. 
A field such as State is 
a good candidate for 
reducing the field size. If 
we use the official USPS 
abbreviations for the states, 
this field should always use 
two characters. Note that 
if we violate the restriction 
we place on a field, Access 
will respond with an error 
statement. The restric-
tion on length can be very 
helpful in this instance. 
Because we know a state 
abbreviation should always 
be exactly two characters, 
an error statement regard-
ing the length of the State 
field indicates that we have 
made an incorrect entry for 
this field.
Figure B.9  DROP-DOWN MENU FOR DELETING FIELDS IN THE DESIGN VIEW

 
B.1 Database Basics and Microsoft Access 
633
Figure B.10  DESIGN VIEW OF TABLE DESIGN FOR TBLBREWERS
Suppose we take this approach to create the table TblSOrders, which contains information 
on orders Stinson’s places with the microbreweries. We have the following data for orders 
from the past week (Table B.2) that we will use to initially populate this table (new orders 
will be added to the table as they are placed).
TABLe B.2  RAW DATA FOR TABLE TBLSORDERS
SOrderNumber
BrewerID
Date of SOrder
EmployeeID
Keg or Case?
SQuantity Ordered
17351
17352
17353
17354
17355
17356
17358
17359
17360
17361
17362
17363
17364
17365
17366
17367
17368
17369
3
9
7
3
2
6
9
4
3
2
7
9
6
2
3
7
9
4
11/5/2012
11/5/2012
11/5/2012
11/6/2012
11/6/2012
11/6/2012
11/7/2012
11/7/2012
11/8/2012
11/8/2012
11/8/2012
11/8/2012
11/8/2012
11/9/2012
11/9/2012
11/9/2012
11/9/2012
11/9/2012
135
 94
 94
 94
135
135
 94
135
 94
 94
 94
135
 94
135
135
 94
135
 94
Keg
Case
Keg
Keg
Keg
Case
Keg
Keg
Case
Keg
Keg
Keg
Keg
Case
Keg
Case
Keg
Keg
3
6
2
3
2
5
3
2
8
1
2
4
2
5
4
4
4
3

634 
Appendix B Data Management and Microsoft Access
The fields represent Stinson’s internal number assigned to each order placed with a brew-
ery (SOrderNumber), Stinson’s internal identification number assigned to the microbrewery 
with which the order has been placed (BrewerID), the date on which Stinson’s placed the 
order (Date of SOrder), the identification number of the Stinson’s employee who placed the 
order (EmployeeID), an indication of whether the order was for kegs or cases of beer (Keg 
or Case?), and the quantity (in units) ordered (SQuantity Ordered). As before, we enter the 
information into the Field Name, Data Type, and Description columns of the Table Design 
grid, remove the ID field, change the primary key field (this time to the field SOrderNumber), 
and revise the properties of the fields as necessary in the Field Properties area as shown in 
Figure B.11.
Now we return to the Database window and manually input the data from Table B.2 
into the table TblSOrders as shown in Figure B.12. Note that in both Datasheet view and 
Design view, we now have separate tabs with the table names TblBrewers and TblSOrders 
and that these two tables are listed in the Navigation Panel. We can use either Datasheet 
view or Design view to move between our tables.
We can also create a table by reading information from an external file. Access is 
 capable of reading information from several types of external files. Here we demon-
strate by reading data from an Excel file into a new table TblSDeliveries. The Excel file 
 SDeliveries .xlsx contains the information on deliveries received by Stinson’s from various 
microbreweries during a recent week. The fields of this table, as shown in Figure B.12, 
will correspond to the column headings in the Excel worksheet displayed in Figure B.13.
The columns in Figure B.13 represent: Stinson’s internal number assigned to each 
order placed with a microbrewery (SOrderNumber), Stinson’s internal identification num-
ber assigned to the microbrewery with which the order has been placed (BrewerID), the 
identification number of the Stinson’s employee who received the delivery (EmployeeID), 
Figure B.11   DESIGN VIEW OF TABLE DESIGN FOR TBLSORDERS

 
B.1 Database Basics and Microsoft Access 
635
Figure B.12   DATASHEET VIEW FOR TBLSORDERS
Figure B.13   EXCEL SPREADSHEET SDELIVERIES.XLSX
A
C
D
SOrderNumber
17358
17356
17355
17354
17353
17352
17351
17359
17364
17365
17366
17367
17368
17369
17360
17361
17363
17362
BrewerID
9
6
2
3
7
9
3
4
6
2
3
7
9
4
3
2
9
7
EmployeeID
135
135
135
94
135
94
94
135
135
94
94
135
94
94
94
135
94
94
Date of SDelivery
11/7/2012
11/6/2012
11/6/2012
11/6/2012
11/6/2012
11/5/2012
11/5/2012
11/7/2012
11/8/2012
11/9/2012
11/9/2012
11/10/2012
11/9/2012
11/9/2012
11/8/2012
11/8/2012
11/9/2012
11/8/2012
E
SQuantity Delivered
3
5
2
3
2
6
3
2
2
5
4
4
4
3
8
1
4
2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
B
file
WEB
SDeliveries

636 
Appendix B Data Management and Microsoft Access
the date on which Stinson’s received the delivery (Date of Sdelivery), and the quantity (in 
units) received in the delivery (SQuantity Delivered). To import these data directly into the 
table TblSDeliveries, we follow these steps:
Step 1. Click the EXTERNAL DATA tab in the Ribbon
Step 2. Click the Excel icon in the Import & Link group (Figure B.14)
Step 3. When the Get External Data—Excel Spreadsheet dialog box appears (Fig-
ure B.15), click the Browse… button
Navigate to the location of the Excel file to be imported into Access 
(in this case,  SDeliveries.xlsx), and indicate the manner in which we 
Figure B.14   EXTERNAL DATA TAB ON THE ACCESS 2013 RIBBON
Figure B.15   GET EXTERNAL DATA—EXCEL SPREADSHEET DIALOG BOX

 
B.1 Database Basics and Microsoft Access 
637
want to import the information in this Excel file by selecting the ap-
propriate radio button (we are importing these data to a new table, 
 TblSDeliveries, in the current database)
Step 4. Click OK
Step 5. When the Import Spreadsheet Wizard dialog box opens (Figure B.16), 
 arrange the information as shown in Figure B.16
Verify that the check box for First Row Contains Column Headings is 
selected because the worksheet from which we are importing the data 
contains column headings
Click Next > to open the second screen of the Import Spreadsheet 
 Wizard dialog box (Figure B.17)
Step 6. Indicate the format for the first field (in this case, SOrderNumber) and whether 
this field is the primary key field for the new table (it is in this case)
Click Next >
We continue to work through the ensuing screens of the Import Spreadsheet  Wizard 
dialog box, indicating the format for each field and identifying the primary key field 
( SOrderNumber) for the new table. When we have completed the final screen, we click 
 Finish and add the table TblSDeliveries to our database. Note that in both Datasheet 
view (Figure B.18) and Design view, we now have separate tabs with the table names 
If the Excel worksheet from 
which we are importing 
the data does not contain 
column headings, Access 
will assign dummy names 
to the fields that can later 
be changed in the Table 
Design grid of Design view.
Figure B.16   FIRST SCREEN OF IMPORT SPREADSHEET WIZARD DIALOG BOX

638 
Appendix B Data Management and Microsoft Access
Figure B.17   SECOND SCREEN OF IMPORT SPREADSHEET WIZARD DIALOG BOX
Figure B.18   DATASHEET VIEW FOR TBLSDELIVERIES

 
B.2 Creating Relationships Between  Tables in  Access 
639
TblBrewers, TblSOrders, and TblSDeliveries, and that these three tables are listed in the 
Navigation Panel.
We have now created the table TblSDeliveries by reading the information from the 
Excel file SDeliveries.xlsx, and we have entered information on the fields and identified the 
primary key field in this process. This procedure for creating a table is more convenient (and 
more accurate) than manually inputting the information in Datasheet view, but it requires 
that the data are in a file that can be imported into Access.
Creating Relationships 
Between  Tables in  Access
One of the advantages of a database over a spreadsheet is the economy of data storage and 
maintenance. Information that is associated with several records can be placed in a separate 
table. As an example, consider that the microbreweries that supply beer to Stinson’s are 
each associated with multiple orders for beer that have been placed by Stinson’s. In this 
case, the names and addresses of the microbreweries do not have to be included in records 
of Stinson’s orders, saving a great deal of time and effort in data entry and maintenance. 
However, the two tables, in this case the table with the information on Stinson’s orders for 
beer and the table with information on the microbreweries that supply Stinson’s with beer, 
must be joined (i.e., have a defined relationship) by a common field. To use the data from 
these two tables for a common purpose, a relationship between the two tables must be cre-
ated to allow one table to share information with another table.
The first step in deciding how to join related tables is to decide what type of relationship 
you need to create between tables. Next we briefly summarize the three types of relation-
ships that can exist between two tables.
One-To-Many This relationship occurs between two tables, which we will label as 
Table A and Table B, when the value in the common field for a record in Table A can 
match the value in the common field for multiple records in Table B, but a value in the 
common field for a record in Table B can match the value in the common field for at 
most a single record in Table A. Consider TblBrewers and TblSOrders with the  common 
field BrewerID. In TblBrewers, each unique value of BrewerID is associated with a 
single record that contains contact information for a single brewer, while in TblSOrders 
each unique value of BrewerID may be associated with several records that contain 
information on various orders placed by Stinson’s with a specific brewer. When these 
tables are linked through the common field BrewerID, each record in TblBrewers can 
potentially be matched with multiple records of orders in TblSOrders, but each record in 
TblSOrders can be matched with only one record in TblBrewers. This makes sense as a 
single brewer can be matched to several orders, but each order can be matched only to 
a single brewer.
One-To-One This relationship occurs when the value in the common field for a record in 
Table A can match the value in the common field for at most one record in Table B, and 
a value in the common field for a record in Table B can match the value in the common 
field for at most a single record in Table A. Here we consider TblBrewers and TblPur-
chasePrices, which also share the common field BrewerID. In TblBrewers each unique 
value of BrewerID is associated with a single record that contains contact information 
for a single brewer, while in TblPurchasePrices each unique value of BrewerID is associ-
ated a single record that contains information on prices charged to Stinson’s by a specific 
brewer for kegs and cases of beer. When these tables are linked through the common 
field BrewerID, each record in TblBrewers can be matched to at most a single record 
B.2
One-To-Many relationships 
are the most common type 
of relationship between two 
tables in a relational da-
tabase; these relationships 
are sometimes abbreviated 
as 1:∞.
One-To-One relationships 
are the least common form 
of relationship between 
two tables in a relational 
database because it is often 
possible to include these 
data in a single table; these 
relationships are sometimes 
abbreviated as 1:1.

640 
Appendix B Data Management and Microsoft Access
of prices in TblPurchasePrices, and each record in TblPurchasePrices can be matched 
with no more than one record in TblBrewers. This makes sense as a single brewer can be 
matched only to the prices it charges, and a specific set of prices can be matched only to 
a single brewer.
Many-To-Many This occurs when a value in the common field for a record in Table A 
can match the value in the common field for multiple records in Table B, and a value in the 
common field for a record in Table B can match the value in the common field for several 
records in Table A. Many-To-Many relationships are not directly supported by Access 
but can be facilitated by creating a third table, called an associate table, that contains a 
primary key and a foreign key to each of the original tables. This ultimately results in one-
to-many relationships between the associate table and the two original tables. Our design 
for  Stinson’s database does not include any many-to-many relationships.
To create any of these three types of relationships between two tables, we must satisfy 
the rules of integrity. Recall that the primary key field for a table is a field that has (and will 
have throughout the life of the database) a unique value for each record. Defining a primary 
key field for a table ensures the table will have entity integrity, which means that the table 
will have no duplicate records.
Note that when the primary key field for one table is a foreign key field in an-
other table, it is possible for a value of this field to occur several times in the table for 
which it is a foreign key field. For example, Job ID is the primary key field in the table 
 TblJobTitle and will have a unique value for each record in this table. But Job ID is a 
foreign field in the table TblEmployHist, and so a value of Job ID can occur several times 
in  TblEmployHist.
Referential integrity is the rule that establishes the relationship between two tables. 
For referential integrity to be established, when the foreign key field in one table (say, 
Table B) and the primary key field in the other table (say, Table A) are matched, each 
value that occurs in the foreign key field in Table B must also occur in the primary key 
field in Table A. For instance, to preserve referential integrity for the relationship between 
 TblEmployHist and TblJobTitle, each employee record in TblEmployHist must have a 
value in the Job ID field that exactly matches a value of the Job ID field in TblJobTitle. If a 
record in TblEmployHist has a value for the foreign key field (Job ID) that does not occur 
in the primary key field (Job ID) of TblJobTitle, the record is said to be orphaned (in this 
case, this would occur if we had an employee who has been assigned a job that does not exist 
in our database). An orphaned record would be lost in any table that results from joining 
TblJobTitle and TblEmployHist. Enforcing referential integrity through Access prevents 
records from becoming orphaned and lost when tables are joined.
Violations of referential integrity lead to inconsistent data, which results in meaningless 
and potentially misleading analyses. Enforcement of referential integrity is critical not only 
for ensuring the quality of the information in the database but also for ensuring the validity 
of all conclusions based on these data.
We are now ready to establish relationships between tables in our database. We will 
first establish a relationship between the tables TblBrewers and TblSOrders. To establish a 
relationship between these two tables, take the following steps: 
Step 1. Click the DATABASE TOOLS tab in the Ribbon (Figure B.19)
Step 2. From the Navigation Panel select one of the tables for which you want to es-
tablish a relationship (we will click on TblBrewers)
Step 3. Click the Relationships icon 
 in the Relationships group
This will open the contextual tab RELATIONSHIP TOOLS in the Rib-
bon and a new display with a tab labeled Relationships in the work-
space, as shown in Figure B.20. A box listing all fields in the table you 
selected before clicking the Relationships icon will be provided
Many-To-Many relation-
ships are sometimes 
 abbreviated as ∞:∞.

 
B.2 Creating Relationships Between  Tables in  Access 
641
Figure B.19   DATABASE TOOLS TAB ON THE RIBBON
Figure B.20   RELATIONSHIP TOOLS CONTEXTUAL TAB ON THE RIBBON 
AND DISPLAY WITH A TAB LABELED RELATIONSHIPS IN THE 
WORKSPACE
Step 4. Click Show Table in the Relationships group
When the Show Table dialog box opens (Figure B.21), select the second 
table for which you want to establish a relationship (in our example this 
is TblSOrders) to establish a relationship between these two tables
Click Add
Once we have selected the two tables (TblBrewers and TblSOrders) for which we 
are establishing a relationship, boxes showing the fields for the two tables will appear in 
the workspace. If Access can identify a common field, it will also suggest a relationship 
between these two tables. In our example, Access has identified BrewerID as a common 
field between TblBrewers and TblSOrders and is showing a relationship between these two 
tables based on this field (Figure B.22).
In this instance, Access has correctly identified the relationship we want to establish 
between TblBrewers and TblSOrders. However, if Access does not correctly identify the 
relationship, we can modify the relationship between these tables. If we double-click on the 
You can select multiple 
tables in the Show Table 
dialog box by holding down 
the CTRL key and selecting 
multiple tables.

642 
Appendix B Data Management and Microsoft Access
line connecting TblBrewers to TblSOrders, we open the relationship’s Edit Relationships 
dialog box, as shown in Figure B.23.
Note here that Access has correctly identified the relationship between TblBrewers and 
TblSOrders to be one-to-many and that we have several options to select. We can use the 
pull-down menu under the name of each table in the relationship to select different fields 
to use in the relationship between the two tables.
By selecting the Enforce Referential Integrity option in the Edit Relationships dia-
log box, we can indicate that we want Access to monitor this relationship to ensure that it 
satisfies relational integrity. This means that every unique value in the BrewerID field in 
TblSOrders also appears in the BrewerID field of TblBrewers; that is, there is a one-to-many 
If Access does not suggest 
a relationship between 
two tables, you can click 
Create New . . . in the Edit 
Relationships dialog box to 
open the Create New dialog 
box, which then will allow 
you to specify the tables to 
be related and the fields in 
these tables to be used to 
establish the relationship.
Figure B.22   UPPER PORTION OF THE RELATIONSHIPS WORKSPACE SHOW-
ING THE RELATIONSHIP BETWEEN TBLBREWERS AND TBLSOR-
DERS
TblBrewers
SOrderNumber
BrewerID
Date of SOrder
EmployeeID
Keg or Case?
SQuantity Ordered
TblSOrders
Relationships
BrewerID
Brewery Name
Street Address
City
State
Zip Code
Telephone Number
Figure B.21  SHOW TABLE DIALOG BOX

 
B.2 Creating Relationships Between  Tables in  Access 
643
relationship between TblBrewers and TblSOrders, and Access will revise the display of the 
relationship as shown in Figure B.22 to reflect that this is a one-to-many relationship.
Finally, we can click Join Type . . in the Edit Relationships dialog box to open the 
Join Properties dialog box (Figure B.24). This dialog box allows us to specify which 
 records are retained when the two tables are joined.
Once we have established a relationship between two tables, we can create new Ac-
cess objects (tables, queries, reports, etc.) using information from both of the joined tables 
simultaneously. Suppose Stinson’s will need to combine information from  TblBrewers, 
TblSOrders, and TblSDeliveries. Using the same steps, we can also establish relationships 
between the three tables TblBrewers, TblSOrders, and  TblSDeliveries as shown in Fig-
ure B.25. Note that for each relationship shown in this figure, we have used the Enforce 
Referential Integrity option in the Edit Relationships dialog box to indicate that we want 
Access to monitor these relationships to ensure they satisfies relational integrity. Thus, 
each relationship is identified in this case as a one-to-many relationship.
This set of relationships will also allow us to combine information from all three tables 
and create new Access objects (tables, queries, reports, etc.) using information from the 
three joined tables simultaneously.
Figure B.23   EDIT RELATIONSHIPS DIALOG BOX
Figure B.24   JOIN PROPERTIES DIALOG BOX

644 
Appendix B Data Management and Microsoft Access
Sorting and Filtering Records
As our tables inevitably grow or are joined to form larger tables, the number of records can 
become overwhelming. One of the strengths of relational database software such as Access 
is that they provide tools, such as sorting and filtering, for dealing with large quantities of 
data. Access provides several tools for sorting the records in a table into a desired sequence 
and filtering the records in a table to generate a subset of your data that meets specific 
criteria. We begin by considering sorting the records in a table to improve the organization 
of the data and increase the value of information in the table by making it easier to find 
records with specific characteristics. Access allows for records to be sorted on values of one 
or more fields, called the sort fields, in either ascending or descending order. To sort on a 
single field, we click on the Filter Arrow in the field on which we wish to sort.
Suppose Stinson’s Manager of Receiving wants to review a list of all deliveries re-
ceived by Stinson’s, and she wants the list sorted by the Stinson’s employee who received 
the orders. To accomplish this, we first open the table TblSDeliveries in Datasheet view. 
We then click on the Filter Arrow 
 for the field EmployeeID (the sort field), as shown 
in Figure B.26; to sort the data in ascending order by values in the EmployeeID field, we 
click on  Sort Smallest to Largest (clicking on  Sort Largest to Smallest will sort the 
data in descending order by values in the EmployeeID field). By using the Filter Arrows, 
we can sort the data in a table on values of any of the table’s fields.
We can also use this pull-down menu to filter our data to generate a subset of data in a 
table that satisfies specific conditions. If we want to create a display of only deliveries that 
were received by employee 135, we would click the Filter Arrow next to EmployeeID, 
select only the check box for 135, and click OK (Figure B.27).
Filtering through the Filter Arrows is convenient if you want to retain records associ-
ated with several different values in a field. For example, if we want to generate a display 
B.3
Note that different data 
types have different sort 
options.
Note that different data 
types have different filter 
options.
Figure B.25   UPPER PORTION OF THE RELATIONSHIPS WORKSPACE SHOWING 
THE RELATIONSHIPS AMONG TBLBREWERS, TBLSORDERS, AND 
 TBLSDELIVERIES
Relationships
TblBrewers
BrewerID
Brewery Name
Street Address
City
State
Zip Code
Telephone Number
TblSOrders
SOrderNumber
BrewerID
Date of SOrder
EmployeeID
Keg or Case?
SQuantity Ordered
TblSDeliveries
SOrderNumber
BrewerID
EmployeeID
Date of SDelivery
SQuantity Delivered

 
B.3 Sorting and Filtering Records 
645
of the records in the table TblSDeliveries associated with breweries with BrewerIDs 3, 4, 
and 9, we would click on the Filter Arrow next to BrewerID, deselect the check boxes for 
2, 4, 6, and 7, and click OK.
The Sort & Filter group in the HOME tab also provides tools for sorting and filter-
ing records in a table. To quickly sort all records in a table on values for a field, open the 
table to be sorted in Datasheet view, and click on any cell in the field to be sorted. Then 
click on  Ascending to sort records from smallest to largest values in the sort field or on 
 Descending to sort records from largest to smallest in the sort field.
Access also allows for simultaneous sorting and filtering through the Advanced func-
tion in the Sort & Filter group of the HOME tab; the advanced Filter/Sort display for the 
table TblSDeliveries is shown in Figure B.28. Once we have opened the table to be filtered 
and sorted in Datasheet view, we click on Advanced in the Sort & Filter group of the 
Clicking Selection in the 
Sort & Filter group will 
also filter on values of a 
single field.
The Advanced Filter/Sort 
feature in Access is actually 
the simplest form of query. 
It allows you to find and 
sort information from one 
table in the database.
Figure B.27   TOP ROWS OF THE TABULAR DISPLAY OF RESULTS OF FILTERING
Figure B.26   PULL-DOWN MENU FOR SORTING AND FILTERING RECORDS IN A TABLE WITH THE 
FILTER ARROW

646 
Appendix B Data Management and Microsoft Access
HOME tab, as shown in Figure B.28. We then select Advanced Filter/Sort…. From this 
display, we double-click on the first field in the field list on which we wish to filter. The 
field we have selected will appear in the heading of the first column in the tabular display 
at the bottom of the screen. We can then indicate in the appropriate portion of this display 
the sorting and filtering to be done on this field. We continue this process for every field 
for which we want to apply a filter and/or sort, remembering that the sorting will be nested 
(the table will be sorted on the first sort field, and then within each unique value of the first 
sort field the sort for the second sort field will be executed, and so on).
Suppose we wish to create a new tabular display of all records for deliveries from brew-
eries with BrewerIDs of 4 or 7 for which fewer than 7 units were delivered, and we want 
the records in this display sorted in ascending order first on values of the field BrewerID 
and then on values of the field SQuantity Delivered. To execute these criteria, we perform 
the following steps:
Step 1. Click the HOME tab in the Ribbon
Step 2. Click Advanced in the Sort & Filter group, and select Advanced Filter/
Sort . . .
Figure B.28   ADVANCED FILTER/SORT DISPLAY FOR TBLSDELIEVERIES
Figure B.29   TABULAR DISPLAY OF CRITERIA FOR SIMULTANEOUS 
 FILTERING AND SORTING USING ADVANCED FILTER/SORT
Field:
Sort:
Criteria:
or:
BrewerID
Ascending
4
7
SQuantity Delivered
Ascending
<7

 
B.4 Queries 
647
Step 3. In the TblSDeliveries box, double-click BrewerID to add this field to the first 
column in the lower pane of the screen
Select Ascending in the Sort: row of the BrewerID column in the lower 
pane
Enter 4 in the Criteria: row of the BrewerID column in the lower pane
Enter 7 in the or: row of the BrewerID column in the lower pane
Step 4. In the TblSDeliveries box, double-click SQuantity Delivered to add this to 
the second column in the lower pane of the screen
Select Ascending in the Sort: row of the SQuantity Delivered column 
in the lower pane
Enter <7 in the Criteria: row of the SQuantity Delivered column in the 
lower pane
Figure B.29 displays the lower pane of the Advanced Filter/Sort after 
these steps have been completed
Step 5. Click Advanced in the Sort & Filter group of the HOME tab
Click Apply Filter/Sort
These steps produce the tabular display in Figure B.30.
Note that the data, after being filtered to show only records with breweries that do 
have values of 4 or 7 in the BrewerID field and all records with deliveries of 7 or fewer 
units, are sorted first in ascending order on the BrewerID field. Within each unique 
value in the BrewerID field, the records are sorted in ascending order on the SQuanity 
Delivered field.
Queries
Queries are a way of searching for and compiling data that meet specific criteria from one 
or more tables. They enable you to extract particular fields from a table or create a new table 
that combines information from several related tables.
Although there are similarities between queries and simple searches or filters, queries 
are far more powerful because they can be used to extract information from multiple tables. 
For example, although you could use a search in the table TblBrewers to find the name of a 
brewer that supplies beer to Stinson’s or a filter on the table TblSOrders to view only orders 
placed by Stinson’s for kegs of beer, neither of those approaches would let you simultane-
ously view both the names of brewers and orders placed for kegs of beer. However, you 
Also note that we can use 
wildcard symbols when 
filtering by substitut-
ing an asterisk symbol, 
*, for any portion of 
the value of a field you 
want to represent with a 
wildcard. For example, 
if we wanted to create a 
table of information on all 
Stinson employees whose 
last names started with the 
letter B, we would filter 
the field EmpLastName in 
the table TblEmployees by 
entering B* in the Criteria: 
row of the Advanced Filter/
Sort. This filter will return 
all records that have the 
combination of the first 
letter “B” and any other 
following characters in the 
EmpLastName field.
B.4
Figure B.30   TABULAR DISPLAY OF FILTERED AND SORTED DATA USING ADVANCED  
FILTER/SORT
We can toggle between 
a display of the filtered/
sorted data and a display of 
the original table by click-
ing on Toggle Filter in the 
Sort & Filter group of the 
HOME tab.

648 
Appendix B Data Management and Microsoft Access
could easily run a query to create a record of every order Stinson’s has placed for kegs of 
beer that includes the name of the brewer and the corresponding order that was placed. By 
taking advantage of the relationships among the tables of a database, a well designed query 
can yield information that would be cumbersome or difficult to discern by examining the 
data in individual tables.
Access allows for several types of queries. The three most commonly used are:
● 
Select queries: These are the simplest and most commonly used queries; they are 
used to extract the subset of data from a table that satisfy one or more criteria. For 
example, Stinson’s manager of receiving may want to review a list of all deliveries 
received by Stinson’s that includes the Stinson’s employee who received each order 
over some period of time. A select query could be applied to the table  TblSDeliveries 
(shown in the original database design illustrated in Figure B.1) to create the subset 
of this table containing only the fields SOrderNumber and EmployeeID.
● 
Action queries: These queries are used to change data in existing tables. For exam-
ple, the sales manager may want to increase the prices charged to retailers by Stin-
son’s for the kegs of microbrews that Stinson’s sells. The sales manager can quickly 
make this change through an action query applied to the table TblSalesPrices to 
quickly perform these calculations and modify these prices in the database. Action 
queries allow the user to modify many records quickly and efficiently. Access pro-
vides four types of action queries:
● 
Update allows the values of one or more fields in the result set to be modi-
fied.
● 
Make table creates a new table based on the results of the query.
● 
Append is similar to a make table query, except that the results of the query 
are appended to an existing table.
● 
Delete deletes all the records in the results of the query from the underlying 
table.
● 
Crosstab queries: These perform calculations on information in a table. Stinson’s 
manager of receiving may be interested in how many kegs and cases of beer have 
been delivered to Stinson’s and which Stinson’s employee received the shipment. 
The manager could find this information by applying a crosstab query to the table 
TblSDeliveries (shown in the original database design in Figure B.1) to create a 
table that shows number of kegs and cases delivered by the Stinson’s employee who 
received the shipment.
We next review how to execute each of these types of queries in Access.
Select Queries
We start by considering the needs of Stinson’s manager of receiving, who wants to review 
a list of all deliveries received by Stinson’s and the Stinson’s employee who received the 
orders during some recent week. This requires us to perform a select query on the table 
TblSDeliveries to create a subset of this table that includes only the fields SOrderNumber 
and EmployeeID for deliveries to Stinson’s during the past week (the only week for which 
we have data in our new database) and display this subset in Datasheet view. To execute 
this select query, we take the following steps:
Step 1. Click the CREATE tab in the Ribbon (Figure B.31)
Step 2. Click Query Wizard in the Queries group
Step 3. When the New Query dialog box appears (Figure B.32)
Select Simple Query Wizard
Click OK
Action queries are also 
known as Data Manipula-
tion Language (DML) 
statements.
You can open the file 
 Stinsons and follow these 
steps to reproduce the 
results of this query.

 
B.4 Queries 
649
Figure B.31  CREATE TAB IN THE RIBBON
Figure B.32  NEW QUERY DIALOG BOX
Step 4. When the next Simple Query Wizard dialog box appears (see Figure B.33)
Select Table: TblSDeliveries in the Tables/Queries box
Select the fields SOrderNumber and EmployeeID from the Available 
Fields: box and move these to the Selected Fields: box using the 
 
button (Figure B.33)
Click Next >
Step 5. When the next Simple Query Wizard dialog box appears
Select Detail (shows every field of every record)
Click Next >
Step 6. Name our query by entering TblSDeliveries Employee Query in the What title 
do you want for your query? box
Select Open the query to view information (Figure B.35)
Click Finish
The display of the query results is provided in Figure B.36.
Note that in both Datasheet view and Design view, we now have a new tab with the 
table TblSDeliveries Employee Query. We can also change the Navigation Panel so that 
Step 5 of using the Simple 
Query Wizard offers us the 
option of generating a de-
tailed display of these fields 
for all records in the table 
or a summary display (see 
Figure B.34 for displays 
of the dialog boxes for this 
step of the Simple Query 
Wizard and Summary Op-
tions). Since the manager of 
receiving wants to review 
a list of all deliveries 
received by Stinson’s and 
the Stinson’s employee who 
received the orders during 
some recent week, we will 
use the Detailed Query 
option.

650 
Appendix B Data Management and Microsoft Access
Figure B.34   SECOND STEP OF THE SIMPLE QUERY WIZARD AND THE 
 SUMMARY OPTIONS DIALOG BOX
Figure B.33   FIRST STEP OF THE SIMPLE QUERY WIZARD
A query can be saved and 
used repeatedly. A saved 
query can be modified to 
suit the needs of future 
users.

 
B.4 Queries 
651
Figure B.35   FINAL STEP OF THE SIMPLE QUERY WIZARD
Figure B.36   DISPLAY OF RESULTS OF A SIMPLE QUERY

652 
Appendix B Data Management and Microsoft Access
it shows a list of all queries associated with this database by using the Navigation Panel’s 
pull-down menu of options as shown in Figure B.37.
Action Queries
Suppose that in reviewing the database system we are designing, Stinson’s sales manager 
notices that we have made an error in the table TblSalesPrices. She shares with us that the 
price she charges for a keg of beer that has been produced by the Midwest Fiddler Crab 
microbrewery (value of 7 for BrewerID) should be $240, not the $230 we have entered 
in this table. We can use an action query applied to the table TblSalesPrices to quickly 
perform these changes. Because we want to modify all values of a field that meet some 
criteria, this is an update query. The Datasheet view of the table TblSalesPrices is provided 
in Figure B.38.
To make this pricing change, we take the following steps:
Step 1. Click the CREATE tab in the Ribbon
Step 2. Click Query Design in the Queries group. This opens the Query Design 
window and the QUERY TOOLS contextual tab (Figure B.39)
You can open the file 
 Stinsons and follow these 
steps to reproduce the 
results of this query.
Figure B.37   PULL-DOWN MENU OF OPTIONS IN THE NAVIGATION PANEL
Figure B.38   DATASHEET VIEW OF TBLSALESPRICES

 
B.4 Queries 
653
Step 3. When the Show Table dialog box appears, select TblSalesPrices and click Add
Click Close
Step 4. In the TblSalesPrices box, double-click on KegSalesPrice. This opens a col-
umn labeled KegSalesPrice in the Field: row at the bottom pane of the display
Click Update, 
, in the Query Type group of the DESIGN tab 
Enter 240 in the Update To: row of the KegSalesPrice column in the 
bottom pane of the display
Step 5. In the TblSalesPrices box, double-click on BrewerID to open a second col-
umn in the bottom pane of the display labeled BrewerID
Enter 7 in the Criteria: row of the BrewerID column (Figure B.40)
Step 6. Click the Run button  in the Results group of the DESIGN tab
When the dialog box alerting us that we are about to update one row of 
the table appears, click Yes
Once we click Yes in the dialog box, the price charged to Stinson’s for a keg of beer 
supplied by the Midwest Fiddler Crab microbrewery (BrewerID equal to 7) in the table 
TblSalesPrices is changed from $230.00 to $240.00.
Step 7. To save this query, click the Save icon 
 in the Quick Access toolbar
When the Save As dialog box opens (Figure B.41), enter the name Change 
Price per Keg Charged by a Microbrewery for Query Name:
Click OK
This step produces the 
TblSalesPrices box that 
contains a list of fields in 
this table.
Once saved, a query can be 
modified and can be saved 
to use later.
Figure B.39   QUERY TOOLS CONTEXTUAL TAB
Figure B.40   DISPLAY OF INFORMATION FOR THE UPDATE QUERY
Field:
Table:
Update To:
Criteria:
or:
KegSalesPrice
TblSalesPrices
240
BrewerID
TblSalesPrices
7
Action queries perma-
nently change the data in 
a database, so we suggest 
you back up the database 
before performing an 
action query. After you 
have reviewed the results 
of the action query and 
are satisfied that the query 
worked as desired, you can 
then save the database with 
the results of the action 
query. Some cautious users 
save the original database 
under a different name so 
that they can revert to the 
original preaction query 
database if they find later 
that the action query has 
had an undesirable effect 
on the database.
Figure B.41   SAVE AS DIALOG BOX

654 
Appendix B Data Management and Microsoft Access
Opening the table TblSalesPrices in Datasheet view (Figure B.42) shows that the price of 
a keg charged to Stinson’s for a keg of beer supplied by the Midwest Fiddler Crab micro-
brewery (BrewerID equal to 7) has been revised from $230 to $240.
Crosstab Queries
We use crosstab queries to summarize data in one field by values of one or more other 
fields. In our example, we will consider an issue faced by Stinson’s inventory manager, 
who wants to know how many kegs and cases of beer have been ordered by each Stinson’s 
employee from each microbrewery. To provide the manager with this information, we 
apply a crosstab query to the table TblSOrders (shown in the original database design il-
lustrated in Figure B.1) to create a table that shows number of kegs and cases ordered by 
each Stinson’s employee from each microbrewery. To create this crosstab query, we take 
the following steps:
Step 1. Click the CREATE tab in the Ribbon
Step 2. Click Query Design in the Queries group. This opens the Query Design 
window and the QUERY TOOLS contextual tab
Step 3. When the Show Table dialog box opens, select TblSOrders, click Add, then 
click Close
Step 4. In the TblSOrders box, double-click BrewerID, Keg or Case? and  SQuantity 
Ordered to add these fields to the columns in the lower pane of the window
Step 5. In the Query Type group of the DESIGN tab, click Crosstab 
Step 6. In the BrewerID column of the window’s lower pane, 
Select Row Heading in the Crosstab: row
Select Ascending in the Sort: row
Step 7. In the Keg or Case? column of the window’s lower pane, 
Select Column Heading in the Crosstab: row
Select Ascending in the Sort: row
Step 8. In the SQuantity Ordered column of the window’s lower pane, 
Select Sum in the Total: row
Select Value in the Crosstab: row
Figure B.43 displays the results of these steps for setting up our crosstab query.
Step 9. In the Results group of the DESIGN tab, click the Run button, , to execute 
the crosstab query
You can open the file 
 Stinsons and follow these 
steps to reproduce the 
results of this query.
Step 3 produces the 
TblSOrders box in Access 
that contains a list of fields 
in this table.
Figure B.42   TBLSALESPRICES IN DATASHEET VIEW AFTER RUNNING THE 
UPDATE QUERY

 
B.5 Saving Data to External Files 
655
The results of the crosstab query appear in Figure B.44. From Figure B.44, we see that 
we have ordered 8 cases and 10 kegs of beer from the microbrewery with a value of 3 for 
the BrewerID field (the Oak Creek Brewery).
Step 10. To save the results of this query, click the Save icon, 
, in the Quick Access 
toolbar
When the Save As dialog box opens, enter Brewer Orders Query for 
Query Name: 
Click OK
Figure B.43   DISPLAY OF DESIGN OF THE CROSSTAB QUERY
Field:
Table:
Total:
Crosstab:
Sort:
Criteria:
or:
BrewerID
TblSOrders
Group By
Row Heading
Ascending
Keg or Case?
TblSOrders
Group By
Column Heading
Ascending
SQuantity Ordered
TblSOrders
Sum
Value
In the first column we have 
indicated we want values 
of the field BrewerID to act 
as the row headings of our 
table (in ascending order), 
whereas in the second 
column we have indicated 
that we want values of the 
field Keg or Case? to act 
as the column headings 
of our table (again, in 
ascending order). In the 
third column, we have 
indicated that values of the 
field SQuantity Ordered 
will be summed for every 
combination of row (value 
of the field BrewerID) and 
column (value of the field 
Keg or Case?). Figure B.44 
provides a summary of 
these entries.
Figure B.44   RESULTS OF CROSSTAB QUERY
Crosstab queries do not 
permanently change the 
data in a database.
NOTES AND COMMENTS
The Make Table, Append, and Delete action que-
ries work in manners similar to Update action 
queries and are also useful ways to modify tables 
to better suit the user’s needs.
Saving Data to External Files
Access can export data to external files in formats that are compatible with a wide variety 
of software. To export the information from the table TblSOrders to an external Excel file, 
we take the following steps:
Step 1. Click the EXTERNAL DATA tab in the Ribbon
Step 2. In the Navigation Panel, click TblSOrders
B.5
You can open the file 
 Stinsons and follow these 
steps to reproduce an 
 external Excel file of the 
data in TblSOrders.

656 
Appendix B Data Management and Microsoft Access
Step 3. In the Export group of the EXTERNAL DATA tab, click the Excel icon, 
Step 4. When the Export—Excel Spreadsheet dialog box opens (Figure B.45), click 
the Browse… button 
Find the destination where you want to save your exported file
In the File Name: box, enter the name of your exported file 
(TblSOrders.xlsx in this example)
Verify that the File format: is set to Excel Workbook (*.xlsx)
Select the check boxes for Export data with formatting and layout. and 
Open the designation file after the export operation is complete.
Click OK
At this point another dialog 
box asks us if we want to 
save the steps we used to 
export the information 
in this table; this can be 
handy to have later if we 
have to export similar data 
again.
Figure B.45   EXPORT—EXCEL SPREADSHEET DIALOG BOX
NOTES AND COMMENTS
1. Exporting information to an external file is par-
ticularly useful when applied to tables that have 
been linked or to query results.
2. Exporting information from a relational da-
tabase such as Access to Excel allows one to 
apply the tools and techniques covered through-
out this textbook to a subset of a large data set. 
This can be much more efficient than trying to 
use Excel to clean and filter large data sets.

 
Glossary 
657
Summary
The amount of data available for analyses is increasing at a rapid rate, and this trend will not 
change in the foreseeable future. Furthermore, the data used by organizations to make deci-
sions are dynamic, and they change rapidly. Thus, it is critical that a data analyst understand 
how data are stored, revised, updated, retrieved, and manipulated. We have reviewed tools 
in Microsoft Access® that can be used for these purposes.
In this appendix we have reviewed basic concepts of database creation and manage-
ment that are important to consider when using data from a database in an analysis. We 
have discussed several ways to create a database in Microsoft Access®, and we have 
demonstrated Access tools for preparing data in an existing database for analysis. These 
include tools for reading data from external sources into tables, creating relationships 
between tables, sorting and filtering records, designing and executing queries, and saving 
data to external files.
Glossary
Database A collection of logically related data that can be retrieved, manipulated, and 
updated to meet a user or organization’s needs.
Table Data arrayed in rows and columns (similar to a worksheet in an Excel spreadsheet) 
in which rows correspond to records and columns correspond to fields.
Records The individual units from which the data for a database have been collected.
Fields The variables or characteristics for which data have been collected from the records.
Form An object that is created from a table to simplify the process of entering data.
Query A question posed by a user about the data in the database.
Report Output from a table or a query that has been put into a specific prespecified format.
Primary key field A field that must have a unique value for each record in the table and is 
used to identify how records from several tables in a database are logically related.
Foreign key field A field that is permitted to have multiple records with the same value.
Leszynski/Reddick guidelines A commonly used set of standards for naming database 
objects.
Datasheet view A view used in Access to control a database; provides access to tables, 
reports, queries, forms, etc. in the database that is currently open. This view can also be 
used to create tables for a database.
Design view A view used in Access to define or edit a database table’s fields and field 
properties as well as rearrange the order of the fields in the database that is currently open.
One-to-many Sometimes abbreviated as 1:∞, a relationship between tables for which a 
value in the common field for a record in one table (say, Table A) can match the value in 
the common field for multiple records in another table (say, Table B), but a value in the 
common field for a record in Table B can match the value in the common field for at most 
a single record in Table A.
One-to-one Sometimes abbreviated as 1:1, a relationship between tables for which a value 
in the common field for a record in one table (say, Table A) can match the value in the 
 common field for at most one record in another table (say, Table B), and value in the 
 common field for a record in Table B can match the value in the common field for at most 
a single record in Table A.
Many-to-many Sometimes abbreviated as ∞:∞, a relationship for which a value in the 
 common field for a record in one table (say, Table A) can match the value in the common 
field for multiple records in another table (say, Table B), and a value in the common field for 
a record in Table B can match the value in the common field for several records in Table A.

658 
Appendix B Data Management and Microsoft Access
Entity integrity The rule that establishes that a table has no duplicate records. Entity integ-
rity can be enforced by assigning a unique primary key to each record in a table.
Referential integrity The rule that establishes the proper relationship between two tables.
Orphaned A record in a table that has a value for the foreign key field of a table that does 
not match the value in the primary key field for any record of a related table. Enforcing 
referential integrity prevents the creation of orphaned records.
Select queries Queries that are used to extract the subset of data that satisfy one or more 
criteria from a table.
Action queries Queries that are used to change data in existing tables. The four types of 
action queries available in Access are update, make table, append, and delete.
Crosstab queries Queries that are used to summarize data in one field across the values of 
one or more other fields.

Longley, P. A., M. Goodchild, D. J. Maguire, and D. W. Rhind. 
Geographic Information Systems and Science. Wiley, 2010.
The Pew Research Center, Internet & American Life Project, 
2011.
Robbins, N. B. Creating More Effective Graphs. Wiley, 2004.
Telea, A. C. Data Visualization Principles and Practice. A. K. 
Peters, 2008.
Tufte, E. R. Envisioning Information. Graphics Press, 1990.
Tufte, E. R. Visual and Statistical Thinking: Displays of Evi-
dence for Making Decisions. Graphics Press, 1997.
Tufte, E. R. Visual Explanations: Images and Quantities, Evi-
dence and Narrative. Graphics Press, 1997.
Tufte, E. R. The Visual Display of Quantitative Information, 
2nd ed. Graphics Press, 2001.
Tufte, E. R. Beautiful Evidence. Graphics Press, 2006.
Wong, D. M. The Wall Street Journal Guide to Information 
Graphics. Norton, 2010.
Young, F. W., P. M. Valero-Mora, and M. Friendly. Visual Sta-
tistics: Seeing Data with Dynamic Interactive Graphics. 
Wiley, 2006.
Decision Analysis
Clemen, R. T., and T. Reilly. Making Hard Decisions with 
 DecisionTools. Cengage Learning, 2004.
Golub, A. L. Decision Analysis: An Integrated Approach. 
Wiley, 1997.
Goodwin, P., and G. Wright. Decision Analysis for Manage-
ment Judgment, 4th ed. Wiley, 2009.
Peterson, M. An Introduction to Decision Theory. Cambridge, 
2009.
Pratt, J. W., H. Raiffa, and R. Schlaiter. Introduction to Statis-
tical Decision Theory. MIT Press, 2008.
Raiffa, H. Decision Analysis. McGraw-Hill, 1997.
Time Series and Forecasting
Bowerman, B. L., and R. T. O’Connell. Forecasting and Time 
Series: An Applied Approach, 3rd ed. Brooks/Cole, 2000.
Box, G. E. P., G. M. Jenkins, and G. C. Reinsel. Time Series 
Analysis: Forecasting and Control, 4th ed. Wiley, 2008.
Hanke, J. E., and D. Wichern. Business Forecasting, 9th ed., 
Prentice Hall, 2009.
Makridakis, S. G., S. C. Wheelwright, and R. J. Hyndman. 
Forecasting Methods and Applications, 3rd ed. Wiley, 1997.
Ord, K., and R. Fildes. Principles of Business Forecasting, 1st 
ed. Cengage Learning, 2013.
Wilson, J. H., B. Keating, and John Galt Solutions, Inc. Busi-
ness Forecasting with Accompanying Excel-Based Fore-
cast X™, 5th ed. McGraw-Hill/Irwin, 2007.
Data Management 
and Microsoft Access
Adamski, J. J., and K. T. Finnegan. New Perspectives on 
 Microsoft® Access 2010, Comprehensive, 1st ed. Cengage 
Learning, 2011.
Alexander, M. The Excel Analyst’s Guide to Access, Wiley, 
2010.
Balter, A. Using Microsoft Access 2010. Que Publishing, 
2010.
Carter, J., and J. Juarez. Microsoft Office Access 2010: A Les-
son Approach, Complete, 1st ed. McGraw-Hill, 2011.
Friedrichsen, L. Microsoft® Access 2010: Illustrated Com-
plete, 1st ed. Cengage Learning, 2011.
Jennings, R. Microsoft Access 2010 in Depth, 1st ed. Que Pub-
lishing, 2010.
Shelly, G. B., P. J. Pratt, and M. Z. Last. Microsoft® Access 
2010: Comprehensive, 1st ed. Cengage Learning, 2011.
Data Mining
Linoff, G. S., and M. J. Berry. Data Mining Techniques: For 
Marketing, Sales, and Cstomer Relationship Management, 
3rd ed. Wiley, 2011.
Berthold, M., and D. J. Hand. Intelligent Data Analysis. 
Springer (Berlin), 1999.
Hand, D. J., H. Mannila, and P. Smyth. Principles of Data 
Mining. MIT Press, 2001.
Hastie, T., R. Tibshirani, and J. Friedman. The Elements of 
Statistical Learning, 2nd ed. Springer (Berlin), 2009.
Schmueli, G., N. R. Patel, and P. C. Bruce. Data Mining for 
Business Intelligence, 2nd ed. Wiley, 2010.
Tan, P.-N., M. Steinbach, and V. Kumar, Introduction to Data 
Mining. Addison-Wesley, 2006.
Data Visualization
Alexander, M., and J. Walkenbach. Excel Dashboards and 
 Reports. Wiley, 2010.
Cleveland, W. S. Visualizing Data. Hobart Press, 1993.
Cleveland, W. S. The Elements of Graphing Data, 2nd ed. 
 Hobart Press, 1994.
Entrepreneur, 2012 Annual Ranking of America’s Top 
 Franchise Opportunities, 2012.
Few, S. Show Me the Numbers: Designing Tables and Graphs 
to Enlighten. Analytics Press, 2004.
Few, S. Information Dashboard Design: The Effective Visual 
Communication of Data. O’Reilly Media, 2006.
Few, S. Now You See It: Simple Visualization Techniques for 
Quantitative Analysis. Analytics Press, 2009.
References

660 
References
Draper, N. R., and H. Smith. Applied Regression Analysis, 
3rd ed. Wiley, 1998.
Graybill, F. A., and H. K. Iyer. Regression Analysis: Concepts 
and Applications. Wadsworth, 1994.
Hosmer, D. W., and S. Lemeshow. Applied Logistic Regres-
sion, 2nd ed. Wiley, 2000.
Kleinbaum, D. G., L. L. Kupper, and K. E. Muller. Applied 
Regression Analysis and Multivariate Methods, 4th ed. 
Cengage Learning, 2007.
Mendenhall, M., T. Sincich., and T. R. Dye. A Second Course 
in Statistics: Regression Analysis, 7th ed. Prentice Hall, 
2011.
Montgomery, D. C., E. A. Peck, and G. G. Vining. Introduc-
tion to Linear Regression Analysis, 5th ed. Wiley, 2012. 
Neter, J., W. Wasserman, M. H. Kutner, and C. Nashtsheim. 
Applied Linear Statistical Models, 5th ed. McGraw-Hill, 
2004.
Monte Carlo Simulation
Law, A. M. Simulation Modeling and Analysis, 4e. McGraw-
Hill, 2006.
Ross, S. Simulation. Academic Press, 2013.
Savage, S. L. Flaw of Averages. Wiley, 2012.
Wainer, H. Picturing the Uncertain World. Princeton Univer-
sity Press, 2009.
Winston, W. Decision Making Under Uncertainty. Palisade 
Corporation, 2007.
Spreadsheet Modeling
Leong, T., and M. Cheong. Business Modeling with Spread-
sheets Problems, Principles, and Practice, 2e. McGraw-
Hill (Asia), 2010.
Powell, S. G., and R. J. Batt. Modeling for Insight. Wiley, 
2008.
Winston, W. Excel 2010 Data: Analysis and Business Model-
ing. Microsoft Press, 2011.
General Business Analytics 
Ayres, I. Super Crunchers: Why Thinking-by-Numbers Is the 
New Way to Be Smart. Bantam, 2008.
Baker, S. The Numerati. Mariner Books, 2009.
Davenport, T. H., and J. G. Harris, Competing on Analytics. 
Harvard Business School Press, 2007.
Davenport, T. H., J. G. Harris. and R. Morrison, Analytics at 
Work. Harvard Business School Press, 2010.
Thomas Davenport, Ed. Enterprise Analytics. FT Press, 2012.
Fisher, M., and A. Raman. The New Science of Retailing. 
 Harvard Business Press, 2010.
Lewis, M. Moneyball: The Art of Winning an Unfair Game. 
Norton, 2004.
Wind, J., P. E. Green, D. Shifflet, and M. Scarbrough. 
“Courtyard by Marriott: Designing a Hotel Facility with 
 Consumer-Based Marketing Models,” Interfaces 19. no. 1 
(January–February 1989): 25–47.
Optimization
Baker, K. R. Optimization Modeling with Spreadsheets, 2nd 
ed. Wiley, 2011.
Bazaraa, M. S., H. D. Sherali, and C.M. Shetty. Nonlinear 
Programming: Theory and Algorithms. Wiley Interscience, 
2006.
Bazaraa, M. S., J. J. Jarvis, and H. D. Sherali. Linear Pro-
gramming and Network Flows. Wiley, 2009.
Chen, D., R. G. Batson, and Y. Dang. Applied Integer Pro-
gramming. Wiley, 2010.
Sashihara, S. The Optimization Edge. McGraw-Hill, 2011.
Winston, W. L. Financial Models Using Simulation and Opti-
mization, 2nd ed. Palisade Corporation, 2000.
Regression Analysis
Chatterjee, S., and A. S. Hadi. Regression Analysis by Exam-
ple, 4th ed. Wiley, 2006.

Index
A
Absolute references, 335–336, 360–364
Access (Microsoft)
Action queries, 648, 652–654
Crosstab queries, 648, 654–655
database applications, 621–639
data export to external files, 655–656
Datasheet view, 627–639
Design view in, 630–639
entity integrity in, 640
Excel software applications in, 634–639
Field Names limitations in, 630
foreign key field, 624
Join Properties box in, 643
Leszynski/Reddick guidelines, 626
Many-to-Many relationships in, 640
One-to-Many relationships in, 639
One-to-One relationships in, 639–640
orphaned records in, 640
primary key field, 623–624
queries in, 622, 647–655
records sorting and filtering, 643–647
referential integrity in, 640
Replication ID field, 623
Select queries, 648–652
table relationships in, 639–643
Action queries, in Access (Microsoft), 648, 652–654
Additivity, linear optimization, 357
Advanced analytics
characteristics of, 9
defined, 14
Advanced charts, data visualization, 103–104
Advertising campaign planning, linear  
optimization, 381–386
All-integer linear program, 406–407
All-time movie box office data, 118–119
Alternative optimal solutions, linear programming  
problems, 367–368
generation of, 386–388
Alumni giving case study, linear regression, 197–198
Analytic Solver Platform (ASP)
decision tree construction, 596–606
integer linear optimization, 442–446
linear optimization models, 399–403
Monte Carlo simulation, 498–518
Land Shark example, 499–506
model optimization, 520–524
random variables dependence, 537–544
Zappos retailer problem, 506–518
Trial execution and output analysis, 517–518
nonlinear optimization problems, 480–484
Antecedents, association rules, 266–269
Applecore children’s clothing case study, integer linear 
programming, 441–442
Arithmetic mean
defined, 8
measures of location, 35–36
Association rules
data mining, 265–269
evaluation of, 269
Auditing, spreadsheet models, 339–343
Autoregressive models, regression-based forecasting, 228
Average error, prediction accuracy, 277
Average group linkage, hierarchical clustering, 259–262
Average linkage, hierarchical clustering, 259–262
B
Backward selection, regression models, 178–179
Bank location problem, binary variables, integer linear 
programming, 420–423
Bar charts
data visualization, 90–93
multiple variables (clustered column chart), 98
Bass forecasting model, nonlinear optimization, 465–469
Bayes’ theorem, decision tree branch probabilities, 568–571
Bernoulli distribution, random variables, 547–548
Best-case scenario, Monte Carlo simulation, what-if  
analysis, 487–488
Best-subsets procedure, variable selection, 178–179
Beta distribution, random variables, 546
Big data, 8
defined, 14
Binary variables, integer linear programming
applications, 415–426
bank location example, 420–423
capital budgeting, 415–416
defined, 407
fixed costs, 416–420
flexibility modeling, 426–428
optimization alternatives, 428–430
product design and market share optimization, 424–426
Solver parameters, 412–415
Binding constraints
linear optimization model, sensitivity analysis, 373–374
linear program solutions, 363–364
Binomial distribution, random variables, 548

662 
Index
Bins, frequency distributions
limits, 29–30
number of, 28
width of, 29
Box plots
distribution analysis, 49–50, 58
in XLMiner, 67–69
Branch-and-bound algorithm, integer linear  
programming, 410
Branch sequences
classification and regression trees, 284–285
decision trees, 553–555
Analytic Solver Platform construction, 597–598
Breakpoint model, piecewise linear regression, 170–173
Bubble charts, 93–95
Business analytics
applications, 9–13
defined, 4, 14
overview, 2–4
C
Capital budgeting, binary variables, integer linear 
programming, 415–416
Categorical data
classification and regression trees, 284–298
defined, 18
frequency distributions, 25–27
Categorical independent variables, linear regression, 161–165
Causal forecasting, 204
regression analysis models, 231–235
combined causal variables and trend/seasonality  
effects, 235
Central limit theorem, 517–518
Chance events, problem formulation, 552–553
Chance nodes, decision trees, 553–555
Analytic Solver Platform addition of, 599–600
Charts
advanced charts, 103–104
bar charts and column charts, 90–93
bubble charts, 93–95
data visualization with, 85–102
geographic information systems, 104–105
heat maps, 95–97
line charts, 87–90
for multiple variables, 97–100
pie charts, 93
PivotCharts (Excel), 101–102
regression equation estimation, Excel chart tools, 132–133
scatter charts, 85–87
3-D charts, 93
CHOOSE function (Excel), 604
Class 0 error rate, classification accuracy, 274–277
Class 1 error rate, classification accuracy, 274–277
Classification
accuracy, supervised learning, 273–277
errors, logistic regression models, 305–307
k-nearest neighbors classification, 277–283
probabilities, 274–277
Classification and regression trees (CART), 283–298
Classification confusion matrix
classification and regression trees, 291–298
k-nearest neighbors classification, 279–283
supervised learning, 273–277
Cluster analysis
data mining, 256–265
hierarchical clustering, 256, 258–262
k-means clustering, 262–265
observation similarities, 256–258
strength disparities, 262
Clustered column chart, 98
Coefficient of determination
inference and sample size, 158–160
logistic regression, 301
multiple regression, 140–143
simple linear regression model, 136–137
Coefficient of variation, measures of variability, 46
Column charts, 90–93
Complete linkage, hierarchical clustering, 259–262
Component ordering, spreadsheet models, 334–336
Computer-generated random numbers, Monte Carlo 
simulation, 491–495
Concave function, nonlinear optimization  
models, 455–459
Conditional constraints, binary variables, integer linear 
programming, 427–428
Conditional data formatting, in Excel, 23–25, 333
Conditional probability, 568–571
Confidence, association rules, 266–269
Confidence intervals. See also Interval estimation
inference, individual regression parameters, 152–153
Confidence level, inference, individual regression  
parameters, 152–153
Conjoint analysis, integer linear programming, 424–426
Consequent items, association rules, 266–269
Conservative approach, nonprobability decision  
analysis, 555
Constrained problem, nonlinear optimization  
models, 450–453
Constraints
integer linear programming, 407–410
binary variables, 426–428
product design and market share optimization, 424–426
linear optimization model
advertising campaign planning, 383–384
binding constraints, 363–364
problem formulation, 355–357
sensitivity analysis, 372–374
simple minimization problem, 364–367
linear optimization models, 353–354
Continuous outcome prediction, regression trees, 291–298
Continuous probability distribution, random variables, 
545–549
Controllable inputs, Monte Carlo simulation, 487

 
Index 
663
Convex function, nonlinear optimization models, global and 
local optima, 456–459
Convex hull, 409–410
Corequisite constraints, 427–428
Correlation coefficient
Monte Carlo simulation
matrix construction, 540–544
random variables dependence, 537–544
Pearson product moment correlation coefficient
measures of association, two variables, 55–56, 58
Monte Carlo simulation, 537–544
COUNTIF function (Excel), 26, 333–336
Covariance, measures of association, two  
variables, 52–55, 58
Cross-sectional data, defined, 18
Crosstab queries, in Access (Microsoft), 648, 654–655
Crosstabulation, table designs, 79–80
Cumulative frequency distribution, data analysis, 34–35
Cumulative lift chart, classification accuracy, 276–277
Custom discrete distribution, random variables, 549
Customer relationship management (CRM), linear regression 
applications, 124
Custom general distribution, random variables, 547
Cutoff value
classification accuracy, 274–277
classification and regression trees, 287
Cutting plane approach, integer linear  
programming, 410
Cyclical patterns, time series analysis, 211–212
D
Data
in Access (Microsoft), export to external  
files, 655–656
cross-sectional and time series data, 16
distributions from, 25–35
cumulative distributions, 34–35
frequency distributions
categorical data, 25–27
quantitative data, 28–31
histograms, 31–34
relative and percent frequency distribution, 27–28
Dow Jones example, 17
Excel modification, 21–25
conditional formatting, 23–25
sorting and filtering, 21–23
overview, 16–17
population and sample data, 17–18
quantitative and categorical data, 18
sources, 18–19
Data-based multicollinearity, independent variable 
interactions, 176–177
Databases
Access (Microsoft) and basic applications, 621–639
defined, 621
design issues, 624–639
Data dashboards
applications, 106–108
Cincinnati Zoo & Botanical Garden  
case study, 71–73
data visualization with, 105–108
descriptive analytics, 5–6, 14
key performance indicators, 106
Data-driven decision making, overview, 2–3
Data-ink ratio, 73–75
Data mining
analytics case study, 252
association rules, 265–269
data preparation, 254–255
data sampling, 252–253
defined, 6, 14
Grey Code Corporation case study, 319
supervised learning
classification accuracy, 273–277
classification and regression trees, 283–298
data partitioning, 269–273
defined, 252
k-nearest neighbors method, 277–283
logistic regression model, 299–307
prediction accuracy, 277
unsupervised learning, 255–269
association rules, 265–269
cluster analysis, 256–265
Data partitioning
classification and regression trees, 285–298
supervised learning, 269–273
Data preparation
data mining, 253, 254–255
missing data treatment, 254
outliers and erroneous data, 254–255
variable representation, 254–255
Data query, classification, 6, 14
Data sampling, data mining, 252–253
Datasheet view, Access (Microsoft), 627–639
Data Table (Excel), what-if analysis, 327–330
Monte Carlo simulation, 488
Data visualization
advanced techniques, 102–105
all-time movie box office data, 118–119
charts, 85–102
Cincinnati Zoo & Botanical Garden case  
study, 71–73
data dashboards, 105–108
design techniques, 73–75
overview, 73–75
tables, 75–85
XLMiner parallel coordinates plot, 120–122
XLMiner scatter chart matrix, 119–120
Decile-wise lift chart, classification accuracy, 276–277
Decision alternatives
Analytic Solver Platform construction and  
naming of, 598
problem formulation, 552–553

664 
Index
Decision analysis
analytics case study, 551
Bayes’ theorem, 568–571
characteristics of, 551–552
defined, 14
nonprobability, 554–557
conservative approach, 555
minimax regret approach, 555–557
optimistic approach, 554–555
predictive analytics, 7
probabilities, 557–561
expected value approach, 557–559
risk analysis, 559–560
sensitivity analysis, 560–561
problem formulation, 552–554
decision trees, 553–554
payoff tables, 553
property purchase strategy case study, 595–596
sample information, 561–568
expected value, 566
perfect information, expected value, 567–569
utility theory, 571–581
exponential utility functions, 580–581
utility functions, 577–580
Decision making, overview, 4–5
Decision nodes, 553–555
Decision strategy, sample information, 563–568
Decision trees
Analytic Solver Platform construction, 596–606
applications of, 554–555
branch probabilities, Bayes’ theorem, 568–571
probability-based decision analysis, 558–559
sample information, decision analysis, 561–568
Decision variables
integer linear programming, 407–410
Solver parameters, 411–415
linear optimization model
problem formulation, 355–357
simple minimization problem, 364–367
transportation planning, 379–381
nonlinear optimization, new product adoption, Bass 
forecasting model, 465–469
nonlinear optimization local optima, 458–459
spreadsheet design, 325
Demand estimates
decision analysis, payoffs/payoff  
tables, 553–554
Monte Carlo simulation
random variables dependence, 538–544
Zappos retailer problem, 513–515
Dendrograms, hierarchical clustering, 260–262
Dependents (Excel), spreadsheet auditing, 339–340
Dependent variables
defined, 125
inference and regression, least squares  
model, 144–148
Descriptive analytics, classification, 5–6, 14
Descriptive statistics
case study, 66–67
cross-sectional and time series data, 18
data definitions and goals, 16–17
data distribution creation, 25–35
data sources, 18–19
distribution analysis, 44–50
Excel data modification, 21–25
measures of association between two  
variables, 51–56
measures of location, 35–40
measures of variability, 40–44
population and sample data, 17–18
quantitative and categorical data, 18
U.S. Census Bureau, 16
Design view in Access (Microsoft), 630–639
Deterministic nonlinear optimization, Monte Carlo 
simulation, 518–519
Dimension reduction, 254–255
Discrete probability distribution, 490
random variables, 547–549
Discrete uniform distribution, random variables, 549
Distribution analysis
box plot, 49–50
data, 25–35
cumulative distributions, 34–35
frequency distributions
categorical data, 25–27
quantitative data, 28–31
histograms, 31–34
relative and percent frequency distribution, 27–28
empirical rule, 48
Monte Carlo simulation, 486–487
outlier identification, 48–49
percentiles, 44–45
quartiles, 45–46
z-scores, 46–48
Divisibility, linear optimization, 357
Double-subscripted decision variables, linear optimization 
model, transportation planning, 379–381
Dow Jones Industrial Index, data for, 17
Dummy variables
linear regression, 161–165
piecewise linear regression models, 171–173
regression-based forecasting, seasonality  
trends, 228–231
E
Efficient frontier, nonlinear optimization  
models, 464–465
Einstein, Albert, 4
Empirical rule, 48, 58
Enterprise resource planning (ERP) software, forecasting 
applications, 204
Entity integrity, in Access (Microsoft), 640
Erroneous data, data preparation, 254

 
Index 
665
Error checking (Excel), spreadsheet auditing, 341–342
Error rates, classification and regression trees, 287
Error term. See also Sums of squares due to error (SSE)
simple linear regression model, 125–127
Estimated regression equation, 126–127
deviations, 135–137
multiple regression, 138–143
categorical independent variables, 163–165
nonlinear relationships modeling, 165–177
Euclidean distance
cluster analysis, 256–258
nonlinear optimization models, location problems, 459–461
Evaluate formulas (Excel), spreadsheet auditing, 340–341
Excel software functions. See also specific functions,  
e.g. VLOOKUP
Access (Microsoft) applications using, 634–639
Analytic Solver Platform
decision tree construction, 596–606
integer linear optimization, 442–446
linear optimization models, 399–403
Monte Carlo simulation, 499–518, 537–544
nonlinear optimization problems, 480–484
basic applications, 609–613
Chart tools, regression equation estimation, 132–133
CHOOSE function, 604
Data Analysis tools, moving averages  
forecasting, 219–221
data modification, 21–25
conditional formatting, 23–25
sorting and filtering, 21–23
exponential smoothing applications, 222–225
frequency distributions, audit times data, 29–31
FREQUENCY function, 30–31, 498
COUNTIF function, 26, 333–336
IF function
Monte Carlo simulation, 499–500
spreadsheet models, 333–336
Monte Carlo simulation, 488–498
random variables generation, 491–495
simulation trial execution, 495–498
Multistart options, nonlinear optimization local optima, 
457–459
NORMINV function, 493–495
PivotCharts, 101–102
PivotTables, 80–85
Regression tool
estimated multiple regression equation, 140–143
histograms, categorical independent  
variables, 161–165
individual regression parameters, 151–153
inference and regression models, 147–149
nonsignificant independent variables, multiple 
regression, 153–154
Solver tool
binary variables, integer linear programming, 415–426
infeasible problems, 367–370
integer linear programming solutions, 410–415
Integer Optimality feature, 415
linear program solutions, 360–367
nonlinear optimization, financial analytics, 463–465
nonlinear optimization local optima, 457–459
nonlinear program solutions, 453
sensitivity report, 372–374
unbounded solutions, 370–371
spreadsheet auditing, 339–343
spreadsheet basics, 613–618
spreadsheet models, 332–338
STDEV function, 43–44, 496–498
SUM and SUMPRODUCT functions, 332–333, 360–367, 
416, 453, 520
VLOOKUP function
Monte Carlo simulation, random variable  
generation, 494–495
spreadsheet models, 336–338
what-if analysis tools
data tables, 327–330
Goal Seek tool, 331–332
tools, 327–332
Expected monetary value, probability-based decision  
analysis, 557–559
Expected utility, decision analysis, 575–577
Expected value approach, decision analysis
perfect information, 567–568
probability-based decision analysis, 557–559
sample information, 566–568
Expected value without perfect information (EVwoPI), 
decision analysis, 567–568
Experimental region, least squares method, 130–132
Experimental study, data sources, 18–19
Exploratory forecasting, 204
Exponential distribution, random variables, 546
Exponential smoothing, forecasting  
applications, 221–225
Exponential utility function
Analytic Solver Platform applications, 604–606
decision analysis, 580–581
External files, Access (Microsoft) data  
export to, 655–656
Extrapolation
least squares method, 130–132
regression-based causal forecasting, 234–235
Extreme points
linear optimization model solutions, 358–364
nonlinear program solutions, constrained  
problem, 452–453
F
False negatives, classification accuracy, 273–277
False positives, classification accuracy, 273–277
Feasible regions
linear optimization modeling, 358–364
nonlinear optimization models, constrained problem, 
450–453

666 
Index
Feasible solutions, linear optimization modeling, 358–364
Field Names limitations, in Access (Microsoft), 630
Filtering of data
in Access (Microsoft), 643–647
in Excel, 21–23
Financial analytics
applications, 9
investment portfolio selection
linear programming, 375–377
strategy case study, 398–399
nonlinear optimization models
Markowitz mean-variance portfolio, 461–465
transaction costs, portfolio optimization, 477–480
utility theory, 571–581
Fixed cost
binary variables, integer linear programming, 416–420
defined, 322
mathematical model, 322–324
spreadsheet design, 324–325
Flexibility modeling, binary variables, integer linear 
programming, 426–428
Food and beverage sales case study, forecasting, 246
Forecast error, measurement of, 213–217
Forecasting. See also Time series analysis
accuracy, 212–217
exponential smoothing applications, 224–225
moving averages forecasting, 221
analytics sample, 203
definition and classification, 203–204
exponential smoothing, 221–225
food and beverage sales case study, 246
model selection and criteria, 236–237
moving averages, 217–221
nonlinear optimization models, new product  
adoption, 465–469
regression analysis, 226–236
causal forecasting, 231–235
combined causal, trend, and seasonality effects, 235
limitations of, 235–236
linear trend projection, 226–228
seasonality, 228–231
Foreign key field, Access (Microsoft), 624
Forward selection, variable selection, regression models, 
178–179
Four Corners case study, Monte Carlo simulation, 536–537
Frequency distributions
categorical data, 25–27
quantitative data, 28–31
relative and percent frequency distribution, 27–28
FREQUENCY function (Excel), 30–31, 498
F test
independent variable interactions, 174–177
inference
overall regression relationship, 150–153
sample size and, 158–160
nonlinear relationships modeling, quadratic regression 
model, 169–170
G
Geographic information systems (GIS) charts, 104–105
Geometric distribution, random variables, 548
Geometric mean, measures of location, 38–40
Geometry
all-integer linear optimization, 408–410
classification and regression trees, 284–286
linear optimization modeling, feasible  
solutions, 358–364
Global maximum, nonlinear optimization  
models, 455–459
Global minimum, nonlinear optimization  
models, 455–459
Global optima, nonlinear optimization  
models, 455–459
Goal seek (Excel), what-if analysis, 331–332
Google trends, business analytics, 10–11
Government analytics, applications, 11–12
Grey Code Corporation case study, data mining, 319
Growth factor, geometric mean, 38–40
H
Health care analytics, applications, 10–11
Heat maps, 95–97
Hierarchical clustering
characteristics of, 256
data mining, 256, 258–262
k-means clustering vs., 264–265
Histograms
categorical independent variables, 161–165
data distribution analysis, 31–34
Monte Carlo simulation, Zappos retailer  
problem, 517–518
Historical data, forecasting accuracy, 215–217
Holdout samples, regression model, 179
Horizontal pattern, time series analysis, 205–207
Human resource analytics, applications, 10
Hypergeometric distribution, random variables, 548
Hypothesis testing
inference, 144
individual regression parameters, 151–153
overall regression relationship, 149–150
nonlinear relationships modeling, quadratic regression 
model, 169–170
variable selection, 178–179
I
IF function (Excel), spreadsheet models, 333–336
Monte Carlo simulation, 499–500
Impurities, classification and regression trees, 283
Independent variables
categorical independent variables, 161–165
defined, 125
forecasting applications, 226

 
Index 
667
inference and regression
least squares model, 144–148
multicollinearity, 154–156
nonsignificant independent variables, 153
interaction between, 173–177
Monte Carlo simulation, Zappos retailer  
problem, 517–518
multiple regression equation, 140–142
regression-based causal forecasting, 234–235
Indifference probability, decision analysis, utility theory, 
573–577
Individual regression parameters, testing  
for, 148–150
Infeasibility, linear programming, 368–370
Inference. See Statistical inference
Influence diagrams, 322–323
Integer linear programming
Applecore children’s clothing case study, 441–442
binary variables
applications, 415–426
bank location example, 420–423
capital budgeting, 415–416
defined, 407
fixed costs, 416–420
optimization alternatives, 428–430
product design and market share  
optimization, 424–426
Solver parameters, 412–415
classification, 406–407
defined, 406
Eastborne Realty example, 407–410
Excel Solver features, 410–415
oil rig crew transport problem, 406
sensitivity analysis, 414–415
Integer uniform distribution, random variables, 549
Intensive care unit (ICU) infection reduction,  
Monte Carlo simulation applications, 486
Interaction, between independent  
variables, 173–177
Interquartile range, 58
Interval estimation. See also Confidence intervals
inference, 144
individual regression parameters, 152–153
Inventory management, spreadsheet models, order-up-to 
policy, 334–336
Investment portfolio selection
linear programming, 375–377
nonlinear optimization models, transaction costs, portfolio 
optimization, 477–480
strategy case study, 398–399
Item sets, association rules, 266–269
J
Jaccard’s coefficient, cluster analysis, 258
Join Properties, in Access (Microsoft), 643
Joint probability, 570–571
K
Key performance indicators (KPIs), data  
dashboards, 106
k-means clustering
characteristics of, 256
data mining, 262–265
hierarchical clustering vs., 264–265
k-nearest neighbors (k-NN) method, supervised learning, 
277–283
Knot models
dummy variables, 171–173
piecewise linear regression, 170–173
k out of n alternatives constraint, binary variables, integer 
linear programming, 427
L
Lagrangian multipliers, nonlinear program models, sensitivity 
analysis and shadow pricing, 454–455
Land Shark spreadsheet model, Monte Carlo simulation
APS model development, 499–500
output measurements, 503–506
random variable generation, 500–503
Least squares method
inference and regression, 144–148
linear regression, 127–129
multiple regression, 139–143
regression parameters, 128–132
Leszynski/Reddick guidelines, Access (Microsoft), 626
Level of significance, inference, overall regression 
relationship, 150–153
Lift charts, classification accuracy, 276–277
Lift ratio, association rules, 266–269
Linear functions, mathematical models, 357
Linear programming optimization. See also Integer linear 
optimization; LP relaxation
advertising campaign planning, 381–386
alternative optimal solutions, 367–368
generation of, 386–388
linear program, 386–388
Analytic Solver Platform, 399–403
defined, 357
Excel Solver, 360–364
feasible solutions, 358–364
infeasibility, 368–370
investment portfolio selection, 375–377
strategy case study, 398–399
linear programming notation and examples, 374–377
notation and examples, 374–377
origins of, 354
sensitivity analysis, 372–374
simple maximization problem, 354–357
simple minimization problem, 364–367
timber harvesting, 353
transportation planning, 378–381
unbounded solutions, 370–371

668 
Index
Linear regression
Alliance Data Systems case study, 124
alumni giving case study, 197–198
applications, 125
categorical independent variables, 161–165
defined, 125
fit assessment, simple model, 133–138
inference and, 143–160
overall regression relationship, testing  
for, 148–150
least squares method, 127–133
logistic regression and, 299–307
model fitting, 177–180
multiple regression model, 138–143
nonlinear relationships modeling, 165–177
simple model, 125–127, 133–137
XLMiner applications, 198–201
Linear trend projection, regression-based forecasting, 
226–228
seasonality and, 228–231
Line charts
data visualization with, 87–90
examples of, 75–76
Local maximum, nonlinear optimization  
models, 455–459
Local minimum, nonlinear optimization  
models, 455–459
Local optima, nonlinear optimization  
models, 455–459
Location problems
binary variables, integer linear programming, 421–423
second-best solutions, 428–430
nonlinear optimization models, 459–461
Logistic function, logistic regression, 301
Logistic regression
supervised learning, 299–307
XLMiner applications, 302–307
Log-normal distribution, random variables, 547
Log odds (logit), logistic regression, 300–307
Loss probability, Monte Carlo simulation, 496–498
LP relaxation, 407
geometry of all-integer linear programming, 408–410
M
Make-versus-buy decision
spreadsheet models, 322, 324–327
what-if analysis, 327–331
Many-to-Many relationships, in Access  
(Microsoft), 640
Marketing analytics, applications, 10
Market segmentation, 256
Market share optimization, integer linear programming, 
424–426
Markowitz mean-variance portfolio, nonlinear optimization 
models, 461–465
Matching coefficient, cluster analysis, 257–258
Maximization problem
linear optimization model, 354–357
nonprobability decision analysis
conservative approach, 555
optimistic approach, 554–555
Mean
arithmetic mean, 35–36
geometric mean, 38–40
Mean absolute deviation (MAD), forecasting accuracy, 
214–217
Mean absolute error (MAE), forecasting accuracy,  
214–217
moving averages forecasting, 221
Mean absolute percentage error (MAPE), forecasting 
accuracy, 215–217
moving averages forecasting, 221
Mean deviation, Monte Carlo simulation, random variable 
generation, 493–495
Mean forecast error (MFE), forecasting accuracy, 213–217
Mean squared error (MSE), forecasting accuracy, 214–217
exponential smoothing applications, 225
moving averages forecasting, 221
Measures of association, inter-variable, 51–56
correlation coefficient, 55–56
covariance, 52–55
scatter charts, 51–52
Measures of location, 35–40
geometric mean, 38–40
mean (arithmetic mean), 35–36
median, 36–37
mode, 37–38
Measures of variability, 40–44
coefficient of variation, 44
range, 41
standard deviation, 43
variance, 41–43
Median, measures of location, 36–37
Microsoft Access, database basic applications and, 621–639
Minimax regret approach, nonprobability decision analysis, 
555–557
Minimization problem
linear optimization model, 364–367
nonprobability decision analysis
conservative approach, 555
optimistic approach, 554–555
Minimum error classification tree, XLMiner construction, 
289–298
Mixed-integer linear program, 407
Mode, measures of location, 37–38
Model assessment, data mining, 253
Model construction, data mining, 253
Model fitting, regression models, 177–180
Monotonic relationship, Monte Carlo simulation, random 
variables dependence, 538–544
Monte Carlo simulation
advantages and disadvantages, 524–525
analytics example, 486

 
Index 
669
Analytic Solver Platform modeling, 498–518
Land Shark example
output measurements, 503–506
random variable generation, 500–503
spreadsheet model, 499–500
Zappos retailer problem, 506–518
random variables modeling, 510–516
spreadsheet model, 507–510
Trial execution and output analysis, 517–518
Excel native functions, 488–498
simulation trial execution, 495
Four Corners case study, 536–537
history and characteristics of, 486–487
output measurement and analysis, 495–498
probability distribution, random variable representation, 
489–491
continuous probability distribution, 544–547
discrete distribution, 547–549
random variables
dependence between, 537–544
Excel generation of, 491–495
probability distributions representing, 489–491
simulation optimization, 518–524
summary of steps, 526
verification and validation, 524
what-if analysis, 487–488
Moving averages method, forecasting  
applications, 217–221
Multicollinearity, independent variable  
interactions, 176–177
Multiple-choice constraints, binary variables, integer linear 
programming, 426–428
Multiple regression
defined, 125
estimated multiple regression equation, 138–143
forecasting applications, 226
inference and, 144–148
least squares method, 139–143
model, 138–143
nonsignificant independent variables, 153
with XLMiner, 308
Multiple variables, charts for, 97–100
Mutual fund performances, nonlinear optimization models, 
Markowitz mean-variance portfolio, 461–465
Mutually exclusive constraints, binary variables, integer linear 
programming, 426–428
N
Naïve forecasting, 212–213
logistic regression, 308
Negative binomial distribution, random variables, 548
Neighborhood of solution, nonlinear optimization models, 
455–459
Net present value, 415–416
Network representation, linear programming, transportation 
planning, 379–381
New data predictions, logistic regression models, 304–307
New product adoption, forecasting, nonlinear optimization 
models, 465–469
Nodes
decision trees, 553–555
linear programming, transportation planning, 379–381
Nonexperimental studies, data sources, 19
Noninteger solutions, all-integer linear  
optimization, 408–410
Nonlinear optimization models
Analytic Solver Platform solutions, 480–484
defined, 449
forecasting applications, new product adoption, 465–469
Intercontinental Hotel example, 449
local and global optima, 455–459
location problem, 459–461
Markowitz mean-variance portfolio, 461–465
Monte Carlo simulation, 518–524
product applications, 449–455
constrained problem, 450–453
sensitivity analysis and shadow pricing, 454–455
Solver tool, 453–454
unconstrained problem, 450
transaction costs, portfolio optimization, 477–480
Nonlinear relationships
independent variable interaction, 173–177
modeling, 165–177
piecewise linear regression models, 170–173
quadratic regression models, 167–170
Nonnegativity constraints
integer linear programming, 407–410
linear optimization model, simple minimization problem, 
364–367
Nonprobability decision analysis, 554–557
conservative approach, 555
minimax regret approach, 555–557
optimistic approach, 554–555
Nonprofit analytics, applications, 11–12
Nonsignificant independent variables, inference and 
regression, 153
Nonstationary time series, forecasting accuracy, historical 
data, 216–217
Normal probability distribution, 490–491
random variables, 546
NORMINV function (Excel), 493–495
O
Objective function
integer linear programming, 407–410
linear optimization model
alternative optimal solutions, 367–368
contouring, 358–364
problem formulation, 355–357
linear optimization models, 353–354
nonlinear optimization models, global and local  
optima, 456–459

670 
Index
Observation
cluster analysis, similarities in, 256–258
defined, 16, 252
Observational studies, data sources, 19
Ockham’s razer, 179
Odds, logistic regression, 299–307
One-to-Many relationships, in Access (Microsoft), 639
One-to-One relationships, in Access (Microsoft), 639–640
One-way data tables, what-if analysis, 327–330
Operational decisions, overview, 4–5, 14
Opportunity loss, nonprobability decision  
analysis, 556–557
Optimal decision strategy, defined, 552
Optimistic approach, nonprobability decision analysis, 
554–555
Optimization models
binary variables integer linear programming, alternative 
models, 428–430
defined, 14
linear optimization, 352–354
predictive analytics, 7
Order-up-to inventory policy, spreadsheet models, 334–336
Orphaned records, in Access (Microsoft), 640
Outcomes, problem formulation, 552–554
Outliers
data preparation, 254
distribution analysis, 48–49, 58
Output measurement and analysis, Monte Carlo simulation, 
495–498
Land Shark spreadsheet model, 503–506
Overall error rate, classification accuracy, 273–277
Overfitting, regression model, 179
Oversampling, data partitioning, supervised learning, 
270–273
P
Parallel coordinates plot, 103–104
XLMiner creation, 120–122
Parameters
forecasting, nonlinear optimization models, new product 
adoption, 465–469
Monte Carlo simulation, 486–487
regression parameters
inference and sample size, 156–160
least squares estimates, 128–132
multicollinearity, 154–156
simple linear regression model, 125–127
spreadsheet design, 325–327
Part-worth, integer linear programming, 424–426
Payoffs/payoff tables
decision analysis, 553–554
expected value without perfect information (EVwoPI), 
567–568
utility theory, 571–581
decision tree construction, Analytic Solver Platform, 
600–601
Pearson product moment correlation coefficient, 538–544
Percent frequency distribution, defined, 27–28
Percentiles, distribution analysis, 44–45, 58
Perfect information, decision analysis, 567–568
PERT distribution, random variables, 546
Piecewise linear regression, nonlinear relationships  
modeling, 170–173
Pie charts, 93
PivotCharts (Excel), 101–102
PivotTable (Excel), 80–85
Point estimator, regression equation, 126–127
Poisson distribution, random variables, 549
Polynomial regression models, nonlinear relationships 
modeling, 170
Population data, 17–18
Posterior probability
decision analysis, sample  
information, 561–568
decision trees, Bayes’ theorem, 568–571
Postoptimality analysis, linear  
optimization, 372–374
Predicted values
inference and regression models, scatter  
chart, 145–149
nonlinear relationships modeling, 165–167
quadratic regression model, 167–170
probability, logistic regression, 302–307
Prediction
logistic regression, 302–307
supervised learning, 252
Prediction accuracy
k-nearest neighbors classification, 280–283
supervised learning, 277
Predictive analytics
characteristics, 6–7
classification, 6
defined, 14
Predictor variables
defined, 125
forecasting applications, 226
Prescriptive analytics
characteristics, 6–8
linear optimization models, 353–354, 386–388
models, 14
Primary key field, in Access (Microsoft) 623–624
Prior probability, decision analysis, sample information, 
561–568
Probabilities
decision analysis, 557–561
expected value approach, 557–559
risk analysis, 559–560
sensitivity analysis, 560–561
decision tree construction, Analytic Solver  
Platform, 600–601
Probability distribution, Monte Carlo simulation, 486–487, 
489–491
Analytic Solver Platform, 500

 
Index 
671
Probability prediction, logistic regression, 302–307
Problem formulation
decision analysis, 552–554
decision trees, 553–554
payoff tables, 553
linear optimization model
simple maximization problem, 355–357
simple minimization problem, 364–367
Procter & Gamble case study, 7–8
Product design
integer linear programming, 424–426
nonlinear optimization models, 449–455
constrained problem, 450–453
forecasting, new product  
adoption, 465–469
sensitivity analysis and shadow  
pricing, 454–455
Solver tool, 453–454
unconstrained problem, 450
Profit contribution maximization
linear optimization model, 354–357
Monte Carlo simulation, Zappos retailer  
problem, 507–510
nonlinear optimization models, constrained problem, 
451–453
Property purchase strategy case study, decision analysis, 
595–596
Proportionality, linear optimization, 357
Pruned classification and regression trees
error rates, 287
XLMiner construction, 290–298
Pseudorandom numbers, Monte Carlo  
simulation, 491–495
p-values
independent variable interactions, 174–177
individual regression parameters, 151–153
inference and sample size, 158–160
logistic regression models, 303–307
nonlinear relationships modeling, quadratic regression 
model, 169–170
Q
Quadratic function, nonlinear optimization models, 
unconstrained problem, 450
Quadratic regression model
independent variable interactions, 176–177
nonlinear relationships, 167–170
Qualitative forecasting, definition and  
classification, 203–204
Quantitative data
defined, 18, 58
frequency distributions, 28–31
Quantitative forecasting, definition and classification, 
203–204
Quartiles, distribution analysis, 45–46, 58
Queries, in Access (Microsoft), 622, 647–655
R
Random number intervals, Monte Carlo simulation, direct 
labor costs, 491–492
Random sampling, defined, 18
Random variables
defined, 17
Monte Carlo simulation, 486–487
dependence between, 537–544
Excel generation of, 491–495
Land Shark spreadsheet model, 500–503
probability distributions, 489–491, 545–549
Zappos retailer problem, 510–515
simple linear regression model, 125–127
Range, measures of variability, 41
Raw data, hierarchical clustering, 259–262
Records sorting and filtering, in Access (Microsoft), 643–647
Reduced costs, linear optimization model, sensitivity  
analysis, 374
Reduced gradient, nonlinear program models, sensitivity 
analysis and shadow pricing, 454–455
Referential integrity, in Access (Microsoft), 640
Regression analysis
defined, 125
forecasting applications, 226–236
causal forecasting, 231–235
combined causal, trend, and seasonality effects, 235
limitations of, 235–236
linear trend projection, 226–228
seasonality, 228–231
inference and, 143–160
overall regression relationship, testing for, 148–150
integer linear programming, product design and market 
share optimization, 424–426
logistic regression, 299–307
model
fitting parameters, 177–180
independent variable interactions, 174–177
multiple regression model, 138–143
simple linear regression model, 125–127, 133–137
XLMiner applications, 198–201
Regression equation
estimated regression equation, 126–127
multiple regression model, 138–143
simple linear regression model, 125–127
Regression parameters
inference and sample size, 156–160
least squares estimates, 128–132
multicollinearity, 154–156
simple linear regression model, 125–127
Regression trees
continuous outcome prediction, 291–298
supervised learning, 283–298
XLMiner construction, 293–296
Regret, nonprobability decision analysis, 555–557
Relative frequency distribution, defined, 27–28
Replication ID field, in Access (Microsoft), 623

672 
Index
Residual plots
inference and regression models, scatter chart, 145–149
logistic regression, 299–300
nonlinear relationships modeling, 165–167
quadratic regression model, 167–170
Retirement plan case study, spreadsheet models, 350–351
Right-hand-side allowable increase, linear optimization  
model, 390
Risk analysis
Monte Carlo simulation, loss probability, 496–498
nonlinear optimization models, Markowitz mean-variance 
portfolio, 463–465
probability-based decision analysis, 559–560
Risk avoidance, decision analysis, utility theory, 574–577
Risk-neutral decision making, utility theory, 579–581
Risk premium, decision analysis, utility theory, 574
Risk profile, construction of, 559–560
Risk taker, decision analysis, utility theory, 577–580
Risk tolerance values, exponential utility function, 580–581
Root mean squared error (RMSE), prediction accuracy, 277
k-nearest neighbors classification, 282–283
S
Sample data, 17–18
Sample information, decision analysis, 561–568
expected value, 566
perfect information, expected value, 567–569
Sample size, inference and regression, 156–160
Sanotronics case study, Monte Carlo simulation, what-if 
analysis, 487–488
Scatter charts
data visualization, 85–87
estimated regression equation, 132–133
inference and regression models, 145–148
least squares methods, 127–128
logistic regression, 299
measures of association, two variables, 51–52, 58
nonlinear relationships modeling, 165–167
quadratic regression model, 167–170
regression-based causal forecasting, 232–235
scatter chart matrix, 98–101
XLMiner creation, 119–120
Schmidt, Eric, 8
S-curve, logistic regression, 299–301
Seasonality/seasonal patterns
regression-based forecasting
analysis techniques, 228–231
combined causal variables and trend/seasonality  
effects, 235
without trends, 228–231
time series analysis, patterns in, 209–211
Second-best solutions, binary variables integer linear 
programming optimization, 428–430
Second-order polynomial models, independent variable 
interactions, 176–177
Segment regression. See Piecewise linear regression
Select queries, in Access (Microsoft), 648
Sensitivity analysis
classification accuracy, 274
integer linear programming, 414–415
linear optimization, 372–374
advertising campaign planning, 385–386
nonlinear program optimization, 454–455
probability-based decision analysis, 560–561
Setup costs, binary variables, integer linear programming, 
418–420
Shadow pricing
linear optimization model, sensitivity analysis, 372–374
nonlinear program optimization, 454–455
Share of choice problem, integer linear programming, 
425–426
Show formulas (Excel), spreadsheet auditing, 340
Simple formulas, spreadsheet models, 326–327
Simple linear regression
estimation process, 126–127
forecasting applications, 226
linear trend projection, 226–228
inference and, 145–148
logistic regression and, 299–307
model fit, 133–137
nonlinear relationships modeling, 165–177
sums of squares due to error, 133–137
total sum of squares (SST), 135–137
Simple regression, defined, 125
Simplex algorithm
linear program solutions, 360–364
nonlinear program solutions, 452–453
Simulation. See also Monte Carlo simulation
defined, 6, 14
discrete-event Monte Carlo simulation
Excel simulation trials, 495, 504–505
optimization, 518–524
output measurement and analysis, 495–498
Simulation-optimization, 7, 518–524
Single linkage, hierarchical clustering, 256, 258–262
Skewness
histogram distribution analysis, 32–34
inference and regression models, 147–148
Slack value, linear optimization model, 363–364
Smoothing constant, exponential smoothing, forecasting 
applications, 221–225
Sorting of data
in Access (Microsoft), 643–647
in Excel, 21–23
Sparkline, data visualization with, 89–90
Spearman rank correlation coefficient, 538–544
Specificity, classification accuracy, 274
Spline model. See Piecewise linear regression
Sports analytics, applications, 12
Spreadsheet models
Access (Microsoft) and, 634–639
analytics example, 321
auditing of, 339–343

 
Index 
673
binary variables, integer linear programming
capital budgeting, 415–418
fixed costs, 418–420
construction, 322–327
influence diagrams, 322
mathematical model, 322–324
design and implementation, 324–327
Excel functions, 332–338
IF and COUNTIF, 333–336
SUM and SUMPRODUCT, 332–333
VLOOKUP, 336–338
integer linear programming solutions, Solver features, 411–415
linear optimization
advertising campaign planning, 384–386
simple minimization problem, 365–367
transportation planning, 381–382
Monte Carlo simulation, 488–498
Zappos retailer problem, 507–510
nonlinear program optimization, 453–455
retirement plan case study, 350–351
Stacked column charts, 97–98
Standard deviation
measures of variability, 43
Monte Carlo simulation, random variable generation, 493–495
Standard error
individual regression parameters, 151–153
multicollinearity, 156
State-of-nature branch probability, probability-based decision 
analysis, 558–559
sensitivity analysis, 560–561
States of nature, decision outcomes and, 552–554
Stationary time series
forecasting accuracy, historical data, 216–217
horizontal patterns, 205–207
Statistical inference
defined, 144
regression and, 143–160
individual regression parameters, 150–153
least squares regression model, 144–148
multicollinearity, 154–156
overall regression relationship, testing for, 148–150
very large samples, 156–160
STDEV function (Excel), 43–44, 496–498
Stepwise procedure, variable selection, 178–179
Stochastic library unit with relationships preserved (SLURP), 
Monte Carlo simulation, random variables 
dependence, 542–544
Strategic decision, overview, 4–5, 13
Structural multicollinearity, independent variable interactions, 
176–177
SUM function (Excel), spreadsheet models, 332–333
Sum of distances minimization, nonlinear optimization 
models, location problems, 459–461
Sum of squares due to error (SSE)
inference, overall regression relationship testing, 150–153
multiple regression equation, 140–142
simple linear regression model, 134
Sum of squares due to regression (SSR)
inference, overall regression relationship  
testing, 150–153
multiple regression equation, 140–142
simple regression model, 136–137
SUMPRODUCT function (Excel)
linear program solutions
simple maximization problem, 360–364
simple minimization problem, 365–367
spreadsheet models, 332–333
Supervised learning
classification accuracy, 273–277
classification and regression trees, 283–298
data mining, 299–307
data partitioning, 269–273
defined, 252
k-nearest neighbors method, 277–283
logistic regression model, 299–307
overview of methods, 309
prediction accuracy, 277
Supply chain analytics, applications, 11
Support counts, association rules, 266–269
Surplus variable, linear program solutions, simple 
minimization problem, 365–367
T
Tables
in Access (Microsoft), 622, 639–643
crosstabulation, 79–80
data visualization with, 75–85
design principles, 77–79
PivotTables (Excel), 80–85
Tactical decisions, overview, 4–5
Test set, data partitioning, supervised learning, 270–273
3-D charts, 93
Tightening (of constraints), linear optimization model, 
sensitivity analysis, 373–374
Time series analysis
data, 18
forecasting and, 204
regression-based causal forecasting, 231–235
patterns, 205–212
combined trend and seasonal patterns, 209–211
cyclical patterns, 211–212
horizontal patterns, 205–207
identification, 212
seasonal patterns, 209, 228–231
trend patterns, 207–209
plot characteristics, 205
stationary time series, 205–207
Total sum of squares (SST)
inference, overall regression relationship  
testing, 150–153
multiple regression equation, 140–142
simple linear regression, 135–137
Trace Precedents (Excel), spreadsheet auditing, 339–340

674 
Index
Training set
data partitioning, supervised learning, 270–273
overfitting, regression model, 179
Transaction costs, nonlinear optimization models, portfolio 
optimization, 477–480
Transportation planning, linear optimization  
model, 378–381
alternative optimal solutions, 386–388
Treemap, 103–104
Trend-cycle effects, time series analysis, 212
Trendlines, data visualization, 86–87
Trend patterns
regression-based causal forecasting, 235
time series analysis, 207–211
Triangular distribution, random  
variables, 546–547
Zappos retailer problem, 514–518
t test
independent variable interactions, 174–177
individual regression parameters, 151–153
Two-way data table (Excel), what-if  
analysis, 327–330
U
Unbounded solutions, linear programming optimization, 
370–371
Uncertain variables
defined, 17
Monte Carlo simulation, Land Shark spreadsheet model, 
500–503
Unconstrained problem, nonlinear optimization  
models, 450
location example, 459–461
Uniform probability distribution, 490–491
random variables, 547
Unity constraint, nonlinear optimization models, Markowitz 
mean-variance portfolio, 463–465
Unsupervised learning
data mining, 255–269
association rules, 265–269
cluster analysis, 256–265
defined, 252
U.S. Census Bureau, descriptive statistics  
applications, 16
Utility, defined, 572
Utility function for money, decision  
analysis, 578–581
Utility theory
decision analysis, 571–581
exponential utility functions, 580–581
utility functions, 577–580
defined, 14
predictive analytics, 7
Utility values, integer linear programming, part-worth, 
425–426
V
Validation
data partitioning, supervised  
learning, 270–273
Monte Carlo simulation, 524–525
overfitting, regression model, 179
Variables
cluster analysis, 257–258
data preparation, 254–255
defined, 16–17
forecasting applications, 226
linear program solutions, 363
slack variables, 364
logistic regression, selection  
criteria, 302–307
measures of association, 51–57
nonlinear program models, sensitivity analysis  
and shadow pricing, 454–455
regression models, selection  
procedures, 177–179
Variance, measures of variability, 41–43
Variation, defined, 55–57
Verification, Monte Carlo simulation, 524–525
VLOOKUP function (Excel)
Monte Carlo simulation, random variable generation, 
494–495
spreadsheet models, 337–338
W
Ward’s method, hierarchical  
clustering, 259–262
Watch Window (Excel), spreadsheet  
auditing, 342–343
Web analytics, applications, 12–13
What-if analysis
data tables, 327–330
defined, 321
Goal Seek tool (Excel), 331–332
linear program solutions, 360–364
Monte Carlo simulation, 487–488
tools, 327–332
Worst-case scenario, what-if analysis, 488
X
XLMiner
association rules applications, 267–269
box plots in, 67–69
classification and regression tree  
construction, 287–298
data partitioning, supervised learning, oversampling 
applications, 270–273
data preparation applications, 254–255
discriminant analysis classification, 308

 
Index 
675
forecasting applications, 247–250
hierarchical clustering applications, 259–262
k-means clustering applications, 263–265
k-nearest neighbors classification, 278–283
logistic regression applications, 302–307
logistic regression models, 302–307
missing data treatment, 254
overfitting, regression model, 179
parallel coordinates plot, 120–122
regression analysis applications, 198–201
scatter chart matrix, 119–120
variable selection procedures, 178–179
Z
Zappos retailer problem, Monte Carlo simulation
Analytic Solver Platform modeling, 506–518
random variables modeling, 510–516
spreadsheet model, 507–510
Trial execution and output analysis, 517–518
model optimization, 519–524
random variables dependence, 538–544
z-scores
cluster analysis, 257–258
distribution analysis, 46–47, 58

Figure 10.18   THE OUTPUT FROM THE OPTIMIZATION FROM ANALYTIC 
SOLVER PLATFORM
NOTES AND COMMENTS
1. The Solver Options and Model Specifications task 
pane can be moved by clicking the banner at the top (the 
location of the title of the box), holding the mouse, and 
dragging. The box can be invoked or hidden by clicking 
on the Model button on the far left of the ASP Ribbon.
2. The default objective type is maximize. To change the 
objective function to minimize, select the objective 
function location in the optimization tree, right-click the 
mouse, select Edit, and then select Min. Alternatively, 
select the objective function location in the optimization 
tree, click the Objective button in the Optimization 
Model group in the ASP Ribbon, select Min, and select 
Normal. 
3. The objective, variables, and constraints can also be 
added to the model by clicking the Decisions, Con-
straints, and Objective buttons, respectively, in the 
Optimization Model group of the ASP Ribbon.
4. The Refresh button 
 (Figure 10.16) should be used 
whenever a model has been changed (new constraints, 
variables, or other changes have been made).
5. To delete elements (variables, constraints, or objective) 
from the model, select the element from the optimiza-
tion tree, and click the Delete button 
 (Figure 10.16). 
To keep a variable or constraint in the model but have it 
ignored for a given run, click the box next to that element 
(the green checkmark will disappear, indicating it is not 
part of the current model run). 
6. The Analyze button 
 (Figure 10.16) provides an analy-
sis of the model, including the number and types of vari-
ables and constraints. The report appears at the bottom of 
the Solver Options and Model Specifications dialog box.
7. To generate an Answer or Sensitivity Report as dis-
cussed in this chapter, after solving the problem, select 
Reports from the Analysis group of the ASP Ribbon. 
Select Optimization and then Answer for an Answer 
Report or Sensitivity for a Sensitivity Report.
8. ASP allows you to solve larger nonlinear programs than 
Standard Excel Solver. Standard Solver is limited to 200 
variables and 100 constraints. ASP is limited to 1000 
variables and 1000 constraints for nonlinear optimiza-
tion problems.
9. To invoke the MultiStart option in ASP, click on the 
Engine tab, select Standard LSGRG Nonlinear En-
gine from the drop-down menu, and then, in the Global 
Optimization section, select True from the drop-down 
menu next to MultiStart.

FIGURE 11.37   CONSTRUCTING A CORRELATION MATRIX IN ASP
FIGURE 11.38   MAKING CORRELATION MATRIX MATHEMATICALLY CONSISTENT

TABLE 3.5   LARGER TABLE SHOWING REVENUES BY LOCATION FOR 12 MONTHS OF DATA
Month
Revenues by Location ($)
1
2
3
4
5
6
7
8
9
10
11
12
Total
Temple
Killeen
Waco
Belton
Granger
Harker Heights
Gatesville
Lampasas
Academy
8,987
8,212
11,603
7,671
7,642
5,257
5,316
5,266
4,170
8,595
9,143
12,063
7,617
7,744
5,326
5,245
5,129
5,266
8,958
8,714
11,173
7,896
7,836
4,998
5,056
5,022
7,472
6,718
6,869
9,622
6,899
5,833
4,304
3,317
3,022
1,594
8,066
8,150
8,912
7,877
6,002
4,106
3,852
3,088
1,732
8,574
8,891
9,553
6,621
6,728
4,980
4,026
4,289
2,025
8,701
8,766
11,943
7,765
7,848
5,084
5,135
5,110
8,772
9,490
9,193
12,947
7,720
7,717
5,061
5,132
5,073
1,956
9,610
9,603
12,925
7,824
7,646
5,186
5,052
4,978
3,304
9,262
10,374
14,050
7,938
7,620
5,179
5,271
5,343
3,090
9,875
10,456
14,300
7,943
7,728
4,955
5,304
4,984
3,579
11,058
10,982
13,877
7,047
8,013
5,326
5,154
5,315
2,487
107,895
109,353
142,967
90,819
88,357
59,763
57,859
56,620
45,446
Total
Costs ($)
64,124
48,123
66,128
56,458
67,125
64,125
48,178
52,158
51,785
54,718
55,687
50,985
69,125
57,898
64,288
62,050
66,128
65,215
68,128
61,819
69,125
67,828
69,258
69,558
759,079
710,935

TABLE 9.2 COUNTIES IN THE OHIO TRUST EXPANSION REGION
Counties Under  
Consideration
Adjacent Counties  
(by Number)
 1. Ashtabula
 2. Lake
 3. Cuyahoga
 4. Lorain
 5. Huron
 6. Richland
 7. Ashland
 8. Wayne
 9. Medina
10. Summit
11. Stark
12. Geauga
13. Portage
14. Columbiana
15. Mahoning
16. Trumbull
17. Knox
18. Holmes
19. Tuscarawas
20. Carroll
2, 12, 16
1, 3, 12
2, 4, 9, 10, 12, 13
3, 5, 7, 9
4, 6, 7
5, 7, 17
4, 5, 6, 8, 9, 17, 18
7, 9, 10, 11, 18
3, 4, 7, 8, 10
3, 8, 9, 11, 12, 13
8, 10, 13, 14, 15, 18, 19, 20
1, 2, 3, 10, 13, 16
3, 10, 11, 12, 15, 16
11, 15, 20
11, 13, 14, 16
1, 12, 13, 15
6, 7, 18
7, 8, 11, 17, 19
11, 18, 20
11, 14, 19
file
WEB
OhioTrust
FIGURE 9.11   OPTIMAL SOLUTION TO THE OHIO TRUST LOCATION PROBLEM
Lake 
Erie
Ohio
Pennsylvania
West 
Virginia
Counties
1. 
2. 
3. 
4. 
5.
Ashtabula 
Lake 
Cuyahoga 
Lorain 
Huron
6. 
7. 
8. 
9. 
10.
Richland 
Ashland 
Wayne 
Medina 
Summit
11. 
12. 
13. 
14. 
15.
Stark 
Geauga 
Portage 
Columbiana
Mahoning
16. 
17. 
18. 
19. 
20.
Trumbull 
Knox 
Holmes 
Tuscarawas
Carroll
A principal place
of business
should be located
in these counties.
★
1
2
12
16
15
14
20
19
11
13
3
10
18
17
6
8
5
4
9
7
★
★
★

CUMULAТIVE PROBABILIТIES FOR ТНЕ STANDARD NORМA L DISTRIBUTION 
z 
-3.0 
-2.9 
-2.8 
-2.7 
-2.6 
-2.5 
-2.4 
-2.3 
-2.2 
-2.1 
-2.0 
-1.9 
-1.8 
-1.7 
-1.6 
-1.5 
-1.4 
-1.3 
-1.2 
-1.1 
-1.0 
-.9 
-.8 
-.7 
-.6 
-.5 
-.4 
-.3 
-.2 
-.1 
-.О 
Cumulative 
probaЬility 
.00 
.0013 
.0019 
.0026 
.0035 
.0047 
.0062 
.0082 
.0107 
.0139 
.0179 
.0228 
.0287 
.0359 
.0446 
.0548 
.0668 
.0808 
.0968 
.1151 
.1357 
.1587 
.1841 
.2119 
.2420 
.2743 
.3085 
.3446 
.3821 
.4207 
.4602 
.5000 
.01 
.0013 
.0018 
.0025 
.0034 
.0045 
.0060 
.0080 
.0104 
.0136 
.0174 
.0222 
.0281 
.0351 
.0436 
.0537 
.0655 
.0793 
.0951 
.1131 
.1335 
.1562 
.1814 
.2090 
.2389 
.2709 
.3050 
.3409 
.3783 
.4168 
.4562 
.4960 
z 
о 
.02 
.0013 
.0018 
.0024 
.0033 
.0044 
.0059 
.0078 
.0102 
.0132 
.0170 
.0217 
.0274 
.0344 
.0427 
.0526 
.0643 
.0778 
.0934 
.1112 
.1314 
.1539 
.1788 
.2061 
.2358 
.2676 
.3015 
.3372 
.3745 
.4129 
.4522 
.4920 
.03 
.0012 
.0017 
.0023 
.0032 
.0043 
.0057 
.0075 
.0099 
.0129 
.0166 
.0212 
.0268 
.0336 
.0418 
.0516 
.0630 
.0764 
.0918 
.1093 
.1292 
.1515 
.1762 
.2033 
.2327 
.2643 
.2981 
.3336 
.3707 
.4090 
.4483 
.4880 
.04 
.0012 
.0016 
.0023 
.0031 
.0041 
.0055 
.0073 
.0096 
.0125 
.0162 
.0207 
.0262 
.0329 
.0409 
.0505 
.0618 
.0749 
.0901 
.1075 
.1271 
.1492 
.1736 
.2005 
.2296 
.2611 
.2946 
.3300 
.3669 
.4052 
.4443 
.4840 
Entries in this tаЫе 
give the area under the 
curve to the left of the 
z value. For example, for 
z = -.85, the cumulative 
probaЬility is .1977. 
.05 
.0011 
.0016 
.0022 
.0030 
.0040 
.0054 
.0071 
.0094 
.0122 
.0158 
.0202 
.0256 
.0322 
.0401 
.0495 
.0606 
.0735 
.0885 
.1056 
.1251 
.1469 
.1711 
.1977 
.2266 
.2578 
.2912 
.3264 
.3632 
.4013 
.4404 
.4801 
.06 
.0011 
.0015 
.0021 
.0029 
.0039 
.0052 
.0069 
.0091 
.0119 
.0154 
.0197 
.0250 
.0314 
.0392 
.0485 
.0594 
.0721 
.0869 
.1038 
.1230 
.1446 
.1685 
.1949 
.2236 
.2546 
.2877 
.3228 
.3594 
.3974 
.4364 
.4761 
.07 
.0011 
.0015 
.0021 
.0028 
.0038 
.0051 
.0068 
.0089 
.0116 
.0150 
.0192 
.0244 
.0307 
.0384 
.0475 
.0582 
.0708 
.0853 
.1020 
.1210 
.1423 
.1660 
.1922 
.2206 
.2514 
.2843 
.3192 
.3557 
.3936 
.4325 
.4721 
.08 
.0010 
.0014 
.0020 
.0027 
.0037 
.0049 
.0066 
.0087 
.0113 
.0146 
.0188 
.0239 
.0301 
.0375 
.0465 
.0571 
.0694 
.0838 
.1003 
.1190 
.1401 
.1635 
.1894 
.2177 
.2483 
.2810 
.3156 
.3520 
.3897 
.4286 
.4681 
.09 
.0010 
.0014 
.0019 
.0026 
.0036 
.0048 
.0064 
.0084 
.0110 
.0143 
.0183 
.0233 
.0294 
.0367 
.0455 
.0559 
.0681 
.0823 
.0985 
.1170 
.1379 
.1611 
.1867 
.2148 
.2451 
.2776 
.3121 
.3483 
.3859 
.4247 
.4641 

CUМULAТIVE PROBABILIТIES FOR ТНЕ STANDARD NORMAL DISTRIВUTION 
Cumulative 
probability 
Entries in the tаЫе 
give the area under the 
curve to the left of the 
z value. For example, for 
z = 1.25, the cumulative 
probability is .8944. 
о 
z 
z 
.00 
.01 
.02 
.03 
.04 
.05 
.06 
.07 
.08 
.09 
.о 
.5000 
.5040 
.5080 
.5120 
.5160 
.5199 
.5239 
.5279 
.5319 
.5359 
.1 
.5398 
.5438 
.5478 
.5517 
.5557 
.5596 
.5636 
.5675 
.5714 
.5753 
.2 
.5793 
.5832 
.5871 
.5910 
.5948 
.5987 
.6026 
.6064 
.6103 
.6141 
.3 
.6179 
.6217 
.6255 
.6293 
.6331 
.6368 
.6406 
.6443 
.6480 
.6517 
.4 
.6554 
.6591 
.6628 
.6664 
.6700 
.6736 
.6772 
.6808 
.6844 
.6879 
.5 
.6915 
.6950 
.6985 
.7019 
.7054 
.7088 
.7123 
.7157 
.7190 
.7224 
.6 
.7257 
.7291 
.7324 
.7357 
.7389 
.7422 
.7454 
.7486 
.7517 
.7549 
.7 
.7580 
.7611 
.7642 
.7673 
.7704 
.7734 
.7764 
.7794 
.7823 
.7852 
.8 
.7881 
.7910 
.7939 
.7967 
.7995 
.8023 
.8051 
.8078 
.8106 
.8133 
.9 
.8159 
.8186 
.8212 
.8238 
.8264 
.8289 
.8315 
.8340 
.8365 
.8389 
1.0 
.8413 
.8438 
.8461 
.8485 
.8508 
.8531 
.8554 
.8577 
.8599 
.8621 
1.1 
.8643 
.8665 
.8686 
.8708 
.8729 
.8749 
.8770 
.8790 
.8810 
.8830 
1.2 
.8849 
.8869 
.8888 
.8907 
.8925 
.8944 
.8962 
.8980 
.8997 
.9015 
1.3 
.9032 
.9049 
.9066 
.9082 
.9099 
.9115 
.9131 
.9147 
.9162 
.9177 
1.4 
.9192 
.9207 
.9222 
.9236 
.9251 
.9265 
.9279 
.9292 
.9306 
.9319 
1.5 
.9332 
.9345 
.9357 
.9370 
.9382 
.9394 
.9406 
.9418 
.9429 
.9441 
1.6 
.9452 
.9463 
.9474 
.9484 
.9495 
.9505 
.9515 
.9525 
.9535 
.9545 
1.7 
.9554 
.9564 
.9573 
.9582 
.9591 
.9599 
.9608 
.9616 
.9625 
.9633 
1.8 
.9641 
.9649 
.9656 
.9664 
.9671 
.9678 
.9686 
.9693 
.9699 
.9706 
1.9 
.9713 
.9719 
.9726 
.9732 
.9738 
.9744 
.9750 
.9756 
.9761 
.9767 
2.0 
.9772 
.9778 
.9783 
.9788 
.9793 
.9798 
.9803 
.9808 
.9812 
.9817 
2.1 
.9821 
.9826 
.9830 
.9834 
.9838 
.9842 
.9846 
.9850 
.9854 
.9857 
2.2 
.9861 
.9864 
.9868 
.9871 
.9875 
.9878 
.9881 
.9884 
.9887 
.9890 
2.3 
.9893 
.9896 
.9898 
.9901 
.9904 
.9906 
.9909 
.9911 
.9913 
.9916 
2.4 
.9918 
.9920 
.9922 
.9925 
.9927 
.9929 
.9931 
.9932 
.9934 
.9936 
2.5 
.9938 
.9940 
.9941 
.9943 
.9945 
.9946 
.9948 
.9949 
.9951 
.9952 
2.6 
.9953 
.9955 
.9956 
.9957 
.9959 
.9960 
.9961 
.9962 
.9963 
.9964 
2.7 
.9965 
.9966 
.9967 
.9968 
.9969 
.9970 
.9971 
.9972 
.9973 
.9974 
2.8 
.9974 
.9975 
.9976 
.9977 
.9977 
.9978 
.9979 
.9979 
.9980 
.9981 
2.9 
.9981 
.9982 
.9982 
.9983 
.9984 
.9984 
.9985 
.9985 
.9986 
.9986 
3.0 
.9987 
.9987 
.9987 
.9988 
.9988 
.9989 
.9989 
.9989 
.9990 
.9990 


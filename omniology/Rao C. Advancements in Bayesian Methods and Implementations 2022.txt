
Handbook of Statistics
Volume 47
Advancements in Bayesian Methods
and Implementation

Handbook of Statistics
Series Editors
C.R. Rao
C.R. Rao AIMSCS, University of Hyderabad Campus,
Hyderabad, India
Arni S.R. Srinivasa Rao
Medical College of Georgia, Augusta University, United States

Handbook of Statistics
Volume 47
Advancements in
Bayesian Methods and
Implementation
Edited by
Arni S.R. Srinivasa Rao
Medical College of Georgia,
Augusta, Georgia, United States
G. Alastair Young
Department of Mathematics, Imperial College London,
London, United Kingdom
C.R. Rao
AIMSCS, University of Hyderabad Campus,
Hyderabad, India

Academic Press is an imprint of Elsevier
50 Hampshire Street, 5th Floor, Cambridge, MA 02139, United States
525 B Street, Suite 1650, San Diego, CA 92101, United States
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, United Kingdom
125 London Wall, London, EC2Y 5AS, United Kingdom
Copyright © 2022 Elsevier B.V. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means,
electronic or mechanical, including photocopying, recording, or any information storage and
retrieval system, without permission in writing from the publisher. Details on how to seek
permission, further information about the Publisher’s permissions policies and our arrangements
with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency,
can be found at our website: www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the
Publisher (other than as may be noted herein).
Notices
Knowledge and best practice in this field are constantly changing. As new research and experience
broaden our understanding, changes in research methods, professional
practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge
in evaluating and using any information, methods, compounds, or experiments
described herein. In using such information or methods they should be mindful of their
own safety and the safety of others, including parties for whom they have a professional
responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or
editors, assume any liability for any injury and/or damage to persons or property as a
matter of products liability, negligence or otherwise, or from any use or operation of any
methods, products, instructions, or ideas contained in the material herein.
ISBN: 978-0-323-95268-2
ISSN: 0169-7161
For information on all Academic Press publications
visit our website at https://www.elsevier.com/books-and-journals
Publisher: Zoe Kruze
Acquisitions Editor: Mariana Kuhl
Developmental Editor: Naiza Ermin Mendoza
Production Project Manager: Abdulla Sait
Cover Designer: Vicky Pearson
Typeset by STRAIVE, India

Contents
Contributors
xi
Preface
xiii
1.
Direct Gibbs posterior inference on risk minimizers:
Construction, concentration, and calibration
1
Ryan Martin and Nicholas Syring
1.
Introduction
1
2.
Gibbs posterior distributions
4
2.1
Problem setup
4
2.2
Definition
6
2.3
FAQs
8
2.4
Illustrations
11
3.
Asymptotic theory
14
3.1
Objectives and general strategies
14
3.2
Consistency
15
3.3
Concentration rates
16
3.4
Distributional approximations
18
4.
Learning rate selection
21
5.
Numerical examples
24
5.1
Quantile regression
24
5.2
Classification
26
5.3
Nonlinear regression
28
6.
Further details
30
6.1
Things we did not discuss
30
6.2
Open problems
31
7.
Conclusion
32
Acknowledgments
33
Appendix
34
A.1
Proofs
34
References
38
2.
Bayesian selective inference
43
Daniel Garcı´a Rasines and G. Alastair Young
1.
Introduction
43
2.
Bayes and selection
45
2.1
Fixed and random parameters
48
v

3.
Noninformative priors for selective inference
50
3.1
Noninformative priors for exponential families
56
4.
Discussion
64
References
65
3.
Dependent Bayesian multiple hypothesis testing
67
Noirrit Kiran Chandra and Sourabh Bhattacharya
1.
Introduction
67
2.
Bayesian multiple hypothesis testing
69
2.1
Preliminaries and setup
69
2.2
The decision problem
71
3.
Dependent multiple testing
72
3.1
New error based criterion
74
3.2
Choice of G1,…,Gm
75
4.
Simulation study
76
4.1
The postulated Bayesian model
76
4.2
Comparison criteria
77
4.3
Comparison of the results
77
5.
Discussion
78
References
79
4.
A new look at Bayesian uncertainty
83
Stephen G. Walker
1.
Introduction
83
2.
Missing data
86
3.
Parametric martingale sequences
88
3.1
Langevin posterior
90
4.
Nonparametric martingale distributions
92
5.
Illustrations
93
5.1
Parametric case
93
5.2
Nonparametric case
94
6.
Mathematical theory
96
7.
Discussion
98
Acknowledgments
100
References
100
5.
50 shades of Bayesian testing of hypotheses
103
Christian P. Robert
1.
Introduction
103
2.
Bayesian hypothesis testing
104
3.
Improper priors united against hypothesis testing
107
4.
The Jeffreys–Lindley paradox
109
5.
Posterior predictive p-values
110
6.
A modest proposal
111
7.
Conclusion
116
Acknowledgments
116
References
117
vi
Contents

6.
Inference approach to ground states of quantum
systems
121
Angelo Plastino and A.R. Plastino
1.
Introduction
121
2.
The Jaynes’ maximum entropy methodology: Brief
resume
122
3.
The quantum maximum entropy approach
123
3.1
Preliminaries
123
4.
Properties of SQ that make our approximate maximum
entropy approach wave functions reasonable ones
124
4.1
SQ is a true Shannon’s ignorance function
124
4.2
Subject to the known quantities bk, the maximum
value of SQ is unique
125
4.3
The entropy SQ obeys an H-theorem
125
4.4
Our SQ-ground-state wave functions respect the virial
theorem
126
4.5
The SQ-ground-state wave functions respect hypervirial
theorems
127
4.6
Saturation
127
4.7
Speculation
127
5.
Coulomb potential
127
5.1
Harmonic oscillator
128
5.2
Morse potential
128
5.3
Ground state of the quartic oscillator
129
5.4
A possible MEM extension
129
6.
Noncommuting observables
129
7.
Other entropic or information measures
130
8.
Conclusions
133
References
133
7.
MCMC for GLMMs
135
Vivekananda Roy
1.
Introduction
135
2.
Likelihood function for GLMMs
136
3.
Conditional simulation for GLMMs
139
3.1
MALA for GLMMs
139
3.2
HMC for GLMMs
141
3.3
Data augmentation for GLMMs
143
4.
MCMC for Bayesian GLMMs
147
4.1
MALA and HMC for Bayesian GLMMs
148
4.2
Data augmentation for Bayesian GLMMs
150
5.
A numerical example
154
6.
Discussion
156
References
157
Contents
vii

8.
Sparsity-aware Bayesian inference and its
applications
161
Geethu Joseph, Saurabh Khanna, Chandra R. Murthy,
Ranjitha Prasad, and Sai Subramanyam Thoota
1.
Introduction
162
1.1
Quick summary of existing methods for sparse signal
recovery
163
1.2
Bayesian approaches: Motivation and related
literature
164
2.
The hierarchical Bayesian framework
165
2.1
Gaussian scale mixtures and sparse Bayesian
learning
166
2.2
SBL framework
167
2.3
Case study: Wireless channel estimation and SBL
169
3.
Joint-sparse signal recovery
173
3.1
The MSBL algorithm
173
3.2
Expectation maximization in MSBL
174
3.3
An interesting interpretation of the MSBL cost
function
174
3.4
A Covariance-matching framework for sparse support
recovery using MMVs
175
3.5
Examples of covariance-matching algorithms for sparse
support recovery
177
4.
Exploiting intervector correlation
178
4.1
Intervector correlation: The Kalman SBL algorithm
179
4.2
Online sparse vector recovery
181
4.3
Case study (continued): Wireless channel estimation
and the KSBL algorithm
185
5.
Intravector correlations: The nested SBL algorithm
186
5.1
Nested SBL (B 6¼ IB)
189
6.
Quantized sparse signal recovery
192
7.
Other extensions
198
7.1
Decentralized SBL
198
7.2
Dictionary learning
199
7.3
Relationship with robust principal component analysis
and sparse + low-rank decomposition
200
7.4
Deep unfolded SBL
202
8.
Discussion and future outlook
202
References
203
9.
Mathematical theory of Bayesian statistics where
all models are wrong
209
Sumio Watanabe
1.
Introduction
209
2.
Mathematical theory of Bayesian statistics
211
2.1
DGP, model, and prior
211
2.2
Generalization loss and free energy
213
viii
Contents

2.3
Regular theory
217
2.4
Singular theory
220
2.5
Phase transitions
225
3.
Applications to statistics and machine learning
227
3.1
Model evaluation
227
3.2
Prior evaluation
230
3.3
Not i.i.d. cases
232
4.
Conclusion
235
References
236
10.
Geometry in sampling methods: A review on manifold
MCMC and particle-based variational inference
methods
239
Chang Liu and Jun Zhu
1.
Geometry consideration in sampling: Why bother?
239
2.
Manifold and related concepts
242
2.1
Manifold
242
2.2
Tangent vector and vector field
244
2.3
Cotangent vector and differential form
246
2.4
Riemannian manifold
247
2.5
Measure
248
2.6
Divergence and Laplacian
249
2.7
Manifold embedding
250
3.
Markov chain Monte Carlo on Riemannian manifolds
251
3.1
Technical description of general MCMC dynamics
252
3.2
Riemannian MCMC in coordinate space
253
3.3
Riemannian MCMC in embedded space
258
4.
Particle-based variational inference methods
260
4.1
Stein variational gradient descent
261
4.2
The Wasserstein space
262
4.3
Geometric view of particle-based variational inference
methods
266
4.4
Geometric view of MCMC dynamics and relation
to ParVI methods
270
4.5
Variants and Techniques Inspired by the Geometric
View
277
5.
Conclusion
286
Acknowledgments
286
References
286
Index
295
Contents
ix

This page intentionally left blank

Contributors
Sourabh Bhattacharya (67), Interdisciplinary Statistical Research Unit, Indian
Statistical Institute, Kolkata, WB, India
Noirrit Kiran Chandra (67), Department of Mathematical Sciences, The University
of Texas at Dallas, Richardson, TX, United States
Geethu Joseph (161), Delft University of Technology (TU Delft), Delft,
The Netherlands
Saurabh Khanna (161), Indian Institute of Technology (IIT) Roorkee, Roorkee, India
Chang Liu (239), Microsoft Research Asia, Beijing, China
Ryan Martin (1), Department of Statistics, North Carolina State University, Raleigh,
NC, United States
Chandra R. Murthy (161), Indian Institute of Science (IISc), Bangalore, India
A.R. Plastino (121), CeBio-Departamento de Ciencias Ba´sicas, Universidad Nacional
del Noroeste de la Prov. de Buenos Aires (UNNOBA), CONICET, Junin, Argentina
Angelo Plastino (121), Instituto de Fı´sica La Plata–CCT-CONICET, Universidad
Nacional de La Plata, La Plata, Argentina
Ranjitha Prasad (161), Indraprastha Institute of Information Technology Delhi
(IIIT-Delhi), New Delhi, India
Daniel Garcı´a Rasines (43), Department of Mathematics, Imperial College London,
London, United Kingdom
Christian P. Robert (103), CEREMADE, Universite Paris Dauphine PSL, Paris;
Department of Statistics, University of Warwick, Coventry, United Kingdom;
CREST–ENSAE, Universite Paris–Saclay, Gif-sur-Yvette, France
Vivekananda Roy (135), Department of Statistics, Iowa State University, Ames, IA,
United States
Nicholas Syring (1), Department of Statistics, Iowa State University, Ames, IA,
United States
Sai Subramanyam Thoota (161), Indian Institute of Science (IISc), Bangalore, India
Stephen G. Walker (83), Department of Mathematics, University of Texas at Austin,
Austin, TX, United States
Sumio Watanabe (209), Department of Mathematical and Computing Science, Tokyo
Institute of Technology, Meguro-ku, Tokyo, Japan
xi

G. Alastair Young (43), Department of Mathematics, Imperial College London,
London, United Kingdom
Jun Zhu (239), Department of Computer Science and Technology, Beijing National
Center for Information Science and Technology, Tsinghua-Bosch Joint Center for
ML, Tsinghua University, Beijing, China
xii
Contributors

Preface
The conceptual simplicity of the Bayesian approach to statistical inference
has been long recognized and is the subject of vigorous debate. A prior
distribution on all unknowns in a statistical model is combined with the like-
lihood function constructed from a given dataset, via Bayes’ theorem, to yield
the posterior distribution of the unknowns, from which the desired inference
on a parameter or quantity of interest is extracted. While arguments over
foundational questions—notably about Bayesian approaches to hypothesis
testing—have continued to rage, a watershed was experienced in the 1990s
with the realization that summaries of the posterior distribution could, even
in complex situations, be obtained by drawing samples using Markov chain
Monte Carlo (MCMC) techniques, such as the Gibbs sampler and Metropo-
lis-Hastings algorithms. As a result, much of the practical data analysis in
recent years can been seen to have been dominated by Bayesian modeling
and inference, supported by increasingly sophisticated sampling schemes:
sequential Monte Carlo, Hamiltonian Monte Carlo, regenerative nonreversible
MCMC, etc.
The contemporary statistical landscape, however, presents significant new
challenges to any inference paradigm. Data-rich environments seen in many
fields typically lead to highly complex parametric statistical models, often
high-dimensional, but underpinned by some notion of parameter sparsity, with
only a subset of the model parameters being important in describing some
response. Data structures themselves may be complex, in particular non-
Euclidean. It may be desirable to address several inference problems simulta-
neously, with some notion of overall error control being maintained. Complex
modeling raises the key issue of performance of inference under model
misspecification and its robustification. The actual inferential questions to
be addressed may only be decided on after initial examination of the data.
All these issues, together with continued interest in foundational issues and
computational refinements, have seen significant conceptual and practical
advances in Bayesian methodology in recent years. This volume brings
together a series of chapters that highlight many of these advances.
We briefly summarize the 10 chapters below.
Chapter 1 by R. Martin and N. Syring presents the reader with a detailed
description of modern Bayesian analysis. The authors discuss the convergence
between “in probability” and “in expectation,” fractional posteriors, the sensi-
tivity of the Bayes solution to model misspecification, etc. Chapter 2 by D.G.
xiii

Rasines and G.A. Young considers Bayesian inference from the perspective of
parameter selection. The authors have theoretically and numerically illu-
strated two classes of noninformative prior densities, namely, those derived
from normal location models and the Jeffreys prior, for understanding infer-
ences after model selection and their probability-matching properties.
Chapter 3 by N.K. Chandra and S. Bhattacharya presents arguments and
justifications in favor of multiple testing in statistical inferences, especially
using Bayesian techniques. The chapter reviews recent developments in
Bayesian multiple testing procedures and the authors’ procedures on multiple
testing in which parameters of interest have structural dependence.
Chapter 4 by S.G. Walker presents an alternative view on causes of uncer-
tainty and demonstrates how this can be handled using Bayesian arguments.
The author’s approach is to consider the role of missing data that causes
uncertainty and to develop a novel way of modeling the missing data that
can recover the traditional Bayesian posterior distribution. Chapter 5 by
C.P. Robert revisits the problem of hypothesis testing from the Bayesian
perspective. Evaluating previous contributions to this contentious problem,
it proposes a compromise approach and argues that this has desirable proper-
ties. In Chapter 6, A. Plastino and A.R. Plastino argue how Jaynes’ maximum
entropy methodology is known to be capable of affording a total reformula-
tion of Gibbs’ statistical mechanics. They describe how the methodology
can provide precise information regarding inference on ground-state wave
functions. Chapter 7 by V. Roy presents different MCMC algorithms, for
example, efficient data augmentation strategies, diffusions-based and Hamil-
tonian dynamics-based methods, etc., for fitting generalized linear mixed
models (GLMMs). The chapter demonstrates that Langevin and Hamiltonian
Monte Carlo methods can be applied to any GLMM.
Chapter 8 by G. Joseph, S. Khanna, C.R. Murthy, R. Prasad, and S.S.
Thoota provides a rigorous and in-depth overview of Bayesian frameworks
to address the general problem of sparse signal recovery and how it can be
applied to improve understanding in communications-related applications.
The authors demonstrate how such frameworks help in the context of 5G
communications, dictionary learning, etc. Chapter 9 by S. Watanabe is about
the formulation of mathematical foundations on which universal theorems
hold for the data-generating process, model building, and fitting a statistical
model based on data observations and a prior distribution. In this chapter,
the author describes how the “uncertainty” cannot be completely captured
by a statistical model and the role of “belief” in a prior distribution.
Chapter 10 by C. Liu and J. Zhu is about Bayesian inference and the geometry
of sampling methods. Through theoretical arguments and methods, the
authors show how the non-Euclidean geometrical philosophy has been
incorporated into Bayesian inference and how such insights continue to
improve Bayesian inference. Riemannian geometry and MCMC methods are
central to the analysis presented in the chapter.
xiv
Preface

We express our sincere thanks to Mariana K€uhl Leme, acquisitions editor
(Elsevier and North-Holland), for her overall administrative support through-
out the preparation of this volume. Her valuable involvement in the project is
highly appreciated. We also thank Mr. Sam Mahfoudh, the previous acquisi-
tions editor (Elsevier and North-Holland), who served in the position for
many years, for his time during the early stages of the project. We thank
Ms. Naiza Mendoza, developmental editor (Elsevier), for providing excellent
assistance to the editors and for engaging with authors in all kinds of technical
queries throughout the preparation and until the proofing stage and produc-
tion. Our thanks also go to Md. Sait Abdulla, project manager of book produc-
tion, RELX India Private Limited, Chennai, India, for leading the production,
responding to several rounds of queries by the authors, being available at all
times for printing activities, and providing assistance to the editors. Our
sincere thanks and gratitude go to all the authors for writing brilliant chapters
in keeping with our requirements for the volume. We very much thank our
referees for their timely assessment of the chapters.
We solidly believe that this volume on advancements in Bayesian methods
has come at the right time and it gives us great satisfaction to have been
involved in its production. We are convinced that this collection will be a use-
ful resource for new researchers in statistical science as well as advanced
scientists working in statistics, mathematics, and other scientific fields.
Arni S.R. Srinivasa Rao
G. Alastair Young
C.R. Rao
Preface
xv

This page intentionally left blank

Chapter 1
Direct Gibbs posterior
inference on risk minimizers:
Construction, concentration,
and calibration
Ryan Martina,∗and Nicholas Syringb
aDepartment of Statistics, North Carolina State University, Raleigh, NC, United States
bDepartment of Statistics, Iowa State University, Ames, IA, United States
∗Corresponding author: e-mail: rgmarti3@ncsu.edu
Abstract
Real-world problems, often couched as machine learning applications, involve quanti-
ties of interest that have real-world meaning, independent of any statistical model. To
avoid potential model misspecification bias or over-complicating the problem formula-
tion, a direct, model-free approach is desired. The traditional Bayesian framework relies
on a model for the data-generating process so, apparently, the desired direct, model-
free, posterior-probabilistic inference is out of reach. Fortunately, likelihood functions
are not the only means of linking data and quantities of interest. Loss functions provide
an alternative link, where the quantity of interest is defined, or at least could be defined,
as a minimizer of the corresponding risk, or expected loss. In this case, one can obtain
what is commonly referred to as a Gibbs posterior distribution by using the empirical
risk function directly. This manuscript explores the Gibbs posterior construction, its
asymptotic concentration properties, and the frequentist calibration of its credible
regions. By being free from the constraints of model specification, Gibbs posteriors cre-
ate new opportunities for probabilistic inference in modern statistical learning problems.
Keywords: Asymptotics, Empirical risk minimization, Bayesian inference, Learning
rate, M-estimation, Model misspecification, Statistical learning
1
Introduction
A hallmark of the Bayesian framework is that it is normative. That is, when
presented with a new problem, a Bayesian can immediately carry out his anal-
ysis by, first, introducing a statistical model for the data, which entails a like-
lihood function and, second, introducing a prior distribution for the unknown
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.06.004
Copyright © 2022 Elsevier B.V. All rights reserved.
1

parameters in that model. Given these two inputs, the Bayesian can apply the
familiar conditional probability formula to get a posterior distribution—
proportional to the likelihood times prior—for the unknown, given the
observed data, from which he can draw his inferences.
Of the two required inputs, the prior attracts the most criticism from non-
Bayesians. But the requirement that a Bayesian must specify a likelihood can
also be a serious obstacle in applications. A major concern is that the statisti-
cal model may not be correctly specified. In such a case, the model para-
meters have no real-world interpretation and, therefore, any inferences about
them would be downright meaningless. Marginal inference on certain features
of the model parameters can still be carried out, but there is no reason to
expect this to be reliable—in fact, inferences can be arbitrarily poor, as we
discuss below. To address this concern, there have been substantial efforts
to develop the subject of Bayesian nonparametrics (e.g., Ghosal and van
der Vaart, 2017; Ghosh and Ramamoorthi, 2003; Hjort et al., 2010), which
treats a key feature of the data-generating process, such as its density function,
as the unknown about which inferences are to be drawn. From a posterior for,
say, the density function, marginal inference about any other relevant feature is
straightforward, at least conceptually. No doubt the nonparametric formulation
makes the Bayesian’s model more flexible and, consequently, his inferences
more robust. Whether the robustness gained by going fully nonparametric is
worth the added complexity is a question that deserves consideration, but that
would have to be addressed on a case-by-case basis. In any case, the parametric
and nonparametric formulations have an important point in common, namely,
that the likelihood must fully specify everything about the data-generating
process; that is, for given values of the unknowns, whether they be finite- or
infinite-dimensional, new data could, at least in principle, be simulated accord-
ing to the posited model. This aspect of the Bayesian framework is restrictive
when, like in those cases presented below, the unknown quantity to be inferred
exists independent of or does not fully determine a statistical model. For exam-
ple, imagine the quantity of interest is a (conditional) quantile or, more gener-
ally, a minimizer of an expected loss function. In such cases, the Bayesian
framework offers no direct path to make posterior inference: only an indirect
path through a model/likelihood specification and marginalization is possible.
What a non-Bayesian approach lacks in normativity compared the Bayesian
approach it makes up for in its ability to directly infer relevant features of—
rather than everything about—the data-generating process.
The idea that statistical problems do not have to be solved as one coherent
whole is anathema to Bayesians but is liberating for frequentists.
Wasserman (2008)
The aim of this manuscript is to present a framework that we believe helps to
liberate Bayesians from the need to specify a statistical model, creating an
opportunity for direct, posterior-probabilistic inference in statistical learning
problems.
2
Handbook of Statistics

Speaking of machine learning, it is often the goal in applications to esti-
mate and make inference on a quantity of interest that is defined as a mini-
mizer of an objective function which, itself, is defined as the expected value
of a suitable loss function. It is at least conceptually straightforward to get
an empirical version of the risk function by averaging the loss function over
the observed data points. Then an estimator is readily obtained by minimizing
this empirical risk function. An upside to this approach is that it requires no
model specification and hence has no risk of model misspecification bias.
A downside, however, is that this approach is largely focused on point
estimation—it is not immediately clear how to quantify uncertainty for the
purpose of inference, except perhaps for asymptotically approximate confi-
dence regions. For a direct, probabilistic quantification of uncertainty about
the quantity of interest, without the introduction of a statistical model and
the risk of misspecification bias, we recommend the construction of a
so-called Gibbs posterior distribution. The two primary ingredients that go
into the construction of a Gibbs posterior are the empirical risk function—a
combination of the data and the loss function that defines the problem—and
a prior distribution about the risk minimizer; there is a third ingredient to be
discussed below. These go together in very much the same way that Bayes,
back in 1763, originally combined a likelihood and prior, but here it is not
based on a joint probability model. The precise definition is given in
Section 2. Then probabilistic inference about the quantity of interest based
on the Gibbs posterior proceeds exactly as it would be based on a Bayesian
posterior.
That the Gibbs posterior distribution assigns probabilities to hypotheses
about the quantity of interest does not, on its own, justify its use. So, what
makes inference based on the Gibbs posterior meaningful? In addition to
some basic principles justifying the specific definition in (4), and there are
asymptotic results of varying strength and precision (Section 3) that suggest
the Gibbs posterior will, with a sufficiently informative sample, concentrate
its mass around the true risk minimizer. Intuitively, this latter point implies
inferences based on a Gibbs posterior cannot be misleading, for example,
point estimators derived from it cannot be far from whatever feature of the
true risk minimizer they are supposed to be estimating.
Of course, we want more from our framework of inference than “not being
misleading” and, for this, special care is needed. Toward this, the third ingre-
dient in the Gibbs posterior distribution construction, left out of the explana-
tion above, is a so-called learning rate. Roughly speaking, this learning rate is
a tuning parameter that controls the spread of the Gibbs posterior. This does
not affect the asymptotic concentration of mass claim above, but it does affect
the Gibbs posterior’s limiting form and, therefore, it also affects the reliability
of inferences, for example, the coverage probability of Gibbs posterior credi-
ble regions. So this learning rate cannot be ignored, but must be treated care-
fully. This is discussed in detail in Section 4. There we describe a particular
algorithm designed to tune the learning rate, in a data-driven way such that
Direct Gibbs posterior inference on risk minimizers Chapter
1
3

the Gibbs posterior credible regions attain the nominal frequentist coverage
probability, hence providing reliable—instead of just “not misleading”—
inferences, even in finite samples.
In Section 5, we present three numerical illustrations in common statistical
or machine learning applications: quantile regression, classification, and (non-
parametric) regression. The focus of these examples is the role played by the
learning rate and, more specifically, how the seemingly inconvenient need to
specify the learning rate can be leveraged to obtain valid Gibbs posterior cred-
ible regions.
Of course, there has been a surge of interest in generalized Bayes in recent
years, so there is more to discuss than could be fit into this one manuscript. In
Section 6 we take the opportunity to mention a few of these developments that
are outside the scope of the present paper, and also to list a few open problems
that we believe, if solved, would make for nice contributions to the expanding
literature in this direction.
Some concluding remarks are made in Section 7 but, for us, the key take-
away message is as follows. Real-world problems often involve quantities of
interest that have real-world meaning independent of a statistical model. To
avoid either risking model misspecification bias or overly complicating the
model formulation, a direct, model-free attack on the quantity of interest is
needed. In statistical learning applications, often the quantity of interest is,
or least can be, expressed as a minimizer of a suitable expected loss function.
This loss- rather than likelihood-focused link between the data and quantity of
interest creates an opportunity for posterior-probabilistic inference, different
from Bayes. In our view, the results presented here make for a substantial first
step toward “liberating” the Bayesian paradigm from its reliance on models
for the data-generating process.
2
Gibbs posterior distributions
2.1
Problem setup
Suppose we have data T1, …, Tn assumed to be independent and identically
distributed (iid) from some distribution P supported on a set . Note that
the individual Ti’s can be very general, so, for example, this setup can accom-
modate the typical supervised learning problem where Ti ¼ (Xi, Yi) consists of
a set of features/examples Xi  r, possibly high-dimensional, and a response/
label Yi  . Of course, dependence within Ti is allowed, and captured by P,
but independence between different Ti’s is assumed.
The key difference between the Gibbs and Bayesian formulation is that,
while the latter is likelihood-based, that is, defined through specification of
a statistical model, the former is loss function-based. That is, define a (real-
valued) loss function (t, θ) 7! ‘θ(t) on  Θ that measures the compatibility
of the value θ of the quantity of interest with the data point t. Common exam-
ples of loss functions include
4
Handbook of Statistics

‘θðtÞ ¼ fy  θðxÞg2
and
‘θðxÞ ¼ 1fy 6¼ θðxÞg,
t ¼ ðx, yÞ,
where, in the former case, θ determines a regression function and its compati-
bility is measured by a squared-error loss and, in the latter case, θ determines
a binary classifier and compatibility is measured by a 0–1 loss; here and
throughout, 1() denotes the indicator function. Of course, other kinds of loss
functions are possible, and we will see several such examples in what follows.
As one would expect, we want the “loss” to be small in a certain sense, so our
goal is to minimize an expected loss, or risk,
RðθÞ ¼ P‘θ,
θ  Θ,
where we use the operator notation for expected value of the random variable
‘θ(T) with respect to T  P. So then the quantity of interest is the risk
minimizer
θ?  arg min
θ  Θ RðθÞ,
(1)
where “ ” allows for the possibility that the risk minimizer is not unique.
Since we do not know P, the risk function is inaccessible and, therefore, so
too is the risk minimizer. Then the goal is to make inference on the unknown
θ? based on iid observations Tn ¼ ðT1, …, TnÞ from the unspecified distribu-
tion P.
Toward this goal, we can proceed by first replacing the inaccessible risk
function R with an empirical risk
RnðθÞ ¼ 1
n
X
n
i¼1
‘θðTiÞ, θ  Θ,
and then estimating the risk minimizer, θ?, by the empirical risk minimizer
^θn  arg min
θ  Θ RnðθÞ:
(2)
In some contexts, the empirical risk minimizer is referred to as an
M-estimator, and its statistical properties have been extensively studied; see,
for example, Huber (1981), van der Vaart and Wellner (1996), van der
Vaart (1998), Kosorok (2008), and Boos and Stefanski (2013). Beyond simply
estimating the risk minimizer, our goal is to incorporate prior information, if
available, and to quantify uncertainty about θ? with a data-dependent proba-
bility distribution on Θ, called a Gibbs posterior, defined next.
The above discussion focused on situations where the quantity of interest
has a concrete, real-world interpretation, for example, θ is defined as the value
that makes the misclassification error probability as small as possible. How-
ever, there are other situations in which a Gibbs posterior-based approach
may have advantages. Suppose, instead, that θ is defined as, say, a feature of
the parameter of a posited statistical model. It is not so uncommon these days
for such models to have many nuisance parameters, intractable likelihood
Direct Gibbs posterior inference on risk minimizers Chapter
1
5

function, or some other complicating aspect. In such cases, it may not be
unreasonable to abandon the statistical model altogether and seek a direct con-
struction of a Gibbs posterior for θ. This would require “reverse engineering” a
loss function such that θ can be re-expressed as a risk minimizer. Examples of
this reverse engineering can be found in Syring and Martin (2020) and Wang
and Martin (2020, 2021).
Finally, there is another broad—and familiar—class of problems in which
the same risk-minimization terminology and methodology can be applied.
Suppose we posit a statistical model P ¼ fPθ : θ  Θg , which could be
finite- or infinite-dimensional, and assume that data T1, …, Tn are iid Pθ. Then
the goal is to estimate the unknown value of the posited model parameter.
However, as the too-often used quote reads, “All models are wrong…,” it is
necessary to investigate the properties of an estimator of θ when the posited
model happens to be wrong. From this perspective, we formulate this as a
risk-minimization problem where the loss function is
‘θðtÞ ¼  log pθðtÞ,
ðt, θÞ   Θ,
(3)
where pθ is the density or mass function associated with Pθ. If P 62 P denotes
the true distribution, then the risk function is R(θ) ¼ K(P, Pθ), the Kullback–
Leibler divergence of Pθ from P, so the inferential target, θ?, is the parameter
value that corresponds to the element in P closest to P in the Kullback–Leibler
sense. Moreover, the empirical risk minimizer is the maximum likelihood esti-
mator and, under certain conditions, consistency and asymptotic normality hold.
However, valid inference on θ? will require some adjustments to account for the
model misspecfication.
If this misspecified posterior is equipped with a learning rate η, that is, a
power η < 1 on the likelihood function in the Bayes formulation, then the
resulting fractional Bayes posterior (e.g., Bhattacharya et al., 2019) coincides
with a Gibbs posterior based on the log-loss (3). We do not pursue this spe-
cific instantiation of Gibbs posteriors any further in this paper, largely because
we find the most compelling case for the Gibbs posterior comes from pro-
blems where there is no statistical model that directly connects the data and
the quantity of interest.
2.2
Definition
With the quantity of interest defined via a loss function, instead of a likeli-
hood, the construction of a genuine Bayes posterior distribution is out of
reach. However, we could easily just mimic the Bayes’s formula by substitut-
ing the empirical risk in place of the negative log-likelihood. This is precisely
the Gibbs posterior, that is,
ΠðηÞ
n ðdθÞ ∝eηnRnðθÞ ΠðdθÞ,
θ  Θ,
(4)
6
Handbook of Statistics

where Π is a prior distribution and η > 0 is a so-called learning rate parame-
ter that will be discussed in more detail below. The normalizing constant is
determined by integrating the right-hand side of (4) with respect to θ, so we
are implicitly assuming integrability here; this holds, for example, whenever
‘θ is bounded away from ∞. Clearly, like Bayes’s rule, this construction
balances the prior and data contributions, with the data component dominat-
ing, at least for large n, thanks to the summation in the exponent.
Our interpretation of the Gibbs posterior distribution, ΠðηÞ
n , defined in (4),
is as a measure that provides uncertainty quantification about θ. This is not so
much different from the (nonsubjective) interpretation of an ordinary Bayes-
ian posterior distribution. While the Gibbs posterior does have an interpreta-
tion as a coherent update of prior information in light of observed data—see
Bissiri et al. (2016) and below—we do not find this alone to be compelling
justification for the use of a Gibbs posterior. Our view is that the value of
the Gibbs posterior, or any other statistical inference procedures for that mat-
ter, is determined by its operating characteristics, its frequentist sampling dis-
tribution properties. That is, the Gibbs posterior is not meaningful or useful
based on its definition alone. Instead, the Gibbs posterior is useful only when
it can be established that inferences drawn based on it are reliable in a
frequentist sense. This distinction is important for various reasons. One in par-
ticular is that it affects the way we approach the learning rate selection
problem; see Section 4.
The loss and empirical risk component of the Gibbs posterior is what distin-
guishes it from a Bayesian posterior, so naturally that has been (and will be)
our focus. However, our alternative perspective also has unexpected implica-
tions on the prior distribution. In a typical Bayesian setting, the quantities of
interest are model parameters and they have no meaning outside that model,
no real-world interpretation. So it should come as no surprise that genuine
prior information would typically be lacking for unknowns that have no
real-world interpretation. For example, suppose the posited model is a gamma
distribution: where would genuine prior information about the shape parame-
ter come from? In the situations we have in mind, however, where the quantity
of interest has a real-world interpretation, through the risk-minimizer charac-
terization, it is not unreasonable to expect that genuine prior information
might be available. For example, in the illustration in Section 2.4.2, the param-
eter is closely related to the efficacy of a medical treatment, and it is reason-
able to expect medical professionals have some prior information about its
value for existing treatments. If this real-life information can be encoded in
a prior distribution Π and incorporated into the Gibbs posterior in (4), the
Gibbs formulation has an upper hand in terms of efficiency compared to a
more traditional model-based approach that would have the difficult task of
re-expressing that available information about θ in terms of its model
parameter.
Direct Gibbs posterior inference on risk minimizers Chapter
1
7

Having briefly explained our interpretation of the Gibbs posterior in (4),
we should also say a few words about another common interpretation. In
the computer science/machine learning literature, the Gibbs posterior is often
understood simply as a “randomized estimator”; see Section 2.3.3. That is,
since their goal is simply to find parameter values that make the risk function
small, ΠðηÞ
n
can be interpreted as an algorithm for generating samples ~θ such
that Rð~θÞ, or Rnð~θÞ, tends to be small. An advantage of the randomized estima-
tor over the empirical risk minimizer is that, from a computational point of
view, it might be easier to simulate from the Gibbs posterior than to solve
the empirical risk minimization problem.
2.3
FAQs
Next, we have made a list of some frequently asked questions (FAQs) about
the Gibbs posterior properties, interpretation, and construction.
2.3.1
Why a learning rate? Is it really needed?
The reader might be surprised by the introduction of the learning rate η. Why
is this needed? The quantity of interest, θ?, is defined solely as the solution to
an optimization problem, and the same θ? emerges as the solution if the loss
function ‘θ is replaced by c ‘θ for any c > 0. This scale of the loss function is
irrelevant when focus is solely on the (empirical) risk minimization; but since
the Gibbs posterior construction requires balancing the influence of the prior
with that of the data, the scale of the loss function matters in practical appli-
cations. Indeed, we will see below that, without a careful choice of the
learning rate η, inference based on the Gibbs posterior can be unreliable.
We will discuss the learning rate selection process in detail in Section 4,
but for now we need to make one important remark about this. Note that
the data do not directly carry any information relevant to η, so one cannot
treat this choice in a Bayesian way, with prior-to-posterior updating. The
point is that η is a tuning parameter, not a model parameter about which data
are directly informative. As such, we must rely on some other data-driven
strategies (e.g., using resampling) to select the learning rate.
2.3.2
Difference between Gibbs and misspecified Bayes?
We mentioned above that one common way in which this loss function-based
perspective emerges is when a model P ¼ fPθ : θ  Θg is specified but it
happens to be misspecified. Then the Gibbs posterior in (4), with η ¼ 1, is pre-
cisely the Bayes posterior for this misspecified model. The behavior of the
Bayesian posterior under model misspecification has been extensively studied;
see, for example, Berk (1966), Bunke and Milhaud (1998), Kleijn and van der
Vaart (2006, 2012), De Blasi and Walker (2013), and Ramamoorthi et al.
(2015). Is the Gibbs posterior really any different than this?
8
Handbook of Statistics

First, even though Gibbs and misspecified Bayes share a resemblance, one
key difference is the learning rate, η. In correctly specified model cases, the
balance between the contributions from prior and data is automatic. In misspe-
cified model cases, however, this balance is thrown off and needs to be
corrected manually through the insertion of a learning rate η 6¼ 1. Ignoring
this learning rate adjustment can lead to posterior credible intervals that have
arbitrarily bad frequentist coverage probability; see, for example, Example 2.1
in Kleijn and van der Vaart (2012). So if one cares about the reliability of
their inferences, the learning rate must be considered and, therefore, a distinc-
tion between the Gibbs and misspecified Bayes perspectives is necessary.
Second, the Gibbs framework is most ideally suited for cases in which no
statistical model is assumed, so one cannot even ask if the model is misspeci-
fied or not. Setting aside those examples, for example, classification, where
the loss function is a defining feature of the problem, there are other examples
where it can be advantageous to introduce a loss function and construct a Gibbs
posterior. A simple but important example is that of inference on the quantile of
a distribution. The quote from Wasserman (2008) stated in Section 1 continues:
To estimate a quantile, an honest Bayesian needs to put a prior on the space of
all distributions and then find the marginal posterior. The frequentist need not
care about the rest of the distribution and can focus on much simpler tasks.
That is, the Bayesian can either start by specifying a statistical model, and risk
introducing model misspecification bias, or go fully nonparametric and deal
with those associated challenges. Wasserman’s “frequentist” is not the only
one with a direct solution to this problem. There is a well-known characteri-
zation of a quantile as the minimizer of suitable expected loss, which can
be used to construct a Gibbs posterior, which can be used to make direct pos-
terior inference on a quantile, with no risk of model misspecification bias and
no nuisance parameters to be marginalized over.
2.3.3
Principles behind the Gibbs posterior construction?
The definition of the Gibbs posterior distribution in (4) may give the reader
the impression that the Gibbs posterior is defined to mimic the Bayesian pos-
terior, with basically the log-likelihood replaced by the empirical risk, but this
is not the case: the Gibbs posterior was first constructed in a principled man-
ner and for a specific purpose. Actually, there are (at least) two such construc-
tions, and here we briefly review both of these derivations. The first
construction is primarily due to Zhang (2006a,b) and shows that Gibbs poste-
rior is the minimax optimal randomized estimator with respect to the
expected, posterior-averaged risk. The second construction is from Bissiri
et al. (2016) and shows that Gibbs posterior can be interpreted as a coherent
updating of beliefs about the parameter, much like the Bayesian posterior,
but in the case the information in the data related to θ is captured via a loss
function rather than a likelihood.
Direct Gibbs posterior inference on risk minimizers Chapter
1
9

Following the setup in Section 2, a reasonable strategy for learning about θ
is to estimate it by minimizing the empirical risk Rn(θ) over θ  Θ; penalty
terms may be added, but that is beyond our scope here. Alternatively, one
may define a randomized estimator of θ, which is a data-dependent distribution
bΠn that depends on data Tn and a prior Π. The idea is that, if bΠn is “good,” then
samples from bΠn ought to be similarly good estimators of θ. (In some cases,
sampling from bΠn is computationally easier than minimizing the empirical risk,
so this may be an attractive choice.) What makes for a “good” randomized
estimator? One reasonable criterion would be to insist that the bΠn-average risk,
R
RðθÞ ^ΠnðdθÞ, is small in some sense. This would imply that samples from bΠn
tend to be close to θ?. Zhang (2006a,b) showed that
Pn
Z
ðη1 log Peη‘θÞ bΠnðdθÞ  Pn
Z
RnðθÞ bΠnðdθÞ+ ðηnÞ1KðbΠn, ΠÞ


,
(5)
where the outer expectation, Pn, is with respect to the data Tn on which bΠn
depends, and K is the Kullback–Leibler divergence. Furthermore, in certain
applications it can be shown that
Pn
Z
RðθÞ bΠnðdθÞ  Pn
Z
ðη1 log Peη‘θÞ bΠnðdθÞ,
for all small η,
so that Zhang’s bound also bounds the expected, posterior-averaged risk.
Zhang suggests the randomized estimator bΠn be chosen so that the bracketed
term in the upper bound in (5) is made as small as possible—a kind of mini-
max optimality. It is straightforward to show the minimizer of Zhang’s bound
is precisely the Gibbs posterior as defined in (4).
Bissiri et al. (2016) derive the Gibbs posterior distribution as a coherent
updating of beliefs from a prior distribution Π on Θ to a posterior distribution
bΠn in light of data T. In the Bayesian setting, the prior Π and the likelihood
both carry information about θ, and Bayes’ Rule provides the mechanism
for combining these two sources of information. When the likelihood is not
available, but a loss ‘θ measuring agreement/discrepancy between data and
parameter is available, then Bissiri et al. (2016) argue there must still be an
optimal way to combine information in the prior and loss to provide a poste-
rior update to beliefs about θ. This optimal update bΠn should be the posterior
distribution that best matches some combination of loss function and prior, or,
in other words, minimizes a discrepancy between the posterior and loss, and
the posterior and prior. They argue that, for coherence of the resulting poste-
rior, this discrepancy must be of the form
Ψ 7!
Z
RnðθÞ ΨðdθÞ + ðηnÞ1KðΨ, ΠÞ:
(6)
10
Handbook of Statistics

Minimizing this discrepancy is equivalent to minimizing Zhang’s upper
bound, and implies the coherent updating rule they seek is the same Gibbs
posterior distribution in (4).
Neither derivation above sheds light on the choice of learning rate η. Any
η > 0 leads to a coherent updating of beliefs, and, likewise, the Gibbs poste-
rior for any learning rate η > 0 minimizes Zhang’s upper bound of the
expected annealed risk, since the latter is a function of η. It must be that other
considerations are needed to guide the choice of learning rate, and we discuss
these in Section 4.
2.4
Illustrations
2.4.1
Quantiles
Wasserman, in the quote above, brought up the example of inference on a
quantile, so we take this as our first illustration. Specifically, suppose we have
data Tn ¼ ðT1, …, TnÞ iid from distribution P, and the quantity of interest is
θ ¼ θ(P), the τth quantile of P. A proper Bayesian solution requires that we
put a prior distribution on P—either through a parametric model, Pζ, and a
prior on ζ, or through a nonparametric model—and then get the corresponding
marginal posterior for θ. To simultaneously avoid the risk of model misspeci-
fication bias from introducing a parametric model and the computational
challenges of working with a prior for the infinite-dimensional P, we proceed
to construct a Gibbs posterior for θ. This requires a characterization of θ as
the minimizer of a suitable risk (expected loss) function. But recall that
the τth quantile of a distribution can be expressed as the minimizer of
R(θ) ¼ P‘θ, where
‘θðtÞ ¼ ðt  θÞ ðτ  1tθÞ:
(7)
This immediately leads to a Gibbs posterior distribution, with a density (with
respect to Lebesgue measure) given by
πðηÞ
n ðθÞ ∝eη nRnðθÞ πðθÞ,
θ  Θ,
(8)
where Rn is the empirical risk corresponding to the loss function in (7), and π
is a prior density for θ supported on Θ  .
Since this is a scalar parameter problem and the loss function is relatively
simple, it is easy to visualize the Gibbs posterior density. Here we focus on
visualizing the role played by the learning rate on the spread of the Gibbs pos-
terior density. Set τ ¼ 0.7 so that interest is in inference on the 70th percen-
tile. We simulate data from a gamma distribution with shape parameter 5
and scale parameter 1, so the true quantile is θ?  5.89. Plots of the Gibbs
posterior density for θ are shown in Fig. 1 for two sample sizes n and three
different learning rate values, η  {0.1, 0.5, 1.0}. The first point that deserves
mention is that the Gibbs posterior densities are roughly centered around θ?
Direct Gibbs posterior inference on risk minimizers Chapter
1
11

and, as expected, they become more concentrated as sample size increases.
Second, as the learning rate varies, the center of the Gibbs posterior is mostly
unchanged, but smaller η clearly increases the posterior spread. This highlights
the impact of the learning rate on the quality of our Gibbs posterior inference.
2.4.2
Minimum clinically important difference
Next, we revisit the example that was the genesis of our investigations into
Gibbs posteriors. In a medical context, imagine that patients are given a treat-
ment and the goal is to determine if the patients’ circumstances have signi-
ficantly improved from pre- to posttreatment. A standard—and purely
statistical—approach would be to determine a cutoff such that, if the patients’
observed change in, say, blood pressure, exceeds that cutoff, then it is deter-
mined that this cannot be attributed to chance alone and, therefore, the
treatment is judged to be statistically significant. But it is well-known that
statistical significance does not imply clinical significance, so it may be
necessary to consider the latter directly. One way to do so is to redefine the
aforementioned cutoff so that it incorporates the patients’ experience, and that
cutoff is commonly referred to as the minimum clinically important differ-
ence, or MCID. Hedayat et al. (2015) formalized this as follows: In addition
to a real-valued, patient-specific diagnostic measure X (e.g., pre- to posttreat-
ment change in blood pressure), suppose we also have a binary, patient-
reported assessment Y, where Y  {1, +1}, with Y ¼ +1 if the patient felt
the treatment was effective and Y ¼ 1 otherwise. Then the data consist of
pairs T ¼ (X, Y ), having distribution P, and the MCID threshold is defined as
θ? ¼ arg min
θ PfY 6¼ signðX  θÞg:
That is, the MCID θ? is the cutoff θ on the diagnostic measure scale that
makes sign(X  θ) as best a predictor of Y as possible. Equivalently, this
2
4
6
8
10
0.0
0.1
0.2
0.3
0.4
0.5
θ
πn(θ)
x
2
4
6
8
10
0.0
0.5
1.0
1.5
θ
πn(θ)
x
A
B
FIG. 1
Plots of the Gibbs posterior density for the τ ¼ 0.7 quantile for two different sample sizes
and three different learning rates: η ¼ 0.1 (black), η ¼ 0.5 (red), and η ¼ 1.0 (green). True θ? 
5.89 is marked by “x” on the θ-axis. (A) n ¼ 25; (B) n ¼ 50.
12
Handbook of Statistics

can be expressed as the minimizer of a risk function R(θ) ¼ P‘θ, as in (2),
with the corresponding loss function
‘θðtÞ ¼ 1
2 f1  y signðx  θÞg,
t ¼ ðx, yÞ:
Given iid data Ti ¼ (Xi, Yi), for i ¼ 1, …, n, from P as described above, the MCID
can be estimated by minimizing the empirical risk RnðθÞ ¼ n1 Pn
i¼1 ‘θðTiÞ.
Alternatively, in Syring and Martin (2017), we constructed a Gibbs posterior dis-
tribution as in (4).
An important point we want the reader to take away from this example is
that, unlike the previous example, where the empirical risk could be inter-
preted as a “log-likelihood” with respect to an asymmetric Laplace model,
the Gibbs posterior solution in the MCID cannot be understood as a Bayesian
solution. That is, there is no plausible model for the data having negative log-
likelihood equal to Rn(θ) above. A data analyst wishing to take a Bayesian
approach to make inference on the MCID problem will likely treat θ as a
feature of a statistical model. For example, he might take a relatively simple
and familiar approach through a binary regression model with, say, the logit
link. The Bayesian solution requires prior specifications for (and posterior
computations of ) the intercept and slope parameters, (α, β), and then marginal
inference on the MCID defined as θ ¼ α/β. Alternatively, to guard against
potential biases due to model misspecification, the data analyst might settle
on a nonparametric formulation in which the link function, say, g(x), in the
binary regression is itself the model parameter and a more complicated ver-
sion of the prior specification and posterior computation for g, followed mar-
ginalization g ! θ must be carried out. We attempted both of these strategies
and found that, for inference on the MCID θ, the former would often be
biased while the latter would often sacrifice efficiency; see, for example,
Syring and Martin (2017), Figure 1.
Moreover, this distinction between the Gibbs and Bayesian solutions is
important in terms of how the learning rate is treated. Fig. 2A shows plots
of the Gibbs posterior density for the MCID for three different learning rate
values. Note the significant effect the learning rate has on the concentration
of the Gibbs posterior around the empirical risk minimizer. Clearly the
learning rate is crucial to the method’s validity, so its choice should be han-
dled with care.
To help drive the latter point home, we present the results of a brief simu-
lation study in Fig. 2B. There we plot the (Monte Carlo estimate of the) cov-
erage probability of 95% Gibbs posterior credible intervals for θ as a function
of the learning rate η. As expected, we find that relative small (resp. large)
learning rates lead to credible intervals that over (resp. under) cover. But
the meaning of “relatively small/large” changes with the sample size n, that
is, the learning rate value needed to hit the nominal coverage probability
exactly depends on n—larger n requires smaller η. Since the distribution
Direct Gibbs posterior inference on risk minimizers Chapter
1
13

P is unknown, the Monte Carlo computations done in Fig. 2B cannot be car-
ried out in practice, so a data-driven learning rate selection procedure is
required; see Section 4.
3
Asymptotic theory
3.1
Objectives and general strategies
If our intention is to use the Gibbs posterior, ΠðηÞ
n , in (4), for making inference
on the risk minimizer, then a first basic requirement is that it should concen-
trate its mass, at least as n ! ∞, around the true value θ?. More specifically,
if d is a suitable distance (or divergence of some kind) on Θ, then we would
expect the Gibbs posterior to satisfy
ΠðηÞ
n ðfθ : dðθ, θ?Þ > εgÞ ! 0
as n ! ∞for any ε:
(9)
The reader should keep in mind that ΠðηÞ
n
depends on data, so it is a random
measure and the convergence above is in a stochastic sense. This will be made
more precise below.
To start, we want to provide some intuition as to why we would expect the
above concentration property to hold. Consider the case where ΠðηÞ
n has a den-
sity with respect to Lebesgue measure, which we denote by πðηÞ
n , as given in
(8). If we ignore the influence of the prior distribution, which is not unreason-
able since nRn() becomes more and more influential as n ! ∞, then we find
that the Gibbs posterior density will be maximized at θ ¼ ^θn, the empirical
risk minimizer. And thanks to normalization, it will tend to concentrate its
mass around the point at which the density is maximized. Since Rn()  R()
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
0
1
2
3
θ
πn(θ)
x
0.0
0.2
0.4
0.6
0.8
0.75
0.80
0.85
0.90
0.95
1.00
η
Coverage
A
B
FIG. 2
Plots associated with the MCID example in Section 2.4.2. Panel (A) shows the Gibbs
posterior density for a simulated data set with three learning rates: η ¼ 0.25 (black), η ¼ 0.5
(red), and η ¼ 1.0 (green); true θ?  0 is marked by “x.” Panel (B) shows the coverage probability
of 95% Gibbs posterior credible regions as a function of the learning rate η, for n ¼ 50 (green), n
¼ 100 (red), and n ¼ 200 (black). (A) Posterior densities; (B) Coverage versus η.
14
Handbook of Statistics

pointwise and often uniformly, we expect ^θn  θ?. Therefore, ΠðηÞ
n is expected
to concentrate its mass around θ?. The technical details that follow explain
how these expectations become reality.
It was mentioned above that our perspective on the Gibbs posterior, which
we consider to be a “statistical perspective,” differs from that commonly taken
in the computer science/machine learning literature. Indeed, if the Gibbs poste-
rior is viewed simply a randomized estimator of the risk minimizer, then the
only thing that matters is the posterior probability assigned to risk difference
neighborhoods, that is, {θ : R(θ)  R(θ?)  δ}. From our perspective, the Gibbs
posterior can be used for general uncertainty quantification so there are other
geometrically more natural metrics to consider and even more refined distribu-
tional properties, for example, asymptotic normality, that would be both interest-
ing and relevant. Below we present the results from our statistical perspective,
and remark on the implications for those who focus solely on risk minimization.
3.2
Consistency
Consistency for a Gibbs posterior distribution means something very similar
to consistency for a point estimator—both imply a certain random variable
converges in probability to the “right” value. This is made precise in
Definition 1. For a given divergence d : Θ  Θ ! +, the Gibbs posterior
distribution ΠðηÞ
n
is consistent at θ? if
ΠðηÞ
n ðfθ : dðθ, θ?Þ > εgÞ ! 0,
in P-probability; as n ! ∞:
(10)
Of course, since the Gibbs posterior probability is bounded, the “in
P-probability” convergence in (9) implies convergence in P-expectation or
“in L1(P).” Strategies that aim to bound the P-expectation of the event in
(10), rather than the P-probability of that event, may produce additional ben-
efits, namely, finite-sample bounds. We do not consider such strategies here,
but see Syring and Martin (2022). This notation of consistency also depends
on the choice of divergence d. Common choices include a natural/generic
metric on Θ, such as Euclidean distance, or a problem-specific divergence
such as d(θ, θ?) ¼ {R(θ)R(θ?)}1/2. Because of this dependence, we should
technically write that (10) implies ΠðηÞ
n is d-consistent, but here d will be taken
as a given feature of the problem and left implicit in the notation.
In the classical consistency results for M-estimators, or empirical risk
minimizers, as presented in van der Vaart (1998, Sec. 5.2), there are two
key sufficient conditions. In words, first a uniform law of large numbers is
needed to ensure that the risk function to be minimized can be estimated accu-
rately, uniformly over Θ; second, a separation or identifiability condition is
needed to ensure that the risk minimizer can be identified. These two condi-
tions, in mathematical detail, are presented in (11a) and (11b), respectively:
Direct Gibbs posterior inference on risk minimizers Chapter
1
15

sup
θ  Θ
jRnðθÞ  RðθÞj ! 0
in P-probability
(11a)
inf
θ:dðθ, θ?Þ>δfRðθÞ  Rðθ?Þg > 0
for any δ > 0:
(11b)
Interestingly, the above sufficient conditions for M-estimator consistency turn
out to be almost enough to establish Gibbs posterior consistency. All that
remains is to ensure that the prior assigns a sufficient amount of mass in the
limit θ?, that is,
Πðfθ : RðθÞ  Rðθ?Þ < δgÞ > 0
for all δ > 0:
(12)
This is a very mild condition and, for example, would be satisfied in finite-
dimensional settings where the prior has a strictly positive density in a neigh-
borhood of θ?.
Theorem 1. If (11) and (12) hold, then the Gibbs posterior as defined in (4),
is consistent in the sense of Definition 1.
Proof. See Appendix A.1.1.
□
Since the conditions in (11) are identical to a common set of sufficient
conditions for consistency of the M-estimator, they can be checked in all sorts
of practically relevant examples. For instance, in the quantile illustration pre-
sented in Section 2.4.1, condition (11b) holds if P admits a unique τth quantile,
and condition (11a) holds, at least for compactly supported P, by the
Glivenko–Cantelli theorem, a consequence of the fact that θ 7! ‘θ is Lipschitz
(e.g., van der Vaart, 1998, Example 19.7).
3.3
Concentration rates
A more refined characterization of the asymptotic behavior of the Gibbs pos-
terior distribution can be described by its concentration rate. Roughly, the
Gibbs posterior has concentration rate εn if radius-εn neighborhoods of θ?
have vanishing ΠðηÞ
n -probability.
Definition 2. For a vanishing sequence εn > 0, the Gibbs posterior distribu-
tion ΠðηÞ
n
has concentration rate εn at θ? if
ΠðηÞ
n ðfθ : dðθ, θ?Þ > MnεngÞ ! 0,
in P-probability; as n ! ∞,
(13)
where Mn is either a sufficiently large constant or a sequence diverging
arbitrarily slowly.
Comparing Definitions 2 and 1, the former has a shrinking radius while the
latter has a fixed radius, so the former result is stronger. In the typical case
where εn ¼ n1/2, (13) can be compared to, for example, a central limit
theorem-type result where the spread of the (in this case, Gibbs posterior)
distribution is shrinking at rate n1/2.
16
Handbook of Statistics

Since the rate result is stronger than consistency, we can expect the suffi-
cient conditions here to be stronger and more difficult to verify than those for
consistency in Section 3.2. Fortunately, like above, connections to the
M-estimator asymptotic theory are available to guide us. Following van der
Vaart (1998, Sec. 5.8), we require (P, ‘θ) to be such that, for some (α, β) with
α > β > 0, and for all small δ > 0,
P
sup
θ:dðθ, θ?Þ<δ
jnð‘θ  ‘θ?Þj ≲δβ
(14a)
inf
θ:dðθ, θ?Þ>δfRðθÞ  Rðθ?Þg ≳δα,
(14b)
where “≲” and “≳” denote inequality up to a constant multiple, and n
denotes the empirical process, that is, n f :¼ n1=2ðn f  PfÞ; note that, in
case the random variable in (14a) is not measurable, the expectation can be
replaced by an upper expectation. Intuitively, at least from the M-estimation
perspective, what matters is that empirical risk Rn has roughly the same
behavior as the risk R; in that case, if R has a discernible minimizer, then
Rn will too. While the empirical process notation complicates matters, condi-
tion (14a) amounts to having some uniform control on the fluctuations of Rn
around R. Moreover, (14b) is a refined version of the condition (11b) that pro-
vides some quantification of how discernible the minimizer of R is. If these
conditions are satisfied, then the Gibbs posterior distribution will concentrate
at θ? at a rate determined by the pair (α, β). In particular, smaller α means
greater discernibility, and smaller β means tighter control on the fluctuations,
which should make the concentration rate faster.
As above, the conditions (14) are almost enough to establish the Gibbs
posterior concentration rate. All that remains is to ensure the prior assigns suf-
ficient mass to neighborhoods of θ of the appropriate radius. This is a bit more
complicated than the analogous condition (12) for consistency. To state this
condition precisely, we will need the two functions
mðθ, θ?Þ ¼ RðθÞ  Rðθ?Þ
and
vðθ, θ?Þ ¼ Pð‘θ  ‘θ?Þ2  m2ðθ, θ?Þ,
and the corresponding neighborhood
ΘðrÞ ¼ fθ : mðθ, θ?Þ _ vðθ, θ?Þ  rg,
r > 0:
Theorem 2. Consider a finite-dimensional θ, taking values in Θ  q for
some q 	 1. Suppose (P, ‘θ) are such that (14) hold with constants (α, β)
satisfying α 	 2β, and define εn ¼ n1/(2α2β). If the prior Π satisfies
ΠfΘðεnÞg ≳εq
n,
(15)
then (13) holds and the Gibbs posterior ΠðηÞ
n
has concentration rate εn at θ?.
Proof. See Appendix A.1.2.
□
Direct Gibbs posterior inference on risk minimizers Chapter
1
17

In regular finite-dimensional problems, where “regular” means that the
empirical risk has a certain degree of smoothness, the Gibbs posterior concen-
tration rate would be root-n, that is, the conditions of Theorem 2 could be
checked with α ¼ 2 and β ¼ 1, so that εn ¼ n1/2. For example, in the quantile
problem from Section 2.4.1, since the loss is Lipschitz, control over the
random fluctuations follows from standard results, for example, Corollary
19.35 in van der Vaart (1998). In particular, (14a) holds with β ¼ 1 where
d(θ, θ?) ¼ jθ  θ?j. Moreover, if P admits a density function that is positive
at θ?, the risk R is approximately quadratic in a neighborhood of θ?, so (14b)
holds with α ¼ 2. Putting this together, if the prior density is bounded away
from 0 in a neighborhood of θ?, then it follows from Theorem 2 that the Gibbs
posterior concentrates at a root-n rate. But rates faster and slower than root-n
are possible outside of these “regular” problems. A good example is the
MCID application in Section 2.4.2: as Syring and Martin (2017) show, the
rate can be as fast as n1 and as slow as n1/3, depending on certain features
of the underlying P.
The proof of Theorem 2 can easily be adapted to handle infinite-
dimensional θ by modifying Lemmas 3 and 4 in Appendix A.1; see, for exam-
ple, Syring and Martin (2020, 2022).
3.4
Distributional approximations
Beyond consistency and rates, there are cases in which the Gibbs posterior
enjoys a version of the celebrated Bernstein–von Mises theorem, that is, that
the Gibbs posterior takes on a Gaussian shape asymptotically. This was
demonstrated for a special case in Bhattacharya and Martin (2022) but their
results are generalized below.
Common folklore is that the Bernstein–von Mises theorem guarantees the
Bayesian posterior will be asymptotically calibrated in the sense that its
credible regions will agree with the frequentist confidence regions; hence
that Bayesian inference would be at least approximately valid, in a frequen-
tist sense, for large n. This suggest a best-of-both-worlds conclusion, that is,
that one can have both the appeal of doing formal probabilistic inference
with Bayes’s theorem and frequentist error rate guarantees. What makes this
“folklore” is that it holds only in well-specified model cases. When the
model is misspecified, a Bernstein–von Mises theorem can still be estab-
lished, modulo regularity conditions, but it does not enjoy the same best-
of-both-worlds interpretation as in the well-specified case. Indeed, Kleijn
and van der Vaart (2012) show that, while the misspecified Bayes posterior
may still be asymptotically normal, misspecification bias creates a mismatch
between the limiting posterior covariance and that of the sampling distribu-
tion of the posterior mean. This covariance mismatch implies, for example,
that the frequentist coverage probability of the Bayesian posterior credible
regions can be arbitrarily far from the nominal level.
18
Handbook of Statistics

While the Bernstein–von Mises theorem in misspecified model cases does
not have the same strong implications as in well-specified cases, it is still an
interesting theoretical result. Moreover, when applied to a generalized/Gibbs
posterior, the result is practically relevant because it sheds light on the
learning rate’s role in the limiting posterior, which in turn can be informative
for the data-driven tuning discussed below.
For the situations involving iid data under consideration here, it suffices to
consider those cases where the Gibbs posterior concentrates at rate εn ¼ n1/2.
As explained above in Section 3.3, a root-n rate is common in fixed, finite-
dimensional problems where loss function has a certain degree of smoothness;
here we let q denote the finite dimension of θ so that Θ  q. This includes
our simple running example of inference on a quantile, with q ¼ 1. In order
to say more about the limiting shape of the Gibbs posterior, even more
smoothness of the loss is required. In the classical theory of well-specified
parametric Bayes models, for example, Theorem 4.2 of Ghosh et al. (2006),
the sufficient conditions for the Bernstein–von Mises theorem include twice
differentiability of the log-likelihood. More modern approaches based on local
asymptotic normality, for example, Le Cam and Yang (2000, Ch. 6) and van
der Vaart (1998, Ch. 7), provide some additional flexibility. Here we follow
this more modern approach and assume only that
l
R is twice differentiable at θ?, with _Rðθ?Þ ¼ 0 and Vθ? :¼ €Rðθ?Þ positive
definite;
l
the loss function θ 7! ‘θ(x) can be differentiated for P-almost all x.
Here, dot and double-dot correspond to first and second derivatives with
respect to θ, so _R and €R denote the gradient vector and the Hessian matrix,
respectively. Note that this approach avoids assuming the loss is twice differ-
entiable as would be required under the classical theory. In our case, the local
asymptotic normality condition takes the form
sup
h  K
Dnðh; θ?Þ  h>Vθ?Δn,θ?  1
2 h>Vθ?h

 ¼ oPð1Þ,
all compact K 
 q,
(16)
where Dn(h; θ?) ¼ n{Rn(θ? + hn1/2)  Rn(θ?)} is the scaled local empirical
risk difference at θ?, Vθ? is as defined above, and
Δn,θ? ¼ n1=2 X
n
i¼1
V1
θ? _‘θ?ðXiÞ:
The intuition behind (16) is that the empirical risk difference is approxi-
mately quadratic, locally near θ?, but in a sense that does not require twice
differentiabilty. And, as usual, a quadratic approximation appearing in the
exponent suggests a Gaussian approximation, where the matrix appearing
in the quadratic term determines the Gaussian’s variance.
Direct Gibbs posterior inference on risk minimizers Chapter
1
19

Theorem 3. Suppose the Gibbs posterior ΠðηÞ
n , for fixed η > 0, has concentra-
tion rate n1/2. In addition, if the loss function ‘θ is such that (16) holds, then the
sequence of appropriately centered and scaled Gibbs posteriors approaches a
sequence of q-variate normal distributions in total variation, as n ! ∞; that is,
sup
B
ΠðηÞ
n ðfθ : n1=2ðθ  θ?Þ  BgÞ  NqðB | ηΔn,θ?, ðηVθ?Þ1Þ

 ¼ oPð1Þ:
(17)
Proof. The proof is virtually identical to that of Theorem 2.1 in Kleijn and
van der Vaart (2012) so we will not reproduce the details here. The only
difference
is
that
their
likelihood
ratio
would
be
replaced
by
our
exp ½ηnfRnðθÞ  Rnðθ?Þg.
□
It would often be the case (e.g., van der Vaart, 1998, Theorem 5.7) that the
empirical risk minimizer, ^θn satisfies
n1=2ð^θn  θ?Þ ¼ Δn,θ? + oPð1Þ:
Then it follows from Theorem 3 above and the location shift invariance of the
total variation distance, that (17) can be re-expressed as
sup
B
ΠðηÞ
n ðBÞ  NqðB|η^θn + ð1  ηÞθ?, ðηnVθ?Þ1Þ

 ¼ oPð1Þ,
n ! ∞:
(18)
Compared to (17), the form in (18)a makes it easier to see the effect of the
learning rate η. Of course, when η ¼ 1, this looks exactly like the Gaussian
approximation presented Kleijn and van der Vaart (2012, p. 362). The effect
of a value η < 1 on the mean is negligible, since ^θn  θ? when n is large.
For the covariance, the effect of η can be more substantial, hopefully in a
good way. Toward this, recall (e.g., M€uller, 2013; van der Vaart, 1998, Theo-
rem 5.23) that the empirical risk minimizer, or M-estimator, has covariance
matrix n1Σθ?, where Σθ? is the sandwich covariance matrix
Σθ? ¼ V1
θ? Pð _‘θ? _‘
>
θ?Þ V1
θ? :
(19)
In general, Σθ? 6¼ V1
θ? , which is the aforementioned covariance mismatch.
However, there are cases when a so-called generalized information equality
(Chernozhukov and Hong, 2003), which states that Σθ? ¼ γV1
θ? , for some sca-
lar γ > 0. In such cases, there exists η such that the covariance matrix of the
Gibbs posterior mean, η2Σθ?, matches the Gibbs posterior covariance matrix,
ðηVθ?Þ1, that is,
aIn Bhattacharya and Martin (2022), the effect of η on the asymptotic Gibbs posterior mean was
overlooked—they stated the posterior mean was ^θn instead of η^θn + ð1  ηÞθ? as in (18). This
small effect went unnoticed because the learning rate suggested in the former case ends up being
larger than in the latter, hence more conservative Gibbs posterior credible regions.
20
Handbook of Statistics

η2Σθ? ¼ η1V1
θ? () η ¼ γ1=3:
So there is a learning rate value that corrects the covariance mismatch and
leads to valid Gibbs posterior inference, at least asymptotically. More gener-
ally, we can expect that there is some value of η for which the above relation-
ship holds at least approximately. This begs the question: how might that
learning rate value be found?
4
Learning rate selection
In Sections 1 and 2, we emphasized the importance of the learning rate, but
then the learning rate was mostly irrelevant in the theoretical results presented
in Section 3. The reason for this apparent discrepancy is that the theoretical
results are all “first-order” in the sense that they only describe features of
the Gibbs posterior relevant to estimation; no “higher-order” claims about
accuracy of inference based on the Gibbs posterior have been made. Even
the Bernstein–von Mises result, despite being distributional in nature, pro-
vides no inference guarantees in our under- or misspecified model setting
the way the analogous result does in a well-specified model setting. So, what
we said in Sections 1 and 2 remains true: the learning rate needs to be chosen
carefully in practical applications to ensure that inferences drawn based on the
Gibbs posterior are reliable.
As mentioned in Section 1, data-driven learning rate selection methods has
been an active area of research in recent years. A number of novel ideas have
been put forth, from different perspectives and with distinct objectives. This
includes the methods by Gr€unwald (2012), Gr€unwald and van Ommen
(2017), Holmes and Walker (2017), Lyddon et al. (2019), and Syring and
Martin (2019). We will focus the presentation here on our preferred method,
the general posterior calibration or GPC algorithm. This is our preferred
method not just because we developed it, but also because it has the best empir-
ical performance—in terms of coverage probability of credible sets—across
various settings and sample sizes compared to the other methods; see Wu and
Martin (2022). In what follows, we explain what the GPC algorithm aims to
do, give some heuristics for why it works, and then describe the algorithm
and its implementation details.
First, we need to justify an important but basic claim, namely, that the pri-
mary role played by the learning rate, η, in the Gibbs posterior, ΠðηÞ
n , defined
in (4), is to control the spread. As we explained in Section 3.1, the prior’s
influence will be rather limited, at least when n is large, so the Gibbs posterior
density is, as in (8), effectively proportional to exp fηnRnðθÞg, for θ  Θ.
Since Rn is minimized at ^θn, independent of η, we see that η can only be
affecting the spread of the Gibbs posterior, with small η making the posterior
wider, more diffuse, and large η making the posterior narrower, more concen-
trated at ^θn. The posterior consistency result in Theorem 1 formalizes this.
Direct Gibbs posterior inference on risk minimizers Chapter
1
21

So if the learning rate controls the spread of the posterior, it also must
influence the coverage probability of Gibbs posterior credible regions. That is,
let CðηÞ
α ðTnÞdenote a 100(1  α)% Gibbs posterior credible region, based on data
Tn; for example, it could be a highest posterior density (HPD) region give by
CðηÞ
α ðTnÞ ¼ fθ : πðηÞ
n ðθÞ > kðα; ηÞg,
where πðηÞ
n
is the Gibbs posterior density in (8) and k(α;η) is the cutoff chosen
to ensure that the region has ΠðηÞ
n -probability 1  α. However, it is worth
pointing out that it is not necessary that the credible region be for the entire
unknown θ, it could be just for some relevant feature ψ ¼ ψ(θ). Now define
the (frequentist) coverage probability function
cαðηÞ ¼ cαðη; PÞ ¼ PfCðηÞ
α ðTnÞ 3 θðPÞg,
(20)
where, here, we have made the notation explicitly reflect the dependence of
the inferential target “θ?” on the underlying probability P. Since η controls
the spread of the Gibbs posterior, it likewise controls the size of the credible
regions. Therefore, η 7! cα(η) is decreasing, so we ought to be able to find a
learning rate value that gets the coverage probability close the advertised/
nominal frequentist level. That is, we seek
η? ¼ η?ðα; PÞ ¼ supfη > 0 : cαðηÞ 	 1  αg:
(21)
Such a learning rate η? would calibrate the Gibbs posterior in the sense that its
100(1  α)% credible region would have (frequentist) coverage probability at
least 1  α, that is, the uncertainty quantification would be valid or honest.
Before moving on to describe what the oracle learning rate η? looks like
and how to approximate it in a data-driven way, it is important that we dispel
with some of the optimism that stems from our many experiences focusing on
well-specified models. That is, at least in general, we cannot expect to find a
single η? to achieve the desired calibration for every α, n, P, relevant feature
ψ ¼ ψ(θ), etc. When dealing with an under- or misspecified model, we have
to choose which battles we want to win and, in our present case, we have cho-
sen to focus on the particular 100(1  α)% credible region and choosing the
learning rate η? that ensures its coverage attains the nominal frequentist level.
For other objectives, a different η? would be required.
For those special cases described in Section 3.4, where the Gibbs posterior
is asymptotically normal, we can shed some light on what the oracle η? looks
like. From Theorem 3 and, in particular, the version of the conclusion in (18),
we have that the 100(1  α)% credible region associated with the Gibbs
posterior distribution ΠðηÞ
n
has the form
ϑ : n ϑ  ^θ
ðηÞ
n

>
ðηVθ?Þ ϑ  ^θ
ðηÞ
n


 kα


,
22
Handbook of Statistics

where kα is the upper-α quantile the chi-square distribution with q degrees of
freedom, and ^θ
ðηÞ
n
¼ η^θn + ð1  ηÞθ? . Then the coverage probability of the
credible region is
cαðηÞ ¼ P nð^θn  θ?Þ
>ðη3Vθ?Þð^θn  θ?Þ  kα
n
o
,
where the probability is with respect to the sampling distribution of ^θn
under P. Under the assumptions of Theorem 3, we also have that
^θn  Nqðθ?, n1Σθ?Þ, for large n, with the latter covariance matrix defined
in Section 3.4. So, if it happened that η3Vθ? ¼ Σ1
θ? , then the coverage prob-
ability would be approximately equal to 1  α. More generally, the qua-
dratic form in the above display is no smaller than
nð^θn  θ?Þ
>Σ1
θ? ð^θn  θ?Þ  η3 λmin Σ1=2
θ? Vθ?Σ1=2
θ?


,
where Σ1=2
θ?
is a suitable square root matrix of Σθ? and λmin is the minimum
eigenvalue operator. Therefore, for large n, the coverage probability satisfies
cαðηÞ 	 P
ChiSqðqÞ 
kα
η3 λmin Σ1=2
θ? Vθ?Σ1=2
θ?


8
<
:
9
=
;:
So it is clear that the set over which the supremum in (21) is taken is non-
empty and, moreover, the oracle η? would be roughly
η? 
λmin Σ1=2
θ? Vθ?Σ1=2
θ?


n
o1=3
:
The obvious question is how can we find the η? in (21)? Since we do not
know P, we clearly cannot evaluate the coverage probability function cα(η)
and, therefore, we cannot solve the equation cα(η) ¼ 1  α. We can, however,
obtain roughly unbiased estimates of the coverage probability function at any
fixed η via the bootstrap (Davison and Hinkley, 1997; Efron, 1979; Efron and
Tibshirani, 1993). That is, if we let ~T
n
b denote an iid sample of size n from the
empirical distribution, n, of the observed data Tn, for b ¼ 1, …, B, then a
bootstrap approximation of the coverage probability function is
^cboot
α
ðηÞ ¼ 1
B
X
B
b¼1
1 CðηÞ
α ð~T
n
bÞ 3 ^θn
n
o
:
Alternatively, this can be viewed as a Monte Carlo approximation of the
plug-in estimate
^cαðηÞ ¼ cαðη; nÞ ¼ n CðηÞ
α ð~T
nÞ 3 θðnÞ
n
o
:
Direct Gibbs posterior inference on risk minimizers Chapter
1
23

Since these coverage probability functions are only estimates/approximations,
we need to acknowledge the variability in how we solve the equation
“cα(η) ¼ 1  α.” For this, we apply the stochastic approximation procedure
of Robbins and Monro (1951), which chooses an initial guess η0 and defines
a sequence of candidate solutions
ηs ¼ ηs1 + κs ^cboot
α
ðηs1Þ  ð1  αÞ


,
s 	 1,
(22)
where ðκsÞ 
 ð0, 1Þ is a deterministic sequence of step-sizes, for example,
κs ∝(1+s)γ for γ  (0.5, 1]. The stochastic approximation updates terminate
when convergence is reached, for example, if jηs  ηs1j is smaller than some
specified tolerance. The output is a learning rate value, ^ηn, depending on Tn,
α, and other inputs.
The reader is sure to notice that the GPC algorithm is potentially compu-
tationally intensive. In particular, it requires posterior computations on B
many bootstrap data sets of size n at each iteration of the stochastic approxi-
mation update (22). In a misspecified Bayes setting, it is not out of the ques-
tion that the model is sufficiently simple (e.g., conjugate priors) that posterior
computations can be done in more-or-less closed form, in which case the GPC
algorithm is relatively inexpensive. In a Gibbs setting where the posterior is
based on a loss function, it is unlikely that the posterior will be available in
closed-form. Regardless, if Markov chain Monte Carlo methods are required
to compute the posterior, then the GPC algorithm is more expensive. In our
experience, however, it is not prohibitively expensive. For one thing, there
does not seem to be a benefit to having large B, so we have taken B to be
200–300 in our applications. Also, the posterior computations for the B boot-
strap data sets at a given η can be done in parallel. Finally, there does not
seem to be any practical benefit to having a strict convergence criterion for
stopping the updates in (22), so capping the number of iterations to, say,
10–20 works fine in practice. Apparently, just having η in a neighborhood
of the solution to “^cboot
α
ðηÞ ¼ 1  α” is enough to achieve practical calibration.
5
Numerical examples
5.1
Quantile regression
Suppose we observe data pairs (X, Y )  P, where Y is the scalar response
variable of primary interest and X is a vector covariate. Quantile regression
(e.g., Koenker, 2005) models the τth conditional quantile of Y, given X ¼ x,
as a linear combination θ>f(x), for a known dictionary of functions
fðxÞ ¼ ð f1ðxÞ, …, fJðxÞÞ, with θ  J
an unknown vector of coefficients.
The quantity of interest is defined as the value θ? that minimizes the risk
R(θ) ¼ P‘θ, where the loss is the so-called check loss function
‘θðx, yÞ ¼ y  θ>f ðxÞ
	

τ  1fy < θ>fðxÞg
	

:
24
Handbook of Statistics

Given an iid sample (Xi, Yi) from P, with i ¼ 1, …, n, and a prior distribution
Π for θ, the Gibbs posterior for inference can be readily constructed via the
formula (4).
As far as the Gibbs posterior’s properties are concerned, the check loss
function admits a θ-derivative almost everywhere, given by
_‘θðx, yÞ ¼
ð1  τÞfðxÞ,
if y < θ>fðxÞ
τfðxÞ,
if y > θ>fðxÞ
;
(
and twice-differentiable risk function with second derivative
€RðθÞ ¼
Z
fðxÞ fðxÞ>pxðθ>fðxÞÞPðdxÞ
where px(y) denotes the conditional density of Y, given X ¼ x. Therefore,
according to the theory presented in Section 3.4, this implies the Gibbs poste-
rior distribution has a root-n concentration rate at θ? and is asymptotically
normal.
For a concrete example, let Yi given Xi ¼ xi be normally distributed with
standard deviation 2 and with median (τ ¼ 0.5 quantile) equal to θ0 + θ1xi
and where Xi + 2  ChiSq(4), independent, for i ¼ 1, …, n, with θ? ¼ (2, 1).
For this illustration, we take a flat prior for θ. In this case, Σθ?  1:25V1
θ? ,
which implies the Gibbs posterior can be asymptotically calibrated by taking
η? ¼ 1.251/3  0.93. The goal here is to investigate the performance of the
GPC algorithm, to see if it can effectively calibrate the Gibbs posterior. To
check this, we simulated 400 data sets of size n ¼ 50 from the aforementioned
joint distribution and, for each data set, ran the GPC algorithm and then
extracted the corresponding 95% Gibbs posterior credible region for θ. In these
simulations, the marginal coverage for θ0 and θ1 was 92.5% and 94.5%, respec-
tively, both within a tolerable range of the target 95%. Here, the average
learning rate selected by the GPC algorithm was 0.99, with standard deviation
0.19, which is in a neighborhood the asymptotic oracle value η?  0.93. Similar
results were obtained with n ¼ 400.
If the GPC-calibrated Gibbs posterior produces reliable inferences for θ
then one would expect it behaves similarly to other reasonable methods, for
example, the bootstrap. For instance, 95% Gibbs posterior joint credible sets
for θ should be close to 95% confidence sets based on the bootstrapped
M-estimator. For a specific comparison, consider the Gibbs posterior distri-
bution of the elliptical transform gðθÞ ¼ ðθ  θÞ
>Ψ1ðθ  θÞ, where θ and
Ψ are the mean and covariance matrix obtained from the Gibbs posterior sam-
ple, respectively. The set fϑ : gðϑÞ  zE
0:95g, where zE
0:95 is the 0.95 quantile of
the marginal Gibbs posterior distribution of g(θ), defines a 95% elliptical
credible region for θ. This is the same shape as the asymptotic credible region,
but its justification does not depend on any asymptotic result. We also con-
sider the highest posterior density credible region, defined by the set of
Direct Gibbs posterior inference on risk minimizers Chapter
1
25

posterior samples fϑ : eωRnðϑÞ  zH
0:95g, where zH
0:95 is the 0.95 quantile of the
marginal Gibbs posterior distribution of eωRnðθÞ . This region need not be
elliptical, but we would expect it to be roughly elliptically shaped, at least
for large n. Fig. 3 compares these two Gibbs posterior credible regions to
elliptical 95% confidence regions based on the bootstrapped M-estimator for
sample sizes of n  {50, 400}. All three regions are similar in shape and ori-
entation, but the bootstrap-based region is slightly larger in both instances,
and the highest posterior density region indeed looks more elliptical at the
larger sample size.
5.2
Classification
Consider response-predictor data pair (X, Y )  P, where X  r is a continu-
ous predictor and Y  {1, +1} is a binary response—a class or label. The
classification boils down to learning the relationship between X and Y, that
is, what values of X tend to correspond to Y ¼ +1 and vice versa. This is typi-
cally carried out through specification of a classifier, a function that maps the
X-space to {1, +1}, often depending on a linear combination θ>f(x), where θ
and f(x) are as in Section 5.1. The unknown θ is linked to the data (X, Y)
through a choice of loss function. A common choice is the 0–1 loss,
‘θðx, yÞ ¼ 1  y signfθ>fðxÞg:
An advantage to this is interpretation: the expected loss is
RðθÞ ¼ P½Y 6¼ signfθ>fðXÞg,
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
2.2
2.4
2.6
0.80
0.85
0.90
0.95
1.00
1.05
1.10
1.15
θ0
θ0
θ1
θ1
B
A
FIG. 3
95% highest posterior density level set of the Gibbs posterior calibrated by the GPC
algorithm (solid), 95% elliptical credible region (dotted), and 95% elliptical confidence region
based on the bootstrapped M-estimator (dashed) for the quantile regression example in
Section 5.1. (A) n ¼ 50, ^η  0:96; (B) n ¼ 400, ^η  0:90.
26
Handbook of Statistics

so the risk minimizer, θ?, determines the classifier with smallest missclassifi-
cation probability. A disadvantage is that the discontinuity makes optimiza-
tion of the empirical risk a challenging computational problem. To remedy
this, smooth versions of the 0–1 loss can be considered. For example, the
so-called hinge loss is given
‘θðx, yÞ ¼ max f0, 1  y  θ>fðxÞg:
This loss function is continuous and leads to a more manageable computa-
tional problem, which at least partially explains the popularity of support vec-
tor machines and maximum-margin classifiers. In any case, once we have iid
data (Xi, Yi) from P, have chosen a loss function, and specified a prior, the
Gibbs posterior distribution for θ obtains as in (4). In what follows, we will
focus on the hinge loss.
As in Section 5.1, we can ask what properties the Gibbs posterior affords.
The hinge loss is continuous and almost everywhere θ-differentiable, with
derivative
_‘θðx, yÞ ¼
yfðxÞ
if 1  y  θ>fðxÞ > 0
0
otherwise:
(
Moreover, the risk function is given by
RðθÞ ¼
Z
fx:1θ>fðxÞ>0g
f1  θ>fðxÞg mðxÞ PðdxÞ
+
Z
fx:1+θ>fðxÞ>0g
f1 + θ>fðxÞg f1  mðxÞg PðdxÞ,
where m(x) ¼ P(Y ¼ +1|X ¼ x) is the conditional probability function deter-
mined by P. R(θ) admits two θ derivatives, which may be computed by the
Leibniz integral rule, but we omit the (complicated) form of this function. The
point is that the existence of these derivatives implies both the M-estimator
and Gibbs posterior for θ corresponding to the hinge loss are asymptotically
normally distributed.
For a concrete example of a Gibbs posterior for classification with the
hinge loss let m(x) ¼ F(θ?>x), where F denotes the distribution function of
a Student’s t random variable with 3 degrees of freedom, θ? ¼ (1, 1)>,
and X  N(1, 1). We investigate the behavior of the Gibbs posterior calibrated
by the GPC algorithm targeting 95% credible intervals for θ?
1 in a short simu-
lation of 400 replications for sample sizes n ¼ 400 and 1000. At sample size
400 the average learning rate selected by GPC was about 0.77 with standard
deviation 0.05. For n ¼ 1000 the learning rate selected by GPC was a bit
smaller, averaging 0.65. For both simulations the GPC-calibrated 95%
Gibbs posterior credible intervals were conservative, with coverage of about
99% for θ1.
Direct Gibbs posterior inference on risk minimizers Chapter
1
27

Fig. 4 displays the same three types of joint credible/confidence regions
for θ?, the hinge risk minimizer, as in Fig. 3. For both moderate and large
sample sizes the Gibbs highest posterior density credible region is very simi-
lar to the elliptical region, and, hence, similar to the shape of the asymptotic
credible region. Both Gibbs posterior credible regions contain the bootstrap-
based confidence region, which reflects the over-coverage observed in the
simulation experiment.
5.3
Nonlinear regression
So far, all our examples have considered finite-dimensional inference pro-
blems, but Gibbs posteriors can be used for inference on high- or even
infinite-dimensional parameters as well. Mean regression is a common appli-
cation of high-dimensional inference and one setting in which Gibbs poster-
iors have already been studied; see, for example, Syring and Martin (2022).
Let (X, Y )  P and consider the loss function ‘θ(x, y) ¼ {yθ(x)}2, for θ a
generic smooth function. If P admits a finite second moment, then it is easy
to show that the risk minimizer, θ?, exists; if the function class is sufficiently
broad, then θ?(x) equals the conditional expectation of Y, given X ¼ x, under
P. In any case, with iid data (Xi, Yi) and a suitable prior Π on θ, we can
construct a Gibbs posterior distribution for inference on the risk minimizer.
For smooth functions, the so-called random series priors (Shen and Ghosal,
2015) are quite convenient. Parametrize θ by a linear combination of a chosen
set of basis functions θ(x) ¼ ϕ>f(x) where ϕ ¼ ðϕ1,ϕ2,…, ϕJÞ J, and
0.8
1.0
1.2
1.4
1.6
−1.8
−1.6
−1.4
−1.2
−1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
−1.7
−1.6
−1.5
−1.4
−1.3
−1.2
−1.1
θ0
θ0
θ1
θ1
A
B
FIG. 4
95% highest posterior density level set of the Gibbs posterior calibrated by the GPC
algorithm (solid), 95% elliptical credible region (dotted), and 95% elliptical confidence region
based on the bootstrapped M-estimator (dashed) for the classification regression example in
Section 5.2. (A) n ¼ 400, ^η  0:70; (B) n ¼ 1000, ^η  0:58.
28
Handbook of Statistics

fðxÞ ¼ ðf1ðxÞ, f2ðxÞ,…, fJðxÞÞ> denotes the first J basis functions from, say, a
Fourier, spline, or polynomial basis. Then, a prior on θ is induced by a hierar-
chical prior on J and on ϕ, given J ¼ j. Common choices include a Poisson prior
on J and normal conditional priors on the coefficients in ϕ. Having a prior dis-
tribution on the number of basis functions makes the model flexible and adap-
tive to functions θ of different levels of smoothness. In practice, the posterior
may perform well for a fixed J and a prior on the coefficient vector only.
A useful feature of a Gibbs posterior for θ is a 100(1  α)% uniform cred-
ible band, a sup-norm ball of functions θ having 1  α Gibbs posterior prob-
ability. This can be used as a confidence band, that is, as a set of functions
with coverage probability 1  α, but this calibration would not be automatic.
Fortunately, the GPC algorithm can used to calibrate these posterior credible
regions, even in this infinite-dimensional setting.
For an example of applying the Gibbs posterior along with GPC to non-
linear regression, consider X  Unif(0, 1) and (Y |X ¼ x)  N(θ?(x), 0.22)
where θ?(x) ¼ 20x3  34x2 + 15.2x  1.2. We used independent, diffuse nor-
mal priors for the coefficients of a cubic polynomial basis, so the number of
basis functions is fixed at J ¼ 4. Fig. 5 displays 95% Gibbs posterior uniform
credible bands for θ, based on n ¼ 100, and using the GPC-selected learning
rate. In this case, the credible band contains θ?.
0.0
0.2
0.4
0.6
0.8
1.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
x
θ(x)
FIG. 5
95% uniform Gibbs posterior credible bands with learning rate selected by GPC for the
nonlinear regression example in Section 5.3.
Direct Gibbs posterior inference on risk minimizers Chapter
1
29

6
Further details
6.1
Things we did not discuss
Our coverage of the relevant results in the Gibbs or generalized Bayes poste-
rior inference was necessarily limited. So there are lots of interesting and
important ideas and results that we did not discuss. For the sake of being
semicomplete in our survey of the relevant literature, here we briefly mention
a few of these other problems and directions.
l
Gibbs is not the only alternative to Bayes. Recall that Gibbs is closely
related to M-estimation and empirical risk minimization, where the param-
eter of interest may be defined as the minimizer of an expectation/risk func-
tion and point estimates are derived by minimizing the empirical analog of
the risk. A closely related technique is Z-estimation defined by moment
conditions in which one or more moments/expectations are exactly zero
as functions of θ at θ ¼ θ?. Chernozhukov and Hong (2003) develop a
quasi-posterior distribution for models based on moment conditions taking
similar form to a Gibbs posterior with an empirical risk function that is
defined as a quadratic form based on the moment conditions. An alternative
approach for handling models defined by moment conditions is the expo-
nentially tilted empirical likelihood (ETEL) posterior; see, for example,
Chib et al. (2018). That approach utilizes an empirical likelihood in place
of the usual parametric likelihood, restricted to distributions satisfying the
moment conditions, and combined with a nonparametric prior favoring
the empirical likelihood. A variation of this approach is the penalized ETEL
appearing in, for example, Tang and Yang (2021), in the context of quantile
regression. The PETEL posterior combines the ETEL approach with a pen-
alty term taking the form of the relevant empirical risk function that forms
the basis of the Gibbs posterior.
l
An advantage of the Bayesian framework is that once the posterior is in
hand, answers to any relevant question can be derived from it. One of
these relevant questions concerns prediction of future observations. When
the model is incorrectly- or under-specified, this advantage disappears. In
particular, the standard/naive construction of a predictive distribution need
not have good properties even if the posterior does. Wu and Martin (2021)
considered the prediction problem, proposed a generalized predictive dis-
tribution, and developed a variation on the GPC algorithm described above
that would ensure the prediction intervals derived from it would be
calibrated in a frequentist sense.
l
When the loss function is rough, the empirical risk function Rn tends to be
rough too. From the empirical risk minimization point of view, this rough-
ness can make optimization a challenge. One option is to suitably smooth
the rough objective function so that the optimization problem remains
(largely) unchanged. While smoothing may not significantly affect the
30
Handbook of Statistics

estimation problem, it can create challenges with inference. From a Gibbsian
perspective, the rough empirical risk can create problems for designing an
efficient posterior sampling algorithm, so here too some smoothing might
be desirable. However, the type of smoothing that leads to simpler optimiza-
tion may not lead to efficient posterior sampling, so perhaps some different
considerations are required. Alternatively, one might consider a varia-
tional approximation to the Gibbs posterior based on the rough empirical
risk function; see, for example, Alquier et al. (2016) and Alquier and
Ridgway (2020). This would have simple posterior computations by con-
struction, but might not be as accurate of an approximation of the original
Gibbs posterior compared to one that directly and appropriately smooths
the empirical risk.
6.2
Open problems
Generalized Bayesian inference has been an active area of research in recent
years. While lots of exciting new developments have been made, there are
still a number of interesting and important questions that remain unanswered.
We take the opportunity here to list just a few open problems. This is just a
biased selection, far from an exhaustive list.
l
The GPC algorithm described in Section 4 has been shown to have strong
empirical performance in a fairly wide range of applications. However,
there is still no rigorous theoretical support to back this up. The challenge
is that there a lot of moving parts: posterior computations via Monte Carlo,
bootstrap, and stochastic approximation. All three of these methods indi-
vidually are theoretically sound, but GPC applies them simultaneously,
which markedly complicates the analysis.
l
Here the learning rate appeared as a power in the pseudo-likelihood
expression. However, there may be other ways in which a “learning rate”
parameter might appear in a generalized posterior construction. For
example, composite likelihoods and the corresponding posterior distribu-
tions (Pauli et al., 2011) often involve at least one adjustment factor that
could be tuned via GPC. Similarly, variational approximations are
known to under-estimate the posterior spread (Blei et al., 2017), so one
could introduce an additional adjustment factor that, again, can be tuned
using GPC. So we see the idea behind the GPC algorithm as a general
strategy that can be applied beyond the Gibbs posterior applications
discussed here.
l
To our knowledge, learning rate adjustment via the GPC algorithm has
only been investigated in relatively low-dimensional problems. Our expec-
tation is that GPC’s performance will deteriorate as the dimension of θ
increases, but it is currently unknown how quickly this deterioration would
happen. Can the algorithm be modified to scale more efficiently with
dimension, or is an entirely new algorithm needed?
Direct Gibbs posterior inference on risk minimizers Chapter
1
31

l
As we argued above, the introduction of the scalar learning rate was a sim-
ple consequence of the loss function’s scale relative to the prior being
ambiguous. But having only a scalar learning rate to tune limits our ability
to adjust the shape of the Gibbs posterior. Other kinds of adjustments are
possible, for example, the sandwich likelihood in Sriram (2015). What
about other more general “learning rate structures” that would allow for
simultaneous adjustment of both the shape and spread of the Gibbs poste-
rior contours?
l
The theoretical results presented here, as well as those in Syring and
Martin (2022), Gr€unwald and Mehta (2020), and elsewhere, focus exclu-
sively on estimation-related question such as consistency and concentra-
tion rates. When θ is high-dimensional, it is common for there to be an
underlying low-dimensional structure that is of practical relevance, so
there is a question of whether the Gibbs posterior would be able to learn
that structure. For example, consider a classification problem that involves
a high-dimensional feature x. In such cases, one might be willing to
believe that only a relatively small fraction of all the features should affect
the classifier, so a sparsity-encouraging prior might be used. But which
features are important or active in the optimal classifier is unknown, and
a natural question is if the Gibbs posterior can identify these. This specific
question was addressed in Jiang and Tanner (2008), and another similar
result in a different context we presented recently in Wang and Martin
(2021). To our knowledge, however, there have been no general investiga-
tions into Gibbs posterior structure learning.
l
We have focused exclusively here on cases where the data Tn ¼
ðT1, …, TnÞ are iid from a common distribution P. From here, an extension
to a case where data Ti are independent but not iid, having distinct distri-
butions Pi for i ¼ 1, …, n, would not be out of reach. The case, however,
where the Ti’s are dependent has, to our knowledge, not been given much
attention in the literature. That the exponent in the definition of πðηÞ
n
in,
say, (8) involves a sum of individual negative loss terms seems uniquely
suited for independent data, so all of what has been presented here would
need to be reworked.
7
Conclusion
This paper considered the problem in which the quantity of interest is defined,
not as a parameter in a statistical model for the data-generating process, but
as something that is, or at least can be, expressed as a minimizer of a
suitable expected loss function. It is often the case that quantities having
a real-world interpretation, beyond a statistical model, can be expressed in
this way, for example, quantiles and moments. More generally, the quantities
of interest in machine learning applications can often be expressed as risk
32
Handbook of Statistics

minimizers. Of course, regardless of how the quantity of interest is most nat-
urally defined, it would be possible to formulate a statistical model, recast the
quantity of interest in terms of the model parameters, and proceed with Bayes-
ian inference as usual. Here we argued, first, that there are good reasons for
not going this indirect route through a statistical model:
l
no risk of model misspecification bias;
l
no need to over-complicate matters by introducing nonparametric models;
l
no need to deal with prior specification and posterior computations
pertaining to nuisance parameters and the associated challenges with mar-
ginalization (Fraser, 2011; Martin, 2019).
Second, we argued that a direct approach can be carried out using a Gibbs poste-
rior. Interpretation of the Gibbs posterior is different from that of a Bayesian pos-
terior, but it can still be used for making inference, and it shares many of the
familiar asymptotic convergence properties of the Bayes posterior. A key point
is that the Gibbs posterior is not automatically calibrated in a frequentist sense.
Calibration only holds for Bayesian posteriors in correctly specified models,
but since “All models are wrong…” this Bayesian result does not provide much
comfort. We argued that calibration can be achieved, just not automatically—we
need to carefully tune the learning rate parameter. With the Gibbs posterior’s
desirable convergence properties, together with a suitable, data-driven learning
rate selection procedure, this appears to us to be a powerful framework, funda-
mental to what could be described as Bayesian statistical learning.
This, of course, was a biased survey of recent developments falling under
the umbrella of generalized Bayesian inference. We briefly mentioned a few
other ideas and approaches in Section 6.1 but that definitely does not do these
developments justice. While there are some technical differences between the
approach advocated for here and those advocated by others, we want to end
this discussion by highlight what they all have in common. Wasserman’s
quote from Section 1 is right: Bayesian inference is too rigid in its insistence
on being able to answer all relevant questions about the data-generating pro-
cess in one stroke. But let’s not throw out the baby with the bath water. That
is, there is no need to abandon hope of principled, probabilistic inference on
interest parameters to get the flexibility Wasserman is looking for. We just
need to be more strategic/direct with our posterior construction. The develop-
ments here and elsewhere in the generalized Bayes literature are in this vein,
and we are excited to see where this goes.
Acknowledgments
This work is partially supported by the U. S. National Science Foundation, grant number
SES–2051225. The authors also thank the editors of this Handbook, Alastair Young in par-
ticular, for the invitation to make a contribution.
Direct Gibbs posterior inference on risk minimizers Chapter
1
33

Appendix
A.1
Proofs
A.1.1
Proof of Theorem 1
The proofs of both Theorems 1 and 2 share a similar strategy. Start by rede-
fining the Gibbs posterior distribution as the ratio
ΠðηÞ
n ðAÞ ¼ NðηÞ
n ðAÞ
DðηÞ
n
,
A  Θ,
where the numerator and denominator, respectively, are
NðηÞ
n ðAÞ ¼
Z
A
eηfRnðθÞRnðθ?Þg ΠðdθÞ
DðηÞ
n
¼
Z
Θ
eηfRnðθÞRnðθ?Þg ΠðdθÞ:
Gibbs posterior consistency requires ΠðηÞ
n ðAεÞ ! 0, for Aε ¼ {θ : d(θ, θ?) > ε},
for any ε > 0. We proceed by showing that (a) the numerator is vanishing and
(b) the denominator is not any smaller than the bound on the numerator, both as
n ! ∞. We start with Lemma 1 that bounds the denominator.
Lemma 1. If (12) holds, then PfDðηÞ
n
> enηδg ! 1 as n ! ∞for any δ > 0.
Proof. Begin by lower-bounding enηδDðηÞ
n
by restricting the domain of integra-
tion in the definition of DðηÞ
n :
enηδDðηÞ
n
¼
Z
eηnfRnðθÞRnðθ?Þδg ΠðdθÞ
	
Z
fθ:RðθÞRðθ?Þδ=2g
eηnfδRnðθÞ+Rnðθ?Þg ΠðdθÞ:
The law of large numbers implies that Rn(θ)  Rn(θ?) converges P-almost
surely to R(θ)  R(θ?), pointwise in θ. So, for θ in the above range of integra-
tion, we get
lim sup
n!∞fRnðθÞ  Rnðθ?Þg  δ
2 ,
Palmost surely:
Then the integrand in the lower bound for enδDðηÞ
n
is converging to ∞point-
wise in θ, P-almost surely. Then Fatou’s lemma and the condition (12) on
the prior mass assigned to risk neighborhoods of θ? together imply that
lim inf
n!∞enδDðηÞ
n
¼ ∞,
Palmost surely,
and, from this, (an even stronger version of ) the claim follows.
□
34
Handbook of Statistics

In contrast to the denominator, which is controlled by properties of the
prior, the behavior of the numerator is largely determined by properties of
the loss function.
Lemma 2. If (11) holds, then PfNðηÞ
n ðAεÞ  eηncg ! 1 as n ! ∞, for a con-
stant c > 0 depending only on ε.
Proof. The risk difference can clearly be rewritten as
RnðθÞRnðθ?Þ ¼ fRnðθÞRðθÞg + fRðθÞRðθ?Þg + fRðθ?ÞRnðθ?Þg:
By (11a), the first term is oP(1) uniformly in θ, by (11b) the second term is
lower-bounded, uniformly on Aε, by a constant ξ ¼ ξ(ε) > 0, and by the
law of large numbers the third term is oP(1) and does not depend on θ.
Therefore,
NðηÞ
n ðAεÞ ¼
Z
Aε
eηnfRnðθÞRnðθ?Þg ΠðdθÞ

Z
Aε
eηnfoPð1Þ+ξg ΠðdθÞ:
Since the oP(1) term vanishes uniformly in θ  Aε, the bracketed term in the
exponent will eventually be bigger than, say, ξ/2 > 0. Therefore, with
P-probability converging to 1, we have that NðηÞ
n ðAεÞ  eηnc for some constant
c > 0, as claimed.
□
Lemmas 1 and 2 together imply Gibbs posterior consistency. Indeed, on a
set with P-probability converging to 1, we have that NðηÞ
n ðAεÞ is exponentially
small and DðηÞ
n
is not exponentially small. Putting these two results together
gives
ΠðηÞ
n ðAεÞ ¼ NðηÞ
n ðAεÞ
DðηÞ
n
 eηnðcδÞ:
The constant c > 0 is fixed, depends on ε, but δ > 0 is arbitrary. So if we take
δ < c, then we can conclude that, on a set with P-probability converging to 1,
ΠðηÞ
n ðAεÞ ! 0, which proves consistency.
A.1.2
Proof of Theorem 2
Our strategy for proving concentration we present in this section mirrors our
proof of consistency above. First, we express the Gibbs posterior probability
of the complement of a shrinking neighborhood An ¼ {θ : d(θ, θ?) > Mnεn}
as the ratio NðηÞðAnÞ=DðηÞ
n . Then we show that conditions (14) and (15) imply
that the numerator is small and the denominator is not too small such that the
ratio is vanishing. These bounds on the denominator and numerator are estab-
lished in the two lemmas presented next.
Direct Gibbs posterior inference on risk minimizers Chapter
1
35

Lemma 3. Define the mean and variance functions of the excess loss:
mðθ, θ?Þ ¼ RðθÞ  Rðθ?Þ, and
vðθ, θ?Þ ¼ Pð‘θ  ‘θ?Þ2  mðθ, θ?Þ2:
And, define the subset of the parameter space Θn :¼ fθ : mðθ, θ?Þ _
vðθ, θ?Þ  Cεα
ng for some C > 0 and where εn and (α, β) are defined in
Theorem 2 and (14). Then,
DðηÞ
n
≳ΠðΘnÞe2bnηnεα
n,
with Pn-probability ! 1,
for any positive sequence bn ! ∞.
Proof. Define a standardized version of the empirical risk difference, that is,
ZnðθÞ ¼ fnRnðθÞ  nRnðθ?Þg  nmðθ, θ?Þ
fnvðθ, θ?Þg1=2
:
Of course, Zn(θ) depends (implicitly) on the data Un. Let
Z n ¼ fðθ, UnÞ : jZnðθÞj 	 ðbnnεα
nÞ1=2g:
Next, define the cross-sections
Z nðθÞ ¼ fUn : ðθ, UnÞ  Z ng
and
Z nðUnÞ ¼ fθ : ðθ, UnÞ  Z ng:
For Θn as above, since
nRnðθÞ  nRnðθ?Þ ¼ nmðθ, θ?Þ + fnvðθ, θ?Þg1=2ZnðθÞ,
and m, v, and Zn are suitably bounded on Θn \ Z nðUnÞc, we immediately get
DðηÞ
n
	
Z
Θn\Z nðUnÞceηnmðθ,θ?Þηfnvðθ,θ?Þg1=2ZnðθÞ ΠðdθÞ 	 e2bnηnεα
nΠfΘn \ Z nðUnÞcg:
From this lower bound, we get
Pn DðηÞ
n
 1
2 ΠðΘnÞe2bnηnεα
n
n
o
 Pn e2bnηnεα
nΠfΘn \ Z nðUnÞcg  1
2 ΠðΘnÞe2bnηnεα
n
h
i
¼ Pn ΠfΘn \ Z nðUnÞg 	 1
2 ΠðΘnÞ
h
i
 2PnΠfΘn \ Z nðUnÞg
ΠðΘnÞ
,
36
Handbook of Statistics

where the last line is by Markov’s inequality. We can then simplify the expec-
tation in the upper bound displayed above using Fubini’s theorem:
PnΠfΘn \ Z nðUnÞg
¼
Z Z
1fθ  Θn \ Z nðUnÞg ΠðdθÞ PnðdUnÞ
¼
Z Z
1fθ  Θng 1fθ  Z nðUnÞg PnðdUnÞ ΠðdθÞ
¼
Z
Θn
PnfZ nðθÞg ΠðdθÞ:
By Chebyshev’s inequality, PnfZ nðθÞg  ðbnnεα
nÞ1, and hence
Pn DðηÞ
n
 1
2 ΠðΘnÞe2bnηnεα
n
n
o
 2ðbnnεα
nÞ1:
Finally, since α 	 2β implies nεα
n 	 1 and bn ! ∞, the upper bound is van-
ishing, which proves the claim.
□
Lemma 4 is a strengthening of Lemma 2, and is used to obtain an upper
bound on the numerator Nn(An) with probability approaching 1. First, we need
to define some notation: let n ¼ n1Pn
i¼1δTi denote the empirical distribu-
tion where δt is the Dirac point-mass at t. Let nf ¼ n1=2ðnf  Pf Þ denote
the empirical process.
Lemma 4. Under the conditions of Theorem 2,
NnðAnÞ≲ðMnεnÞqeηcMα
nnεα
n,
with Pn-probability ! 1,
where c > 0 is a constant.
Proof. Start by expressing the empirical risk difference as
Rnðθ?Þ  RnðθÞ ¼
Rðθ?Þ  RðθÞ  n1=2nð‘θ  ‘θ?Þ
n
o
:
Then the Gibbs posterior numerator can be written as
NðηÞ
n ðAnÞ ¼
X
∞
t¼1
Z
tMnεn<dðθ,θ?Þ<ðt + 1ÞMnεn
eηnfRnðθÞRnðθ?Þg ΠðdθÞ
¼
X
∞
t¼1
Z
tMnεn<dðθ,θ?Þ<ðt + 1ÞMnεn
eηnfRðθ?ÞRðθÞn1=2nð‘θ‘θ?Þg ΠðdθÞ:
By (14b), the right-hand side above can be upper bounded as
Direct Gibbs posterior inference on risk minimizers Chapter
1
37

NðηÞ
n ðAnÞ 
X
∞
t¼1
eηCtαMα
nnεα
n
Z
dðθ,θ?Þ<ðt +1ÞMnεn
eηn1=2jnð‘θ‘θ?Þj ΠðdθÞ,
where C > 0 is the constant hidden in “≳” in (14b). Next, it follows immedi-
ately from (14a) and Markov’s inequality that, for any an ! ∞,
sup
dðθ, θ?Þ<ðt + 1ÞMnεn
jnð‘θ  ‘θ?Þj ≲anfðt + 1ÞMnεngβ,
with Pn-probability converging to 1 as n ! ∞. This uniform bound on the
integrand above leads to
NðηÞ
n ðAnÞ 
X
∞
t¼1
eηCtαMα
nnεα
n + ηn1=2anfðt + 1ÞMnεngβ Πfdðθ, θ?Þ < ðt + 1ÞMnεng
 ðMnεnÞq X
∞
t¼1
ðt + 1ÞqeηCtαMα
nnεα
n+ηn1=2anfðt +1ÞMnεngβ,
where the second inequality follows by the condition on the prior distribution
Π. Next, the term in the exponent can be upper bounded by
ηCtαMα
nnεα
n½1  C0anfðt + 1ÞMnεngβαn1=2:
Since εβα
n
¼ n1=2 and we are free to choose an ≪Mαβ
n
, it follows that the
term in square brackets is bigger than, say, 1
2 for all sufficiently large n.
Therefore,
NðηÞ
n ðAnÞ ≲ðMnεnÞq X
∞
t¼1
tqeηðC=2ÞtαMα
nnεα
n,
with Pn-probability ! 1:
The summation is of the order eηcMα
nnεα
n, which proves the claim.
□
The Gibbs posterior concentration rate result follows directly from
Lemmas 3 and 4. Indeed, with Pn-probability converging to 1, we have
ΠðηÞ
n ðAnÞ ¼ NðηÞ
n ðAnÞ
DðηÞ
n
≲ðMnεnÞqeηcMα
nnεα
n
εq
ne2ηbnnεαn
¼ Mq
neηðcMα
n2bnÞnεα
n:
We are free to choose bn as small as we like and, if we take bn≪Mα
n, then the
upper bound vanishes, proving Theorem 2.
References
Alquier, P., Ridgway, J., 2020. Concentration of tempered posteriors and of their variational
approximations. Ann. Stat. 48 (3), 1475–1497.
Alquier, P., Ridgway, J., Chopin, N., 2016. On the properties of variational approximations of
Gibbs posteriors. J. Mach. Learn. Res. 17 (236), 1–41.
38
Handbook of Statistics

Berk, R.H., 1966. Limiting behavior of posterior distributions when the model is incorrect. Ann.
Math. Stat. 37, 745–746.
Bhattacharya, I., Martin, R., 2022. Gibbs posterior inference on multivariate quantiles. J. Stat.
Plan. Inference 218, 106–121.
Bhattacharya, A., Pati, D., Yang, Y., 2019. Bayesian fractional posteriors. Ann. Stat. 47 (1),
39–66.
Bissiri, P.G., Holmes, C.C., Walker, S.G., 2016. A general framework for updating belief distri-
butions. J. R. Stat. Soc. B. Stat. Methodol. 78 (5), 1103–1130.
Blei, D.M., Kucukelbir, A., McAuliffe, J.D., 2017. Variational inference: a review for statisti-
cians. J. Am. Stat. Assoc. 112 (518), 859–877.
Boos, D.D., Stefanski, L.A., 2013. Essential Statistical Inference. Springer Texts in Statistics,
Springer, New York, p. xviii+568.
Bunke, O., Milhaud, X., 1998. Asymptotic behavior of Bayes estimates under possibly incorrect
models. Ann. Stat. 26 (2), 617–644.
Chernozhukov, V., Hong, H., 2003. An MCMC approach to classical estimation. J. Econometrics
115 (2), 293–346.
Chib, S., Shin, M., Simoni, A., 2018. Bayesian estimation and comparison of moment condition
models. J. Am. Stat. Assoc. 113 (524), 1656–1668.
Davison, A.C., Hinkley, D.V., 1997. Bootstrap Methods and their Application. vol. 1 Cambridge
University Press, Cambridge, ISBN: 0-521-57391-2, p. x+582.
De Blasi, P., Walker, S.G., 2013. Bayesian asymptotics with misspecified models. Stat. Sin. 23,
169–187.
Efron, B., 1979. Bootstrap methods: another look at the jackknife. Ann. Stat. 7 (1), 1–26.
Efron, B., Tibshirani, R.J., 1993. An Introduction to the Bootstrap. Chapman & Hall, New York,
ISBN: 0-412-04231-2, p. xvi+436.
Fraser, D.A.S., 2011. Is Bayes posterior just quick and dirty confidence? Stat. Sci. 26 (3),
299–316.
Ghosal, S., van der Vaart, A., 2017. Fundamentals of Nonparametric Bayesian Inference. Cam-
bridge Series in Statistical and Probabilistic Mathematics, vol. 44 Cambridge University
Press, Cambridge, ISBN: 978-0-521-87826-5, p. xxiv+646.
Ghosh, J.K., Ramamoorthi, R.V., 2003. Bayesian Nonparametrics. Springer-Verlag, New York,
ISBN: 0-387-95537-2, p. xii+305.
Ghosh, J.K., Delampady, M., Samanta, T., 2006. An Introduction to Bayesian Analysis. Springer,
New York, p. xiv+352.
Gr€unwald, P., 2012. The safe Bayesian: learning the learning rate via the mixability gap. In: Algo-
rithmic Learning Theory. Lecture Notes in Comput. Sci., vol. 7568. Springer, Heidelberg,
pp. 169–183.
Gr€unwald, P.D., Mehta, N.A., 2020. Fast rates for general unbounded loss functions: from ERM
to generalized Bayes. J. Mach. Learn. Res. 21 (56), 1–80.
Gr€unwald, P., van Ommen, T., 2017. Inconsistency of Bayesian inference for misspecified linear
models, and a proposal for repairing it. Bayesian Anal. 12 (4), 1069–1103.
Hedayat, S., Wang, J., Xu, T., 2015. Minimum clinically important difference in medical studies.
Biometrics 71, 33–41.
Hjort, N.L., Holmes, C.C., M€uller, P., Walker, S.G., 2010. Bayesian Nonparametrics. Cambridge
University Press.
Holmes, C.C., Walker, S.G., 2017. Assigning a value to a power likelihood in a general Bayesian
model. Biometrika 104 (2), 497–503.
Huber, P.J., 1981. Robust Statistics. John Wiley & Sons Inc., New York, ISBN: 0-471-41805-6,
p. ix+308.
Direct Gibbs posterior inference on risk minimizers Chapter
1
39

Jiang, W., Tanner, M.A., 2008. Gibbs posterior for variable selection in high-dimensional classi-
fication and data mining. Ann. Stat. 36 (5), 2207–2231.
Kleijn, B.J.K., van der Vaart, A.W., 2006. Misspecification in infinite-dimensional Bayesian sta-
tistics. Ann. Stat. 34 (2), 837–877.
Kleijn, B.J.K., van der Vaart, A.W., 2012. The Bernstein-Von-Mises theorem under misspecifica-
tion. Electron. J. Stat. 6, 354–381.
Koenker, R., 2005. Quantile Regression. Cambridge University Press, Cambridge, p. xvi+349.
ISBN 978-0-521-60827-5; 0-521-60827-9.
Kosorok, M.R., 2008. Introduction to Empirical Processes and Semiparametric Inference.
Springer Series in Statistics, Springer, New York, ISBN: 978-0-387-74977-8, p. xiv+483.
Le Cam, L., Yang, G.L., 2000. Asymptotics in Statistics, second ed. Springer Series in Statistics,
Springer-Verlag, New York, ISBN: 0-387-95036-2, p. xiv+285.
Lyddon, S.P., Holmes, C.C., Walker, S.G., 2019. General Bayesian updating and the loss-
likelihood bootstrap. Biometrika 106 (2), 465–478.
Martin, R., 2019. False confidence, non-additive beliefs, and valid statistical inference. Int. J.
Approx. Reason. 113, 39–73.
M€uller, U.K., 2013. Risk of Bayesian inference in misspecified models, and the sandwich covari-
ance matrix. Econometrica 81 (5), 1805–1849.
Pauli, F., Racugno, W., Ventura, L., 2011. Bayesian composite marginal likelihoods. Stat. Sin.
21 (1), 149–164.
Ramamoorthi, R.V., Sriram, K., Martin, R., 2015. On posterior concentration in misspecified
models. Bayesian Anal. 10, 759–789.
Robbins, H., Monro, S., 1951. A stochastic approximation method. Ann. Math. Stat. 22, 400–407.
Shen, W., Ghosal, S., 2015. Adaptive Bayesian procedures using random series priors. Scand. J.
Stat. 42, 1194–1213.
Sriram, K., 2015. A sandwich likelihood correction for Bayesian quantile regression based on the
misspecified asymmetric Laplace density. Stat. Probab. Lett. 107, 18–26.
Syring, N., Martin, R., 2017. Gibbs posterior inference on the minimum clinically important dif-
ference. J. Stat. Plan. Inference 187, 67–77.
Syring, N., Martin, R., 2019. Calibrating general posterior credible regions. Biometrika 106 (2),
479–486.
Syring, N., Martin, R., 2020. Robust and rate-optimal Gibbs posterior inference on the boundary
of a noisy image. Ann. Stat. 48 (3), 1498–1513.
Syring, N., Martin, R., 2022. Gibbs posterior concentration rates under sub-exponential type
losses. Bernoulli. To appear; arXiv:2012.04505.
Tang, R., Yang, Y., 2021. Statistical inference for Bayesian risk minimization via exponentially
tilted empirical likelihood. arXiv:2109.07792.
van der Vaart, A.W., 1998. Asymptotic Statistics. Cambridge University Press, Cambridge, p. xvi
+443. ISBN 0-521-49603-9; 0-521-78450-6.
van der Vaart, A.W., Wellner, J.A., 1996. Weak Convergence and Empirical Processes. Springer-
Verlag, New York, ISBN: 0-387-94640-3, p. xvi+508.
Wang, Z., Martin, R., 2020. Model-free posterior inference on the area under the receiver
operating characteristic curve. J. Stat. Plan. Inference 209, 174–186.
Wang, Z., Martin, R., 2021. Gibbs posterior inference on a Levy density under discrete sampling.
arXiv:2109.06567.
Wasserman, L., 2008. Why isn’t everyone a Bayesian? In: Morris, C., Tibshirani, R. (Eds.), The
Science of Bradley Efron. Springer, New York, pp. 260–261.
Wu, P.-S., Martin, R., 2021. Calibrating generalized predictive distributions. arXiv:2107.01688.
40
Handbook of Statistics

Wu, P.-S., Martin, R., 2022. A comparison of learning rate selection methods in generalized
Bayesian inference. Bayesian Anal. To appear; arXiv:2012.11349.
Zhang, T., 2006a. From E-entropy to KL-entropy: analysis of minimum information complexity
density estimation. Ann. Stat. 34 (5), 2180–2210.
Zhang, T., 2006b. Information theoretical upper and lower bounds for statistical estimation. IEEE
Trans. Inf. Theory 52 (4), 1307–1321.
Direct Gibbs posterior inference on risk minimizers Chapter
1
41

This page intentionally left blank

Chapter 2
Bayesian selective inference
Daniel Garcı´a Rasines and G. Alastair Young*
Department of Mathematics, Imperial College London, London, United Kingdom
∗Corresponding author: e-mail: alastair.young@imperial.ac.uk
Abstract
We discuss Bayesian inference for parameters selected using the data. First, we provide
a critical analysis of the existing positions in the literature regarding the correct Bayes-
ian approach under selection. Second, we discuss two types of noninformative prior for
selection models. These priors may be employed to produce a posterior distribution in
the absence of prior information, as well as to provide well-calibrated frequentist infer-
ence for the selected parameter. We illustrate the proposed priors empirically through
several examples.
Keywords: Bayesian inference, Selective inference, Noninformative prior, Probability
matching
1
Introduction
Selective inference considers problems in which the parameter of interest is
selected using the data. It is well known that failing to acknowledge such
adaptivity in the subsequent inferences yields the reported error assessments
invalid, if the same data that was used for selection is then used for inference.
For example, type I error guarantees of frequentist testing procedures are lost.
In this work we review selective inference from a Bayesian viewpoint and
address the problem of specifying noninformative priors in these situations.
The settings considered here can be formalized as follows. Suppose we
have data Y whose distribution has a density or probability mass function
fðy; θÞ, y  Y, known up to a finite-dimensional parameter θ  Θ, and that
there exists a potential subparameter of interest, ψ ≡ψ(θ), that may be
selected for future study after observing the data, possibly by an artificially
randomized procedure. More precisely, we assume that there exists a function
p : Y ! ½0, 1
determined prior to the data collection such that, having
observed Y ¼ y, inference on ψ is performed with probability p(y). The func-
tion p(y), which encodes all the relevant mathematical information about the
selection mechanism, will be referred to as the selection function.
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.06.006
Copyright © 2022 Elsevier B.V. All rights reserved.
43

A popular illustrative example of a selection problem is the so-called
selected mean problem, where we have Y ¼ ðY1,…, YmÞT for some m > 1, with
Yi  N(θi, 1) independently and θ ¼ ðθ1,…, θmÞT m . In this setting the
parameter of interest is typically defined as the subset of θi’s that produce the
largest k observations for some prespecified k < m, or those that produce obser-
vations above a given threshold t  determined by some formal rule, such as
a multiplicity adjustment of a significance test. This model and generalizations
thereof have applications in many areas. For example, in biostatistics each
θi might represent a treatment effect for a given disease, and it could be the case
that only the most promising treatments are selected for further investigation
due to economic or time constraints.
An important part of analysis of Bayesian methods of inference concerns
behavior from a frequentist or repeated sampling perspective. Such consi-
derations relate to proper long-run calibration of Bayesian inference, ensuring,
for instance that a Bayesian 1  α credible set is, at least approximately, a
conventional 1  α frequentist confidence set. This repeated sampling per-
spective means that we consider, for a fixed “true” generating parameter value
θ0, samples obtained under the selection rule from density or mass function
proportional to f(y; θ0)p(y). From such a viewpoint, selection alters the sam-
pling distribution of the data by favoring data points with a higher selection
probability. This has clear implications in the frequentist paradigm and has
led to the development of the so-called conditional approach, which advocates
basing inference for the selected parameter on the conditional distribution of
the data given selection. This approach was lucidly formalized by Fithian
et al. (2017) and framed withing Fisherian statistical thinking by Kuffner
and Young (2018). Panigrahi et al. (2020) note that the conditional approach
to frequentist inference may be challenging: in particular, the necessary con-
ditional sampling distribution of the statistic used for inference may be
awkward, or intractable, typically involving nuisance parameters. A Bayesian
approach to inference is therefore attractive, on account of its conceptual
simplicity and ease of handling of nuisance parameters through marginaliza-
tion. If, however, Bayesian inference is to be used as a surrogate for a more
complicated frequentist analysis, it is necessary to consider repeated sampling
properties, in particular specification of priors which ensure sensible repeated
sampling calibration under selection. A key aspect of the current work is to
advocate use of priors in Bayesian selective inference which yield such
frequentist calibration.
The Bayesian standpoint regarding inference under selection is less clear cut
than for frequentist approaches. The conventional view is that inference should
not be altered by selection. The argument is that, since Bayesian inference oper-
ates conditionally on the data, in particular it conditions on the selection event,
which is therefore automatically accounted for, as explained by Dawid (1994).
This viewpoint was however questioned by several authors, most notably by
44
Handbook of Statistics

Yekutieli (2012), who argued that in some situations the posterior distribution
has to be appropriately modified in the presence of selection.
In the following section we analyze this discrepancy between the frequentist
and Bayesian approaches to inference under selection. We argue that, in gen-
eral, an adjustment for selection is necessary in order to achieve approximate
repeated-sampling validity. Formally, this adjustment is achieved by attaching
the prior density to the likelihood of the conditional distribution of the data
given selection. Then, in Section 3, we consider the problem of specifying non-
informative prior densities for selected parameters of exponential families via
the requirement of accurate repeated-sampling calibration. These priors admit
a simple expression and enable us to extend inferential methods for selective
normal models to more general settings in a relatively simple manner. We pro-
pose the use of either a data-dependent prior, which by construction achieves
the appropriate frequentist probability matching, or a Jeffreys prior, constructed
from the selective likelihood, which is demonstrated to provide similar results
and is easier to implement. Derivations are provided by Garcı´a Rasines and
Young (2021). The example of “play the winner,” which involves a high-
dimensional parameter, is considered in Section 3.1.2. This practically impor-
tant example has been a key element of discussion on Bayesian inference under
selection; see Dawid (1994) and Senn (2008).
2
Bayes and selection
Different approaches to statistical inference lead to two opposing views as to
the correct analysis of the data in the presence of selection. On the one hand,
frequentist methods evaluate the accuracy of inferential procedures with
respect to the sampling distribution of the data at a fixed value of the param-
eter. Since selection modifies the sampling distribution by favoring data
points with higher selection probability, it is clear that the reported accuracy
should be appropriately modified. On the other hand, Bayesians typically
adopt the view that, once the data has been observed, the recognition that a
different realization could have resulted in a different inferential problem,
or in no problem at all, should have no effect on the inference (Dawid, 1994).
Adoption of the first viewpoint leads to adherence to the so-called condi-
tional approach, which advocates that inference for the selected ψ should be
based on the conditional distribution of the data given selection. Such distri-
bution has density or mass function
fðy | S; θÞ ¼ fðy; θÞpðyÞ
φðθÞ
,
φðθÞ ¼ θ½pðYÞ;
(1)
where the normalizing constant φ(θ) is the probability that ψ gets selected
when θ is the true parameter, and S denotes the selection event. The motiva-
tion for basing inference on (1) is that, under repeated sampling from a given
Bayesian selective inference Chapter
2
45

f(y; θ), if inferences are only provided for those samples that get selected, the
reported error assessments are well calibrated. For example, nominal 1  α
confidence sets would contain the true parameter at least (1  α)% of the
times they are reported. Henceforth we will refer to the conditional distribu-
tion of Y given selection as the selective distribution, and to the corresponding
likelihood function, LS(θ) ¼ f(y|S;θ), as the selective likelihood.
An intuitive way to interpret the conditional approach is as a form of infor-
mation splitting. For a given ψ, let R be the random variable that takes the
value 1 if ψ gets selected and 0 otherwise. That is, RjY  Bernoulli{p(Y)}.
Following Fithian et al. (2017), the data-generating process of Y may be
thought of as consisting of two stages. In the first stage, the value of R,
r say, is sampled from its marginal distribution, and in the second, Y is sam-
pled from the conditional distribution Y | r. Since it is R that determines
whether we are going to provide inference for ψ or not, inference based on
information revealed in stage two is necessarily free of any selection bias,
as it removes the information about the parameter provided by R.
In the Bayesian literature the appropriate mode of inference under selec-
tion is less clear. The predominant viewpoint until recent years was that
Bayesian inference should not be modified in the presence of selection. Quoting
(Dawid, 1994):
Since Bayesian posterior distributions are already fully conditioned on the data,
the posterior distribution of any quantity is the same, whether it was chosen in
advance or selected in the light of the data: that is, for a Bayesian, the face-value
approach is fully valid, and no further adjustment for selection is required.
As Dawid points out, the contrast between the Bayesian and the frequentist
standpoints is somewhat paradoxical. In many situations, Bayesian analyses
formally match, either exactly or approximately, face-value frequentist ana-
lyses, and it is universally agreed that the latter methods are not valid in the
presence of selection. Why, then, would such results be correct if reached
by a Bayesian argument? Dawid argues that, certainly, whenever a frequentist
approach is unreasonable, so is any Bayesian approach that provides similar
answers. However, according to Dawid, this does not reflect a fundamental
issue about the Bayesian updating mechanism under selection. Rather, it is
the result of a poor prior specification.
To understand why some prior distributions can be problematic under
selection, consider the example, due to Dawid (1994), of providing inference
for the mean of Y  N(θ, 1) only if Y > 0, and consider the standard class of
conjugate priors for θ given by {N(0, λ2): λ > 0}. For a given λ, the posterior
distribution of θ given Y ¼ y is normal with mean {λ2/(1 + λ2)}y and variance
λ2/(1 + λ2). Small values of λ produce a shrinkage effect, pulling the posterior
density of θ toward 0, while large values produce posterior inferences which
are very similar to those provided by a face-value frequentist approach.
Now, consider this problem from a frequentist perspective. If the true θ is
46
Handbook of Statistics

significantly greater than zero, the selection bias will be very small and almost
no adjustment is needed. Conversely, if θ is small, selection needs to be
accounted for, as Y will tend to overestimate it. Therefore, the Bayesian anal-
ysis described before carried out with a large value of λ is appropriate only if
we expect θ to be large, and will lead us astray if it is not. This is obvious, in a
way: we would not adjust for selection if we believed a priori that θ is large
and that an adjustment would not be necessary, and we would adjust for it
if we suspected that θ is small and that therefore an adjustment would be
required.
Of course, the fact that the prior choice is important and should not be
taken lightly is not surprising. The key issue that the previous analysis high-
lights is that the impact of the prior on the analysis is stronger under selection,
and, perhaps more importantly, that noninformative or weakly informative
Bayesian analyses in these contexts are much more challenging than in nonse-
lective settings. In the previous problem, if no selection takes place, lack of
information about θ can be dealt with by taking a large, or even infinite, value
of λ, as this minimizes the influence of the prior on the analysis. In the pres-
ence of selection, however, this is no longer appropriate, as such theoretically
“noninformative” priors do in fact entail critical information about the param-
eter, namely, its likelihood to lie in a region of the parameter space for which
a selection adjustment would be appropriate.
However, even if we are comfortable with the implications of a certain
prior in a selective problem, unadjusted Bayesian inference can still be
problematic from a repeated-sampling viewpoint. Consider the model Y | θ
 N(θ, 0.2) and suppose that the prior distribution of θ is standard normal.
In Fig. 1 we have plotted the observed coverages of equi-tailed 0.9-credible
−1.5
−0.5
0.5
1.5
0.0
0.2
0.4
0.6
0.8
1.0
θ
Coverage
FIG. 1
Coverage of credible intervals under repeated sampling from fixed values of θ derived
from the unadjusted posterior (the red line corresponds to sampling with selection and the blue
one without it), and from the selective posterior (green).
Bayesian selective inference Chapter
2
47

intervals for fixed values of the true parameter in two different sampling
regimes: a selective one, where only the observations with Y > 0 were kept,
and a nonselective one, where we constructed the intervals for all the sampled
values of Y. The curves were obtained by simulation. We see that in the
selective regime the intervals are very poorly calibrated for small values
of θ, even for values with nonnegligible selection probability (for example,
when θ ¼ 0.5, the probability that Y > 0 is 0.13). By contrast, the coverage
figures in the nonselective settings are much more stable. The instability
observed in the selective regime is clearly undesirable, and, while a purist
may argue that inferences are still well calibrated once the coverage curve is
averaged over the prior distribution, which in this case is π(θ|S) ∝ϕ(θ)φ(θ),
where ϕ is the standard normal density, many Bayesians would presumably
agree that this behavior is too extreme.
To avoid issues relating to selection, a natural option would be to construct
the posterior distribution using only the information about θ provided by
Y |{R ¼ 1}, whereas before R is the indicator variable of the selection event.
This way, the prior distribution is updated using only information which is not
influenced by selection. Denoting the prior density by π(θ), the resulting
posterior density would be
πðθ | yÞ ∝πðθÞLSðθÞ ∝πðθÞfðy; θÞ
φðθÞ
:
(2)
In the remainder of this work we will refer to posteriors of this form as selec-
tive posteriors. Inference based on selective posteriors allows the injection of
prior information while avoiding potential problems arising from selection.
These type of posterior have been considered by several authors, typically
in settings where there is a explicit bias in the sampling mechanism of the
data, like survival models. They are discussed by Bayarri and DeGroot
(1987) and Bayarri and Berger (1998), and they also appear, in a different
context, in Bayarri and Berger (2000), where they are referred to as “partial
posterior densities” and are used for computing Bayesian p-values in a two-
step procedure. As noted by Harville (2022), posterior (2) can also be thought
of as following from a standard Bayesian updating of the modified prior
π*(θ) ¼ π(θ)/φ(θ). This could be given the following interpretation: since
unadjusted inference is very imprecise for values of θ with small selection
probability, we assign a larger prior probability to those values to achieve a
higher protection in those regions. In Fig. 1 we have plotted the coverage
curve produced by the selective posterior in the selective regime, which shows
a much smoother behavior than the unadjusted posterior.
2.1
Fixed and random parameters
The belief that Bayesian inference is unaffected by selection has also been
questioned by other authors on more conceptual grounds. Mandel and
Rinott (2007) and Mandel and Rinott (2009) considered a sequence of
48
Handbook of Statistics

binomial experiments in which only some of the inferences, corresponding to
promising results in the setting considered, were reported. They made the
observation that, if the sequence of parameters analyzed had been sampled
independently from a prior distribution, then no correction for selection was
needed, in agreement with the classical stance. However, if the sequence of
parameters had been generated by sampling one parameter from the prior
and then successively testing it, then the appropriate inference needed a cor-
rection for selection. This idea was later extended and refined by Yekutieli
(2012), who considered more general settings and linked these ideas to a
Bayesian analog of the false discovery rate.
According to Yekutieli, the correct Bayesian inference for a selected
parameter depends on how the selection mechanism acts on the parameter
space. Consider the joint sampling model of (θ, Y) and a selection function
p(y). Yekutieli calls θ random if the joint sampling scheme for the parameter
and data is such that pairs (θ, Y) are sampled from their joint distribution until
the selection event is observed, and calls θ fixed if it is sampled from its
marginal distribution, held fixed, and Y is sampled from the conditional distri-
bution Y | θ until selection takes place.
As before, let R be the binary random variable that indicates if selection of
the parameter under consideration has happened. If θ is random, its density
given selection is π(θ|R ¼ 1) ∝π(θ)φ(θ), whereas if it is fixed, its condi-
tional density in unchanged, π(θ|R ¼ 1) ¼ π(θ). In the case of the data, its
conditional density given θ and the selection event is fS(y;θ) ¼ f(y;θ)p(y)/φ(θ)
in both cases. Thus, the posterior distribution for a random parameter is
πðθ | yÞ∝πðθÞφðθÞfðy; θÞ
φðθÞ
¼ πðθÞfðy; θÞ,
(3)
while for a fixed parameter it is given by
πðθ | yÞ∝πðθÞfðy; θÞ
φðθÞ
:
(4)
Hence, Bayesian inference for a selected random parameter is unaffected by
selection, but for a fixed parameter the posterior needs to be adjusted, and it
is formally constructed by attaching the prior density to the selective likeli-
hood. Yekutieli argues that Dawid’s analysis follows from the implicit
assumption that the parameter is random.
To elucidate the difference between these two regimes let us consider the
following example, which is a slight variation of the first example in Yekutieli
(2012). Suppose that θ represents the average academic ability of the students
in a given population. Each student takes an exam and gets a grade Yi, and the
students whose grade exceeds a given threshold t are admitted to university.
If a person in the university, who has access to the grades obtained by the
admitted students and to prior knowledge about θ in the form of historical
data, say, wants to estimate the average academic ability in the population,
they should carry out the analysis treating θ as a fixed parameter. If, instead,
Bayesian selective inference Chapter
2
49

θ is a student-specific parameter that represents the student’s academic ability,
and the person in the university wants to estimate each of the academic
abilities of the admitted students, then each estimated θ should be treated as
a random parameter.
The previous analysis does not cover situations with noninformative
priors. It is argued by Yekutieli that, in cases where a noninformative prior
would have been used without selection, a noninformative prior should also
be used with selection, regardless of whether θ is random or fixed. The argu-
ment is that, just as in the nonselective regime, all the information about
the parameter in the selective problem comes from the data. Yekutieli further
suggests to use the same noninformative prior in the selective and nonselec-
tive models, because the decision of providing inference for a particular
parameter should not alter the prior.
We agree with the first assertion but not with the second one, that is, we
believe that the noninformative prior used in the selective model should be
different from the one that would be used in the nonselective one. noninfor-
mative priors are typically designed to achieve certain properties with respect
to the sampling model of the data. In a selection model, both in the random
and fixed parameter regimes, the sampling distribution of the data is the same
(it has density or mass function f(y|S;θ)), so the noninformative prior should
also be the same in both cases. Furthermore, since the sampling distribution of
the data depends on the selection mechanism, we argue that any appropriate
choice of noninformative prior should also depend on selection. Consider,
for example, a location model f(y;θ) ¼ g(y  θ). The standard noninformative
prior for θ in absence of selection is π(θ) ∝1, which enjoys several interest-
ing properties: it is the Jeffreys prior, minimizes the prior influence, and pro-
duces inferences with a valid frequentist interpretation. However, all these
nice properties no longer hold if inference for θ is only provided for certain
values of y, as the selective model {g(y  θ)p(y)/φ(θ): θ  Θ} is not a location
model unless p(y) ∝1. We expand on these ideas in the following section.
As for informative priors, while the posterior densities (3) and (4) are
formally correct given the respective sampling mechanisms, it is not clear that
a parameter can be labeled as random or fixed without explicit consideration
of this sampling process, which may not be well defined in some cases. We
therefore suggest to use the adjusted posterior to avoid any potential bias that
may arise due to selection.
3
Noninformative priors for selective inference
Noninformative priors allow the derivation of posterior distributions without
explicit incorporation of prior information. They serve different purposes:
the resulting posterior can be employed as a reference against which poster-
iors derived from subjective priors can be compared; they can be used to
derive frequentist methods that retain some of the appealing characteristics
50
Handbook of Statistics

of Bayesian methods; and they allow to carry out an analysis when very little
or no prior information is available. One natural way to derive noninformative
priors is by requiring that they are approximately well calibrated under
repeated sampling with a fixed parameter. More precisely, we may require
that they satisfy
θfψ  Π1ðαjYÞg ¼ α + εðα, θÞ,
(5)
where Π(ψjY) is the marginal posterior distribution function of the interest
parameter and ε(α, θ) is small for all α  (0, 1) and θ  Θ. Condition (5) ensures
that posterior claims about the parameter have approximate validity in a frequen-
tist sense. For example, (1  α)-credible intervals for ψ are also approximate
(1  α)-confidence intervals. In the context of selective inference, this condition
would be required to hold with respect to the selective distribution of the data.
In nonselective regimes involving independent and identically distributed
observations from a regular model, Datta and Mukerjee (2004) demonstrate
that an asymptotic expansion of the error quantity ε(α, θ) involves terms in
successive powers of n1/2. The quantile-matching requirement holds with
an error decreasing at rate n1/2 for any prior that places positive probability
around the true parameter value, where n is the sample size. In addition,
certain priors, known as probability-matching priors, improve on this error
rate, lowering it to n1 in general, and to n3/2 for some distributions; see
Datta and Mukerjee (2004). Unfortunately, the formal analysis leading to
the derivation of these priors cannot be replicated in selection models, as it
relies on the asymptotic normality of the posterior, which does not hold under
selection except in the trivial cases where this has an asymptotically negligi-
ble effect. Fig. 2 shows how the selective posterior distribution based on a
normal prior and sampling model can deviate significantly from normality if
the selection probability is not large.
−0.6
−0.2
0.2
0
1
2
3
4
y = 0.1
θ
−0.2
0.2
0.4
0.6
0
1
2
3
4
y = 0.2
θ
−0.2
0.2
0.4
0.6
0.8
0
1
2
3
4
y = 0.3
θ
FIG. 2
Selective posterior densities for the model Y  N(θ, 1001), θ  N(0, 1), and selection
event Y > 0 for different values of y. In dashed blue, the densities of N(y, 1001) distributions,
corresponding to the first-order asymptotic approximation of the posteriors in the absence of
selection.
Bayesian selective inference Chapter
2
51

Yekutieli (2012) argues that noninformative priors should not be altered in
the presence of selection. In particular, for a selective normal-location model,
the improper uniform prior π(θ) ∝1 is suggested. This prior is also used by
Panigrahi and Taylor (2018) for the case where θ is a linear regression para-
meter. We argue that a noninformative prior should be appropriately modified
in the presence of selection, to account for the change of sampling distribution.
As the following result illustrates, not doing so can lead to poorly calibrated pos-
terior inferences. The proof can be found in Garcı´a Rasines and Young (2021).
Proposition 1. Let Y  N(θ, σ2), with σ2 > 0 known, and p(y) ¼ 1(y > t) for
some fixed t  , and let
Πðθ j yÞ ¼
Z θ
∞
ϕfσ1ðeθ  yÞgφðeθÞ
1deθ
Z ∞
∞
ϕfσ1ðeθ  yÞgφðeθÞ
1deθ
(6)
be the selective posterior distribution function based on the uniform prior
π(θ) ∝1. Then,
θ0fθ0  Π1ðαjYÞ | Sg < α
8ðα, θ0Þ  ð0,1Þ  :
(7)
Proposition 1 says that, when selection takes the form of one-sided trunca-
tion, the uniform prior produces inferences that overstate values of θ with low
selection probability. So, in a way, correcting for selection overcompensates
the selection bias. In addition, simulation results at the end of this section
suggest that this claim holds for any selection mechanism with a increasing
selection function p(y). In Section 3.1 we will see that this overcompensation
also occurs in nonnormal models under standard noninformative priors.
Unlike in the nonselective case, where the uniform prior leads to exact
probability matching, it can be shown that in a selective normal-location
model exact matching can only be achieved with a data-dependent prior.
Before doing so, we introduce the concept of a confidence distribution.
Consider the model Y  N(θ, σ2), with σ2 known, and an arbitrary selection
function p(y) which does not vanish almost everywhere. Conditionally on
selection, the p-value function
Hðθ; yÞ ¼ θ Y  y|S
ð
Þ
(8)
is uniformly distributed over (0, 1) when θ is the true parameter value.
Furthermore, if y is such that 0 < H(0; y) < 1, then θ ! H(θ; y) is a distribu-
tion function. Details are given by Garcı´a Rasines and Young (2021).
Considered as a random function of Y, H(θ;Y) is an example of a so-called
confidence distribution (Xie and Singh, 2013). In a one-dimensional statistical
model ffðy; θÞ : θ  Θ  g, a confidence distribution is any function G(θ;Y)
that satisfies the following two conditions:
52
Handbook of Statistics

1. G(θ;Y) is uniformly distributed over (0, 1) when θ is the true parameter;
2. θ ! G(θ;y) is a distribution function for every y.
Note that, in the case discussed here, H(θ;y) is a distribution function if
0 < H(θ;y) < 1 for some θ, but since H(θ;Y)  U(0, 1) under θ, it follows that
θfHðθ; YÞ  ð0,1Þ|Sg ¼ 1, so the second condition is satisfied almost surely;
note that the condition 0 < H(0;y) < 1 trivially implies that 0 < H(θ;y) < 1
for all θ.
Given a confidence distribution H(θ;y), one can trivially construct an exact
probability-matching prior by requiring that the posterior distribution function
Π(θjy) equals H(θ;y). This type of construction appears, for example, in
Fraser et al. (2010), and can be used as a basis for deriving probability-
matching priors in more complex scenarios, as we will do in the next section.
The resulting prior density is usually data dependent, and is computed as
πyðθÞ ∝
∂
∂θ Hðθ; yÞ
∂
∂y Hðθ; yÞ
:
(9)
In the case of the selective normal-location model, this is, in addition, the only
sensible prior that provides exact matching. Indeed, consider the matching
equation
θ ΠðθjYÞ  αjS
f
g ¼ α,
α  ð0,1Þ:
(10)
Since the model is stochastically increasing in θ, Π(θjy) ought to be a decreas-
ing function of y for every θ. Denoting its inverse with respect to y by lθ(α),
we have that
θ Y  lθðαÞjS
f
g ¼ Hfθ; lθðαÞg ¼ α,
α  ð0,1Þ:
(11)
Letting α ¼ Π(θjy) gives the equality.
For a general selection mechanism the functional form of πy(θ) does not
admit a simple expression. In what follows we will, for illustration, restrict
attention to a class of natural selection mechanisms following from one-sided
truncation of the data. Let us assume that the data consists of two batches of
random samples of sizes n1 and n2 from the distribution N(θ, 1), with respective
sample means Y1  Nðθ, n1
1 Þ and Y2  Nðθ, n1
2 Þ, and suppose that the selec-
tion criterion is Y1 > t for some threshold t  . This situation arises when
either the data has been artificially split in order to achieve higher inferential
power, or when only the first batch is initially available and the second one is
gathered after selection occurs in order to obtain more information about the
parameter. We will in particular consider the case n2 ¼ 0, corresponding to a
situation where all the data available for inference has been used for selection.
Letting n ¼ n1 + n2, γ ¼ n1/n, and Y ¼ γY1 + (1  γ)Y2, this model can be
reduced by sufficiency to one involving a single observation Y  N(θ, n1) and
Bayesian selective inference Chapter
2
53

selection function pðyÞ ¼ ðy + W > tÞ, where W  N(0, n1{γ1  1}) is
independent of Y. To see this, simply define W ¼ (1  γ)(Y1  Y2), which fol-
lows the claimed distribution and is independent of Y, as they are uncorrelated
and normal, and note that Y1 ¼ Y + W. Observe that this analysis shows that, in
a normal-location model, selection based on a γ-split of the data is equivalent to
an additive randomized selection scheme with randomization noise W, as both
selection strategies produce the same selection function.
Note that many “selected mean” problems can be reduced to a situation of
the previous type via conditioning. Suppose that there are m independent
batches of size n1 from m different populations, with respective sample means
YðiÞ  Nðθi, n1
1 Þ, and that inference is sought for the means yielding thew
largest k observations, which we may assume to be θ1,…, θk without loss of
generality. If we condition on the data from the nonselected parameters,
Yðk+1Þ,…, YðmÞ, the resulting selective model becomes
fðyð1Þ,…, yðkÞ | S, yðk+1Þ,…, yðmÞ; θÞ ¼
Y
k
i¼1
fðyðiÞ |YðiÞ > t; θiÞ
(12)
where t ¼ max fyðk+1Þ,…,yðmÞg. Thus, the problem can be written as k sepa-
rate inferential problems of the previous type.
For models of the type assumed here, the exact probability-matching prior
can be written as
πyðθÞ ∝1  h1fn1=2ðθ  tÞg
h1fn1=2ðθ  yÞg ,
(13)
when n2 ¼ 0 (γ ¼ 1), where h1ðxÞ ¼ ð∂=∂xÞ log ΦðxÞ ¼ ϕðxÞ=ΦðxÞ. For γ < 1,
the prior can be simplified to
πyðθÞ ∝
γ1=2 ϕðn1=2
1
ðθ  tÞÞ
ϕðn1=2ðθ  yÞÞ

1  Φ
n1=2ðy  θ + γðθ  tÞÞ
ð1  γÞ1=2
(
)

Z ∞
y
n1=2ϕðn1=2ðey  θÞÞpðeyÞdey
Φðn1=2
1
ðθ  tÞÞ
8
>
>
<
>
>
:
9
>
>
=
>
>
;
+ Φ
n1=2
1
ðy  tÞ
ð1  γÞ1=2
8
<
:
9
=
;:
(14)
The derivation is given by Garcı´a Rasines and Young (2021), and this prior is
employed in the numerical implementations considered below.
Since evaluation of πy(θ) in the γ < 1 case requires approximating an
integral and dividing by small numbers, it can sometimes be numerically
unstable. As a computationally lighter alternative, we will also consider the
Jeffreys prior for the selective model, given by
54
Handbook of Statistics

πJðθÞ∝
n + ∂2
∂θ2 log φðθÞ

1=2
:
(15)
For the one-sided truncation models considered before, this prior can be
written as
πJðθÞ∝1 + γh2fn1=2
1 ðθ  tÞg
h
i1=2
,
(16)
where h2ðxÞ ¼ ð∂2=∂x2Þ log ΦðxÞ ¼ xh1ðxÞ  h1ðxÞ2.
Note that both priors depend on the sample size, which is unusual in
nonselective settings. In Fig. 3 we have plotted both priors and the resulting
posterior densities for this model, with n ¼ 20, γ ¼ 1, t ¼ 0, and y ¼ 0.2,
and in Fig. 4 for n ¼ 20, γ ¼ 0.75, t ¼ 0, and y ¼ 0. The uniform prior
and the respective posterior densities are also plotted for comparison. The
posterior densities corresponding to the two proposed priors are virtually iden-
tical, a behavior that extends to other choices of parameters and observations,
which suggests that the Jeffreys prior is well calibrated from a frequentist
viewpoint. Formal theoretical frequentist guarantees are currently unresolved,
but we provide empirical evidence of them in Table 1. The table shows the
coverage of the intervals (∞, Π1(αjY)] for this model with n ¼ 20,
t ¼ 0, and different combinations of (θ, γ, α), computed using numerical inte-
gration. The posterior distributions were constructed using the uniform prior
(U) and Jeffreys prior (J). We see that the latter option performs better in most
cases, and is considerably superior to the uniform prior when all the data is
used for selection. A key characteristic of these priors is that they assign
higher prior probabilities to regions of the parameter space with large selection
probability φ(θ), thereby correcting the overcompensation of the selection bias
described at the start of the section.
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
θ
Prior
−2.0
−1.0
0.0
0.5
1.0
0.0
0.4
0.8
1.2
θ
Posterior
FIG. 3
Left: uniform prior (black), probability-matching prior for y ¼ 0.2 (red), and Jeffreys
prior (blue) for the normal model with (n, γ, t) ¼ (20, 1, 0). Right: the resulting posteriors for
y ¼ 0.2 (the red density overlaps the blue one in this panel).
Bayesian selective inference Chapter
2
55

3.1
Noninformative priors for exponential families
The derivation of the probability-matching prior in the normal model is only
useful to the extent that it can be used to devise priors for other models. If the
model is normal, we can provide inference directly via the confidence distri-
bution. In this section we use the probability-matching and Jeffreys priors
discussed in the previous section to derive noninformative priors for selected
one-dimensional parameters of an exponential family, given a random sample
from it. This allows us to extend inferential procedures for selective normal-
location models to other models asymptotically. An attractive feature of expo-
nential families that is useful here is that they always admit a sufficient statis-
tic of the same dimension as the parameter. They are, in fact, the only regular
models for which this holds, by the Pitman–Koopman–Darmois Theorem.
This enables us to reduce the dimensionality of the selective model to that
of the parameter, leading to a substantial mathematical simplification of the
problem.
First, consider the one-dimensional case. Suppose that we have a random
sample Y1,…,Yn
from a full exponential family, with density or mass
function
fðyi; θÞ ¼ hðyiÞ exp ηðθÞsðyiÞ  AðθÞ
f
g:
(17)
We assume that the MLE of the nonselective model, ^θ ¼ argmaxθ Θ ηðθÞ
Pn
i¼1 sðyiÞnAðθÞ, exists for all realizations of the data. Assume, as before,
that the sample is divided into two sets of sizes n1 and n2, Y1,…, Yn1, and
Yn1+1,…, Yn, with respective maximum likelihood estimators ^θ1 and ^θ2, and
that selection only uses the first set of samples so that the selection function
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
θ
Prior
−2.0
−1.0
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
θ
Posterior
FIG. 4
Left: uniform prior (black), probability-matching prior for y ¼ 0 (red), and Jeffreys
prior (blue) for the normal model with (n, γ, t) ¼ (20, 0.75, 0). Right: the resulting posteriors
for y ¼ 0 (the red density overlaps the blue one in this panel).
56
Handbook of Statistics

TABLE 1 Estimated coverages of (2∞, Π21(α | Y)] for the normal-location model derived from the uniform (U) and Jeffreys (J) priors.
α
0.05
0.1
0.25
0.5
0.75
0.9
0.95
Prior
U
J
U
J
U
J
U
J
U
J
U
J
U
J
θ ¼ 0.5
γ ¼ 0.5
0.045
0.049
0.092
0.099
0.237
0.250
0.487
0.501
0.742
0.751
0.896
0.901
0.948
0.950
γ ¼ 0.75
0.035
0.047
0.075
0.096
0.209
0.247
0.458
0.502
0.724
0.754
0.889
0.903
0.944
0.952
γ ¼ 1
0.020
0.051
0.041
0.098
0.110
0.234
0.250
0.465
0.448
0.714
0.637
0.879
0.738
0.938
θ ¼ 0
γ ¼ 0.5
0.043
0.049
0.088
0.099
0.228
0.248
0.473
0.498
0.730
0.750
0.890
0.901
0.945
0.950
γ ¼ 0.75
0.038
0.050
0.077
0.099
0.199
0.243
0.427
0.492
0.693
0.748
0.872
0.901
0.935
0.952
γ ¼ 1
0.033
0.056
0.065
0.108
0.161
0.251
0.331
0.477
0.537
0.710
0.709
0.868
0.794
0.929
θ ¼ 0.5
γ ¼ 0.5
0.048
0.051
0.095
0.101
0.238
0.251
0.479
0.498
0.728
0.747
0.886
0.897
0.941
0.948
γ ¼ 0.75
0.048
0.052
0.095
0.103
0.236
0.255
0.471
0.501
0.713
0.744
0.871
0.893
0.930
0.945
γ ¼ 1
0.048
0.052
0.096
0.105
0.237
0.259
0.469
0.509
0.702
0.749
0.852
0.890
0.908
0.939

may be written as pðy1,…, yn1Þ . Furthermore, let us denote by p^θ1ð^θ1Þ ¼
½pðY1,…,Yn1Þ|^θ1 the selection function in terms of ^θ1, which is indepen-
dent of θ, as ^θ1 is a sufficient reduction of the first set of samples.
Heuristically, a noninformative prior density for θ may be derived as
follows. Let iðθÞ ¼ θ½ð∂2=∂θ2Þ log fðYi; θÞ be the per-sample Fisher infor-
mation of the nonselective model. Any function g(θ) satisfying g0(θ) ¼ i(θ)1/2
is known as a variance-stabilizing transformation, as it satisfies
n1=2fgð^θÞ  gðθÞg !
d Nð0,1Þ
(18)
by the Delta Method. In particular, for i ¼ 1, 2, we have that n1=2
i
fgð^θiÞ 
gðθÞg !
d Nð0,1Þ as ni ! ∞. Thus, for large values of n1 and n2, the original
selective model is approximately equivalent to a selective normal-location
model involving two independent observations, U1  NðgðθÞ, n1
1 Þ
and
U2  NðgðθÞ, n1
2 Þ , and selection function pðu1, u2Þ ¼ p^θ1fg1ðu1Þg . This
argument suggests choosing as a noninformative prior for the parameter
ν ¼ g(θ) a prior that is reliable in the limiting normal model. Denoting the
chosen prior by πν(ν), the resulting prior in the original parameterization is
given by πθ(θ) ∝i(θ)1/2πν{g(θ)}. In view of the previous discussion and the sim-
ulation results, we will take πν(ν) to be either the probability-matching prior or
the Jeffreys prior. Note that, in the absence of selection, both choices give
πν(ν) ∝1, so πθ(θ) specializes to the Jeffreys prior of the nonselective model,
πθ(θ) ∝i(θ)1/2, which is probability matching to order O(n1).
As an example, consider a model involving two binomial observations
Y1  Bin(n1, θ) and Y2 Bin(n2, θ) and selection event n1
1 Y1  0:5 ,
where θ  [0, 1]. The variance-stabilizing transformation of this model is
gðθÞ ¼ sin 1ðθ1=2Þ. Therefore, the option based on the Jeffreys prior produces
πJðθÞ∝θ1=2ð1  θÞ1=2 1 + γh2 n1=2
1
sin1ðθ1=2Þ  π
4


n
o
h
i1=2
,
(19)
and similarly for the option based on the exact probability-matching prior,
πy(θ), which we do not reproduce here. The left panel of Fig. 5 shows the
standard, nonselective Jeffreys prior π(θ) ∝θ1/2(1θ)1/2, and the two non-
informative selective priors for n1 ¼ 8, n2 ¼ 2, y1 ¼ 4, and y2 ¼ 1. The right
panel shows the corresponding posterior densities. As in the normal case, the
two selective priors are almost identical and favor parameter values with large
selection probability.
In the following example we illustrate the repeated-sampling performance
of the proposed priors, showing that they do in fact lead to well-calibrated
posterior inference in a frequentist sense.
58
Handbook of Statistics

Example 1. Exponential. Let Y1,…, Yn be a random sample from an exponen-
tial distribution with rate parameter θ > 0. For this model the variance-
stabilizing transformation is gðθÞ ¼ log ðθÞ. We consider the selection event
^θ1 > 1, where ^θ1 is the maximum likelihood estimator of θ based on a sub-
sample of size n1 ¼ 0.8  n. For the simulation we consider the values of
n ¼ 10, 80, and for each sample size we consider two true parameter values
θ0 defined to satisfy φn(θ0) ¼ 0.25, 0.75, corresponding to situations with
small and large selection probabilities. For each pair (n, θ0), we plot the
coverage of the interval ð∞, Π1ðαjY1,…, YnÞ as a function of α for the
nonselective Jeffreys prior π(θ) ∝θ1 and for the two noninformative priors
proposed in this work. The coverages were approximated via 104 simulations
from the conditional model ðY1,…,YnÞTj ^θ1 > 1. The results can be found in
Fig. 6. The performances of the selective Jeffreys and probability-matching
priors are practically identical and significantly better than that of the nonse-
lective prior.
We now consider situations where the full parameter θ is multidimensional
and the selected parameter, ψ, is one dimensional. Again, we assume that the
nonselective model is a full exponential family, with density or mass function
fðyi; θÞ ¼ hðyiÞ exp
ηðθÞTsðyiÞ  AðθÞ
n
o
,
θ Θ  p,
(20)
and, as before, we assume that selection is based on a random sample of size
n1, Y1,…,Yn1, and that there is a second set of samples of size n2, Yn1+1,…,Yn,
available for inference.
Suppose that we can write θ ¼ (ψ, χ), and let us reparameterize the model
to (ψ, λ), where λ ≡λ(ψ, χ) is orthogonal to ψ in the sense of Cox and Reid
(1987). This means that the Fisher information matrix in the latter
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
θ
Prior
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
θ
Posterior
FIG. 5
Left: nonselective Jeffreys prior (black), probability-matching prior for y1 ¼ 4 and y2 ¼ 1
(red), and Jeffreys prior (blue) for the binomial model. Right: the resulting posteriors (the red
lines overlap the blue ones).
Bayesian selective inference Chapter
2
59

parametrization has zeroes in all the nondiagonal entries of the first row and
column, or equivalently, that the maximum likelihood estimators ^ψ and ^λ
are asymptotically independent, in the absence of selection. Such a para-
meterization always exists when ψ is one dimensional, and admits a very
simple expression in terms of the cumulant function A(θ) if the model is para-
meterized in its natural form, so that η(θ) ¼ θ.
Let us denote by iψψ(ψ, λ) and iλλ(ψ, λ) the components of the Fisher infor-
mation matrices corresponding to ψ and λ, respectively. Also, let ^ψi and ^λi,
i ¼ 1, 2, be the maximum likelihood estimators based on the first and second
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
n = 10, prob. = 0.25
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
n = 80, prob. = 0.25
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
n = 10, prob. = 0.75
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
n = 80, prob. = 0.75
FIG. 6
Example 1. Coverage of the interval ð∞, Π1ðαjY1,…, YnÞ as a function of α for the
nonselective Jeffreys prior (red), the probability-matching prior (orange), and the selective
Jeffreys prior (green). The orange lines partially overlap the green ones.
60
Handbook of Statistics

set of samples, assumed to exist, and let p ^ψ1,^λ1ð ^ψ1, ^λ1Þ be the selection function
in terms of ð ^ψ1, ^λ1Þ . By orthogonality, the asymptotic distribution of ^ψ1
given ^λ1 and the selection event is selective normal with mean ψ, variance
{n1iψψ(ψ, λ)}1, and selection function p ^ψ1ð ^ψ1Þ ¼ p ^ψ1,^λ1ð ^ψ1, ^λ1Þ, where ^λ1 is
fixed at its observed value. Also, the asymptotic distribution of ^ψ2 given ^λ2
is N(ψ, {n2iψψ(ψ, λ)}1). Therefore, for a fixed value of λ, ð ^ψ1, ^ψ2Þjð^λ1, ^λ2Þ
follows asymptotically a selective normal distribution, with variance-
stabilizing transformation g(ψ; λ) satisfying g0(ψ; λ) ¼ iψψ(ψ, λ)1/2. Since ^λ1
is only mildly informative about ψ, the same heuristic argument as before would
lead to adoption of the conditional prior π(ψjλ) ∝iψψ(ψ, λ)1/2πν{g(ψ; λ)},
where πν(ν) is a suitable noninformative prior for the selective normal-location
model with location parameter ν ¼ g(ψ; λ). Since this choice does not constrain
the marginal prior of λ, a generic joint prior for (ψ, λ) would be of the form
πðψ,λÞ∝cðλÞiψψðψ,λÞ1=2πνfgðψ; λÞg,
(21)
where c(λ) is an arbitrary prior density for λ.
3.1.1
Inference for a selected normal mean with an unknown
variance
A simple selection problem involving nuisance parameters occurs when
inference is required for a selected normal mean and the variance is unknown.
Let Y1,…, Yn  Nðμ,σ2Þ, where both μ and σ2 are unknown, but only μ is of
direct interest to us. Suppose that the sample is divided into two sets of sizes
n1 and n2, with respective maximum likelihood estimators ^θ1 ¼ ðY1, V1Þ and
^θ2 ¼ ðY2, V2Þ of θ ¼ (μ, σ2). Suppose that, in order to determine whether
μ is of interest, we conduct the t-test V1=2
1
Y1 > n1=2
1
t for some prespecified
value of t. The selective density of the data is
f Sðy1,…, yn; μ, σ2Þ ¼ fðy1,…, yn; μ, σ2Þ1ðv1=2
1
y1 > n1=2
1
tÞ
μ,σ2ðV1=2
1
Y1 > n1=2
1
tÞ
:
(22)
Note that in the selective model the marginal distribution of V1, with marginal
density
f Sðv1; μ, σ2Þ ¼ fðv1; σ2Þμ,σ2ðv1=2
1
Y1 > n1=2
1
t|v1Þ
μ,σ2ðV1=2
1
Y1 > n1=2
1
tÞ
,
(23)
depends on μ, even though its nonselective density is free of it. However, in
this case the nonselective distribution of Y1jV1 is not free of σ2, so there is
no justification for conditioning on V1.
As expected by comparison with the univariate case, standard noninfor-
mative priors such as π(μ, σ2) ∝σ1 produce marginal posteriors for μ that
Bayesian selective inference Chapter
2
61

overstate, on average, smaller values of μ. In this case, the proposed nonin-
formative priors can be found easily, as the model is already parameterized
orthogonally. For example, the option based on the Jeffreys prior takes
the form
πJðμ,σ2; v1Þ∝cðσ2Þ
1 + n1
n h2
n1=2
1 μ
σ
 tv1=2
1
σ
 
!
(
)1=2
,
(24)
where v1 is the observed value of V1, and the option based on the probability-
matching prior for γ ¼ 1 is
πyðμ, σ2; v1Þ∝cðσ2Þ
1  h1 n1=2yμ
σ
 n1=2
σ

1
h1
n1=2μ
σ
 tv1=2
1
σ
 
!
(
)
:
(25)
The corresponding expression for the case γ < 1 is rather complicated and
therefore omitted, but can be obtained easily from (14). These priors assign
lower probabilities to small values of μ relative to σ. Fig. 7 illustrates the per-
formance of πJ(μ, σ2; v1) for c(σ2) ¼ σ1 under repeated sampling from the
selective model with n1 ¼ 50, n2 ¼ 10, t ¼ 2, and true parameter ðμ0,σ2
0Þ ¼
ð0,1Þ. The prior πy(μ, σ2; v1) gave numerical issues for small values of σ and
was not considered. The plot shows the empirical distribution functions of
Πðμ0jY1,…, YnÞ for the prior π(μ, σ2) ∝σ1 and for the proposed prior, as
estimated from 5  103 repetitions. The selection probability was computed by
numerical evaluation of the integral
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
FIG. 7
Estimated empirical CDFs of Πðμ0 jY1,…, YnÞ for the prior π(μ, σ2) ∝σ1 (red) and for
selective Jeffreys prior (green).
62
Handbook of Statistics

μ,σ2
Y1
V1=2
1
>
t
n1=2
1
 
!
¼
Z ∞
0
fðv1; σ2ÞΦ
n1=2
1
σ
μ  tv1=2
1
n1=2
1
 
!
(
)
dv1,
(26)
and the marginal posterior distribution of μ was approximated with a
Metropolis–Hastings algorithm with 5  103 steps. The results show that
the selection-adjusted prior produces posterior inference with a more reliable
frequentist calibration than the unadjusted one.
3.1.2
Inference for the winner
Consider the standard problem of providing inference for the mean of a larg-
est normal sample. Suppose we have n1 observations from m different means,
with sample means Yi  Nðθi,n1
1 Þ, i ¼ 1,…,m, and assume without loss of
generality that Y1 ¼ max fYi : i ¼ 1,…,mg so that θ1 is the parameter of
interest. Assume also that after selection we obtain another sample of size
n2 from θ1, with mean eY1  Nðθ1,n1
2 Þ. There are two natural inferential
models that are compatible with this selection rule. One has likelihood
L1ðθÞ∝ϕfn1=2
2 ðθ1  ey1ÞgQm
i¼1ϕfn1=2
1 ðθi  yiÞg
θ½Φfn1=2
1 ðθ1  TÞg
(27)
where n ¼ n1 + n2, θ ¼ ðθ1,…,θmÞT, and T ¼ max fYi : i ¼ 2,…,mg, and the
other has likelihood
L2ðθÞ∝ϕfn1=2
2 ðθ1  ey1ÞgQm
i¼1ϕfn1=2
1 ðθi  yiÞg
Φfn1=2
1 ðθ1  tÞg
:
(28)
The latter is derived from the reasonable procedure of conditioning on the
data Y2,…,Ym from the nonselected means: these only depend mildly on the
parameter of interest, under selection, and the resulting inference is simpler,
reducing to a one-dimensional normal problem.
For this model, the prior proposed in the discussion above takes the form
πðθÞ∝cðθ2,…,θmÞ 1 + n1
n h2 n1=2
1 ðθ1  tÞ
n
o
h
i1=2
(29)
for the case of utilizing the Jeffreys prior, and can similarly be constructed
based on the probability-matching prior. Note that in this case the original
parameterization is also orthogonal.
Since the distribution of Y2,…,Ym depends on θ1 given selection, condition-
ing on them, even if advisable on computational grounds, will generally reduce
the inferential power. In Table 2 we investigate this by comparing the length
ofcredible intervals obtained by attaching prior (29) with cðθ2,…,θmÞ≡1 to
the likelihoods L1(θ) and L2(θ). We set n1 ¼ n2 ¼ 5, true parameter
θ ¼ ð0,…,0ÞT , and m ¼ 2, 5, 10, 20. The credible intervals were computed
by taking the lower and upper 0.05 quantiles of the marginal posterior
Bayesian selective inference Chapter
2
63

distribution of θ1. To compute each of the intervals we run a Metropolis–Hast-
ings algorithm with 104 steps. The simulation size was 5  103, and the maxi-
mum standard error of the figures in the table was 0.005. We can see there is
some loss of power when we condition on the data from the nonselected means,
as indicated by longer intervals, and that this loss worsens slightly as the number
of means increases. We note, however, that the simulation for L1(θ) was much
more computationally challenging than for L2(θ) for large values of m. The
results for L1(θ) were found to be quite sensitive to the choice of integration
boundaries in the approximation of the selection probability. Similarly, use of
the prior based on probability matching is prone to numerical instability, as it
involves a high-dimensional numerical integration, so results are only presented
for the Jeffreys prior. Again, use of the Jeffreys prior is strongly supported in
terms of frequentist calibration and computational simplicity.
4
Discussion
We have discussed the selective inference problem from a Bayesian perspec-
tive, arguing in favor of a selection adjustment of the inference, contrary to
the conventional viewpoint. Furthermore, we have illustrated two classes of
noninformative prior densities for selection models which are approximately
probability matching. These priors provide some level of regularization in
low-probability regions and behave better than alternatives proposed in the
literature which are independent of selection. To derive these priors we first
considered the case of normal-location models, and then extended the analysis
to full exponential families by appealing to asymptotic considerations. One
of the proposed priors provides exact matching in the normal case by cons-
truction. The other is the Jeffreys prior, constructed from the selective likeli-
hood, which provides similar results and is more computationally stable.
These priors were derived heuristically and their behavior was demonstrated
TABLE 2 Coverage and average length of 0.9-credible intervals for the
mean giving the largest sample, derived from likelihoods L1(θ) and L2(θ)
and Jeffreys prior.
L1(θ)
L2(θ)
Cov.
Length
Cov.
Length
m ¼ 2
0.913
1.149
0.902
1.205
m ¼ 5
0.899
1.193
0.902
1.292
m ¼ 10
0.899
1.183
0.906
1.329
m ¼ 20
0.887
1.148
0.903
1.352
64
Handbook of Statistics

empirically. Formal analyses are challenging due to the nonstandard asymp-
totic behavior of selective models. Theory on probability-matching priors
typically relies on second-order normal approximations to the posterior which
are not applicable in the selective inference setting.
References
Bayarri, M.J., Berger, J.O., 1998. Robust Bayesian analysis of selection models. Ann. Stat. 26 (2),
645–659.
Bayarri, M.J., Berger, J.O., 2000. P values for composite null models. J. Am. Stat. Assoc.
95 (452), 1127–1142.
Bayarri, M.J., DeGroot, M.H., 1987. Bayesian analysis of selection models. J. R. Stat. Soc. D
(The Statistician) 36 (2/3), 137–146.
Cox, D.R., Reid, N., 1987. Parameter orthogonality and approximate conditional inference. J. R.
Stat. Soc. B 49 (1), 1–39.
Datta, G.S., Mukerjee, R., 2004. Probability Matching Priors: Higher Order Asymptotics.
Springer, New York.
Dawid, A.P., 1994. Selection paradoxes of Bayesian inference. In: Anderson, T.W., Fang, K.T.,
Olkin, I. (Eds.), Multivariate Analysis and its Applications. Monograph Series., vol. 24.
Institute of Mathematical Statistics, pp. 211–220.
Fithian,
W.,
Sun,
D.L.,
Taylor,
J.E.,
2017.
Optimal
inference
after
model
selection.
arXiv:1410.2597v4.
Fraser, D.A.S., Reid, N., Marras, E., Yi, G.Y., 2010. Default priors for Bayesian and frequentist
inference. J. R. Stat. Soc. B 72 (5), 631–654.
Garcı´a Rasines, D., Young, G. A., 2021. Bayesian selective inference: non-informative priors.
arXiv:2008.04584v2.
Harville, D.A., 2022. Bayesian inference is unaffected by selection: fact or fiction? Am. Stat.
76 (1), 22–28.
Kuffner, T.A., Young, G.A., 2018. Principled statistical inference in data science. In: Adams, N.,
Cohen, E., Guo, Y.-K. (Eds.), Statistical Data Science, World Scientific Publishing,
pp. 21–36.
Mandel, M., Rinott, Y., 2007. On Statistical Inference Under Selection Bias (Technical Report).
Center for Rationality and Interactive Decision Theory, Hebrew University, Jerusalem.
Mandel, M., Rinott, Y., 2009. A selection bias conflict and frequentist versus Bayesian view-
points. Am. Stat. 63 (3), 211–217.
Panigrahi, S., Taylor, J., 2018. Scalable methods for Bayesian selective inference. Electron.
J. Stat. 12, 2355–2400.
Panigrahi, S., Taylor, J., Weinstein, A., 2020. Integrative methods for post-selection inference
under convex constraints. arXiv:1605.08824v7.
Senn, S., 2008. A note concerning a selection “paradox” of Dawid’s. Am. Stat. 62 (3), 206–210.
Xie, M., Singh, K., 2013. Confidence distribution, the frequentist distribution estimator of a
parameter: a review. Int. Stat. Rev. 81 (1), 3–39.
Yekutieli, D., 2012. Adjusted Bayesian inference for selected parameters. J. R. Stat. Soc. B 74 (3),
515–541.
Bayesian selective inference Chapter
2
65

This page intentionally left blank

Chapter 3
Dependent Bayesian multiple
hypothesis testing
Noirrit Kiran Chandraa,* and Sourabh Bhattacharyab
aDepartment of Mathematical Sciences, The University of Texas at Dallas, Richardson, TX,
United States
bInterdisciplinary Statistical Research Unit, Indian Statistical Institute, Kolkata, WB, India
*Corresponding author: e-mail: noirritchandra@gmail.com
Abstract
In modern-day practical statistical problems with many parameters, one is seldom inter-
ested in testing only one hypothesis. Simultaneous inference on hundreds of parameters
is often necessary, for instance, in spatial, microarray datasets or in analyses of fMRI
data. Thus multiple testing has emerged as a very important area in statistical inference
and received substantial attention from researchers in both frequentist and Bayesian
paradigms. Here we provide a brief overview of multiple testing procedures with partic-
ular focus on the Bayesian techniques. In recent times dependent multiple testing has
drawn the attention of researchers as often in practical applications the test statistics
or parameters of interest have structural dependence. We further discuss a Bayesian
nonmarginal multiple testing procedure that uses such dependence structure in the deci-
sion procedure for improving accuracy of inference.
Keywords: Bayesian multiple testing, Dependent data, False discovery rate, Multiple
comparison procedures, Variable selection
1
Introduction
Multiple testing has emerged as a major area in modern statistics where the
goal is to test thousands of hypotheses simultaneously sometimes scaling up
to millions. Applications commonly arise in domains like spatial, genomics,
functional magnetic resonance (fMRI) data among others. Direct application
of single-parameter-based inference to many parameters in the multiple test-
ing context induces high chance of false positives. For example, consider
the problem of hypothesis testing on m parameters simultaneously. If statisti-
cally significant tests are performed at the 5% level, then on an average deci-
sions regarding 50 hypotheses would be incorrect for m ¼ 1000 even if all of
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.07.001
Copyright © 2022 Elsevier B.V. All rights reserved.
67

the null hypotheses are true. To control this multiplicity effect, classical mul-
tiple testing procedures focus on controlling the type-I error in the simulta-
neous inference context. Addressing multiplicity problems was started in the
1950s (Lehmann, 1957a, b). To illustrate, consider Table 1:
Note that, S, T, U, and V are unobservable quantities in real-life situations.
A large V is considered undesirable as this is the number of true null hypotheses
that are falsely rejected. The following are some measures of type-I error in
multiple testing context.
l
Family-wise error rate (FWER): P(V  1).
l
Per comparison error rate (PCER): EðVÞ
m .
l
False discovery rate (FDR): E
V
R_1


where a _ b ¼ max fa,bg.
l
Positive false discovery rate (pFDR): E V
R | R > 1


.
FWER is the probability of rejecting any true null, and PCER is the expected
proportion of false discoveries among all hypotheses. The FDR is the
expected proportion of false discoveries among all discoveries. Similarly, as
a measure of type-II error False nondiscovery rate (FNR) is defined as
E
T
ðmRÞ_1
n
o
. Dudoit et al. (2003) discussed in detail various issues related to
controlling different types of errors.
Several methods have been established to control different types of errors.
The FWER controlling procedure uses the Bonferroni correction that rejects
individual null hypotheses at α/m level of significance. However, this proce-
dure is too conservative and results in low power for substantially large num-
ber of tests. Step-down and step-up procedures have been developed in the
literature. In step-down procedures, testing generally starts with the hypothe-
sis corresponding to the most significant test statistic proceeding toward the
least significant and vice versa for step-up procedures. Holm (1979) proposed
an improved step-down procedure of controlling the FWER over the Bonfer-
roni correction method. Simes (1986) proposed a methodology for global test-
ing, that is, for testing the intersection of null hypotheses. From a multiple
testing point of view, it controls the FWER. Although it was originally
TABLE 1 Summary of m simultaneous hypotheses tests.
Accept H0
Reject H0
True H0
U
V
m0
False H0
T
S
m  m0
Observed
W
R
m
68
Handbook of Statistics

developed under independence, Sarkar and Chang (1997) and Sarkar (1998)
later validated it for certain types of positive dependence conditions as well.
Hommel (1988) derived a procedure based on Simes test that improves
Holm’s procedure. Based on Simes critical values, Benjamini and Hochberg
(1995) proposed the powerful approach of controlling the FDR under inde-
pendence. Later Benjamini and Yekutieli (2001) proposed a simple correction
that controls the FDR under positive correlation between the test statistics.
Somerville (2004) introduced the concept of the minimum critical value
(MCV) and developed step-down and step-up FDR procedures which are valid
for dependent or independent hypotheses. They conjectured that there are no
step-down FDR procedures which, for a given MCV, are uniformly more
powerful.
Storey (2003) advocated the positive FDR ¼ E V
R | R > 1


or pFDR over
the FDR as a measure of type-I error. Note that FDR ¼ pFDR  PrðR > 0Þ.
The pFDR considers the false discovery rate conditioned on at least one rejec-
tion as no rejection may not be an interesting situation in practice. Under
model (2), discussed later in Section 2.1, Storey (2003) discussed that the
pFDR has a simple Bayesian posterior probability like interpretation. Storey
(2002) showed that controlling the pFDR yields higher power when compared
to the Benjamini and Hochberg (1995) method.
In this chapter, we primarily focus on Bayesian multiple hypothesis test-
ing procedures, reviewing the key principles in Section 2. An overview of
general dependent multiple testing, followed by a more specialized discus-
sion on dependent Bayesian multiple testing, is provided in Section 3.
A new Bayesian procedure is discussed in the section that improves infer-
ence by making appropriate joint, dependent decisions, and in Section 4
its efficacy is demonstrated with a simulation experiment in a variable selec-
tion context with autoregressive dependence structure. We summarize this
chapter in Section 5.
2
Bayesian multiple hypothesis testing
2.1
Preliminaries and setup
Let Xn ¼ ðX1, X2,…, XnÞ be the observed data. Let the joint distribution of Xn
given θ ¼ ðθ1, θ2,…, θmÞ be PXn|θ() where θ are the parameters of interest and
θi  Θi for all i ¼ 1,…, m. In a Bayesian framework, we consider a prior Π()
on the parameter space. Let θ|Xn() and θ|Xn() denote the posterior probabi-
lity and posterior expectation of θ, respectively, given Xn. Additionally, let
Xn() and Xn() denote the probability and expectation, respectively, with
respect to the marginal distribution of Xn. Although in many applications,
including the one we consider below, m and n are considered to be equal, it
need not be so necessarily.
Bayesian multiple testing Chapter
3
69

Consider the following hypotheses:
H0i : θi  Θ0i vs H1i : θi  Θ1i,
where Θ0i
T Θ1i ¼ ∅and Θ0i
S Θ1i ¼ Θi, for i ¼ 1,…, m. The following setup
has been extensively studied in the multiple testing context:
Xi 
ind Nðθi,σ2Þ
(1)
and for i ¼ 1,…, n, the test of interest is
H0i : θi ¼ 0 vs H1i : θi 6¼ 0:
Letting ri ¼ 1(θi 6¼ 0), that is, ri ¼ 1 if and only if H1i is true, the following
mixture prior is widely assumed on the θi’s (Efron and Tibshirani, 2002;
Scott and Berger, 2006; Storey, 2002, and others)
θi|ri  ð1  riÞδf0g + rigðθiÞ,
Πðri ¼ 1Þ ¼ π,
(2)
where δ{a} is a degenerate distribution at a, g() is a heavy-tailed distribution,
and 0 < π < 1. The quantity vi ¼ θ|Xnðri ¼ 1Þ is often referred to as the pos-
terior inclusion probability of the variable Xi. A high value of vi indicates
strong evidence toward H0i being false.
This setting got popular with the advent of high-throughput sequencing
technologies (Pradhan et al., 2019) where one could get the expressions of
millions of genes. In case-control studies of high-throughput sequencing
experiments, Xi’s typically correspond to the difference in expressions of
the ith gene or microRNA between case and control. In such studies, scientists
are often interested in finding differentially expressed genes with high statis-
tical significance, that is, genes with high posterior inclusion probabilities.
By fixing π ¼ 0.5, a priori, the null and alternate hypotheses are favored
equally. Essentially in this case, the tests are performed independently. Note
that, by fixing π at some prespecified value, borrowing information across dif-
ferent hypotheses is precluded. In this case, Scott and Berger (2010) showed
that multiplicity will not be adjusted as the hypotheses are essentially being
tested independently. However, they showed that the multiplicity issue can
be taken care of by assigning a prior on π. The following conjugate beta prior
is widely considered because of its computational tractability:
π  Betaðaπ, bπÞ:
Note that, aπ ¼ bπ ¼ 1 implies π Unif(0, 1), that is, a priori Π(ri ¼ 1) ¼ 0.5
marginally. Although the uniform prior equally favors the null and the alter-
nate hypotheses, multiplicity is adjusted via borrowing of information using
the hierarchical prior on π. With a prior on π the model allows adaptive
learning on the proportion of true null hypotheses resulting in adjusted posterior
inclusion probabilities vis; see Scott and Berger (2010, Section 3.3) for a
detailed discussion. The results, however, can be sensitive to the choice of
70
Handbook of Statistics

the hyperparameters (Scott and Berger, 2006). Scott and Berger (2010) showed
that in an empirical Bayes approach if we estimate π in model (2), then also
multiplicity is adjusted. They have also provided conditions where the results
of the empirical Bayes and fully Bayes approaches are alike and when they
differ.
In this section, we have discussed essentially the modeling aspect of
Bayesian multiple testing approaches. In order to make decisions regarding
the concerned parameters, one additionally needs appropriate loss functions
and notions of errors. We discuss these in the following section.
2.2
The decision problem
Here we discuss the multiple hypothesis problem in a Bayesian decision the-
oretic framework. For i ¼ 1,…, m, let us first define the following quantities:
di ¼
1
if the ith hypothesis is rejected;
0
otherwise;
(
ri ¼
1
if H1i is true;
0
if H0i is true:
(
The false discoveries (FD) and false nondiscoveries (FN) are defined as
FD ¼
X
dið1  riÞ,
FN ¼
X
ð1  diÞri:
Similarly, FDR and FNR can be defined as
FDR ¼
X
dið1  riÞ
X
di _ 1
,
FNR ¼
X
ð1  diÞri
X
ð1  diÞ _ 1
:
With these notations of errors, the following loss functions are widely
considered:
LNðd, rÞ ¼ cFD + FN;
LRðd, rÞ ¼ cFDR + FNR,
(3)
where d ¼ ðd1,…, dmÞ and r ¼ ðr1,…, rmÞ. Since we do not observe r, we take
expectations with respect to the posterior distribution of the unknown para-
meters. This leaves the posterior expected losses to be functions of d and
the data Xn only. The posterior expectations of the losses are then minimized
with respect to d to obtain the optimal decision rule.
The following bivariate loss functions are also considered:
L2Nðd, rÞ ¼ ðFD, FNÞ;
L2Rðd, rÞ ¼ ðFDR, FNRÞ:
(4)
The optimal decisions under L2N are defined as the minimizer of θ|XnðFNÞ
subject to θ|XnðFDÞ  αN . Similarly under L2R, θ|XnðFNRÞ is minimized
subject to θ|XnðFDRÞ  αR. For all of the four aforementioned loss functions,
Bayesian multiple testing Chapter
3
71

M€uller et al. (2004) derived the form of the optimal decision rule in the
following theorem:
Theorem 1 Let FD(s, Xn) and FDR(s, Xn) are the respective FD and FDR
given data Xn for the decision rule di ¼ 1(vi  s). Then, under all four loss
functions, the optimal decision takes the form di ¼ 1(vi  t⁎). The optimal cut-
off t⁎ is t⁎
N ¼ c=ðc + 1Þ, t⁎
2NðXnÞ ¼ min fs : FDðs, XnÞ  αNg, and t⁎
2RðXnÞ ¼
min fs : FDRðs, XnÞ  αRg, under LN, L2N, and L2R, respectively.
The cutoff for LR is slightly more complicated and therefore we omit it in
this chapter. The proof of Theorem 1 can be found in M€uller et al. (2004).
To summarize, in Section 2.1 we discussed how multiplicity is adjusted
via borrowing of information across different hypotheses using a hierarchical
prior. Then in the current section, we discussed a rigorous decision theoretic
approach to make decisions on the parameters with some uncertainty quanti-
fication. After fitting a suitable model on the data, the desired loss function is
minimized to obtain the optimal decision regarding the concerned parameters.
The optimal decision configuration bd ¼ arg min dθ|XnLðd, rÞ for different
loss functions depends on the respective hyperparameters, for example, the
optimal decision rule corresponding to LN(d, r) is ^di ¼ 1 vi 
c
c + 1


. It is
straightforward to see that FDR is controlled at level 1/(c + 1) under this loss
function. Thus, the hyperparameter c is often tuned such that FDR  α where
0 < α < 1 is a prespecified threshold. An analogy can be provided with single
hypothesis testing methods where a parametric/nonparametric model is
assumed for the data and typically the interest is to test something on some
functional of the data, for example, whether the data are centered around 0 or
not. Parametric and nonparametric tests like likelihood ratio test, Wilcoxon rank
sum test, etc. are often used subject to controlling the type-I error at α level of
significance which is a measure of uncertainty quantification. Similarly, in the
multiple testing setup one makes decisions subject to controlling the FDR quan-
tifying the uncertainty associated with the decisions taken.
3
Dependent multiple testing
Early studies on multiple testing procedures assumed independence of the test
statistics, for example, Benjamini and Hochberg (1995) showed validity of
their FDR controlling procedure under independence between the test statis-
tics. This assumption is quite unrealistic in most applications of multiple test-
ing, for example, while testing for differentially expressed genes, the gene
expressions have dependence between them. Similarly, while testing for sig-
nals in spatial studies, locations that are geographically close are often highly
positively correlated. Benjamini and Yekutieli (2001) later showed that the
FDR control procedure is also valid when the test statistics have positive cor-
relations among each other. Finner and Roters (2002), Finner et al. (2007),
72
Handbook of Statistics

and Efron (2007) discussed the effect of dependence among test statistics,
among others. Qiu et al. (2005) showed that dependence among test statistics
significantly affects the power of many FDR controlling procedures.
Schwartzman and Lin (2011) and Fan et al. (2012) discussed estimation of
FDR under correlation.
In most of the aforementioned works, either Bayesian or non-Bayesian, the
main focus is checking the validity of the test procedures rather than actually
utilizing the dependency to increase efficiency. The decisions regarding dif-
ferent hypotheses are marginal in the sense that they do not depend upon each
other directly. Benjamini and Yekutieli (2001) proposed a correction on the
original procedure but do not use the dependence structure. In practical appli-
cations, the hypotheses are often dependent and exploiting the dependence
structure in the decision rule can lead to more reliable and closer-to-truth
inference.
When the decisions are not directly (deterministically) dependent, infor-
mation provided by the joint structure inherent in the hypotheses is somewhat
neglected by the marginal multiple testing approaches, even when we have
additional information regarding the dependence structure between the
concerned parameters. To illustrate, suppose that we want to test H0i : θi 
0 vs H1i : θi < 0, i ¼ 1, 2. Let T1 and T2 be the test statistics and suppose that
they are highly positively correlated. Let us consider the decision rule that we
reject H0i in favor of H1i if Ti < c, for some threshold c. Due to the high posi-
tive correlation between the test statistics, it is a natural guess that the tests
should be accepted or rejected together. Suppose both the null hypotheses
are true. However, for sampling perturbations, it is of course possible that
T1 < c but T2 > c, which would yield the counterintuitive result that H01 is
rejected but H02 is accepted. Using dependent decision rules should be helpful
to rectify these kinds of errors if the information provided by the dependence
is utilized judiciously.
In this regard, Chandra and Bhattacharya (2019) developed a novel Bayes-
ian nonmarginal multiple testing procedure that coherently takes the depen-
dence structure into consideration. Their approach is based on new notions
of error and nonerror terms, substantially enhancing efficiency by judicious
exploitation of the dependence structure among the hypotheses. In many
real-life situations, dependent prior structure is envisaged on the parameter
space based on available domain knowledge. For example in spatial statistics,
Gaussian process prior is often considered. In fMRI data, Gaussian Markov
random field prior is a common prior. In such cases, additional information
on the parameters is incorporated in the model through the prior distribution.
Various applications in recent times in fields as diverse as spatial statistics and
environment (Risser et al., 2019), time series (Scott, 2009), neurosciences
(Brown et al., 2014), biological sciences ( Jensen et al., 2009), to name only
a few, consider Bayesian models with dependent prior structures. The basic
idea behind the new nonmarginal multiple testing methodology is to
Bayesian multiple testing Chapter
3
73

incorporate such information, when available, in the testing procedure as well
to obtain an improved decision rule. The principle is coherent with the tradi-
tional Bayesian philosophy that using prior information (when available)
inference can be enhanced. We elaborate the method in the following section.
3.1
New error based criterion
Let Gi be the set of hypotheses (including hypothesis i) where the parameters
are dependent on θi. Define the following quantity:
si ¼
1
if Hdj,j is true for all j  Ginfig;
0
otherwise:

(5)
If Gi is a singleton, then we set si ¼ 1.
Now consider the term
TP ¼
X
m
i¼1
dirisi:
(6)
This is the number of cases i for which di ¼ 1, ri ¼ 1, and si ¼ 1; in words, TP
is the number of cases for which the ith decision correctly accepts H1i, and all
other decisions in Gi, which may or may not accept H1j, for j 6¼ i, are correct.
This quantity is referred to as the number of true positives (TP). TP is maxi-
mized subject to controlling the error term
E ¼
X
m
i¼1
dið1risiÞ:
(7)
The posterior expectation of  TP is minimized subject to controlling the
error E. In a Lagrangian formulation, we thus minimize the following function
gλðdÞ ¼ 
X
m
i¼1
diθ|XnðrisiÞ + λ
X
m
i¼1
diθ|Xnð1risiÞ
¼ 
X
m
i¼1
diwiðdÞ + λ
X
m
i¼1
dif1wiðdÞg
¼ ð1 + λÞ
X
m
i¼1
di
wiðdÞ
λ
1 + λ


where wiðdÞ ¼ θ|XnðrisiÞ ¼ θ|Xn
H1i \ ð
\
j6¼i, j GiHdj, jÞ


and λ is the Lagrange
multiplier. In case Gi is a singleton, wi(d) is replaced with the marginal
posterior probability θ|Xn H1i
ð
Þ.
For fixed λ, equivalent to minimizing gλ(d) with respect to d, one can
maximize
74
Handbook of Statistics

X
m
i¼1
di
wiðdÞ
λ
1 + λ


¼
X
m
i¼1
di wiðdÞβ
f
g ¼ fβðdÞ, where β ¼
λ
1 + λ:
Similar to the hyperparameter c in (3), it can be seen that with increasing β the
number of discoveries decreases in the nonmarginal method as well. The
quantity β (or equivalently λ) specifies the level of control on the error E.
Chandra and Bhattacharya (2019, Section 6.2) discussed an algorithm of
choosing β in practice while controlling the error at some desired level.
Definition 1 Let be the set of all m-dimensional binary vectors denoting all
possible decision configurations. Define
^d ¼ arg max d f βðdÞ
where 0 < β < 1. Then ^d is the optimal decision configuration obtained as the
solution of the nonmarginal multiple testing method.
The idea behind group formation of this nonmarginal multiple testing
method is completely different from existing cluster-based approaches in the
multiple testing literature (Benjamini and Heller, 2007; Heller et al., 2006;
Sun et al., 2015, and others). To the best of our knowledge, in most of the
existing cluster-based approaches disjoint clusters of signals are formed and
a single decision is considered for all signals in the respective clusters. Such
approaches are useful in situations where signals appear in clusters, for exam-
ple, signals corresponding to each voxel in fMRI studies. However, in the
nonmarginal method, the possibility that all the decisions within a group
may not be the same is allowed. Decisions regarding hypotheses in a group
highly influence each other through the si term introduced in (5). Moreover,
the groups are overlapping in general due to interdependence among hypoth-
eses in different groups. Thanks to this, decisions in two different groups can
also be dependent.
3.2
Choice of G1,…,Gm
The nonmarginal method depends on the choice of the Gis. However, in
implementation of the method, forming groups based on all dependent para-
meters can result in an overpenalization making the method unnecessarily
conservative. Chandra and Bhattacharya (2019) prescribed the following strat-
egy of group formation.
Let Ψmm ¼ (ψij) be the prior correlation matrix of θ. First, a desired per-
centile ψ of the ψij’s for i < j is obtained. Then only those indices j (6¼ i) are
included in Gi such that ψij  ψ. Thus, the ith group contains indices of the
parameters that are highly correlated with the ith parameter. If there exists
no index j such that ψij  ψ, then Gi ¼ {i}.
Bayesian multiple testing Chapter
3
75

Once the prior associated with the model is decided and well chosen, the
Gis as defined above will also be fixed and would lead to reliable results. In
many practical applications, for example, spatial, genomic, and others, there
exists prior grouping structure. Chandra et al. (2019) used this scheme in dis-
covering differentially expressed miRNAs in head and neck carcinoma.
In case the prior information on the correlation structure of the parameters
is weak, Ψ can be considered to be the posterior correlation matrix of the
parameters. In thorough simulation studies, Chandra and Bhattacharya
(2019) showed that such strategy substantially improves accuracy. Although
the nonmarginal procedure is asymptotically robust with respect to the choice
of group structure (Chandra and Bhattacharya, 2021), finite sample perfor-
mance can be sensitive on the thresholding. In thorough simulation studies
and multiple applications in diverse domains, considering ψ to be the 95%
quantile of the ψijs exhibits excellent empirical performance.
4
Simulation study
In this section we compare the performance of the nonmarginal decision
(NMD) procedure with the widely used Bayesian multiple testing methods
of M€uller et al. (2004, MPR) and Sarkar et al. (2008, SZG). With increasing
sample sizes, we study the performance of these methods in a variable selec-
tion context. We generate data from the following model with autoregressive
dependence structure of order 1 among the responses:
xi ¼ ρxi1 +
X
m
j¼0
βjzji + Ei, i ¼ 1,…, n,
(8)
where x0 ≡0, we set jρj < 1 for stationarity and Ei 
iid Nð0,σ2Þ. In particular we
take ρ ¼ 0.5, σ2 ¼ 1 and m ¼ 150. Regarding the m-dimensional true regres-
sion vector β ¼ ðβ1,…, βmÞT, we take 10 of those to be nonzero and the rest to
be zero. We generate the covariates zi ¼ ðz1,…,zmÞ from a Nm(0, Φ) distribu-
tion where Φ is a fixed m  m order positive definite matrix. We consider the
following multiple hypothesis testing problem
H0j : βj ¼ 0 vs H1j : βj 6¼ 0 for j ¼ 1,…, m:
(9)
4.1
The postulated Bayesian model
Since most of the true βjs are zero, we consider the following global local
shrinkage prior (Ishwaran and Rao, 2005)
βj |γj 
ind γjNð0, τ2
j Þ + ð1  γjÞNð0, vτ2
j Þ,
γj |π 
iid BernoulliðπÞ,
π  Betaðaπ, bπÞ,
τ2
j

iid Gaðaτ, bτÞ,
v  C+ð0,1Þ,
σ2  Gaðaσ, bσÞ,
ρ  Nð0,1Þ,
76
Handbook of Statistics

where Ga(a, b) denotes a gamma distribution with mean a/b and variance
a/b2, and C+(0, 1) is the standard Cauchy distribution restricted on the positive
real line. As discussed earlier, we share information across the hypotheses via
this hierarchical prior. By considering an unconstrained prior on ρ, we do not
enforce stationarity in the postulated model and let it learn from the data.
Note that, γj is the inclusion indicator of the jth covariate in the regression
model. Hence, we can equivalently rephrase the hypothesis testing problem in
(9) as the following:
H0j : γj ¼ 0 vs H1j : γj ¼ 1, for j ¼ 1,…, m:
4.2
Comparison criteria
Different multiple testing methods can yield different decision configurations
for the same given dataset. We consider two different criteria for comparing
the performances of the competing multiple testing procedures. Let dM be
the decision configuration obtained by a multiple testing method M. We com-
pute the Jaccard similarity coefficient ( Jaccard, 1901, 1908) between the true
decision configuration d0 and dM for each of three multiple testing methods
and compare their performances.
Let βM and ^ρ be the posterior means of β and ρ, respectively. In βM we
only consider those βjs such that H0j is rejected using method M and set
the rest equal to 0. We then compute the Euclidean distance between the true
(β, ρ) and ðβM,^ρÞ.
In other words, we compare the performance and accuracy of the three
competing Bayesian multiple testing methods by means of the Jaccard simi-
larity coefficient and Euclidean distance. For five different sample sizes, we
replicate our simulation experiments 750 times and compare the boxplots in
Fig. 1. For all the three competing Bayesian multiple testing methods, the pos-
terior FDR is controlled at level 0.05.
4.3
Comparison of the results
From Fig. 1A, we see that the Jaccard similarity coefficients have stabilized
near 1 sample size 75 onward indicating that the asymptotic theory indeed
takes precedence for all the methods, when the sample size gets sufficiently
large. Interestingly the NMD method has the fastest convergence rate with
respect to sample size in terms of accurately detecting the truly significant
covariates and also exhibits the best performance when the sample sizes are
small. Similar behavior can be observed with respect to the Euclidean dis-
tance in Fig. 1B. In this study, greater accuracy of the NMD method, particu-
larly for small sample size, indicates that in practical multiple hypothesis
testing applications where the sample size is generally much smaller as com-
pared to the number of parameters, incorporating the dependence structure in
the multiple testing method indeed enhances accuracy.
Bayesian multiple testing Chapter
3
77

5
Discussion
The important area of multiple testing has seen substantial developments over
recent years in both frequentist and Bayesian paradigms. Although multiplic-
ity control was a long-standing challenging problem in the frequentist para-
digm, using a Bayesian hierarchical model the issue can be taken care of
relatively easily. In this review, we provided the key ideas of classical and
Bayesian multiple testing procedures with a focus on the latter.
In recent times multiple testing on dependent scenarios received attention
of researchers as in most practical applications the parameters of interest have
0.25
0.50
0.75
1.00
25
50
75
100
200
Sample Size
Jaccard Similarity Coefficient
Method
MPR
NMD
SZG
10
20
30
40
25
50
75
100
200
Sample Size
Euclidean Distance
Method
MPR
NMD
SZG
A
B
FIG. 1
Performance comparison via boxplots for increasing sample sizes with X-axes plotting
the sample sizes: (A) the Jaccard similarity coefficient; (B) the Euclidean distance between the
estimated model parameters and the true parameters. (A) Comparing Jaccard similarity coeffi-
cient; (B) Comparing Euclidean distance between the true and estimated parameters.
78
Handbook of Statistics

structural dependence between them. In the Bayesian paradigm, dependence
structures among parameters are often incorporated into the model via joint
multivariate distribution priors, structured stochastic process priors, etc. Such
priors are very natural in various scientific undertakings and often imply
strong structural dependence among the hypotheses. In this chapter, we dis-
cussed the nonmarginal decision procedure of Chandra and Bhattacharya
(2019) that uses such dependence structure via dependent decision rules for
significantly improving inference. Thorough theoretical investigation on the
nonmarginal procedure has been done later by Chandra and Bhattacharya
(2021) including the case of model misspecification. In various applications
the procedure yielded scientifically meaningful inference, indicating that
incorporating dependence structure not only in the model but also in the deci-
sion procedure can substantially enhance inference.
References
Benjamini, Y., Heller, R., 2007. False discovery rates for spatial signals. J. Am. Stat. Assoc. 102
(480), 1272–1281. http://www.jstor.org/stable/27639977.
Benjamini, Y., Hochberg, Y., 1995. Controlling the false discovery rate: a practical and powerful
approach to multiple testing. J. R. Stat. Soc. B (Methodol.) 57 (1), 289–300. http://www.jstor.
org/stable/2346101.
Benjamini, Y., Yekutieli, D., 2001. The control of the false discovery rate in multiple testing
under dependency. Ann. Stat. 29 (4), 1165–1188. https://doi.org/10.1214/aos/1013699998.
Brown, A., Lazar, N.A., Dutta, G.S., Jang, W., McDowell, J.E., 2014. Incorporating spatial depen-
dence into Bayesian multiple testing of statistical parametric maps in functional neuroimag-
ing. NeuroImage 84 (1), 97–112.
Chandra, N.K., Bhattacharya, S., 2019. Non-marginal decisions: a novel Bayesian multiple testing
procedure. Electron. J. Stat. 13 (1), 489–535.
Chandra, N.K., Bhattacharya, S., 2021. Asymptotic theory of dependent Bayesian multiple testing
procedures under possible model misspecification. Ann. Inst. Stat. Math. 73 (5), 891–920.
Chandra, N.K., Singh, R., Bhattacharya, S., 2019. A novel Bayesian multiple testing approach to
deregulated miRNA discovery harnessing positional clustering. Biometrics 75 (1), 202–209.
https://doi.org/10.1111/biom.12967. https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12967.
Dudoit, S., Shaffer, J.P., Boldrick, J.C., 2003. Multiple hypothesis testing in microarray experi-
ments. Stat. Sci. 18 (1), 71–103. https://doi.org/10.1214/ss/1056397487.
Efron, B., 2007. Correlation and large-scale simultaneous significance testing. J. Am. Stat. Assoc.
102 (477), 93–103. http://www.jstor.org/stable/27639823.
Efron, B., Tibshirani, R., 2002. Empirical Bayes methods and false discovery rates for microar-
rays. Genet. Epidemiol. 23 (1), 70–86. https://doi.org/10.1002/gepi.1124.
Fan, J., Han, X., Gu, W., 2012. Estimating false discovery proportion under arbitrary covariance
dependence. J. Am. Stat. Assoc. 107 (499), 1019–1035. https://doi.org/10.1080/01621459.2012.
720478. PMID: 24729644.
Finner, H., Roters, M., 2002. Multiple hypotheses testing and expected number of type I. Errors.
Ann. Stat. 30 (1), 220–238. https://doi.org/10.1214/aos/1015362191.
Finner, H., Dickhaus, T., Roters, M., 2007. Dependency and false discovery rate: asymptotics.
Ann. Stat. 35 (4), 1432–1455. https://doi.org/10.1214/009053607000000046.
Bayesian multiple testing Chapter
3
79

Heller, R., Stanley, D., Yekutieli, D., Rubin, N., Benjamini, Y., 2006. Cluster-based analysis of
FMRI data. NeuroImage 33 (2), 599–608.
Holm, S., 1979. A simple sequentially rejective multiple test procedure. Scand. J. Stat. 6 (2),
65–70. http://www.jstor.org/stable/4615733.
Hommel, G., 1988. A stagewise rejective multiple test procedure based on a modified Bonferroni
test. Biometrika 75 (2), 383–386. https://doi.org/10.1093/biomet/75.2.383.
Ishwaran, H., Rao, J.S., 2005. Spike and slab variable selection: frequentist and Bayesian strate-
gies. Ann. Stat. 33 (2), 730–773. https://doi.org/10.1214/009053604000001147.
Jaccard, P., 1901. Etude comparative de la distribution Florale dans une portion des Alpes et des
Jura. Bull. Soc. Vaudoise Sci. Nat. 37, 547–579.
Jaccard, P., 1908. Nouvelles recherches sur la distribution Florale. Bull. Soc. Vaud. Sci. Nat. 44,
223–270.
Jensen, S.T., Erkan, I., Arnardottir, E.S., Small, D.S., 2009. Bayesian testing of many hypotheses
 many genes: a study of sleep Apnea. Ann. Appl. Stat. 3 (3), 1080–1101.
Lehmann, E.L., 1957a. A theory of some multiple decision problems, I. Ann. Math. Stat. 28 (1),
1–25. https://doi.org/10.1214/aoms/1177707034.
Lehmann, E.L., 1957b. A theory of some multiple decision problems, II. Ann. Math. Stat. 28 (3),
547–572. https://doi.org/10.1214/aoms/1177706873.
M€uller, P., Parmigiani, G., Robert, C., Rousseau, J., 2004. Optimal sample size for multiple test-
ing: the case of gene expression microarrays. J. Am. Stat. Assoc. 99 (468), 990–1001.
Pradhan, D., Kumar, A., Singh, H., Agrawal, U., 2019. Chapter 4–High-throughput sequencing.
In: Misra, G. (Ed.), Data Processing Handbook for Complex Biological Data Sources. Aca-
demic Press, pp. 39–52.
Qiu, X., Lev, K., Andrei, Y., 2005. Correlation between gene expression levels and limitations of
the empirical Bayes methodology for finding differentially expressed genes. Stat. Appl. Genet.
Mol. Biol. 4 (1), 1–32. https://EconPapers.repec.org/RePEc:bpj:sagmbi:v:4:y:2005:i:1:n:34.
Risser, M.D., Paciorek, C.J., Stone, D.A., 2019. Spatially dependent multiple testing under model
misspecification, with application to detection of anthropogenic influence on extreme climate
events. J. Am. Stat. Assoc. 114 (525), 61–78.
Sarkar, S.K., 1998. Some probability inequalities for ordered MTP2 random variables: a proof of
the Simes conjecture. Ann. Stat. 26 (2), 494–504. https://doi.org/10.1214/aos/1028144846.
Sarkar, S.K., Chang, C.-K., 1997. The Simes method for multiple hypothesis testing with posi-
tively dependent test statistics. J. Am. Stat. Assoc. 92 (440), 1601–1608. http://www.jstor.
org/stable/2965431.
Sarkar, S.K., Zhou, T., Ghosh, D., 2008. A general decision theoretic formulation of procedures
controlling FDR and FNR from a Bayesian perspective. Stat. Sin. 18 (3), 925–945. http://
www.jstor.org/stable/24308523.
Schwartzman, A., Lin, X., 2011. The effect of correlation in false discovery rate estimation.
Biometrika 98 (1), 199–214.
Scott, J.G., 2009. Nonparametric Bayesian multiple testing for longitudinal performance stratifica-
tion. Ann. Appl. Stat. 3 (4), 1655–1674.
Scott, J.G., Berger, J.O., 2006. An exploration of aspects of Bayesian multiple testing. J. Stat.
Plan. Inference 136 (7), 2144–2162. https://doi.org/10.1016/j.jspi.2005.08.031.
Scott, J.G., Berger, J.O., 2010. Bayes and empirical-Bayes multiplicity adjustment in the variable-
selection problem. Ann. Stat. 38 (5), 2587–2619. https://doi.org/10.1214/10-AOS792.
Simes, R.J., 1986. An improved Bonferroni procedure for multiple tests of significance. Biome-
trika 73 (3), 751–754. https://doi.org/10.1093/biomet/73.3.751.
80
Handbook of Statistics

Somerville, P.N., 2004. FDR step-down and step-up procedures for the correlated case. In: Recent
Developments in Multiple Comparison Procedures. Lecture Notes—Monograph Series,
vol. 47. http://www.jstor.org/stable/4356347.
Storey, J.D., 2002. A direct approach to false discovery rates. J. R. Stat. Soc. B (Stat. Methodol.)
64 (3), 479–498.
Storey, J.D., 2003. The positive false discovery rate: a Bayesian interpretation and the q-value.
Ann. Stat. 31 (6), 2013–2035. https://doi.org/10.1214/aos/1074290335.
Sun, W., Reich, B.J., Tony Cai, T., Guindani, M., Schwartzman, A., 2015. False discovery control
in large-scale spatial multiple testing. J. R. Stat. Soc. B (Stat. Methodol.) 77 (1), 59–83.
https://doi.org/10.1111/rssb.12064.
Bayesian multiple testing Chapter
3
81

This page intentionally left blank

Chapter 4
A new look at Bayesian
uncertainty
Stephen G. Walker*
Department of Mathematics, University of Texas at Austin, Austin, TX, United States
*Corresponding author: e-mail: s.g.walker@math.utexas.edu
Abstract
The aim of this paper is to present an alternative perspective on the foundations of the
Bayesian approach when considering uncertainty. We take the view that uncertainty is
caused by the missing data rather than having only a finite set of data. This turns out to
be an important distinction. The missing here refers to what would need to be seen for
the parameter or statistic of interest to be fully and uniquely specified. The connection
to the traditional view of Bayes can be understood via an obscure result of Doob in the
1940s; the point being that if the conditional distribution for the missing observations
given the observed data is constructed in a particular way, then one can recover the
traditional Bayesian posterior distribution. However, there is flexibility with how
the missing data can be modeled given the observations and this forms the basis of
the paper and the new ideas.
Keywords: Martingale, Missing data, Stochastic gradient descent
1
Introduction
This paper is an attempt to get to an understanding of the foundations of the
Bayesian approach, focusing on the notion of uncertainty. The paper can be
seen as a follow–up to Fong et al. (2022), where the emphasis is now put
on parametric models; whereas in Fong et al. (2022) the emphasis was placed
on nonparametric models.
There are a number of ways to introduce Bayesian analysis, among
which include the use of subjective prior information (Goldstein, 2006); an
exchangeable assumption for the observables (de Finetti, 1937); the elegant
but simple prior times likelihood (Robert, 2007); and a decision theoretic
approach appealing to rational behavior [Chapter 2 from Bernardo and
Smith (1994)]. At a practical level it could be argued that the foundations
are not relevant or can be regarded as not important, in that the goal is to
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.06.002
Copyright © 2022 Elsevier B.V. All rights reserved.
83

obtain a proper posterior distribution and the mechanics of doing this are very
clear. There are some issues surrounding the use of improper priors and their
interpretation, see, for example, Akaike (1980), but we will not be discussing
this in the current paper.
At least to me, there appears no serious argument as to why the Bayesian
analyst is required to start with a prior. If it is about the elicitation of a prob-
ability distribution from available information, there is no reason why this
cannot be done first once the data have been observed. Even if one does start
with a prior, the typical posterior will get a shape from the likelihood func-
tion, have a mean which is very close to a classical estimator, and a variance
which is of order the reciprocal of the sample size. So it is not as if it would
be difficult to construct a posterior directly. Our aim is to go one step further
than this; to motivate and demonstrate that a Bayesian only needs to construct
a predictive distribution conditional on any dataset of any size and, crucially,
without having to go through a prior or posterior distribution.
A reason for an attempt to rethink the Bayesian foundations is relevant
when the prior and likelihood to posterior approach as a procedure starts to
become problematic. This is particularly so in the era of big data and big
models and is even a concern currently with many Bayesian nonparametric
models; see for example Hjort et al. (2010). One would need to revert to some
foundational settings to know what to do if the standard approach can no
longer be pushed through and simplifications to the standard approach are
required.
The Frequentist approach is well understood; a knowledge of the sampling
distribution of a statistic or estimator. The simplest case would be an
unknown population mean. The sample mean has an expectation as the true
mean and has a variance which decreases as the sample size increases. Under
normal assumptions one can use the known distribution of the sample mean
and how it depends on the true mean to perform uncertainty quantification
operations such as the construction of confidence intervals and the implemen-
tation of hypothesis tests. That is, one infers about the true mean by observing
a sample mean and knowing the connection in terms of a distribution between
the two.
If specific distributions are not suitable, such as the normal, nonparametric
methods are a good stand in and a sampling possibility for constructing con-
fidence intervals and undertaking tests can be done using bootstrap techni-
ques, see Efron (1979); effectively working exclusively off the empirical
distribution function.
It is instructive to look at the bootstrap idea for handling statistical uncer-
tainty (the uncertainty created by the finite sample size), a worthwhile digres-
sion. So let n be the sample size and y1:n the observed data, assumed to be
independent and identically distributed from some unknown distribution F*.
Write Fn to be the empirical distribution function, arguably the best estimator
for F*. Interest is in some statistic T, which would be known if F*
84
Handbook of Statistics

were known, and would be known if n were arbitrarily large (since then Fn
would be F* and so we would know F*). If F* were known we could carry
on with the strict Frequentist plan by sampling multiple copies of Tn,
the observed estimator, and write these copies as (T*), and use these to con-
struct confidence intervals for T and test for specific values of T. This not
being possible as F* is not known; the next best strategy is to sample the
(T*) from Fn. Then proceed, with perhaps some asymptotic improvements,
as though these (T*) are samples from F*. The intermediary step to get each
T* is to sample an independent and identically distributed y
1:n from Fn and
obtain T* from the y
1:n in the same way the observed estimator for T; i.e.,
Tn, is obtained from y1:n.
The foundational thinking behind this frequentist agenda is clear; it is nat-
ural, intuitive, and it works. Perhaps the only criticism is the plan of trying to
infer about T by observing Tn, then attempt to obtain the sampling distribution
of Tn and work backward to better understand T. But how else can the uncer-
tainty associated with the finite n be dealt with?
Rather than get a repetition of estimators of T, that is, the T* from the i.i.d.
y
1:n, another plan would be to use the same starting point but instead to use it
to get a complete data set, ycomp ¼ (yobs, ymis) via obtaining ymis ¼ y
n+1:∞and,
along the way, updating the empirical distribution function as we gather up
the extra samples. With each complete data set we recover a “true” parameter
value, which is random, and is conditional on the observed data. This specific
plan turns out to be the Bayesian bootstrap (Rubin, 1981). While in Rubin
(1981) the emphasis was placed on a different sampling strategy, involving
randomly weighting the data, the idea of sampling the missing data as a
means by which to tackle the uncertainty created by the missing data is an
appealing one.
Following the previous paragraph, while we require a certain level of tech-
nical development, the key idea is the following: If the observed data yobs
combined with the missing data, ymis, give us the parameter or statistica
of interest precisely; that is, T ¼ T(yobs, ymis), then we can construct a
p(ymis|yobs), which then defines a p(T|yobs). We already have a “posterior”
distribution. If one argues this is not Bayesian, it actually is. For the formal
Bayesian procedure the p(ymis|yobs) is constructed in a very specific way,
using a prior distribution to start it all off. The question is why start with a
prior? All that is needed is a p(ymis|yobs) to adequately deal with the missing
data causing the uncertainty, and as we shall demonstrate during the paper,
this is not difficult to do, amounting to no more than the construction of a
density estimator from the data.
aWe talk about a parameter as indexing a model and the statistic as a population value; so the
parameter could be a normal mean and the statistic the corresponding population mean. Both
would be the same thing and known with a complete data set.
A new look at Bayesian uncertainty Chapter
4
85

In short, and putting the conclusion up front, Bayes implies handling the
uncertainty created by the missing data as the construction of a p(ymis|yobs);
and while there are constraints on how this can be done, there is a lot more
flexibility than the fully Bayesian approach which starts at the prior.
Detailing the layout of the paper. In Section 2 we further discuss the
notion of the missing data and discuss the general techniques for the construc-
tion of p(ymis|yobs) in the parametric and nonparametric cases. In Section 3
we focus on the parametric case and in Section 4 the nonparametric case.
Section 5 contains illustrations and Section 6 goes through the mathematical
theory supporting results from the previous sections. Finally, Section 7 con-
cludes with a brief discussion.
2
Missing data
An arbitrarily large sample size implies T would be known. To make this
more precise, the idea is that if TN were computed based on the sample y1:N,
then TN ! T as N ! ∞, and the convergence can be regarded as with proba-
bility one to T. To make a concrete example, TN could be a sample mean; that
is, TN ¼ N1PN
i¼1yi: One approach to “imputing” a T would be to sample
y
n+1:N, for some large N, and then take a T* from the complete data
ðy1:n, y
n+1:NÞ: The idea is that this would be a true sampled T while taking into
account the uncertainty in T. Repetition of this would yield multiple (T*) and
these would represent the uncertainty of T as a consequence of the missing data.
Obviously, to implement this, it would be required to construct a distribution
for the y
n+1:N given y1:n, and we write this as pðy
n+1:N |y1:nÞ.
To simplify this joint distribution, the obvious plan would be to use a
sequential sampling scheme; so
pðy
n + 1:N |y1:nÞ ¼
Y
N
m¼n + 1
pðym |y1:n,y
n + 1:m1Þ:
Hence, it is only required to specify a single “density estimator” for every
possible sample size. However, there will be a constraint. For if we define
T
m to be the statistic based on the sample ðy1:n, y
n+1:mÞ, we need the sequence
(T
m) to be convergent to a random variable as m ! ∞, although in practice it
will be stopped at some large N. The best way to achieve this, and is a prop-
erty of the full Bayesian approach also, is to rely on martingales. See, for
example, Williams (1991).
The choice of martingale can be separated into two types. For the nonpara-
metric model, we would arrange for the distribution Pð  | y1:n, y
n+1:mÞ to con-
verge to a random probability distribution, and this can be secured by asking
that the sequence of distribution functions ðPð  | y1:n, y
n+1:mÞÞ forms a martin-
gale. The mathematical requirement for the martingale is
Z
Pðy | y1:n, y
n+1:mÞ dPðy
m | y1:n, y
n+1:m1Þ ¼ Pðy | y1:n, y
n+1:m1Þ
(1)
86
Handbook of Statistics

for all m > n. This is satisfied if, for example, the sequence ðPð  | y1:n, y
n+1:mÞÞ
arises from a sequence of predictive distributions, derived via a prior and
likelihood to posterior to predictive construction. In this case, an obscure result
from the 1940s, attributable to Doob (1949), is that for the specific parameter
of interest θ for which the prior has been assigned, and for which T is the
population value and bT the corresponding estimator, the ðT
mÞ converges to a
T* and T* is a sample from the posterior distribution. In other words, there is
a short–cut when the ðPð  | y1:n, y
n+1:mÞÞ are this special case.b
To be more explicit, if
Pð  | y1:mÞ ¼
Z
Fð  Þ ΠðdF j y1:mÞ
represents the usual Bayesian predictive distribution, with Π(dF | y1:m) the
usual posterior distribution, and for m > n we sample y
n+1:∞and construct
PNð  Þ ¼ Pð  | y1:n, y
n+1:NÞ ¼
Z
Fð  Þ ΠðdF j y1:n, y
n+1:NÞ;
then, as N ! ∞, the PN converges almost surely to a random probability
distribution P and equivalently this P can be generated from Π(dF | y1:n).
To see a specific example, in a parametric normal location model, consider
the normal model, with πðθ | y1:nÞ ¼ Nðθ | y, σ2=nÞ and predictive density
given by
pð  | y1:nÞ ¼ Nð  | yn, σ2ð1 + 1=nÞÞ:
In general, for m  n, and write θm ¼ ym,
θm +1 ¼ θm + zm σ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
mðm + 1Þ
r
:
This follows from sampling ym+1  Nðym, σ2ð1 + 1=mÞÞ and then updating the
θm using the new sample mean; i.e., θm+1 ¼ (mθm + ym+1)/(m + 1). We get
that E θ∞¼ θn ¼ yn and the variance is given by
σ2 X
mn
1
mðm + 1Þ ¼ σ2
n :
So θ∞ Nðyn, σ2=nÞ.
For a nonparametric model, a well-known example is the Bayesian boot-
strap posterior; that is, F  DP(n Fn), where Fn is the empirical distribution
of the y1:n, and DP denotes the Dirichlet process (Ferguson, 1973). Such an
F can be sampled by taking
bFor example, θ could be a normal mean, T represents the population mean, and bT is the estimator
such as the sample mean.
A new look at Bayesian uncertainty Chapter
4
87

F ¼
X
n
i¼1
wi 1yi
(2)
where the w has a Dirichlet distribution with all parameters set to 1. On the
other hand, the predictive distribution is Pn ¼ Fn, and then sampling from
Pn, and updating the empirical distribution with the new sample, is equivalent
to a Po´lya–urn sampling scheme with parameters (y1:n, 1/n). It is known that
the limiting random distribution is equivalent to (2); see Blackwell and
MacQueen (1973).
Note that in both cases, the predictive distributions form a martingale
sequence, while in the parametric case the sequence of Bayes estimators also
forms a martingale. The Dirichlet process model is the most useful case for us
to look at. We can write
PmðyÞ ¼ ð1  wmÞ Pm1ðyÞ + wm 1ðym  yÞ
(3)
with the ym being sampled from Pm1 in order to represent the Polya–urn
scheme which starts at the empirical distribution function. Here the (wm) are
given by wm ¼ m/(c + m) for some c > 0. So (3) is easily seen to be a
martingale. For the parametric sequence, we have
y ¼ mym
m + 1 + ym + 1
m + 1 + σ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1=ðm + 1Þ
p
z1 and ym + 1 ¼ ym + σ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1=m
p
z2,
where z1 and z2 are independent standard normal, yielding, as required,
y ¼ ym + σ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1=m
p
z,
where z is also a standard normal random variable.
The usual Bayesian predictive sequences from m > n onward form a
martingale. For more complicated Bayesian models the predictive distri-
butions can be hard to obtain, for example, a Dirichlet mixture of normal
model. The aim in the next two sections is to construct martingale sequences
and to not be concerned with whether the yn+1:∞form an exchangeable
sequence.
3
Parametric martingale sequences
For the full Bayesian approach, both the posterior means and the sequence of
predictive distributions are martingales. In the nonparametric case, these are
the same object. In fact, as a consequence, adapting p(ymis | yobs) in the para-
metric case is harder in that we cannot get martingales for both parameter
and predictive very simply, unless of course we are fully Bayesian. Since
we are working with the parameters predominantly, we will ensure the
sequence of parameter estimators form a martingale.
To set the scene, we consider the update of a Bayesian posterior mean
from the parametric model f(y| θ). From the observed posterior π(θ|y1:n), with
88
Handbook of Statistics

posterior mean θn, we sample yn+1 from p(y|y1:n), the predictive density func-
tion given by
pðy|y1:nÞ ¼
Z
fðy|θÞ πðθ|y1:nÞ dθ:
(4)
Then the updated posterior mean is
θn+1 ¼
Z
θ fðyn +1 |θÞ πðθ|y1:nÞ dθ
pðyn +1 |y1:nÞ
:
(5)
Further, allowing such a sequence to proceed from m ¼ n + 1 onward, the
sequence (θm>n) is a martingale and converges to a random variable which
is a sample from π(θ|y1:n).
Our aim is to create a simpler martingale for (θm>n) while maintaining a
proximity to the Bayesian case. We start off by writing
θn+1  θn ¼
Z
ðθ  θnÞ fðyn+1 |θÞ πðθ|y1:nÞ dθ
pðyn+1 |y1:nÞ
:
Expanding the f(yn+1|θ) about θn to a second-order term, we get
Z
ðθ  θnÞ fðyn+1 |θÞ πðθ|y1:nÞ dθ ¼ σ2
n ∂f=∂θðyn+1 |θnÞ + oð1=nÞ,
where σ2
n is the posterior variance for π(θ|y1:n). Also
pðyn + 1 |y1:nÞ ¼ fðyn + 1 |θnÞ + 1
2σ2
n∂2f=∂θ2ðyn +1 |θnÞ + oð1=nÞ:
From these two expansions we can motivate the approximation
θn +1  θn ¼ σ2
n sðyn+1, θnÞ,
where sðy, θÞ ¼ ∂log fðy|θÞ=∂θ, which is the usual score function. This can
serve as a means by which to construct a parametric martingale sequence
(θm>n) whereby
θm+1 ¼ θm + σ2
m sðym+1, θmÞ
with ym+1  f(y|θm).
Clearly, due to the martingale, E (θm|θn) ¼ θn for all m > n. Further,
Varðθm+1 |θmÞ ¼ σ4
m IðθmÞ,
where IðθÞ ¼ 
R
∂l2=∂θ2ðy|θÞ fðy|θÞ dy is the Fisher information for the
model at θ, with l ¼ log fðy|θÞ. Consequently,
Varðθm + 1Þ ¼
X
m
i¼n
σ4
i IðθiÞ:
A new look at Bayesian uncertainty Chapter
4
89

For the martingale theorem to apply, this sum to infinity would need to be
finite. This would hold in general with σ2
i behaving as 1/i and the Fisher infor-
mation at θi being suitably bounded. The limit random variable θ∞is regarded
as a single sample from the “posterior” distribution, and E (θ∞|y1:n) ¼ θn
while just as with the usual Bayesian model, we can obtain Var(θ∞|y1:n) to
be of order 1/n with pretty much any choice for the constant.
Further, in this case, the chosen joint distribution for the missing data is
pðyn + 1:∞Þ ¼
Y
∞
m¼n + 1
fðym |θm1Þ
(6)
with
θm ¼ θm1 + σ2
m1sðym, θm1Þ,
m > n,
(7)
for the chosen sequence ðσ2
m>nÞ. We will present an illustration in Section 5.
Further motivation for the sequence (θm) for all m ¼ 1, 2, … is to be found
in stochastic gradient descent learning algorithms. If the (yn) are i.i.d. from
f(|θ*), then starting from some suitable θ0 we construct
θn+1 ¼ θn + σ2
nsðyn+1, θnÞ,
n  0:
This is precisely an iterative stochastic gradient ascent (maximizing rather
than minimizing) and under suitable conditions on s(y, θ) the (θn) converges
to the θ*, which is the unique value of θ for which
Z
sðy, θÞ fðy|θÞ dy ¼ 0:
In short, we can see the essence of the plan as replacing (4) and (5) with (6)
and (7).
3.1
Langevin posterior
It is of interest to compare the martingale sequence (6) and (7) with a Lange-
vin diffusion approach to sampling a posterior distribution π(θ|y1:n), with
model f(y|θ). The sequence for sampling the posterior is based on a discrete
time Markov chain approximation to a Langevin diffusion, and given by
θm+1 ¼ θm + γ2
m
∂
∂θ log πðθ|y1:nÞ +
ﬃﬃﬃ
2
p
γm
zm


,
where the (γm) are step sizes and the (zm) are i.i.d. standard normal random
variables. Ignoring the prior component, assuming it is relatively flat and neg-
ligible, this becomes
θm + 1 ¼ θm + γ2
m
X
n
i¼1
sðyi,θmÞ +
ﬃﬃﬃ
2
p
γm
zm
(
)
:
See, for example, Durmus and Moulines (2017). The (θm) for large m will
have π(θ|y1:n) as the approximate stationary density function. While this
90
Handbook of Statistics

looks very different to the martingale sequence, some math can be done to
see how they are connected, which would involve normal approximations to
s(ym+1, θm) terms.
To see some easy connections where it is most evident, we will take a look
at a Gaussian model; f(y|θ) ¼ N(y|θ, 1) and s(y, θ) ¼ y  θ. The martingale
sequence is, for m  n, given by
θm+1 ¼ θm + σ2
m zm,
with θn ¼ y. The Langevin sequence, for m  1, is given by
θm+1 ¼ θm + γ2
mnðy  θmÞ +
ﬃﬃﬃ
2
p
γm zm:
We compare these two approaches, taking σ2
m ¼ 1=m , to be motivated in
Section 5, and γm ¼ 1/m. Each sequence was run for 500 iterations and
1000 samples collected, that is, 1000 sequences were run. The two samples,
represented as histograms, are shown in Fig. 1. They are seen to be very
similar.
Martingale posterior
theta
Density
-0.8
-0.6
-0.4
-0.2
0.0
0.2
0.4
0.0
0.5
1.0
1.5
Langevin posterior
theta
Density
-1.0
-0.5
0.0
0.5
0.0
0.5
1.0
1.5
FIG. 1
Martingale posterior (upper panel) and Langevin posterior (lower panel).
A new look at Bayesian uncertainty Chapter
4
91

4
Nonparametric martingale distributions
This section is based on Fong et al. (2022). In the nonparametric case, there
is the single martingale, which is the sequence of predictive distributions.
The aim is, as with the parametric case, to modify the sequence of predictive
distributions and to ensure we keep a martingale while also ensuring the
sequence is easy to construct. We have already mentioned that Bayesian predic-
tive distributions can be difficult to get. For the full Bayesian model, we have
pðy|y1:n+1Þ ¼
Z
fðy|θÞ πðθ|y1:n+1Þ dθ
which we can equivalently write as
Z
fðy|θÞ fðyn+1 |θÞ πðθ|y1:nÞ dθ
Z
fðyn+1 |θÞ πðθ|y1:nÞ dθ
:
Hence
pðy|y1:n+1Þ ¼ pðy|y1:nÞ
Z
fðy|θÞ fðyn+1 |θÞ πðθ|y1:nÞ dθ
pðy|y1:nÞ pðyn+1|y1:nÞ
:
The rightmost term can be written as a copula; it is a joint density divided by
the two marginal density functions. For some copula density function c, we
can write
pðy|y1:n+1Þ ¼ pðy|y1:nÞÞ cðPðy|y1:nÞ, Pðyn+1 |y1:nÞÞ,
where the c could depend on y1:n. Indeed, many of the complicated Bayesian
predictive models do have the copula depending on the historical data. Some
notable exceptions would be conjugate models. Forcing the copula to only
depend on the sample size allows for the sequence of predictives to be easily
obtained without compromising accuracy; see Hahn et al. (2018).
Let us write the predictive copula model using the distribution rather than
density function; so
Pðy|y1:n+1Þ ¼ C0ðPðy|y1:nÞ, Pðyn+1 |y1:nÞÞ
where C0 ¼ ∂C/∂v and C(u, v) is the copula distribution function. Recall that
c(u, v) ¼ ∂2C/∂u∂v. It is now easy to check that
Z
Pðy|y1:n+1Þ pðyn+1 |y1:nÞ dyn+1 ¼ Pðy|y1:nÞ
using standard properties of a copula. Hence, whatever the copula is, the mar-
tingale is guaranteed.
92
Handbook of Statistics

The aim is to only employ copula which depend on the sample size. The
aim also is to provide a nonparametric flavor to the copula. In order to inves-
tigate this we consider the most well-known Bayesian nonparametric model
which is the Dirichlet process (Ferguson, 1973); known to be conjugate to
independent and identically distributed sampling. The predictive distribution
for the Dirichlet process predictive is of the form
Pðy|y1:n + 1Þ ¼ ð1wnÞ Pðy|y1:nÞ + wn 1fPðyn + 1 |y1:nÞ  Pðy|y1:nÞg,
where wn ¼ α/(α + n) for some α > 0. Here the indicator copula can be
viewed as the derivative of the maximal copula, i.e., Cðu, vÞ ¼ min fu, vg
and C0 ¼ 1(v < u).
The observation now is that we can retain the nonparametric model while
smoothing the indicator copula. This would lose the Bayesian predictive but
would, as we have seen, retain the martingale. For example, for some ρ > 0,
we take C(u, v) to be the Gaussian copula with correlation ρ. That is,
C0ðu, vÞ ¼ Φ Φ1ðuÞ  ρΦ1ðvÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
 
!
:
So the predictive model is given by P(y|y1:m) ¼
ð1wmÞ Pðy|y1:mÞ + wm Φ Φ1ðPðy|y1:m1ÞρΦ1ðPðym |y1:m1ÞÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1ρ2
p
 
!
: (8)
This can also be seen as a stochastic gradient ascent algorithm. In the next
section we will look at some illustrations.
5
Illustrations
This section is split into two parts; the first is the parametric case, for which
we construct a martingale on the parameter, and the second is the nonpara-
metric case for which we construct a martingale on the predictive distribution,
which becomes the “parameter.” In the parametric case we construct the
martingale so that the posterior mean is the maximum likelihood estimator,
θn, and the posterior variance is (n I(θn))1, with I(θ) denoting the Fisher
information.
5.1
Parametric case
For the parametric framework, we start by looking at the exponential
family whereby for some convex function b(θ) we have l(y, θ) ¼ yθ  b(θ),
so s(y, θ) ¼ y  b0(θ). The sequence (θm>n) is given by
θm+1 ¼ θm + σ2
m fym+1  b0ðθmÞg
A new look at Bayesian uncertainty Chapter
4
93

and so
Varðθm + 1 |θmÞ ¼ σ4
m b
00ðθmÞ:
This yields
Varðθ∞Þ ¼
X
∞
m¼n
σ4
m b
00ðθmÞ:
If the desire is to achieve a posterior with variance σ2/n, where, for example,
σ2 ¼ 1/b00(θn), then we would choose
σ2
m ¼
1
κm
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
b00ðθmÞ b00ðθnÞ
p
where P∞
m¼n1=κ2
m ¼ 1=n. The point here is that all posterior variances are of
the type c/n for some constant c > 0 and the precise value is determined by
the prior variance, which is subjectively chosen. Nevertheless, there are objec-
tive posterior variances and these can be achieved using the martingale
approach by selecting the (σm) appropriately.
To make such an illustration explicit we consider the Poisson model with
mean eθ. We generated n ¼ 50 samples with a true value of θ of as 1.4. We
generated the martingale sequence using
σ2
m ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
exp ðθn  θmÞ
p
m
with θn ¼ log y ¼ 1:42, and note that b00(θ) ¼ eθ. The martingale posterior is
the histogram in Fig. 2 alongside the posterior with a flat prior, that is,
πðθ|y1:nÞ∝exp ðθny  neθÞ.
5.2
Nonparametric case
Here we use the well-known galaxy dataset (Roeder, 1990) to demonstrate how
we obtain the martingale posterior distribution. The sample size is n ¼ 82 and
on transforming the data the values lie between 9 and 35. Fig. 3 is a plot of the
empirical distribution function.
For the sample size n we obtain P(y|y1:n) using the sequence (8) from
i ¼ 1 : n. We used wi ¼ 1/(i + 1) and started the first distribution to be the
normal distribution with mean and standard deviation provided by the data.
Since the correct distribution is far from this, it is not to be regarded as too
much of a leg–up a priori. The estimator from this is presented in Fig. 4.
Essentially, as was motivating, we have a smoothed version of the empirical
distribution function.
Next we generate extensions of the estimator using (8), now for m ¼ n + 1 : N,
with N ¼ 500. Each final random distribution would be a sample from
the martingale posterior distribution and Fig. 5 represents 10 such random
distributions.
94
Handbook of Statistics

theta
Density
1.2
1.3
1.4
1.5
1.6
0
1
2
3
4
5
6
FIG. 2
Martingale posterior (histogram) and noninformative posterior (line) for Poisson example.
5
10
15
20
25
30
35
40
0.0
0.2
0.4
0.6
0.8
1.0
y
distribution
FIG. 3
Empirical distribution function of the galaxy dataset.
A new look at Bayesian uncertainty Chapter
4
95

These random distributions are extremely easy to obtain. The modification
to (8) being that for m > n we take
Pðy|y1:mÞ ¼ ð1wmÞ Pðy|y1:m1Þ + wm Φ Φ1ðPðy|y1:m1ÞρΦ1ðumÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1ρ2
p
 
!
,
where the (um) are independently and identically distributed standard uniform
random variables.
In the next section we provide the mathematical foundations for the
existence of the martingale posterior distribution.
6
Mathematical theory
One of the theories on which the ideas rely is the stochastic gradient descent,
or ascent in our case, algorithm. If (yn) are i.i.d. from f(|θ*) and
θn ¼ θn1 + σ2
n sðyn, θn1Þ
(9)
for some suitable sequence ðσ2
nÞ
and starting point θ0, where sðy, θÞ ¼
∂log fðy|θÞ=∂θ, then we require that θn ! θ* as n ! ∞in the sense that
E[(θnθ*)2] ! 0. See, for example, Murphy (2021).
5
10
15
20
25
30
35
40
0.0
0.2
0.4
0.6
0.8
1.0
y
distribution
FIG. 4
Distribution estimator from copula predictive model.
96
Handbook of Statistics

The essence of the argument for convergence is as follows:
ðθn+1  θÞ2 ¼ ðθn + σ2
n+1sðyn+1, θnÞ  θÞ2
¼ ðθn  θÞ2 + σ4
n+1s2ðyn+1, θnÞ+2σ2
n+1sðyn+1, θnÞ ðθn  θÞ:
The last term on the right side is the important part and we write the expecta-
tion as
2σ2
n+1ðθn  θÞ2 sðθn, θÞ
θn  θ
where
sðθ, θÞ ¼
Z
sðy, θÞ fðy|θÞ dy
and the assumption is that for θ sufficiently close to θ*, it is that s(θ, θ*) > 0
if θ < θ*, s(θ, θ*) < 0 if θ > θ* and s(θ*, θ*) ¼ 0, and further that s(θ, θ*)/
(θ  θ*) < L within a neighborhood for some finite L > 0. Hence, we can
write
5
10
15
20
25
30
35
40
0.0
0.2
0.4
0.6
0.8
1.0
y
distribution
FIG. 5
Ten random distribution functions from martingale posterior.
A new look at Bayesian uncertainty Chapter
4
97

E ½ðθn+1  θÞ2 |y1:n  ðθn  θÞ2 1  2Lσ2
n+1


+ σ4
n+1s2ðyn+1, θnÞ:
Proceeding, for some constants c1 > 0 and c2 > 0, we have
dn+1  ð1  c1σ2
n+1Þ dn + c2σ4
n+1,
dn ¼ E ½ðθn  θÞ2,
and recall σ2
n+1 is of order 1/n. It is now standard mathematics to demonstrate
that dn ! 0.
This supports (9) as a Bayesian style learning algorithm, updating the
Bayesian estimator θ0 with data. Once we reach the end of the data, following
Doob, we revert to a martingale, generating the data with the current point
estimator and proceeding with (9) to infinity. The θ∞exists and has expected
value θn and the variance, just as with usual Bayesian updating, can, up to the
right order of 1/n, be arbitrary. Hence, the “posterior” is available. It simpli-
fies and is as valid as the traditional Bayesian approach.
That the sequence ðPmÞ∞
m¼n+1 given by (8) converges with probability one
to a random distribution function is a consequence of Theorem 2.2 appearing
in Berti et al. (2006). A modified version of the theorem relevant to the pres-
ent paper is provided here.
Theorem 1 If Pm(y), for m > n, converges almost surely to P∞(y) for each
y with E Pm ¼ Pn, and Pn is a tight distribution function, then Pi converges
weakly to the random distribution function P∞almost surely.
From the construction of Pm(y) for m > n it is clearly evident that Pm(y) is
a martingale for each y. From the martingale convergence theorem there
exists a P∞(y) for each y for which Pm(y) ! P∞(y) almost surely. See, for
example, Williams (1991).
Now Pn is a fixed distribution function and hence for any E > 0 there will
be a compact set K such that Pn(K) > 1  E. Hence, as such, Pn is a tight dis-
tribution function. For more on tightness see Billingsley (1999). Hence, from
the theorem, Pi converges weakly to the random distribution function P∞
almost surely.
Doob’s result on Bayesian inference utilized the principles of the theorem.
The martingales considered by Doob are derived from prior to posterior to
predictive sequences and are more than is required for convergence. Martin-
gales can be constructed which are not associated with exchangeable
sequences; they are generated however by predictive density sequences which
can be thought of more as density estimators driven solely by the data rather
than a prior and data.
7
Discussion
It is useful to think about the key idea which forms the backbone of the paper.
Bayes starts with a prior and moves through a sequence of posteriors stopping
at the point at which there are no more observed samples. One can carry on
98
Handbook of Statistics

with the sequence, now sampling data from the future sequence of predictive
distributions. The sequence of posterior means constructed in this way and
taken to a limit at infinity produces a sample from the observed true posterior
distribution.
The idea is to alter the sequence of posteriors to “posterior” means. So, if
the y1:n are i.i.d. f(| θ*) then the sequence starting at the subjective prior mean
θ0, and with
θn ¼ θn1 + σ2
n1 sðyn, θn1Þ,
n  1,
under regularity conditions, converges as n ! ∞to θ*. For a finite observed
sample of size n, proceed with this algorithm for m > n to infinity and sample
the (ym>n) from f(| θm). This will provide a sample from the “posterior.”
There is a flavor of linear Bayes (Goldstein, 1981), in which only means
and variances are utilized, yet our strategy provides samples from a full pos-
terior. Note, also, the observed and data-driven θn can be obtained in any way
the experimenter so desires; the point about the convergence of θn to θ* is a
means by which to motivate the sequence (θm>n).
For those readers remaining to see the key ideas and how it releases
Bayesian inference from the constraints of priors and likelihoods, we present
a simple demonstration. Suppose the problem is a mean parameter and assume
the observations are normal with unknown mean and a known variance of 1.
The data are actually i.i.d. standard normal and of size n ¼ 100. A progression
of the sample means can be constructed. See Fig. 6 for m ¼ 1, …, 100. This
initial sequence then ends at m ¼ n. From here we generate the missing data
from the current model using bθn ¼ yn, yn+1  Nðyn, 1Þ and then update the
mean to bθn+1 ¼ ðnbθn + yn+1Þ=ðn + 1Þ . This is equivalent to taking bθn+1 ¼
bθn + z=ðn + 1Þ and we repeat this to get a missing data set for each run of size
m ¼ 1000. Again see Fig. 6 for 10 sequences; so at the end of each sequence
is a sample from a martingale posterior. The point is that while this is Bayes-
ian in the sense of employing a p(ymis|yobs), it is not derived from a prior and
likelihood to posterior. Note that up to the sample size n, there is no uncer-
tainty. The observations have been seen and so there is no uncertainty in bθn.
However, there remains uncertainty in the true value and each martingale
sequence represents one possible continuation.
Finally, we link up the two strands of parametric and nonparametric,
which at first sight seem somewhat different. They are, actually, elegantly
connected. Let us write (8) as
PmðyÞ ¼ Pm1ðyÞ + wm C
0ðPm1ðyÞ,Pm1ðymÞÞPm1ðyÞ


,
and recall that
Z
C0ðPm1ðyÞ, Pm1ðymÞÞ  Pm1ðyÞ
f
g pm1ðymÞ dym ¼ 0
A new look at Bayesian uncertainty Chapter
4
99

for all y. The term s(y, P()) ¼ C0(Pm1(), Pm1(ym))  Pm1() then corre-
sponds to the score function s(y, θ) and can be seen as a nonparametric score
function; where wm corresponds to σ2
m.
Acknowledgments
I would like to thank two referees for comments on an earlier version of the paper.
References
Akaike, H., 1980. The interpretation of improper prior distributions as limit of dependent proper
prior distributions. J. R. Stat. Soc. B 42, 46–52.
Bernardo, J.M., Smith, A.F.M., 1994. Bayesian Theory. Wiley.
Berti, P., Pratelli, L., Rigo, P., 2006. Almost sure weak convergence of random probability mea-
sures. Stochastics 78, 91–97.
Billingsley, P., 1999. Convergence of Probability Measures. John Wiley & Sons Inc., New York.
Blackwell, D., MacQueen, J.B., 1973. Ferguson distributions via Polya urn schemes. Ann. Stat. 1,
353–355.
de Finetti, B., 1937. Foresight: its logical laws, its subjective sources. In: Annales de l’Institut
Henri Poincare, vol. 7, pp. 1–68.
0
200
400
600
800
1000
-0.2
0.0
0.2
0.4
m
theta
FIG. 6
Ten martingale sequences from observed sample mean.
100
Handbook of Statistics

Doob, J.L., 1949. Application of the theory of martingales. In: Le Calcul des Probabilites et
ses Applications 23–27, Colloques Internationaux du Centre National de la Recherche
Scientifique, Paris.
Durmus, A., Moulines, E., 2017. Nonasymptotic convergence analysis for the unadjusted
Langevin algorithm. Ann. Appl. Prob. 27, 1551–1587.
Efron, B., 1979. Bootstrap methods: another look at the jackknife. Ann. Stat. 71, 1–26.
Ferguson, T.S., 1973. A Bayesian analysis of some nonparametric problems. Ann. Stat. 1,
209–230.
Fong, E., Holmes, C., Walker, S.G., 2022. Martingale posterior distributions. J. R. Stat. Soc. B
(To appear with discussion).
Goldstein, M., 1981. Revising previsions: a geometric interpretation (with discussion). J. R. Stat.
Soc. B 43, 105–130.
Goldstein, M., 2006. Subjective Bayesian analysis: principles and practice. Bayesian Anal. 1,
403–420.
Hahn, P.R., Martin, R., Walker, S.G., 2018. On recursive Bayesian predictive distributions. J. Am.
Stat. Assoc. 113, 1085–1093.
Hjort, N.L., Holmes, C., Mueller, P., Walker, S.G., 2010. Bayesian Nonparametrics. Cambridge
University Press.
Murphy, K., 2021. Probabilistic Machine Learning: An Introduction. MIT Press.
Robert, C.P., 2007. The Bayesian Choice. Springer.
Roeder, K., 1990. Density estimation with confidence sets exemplified by superclusters and voids
in galaxies. J. Am. Stat. Assoc. 85, 617–624.
Rubin, D.B., 1981. The Bayesian bootstrap. Ann. Stat. 9, 130–134.
Williams, D., 1991. Probability With Martingales. Cambridge University Press.
A new look at Bayesian uncertainty Chapter
4 101

This page intentionally left blank

Chapter 5
50 shades of Bayesian testing
of hypotheses☆
Christian P. Robert∗
CEREMADE, Universite Paris Dauphine PSL, Paris, France
Department of Statistics, University of Warwick, Coventry, United Kingdom
CREST–ENSAE, Universite Paris–Saclay, Gif-sur-Yvette, France
∗Corresponding author: e-mail: xian@ceremade.dauphine.fr
Abstract
Hypothesis testing and model choice are quintessential questions for statistical inference
and while the Bayesian paradigm seems ideally suited for answering these questions, it
faces difficulties of its own ranging from prior modeling to calibration, to numerical
implementation. This chapter reviews these difficulties, from a subjective and personal
perspective.
Keywords: Bayesian model selection, BIC, DIC, Hypothesis testing, Improper priors,
Information criterion, Mixtures, Monte Carlo, Posterior predictive, Prior specification,
WAIC
1
Introduction
The concept of hypothesis testing is somewhat inseparable from statistics as
principled hypothesis testing is unfeasible outside a statistical framework,
while testing may be the most ubiquitous and long-standing manifestation
of statistical practice, if not in theoretical statistics. The implementation of
this goal is also subject to many interpretations and controversies, as illu-
strated by the recent American Statistical Association statement on the dan-
gers of over-interpreting p-values (Wasserstein and Lazar, 2016) and other
calls (Benjamin et al., 2018; Gelman and Robert, 2014; Johnson, 2013;
McShane et al., 2019). In particular, testing is a dramatically differentiating
☆Some paragraphs of this chapter have first appeared on the author’s personal blog. Section 6
mostly summarizes the proposal made in Kamary et al. (2014). The author is very much grateful
to Alastair Young for his helpful comments on earlier versions.
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.06.003
Copyright © 2022 Elsevier B.V. All rights reserved.
103

feature separating classical from Bayesian paradigms, both conceptually and
practically (Berger and Sellke, 1987; Casella and Berger, 1987). This opposi-
tion will not be covered by this chapter.
Even within the Bayesian community, testing hypotheses remains an area
that is wide open to controversy and divergent opinions, from banning any
form of testing to constructing pseudo-p-values. While the notion of the pos-
terior probability of an hypothesis appears as a “natural” answer in a Bayesian
context, there exist many issues with that choice, from the impact of the prior
modeling to the impossibility of using improper priors, as shown by the
Jeffreys–Lindley paradox (Lindley, 1957; Robert, 2014b). Furthermore, the
most common binary (i.e., accept vs reject) outcome of an hypothesis test
appears more suited for immediate decision (if any) than for model evalua-
tion, clashing with the all-encompassing nature of Bayesian inference.
The literature on Bayesian hypothesis testing is huge and we can only
point out to a few significant entries like Berger (1985), Gelman et al.
(2013), Vehtari and Ojanen (2012), and Gelman et al. (2014). The in-depth
analysis of Harold Jeffreys’ input by Ly et al. (2016) is quite noteworthy.
2
Bayesian hypothesis testing
In induction there is no harm in being occasionally wrong; it is inevitable that
we shall be. But there is harm in stating results in such a form that they do
not represent the evidence available at the time when they are stated.
Jeffreys (1939)
As a preliminary, let me point out that Bayesian hypothesis testing (or model
selection, as I will use both terms interchangeably) can be seen as a compari-
son of k > 1 potential statistical models toward the selection of the model that
fits the data “best.” A mostly accepted perspective is indeed that it does not
primarily seek to identify which model is “true,” but compares fits through
marginal likelihoods or other quantities.
A marginal likelihood (or evidence) is defined as the average likelihood
function
mðxÞ ¼
Z
Θ
LðθjxÞπðθÞ dθ
where L(θjx) denotes the likelihood function attached to the sample x, π() is
the prior density, and Θ is the parameter space.a This quantity naturally
includes a penalization addressing model complexity and over-fitting, through
the averaging over the whole set Θ, that is mimicked by Bayes information
aThis marginal sampling distribution is also called the prior predictive distribution as the
integrated standard sampling distribution with respect to the prior distribution. It can be simulated
by first generating a parameter value from the prior and second generating from the sampling
distribution indexed by this realization of the parameter.
104
Handbook of Statistics

(BIC) (Schwartz, 1965; Schwarz, 1978) and deviance information (DIC) cri-
teria (Plummer, 2008; Spiegelhalter et al., 2002). A fundamental difficulty
with the marginal likelihood is that it exhibits a long-lasting impact of prior
modeling, in that the likelihood input does not quickly counter-balance the tail
behavior of the prior.
Each model (or hypothesis) Mi
under consideration comes with an
attached triplet (Li(jx), πi(), Θi) ði ¼ 1, …, kÞ. In addition, prior weights ωi
are characterizing the prior probabilities of the models, leading to an encom-
passing prior
πðθÞ ¼ ω1π1ðθÞΘ1ðθÞ+⋯+ωkπkðθÞΘkðθÞ
and the corresponding posterior probabilities
πðMijxÞ ¼
ωi
Z
Θi
LiðθijxÞπiðθiÞ dθi
Xk
j¼1
Z
Θj
ωjLjðθjjxÞπjðθjÞ dθj
¼
miðxÞ
Xk
j¼1 mjðxÞ
A decision-theoretic approach based on the Neyman–Pearson formalism of
hypothesis testing leads to selecting the most probable model (Berger, 1985;
Lehmann, 1986). However, the strong impact of the values of the prior
weights ωi on the numerical values of these posterior probabilities led to their
removal and the construction of the Bayes factors (Haldane, 1932; Jeffreys,
1939; Wrinch and Jeffreys, 1919), comparing the marginal likelihoods
between models (Li, πi) and (Lj, πj) by the ratio
Bij ¼
Z
Θi
LiðθijxÞπiðθiÞ dθi
Z
Θj
LjðθjjxÞπjðθjÞ dθj
(1)
which amounts to selecting among models the model with the highest
marginal. This is also advocated as an even weighting of both models in
Jeffreys (1939), but I see little justification for this choice, especially when
considering multiple model selection with possibly embedded models.
The coexistence of both notions—posterior probability versus Bayes
factor—exhibits a tension between using (i) posterior probabilities as justified
by binary loss functions but depending on subjective prior weights that prove
difficult to specify in most settings, especially when comparing many hypoth-
eses, and (ii) Bayes factors that eliminate this dependence but escape a direct
connection with a posterior distribution, unless the prior weights are themselves
integrated within the loss function. A further difficulty attached to the Bayes
factors is that they face a delicate interpretation (or calibration) of the strength
of their support of a given hypothesis or model, even when shown to be
50 Shades of Bayesian testing of hypotheses Chapter
5 105

consistent in the sample size (Berger et al., 2003; Dass and Lee, 2006), as illu-
strated by Fig. 1. That is, under fairly generic conditions on the priors, the
Bayes factor Bij will diverge to infinity or to zero when the sampling model
is Mi or Mj, respectively, as the sample size grows to infinity (Chib and
Kuffner, 2016; Johnson, 2008; Moreno et al., 2010; O’Hagan, 1997). The dif-
ferentiation between the simulated values of the Bayes factors under the null
model and under the alternative model is getting more pronounced in this figure
as the sample size grows. As functions of the data x, both notions are such
that their calibration and a necessary variability assessment seem to require (fre-
quentist or posterior predictive) simulations under both hypotheses, which
clashes with the Bayesian paradigm. However, posterior probabilities face a
similar difficulty if one wants to avoid interpreting them as p-value substitutes
or probabilities of selecting the “true” model since they only report of respec-
tive strengths of fitting the data x to the models under comparison.
Bayes factors also suffers from several theoretical and practical difficulties.
First, when improper prior distributions are used, Bayes factors contains unde-
fined constants and takes arbitrary values (...) Second, when a proper but vague
prior distribution with a large spread is used to represent prior ignorance, Bayes
FIG. 1
Example of a normal Xn  N ðμ, 1=nÞ model when μ  N ð0, 1Þ under H1 and μ ¼ 0 under
H0, leading to the Bayes factor B10 ¼ ð1 + nÞ1=2 exp fn2x2
n=2ð1 + nÞg. The three panels represent
the variability of log B10 over 250 replicas under the null (left) and the alternative (right)
hypotheses, for three different sample sizes n. In the latter case, the replications are generated
from the prior predictive distribution. This experiment is supporting the statement about the
consistency of the Bayes factor, namely that the true model is selected by the Bayes factor when
the sample size n goes to infinity, since the Bayes factor B10 accumulates at zero under H0 and
diverges under H1.
106
Handbook of Statistics

factors tends to favour the null hypothesis. The problem may persist even when
the sample size is large (...) Third, the calculation of Bayes factors generally
requires the evaluation of marginal likelihoods. In many models, the marginal
likelihoods may be difficult to compute.
Li et al. (2014)
Among other difficulties inherent to the use of the Bayes factor, and as illu-
strated by the above quote, let me mention an impossibility to ascertain
simultaneous misfits to all proposed models (unless a nonparametric alter-
native is added as in Holmes et al., 2015) or to detect outliers, that is a sub-
set of the dataset that does not fit a particular model. Another pressing issue
I will not address here is the challenging numerical computation of marginal
likelihoods in most settings. While numerous proposals have been made
(see, e.g., Chen et al., 2000; Chopin and Robert, 2010; Friel and Pettitt,
2008; Friel and Wyse, 2012; Marin and Robert, 2011; Robert and Casella,
2004; Robert and Marin, 2008), there is no universal solution and new settings
require a careful design of the numerical apparatus producing the evidence.
A last difficulty worth pointing out is the strong dependence of posterior prob-
abilities and Bayes factors on conditioning statistics, which undermines their
validity for model assessment. This issue was exhibited when considering
ABC (approximate Bayesian computation, see Sisson et al., 2018) model choice
as the lack of inter-model sufficiency may drastically alter the value of a Bayes
factor and even its consistency (Didelot et al., 2011; Marin et al., 2014; Robert
et al., 2011).
3
Improper priors united against hypothesis testing
Hypothesis testing sees a glaring discontinuity occur in the valid use of
improper (infinite mass) priors since calling on these is not directly justified
in most testing situations, leading to many alternative and ad hoc solutions,
where data are either used twice or split in artificial ways. This is most
unfortunate in that the remainder of Bayesian analysis accommodates rather
smoothly the extension from proper (that is, true probability) prior distribu-
tions to improper (that is, σ-finite) prior distributions, which is in particular
most helpful in closing the range of Bayesian procedures. The fundamental
reason for the difficulty in the testing context is the necessity to define a
prior distribution for each model under comparison, independently of the
other priors. This means there is no rigorous way of defining a model-by-
model normalization of these priors when they are σ-finite. (Principled con-
structions of reference priors for testing have been investigated, see for
instance Bayarri and Garcı´a-Donato, 2007 and Bayarri and Garcı´a-Donato,
2008. Their proposal is based on symmetrized versions of the Kullback–
Leibler divergence κ between the null and alternative distributions,
50 Shades of Bayesian testing of hypotheses Chapter
5 107

transformed into a prior that looks like an inverse power of 1 + κ, with a
power large enough to make the prior proper.)
A first difficulty with the proposed resolutions of the improper conundrum
stands with the choice (already found in Jeffreys, 1939) of opting for identical
priors on the parameters present in both models (Berger et al., 1998), which
amounts in endowing them with the same meaning. The following argument
of then using the same prior, whether or not proper, then eliminates the need
for a normalizing constant. (Note that the Savage–Dickey approximation of
the marginal likelihood relies on this assumption and operates only under
the alternative hypothesis, see Marin and Robert, 2010; Verdinelli and
Wasserman, 1995.)
A second difficulty is attached to the pseudo-Bayes factors proposed in the
1990s (Berger et al., 1998; O’Hagan, 1995), where a fraction of the data is
used to turn all improper priors into proper posteriors and these posteriors
are used as new “priors” on the remaining fraction. On the one hand this is
a nice bypass of the normalization constant issue and it enjoys consistency
properties as the sample size of the remaining fraction grows to infinity. Fur-
thermore, it does not use the data twice as (Aitkin, 1991, 2010). On the other
hand, being a leave-one-out approach, it does not qualify as a Bayesian proce-
dure proper and the different manners to average over all possible choices of
the normalizing sample lead to different numerical answers, while there exist
cases when this division proves impossible.
Alternative approaches have advocated the use of score functions (Dawid
and Musio, 2015; Gutmann and Hyv€arinen, 2012; Hyv€arinen, 2005; Li et al.,
2014; Shao et al., 2017) to overcome the issue with improper priors. While
quite sympathetic to this perspective, I will not cover this aspect in this
chapter.
While the deviance information criterion (DIC) of Spiegelhalter et al.
(2002, 2014) remains quite popular for model comparison, its uses of the
posterior expectation of the log-likelihood function, meaning that the data
are used twice, as in Aitkin (2010), and of a plug-in term, make it disput-
able, as discussed in Robert (2014a) and prone to conflicting interpretations,
as shown in Celeux et al. (2006) for mixtures of distributions. A related
approach with stronger theoretical backup is the widely applicable informa-
tion criterion (WAIC) of Watanabe (2010, 2018), which is asymptotically
equivalent to the average Bayes generalization and cross validation losses.
The fundamental setting of WAIC is one where both the sampling and the
prior distributions are different from respective “true” distributions. This
requires using a tool toward the assessment of the discrepancy when utiliz-
ing a specific pair of such distributions, especially when the posterior distri-
bution cannot be approximated by a Normal distribution. The WAIC is
supported for the determination of the “true” model, in opposition to AIC
and DIC. In addition, it escapes the “plug-in” sin and is handling mixture
models.
108
Handbook of Statistics

4
The Jeffreys–Lindley paradox
The weight of Lindley’s paradoxical result (...) burdens proponents of the
Bayesian practice.
Lad (2003)
The Lindley paradox (or Jeffreys–Lindley paradox) is due to Lindley (1957)
pointing out an irreconciliable divergence between the classic and Bayesian
procedures when testing a point null hypothesis. (This property was briefly
mentioned in Jeffreys, 1939, V, §5.2, although in their scholarly review of
the paradox, Wagenmakers and Ly, 2021 stress that it plays a central role in
his construction of Bayesian hypothesis testing.) The paradox is that, regard-
less of the prior choice, the Bayes factor against the null hypothesis converges
to zero with the sample size when the associated p-value remains constant.
For instance, when testing the nullity of a Normal mean,
xn  N ðθ, σ2=nÞ,
H0 : θ ¼ θ0,
with the following H1 prior, θ  N ðθ0, σ2Þ, the Bayes factor is
B01ðtnÞ ¼ ð1 + nÞ1=2 exp nt2
n=2½1 + n


,
where tn ¼
ﬃﬃﬃn
p jxn  θ0j=σ. When setting tn to a fixed value, it converges to
infinity with n.
The Jeffreys–Lindley paradox exposes a rift between Bayesian and frequentist
hypothesis testing that strikes at the heart of statistical inference.
Wagenmakers and Ly (2021)
Since the apparent paradox of “always” accepting the null hypothesis can be
reformulated in terms of a prior variance going to infinity, it also relates with
the difficulty in using Bayes factors and improper priors, in that the posterior
mass of the region with nonnegligible likelihood goes to zero as the variance
increases (Robert, 2014c). This is thus a coherent framework in that the only
remaining item of information is that the null hypothesis could be true! (Note
also that the paradox can be circumvoluted by replacing the point null hypoth-
esis with an interval substitute in order for a single proper or improper prior to
be used, see also Robert, 1993.)
The opposition between frequentist and Bayesian procedures is not a sur-
prise either. The former relies solely on the point-null hypothesis H0 and the
corresponding sampling distribution, while the latter opposes H0 to a (predic-
tive) marginal version of H1. Furthermore, the fact that the statistic tn remains
constant (or equivalently that the Type I error remains constant) is incorrect.
The rejection bound cannot be a constant multiple of the standard error as
the sample size n increases, as demonstrated by Jeffreys (1939) and discussed
in details by Wagenmakers and Ly (2021). Let me conclude by mentioning
50 Shades of Bayesian testing of hypotheses Chapter
5 109

the case for specific priors isolating the null from the alternative hypotheses,
as in Consonni and Veronese (1987), Johnson and Rossell (2010), and
Consonni et al. (2013).
5
Posterior predictive p-values
Once a Bayes factor B01 is computed, one need assess its strength in support-
ing one of the hypotheses, if any. In my opinion, the much vaunted Jeffreys’
1939 scale has very little validation as it is absolute (i.e., with no dependence
on the model, the sample size, the prior). Following earlier proposals in the
literature (Box and Tiao, 1992; Garcı´a-Donato and Chen, 2005; Geweke and
Amisano, 2008), an evaluation of this strength within the issue at stake, that
is, the comparison of two hypotheses (or models), can be based on the predic-
tive distributions. That is, the likelihood of observing B01ðxobsÞ, the observed
dataset, is evaluated under these distributions
pred
0
ðB01ðXpredÞ  B01ðxobsÞjxobsÞ
and
pred
1
ðB01ðXpredÞ  B01ðxobsÞjxobsÞ
the probabilities being computed under models M0 and M1, respectively.b
While most authors (like Garcı´a-Donato and Chen, 2005) consider this
should be the prior predictive distribution, I agree with Gelman et al.
(2013) that using the push-forward image by B01ð  Þ of the posterior predic-
tive distribution
πðxrepjxobsÞ ¼
Z
pðxrepjθÞπðθjxobsÞ dθ
(2)
(where xobs denotes the observed dataset and xrep an artificial replication or a
running variate) is more relevant. Indeed, by exploiting the information
contained in the data (through the posterior), (2) concentrates on a region of
relevance in the parameter space(s), which is especially interesting in weakly
informative settings, despite “using the likelihood twice.”c Furthermore, (2)
evaluate the behavior of the Bayes factor for values of x that are similar to
the original observation, provided the posterior predictive fits the data well
enough. Note also that, under this approach, issues of indeterminacy linked
with improper priors are not evacuated, since the Bayes factor remains
bUsing a single encompassing predictive is possible, but this distribution depends on the usually
arbitrary prior probabilities of both models.
cThis double use can possibly be argued for or against, once a data-dependent loss function is
built, but the potential for over-fitting must be investigated on its own, globally or model by
model. However, the above probabilities can also be perceived as producing an estimator of the
posterior loss.
110
Handbook of Statistics

indeterminate, even with a well-defined predictive. Note further that, even
though probabilities of errors of type I and errors of type II can be computed,
they fail to account for the posterior probabilities of both models. (This is a
potentially delicate issue with the solution of Garcı´a-Donato and Chen,
2005.) A nice feature is that the predictive distribution of the Bayes factor
can be computed even in complex settings when ABC (Sisson et al., 2018)
need be used.
If the model fits, then replicated data generated under the model should look
similar to the observed data. The observed data should look plausible under
the posterior predictive distribution. This is really a self-consistency check: an
observed discrepancy can be due to model misfit or chance.
Gelman et al. (2013)
In Bayesian Data Analysis (2013, Chapter 6), based on the choice of a mea-
sure of discrepancy T(, ), the authors (strongly) suggest replacing the classi-
cal p-value
pðxobsjθÞ ¼ ðTðXrep, θÞ  Tðxobs, θÞjθÞ
with a Bayesian alternative (or Bayesian posterior p-value)
ðTðXrep, θÞ  Tðxobs, θÞjxobsÞ:
Extremes p-values indicate a poor fit of the model, with the usual caution
applying about setting golden bounds such as 0.05 or 0.99. There are however
issues with the implementation of this approach, from deciding on which
aspect of the data or of the model is to be examined, that is, the choice of
the discrepancy measure T, to its calibration.
6
A modest proposal
Given this rather pessimistic perspective on Bayesian testing, one may won-
der at the overall message of this chapter besides the not-particularly useful
“it is complicated.” Given the difficulty in moderating the impact of the
prior modeling in ways more useful than a sheer assurance of consistency,
my preference leans toward a proposal that feels more estimation-based than
testing-based, and that is leaning more toward quantification than toward
binary decision. In Kamary et al. (2014), we have sketched the basis of a
novel approach that advocates the replacement of the posterior probability
of a model or of an hypothesis with the posterior distribution of the weights
of a mixture of the models under comparison. That is, given two Bayesian
models under comparison,
M1 : x  f1ðxjθ1Þ, θ1  π1ðθ1Þ versus M2 : x  f2ðxjθ2Þ, θ2  π2ðθ2Þ
we propose to estimate the (artificial) mixture model
Mα : x  αf1ðxjθ1Þ+ð1  αÞf2ðxjθ2Þ
0  α  1
(3)
50 Shades of Bayesian testing of hypotheses Chapter
5 111

and in particular to derive the posterior distribution of α. This (marginal)
posterior can then be exploited to assess the better fit and if need be to
achieve a decision, either by assessing tail probabilities that α is close
to 0 or 1, or by calibrating a bound from the prior or posterior predictives.
In most settings, this approach can indeed be easily calibrated by a para-
metric bootstrap experiment providing a posterior distribution of α under
each of the models under comparison. The prior predictive error can there-
fore be directly estimated and drive the choice of a decision cut-off on the
tails of α, if need be.
Consider for instance a simple example in Kamary et al. (2014) where
f1(jθ1) corresponds to a Poisson Pðθ1Þ distribution and f2(jθ1) to a Geometric
Geoðθ2Þ failure distribution, both parameterized in terms of their mean.
A mixture
αPðλÞ + ð1  αÞGeoð1=1 + λÞ
can then be proposed, with the same parameter λ > 0 behind both compo-
nents. Further, Kamary et al. (2014) show that the noninformative prior π(λ)
¼ 1/λ can be used in this setting, whatever the sample size n. Fig. 2 demon-
strates the concentration of the posterior on α around 1 as n increases when
the data generating distribution is a PðλÞ distribution.
In this example, the exact (standard) Bayes factor comparing the Poisson
to the Geometric models is given by
B12 ¼ nnxnY
n
i¼1
xi! Γ n + 2 + nxn
ð
Þ=Γðn + 2Þ:
when using the same improper prior on the parameter λ (Berger et al., 1998;
DeGroot, 1973, 1982). The posterior probability of the Poisson model is then
derived as
ðM1jxÞ ¼
B12
1 + B12
when adopting (without much of a justification) identical prior weights on
both models. Fig. 3 compares the concentration of the posterior probability
and of the posterior median on α around 1 as n increases.
One may object that the mixture model (3) is neither one nor the other of
the models under comparison, but it includes both of these at the boundaries,
that is, when α ¼ 0, 1. Thus, if we use a prior distribution on α that favors
neighborhoods of 0 and 1, albeit avoiding atoms at these values, we should
be able to witness the posterior concentrate near 0 or 1, depending on which
model is true (or fits the data better). This is indeed the case: as shown in
Kamary et al. (2014), for any given Beta prior on α, we observe a higher and
higher concentration at the right boundary as the sample size n increases, as
illustrated by Fig. 2.
112
Handbook of Statistics

0
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
a0=.1
log(sample size)
0
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
a0=.3
log(sample size)
0
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
a0=.5
log(sample size)
FIG. 2
Posterior means (sky-blue) and medians (gray-dotted) of the posterior distributions on α, displayed over 100 simulations of Poisson Pð4Þ datasets for
sample sizes n between 1 and 1000. The shaded and dotted areas indicate the range of the estimates. Each plot corresponds to a Beða0, a0Þ prior on α and each
posterior approximation is based on 104 MCMC iterations. This experiment is supporting the statement about the concentration of the posterior on the mixture
weight near the upper limit 1, in agreement with the correct model. Reproduced with permission from Kamary, K., Mengersen, K. L., Robert, C. P.,
Rousseau, J., 2014. Testing hypotheses as a mixture estimation model. arxiv:1214.4436.

In our opinion, this (novel) mixture approach offers numerous advan-
tages. First, achieving a decision while relying on a Bayesian estimator of
the weight α or on its posterior distribution, rather than on the posterior
probability of the associated model lifts the embarrassing need of specifying
almost invariably artificial prior probabilities on the models, as discussed
above. This is also a most Bayesian choice, replacing an unknown quantity
with a probability distribution. It is however not addressed by a classical
Bayesian approach, even though those probabilities linearly impact the pos-
terior probabilities and their indeterminacy is often brought forward to pro-
mote the alternative of using instead the Bayes factor. In the mixture
estimation setting, prior modeling only involves selecting a prior on α, for
instance a Beta Bða, aÞ distribution, with a wide range of acceptable values
for the hyperparameter a. While its value obviously impacts the posterior
distribution of α, it can be argued that it still induces an accumulation of
the posterior mass near 1 or 0, that is, favors the most favorable or the true
model over the other one, and a subsequent sensitivity analysis on the
impact of a is elementary to implement
0
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
log(sample size)
FIG. 3
Comparison between the ranges of ðM1jxÞ (red dotted area) and of the posterior med-
ians of α for 100 simulations of Poisson Pð4Þ datasets with sample sizes n ranging from 1 to 103.
Reproduced with permission from Kamary, K., Mengersen, K. L., Robert, C. P., Rousseau, J.,
2014. Testing hypotheses as a mixture estimation model. arxiv:1214.4436.
114
Handbook of Statistics

The interpretation of the estimator of α is furthermore at least as “natural”
as handling the corresponding posterior probability, while avoiding the rudi-
mentary zero–one loss function. The quantity α and its posterior distribution
provide a Bayesian measure of proximity to either model for the data at hand,
which is a most reasonable measure of fit, while being also interpretable as a
propensity of the data to stand with (or to stem from) these models. This rep-
resentation further allows for alternative perspectives on testing and model
choices, through the notions of predictive tools, cross-validation (Gelman
et al., 2013; Vehtari and Lampinen, 2002; Vehtari and Ojanen, 2012), and
information indices like WAIC (Watanabe, 2010, 2018). They may further
relate to sparsity priors like the horseshoe priors (Carvalho et al., 2009) in
Bayesian variable selection, which is examined in Kamary et al. (2014).
From a computational perspective, the highly challenging computation of
the marginal likelihoods is absent from this approach, since standard algo-
rithms are available for Bayesian mixture estimation (Celeux et al., 2018).
In addition, the extension of this perspective to a finite collection of models
under comparison is immediate, as this modeling simply expands the mixture
into a larger number of components. This approach further allows to involve
all models at once rather than engaging in many pairwise comparisons, thus
eliminating the least likely models by simulation. This is much more efficient
than in the alternative reversible jump strategies (Green, 1995). Note as a side
remark that the (conceptually and computationally) challenging issue of “label
switching” ( Jasra et al., 2005; Stephens, 2000) attached with most mixture
models does not appear in this particular context, since mixture components
are then not exchangeable. In particular, the mixture representation involves
neither a Bayes factor nor a posterior probability and hence it bypasses the
difficulty of exploring all modes of the posterior distribution. Thus, this per-
spective is solely focused on estimating the parameters of a mixture model
where all components are identifiable.
From an inferential perspective, we deem that the posterior distribution of
α evaluates more thoroughly the strength of the support for a given model
than the single-figure outcome of a Bayes factor derivation. The valuable
variability of the posterior distribution on α allows for a more thorough
assessment of the strength of the data-support of one model against the other.
In a related manner, an additional and crucial feature missing from more tra-
ditional Bayesian answers is that a mixture model also acknowledges the sig-
nificant possibility that, for a finite dataset, both models or none could be
acceptable. Also significantly, while standard (proper and informative) prior
modeling can be painlessly reproduced in this mixture setting, we stress that
some (if not all) improper priors can be managed via this approach, provided
both models under comparison are first reparametrized toward common
meaning and cross-model common parameters, as for instance with location
50 Shades of Bayesian testing of hypotheses Chapter
5 115

or/and scale parameters. In the special case when all parameters can be made
common to both modelsd the mixture model reads as
Mα : x  αf1ðxjθÞ + ð1  αÞf2ðxjθÞ:
For instance, if θ is a location parameter, a flat prior can be used with no foun-
dational difficulty, in opposition to the testing case. Following this line of
argument, we feel that using the same parameters or some identical para-
meters on both components is an essential feature of this reformulation of
Bayesian testing, as it highlights the fact that the opposition between both
components of the mixture is not an issue of enjoying different parameters,
but quite the opposite: as further stressed below, this or even those common
parameter(s) is (are) nuisance parameters that need be integrated out (as they
also are in the traditional Bayesian approach through the computation of the
marginal likelihoods).
7
Conclusion
This coverage of the different options for conducting Bayesian testing and
Bayesian model choices is obviously subjective and other authors Gelman
(2018), Held and Ott (2018), Magnusson et al. (2020), Wagenmakers et al.
(2018), and van de Schoot et al. (2021) in the field differ in their assessment
of how this aspect of Bayesian inference should be conducted (or prohibited).
However, my exposition hopefully reflects on the complexity of the task at
hand and on the necessity to avoid ready-made solutions as those based on
binary losses. Whatever the perspective adopted, it should always be cali-
brated through simulated synthetic data from the different models under
comparison.
Acknowledgments
The work of the author was partly supported in part by the French Government under man-
agement of Agence Nationale de la Recherche as part of the “Blanc SIMI 1” program,
reference ANR-18-CE40-0034 and in part by the French Government under management
of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program,
reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute). The author also gratefully acknowl-
edges the support of l’Institut Universitaire de France through two consecutive senior chairs.
dWhile this may sound like an extremely restrictive requirement in a traditional mixture model, let
us stress here that the presence of common parameters becomes quite natural within a testing
setting. To wit, when comparing two different models for the same data, moments are defined
in terms of the observed data and hence should enjoy the same meaning across models. Repara-
metrizing the models in terms of those common-meaning moments does lead to a mixture model
with some and maybe all common parameters. We thus advise the use of a common parametriza-
tion, whenever possible.
116
Handbook of Statistics

Abbreviations
MCMC
Markov chain Monte Carlo
ABC
approximate Bayesian computation
References
Aitkin, M., 1991. Posterior Bayes factors (with discussion). J. R. Stat. Soc. B 53, 111–142.
Aitkin, M., 2010. Statistical Inference: A Bayesian/Likelihood approach. CRC Press/Chapman &
Hall, New York.
Bayarri, M.J., Garcı´a-Donato, G., 2007. Extending conventional priors for testing general hypoth-
eses in linear models. Biometrika 94, 135–152. https://doi.org/10.1093/biomet/asm014.
Bayarri, M.J., Garcı´a-Donato, G., 2008. Generalization of Jeffreys divergence-based priors for
Bayesian hypothesis testing. J. R. Stat. Soc. B 70 (5), 981–1003. https://doi.org/10.1111/
j.1467-9868.2008.00667.x.
Benjamin, D.J., Berger, J.O., Johannesson, M., et al., 2018. Redefine statistical significance. Nat.
Hum. Behav. 2, 6–10.
Berger, J.O., 1985. Statistical Decision Theory and Bayesian Analysis, second ed. Springer-
Verlag, New York.
Berger, J.O., Sellke, T., 1987. Testing a point-null hypothesis: the irreconcilability of significance
levels and evidence (with discussion). J. Am. Stat. Assoc. 82, 112–122.
Berger, J.O., Pericchi, L.R., Varshavsky, J., 1998. Bayes factors and marginal distributions in
invariant situations. Sankhya A 60, 307–321.
Berger, J.O., Ghosh, J.K., Mukhopadhyay, N., 2003. Approximations and consistency of Bayes
factors as model dimension grows. J. Stat. Plan. Inference 112 (1-2), 241–258.
Box, G.E.P., Tiao, G.C., 1992. Bayesian Inference in Statistical Analysis. Wiley, New York.
Carvalho, C.M., Polson, N.S., Scott, J.G., 2009. Handling sparsity via the horseshoe. J. Mach.
Learn. Res. AISTATS, W&CP 5.
Casella, G., Berger, R., 1987. Reconciling Bayesian and frequentist evidence in the one-sided test-
ing problem. J. Am. Stat. Assoc. 82, 106–111.
Celeux, G., Forbes, F., Robert, C., Titterington, D.M., 2006. Deviance information criteria for
missing data models (with discussion). Bayesian Anal. 1 (4), 651–674.
Celeux, G., Fr€uhwirth-Schnatter, S., Robert, C., 2018. Model selection for mixture models-
perspectives and strategies. In: Handbook of Mixture Analysis, December, CRC Press.
https://hal.archives-ouvertes.fr/hal-01961077.
Chen, M.H., Shao, Q.M., Ibrahim, J.G., 2000. Monte Carlo Methods in Bayesian Computation.
Springer-Verlag, New York.
Chib, S., Kuffner, T.A., 2016. Bayes factor consistency. arXiv:1607.00292. https://doi.org/
10.48550/ARXIV.1607.00292.
Chopin, N., Robert, C.P., 2010. Properties of nested sampling. Biometrika 97, 741–755.
Consonni, G., Veronese, P., 1987. Coherent distributions and Lindley’s paradox. In: Viertl, R.
(Ed.), Probability and Bayesian Statistics. Plenum, pp. 111–120.
Consonni, G., Forster, J.J., Rocca, L.L., 2013. The whetstone and the alum block: balanced objec-
tive Bayesian comparison of nested models for discrete data. Stat. Sci. 28 (3), 398–423.
https://doi.org/10.1214/13-STS433.
Dass, S.C., Lee, J., 2006. A note on the consistency of Bayes factors for testing point null versus
non-parametric alternatives. J. Stat. Plan. Inference 119, 143–152.
50 Shades of Bayesian testing of hypotheses Chapter
5 117

Dawid, A.P., Musio, M., 2015. Bayesian model selection based on proper scoring rules. Bayesian
Anal. 10 (2), 479–499. https://doi.org/10.1214/15-BA942.
DeGroot, M.H., 1973. Doing what comes naturally: interpreting a tail area as a posterior probabil-
ity or as a likelihood ratio. J. Am. Stat. Assoc. 68, 966–969.
DeGroot, M.H., 1982. Discussion of Shafer’s ‘Lindley’s paradox’. J. Am. Stat. Assoc. 378,
337–339.
Didelot, X., Everitt, R., Johansen, A., Lawson, D., 2011. Likelihood-free estimation of model evi-
dence. Bayesian Anal. 6, 48–76.
Friel, N., Pettitt, A.N., 2008. Marginal likelihood estimation via power posteriors. J. R. Stat. Soc.
B 70 (3), 589–607.
Friel, N., Wyse, J., 2012. Estimating the evidence: a review. Stat. Neerl. 66 (3), 288–308.
Garcı´a-Donato, G., Chen, M.-H., 2005. Calibrating Bayes factor under prior predictive distribu-
tions. Stat. Sin. 15, 359–380.
Gelman, A., 2018. The failure of null hypothesis significance testing when studying incremental
changes, and what to do about it. Pers. Soc. Psychol. Bull. 44 (1), 16–23. https://doi.org/
10.1177/0146167217729162.
Gelman, A., Robert, C.P., 2014. Revised evidence for statistical standards. Proc. Natl. Acad. Sci.
U.S.A 111 (19), E1933.
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.S., Vehtari, A., Rubin, D.B., 2013. Bayesian
Data Analysis, third ed. Chapman and Hall, New York.
Gelman, A., Hwang, J., Vehtari, A., 2014. Understanding predictive information criteria for
Bayesian models. Stat. Comput. 24 (6), 997–1016.
Geweke, J., Amisano, G., 2008. Optimal prediction pools. J. Econom. 164, 130–141. https://doi.
org/10.1016/j.jeconom.2011.02.017.
Green, P.J., 1995. Reversible jump MCMC computation and Bayesian model determination. Bio-
metrika 82 (4), 711–732.
Gutmann, M.U., Hyv€arinen, A., 2012. Noise-contrastive estimation of unnormalized statistical
models, with applications to natural image statistics. J. Mach. Learn. Res. 13 (1), 307–361.
Haldane, J.B.S., 1932. A note on inverse probability. Proc. Cambridge Philos. Soc. 28, 55–61.
Held, L., Ott, M., 2018. On p-values and Bayes factors. Annu. Rev. Stat. Appl. 5, 393–419.
Holmes, C.C., Caron, F., Griffin, J.E., Stephens, D.A., 2015. Two-sample Bayesian nonparametric
hypothesis testing. Bayesian Anal. 10 (2), 297–320. https://doi.org/10.1214/14-BA914.
Hyv€arinen, A., 2005. Estimation of non-normalized statistical models by score matching. J. Mach.
Learn. Res. 6, 695–709.
Jasra, A., Holmes, C.C., Stephens, D.A., 2005. Markov Chain Monte Carlo methods and the label
switching problem in Bayesian mixture modeling. Stat. Sci. 20 (1), 50–67.
Jeffreys, H., 1939. Theory of Probability, first ed. The Clarendon Press, Oxford.
Johnson, V.E., 2008. Properties of Bayes factors based on test statistics. Scand. J. Stat. 35 (2),
354–368. https://doi.org/10.1111/j.1467-9469.2007.00576.x.
Johnson, V.E., 2013. Revised standards for statistical evidence. Proc. Natl. Acad. Sci. U.S.A 110,
19313–19317. https://doi.org/10.1073/pnas.1313476110. http://www.pnas.org/content/early/
2013/10/28/1313476110.abstract.
Johnson, V.E., Rossell, D., 2010. On the use of non-local prior densities in Bayesian hypothesis
tests. J. R. Stat. Soc. B 72, 143–170.
Kamary, K., Mengersen, K.L., Robert, C.P., Rousseau, J., 2014. Testing hypotheses as a mixture
estimation model. arxiv:1214.4436.
Lad, F., 2003. Appendix: the Jeffreys-Lindley paradox and its relevance to statistical testing. In:
Conference on Science and Democracy, Palazzo Serra di Cassano, Napoli.
118
Handbook of Statistics

Lehmann, E.L., 1986. Testing Statistical Hypotheses. John Wiley, New York.
Li, Y., Zeng, T., Yu, J., 2014. A new approach to Bayesian hypothesis testing. J. Econom. 178 (3),
602–612.
Lindley, D.V., 1957. A statistical paradox. Biometrika 44, 187–192.
Ly, A., Verhagen, J., Wagenmakers, E.-J., 2016. Harold Jeffreys’s default Bayes factor hypothesis
tests: explanation, extension, and application in psychology. J. Math. Psychol. 72, 19–32.
Magnusson, M., Vehtari, A., Jonasson, J., Andersen, M., 2020. Leave-one-out cross-validation for
Bayesian model comparison in large data. In: International Conference on Artificial Intelli-
gence and Statistics, PMLR, pp. 341–351.
Marin, J.-M., Robert, C.P., 2010. On resolving the Savage-Dickey paradox. Electron. J. Stat. 4,
643–654. https://doi.org/10.1214/10-EJS564.
Marin, J.-M., Robert, C.P., 2011. Importance sampling methods for Bayesian discrimination
between embedded models. In: Chen, M.-H., Dey, D.K., M€uller, P., Sun, D., Ye, K. (Eds.),
Frontiers of Statistical Decision Making and Bayesian Analysis. Springer, pp. 513–527.
Marin, J.-M., Pillai, N., Robert, C.P., Rousseau, J., 2014. Relevant statistics for Bayesian model
choice. J. R. Stat. Soc. B 76 (5), 833–859.
McShane, B.B., Gal, D., Gelman, A., Robert, C., Tackett, J.L., 2019. Abandon statistical signifi-
cance. Am. Stat. 73, 235–245.
Moreno, E., Giro´n, F.J., Casella, G., 2010. Consistency of objective Bayes factors as the model
dimension grows. Ann. Stat. 38 (4), 1937–1952.
O’Hagan, A., 1995. Fractional Bayes factors for model comparisons. J. R. Stat. Soc. B 57,
99–138.
O’Hagan, A., 1997. Properties of intrinsic and fractional Bayes factors. Test 6, 101–118.
Plummer, M., 2008. Penalized loss functions for Bayesian model comparison. Biostatistics 9 (3),
523–539. https://doi.org/10.1093/biostatistics/kxm049.
Robert, C., 1993. A note on Jeffreys-Lindley paradox. Stat. Sin. 3 (2), 601–608.
Robert, C.P., 2014a. Discussion of “the deviance information criterion: 12 years on”. J. R. Stat.
Soc. B 76 (3), 492–493.
Robert, C.P., 2014b. On the Jeffreys-Lindley paradox. Philos. Sci. 5 (2), 216–232.
Robert, C.P., 2014c. On the Jeffreys-Lindley paradox. Philos. Sci. 81 (2), 216–232. https://doi.org/
10.1086/675729.
Robert, C.P., Casella, G., 2004. Monte Carlo Statistical Methods, second. Springer-Verlag, New
York.
Robert, C.P., Marin, J.-M., 2008. On some difficulties with a posterior probability approximation
technique. Bayesian Anal. 3 (2), 427–442.
Robert, C.P., Cornuet, J.-M., Marin, J.-M., Pillai, N., 2011. Lack of confidence in ABC model
choice. Proc. Natl. Acad. Sci. U.S.A 108(37), 15112–15117.
Schwartz, L., 1965. On Bayes procedures. Z. Warsch. Verw. Gebiete 4, 10–26.
Schwarz, G., 1978. Estimating the dimension of a model. Ann. Stat. 6, 461–464.
Shao, S., Jacob, P., Ding, J., Tarokh, V., 2017. Bayesian model comparison with the Hyv€arinen
score: computation and consistency. J. Am. Stat. Assoc. 114, 1826–1837. https://doi.org/
10.1080/01621459.2018.1518237.
Sisson, S., Fan, Y., Beaumont, M., 2018. Handbook of Approximate Bayesian Computation.
Chapman and Hall/CRC, New York, https://doi.org/10.1201/9781315117195.
Spiegelhalter, D.J., Best, N.G., Carlin, B.P., Van der Linde, A., 2002. Bayesian measures of
model complexity and fit. J. R. Stat. Soc. B 64, 583–640.
Spiegelhalter, D.J., Best, N.G., Carlin, B.P., van der Linde, A., 2014. The deviance information
criterion: 12 years on. J. R. Stat. Soc. B 76 (3), 485–493.
50 Shades of Bayesian testing of hypotheses Chapter
5 119

Stephens, M., 2000. Dealing with label switching in mixture models. J. R. Stat. Soc. B 62 (4),
795–809.
van de Schoot, R., Depaoli, S., King, R., Kramer, B., M€artens, K., Tadesse, M.G., Vannucci, M.,
Gelman, A., Veen, D., Willemsen, J., et al., 2021. Bayesian statistics and modelling. Nat.
Rev. Methods Primers 1 (1), 1–26.
Vehtari, A., Lampinen, J., 2002. Bayesian model assessment and comparison using crossvalida-
tion predictive densities. Neural Comput. 14, 2439–2468.
Vehtari, A., Ojanen, J., 2012. A survey of Bayesian predictive methods for model assessment,
selection and comparison. Stat. Surv. 6, 142–228.
Verdinelli, I., Wasserman, L., 1995. Computing Bayes factors using a generalization of the
Savage-Dickey density ratio. J. Am. Stat. Assoc. 90, 614–618.
Wagenmakers, E.-J., Ly, A., 2021. History and nature of the Jeffreys-Lindley paradox. arXiv
e-prints arXiv:2111.10191.
Wagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., Selker, R.,
Gronau, Q.F., Sˇmı´ra, M., Epskamp, S., et al., 2018. Bayesian inference for psychology. Part
I: theoretical advantages and practical ramifications. Psychon. Bull. Rev. 25 (1), 35–57.
Wasserstein, R.L., Lazar, N.A., 2016. The ASA statement on p-values: context, process, and pur-
pose. Am. Stat. 70 (2), 129–133. https://doi.org/10.1080/00031305.2016.1154108.
Watanabe, S., 2010. Asymptotic equivalence of Bayes cross validation and widely applicable
information criterion in singular learning theory. J. Mach. Learn. Res. 11 (116), 3571–3594.
Watanabe, S., 2018. Mathematical Theory of Bayesian Statistics. Chapman and Hall/CRC Press.
Wrinch, D., Jeffreys, H., 1919. On some aspects of the theory of probability. Philos. Mag. 38,
715–731.
120
Handbook of Statistics

Chapter 6
Inference approach to ground
states of quantum systems
Angelo Plastinoa,∗and A.R. Plastinob
aInstituto de Fı´sica La Plata–CCT-CONICET, Universidad Nacional de La Plata,
La Plata, Argentina
bCeBio-Departamento de Ciencias Ba´sicas, Universidad Nacional del Noroeste de la Prov. de
Buenos Aires (UNNOBA), CONICET, Junin, Argentina
∗Corresponding author: e-mail: angeloplastino@gmail.com
Abstract
We discuss how to infer ground-state wave functions using a special version of Jaynes’
maximum entropy principle.
Keywords: Wave functions, Pure states, Inference approach
1
Introduction
This chapter deals with the application of information theory notions (Balian,
1991; Brillouin, 1956; Canosa et al., 1989; Jaynes, 1957; Katz, 1967; Plastino
and Plastino, 1993; Shannon, 1948) to the description of pure quantum states,
with special focus on ground states. We shall consider concepts that were first
introduced within the context of statistical mechanics. Recall that statistical
mechanics basic idea is that of using a small set of important expectation
values and infer from them a way of describing interesting features of the
physical system under consideration. An elegant road for undertaking such
an enterprise was that first traversed by Jaynes (Katz, 1967) that is today
called the maximum entropy methodology (MEM) to statistical mechanics.
The MEM supposedly yields the least biased system’s representation compat-
ible with the extant data (Katz, 1967). Of course, as befits statistical mechan-
ics, this optimal representation is given by a statistical operator. In fact, since
pioneering work of von Neumann, entropies in quantum physics are usually
associated with statistical operators. It is possible, however, to link in a mean-
ingful fashion entropy to a pure quantum state. The modulus squared of the
wave function (expressed in a given basis) is normalized to unity and can
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.07.002
Copyright © 2022 Elsevier B.V. All rights reserved.
121

be formally regarded as a probability distribution (PD) that is endowed
with its own appropriate entropy. Such entropies are indeed very useful and
have been applied, for example, to the formulation of entropic uncertainty
relations. In this chapter we wish to discuss a different kind of application:
the inference of ground-state wave functions. We wish to appeal a technique
reminiscent of the maximum entropy approach (Canosa et al., 1989; Plastino
and Plastino, 1993).
It is worth to emphasize that the present chapter deals only with applica-
tions of the maximum entropy principle to the inference of quantum-
mechanical pure states, or to the development of methods for solving the
Schrodinger equation (in an exact, or in an approximate way, depending on
the problem at hand) by recourse to a maxent inspired ansatz. We shall not
discuss other kinds of applications of entropic measures to the study of quan-
tum pure states, such as their use as quantitative indicators of the amount of
delocalization (“spread”) of the eigenfunctions of quantum systems. For a
recent, representative example of this last line of enquiry, see Toranzo and
Dehesa (2019).
2
The Jaynes’ maximum entropy methodology: Brief resume
The MEM aspires to find the least biased PD P and contends that this P is the
one providing the best possible distribution of our system S, that is supposed
to exist in N different microscopic states labeled by the index i (Katz, 1967).
Jaynes assumes that our prior available information is that of M empirical
expectation values of quantities Ak, i.e., hAki. His recipe to determine P is to
obtain that distribution that maximizes its associated entropy SJ
SJ ¼ 
X
N
i
Pi ln Pi,
(1)
subject to the M known mean values hAki. This is done using M unknown
Lagrange multipliers ak. The ensuing variational problem yields (Katz, 1967)
Pi ¼ Z1 exp

X
M
k¼1
akhAkðiÞi
 
!
,
(2)
Z ¼
X
N
i¼1
exp

X
M
k¼1
akhAkðiÞi
 
!
:
(3)
The Lagrange multipliers are determined via
ak ¼  ∂ln Z
∂hAki ,
(4)
with the expectation values expressed in terms of the Pi
122
Handbook of Statistics

hAki ¼
X
N
i¼1
PiAkðiÞ,
(5)
so that (4) becomes a non linear system of equations. The scheme described
above is today the standard way of proceeding in applying statistical mechan-
ics to variegated scenarios.
We wish now to see how the scheme can be adapted to finding a ground-
state quantum scenario.
3
The quantum maximum entropy approach
Most quantum problems pose the difficulty of confronting the practitioner
with Hilbert spaces of huge dimension which precludes the possibility of
devising an exact wave-function description. This fact forces the necessity
of appealing to approximate ground-state wave functions (GSAWF). Indeed,
devising, designing, and verifying the reliability of different GSAWF consti-
tute a large portion of the quantum practitioner daily task, consuming a lot
of time. In this chapter we examine possible GSAWF families to be treated
‘a la Jaynes.
3.1
Preliminaries
The basic idea, originated in Canosa et al. (1989), is to replace the maximum
entropy approach, i.e., the Jaynes’ entropy SJ, by a new construct that might
share basis entropic properties (Plastino and Plastino, 1993). This construct
was called a “quantum entropy” SQ (Canosa et al., 1989; Plastino and
Plastino, 1993). Let us see how to proceed in order to get this SQ.
In a given orthogonal, finite Hilbert-basis jji, j ¼ 1,…, N, any ground-
state wave function ψ can be written in the fashion
ψ ¼
X
N
j¼1
Cjjji, ðCj a complex scalarÞ,
(6)
X
N
j¼1
jCjj2 ¼ 1:
(7)
Note that the true statistical operator here is ρ ¼ jψihψj. For building SQ,
we will use the trace of the diagonal part ρd of ρ (Plastino and Plastino, 1993).
Let us presuppose again foreknowledge of M expectation values bk of M
Hermitian, commuting operators Ak, that is,
hAki ¼ hψjAkjψi ¼ bk,
(8)
with k ¼ 1,…, M (these will be our “constraints”).
Inference approach to ground states of quantum systems Chapter
6 123

We shall also assume that the set of M mean values is incomplete so that
they do not suffice to uniquely determine ρd. We wish to be in a position to
predict the mean value of any diagonal observable. This includes ground
states and cases where the phase of the coefficients Cj is known a priori
(for instance, those “coherent” cases where all Cj possess the same phase).
Hence, predictions of expectations values of any observable could be made.
The essential idea is to assume that one can express the modulus squared
of Cj in a fashion that mimics (2) for probabilities
jCjj2 ¼ Z1 exp

X
M
k¼1
akhjjAkjji
 
!
,
(9)
Z ¼
X
j
exp

X
M
k¼1
akhjjAkjji
 
!
:
(10)
This entails that there exists an underlying quantum entropy SQ of the diagonal
form ρD (it takes into account only the diagonal elements of the “true”
statistical operator jψihψj)
TrρD ln ρD ¼ SQ ¼ 
X
j
jCjj2 ln ½jCjj2,
(11)
that has been extremized subject to the M constraints (known values of the)
hAki, using Lagrange multipliers ak. Thus, if we a priori know these values,
we can infer from them, according to what was expounded above, valuable
information regarding the wave function ψ. This methodology is a quantum
adaptation of the MEA based on the “quantum entropy” SQ. At this stage,
two questions arise
l
How valuable is the information about ψ that the quantal MEA provides?
l
What properties does SQ exhibit that justifies calling it an “entropy.”
We can in advance assert that the answer to the first question generates a quite
favorable impression, as we shall see below. We begin next dealing with the
second question.
4
Properties of SQ that make our approximate maximum entropy
approach wave functions reasonable ones
We discuss some of them in what follows. They seem rather convincing ones
indeed.
4.1
SQ is a true Shannon’s ignorance function
Consider a large box that contains N compartments. There is a probability Pi
associated with each of these subdivisions. Let a valuable object lie hidden on
124
Handbook of Statistics

one of them. The probability of finding it there is obviously Pi and Shannon’s
ignorance function I in this scenario is (Shannon, 1948)
I ¼ 
X
i
Pi ln Pi:
(12)
Now revisit our quantum scenario above and assume that our valuable object
is our physical system. It necessarily lies in one “compartment” that we call
an orthogonal basis ket jjji. This is the essential point. The basis elements
are our compartments and there are N of them (finite basis). Then, it is evident
that SQ ¼ I, that is, Shannon’s ignorance while Pj ¼ jCjj2.
The above proposed scheme gives us the least biased (most flat distribu-
tion) consistent with the information at hand. Whether this choice of wave
function is appropriate or not, it can be self-consistently ascertained from
the stability of the physical quantities of interest. If by adding new available
data our former predictions do not change beyond our desired precision, no
new relevant information is gotten. Our information is concentrated only on
the a priori known expectation values and hence, predictions with accept-
able accuracy can be made, as shown in Canosa et al. (1989).
4.2
Subject to the known quantities bk, the maximum value of SQ is
unique
Consider the M-dimensional vectors br and fr ¼ br hAri, with r ¼ 1,…, M.
One Hessian from it, a positive-definite symmetric matrix, can be constructed
in the fashion [Katz (1967), p. 43, Eq. (3.11)]
Krs ¼ ∂br
∂as ¼ hArihAsi:
(13)
A necessary and sufficient condition for the solution (9) to be unique is that
the matrix K be regular (Katz, 1967). A bit of further elaboration [see p. 44,
Eq. (3.19) of Katz (1967)] shows that the entropy SQ displays a unique maxi-
mum. The properties of K guarantee the existence of a potential function F
(Agmon et al., 1979; Plastino and Plastino, 1993)
f ¼ gradFða1,…, aMÞ, with F ¼ ln Z +
X
r
arbr:
(14)
4.3
The entropy SQ obeys an H-theorem
Our diagonal statistical operator ρD (11), diagonal at the time t ¼ 0, loses the
diagonal character as time goes by Klein (1931). That is, it evolves into an
operator ρ(t2) no longer diagonal in the basis jki. In this last reference, it is
advised to look at the quantity
Inference approach to ground states of quantum systems Chapter
6 125

SQ½ρðt2Þ ¼ 
X
k
ρðt2Þkk ln ½ρðt2Þkk,
(15)
and it is the shown that, if t1 ¼ 0 and t2it1, the following inequality holds
TrρD ln ρD  
X
k
ρðt2Þkk ln ½ρðt2Þkk:
(16)
Evidently, this inequality implies the following one
SQðt1Þ  SQðt2Þ:
(17)
i.e., an H-theorem.
4.4
Our SQ-ground-state wave functions respect the virial theorem
We follow here references (Fernandez and Castro, 1987; van Kampen, 1956)
and restrict ourselves, for economy’s sake, to one-dimensional Hamiltonians
of the form
H ¼ p2=2m + VðxÞ,
(18)
and consider trial wave functions of the MEM form. The M a priori relevant
operators are the powers of the coordinate xni, ni ¼ 1,…, M. Then our MEM
wave functions acquire the aspect
ψðxÞi ¼ hxjψiZ1 exp 
X
i
cixni
"
#
:
(19)
Even so, that vector whose components are cr is not the one that necessarily
minimizes the potential F (14) considered before. Alternatively, one presup-
poses here that the vector cr extremizes the expectation value hψjHjψi and
then minimizes the ground-state energy. Let us now introduce a convenient,
real scale factor τ 6¼ 0 and set di ¼ ciτni. Write then
X
i
cixni ¼
X
i
diðx=τÞni:
(20)
If one fixes the di but permits some slight scale changes (of the sort η ¼ τ + δτ),
this is equivalent to doing special variations on the original ci (taking care of
course of the Z-normalization) (Fernandez and Castro, 1987; van Kampen,
1956). In this special type of variations, the expectation value hψjHjψi depends
only upon τ + δτ, turning out to be stationary at η ¼ τ, as guaranteed by the
variational principle of quantum mechanics, as the ground-state energy is mini-
mal for arbitrary variations of the ci. Let us call scale-function wave function
(WF) those WF of the form (van Kampen, 1956)
jψi ¼ Z1 exp

X
j
djðx=ηÞnj
 
!
(21)
126
Handbook of Statistics

that fulfill the virial theorem (VT) for sure (van Kampen, 1956). The VT is
also fulfilled, obviously, by the exact wave function. Now, our MEM approx-
imation is clearly of the appropriate form and thus verifies the virial theorem.
Thus, it can be regarded as a reasonable approach.
4.5
The SQ-ground-state wave functions respect hypervirial
theorems
This has been proved in Canosa et al. (1992a). What are hypervirial
theorems (VT)?
Consider an arbitrary Hermitian operator W and jψi an eigenvector of H.
Then it holds that (Fernandez and Castro, 1987)
hψj½H,Wjψi ¼ 0:
(22)
This relation is the content of the VT. An approximate wave function is
reasonable if it fulfills the VT, which gives still more credibility to our
maximum entropy approach to wave functions. A stronger form of (22) is
hψjHWjψi ¼ hψjHjψihψjWjψi:
(23)
This has also been verified in Canosa et al. (1992a), which lends even more
credibility to our MEM wave functions as good approximations.
4.6
Saturation
The square modulus of our jψi is a PD. Any PD is totally determined by its
momenta (van Kampen, 1956). The prior information one provides to our
MEM apparatus as input information can be regarded as a provision of some
momenta of jjh jjψij2 (Plastino and Plastino, 1993). As the amount of a priori
information augments, our approximate MEM (Plastino and Plastino, 1993)
tends to the exact wave function, as has been verified in Canosa et al.
(1989, 1990, 1992b).
4.7
Speculation
One might speculate that the (subjective) maximum entropy treatment is in
philosophical accord with the Copenhagen interpretation of quantum mechan-
ics. How? In the sense that it describes the observer’s knowledge of the state.
5
Coulomb potential
It is well known that the exact Coulomb wave function, as a function of the
radius r, reads
ψðrÞ ¼ exp  1
2 ða0 + arÞ
h
i
(24)
Inference approach to ground states of quantum systems Chapter
6 127

and is seen to be of the MEM form. Thus, it can be obtained ‘a la MEM
giving as a priori information
hri ¼ 3=a:
(25)
In turn, the entropy is, setting first
C ¼ ln ð8=π=27Þ + 3,
(26)
SQ ¼ a0 + ahri ¼ C + 3 ln hri:
(27)
5.1
Harmonic oscillator
The one-dimensional HO of mass m and frequency ω has as its exact ground
state just an exponential of the MEM aspect, namely,
a ¼ b
(28)
This can be MEM-predicted with the sole knowledge of hx2i. The entropy is
SQ ¼ ð ln ½ð2π + 1ÞÞ=2 + ln ½ðhx2iÞ
1=2:
(29)
5.2
Morse potential
The Morse potential (MP) is a useful interatomic interaction devised to mimic
the potential energy of a diatomic molecule. It is regarded as a better approxi-
mation for the molecular vibrational structure than the celebrated harmonic
oscillator because it explicitly includes the effects of bond breaking, as for
instance the existence of unbound states. The MP accounts as well for the
an-harmonicity of actual bonds. It is also employed to model other interac-
tions such as that between an atom and a surface. Its ground state (gs) is ame-
nable to be treated with our MEM.
It reads (Canosa et al., 1989)
H ¼ P2=2 + A½1  exp ðXÞ2,
(30)
with A the coupling constant.
The exact ground-state wave function is [C ¼
ﬃﬃﬃﬃﬃﬃ
2A
p
] (Canosa et al., 1989)
ψgs ∝exp ½Cex + ðC  1=2Þx,
(31)
so that the exact MEM Lagrange multipliers an are (Canosa et al., 1989)
2ð1ÞnC=n!,
ni1,
(32)
and rapidly decrease with n.
128
Handbook of Statistics

5.3
Ground state of the quartic oscillator
It reads
H ¼
P2=2 +
X
4
n¼2
αnXn
 
!
, ½X,P ¼ i:
(33)
It was shown in Canosa et al. (1989) that an a priori knowledge of just three
moment a yields MEM ground-state wave functions of excellent quality.
5.4
A possible MEM extension
The maximum entropy criterion is especially suited for the reconstruction of
ground states, i.e., states characterized by smooth distributions over a given
unperturbed basis. In this sense, it is also feasible to consider the wave func-
tion constructed with the inferred coefficients as a trial state for approximat-
ing the ground state of a specific Hamiltonian. In this scenario we can
consider a mixed description, where in addition to the operators Ak whose
mean values are at our disposal, we add to the exponent of ρd some extra diag-
onal operators k ¼ M + 1,…, M + n, whose associated multipliers ak are to be
determined by recourse to the minimization of the mean ground-state energy.
This was successfully done in Canosa et al. (1990).
6
Noncommuting observables
Reconsider the hypervirial relation (22)
hψj½H,Wjψi ¼ 0,
(34)
together with
H ¼ p2=2m + VðxÞ:
(35)
Choose now an operator A that combines the momentum p with a function
f(x) of the coordinate (Plastino et al., 1995)
A ¼ pfðxÞ + fðxÞp,
(36)
taking into account
p ¼ iħd=dx,
(37)
and
½x, p ¼ iħ:
(38)
Then (22) yields the useful relation (Plastino et al., 1995)
Inference approach to ground states of quantum systems Chapter
6 129

1
4m ½ p2f 0ðxÞ2pf 0ðxÞp + f 0ðxÞp2 ¼
fðxÞ dVðxÞ
dx


:
(39)
The ordinary virial theorem obtains whenever
fðxÞ ¼ x,
(40)
that gives
1
2m hp2i ¼
x dVðxÞ
dx
=2


:
(41)
We set m ¼ 1 below. If V is known, then the above relation yields the
expectation value the impulse-squared, to be incorporated as a priori input
information in our maximum entropy procedure.
As an example, we consider the MP given above in (30) that possesses
exact analytical solution that exhibits a MEM aspect, with the exact MEM
Lagrange multipliers an being given by (Canosa et al., 1989)
an ¼ 2ð1ÞnC=n!,
ni1,
(42)
and rapidly decrease with n. Now, with the a priori knowledge of hxi ¼ hA1i,
hx2i ¼ A2, hp2i ¼ A3, so that the approximate
ψMEMðxÞ ¼ hxjψiMEM∝exp 
X
3
k¼1
akAk
"
#
,
(43)
after fixing the Lagrange multipliers with the MEM apparatus above
described, we obtain for jψiMEMj2 an almost exact coincidence with the exact
result (Plastino et al., 1995).
7
Other entropic or information measures
During the last 25 years or so, there has been an increasing amount of work
devoted to the study of generalizations and extensions of the principle of
maximum entropy (Beck, 2009; Frieden, 2004; Frieden et al., 1999; Korbel,
2021; Plastino et al., 2004, 2007; Tempesta, 2011; Tsallis, 2009), and to their
manyfold applications (Beck, 2009; Nobre et al., 2017; Tsallis, 2009). These
efforts rest, in part, on theoretical developments showing that several funda-
mental properties of the standard maximum entropy principle have a universal
character, shared by large families of optimization formalisms based on
nonstandard entropic or information functionals (see, for instance, Tempesta
(2011), Plastino and Plastino (1997, 1998), and Plastino et al. (1997b) and refer-
ences therein). As already mentioned, the generalized forms of the maxent pre-
scription have been applied to multiple problems, including the one that
concerns us here: the maxent treatment of quantum-mechanical pure states.
For instance, optimization schemes extremizing Fisher information (Frieden,
2004) have been applied to the description of solutions to Schrodinger equation
130
Handbook of Statistics

(Flego et al., 2011a, b, c; Puente et al., 2000). It was proved, in particular,
that the Jaynes relations, which arise when deriving the time-independent
Schrodinger equation from a Fisher information-based variational principle,
are closely related to the Feynman–Hellman theorem (Flego et al., 2011a).
One remarkable aspect of the application of information-theoretical ideas,
such as the maximum entropy approach, to the study of solutions of
Schrodinger equation, is that it has suggested unexpected relations between
different areas of physics. This is partially due to the fact that some of the
different entropic or information measures used in theoretical physics are
closely related to each other, as can be clearly appreciated from their appli-
cations to statistical physics (see, for example, Plastino et al., 1997a). With
regard to the analysis of quantum pure states, the information-theoretical
approach helped, for instance, to establish mathematical connections between
solutions of Schrodinger equation, on the one hand, and aspects of Boltzmann
transport equation as applied to the description of dilute gasses, on the other
hand (Flego et al., 2003).
Various applications of the Sq (q is called the nonextensivity index) power-
like entropies (Tsallis, 2009) (Sq should not be confused with the quantum
entropies SQ discussed above), to the description of pure quantum states, have
been advanced (Batle et al., 2002; Rigo et al., 2000; Vignat et al., 2012). With
regard to these developments, it is worth to emphasize that they are not
related to the generalized thermostatistics based on the Sq entropies. Rather,
they employ the Sq entropies as a mathematical tool for studying diverse
aspects of standard quantum theory. These works, indeed, illustrate one side
of the twofold role that the Sq entropies currently play in physics. On the
one hand, the Sq entropies constitute the basis of a thermostatistical formalism
for studying a variety of complex systems. On the other hand, they have
become part of the toolkit used by physicists (and other scientists) for the
study of diverse problems involving probabilistic or information-related ideas.
The applications mentioned here are related to the latter facet of the Sq entro-
pies. In Rigo et al. (2000), a family of one-dimensional potential functions
was investigated that admit ground-state wave functions with associated prob-
ability densities maximizing an Sq entropy under simple constraints. The
research reported in Rigo et al. (2000) established a connection between the
maxent approach to pure states, and the supersymmetry formalism in quantum
mechanics (Cooper et al., 1995). The supersymmetry formalism provides a
powerful set of ideas and methods for the analysis and classification of
exactly solvable potentials in quantum mechanics (Cooper et al., 1995), and
for the development of approximate schemes for obtaining the eigenfunctions
of potentials not admitting exact solution (Cooper et al., 1994, 1995). Super-
symmetry techniques have been applied to numerous problems in quantum
mechanics including, for instance, systems with position-dependent effective
masses (Plastino et al., 1999). A key ingredient of the supersymmetry
approach to the solution of the time-independent Schrodinger equation is
Inference approach to ground states of quantum systems Chapter
6 131

the concept of shape invariance. The idea of shape invariance was shown to
be relevant for analyzing the potential functions studied in Rigo et al.
(2000). These potentials do not exhibit exact shape invariance, but, instead,
they are characterized by approximate shape invariance. As far as we know,
the family of potential functions discussed in Rigo et al. (2000) is the only
one for which the concept of approximate shape invariance has been investi-
gated in a quantitative and systematic way. In Batle et al. (2002) we investi-
gated an approximate scheme for solving Schrodinger’s time-independent
equation that combines appropriate maximum Sq-entropy ansatz with a
supersymmetric-based variational method proposed by Cooper et al. (1994).
As an illustrative test of our approximate scheme, we applied it to the cele-
brated Poschl–Teller potential (Batle et al., 2002). In Vignat et al. (2012)
we proved that there is a family of potential functions, including the Coulomb
potential (associated, for instance, with the hydrogen atom) whose ground-
state wave functions, when expressed in momentum space, have the form of
q-Gaussians. The corresponding probability densities (again, in momentum
space) are Sq-maximum entropy densities. The family of potentials investi-
gated in Vignat et al. (2012) include potentials with different space dimen-
sions. The value of the parameter q characterizing the ground-state wave
functions depends on the value of the space dimension.
This chapter is focused on applications of maxent ideas and techniques to
the study of pure states in standard quantum theory, which is based on the lin-
ear Schrodinger equation. We would like, however, for the sake of complete-
ness, to mention very briefly some recent applications of the maxent approach
to the study of solutions of nonlinear wave equations. Nonlinear wave equa-
tions admitting exact time-depending q-Gaussian solutions were explored in
Plastino and Wedemann (2017). The wave equations investigated in Plastino
and Wedemann (2017) have nonlinearities involving a combination of differ-
ent power laws. The nonlinearities appear in a potential-like term, akin to the
one appearing in the celebrated Gross–Pitaevskii equation. Other types of
nonlinear wave equations having q-Gaussian time-dependent solutions have
been investigated by various authors in the last 10 years or so (see Nobre
et al. (2017) for a recent review). The wave equations reviewed in Nobre
et al. (2017) have power-law nonlinearities affecting the Laplacian term of
the equations. It is instructive to compare some of the basic features of the
evolution equation investigated in Plastino and Wedemann (2017), with those
of the equations discussed in Nobre et al. (2017). The wave equations ana-
lyzed in Plastino and Wedemann (2017) lead to a conservative dynamics,
and to a time evolution where the norm of the wave function is strictly pre-
served. The wave equations discussed in Nobre et al. (2017), on the other
hand, exhibit a much richer and complex behavior than those studied in
Plastino and Wedemann (2017), and are endowed with both conservative
and dissipative features. The norm of the wave function is not preserved in
general, but, for some particular solutions characterized by a soliton-like
132
Handbook of Statistics

behavior, the norm is preserved. The soliton-like solutions have physically
appealing properties. In particular, they are consistent with the Einstein–de
Broglie relations connecting frequency and wave number with energy and
momentum, which suggests that the soliton-like solutions might describe free
particles.
8
Conclusions
We have seen how researchers have been able to adapt Jaynes’ MEM, that
was capable of affording a total reformulation of Gibbs’ statistical mechanics
in the 1960s, to a new endeavor.
The endeavor was that of inferring, via the MEM, precise information
regarding the probabilities jCjj2 associated with the expansion (in an orthonor-
mal Hilbert basis jji) of a pure state’s wave function
jψi ¼
X
j
Cjjji:
(44)
This results arises out of maximizing the special quantum entropy SQ
SQ ¼ 
X
j
jCjj2 ln ½jCjj2,
(45)
subject to appropriate a priori known values of relevant mean values. We have
reviewed variegated particular applications of this methodology as well as its
relations to basic general quantum properties.
References
Agmon, N., Alhassid, Y., Levine, R.D., 1979. In: Levine, R.D., Tribus, M. (Eds.), The Maximum
Entropy Formalism. MIT Press, Cambridge, MA.
Balian, R., 1991. From Microphysics to Macrophysics. Springer, Berlin.
Batle, J., Casas, M., Plastino, A.R., Plastino, A., 2002. Physica A 305, 316.
Beck, C., 2009. Contemp. Phys. 50, 495.
Brillouin, L., 1956. Science and Information Theory. Academic Press, New York.
Canosa, N., Plastino, A., Rossignoli, R., 1989. Phys. Rev. A 40, 519.
Canosa, N., Rossignoli, R., Plastino, A., 1990. Nucl. Phys. A 512, 520.
Canosa, N., Plastino, A., Rossignoli, R., 1992a. Nucl. Phys. A 550, 453.
Canosa, N., Rossignoli, R., Plastino, A., Miller, H.G., 1992b. Phys. Rev. C 45, 1162.
Cooper, F., Dawson, J., Shepard, H., 1994. Phys. Lett. A 187, 140.
Cooper, F., Khare, A., Sukhatme, U.P., 1995. Phys. Rep. 251, 267.
Fernandez, F.M., Castro, E.A., 1987. Hypervirial Theorems. Springer, Berlin.
Flego, S.P., Frieden, B.R., Plastino, A., Plastino, A.R., Soffer, B.H., 2003. Phys. Rev. E 68, 016105.
Flego, S.P., Plastino, A., Plastino, A.R., 2011a. Ann. Phys. 326, 2533–2543.
Flego, S.P., Plastino, A., Plastino, A.R., 2011b. Physica A 390, 2276.
Flego, S.P., Plastino, A., Plastino, A.R., 2011c. J. Math. Phys. 52, 082103.
Frieden, B.R., 2004. Science From Fisher Information: A Unification. Cambridge University Press.
Frieden, B.R., Plastino, A., Plastino, A.R., Soffer, B.H., 1999. Phys. Rev. E 60, 48.
Inference approach to ground states of quantum systems Chapter
6 133

Jaynes, E.T., 1957. Phys. Rev. 108, 171.
Katz, A., 1967. Principles of Statistical Mechanics. Freeman, San Francisco.
Klein, A., 1931. Z. Phys. 72, 767.
Korbel, J., 2021. Entropy 23, 96.
Nobre, F.D., Rego-Monteiro, M., Tsallis, C., 2017. Entropy 19, 39.
Plastino, A., Plastino, A.R., 1993. Phys. Lett. A 181, 446.
Plastino, A., Plastino, A.R., 1997. Phys. Lett. A 226, 257–263.
Plastino, A.R., Plastino, A., 1998. Physica A 258, 429–445.
Plastino, A.R., Wedemann, R., 2017. Entropy 19, 60.
Plastino, A.R., Casas, M., Plastino, A., Puente, A., 1995. Phys. Rev. A 52, 2601.
Plastino, A., Plastino, A.R., Miller, H.G., 1997a. Phys. Lett. A 235, 129–134.
Plastino, A.R., Miller, H.G., Plastino, A., Yen, G.D., 1997b. J. Math. Phys. 38, 6675–6682.
Plastino, A.R., Rigo, A., Casas, M., Garcias, F., Plastino, A., 1999. Phys. Rev. A 60, 4318.
Plastino, A.R., Miller, H.G., Plastino, A., 2004. Contin. Mech. Thermodyn. 16, 269.
Plastino, A.R., Plastino, A., Soffer, B.H., 2007. Phys. Lett. A 363, 48.
Puente, A., Plastino, A.R., Casas, M., Garcias, F., Plastino, A., 2000. Physica A 277, 146.
Rigo, A., Plastino, A.R., Casas, M., Garcias, F., Plastino, A., 2000. J. Phys. A Math. Gen. 33, 6457.
Shannon, C., 1948. Bell Syst. Tech. J. 27 (379), 623.
Tempesta, P., 2011. Phys. Rev. E 84, 021121.
Toranzo, I.V., Dehesa, J.S., 2019. Physica A 516, 273.
Tsallis, C., 2009. Introduction to Nonextensive Statistical Mechanics, Approaching a Complex
World. Springer, New York.
van Kampen, N.G., 1956. Stochastic Processes in Physics and Press. New York.
Vignat, C., Plastino, A., Plastino, A.R., Dehesa, J.S., 2012. Physica A 391, 1068–1073.
134
Handbook of Statistics

Chapter 7
MCMC for GLMMs
Vivekananda Roy*
Department of Statistics, Iowa State University, Ames, IA, United States
*Corresponding author: e-mail: vroy@iastate.edu
Abstract
Generalized linear mixed models (GLMMs) are often used for analyzing correlated
non-Gaussian data. The likelihood function in a GLMM is available only as an intrac-
table high dimensional integral, and thus closed-form inference and prediction are not
possible for GLMMs. Since the likelihood is not available in a closed form, the asso-
ciated posterior densities in Bayesian GLMMs are also intractable. Generally, Markov
chain Monte Carlo (MCMC) algorithms are used for conditional simulation in GLMMs
and exploring these posterior densities. In this article, we present different state of the
art MCMC algorithms for fitting GLMMs. These MCMC algorithms include efficient
data augmentation strategies, as well as diffusions based and Hamiltonian dynamics
based methods. The Langevin and Hamiltonian Monte Carlo methods presented here
are applicable to any GLMMs, and are illustrated using three most popular GLMMs,
namely, the logistic and probit GLMMs for binomial data and the Poisson-log GLMM
for count data. We also present efficient data augmentation algorithms for probit and
logistic GLMMs. Some of these algorithms are compared using a numerical example.
Keywords: Bayesian GLMM, Data augmentation, EM, GLM, Hamiltonian Monte
Carlo, MALA, Metropolis–Hastings, Mixed models, Monte Carlo maximum likeli-
hood, spatial GLMM
1
Introduction
Generalized linear mixed models (GLMMs) are a natural extension of both
the linear mixed models and the generalized linear models (GLMs). GLMMs
allow for non-Gaussian responses and the random effects in the GLMMs can
accommodate overdispersion often present in non-Gaussian data, as well as
dependence among correlated observations arising from longitudinal or
repeated measures studies. GLMM is one of the most frequently used statisti-
cal models (Jiang and Nguyen, 2007).
Unlike the linear mixed models and the GLMs, the likelihood function for
the GLMM is not available in a closed-form. Since the likelihood function
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.06.005
Copyright © 2022 Elsevier B.V. All rights reserved.
135

for the GLMM is obtained by integrating a GLM likelihood with respect to the
distribution of the random effects, it is available only as a high-dimensional
integral. Both analytical and Monte Carlo approximations for the GLMM
likelihood have been proposed in the literature. For example, Breslow and
Clayton (1993) considered the Laplace’s method, and Wolfinger and
O’connell (1993) used a Taylor expansion for the integral approximation.
Implementations involving Markov chain Monte Carlo (MCMC) methods
include Zeger and Karim (1991) who used Gibbs sampling and McCulloch
(1997) who used Metropolis–Hastings (MH) algorithm for making inference
in GLMMs. Use of sampling-based methods in GLMMs can be found in
McCulloch (1994), Gamerman (1997), and Booth and Hobert (1999) among
others.
Over the last two decades, extensive efforts have gone into producing effi-
cient, fast mixing MCMC algorithms, in general, and in the context of
GLMMs, in particular. For example, effective data augmentation (DA) strate-
gies have been proposed for specific GLMMs (see e.g., Polson et al., 2013;
Rao and Roy, 2021; Wang and Roy, 2018a, b). Also novel MH algorithms
based on Langevin diffusions and Hamiltonian dynamics such as the Metrop-
olis adjusted Langevin algorithms (MALA) (Roberts and Tweedie, 1996) and
the Hamiltonian Monte Carlo (HMC) algorithms (Neal, 2011) have now
emerged as the popular methods for MCMC sampling due to their ability to
make distant moves with high acceptance probability and favorable scalability
with respect to increasing state space dimensions. MALA and HMC have also
been applied for inference for GLMMs (see e.g., B€urkner, 2017; Roy and
Zhang, 2022). The goal of this chapter is to present these efficient MCMC
algorithms for fitting GLMMs.
The rest of the article is organized as follows. In Section 2, we present the
likelihood function for GLMMs and describe two sampling-based approaches,
namely the Monte Carlo EM and the Monte Carlo maximum likelihood meth-
ods for approximating this likelihood function. In Section 3, we construct dif-
ferent MCMC algorithms for conditional simulation in GLMMs. Thus, these
algorithms can be used for sampling-based inference in GLMMs. Several pop-
ular GLMMs are used to illustrate these algorithms. Next, Section 4 presents
MCMC algorithms for Bayesian GLMMs under popular priors on the fixed
effects and the variance components parameters. Section 5 contains compari-
sons of some of these MCMC algorithms using a publicly available dataset.
Some concluding remarks are provided in Section 6.
2
Likelihood function for GLMMs
Let Y ¼ ðY1, …, YmÞ> denote the vector of response variables. Let xi and zi be
the p  1 and q  1 vectors of fixed and random effects covariates associated
with the ith response, respectively, i ¼ 1, …, m. Let β  p be the regression
coefficients vector and u  q be the random effects vector. A GLMM can be
136
Handbook of Statistics

built with a link function g that connects μi, the (conditional) expectation of
Yi, with the covariates xi, zi satisfying gðμiÞ ¼ x>
i β + z>
i u, and assuming that
conditional on the random effects u, the responses Yi’s are independent with
Yi|β, u 
ind fðyijβ, uÞ ≡exp
yiξi  bðξiÞ
aiðιÞ
+ ciðyi, ιÞ


for
i ¼ 1, …, m, (1)
where ι is the dispersion parameter and ai(), b(), ci(, ) are known functions.
To simplify the presentation, here we assume that ι is known. The quantity ξi
is associated with the conditional mean μi and hence with β, u. The description
of a GLMM is completed by specifying the distribution of u and the exponen-
tial family pdf (1) with the forms of the functions ai(), b() and ci(, ). Assume
there are r random effects u>
1 , u>
2 , …, u>
r , where uj is a qj  1 vector with qj > 0,
with q1 + q2 + ⋯+ qr ¼ q and u ¼ u>
1 , …, u>
r

>. A common assumption is
that uj 
ind Nð0, Λj  RjÞ where the low-dimensional covariance matrix Λj is
unknown and need to be estimated, and the structured matrix Rj is usually
known. Here,  indicates the Kronecker product. Let y ¼ ðy1, y2, …, ymÞ
denote the vector of observed responses. For specific examples of (1) and
the link function g(), we consider the three most popular GLMMs, namely
the logistic GLMM, the probit GLMM and the Poisson GLMM with the
log link.
Example 1 (Logistic GLMM). For the logistic GLMM, (1) is the binomial
pmf given by
‘i
yi
 
ðμi=‘iÞyið1  μi=‘iÞ‘iyi, yi ¼ 0, 1, …, ‘i,
(2)
where ‘i is a positive integer indicating the number of trials. The logistic
GLMM uses the logit link gðμiÞ ¼ log ðμi=½‘i  μiÞ ¼ x>
i β + z>
i u.
Example 2 (Probit GLMM). In this case, (1) is also the binomial pmf (2)
whereas, the probit link gðμiÞ ¼ Φ1ðμi=‘iÞ ¼ x>
i β + z>
i u is used in the probit
GLMM. Here, Φ() is the cdf of the standard normal distribution.
Example 3 (Poisson-log GLMM). For the Poisson-log link model, (1) is the
Poisson pmf given by
exp ðμiÞμyi
i =yi!, yi ¼ 0, 1, …,
(3)
with gðμiÞ ¼ log ðμiÞ ¼ x>
i β + z>
i u.
Let Λ ¼ ðΛ1, …, ΛrÞ. Then the likelihood function for (β, Λ) is given by
Lðβ, Λ|yÞ ¼
Z
q
Y
m
i¼1
fðyijβ, uÞ
"
#
ϕqðu; 0, GÞdu:
(4)
MCMC for GLMMs Chapter
7 137

Here, ϕq(u; 0, D) denotes the probability density function of the q-dimensional
normal distribution with mean vector 0, covariance matrix D, evaluated at u.
Also, G ¼  r
j¼1Λj  Rj, with  indicating the direct sum. Outside the linear
mixed model where (1) is the normal density, L(β,Λ|y) in (4) nearly always
involves intractable integrals and is not available in closed form.
There are two widely used Monte Carlo approaches for approximating the
likelihood function (4) and making inference on (β,Λ), namely the Monte
Carlo EM algorithm (Booth and Hobert, 1999) and the Monte Carlo maxi-
mum likelihood based on importance sampling (Geyer, 1994; Geyer and
Thompson, 1992). The EM is an iterative method and each iteration of this
algorithm consists of an “E-step” and an “M-step.” The ( j + 1)st E-step entails
the calculation of
E½ log fðy, ujβ, ΛÞjβðjÞ, ΛðjÞ,
(5)
where
fðy, ujβ, ΛÞ ¼
Y
m
i¼1
fðyijβ, uÞ
"
#
ϕqðu; 0, GÞ
(6)
is the joint density of (y, u) and (β(j), Λ(j)) is the value of (β, Λ) from the
jth iteration. The expectation in (5) is with respect to f(ujy, β(j), Λ(j)), where
f(ujy, β, Λ) is the conditional density of u given y
fðujβ, Λ, yÞ ¼ fðy, ujβ, ΛÞ
Lðβ, ΛjyÞ ,
(7)
with f(y, ujβ, Λ) and L(β, Λ|y) given in (6) and (4), respectively. Since closed-
form expressions of (4) and hence (7) are not available, so are the means (5)
with respect to the conditional density (7). In the Monte Carlo EM algorithm
(Wei and Tanner, 1990), (5) is approximated by a Monte Carlo estimate
which is then maximized in the M-step. Indeed, if fuðn,jÞ, n ¼ 1, …, Ng
are samples obtained by running a Markov chain with invariant density f(ujβ(j),
Λ(j), y), then a Monte Carlo estimate of (5) is PN
n¼1 log ½ fðy, uðn,jÞj β, ΛÞ=N
(McCulloch, 1994).
In the Monte Carlo maximum likelihood (Geyer, 1994; Geyer and
Thompson, 1992), the likelihood function (4) is expressed as
Lðβ, ΛjyÞ ¼
Z
q fðy, ujβ, ΛÞdu
¼
Z
q
fðy, ujβ, ΛÞ
fðy, ujβð0Þ, Λð0ÞÞ
fðy, ujβð0Þ, Λð0ÞÞdu ∝E
fðy, ujβ, ΛÞ
fðy, ujβð0Þ, Λð0ÞÞ
"
#
,
(8)
where the expectation is with respect to the density (7) at some value
(β(0), Λ(0)). Since the expectation in (8) can be approximated by
138
Handbook of Statistics

ð1=NÞPN
n¼1 fðy,uðn,0Þjβ,ΛÞ=f y,uðn,0Þjβð0Þ,Λð0Þ


, maximum likelihood esti-
mates of (β, Λ) are calculated by maximizing this Monte Carlo approxima-
tion. Geyer (1994) recommends making a few pilot iterations to find an
appropriate value for (β(0), Λ(0)). The importance sampling technique has been
successfully used for analyzing spatial generalized linear mixed models
(SGLMMs) which are GLMMs with the random effects u being derived from
a spatial process (see e.g., Christensen, 2004; Evangelou and Roy, 2019; Roy
et al., 2016, 2018).
3
Conditional simulation for GLMMs
Both the Monte Carlo EM and the Monte Carlo maximum likelihood methods
for making inference on (β, Λ) require effective methods for sampling from
the density f(ujy, β, Λ) given in (7). To simplify notations, we use f(ujy) to
denote this conditional density. In the context of some numerical examples
involving SGLMMs, Roy and Zhang (2022) observe poor performance of ran-
dom walk Metropolis compared to some other MH algorithms for sampling
from (7). In this section, we present different variants of the MALA and
HMC methods for exploring (7). We also describe some DA algorithms for
the two particular GLMMs, namely the logistic and the probit GLMMs.
3.1
MALA for GLMMs
MCMC methods are based on discrete time Markov chains. For example, as
mentioned in Section 2, both Monte Carlo EM and Monte Carlo maximum
likelihood methods require Markov chains {u(n)}n	1 with appropriate station-
ary densities. However, often, there are great benefits to first considering an
appropriate continuous time stochastic process that possesses desirable prop-
erties. In particular, one can specify these continuous time processes by some
differential equations as illustrated in this section as well as in Section 3.2.
For example, MALA is a discrete time Markov chain based on the Langevin
diffusion ut defined as
dut ¼ ð1=2Þr log fðutjyÞdt + dst,
(9)
where st is the q-dimensional standard Brownian motion. It is known that
f(ujy) is stationary for ut given in (9). On the other hand, discretizations of
(9), say, by using the Euler-Maruyama method may fail to maintain the statio-
narity with respect to f(ujy). MALA is an MH chain {u(n)}n	1 where, in each
iteration, the proposal u0 is drawn following a simple discretization of (9)
given by
u0 ¼ uðn1Þ + Er log fðuðn1ÞjyÞ=2 +
ﬃﬃE
p υðnÞ
(10)
for a chosen step-size E with υðnÞ 
iid Nð0, IqÞ. The proposal u0 is accepted with
probability
MCMC for GLMMs Chapter
7 139

αðuðn1Þ, u0Þ ¼ 1 ^
fðu0jyÞkðu0, uðn1ÞÞ
fðuðn1ÞjyÞkðuðn1Þ, u0Þ ,
(11)
where the proposal density k(u, u0) is the Nðu + Er log fðujyÞ=2, EIqÞ density
evaluated at u0. Since the mean of the proposal density of the MALA is gov-
erned by the gradient of log of the target distribution, it is likely to make
moves in the directions in which f is increasing. That way, the chain is encour-
aged to move toward the nearest mode of f(ujy) and stay near the high mass
regions of the target density.
The MALA chain {u(n)}n	1 can be used to approximate (5) or (8). In par-
ticular, if f(ujβ(j), Λ(j), y) is used for the stationary density f(ujy) in (10) then
the corresponding MALA chain can be used for the jth E-step in the EM algo-
rithm and when f(ujy) in (10) is replaced with f(ujβ(0), Λ(0), y), it results in a
MALA chain that can be used for estimating (8). In practice, the step-size E
is chosen as O(q1/3) obtaining an acceptance rate of between 40% and
70% (Roberts and Rosenthal, 1998). For implementing MALA we need the
derivatives of log fðujyÞ. Here, we derive r log fðujyÞ for the three popular
GLMMs mentioned in Section 2. Let X and Z be the m  p and m  q known
design matrices with ith row being x>
i
and z>
i , i ¼ 1, …, m respectively. Let
γi ¼ x>
i β + z>
i u, i ¼ 1, …, m and γ ¼ Xβ + Zu.
Example 1 (Continued). For the binomial-logit link model, the log fðujyÞ (up
to an additive constant) is
 u>G1u + log jGj
	

=2 +
X
m
i¼1
yiγi  ‘i log ð1 + exp ðγiÞÞ
½
:
(12)
Letting ξ be the m  1 vector with ith element ‘i exp ðγiÞ=ð1 + exp ðγiÞÞ,
i ¼ 1, …, m, we have
r log fðujyÞ ¼ G1u + Z>y  Z>ξ:
Example 2 (Continued). For the binomial-probit model, the log fðujyÞ (up to
an additive constant) is
 u>G1u + log jGj
	

=2 +
X
m
i¼1
yi log ðΦðγiÞÞ + ð‘i  yiÞ log ð1  ΦðγiÞÞ
½
: (13)
Let τ1 and τ2 be the m  1 vectors with ith element yiϕ(γi)/Φ(γi) and
[‘i  yi]ϕ(γi)/[1 Φ(γi)], respectively, i ¼ 1, …, m. Here, ϕ() ≡ϕ1() is the
standard normal pdf. For the probit GLMM we have
ALGORITHM 1 The nth iteration for the MALA.
1: Given u(n1) draw u0  Nðuðn1Þ + Erlog f ðuðn1ÞjyÞ=2,EIqÞ.
2: Draw δ  Uniform (0, 1). If δ < α(u(n1), u0) then set u(n)  u0, else set u(n)  
u(n1). Here, α(, ) is as defined in (11).
140
Handbook of Statistics

r log fðujyÞ ¼ G1u + Z>τ1  Z>τ2:
Example 3 (Continued). For the Poisson-log GLMM, we derive log fðujyÞ
which (up to an additive constant) is
log f ðujyÞ ¼  u>G1u + log jGj
	

=2 +
X
m
i¼1
ðyiγi  exp fγigÞ,
(14)
implying
r log fðujyÞ ¼ G1u + Z>y  Z> exp ðγÞ:
There are other variants of MALA, e.g., the preconditioned MALA
(Stramer and Roberts, 2007), and the manifold MALA (Girolami and
Calderhead, 2011) proposed in the literature. In the preconditioned MALA,
the proposal density is Nðu + hMr log fðujyÞ=2, hMÞ for some positive defi-
nite matrix M. For conditional simulation in SGLMMs, Roy and Zhang
(2022) observe that the preconditioned MALA with an appropriately chosen
M, and the manifold MALA can have superior performance over the
standard MALA.
3.2
HMC for GLMMs
In the HMC algorithm (Duane et al., 1987; Neal, 2011), an auxiliary variable
ρ  f(ρ) ≡N(0, M) is introduced for some q  q positive definite matrix M.
The HMC chain {u(n), ρ(n)}n	1 alternates between draws from f(ρ(n)ju(n1)) ≡
f(ρ(n)) ≡N(0, M) and f uðnÞjρðnÞ, uðn1Þ


. We now describe how a draw from
f uðnÞjρðnÞ, uðn1Þ


is made using ideas from classical mechanics. The negative
of logarithm of the joint density of u and ρ given by
Hðu, ρÞ ¼  log fðujyÞ + ð log ðð2πÞqjMjÞ + ρ>M1ρÞ=2
is a Hamiltonian function of the position (u) and the momentum (ρ). Intui-
tively, the Hamiltonian H(u, ρ) measures the total energy of a physical sys-
tem and it consists of the potential energy  log fðujyÞ and the kinetic
energy ρ>M1ρ/2. This is the reason M is referred to as the mass matrix.
The Hamiltonian equations are given by the following first-order differen-
tial equations
du
dt ¼ ∂H
∂ρ ¼ M1ρ and dρ
dt ¼  ∂H
∂u ¼ r log fðujyÞ:
(15)
Solution of the Hamiltonian equations (15) results in the Hamiltonian flow
from an initial (u0, ρ0) to (ut, ρt). Here, du/dt and dρ/dt denote the deriva-
tives of u and ρ with respect to the (fictitious) continuous time t. To garner
MCMC for GLMMs Chapter
7 141

intuition behind (15) the following analogy in 2 is useful (Neal, 2011).
Imagine a sledge sliding over a friction-less surface of varying height pro-
portional to 1/f(ujy). The potential energy is based on the height of the
surface at the current position, u, whereas the kinetic energy is determined
by the sledge’s momentum, ρ, and its mass, M. In a flat surface, that is,
when r log fðujyÞ ¼ 0, 8u, the sledge moves at a constant velocity. On the
other hand, when slope is positive (r log fðujyÞ < 0), the kinetic energy
decreases as the potential energy increases until it vanishes (ρ ¼ 0). The
sledge then slides back down the hill increasing its kinetic energy and
decreasing the potential energy.
Over any interval, the Hamiltonian dynamics (15) defines the Hamiltonian
flow (u0, ρ0) ! (ut, ρt) that satisfies three important properties, namely, (i) it
is energy preserving, that is, H(ut, ρt) ¼ H(u0, ρ0), (ii) it is volume preserving,
that is, dutdρt ¼ du0dρ0, and (iii) it is time reversible, which implies if, (u0, ρ0)
 ν then (ut, ρt)  ν. Since (15) can-not be solved analytically for practical
examples, the St€ormer–Verlet or the leapfrog method is a standard approach
for approximating the solutions to (15) (Duane et al., 1987). In particular, this
method uses a discrete step-size E to make a move, according to
ρt+E=2 ¼ ρt + ðE=2Þr log fðutjyÞ
ut+E ¼ ut + EM1ρt+E=2
ρt+E ¼ ρt+E=2 + ðE=2Þr log fðut+EjyÞ:
(16)
In HMC, in order to draw from f(u(n)jρ(n), u(n1)), starting from (u(n1), ρ(n)),
the above set of deterministic steps (16) (referred to as Leapfrog (, , E, M)) is
repeated L times to generate a proposal (u0, ρ0) which is then accepted/rejected
with an MH step. The Leapfrog method preserves the volume exactly and it is
also reversible by simply negating ρ (see Neal, 2011, for details).
Note that, if we could simulate the Hamiltonian dynamics (15) exactly, by the
energy preserving property (i), energy would be preserved exactly, and the MH
acceptance probability would always be min f1, exp ð0Þg ¼ 1: Since we use
the leapfrog integrator, that approximately simulates the dynamics, if the approx-
imation is good, then H(u0, ρ0)  H(u(n1), ρ(n)) would be small, and the accep-
tance rate will be high. Indeed, for HMC, α still tends to be high even for
proposals that are far from the current state, reducing the random walk behavior
ALGORITHM 2 The nth iteration for the HMC.
1: Draw ρ(n)  N(0, M).
2: Set u0 u(n1) ρ0 ρ(n).
3: For i ¼ 1,…,L do (u0, ρ0)  Leapfrog (u0, ρ0, E, M).
4: α  min ð1, expfHðu0,ρ0Þ + Hðuðn1Þ,ρðnÞÞgÞ
5: Draw δ  Uniform (0, 1). If δ < α then set u(n)  u0, ρ(n)  ρ0, else set
u(n)  u(n1).
142
Handbook of Statistics

of some other MH algorithms. The marginal chain {u(n)}n	1 of the HMC chain
{u(n), ρ(n)}n	1 can be used to approximate the expectations in (5) or (8).
The choices of E, L, and the mass matrix M should be such that the result-
ing algorithm mixes well (i.e., the sampled distribution is “close” to the target
distribution), leading to suitable acceptance rates and lower Monte Carlo
errors. Often, in practice, the mass matrix M is chosen to be the identity
matrix and E, L are adjusted to achieve around 70% acceptance rates. The
No-U-Turn sampler (NUTS) (Hoffman and Gelman, 2014) is an extension
of HMC that eliminates the need of manual tuning of L. Hoffman and
Gelman (2014) also propose a method for dynamically adapting the E param-
eter on the fly. NUTS is employed in the programming language Stan
(Carpenter et al., 2017). Girolami and Calderhead (2011) propose the
Riemannian manifold HMC algorithm that uses a position-dependent M that
changes in every iteration, eliminating the need for manually tuning the mass
matrix. A comparison of performance of different HMC algorithms in the
context of analyzing GLMMs can be found in Zhang (2022).
3.3
Data augmentation for GLMMs
DA is an MCMC algorithm that has been widely used for analyzing Bayesian
probit and logistic GLMs (see e.g., Albert and Chib, 1993; Polson et al.,
2013). Recently, DA algorithms have been developed and studied for
Bayesian GLMMs (Polson et al., 2013; Rao and Roy, 2021; Wang and Roy,
2018a, b). In this section, we propose DA algorithms for simulating from
(7) corresponding to the probit and logistic mixed models. For constructing a
valid and efficient DA algorithm (Tanner and Wong, 1987) for f(ujβ, Λ, y) in
(7) we need to construct a joint density f(u, djβ, Λ, y) with augmented variables
d satisfying the following two properties
(i) the u-marginal of the joint density f(u, djβ, Λ, y) is the target density
(7) and
(ii) sampling from the two corresponding conditional densities fujd and fdju is
straightforward.
Each iteration of the DA algorithm consists of two steps—a draw from fdju
followed by a draw from fujd. Thus, the DA Markov chain {u(n), d(n)}n	1 is
a two-variable Gibbs sampler. The DA algorithm, like its deterministic coun-
terpart the EM algorithm, is widely used. In Sections 3.3.1 and 3.3.2, we pro-
vide appropriate DA algorithms for the probit and logistic mixed models,
respectively.
3.3.1
Data augmentation for probit mixed models
In this section, we consider probit GLMM for binary data, that is, ‘i ¼ 1 for
i ¼ 1, …, m in Example 2. Thus, ðY1, Y2, …, YmÞ are independent Bernoulli
random variables with PðYi ¼ 1Þ ¼ Φðx>
i β + z>
i uÞ . Following Albert and
Chib (1993), let vi  be the continuous latent variable corresponding to
MCMC for GLMMs Chapter
7 143

binary observation Yi such that Yi ¼ I(vi > 0), where vijβ, u 
ind Nðγi, 1) for
i ¼ 1, …, m. Then
PðYi ¼ 1Þ ¼ Pðvi > 0Þ ¼ ΦðγiÞ:
(17)
Let v ¼ ðv1, …, vmÞ>, then vjβ, u  N(Xβ + Zu, Im). Using the latent variables
v, we introduce the joint density
fðu, vjβ, Λ, yÞ ¼
1
Lðβ, ΛjyÞ
Y
m
i¼1
ϕðvi; γi, 1Þ 1ð0,∞Þ vi
ð Þ
	

yi 1 ∞,0
ð
 vi
ð Þ
	

1yi
"
#
 ϕqðu; 0, GÞ:
(18)
From (17) it follows that
Z
mfðu, vjβ, Λ, yÞdv ¼ fðujβ, Λ, yÞ,
(19)
where f(ujβ, Λ, y), given in (7), is the target density. Thus, the condition (i) of
DA construction mentioned before holds. From (18), it follows that
viju, β, Λ, y 
ind TNðγi, 1, yiÞ, i ¼ 1, …, m,
(20)
where TN(μ, σ2, e) denotes the distribution of the normal random variable
with mean μ and variance σ2, that is truncated to have only positive values
if e ¼ 1, and, it has only negative values if e ¼ 0.
From (18) it follows that the conditional density of u given β, Λ, v, y is
fðu | β, Λ, v, yÞ ∝
Q
m
i¼1
exp
 1
2 ðz>
i uÞ
2  2ðz>
i uÞðvi  x>
i βÞ
h
i
n
o
"
#
exp  1
2 u>G1u
h
i
¼ exp  1
2 u>ðZ>Z + G1Þu  2u>Z>ðv  XβÞ


h
i
:
Thus, the conditional distribution of u is
ujv, β, Λ, y  Nq ðZ>Z + G1Þ
1Z>ðv  XβÞ, ðZ>Z + G1Þ
1


:
(21)
Thus, every iteration of the proposed DA algorithm for (18) consists of
making the draws of v and u from (20) and (21), respectively.
The conditional distribution of u in the above DA algorithm and several
other conditional distributions appearing in this article are normal distributions
of the form N(S1t, S1) for some positive definite matrix S and a vector t.
ALGORITHM 3 The nth iteration for the DA algorithm.
1: Given u(n1), draw vðnÞ
i

ind TNðx>
i β + z>
i uðn1Þ,1,yiÞ for i ¼ 1,…,m.
2: Draw uðnÞ  Nq ðZ >Z + G1Þ1Z >ðvðnÞ XβÞ,ðZ>Z + G1Þ1


.
144
Handbook of Statistics

A naive method of drawing from N(S1t, S1) is inefficient when the dimen-
sion of S is large as it involves calculating inverse of the matrix S. Rao
and Roy (2021) advocate using the following method of drawing from
N(S1t, S1) that does not require computing S1.
DA algorithms, although popular, often suffer from slow convergence and
high autocorrelations. Liu and Wu (1999) proposed the parameter expansion
for data augmentation (PX-DA) algorithms for speeding up the convergence
of DA algorithms. More recently, Hobert and Marchev (2008) compared the
performance of PX-DA algorithms based on a Haar measure (called the Haar
PX-DA algorithms), the PX-DA algorithms based on a probability measure
and the DA algorithms. Hobert and Marchev (2008) showed that, under some
mild conditions, the Haar PX-DA algorithms are better than the PX-DA and
the DA algorithms in terms of different ordering. In PX-DA, an extra step
is added (sandwiched) between the two steps of the original DA algorithm.
In order to construct this extra step, we derive the marginal density of v from
the joint density (18) as
f vjy
ð
Þ ¼
Z
qfðu, vjyÞdu
∝Q
m
i¼1
1ð0,∞Þ vi
ð Þ
	

yi 1 ∞,0
ð
 vi
ð Þ
	

1yi exp
 1
2 v>Z1v  2v>Z1Xβ
	

n
o
,
(22)
where
Z1 ¼ Im  Z Z>Z + G1

1Z>
h
i
:
Let V denote the subset of m where v lives, that is, V is the Cartesian cpro-
duct of m half (positive or negative) lines, where the ith component is (0, ∞)
(if yi ¼ 1) or (∞, 0] (if yi ¼ 0). Let ψ be the unimodular multiplicative
group on + with Haar measure ν(dh) ¼ dh/h, where dh is Lebesgue mea-
sure on +. For constructing an efficient extra step, as in Roy and Hobert
(2007), we let the group ψ act on V through a group action TðvÞ ¼ hv ¼
ðhv1, hv2, …, hvmÞ>. With the group action defined this way, it can be shown
that the Lebesgue measure on V is relatively left invariant with the multi-
plier χ(h) ¼ hm (Hobert and Marchev, 2008; Roy, 2014). Following
Hobert and Marchev (2008), consider a probability density function ω(h)
on ψ where
ALGORITHM 4 An algorithm for drawing from N (S21t, S21).
1: Let S ¼ LL> be the Cholesky decomposition of S.
2: Solve Lw ¼ t.
3: Draw z  N (0, Iq) where q is the dimension of S.
4: Solve L>x ¼ w + z. Then x  N (S1t, S1).
MCMC for GLMMs Chapter
7 145

ω h
ð Þdh ∝f hvjy
ð
Þ χ h
ð ÞνðdhÞ
∝hm1 exp
 1
2 h2v>Z1v  2hv>Z1Xβ
	

n
o
dh:
(23)
Since Z1 is a positive definite matrix, given v, ω h
ð Þ is a valid density. From
Hobert and Marchev (2008), it follows that the transition v ! v0 ≡T(v) ¼ hv
where h  ω(h), is reversible with respect to f(vjy) defined in (22). As men-
tioned in Roy (2014), intuitively, the extra step (23) reduces the correlation
between u(n1) and u(n) and thus improves the mixing of the DA algorithm.
Below are the three steps involved in every iterations of the proposed Haar
PX-DA algorithm.
Since ω(h) in (39) is log-concave, adaptive rejection sampling algorithm
(Gilks and Wild, 1992) can be used to efficiently sample from ω(h). The only
difference between the Haar PX-DA algorithm (Algorithm 5) and the DA
algorithm (Algorithm 3) is a single draw from the univariate density ω(h),
which is easy to sample from. Thus, the computational burden, per iteration,
for the Haar PX-DA algorithm is similar to that of the DA algorithm.
3.3.2
Data augmentation for logistic mixed models
Since the highly cited paper of Albert and Chib (1993) for probit GLMs, there
have been several attempts to construct such a DA sampler for the logistic
model. Recently, Polson et al. (2013) have proposed an efficient DA Gibbs
sampler for Bayesian logistic models with Po´lya–Gamma (PG) latent vari-
ables. A random variable φ has PG distribution with parameters a, b, that
is, φ PG(a, b), if φ ¼
d ð1=ð2π2ÞÞP∞
i¼1φi=½ði  1=2Þ2 + b2=ð4π2Þ, where
φi 
iid Gammaða, 1Þ, a > 0, b  . From Wang and Roy (2018c), the pdf for
PG(a, b) is
pðφ | a, bÞ ¼
cosh b
2
 
h
ia 2a1
ΓðaÞ
X
∞
j¼0
ð1Þj Γð j + aÞ
Γð j + 1Þ
ð2j + aÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πφ3
p
exp
 ð2j + aÞ2
8φ
 φb2
2


,
for φ > 0, where the hyperbolic cosine function cosh ðtÞ ¼ ðet + etÞ=2.
Polson et al.’s (2013) DA technique can be extended to construct a Gibbs
sampler for logistic GLMMs (Example 1). Indeed, from Polson et al. (2013)
we have
ALGORITHM 5 The nth iteration for the Haar PX-DA algorithm.
1: Draw vðnÞ
i

ind TNðx>
i β + z>
i uðn1Þ,1,yiÞ for i ¼ 1,…,m.
2: Draw h from (23).
3: Calculate v0
i ¼ hvi for i ¼ 1,…,m, and draw u(n) from (21) conditional on
v0 ¼ ðv0
1,…,v0
mÞ>, that is, draw
uðnÞ  Nq ðZ>Z + G1Þ1Z>ðv0 XβÞ,ðZ>Z + G1Þ1


:
146
Handbook of Statistics

½ exp ðγiÞyi
½1 + exp ðγiÞ‘i ¼ 2‘i exp ðκiγiÞ
Z ∞
0
exp ½wiγ2
i =2pðwiÞdwi,
(24)
where κi ¼ yi  1=2, i ¼ 1, …, m and p(wi) is the pdf of PG(‘i, 0). Using PG
latent variables w ¼ ðw1, w2, …, wmÞ, we construct the joint density
fðu, wjβ, Λ, yÞ∝
Y
m
i¼1
exp fκiγi  wiγ2
i =2gpðwiÞ
"
#
ϕqðu; 0, GÞ:
(25)
From (24) it follows that the u—marginal of (25) is the target density f(ujβ,
Λ, y). The conditional density for wi is
fðwi|u, β, Λ, yÞ ∝exp ðwiγ2
i =2ÞpðwiÞ:
From Rao and Roy (2021) we then have
wi|u, β, Λ, y 
ind PGð‘i, γiÞ, i ¼ 1, …, m:
(26)
Polson et al. (2013) describe an efficient method for sampling from the PG
distribution. Also, from (25), as in Rao and Roy (2021), the conditional
density of u given β, Λ, w, y is
fðu|β, Λ, w, yÞ ∝Q
m
i¼1
exp
κiz>
i u  wi
2
ðz>
i uÞ
2 + 2ðz>
i uÞðx>
i βÞ
h
i
n
o
exp  1
2 u>G1u
h
i
¼ exp  1
2 u>ðZ>WZ + G1Þu + u>ðZ>κ  Z>WXβÞ
h
i
,
where W is the m  m diagonal matrix with ith diagonal element wi and
κ ¼ ðκ1, …, κmÞ>. Thus, the conditional distribution of u is
ujw, β, Λ, y  Nq ðZ>WZ + G1Þ
1ðZ>κ  Z>WXβÞ, ðZ>WZ + G1Þ
1


: (27)
So, every iteration of the PG sampler for (25) consists of making the draws of
w and u from (26) and (27), respectively.
4
MCMC for Bayesian GLMMs
Here, we consider MALA, HMC and DA algorithms for Bayesian GLMMs.
In the Bayesian framework, one needs to specify the prior distributions of
β and Λ. We assume the Gaussian prior for β given by
fðβÞ ∝exp  1
2 ðβ  μ0Þ>Qðβ  μ0Þ
h
i
,
(28)
where μ0  p and Q is a p  p positive definite matrix.
ALGORITHM 6 The nth iteration for the DA algorithm.
1: Given u(n1), draw ωðnÞ
i

ind PGð‘i,x>
i β + z>
i uðn1ÞÞ, i ¼ 1,…,m:
2: Draw
uðnÞ  Nq ðZ>WZ + G1Þ1ðZ>κ Z >WXβÞ,ðZ>WZ + G1Þ1


with
w ¼ w(n).
MCMC for GLMMs Chapter
7 147

For simplifying the presentations, we assume that the structured matrices
Rj’s to be identity matrices and the covariance matrices Λj’s correspond to
scalar variances. Thus, Λj  Rj ¼ ð1=λjÞIqj, where λj > 0, that is, the compo-
nents in uj are independent with a common variance 1/λj. Let λ ¼ ðλ1, …, λrÞ.
We assume that the prior for λj is
fðλjÞ ∝λaj1
j
ebjλj, j ¼ 1, …, r,
(29)
for aj > 0, bj > 0, that is, a priori λj  Gamma (aj, bj), j ¼ 1, …, r. Finally, we
assume that β and λ are a priori independent and all λjs are also a priori
independent. Hence, the joint posterior density for (u, β, λ) is
f ðu, β, λjyÞ ∝fðy, ujβ, λÞ fðβÞ Q
r
j¼1
fðλjÞ
∝Q
m
i¼1
fðyijβ, uÞ


fðβÞ Q
r
j¼1
λ
aj1+qj=2
j
exp ½ðbj + u>
j uj=2Þλj
"
#
,
(30)
where f(yijβ, u), f(y, ujβ, λ), f(β) and f(λj) are given in (1), (6), (28), and (29),
respectively.
In (28) if Q ¼ 0, then π(β) ∝1, that is, in that case, (28) becomes the
improper uniform prior on β. Similarly, the prior on λ in (29) will be improper
if aj and/or bj takes nonpositive values. Several of the MCMC algorithms pre-
sented here are also applicable to the situations when π(β) ∝1 and/or π(λ) in
(29) is improper. But, we do not pursue the use of improper priors here. Inter-
ested readers may look at Wang and Roy (2018b) and Rao and Roy (2021).
On the other hand, if improper priors are used, then the posterior density
(30) is not guaranteed to be proper. Hence, in such cases, it is necessary to
show that (30) is a proper pdf before carrying out further inference. Also, it
is known that the usual (sample average) Monte Carlo estimators converge
to zero with probability one if the MCMC chain corresponds to an improper
target distribution (Athreya and Roy, 2014). We now present various MCMC
algorithms for exploring the posterior density (30).
4.1
MALA and HMC for Bayesian GLMMs
The logarithm of the posterior density (30) (up to an additive constant) is
log fðu, β, λjyÞ ¼
X
m
i¼1
ðyiξi  bðξiÞÞ=aiðιÞ
½
  ðβ  μ0Þ>Qðβ  μ0Þ=2
+
X
r
j¼1
ðaj  1 + qj=2Þ log λj  ðbj + u>
j uj=2Þλj
h
i
:
(31)
We can construct a MALA for (30) following Algorithm 1 given in Section 3.1
using the derivatives of log fðu, β, log ðλÞjyÞ, but we propose a different
algorithm. From (30), we know that conditional on (u, β, y),
148
Handbook of Statistics

λj 
ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1, …, r:
(32)
Denoting ζ ¼ (u, β) and ζ(n) ¼ (u(n), β(n)), we suggest running a MALA within
Gibbs chain {ζ(n), λ(n)}n	1, where each iteration alternates between a MALA
step for f(ζjλ, y) and a draw of λ from (32). Here, f(ζjλ, y) is the conditional
density of ζ given by
fðζjλ, yÞ ¼ fðu, βjλ, yÞ
∝Q
m
i¼1
fðyijβ, uÞ


exp  1
2 ðβ  μ0Þ>Qðβ  μ0Þ
h
i
exp 
X
r
j¼1
λju>
j uj=2
"
#
:
For the three GLMM examples, ru log fðζjλ,yÞ, the derivatives of
log fðζjλ,yÞ with respect to u are given in Section 3.1 with G ¼ D(λ)1 where
DðλÞ ¼ r
j¼1λjIqj. Here, we derive rβ log fðζjλ,yÞ for these three popular
GLMMs.
Example 1 (Continued). For the binomial-logit link model, from (12) and (31)
it follows that
rβ log fðζjλ, yÞ ¼ X>y  X>ξ  Qðβ  μ0Þ:
Example 2 (Continued). For the binomial-probit model, from (13) and (31) it
follows that
rβ log fðζjλ, yÞ ¼ X>τ1  X>τ2  Qðβ  μ0Þ:
Example 3 (Continued). For the Poisson-log GLMM, from (14) and (31)
we have
rβ log fðζjλ, yÞ ¼ X>y  X> exp ðγÞ  Qðβ  μ0Þ:
Also, in this case, we propose a HMC within Gibbs chain {ζ(n), λ(n)}n	1,
where each iteration alternates between a HMC step for f(ζjλ, y) as in
Algorithm 2 for the Hamiltonian function Hðζ, ρÞ ¼  log fðζjλ, yÞ + ½ðp + qÞ
log ð2πÞ + log ðjMjÞ + ρ>M1ρ=2 with ρ  N(0, M) for a (p + q)  (p + q)
positive definite matrix M and a draw of λ from (32).
ALGORITHM 7 The nth iteration for the MALA.
1: Given (ζ(n1), λ(n1)) draw ζ0  Nðζðn1Þ + Erlog f ðζðn1Þjλðn1Þ,yÞ=2,EIp + qÞ.
2: Draw δ  Uniform (0, 1). If δ < α(ζ(n1), ζ0) then set ζ(n)  ζ0, else set ζ(n)  
ζ(n1). Here, α(, ) is obtained from (11) by replacing f(ujy) with f(ζjλ(n1), y)
and k(u, u0) with k(ζ, ζ0) which is Nðζ + Erlog f ðζjλðn1Þ,yÞ=2,EIp + qÞ density
evaluated at ζ0.
3: Draw λðnÞ
j

ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1,…,r with u ¼ u(n).
MCMC for GLMMs Chapter
7 149

4.2
Data augmentation for Bayesian GLMMs
As mentioned in Section 3.3, for a successful DA for Bayesian GLMMs, we
need to construct a joint density f (u, β, λ, djy) with augmented variables d
whose (u, β, λ)  marginal is the density (30). In this section, we show that
the augmented variables derived in Sections 3.3.1 and 3.3.2 can be used for con-
structing DA for the Bayesian probit and logistic mixed models, respectively.
4.2.1
Data augmentation for Bayesian probit mixed models
As in Section 3.3.1, we consider a vector of Bernoulli random variables
ðY1, Y2, …, YmÞ and assume PðYi ¼ 1Þ ¼ ΦðγiÞ, i ¼ 1, …, m. Using the latent
variables v introduced in Section 3.3.1, Wang and Roy (2018b) introduce
the joint density
fðu, β, λ, vjyÞ ∝Q
m
i¼1
ϕðvi; x>
i β + z>
i u, 1Þ 1ð0,∞Þ vi
ð Þ
	

yi 1 ∞,0
ð
 vi
ð Þ
	

1yi


 ϕqðu; 0, DðλÞ1ÞfðβÞ Q
r
j¼1
fðλjÞ:
(33)
From (19) it follows that
Z
mfðu, β, λ, vjyÞdv ¼ fðu, β, λjyÞ:
Thus, (u, β, λ)  marginal of (33) is the target density (30).
From (33), the conditional density of β given u, λ, v, y is
fðβ|u, λ, v, yÞ ∝Q
m
i¼1
exp
 1
2 ðx>
i βÞ
2  2ðx>
i βÞðvi  z>
i uÞ
h
i
n
o


 exp  1
2 ðβ  μ0Þ>Qðβ  μ0Þ
h
i
∝exp  1
2 β>ðX>X + QÞβ  2β>ðX>v  X>Zu + Qμ0Þ


h
i
:
Thus, the conditional distribution of β given u, λ, v, y is
β | u, λ, v, y  NððX>X + QÞ
1ðX>v + Qμ0  X>ZuÞ, ðX>X + QÞ
1Þ:
(34)
The conditional densities of v, u, and λ are given in (20), (21), and (32),
respectively. Using these conditional distributions, we develop the following
full Gibbs sampler for Bayesian probit mixed models.
ALGORITHM 8 The nth iteration of the full Gibbs sampler.
1: Draw λðnÞ
j

ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1,…,r with u ¼ u(n1).
2: Draw vi 
ind TNðγi,1,yiÞ, i ¼ 1,…,m, with u ¼ u(n1) and β ¼ β(n1).
3: Draw u(n)  (21) with λ ¼ λ(n) and v ¼ v(n).
4: Draw β(n)  (34) with v ¼ v(n) and u ¼ u(n).
150
Handbook of Statistics

It is known that blocking parameters can improve the performance of a
Gibbs sampler in terms of reducing its operator norm (Liu et al., 1994). When
one or more variables are correlated, sampling them jointly can generally
improve efficiency of the MCMC algorithms. On the other hand, blocking
may result in complex conditional distributions that are not easy to sample
from. For the probit linear mixed models, Wang and Roy (2018b) show that
an efficient two-block Gibbs sampler can be constructed by using the two
blocks, η ≡(β>,u>)> and (v,λ). Below we present Wang and Roy’s (2018b)
block Gibbs sampler.
Let E ¼ (X, Z) with the ith row being e>
i
for i ¼ 1, …, n . Thus,
γi ¼ x>
i β + z>
i u ¼ e>
i η. From (33), we have the conditional density of η given
λ, v, y as
fðηjλ, v, yÞ ∝Q
m
i¼1
exp  1
2 vi  x>
i β  z>
i u

2
h
i

 Q
r
j¼1
exp  1
2 λju>
j uj
h
i
"
#
 exp  1
2 β  μ0
ð
Þ>Q β  μ0
ð
Þ
h
i
∝exp  1
2 v  Eη
ð
Þ> v  Eη
ð
Þ
h
i
 exp  1
2 η>AðλÞη + ηTθ
h
i
,
(35)
where
θðp+qÞ1 ¼
Qμ0
0q1


and AðλÞðp+qÞðp+qÞ ¼
Q
0
0
DðλÞ


:
(36)
Thus, from (35) we have
ηjλ, v, y  Np+qððE>E + AðλÞÞ
1ðE>v + θÞ, ðE>E + AðλÞÞ
1Þ:
(37)
From (33) note that conditional on (η, y), v and λ are independent. Thus, (37)
together with (20) and (32) results in the following two-block Gibbs sampler
for exploring the joint density (33).
As in Section 3.3.1, we now construct a Haar PX-DA algorithm improving
the block Gibbs sampler (Algorithm 9). In order to construct the extra step in
the Haar PX-DA, we derive the marginal posterior density of (λ, v) from the
joint density (33) as
ALGORITHM 9 The nth iteration for the two block Gibbs sampler.
1: Draw λðnÞ
j

ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1,…,r with u ¼ u(n1), and
independently draw vðnÞ
i
jηðn1Þ,y 
ind TNðe>
i ηðn1Þ,1,yiÞ for i ¼ 1,…,m.
2: Draw η(n) from (37), that is,
ηðnÞ  N
E>E + AðλðnÞÞ

1 E>vðnÞ + θ


, E>E + AðλðnÞÞ

1


:
MCMC for GLMMs Chapter
7 151

f λ, vjy
ð
Þ ¼
Z
p+q fðη, λ, vjyÞdη
∝Q
m
i¼1
1ð0,∞Þ vi
ð Þ
h
iyi 1 ∞,0
ð
 vi
ð Þ
h
i1yi Q
r
j¼1
λ
qj
2
j
 exp
 1
2 v>E1v  2v>E2  θ>ðE>E + AðλÞÞ
1θ
h
i
n
o Q
r
j¼1
fðλjÞ,
(38)
where
E1 ¼ Im  E E>E + AðλÞ

1E>
h
i
and E2 ¼ E E>E + AðλÞ

1θ:
For constructing an efficient sandwich step, Wang and Roy (2018b) let
the group ψ act on V  r
+
through a group action T*ðv, λÞ ¼ ðhv, λÞ ¼
ðhv1, hv2, …, hvm, λÞ. With the group action defined this way, it can be shown
that the Lebesgue measure on V  r
+ is relatively left invariant with multi-
plier χ(h) ¼ hm (Hobert and Marchev, 2008; Roy, 2014). Wang and Roy
(2018b) then consider a probability density function ω*(h) on ψ where
ω* h
ð Þdh ∝f λ, hvjy
ð
Þχ h
ð ÞνðdhÞ
∝hm1 exp
 1
2 h2v>E1v  2hv>E2
	

n
o
dh:
(39)
Given (λ, v), ω* h
ð Þ is a valid density since E1 is a positive definite matrix.
Below are the three steps involved in every iterations of Wang and Roy’s
(2018b) Haar PX-DA algorithm.
The adaptive rejection sampling algorithm (Gilks and Wild, 1992) can be
used to sample from ω*(h) as it is log-concave. In general, the PX-DA algo-
rithm is known to be theoretically more efficient than the DA (Hobert and
Marchev, 2008; Roy, 2012b). In the context of some numerical examples of
the probit GLMs, Roy and Hobert (2007) showed that huge gains in efficiency
are possible by using the Haar PX-DA algorithm instead of the DA algorithm
of Albert and Chib (1993) (see also Roy, 2012a, 2014, for comparisons of DA
and PX-DA algorithms for the probit GLM). A numerical comparison of the
three samplers presented in this section for the probit mixed models is given
in Section 5.
ALGORITHM 10 The nth iteration for the Haar PX-DA algorithm.
1: Draw λðnÞ
j

ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1,…,r with u ¼ u(n1), and
independently draw vðnÞ
i
jηðn1Þ,y 
ind TNðe>
i ηðn1Þ,1,yiÞ for i ¼ 1,…,m.
2: Draw h from (39).
3: Calculate v0
i ¼ hvi for i ¼ 1,…,m, and draw η(n) from (37) conditional on
v0 ¼ ðv0
1,…,v0
mÞ>, that is, draw
ηðnÞ  N
E>E + AðλðnÞÞ

1 E>v0 + θ
ð
Þ, E>E + AðλðnÞÞ

1


:
152
Handbook of Statistics

4.2.2
Data augmentation for Bayesian logistic mixed models
Define the joint posterior density of u, β, λ, w given y as
fðu, β, λ, w | yÞ ∝
Q
m
i¼1
exp fκiγi  wiγ2
i =2gpðwiÞ
"
#
ϕqðu; 0, DðλÞ1Þ fðβÞ fðλÞ
¼
Q
m
i¼1
exp fkiðx>
i β + z>
i uÞ  wiðx>
i β + z>
i uÞ2=2gpðwiÞ
"
#
 ϕqðu; 0, DðλÞ1Þ  Q
r
j¼1
λaj1
j
exp ðbjλjÞ
 exp  1
2 ðβ  μ0Þ>Qðβ  μ0Þ
h
i
,
(40)
where (40) follows from the priors on β and λ given in (28) and (29), respec-
tively. From (24) it follows that the (u, β, λ)  marginal of (40) is the target
density (30).
Based on (40), as in Rao and Roy (2021), the conditional density of β
given u, λ, w, y is
f ðβ|u, λ, w, yÞ ∝Q
m
i¼1
exp κix>
i β  wiðx>
i βÞ2=2  wiðx>
i βÞðz>
i uÞ
h
i
 exp  1
2 ðβ  μ0Þ>Qðβ  μ0Þ
h
i
∝exp  1
2 β>ðX>WX + QÞβ + β>ðX>κ + Qμ0  X>WZuÞ
h
i
:
Thus, the conditional distribution of β given u, λ, w, y is
β|u, λ, w, y  NððX>WX + QÞ
1ðX>κ + Qμ0  X>WZuÞ, ðX>WX + QÞ
1Þ:
(41)
The conditional densities of w, u, and λ are given in (26), (27), and (32),
respectively. Rao and Roy (2021) use these conditional distributions to con-
struct the following full Gibbs sampler for Bayesian logistic mixed models.
Rao and Roy (2021) also construct an efficient two-block Gibbs sampler
with the two blocks being η and (w, λ). From (40), the conditional density
of η given λ, w, y is given by
ALGORITHM 11 The nth iteration of the full Gibbs sampler.
1: Draw λðnÞ
j

ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1,…,r with u ¼ u(n1).
2: Draw wðnÞ
i

ind PGð‘i, γiÞ, i ¼ 1,…,m with u ¼ u(n1) and β ¼ β(n1).
3: Draw u(n)  (27) with λ ¼ λ(n), β ¼ β(n1) and w ¼ w(n).
4: Draw β(n)  (41) with w ¼ w(n) and u ¼ u(n).
MCMC for GLMMs Chapter
7 153

fðη|λ, w, yÞ ∝Q
n
i¼1
exp κie>
i η  wiðe>
i ηÞ2=2
h
i
exp u>DðλÞu=2
½

 exp ðβ  μ0Þ>Qðβ  μ0Þ=2
h
i
:
Thus, the conditional distribution of η given λ, w, y is given by
η|λ, w, y  NððE>WE + AðλÞÞ
1ðE>κ + θÞ, ðE>WE + AðλÞÞ
1Þ,
(42)
where A(λ) and θ are defined in (36).
(42) together with (26) and (32) result in the following two-block Gibbs
sampler.
A comparison of the full Gibbs sampler (Algorithm 11) and the block Gibbs
sampler (Algorithm 11) in the context of some numerical examples as the
dimensions of the design matrices vary can be found in Rao and Roy (2021).
5
A numerical example
In this section, we consider a publicly available simulated data set named
“pbDat” from the R package pbnm to compare the full Gibbs sampler
(Algorithm 8), the block Gibbs sampler (Algorithm 9) and the Haar PX-DA algo-
rithm (Algorithm 10) for the probit mixed models. This data set has m ¼ 100
binary observations. There are p ¼ 3 covariates including an intercept term.
There is r ¼ 1 random effect with q1 ¼ 12 levels. We analyze the data set by fit-
ting probit linear mixed models with a normal prior (28) on β with μ0 ¼ 0 and
Q ¼ 0.001I3 and a Gamma prior (29) on λ1 with a1 ¼ 0.01 and b1 ¼ 0.01. We
ran the three samplers for N ¼ 100, 000 iterations starting at an initial value
(β(0), u(0)) with burn-in B ¼ 20, 000 iterations. Here β(0) is the estimate of β
obtained by fitting a probit linear model without any random effect. The initial
value u(0) is a sample drawn from Nð0, ð1=λð0Þ
1 ÞI12Þ where 1=λð0Þ
1
is the estimate
of random effect variance component obtained from the R package lme4. We
use the R package ars to make draws from the density (39).
Next, we compare the performance of the full Gibbs (FG) sampler, the
block Gibbs (BG) sampler and the Haar PX-DA algorithm in the context of
this pbDat data. The samplers are compared using lag k autocorrelation func-
tion (ACF) values k ¼ 1, …, 5, effective sample size (ESS) and multivariate
ESS (mESS) (See Roy (2020) for a simple introduction to the different con-
vergence diagnostic measures.). The ESS and mESS are calculated using
ALGORITHM 12 The nth iteration of the block Gibbs sampler.
1: Draw λðnÞ
j

ind Gammaðaj + qj=2, bj + u>
j uj=2Þ, j ¼ 1,…,r with u ¼ u(n1), and
independently draw wðnÞ
i

ind PGð‘i,e>
i ηðn1ÞÞ, i ¼ 1,…,m.
2: Draw η(n)  (42) with λ ¼ λ(n) and w ¼ w(n).
154
Handbook of Statistics

the R package mcmcse. We also compute the mean squared jumps (MSJ)
(defined as PN
i¼B+1 k βði+1Þ  βðiÞk2=ðN  BÞ for the β variable, and similarly
for the other variables. Here, kk denotes the Euclidean norm. Lower ACF
values and higher ESS and MSJ numbers are preferred. Table 1 provides
the values of ACF for the three samplers. Better performance of the Haar
PX-DA and the block Gibbs samplers compared to the full Gibbs sampler is
observed from their smaller ACF values. Table 2 provides the ESS values
of the intercept parameter, first two regression coefficients and λ1. It also
gives the mESS values for u and (β, λ1). Again, better efficiency of the Haar
PX-DA and the block Gibbs samplers compared to the full Gibbs sampler is
TABLE 1 ACF for different samplers for the pbDat data.
Parameter
Sampler
lag 1
lag 2
lag 3
lag 4
lag 5
β0
FG
0.957
0.915
0.876
0.838
0.801
BG
0.085
0.062
0.042
0.023
0.026
Haar
0.089
0.061
0.038
0.030
0.020
β1
FG
0.724
0.539
0.410
0.317
0.249
BG
0.701
0.510
0.386
0.297
0.234
Haar
0.680
0.472
0.334
0.243
0.177
β2
FG
0.836
0.715
0.622
0.547
0.488
BG
0.812
0.681
0.584
0.509
0.449
Haar
0.720
0.528
0.395
0.297
0.223
λ1
FG
0.682
0.597
0.549
0.486
0.424
BG
0.635
0.521
0.442
0.375
0.320
Haar
0.636
0.442
0.344
0.266
0.207
TABLE 2 Multivariate ESS and ESS for different samplers for the pbDat
data.
Sampler
mESS
(β λ)
mESS
(u)
ESS
(β0)
ESS
(β1)
ESS
(β2)
ESS
(λ1)
FG
4915
14,036
1565
8362
3481
4558
BG
13,142
19156
47,437
8867
4449
7093
Haar
18,865
21,940
47,964
12,891
10,921
11,177
MCMC for GLMMs Chapter
7 155

demonstrated from their larger ESS and mESS values. From Table 3, it can be
seen that the Haar PX-DA sampler leads to higher MSJ values than the full
Gibbs sampler. Also, the block Gibbs sampler results in higher MSJ values
than the full Gibbs sampler with the exception of λ1. Thus, Table 3 also cor-
roborates better mixing of the Haar PX-DA and the block Gibbs samplers than
the full Gibbs sampler.
6
Discussion
In this article, we have presented several MCMC algorithms for both frequen-
tist a Bayesian GLMMs. While some of these algorithms discussed here are
available in the literature, others are developed here. Since these algorithms
result in Harris ergodic Markov chains, the (Monte Carlo) sample averages
are consistent estimators of the means with respect to the corresponding target
densities. On the other hand, in practice, it is important to ascertain the errors
associated with these Monte Carlo estimates. An advantage of being able to
calculate a valid standard error is that it can be used to decide “when to stop”
running the MCMC chain (Roy, 2020). A valid standard error for the Monte
Carlo estimates can be formed if a central limit theorem is available for the
time average estimator. Establishing geometric ergodicity (GE) of the under-
lying Markov chains is the most standard method for guaranteeing a central
limit theorem for MCMC based estimators (Meyn and Tweedie, 1993). GE
is also used for consistently estimating the asymptotic variance in the central
limit theorem (Vats et al., 2019).
For several of the MCMC algorithms presented here, GE has been estab-
lished in the literature. For example, Roy and Zhang (2022) demonstrate
GE of Markov chains underlying different MALA for GLMMs. The GE of
the PG block Gibbs sampler for Bayesian logistic mixed models under proper
and improper priors has been established in Wang and Roy (2018a) and Rao
and Roy (2021), respectively. Wang and Roy (2018b) derive conditions under
which the block Gibbs sampler and the Haar PX-DA algorithm for the probit
mixed models are geometrically ergodic when improper priors are assumed on
the regression coefficients and the variance components.
TABLE 3 Mean squared jumps for different samplers for the pbDat data.
FG
BG
Haar
β
u
λ
β
u
λ
β
u
λ
0.018
0.224
0.232
0.154
0.613
0.198
0.155
0.622
0.235
156
Handbook of Statistics

It would be interesting to construct and study efficient DA samplers
for other GLMMs, for example the GLMMs with the robit link (Roy,
2012a). A potential future study can be to extend Wang and Roy’s (2018b)
GE results to the probit mixed models with proper priors. Another potential
project is to study convergence properties of the HMC chains in the context
of GLMMs.
References
Albert, J.H., Chib, S., 1993. Bayesian analysis of binary and polychotomous response data. J. Am.
Stat. Assoc. 88, 669–679.
Athreya, K.B., Roy, V., 2014. Monte Carlo methods for improper target distributions. Electron.
J. Stat. 8, 2664–2692.
Booth, J.G., Hobert, J.P., 1999. Maximizing generalized linear mixed model likelihoods with an
automated Monte Carlo EM algorithm. J. R. Stat. Soc. B (Stat. Methodol.) 61, 265–285.
Breslow, N.E., Clayton, D.G., 1993. Approximate inference in generalized linear mixed models.
J. Am. Stat. Assoc. 88, 9–25.
B€urkner, P.-C., 2017. brms: An R package for Bayesian multilevel models using Stan. J. Stat.
Soft. 80, 1–28.
Carpenter, B., Gelman, A., Hoffman, M.D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M.,
Guo, J., Li, P., Riddell, A., 2017. Stan: a probabilistic programming language. J. Stat. Soft.
76, 1–32.
Christensen, O.F., 2004. Monte Carlo maximum likelihood in model based geostatistics. J. Com-
put. Graph. Stat. 13, 702–718.
Duane, S., Kennedy, A.D., Pendleton, B.J., Roweth, D., 1987. Hybrid Monte Carlo. Phys. Lett.
B 195, 216–222.
Evangelou, E., Roy, V., 2019. Estimation and prediction for spatial generalized linear mixed mod-
els with parametric links via reparameterized importance sampling. Spatial Stat. 29, 289–315.
Gamerman, D., 1997. Sampling from the posterior distribution in generalized linear mixed mod-
els. Stat. Comput. 7, 57–68.
Geyer, C.J., 1994. On the convergence of Monte Carlo maximum likelihood calculations. J. R.
Stat. Soc. B 56, 261–274.
Geyer, C.J., Thompson, E.A., 1992. Constrained Monte Carlo maximum likelihood for dependent
data. J. R. Stat. Soc. B 54, 657–699.
Gilks, W.R., Wild, P., 1992. Adaptive rejection sampling for Gibbs sampling. Appl. Stat. 41,
337–348.
Girolami, M., Calderhead, B., 2011. Riemann Manifold Langevin and Hamiltonian Monte Carlo
methods. J. R. Stat. Soc. B (Stat. Methodol.) 73, 123–214.
Hobert, J.P., Marchev, D., 2008. A theoretical comparison of the data augmentation, marginal
augmentation and PX-DA algorithms. Ann. Stat. 36, 532–554.
Hoffman, M.D., Gelman, A., 2014. The No-U-Turn sampler: adaptively setting path lengths in
Hamiltonian Monte Carlo. J. Mach. Learn. Res. 15, 1593–1623.
Jiang, J., Nguyen, T., 2007. Linear and Generalized Linear Mixed Models and Their Applications.
vol. 1 Springer.
Liu, J.S., Wu, Y.N., 1999. Parameter expansion for data augmentation. J. Am. Stat. Assoc. 94,
1264–1274.
MCMC for GLMMs Chapter
7 157

Liu, J.S., Wong, W.H., Kong, A., 1994. Covariance structure of the Gibbs sampler with applica-
tions to comparisons of estimators and augmentation schemes. Biometrika 81, 27–40.
McCulloch, C.E., 1994. Maximum likelihood variance components estimation for binary data.
J. Am. Stat. Assoc. 89, 330–335.
McCulloch, C.E., 1997. Maximum likelihood algorithms for generalized linear mixed models.
J. Am. Stat. Assoc. 92, 162–170.
Meyn, S.P., Tweedie, R.L., 1993. Markov Chains and Stochastic Stability. Springer Verlag,
London.
Neal, R.M., 2011. Mcmc using Hamiltonian dynamics. In: Handbook of Markov chain Monte
Carlo, CRC Press, Boca Raton, FL, pp. 113–162.
Polson, N.G., Scott, J.G., Windle, J., 2013. Bayesian inference for logistic models using Po´lya-
Gamma latent variables. J. Am. Stat. Assoc. 108, 1339–1349.
Rao, Y., Roy, V., 2021. Block Gibbs samplers for logistic mixed models: convergence properties
and a comparison with full Gibbs samplers. Electron. J. Stat. 15, 5598–5625.
Roberts, G.O., Rosenthal, J.S., 1998. Optimal scaling of discrete approximations to Langevin dif-
fusions. J. R. Stat. Soc. B (Stat. Methodol.) 60, 255–268.
Roberts, G.O., Tweedie, R.L., 1996. Exponential convergence of Langevin distributions and their
discrete approximations. Bernoulli 2, 341–363.
Roy, V., 2012a. Convergence rates for MCMC algorithms for a robust Bayesian binary regression
model. Electron. J. Stat. 6, 2463–2485.
Roy, V., 2012b. Spectral analytic comparisons for data augmentation. Stat. Prob. Lett. 82,
103–108.
Roy, V., 2014. Efficient estimation of the link function parameter in a robust Bayesian binary
regression model. Comput. Stat. Data Anal. 73, 87–102.
Roy, V., 2020. Convergence diagnostics for Markov chain Monte Carlo. Annu. Rev. Stat. Appl.
7, 387–412.
Roy, V., Hobert, J.P., 2007. Convergence rates and asymptotic standard errors for MCMC algo-
rithms for Bayesian probit regression. J. R. Stat. Soc., B 69, 607–623.
Roy, V., Zhang, L., 2022. Convergence of position-dependent MALA with application to condi-
tional simulation in GLMMs. J. Comput. Graph. Stat. to appear.
Roy, V., Evangelou, E., Zhu, Z., 2016. Efficient estimation and prediction for the Bayesian binary
spatial model with flexible link functions. Biometrics 72, 289–298.
Roy, V., Tan, A., Flegal, J., 2018. Estimating standard errors for importance sampling estimators
with multiple Markov chains. Stat. Sin. 28, 1079–1101.
Stramer, O., Roberts, G.O., 2007. On Bayesian analysis of nonlinear continuous-time autoregres-
sion models. J. Time Ser. Anal. 28, 744–762.
Tanner, M.A., Wong, W.H., 1987. The calculation of posterior distributions by data
augmentation(with discussion). J. Am. Stat. Assoc. 82, 528–550.
Vats, D., Flegal, J.M., Jones, G.L., 2019. Multivariate output analysis for Markov chain Monte
Carlo. Biometrika 106, 321–337.
Wang, X., Roy, V., 2018a. Analysis of the Po´lya-Gamma block Gibbs sampler for Bayesian logis-
tic linear mixed models. Stat. Prob. Lett. 137, 251–256.
Wang, X., Roy, V., 2018b. Convergence analysis of the block Gibbs sampler for Bayesian probit
linear mixed models with improper priors. Electron. J. Stat. 12, 4412–4439.
Wang, X., Roy, V., 2018c. Geometric ergodicity of Po´lya-Gamma Gibbs sampler for Bayesian
logistic regression with a flat prior. Electron. J. Stat. 12, 3295–3311.
Wei, G.C., Tanner, M.A., 1990. A Monte Carlo implementation of the EM algorithm and the poor
man’s data augmentation algorithms. J. Am. Stat. Assoc. 85, 699–704.
158
Handbook of Statistics

Wolfinger, R., O’connell, M., 1993. Generalized linear mixed models a pseudo-likelihood
approach. J. Stat. Comput. Simul. 48, 233–243.
Zeger, S.L., Karim, M.R., 1991. Generalized linear models with random effects; a Gibbs sampling
approach. J. Am. Stat. Assoc. 86, 79–86.
Zhang, L., 2022. Hamiltonian and Langevin Monte Carlo methods with application to GLMMs
(Ph.D. thesis), Iowa State University.
MCMC for GLMMs Chapter
7 159

This page intentionally left blank

Chapter 8
Sparsity-aware Bayesian
inference and its applications
Geethu Josepha, Saurabh Khannab, Chandra R. Murthyc,*,
Ranjitha Prasadd, and Sai Subramanyam Thootac
aDelft University of Technology (TU Delft), Delft, The Netherlands
bIndian Institute of Technology (IIT) Roorkee, Roorkee, India
cIndian Institute of Science (IISc), Bangalore, India
dIndraprastha Institute of Information Technology Delhi (IIIT-Delhi), New Delhi, India
∗Corresponding author: e-mail: cmurthy@iisc.ac.in
Abstract
The emergence of compressive sensing and the associated ‘1 recovery algorithms and
theory have generated considerable excitement and interest in their applications. This
chapter will examine recent developments and a complementary set of tools based on
a Bayesian framework to address the general problem of sparse signal recovery and
the challenges associated with it. Bayesian methods offer superior performance com-
pared to convex optimization-based methods and are parameter tuning-free. They also
have the flexibility necessary to deal with a diverse range of measurement modalities
and structured sparsity in signals than hitherto possible. Parsimonious signal representa-
tion using overcomplete dictionaries for compression, estimation of sparse communica-
tion channels with large delay spread as in underwater acoustics, low-dimensional
representation of MIMO wireless channels, brain imaging techniques, such as MEG
and EEG, are a few examples.
We provide a mathematically rigorous and in-depth overview of this fascinating
area within sparse signal recovery. We highlight the generality and flexibility of Bayes-
ian approaches and show how it greatly facilitates their deployment in communications-
related applications, even though they generally lead to nonconvex optimization
problems. Further, we show that, by reinterpreting the Bayesian cost function as a tech-
nique to perform covariance matching, one can develop new, ultrafast Bayesian algo-
rithms for sparse signal recovery. As an example application, we discuss the utility of
these algorithms in the context of 5G communications with several case studies includ-
ing wideband time-varying channel estimation and low-resolution analog-to-digital
conversion-based signal recovery.
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.07.003
Copyright © 2022 Elsevier B.V. All rights reserved.
161

Keywords: Bayesian inference, Covariance matching, Dictionary learning, Quantized
compressed sensing, Sparse Bayesian learning, Structured sparsity, Wireless channel
estimation
1
Introduction
Consider the problem of recovering a high-dimensional vector x  N from
noisy, low-dimensional linear measurements:
y ¼ Ax + w,
(1)
where A  mN is called the measurement matrix or dictionary, y  m is
called the measurement (or observation) vector, and w  m is the additive
noise. Here, N > m, and we will be particularly interested in the case where
N ≫m. We will also assume that Rank A
f g ¼ m, i.e., there are no linearly
dependent rows in the measurement matrix. As stated, this is an under-
determined problem, and there are infinitely many solutions which minimize
ky Axk, where kk denotes the Euclidean or ‘2 norm of the argument. There-
fore, without imposing additional constraints on the structure of the vector x
that we seek, it is not possible to uniquely recover x from (1). The structure
we will focus on in this chapter is that of sparsity, namely, that most of the
entries of x are zero, but the locations and values of the nonzero entries
are unknown.
The problem of finding the sparsest solution to (1) can be mathematically
written as
P0 :
min
x
kxk0 subject to ky  Axk  β,
(2)
where kk0 denotes the ‘0 norm of a vector and counts the number of nonzero
entries in the vector, and β is a positive number whose value depends on the
noise variance.
The problem P0 in (2) is NP-hard in general (Natarajan, 1995); finding its
solution requires a combinatorial search, which is of exponential complexity
in the problem size. As such, it cannot be solved even for moderately sized
matrices. Another issue with solving P0 directly is that the solution is not
robust to noise. Therefore, we quickly set this problem aside and look for
alternative suboptimal approaches that work well most of the time.a Happily,
the theory of compressed sensing allows us to recover sparse signals from
noisy underdetermined linear measurements using low-complexity algorithms
(i.e., algorithms whose complexity scales polynomially, rather than exponen-
tially, in the problem size).
aThis is, of course, a fast-and-loose statement at this point, but its meaning will become clear as
we proceed.
162
Handbook of Statistics

Applications: Solving for sparse vectors using measurements given by (1)
has a variety of applications in diverse fields. Examples include signal represen-
tation, functional approximation, spectral estimation, ElectroEncephaloGram
(EEG) and MagnetoEncephaloGraphy (MEG) signal processing, medical imag-
ing (Gamper et al., 2008; Gorodnitsky et al., 1995; Wipf et al., 2010; Zhang
et al., 2014), speech signal processing, radar signal processing (Ender, 2010;
Malioutov et al., 2005), and wireless channel estimation (Prasad et al., 2014a).
We will touch upon the wireless channel estimation-related application in this
chapter. Wideband wireless channels can be well modeled via their impulse
responses. The duration of the impulse response depends on the so-called chan-
nel delay spread, which is the maximum delay between the first arriving path
and the last arriving path at a receiver. Moreover, when represented in discrete
time using the sampling rate (typically, the sampling rate is set to be at least
twice the signal bandwidth), the impulse response has only a few nonzero taps
within the delay spread. Due to this, the time-domain representation of the chan-
nel is sparse. Therefore, sparse signal recovery techniques can be used to esti-
mate the channel impulse response using a small number of pilot or training
symbols emitted by the transmitter. We will describe this in more detail in the
sequel.
1.1
Quick summary of existing methods for sparse signal recovery
We briefly recap some of the existing methods for sparse recovery, their pros
and cons, and motivate the need for Bayesian methods. Broadly, sparse recov-
ery algorithms can be classified into greedy methods, relaxation-based meth-
ods, and iterative reweighted methods. Greedy methods include techniques
such as matching pursuit (Mallat and Zhang, 1993) and orthogonal matching
pursuit (Tropp and Gilbert, 2007). Convex relaxation-based methods are
based on approximating the ‘0 norm with a more tractable cost function, such
as the ‘1 norm. Algorithms such as basis pursuit denoising (Chen et al., 2001),
least absolute shrinkage and selection operator (LASSO) (Efron et al., 2004;
Tibshirani, 1996), and the Dantzig selector (Candes and Tao, 2007) are based
on such convex relaxations. Other approaches include threshold-based meth-
ods, such as iterative soft/hard thresholding (Blumensath and Davies, 2009),
hard thresholding pursuit (Foucart, 2011), compressive sampling matching
pursuit (Needell and Tropp, 2009), etc., which combine convex relaxation-
based methods with greedy methods, thereby benefiting from the speed of
greedy methods and the performance of convex optimization-based methods.
Another class of algorithms aim to find the maximum a posteriori probability
(MAP) point estimate of the vector x under a particular choice of prior. Typi-
cally, heavy-tailed priors on x promote sparse solutions, but often such priors
lead to nonconvex optimization problems for finding the MAP estimates.
Examples of such methods include the FOcal Underdetermined System Solver
(Gorodnitsky and Rao, 1997), Bayesian compressive sensing using Laplace
Sparsity-aware Bayesian inference and its applications Chapter
8 163

priors (Babacan et al., 2009), sampling-based Monte Carlo approaches (George
and McCulloch, 1993, 1997; Olshausen and Millman, 1999), etc. Iterative
reweighted ‘1 and ‘2 methods attempt to minimize the nonconvex cost function
using successive convex approximations, where, in each iteration, an appropri-
ately weighted convex approximation of the cost function is minimized
(Chartrand and Yin, 2008).
It turns out, quite remarkably, that under certain conditions, the solutions
to (2) with β ¼ 0 and minxkxk1 subject to y ¼ Ax coincide. This also holds
true for other nonconvex optimization problems that arise from imposing
sparsity-promoting priors on x, and under similar conditions. One such condi-
tion is that the matrix A must satisfy a restricted isometry property (Candes,
2008). Loosely speaking, this property mandates that the Euclidean length
of sparse vectors should be approximately preserved under the linear transfor-
mation represented by A. It turns out that matrices with random, independent,
and identically distributed (i.i.d.) entries often satisfy the restricted isometry
property, provided the number of rows, m, in the matrix is of the order
s log N, where s is the maximum sparsity level (the maximum number of non-
zero entries) in the N-length vector x. Another condition is that the mutual
coherence of the measurement matrix A, defined as the maximum absolute
inner product between any two columns of A, should be sufficiently small.
We refer the reader to Foucart and Rauhut (2013) for a detailed discussion
of different sparse recovery algorithms and their performance guarantees.
1.2
Bayesian approaches: Motivation and related literature
Despite the excellent progress on developing low-complexity and high-
performing algorithms for sparse signal recovery, the above-described
approaches have several drawbacks, which limit their utility in practical
systems.
The first is the requirement that the measurement matrix satisfies condi-
tions such as the restricted isometry property or a mutual coherence property.
In practice, the measurement matrix is often determined by the physical sys-
tem used to acquire measurements, and cannot be chosen arbitrarily. As a
result, the columns of A could be highly correlated, which disrupts the ‘0–‘1
equivalence described above. Second, the recovery guarantees available in
the literature are oblivious to the specific measurement matrix used, provided
it satisfies conditions such as the mutual coherence property or the restricted
isometry property. On one hand, testing whether A satisfies the restricted
isometry property is itself an NP-hard problem (Bandeira et al., 2013). More-
over, it is also hard to relate the estimation error to the measurement matrix.
Third, the algorithms work well for certain types of sparse vectors, but not all.
For example, the performance of the basis pursuit algorithm, which is based
on ‘1 relaxation, is independent of the magnitudes of the nonzero coefficients.
As a result, it cannot take advantage of more “favorable” conditions where
some nonzero coefficients are significantly larger than the other. The OMP
164
Handbook of Statistics

algorithm works very well when the nonzero coefficients span a large
dynamic range, but its performance is poorer than basis pursuit when the
magnitudes of the nonzero entries are nearly equal. Fourth, the approaches
described above do not always naturally extend to cases where additional
structure beyond sparsity is available. For example, in the multiple measure-
ment vector (MMV) version of the sparse recovery problem, several sparse
vectors are observed using either the same or independent measurement matri-
ces, and these sparse vectors could have correlated nonzero entries between/
within each of the sparse vectors. The noise vector w could have correlated
entries, or the dictionary matrix A could have unknown parameters embedded
in it or it may be partially unknown. Extending traditional approaches to these
cases are usually hand-crafted, tailor-made solutions to these scenarios, and
the performance guarantees generally do not extend to them. Moreover, the
resulting constrained sparse recovery problem is typically solved using solvers
that do not provide insight into the solution obtained. These drawbacks in
greedy and relaxation-based algorithms motivate us to look for alternative,
nonconvex, and Bayesian approaches to sparse signal recovery, which we
describe in the next section. Additionally, Bayesian methods lead to the pos-
terior distribution of the sparse vector itself, instead of the point estimates of
the sparse vector. This allows us to quantify the uncertainty in the estimates.
Moreover, it is well known in machine learning that Bayesian methods abate
overfitting as they can utilize the prior information in lieu of the observations.
In a Bayesian framework, the sparsity constraint is incorporated by choos-
ing a suitable prior on the sparse vector. The most popular methods of prob-
lem formulation include the MAP-based Type-I approaches (Babacan et al.,
2009; Ji et al., 2008) and the hierarchical evidence maximization-based
Type-II approaches (Figueiredo, 2003; Tipping, 1999). One of the most pop-
ular frameworks employing the Type-II approach is the relevance vector
machine, which is an automatic relevance determination (ARD) technique
(Tipping, 1999). Laplacian priors which equivalently result in the 1-norm
constraint have been incorporated in the Type II framework, using a Hierar-
chical Gaussian scale mixture (GSM) representation (Babacan et al., 2009;
Figueiredo, 2003). A hierarchical Gaussian–Exponential prior-based Type-II
inference technique, also known as Bayesian Lasso, is proposed in Park and
Casella (2008). In the above-mentioned Bayesian methods, one obtains a
closed-form posterior distribution. In other useful prior models, that may not
be feasible, for example, in Horseshoe priors-based Type-I and Type-II infer-
ence (Carvalho et al., 2009) and spike-and-slab priors (Andersen et al., 2014),
where approximate inference techniques are used.
2
The hierarchical Bayesian framework
In this section, we first introduce the scale mixture-based hierarchical Bayesian
framework. We discuss sparse Bayesian learning (SBL) as a special case of the
power exponential scale mixture (PESM).
Sparsity-aware Bayesian inference and its applications Chapter
8 165

2.1
Gaussian scale mixtures and sparse Bayesian learning
GSM and Laplacian scale mixtures (LSM) are popular since they are capable
of hierarchical data modeling of variables sampled from complex heavy-tailed
super-Gaussian distributions. Recently, a more general PESM distribution,
which is a generalization of GSM and LSM, was presented (Giri and Rao,
2016). The foundation for the PESM distribution is the power exponential
(PE) distribution, which has the following parameterized form with mean 0:
PEðxi; p, γiÞ ¼ p e

jxij
γi

p
2γiΓ 1
p
 
(3)
In the above form p ¼ 2 results in the normal distribution (with γi representing
the standard deviation), whereas p ¼ 1 connects to the Laplacian distribution.
Further, p < 2 leads to distributions with heavier tails as compared to the
Gaussian distribution. The PESM family of distributions can be represented
as follows:
pðxÞ ¼
Z
PEðx; p, γÞpðγÞdγ ¼
Z
pðxjγÞpðγÞdγ
(4)
Different values of distributional parameter p along with different mixing
densities, i.e., p(γ), lead to different distributions p(x). This scale mixture rep-
resentation provides a hierarchical view for the generation of the random vec-
tor X by first generating γ using p(γ), and then generating X using p(xjγ). As
special cases, the choice of p ¼ 2 leads to GSM, and p ¼ 1 leads to the LSM.
In the context of Type-I Bayesian inference which is the standard MAP esti-
mation technique, a special generalized t-distribution in the PESM framework
reduces to several well-known sparse signal recovery algorithms such as
Lasso (Tibshirani, 1996), reweighted ‘1 (Candes et al., 2008), and reweighted
‘2 (Chartrand and Yin, 2008) algorithms. Further, in the context of Type-II
Bayesian inference, setting p ¼ 2 leads to the GSM-based inference of x
and γ. In Relevance Vector Machine (Type II) (Tipping, 1999), it is shown
that the prior on x used in SBL actually follows a Student-t distribution
(GSM with the Gamma distribution as the mixing density). Hence, the Type
II formulation of PESM with p ¼ 2 and specific choice of the distribution
of γ leads to SBL.
SBL adopts a probabilistic framework with a prior over the model weights
governed by a set of hyperparameters, one hyperparameter associated with
each weight. These hyperparameters are treated as unknowns (or can be mod-
eled as random with a noninformative, conjugate hyperprior distribution) and
are estimated from the observations. Instead of working directly with the prior
166
Handbook of Statistics

density function p(x), SBL typically uses a two-layer hierarchical prior model
that involves a conditional prior probability density function (pdf ) p(xjγ) and
a hyperprior p(γ). It is observed that, when the posterior mean is computed,
several entries of x tend to zero, since the posterior distributions of the entries
of x are sharply peaked around zero. Intuitively, a sparse posterior distribution
may be obtained if the prior imposes a significant probability mass around
zero. In particular, one version of SBL imposes a Gaussian prior on each of
the weights with the variance of the Gaussian being modeled as unknown
with a conjugate hyperprior, namely, an inverse gamma distribution. The
Gaussian prior, by itself, is not sparsity promoting, but the hierarchical nature
of the prior leads to the overall marginal prior on x as a product of indepen-
dent Student-t distributions, which is known to be a sparsity-promoting prior
(Tipping, 2001).
2.2
SBL framework
Obtaining sparse representations from (1) is tantamount to determining the
weight vector x which allows accurate approximation of y while having a
large number of zeros. The problem of sparse representation is also referred
to as the basis selection problem since the zero and nonzero entries of x select
the subset of basis vectors in A which are used to construct y.
2.2.1
Likelihood, prior, and posterior in SBL
SBL assumes a Gaussian likelihood, given as
pðyjx; σ2Þ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πσ2
p

m
exp
 y  Ax
k
k2
2σ2


:
(5)
The above likelihood is obtained by assuming that w  N ð0, σ2IÞ in (1),
where σ2 is the noise variance, I denotes the identity matrix, and the notation
N ðμ, ΣÞ represents the Gaussian distribution with mean vector μ and covari-
ance matrix Σ. The maximum likelihood (ML) solution can be derived by
maximizing the log-likelihood using (5), which is equivalent to finding the
minimum ‘2-norm solution to (1). However, it is well known that this
ML-based solution does not lead to sparse representations, and hence, SBL
applies a prior on the weights x in order to promote sparsity. In particular,
SBL employs a parametric prior given as
pðx; γÞ ¼
Y
N
i¼1
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πγi
p
exp
 x2
i
2γi


,
(6)
where γi is the ith component of the N-length hyperparameter vector γ,
i.e., γ ¼ [γ1, …, γi, …, γN]T, and γi controls the variance of the weight xi.
Sparsity-aware Bayesian inference and its applications Chapter
8 167

In the SBL framework, these hyperparameters along with the error variance
σ2 are jointly estimated from the data. The marginalized density is obtained
using the prior in (6) and the likelihood in (5) and is given by
pðy; σ2Þ ¼
1
ð2πÞm=2 jΣyj1=2 exp

yTΣ1
y y
2
(
)
,
(7)
where Σy ¼ σ2I + AΓAT, Γ ≜Diag
γ
f g. For fixed values of the hyperpara-
meters γ and σ2, using Bayes’ theorem, the posterior distribution of the
weights x can be written as
pðxjy; γ, σ2Þ ¼
1ﬃﬃﬃﬃﬃ
2π
p

N
exp
ðx  μÞTΣ1
x ðx  μÞ
2
(
)
,
(8)
where the mean vector and covariance matrix of the posterior Gaussian distri-
bution are given by μ ¼ (ATA + σ2Γ1)1ATy and Σx ¼ (ATA + σ2Γ1). It is
evident that in order to sample from (8), or find point estimates for x, we need
to estimate the hyperparameters γ and σ2. Potential advantages of this
approach are:
l
Given γ, the posterior distribution of x given by (8) is Gaussian, so it is
easy to find point estimates (e.g., the posterior mean μ can be used).
l
The averaging over x in obtaining the marginalized likelihood in (7)
smooths out the cost function, eliminating many of the local minima.
l
The covariance matrix Σx can help quantify the uncertainty in the recov-
ered x (note that Σx is dependent on the measurement matrix A, and there-
fore, such a characterization of the uncertainty yields confidence bounds
that capture the dependance on the measurement matrix, unlike classical
recovery guarantees in compressed sensing).
l
Many of the γi’s can be tied together, allowing one to naturally incorporate
any additional structure in the problem.
In the next section, we describe the Expectation Maximization (EM) algo-
rithm for estimating the hyperparameters and obtain the posterior distribution
of x under the SBL prior model from the observation vector y.
2.2.2
Expectation maximization
The logarithm of the type-II likelihood given in (7) needs to be maximized in
order to obtain the hyperparameter estimates. However, this approach does
not lead to a simple tractable hyperparameter estimates. Instead, the EM for-
mulation is employed for obtaining the maximum likelihood estimates of the
hyperparameters, treating x as a hidden variable. The E and the M steps are
given as (Bishop, 2006)
168
Handbook of Statistics

E-step : Qðγ; γðrÞÞ ¼ xjy; γðrÞ,ðσ2ÞðrÞ log ðpðy, x; γ, σ2ÞÞ
½
,
M-step : γðr+1Þ
i
¼ xjy; γðrÞ,ðσ2ÞðrÞ x2
i
	

¼ μ2
i + ðΣxÞii
ðσ2Þðr+1Þ ¼
y  Aμ
k
k2 + ðσ2ÞðrÞ XN
i¼1 1 
γðrÞ
i

1
ðΣxÞi,i


m
(9)
The E-step involves computing the posterior distribution p(xjy; γ(r), (σ2)(r)),
which is Gaussian with mean μ ¼ (ATA + (σ2)(r)(Γ(r))1)1ATy and covariance
Σx ¼ (ATA + (σ2)(r)(Γ(r))1). Thus, the SBL algorithm initializes Γ and
σ2 arbitrarily, and then alternates between the E-step involving computing
μ and Σx and the M-step where the values of γ and σ2 are updated. Note that, if
the noise variance σ2 is known, one can skip the variance update step above. Upon
convergence, several of the γi’s are driven to zero, which effectively forces the
associated weights at the mean of the posterior distribution to zero. The complete
derivation of the SBL algorithm, along with a detailed and lucid discussion of
the sparsity-promoting nature of the prior, can be found in Wipf and Rao (2004).
2.3
Case study: Wireless channel estimation and SBL
We demonstrate the performance of the SBL-based sparse signal recovery in
the application of joint channel estimation and data detection in orthogonal
frequency division multiplexing (OFDM)-based wireless communication sys-
tems. The precise problem statement and algorithm was developed in Prasad
et al. (2015), and we present a few highlights from that study here. In accor-
dance with the 3GPP/LTE broadband wireless communication standard
(Universal Mobile Telecommunications System (UMTS), 1997; Zyren and
McCoy, 2007), we use a 3-MHz bandwidth OFDM system with 256 subcar-
riers, with a complex baseband signal sampling frequency of fs ¼ 3.84
MHz, resulting in an OFDM symbol duration of 83.3 μs with cyclic prefix
of 16.67μs (corresponding to the use of a cyclic prefix spanning 64 subcar-
riers). The received signal over these 256 subcarriers forms the observation
vector y. Further, in the pilot-based sparse estimation framework, A ¼ TpF,
where Tp is a diagonal (rectangular) matrix consisting of transmitted pilots
and picks out the rows of the Fourier matrix F corresponding to the pilot sub-
carriers. Hence, A is fully known. In the joint channel estimation and data
detection problem, A ¼Tp,xF, where Tp,x is a diagonal (square) matrix consist-
ing of transmitted pilots and unknown data samples on the diagonal, and F is
a Fourier matrix, and hence A is not fully known. Further, the sampled
impulse response of such wireless channels tends to be approximately sparse
(a-sparse), rather than exactly sparse, owing to the leakage effect related to
finite bandwidth sampling as well as practical filtering considerations
(Kannu and Schniter, 2011; Prasad et al., 2015). The length of the a-sparse
Sparsity-aware Bayesian inference and its applications Chapter
8 169

channel is taken to be equal to the length of the cyclic prefix. The a-sparse
channel is the unknown in the problem of channel estimation, and we use
Bayesian inference techniques for estimating it.
A sample instantiation of the a-sparse channel used in the simulations
and the filtered multipath intensity profile (MIP) is depicted in Fig. 1. The
figure captures the leakage effect due to finite bandwidth sampling and practical
symbol pulse-shape-based filtering. To generate the plot, we have used the
Pedestrian B (Ped-B) channel model (ITU-R Rec. M.1225, 1997) with Rayleigh
fading. We have also used raised cosine-based pulse-shape filtering at the
receiver and transmitter with a roll-off factor of 0.5 (Universal Mobile
Telecommunications System (UMTS), 1997). At the sampling frequencies con-
sidered, the number of significant channel taps is far fewer than the weak chan-
nel taps in the filtered impulse response, as seen in Fig. 1. In the following, we
present the simulation results for the block fading and time-varying scenarios.
Each OFDM frame consists of L ¼ 7 OFDM symbols, which is also
known as an OFDM slot. The data are transmitted using a rate 1/2 Turbo
code, with quadrature phase shift keying modulation. For the Turbo code gen-
eration, we use the publicly available software (Studer et al., 2011), which
uses a maximum of 10 Turbo iterations. We consider a block-fading channel
0
10
20
30
40
50
60
70
−250
−200
−150
−100
−50
0
50
Lag
dB
 
Channel Realization
Strong Channel taps
Weak Channel taps
Filtered MIP
Filtered MIP: Strong components
Filtered MIP: Weak components
FIG. 1
An example channel realization of the a-sparse channel, along with the filtered MIP, i.e.,
the MIP when raised cosine filters are employed at the transmitter and receiver. The plot also
shows the strong (> 30 dB) and weak (< 30 dB) channel taps and filtered-MIP components,
to illustrate that the channel can indeed be modeled as being approximately sparse.
170
Handbook of Statistics

(where the channel remains constant over a block of symbols and is drawn
independently from the Ped-B channel model from block-to-block) and use
Pb ¼ 44 pilot subcarriers, uniformly placed across the M ¼ 256 subcarriers
in each OFDM symbol. Each OFDM frame consists of L ¼ 7 OFDM symbols.
Evidently, it is possible to incorporate SBL for a-sparse channel estimation.
However, note that, we also have unknown data (nonpilot) embedded in each
OFDM symbol, which also need to be detected. A key feature of the joint
SBL (J-SBL) algorithm presented in Prasad et al. (2015) is that the observa-
tions from both the data and the pilot subcarriers are incorporated to jointly
estimate the a-sparse channel as well as the unknown data. We plot the mean
squared error (MSE) performance of both the algorithms in Fig. 2A, using a
convergence criteria of E ¼ 109 and maximum number of iterations limited
to rmax ¼ 200, for both the algorithms. We compare the MSE between the
estimated and true channels for the presented algorithms against a compressed
sensing-based channel estimation technique (Sharp and Scaglione, 2008), and
MIP-aware methodsb : pilot-only MIP-aware estimation (Van de Beek et al.,
1995) and the MIP-aware joint data and channel estimation algorithm, which
we refer to as the EM-OFDM algorithm (Al-Naffouri et al., 2002). From
Fig. 2A, we observe that the SBL algorithms perform better than the MIP-
unaware, noniterative schemes such as the frequency domain interpolation
technique. Among the iterative methods, the J-SBL algorithm performs an
order of magnitude better than the SBL algorithm, especially at higher values
of signal-to-noise ratio (SNR), while being within only 3 dB from the MIP-
aware EM-OFDM algorithm, in terms of the SNR required to achieve a given
estimation accuracy. The J-SBL jointly detects the (LM  Pb) data symbols
along with estimating the channel, resulting in a significantly lower overall
MSE. Note that the recursive J-SBL (RJ-SBL) is a mathematically equivalent,
recursive, and computationally simpler counterpart of the J-SBL algorithm
(Prasad et al., 2015). Hence, they have the same performance.
The coded and the uncoded bit error rate (BER) performance of the EM,
J-SBL, and a genie receiver, i.e., a receiver with perfect knowledge of the
channel (labeled as Genie), is shown in Fig. 2B. We also compare the
performance with SBL and MIP-aware pilot-only channel estimation fol-
lowed by data detection. The BER performance of the RJ-SBL is supe-
rior that of the SBL and compressed sensing algorithms in both coded and
uncoded cases. The MIP-aware pilot-only estimation method has a better BER
performance compared to RJ-SBL for SNRs <15 dB, in both coded and the
uncoded cases. Also, the MIP-aware EM-OFDM algorithm outperforms the pre-
sented RJ-SBL algorithm by 3 dB. This is because, in the block-fading case,
bNote that MIP-aware methods are genie-aided approaches, since the support of the sparse vectors
is assumed to be known. Therefore, the system becomes overdetermined, and sparse recovery
techniques are not necessary.
Sparsity-aware Bayesian inference and its applications Chapter
8 171

10
15
20
25
30
SNR
MSE
 
 
FDI
CS
SBL
MIP−aware pilot−only
J−SBL
RJ−SBL
EM−OFDM
5
10
15
20
25
30
10−5
10−4
10−3
10−2
10−1
10−5
10−6
10−4
10−3
10−2
10−1
100
101
Eb/N0
BER
 
 
CS
SBL
MIP−aware pilot−only
J−SBL
RJ−SBL
EM−OFDM
Genie
Solid: Uncoded
Dashed: Coded
FIG. 2
Performance of SBL-based joint a-sparse channel estimation and data detection. (A) MSE performance of SBL, J-SBL/RJ-SBL algorithms compared to
frequency domain interpolation (FDI) (Coleri et al., 2002), compressed sensing (CS) (Sharp and Scaglione, 2008), MIP-aware pilot-only (Van de Beek et al.,
1995), and EM (Al-Naffouri et al., 2002) schemes in a block-fading channel, with 44 pilot subcarriers, as a function of SNR in dB. (B) BER performance of
the presented algorithms in a block-fading channel, with 44 pilot subcarriers, as a function of the signal-to-noise ratio (Eb/N0).

J-SBL algorithm suffers due to error propagation from the large number of data
symbols that are simultaneously detected.
3
Joint-sparse signal recovery
In several applications such as EEG source localization, direction of arrival
estimation, and channel estimation in wireless communications, a sequence
of measurement vectors are available simultaneously. Mathematically, this
leads to a natural extension of the single measurement vector model (1),
known as multiple measurement vector (MMV) model, given by
Y ¼ AX + W,
(10)
where the matrix Y ≜½y1,…, yL  mL contains the L measurement vectors
as its columns. Thus, each measurement vector yi  m is modeled as in (1).
Further, X  NL is a matrix consisting of unknown column vectors xi, which
are modeled as being sparse, and W is a matrix consisting of noise vectors as
its columns. The goal then becomes the simultaneous approximation of each
weight vector xi assuming a structured sparsity profile among all the vectors
in X. The most commonly encountered structure in practical applications is
that of joint sparsity, where the indices of the nonzero entries are common
across the columns of X. That is, ith row of X is either all zero or all nonzero,
but the nonzero entries are independent of each other. This model is called the
joint sparsity model-2 in the literature (Duarte et al., 2005). In a Bayesian
framework, this can be enforced naturally, by considering a common hyper-
parameter vector γ  N to model the variances of the entries of all the
columns in X. This leads to the MSBL algorithm, described next.
3.1
The MSBL algorithm
In Wipf and Rao (2007), an ARD-based solution for the simultaneous sparse
approximation problem in (10) is developed. Similar to the SBL framework
presented in Section 2, a flexible prior distribution is incorporated to induce
the desired joint-sparse structure in the final estimates. To begin, we assume
p(YjX) to be Gaussian with independent and identically distributed (i.i.d.)
entries with mean zero and variance σ2. We adopt the notation where xj
denotes the jth column of X, while xi represents the ith row of X. For the pair
(yj, xj) we have pðy  jjx  jÞ ¼ N ðAx  j, σ2ImÞ. Further, a parametric Gaussian
prior on the ith row of X is assumed, given by pðxi  ; γiÞ ¼ N ð0,γiIÞ, where
γi is ith component of the N length hyperparameter vector γ that controls
the prior variance of each column of X, i.e., pðX; γÞ ¼ Qm
i¼1 pðxi  ; γiÞ . The
hyperparameter vector γ is unknown and needs to be estimated from the data.
The resulting posterior distribution of the weights xj is given by pðx  jjy  j; γÞ ¼
N ðμ  j,ΣÞ, where the covariance and mean are given as
Sparsity-aware Bayesian inference and its applications Chapter
8 173

Σ ¼ Γ  ΓATΣ1
y AΓ,
(11)
M ¼ ½ μ1,…, μN ¼ ΓATΣ1
y Y,
(12)
respectively, with Σy ≜σ2I + AΓAT. Typically, the columns in M, which are
the posterior means, are used as the point estimates of the columns in X.
3.2
Expectation maximization in MSBL
Incorporating the type-II ML for obtaining estimates of γ involves treating the
unknown weights X as nuisance parameters and integrating them out to obtain
the marginal likelihood as
LðγÞ ≜2 log
Z
pðYjXÞpðX; γÞdX ¼ m log jΣyj+
X
L
j¼1
yT
jΣ1
y yj
(13)
However, directly minimizing LðγÞ using the above cost function approach
does not lead to closed-form hyperparameter estimates. Instead, the EM for-
mulation is employed for obtaining the maximum likelihood estimates of
the hyperparameters, while X is treated as the hidden variables. The E and
the M steps are given as (Bishop, 2006)
E-step : Qðγ; γðrÞÞ ¼ XjY; γðrÞ,σ2 log ðpðW, X; γ, σ2ÞÞ
½
,
M-step : γðr+1Þ
i
¼ XjY; γðrÞ,σ2 x2
i
	

¼ μðiÞ2 + Σði,iÞ:
(14)
The MSBL algorithm thus works by computing the posterior distribution
P(XjY; γ(r), σ2), which is Gaussian with covariance matrix and mean vector
given by (11) and (12), respectively, but with Γ replaced by Γ(r), followed
by updating Γ(r) using (14), in an iterative fashion. Upon convergence, several
γi’s are driven to zero, which effectively forces the associated weights at the
mean of the posterior to zero.
3.3
An interesting interpretation of the MSBL cost function
Why do SBL and MSBL lead to sparse solutions? To answer this question, we
now present an interesting interpretation of the log-marginalized likelihood
objective in (13) which provides a deeper understanding into the working of
the MSBL optimization. We begin by introducing the Bregman matrix Diver-
gence Dϕ(X, Y) between any two positive definite matrices X and Y as
Dϕ ≜ϕðXÞϕðYÞhrϕðYÞ, X Yi,
(15)
where ϕ : m
+ ! is a convex function on the space of positive semidefinite
matrices of size m  m (denoted by m
+), and rϕ(Y) is the gradient of ϕ eval-
uated at Y. Let jj denote the determinant of a matrix. For ϕðÞ ¼  log jj and
rϕ(Y) ¼ Y1, we obtain the Bregman LogDet divergence:
174
Handbook of Statistics

DlogdetðX,YÞ ¼ logjXY1j + Tr XY1


m:
(16)
Juxtaposing the MSBL objective in (13) with (16), it is evident that the nega-
tive log likelihood  log pðY; γÞ in (13) and Dlog detðRyy,ΣγÞ are effectively the
same up to a constant term. In the divergence term Dlog detðRyy,ΣγÞ, the first
argument Ryy ≜1
L
PL
j¼1 yjyT
j
is the sample covariance matrix of the i.i.d.
MMVs, and the second argument Σγ ¼ σ2Im + AΓAT is the parameterized
covariance matrix of the MMVs, parameterized by γ. This connection
between the MSBL’s log likelihood objective and the Bregman LogDet diver-
gence reveals that solving the MSBL optimization amounts to finding the γ
that minimizes the distance between covariance matrices Ryy and Σγ, with
the pointwise distances measured using the Bregman LogDet divergence.
Thus, the MSBL algorithm is essentially a second-order moment (covariance)
matching procedure which finds the γ that minimizes the gap between the
sample and the parameterized covariance matrices of the MMVs, in the
Bregman LogDet divergence sense.
This alternate interpretation of the MSBL procedure raises an interesting
question. Could there be another matrix divergence different from the Bregman
LogDet divergence used in the MSBL optimization which is better suited for
matching the covariance matrices Ryy and Σγ to find γ? The choice of such a
matrix divergence should certainly be driven by how well it allows for efficient
optimization. Initial investigations into the use of alternative matrix diver-
gences for covariance matching-based sparse recovery using the SBL prior
have shown promising results. For example, in Khanna and Murthy (2017e),
an α-Renyi divergence objective is considered for covariance matching, and
an ultrafast greedy algorithm is developed for joint-sparse (support) recovery.
3.4
A Covariance-matching framework for sparse support recovery
using MMVs
As alluded to above, the Bregman matrix divergence minimization view of
the MSBL optimization can be formalized as a more general covariance-
matching framework for sparse support recovery in the canonical MMV
problem. Just like in MSBL, the columns of X are assumed to be i.i.d.
N ð0, ΓÞ distributed, where Γ ¼ Diag γ
f g, γ n
+. As highlighted in Pal and
Vaidyanathan (2015a), the diagonal covariance matrix Γ naturally captures
the common support across the columns as well as the uncorrelatedness of
the nonzero coefficients within the individual columns of X. Another con-
venient feature of the Gaussian prior for X is that it induces Gaussian mea-
surements yj 
i:i:d: N ð0, ΣγÞ, where Σγ ≜σ2Im + AΓAT. We shall refer to Σγ
as the parameterized MMV covariance matrix as it depends on the hyper-
parameters γ. Further, let ^Ryy ≜1
LYYT denote the empirical MMV covariance
matrix computed from the observation vectors.
Sparsity-aware Bayesian inference and its applications Chapter
8 175

Consider a matrix function d : m
+  m
+ ! +, which inputs two positive
definite matrices and outputs a nonnegative real number that represents
the degree of “nearness” or “distance” between the input matrices. Suitable
candidates for d include matrix divergences and matrix norms. In the
covariance-matching framework, the row-support of X is recovered as
Supp ^γ
f g where ^γ is a solution of the following constrained optimization
problem:
^γ ≜arg min
γ n
d
^Ryy, σ2Im + AΓAT
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Σγ
0
B
@
1
C
A + λhðγÞ subject to γ  0,
(17)
where λ is a positive constant and h : n ! serves as a penalty function
designed to regularize the solution space of γ. In the current context, h is a
sparsity-promoting concave function of γ. Thus, in (17), we seek a sparse non-
negative vector γ such that the parameterized covariance Σγ is a good approx-
imation of the empirical covariance ^Ryy. Finally, Supp ^γ
f g is declared as an
estimate of the true row-support of X. The following result shows that solving
(17) results in sparse solutions.
Theorem 1. If h is a concave and N 	 m2 + m
2
, then ^γ in (17) satisfies k^γk0 
m2 + m
2
.
Proof. Suppose γ* is a solution of (17). Then, γ* is also a solution to the fol-
lowing constrained optimization problem:
Pd,h :
maximize
γ
 hðγÞ
such that AΓ*AT ¼ AΓAT and γ  0:
(18)
To see this, first note that γ* is Pd,h feasible. Furthermore, for λ > 0, h(γ) 	
h(γ*) as dð^Ryy,ΣγÞ ¼ dð^Ryy,Σγ
Þ for all feasible γ in Pd,h.
Note that (18) seeks maximization of a convex function  h(γ) over a
bounded convex polytope A ¼
γ  n : γ  0, AΓ*AT ¼

AΓAT
. By Luen-
berger and Ye (2010, Chapter 7, Theorem 3), any solution of (18) is an
extreme point of the polytope A, and thus a basic feasible solution of the vec-
torized linear constraints, Vec AΓ*AT


¼ ðA ⊙AÞγ, where A ⊙B  mpN
denotes the Khatri–Rao product (column-wise Kronecker product) (Khanna
and Murthy, 2018a) of matrices A  mN and B  pN. Now, A ⊙A is an
m2  N matrix, and for N 	 m2 +m
2 , it has at most m2 +m
2
linearly independent
rows. Therefore, any basic feasible solution of (18) (including ^γ) must have
at most m2 +m
2
nonzero elements.
□
From Theorem 1, one can conclude that any concave penalty function
h induces a sparse solution ^γ in (18), assuming m2 < N. The following
176
Handbook of Statistics

theorem lays forward sufficient conditions under which the support of ^γ in
(17) equals the true row-support of X.
Theorem 2. Suppose 
	
xjxT
j

¼ Diag γ
f
g and kγ*k0  k. Then, for L ! ∞,
if k^γk0  k, then ^γ ¼ γ
, provided Krank A ⊙A
f
g 	 2k. Here, Krank B
f g
denotes the Kruskal rankc of the matrix B.
Proof. Let ^γ 6¼ γ* be a k-sparse solution of (17). Then, ^γ is also a solution
of Pd,h, and therefore it must satisfy the feasibility condition: Vec ^Ryy


¼
ðA ⊙AÞ^γ. Since the sample covariance ^Ryy is an asymptotically consistent
estimate
of
the
true covariance,
it follows
that,
as
L
!∞,
^Ryy ¼
ADiag γ
f
gAT, or equivalently Vec ^Ryy


¼ ðA ⊙AÞγ
. Therefore, as L ! ∞,
we have ðA ⊙AÞð^γ γ
Þ ¼ 0. Since both ^γ and γ* are at most k-sparse, ^γ γ*
is at most 2k sparse. This implies that there exists a 2k or less sparse vector
in Null A ⊙A
f
g , which contradicts Krank A ⊙A
f
g 	 2k . Hence, ^γ must be
equal to γ*.
□
From Theorem 2, when the output of (17) is a k-sparse vector with k 
Krank A ⊙A
f
g
2
j
k
, the constrained optimization problem in (17) correctly recovers
the row-support of X.
3.5
Examples of covariance-matching algorithms for sparse
support recovery
The covariance matrices Σγ and ^Ryy together with the matrix distance function
d and penalty function h are the building blocks of the covariance-matching
framework. Different choices of d and h result in different support recovery
algorithms.
3.5.1
The MSBL algorithm
The MSBL optimization of log-likelihood function in (13) can be cast as the
canonical covariance-matching problem in (17) by choosing dð ^Ryy, ΣγÞ ¼
Tr Σ1 ^Ryy


 m, hðγÞ ¼ log jΣγj and λ ¼ 1. This leads to the MSBL algo-
rithm described in Section 3.2.
3.5.2
Covariance matching using Renyi divergence
An interesting approach for recovering the hyperparameter vector γ is to solve
the optimization:
cThe Kruskal rank (Sidiropoulos and Kyrillidis, 2012) of an m  n matrix B is the largest integer
k such that any k columns of B form a linearly independent set of vectors.
Sparsity-aware Bayesian inference and its applications Chapter
8 177

^γ ¼ arg min
γ n
+
Dα ep, pγ


(19)
where Dα ep, pγ


is the α-Renyi divergence between the multivariate Gaussian
distributions ep ¼ N ð0, ^RyyÞ and pγ ¼ N ð0, ΣγÞ. Then, as shown in Gil et al.
(2013), the objective function Dαðep, pγÞ evaluates as
Dαðep, pγÞ ¼
1
2ð1  αÞ log ð1  αÞ ^Ryy + αΣγ

  α log Σγ

  ð1  αÞ log ^Ryy




∝log ð1  αÞ ^Ryy + αΣγ

  α log Σγ

 + terms indep: of γ:
(20)
We observe that by choosing dð^Ryy,ΣγÞ ¼ log ð1αÞ^Ryy + αΣγ

, hðγÞ ¼
log Σγ

, and λ ¼ α, the Re`nyi divergence minimization in (19) can be
expressed as a canonical covariance-matching problem for recovering γ and
its sparse support. Based upon these observations, a novel support recovery
algorithm called Re`nyi Divergence Covariance-Matching Pursuit has been
discussed in Khanna and Murthy (2017e).
3.5.3
Co-LASSO
A natural choice for the functions d and h are dð^Ryy, ΣγÞ ¼k ^Ryy Σγ k2
F and
h(γ) ¼ kγk1, where kkF represents the Frobenius norm, which gives rise to
the Co-LASSO problem:
Co-LASSO : ^γ ¼ arg min
γ N
k ^Ryy  Σγk2
F + λ k γk1:
(21)
First presented by Pal and Vaidyanathan (2015a), the Co-LASSO algorithm is
capable of recovering supports of size as high as Oðm2Þ from only m measure-
ments per joint sparse column of X. The sample complexity of Co-LASSO for
successful support recovery has also been analyzed in Pal and Vaidyanathan
(2015a).
4
Exploiting intervector correlation
In the previous section, we considered the recovery of multiple sparse vectors
with a common support, i.e., the joint-sparse recovery problem. The Bayesian
prior we considered modeled the sparse vectors as independent, Gaussian
distributed with an unknown covariance matrix. But what if the nonzero
entries of consecutive sparse vectors are correlated, in addition to having a
common support? Such structures do arise in many practical applications,
and they can (need to) be exploited to obtain better performance in the recov-
ery algorithms.
In this section, we first present algorithms for tracking time-varying sparse
vectors using the SBL framework. Note that the conventional Kalman filter
(KF) approach is not sufficient here since it is not equipped for handling
178
Handbook of Statistics

sparse vectors and hyperparameter estimation in the ARD framework. We
derive recursive techniques based on the Kalman filter and smoother (KFS)
with an autoregressive (AR) model for the evolution of the sparse vector.
We assume that, at the kth instant, the output yk  m1 is given by
yk ¼ Axk + wk,
(22)
where A  mN is the overcomplete basis matrix as defined in Section 2 and
xk  N is the sparse vector to be estimated at the kth instant. Further, we assume
that the variation in the xk with k can be captured using an AR model given by
xk ¼ ρxk1 + ρzk,
(23)
where ρ  [0, 1], ρ ≜
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  ρ2
p
, xk1  N is the sparse vector at the (k  1)th
instant, and x0, zk  N ð0, ΓÞ. Note that the sparsity profile of xk is the same
for all k, if we assume that the support of zk is the same as that of xk1. Also,
ρ controls how fast the nonzero entries in xk vary over time: if ρ ! 1, xk is
nearly constant over time, while when ρ ! 0, the model reduces to the
MMV model described earlier, since xk are nearly independent over k but with
a common support.
4.1
Intervector correlation: The Kalman SBL algorithm
We derive recursive inference techniques based on the KFS, with an AR
model for the temporal evolution of the sparse vector. In a KF approach,
the goal is to recursively estimate the sparse vector and its covariance matrix
using forward and backward recursions, given the observations y1, …, yL. In
the forward recursion, the KF obtains the estimates of the sparse vector as a
weighted average of the previous estimate and the current measurement.
These weights are given by the Kalman gain matrix. In the backward recur-
sion, the Kalman smoother ensures that the observations until the Lth instant
are included in the estimation of the sparse vector corresponding to the lth
observation for 1  l  L.
For the moment, if we assume that Γ is known, and if we denote the pos-
terior mean and the covariance matrix of channel in the lth instant by ^μljl and
Pljl, respectively, for 1  l  L, then the KFS update equations are as follows
(Anderson and Moore, 2005; Ghahramani and Hinton, 1996):
for l ¼ 1,…,L do
Prediction : ^μljl1 ¼ ρ^μl1jl1
(24)
Pljl1 ¼ ρ2Pl1jl1 + ð1  ρ2ÞΓ
(25)
Filtering:
Gl ¼ Pljl1AT σ2I + APljl1AT

1
(26)
^μljl ¼ ^μljl1 + Glðyl A^μljl1Þ
(27)
Sparsity-aware Bayesian inference and its applications Chapter
8 179

Pljl ¼ ðIGlAÞPljl1
(28)
end
for j ¼ L, L1,…, 2 do
Smoothing : ^μj1jL ¼ ^μj1jj1 + Jj1ð^μjjL  ^μjjj1Þ
(29)
Pj1jL ¼ Pj1jj1 + Jj1ðPjjL Pjjj1ÞJT
j1,
end
(30)
where Jj1 ≜ρPj1jj1P1
jjj1 and Gl is the Kalman gain. In the above, the sym-
bols ^μljl1, Pljl1, etc. have their usual meanings as in the KF literature
(Ghahramani and Hinton, 1996). For example, ^μljl1 is the estimate of the
sparse vector at the lth instant given the observations Yl1 ¼ [y1, …, yl1]
and Pljl1 is the covariance of the lth channel estimate given Yl1. The above
KFS equations are initialized by setting ^μ0j0 ¼ 0 and P0j0 ¼ Γ. They track the
channel in the forward direction using the prediction and the filtering equa-
tions in (24)–(28) and smooth the obtained sparse vector estimates using the
backward recursions in (29) and (30). However, in the sparse channel tracking
problem, Γ is unknown. Hence, we develop a KSBL algorithm, which simul-
taneously estimates the channel coefficients and also learns the unknown Γ.
Recall that the sparse vectors have a common support, and hence γ is a com-
mon hyperparameter. We present the KSBL algorithm using the EM updates,
as follows:
E-step : Q γjγðrÞ


¼ x1,…,xLjYL; γðrÞ½ log pðYL, x1,…, xL; γÞ
(31)
M-step : γðr+1Þ ¼ arg max
γ N1
+
Q γjγðrÞ


:
(32)
To compute the E-step, we require the posterior distribution of the unknown
sparse vector, which is obtained using the recursive update equations given
by (24)–(30). In order to obtain an ML estimate of γ, KSBL incorporates an
M-step, which, in turn, utilizes the mean and covariance of the posterior dis-
tribution from the E-step. The M-step results in the following optimization
problem:
γðr + 1Þ ¼ arg max
γ N1
+
x1,…,xLjYL;γðrÞ
cLlogjΓj
X
L
j¼2
ðxj ρxj1ÞTΓ1ðxj ρxj1Þ
ð1ρ2Þ
xT
1Γ1x1
"
#
,
(33)
where c is a constant independent of γ. As mentioned earlier, we see that the
M-step requires the computation of ^μjjL ≜x1,…, xLjYL; γðrÞ½xj, and covariance
180
Handbook of Statistics

x1,…, xLjYL; γðrÞ½xjxT
j  ≜PjjL + ^μjjL^μT
jjL for j ¼ 1, …, L, which is obtained from
(24)–(30). The M-step also requires the computation of x1,…,xLjYL; γðrÞ½xjxT
j1 ≜
Pj,j1jL + ^μjjL^μT
j1jL for j ¼ L, L  1, …, 2, which we obtain from Ghahramani
and Hinton (1996) as follows:
Pj1, j2jL ¼ Pj1jj1JT
j2 + JT
j1ðPj, j1jL  ρPj1jj1ÞJj2:
(34)
The above recursion is initialized using PL,L1jL ¼ ρ(I GLAL)PL1jL1.
Using the above expressions, (33) simplifies as
γðr+1Þ ¼ arg max
γ N1
+
c0  L log jΓj  Tr Γ1M1jL



1
ð1  ρ2Þ
X
L
j¼2
Tr Γ1MjjL


(
)
,
(35)
where c0 is a constant independent of γ, MjjL ≜PjjL + ^μjjL^μT
jjL + ρ2ðPj1jL +
^μj1jL^μT
j1jLÞ2ρ< Pj, j1jL + ^μjjL^μT
j1jL
n
o
, and M1jL ≜P1jL + ^μ1jL^μT
1jL. Differen-
tiating (35) w.r.t. γi and setting the resulting equation to zero gives the update for
the ith hyperparameter as follows:
γðr+1Þ
i
¼ 1
L
X
L
j¼2
MjjLði, iÞ
ð1  ρ2Þ + M1jLði, iÞ
 
!
,
(36)
for i ¼ 1, …, N. Thus, the KSBL algorithm learns γ in the M-step and pro-
vides low-complexity and recursive estimates of the sparse vector in the
E-step.
In Figs. 3 and 4, we demonstrate the MSE and support recovery perfor-
mance of the KSBL algorithm, respectively, and compare it to conventional
SBL and the OMP algorithms, which are unaware of the correlated group-
sparse nature of sparse vectors. We see that KSBL which benefits from group
sparsity and temporal correlation among sparse vectors has a superior perfor-
mance compared to the SBL and OMP algorithms.
4.2
Online sparse vector recovery
In this section, we derive the online version of the (offline) KSBL algorithm
discussed in Section 4.1, considering the similar system model given by
xk ¼ Dxk1 + zk
(37)
yk ¼ Axk + wk:
(38)
Here, D  [0, 1)NN is a known diagonal correlation matrix and the support of
zk coincides with that of
xk, k ¼ 1, 2, …
f
g. Also, A  mN is the known
Sparsity-aware Bayesian inference and its applications Chapter
8 181

30
35
SNR(dB)
MSE
40
45
10−4
10−3
10−2
10−1
 
 
OMP
SBL
KSBL
Kalman (Support aware)
FIG. 3
MSE performance of the KSBL algorithm compared to SBL and OMP algorithms and
support-aware Kalman filter for m ¼ 30, N ¼ 64, and sparsity S ¼ 3.
5
Success Rate
10
15
20
Number of Measurements(N)
25
30
35
40
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
KSBL
SBL
OMP
FIG. 4
Success rate of the KSBL algorithm compared to SBL and OMP algorithms for SNR ¼ 20,
N ¼ 64, and S ¼ 3.
182
Handbook of Statistics

measurement matrix, yk  m is the corresponding noisy measurement, and
wk  N ð0,σ2IÞ is the measurement noise at time k.
We recall that the offline KSBL algorithm takes all L measurement vectors
as input and processes them together in an iterative fashion to estimate the L
corresponding sparse vectors. However, the goal of the online algorithm is to
estimate the sparse vectors x1,x2,… sequentially, without storing and proces-
sing all the measurements together. Specifically, at every time instant k, the
online algorithm stores a small subset of Δ recent measurements and pro-
cesses them to give the estimate of xk. Here, 0  Δ < ∞denotes the maxi-
mum delay between the measurement and estimation. We show how the
KSBL algorithm can be modified to obtain a low-complexity noniterative
online algorithm with low memory requirement and bounded latency (specified
by the parameter Δ).
Unlike the offline algorithm, the online version recursively updates the
common hyperparameter γ at every time instant k using only Δ available mea-
surements. Then, the estimate of xk is computed using the estimate ^γk of γ at
time k. The online KSBL updates the hyperparameter at every time instant via
one iteration of the EM step of the offline KSBL using the hyperparameter
estimate at the previous time instant. The estimate of γ at time k can be
obtained by maximizing the Q function, which simplifies as
^γk ¼ arg min
γ N1
+
 k
2 log jΓj  1
2 Tr
Γ1 I  D2

1 X
k
t¼2
Mtjt+Δ
(
)
,
(39)
where the last step is similar to (35) discussed in Section 4.1, and we define
Mtjk ¼
I  D2


Ctjk+t
for t ¼ 1
Ctjk + DCt1jkD  2DCt,t1jk
otherwise,
(
(40)
and Ctjk ¼ Ptjk + ^xtjk^xT
tjk and Ct,t1jk ¼ Pt,t1jk + ^xtjk^xT
t1jk with ^xtjk (it was
denoted by ^μkjL in Section 4.1, since all the L measurements were used in
its computation), and Ptjk and Pt,t1jk are defined in Section 4.1. The above
optimization problem admits a closed-form expression given by
^γk ¼ 1
k Diag
I  D2

1 X
k
t¼1
Mtjt+Δ
(
)
:
(41)
The above equation requires the computation of k matrices Mtjk+Δ, for
t ¼ 2,3,…,k which leads to computational complexity that increases with k.
Therefore, we make an approximation to arrive at a recursive update step with
complexity independent of k using two Kalman smoothing schemes: fixed lag
and sawtooth lag schemes. We note that using γk, the online estimate of xk is
obtained using the fixed interval Kalman smoothing using Δ + 1 measure-
ments yk,yk + 1,…, yk + Δ. This step is similar to the sparse vector estimation
step of KSBL.
Sparsity-aware Bayesian inference and its applications Chapter
8 183

The fixed lag scheme uses the following approximation:
^γk  1
kDiag
I D2

1X
k
t¼1
Mtjt + Δ
(
)
(42)
¼
k 1
k


^γk1 + 1
kDiag
I D2

1Mkjk+Δ
n
o
:
(43)
The above relation recursively updates the hyperparameter estimate at time k
using ^γk1 and Mkjk+Δ. Further, we recall from (40) that Mkjk+Δ is a function
of ^xkjk+Δ , the autocovariance Pkjk+Δ, and the cross-covariance Pk,k1jk+Δ.
The three quantities related to the state statistics are computed by exploiting
the relation between xk and yk+Δ. From (38), we have
yk + Δ ¼
ADΔ
|ﬄ{zﬄ}
measurement
matrix
xk + A
X
Δ1
i¼0
Dizk + Δi + wk + Δ
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
measurement noise
:
(44)
We define a new measurement model using the above equation where yk+Δ is
the measurement vector at time k, ADΔ is the measurement matrix, and the
measurement noise (independent of the xk) is APΔ1
i¼0 Dizk+Δi + wk+Δ 
N ð0, RðγÞÞ . Here, the noise covariance RðγÞ ¼ A I  D2Δ


ΓAT + σ2I of
the new measurement model is computed using the fact that zkzT
k


¼
I  D2


Γ from (37), for any integer k > 0. Now, it is easy to see that the three
quantities ^xkjk+Δ, Pkjk+Δ, and Pk,k1jk+Δ are the state statistics of the new state
space model defined by (38) and (44). Thus, we use KF to the new system
to obtain the desired statistics. This estimation scheme is called fixed lag
Kalman smoothing.d
We note that the fixed lag scheme updates the state statistics at every time
instant k using yk+Δ. The sawtooth lag scheme waits for a block of Δs  Δ + 1
measurements to arrive and updates the statistics after every Δs time instants.
To elaborate, the sawtooth lag smoothing updates the hyperparameter at time
instant k ¼ lΔs, for all integer l > 0. Therefore, the approximation of (41)
used by the sawtooth lag smoothing at time k ¼ lΔs is as follows:
γl  1
lΔs
Diag
I D2

1X
l1
‘¼0
X
Δs
t¼1
M‘Δs + tj‘Δs + Δ + 1
(
)
(45)
¼
l1
l


γl1 + 1
lΔs
X
Δs
t¼1
Diag
I D2

1Mðl1ÞΔs + tjΔl
n
o
,
(46)
with Δl ¼ (l  1)Δs + Δ + 1.
dWith a slight abuse of notation, in this section, we define Γk ¼ Diag γk
f
g and not the kth column
of the matrix Γ.
184
Handbook of Statistics

Thus, the sawtooth lag smoothing computes Mðl1ÞΔs+tjΔl or equivalently
the state statistics ^xðl1ÞΔs+tjΔl, Pðl1ÞΔs+tjΔl, and Pðl1ÞΔs+tjΔl for t ¼ 1,2,…,Δs,
at time k ¼ lΔs. To compute the state statistics over a block of size Δs, we
use the fixed interval Kalman smoothing algorithm on overlapping blocks
of data of size Δ + 1, and discard the last Δ + 1 Δs values of every block.
This scheme is called the sawtooth lag smoothing in the Kalman filtering
literature (Krishnamurthy and Moore, 1993).
The main difference between the two schemes is the latency in estimation.
The latency of the fixed lag scheme is Δ, while it varies from Δ to Δ Δs + 1
for the sawtooth lag scheme. Thus, the average latency of the sawtooth lag
scheme is Δ  Δs  1
ð
Þ=2. Furthermore, the latency of the two lag schemes
equals Δ when Δs ¼ 1. However, the two schemes are different because the
fixed lag scheme does not use backward recursions to compute the statistics.
Further, the computational complexity of the fixed lag scheme at each
time is OðN2mÞ, while it is OðN3Þ for the sawtooth lag scheme because of
the extra steps in the backward recursions. The overall complexities also lin-
early scale with the total number of measurements L. The offline KSBL has
similar complexity as that of the sawtooth lag scheme per algorithm iteration.
Its overall complexity is OðTLN3Þ where T is the number of iterations. Fur-
ther, the online KSBL schemes store the measurements (OðmÞ) and state sta-
tistics (OðN2Þ) over a time window of size Δ. Thus, the memory requirement
for the online schemes is OðΔN2Þ because m < N. On the contrary, the offline
KSBL stores the entire data over a time window of L. The reader is referred to
Joseph et al. (2015) and Joseph and Murthy (2017) for a detailed presentation
of algorithms, performance analysis, and guarantees for online Bayesian
sparse signal recovery problems.
4.3
Case study (continued): Wireless channel estimation
and the KSBL algorithm
In this section, we consider the estimation of a slowly time-varying wireless
channel from training and pilot symbols. The channel simulated according
to a Jakes’ model (Zheng and Xiao, 2003), with a normalized fade (Doppler)
rate of fdTs ¼ 0.001 and Pb ¼ 44 pilot subcarriers in every OFDM symbol.
The MSE performance of the KSBL and the JK-SBL algorithms is plotted
against SNR in Fig. 5 and compared with the per-symbol MIP-unaware fre-
quency domain interpolation (Coleri et al., 2002), and the per-symbol J-SBL
and the SBL algorithms. Fig. 5 also shows the performance of the optimal
genie-aided MIP-aware Kalman tracking algorithm (Chen and Zhang, 2004)
which considers all the subcarriers as carrying pilot symbols. The SBL and
the J-SBL algorithms are not designed to exploit the temporal correlation in
the channel, and hence, they perform 7–8 dB poorer than their recursive coun-
terparts, the KSBL and the JK-SBL algorithms. At higher SNR, we observe
that the performance of the JK-SBL algorithm is only 2 dB worse than the
genie-aided MIP-aware Kalman tracking algorithm with all subcarriers being
pilot subcarriers.
Sparsity-aware Bayesian inference and its applications Chapter
8 185

In Fig. 6, we depict the BER performance of the presented algorithms. In
the coded case, the JK-SBL performs about 2 dB better than the J-SBL algo-
rithm, and it is only a fraction of a dB away from performance of the genie
receiver which has perfect channel knowledge. The JK-SBL outperforms
pilots-only-based channel estimation using the K-SBL and the SBL algo-
rithms by a large (4–5 dB) margin. Further, it outperforms the MIP-aware
EM-OFDM algorithm, since the latter is unaware of the channel correlation,
and performs channel estimation on a per-OFDM symbol basis; while the
JK-SBL algorithm exploits the channel correlation to improve the channel
estimates.
5
Intravector correlations: The nested SBL algorithm
In this section, we consider sparse signals in which the nonzero entries are
constrained to occur in a few clusters, i.e., they are block-sparse, and the
entries within a block are possibly correlated with each other. We reformulate
the sparse recovery problem in (1), where, in addition to the sparse vector, a
set of hidden variables are introduced in order to decompose the sparse vector
recovery problem into a set of low-dimensional sparse vector recovery pro-
blems to explicitly impose the block-sparse structure of the signal. This leads
us to a Nested SBL (NSBL) algorithm for block-sparse vector recovery, by
10
15
20
25
30
10−5
10−4
10−3
10−2
10−1
100
101
SNR
MSE
 
 
FDI
SBL
J−SBL
K−SBL
EM−OFDM
JK−SBL
MIP−aware Kalman
FIG. 5
MSE of different schemes in a time-varying channel vs the SNR (dB), compared to the
optimal Kalman tracker (Chen and Zhang, 2004), with fdTs ¼ 0.001 and Pb ¼ 44.
186
Handbook of Statistics

employing the Nested EM (Van Dyk, 2000) approach and a Kalman-based
learning framework for learning the intrablock correlation. NSBL is based on
a divide-and-conquer approach: the problem of estimating one block-sparse
vector is formulated as a problem of estimating a set of low-dimensional corre-
lated group-sparse vectors.
We model the sparse vector x as consisting of B blocks b1, …, bB, each of
length m0, given by
x ¼ ½x11,x12,…,x1m0
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
bT
1 :b1 m01
; …; xB1,xB2,…,xBm0
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
bT
B:bB m01
:
(47)
The m0 entries of each block bi are either all zero or all nonzero. In the
Cluster-SBL framework, the block-sparse structure is exploited by modeling
bi  N ð0,γiBiÞ , where γi is an unknown hyperparameter such that when
γi ¼ 0, the ith block of x is zero (Zhang and Rao, 2012). Here, Bi  m0m0
is a positive-definite covariance matrix that captures the intrablock correlation
of the ith block. Moreover, different blocks are mutually uncorrelated, and
hence, the block-sparse vector x  N ð0,Σ0Þ, where Σ0 is a block-diagonal
matrix where each principal block is given by γiBi, 1  i  B.
Restructuring the sparse vector x in such a way that the first entries in all
the blocks bi occur together, followed by their second entries, etc., we can
5
10
15
20
25
30
Eb/N0
BER
 
 
SBL
J−SBL
K−SBL
EM−OFDM
JK−SBL
Genie
Solid: Uncoded
Dashed: Coded
10−5
10−6
10−4
10−3
10−2
10−1
FIG. 6
BER of different schemes in a time-varying channel with fdTs ¼ 0.001 and Pb ¼ 44, as a
function of the signal to noise ratio Eb/N0 (dB).
Sparsity-aware Bayesian inference and its applications Chapter
8 187

write x as consisting of m0 blocks x1,x2,…,xm0 , with each block containing
B entries (that is, N ¼ m0B.) Then, the problem of block-sparse vector
recovery can be stated as that of recovering the vectors x1, …, xm0. Since
bi  N ð0, γiBiÞ for 1  i  B, xi  N ð0, ΓÞ where Γ ¼ Diag
γ1, …, γB
f
g,
i.e., x1, …, xm0 represent the group-sparse vectors. It can be seen that by cor-
respondingly rearranging the columns of A, the system model in (22) can be
rewritten in terms of tm  m1 as
y ¼
X
m0
j¼1
tj
where tj ¼ Ajxj + wj, 1  j  m0,
(48)
where Aj  mB
consists of the columns of A such that the weights
corresponding to its columns are given by xj. Although the vectors wj cannot
be explicitly obtained, we note that they can be modeled as wj  N ð0,βjσ2I0
mÞ
where 0  βj  1 and Pm0
j¼1 βj ¼ 1. Now, if tj were known, recovering
xj from tj is a multiple measurement vector-based group-sparse recovery
problem (Wipf and Rao, 2007) in a lower dimension (B), as compared
to the original problem dimension (m0B). In this section, we focus on
recovering the block-sparse vector by recovering its components x1, …,
xm0, using the restructured problem given by (48). We present the NSBL
algorithm for block-sparse vector recovery with intrablock correlation
(B 6¼ IB).
The conventional SBL framework treats (y, x) in (1) as the complete data,
and x as the hidden variable. However, for the reformulated system model
in (48), it is essential to augment the set of hidden variables x with
t ¼ ½tT
1,…,tT
m0
T since t is hidden (Feder and Weinstein, 1988). Accordingly,
the complete information is given by (y, t, x), and (t, x) constitute the hidden
variables. In order to obtain the maximum likelihood estimate of the unknown
parameter γ, we extend the EM algorithm as follows. The E-step requires the
computation of p t, xjy; γðrÞ


, which can be written as
p t, xjy; γðrÞ


¼ p xjt, y; γðrÞ


p tjy; γðrÞ


¼ p xjt; γðrÞ


p tjy; γðrÞ


(49)
Hence, the E-step can be rewritten as E-step : Q γjγðrÞ


¼ tjy; γðrÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
t
xjt; γðrÞ
|ﬄﬄﬄ{zﬄﬄﬄ}
x
½ log pðy,t,x; γÞ. To compute Q γjγðrÞ


, we first compute the posterior distri-
bution p(tjy;γ(r)) using the likelihood pðtjjxjÞ ¼ N ðAjxj,βjσ2Þ for 1  j  m0,
and the prior pðx; γÞ ¼ N ð0,ΓBÞ, with ΓB ¼ B  Γ, where Γ ¼ Diag
γ
f g
and A  B represents the Kronecker product of the matrices A and B. Given
H ¼1m0  Im0, where 1m0 is a m0 length vector of ones, and y ¼ Ht, we compute
pðtjy; γðrÞÞ ¼ N ðμt,ΣtÞ where
188
Handbook of Statistics

μt ¼ ðR + ABΓBAT
BÞHTðHðR + ABΓBAT
BÞHTÞ1y
Σt ¼ ðR + ABΓBAT
BÞðR + ABΓBAT
BÞHTðHðR + ABΓBAT
BÞHTÞ1HðR + ABΓBAT
BÞ:
(50)
Here, AB  mm0Bm0 is a block diagonal matrix with A1, …, Am0 along the
diagonal. The block diagonal matrix R consists of diagonal entries given by
Rj ¼ βjσ2Im0. Note that the posterior mean μt  mm01 consists of m0 vectors,
μt1, …, μtm0 such that Hμt ¼ y, i.e., y ¼ Pm0
j¼1 μtj. Further, the posterior distri-
bution p(xjt; γ(r)) depends on the correlation between the vectors x1, …, xm0.
In the literature on block-sparse recovery, several papers look at the case
where the sparse vectors are uncorrelated, i.e., B ¼ IB (Eldar and Mishali,
2009; Prasad et al., 2014b; Stojnic et al., 2009). In the next section, we present
an algorithm for block-sparse vector recovery when B 6¼ IB.
5.1
Nested SBL (B 6¼ IB)
Here, we model the intrablock correlation using the first-order AR model and
propose the NSBL algorithm to learn the unknown parameters, γ, and the cor-
relation coefficient.
The first-order AR model is a widely accepted one and finds several appli-
cations (Akaike, 1969; Wei, 1994; Zhang and Kassam, 1999). Here, we
employ this model for the intrablock correlation as it avoids overfitting and
allows for a Kalman-based learning framework. The evolution of the mth
group sparse vector is given by
xj ¼ ρxj1 + uj, j ¼ 1,…, m0,
(51)
where the driving innovation process uj is independent across time and
distributed as ujðiÞ  N ð0,ð1  ρ2ÞγiÞ, ρ  (0, 1) is the AR coefficient. Overall,
this leads to a common correlation matrix given by B ¼ Toep ½1,ρ,…, ρm01


,
where Toep a
f g represents the symmetric Toeplitz matrix defined by its first
row a. The state space model for tm and xm is given as
tj ¼ Ajxj + wj,
(52)
xj ¼ ρxj1 + uj,
j ¼ 1,…, m0:
(53)
Since x1, …, xm0 are group-sparse, the joint pdf is given by
pðt,x1,…, xm0;γÞ ¼
Y
m0
j¼1
pðtjjxjÞpðxjjxj1; γÞ,
(54)
where pðx1jx0; γÞ ≜pðx1; γÞ . Using (54), the posterior distribution of the
sparse vectors given by p(x1, …, xm0jt;γ(r)) is computed using the recursive
KFS equations for 1  j  m0 as follows
Sparsity-aware Bayesian inference and its applications Chapter
8 189

for j ¼ 1,…,m0 do
Prediction : ^xjj j1 ¼ ρ^xj1j j1
(55)
Pjj j1 ¼ ρ2Pj1j j1 + ð1  ρ2ÞΓ
(56)
Filtering : Gj ¼ Pjj j1AT
j
σ2I + AjPjj j1AT
j

1
(57)
^xjj j ¼ ^xjj j1 + Gjðtj  Aj^xjj j1Þ
(58)
Pjj j ¼ ðI  GjAjÞPjj j1,
(59)
end
Smoothing : For j0 ¼ m0,m0  1,…,2
^xj01j j ¼ ^xj01jj01 + Jj01ð^xj0j j  ^xj0jj01Þ
(60)
Pj01j j ¼ Pj01jj01 + Jj01ðPj0j j  Pj0jj01ÞJT
j01
end,
(61)
where Jj01 ¼ ρPj01jj01P1
j0jj01
and Gj is the Kalman gain. The above-
mentioned KFS equations are initialized by setting ^x0j0 ¼ 0, i.e., a zero vector,
and P0j0 ¼ Γ. E-step also requires the computation of x1,…,xm0jy;γðrÞ½xj0xT
j01 ≜
Pj0, j01jj + ^xj0jj^xT
j01jj for j ¼ m0, m0 1, …, 2, which we obtain from
Ghahramani and Hinton (1996) as follows:
Pj01,j02j j ¼ Pj01jj01JT
j02 + JT
j01ðPj0,j01j j  ρPj01jj01ÞJj02:
(62)
The above recursion is initialized using Pj, j1jm0 ¼ ρ(I GjAj)Pj1jj1. Note
that xijm0 and Pijm0, 1  i  m0 represent the posterior mean and covariance
of x w.r.t. t, respectively. The expectation t involves computing p(tjy;γ(r))
using (50). Due to the recursive nature of the inner E-step, x, the expectation
of μxm w.r.t. posterior density of t is a recursive function of tj, …, t1. As m0
increases, the complexity of such a recursive expectation expands. In order
to circumvent this problem, we employ an alternate technique known as the
Nested EM approach (Van Dyk, 2000). This monotonically convergent
approach allows us to simplify the overall algorithm into an inner and outer
EM loop, while the unknown parameter γ is the common factor between
the two loops. We call this algorithm as the NSBL algorithm, where the
nested E and the M steps are given as
E-step : Q γjγ r+k
K
ð
Þ, γðrÞ


¼ tjy; γðrÞ 
x1,…,xm0jt; γ r+k
K
ð
Þ½ log pðy, t, x1, …, xm0; γÞ


M-step : γ r+k+1
K
ð
Þ ¼ arg max
γ B1
+
Q γjγ r+k
K
ð
Þ, γðrÞ


:
(63)
190
Handbook of Statistics

The inner EM loop is initialized by γ r+0
K
ð
Þ ¼ γðrÞ. Note that when γ is updated
in every iteration, only the inner E-step (x ¼ 
x1,…,xm0jt; γ
r+k
K
ð
Þ½) is updated.
The overall NSBL algorithm is executed by nesting one EM loop within the
other. The inner EM loop consists of x and the corresponding posterior dis-
tribution is given by (55)–(59). Further, the M-step for the inner EM loop is
given by
γ
r+ðk+1Þ
K
ð
Þ
i
¼ 1
m0
X
m0
j¼2
Mjjm0ði, iÞ
ð1  ρ2Þ + M1jm0ði, iÞ
 
!
(64)
for 1  i  B, where Mjjm0 ≜Pjjm0 + ^xjjm0 ^xT
jjm0 + ρ2ðPj1jm0 + ^xj1jm0 ^xT
j1jm0Þ 
2ρðPj,j1jm0 + ^xjjm0 ^xT
j1jm0Þand M1jm0 ≜P1jm0 + ^x1jm0 ^xT
1jm0 . After K iterations of
the inner EM loop, we obtain γr+K
K ¼ γr+1, which affects the posterior distribu-
tion of t. The outer EM loop consists of updating the posterior distribution of
t given in (50). In a similar way, an update step for the correlation coefficient
ρ, if it is unknown, can be incorporated into the M-step of the NSBL algorithm.
The correlation coefficient ρ r+ðk+1Þ
K
ð
Þ is obtained as given in Prasad et al. (2014b).
In Figs. 7 and 8, we demonstrate the MSE and the support recovery perfor-
mance of the NSBL algorithm, and compare it with BSBL (Zhang and Rao,
2013), SBL, and the OMP algorithms, which are unaware of the correlated
20
25
30
35
40
100
101
102
103
104
SNR
MSE
 
 
OMP
SBL
BOMP
BSBL
NSBL
FIG. 7
MSE performance of the NSBL as compared to BSBL, BOMP, SBL, and OMP
algorithms for m0 ¼ 40, N ¼ 100, B ¼ 5, and S ¼ 1.
Sparsity-aware Bayesian inference and its applications Chapter
8 191

cluster-sparse nature of sparse vectors. Since NSBL employs EM-based updates
for updating the unknown correlation coefficient ρ unlike the heuristic updates-
based BSBL algorithm, it offers superior performance. Further, the cluster-
sparse structure-aware NSBL and the BSBL algorithms outperform the SBL
and OMP algorithms.
6
Quantized sparse signal recovery
Quantization plays a pivotal role in applications such as wireless communi-
cations, where the complex baseband signal is passed through analog-to-
digital converters before further processing. Reducing the number of bits used
for quantization saves power in the analog-to-digital converter. In addition, all
the further downstream processing happens at lower resolution and can therefore
be done faster, using lower hardware resources and energy consumption. There-
fore, if the signal can still be recovered from low-resolution quantized samples,
it saves hardware cost as well as energy consumption at the receiver. Now, since
wideband wireless channels have a sparse time-domain impulse response, the
problem of channel estimation from training or pilot symbols sent by the trans-
mitter can be equivalently cast as a sparse signal recovery problem. When the
received signals are first quantized before further processing, this then becomes
a problem of recovering high-dimensional sparse vectors from quantized, noisy,
compressive (low-dimensional, linear) measurements.
20
30
40
Number of Measurements (N)
Success Rate
50
60
70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 
 
NSBL
BSBL
BOMP
SBL
OMP
FIG. 8
Success rate of the NSBL as compared to BSBL, BOMP, SBL, and OMP algorithms for
SNR ¼ 40, N ¼ 100, B ¼ 5, and S ¼ 1.
192
Handbook of Statistics

This section describes a Bayesian statistical inference procedure to recon-
struct sparse signals from quantized compressed sensing measurements.
Recall the unquantized noisy compressed sensing system model considered
thus far in this chapter:
Y ¼ AX + W,
(65)
where
Y ¼ y1…yL
½
  mL,
X ¼ x1…xL
½
  NL,
A  mN is a known measurement matrix, and m < N. As mentioned earlier,
X has a joint row-sparse structure, i.e., the support set of each column of X is
the same. We quantize the real and imaginary parts of Y using a b-bit scalar
quantizer Qb to get the quantized observations
Q ¼ QbðYÞ  mL:
(66)
A b-bit quantizer on a real-valued input z is defined as QbðyÞ ¼
Li, z  ½δi,δi+1Þ, i ¼ 0,1,…,B  1, where B ¼ 2b is the number of quantization
levels, ∞¼ δ0 < δ1 < ⋯< δB ¼ ∞are the quantization thresholds, and
L0,L1,…,LB1 are the quantizer outputs. For simplicity and concreteness,
we consider a uniform quantizer, where δ‘ ¼ ðB=2 + lÞΔ, ‘ ¼ 1,…, B  1,
Δ is the quantization step size, and L‘ ¼ ðδ‘ + δ‘+1Þ=2,‘ ¼ 0,…, B  1.
We adopt a statistical inference approach, where we represent the sparse
signal recovery problem using a probabilistic graphical model. We describe
the procedure to infer the posterior distribution of X given the measurement
matrix A and the quantized compressed sensing measurements Q. Exact com-
putation of the posterior distribution is computationally intractable, as it
requires solving high-dimensional integrals over X to obtain the partition
function P(Q). This motivates the need for alternative approximate inference
techniques, where we replace the exact posterior distribution with a distribu-
tion that is close to the original in a particular distance measure, and is also
easy to compute. As we will show, this leads to a computationally tractable
algorithm for the problem at hand.
We briefly describe the variational Bayesian (VB) inference, which is the
approximate inference technique adopted to solve the joint row-sparse recov-
ery problem given in (66). VB is an iterative procedure to compute approxi-
mate posterior distributions of the latent variables given the observations
(Bishop, 2006; Ding et al., 2017; Thoota and Murthy, 2022). In this, we first
write the logarithm of the observations Q as the sum of two terms and lower-
bound it as
ln pðQÞ ¼ LðqÞ + KLðq k pÞ 	 LðqÞ,
(67)
Sparsity-aware Bayesian inference and its applications Chapter
8 193

where
LðqÞ ≜
Z
qðXÞ ln
pðQ, X; A, σ2
wÞ
qðXÞ


dX
(68)
KLðqkpÞ ≜
Z
qðXÞ ln
pðXjQ, A, σ2
wÞ
qðXÞ


dX 	 0
(69)
are the evidence lower-bound (ELBO) and nonnegative Kullback Leibler
(KL) divergence terms, respectively. Here, q(X) is a posterior distribution,
which is arbitrary, and can be approximated and optimized. Note that q(X)
depends on Q, but we do not explicitly include it in the notation for brevity.
In the above, maximizing the ELBO LðqÞ would render a distribution q that
is close to the original model evidence. We formally state the ELBO maximi-
zation problem as
qopt ¼ arg max
q P
LðqÞ ¼ arg min
q P
KLðq k pÞ,
(70)
where P is the space of probability distributions. The maximum of LðqÞ
occurs when qðXÞ ¼ pðXjQ, A,σ2
wÞ, but computing it is intractable. Therefore,
we impose a factorized structure on each column of X, i.e.,
qðXÞ ¼
Y
L
i¼1
qiðxiÞ:
(71)
Substituting this in the ELBO, and simplifying it by fixing one of the factors,
say qj(xj), we get
LðqÞ ¼ KL qj k epðQ, xj; A, σ2
wÞ


+ constant,
(72)
where the constant terms do not depend on qj(xj), and epðQ,xj; A,σ2
wÞ is
defined using
ln epðQ,xj; A,σ2
wÞ ≜i6¼j ln pðQ, X; A, σ2
wÞ
	

+ constant,
(73)
where the notation i6¼j½: denotes the expectation with respect to the distribu-
tions q1ðx1Þ,…,qTðxTÞ except qj(xj). Now, LðqÞ is maximized when the KL
divergence term in (72) is minimized, which happens when qjðxjÞ ¼
epðQ,xj; A,σ2
wÞ. Therefore, the optimal marginal distribution is
qjðxjÞ ¼ constant  exp i6¼j ln pðQ, X; A, σ2
wÞ
	



,
(74)
where the constant is chosen such that qj becomes a probability distribution.
Thus, VB is an iterative algorithm that falls in the category of minorization-
maximization (MM), which solves a maximization problem by iteratively obtain-
ing a lower bound on the objective function as in (67), and maximizing it. It is
known that MM-based optimization converges to a stationary point of the origi-
nal optimization problem from any initialization (Hunter and Lange, 2004).
194
Handbook of Statistics

The marginal distribution in (74) is still hard to compute, as pðQ,X; A,σ2
wÞ
contains terms involving the difference of the CDF of complex Gaussian
random vectors. Hence, we add Y also as a latent variable. This leads to a
closed-form solution as described below.
We use the Bayesian network in Fig. 9 to express the logarithm of the joint
probability distribution of the observations and latent variables as
ln pðQ, Y, X, α; A, σ2
w, a, rÞ ¼ ln pðQ j YÞ+ ln pðY j X; A, σ2
wÞ+ ln pðXjPÞ
+ ln pðα; a, rÞ,
(75)
where the prior distributions of X and α are
pðXjPÞ ¼
Y
L
n¼1
jPj
πN exp xH
n Pxn


,
pðα;a, rÞ ¼
Y
N
k¼1
ra
ΓðaÞαa1
k
exp rαk
ð
Þ,
(76)
respectively, where Γ(a) is the Gamma function evaluated at a > 0. We set
a and r to small values (say, 106) such that the hyperprior p(α;a, r) is
noninformative. We approximate the posterior pðY, X, α j Q;A,σ2
w, a, rÞ of
the latent variables as the factorized distribution:
pðY, X, α j Q;A, σ2
w, a, rÞ  qXðXÞqYðYÞqαðαÞ
¼
Y
L
n¼1
qxnðxnÞ
Y
L
n¼1
qynðynÞ
Y
N
k¼1
qαkðαkÞ:
(77)
Next, we express the conditional probability distributions of the observations
and latent variables that are needed to compute the posterior distributions
under the factorized structure as
pðQ j YÞ¼
Y
m
t¼1
Y
L
n¼1
1 <ðytnÞ < yðloÞ
tn


, < yðhiÞ
tn






1 IðytnÞ I yðloÞ
tn


, I yðhiÞ
tn






≜
Y
L
n¼1
1 yn  yðloÞ
n
,yðhiÞ
n




,
(78)
FIG. 9
Bayesian network model for the estimation problem (66).
Sparsity-aware Bayesian inference and its applications Chapter
8 195

p Y j X; A, σ2w


¼
Y
L
n¼1
1
ðπσ2wÞM exp
 1
σ2w
k yn Axnk2
 
!
,
(79)
where ytn is the (t, n)th entry of Y, 1() is the indicator function, and yðloÞ
tn
and
yðhiÞ
tn
are the lower and upper quantization thresholds corresponding to the
(t, n)th entry of Q, respectively. The posterior distributions of the latent vari-
ables are computed by finding the expectations of the logarithm of the joint
distribution (75) with respect to the latent variables, and are provided in
closed form in the following three lemmas.
Lemma 1. (Computation of qX(X)). The posterior distribution qX(X) is com-
plex normal with the covariance matrix of each of its columns and mean
given by
ΣX ¼
1
σ2
w
AHA + P
h i

1
,
(80)
X
h i ¼ 1
σ2
w
ΣXAH Y
h i,
(81)
respectively. Here, P
h i ¼ Diag
α
h i
f
g, and Y
h i and α
h i are the posterior means
of qY(Y) and qα(α), respectively.
Lemma 2. (Computation of qY(Y)). The posterior distribution qY(Y) is
truncated complex normal with mean Y
h i given by
Y
h i ¼ A X
h i + σwﬃﬃﬃ
2
p
f
YðloÞ  A X
h i
σw=
ﬃﬃﬃ
2
p
 
!
 f
YðhiÞ  A X
h i
σw=
ﬃﬃﬃ
2
p
 
!
F YðhiÞ  A X
h i
σw=
ﬃﬃﬃ
2
p
 
!
 F YðloÞ  A X
h i
σw=
ﬃﬃﬃ
2
p
 
! ,
(82)
where Y(lo) and Y(hi) are the lower and upper quantization levels corresponding
to the observation Q, respectively, and hXi is the posterior mean of qX(X). Also,
f() and F() are the pdf and cumulative density function of a standard normal
random variable, respectively, computed element-wise on the real and imagi-
nary parts of the argument. The division operation in (82) is also performed
element-wise.
Lemma 3. (Computation of qαkðαkÞ, k ¼ 1,…, N). The posterior distribution
qαkðαkÞ follows a Gamma distribution with shape and rate parameters
given by
196
Handbook of Statistics

eak ¼ a + L
and
erk ¼ r +
X
L
n¼1
hjxknj2i,
respectively. Its mean is given by
αk
h
i ¼
a + L
r +
XL
n¼1hjxknj2i
,
(83)
where hxkni is the (k,n) th element of hXi, and hjxknj2i ¼ jhxknij2 + ΣX[k, k].
Note that we have included the subscript k in eak for consistency of nota-
tion, even though it is independent of k. From (80), (81), (82), and (83), we
see that the statistics of the posterior distributions qX(X), qY(Y), and qα(α)
depend on each other. The VB algorithm proceeds iteratively by randomly
initializing the posteriors and alternately computing each of the posterior dis-
tributions until a suitable convergence condition is satisfied. Once the algo-
rithm converges, we use the posterior mean from (81) as the final estimate.
We present the quantized VB procedure below:
Input: Q, A, σw
Output: hXi
Initialize : hYi, hαi, a, r
repeat
hPi ¼ Diag
hαi
f
g
Compute :ΣX,hXi, hYi, hαkiusing ð80Þ; ð81Þ; ð82Þ; ð83Þ; respectively
until stopping condition is met
We demonstrate the normalized MSE performance of the quantized VB
procedure in the context of lag/delay-domain sparse channel estimation in a
massive multiple-input-multiple-output (MIMO) OFDM communication sys-
tem with low-resolution analog-to-digital-converters (ADCs) (Thoota and
Murthy, 2022). The number of measurements is the product of the number
of pilot OFDM symbols (≜τp) and the number of subcarriers (≜Nc). The
number of multiple measurement vectors is equal to the number of receive
antennas (≜Nr) at the base station (BS). The dimension of the sparse vector,
N, is the product of the number of users and the length of the delay domain
channel of each user. We denote the number of nonzero taps of each user’s
channel by Lsp, and therefore, the total sparsity is the product of the number
of users and Lsp. We define the SNR as 1=σ2
w and set it to 10 dB.
Fig. 10 shows the normalized MSE (dB) of the QVB algorithm as a func-
tion of the number of users for the two cases of 1 and 2 pilot OFDM symbols
per user. We also include the performance of the MSBL algorithm with
Sparsity-aware Bayesian inference and its applications Chapter
8 197

unquantized measurements (labeled “UQ”) for comparison. We see that the
performance of the QVB algorithm improves as the ADC resolution increases
from 2 to 4 bits. As τp increases from 1 to 2, the number of measurements gets
doubled, which leads to improvement in the performance of QVB for all the
ADC resolutions. Moreover, an ADC resolution of 2 bits leads to large quan-
tization noise which results in a severe performance degradation. Therefore, a
minimum ADC resolution of 3 bits provides a good trade-off between perfor-
mance and compute requirements.
7
Other extensions
In this section, we briefly discuss a few useful extensions of the SBL frame-
work, for example, to the decentralized setting, dictionary learning problem,
and deep learning-aided implementation.
7.1
Decentralized SBL
A useful extension of the MMV problem in (10) considers in-network, decen-
tralized estimation of the common nonzero support of the multiple jointly
sparse column vectors in X by a network of computing nodes/agents. The
jth column of X, denoted by xj, is estimated by the jth network node with
access to its m compressive measurements in the form of yj. Each node is
allowed to communicate with a small subset of its neighboring nodes. By col-
laborating among themselves via message exchanges, the nodes can exploit
the underlying joint sparsity of their respective local sparse vectors to jointly
estimate their common support from significantly fewer measurements com-
pared to estimating it independently using only the local measurements. Once
the common support has been estimated, the nonzero coefficients in the jth
column of X can be estimated locally by node j.
4
8
12
16
20
24
Number of users
-13
-12
-10
-8
-6
-4
-2
NMSE (dB)
p=1, 2 bits
p=1, 3 bits
p=1, 4 bits
p=1, UQ
p=2, 2 bits
p=2, 3 bits
p=2, 4 bits
p=2, UQ
FIG. 10
Normalized MSE (dB) as a function of the number of users in a massive MIMO-OFDM
system with low-resolution ADCs. SNR is set to 10 dB.
198
Handbook of Statistics

This decentralized extension of the MMV problem is particularly useful in
scenarios involving multiple connected agents forming a network, trying to
learn a local sparse parameterized model of an unknown but common physical
phenomenon. Since each agent observes the same underlying phenomenon,
their respective sparse model parameters tend to exhibit joint sparsity. The
reader is referred to Khanna and Murthy (2017b) and Khanna and Murthy
(2017a) for a detailed presentation of algorithms, performance analysis, and
guarantees for Bayesian decentralized sparse signal recovery problems.
7.2
Dictionary learning
All the algorithms discussed so far assume the knowledge of the measurement
matrix, or dictionary, A. In applications such as image denoising, audio pro-
cessing, and classification tasks, we need to design this dictionary depending
on the class of sparse signals of interest. The dictionary can either be non-
adaptive and predefined (such as Fourier, Gabor, discrete cosine transform,
and wavelet) or adaptive and learned specific to the given set of sparse sig-
nals. The problem of learning a dictionary, such that a set of training signals
admits a sparse representation, is called the dictionary learning problem.
Specifically, we consider a set of L noisy measurements y1,y2,…,yL. The goal
is to find a dictionary matrix A  mN, for a given N, such that yk admits a
representation in terms of the columns of A in the following manner:
yk ¼ Axk + wk,
for k ¼ 1,2,…, L,
(84)
where xk is sparse and the noise term wk  N ð0, σ2IÞ. Obviously, xk cannot
have a common support in this case: we cannot recover A unless all its col-
umns participate in obtaining the measurement vectors. We note that scaling
xk and A up and down, respectively, by any nonzero factor does not change
the sparsity level of xk or the representation of yk. So, to resolve the ambiguity
in amplitude, we assume that the unknown dictionary A has unit norm
columns, i.e., A  A, where
A ¼
A  mN : AT
i Ai ¼ 1, i ¼ 1, 2, …, N


:
(85)
To develop a dictionary learning algorithm using the SBL framework
( Joseph and Murthy, 2020), we impose a Gaussian prior on the unknown
sparse vectors xk  N ð0,Diag γk
f
gÞ, with γk  N
+ being an unknown hyper-
parameter. We do not impose any prior on A, which is equivalent to assuming
a uniform prior over the space A. Therefore, the hyperparameters of the model
are A and γk, k ¼ 1,2,…,L.
Similar to SBL, the dictionary learning algorithm first computes the ML
estimates ^γk and ^A of the hyperparameters which are then used to estimate
the sparse vectors as ^xk ¼ xkjyk, ^γk, ^A


: To obtain ^γk and ^A, we minimize
the negative log likelihood of the measurements using the EM algorithm.
Here, the optimization in the M-step is separable in its variables γk and A.
Sparsity-aware Bayesian inference and its applications Chapter
8 199

As before, the update rule for γk has a closed-form expression. The dictionary
update step in the EM algorithm is a quadratic optimization problem with unit
norm constraints, which is a nonconvex problem because of the constraint.
Since a closed-form solution is not available, the optimization problem can
be solved using two iterative methods, namely, alternating minimization and
Armijo line search.
Apart from the superior recovery performance, the SBL-based dictionary
learning algorithm enjoys unique theoretical guarantees compared to other
dictionary learning algorithms. In particular, the underlying cost function of
this formulation ensures that the algorithm converges to the sparsest possible
representation. Further, the algorithm has convergence and stability guaran-
tees. The reader is referred to Joseph and Murthy (2020) for the derivation
of the algorithms, their convergence and stability guarantees, and the analysis
of sparsity of the solution.
7.3
Relationship with robust principal component analysis and
sparse + low-rank decomposition
In the previous sections, we have presented several Bayesian algorithms for
recovering an unknown sparse vector from noisy underdetermined measure-
ments as in (1) and (10). The conventional sparse recovery framework
assumes the noise vector w to be Gaussian-distributed with zero mean and
(scaled) identity covariance matrix, i.e., w  N ð0,σ2ImÞ, or that it is bounded
(Cai et al., 2009). However, in many real-world scenarios, the noise is not
white, i.e., ½ww| ¼ Q, where Q is nonnegative definite and possibly rank-
deficient (Ling and Tian, 2010; Qiu and Vaswani, 2011; Tian et al., 2014),
where the matrix Q may be known or unknown. It is therefore pertinent to
consider the problem of recovering sparse vectors from multiple measurement
vectors (10), where the underdetermined linear measurements are corrupted
by colored noise, with a possibly unknown and rank-deficient covariance
matrix. Rank-deficient noise occurs in many practical scenarios such as mag-
netic resonance imaging (Lu et al., 2011), real-time video surveillance (Qiu
and Vaswani, 2011), and direction-of-arrival (DoA) estimation (Malek-
Mohammadi et al., 2014). Bayesian approaches are capable of elegantly
incorporating the structure in the noise covariance matrix into the problem
of sparse recovery.
When the additive noise is colored, one can exploit knowledge of its
covariance matrix to improve the performance of sparse signal recovery algo-
rithms. Furthermore, when the noise covariance matrix is rank-deficient, it
offers the possibility of projecting the measurements into noise-free and
noise-corrupted subspaces, and jointly recovering the sparse vector from a
combination of noiseless and noisy measurements. However, when the noise
covariance is unknown, it is unclear whether the deleterious effect of the error
in estimating the noise covariance matrix can overcome the benefit obtained
200
Handbook of Statistics

by exploiting its (partial) knowledge in sparse signal recovery algorithms. We
also remark that, in the MMV setup with unknown noise covariance matrix,
the observation model in (22) corrupted by rank-deficient noise can be seen
as a generalization of the robust PCA problem (Cande`s et al., 2011; Wright
et al., 2009). Here, the vectors wi, which lie in a low-dimensional subspace
that needs to be identified, are corrupted by vectors that are sparse in an over-
complete dictionary. In contrast, the robust PCA problem restricts attention to
the case where the sparse corruption is in the canonical basis, i.e., A ¼Im.
Thus, progress in this area would yield insights into this important extension
of the robust PCA problem as well.
7.3.1
SBL-based algorithms in the presence of colored noise
Let us consider the case where Q is known. We decompose Q using the eigen-
value decomposition as Q ¼ VΛV|, where the columns of V  mm contain
the orthonormal eigenvectors of Q, and the diagonal matrix Λ contains the
eigenvalues of Q. When the rank of Q is p ( m), it can be decomposed as:
Q ¼ ½V1V2
D
0pmp
0mpp
0mpmp

 V|
1
V|
2


,
(86)
where 0pm represents a p  m all zero matrix, and D is a diagonal matrix
containing the p nonzero eigenvalues of Q. By premultiplying the observa-
tions Y by V|
1 and V|
2, we get the projected observations Yi ¼ AiX + Wi, where
Yi ¼ V|
i Y, Ai ¼ V|
i A, and Wi ¼ V|
i W. Under this projection, W1 has a diago-
nal, full rank covariance matrix D, while W2 ¼ 0mpL. Our problem thus
reduces to the recovery of sparse signals from a mixture of noisy and noiseless
measurements. EM-based algorithms can be employed to solve the above
problem.
7.3.2
Covariance matching-based algorithms
In an alternate approach, in order to estimate the row sparse matrix (and noise
covariance matrix if unknown), we use the empirical covariance of the obser-
vations,
1
L YY| , and perform covariance matching motivated by Pal and
Vaidyanathan (2015b) and Khanna and Murthy (2017e). In covariance match-
ing, we seek to match the empirical covariance matrix with a parameterized
covariance matrix derived from an assumed prior on the sparse vectors and
noise. Following the SBL approach, we assume that the sparse vectors are
independent and identically distributed (i.i.d.) Gaussian with zero mean and
unknown diagonal covariance matrix Γ. That is, the ith diagonal entry γi
of Γ represents the variance of the elements in the ith row of X. Note that
γi ¼ 0 indicates that the corresponding row of X is all zero almost surely.
Under this model, learning a sparse Γ results in a joint row-sparse solution
to the problem. As before, the noise is modeled as Gaussian distributed with
zero mean and covariance matrix Q. Further, we let Σy ¼ AΓA| + Q denote
Sparsity-aware Bayesian inference and its applications Chapter
8 201

the parameterized covariance matrix of the observations, resulting from our
prior on the inputs and noise, and the linear observation model. Thus, we seek
to solve
min
Q, Γ 0dist 1
L YY|, Σy


,
(87)
where dist(, ) is a function that measures the distance between a pair of non-
negative definite matrices. In this work, similar to Pal and Vaidyanathan
(2015b), we use the Frobenius norm as a distance measure; other choices such
as Renyi divergence can also be explored (Khanna and Murthy, 2017c,e).
In order to promote a sparse solution for Γ and a low-rank solution for
Q, we introduce appropriate penalties in the optimization problem above.
Convex relaxation-based algorithms can be employed for estimating the
hyperparameters Γ and Q.
7.4
Deep unfolded SBL
Deep unfolding (see, e.g., Balatsoukas-Stimming and Studer, 2019) is a tech-
nique where an iterative algorithm is implemented using neural networks.
Here, the iterations of the algorithm are unfolded into a layer-wise architec-
ture, and the computations within each layer are replaced with a neural net-
work. The parameters of the neural network are then learned using suitable
training data. Such an approach can potentially bring many benefits. First,
such an implementation seeks to find weights to offer the best possible perfor-
mance given that we are only allowed to execute fixed number of iterations of
the original algorithm. Surprisingly, in our experiments, we find that the neu-
ral network-based implementation with a fixed, small number of layers (itera-
tions) can even outperform the original algorithm when it is allowed 10 or
20 times the number of iterations. Second, it can be used to learn structure
in the signal to be recovered beyond sparsity, such as group sparsity, cluster
sparsity, piece-wise sparsity, inclusion-exclusion sparsity, etc. Due to lack
of space, we omit the description of an unfolded version of the SBL algorithm
here; we refer the reader to Peter and Murthy (2019) for details.
8
Discussion and future outlook
In this chapter, we discussed the SBL approach to sparse signal recovery, and
presented several interesting and practically relevant extensions such as joint
sparse recovery, cluster-sparse recovery, handling correlations within and
among the sparse vectors, dictionary learning, etc. The primary advantage
of these techniques compared to what one might call first-order techniques
such as convex relaxation, greedy, or iterative reweighted techniques is their
superlative performance, especially in measurement-constrained regimes.
Other interesting aspects that we did not discuss in detail here include faster
versions of the algorithms based on techniques such as approximate message
202
Handbook of Statistics

passing (Al-Shoukairi et al., 2018), exponentiated gradient updates (Khanna
and Murthy, 2018b) and deep unfolding (Peter and Murthy, 2019), extensions
to handle correlated noise with a possibly rank-deficient covariance matrices
(Vinjamuri et al., 2015), Cramer–Rao bounds on Bayesian sparse signal
recovery using unquantized (Prasad and Murthy, 2013) and quantized obser-
vations (Thoota and Murthy, 2022), and theoretical analysis of Bayesian
sparse signal recovery, which provides a deep understanding of the superlative
performance of the approach (see Khanna and Murthy, 2017d, 2018a; Ramesh
et al., 2021 and references therein).
There are several interesting avenues that are worth exploring to further
develop the SBL framework. Adversarial learning of the SBL’s hyperpara-
meters is one such promising direction wherein adversarial learning principles
can be invoked to robustly maximize the SBL’s log-marginalized likelihood
objective. On the computational side, SBL is still not computationally fast
enough to handle very large signal dimensions encountered in big data appli-
cations. Any SBL-based inference suffers from slow convergence due to its
reliance on the Expectation-Maximization procedure for maximizing its
Type-II likelihood objective. Equipped with the new covariance-matching
interpretation of the SBL optimization developed in Sections 3.3 and 3.4, fast
scalable SBL solvers can be envisaged along the lines of the various efficient
methods developed for matrix divergence minimization. On the theoretical
front, it would be worthwhile to study the landscape of the SBL’s margina-
lized likelihood function for both single and multiple MMV scenarios and
precisely characterize the nature of its local maxima. These are promising
directions for future work.
References
Akaike, H., 1969. Fitting autoregressive models for prediction. Ann. Inst. Stat. Math. 21 (1),
243–247.
Al-Naffouri, T.Y., Bahai, A., Paulraj, A., 2002. Semi-blind channel identification and equalization
in OFDM: an expectation-maximization approach. In: Proceedings of the Vehicular Technol-
ogy Conference (VTC), vol. 1, pp. 13–17.
Al-Shoukairi, M., Schniter, P., Rao, B.D., 2018. A GAMP-based low complexity sparse Bayesian
learning algorithm. IEEE Trans. Signal Process. 66 (2), 294–308.
Andersen, M.R., Winther, O., Hansen, L.K., 2014. Bayesian inference for structured spike and
slab priors. Adv. Neural Inform. Process. Syst. 27, 1–9.
Anderson, B.D.O., Moore, J.B., 2005. Optimal Filtering. Courier Dover Publications.
Babacan, S.D., Molina, R., Katsaggelos, A.K., 2009. Bayesian compressive sensing using Laplace
priors. IEEE Trans. Image Process. 19 (1), 53–63.
Balatsoukas-Stimming, A., Studer, C., 2019. Deep unfolding for communications systems: a sur-
vey and some new directions. In: IEEE Int. Workshop on Signal Process. Systems (SiPS),
pp. 266–271.
Bandeira, A.S., Dobriban, E., Mixon, D.G., Sawin, W.F., 2013. Certifying the restricted isometry
property is hard. IEEE Trans. Inf. Theory 59 (6), 3448–3450.
Bishop, C.M., 2006. Pattern Recognition and Machine Learning. Springer New York.
Sparsity-aware Bayesian inference and its applications Chapter
8 203

Blumensath, T., Davies, M.E., 2009. Iterative hard thresholding for compressed sensing. Appl.
Comput. Harmon. Anal. 27 (3), 265–274.
Cai, T.T., Xu, G., Zhang, J., 2009. On recovery of sparse signals via ‘1 minimization. IEEE Trans.
Inf. Theory 55 (7), 3388–3397. https://doi.org/10.1109/TIT.2009.2021377.
Candes, E.J., 2008. The restricted isometry property and its implications for compressed sensing.
C. R. Math. 346 (9-10), 589–592.
Candes, E., Tao, T., 2007. The Dantzig selector: statistical estimation when p is much larger than
n. Ann. Stat. 35 (6), 2313–2351.
Candes, E.J., Wakin, M.B., Boyd, S.P., 2008. Enhancing sparsity by reweighted ‘1 minimization.
J. Fourier Anal. Applic. 14 (5), 877–905.
Cande`s, E.J., Li, X., Ma, Y., Wright, J., 2011. Robust principal component analysis? J. ACM
58 (3), 11:1–11:37.
Carvalho, C.M., Polson, N.G., Scott, J.G., 2009. Handling sparsity via the horseshoe. In: Artificial
Intelligence and Statistics, pp. 73–80.
Chartrand, R., Yin, W., 2008. Iteratively reweighted algorithms for compressive sensing.
In: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 3869–3872.
Chen, W., Zhang, R., 2004. Kalman-filter channel estimator for OFDM systems in time and
frequency-selective fading environment. In: IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), vol. 4, p. 377.
Chen, S.S., Donoho, D.L., Saunders, M.A., 2001. Atomic decomposition by basis pursuit. SIAM
Rev. 43 (1), 129–159.
Coleri, S., Ergen, M., Puri, A., Bahai, A., 2002. Channel estimation techniques based on pilot
arrangement in OFDM systems. IEEE Trans. Broadcast. 48 (3), 223–229.
Ding, Y., Chiu, S.-E., Rao, B.D., 2017. Sparse recovery with quantized multiple measurement
vectors. In: Proceedings of the Asilomar Conference on Signals, Systems, and Computers,
pp. 845–849.
Duarte, M.F., Sarvotham, S., Wakin, M.B., Baron, D., Baraniuk, R.G., 2005. Joint sparsity models
for distributed compressed sensing. In: IEEE Workshop on Signal Processing with Adaptative
Sparse Structured Representations (SPARS), April, pp. 1–5.
Efron, B., Hastie, T., Johnstone, I., Tibshirani, R., 2004. Least angle regression. Ann. Stat. 32 (2),
407–499.
Eldar, Y.C., Mishali, M., 2009. Robust recovery of signals from a structured union of subspaces.
IEEE Trans. Inf. Theory 55 (11), 5302–5316.
Ender, J.H.G., 2010. On compressive sensing applied to radar. Signal Process. 90 (5), 1402–1414.
Feder, M., Weinstein, E., 1988. Parameter estimation of superimposed signals using the EM algo-
rithm. IEEE Trans. Acoust. Speech Signal Process. 36 (4), 477–489.
Figueiredo, M.A., 2003. Adaptive sparseness for supervised learning. IEEE Trans. Pattern Anal.
Mach. Intell. 25 (9), 1150–1159.
Foucart, S., 2011. Hard thresholding pursuit: an algorithm for compressive sensing. SIAM
J. Numer. Anal. 49 (6), 2543–2563.
Foucart, S., Rauhut, H., 2013. A Mathematical Introduction to Compressive Sensing. Birkh€auser.
Gamper, U., Boesiger, P., Kozerke, S., 2008. Compressed sensing in dynamic MRI. Magn. Reson.
Med. 59 (2), 365–373.
George, E.I., McCulloch, R.E., 1993. Variable selection via Gibbs sampling. J. Am. Stat. Assoc.
88 (423), 881–889.
George, E.I., McCulloch, R.E., 1997. Approaches for Bayesian variable selection. Stat. Sin. 7 (2),
339–373.
204
Handbook of Statistics

Ghahramani, Z., Hinton, G.E., 1996. Parameter Estimation for Linear Dynamical Systems. CRG-
TR-96-2, University of Toronto, Dept. Computer Science.
Gil, M., Alajaji, F., Linder, T., 2013. Renyi divergence measures for commonly used univariate
continuous distributions. Inf. Sci. 249, 124–131.
Giri, R., Rao, B., 2016. Type I and type II Bayesian methods for sparse signal recovery using
scale mixtures. IEEE Trans. Signal Process. 64 (13), 3418–3428.
Gorodnitsky, I.F., Rao, B.D., 1997. Sparse signal reconstruction from limited data using FOCUSS:
a re-weighted minimum norm algorithm. IEEE Trans. Signal Process. 45 (3), 600–616.
Gorodnitsky, I.F., George, J.S., Rao, B.D., 1995. Neuromagnetic source imaging with FOCUSS:
a recursive weighted minimum norm algorithm. Electroencephalogr. Clin. Neurophysiol.
95 (4), 231–251.
Hunter, D.R., Lange, K., 2004. A tutorial on MM algorithms. Am. Stat. 58 (1), 30–37.
ITU-R Rec. M.1225, 1997. Guidelines for evaluation of radio transmission technologies (RTTs)
for IMT-2000. Rec. ITU-R M. 1225.
Ji, S., Xue, Y., Carin, L., 2008. Bayesian compressive sensing. IEEE Trans. Signal Process.
56 (6), 2346–2356.
Joseph, G., Murthy, C.R., 2017. A noniterative online Bayesian algorithm for the recovery of tem-
porally correlated sparse vectors. IEEE Trans. Signal Process. 65 (20), 5510–5525.
Joseph, G., Murthy, C.R., 2020. On the convergence of a Bayesian algorithm for joint dictionary
learning and sparse recovery. IEEE Trans. Signal Process. 68, 343–358.
Joseph, G., Murthy, C.R., Prasad, R., Rao, B.D., 2015. Online recovery of temporally correlated
sparse signals using multiple measurement vectors. In: Proceedings of the IEEE Global Com-
munications Conference, pp. 1–6.
Kannu, A.P., Schniter, P., 2011. On communication over unknown sparse frequency-selective
block-fading channels. IEEE Trans. Inf. Theory 57 (10), 6619–6632.
Khanna, S., Murthy, C.R., 2017a. Communication-efficient decentralized sparse Bayesian
learning of joint sparse signals. IEEE Trans. Signal Inf. Process. Netw. 3 (3), 617–630.
Khanna, S., Murthy, C.R., 2017b. Decentralized joint-sparse signal recovery: a sparse Bayesian
learning approach. IEEE Trans. Signal Inf. Process. Netw. 3 (1), 29–45.
Khanna, S., Murthy, C.R., 2017c. On the support recovery of jointly sparse gaussian sources
using sparse Bayesian learning. CoRR abs/1703.04930. http://arxiv.org/abs/1703.04930.
1703.04930.
Khanna, S., Murthy, C.R., 2017d. On the support recovery of jointly sparse Gaussian sources
using sparse Bayesian learning. arXiv:1212.4551.
Khanna, S., Murthy, C.R., 2017e. Renyi divergence based covariance matching pursuit of joint
sparse support. In: IEEE International Workshop on Signal Processing Advances in Wireless
Communications (SPAWC), July, pp. 1–5.
Khanna, S., Murthy, C.R., 2018a. On the restricted isometry of the columnwise Khatri-Rao prod-
uct. IEEE Trans. Signal Process. 66 (5), 1170–1183.
Khanna, S., Murthy, C.R., 2018b. Sparse recovery from multiple measurement vectors using
exponentiated gradient updates. IEEE Signal Process. Lett. 25 (10), 1485–1489.
Krishnamurthy, V., Moore, J.B., 1993. On-line estimation of hidden Markov model parameters based
on the Kullback-Leibler information measure. IEEE Trans. Signal Process. 41 (8), 2557–2573.
Ling, Q., Tian, Z., 2010. Decentralized sparse signal recovery for compressive sleeping wireless
sensor networks. IEEE Trans. Signal Process. 58 (7), 3816–3827.
Lu, W., Li, T., Atkinson, I.C., Vaswani, N., 2011. Modified-CS-residual for recursive reconstruc-
tion of highly undersampled functional MRI sequences. In: 2011 18th IEEE International
Conference on Image Processing, September, pp. 2689–2692.
Sparsity-aware Bayesian inference and its applications Chapter
8 205

Luenberger, D.G., Ye, Y., 2010. Linear and Nonlinear Programming. Springer, ISBN:
9783319188416. 3319188410.
Malek-Mohammadi, M., Jansson, M., Owrang, A., Koochakzadeh, A., Babaie-Zadeh, M., 2014.
DOA estimation in partially correlated noise using low-rank/sparse matrix decomposition.
In: 2014 IEEE 8th Sensor Array and Multichannel Signal Processing Workshop (SAM), June,
pp. 373–376.
Malioutov, D., Cetin, M., Willsky, A.S., 2005. A sparse signal reconstruction perspective for
source localization with sensor arrays. IEEE Trans. Signal Process. 53 (8), 3010–3022.
Mallat, S.G., Zhang, Z., 1993. Matching pursuits with time-frequency dictionaries. IEEE Trans.
Signal Process. 41 (12), 3397–3415.
Natarajan, B.K., 1995. Sparse approximate solutions to linear systems. SIAM J. Comput. 24 (2),
227–234.
Needell, D., Tropp, J.A., 2009. CoSaMP: iterative signal recovery from incomplete and inaccurate
samples. Appl. Comput. Harmon. Anal. 26 (3), 301–321.
Olshausen, B., Millman, K., 1999. Learning sparse codes with a mixture-of-Gaussians prior. In:
Solla, S., Leen, T., M€uller, K. (Eds.), Advances in Neural Information Processing Systems.
vol. 12. MIT Press.
Pal, P., Vaidyanathan, P.P., 2015a. Pushing the limits of sparse support recovery using correlation
information. IEEE Trans. Signal Process. 63 (3), 711–726.
Pal, P., Vaidyanathan, P.P., 2015b. Pushing the limits of sparse support recovery using correlation
information. IEEE Trans. Signal Process. 63 (3), 711–726.
Park, T., Casella, G., 2008. The bayesian lasso. J. Am. Stat. Assoc. 103 (482), 681–686.
Peter, R.J., Murthy, C.R., 2019. Learned-SBL: a deep learning architecture for sparse signal
recovery. arXiv:1909.08185.
Prasad, R., Murthy, C.R., 2013. Cramer-Rao-type bounds for sparse Bayesian learning. IEEE
Trans. Signal Process. 61 (3), 622–632.
Prasad, R., Murthy, C.R., Rao, B.D., 2014a. Joint approximately sparse channel estimation and
data detection in OFDM systems using sparse Bayesian learning. IEEE Trans. Signal Process.
62 (14), 3591–3603.
Prasad, R., Murthy, C.R., Rao, B.D., 2014b. Nested sparse Bayesian learning for block-sparse
signals with intra-block correlation. In: 2014 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), IEEE, pp. 7183–7187.
Prasad, R., Murthy, C.R., Rao, B.D., 2015. Joint channel estimation and data detection in MIMO-
OFDM systems: a sparse Bayesian learning approach. IEEE Trans. Signal Process. 63 (20),
5369–5382.
Qiu, C., Vaswani, N., 2011. ReProCS: a missing link between recursive robust PCA and recursive
sparse recovery in large but correlated noise. CoRR abs/1106.3286. http://arxiv.org/abs/1106.
3286.
Ramesh, L., Murthy, C.R., Tyagi, H., 2021. Sample-measurement tradeoff in support recovery
under a subgaussian prior. IEEE Trans. Inf. Theory 67 (12), 8140–8153.
Sharp, M., Scaglione, A., 2008. Application of sparse signal recovery to pilot-assisted channel
estimation. In: IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, pp. 3469–3472.
Sidiropoulos, N.D., Kyrillidis, A., 2012. Multi-way compressed sensing for sparse low-rank ten-
sors. IEEE Signal Process. Lett. 19 (11), 757–760.
Stojnic, M., Parvaresh, F., Hassibi, B., 2009. On the reconstruction of block-sparse signals with an
optimal number of measurements. IEEE Trans. Signal Process. 57 (8), 3075–3085.
Studer, C., Benkeser, C., Belfanti, S., Huang, Q., 2011. Design and implementation of a parallel
turbo-decoder ASIC for 3GPP-LTE. IEEE J. Solid-State Circuits 46 (1), 8–17.
206
Handbook of Statistics

Thoota, S.S., Murthy, C.R., 2022. Massive MIMO-OFDM systems with low resolution ADCs:
Cramer-Rao bound, sparse channel estimation, and soft symbol decoding. IEEE Trans. Signal
Process. (Early Access).
Tian, Y., ying Sun, X., di Qin, Y., 2014. DOA estimation in unknown colored noise using covariance
differencing and sparse signal recovery. J. China Univ. Posts Telecommun. 21 (3), 106–112.
Tibshirani, R., 1996. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. B Stat.
Methodol. 58 (1), 267–288.
Tipping, M., 1999. The relevance vector machine. Adv. Neural Inf. Process. Syst. 12, 652–658.
Tipping, M.E., 2001. Sparse Bayesian learning and the relevance vector machine. J. Mach. Learn.
Res. 1 (Jun), 211–244.
Tropp, J.A., Gilbert, A.C., 2007. Signal recovery from random measurements via orthogonal
matching pursuit. IEEE Trans. Inf. Theory 53 (12), 4655–4666.
Universal Mobile Telecommunications System (UMTS), 1997. Selection Procedures for the
Choice of Radio Transmission Technologies of the UMTS.
Van de Beek, J.J., Edfors, O., Sandell, M., Wilson, S.K., Borjesson, P.O., 1995. On channel esti-
mation in OFDM systems. In: Proceedings of the Vehicular Technology Conference (VTC),
vol. 2, pp. 815–819.
Van Dyk, D.A., 2000. Nesting EM algorithms for computational efficiency. Stat. Sin. 10 (1),
203–226.
Vinjamuri, V., Prasad, R., Murthy, C.R., 2015. Sparse signal recovery in the presence of colored
noise and rank-deficient noise covariance matrix: an SBL approach. In: IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3761–3765.
Wei, W., 1994. Time series analysis. Addison-Wesley Redwood City, California.
Wipf, D.P., Rao, B.D., 2004. Sparse Bayesian learning for basis selection. IEEE Trans. Signal
Process. 52 (8), 2153–2164.
Wipf, D.P., Rao, B.D., 2007. An empirical Bayesian strategy for solving the simultaneous sparse
approximation problem. IEEE Trans. Signal Process. 55 (7), 3704–3716.
Wipf, D., Owen, J., Attias, H., Sekihara, K., Nagarajan, S., 2010. Robust Bayesian estimation of
the location, orientation, and time course of multiple correlated neural sources using MEG.
NeuroImage 49 (1), 641–655.
Wright, J., Ganesh, A., Rao, S., Peng, Y., Ma, Y., 2009. Robust principal component analysis:
exact recovery of corrupted low-rank matrices via convex optimization. In: Advances in
Neural Information Processing Systems 22, Curran Associates, Inc, pp. 2080–2088.
Zhang, Q., Kassam, S.A., 1999. Finite-state Markov model for Rayleigh fading channels. IEEE
Trans. Commun. 47 (11), 1688–1692.
Zhang, Z., Rao, B.D., 2012. Recovery of block sparse signals using the framework of block sparse
Bayesian learning. In: IEEE International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), IEEE, pp. 3345–3348.
Zhang, Z., Rao, B., 2013. Extension of SBL algorithms for the recovery of block sparse signals
with intra-block correlation. IEEE Trans. Signal Process. 61 (8), 2009–2015.
Zhang, Z., Jung, T.-P., Makeig, S., Pi, Z., Rao, B.D., 2014. Spatiotemporal sparse Bayesian
learning with applications to compressed sensing of multichannel physiological signals. IEEE
Trans. Neural Syst. Rehabil. Eng. 22 (6), 1186–1197.
Zheng, Y.R., Xiao, C., 2003. Simulation models with correct statistical properties for Rayleigh
fading channels. IEEE Trans. Commun. 51 (6), 920–928.
Zyren, J., McCoy, W., 2007. Overview of the 3GPP Long Term Evolution Physical Layer. Free-
scale Semiconductor, Inc, (white paper).
Sparsity-aware Bayesian inference and its applications Chapter
8 207

This page intentionally left blank

Chapter 9
Mathematical theory of
Bayesian statistics where all
models are wrong
Sumio Watanabe∗
Department of Mathematical and Computing Science, Tokyo Institute of Technology, Meguro-ku,
Tokyo, Japan
∗Corresponding author: e-mail: swatanab@c.titech.ac.jp
Abstract
Nowadays, we know all statistical models are wrong, both subjectively and objectively.
In the 20th century, it was assumed that we are able to capture “uncertainty” completely
by a statistical model and “belief” by a prior distribution. However, now we know that
an arbitrary pair of a statistical model and a prior distribution is only a candidate sys-
tem, which is different from an unknown data-generating process in a large world.
In this article, in order to study Bayesian statistics where all models are too simple
or too redundant, we construct mathematical foundation on which universal theorems
hold for an arbitrary triple made of a data-generating process, a statistical model, and
a prior distribution. Since mathematical theory enables us to estimate the generalization
losses and the free energies in both cases when the posterior distributions are regular
and singular, we can apply them to checking and improving a statistical model and a
prior distribution, in both scientific research and personal decision-making.
Keywords: Generalization loss, Free energy, Model evaluation, Prior evaluation,
Cross validation, Information criterion
1
Introduction
In statistics and machine learning, an unknown data-generating process (DGP)
is estimated by a predictive distribution using a statistical model and a prior
distribution. In an older Bayesian framework of the 20th century, the Bayes-
ian predictive distribution was explained to be optimal on the assumption that
a person or an agent has abilities
(A) to capture “uncertainty” by a statistical model,
(B) to represent “belief” by a prior distribution.
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.06.001
Copyright © 2022 Elsevier B.V. All rights reserved.
209

Needless to say, it is important to devise candidate pairs of statistical models
and prior distributions, based on knowledges about an unknown phenomenon.
However, nowadays, we are aware that all models are wrong (Box, 1976),
both subjectively and objectively. A statistical model and a prior may be vir-
tual and fictional; hence, we need a modern framework of Bayesian statistics
where a statistical model and a prior distribution are different from DGP
(Akaike, 1974, 1980; Gelman et al., 2013; McElreath, 2020; Watanabe,
2021a). For example, the set of all probability distributions on the Euclidean
space has the infinite dimension, resulting that we cannot make a statistical
model completely equal to an unknown probability distribution. From the
decision-theoretic point of view, “uncertainty” cannot be completely repre-
sented by statistical models even if they are prepared as carefully as possible
(Binmore, 2017). A prepared model is redundant and overparametrized when
a sample size is small; however, it becomes too simple as the sample size
increases; hence, the optimal model depends on the sample size (Akaike,
1980; Gelman and Shalizi, 2013; Watanabe, 2001).
In this article, in order to establish a mathematical foundation of modern
Bayesian statistics where all models are wrong, we derive universal theorems
which hold even if a statistical model and a prior distribution are too simple or
too redundant for the unknown DGP. The purpose of this article is to show
that basic theorems hold for an arbitrary triple,
fdata  generating process, statistical model, prior distributiong,
even where we cannot do either (A) or (B). Therefore, this article may be use-
ful for readers who are interested in the following situations.
1. In the real world, all models are wrong. If a model is wrong, coherent
inference takes us to wrong prediction.
2. If we reject the existence of an DGP, and if we assign a model and a prior
to an unknown phenomenon, we replace nonexisting DGP by a model and
a prior.
3. If we use our own model and prior as DGP, our decision is neither rational
nor scientific, because we know that our model is different from DGP.
4. Even for a personal decision-making, we want to check and improve both
a model and a prior, since we know that wrong model and prior make an
utility function far from optimal.
5. Based on universal mathematical theorems, several useful models and
priors may be found according to the given sample size.
In Section 2, we show that several universal theorems hold, even when the
posterior distribution is regular or singular. If a posterior distribution can be
approximated by some normal distribution, a DGP is called regular for a sta-
tistical model, or if otherwise, it is called singular. Many statistical models
and learning machines, which have hierarchical structures or latent variables,
contain singularities in their parameter spaces; hence unknown DGP may be
210
Handbook of Statistics

regular or singular for a statistical model and a learning machine. The univer-
sal theorems hold for both cases; hence we can employ them when a data-
generating function is unknown. Moreover, it is shown that Bayesian infer-
ence makes the generalization loss smaller than the maximum likelihood
method if an unknown DGP is singular for a statistical model.
In Section 3, we introduce three applications of mathematical theorems to
statistics and machine learning. First, a model evaluation problem is studied,
where information criteria, cross-validation losses, and a marginal likelihood
are compared as measures of statistical models. Second, a prior evaluation
problem is examined. The above model evaluation measures are compared as
functionals of a prior distribution. And lastly, generalization of theorems onto
not i.i.d. cases is introduced. Recently, these theorems are being employed in
several Bayesian statistics successfully (Antonia Amaral Turkman et al.,
2019; Congdon, 2019; Hobbs et al., 2015; Korner-Nievergelt et al., 2015;
Lambert, 2018; Martin, 2016; Reich and Ghosh, 2019; Wang et al., 2018),
and it is expected to be studied more extensively in the future.
2
Mathematical theory of Bayesian statistics
2.1
DGP, model, and prior
Let N be an N-dimensional real Euclidean space and X be a N-valued ran-
dom variable whose probability density function is given by an unknown
probability distribution q(x). Let
Xn ¼ fX1, X2, …, Xng
be a set of independent random variables whose probability distributions are
same as X. In this section we mainly study the i.i.d. case. For the not i.i.d.
cases, see Section 3.3. The probability density function q(x) is sometimes
referred to as a DGP or an unknown true distribution. That is to say
Xn 
Y
n
i¼1
qðxiÞ:
(1)
We may assume that there exists an unknown functional prior distribution
QðqÞ, from which q(x) is generated,
qðxÞ  QðqÞ
(2)
which is equivalent to the condition that Xn is exchangeable. For such cases,
see Section 3.3.
In order to estimate the DGP q(x), a statistical model p(xjw) is defined by a
conditional probability density of x for a given parameter w  W  d, and a
prior distribution φ(w) is defined by a probability density function on W.
It should be emphasized that both a statistical model and a prior distribution
are made by a person or an agent who knows neither q(x) nor QðqÞ.
Mathematical theory of Bayesian statistics Chapter
9 211

Even if they are determined by experts or scientists, both p(xjw) and φ(w) are
virtual and fictional candidates that may be different from DGP. In fact, true
experts and scientists know that their modeling is not perfect and want to
check or improve them.
Note. In a Bayesian framework of the 20th century, it was assumed that a
person or an agent had an ability to capture “an uncertainty” by a statistical
model p(xjw) and to represent “a subjective belief” by a prior distribution
φ(w). Based on such an assumptions, the wrong assignment was chosen
QðqÞ ( φðwÞ,
(3)
Y
n
i¼1
qðxiÞ (
Y
n
i¼1
pðxijwÞ:
(4)
If replacement Eqs. (3) and (4) were adopted, then Bayesian inference would
be optimal. However, we know an arbitrary model is not completely equal to
DGP. If a person or an agent rejects the existence of unknown QðqÞ or q(x)
and accepts Eqs. (3) and (4), then the virtual and fictional assumptions are
substituted for nonexisting DGP, which is contradiction. Nowadays, we know
all models are wrong both subjectively and objectively; hence we have to dis-
tinguish an unknown DGP from the unreal models and priors. One of the most
important procedures in statistics and machine learning is to find several use-
ful models and priors that relatively make the difference between DGP and a
predictive distribution smaller.
For a given candidate pair of a statical model p(xjw) and a prior distribu-
tion φ(w), the posterior distribution on W is defined by
pðwjXnÞ ¼ 1
Zn
φðwÞ
Y
n
i¼1
pðXijwÞ,
(5)
where Zn is a normalizing constant
Zn ≡pðXnÞ ≡
Z
φðwÞ
Y
n
i¼1
pðXijwÞdw,
(6)
which is sometimes called the marginal likelihood (Good, 1952). Since the
statistical model p(xjw) and a prior distribution φ(w) are not GDP, the poste-
rior distribution is also different from GDP of Xn.
The expectation value and the variance by the posterior distribution are
denoted by w½  and w½ , respectively. For a given function H(w), they
are defined by
w½HðwÞ ¼
Z
HðwÞpðwjXnÞdw,
(7)
w½HðwÞ ¼ w½HðwÞ2  w½HðwÞ2:
(8)
212
Handbook of Statistics

Both w½HðwÞ and w½HðwÞ are random variables, because they are func-
tions of a random variable Xn. Then the Bayesian predictive distribution is
defined by
pðxjXnÞ¼ w½pðxjwÞ ¼
Z
pðxjwÞpðwjXnÞdw,
which estimates the unknown DGP q(x). The marginal likelihood p(xn) esti-
mates the unknown simultaneous DGP,
qðxnÞ ¼
Y
n
i¼1
qðxiÞ:
Since a statistical model and a prior distribution are different from DGP, nei-
ther the predictive distribution nor the marginal likelihood is optimal estima-
tion of q(x) or q(Xn). Hence we need mathematical theory by which the
differences of q(x) and p(xjXn) and q(Xn) and p(Xn) are estimated.
2.2
Generalization loss and free energy
In order to study the appropriateness of a statistical model and a prior distri-
bution for an unknown DGP, the generalization loss Gn and the training loss
Tn are defined by
Gn ¼ 
Z
qðxÞ log pðxjXnÞdx,
(9)
Tn ¼ 1
n
X
n
i¼1
logpðXijXnÞ:
(10)
The entropy S of DGP and Kullback–Leibler divergence D(q(x)jjp(x)) are
respectively defined by
S ¼ 
Z
qðxÞ log qðxÞdx,
(11)
DðqðxÞjjpðxÞÞ ¼
Z
qðxÞ log qðxÞ
pðxÞ dx:
(12)
Then it follows that
Gn ¼ S + DðqðxÞjjpðxjXnÞÞ:
Hence the generalization loss is smaller if and only if Kullback–Leibler diver-
gence between q(x) and p(xjXn) is smaller. By the generalization loss, the differ-
ence between DGP and the predictive distribution can be measured; however, it
contains the information about unknown DGP in the definition. The training
loss can be calculated by using data, a model, and a prior; however, it is not
equal to the generalization loss.
Mathematical theory of Bayesian statistics Chapter
9 213

We prepare two random variables, a leave-one-out cross validation loss
(LOO) (Gelfand et al., 1992; Vehtari and Lampinen, 2002; Vehtari et al.,
2017) and a widely applicable information criterion (WAIC) (Watanabe,
2009), respectively,
Cn ¼ 1
n
X
n
i¼1
log pðXijXnnXiÞ,
(13)
Wn ¼ Tn + 1
n
X
n
i¼1
w½log pðXijwÞ:
(14)
Both random variables can be calculated by using data, a model, and a prior,
without information about DGP. In the following sections, we show their
behaviors as estimators of the generalization loss.
The free energy, which is the minus log marginal likelihood, is defined by
Fn ¼ log pðXnÞ ¼ log
Z
φðwÞ
Y
n
i¼1
pðXijwÞdw:
(15)
Then it follows that
½Fn ¼ nS + DðqðxnÞjjpðxnÞÞ:
In other words, ½Fn is smaller if and only if the Kullback–Leibler divergence
between q(xn) and p(xn) is smaller. The free energy can be calculated by data, a
model, and a prior; however, it is not easy to perform the integration over the
parameter set in Eq. (15). In fact, even if the posterior distribution can be
approximated by one Markov chain Monte Carlo (MCMC) process, the integra-
tion in (15) cannot be obtained by one MCMC process. Hence several mathe-
matical approximation methods are often applied to numerical calculation.
There is a formal relation between the free energy and the generalization
loss,
Fn ¼ log
pðX1Þ
Y
n1
j¼1
pðXj + 1jXjÞ
 
!
(16)
¼ log pðX1Þ
X
n1
j¼1
log pðXj + 1jXjÞ:
(17)
Consequently, for an arbitrary positive integer n,
½Gn ¼ ½Fn+1  ½Fn
holds, which shows that the free energy and the generalization loss have a
tight relation to each other; however, minimization of Gn is not equivalent
to minimization of Fn. That is to say, the accuracy of prediction is different
from that of the marginal estimation.
214
Handbook of Statistics

Note. Two values Gn and Fn can be understood as functional of a pair
p(xjw) and φ(w), that is to say, Gn ¼ Gn(p, φ) and Fn ¼ Fn(p, φ). Then, for
a given QðqÞ, both
Z
QðqÞdq
Z
qðxnÞdxn Gnðp, φÞ,
(18)
Z
QðqÞdq
Z
qðxnÞdxn Fnðp, φÞ
(19)
are simultaneously minimized when QðqÞ ¼ φðwÞ and q(x) ¼ p(xjw). That is
to say, the minimal values are attained by the same pair (p, φ), if we know
DGP and if (p, φ) can be chosen as the unknown DGP. However, in a large
world, we do not know QðqÞ; hence the pair (p, φ) is chosen from the set
of restricted candidates, resulting that the optimal pairs for Gn and Fn are dif-
ferent from each other. The generalization loss and the free energy are differ-
ent criteria for evaluating a statistical model or a prior distribution in a large
world.
The average and empirical minus log likelihood functions L(w) and Ln(w)
are respectively defined by
LðwÞ ¼ 
Z
qðxÞ log pðxjwÞdx,
(20)
LnðwÞ ¼ 1
n
X
n
i¼1
log pðXijwÞ:
(21)
For simplicity, we assume that L(w) is an analytic function of w and there
exists w0  W  d
that minimizes L(w). Note that, in general, q(x) 6¼
p(xjw0) and w0 is not unique. The set of all parameters that makes L(w)
minimal is denoted by
W0 ¼ fw  W ; LðwÞ  Lðw0Þ ¼ 0g:
(22)
Also we assume that p(xjw0) does not depend on w0  W0. For the case when
this assumption is not satisfied, see Watanabe (2010). The condition that L(w)
is analytic is not necessary in general. For example, the same theory can be
derived if L(w) is a piecewise analytic function. When the sample size n tends
to infinity, the posterior distribution outside of the neighborhoods of W0 con-
verges to zero, resulting that
fw  W ; pðwjXnÞ > 0g  fw  W ; LðwÞ  Lðw0Þ  0g:
Hence, in order to construct mathematical theory of Bayesian statistics, we
need to study several geometric properties of the set W0, which is the set of
all zero points of an analytic function L(w)  L(w0).
Mathematical theory of Bayesian statistics Chapter
9 215

Definition. If there exists w0  W0 such that q(x) ¼ p(xjw0), then q(x) is
said to be realizable by a statistical model p(xjw). If the set W0 consists of a
single element w0, and Hessian matrix J ¼ (Jij) of L(w) at w0
Jij ¼
∂2
∂wi∂wj Lðw0Þ,
is positive definite, then q(x) is said to be regular for a statistical model
p(xjw). If otherwise, q(x) is called singular for a statistical model.
We introduce a log density ratio function defined by
fðx, wÞ ¼ log
pðxjw0Þ
pðxjwÞ


:
(23)
Then f(x, w) ¼ 0 if w  W0, and a statistical model can be rewritten as
pðxjwÞ ¼ pðxjw0Þ exp ðfðx, wÞÞ:
We define two functions,
KðwÞ ¼ ½ fðX, wÞ ¼ LðwÞ  Lðw0Þ,
(24)
KnðwÞ ¼ 1
n
X
n
i¼1
fðXi,wÞ ¼ LnðwÞLnðw0Þ:
(25)
The following integration is necessary in a study of Bayesian theory,
ΩðwÞdw ≡φðwÞ
Y
n
i¼1
pðXijwÞdw,
(26)
which can be rewritten as
ΩðwÞdw
exp ðnLnðw0ÞÞ ¼ exp ðnKnðwÞÞφðwÞdw:
(27)
The probabilistic behavior of this integration strongly depends on the condi-
tion whether the posterior distribution is regular or singular. Therefore let us
explain both cases in the following Sections 2.3 and 2.4, respectively.
In order to study the posterior distribution, functional cumulant generating
functions for α  are introduced using the posterior average w½  in Eq. (7),
GnðαÞ ¼
Z
log w½pðxjwÞαqðxÞdx,
(28)
T nðαÞ ¼ 1
n
X
n
i¼1
logw½pðXijwÞα:
(29)
Then it follows that Gnð0Þ ¼ T nð0Þ ¼ 0, and
216
Handbook of Statistics

Gn ¼ Gnð1Þ,
(30)
Tn ¼ T nð1Þ,
(31)
Cn ¼ T nð1Þ,
(32)
Wn ¼ T nð1Þ + T
00
nð0Þ:
(33)
Moreover, by the definition,
G0
nð0Þ ¼
Z
w½ log pðXjwÞqðxÞdx,
(34)
G00
nð0Þ ¼
Z
w½ log pðXjwÞqðxÞdx,
(35)
T 0
nð0Þ ¼ 1
n
X
n
i¼1
w½logpðXijwÞ,
(36)
T
00
n ð0Þ ¼ 1
n
X
n
i¼1
w½logpðXijwÞ:
(37)
We can show that, for k  2, GðkÞ
n ðαÞ ¼ Opð1=nk=2Þ and T ðkÞ
n
ðαÞ ¼ Opð1=nk=2Þ
in both regular and singular cases, hence
Gn ¼ G0
nð0Þ  1
2 G00
nð0Þ + Opð1=n3=2Þ,
(38)
Tn ¼ T 0
nð0Þ  1
2 T 00
nð0Þ + Opð1=n3=2Þ,
(39)
Cn ¼ T 0
nð0Þ + 1
2 T 00
nð0Þ + Opð1=n3=2Þ,
(40)
Wn ¼ T 0
nð0Þ + 1
2 T 00
nð0Þ + Opð1=n3=2Þ:
(41)
These equations show that, if probabilistic behaviors of Eqs. (34)–(37) are
derived, then the relations among the generalization loss, the training loss,
the leave-one-out cross validation, and the information criterion are clarified.
In the following sections, we show their behaviors in both regular and singular
cases, which reveal the unified mathematical structure of Bayesian statistics.
2.3
Regular theory
For a regular case, we define Fisher information matrix I ¼ (Iij) at w0 by
Iij ¼
Z ∂
∂wi log pðxjw0Þ ∂
∂wj log pðxjw0Þ pðxjw0Þ dx:
If the data generating process q(x) is realizable by a statistical model, in other
words, q(x) ¼ p(xjw0), then I ¼ J, whereas, if otherwise, I 6¼ J in general.
Mathematical theory of Bayesian statistics Chapter
9 217

In this section, we study the case when a statistical model p(xjw) is regular
for DGP q(x) and 0 < φ(w0) < ∞. In such a case, the posterior distribution
can be approximated by a normal distribution in the neighborhood of w0;
hence the asymptotic behaviors of the generalization loss and the free energy
can be derived. A random variable ξn  d is defined by
ξn ¼ J1=2
1ﬃﬃﬃn
p
X
n
i¼1
∂
∂w ð log pðXijw0ÞÞ
 
!
:
Then it is proved that ξn converges to N ð0, J1=2IJ1=2Þ in distribution, where
N(a, Σ) is the normal distribution with average a and the covariance matrix Σ.
The average of kξnk2 satisfies
½kξnk2 ¼ trðIJ1Þ:
Then, the posterior distribution in kw  w0k > n1/2+ε (0 < ε < 1/2) con-
verges to zero faster than that in kw  w0k  n1/2+ε. By using a transform
of a parameter
u ¼
ﬃﬃﬃn
p J1=2ðw  w0Þ + ξn,
the following asymptotic behavior can be derived in the neighborhood of kuk
 nε (0 < ε < 1/2),
nKnðwÞ ¼ n
2 ðw  w0ÞTJðw  w0Þ 
ﬃﬃﬃn
p ðw  w0ÞTJ1=2ξn + opð1Þ
¼ kuk2 kξnk2
2
+ opð1Þ,
(42)
where (w  w0)T shows the transposed vector of w  w0. Since
du ¼ nd=2ðdet JÞ1=2dw,
the integration Eq. (26) is given by
ΩðwÞdw
exp ðnLnðw0ÞÞ 
φðw0Þ
nd=2ðdet JÞ1=2 exp
 kuk2 + kξnk2
2


du:
(43)
As a consequence, the free energy and its expectation are given by
Fn ¼ log
Z
ΩðwÞdw
¼ nLnðw0Þ + d
2 logn + 1
2 log det J
d
2 logð2πÞ1
2 kξnk2  logφðw0Þ + opð1Þ,
(44)
½Fn ¼ nLðw0Þ + d
2 logn + 1
2 logdetJ
d
2 logð2πÞ1
2trðIJ1Þ logφðw0Þ + oð1Þ:
(45)
218
Handbook of Statistics

Next, we study the performance of the generalization loss. The expectation
value of a function H(w) by the posterior distribution is given by using
Ω(w)dw,
w½HðwÞ ¼
Z
HðwÞΩðwÞdw
Z
ΩðwÞdw
:
(46)
By applying this definition to Eqs. (34)–(37), it follows that
G0
nð0Þ¼ Lðw0Þ + d
2n + 1
2n kξnk2 + Opð1=n3=2Þ,
(47)
G
00
nð0Þ¼ 1
ntrðIJ1Þ + Opð1=n3=2Þ,
(48)
T 0
nð0Þ¼ Lnðw0Þ + d
2n 1
2n kξnk2 + Opð1=n3=2Þ,
(49)
T
00
n ð0Þ¼ 1
ntrðIJ1Þ + Opð1=n3=2Þ:
(50)
The relations among the generalization loss, the training losses, the leave-one-
out cross validation, and the information criterion are given by
Gn ¼ Lðw0Þ + 1
2n d + kξnk2 trðIJ1Þ


+ Op
1
n3=2


,
(51)
Tn ¼ Lnðw0Þ + 1
2n d kξnk2 trðIJ1Þ


+ Op
1
n3=2


,
(52)
Cn ¼ Lnðw0Þ + 1
2n d kξnk2 + trðIJ1Þ


+ Op
1
n3=2


,
(53)
Wn ¼ Lnðw0Þ + 1
2n d kξnk2 + trðIJ1Þ


+ Op
1
n3=2


:
(54)
Moreover, their expectation values are given by
½Gn ¼ Lðw0Þ + d
2n + O
1
n2


,
(55)
½Tn ¼ Lðw0Þ + 1
n
d
2trðIJ1Þ


+ O
1
n2


,
(56)
½Cn ¼ Lðw0Þ + d
2n + O
1
n2


,
(57)
½Wn ¼ Lðw0Þ + d
2n + O
1
n2


:
(58)
Mathematical theory of Bayesian statistics Chapter
9 219

Note that the prior distribution φ(w) does not affect the terms Op(1/n). For the
effect of the prior to the higher order terms, see Section 3.2. The expectation
values of the generalization loss, the leave-one-out cross validation, and the
information criterion are equal to each other; however, they have inverse
correlations
ðGn Lðw0ÞÞ + ðCn Lnðw0ÞÞ ¼ d
n + op
1
n
 
,
(59)
ðGn Lðw0ÞÞ + ðWn Lnðw0ÞÞ ¼ d
n + op
1
n
 
:
(60)
These properties are not trivial from the definition of the cross validation and
the information criterion, and show that a careful treatment of cross validation
and information criterion is necessary (Watanabe, 2022) in the real world.
2.4
Singular theory
If a statistical model is singular for a DGP, the posterior distribution cannot be
approximated by any normal distribution; hence the regular theory does not
hold. Even if a statistical model is regular for a DGP, the minimum eigenvalue
of the Hessian matrix J(w0) is smaller than the constant order, then singular the-
ory is necessary. In order to study statistical models and learning machines,
which have hierarchical structures or latent variables, singular theory plays an
important role. For example, neural networks, normal mixtures, matrix factori-
zations, latent Dirichlet allocations, and Boltzmann machines are typical mod-
els.
In
such
statistical
models
and
learning
machines,
the
posterior
distributions are in overparametrizable states in general.
In singular cases, the posterior distribution does not concentrate on a sin-
gle parameter, but spreads over the union of neighborhoods among the set
W0 ¼ fw  d ; KðwÞ ¼ LðwÞ  Lðw0Þ ¼ 0g:
By the definition, Kn(w) ¼ Ln(w)  Ln(w0) ¼ 0 if w  W0. However, it is
difficult to derive the behavior of the posterior distribution because W0 is an
analytic set or an algebraic variety, which contains complex singularities in
general.
A mathematical foundation on which any singular posterior distribution can
be studied is Hironaka resolution theorem (Atiyah, 1970; Hironaka, 1964) in
algebraic geometry. For an arbitrary analytic function K(w) (w  d) and for
an arbitrary neighborhood of K(w) ¼ 0, there exists a d-dimensional real ana-
lytic manifold M and a proper analytic function w ¼ g(u), such that, in each
local coordinate of M,
KðgðuÞÞ ¼ u2k ≡u2k1
1 u2k2
2 ⋯u2kd
d ,
(61)
φðgðuÞÞjg0ðuÞj ¼ bðuÞjuhj ≡bðuÞjuh1
1 uh2
2 ⋯uhd
d j,
(62)
220
Handbook of Statistics

hold, where k ¼ ðk1, k2, …, kdÞ and h ¼ ðh1, h2, …, hdÞ are sets of nonnegative
integers, b(u) > 0 is a positive function, and jg0(u)j is the absolute value of the
Jacobian determinant of w ¼ g(u). Note that at least one kj is a positive inte-
ger. Here a function w ¼ g(u) is called proper if g1(S) is compact for an
arbitrary compact set S  d. Since an integrated value by the posterior distri-
bution can be represented by the sum of them in local sets of parameters, this
theorem enables us to analyze the singular posterior distribution in each local
coordinate. By using Hironaka theorem, it is proved that there exists a func-
tion a(x, u) such that
KðgðuÞÞ ¼ u2k,
(63)
fðx, gðuÞÞ ¼ aðx, uÞuk,
(64)
and ½aðX, uÞ2 < ∞, where f(x, w) is defined in Eq. (23). Then ½aðX, uÞ ¼ uk
holds. By using this local coordinate u, nKn(w) can be rewritten as
nKnðgðuÞÞ ¼ nu2k 
ﬃﬃﬃn
p ukξnðuÞ,
where
ξnðuÞ ¼ 1ﬃﬃﬃn
p
X
n
i¼1
faðXi,uÞukg
is an empirical process which converges to a Gaussian process in distribution
ξn(u) ! ξ(u). The Gaussian process ξ(u) has the same expectation and vari-
ance as ξn(u),
½ξnðuÞ ¼ ½ξðuÞ ¼ 0,
(65)
½ξnðuÞξnðu0Þ ¼ ½ξðuÞξðu0Þ:
(66)
The singular posterior distribution can be written as
ΩðwÞdw
exp ðnLnðw0ÞÞ
¼ exp ðnKnðwÞÞφðwÞdw
¼ exp ðnu2k +
ﬃﬃﬃn
p ukξnðuÞÞbðuÞjuhj du
¼
Z
dt δðt  nu2kÞ exp ðt +
ﬃﬃ
t
p
ξnðuÞÞbðuÞjuhj du,
(67)
where δ(t  nu2k) is a Schwartz distribution. Its asymptotic behavior as n ! ∞
can be derived by the following way.
2.4.1
Real log canonical threshold (RLCT)
Using the resolution theorem Eqs. (61) and (62), the real log canonical
threshold λ and multiplicity m are defined by
Mathematical theory of Bayesian statistics Chapter
9 221

λ ¼ min
L:C: min
1jd
hj + 1
2kj


,
(68)
m ¼ max
L:C: # j; hj + 1
2kj
¼ λ


,
(69)
where minL.C. shows the minimum value over all local coordinates. Note that
we define (hj + 1)/(2kj) ¼ ∞for kj ¼ 0, and that # means the number of ele-
ments in a set.
2.4.2
Geometrical property of RLCT
In the followings, we show that RLCT determines the generalization perfor-
mance of Bayesian prediction. Here we explain two important properties of
RLCT. Let K(w) be the function defined by Eq. (24). First, the zeta function
defined by a model and a prior
ζðzÞ ¼
Z
KðwÞzφðwÞdw
ðz  Þ
(70)
is an analytic function of a complex variable of z in RðzÞ > 0, which can be
analytically continued to the unique meromorphic function on the entire com-
plex plane. Its poles are all real and negative values. The largest pole is equal
to (λ) and its order is m. That is to say, RLCT shows the algebraic property
of the zeta function. Second, RLCT is equal to the volume dimension of the
neighborhood of the almost optimal parameters
λ ¼ lim
ε!+0
1
log ε log
Z
KðwÞ<ε
φðwÞdw
 
!
:
(71)
It should be emphasized that the accuracy of the Bayesian prediction is deter-
mined by the volume defined by the likelihood function and a prior distribu-
tion. RLCT is an important birational invariant in algebraic geometry. If a
statistical model is regular for DGP, then λ ¼ d/2 and m ¼ 1. Hence the regular
theory is contained in the singular theory as a very special part. If there exists
w0  W0 such that the Hessian matrix at w0 is singular and φ(w0) > 0, then
λ < d/2.
Without loss of generality, we can assume that
u ¼ ðua, ubÞ  m 	 dm,
which satisfies
hj + 1
2kj


¼ λ
ð1  j  mÞ,
hj + 1
2kj


> λ
ðm + 1  j  dÞ:
222
Handbook of Statistics

A multiindex μ  dm is defined by
μ ¼ fμj ¼ 2λkj + hj ; m + 1  j  dg  dm:
Then the following asymptotic expansion holds as t ! +0 (Watanabe, 2009),
δðt  u2kÞjuhjbðuÞdu ¼ tλ1ð log tÞm1du* + oðtλ1ð log tÞm1Þ,
(72)
where
du* ¼ δðuaÞðubÞμ bðuÞ
ðm1Þ!2m Qm
j¼1 kj
du:
By applying Eq. (72) to Eq. (67), we obtain
ΩðwÞdw
exp ðnLnðw0ÞÞ ¼
Z dt
n δðt=n  u2kÞ exp ðt +
ﬃﬃ
t
p
ξnðuÞÞbðuÞjuhj du
 ð log nÞm1
nλ
Z
dt tλ1 exp ðt +
ﬃﬃ
t
p
ξnðuÞÞ du*:
(73)
Note that the support of the measure du* is contained in a set g1(W0). It fol-
lows that
Fn ¼ nLnðw0Þ + λlognðm1Þlog logn + ΘðξnÞ + opð1Þ,
(74)
½Fn ¼ nLnðw0Þ + λlognðm1Þlog logn + ½ΘðξÞ + oð1Þ,
(75)
where the random variable Θ(ξn) is defined by
ΘðξnÞ ¼  log
X
L:C:
Z
dt tλ1 exp ðt +
ﬃﬃ
t
p
ξnðuÞÞdu*
 
!
,
which converges in distribution to Θ(ξ). We define the average by a renorma-
lized posterior distribution of a given function F(t, u) by
hFðt, uÞi ¼
X
L:C:
Z
du* dt Fðt, uÞ tλ1 exp ðt +
ﬃﬃ
t
p
ξnðuÞÞ
X
L:C:
Z
du* dt tλ1 exp ðt +
ﬃﬃ
t
p
ξnðuÞÞ
:
By using a partial integration, it follows that
hti ¼ λ + 1
2 h
ﬃﬃ
t
p
ξnðuÞi:
(76)
A random variable V (ξn) is defined by
VðξnÞ ¼ X½htaðX, uÞ2i  h
ﬃﬃ
t
p
aðX, uÞi
2:
Mathematical theory of Bayesian statistics Chapter
9 223

Then by using the partial integration over the functional space ξ(u), it
follows that
½h
ﬃﬃ
t
p
ξðuÞ ¼ ½VðξÞ:
Based on the above definitions, the following equations are derived:
Gn ¼ nLðw0Þ + 1
n
λ + 1
2h
ﬃﬃ
t
p
ξnðuÞi1
2VðξnÞ


+ op
1
n
 
,
(77)
Tn ¼ nLnðw0Þ + 1
n
λ1
2h
ﬃﬃ
t
p
ξnðuÞi1
2VðξnÞ


+ op
1
n
 
,
(78)
Cn ¼ nLnðw0Þ + 1
n
λ1
2h
ﬃﬃ
t
p
ξnðuÞi + 1
2VðξnÞ


+ op
1
n
 
,
(79)
Wn ¼ nLnðw0Þ + 1
n
λ1
2h
ﬃﬃ
t
p
ξnðuÞi + 1
2VðξnÞ


+ op
1
n
 
:
(80)
Let ν ¼ ½VðξÞ=2 be the singular fluctuation. It follows that
½Gn ¼ nLðw0Þ + λ
n + o 1
n
 
,
(81)
½Tn ¼ nLðw0Þ + λ2ν
n
+ o 1
n
 
,
(82)
½Cn ¼ nLðw0Þ + λ
n + o 1
n
 
,
(83)
½Wn ¼ nLðw0Þ + λ
n + o 1
n
 
:
(84)
Also in any singular case, the generalization loss, the leave-one-out cross vali-
dation, and the information criterion have inverse correlation (Watanabe, 2022),
ðGn Lðw0ÞÞ + ðCn Lnðw0ÞÞ ¼ 2λ
n + op
1
n
 
,
(85)
ðGn Lðw0ÞÞ + ðWn Lnðw0ÞÞ ¼ 2λ
n + op
1
n
 
:
(86)
Note. If a statistical model is regular for an unknown DGP q(x), λ ¼ d/2, m ¼ 1,
and ν ¼ tr(IJ1), which shows the singular theory contains the regular theory
as a special example. The concrete values of RLCTs for several models
have been found in several statistical models. In fact, RLCTs are clarified
in neural networks (Aoyagi and Nagata, 2012; Watanabe, 2001), normal
mixtures (Yamazaki and Watanabe, 2003), matrix factorization (Aoyagi and
Watanabe, 2005), Poisson mixture (Sato and Watanabe, 2019), generalized
224
Handbook of Statistics

Markov models (Zwiernik, 2011), and Latent Dirichlet allocation (Hayashi,
2021). These results have been applied to singular BIC (Drton and Plummer,
2017) and optimal design of MCMC (Nagata and Watanabe, 2008). The mean
field approximation of the posterior distribution is called the variational Bayes-
ian inference. Both the generalization loss and the free energy of the variational
Bayes are different from those of ordinary Bayes; however, they have been
clarified (Nakajima et al., 2019; Watanabe and Watanabe, 2006).
2.5
Phase transitions
If a statistical model or a learning machine has hierarchical structures or latent
variables, then it has singularities in the parameter space (Aoyagi and Nagata,
2012; Aoyagi and Watanabe, 2005; Hayashi, 2021; Sato and Watanabe, 2019;
Watanabe, 2001; Yamazaki and Watanabe, 2003; Zwiernik, 2011). For exam-
ple, in the parameter space of a normal mixture with K components, the set of
parameters, which represent a model with H (H < K) components, contains
singularities. Many statistical models and learning machines have the same
structure. In such models, the more complex a singularity is, the simpler a
probability distribution is and the smaller RLCT is. From the viewpoint of
statistical balance between the bias and the variance, a complex singularity
makes the bias large and the variance small. In this section we study the phase
transition phenomena caused by singularities.
Note. Phase transition is a concept in statistical physics. A typical example
is illustrated as follows: Let E(w) be an energy function of a higher-
dimensional variable w and β > 0 be an inverse temperature. The free energy
F is defined by
F ¼  1
β log
Z
exp ðβEðwÞÞdw:
Then the main integrated regions of w depend on β and drastically change at
some β* which is called the critical point. In Bayesian statistics, the mathemat-
ically same phenomena can be observed by replacing β by n and E(w) ¼
Kn(w). In other words, the main integrated region in the posterior distribution
depends on the sample size and drastically changes according to its increase.
In general, the number β may be another number than the sample size.
2.5.1
Phase transition with sample size
First, we study a phase transition with respect to the increase of the sample size
n. Let fws  W ; s ¼ 1, 2, …, g be a set of parameters and W(ws) be a set of
local parameters which contains ws. Assume that {W(ws)} be a covering of W,
W ¼
[
s
WðwsÞ:
Mathematical theory of Bayesian statistics Chapter
9 225

By using the partition of unity, there exists {φs(w)} such that
φðwÞ ¼
X
s
φsðwÞ,
where the support of φs(w) is compact and contained in W(ws). The local
marginal likelihood is defined by
ZnðwsÞ ¼
Z
φsðwÞ
Y
n
i¼1
pðXijwÞdw:
The RLCT and the multiplicity defined by K(w) and φs(w) are referred to as
λs and ms, respectively. The marginal likelihood is the sum of all local
marginal likelihoods
Zn ¼
X
s
ZnðwsÞ:
(87)
Here the local marginal likelihood is given by
ZnðwsÞ ¼ exp ðnLnðwsÞ  λs log n + ðms  1Þ log log nÞCsðξnÞ,
(88)
where
CsðξnÞ ¼
Z Z
g1ðWðwsÞÞ
dt du* tλs1 exp ðt +
ﬃﬃ
t
p
ξnðuÞÞ
is a constant order random variable. Two values nLn(ws) and λs log n 
ðms  1Þ log log n represent the bias and the variance of the neighborhood
of ws, respectively. The posterior probability of the local set W(ws) is in pro-
portion to the local marginal likelihood Zn(ws); hence the posterior probability
of the local parameter set W(ws) is maximized if and only if
nLnðwsÞ + λs lognðms 1Þlog logn
is minimized. Therefore, if a sample size n is small, the local parameter set is
chosen whose bias nLn(ws) is large and variance λs log n is small; whereas, if n
becomes large, the other local parameter set is chosen whose bias is small and
variance is large. The chosen parameter set by the posterior distribution is
WðwsÞ,
s ¼ argminfnLnðwsÞ + λs log n  ðms  1Þ log log ng,
for a given sample size n. This property means that the posterior probability
naturally minimizes the free energy according to the sample size n. However,
the generalization loss is given by
Gn ¼ L0ðwsÞ + λs
n ,
where ws is determined by the posterior distribution. By the difference
between the generalization loss and the free energy, the chosen parameter
226
Handbook of Statistics

set by the posterior probability does not minimize the generalization loss
(Watanabe, 2001). The effect of singularities depends on the dimension of
the parameter. In fact, if the parameter dimension is low, the generalization
loss becomes larger at the critical point, else if it is high, the generalization
loss is made smaller (Watanabe and Amari, 2003).
2.5.2
Phase transitions with hyperparameter
Both the generalization loss and the free energy are determined by RLCT, λ,
which is characterized by the maximum pole 1/(z + λ)m of the zeta function,
Eq. (70). If a prior distribution does not satisfy the condition 0 < φ(w0) < ∞
for some w0  W0. Then RLCT depends on the prior. For example, let us study
a case when there exists a hyperparameter θ such that w ¼ ðw1, w2Þ  2,
Kðw1, w2Þ ¼ ðw1Þ2ðw2Þ2,
(89)
φðw1, w2jθÞ∝jw1jjw2jθ,
(90)
in the neighborhood of the origin, then RLCT and the multiplicity are given
by
ðλ, mÞ ¼
ððθ + 1Þ=2, 1Þ
ðθ < 1Þ
ð1, 2Þ
ðθ ¼ 1Þ
ð1, 1Þ
ðθ > 1Þ
8
>
<
>
:
:
(91)
That is to say, λ changes according to the hyperparameter, where the critical
point is θ ¼ 1. Note that the posterior distribution drastically changes at the
critical pint. If θ < 1, then the posterior distribution satisfies w2 ﬃ0, w1 is
free. If θ > 1, then w1 ﬃ0, w2 is free. In the neighborhood of the critical point
θ ¼ 1, the posterior distribution becomes sensitive which is not appropriate
for numerical approximation by the MCMC. The phase transition of this type
is often observed in the mixture models if the prior distribution of the mixture
ratio is chosen by Dirichlet one (Watanabe, 2018a; Watanabe and Watanabe,
2022; Yamazaki and Kaji, 2013).
3
Applications to statistics and machine learning
3.1
Model evaluation
In this section, we study several evaluation methods of statistical models and
learning machines. There are mainly two types of evaluation measures. One is
to estimate generalization loss and the other is to calculate the free energy.
3.1.1
Estimation of generalization loss
Firstly we study several methods by which the generalization loss is esti-
mated. When information criteria and cross validations are employed, their
Mathematical theory of Bayesian statistics Chapter
9 227

scale is defined so as to estimate 2n 	 ½Gn according to the pioneer work by
Akaike’s AIC in several implementation software. However, in this article we
adopt a scaling which estimates ½Gn, because they are compared as asymp-
totically unbiased estimators of ½Gn.
The concept of the generalization loss as a functional of a statistical model
was firstly proposed by Akaike (1974), and the information criterion Akaike
information criterion (AIC) was created. The definition of AIC is given by
AIC ¼ Tn + d=n,
(92)
which is called Akaike information criterion (AIC). At first, AIC was made
for estimating the generalization loss of the maximum likelihood method,
but it can be employed in Bayesian estimation. In the maximum likelihood
method, Tn is defined by using the maximum likelihood estimator, whereas,
in Bayesian inference, Tn is defined by using the Bayesian predictive distribu-
tion, Eq. (10). In Bayesian estimation, the deviance information criterion
(DIC) was proposed (Spiegelhalter et al., 2002),
DIC ¼ 1
n
X
n
i¼1
logpðXijw½wÞ2
n
X
n
i¼1
w logpðXijwÞ
½
,
(93)
where w½w is the average of the parameter using the posterior distribution.
Definitions of LOO and WAIC are shown in Eqs. (13) and (14). In order to
calculate LOO, n times MCMC processes are necessary because the posterior
distributions p(wjXn n Xi) should be prepared for all i ¼ 1, 2, …, n. However,
LOO can be estimated by the importance sampling cross validation (ISCV),
ISCV ¼ 1
n
X
n
i¼1
logw 1=pðXijwÞ
½
,
(94)
which can be calculated by one MCMC process. If the average w½1=pðXijwÞ
is finite for all i ¼ 1, 2, …, n, then ISCV is equal to LOO.
If q(x) is realizable by and regular for a statistical model p(xjw), then the
posterior distribution can be approximated by a normal distribution when
the sample size n tends to infinity. On such regular and realizable conditions,
AIC, DIC, WAIC, LOO, and ISCV are asymptotically equivalent to each
other as random variables. That is to say
AIC ¼ DIC + opð1=nÞ ¼ WAIC + opð1=nÞ
¼ LOO + opð1=nÞ ¼ ISCV + opð1=nÞ:
(95)
If q(x) is realizable by and regular for a model p(xjw), they are all asymptoti-
cally unbiased estimators of the generalization loss
½Gn ¼ ½AIC + oð1=nÞ ¼ ½DIC + oð1=nÞ ¼ ½WAIC + oð1=nÞ
¼ ½LOO + oð1=nÞ ¼ ½ISCV + oð1=nÞ:
228
Handbook of Statistics

If q(x) is unrealizable by or singular for a statistical model, then neither AIC
nor DIC is an unbiased estimator of the generalization loss. However, even in
such cases
½Gn ¼ ½WAIC + oð1=nÞ ¼ ½LOO + oð1=nÞ
¼ ½ISCV + oð1=nÞ:
(96)
If q(x) is singular for a statistical model, then ½AIC > ½Gn
and
½DIC < ½Gn. Numerical experiments about several cases in normal mix-
tures are shown in Table 1 of Watanabe (2021a). In order to ensure Eq.
(96), the i.i.d. condition is necessary. For not i.i.d. cases, see Section 3.3.
3.1.2
Estimation of free energy
Secondly, let us study methods for calculating the free energy. The Bayesian
information criterion (BIC) (Schwarz, 1978) and the singular BIC (sBIC)
(Drton and Plummer, 2017) are respectively defined by
BIC ¼ nLnð ^wÞ + d
2 logn,
(97)
sBIC ¼ nLnð ^wÞ +^λ logn,
(98)
where ^w is the maximum likelihood estimator, and ^λ is the estimated RLCT
using the relation among candidate models and theoretical results about
RLCTs. An alternative method to approximate Fn is WBIC (Watanabe,
2013) which is defined by
WBIC ¼
Z
nLnðwÞφðwÞ
Y
n
i¼1
pðXijwÞβdw
Z
φðwÞ
Y
n
i¼1
pðXijwÞβdw
,
(99)
where β ¼ 1= log n.
If q(x) is regular for a statistical model p(xjw),
Fn ¼ BIC + Opð1Þ ¼ sBIC + Opð1Þ ¼ WBIC + Opð1Þ,
(100)
sBIC ¼ BIC + Opð1Þ,
(101)
WBIC ¼ BIC + opð1Þ:
(102)
If q(x) is singular for a statistical model, and if the set of optimal parameters
W0 is compact, then the difference between nTn and nLnð ^wÞ is a constant order
random variable
Fn ¼ sBIC + opð log nÞ:
(103)
Fn ¼ WBIC + opð log nÞ:
(104)
Mathematical theory of Bayesian statistics Chapter
9 229

It is shown in several numerical experiments that the more precise model can
be chosen by sBIC than BIC in singular models (Drton and Plummer, 2017).
For numerical approximations, Fn needs MCMCs for all inverse temperatures
0 ¼ β0 < β1 < ⋯< βK ¼ 1, WBIC does one MCMC, and both BIC and sBIC
do no MCMC. For mixture models, Gibbs sampler can be effectively
employed for the simultaneous posterior distribution of both the parameter
and the latent variables. In order to use WBIC, an improved Gibbs sampler
for the posterior distribution with β is shown in Table 3 in Watanabe (2021b).
In practical model selection problems, both the generalization loss and the
free energy are being employed. Let fpkðxjwkÞ ; k ¼ 1, 2, …, Kg be a set of
candidate models, which are ordered from simple to complex. If a DGP is
contained in this set, and if the simplest model by which DGP is realizable is
chosen by a model selection algorithm with probability that tends to one, then
such an algorithm is called to have consistency in model selection. If a DGP
is not contained in the set of candidate models, and if the model which makes
the generalization loss smallest according to the sample size is chosen by a
model selection algorithm, then such an algorithm is called to have efficiency
in model selection. In general, model selection algorithms which estimate the
generalization loss have efficiency but do not have consistency. On the other
hand, model selection algorithms which estimate the free energy have consis-
tency but do not have efficiency. Therefore, both algorithms are useful accord-
ing to the purposes of model evaluation.
3.2
Prior evaluation
In this section, we study a prior evaluation problem in regular cases. Here
we study prior’s effect to the generalization loss, on the condition that a can-
didate prior function φ(w) is assumed to be a general nonnegative function of
a parameter w  d which may be improper,
R
φðwÞdw ¼ ∞. Even for an
improper prior function, the posterior and predictive distributions can be
defined by the same equations as Eqs. (5) and (6). Hence the generalization
loss Gn, information criterion Wn, and leave-one-out cross validation Cn are
defined by the same way as Eqs. (9), (13), and (14). On the other hand, when
we study a prior optimization problem according to the free energy, a candi-
date prior function φ(w) should be proper, because, if it is not proper, then the
free energy can be made infinitely small. Therefore, from the viewpoint of
prior evaluation, the generalization loss and the free energy are different for
the set of candidates prior distributions.
3.2.1
Generalization loss and prior
Firstly, for a fixed statistical model, the generalization loss, the information
criteria, and the cross validation loss are compared as functionals of a prior
function. Let G(φ), WAIC(φ), and LOO(φ) be the generalization loss, WAIC,
230
Handbook of Statistics

and LOO which are functionals of a given candidate prior function φ(w),
which may be improper. In this section, we assume the regularity condition
that q(x) is regular for a statistical model p(xjw); however, it may be
unrealizable by a statistical model.
Let φ0(w) > 0 be an arbitrary fixed nonnegative function on d which may
be improper. For example, one can choose φ0(w) ¼ 1 for an arbitrary w  d.
We would like to compare generalization losses of a candidate prior φ(w) and
the fixed one φ0(w). For a given candidate prior function φ(w), we define
ϕðwÞ ¼ φðwÞ=φ0ðwÞ:
If φ0(w) ≡1 is chosen, then ϕ(w) ¼ φ(w). On the regularity condition, the
posterior distribution can be approximated by a normal distribution, it is
proved in Watanabe (2018a,b) that there exists a function Mðϕ, wÞ of ϕ(w)
and w which satisfies
½GnðφÞ ¼ ½Gnðφ0Þ + Mðϕ,w0Þ
n2
+ o
1
n2


,
(105)
½LOOðφÞ ¼ ½Gnðφ0Þ + d=2 + Mðϕ,w0Þ
n2
+ o
1
n2


,
(106)
½WAICðφÞ ¼ ½Gnðφ0Þ + d=2 + Mðϕ,w0Þ
n2
+ o
1
n2


,
(107)
where w0 is the parameter that minimizes L(w) in Eq. (20). That is to say, the
expectation values of the generalization loss, LOO, and WAIC are equivalent
in the higher order. Also it was proved in Watanabe (2018a,b) that there exists
a function M(ϕ, w) of ϕ(w) and w which satisfies
LOOðφÞ ¼ LOOðφ0Þ + Mðϕ, ^wÞ
n2
+ op
1
n2


,
(108)
WAICðφÞ ¼ WAICðφ0Þ + Mðϕ, ^wÞ
n2
+ op
1
n2


,
(109)
where ^w is the parameter that maximizes φ0ðwÞQn
i¼1 pðXijwÞ; in other words,
^w is the maximum a posteriori estimator for a fixed prior function φ0(w).
Hence LOO and WAIC are equivalent in the higher order as random vari-
ables. The concrete functionals Mðϕ, wÞ and M(ϕ, w) are defined by using
higher order differential geometric forms of the log density function
log pðxjwÞ (Watanabe, 2018a,b). They satisfy
Mðϕ, ^wÞ ¼ Mðϕ,w0Þ + Op
1
n1=2


,
(110)
Mðϕ,w½wÞ ¼ Mðϕ, ^wÞ + Op
1
n
 
,
(111)
Mathematical theory of Bayesian statistics Chapter
9 231

½Mðϕ, ^wÞ ¼ Mðϕ,w0Þ + O 1
n
 
:
(112)
It should be emphasized that the generalization loss as a random variable has
the different behavior from LOO and WAIC
GnðφÞ ¼ Gnðφ0Þ + Op
1
n3=2


:
(113)
Hence minimization of WAIC or LOO by choosing φ(w) makes the average
generalization loss ½GnðφÞ smallest asymptotically; however, it does not
minimize the generalization loss Gn(φ) as a random variable.
3.2.2
Free energy and prior
Second, we study the prior evaluation problem according to the free energy.
Let Fn(φ) be a functional of a prior distribution. Assume that φ(w) and
φ0(w) satisfy
R
φðwÞdw ¼
R
φ0ðwÞdw ¼ 1. Then by Eq. (44),
FnðφÞ ¼ Fnðφ0Þ  log φðw0Þ
φ0ðw0Þ + opð1Þ:
(114)
¼ Fnðφ0Þ  log φðwmleÞ
φ0ðwmleÞ + opð1Þ,
(115)
where wmle is the maximum likelihood estimator. Hence the minimization of
Fn(φ) is asymptotically equivalent to maximization of φ at the maximum
likelihood estimator.
If a set of candidate prior distributions is given by {φ(wjθ) ; θ}, where θ is
a hyperparameter, then it is optimized by minimization of Fn(φ(wjθ)) or min-
imization of LOO(φ(wjθ)), WAIC(φ(wjθ))}. Note that the optimal hyperpara-
meters by these two methods are different even if the sample size tends to
infinity.
3.3
Not i.i.d. cases
In this section, we study several “not i.i.d. cases”, an exchangeable chase, a
conditional independent case, and a time sequence analysis are studied.
3.3.1
Exchangeable cases
Assume that X1, X2, …, Xn, … are exchangeable, that is to say, an arbitrary
marginal distribution of an arbitrary subset of random variables does not
depend on the choice of variables. In this case X1, X2, …, Xn, … are not inde-
pendent in general; however, by de Finetti’s theorem, there exists QðqÞ such
that, for an arbitrary positive integer n, the marginal distribution of
X1, X2, …, Xn is given by
232
Handbook of Statistics

qðx1,x2,…,xnÞ ¼
Z Y
n
i¼1
qðxiÞdQðqÞ,
where dQðqÞ shows the integration over functional space. This theorem shows
that, for arbitrary exchangeable random variables, there exists a DGP such that
qðxÞ  QðqÞ,
(116)
xn 
Y
n
i¼1
qðxiÞ:
(117)
Therefore X1, X2, …, Xn, … are independent on the condition that an unknown
q(x) is fixed. Even if q(x) is unrealizable or singular for a statistical model, the
generalization loss, the cross validation loss, and an information criterion
satisfy
½Gnjq ¼ ½Cnjq + Oð1=n2Þ,
(118)
½Gnjq ¼ ½Wnjq + Oð1=n2Þ,
(119)
where ½ jq shows the expectation value for a given q(x). Therefore the
generalization loss Gn for a given q(x) can be estimated by both LOO and
WAIC. It follows that
½Gn ¼ ½Cn + Oð1=n2Þ,
(120)
½Gn ¼ ½Wn + Oð1=n2Þ,
(121)
hold (Watanabe, 2021b). However, in exchangeable cases,
½Gn 6¼ ½Gnjq,
(122)
lim
n!∞½Gn 6¼ lim
n!∞½Gnjq,
(123)
even if n tends to infinity.
3.3.2
Conditional independent cases
Assume that xn ¼ fxi; i ¼ 1, 2, …, ng is not a set of independent random vari-
ables but a sequence or a set of dependent random variables and that
fYi; i ¼ 1, 2, …, ng is a set of a conditional independent random variables
whose probability distribution is Qn
i¼1qðyijxiÞ, where q(yjx) is an unknown
conditional DGP. In this situation, the posterior average w½  and the poste-
rior variance w½  are defined by the following posterior distribution:
pðwjxn,YnÞ ¼ 1
Zn
φðwÞ
Y
n
i¼1
pðYijxi,wÞ
(124)
The predictive distribution is defined by of Y for a given x is defined by
Mathematical theory of Bayesian statistics Chapter
9 233

pðyjx, xn, YnÞ ¼ w½pðyjx, wÞ:
(125)
The generalization and training losses are defined by
Gn ¼ 1
n
X
n
i¼1
Z
qðyjxiÞlogpðyjxi,xn,YnÞdy,
(126)
Tn ¼ 1
n
X
n
i¼1
logw½pðYijxi,xn,YnÞ:
(127)
Note that the posterior distribution Eq. (124), the predictive distribution Eq.
(125), and the training loss Eq. (127) have the same definitions as the case
that xn is independent. However, the generalization loss Gn in Eq. (126) for
a given xn is different from Gn for the i.i.d. case, xn  Q qðxiÞ,
Gn ≡
Z Z
qðxÞqðyjxÞ log pðyjx, xn, YnÞdxdy:
(128)
In statistics and machine learning, both Gn and Gn are necessary and important
according to the purposes of applications. Let us compare LOO and WAIC
from the viewpoint of conditional inference
LOO ¼ 1
n
X
n
i¼1
logpðYijxi,xnnxi,YnnYiÞ,
(129)
WAIC ¼ Tn + 1
n
X
n
i¼1
w½logpðYijxi,wÞ:
(130)
If xn is i.i.d., then ½LOO ¼ ½ Gn1 by definition; moreover, Gn and Gn are
asymptotically equivalent, and LOO and WAIC are also asymptotically
equivalent.
If xn is not i.i.d., Gn cannot be defined. LOO is not an unbiased estimator
of the generalization loss Gn. On the other hand, it is shown (Watanabe,
2018b) that WAIC is an asymptotically unbiased estimator of the generaliza-
tion loss, ½Gn ﬃ½Wn, because the property of WAIC is derived from the
convergence of the empirical process ξn(u) ! ξ(u) based on the conditional
independence of Yn.
In statistical estimation of the unknown conditional probability q(yjx), a
leverage sample point often should be considered which strongly affects the
estimated result. If a leverage sample point is contained in a sample, the pos-
terior average or the variance of w½1=pðYijxi, wÞ in the definition of ISCV,
Eq. (94) is infinite, hence LOO cannot be approximated by ISCV (Epifani
et al., 2008; Peruggia, 1997). When xi is defined on a high dimensional space,
then there are many leverage sample points, resulting that Gn and Gn are quite
different, and that LOO, ISCV, and WAIC are also quite different. If xn is i.i.
d. on a high dimensional space, then LOO is an unbiased estimator of Gn1,
234
Handbook of Statistics

whereas WAIC is an asymptotic unbiased estimator of Gn. ISCV is unstable
and cannot be used as numerical approximation of LOO. Unfortunately, in a
high dimensional case, variances of LOO, WAIC, and ISCV are not small,
hence careful treatment is necessary in the use of them. Experimental results
using them in a linear regression are shown in Table 3 in Watanabe (2021a).
The functional variance
VðiÞ ¼ w log pðYijxi, wÞ
½
:
can be used as a measure of a leverage of a sample point (xi, Yi). If this value
is larger than the others, then (xi, Yi) is a leverage sample point.
3.3.3
Time sequences
For general dependent random variables, the DGP qðx1, x2, …, xn, …Þ
is
unknown if no assumption is not set; hence there is not yet any universal
definition of the generalization loss. Assume that pðx1, x2, …, xn, …Þ is a prob-
ability distribution defined by a model and a prior, and that X1, X2, …, Xn, …
be a set of random variables whose GDP is unknown qðx1, x2, …, xn, …Þ. The
free energy, which is defined by the minus log marginal likelihood,
Fn ¼  log pðx1, x2, …, xnÞ:
can be understood as a measure of estimated pðx1, x2, …, xnÞ. The hold-out
cross-validation loss or the out-of-sample test is defined by
Hn ¼  log pðxj+1, x,j+2, …, xnjx1, x2, …, xjÞ:
The leave-future-out cross-validation loss (B€urkner et al., 2020) is defined by
LFO ¼ 
X
nm
i¼j
log pðxj+1, xj+2, …, xj+mjx1, x2, …, xjÞ:
These criteria can be employed in practical problems; however, mathematical
properties of these criteria are not yet clarified. It is an important future study
to clarify their performance and to create new theories and methods for such
general dependent random variables.
4
Conclusion
In this article, mathematical theory of Bayesian statistics where all models are
wrong was introduced. We derived mathematical theories for the information
criteria, the cross validation, and the free energy, which hold even if a DGP is
unrealizable by and singular for a statistical model. Mathematical theorems
hold without the assumption that “uncertainty” is captured by a statistical
model and that “belief” is represented by a prior distribution, hence they are
useful in both philosophical and practical applications.
Mathematical theory of Bayesian statistics Chapter
9 235

Abbreviations
DGP
data-generating process
ISCV
importance sampling cross validation
LOO
leave-one-out cross validation
WAIC
widely applicable information criterion
References
Akaike, H., 1974. A new look at the statistical model identification. IEEE Trans. Autom. Control
19 (6), 716–723.
Akaike, H., 1980. On the transition of the paradigm of statistical inference. Proc. Inst. Stat. Math.
27, 5–12.
Antonia Amaral Turkman, M., Carlos Daniel, P., Peter, M., 2019. Computational Bayesian Statis-
tics. Cambridge University Press.
Aoyagi, M., Nagata, K., 2012. Learning coefficient of generalization error in Bayesian estimation
and Vandermonde matrix type singularity. Neural Comput. 24 (6), 1569–1610.
Aoyagi, M., Watanabe, S., 2005. Stochastic complexities of reduced rank regression in Bayesian
estimation. Neural Netw. 18, 924–933.
Atiyah, M.F., 1970. Resolution of singularities and division of distributions. Commun. Pure Appl.
Math. 23 (2), 145–150.
Binmore, K., 2017. On the foundations of decision theory. Homo Oecon. 34, 259–273.
Box, G.E.P., 1976. Science and statistics. J. Am. Stat. Assoc. 71, 791–799. https://doi.org/
10.1080/01621459.1976.10480949.
B€urkner, P.C., Gabry, I.J., Vehtari, A., 2020. Approximate leave-future-out cross-validation for
Bayesian time series models. J. Stat. Comput. Simul. 90 (14), 2499–2523. https://doi.org/
10.1080/00949655.2020.1783262.
Congdon, P.D., 2019. Bayesian Hierarchical Models. CRC Press.
Drton, M., Plummer, M., 2017. A Bayesian information criterion for singular models. J. R. Stat.
Soc. B. 56, 1–38.
Epifani, I., MacEchern, S.N., Peruggia, M., 2008. Case-deletion importance sampling estimators:
central limit theorems and related results. Electric J. Stat. 2, 774–806.
Gelfand, A.E., Dey, D.K., Chang, H., 1992. Model determination using predictive distributions
with implementation via sampling-based method. Technical report no. 462. Department of
statistics, Stanford University, pp. 147–167.
Gelman, A., Shalizi, C.S., 2013. Philosophy and the practice of Bayesian statistics. Br. J. Math.
Stat. Psychol. 66, 8–38.
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B., 2013. Bayesian
Data Analysis III. CRC Press.
Good, I.J., 1952. Rational decisions. J. R. Stat. Soc. B 14, 107–114.
Hayashi, N., 2021. The exact asymptotic form of Bayesian generalization error in latent
Dirichlet allocation. Neural Netw. 137, 127–137.
Hironaka, H., 1964. Resolution of singularities of an algebraic variety over a field of characteristic
zero. I, II. Ann. Math. 79, 109–326.
Hobbs, N.T., Mevin, B., Hooten, M.B., 2015. Bayesian Models: A Statistical Primer for Ecolo-
gists. Princeton University Press.
236
Handbook of Statistics

Korner-Nievergelt, F., et al., 2015. Bayesian Data Analysis in Ecology Using Linear Models With
R, BUGS, and Stan. Academic Press.
Lambert, B., 2018. A Student’s Guide to Bayesian Statistics. SAGE.
Martin, O., 2016. Bayesian Analysis With Python. Packt Publishing Ltd.
McElreath, S., 2020. Statistical Rethinking: A Bayesian Course With Examples in R and STAN,
second ed. CRC Press.
Nagata, K., Watanabe, S., 2008. Asymptotic behavior of exchange ratio in exchange Monte Carlo
method. Neural Netw. 21 (7), 980–988.
Nakajima, S., Watanake, K., Sugiyama, M., 2019. Variational Bayesian Learning Theory. Cam-
bridge University Press.
Peruggia, M., 1997. On the variability of case-detection importance sampling weights in the
bayesian linear model. J. Am. Stat. Assoc. 92, 199–207.
Reich, B.J., Ghosh, S.K., 2019. Bayesian Statistical Methods. CRC Press.
Sato, K., Watanabe, S., 2019. Bayesian generalization error of poisson mixture and simplex Van-
dermonde matrix type singularity. arXiv:1912.13289.
Schwarz, G., 1978. Estimating the dimension of a model. Ann. Stat. 6 (2), 461464.
Spiegelhalter, D.J., Best, N.G., Carlin, B.P., Linde, A., 2002. Bayesian measures of model com-
plexity and fit. J. R. Stat. Soc. B 64 (4), 583–639.
Vehtari, A., Lampinen, J., 2002. Bayesian model assessment and comparison using cross-
validation predictive densities. Neural Comput. 14 (10), 2439–2468.
Vehtari, A., Gelman, A., Gabry, J., 2017. Practical bayesian model evaluation using leave-one-out
cross-validation and WAIC. Stat. Comput. 27 (5), 1413–1432.
Wang, X., Ryan, Y.Y., Faraway, J.J., 2018. Bayesian Regression Modeling With INLA. CRC
Press.
Watanabe, S., 2001. Algebraic geometrical methods for hierarchical learning machines. Neural
Netw. 14, 1049–1060.
Watanabe, S., 2009. Algebraic Geometry and Statistical Learning Theory. Cambridge University
Press.
Watanabe, S., 2010. Asymptotic learning curve and renormalizable condition in statistical
learning theory. J. Phys. Conf. Ser. 233 (1). https://doi.org/10.1088/1742-6596/233/
1/012014.
Watanabe, S., 2013. A widely applicable Bayesian information criterion. J. Mach. Learn. Res. 14,
867–897.
Watanabe, S., 2018a. Higher order equivalence of Bayes cross validation and WAIC. In: Springer
Proceedings in Mathematics and Statistics Information Geometry and Its Applications,
pp. 47–73.
Watanabe, S., 2018b. Mathematical Theory of Bayesian Statistics. CRC Press.
Watanabe, S., 2021a. Information criteria and cross validation for Bayesian inference in regular
and singular cases. Jpn. J. Stat. Data Sci. 4, 1–19.
Watanabe, S., 2021b. WAIC and WBIC for mixture models. Behaviormetrika 48, 5–21. https://
doi.org/10.1007/s41237-021-00133-z.
Watanabe, S., 2022. Mathematical theory of Bayesian statistics for unknwon information source.
arxiv. https://doi.org/10.48550/arXiv.2206.05630.
Watanabe, S., Amari, S., 2003. Learning coefficients of layered models when the true distribution
mismatches the singularities. Neural Comput. 15, 1013–1033.
Watanabe, T., Watanabe, S., 2022. Asymptotic behavior of Bayesian generalization error in mul-
tinomial mixtures. arXiv:2203.06884.
Mathematical theory of Bayesian statistics Chapter
9 237

Watanabe, K., Watanabe, S., 2006. Stochastic complexities of Gaussian mixtures in variational
Bayesian approximation. J. Mach. Learn. Res. 7, 625–644.
Yamazaki, K., Kaji, D., 2013. Comparing two Bayes methods based on the free energy functions
in Bernoulli mixtures. Neural Netw. 44, 36–43.
Yamazaki, K., Watanabe, S., 2003. Singularities in mixture models and upper bounds of stochas-
tic complexity. Int. J. Neural Netw. 6 (7), 1029–1038.
Zwiernik, P., 2011. An asymptotic behaviour of the marginal likelihood for general Markov mod-
els. J. Mach. Learn. Res. 12, 3283–3310.
238
Handbook of Statistics

Chapter 10
Geometry in sampling methods:
A review on manifold MCMC
and particle-based variational
inference methods
Chang Liua and Jun Zhub,∗
aMicrosoft Research Asia, Beijing, China
bDepartment of Computer Science and Technology, Beijing National Center for Information
Science and Technology, Tsinghua-Bosch Joint Center for ML, Tsinghua University,
Beijing, China
∗Corresponding author: e-mail: dcszj@mail.tsinghua.edu.cn
Abstract
Sampling methods are an indispensable tool for Bayesian inference, as they provide a
flexible and asymptotically exact approximation to the intractable posterior in an out-
of-the-box way. These methods generate or update the samples by simulating a dynami-
cal process, which is a construct on a space with certain geometry. Non-Euclidean
geometry has long been incorporated in Bayesian inference and continues to generate
impact. It is considered either (1) directly due to that the target distribution is defined
on a non-Euclidean manifold, or (2) for a proper dynamics that respects the geometry
of a distribution space. In this chapter, we review the background and some recent prog-
ress on the interplay between geometry and sampling methods. We consider two major
classes of sampling methods: Markov chain Monte Carlo (MCMC) and particle-based
variational inference (ParVI). For MCMC, we cover some dynamics on manifolds
and their simulation for both cases (1) and (2). For ParVI, we describe its geometric
interpretation under the view of case (2), and introduce the variants that the interpreta-
tion inspires, including those for case (1).
Keywords: Bayesian inference, Riemannian geometry, Markov chain Monte Carlo,
Particle-based variational inference
1
Geometry consideration in sampling: Why bother?
Bayesian inference is the central task in Bayesian modeling. Given a prior dis-
tribution p0(x) of a latent variable x and a likelihood distribution p(ojx) that
Handbook of Statistics, Vol. 47. https://doi.org/10.1016/bs.host.2022.07.004
Copyright © 2022 Elsevier B.V. All rights reserved.
239

links x to an observation/data variable o, the task is to approximate the poste-
rior distribution p(xjo) of the latent variable x given an observation value o.
By Bayes’ rule, we know that pðxjoÞ ¼ p0ðxÞpðojxÞ
pðoÞ
¼
p0ðxÞpðojxÞ
R
p0ðxÞpðojxÞ dx, but the task
is still highly intractable due to the commonly high-dimensional integral.
Sampling methods are a natural fit to Bayesian inference. On one hand, the
most common expectation on a posterior approximator is to estimate an
expectation of a function under the posterior. With samples of the posterior,
this can be conveniently estimated using the average of the function values
on the samples. On the other hand, sampling methods often allow an unnorma-
lized density function of the target distribution, which is the case in Bayesian
inference: p(xjo) ∝p0(x)p(ojx), so there is an easily accessible unnormalized
density.
Commonly the latent variable is represented as a vector in a Euclidean
space. But in many applications, the latent variable may be more appropriate
to be defined in a non-Euclidean (“curly”/“nonflat”) space, often formalized
as a manifold. This is the first case for considering non-Euclidean geometry
in sampling methods. For example, in some applications we only care about
the direction of a data vector while its magnitude does not convey useful
information. The vector is thus normalized and lies in a hypersphere
n1 :¼ fx  n|x>x ¼ 1g. This includes the common “term frequency–
inverse document frequency” (tf–idf ) feature representing a document by a
weighted term (i.e., word) distribution in it, which is normalized. More
instances can be found in geology and bioinformatics, which leads to the
study of directional/orientational statistics. Modeling such data often requires
the latent variable also be in the hypersphere,a leading to the sampling task
on a hypersphere. For tf–idf feature, spherical admixture model (Reisinger
et al., 2010) confines the “topic” latent vectors also on the hypersphere. Hyper-
spherical latent vector is also used in variational autoencoder (Davidson et al.,
2018), which improves the performance for hyperspherical data and also enables
an uninformative prior that helps better clustering for the usual Euclidean data.
Lan et al. (2014) considered sampling from a general norm-constrained space by
converting the space to a hypersphere.
Another example is latent variables on a simplex Δn1 :¼ fx  ð+Þn|
Pn
i¼1xi ¼ 1g,b which is common since it is where the probability parameter of
a categorical likelihood lives. Although the simplex is flat, people often desire
a parameterization using (n  1) free/unconstrained parameters, which is a non-
linear representation of the space (Beck and Teboulle, 2003; Patterson and Teh,
2013). For more examples, in matrix completion (e.g., recommender system),
the matrix is often seen as being generated from a singular-value decomposition
form to handle the rank constraint (Salakhutdinov and Mnih, 2008; Song and
aThis can be promoted from the fact that hyperspheres are not homeomorphic to Euclidean spaces.
bThe superscript i here represents the contravariant index of a vector x, but not the exponent.
240
Handbook of Statistics

Zhu, 2016; Yanush and Kropotov, 2019). Under a Bayesian treatment, this leads
to inferring latent variable on the set of orthogonal matrices (may not be in a
squared shape), called the Stiefel manifold ( James, 1976; Stiefel, 1935). Some
works on variational autoencoders (Grattarola et al., 2018; Mathieu et al.,
2019; Nagano et al., 2019; Ovinnikov, 2019) take a hyperbolic latent space for
tree-structured data, due to their similar geometry that the volume (resp. number
of nodes) grows exponentially with the distance to the origin (resp. the distance
to the root node). Some recent works also consider endowing the latent space
with the pulled-back metric from the data space so that the induced geometry
objects such as gradient, geodesic (Arvanitidis et al., 2018, 2019; Chen et al.,
2018b), exponential map (Shao et al., 2018), and isotropic Gaussian (Kalatzis
et al., 2020) in the latent space follow the data-manifold geometry, which make
operations such as interpolation in the latent space represent a semantic
meaning.
The manifold perspective is more common for Bayesian methods than one
may expect, as the concept of information geometry (Amari, 1998, 2016;
Amari and Nagaoka, 2007) introduces a “natural” metric to the latent space
(even for Euclidean latent space). Noting that each value of the latent variable
x defines a likelihood distribution p(ojx) on the observation variable o, the dif-
ference between x values may be more naturally measured by the difference
between the distributions they define (see Fig. 1). If the infinitesimal distribu-
tion difference is measured by the KL divergence, the induced latent space
metric is the Fisher–Rao metric, which is invariant to reparameterization.
The well-known natural gradient (Amari, 1998; Khan and Nielsen, 2018) is
the gradient under this metric, which is the fastest ascending direction of a
distribution objective function, and remains the same under any parameteriza-
tion of the distribution. Using natural gradient in optimization often achieves
a much faster convergence.
FIG. 1
Information geometry as an example of the second case for considering geometry in
sampling methods: to respect the geometry of a distribution space. The difference between latent
variable values x1, x2 is more appropriately measured as the difference between the likelihood dis-
tributions p(ojx1), p(ojx2) they define. Equally separated x1, x2 pairs in the latent space X may be
differently separated in the distribution space M.
Geometry in sampling methods Chapter
10 241

Information geometry is the first example of the second case for considering
geometry in sampling methods, i.e., to respect the geometry of a distribution
space. Such a consideration is also the underlying principle of particle-based
variational inference (ParVI) methods, which are a relatively new and fast-
developing branch of sampling methods. In contrast to conventional MCMC
methods, ParVI methods use a deterministic dynamics to iteratively update a
fixed-sized set of particles (i.e., samples) toward the target distribution, and
can often achieve a better sample efficiency. The dynamics is chosen to fastest
descending the difference between the particle distribution and the target distri-
bution under some metric, which is formally the gradient flow of the difference
function on a certain space of distributions. Studying the geometry of the dis-
tribution space as a manifold then reveals the assumptions and convergence
analysis of ParVI, and also draws the link to general MCMC dynamics. The geo-
metric understanding also inspires ParVI variants that converge faster, produce
more accurate approximations, and handles non-Euclidean latent space.
This review is organized as follows. We first introduce basic concepts of
manifold in Section 2. We then describe general MCMC dynamics in
Section 3.1, followed by MCMC instances on manifolds in Section 3.2 which
can be simulated in the coordinate space of the manifold. For some manifolds,
simulation in their embedded space is advantageous, as shown in Section 3.3.
Next, we introduce ParVI methods starting with perhaps the most popular
instance [Stein variational gradient descent (SVGD)] in Section 4.1. After intro-
ducing some background knowledge in Section 4.2, we present in Section 4.3
the geometric interpretation as the gradient flow on certain distribution spaces,
which reveals the assumptions and convergence analysis of ParVI. This interpre-
tation also draws the link to general MCMC dynamics as we show in
Section 4.4, and inspires ParVI variants that converge faster, produce more
accurate approximations, and handles non-Euclidean latent space as shown in
Section 4.5.
2
Manifold and related concepts
In this part we introduce some basic concepts pertaining to manifold. The
concept of manifold is a generalization of vector spaces, which allows a
“curly” intuition and spacial heterogeneity. The so-called Riemannian mani-
fold introduces a light structure but which then induces almost all counterparts
of common concepts in an inner-product space, which enables tractable com-
putation. We focus on the scheme and intuition of the concepts and include
relations to linear space when possible. See formal textbooks (e.g., Abraham
et al., 2012; Do Carmo, 1992; Nicolaescu, 2007; Romano, 2007) for a com-
plete introduction.
2.1
Manifold
A common intuitive description of an m-dimensional manifold M is that it is
a space that locally looks like the m-dimensional Euclidean space m. It is
242
Handbook of Statistics

formally defined as a topological space,c any point on which has a neighbor-
hood I homeomorphic to a Euclidean open subset Ω  m, meaning that there
is a continuous bijection Φ : I ! Ω whose inverse is also continuous (such Φ
is called a homeomorphism) (see Fig. 2A). By definition, around any point the
manifold can be locally represented using an m-dimensional coordinate
^x ¼ ΦðxÞ  m, so ðI,ΦÞ is called a local coordinate system, and Ω a coordi-
nate space. A function f : M ! on the manifold can also be concretized as a
usual multivariate function f ∘Φ1 : m ! , which holds the same continuity
as f. This is what formalizes the intuition “locally looks like the Euclidean
space.” Note that for some manifolds there is no global coordinate system,
e.g., the hypersphere n1 :¼ fx  n|x>x ¼ 1g, since it is not homeomorphic
to any Euclidean space.
The definition for now covers the continuity of the manifold, but we often
also care about differentiability and smoothness. A smooth manifold is
expected to define smooth functions on it. For a function f, its smoothness
around a point x can be characterized by that of its coordinate version
f ∘Φ1 : m ! . To make a consistent characterization independent of
the choice of coordinate system, under any other coordinate system ðJ , ΨÞ
containing x (Fig. 2B), the multivariate function f ∘Ψ1 should have the same
smoothness as f ∘Φ1. This requires the coordinate conversions Ψ ∘Φ1 and
Φ ∘Ψ1 be smooth as m ! m functions. Such coordinate systems are
called compatible, and the manifold M is called smooth if there is a set of
compatible coordinate systems that covers M. The differentiability/smooth-
ness of a function f or a curve γ  : ½a,b ! M on a smooth manifold can be
consistently determined by that of f ∘Φ1 : m ! m or t 7! ΦðγtÞ : ! m.
FIG. 2
Illustration of the concepts of manifold and coordinate system. (A) Concept of manifold
and local coordinate system. (B) Intersecting coordinates for characterizing a smooth manifold.
cA topological space is a set with a topology, which is roughly a set of abstract open subsets
(containing ; and the entire set, closed under finite intersection and countable union), enabling
the definitions of neighborhood, limit, and continuity. Commonly for defining a manifold, the
topological space requires second-countable (the topology can be generated from a countable
set of open subsets) and second-separable (any two points have nonintersecting neighborhoods).
Geometry in sampling methods Chapter
10 243

Denote the set of smooth functions on M as C∞ðMÞ. Since common mani-
folds are smooth and common objects of interest are defined on smooth mani-
folds, we focus on smooth manifolds and smooth functions hereafter, and still
call them “manifold” and “function” for brevity.
2.2
Tangent vector and vector field
(See Fig. 3 for the relations among the introduced concepts in this and the
next subsection.) Fundamental geometric descriptions of the manifold and
algorithmically concerned dynamics (e.g., gradient descent and Langevin
dynamics analogies) calls for the concept of tangent vector. In m the tangent
vector at a point γ0 on a smooth curve (γt)t (Fig. 4A) is the limiting vector
v :¼ dγt
dt

t¼0 ¼ lim h!0 1
h ðγh  γ0Þ, which holds the meaning of the velocity of
a particle moving along the curve. Unfortunately on manifolds there is no vec-
tor subtraction (yet). For another characterization, note that the curve induces
a directional derivative of a function f at γ0 as d
dt fðγðtÞÞ

t¼0, which is, by the
chain rule, ∂i fðγ0Þdγi
t
dt

t¼0 ¼ viðγ0Þ∂i fðγ0Þ, where ∂if is short for ∂
∂xi fðx1,…, xmÞ,
and we have used Einstein’s summation convention that the index repeated in
both a subscript and a superscript (e.g., i here) is summed over automatically
FIG. 3
Relations among some concepts on a manifold M. A symbol just above a curly arrow
represents an instance of the map. See Sections 2.2 and 2.3 for details.
FIG. 4
Illustration of tangent vector, tangent space, vector field, and flow on a manifold M. See
Section 2.2 for details. (A) Tangent vector on a curve. (B) Tangent vector and tangent space on a
manifold. (C) Vector field and its flow.
244
Handbook of Statistics

(i.e., the summation symbol “Pm
i¼1” is omitted). So the tangent vector has
another appearance v ¼ vi∂i as a directional derivative operator on functions,
and its action on a function v[f] is the directional derivative of f along v.
This perspective can be extended to manifolds, since the directional deriv-
ative d
dt fðγtÞ

t¼0 is still well defined (note t 7! f(γt) is a ! function).
Using any coordinate system ðI,ΦÞ
around γ0, it can be expressed as
d
dt ðf∘Φ1ÞðΦðγtÞÞ

t¼0 ¼ vi∂i f in the sense that vi :¼ d
dt ΦiðγtÞ

t¼0 and ∂i f :¼
∂
∂^xi fðΦ1ð^xÞÞ

^x¼Φðγ0Þ (Fig. 4A). Covering all possible smooth curves passing
through x, a general tangent vector v at x  M as a directional derivative
operator, is defined as a linear function on C∞ðMÞd satisfying the Leibniz
rule: v[fh] ¼ f(x)v[h] + h(x)v[f]. All tangent vectors at x form an m-dimensional
linear space called the tangent space TxM at x (Fig. 4B), and under any
coordinate system containing x, f∂igm
i¼1
is a basis, which is defined as
∂i f :¼ ∂
∂^xi fðΦ1ð^xÞÞ

^x¼ΦðxÞ. Any v  TxM can be expressed as v ¼ vi∂i where
vi ¼ v[Φi].e Under the change of coordinate system, the basis transforms as
~∂α ¼ ∂^xi
∂^yα ∂i, where ~∂α f :¼
∂
∂^yα fðΨ1ð^yÞÞ

^y¼ΨðxÞ forms the basis under the new
coordinate system ðJ ,ΨÞ, and ∂^xi
∂^yα :¼ ∂
∂^yα Φi∘Ψ1ð^yÞ

^y¼ΨðxÞ. For a given tangent
vector v, its coordinate transforms similarly: ~vα ¼ ∂^yα
∂^xi vi. Note that the coordinate
expression v ¼ vi∂i ¼ ~vα~∂α is invariant under coordinate change.
If M is a linear space, then it is isomorphic to the tangent space TxM
for any x  M : y  M 7! vy  TxM,vy½f :¼ d
dt fðx + tyÞ

t¼0. But generally
tangent spaces at different points are different linear spaces.
A vector field V (Fig. 4C) defines a tangent vector V (x) at every point x on
the manifold, and V (x) depends on x smoothly (e.g., Vi ∘Φ1 for each i is
smooth in any coordinate system). Denote the set of vector fields on M as
T ðMÞ. A vector field defines a dynamics (dynamical system) on the mani-
fold: dxt
dt ¼ VðxtÞ, which describes how a particle moves on the manifold as
time proceeds. Its solution is called a flow, which is a set of curves
fðϕtðxÞÞt|x  Mg such that ϕ0(x) ¼ x and d
dt ϕtðxÞ

t¼0 ¼ VðxÞ. The flow of
any vector field V exists at least locally (due to Picard–Lindel€of theorem).
For a concise and conventional notation, in the following, symbols with
indices (e.g., xi, vi) represent the coordinates of the same objects (e.g., x, v)
in some coordinate system. Particularly we use xi to represent the same thing
as ^xi, i.e., the coordinates of a manifold point x.
dMore precisely, instead of C∞ðMÞ, it is sufficient to only consider functions that are smooth in a
neighborhood of x.
eThis can be seen through v½Φj ¼ vi∂iΦj ¼ vi ∂
∂^xi ΦjðΦ1ð^x1,…, ^xmÞÞ

^x¼ΦðxÞ ¼ vi∂^xj
∂^xi

^x¼ΦðxÞ
¼ viδj
i ¼ vj.
Geometry in sampling methods Chapter
10 245

2.3
Cotangent vector and differential form
The cotangent space T
xM at x is the dual space of the tangent space TxM.
Under a coordinate system, the basis f∂igm
i¼1 of TxM induces a dual basis
f∂*ig
m
i¼1 for T
xM: ∂*i[v] :¼ vi ¼ v[Φi], which satisfies ∂*i½∂j ¼ δi
j.f Under this
perspective, the directional derivative along vector v, v[f] ¼ vi∂if ¼ (∂i f∂*i)
[vj∂j] can be viewed as the action of the covector ∂if∂*i on the vector v.
We define this covector as the differential of f: df  T
xM, df½v :¼
v½ f,8v  TxM . We then recognize that dΦi½∂j ¼ ∂j½Φi ¼ ∂
∂^xj ΦiðΦ1ð^xÞÞ ¼
∂^xi
∂^xj ¼ δi
j, so ∂*i is essentially dΦi. Conventionally dΦi is denoted using the sym-
bol of the corresponding coordinates as dxi. The covector is then expressed as
df ¼ ∂if dxi, which is the form of the usual differential in calculus.
To describe a k-dimensional volume element on a manifold and more, we
need the concept of k-form. To draw the intuition, consider the volume of
the k-dimensional parallelepiped formed by k vectors. We know that the
volume responds linearly to each vector, and switching two vectors alters
the orientation of the parallelepiped, so the volume changes its sign. This tells
us that the volume is an antisymmetric linear function on the k vectors.
Indeed, in k, the volume formed by k vectors is the k  k determinant of
the vector-stacking matrix, which is an antisymmetric linear function.
On a manifold, a k-dimensional infinitesimal volume element at a point x
is formed by k tangent vectors from TxM. We call the map from the k vectors
to the volume value as a k-differential form, or just k-form. Formally, a k-form
μ : ðTxMÞk ! is an antisymmetric k-multilinear function on TxM. Denote
the space of k-forms as ^kT
xM, which is a subspace of ðT
xMÞk :¼  kT
xM,
i.e. the space of k-multilinear functions. In this sense, a covector from T
xM is
also recognized as a 1-form. Due to antisymmetry, if there are two identical
vectors in the k input vectors, the k-form outputs zero. Due to linearity, this
case can be extended to k linearly dependent vectors. Since when k > m,
any k vectors are linearly dependent, and we know that all k-forms are
trivially zero for k > m. In other cases, due to antisymmetry, ^kT
xM is
m
k
 
-dimensional.
To construct a k-form using k covectors frð1Þ,…, rðkÞg from T
xM (i.e.,
using k 1-forms), we introduce the wedge product, which is essentially the
antisymmetrized tensor product: rð1Þ ^⋯^rðkÞ :¼ P
σð1Þσrðσ1Þ  ⋯ rðσkÞ,
where σ traverses over all the permutations of f1,…,kg and (1)σ is its sign.g
fThe symbol δi
j is the Kronecker delta tensor, i.e., δi
j :¼ 1 if i ¼ j and δi
j :¼ 0 otherwise. Similarly,
δij, δij are defined for the corresponding types of tensors.
gThe notation as the product of a binary operator ^ (instead of an operator on k covectors alto-
gether) is valid since ðrð1Þ ^⋯^rðiÞÞ^ðrði + 1Þ ^⋯^rðkÞÞ ¼ rð1Þ ^⋯^rðkÞ for any 1 	 i < k.
246
Handbook of Statistics

Using this notion, the
m
k
 
k-forms fdxi1 ^⋯^dxikg1	i1<⋯<ik	m build a basis
of ^kT
xM. As a tensor in ðT
xMÞk, any k-form can be expanded as μ ¼
μi1…ik dxi1  ⋯ dxik, where μi1…ik ¼ μ½∂i1,…, ∂ik. Due to antisymmetry, per-
mutationally
equivalent
index
tuples
can
be
grouped
together:
μ ¼ P
1	i1<⋯<ik	m
P
σμiσ1…iσk dxiσ1  ⋯ dxiσk ¼ P
1	i1<⋯<ik	m μi1⋯ik
P
σð1Þσ
dxiσ1  ⋯ dxiσk ¼P
1	i1<⋯<ik	m μi1⋯ikdxi1^⋯^dxik ¼ 1
k!μi1…ik dxi1^⋯^dxik.
The differential operator d can be extended as exterior derivative to act on
a k-form and give a (k + 1)-form: dμ :¼ P
1	i1<⋯<ik	mdμi1…ik ^ dxi1 ^ ⋯^
dxik ¼ 1
k! ∂iμi1…ik dxi ^ dxi1 ^ ⋯^ dxik . This definition is independent of the
choice of coordinate system; particularly it has an alternative definition: df is
the differential of function for a 0-form f (i.e., a function), and d(μ ^ ν) ¼
dμ ^ ν + (1)kμ ^ dν for k-form μ, and d ∘d ¼ 0.
2.4
Riemannian manifold
A Riemannian manifold is a manifold M in any of whose tangent space TxM
there is an inner product h  ,  iTxM
defined, and that it depends on x
smoothly. This h  ,  iTxM is called a Riemannian structure or metric. In any
coordinate system it can be expressed as a positive definite matrix,
hu,viTxM ¼ gijðxÞuivi where gijðxÞ :¼ h∂i,∂jiTxM. It also induces a norm in each
tangent space, kvkTxM :¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
hv,viTxM
q
. This simple structure endows the
manifold with many useful concepts that make computation possible.
The gradient of a function f  C∞ðMÞ at x  M is defined as the tangent
vector
grad fðxÞ  TxM
that
satisfies
hgrad fðxÞ,viTxM ¼ df½v ¼ v½ f,
8v  TxM. It has the following coordinate expression:
grad fðxÞ ¼ gijðxÞ∂j fðxÞ∂i ¼ GðxÞ1rfðxÞ,
(1)
where (gij) ¼ G1 is the inverse matrix of the Riemann metric tensor G :¼ (gij).
The gradient can be defined all over the manifold, and the flow of the nega-
tive of this vector field is called a gradient flow. This abstract definition of
gradient meets the common intuition of a fastest ascending direction for
f:
max  argmaxv TxM:kvk¼1v½ f ¼ max  argmaxv TxM:kvk¼1hgrad fðxÞ,viTxM ¼
grad fðxÞ, where “max  argmax” denotes the scalar product of the maximum
to the maximizing vector.
The length of a curve γ : ½a,b ! M can be defined using the Riemannian
structure as: LðγÞ :¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
R b
a kdγt=dtk2
TγtM dt
q
,
which leads to the distance
between two points on the manifold:
dMðx,yÞ :¼
inf
ðγtÞt ½0,1:γ0¼x,γ1¼yLðγÞ:
(2)
If the manifold is complete under this distance, the distance-minimizing curve
exists [Hopf–Rinow theorem (Hopf and Rinow, 1931)], which is called a
Geometry in sampling methods Chapter
10 247

geodesic. It is the counterpart of straight line in Euclidean spaces. As illu-
strated in Fig. 5A, in analogy to vector addition x + v in linear space that
moves a point x along the straight line in the direction of v, define the expo-
nential map Expx : TxM ! M,v 7! γ1 as moving a point x along the geode-
sic (γt)t [0, 1] tangent to v at x : γ0 ¼ x, dγt
dt

t¼0 ¼ v, which exists uniquely. As
mentioned in Section 2.2, tangent space of a linear space is everywhere the
same, but is not for a general manifold. Fortunately the Riemannian structure
induces a link between tangent spaces at different points, which is the parallel
transport Γy
x : TxM ! TyM (Fig. 5B). It transports a tangent vector at x to a
tangent vector at y along the geodesich (γt)t [0,1] from x to y in a certain way
that is regarded “parallel”; particularly
dγt
dt , vt
D
E
TγtM does not change with t,
where v0 ¼ v, v1 ¼ Γy
xðvÞ.i
2.5
Measure
A measure on an m-dimensional manifoldj is represented by an m-form ω,
which measures an m-dimensional infinitesimal volume element everywhere
on the manifold. Since the space of m-forms is one-dimensional, ω is repre-
sented by ω1…m dx1 ^ ⋯^ dxm, or simply ω dx, where dx :¼ dx1 ^ ⋯^ dxm
represents the usual Lebesgue measure of the Euclidean coordinate space.
As a measure, ω1…m is required to be a nonnegative function. Such an m-form
is also called a volume form. Under coordinate system change, the coordinate
FIG. 5
Illustration of concepts on a Riemannian manifold, with analogy to a linear space. See
Section 2.4 for more details. (A) Vector addition in linear space (left) and exponential map on
Riemannian manifold (right). (B) Parallel transport in linear space (left) and on Riemannian man-
ifold (right).
hA noteworthy distinction from the linear case is that transporting the vector in the parallel way
but along different paths would generally yield different results, and the difference is related to
the curvature of the manifold. If there is no difference, the manifold is seen as flat (though unnec-
essarily linear).
iWe would like to mention that the general definitions of geodesic, exponential map and parallel
transport are built on an independent manifold structure called affine connection, and the defini-
tions here correspond to the special version under the Levi-Civita connection, which is an affine
connection induced from the Riemannian structure.
jTo define a measure, the manifold is required to be orientable: there exists a set of coordinate
systems covering the manifold and in the intersection of any two coordinate systems, the Jacobian
determinant of the coordinate transformation
∂^yα
∂^xi



 is positive.
248
Handbook of Statistics

expression transforms with the Jacobian determinant of the coordinate trans-
formation: ~ωðyÞ ¼ ωðxÞ=
∂^yα
∂^xi



.
On a Riemannian manifold, there is a special measure called the Rieman-
nian measure, which has the coordinate expression ωg ¼
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
dx, where jGj
is the determinant of G. This expression is coordinate invariant, meaning that
in another coordinate system, it becomes ~ωg ¼
ﬃﬃﬃﬃﬃﬃﬃ
j ~Gj
q
dy.
Integral on the manifold can be defined under a measure. Particularly the
density function of a distribution/measure η on the manifold can be defined:
for any measurable subset I  M, ηðIÞ ¼
R
ΦðIÞpL dx ¼
R
ΦðIÞpR dωg, where
pL is the density w.r.t. the Lebesgue measure dx in the coordinate space, and
pR is the density function w.r.t. the Riemannian measure ωg. They are related by:
pL ¼ pR
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
:
Finally we mention the well-known Stokes theorem: for a proper manifold
region I with boundary ∂I, and an (m  1)-form η, we have
R
Idη ¼
R
∂Iη.
2.6
Divergence and Laplacian
In Euclidean space, the divergence of a vector field V is defined as (r V )(x)
:¼ ∂iVi(x). It is considered in a type of integral with compactly supported
function
f:
R
mV  rf dx ¼
R
mVið∂ifÞ dx ¼ 
R
m f∂iVi dx ¼ 
R
m fr  V dx,
where the second equality is due to integration by parts and that the term
R
m∂iðfViÞ dx ¼
R
∂m fVi dSi (due to Stokes theorem; ( dSi)i is the infinitesimal
surface element with out-pointing normal direction on the infinitely large
sphere ∂m, such that ddS ¼ dx) vanishes since f ¼ 0 on ∂m. On a Rieman-
nian manifold, the concept of divergence can be extended similarly under this
characterization. For a coordinate-invariant definition, integrals are consid-
ered under the Riemannian measure:
div : T ðMÞ ! C∞ðMÞ,
s:t:
Z
M
V½ f dωg ¼ 
Z
M
fdiv V dωg, 8f  C∞
c ðMÞ,
(3)
where C∞
c ðMÞ denotes the set of all compactly supported functions on
manifold M. Note by the definition of gradient, the l.h.s. can also be
written as
R
hgrad fðxÞ,VðxÞiTxM dωgðxÞ. In any coordinate space, the l.h.s.
is
R
Vjð∂j fÞ
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
dx ¼ 
R
f∂jðVj
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
Þ dx,
which
equals
to
the
r.h.s

R
fdiv V
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
dx by the definition. This then leads to the coordinate
expression of the divergence:
div V ¼ ∂ið
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
ViÞ=
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
¼ ∂iVi + Vi∂i log
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
:
Geometry in sampling methods Chapter
10 249

In Euclidean space, the Laplacian of a function r2f :¼ Pm
i¼1∂i∂if ¼
r  rf
can be seen as the divergence of the gradient of f. This can be
extended to Riemannian manifold:
Lap f :¼ divðgrad fÞ ¼ ∂ið
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
gij∂jfÞ=
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
:
2.7
Manifold embedding
Many manifolds are defined as a subset of a Euclidean space n, e.g., the
hypersphere n1 :¼ fx  n|x>x ¼ 1g. This is often how we imagine or pic-
ture a manifold (even in our illustrative figures), but is it possible for any
manifold defined in the abstract way described in Section 2.1? This is for-
mally described as the embedding of a manifold M, which is a smooth injec-
tion Ξ : M ! n
to a Euclidean space so that we can understand the
manifold as a subset ΞðMÞ of n (Fig. 6). Whitney embedding theorem
(Persson, 2014; Whitney, 1944) shows that an m-dimensional manifold can
always be embedded into 2m.
For a Riemannian manifold, the embedding also pulls back the Euclidean
metric δαβ to the manifold as ~gij ¼ δαβ
∂^yα
∂^xi
∂^yβ
∂^xj for ^y ¼ ΞðΦ1ð^xÞÞ. If it coincides
with the original metric gij, the embedding is said isometric. Nash embedding
theorem (Nash, 1956) shows that any Riemannian manifold can be isometri-
cally embedded into a Euclidean space.
In the embedded space n, the restriction of the n-dimensional Lebesgue
measure onto the subset ΞðMÞ  n induces a measure on ΞðMÞ, which is
called the Hausdorff measure. A distribution on manifold then also admits a
density function pH : ΞðMÞ ! 
0 w.r.t. this measure. If the manifold is
Riemannian and the embedding is isometric, then the Hausdorff measure
coincides with the Riemannian measure, and pR coincides with pH in the sense
that pR ¼ pH ∘Ξ∘Φ1.
FIG. 6
Illustration of the embedding of a manifold.
250
Handbook of Statistics

3
Markov chain Monte Carlo on Riemannian manifolds
Markov chain Monte Carlo (MCMC) methods have long been a workhorse for
Bayesian inference. When for inferring latent variables on a manifold, classi-
cal MCMC methods such as Gibbs sampling and Metropolis–Hastings (MH)
can be adapted in some cases. For example, for sampling from a hypersphere
n1 using MH, one can draw a proposal from an isotropic Gaussian centered
at the current sample and project (normalize) the sample onto n1, then com-
pute the acceptance rate using the (unnormalized) target distribution density
(the proposal density ratio is 1 since the Gaussian is isotropic thus symmetric
after projection) (Reisinger et al., 2010). But for a general manifold there does
not seem to be a systematic way to construct such variants. More importantly,
same as in the usual Euclidean case, these classical methods are not suffi-
ciently efficient as their Markov chains mix slowly due to the uninformative
and local transition.
Dynamics-based MCMC methods are more efficient and lead to the recent
trend. They carry out the sampling process by simulating a dynamics, or for-
mally a diffusion process or a continuous-time no-jump Markov process [see
e.g., S€arkk€a and Solin (2019) for a comprehensive introduction]. The dynam-
ics typically leverages the gradient of the target distribution log-density so the
move is more informative and leads to distant transition. This lowers autocor-
relation and increases the effective sample size and sampling efficiency.
Many dynamics show nice convergence properties (Eberle et al., 2019;
Mangoubi and Smith, 2017; Roberts et al., 1996; Seiler et al., 2014), which
gift an exponential convergence guarantee to many algorithms under proper
conditions (Cheng and Bartlett, 2017; Cheng et al., 2018; Dalalyan, 2017;
Durmus and Moulines, 2016; Durmus et al., 2017; Livingstone et al., 2019;
Roberts et al., 1996; Seiler et al., 2014). Particularly for Bayesian inference,
the gradient formulation also allows using stochastic gradient in the simulation
of many dynamics (Chen et al., 2014; Welling and Teh, 2011) to scale to large
datasets. Given a set of independently and identically distributed (IID) data
points foðiÞg
N
i¼1, the stochastic gradient estimates the gradient of the posterior
log-density rx log pðxjfoðiÞg
N
i¼1Þ ¼ rx log p0ðxÞ + PN
i¼1 log pðoðiÞjxÞ using a
uniformly randomly chosen sub-dataset S (resampled for each call to the
gradient) of a fixed size jSj as:
ðStochastic gradientÞ
rxlogp0ðxÞ + N
jSj
X
o  S
logpðojxÞ:
(4)
It is a stochastic but unbiased and cheap estimate of the true gradient so that
the method is scalable to large datasets. Moreover, dynamics are constructed
using geometric objects, so can be naturally extended to Riemannian mani-
folds. We thus focus on dynamics-based MCMC methods in the following.
Geometry in sampling methods Chapter
10 251

3.1
Technical description of general MCMC dynamics
In the Euclidean case, a dynamics is described by the so-called stochastic dif-
ferential equation (SDE):
ðDynamicsÞ
dx ¼ VtðxÞ dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2DtðxÞ
p
dBtðxÞ,
(5)
where Vt(x) is a vector or velocity field (may be time dependent) describing
the deterministic drift of particles, Dt(x) is a symmetric positive semidefinite
matrix called the diffusion matrix (also may be time dependent) describing
the strength of the stochastic diffusion, and Bt(x) is the standard Brownian
motion (or called the Wiener process), which can be seen as adding a sample
of infinitesimal scale from the normal distribution N ðx,dtÞ independently
ateach time instance t for advancing an infinitesimal time period dt. The
square root
ﬃﬃﬃﬃﬃﬃﬃﬃ
2Dt
p
represents the positive semidefinite matrix such that
ﬃﬃﬃﬃﬃﬃﬃﬃ
2Dt
p
ﬃﬃﬃﬃﬃﬃﬃﬃ
2Dt
p
> ¼ 2Dt. A dynamics induces the evolution of the distribution of
particles moving under the dynamics (see Fig. 7). Denoting the evolving dis-
tribution as (qt)t in terms of the density function w.r.t. the Lebesgue measure,
the evolution rule is explicitly given by the Fokker–Planck equation (FPE):
ðFPEÞ
∂tqt ¼ r  ðqtVtÞ + rr> : ðqtDtÞ,
(6)
where rr> : ðqtDtÞ :¼ ∂i∂jðqtDij
t Þ ¼ trðrr>ðqtD>
t ÞÞ is the double-dot prod-
uct of two matrices of the same size (see e.g., Villani (2008, pp. 27–30), for
the time-dependent, weak derivative, and Riemannian manifold extension).
The FPE plays a central role in developing proper dynamics for MCMC.
Particularly, the target distribution p needs to be a stationary distribution
of the dynamics, which leads to the requirement on the dynamics that
0 ¼ r (pVt) + rr> : (pDt). Based on this, Ma et al. (2015) developed a
FIG. 7
Illustration of the evolving distribution induced by a dynamics equation (5). Given par-
ticles fxðiÞ
t gi at time t that distribute obeying distribution qt, if they move according to the dynam-
ics equation (5), then after a short period of time ε their positions undergo a displacement (shown
as the red arrow) and their new positions fxðiÞ
t+εgi distribute obeying a new distribution qt+ε. When
ε approaches to zero, the distribution qt evolves continuously in time. The rule of the evolution is
given by the Fokker–Planck equation (6).
252
Handbook of Statistics

complete recipe for all the dynamics that has p as a stationary distribution
(and with time-independent V and D)k:
ðp-stationary dynamicsÞ
dx ¼ VpðxÞ dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2DðxÞ
p
dBtðxÞ,
(7)
where
VpðxÞ :¼ r  pðxÞDðxÞ + pðxÞQðxÞ
ð
Þ=pðxÞ
which means
Vi
pðxÞ :¼ ∂j pðxÞDijðxÞ + pðxÞQijðxÞ


=pðxÞ,
(8)
and Q(x) is any antisymmetric matrix (Q> ¼ Q) called a curl matrix. When
D is positive (strictly) definite (i.e., it is nonsingular), p is the only stationary
distribution of this dynamics. By the FPE (6), the dynamics equation (7)
is equivalent to the following deterministic dynamics (Liu et al., 2019b,
Lem. 1, Thm. 5) in the sense of the induced evolving distribution:
dx
dt ¼ DðxÞr log ðpðxÞ=qtðxÞÞ + QðxÞr log pðxÞ + r  QðxÞ,
(9)
or; dx
dt ¼ DðxÞr log ðpðxÞ=qtðxÞÞ + QðxÞr log ðpðxÞ=qtðxÞÞ,
(10)
where qt is the distribution of xt. This formulation will be explored in
Section 4.4 for a geometric interpretation of a general MCMC dynamics.
The deterministic equivalent of a diffusion process is also leveraged in
diffusion-based generative models (Song et al., 2021) (“probability flow”).
In early versions, dynamics simulation is followed by an MH step to cor-
rect discretization error. But in many cases the MH step is intractable or
costly, so it is omitted when using small enough discretization.
3.2
Riemannian MCMC in coordinate space
By definition (Section 2.1), a manifold can naturally be described in each
coordinate space, and we can then carry out Euclidean space operations there.
This is particularly convenient for manifolds that has a global coordinate
system.
3.2.1
Langevin dynamics
Perhaps the first dynamics-based MCMC method that got extended is the
Langevin dynamics (Langevin, 1908; Roberts et al., 1996). Its plain form in
the Euclidean space m is described by the SDE:
ðLDÞ
dx ¼ r log pL dt +
ﬃﬃﬃ
2
p
dBt,
(11)
kThe “matrix-divergence” operation ∂jMij may be better denoted as (r  M>)>. Here we just use
r  M for a concise notation.
Geometry in sampling methods Chapter
10 253

where pL is the density of the target distribution in the usual sense, i.e., w.r.t.
the Lebesgue measure in m (see Section 2.5). Its manifold extension is then
developed following the same form:
ðRLDÞ
dx ¼ grad log pR dt +
ﬃﬃﬃ
2
p
d~Bt
(12)
¼ G1rlog pR dt + r
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
G1


=
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2G1
p
dBt
(13)
¼ G1rlogpL dt + rG1 dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2G1
p
dBt:
(14)
Here, ~Bt is the standard Brownian motion on the Riemannian manifold. This
extension can be developed from their effects in evolving distribution.
In the Euclidean space the standard Brownian motion (V ≡0, D ≡1
2 Im in
Eq. 5) leads to the diffusion equation or heat equation ∂tqt ¼ 1
2 r2qt by the
FPE (6), so on the Riemannian manifold the standard Brownian motion
should lead to the distribution evolution ∂tqt ¼ 1
2 Lap qt (see Section 2.6).
This relates the manifold motion to the Euclidean motion in the coordinate
space, which leads to its coordinate expression (Kent, 1978) and subsequently
Eq. (13).
Algorithmic developments are then pursued. Roberts and Stramer (2002)
developed sampling algorithms using the expression (14). Following the com-
mon practice of plain Langevin dynamics, the Euler–Maruyama integrator is
used due to its simplicity:
xk+1 ¼ xk + εG1ðxkÞr log pLðxkÞ + εr  G1ðxkÞ + N ð0,2G1ðxkÞεÞ, (15)
where “ + N ð0, 2G1ðxkÞεÞ” means adding a random sample from the speci-
fied normal distribution for this step of update. As the counterpart of the Euler
integrator for ordinary differential equations, the Euler–Maruyama integrator is
also of first order, i.e., the discretization error is proportional to the step size ε.
An MH step is also called after each update to correct discretization error.
This is possible since the density function of the proposal distribution by
Eq. (15) can be computed. Compared to random-walk MH samplers, using
Langevin dynamics makes more effective move as there is a driving force
toward the nearby high-probability region (while the Brownian motion
keeps the samples a reasonable dispersion and helps exploring all the
high-probability regions). Moreover, if discretization error can be omitted,
then the MH acceptance ratio is 1. The work also includes a convergence
analysis on the simulation. Girolami and Calderhead (2011) applied the
algorithm to Bayesian inference under the information geometry perspec-
tive, i.e., using the Fisher information matrix as the Riemann metric tensor:
GðxÞ :¼ pðojxÞ½rx logpðojxÞr>
x logpðojxÞ. Improved convergence rate is
shown empirically over several Bayesian models including Bayesian logistic
regression. The same spirit is applied to Bayesian neural networks (Li et al.,
2016), where a different metric is designed based on adaptive-gradient
254
Handbook of Statistics

optimization methods, as computing the Fisher information matrix is unaf-
fordably costly in this case. Patterson and Teh (2013) made the extension
to using stochastic gradient and applied to the inference task of latent
Dirichlet allocation (LDA) (Blei et al., 2003) on large datasets. In the task
the latent variable lies on simplexes, which is represented by nonlinear
coordinate systems.
A literature remark is that the dynamics formulated in Roberts and Stramer
(2002) and in Girolami and Calderhead (2011) and Patterson and Teh (2013)
turn out to bel :
dx ¼ G1r log pL dt + 2r  G1 dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2G1
p
dBt, and
(16)
dx ¼ G1r log pL dt + 2r 
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
G1


=
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2G1
p
dBt,
(17)
respectively. The respective differences from Eqs. (14) and (13) are pointed out
by Xifara et al. (2014) and are seen as a transcription error. However, Xifara
et al. (2014) also showed that Eq. (17) recovers the correct dynamics by noting:
M1r log jMj + r  M1 ¼ 0,
if Mij ¼ Mji, and ∂kMij ¼ ∂iMkj,
(18)
which is satisfied if G is defined as the Hessian of a convex function (e.g., in
mirror descent).
3.2.2
Hamiltonian dynamics
The Hamiltonian dynamics is a reformulation of Newton’s law in classical
mechanics. It describes the state of a system with the position x and also
the momentum r. The augmented (x, r) space is called phase space. A nice
property is that, for a target distribution p(x) on the positional space, and
any constant symmetric positive definite matrix Σ, the dynamics under the
potential energy  log pðxÞ,
ðHMCÞ
dx ¼ Σ1r dt,
dr ¼ r log pðxÞ dt,
(19)
keeps the phase-space distribution pðxÞN ðrj0,ΣÞ stationary. This can be seen
from the FPE (6) when taking x as (x, r) on the phase space. A physical under-
standing can be seen by noting that the log-density of the distribution is the
energy of the state (x, r) (hence Σ holds the physical meaning of a mass matrix),
which the dynamics conserves. Moreover the dynamics is so-called volume
preserving (Liouville theorem), i.e., the Jacobian determinant of an infinitesi-
mal transformation by the dynamics (ratio of the infinitesimal volume change)
is 1, so the distribution is kept stationary. By this property, Duane et al. (1987)
proposed an MCMC algorithm called hybrid Monte Carlo, which is subse-
quently called Hamiltonian Monte Carlo (HMC) as the common name.
lWe use a half time scale here: Eqs. (16) and (17) are the results after replacing t with 2t in the
formulations in the references.
Geometry in sampling methods Chapter
10 255

However, the simulation trajectory only covers a subset of the positional
space (particularly, due to energy conservation, the trajectory cannot go beyond
regions with higher potential energy than the total energy). So for drawing each
sample/proposal, the momentum variable r is resampled from N ðrj0,ΣÞ. More-
over, as the volume-preserving property is a key ingredient to keep the target
stationary, the simulation method (integrator) is also expected so, and to make
this property well defined, the integrator is also required invertible (the
continuous-time dynamics is naturally invertible). Otherwise, the volume
change would accumulate for long simulations and strongly deviate the station-
ary distribution, and the MH acceptance probability drops, making ineffective
move. Due to this consideration, the St€ormer–Verlet or called the leapfrog inte-
grator is used, which updates x and r separately on an interleaving time-
discretization scheme and is both invertible and volume preserving. In all, for
drawing a sample using L inner simulation steps, the process of HMC is:
xðk,0Þ :¼ xðk1Þ,rðk,0Þ  N ð0,ΣÞ,rðk, 1
2Þ :¼ rðk,0Þ + ε
2rlogpðxðk,0ÞÞ;
xðk:lÞ :¼ xðk,l1Þ + εΣ1rðk,l 1
2Þ,rðk,l + 1
2Þ :¼ rðk,l 1
2Þ + εrlogpðxðk,lÞÞ,
l ¼ 1,…,L; xðkÞ :¼ xðk,LÞ:
(20)
Although the leapfrog integrator does not require second-order quantities
(e.g., Hessian), it is a second-order method. An MH step is appended (where
the ratio of proposal transition is 1 due to volume preservation). When L ¼ 1,
HMC recovers the Langevin dynamics (with ε
2 as the step size), so HMC walks
much farther, making a distant thus less correlated proposal still with a high
MH acceptance rate. See e.g., Neal (2011) and Betancourt (2017) for more
detailed descriptions. The leapfrog integrator can be seen to split the dynam-
ics equation (19) into dx ¼ Σ1r dt, dr ¼ 0 and dx ¼ 0, dr ¼ r log pðxÞ dt,
and alternately simulate each symmetrically, in closed form. This pattern
can be generalized to allow multiple splitting and simulating SDE (Chen
et al., 2015; Hairer et al., 2006), which is called symmetric splitting integrator
(SSI). The discretization error is of a higher order in ε than the Euler–
Maruyama integrator. Interestingly, the interleaving structure for invertibility
is of the same pattern as the coupling layer in flow-based generative models
(Dinh et al., 2017; Kingma and Dhariwal, 2018), and some other works more
directly use the Hamiltonian dynamics for generative modeling (Caterini
et al., 2018; Dockhorn et al., 2021; Toth et al., 2020).
Riemannian HMC is developed also by Girolami and Calderhead (2011).
Hamiltonian dynamics on a Riemannian manifold is given by the coordinate
expression
ðRHMCÞ
dx ¼ GðxÞ1r dt,
dr ¼ r log pRðxÞ dt  1
2 rx r>GðxÞ1r


dt:
(21)
256
Handbook of Statistics

The algorithm redraws momentum from r  N ðrj0,GðxÞÞ, and a generalized
leapfrog integrator is used in simulation. Invertibility and volume preservation
are again guaranteed, but each update step requires solving two equations,
which is done by fixed point iteration. To bypass this costly inner iteration,
Lan et al. (2015) reformulated the dynamics using velocity instead of
momentum, i.e., in the form of Lagrangian dynamics. The new form avoids
one equation, and the rest equation can also be eliminated by an approxima-
tion which slightly violates volume-preservation. Lee and Vempala (2018)
developed nonasymptotic convergence rate of Riemannian HMC and applied
to sampling on polytopes.
3.2.3
Stochastic gradient Hamiltonian dynamics
Assuming large IID data, the stochastic gradient equation (4) can be seen as
the true gradient corrupted by a Gaussian noise with zero mean and some
covariance matrix A(x), due to the central limit theorem. Directly using sto-
chastic gradient in Langevin dynamics equation (11) simulation (Welling
and Teh, 2011) does not introduce much problem. The change in its dis-
cretization xk+1 ¼ xk + εr log pLðxkÞ + N ð0,2εÞ is to add a noise obeying
N ð0, AðxkÞε2Þ to the r.h.s. The variance of this new noise is a higher-order
infinitesimal of that of the originally included noise (Chen et al., 2015).
But it is not the case for HMC which is found fragile to gradient noise
(Betancourt, 2015; Chen et al., 2014), since the original dynamics is determin-
istic. In another way, the dynamics preserves energy so the energy from the
gradient noise gets accumulated all the way, driving the distribution toward
uniformity (Chen et al., 2014). To counteract the noise, Chen et al. (2014)
introduced a friction term into the dynamics that dissipates the accumulated
energy. The resulting stochastic-gradient HMC (SGHMC) dynamics is
known as the second-order (underdamped) Langevin dynamics (Wang and
Uhlenbeck, 1945) in physicsm:
ðSGHMCÞ
dx ¼ Σ1r dt,
dr ¼ r log pðxÞ dt  Cr dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2CΣ
p
dBtðrÞ,
(22)
where C is a constant positive definite matrix controlling the friction inten-
sity, and the Brownian motion dominates over the gradient noise as is in the
Langevin dynamics case. To adaptively adjust C to match the gradient
noise, Ding et al. (2014) proposed stochastic gradient Nose–Hoover thermo-
stats (SGNHT) which introduces a thermostat variable ξ  and composes
the dynamics asn:
mCompared to Eq. (13) in Chen et al. (2014), we replace its M with Σ and its C with CΣ.
nCompared to Eqs. (5,6) in Ding et al. (2014), we allow a variance in the momentum distribution
N ðrj0,ΣÞ and take the diffusion factor A as CΣ.
Geometry in sampling methods Chapter
10 257

ðSGNHTÞ
dx ¼ Σ1r dt,
dr ¼ r log pðxÞ dt  ξr dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2CΣ
p
dBtðrÞ,
dξ ¼
1
m r>Σ1r  1


dt:
(23)
Their extensions to Riemannian manifolds are also developed. Ma et al.
(2015) designed the following SGRHMC dynamics using the complete recipe
(Eq. 8) to extend SGHMC:
ðSGRHMCÞ
dx ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
GðxÞ1
q
r dt,
dr ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
GðxÞ1
q
rlogpLðxÞ dt + r 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
GðxÞ1
q
dtGðxÞ1r dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2GðxÞ1
q
dBtðrÞ:
Although it converges correctly, it does not seem to have a physical or geo-
metric interpretation. Particularly, it is unknown if the dynamics can be
simulated in an embedded space of the manifold, which is required in many
applications as will be discussed next. To tackle this problem, Liu et al.
(2016a) developed the following dynamics:
ðSGGMCÞ
dx ¼ GðxÞ1r dt,
dr ¼ rx logpRðxÞ dt1
2rx r>GðxÞ1r


dtJðxÞ>CJðxÞGðxÞ1r dt
+
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2JðxÞ>CJðxÞ
q
dBtðrÞ,
8
>
>
>
<
>
>
>
:
(24)
where JðxÞ :¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
GðxÞ
p
>. If C commutes with J(x) (e.g., when C is a scalar
matrix), the last two terms become Cr dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2CGðxÞ
p
dBtðrÞ. It recovers
the RHMC dynamics equation (21) when no friction C ¼ 0, and recovers
the SGHMC dynamics equation (22) on a Hilbert space G(x) ≡Σ (note in
which case r log pR ¼ r log pL). Liu et al. (2016a) also proposed a manifold
version of SGNHT:
ðgSGNHTÞ
dx ¼ GðxÞ1r dt,
dr ¼ rx log pRðxÞ dt  1
2 rx r>GðxÞ1r


dt  ξr dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2CGðxÞ
q
dBtðrÞ,
dξ ¼
1
m r>GðxÞ1r  1


dt,
8
>
>
>
>
<
>
>
>
>
:
(25)
which recovers the SGNHT dynamics equation (23) when G(x) ≡Σ.
3.3
Riemannian MCMC in embedded space
The coordinate expressions of these dynamics allow straightforward simulation
in the coordinate space, as if in the usual Euclidean space. However, this may
also be problematic in some cases. When the manifold does not have a global
coordinate system, such as hyperspheres, coordinate simulation requires
258
Handbook of Statistics

changing local coordinate system when the sample is about to move out of the
current one. In addition to this inconvenience, the metric tensor G(x) often goes
to singularity near the boundary of the coordinate system, which may cause
numerical problems. On the other hand, common manifolds are defined as a
subset of a higher-dimensional Euclidean space, which also endows the mani-
fold a Riemannian metric by pulling-back the Euclidean metric via the inclu-
sion map. This naturally gives an isometric embedding of the manifold as a
result (Section 2.7). In the embedded space the manifold is described globally.
Distributions on the manifold are also commonly defined in the embedded
space in the form of the density function pH w.r.t. the Hausdorff measure on
ΞðMÞ, such as the von Mises-Fisher distribution on hyperspheres. It is thus
more attractive to simulate the dynamics in the embedded space.
Brubaker et al. (2012) considered general manifold defined as a subset of
n via a constraint, and generalized HMC on such manifold. Simulation is
basically done using the usual leapfrog process in the embedded space subject
to the constraint for the sample and the induced constraint on the momentum.
This generalized leapfrog is called RATTLE and is still invertible, volume
preserving, and of the second order. But each simulation step requires solving
several nonlinear equations, which is done using Newton’s method in
the work.
Byrne and Girolami (2013) developed a simulation method in the embed-
ded space for HMC, called geodesic Monte Carlo (GMC). The name comes
from the use of explicit geodesic flow solutions for simulation on the embed-
ded manifold, which is available for common manifolds such as the simplex,
hypersphere, and the Stiefel manifold. It avoids the costly inner iteration for
solving nonlinear equations in each update step, in contrast to, e.g.,
Girolami and Calderhead (2011) and Brubaker et al. (2012). Adopting the
leapfrog integrator pattern (or, SSI), GMC splits the Riemannian Hamiltonian
dynamics equation (21) as:
dx ¼ GðxÞ1r dt,
dr ¼  1
2 rx r>GðxÞ1r


dt,
8
<
:
+
dx ¼ 0,
dr ¼ r log pRðxÞ dt:

The first dynamics turns out to be the geodesic equation, which describes the
free motion on the manifold, in analogy to the uniform linear motion in a
Euclidean space. Its solution is just the geodesic flow, and for some common
manifolds there is a closed-form expression, e.g., this is the rotation along the
great circle on the hypersphere n1:
yðtÞ ¼ yð0Þ cos ðαtÞ + ðvð0Þ=αÞ sin ðαtÞ,
vðtÞ ¼ αyð0Þ sin ðαtÞ + vð0Þ cos ðαtÞ,
where y  n1 and v  Tyðn1Þ are the embedded version of x and r, and
α ¼kvð0Þk. For the second dynamics, since x does not change with time, the
solution is rðtÞ ¼ rð0Þ + trlogpRðxÞ, or yðtÞ ¼ yð0Þ + tΛðyð0ÞÞrlogpHðyÞ in
Geometry in sampling methods Chapter
10 259

the embedded space, where Λ(y) is the projection from the tangent space of
n to the tangent space of ΞðMÞ at y. For n1, Λ(y) ¼ In  yy>. For drawing
the momentum variable, sampling r  N ð0, GðxÞÞ in the coordinate space is
equivalent to drawing r  N ð0, InÞ in the isometrically embedded space n
and projecting onto the tangent space using Λ. The final process is to draw
r from N ð0,GðxÞÞ and alternately simulating the two dynamics on an inter-
leaving time scheme, similar to the leapfrog integrator equation (20).
Byrne and Girolami (2013) also showed closed-form expressions of the geo-
desic flow and tangent space projection for the Stiefel manifold, which involves
computationally expensive matrix exponential. Yanush and Kropotov (2019)
then made the computation cheaper by replacing the exact operations with
a retraction operation, which is explored in the field of optimization on
Riemannian manifolds, and is shown to be sufficient to simulate the
dynamics.
Following this spirit, Liu et al. (2016a) developed simulation methods in
the embedded space for their proposed dynamics. The SGGMC dynamics
equation (24) is split as:
dx ¼ GðxÞ1r dt,
dr ¼  1
2 rx r>GðxÞ1r


dt,
8
<
:
+
dx ¼ 0,
dr ¼ JðxÞ>CJðxÞGðxÞ1r dt,

+
dx ¼ 0,
dr ¼ rx log pRðxÞ dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2JðxÞ>CJðxÞ
q
dBtðrÞ,
(
where the first dynamics is the same. The second dynamics has solution r(t) ¼
J>expm(Ct)JG1r(0) or v(t) ¼ Λ expm(Ct)v(0) in the embedded space,
where expm is matrix exponential and the quantities are evaluated at x(0) or
y(0) if not specified. For infinitesimal time interval ε, the solution is approxi-
mately v(ε) ¼ v(0)  εΛCv(0). The third dynamics can be simulated by
vðEÞ ¼ vð0Þ + Λðεr log pH + N ð0,2CεÞÞ. The gSGNHT dynamics equation
(25) can be modified similarly. The two methods show remarkable accuracy
and scalability for the hypersphere inference task of the spherical admixture
model (Reisinger et al., 2010) on large datasets.
4
Particle-based variational inference methods
Although MCMC methods are perhaps the most popular sampling method for
their wide applicability and numerous success in various domains, there are
also complaints. The central drawback is the (auto)correlation among samples
since they are in a Markov chain. It downgrades the effective sample size, and
a long run is required for a reasonable approximation. This can be imagined
that if the target distribution has two equally high separated modes, the
260
Handbook of Statistics

correlation tends to put the next sample in the same mode, so such set of sam-
ples are not as representative as the same amount of IID samples, which dis-
tribute in the two modes roughly equally. A considerable number of
simulation steps are required to get a chance for the sampler to find the other
mode. Besides the cost of the long run itself, evaluating the expectation of a
function also requires quite a lot evaluations on these large set of samples.
Recently there emerged another class of sampling methods that try to
approximate the target distribution using a fixed number of samples, or called
particles. They iteratively update the particles deterministically to minimize
the difference (typically the KL divergence) of the distribution they represent
to the target distribution. This is the spirit of variational inference, hence the
name particle-based variational inference (ParVI). The use of particles is more
flexible thus more accurate than parametric approximations that classical var-
iational inference uses. Since the particles need to approximate the target dis-
tribution jointly as a whole, they communicate/interact with each other, and
appear negatively correlated. So sample efficiency is promoted.
In the following we start with the groundbreaking version, SVGD (Liu and
Wang, 2016). The algorithm is then viewed from a geometric perspective as
approximate simulation to the gradient flow of the KL divergence on the Was-
serstein space and its variants, which can be seen as Riemannian manifolds of
distributions. We then show the consequences of the geometric perspective,
including variant algorithms, approximation techniques, and convergence
analysis.
4.1
Stein variational gradient descent
SVGD (Liu and Wang, 2016) updates the particles fxðiÞg
N
i¼1 using a determin-
istic dynamics dx ¼ Vt(x) dt such that the evolving distribution (qt)t decreases
the KL divergence to the target distribution p. The dynamics is chosen such
that the KL divergence KLpðqtÞ :¼ KLðqt k pÞ :¼ qt½ log ðqt=pÞ is mini-
mized steepest. To derive the expression for Vt(x), one needs to connect the
decreasing rate to the dynamics:
 d
dt KLpðqtÞ ¼ qt½r  ðpVtÞ=p ¼ qt½Vt  r log p + r  Vt:
(26)
The steepest descent is achieved if Vt maximizes Eq. (26). As the objective is
linear in Vt, it can be made arbitrarily large by increasing the norm of Vt. So
the maximization makes sense only if the norm is fixed, i.e., finding the stee-
pest descending direction, and the minimum represents the steepest rate of
decrease which is used as the magnitude in that direction:
Vopt
t
:¼ max  arg max
Vt X,kVtkX¼1
 d
dt KLpðqtÞ:
(27)
Geometry in sampling methods Chapter
10 261

This leads to the next subtlety that which normed space Xshould be used for Vt.
If X is taken as the Hilbert space L2
qtðmÞ :¼ fV : m ! m| kVkL2
qt < ∞g
with inner product hU,ViL2
q :¼
R
UðxÞ  VðxÞqðxÞ dx, we have:
VL2
t
¼ r log ðp=qtÞ:
(28)
But this result is not easily estimable since it requires r log qt while we only
have samples/particles of qt. Liu and Wang (2016) then used X ¼ Hm, where
H is the reproducing kernel Hilbert space (RKHS) of a kernel function K :
m  m ! (Steinwart and Christmann, 2008, Ch. 4), which can be seen
as the linear space of functions
P
lαlKð  , xðlÞÞ
	

(summation over finite
set or countably infinite set in the sense of pointwise convergence) with inner
product
P
lαlKð  , xðlÞÞ, P
l0βl0Kð  , yðl0ÞÞ


H :¼ P
l,l0 αlβl0KðxðlÞ, yðl0ÞÞ . It is
named after its reproducing property hfð  Þ,Kð  ,xÞiH ¼ fðxÞ, 8f  H and that
it is a Hilbert space. The solution is then:
VSVGD
t
ðxÞ :¼ qtðx0Þ½Kðx, x0Þrx0 log pðx0Þ + rx0Kðx, x0Þ:
(29)
The expression depends on qt only in terms of expectation, which can be esti-
mated by averaging over the particles. The particles are then updated by simu-
lating the dynamics:
xðiÞ
k+1 ¼ xðiÞ
k + ε 1
N
X
N
j¼1
KðijÞ
k r log p xðjÞ
k


+ rxðjÞ
k KðijÞ
k


,
(30)
where KðijÞ
k
:¼ K xðiÞ
k , xðjÞ
k


. Since K(ij) increases as x(i) and x(j) move closer,
the gradient rxðjÞKðijÞ points toward x(i) at x(j), and points away from x(j) at
x(i). So the second term in the update pushes x(i) away from all other particles,
which presents a repulsion force and incurs a negative correlation among the
particles. Another attractive property is that it degenerates to gradient descent tar-
geting the mode of p(x) if there is only one particle, since rx0K(x, x0)jx0¼x ¼ 0, so
it naturally transits to the maximum a posteriori (MAP) estimate.
4.2
The Wasserstein space
As seen above, the derivation of SVGD involves geometric considerations
such as steepest descending direction, which is very much alike a gradient.
Can this geometric perspective be formalized so that Eq. (27) is the gradient
of KLp on some space of distributions? The Wasserstein space gives an
elegant answer.
Particularly we consider the 2-Wasserstein space P2ðMÞ on a complete
metric space M (¼ m for common ParVIs), defined as the set of all (unnec-
essarily absolutely continuous) distributions on M with finite variance:
9x0  M s:t: q½dMðx, x0Þ2 < ∞. It is usually equipped with the 2-Wasserstein
262
Handbook of Statistics

distance and seen as a metric space (Villani, 2008, Def. 6.4; Ambrosio et al.,
2008, Ch. 7), which explains the name. The 2-Wasserstein distance (Villani,
2008, Def. 6.1; Ambrosio et al., 2008, Eq. (7.1.1)) between two distributions
p,q  P2ðMÞ arises from the optimal transport problem, and is defined as the
minimal cost for transporting mass on M distributed as p(x) to distribute as
q(x0) by all possible (may be stochastic) transport plans π(x0jx):
dP2ðp,qÞ :¼
inf
πðx0jxÞ: R
Mπðx0jxÞ dpðxÞ¼qðx0Þ
Z
MM
dMðx,x0Þ2 d πðx0jxÞpðxÞ
ð
Þ
0
@
1
A
1=2
:
(31)
We first note that quite a range of geometric structures can be extended
to a general metric space (Ambrosio et al., 2008, Part I). For future refer-
ence, we mention a particular example of the definition of the gradient flow
(qt)t of a function F on P2ðMÞ as a metric space in terms of the minimizing
movement scheme (Ambrosio et al., 2008, Def. 2.0.6): (qt)t is the limiting
curve when τ ! 0 of the piecewise constant curve (Ambrosio et al., 2008,
Def. 2.0.2):
qt+ε :¼ argmin
q P2ðMÞ
1
2τ d2
P2ðq,qtÞ + FðqÞ,
8ε  ð0,τ:
(32)
It extends the implicit Euler discretization.o Nevertheless, it is more attractive
to consider the space as a Riemannian manifold (if possible) for explicit cal-
culation and simulation. This is possible for P2ðMÞ. We start with character-
izing tangent vectors on it. See Fig. 8 for an illustration.
Similar to the process in defining tangent vector for a general manifold in
Section 2.2, consider a curve (qt)t on P2 which is an evolving distribution on
M. As mentioned in Section 3.1, a dynamics induces an evolving distribution,
and the connection is given by the FPE (6). If we only consider deterministic
dynamics (D ≡0), the FPE becomes what is commonly called the continuity
oOther examples of concepts mentioned in Ambrosio et al. (2008) for a general complete metric
space: the speed (metric derivative) at a point on a curve (Thm. 1.1.2), curve absolute continuity
(Def. 1.1.1) and length (Lem. 1.1.4), geodesic (Def. 2.4.2), gradient of a function in terms of
norm/modulus (strong (Def. 1.2.1) and weak (Def. 1.2.2) upper gradient; local (also Villani,
2008, Prop. 23.1(ii)), and global slopes (Def. 1.2.4) are weak, and strong (for lower semicontin-
uous functions) upper gradients (Thm. 1.2.5); relaxed slope (Section 2.3) is the local slope under
some conditions (Rem. 2.3.2) ), and gradient flow [curve of maximal slope (Def. 1.3.2) in terms of
the evolution variational inequality, which indicates energy identity (also Villani, 2008, Prop.23.1
(ii)) in some cases (Rem. 1.3.3); (generalized) minimizing movement scheme (Def. 2.0.6), whose
convergence (Cor. 2.2.2) and connection to curve of maximal slope (Thm. 2.3.1) and energy identity
(Thm. 2.3.3) are made; curve with dispersion from geodesic bounded by directional derivative along
the geodesic (Villani, 2008, Prop. 23.1(iv), Def. 23.7); they all coincide with the Riemannian
gradient flow (Villani, 2008, Prop.23.1, Rem. 23.4; Ambrosio et al., 2008, Thm. 11.1.6)].
Geometry in sampling methods Chapter
10 263

equation or the conservation-of-mass formula, which takes the following form
on manifold M:
∂tqt ¼ divðqtVtÞ,
(33)
where qt is the density under the Riemannian measure [(Villani, 2008, p. 28);
see e.g., Liu and Zhu (2018, Appx. A1) for deriving the manifold extension].
The conclusion equation (33) can be extended to distributions that do not have
a density (i.e., not absolutely continuous) under the Riemannian measure in
the sense of weak derivatives (or called in the sense of distributions)
(Ambrosio et al., 2008, Eq. (8.1.3), Rem. 8.1.1):

Z

Z
M
∂tftðxÞ dqtðxÞdt ¼
Z

Z
M
hgrad ftðxÞ,VtðxÞiTxM dqtðxÞdt, 8f  C∞
c ðMÞ:
(Note Eq. 3 for understanding this form of Eq. 33.) Conversely, it is shown
(Ambrosio et al., 2008, Thm. 8.3.1; Villani, 2008, Thm. 13.8; Erbar et al.,
2010, Prop. 2.5) that for any curve (qt)t on P2ðMÞ, at point qt there exists a
vector field Vt on M such that the continuity equation (33) holds in the weak
sense, and the unique existence (up to a set of dqt(x)dt-measure zero) is
attained in a subspace of vector fields,
FIG. 8
Illustration of the support-space vector-field representation of a Wasserstein tangent vec-
tor. Similar to the illustration in Fig. 7, when particles fxðiÞ
t gi distribute obeying qt at time t and
move under a deterministic dynamics dxt ¼ Vt(xt) dt (bottom two red arrows), i.e., each moving
with velocity VtðxðiÞ
t Þ, they form a new configuration at time t + ε which defines a new distribution
qt+ε. As ε ! 0, the dynamics induces a continuously evolving distribution (qt)t, which is a curve
on the Wasserstein space P2ðMÞ. The tangent vector along the curve at qt is the instantaneous
evolution of the distribution at t, which is given by the continuity equation (33) (or, the FPE
(6) without diffusion), which is in turn determined by Vt. So the vector field Vt represents a
Wasserstein tangent vector (the red arrow at the top).
264
Handbook of Statistics

fgrad φ|φ  C∞
c ðMÞg
L2
qtðMÞ,
(34)
where the overline means the closure as a subset of the Hilbert space L2
qtðMÞ.p,q
This correspondence suggests using a vector field on M to represent a tangent
vector on P2ðMÞ, and using the above subspace of vector fields equation (34)
as the tangent space TqtP2ðMÞ (Ambrosio et al., 2008, Def. 8.4.1, Prop. 8.4.5;
Erbar et al., 2010, Def. 2.3).
What is particularly insightful of this correspondence is that the vector
field Vt defines a dynamics on M which determines how the Wasserstein
curve (qt)t moves along the corresponding Wasserstein tangent vector as an
evolving distribution. So we can carry out simulation of the Wasserstein curve
by simulating the dynamics on samples/particles in M. Specifically,
Exp(εVt())#[qt] is a first-order approximation of qt+ε in terms of the Wasser-
stein distance dP2 (Ambrosio et al., 2008, Prop. 8.4.6), where Exp  ðεVtð  ÞÞ :
M ! M, x 7! ExpxðεVtðxÞÞ is the exponential map on the support space M
as a function of position, and ϕ#[q] is the pushed-forward distribution of
q by measurable map ϕ : M ! M , defined by ϕ#½qðIÞ :¼ qðϕ1ðIÞÞ for
any measurable subset I  M (Billingsley, 2012, p. 196). More concretely,
this means that if {x(i)} is a set of particles of qt, then fExpxðiÞðεVtðxðiÞÞÞg is
approximately a set of particles of qt+ε.
For a Riemannian structure, a natural inner product in the tangent space
TqtP2ðMÞ is the one inherited from L2
qtðMÞ. What makes a coincidence is
that the induced distance in the Riemannian sense (Eq. 2) is exactly the
2-Wasserstein distance (Eq. 31), which is revealed by the Benamou–Brenier
formula (Benamou and Brenier, 2000; Otto, 2001; Villani, 2008, Ch. 15).
So this Riemannian structure can be seen as a finer characterization of the
Wasserstein space as a metric space. Various geometric objects can be then
induced. Of particular interest is the gradient of a function F on P2ðMÞ.
The general formulation is (Villani, 2008, Ex. 15.10; Ambrosio et al., 2008,
Lem. 10.4.1):
pFor this conclusion, Ambrosio et al. (2008, Thm. 8.3.1, Def. 5.1.11) and Erbar et al. (2010,
Prop. 2.5) require the curve (qt)t to be locally absolutely continuous in a metric-space sense
(Ambrosio et al., 2008, Def. 1.1.1; Erbar et al., 2010, Def. 2.2), while Villani (2008, Thm.
13.8) further restricts (qt)t to be Lipschitz-continuous which enlarges the subspace by allowing
φ  C1
cðMÞ.
qThe space defined in Eq. (34) can be seen as the quotient space of L2
qtðMÞ under the equivalent
relation U  V : div(qtU) ¼ div(qtV ), i.e., they induce the same evolving distribution via the
continuity equation (33). More precisely, the space (34) is the orthogonal complement of
fV  L2
qtðMÞ j divðqtVÞ ¼ 0g (Erbar et al., 2010, Lem. 2.4), as can be seen from Eq. (3):
divðqtVÞ ¼ 0 , R
Mφ divðqtVÞ dωg ¼ R
Mhgrad φ, ViTxMqt dωg ¼ hgrad φ, ViL2
qt ðMÞ ¼
0, 8φ  C∞
c ðMÞ. This space (34) is also equivalent to the set of vector fields that achieve a certain
evolving distribution with the minimum L2
qtðMÞ norm: fargminV  L2
qt ðMÞ,divðqtVÞ¼f k VkL2
qt ðMÞ
j f  L1
ωgðMÞ s:t:
R
Mf dωg ¼ 0g (Erbar et al., 2010, Lem. 2.4).
Geometry in sampling methods Chapter
10 265

grad FðqÞ ¼ grad δF
δq ðqÞ,
(35)
where the function δF
δq ðqÞ on M is the variation of F at q as a functional of
distribution q, characterized by
d
dt FðqtÞ ¼
R
M
δF
δq ðqtÞ d∂tqt in the sense of
weak derivative for any evolving distribution (qt)t. Note that the l.h.s. is a gra-
dient vector at q on the Wasserstein space P2ðMÞ, while the r.h.s is a gradient
function as a vector field on the supporting manifold M. Particularly for the
KL divergence, its gradient is given by (Villani, 2008, Formula 15.2, Thm.
23.18)r:
grad KLpðqÞ ¼ grad log ðq=pÞ:
(36)
Note that this coincides with VL2
t
(Eq. 28) on Euclidean spaces. For an anal-
ogy to the Euclidean case, the gradient flow converges exponentially if the
Wasserstein function is geodesically strongly convex on the Wasserstein space
(Villani, 2008, Thm. 23.25, Thm. 24.7; Ambrosio et al., 2008, Ex. 11.1.2). For
the KL divergence, this requires log p to be strongly log-concave for M ¼ m
(Ambrosio et al., 2008, Def. 9.4.9, Lem. 9.4.7) and a similar but involved
requirement for a general manifold M (Villani, 2008, Thm. 17.15). Gradient
flow can then be defined and simulated in the way described in the previous
paragraph.
As a literature remark, Villani (2008) also made detailed description on the
optimal transport problem, dedicated analysis on the curvature of P2ðMÞ
induced from that of the base manifold M, and on the convexity of functions
on P2ðMÞ. Ambrosio et al. (2008) extended and formalized the concept of gra-
dient flow and other geometric objects for general metric spaces. For the
Wasserstein space, they only considered the Euclidean support but made anal-
ysis in parallel on the p-Wasserstein space PpðmÞ for p 6¼ 2, which is no longer
a Riemannian manifold but a metric space. Erbar et al. (2010) and Santambrogio
(2017) gave concise summaries of the two books. Lott (2008) presents explicit
calculations of more Riemannian manifold objects for P2ðMÞ.
In the spirit of information geometry (see Fig. 1), Chen and Li (2018) and
Wang and Li (2022) also considered using the Wasserstein distance (resp. KL
divergence) to measure the infinitesimal difference between two likelihood
distributions, and induced a parametric Wasserstein metric (resp. Fisher–
Rao metric). More variants are also considered (e.g., the Stein geometry to
be introduced in Section 4.3.2).
4.3
Geometric view of particle-based variational inference methods
As mentioned in Section 4.1, the development of SVGD resembles the pro-
cess of defining the gradient as a steepest ascending direction on an abstract
rTo make the expression well defined, q needs to be absolutely continuous w.r.t. p.
266
Handbook of Statistics

space. We now review some formal descriptions of this intuition from the
view on the Wasserstein space and a variant space. In this section we restrict
our attention to the Euclidean case M ¼ m.
4.3.1
View from the Wasserstein space
As seen from Section 4.2, theory on the Wasserstein space formalizes the
intuition of the gradient of a Wasserstein function, and links it to a particle
dynamics, so it is a natural choice (Chen et al., 2018a; Liu et al., 2019a).
Indeed, as mentioned right below Eq. (27), if the space X is taken as
L2
qtðmÞ, the optimal vector field VL2
t
is just grad KLp by Eq. (36). To relate
it to the SVGD vector field equation (29), Liu et al. (2019a, Thm. 2) gave the
following relation:
VSVGD
t
¼ max  argmax
Vt Hm,kVtkHm¼1
hgrad KLpðqtÞ,VtiL2
qtðmÞ,
which suggests VSVGD
t
is the projection of the Wasserstein gradient of KLp
onto the vector-valued RKHS Hm.
To further understand this approximation, note that if the space of optimiza-
tion is taken as L2
qtðmÞ, then grad KLp is recovered. For a Gaussian kernel,
the replacement from L2
qtðmÞ to Hm is roughly doing a kernel smoothing:
Hm is isometrically isomorphic to fϕ  K j ϕ  L2
qtg
L2
qt (Liu et al., 2019a,
Thm. 3), where ðϕ  KÞðxÞ :¼
R
mϕðx0ÞKðx, x0Þ dx0 is the convolution of ϕ with
the kernel K. Furthermore, for each individual ϕ, this kernel smoothing
reduces its “sharpness”: elementwise, ϕi * K is
ﬃﬃ
2
π
q
B
σ -Lipschitz, where B :¼
supx  mjϕiðxÞj bounds the original function ϕi, and σ is the bandwidth of the
Gaussian kernel K (He et al., 2022, Thm. 2). This smoothing operation
over vector fields is shown equivalent to smoothing the density itself,
i.e., using ~qK :¼ q  K in place of q so that the empirical distribution ^qðxÞ :¼
1
N
PN
i¼1δxðiÞðxÞ can be plugged in for q, where δxðiÞðxÞ denotes the Dirac delta
measure that puts all the probability mass only at the point x(i). The smoothing
operation (in either way) is regarded mandatory for a well-defined vector field
(Liu et al., 2019a), as KLp(q) is defined as infinity if q is not absolutely continu-
ous w.r.t. p, which is the case when q ¼ ^q.
4.3.2
View from the Stein geometry
Under the view of Wasserstein space, the kernel enters only as a smoothing
approximation to geometric objects. This may be insufficient to analyze the
SVGD dynamics precisely, and a geometric structure involving the kernel is
expected. Liu (2017) made an attempt where the tangent space, i.e., a space
Geometry in sampling methods Chapter
10 267

of vector fields, is taken as the vector-valued RKHS Hm with the same inner
product. Under this choice, the gradient of KLp recovers the SVGD vector
field equation (29) indeed. Nevertheless, this is only a formulation. For a
complete geometric development, it remains to show the manifold as a set
and the one-to-one correspondence between an element in the defined tangent
space and a tangent vector of a curve on the manifold. In this spirit, the theory
is improved by Duncan et al. (2019), who called it Stein geometry.s
Specifically, the manifold as a set is taken as the collection of fully sup-
ported, absolutely continuous distributions that make K(x, x) have finite
expectation, denoted PK. In contrast to using the entire Hm, they refine the
tangent space at q  PK as (Duncan et al., 2019, Def. 5):
TqPK :¼ fKqrφ|φ  C∞
c ðmÞg
Hm
,
where
Kq : L2
qðmÞ ! H,
ðKq fÞðxÞ :¼
Z
mKðx, x0Þfðx0Þqðx0Þ dx0
is the integral operator of kernel K and is applied elementwise if the operand
is a vector.t This is similar to Eq. (34) except that kernel smoothing is applied
to each vector field, so it guarantees a unique representation of a tangent vec-
tor of PK (Duncan et al., 2019, Lem. 7(2)) (see Footnote 4.2). Under the same
inner product of Hm, the gradient of a function F on PK is (Duncan et al.,
2019, Lem. 9) gradPKFðqÞ ¼ Kqr δF
δq ðqÞ,
which is the kernel-smoothed
Wasserstein gradient equation (35). This leads to the SVGD vector field
gradPKKLpðqtÞ ¼ VSVGD
t
,
(37)
since VSVGD
t
¼ KqtVL2
t
¼ Kqtgrad KLpðqtÞ by definition (Eq. 29) (and inte-
gration by parts). More geometric structures such as the corresponding dis-
tance (Duncan et al., 2019, Def. 13) and geodesic (Duncan et al., 2019,
Prop. 18) are developed. Note that if K(x, x0) ¼ δx(x0)/q(x0), then Kq is the
identity operator, and Stein geometry reduces to the Wasserstein geometry.
sNevertheless, this name may not well reflect the exact characteristics of the theory. Back to the
origin (Liu and Wang, 2016), the label “Stein” is introduced via the connection of the objective in
Eq. (27) to Stein’s method (Stein, 1972) for constructing probability metric (Stein discrepancy
(Gorham and Mackey, 2015)), where Sp : V 7!  r  ðpVÞ=p
is called Stein operator. It
describes how the distribution evolves and is also used in the Wasserstein space case. What is spe-
cial in the new geometric structure is the incorporation of kernels. So it may be better called
“kernel-smoothed geometry” or “kernelized-Stein geometry.”
tDuncan et al. (2019) originally defined the tangent space as the corresponding space of ∂tqt via
the continuity equation (33), i.e., applying V 7! r  (qV ) to the definition here. The results
of the two definitions are isometrically isomorphic thus both are Hilbert spaces (Duncan et al.,
2019, Lem. 7).
268
Handbook of Statistics

Duncan et al. (2019) also introduced some concepts and results that are
particularly useful for analyzing the SVGD dynamics. They defined the Hes-
sian of KLp (Lem. 22), local geodesic strong convexity of KLp in terms of the
Hessian (Lem. 25), and the consequent exponential convergence of KLp near
the optimal solution p (Thm. 20). Furthermore, they found an equivalent cri-
terion for the strong convexity of KLp near p with strength λ > 0, called the
Stein–Poincare inequality (Lem. 32):
hφ,Ap,KφiL2pðmÞ 
 λhφ,φiL2pðmÞ, 8φ  C∞
c ðmÞ s:t:
Z
mφ dp ¼ 0,
where
Ap,Kφ :¼ r  ðpKprφÞ=p, 8φ  C∞
c ðmÞ
(38)
is the kernelized Barbour’s generator of the Stein gradient flow of KLp, which
relates to the kernelized Stein operator by Ap,Kφ ¼ Sp,Krφ.u This criterion
makes convexity identification easier, since Ap,K is a self-adjoint and positive
semidefinite operator on L2
qðmÞv which leads to the linear algebra problem of
spectral gap.
The convexity can also be described in other ways. Recall that a nice prop-
erty of a convex function is the exponential convergence along its gradient
flow. For KLp, we can impose a simple condition to achieve exponential con-
vergence: KLpðqtÞ 	  1
2λ
d
dt KLpðqtÞ, where (qt)t is the gradient flow of KLp
on PK . Substituting Eq. (37) into Eq. (26), we get the Stein-log-Sobolev
inequality:
KLpðqÞ 	 1
2λ Ip,KðqÞ,
where Ip,KðqÞ :¼ p½rðq=pÞ  Kprðq=pÞ ¼kKpr log ðq=pÞk2
H
(39)
is again a kernelized object called Stein–Fisher information. In fact, it is a
probability metric constructed using Stein’s method, called the kernel Stein
discrepancy (after square-rooted) (Chwialkowski et al., 2016; Gorham and
Mackey, 2017; Liu et al., 2016b), which generalizes the Fisher divergence.
Stein-log-Sobolev inequality is a stronger condition than Stein–Poincare
inequality (Duncan et al., 2019, Eq. (61)). Note that this line of developing
a convexity theory on a probability manifold is in parallel to that of Villani
(2008) for the Wasserstein space P2ðMÞ (Hessian formula (15.7), convexity
uLikewise, the vanilla Barbour’s generator (Barbour, 1990) of the Wasserstein gradient flow of
KLp (which is the overdamped Langevin dynamics) is Apφ :¼ r  ðprφÞ=p, which holds the
same relation to the Stein operator SpðVÞ :¼ r  ðpVÞ=p (see also Footnote s).
vThe domain in the definition (38) can be extended to L2
qðmÞ in the sense of weak derivative.
Also note C∞
c ðmÞ
L2
qðmÞ ¼ L2
qðmÞ if q is absolutely continuous (Kova´cik and Ra´kosnı´k, 1991,
Thm. 2.11).
Geometry in sampling methods Chapter
10 269

(Def. 16.5), Poincare inequality (Def. 21.17), log-Sobolev inequality (Def.
21.1) and the sufficiency for the former (Thm. 22.17)). The Wasserstein
theory can be recovered by choosing K(x, x0) ¼ δx(x0)/q(x0).
This kernelized extension is more suitable for analyzing SVGD. Korba
et al. (2020) made a nonasymptotic analysis. In the infinite-particle regime,
the difference of KLp between two adjacent updates is bounded by Ip,K at
the former update (Prop. 5; also Liu, 2017, 3.3) which serves as a descent
lemma in optimization theory, and the cumulative moving average of Ip,K
converges to zero inversely linearly (Cor. 6). For bounded log p and Lipschitz
kernel with Lipschitz gradients, they also found a bound on the expected
squared Wasserstein distance between the empirical distribution of finite
particles and the true particle distribution, which decays inversely to the
square-root of the number of particles. Chewi et al. (2020) reformulated
VSVGD
t
¼ Kqtr log ðqt=pÞ as Kqtððp=qtÞrðqt=pÞÞ ¼ Kprðqt=pÞ , which
is
the
Wasserstein
gradient
of
the
chi-squared
divergence
χ2
pðqtÞ :¼
R
ðq2
t =pÞ dx  1 by Eq. (35) (up to a factor of 2). The integral operator Kp does
not depend on time anymore which inspires a characterization for a desired
kernel and a new ParVI method. They also proved exponential convergence of
KLp along the (unkernelized) χ2
p gradient flow given Poincare inequality, and
the inverse quadratic, or exponential convergence of χ2
p itself given Poincare
inequality, or the log-Sobolev inequality (implied by log-concavity of p).
4.4
Geometric view of MCMC dynamics and relation to ParVI
methods
As an interlude, one may also wonder if a similar geometric view for MCMC
dynamics is possible. As MCMC dynamics do not need kernel smoothing, the
Wasserstein-space view is more relevant.
4.4.1
Langevin dynamics
To begin with, it was found decades ago by Jordan et al. (1998) that the
Langevin dynamics equation (11) induces evolving distributions that are the
gradient flow of KLp on the Wasserstein space P2ðmÞ. The argument was
made where the Wasserstein gradient flow is defined in the metric-space
sense of the minimizing movement scheme (Eq. 32) (so it is also called the
JKO scheme following the authors’ initials). With the Riemannian structure
introduced in Section 4.2, this can be seen more directly: the dynamics
dxt ¼ r log ðp=qtÞ dt (in the sense of weak derivative if p or qt as a distribu-
tion is not absolutely continuous) defined by the Wasserstein gradient of KLp
(Eqs. 36 and 28) and the Langevin dynamics equation (11) induce the same
evolving distribution due to FPE (6).
270
Handbook of Statistics

This coincidence also holds for their Riemannian versions, dxt ¼
G1r log ðpL=qLtÞ dt
in coordinate space from the gradient-flow side
(Eq. 36), and Eq. (14) in coordinate space from the Riemannian Langevin
dynamics side (Eq. 12).w Alternatively, we can assume this coincidence in
the first place, and derive the coordinate expression of the Riemannian
Brownian motion via FPE (6):
ﬃﬃﬃ
2
p
d~Bt ¼ r  ð
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
G1Þ=
ﬃﬃﬃﬃﬃﬃﬃ
jGj
p
dt +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2G1
p
dBt,
which bridges Eq. (12) to Eq. (13) or Eq. (14). This could serve as an alterna-
tive way to generalize Brownian motion to Riemannian manifolds, in addition
to using the generalization of the heat equation as introduced in Section 3.2
(Eq. 14).
Compared to the general form of MCMC dynamics equation (8), the
Wasserstein gradient flow of KLp, i.e., the Riemannian Langevin dynamics
equation (14), covers the case when Q ¼ 0 and D is nondegenerate.
4.4.2
Hamiltonian dynamics
Another instance that has a geometric interpretation is the Hamiltonian
dynamics used in HMC. Since it is a classical physical process, its analysis
has a longer history. Typically it is interpreted as the Hamiltonian flow/vector
field of a function called Hamiltonian, which is log pðx,rÞ in this context, on a
symplectic manifold,x or more commonly on the cotangent bundle T*B of a
manifold B with its canonical symplectic structure (Betancourt et al., 2017).y
A key feature of the Hamiltonian flow is that it keeps Hamiltonian constant.
wThe dynamics expressions in coordinate space all use densities w.r.t. the Lebesgue measure per
requirement of the FPE (6).
xA symplectic structure is a nondegenerate (ςx 6¼ 0, 8x  M ) and closed (dς ¼ 0) 2-form
(Section 2.3) on the manifold M. The Hamiltonian vector field Xf of a function f, which is called
the
Hamiltonian,
is
characterized
by
ςðXf ,VÞ ¼ V½f, 8V  T ðMÞ.
Such
Xf
is
unique:
ςðXf X0
f ,VÞ ¼ ςðXf ,VÞςðX0
f ,VÞ ¼ V½fV½f ¼ 0 indicates Xf X0
f ¼ 0 since ς is nondegener-
ate. It keeps Hamiltonian invariant: Xf[f] ¼ ς(Xf, Xf) which is 0 since ς is antisymmetric. Its coor-
dinate expression is ς ¼ P
1	i<j	m ςij dxi ^ dxj, where the m  m matrix (ςij) is antisymmetric.
Note that det½ðςijÞ ¼ det½ðςijÞ> ¼ det½ðςijÞ ¼ ð1Þmdet½ðςijÞ, so the nondegeneracy requires m
to be even.
yLet fxigm
i¼1 be a coordinate system of B. Recalling Section 2.3, at each point x  B, fdxigm
i¼1 is a
basis of T*xB, and any 1-form χ has a coordinate expression χ ¼ ri dxi. So fxi,rigm
i¼1 is a coordinate
system of T*B. The canonical symplectic structure is then defined as ς ¼ Pm
i¼1dxi ^ dri, which
can be shown to be coordinate independent. See e.g., Da Silva (2001) for more information. Under
this construction, the coordinate expression of ς is Sm defined below.
Geometry in sampling methods Chapter
10 271

Similar to the Riemannian structure case, a symplectic structure can be
constructed to the Wasserstein space P2ðMÞ using that of M (Ambrosio
and Gangbo, 2008; Gangbo et al., 2010, Ch. 6). For the cotangent bundle
T*B of a base Riemannian manifold B with its canonical symplectic structure,
the resulting Hamiltonian vector field of a function H on the Wasserstein
space P2ðT*BÞ is:
XFðqÞ ¼ πq Smgradx,r
δH
δq


,
where Sm :¼
0 Im
Im
0


, and πq is the projection from L2
qðT*BÞ to TqP2ðTBÞ.z
Particularly for KLp where pðx,rÞ :¼ pðxÞN ðrj0,ΣÞ in the Euclidean case, this
Hamiltonian vector field recovers the Hamiltonian dynamics equation (19).aa
This corresponds to the general form of MCMC dynamics equation (8)
when D ¼ 0 and Q ¼ S. Although a more general Q can be interpreted as a
general symplectic structure, a symplectic structure always requires the
dimension of the manifold to be even, following the nondegeneracy require-
ment on its 2-form representation (see Footnote x).
4.4.3
General MCMC dynamics
To enclose the gap to the general form of MCMC dynamics equation (8), new
geometric constructions are expected. For this, Liu et al. (2019b) introduced
the so-called fiber-Riemannian Poisson (fRP) structure that generalizes the
Riemannian and symplectic structures, and the fiber-gradient Hamiltonian
(fGH) flow that generalizes the gradient and Hamiltonian flows.
4.4.3.1
Fiber-Riemannian structure and fiber-gradient flow
For a positive semidefinite D, it can be transformed to DðxÞ ¼
CðxÞ
0
0
0


by choosing a proper coordinate system ðx1,…,xmÞ ¼ ðy1,…,y‘,z1,…,zm‘Þ,
where the ‘  ‘ matrix C(x) is everywhere positive definite (thus nondegener-
ate). The manifold M can then be split into two parts: the space F with coor-
dinates ðy1,…,y‘Þ that correspond to the matrix C, and the space M0 with the
rest coordinates ðz1,…,zm‘Þ. On the first part, C(x) ¼ C(y, z) can be treated
as the inverse Riemannian metric G1(yjz) on that space, which matches the
Wasserstein gradient flow. But such a metric in general also depends on the
position z in the second part M0 , which is out of the F space itself. To
zThat is, subtracting a vector field V that satisfies div(qV ) ¼ 0 to make the vector field achieve
minimal L2
q norm.
aaUp to a minus sign; note that the additional part V ¼
rr log q
rx log q


has r (qV ) ¼ 0.
272
Handbook of Statistics

describe such dependency, the construct of fiber bundle is introduced, which
is roughly a generalization of the product space M ¼ F  M0, where both
factor spaces F and M0 can be manifolds, and at each point of the second
space z  M0, the first space is allowed to have a z-specific structure (hence
denoted F z) while is still diffeomorphic to F. The above G1(yjz) can then be
interpreted to define a Riemannian structure on the corresponding fiber
space F z. This partial Riemannian structure defined by D(x) is thus called a
fiber-Riemannian structure by Liu et al. (2019b). Correspondingly, the gradi-
ent now can only be defined on each fiber space, and the union of gradients
over all fibers F z,8z  M0 is defined as a fiber gradient:
gradfib fðxÞ ¼ DijðxÞ∂j fðxÞ∂i:
(40)
However, it is hard to develop a fiber-Riemannian structure on the
Wasserstein space P2ðMÞ, so Liu et al. (2019b) turn to consider P

2ðMÞ :¼
fqð  jzÞ  P2ðF zÞ|z  M0g. It naturally has a fiber-Riemannian structure.
The fiber gradient of KLp then recovers the first part of the MCMC dynamics
equation (10) exactly:
πqðgradfibKLpðqÞÞ ¼ Dr log ðp=qÞ:
(41)
4.4.3.2
Poisson structure and Hamiltonian flow
For a general antisymmetric Q, it can be viewed as a Poisson structure on the
manifold M. A Poisson structure refers to a Poisson bracket {, }, which has
its origin from classical mechanics describing the evolution of a mechanical
quantity. Formally, {, } is a Lie bracket on C∞ðMÞ, i.e., an antisymmetric
bilinear C∞ðMÞ  C∞ðMÞ ! C∞ðMÞ map with Jacobi identity {f, {g, h}} +
{g, {h, f}} + {h, {f, g}} ¼ 0, that satisfies the Leibniz rule {f, gh} ¼ g{f, h} +
{f, g}h. Since the Poisson bracket is linear and has a differentiation behavior, it
can be linearly represented using tangent-vector basis for any given coordinate
system: {f, h} ¼ {xi, xj}∂if∂jh ¼ P
1	i<j	m{xi, xj}(∂if∂jh  ∂jf∂ih) ¼ P
1	i<j	m
{xi, xj}(∂i  ∂j  ∂j  ∂i)(df  dh) ¼ P
1	i<j	m{xi, xj}(∂i^∂j)(df  dh). This sug-
gests another representation of the Poisson structure as a bivector (or 2-vector)
fieldab β(x) ¼ P
1	i<j	mβij(x)∂i ^ ∂j, where (βij)(x) is everywhere an antisymmet-
ric matrix (dual to the symplectic structure; see Footnote x) and satisfies the
corresponding differential Jacobi identity βil∂lβjk + βjl∂lβki + βkl∂lβij ¼ 0, 8i, j, k.
abFormally, dual to a 2-form (Section 2.3), a bivector field is everywhere an antisymmetric
bilinear function on ðT
xMÞ2 (where df  dh lives), or alternatively a combination of the wedge
products (antisymmetrized tensor product) of vector pairs (hence the name).
Geometry in sampling methods Chapter
10 273

The correspondence between the two representations is given by {f, h} ¼ β(df, dh).
As both the Poisson structure (a 2-vector) and the symplectic structure (a 2-form)
bear the antisymmetric bilinearity, they have a one-to-one correspondence via the
linear space duality if the Poisson structure is nondegenerate (see Footnote x;
Fernandes and Marcut, 2014, Lem. 1.28). So a Poisson structure covers more than
a symplectic structure, as it does not have to be nondegenerate; particularly, it
allows an odd-dimensional manifold.
Similar to the case on a symplectic manifold, a Hamiltonian flow can also
be defined under a Poisson structure. The corresponding Hamiltonian vector
field is defined as Xh() :¼ {, h}, which keeps the Hamiltonian invariant
Xh(h) ¼ 0, as expected. It holds the following coordinate expression from
definition:
XhðxÞ ¼ βijðxÞ∂jhðxÞ∂i:
(42)
A Poisson structure f  ,  gP2ðMÞ on the Wasserstein space P2ðMÞ can
also be developed using that f  ,  gM of M (Lott, 2008, Sec. 6; Gangbo
et al., 2010, Sec. 7.2). For functions F(q), H(q) on P2ðMÞ, define their Pois-
son bracket as fF,HgP2ðMÞðqÞ :¼
R
M
δF
δq ðqÞ, δH
δq ðqÞ


M
dq (note δF
δqðqÞ and
δH
δq ðqÞ are functions on M). The corresponding Hamiltonian flow on the
Wasserstein space P2ðMÞ is given per point q as XHðqÞ ¼ πqðXδH
δqðqÞÞ, where
the projection πq is used to map the dynamics XδH
δqðqÞ to the Wasserstein tan-
gent space TqP2ðMÞ (Eq. 34) while achieving the same evolving distribu-
tion. Particularly, for KLp on P2ðMÞ, the Hamiltonian flow is (Liu et al.,
2019b, Lem. 2):
XKLpðqÞ ¼ πqðβij∂j log ðq=pÞ∂iÞ:
(43)
This form exactly matches the rest part of their reformulation of a general
MCMC dynamics equation (10) if (βij(x)) is taken as Q(x) (Liu et al.,
2019b, Thm. 5).ac
4.4.3.3
fRP structure and fGH flow
To wrap up, if a manifold M is equipped with both structures, it is called a
fiber-Riemannian Poisson (fRP) manifold, and the combination of both
acCommon instances of Q satisfy the differential Jacobi identity, required by a Poisson structure.
Exceptions include SGNHT-related dynamics. Nevertheless, the identity does not seem funda-
mental for this geometric construction (even unnecessary for the conservation of Hamiltonian
along Hamiltonian flow).
274
Handbook of Statistics

flows Eqs. (40) and (42)) of a function is called the fiber-gradient Hamilto-
nian (fGH) flow. The geometric view of a general MCMC dynamics
targeting p can be then stated as the fGH flow of KLp (Eqs. 41 and 43) on
the Wasserstein space P2ðMÞ of an fRP manifold M (Liu et al., 2019b,
Thm. 5).
To draw more intuitions from this geometric view, the fiber-gradient flow
decreases KLp(jz) on each fiber and drives each q(jz) toward p(jz), while the
Hamiltonian flow keeps the target distribution p(x) ¼ p(y, z) invariant while
makes more exploration in the sample space. The Langevin dynamics (Eqs.
11 and 14) defines a usual Riemannian structure (nondegenerate D, all fibers
¼ M, M0 degenerates) and null Poisson structure (Q ¼ 0). The evolving dis-
tribution is driven to the target distribution p by the gradient flow. On the
other extreme, the Hamiltonian dynamics (Eqs. 19 and 21) defines a null
fiber-Riemannian structure (D ¼ 0, fiber space degenerates, M0 ¼ M) and
a nondegenerate Poisson structure (nondegenerate Q). It allows wilder while
eligible exploration hence appears more efficient than the Langevin dynamics.
Nevertheless it is fragile to gradient noise (Betancourt, 2015; Chen et al.,
2014), as it only guarantees p is a stationary point but lacks a driving force
toward p under perturbation. The SGHMC dynamics (22) interpolates
between the two extremes. Its half-ranked D matrix defines a Riemannian
structure only in each cotangent space of a base manifold B, so the fiber bun-
dle M is the cotangent bundle T*B. The symplectic structure of T*B defines
the Hamiltonian flow, and the fiber-gradient flow stabilizes the process in
each fiber space, where the gradient noise comes.
4.4.3.4
Inspiration for more general ParVI methods
As mentioned in Section 4.3.1, the classical ParVI method, SVGD, can be
seen as a deterministic simulation of the Wasserstein gradient flow of KLp.
Under the general geometric view here, there are now more options. Liu
et al. (2019b) introduced ParVI methods that simulate the SGHMC dynamics
(22). The development is done by reformulating the dynamics as equivalent
deterministic ones using Eq. (9) or Eq. (10),
dx
dt ¼ Σ1r,
dr
dt ¼ rx logpðxÞCr CΣrr logqtðrÞ,
8
>
<
>
:
dx
dt ¼ Σ1r + rr logqtðrÞ,
dr
dt ¼ rx logpðxÞCr CΣrr logqtðrÞrx logqtðxÞ,
8
>
<
>
:
and then leveraging ParVI techniques (including SVGD) to estimate all the
r log qt with corresponding particles. For example, the Blob method (Chen
et al., 2018a) (see Section 4.5.1) leads to the following particle updates:
Geometry in sampling methods Chapter
10 275

1
ε ðxðiÞ
k+1  xðiÞ
k Þ ¼ Σ1rðiÞ
k ,
1
ε ðrðiÞ
k+1  rðiÞ
k Þ ¼ rxðiÞ
k log pðxðiÞ
k Þ  CrðiÞ
k
CΣ
X
lrrðiÞ
k Kði,lÞ
r,k
X
jKði,jÞ
r,k
+
X
l
rrðiÞ
k Kði,lÞ
r,k
X
jKðj,lÞ
r,k
0
@
1
A,
8
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
1
ε ðxðiÞ
k+1  xðiÞ
k Þ ¼ Σ1rðiÞ
k +
X
lrrðiÞ
k Kði,lÞ
r,k
X
jKði,jÞ
r,k
+
X
l
rrðiÞ
k Kði,lÞ
r,k
X
jKðj,lÞ
r,k
,
1
ε ðrðiÞ
k+1  rðiÞ
k Þ ¼ rxðiÞ
k log pðxðiÞ
k Þ 
X
lrxðiÞ
k Kði,lÞ
xðiÞ
k
X
jKði,jÞ
xðiÞ
k
+
X
l
rxðiÞ
k Kði,lÞ
xðiÞ
k
X
jKðj,lÞ
xðiÞ
k
0
B
@
1
C
A
CrðiÞ
k  CΣ
X
lrrðiÞ
k Kði,lÞ
r,k
X
jKði,jÞ
r,k
+
X
l
rrðiÞ
k Kði,lÞ
r,k
X
jKðj,lÞ
r,k
0
@
1
A,
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:
(44)
where Kði,jÞ
r,k :¼ KrðrðiÞ
k ,rðjÞ
k Þ for a kernel Kr for the momentum r, and similarly for Kði,jÞ
x,k . The new methods inherit the faster
exploration from the Hamiltonian flow.

4.5
Variants and Techniques Inspired by the Geometric View
4.5.1
Other methods for Wasserstein gradient flow simulation
The key to the deterministic simulation of the Wasserstein gradient flow (36)
is the estimation of the set of vectors U* : U*:,i :¼ r log qðxðiÞÞ using particles
fxðiÞg
N
i¼1 of q. This is also where a smoothing operation is required as dis-
cussed in Section 4.3.1. Besides the way that SVGD estimates it, other meth-
ods are developed.
Perhaps the most straightforward treatment is the kernel density estimator
qðxÞ  ~qKðx; fxðiÞgiÞ :¼ 1
N
PN
i¼1Kðx, xðiÞÞ, which forms a ParVI method named
gradient flow with smoothed density (GFSD) (Liu et al., 2019a). The corres-
ponding approximation is UGFSD:,i ¼
P
krxðiÞKði,kÞ
P
jKði,jÞ
, where K(i,j) :¼ K(x(i), x(j)).
It shows less diverse particles than SVGD.
This spirit of smoothing density with kernel can be extended. Noting that
r log q ¼ rð δ
δq q½ log qÞ, it only requires smoothing the q in log q for allow-
ing the use of the empirical distribution ^qðxÞ hence particles. This can be done
by: UBlob :¼ rð δ
δq q½ log ðq  KÞÞ ¼ r log ðq  KÞ + rð q
qK  KÞ, or in terms
of particles, UBlob:,i ¼
P
krxðiÞKði,kÞ
P
jKði,jÞ
+ P
k
rxðiÞKði,kÞ
P
jKðj,kÞ
(Chen et al., 2018a). The
method is called Blob due to its origin in fluid mechanics. Note that it adds
a term to UGFSD, which perhaps explains its more spread particles than GFSD.
A more involved method is proposed by Liu et al. (2019a) under the per-
spective of doing kernel smoothing on vector fields (vs on densities), hence
called gradient flow with smoothed function (GFSF). This is done by noting
that rlogq ¼ argminU L2 maxϕ C∞
c ,kϕkL2q¼1ðq½ϕ  U r  ϕÞ2, and smooth-
ing the vector field can be done by replacing L2
q by the RKHS Hm as done
by SVGD: UGFSF :¼ argminU L2 maxϕ Hm,kϕkHm¼1ðq½ϕ  U r  ϕÞ2 . The
solution using particles is given by UGFSF ¼ JK1, where J:,i :¼ P
jrxðjÞKði,jÞ.
An interesting connection to SVGD is that, if denoting P:,i :¼ rxðiÞ logpðxðiÞÞ, then
VGFSF ¼ P + JK1 while VSVGD ¼ PK + J, so VGFSF ¼VSVGDK1. Another
noticeable connection is to the research direction of gradient estimation for
implicit (meaning no density; only samples) generative models. Particularly,
UGFSF coincides with the result by Li and Turner (2018) developed from another
intuition. Moreover, there are other techniques (e.g., Shi et al., 2018) in the direc-
tion that are worth a trial for ParVI.
Besides simulation using the Wasserstein gradient under a Riemannian
perspective, there are also works that explored gradient-flow simulation in
the metric-space sense. Chen et al. (2018a) leveraged the minimizing move-
ment scheme (Eq. 32) for defining the gradient flow (Ambrosio et al., 2008,
Def. 2.0.2). Due to the definition of the Wasserstein distance equation (31),
Geometry in sampling methods Chapter
10 277

each simulation step amounts to a regularized discrete optimal transport prob-
lem, which is solved approximately and analytically with preset Lagrange
multipliers for both marginal-distribution constraints. For other variants,
Ranganath et al. (2016) solved Eq. (27) over the parameter space of a neural
network model, but this introduces additional optimization cost in each
update. The smoothing effect is implicitly controlled by the Lipschitzness of
the neural network.
4.5.2
Riemannian-manifold support space
4.5.2.1
Riemannian SVGD
Liu and Zhu (2018) made the first attempt to extend SVGD to a Riemannian-
manifold support/particle space ðM,gÞ, called RSVGD. They started with the
Riemannian version of Eq. (26) based on the Riemannian continuity equation
(33) (Liu and Zhu, 2018, Lem. 1):
 d
dt KLpðqtÞ ¼ qt½divðpVtÞ=p ¼ qt½Vt½ log p + div Vt,
(45)
where the densities are w.r.t. the Riemannian measure ωg on the manifold
ðM,gÞ (Liu and Zhu, 2018, Thm. 2). To find a vector field Vt maximizing
the decreasing rate by solving Eq. (27), requirements on the optimization
domain X are introduced: the analytic solution should be a valid vector field
on M and is coordinate independent. The requirements appear very natural,
but are not easily satisfied. For example, every valid vector field on an even-
dimensional hypersphere must have a zero point [hairy ball theorem
(Abraham et al., 2012, Thm. 8.5.13)], which cannot be guaranteed by the choice
in SVGD X ¼ Hm for a particular coordinate system, nor can coordinate inde-
pendency. Liu and Zhu (2018) then chose the space X ¼ fgrad φ|φ  Hg, where
H is the RKHS of a kernel K defined on M. It naturally guarantees the require-
ments since the gradient is always a valid and coordinate-independent vector
field. For common kernels (e.g., Gaussian kernel), such X inherits an inner prod-
uct from H, which makes X a Hilbert space (Liu and Zhu, 2018, Lem. 3) and
leads to the analytic solution:
VRSVGD
t
ðx0Þ ¼ gradx0qtðxÞ ðgradxKÞ½logpðxÞ + LapxK
½

¼ gðx0Þi0j0∂xj0 qtðxÞ
gðxÞij∂xi logpLðxÞ + ∂xigðxÞij


∂xjK + gðxÞij∂xi∂xjK


∂xi0
¼ Gðx0Þ1rx0qtðxÞ
GðxÞ1rlogpLðxÞ + r  GðxÞ1


rxK + GðxÞ1 : rxr>x K


,
(46)
where K is evaluated at (x, x0), and pLðxÞ ¼ pðxÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jGðxÞj
p
(Liu and Zhu, 2018,
Thm. 4). This expression can also be estimated using particles since qt only
appears in expectations. When used with the Fisher–Rao metric, RSVGD
achieves faster convergence than SVGD for Bayesian logistic regression.
For applications on manifolds without global coordinate system, e.g., hyper-
spheres, simulation in the embedded space ΞðMÞ  n of the manifold is
278
Handbook of Statistics

preferred due to the argument in Section 3.3. Liu and Zhu (2018) also derived
such an expression:
VRSVGD;emb
t
ðy0Þ ¼ Λðy0Þry0qtðyÞ
rlog pðyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
jGðyÞj
p

>ΛðyÞryK + r2
yK
h
tr PðyÞ>ðryr>
y KÞPðyÞ


+ ðJðyÞ>rÞ>ðGðyÞ1JðyÞ>ÞryK
i
,
where PðyÞ  nðnmÞ is a set of orthonormal basis of the orthogonal com-
plement of the embedded tangent space Ξ*ðTxMÞ. Particularly for the hyper-
sphere n1, this reduces to:
VRSVGD;sph
t
ðy0Þ ¼ In y0y0>


ry0qtðyÞ r logpðyÞ>ryK + r2
yK y>
ryr>
y K


y
h
ðy> r logpðyÞ + n1Þy> ryK
i
:
It achieves a faster convergence and better particle efficiency than embedded-
space MCMC methods introduced in Section 3.3 for the spherical admixture
model (Reisinger et al., 2010).
4.5.2.2
Mirrored SVGD
More recently, Shi et al. (2022) considered an extension of SVGD to leverage
the mirror descent technique (Beck and Teboulle, 2003), which is an approach
in optimization to handle constrained domain and Riemannian geometry.
For minimizing a function f on a possibly constrained Euclidean domain
X  m , the method follows the minimizing movement scheme (Eq. 32),
xk + 1 ¼ argminx X
1
εdψðx, xkÞ + rfðxkÞ>x, using the so-called Bregman diver-
gence (may not be a distance) dψ(x, xk) :¼ ψ(x)  ψ(xk) rψ(xk)>(x  xk)
defined by a strongly convex smooth function ψ. The solution has an expres-
sion xk+1 ¼ rψ*(rψ(xk)  εrf(xk)), where ψ*ðyÞ :¼ supx  Xy>xψðxÞ is the
convex conjugate (Legendre transformation) of ψ, and the strong convexity
indicates (ψ*)* ¼ ψ and that y ¼ rψ(x) is a bijection to the mirrored space
:¼ rψðXÞ with inverse x ¼ rψ*(y). The expression is interpreted as first
mirroring xk to the mirrored space yk ¼ rψ(xk) and conducting gradient descent
there yk+1 ¼ yk  εrf(xk), then mirroring back to the original space xk+1 ¼ rψ*
(yk+1). This explains the name. For an example to handle a constrained opti-
mization domain, consider the simplex Δm :¼ fx  ð+Þm|Pm
i¼1xi < 1g and
ψðxÞ :¼ Pm
i¼1 xi logxi + xm + 1ðxÞlogxm + 1ðxÞ where xm+1ðxÞ :¼ 1Pm
i¼1xi . The
mirror map is rψðxÞ¼ logðx=xm+1ðxÞÞ elementwise, which leads to an uncon-
strained mirror space rψðΔmÞ ¼ m.
For developing mirrored SVGD, a helpful insight is that mirror descent can
be seen as a Riemannian gradient descent with G ¼ rr>ψ as the Riemannian
metric, since lim ε!0 1
ε ðxk+1  xkÞ ¼ ðrr>ψðxkÞÞ
1rfðxkÞ matches Eq. (1).
Particularly, the above simplex example also shows the usage of mirror descent
Geometry in sampling methods Chapter
10 279

for information geometry: the Bregman divergence dψ(x, x0) recovers the KL
divergence KLðCatð  jxÞ k Catð  jx0ÞÞ if the simplex points x, x0 are treated
as the parameter of a categorical distribution (on m + 1 categories), and the
Riemannian metric rr>ψ(x) recovers the Fisher–Rao metric (Fisher informa-
tion matrix) of Cat(jx).
Under this insight, mirrored SVGD may be seen as a special case of
RSVGD, but there are more subtleties. The tasks are on Euclidean spaces,
meaning a natural, global coordinate system of the manifold, so vector-field
validity and coordinate-invariance are not concerned. On the other hand, as
shown in the simplex example, Δm is a constrained space so updating particles
there is quite involved. But it is much easier to update particles in the uncon-
strained mirrored space, which exactly guarantees the constraint in the origi-
nal space. So Shi et al. (2022) considered solving a dynamics in the
mirrored space, dyt ¼ Ut(xt) dt, similar to the form of mirror descent in the
mirror space. This is equivalent to formulating the dynamics in the original
space as dxt ¼ (rr>ψ(xt))1Ut(xt) dt ¼ G(xt)1Ut(xt) dt. Under this formula-
tion, the Riemannian version of KL decreasing rate equation (45)becomesad :
 d
dt KLpðqtÞ ¼ qt½Sψ
p Ut,
where
Sψ
p Ut :¼ U>
t ðrr>ψÞ
1r log p + r 
ðrr>ψÞ
1Ut


¼ U>
t ðrr>ψÞ
1r log ðp=jrr>ψjÞ+ðrr>ψÞ
1 : ðrU>
t Þ,
is called the mirrored Stein operator, and p is the density under the Lebesgue
measure of X (different from the case in Eq. 45). Using this expression, Shi
et al. (2022) solved Eq. (27) for maximizing the decreasing rate in the RKHS
of a matrix-valued kernel K on X:
UMSVGD
t
ðx0Þ :¼ qtðxÞ½Sψ
p Kðx,x0Þ,
where Sψ
p operates on each row of K(x, x0) as a vector-valued function of x
(Shi et al., 2022, Thm. 3). Similar to mirror descent, dynamics simulation
for mirrored SVGD is done by first mirroring all x particles to y particles
using rψ, updating y particles using UMSVGD, and then mirroring back to x
particles.
The simplest choice of kernel is K(x,x0) ¼ K(x,x0)Im where K is a scalar-
valued kernel on X . But Shi et al. (2022) found with one particle, the
algorithm does not reduce to mirror descent targeting the mode of p(x). They
thus introduced a geometry-aware, position-dependent kernel:
Kψ
t ðx,x0Þ :¼ qtðx00Þ K1=2
t
ðx, x00Þrr>ψðx00ÞK1=2
t
ðx00, x0Þ
h
i
,
adEq. (18) is used in the last equality.
280
Handbook of Statistics

where K1=2
t
ðx,x0Þ :¼ P
αλ1=2
t,α ut,αðxÞut,αðx0Þ
is defined under the spectral
expansion/Mercer representation of the scalar-valued kernel, K(x, x0) ¼
P
αλt,αut,α(x)ut,α(x0), where each (λt,α, ut,α) solves the eigenvalue problem
qtðx0Þ½Kðx,x0Þutðx0Þ ¼ λtutðxÞ. It drives to the mode of p(x) thus transits to
MAP estimate when using one particle (Shi et al., 2022, Prop. 5), as desired.
Shi et al. (2022) also used a general Riemannian metric in place of rr>ψ
(and using the geometry-aware kernel) for leveraging information geometry
in general Euclidean/unconstrained inference tasks, which recovers natural
gradient descent using one particle.
4.5.3
Accelerated gradient flow
Inspired by the geometric view of ParVI methods as approximations to the
gradient descent of KLp on the Wasserstein space (see Sections 4.3.1 and
4.5.1), Liu et al. (2019a) developed new ParVI methods corresponding to
accelerated first-order optimization on the Wasserstein space. Acceleration
of gradient descent in the Euclidean space is done by the well-known Nester-
ov’s acceleration method (Nesterov, 1983), and it has been extended to Rie-
mannian manifolds, including Riemannian accelerated gradient (RAG) (Liu
et al., 2017) and Riemannian Nesterov’s method (RNes) Zhang and Sra
(2018). When written for the Wasserstein space P2ðmÞ, these accelerated
methods introduce an auxiliary distribution variable ρ  P2ðmÞ in addition
to the optimized distribution variable q, and the optimization updates are
given by [with slight simplification by Liu et al. (2019a)]:
qk ¼ Expρk1ðεVk1Þ,
ρk ¼
Expqk Γqk
ρk1
k 1
k
Exp1
ρk1ðqk1Þk + α2
k
εVk1




,
ðRAGÞ
Expqk c1Exp1
qk
Expρk1 ð1c2ÞExp1
ρk1ðqk1Þ + c2Exp1
ρk1ðqkÞ


h
i
n
o
,
ðRNesÞ
8
>
<
>
:
8
>
>
>
<
>
>
>
:
where Vk :¼ grad KL(ρk), and α > 3 and c1, c2 > 0 are hyperparameters.
Implementing the algorithms requires estimating the exponential map
Expq(V ), its inverse Exp1
q ðρÞ, and the parallel transport Γρ
q using particles
{x(i)}i of q and particles {y(j)}j of ρ. Recall from Section 4.3.1 (Ambrosio
et al., 2008, Prop. 8.4.6), since the geodesic defining the exponential map
is obviously tangent to V at q, the Wasserstein-space exponential map
Expq(V ) can be estimated by:
ExpqðVÞ ¼ Exp ðVð  ÞÞ#½q ¼ ðid + VÞ#½q,
(47)
where the second Exp is the exponential map of the support space, and the last
equality holds on Euclidean support spaces. This means that {x(i) + V(x(i))}i is
a set of particles of Expq(V). The inverse exponential map Exp1
q ðρÞ is given
by T ρ
q  id on Euclidean support space m, where T ρ
q is the optimal transport
map from q to ρ (Ambrosio et al., 2008, Thm. 7.2.2; Villani, 2008, Cor. 7.22).
Solving the optimal transport problem directly is costly, but Liu et al. (2019a)
Geometry in sampling methods Chapter
10 281

noticed that Exp1
q ðρÞ is invoked only for ρ as infinitesimally updated q,
which means the two sets of particles possibly hold the pairwise close
assumption:
dðxðiÞ,yðiÞÞ ≪minfmin j6¼idðxðiÞ, xðjÞÞ, minj6¼idðyðiÞ, yðjÞÞg.
The
optimal transport evaluated at x(i) can then be approximated as y(i)  x(i).
As for the parallel transport, Liu et al. (2019a) leveraged the Schild’s ladder
(Ehlers et al., 1972; Kheyfets et al., 2000) approximation method that only
requires the exponential map and its inverse. The resulting accelerated ParVIs
are named Wasserstein accelerated gradient (WAG) and Wasserstein Nester-
ov’s method (WNes) that update particles as:
xðiÞ
k ¼ yðiÞ
k1 + εV yðiÞ
k1


,
yðiÞ
k ¼ xðiÞ
k +
k 1
k
yðiÞ
k1 xðiÞ
k1


+ k + α2
k
εV yðiÞ
k1


,
ðWAGÞ
c1ðc2 1Þ xðiÞ
k xðiÞ
k1


,
ðWNesÞ
8
>
<
>
:
8
>
>
>
>
<
>
>
>
>
:
where each V (y(i))k is estimated using any existing ParVI techniques
(e.g., SVGD, variants in Section 4.5.1).
Taghvaei and Mehta (2019) considered accelerating ParVI dynamics using
a recent powerful unifying framework for optimization dynamics (Wibisono
et al., 2016). The resulting algorithm is in a similar form of the ParVI coun-
terpart of SGHMC, Eq. (44) introduced in Section 4.4.3. The framework has
also been leveraged to accelerate MCMC dynamics (Ma et al., 2019;
Wibisono, 2018).
Another way to accelerate optimization is Newton’s method, which steers
the gradient with the inverse Hessian matrix of the objective. Detommaso
et al. (2018) developed such generalization under the early version of the
Stein geometry, i.e., taking the vector-valued RKHS Hm as the tangent
space (Liu, 2017). Specifically, the Hessian of KLp at q is defined as a
function
of
two
tangent
vectors
V,
W:
Hess KLpðqÞðV,WÞ :¼
limε!0 1
εðV½KLpðExpqðεWÞÞV½KLpðqÞÞ, where Expq(εW) ¼ (id + εW)#q is
the exponential map (see Eq. 47), and V½FðqÞ ¼ hVðqÞ, gradPHm FðqÞiHm ¼
hVðqÞ,r δF
δqðqÞiHm is the action of the tangent vector V on F at q (i.e., the direc-
tional derivative along V; see Section 2.2). Its explicit expression is
Hess KLpðqÞðV,WÞ ¼ hqðxÞ½Hq,pðx,  ÞWðxÞ,Vð  ÞiHm, where Hq,pðx,x0Þ :¼
Kðx,x0Þrr>logpðxÞ + rxr>
x Kðx,x0Þ + rxKðx,x0ÞrlogqðxÞ>. In the Euclidean
case, the Newtonian descending direction w is  (rr>f )1rf, or v>(rr>f )
w ¼ v>rf for all vector v. So the Newtonian descending direction W for
KLp at q is given by: Hess KLp(q)(V, W) ¼ V [KLp(q)] for all tangent vector
V  Hm.
This
amounts
to
solving
qðxÞ½Hq,pðx,x0ÞWðxÞ ¼ VSVGDðx0Þ ¼
qðxÞ½Kðx,x0ÞrlogpðxÞ + rxKðx,x0Þ,
which
is
very
costly
(Ω(N3d3)).
Detommaso et al. (2018) made a relaxation of the problem as qðxÞ½ ~Hq,pðx,x0Þ
Wðx0Þ ¼ VSVGDðx0Þ, where the interaction of W between particles is decoupled,
282
Handbook of Statistics

and Hq,p is simplified as ~Hq,pðx,x0Þ :¼ ~NpðxÞKðx,x0Þ +
~NKðx,x0Þ where the
Hessians of logp and K are replaced with their Gauss–Newton approximations,
and the third term is omitted due to the difficulty of estimation by particles.ae
They
also
consider
using
a
kernel
with
preconditioning,
Kðx,x0Þ :¼
expððxx0Þ>q½rr>logpðxx0Þ=ð2mÞÞfor faster convergence. Although
this method also leverages Fisher information (through ~Np ) and the kernel
Hessian rxr>
x K as RSVGD (coordinate-space version (46)) does, the two
methods are different in formulation, and the connection is yet to be studied.
Quasi-Newton methods for ParVI are also considered to reduce the costly
second-order derivative computation. Zhu et al. (2020) leveraged a Rieman-
nian quasi-Newton method (Kasai et al., 2018) that also allows stochastic gra-
dient and variance reduction, and applied it to the Wasserstein space to develop
a quasi-Newton ParVI method. Geometric constructions are implemented using
the techniques by Liu et al. (2019a) introduced in Section 4.5.3. They also lev-
eraged this general approach to develop variance-reduced ParVI methods,
based on Riemannian stochastic variance reduction gradient (Zhang et al.,
2016) and Riemannian stochastic path integrated differential estimator (Zhou
et al., 2019).
4.5.4
Treatment of the kernel
As mentioned in Section 4.3.1, under the perspective of ParVI as KLp minimi-
zation on the Wasserstein space, a smoothing operation is mandatory. The
most popular way for this is using kernels (either using kernel density estima-
tion or using RKHS for a function class; see Section 4.5.1), due to its elegant
theoretical properties and efficient implementation. Nevertheless, it also faces
challenges.
It is well known that kernel methods do not work as well in high dimen-
sions. Zhuo et al. (2018) studied the implication for SVGD, and found the par-
ticles tend to collapse and underestimate the marginal variances as dimension
increases. This is due to the vanishing of the repulsive term (second term in
Eq. 30) which is responsible for diversity. Specifically, for a Gaussian kernel,
a Gaussian target distribution, and a Gaussian or compactly supported particle
distribution, the maximal scale of the repulsion was shown to decrease
inversely sublinearly to dimension. To work around this limitation, they pro-
posed a message-passing SVGD, which leverages the conditional indepen-
dency in the high-dimensional random variable under the target distribution,
and remove the corresponding noninteracting dimension pairs in the kernel.
This breaks down a high-dimensional problem into several low-dimensional
problems, and better performance was seen. A similar method is concurrently
aeA better approximation may be omitting the last two terms together, since they cancel each other
under the expectation with q(x) under mild boundary conditions:
R
qðxÞrxKðx,x0Þr log qðxÞ> dx ¼
R
rxKðx,x0ÞrqðxÞ> dx ¼ 
R
rxr>
x Kðx,x0ÞqðxÞ dx.
Geometry in sampling methods Chapter
10 283

developed by Wang et al. (2018) which is called graphical SVGD. Ba et al.
(2019) made some further analysis. They showed that for a Gaussian kernel
and Gaussian target distribution, the marginal variance of SVGD-stationary
particles is proportional to the ratio of the number of particles over dimension,
when both are sufficiently large and the ratio is less than 1. They also empha-
sized the collapse is due to both the randomness of estimating the first term of
Eq. (30) using finite particles, and the deterministic update. They then pro-
posed variants to reduce the randomness and enable stochastic particle resam-
pling, though impractical for real applications. Some other works considered
general approaches to reducing the dimensionality of the kernel to combat
the variance collapse problem. Chen and Ghattas (2020) find a dominating
subspace support of the target Bayesian posterior and run SVGD there.
Gong et al. (2021) introduced a sliced kernelized Stein discrepancy [in a sim-
ilar spirit as the sliced Wasserstein distance (Kolouri et al., 2016, 2019)],
which can be unbiased estimated by a one-dimensional kernel calculation
on a randomly/properly selected direction, which largely reduces the
dimensionality. The resulting ParVI is also shown to outperform SVGD. Liu
et al. (2022) took a step further to make a k-dimensional slice (1 	 k 	 m).
The k-dimensional subspace is defined by an optimization problem, which
is solved by simulating a diffusion process on the space of k-dimensional sub-
spaces, called the Grassmann manifold. The Grassmann manifold can be seen
as the quotient space of the Stiefel manifold (all m  k orthogonal matrices)
over the orthogonal group of order k.
There are works that analyzed the impact of kernels on the approximation
power of SVGD convergent particles. Gorham and Mackey (2017) studied the
discriminative power of kernel Stein discrepancy Ip,K(q) defined in Eq. (39),
which indicates properties of SVGD convergent particles since Ip,K(q) con-
verges to zero along SVGD updates (Korba et al., 2020, Cor. 6). They found
Ip,K(q) detects convergence (i.e., it converges to zero for a q sequence weakly
converges to p) if K is twice continuously differentiable with uniformly
bounded second-order cross derivatives and r log p is Lipschitz with finite
L2
p norm. However for the converse, they found a counterexample that
light-tailed kernelsaf fail to detect nonconvergence to standard Gaussian
for dimension m 
 3. This failure unfortunately applies to common kernels
such as the Gaussian kernel, Matern kernel, compactly supported kernels,
and inverse multiquadric (IMQ) kernel KIMQðx,yÞ :¼ ðc2 + k x  yk2
2Þ
β
with β  (1, m/(m  2)). A choice that guarantees the success of detecting
nonconvergence is an IMQ kernel with β  (0, 1). Liu and Wang (2018) ana-
lyzed the condition for a set of particles to be SVGD convergent, and found
the convergent particles give the exact expectation under p for Stein-operator-
transformed RKHS functions. Implications of this result include that linear
afFormally, this means supf max fjKðx,yÞj, k rxKðx,yÞk2,jr>
x ryKðx,yÞjg; j k x  yk2 
 rg ¼
o r1= 1
21
m
ð
Þ


.
284
Handbook of Statistics

kernels K(x, x0) :¼ x>x0 + c2 lead to convergent particles that exactly estimate
the mean and variance of Gaussian distributions, and that a kernel combining
randomly chosen feature maps of another kernel yields Oð1=
ﬃﬃﬃﬃ
N
p
Þ -close
convergent particles in terms of kernel Stein discrepancy to p.
Liu et al. (2019a) studied the impact of kernel under a dynamics pers-
pective. Under their view of approximate Wasserstein-gradient-flow simula-
tion (Section 4.3.1), the kernel is introduced for the mandatory smoothing
operation for the intractable gradient r log qðxÞ (see Section 4.5.1 for variants
besides SVGD). Due to the equivalence between smoothing densities and
smoothing functions (Section 4.3.1), they took GFSD (Section 4.5.1) as an
example, which uses kernel density estimation for the particle density,
qðxÞ  ~qKðx; fxðiÞg
N
i¼1Þ :¼ 1
N
PN
i¼1Kðx,xðiÞÞ.
What
matters
of
this
kernel
smoothing is that the density update from the resulting particle update
matches that from the exact dynamics dx ¼ r log qðxÞ dt. The updated par-
ticles after a time step ε are fxðiÞ  εr log ~qðxðiÞ; fxðjÞgjÞgi, so the resulting
density update is qt+εðxÞ  ~qðx; fxðiÞ  εr log ~qðxðiÞ; fxðjÞgjÞgiÞ. On the other
hand, the exact dynamics leads to the density update ∂tqt(x) ¼ r2qt(x) from
FPE (6), so qt+εðxÞ  ~qðx; fxðiÞgiÞ + εr2~qðx; fxðiÞgiÞ. The principle then trans-
lates to the requirement ~qðx; fxðiÞ  εr log ~qðxðiÞ; fxðjÞgjÞgiÞ ¼ ~qðx; fxðiÞgiÞ +
εr2~qðx; fxðiÞgiÞ, which can be enforced by minimizing the averaged squared
difference over the particles. In the limit ε ! 0, the objective can be formu-
lated as 1
N
P
l r2~qðxðlÞ; fxðiÞgiÞ + P
jrxðjÞ ~qðxðlÞ; fxðiÞgiÞ  r log ~qðxðjÞ; fxðiÞgiÞ
h
i2
:
Liu et al. (2019a) applied this objective to select the bandwidth parameter σ of
a Gaussian kernel,ag which achieves an attractive well-aligned pattern that
ameliorates particle collapse.
Wang et al. (2019) made a generalization of SVGD that uses a matrix-
valued kernel K(, ), which is a symmetric matrix-valued function K(x, x0)
¼ K(x0, x)> that is positive-definite everywhere. They solved Eq. (27) for
the optimal vector field in the associated vector-valued RKHS HK of the ker-
nel, which is correspondingly defined as the linear space of vector-valued
functions
P
lKð  , xðlÞÞαðlÞ
	

(every α(l) is a vector; summation over a count-
able index set) with inner product
P
lKð  , xðlÞÞαðlÞ, P
l0Kð  , yðl0ÞÞβðl0Þ
D
E
HK :¼
P
l,l0αðlÞ>KðxðlÞ, xðl0ÞÞβðl0Þ (c.f. Section 4.1). The corresponding reproducing
property is hfð  Þ, Kð  , xÞαiHK ¼ α>fðxÞ. The resulting matrix-valued SVGD
vector field is:
VSVGD,K
t
ðxÞ :¼ qtðx0Þ½Kðx,x0Þrx0 log pðx0Þ + rx0  Kðx,x0Þ:
agFor optimizing bandwidth σ, they divided the objective by σ2m+4 to make a dimensionless
objective.
Geometry in sampling methods Chapter
10 285

Apparently, K(, ) ¼ K(, )Im recovers the previous vector-valued RKHS
HK ¼ Hm and the SVGD vector field equation (29). The general formulation
includes message-passing/graphical SVGD (Wang et al., 2018; Zhuo et al.,
2018) and mirrored SVGD (Shi et al., 2022) that came afterward. They made
a discussion on the choice of K by using the induced kernel under a geometric
transformation, and proposed a second-order method that is cheaper than
Newton SVGD (Detommaso et al., 2018).
5
Conclusion
In this chapter we have reviewed the geometry consideration in sampling
methods, including MCMC and ParVI methods, which are among the major
tools for Bayesian inference. This is directly required when latent variables
are defined on a manifold to better reflect the data structure/semantics. We
have shown how MCMC and ParVI methods on manifolds are developed
and simulated to solve the inference task of such latent variables. These meth-
ods also enable leveraging information geometry, which endows the latent
space with a metric on the likelihood distribution, and leads to a faster conver-
gence. Particularly for ParVI methods as a formulation of variational infer-
ence, a geometric interpretation as the gradient flow of the KL divergence
on certain abstract distribution manifolds can be coined. This inspires various
analysis and methods of ParVI, and also connects ParVI with MCMC which
benefits one with techniques of the other. Nevertheless, computation involv-
ing non-Euclidean geometry is often more costly. Dynamics simulation in
the coordinate space requires matrix inversion or higher-order derivatives,
and only a few manifolds have a closed-form expression of geodesic and
exponential map in the embedded space. ParVIs need to find a tractable
approximation to the gradient flow, and the currently prevailing kernel
method is not as effective in high dimensions. Further progress addressing
these issues would enable people to enjoy the benefits of these geometric
methods with less cost.
Acknowledgments
This work was supported by the National Key Research and Development Program of China
(2021ZD0110502, 2017YFA0700904), NSFC Projects (Nos. 62061136001, 61621136008,
62106121, U19B2034, U19A2081, U1811461), the major key project of PCL (No.
PCL2021A12), and Tsinghua Guo Qiang Institute, and the High Performance Computing
Center, Tsinghua University.
References
Abraham, R., Marsden, J.E., Ratiu, T., 2012. Manifolds, Tensor Analysis, and Applications.
vol. 75, Springer Science & Business Media, New York.
Amari, S.-I., 1998. Natural gradient works efficiently in learning. Neural Comput. 10 (2),
251–276.
286
Handbook of Statistics

Amari, S.-I., 2016. Information Geometry and Its Applications. Springer, Tokyo.
Amari, S.-I., Nagaoka, H., 2007. Methods of Information Geometry. vol. 191, American
Mathematical Soc., Providence, Rhode Island.
Ambrosio, L., Gangbo, W., 2008. Hamiltonian ODEs in the Wasserstein space of probability
measures. Commun. Pure Appl. Math. 61 (1), 18–53.
Ambrosio, L., Gigli, N., Savare, G., 2008. Gradient Flows: In Metric Spaces and in the Space of
Probability Measures. Springer Science & Business Media, Berlin.
Arvanitidis, G., Hansen, L.K., Hauberg, S., 2018. Latent space oddity: on the curvature of deep
generative models. In: International Conference on Learning Representations.
Arvanitidis, G., Hauberg, S., Hennig, P., Schober, M., 2019. Fast and robust shortest paths on
manifolds learned from data. In: The 22nd International Conference on Artificial Intelligence
and Statistics, PMLR, pp. 1506–1515.
Ba, J., Erdogdu, M.A., Ghassemi, M., Suzuki, T., Wu, D., Sun, S., Zhang, T., 2019. Towards char-
acterizing the high-dimensional bias of kernel-based particle inference algorithms. In: The
2nd Symposium on Advances in Approximate Bayesian Inference.
Barbour, A.D., 1990. Stein’s method for diffusion approximations. Probab. Theory Relat. Fields
84 (3), 297–322.
Beck, A., Teboulle, M., 2003. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Oper. Res. Lett. 31 (3), 167–175.
Benamou, J.-D., Brenier, Y., 2000. A computational fluid mechanics solution to the Monge-
Kantorovich mass transfer problem. Numer. Math. 84 (3), 375–393.
Betancourt, M., 2015. The fundamental incompatibility of scalable Hamiltonian Monte Carlo and
naive data subsampling. In: Proceedings of the 32nd International Conference on Machine
Learning (ICML 2015), IMLS, Lille, France, pp. 533–540.
Betancourt, M., 2017. A conceptual introduction to Hamiltonian Monte Carlo. arXiv:1701.02434.
Betancourt, M., Byrne, S., Livingstone, S., Girolami, M., et al., 2017. The geometric foundations
of Hamiltonian Monte Carlo. Bernoulli 23 (4A), 2257–2298.
Billingsley, P., 2012. Probability and Measure. John Wiley & Sons, New Jersey, ISBN: 978-1-
118-12237-2.
Blei, D.M., Ng, A.Y., Jordan, M.I., 2003. Latent Dirichlet allocation. J. Mach. Learn. Res. 3,
993–1022.
Brubaker, M.A., Salzmann, M., Urtasun, R., 2012. A family of MCMC methods on implicitly
defined manifolds. In: Proceedings of the 15th International Conference on Artificial Intelli-
gence and Statistics (AISTATS-12), AISTATS Committee, La Palma, Canary Islands, pp.
161–172.
Byrne, S., Girolami, M., 2013. Geodesic Monte Carlo on embedded manifolds. Scand. J. Stat.
40 (4), 825–845.
Caterini, A.L., Doucet, A., Sejdinovic, D., 2018. Hamiltonian variational auto-encoder. In:
Advances in Neural Information Processing Systems, vol. 31.
Chen, P., Ghattas, O., 2020. Projected Stein variational gradient descent. Adv. Neural Inf. Proces.
Syst. 33, 1947–1958.
Chen,
Y.,
Li,
W.,
2018.
Natural
gradient
in
Wasserstein
statistical
manifold.
arXiv:1805.08380.
Chen, T., Fox, E., Guestrin, C., 2014. Stochastic gradient Hamiltonian Monte Carlo. In: Proceed-
ings of the 31st International Conference on Machine Learning (ICML 2014), IMLS, Beijing,
China, pp. 1683–1691.
Chen, C., Ding, N., Carin, L., 2015. On the convergence of stochastic gradient MCMC algorithms
with high-order integrators. In: Advances in Neural Information Processing Systems, NIPS
Foundation, Montreal, Canada, pp. 2269–2277.
Geometry in sampling methods Chapter
10 287

Chen, C., Zhang, R., Wang, W., Li, B., Chen, L., 2018a. A unified particle-optimization frame-
work for scalable Bayesian sampling. In: Proceedings of the Conference on Uncertainty in
Artificial Intelligence (UAI 2018), Association for Uncertainty in Artificial Intelligence,
Monterey, California USA.
Chen, N., Klushyn, A., Kurle, R., Jiang, X., Bayer, J., Smagt, P., 2018b. Metrics for deep genera-
tive models. In: International Conference on Artificial Intelligence and Statistics, PMLR, pp.
1540–1550.
Cheng,
X.,
Bartlett,
P.,
2017.
Convergence
of
Langevin
MCMC
in
KL-divergence.
arXiv:1705.09048.
Cheng, X., Chatterji, N.S., Bartlett, P.L., Jordan, M.I., 2018. Underdamped Langevin MCMC: a
non-asymptotic analysis. In: Conference on Learning Theory, PMLR, pp. 300–323.
Chewi, S., Le Gouic, T., Lu, C., Maunu, T., Rigollet, P., 2020. SVGD as a kernelized Wasserstein
gradient flow of the chi-squared divergence. In: Advances in Neural Information Processing
Systems, vol. 33, pp. 2098–2109.
Chwialkowski, K., Strathmann, H., Gretton, A., 2016. A kernel test of goodness of fit. In: Pro-
ceedings of the 33rd International Conference on Machine Learning (ICML 2016), IMLS,
New York, New York USA, pp. 2606–2615.
Da Silva, A.C., 2001. Lectures on Symplectic Geometry. vol. 3575 Springer, Boston.
Dalalyan, A.S., 2017. Theoretical guarantees for approximate sampling from smooth and
log-concave densities. J. R. Stat. Soc. B (Stat. Methodol.) 79 (3), 651–676.
Davidson, T.R., Falorsi, L., De Cao, N., Kipf, T., Tomczak, J.M., 2018. Hyperspherical
variational auto-encoders. arXiv:1804.00891.
Detommaso, G., Cui, T., Marzouk, Y., Spantini, A., Scheichl, R., 2018. A Stein variational
Newton method. In: Advances in Neural Information Processing Systems, NIPS Foundation,
Montreal, Canada, pp. 9187–9197.
Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R.D., Neven, H., 2014. Bayesian sampling
using stochastic gradient thermostats. In: Advances in Neural Information Processing Sys-
tems, NIPS Foundation, Montreal, Canada, pp. 3203–3211.
Dinh, L., Sohl-Dickstein, J., Bengio, S., 2017. Density estimation using real NVP. In: Proceedings
of the International Conference on Learning Representations (ICLR 2017).
Do Carmo, M.P., 1992. Riemannian Geometry. Birkh€auser, Boston.
Dockhorn, T., Vahdat, A., Kreis, K., 2021. Score-based generative modeling with critically-
damped Langevin diffusion. In: Proceedings of the International Conference on Learning
Representations (ICLR 2021).
Duane, S., Kennedy, A.D., Pendleton, B.J., Roweth, D., 1987. Hybrid Monte Carlo. Phys. Lett.
B 195 (2), 216–222.
Duncan, A., N€usken, N., Szpruch, L., 2019. On the geometry of Stein variational gradient descent.
arXiv:1912.00894.
Durmus, A., Moulines, E., 2016. High-dimensional Bayesian inference via the unadjusted Lange-
vin algorithm. arXiv:1605.01559.
Durmus, A., Moulines, E., Saksman, E., 2017. On the convergence of Hamiltonian Monte Carlo.
arXiv:1705.00166.
Eberle, A., Guillin, A., Zimmer, R., 2019. Couplings and quantitative contraction rates for
Langevin dynamics. Ann. Probab. 47 (4), 1982–2010.
Ehlers, J., Pirani, F., Schild, A., 1972. The geometry of free fall and light propagation. In: General
Relativity, Clarendon Press, Oxford, pp. 63–84 (papers in honour of JL Synge).
Erbar, M., et al., 2010. The heat equation on manifolds as a gradient flow in the Wasserstein
space. Ann. Inst. H. Poincare Probab. Stat. 46 (1), 1–23.
288
Handbook of Statistics

Fernandes, R.L., Marcut, I., 2014. Lectures on Poisson Geometry. Springer, Basel.
Gangbo, W., Kim, H.K., Pacini, T., 2010. Differential Forms on Wasserstein Space and
Infinite-Dimensional Hamiltonian Systems. American Mathematical Soc., Providence, Rhode
Island.
Girolami, M., Calderhead, B., 2011. Riemann manifold Langevin and Hamiltonian Monte Carlo
methods. J. R. Stat. Soc. B (Stat. Methodol.) 73 (2), 123–214.
Gong, W., Li, Y., Herna´ndez-Lobato, J.M., 2021. Sliced kernelized Stein discrepancy. In: Pro-
ceedings of the International Conference on Learning Representations (ICLR 2021).
Gorham, J., Mackey, L., 2015. Measuring sample quality with Stein’s method. In: Advances in
Neural Information Processing Systems, NIPS Foundation, Montreal, Canada, pp. 226–234.
Gorham, J., Mackey, L., 2017. Measuring sample quality with kernels. arXiv:1703.01717.
Grattarola, D., Livi, L., Alippi, C., 2018. Adversarial autoencoders with constant-curvature latent
manifolds. arXiv:1812.04314.
Hairer, E., Lubich, C., Wanner, G., 2006. Geometric Numerical Integration: Structure-Preserving
Algorithms for Ordinary Differential Equations. vol. 31 Springer Science & Business Media.
He, D., Shi, W., Li, S., Gao, X., Zhang, J., Bian, J., Wang, L., Liu, T.-Y., 2022. Learning physics-
informed neural networks without stacked back-propagation. arXiv:2202.09340.
Hopf, H., Rinow, W., 1931. €Uber den begriff der vollst€andigen differential geometrischen fl€ache.
Comment. Math. Helv. 3 (1), 209–225.
James, I.M., 1976. The Topology of Stiefel Manifolds. vol. 24 Cambridge University Press, New York.
Jordan, R., Kinderlehrer, D., Otto, F., 1998. The variational formulation of the Fokker-Planck
equation. SIAM J. Math. Anal. 29 (1), 1–17.
Kalatzis, D., Eklund, D., Arvanitidis, G., Hauberg, S., 2020. Variational autoencoders with
Riemannian Brownian motion priors. In: International Conference on Machine Learning,
PMLR, pp. 5053–5066.
Kasai, H., Sato, H., Mishra, B., 2018. Riemannian stochastic quasi-Newton algorithm with
variance reduction and its convergence analysis. In: International Conference on Artificial
Intelligence and Statistics, PMLR, pp. 269–278.
Kent, J., 1978. Time-reversible diffusions. Adv. Appl. Probab. 10 (4), 819–835.
Khan, M.E., Nielsen, D., 2018. Fast yet simple natural-gradient descent for variational inference
in complex models. In: 2018 International Symposium on Information Theory and Its Appli-
cations (ISITA), IEEE, Singapore, pp. 31–35.
Kheyfets, A., Miller, W.A., Newton, G.A., 2000. Schild’s ladder parallel transport procedure for
an arbitrary connection. Int. J. Theor. Phys. 39 (12), 2891–2898.
Kingma, D.P., Dhariwal, P., 2018. Glow: generative flow with invertible 1  1 convolutions.
In: Advances in Neural Information Processing Systems, vol. 31.
Kolouri, S., Zou, Y., Rohde, G.K., 2016. Sliced Wasserstein kernels for probability distributions.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5258–5267.
Kolouri, S., Nadjahi, K., Simsekli, U., Badeau, R., Rohde, G., 2019. Generalized sliced
Wasserstein distances. In: Advances in Neural Information Processing Systems, vol. 32.
Korba, A., Salim, A., Arbel, M., Luise, G., Gretton, A., 2020. A non-asymptotic analysis for Stein
variational gradient descent. In: Advances in Neural Information Processing Systems, vol. 33,
pp. 4672–4682.
Kova´cik, O., Ra´kosnı´k, J., 1991. On spaces Lp(x) and Wk,p(x). Czechoslov. Math. J. 41 (4), 592–618.
Lan, S., Zhou, B., Shahbaba, B., 2014. Spherical Hamiltonian Monte Carlo for constrained target
distributions. In: Proceedings of the 31st International Conference on Machine Learning
(ICML 2014), IMLS, Beijing, China, pp. 629–637.
Geometry in sampling methods Chapter
10 289

Lan, S., Stathopoulos, V., Shahbaba, B., Girolami, M., 2015. Markov chain Monte Carlo from
Lagrangian dynamics. J. Comput. Graph. Stat. 24 (2), 357–378.
Langevin, P., 1908. Sur la theorie du mouvement Brownien. Compt. Rendus 146, 530–533.
Lee, Y.T., Vempala, S.S., 2018. Convergence rate of Riemannian Hamiltonian Monte Carlo and
faster polytope volume computation. In: Proceedings of the 50th Annual ACM SIGACT
Symposium on Theory of Computing, pp. 1115–1121.
Li, Y., Turner, R.E., 2018. Gradient estimators for implicit models. In: Proceedings of the
International Conference on Learning Representations (ICLR 2018), ICLR Committee,
Vancouver, Canada. https://openreview.net/forum?id¼SJi9WOeRb.
Li, C., Chen, C., Carlson, D.E., Carin, L., 2016. Preconditioned stochastic gradient Langevin
dynamics for deep neural networks. In: The 30th AAAI Conference on Artificial Intelligence
(AAAI-16), AAAI Press, Phoenix, Arizona USA, vol. 2, pp. 1788–1794. 3.
Liu, Q., 2017. Stein variational gradient descent as gradient flow. In: Advances in Neural Infor-
mation Processing Systems, NIPS Foundation, Long Beach, California USA, pp. 3118–3126.
Liu, Q., Wang, D., 2016. Stein variational gradient descent: a general purpose Bayesian inference
algorithm. In: Advances in Neural Information Processing Systems, NIPS Foundation,
Barcelona, Spain, pp. 2370–2378.
Liu, Q., Wang, D., 2018. Stein variational gradient descent as moment matching. In: Advances in
Neural Information Processing Systems, vol. 31.
Liu, C., Zhu, J., 2018. Riemannian Stein variational gradient descent for Bayesian inference. In:
The 32nd AAAI Conference on Artificial Intelligence, AAAI Press, New Orleans, Louisiana
USA, pp. 3627–3634.
Liu, C., Zhu, J., Song, Y., 2016a. Stochastic gradient geodesic MCMC methods. In: Advances
in Neural Information Processing Systems 29, NIPS Foundation, Barcelona, Spain,
pp. 3009–3017.
Liu, Q., Lee, J.D., Jordan, M.I., 2016b. A kernelized Stein discrepancy for goodness-of-fit tests.
In: Proceedings of the 33rd International Conference on Machine Learning (ICML 2016),
IMLS, New York, New York USA.
Liu, Y., Shang, F., Cheng, J., Cheng, H., Jiao, L., 2017. Accelerated first-order methods for geo-
desically convex optimization on Riemannian manifolds. In: Advances in Neural Information
Processing Systems, NIPS Foundation, Long Beach, California USA, pp. 4875–4884.
Liu, C., Zhuo, J., Cheng, P., Zhang, R., Zhu, J., Carin, L., 2019a. Understanding and accelerating
particle-based variational inference. In: Proceedings of the 36th International Conference on
Machine Learning, IMLS, Long Beach, California USA, vol. 97, pp. 4082–4092.
Liu, C., Zhuo, J., Zhu, J., 2019b. Understanding MCMC dynamics as flows on the Wasserstein
space. In: Proceedings of the 36th International Conference on Machine Learning, IMLS,
Long Beach, California USA, vol. 97, pp. 4093–4103.
Liu, X., Zhu, H., Ton, J.-F., Wynne, G., Duncan, A., 2022. Grassmann Stein variational gradient
descent. In: International Conference on Artificial Intelligence and Statistics, PMLR, pp.
2002–2021.
Livingstone, S., Betancourt, M., Byrne, S., Girolami, M., 2019. On the geometric ergodicity of
Hamiltonian Monte Carlo. Bernoulli 25 (4A), 3109–3138.
Lott, J., 2008. Some geometric calculations on Wasserstein space. Commun. Math. Phys. 277 (2),
423–437.
Ma, Y.-A., Chen, T., Fox, E., 2015. A complete recipe for stochastic gradient MCMC. In:
Advances in Neural Information Processing Systems, NIPS Foundation, Montreal, Canada,
pp. 2899–2907.
290
Handbook of Statistics

Ma, Y.-A., Chatterji, N., Cheng, X., Flammarion, N., Bartlett, P., Jordan, M.I., 2019. Is there an
analog of Nesterov acceleration for MCMC? arXiv:1902.00996.
Mangoubi, O., Smith, A., 2017. Rapid mixing of Hamiltonian Monte Carlo on strongly log-
concave distributions. arXiv:1708.07114.
Mathieu, E., Lan, C.L., Maddison, C.J., Tomioka, R., Teh, Y.W., 2019. Hierarchical representa-
tions with Poincare variational auto-encoders. arXiv:1901.06033.
Nagano, Y., Yamaguchi, S., Fujita, Y., Koyama, M., 2019. A differentiable Gaussian-like distri-
bution on hyperbolic space for gradient-based learning. arXiv:1902.02992.
Nash, J., 1956. The imbedding problem for Riemannian manifolds. Ann. Math. 63 (1), 20–63.
Neal, R.M., 2011. MCMC using Hamiltonian dynamics. In: Handbook of Markov Chain Monte
Carlo, vol. 2.
Nesterov, Y., 1983. A method of solving a convex programming problem with convergence rate
O(1/k2). Soviet Math. Doklady 27 (2), 372–376.
Nicolaescu, L.I., 2007. Lectures on the Geometry of Manifolds. World Scientific, Singapore.
Otto, F., 2001. The geometry of dissipative evolution equations: the porous medium equation.
Commun. Partial Differ. Equ. 26 (1), 101–174.
Ovinnikov, I., 2019. Poincare Wasserstein autoencoder. arXiv:1901.01427.
Patterson, S., Teh, Y.W., 2013. Stochastic gradient Riemannian Langevin dynamics on the prob-
ability simplex. In: Advances in Neural Information Processing Systems, NIPS Foundation,
Lake Tahoe, Nevada USA, pp. 3102–3110.
Persson, M., 2014. The Whitney Embedding Theorem. Umea˚ University, Umea˚, Sweden.
Ranganath, R., Tran, D., Altosaar, J., Blei, D., 2016. Operator variational inference. In: Advances
in Neural Information Processing Systems, NIPS Foundation, Barcelona, Spain, pp. 496–504.
Reisinger, J., Waters, A., Silverthorn, B., Mooney, R.J., 2010. Spherical topic models. In: Pro-
ceedings of the 27th International Conference on Machine Learning (ICML 2010), IMLS,
Haifa, Israel, pp. 903–910.
Roberts, G.O., Stramer, O., 2002. Langevin diffusions and Metropolis-Hastings algorithms. Meth-
odol. Comput. Appl. Probab. 4 (4), 337–357.
Roberts, G.O., Tweedie, R.L., et al., 1996. Exponential convergence of Langevin distributions and
their discrete approximations. Bernoulli 2 (4), 341–363.
Romano, G., 2007. Continuum mechanics on manifolds. In: Lecture notes University of Naples
Federico II, Naples, Italy, pp. 1–695.
Salakhutdinov, R., Mnih, A., 2008. Bayesian probabilistic matrix factorization using Markov
chain Monte Carlo. In: Proceedings of the 25th International Conference on Machine
Learning (ICML 2008), IMLS, Helsinki, Finland, Omnipress, pp. 880–887.
Santambrogio, F., 2017. Euclidean, metric, and Wasserstein gradient flows: an overview. Bull.
Math. Sci. 7 (1), 87–154.
S€arkk€a, S., Solin, A., 2019. Applied Stochastic Differential Equations. vol. 10 Cambridge Univer-
sity Press.
Seiler, C., Rubinstein-Salzedo, S., Holmes, S., 2014. Positive curvature and Hamiltonian Monte
Carlo. In: Advances in Neural Information Processing Systems, vol. 27.
Shao, H., Kumar, A., Thomas Fletcher, P., 2018. The Riemannian geometry of deep generative
models. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-
tion Workshops, pp. 315–323.
Shi, J., Sun, S., Zhu, J., 2018. A spectral approach to gradient estimation for implicit distributions.
In: Proceedings of the 35th International Conference on Machine Learning (ICML 2018),
IMLS, Stockholm, Sweden, pp. 4651–4660.
Geometry in sampling methods Chapter
10 291

Shi, J., Liu, C., Mackey, L., 2022. Sampling with mirrored Stein operators. In: Proceedings of the
International Conference on Learning Representations (ICLR 2022).
Song, Y., Zhu, J., 2016. Bayesian matrix completion via adaptive relaxed spectral regularization.
In: The 30th AAAI Conference on Artificial Intelligence (AAAI-16), AAAI Press, Phoenix,
Arizona USA, pp. 2044–2050.
Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B., 2021. Score-based
generative modeling through stochastic differential equations. In: Proceedings of the Interna-
tional Conference on Learning Representations (ICLR 2021).
Stein, C., 1972. A bound for the error in the normal approximation to the distribution of a sum of
dependent random variables. In: Proceedings of the Sixth Berkeley Symposium on Mathemat-
ical Statistics and Probability, Volume 2: Probability Theory, The Regents of the University
of California, Oakland.
Steinwart, I., Christmann, A., 2008. Support Vector Machines. Springer Science & Business
Media, New York.
Stiefel, E.L., 1935. Richtungsfelder und fernparallelismus in n-dimensionalen mannigfaltigkeiten.
Comment. Math. Helv. 8 (1), 305–353.
Taghvaei, A., Mehta, P.G., 2019. Accelerated gradient flow for probability distributions. In: Pro-
ceedings of the 36th International Conference on Machine Learning (ICML 2019), IMLS,
Long Beach, California USA.
Toth, P., Rezende, D.J., Jaegle, A., Racanie`re, S., Botev, A., Higgins, I., 2020. Hamiltonian gen-
erative networks. In: Proceedings of the International Conference on Learning Representa-
tions (ICLR 2020).
Villani, C., 2008. Optimal Transport: Old and New. vol. 338 Springer Science & Business Media,
Berlin.
Wang, Y., Li, W., 2022. Accelerated information gradient flow. J. Sci. Comput. 90 (1), 1–47.
Wang, M.C., Uhlenbeck, G.E., 1945. On the theory of the Brownian motion II. Rev. Mod. Phys.
17 (2–3), 323.
Wang, D., Zeng, Z., Liu, Q., 2018. Stein variational message passing for continuous graphical
models. In: International Conference on Machine Learning, PMLR, pp. 5219–5227.
Wang, D., Tang, Z., Bajaj, C., Liu, Q., 2019. Stein variational gradient descent with matrix-valued
kernels. In: Advances in Neural Information Processing Systems, vol. 32.
Welling, M., Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics. In:
Proceedings of the 28th International Conference on Machine Learning (ICML 2011), IMLS,
Bellevue, Washington USA, pp. 681–688.
Whitney, H., 1944. The self-intersections of a smooth n-manifold in 2n-space. Ann. Math.
45 (220-446), 180.
Wibisono, A., 2018. Sampling as optimization in the space of measures: the Langevin dynamics
as a composite optimization problem. arXiv:1802.08089.
Wibisono, A., Wilson, A.C., Jordan, M.I., 2016. A variational perspective on accelerated methods
in optimization. Proc. Natl. Acad. Sci. 113 (47), E7351–E7358.
Xifara, T., Sherlock, C., Livingstone, S., Byrne, S., Girolami, M., 2014. Langevin diffusions and
the Metropolis-adjusted Langevin algorithm. Stat. Probab. Lett. 91, 14–19.
Yanush,
V.,
Kropotov,
D.,
2019.
Hamiltonian
Monte-Carlo
for
orthogonal
matrices.
arXiv:1901.08045.
Zhang, H., Sra, S., 2018. An estimate sequence for geodesically convex optimization. In: Proceed-
ings of the 31st Annual Conference on Learning Theory (COLT 2018), IMLS, Stockholm,
Sweden, pp. 1703–1723.
292
Handbook of Statistics

Zhang, H., Reddi, S.J., Sra, S., 2016. Riemannian SVRG: fast stochastic optimization on Rieman-
nian manifolds. In: Advances in Neural Information Processing Systems, NIPS Foundation,
Barcelona, Spain, pp. 4592–4600.
Zhou, P., Yuan, X.-T., Feng, J., 2019. Faster first-order methods for stochastic non-convex opti-
mization on Riemannian manifolds. In: The 22nd International Conference on Artificial Intel-
ligence and Statistics, PMLR, pp. 138–147.
Zhu, M., Liu, C., Zhu, J., 2020. Variance reduction and quasi-Newton for particle-based varia-
tional inference. In: Proceedings of the 37th International Conference on Machine Learning,
Virtual, vol. 119, pp. 11576–11587.
Zhuo, J., Liu, C., Shi, J., Zhu, J., Chen, N., Zhang, B., 2018. Message passing Stein variational
gradient descent. In: Proceedings of the 35th International Conference on Machine Learning,
Stockholmsm€assan, Stockholm Sweden, vol. 80, pp. 6018–6027.
Geometry in sampling methods Chapter
10 293

This page intentionally left blank

Index
Note: Page numbers followed by “f ” indicate figures, “t” indicate tables, “b” indicate boxes, and
“np” indicate footnotes.
A
Accelerated gradient flow, 281–283
ADCs. See Analog-to-digital-converters
(ADCs)
Akaike information criterion (AIC), 228
American Statistical Association, 103–104
Analog-to-digital-converters (ADCs),
197–198, 198f
Approximate Bayesian computation (ABC),
107
ARD. See Automatic relevance determination
(ARD)
Asymptotic theory
concentration rate, 16–18, 35–38
consistency, 15–16, 34–35
distributional approximations, 18–21
objectives and general strategies, 14–15
Autocorrelation function (ACF) values,
154–156, 155t
Automatic relevance determination (ARD),
165, 173–174, 178–179
Autoregressive (AR) model, 178–179
B
Basis pursuit algorithm, 164–165
Bayes factor, 105–107, 110
Bayesian bootstrap, 85
Bayesian data analysis, 111
Bayesian framework, 1–2, 30
Bayesian GLMMs
Bayesian logistic models, 143, 146–147,
153–154
Bayesian probit mixed models, 143–146,
150–152
Hamiltonian Monte Carlo (HMC)
algorithms, 149
Metropolis adjusted Langevin algorithms
(MALA), 148–149, 149b
Bayesian hypothesis testing, 104–107, 106f
Bayesian inference, 44, 239–240, 251,
254–255
Bayes and selection, 45–50
fixed and random parameters, 48–50
noninformative priors, 50–64
Bayesian information criterion (BIC),
104–105, 229
Bayesian Lasso, 165
Bayesian logistic regression, 254–255,
278–279
Bayesian multiple hypothesis testing, 78–79
decision problem, 71–72
dependent multiple testing, 72–76
preliminaries and setup, 69–71
simulation study, in variable selection,
76–77
Bayesian network model, 195–196, 195f
Bayesian neural networks, 254–255
Bayesian nonmarginal multiple testing, 73–74
Bayesian nonparametrics, 2
Bayesian posterior, 7–8, 18
Bayesian posterior mean, 88–89
Bayesian statistics, mathematical theory of
DGP, model, and prior, 211–213
free energy, 213–217
generalization loss, 213–217
model evaluation, 227–230
not i.i.d. cases, 232–235
phase transitions, 225–227
prior evaluation, 230–232
regular theory, 217–220
singular theory, 220–225
Bayesian uncertainty
mathematical theory, 96–98
missing data, 86–88
nonparametric martingale distributions,
92–96
parametric martingale sequences, 88–91,
93–94
Bayes’ rule, 10–11
Bayes’ theorem, 167–168
Benamou–Brenier formula, 265–266
Bernstein–von Mises theorem, 18–19
Binomial-logit link model, 140, 149
295

Binomial-probit model, 140, 149
Bit error rate (BER), 171–173
Bivariate loss functions, 71–72
Blob method, 275–277
Block Gibbs (BG) sampler
autocorrelation function (ACF) values,
154–156, 155t
for Bayesian logistic mixed models,
153–154, 154b
for Bayesian probit mixed models,
151, 151b
ESS and mESS values, 154–156, 155t
mean squared jumps (MSJ) values,
154–156, 156t
Block-sparse vector recovery, 186–189
Boltzmann transport equation, 130–131
Bonferroni correction method, 68–69
Bootstrap approximation, 23–24
Bregman LogDet divergence, 174–175
C
Cauchy distribution, 76–77
Channel delay spread, 163
Check loss function, 24–25
Chi-square distribution, 22–23
Cluster-based approach, 75
Cluster-SBL framework, 187
Co-LASSO algorithm, 178
Concentration rate, Gibbs posterior, 16–18,
35–38
Conditional simulation, for GLMMs
data augmentation (DA), for Bayesian
GLMMs, 143–147
Hamiltonian Monte Carlo (HMC)
algorithms, 141–143, 142b
Metropolis adjusted Langevin algorithms
(MALA), 136, 139–141, 140b
Conservation-of-mass formula, 263–265
Continuity equation, 263–265
Continuous-time no-jump Markov
process, 251
Convex relaxation-based methods,
163–164
Cotangent vector, 246–247
Coulomb potential
Coulomb wave function, 127–128
Harmonic oscillator (HO), 128
maximum entropy methodology (MEM)
extension, 129
Morse potential (MP), 128
quartic oscillator, ground state of, 129
Coulomb wave function, 127–128
Covariance matching, for sparse support
recovery
Co-LASSO algorithm, 178
MSBL algorithm, 177
multiple measurement vector (MMV),
175–177
SBL approach, 201–202
using Renyi divergence, 177–178
Covariance matrix, of Gibbs posterior, 20–21
Coverage probability function, 23–24
Curl matrix, 252–253
D
Dantzig selector, 163–164
Data augmentation (DA), for GLMMs, 136
Bayesian logistic models, 143, 146–147,
153–154
Bayesian probit mixed models, 143–146,
150–152
steps, 143
Data-generating process (DGP), 2, 4,
209–213, 230
Decentralized sparse Bayesian learning (SBL),
198–199
Deep unfolded sparse Bayesian learning
(SBL), 202
De Finetti’s theorem, 232–233
Delta method, 58
Dependent multiple testing
Bayesian nonmarginal multiple testing,
73–74
choice of G1,…,Gm, 75–76
decision rules, 73
FDR control procedure, 72–73
new error based criterion, 74–75
Deviance information criterion (DIC),
104–105, 108, 228
DGP. See Data-generating process (DGP)
DIC. See Deviance information criterion (DIC)
Dictionary learning, 199–200
Dictionary matrix, 162, 164–165
Diffusion-based generative models, 252–253
Diffusion matrix, 252
Direction-of-arrival (DoA) estimation, 200
Dirichlet process, 87–88, 93
Dynamics-based MCMC methods,
251–253, 252f
E
Effective sample size (ESS), 154–156, 155t
Einstein’s summation convention, 244–245
Empirical Bayes approach, 70–71
296
Index

Empirical risk, 5–7
function, 3
minimizer, 5–6, 8, 13, 15–17, 20–21, 30
Euclidean distance, 15, 77, 78f
Euler–Maruyama method, 139–140,
254–255
Evidence lower-bound (ELBO), 193–194
Expectation maximization (EM)
MSBL algorithm, 174
sparse Bayesian learning (SBL) algorithm,
168–169
Exponentially tilted empirical likelihood
(ETEL) posterior, 30
Exponential map, 247–248, 248f,
281–282
Exterior derivative, 246–247
F
Face-value frequentist approach, 46–47
False discovery (FD), 71–72
False discovery rate (FDR), 68–69, 71–73
False nondiscovery (FN), 71
False nondiscovery rate (FNR), 68, 71–72
Family-wise error rate (FWER), 68–69
FDR. See False discovery rate (FDR)
Feynman–Hellman theorem, 130–131
Fiber-gradient flow, 272–273
Fiber-gradient Hamiltonian (fGH) flow, 272,
274–275
Fiber-Riemannian Poisson (fRP) structure,
272, 274–275
Fiber-Riemannian structure, 272–273
Finite bandwidth sampling, 169–170
First-order AR model, 189
Fisher information-based variational principle,
130–131
Fisher information matrix, 59–61, 217,
254–255
Fisher–Rao metric, 241, 278–280
Fixed interval Kalman smoothing algorithm,
185
Fixed lag Kalman smoothing, 184
Fokker–Planck equation (FPE), 252–253, 255,
263–265, 285
Fourier matrix, 169–170
FPE. See Fokker–Planck equation (FPE)
Fractional Bayes posterior, 6
Free energy, 213–217, 225
estimation, 229–230
and prior, 232
Frequency–inverse document frequency
(tf–iidf ), 240
Full Gibbs (FG) sampler, 154–156
autocorrelation function (ACF) values,
154–156, 155t
for Bayesian logistic mixed models,
153–154, 153b
for Bayesian probit mixed models,
150, 150b
ESS and mESS values, 154–156, 155t
mean squared jumps (MSJ) values, 154–156,
156t
FWER. See Family-wise error rate (FWER)
G
Gamma distribution, 76–77
Gaussian approximation, 19–21
Gaussian distribution, 166
Gaussian kernel, 283–285
Gaussian likelihood, 167–168
Gaussian Markov random field prior, 73–74
Gaussian prior, 166–167
Gaussian scale mixtures (GSM), 166
Gaussian target distribution, 283–284
Gauss–Newton approximations, 282–283
Generalization loss, 213–217, 225–227
estimation, 227–229
and prior, 230–232
Generalized information equality, 20–21
Generalized linear mixed models
(GLMMs), 135
data augmentation (DA) (see Data
augmentation (DA), for GLMMs)
Gibbs sampling, 135–136
Hamiltonian Monte Carlo (HMC)
algorithms, 136, 141–143, 142b, 149
integral approximation, Taylor
expansion for, 135–136
Laplace’s method, 135–136
likelihood function for, 135–139
Markov chain Monte Carlo (MCMC)
algorithms (see Markov chain Monte
Carlo (MCMC) algorithms, for Bayesian
GLMMs)
Metropolis adjusted Langevin algorithms
(MALA), 136, 139–141, 140b, 148–149,
149b
Metropolis–Hastings (MH) algorithm,
135–136
pbDat data, 154–156
sampling-based methods, 135–136
Generalized linear models (GLMs), 135
General posterior calibration (GPC) algorithm,
21, 24–29, 26f, 28–29f, 31
Index
297

Geodesic Monte Carlo (GMC), 259–260
Geometric ergodicity (GE), 156
Gibbs posterior distribution, 3–6, 32–33
asymptotics (see Asymptotic theory)
Bayesian framework, 30
classification, 26–28
construction, principles, 9–11
definition, 6–8
exponentially tilted empirical likelihood
(ETEL) posterior, 30
Gibbs vs. misspecified Bayes, 8–9
learning rate, 3–4, 6–8
learning rate selection, 21–24
minimum clinically important
difference (MCID), 12–14, 14f
nonlinear regression, 28–29
open problems, 31–32
penalized ETEL (PETEL) posterior, 30
problem setup, 4–6
quantile regression, 24–26
quantiles, 11–12
variational approximation, 30
Z-estimation, 30
Gibbs sampling, 251
Glivenko–Cantelli theorem, 16
GLMMs. See Generalized linear mixed models
(GLMMs)
GPC algorithm. See General posterior
calibration (GPC) algorithm
Gradient flow, 247
Gradient flow with smoothed density (GFSD),
277, 285
Gradient flow with smoothed function
(GFSF), 277
Graphical SVGD, 283–284
Greedy methods, 163–164
Gross–Pitaevskii equation, 132–133
Ground-state wave functions (GSAWF),
121–123
Coulomb potential, 127–129
quantum entropy (SQ), maximum entropy
approach (see Quantum entropy (SQ))
H
Haar PX-DA algorithms, 145–146, 146b,
151–152, 152b
autocorrelation function (ACF) values,
154–156, 155t
ESS and mESS values, 154–156, 155t
mean squared jumps (MSJ) values, 154–156,
156t
Hamiltonian dynamics, 255–257, 271–272
Hamiltonian equations, 141–142
Hamiltonian flow, 273–274
Hamiltonian Monte Carlo (HMC), 255–257
conditional simulation, 136, 141–143, 142b
Markov chain Monte Carlo (MCMC)
algorithms, 149
Harmonic oscillator (HO), 128
Hausdorff measure, 250
Hessian matrix, 19
Hessian of convex function, 255
Hierarchical Bayesian framework
Gaussian scale mixtures (GSM), 166
Laplacian scale mixtures (LSM), 166
sparse Bayesian learning (SBL) (see Sparse
Bayesian learning (SBL))
Hierarchical Gaussian scale mixture (GSM),
165
Highest posterior density (HPD), 22
High-throughput sequencing, 70
Hilbert space, 261–262
Hinge loss, 26–27
Hironaka resolution theorem, 220–221
HMC. See Hamiltonian Monte Carlo (HMC)
Holm’s procedure, 68–69
Homeomorphism, 242–243
Hopf–Rinow theorem, 247–248
H-theorem, 125–126
Hyperprior, 166–167
Hyperspheres, 240, 242–243, 258–259
Hypervirial theorems, 127
Hypothesis testing, 103–104
I
Improper priors, 107–108
Information geometry, 241–242, 241f
Intervector correlation
Kalman filter (KF) approach, 178–179
KSBL algorithm (see Kalman SBL (KSBL)
algorithm)
nested SBL (NSBL) algorithm, 186–192
Inverse exponential map, 281–282
Inverse gamma distribution, 166–167
Inverse multiquadric (IMQ) kernel,
284–285
Isometric, 250
Iterative reweighted methods, 163–164
J
Jaccard similarity coefficient, 77, 78f
Jaynes’ maximum entropy methodology,
122–123, 133
Jeffreys–Lindley paradox, 109–110
298
Index

Jeffreys prior, 45, 50, 54–55, 57t, 58
Joint SBL (J-SBL) algorithm, 170–173, 172f,
185–186
Joint-sparse signal recovery, 173–178
K
Kalman filter and smoother (KFS), 178–181,
189–191
Kalman filter (KF) approach, 178–179
Kalman SBL (KSBL) algorithm, 179–181
expectation maximization (EM) updates,
179–181
mean squared error (MSE) performance,
181, 182f
online sparse vector recovery, 181–185
success rate, 181, 182f
wireless channel estimation, 185–186
Kernel density estimator, 277
Kernel Stein discrepancy, 269–270
Kullback–Leibler divergence, 6, 10,
107–108, 213
L
Lag/delay-domain sparse channel estimation,
197
Lagrange multiplier, 74, 122–124, 128, 130
Lagrangian dynamics, 256–257
Langevin diffusion, 90–91, 139–140
Langevin dynamics (LD), 253–255, 257–258,
270–271
Langevin posterior, 90–91, 91f
Laplace’s method, 135–136
Laplacian distribution, 166
Laplacian scale mixtures (LSM), 166
Latent Dirichlet allocation (LDA),
254–255
LD. See Langevin dynamics (LD)
Leapfrog integrator, 256–257
Leapfrog method. See St€ormer–Verlet method
Learning rate, 3–4, 6–8, 21–24
Least absolute shrinkage and selection operator
(LASSO), 163–164
Leave-one-out cross validation (LOO), 214,
228–235
Lebesgue measure, 14–15, 151–152, 248–250,
252–254, 280
Legendre transformation, 279
Leibniz integral rule, 27
Likelihood distribution, 239–241
Likelihood function, for GLMMs, 135–139
Likelihood ratio test, 72
Linear Schrodinger equation, 132–133
Liouville theorem, 255
Local coordinate system, 242–243, 243f
Log density ratio function, 216
Logistic GLMM, 137, 146–147, 153–154
Log-likelihood, 13, 19
Loss function, 3–4
examples, 4–5
hinge loss, 26–27
and prior, 10–11
reverse engineering, 5–6
risk-minimization problem, 6
M
Machine learning, 3–4
Manifold
cotangent vector and differential form,
246–247
divergence and Laplacian, 249–250
embedding, 250, 250f
Grassmann manifold, 283–284
local coordinate system, 242–243, 243f
m-dimensional manifold, 242–243
measure, 248–249
Metropolis adjusted Langevin algorithms
(MALA), 141
relations among, 244–245, 244f
Riemannian manifold (see Riemannian
manifold)
smooth manifold, 243–244, 243f
tangent vector, tangent space, vector field,
244–245, 244f
Marginal likelihood, 104–105, 212
Marginal sampling distribution, 104np
Markov chain Monte Carlo (MCMC)
algorithms, for Bayesian GLMMs,
135–136, 147–148
data augmentation (DA), 136, 150–154
Hamiltonian Monte Carlo (HMC)
algorithms, 136, 149
Metropolis adjusted Langevin algorithms
(MALA), 136, 148–149, 149b
Markov chain Monte Carlo (MCMC)
dynamics, ParVI methods
fiber-gradient Hamiltonian (fGH) flow,
274–275
fiber-Riemannian Poisson (fRP) structure,
272, 274–275
fiber-Riemannian structure and
fiber-gradient flow, 272–273
Hamiltonian dynamics, 271–272
inspiration for, 275–276
Langevin dynamics (LD), 270–271
Index
299

Markov chain Monte Carlo (MCMC)
dynamics, ParVI methods (Continued)
Poisson structure and Hamiltonian flow,
273–274
Markov chain Monte Carlo (MCMC), on
Riemannian manifold
in coordinate space, 253–258
drawback, 260–261
dynamics-based MCMC methods, 251–253,
252f
in embedded space, 258–260
Gibbs sampling, 251
Metropolis–Hastings (MH), 251
Massive multiple-input-multiple-output
(MIMO) OFDM communication
system, 197, 198f
Mass matrix, 141–143
Matern kernel, 284–285
Mathematical theory, of Bayesian
statistics
DGP, model, and prior, 211–213
free energy, 213–217
generalization loss, 213–217
model evaluation, 227–230
not i.i.d. cases, 232–235
phase transitions, 225–227
prior evaluation, 230–232
regular theory, 217–220
singular theory, 220–225
Maximum a posteriori probability (MAP),
163–165, 261–262
Maximum entropy methodology (MEM)
Coulomb potential (see Coulomb potential)
Jaynes’ maximum entropy methodology,
122–123
noncommuting observables, 129–130
quantum maximum entropy approach,
123–124
to statistical mechanics, 121–122
Maximum likelihood (ML), 167–168, 174,
188–189
Maximum-margin classifiers, 26–27
Mean squared error (MSE)
Kalman SBL algorithm, 181, 182f
nested SBL (NSBL) algorithm, 191–192,
191f
SBL, J-SBL/RJ-SBL algorithms,
170–171, 172f
Mean squared jumps (MSJ), 154–156, 156t
Measurement matrix, 162, 164–165, 184, 193
Measurement vector, 162
MEM. See Maximum entropy methodology
(MEM)
Metropolis adjusted Langevin algorithms
(MALA)
conditional simulation, 139–141, 140b
Markov chain Monte Carlo (MCMC)
algorithms, 136, 148–149, 149b
Metropolis–Hastings (MH) algorithm, 63–64,
135–136, 251
Minimum clinically important difference
(MCID), 12–14, 14f
Minimum critical value (MCV), 68–69
Minorization maximization (MM), 193–194
Mirrored Stein operator, 280
Mirrored SVGD, 279–281
Misspecification bias, risk of, 3–4
MMV. See Multiple measurement vector
(MMV)
Model evaluation
free energy, estimation of, 229–230
generalization loss, estimation of, 227–229
Model misspecification bias, 3–4
Momentum space, 131–132
Monte Carlo approximation, 23–24
Monte Carlo EM algorithm, 138–139
Monte Carlo maximum likelihood, 138–139
Morse potential (MP), 128, 130
Mrginal likelihood, 225–227
MSBL algorithm
cost function, interpretation of, 174–175
covariance and mean, 173–174
covariance matching, 177
expectation maximization (EM), 174
hyperparameter vector, 173–174
parametric Gaussian prior, 173–174
posterior distribution of weights, 173–174
Multipath intensity profile (MIP), 170–173
Multiple measurement vector (MMV),
164–165, 173–179
Multiple testing, 67–68
Bayesian multiple testing (see Bayesian
multiple hypothesis testing)
simultaneous hypotheses tests, 67–68, 68t
type-I error, measures of, 68–69
type-II error, measures of, 68
Multivariate ESS (mESS), 154–156, 155t
N
Nested SBL (NSBL) algorithm, 186–192
Nesterov’s acceleration method, 281–282
Newton’s method, 259, 282–283
Neyman–Pearson formalism, 105
Non-Bayesian approach, 2
Noncommuting observables, 129–130
300
Index

Noninformative priors, 50
for exponential families, 56–64
for selective inference, 50–64
Nonlinear regression, 28–29
Nonlinear wave equations, 132–133
Nonmarginal decision (NMD) method, 76–77
Nonmarginal multiple testing method, 73–75
Nonparametric martingale distributions, 92–93
Not i.i.d. cases, 232–235
conditional independent cases, 233–235
exchangeable cases, 232–233
time sequences, 235
No-U-Turn sampler (NUTS), 143
O
OMP algorithm, 164–165
Online sparse vector recovery, 181–185
Orthogonal frequency division multiplexing
(OFDM)-based wireless communication
systems, 169–171
P
Parameter expansion for data augmentation
(PX-DA) algorithms, 145–146, 152
Parametric Bayes models, 19
Parametric martingale sequences, 88–91
Parametric/nonparametric model, 72
Particle-based variational inference (ParVI)
methods
accelerated gradient flow, 281–283
deterministic dynamics, 242
geometric view of, 266–270
kernel, treatment of, 283–286
MCMC dynamics, geometric view of,
270–276
Riemannian-manifold support/particle
space, 278–281
Stein geometry view, 267–270
Stein variational gradient descent (SVGD),
261–262
Wasserstein gradient flow simulation
methods, 277–278
Wasserstein space, 262–267, 264f
Pedestrian B (Ped-B) channel model, 170–171
Penalized ETEL (PETEL) posterior, 30
Per comparison error rate (PCER), 68
PESM. See Power exponential scale mixture
(PESM)
Phase space, 255
Phase transitions, 225–227
with hyperparameter, 227
with sample size, 225–227
Picard–Lindel€of theorem, 245
Pitman–Koopman–Darmois theorem, 56
Poisson-log GLMM, 137, 141, 149
Poisson-log link model, 137
Poisson model, 112
Poisson prior, 28–29
Poisson structure, 273–274
Poschl–Teller potential, 131–132
Positive false discovery rate (pFDR), 68–69
Posterior correlation matrix, 76
Posterior distribution, 1–2, 239–240
Posterior Gaussian distribution, 167–168
Posterior inclusion probability, 70
Postulated Bayesian model, 76–77
Power exponential (PE) distribution, 166
Power exponential scale mixture (PESM)
distributions, 166
Gaussian scale mixtures (GSM), 166
Laplacian scale mixtures (LSM), 166
power exponential (PE) distribution, 166
sparse Bayesian learning (SBL) (see Sparse
Bayesian learning (SBL))
Preconditioned MALA, 141
Predictive copula model, 92
Principle of maximum entropy, 122, 130–131
Prior correlation matrix, 75
Prior distribution, 1–2
Prior evaluation, 230–232
Prior predictive distribution, 110–111
Prior predictive error, 111–112
Probabilistic graphical model, 193
Probability density function (pdf ), 145–146,
151–152, 166–167
Probability distribution (PD), 121–122, 127,
211
Probability-matching priors, 51, 53–54,
55–56f, 56, 61–63
Probit GLMM, 137, 140, 143–146, 150–152
Prtition of unity, 225–227
p-value, posterior predictive distribution,
110–111
Q
Quantile regression, 24–26
Quantized sparse signal recovery, 192–198
Quantized VB (QVB) algorithm, 197–198
Quantum entropy (SQ), 133
H-theorem, 125–126
hypervirial theorems, ground-state wave
functions, 127
preliminaries, 123–124
saturation, 127
Index
301

Quantum entropy (SQ) (Continued)
Shannon’s ignorance function, 124–125
speculation, 127
unique maximum value, 125
virial theorem, ground-state wave functions,
126–127
Quantum maximum entropy approach,
123–124
Quantum pure states, 121–122
Quasi-Newton method, 283
Quasi-Newton ParVI method, 283
R
Randomized estimator, 8, 10
Rank-deficient, 200
Real log canonical threshold (RLCT),
221–222
concrete values, 222–225
geometrical property, 222–225
Recommender system, 240–241
Recursive J-SBL (RJ-SBL), 170–173, 172f
Regular theory, 217–220
Renyi divergence covariance matching,
177–178
Reproducing kernel Hilbert space (RKHS),
261–262
Riemannian accelerated gradient (RAG),
281–282
Riemannian manifold
embedding, 250
exponential map, 247–248, 248f
gradient flow, 247
gradient of function, 247
Markov chain Monte Carlo (MCMC)
methods (see Markov chain Monte Carlo
(MCMC), on Riemannian manifold)
mirrored SVGD, 279–281
parallel transport, in linear space, 247–248,
248f
Riemannian measure, 249–250
Riemannian structure, 247–248
Riemannian SVGD (RSVGD), 278–279
vector addition, in linear space, 247–248,
248f
Riemannian manifold HMC algorithm, 143
Riemannian Nesterov’s method (RNes),
281–282
Riemannian quasi-Newton method, 283
Riemannian SVGD (RSVGD), 278–279
RLCT. See Real log canonical threshold
(RLCT)
Robust principal component analysis, 200–202
S
Sampling methods, geometry in, 286
Bayesian inference, 239–240
hyperbolic latent space, 240–241
hypersphere, 240
information geometry, 241–242, 241f
manifold, concept of (see Manifold)
non-Euclidean geometry, 240
particle-based variational inference (ParVI)
(see Particle-based variational inference
(ParVI) methods)
simplex, latent variables on, 240–241
Stiefel manifold, 240–241
term frequency–inverse document frequency
(tf–idf ) feature, 240
unnormalized density function, 239–240
Savage–Dickey approximation, 108
Sawtooth lag smoothing, 184–185
SBL. See Sparse Bayesian learning (SBL)
Schrodinger equation, 122, 130–131
Selective inference, noninformative priors for,
50–64
Selective normal-location model, 52, 60–61
Shannon’s ignorance function, 124–125
Shape invariance, 131–132
Signal-to-noise ratio (SNR), 170–171
Simes test, 68–69
Simultaneous hypotheses tests, 67–68, 68t
Singular Bayesian information criterion
(sBIC), 229
Singular theory, 220–225
Sparse Bayesian learning (SBL), 202–203
covariance matching-based algorithms,
201–202
decentralized SBL, 198–199
deep unfolded SBL, 202
dictionary learning algorithm, 199–200
expectation maximization (EM) algorithm,
168–169
Gaussian likelihood, 167–168
Gaussian prior, 166–167
hyperparameters, 166–168
KSBL algorithm (see Kalman SBL (KSBL)
algorithm)
maximum likelihood (ML) solution,
167–168
and MSBL (see MSBL algorithm)
nested SBL (NSBL) algorithm, 186–192
parametric prior, 167–168
posterior Gaussian distribution, 167–168
in presence of colored noise, 201
Student-t distribution, 166
302
Index

two-layer hierarchical prior model,
166–167
wireless channel estimation, 169–173
Sparse signal recovery, 163–165
Bayesian methods, 164–165
convex relaxation-based methods, 163–164
Gaussian scale mixtures (GSM), 166
greedy methods, 163–164
intervector correlation, 178–186
iterative reweighted methods, 163–164
joint-sparse signal recovery, 173–178
Laplacian scale mixtures (LSM), 166
maximum a posteriori probability (MAP),
163–165
quantized sparse signal recovery, 192–198
sparse Bayesian learning (SBL) algorithms
(see Sparse Bayesian learning (SBL))
threshold-based methods, 163–164
Spatial generalized linear mixed models
(SGLMMs), 138–139
Spherical admixture model, 240
Sq entropy, 131–132
Standard Brownian motion, 252–254
Statistical mechanics, 121–122
Statistical model, 1–6
Stein–Fisher information, 269–270
Stein-log-Sobolev inequality, 269–270
Stein–Poincare inequality, 269
Stein variational gradient descent (SVGD),
275–277, 285–286
graphical, 283–286
Hilbert space, 261–262
KL divergence, 261–262
maximization, 261–262
maximum a posteriori (MAP) estimate,
261–262
mirrored, 279–281, 285–286
reproducing kernel Hilbert space (RKHS),
261–262
Riemannian SVGD (RSVGD), 278–279
Stein geometry, view from, 267–270
Stiefel manifold, 240–241, 260
Stochastic approximation, 23–24
Stochastic differential equation (SDE),
252, 256
Stochastic gradient, 251
Stochastic gradient descent, 90
Stochastic gradient Hamiltonian dynamics,
257–258
Stochastic-gradient HMC (SGHMC)
dynamics, 257–258
Stochastic gradient Nose–Hoover thermostats
(SGNHT), 257–258
Stochastic-gradient Riemannian HMC
(SGRHMC), 258
Stochastic particle resampling, 283–284
Stokes theorem, 249
St€ormer–Verlet method, 142
Student-t distribution, 166
Supersymmetry formalism, in quantum
mechanics, 131–132
SVGD. See Stein variational gradient descent
(SVGD)
Symmetric splitting integrator (SSI), 256
T
Tangent space, 244f, 245, 259–260
Tangent vector, 244–245, 244f
Threshold-based methods, 163–164
Time-independent Schrodinger equation,
131–132
True positives (TP), 74
Two-block Gibbs sampler
for Bayesian logistic mixed models,
153–154, 154b
for Bayesian probit mixed models,
151–152, 151b
U
Uncertainty quantification, 72
Unnormalized density function, 239–240
Unquantized noisy compressed sensing
system model, 193
V
Variance-stabilizing transformation, 58
Variational Bayesian (VB) inference, 193–194,
197
Vector field, 244f, 245, 265
Virial theorem (VT), 126–127, 129–130
Volume form, 248–249
von Mises-Fisher distribution, 258–259
W
WAIC. See Widely applicable information
criterion (WAIC)
Wasserstein accelerated gradient (WAG),
281–282
Wasserstein distance equation, 277–278
Wasserstein gradient flow simulation
methods, 277–278, 285
Wasserstein Nesterov’s method (WNes),
281–282
Index
303

Wasserstein space, 262–266, 264f
exponential map, 281–282
Hamiltonian flow, 274
view from, 267
Wave function (WF), 126–127
Coulomb wave function, 127–128
ground-state wave functions (GSAWF)
(see Ground-state wave functions
(GSAWF))
Wedge product, 246–247
Wideband wireless channels, 163
Widely applicable information criterion
(WAIC), 108, 214, 228, 231–232
Wiener process, 252
Wilcoxon rank sum test, 72
Wireless channel estimation, 163
and KSBL algorithm, 185–186
and SBL, 169–173
Z
Z-estimation, 30
304
Index



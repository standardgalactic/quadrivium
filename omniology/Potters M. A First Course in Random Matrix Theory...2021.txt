
A FIRST COURSE IN RANDOM MATRIX THEORY
The real world is perceived and broken down as data, models and algorithms in the eyes
of physicists and engineers. Data is noisy by nature and classical statistical tools have so
far been successful in dealing with relatively smaller levels of randomness. The recent
emergence of Big Data and the required computing power to analyze them have rendered
classical tools outdated and insufï¬cient. Tools such as random matrix theory and the study
of large sample covariance matrices can efï¬ciently process these big datasets and help make
sense of modern, deep learning algorithms. Presenting an introductory calculus course for
random matrices, the book focuses on modern concepts in matrix theory, generalizing
the standard concept of probabilistic independence to non-commuting random variables.
Concretely worked out examples and applications to ï¬nancial engineering and portfolio
construction make this unique book an essential tool for physicists, engineers, data analysts
and economists.
marc potters is Chief Investment Ofï¬cer of CFM, an investment ï¬rm based in Paris.
Marc maintains strong links with academia and, as an expert in random matrix theory, he
has taught at UCLA and Sorbonne University.
jean-philippe bouchaud
is a pioneer in econophysics. His research includes ran-
dom matrix theory, statistics of price formation, stock market ï¬‚uctuations, and agent-based
models for ï¬nancial markets and macroeconomics. His previous books include Theory of
Financial Risk and Derivative Pricing (Cambridge University Press, 2003) and Trades,
Quotes and Prices (Cambridge University Press, 2018), and he has been the recipient of
several prestigious, international awards.


A FIRST COURSE IN RANDOM
MATRIX THEORY
for Physicists, Engineers and Data Scientists
MARC POTTERS
Capital Fund Management, Paris
JEAN-PHILIPPE BOUCHAUD
Capital Fund Management, Paris

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314â€“321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi â€“ 110025, India
79 Anson Road, #06â€“04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the Universityâ€™s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781108488082
DOI: 10.1017/9781108768900
Â© Cambridge University Press 2021
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2021
Printed in the United Kingdom by TJ Books Limited
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging-in-Publication Data
Names: Potters, Marc, 1969â€“ author. | Bouchaud, Jean-Philippe, 1962â€“ author.
Title: A ï¬rst course in random matrix theory : for physicists, engineers
and data scientists / Marc Potters, Jean-Philippe Bouchaud.
Description: Cambridge ; New York, NY : Cambridge University Press, 2021. |
Includes bibliographical references and index.
Identiï¬ers: LCCN 2020022793 (print) | LCCN 2020022794 (ebook) |
ISBN 9781108488082 (hardback) | ISBN 9781108768900 (epub)
Subjects: LCSH: Random matrices.
Classiï¬cation: LCC QA196.5 .P68 2021 (print) | LCC QA196.5 (ebook) |
DDC 512.9/434â€“dc23
LC record available at https://lccn.loc.gov/2020022793
LC ebook record available at https://lccn.loc.gov/2020022794
ISBN 978-1-108-48808-2 Hardback
Additional resources for this title at www.cambridge.org/potters
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

Contents
Preface
page ix
List of Symbols
xiv
Part I
Classical Random Matrix Theory
1
1
Deterministic Matrices
3
1.1
Matrices, Eigenvalues and Singular Values
3
1.2
Some Useful Theorems and Identities
9
2
Wigner Ensemble and Semi-Circle Law
15
2.1
Normalized Trace and Sample Averages
16
2.2
The Wigner Ensemble
17
2.3
Resolvent and Stieltjes Transform
19
3
More on Gaussian Matrices*
30
3.1
Other Gaussian Ensembles
30
3.2
Moments and Non-Crossing Pair Partitions
36
4
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
43
4.1
Wishart Matrices
43
4.2
MarË‡cenkoâ€“Pastur Using the Cavity Method
48
5
Joint Distribution of Eigenvalues
58
5.1
From Matrix Elements to Eigenvalues
58
5.2
Coulomb Gas and Maximum Likelihood Conï¬gurations
64
5.3
Applications: Wigner, Wishart and the One-Cut Assumption
69
5.4
Fluctuations Around the Most Likely Conï¬guration
73
5.5
An Eigenvalue Density Saddle Point
78
6
Eigenvalues and Orthogonal Polynomials*
83
6.1
Wigner Matrices and Hermite Polynomials
83
6.2
Laguerre Polynomials
87
6.3
Unitary Ensembles
91
v

vi
Contents
7
The Jacobi Ensemble*
97
7.1
Properties of Jacobi Matrices
97
7.2
Jacobi Matrices and Jacobi Polynomials
102
Part II
Sums and Products of Random Matrices
109
8
Addition of Random Variables and Brownian Motion
111
8.1
Sums of Random Variables
111
8.2
Stochastic Calculus
112
9
Dyson Brownian Motion
121
9.1
Dyson Brownian Motion I: Perturbation Theory
121
9.2
Dyson Brownian Motion II: ItË†o Calculus
124
9.3
The Dyson Brownian Motion for the Resolvent
126
9.4
The Dyson Brownian Motion with a Potential
129
9.5
Non-Intersecting Brownian Motions and the Karlinâ€“McGregor Formula
133
10
Addition of Large Random Matrices
136
10.1
Adding a Large Wigner Matrix to an Arbitrary Matrix
136
10.2
Generalization to Non-Wigner Matrices
140
10.3
The Rank-1 HCIZ Integral
142
10.4
Invertibility of the Stieltjes Transform
145
10.5
The Full-Rank HCIZ Integral
149
11
Free Probabilities
155
11.1
Algebraic Probabilities: Some Deï¬nitions
155
11.2
Addition of Commuting Variables
156
11.3
Non-Commuting Variables
161
11.4
Free Product
170
12
Free Random Matrices
177
12.1
Random Rotations and Freeness
177
12.2
R-Transforms and Resummed Perturbation Theory
181
12.3
The Central Limit Theorem for Matrices
183
12.4
Finite Free Convolutions
186
12.5
Freeness for 2 Ã— 2 Matrices
193
13
The Replica Method*
199
13.1
Stieltjes Transform
200
13.2
Resolvent Matrix
204
13.3
Rank-1 HCIZ and Replicas
209
13.4
Spin-Glasses, Replicas and Low-Rank HCIZ
215

Contents
vii
14
Edge Eigenvalues and Outliers
220
14.1
The Tracyâ€“Widom Regime
221
14.2
Additive Low-Rank Perturbations
223
14.3
Fat Tails
229
14.4
Multiplicative Perturbation
231
14.5
Phase Retrieval and Outliers
234
Part III
Applications
241
15
Addition and Multiplication: Recipes and Examples
243
15.1
Summary
243
15.2
R- and S-Transforms and Moments of Useful Ensembles
245
15.3
Worked-Out Examples: Addition
249
15.4
Worked-Out Examples: Multiplication
252
16
Products of Many Random Matrices
257
16.1
Products of Many Free Matrices
257
16.2
The Free Log-Normal
261
16.3
A Multiplicative Dyson Brownian Motion
262
16.4
The Matrix Kesten Problem
264
17
Sample Covariance Matrices
267
17.1
Spatial Correlations
267
17.2
Temporal Correlations
271
17.3
Time Dependent Variance
276
17.4
Empirical Cross-Covariance Matrices
278
18
Bayesian Estimation
281
18.1
Bayesian Estimation
281
18.2
Estimating a Vector: Ridge and LASSO
288
18.3
Bayesian Estimation of the True Covariance Matrix
295
19
Eigenvector Overlaps and Rotationally Invariant Estimators
297
19.1
Eigenvector Overlaps
297
19.2
Rotationally Invariant Estimators
301
19.3
Properties of the Optimal RIE for Covariance Matrices
309
19.4
Conditional Average in Free Probability
310
19.5
Real Data
311
19.6
Validation and RIE
317
20
Applications to Finance
321
20.1
Portfolio Theory
321
20.2
The High-Dimensional Limit
325
20.3
The Statistics of Price Changes: A Short Overview
330
20.4
Empirical Covariance Matrices
334

viii
Contents
Appendix
Mathematical Tools
339
A.1
Saddle Point Method
339
A.2
Tricomiâ€™s Formula
341
A.3
Toeplitz and Circulant Matrices
343
Index
347

Preface
Physicists have always approached the world through data and models inspired by this
data. They build models from data and confront their models with the data generated by
new experiments or observations. But real data is by nature noisy; until recently, classical
statistical tools have been successful in dealing with this randomness. The recent emergence
of very large datasets, together with the computing power to analyze them, has created a
situation where not only the number of data points is large but also the number of studied
variables. Classical statistical tools are inadequate to tackle this situation, called the large
dimension limit (or the Kolmogorov limit). Random matrix theory, and in particular the
study of large sample covariance matrices, can help make sense of these big datasets, and
is in fact also becoming a useful tool to understand deep learning. Random matrix theory
is also linked to many modern problems in statistical physics such as the spectral theory of
random graphs, interaction matrices of spin-glasses, non-intersecting random walks, many-
body localization, compressed sensing and many more.
This book can be considered as one more book on random matrix theory. But our aim
was to keep it purposely introductory and informal. As an analogy, high school seniors
and college freshmen are typically taught both calculus and analysis. In analysis one learns
how to make rigorous proofs, deï¬ne a limit and a derivative. At the same time in calculus
one can learn about computing complicated derivatives, multi-dimensional integrals and
solving differential equations relying only on intuitive deï¬nitions (with precise rules) of
these concepts. This book proposes a â€œcalculusâ€ course for random matrices, based in
particular on the relatively new concept of â€œfreenessâ€, that generalizes the standard concept
of probabilistic independence to non-commuting random variables.
Rather than make statements about the most general case, concepts are deï¬ned with
some strong hypothesis (e.g. Gaussian entries, real symmetric matrices) in order to simplify
the computations and favor understanding. Precise notions of norm, topology, convergence,
exact domain of application are left out, again to favor intuition over rigor. There are many
good, mathematically rigorous books on the subject (see references below) and the hope is
that our book will allow the interested reader to read them guided by his/her newly built
intuition.
ix

x
Preface
Readership
The book was initially conceived as a textbook for a graduate level standard 30 hours course
in random matrix theory, for physicists or applied mathematicians, given by one of us (MP)
during a sabbatical at UCLA in 2017â€“2018. As the book evolved many new developments,
special topics and applications have been included. Lecturers can then customize their
course offering by complementing the ï¬rst few essential chapters with their own choice
of chapters or sections from the rest of the book.
Another group of potential readers are seasoned researchers analyzing large datasets
who have heard that random matrix theory may help them distinguish signal from noise
in singular value decompositions or eigenvalues of sample covariance matrices. They have
heard of the MarË‡cenkoâ€“Pastur distribution but do not know how to extend it to more real-
istic settings where they might have non-Gaussian noise, true outliers, temporal (sample)
correlations, etc. They need formulas to compute null hypothesis and so forth. They want
to understand where these formulas come from intuitively without requiring full precise
mathematical proofs.
The reader is assumed to have a background in undergraduate mathematics taught in
science and engineering: linear algebra, complex variables and probability theory. Impor-
tant results from probability theory are recalled in the book (addition of independent vari-
ables, law of large numbers and central limit theorem, etc.) while stochastic calculus and
Bayesian estimation are not assumed to be known. Familiarity with physics approximation
techniques (Taylor expansions, saddle point approximations) is helpful.
How to Read This Book
We have tried to make the book accessible for readers of different levels of expertise. The
bulk of the text is hopefully readable by graduate students, with most calculations laid
out in detail. We provide exercises in most chapters, which should allow the reader to
check that he or she has understood the main concepts. We also tried to illustrate the book
with as many ï¬gures as possible, because we strongly believe (as physicists) that pictures
considerably help forming an intuition about the issue at stake.
More technical issues, directed at experts in rmt or statistical physics, are signaled by
the use of a different, smaller font and an extra margin space. Chapters 3, 6, 7 and 13
are marked with a star, meaning that they are not essential for beginners and they can be
skipped at ï¬rst reading.
At the end of each chapter, we give a non-exhaustive list of references, some general and
others more technical and specialized, which direct the reader to more in-depth information
related to the subject treated in the chapter.
Other Books on Related Subjects
Books for Mathematicians
There are many good recent books on random matrix theory and free probabilities written
by and for mathematicians: Blower [2009], Anderson et al. [2010], Bai and Silverstein

Preface
xi
[2010], Pastur and Scherbina [2010], Tao [2012], ErdËos and Yau [2017], Mingo and
Speicher [2017]. These books are often too technical for the intended readership of the
present book. We nevertheless rely on these books to extract some relevant material for our
purpose.
Books for Engineers
Communications engineers and now ï¬nancial engineers have become big users of random
matrix theory and there are at least two books speciï¬cally geared towards them. The style
of engineering books is closer to the style of the present book and these books are quite
readable for a physics audience.
There is the short book by Tulino and VerdÂ´u [2004]; it gets straight to the point and gives
many useful formulas from free probabilities to compute the spectrum of random matrices.
The ï¬rst part of this book covers some of the topics covered here, but many other subjects
more related to statistical physics and to ï¬nancial applications are absent.
Part I of Couillet and Debbah [2011] has a greater overlap with the present book. Again
about half the topics covered here are not present in that book (e.g. Dyson Brownian
motion, replica trick, low-rank hciz and the estimation of covariance matrices).
Books for Physicists
Physicists interested in random matrix theory fall into two broad categories:
Mathematical physicists and high-energy physicists use it to study fundamental quan-
tum interactions, from Wignerâ€™s distribution of nuclear energy level spacing to models of
quantum gravity using matrix models of triangulated surfaces.
Statistical physicists encounter random matrices in the interaction matrices of spin-
glasses, in the study of non-intersecting random walks, in the spectral analysis of large
random graphs, in the theory of Anderson localization and many-body localization, and
ï¬nally in the study of sample covariance matrices from large datasets. This book focuses
primarily on statistical physics and data analysis applications.
The classical book by Mehta [2004] is at the crossroads of these different approaches,
whereas Forrester [2010], BrÂ´ezin and Hikami [2016] and Eynard et al. [2006] are examples
of books written by mathematical physicists. Livan et al. [2018] is an introductory book
geared towards statistical physicists. That book is very similar in spirit to ours. The topics
covered do not overlap entirely; for example Livan et al. do not cover the Dyson Brownian
motion, the hciz integral, the problem of eigenvector overlaps, sample covariance matrices
with general true covariance, free multiplication, etc.
We should also mention the handbook by Akemann et al. [2011] and the Les Houches
summer school proceedings [Schehr et al., 2017], in which we co-authored a chapter on
ï¬nancial applications of rmt. That book covers a very wide range of topics. It is a useful
complement to this book but too advanced for most of the intended readers.
Finally, the present book has some overlap with a review article written with Joel Bun
[Bun et al., 2017].

xii
Preface
Acknowledgments
The two of us want to warmly thank the research team at CFM with whom we have had
many illuminating discussions on these topics over the years, and in particular Jean-Yves
Audibert, Florent Benaych-Georges, Raphael BÂ´enichou, RÂ´emy Chicheportiche, Stefano
Ciliberti, Sungmin Hwang, Vipin Kerala Varma, Laurent Laloux, Eric Lebigot, Thomas
Madaule, Iacopo Mastromatteo, Pierre-Alain Reigneron, Adam Rej, Jacopo Rocchi,
Emmanuel SÂ´eriÂ´e, Konstantin Tikhonov, Bence Toth and Dario Vallamaina.
We also want to thank our academic colleagues for numerous, very instructive inter-
actions, collaborations and comments, including Gerard Ben Arous, Michael Benzaquen,
Giulio Biroli, Edouard BrÂ´ezin, ZdzisÅ‚aw Burda, BenoË†Ä±t Collins, David Dean, Bertrand
Eynard, Yan Fyodorov, Thomas Guhr, Alice Guionnet, Antti Knowles, Reimer Kuehn,
Pierre Le Doussal, Fabrizio Lillo, Satya Majumdar, Marc MÂ´ezard, Giorgio Parisi, Sandrine
PÂ´echÂ´e, Marco Tarzia, Matthieu Wyart, Francesco Zamponi and Tony Zee.
We want to thank some of our students and post-docs for their invaluable contribution
to some of the topics covered in this book, in particular Romain Allez, Joel Bun, Tristan
GautiÂ´e and Pierre Mergny. We also thank Pierre-Philippe CrÂ´epin, ThÂ´eo Dessertaine, Tristan
GautiÂ´e, Armine Karami and JosÂ´e Moran for carefully reading the manuscript.
Finally, Marc Potters wants to thank Fan Yang, who typed up the original hand-written
notes. He also wants to thank Andrea Bertozzi, Stanley Osher and Terrence Tao, who
welcomed him for a year at UCLA. During that year he had many fruitful discussions with
members and visitors of the UCLA mathematics department and with participants of the
IPAM long program in quantitative linear algebra, including Alice Guionnet, Horng-Tzer
Yau, Jun Yin and more particularly with Nicholas Cook, David Jekel, Dimitri Shlyakhtenko
and Nikhil Srivastava.
Bibliographical Notes
Here is a list of books on random matrix theory that we have found useful.
â€¢ Books for mathematicians
â€“ G. Blower. Random Matrices: High Dimensional Phenomena. Cambridge University
Press, Cambridge, 2009,
â€“ G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010,
â€“ Z. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matri-
ces. Springer-Verlag, New York, 2010,
â€“ L. Pastur and M. Scherbina. Eigenvalue Distribution of Large Random Matrices.
American Mathematical Society, Providence, Rhode Island, 2010,
â€“ T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
â€“ L. ErdËos and H.-T. Yau. A Dynamical Approach to Random Matrix Theory. American
Mathematical Society, Providence, Rhode Island, 2017,

Preface
xiii
â€“ J. A. Mingo and R. Speicher. Free Probability and Random Matrices. Springer, New
York, 2017.
â€¢ Books for physicists and mathematical physicists
â€“ M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
â€“ B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006,
â€“ P. J. Forrester. Log Gases and Random Matrices. Princeton University Press,
Princeton, NJ, 2010,
â€“ G. Akemann, J. Baik, and P. D. Francesco. The Oxford Handbook of Random Matrix
Theory. Oxford University Press, Oxford, 2011,
â€“ E. BrÂ´ezin and S. Hikami. Random Matrix Theory with an External Source. Springer,
New York, 2016,
â€“ G. Schehr, A. Altland, Y. V. Fyodorov, N. Oâ€™Connell, and L. F. Cugliandolo, editors.
Stochastic Processes and Random Matrices, Les Houches Summer School, 2017.
Oxford University Press, Oxford,
â€“ G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018.
â€¢ More â€œappliedâ€ books
â€“ A. M. Tulino and S. VerdÂ´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,
â€“ R. Couillet and M. Debbah. Random Matrix Methods for Wireless Communications.
Cambridge University Press, Cambridge, 2011.
â€¢ Our own review paper on the subject, with signiï¬cant overlap with this book
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1â€“109, 2017.

Symbols
Abbreviations
bbp:
Baik, Ben Arous, PÂ´echÂ´e
cdf:
Cumulative Distribution Function
clt:
Central Limit Theorem
dbm:
Dyson Brownian Motion
ema:
Exponential Moving Average
ï¬d:
Free Identically Distributed
goe:
Gaussian Orthogonal Ensemble
gse:
Gaussian Symplectic Ensemble
gue:
Gaussian Unitary Ensemble
hciz:
Harish-Chandraâ€“Itzyksonâ€“Zuber
iid:
Independent Identically Distributed
lln:
Law of Large Numbers
map:
Maximum A Posteriori
mave:
Mean Absolute Value
mmse:
Minimum Mean Square Error
pca:
Principal Component Analysis
pde:
Partial Differential Equation
pdf:
Probability Distribution Function
rie:
Rotationally Invariant Estimator
rmt:
Random Matrix Theory
scm:
Sample Covariance Matrix
sde:
Stochastic Differential Equation
svd:
Singular Value Decomposition
Conventions
0+:
inï¬nitesimal positive quantity
1:
identity matrix
xiv

List of Symbols
xv
âˆ¼:
scales as, of the order of, also, for random variables, drawn from
â‰ˆ:
approximately equal to (mathematically or numerically)
âˆ:
proportional to
:= :
equal by deï¬nition to
â‰¡:
identically equal to
+ :
free sum
Ã— :
free product
E[.]:
mathematical expectation
V[.]:
mathematical variance
âŸ¨.âŸ©:
empirical average
[x]:
dimension of x
i:
âˆš
âˆ’1
Re:
real part
Im:
imaginary part
âˆ’
:
principal value integral
Â±
âƒâˆšÂ·:
special square root, Eq. (4.56)
AT :
matrix transpose
supp(Ï):
domain where Ï(Â·) is non-zero
Note: most of the time f (t) means that t is a continuous variable, and ft means that t is
discrete.
Roman Symbols
A:
generic constant, or generic free variable
A:
generic matrix
a:
generic coefï¬cient, as in the gamma distribution, Eq. (4.17), or in the free
log-normal, Eq. (16.15)
B:
generic free variable
B:
generic matrix
b:
generic coefï¬cient, as in the gamma distribution, Eq. (4.17), or in the free
log-normal, Eq. (16.15)
C:
generic coefï¬cient
Ck:
Catalan numbers
C:
often population, or â€œtrueâ€ covariance matrix, sometimes C = A + B
C:
total investable capital or cross-covariance matrix
ck:
cumulant of order k
d:
distance between eigenvalues
dB:
Wiener noise
E:
sample, or empirical matrix; matrix corrupted by noise
E:
error
e:
normalized vector of 1â€™s e = (1,1, . . . ,1)T /
âˆš
N

xvi
List of Symbols
e1:
unit vector in the ï¬rst direction e1 = (1,0, . . . ,0)T
F:
free energy
F(Â·):
generic function
FÎ²(Â·):
Tracyâ€“Widom distributions
FK(Â·, Â· , Â· Â·):
K-point correlation function of eigenvalues
F:
dual of the sample covariance matrix, see Section 4.1.1
F(x):
force ï¬eld
F :
generalized force in the Fokkerâ€“Planck equation
Fn:
replica free energy
f (Â·):
generic function
G(Â·):
resolvent matrix
G:
return target of a portfolio
gN(Â·):
normalized trace of the resolvent
g:
vector of expected gains
g(Â·):
generic Stieltjes transform
gA(Â·):
Stieltjes transform of the spectrum of A
gX(Â·):
Stieltjes transform of a Wigner matrix
H:
rectangular N Ã— T matrix with random entries
H(Â·):
log of the generating function of a random variable, H(k) := log E[eikX]
HX(Â·):
log of the generating function (rank-1 hciz) of a random matrix
Ë†H:
annealed version of HX(t)
Hn:
Hermite polynomial (probabilistsâ€™ convention)
H:
Hamiltonian of a spin system
h:
auxiliary function
h(Â·):
real part of the Stieltjes transform on the real axis (equal to the Hilbert
transform times Ï€)
I:
generic integral
I(A,B):
hciz integral
It(A):
hciz integral when B is of rank-1 with eigenvalue t
I:
interval
i,j,k,â„“:
generic integer indices
J:
Jacobi matrices, or interaction matrix in spin-glasses
K:
number of terms in a sum or a product, or number of blocks in a cross-
validation experiment
K(Â·):
kernel
KÎ·(Â·):
Cauchy kernel, of width Î·
K:
Toeplitz matrix
L:
length of an interval containing eigenvalues
L(Î±)
n :
generalized Laguerre polynomial
LC,Î²
Î¼
:
LÂ´evy stable laws
L:
lower triangular matrix

List of Symbols
xvii
L:
log-likelihood
M:
generic matrix
W
p:
inverse-Wishart matrix with coefï¬cient p
mk:
moment of order k
N:
size of the matrix, number of variables
N(Î¼,Ïƒ 2):
Gaussian distribution with mean Î¼ and variance Ïƒ 2
N(Î¼,C):
multivariate Gaussian distribution with mean Î¼ and covariance C
n:
number of eigenvalues in some interval, or number of replicas
O(N):
orthogonal group in N dimensions
O:
generic orthogonal matrix
P(Â·):
generic probability distribution function deï¬ned by its argument: P(x) is
a short-hand for the probability density of variable x
PÎ³ (Â·):
gamma distribution
P>(Â·):
complementary cumulative distribution function
P0(Â·):
prior distribution in a Bayesian framework
Pi(t):
probability to be in state i at time t
Pn:
Legendre polynomial
P(x|y):
conditional distribution of x knowing y
P (a,b)
n
:
Jacobi polynomial
P:
rank-1 (projector) matrix
P[X]:
probability of event X
p:
variance of the inverse-Wishart distribution, or quantile value
p(x):
generic polynomial of x
p(A):
generic matrix polynomial of A
pN(Â·):
generic monic orthogonal polynomial
p(y,t|x):
propagator of Brownian motion, with initial position x at t = 0
Q(Â·):
generic polynomial
QN(Â·):
(expected) characteristic polynomial
q:
ratio of size of matrix N to size of sample T : q = N/T
qâˆ—:
effective size ratio qâˆ—= N/T âˆ—
q(A):
generic matrix polynomial of A
qN(Â·):
normalized characteristic polynomial
R:
circle radius
RA(Â·):
R-transform of the spectrum of A
R:
portfolio risk, or error
r:
signal-to-noise ratio; also an auxiliary variable in the spin-glass section
ri,t:
return of asset i at time t
SA(Â·):
S-transform of the spectrum of A
S:
diagonal matrix of singular values
s:
generic singular value
T :
size of the sample

xviii
List of Symbols
T âˆ—:
effective size of the sample, accounting for correlations
Tn:
Chebyshev polynomials of the ï¬rst kind
T:
matrix T-transform
tA(Â·):
T-transform of the spectrum of A
U(N):
unitary group in N dimensions
Un:
Chebyshev polynomials of the second kind
U:
generic rotation matrix deï¬ning an eigenbasis
u:
eigenvector (often of the population matrix E)
V (Â·):
generic potential
V:
generic rotation matrix deï¬ning an eigenbasis
V:
generalized potential, or variogram
v(x,t):
velocity ï¬eld in Matytsinâ€™s formalism
v:
eigenvector (often of the sample matrix C)
W(Â·):
Fokkerâ€“Planck auxiliary function
Wn(Ï€,Ïƒ):
Wiengarten coefï¬cient
W:
white Wishart matrix, sometimes generic multiplicative noise matrix
Wq:
white Wishart matrix of parameter q
w:
generic weight
X:
Wigner matrix, sometimes generic additive noise matrix
x:
generic variable
x:
generic vector
Ë†x:
estimator of x
yk:
data points
y:
generic vector
Z:
normalization, or partition function
Zn:
Kesten variable
z:
generic complex variable
z(Â·):
functional inverse of the Stieltjes function g(Â·)
Greek Symbols
Î±:
generic scale factor
Î²:
effective inverse â€œtemperatureâ€ for the Coulomb gas, deï¬ning the symmetry
class of the matrix ensemble (Î² = 1 for orthogonal, Î² = 2 for unitary,
Î² = 4 for symplectic)
Î²i:
exposure of asset i to the common (market) factor
Î²:
vector of Î²iâ€™s

:
gamma function

N:
multivariate gamma function
Î³ :
generic parameter or generic quantity
Î³c:
inverse correlation time

List of Symbols
xix
:
Jacobian matrix, or discrete Laplacian
(x):
Vandermonde determinant := 
i<j(xj âˆ’xi)
Î´:
small increment or small distance
Î´(Â·):
Dirac Î´-function
Ïµ:
generic small quantity Ïµ â‰ª1
Ïµi:
deviation of eigenvalue i from its most likely position
Îµ:
generic iid noise
Î¶:
ridge regression parameter
Î¶(Â·):
inverse of the T-function t(Â·)
Î·:
small quantity, 1/N â‰ªÎ· â‰ª1
Î¸:
edge exponent
Î¸,Î¸kâ„“:
angle and generalized angles
(Â·):
Heaviside function, (x < 0) = 0, (x > 0) = 1
Îºk:
free cumulant of order k
:
diagonal matrix
Î»:
eigenvalue, often those of the sample (or â€œempiricalâ€) matrix
Î»1:
largest eigenvalue of a given matrix
Î»+:
upper edge of spectrum
Î»âˆ’:
lower edge of spectrum
Î¼:
eigenvalue, often those of the population (or â€œtrueâ€) matrix
Ï€:
portfolio weights, or partition
N(Â·):
auxiliary polynomial
(Â·):
limit polynomial
(x,t):
pressure ï¬eld in Matytsinâ€™s formalism
:
Rotationally Invariant Estimator (rie)
Î¾i:
rie eigenvalues
Î¾(Â·):
rie shrinkage function
Ï:
correlation coefï¬cient
Ï(Â·):
eigenvalue distribution
Ï(x,t):
density ï¬eld in Matytsinâ€™s formalism
Ï±:
square overlap between two vectors
Ïƒ:
volatility, or root-mean-square
Ï„(Â·):
normalized trace 1/N Tr(Â·), or free expectation operator
Ï„R(Â·):
normalized trace, further averaged over rotation matrices appearing inside
the trace operator
Ï„:
time lag
Ï„c:
correlation time
Ï’:
volume in matrix space
(Â·):
auxiliary function
(Î»,Î¼):
scaled squared overlap between the eigenvectors of the sample (Î») and
population (Î¼) matrices

xx
List of Symbols
Ï†:
angle
Ï†(Â·):
effective two-body potential in Matytsinâ€™s formalism
Ï•(Â·):
generating function, or Fourier transform of a probability distribution
(Î»,Î»â€²):
scaled squared overlap between the eigenvectors of two sample matrices
(Â·),Ïˆ(Â·):
auxiliary functions
Ïˆ:
auxiliary integration vector
 :
generic rotation matrix
Ï‰(Â·):
generic eigenvalue ï¬eld in a large deviation formalism (cf. Section 5.5)

Part I
Classical Random Matrix Theory


1
Deterministic Matrices
Matrices appear in all corners of science, from mathematics to physics, computer science,
biology, economics and quantitative ï¬nance. In fact, before Schrodingerâ€™s equation, quan-
tum mechanics was formulated by Heisenberg in terms of what he called â€œMatrix Mechan-
icsâ€. In many cases, the matrices that appear are deterministic, and their properties are
encapsulated in their eigenvalues and eigenvectors. This ï¬rst chapter gives several elemen-
tary results in linear algebra, in particular concerning eigenvalues. These results will be
extremely useful in the rest of the book where we will deal with random matrices, and in
particular the statistical properties of their eigenvalues and eigenvectors.
1.1 Matrices, Eigenvalues and Singular Values
1.1.1 Some Problems Where Matrices Appear
Let us give three examples motivating the study of matrices, and the different forms that
those can take.
Dynamical System
Consider a generic dynamical system describing the time evolution of a certain
N-dimensional vector x(t), for example the three-dimensional position of a point in
space. Let us write the equation of motion as
dx
dt = F(x),
(1.1)
where F(x) is an arbitrary vector ï¬eld. Equilibrium points xâˆ—are such that F(xâˆ—) = 0.
Consider now small deviations from equilibrium, i.e. x = xâˆ—+ Ïµy where Ïµ â‰ª1. To ï¬rst
order in Ïµ, the dynamics becomes linear, and given by
dy
dt = Ay,
(1.2)
where A is a matrix whose elements are given by Aij = âˆ‚jFi(xâˆ—), where i,j are indices
that run from 1 to N. When F can itself be written as the gradient of some potential V , i.e.
Fi = âˆ’âˆ‚iV (x), the matrix A becomes symmetric, i.e. Aij = Aji = âˆ’âˆ‚ijV . But this is not
3

4
Deterministic Matrices
always the case; in general the linearized dynamics is described by a matrix A without any
particular property â€“ except that it is a square N Ã— N array of real numbers.
Master Equation
Another standard setting is the so-called Master equation for the evolution of probabilities.
Call i = 1, . . . ,N the different possible states of a system and Pi(t) the probability to
ï¬nd the system in state i at time t. When memory effects can be neglected, the dynamics
is called Markovian and the evolution of Pi(t) is described by the following discrete time
equation:
Pi(t + 1) =
N

j=1
AijPj(t),
(1.3)
meaning that the system has a probability Aij to jump from state j to state i between t and
t +1. Note that all elements of A are positive; furthermore, since all jump possibilities must
be exhausted, one must have, for each j, 
i Aij = 1. This ensures that 
i Pi(t) = 1 at
all times, since
N

i=1
Pi(t + 1) =
N

i=1
N

j=1
AijPj(t) =
N

j=1
N

i=1
AijPj(t) =
N

j=1
Pj(t) = 1.
(1.4)
Matrices such that all elements are positive and such that the sum over all rows is equal
to unity for each column are called stochastic matrices. In matrix form, Eq. (1.3) reads
P(t + 1) = AP(t), leading to P(t) = AtP(0), i.e. A raised to the t-th power applied to the
initial distribution.
Covariance Matrices
As a third important example, let us consider random, N-dimensional real vectors X, with
some given multivariate distribution P(X). The covariance matrix C of the Xâ€™s is deï¬ned as
Cij = E[XiXj] âˆ’E[Xi]E[Xj],
(1.5)
where E means that we are averaging over the distribution P(X). Clearly, the matrix C is
real and symmetric. It is also positive semi-deï¬nite, in the sense that for any vector x,
xT Cx â‰¥0.
(1.6)
If it were not the case, it would be possible to ï¬nd a linear combination of the vectors X
with a negative variance, which is obviously impossible.
The three examples above are all such that the corresponding matrices are N Ã—N square
matrices. Examples where matrices are rectangular also abound. For example, one could
consider two sets of random real vectors: X of dimension N1 and Y of dimension N2. The
cross-covariance matrix deï¬ned as

1.1 Matrices, Eigenvalues and Singular Values
5
Cia = E[XiYa] âˆ’E[Xi]E[Ya];
i = 1, . . . ,N1;
a = 1, . . . ,N2,
(1.7)
is an N1 Ã— N2 matrix that describes the correlations between the two sets of vectors.
1.1.2 Eigenvalues and Eigenvectors
One learns a great deal about matrices by studying their eigenvalues and eigenvectors. For
a square matrix A a pair of scalar and non-zero vector (Î»,v) satisfying
Av = Î»v
(1.8)
is called an eigenvalueâ€“eigenvector pair.
Trivially if v is an eigenvector Î±v is also an eigenvector when Î± is a non-zero real
number. Sometimes multiple non-collinear eigenvectors share the same eigenvalue; we say
that this eigenvalue is degenerate and has multiplicity equal to the dimension of the vector
space spanned by its eigenvectors.
If Eq. (1.8) is true, it implies that the equation (A âˆ’Î»1)v = 0 has non-trivial solutions,
which requires that det(Î»1 âˆ’A) = 0. The eigenvalues Î» are thus the roots of the so-called
characteristic polynomial of the matrix A, obtained by expanding det(Î»1âˆ’A). Clearly, this
polynomial1 is of order N and therefore has at most N different roots, which correspond
to the (possibly complex) eigenvalues of A. Note that the characteristic polynomial of AT
coincides with the characteristic polynomial of A, so the eigenvalues of A and AT are
identical.
Now, let Î»1,Î»2, . . . ,Î»N be the N eigenvalues of A with v1,v2, . . . ,vN the corresponding
eigenvectors. We deï¬ne  as the N Ã— N diagonal matrix with Î»i on the diagonal, and V
as the N Ã— N matrix whose jth column is vj, i.e. Vij = (vj)i is the ith component of vj.
Then, by deï¬nition,
AV = V,
(1.9)
since once expanded, this reads

k
AikVkj = VijÎ»j,
(1.10)
or Avj = Î»jvj. If the eigenvectors are linearly independent (which is not true for all
matrices), the matrix inverse Vâˆ’1 exists and one can therefore write A as
A = VVâˆ’1,
(1.11)
which is called the eigenvalue decomposition of the matrix A.
Symmetric matrices (such that A = AT ) have very nice properties regarding their
eigenvalues and eigenvectors.
1 The characteristic polynomial QN (Î») = det(Î»1 âˆ’A) always has a coefï¬cient 1 in front of its highest power (QN (Î») =
Î»N + O(Î»Nâˆ’1)), such polynomials are called monic.

6
Deterministic Matrices
â€¢ They have exactly N eigenvalues when counted with their multiplicity.
â€¢ All their eigenvalues and eigenvectors are real.
â€¢ Their eigenvectors are orthogonal and can be chosen to be orthonormal (i.e. vT
i vj =
Î´ij). Here we assume that for degenerate eigenvalues we pick an orthogonal set of
corresponding eigenvectors.
If we choose orthonormal eigenvectors, the matrix V has the property VT V = 1 (â‡’
VT = Vâˆ’1). Hence it is an orthogonal matrix V = O and Eq. (1.11) reads
A = OOT,
(1.12)
where  is a diagonal matrix containing the eigenvalues associated with the eigenvectors
in the columns of O. A symmetric matrix can be diagonalized by an orthogonal matrix.
Remark that an N Ã— N orthogonal matrix is fully parameterized by N(N âˆ’1)/2 â€œanglesâ€,
whereas  contains N diagonal elements. So the total number of parameters of the diagonal
decomposition is N(N âˆ’1)/2 + N, which is identical, as it should be, to the number of
different elements of a symmetric N Ã— N matrix.
Let us come back to our dynamical system example, Eq. (1.2). One basic question is to
know whether the perturbation y will grow with time, or decay with time. The answer to
this question is readily given by the eigenvalues of A. For simplicity, we assume F to be
a gradient such that A is symmetric. Since the eigenvectors of A are orthonormal, one can
decompose y in term of the vâ€™s as
y(t) =
N

i=1
ci(t)vi.
(1.13)
Taking the dot product of Eq. (1.2) with vi then shows that the dynamics of the coefï¬cients
ci(t) are decoupled and given by
dci
dt = Î»ici,
(1.14)
where Î»i is the eigenvalue associated with vi. Therefore, any component of the initial
perturbation y(t = 0) that is along an eigenvector with positive eigenvalue will grow expo-
nentially with time, until the linearized approximation leading to Eq. (1.2) breaks down.
Conversely, components along directions with negative eigenvalues decrease exponentially
with time. An equilibrium xâˆ—is called stable provided all eigenvalues are negative, and
marginally stable if some eigenvalues are zero while all others are negative.
The important message carried by the example above is that diagonalizing a matrix
amounts to ï¬nding a way to decouple the different degrees of freedom, and convert a matrix
equation into a set of N scalar equations, as Eqs. (1.14). We will see later that the same
idea holds for covariance matrices as well: their diagonalization allows one to ï¬nd a set of
uncorrelated vectors. This is usually called Principal Component Analysis (pca).

1.1 Matrices, Eigenvalues and Singular Values
7
Exercise 1.1.1
Instability of eigenvalues of non-symmetric matrices
Consider the N Ã— N
square band diagonal matrix M0 deï¬ned by
[M0]ij = 2Î´i,jâˆ’1:
M0 =
â›
âœâœâœâœâœâ
0
2
0
Â· Â· Â·
0
0
0
2
Â· Â· Â·
0
0
0
0
...
0
0
0
0
Â· Â· Â·
2
0
0
0
Â· Â· Â·
0
â
âŸâŸâŸâŸâŸâ 
.
(1.15)
(a)
Show that MN
0
=
0 and so all the eigenvalues of M0 must be zero.
Use a numerical eigenvalue solver for non-symmetric matrices and conï¬rm
numerically that this is the case for N = 100.
(b)
If O is an orthogonal matrix (OOT = 1), OM0OT has the same eigenvalues
as M0. Following Exercise 1.2.4, generate a random orthogonal matrix O.
Numerically ï¬nd the eigenvalues of OM0OT . Do you get the same answer as
in (a)?
(c)
Consider M1 whose elements are all equal to those of M0 except for one
element in the lower left corner [M1]N,1 = (1/2)Nâˆ’1. Show that MN
1 = 1;
more precisely, show that the characteristic polynomial of M1 is given by
det(M1 âˆ’Î»1) = Î»N âˆ’1, therefore M1 has N distinct eigenvalues equal to the
N complex roots of unity Î»k = e2Ï€ik/N.
(d)
For N greater than about 60, OM0OT and OM1OT are indistinguishable to
machine precision. Compare numerically the eigenvalues of these two rotated
matrices.
1.1.3 Singular Values
A non-symmetric, square matrix cannot in general be decomposed as A = OOT , where
 is a diagonal matrix and O an orthogonal matrix. One can however ï¬nd a very useful
alternative decomposition as
A = VSUT,
(1.16)
where S is a non-negative diagonal matrix, whose elements are called the singular values
of A, and U,V are two real, orthogonal matrices. Whenever A is symmetric positive semi-
deï¬nite, one has S =  and U = V.
Equation (1.16) also holds for rectangular NÃ—T matrices, where V is NÃ—N orthogonal,
U is T Ã— T orthogonal and S is N Ã— T diagonal as deï¬ned below. To construct the
singular value decomposition (svd) of A, we ï¬rst introduce two matrices B and B, deï¬ned
as B := AAT and B = AT A. It is plain to see that these matrices are symmetric, since

8
Deterministic Matrices
BT = (AAT )T = AT T AT = B (and similarly for B). They are also positive semi-deï¬nite as
for any vector x we have xT Bx = ||AT x||2 â‰¥0.
We can show that B and B have the same non-zero eigenvalues. In fact, let Î» > 0 be an
eigenvalue of B and v  0 is the corresponding eigenvector. Then we have, by deï¬nition,
AAT v = Î»v.
(1.17)
Let u = AT v, then we can get from the above equation that
AT AAT v = Î»AT v â‡’Bu = Î»u.
(1.18)
Moreover,
âˆ¥uâˆ¥2 = vT AAT v = vT Bv  0 â‡’u  0.
(1.19)
Hence Î» is also an eigenvalue of B. Note that for degenerate eigenvalues Î» of B, an
orthogonal set of corresponding eigenvectors {vâ„“} gives rise to an orthogonal set {AT vâ„“}
of eigenvectors of B. Hence the multiplicity of Î» in B is at least that of B. Similarly, we can
show that any non-zero eigenvalue of B is also an eigenvalue of B. This ï¬nishes the proof
of the claim.
Note that B has at most N non-zero eigenvalues and B has at most T non-zero eigen-
values. Thus by the above claim, if T > N, B has at least T âˆ’N zero eigenvalues, and if
T < N, B has at least N âˆ’T zero eigenvalues. We denote the other min{N,T } eigenvalues
of B and B by {Î»k}1â‰¤kâ‰¤min{N,T }. Then the svd of A is expressed as Eq. (1.16), where V is
the N Ã— N orthogonal matrix consisting of the N normalized eigenvectors of B, U is the
T Ã— T orthogonal matrix consisting of the T normalized eigenvectors of B, and S is an
N Ã—T rectangular diagonal matrix with Skk = âˆšÎ»k â‰¥0, 1 â‰¤k â‰¤min{N,T } and all other
entries equal to zero.
For instance, if N < T , we have
S =
â›
âœâœâœâ
âˆšÎ»1
0
0
0
Â· Â· Â·
0
0
âˆšÎ»2
0
0
Â· Â· Â·
0
0
0
...
0
Â· Â· Â·
0
0
0
0
âˆšÎ»N
Â· Â· Â·
0
â
âŸâŸâŸâ .
(1.20)
Although (non-degenerate) normalized eigenvectors are unique up to a sign, the choice of
the positive sign for the square-root âˆšÎ»k imposes a condition on the combined sign for the
left and right singular vectors vk and uk. In other words, simultaneously changing both vk
and uk to âˆ’vk and âˆ’uk leaves the matrix A invariant, but for non-zero singular values one
cannot individually change the sign of either vk or uk.
The recipe to ï¬nd the svd, Eq. (1.16), is thus to diagonalize both AAT (to obtain V
and S2) and AT A (to obtain U and again S2). It is insightful to again count the number of
parameters involved in this decomposition. Consider a general N Ã— T matrix with T â‰¥N
(the case N â‰¥T follows similarly). The N eigenvectors of AAT are generically unique
up to a sign, while for T âˆ’N > 0 the matrix AT A will have a degenerate eigenspace
associated with the eigenvalue 0 of size T âˆ’N, hence its eigenvectors are only unique up

1.2 Some Useful Theorems and Identities
9
to an arbitrary rotation in T âˆ’N dimension. So generically the svd decomposition amounts
to writing the NT elements of A as
NT â‰¡1
2N(N âˆ’1) + N + 1
2T (T âˆ’1) âˆ’1
2(T âˆ’N)(T âˆ’N âˆ’1).
(1.21)
The interpretation of Eq. (1.16) for N Ã— N matrices is that one can always ï¬nd an
orthonormal basis of vectors {u} such that the application of a matrix A amounts to a
rotation (or an improper rotation) of {u} into another orthonormal set {v}, followed by a
dilation of each vk by a positive factor âˆšÎ»k.
Normal matrices are such that U = V. In other words, A is normal whenever A com-
mutes with its transpose: AAT = AT A. Symmetric, skew-symmetric and orthogonal matri-
ces are normal, but other cases are possible. For example a 3 Ã— 3 matrix such that each row
and each column has exactly two elements equal to 1 and one element equal to 0 is normal.
1.2 Some Useful Theorems and Identities
In this section, we state without proof very useful theorems on eigenvalues and matrices.
1.2.1 Gershgorin Circle Theorem
Let A be a real matrix, with elements Aij. Deï¬ne Ri as Ri = 
ji |Aij|, and Di a disk in
the complex plane centered on Aii and of radius Ri. Then every eigenvalue of A lies within
at least one disk Di. For example, for the matrix
A =
â›
â
1
âˆ’0.2
0.2
âˆ’0.3
2
âˆ’0.2
0
1.1
3
â
â ,
(1.22)
the three circles are located on the real axis at x = 1,2 and 3 with radii 0.4, 0.5 and 1.1
respectively (see Fig. 1.1).
In particular, eigenvalues corresponding to eigenvectors with a maximum amplitude on
i lie within the disk Di.
1.2.2 The Perronâ€“Frobenius Theorem
Let A be a real matrix, with all its elements positive Aij > 0. Then the top eigenvalue Î»max
is unique and real (all other eigenvalues have a smaller real part). The corresponding top
eigenvector vâˆ—has all its elements positive:
Avâˆ—= Î»maxvâˆ—;
vâˆ—
k > 0, âˆ€k.
(1.23)
The top eigenvalue satisï¬es the following inequalities:
min
i

j
Aij â‰¤Î»max â‰¤max
i

j
Aij.
(1.24)

10
Deterministic Matrices
0
1
2
3
4
Re ( )
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
Im ( )
Figure 1.1 The three complex eigenvalues of the matrix (1.22) (crosses) and its three Gershgorin
circles. The ï¬rst eigenvalue Î»1 â‰ˆ0.92 falls in the ï¬rst circle while the other two Î»2,3 â‰ˆ2.54 Â± 0.18i
fall in the third one.
Application: Suppose A is a stochastic matrix, such that all its elements are positive
and satisfy 
i Aij = 1, âˆ€j. Then clearly the vector âƒ—1 is an eigenvector of AT , with
eigenvalue Î» = 1. But since the Perronâ€“Frobenius can be applied to AT , the inequalities
(1.24) ensure that Î» is the top eigenvalue of AT , and thus also of A. All the elements of the
corresponding eigenvector vâˆ—are positive, and describe the stationary state of the associated
Master equation, i.e.
P âˆ—
i =

j
AijP âˆ—
j âˆ’â†’P âˆ—
i =
vâˆ—
i

k vâˆ—
k
.
(1.25)
Exercise 1.2.1
Gershgorin and Perronâ€“Frobenius
Show that the upper bound in Eq. (1.24) is a simple consequence of the
Gershgorin theorem.
1.2.3 The Eigenvalue Interlacing Theorem
Let A be an NÃ—N symmetric matrix (or more generally Hermitian matrix) with eigenvalues
Î»1 â‰¥Î»2 Â· Â· Â· â‰¥Î»N. Consider the N âˆ’1Ã—N âˆ’1 submatrix A\i obtained by removing the ith
row and ith columns of A. Its eigenvalues are Î¼(i)
1
â‰¥Î¼(i)
2 Â· Â· Â· â‰¥Î¼(i)
Nâˆ’1. Then the following
interlacing inequalities hold:
Î»1 â‰¥Î¼(i)
1 â‰¥Î»2 Â· Â· Â· â‰¥Î¼(i)
Nâˆ’1 â‰¥Î»N.
(1.26)

1.2 Some Useful Theorems and Identities
11
Very recently, a formula relating eigenvectors to eigenvalues was (re-)discovered. Calling
vi the eigenvector of A associated with Î»i, one has2
(vi)j
2 =
Nâˆ’1
k=1 Î»i âˆ’Î¼(j)
k
N
â„“=1,â„“i Î»i âˆ’Î»â„“
.
(1.27)
1.2.4 Shermanâ€“Morrison Formula
The Shermanâ€“Morrison formula gives the inverse of a matrix A perturbed by a rank-1
perturbation:
(A + uvT )âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1uvT Aâˆ’1
1 + vT Aâˆ’1u,
(1.28)
valid for any invertible matrix A and vectors u and v such that the denominator does not
vanish. This is a special case of the Woodbury identity, which reads

A + UCVT âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1U

Câˆ’1 + VT Aâˆ’1U
âˆ’1
VT Aâˆ’1,
(1.29)
where U,V are N Ã— K matrices and C is a K Ã— K matrix. Equation (1.28) corresponds to
the case K = 1.
The associated Shermanâ€“Morrison determinant lemma reads
det(A + vuT ) = det A Â·

1 + uT Aâˆ’1v

(1.30)
for invertible A.
Exercise 1.2.2
Shermanâ€“Morrison
Show that Eq. (1.28) is correct by multiplying both sides by (A + uvT ).
1.2.5 Schur Complement Formula
The Schur complement, also called inversion by partitioning, relates the blocks of the
inverse of a matrix to the inverse of blocks of the original matrix. Let M be an invertible
matrix which we divide in four blocks as
M =
 M11
M12
M21
M22

and Mâˆ’1 = Q =
 Q11
Q12
Q21
Q22

,
(1.31)
where [M11] = n Ã— n, [M12] = n Ã— (N âˆ’n), [M21] = (N âˆ’n) Ã— n, [M22] = (N âˆ’n) Ã—
(N âˆ’n), and M22 is invertible. The integer n can take any values from 1 to N âˆ’1.
2 See: P. Denton, S. Parke, T. Tao, X. Zhang, Eigenvalues from Eigenvectors: a survey of a basic identity in linear algebra,
arXiv:1908.03795.

12
Deterministic Matrices
Then the upper left n Ã— n block of Q is given by
Qâˆ’1
11 = M11 âˆ’M12(M22)âˆ’1M21,
(1.32)
where the right hand side is called the Schur complement of the block M22 of the matrix M.
Exercise 1.2.3
Combining Schur and Shermanâ€“Morrison
In the notation of Eq. (1.31) for n = 1 and any N > 1, combine the Schur
complement of the lower right block with the Shermanâ€“Morrison formula to
show that
Q22 = (M22)âˆ’1 + (M22)âˆ’1M21M12(M22)âˆ’1
M11 âˆ’M12(M22)âˆ’1M21
.
(1.33)
1.2.6 Function of a Matrix and Matrix Derivative
In our study of random matrices, we will need to extend real or complex scalar functions
to take a symmetric matrix M as its argument. The simplest way to extend such a function
is to apply it to each eigenvalue of the matrix M = OOT :
F(M) = OF()OT,
(1.34)
where F() is the diagonal matrix where we have applied the function F to each (diag-
onal) entry of . The function F(M) is now a matrix valued function of a matrix. Scalar
polynomial functions can obviously be extended directly as
F(x) =
K

k=0
akxk â‡’F(M) =
K

k=0
akMk,
(1.35)
but this is equivalent to applying the polynomial to the eigenvalues of M. By extension,
when the Taylor series of the function F(x) converges for every eigenvalue of M the matrix
Taylor series coincides with our deï¬nition.
Taking the trace of F(M) will yield a matrix function that returns a scalar. This con-
struction is rotationally invariant in the following sense:
Tr F(UMUT ) = Tr F(M) for any UUT = 1.
(1.36)
We can take the derivative of a scalar-valued function Tr F(M) with respect to each
element of the matrix M:
d
d[M]ij
Tr(F(M)) = [F â€²(M)]ij â‡’
d
dM Tr(F(M)) = F â€²(M).
(1.37)
Equation (1.37) is easy to derive when F(x) is a monomial akxk and by linearity for
polynomial or Taylor series F(x).

1.2 Some Useful Theorems and Identities
13
1.2.7 Jacobian of Simple Matrix Transformations
Suppose one transforms an N Ã— N matrix A into another N Ã— N matrix B through some
function of the matrix elements. The Jacobian of the transformation is deï¬ned as the
determinant of the partial derivatives:
Gij,kâ„“= âˆ‚Bkâ„“
âˆ‚Aij
.
(1.38)
The simplest case is just multiplication by a scalar: B = Î±A, leading to Gij,kâ„“= Î±Î´ikÎ´jâ„“.
G is therefore the tensor product of Î±1 with 1, and its determinant is thus equal to Î±N.
Not much more difï¬cult is the case of an orthogonal transformation B = OAOT , for which
Gij,kâ„“= OikOjâ„“. G is now the tensor product G = O âŠ—O and therefore its determinant is
unity.
Slightly more complicated is the case where B = Aâˆ’1. Using simple algebra, one readily
obtains, for symmetric matrices,
Gij,kâ„“= 1
2[Aâˆ’1]ik[Aâˆ’1]jâ„“+ 1
2[Aâˆ’1]iâ„“[Aâˆ’1]jk.
(1.39)
Let us now assume that A has eigenvalues Î»Î± and eigenvectors vÎ±. One can easily diago-
nalize Gij,kâ„“within the symmetric sector, since

kâ„“
Gij,kâ„“

vÎ±,kvÎ²,â„“+ vÎ±,â„“vÎ²,k

=
1
Î»Î±Î»Î²

vÎ±,ivÎ²,j + vÎ±,jvÎ²,i

.
(1.40)
So the determinant of G is simply 
Î±,Î²â‰¥Î±(Î»Î±Î»Î²)âˆ’1. Taking the logarithm of this product
helps avoiding counting mistakes, and ï¬nally leads to the result
det G = (det A)âˆ’Nâˆ’1.
(1.41)
Exercise 1.2.4
Random Matrices
We conclude this chapter on deterministic matrices with a numerical exercise
on random matrices. Most of the results of this exercise will be explored
theoretically in the following chapters.
â€¢
Let M be a random real symmetric orthogonal matrix, that is an N Ã—N matrix
satisfying M = MT = Mâˆ’1. Show that all the eigenvalues of M are Â±1.
â€¢
Let X be a Wigner matrix, i.e. an NÃ—N real symmetric matrix whose diagonal
and upper triangular entries are iid Gaussian random numbers with zero mean
and variance Ïƒ 2/N. You can use X = Ïƒ(H + HT )/
âˆš
2N where H is a non-
symmetric N Ã— N matrix with iid standard Gaussians.
â€¢
The matrix P+ is deï¬ned as P+ = 1
2(M + 1N). Convince yourself that P+ is
the projector onto the eigenspace of M with eigenvalue +1. Explain the effect
of the matrix P+ on eigenvectors of M.

14
Deterministic Matrices
â€¢
An easy way to generate a random matrix M is to generate a Wigner matrix
(independent of X), diagonalize it, replace every eigenvalue by its sign and
reconstruct the matrix. The procedure does not depend on the Ïƒ used for the
Wigner.
â€¢
We consider a matrix E of the form E = M+X. To wit, E is a noisy version of
M. The goal of the following is to understand numerically how the matrix E is
corrupted by the Wigner noise. Using the computer language of your choice,
for a large value of N (as large as possible while keeping computing times
below one minute), for three interesting values of Ïƒ of your choice, do the
following numerical analysis.
(a)
Plot a histogram of the eigenvalues of E, for a single sample ï¬rst, and then for
many samples (say 100).
(b)
From your numerical analysis, in the large N limit, for what values of Ïƒ do
you expect a non-zero density of eigenvalues near zero.
(c)
For every normalized eigenvector vi of E, compute the norm of the vector
P+vi. For a single sample, do a scatter plot of |P+vi|2 vs Î»i (its eigenvalue).
Turn your scatter plot into an approximate conditional expectation value
(using a histogram) including data from many samples.
(d)
Build an estimator (E) of M using only data from E. We want to minimize
the error E =
1
N ||((E) âˆ’M)||2
F where ||A||2
F = TrAAT . Consider ï¬rst
1(E) = E and then 0(E) = 0. What is the error E of these two estimators?
Try to build an ad-hoc estimator (E) that has a lower error E than these two.
(e)
Show numerically that the eigenvalues of E are not iid. For each sample E
rank its eigenvalues Î»1 < Î»2 < Â· Â· Â· < Î»N. Consider the eigenvalue spacing
sk = Î»k âˆ’Î»kâˆ’1 for eigenvalues in the bulk (.2N < k < .3N and .7N < k <
.8N). Make a histogram of {sk} including data from 100 samples. Make 100
pseudo-iid samples: mix eigenvalues for 100 different samples and randomly
choose N from the 100N possibilities, do not choose the same eigenvalue
twice for a given pseudo-iid sample. For each pseudo-iid sample, compute
sk in the bulk and make a histogram of the values using data from all 100
pseudo-iid samples. (Bonus) Try to ï¬t an exponential distribution to these two
histograms. The iid case should be well ï¬tted by the exponential but not the
original data (not iid).

2
Wigner Ensemble and Semi-Circle Law
In many circumstances, the matrices that are encountered are large, and with no particular
structure. Physicist Eugene Wigner postulated that one can often replace a large complex
(but deterministic) matrix by a typical element of a certain ensemble of random matrices.
This bold proposal was made in the context of the study of large complex atomic nuclei,
where the â€œmatrixâ€ is the Hamiltonian of the system, which is a Hermitian matrix describ-
ing all the interactions between the neutrons and protons contained in the nucleus. At the
time, these interactions were not well known; but even if they had been, the task of diag-
onalizing the Hamiltonian to ï¬nd the energy levels of the nucleus was so formidable that
Wigner looked for an alternative. He suggested that we should abandon the idea of ï¬nding
precisely all energy levels, but rather rephrase the question as a statistical question: what
is the probability to ï¬nd an energy level within a certain interval, what is the probability
that the distance between two successive levels is equal to a certain value, etc.? The idea
of Wigner was that the answer to these questions could be, to some degree, universal,
i.e. independent of the speciï¬c Hermitian matrix describing the system, provided it was
complex enough. If this is the case, why not replace the Hamiltonian of the system by a
purely random matrix with the correct symmetry properties? In the case of time-reversal
invariant quantum systems, the Hamiltonian is a real symmetric matrix (of inï¬nite size).
In the presence of a magnetic ï¬eld, the Hamiltonian is a complex, Hermitian matrix (see
Section 3.1.1). In the presence of â€œspinâ€“orbit couplingâ€, the Hamiltonian is symplectic (see
Section 3.1.2).
This idea has been incredibly fruitful and has led to the development of a subï¬eld
of mathematical physics called â€œrandom matrix theoryâ€. In this book we will study the
properties of some ensembles of random matrices. We will mostly focus on symmetric
matrices with real entries as those are the most commonly encountered in data analy-
sis and statistical physics. For example, Wignerâ€™s idea has been transposed to glasses
and spin-glasses, where the interaction between pairs of atoms or pairs of spins is often
replaced by a real symmetric, random matrix (see Section 13.4). In other cases, the ran-
domness stems from noisy observations. For example, when one wants to measure the
covariance matrix of the returns of a large number of assets using a sample of ï¬nite length
(for example the 500 stocks of the S&P500 using 4 years of daily data, i.e. 4 Ã— 250 =
1000 data points per stock), there is inevitably some measurement noise that pollutes the
15

16
Wigner Ensemble and Semi-Circle Law
determination of said covariance matrix. We will be confronted with this precise problem in
Chapters 4 and 17.
In the present chapter and the following one, we will investigate the simplest of all
ensembles of random matrices, which was proposed by Wigner himself in the context
recalled above. These are matrices where all elements are Gaussian random variables, with
the only constraint that the matrix is real symmetric (the Gaussian orthogonal ensemble,
goe), complex Hermitian (the Gaussian unitary ensemble, gue) or symplectic (the Gaus-
sian symplectic ensemble, gse).
2.1 Normalized Trace and Sample Averages
We ï¬rst generalize the notion of expectation value and moments from classical probabilities
to large random matrices. We could simply consider the moments E[Ak] but that object is
very large (N Ã— N dimensional). It is not clear how to interpret it as N â†’âˆ. It turns
out that the correct analog of the expectation value is the normalized trace operator Ï„(.),
deï¬ned as
Ï„(A) := 1
N E[Tr A].
(2.1)
The normalization by 1/N is there to make the normalized trace operator ï¬nite as N â†’âˆ.
For example for the identity matrix, Ï„(1) = 1 independently of the dimension and our
deï¬nition therefore makes sense as N â†’âˆ. When using the notation Ï„(A) we will only
consider the dominant term as N â†’âˆ, implicitly taking the large N limit.
For a polynomial function of a matrix F(A) or by extension for a function that can be
written as a power series, the trace of the function can be computed on the eigenvalues:
1
N Tr F(A) = 1
N
N

k=1
F(Î»k).
(2.2)
In the following, we will denote as âŸ¨.âŸ©the average over the eigenvalues of a single matrix
A (sample), i.e.
âŸ¨F(Î»)âŸ©:= 1
N
N

k=1
F(Î»k).
(2.3)
For large random matrices, many scalar quantities such as Ï„(F(A)) do not ï¬‚uctuate from
sample to sample, or more precisely such ï¬‚uctuations go to zero in the large N limit.
Physicists speak of this phenomenon as self-averaging and mathematicians speak of con-
centration of measure.
Ï„(F(A)) = 1
N E[Tr F(A)] â‰ˆâŸ¨F(Î»)âŸ©for a single A.
(2.4)
When the eigenvalues of a random matrix A converge to a well-deï¬ned density Ï(Î»), we
can write

2.2 The Wigner Ensemble
17
Ï„(F(A)) =

Ï(Î»)F(Î»)dÎ».
(2.5)
Using F(A) = Ak, we can deï¬ne the kth moment of a random matrix by mk := Ï„(Ak).
The ï¬rst moment m1 is simply the normalized trace of A, while m2 = 1/N 
ij A2
ij is
the normalized sum of the squares of all the elements. The square-root of m2 satisï¬es the
axioms of a norm and is called the Frobenius norm of A:
||A||F := âˆšm2.
(2.6)
2.2 The Wigner Ensemble
2.2.1 Moments of Wigner Matrices
We will deï¬ne a Wigner matrix X as a symmetric matrix (X = XT ) with Gaussian entries
with zero mean. In a symmetric matrix there are really two types of elements: diagonal and
off-diagonal, which can have different variances. Diagonal elements have variance Ïƒ 2
d and
off-diagonal elements have variance Ïƒ 2
od. Note that Xij = Xji so they are not independent
variables.
In fact, the elements in a Wigner matrix do not need to be Gaussian or even to be iid,
as there are many weaker (more general) deï¬nitions of the Wigner matrix that yield the
same ï¬nal statistical results in the limit of large matrices N â†’âˆ. For the purpose of this
introductory book we will stick to the strong Gaussian hypothesis.
The ï¬rst few moments of our Wigner matrix X are given by
Ï„(X) = 1
N E[Tr X] = 1
N Tr E[X] = 0,
(2.7)
Ï„(X2) = 1
N E[Tr XXT ] = 1
N E
â¡
â£
N

ij=1
X2
ij
â¤
â¦= 1
N [N(N âˆ’1)Ïƒ 2
od + NÏƒ 2
d ].
(2.8)
The term containing Ïƒ 2
od dominates when the two variances are of the same order of mag-
nitude. So for a Wigner matrix we can pick any variance we want on the diagonal (as long
as it is small with respect to NÏƒ 2
od). We want to normalize our Wigner matrix so that its
second moment is independent of the size of the matrix (N). Let us pick
Ïƒ 2
od = Ïƒ 2/N.
(2.9)
For Ïƒ 2
d the natural choice seems to be Ïƒ 2
d = Ïƒ 2/N. However, we will rather choose Ïƒ 2
d =
2Ïƒ 2/N, which is easy to generate numerically and more importantly respects rotational
invariance for ï¬nite N, as we show in the next subsection. The ensemble described here
(with the choice Ïƒ 2
d = 2Ïƒ 2
od) is called the Gaussian orthogonal ensemble or goe.1
1 Some authors deï¬ne a goe matrix to have Ïƒ2 = 1 others as Ïƒ2 = N. For us a goe matrix can have any variance and is thus
synonymous with the Gaussian rotationally invariant Wigner matrix.

18
Wigner Ensemble and Semi-Circle Law
To generate a goe matrix numerically, ï¬rst generate a non-symmetric random square
matrix H of size N with iid N(0,Ïƒ 2/(2N)) coefï¬cients. Then let the Wigner matrix X
be X = H + HT . The matrix X will then be symmetric with diagonal variance twice the
off-diagonal variance. The reason is that off-diagonal terms are sums of two independent
Gaussian variables, so the variance is doubled. Diagonal elements, on the other hand, are
equal to twice the original variables Hii and so their variance is multiplied by 4.
With any choice of Ïƒ 2
d we have
Ï„(X2) = Ïƒ 2 + O(1/N),
(2.10)
and hence we will call the parameter Ïƒ 2 the variance of the Wigner matrix.
The third moment Ï„(X3) = 0 from the fact that the Gaussian distribution is even. Later
we will show that
Ï„(X4) = 2Ïƒ 4.
(2.11)
For standard Gaussian variables E[x4] = 3Ïƒ 4, this implies that the eigenvalue density of
a Wigner is not Gaussian. What is this eigenvalue distribution? As we will show many
times over in this book, it is given by the semi-circle law, originally derived by Wigner
himself:
Ï(Î») =
âˆš
4Ïƒ 2 âˆ’Î»2
2Ï€Ïƒ 2
for âˆ’2Ïƒ < Î» < 2Ïƒ.
(2.12)
2.2.2 Rotational Invariance
We remind the reader that to rotate a vector v, one applies a rotation matrix O: w = Ov
where O is an orthogonal matrix OT = Oâˆ’1 (i.e. OOT = 1). Note that in general O is not
symmetric. To rotate the basis in which a matrix is written, one writes X = OXOT . The
eigenvalues of X are the same as those of X. The eigenvectors are {Ov} where {v} are the
eigenvectors of X.
A rotationally invariant random matrix ensemble is such that the matrix OXOT is as
probable as the matrix X itself, i.e. OXOT in law
= X.
Let us show that the construction X = H + HT with a Gaussian iid matrix H leads to
a rotationally invariant ensemble. First, note an important property of Gaussian variables,
namely that a Gaussian iid vector v (a white multivariate Gaussian vector) is rotationally
invariant. The reason is that w = Ov is again a Gaussian vector (since sums of Gaussians
are still Gaussian), with covariance given by
E[wiwj] =

kâ„“
OikOjâ„“E[vkvâ„“] =

kâ„“
OikOjâ„“Î´kâ„“= [OOT ]ij = Î´ij.
(2.13)
Now, write
X = H + HT,
(2.14)

2.3 Resolvent and Stieltjes Transform
19
where H is a square matrix ï¬lled with iid Gaussian random numbers. Each column of H
is rotationally invariant: OH
in law
= H and the matrix OH is row-wise rotationally invariant:
OHOT in law
= OH. So H is rotationally invariant as a matrix. Now
OXOT = O(H + HT )OT in law
= H + HT = X,
(2.15)
which shows that the Wigner ensemble with Ïƒ 2
d = 2Ïƒ 2
od is rotationally invariant for any
matrix size N. More general deï¬nitions of the Wigner ensemble (including non-Gaussian
ensembles) are only asymptotically rotationally invariant (i.e. when N â†’âˆ).
Another way to see the rotational invariance of the Wigner ensemble is to look at the
joint law of matrix elements:
P({Xij}) =

1
2Ï€Ïƒ 2
d
N/2 
1
2Ï€Ïƒ 2
od
N(Nâˆ’1)/4
exp
â§
â¨
â©âˆ’
N

i=1
X2
ii
2Ïƒ 2
d
âˆ’
N

i<j
X2
ij
2Ïƒ 2
od
â«
â¬
â­,
(2.16)
where only the diagonal and upper triangular elements are independent variables. With the
choice Ïƒ 2
od = Ïƒ 2/N and Ïƒ 2
d = 2Ïƒ 2/N this becomes
P({Xij}) âˆexp
$
âˆ’N
4Ïƒ 2 Tr X2
%
.
(2.17)
Under the change of variable X â†’X = OXOT the argument of the exponential is invariant,
because the trace of a matrix is independent of the basis, and because the Jacobian of the
transformation is equal to 1 (see Section 1.2.7), therefore X
in law
= OXOT .
By the same argument any matrix whose joint probability density of its elements can
be written as P({Mij}) âˆexp {âˆ’N Tr V (M)}, where V (.) is an arbitrary function, will be
rotationally invariant. We will study such matrix ensembles in Chapter 5.
2.3 Resolvent and Stieltjes Transform
2.3.1 Deï¬nition and Basic Properties
In this section we introduce the Stieltjes transform of a matrix. It will give us information
about all the moments of the random matrix and also about the density of its eigenvalues in
the large N limit. First we need to deï¬ne the matrix resolvent.
Given an N Ã— N real symmetric matrix A, its resolvent is given by
GA(z) = (z1 âˆ’A)âˆ’1,
(2.18)
where z is a complex variable deï¬ned away from all the (real) eigenvalues of A and 1
denotes the identity matrix. Then the Stieltjes transform of A is given by2
gA
N(z) = 1
N Tr (GA(z)) = 1
N
N

k=1
1
z âˆ’Î»k
,
(2.19)
2 In mathematical literature, the Stieltjes transform is more commonly deï¬ned as sA(z) = âˆ’(1/N) Tr GA(z), i.e. with an extra
minus sign. Some authors prefer the name Cauchy transform.

20
Wigner Ensemble and Semi-Circle Law
where Î»k are the eigenvalues of A. The subscript N indicates that this is the ï¬nite N Stieltjes
transform of a single realization of A. When it is clear from context which matrix we
consider we will drop the superscript A and write gN(z).
Let us see why the Stieltjes transform gives useful information about the density of
eigenvalues of A. For a given random matrix A, we can deï¬ne the empirical spectral
distribution (ESD) also called the sample eigenvalue density:
ÏN(Î») = 1
N
N

k=1
Î´(Î» âˆ’Î»k),
(2.20)
where Î´(x) is the Dirac delta function. Then the Stieltjes transform can be written as
gN(z) =
 +âˆ
âˆ’âˆ
ÏN(Î»)
z âˆ’Î» dÎ».
(2.21)
Note that gN(z) is well deï¬ned for any z  {Î»k : 1 â‰¤k â‰¤N}. In particular, it is well
behaved at âˆ:
gN(z) =
âˆ

k=0
1
zk+1
1
N Tr(Ak),
1
N Tr(A0) = 1.
(2.22)
We will consider random matrices A such that, for large N, the normalized traces of powers
of A converge to their expectation values, which are deterministic numbers:
lim
Nâ†’âˆ
1
N Tr(Ak) = Ï„(Ak).
(2.23)
We then expect that, for large enough z, the function gA(z) converges to a deterministic
limit g(z) deï¬ned as g(z) = limNâ†’âˆE[gN(z)], whose Taylor series is
g(z) =
âˆ

k=0
1
zk+1 Ï„(Ak),
(2.24)
for z away from the real axis.
Thus g(z) is a moment generating function of A. In other words, the knowledge of g(z)
near inï¬nity is equivalent to the knowledge of all the moments of A. To the level of rigor
of this book, the knowledge of all the moments of A is equivalent to the knowledge of the
density of its eigenvalues. For any function F(x) deï¬ned over the support of the eigenvalues
[Î»âˆ’,Î»+] of A we can compute its expectation:
Ï„(F(A)) =
 Î»+
Î»âˆ’
Ï(Î»)F(Î»)dÎ»;
Ï(Î») := E[ÏA(Î»)].
(2.25)
Alternatively we can approximate the function F(x) arbitrarily well by a polynomial
Q(x) = a0 + a1x + Â· Â· Â· + aKxK and ï¬nd
Ï„(F(A)) â‰ˆÏ„(Q(A)) =
K

k=0
akÏ„(Ak).
(2.26)

2.3 Resolvent and Stieltjes Transform
21
To recap, we only need to know g(z) in the neighborhood of |z| â†’âˆto know all the
moments of A and these moments tell us everything about Ï(Î»). In computing the Stieltjes
transform in concrete cases, we will often make use of that fact and only estimate it for
very large values of z.
The Stieltjes transform also gives the negative moments when they exist. If the eigen-
values of A satisfy minâ„“Î»â„“> c for some constant c > 0, then the inverse moments of A
exist and are given by the expansion of g(z) around z = 0:
g(z) = âˆ’
âˆ

k=0
zkÏ„(Aâˆ’kâˆ’1).
(2.27)
In particular, we have
g(0) = âˆ’Ï„(Aâˆ’1).
(2.28)
Exercise 2.3.1
Stieltjes transform for shifted and scaled matrices
Let A be a random matrix drawn from a well-behaved ensemble with Stieltjes
transform g(z). What are the Stieltjes transforms of the random matrices Î±A and
A + Î²1 where Î± and Î² are non-zero real numbers and 1 the identity matrix?
2.3.2 Stieltjes Transform of the Wigner Ensemble
We are now ready to compute the Stieltjes transform of the Wigner ensemble. The ï¬rst
technique we will use is sometimes called the cavity method or the self-consistent equation.
We will ï¬nd a relation between the Stieltjes transform of a Wigner matrix of size N and
one of size N âˆ’1. In the large N limit, the two converge to the same limiting Stieltjes
transform and give us a self-consistent equation that can be solved easily.
We would like to calculate gX
N(z) when X is a Wigner matrix, with Xij âˆ¼N(0,Ïƒ 2/N)
and Xii âˆ¼N(0,2Ïƒ 2/N). In the large N limit, we expect that gX
N(z) converges towards a
well-deï¬ned limit g(z).
We can use the Schur complement formula (1.32) to compute the (1,1) element of the
inverse of M = z1 âˆ’X. Then we have
1
(GX)11
= M11 âˆ’
N

k,l=2
M1k(M22)âˆ’1
kl Ml1,
(2.29)
where the matrix M22 is the (N âˆ’1)Ã—(N âˆ’1) submatrix of M with the ï¬rst row and column
removed. For large N, we argue that the right hand side is dominated by its expectation
value with small (O(1/
âˆš
N)) ï¬‚uctuations. We will only compute its expectation value,
but getting a more precise handle on its ï¬‚uctuations would not be difï¬cult. First, we note

22
Wigner Ensemble and Semi-Circle Law
that E[M11] = z. We then note that the entries of M22 are independent of the ones of
M1i = âˆ’X1i. Thus we can ï¬rst take the partial expectation over the {X1i}, and get
E{X1i}
&
M1i(M22)âˆ’1
ij M1j
'
= Ïƒ 2
N (M22)âˆ’1
ii Î´ij
(2.30)
so we have
E{X1i}
â¡
â£
N

k,l=2
M1k(M22)âˆ’1
kl Ml1
â¤
â¦= Ïƒ 2
N Tr

(M22)âˆ’1
.
(2.31)
Another observation is that 1/(N âˆ’1) Tr

(M22)âˆ’1
is the Stieltjes transform of a Wigner
matrix of size N âˆ’1 and variance Ïƒ 2(N âˆ’1)/N. In the large N limit, the Stieltjes transform
should be independent of the matrix size and the difference between N and (N âˆ’1) is
negligible. So we have
E
( 1
N Tr

(M22)âˆ’1)
â†’g(z).
(2.32)
We therefore have that 1/(GX)11 equals a deterministic number with negligible ï¬‚uctua-
tions; hence in the large N limit we have
E
(
1
(GX)11
)
=
1
E[(GX)11].
(2.33)
From the rotational invariance of X and therefore of GX, all diagonal entries of GX must
have the same expectation value:
E [(GX)11] = 1
N E[Tr(GX)] = E[gN] â†’g.
(2.34)
Putting all the pieces together, we ï¬nd that in the large N limit Eq. (2.29) becomes
1
g(z) = z âˆ’Ïƒ 2g(z).
(2.35)
Solving (2.35) we obtain that
Ïƒ 2g2 âˆ’zg + 1 = 0 â‡’g = z Â±
âˆš
z2 âˆ’4Ïƒ 2
2Ïƒ 2
.
(2.36)
We know that g(z) should be analytic for large complex z but the square-root above can run
into branch cuts. It is convenient to pull out a factor of z and express the square-root as a
function of 1/z which becomes small for large z:
g(z) = z Â± z
*
1 âˆ’4Ïƒ 2/z2
2Ïƒ 2
.
(2.37)
We can now choose the correct root: the + sign gives an incorrect g(z) âˆ¼z/Ïƒ 2 for large z
while the âˆ’sign gives g(z) âˆ¼1/z for any large complex z as expected, so we have:
g(z) = z âˆ’z
*
1 âˆ’4Ïƒ 2/z2
2Ïƒ 2
.
(2.38)

2.3 Resolvent and Stieltjes Transform
23
z âˆˆC
âˆ’2Ïƒ
2Ïƒ
g(z) is analytic
|z| < 2Ïƒ
|z| > 2Ïƒ
branch point
branch cut
Figure 2.1 The branch cuts of the Wigner Stieltjes transform.
Note, for numerical applications, it is very important to pick the correct branch of the
square-root. The function g(z) is analytic for |z| > 2Ïƒ, the branch cuts of the square-root
must therefore be conï¬ned to the interval [âˆ’2Ïƒ,2Ïƒ] (Fig. 2.1). We will come back to this
problem of determining the correct branch of Stieltjes transform in Section 4.2.3.
It might seem strange that g(z) given by Eq. (2.38) has no poles but only branch cuts.
For ï¬nite N, the sample Stieltjes transform
gN(z) := 1
N
N

k=1
1
z âˆ’Î»k
(2.39)
has poles at the eigenvalues of X. As N â†’âˆ, the poles fuse together and
1
N
N

k=1
Î´(x âˆ’Î»k) âˆ¼Ï(x).
(2.40)
The density Ï(x) can have extended support and/or isolated Dirac masses. Then as N â†’âˆ,
we have
g(z) =

supp{Ï}
Ï(x)dx
z âˆ’x ,
(2.41)
which is the Stieltjes transform of the limiting measure Ï(x).
2.3.3 Convergence of Stieltjes near the Real Axis
It is natural to ask the following questions: how does gN(z) given by Eq. (2.39) converge
to g(z) =
 Ï(x)dx
zâˆ’x , and how do we recover Ï(x) from g(z)?

24
Wigner Ensemble and Semi-Circle Law
We have argued before that gN(z) converges to g(z) for very large complex z such
that the Taylor series around inï¬nity is convergent. The function g(z) is not deï¬ned on
the real axis for z = x on the support of Ï(x), nevertheless, immediately below (and
above) the real axis the random function gN(z) converges to g(z). (The case where z is
right on the real axis is discussed in Section 2.3.6.) Let us study the random function gN(z)
just below the support of Ï(x).
We let z = x âˆ’iÎ·, with x âˆˆsupp{Ï} and Î· is a small positive number. Then
gN(x âˆ’iÎ·) := 1
N
N

k=1
1
x âˆ’iÎ· âˆ’Î»k
= 1
N
N

k=1
x âˆ’Î»k + iÎ·
(x âˆ’Î»k)2 + Î·2 .
(2.42)
We focus on the imaginary part of gN(x âˆ’iÎ·) (the real part is discussed in Section 19.5.2).
Note that it is a convolution of the empirical spectral density ÏN(Î») and Ï€ times the Cauchy
kernel:
Ï€KÎ·(x) =
Î·
x2 + Î·2 .
(2.43)
The Cauchy kernel KÎ·(x) is strongly peaked around zero with a window width of order
Î· (Fig. 2.2). Since there are N eigenvalues lying inside the interval [Î»âˆ’,Î»+], the typical
eigenvalue spacing is of order (Î»+ âˆ’Î»âˆ’)/N = O(Nâˆ’1).
(1) Suppose Î· â‰ªNâˆ’1. Then there are typically 0 or 1 eigenvalue within a window of size
Î· around x. Then Im gN will be affected by the ï¬‚uctuations of single eigenvalues of X,
and hence it cannot converge to any deterministic function. (see Fig. 2.3).
âˆ’4
âˆ’2
0
2
4
x
0.0
0.2
0.4
0.6
K (x)
2
Figure 2.2 The Cauchy kernel for Î· = 0.5. It is strongly peaked around zero with a window width of
order Î·. When Î· â†’0, the Cauchy kernel is a possible representation of Diracâ€™s Î´-function.

2.3 Resolvent and Stieltjes Transform
25
âˆ’2
0
2
x
0.0
0.2
0.4
0.6
0.8
1.0
Im (xâˆ’i )
= 1/
N
= 1
analytic
âˆ’0.1
0.0
0.1
x
0.0
0.5
1.0
1.5
2.0
2.5
Im (xâˆ’i )
= 1/ N
= 1/
N
analytic
Figure 2.3 Imaginary part of g(x âˆ’iÎ·) for the Wigner ensemble. The analytic result for Î· â†’0+
is compared with numerical simulations (N = 400). On the left for Î· = 1/
âˆš
N and Î· = 1. Note
that for Î· = 1 the density is quite deformed. On the right (zoom near x = 0) for Î· = 1/
âˆš
N and
Î· = 1/N. Note that for Î· = 1/N, the density ï¬‚uctuates wildly as only a small number of (random)
eigenvalues contribute to the Cauchy kernel.
(2) Suppose Nâˆ’1 â‰ªÎ· â‰ª1 (e.g. Î· = Nâˆ’1/2). Then on a small scale Î· â‰ªx â‰ª1, the
density Ï is locally constant and there are a great number n of eigenvalues inside:
n âˆ¼NÏ(x)x â‰«NÎ· â‰«1.
(2.44)
The law of large numbers allows us to replace the sum with an integral; we obtain that
1
N

k:Î»kâˆˆ[xâˆ’x,x+x]
iÎ·
(x âˆ’Î»k)2 + Î·2 â†’i
 x+x
xâˆ’x
Ï(x)Î·dy
(x âˆ’y)2 + Î·2 â†’iÏ€Ï(x),
(2.45)
where the last limit is obtained by writing u = (y âˆ’x)/Î· and noting that as Î· â†’0
we have
 âˆ
âˆ’âˆ
du
u2 + 1 = Ï€.
(2.46)
Exercise 2.3.2
Finite N approximation and small imaginary part
Im gN(x âˆ’iÎ·)/Ï€ is a good approximation to Ï(x) for small positive Î·,
where gN(z) is the sample Stieltjes transform (gN(z) = (1/N) 
k 1/(z âˆ’Î»k)).
Numerically generate a Wigner matrix of size N and Ïƒ 2 = 1.
(a)
For three values of Î·, {1/N,1/
âˆš
N,1}, plot Im gN(x âˆ’iÎ·)/Ï€ and the
theoretical Ï(x) on the same plot for x between âˆ’3 and 3.

26
Wigner Ensemble and Semi-Circle Law
(b)
Compute the error as a function of Î· where the error is (Ï(x) âˆ’Im gN(x âˆ’
iÎ·)/Ï€)2 summed for all values of x between âˆ’3 and 3 spaced by intervals of
0.01. Plot this error for Î· between 1/N and 1. You should see that 1/
âˆš
N is
very close to the minimum of this function.
2.3.4 Stieltjes Inversion Formula
From the above discussions, we extract the following important results:
(1) The Stieltjes inversion formula (also called the Sokhotskiâ€“Plemelj formula):
lim
Î·â†’0+ Im g(x âˆ’iÎ·) = Ï€Ï(x).
(2.47)
(2) When applied to ï¬nite size Stieltjes transform gN(z), we should take Nâˆ’1 â‰ªÎ· â‰ª1
for gN(x âˆ’iÎ·) to converge to g(x âˆ’iÎ·) and for (2.47) to hold. Numerically, Î· = Nâˆ’1/2
works quite well.
We discuss brieï¬‚y why Î· = Nâˆ’1/2 works best. First, we want Î· to be as small as
possible such that the local density Ï(x) is not blurred. If Î· is too large, one introduces
a systematic error of order Ïâ€²(x)Î·. On the other hand, we want NÎ· to be as large as
possible such that we include the statistics of a sufï¬cient number of eigenvalues so that
we measure Ï(x) accurately. In fact, the error between gN and g is of order
1
NÎ· . Thus we
want to minimize the total error E given by
E = Ïâ€²(x)Î· +
1
NÎ·,
Ïâ€²(x)Î· : systematic error,
1
NÎ· : statistical error.
(2.48)
Then it is easy to see that the total error is minimized when Î· is of order 1/
*
NÏâ€²(x).
2.3.5 Density of Eigenvalues of a Wigner Matrix
We go back to study the Stieltjes transform (2.38) of the Wigner matrix. Note that for
z = x âˆ’iÎ· with Î· â†’0, g(z) can only have an imaginary part if
âˆš
x2 âˆ’4Ïƒ 2 is imaginary.
Then, using (2.47), we get the Wigner semi-circle law:
Ï(x) = 1
Ï€
lim
Î·â†’0+ Im g(x âˆ’iÎ·) =
âˆš
4Ïƒ 2 âˆ’x2
2Ï€Ïƒ 2
,
âˆ’2Ïƒ â‰¤x â‰¤2Ïƒ.
(2.49)
Note the following features of the semi-circle law (see Fig. 2.4): (1) asymptotically there
is no eigenvalue for x > 2Ïƒ and x < âˆ’2Ïƒ; (2) the eigenvalue density has square-root
singularities near the edges: Ï(x) âˆ¼
âˆš
x + 2Ïƒ near the left edge and Ï(x) âˆ¼
âˆš
2Ïƒ âˆ’x
near the right edge. For ï¬nite N, some eigenvalues are present in a small region of width
Nâˆ’2/3 around the edges, see Section 14.1.

2.3 Resolvent and Stieltjes Transform
27
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
0.0
0.1
0.2
0.3
Figure 2.4 Density of eigenvalues of a Wigner matrix with Ïƒ = 1: the semi-circle law.
Exercise 2.3.3
From the moments to the density
A large random matrix has moments Ï„(Ak) = 1/k.
(a)
Using Eq. (2.24) write the Taylor series of g(z) around inï¬nity.
(b)
Sum the series to get a simple expression for g(z). Hint: look up the Taylor
series of log(1 + x).
(c)
Where are the singularities of g(z) on the real axis?
(d)
Use Eq. (2.47) to ï¬nd the density of eigenvalues Ï(Î»).
(e)
Check your result by recomputing the moments and the Stieltjes transform
from Ï(Î»).
(f)
Redo all the above steps for a matrix whose odd moments are zero and even
moments are Ï„(A2k) = 1. Note that in this case the density Ï(Î») has Dirac
masses.
2.3.6 Stieltjes Transform on the Real Axis
What about computing the Stieltjes transform when z is real and inside the spectrum?
This seems dangerous at ï¬rst sight, since gN(z) diverges when z is equal to one of the
eigenvalues of X. As these eigenvalues become more and more numerous as N goes to
inï¬nity, z will always be very close to a pole of the resolvent gN(z). Interestingly, one can
turn this predicament on its head and actually exploit these divergences. In a hand-waving
manner, the probability that the difference di = |z âˆ’Î»i| between z (now real) and a given
eigenvalue Î»i is very small, is given by
P[di < Ïµ/N] = 2ÏµÏ(z),
(2.50)

28
Wigner Ensemble and Semi-Circle Law
where Ï(z) is the normalized density of eigenvalues around z. But as Ïµ â†’0, i.e. when
z is extremely close to Î»i, the resolvent becomes dominated by a unique contribution â€“
that of the Î»i term, all other terms (z âˆ’Î»j)âˆ’1, j  i, become negligible. In other words,
gN(z) â‰ˆÂ±(Ndi)âˆ’1, and therefore
P[|g| > Ïµâˆ’1] = P[di < Ïµ/N] = 2ÏµÏ(z).
(2.51)
Hence, gN(z) does not converge for N â†’âˆwhen z is real, but the tail of its distribution
decays precisely as Ï(z)/g2. Studying this tail thus allows one to extract the eigenvalue
density Ï(z) while working directly on the real axis. Let us show how this works in the
case of Wigner matrices.
The idea is that, for a rotationally invariant problem, the distribution of a randomly
chosen diagonal element of the resolvent (say G11) is the same as the distribution P(g)
of the normalized trace.3 With this assumption, Eq. (2.29) can be interpreted as giving the
evolution of P(g) itself, i.e.
P (N)(g) =
 +âˆ
âˆ’âˆ
dgâ€²P (Nâˆ’1)(gâ€²)Î´

g âˆ’
1
z âˆ’Ïƒ 2gâ€²

,
(2.52)
where we have used the fact that, for large N, N
k,â„“=2 M1k(M22)âˆ’1
kâ„“Mâ„“â„“â†’Ïƒ 2g(Nâˆ’1).
Now, this functional iteration admits the following Cauchy distribution as a ï¬xed point:
P âˆ(g) =
Ï(z)
(g âˆ’
z
2Ïƒ 2 )2 + Ï€2Ï(z).
(2.53)
This simple result, that the resolvent of a Wigner matrix on the real axis is a Cauchy vari-
able, calls for several comments. First, one ï¬nds that P âˆ(g) indeed behaves as Ï(z)/g2
for large g, as argued above. Second, it would have been entirely natural to ï¬nd a Cauchy
distribution for g had the eigenvalues been independent. Indeed, since g is then the sum
of N random variables (i.e. the 1/diâ€™s) distributed with an inverse square power, the
generalized clt predicts that the resulting sum is Cauchy distributed. In the present case,
however, the eigenvalues are strongly correlated â€“ see Section 5.1.4. It was recently proven
that the Cauchy distribution is in fact super-universal and holds for a wide class of point
processes on the real axis, in particular for the eigenvalues of random matrices. It is in fact
even true when these eigenvalues are strictly equidistant, with a random global shift.
Bibliographical Notes
â€¢ The overall content of this chapter is covered in many books and textbooks, see for
example
â€“ M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
â€“ T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
â€“ G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010,
â€“ G. Blower. Random Matrices: High Dimensional Phenomena. Cambridge University
Press, Cambridge, 2009,
3 This is not a trivial statement but it can be proven along the lines of Aizenman and Warzel [2015].

2.3 Resolvent and Stieltjes Transform
29
â€“ B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006,
â€“ G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018.
â€¢ For some historical papers, see
â€“ E. P. Wigner. On the statistical distribution of the widths and spacings of nuclear res-
onance levels. Mathematical Proceedings of the Cambridge Philosophical Society,
47(4):790â€“798, 1951,
â€“ F. J. Dyson. The threefold way: algebraic structure of symmetry groups and ensem-
bles in quantum mechanics. Journal of Mathematical Physics, 3(6):1199â€“1215,
1962.
â€¢ About the Stieltjes transform on the real line and the super-universality of the Cauchy
distribution, see
â€“ M. Aizenman and S. Warzel. On the ubiquity of the Cauchy distribution in spectral
problems. Probability Theory and Related Fields, 163:61â€“87, 2015,
â€“ Y. V. Fyodorov and D. V. Savin. Statistics of impedance, local density of states, and
reï¬‚ection in quantum chaotic systems with absorption. Journal of Experimental and
Theoretical Physics Letters, 80(12):725â€“729, 2004,
â€“ Y. V. Fyodorov and I. Williams. Replica symmetry breaking condition exposed by
random matrix calculation of landscape complexity. Journal of Statistical Physics,
129(5-6):1081â€“1116, 2007,
â€“ J.-P. Bouchaud and M. Potters. Two short pieces around the Wigner problem. Journal
of Physics A: Mathematical and Theoretical, 52(2):024001, 2018,
and for a related discussion see also
â€“ M. Griniasty and V. Hakim. Correlations and dynamics in ensembles of maps: Simple
models. Physical Review E, 49(4):2661, 1994.

3
More on Gaussian Matrices*
In the previous chapter, we dealt with the simplest of all Gaussian matrix ensembles, where
entries are real, Gaussian random variables, and global symmetry is imposed. It was pointed
out by Dyson that there exist precisely three division rings that contain the real numbers,
namely, the real themselves, the complex numbers and the quaternions. He showed that this
fact implies that there are only three acceptable ensembles of Gaussian random matrices
with real eigenvalues: goe, gue and gse. Each is associated with a Dyson index called Î²
(1, 2 and 4, respectively) and except for this difference in Î² almost all of the results in
this book (and many more) apply to the three ensembles. In particular their moments and
eigenvalue density are the same as N â†’âˆ, while correlations and deviations from the
asymptotic formulas follow families of laws with Î² as a parameter. In this chapter we will
review the other two ensembles (gue and gse),1 and also discuss the general moments of
Gaussian random matrices, for which some interesting mathematical tools are available,
that are useful beyond rmt.
3.1 Other Gaussian Ensembles
3.1.1 Complex Hermitian Matrices
For matrices with complex entries, the analog of a symmetric matrix is a (complex) Her-
mitian matrix. It satisï¬es Aâ€  = A where the dagger operator is the combination of matrix
transposition and complex conjugation. There are two important reasons to study com-
plex Hermitian matrices. First they appear in many applications, especially in quantum
mechanics. There, the energy and other observables are mapped into Hermitian operators,
or Hermitian matrices for systems with a ï¬nite number of states. The ï¬rst large N result
of random matrix theory is the Wigner semi-circle law. As recalled in the introduction to
Chapter 2, it was obtained by Wigner as he modeled the energy levels of complex heavy
nuclei as a random Hermitian matrix.
1 More recently, it was shown how ensembles with an arbitrary value of Î² can be constructed, see Dumitriu and Edelman
[2002], Allez et al. [2012].
30

3.1 Other Gaussian Ensembles
31
The other reason Hermitian matrices are important is mathematical. In the large N limit,
the three ensembles (real, complex and quaternionic (see below)) behave the same way.
But for ï¬nite N, computations and proofs are much simpler in the complex case. The main
reason is that the Vandermonde determinant which we will introduce in Section 5.1.4
is easier to manipulate in the complex case. For this reason, most mathematicians
discuss the complex Hermitian case ï¬rst and treat the real and quaternionic cases as
extensions. In this book we want to stay close to applications in data science and statistical
physics, so we will discuss complex matrices only in the present chapter. In the rest of
the book we will indicate in footnotes how to extend the result to complex Hermitian
matrices.
A complex Hermitian matrix A has real eigenvalues and it can be diagonalized with
a suitable unitary matrix U. A unitary matrix satisï¬es Uâ€ U = 1. So A can be written as
A = UUâ€ , with  the diagonal matrix containing its N eigenvalues.
We want to build the complex Wigner matrix: a Hermitian matrix with iid Gaussian
entries. We will choose a construction that has unitary invariance for every N. Let us study
the unitary invariance of complex Gaussian vectors. First we need to deï¬ne a complex
Gaussian variable.
We say that the complex variable z is centered Gaussian with variance Ïƒ 2 if z = xr +i xi
where xr and xi are centered Gaussian variables of variance Ïƒ 2/2. We have
E[|z|2] = E[x2
r ] + E[x2
i ] = Ïƒ 2.
(3.1)
A white complex Gaussian vector x is a vector whose components are iid complex centered
Gaussians. Consider y = Ux where U is a unitary matrix. Each of the components is a linear
combination of Gaussian variables so y is Gaussian. It is relatively straightforward to show
that each component has the same variance Ïƒ 2 and that there is no covariance between
different components. Hence y is also a white Gaussian vector. The ensemble of a white
complex Gaussian vector is invariant under unitary transformation.
To deï¬ne the Hermitian Wigner matrix, we ï¬rst deï¬ne a (non-symmetric) square
matrix H whose entries are centered complex Gaussian numbers and let X be the Hermitian
matrix deï¬ned by
X = H + Hâ€ .
(3.2)
If we repeat the arguments of Section 2.2.2, we can show that the ensemble of X is invariant
under unitary transformation: UXUâ€  in law
= X.
We did not specify the variance of the elements of H. We would like X to be normalized
as Ï„(X2) = Ïƒ 2 +O(1/N). Choosing the variance of the H as E[|Hij|2] = 1/(2N) achieves
precisely that.
The Hermitian matrix X has real diagonal elements with E[X2
ii] = 1/N and off-diagonal
elements that are complex Gaussian with E[|Xij|2] = 1/N. In other words the real and
imaginary parts of the off-diagonal elements of X have variance 1/(2N). We can put

32
More on Gaussian Matrices
all this information together in the joint law of the matrix elements of the Hermitian
matrix H:
P({Xij}) âˆexp
$
âˆ’N
2Ïƒ 2 Tr X2
%
.
(3.3)
This law is identical to the real symmetric case (Eq. 2.17) up to a factor of 2. We can then
write both the symmetric and the Hermitian case as
P({Xij}) âˆexp
$
âˆ’Î²N
4Ïƒ 2 Tr X2
%
,
(3.4)
where Î² is 1 or 2 respectively.
The complex Hermitian Wigner ensemble is called the Gaussian unitary ensemble
or gue.
The results of the previous chapter apply equally to the real symmetric and the
complex Hermitian case. Both the self-consistent equation for the Stieltjes transform
and the counting of non-crossing pair partitions (see next section) rely on the independence
of the elements of the matrix and on the fact that E[|Xij|2] = 1/N, true in both cases.
We then have that the Stieltjes transform of the two ensembles is the same and they have
exactly the same semi-circle distribution of eigenvalues in the large N limit. The same will
be true for the quaternionic case (Î² = 4) in the next section, and in fact for all values of Î²
provided NÎ² â†’âˆwhen N â†’âˆ, see Section 5.3.1:
ÏÎ²(Î») =
âˆš
4Ïƒ 2 âˆ’Î»2
2Ï€Ïƒ 2
,
âˆ’2Ïƒ â‰¤Î» â‰¤2Ïƒ.
(3.5)
3.1.2 Quaternionic Hermitian Matrices
We will deï¬ne here the quaternionic Hermitian matrices and the gse. There are many
fewer applications of quaternionic matrices than the more common real or complex matri-
ces. We include this discussion here for completeness. In the literature the link between
symplectic matrices and quaternions can be quite obscure for the novice reader. Except for
the existence of an ensemble of matrices with Î² = 4 we will never refer to quaternionic
matrices after this section, which can safely be skipped.
Quaternions are non-commutative extensions of the real and complex numbers. They
are written as real linear combinations of the real number 1 and three abstract non-
commuting objects (i,j,k) satisfying
i2 = j2 = k2 = ijk = âˆ’1
â‡’
ij = âˆ’ji = k,
jk = âˆ’kj = i,
ki = âˆ’ik = j. (3.6)
So we can write a quaternion as h = xr+i xi+j xj+k xk. If only xr is non-zero we say that
h is real. We deï¬ne the quaternionic conjugation as 1âˆ—= 1,iâˆ—= âˆ’i,jâˆ—= âˆ’j,kâˆ—= âˆ’k
so that the norm |h|2 := hhâˆ—= x2r + x2
i + x2
j + x2
k is always real and non-negative. The
abstract objects i, j and k can be represented as 2 Ã— 2 complex matrices:
1 =

1
0
0
1

,
i =

i
0
0
âˆ’i

,
j =

0
1
âˆ’1
0

,
k =

0
i
i
0

,
(3.7)
where the i in the matrices is now the usual unit imaginary number.
Quaternions share all the algebraic properties of real and complex numbers except for
commutativity (they form a division ring). Since matrices in general do not commute,
matrices built out of quaternions behave like real or complex matrices.

3.1 Other Gaussian Ensembles
33
A Hermitian quaternionic matrix is a square matrix A whose elements are quaternions
and satisfy A = Aâ€ . Here the dagger operator is the combination of matrix transposition
and quaternionic conjugation. They are diagonalizable and their eigenvalues are real.
Matrices that diagonalize Hermitian quaternionic matrices are called symplectic. Written
in terms of quaternions they satisfy SSâ€  = 1.
Given representation of quaternions as 2Ã—2 complex matrices, an N Ã—N quaternionic
Hermitian matrix A can be written as a 2N Ã— 2N complex matrix Q(A). We choose a
representation where
Z := Q(1j) =

0
1
âˆ’1
0

.
(3.8)
For a 2N Ã— 2N complex matrix Q to be the representation of a quaternionic Hermitian
matrix it has to have two properties. First, quaternionic conjugation acts just like Hermitian
conjugation so Qâ€  = Q. Second, it has to be expressible as a real linear combination of
unit quaternions. One can show that such matrices (and only them) satisfy
QR := ZQT Zâˆ’1 = Qâ€ ,
(3.9)
where QR is called the dual of Q. In other words an N Ã—N Hermitian quaternionic matrix
corresponds to a 2N Ã— 2N self-dual Hermitian matrix (i.e. Q = Qâ€  = QR). In this
2N Ã— 2N representation symplectic matrices are complex matrices satisfying
SSâ€  = SSR = 1.
(3.10)
To recap, a 2N Ã— 2N Hermitian self-dual matrix Q can be diagonalized by a symplectic
matrix S. Its 2N eigenvalues are real and they occur in pairs as they are the N eigenvalues
of the equivalent Hermitian quaternionic N Ã— N matrix.
We can now deï¬ne the third Gaussian matrix ensemble, namely the Gaussian symplec-
tic ensemble (gse) consisting of Hermitian quaternionic matrices whose off-diagonal ele-
ments are quaternions with Gaussian distribution of zero mean and variance E[|Xij|2] =
1/N. This means that each of the four components of each Xij is a Gaussian number of
zero mean and variance 1/(4N). The diagonal elements of X are real Gaussian numbers
with zero mean and variance 1/(2N). As usual Xij = Xâˆ—
ji so only the upper (or lower)
triangular elements are independent. The joint law for the elements of a gse matrix with
variance Ï„(X2) = Ïƒ 2 is given by
P({Xij}) âˆexp
$
âˆ’N
Ïƒ 2 Tr X2
%
,
(3.11)
which we identify with Eq. (3.4) with Î² = 4. This parameter Î² = 4 is a fundamental prop-
erty of the symplectic group and will consistently appear in contrast with the orthogonal
and unitary cases, Î² = 1 and Î² = 2 (see Section 5.1.4).
The parameter Î² can be interpreted as the randomness in the norm of the matrix
elements. More precisely, we have
|Xij|2 =
â§
âªâªâ¨
âªâªâ©
x2r
for real symmetric,
x2r + x2
i
for complex Hermitian,
x2r + x2
i + x2
j + x2
k
for quaternionic Hermitian,
(3.12)
where xr,xi,xj,xk are real Gaussian numbers such that E[|Xij|2] = 1. We see that the
ï¬‚uctuations of |Xij|2 decrease with Î² (precisely V[|Xij|2] = 2/Î²). By the law of large
numbers (lln), in the Î² â†’âˆlimit (if such an ensemble existed) we would have
|Xij|2 = 1 with no ï¬‚uctuations.

34
More on Gaussian Matrices
Exercise 3.1.1
Quaternionic matrices of size one
The four matrices in Eq. (3.7) can be thought of as the 2 Ã— 2 complex
representations of the four unit quaternions.
(a)
Deï¬ne Z := j and compute Zâˆ’1.
(b)
Show that for all four matrices Q, we have ZQT Zâˆ’1 = Qâ€  where the dagger
here is the usual transpose plus complex conjugation.
(c)
Convince yourself that, by linearity, any Q that is a real linear combination of
the 2 Ã— 2 matrices i, j, k and 1 must satisfy ZQT Zâˆ’1 = Qâ€ .
(d)
Give an example of a matrix Q that does not satisfy ZQT Zâˆ’1 = Qâ€ .
3.1.3 The Ginibre Ensemble
The Gaussian orthogonal ensemble is such that all matrix elements of X are iid Gaussian,
but with the strong constraint that Xij = Xji, which makes sure that all eigenvalues of
X are real. What happens if we drop this constraint and consider a square matrix H with
independent entries? In this case, one may choose two different routes, depending on the
context.
â€¢ One route is simply to allow eigenvalues to be complex numbers. One can then study
the eigenvalue distribution in the complex plane, so the distribution becomes a two-
dimensional density. Some of the tools introduced in the previous chapter, such as the
Sokhotskiâ€“Plemelj formula, can be generalized to complex eigenvalues. The ï¬nal result
is called the Girko circular law: the density of eigenvalues is constant within a disk
centered at zero and of radius Ïƒ (see Fig. 3.1). In the general case where E[HijHji] =
ÏÏƒ 2, the eigenvalues are conï¬ned within an ellipse of half-width (1 + Ï)Ïƒ along the
real axis and (1 âˆ’Ï)Ïƒ in the imaginary direction, interpolating between a circle for
Ï = 0 (independent entries) and a line segment on the real axis of length 4Ïƒ for Ï =
1 (symmetric matrices).
â€¢ The other route is to focus on the singular values of H. One should thus study the real
eigenvalues of HT H when H is a square random matrix made of independent Gaussian
elements. This is precisely the Wishart problem that we will study in Chapter 4, for the
special parameter value q = 1. Calling s the square-root of these real eigenvalues, the
ï¬nal result is a quarter-circle:
Ï(s) =
âˆš
4Ïƒ 2 âˆ’s2
Ï€Ïƒ 2
;
s âˆˆ(0,2Ïƒ).
(3.13)

3.1 Other Gaussian Ensembles
35
âˆ’1.5
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
1.5
Rel
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
Iml
Figure 3.1 Complex eigenvalues of a random N = 1000 matrix taken from the Gaussian Ginibre
ensemble, i.e. a non-symmetric matrix with iid Gaussian elements with variance Ïƒ 2 = 1/N.
The dash line corresponds to the circle |Î»|2 = 1. As N â†’âˆthe density becomes uniform in
the complex unit disk. This distribution is called the circle law or sometimes, more accurately, the
disk law.
Exercise 3.1.2
Three quarter-circle laws
Let H be a (non-symmetric) square matrix of size N whose entries are iid
Gaussian random variable of variance Ïƒ 2/N. Then as a simple consequence
of the above discussion the following three sets of numbers are distributed
according to the quarter-circle law (3.13) in the large N limit. Deï¬ne
wi = |Î»i| where {Î»i} are the eigenvalues of H + HT
âˆš
2
,
ri = 2| Re Î»i| where {Î»i} are the eigenvalues of H,
si =
*
Î»i where {Î»i} are the eigenvalues of HHT .
(a)
Generate a large matrix H with say N = 1000 and Ïƒ 2 = 1 and plot the
histogram of the three above sets.
(b)
Although these three sets of numbers converge to the same distribution there
is no simple relation between them. In particular they are not equal. For a
moderate N (10 or 20) examine the three sets and realize that they are all
different.

36
More on Gaussian Matrices
3.2 Moments and Non-Crossing Pair Partitions
3.2.1 Fourth Moment of a Wigner Matrix
We have stated in Section 2.2.1 that for a Wigner matrix we have Ï„(X4) = 2Ïƒ 4. We will
now compute this fourth moment directly and then develop a technique to compute all other
moments.
We have
Ï„(X4) = 1
N E[Tr(X4)] = 1
N

i,j,k,l
E[XijXjkXklXli].
(3.14)
Recall that (Xij : 1 â‰¤i â‰¤j â‰¤N) are independent Gaussian random variables of mean
zero. So for the expectations in the above sum to be non-zero, each X entry needs to be
equal to another X entry.2 There are two possibilities. Either all four are equal or they are
equal pairwise. In the following we will not distinguish between diagonal and off-diagonal
terms; as there are many more off-diagonal terms these terms always dominate.
(1) If Xij = Xjk = Xkl = Xli, then
E[XijXjkXklXli] = 3Ïƒ 4
N2 ,
(3.15)
and there are N2 of them. Thus the total contribution from these terms is
1
N N2 3Ïƒ 4
N2 = 3Ïƒ 4
N
â†’0.
(3.16)
(2) Suppose there are two different pairs. Then there are three possibilities (see Fig. 3.2):
(i) Xij = Xjk, Xkl = Xli, and Xij is different than Xli (i.e. j  l). Then
(3.14) = 1
N

i,jl
E[X2
ijX2
il] = 1
N (N3 âˆ’N2)
Ïƒ 2
N
2
â†’Ïƒ 4
(3.17)
as N â†’âˆ.
Xi1,i2
Xi2,i3
Xi3,i4
Xi4,i3
Figure 3.2 Graphical representation of the three terms contributing to Ï„(X4). The last one is a
crossing partition and has a zero contribution.
2 When we say that Xij = Xkl, we mean that they are the same random variable; given that X is a symmetric matrix it means
either (i = k and j = l) or (i = l and j = k).

3.2 Moments and Non-Crossing Pair Partitions
37
(ii) Xij = Xli, Xjk = Xkl, and Xij is different than Xjk (i.e. i  k). Then
(3.14) = 1
N

ik,j
E[X2
ijX2
jk] = 1
N (N3 âˆ’N2)
Ïƒ 2
N
2
â†’Ïƒ 4
(3.18)
as N â†’âˆ.
(iii) Xij = Xkl, Xjk = Xli, and Xij is different than Xjk (i.e. i  k). Then we must have
i = l and j = k from Xij = Xkl, and i = j and k = l from Xjk = Xli. This gives a
contradiction: there are no such terms.
In sum, we obtain that
Ï„(X4) â†’Ïƒ 4 + Ïƒ 4 = 2Ïƒ 4
(3.19)
as N â†’âˆ, where the two terms come from the two non-crossing partitions, see
Figure 3.2.
In the next technical section, we generalize this calculation to arbitrary moments of
X. Odd moments are zero by symmetry. Even moments Ï„(X2k) can be written as sums
over non-crossing diagrams (non-crossing pair partitions of 2k elements), where each such
diagram contributes Ïƒ 2k. So
Ï„(X2k) = CkÏƒ 2k,
(3.20)
where Ck are Catalan numbers, the number of such non-crossing diagrams. They satisfy
Ck =
k

j=1
Cjâˆ’1Ckâˆ’j =
kâˆ’1

j=0
CjCkâˆ’jâˆ’1,
(3.21)
with C0 = C1 = 1, and can be written explicitly as
Ck =
1
k + 1
2k
k

,
(3.22)
see Section 3.2.3.
3.2.2 Catalan Numbers: Counting Non-Crossing Pair Partitions
We would like to calculate all moments of X. As written above, all the odd moments
Ï„(X2k+1) vanish (since the odd moments of a Gaussian random variable vanish). We only
need to compute the even moments:
Ï„(X2k) = 1
N E
&
Tr(X2k)
'
= 1
N

i1,...,i2k
E

Xi1i2Xi2i3 . . . Xi2ki1

.
(3.23)
Since we assume that the elements of X are Gaussian, we can expand the above
expectation value using Wickâ€™s theorem using the covariance of the {Xij}â€™s. The matrix
X is symmetric, so we have to keep track of the fact that Xij is the same variable as Xji.
For this reason, using Wickâ€™s theorem proves quite tedious and we will not follow this
route here.

38
More on Gaussian Matrices
From the Taylor series at inï¬nity of the Stieltjes transform, we expect every even
moment of X to converge to an O(1) number as N â†’âˆ. We will therefore drop any
O(1/N) or smaller term as we proceed. In particular the difference of variance between
diagonal and off-diagonal elements of X does not matter to ï¬rst order in 1/N.
In Eq. (3.23), each X entry must be equal to at least one another X entry, otherwise the
expectation is zero. On the other hand, it is easy to show that for the partitions that contain
at least one group with > 2 (actually â‰¥4) X entries that are equal to each other, their total
contribution will be of order O(1/N) or smaller (e.g. in case (1) of the previous section).
Thus we only need to consider the cases where each X entry is paired to exactly one other
X entry, which we also referred to as a pair partition.
We need to count the number of types of pairings of 2k elements that contribute to
Ï„(X2k) as N â†’âˆ. We associate to each pairing a diagram. For example, for k = 3, we
have 5! ! = 5 Â· 3 Â· 1 = 15 possible pairings (see Fig. 3.3).
To compute the contribution of each of these pair partitions, we will compute the
contribution of non-crossing pair partitions and argue that pair partitions with crossings
do not contribute in the large N limit. First we need to deï¬ne what is a non-crossing pair
partition of 2k elements. A pair partition can be draw as a diagram where the 2k elements
are points on a line and each point is joined with its pair partner by an arc drawn above that
line. If at least two arcs cross each other the partition is called crossing, and non-crossing
otherwise. In Figure 3.3 the ï¬ve partitions on the left are non-crossing while the ten others
are crossing.
In a non-crossing partition of size 2k, there is always at least one pairing between
consecutive points (the smallest arc). If we remove the ï¬rst such pairing we get a non-
crossing pair partition of 2k âˆ’2 elements. We can proceed in this way until we get to
a paring of only two elements: the unique (non-crossing) pair partition contributing to
(Fig. 3.4)
Ï„(X2) = Ïƒ 2.
(3.24)
Figure 3.3 Graphical representation of the 15 terms contributing to Ï„(X6). Only the ï¬ve on the left
are non-crossing and have a non-zero contribution as N â†’âˆ.
Xij
Xji
Figure 3.4 Graphical representation of the only term contributing to Ï„(X2). Note that the indices of
two terms are already equal prior to pairing.

3.2 Moments and Non-Crossing Pair Partitions
39
Xiâ„“iâ„“+1
Xiâ„“+1iâ„“+2
Xiâ„“+2iâ„“+3
Xiâ„“+3iâ„“+4
Figure 3.5 Zoom into the smallest arc of a non-crossing partition. The two middle matrices are paired
while the other two could be paired together or to other matrices to the left and right respectively.
After the pairing of Xiâ„“+1,iâ„“+2 and Xiâ„“+2,iâ„“+3, we have iâ„“+1 = iâ„“+3 and the index iâ„“+2 is free.
We can use this argument to prove by induction that each non-crossing partition con-
tributes a factor Ïƒ 2k. In Figure 3.5, consecutive elements Xiâ„“+1,iâ„“+2 and Xiâ„“+2,iâ„“+3 are
paired; we want to evaluate that pair and remove it from the diagram. The variance con-
tributes a factor Ïƒ 2/N. We can make two choices for index matching. First consider
iâ„“+1 = iâ„“+3 and iâ„“+2 = iâ„“+2. In that case, the index iâ„“+2 is free and its summation
contributes a factor of N. The identity iâ„“+1 = iâ„“+3 means that the previous matrix Xiâ„“,iâ„“+1
is now linked by matrix multiplication to the following matrix Xiâ„“+1,iâ„“+4. In other words
we are left with Ïƒ 2 times a non-crossing partition of size 2k âˆ’2, which contributes Ïƒ 2kâˆ’2
by our induction hypothesis. The other choice of index matching, iâ„“+1 = iâ„“+2 = iâ„“+3,
can be viewed as ï¬xing a particular value for iâ„“+2 and is included in the sum over iâ„“+2
in the previous index matching. So by induction we do have that each non-crossing pair
partition contributes Ïƒ 2k.
Before we discuss the contribution of crossing pair partitions, letâ€™s analyze in terms
of powers of N the computation we just did for the non-crossing case. The computation
of each term in Ï„(X2k) involves 2k matrices that have in total 4k indices. The trace and
the matrix multiplication forces 2k equalities among these indices. The normalization of
the trace and the k variance terms gives a factor of Ïƒ 2k/Nk+1. To get a result of order
1 we need to be left with k + 1 free indices whose summation gives a factor of Nk+1.
Each k pairing imposes a matching between pairs of indices. For the ï¬rst k âˆ’1 choice of
pairing we managed to match one pair of indices that were already equal. At the last step
we matched to pairs of indices that were already equal. Hence in total we added only k +1
equality constraints which left us with k + 1 free indices as needed.
We can now argue that crossing pair partitions do not contribute in the large N limit.
For crossing partition it is not possible to choose a matching at every step that matches
a pair of indices that are already equal. If we use the previous algorithm of removing at
each step the leftmost smallest arc, at some point, the smallest arc will have a crossing
and we will be pairing to matrices that share no indices, adding two equality constraints
at this step. The result will therefore be down by at least a factor of 1/N with respect to
the non-crossing case. This argument is not really a proof but an intuition why this might
be true.3
We can now complete our moments computation. Let
Ck := # of non-crossing pairings of 2k elements.
(3.25)
Since every non-crossing pair partition contributes a factor Ïƒ 2k, summing over all non-
crossing pairings we immediately get that
Ï„(X2k) = CkÏƒ 2k.
(3.26)
3 A more rigorous proof can be found in e.g. Anderson et al. [2010], Tao [2012] or Mingo and Speicher [2017]. In this last
reference, the authors compute the moments of X exactly for every N (when Ïƒ2
d = Ïƒ2
od).

40
More on Gaussian Matrices
1
2
2j âˆ’1
2j
2j + 1
2k
Figure 3.6 In a non-crossing pairing, the paring of site 1 with site 2j splits the graph into two disjoint
non-crossing parings.
3.2.3 Recursion Relation for Catalan Numbers
In order to compute the Catalan numbers Ck, we will write a recursion relation for
them. Take a non-crossing pairing, site 1 is linked to some even site 2j (it is easy to see
that 1 cannot link to an odd site in order for the partition to be non-crossing). Then the
diagram is split into two smaller non-crossing pairings of sizes 2(j âˆ’1) and 2(k âˆ’j),
respectively (see Fig. 3.6). Thus we get the inductive relation4
Ck =
k

j=1
Cjâˆ’1Ckâˆ’j =
kâˆ’1

j=0
CjCkâˆ’jâˆ’1,
(3.27)
where we let C0 = C1 = 1. One can then prove by induction that Ck is given by the
Catalan number:
Ck =
1
k + 1

2k
k

.
(3.28)
Using the Taylor series for the Stieltjes transform (2.22), we can use the Catalan number
recursion relation to ï¬nd an equation for the Stieltjes transform of the Wigner ensemble:
g(z) =
âˆ

k=0
Ck
z2k+1 Ïƒ 2k.
(3.29)
Thus, using (3.27), we obtain that
g(z) âˆ’1
z =
âˆ

k=1
Ïƒ 2k
z2k+1
â›
â
kâˆ’1

j=0
CjCkâˆ’jâˆ’1
â
â 
= Ïƒ 2
z
âˆ

j=0
Cj
z2j+1 Ïƒ 2j
â›
â
âˆ

k=j+1
Ckâˆ’jâˆ’1
z2(kâˆ’jâˆ’1)+1 Ïƒ 2(kâˆ’jâˆ’1)
â
â 
= Ïƒ 2
z
â›
â
âˆ

j=0
Cj
z2j+1 Ïƒ 2j
â
â 
â›
â
âˆ

â„“=0
Câ„“
z2â„“+1 Ïƒ 2â„“
â
â = Ïƒ 2
z g2(z),
(3.30)
which gives the same self-consistent equation for g(z) as in (2.35) and hence the same
solution:
g(z) = z âˆ’z
*
1 âˆ’4Ïƒ 2/z2
2Ïƒ 2
.
(3.31)
4 Interestingly, this recursion relation is also found in the problem of RNA folding. For deep connections between the physics of
RNA and rmt, see Orland and Zee [2002].

3.2 Moments and Non-Crossing Pair Partitions
41
The same result could have been derived by substituting the explicit solution for the
Catalan number Eq. (3.28) into (3.29), but this route requires knowledge of the Taylor
series:
âˆš
1 âˆ’x = 1 âˆ’
âˆ

k=0
2
k + 1
2k
k
 x
4
k+1
.
(3.32)
Exercise 3.2.1
Non-crossing pair partitions of eight elements
(a)
Draw all the non-crossing pair partitions of eight elements. Hint: use the
recursion expressed in Figure 3.6.
(b)
If X is a unit Wigner matrix, what is Ï„(X8)?
Bibliographical Notes
â€¢ Again, several books cover the content of this chapter, see for example
â€“ M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
â€“ G. Blower. Random Matrices: High Dimensional Phenomena. Cambridge University
Press, Cambridge, 2009,
â€“ B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006,
and in particular for a detailed discussion of the relation between non-crossing and large
matrices, see
â€“ T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
â€“ G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010,
â€“ J. A. Mingo and R. Speicher. Free Probability and Random Matrices. Springer, New
York, 2017.
â€¢ Concerning the construction of generalized beta ensembles, see e.g.
â€“ I. Dumitriu and A. Edelman. Matrix models for beta ensembles. Journal of Mathe-
matical Physics, 43(11):5830â€“5847, 2002,
â€“ R. Allez, J. P. Bouchaud, and A. Guionnet. Invariant beta ensembles and the Gauss-
Wigner crossover. Physical Review Letters, 109(9):094102, 2012.
â€¢ On the Ginibre ensemble and the circular law, see
â€“ V. L. Girko. Circular law. Theory of Probability and Its Applications, 29(4):694â€“706,
1985,
â€“ H. J. Sommers, A. Crisanti, H. Sompolinsky, and Y. Stein. Spectrum of large random
asymmetric matrices. Physical Review Letters, 60(19):1895â€“1899, 1988,
â€“ C. Bordenave and D. ChafaÂ¨Ä±. Around the circular law. Probability Surveys, 9, 2012.

42
More on Gaussian Matrices
â€¢ On the connection between non-crossing diagrams and RNA folding, see
â€“ P.-G. de Gennes. Statistics of branching and hairpin helices for the dAT copolymer.
Biopolymers, 6(5):715â€“729, 1968,
â€“ H. Orland and A. Zee. RNA folding and large N matrix theory. Nuclear Physics B,
620(3):456â€“476, 2002.

4
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
In this chapter we will study the statistical properties of large sample covariance matrices of
some N-dimensional variables observed T times. More precisely, the empirical set consists
of N Ã— T data {xt
i }1â‰¤iâ‰¤N,1â‰¤tâ‰¤T , where we have T observations and each observation
contains N variables. Examples abound: we could consider the daily returns of N stocks,
over a certain time period, or the number of spikes ï¬red by N neurons during T consecutive
time intervals of length t, etc. Throughout this book, we will use the notation q for the
ratio N/T . When the number of observations is much larger than the number of variables,
one has q â‰ª1. If the number of observations is smaller than the number of variables (a
case that can easily happen in practice), then q > 1.
In the case where q â†’0, one can faithfully reconstruct the â€œtrueâ€ (or population)
covariance matrix C of the N variables from empirical data. For q = O(1), on the other
hand, the empirical (or sample) covariance matrix is a strongly distorted version of C, even
in the limit of a large number of observations. This is not surprising since we are trying
to estimate O(N2/2) matrix elements from O(NT ) observations. In this chapter, we will
derive the well-known MarË‡cenkoâ€“Pastur law for the eigenvalues of the sample covariance
matrix for arbitrary values of q, in the â€œwhiteâ€ case where the population covariance matrix
C is the identity matrix C = 1.
4.1 Wishart Matrices
4.1.1 Sample Covariance Matrices
We assume that the observed variables xt
i have zero mean. (Otherwise, we need to remove
the sample mean T âˆ’1 
t xt
i from xt
i for each i. For simplicity, we will not consider this
case.) Then the sample covariances of the data are given by
Eij = 1
T
T

t=1
xt
i xt
j.
(4.1)
Thus Eij form an N Ã— N matrix E, called the sample covariance matrix (scm), which we
we write in a compact form as
43

44
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
E = 1
T HHT,
(4.2)
where H is an N Ã— T data matrix with entries Hit = xt
i .
The matrix E is symmetric and positive semi-deï¬nite:
E = ET,
and
vT Ev = (1/T )âˆ¥HT vâˆ¥2 â‰¥0,
(4.3)
for any v âˆˆRN. Thus E is diagonalizable and has all eigenvalues Î»E
k â‰¥0.
We can deï¬ne another covariance matrix by transposing the data matrix H:
F = 1
N HT H.
(4.4)
The matrix F is a T Ã—T matrix, it is also symmetric and positive semi-deï¬nite. If the index
i (1 < i < N) labels the variables and the index t (1 < t < T ) the observations, we can
call the matrix F the covariance of the observations (as opposed to E the covariance of the
variables). Fts measures how similar the observations at t are to those at s â€“ in the above
example of neurons, it would measure how similar is the ï¬ring pattern at time t and at
time s.
As we saw in Section 1.1.3, the matrices T E and NF have the same non-zero eigenval-
ues. Also the matrix E has at least N âˆ’T zero eigenvalues if N > T (and F has at least
T âˆ’N zero eigenvalues if T > N).
Assume for a moment that N â‰¤T (i.e. q â‰¤1), then we know that F has N (zero or
non-zero) eigenvalues inherited from E and equal to qâˆ’1Î»E
k , and T âˆ’N zero eigenvalues.
This allows us to write an exact relation between the Stieltjes transforms of E and F:
gF
T (z) = 1
T
T

k=1
1
z âˆ’Î»F
k
= 1
T
 N

k=1
1
z âˆ’qâˆ’1Î»E
k
+ (T âˆ’N)
1
z âˆ’0

= q2gE
N(qz) + 1 âˆ’q
z
.
(4.5)
A similar argument with T < N leads to the same Eq. (4.5) so it is actually valid for any
value of q. The relationship should be true as well in the large N limit:
gF(z) = q2gE(qz) + 1 âˆ’q
z
.
(4.6)
4.1.2 First and Second Moments of a Wishart Matrix
We now study the scm E. Assume that the column vectors of H are drawn independently
from a multivariate Gaussian distribution with mean zero and â€œtrueâ€ (or â€œpopulationâ€)
covariance matrix C, i.e.
E[HitHjs] = CijÎ´ts,
(4.7)

4.1 Wishart Matrices
45
with, again,
E = 1
T HHT .
(4.8)
Sample covariance matrices of this type were ï¬rst studied by the Scottish mathematician
John Wishart (1898â€“1956) and are now called Wishart matrices.
Recall that if (X1, . . . ,X2n) is a zero-mean multivariate normal random vector, then by
Wickâ€™s theorem,
E[X1X2 Â· Â· Â· X2n] =

pairings
,
pairs
E[XiXj] =

pairings
,
pairs
Cov(Xi,Xj),
(4.9)
where 
pairings

pairs means that we sum over all distinct pairings of {X1, . . . ,X2n} and
each summand is the product of the n pairs.
First taking expectation, we obtain that
E[Eij] = 1
T E
-
T

t=1
HitHjt
.
= 1
T
T

t=1
Cij = Cij.
(4.10)
Thus, we have E[E] = C: as it is well known, the scm is an unbiased estimator of the true
covariance matrix (at least when E[xt
i ] = 0).
For the ï¬‚uctuations, we need to study the higher order moments of E. The second
moment can be calculated as
Ï„(E2) :=
1
NT 2 E

Tr(HHT HHT )

=
1
NT 2

i,j,t,s
E

HitHjtHjsHis

.
(4.11)
Then by Wickâ€™s theorem, we have (see Fig. 4.1)
Ï„(E2) =
1
NT 2

t,s

i,j
C2
ij +
1
NT 2

t,s

i,j
CiiCjjÎ´ts +
1
NT 2

t,s

i,j
C2
ijÎ´ts
= Ï„(C2) + N
T Ï„(C)2 + 1
T Ï„(C2).
(4.12)
Suppose N,T â†’âˆwith some ï¬xed ratio N/T = q for some constant q > 0. The last
term on the right hand side then tends to zero and we get
Hit
Hjt
Hjs
His
Hit
Hjt
Hjs
His
Hit
Hjt
Hjs
His
Figure 4.1 Graphical representation of the three Wickâ€™s contractions corresponding to the three terms
in Eq. (4.12).

46
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
Ï„(E2) â†’Ï„(C2) + qÏ„(C)2.
(4.13)
The variance of the scm is greater than that of the true covariance by a term proportional to
q. When q â†’0 we recover perfect estimation and the two matrices have the same variance.
If C = Î±1 (a multiple of the identity) then Ï„(C2) âˆ’Ï„(C)2 = 0 but Ï„(E2) âˆ’Ï„(E)2 â†’qÎ±2.
4.1.3 The Law of Wishart Matrices
Next, we give the joint distribution of elements of E. For each ï¬xed column of H, the joint
distribution of the elements is
P

{Hit}N
i=1

=
1
*
(2Ï€)N det C
exp
â¡
â£âˆ’1
2

i,j
Hit(C)âˆ’1
ij Hjt
â¤
â¦.
(4.14)
Taking the product over 1 â‰¤t â‰¤T (since the columns are independent), we obtain
P (H) =
1
(2Ï€)
NT
2 det CT/2 exp
(
âˆ’1
2 Tr

HT Câˆ’1H
)
=
1
(2Ï€)
NT
2 det CT/2 exp
(
âˆ’T
2 Tr

ECâˆ’1)
.
(4.15)
Let us now make a change in variables H â†’E. As shown in the technical paragraph 4.1.4,
the Jacobian of the transformation is proportional to (det E)
T âˆ’Nâˆ’1
2
. The following exact
expression for the law of the matrix elements was obtained by Wishart:1
P (E) = (T /2)NT/2

N(T /2)
(det E)(T âˆ’Nâˆ’1)/2
(det C)T/2
exp
(
âˆ’T
2 Tr

ECâˆ’1)
,
(4.16)
where 
N is the multivariate gamma function. Note that the density is restricted to positive
semi-deï¬nite matrices E. The Wishart distribution can be thought of as the matrix general-
ization of the gamma distribution. Indeed for N = 1, P(E) reduces to a such a distribution:
PÎ³ (x) =
ba

(a)xaâˆ’1eâˆ’bx,
(4.17)
where b = T /(2C) and a = T /2. Using the identity det E = exp(Tr log E), we can rewrite
the above expression as2
1 Note that the Wishart distribution is often given with the normalization E[E] = T C as opposed to E[E] = C used here.
2 Complex and quaternionic Hermitian white Wishart matrices have a similar law of the elements with a factor of Î² in the
exponential:
P (W) âˆexp
(
âˆ’Î²N
2
Tr V (W)
)
with V (x) = N âˆ’T âˆ’1 + 2/Î²
N
log x + T
N x,
(4.18)
with Î² equal to 1, 2 or 4 as usual. The large N limit V (x) is the same in all three cases and is given by Eq. (4.21).

4.1 Wishart Matrices
47
P (E) = (T /2)NT/2

N(T /2)
1
(det C)T/2 exp
(
âˆ’T
2 Tr

ECâˆ’1
+ T âˆ’N âˆ’1
2
Tr log E
)
.
(4.19)
We will denote by W a scm with C = 1 and call such a matrix a white Wishart matrix.
In this case, as N,T â†’âˆwith q := N/T , we get that
P (W) âˆexp
(
âˆ’N
2 Tr V (W)
)
,
(4.20)
where
V (W) := (1 âˆ’qâˆ’1) log W + qâˆ’1W.
(4.21)
Note that the above P(W) is rotationally invariant in the white case. In fact, if a vector
v has Gaussian distribution N(0,1NÃ—N), then Ov has the same distribution N(0,1NÃ—N)
for any orthogonal matrix O. Hence OH has the same distribution as H, which shows that
OEOT has the same distribution as E.
4.1.4 Jacobian of the Transformation H â†’E
The aim here is to compute the volume Ï’(E) corresponding to all Hâ€™s such that
E = T âˆ’1HHT :
Ï’(E) =

dH Î´(E âˆ’T âˆ’1HHT ).
(4.22)
Note that this volume is the inverse of the Jacobian of the transformation H â†’E. Next
note that one can choose E to be diagonal, because one can always rotate the integral over
H to an integral over OH, where O is the rotation matrix that makes E diagonal. Now,
introducing the Fourier representation of the Î´ function for all N(N + 1)/2 independent
components of E, one has
Ï’(E) =

dHdA exp

i Tr(AE âˆ’T âˆ’1AHHT )

,
(4.23)
where A is the symmetric matrix of the corresponding Fourier variables, to which we add a
small imaginary part proportional to 1 to make all the following integrals well deï¬ned. The
Gaussian integral over H can now be performed explicitly for all t = 1, . . . ,T , leading to

dH exp

âˆ’iT âˆ’1 Tr(AHHT )

âˆ(det A)âˆ’T/2,
(4.24)
leaving us with
Ï’(E) âˆ

dA exp (i Tr(AE)) (det A)âˆ’T/2.
(4.25)
We can change variables from A to B = E
1
2 AE
1
2 . The Jacobian of this transformation is
,
i
dAii
,
j>i
dAij =
,
i
Eâˆ’1
ii
,
j>i
(EiiEjj)âˆ’1
2
,
i
dBii
,
j>i
dBij
= (det(E))âˆ’N+1
2
,
i
dBii
,
j>i
dBij.
(4.26)

48
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
So ï¬nally,
Ï’(E) âˆ
(
dB exp (i Tr(B)) (det B)âˆ’T/2
)
(det(E))
T âˆ’Nâˆ’1
2
,
(4.27)
as announced in the main text.
4.2 MarË‡cenkoâ€“Pastur Using the Cavity Method
4.2.1 Self-Consistent Equation for the Resolvent
We ï¬rst derive the asymptotic distribution of eigenvalues of the Wishart matrix with C = 1,
i.e. the MarË‡cenkoâ€“Pastur distribution. We will use the same method as in the derivation of
the Wigner semi-circle law in Section 2.3. In the case C = 1, the N Ã— T matrix H is ï¬lled
with iid standard Gaussian random numbers and we have W = (1/T )HHT .
As in Section 2.3, we wish to derive a self-consistent equation satisï¬ed by the Stieltjes
transform:
gW(z) = Ï„ (GW(z)),
GW(z) := (z1 âˆ’W)âˆ’1.
(4.28)
We ï¬x a large N and ï¬rst write an equation for the element 11 of GW(z). We will argue
later that G11(z) converges to g(z) with negligible ï¬‚uctuations. (We henceforth drop the
subscript W as this entire section deals with the white Wishart case.)
Using again the Schur complement formula (1.32), we have that
1
(G(z))11
= M11 âˆ’M12(M22)âˆ’1M21,
(4.29)
where M := z1 âˆ’W, and the submatrices of size, respectively, [M11] = 1 Ã— 1, [M12] =
1 Ã— (N âˆ’1), [M21] = (N âˆ’1) Ã— 1, [M22] = (N âˆ’1) Ã— (N âˆ’1). We can expand the above
expression and write
1
(G(z))11
= z âˆ’W11 âˆ’1
T 2
T

t,s=1
N

j,k=2
H1tHjt(M22)âˆ’1
jk HksH1s.
(4.30)
Note that the three matrices M22, Hjt (j â‰¥2) and Hks (k â‰¥2) are independent of the
entries H1t for all t. We can write the last term on the right hand side as
1
T
N

t,s=1
H1t tsH1s
with
 ts := 1
T
N

j,k=2
Hjt(M22)âˆ’1
jk Hks.
(4.31)

4.2 MarË‡cenkoâ€“Pastur Using the Cavity Method
49
Provided Î³ 2 := T âˆ’1 Tr  2 converges to a ï¬nite limit when T â†’âˆ,3 one readily shows
that the above sum converges to T âˆ’1 Tr  with ï¬‚uctuations of the order of Î³ T âˆ’1
2 . So we
have, for large T ,
1
(G(z))11
= z âˆ’W11 âˆ’1
T

2â‰¤j,kâ‰¤N

t HktHjt
T
(M22)âˆ’1
jk + O

T âˆ’1
2

= z âˆ’W11 âˆ’1
T

2â‰¤j,kâ‰¤N
Wkj(M22)âˆ’1
jk + O

T âˆ’1
2

= z âˆ’1 âˆ’1
T Tr W2G2(z) + O

T âˆ’1
2

,
(4.32)
where in the last step we have used the fact that W11 = 1 + O

T âˆ’1
2 
and noted W2 and
G2(z) the scm and resolvent of the N âˆ’1 variables excluding (1). We can rewrite the trace
term:
Tr(W2G2(z)) = Tr

W2(z1 âˆ’W2)âˆ’1
= âˆ’Tr 1 + z Tr

(z1 âˆ’W2)âˆ’1
= âˆ’Tr 1 + z Tr G2(z).
(4.33)
In the region where Tr G(z)/N converges for large N to the deterministic g(z), Tr G2(z)/N
should also converge to the same limit as G2(z) is just an (N âˆ’1) Ã— (N âˆ’1) version of
G(z). So in the region of convergence we have
1
(G(z))11
= z âˆ’1 + q âˆ’qzg(z) + O

Nâˆ’1
2

,
(4.34)
where we have introduced q = N/T = O(1), such that Nâˆ’1
2 and T âˆ’1
2 are of the same
order of magnitude. This last equation states that 1/G11(z) has negligible ï¬‚uctuations and
can safely be replaced by its expectation value, i.e.
1
(G(z))11
= E
(
1
(G(z))11
)
+ O

Nâˆ’1
2

=
1
E [(G(z))11] + O

Nâˆ’1
2

.
(4.35)
By rotational invariance of W, we have
E[G(z)11] = 1
N E[Tr(G(z))] â†’g(z).
(4.36)
3 It can be self-consistently checked from the solution below that limT â†’âˆÎ³ 2 = âˆ’qgâ€²
W(z).

50
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
In the large N limit we obtain the following self-consistent equation for g(z):
1
g(z) = z âˆ’1 + q âˆ’qzg(z).
(4.37)
4.2.2 Solution and Density of Eigenvalues
Solving (4.37) we obtain
g(z) = z + q âˆ’1 Â±
*
(z + q âˆ’1)2 âˆ’4qz
2qz
.
(4.38)
The argument of the square-root is quadratic in z and its roots (the edge of spectrum) are
given by
Î»Â± = (1 Â± âˆšq)2.
(4.39)
Finding the correct branch is quite subtle, this will be the subject of Section 4.2.3. We will
see that the form
g(z) = z âˆ’(1 âˆ’q) âˆ’âˆšz âˆ’Î»+
âˆšz âˆ’Î»âˆ’
2qz
(4.40)
has all the correct analytical properties. Note that for z = xâˆ’iÎ· with x  0 and Î· â†’0, g(z)
can only have an imaginary part if
*
(x âˆ’Î»+)(x âˆ’Î»âˆ’) is imaginary. Then using (2.47),
we get the famous MarË‡cenkoâ€“Pastur distribution for the bulk:
Ï(x) = 1
Ï€
lim
Î·â†’0+ Im g(x âˆ’iÎ·) =
*
(Î»+ âˆ’x)(x âˆ’Î»âˆ’)
2Ï€qx
,
Î»âˆ’< x < Î»+.
(4.41)
Moreover, by studying the behavior of Eq. (4.40) near z = 0 one sees that there is a pole
at 0 when q > 1. This gives a delta mass as z â†’0:
q âˆ’1
q
Î´(x),
(4.42)
which corresponds to the Nâˆ’T trivial zero eigenvalues of E in the N > T case. Combining
the above discussions, the full MarË‡cenkoâ€“Pastur law can be written as
ÏMP(x) =
/
(Î»+ âˆ’x)(x âˆ’Î»âˆ’)

+
2Ï€qx
+ q âˆ’1
q
Î´(x)(q âˆ’1),
(4.43)
where we denote [a]+ := max{a,0} for any a âˆˆR, and
(q âˆ’1) :=
0
0,
if q â‰¤1,
1
if q > 1.
(4.44)

4.2 MarË‡cenkoâ€“Pastur Using the Cavity Method
51
0
1
2
3
4
5
6
l
0.0
0.2
0.4
0.6
0.8
1.0
r(l)
q = 1/ 2
q = 2
Figure 4.2 MarË‡cenkoâ€“Pastur distribution: density of eigenvalues for a Wishart matrix for q = 1/2
and q = 2. Note that for q = 2 there is a Dirac mass at zero ( 1
2Î´(Î»)). Also note that the two bulk
densities are the same up to a rescaling and normalization Ï1/q(Î») = q2Ïq(qÎ»).
Note that the Stieltjes transforms (Eq. (4.40)) for q and 1/q are related by Eq. (4.5).
As a consequence the bulk densities for q and 1/q are the same when properly rescaled
(see Fig. 4.2):
Ï1/q(Î») = q2Ïq(qÎ»).
(4.45)
Exercise 4.2.1
Properties of the MarË‡cenkoâ€“Pastur solution
We saw that the Stieltjes transform of a large Wishart matrix (with q = N/T )
should be given by
g(z) = z + q âˆ’1 Â±
*
(z + q âˆ’1)2 âˆ’4qz
2qz
,
(4.46)
where the sign of the square-root should be chosen such that g(z) â†’1/z when
z â†’Â±âˆ.
(a)
Show that the zeros of the argument of the square-root are given by Î»Â± =
(1 Â± âˆšq)2.
(b)
The function
g(z) = z + q âˆ’1 âˆ’
*
(z âˆ’Î»âˆ’)
*
(z âˆ’Î»+)
2qz
(4.47)
should have the right properties. Show that it behaves as g(z) â†’1/z when
z â†’Â±âˆ. By expanding in powers of 1/z up to 1/z3 compute the ï¬rst and
second moments of the Wishart distribution.

52
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
(c)
Show that Eq. (4.47) is regular at z = 0 when q < 1. In that case, compute
the ï¬rst inverse moment of the Wishart matrix Ï„(Eâˆ’1). What happens when
q â†’1? Show that Eq. (4.47) has a pole at z = 0 when q > 1 and compute
the value of this pole.
(d)
The non-zero eigenvalues should be distributed according to the MarË‡cenkoâ€“
Pastur distribution:
Ïq(x) =
*
(x âˆ’Î»âˆ’)(Î»+ âˆ’x)
2Ï€qx
.
(4.48)
Show that this distribution is correctly normalized when q < 1 but not when
q > 1. Use what you know about the pole at z = 0 in that case to correctly
write down Ïq(x) when q > 1.
(e)
In the case q = 1, Eq. (4.48) has an integrable singularity at x = 0. Write
a simpler formula for Ï1(x). Let u be the square of an eigenvalue from a
Wigner matrix of unit variance, i.e. u = y2 where y is distributed according
to the semi-circular law Ï(y) =
*
4 âˆ’y2/(2Ï€). Show that u is distributed
according to Ï1(x). This result is a priori not obvious as a Wigner matrix is
symmetric while the square matrix H is generally not; nevertheless, moments
of high-dimensional matrices of the form HHT are the same whether the
matrix H is symmetric or not.
(f)
Generate three matrices E = HHT /T where the matrix H is an N Ã— T
matrix of iid Gaussian numbers of variance 1. Choose a large N and three
values of T such that q = N/T equals {1/2,1,2}. Plot a normalized histogram
of the eigenvalues in the three cases vs the corresponding MarË‡cenkoâ€“Pastur
distribution; donâ€™t show the peak at zero. In the case q = 2, how many zero
eigenvalues do you expect? How many do you get?
4.2.3 The Correct Root of the Stieltjes Transform
In our study of random matrices we will often encounter limiting Stieltjes transforms
that are determined by quadratic or higher order polynomial equations, and the problem
of choosing the correct solution (or branch) will come up repeatedly.
Let us go back to the unit Wigner matrix case where we found (see Section 2.3.2)
g(z) = z Â±
*
z2 âˆ’4
2
.
(4.49)
On the one hand we want g(z) that behaves like 1/z as |z| â†’âˆand we want the solution
to be analytical everywhere but on the real axis in [âˆ’2,2]. The square-root term must
thus behave as âˆ’z for real z when z â†’Â±âˆ. The standard deï¬nition of the square-root
behaves as
*
z2 âˆ¼|z| and cannot be made to have the correct sign on both sides. Another
issue with
*
z2 âˆ’4 is that it has a more extended branch cut than allowed. We expect the

4.2 MarË‡cenkoâ€“Pastur Using the Cavity Method
53
z âˆˆC
branch point
branch cut
present in
Stieltjes
branch cut
absent in
Stieltjes
âˆ’2
2
f (z) â†’z
f (z) â†’âˆ’z
Figure 4.3 The branch cuts of f (z) =
*
z2 âˆ’4. The vertical branch cut (for z pure imaginary)
should not be present in the Stieltjes transform of the Wigner ensemble. We have f (Â±0+ + ix) â‰ˆ
Â±i
*
x2 + 4; this branch cut can be eliminated by multiplying f (z) by sign(Re z).
function g(z) to be analytic everywhere except for real z âˆˆ[âˆ’2,2]. The branch cut of a
standard square-root is a set of points where its argument is real and negative. In the case
of
*
z2 âˆ’4, this includes the interval [âˆ’2,2] as expected but also the pure imaginary line
z = ix (Fig. 4.3). The ï¬nite N Stieltjes transform is perfectly regular on the imaginary
axis so we expect its large N to be regular there as well.
For the unit Wigner matrix, there are at least three solutions to the branch problem:
g1(z) =
z âˆ’z
/
1 âˆ’4
z2
2
,
(4.50)
g2(z) = z âˆ’
âˆš
z âˆ’2
âˆš
z + 2
2
,
(4.51)
g3(z) = z âˆ’sign(Re z)
*
z2 âˆ’4
2
.
(4.52)
All three deï¬nitions behave as g(z) âˆ¼1/z at inï¬nity. For the second one, we need to deï¬ne
the square-root of a negative real number. If we deï¬ne it as i âˆš|z|, the two factors of i give
a âˆ’1 for real z < âˆ’2. The three functions also have the correct branch cuts. For the ï¬rst
one, one can show that the argument of the square-root can be a negative real number only
if z âˆˆ(âˆ’2,2), there are no branch cuts elsewhere in the complex plane. For the second
one, there seems to be a branch cut for all real z < 2, but a closer inspection reveals
that around real z < âˆ’2 the function has no discontinuity as one goes up and down the
imaginary axis, as the two branch cuts exactly compensate each other. For the third one,
the discontinuous sign function exactly cancels the branch cut on the pure imaginary axis
(Fig. 4.3).

54
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
For z with a large positive real part the three functions are clearly the same. Since they
are analytic functions everywhere except on the same branch cuts, they are the same and
unique function g(z).
For a Wigner matrix shifted by Î»0 and of variance Ïƒ 2 we can scale and shift the
eigenvalues, now equal Î»Â± = Î»0 Â± 2Ïƒ and ï¬nd
g1(z) =
z âˆ’Î»0 âˆ’(z âˆ’Î»0)
1
1 âˆ’
4Ïƒ 2
(zâˆ’Î»0)2
2Ïƒ
,
(4.53)
g2(z) = z âˆ’Î»0 âˆ’âˆšz âˆ’Î»+
âˆšz âˆ’Î»âˆ’
2Ïƒ
,
(4.54)
g3(z) = z âˆ’Î»0 âˆ’sign(Re z âˆ’Î»0)
*
(z âˆ’Î»0)2 âˆ’4Ïƒ 2
2Ïƒ
.
(4.55)
The three deï¬nitions are still equivalent as they are just the result of a shift and a scaling
of the same function.
For more complicated problems, writing explicitly any one of these three prescriptions
can quickly become very cumbersome (except maybe in cases where Î»+ + Î»âˆ’= 0). We
propose here a new notation. When ï¬nding the correct square-root of a second degree
polynomial we will write
Â±
âƒ*
az2 + bz + c : = âˆša
*
z âˆ’Î»+
*
z âˆ’Î»âˆ’
= âˆša(z âˆ’Î»0)
2
1 âˆ’

(z âˆ’Î»0)2
= sign(Re z âˆ’Î»0)
*
az2 + bz + c,
(4.56)
for a > 0 and where Î»Â± = Î»0 Â±
âˆš
 are the roots of az2 + bz + c assumed to be real.
While the notation is deï¬ned everywhere in the complex plane, it is easily evaluated for
real arguments:
Â±
âƒ*
ax2 + bx + c =
0
âˆ’
*
ax2 + bx + c
for x â‰¤Î»âˆ’,
*
ax2 + bx + c
for x â‰¥Î»+.
(4.57)
The value on the branch cut is ill-deï¬ned but we have
lim
zâ†’xâˆ’i0+
Â±
âƒ*
az2 + bz + c = i
/
|ax2 + bx + c|
for Î»âˆ’< x < Î»+.
(4.58)
With our new notation, we can now safely write for the white Wishart:
g(z) = z + q âˆ’1 âˆ’
Â±
âƒ*
(z + q âˆ’1)2 âˆ’4qz
2qz
,
(4.59)
or, more explicitly, using the second prescription:
g(z) = z + q âˆ’1 âˆ’âˆšz âˆ’Î»+
âˆšz âˆ’Î»âˆ’
2qz
,
(4.60)
where Î»Â± = (1 Â± âˆšq)2.

4.2 MarË‡cenkoâ€“Pastur Using the Cavity Method
55
Exercise 4.2.2
Finding the correct root
(a)
For the unit Wigner Stieltjes transform show that regardless of choice of sign
in Eq. (4.49) the point z = 2i is located on a branch cut and the function is
discontinuous at that point.
(b)
Compute the value of Eqs. (4.50), (4.51) and (4.52) at z = 2i. Hint: for g2(z)
write âˆ’2 + 2i =
âˆš
8e3iÏ€/4 and similarly for 2 + 2i. The deï¬nition g3(z) is
ambiguous for z = 2i, compute the limiting value on both sides: z = 0+ + 2i
and z = 0âˆ’+ 2i.
4.2.4 General (Non-White) Wishart Matrices
Recall our deï¬nition of a Wishart matrix from Section 4.1.2: a Wishart matrix is a matrix
EC deï¬ned as
EC = 1
T HCHT
C,
(4.61)
where HC is an N Ã—T rectangular matrix with independent columns. Each column is a ran-
dom Gaussian vector with covariance matrix C; EC corresponds to the sample (empirical)
covariance matrix of variables characterized by a population (true) covariance matrix C.
To understand the case where the true matrix C is different from the identity we ï¬rst
discuss how to generate a multivariate Gaussian vector with covariance matrix C. We
diagonalize C as
C = OOT,
 =
â›
âœâ
Ïƒ 2
1
...
Ïƒ 2
N
â
âŸâ .
(4.62)
The square-root of C can be deï¬ned as4
C
1
2 = O
1
2 OT,

1
2 =
â›
âœâ
Ïƒ1
...
ÏƒN
â
âŸâ .
(4.63)
We now generate N iid unit Gaussian random variables xi, 1 â‰¤i â‰¤N, which form a
random column vector x with entries xi. Then we can generate the vector y = C
1
2 x. We
claim that y is a multivariate Gaussian vector with covariance matrix C. In fact, y is a linear
combination of multivariate Gaussians, so it must itself be multivariate Gaussian. On the
other hand, we have, using E[xxT ] = 1,
4 This is the canonical deï¬nition of the square-root of a matrix, but this deï¬nition is not unique â€“ see the technical paragraph
below.

56
Wishart Ensemble and MarË‡cenkoâ€“Pastur Distribution
E(yyT ) = E(C
1
2 xxT C
1
2 ) = C.
(4.64)
By repeating the argument above for every column of HC, t = 1, . . . ,T , we see that this
matrix can be written as HC = C
1
2 H, with H a rectangular matrix with iid unit Gaussian
entries. The matrix EC is then equivalent to
EC = 1
T HCHT
C = 1
T C
1
2 HHT C
1
2 = C
1
2 WqC
1
2,
(4.65)
where Wq = 1
T HHT is a white Wishart matrix with q = N/T .
We will see later that the above combination of matrices is called the free product of C
and W. Free probability will allow us to compute the resolvent and the spectrum in the case
of a general C matrix.
The variables x deï¬ned above are called the â€œwhitenedâ€ version of y. If a zero mean
random vector y has positive deï¬nite covariance matrix C, we can deï¬ne a whitening of
y as a linear combination x = My such that E[xxT ] = 1. One can show that the matrix
M satisï¬es MT M = Câˆ’1 and has to be of the form M = OCâˆ’1
2 , where O can be
any orthogonal matrix and C
1
2 the symmetric square-root of C deï¬ned above. Since O is
arbitrary the procedure is not unique, which leads to three interesting choices for whitened
varaibles:
â€¢ Perhaps the most natural one is the symmetric or Mahalanobis whitening where M =
Câˆ’1
2 . In addition to being the only whitening scheme with a symmetric matrix M, the
white variables x = Câˆ’1
2 y are the closest to y in the following sense: the distance
||x âˆ’y||CÎ± := E Tr

(x âˆ’y)T CÎ±(x âˆ’y)

(4.66)
is minimal over all other choices of O for any Î±. The case Î± = âˆ’1 is called the
Mahalanobis norm.
â€¢ Triangular or Gramâ€“Schmidt whitening where the vector x can be constructed using the
Gramâ€“Schmidt orthonormalization procedure. If one starts from the bottom with xN =
yN/ âˆšCNN, then the matrix M is upper triangular. The matrix M can be computed
efï¬ciently using the Cholesky decomposition of Câˆ’1. The Cholesky decomposition of
a symmetric positive deï¬nite matrix A amounts to ï¬nding a lower triangular matrix L
such that LLT = A. In the present case, A = Câˆ’1 and the matrix M we are looking for
is given by
M = LT .
(4.67)
This scheme has the advantage that the whitened variable xk only depends on physical
variables yâ„“for â„“â‰¥k. In ï¬nance, for example, this allows one to construct whitened
returns of a given stock using only the returns of itself and those of (say) more liquid
stocks.
â€¢ Eigenvalue (or pca) whitening where O corresponds to the eigenbasis of C, i.e. such
that C = OOT where  is diagonal. M is then computed as M = âˆ’1
2 OT . The
whitened variables x are then called the normalized principal components of y.

4.2 MarË‡cenkoâ€“Pastur Using the Cavity Method
57
Bibliographical Notes
â€¢ For a historical perspective on Wishart matrices and the MarË‡cenkoâ€“Pastur, see
â€“ J. Wishart. The generalised product moment distribution in samples from a normal
multivariate population. Biometrika, 20A(1-2):32â€“52, 1928,
â€“ V. A. Marchenko and L. A. Pastur. Distribution of eigenvalues for some sets of
random matrices. Matematicheskii Sbornik, 114(4):507â€“536, 1967.
â€¢ For more recent material on the content of this chapter, see e.g.
â€“ L. Pastur and M. Scherbina. Eigenvalue Distribution of Large Random Matrices.
American Mathematical Society, Providence, Rhode Island, 2010,
â€“ Z. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matri-
ces. Springer-Verlag, New York, 2010,
â€“ A. M. Tulino and S. VerdÂ´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,
â€“ R. Couillet and M. Debbah. Random Matrix Methods for Wireless Communications.
Cambridge University Press, Cambridge, 2011,
â€“ G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018,
and also
â€“ J. W. Silverstein and S.-I. Choi. Analysis of the limiting spectral distribution of large
dimensional random matrices. Journal of Multivariate Analysis, 54(2):295â€“309,
1995,
â€“ A. Sengupta and P. P. Mitra. Distributions of singular values for some random matri-
ces. Physical Review E, 60(3):3389, 1999.
â€¢ A historical remark on the Cholesky decomposition: AndrÂ´e-Louis Cholesky served in
the French military as an artillery ofï¬cer and was killed in battle a few months before
the end of World War I; his discovery was published posthumously by his fellow ofï¬cer
Commandant BenoË†Ä±t in the Bulletin GÂ´eodÂ´esique (Wikipedia).

5
Joint Distribution of Eigenvalues
In the previous chapters, we have studied the moments, the Stieltjes transform and the
eigenvalue density of two classical ensembles (Wigner and Wishart). These quantities in
fact relate to single eigenvalue properties of these ensembles. By this we mean that the
Stieltjes transform and the eigenvalue density are completely determined by the univariate
law of eigenvalues but they do not tell us anything about the correlations between different
eigenvalues.
In this chapter we will extend these results in two directions. First we will consider a
larger class of rotationally invariant (or orthogonal) ensembles that contains Wigner and
Wishart. Second we will study the joint law of all eigenvalues. In these models, the eigen-
values turn out to be strongly correlated and can be thought of as â€œparticlesâ€ interacting
through pairwise repulsion.
5.1 From Matrix Elements to Eigenvalues
5.1.1 Matrix Potential
Consider real symmetric random matrices M whose elements are distributed as the expo-
nential of the trace of a certain matrix function V (M), often called a potential by analogy
with statistical physics:1
P(M) = Zâˆ’1
N exp
$
âˆ’N
2 Tr V (M)
%
,
(5.1)
where ZN is a normalization constant. These matrix ensembles are called orthogonal
ensembles for they are rotationally invariant, i.e. invariant under orthogonal transforma-
tions.2 For the Wigner ensemble, for example, we have (see Chapter 2)
1 V (M) is a matrix function best deï¬ned in the eigenbasis of M through a transformation of all its eigenvalues through a
function of a scalar, V (x), see Section 1.2.6.
2 The results of this chapter extend to Hermitian (Î² = 2) or quarternion-Hermitian (Î² = 4) matrices with the simple
introduction of a factor Î² in the probability distribution:
P (M) âˆexp
$
âˆ’Î²N
2
Tr V (M)
%
,
(5.2)
58

5.1 From Matrix Elements to Eigenvalues
59
0
1
2
3
4
x
âˆ’2
0
2
4
6
V(x)
q = 1/ 2
q = 2
Figure 5.1 The Wishart matrix potential (Eq. (5.4)) for q = 1/2 and q = 2. The integration over
positive semi-deï¬nite matrices imposes that the eigenvalues must be greater than or equal to zero.
For q < 1 the potential naturally ensures that the eigenvalues are greater than zero and the constraint
will not be explicitly needed in the computation. For q â‰¥1, the constraint is needed to obtain a
sensible result.
V (x) = x2
2Ïƒ 2,
(5.3)
whereas the Wishart ensemble (at large N) is characterized by (see Chapter 4)
V (x) = x + (q âˆ’1) log x
q
(5.4)
(see Fig. 5.1). We can also consider other matrix potentials, e.g.
V (x) = x2
2 + gx4
4 .
(5.5)
Note that Tr V (M) depends only on the eigenvalues of M. We would like thus to write
down the joint distribution of these eigenvalues alone. The key is to ï¬nd the Jacobian of the
change of variables from the entries of M to the eigenvalues {Î»1, . . . ,Î»N}.
5.1.2 Matrix Jacobian
Before computing the Jacobian of the transformation from matrix elements to eigenvalues
and eigenvectors (or orthogonal matrices), let us count the number of variables in both
parameterizations. Suppose M can be diagonalized as
M = OOT .
(5.6)
this factor will match the factor of Î² from the Vandermonde determinant. These two other ensembles are called unitary
ensembles and symplectic ensembles, respectively. Collectively they are called the beta ensembles.

60
Joint Distribution of Eigenvalues
The symmetric matrix M has N(N + 1)/2 independent variables, and  has N inde-
pendent variables as a diagonal matrix. To ï¬nd the number of independent variables in
O we ï¬rst realize that OOT = 1 is an equation between two symmetric matrices and
thus imposes N(N + 1)/2 constraints out of N2 potential values for the elements of O,
therefore O has N(N âˆ’1)/2 independent variables. In total, we thus have N(N + 1)/2 =
N + N(N âˆ’1)/2.
The change of variables from M to (,O) will introduce a factor | det()|, where
 := (M) =
(âˆ‚M
âˆ‚, âˆ‚M
âˆ‚O
)
(5.7)
is the Jacobian matrix of dimension N(N + 1)/2 Ã— N(N + 1)/2.
First, let us establish the scaling properties of the Jacobian. We assume that the matrix
elements of M have some dimension [M] = d (say centimeters). Using dimensional
analysis, we thus have
[DM] = dN(N+1)/2,
[D] = dN,
[DO] = d0,
(5.8)
since rotations are dimensionless. Hence we must have
[| det()|] âˆ¼dN(Nâˆ’1)/2,
(5.9)
which has the dimension of an eigenvalue raised to the power N(N âˆ’1)/2, the number of
distinct off-diagonal elements in M.
We now compute this Jacobian exactly. First, notice that the Jacobian relates the volume
â€œaroundâ€ (,O) when  and O change by inï¬nitesimal amounts, to the volume â€œaroundâ€
M when its elements change by inï¬nitesimal amounts. We note that volumes are invariant
under rotations, so in order to compute the inï¬nitesimal volume we can choose the rotation
matrix O to be the identity matrix, which amounts to saying that we work in the basis where
M is diagonal. Another way to see this is to note that the orthogonal transformation
M â†’UT MU;
UT U = 1
(5.10)
has a Jacobian equal to 1, see Section 1.2.7. One can always choose U such that M is
diagonal.
5.1.3 Inï¬nitesimal Rotations
For rotations O near the identity, we set
O = 1 + Ïµ Î´O,
(5.11)
where Ïµ is a small number and Î´O is some matrix. From the identity
1 = OOT = 1 + Ïµ(Î´O + Î´OT ) + Ïµ2Î´OÎ´OT,
(5.12)

5.1 From Matrix Elements to Eigenvalues
61
we get Î´O = âˆ’Î´OT by comparing terms of the ï¬rst order in Ïµ, i.e. Î´O is skew-symmetric.3
A convenient basis to write such inï¬nitesimal rotations is
Ïµ Î´O =

1â‰¤k<lâ‰¤N
Î¸klA(kl),
(5.13)
where A(kl) are the elementary skew-symmetric matrices such that A(kl) has only two non-
zero elements: [A(kl)]kl = 1 and [A(kl)]lk = âˆ’1:
A(kl) =
â›
âœâœâœâœâœâœâœâœâœâœâœâ
0
. . .
. . .
0
...
...
1
...
âˆ’1
...
...
...
0
. . .
. . .
0
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(5.14)
An inï¬nitesimal rotation is therefore fully described by N(N âˆ’1)/2 generalized â€œanglesâ€
Î¸kl.
5.1.4 Vandermonde Determinant
Now, in the neighborhood of (,O = 1), the matrix M + Î´M can be parameterized as
M + Î´M â‰ˆ
â›
â1 +

k,l
Î¸klA(kl)
â
â ( + Î´)
â›
â1 âˆ’

k,l
Î¸klA(kl)
â
â .
(5.15)
So to ï¬rst order in Î´ and Î¸kl,
Î´M â‰ˆÎ´ +

k,l
Î¸kl
&
A(kl) âˆ’A(kl)'
.
(5.16)
Using this local parameterization, we can compute the Jacobian matrix and ï¬nd its deter-
minant. For the diagonal contribution, we have
âˆ‚Mij
âˆ‚nn
= Î´inÎ´jn,
(5.17)
i.e. perturbing a given eigenvalue only changes the corresponding diagonal element with
slope 1.
For the rotation contribution, one has, for k < l and i < j,
3 The reader familiar with the analysis of compact Lie groups will recognize the statement that skew-symmetric matrices form
the Lie algebra of O(N).

62
Joint Distribution of Eigenvalues
âˆ‚Mij
âˆ‚Î¸kl
=

A(kl) âˆ’A(kl)
ij =
0
Î»l âˆ’Î»k,
if i = k,j = l,
0,
otherwise.
(5.18)
i.e. an inï¬nitesimal rotation in the direction kl modiï¬es only one distinct off-diagonal
element (Mkl â‰¡Mlk) with slope Î»l âˆ’Î»k. In particular, if two eigenvalues are the same
(Î»k = Î»l) a rotation of the eigenvectors in that subspace has no effect on the matrix M. This
is expected since eigenvectors in a degenerate subspace are only deï¬ned up to a rotation
within that subspace.
Finally, the N(N + 1)/2 Ã— N(N + 1)/2 determinant has its ï¬rst N diagonal elements
equal to unity, and the next N(N âˆ’1)/2 are equal to all possible pair differences Î»i âˆ’Î»j.
Hence,
(M) = det
â›
âœâœâœâœâœâœâœâœâœâœâœâ
1
...
1
Î»2 âˆ’Î»1
Î»3 âˆ’Î»1
...
Î»N âˆ’Î»Nâˆ’1
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
=
,
k<â„“
(Î»â„“âˆ’Î»k).
(5.19)
The absolute value of  is then given by
|(M)| =
,
k<â„“
|Î»â„“âˆ’Î»k|.
(5.20)
We can check that this result has the expected dimension dN(Nâˆ’1)/2, since the product
contains exactly N(N âˆ’1)/2 terms. The determinant (M) is called the Vandermonde
determinant as it is equal to the determinant of the following N Ã— N Vandermonde matrix:
â›
âœâœâœâœâœâœâ
1
1
1
. . .
1
Î»1
Î»2
Î»3
. . .
Î»N
Î»2
1
Î»2
2
Î»2
3
. . .
Î»2
N
...
...
...
...
...
Î»Nâˆ’1
1
Î»Nâˆ’1
2
Î»Nâˆ’1
3
. . .
Î»Nâˆ’1
N
â
âŸâŸâŸâŸâŸâŸâ 
.
(5.21)
Since the above Jacobian has no dependence on the matrix O, we can integrate out the
rotation part of (5.1) to get the joint distribution of eigenvalues:
P({Î»i}) âˆ
,
k<l
|Î»k âˆ’Î»l| exp
0
âˆ’N
2
N

i=1
V (Î»i)
3
.
(5.22)

5.1 From Matrix Elements to Eigenvalues
63
A key feature of the above probability density is that the eigenvalues are not independent,
since the term 
k<l |Î»k âˆ’Î»l| indicates that the probability density vanishes when
two eigenvalues tend towards one another. This can be interpreted as some effective
â€œrepulsionâ€ between eigenvalues, as we will expand on now using an analogy with
Coulomb gases.
Exercise 5.1.1
Vandermonde determinant for 2 Ã— 2 matrices
In this exercise we will explicitly compute the Vandermonde determinant for
2 Ã— 2 matrices. We deï¬ne O and  as
O =
 cos(Î¸)
sin(Î¸)
âˆ’sin(Î¸)
cos(Î¸)

and  =
Î»1
0
0
Î»2

.
(5.23)
Then any 2 Ã— 2 symmetric matrix can be written as M = OOT .
(a)
Write explicitly M11, M12 and M22 as a function of Î»1, Î»2 and Î¸.
(b)
Compute the 3 Ã— 3 matrix  of partial derivatives of M11, M12 and M22 with
respect to Î»1, Î»2 and Î¸.
(c)
In the special cases where Î¸ equals 0, Ï€/4 and Ï€/2 show that | det | =
|Î»1 âˆ’Î»2|. If you have the courage show that | det | = |Î»1 âˆ’Î»2| for all Î¸.
Exercise 5.1.2
Wigner surmise
Wigner was interested in the distribution of energy level spacings in heavy
nuclei, which he modeled as the eigenvalues of a real symmetric random
matrix (time reversal symmetry imposes that the Hamiltonian be real). Let
x = |Î»k+1 âˆ’Î»k| for k in the bulk. In principle we can obtain the probability
density of x by using Eq. (5.22) and integrating out all other variables. In practice
it is very difï¬cult to go much beyond N = 2. Since the N = 2 result (properly
normalized) has the correct small x and large x behavior, Wigner surmised that
it must be a good approximation at any N.
(a)
For an N = 2 goe matrix, i.e. V (Î») = Î»2/2Ïƒ 2, write the unnormalized law
of Î»1 and Î»2, its two eigenvalues.
(b)
Change variables to Î»Â± = Î»2 Â± Î»1, integrate out Î»+ and write the unnormal-
ized law of x = |Î»âˆ’|.
(c)
Normalize your law and choose Ïƒ such that E[x] = 1; you should ï¬nd
P(x) = Ï€
2 x exp
&
âˆ’Ï€
4 x2'
.
(5.24)
(d)
Using Eq. (5.26) redo the computation for gue (Î² = 2). You should ï¬nd
P(x) = 32
Ï€2 x2 exp
(
âˆ’4
Ï€ x2
)
.
(5.25)

64
Joint Distribution of Eigenvalues
5.2 Coulomb Gas and Maximum Likelihood Conï¬gurations
5.2.1 A Coulomb Gas Analogy
The orthogonal ensemble deï¬ned in the previous section can be generalized to complex
or quaternion Hermitian matrices. The corresponding joint distribution of eigenvalues is
simply obtained by adding a factor of Î² (equal to 1, 2 or 4) to both the potential and the
Vandermonde determinant:
P({Î»i}) = Zâˆ’1
N
-,
k<l
|Î»k âˆ’Î»l|Î²
.
exp
0
âˆ’Î²
2
- N

i=1
NV (Î»i)
.3
= Zâˆ’1
N exp
â§
âªâªâ¨
âªâªâ©
âˆ’Î²
2
â¡
â¢â¢â£
N

i=1
NV (Î»i) âˆ’
N

i,j=1
ji
log |Î»i âˆ’Î»j|
â¤
â¥â¥â¦
â«
âªâªâ¬
âªâªâ­
.
(5.26)
This joint law is exactly the Gibbsâ€“Boltzmann factor (eâˆ’E/T ) for a gas of N particles
moving on a one-dimensional line, at temperature T = 2/Î², whose potential energy is
given by NV (x) and that interact with each other via a pairwise repulsive force generated
by the potential VR(x,y) = âˆ’log(|x âˆ’y|). Formally, the repulsive term happens to be the
Coulomb potential in two dimensions for particles that all have the same sign. In a truly
one-dimensional problem, the Coulomb potential would read V1d(x,y) = âˆ’|x âˆ’y|, but
with a slight abuse of language one speaks about the eigenvalues of a random matrix as a
Coulomb gas (in one dimension).
Even though we are interested in one particular value of Î² (namely Î² = 1), we can build
an intuition by considering this system at various temperatures. At very low temperature
(i.e. Î² â†’âˆ), the N particles all want to minimize their potential energy and sit at the
minimum of NV (x), but if they try to do so they will have to pay a high price in interaction
energy as this energy increases as the particles get close to one another. The particles will
have to spread themselves around the minimum of NV (x) to minimize the sum of the
potential and interaction energy and ï¬nd the conï¬guration corresponding to â€œmechanical
equilibriumâ€, i.e. such that the total force on each particle is zero. At non-zero temperature
(ï¬nite Î²) the particles will ï¬‚uctuate around this equilibrium solution. Since the repulsion
energy diverges as any two eigenvalues get inï¬nitely close, the particles will always avoid
each other. Figure 5.2 shows a typical conï¬guration of particles/eigenvalues for N = 20 at
Î² = 1 in a quadratic potential (goe matrix).
In the next section, we will study this equilibrium solution, which is exact at low tem-
perature Î² â†’âˆor when N â†’âˆ, and is the maximum likelihood solution at ï¬nite Î² and
ï¬nite N.
5.2.2 Maximum Likelihood Conï¬guration and Stieltjes Transform
In the previous section, we saw that in the Coulomb gas analogy, Î² â†’âˆcorresponds to
the zero temperature limit, and that in this limit the eigenvalues freeze to the minimum of

5.2 Coulomb Gas and Maximum Likelihood Conï¬gurations
65
âˆ’2
âˆ’1
0
1
2
x
0.0
0.5
1.0
1.5
2.0
V(x)
V(x)
N = 20
Figure 5.2 Representation of a typical N = 20 goe matrix as a Coulomb gas. The full curve
represents the potential V (x) = x2/2 and the 20 dots, the positions of the eigenvalues of a typical
conï¬guration. In this analogy, the eigenvalues feel a potential NV (x) and a repulsive pairwise
interaction V (x,y) = âˆ’log(|x âˆ’y|). They ï¬‚uctuate according to the Boltzmann weight eâˆ’Î²E/2
with Î² = 1 in the present case.
the energy (potential plus interaction). We will argue that this freezing (or concentration of
the equilibrium measure) also happens when N â†’âˆfor ï¬xed Î².
Let us study the minimum energy conï¬guration. We can rewrite Eq. (5.26) as
P({Î»i}) âˆe
1
2 Î²NL({Î»i}),
L({Î»i}) = âˆ’
N

i=1
V (Î»i) + 1
N
N

i,j=1
ji
log |Î»i âˆ’Î»j|,
(5.27)
where L stands for â€œlog-likelihoodâ€. For ï¬nite N and ï¬nite Î², we can still consider the
solution that maximizes L({Î»i}). This is the maximum likelihood solution, i.e. the conï¬g-
uration of {Î»i} that has maximum probability. The maximum of L is determined by the
equations
âˆ‚L
âˆ‚Î»i
= 0 â‡’V â€²(Î»i) = 2
N
N

j=1
ji
1
Î»i âˆ’Î»j
.
(5.28)
These are N coupled equations of N variables which can get very tedious to solve even
for moderate values of N. In Exercise 5.2.1 we will ï¬nd the solution for N = 3 in
the Wigner case. The solution of these equations is the set of equilibrium positions of
all the eigenvalues, i.e. the set of eigenvalues that maximizes the joint probability. To
characterize this solution (which will allow us to obtain the density of eigenvalues), we
will compute the Stieltjes transform of the {Î»i} satisfying Eq. (5.28). The trick is to make
algebraic manipulations to both sides of the equation to make the Stieltjes transform
explicitly appear.

66
Joint Distribution of Eigenvalues
In a ï¬rst step we multiply both sides of Eq. (5.28) by 1/(z âˆ’Î»i) where z is a complex
variable not equal to any eigenvalues, and then we sum over the index i. This gives
1
N
N

i=1
V â€²(Î»i)
z âˆ’Î»i
= 2
N2
N

i,j=1
ji
1

Î»i âˆ’Î»j

(z âˆ’Î»i)
= 1
N2
â¡
â¢â¢â£
N

i,j=1
ji
1

Î»i âˆ’Î»j

(z âˆ’Î»i) +
N

i,j=1
ji
1

Î»j âˆ’Î»i

(z âˆ’Î»j)
â¤
â¥â¥â¦
= 1
N2
N

i,j=1
ji
1
(z âˆ’Î»i) (z âˆ’Î»j) = g2
N(z) âˆ’1
N2
N

i=1
1
(z âˆ’Î»i)2
= g2
N(z) + gâ€²
N(z)
N
,
(5.29)
where gN(z) is the Stieltjes transform at ï¬nite N:
gN(z) := 1
N
N

i=1
1
z âˆ’Î»i
.
(5.30)
We still need to handle the left hand side of the above equation. First we add and subtract
V â€²(z) on the numerator, yielding
1
N
N

i=1
V â€²(Î»i)
z âˆ’Î»i
= V â€²(z)gN(z) âˆ’1
N
N

i=1
V â€²(z) âˆ’V â€²(Î»i)
z âˆ’Î»i
= V â€²(z)gN(z) âˆ’N(z), (5.31)
where we have deï¬ned a new function N(z) as
N(z) := 1
N
N

i=1
V â€²(z) âˆ’V â€²(Î»i)
z âˆ’Î»i
.
(5.32)
This does not look very useful as the equation for gN(z) will depend on some unknown
function N(z) that depends on the eigenvalues whose statistics we are trying to determine.
The key realization is that if V â€²(z) is a polynomial of degree k then N(z) is also a
polynomial and it has degree k âˆ’1. Indeed, for each i in the sum, V â€²(z) âˆ’V â€²(Î»i) is a
degree k polynomial having z = Î»i as a zero, so (V â€²(z) âˆ’V â€²(Î»i))/(z âˆ’Î»i) is a polynomial
of degree k âˆ’1. N(z) is the sum of such polynomials so is itself a polynomial of degree
k âˆ’1.
In fact, the argument is easy to generalize to the Laurent polynomials, i.e. such that
zkV â€²(z) is a polynomial for some k âˆˆN. For example, in the Wishart case we have a
Laurent polynomial
V â€²(z) = 1
q

1 + q âˆ’1
z

.
(5.33)

5.2 Coulomb Gas and Maximum Likelihood Conï¬gurations
67
Nevertheless, from now on we make the assumption that V â€²(z) is a polynomial. We will
later discuss how to relax this assumption.
Thus we get from Eq. (5.29) that
V â€²(z)gN(z) âˆ’N(z) = g2
N(z) + gâ€²
N(z)
N
(5.34)
for some polynomial N of degree deg(V â€²(z)) âˆ’1, which needs to be determined self-
consistently using Eq. (5.32). For a given V â€²(z), the coefï¬cients of N are related to the
moments of the {Î»i}, which themselves can be obtained from expanding gN(z) around
inï¬nity. In some cases, Eq. (5.34) can be solved exactly at ï¬nite N, for example in the case
where V (z) = z2/2, in which case the solution can be expressed in terms of Hermite poly-
nomials â€“ see Chapter 6. In the present chapter we will study this equation in the large N
limit, which will allow us to derive a general formula for the limiting density of eigenvalues.
Note that since Eq. (5.34) does not depend on the value of Î², the corresponding eigenvalue
density will also be independent of Î².
Exercise 5.2.1
Maximum likelihood for 3 Ã— 3 Wigner matrices
In this exercise we will write explicitly the three eigenvalues of the maximum
likelihood conï¬guration of a 3 Ã— 3 goe matrix. The potential for this ensemble
is V (x) = x2/2.
(a)
Let Î»1,Î»2,Î»3 be the three maximum likelihood eigenvalues of the 3 Ã— 3 goe
ensemble in decreasing order. By symmetry we expect Î»3 = âˆ’Î»1. What do
you expect for Î»2?
(b)
Consider Eq. (5.28). Assuming (Î»3 = âˆ’Î»1), check that your guess for Î»2 is
indeed a solution. Now write the equation for Î»1 and solve it.
(c)
Using your solution and the deï¬nition (5.30), show that the Stieltjes transform
of the maximum likelihood conï¬guration is given by
g3(z) = z2 âˆ’1
3
z3 âˆ’z .
(5.35)
(d)
In the simple case V (x) = x2/2, the zero-degree polynomial N(z) is just a
constant (independent of N) that can be evaluated from the deï¬nition (5.32).
What is this constant?
(e)
Verify that your g3(z) satisï¬es Eq. (5.34) with N = 3.
5.2.3 The Large N Limit
In the large N limit, gN(z) is self-averaging so computing gN(z) for the most likely con-
ï¬guration is the same as computing the average g(z). As N â†’âˆ, Eq. (5.34) becomes
V â€²(z)g(z) âˆ’(z) = g2(z).
(5.36)

68
Joint Distribution of Eigenvalues
Each value of N gives a different degree-(k âˆ’1) polynomial N(z). From the deï¬nition
(5.32), we can show that the coefï¬cients of N(z) are related to the moments of the
maximum likelihood conï¬guration of size N. In the large N limit these moments converge
so the sequence N(z) converges to a well-deï¬ned polynomial of degree (k âˆ’1) which we
call (z).
Since Eq. (5.36) is quadratic in g(z), its solution is given by
g(z) = 1
2

V â€²(z) Â±
*
V â€²(z)2 âˆ’4(z)

,
(5.37)
where we have to choose the branch where g(z) goes to zero for large |z|.
The eigenvalues of M will be located where g(z) has an imaginary part for z very close
to the real axis. The ï¬rst term V â€²(z) is a real polynomial and is always real for real z. The
expression V â€²(z)2 âˆ’4(z) is also a real polynomial so g(z) cannot be complex on the real
axis unless V â€²(z)2 âˆ’4(z) < 0. In this case
*
V â€²(z)2 âˆ’4(z) is purely imaginary. We
conclude that, when x is such that Ï(x)  0,
Re(g(x)) := âˆ’
 Ï(Î»)dÎ»
x âˆ’Î» = V â€²(x)
2
,
(5.38)
where âˆ’
denotes the principal part of the integral. Re(g(x))/Ï€ is also called the Hilbert
transform of Ï(Î»).4
We have thus shown that the Hilbert transform of the density of eigenvalues is (within its
support) equal to Ï€/2 times the derivative of the potential. We thus realize that the potential
outside the support of the eigenvalue has no effect on the distribution of eigenvalues. This
is natural in the Coulomb gas analogy. At equilibrium, the particles do not feel the potential
away from where they are. One consequence is that we can consider potentials that are not
conï¬ning at inï¬nity as long as there is a conï¬nement region and that all eigenvalues are
within that region. For example, we will consider the quartic potential V (x) = x2/2 +
Î³ x4/4. For small negative Î³ the region around x = 0 is convex. If all eigenvalues are
found to be contained in that region, we can modify at will the potential away from it so
that V (x) â†’+âˆfor |x| â†’âˆand keep Eq. (5.1) normalizable, as a probability density
should be.
Suppose now that we have a potential that is not a polynomial. In a ï¬nite region we can
approximate it arbitrarily well by a polynomial of sufï¬ciently high degree. If we choose the
region of approximation such that for every successive approximation all eigenvalues lie in
that region, we can take the limits of these approximations and ï¬nd that Eq. (5.38) holds
even if V â€²(x) is not a polynomial.
We can also ask the reverse question. Given a density Ï(x), does there exist a model from
the orthogonal ensemble (or other Î²-ensemble) that has Ï(Î») as its eigenvalue density? If
the Hilbert transform of Ï(x) is well deï¬ned, then the answer is yes and Eq. (5.38) gives the
4 This is a slight abuse of terms, however, that can lead to paradoxes, if one extends Eq. (5.38) to the region where Ï(x) = 0. In
this case, the right hand side of the equation is not equal to V â€²(x)/2. See also Appendix A.2.

5.3 Applications: Wigner, Wishart and the One-Cut Assumption
69
corresponding potential. Note that the potential is only deï¬ned up to an additive constant
(it can be absorbed in the normalization of Eq. (5.1)) so knowing its derivative is enough
to compute V (x). Note also that we only know the value of V (x) on the support of Ï(x);
outside this support we can arbitrarily choose V (x) provided it is convex and goes to inï¬nity
as |x| â†’âˆ.
Exercise 5.2.2
Matrix potential for the uniform density
In Exercise 2.3.3, we saw that the Stieltjes transform for a uniform density of
eigenvalues between 0 and 1 is given by
g(z) = log

z
z âˆ’1

.
(5.39)
(a)
By computing Re(g(x)) for x between 0 and 1, ï¬nd V â€²(x) using Eq. (5.38).
(b)
Compute the Hilbert transform of the uniform density to recover your answer
in (a).
(c)
From your answer in (a) and (b), show that the matrix potential is given by
V (x) = 2[(1 âˆ’x) log(1 âˆ’x) + x log(x)] + C
for 0 < x < 1,
(5.40)
where C is an arbitrary constant. Note that for x < 0 and x > 1 the potential
should be completed by a convex function that goes to inï¬nity as |x| â†’âˆ.
5.3 Applications: Wigner, Wishart and the One-Cut Assumption
5.3.1 Back to Wigner and Wishart
Now we apply the results of the previous section to the Gaussian orthogonal case where
V (z) = z2/2Ïƒ 2. In this simple case, (z) can be computed from its deï¬nition without
knowing the eigenvalues, since
V â€²(z) = z
Ïƒ 2 â‡’(z) = 1
Ïƒ 2 .
(5.41)
Then (5.37) gives
g(z) = z âˆ’
Â±
âƒâˆš
z2 âˆ’4Ïƒ 2
2Ïƒ 2
,
(5.42)
which recovers, independently of the value of Î², the Wigner result Eq. (2.38), albeit within
a completely different framework (the notation
Â±
âƒâˆšÂ· was introduced in Section 4.2.3). In
particular, the cavity method does not assume that the matrix ensemble is rotationally
invariant, as we do here.

70
Joint Distribution of Eigenvalues
In the Wishart case, we only consider the case q < 1, otherwise (q â‰¥1) the potential is
not conï¬ning and we need to impose the positive semi-deï¬niteness of the matrix to avoid
eigenvalues running to minus inï¬nity. We have
V â€²(z) = 1
q

1 + q âˆ’1
z

.
(5.43)
In this case zV â€²(z) is of degree one, so z(z) is a polynomial of degree zero:
(z) = c
z
(5.44)
for some constant c. Thus (5.37) then gives
g(z) = z + q âˆ’1 âˆ’
Â±
âƒ*
(z + q âˆ’1)2 âˆ’4cq2z
2qz
.
(5.45)
As z â†’+âˆ, this expression becomes
g(z) = cq
z + O(1/z2).
(5.46)
Imposing g(z) âˆ¼zâˆ’1 gives c = qâˆ’1. After some manipulations, we recover Eq. (4.40).
5.3.2 General Convex Potentials and the One-Cut Assumption
For more general polynomial potentials, ï¬nding an explicit solution for the limiting Stieltjes
transform is a little bit more involved. We recall Eq. (5.37):
g(z) = 1
2

V â€²(z) Â±
*
V â€²(z)2 âˆ’4(z)

.
(5.47)
For a particular polynomial V â€²(z), (z) is a polynomial that depends on the moments of the
matrix M. The expansion of g(z) near z â†’âˆwill give a set of self-consistent equations
for the coefï¬cients of (z).
The problem simpliï¬es greatly if the support of density of eigenvalues is compact, i.e. if
the density Ï(Î») is non-zero for all Î»â€™s between two edges Î»âˆ’and Î»+. We expect this to be
true if the potential V (x) is convex. Indeed, by the Coulomb gas analogy we could place
all eigenvalues near the minimum of V (x) and let them ï¬nd their equilibrium conï¬guration
by repelling each other. For a convex potential it is natural to assume that the equilibrium
conï¬guration would not have any gaps. This assumption is equivalent to assuming that
the limiting Stieltjes transform has a single branch cut (from Î»âˆ’and Î»+), hence the name
one-cut assumption.
So, for a convex polynomial potential V (x), we expect that there exists a well-deï¬ned
equilibrium density Ï(Î») that is non-zero if and only if Î»âˆ’< Î» < Î»+ and that g(z) satisï¬es
g(z) =
 Î»+
Î»âˆ’
Ï(Î»)
z âˆ’Î»dÎ».
(5.48)

5.3 Applications: Wigner, Wishart and the One-Cut Assumption
71
From this equation we notice three important properties of g(z):
â€¢ The function g(z) is potentially singular at Î»âˆ’and Î»+.
â€¢ Near the real axis (Im z = 0+) g(z) has an imaginary part if z âˆˆ(Î»âˆ’,Î»+) and is real
otherwise.
â€¢ The function g(z) is analytic everywhere else.
If we go back to Eq. (5.37), we notice that any non-analytic behavior must come from
the square-root. On the real axis, the only way g(z) can have an imaginary part is if D(z) :=
V â€²(z)2âˆ’4(z) < 0 for some values of z. So D(z) (a polynomial of degree 2k) must change
sign at some values Î»âˆ’and Î»+, hence these must be zeros of the polynomial. On the real
axis, the other possible zeros D(z) can only be of even multiplicity (otherwise D(z) would
change sign). Elsewhere in the complex plane, zeros should also be of even multiplicity,
otherwise âˆšD(z) would be singular at those zeros. In other words D(z) must be of the
form
D(z) = (z âˆ’Î»âˆ’)(z âˆ’Î»+)Q2(z),
(5.49)
for some polynomial Q(z) of degree k âˆ’1 where k is the degree of V â€²(z). We can therefore
write g(z) as
g(z) = V â€²(z) Â± Q(z)
*
(z âˆ’Î»âˆ’)(z âˆ’Î»+)
2
,
(5.50)
where again Q(z) is a polynomial with real coefï¬cients of degree one less than V â€²(z). The
condition that
g(z) â†’1
z when |z| â†’âˆ
(5.51)
is now sufï¬cient to compute Q(z) and also Î»Â± for a given potential V (z). Indeed, expanding
Eq. (5.50) near z â†’âˆ, the coefï¬cients of Q(z) and the values Î»Â± must be such as to
cancel the k + 1 polynomial coefï¬cients of V â€²(z) and also ensure that the 1/z term has unit
coefï¬cient. This gives k + 2 equations to determine the k coefï¬cients of Q(z) and the two
edges Î»Â±, see next section for an illustration.
Once the polynomial Q(x) is determined, we can read off the eigenvalue density:
Ï(Î») = Q(Î»)
*
(Î»+ âˆ’Î»)(Î» âˆ’Î»âˆ’)
2Ï€
for
Î»âˆ’â‰¤Î» â‰¤Î»+.
(5.52)
We see that generically the eigenvalue density behaves as Ï(Î»Â± âˆ“Î´) âˆ
âˆš
Î´ near both edges
of the spectrum. If by chance (or by construction) one of the edges is a zero of Q(z), then
the behavior changes to Î´Î¸ near that edge, with Î¸ = n + 1
2 and n the multiplicity of root of
Q(z). A potential with generic
âˆš
Î´ behavior at the edge of the density is called non-critical.
Other non-generic cases are called critical. In Section 14.1 we will see how the
âˆš
Î´ edge
singularity is smoothed over a region of width Nâˆ’2/3 for ï¬nite N. In the critical case, the
smoothed region is of width Nâˆ’2/(3+2n).

72
Joint Distribution of Eigenvalues
5.3.3 M2 + M4 Potential
One of the original motivations of BrÂ´ezin, Itzykson, Parisi and Zuber to study the
ensemble deï¬ned by Eq. (5.1) was to count the so-called planar diagrams in some ï¬eld
theories. To do so they considered the potential
V (x) = x2
2 + Î³ x4
4 .
(5.53)
We will not discuss how one can count planar diagrams from such a potential, but use
this example to illustrate the general recipe given in the main text to compute the Stieltjes
transform and the density of eigenvalues. Interestingly, for a certain value of Î³ , the edge
singularity is Î´3/2 instead of
âˆš
Î´.
Since the potential is symmetric around zero we expect Î»+ = âˆ’Î»âˆ’=: 2a. We
introduce this extra factor of 2, so that if Î³ = 0, we obtain the semi-circular law with
a = 1. Since V â€²(z) = z + Î³ z3 is a degree three polynomial, we write
Q(z) = a0 + a1z + Î³ z2,
(5.54)
where the coefï¬cient of z2 was chosen to cancel the Î³ z3 term at inï¬nity. Expanding
Eq. (5.50) near z â†’âˆand imposing g(z) = 1/z + O(1/z2) we get
a1 = 0,
1 âˆ’a0 + 2Î³ a2 = 0,
(5.55)
2a4Î³ + 2a2a0 = 2.
Solving for a0, we ï¬nd
g(z) = z + Î³ z3 âˆ’(1 + 2Î³ a2 + Î³ z2)
Â±
âƒ*
z2 âˆ’4a2
2
,
(5.56)
where a is a solution of
3Î³ a4 + a2 âˆ’1 = 0
â‡’
a2 =
âˆš1 + 12Î³ âˆ’1
6Î³
.
(5.57)
The density of eigenvalues for the potential (5.53) reads
Ï(Î») = (1 + 2Î³ a2 + Î³ Î»2)
*
4a2 âˆ’Î»2
2Ï€
for
Î³ > âˆ’1
12,
(5.58)
with a deï¬ned as above. For positive values of Î³ , the potential is conï¬ning (it is convex
and grows faster than a logarithm for z â†’Â±âˆ). In that case the equation for a always has
a solution, so the Stieltjes transform and the density of eigenvalues are well deï¬ned; see
Figure 5.3. For small negative values of Î³ , the problem still makes sense. The potential is
convex near zero and the eigenvalues will stay near zero as long as the repulsion does not
push them too far in the non-convex region.
There is a critical value of Î³ at Î³c = âˆ’1/12, which corresponds to a =
âˆš
2. At this
critical point, gc(z) and the density are given by
gc(z) = z3
24
â¡
â£

1 âˆ’8
z2
 3
2
âˆ’1 + 12
z2
â¤
â¦and Ïc(Î») =

8 âˆ’Î»23/2
24Ï€
.
(5.59)
At this point the density of eigenvalues at the upper edge (Î»+ = 2
âˆš
2) behaves as Ï(Î») âˆ¼
(2
âˆš
2 âˆ’Î»)Î¸
+ with Î¸ = 3/2 and similarly at the lower edge (Î»âˆ’= âˆ’2
âˆš
2). For values of

5.4 Fluctuations Around the Most Likely Conï¬guration
73
âˆ’2
0
2
l
0.0
0.1
0.2
0.3
0.4
r(l)
g = 1
g = 0
g = âˆ’1/ 12
âˆ’2.5
0.0
2.5
x
0
1
2
3
4
5
V(x)
g = 1
g = 0
g = âˆ’1/ 12
Figure 5.3 (left) Density of eigenvalues for the potential V (x) = 1
2x2 + Î³
4 x4 for three values of Î³ .
For Î³ = 1, even if the minimum of the potential is at Î» = 0, the density develops a double hump due
to the repulsion of the eigenvalues. Î³ = 0 corresponds to the Wigner case (semi-circle law). Finally
Î³ = âˆ’1/12 is the critical value of Î³ . At this point the density is given by Eq. (5.59). For smaller
values of Î³ the density does not exist. (right) Shape of the potential for the same three values of Î³ .
The dots on the bottom curve indicate the edges of the critical spectrum.
Î³ more negative than Î³c, there are no real solutions for a and Eq. (5.56) ceases to make
sense. In the Coulomb gas analogy, the eigenvalues push each other up to a critical point
after which they run off to inï¬nity. There is no simple argument that gives the location of
the critical point (except for doing the above computation). It is given by a delicate balance
between the repulsion of the eigenvalues and the conï¬ning potential. In particular it is not
given by the point V â€²(2a) = 0 as one might naively expect. Note that at the critical point
V â€²â€²(2a) = âˆ’1, so we are already outside the convex region.
5.4 Fluctuations Around the Most Likely Conï¬guration
5.4.1 Fluctuations of All Eigenvalues
The most likely positions of the N eigenvalues are determined by the equations (5.28) that
we derived in Section 5.2.2. These equations balance a Coulomb (repulsive) term and a
conï¬ning potential. On distances d of order 1/N, the Coulomb force on each â€œchargeâ€ is
of order dâˆ’1 âˆ¼N, whereas the conï¬ning force V â€² is of order unity. Therefore, the Coulomb
force is dominant at small scales and the most likely positions must be locally equidistant.
This is expected to be true within small enough intervals
I := [Î» âˆ’L/2N,Î» + L/2N],
(5.60)
where L is sufï¬ciently small, such that the average density Ï(Î») does not vary too much,
i.e. Ïâ€²(Î»)L/N â‰ªÏ(Î»).

74
Joint Distribution of Eigenvalues
Of course, some ï¬‚uctuations around this crystalline order must be expected, and the aim
of this section is to introduce a simple method to characterize these ï¬‚uctuations. Before
doing so, let us discuss how the number of eigenvalues n(L) in interval I behaves. For
a perfect crystalline arrangement, there are no ï¬‚uctuations at all and one has n(L) =
Ï(Î»)L + O(1), where the O(1) term comes from â€œrounding errorsâ€ at the edge of the
interval.
For a Poisson process, where points are placed independently at random with some
density NÏ(Î»), then it is well known that
n(L) = Â¯n + Î¾
âˆš
Â¯n,
with
Â¯n := Ï(Î»)L,
(5.61)
where Î¾ is a Gaussian random variable N(0,1). For the eigenvalues of a large symmetric
random matrix, on the other hand, the exact result is given by
n(L) = Â¯n +
âˆš
Î¾,
 := 2
Ï€2

log(Â¯n) + C

+ O(Â¯nâˆ’1),
(5.62)
where C is a numerical constant.5 This result means that the typical ï¬‚uctuations of the
number of eigenvalues is of order âˆšlog Â¯n for large L, much smaller than the Poisson result
âˆš
Â¯n. In fact, the growth of  with L is so slow that one can think of the arrangement of
eigenvalues as â€œquasi-crystallineâ€, even after factoring in ï¬‚uctuations.
Let us see how this âˆšlog Â¯n behavior of the ï¬‚uctuations can be captured by computing
the Hessian matrix H of the log-likelihood L deï¬ned in Eq. (5.27). One ï¬nds
Hij := âˆ’âˆ‚2L
âˆ‚Î»iâˆ‚Î»j
=
â§
â¨
â©
V â€²â€²(Î»i) + 1
N

ki
2
(Î»iâˆ’Î»k)2
(i = j),
âˆ’2
N
1
(Î»iâˆ’Î»j )2
(i  j).
This Hessian matrix should be evaluated at the maximum of L, i.e. for the most likely
conï¬gurations of the Î»i. If we call Ïµi/N the deviation of Î»i away from its most likely
value, and assume all Ïµ to be small enough, their joint distribution can be approximated
by the following multivariate Gaussian distribution:
P({Ïµi}) âˆexp
â¡
â£âˆ’Î²
4N

i,j
ÏµiHijÏµj
â¤
â¦,
(5.63)
from which one can obtain the covariance of the deviations as
E[ÏµiÏµj] = 2N
Î² [Hâˆ’1]ij.
(5.64)
Since the most likely positions of the Î»i are locally equidistant, one can approximate
Î»i âˆ’Î»j as (i âˆ’j)/NÏ in the above expression. This is justiï¬ed when the contribution of
far-away eigenvalues to Hâˆ’1 is negligible in the region of interest, which will indeed be
the case because the off-diagonal elements of H decay fast enough (i.e. as (i âˆ’j)âˆ’2).
5 For Î² = 1, one ï¬nds C = log(2Ï€) + Î³ + 1 + Ï€2
8 , where Î³ is Eulerâ€™s constant, see Mehta [2004].

5.4 Fluctuations Around the Most Likely Conï¬guration
75
For simplicity, let us consider the case where V (x) = x2/2, in which case V â€²â€²(x) = 1.
The matrix H is then a Toeplitz matrix, up to unimportant boundary terms. It can be
diagonalized using plane waves (see Appendix A.3), with eigenvalues given by6
Î¼q = 1 + 4NÏ2
Nâˆ’1

â„“=1
1 âˆ’cos 2Ï€qâ„“
N
â„“2
,
q = 0,1, . . . ,N âˆ’1,
(5.65)
where Ï is the local average eigenvalue density. In the large N limit, the (convergent) sum
can be replaced by an integral:
Î¼q = 1 + 4NÏ2 Ã— 2Ï€|q|
N
 âˆ
0
du1 âˆ’cos u
u2
= 1 + 4Ï€2Ï2|q|.
(5.66)
The eigenvalues of Hâˆ’1 are then given by 1/Î¼q and the covariance of the deviations for
i âˆ’j = n is obtained from Eq. (5.64) as an inverse Fourier transform. After transforming
q â†’uN/2Ï€ this reads
E[ÏµiÏµj] = 2N
Î²
1
N
 Ï€
âˆ’Ï€
du
2Ï€
eâˆ’iun
Nâˆ’1 + 2Ï€Ï2|u|.
(5.67)
Now, the ï¬‚uctuating distance dij between eigenvalues i and j = i + n is, by deï¬nition of
the Ïµ,
dij =
n
NÏ + Ïµi âˆ’Ïµj
N
.
(5.68)
Its variance is obtained, for N â†’âˆ, from
E[(Ïµi âˆ’Ïµj)2] â‰ˆ
2
Î²Ï€2(ÏN)2
 Ï€
0
du1 âˆ’cos un
u
â‰ˆnâ‰ª1
2
Î²Ï€2Ï2 log n.
(5.69)
The variables Ïµ are thus long-ranged, log-correlated Gaussian variables. Interestingly,
there has been a ï¬‚urry of activity concerning this problem in recent years (see references
at the end of this chapter).
Finally, the ï¬‚uctuating local density of eigenvalues can be computed from the number
of eigenvalues within a distance dij, i.e.
1
N
n
dij
â‰ˆÏ + Ï2 Ïµi âˆ’Ïµj
n
(n â‰«1).
(5.70)
Its variance is thus given by
V[Ï] =
2Ï2
Î²Ï€2n2 log n.
(5.71)
Hence, the ï¬‚uctuation of the number of eigenvalues in a ï¬xed interval of size L/N and
containing on average Â¯n = ÏL eigenvalues is
V[ÏL] = L2V[Ï] =
2
Î²Ï€2 log Â¯n.
(5.72)
This argument recovers the leading term of the exact result for all values of Î² (compare
with Eq. (5.62) for Î² = 1).
6 In fact, the Hessian H can be diagonalized exactly in this case, without any approximations, see Agarwal et al. [2019] and
references therein.

76
Joint Distribution of Eigenvalues
5.4.2 Large Deviations of the Top Eigenvalue
Another interesting question concerns the ï¬‚uctuations of the top eigenvalue of a matrix
drawn from a Î²-ensemble. Consider Eq. (5.26) with Î»max isolated:
P(Î»max;{Î»i}) = PNâˆ’1({Î»i}) exp
0
âˆ’NÎ²
2
-
V (Î»max) âˆ’2
N
Nâˆ’1

i=1
log(Î»max âˆ’Î»i)
.3
, (5.73)
with
PNâˆ’1({Î»i}) := Zâˆ’1
N exp
â§
âªâªâ¨
âªâªâ©
âˆ’Î²
2
â¡
â¢â¢â£
Nâˆ’1

i=1
NV (Î»i) âˆ’
Nâˆ’1

i,j=1
ji
log |Î»i âˆ’Î»j|
â¤
â¥â¥â¦
â«
âªâªâ¬
âªâªâ­
.
(5.74)
At large N, this probability is dominated by the most likely conï¬guration, which is deter-
mined by Eq. (5.27) with Î»max removed. Clearly, the most likely positions {Î»âˆ—
i } of the N âˆ’1
other eigenvalues are only changed by an amount O(Nâˆ’1), but since the log-likelihood is
close to its maximum, the change of L (i.e. the quantity in the exponential) is only of order
Nâˆ’2 and we will neglect it. Then one has the following large deviation expression:
P(Î»max;{Î»âˆ—
i })
P(Î»+;{Î»âˆ—
i }) := exp
(
âˆ’NÎ²
2 (Î»max)
)
,
(5.75)
with
(x) = V (x) âˆ’V (Î»+) âˆ’2
N
Nâˆ’1

i=1
log(x âˆ’Î»âˆ—
i ) + 2
N
Nâˆ’1

i=1
log(Î»+ âˆ’Î»âˆ—
i ),
(5.76)
where Î»+ is the top edge of the spectrum. Note that (Î»+) = 0 by construction. To deal
with the large N limit of this expression, we take the derivative of (x) with respect to x
to ï¬nd
â€²(x) = V â€²(x) âˆ’2
N
Nâˆ’1

i=1
1
x âˆ’Î»âˆ—
i
Nâ†’âˆ
â†’
V â€²(x) âˆ’2g(x).
(5.77)
Hence,
(x) =
 x
Î»+

V â€²(s) âˆ’2g(s)

ds.
(5.78)
When the potential V â€²(x) is a polynomial of degree k â‰¥1, we can use Eq. (5.37),
yielding
(x) =
 x
Î»+
*
V â€²(s)2 âˆ’4(s) ds,
(5.79)
where Î»+ is the largest zero of the polynomial V â€²(s)2 âˆ’4(s). Since (s) is a polynomial
of order k âˆ’1, for large s one always has
V â€²(s)2 â‰«|(s)|,
(5.80)

5.4 Fluctuations Around the Most Likely Conï¬guration
77
and therefore
(x) â‰ˆV (x).
(5.81)
As expected, the probability to observe a top eigenvalue very far from the bulk is dominated
by the potential term, and the Coulomb interaction no longer plays any role. When x âˆ’Î»+
is small but still much larger than any inverse power of N, we have
*
V â€²(s)2 âˆ’4(s) â‰ˆC(s âˆ’Î»+)Î¸,
(5.82)
where C is a constant and Î¸ depends on the type of root of V â€²(s)2 âˆ’4(s) at s = Î»+. For
a single root, which is the typical case, Î¸ = 1/2. Performing the integral we get
(Î»max) =
C
Î¸ + 1(Î»max âˆ’Î»+)Î¸+1,
Î»max âˆ’Î»+ â‰ª1.
(5.83)
Note that the constant C can be determined from the eigenvalue density near (but slightly
below) Î»+, to wit, Ï€Ï(Î») â‰ˆC(Î»+ âˆ’Î»)Î¸.
For a generic edge with Î¸ = 1/2, one ï¬nds that (Î»max) âˆ(Î»max âˆ’Î»+)3/2, and thus
the probability to ï¬nd Î»max just above Î»+ decays as
P(Î»max) âˆ¼exp
(
âˆ’2Î²C
3
u3/2
)
;
u = N2/3(Î»max âˆ’Î»+),
(5.84)
where we have introduced the rescaled variable u in order to show that this probability
decays on scale Nâˆ’2/3. We will further discuss this result in Section 14.1, where we will
see that the u3/2 behavior coincides with the right tail of the Tracyâ€“Widom distribution.
For a unit Wigner, we have (see Fig. 5.4)
2
3
4
5
6
7
8
x
0
5
10
15
20
25
F(x)
Wigner
Wishart
Figure 5.4 Large deviation function for the top eigenvalue (x) for a unit Wigner and a Wishart
matrix with Î»+ = 2 (q = 3 âˆ’2
âˆš
2). The Wishart curve was obtained by numerically integrating
Eq. (5.78).

78
Joint Distribution of Eigenvalues
(x) = 1
2x
*
x2 âˆ’4 âˆ’2 log
 âˆš
x2 âˆ’4 + x
2

for
x > 2.
(5.85)
For a Wishart matrix with parameter q = 1,
(x) =
*
(x âˆ’4)x + 2 log
x âˆ’âˆš(x âˆ’4)x âˆ’2
2

for
x > 4.
(5.86)
Remember that the Stieltjes transform g(z) and the density of eigenvalues Ï(Î») are only
sensitive to the potential function V (x) for values in the support of Ï (Î»âˆ’â‰¤x â‰¤Î»+).
The large deviation function (x), on the other hand, depends on the value of the potential
for x > Î»+. For Eq. (5.79) to hold, the same potential function must extend analytically
outside the support of Ï. In Section 5.3.3, we considered a non-conï¬ning potential (when
Î³ < 0). We argued that we could compute g(z) and Ï(Î») as if the potential were conï¬ning
as long as we could replace the potential outside (Î»âˆ’,Î»+) by a convex function going
to inï¬nity. In that case, the function (x) depends on the choice of regularization of the
potential. Computing Eq. (5.78) with the non-conï¬ning potential would give nonsensical
results.
Exercise 5.4.1
Large deviations for Wigner and Wishart
(a)
Show that Eqs. (5.85) and (5.86) are indeed the large deviation function for
the top eigenvalues of a unit Wigner and a Wishart q = 1. To do so, show
that they satisfy Eq. (5.77) and that (Î»+) = 0 with Î»+ = 2 for Wigner and
Î»+ = 4 for Wishart q = 1.
(b)
Find the dominant contribution of both (x) for large x. Compare to their
respective V (x).
(c)
Expand the two expressions for (x) near their respective Î»+ and show that
they have the correct leading behavior. What are the exponent Î¸ and constant
C?
5.5 An Eigenvalue Density Saddle Point
As an alternative approach to the most likely conï¬guration of eigenvalues, one often
ï¬nds in the random matrix literature the following density formalism. One ï¬rst introduces
the density of â€œchargesâ€ Ï‰(x) for a given set of eigenvalues Î»1,Î»2, . . . ,Î»N (not necessarily
the most likely conï¬guration), as
Ï‰(x) = 1
N
N

i=1
Î´(Î»i âˆ’x).
(5.87)
Expressed in terms of this density ï¬eld, the joint distribution of eigenvalues, Eq. (5.26)
can be expressed as

5.5 An Eigenvalue Density Saddle Point
79
P({Ï‰}) = Zâˆ’1 exp

âˆ’Î²N2
2
(
dxÏ‰(x)V (x) âˆ’âˆ’

dxdyÏ‰(x)Ï‰(y) log |x âˆ’y|
)
âˆ’N

dxÏ‰(x) log Ï‰(x)

,
(5.88)
where the last term is an entropy term, formally corresponding to the change of variables
from the {Î»i} to Ï‰(x). Since this term is of order N, compared to the two ï¬rst terms that
are of order N2, it is usually neglected.7
One then proceeds by looking for the density ï¬eld that maximizes the term in the
exponential, which is obtained by taking its functional derivative with respect to all Ï‰(x):
Î´
Î´Ï‰(x)
(
dyÏ‰(y)V (y) âˆ’âˆ’

dydyâ€²Ï‰(y)Ï‰(yâ€²) log |y âˆ’yâ€²| âˆ’Î¶

dyÏ‰(y)
)
Ï‰âˆ—= 0,
(5.89)
where Î¶
is a Lagrange multiplier, used to impose the normalization condition

dxÏ‰(x) = 1. This leads to
V (x) = 2âˆ’

dyÏ‰âˆ—(y) log |x âˆ’y| + Î¶.
(5.90)
We can now take the derivative with respect to x to get
V â€²(x) = 2âˆ’

dy Ï‰âˆ—(y)
x âˆ’y ,
(5.91)
which is nothing but the continuum limit version of Eq. (5.28), and is identical to
Eq. (5.38). Although this whole procedure looks somewhat ad hoc, it can be fully
justiï¬ed mathematically. In the mathematical literature, it is known as the large deviation
formalism.
Equation (5.91) is a singular integral equation for Ï‰(x) of the so-called Tricomi type.
Such equations often have explicit solutions, see Appendix A.2. In the case where V (x) =
x2/2, one recovers, as expected, the semi-circle law:
Ï‰âˆ—(x) = 1
2Ï€
*
4 âˆ’x2.
(5.92)
One interesting application of the density formalism is to investigate the case of Gaus-
sian orthogonal random matrices conditioned to have all eigenvalues strictly positive.
What is the probability for this to occur spontaneously? In such a case, what is the resulting
distribution of eigenvalues?
The trick is to solve Eq. (5.91) with the constraint that Ï‰(x < 0) = 0. This leads to
x = 2âˆ’
 âˆ
0
dy Ï‰âˆ—(y)
x âˆ’y .
(5.93)
The solution to this truncated problem can also be found analytically using the general
result of Appendix A.2. One ï¬nds
Ï‰âˆ—(x) = 1
4Ï€
1
Î»+ âˆ’x
x
(Î»+ + 2x),
with
Î»+ =
4
âˆš
3
.
(5.94)
The resulting density has a square-root divergence close to zero, which is a stigma of all
the negative eigenvalues being pushed to the positive side. The right edge itself is pushed
from 2 to 4/
âˆš
3, see Figure 5.5.
7 Note however that when Î² = c/N, as considered in Allez et al. [2012], the entropy term must be retained.

80
Joint Distribution of Eigenvalues
0.0
0.5
1.0
1.5
2.0
2.5
l
0.00
0.25
0.50
0.75
1.00
1.25
1.50
r(l)
Conditioned Wigner
Wigner
Figure 5.5 Density of eigenvalues for a Wigner matrix conditioned to be positive semi-deï¬nite. The
positive part of the density of a standard Wigner is shown for comparison.
Injecting this solution back into Eq. (5.88) and comparing with the result corresponding
to the standard semi-circle allows one to compute the probability for such an excep-
tional conï¬guration to occur. After a few manipulations, one ï¬nds that this probability
is given by eâˆ’Î²CN2 with C = log 3/4 â‰ˆ0.2746.... The probability that a Gaussian
random matrix has by chance all its eigenvalues positive therefore decreases extremely fast
with N.
Other constraints are possible as well, for example if one chooses to conï¬ne all eigen-
values in a certain interval [â„“âˆ’,â„“+] with â„“âˆ’â‰¥Î»âˆ’or â„“+ â‰¤Î»+. (In the cases where
â„“âˆ’< Î»âˆ’and â„“+ > Î»+ the conï¬nement plays no role.) Let us study this problem in
the case where there is no external potential at all, i.e. when V (x) â‰¡C, where C is an
arbitrary constant, but only conï¬ning walls. In this case, the general solution of Appendix
A.2 immediately leads to
Ï‰âˆ—(x) =
1
Ï€
*
(x âˆ’â„“âˆ’)(â„“+ âˆ’x)
,
(5.95)
which has a square-root divergence at both edges. This law is called the arcsine law, which
appears in different contexts, see Sections 7.2 and 15.3.1.
Note that the minimization of the quadratic form âˆ’
dxdyÏ‰(x)Ï‰(y)G(|x âˆ’y|) for a
general power-law interaction G(u) = uâˆ’Î³ , subject to the constraint

dxÏ‰(x) = 1, has
been solved in the very different, ï¬nancial engineering context of optimal execution with
quadratic costs. The solution in that case reads
Ï‰âˆ—(x) = A(Î³ )â„“âˆ’Î³
/
(x âˆ’â„“âˆ’)Î³ âˆ’1(â„“+ âˆ’x)Î³ âˆ’1,
(5.96)
with â„“:= â„“âˆ’+ â„“+ and A(Î³ ) := Î³ 
[Î³ ]/
2[(1 + Î³ )/2]. The case G(u) = âˆ’log(u)
formally corresponds to Î³ = 0, in which case one recovers Eq. (5.95).

5.5 An Eigenvalue Density Saddle Point
81
Bibliographical Notes
â€¢ For a general introduction to the subject, see
â€“ M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
â€“ P. J. Forrester. Log Gases and Random Matrices. Princeton University Press,
Princeton, NJ, 2010,
and for more technical aspects,
â€“ B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006.
â€¢ One-cut solution and the M2 + M4 model:
â€“ E. BrÂ´ezin, C. Itzykson, G. Parisi, and J.-B. Zuber. Planar diagrams. Communications
in Mathematical Physics, 59(1):35â€“51, 1978.
â€¢ On the relation between log gases and the Calogero model, see
â€“ S. Agarwal, M. Kulkarni, and A. Dhar. Some connections between the classical
Calogero-Moser model and the log gas. Journal of Statistical Physics, 176(6), 1463â€“
1479, 2019.
â€¢ About spectral rigidity and log-ï¬‚uctuations of eigenvalues, see
â€“ M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
â€“ M. V. Berry. Semiclassical theory of spectral rigidity. Proceedings of the Royal Soci-
ety of London, Series A, 400(1819):229â€“251, 1985,
for classical results, and
â€“ Y. V. Fyodorov and J. P. Keating. Freezing transitions and extreme values: Random
matrix theory, and disordered landscapes. Philosophical Transactions of the Royal
Society A: Mathematical, Physical and Engineering Sciences, 372(2007):2012053,
2014,
â€“ Y. V. Fyodorov, B. A. Khoruzhenko, and N. J. Simm. Fractional Brownian motion
with Hurst index H = 0 and the Gaussian Unitary Ensemble. The Annals of Proba-
bility, 44(4):2980â€“3031, 2016,
â€“ R. Chhaibi and J. Najnudel. On the circle, gmcÎ³ = cÎ²eâˆfor Î³ = âˆš2/Î², (Î³ â‰¤1).
preprint arXiv:1904.00578, 2019,
for the recent spree of activity in the ï¬eld of log-correlated variables.
â€¢ On the probability of a very large eigenvalue, see Chapter 14 and
â€“ S. N. Majumdar and M. Vergassola. Large deviations of the maximum eigenvalue for
Wishart and Gaussian random matrices. Physical Review Letters, 102:060601, 2009.
â€¢ On the continuous density formalism, see
â€“ D. S. Dean and S. N. Majumdar. Extreme value statistics of eigenvalues of Gaussian
random matrices. Physical Review E, 77:041108, 2008
for a physicistsâ€™ introduction, and

82
Joint Distribution of Eigenvalues
â€“ G. Ben Arous and A. Guionnet. Large deviations for Wignerâ€™s law and Voiculescuâ€™s
non-commutative entropy. Probability Theory and Related Fields, 108(4):517â€“542,
1997,
â€“ G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010
for more mathematical discussions in the context of large deviation theory. A case where
the entropy term must be retained is discussed in
â€“ R. Allez, J. P. Bouchaud, and A. Guionnet. Invariant beta ensembles and the Gauss-
Wigner crossover. Physical Review Letters, 109(9):094102, 2012.
â€¢ On the relation with optimal execution problems, see
â€“ J. Gatheral, A. Schied, and A. Slynko. Transient linear price impact and Fredholm
integral equations. Mathematical Finance, 22(3):445â€“474, 2012,
â€“ J.-P. Bouchaud, J. Bonart, J. Donier, and M. Gould. Trades, Quotes and Prices.
Cambridge University Press, Cambridge, 2nd edition, 2018.

6
Eigenvalues and Orthogonal Polynomials*
In this chapter, we investigate yet another route to shed light on the eigenvalue density
of the Wigner and Wishart ensembles. We show (a) that the most probable positions of
the Coulomb gas problem coincide with the zeros of Hermite polynomials in the Wigner
case, and of Laguerre polynomials in the Wishart case; and (b) that the average (over
randomness) of the characteristic polynomials (deï¬ned as det(z 1 âˆ’XN)) of Wigner or
Wishart random matrices of size N obey simple recursion relations that allow one to express
them as, respectively, Hermite and Laguerre polynomials. The fact that the two methods
lead to the same result (at least for large N) reï¬‚ects the fact that eigenvalues ï¬‚uctuate
very little around their most probable positions. Finally we show that for unitary ensembles
Î² = 2, the expected characteristic polynomial is always an orthogonal polynomial with
respect to some weight function related to the matrix potential.
6.1 Wigner Matrices and Hermite Polynomials
6.1.1 Most Likely Eigenvalues and Zeros of Hermite Polynomials
In the previous chapter, we established a general equation for the Stieltjes transform of
the most likely positions of the eigenvalues of random matrices belonging to a general
orthogonal ensemble, see Eq. (5.34). In the special case of a quadratic potential V (x) =
x2/2, this equation reads
zgN(z) âˆ’1 = g2
N(z) + gâ€²
N(z)
N
.
(6.1)
This ordinary differential equation is of the Ricatti type,1 and can be solved by setting
gN(z) := Ïˆâ€²(z)/NÏˆ(z). This yields, upon substitution,
Ïˆâ€²â€²(z) âˆ’NzÏˆâ€²(z) + N2Ïˆ(z) = 0,
(6.2)
or, with Ïˆ(z) = (x =
âˆš
Nz),
â€²â€²(x) âˆ’xâ€²(x) + N(x) = 0.
(6.3)
1 A Ricatti equation is a ï¬rst order differential equation that is quadratic in the unknown function, in our case gN (z).
83

84
Eigenvalues and Orthogonal Polynomials
The solution of this last equation with the correct behavior for large x is the Hermite
polynomial of order N. General Hermite polynomials Hn(x) are deï¬ned as the nth order
polynomial that starts as xn and is orthogonal to all previous ones under the unit Gaussian
measure:2

dx
âˆš
2Ï€
Hn(x)Hm(x)eâˆ’x2
2 = 0 when n  m.
(6.4)
The ï¬rst few are given by
H0(x) = 1,
H1(x) = x,
H2(x) = x2 âˆ’1,
(6.5)
H3(x) = x3 âˆ’3x,
H4(x) = x4 âˆ’6x2 + 3.
In addition to the above ODE (6.3), they satisfy
d
dx Hn(x) = nHnâˆ’1(x),
(6.6)
and the recursion
Hn(x) = xHnâˆ’1(x) âˆ’(n âˆ’1)Hnâˆ’2(x),
(6.7)
which combined together recovers Eq. (6.3). Hermite polynomials can be written explic-
itly as
Hn(x) = exp
-
âˆ’1
2
 d
dx
2.
xn =
âŒŠn/2âŒ‹

m=0
(âˆ’1)m
2m
n!
m! (n âˆ’2m)!xnâˆ’2m,
(6.8)
where âŒŠn/2âŒ‹is the integer part of n/2.
Coming back to Eq. (6.1), we thus conclude that the exact solution for the Stieltjes
transform gN(z) at ï¬nite N is
gN(z) =
H â€²
N(
âˆš
Nz)
âˆš
NHN(
âˆš
Nz)
(6.9)
or, writing HN(x) = N
i=1(xâˆ’
âˆš
Nh(N)
i
), where
âˆš
Nh(N)
i
are the N (real) zeros of HN(x),
gN(z) = 1
N
N

i=1
1
z âˆ’h(N)
i
.
(6.10)
2 Hermite polynomials can be deï¬ned using two different conventions for the unit Gaussian measure. We use here the
â€œprobabilistsâ€™ Hermite polynomialsâ€, while the â€œphysicistsâ€™ conventionâ€ uses a Gaussian weight proportional to eâˆ’x2.

6.1 Wigner Matrices and Hermite Polynomials
85
âˆ’10
0
10
l
0.00
0.01
0.02
0.03
0.04
r(l)
Figure 6.1 Histogram of the 64 zeros of H64(x). The full line is the asymptotic prediction from the
semi-circle law.
Comparing with the deï¬nition of gN(z), Eq. (5.30), one concludes that the most likely
positions of the Coulomb particles are exactly given by the zeros of Hermite polynomi-
als (scaled by
âˆš
N). This is a rather remarkable result, which holds for more general
conï¬ning potentials, to which are associated different kinds of orthogonal polynomials.
Explicit examples will be given later in this section for the Wishart ensemble, where we
will encounter Laguerre polynomials (see also Chapter 7 for Jacobi polynomials).
Since we know that gN(z) converges, for large N, towards the Stieltjes transform of
the semi-circle law, we can conclude that the rescaled zeros of Hermite polynomials are
themselves distributed, for large N, according to the same semi-circle law. This classical
property of Hermite polynomials is illustrated in Figure 6.1.
6.1.2 Expected Characteristic Polynomial of Wigner Matrices
In this section, we will show that the expected characteristic polynomial of a Wigner matrix
XN, deï¬ned as QN(z) := E[det(z 1 âˆ’XN)], is given by the same Hermite polynomial as
above. The idea is to write a recursion relation for QN(z) by expanding the determinant in
minors. Since we will be comparing Wigner matrices of different size, it is more convenient
to work with unscaled matrices YN =
âˆš
NXN, i.e. symmetric matrices of size N with
elements of zero mean and variance 1 (it will turn out that the variance of diagonal elements
is actually irrelevant, and so can also be chosen to be 1). We deï¬ne
qN(z) := E[det(z 1 âˆ’YN)].
(6.11)
Using det(Î±A) = Î±N det(A), we then have
QN(z) = Nâˆ’N/2qN(
âˆš
Nz).
(6.12)

86
Eigenvalues and Orthogonal Polynomials
We can compute the ï¬rst two qN(z) by hand:
q1(z) = E[z âˆ’Y11] = z;
q2(z) = E
&
(z âˆ’Y11)(z âˆ’Y22) âˆ’Y2
12
'
= z2 âˆ’1. (6.13)
To compute the polynomials for N â‰¥3 we ï¬rst expand the determinant in minors from the
ï¬rst line. We call Mi,j the ij-minor, i.e. the determinant of the submatrix of z 1 âˆ’YN with
the line i and column j removed:
det(z 1 âˆ’YN) =
N

i=1
(âˆ’1)i+1(zÎ´i1 âˆ’Y1i)M1,i
= zM1,1 âˆ’Y11M1,1 +
N

i=2
(âˆ’1)iY1iM1,i.
(6.14)
We would like to take the expectation value of this last expression. The ï¬rst two terms
are easy: the minor M1,1 is the same determinant with a Wigner matrix of size N âˆ’1, so
E[M1,1] â‰¡qNâˆ’1(z); the diagonal element Y11 is independent from the rest of the matrix
and its expectation is zero.
For the other terms in the sum, the minor Mi,1 is not independent of Yi1. Indeed, because
XN is symmetric, the corresponding submatrix contains another copy of Y1i. Let us then
expand M1,i itself on the ith row, to make the other term Yi1 appear explicitly. For i  1,
we have
Mi,1 =
N

j=1,ji
(âˆ’1)iâˆ’jYijM1i,ij
= (âˆ’1)iâˆ’1Yi1M1i,i1 +
N

j=2,ji
(âˆ’1)iâˆ’jYijM1i,ij,
(6.15)
where Mij,kl is the â€œsub-minorâ€, with rows i,j and columns k,l removed.
We can now take the expectation value of Eq. (6.14) by noting that Y1i is independent
of all the terms in Eq. (6.15) except the ï¬rst one. We also realize that M1i,i1 is the same
determinant with a Wigner matrix of size N âˆ’2 that is now independent of Y1i, so we have
E[M1i,i1] = qNâˆ’2(z). Putting everything together we get
qN(z) := E[det(z 1 âˆ’YN)] = zqNâˆ’1(z) âˆ’(N âˆ’1)qNâˆ’2(z).
(6.16)
We recognize here precisely the recursion relation (6.7) that deï¬nes Hermite polynomials.
How should this result be interpreted? Suppose for one moment that the positions of
the eigenvalues Î»i of YN were not ï¬‚uctuating from sample to sample, and ï¬xed to their
most likely values Î»âˆ—
i . In this case, the expectation operator would not be needed and one
would have
gN(z) = d
dz log QN(z) =
H â€²
N(
âˆš
Nz)
âˆš
NHN(
âˆš
Nz)
,
(6.17)

6.2 Laguerre Polynomials
87
recovering the result of the previous section. What is somewhat surprising is that
E
- N
,
i=1
(z âˆ’Î»i)
.
â‰¡
n
,
i=1
(z âˆ’Î»âˆ—
i )
(6.18)
even when ï¬‚uctuations are accounted for. In particular, in the limit N â†’âˆ, the average
Stieltjes transform should be computed from the average of the logarithm of the character-
istic polynomial:
g(z) = lim
Nâ†’âˆ
1
N E
( d
dz log det(z 1 âˆ’XN)
)
;
(6.19)
but the above calculation shows that one can compute the logarithm of the average charac-
teristic polynomial instead. The deep underlying mechanism is the eigenvalue spectrum of
random matrices is rigid â€“ ï¬‚uctuations around most probable positions are small.
Exercise 6.1.1
Hermite polynomials and moments of the Wigner
Show that (for n â‰¥4)
QN(x) = xn âˆ’n âˆ’1
2
xnâˆ’2 + (n âˆ’1)(n âˆ’2)(n âˆ’3)
8n
xnâˆ’4 + O

xnâˆ’6
,
(6.20)
therefore
gN(z) = 1
z âˆ’n âˆ’1
N
1
z3 âˆ’(n âˆ’1)(2n âˆ’3)
N
1
z5 + O
 1
z7

,
(6.21)
so in the large N limit we recover the ï¬rst few terms of the Wigner Stieltjes
transform
g(z) = 1
z âˆ’1
z3 âˆ’2
z5 + O
 1
z7

.
(6.22)
6.2 Laguerre Polynomials
6.2.1 Most Likely Characteristic Polynomial of Wishart Matrices
Similarly to the case of Wigner matrices, the Stieltjes transform of the most likely positions
of the Coulomb charges in the Wishart ensemble can be written as gN(z) := Ïˆâ€²(z)/NÏˆ(z),
where Ïˆ(z) is a monic polynomial of degree N satisfying
Ïˆâ€²â€²(x) âˆ’NV â€²(x)Ïˆâ€²(x) + N2N(x)Ïˆ(x) = 0,
(6.23)
with
NV â€²(x) = N âˆ’T âˆ’1 + 2Î²âˆ’1
x
+ T,
(6.24)

88
Eigenvalues and Orthogonal Polynomials
and, using Eq. (5.32),
N(x) = 1
N
N

k=1
V â€²(x) âˆ’V â€²(Î»âˆ—
k)
x âˆ’Î»âˆ—
k
= cN
x ,
(6.25)
where
cN = âˆ’N âˆ’T âˆ’1 + 2Î²âˆ’1
N2
N

k=1
1
Î»âˆ—
k
.
(6.26)
Writing now Ïˆ(x) = (T x) and u = T x, Eq. (6.23) becomes
uâ€²â€²(u) âˆ’(N âˆ’T âˆ’1 + 2Î²âˆ’1 + u)â€²(u) + N2
T cN(u) = 0.
(6.27)
This is the differential equation for the so-called associated Laguerre polynomials L(Î±) with
Î± = T âˆ’N âˆ’2Î²âˆ’1. It has polynomial solutions of degree N if and only if the coefï¬cient
of the (u) term is an integer equal to N (i.e. if cN = T /N). The solution is then given by
(u) âˆL(T âˆ’Nâˆ’2Î²âˆ’1)
N
(u),
(6.28)
where
L(Î±)
n (x) = xâˆ’Î± ( d
dx âˆ’1)n
n!
xÎ±+n.
(6.29)
Note that associated Laguerre polynomials are orthogonal with respect to the measure
xÎ±eâˆ’x, i.e.
 âˆ
0
dxxÎ±eâˆ’xL(Î±)
n (x)L(Î±)
m (x) = Î´nm
(n + Î±)!
n!
.
(6.30)
Given that the standard associated Laguerre polynomials have as a leading term
(âˆ’1)N/N! zN and that Ïˆ(x) is monic, we ï¬nally ï¬nd
Ïˆ(x) =
0
(âˆ’1)NN! T âˆ’NL(T âˆ’Nâˆ’2)(T x)
real symmetric
(Î² = 1),
(âˆ’1)NN! T âˆ’NL(T âˆ’Nâˆ’1)(T x)
complex Hermitian
(Î² = 2).
(6.31)
Hence, the most likely positions of the Coulombâ€“Wishart charges are given by the zeros
of associated Laguerre polynomials, exactly as the most likely positions of the Coulombâ€“
Wigner charges are given by the zeros of Hermite polynomials.
We should nevertheless check that cN = T /N is compatible with Eq. (6.26), i.e. that the
following equality holds:
T
N = Î± + 1
N2
N

k=1
1
Î»âˆ—
k
,
(6.32)

6.2 Laguerre Polynomials
89
0
200
400
600
l
0.000
0.001
0.002
0.003
0.004
r(l)
Figure 6.2 Histogram of the 100 zeros of L(100)
100 (x). The full line is the MarË‡cenkoâ€“Pastur distribution
(4.43) with q = 1
2 scaled by a factor T = 200.
where the Î»âˆ—
k are the zeros of T âˆ’1L(T x), i.e. Î»âˆ—
k = â„“(Î±)
k /T , where â„“(Î±)
k
are the zeros of the
associate Laguerre polynomials L(Î±)
N , which indeed obey the following relation:3
1
N
N

k=0
1
â„“(Î±)
k
=
1
Î± + 1.
(6.33)
From the results of Section 5.3.1, we thus conclude that the zeros of the Laguerre polyno-
mials L(T âˆ’Nâˆ’2)(T x) converge to a Wishart distribution with q = N/T . Figure 6.2 shows
the histogram of zeros of L(100)
100 (x) with the asymptotic prediction for large N and T . Note
that Î± â‰ˆN(qâˆ’1 âˆ’1) in that limit.
6.2.2 Average Characteristic Polynomial
As in the Wigner case we would like to get a recursion relation for Qq,N(z) := E[det(z 1âˆ’
W(N)
q
)], where W(N)
q
is a white Wishart matrix of size N and parameter q = T /N. This
time the recursion will be over T at N ï¬xed. So we keep N ï¬xed (we will drop the (N)
index to keep the notation light) and consider an unnormalized white Wishart matrix:
YT =
T

t=1
vtvT
t ,
(6.34)
where vt are T N-dimensional independent random vectors uniformly distributed on the
sphere. We want to compute
3 See e.g. AlÄ±cÄ± and Taeli [2015], where other inverse moments of the â„“(Î±)
k
are also derived.

90
Eigenvalues and Orthogonal Polynomials
qT,N(z) = E[det(z 1 âˆ’YT )].
(6.35)
The properly normalized expected characteristic polynomial is then given by Qq,N(z) =
T âˆ’nqT,N(T z). To construct our recursion relation, we will make use of the Shermannâ€“
Morrison formula (Eq. (1.30)), which states that for an invertible matrix A and vectors u
and v,
det(A + uvT ) = (1 + vT Aâˆ’1u) det A.
(6.36)
Applying this formula with A = z 1 âˆ’YT âˆ’1, we get
det(z 1 âˆ’YT ) = (1 âˆ’vT
T (z 1 âˆ’YT âˆ’1)âˆ’1vT ) det(z 1 âˆ’YT âˆ’1).
(6.37)
The vector vT is independent of the rest of YT âˆ’1, so taking the expectation value with
respect to this last vector we get
EvT
&
vT
T (z 1 âˆ’YT âˆ’1)âˆ’1vT
'
= Tr[(z 1 âˆ’YT âˆ’1)âˆ’1].
(6.38)
Now, using once again the general relation (easily derived in the basis where A is diagonal),
Tr[(z 1 âˆ’A)âˆ’1] det(z 1 âˆ’A) = d
dz det(z 1 âˆ’A),
(6.39)
with A = YT âˆ’1, we can take the expectation value of Eq. (6.37). We obtain
qT,N(z) =

1 âˆ’d
dz

qT âˆ’1,N(z).
(6.40)
To start the recursion relation, we note that Y0 is the N-dimensional zero matrix for which
q0(z) = zN. Hence,4
qT,N(z) =

1 âˆ’d
dz
T
zN.
(6.41)
If we apply an extra

1 âˆ’d
dz

to Eq. (6.41), we get the following recursion relation:
qT +1,N(z) = qT,N(z) âˆ’NqT,Nâˆ’1(z),
(6.42)
which is similar to the classic â€œthree-point ruleâ€ for Laguerre polynomials:
L(Î±+1)
N
(x) = L(Î±)
N (x) + L(Î±+1)
Nâˆ’1 (x).
(6.43)
This allows us to make the identiï¬cation
qT,N(z) = (âˆ’1)NN! L(T âˆ’N)
N
(z).
(6.44)
The correctly normalized average characteristic polynomial ï¬nally reads:
QT,N(z) = (âˆ’1)NT âˆ’NN! L(T âˆ’N)
N
(T z).
(6.45)
4 This relation will be further discussed in the context of ï¬nite free convolutions, see Chapter 12.

6.3 Unitary Ensembles
91
Hence, the average characteristic polynomial of real Wishart matrices is a Laguerre polyno-
mial, albeit with a slightly different value of Î± compared to the one obtained in Eq. (6.31)
above (Î± = T âˆ’N instead of Î± = T âˆ’N âˆ’2). The difference however becomes small
when N,T â†’âˆ.
6.3 Unitary Ensembles
In this section we will discuss the average characteristic polynomial for unitary ensem-
bles, ensembles of complex Hermitian matrices that are invariant under unitary transfor-
mations. Although this book mainly deals with real symmetric matrices, more is known
about complex Hermitian matrices, so we want to give a few general results about these
matrices that do not have a known equivalent in the real case. The main reason unitary
ensembles are easier to deal with than orthogonal ones has to do with the Vandermonde
determinant which is needed to change variables from matrix elements to eigenvalues.
Recall that
| det((M))| = | det V|Î² with V =
â›
âœâœâœâœâœâœâ
1
1
1
. . .
1
Î»1
Î»2
Î»3
. . .
Î»N
Î»2
1
Î»2
2
Î»2
3
. . .
Î»2
N
...
...
...
...
...
Î»Nâˆ’1
1
Î»Nâˆ’1
2
Î»Nâˆ’1
3
. . .
Î»Nâˆ’1
N
â
âŸâŸâŸâŸâŸâŸâ 
(6.46)
in the orthogonal case Î² = 1, the absolute value sign is needed to get the correct result.
In the case Î² = 2, (det V)2 is automatically positive and no absolute value is needed.
The absolute value for Î² = 1 is very hard to deal with analytically, while for Î² = 2 the
Vandermonde determinant is a polynomial in the eigenvalues.
6.3.1 Complex Wigner
In Section 6.1.2 we have shown that the expected characteristic polynomial of a unit vari-
ance real Wigner matrix is given by the Nth Hermite polynomial properly rescaled. The
argument relied on two facts: (i) the expectation value of any element of a Wigner matrix
is zero and (ii) all matrix elements are independent, save for
E[WijWji] = 1/N for i  j.
(6.47)
These two properties are shared by complex Wigner matrices. Therefore, the expected
characteristic polynomial of a complex Wigner matrix is the same as for a real Wigner
of the same size. We have shown that for a real or complex Wigner of size N,
QN(z) := E[det(z 1 âˆ’W)] = Nâˆ’N/2HN(
âˆš
Nz),
(6.48)

92
Eigenvalues and Orthogonal Polynomials
where HN(x) is the Nth Hermite polynomial, i.e. the Nth monic polynomial orthogonal in
the following sense:
 âˆ
âˆ’âˆ
Hi(x)Hj(x)eâˆ’x2/2 = 0 when i  j.
(6.49)
We can actually absorb the factors of N in QN(z) in the measure exp(âˆ’x2/2) and realize
that the polynomial QN(z) is the Nth monic polynomial orthogonal with respect to the
measure
wN(x) = exp(âˆ’Nx2/2).
(6.50)
There are two important remarks to be made about the orthogonality of QN(x) with respect
to the measure wN(x). First, QN(x) is the Nth in a series of orthogonal polynomials with
respect to an N-dependent measure. In particular QM(x) for M  N is an orthogonal
polynomial coming from a different measure. Second, the measure wN(x) is exactly the
weight coming from the matrix potential exp(âˆ’Î²NV (Î»)/2) for Î² = 2 and V (x) = x2/2.
In Section 6.3.3, we will see that these two statements are true for a general potential V (x)
when Î² = 2.
6.3.2 Complex Wishart
A complex white Wishart matrix can be written as a normalized sum of rank-1 complex
Hermitian projectors:
W = 1
T
T

t=1
vtvâ€ 
t ,
(6.51)
where the vectors vt are vectors of iid complex Gaussian numbers with zero mean and
normalized as
E[vtvâ€ 
t ] = 1.
(6.52)
The derivation of the average characteristic polynomial in the Wishart case in Section 6.2.2
only used the independence of the vectors vt and the expectation value E[vtvT
t ] = 1. So,
by replacing the matrix transposition by the Hermitian conjugation in the derivation we
can show that the expected characteristic polynomial of a complex white Wishart of size
N is also given by a Laguerre polynomial, as in Eq. (6.45). The Laguerre polynomials
L(T âˆ’N)
k
(x) are orthogonal in the sense of Eq. (6.30), with Î± = T âˆ’N. As in the Wigner
case, we can include the extra factor of T in the orthogonality weight and realize that the
expected characteristic polynomial of a real or complex Wishart matrix is the Nth monic
polynomial orthogonal with respect to the weight:
wN(x) = xT âˆ’Neâˆ’T x for 0 â‰¤x < âˆ.
(6.53)
This weight function is precisely the single eigenvalue weight, without the Vandermonde
term, of a complex Hermitian white Wishart of size N (see the footnote on page 46).

6.3 Unitary Ensembles
93
Note that the normalization of the weight wN(x) is irrelevant: the condition that the poly-
nomial is monic uniquely determines its normalization. Note as well that the real case is
given by the same polynomials, i.e. polynomials that are orthogonal with respect to the
complex weight, which is different from the real weight.
6.3.3 General Potential V (x)
The average characteristic polynomial for a matrix of size N in a unitary ensemble with
potential V (M) is given by
QN(x) := E[det(z 1 âˆ’M)],
(6.54)
which we can express via the joint law of the eigenvalues of M:
QN(z) âˆ

dNx
N
,
k=1
(z âˆ’xk) 2(x)eâˆ’N N
k=1 V (xk),
(6.55)
where (x) is the Vandermonde determinant:
(x) =
,
k<â„“
(xâ„“âˆ’xk).
(6.56)
We do not need to worry about the normalization of the above expectation value as we know
that QN(z) is a monic polynomial of degree N. In other words, the condition QN(z) =
zN + O(zNâˆ’1) is sufï¬cient to properly normalize QN(z). The ï¬rst step of the computation
is to combine one of the two Vandermonde determinants with the product of (z âˆ’xk):
2(x)
N
,
k=1
(z âˆ’xk) = (x)
,
k<â„“
(xâ„“âˆ’xk)
N
,
k=1
(z âˆ’xk) â‰¡(x)(x;z),
(6.57)
where (x;z) is a Vandermonde determinant of N + 1 variables, namely the N variables
xk and the extra variable z.
The second step is to write the determinants in the Vandermonde form:
(x) = det
â›
âœâœâœâœâœâœâ
1
1
1
. . .
1
x1
x2
x3
. . .
xN
x2
1
x2
2
x2
3
. . .
x2
N
...
...
...
...
...
xNâˆ’1
1
xNâˆ’1
2
xNâˆ’1
3
. . .
xNâˆ’1
N
â
âŸâŸâŸâŸâŸâŸâ 
.
(6.58)
We can add or subtract to any line a multiple of any other line and not change the above
determinant. By doing so we can transform all monomials xk
â„“into a monic polynomial of
degree k of our choice, so we have

94
Eigenvalues and Orthogonal Polynomials
(x) = det
â›
âœâœâœâœâœâ
1
1
1
. . .
1
p1(x1)
p1(x2)
p1(x3)
. . .
p1(xN)
p2(x1)
p2(x2)
p2(x3)
. . .
p2(xN)
...
...
...
...
...
pNâˆ’1(x1)
pNâˆ’1(x2)
pNâˆ’1(x3)
. . .
pNâˆ’1(xN)
â
âŸâŸâŸâŸâŸâ 
.
(6.59)
We will choose the polynomials pn(x) to be the monic polynomials orthogonal with respect
to the measure w(x) = eâˆ’NV (x), this will turn out to be extremely useful. We can now
perform the integral of the vector x in the following expression:
QN(z) âˆ

dNx (x)(x;z)eâˆ’N N
k=1 V (xk).
(6.60)
If we expand the two determinants (x) and (x;z) as signed sums over all permutations
and take their product, we realize that in each term each variable xk will appear exactly
twice in two polynomials, say pn(xk) and pm(xk), but by orthogonality we have

dxkpn(xk)pm(xk)eâˆ’NV (xk) = ZnÎ´mn,
(6.61)
where Zn is a normalization constant that will not matter in the end. The only terms that will
survive are those for which every xk appears in the same polynomial in both determinants.
For this to happen, the variable z must appear as pN(z), the only polynomial not in the ï¬rst
determinant. So this trick allows us to conclude with very little effort that
QN(z) âˆpN(z).
(6.62)
But since both QN(z) and pN(z) are monic, they must be equal. We have just shown that
for a Hermitian matrix M of size N drawn from a unitary ensemble with potential V (x),
E[det(z 1 âˆ’M)] = pN(z),
(6.63)
where pN(x) is the Nth monic orthogonal polynomial with respect to the measure eâˆ’NV (x).
It is possible to generalize this result to expectation of products of characteristic poly-
nomials evaluated at K different points zk, which allows one to study the joint distribution
of K eigenvalues. We give here the result without proof.5 We ï¬rst deï¬ne the expectation
value of a product of K characteristic polynomials:
FK(z1,z2, . . . ,zK) := E[det(z11 âˆ’M) det(z21 âˆ’M). . . det(zK1 âˆ’M)].
(6.64)
The multivariate function FK can be expressed as a determinant of orthogonal
polynomials:
FK(z1,z2, . . . ,zK) = 1
 det
â›
âœâœâ
pN(z1)
pN(z2)
. . .
pN(zK)
pN+1(z1)
pN+1(z2)
. . .
pN+1(zK)
...
...
...
...
pN+Kâˆ’1(z1)
pN+Kâˆ’1(z2)
. . .
pN+Kâˆ’1(zK)
â
âŸâŸâ ,
(6.65)
5 See BrÂ´ezin and Hikami [2011] for a derivation. Note that their formula equivalent to Eq. (6.67) is missing the K-dependent
constant factor.

6.3 Unitary Ensembles
95
where  := (z1,z2, . . . ,zK) is the usual Vandermonde determinant and the pâ„“(x) are
the monic orthogonal polynomials orthogonal with respect to eâˆ’NV (x). When K = 1,
 = 1 by deï¬nition and we recover our previous result F1(z) = pN(z). When the
arguments of FK are not all different, Eq. (6.65) gives an undetermined result (0/0) but
the limit is well deï¬ned. A useful case is when all arguments are equal:
FK(z) := FK(z,z, . . . ,z) = E[det(z 1 âˆ’M)K].
(6.66)
Taking the limit of Eq. (6.65) we ï¬nd the rather simple result
FK(z) =
1
Kâˆ’1
â„“=0 â„“!
det
â›
âœâœâœâœâœâ
pN(z)
pâ€²
N(z)
. . .
p(Kâˆ’1)
N
(z)
pN+1(z)
pâ€²
N+1(z)
. . .
p(Kâˆ’1)
N+1 (z)
...
...
...
...
pN+Kâˆ’1(z)
pâ€²
N+Kâˆ’1(z)
. . .
p(Kâˆ’1)
N+Kâˆ’1(z)
â
âŸâŸâŸâŸâŸâ 
,
(6.67)
where p(k)
â„“(x) is the kth derivative of the â„“th polynomial. In particular the average-square
characteristic polynomial is given by
F2(z) = pN(z)pâ€²
N+1(z) âˆ’pâ€²
N(z)pN+1(z).
(6.68)
Exercise 6.3.1
Variance of the Characteristic Polynomial of a 2 Ã— 2 Hermitian
Wigner Matrix
(a)
Show that the characteristic polynomial of a 2 Ã— 2 Hermitian Wigner matrix
is given by
QW
2 (z) = (z âˆ’w11)(z âˆ’w22) âˆ’(wR
12)2 âˆ’(wI
12)2,
(6.69)
where w11,w22,wR
12 and wI
12 are four real independent Gaussian random
numbers with variance 1 for the ï¬rst two and 1/2 for the other two.
(b)
Compute directly the mean and the variance of QW
2 (z).
(c)
Use Eqs. (6.63) and (6.68) and the ï¬rst few Hermite polynomials given in
Section 6.1.1 to obtain the same result, namely V[QW
2 (z)] = 2z2 + 2.
Bibliographical Notes
â€¢ On orthogonal polynomials, deï¬nitions and recursion relations:
â€“ G. SzegËo. Orthogonal Polynomials. AMS Colloquium Publications, volume 23.
American Mathematical Society, 1975,
â€“ D. Zwillinger, V. Moll, I. Gradshteyn, and I. Ryzhik, editors. Table of Integrals,
Series, and Products (Eighth Edition). Academic Press, New York, 2014,
â€“ R. Beals and R. Wong. Special Functions and Orthogonal Polynomials. Cambridge
Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2016.

96
Eigenvalues and Orthogonal Polynomials
â€¢ On the relation between Hermite polynomials, log gases and the Calogero model:
â€“ S. Agarwal, M. Kulkarni, and A. Dhar. Some connections between the classical
Calogero-Moser model and the log gas. Journal of Statistical Physics, 176(6), 1463â€“
1479, 2019.
â€¢ Orthogonal polynomials and random matrix theory:
â€“ M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
â€“ E. BrÂ´ezin and S. Hikami. Characteristic polynomials. In The Oxford Handbook of
Random Matrix Theory. Oxford University Press, Oxford, 2011,
â€“ P. Deift. Orthogonal Polynomials and Random Matrices: A Riemann-Hilbert Approach.
Courant Institute, New York, 1999.
â€¢ For identities obeyed by the zeros of Laguerre polynomials:
â€“ H. AlÄ±cÄ± and H. Taeli. Uniï¬cation of Stieltjes-Calogero type relations for the zeros
of classical orthogonal polynomials. Mathematical Methods in the Applied Sciences,
38(14):3118â€“3129, 2015.

7
The Jacobi Ensemble*
So far we have encountered two classical random matrix ensembles, namely Wigner and
Wishart. They are, respectively, the matrix equivalents of the Gaussian and the gamma
distribution. For example a 1 Ã— 1 Wigner matrix is a single Gaussian random number and
a 1 Ã— 1 Wishart is a gamma distributed number (see Eq. (4.16) with N = 1). We also
saw in Chapter 6 that these ensembles are intimately related to classical orthogonal poly-
nomials, respectively Hermite and Laguerre. The Gaussian distribution and its associated
Hermite polynomials appear very naturally in contexts where the underlying variable is
unbounded above and below. Gamma distributions and Laguerre polynomials appear in
problems where the variable is bounded from below (e.g. positive variables). Variables
that are bounded both from above and from below have their own natural distribution
and associated classical orthogonal polynomials, namely the beta distribution and Jacobi
polynomials.
In this chapter, we introduce a third classical random matrix ensemble: the Jacobi
ensemble. It is the random matrix equivalent of the beta distribution (and hence often
called matrix variate beta distribution). It will turn out to be strongly linked to Jacobi
orthogonal polynomials.
Jacobi matrices appear in multivariate analysis of variance and hence the Jacobi ensem-
ble is sometimes called the MANOVA ensemble. An important special case of the Jacobi
ensemble is the arcsine law which we already encountered in Section 5.5, and will again
encounter in Section 15.3.1. It is the law governing Coulomb repelling eigenvalues with
no external forces save for two hard walls. It also shows up in simple problems of matrix
addition and multiplications for matrices with only two eigenvalues.
7.1 Properties of Jacobi Matrices
7.1.1 Construction of a Jacobi Matrix
A beta-distributed random variable x âˆˆ(0,1) has the following law:
Pc1,c2(x) = 
(c1)
(c2)

(c1 + c2) xc1âˆ’1(1 âˆ’x)c2âˆ’1,
(7.1)
where c1 > 0 and c2 > 0 are two parameters characterizing the law.
97

98
The Jacobi Ensemble
To generalize (7.1) to matrices, we could deï¬ne J as a matrix generated from a beta
ensemble with a matrix potential that tends to V (x) = âˆ’log(Pc1,c2(x)) in the large N
limit. Although this is indeed the result we will get in the end, we would rather use a more
constructive approach that will give us a sensible deï¬nition of the matrix J at ï¬nite N and
for the three standard values of Î².
A beta(c1,c2) random number can alternatively be generated from two gamma-
distributed variables:
x =
w1
w1 + w2
,
w1,2 âˆ¼Gamma(c1,2,1).
(7.2)
The same relation can be rewritten as
x =
1
1 + wâˆ’1
1 w2
.
(7.3)
This is the formula we need for our matrix generalization. An unnormalized white Wishart
with T = cN is the matrix generalization of a Gamma(c,1) random variable. Combining
two such matrices as above will give us our Jacobi random matrix J. One last point before
we proceed, we need to symmetrize the combination wâˆ’1
1 w2 to yield a symmetric matrix.
We choose âˆšw2wâˆ’1
1
âˆšw2 which makes sense as Wishart matrices (like gamma-distributed
numbers) are positive deï¬nite.
We can now deï¬ne the Jacobi matrix. Let E be the symmetrized product of a white
Wishart and the inverse of another independent white Wishart, both without the usual 1/T
normalization:
E = 
W1/2
2

Wâˆ’1
1 
W1/2
2 ,
where

W1,2 := H1,2HT
1,2.
(7.4)
The two matrices H1,2 are rectangular matrices of standard Gaussian random numbers with
aspect ratio c1 = T1/N and c2 = T2/N (note that the usual aspect ratio is q = câˆ’1). The
standard Jacobi matrix is deï¬ned as
J = (1 + E)âˆ’1.
(7.5)
A Jacobi matrix has all its eigenvalues between 0 and 1. For the matrices 
W1,2 to make
sense we need c1,2 > 0. In addition, to ensure that 
W1 is invertible we need to impose
c1 > 1. It turns out that we can relax that assumption later and the ensemble still makes
sense for any c1,2 > 0.
7.1.2 Joint Law of the Elements
The joint law of the elements of a Jacobi matrix for Î² = 1,2 or 4 is given by
PÎ² (J) = cÎ²,T1,T2
J
[det(J)]Î²(T1âˆ’N+1)/2âˆ’1[det(1 âˆ’J)]Î²(T2âˆ’N+1)/2âˆ’1,
(7.6)
cÎ²,T1,T2
J
=
N
,
j=1

(1 + Î²/2)
(Î²(T1 + T2 âˆ’N + j)/2)

(1 + Î²j/2)
(Î²(T1 âˆ’N + j)/2)
(Î²(T2 âˆ’N + j)/2),
(7.7)

7.1 Properties of Jacobi Matrices
99
over the space of matrices of the proper symmetry such that both J and 1 âˆ’J are positive
deï¬nite.
To obtain this result, one needs to know the law of Wishart matrices (Chapter 4) and the
law of a matrix given the law of its inverse (Chapter 1).
Here is the derivation in the real symmetric case. We ï¬rst write the law of the matrix E
by realizing that for a ï¬xed matrix 
W1, the matrix E/T2 is a Wishart matrix with T = T2
and true covariance 
Wâˆ’1
1 . From Eq. (4.16), we thus have
P (E|W1) = (det E)(T2âˆ’Nâˆ’1)/2(det 
W1)T2/2
2NT2/2
N(T2/2)
exp
(
âˆ’1
2 Tr

E
W1
)
.
(7.8)
The matrix 
W1/T1 is itself a Wishart with probability
P

W1

= (det 
W1)(T1âˆ’Nâˆ’1)/2
2NT1/2
N(T1/2)
exp
(
âˆ’1
2 Tr

W1
)
.
(7.9)
Averaging Eq. (7.8) with respect to 
W1 we ï¬nd
P (E) =
(det E)(T2âˆ’Nâˆ’1)/2
2N(T1+T2)/2
N(T1/2)
N(T2/2)
Ã—

dW(det W)(T1+T2âˆ’Nâˆ’1)/2 exp
(
âˆ’1
2 Tr ((1 + E)W)
)
.
(7.10)
We can perform the integral over W by realizing that W/T is a Wishart matrix with
T = T1 + T2 and true covariance C = (1 + E)âˆ’1, see Eq. (4.16). We just need to
introduce the correct power of det C and numerical factors to make the integral equal to 1.
Thus,
P (E) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)(det E)(T2âˆ’Nâˆ’1)/2(det(1 + E))âˆ’(T1+T2)/2.
(7.11)
The miracle that for any N one can integrate exactly the product of a Wishart matrix and
the inverse of another Wishart matrix will appear again in the Bayesian theory of scm (see
Section 18.3).
Writing E+ = E + 1 we ï¬nd
P (E+) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)(det(E+ âˆ’1))(T2âˆ’Nâˆ’1)/2(det E+)âˆ’(T1+T2)/2. (7.12)
Finally we want J := Eâˆ’1
+ . The law of the inverse of a symmetric matrix A = Mâˆ’1 of
size N is given by (see Section 1.2.7)
PA(A) = PM(Aâˆ’1) det(A)âˆ’Nâˆ’1.
(7.13)
Hence,
P (J) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)

det(Jâˆ’1 âˆ’1)
(T2âˆ’Nâˆ’1)/2
(det J)(T1+T2)/2âˆ’Nâˆ’1 .
(7.14)
Using det(Jâˆ’1 âˆ’1) det(J) = det(1 âˆ’J) we can reorganize the powers of the determinants
and get
P (J) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)(det(1 âˆ’J))(T2âˆ’Nâˆ’1)/2(det J)(T1âˆ’Nâˆ’1)/2,
(7.15)
which is equivalent to Eq. (7.6) for Î² = 1.

100
The Jacobi Ensemble
7.1.3 Potential and Stieltjes Transform
The Jacobi ensemble is a beta ensemble satisfying Eq. (5.2) with matrix potential
V (x) = âˆ’T1 âˆ’N + 1 âˆ’2/Î²
N
log(x) âˆ’T2 âˆ’N + 1 âˆ’2/Î²
N
log(1 âˆ’x).
(7.16)
The derivative of the potential, in the large N limit, is given by
V â€²(x) = c1 âˆ’1 âˆ’(c1 + c2 âˆ’2)x
x(x âˆ’1)
.
(7.17)
The function V â€²(x) is not a polynomial or a Laurent polynomial but the function
x(x âˆ’1)V â€²(x) is a degree one polynomial. With a slight modiï¬cation of the argument of
Section 5.2.2, we can show that x(x âˆ’1)(x) = r + sx is a degree one polynomial. In the
large N limit we then have
g(z) =
c1 âˆ’1 âˆ’(c+ âˆ’2)z +
Â±
âƒ/
c2
+z2 âˆ’2(c1c+ + câˆ’)z + (c1 âˆ’1)2
2z(z âˆ’1)
,
(7.18)
where we have used the fact that we need s = 0 and r = 1 + c1 + c2 to get a 1/z behavior
at inï¬nity; we used the shorthand cÂ± = c2 Â± c1.
From the large z limit of (7.18) one can read off the normalized trace (or the average
eigenvalue):
Ï„(J) =
c1
c1 + c2
,
(7.19)
which is equal to one-half when c1 = c2. For c1 > 1 and c2 > 1, there are no poles, and
eigenvalues exist only when the argument of the square-root is negative. The density of
eigenvalues is therefore given by (see Fig. 7.1)
0.0
0.2
0.4
0.6
0.8
1.0
l
0.0
0.5
1.0
1.5
2.0
2.5
r(l)
Figure 7.1 Density of eigenvalues for the Jacobi ensemble with c1 = 5 and c2 = 2. The histogram
is a simulation of a single N = 1000 matrix with the same parameters.

7.1 Properties of Jacobi Matrices
101
Ï(Î») = c+
*
(Î»+ âˆ’Î»)(Î» âˆ’Î»âˆ’)
2Ï€Î»(1 âˆ’Î»)
,
(7.20)
where the edges of the spectrum are given by
Î»Â± = c1c+ + câˆ’Â± 2
*
c1c2(c+ âˆ’1)
c2
+
.
(7.21)
For 0 < c1 < 1 or 0 < c2 < 1, Eq. (7.18) will have Dirac deltas at z = 0 or z = 1,
depending on cases (see Exercise 7.1.1).
In the symmetric case c1 = c2 = c, we have explicitly
g(z) = (c âˆ’1)(1 âˆ’2z) +
Â±
âƒ*
c2(2z âˆ’1)2 âˆ’c(c âˆ’2)
2z(z âˆ’1)
.
(7.22)
The density for c â‰¥1 has no Dirac mass and is given by
Ï(Î») = c
*
(Î»+ âˆ’Î»)(Î» âˆ’Î»âˆ’)
Ï€Î»(1 âˆ’Î»)
,
(7.23)
with the edges given by
Î»Â± = 1
2 Â±
âˆš
2c âˆ’1
2c
.
(7.24)
Note that the distribution is symmetric around Î» = 1/2 (see Fig. 7.2).
As c â†’1, the edges tend to 0 and 1 and we recover the arcsine law:
Ï(Î») =
1
Ï€ âˆšÎ»(1 âˆ’Î»).
(7.25)
0.0
0.2
0.4
0.6
0.8
1.0
l
0
1
2
3
r(l)
c = 10
c = 2
arcsine law
Figure 7.2 Density of eigenvalues for a Jacobi matrix in the symmetric case (c1 = c2 = c) for
c = 20,2 and 1. The case c = 1 is the arcsine law.

102
The Jacobi Ensemble
Exercise 7.1.1
Dirac masses in the Jacobi density
(a)
Assuming that c1 > 1 and c2 > 1 show that there are no poles in Eq. (7.18) at
z = 0 or z = 1 by showing that the numerator vanishes for these two values
of z.
(b)
The parameters c1,2 can be smaller than 1 (as long as they are positive). Show
that in that case g(z) can have poles at z = 0 and/or z = 1 and ï¬nd the residue
at these poles.
7.2 Jacobi Matrices and Jacobi Polynomials
7.2.1 Centered-Range Jacobi Ensemble
The standard Jacobi matrix deï¬ned above has all its eigenvalues between 0 and 1. We would
like to use another deï¬nition of the Jacobi matrix with eigenvalues between âˆ’1 and 1. This
will make easier the link with orthogonal polynomials. We deï¬ne the centered-range Jacobi
matrix1
Jc = 2J âˆ’1.
(7.26)
This deï¬nition is equivalent to
Jc = 
Wâˆ’1/2
+
(
Wâˆ’)
Wâˆ’1/2
+
, where 
WÂ± = H1HT
1 Â± H2HT
2,
(7.27)
with H1,2 as above.
The matrix Jc is still a member of a beta ensemble satisfying Eq. (5.2) with a slightly
modiï¬ed matrix potential:
NV (x) = âˆ’(T1 âˆ’N + 1 âˆ’2/Î²) log(1 + x) âˆ’(T2 âˆ’N + 1 âˆ’2/Î²) log(1 âˆ’x). (7.28)
In the large N limit, the density of eigenvalues can easily be obtained from (7.20):
Ï(Î») = c+
*
(Î»+ âˆ’Î»)(Î» âˆ’Î»âˆ’)
2Ï€(1 âˆ’Î»2)
,
(7.29)
where the edges of the spectrum are given by
Î»Â± = câˆ’(2 âˆ’c+) Â± 4
*
c1c2(c+ âˆ’1)
c2
+
.
(7.30)
The special case c1 = c2 = 1 is the centered arcsine law:
Ï(Î») =
1
Ï€
*
(1 âˆ’Î»2)
.
(7.31)
1 Note that the matrix Jc is not necessarily centered in the sense Ï„(Jc) = 0 but the potential range of its eigenvalues is [âˆ’1,1]
centered around zero.

7.2 Jacobi Matrices and Jacobi Polynomials
103
7.2.2 Average Expected Characteristic Polynomial
In Section 6.3.3, we saw that the average characteristic polynomial when Î² = 2 is the Nth
monic polynomial orthogonal to the weight w(x) âˆexp(âˆ’NV (x)). For the centered-range
Jacobi matrix we have
w(x) = (1 + x)T1âˆ’N(1 âˆ’x)T2âˆ’N.
(7.32)
The Jacobi polynomials P (a,b)
n
(x) are precisely orthogonal to such weight functions with
a = T2 âˆ’N and b = T1 âˆ’N. (Note that unfortunately the standard order of the param-
eters is inverted with respect to Jacobi matrices.) Jacobi polynomials satisfy the following
differential equation:
(1 âˆ’x2)yâ€²â€² + (b âˆ’a âˆ’(a + b + 2)x)yâ€² + n(n + a + b + 1)y = 0.
(7.33)
This equation has polynomial solutions if and only if n is an integer. The solution is then
y âˆP (a,b)
n
(x).
The ï¬rst three Jacobi polynomials are
P (a,b)
0
(x) = 1,
P (a,b)
1
(x) = (a + b + 2)
2
x + a âˆ’b
2
,
(7.34)
P (a,b)
2
(x) = (a + b + 3)(a + b + 4)
8
(x âˆ’1)2 + (a + 2)(a + b + 3)
2
(x âˆ’1)
+ (a + 1)(a + 2)
2
.
The normalization of Jacobi polynomials is arbitrary but in the standard normalization
they are not monic. The coefï¬cient of xn in P (a,b)
n
(x) is
an =

[a + b + 2n + 1]
2nn! 
[a + b + n âˆ’1].
(7.35)
In summary we have (for Î² = 2)
E [det[z1 âˆ’Jc]] = 2NN! 
[T1 + T2 + 1 âˆ’N]

[T1 + T2 + 1]
P (T2âˆ’N,T1âˆ’N)
N
(z).
(7.36)
Note that we must have T1 â‰¥N and T2 â‰¥N.
When T1 = T2 = N (i.e. c1 = c2 = 1, corresponding to the arcsine law), the
polynomials P (0,0)
N
(z) are called Legendre polynomials PN(z):2
E [det[z1 âˆ’Yc]] = 2N(N! )2
(2N)! PN(z).
(7.38)
2 Legendre polynomials are deï¬ned as the polynomial solution of
d
dx
(
(1 âˆ’x2) dPn(x)
dx
)
+ n(n + 1)Pn(x) = 0,
Pn(1) = 1.
(7.37)

104
The Jacobi Ensemble
7.2.3 Maximum Likelihood Conï¬guration at Finite N
In Chapter 6, we studied the most likely conï¬guration of eigenvalues for the beta ensemble
at ï¬nite N. We saw that the ï¬nite-N Stieltjes transform of this solution gN(z) is related to
a monic polynomial Ïˆ(x) via
gN(z) = 1
N
N

i=1
1
z âˆ’Î»i
= Ïˆâ€²(z)
NÏˆ(z),
where
Ïˆ(x) =
N
,
i=1
(x âˆ’Î»i).
(7.39)
The polynomial Ïˆ(x) satisï¬es Eq. (6.23) which we recall here:
Ïˆâ€²â€²(x) âˆ’NV â€²(x)Ïˆâ€²(x) + N2N(x)Ïˆ(x) = 0,
(7.40)
where the function N(x) is deï¬ned by Eq. (5.32). For the case of the centered-range
Jacobi ensemble we have
NV â€²(x) = a âˆ’b + (a + b + 2)x
1 âˆ’x2
,
(7.41)
where we have anticipated the result by introducing the notation a = T2 âˆ’N âˆ’2/Î² and
b = T1 âˆ’N âˆ’2/Î². The function N(x) is given by
N(x) = rN + sNx
1 âˆ’x2 .
(7.42)
We will see below that the coefï¬cient sN is zero because of the symmetry {c1,c2,Î»k} â†’
{c2,c1, âˆ’Î»k} of the most likely solution.
The equation for Ïˆ(x) becomes
(1 âˆ’x2)Ïˆâ€²â€²(x) + (b âˆ’a âˆ’(a + b + 2)x)Ïˆâ€²(x) + rNN2Ïˆ(x) = 0.
(7.43)
We recognize the differential equation satisï¬ed by the Jacobi polynomials (7.33). Its solu-
tions are polynomials only if rNN = N + a + b + 1, which implies that rN = c1 + c2 âˆ’
1 + (1 âˆ’4Î²)/N. This is consistent with the large N limit r = c1 + c2 âˆ’1 in Section 7.1.3.
The solutions are given by
Ïˆ(x) âˆP (a,b)
N
(x),
(7.44)
with the proportionality constant chosen such that Ïˆ(x) is monic.
In the special case T1 = T2 = N + 2/Î², i.e. T = N + 2 for real symmetric matrices
and T = N + 1 for complex Hermitian matrices, we have a = b = 0 and the polynomials
reduce to Legendre polynomials.
Another special case corresponds to Chebyshev polynomials:
Tn(x) = P

âˆ’1
2,âˆ’1
2

n
(x)
and
Un(x) = P

1
2, 1
2

n
(x),
(7.45)
where Tn(x) and Un(x) are the Chebyshev polynomials of ï¬rst and second kind respec-
tively. Since a = T2 âˆ’N âˆ’2/Î² and b = T1 âˆ’N âˆ’2/Î², they appear as solutions for
T1 = T2 = N + 2/Î² âˆ’1/2 (ï¬rst kind) or T1 = T2 = N + 2/Î² + 1/2 (second kind).
These values of T1 = T2 are not integers but we can still consider the matrix potential

7.2 Jacobi Matrices and Jacobi Polynomials
105
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
l
0.00
0.25
0.50
0.75
1.00
1.25
r(l)
Figure 7.3 Histogram of the 200 zeros of P (200,600)
200
(x). The full line is the Jacobi eigenvalue density,
Eq. (7.29), with c1 = 4 and c2 = 2.
given by Eq. (7.28) without the explicit Wishart matrix construction. In the large N limit,
the density of the zeros of the Jacobi polynomials P (T2âˆ’N,T1âˆ’N)
N
(x) is given by Eq. (7.29)
with c1,2 = T1,2/N. Figure 7.3 shows a histogram of the zeros of P (200,600)
200
(x).
When c1 â†’1 and c2 â†’1, the density becomes the centered arcsine law. As a conse-
quence, we have shown that the zeros of Chebyshev (both kinds) and Legendre polynomials
(for which T1 = T2 = N + O(1)) are distributed according to the centered arcsine law in
the large N limit.
We have seen that in order for Eq. (7.40) to have polynomial solutions we must have
N(1 âˆ’x2)N(x) = N + a + b + 1,
(7.46)
where the function N(x) is deï¬ned from the most likely conï¬guration or, equivalently,
the roots of the Jacobi polynomial P (a,b)
N
(z) by
N(x) = 1
N
N

k=0
V â€²(x) âˆ’V â€²(Î»k)
x âˆ’Î»k
.
(7.47)
From these expressions we can ï¬nd a relationship that roots of Jacobi polynomials must
satisfy. Indeed, injecting Eq. (7.41), we ï¬nd
N(1 âˆ’x2)N(x) =
N

k=1
(a âˆ’b)(x2 âˆ’Î»2
k) + (a + b + 2)(x âˆ’Î»k)(1 + xÎ»k)
(1 âˆ’Î»2
k)(x âˆ’Î»k)
.
(7.48)
For each k the numerator is a second degree polynomial in x that is zero at x = Î»k,
canceling the x âˆ’Î»k factor in the denominator, so the whole expression is a ï¬rst degree
polynomial. Equating this expression to Eq. (7.47), we ï¬nd that the term linear in x of

106
The Jacobi Ensemble
this polynomial must be zero and the constant term equal to N + a + b + 1, yielding two
equations:
1
N
N

k=1
(a + b + 2)Î»k + (a âˆ’b)
(1 âˆ’Î»2
k)
= 0,
(7.49)
1
N
N

k=1
(a + b + 2) + (a âˆ’b)Î»k
(1 âˆ’Î»2
k)
= N + a + b + 1.
(7.50)
These equations give us non-trival relations satisï¬ed by the roots of Jacobi polynomials
P (a,b)
N
(x). If we sum or subtract the two equations above, we ï¬nally obtain3
1
N
N

k=1
1
(1 âˆ’Î»k) = a + b + N + 1
2(a + 1)
,
(7.51)
1
N
N

k=1
1
(1 + Î»k) = a + b + N + 1
2(b + 1)
.
(7.52)
7.2.4 Discrete Laplacian in One Dimension and Chebyshev Polynomials
Chebyshev polynomials and the arcsine law are also related via a simple deterministic
matrix: the discrete Laplacian in one dimension, deï¬ned as
 = 1
2
â›
âœâœâœâœâœâœâœâœâ
2
âˆ’1
0
Â· Â· Â·
0
0
âˆ’1
2
âˆ’1
Â· Â· Â·
0
0
0
âˆ’1
2
Â· Â· Â·
0
0
0
0
âˆ’1
Â· Â· Â·
0
0
0
0
0
...
0
âˆ’1
0
0
0
Â· Â· Â·
âˆ’1
2
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(7.53)
We will see that the spectrum of âˆ’1 at large N is again given by the arcsine law. One way
to obtain this result is to modify the top-right and bottom-left corner elements by adding
âˆ’1. The modiï¬ed matrix is then a circulant matrix which can be diagonalized exactly (see
Exercise 7.2.1 and Appendix A.3).
We will use a different route which will also uncover the link to Chebyshev polynomials.
We will compute the characteristic polynomial QN(z) of  âˆ’1 for all values of N by
induction. The ï¬rst two are
Q1(z) = z,
(7.54)
Q2(z) = z2 âˆ’1
4.
(7.55)
3 These relations were recently obtained in AlÄ±cÄ± and Taeli [2015].

7.2 Jacobi Matrices and Jacobi Polynomials
107
For N â‰¥3 we can write a recursion relation by expanding in minors the ï¬rst line of
the determinant of z1 âˆ’ + 1. The (11)-minor is just QNâˆ’1(z). The ï¬rst column of the
(12)-minor only has one element equal to 1/2; if we expand this column its only minor is
QNâˆ’2(z). We ï¬nd
QN(z) = zQNâˆ’1(z) âˆ’1
4QNâˆ’2(z).
(7.56)
This simple recursion relation is similar to that of Chebyshev polynomials UN(x):
UN(x) = 2xUNâˆ’1 âˆ’UNâˆ’2(x).
(7.57)
The standard Chebyshev polynomials are not monic but have leading term UN(x) âˆ¼2NxN.
Monic Chebyshev (
UN(x) = 2âˆ’NUN(x)) in fact precisely satisfy Eq. (7.56). Given our
ï¬rst two polynomials are the monic Chebyshev of the second kind, we conclude that
QN(z) = 2âˆ’NUN(z) for all N. The eigenvalues of  âˆ’1 at size N are therefore given
by the zeros of the Nth Chebyshev polynomial of the second kind. In the large N limit
those are distributed according to the centered arcsine law. QED.
We will see in Section 15.3.1 that the sum of two random symmetric orthogonal matrices
also has eigenvalues distributed according to the arcsine law.
Exercise 7.2.1
Diagonalizing the Discrete Laplacian
Consider M =  âˆ’1 with âˆ’1/2 added to the top-right and bottom-left
corners.
(a)
Show that the vectors [vk]j = ei2Ï€kj are eigenvectors of M with eigenvalues
Î»k = cos(2Ï€k).
(b)
Show that the eigenvalue density of M in the large N limit is given by the
centered arcsine law (7.31).
Bibliographical Notes
â€¢ For general references on orthogonal polynomials, see
â€“ R. Beals and R. Wong. Special Functions and Orthogonal Polynomials. Cambridge
Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2016,
â€“ D. Zwillinger, V. Moll, I. Gradshteyn, and I. Ryzhik, editors. Table of Integrals,
Series, and Products (Eighth Edition). Academic Press, New York, 2014.
â€¢ On relations obeyed by zeros of Jacobi polynomials, see
â€“ H. AlÄ±cÄ± and H. Taeli. Uniï¬cation of Stieltjes-Calogero type relations for the zeros
of classical orthogonal polynomials. Mathematical Methods in the Applied Sciences,
38(14):3118â€“3129, 2015.
â€¢ On the MANOVA method, see e.g.

108
The Jacobi Ensemble
â€“ R. T. Warne. A primer on multivariate analysis of variance (MANOVA) for behav-
ioral scientists. Practical Assessment, Research & Evaluation, 19(17):1â€“10, 2014.
â€¢ On the spectrum of Jacobi matrices using Coulomb gas methods, see
â€“ H. M. Ramli, E. Katzav, and I. P. Castillo. Spectral properties of the Jacobi ensembles
via the Coulomb gas approach. Journal of Physics A: Mathematical and Theoretical,
45(46):465005, 2012.
and references therein.

Part II
Sums and Products of Random Matrices


8
Addition of Random Variables and Brownian Motion
In the following chapters we will be interested in the properties of sums (and products)
of random matrices. Before embarking on this relatively new ï¬eld, the present chapter
will quickly review some classical results concerning sums of random scalars, and the
corresponding continuous time limit that leads to the Brownian motion and stochastic
calculus.
8.1 Sums of Random Variables
Let us thus consider X = X1+X2 where X1 and X2 are two random variables, independent,
and distributed according to, respectively, P1(x1) and P2(x2). The probability that X is
equal to x (to within dx) is given by the sum over all combinations of x1 and x2 such
that x1 + x2 = x, weighted by their respective probabilities. The variables X1 and X2
being independent, the joint probability that X1 = x1 and X2 = x âˆ’x1 is equal to
P1(x1)P2(x âˆ’x1), from which one obtains
P (2)(x) =

P1(xâ€²)P2(x âˆ’xâ€²) dxâ€².
(8.1)
This equation deï¬nes the convolution between P1 and P2, which we will write P (2) =
P1 â‹†P2. The generalization to the sum of N independent random variables is immediate. If
X = X1 + X2 + Â· Â· Â· + XN with Xi distributed according to Pi(xi), the distribution of X is
obtained as
P (N)(x) =

N
,
i=1
dxiP1(x1)P2(x2). . . PN(xN)Î´

x âˆ’
N

i=1
xi

,
(8.2)
where Î´(.) is the Dirac delta function. The analytical or numerical manipulations of Eqs.
(8.1) and (8.2) are much eased by the use of Fourier transforms, for which convolutions
become simple products. The equation P (2)(x) = [P1 â‹†P2](x), reads, in Fourier space,
Ï•(2)(k) =

eik(xâˆ’xâ€²+xâ€²)

P1(xâ€²)P2(x âˆ’xâ€²) dxâ€² dx â‰¡Ï•1(k)Ï•2(k),
(8.3)
111

112
Addition of Random Variables and Brownian Motion
where Ï•(k) denotes the Fourier transform of the corresponding probability density P(x). It
is often called its characteristic (or generating) function. Since the characteristic functions
multiply, their logarithms add, i.e. the function H(k) deï¬ned below is additive:
H(k) := log Ï•(k) = log E[eikX].
(8.4)
It allows one to recovers its so called cumulants cn (provided they are ï¬nite) through
cn := (âˆ’i)n dn
dzn H(k)

k=0
.
(8.5)
The cumulants cn are polynomial combinations of the moments mp with p â‰¤n. For
example c1 = m1 is the mean of the distribution and c2 = m2 âˆ’m2
1 = Ïƒ 2 its variance. It
is clear that the mean of the sum of two random variables (independent or not) is equal to
the sum of the individual means. The mean is thus additive under convolution. The same is
true for the variance, but only for independent variables.
More generally, from the additive property of H(k) all the cumulants of two independent
distributions simply add. The additivity of cumulants is a consequence of the linearity
of derivation. The cumulants of a given law convoluted N times with itself thus follow
the simple rule cn,N = Ncn,1, where the {cn,1} are the cumulants of the elementary
distribution P1.
An important case is when P1 is a Gaussian distribution,
P1(x) =
1
âˆš
2Ï€Ïƒ 2 eâˆ’(xâˆ’m)2
2Ïƒ2 ,
(8.6)
such that log Ï•1(k) = imk âˆ’Ïƒ 2k2/2. The Gaussian distribution is such that all cumulants
of order â‰¥3 are zero. This property is clearly preserved under convolution: the sum of
Gaussian random variables remains Gaussian. Conversely, one can always write a Gaussian
variable as a sum of an arbitrary number of Gaussian variables: Gaussian variables are
inï¬nitely divisible.
In the following, we will consider inï¬nitesimal Gaussian variables, noted dB, such that
E[dB] = 0 and E[dB2] = dt, where dt â†’0 is an inï¬nitesimal quantity, which we will
interpret as an inï¬nitesimal time increment. In other words, dB is a mean zero Gaussian
random variable which has ï¬‚uctuations of order
âˆš
dt.
8.2 Stochastic Calculus
8.2.1 Brownian Motion
The starting point of stochastic calculus is the Brownian motion (also called Wiener pro-
cess) Xt, which is a Gaussian random variable of mean Î¼t and variance Ïƒ 2t. From the
inï¬nite divisibility property of Gaussian variables, one can always write
Xtk =
kâˆ’1

â„“=0
Î¼Î´t +
kâˆ’1

â„“=0
ÏƒÎ´Bâ„“,
(8.7)

8.2 Stochastic Calculus
113
0.0
0.2
0.4
0.6
0.8
1.0
t
âˆ’0.75
âˆ’0.50
âˆ’0.25
0.00
0.25
Bt
Figure 8.1 An example of Brownian motion.
where tk = kt/N, 0 â‰¤k â‰¤N, Î´t = T /N and Î´Bâ„“âˆ¼N(0,Î´t) for each â„“. By construction
X(tN) = Xt. In the limit N â†’âˆ, we have Î´t â†’dt, Î´Bk â†’dB and Xtk becomes a
continuous time process with
dXt = Î¼dt + ÏƒdBt,
X0 = 0,
(8.8)
where dBt are independent, inï¬nitesimal Gaussian variables as deï¬ned above (see Fig. 8.1).
The process Xt is continuous but nowhere differentiable. Note that Xt and Xtâ€² are not
independent but their increments are, i.e. Xtâ€² and Xt âˆ’Xtâ€² are independent whenever tâ€² < t.
Note the convention that Xtk is built from past increments Î´Bâ„“for â„“< k but does not
include Î´Bk. This convention is called the ItË†o prescription.1 Its main advantage is that Xt
is independent of the equal-time dBt, but this comes at a price: the usual chain rule for
differentiation has to be corrected by the so-called ItË†o term, which we now discuss.
8.2.2 ItË†oâ€™s Lemma
We now study the behavior of functions F(Xt) of a Wiener process Xt. Because dB2 is of
order dt, and not dt2, one has to be careful when evaluating derivatives of functions of Xt.
Given a twice differentiable function F(.), we consider the process F(Xt). Reverting
for a moment to a discretized version of the process, one has
F(X(t + Î´t)) = F(Xt) + Î´X F â€²(Xt) + (Î´X)2
2
F â€²â€²(Xt) + o(Î´t),
(8.9)
1 In the Stratonovich prescription, half of Î´Bk contributes to Xtk . In this prescription, the ItË†o lemma is not needed, i.e. the chain
rule applies without any correction term, but the price to pay is a correlation between Xt and dB(t). We will not use the
Stratonovich prescription in this book.

114
Addition of Random Variables and Brownian Motion
where
Î´X = Î¼Î´t + ÏƒÎ´B,
(8.10)
and
(Î´X)2 = Î¼2(Î´t)2 + Ïƒ 2Î´t + Ïƒ 2 &
(Î´B)2 âˆ’Î´t
'
+ 2Î¼ÏƒÎ´tÎ´B.
(8.11)
The random variable (Î´B)2 has mean Ïƒ 2Î´t so the ï¬rst and last terms are clearly o(Î´t) when
Î´t â†’0. The third term has standard deviation given by
âˆš
2Ïƒ 2Î´t; so the third term is also
of order Î´t but of zero mean. It is thus a random term much like Î´B, but much smaller since
Î´B is of order
âˆš
Î´t â‰«Î´t. The ItË†o lemma is a precise mathematical statement that justiï¬es
why this term can be neglected to ï¬rst order in Î´t. Hence, letting Î´t â†’dt, we get
dFt = âˆ‚F
âˆ‚XdXt + Ïƒ 2
2
âˆ‚2F
âˆ‚2Xdt.
(8.12)
When compared to ordinary calculus, there is a correction term â€“ the ItË†o term â€“ that depends
on the second order derivative of F.
More generally, we can consider a general ItË†o process where Î¼ and Ïƒ themselves depend
on Xt and t, i.e.
dXt = Î¼(Xt,t)dt + Ïƒ(Xt,t)dBt.
(8.13)
Then, for functions F(X,t) that may have an explicit time dependence, one has
dFt = âˆ‚F
âˆ‚XdXt +
(âˆ‚F
âˆ‚t + Ïƒ 2(Xt,t)
2
âˆ‚2F
âˆ‚2X
)
dt.
(8.14)
ItË†oâ€™s lemma can be extended to functions of several stochastic variables. Consider a
collection of N independent stochastic variables {Xi,t} (written vectorially as Xt) and
such that
dXi,t = Î¼i(Xt,t)dt + dWi,t,
(8.15)
where dWi,t are Wiener noises such that
E

dWi,tdWj,t

:= Cij(Xt,t)dt.
(8.16)
The vectorial form of ItË†oâ€™s lemma states that the time evolution of a function F(Xt,t) is
given by the sum of three contributions:
dFt =
N

i=1
âˆ‚F
âˆ‚Xi
dXi,t +
â¡
â£âˆ‚F
âˆ‚t +
N

i,j=1
Cij(Xt,t)
2
âˆ‚2F
âˆ‚Xiâˆ‚Xj
â¤
â¦dt.
(8.17)
The formula simpliï¬es when all the Wiener noises are independent, in which case the ItË†o
term only contains the second derivatives âˆ‚2F/âˆ‚2Xi.

8.2 Stochastic Calculus
115
8.2.3 Variance as a Function of Time
As an illustration of how to use ItË†oâ€™s formula let us recompute the time dependent variance
of Xt. Assume Î¼ = 0 and choose F(X) = X2. Applying Eq. (8.12), we get that
dFt = 2XtdXt + Ïƒ 2dt â‡’F(Xt) = 2
 t
0
ÏƒXsdBs + Ïƒ 2t.
(8.18)
In order to take the expectation value of this equation, we scrutinize the term E[XsdBs]. As
alluded to above, the random inï¬nitesimal element dBs does not contribute to Xs, which
only depends on dBsâ€²<s. Therefore E[XsdBs] = 0, and, as expected,
E[X2
t ] = E[F(Xt)] = Ïƒ 2t.
(8.19)
The Brownian motion has a variance from the origin that grows linearly with time. The
same result can of course be derived directly from the integrated form Xt = ÏƒBt, where Bt
is a Gaussian random number of variance equal to t.
8.2.4 Gaussian Addition
ItË†oâ€™s lemma can be used to compute a special case of the law of addition of independent
random variables, namely when one of the variables is Gaussian. Consider the random
variable Z = Y + X, where Y is some random variable, and X is an independent Gaussian
(X âˆ¼N(Î¼,Ïƒ 2)). The law of Z is uniquely determined by its characteristic function:
Ï•(k) := E[eikZ].
(8.20)
We now let Z â†’Zt be a Brownian motion with Z0 = Y:
dZt = Î¼dt + ÏƒdBt,
Z0 = Y.
(8.21)
Note that Zt=1 has the same law as Z. The idea is now to study the function F(Zt) := eikZt
using ItË†oâ€™s lemma, Eq. (8.14). Hence,
dFt = ikeikZt dZt âˆ’k2Ïƒ 2
2
eikZt dt =

ikÎ¼F âˆ’k2Ïƒ 2
2
F

dt + ikFdBt.
(8.22)
Taking the expectation value, writing Ï•t(k) = E[F(t)], and noting that the differential d is
a linear operator and therefore commutes with the expectation value, we obtain
dÏ•t(k) =

ikÎ¼ âˆ’k2Ïƒ 2
2

Ï•t(k)dt,
(8.23)
or
1
Ï•t(k)
d
dt Ï•t(k) = d
dt log (Ï•t(k)) =

ikÎ¼ âˆ’k2Ïƒ 2
2

.
(8.24)

116
Addition of Random Variables and Brownian Motion
From its solution at t = 1, we get
log (Ï•1(k)) = log (Ï•0(k)) + ikÎ¼ âˆ’k2Ïƒ 2
2
.
(8.25)
Recognizing the last two terms in the right hand side as the characteristic function of a
Gaussian variable, we recover the fact that the log-characteristic function is additive under
the addition of independent random variables. Although the result is true in general, the
calculation above using stochastic calculus is only valid if one of the random variable is
Gaussian.
8.2.5 The Langevin Equation
We would like to construct a stochastic process for a variable Xt such that in the steady-
state regime the values of Xt are drawn from a given probability distribution P(x). To build
our stochastic process, let us ï¬rst consider the simple Brownian motion with unit variance
per unit time:
dXt = dBt.
(8.26)
As revealed by Eq. (8.19) the variance of Xt grows linearly with time and the process
never reaches a stationary state. To make it stationary we need a mechanism to limit the
variance of Xt. We cannot â€˜subtractâ€™ variance but we can reduce Xt by scaling. If at every
inï¬nitesimal time step we replace Xt+dt by Xt+dt/
âˆš
1 + dt, the variance of Xt will remain
equal to unity. We also know that the distribution of Xt is Gaussian (if the initial condition is
Gaussian or constant). With this extra rescaling, Xt is still Gaussian at every step, so clearly
this will describe the stationary state of our rescaled process. As a stochastic differential
equation, we have, neglecting terms of order (dt)3/2
dXt = dBt +
Xt
âˆš
1 + dt
âˆ’Xt = dBt âˆ’1
2Xtdt.
(8.27)
This stationary version of the random walk is the Ornsteinâ€“Uhlenbeck process (see
Fig. 8.2). A physical interpretation of this equation is that of a particle located at Xt
moving in a viscous medium subjected to random forces dBt/dt and a deterministic
harmonic force (â€œspringâ€) âˆ’Xt/2. The viscous medium is such that velocity (and not
acceleration) is proportional to force.
We would like to generalize the above formalism to generate any distribution P(x) for
the distribution of X in the stationary state. One way to do so is to change the linear force
âˆ’Xt/2 to a general non-linear force F(Xt) := âˆ’V â€²(Xt)/2, where we have written the
force as the derivative of a potential V and introduced a factor of 2 which will prove to be
convenient. If the potential is convex, the force will drive the particle towards the minimum
of the potential while the noise dBt will drive the particle away. We expect that this system
will reach a steady state. Our stochastic equation is now
dXt = dBt + F(Xt)dt.
(8.28)

8.2 Stochastic Calculus
117
0
20
40
t
âˆ’3
âˆ’2
âˆ’1
0
1
2
Xt
âˆ’4
âˆ’2
0
2
4
X
0.0
0.1
0.2
0.3
0.4
P(X)
Figure 8.2 (left) A simulation of the Langevin equation for the Ornsteinâ€“Uhlenbeck process (8.27)
with 50 steps per unit time. Note that the correlation time is Ï„c = 2 here, so excursions away from
zero typically take 2 time units to mean-revert back to zero. Farther excursions take longer to come
back. (right) Histogram of the values of Xt for the same process simulated up to t = 2000 and
comparison with the normal distribution. The agreement varies from sample to sample as a rare far
excursion can affect the sample distribution even for t = 2000.
What is the distribution of Xt in the steady state? To ï¬nd out, let us consider an arbi-
trary test function f (Xt) and see how it behaves in the steady state. Using ItË†oâ€™s lemma,
Eq. (8.12), we have
df (Xt) = f â€²(Xt)
(
dBt âˆ’1
2V â€²(Xt)dt
)
+ 1
2f â€²â€²(Xt)dt.
(8.29)
Taking the expectation value on both sides and demanding that dE[ft]/dt = 0 in the steady
state we ï¬nd
E

f â€²(Xt)V â€²(Xt)

= E

f â€²â€²(Xt)

.
(8.30)
This must be true for any function f . In order to infer the corresponding stationary distri-
bution P(x), let us write h(x) = f â€²(x) and write these expectation values as

h(x)V â€²(x)P(x)dx =

hâ€²(x)P(x)dx.
(8.31)
Since we want to relate an integral of h to one of hâ€² we should use integration by parts:

hâ€²(x)P(x)dx = âˆ’

h(x)P â€²(x)dx = âˆ’

h(x)P â€²(x)
P(x) P(x)dx.
(8.32)
Since Eq. (8.31) is true for any function h(x) we must have

118
Addition of Random Variables and Brownian Motion
V â€²(x) = âˆ’P â€²(x)
P(x)
â‡’
P(x) = Zâˆ’1 exp[âˆ’V (x)],
(8.33)
where Z is an integration constant that ï¬xes the normalization of the law P(x).
To recapitulate, given a probability density P(x), we can deï¬ne a potential V (x) =
âˆ’log P(x) (up to an irrelevant additive constant) and consider the stochastic differential
equation
dXt = dBt âˆ’1
2V â€²(Xt)dt.
(8.34)
The stochastic variable Xt will eventually reach a steady state. In that steady state the law of
Xt will be given by P(x). Equation (8.34) is called the Langevin equation. The strength of
the Langevin equation is that it allows one to replace the average over the probability P(x)
by a sample average over time of a stochastic process.2 Any rescaling of time t â†’Ïƒ 2t
would yield a Langevin equation with the same stationary state:
dXt = ÏƒdBt âˆ’Ïƒ 2
2 V â€²(Xt)dt.
(8.35)
We have learned another useful fact from Eq. (8.30): the random variable V â€²(X) acts
as a derivative with respect to X under the expectation value. In that sense V â€²(X) can be
considered the conjugate variable to X.
It is very straightforward to generalize our one-dimensional Langevin equation to a set
of N variables {Xi} that are drawn from the joint law P(x) = Zâˆ’1 exp[âˆ’V (x)]. We get
dXi = ÏƒdBi + Ïƒ 2
2
âˆ‚
âˆ‚Xi
log P(x) dt,
(8.36)
where we have dropped the subscript t for clarity.
Exercise 8.2.1
Langevin equation for Studentâ€™s t-distributions
The family of Studentâ€™s t-distributions, parameterized by the tail exponent Î¼,
is given by the probability density
PÎ¼(x) = Zâˆ’1
Î¼

1 + x2
Î¼
âˆ’Î¼+1
2
with Zâˆ’1
Î¼
=


Î¼+1
2

âˆšÎ¼Ï€
 Î¼
2
.
(8.37)
(a)
What is the potential V (x) and its derivative V â€²(x) for these laws?
(b)
Using Eq. (8.30), show that for a t-distributed variable x we have
E
(
x2
x2 + Î¼
)
=
1
1 + Î¼.
(8.38)
2 A process for which the time evolution samples the entire set of possible values according to the stationary probability is
called ergodic. A discussion of the condition for ergodicity is beyond the scope of this book.

8.2 Stochastic Calculus
119
(c)
Write the Langevin equation for a Studentâ€™s t-distribution. What is the
Î¼ â†’âˆlimit of this equation?
(d)
Simulate your Langevin equation for Î¼ = 3, 20 time steps per unit time and
run the simulation for 20 000 units of time. Make a normalized histogram of
the sampled values of Xt and compare with the law for Î¼ = 3 given above.
(e)
Compared to the Gaussian process (Ornsteinâ€“Uhlenbeck), the Student
t-process has many more short excursions but the long excursions are much
longer than the Gaussian ones. Explain this behavior by comparing the
function V â€²(x) in the two cases. Describe their relative small |x| and large
|x| behavior.
8.2.6 The Fokkerâ€“Planck Equation
It is interesting to derive, from the Langevin equation Eq. (8.28), the so-called Fokkerâ€“
Planck equation that describes the dynamical evolution of the time dependent probability
density P(x,t) of the random variable Xt. The trick is to use Eq. (8.29) again, with f (x)
an arbitrary test function. Taking expectations, one ï¬nds
dE[f (Xt)] = E

f â€²(Xt)F(Xt)dt

+ 1
2E[f â€²â€²(Xt)]dt,
(8.39)
where we have used the fact that, in the ItË†o convention, E[f â€²(Xt)dBt] = 0. But by deï¬ni-
tion of P(x,t), one also has
E[f (Xt)] :=

f (x)P(x,t)dx.
(8.40)
Hence,

f (x)âˆ‚P(x,t)
âˆ‚t
dx =

f â€²(x)F(x)P(x,t)dx + 1
2

f â€²â€²(x)P(x,t)dx.
(8.41)
Integrating by parts the right-hand side leads to

f (x)âˆ‚P(x,t)
âˆ‚t
dx = âˆ’

f (x)âˆ‚F(x)P(x,t)
âˆ‚x
dx + 1
2

f (x)âˆ‚2P(x,t)
âˆ‚x2
dx.
(8.42)
Since this equation holds for an arbitrary test function f (x), it must be that
âˆ‚P(x,t)
âˆ‚t
= âˆ’âˆ‚F(x)P(x,t)
âˆ‚x
+ Ïƒ 2
2
âˆ‚2P(x,t)
âˆ‚x2
,
(8.43)
which is called the Fokkerâ€“Planck equation. We have reintroduced an arbitrary value
of Ïƒ here, to make the equation more general. One can easily check that when F(x) =
âˆ’V â€²(x)/2, the stationary state of this equation, such that the left hand side is zero, is
P(x) = Zâˆ’1 exp[âˆ’V (x)/Ïƒ 2],
(8.44)
as expected from the previous section.

120
Addition of Random Variables and Brownian Motion
Bibliographical Notes
â€¢ A general introduction to probability theory:
â€“ W. Feller. An Introduction to Probability Theory and Its Applications. Wiley, 1968.
â€¢ For introductory textbooks on the Langevin and Fokkerâ€“Planck equation, see
â€“ C. W. Gardiner. Handbook of Stochastic Methods for Physics, Chemistry and the
Natural Sciences, volume 13 of Springer Series in Synergetics. Springer-Verlag,
Berlin, 2004,
â€“ N. V. Kampen. Stochastic Processes in Physics and Chemistry. North Holland, Ams-
terdam, 2007.
â€¢ The central limit theorem:
â€“ P. LÂ´evy. ThÂ´eorie de lâ€™addition des variables alÂ´eatoires. Gauthier Villars, Paris,
1937â€“1954,
â€“ B. V. Gnedenko and A. N. Kolmogorov. Limit Distributions for Sums of Independent
Random Variables. Addison-Wesley Publishing Co., Reading, Mass., London, Don
Mills., Ont., 1968.
â€¢ For a physicist discussion of the central limit theorem, see
â€“ J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pric-
ing: From Statistical Physics to Risk Management. Cambridge University Press,
Cambridge, 2nd edition, 2003.

9
Dyson Brownian Motion
In this chapter we would like to start our investigation of the addition of random matrices,
which will lead to the theory of so-called â€œfree matricesâ€. This topic has attracted substan-
tial interest in recent years and will be covered in the next chapters.
We will start by studying how a ï¬xed large matrix (random or not) is modiï¬ed when
one adds a Wigner matrix. The elements of a Wigner matrix are Gaussian random numbers
and, as we saw in the previous chapter, each of them can be written as a sum of Gaussian
numbers with smaller variance. By pushing this reasoning to the limit we can write the
addition of a Wigner matrix as a continuous process of addition of inï¬nitesimal Wigner
matrices. Such a matrix Brownian motion process, viewed through the lens of eigenvalues
and eigenvectors, is called a Dyson Brownian motion (dbm) after the physicist Freeman
Dyson who ï¬rst introduced it in 1962.
9.1 Dyson Brownian Motion I: Perturbation Theory
9.1.1 Perturbation Theory: A Short Primer
We begin by recalling how perturbation theory works for eigenvalues and eigenvectors.
This is a standard topic in elementary quantum mechanics, but is not necessarily well
known in other circles. Let us consider a matrix H = H0 + ÏµH1, where H0 is a real
symmetric matrix whose eigenvalues and eigenvectors are assumed to be known, and H1 is
a real symmetric matrix that gives the perturbation, with Ïµ a small parameter. (In quantum
mechanics, H0 and H1 are complex Hermitian, but the ï¬nal equations below are the same
in both cases.)
Suppose Î»i,0, 1 â‰¤i â‰¤N, are the eigenvalues of H0 and vi,0, 1 â‰¤i â‰¤N, are the
corresponding eigenvectors. We assume that the perturbed eigenvalues and eigenvectors
are given by the series expansion (in Ïµ):
Î»i = Î»i,0 +
âˆ

k=1
ÏµkÎ»i,k,
vi = vi,0 +
âˆ

k=1
Ïµkvi,k,
(9.1)
with the constraint that
âˆ¥viâˆ¥= âˆ¥vi,0âˆ¥= 1,
1 â‰¤i â‰¤N.
(9.2)
121

122
Dyson Brownian Motion
Since the quantity âˆ¥viâˆ¥is constant, its ï¬rst order variation with respect to Ïµ must be zero.
This constraint gives that vi,1 âŠ¥vi,0.
We assume that the Î»i,0 are all different, i.e. we consider non-degenerate perturbation
theory. Then, plugging (9.1) into
Hvi = Î»ivi
(9.3)
and matching the left and right hand side term by term in powers of Ïµ, one obtains
Î»i = Î»i,0 + Ïµ(H1)ii + Ïµ2
N

j=1
ji
|(H1)ij|2
Î»i,0 âˆ’Î»j,0
+ O(Ïµ3),
(9.4)
where (H1)ij := vT
j,0H1vi,0, and
vi = vi,0 + Ïµ
N

j=1
ji
(H1)ij
Î»i,0 âˆ’Î»j,0
vj,0 + O(Ïµ2).
(9.5)
Notice that the ï¬rst order correction to vi is indeed perpendicular to vi,0 as it does not have
a component in that direction.
9.1.2 A Stochastic Differential Equation for Eigenvalues
Next we use the above formulas to derive the so-called Dyson Brownian motion (dbm),
which gives the evolution of eigenvalues of a random matrix plus a Wigner ensemble whose
variance grows linearly with time. Let M0 be the initial matrix (random or not), and X1 be a
unit Wigner matrix that is independent of M0. Then we study, using (9.4), the eigenvalues of
M = M0 +
âˆš
dt X1,
(9.6)
where dt is a small quantity which will be interpreted as a differential time step.
The derivation of the sde for eigenvalues is much simpler if we use the rotational
invariance of the Wigner ensemble. The matrix X1 has the same law in any basis, we
therefore choose to express it in the diagonal basis of M0. In order to do so we must work
with the exact rotationally invariant Wigner ensemble where the diagonal variance is twice
the off-diagonal variance.
First, for the ï¬rst order term (in terms of Ïµ), we have
(X1)ii := vT
i,0X1vi,0 âˆ¼N

0, 2
N

.
(9.7)
Note that the (X1)ii are independent for different iâ€™s.
Then we study the second order term. We have
(X1)ji := vT
j,0X1vi,0 âˆ¼N

0, 1
N

.
(9.8)

9.1 Dyson Brownian Motion I: Perturbation Theory
123
0.0
0.2
0.4
0.6
0.8
1.0
t
âˆ’2
âˆ’1
0
1
2
lt
Figure 9.1 A simulation of dbm for an N = 25 matrix starting for a Wigner matrix with Ïƒ 2 = 1/4
and evolving for one unit of time.
So |(X1)ji|2 is a random variable with mean 1/N and with ï¬‚uctuations around the mean
also of order 1/N. As in Section 8.2.2, one can argue that these ï¬‚uctuations are negligible
when integrated over time in the limit dt â†’0. In other words, |(X1)ji|2 can be treated as
deterministic.
Now using (9.4), we get that
dÎ»i =
2
2
Î²N dBi + 1
N
N

j=1
ji
dt
Î»i âˆ’Î»j
,
(9.9)
where dBi denotes a Brownian increment that comes from the (X1)ii term, and we have
added a factor Î² for completeness (equal to 1 for real symmetric matrices). This is the
fundamental evolution equation for eigenvalues in a ï¬ctitious time that describes how much
of a Wigner matrix one progressively adds to the â€œinitialâ€ matrix M0 (see Fig. 9.1). The
astute reader will probably have recognized in the second term a Coulomb force deriving
from the logarithmic Coulomb potential log |Î»i âˆ’Î»j| encountered in Chapter 5.
One can also derive a similar process for the eigenvectors that we give here for Î² = 1:
dvi =
1
âˆš
N
N

j=1
ji
dBij
Î»i âˆ’Î»j
vj âˆ’1
2N
N

j=1
ji
dt
(Î»i âˆ’Î»j)2 vi,
(9.10)
where dBij = dBji (i  j) is a symmetric collection of Brownian motions, independent of
each other and of the {dBi} above.
The formulas (9.9) and (9.10) give the Dyson Brownian motion for the stochastic evo-
lution of the eigenvalues and eigenvectors of matrices of the form

124
Dyson Brownian Motion
M = M0 + Xt,
(9.11)
where M0 is some initial matrix and Xt is an independent Wigner ensemble with parameter
Ïƒ 2 = t. We will call the above matrix process a matrix Dyson Brownian motion.
In our study of large random matrices, we will be interested in the dbm when N is large,
but actually the dbm is well deï¬ned for any N. As for the ItË†o lemma, we used the fact
that the Gaussian process can be divided into inï¬nitesimal increments and that perturbation
theory becomes exact at that scale. We made no assumption about the size of N. We did
need a rotationally invariant Gaussian process so the diagonal variance must be twice the
off-diagonal one. In the most extreme example of N = 1, the eigenvalue of a 1 Ã— 1 matrix
is just the value of its only element. Under dbm it simply undergoes a standard Brownian
motion with a variance of 2 per unit time.
Exercise 9.1.1
Variance as a function of time under dbm
Consider the Dyson Brownian motion for a ï¬nite N matrix:
dÎ»i =
1
2
N dBi + 1
N
N

j=1
ji
dt
Î»i âˆ’Î»j
(9.12)
and the function F({Î»i}) that computes the second moment:
F({Î»i}) = 1
N
N

i=1
Î»2
i .
(9.13)
(a)
Write down the stochastic process for F({Î»i}) using the ItË†o vectorial formula
(8.17). In the case at hand F does not depend explicitly on time and Ïƒ 2
i =
2/N. You will need to use the following identity:
2
N

i,j=1
ji
Î»i
Î»i âˆ’Î»j
=
N

i,j=1
ji
Î»i âˆ’Î»j
Î»i âˆ’Î»j
= N(N âˆ’1).
(9.14)
(b)
Take the expectation value of your equation and show that F(t)
:=
E[F({Î»i(t)})] follows
F(t) = F(0) + N + 1
N
t.
(9.15)
Do not assume that N is large.
9.2 Dyson Brownian Motion II: ItË†o Calculus
Another way to derive the Dyson Brownian motion for the eigenvalues is to consider the
matrix Brownian motion (9.11) as a Brownian motion of all the elements of the matrix X.
We have to treat the diagonal and off-diagonal elements separately because we want to use

9.2 Dyson Brownian Motion II: ItË†o Calculus
125
the rotationally invariant Wigner matrix (goe) with diagonal variance equal to twice the
off-diagonal variance. Also, only half of the off-diagonal elements are independent (the
matrix X is symmetric). We have
dXkk =
1
2
N dBkk
and
dXkâ„“=
1
1
N dBkâ„“
for
k < â„“,
(9.16)
where dBkk and dBkâ„“are N and N(N âˆ’1)/2 independent unit Brownian motions.
Each eigenvalue Î»i is a (complicated) function of all the matrix elements of X. We can
use the vectorial form of ItË†oâ€™s lemma (8.17) to write a stochastic differential equation (SDE)
for Î»i(X):
dÎ»i =
N

k=1
âˆ‚Î»i
âˆ‚Xkk
1
2
N dBkk +
N

k=1
â„“=k+1
âˆ‚Î»i
âˆ‚Xkâ„“
1
1
N dBkâ„“+
N

k=1
âˆ‚2Î»i
âˆ‚X2
kk
dt
N +
N

k=1
â„“=k+1
âˆ‚2Î»i
âˆ‚X2
kâ„“
dt
2N .
(9.17)
The key is to be able to compute the following partial derivatives:
âˆ‚Î»i
âˆ‚Xkk
;
âˆ‚Î»i
âˆ‚Xkâ„“
;
âˆ‚2Î»i
âˆ‚X2
kk
;
âˆ‚2Î»i
âˆ‚X2
kâ„“
,
(9.18)
where k < â„“.
Since Xt is rotational invariant, we can rotate the basis such that X0 is diagonal:
X0 = diag(Î»1(0), . . . ,Î»N(0)).
(9.19)
In order to compute the partial derivatives above, we can consider adding one small element
to the matrix X0, and compute the corresponding change in eigenvalues. We ï¬rst perturb
diagonal elements and later we will deal with off-diagonal elements.
A shift of the kth diagonal entry of X0 by Î´Xkk affects Î»i with i = k in a linear fashion
but leaves all other eigenvalues unaffected:
Î»i â†’Î»i + Î´XkkÎ´ki.
(9.20)
Thus we have
âˆ‚Î»i
âˆ‚Xkk
= Î´ik;
âˆ‚2Î»i
âˆ‚X2
kk
= 0.
(9.21)
Next we discuss how a perturbation in an off-diagonal entry of X0 can affect the eigen-
values. A perturbation of the (kâ„“) entry by Î´Xkâ„“= Î´Xâ„“k entry leads to the following
matrix:
X0 + Î´X =
â›
âœâœâœâœâœâœâœâœâœâœâœâœâ
Î»1
...
Î»k
Î´Xkâ„“
...
Î´Xkâ„“
Î»â„“
...
Î»N
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(9.22)

126
Dyson Brownian Motion
Since this matrix is block diagonal (after a simple permutation), the eigenvalues in the Nâˆ’2
other 1 Ã— 1 blocks are not affected by the perturbation, so trivially
âˆ‚Î»i
âˆ‚Xkâ„“
= 0;
âˆ‚2Î»i
âˆ‚X2
kâ„“
= 0,
âˆ€i  k,â„“.
(9.23)
On the other hand, the eigenvalues of the block
 Î»k
Î´Xkâ„“
Î´Xkâ„“
Î»â„“

(9.24)
are modiï¬ed and and exact diagonalization gives
Î»Â± = Î»k + Î»â„“
2
Â± Î»k âˆ’Î»â„“
2
2
1 + 4(Î´Xkâ„“)2
(Î»k âˆ’Î»â„“)2 .
(9.25)
We can expand this result to second order in Î´Xkâ„“to ï¬nd
Î»k â†’Î»k + (Î´Xkâ„“)2
Î»k âˆ’Î»â„“
and
Î»â„“â†’Î»â„“+ (Î´Xkâ„“)2
Î»â„“âˆ’Î»k
.
(9.26)
We thus readily see that the ï¬rst partial derivative of Î»i with respect to any off-diagonal
element is zero:
âˆ‚Î»i
âˆ‚Xkâ„“
= 0 for k < â„“.
(9.27)
For the second derivative, on the other hand, we obtain
âˆ‚2Î»i
âˆ‚X2
kâ„“
=
2Î´ik
Î»i âˆ’Î»â„“
+
2Î´iâ„“
Î»i âˆ’Î»k
for k < â„“.
(9.28)
Of the two terms on the right hand side, the ï¬rst term exists only if â„“> i while the second
term is only present when k < i. So, for a given i, only N âˆ’1 terms of the form 2/(Î»i âˆ’Î»j)
are present (note that the problematic term i = j is absent). Putting everything back into
Eq. (9.17), we ï¬nd, with Î² = 1 here,
dÎ»i =
2
2
Î²N dBi + 1
N
N

j=1
ji
dt
Î»i âˆ’Î»j
,
(9.29)
where dBi are independent Brownian motions (the old dBkk for k = i). We have thus
precisely recovered Equation (9.9) using ItË†oâ€™s calculus.
9.3 The Dyson Brownian Motion for the Resolvent
9.3.1 A Burgersâ€™ Equation for the Stieltjes Transform
Consider a matrix Mt that undergoes a dbm starting from a matrix M0. At each time t, Mt
can be viewed as the sum of the matrix M0 and a Wigner matrix Xt of variance t:
Mt = M0 + Xt.
(9.30)

9.3 The Dyson Brownian Motion for the Resolvent
127
In order to characterize the spectrum of the matrix Mt, one should compute its Stieltjes
transform g(z), which is the expectation value of the large N limit of function gN(z),
deï¬ned by
gN (z,{Î»i}) := 1
N
N

i=1
1
z âˆ’Î»i
.
(9.31)
gN is thus a function of all eigenvalues {Î»i} that undergo dbm while z is just a constant
parameter. Since the eigenvalues evolve with time gN is really a function of both z and t.
We can use ItË†oâ€™s lemma to write a sde for gN (z,{Î»i}). The ingredients are, as usual now,
the following partial derivatives:
âˆ‚gN
âˆ‚Î»i
= 1
N
1
(z âˆ’Î»i)2 ;
âˆ‚2gN
âˆ‚Î»2
i
= 2
N
1
(z âˆ’Î»i)3 .
(9.32)
We can now apply ItË†oâ€™s lemma (8.17) using the dynamical equation for eigenvalues (9.9)
to ï¬nd
dgN = 1
N
1
2
N
N

i=1
dBi
(z âˆ’Î»i)2 + 1
N2
N

i,j=1
ji
dt
(z âˆ’Î»i)2(Î»i âˆ’Î»j) + 2
N2
N

i=1
dt
(z âˆ’Î»i)3 .
(9.33)
We now massage the second term to arrive at a symmetric form in i and j. In order to do
so, note that i and j are dummy indices that are summed over, so we can rename i â†’j
and vice versa and get the same expression. Adding the two versions and dividing by 2, we
get that this term is equal to
1
N2
N

i,j=1
ji
dt
(z âˆ’Î»i)2(Î»i âˆ’Î»j) =
1
2N2
N

i,j=1
ji
(
dt
(z âˆ’Î»i)2(Î»i âˆ’Î»j) +
dt
(z âˆ’Î»j)2(Î»j âˆ’Î»i)
)
= 1
2N2
N

i,j=1
ji
(2z âˆ’Î»i âˆ’Î»j)dt
(z âˆ’Î»i)2(z âˆ’Î»j)2 = 1
N2
N

i,j=1
ji
dt
(z âˆ’Î»i)(z âˆ’Î»j)2
= 1
N2
N

i,j=1
dt
(z âˆ’Î»i)(z âˆ’Î»j)2 âˆ’1
N2
N

i=1
dt
(z âˆ’Î»i)3 = âˆ’gN
âˆ‚gN
âˆ‚z dt âˆ’1
N2
N

i=1
dt
(z âˆ’Î»i)3 .
(9.34)
Note that very similar manipulations have been used in Section 5.2.2. Thus we have
dgN = 1
N
1
2
N
N

i=1
dBi
(z âˆ’Î»i)2 âˆ’gN
âˆ‚gN
âˆ‚z dt + 1
N2
N

i=1
dt
(z âˆ’Î»i)3
= 1
N
1
2
N
N

i=1
dBi
(z âˆ’Î»i)2 âˆ’gN
âˆ‚gN
âˆ‚z dt + 1
2N
âˆ‚2gN
âˆ‚z2 dt.
(9.35)

128
Dyson Brownian Motion
Taking now the expectation of this equation (such that the dBi term vanishes), we get
E[dgN(z)] = âˆ’E
(
gN(z)âˆ‚gN(z)
âˆ‚z
)
dt + 1
2N E
(âˆ‚2gN(z)
âˆ‚z2
)
dt.
(9.36)
This equation is exact for any N. We can now take the N â†’âˆlimit, where gN(z) â†’
gt(z). Using the fact that the Stieltjes transform is self-averaging, we get a pde for the time
dependent Stieltjes transform gt(z):
âˆ‚gt(z)
âˆ‚t
= âˆ’gt(z)âˆ‚gt(z)
âˆ‚z
.
(9.37)
Equation (9.37) is called the inviscid Burgersâ€™ equation. Such an equation can in fact
develop singularities, so it often needs to be regularized by a viscosity term, which is in
fact present for ï¬nite N: it is precisely the last term of Eq. (9.36). Equation (9.37) can be
exactly solved using the methods of characteristics. This will be the topic of Section 10.1
in the next chapter.
9.3.2 The Evolution of the Resolvent
Let us now consider the full matrix resolvent of Mt, deï¬ned as Gt(z) = (z1 âˆ’Mt)âˆ’1.
Clearly, the quantity gt(z) studied above is simply the trace of Gt(z), but Gt(z) also
contains information on the evolution of eigenvectors. Since each element of Gt depends
on all the elements of Mt, one can again use ItË†oâ€™s calculus to derive an evolution equation
for Gt(z). The calculation is more involved because one needs to carefully keep track of
all indices. In this technical section, we sketch the derivation of Dyson Brownian motion
for the resolvent and brieï¬‚y discuss the result, which will be used further in Section 10.1.
Since Mt = M0 + Xt, the ItË†o lemma gives
dGij(z) =
N

k,â„“=1
âˆ‚Gij
âˆ‚Mkâ„“
dXkâ„“+ 1
2
N

k,â„“,m,n=1
âˆ‚2Gij
âˆ‚Mkâ„“âˆ‚Mmn
d

Xkâ„“Xmn

,
(9.38)
where the last term denotes the covariation of Xkâ„“and Xmn, and we have considered Mkl
and Mlk to be independent variables following 100% correlated Brownian motions. Next,
we compute the derivatives
âˆ‚Gij
âˆ‚Mkâ„“
= 1
2

GikGjâ„“+ GjkGiâ„“

,
(9.39)
from which we deduce the second derivatives
âˆ‚2Gij
âˆ‚Mkâ„“âˆ‚Mmn
= 1
4

(GimGkn + GimGkn) Gjâ„“+ Â· Â· Â·

,
(9.40)
where we have not written the other GGG products obtained by applying Eq. (9.39) twice.
Now, using the properties of the Brownian noise, the quadratic covariation reads
d

Xkâ„“Xmn

= dt
N

Î´kmÎ´â„“n + Î´knÎ´â„“m

,
(9.41)
so that we get from (9.38) and taking into account symmetries:
dGij(z,t) =
N

k,â„“=1
GikGjâ„“dXkâ„“+ 1
N
N

k,â„“=1

GikGâ„“kGâ„“j + GikGkjGâ„“â„“

dt.
(9.42)

9.4 The Dyson Brownian Motion with a Potential
129
If we now take the average over the Brownian motions dXkâ„“, we ï¬nd the following
evolution for the average resolvent:
âˆ‚tE[Gt(z)] = Tr Gt(z) E[G2
t (z)] + 1
N E[G3
t (z)].
(9.43)
Now, one can notice that
G2(z,t) = âˆ’âˆ‚zG(z,t);
G3(z,t) = 1
2âˆ‚2
zzG(z,t),
(9.44)
which hold even before averaging. By sending N â†’âˆ, we then obtain the following
matrix pde for the resolvent:
âˆ‚tE[Gt(z)] = âˆ’gt(z) âˆ‚zE[Gt(z)],
with
E[G0(z)] = GM0(z).
(9.45)
Note that this equation is linear in Gt(z) once the Stieltjes transform gt(z) is known.
Taking the trace of Eq. (9.45) immediately leads back to the Burgersâ€™ equation (9.37) for
gt(z) itself, as expected.
9.4 The Dyson Brownian Motion with a Potential
9.4.1 A Modiï¬ed Langevin Equation for Eigenvalues
In this section, we modify Dyson Brownian motion by adding a potential such that the
stationary state of these interacting random walks coincides with the eigenvalue measure
of Î²-ensembles, namely
P({Î»i}) = Zâˆ’1
N exp
â§
âªâªâ¨
âªâªâ©
âˆ’Î²
2
â¡
â¢â¢â£
N

i=1
NV (Î»i) âˆ’
N

i,j=1
ji
log |Î»i âˆ’Î»j|
â¤
â¥â¥â¦
â«
âªâªâ¬
âªâªâ­
.
(9.46)
The general vectorial Langevin equation (8.36) leading to such an equilibrium with Ïƒ 2 =
2/N immediately gives us the following dbm in a potential V (Î»):
dÎ»k =
1
2
N dBk +
â›
âœâ1
N
N

â„“=1
â„“k
Î²
Î»k âˆ’Î»â„“
âˆ’Î²
2 V â€²(Î»k)
â
âŸâ dt,
(9.47)
which recovers Eq. (9.9) in the absence of a potential. See Figure 9.2 for an illustration.
Dyson Brownian motion in a potential has many applications. Numerically it can be used
to generate matrices for an arbitrary potential and an arbitrary value of Î², a task not obvious
a priori from the deï¬nition (9.46). Figure 9.2 shows a simulation of the matrix potential
studied in Section 5.3.3. Note that dbm generates the correct density of eigenvalues; it also
generates the proper statistics for the joint distribution of eigenvalues.
It is interesting to see how Burgersâ€™ equation for the Stieltjes transform, Eq. (9.37), is
changed in the case where V (Î») = Î»2/2, i.e. in the standard goe case Î² = 1. Redoing
the steps leading to Eq. (9.36) with the extra V â€² term in the right hand side of Eq. (9.47)
modiï¬es the Burgersâ€™ equation into

130
Dyson Brownian Motion
0
5
10
t
âˆ’1
0
1
lt
âˆ’1
0
1
l
0.0
0.1
0.2
0.3
0.4
r(l)
Figure 9.2 (left) A simulation of dbm with a potential for an N = 25 matrix starting from a Wigner
matrix with Ïƒ 2 = 1/10 and evolving within the potential V (x) = x2
2 + x4
4 for 10 units of time. Note
that the steady state is quickly reached (within one or two units of time). (right) Histogram of the
eigenvalues for the same process for N = 400 and 200 discrete steps per unit time. The histogram
is over all matrices from time 3 to 10 (560 000 points). The agreement with the theoretical density
(Eq. (5.58)) is very good.
âˆ‚gt(z)
âˆ‚t
= âˆ’gt(z)âˆ‚gt(z)
âˆ‚z
+ 1
2
âˆ‚(zgt(z))
âˆ‚z
.
(9.48)
The solution to this equation will be discussed in the next chapter.
More theoretically, dbm can be used in proofs of local universality, which is one of the
most important results in random matrix theory. Local universality is the concept that many
properties of the joint law of eigenvalues do not depend on the speciï¬cs of the random
matrix in question, provided one looks at them on a scale Î¾ comparable to the average
distance between eigenvalues, i.e. on scales Nâˆ’1 â‰²Î¾ â‰ª1. Many such properties arise
from the logarithmic eigenvalue repulsion and indeed depend only on the symmetry class
(Î²) of the model.
Another useful property of dbm is its speed of convergence to the steady state. With time
normalized as in Eq. (9.47), global properties (such as the density of eigenvalues) converge
in a time of order 1, as we discuss in the next subsection. Local properties on the other hand
(e.g. eigenvalue spacing) converge much faster, in a time of order 1/N, i.e. as soon as the
eigenvalues have â€œcollidedâ€ a few times with one another.1
1
The time needed for two Brownian motions a distance d apart to meet for the ï¬rst time is of order d2/Ïƒ2. In our
case d = 1/N (the typical distance between two eigenvalues) and Ïƒ2 = 2/N. The typical collision time is therefore (2N)âˆ’1.
Note however that eigenvalues actually never cross under Eq. (9.47), but the corresponding eigenvectors strongly mix when
such quasi-collisions occur.

9.4 The Dyson Brownian Motion with a Potential
131
Exercise 9.4.1
Moments under dbm with a potential
Consider the moments of the eigenvalues as a function of time:
Fk(t) = 1
N
N

i=1
Î»k
i (t),
(9.49)
for eigenvalues undergoing dbm under a potential V (x), Eq. (9.47), in the
orthogonal case Î² = 1. In this exercise you will need to show (and to use)
the following identity:
2
N

i,j=1
ji
Î»k
i
Î»i âˆ’Î»j
=
N

i,j=1
ji
Î»k
i âˆ’Î»k
j
Î»i âˆ’Î»j
=
N

i,j=1
ji
k

â„“=0
Î»â„“
i Î»kâˆ’â„“
j
.
(9.50)
(a)
Using ItË†o calculus, write a SDE for F2(t).
(b)
By taking the expectation value of your equation, show that
d
dt E[F2(t)] = 1 âˆ’E
-
1
N
N

i=1
Î»i(t)V â€²(Î»i(t))
.
+ 1
N .
(9.51)
(c)
In the Wigner case, V â€²(x) = x, ï¬nd the steady-state value of E[F2(t)] for any
ï¬nite N.
(d)
For a random matrix X drawn from a generic potential V (x), show that in the
large N limit, we have
Ï„

V â€²(X)X

= 1,
(9.52)
where Ï„ is the expectation value of the normalized trace deï¬ned by (2.1).
(e)
Show that this equation is consistent with Ï„(W) = 1 for a Wishart matrix
whose potential is given by Eq. (5.4).
(f)
In the large N limit, ï¬nd a general expression for Ï„[V â€²(X)Xk] by writing the
steady-state equation for E[Fk+1(t)]; you can neglect the ItË†o term. The ï¬rst
two should be given by
Ï„[V â€²(X)X2] = 2Ï„[X]
and
Ï„[V â€²(X)X3] = 2Ï„[X2] + Ï„[X]2.
(9.53)
(g)
In the unit Wigner case V â€²(x) = x, show that your relation in (f) is equivalent
to the Catalan number inductive relation (3.27), with Ï„(X2m) = Cm and
Ï„(X2m+1) = 0.
9.4.2 The Fokkerâ€“Planck Equation for DBM
From the Langevin equation, Eq. (9.47), one can derive the Fokkerâ€“Planck equation
describing the time evolution of the joint distribution of eigenvalues, P({Î»i},t). It reads

132
Dyson Brownian Motion
âˆ‚P
âˆ‚t = 1
N
N

i=1
âˆ‚
âˆ‚Î»i
( âˆ‚P
âˆ‚Î»i
âˆ’FiP
)
,
(9.54)
where we use P as an abbreviation for P({Î»i},t), and, for a quadratic conï¬ning potential
V (Î») = Î»2/2, a generalized force given by
Fi := Î²
N

j=1
ji
1
Î»i âˆ’Î»j
âˆ’NÎ²Î»i
2
.
(9.55)
The trick now is to introduce an auxiliary function W({Î»i},t), deï¬ned as
P({Î»i},t) := exp
â¡
â¢â¢â¢â£
Î²
4
N

i,j=1
ji
log |Î»i âˆ’Î»j| âˆ’Î²N
8
N

i=1
Î»2
i
â¤
â¥â¥â¥â¦W({Î»i},t).
(9.56)
Then after a little work, one ï¬nds the following evolution equation for W:2
âˆ‚W
âˆ‚t
= 1
N
N

i=1
-
âˆ‚2W
âˆ‚Î»2
i
âˆ’ViW
.
,
(9.58)
with
Vi({Î»j}) := Î²2N2
16
Î»2
i âˆ’Î²(2 âˆ’Î²)
4
N

j=1
ji
1
(Î»i âˆ’Î»j)2 âˆ’NÎ²
4

1 + Î²(N âˆ’1)
2

. (9.59)
Looking for W
 such that
âˆ‚W
âˆ‚t
= âˆ’
W
,
(9.60)
one ï¬nds that W
 is the solution of the following eigenvalue problem:
2
N
N

i=1
-
âˆ’1
2
âˆ‚2W
âˆ‚Î»2
i
+ 1
2ViW
.
= 
W
.
(9.61)
One notices that Eq. (9.61) is a (real) Schrodinger equation for N interacting â€œparticlesâ€ in
a quadratic potential, with an interacting potential that depends on the inverse of the square
distance between particles. This is called the Calogero model, which happens to be exactly
soluble in one dimension, both classically and quantum mechanically. In particular, the
whole spectrum of this Hamiltonian is known, and given by3

(n1,n2, . . . ,nN) = Î²
2
â›
â
N

i=1
ni âˆ’N(N âˆ’1)
2
â
â ;
0 â‰¤n1 < n2 Â· Â· Â· < nN.
(9.62)
2 One has to use, along the line, the following identity:

ijk
1
Î»i âˆ’Î»j
1
Î»i âˆ’Î»k
â‰¡0.
(9.57)
3 Because W
({Î»i}) must vanish as two Î»â€™s coincide, one must choose the so-called fermionic branch of the spectrum.

9.5 Non-Intersecting Brownian Motions and the Karlinâ€“McGregor Formula
133
The smallest eigenvalue corresponds to n1 = 0,n2 = 1, . . . ,nN = N âˆ’1 and is such that

 â‰¡0. This corresponds to the equilibrium state of the Fokkerâ€“Planck equation:
W0({Î»i}) = exp
â¡
â¢â¢â¢â£
Î²
4
N

i,j=1
ji
log |Î»i âˆ’Î»j| âˆ’Î²N
8
N

i=1
Î»2
i
â¤
â¥â¥â¥â¦.
(9.63)
All other â€œexcited statesâ€ have positive 
â€™s, corresponding to exponentially decaying
modes (in time) of the Fokkerâ€“Planck equation. The smallest, non-zero value of 
 is such
that nN = N, all other values of ni being unchanged. Hence 
1 is equal to Î²/2.
In conclusion, we have explicitly shown that the equilibration time of the dbm in a
quadratic potential is equal to 2/Î². As announced at the end of the previous section, the
density of eigenvalues indeed converges in a time of order unity.
The case Î² = 2 is particularly simple, since the interaction term in Vi disappears
completely. We will use this result to show that, in the absence of a quadratic potential, the
time dependent joint distribution of eigenvalues, P({Î»i},t), can be expressed, for Î² = 2,
as a simple determinant: this is the so-called Karlinâ€“McGregor representation, see next
section.
9.5 Non-Intersecting Brownian Motions and the Karlinâ€“McGregor Formula
Let p(y,t|x) be the probability density that a Brownian motion starting at x at t = 0 is
at y at time t
p(y,t|x) =
1
âˆš
2Ï€t
exp

âˆ’(x âˆ’y)2
2t

,
(9.64)
where we set Ïƒ 2 = 1. Note that p(y,t|x) obeys the diffusion equation
âˆ‚p(y,t|x)
âˆ‚t
= 1
2
âˆ‚2p(y,t|x)
âˆ‚y2
.
(9.65)
We now consider N independent Brownian motions starting at x = (x1,x2, . . . xN)
at t = 0, with x1 > x2 > . . . > xN. The Karlinâ€“McGregor formula states that the
probability PKM(y,t|x) that these Brownian motions have reached positions y = (y1 >
y2 > . . . > yN) at time t without ever intersecting between 0 and t is given by the
following determinant:
PKM(y,t|x) =

p(y1,t|x1)
p(y1,t|x2)
. . .
p(y1,t|xN)
p(y2,t|x1)
p(y2,t|x2)
. . .
p(y2,t|xN)
...
...
...
...
p(yN,t|x1)
p(yN,t|x2)
. . .
p(yN,t|xN)

.
(9.66)
One can easily prove this by noting that the determinant involves sums of products of N
terms p(yi,t|xj), each product  involving one and only one yi. Each product  therefore
obeys the multidimensional diffusion equation:
âˆ‚
âˆ‚t = 1
2
N

i=1
âˆ‚2
âˆ‚y2
i
.
(9.67)

134
Dyson Brownian Motion
Being the sum of such products, PKM(y,t|x) also obeys the same diffusion equation, as it
should since we consider N independent Brownian motions:
âˆ‚PKM(y,t|x)
âˆ‚t
= 1
2
N

i=1
âˆ‚2PKM(y,t|x)
âˆ‚y2
i
,
(9.68)
The determinant structure also ensures that PKM(y,t|x) = 0 as soon as any two yâ€™s are
equal. Finally, because the xâ€™s and the yâ€™s are ordered, PKM(y,t = 0|x) obeys the correct
initial condition.
Note that the total survival probability P(t;x) :=

dyPKM(y,t|x) decreases with time,
since as soon as two Brownian motions meet, the corresponding trajectory is killed. In fact,
one can show that P(t;x) âˆ¼tâˆ’N(Nâˆ’1)/4 at large times.
Now, the probability P(y,t|x) that these N independent Brownian motions end at y at
time t conditional on the fact that the paths never ever intersect, i.e. for any time between
t = 0 and t = âˆ, turns out to be given by a very similar formula:
P(y,t|x) = (y)
(x)PKM(y,t|x),
(9.69)
where (x) is the Vandermonde determinant 
i<j (xj âˆ’xi) (and similarly for (y)).
What we want to show is that P(y,t|x) is the solution of the Fokkerâ€“Planck equation
for the Dyson Brownian motion, Eq. (9.54), with Î² = 2 and in the absence of any con-
ï¬ning potential. Indeed, as shown above, PKM(y,t|x) obeys the diffusion equation for N
independent Brownian motions with the annihilation boundary condition PKM(y,t|x) = 0
when yi = yj for any given pair i,j.
Now compare with the deï¬nition Eq. (9.56) of W for the Dyson Brownian motions
with Î² = 2 and without any conï¬ning potential:
P({Î»i},t) := exp
â¡
â¢â¢â¢â£
1
2
N

j=1
ji
log |Î»i âˆ’Î»j|
â¤
â¥â¥â¥â¦W({Î»i},t) â‰¡({Î»i})W({Î»i},t).
(9.70)
From Eq. (9.58) we see that in the present case W({Î»i},t) also obeys the diffusion equation
for N independent Brownian motions. Since P({Î»i},t) âˆ¼|Î»iâˆ’Î»j|2 when two eigenvalues
are close, we also see that W({Î»i},t) vanishes linearly whenever two eigenvalues meet,
and therefore obeys the same boundary conditions as PKM(y,t|x) with yi = Î»i.
The conclusion is therefore that the Dyson Brownian motion without external forces
is, for Î² = 2, equivalent to N Brownian motions constrained to never ever intersect.
Bibliographical Notes
â€¢ The Dyson Brownian motion was introduced in
â€“ F. J. Dyson. A Brownian-motion model for the eigenvalues of a random matrix.
Journal of Mathematical Physics, 3:1191â€“1198, 1962.
â€¢ It is not often discussed in books on random matrix theory. The subject is treated in
â€“ J. Baik, P. Deift, and T. Suidan. Combinatorics and Random Matrix Theory. Ameri-
can Mathematical Society, Providence, Rhode Island, 2016,
â€“ L. ErdËos and H.-T. Yau. A Dynamical Approach to Random Matrix Theory. American
Mathematical Society, Providence, Rhode Island, 2017,

9.5 Non-Intersecting Brownian Motions and the Karlinâ€“McGregor Formula
135
the latter discusses dbm with a potential beyond the Ornsteinâ€“Uhlenbeck model.
â€¢ On the Burgersâ€™ equation for the Stieltjes transform, see
â€“ L. C. G. Rodgers and Z. Shi. Interacting Brownian particles and the Wigner law.
Probability Theory and Related Fields, 95:555â€“570, 1993.
â€¢ On the evolution of the matrix resolvent, see
â€“ R. Allez, J. Bun, and J.-P. Bouchaud. The eigenvectors of Gaussian matrices with an
external source. preprint arXiv:1412.7108, 2014,
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1â€“109, 2017.
â€¢ On the universality of the local properties of eigenvalues, see e.g.
â€“ E. BrÂ´ezin and A. Zee. Universality of the correlations between eigenvalues of large
random matrices. Nuclear Physics B, 402(3):613â€“627, 1993,
â€“ L. ErdËos. Universality of Wigner random matrices: A survey of recent results. Rus-
sian Mathematical Surveys, 66(3):507, 2011.
â€¢ On the Calogero model, see
â€“ F. Calogero. Solution of the one-dimensional n-body problem with quadratic and/or
inversely quadratic pair potentials. Journal of Mathematical Physics, 12:419â€“436,
1971,
â€“ P. J. Forrester. Log Gases and Random Matrices. Princeton University Press,
Princeton, NJ, 2010,
â€“ A. P. Polychronakos. The physics and mathematics of Calogero particles. Journal of
Physics A: Mathematical General, 39(41):12793â€“12845, 2006,
and http://www.scholarpedia.org/article/Calogero-Moser system
â€¢ On the Karlinâ€“McGregor formula, see
â€“ S. Karlin and J. McGregor. Coincidence probabilities. Paciï¬c Journal of Mathemat-
ics, 9(4):1141â€“1164, 1959,
â€“ P.-G. de Gennes. Soluble model for ï¬brous structures with steric constraints. The
Journal of Chemical Physics, 48(5):2257â€“2259, 1968,
and for recent applications
â€“ T. GautiÂ´e, P. Le Doussal, S. N. Majumdar, and G. Schehr. Non-crossing Brownian
paths and Dyson Brownian motion under a moving boundary. Journal of Statistical
Physics, 177(5):752â€“805, 2019,
and references therein.

10
Addition of Large Random Matrices
In this chapter we seek to understand how the eigenvalue density of the sum of two large
random matrices A and B can be obtained from their individual densities. In the case where,
say, A is a Wigner matrix X, the Dyson Brownian motion formalism of the previous chapter
allows us to swiftly answer that question. We will see that a particular transform of the
density of B, called the R-transform, appears naturally. We then show that the R-transform
appears in the more general context where the eigenbases of A and B are related by a
random rotation matrix O. In this case, one can construct a Fourier transform for matrices,
which allows us to deï¬ne the analog of the generating function for random variables. As
in the case of iid random variables, the logarithm of this matrix generating function is
additive when one adds two randomly rotated, large matrices. The derivative of this object
turns out to be the R-transform, leading to the central result of the present chapter (and
of the more abstract theory of free variables, see Chapter 11): the R-transform of the
sum of two randomly rotated, large matrices is equal to the sum of R-transforms of each
individual matrix.
10.1 Adding a Large Wigner Matrix to an Arbitrary Matrix
Let Mt = M0 + Xt be the sum of a large matrix M0 and a large Wigner matrix Xt, such
that the variance of each element grows as t. This deï¬nes a Dyson Brownian motion as
described in the previous chapter, see Eq. (9.11). We have shown in Section 9.3.1 that in
this case the Stieltjes transform gt(z) of Mt satisï¬es the Burgersâ€™ equation:
âˆ‚gt(z)
âˆ‚t
= âˆ’gt(z)âˆ‚gt(z)
âˆ‚z
,
(10.1)
with initial condition g0(z) := gM0(z). We now proceed to show that the solution of
this Burgersâ€™ equation can be simply expressed using an M0 dependent function: its R-
transform.
Using the so-called method of characteristics, one can show that
gt(z) = g0(z âˆ’tgt(z)).
(10.2)
136

10.1 Adding a Large Wigner Matrix to an Arbitrary Matrix
137
If the method of characteristics is unknown to the reader, one can verify that (10.2) indeed
satisï¬es Eq. (10.1) for any function g0(z). Indeed, let us compute âˆ‚tgt(z) and âˆ‚zgt(z) using
Eq. (10.2):
âˆ‚tgt(z) = gâ€²
0(z âˆ’tgt(z)) [âˆ’gt(z) âˆ’tâˆ‚tgt(z)] â‡’âˆ‚tgt(z) = âˆ’gt(z)gâ€²
0(z âˆ’tgt(z))
1 + tgâ€²
0(z âˆ’tgt(z)), (10.3)
and
âˆ‚zgt(z) = gâ€²
0(z âˆ’tgt(z))

1 âˆ’tâˆ‚zgt(z)

â‡’âˆ‚zgt(z) =
gâ€²
0(z âˆ’tgt(z))
1 + tgâ€²
0(z âˆ’tgt(z)),
(10.4)
such that Eq. (10.1) is indeed satisï¬ed.
Example: Suppose M0 = 0. Then we have g0(z) = zâˆ’1. Plugging into (10.2), we obtain
that
gt(z) =
1
z âˆ’tgt(z),
(10.5)
which is the self-consistent Eq. (2.35) in the Wigner case with Ïƒ 2 = t. Indeed, if we start
with the zero matrix, then Mt = Xt is just a Wigner with parameter Ïƒ 2 = t.
Back to the general case, we denote as zt(g) the inverse function1 of gt(z). Now ï¬x
g = gt(z) = g0(z âˆ’tg) and z = zt(g), we apply the function z0 to g and get
z0(g) = z âˆ’tg = zt(g) âˆ’tg,
zt(g) = z0(g) + tg.
(10.6)
The inverse of the Stieltjes transform of Mt is given by the inverse of that of M0 plus a
simple shift tg. If we know g0(z) we can compute its inverse z0(g) and thus easily obtain
zt(g), which after inversion hopefully recovers gt(z).
Example: Suppose M0 is a Wigner matrix with variance Ïƒ 2. We ï¬rst want to compute
the inverse of g0(z); to do so we use the fact that g0(z) satisï¬es Eq. (2.35), and we get that
z0(g) = Ïƒ 2g + 1
g .
(10.7)
Then, by (10.6), we get that
zt(g) = z0(g) + tg =

Ïƒ 2 + t

g + 1
g,
(10.8)
which is the inverse Stieltjes transform for Wigner matrices with variance Ïƒ 2 + t. In other
words gt(z) satisï¬es the Wigner equation (2.35) with Ïƒ 2 replaced by Ïƒ 2 + t. This result is
not surprising, each element of the sum of two Wigner matrices is just the sum of Gaussian
random variables. So Mt is itself a Wigner matrix with the sum of the variances as its
variance.
1 We will discuss in Section 10.4 the invertibility of the function g(z).

138
Addition of Large Random Matrices
We can now tackle the more general case when the initial matrix is not necessarily
Wigner. Call B = Mt and A = M0. Then by (10.6), we get
zB(g) = zA(g) + tg = zA(g) + zXt (g) âˆ’1
g .
(10.9)
We now deï¬ne the R-transform as
R(g) := z(g) âˆ’1
g .
(10.10)
Note that the R-transform of a Wigner matrix of variance t is simply given by
RX(g) = tg.
(10.11)
This deï¬nition allows us to rewrite Eq. (10.9) above as a nice additive relation between
R-transforms:
RB(g) = RA(g) + RXt (g).
(10.12)
In the next section we will generalize this law of addition to (large) matrices X that are
not necessarily Wigner. The R-transform will prove to be a very powerful tool to study
large random matrices. Some of its properties are left to be derived in Exercises 10.1.1 and
10.1.2 and will be further discussed in Chapter 15. We ï¬nish this section by computing
the R-transform of a white Wishart matrix. Remember that its Stieltjes transform satisï¬es
Eq. (4.37), i.e.
qzg2 âˆ’(z âˆ’1 + q)g + 1 = 0,
(10.13)
which can be written in terms of the inverse function z(g):
z(g) =
1
1 âˆ’qg + 1
g .
(10.14)
From which we can read off the R-transform:
RW(g) =
1
1 âˆ’qg .
(10.15)
Exercise 10.1.1
Taylor series for the R-transform
Let g(z) be the Stieltjes transform of a random matrix M:
g(z) = Ï„

(z1 âˆ’M)âˆ’1
=

supp{Ï}
Ï(Î»)dÎ»
z âˆ’Î» .
(10.16)
We saw that the power series of g(z) around z = âˆis given by the moments of
M (mn := Ï„(Mn)):
g(z) =
âˆ

n=0
mn
zn+1 with m0 â‰¡1.
(10.17)

10.1 Adding a Large Wigner Matrix to an Arbitrary Matrix
139
Call z(g) the functional inverse of g(z) which is well deï¬ned in a neighborhood
of g = 0. And deï¬ne R(g) as
R(g) = z(g) âˆ’1/g.
(10.18)
(a)
By writing the power series of R(g) near zero, show that R(g) is regular at
zero and that R(0) = m1. Therefore the power series of R(g) starts at g0:
R(g) =
âˆ

n=1
Îºngnâˆ’1.
(10.19)
(b)
Now assume m1 = Îº1 = 0 and compute Îº2, Îº3 and Îº4 as a function of m2, m3
and m4 in that case.
Exercise 10.1.2
Scaling of the R-transform
Using your answer from Exercise 2.3.1: If A is a random matrix drawn from
a well-behaved ensemble with Stieltjes transform gA(z) and R-transform RA(g),
what is the R-transform of the random matrices Î±A and A + b1 where Î± and b
are non-zero real numbers?
Exercise 10.1.3
Sum of symmetric orthogonal and Wigner matrices
Consider as in Exercise 1.2.4 a random symmetric orthogonal matrix M and a
Wigner matrix X of variance Ïƒ 2. We are interested in the spectrum of their sum
E = M + X.
(a)
Given that the eigenvalues of M are Â±1 and that in the large N limit each
eigenvalue appears with weight 1
2, write the limiting Stieltjes transform gM(z).
(b)
E can be thought of as undergoing Dyson Brownian motion starting at E(0) =
M and reaching the desired E at t = Ïƒ 2. Use Eq. (10.2) to write an equation
for gE(z). This will be a cubic equation in g.
(c)
You can obtain the same equation using the inverse function zM(g) of your
answer in (a). Show that
zM(g) = 1 +
*
1 âˆ’4g2
2g
,
(10.20)
where one had to pick the root that makes z(g) âˆ¼1/g near g = 0.
(d)
Using Eq. (10.6), write zE(g) and invert this relation to obtain an equation for
gE(z). You should recover the same equation as in (b).
(e)
Eigenvalues of E will be located where your equation admits non-real
solutions for real z. First look at z = 0; the equation becomes quadratic after
factorizing a trivial root. Find a criterion for Ïƒ 2 such that the equation admits
non-real solutions. Compare with your answer in Exercise 1.2.4 (b).
(f)
At Ïƒ 2 = 1, the equation is still cubic but is somewhat simpler. A real cubic
equation of the form ax3 + bx2 + cx + d = 0 will have non-real solutions
iff  < 0 where  = 18abcd âˆ’4b3d + b2c2 âˆ’4ac3 âˆ’27a2d2. Using this

140
Addition of Large Random Matrices
criterion show that for Ïƒ 2 = 1 the edges of the eigenvalue spectrum are given
by Î» = Â±3
âˆš
3/2 â‰ˆÂ±2.60.
(g)
Again at Ïƒ 2 = 1, the solution near g(0) = 0 can be expanded in fractional
powers of z. Show that we have
g(z) = z1/3 + O(z), which implies Ï(x) =
âˆš
3
2
3*
|x|,
(10.21)
for x near zero.
(h)
For Ïƒ 2 = 1/2,1 and 2, solve numerically the cubic equation for gE(z) for
z = x real and plot the density of eigenvalues Ï(x) = | Im(gE(x))|/Ï€ for one
of the complex roots if present.
10.2 Generalization to Non-Wigner Matrices
10.2.1 Set-Up
In the previous section, we derived a formula for the Stieltjes transform of the sum of a
Wigner matrix and an arbitrary matrix. We would like to ï¬nd a generalization of this result
to a larger class of matrices.
Take two N Ã— N matrices: A, with eigenvalues {Î»i}1â‰¤iâ‰¤N and eigenvectors {vi}1â‰¤iâ‰¤N,
and B, with eigenvalues {Î¼i}1â‰¤iâ‰¤N and eigenvectors {ui}1â‰¤iâ‰¤N. Then the eigenvalues of
C = B + A will in general depend in a complicated way on the overlaps between the
eigenvectors of B and the eigenvectors of A. In the trivial case where vi = ui for all i, we
have that the eigenvalues of B + A are simply given by Î½i = Î»i + Î¼i. However, this is
neither generic nor very interesting.
One important property of Wigner matrices is that their eigenvectors are Haar dis-
tributed, that is, the matrix of eigenvectors is distributed uniformly in the group O(N) and
each eigenvector is uniformly distributed on the unit sphere SNâˆ’1. Thus, when N is large,
it is very unlikely that any one of them will have a signiï¬cant overlap with the eigenvectors
of B. This is the property that we want to keep in our generalization. We will study what
happens for general matrices B and A when their eigenvectors are random with respect
to one another. We will deï¬ne this relative randomness notion (called â€œfreenessâ€) more
precisely in the next chapter. Here, to ensure the randomness of the eigenvectors, we will
apply a random rotation to the matrix A and deï¬ne the free addition as
C = B + OAOT,
(10.22)
where O is a Haar distributed random orthogonal matrix. Then it is easy to see that
OAOT is rotational invariant since Oâ€²O is also Haar distributed for any ï¬xed Oâ€² âˆˆO(N).

10.2 Generalization to Non-Wigner Matrices
141
10.2.2 Matrix Fourier Transform
We saw in Section 8.1 that the function HX(t) = log E exp(itX) is additive when one adds
independent scalar variables. When X is a matrix, it is plausible that t should also be a
matrix T, but in the end we need to take the exponential of a scalar, so a possible candidate
would be
I(X,T) :=
6
exp
N
2 Tr TOXOT
7
O
.
(10.23)
The notation âŸ¨Â·âŸ©O means that we average over all orthogonal matrices O (with a ï¬‚at weight)
normalized such that âŸ¨1âŸ©O = 1. This deï¬nes the Haar measure on the group of orthogonal
matrices. Equation (10.23) deï¬nes the so-called Harish-Chandraâ€“Itzyksonâ€“Zuber (hciz)
integral.2
Note that by deï¬nition, I(O1XOT
1,T) = I(X,O1TOT
1) = I(X,T) for an
arbitrary rotation matrix O1. This means that I(X,T) only depends on the eigenvalues of
X and T.
Now consider C = B+O1AOT
1 with a random O1. For large matrix sizes, the eigenvalue
spectrum of C will turn out not to depend on the speciï¬c choice of O1, provided it is chosen
according to the Haar measure. Therefore, one can average I(C,T) over O1 and obtain
I(C,T) =
6
exp
N
2 Tr TO(B + O1AOT
1)OT
7
O,O1
= I(B,T)I(A,T),
(10.25)
where we have used that OO1 = Oâ€² is a random rotation independent from O itself. Hence
we conclude that log I is additive in this case, as is the logarithm of the characteristic
function in the scalar case.
For a general matrix T, the hciz integral is quite complicated, as will be further dis-
cussed in Section 10.5. Fortunately, for our purpose we can choose the â€œFourierâ€ matrix T
to be rank-1 and in this case the integral can be computed. A symmetric rank-1 matrix can
be written as
T = t vvT,
(10.26)
where t is the eigenvalue and v is a unit vector. We will show that the large N behavior of
I(T,B) is given, in this case, by
I(T,B) â‰ˆexp
N
2 HB(t)

,
(10.27)
for some function HB(t) that depends on the particular matrix B.
2 The hciz can be deï¬ned with an integral over orthogonal, unitary or symplectic matrices. In the general case it is deï¬ned as
IÎ²(X,T) :=
6
exp
 NÎ²
2
Tr XOTOâ€ 
7
O
,
(10.24)
with beta equal to 1, 2 or 4 and O is averaged over the corresponding group. The unitary Î² = 2 case is the most often studied,
for which some explicit results are available.

142
Addition of Large Random Matrices
More formally we deï¬ne
HB(t) = lim
Nâ†’âˆ
2
N log
6
exp
tN
2 Tr vvT OBOT
7
O
.
(10.28)
If C = B + A where A is randomly rotated with respect to B, the precise statement is that
HC(t) = HB(t) + HA(t),
(10.29)
i.e. the function H is additive. We now need to relate this function to the R-transform
encountered in the previous section.
10.3 The Rank-1 HCIZ Integral
To get a useful theory, we need to have a concrete expression for this function HB. Without
loss of generality, we can assume B is diagonal (in fact, we can diagonalize B and absorb
the eigenmatrix into the orthogonal matrix O we integrate over). Moreover, for simplicity
we assume that t > 0. Then OT TO can be regarded as proportional to a random projector:
OT TO = ÏˆÏˆ T,
(10.30)
with âˆ¥Ïˆâˆ¥2 = t and Ïˆ/âˆ¥Ïˆâˆ¥uniformly distributed on the unit sphere. Then we make a
change of variable Ïˆ â†’Ïˆ/
âˆš
N, and calculate
Zt(B) =

dNÏˆ
(2Ï€)N/2 Î´

âˆ¥Ïˆâˆ¥2 âˆ’Nt

exp
1
2Ïˆ T BÏˆ

,
(10.31)
where we have added a factor of (2Ï€)âˆ’N/2 for later convenience. Because Zt(B) is not
properly normalized (i.e. Zt(0)  1), we will need to normalize it to compute I(T,B):
6
exp
N
2 Tr TOBOT
7
O
= Zt(B)
Zt(0) .
(10.32)
10.3.1 A Saddle Point Calculation
We can now express the Dirac delta as an integral over the imaginary axis:
Î´(x) =
 âˆ
âˆ’âˆ
eâˆ’izx
2Ï€ dz =
 iâˆ
âˆ’iâˆ
eâˆ’zx/2
4iÏ€ dz.
Now let  be a parameter larger than the maximum eigenvalue of B:  > Î»max(B). We
introduce the factor
1 = exp

âˆ’

âˆ¥Ïˆâˆ¥2 âˆ’Nt

2

,
since âˆ¥Ïˆâˆ¥2 = Nt. Then, absorbing  into z, we get that

10.3 The Rank-1 HCIZ Integral
143
Zt(B) =
 +iâˆ
âˆ’iâˆ
dz
4Ï€

dNÏˆ
(2Ï€)N/2 exp

âˆ’1
2Ïˆ T (z âˆ’B)Ïˆ + Nzt
2

.
(10.33)
We can now perform the Gaussian integral over the vector Ïˆ:
Zt(B) =
 +iâˆ
âˆ’iâˆ
dz
4Ï€ det (z âˆ’B)âˆ’1/2 exp
Nzt
2

=
 +iâˆ
âˆ’iâˆ
dz
4Ï€ exp
-
N
2

zt âˆ’1
N

k
log(z âˆ’Î»k(B))
.
,
(10.34)
where Î»k(B), 1 â‰¤k â‰¤N, are the eigenvalues of B. Then we denote
Ft(z,B) := zt âˆ’1
N

k
log(z âˆ’Î»k(B)).
(10.35)
The integral in (10.34) is oscillatory, and by the stationary phase approximation (see
Appendix A.1), it is dominated by the point where
âˆ‚zFt(z,B) = 0 â‡’t âˆ’1
N

k
1
z âˆ’Î»k(B) = t âˆ’gB
N(z) = 0.
(10.36)
If gB
N(z) can be inverted then we can express z as z(t). For x > Î»max, gB
N(x) is mono-
tonically decreasing and thus invertible. So for t < gB
N(Î»max), a unique z(t) exists and
z(t) > Î»max (see Section 10.4). Since Ft(z,B) is analytic to the right of z = Î»max, we
can deform the contour to reach this point (see Fig. 10.1). Using the saddle point formula
(Eq. (A.3)), we have
Re z
Im z
L
(t)
lmax
L+ i
Lâˆ’i
Figure 10.1 Graphical representation of the integral Eq. (10.34) in the complex plane. The crosses
represent the eigenvalues of B and are singular points of the integrand. The integration is from âˆ’iâˆ
to  + iâˆwhere  > Î»max. The saddle point is at z = z(t) > Î»max. Since the integrand is analytic
right of Î»max, the integration path can be deformed to go through z(t).

144
Addition of Large Random Matrices
Zt(B) âˆ¼
âˆš
4Ï€/(4Ï€)
|Nâˆ‚2z F(z(t),B)|1/2 exp
-
N
2

z(t)t âˆ’1
N

k
log(z(t) âˆ’Î»k(B))
.
âˆ¼
1
2
/
NÏ€|gâ€²
B(z(t))|
exp
-
N
2

z(t)t âˆ’1
N

k
log(z(t) âˆ’Î»k(B))
.
.
(10.37)
For the case B = 0, we have gB(z) = zâˆ’1 â‡’z(t) = tâˆ’1, so we get
Zt(0) âˆ¼
1
2t
âˆš
NÏ€
exp
(N
2 (1 + log t)
)
.
(10.38)
In the large N limit, the prefactor in front of the exponential does not contribute to HB(t)
and we ï¬nally get
lim
Nâ†’âˆ
2
N log
6
exp
N
2 Tr TOBOT
7
O
= z(t)t âˆ’1 âˆ’log t âˆ’1
N

k
log(z(t) âˆ’Î»k(B)).
(10.39)
By the deï¬nition (10.28), we then get that
HB(t) = H(z(t),t),
H(z,t) := zt âˆ’1 âˆ’log t âˆ’1
N

k
log(z âˆ’Î»k(B)).
(10.40)
10.3.2 Recovering R-Transforms
We found an expression for HB(t) but in a form that is not easy to work with. But note
H(z,t) comes from a saddle point approximation and therefore its partial derivative with
respect to z is zero: âˆ‚zH(z(t),t) = 0. This allows us to compute a much simpler expression
for the derivative of HB(t):
dHB(t)
dt
= âˆ‚H
âˆ‚z (z(t),t)dz(t)
dt
+ âˆ‚H
âˆ‚t (z(t),t) = âˆ‚H
âˆ‚t (z(t),t) = z(t) âˆ’1
t â‰¡RB(t), (10.41)
where RB(t) denotes the R-transform deï¬ned in (10.10) (we have used the very deï¬nition of
z(t) from the previous section). Moreover, from its deï¬nition, we trivially have HB(0) = 0.
Hence we can write
HB(t) :=
 t
0
RB(x)dx.
(10.42)
We already know that H is additive. Thus its derivative, i.e. the R-transform, is also
additive:
RC(t) = RB(t) + RA(t),
(10.43)
as is the case when A is a Wigner matrix. This property is therefore valid as soon as A is
â€œfreeâ€ with respect to B, i.e. when the basis that diagonalizes A is a random rotation of the
basis that diagonalizes B.

10.4 Invertibility of the Stieltjes Transform
145
The discussion leading to Eq. (10.42) can be extended to the hciz integral (Eq. (10.23)),
when the rank of the matrix T is very small compared to N. In this case we get3
I(T,B) â‰ˆexp

N
2
n

i=1
HB(ti)

= exp
N
2 Tr HB(T)

,
(10.45)
where ti are the n non-zero eigenvalues of T and with the same HB(t) as above. When T
has rank-1 we recover that Tr HB(T) = HB(t), where t is the sole non-zero eigenvalue
of T.
The above formalism is based on the assumption that g(z) is invertible, which is gen-
erally only true when t = g(z) is small enough. This corresponds to the case where z is
sufï¬ciently large. Recall that the expansion of g(z) at large z has coefï¬cients given by the
moments of the random matrix by (2.22). On the other hand, the expansion of H(t) around
t = 0 will give coefï¬cients called the free cumulants of the random matrix, which are
important objects in the study of free probability, as we will show in the next chapter.
10.4 Invertibility of the Stieltjes Transform
The question of the invertibility of the Stieltjes transform arises often enough that it is
worth spending some time discussing it. In Section 10.1, we used the inverse of the lim-
iting Stieltjes transform g(z) to solve Burgersâ€™ equation, which led to the introduction
of the R-transform R(g) = z(g) âˆ’1/g. In Section 10.3.1 we invoked the invertibil-
ity of the discrete Stieltjes transform gN(z) to compute the rank-1 hciz integral.
10.4.1 Discrete Stieltjes Transform
Recall the discrete Stieltjes transform of a matrix A with N eigenvalues {Î»k}:
gA
N(z) = 1
N
N

k=1
1
z âˆ’Î»k
.
(10.46)
This function is well deï¬ned for any z on the real axis except on the ï¬nite set {Î»k}. For
z > Î»max, each of the terms in the sum is positive and monotonically decreasing with z so
gA
N(z) is a positive monotonically decreasing function of z. As z â†’âˆ, gA
N(z) â†’0. By
the same argument, for z < Î»min, gA
N(z) is a negative monotonically decreasing function
of z tending to zero as z goes to minus inï¬nity. Actually, the normalization of gA
N(z) is such
that we have
3 The same computation can be done for any value of beta, yielding
IÎ²(T,B) â‰ˆexp
â›
âNÎ²
2
n

i=1
HB(ti)
â
â = exp
 NÎ²
2
Tr HB(T)

,
(10.44)
where IÎ²(T,B) is deï¬ned in the footnote on page 141 and T has low rank.

146
Addition of Large Random Matrices
âˆ’4
âˆ’2
0
2
4
z
âˆ’5
0
5
gn(z)
âˆ’5
0
5
g
âˆ’5
0
5
(g)
(g)
lmax
lmin
Figure 10.2 (left) A particular gA
N(z) for A a Wigner matrix of size N = 5 shown for real values of
z. The gray areas left of Î»min and right of Î»max show the values of z for which it is invertible. (right)
The inverse function z(g). Note that g(z) behaves as 1/z near zero and tends to Î»min and Î»max as g
goes to plus or minus inï¬nity respectively.
gA
N(z) = 1
z + O
 1
z2

when
|z| â†’âˆ.
(10.47)
For large |z|, gA
N(z) is thus invertible and its inverse behaves as
z(g) = 1
g + regular terms
when
|g| â†’0.
(10.48)
If we consider values of gA
N(z) for z > Î»max, we realize that the function takes all possible
positive values once and only once, from the extremely large (near z = Î»max) to almost zero
(when z â†’âˆ). Similarly, all possible negative values are attained when z âˆˆ(âˆ’âˆ,Î»min)
(see Fig. 10.2 left). We conclude that the inverse function z(g) exists for all non-zero values
of g. The behavior of gA
N(z) near Î»min and Î»max gives us the asymptotes
lim
gâ†’âˆ’âˆz(g) = Î»min
and
lim
gâ†’âˆz(g) = Î»max.
(10.49)
10.4.2 Limiting Stieltjes Transform
Let us now discuss the inverse function of the limiting Stieltjes transform g(z). The limiting
Stieltjes transform satisï¬es Eq. (2.41), which we recall here:
g(z) =

supp{Ï}
Ï(x)dx
z âˆ’x ,
(10.50)

10.4 Invertibility of the Stieltjes Transform
147
where Ï(Î») is the limiting spectral distribution and may contain Dirac deltas. We denote
Î»Â± the edges of the support of Ï. We have that for z > Î»+, g(z) is a positive, monotonically
decreasing function of z. Similarly for z < Î»âˆ’, g(z) is a negative, monotonically decreasing
function of z. From the normalization of Ï(Î»), we again ï¬nd that
g(z) = 1
z + O
 1
z2

when
|z| â†’âˆ.
(10.51)
Using the same arguments as for the discrete Stieltjes transform, we have that the inverse
function z(g) exists for small arguments and behaves as
z(g) = 1
g + regular terms
when
|g| â†’0.
(10.52)
The behavior of g(z) at Î»Â± can be different from that of gN(z) at its extreme eigenval-
ues. The points Î»Â± are singular points of g(z). If the density near Î»+ goes to zero as
Ï(Î») âˆ¼(Î»+ âˆ’Î»)Î¸ for some Î¸ > 0 (typically Î¸ = 1/2) then the integral (10.50) converges
at z = Î»+ and g+ := g(Î»+) is a ï¬nite number. For z < Î»+ the function g(z) has a branch
cut and is ill deï¬ned for z on the real axis. The point z = Î»+ is an essential singularity of
g(z). The function is clearly no longer invertible for z < Î»+. Similarly, if Ï(Î») grows as a
positive power near Î»âˆ’, then g(z) is invertible up to the point gâˆ’:= g(Î»âˆ’).
If the density Ï(Î») does not go to zero at one of its edges (or if it has a Dirac delta), the
function g(z) diverges at that edge. We may still deï¬ne gÂ± = limzâ†’Î»Â± g(z) if we allow gÂ±
to be plus or minus inï¬nity.
In all cases, the inverse function z(g) exists in the range gâˆ’â‰¤g â‰¤g+, with the property
z(gÂ±) = Î»Â±.
(10.53)
In the unit Wigner case, we have Î»Â± = Â±2 and gÂ± = Â±1 and the inverse function z(g)
only exists between âˆ’1 and 1 (see Fig. 10.3).
10.4.3 The Inverse Stieltjes Transform for Larger Arguments
In some computations, as in the hciz integral, one needs the value of z(g) beyond gÂ±.
What can we say then? First of all, one should not be fooled by spurious solutions of the
inversion problem. For example in the Wigner case we know that g(z) satisï¬es
g + 1
g âˆ’z = 0,
(10.54)
so we would be tempted to write
z(g) = g + 1
g
(10.55)
for all g. But this is wrong as g + 1/g is not the inverse of g(z) for |g| > 1 (Fig. 10.3).
The correct way to extend z(g) beyond gÂ± is to realize that in most computation, we
use g(z) as an approximation for gN(z) for very large N. For z > Î»+ the function gN(z)

148
Addition of Large Random Matrices
âˆ’5
0
5
z
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
(z)
Re (z)
Im (z)
âˆ’4
âˆ’2
0
2
4
g
âˆ’5
0
5
(g)
(g)
lÂ± = Â± 2
wrong
Figure 10.3 (left) The limiting function g(z) for a Wigner matrix, a typical density that vanishes at
its edges. The function is plotted against a real argument. In the white part of the graph, the function
is ill deï¬ned and it is shown here for a small negative imaginary part of its argument. In the gray
part (z < Î»âˆ’and z > Î»+) the function is well deï¬ned, real and monotonic. It is therefore invertible.
(right) The inverse function z(g) only exists for gâˆ’â‰¤g â‰¤g+ and has a 1/g singularity at zero. The
dashed lines show the extension of z(g) to all values of g that are natural when we think of g(z) as
the limit of gN(z) with maximal and minimal eigenvalues Î»Â±. The dotted lines indicate the wrong
branch of the solution of z(g) = g + 1/g.
converges to g(z) and this approximation can be made arbitrarily good for large enough
N. On the other hand we know that on the support of Ï, gN(z) does not converge to g(z).
The former has a series of simple poles at random locations, while the later has typically
a branch cut.
At large but ï¬nite N, there will be a maximum eigenvalue Î»max. This eigenvalue is
random but to dominant order in N it converges to Î»+, the edge of the spectrum. For z
above but very close to Î»+ we should think of gN(z) as
gN(z) â‰ˆg(z) + 1
N
1
z âˆ’Î»max
â‰ˆg(z) + 1
N
1
z âˆ’Î»+
.
(10.56)
Because 1/N goes to zero, the correction above does not change the limiting value of g(z)
at any ï¬nite distance from Î»+. On the other hand, this correction does change the behavior
of the inverse function z(g). We now have
lim
zâ†’Î»+
gN(z) â†’âˆ
and
z(g) = Î»+ for g > g+.
(10.57)
For negative z and negative g, the same argument follows near Î»âˆ’. We realize that, while
the limiting Stieltjes transform g(z) loses all information about individual eigenvalues,
its inverse function z(g), or really the large N limit of the inverse of the function gN(z),
retains information about the smallest and largest eigenvalues. In Chapter 14 we will study
random matrices where a ï¬nite number of eigenvalues lie outside the support of Ï. In the
large N limit, these eigenvalues do not change the density or g(z) but they do show up in
the inverse function z(g).

10.5 The Full-Rank HCIZ Integral
149
Let us deï¬ne zbulk(g), the inverse function of g(z) without considering extreme eigen-
values or outliers. In the presence of outliers we have Î»max â‰¥Î»+ and gmax := g(Î»max) â‰¤
g(Î»+) and similarly for gmin. With arguments similar to those above we ï¬nd the following
result for the limit of the inverse of gN(z):
z(g) =
â§
âªâ¨
âªâ©
Î»min
for g â‰¤gmin,
zbulk(g)
for gmin < g < gmax,
Î»max
for g â‰¥gmax.
(10.58)
In the absence of outliers the result still applies with max and min (extreme eigenvalues)
replaced by + and âˆ’(edge of the spectrum), respectively.
10.4.4 Large t Behavior of It
Now that we understand the behavior of z(g) for larger arguments we can go back to
our study of the rank-1 hciz integral. There is indeed an apparent paradox in the result
of our computation of It(B). For a given matrix B there are two immediate bounds to
It(B) = Zt(B)/Zt(0):
exp
NtÎ»min
2

â‰¤It(B) â‰¤exp
NtÎ»max
2

,
(10.59)
where Î»min and Î»max are the smallest and largest eigenvalues of B, respectively. Focusing
on the upper bound, we have
HB(t) â‰¤tÎ»max.
(10.60)
On the other hand, the anti-derivative of the R-transform for a unit Wigner matrix reads
RW(t) = t âˆ’â†’HW(t) = t2
2 ,
(10.61)
whereas Î»max â†’Î»+ = 2. One might thus think that the quadratic behavior of HW(t)
violates the bound (10.60) for t > 4. We should, however, remember that Eq. (10.42) is in
fact only valid for t < g+, the value at which g(z) ceases to be invertible. In the absence
of outliers, g+ = g(Î»+). For a unit Wigner this point is g+ = g(2) = 1; the bound is
not violated. For t > g+, one can still compute HB(t) but the result depends explicitly
on Î»max.
Now that we understand the behavior of z(g) for larger arguments, including in the
presence of outliers, we can extend our result for HB(t) for large tâ€™s. We just need to use
Eq. (10.58) into Eq. (10.41):
dHB(t)
dt
=
0
RB(t)
for t â‰¤gmax := g(Î»max),
Î»max âˆ’1/t
for t > gmax,
(10.62)
where the largest eigenvalue Î»max can be either the edge of the spectrum Î»+ or a true
outlier. We will show in Section 13.3 how this result can also be derived for Wigner
matrices using the replica method.
10.5 The Full-Rank HCIZ Integral
We have deï¬ned in Eq. (10.23) the hciz integral as a generalization of the Fourier transform
for matrices, and have seen how to evaluate this integral in the limit N â†’âˆwhen one
matrix is of low rank. A generalized hciz integral IÎ²(A,B) can be deï¬ned as

150
Addition of Large Random Matrices
IÎ²(A,B) =

G(N)
dU e
Î²N
2 Tr AUBUâ€ ,
(10.63)
where the integral is over the (ï¬‚at) Haar measure of the compact group U âˆˆG(N) =
O(N),U(N) or Sp(N) in N dimensions and A,B are arbitrary N Ã— N symmetric (resp.
Hermitian or symplectic) matrices, with, correspondingly, Î² = 1,2 or 4. Note that by
construction IÎ²(A,B) can only depend on the eigenvalues of A and B, since any change of
basis on B (say) can be reabsorbed in U, over which we integrate. Note also that the Haar
measure is normalized, i.e.

G(N) dU = 1.
Interestingly, it turns out that in the unitary case G(N) = U(N) (Î² = 2), the hciz
integral can be expressed exactly, for all N, as the ratio of determinants that depend on
A,B and additional N-dependent prefactors. This is the Harish-Chandraâ€“Itzyksonâ€“Zuber
celebrated result, which cannot be absent from a book on random matrices:
I2(A,B) =
cN
N(N2âˆ’N)/2
det

eNÎ½iÎ»j 
(A)(B) ,
(10.64)
with {Î½i}, {Î»i} the eigenvalues of A and B, (A), (B) are the Vandermonde determinants
of A and B, and cN = Nâˆ’1
â„“
â„“!.
Although this result is fully explicit for Î² = 2, the expression in terms of determinants is
highly non-trivial and quite tricky. For example, the expression becomes degenerate (0/0)
whenever two eigenvalues of A (or B) coincide. Also, as is well known, determinants
contain N! terms of alternating signs, which makes their order of magnitude very hard
to estimate a priori. The aim of this technical section is to discuss how the hciz result can
be obtained using the Karlinâ€“McGregor equation (9.69). We then use the mapping to the
Dyson Brownian motion to derive a large N approximation for the full-rank hciz integral
in the general case.
10.5.1 HCIZ and Karlinâ€“McGregor
In order to understand the origin of Eq. (10.64), the basic idea is to interpret the hciz
integrand in the unitary case, exp[N Tr AUBUâ€ ], as a part of the diffusion propagator in
the space of Hermitian matrices, and use the Karlinâ€“McGregor formula.
Indeed, adding to A a sequence of inï¬nitesimal random Gaussian Hermitian matrices
of variance dt/N, the probability to end up with matrix B in a time t = 1 is given by
P(B|A) âˆNN2/2 eâˆ’N/2 Tr(Bâˆ’A)2,
(10.65)
where we drop an overall normalization constant in our attempt to understand the structure
of Eq. (10.64). The corresponding eigenvalues follow a Dyson Brownian motion, namely
dxi =
1
1
N dBi + 1
N
N

j=1
ji
dt
xi âˆ’xj
,
(10.66)
with xi(t = 0) = Î½i and xi(t = 1) = Î»i. Now, for Î² = 2 we can use the Karlinâ€“McGregor
equation (9.69) to derive the conditional distribution of the {Î»i}, given by

10.5 The Full-Rank HCIZ Integral
151
P({Î»i}|{Î½i}) = (B)
(A)P(âƒ—Î»,t = 1|âƒ—Î½),
(10.67)
where P(âƒ—Î»,t = 1|âƒ—Î½) is given by a determinant, Eq. (9.66). With the present normalization,
this determinant reads
P(âƒ—Î»,t = 1|âƒ—Î½) =
 N
2Ï€
N/2
eâˆ’N
2 (Tr A2+Tr B2) det

eNÎ½iÎ»j

.
(10.68)
Now, the distribution of eigenvalues of B can be computed from Eq. (10.65). First we
make P(B|A) unitary-invariant by integrating over U(N):
P(B|A) â†’P(B|A) = NN2/2

U(N) dU eâˆ’N/2 Tr(UBUâ€ âˆ’A)2
 N
= NN2/2eâˆ’N
2 (Tr A2+Tr B2) I2(A,B)
 N
,
(10.69)
where  N =

U(N) dU is the â€œvolumeâ€ of the unitary group U(N). This new measure,
by construction, only depends on {Î»i}, the eigenvalues of B. Changing variables from B
to {Î»i} introduces a Jacobian, which in the unitary case is the square of the Vandermonde
determinant of B, 2(B). We thus ï¬nd a second expression for the distribution of the {Î»i}:
P({Î»i}|{Î½i}) âˆNN2/22(B)eâˆ’N
2 (Tr A2+Tr B2)I2(A,B).
(10.70)
Comparing with Eqs. (10.67) and (10.68) we thus ï¬nd
I2(A,B) âˆN(Nâˆ’N2)/2 det

eNÎ½iÎ»j

(A)(B) ,
(10.71)
which coincides with Eq. (10.64), up to an overall constant cN which can be obtained
by taking the limit A = 1, i.e. when all the eigenvalues of A are equal to 1. The limit is
singular but one can deal with it in a way similar to the one used by BrÂ´ezin and Hikami
to go from Eq. (6.65) to (6.67). In this limit, the right hand side of Eq. (10.71) reads
exp(N Tr B)/cN, while the left hand side is trivially equal to exp(N Tr B). Hence a factor
cN is indeed missing in the right hand side of Eq. (10.71).
Equation (10.64) can also be used to obtain an exact formula for the rank-1 hciz
integral (when Î² = 2). The trick is to have one of the eigenvalues of Î½i equal to some
non-zero number t and let the N âˆ’1 others go to zero. The limit can again be dealt with
in the same way as Eq. (6.65). One ï¬nally ï¬nds
I2(t,B) = (N âˆ’1)!
(Nt)Nâˆ’1
N

j=1
eNtÎ»j

kj(Î»j âˆ’Î»k).
(10.72)
The above formula may look singular at t = 0, but we have limtâ†’0 I2(t,B) = 1 as
expected.
10.5.2 HCIZ at Large N: The Eulerâ€“Matytsin Equations
We now explain how I2(A,B) can be estimated for large matrix size, using a Dyson
Brownian representation of P(B|A), Eq. (10.66). In terms of these interacting Brownian
motions, the question is how to estimate the probability that the xi(t) start at xi(t = 0) =

152
Addition of Large Random Matrices
Î½i and end at xi(t = 1) = Î»i, when their trajectories are determined by Eq. (10.66), which
we rewrite as
dxi =
1
1
N dBi âˆ’âˆ‚xi V dt,
V ({xi}) := âˆ’1
N

i<j
ln |xi âˆ’xj|.
(10.73)
The probability of a given trajectory for the N Brownian motions between time t = 0 and
time t = 1 is then given by4
P({xi(t)}) = Zâˆ’1 exp âˆ’
â¡
â£N
2
 1
0
dt

i

Ë™xi + âˆ‚xi V
2
â¤
â¦:= Zâˆ’1eâˆ’N2S,
(10.74)
where Z is a normalization factor that we will not need explicitly. Expanding the square
as Ë™x2
i + 2âˆ‚xi V Ë™xi + (âˆ‚xi V )2, one can decompose S = S1 + S2 into a total derivative term
equal, in the continuum limit, to boundary terms, i.e.
S1 = âˆ’1
2
(
dxdyÏC(x)ÏC(y) ln |x âˆ’y|
)C=B
C=A
(10.75)
and
S2 :=
1
2N
 1
0
dt
N

i=1
&
Ë™x2
i + (âˆ‚xi V )2'
.
(10.76)
We now look for the â€œinstantonâ€ trajectory that contributes most to the probability P for
large N, in other words the trajectory that minimizes S2. This extremal trajectory is such
that the functional derivative of S2 with respect to all xi(t) is zero:
âˆ’2d2xi
dt2 + 2
N

â„“=1
âˆ‚2
xi,xâ„“V âˆ‚xâ„“V = 0,
(10.77)
which leads, after a few algebraic manipulations, to
d2xi
dt2 = âˆ’2
N2

â„“i
1
(xi âˆ’xâ„“)3 .
(10.78)
This can be interpreted as the motion of unit mass particles, accelerated by an attractive
force that derives from an effective two-body potential Ï†(r) = âˆ’(Nr)âˆ’2. The hydrody-
namical description of such a ï¬‚uid, justiï¬ed when N â†’âˆ, is given by the Euler equations
for the density ï¬eld Ï(x,t) and the velocity ï¬eld v(x,t):
âˆ‚tÏ(x,t) + âˆ‚x[Ï(x,t)v(x,t)] = 0
(10.79)
and
âˆ‚tv(x,t) + v(x,t)âˆ‚xv(x,t) = âˆ’
1
Ï(x,t)âˆ‚x(x,t),
(10.80)
where (x,t) is the pressure ï¬eld, which reads, from the â€œvirialâ€ formula for an interact-
ing ï¬‚uid at temperature T ,5
4 We neglect here a Jacobian which is not relevant to compute the leading term of I2(A,B) in the large N limit.
5 See e.g. Le Bellac et al. [2004], p. 138.

10.5 The Full-Rank HCIZ Integral
153
 = ÏT âˆ’1
2Ï

â„“i
|xi âˆ’xâ„“|Ï†â€²(xi âˆ’xâ„“) â‰ˆâˆ’Ï
N2

â„“i
1
(xi âˆ’xâ„“)2,
(10.81)
because the ï¬‚uid describing the instanton is at zero temperature, T = 0. Now, writing
xi âˆ’xâ„“â‰ˆ(i âˆ’â„“)/(NÏ) and âˆ
n=1 nâˆ’2 = Ï€2
6 , one ï¬nally ï¬nds
(x,t) = âˆ’Ï€2
3 Ï(x,t)3.
(10.82)
Equations (10.79) and (10.80) for Ï and v with  given by (10.82) are called the Eulerâ€“
Matytsin equations. They should be solved with the following boundary conditions:
Ï(x,t = 0) = ÏA(x);
Ï(x,t) = ÏB(x);
(10.83)
the velocity ï¬eld v(x,t = 0) should be chosen such that these boundary conditions are
fulï¬lled.
Expressing S2 in terms of the solution of the Eulerâ€“Matytsin equations gives, in the
continuum limit,
S2(A,B) â‰ˆ1
2

dxÏ(x,t)
-
v2(x,t) + Ï€2
3 Ï2(x,t)
.
.
(10.84)
Hence, the probability P({Î»i}|{Î½i}) to observe the set of eigenvalues {Î»i} of B for a
given set of eigenvalues Î½i for A is, in the large N limit, proportional to exp[âˆ’N2(S1 +
S2)]. Comparing with Eq. (10.70), we get as a ï¬nal expression for F2(A,B)
:=
âˆ’limNâ†’âˆNâˆ’2 ln I2(A,B):
F2(A,B) = 3
4 + S2(A,B) âˆ’1
2

dx x2(ÏA(x) + ÏB(x))
+ 1
2

dxdy [ÏA(x)ÏA(y) + ÏB(x)ÏB(y)] ln |x âˆ’y|.
(10.85)
This result was ï¬rst derived in Matytsin [1994], and proven rigorously in Guionnet and
Zeitouni [2002]. Note that this expression is symmetric in A,B, as it should be, because
the solution of the Eulerâ€“Matytsin equations for the time reversed path from ÏB to ÏA are
simply obtained from Ï(x,t) â†’Ï(x,1 âˆ’t) and v(x,t) â†’âˆ’v(x,1 âˆ’t), which leaves
S2(A,B) unchanged.
The whole calculation above can be repeated for the Î² = 1 (orthogonal group) or
Î² = 4 (symplectic group) with the ï¬nal (simple) result FÎ²(A,B) = Î²F2(A,B)/2.
Bibliographical Notes
â€¢ The Burgersâ€™ equation in the context of random matrices:
â€“ L. C. G. Rodgers and Z. Shi. Interacting Brownian particles and the Wigner law.
Probability Theory and Related Fields, 95:555â€“570, 1993,
â€“ J.-P. Blaizot and M. A. Nowak. Universal shocks in random matrix theory. Physical
Review E, 82:051115, 2010,
â€“ G. Menon. Lesser known miracles of Burgers equation. Acta Mathematica Scientia,
32(1):281â€“94, 2012.

154
Addition of Large Random Matrices
â€¢ The hciz integral: Historical papers:
â€“ Harish-Chandra. Differential operators on a semisimple Lie algebra. American Jour-
nal of Mathematics, 79:87â€“120, 1957,
â€“ C. Itzykson and J.-B. Zuber. The planar approximation. II. Journal of Mathematical
Physics, 21:411â€“421, 1980,
for a particularly insightful introduction, see T. Tao,
http://terrytao.wordpress.com/2013/02/08/theharish-chandra-itzykson-zuber-integral-
formula/.
â€¢ The low-rank hciz integral:
â€“ E. Marinari, G. Parisi, and F. Ritort. Replica ï¬eld theory for deterministic models. II.
A non-random spin glass with glassy behaviour. Journal of Physics A: Mathematical
and General, 27(23):7647, 1994,
â€“ A. Guionnet and M. MaÂ¨Ä±da. A Fourier view on the R-transform and related asymp-
totics of spherical integrals. Journal of Functional Analysis, 222(2):435â€“490, 2005.
â€¢ The hciz integral: Large N limit:
â€“ A. Matytsin. On the large-N limit of the Itzykson-Zuber integral. Nuclear Physics B,
411:805â€“820, 1994,
â€“ A. Guionnet and O. Zeitouni. Large deviations asymptotics for spherical integrals.
Journal of Functional Analysis, 188(2):461â€“515, 2002,
â€“ B. Collins, A. Guionnet, and E. Maurel-Segala. Asymptotics of unitary and orthogo-
nal matrix integrals. Advances in Mathematics, 222(1):172â€“215, 2009,
â€“ J. Bun, J. P. Bouchaud, S. N. Majumdar, and M. Potters. Instanton approach to large
N Harish-Chandra-Itzykson-Zuber integrals. Physical Review Letters, 113:070201,
2014,
â€“ G. Menon. The complex Burgers equation, the HCIZ integral and the Calogero-
Moser system, unpublished, 2017, available at: https://www.dam.brown.edu/people/
menon/talks/cmsa.pdf.
â€¢ On the classical virial theorem, see
â€“ M. Le Bellac, F. Mortessagne, and G. G. Batrouni. Equilibrium and Non-Equilibrium
Statistical Thermodynamics. Cambridge University Press, Cambridge, 2004.

11
Free Probabilities
In the previous chapter we saw how to compute the spectrum of the sum of two large
random matrices, ï¬rst when one of them is a Wigner and later when one is â€œrotationally
invariantâ€ with respect to the other. In this chapter, we would like to formalize the notion
of relative rotational invariance, which leads to the abstract concept of freeness.
The idea is as follows. In standard probability theory, one can work abstractly by deï¬n-
ing expectation values (moments) of random variables. The concept of independence is
then equivalent to the factorization of moments (e.g. E[A3B2] = E[A3]E[B2] when A and
B are independent).
However, random matrices do not commute in general and the concept of factorization
of moments is not powerful enough to deal with non-commuting random objects. Follow-
ing von Neumann, Voiculescu extended the concept of independence to non-commuting
objects and called this property freeness. He then showed how to characterize the sum and
the product of free variables. It was later realized that large rotationally invariant matrices
provide an explicit example of (asymptotically) free random variables. In other words, free
probabilities gave us very powerful tools to compute sums and products of large random
matrices. We have already encountered the free addition; the free product will allow us to
study sample covariance matrices in the presence of non-trivial true correlations.
This chapter may seem too dry and abstract for someone looking for applications. Bear
with us, it is in fact not that complicated and we will keep the jargon to a minimum. The
reward will be one of the most powerful and beautiful recent developments in random
matrix theory, which we will expand upon in Chapter 12.
11.1 Algebraic Probabilities: Some Deï¬nitions
The ingredients we will need in this chapter are as follows:1
â€¢ A ring R of random variables, which can be non-commutative with respect to the multi-
plication.2
1 In mathematical language, the ï¬rst three items give a *-algebra, while Ï„ gives a tracial state on this algebra.
2 Recall that a ring is a set equipped with two binary operations that generalize the arithmetic operations of addition and
multiplication.
155

156
Free Probabilities
â€¢ A ï¬eld of scalars, which is usually taken to be C. The scalars commute with everything.
â€¢ An operation âˆ—, called involution. For instance, âˆ—denotes the conjugate for com-
plex numbers, the transpose for real matrices, and the conjugate transpose for complex
matrices.
â€¢ A positive linear functional Ï„(.) (R â†’C) that satisï¬es Ï„(AB) = Ï„(BA) for A,B âˆˆR.
By positive we mean Ï„(AAâˆ—) is real non-negative. We also require that Ï„ be faithful,
in the sense that Ï„(AAâˆ—) = 0 implies A = 0. For instance, Ï„ can be the expectation
operator E[.] for standard probability theory, or the normalized trace operator 1
N Tr(.)
for a ring of matrices, or the combined operation 1
N E[Tr(.)].
We will call the elements in R the random variables and denote them by capital letters.
For any A âˆˆR and k âˆˆN, we call Ï„(Ak) the kth moment of A and we assume in what
follows that Ï„(Ak) is ï¬nite for all k. In particular, we call Ï„(A) the mean of A and Ï„(A2) âˆ’
Ï„(A)2 the variance of A. We will say that two elements A and B have the same distribution
if they have the same moments of all orders.3
The ring of variables must have an element called 1 such that A1 = 1A = A for every
A. It satisï¬es Ï„(1) = 1. We will call 1 and its multiples Î±1 constants. Adding a constant
simply shifts the mean as
Ï„(A + Î±1) = Ï„(A) + Î±.
(11.1)
11.2 Addition of Commuting Variables
In this section, we recall some well-known properties of commuting random variables, i.e.
such that
AB = BA,
âˆ€A,B âˆˆR.
(11.2)
Note that A is not necessarily a real (or complex) number but can be an element of
a more abstract ring. We will say that A and B are independent, if Ï„(p(A)q(B)) =
Ï„(p(A))Ï„(q(B)) for any polynomial p,q. This condition is equivalent to the factorization
of moments.
From a scalar Î± we can build the constant Î±1 and write A+Î± to mean A+Î±1. Constants
of the ring are independent of all other random variables, so if A and B are independent,
A + Î± and B are also independent. This setting recovers the classical probability theory of
commutative random variables (with ï¬nite moments to every order).
11.2.1 Moments
Now let us study the moments of the sum of independent random variables A+B. First we
trivially have by linearity
Ï„(A + B) = Ï„(A) + Ï„(B).
(11.3)
3 This is of course not correct for standard commuting random variables: some distributions are not uniquely determined by
their moments.

11.2 Addition of Commuting Variables
157
From now on we will assume Ï„(A) = Ï„(B) = 0, i.e. A,B have mean zero, unless stated
otherwise. For a non-zero mean variable ËœA, we write A = ËœA âˆ’Ï„( ËœA) such that Ï„(A) = 0.
One can recover the formulas for moments and cumulants of ËœA simply by substituting
A â†’ËœA âˆ’Ï„( ËœA) in all formulas written for zero mean A. The procedure is straightforward
but leads to rather cumbersome results.
For the second moment,
Ï„

(A + B)2
= Ï„(A2) + Ï„(B2) + 2Ï„(AB)
= Ï„(A2) + Ï„(B2) + 2Ï„(A)Ï„(B) = Ï„(A2) + Ï„(B2),
(11.4)
i.e. the variance is also additive. For the third moment, we have
Ï„

(A + B)3
= Ï„(A3) + Ï„(B3) + 3Ï„(A)Ï„(B2) + 3Ï„(B)Ï„(A2) = Ï„(A3) + Ï„(B3),
(11.5)
which is also additive. However, the fourth and higher moments are not additive anymore.
For example we get, expanding (A + B)4,
Ï„

(A + B)4
= Ï„(A4) + Ï„(B4) + 6Ï„(A2)Ï„(B2).
(11.6)
11.2.2 Cumulants
For zero mean variables the ï¬rst three moments are additive but not the higher ones.
Nevertheless, certain combinations of higher moments are additive; we call them cumulants
and denote them as Îºn for the nth cumulant. Note that for a variable with non-zero mean
ËœA, the second and third cumulants are the second and third moments of A := ËœA âˆ’Ï„( ËœA):
Îº1( ËœA) = Ï„( ËœA),
Îº2( ËœA) = Ï„(A2) = Ï„( ËœA2) âˆ’Ï„( ËœA)2,
(11.7)
Îº3( ËœA) = Ï„(A3) = Ï„( ËœA3) âˆ’3Ï„( ËœA2)Ï„( ËœA) + 2Ï„( ËœA)3.
For the fourth cumulant, let us consider for simplicity zero mean variables A and B, and
deï¬ne Îº4 as
Îº4(A) := Ï„(A4) âˆ’3Ï„(A2)2.
(11.8)
Then we can verify that
Îº4 (A + B) = Ï„

(A + B)4
âˆ’3Ï„

(A + B)22
= Ï„(A4) + Ï„(B4) + 6Ï„(A2)Ï„(B2) âˆ’3

Ï„(A2) + Ï„(B2)
2
= Ï„(A4) âˆ’3Ï„(A2)2 + Ï„(B4) âˆ’3Ï„(B2)2 = Îº4(A) + Îº4(B),
(11.9)
which is additive again.
In general, Ï„((A + B)n) will be of the form Ï„(An) + Ï„(Bn) plus some homogeneous
mix of lower order terms. We can then deï¬ne the nth cumulant Îºn iteratively such that

158
Free Probabilities
Îºn(A + B) = Îºn(A) + Îºn(B),
(11.10)
where
Îºn(A) = Ï„(An) + lower order terms moments.
(11.11)
In order to have a compact deï¬nition of cumulants, recall that we are looking for quantities
that are additive for independent variables. But we already know that the log-characteristic
function introduced in Eq. (8.4) is additive. In the present context, we deï¬ne the character-
istic function as4
Ï•A(t) = Ï„

eitA
,
(11.12)
where the exponential function is formally deï¬ned through its power series:
Ï„(eitA) =
âˆ

â„“=0
(it)â„“
â„“! Ï„(Aâ„“),
(11.13)
hence the characteristic function is also the moment generating function. Now, from the
formal deï¬nition of the exponential and the factorization of moments one can easily show
that for independent, commuting A,B,
Ï•A+B(t) = Ï•A(t)Ï•B(t).
(11.14)
Here is an algebraic proof. For each k,
Ï„((A + B)k) =
k

i=0
k
i

Ï„(Ai)Ï„(Bkâˆ’i),
(11.15)
with which we get
Ï•A+B(t) =
âˆ

k=0

iâ‰¤k
(it)k
k!
k
i

Ï„(Ai)Ï„(Bkâˆ’i) =

iâ‰¤k
(it)kâˆ’iÏ„(Bkâˆ’i)
(k âˆ’i)!
(it)iÏ„(Ai)
i!
=
â›
â
i
(it)iÏ„(Ai)
i!
â
â 
â›
â
j
(it)jÏ„(Bj)
j!
â
â = Ï•A(t)Ï•B(t).
(11.16)
We now deï¬ne HA(t) := log Ï•A(t). Then, for independent, commuting A,B, we have
HA+B(t) = HA(t) + HB(t).
(11.17)
We can expand HA(t) as a power series of t and call the corresponding coefï¬cients the
cumulants, i.e.
HA(t) = log Ï„

eitA
:=
âˆ

n=1
Îºn(A)
n!
(it)n.
(11.18)
4 The factor i in the deï¬nition is not necessary in this setting as the formal power series of the exponential and the logarithm do
not need to converge. We nevertheless include it by analogy with the Fourier transform.

11.2 Addition of Commuting Variables
159
From the additive property of H, the cumulants deï¬ned in the above way are automatically
additive. In fact, using the power series for log(1 + x), we have
HA(t) =
âˆ

n=1
(âˆ’1)nâˆ’1
n
 âˆ

k=1
(it)k
k! Ï„(Ak)
n
â‰¡
âˆ

n=1
Îºn(A)
n!
(it)n.
(11.19)
Matching powers of (it) we obtain an expression for Îºn for any n. We can work out by hand
the ï¬rst few cumulants. For example, for n = 1, Eq. (11.19) readily yields Îº1(A) = Ï„(A).
We now assume A has mean zero, i.e. Ï„(A) = 0. Then
Ï„

eitA
= 1 + (it)2
2 Ï„(A2) + (it)3
6 Ï„(A3) + (it)4
24 Ï„(A4) + Â· Â· Â· ,
(11.20)
whereas the ï¬rst few terms in the expansion of (11.19) are
HA(t) = (it)2
2 Ï„(A2) + (it)3
6 Ï„(A3) + (it)4
Ï„(A4)
24
âˆ’Ï„(A2)2
8

+ Â· Â· Â· ,
(11.21)
from which we recover the ï¬rst four cumulants deï¬ned above:
Îº1(A) = 0,
Îº2(A) = Ï„(A2),
Îº3(A) = Ï„(A3),
Îº4(A) = Ï„(A4) âˆ’3Ï„(A2)2. (11.22)
The expression for Îºn soon becomes very cumbersome for larger n. Nevertheless, by
exponentiating Eq. (11.18) and matching with Eq. (11.13), one can extract the following
momentâ€“cumulant relation for commuting variables:
Ï„(An) =

r1,r2,...,rnâ‰¥0
r1+2r2+Â·Â·Â·+nrn=n
n! Îºr1
1 Îºr2
2 Â· Â· Â· Îºrn
n
(1! )r1(2! )r2 Â· Â· Â· (n! )r2r1! r2! Â· Â· Â· rn!
= Îºn + products of lower order terms + Îºn
1 .
(11.23)
In particular, the scaling properties for the moments and cumulants (see (11.28) below)
are consistent due to the relation r1 + 2r2 + Â· Â· Â· + nrn = n.
Exercise 11.2.1
Cumulants of a constant
Show that a constant Î±1 has Îº1 = Î± and Îºn = 0 for n â‰¥2. (Hint: compute
HÎ±1(k) = log

Ï„

eikÎ±1
.)
11.2.3 Scaling of Moments and Cumulants
Moments and cumulants obey simple transformation rules under scalar addition and multi-
plication. For example, when adding a constant to a variable, ËœA := A+Î±, where Ï„(A) = 0,
we only change the ï¬rst cumulant:
Îº1( ËœA) = Î±
and
Îºn( ËœA) = Îºn(A)
for
n â‰¥2.
(11.24)

160
Free Probabilities
For the case of multiplication by an arbitrary scalar Î±, by commutativity of scalars and
linearity of Ï„ we have
Ï„

(Î±A)k
= Î±kÏ„

Ak
.
(11.25)
For the cumulant, we ï¬rst look at the scaling of the log-characteristic function:
HÎ±A(t) = log

Ï„

eitÎ±A
= HA(Î±t).
(11.26)
And by (11.18), we have
HÎ±A(t) = HA(Î±t) =
âˆ

n=1
Î±nÎºn(A)
n!
(it)n.
(11.27)
Thus we have the simple scaling property
Îºn(Î±A) = Î±nÎºn(A).
(11.28)
11.2.4 Law of Large Numbers and Central Limit Theorem
Continuing our study of algebraic probabilities, we would like to recover two very impor-
tant theorems in probability theory, namely the law of large numbers (lln) and the central
limit theorem (clt). The ï¬rst states that the sample average converges to the mean (a
constant) as the number of observations N â†’âˆ, and the second that a large sum of
properly centered and rescaled random variables converge to a Gaussian.
First we need to deï¬ne in our context what we mean by a constant and a Gaussian. For
simplicity, we can think of the variables in this section as standard random variables. We
will later introduce non-commutating cumulants. The arguments of this section apply in
the non-commutative case with independence replaced by freeness.
We have deï¬ned the constant variable A = Î±1, which satisï¬es
Îº1(A) = Î±,
Îºâ„“(A) = 0, âˆ€â„“> 1.
(11.29)
Then we deï¬ne the â€œGaussianâ€ random variable as an element A that satisï¬es
Îº2(A)  0,
Îºâ„“(A) = 0, âˆ€â„“> 2.
(11.30)
Note that this deï¬nition (in the scalar case) is equivalent to the standard Gaussian random
variable with density
PÎ¼,Ïƒ 2(x) =
1
âˆš
2Ï€Ïƒ 2 exp

âˆ’(x âˆ’Î¼)2
2Ïƒ 2

,
(11.31)
with Îº1 = Î¼ and Îº2 = Ïƒ 2.
By extension, we call Îº1(A) the mean, and Îº2(A) the variance. Now we can give a
simple proof for the lln and clt within our algebraic setting. Let

11.3 Non-Commuting Variables
161
SK := 1
K
K

i=1
Ai,
(11.32)
where Ai are K iid variables.5 Then by (11.28) and the additive property of cumulants, we
get that
Îºâ„“(SK) = K
Kâ„“Îºâ„“(A)
Kâ†’âˆ
â†’
0
Îº1(A),
if â„“= 1,
0,
if â„“> 1.
(11.33)
In other words, SK converges to a constant in the sense of cumulants.
Assume now that Îº1(A) = 0 and consider
SK :=
1
âˆš
K
K

i=1
Ai.
(11.34)
Then it is easy to see that
Îºâ„“(SK) =
K
Kâ„“/2 Îºâ„“(A) â†’
â§
âªâªâ¨
âªâªâ©
0,
if â„“= 1,
Îº2(A),
if â„“= 2,
0,
if â„“> 2.
(11.35)
In other words, SK converges to a â€œGaussianâ€ random variable with variance Îº2(A) =
Ï„(A2), in the sense of cumulants.
In our algebraic probability setting we have made the assumption that the variables we
consider have ï¬nite moments of all orders. This is a very strong assumption. In particular it
excludes any variable whose probability decays as a power law. If we relaxed this assump-
tion we would ï¬nd that some sums of power-law distributed variables converge not to a
Gaussian but to a LÂ´evy-stable distribution. A similar concept exists in the non-commutative
case, but it is beyond the scope of this book.
11.3 Non-Commuting Variables
We now return to our original goal of developing an extension of standard probabilities for
non-commuting objects. One of the goals is to generalize the law of addition of independent
variables. We consider a variable equal to A+B where A and B are now non-commutative
objects such as large random matrices. If we compute the ï¬rst three moments of A + B,
no particular problems arise thanks to the tracial property of Ï„, and they behave as in the
commutative case. For example, consider the third moment
5 iid copies are variables Ai that have exactly the same cumulants and are all independent. We did not deï¬ne independence for
more than two variables but the factorization of moments can be easily extended to more variables. Note that pairwise
independence is not enough to assure independence as a group. For example if x1, x2 and x3 are iid Gaussians, the Gaussian
variables x1, x2 and y3 = sign(x1x2)|x3| are pairwise independent but not all independent as E[x1x2y3] > 0 whereas
E[x1]E[x2]E[y3] = 0.

162
Free Probabilities
Ï„

(A + B)3
= Ï„(A3) + Ï„(A2B) + Ï„(ABA) + Ï„(BA2)
+ Ï„(AB2) + Ï„(BAB) + Ï„(B2A) + Ï„(B3).
(11.36)
But since Ï„(A2B) = Ï„(ABA) = Ï„(BA2) (and similarly when A appears once and B
twice), the classic independence property
Ï„(A2B) = Ï„(A2)Ï„(B) = 0
(11.37)
appears to be sufï¬cient. Things become more interesting for the fourth moment. Indeed,
Ï„

(A + B)4
= Ï„(A4) + Ï„(A3B) + Ï„(A2BA) + Ï„(ABA2) + Ï„(BA3)
+ Ï„(A2B2) + Ï„(ABAB) + Ï„(BA2B) + Ï„(AB2A) + Ï„(BABA)
+ Ï„(B2A2) + Ï„(B3A) + Ï„(B2AB) + Ï„(BAB2) + Ï„(AB3) + Ï„(B4)
= Ï„(A4) + 4Ï„(A3B) + 4Ï„(A2B2) + 2Ï„(ABAB) + 4Ï„(AB3) + Ï„(B4),
(11.38)
where in the second step we again used the tracial property of Ï„. For commutative random
variables, independence of A,B means Ï„(A2B2) = Ï„(A2)Ï„(B2), and this is enough to
treat all the terms above. In the non-commutative case, we also need to handle the term
Ï„ (ABAB). In general ABAB is not equal to A2B2. â€œIndependenceâ€ is therefore not
enough to deal with this term, so we need a new concept. A radical solution would be
to postulate that Ï„(ABAB) is zero whenever Ï„(A) = Ï„(B) = 0. As we compute higher
moments of A + B we will encounter more and more complicated similar mixed moments.
The concept of freeness deals with all such terms at once.
11.3.1 Freeness
Given two random variables A,B, we say they are free if for any polynomials p1, . . . ,pn
and q1, . . . ,qn such that
Ï„(pk(A)) = 0,
Ï„(qk(B)) = 0,
âˆ€k,
(11.39)
we have
Ï„ (p1(A)q1(B)p2(A)q2(B) Â· Â· Â· pn(A)qn(B)) = 0.
(11.40)
We will call a polynomial (or a variable) traceless if Ï„(p(A)) = 0. Note that Î±1 is free with
respect to any A âˆˆR because Ï„(p(Î±1)) â‰¡p(Î±1), from the deï¬nition of 1. Hence,
Ï„(p(Î±1)) = 0 â‡”p(Î±1) = 0.
(11.41)
Moreover, it is easy to see that if A,B are free, then p(A),q(B) are free for any polynomials
p,q. By extension, F(A) and G(B) are also free for any function F and G deï¬ned by their
power series.
The freeness is non-trivial only in the non-commutative case. For the commutative case,
it is easy to check that A,B are free if and only if either A or B is a constant. Free random

11.3 Non-Commuting Variables
163
variables are â€œmaximallyâ€ non-commuting, in a sense made more precise for the example
of free random matrices in the next chapter. For example, for free and mean zero variables A
and B, we have Ï„(ABAB) = 0 whereas Ï„(A2B2) = Ï„((A2 âˆ’Ï„(A2))B2)+Ï„(A2)Ï„(B2) =
Ï„(A2)Ï„(B2).
Assuming A,B are free with Ï„(A) = Ï„(B) = 0, we can compute the moments of the
free addition A + B. The second moment is easy:
Ï„

(A + B)2
= Ï„(A2) + Ï„(B2) + 2Ï„(AB) = Ï„(A2) + Ï„(B2),
(11.42)
because both Ï„(A),Ï„(B) are zero. For the third and higher moments the trick is, as just
above, to add and subtract quantities such that, in each term, at least one object of the form
(C âˆ’Ï„(C)) is present:
Ï„

(A + B)3
= Ï„(A3) + Ï„(B3) + 3Ï„(A2B) + 3Ï„(AB2)
= Ï„(A3) + Ï„(B3) + 3Ï„((A2 âˆ’Ï„(A2))B) + 3Ï„(A2)Ï„(B)
+ 3Ï„(A(B2 âˆ’Ï„(B2))) + 3Ï„(A)Ï„(B2)
= Ï„(A3) + Ï„(B3),
(11.43)
and for the fourth moment:
Ï„

(A + B)4
= Ï„(A4) + 4Ï„(A3B) + 4Ï„(A2B2) + 2Ï„(ABAB) + 4Ï„(AB3) + Ï„(B4)
= Ï„(A4) + 4Ï„((A3 âˆ’Ï„(A3))B) + 4Ï„(A3)Ï„(B)
+ 4Ï„

A2 âˆ’Ï„(A2)

B2 âˆ’Ï„(B2)

+ 4Ï„(A2)Ï„(B2)
+ 4Ï„

A

B3 âˆ’Ï„(B3)

+ 4Ï„(A)Ï„(B3) + Ï„(B4)
= Ï„(A4) + Ï„(B4) + 4Ï„(A2)Ï„(B2).
(11.44)
In particular, we ï¬nd
Ï„

(A + B)4
âˆ’2Ï„

(A + B)22 = Ï„(A4) + Ï„(B4) âˆ’2Ï„(A2)2 âˆ’2Ï„(B2)2.
(11.45)
11.3.2 Free Cumulants
Let us deï¬ne the cumulants of A as
Îº1(A) = Ï„(A),
Îº2(A) = Ï„(A2
0),
Îº3(A) = Ï„(A3
0),
Îº4(A) = Ï„(A4
0) âˆ’2Ï„(A2
0)2,
(11.46)
where A0 is a short-hand for A âˆ’Ï„(A)1. Then these objects are additive for free random
variables. The ï¬rst three are the same as the commutative ones. But for the fourth cumulant,
the coefï¬cient in front of Ï„(A2
0)2 is now 2 instead of 3. Higher cumulants all differ from
their commutative counterparts.
As in the commutative case, we can deï¬ne the kth free cumulant iteratively as
Îºk(A) = Ï„(Ak) + homogeneous products of lower order moments,
(11.47)

164
Free Probabilities
such that
Îºk(A + B) = Îºk(A) + Îºk(B),
âˆ€k,
(11.48)
whenever A,B are free.
An important example of non-commutative free random variables is two independent
large random matrices where one of them is rotational invariant â€“ see next chapter.6
11.3.3 Additivity of the R-Transform
In the previous chapter, we saw that the R-transform is additive for large rotationally
invariant matrices. We will show here that we can deï¬ne the R-transform in our abstract
algebraic probability setting and that this R-transform is also additive for free variables. In
the next chapter, we will dwell on why large rotationally invariant matrices are free.
First we deï¬ne the Stieltjes transform as a moment generating function as in (2.22); we
can deï¬ne gA(z) for large z as
gA(z) =
âˆ

k=0
1
zk+1 Ï„(Ak).
(11.49)
Then we can also deï¬ne the R-transform as before:
RA(g) := zA(g) âˆ’1
g
(11.50)
for small enough g. Here the inverse function zA(g) is deï¬ned as the formal power series
that satisï¬es gA(zA(g)) = g to all orders.
We now claim that the R-transform is additive for free random variables, i.e.
RA+B(g) = RA(g) + RB(g),
(11.51)
whenever A,B are free.
We let zA(g) be the inverse function of
gA(z) = Ï„
&
(z âˆ’A)âˆ’1'
,
(11.52)
whose power series is actually given by (11.49). Consider a ï¬xed scalar g. By construction
Ï„(g1) = g = gA(zA(g)) = Ï„
&
(zA(g) âˆ’A)âˆ’1'
.
(11.53)
The arguments of Ï„(.) on the left and on the right of the above equation have the same mean
but they are in general different, so let us deï¬ne their difference as gXA via
gXA := (zA âˆ’A)âˆ’1 âˆ’g1,
(11.54)
where zA := zA(g). From its very deï¬nition, we have Ï„(XA) = 0.
6 Freeness is only exact in the large N limit of random matrices.

11.3 Non-Commuting Variables
165
We can invert Eq. (11.54) and ï¬nd
A âˆ’zA = âˆ’1
g (1 + XA)âˆ’1.
(11.55)
Consider another variable B, free from A. For the same ï¬xed g we can ï¬nd the scalar
zB := zB(g) and deï¬ne XB with Ï„(XB) = 0 as for A, to ï¬nd
B âˆ’zB = âˆ’1
g (1 + XB)âˆ’1.
(11.56)
Since XA and XB are functions of A and B, XA and XB are also free. Now,
A + B âˆ’zA âˆ’zB = âˆ’1
g (1 + XA)âˆ’1 âˆ’1
g (1 + XB)âˆ’1
= âˆ’1
g (1 + XA)âˆ’1(2 + XA + XB)(1 + XB)âˆ’1.
(11.57)
Hence, noting that (1 + XA)(1 + XB) + 1 âˆ’XAXB = 2 + XA + XB,
A + B âˆ’zA âˆ’zB + 1
g = âˆ’1
g (1 + XA)âˆ’1(1 âˆ’XAXB)(1 + XB)âˆ’1,
(
A + B âˆ’

zA + zB âˆ’1
g
)âˆ’1
= âˆ’g(1 + XB)(1 âˆ’XAXB)âˆ’1(1 + XA).
(11.58)
Using the identity
(1 âˆ’XAXB)âˆ’1 =
âˆ

n=0
(XAXB)n,
(11.59)
we can expand the expression
Ï„
&
(1 + XB)(1 âˆ’XAXB)âˆ’1(1 + XA)
'
,
(11.60)
which will contain 1 plus terms of the form Ï„(XAXBXAXB . . . XB) where the initial and
ï¬nal factor might be either XA or XB but the important point is that XA and XB always
alternate. By the freeness and zero mean of XA and XB, all these terms are thus zero. Hence
we get
Ï„
0(
A + B âˆ’

zA + zB âˆ’1
g
)âˆ’13
= âˆ’g â‡’gA+B(zA + zB âˆ’gâˆ’1) = g,
(11.61)
ï¬nally leading to the announced result:7
zA+B = zA + zB âˆ’gâˆ’1 â‡’RA+B = RA + RB.
(11.62)
7 The above compact proof is taken from Tao [2012].

166
Free Probabilities
11.3.4 R-Transform and Cumulants
The R-transform is deï¬ned as a power series in g. We claim that the coefï¬cients of this
power series are in fact exactly the non-commutative cumulants deï¬ned earlier. In other
words, RA(g) is the cumulant generating function:
RA(g) :=
âˆ

k=1
Îºk(A)gkâˆ’1.
(11.63)
To show that these coefï¬cients are indeed the cumulants we ï¬rst realize that the general
equality R(g) = z(g) âˆ’1/g is equivalent to
zgA(z) âˆ’1 = gA(z)RA(gA(z)).
(11.64)
We can compute the power series of the two sides of this equality:
zgA(z) âˆ’1 =
âˆ

k=1
mk
zk ,
(11.65)
where mk := Ï„(Ak) denotes the kth moment, and
gA(z)RA(gA(z)) =
âˆ

k=1
Îºk

1
z +
âˆ

â„“=1
ml
zâ„“+1
k
.
(11.66)
Equating the right hand sides of Eqs. (11.65) and (11.66) and matching powers of 1/z we
get recursive relations between moments (mk) and cumulants (Îºk):
m1 = Îº1
â‡’
m1 = Îº1,
m2 = Îº2 + Îº1m1
â‡’
m2 = Îº2 + Îº2
1,
m3 = Îº3 + 2Îº2m1 + Îº1m2
â‡’
m3 = Îº3 + 3Îº2Îº1 + Îº3
1,
m4 = Îº4 + 4Îº3m1 + Îº2[2m2 + m2
1] + Îº1m3
â‡’
m4 = Îº4 + 6Îº2Îº2
1 + 2Îº2
2 + 4Îº3Îº1 + Îº4
1.
(11.67)
By looking at the zâˆ’k term coming from the [1/z + Â· Â· Â· ]k term in Eq. (11.66) we realize
that mk = Îºk + Â· Â· Â· where â€œÂ· Â· Â· â€ are homogeneous combinations of lower order Îºk and mk.
Since the coefï¬cients of the power series Eq. (11.63) are additive under addition of free
variables and obey the property
Îºk(A) = Ï„(Ak) + homogeneous products of lower order moments,
(11.68)
they are therefore the cumulants deï¬ned in Section 11.3.2.
11.3.5 Cumulants and Non-Crossing Partitions
We saw that Eq. (11.64) can be used to compute cumulants iteratively. Actually that
equation can be translated into a systematic relation between moments and cumulants:
mn =

Ï€âˆˆNC(n)
ÎºÏ€1 Â· Â· Â· ÎºÏ€â„“Ï€ ,
(11.69)

11.3 Non-Commuting Variables
167
m4 =
+
+
+
+
+
+
+
+
+
+
+
+
+
Figure 11.1 List of all non-crossing partitions of four elements. In Eq. (11.69) for m4, the ï¬rst
partition contributes Îº1Îº1Îº1Îº1 = Îº4
1. The next six all contribute Îº2Îº2
1 and so forth. We read
m4 = Îº4
1 + 6Îº2Îº2
1 + 2Îº2
2 + 4Îº3Îº1 + Îº4.
mâ„“1
mâ„“2
mâ„“3
mâ„“4
mâ„“5
Îº5
Figure 11.2 A typical non-crossing diagram contributing to a large moment mn. In this example
the ï¬rst element is connected to four others (giving a factor of Îº5) which breaks the diagram into
ï¬ve disjoint non-crossing diagrams contributing a factor mâ„“1mâ„“2mâ„“3mâ„“4mâ„“5. Note that we must
have â„“1 + â„“2 + â„“3 + â„“4 + â„“5 = n.
where Ï€ âˆˆNC(n) indicates that the sum is over all possible non-crossing partitions of n
elements. For any such partition Ï€ the integers {Ï€1,Ï€2, . . . ,Ï€â„“Ï€ } (1 â‰¤â„“Ï€ â‰¤n) equal the
number elements in each group (see Fig. 11.3). They satisfy
n =
â„“Ï€

k=1
Ï€k.
(11.70)
We will show that, provided we deï¬ne cumulants by Eq. (11.69), we recover
Eq. (11.64). But before we do so, let us ï¬rst show this relation on a simple example.
Figure 11.1 shows the computation of the fourth moment in terms of the cumulants.
The argument is very similar to the recursion relation obtained for Catalan numbers
where we considered non-crossing pair partitions (see Section 3.2.3). Here the argument
is slightly more complicated as we have partitions of all possible sizes. We consider the
moment mn for n â‰¥1. We break down the sum over all non-crossing partitions of n
elements by looking at â„“, the size of the set containing the ï¬rst element (for example in
Fig. 11.3, the ï¬rst element belongs to a set of size â„“= 5). The size of this ï¬rst set can
be 1 â‰¤â„“â‰¤n. This initial set breaks the partition into â„“(possibly empty) disjoint smaller
partitions. They must be disjoint, otherwise there would be a crossing. In Figure 11.2 we
show how an initial 5-set breaks the full partition into 5 blocks. In each of these blocks,
every non-crossing partition is possible, the only constraint is that the total size of the
partition must be n. The sum over all possible non-crossing partitions of size k of the
relevant Îºâ€™s is the moment mk. Note that the empty partition contributes a multiplicative
factor 1, so we deï¬ne m0 â‰¡1. Putting everything together we obtain the following
recursion relation for mn:
mn =
n

â„“=1
Îºâ„“
,
k1,k2,...,kâ„“â‰¥0
k1+k2+Â·Â·Â·+kâ„“=nâˆ’â„“
mk1mk2 . . . mkâ„“.
(11.71)
Let us multiply both sides of this equation by zâˆ’n and sum over n from 1 to âˆ. The left
hand side gives zg(z) âˆ’1, by deï¬nition of g(z). The right hand side reads

168
Free Probabilities
Figure 11.3 Generic non-crossing partition of 23 elements with two singletons, ï¬ve doublets, two
triplets, and one quintent, such that 23 = 5+2Â·3+5Â·2+2Â·1. In Eq. (11.69), this particular partition
appears for m23 and contributes Îº5Îº2
3Îº5
2Îº2
1.
Ï„(A1A2A3) =
+
+
+
+
Figure 11.4 List of all non-crossing partitions of three elements. From this we get Eq. (11.74) for
three elements: Ï„(A1A2A3) = Îº1(A1)Îº1(A2)Îº1(A3) + Îº2(A1,A2)Îº1(A3) + Îº2(A1,A3)Îº1(A2) +
Îº2(A2,A3)Îº1(A3) + Îº3(A1,A2,A3).
âˆ

n=1
n

â„“=1
Îºâ„“
,
k1,k2,...,kâ„“â‰¥0
Î´k1+k2+Â·Â·Â·+kâ„“=nâˆ’â„“
mk1mk2 . . . mkâ„“
z1+k1z1+k2 . . . z1+kâ„“,
(11.72)
which can be transformed into
âˆ

â„“=1
Îºâ„“
â¡
â£
âˆ

k1=0
mk1
z1+k1
â¤
â¦
â„“
= g(z)R(g(z)),
(11.73)
where we have used Eq. (11.63). We thus recover exactly Eq. (11.64), showing that the
relation (11.69) is equivalent to our previous deï¬nition of the free cumulants.
It is interesting to contrast the momentâ€“cumulant relation in the standard (commuta-
tive) case (Eq. (11.23)) and the free (non-commutative) case (Eq. (11.69)). Both can be
written as a sum over all partitions on n elements; in the standard case all partitions are
allowed, while in the free case the sum is only over non-crossing partitions.
11.3.6 Freeness as the Vanishing of Mixed Cumulants
We have deï¬ned freeness in Section 11.3.1 as the property of two variables A and
B such that the trace of any mixed combination of traceless polynomials in A and in B
vanishes. There exists another equivalent deï¬nition of freeness, namely that every mixed
cumulant of A and B vanish. To make sense of this deï¬nition we ï¬rst need to introduce
cumulants of several variables. They are deï¬ned recursively by
Ï„(A1A2 . . . An) =

Ï€âˆˆNC(n)
ÎºÏ€(A1A2 . . . An),
(11.74)
where the Aiâ€™s are not necessarily distinct and NC(n) is the set of all non-crossing parti-
tions of n elements. Here
ÎºÏ€(A1A2 . . . An) = ÎºÏ€1(. . .). . . ÎºÏ€â„“(. . .)
(11.75)
are the products of cumulants of variables belonging to the same group of the correspond-
ing partition â€“ see Figure 11.4 for an illustration. We also call these generalized cumulants
the free cumulants.
When all the variables in Eq. (11.74) are the same (Ai = A) we recover the previous
deï¬nition of cumulants with a slightly different notation (e.g. Îº3(A,A,A) â‰¡Îº3(A)).

11.3 Non-Commuting Variables
169
Cumulants with more than one variable are called mixed cumulants (e.g. Îº4(A,A,B,A)).
By applying Eq. (11.74) we ï¬nd for the low generalized cumulants of two variables
m1(A) = Îº1(A),
m2(A,B) = Îº1(B)Îº1(B) + Îº2(A,B),
m3(A,A,B) = Îº1(A)2Îº1(B) + Îº2(A,A)Îº1(B) + 2Îº2(A,B)Îº1(A) + Îº3(A,A,B).
(11.76)
We can now state more precisely the alternative deï¬nition of freeness: a set of variables
is free if and only if all their mixed cumulants vanish. For example, in the low cumulants
listed above, freeness of A and B implies that Îº2(A,B) = Îº3(A,A,B) = 0.
This deï¬nition of freeness is easy to generalize to a collection of variables, i.e. a
collection of variables is free if all its mixed cumulants are zero. As noted at the end
of Section 11.3.7 below, pairwise freeness is not enough to ensure that a collection is free.
We remark that vanishing of mixed cumulants implies that free cumulants are additive.
In Speicherâ€™s notation, Îºk(A,B,C, . . .) is a multi-linear function in each of its arguments,
where k gives the number of variables. Thus we have
Îºk(A + B,A + B, . . .) = Îºk(A,A, . . .) + Îºk(B,B, . . .) + mixed cumulants
= Îºk(A,A, . . .) + Îºk(B,B, . . .),
(11.77)
i.e. Îºk is additive.
We will give a concrete application of the formalism of free mixed cumulants in
Section 12.2.
11.3.7 The Central Limit Theorem for Free Variables
We can now go back and re-read Section 11.2.4. We can replace every occurrence of the
word independent with free, and cumulant with free cumulant. The lln now states that the
sum of K free identically distributed (ï¬d) variables normalized by Kâˆ’1 converges to a
constant (also called a scalar) with the same mean.
Let us deï¬ne a free Wigner variable as a variable with second free cumulant Îº2 > 0 and
all other free cumulants equal to zero. In other words, a free Wigner variable is such that
RW(x) = Îº2x. The clt then states that the sum of K zero-mean free identical variables
normalized by Kâˆ’1/2 converges to a free Wigner variable with the same second cumulant.
In the case where our free random variables are large symmetric random matrices,
the Wigner deï¬ned here by its cumulant coincides with the Wigner matrices deï¬ned in
Chapter 2. We indeed saw that the R-transform of a Wigner matrix is given by R(x) = Ïƒ 2x,
i.e. the cumulant generating function has a single term corresponding to Îº2 = Ïƒ 2.
Alternatively, we note that the moments of a Wigner are given by the sum over non-
crossing pair partitions (Eq. (3.26)). Comparing with Eq. (11.69), we realize that partitions
containing anything other than pairs must contribute zero, hence only the second cumulant
of the Wigner is non-zero.
The lln and the clt require variables to be collectively free, in the sense that all mixed
cumulants are zero. As is the case with independence, pairwise freeness is not enough to
ensure freeness as a collection (see footnote on page 161). Indeed, in Section 12.5 we will
encounter variables that are pairwise free but not free as a collection. One can have A
and B mutually free and both free with respect to C but A + B is not free with respect

170
Free Probabilities
to C. This does not happen for rotationally invariant large matrices but can arise in other
constructions. The deï¬nition of a free collection is just an extension of deï¬nition (11.40)
including traceless polynomials in all variables in the collection. With this deï¬nition, sums
of variables in the collection are free from those not included in the sum (e.g. A + B is
free from C).
11.3.8 Subordination Relation for Addition of Free Variables
We now introduce the subordination relation for free addition, which is just a rewriting of
the addition of R-transforms. For free A and B, we have
RA(g) + RB(g) = RA+B(g) â‡’zA(g) + RB(g) = zA+B(g),
(11.78)
where
gA+B(zA+B) = g = gA(zA) = gA (zA+B âˆ’RB(g)) .
(11.79)
We call z := zA+B(g), then the above relations give
gA+B(z) = gA (z âˆ’RB(gA+B(z))),
(11.80)
which is called a subordination relation (compare with Eq. (10.2)).
11.4 Free Product
In the previous section, we have studied the property of the sum of free random variables. In
the case of commuting variables, the question of studying the product of (positive) random
variables is trivial, since taking the logarithm of this product we are back to the problem of
sums again. In the case of non-commuting variables, things are more interesting. We will
see below that one needs to introduce the so-called S-transform, which is the counterpart
of the R-transform for products of free variables.
We start by noticing that the free product of traceless variables is trivial. If A,B are free
and Ï„(A) = Ï„(B) = 0, we have
Ï„((AB)k) = Ï„(ABAB . . . AB) = 0.
(11.81)
11.4.1 Low Moments of Free Products
We will now compute the ï¬rst few moments of the free products of two variables with a
non-zero trace: C := AB where A,B are free and Ï„(A)  0, Ï„(B)  0. Without loss of
generality, we can assume that Ï„(A) = Ï„(B) = 1 by rescaling A and B. Then
Ï„(C) = Ï„ ((A âˆ’Ï„(A))(B âˆ’Ï„(B))) + Ï„(A)Ï„(B) = Ï„(A)Ï„(B) = 1.
(11.82)
We can also use (11.74) to get
Ï„(C) = Îº2(A,B) + Îº1(A)Îº1(B) = Îº1(A)Îº1(B) = 1,
(11.83)

11.4 Free Product
171
m4 =
+
+
+
+
+
+
+
+
+
+
+
Figure 11.5 List of all non-crossing partitions of six elements contributing to Ï„(ABABAB)
excluding mixed AB terms. Terms involving A are in thick gray and B in black. Equation (11.86)
can be read off from these diagrams. Note that Îº1(A) = Îº2(B) = 1.
since mixed cumulants are zero for mutually free variables. Similarly, using Eq. (11.74) we
can get that (see Fig. 11.1 for the non-crossing partitions of four elements)
Ï„(C2) = Ï„(ABAB) = Îº1(A)2Îº1(B)2 + Îº2(A)Îº1(B)2 + Îº1(A)2Îº2(B)
= 1 + Îº2(A) + Îº2(B),
(11.84)
which gives
Îº2(C) := Ï„(C2) âˆ’Ï„(C)2 = Îº2(A) + Îº2(B).
(11.85)
For the third moment of C = AB, we can follow Figure 11.5 and get
Ï„(C3) = Ï„(ABABAB)
= 1 + 3Îº2(A) + 3Îº2(B) + 3Îº2(A)Îº2(B) + Îº3(A) + Îº3(B),
(11.86)
leading to
Îº3(C) : = Ï„(C3) âˆ’3Ï„(C2)Ï„(C) + 2Ï„(C)3
= Îº3(A) + Îº3(B) + 3Îº2(A)Îº2(B).
(11.87)
Under free products of unit-trace variables, the mean remains equal to one and the
variance is additive. The third cumulant is not additive; it is strictly greater than the sum of
the third cumulants unless one of the two variables is the identity (unit scalar).
11.4.2 Deï¬nition of the S-Transform
We will now show that the above relations can be encoded into the S-transform S(t) which
is multiplicative for products of free variables:
SAB(t) = SA(t)SB(t)
(11.88)
for A and B free. To deï¬ne the S-transform, we ï¬rst introduce the T-transform as
tA(Î¶) = Ï„
&
(1 âˆ’Î¶ âˆ’1A)âˆ’1'
âˆ’1
= Î¶gA(Î¶) âˆ’1
=
âˆ

k=1
mk
Î¶ k .
(11.89)

172
Free Probabilities
The behavior at inï¬nity of the T-transform depends explicitly on m1, the ï¬rst moment of A
(t(Î¶) âˆ¼m1/Î¶), unlike the Stieltjes transform which always behaves as 1/z.
The T-transform has the same singularities as the Stieltjes transform except maybe at
zero. When A is a matrix, one can recover the continuous part of its eigenvalue density
Ï(x) using the following T-version of the Sokhotskiâ€“Plemelj formula:
lim
Î·â†’0+ Im t(x âˆ’iÎ·) = Ï€xÏ(x).
(11.90)
Poles in the T-transform indicate Dirac masses. If t(Î¶) âˆ¼A/(Î¶ âˆ’Î»0) near Î»0 then the
density is a Dirac mass of amplitude A/Î»0 at Î»0. The behavior at zero of the T-transform is
a bit different from that of the Stieltjes transform. A regular density at zero gives a regular
Stieltjes and hence t(0) = âˆ’1. Deviations from this value indicate a Dirac mass at zero,
hence when t(0)  âˆ’1, the density has a Dirac at zero of amplitude t(0) + 1.
The T-transform can also be written as
tA(Î¶) = Ï„
&
A (Î¶ âˆ’A)âˆ’1'
.
(11.91)
We deï¬ne Î¶A(t) to be the inverse function of tA(Î¶). When m1  0, tA is invertible for large
Î¶, and hence Î¶A exists for small enough t. We then deï¬ne the S-transform as8
SA(t) := t + 1
tÎ¶A(t),
(11.92)
for variables A such that Ï„(A)  0.
Let us compute the S-transform of the identity S1(t):
t1(Î¶) =
1
Î¶ âˆ’1 â‡’Î¶1(t) = t + 1
t
â‡’S1(t) = 1,
(11.93)
as expected as the identity is free with respect to any variable. The S-transform scales in a
simple way with the variable A. To ï¬nd its scaling we ï¬rst note that
tÎ±A(Î¶) = Ï„
&
(1 âˆ’(Î±âˆ’1Î¶)âˆ’1A)âˆ’1'
âˆ’1 = tA(Î¶/Î±),
(11.94)
which gives
Î¶Î±A(t) = Î±Î¶A(t).
(11.95)
Then, using (11.92), we get that
SÎ±A(t) = Î±âˆ’1SA(t).
(11.96)
The above scaling is slightly counterintuitive but it is consistent with the fact that
SA(0) = 1/Ï„(A). We will be focusing on unit trace objects such that S(0) = 1.
8 Most authors prefer to deï¬ne the S-transform in terms of the moment generating function Ïˆ(z) := t(1/z). The deï¬nition
S(t) = Ïˆâˆ’1(t)(t + 1)/t is equivalent to ours (Ïˆâˆ’1(t) is the functional inverse of Ïˆ(z)). We prefer to work with the
T-transform as the function t(Î¶) has an analytic structure very similar to that of g(z). The function Ïˆ(z) is analytic near zero
and singular for large values of z corresponding to the reciprocal of the eigenvalues.

11.4 Free Product
173
The construction of the S-transform relies on the properties of mixed moments of free
variables. In that respect it is closely related to the R-transform. Using the relation tA(Î¶) =
Î¶gA(Î¶) âˆ’1, one can get the following relationships between RA and SA:
SA(t) =
1
RA(tSA(t)),
RA(g) =
1
SA(gRA(g)).
(11.97)
11.4.3 Multiplicativity of the S-Transform
We can now show the multiplicative property (11.88). The proof is similar to the one
given for the additive case and is adapted from it.
We ï¬x t and let Î¶A and Î¶B be the inverse T-transforms of tA and tB. Then we deï¬ne
EA through
1 + t + EA = (1 âˆ’A/Î¶A)âˆ’1,
(11.98)
and similarly for EB. We have Ï„(EA) = 0, Ï„(EB) = 0, and, since A and B are free,
EA,EB are also free. Then we have
A
Î¶A
= 1 âˆ’(1 + t + EA)âˆ’1,
(11.99)
which gives
AB
Î¶AÎ¶B
=
&
1 âˆ’(1 + t + EA)âˆ’1' &
1 âˆ’(1 + t + EB)âˆ’1'
= (1 + t + EA)âˆ’1 [(t + EA)(t + EB)] (1 + t + EB)âˆ’1.
(11.100)
Using the identity
t(EA + EB) =
t
1 + t
&
(1 + t + EA)(1 + t + EB) âˆ’(1 + t)2 âˆ’EAEB
'
,
(11.101)
we can rewrite the above expression as
AB
Î¶AÎ¶B
=
t
1 + t + (1 + t + EA)âˆ’1
(
âˆ’t + EAEB
1 + t
)
(1 + t + EB)âˆ’1
â‡’1 âˆ’1 + t
t
AB
Î¶AÎ¶B
= (1 + t)(1 + t + EA)âˆ’1
(
1 âˆ’EAEB
t(1 + t)
)
(1 + t + EB)âˆ’1
â‡’
(
1 âˆ’1 + t
t
AB
Î¶AÎ¶B
)âˆ’1
=
1
1 + t (1 + t + EB)
(
1 âˆ’EAEB
t(1 + t)
)âˆ’1
(1 + t + EA).
(11.102)
Using the expansion
(
1 âˆ’EAEB
t(1 + t)
)âˆ’1
=

n=0
 EAEB
t(1 + t)
n
,
(11.103)
one can check that
Ï„
-
(1 + t + EB)
(
1 âˆ’EAEB
t(1 + t)
)âˆ’1
(1 + t + EA)
.
= (1 + t)2,
(11.104)

174
Free Probabilities
where we used the freeness condition for EA and EB. Thus we get that
Ï„
0(
1 âˆ’1 + t
t
AB
Î¶AÎ¶B
)âˆ’13
= 1 + t
â‡’
tAB
tÎ¶AÎ¶B
1 + t

= t,
(11.105)
which gives that
SAB(t) = SA(t)SB(t)
(11.106)
thanks to the deï¬nition (11.92).
11.4.4 Subordination Relation for the Free Product
We next derive a subordination relation for the free product using (11.88) and (11.92):
SAB(t) = SA(t)SB(t)
â‡’
Î¶AB(t) = Î¶A(t)
SB(t),
(11.107)
where
tAB(Î¶AB(t)) = t = tA(Î¶A(t)) = tA(Î¶AB(t)SB(t)).
(11.108)
We call Î¶ := Î¶AB(t), then the above relations give
tAB(Î¶) = tA (Î¶SB(tAB(Î¶))),
(11.109)
which is the subordination relation for the free product. In fact, the above is true even when
SA does not exist, e.g. when Ï„(A) = 0.
When applied to free random matrices, the form AB is not very useful since it is not
necessarily symmetric even if A and B are. But if A âª°0 (i.e. A is positive semi-deï¬nite
symmetric) and B is symmetric, then A
1
2 BA
1
2 has the same moments as AB and is also
symmetric. In our applications below we will always encounter the case A âª°0 and call
A
1
2 BA
1
2 the free product of A and B.
Exercise 11.4.1
Properties of the S-transform
(a)
Using Eq. (11.92), show that
R(x) =
1
S(xR(x)).
(11.110)
Hint: deï¬ne t = xR(x) = zg âˆ’1 and identify x as g.
(b)
For a variable such that Ï„(M) = Îº1 = 1, write S(t) as a power series in t,
compute the ï¬rst few terms of the powers series, up to (and including) the t2
term, using Eq. (11.110) and Eq. (11.63). You should ï¬nd
S(t) = 1 âˆ’Îº2t + (2Îº2
2 âˆ’Îº3)t2 + O(t3).
(11.111)

11.4 Free Product
175
(c)
We have shown that, when A and B are mutually free with unit trace,
Ï„(AB) = 1,
(11.112)
Ï„(ABAB) âˆ’1 = Îº2(A) + Îº2(B),
(11.113)
Ï„(ABABAB) = Îº3(A) + Îº3(B) + 3Îº2(A)Îº2(B) + 3(Îº2(A) + Îº2(B)) + 1.
(11.114)
Show that these relations are compatible with SAB(t) = SA(t)SB(t) and the
ï¬rst few terms of your power series in (b).
(d)
Consider M1 = 1 + Ïƒ1W1 and M2 = 1 + Ïƒ2W2 where W1 and W2 are
two different (free) unit Wigner matrices and both Ïƒâ€™s are less than 1/2. M1
and M2 have Îº3 = 0 and are positive deï¬nite in the large N limit. What is
Îº3(M1M2)?
Exercise 11.4.2
S-transform of the matrix inverse
(a)
Consider M an invertible symmetric random matrix and Mâˆ’1 its inverse.
Using Eq. (11.89), show that
tM(Î¶) + tMâˆ’1
1
Î¶

+ 1 = 0.
(11.115)
(b)
Using Eq. (11.115), show that
SMâˆ’1(x) =
1
SM(âˆ’x âˆ’1).
(11.116)
Hint: write u(x) = 1/Î¶(t) where u(x) is such that x = tMâˆ’1(u(x)). Equation
(11.115) is then equivalent to x = âˆ’1 âˆ’t.
Bibliographical Notes
â€¢ The concept of freeness was introduced in
â€“ D. Voiculescu. Symmetries of some reduced free product C*-algebras. In H. Araki
et al. (eds.), Operator Algebras and Their Connections with Topology and Ergodic
Theory. Lecture Notes in Mathematics, volume 1132. Springer, Berlin, Heidelberg,
1985,
â€“ D. Voiculescu. Limit laws for random matrices and free products. Inventiones math-
ematicae, 104(1):201â€“220, 1991,
with the second reference making the link to rmt.
â€¢ For general introductions to freeness, see
â€“ J. A. Mingo and R. Speicher. Free Probability and Random Matrices. Springer, New
York, 2017,

176
Free Probabilities
â€“ J. Novak. Three lectures on free probability. In Random Matrix Theory, Interacting
Particle Systems, and Integrable Systems. Cambridge University Press, Cambridge,
2014.
â€¢ For a more operational approach to freeness, see
â€“ A. M. Tulino and S. VerdÂ´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,.
â€¢ The proof of the additivity of the R-transform presented here is from
â€“ T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
see also
â€“ P. Zinn-Justin. Adding and multiplying random matrices: A generalization of
Voiculescuâ€™s formulas. Physical Review E, 59:4884â€“4888, 1999,
for an alternative point of view.
â€¢ Central limit theorems and LÂ´evy-stable free variables:
â€“ H. Bercovici, V. Pata, and P. Biane. Stable laws and domains of attraction in free
probability theory. Annals of Mathematics, 149(3):1023â€“1060, 1999,
â€“ Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, and I. Zahed. Free random LÂ´evy and
Wigner-LÂ´evy matrices. Physical Review E, 75:051126, 2007.

12
Free Random Matrices
In the last chapter, we introduced the concept of freeness rather abstractly, as the proper
non-commutative generalization of independence for usual random variables. In the present
chapter, we explain why large, randomly rotated matrices behave as free random variables.
This justiï¬es the use of R-transforms and S-transforms to deal with the spectrum of sums
and products of large random matrices. We also revisit the abstract central limit theorem
of the previous chapter (Section 11.2.4) in the more concrete case of sums of randomly
rotated matrices.
12.1 Random Rotations and Freeness
12.1.1 Statement of the Main Result
Recall the deï¬nition of freeness. A and B are free if for any set of traceless polynomials
p1, . . . ,pn and q1, . . . ,qn the following equality holds:
Ï„ (p1(A)q1(B)p2(A)q2(B). . . pn(A)qn(B)) = 0.
(12.1)
In order to make the link with large matrices we will consider A and B to be large symmetric
matrices and Ï„(M) := 1/N Tr(M). The matrix A can be diagonalized as UUT and B as
Vâ€²VT . A traceless polynomial pi(A) can be diagonalized as UiUT , where U is the same
orthogonal matrix as for A itself and i = pi() is some traceless diagonal matrix, and
similarly for qi(B). Equation (12.1) then becomes
Ï„

1Oâ€²
1OT 2Oâ€²
2OT . . . nOâ€²
nOT 
= 0,
(12.2)
where we have introduced O = UT V as the orthogonal matrix of basis change rotating the
eigenvectors of A into those of B.
As we argue below, in the large N limit Eq. (12.2) always holds true when averaged over
the orthogonal matrix O and whenever matrices i and â€²
i are traceless. We also expect
that in the large N limit Eq. (12.2) becomes self-averaging, so a single matrix O behaves as
the average over all such matrices. Hence, two large symmetric matrices whose eigenbases
are randomly rotated with respect to one another are essentially free. For example, Wigner
matrices X and white Wishart matrices W are rotationally invariant, meaning that the
177

178
Free Random Matrices
matrices of their eigenvectors are random orthogonal matrices. We conclude that for N
large, both X and W are free with respect to any matrix independent from them, in particular
they are free from any deterministic matrix.
12.1.2 Integration over the Orthogonal Group
We now come back to the central statement that in the large N limit the average over O
of Eq. (12.2) is zero for traceless matrices i and â€²
i. In order to compute quantities like
8
Ï„

1Oâ€²
1OT 2Oâ€²
2OT . . . nOâ€²
nOT 9
O ,
(12.3)
one needs to understand how to compute the following moments of rotation matrices,
averaged over the Haar (ï¬‚at) measure over the orthogonal group O(N):
I(i,j,n) :=
8
Oi1j1Oi2j2 . . . Oi2nj2n
9
O .
(12.4)
The general formula has been worked out quite recently and involves the Weingarten
functions. A full discussion of these functions is beyond the scope of this book, but
we want to give here a brief account of the structure of the result. When N â†’âˆ, the
leading term is quite simple: one recovers the Wickâ€™s contraction rules, as if Oi1j1 were
independent random Gaussian variables with variance 1/N. Namely,
I(i,j,n) = Nâˆ’n

pairings Ï€
Î´iÏ€(1)iÏ€(2)Î´jÏ€(1)jÏ€(2) . . . Î´iÏ€(2nâˆ’1)iÏ€(2n)Î´jÏ€(2nâˆ’1)jÏ€(2n) + O(Nâˆ’nâˆ’1).
(12.5)
Note that all pairings of the i-indices are the same as those of the j-indices. For example,
for n = 1 and n = 2 one has explicitly, for N â†’âˆ,
N
8
Oi1j1Oi2j2
9
O = Î´i1i2Î´j1j2
(12.6)
and
N2 8
Oi1j1Oi2j2Oi3j3Oi4j4
9
O = Î´i1i2Î´j1j2Î´i3i4Î´j3j4
+ Î´i1i3Î´j1j3Î´i2i4Î´j2j4
+ Î´i1i4Î´j1j4Î´i2i3Î´j2j3.
(12.7)
The case n = 1 is exact and has no subleading corrections in N, so we can use it to
compute
8
Ï„(1Oâ€²
1OT )
9
O = Nâˆ’1
N

i,j=1
(1)i
:
Oij(â€²
1)jOT
ji
;
O
= Nâˆ’2
N

i,j=1
(1)i(â€²
1)j
= Ï„(1)Ï„(â€²
1).
(12.8)
(Recall that Ï„(A) is equal to Nâˆ’1 Tr A.) Clearly the result is zero when Ï„(1) =
Ï„(â€²
1) = 0, as required by the freeness condition.

12.1 Random Rotations and Freeness
179
Figure 12.1 Number of loops â„“(Ï€,Ï€) for Ï€ = (1,2)(3,6)(4,5). The bottom part of the diagram (thick
gray) corresponds to the ï¬rst partition Ï€ and the top part (black) to the second partition, here also
equal to Ï€. In this example there are three loops each of size 2.
Now, using only the leading Wick terms for n = 2 and after some index contractions
and manipulations, one would obtain
lim
Nâ†’âˆ
8
Ï„(1Oâ€²
1OT 2Oâ€²
2OT )
9
O = Ï„(12)Ï„(â€²
1)Ï„(â€²
2) + Ï„(1)Ï„(2)Ï„(â€²
1â€²
2).
(12.9)
However, this cannot be correct. Take for example 1 = 2 = 1, for which Ï„(1Oâ€²
1OT
2Oâ€²
2OT )
=
Ï„(â€²
1â€²
2) exactly, whereas the formula above adds an extra term
Ï„(â€²
1)Ï„(â€²
2). The correct formula actually reads
lim
Nâ†’âˆ
8
Ï„(1Oâ€²
1OT 2Oâ€²
2OT )
9
O = Ï„(12)Ï„(â€²
1)Ï„(â€²
2) + Ï„(1)Ï„(2)Ï„(â€²
1â€²
2)
âˆ’Ï„(1)Ï„(2)Ï„(â€²
1)Ï„(â€²
2),
(12.10)
which again is zero whenever all individual traces are zero (i.e. the freeness condition).
12.1.3 Beyond Wick Contractions: Weingarten Functions
Where does the last term in Eq. (12.10) come from? The solution to this puzzle lies in
the fact that some subleading corrections to Eq. (12.7) also contribute to the trace we are
computing: summing over indices from 1 to N can prop up some subdominant terms and
make them contribute to the ï¬nal result. Hence we need to know a little more about the
Weingarten functions. This will allow us to conclude that the freeness condition holds for
arbitrary n. The general Weingarten formula reads
I(i,j,n) =

pairings Ï€,Ïƒ
Wn(Ï€,Ïƒ)Î´iÏ€(1)iÏ€(2)Î´jÏƒ(1)jÏƒ(2) . . . Î´iÏ€(2nâˆ’1)iÏ€(2n)Î´jÏƒ(2nâˆ’1)jÏƒ(2n),
(12.11)
where now the pairings Ï€ of iâ€™s and Ïƒ of jâ€™s do not need to coincide. The Weingarten
functions Wn(Ï€,Ïƒ) can be thought of as matrices with pairings as indices. They are given
by the pseudo-inverse1 of the matrices Mn(Ï€,Ïƒ) := Nâ„“(Ï€,Ïƒ), where â„“(Ï€,Ïƒ) is the number
of loops obtained when superposing Ï€ and Ïƒ. For example, when Ï€ = Ïƒ one ï¬nds n
loops, each of length 2, see Figure 12.1. While when Ï€  Ïƒ the number of loops is always
less than n (â„“(Ï€,Ïƒ) < n), see Figure 12.2. At large N, the diagonal of the matrix Mn
dominates and the matrix is always invertible. By expanding in powers of 1/N, we see
that its inverse Wn, whose elements are the Weingarten functions, has an Nâˆ’n behavior
1 The pseudo-inverse of M is such that WMW = W and MWM = M. When M is invertible, W = Mâˆ’1. If M is
diagonalizable, the eigenvalues of W are the reciprocal of those of M with the rule 1/0 â†’0.

180
Free Random Matrices
Figure 12.2 Number of loops â„“(Ï€,Ïƒ) for Ï€ = (1,4)(2,3) and Ïƒ = (1,2)(3,4). In this example there
is only one loop.
on the diagonal and off-diagonal terms are at most Nâˆ’nâˆ’1. More generally, one has the
following beautiful expansion:
Wn(Ï€,Ïƒ) = Nâ„“(Ï€,Ïƒ)âˆ’2n
âˆ

g=0
 g(Ï€,Ïƒ)Nâˆ’g,
(12.12)
where the coefï¬cients  g(Ï€,Ïƒ) depend on properties of certain â€œgeodesic pathsâ€ in the
space of partitions. The bottom line of this general expansion formula is that non-Wick
contractions are of higher order in Nâˆ’1.
As an illustration, let us come back to the missing term in Eq. (12.9) when one
restricts to Wick contractions, for which the number of loops â„“(Ï€,Ï€) = 2. The next term
corresponds to â„“(Ï€,Ïƒ) = 1 for which W2(Ï€,Ïƒ) âˆ¼âˆ’Nâˆ’3 (see Exercise 12.1.1). Consider
the pairings i1 = i4, i2 = i3 and j1 = j2, j3 = j4, for which â„“(Ï€,Ïƒ) = 1 (see Fig. 12.2).
Such pairings do not add any constraint on the 2n = 4 free indices that appear in
Eq. (12.3); summing over them thus yields Ï„(1)Ï„(2)Ï„(â€²
1)Ï„(â€²
2), with a âˆ’1 coming
from the Weingarten function W2(Ï€,Ïƒ). Hence we recover the last term of Eq. (12.10).
Exercise 12.1.1
Exact Weingarten functions at n = 2
There are three possible pair partitions of four elements. They are shown
on Figure 3.2. If we number these Ï€1,Ï€2 and Ï€3, M2 is a 3 Ã— 3 matrix whose
elements are equal to Nâ„“(Ï€i,Ï€j ).
(a)
By trying a few combinations of the three pairings, convince yourself that
for n = 2, â„“(Ï€i,Ï€j) = 2 if i = j and 1 otherwise.
(b)
Build the matrix M2. For N > 1 it is invertible, ï¬nd its inverse W2. Hint:
use an ansatz for W2 with a variable a on the diagonal and b off-diagonal.
For N = 1 ï¬nd the pseudo-inverse of M2.
(c)
Finally show that (when N > 1) of the nine pairs of pairings, the three
Wick contractions (diagonal elements) have
WWick
2
=
N + 1
N3 + N2 âˆ’2N
Nâ†’âˆ
â†’
Nâˆ’2,
(12.13)
and the six non-Wick parings (off-diagonal) have
Wnon-Wick
2
= âˆ’
1
N3 + N2 âˆ’2N
Nâ†’âˆ
â†’
âˆ’Nâˆ’3.
(12.14)
For N = 1, all nine Weingarten functions are equal to 1/9.
(d)
The expression âŸ¨Ï„(OOT OOT )âŸ©is always equal to 1. Write it as a sum over
four indices and expand the expectation value over orthogonal matrices as
nine terms each containing two Dirac deltas multiplied by a Weingarten
function. Each sum of delta terms gives a power of N; ï¬nd these for all
nine terms and using your result from (c), show that indeed the Weingarten
functions give âŸ¨Ï„(OOT OOT )âŸ©= 1 for all N.

12.2 R-Transforms and Resummed Perturbation Theory
181
12.1.4 Freeness of Large Matrices
We are now ready to show that all expectations of the form (12.3) are zero. Let us look
at them more closely:
8
Ï„

1Oâ€²
1OT 2Oâ€²
2OT . . . nOâ€²
nOT 9
O
= 1
N

ij
I(i,j,n)[1]i2ni1[2]i2i3 . . . [n]i2nâˆ’2i2nâˆ’1[â€²
1]j1j2[â€²
2]j3j4 . . . [â€²
n]j2nâˆ’1j2n.
(12.15)
The object I(i,j,n) contains all possible pairings of the i indices and all those of the j
indices with a Weingarten function as its prefactor. The i and j indices never mix. We
concentrate on i pairings. Each pairing will give rise to a product of normalized traces of
i matrices. For example, the term
Ï„(5)Ï„(412)Ï„(36)
(12.16)
would appear for n = 6. Each normalized trace introduces a factor of N when going from
Tr(.) to Ï„(.). Since by hypothesis Ï„(i) = 0, the maximum number of non-zero traces is
âŒŠn/2âŒ‹, e.g.
Ï„(13)Ï„(56)Ï„(24).
(12.17)
The maximum factor of N that can be generated is therefore NâŒŠn/2âŒ‹. Applying the same
reasoning to the j pairing, and using the fact that the Weingarten function is at most
O(Nâˆ’n), we ï¬nd
8
Ï„

1Oâ€²
1OT 2Oâ€²
2OT . . . nOâ€²
nOT 9
O
 â‰¤O

Nâˆ’1+2âŒŠn/2âŒ‹âˆ’n Nâ†’âˆ
â†’
0.
(12.18)
Using the same arguments one can shown that large unitary invariant complex Her-
mitian random matrices are free. In this case one should consider an integral of uni-
tary matrices in Eq. (12.4). The result is also given by Eq. (12.11) where the functions
Wn(Ï€,Ïƒ) are now unitary Weingarten functions. They are different than the orthogonal
Weingarten functions presented above but they share an important property in the large N
limit, namely
Wn(Ï€,Ïƒ) =
0
O(Nâˆ’n) if Ï€ = Ïƒ,
O(Nâˆ’nâˆ’k) if Ï€  Ïƒ for some k â‰¥1,
(12.19)
which was the property needed in our proof of freeness of large rotationally invariant
symmetric matrices.
12.2 R-Transforms and Resummed Perturbation Theory
In this section, we want to explore yet another route to obtain the additivity of R-
transforms, which makes use of perturbation theory and of the mixed cumulant calculus
introduced in the last chapter, Section 11.3.6, exploited in a concrete case.
We want to study the average Stieltjes transform of A + BR where BR is a randomly
rotated matrix B: BR := OBOT . We thus write
g(z) :=
:
Ï„

(z1 âˆ’A âˆ’BR)âˆ’1;
O := Ï„R

(z1 âˆ’A âˆ’BR)âˆ’1
,
(12.20)

182
Free Random Matrices
where Ï„R is meant for both the normalized trace Ï„ and the average over the Haar measure
of the rotation group. We now formally expand the resolvent in powers of BR. Introducing
GA = (z1 âˆ’A)âˆ’1, one has
g(z) = Ï„R(GA) + Ï„R(GABRGA) + Ï„R(GABRGABRGA) + Â· Â· Â· .
(12.21)
Now, since in the large N limit GA and BR are free, we can use the general tracial formula,
Eq. (11.74), noting that all mixed cumulants (containing both GA and BR) are identically
zero.
In order to proceed, one needs to introduce three types of mixed moments where BR
appears exactly n times:
m(1)
n
:= Ï„R(GABRGA . . . GABRGA),
m(2)
n
:= Ï„R(BRGA . . . GABR)
(12.22)
and
m(3)
n
:= Ï„R(BRGA . . . BRGA) = Ï„R(GABR . . . GABR).
(12.23)
Note that m(1)
0
â‰¡gA(z) and m(2)
0
= m(3)
0
= 0. We also introduce, for full generality, the
corresponding generating functions:
ËœM(a)(u) =
âˆ

n=0
m(a)
n un,
a = 1,2,3.
(12.24)
Note however that we will only be interested here in g(z) :=
ËœM(1)(u = 1) (cf. Eq.
(12.21)).
Let us compute m(1)
n
using the same method as in Section 11.3.5, i.e. expanding in the
size â„“of the group to which the ï¬rst GA belongs (see Eq. (11.71)):
m(1)
n
=
n+1

â„“=1
ÎºGA,â„“
,
k1,k2,...,kâ„“â‰¥0
k1+k2+Â·Â·Â·+kâ„“=nâˆ’â„“
m(2)
k1 m(2)
k2 . . . m(2)
kâ„“âˆ’1m(3)
kâ„“,
(12.25)
where n â‰¥1 and ÎºGA,â„“are the free cumulants of GA. Similarly,
m(2)
n
=
n

â„“=1
ÎºB,â„“
,
k1,k2,...,kâ„“â‰¥0
k1+k2+Â·Â·Â·+kâ„“=nâˆ’â„“
m(1)
k1 m(1)
k2 . . . m(1)
kâ„“âˆ’1m(3)
kâ„“
(12.26)
and
m(3)
n
=
n+1

â„“=1
ÎºB,â„“
,
k1,k2,...,kâ„“â‰¥0
k1+k2+Â·Â·Â·+kâ„“=nâˆ’â„“
m(1)
k1 m(1)
k2 . . . m(1)
kâ„“,
(12.27)
where ÎºB,â„“are the free cumulants of B. Multiplying both sides of these equations by un
and summing over n leads to, respectively,
ËœM(1)(u) = gA(z) +
âˆ

â„“=1
ÎºGA,â„“uâ„“[ ËœM(2)(u)]â„“âˆ’1 ËœM(3)(u),
(12.28)

12.3 The Central Limit Theorem for Matrices
183
and
ËœM(2)(u) =
âˆ

â„“=1
ÎºB,â„“uâ„“[ ËœM(1)(u)]â„“âˆ’1 ËœM(3)(u),
ËœM(3)(u) =
âˆ

â„“=1
ÎºB,â„“uâ„“[ ËœM(1)(u)]â„“.
(12.29)
Recalling the deï¬nition of R-transforms as a power series of cumulants, we thus get
ËœM(1)(u) = gA(z) + u ËœM(3)(u)RGA

u ËœM(2)(u)

(12.30)
and
ËœM(2)(u) = u ËœM(3)(u)RB

u ËœM(1)(u)

,
ËœM(3)(u) = u ËœM(1)(u)RB

u ËœM(1)(u)

.
(12.31)
Eliminating ËœM(2)(u) and ËœM(3)(u) and setting u = 1 then yields the following relation:
g(z) = gA(z) + g(z) RB(g(z)) RGA

g(z)R2
B(g(z))

.
(12.32)
In order to rewrite this result in more familiar terms, let us consider the case where B = b1,
in which case RB(z) â‰¡b and, since A+B = A+b1, g(z) â‰¡gA(zâˆ’b). Hence, for arbitrary
b, RGA must obey the relation
RGA(b2g(z)) = g(z) âˆ’g(z + b)
bg(z)
.
(12.33)
Now, if for a ï¬xed z we set b = RB(g(z)), we ï¬nd that Eq. (12.32) is obeyed provided
g(z) = gA(z âˆ’RB(g(z))), i.e. precisely the subordination relation Eq. (11.80).
12.3 The Central Limit Theorem for Matrices
In the last chapter, we brieï¬‚y discussed the extension of the clt for non-commuting vari-
ables. We now restate the result in the context of random matrices, with a special focus on
the preasymptotic (cumulant) corrections to the Wigner distribution.
Let us consider the following sum of K large, randomly rotated matrices, all assumed to
be traceless:
MK :=
1
âˆš
K
K

i=1
OiAiOT
i ,
Tr Ai = 0,
(12.34)
where Oi are independent, random rotation matrices, chosen with a ï¬‚at measure over
the orthogonal group O(N). We also assume, for simplicity, that all Ai have the same
(arbitrary) moments:
Ï„(Aâ„“
i ) â‰¡mâ„“,
âˆ€i.
(12.35)
This means in particular that all Aiâ€™s share the same R-transform:
RAi(z) â‰¡
âˆ

â„“=2
Îºâ„“zâ„“âˆ’1.
(12.36)

184
Free Random Matrices
Using the fact that R-transforms of randomly rotated matrices simply add, together with
RÎ±M(z) = RM(Î±z), one ï¬nds
RMK(z) =
âˆ

â„“=2
K1âˆ’â„“/2Îºâ„“zâ„“âˆ’1.
(12.37)
We thus see that, as K becomes large, all free cumulants except the second one tend to
zero, which implies that the limit of MK when K goes to inï¬nity is a Wigner matrix, with
a semi-circle eigenvalue spectrum.
It is interesting to study the ï¬nite K corrections to this result. First, assume that the
spectrum of Ai is not symmetric around zero, such that the skewness m3 = Îº3  0. For
large K, the R-transform of MK can be approximated as
RMK(z) â‰ˆÏƒ 2z +
Îº3
âˆš
K
z2 + Â· Â· Â· .
(12.38)
In order to derive the corrections to the semi-circle induced by skewness, we posit that the
Stieltjes transform can be expanded around the Wigner result gX(z) as
gMK(z) = gX(z) +
Îº3
âˆš
K
g3(z) + Â· Â· Â·
(12.39)
and assume the second term to be very small. The R-transform, RMK(z), can be similarly
expanded, yielding
RX

gX(z) +
Îº3
âˆš
K
g3(z)

+
Îº3
âˆš
K
R3(gX(z)) = z â‡’R3(gX(z)) = âˆ’g3(z)
gâ€²
X(z)
(12.40)
or, equivalently,
g3(z) = âˆ’gâ€²
X(z)R3(gX(z)) = âˆ’gâ€²
X(z)g2
X(z).
(12.41)
For simplicity, we normalize the Wigner semi-circle such that Ïƒ 2 = 1, and hence
gX(z) = 1
2

z âˆ’
Â±
âƒâˆš


;
 := z2 âˆ’4.
(12.42)
One then ï¬nds
g3(z) = âˆ’1
4

1 âˆ’z
Â±
âƒâˆš


 
z2 âˆ’2 âˆ’z
Â±
âƒâˆš


.
(12.43)
The imaginary part of this expression when z â†’Î» + i0 gives the correction to the semi-
circle eigenvalue spectrum, and reads, for Î» âˆˆ[âˆ’2,2],
Î´Ï(Î») =
Îº3
2Ï€
âˆš
K
Î»(Î»2 âˆ’3)
âˆš
4 âˆ’Î»2 .
(12.44)
This correction is plotted in Figure 12.3. Note that it is odd in Î», as expected, and diverges
near the edges of the spectrum, around which the above perturbation approach breaks down

12.3 The Central Limit Theorem for Matrices
185
âˆ’2
âˆ’1
0
1
2
l
âˆ’0.4
âˆ’0.2
0.0
0.2
0.4
dr (l)
dr3(l)
dr4(l)
Figure 12.3 First order correction to the Wigner density of eigenvalues for non-zero skewness
(Î´Ï3(Î»)) and kurtosis (Î´Ï4(Î»)), Eqs. (12.44) and (12.47), respectively.
(because the density of eigenvalues of the Wigner matrix goes to zero). Note that the excess
skewness is computed to be
 2
âˆ’2
Î»3Î´Ï(Î»)dÎ» =
Îº3
âˆš
K
,
(12.45)
as expected.
One can obtain the corresponding correction when Ai is symmetric around zero, in
which case the ï¬rst correction term comes from the kurtosis. For large K, the R-transform
of MK can now be approximated as
RMK(z) â‰ˆÏƒ 2z + Îº4
K z3 + Â· Â· Â· .
(12.46)
Following the same path as above, one ï¬nally derives the correction to the eigenvalue
spectrum, which in this case reads, for Î» âˆˆ[âˆ’2,2],
Î´Ï(Î») =
Îº4
2Ï€K
Î»4 âˆ’4Î»2 + 2
âˆš
4 âˆ’Î»2
(12.47)
(see Fig. 12.3). The correction is now even in Î» and one can check that
 2
âˆ’2
Î´Ï(Î»)dÎ» = 0,
(12.48)
as it should be, since all the mass is carried by the semi-circle. Another check that this
result is correct is to compute the excess free kurtosis, given by
 2
âˆ’2
(Î»4 âˆ’2Î»2)Î´Ï(Î»)dÎ» = Îº4
K ,
(12.49)
again as expected.

186
Free Random Matrices
These corrections to the free clt are the analog of the Edgeworth series for the classical
clt. In full generality, the contribution of the nth cumulant to Î´Ï(Î») reads
Î´Ïn(Î») =
Îºn
Ï€Kn/2âˆ’1
Tn
 Î»
2

âˆš
4 âˆ’Î»2,
(12.50)
where Tn(x) are the Chebyshev polynomials of the ï¬rst kind.
12.4 Finite Free Convolutions
We saw that rotationally invariant random matrices become asymptotically free as their
size goes to inï¬nity. At ï¬nite N freeness is not exact, except in very special cases (see
Section 12.5). In this section we will discuss operations on polynomials that are analogous
to the free addition and multiplication but unfortunately lack the full set of properties
of freeness as deï¬ned in Chapter 11. When the polynomials are thought of as expected
characteristic polynomials of large matrices, ï¬nite free addition and multiplication do
indeed converge to the free addition and multiplication when the size of the matrix (degree
of the polynomial) goes to inï¬nity.
12.4.1 Notations: Roots and Coefï¬cients
Let p(z) be a polynomial of degree N. By the fundamental theorem of algebra, this
polynomial will have exactly N roots (when counted with their multiplicity) and can be
written as
p(z) = a0
N
,
i=1
(z âˆ’Î»i).
(12.51)
If a0 = 1 we say that p(z) is monic and if all the Î»iâ€™s are real the polynomial is called
real-rooted. In this section we will only consider real-rooted monic polynomials. Such a
polynomial can always be viewed as the characteristic polynomial of the diagonal matrix
 containing its roots, i.e.
p(z) = det (z1 âˆ’) .
(12.52)
We can expand the product (12.51) as
p(z) =
N

k=0
(âˆ’1)kakzNâˆ’k.
(12.53)
Note that we have deï¬ned the coefï¬cient ak as the coefï¬cient of zNâˆ’k and not that of zk
and that we have included alternating signs (âˆ’1)k in its deï¬nition. The reason is that we
want a simple link between the coefï¬cients ak and the roots Î»i. We have
ak =
N

ordered
k-tuples i
Î»i1Î»i2 . . . Î»ik,
(12.54)
a0 = 1,
a1 =
N

i=1
Î»i,
a2 =
N

i=1
j=i+1
Î»iÎ»j,
. . . ,
aN =
N
,
i=1
Î»i.
(12.55)

12.4 Finite Free Convolutions
187
Note that the coefï¬cient ak is homogeneous to Î»k. From the coefï¬cients ak we can
compute the sample moments2 of the set of Î»iâ€™s. In particular we have
Î¼({Î»i}) = a1
N
and
Ïƒ 2({Î»i}) = a2
1
N âˆ’
2a2
N âˆ’1.
(12.56)
The polynomial p(z) will often be the expected characteristic polynomial of some
random matrix M of size N with joint distribution of eigenvalues P({Î¼i}). The coef-
ï¬cients ai are then multi-linear moments of this joint distribution. If the random
eigenvalues Î¼i do not ï¬‚uctuate, we have Î»i = Î¼i but in general we should think
of the Î»iâ€™s as deterministic numbers ï¬xed by the random ensemble considered. The
case of independent eigenvalues gives a trivial expected characteristic polynomial,
i.e. p(z) = (z âˆ’E(Î¼))N or else Î»i = E(Î¼) âˆ€i.
Shifts and scaling of the matrix M can be mapped onto operations on the polynomial
pM(z). For a shift, we have
pM+Î±1(z) = pM(z âˆ’Î±).
(12.57)
Multiplication by a scalar gives
pÎ±M(z) = Î±NpM(Î±âˆ’1z) â‡â‡’ak = Î±kak.
(12.58)
Finally there is a formula for matrix inversion which is only valid when the eigenvalues
are deterministic (e.g. M = OOT with ï¬xed ):
pMâˆ’1(z) = zN
aN
pM(1/z) â‡â‡’ak = aNâˆ’k
aN
.
(12.59)
A degree N polynomial can always be written as a degree N polynomial of the deriva-
tive operator acting on the monomial zN. We introduce the notation Ë†p as
p(z) =: Ë†p (dz) zN
the coefï¬cients of Ë†p are
Ë†ak = (âˆ’1)N k!
N!aNâˆ’k,
(12.60)
where dz is a shorthand notation for d/dz. It will prove useful to compute ï¬nite free
convolutions.
12.4.2 Finite Free Addition
The equivalent of the free addition for two monic polynomials p1(x) and p2(x) of the
same degree N is the ï¬nite free additive convolution deï¬ned as
p1 + p2(z) =
8
det[z1 âˆ’1 âˆ’O2OT ]
9
O ,
(12.61)
where the diagonal matrices 1,2 contain the roots of p1,2(z) all assumed to be real. The
averaging over the orthogonal matrix O is as usual done over the ï¬‚at (Haar) measure. We
could have chosen to integrate O over unitary matrices or even permutation matrices; the
ï¬nal result would be the same.
As we will show in Section 12.4.5, the additive convolution can be expressed in a very
concise form using the polynomials Ë†p1,2(x):
p1 + p2(z) = Ë†p1 (dz) p2(z) = Ë†p2 (dz) p1(z) = Ë†p1 (dz) Ë†p2 (dz) zN.
(12.62)
2 Here we have chosen to normalize the sample variance with a factor (N âˆ’1)âˆ’1. It may seem odd to use the formula suited to
when the mean is unknown for computing the variance of deterministic numbers, but this deï¬nition will later match that of the
ï¬nite free cumulant and give the Hermite polynomial the variance of the corresponding Wigner matrix.

188
Free Random Matrices
It is easy to see that when ps(z) = p1 + p2(z), ps(z) is again a monic polynomial of
degree N. What is less obvious, but true, is that ps(x) is also real-rooted. A proof of this
is beyond the scope of this book. The additive convolution is bilinear in the coefï¬cients of
p1(z) and p2(z), which means that the operation commutes with the expectation value. If
p1,2(z) are independent random polynomials (for example, characteristic polynomials of
independent random matrices) we have a relation for their expected value:
E[p1 + p2(z)] = E[p1(z)] + E[p2(z)].
(12.63)
The ï¬nite free addition can also be written as a relation between the coefï¬cients a(s)
k
of ps(z) and those of p1,2(z):
a(s)
k
=

i+j=k
(N âˆ’i)! (N âˆ’j)!
N! (N âˆ’k)!
a(1)
i
a(2)
j .
(12.64)
More explicitly, the ï¬rst three coefï¬cients are given by
a(s)
0
= 1,
a(s)
1
= a(1)
1
+ a(2)
1 ,
(12.65)
a(s)
2
= a(1)
2
+ a(2)
2
+ N âˆ’1
N
a(2)
1 a(2)
1 .
From which we can verify that both the sample mean and the variance (Eq. (12.56)) are
additive under the ï¬nite free addition.
If we call pÎ¼(z) = (z âˆ’Î¼)N the polynomial with a single root Î¼ so p0(z) = zN is the
trivial monic polynomial, we have that, under additive convolution with any p(z), p0(z)
acts as a null element and pÎ¼(z) acts as a shift:
p + p0(z) = p(z)
and
p + pÎ¼(z) = p(z âˆ’Î¼).
(12.66)
Hermite polynomials are stable under this addition:
HN + HN(z) = 2N/2HN(2âˆ’N/2z),
(12.67)
where the factors 2N/2 compensate the doubling of the sample variance.
The average characteristic polynomials of Wishart matrices can easily be understood
in terms the ï¬nite free sum. Consider an N-dimensional rank-1 matrix M = xxT where x
is a vector of iid unit variance numbers. It has one eigenvalue equal to N (on average) and
all others are zero, so its average characteristic polynomial is
p(z) = zNâˆ’1(z âˆ’N) = (1 âˆ’dz) zN,
(12.68)
from which we read that Ë†p(dz) = (1 âˆ’dz). But since an unnormalized Wishart matrix of
parameter T is just the free sum of T such projectors, Eq. (12.62) immediately leads to
pT (z) = (1 âˆ’dz)T zN, which coincides with Eq. (6.41).
12.4.3 A Finite R-Transform
If we look back at Eq. (12.62), we notice that the polynomial Ë†p(dz) behaves like the
Fourier transform under free addition. Its logarithm is therefore additive. We need to be
a bit careful of what we mean by the logarithm of a polynomial. The function Ë†p(dz) has
two important properties: ï¬rst as a power series in dz it always starts 1 + O(dz); second it
is deï¬ned by its action on Nth order polynomials in z, so only its N + 1 ï¬rst terms in its
Taylor series matter. We will say that the polynomial is deï¬ned modulo dN+1
z
, meaning

12.4 Finite Free Convolutions
189
that higher order terms are set to zero. When we take the logarithm of Ë†p(u) we thus mean:
apply the Taylor series of the logarithm around 1 and expand up to the power uN. The
resulting function
L(u) := âˆ’

log Ë†p(u)

mod uN+1
(12.69)
is then additive. For average characteristic polynomials of random matrices, it should be
related to the R-transform in the large N limit.
Let us examine more closely L(u) in three simple cases: identity, Wigner and Wishart.
â€¢ For the identity matrix of size N we have
p1(z) = (z âˆ’1)N =
N

k=0
N
k

(âˆ’1)kzNâˆ’k =
N

k=0
(âˆ’dz)k
k!
zN = exp(âˆ’dz)zN.
(12.70)
So we ï¬nd
Ë†p1(u) =

exp(âˆ’u)

mod uN+1
â‡’
L1(u) = u.
(12.71)
â€¢ For Wigner matrices, we saw in Chapter 6 that the expected characteristic polynomial
of a unit Wigner matrix is given by a Hermite polynomial normalized as
pX(z) = Nâˆ’N/2HN(
âˆš
Nz) = exp
-
âˆ’1
2N
 d
dz
2.
zN,
(12.72)
where the right hand side comes from Eq. (6.8). We then have
Ë†pX(u) =
-
exp

âˆ’u2
2N
.
mod uN+1
â‡’
LX(u) = u2
2N .
(12.73)
â€¢ For Wishart matrices, Eq. (6.41) expresses the expected characteristic polynomial as
a derivative operator acting on the monomial zN. For a correctly normalized Wishart
matrix, we then ï¬nd the monic Laguerre polynomial:
pW(z) =

1 âˆ’1
T
d
dz
T
zN,
(12.74)
from which we can immediately read off the polynomial Ë†p(z):
Ë†pW(u) =
(
1 âˆ’qu
N
N/q)
mod uN+1
â‡’LW(u) = âˆ’N
q
&
log

1 âˆ’qu
N
'
mod uN+1.
(12.75)
We notice that in these three cases, the L function is related to the corresponding limiting
R-transform of the inï¬nite size matrices as
Lâ€²(u) =

R(u/N)

mod uN+1.
(12.76)
The equality at ï¬nite N holds for these simple cases but not in the general case. But the
equality holds in general in the limiting case:
lim
Nâ†’âˆLâ€²(Nx) = R(x).
(12.77)

190
Free Random Matrices
12.4.4 Finite Free Product
The free product can also be generalized to an operation on a real-rooted monic poly-
nomial of the same degree. We deï¬ne
p1 Ã— p2(z) =
8
det[z1 âˆ’1O2OT ]
9
O ,
(12.78)
where as usual the diagonal matrices 1,2 contain the roots of p1,2(z). As in the additive
case, averaging the matrix O over the permutation, orthogonal or unitary group gives the
same result.
We will show in Section 12.4.5 that the result of the ï¬nite free product has a simple
expression in terms of the coefï¬cient ak deï¬ned by (12.53). When pm(z) = p1 Ã— p2(z)
we have
a(m)
k
=
(N
k
)âˆ’1
a(1)
k a(2)
k .
(12.79)
Note that if 1 = Î±1 is a multiple of the identity with Î±  0 we have
pÎ±1 = (z âˆ’Î±)N
â‡’
a(Î±1)
k
=
N
k

Î±k.
(12.80)
Plugging this into Eq. (12.79), we see that the free product with a multiple of the identity
multiplies each ak by Î±k which is equivalent to multiplying each root by Î±. In particular
the identity matrix (Î± = 1) is the neutral element for that convolution.
When pm(z) = p1 Ã— p2(z), the sample mean of the roots of pm(z) (Eq. (12.56))
behaves as
Î¼(m) = Î¼(1)Î¼(2).
When both means are unity, we have for the sample variance
Ïƒ 2
(m) = Ïƒ 2
(1) + Ïƒ 2
(2) âˆ’
Ïƒ 2
(1)Ïƒ 2
(2)
N
.
(12.81)
At large N the last term becomes negligible and we recover the additivity of the variance
for the product of unit-trace free variables (see Eq. (11.85)).
Exercise 12.4.1
Free product of polynomials with roots 0 and 1
Consider an even degree N = 2M polynomial with roots 0 and 1 both with
multiplicity M: p(z) = zM(z âˆ’1)M. We will study the ï¬nite free product of this
polynomial with itself pm(z) = p Ã— p(z). We will study the large N limit of this
problem in Section 15.4.2.
(a)
Expand the polynomial and write the coefï¬cients ak in terms of binomial
coefï¬cients.
(b)
Using Eq. (12.79) show that the coefï¬cients of pm(z) are given by
a(m)
k
=
â§
â¨
â©
M
k
2 &N
k
'âˆ’1
when k â‰¤M,
0
otherwise.
(12.82)
(c)
The polynomial pm(z) always has zero as a root. What is its multiplicity?

12.4 Finite Free Convolutions
191
(d)
The degree M polynomial q(z) = zâˆ’Mpm(z) only has non-zero roots. What
is its average root?
(e)
For M = 2, show that the two roots of q(z) are 1/2 Â± 1/
âˆš
12.
(f)
The case M = 4 can be solved by noticing that q(z) is a symmetric function
of (z âˆ’1/2). Show that the four roots are given by
Î»Â±Â± = 1
2 Â± 1
2
2
15 Â± 2
âˆš
30
35
.
(12.83)
12.4.5 Finite Free Convolutions: Derivation of the Results
We will ï¬rst study the deï¬nition (12.61) when the matrix O is averaged over the
permutation group SN. In this case we can write
ps(z) := p1 + p2(z) = 1
N!

permutations
Ïƒ
N
,
i=1

z âˆ’Î»(1)
i
âˆ’Î»(2)
Ïƒ(i)

.
(12.84)
The coefï¬cients a(s)
k
are given by the average over the permutation group of the coefï¬-
cients of the polynomial with roots {Î»(1)
i
+ Î»(2)
Ïƒ(i)}. For example for a(s)
1
we have
a(s)
1
= 1
N!

permutations
Ïƒ
N

i

Î»(1)
i
+ Î»(2)
Ïƒ(i)

=
N

i

Î»(1)
i
+ Î»(2)
i

= a(1)
1
+ a(2)
1 .
(12.85)
For the other coefï¬cients ak, the combinatorics is a bit more involved. Let us ï¬rst ï¬gure
out the structure of the result. For each permutation we can expand the product

z âˆ’Î»(1)
1
âˆ’Î»(2)
Ïƒ(1)
 
z âˆ’Î»(1)
2
âˆ’Î»(2)
Ïƒ(2)

Â· Â· Â·

z âˆ’Î»(1)
N âˆ’Î»(2)
Ïƒ(N)

.
(12.86)
To get a contribution to ak we need to choose the variable z (N âˆ’k) times, we can then
choose a Î»(1) i times and a Î»(2) (k âˆ’i) times. For a given k and for each choice of i, once
averaged over all permutations, the product of the Î»(1) and that of the Î»(2) must both be
completely symmetric and therefore proportional to a(1)
i
a(2)
kâˆ’i. We thus have
a(s)
k
=
k

i=0
C(i,k,N)a(1)
i
a(2)
kâˆ’i,
(12.87)
where C(i,k,N) are combinatorial coefï¬cients that we still need to determine. There is
an easy way to get these coefï¬cients: we can use a case that we can compute directly and
match the coefï¬cients. If 2 = 1, the identity matrix, we have
p1(z) = (z âˆ’1)N
â‡’
a(1)
k
=
N
k

.
(12.88)

192
Free Random Matrices
For a generic polynomial p(z), the free sum with p1(z) is given by a simple shift in the
argument z by 1:
ps(z) = p(z âˆ’1) =
N

k=0
(âˆ’1)ka(1)
k (z âˆ’1)Nâˆ’k
â‡’
a(s)
k
=
k

i=0
N âˆ’i
N âˆ’k

a(1)
i
.
(12.89)
Combining Eqs. (12.87) and (12.88) and matching the coefï¬cient to (12.89), we arrive at
a(s)
k
=
k

i=0
(N âˆ’i)! (N âˆ’k + i)!
N! (N âˆ’k)!
a(1)
i
a(2)
kâˆ’i,
(12.90)
which is equivalent to Eq. (12.64). For the equivalence with Eq. (12.62) see
Exercise 12.4.2.
Now suppose we want to average Eq. (12.61) with respect to the orthogonal or unitary
group (O(N) or U(N)). For a given rotation matrix O, we can expand the determinant,
keeping track of powers of z and of the various Î»(1) that appear in products containing at
most one of each Î»(1)
i
. After averaging over the group, the combinations of Î»(1) must be
permutation invariant, i.e. proportional to a(1)
i
; we then get for the coefï¬cient a(s)
k ,
a(s)
k
=
k

i=0
C

i,k,N,
<
Î»(2)
j
=
a(1)
i
,
(12.91)
where the coefï¬cients C

i,k,N,
<
Î»(2)
j
=
depend on the roots Î»(2)
j . By dimensional anal-
ysis, they must be homogeneous to (Î»(2))kâˆ’i. Since the expression must be symmetrical
in (1) â†”(2), it must be of the form (12.87). And since the free addition with the
unit matrix is the same for all three groups, Eq. (12.64) must be true in all three cases
(SN, O(N) and U(N)).
We now turn to the proof of Eq. (12.79) for the ï¬nite free product. Consider Eq. (12.78),
where the matrix O is averaged over the permutation group SN. For pm(z) = p1 Ã— p2(z)
we have
pm(z) = 1
N!

permutations
Ïƒ
N
,
i=1

z âˆ’Î»(1)
i
Î»(2)
Ïƒ(i)

:= 1
N!

permutations
Ïƒ
pÏƒ (z).
(12.92)
For a given permutation Ïƒ, the coefï¬cients aÏƒ
k are given by
aÏƒ
k =
N

ordered
k-tuples i
Î»(1)
i1 Î»(1)
i2 Â· Â· Â· Î»(1)
ik Î»(2)
Ïƒ(i1)Î»(2)
Ïƒ(i2) Â· Â· Â· Î»(2)
Ïƒ(ik).
(12.93)
After averaging over the permutations Ïƒ, we must have that a(s)
k
âˆa(1)
k a(2)
k . By counting
the number of terms in the sum deï¬ning each ak, we realize that the proportionality
constant must be one over this number. We then have
a(m)
k
=
(N
k
)âˆ’1
a(1)
k a(2)
k .
(12.94)
We could have derived the proportionality constant by requiring that the polynomial
p1(z) = (z âˆ’1)N is the neutral element of this convolution.

12.5 Freeness for 2 Ã— 2 Matrices
193
Exercise 12.4.2
Equivalence of ï¬nite free addition formulas
For a real-rooted monic polynomial p1(z) of degree N, we deï¬ne the
polynomial Ë†p1(u) as
Ë†p1(u) =
N

k=0
(N âˆ’k)!
N!
a(1)
k (âˆ’1)kuk,
(12.95)
where the coefï¬cients a(1)
k
are given by Eq. (12.53).
(a)
Show that
p1(z) = Ë†p1
 d
dz

zN.
(12.96)
(b)
For another polynomial p2(z), show that
Ë†p1
 d
dz

p2(z) =
N

k=0
(âˆ’1)k
k

i=0
(N âˆ’i)! (N âˆ’k + i)!
N! (N âˆ’k)!
a(1)
i
a(2)
kâˆ’izNâˆ’k,
(12.97)
which shows the equivalence of Eqs. (12.62) and (12.64).
12.5 Freeness for 2 Ã— 2 Matrices
Certain low-dimensional random matrices can be free. For N = 1 (1 Ã— 1 matrices) all
matrices commute and thus behave as classical random numbers. As mentioned in Chapter
11, freeness is trivial for commuting variables as only constants (deterministic variables)
can be free with respect to non-constant random variables.
For N = 2, there exist non-trivial matrices that can be mutually free. To be more precise,
we consider the space of 2 Ã— 2 symmetric random matrices and deï¬ne the operator3
Ï„(A) = 1
2E[Tr A].
(12.98)
We now consider matrices that have deterministic eigenvalues but random, rotationally
invariant eigenvectors. We will see that any two such matrices are free. Since 2Ã—2 matrices
only have two eigenvalues we can write these matrices as
A = a1 + Ïƒ O
1
0
0
âˆ’1

OT,
(12.99)
3 More formally, we need to consider 2 Ã— 2 symmetric random matrices with ï¬nite moments to all orders. This space is closed
under addition and multiplication and forms a ring satisfying all the axioms described in Section 11.1.

194
Free Random Matrices
where a is the mean of the two eigenvalues and Ïƒ their half-difference. The matrix O is a
random rotation matrix which for N = 2 only has one degree of freedom and can always
be written as
O =
 cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸

,
(12.100)
where the angle Î¸ is uniformly distributed in [0,2Ï€]. Note again that we are considering
matrices for which a and Ïƒ are non-random. If we perform the matrix multiplications and
use some standard trigonometric identities we ï¬nd
A = a1 + Ïƒ
cos 2Î¸
sin 2Î¸
sin 2Î¸
âˆ’cos 2Î¸

.
(12.101)
12.5.1 Freeness of Matrices with Deterministic Eigenvalues
We can now show that any two such matrices A and B are free. If we put ourselves in
the basis where A is diagonal, we see that traceless polynomials pk(A) and qk(B) are
necessarily of the form
pk(A) = ak
1
0
0
âˆ’1

and
qk(B) = bk
cos 2Î¸
sin 2Î¸
sin 2Î¸
âˆ’cos 2Î¸

,
(12.102)
for some deterministic numbers ak and bk and where Î¸ is the random angle between the
eigenvectors of A and B. We can now compute the expectation value of the trace of products
of such polynomials:
Ï„
- n
,
k=1
pk(A)qk(B)
.
= 1
2
 n
,
k=1
akbk

E Tr
 cos 2Î¸
sin 2Î¸
âˆ’sin 2Î¸
cos 2Î¸
n
.
(12.103)
We notice that the matrix on the right hand side is a rotation matrix (of angle 2Î¸) raised
to the power n and therefore itself a rotation matrix (of angle 2nÎ¸). The average of such a
matrix is zero, thus ï¬nishing our proof that two rotationally invariant 2 Ã— 2 matrices with
deterministic eigenvalues are free.
As a consequence, we can use the R-transform to compute the average eigenvalue
spectrum of A + OBOT when A and B have deterministic eigenvalues and O is a
random rotation matrix. In particular if A and B have the same variance (Ïƒ) then this
spectrum is given by the arcsine law that we encountered in Section 7.1.3. For positive
deï¬nite A we can also compute the spectrum of
âˆš
AOBOT âˆš
A using the S-transform (see
Exercises 12.5.1 and 12.5.2).
Exercise 12.5.1
Sum of two free 2 Ã— 2 matrices
(a)
Consider A1 a traceless rotationally invariant 2 Ã— 2 matrix with deterministic
eigenvalues Î»Â± = Â±Ïƒ. Show that

12.5 Freeness for 2 Ã— 2 Matrices
195
gA(z) =
z
z2 âˆ’Ïƒ 2
and
R(g) =
*
1 + 4Ïƒ 2g2 âˆ’1
2g
.
(12.104)
(b)
Two such matrices A1 and A2 (with eigenvalues Â±Ïƒ1 and Â±Ïƒ2 respectively)
are free, so we can sum their R-transforms to ï¬nd the spectrum of their sum.
Show that gA1+A2(z) is given by one of the roots of
gA1+A2(z) =
Â±z
/
(Ïƒ 4
1 + Ïƒ 4
2 ) âˆ’2(Ïƒ 2
1 + Ïƒ 2
2 )z2 + z4
.
(12.105)
(c)
In the basis where A1 is diagonal A1 + A2 has the form
A =
Ïƒ1 + Ïƒ2 cos 2Î¸
Ïƒ2 sin 2Î¸
Ïƒ2 sin 2Î¸
âˆ’Ïƒ1 âˆ’Ïƒ2 cos 2Î¸

,
(12.106)
for a random angle Î¸ uniformly distributed between [0,2Ï€]. Show that the
eigenvalues of A1 + A2 are given by
Î»Â± = Â±
/
Ïƒ 2
1 + Ïƒ 2
2 + 2Ïƒ1Ïƒ2 cos 2Î¸.
(12.107)
(d)
Show that the densities implied by (b) and (c) are the same. For Ïƒ1 = Ïƒ2, it is
called the arcsine law (see Section 7.1.3).
Exercise 12.5.2
Product of two free 2 Ã— 2 matrices
A rotationally invariant 2 Ã— 2 matrix with deterministic eigenvalues 0 and
a1 â‰¥0 has the form
A1 = O
0
0
0
a1

OT .
(12.108)
Two such matrices are free so we can use the S-transform to compute the
eigenvalue distribution of their product.
(a)
Show that the T-transform and the S-transform of A1 are given by
t(Î¶) =
a1
2(Î¶ âˆ’a1)
and
S(t) = 2
a1
t + 1
2t + 1.
(12.109)
(b)
Consider another such matrix A2 with independent eigenvectors and non-zero
eigenvalue a2. Using the multiplicativity of the S-transform, show that the
T-transform and the density of eigenvalues of âˆšA1A2
âˆšA1 are given by
tA1A2 = 1
2 âˆ’1
2
2
Î¶
Î¶ âˆ’a1a2
(12.110)
and
ÏA1A2(Î») = 1
2Î´(Î») +
1
2Ï€ âˆšÎ»(a1a2 âˆ’Î») for 0 â‰¤Î» â‰¤a1a2,
(12.111)

196
Free Random Matrices
where the delta function in the density indicates the fact that one eigenvalue
is always zero.
(c)
By directly computing the matrix product in the basis where A1 is diagonal,
show that, in that basis,
*
A1A2
*
A1 = a1a2
0
0
0
1 âˆ’cos 2Î¸

,
(12.112)
where Î¸ is a random angle uniformly distributed between 0 and 2Ï€.
(d)
Show that the distribution of the non-zero eigenvalue implied by (b) and (c) is
the same. It is the shifted arcsine law.
12.5.2 Pairwise Freeness and Free Collections
For the 2 Ã— 2 matrices A and B to be free, they need to have deterministic eigenvalues.
The proof above does not work if the eigenvalues of one of these matrices are random. As
an illustration, consider three matrices A, B and C as above (2 Ã— 2 symmetric rotationally
invariant matrices with deterministic eigenvalues). Each pair of these matrices is free. On
the other hand, the free sum A + B has random eigenvalues and it is not necessarily free
from C. Actually we can show that it is not free with respect to C.
First we show that A, B and C do not form a free collection. For simplicity, we consider
them traceless with ÏƒA = ÏƒB = ÏƒC = 1. Then one can show that
Ï„ (ABCABC) = Ï„((ABC)2) = 1,
(12.113)
violating the freeness condition for three variables. Indeed, let us compute explicitly ABC
in the basis where A is diagonal. We ï¬nd
ABC =
cos 2Î¸ cos 2Ï† + sin 2Î¸ sin 2Ï†
cos 2Î¸ sin 2Ï† âˆ’sin 2Î¸ cos 2Ï†
cos 2Î¸ sin 2Ï† âˆ’sin 2Î¸ cos 2Ï†
âˆ’cos 2Î¸ cos 2Ï† âˆ’sin 2Î¸ sin 2Ï†

,
(12.114)
where Ï† is the angle between the eigenvectors of A and those of C. The matrix ABC is a
non-zero symmetric matrix, so the trace of its square must be non-zero. Actually one ï¬nds
(ABC)2 = 1.
Another way to see that A, B and C are not free as a group is to compute the mixed
cumulant Îº6 (A,B,C,A,B,C), given that odd cumulants such as Îº1(A) and Îº3(A,B,C)
are zero and that mixed cumulants involving two matrices are zero (they are pairwise
free). The only non-zero term in the momentâ€“cumulant relation for Ï„ (ABCABC) (see
Eq. (11.74)) is
Ï„ (ABCABC) = Îº6 (A,B,C,A,B,C) = 1.
(12.115)

12.5 Freeness for 2 Ã— 2 Matrices
197
The matrices A, B and C have therefore at least one non-zero cross-cumulant and cannot
be free as a collection.
Now, to show that A+B is not free from C, we realize that if we expand the sixth cross-
cumulant Îº6 (A + B,A + B,C,A + B,A + B,C), we will encounter the above non-zero
cross-cumulant of A, B and C. Indeed, a cross-cumulant is linear in each of its arguments.
Since all other terms in this expansion are zero, we ï¬nd
Îº6(A + B,A + B,C,A + B,A + B,C)
= Îº6 (A,B,C,A,B,C) + Îº6 (B,A,C,A,B,C)
+ Îº6 (A,B,C,B,A,C) + Îº6 (B,A,C,B,A,C) = 4  0.
(12.116)
As a consequence, even though 2 Ã— 2 symmetric rotationally invariant random matrices
with deterministic eigenvalues are pairwise free, they do not satisfy the free clt. If they
did, it would imply that 2 Ã— 2 matrices with a semi-circle spectrum would be stable under
addition, which is not the case. Note that Gaussian 2Ã—2 Wigner matrices (which are stable
under addition) do not have a semi-circle spectrum (see Exercise 12.5.3).
Exercise 12.5.3
Eigenvalue spectrum of 2 Ã— 2 Gaussian matrices
Real symmetric and complex Hermitian Gaussian 2 Ã— 2 matrices are stable
under addition but they are not free. In this exercise we see that their spectrum is
not given by a semi-circle law.
(a)
Use Eq. (5.22) and the Gaussian potential V (x) = x2/2 to write the joint
probability (up to a normalization) of Î»1 and Î»2, the two eigenvalues of a real
symmetric Gaussian 2 Ã— 2 matrix.
(b)
To ï¬nd the eigenvalue density, we need to compute
Ï(Î»1) =
 âˆ
âˆ’âˆ
dÎ»2P(Î»1,Î»2).
(12.117)
This integral will involve an error function because of the absolute value in
P(Î»1,Î»2). If you have the courage compute Ï(Î») leaving the normalization
undetermined.
(c)
It is easier to do the complex Hermitian case. Use Eq. (5.26) and the same
potential to adapt your answer in (a) to the Î² = 2 case.
(d)
The absolute value has now disappeared and the integral in (b) is now much
easier. Perform this integral and ï¬nd the normalization constant. You should
obtain
Ï(Î») = Î»2 + 1
2
âˆšÏ€ eâˆ’Î»2.
(12.118)

198
Free Random Matrices
Bibliographical Notes
â€¢ On integration over the orthogonal and unitary groups, see
â€“ B. Collins and P. Â´Sniady. Integration with respect to the Haar measure on uni-
tary, orthogonal and symplectic group. Communications in Mathematical Physics,
264(3):773â€“795, 2006,
â€“ B. Collins, A. Guionnet, and E. Maurel-Segala. Asymptotics of unitary and orthogo-
nal matrix integrals. Advances in Mathematics, 222(1):172â€“215, 2009,
â€“ M. Berg`ere and B. Eynard. Some properties of angular integrals. Journal of Physics
A: Mathematical and Theoretical, 42(26):265201, 2009.
â€¢ On the Weingarten coefï¬cients, see
â€“ D. Weingarten. Asymptotic behavior of group integrals in the limit of inï¬nite rank.
Journal of Mathematical Physics, 19(5):999â€“1001, 1978,
â€“ P. W. Brouwer and C. W. J. Beenakker. Diagrammatic method of integration over
the unitary group, with applications to quantum transport in mesoscopic systems.
Journal of Mathematical Physics, 37(10):4904â€“4934, 1996,
â€“ B. Collins and S. Matsumoto. On some properties of orthogonal Weingarten func-
tions. Journal of Mathematical Physics, 50(11):113516, 2009,
â€“ T. Banica. The orthogonal Weingarten formula in compact form. Letters in Mathe-
matical Physics, 91(2):105â€“118, 2010.
â€¢ On the Edgeworth series, see e.g. https://en.wikipedia.org/wiki/Edgeworth series and
â€“ J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pric-
ing: From Statistical Physics to Risk Management. Cambridge University Press,
Cambridge, 2nd edition, 2003.
â€¢ On the Edgeworth series for the free CLT, see
â€“ G. P. Chistyakov and F. GÂ¨otze. Asymptotic expansions in the CLT in free probability.
Probability Theory and Related Fields, 157:107â€“156, 2011.
â€¢ On ï¬nite N free matrices, see
â€“ A. Marcus. Polynomial convolutions and (ï¬nite) free probability, 2018: preprint
available at https://web.math.princeton.edu/âˆ¼amarcus/papers/ff main.pdf,
â€“ A. Marcus, D. A. Spielman, and N. Srivastava. Finite free convolutions of polynomi-
als. preprint arXiv:1504.00350, 2015.

13
The Replica Method*
In this chapter we will review another important tool to perform compact computations in
random systems and in particular in random matrix theory, namely the â€œreplica methodâ€.
For example, one can use replicas to understand the R-transform addition rule when one
adds two large, randomly rotated matrices.
Suppose that we want to compute the free energy of a large random system. The free
energy is the logarithm of some partition function Z.1 We expect that the free energy does
not depend on the particular sample so we can average the free energy with respect to
the randomness in the system to get the typical free energy of a given sample. Unfor-
tunately, averaging the logarithm of a partition function is hard. What we can do more
easily is to compute the partition function to some power n and later let n â†’0 using the
â€œreplica trickâ€:
log Z = lim
nâ†’0
Zn âˆ’1
n
.
(13.1)
The partition function Zn is just the partition function of n non-interacting copies of the
same system Z, these copies are called â€œreplicasâ€, hence the name of the technique. Averag-
ing the logarithm is then equivalent to averaging Zn and taking the limit n â†’0 as above.
The averaging procedure will however couple the n copies and the resulting interacting
system is in general hard to solve. In many interesting cases, the partition function can only
be computed as the size of the system (say N) goes to inï¬nity. Naturally one is tempted to
interchange the limits (n â†’0 and N â†’âˆ) but there is no mathematical justiï¬cation (yet)
for doing so. Another problem is that we can hope to compute E[Zn] for all integers n but
is that really sufï¬cient to do a proper n â†’0 limit?
For all these reasons, replica computations are not considered rigorous. Nevertheless,
they are a precious source of intuition and they allow one to obtain results mathematicians
would call conjectures, but that often turn out to be mathematically exact. Although a lot
of progress has been made to understand why the replica trick works, there is still a halo of
mystery and magic surrounding the method, and a nagging impression that an equivalent
but more transparent formalism awaits revelation.
1 See e.g. Section 13.4 for an explicit example.
199

200
The Replica Method
In the present chapter, we will show how all the results obtained up to now can be
rederived using replicas. We start by showing how the Stieltjes transform can be expressed
using the replica method, and obtain once more the semi-circle law for Gaussian random
matrices. We then discuss R-transforms and S-transforms in the language of replicas.
13.1 Stieltjes Transform
As we have shown in Chapter 2, the density of eigenvalues Ï(Î») of a random matrix is
encoded in the trace of the resolvent of that matrix, which deï¬nes the Stieltjes transform of
Ï(Î»). Here we show how this quantity can be computed using the replica formalism.
13.1.1 General Set-Up
To use the replica trick in random matrix theory, we ï¬rst need to express the Stieltjes
transform as the average logarithm of a (possibly random) determinant. In the large N
limit and for z sufï¬ciently far from the real eigenvalues, the discrete Stieltjes transform
gN(z) converges to a deterministic function g(z). The replica trick will allow us to compute
E[gN(z)], which also converges to g(z).
Using the deï¬nition Eq. (2.19) and dropping the N subscript, we have
E[gA(z)] = 1
N E
- N

k=1
1
z âˆ’Î»k
.
,
(13.2)
whereas the determinant of z1 âˆ’A is given by
det(z1 âˆ’A) =
N
,
k=1
(z âˆ’Î»k).
(13.3)
We can turn the product in the determinant into a sum by taking the logarithm and obtain
(z âˆ’Î»k)âˆ’1 from log(z âˆ’Î»k) by taking a derivative with respect to z. We then get
E[gA(z)] = 1
N E
( d
dz log det(z1 âˆ’A)
)
.
(13.4)
To compute the determinant we may use the multivariate Gaussian identity

dNÏˆ
(2Ï€)N/2 exp

âˆ’Ïˆ T MÏˆ
2

=
1
âˆš
det M
,
(13.5)
which is exact for any N as long as the matrix M is positive deï¬nite. For z larger than the
top eigenvalue of A, (z1 âˆ’A) will be positive deï¬nite. The Gaussian formula allows us to
compute the inverse of the square-root of the determinant, but we can neutralize the power
âˆ’1/2 by introducing an extra factor âˆ’2 in front of the logarithm. Applying the replica trick
(13.1) we thus get

13.1 Stieltjes Transform
201
E[gA(z)] = âˆ’2E
( d
dz lim
nâ†’0
Zn âˆ’1
Nn
)
,
(13.6)
with
Zn :=

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 exp

âˆ’
n

Î±=1
ÏˆT
Î±(z1 âˆ’A)ÏˆÎ±
2

,
(13.7)
where we have written Zn as the product of n copies of the same Gaussian integral. This
is all ï¬ne, except our Zn is only deï¬ned for integer n and we need to take n â†’0. The
limiting Stieltjes transform is deï¬ned as gA(z) = limNâ†’âˆE[gA(z)], but in practice the
replica trick will allow us to compute
gA(z) = âˆ’2 d
dz lim
nâ†’0 lim
Nâ†’âˆ
E[Zn] âˆ’1
Nn
,
(13.8)
and hope that the two limits n â†’0 and N â†’âˆcommute, such thatgA(z) = gA(z). There
is, however, no guarantee that these limits do commute.
13.1.2 The Wigner Case
As a detailed example of replica trick calculation, we now give all the steps necessary to
compute the Stieltjes transform for the Wigner ensemble. We want to take the expectation
value of Eq. (13.7) in the case where A = X, a symmetric Gaussian rotational invariant
matrix:
E[Zn] =

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 E
â¡
â£exp
â›
ââˆ’z
2
n

Î±=1
N

i=1
Ïˆ2
Î±i âˆ’
N

i<j
XijÏˆÎ±iÏˆÎ±j âˆ’1
2
N

i
XiiÏˆÎ±iÏˆÎ±i
â
â 
â¤
â¦,
=

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 exp

âˆ’z
2
n

Î±=1
N

i=1
Ïˆ2
Î±i
 N
,
i<j
E
-
exp

âˆ’Xij
n

Î±=1
ÏˆÎ±iÏˆÎ±j
.
Ã—
N
,
i
E
-
exp

âˆ’1
2Xii
n

Î±=1
ÏˆÎ±iÏˆÎ±i
.
,
(13.9)
where we have isolated the products of expectation of independent terms and separated the
diagonal and off-diagonal terms. We can evaluate the expectation values using the following
identity: for a centered Gaussian variable x of variance Ïƒ 2, we have
E[eax] = eÏƒ 2a2/2.
(13.10)
Using the fact that the diagonal and off-diagonal elements have a variance equal to, respec-
tively, 2Ïƒ 2/N and Ïƒ 2/N (see Section 2.2.2), we get

202
The Replica Method
E[Zn] =

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 exp

âˆ’z
2
n

Î±=1
N

i=1
Ïˆ2
Î±i
 N
,
i<j
exp
â›
âÏƒ 2
2N
 n

Î±=1
ÏˆÎ±iÏˆÎ±j
2â
â 
Ã—
N
,
i
exp
â›
âÏƒ 2
4N
 n

Î±=1
ÏˆÎ±iÏˆÎ±i
2â
â .
(13.11)
We can now combine the last two sums in the exponential into a single sum over {ij}, which
we can further transform into
Ïƒ 2
4N
N

i,j=1
 n

Î±=1
ÏˆÎ±iÏˆÎ±j
2
= Ïƒ 2
4N
n

Î±,Î²=1
 N

i=1
ÏˆÎ±iÏˆÎ²i
2
.
(13.12)
We would like to integrate over the variables ÏˆÎ±i but the argument of the exponential
contains fourth order terms in the Ïˆâ€™s. To tame this term, one uses the Hubbardâ€“
Stratonovich identity:
exp
ax2
2

=

dq
âˆš
2Ï€a
exp

âˆ’q2
2a + xq

.
(13.13)
Before we use Hubbardâ€“Stratonovich, we need to regroup diagonal and off-diagonal terms
in Î±Î²:
Ïƒ 2
4N
n

Î±,Î²=1
 N

i=1
ÏˆÎ±iÏˆÎ²i
2
= Ïƒ 2
2N
n

Î±<Î²
 N

i=1
ÏˆÎ±iÏˆÎ²i
2
+ Ïƒ 2
N
n

Î±=1
 N

i=1
ÏˆÎ±iÏˆÎ±i
2
2
,
(13.14)
where in the diagonal terms we have pushed the factor 1/4 in the squared quantity for later
convenience. We can now use Eq. (13.13), introducing diagonal qÎ±Î± and upper triangular
qÎ±Î² to linearize the squared quantities. Writing the qâ€™s as a symmetric matrix q we have
E[Zn] âˆ

dq

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 exp âˆ’
â›
âN Tr q2
4Ïƒ 2
+
N

i=1
n

Î±,Î²=1
(zÎ´Î±Î² âˆ’qÎ±Î²)ÏˆÎ±iÏˆÎ²i
2
â
â ,
(13.15)
where dq is the integration over the independent component of the n Ã— n symmetric
matrix q; note that we have dropped z-independent constant factors. The integral of ÏˆÎ±i
is now a multivariate Gaussian integral, actually N copies of the very same n-dimensional
Gaussian integral:

n
,
Î±=1
dÏˆÎ±
âˆš
2Ï€
exp âˆ’
â›
â
n

Î±,Î²=1
(zÎ´Î±Î² âˆ’qÎ±Î²)ÏˆÎ±ÏˆÎ²
2
â
â = (det(z1 âˆ’q))âˆ’1/2.
(13.16)

13.1 Stieltjes Transform
203
Raising this integral to the Nth power and using det M = exp Tr log M we ï¬nd
E[Zn] âˆ

dq exp âˆ’
(
N Tr
 q2
4Ïƒ 2 + 1
2 log(z1 âˆ’q)
)
:=

dq exp

âˆ’N
2 F(q)

.
(13.17)
We now ï¬x n and evaluate E[Zn] for very large N, leaving the limit n â†’0 for later. In
the large N limit, the integral over the matrix q can be done by the saddle point method.
More precisely, we should ï¬nd an extremum of F(q) in the n(n + 1)/2 elements of q.
Alternatively we can diagonalize q, introducing the log of a Vandermonde determinant in
the exponential (see Section 5.1.4).2 In terms of the eigenvalues qÎ± of q, one has
F({qÎ±}) =
n

Î±=1
q2
Î±
2Ïƒ 2 + log(z âˆ’qÎ±) âˆ’1
N

Î±Î²
log |qÎ± âˆ’qÎ²|.
(13.18)
To ï¬nd the saddle point, we take the partial derivatives of F{qÎ±} with respect to the {qÎ±}
and equate them to zero:
qÎ±
Ïƒ 2 âˆ’
1
z âˆ’qÎ±
âˆ’1
N

Î±Î²
2
qÎ± âˆ’qÎ²
= 0.
(13.19)
The effect of the last term is to push the eigenvalues qÎ± away from one another, such that
in equilibrium their relative distance is of order 1/N and the last term is of the same order
as the ï¬rst two. Since there are only n such eigenvalues, the total spread (from the largest
to smallest) is of order n/N, which we will neglect when N â†’âˆ. Hence we can assume
that all eigenvalues are identical and equal to qâˆ—(z), where qâˆ—(z) satisï¬es
z âˆ’qâˆ—= Ïƒ 2
qâˆ—.
(13.20)
We recognize the self-consistent equation for the Stieltjes transform of the Wigner (Eq.
(2.35)) if we make the identiï¬cation qâˆ—(z) = Ïƒ 2gX(z). For N large and n small we indeed
have
E[Zn] = exp

âˆ’Nn
2 F1(z,qâˆ—(z))

with F1(z,q) = q2
2Ïƒ 2 + log(z âˆ’q),
(13.21)
so
lim
nâ†’0 lim
Nâ†’âˆ
E[Zn] âˆ’1
Nn
= âˆ’F1(z,qâˆ—(z))
2
.
(13.22)
Finally, from Eq. (13.8), we should have
gX(z) = d
dzF1(z,qâˆ—(z)).
(13.23)
2 A third method is used in spin-glass problems where the integrand has permutation symmetry but not necessarily rotational
symmetry, see Section 13.4.

204
The Replica Method
To ï¬nish the computation we need to take the derivative of F1(z,qâˆ—(z)) with respect to z,
but since qâˆ—(z) is an extremum of F1, the partial derivative of F1(z,q) with respect to q is
zero at q = qâˆ—(z). Hence
gX(z) = âˆ‚
âˆ‚zF1(z,q)

q=qâˆ—(z)
=
1
(z âˆ’qâˆ—) = qâˆ—(z)
Ïƒ 2 .
(13.24)
We thus recover the usual solution of the self-consistent Wigner equation.
13.2 Resolvent Matrix
13.2.1 General Case
We saw that the replica trick can be used to compute the average Stieltjes transform of
a random matrix. The Stieltjes transform is the normalized trace of the resolvent matrix
GA(z) = (z1 âˆ’A)âˆ’1. In Chapter 19 we will need to know the average of the elements
of the resolvent matrix for free addition and multiplication. These averages can also be
computed using the replica trick. An element of an inverse matrix can indeed be written as
a multivariate Gaussian integral:
&
Mâˆ’1'
ij â‰¡1
Z

dNÏˆ
(2Ï€)N/2 ÏˆiÏˆj exp

âˆ’ÏˆT MÏˆ
2

,
Z :=

dNÏˆ
(2Ï€)N/2 exp

âˆ’Ïˆ T MÏˆ
2

,
(13.25)
which we can rewrite as
&
Mâˆ’1'
ij =
lim
mâ†’âˆ’1 Zm

dNÏˆ ÏˆiÏˆj exp

âˆ’ÏˆT MÏˆ
2

.
(13.26)
If we express Zm for m âˆˆN+ as m Gaussian integrals and combine them with the integral
with the ÏˆiÏˆj term (which we label number 1) we get, with n = m + 1:
&
Mâˆ’1'
ij = lim
nâ†’0

n
,
Î±=1
dNÏˆÎ±
(2Ï€)nN/2 Ïˆ1iÏˆ1j exp

âˆ’
n

Î±=1
ÏˆT
Î±MÏˆÎ±
2

:=
8
Ïˆ1iÏˆ1j
9
n=0 .
(13.27)
This equation can then be used to compute averages of elements of the resolvent matrix
by using M = z1 âˆ’A for the relevant random matrix A. For example, in the case of
Gaussian Wigner matrices, the correlation
8
Ïˆ1iÏˆ1j
9
n=0 can be computed using the saddle
point conï¬guration of the Ïˆâ€™s. Since different i all decouple and play the same role, it is
clear that
E[GX(z)]ij =
8
Ïˆ1iÏˆ1j
9
n=0 = Î´ijgX(z),
(13.28)
i.e. the average resolvent of a Gaussian matrix is simply the average Stieltjes transform
times the identity matrix.

13.2 Resolvent Matrix
205
13.2.2 Free Addition
In this section we will show how to use Eq. (13.27) to compute the average of the full
resolvent for the sum of two randomly rotated matrices. Since we know these matrices are
free in the large N limit, we expect to recover the additivity of R-transforms, but we will in
fact obtain a slightly richer result. Also, the replica method is very convenient to manipulate
and resum the perturbation theory alluded to in Section 12.2.
Consider two symmetric matrices A and B and the new matrix C = A+OBOT where O
is a random orthogonal matrix. We want to compute
E[GC(z)] = E
&
(z1 âˆ’A âˆ’OBOT )âˆ’1'
,
(13.29)
where the expectation value is over the orthogonal matrix O. We can always choose B to
be diagonal. If B is not diagonal to start with, we just absorb the orthogonal matrix that
diagonalizes B in the matrix O. Expressing GC(z) in the eigenbasis of A is equivalent
to choosing A to be diagonal. In that basis, the off-diagonal elements of E[GC(z)] must
be zero by the following argument: since both A and B are diagonal, for every matrix O
that contributes to an off-diagonal element of E[GC(z)] there exists an equally probable
matrix Oâ€² with the same contribution but opposite sign, hence the average must be zero.
Note that while the average matrix E[GC(z)] commutes with A, a particular realization of
the random matrix GC(z) (corresponding to a speciï¬c choice for O) will not in general
commute with A.
Now, let us use the replica formalism to compute E[GC(z)], i.e. start with
GC(z)ij = lim
nâ†’0

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 Ïˆ1iÏˆ1j exp

âˆ’
n

Î±=1
Ïˆ T
Î±(z1 âˆ’A)ÏˆÎ±
2

Ã— E
-
exp
 n

Î±=1
Ïˆ T
Î±OBOT ÏˆÎ±
2
.
,
(13.30)
where now we skip the i,j indices on ÏˆÎ±, treated as vectors.
The last term with the expectation value can be rewritten as
I = E
-
exp

N
2
n

Î±=1
Tr QÎ±,Î±OBOT
.
,
(13.31)
where QÎ±,Î² = ÏˆÎ±Ïˆ T
Î²/N is an nÃ—n symmetric matrix. We recognize the Harish-Chandraâ€“
Itzyksonâ€“Zuber integral discussed in Chapter 10, with one matrix (n
Î±=1 QÎ±,Î±) being at
most of rank n â‰ªN, so we can use the low-rank formula Eq. (10.45). Our expectation
value thus becomes
I = exp
N
2 Trn HB(Q)

,
(13.32)
where Trn denotes the trace of an n Ã— n matrix and HB is the anti-derivative of the
R-transform of B.

206
The Replica Method
We now need to perform the integral of ÏˆÎ± in Eq. (13.30). But in order to do so we must
deal with Tr HB(Q), which is a non-linear function of the ÏˆÎ±. The trick is to make the
matrix Q an integration variable that we ï¬x to its deï¬nition using a delta function. The
delta function is itself represented as an integral over another (n Ã— n) symmetric matrix Y
along the imaginary axis. In other words, we introduce the following representation of the
delta function in Eq. (13.30):
 iâˆ
âˆ’iâˆ
Nn(n+1)/2dY
23n/2Ï€n/2
exp
â›
ââˆ’N
2 Trn QY + 1
2
n

Î±,Î²=1
YÎ±,Î²ÏˆÎ±ÏˆT
Î²
â
â ,
(13.33)
where the integrals over dQ and dY are over symmetric matrices. We have absorbed a factor
of N in Y and a factor of 2 on its diagonal, hence the extra factors of 2 and N in front of
dY. We can now perform the following Gaussian integral over ÏˆÎ±:
Jij =

N
,
k=1
n
,
Î±=1
dÏˆÎ±k
âˆš
2Ï€
Ïˆ1iÏˆ1j exp
â›
ââˆ’1
2
N

k=1
n

Î±,Î²=1
ÏˆÎ±k(zÎ´Î±,Î² âˆ’akÎ´Î±,Î² âˆ’YÎ±Î²)ÏˆÎ²k
â
â ,
(13.34)
where we have written the vectors ÏˆÎ± in terms of their components ÏˆÎ±k, and where ak are
the eigenvalues of A. We notice that the Gaussian integral is diagonal in the index k, so we
have N âˆ’1 n-dimensional Gaussian integrands differing only by their value of ak, and a
last integral for k = i = j, where the term Ïˆ2
1i is in front of the Gaussian integrand (the
integral is zero if i  j, meaning that GC(z) is diagonal, as expected). The result is then
Jij = Î´ij
&
((z âˆ’ai)1n âˆ’Y)âˆ’1'
11
N
,
k=1
(det((z âˆ’ak)1n âˆ’Y))âˆ’1/2,
(13.35)
where the ï¬rst term is the 11 element of an n Ã— n matrix, coming from the term Ïˆ2
1i.
Returning to our main expression Eq. (13.30) and dropping constants that are 1 as
n â†’0,
E[GC(z)]ij = lim
nâ†’0
 iâˆ
âˆ’iâˆ
dQ

dYÎ´ij
&
((z âˆ’ai)1n âˆ’Y)âˆ’1'
11
Ã— exp
-
N
2

âˆ’Trn QY + Trn HB(Q) âˆ’1
N
N

k=1
Trn log((z âˆ’ak)1n âˆ’Y)
.
.
(13.36)
For large N the integral over Y and Q is dominated by the saddle point, i.e. the extremum
of the argument of the exponential. The inverse-matrix term in front of the exponential does
not contain a power of N so it does not contribute to the determination of the saddle point.
The extremum is over a function of two n Ã— n symmetric matrices. Taking derivatives with
respect to QÎ±Î² and equating it to zero gives
YÎ±Î² = [RB(Q)]Î±Î²,
(13.37)

13.2 Resolvent Matrix
207
and similarly when taking derivatives with respect to YÎ±Î²:
QÎ±Î² = 1
N
N

k=1
&
((z âˆ’ak)1n âˆ’Y)âˆ’1'
Î±Î² .
(13.38)
Let us look at these equations in a basis where Q is diagonal. The ï¬rst equation shows that
Y is also diagonal, so that the second equation reads
QÎ±Î± = 1
N
N

k=1
1
z âˆ’ak âˆ’YÎ±Î±
â‰¡gA(z âˆ’YÎ±Î±).
(13.39)
Hence all n diagonal elements QÎ±Î± and YÎ±Î±, Î± = 1, . . . ,n satisfy the same pair of equa-
tions. For large z, there is a unique solution to these equations, hence Q and Y must
be multiples of the identity Q = qâˆ—1n and Y = yâˆ—1n, as expected from the rotational
symmetry of the argument of the exponential that we are maximizing. The quantities qâˆ—
and yâˆ—are the unique solutions of
y = RB(q)
and
q = gA(z âˆ’y).
(13.40)
The saddle point for Y is real while our integral representation Eq. (13.33) was over purely
imaginary matrices; but for large values of z, the solutions of Eqs. (13.40) give small values
for qâˆ—and yâˆ—, and for such small values the integral contour can be deformed without
encountering any singularities. We also justify the use of RB(qâˆ—) = H â€²
B(qâˆ—) as qâˆ—can be
made arbitrarily small by choosing a large enough z.
The expectation of the resolvent is thus given, for large enough z and N, by
E[GC(z)ij]
â‰ˆlim
nâ†’0
Î´ij
(z âˆ’ai âˆ’yâˆ—) exp
-
nN
2

âˆ’qâˆ—yâˆ—+ HB(qâˆ—) âˆ’1
N
N

k=1
log(z âˆ’ak âˆ’yâˆ—)
.
.
(13.41)
As n â†’0 the exponential drops out and we obtain, in matrix form,
E[GC(z)] = GA(z âˆ’RB(qâˆ—))
with
qâˆ—= gA(z âˆ’RB(qâˆ—)).
(13.42)
13.2.3 Resolvent Subordination for Addition and Multiplication
Equation (13.42) relates the average resolvent of C to that of A. By taking the normalized
trace on both sides we ï¬nd
qâˆ—= gC(z) = gA(z âˆ’RB(qâˆ—)),
(13.43)
which is precisely the subordination relation for the Stieltjes transform of a free sum that
we found in Section 11.3.8. We just have rederived this result once again with replicas.

208
The Replica Method
But what is more interesting is that we have found a relationship for the average of a matrix
element of the full resolvent matrix, namely
E[GC(z)] = GA (z âˆ’RB(gC(z))) .
(13.44)
This relation will give precious information on the overlap between the eigenvalues of A
and B and those of C. Note that, by symmetry, one also has
E[GC(z)] = GB (z âˆ’RA(gC(z))) .
(13.45)
In the free product case, namely C = A
1
2 BA
1
2 where A and B are large positive deï¬nite
matrices whose eigenvectors are mutually random, a very similar replica computation gives
a subordination relation for the average T matrix:
E[TC(Î¶)] = TA[SB(tC(Î¶))Î¶],
(13.46)
with SB(t) the S-transform of the matrix B. If we take the normalized trace on both sides,
we recover the subordination relation Eq. (11.109). Equation (13.46) can then be turned
into a subordination relation for the full resolvent:
E[GC(z)] = Sâˆ—GA(zSâˆ—)
with
Sâˆ—:= SB(zgC(z) âˆ’1).
(13.47)
13.2.4 â€œQuenchedâ€ vs â€œAnnealedâ€
The replica trick is quite burdensome as one has to keep track of n copies of an integration
vector Ïˆn and these vectors interact through the averaging process. At large N one typically
has to do a saddle point over one or several n Ã— n matrices (e.g. Q and Y in the free
addition computation of the previous section), and at the end take the n â†’0 limit. But in
all computations of Stieltjes transforms so far, taking n = 1 instead of n â†’0 gives the
correct saddle point and the correct ï¬nal result. In other words, assuming that
E[log Z] â‰ˆlog E[Z]
(13.48)
leads to the correct result. For historical reasons coming from physics, E[log Z] is called a
quenched average whereas log E[Z] is called an annealed average.
For example, if we go back to Eq. (13.21) we see that taking the logarithm of the n = 1
result gives the same result as the correct n â†’0 limit. The same is true for the Wishart
case. For the free addition and multiplication one can also compute the Stieltjes transform
using n = 1. This is a general result for bulk properties of random matrices. Most natural
ensembles of random symmetric matrices (such as those from Chapter 5 and those arising
from free addition and multiplication) feature a strong repulsion of eigenvalues. Because
of this repulsion, eigenvalues do not ï¬‚uctuate much around their classical positions â€“ see
the detailed discussion in Section 5.4.1. It is the absence of eigenvalue ï¬‚uctuations on the
global scale that makes the n = 1 and n â†’0 saddle points equivalent.
For the rank-1 hciz integral, on the other hand, things are more subtle. As we show
in the next section, the annealed average n = 1 gives the right answer in some interval

13.3 Rank-1 HCIZ and Replicas
209
of parameters, when the integral is dominated by the bulk properties of eigenvalues. Out-
side this regime, ï¬‚uctuations of the largest eigenvalue matter and the n = 1 result is no
longer correct.
13.3 Rank-1 HCIZ and Replicas
In Chapter 10 we studied the rank-1 hciz integral and deï¬ned the function HB(t) as
HB(t) := lim
Nâ†’âˆ
2
N log
6
exp
N
2 Tr TOBOT
7
O
,
(13.49)
where the averaging is done over the orthogonal group for O, T is a rank-1 matrix with
eigenvalue t and B a ï¬xed matrix. If B is a member of a random ensemble, such as the
Wigner ensemble, the averaging over O should be done for a ï¬xed B and only later the
function HB(t) can be averaged, if needed, over the randomness of B (quenched average).
One could also do an annealed average over B, deï¬ning another function Ë†H(t) as
Ë†H(t) := lim
Nâ†’âˆ
2
N log
6
exp
N
2 Tr TOBOT
7
O,B
.
(13.50)
It turns out that for small enough values of t, the two quantities are equal, i.e. Ë†H(t) =
HB(t). For larger values of t, however, these two quantities differ. The aim of this section is
to compute explicitly these quantities in the Wigner case using the replica trick, and show
that there is a phase transition for a well-deï¬ned value t = tc beyond which quenched and
annealed averages do not coincide.
13.3.1 Annealed Average
Let us compute directly the â€œannealedâ€ average when B = X is a Wigner matrix and T =
t e1eT
1, where t is the only non-zero eigenvalue of T and e1 is the unit vector (1,0, . . . ,0)T .
Then
6
exp
N
2 Tr TX
7
X
=
6
exp
Nt
2 eT
1Xe1
7
X
=

dX11
*
4Ï€Ïƒ 2/N
exp
NtX11
2
âˆ’N
4Ïƒ 2 X2
11

= exp
N
2
t2Ïƒ 2
2

,
(13.51)
so the annealed Ë†Hwig(t) is given by
Ë†Hwig(t) = Ïƒ 2
2 t2,
(13.52)
which, at least superï¬cially, coincides with the integral of the R-transform of a Wigner
matrix, Eq. (10.61).

210
The Replica Method
13.3.2 Quenched Average
The annealed average corresponds, in the replica language, to n = 1. Let us now turn to
arbitrary (integer) n. To keep notation light we will set Ïƒ 2 = 1. As in Eq. (10.31) we deï¬ne
the partition function
Zt(X) =

dNÏˆ
(2Ï€)N/2 Î´

âˆ¥Ïˆâˆ¥2 âˆ’Nt

exp
1
2Ïˆ T XÏˆ

,
(13.53)
seeking to compute, at the end of the calculation,
E[HX(t)] = lim
Nâ†’âˆ
2
N lim
nâ†’0
Zn
t (X) âˆ’1
n

âˆ’1 âˆ’log t,
(13.54)
where 1+log t is the large N limit of 2/N log Zt(X = 0), with Zt(0) given by Eq. (10.38).
If we write Zn
t (X) as multiple copies of the same integral and express the Dirac deltas as
Fourier integrals over zÎ±, we get
Zn
t (X) =
 iâˆ
âˆ’iâˆ
n
,
Î±=1
dzÎ±

n
,
Î±=1
dNÏˆÎ±
(2Ï€)N/2 exp

1
2
n

Î±=1

NzÎ±t âˆ’zÎ±Ïˆ T
Î±ÏˆÎ±

+ 1
2
n

Î±=1
Ïˆ T
Î±XÏˆÎ±

.
(13.55)
In order to take the expectation value over the Gaussian random matrix X, we need as
always to separate the diagonal and off-diagonal elements of X. The steps are the same as
those we took in Section 13.1.2:
E
-
exp

1
2
n

Î±=1
Ïˆ T
Î±XÏˆÎ±
.
= exp
â›
â
N

i,j=1
1
4N
 n

Î±=1
ÏˆÎ±iÏˆÎ±j
2â
â 
= exp
â›
â1
4N
n

Î±,Î²=1
 N

i=1
ÏˆÎ±iÏˆÎ²i
2â
â ,
(13.56)
which can be rewritten as a Hubbardâ€“Stratonovich integral over an n Ã— n matrix q:
E[. . .] = C(n)

dq exp
â›
ââˆ’N Tr q2
4
+
N

i=1
n

Î±,Î²=1
qÎ±Î²ÏˆÎ±iÏˆÎ²i
2
â
â ,
(13.57)
where C(n) is a numerical coefï¬cient. After Gaussian integration, one thus ï¬nds
E[Zn
t ] =
 iâˆ
âˆ’iâˆ
n
,
Î±=1
dzÎ±

dqC(n) exp
(N
2

t Tr z âˆ’Tr q2
2
âˆ’Tr log(z âˆ’q)
)
, (13.58)
which makes sense provided that the real part of z is larger than all the eigenvalues of q.
We now deï¬ne
Fn(q,z;t) = t Tr z âˆ’Tr q2
2
âˆ’Tr log(z âˆ’q),
(13.59)

13.3 Rank-1 HCIZ and Replicas
211
where z is the vector of zÎ± treated as a diagonal matrix. For n â‰¥1 we need to ï¬nd a
saddle point in the space of n Ã— n matrices z and q, i.e. a point in that space where the ï¬rst
derivatives of Ft(q,z) are zero with a negative Hessian.
As a check, for n = 1 we have at the saddle point
qâˆ—=
1
zâˆ—âˆ’qâˆ—
and
t =
1
zâˆ—âˆ’qâˆ—
â‡’
zâˆ—= t + 1
t
and
qâˆ—= t.
(13.60)
Hence
F1(qâˆ—,zâˆ—;t) = t2 + 1 âˆ’t2
2 + log t,
(13.61)
or
Ë†Hwig(t) = t2
2 ,
(13.62)
as it should be.
We now go back to the general n case. Using Eq. (1.37) we can take a matrix derivative
of Eq. (13.59) with respect to q and z:
q = (z âˆ’q)âˆ’1
and
&
(z âˆ’q)âˆ’1'
Î±Î± = t.
(13.63)
In the following technical part, we solve these equations for integer n â‰¥1. We discuss the
ï¬nal result at the end of this subsection.
The second equation in (13.63) comes from the derivative with respect to z; remember
z is only a diagonal matrix, the derivative with respect to z tells us only about the diagonal
elements.
From this we can argue that z must be a multiple of the identity and q of the form3
q =
â›
âœâœâ
t
b
. . .
b
b
t
. . .
b
...
...
...
b
b
b
. . .
t
â
âŸâŸâ ,
(13.64)
for some b to be determined. To ï¬nd an equation for b and z we need to express Eqs.
(13.63) in terms of those quantities. To do so we ï¬rst write the matrix q as a rank-1
perturbation of a multiple of the identity matrix:
q = (t âˆ’b)1 + nbP1,
(13.65)
where P1 = eeT is the projector onto the normalized vector of all 1:
e =
1
âˆšn
â›
âœâœâ
1
1
...
1
â
âŸâŸâ .
(13.66)
3 More complicated, block diagonal structures for q sometimes need to be considered in the limit n â†’0. This is called â€œreplica
symmetry breakingâ€, a phenomenon that occurs in many â€œcomplexâ€ optimization problems, such as spin-glasses â€“ see Section
13.4. Fortunately, in the present case, these complications are not present.

212
The Replica Method
Note that the eigenvalues of the matrix q are (t âˆ’b) + nb (with multiplicity 1) and (t âˆ’b)
(with multiplicity (n âˆ’1)). Since z is a multiple of the identity, the matrix z âˆ’q is a
rank-1 perturbation of a multiple of the identity and it can be inverted using the Shermanâ€“
Morrison formula, Eq. (1.28). The ï¬rst of Eqs. (13.63) becomes
(t âˆ’b)1 + nbP1 =
1
z âˆ’t + b +
nbP1
(z âˆ’t + b)2(1 âˆ’nb(z âˆ’t + b)âˆ’1).
(13.67)
We can now equate the prefactors in front of the identity matrix 1 and of the projector P1
separately, to get two equations for our two unknowns (z and b). For the identity matrix
we get
(t âˆ’b) =
1
z âˆ’t + b
â‡’
z = (t âˆ’b) +
1
t âˆ’b .
(13.68)
For the second equation, we ï¬rst replace (z âˆ’t + b)âˆ’1 by t âˆ’b and get
nb =
(t âˆ’b)2nb
1 âˆ’nb(t âˆ’b).
(13.69)
We immediately ï¬nd one solution: b = 0. For this solution both q = q01 and z = z01
are multiples of the identity and we have q0 = t and z0 = t + tâˆ’1. This coincides with
the (unique) solution we found in the annealed (n = 1) case. For general n, there are
potentially other solutions. Simplifying off nb, we ï¬nd a quadratic equation for b:
1 âˆ’nb(t âˆ’b) = (t âˆ’b)2,
(13.70)
whose solutions we write as
bÂ± = (n âˆ’2)t Â±
*
(n2t2 âˆ’4(n âˆ’1))
2(n âˆ’1)
.
(13.71)
From the two solutions for b we can compute the corresponding values of z using Eq.
(13.68). We get a term with a square-root on the denominator that we simplify using
(c Â±
âˆš
d)âˆ’1 â‰¡(c âˆ“
âˆš
d)/(c2 âˆ’d). After further simpliï¬cation we ï¬nd
zÂ± = n2t Â± (n âˆ’2)
*
(n2t2 âˆ’4(n âˆ’1))
2(n âˆ’1)
.
(13.72)
We now need to choose one of the three solutions z0, z+ and zâˆ’. First, we consider
integer n â‰¥1 where the replica method is perfectly legitimate. We will later deal with the
n â†’0 limit.
For n = 1, zâˆ’is ill deï¬ned while z+ becomes identical to z0. The only solution for all
t is therefore z0 and we recover the annealed result discussed in the previous subsection.
For n â‰¥2, we ï¬rst notice that the solutions zÂ± do not exist for t < ts := 2
âˆš
n âˆ’1/n;
they yield a complex result when the result must be real. So for t < ts, z0 is the solution.
For larger values of t we should compare the values of Fn(qâˆ—,zâˆ—;t) and choose the
maximum one. For t > ts, the z+ solution always dominates zâˆ’, so we only consider
z+ and z0 henceforth.
For n = 2, the analysis is easy, ts = 1 and tc = 1: at t = 1, z+ = z0 and for t > 1
the z+ solution dominates. For n > 2, the situation is a bit more subtle. The z+ solution
appears at ts < 1 but at that point z0 still dominates. At t = 1, z+ dominates. At some
t = tc, with ts < tc < 1, we must have Fn(q0,z0;tc) = Fn(q+,z+;tc). This point could
in principle be shown analytically but it is easier numerically. In particular we do not have
an analytical expression for tc(n) except the above bound tc(n > 2) < 1 and the value
tc(2) = 1 (see Fig. 13.1).

13.3 Rank-1 HCIZ and Replicas
213
2
4
6
8
10
n
0.6
0.7
0.8
0.9
1.0
tc
tc
ts
Figure 13.1 The point tc(n) where the Fn(q0,z0;tc) = Fn(q+,z+;tc) and beyond which the z+
solution dominates. Also shown is the point ts(n) = 2
âˆš
n âˆ’1/n where the z+ solution starts to
exist. Note that ts â‰¤tc â‰¤1 for all n â‰¥2. Hence the transition appears below t = 1. Note also that
tc â†’0 as n â†’âˆ.
We can now put everything together but there is a trick to save computation effort.
Given that our solutions cancel the partial derivatives of Ft(q,z;t) with respect to q and z,
we can easily compute its derivative with respect to t:
d
dt Fn(qâˆ—,zâˆ—;t) = âˆ‚
âˆ‚t Fn(qâˆ—,zâˆ—;t) = Tr zâˆ—(t) = nzâˆ—(t).
(13.73)
Note that we can follow the value of Fn(qâˆ—,zâˆ—;t) through the critical point tc because
Fn(qâˆ—,zâˆ—;t) is continuous at that point (even if its derivative is not).
The above analysis therefore allows us to ï¬nd, for n â‰¥1,
log E[Zn
t ] âˆ¼Nn
2 Fn(t),
(13.74)
where
d
dt Fn(t) =
â§
â¨
â©
t + 1
t
for t â‰¤tc(n),
n2t+(nâˆ’2)âˆš
(n2t2âˆ’4(nâˆ’1))
2(nâˆ’1)
for t > tc(n),
(13.75)
with the boundary condition Fn(0) = 0.
We can now analytically continue this solution down to n â†’0. The ï¬rst regime, for
small t, is easy as it does not depend on n. In the large t regime, the extrapolation of z+(t)
to n â†’0 gives the very simple result (see Eq. (13.72)): z+ = 2 for all t. The most tricky
part is to ï¬nd the critical point where one goes from the z0 = t +1/t solution to the z+ = 2
solution. We cannot analytically continue tc(n) to n â†’0, as we have no explicit formula
for it. On the other hand, we can directly ï¬nd the point tc(n = 0) at which the two solutions

214
The Replica Method
0
1
2
3
4
5
t
0.0
2.5
5.0
7.5
10.0
12.5
HX(t)
quenched
annealed
upper bound
Figure 13.2 The function HX(t) for a unit Wigner computed with a â€œquenchedâ€ average (hciz
integral) and an â€œannealedâ€ average. We also show the upper bound given by Eq. (10.59). The
annealed and quenched averages are identical up to t = tc = 1 and differ for larger t. The annealed
average violates the bound, which is expected as in this case Î»max ï¬‚uctuates and exceptionally large
values of Î»max dominate the average exponential hciz integral.
lead to the same F0. It is relatively straightforward to show that this point is tc = 1.4
Correspondingly,
d
dt F0(t) =
$ t + 1
t
for t â‰¤tc(0) = 1,
2
for t > tc(0) = 1.
(13.76)
We can now go back to the deï¬nition of the function E[HX(t)] (Eq. (13.54)). After
taking the n â†’0 and N â†’âˆlimits we ï¬nd (see Fig. 13.2)
d
dt E[HX(t)] =
$ t
for t â‰¤1,
2 âˆ’1
t
for t > 1,
(13.77)
with the condition E[HX(t = 0)] = 0.
The upshot of this (rather complex) calculation is that, as announced in the introduction,
for t â‰¤tc = 1 the quenched and annealed results coincide, i.e. Ë†Hwig(t) = E[HX(t)].
For t > tc, on the other hand, the two quantities are different. The reason is that for
sufï¬ciently large values of t, the average hciz integral becomes dominated by very rare
Wigner matrices that happen to have a largest eigenvalue signiï¬cantly larger than the
Wigner semi-circle edge Î» = 2. This allows Ë†Hwig(t) to continue its quadratic progression,
while E[HX(t)] is dominated by the edge of the average spectrum and its growth with t
is contained (see Fig. 13.2). When one computes higher and higher moments of the hciz
4 Something peculiar happens as n â†’0, namely the minimum solution becomes the maximum one and vice versa. In other
words for small t, where we know that z0 is the right solution, we have F0(z0;t) < F0(z+;t) and the opposite at large t. This
paradox is always present within the replica method when n â†’0.

13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
215
integral (i.e. as the number of replicas n increases), the dominance of extreme eigenvalues
becomes more and more acute, leading to a smaller and smaller transition point tc(n).5
13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
â€œSpin-glassesâ€ are disordered magnetic materials exhibiting a freezing transition as the
temperature is reduced. Typical examples are silverâ€“manganese (or copperâ€“manganese)
alloys, where the manganese atoms carry a magnetic spin and are randomly dispersed
in a non-magnetic matrix. Contrary to a ferromagnet (i.e. usual magnets like permalloy),
where all microscopic spins agree to point more or less in the same direction when the
temperature is below a certain transition temperature (872 K for permalloy), spins in spin-
glasses freeze, but the conï¬guration they adopt is disordered, â€œamorphousâ€, with zero net
magnetization.
A simple model to explain the phenomenon is the following. The energy of N spins
Si = Â±1 is given by
H({S}) = 1
2
N

i,j=1
JijSiSj,
(13.78)
where J is a random matrix, which we take to be drawn from a rotational invariant
ensemble, i.e. J = OOT where O is chosen according to the (ï¬‚at) Haar measure over
O(N) and  is a certain ï¬xed diagonal matrix with Ï„() = 0, such that any pair of spins is
as likely to want to point in the same direction or in opposite directions. The simplest case
corresponds to J = X, a Wigner matrix, in which case the spectrum of  is the Wigner
semi-circle. This case corresponds to the celebrated Sherringtonâ€“Kirkpatrick (SK) model,
but other cases have been considered in the literature as well.
The physical properties of the system are encoded in the average free energy F,
deï¬ned as
F := âˆ’T EJ

log Z

;
Z :=

{S}
exp
H({S})
T

,
(13.79)
where the partition function Z is obtained as the sum over all 2N conï¬gurations of the N
spins, and T is the temperature. One of the difï¬culties of the theory of spin-glasses is to
perform the average over the interaction matrix J of the logarithm of Z. Once again, one
can try to use the replica trick to perform this average, to wit,
EJ

log Z

=
âˆ‚
âˆ‚nEJ

Znn=0
.
(13.80)
One then computes the right hand side for integer n and hopes that the analytic continua-
tion to n â†’0 makes sense. Introducing n replicas of the system, one has
EJ

Zn
= EJ
â¡
â£

{S1,S2,...,Sn}
exp
â›
â
n
Î±=1
N
i,j=1 JijSÎ±
i SÎ±
j
2T
â
â 
â¤
â¦.
(13.81)
Now, for a ï¬xed conï¬guration of all nN spins {S1,S2, . . . ,Sn}, the N Ã—N matrix K(n)
ij
:=
n
Î±=1 SÎ±
i SÎ±
j /N is at most of rank n, which is small compared to N which we will
5 A very similar mechanism is at play in Derridaâ€™s random energy model, see Derrida [1981].

216
The Replica Method
take to inï¬nity. So we have to compute a low-rank hciz integral, which is given by
Eq. (10.45):
EJ
(
exp
 N
2T Tr OOT K(n)
)
â‰ˆexp
N
2 Tr HJ(K(n)/T )

,
(13.82)
where HJ is the anti-derivative of the R-transform of J (or ). Now the non-zero eigen-
values of the N Ã— N matrix K(n) are the same as those of the n Ã— n matrix QÎ±Î² =
N
i=1 SÎ±
i SÎ²
i /N, called the overlap matrix, because its elements measure the similarity of
the conï¬gurations {Sa} and {Sb}. Hence, we have to compute
EJ

Zn
=

{S1,S2,...,Sn}
exp
N
2 Trn HJ(Q/T )

.
(13.83)
It should be clear to the reader that all the steps above are very close to the ones followed in
Section 13.2.2. We continue in the same vein, introducing a new nÃ—n matrix for imposing
the constraint QÎ±Î² = N
i=1 SÎ±
i SÎ²
i /N:
1 =
 iâˆ
âˆ’iâˆ
Nn(n+1)/2dY
23n/2Ï€n/2

dQ exp
â›
ââˆ’N Trn QY +
n

Î±,Î²=1
YÎ±,Î²
N

i=1
SÎ±
i SÎ²
i
â
â . (13.84)
The nice thing about this representation is that sums over i become totally decoupled. So
we get
EJ

Zn
= C
 iâˆ
âˆ’iâˆ
dY

dQ exp
N
2 Trn HJ(Q/T ) âˆ’N Trn QY + NS(Y)

,
(13.85)
where C is an irrelevant constant and
S(Y) := log Z
with
Z :=
â¡
â£
S
e
n
Î±,Î²=1 YÎ±,Î²SÎ±SÎ²
â¤
â¦.
(13.86)
In the large N limit, Eq. (13.85) can be estimated using a saddle point method over Y and
Q. As in Section 13.2.2 the ï¬rst equation reads
YÎ±Î² = 1
2T [RJ(Q/T )]Î±Î²,
(13.87)
and taking derivatives with respect to YÎ±Î², we ï¬nd
QÎ±Î² = 1
Z

S
SÎ±SÎ²e
n
Î±â€²,Î²â€²=1 YÎ±â€²,Î²â€²SÎ±â€²SÎ²â€²
,
(13.88)
which leads to the following self-consistent equation for Q:
QÎ±Î² =

S SÎ±SÎ²e
1
2T
n
Î±â€²,Î²â€²=1[RJ(Q/T )]Î±â€²Î²â€²SÎ±â€²SÎ²â€²

S e
1
2T
n
Î±â€²,Î²â€²=1[RJ(Q/T )]Î±â€²Î²â€²SÎ±â€²SÎ²â€²
:= âŸ¨SÎ±SÎ²âŸ©T .
(13.89)
At sufï¬ciently high temperatures, one can expect the solution of these equations to be
â€œreplica symmetricâ€, i.e.
QÎ±Î² = Î´Î±Î²(1 âˆ’q) + q.
(13.90)

13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
217
This matrix has two eigenvalues, one non-degenerate equal to 1 + (n âˆ’1)q and another
(n âˆ’1)-fold degenerate equal to 1 âˆ’q. Correspondingly, RJ(Q) has eigenvalues
RJ((1 + (n âˆ’1)q)/T ) and RJ((1 âˆ’q)/T ), from which we reconstruct the diagonal
and off-diagonal elements of RJ(Q):
r : = [RJ(Q)]Î±Î² = 1
n

RJ((1 + (n âˆ’1)q)/T ) âˆ’RJ((1 âˆ’q)/T )

,
rd : = [RJ(Q)]Î±Î± = RJ((1 âˆ’q)/T ) + r.
(13.91)
Injecting in the deï¬nition of Z, we ï¬nd
Z =

S
exp
â›
â1
2T
â¡
â£nrd + r
n

Î±Î²=1
SÎ±SÎ²
â¤
â¦
â
â ,
(13.92)
where we have used S2Î± â‰¡1. Writing n
Î±Î²=1 SÎ±SÎ² =
n
Î± SÎ±2 âˆ’n and using a
Hubbardâ€“Stratonovich transformation, one gets
Z =

S
en rd âˆ’r
2T
 +âˆ
âˆ’âˆ
dx exp

âˆ’x2
2 + x
1 r
T
n

Î±
SÎ±

.
(13.93)
The sums of different SÎ± now again decouple, leading to
Z = en rd âˆ’r
2T +n log 2
 +âˆ
âˆ’âˆ
dx eâˆ’x2
2
âˆš
2Ï€
coshn
(
x
1 r
T
)
.
(13.94)
Now, one can notice that
n

Î±Î²=1
âŸ¨SÎ±SÎ²âŸ©T = n(n âˆ’1)q = 2T âˆ‚log Z
âˆ‚r
,
(13.95)
to get, in the limit n â†’0 and a few manipulations (including an integration by parts), an
equation involving only q:
q =
 +âˆ
âˆ’âˆ
dx
âˆš
2Ï€
eâˆ’x2
2 tanh2
(
x
1 r
T
)
,
(13.96)
where, in the limit n â†’0,
r = q
T Râ€²
J
1 âˆ’q
T

.
(13.97)
Clearly, q = 0 is always a solution of this equation. The physical interpretation of q is
the following: choose randomly two microscopic conï¬gurations of spins {SÎ±
i } and {SÎ²
i },
each with a weight given by exp
 H({S})
T

/Z. Then, the average overlap between these
conï¬gurations, N
i=1 SÎ±
i SÎ²
i /N, is equal to q. When q = 0, these two conï¬gurations are
thus uncorrelated. One expects this to be the case at high enough temperature, where the
system explores randomly all conï¬gurations.
When the spins start freezing, on the other hand, one expects the system to strongly
favor some (amorphous) conï¬gurations over others. Hence, one expects that q > 0

218
The Replica Method
in the spin-glass phase. Expanding the right hand side of Eq. (13.96) for small q
gives
 +âˆ
âˆ’âˆ
dx
âˆš
2Ï€
eâˆ’x2
2 tanh2
(
x
1 r
T
)
= q
Râ€²
J(1/T )
T 2
âˆ’q2
â›
âRâ€²â€²
J(1/T )
T 3
+ 2

Râ€²
J(1/T )
T 2
2â
â + O(q3).
(13.98)
Assuming that the coefï¬cient in front of the q2 term is negative, we see that a non-zero q
solution appears continuously below a critical temperature Tc given by
T 2
c = Râ€²
J
 1
Tc

.
(13.99)
When J is a Wigner matrix, the spin-glass model is the one studied originally by Sherring-
ton and Kirkpatrick in 1975. In this case, RJ(x) = x and therefore Tc = 1. There are cases,
however, where the transition is discontinuous, i.e. where the overlap q jumps from zero
for T > Tc to a non-zero value at Tc. In these cases, the small q expansion is unwarranted
and another method must be used to ï¬nd the critical temperature. One example is the
â€œrandom orthogonal modelâ€, where the coupling matrix J has N/2 eigenvalues equal to
+1 and N/2 eigenvalues equal to âˆ’1.
The spin-glass phase T < Tc is much more complicated to analyze, because the replica
symmetric ansatz QÎ±Î² = Î´Î±Î²(1 âˆ’q) + q is no longer valid. One speaks about â€œreplica
symmetry breakingâ€, which encodes an exquisitely subtle, hierarchical organization of the
phase space of these models. This is the physical content of the celebrated Parisi solution
of the SK model, but is much beyond the scope of the present book, and we encourage the
curious reader to learn more from the references given below.
Bibliographical Notes
â€¢ The replica trick was introduced by
â€“ R. Brout. Statistical mechanical theory of a random ferromagnetic system. Physical
Review, 115:824â€“835, 1959,
and popularized in
â€“ S. F. Edwards and P. W. Anderson. Theory of spin glasses. Journal of Physics F:
Metal Physics, 5(5):965, 1975,
for an authoritative book on the subject, see
â€“ M. MÂ´ezard, G. Parisi, and M. A. Virasoro. Spin Glass Theory and Beyond. World
Scientiï¬c, Singapore, 1987,
see also
â€“ M. MÂ´ezard and G. Parisi. Replica ï¬eld theory for random manifolds. Journal de
Physique I, France, 1(6):809â€“836, 1991.
â€¢ The replica method was ï¬rst applied to random matrices by
â€“ S. F. Edwards and R. C. Jones. The eigenvalue spectrum of a large symmetric random
matrix. Journal of Physics A: Mathematical and General, 9(10):1595, 1976,
it is rarely discussed in random matrix theory books with the exception of

13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
219
â€“ G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018.
â€¢ For a replica computation for the average resolvent for free addition and free product,
see
â€“ J. Bun, R. Allez, J.-P. Bouchaud, and M. Potters. Rotational invariant estimator for
general noisy matrices. IEEE Transactions on Information Theory, 62:7475â€“7490,
2016,
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1â€“109, 2017.
â€¢ For Derridaâ€™s random energy model where different moments of Z have different transi-
tion temperatures, see
â€“ B. Derrida. Random-energy model: An exactly solvable model of disordered systems.
Physical Review B, 24:2613â€“2626, 1981.
â€¢ For an introduction to experimental spin-glasses, see
â€“ K. H. Fischer and J. A. Hertz. Spin Glasses. Cambridge University Press, Cambridge,
1991,
â€“ A. P. Young. Spin Glasses and Random Fields. World Scientiï¬c, Singapore, 1997,
see also M. MÂ´ezard, G. Parisi, M. Virasoro, op. cit.
â€¢ For general orthogonal invariant spin-glasses, see
â€“ R. Cherrier, D. S. Dean, and A. Lef`evre. Role of the interaction matrix in mean-ï¬eld
spin glass models. Physical Review E, 67:046112, 2003.

14
Edge Eigenvalues and Outliers
In many instances, the eigenvalue spectrum of large random matrices is conï¬ned to a
single interval of ï¬nite size. This is of course the case for Wigner matrices, where the
correctly normalized eigenvalues fall between Î»âˆ’= âˆ’2 and Î»+ = +2, with a semi-
circular distribution between the two edges, and, correspondingly, a square-root singularity
of the density of eigenvalues close to the edges. This is also the case for Wishart matrices,
for which again the density of eigenvalues has square-root singularities close to both edges.
As discussed in Section 5.3.2, this is a generic property, with a few notable exceptions. One
example is provided by Wishart matrices with parameter q = 1, for which the eigenvalue
spectrum extends down to Î» = 0 with an inverse square-root singularity there. Another
case is that of Wigner matrices constrained to have all eigenvalues positive: the spectrum
also has an inverse square-root singularity â€“ see Eq. (5.94). One speaks of a â€œhard edgeâ€ in
that case, because the minimum eigenvalue is imposed by a strict constraint. The Wigner
semi-circle edge at Î»+ = 2, on the other hand, is â€œsoftâ€ and appears naturally as a result
of the minimization of the energy of a collection of interacting Coulomb charges in an
external potential.1
Consider for example Wigner matrices of size N. The existence of sharp edges delimit-
ing a region where one expects to see a non-zero density of eigenvalues from a region where
there should be none is only true in the asymptotically large size limit N â†’âˆ. For large
but ï¬nite N, on the other hand, one expects that the probability to ï¬nd an eigenvalue beyond
the Wigner sea is very small but non-zero. The width of the transition region, and the tail
of the density of states was investigated a while ago, culminating in the beautiful results by
Tracy and Widom on the distribution of the largest eigenvalue of a random matrix, which
we will describe in the next section. The most important result is that the width of the region
around Î»+ within which one expects to observe the largest eigevalue of a Wigner matrix
goes down as Nâˆ’2/3.
Hence the largest eigenvalue Î»max does not ï¬‚uctuate very far away from the classical
edge Î»+. Take for example N = 1000; Î»max is within 1000âˆ’2/3 = 0.01 away from Î»+ = 2.
In real applications the largest eigenvalue can deviate quite substantially from the classical
1 Note that there are also cases where the soft edge has a different singularity, see Section 5.3.3, and cases where the eigenvalue
spectrum extends up to inï¬nity, for example â€œLÂ´evy matricesâ€ with iid elements of inï¬nite variance.
220

14.1 The Tracyâ€“Widom Regime
221
edge. The origin of such a large eigenvalue is usually not an improbably large Tracyâ€“
Widom ï¬‚uctuation but rather a true outlier that should be modeled separately. This is the
goal of the present chapter. We will see in particular that perturbing a Wigner (or Wishart)
matrix with a deterministic, low-rank matrix of sufï¬cient amplitude a > ac generates
â€œtrueâ€ outliers, which remain at a distance O(a) from the upper edge. For a < ac on the
other hand, the largest eigenvalue remains at distance Nâˆ’2/3 from Î»+.
14.1 The Tracyâ€“Widom Regime
The Tracyâ€“Widom result characterizes precisely the distance between the largest eigen-
value Î»max of Gaussian Wigner or Wishart matrices and the upper edge of the spectrum
which we denoted by Î»+. This result can be (formally) stated as follows: the rescaled
distribution of Î»max âˆ’Î»+ converges, for N â†’âˆ, towards the Tracyâ€“Widom distribution,
usually noted F1:
P

Î»max â‰¤Î»+ + Î³ Nâˆ’2/3u

= F1(u),
(14.1)
where Î³ is a constant that depends on the problem and F1(u) is the Î² = 1 Tracyâ€“Widom
distribution. For the Wigner problem, Î»+ = 2 and Î³ = 1, whereas for Wishart matrices,
Î»+ = (1 + âˆšq)2 and Î³ = âˆšqÎ»2/3
+ . In fact, Eq. (14.1) holds for a much wider large class
of N Ã— N random matrices, for example symmetric random matrices with arbitrary iid
elements with a ï¬nite fourth moment. The Tracyâ€“Widom distribution for all three values
of Î² is plotted in Figure 14.1. (The case where the fourth moment is inï¬nite is discussed in
Section 14.3 below.)
âˆ’4
âˆ’2
0
2
u
0.0
0.1
0.2
0.3
0.4
0.5
fb(u)
f4(u)
f2(u)
f1(u)
Figure 14.1 Rescaled and shifted probability density of the largest eigenvalue for a large class
of random matrices such as Wigner and Wishart: the Tracyâ€“Widom distribution. The distribution
depends on the Dyson index (Î²) and is shown here for Î² = 1,2 and 4.

222
Edge Eigenvalues and Outliers
Everything is known about the Tracyâ€“Widom density f1(u) = F â€²
1(u), in particular its
left and right far tails:
ln f1(u) âˆâˆ’u3/2,
(u â†’+âˆ);
ln f1(u) âˆâˆ’|u|3,
(u â†’âˆ’âˆ).
(14.2)
One notices that the left tail is much thinner than the right tail: pushing the largest eigen-
value inside the Wigner sea implies compressing the whole Coulomb gas of repulsive
charges, which is more difï¬cult than pulling one eigenvalue away from Î»+. Using this
analogy and the formalism of Section 5.4.2, the large deviation regime of the Tracyâ€“Widom
problem (i.e. for Î»max âˆ’Î»+ = O(1)) can be obtained. Note that the result is exponentially
small in N as the u3/2 behavior for u â†’âˆcombines with N2/3 to give a linear in N
dependence.
The distribution of the smallest eigenvalue Î»min around the lower edge Î»âˆ’is also Tracyâ€“
Widom, except in the particular case of Wishart matrices with q = 1. In this case Î»âˆ’= 0,
which is a â€œhardâ€ edge since all eigenvalues of the empirical matrix must be non-negative.2
The behavior of the width of the transition region can be understood using a simple
heuristic argument. Suppose that the N = âˆdensity goes to zero near the upper edge Î»+
as (Î»+ âˆ’Î»)Î¸ (generically, Î¸ = 1/2 as is the case for the Wigner and the MarË‡cenkoâ€“Pastur
distributions). For ï¬nite N, one expects not to be able to say whether the density is zero or
non-zero when the probability to observe an eigenvalue is of order 1/N, i.e. when the O(1)
eigenvalue is within the â€œblurredâ€ region. This leads to a blurred region of width
|Î»âˆ—âˆ’Î»+|Î¸+1 âˆ1
N â†’Î»âˆ—âˆ¼Nâˆ’
1
1+Î¸ ,
(14.3)
which goes to zero as Nâˆ’2/3 in the generic square-root case Î¸ = 1/2. More precisely, for
Gaussian ensembles, the average density of states at a distance âˆ¼Nâˆ’2/3 from the edge
behaves as
ÏN(Î» â‰ˆÎ»+) = Nâˆ’1/31
&
N2/3(Î» âˆ’Î»+)
'
,
(14.4)
with 1(u â†’âˆ’âˆ) â‰ˆâˆšâˆ’u/Ï€ so as to recover the asymptotic square-root singularity,
since the N dependence disappears in that limit. Far from the edge, ln 1(u â†’+âˆ) âˆ
âˆ’u3/2, showing that the probability to ï¬nd an eigenvalue outside of the allowed band
decays exponentially with N and super-exponentially with the distance to the edge. The
function 1(u) is not known analytically for real Wigner matrices (Î² = 1) but an explicit
expression is available for complex Hermitian Wigner matrices, and reads (Fig. 14.2)
2(u) = Aiâ€²2(u) âˆ’u Ai2(u),
(14.5)
with the same asymptotic behaviors as 1(u). (Ai(u) is the standard Airy function.)
2 This special case is treated in PÂ´echÂ´e [2003].

14.2 Additive Low-Rank Perturbations
223
âˆ’7.5
âˆ’5.0
âˆ’2.5
0.0
u
0.0
0.2
0.4
0.6
0.8
1.0
F2(u)
f2(u)
âˆ’u/ p
âˆ’2.5
0.0
2.5
5.0
u
10âˆ’14
10âˆ’11
10âˆ’8
10âˆ’5
10âˆ’2
F2(u)
f2(u)
exp(âˆ’4u3/ 2/ 3)/ (8pu)
Figure 14.2 Behavior of the density near the edge Î»+ at the scale Nâˆ’2/3 for complex Hermitian
Wigner matrices given by Eq. (14.5). For comparison the probability of the largest eigenvalue
f2(u) is also shown. For positive values, the two functions are almost identical and behave as
exp(âˆ’4u3/2/3)/(8Ï€u) for large u (right). For negative arguments the functions are completely
different: 2(u) behaves as âˆšâˆ’u/Ï€ for large negative u while f2(u) â†’0 as the largest eigenvalue
cannot be in the bulk (left).
14.2 Additive Low-Rank Perturbations
14.2.1 Eigenvalues
We will now study the outliers for an additive perturbation to a large random matrix. Take a
large symmetric random matrix M (e.g. Wigner or Wishart) with a well-behaved asymptotic
spectrum that has a deterministic right edge Î»+. We would like to know what happens when
one adds to M a low-rank (deterministic) perturbation. For simplicity, we only consider the
rank-1 perturbation auuT with âˆ¥uâˆ¥= 1 and a of order 1, but the results below easily
generalize to the case of a rank-n perturbation with n â‰ªN.
We want to know whether there will be an isolated eigenvalue of M + auuT outside the
spectrum of M (i.e. an â€œoutlierâ€) or not. To answer this question, we calculate the matrix
resolvent
Ga(z) =

z âˆ’M âˆ’auuT âˆ’1 .
(14.6)
The matrix Ga(z) has a pole at every eigenvalue of M + auuT . An alternative approach
would have been to study the zeros of the function det

z âˆ’M âˆ’auuT 
, but the full resol-
vent Ga(z) also gives us information about the eigenvectors.
Now we apply the Shermanâ€“Morrison formula (1.28); taking A = z âˆ’M, we get
Ga(z) = G(z) + a G(z)uuT G(z)
1 âˆ’auT G(z)u,
(14.7)

224
Edge Eigenvalues and Outliers
where G(z) is the resolvent of the original matrix M. We are looking for a real eigenvalue
such that Î»1 > Î»+. Let us take z = Î»1 âˆˆR outside the spectrum of M, so G(Î») is real and
regular. To have an outlier at Î»1, we need a pole of Ga at Î»1, i.e. the following equation
needs to be satisï¬ed:
1 âˆ’auT G(Î»1)u = 0.
(14.8)
Assume now that M is drawn from a rotationally invariant ensemble or, equivalently, that
the vector u is an independent random vector uniformly distributed on the unit sphere. In
the language of Chapter 11, we say that the perturbation auuT is free from the matrix M.
We then have, in the eigenbasis of G,
uT G(z)u =

i
u2
i Gii(z) â‰ˆ1
N Tr G(z) = gN(z)
Nâ†’âˆ
â†’
g(z).
(14.9)
Thus we have a pole when
ag(Î»1) = 1 â‡’g(Î»1) = 1/a.
(14.10)
If z(g), the inverse function of g(z), exists, we arrive at
Î»1 = z
1
a

.
(14.11)
The condition for the invertibility of g(z) happens to be precisely the same as the condition
to have an outlier, i.e. Î»1 > Î»+ â€“ see Section 10.4. We have established there that Î»1 =
z(1/a) is monotonically increasing in a, and Î»1 = Î»+ when a = aâˆ—= 1/g(Î»+), which
is the critical value of a for which an outlier ï¬rst appears. Generically, g+ = g(Î»+) is a
minimum of z(g):
dz(g)
dg

g+
= 0
when
z(g+) = Î»+.
(14.12)
For instance, for Wigner matrices, we have z(g) = Ïƒ 2g + gâˆ’1, for which
Ïƒ 2 âˆ’gâˆ’2
+ = 0 â‡’g+ = Ïƒ âˆ’1,
(14.13)
and Î»+ = z(Ïƒ âˆ’1) = 2Ïƒ, which is indeed the right edge of the semi-circle law.
In sum, for a > aâˆ—= 1/g+, there exists a unique outlier eigenvalue that is increasing
with a. The smallest value for which we can have an outlier is aâˆ—= 1/g+, corresponding
to Î»1 = Î»+. For a < aâˆ—there is no outlier to the right of Î»+.3
Using the relation between the inverse function z(g) and the R-transform (10.10), we
can express the position of the outlier as
Î»1 = R
1
a

+ a
for
a > aâˆ—= 1
g+
.
(14.14)
3 Outliers such that Î» < Î»âˆ’behave similarly, we just need to consider the matrix âˆ’M âˆ’auuT and follow the same logic.

14.2 Additive Low-Rank Perturbations
225
0
1
2
3
4
a
2.0
2.5
3.0
3.5
4.0
l1
simulation
l1 = Ëœa+ 1/ Ëœa; Ëœa = max(a,1)
Figure 14.3 Largest eigenvalue of a Gaussian Wigner matrix with Ïƒ 2 = 1 with a rank-1 perturbation
of magnitude a. Each dot is the largest eigenvalue of a single random matrix with N = 200. Equation
(14.16) is plotted as the solid curve. For a < 1, the ï¬‚uctuations follow a Tracyâ€“Widom law with
Nâˆ’2/3 scaling, while for a > 1 the ï¬‚uctuations are Gaussian with Nâˆ’1/2 scaling. From the graph,
we see ï¬‚uctuations that are indeed smaller when a < 1. They also have a negative mean and positive
skewness, in agreement with the Tracyâ€“Widom distribution.
Using the cumulant expansion of the R-transform (11.63), we then get a general expression
for large a:
Î»1 = a + Ï„(M) + Îº2(M)
a
+ O(aâˆ’2).
(14.15)
For Wigner matrices, we actually have for all a (see Fig. 14.3)
Î»1 = a + Ïƒ 2
a
for
a > aâˆ—= Ïƒ.
(14.16)
When a â†’aâˆ—, on the other hand, one has
dÎ»1(a)
da

aâˆ—= dz(g)
dg

g+=1/aâˆ—= 0.
(14.17)
Hence, one has, for a â†’aâˆ—and for generic square-root singularities,
Î»1 = Î»+ + C(a âˆ’aâˆ—)2 + O

(a âˆ’aâˆ—)3
,
(14.18)
where C is some problem dependent coefï¬cient.
By studying the ï¬‚uctuations of uG(Î»)uT around g(Î»), one can show that the ï¬‚uctuations
of the outlier around Î»1 = R(aâˆ’1) + a are Gaussian and of order Nâˆ’1/2. This is to
be contrasted with the ï¬‚uctuations of the largest eigenvalue when there are no outliers
(a < g+), which are Tracyâ€“Widom and of order Nâˆ’2/3. The transition between the two
regimes is called the Baikâ€“Ben Arousâ€“PÂ´echÂ´e (bbp) transition.

226
Edge Eigenvalues and Outliers
0.5
1.0
1.5
2.0
2.5
3.0
g
2.0
2.5
3.0
3.5
4.0
(g)
g+ 1/ g
g+
Figure 14.4 Plot of the inverse function z(g) = g + 1/g for the unit Wigner function g(z). The gray
dot indicates the point (g+,Î»+). The line to the left of this point is the true inverse of g(z): z(g) is
deï¬ned on [0,g+) and is monotonously decreasing in g. The line to the right is a spurious solution
introduced by the R-transform. Note that the point g = g+ is a minimum of z(g) = g + 1/g.
We ï¬nish this section with two remarks.
â€¢ One concerns the solutions to Eq. (14.14), and the way to ï¬nd the value aâˆ—beyond
which an outlier appears. The point is that, while the function R(g = 1/a) is well
deï¬ned for g âˆˆ[0,g+), it also often makes sense even beyond g+ (see the discussion in
Section 10.4). In that case, one will ï¬nd spurious solutions: Figure 14.4 shows a plot of
z(g) = R(g) + 1/g in the unit Wigner case, which is still well deï¬ned for g > g+ = 1
even if this function is no longer the inverse of g(z) (Section 10.4). There are two
solutions to z(g) = Î»1, one such that g < g+ and the other such that g > g+. As
noted above, the point g+ is a minimum of z(g), beyond which the relation between Î»1
and a is monotonically increasing because g(z) is monotonically decreasing for z > Î»+.
â€¢ The second concerns the case of a free rank-n perturbation, when n â‰ªN. In this case
one cannot use the Shermanâ€“Morrison formula but one can compute the R-transform
of the perturbed matrix, and infer the 1/N correction to the Stieltjes transform. The
poles of this correction term give the possible outliers. To each eigenvalue ak (k =
1, . . . ,n) of the perturbation, one can associate a candidate outlier Î»k given by
Î»k = R
 1
ak

+ ak
when
ak > 1
g+
.
(14.19)
14.2.2 Outlier Eigenvectors
The matrix resolvent in Eq. (14.7) can also tell us about the eigenvectors of the perturbed
matrix. We expect that, for a very strong rank-1 perturbation auuT , the eigenvector v1

14.2 Additive Low-Rank Perturbations
227
associated with the outlier Î»1 will be very close to the perturbation vector u. On the other
hand, for Î»1 â‰ˆÎ»+, the vector u will strongly mix with bulk eigenvectors of M so the
eigenvector v1 will not contain much information about u.
To understand this phenomenon quantitatively, we will study the squared overlap |vT
1u|2.
With the spectral decomposition of M + auuT , we can write
Ga(z) =
N

i=1
vivT
i
z âˆ’Î»i
,
(14.20)
where Î»1 denotes the outlier and v1 its eigenvector, and Î»i,vi, i > 1 all other eigenval-
ues/eigenvectors. Thus we have
lim
zâ†’Î»1
uT Ga(z)u Â· (z âˆ’Î»1) = |vT
1u|2.
(14.21)
Hence, by (14.7) and (14.9), we get
|vT
1u|2 = lim
zâ†’Î»1

g(z) + a
g(z)2
1 âˆ’ag(z)

(z âˆ’Î»1)
= lim
zâ†’Î»1
g(z) z âˆ’Î»1
1 âˆ’ag(z).
(14.22)
We cannot simply evaluate the fraction above at z = Î»1, for at that point g(Î»1) = aâˆ’1 and
we would get 0/0. We can however use lâ€™Hospitalâ€™s rule4 and ï¬nd
|vT
1u|2 = âˆ’g(Î»1)2
gâ€²(Î»1) ,
(14.24)
where we have used aâˆ’1 = g(Î»1). The right hand side is always positive since g is a
decreasing function for Î» > Î»+.
We can rewrite Eq. (14.24) in terms of the R-transform and get a more useful formula.
To compute gâ€²(z), we take the derivative with respect to z of the implicit equation z =
R(g(z)) + gâˆ’1(z) and get
1 = Râ€²(g(z))gâ€²(z) âˆ’gâ€²(z)
g2
â‡’gâ€²(z) =
1
Râ€²(g(z)) âˆ’gâˆ’2(z).
(14.25)
Hence we have
|vT
1u|2 = 1 âˆ’g(Î»1)2Râ€²(g(Î»1))
= 1 âˆ’aâˆ’2Râ€²(aâˆ’1).
(14.26)
We can now check our intuition about the overlap for large and small perturbations. For a
large perturbation a â†’âˆ, Eq. (14.26) gives
4 Lâ€™Hospitalâ€™s rule states that
lim
xâ†’x0
f (x)
g(x) = f â€²(x0)
gâ€²(x0) when f (x0) = g(x0) = 0.
(14.23)

228
Edge Eigenvalues and Outliers
0
1
2
3
4
a
0.0
0.2
0.4
0.6
0.8
1.0
|vT
1u|2
simulation
|vT
1u|2 = max(1âˆ’aâˆ’2,0)
Figure 14.5 Overlap between the largest eigenvector and the perturbation vector of a Gaussian
Wigner matrix with Ïƒ 2 = 1 with a rank-1 perturbation of magnitude a. Each dot is the overlap
for a single random matrix with N = 200. Equation (14.31) is plotted as the solid curve.
|vT
1u|2 = 1 âˆ’Îº2(M)
a2
+ O(aâˆ’3)
when
a â†’âˆ.
(14.27)
As expected, v1 â†’u when a â†’âˆ: the angle between the two vectors decreases as 1/a.
The overlap near the transition Î»1 â†’Î»+ can be analyzed as follows. The derivative of
g(z) can be written as
gâ€²(z) = âˆ’
 Î»+
Î»âˆ’
Ï(x)
(z âˆ’x)2 dx.
(14.28)
For a density that vanishes at the edge as Ï(Î») âˆ¼(Î»+ âˆ’Î»)Î¸ with exponent Î¸ between 0 and
1, we have that g(z) is ï¬nite at z = Î»+ but gâ€²(z) diverges at the same point, as |z âˆ’Î»+|Î¸âˆ’1.
From Eq. (14.24), we have in that case5
|vT
1u|2 âˆ(Î»1 âˆ’Î»+)1âˆ’Î¸
when
Î»1 â†’Î»+.
(14.29)
In the generic case, one has Î¸ = 1/2 and, from Eq. (14.18), Î»1âˆ’Î»+ âˆ(aâˆ’aâˆ—)2, leading to
|vT
1u|2 âˆa âˆ’aâˆ—
when
a â†’aâˆ—.
(14.30)
These general results are nicely illustrated by Wigner matrices, for which R(x) = Ïƒ 2x.
The overlap is explicitly given by (see Fig. 14.5)
|vT
1u|2 = 1 âˆ’
aâˆ—
a
2
when
a > aâˆ—= Ïƒ.
(14.31)
5 In Chapter 5, we encountered a critical density where Ï(Î») behaves as (Î»+ âˆ’Î»)Î¸ with an exponent Î¸ = 3
2 > 1. In this case
gâ€²(z) does not diverge as z â†’Î»+ and the squared overlap at the edge of the bbp transition does not go to zero (ï¬rst order
transition). For example for the density given by Eq. (5.59) we ï¬nd |vT
1 u|2 = 4
9 at the edge Î»1 = 2
âˆš
2.

14.3 Fat Tails
229
As a â†’aâˆ—, Î»1 â†’2Ïƒ and |vT
1u|2 = 2(a âˆ’aâˆ—)/aâˆ—â†’0: the eigenvector becomes
delocalized as the eigenvalue merges with the bulk. For a < aâˆ—, one can rigorously show
that there is no information left in the eigenvalues of the perturbed matrix that would allow
us to reconstruct u.
Note that for Î»1 > Î»+, |vT
1u|2 is of order unity. In Chapter 19 we will see that this is
not the case for the overlaps between perturbed and unperturbed eigenvectors in the bulk,
which have typical sizes of order Nâˆ’1.
Exercise 14.2.1
Additive perturbation of a Wishart matrix
Deï¬ne a modiï¬ed Wishart matrix W1 such that every element (W1)ij =
Wij + a/N, where W is a standard Wishart matrix and a is a constant of order
1. W1 is a standard Wishart matrix plus a rank-1 perturbation W1 = W + auuT .
(a)
What is the normalized vector u in this case?
(b)
Using Eqs. (14.14) and (10.15) ï¬nd the value of the outlier and the minimal a
in the Wishart case.
(c)
The square-overlap between the vector u and the new eigenvector v1 is given
by Eq. (14.26). Give an explicit expression in the Wishart case.
(d)
Generate a large modiï¬ed Wishart (q = 1/4, N = 1000) for a few a in the
range [1,5]. Compute the largest eigenvalue Î»1 and associated eigenvector v1.
Plot Î»1 and |vT
1u|2 as a function of a and compare with the predictions of (b)
and (c).
14.3 Fat Tails
The previous section allows us to discuss the very interesting situation of real symmetric
random matrices X with iid elements Xij that have a fat-tailed distribution, but with a ï¬nite
variance (the case of inï¬nite variance will be alluded to at the end of the section). In order
to have eigenvalues of order unity, the random elements must be of typical size Nâˆ’1/2, so
we write
Xij = xij
âˆš
N
,
(14.32)
with xij distributed according to some density P(x) of mean zero and variance unity, but
that decays as Î¼|x|âˆ’1âˆ’Î¼ for large x. This means that most elements Xij are small, of
order Nâˆ’1/2, with some exceptional elements that are of order unity. The probability that
|Xij| > 1 is actually given by
P(|Xij| > 1) â‰ˆ2
 âˆ
âˆš
N
dx
Î¼
x1+Î¼ =
2
NÎ¼/2 .
(14.33)
Since there are in total N(N âˆ’1)/2 â‰ˆN2/2 such random variables, the total number of
such variables that exceed unity is given by N2âˆ’Î¼/2. Hence, for Î¼ > 4, this number tends

230
Edge Eigenvalues and Outliers
2
3
4
5
6
lmax
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
101
P(lmax)
Tracy-Widom
N = 1000
Figure 14.6 Distribution of the largest eigenvalue of N = 1000 Wigner matrices with elements
drawn from a distribution with Î¼ = 5 compared with the prediction from the Tracyâ€“Widom
distribution. Even though as N â†’âˆthis distribution converges to Tracyâ€“Widom, at N = 1000
there is no agreement between the laws as the power law tail P(Î»max > x) âˆ¼xâˆ’5/
âˆš
N still
dominates.
to zero with N: there are typically no such large elements in the considered random matrix.
Since each pair of large entries Xij = Xji can be considered as a rank-2 perturbation of a
matrix with all elements of order Nâˆ’1/2, one concludes that for Î¼ > 4 there are no outliers,
and the statistics of the largest eigenvalue is given by the Tracyâ€“Widom distribution around
Î»+ = 2. This hand-waving argument can actually be made rigorous: in the large N limit,
the ï¬niteness of the fourth moment of the distribution of matrix elements is sufï¬cient to
ensure that the largest eigenvalue is given by the Tracyâ€“Widom distribution. However, one
should be careful in interpreting this result, because although very large elements appear
with vanishing probability, they still dominate the tail of the Tracyâ€“Widom distribution for
ï¬nite N. The reason is that, whereas the former decreases as N2âˆ’Î¼/2, the latter decreases
much faster, as exp(âˆ’N(Î»max âˆ’Î»+)3/2) (see Fig. 14.6).
Now consider the case 2 < Î¼ < 4. Since Î¼ > 2, the variance of Xij is ï¬nite and
one knows that the asymptotic distribution of eigenvalues of X is given by the Wigner
semi-circle, with Î»+ = 2. But now the number of large entries in the matrix X grows
with N as N2âˆ’Î¼/2, which is nevertheless still much smaller than N. Each large pair of
entries Xij = Xji = a larger than unity (in absolute value) contributes to two outliers,
given by Î» = Â±(a + 1/a). So there are in total O(N2âˆ’Î¼/2) outliers, the density of which
is given by
Ïout(Î» > 2) = N1âˆ’Î¼/2
 âˆ
1
dx
Î¼
x1+Î¼ Î´

Î» âˆ’x âˆ’1
x

.
(14.34)

14.4 Multiplicative Perturbation
231
This is a rather strange situation where the density of outliers goes to zero as N â†’âˆas
soon as Î¼ > 2, but at the same time the largest eigenvalue in this problem goes to inï¬nity as
Î»max âˆ¼N
2
Î¼ âˆ’1
2,
2 < Î¼ < 4.
(14.35)
Finally, let us brieï¬‚y comment on the case Î¼ < 2, for which the variance of the entries
of X diverges (a case called â€œLÂ´evy matricesâ€ in the literature). For eigenvalues to remain
of order unity, one needs to scale the matrix elements differently, as
Xij = xij
N
1
Î¼
,
(14.36)
with
P(x)
âˆ¼
|x|â†’âˆ

(1 + Î¼) sin( Ï€Î¼
2 )
|x|1+Î¼
,
(14.37)
where the funny factor involving the gamma function is introduced for convenience only.
The eigenvalue distribution is no longer given by a semi-circle. In fact, the support of the
distribution is in this case unbounded. For completeness, we give here the exact expression
of the distribution in terms of LÂ´evy stable laws LC,Î²
Î¼
(u), where Î² is called the asymmetry
parameter and C the scale parameter.6 For a given value of Î», one should ï¬rst solve the
following two self-consistent equations for C and Î²:
C =
 +âˆ
âˆ’âˆ
dg|g|Î¼/2âˆ’2LC,Î²
Î¼/2(Î» âˆ’1/g),
CÎ² =
 +âˆ
âˆ’âˆ
dg sign(g)|g|Î¼/2âˆ’2LC,Î²
Î¼/2(Î» âˆ’1/g).
(14.38)
Finally, the distribution of eigenvalues ÏL(Î») of LÂ´evy matrices is obtained as
ÏL(Î») = LC(Î»),Î²(Î»)
Î¼/2
(Î»).
(14.39)
One can check in particular that this distribution decays for large Î» exactly as P(x)
itself. In other words, the tail of the eigenvalue distribution is the same as the tail of the
independent entries of the LÂ´evy matrix.
14.4 Multiplicative Perturbation
In data-analysis applications, we often need to understand the largest eigenvalue of a sample
covariance matrix. A true covariance with a few isolated eigenvalues can be treated as a
matrix C0 with no isolated eigenvalue plus a low-rank perturbation. The passage from the
true covariance to the sample covariance is equivalent to the free product of the true covari-
ance with a white Wishart matrix with appropriate aspect ratio q = N/T . To understand
such matrices, we will now study outliers for a multiplicative process.
Consider the free product of a certain covariance matrix C0 with a rank-1 perturbation
and another matrix B:
E = B
1
2 C
1
2
0

1 + auuT 
C
1
2
0 B
1
2,
(14.40)
6 More precisely, LC,Î²
Î¼
(x) is the Fourier transform of exp(âˆ’C|k|Î¼(1 + iÎ²sign(k) tan(Ï€Î¼/2))) for Î¼  1, and of
exp(âˆ’C|k|(1 + i(2Î²/Ï€)sign(k) log |k|)) for Î¼ = 1.

232
Edge Eigenvalues and Outliers
where u is a normalized eigenvector of C0 with eigenvalue Î»0, and B is positive semi-
deï¬nite, free from C0, and with Ï„(B) = 1. In the special case where B is a white Wishart,
our problem corresponds to a noisy observation of a perturbed covariance matrix, where
one of the modes (the one corresponding to u) has a variance boosted by a factor (1 + a).
The matrix E0 := B
1
2 C0B
1
2 has an unperturbed spectrum with a lower edge Î»âˆ’and an
upper edge Î»+. We want to establish, as in the additive case, the condition for the existence
of an outlier Î»1 > Î»+ or Î»1 < Î»âˆ’, and the exact position of the outlier when it exists.
The eigenvalues of E are the zeros of its characteristic polynomial, in particular for the
largest eigenvalue Î»1 we have
det(Î»11 âˆ’E0 âˆ’aB
1
2 C0uuT B
1
2 ) = 0.
(14.41)
We are looking for an eigenvalue outside the spectrum of E0, i.e. Î»1 > Î»+ or Î»1 < Î»âˆ’. For
such a Î»1, the matrix Î»11 âˆ’E0 is invertible and we can use the matrix determinant lemma
Eq. (1.30):
det(A + uvT ) = det A Â·

1 + vT Aâˆ’1u

,
(14.42)
with A = Î»11 âˆ’E0 and u = âˆ’v = âˆšaB
1
2 C
1
2
0 u. Equation (14.41) becomes
det(Î»11 âˆ’E0) Â·

1 âˆ’auT C
1
2
0 B
1
2 G0(Î»1)B
1
2 C
1
2
0 u

= 0,
(14.43)
where we have introduced the matrix resolvent G0(Î»1) := (Î»11 âˆ’E0)âˆ’1. As we said, the
matrix Î»11âˆ’E0 is invertible so its determinant is non-zero. Thus any outlier needs to solve
aÎ»0uT B
1
2 G0(Î»1)B
1
2 u = 1.
(14.44)
Again we assume that B is a rotationally invariant matrix with respect to C0. Then we
know that in the large N limit G0(z) is diagonal in the basis of B and reads (see, mutatis
mutandis, Eq. (13.47))
G0(z) â‰ˆSâˆ—(z)GB(zSâˆ—(z)),
Sâˆ—(z) := SC0(zg0(z) âˆ’1).
(14.45)
Furthermore, since u and B are also free,
uT B
1
2 G0(Î»1)B
1
2 u â‰ˆNâˆ’1 Tr

B
1
2 G0(Î»1)B
1
2

â‰¡Sâˆ—(Î»1)tB(Î»1Sâˆ—(Î»1)),
(14.46)
where we have recognized the T-transform of the matrix B:
tB(z) := Ï„

B(z âˆ’B)âˆ’1
.
(14.47)
Thus, the position of the outlier Î»1 is given by the solution of
aÎ»0Sâˆ—(Î»1)tB(Î»1Sâˆ—(Î»1)) = 1.
(14.48)
In order to keep the calculation simple, we now assume that C0 = 1. In this case, Sâˆ—= 1
and Î»0 = 1, so the equation simpliï¬es to

14.4 Multiplicative Perturbation
233
atB(Î»1) = 1.
(14.49)
To know whether this equation has a solution we need to know if tB(Î¶) is invertible. The
argument is very similar to the one for g(z) in the additive case. In the large N limit, tB(Î¶)
converges to
tB(Î¶) =
 Î»+
Î»âˆ’
ÏB(x)x
Î¶ âˆ’x dx.
(14.50)
So tB(Î¶) is monotonically decreasing for Î¶ > Î»+ and is therefore invertible. We then have
Î»1 = Î¶(aâˆ’1)
when
Î»1 > Î»+,
(14.51)
where we use the notation Î¶(t) for the inverse of the T-transform of B, in the region where
it is invertible.
The inverse function Î¶(t) can be expressed in terms of the S-transform via Eq. (11.92).
We get
Î»1 = Î¶(aâˆ’1) =
a + 1
SB(aâˆ’1)
when
a >
1
tB(Î»+).
(14.52)
Applying the theory to a Wishart matrix B = W with
SW(x) =
1
1 + qx,
Î»Â± = (1 Â± âˆšq)2,
(14.53)
one ï¬nds that an outlier appears to the right of Î»+ for a > âˆšq, with
Î»1 = (a + 1)

1 + q
a

.
(14.54)
For large a, we have Î»1 â‰ˆa + 1 + q, i.e. a large eigenvalue a + 1 in the covariance matrix
C will appear shifted by q in the eigenvalues of the sample covariance matrix.
Nothing prevents us from considering negative values of a, such that a > âˆ’1 to preserve
the positive deï¬nite nature of C. In this case, an outlier appears to the left of Î»âˆ’when
a < âˆ’âˆšq. Its position is given by the very same equation (14.54) as above.
Exercise 14.4.1
Transpose version of multiplicative perturbation
Consider a positive deï¬nite rotationally invariant random matrix B and a
normalized vector u. In this exercise, we will show that the matrix F deï¬ned by
F = (1 + cuuT )B(1 + cuuT ),
(14.55)
with c > 0 sufï¬ciently large, has an outlier Î»1 given by Eq. (14.52) with b +1 =
(c + 1)2.
(a)
Show that for two positive deï¬nite matrices A and B, B
1
2 AB
1
2 has the same
eigenvalues as A
1
2 BA
1
2 .

234
Edge Eigenvalues and Outliers
(b)
Show that for a normalized vector u

1 + (a âˆ’1)uuT  1
2 = 1 + ( âˆša âˆ’1)uuT .
(14.56)
(c)
Finish the proof of the above statement.
Exercise 14.4.2
Multiplicative perturbation of an inverse-Wishart matrix
We will see in Section 15.2.3 that the inverse-Wishart matrix
W
p is deï¬ned as
W
p = (1 âˆ’q)Wâˆ’1
q ,
(14.57)
where Wq is a Wishart matrix with parameter q and p the variance of the inverse-
Wishart is given by p =
q
1âˆ’q . The S-transform of
W
p is given by
S
W
p(t) = 1 âˆ’pt.
(14.58)
Consider the diagonal matrix D with D11 = d and all other diagonal entries
equal to 1.
(a)
D can be written as 1 + cuuT . What is the normalized vector u and the
constant c?
(b)
Using the result from Exercise 14.4.1, ï¬nd the value of the largest eigenvalue
of the matrix D
W
pD as a function of d. Note that your expression will only
be valid for sufï¬ciently large d.
(c)
Numerically generate matrices
W
p with N = 1000 and p = 1/2 (q = 1/3).
Find the largest eigenvalue of D
W
pD for various values of d and make a plot
of Î»1 vs d. Superimpose your analytical result.
(d)
(Harder) Find analytically the minimum value of d to have an outlier Î»1.
14.5 Phase Retrieval and Outliers
Optical detection devices like CCD cameras or photosensitive ï¬lms measure the photon
ï¬‚ux but are blind to the phase of the incoming light. More generally, it is often the case
that one can only measure the power spectral density of a signal, which is the magnitude
of its Fourier transform. Can one recover the full signal based on this partial information?
This problem is called phase retrieval and can be framed mathematically as follows. Let an
unknown vector x âˆˆRN be â€œprobedâ€ with T vectors ak, in the sense that the measurement
apparatus gives us yk = |aT
k x|2 with k = 1, . . . ,T .7 Vectors x and ak are taken to be real
but they can easily be made complex.
The phase retrieval problem is
Ë†x = argmin
x
â›
â
k

aT
k x
2 âˆ’yk

2
â
â .
(14.59)
7 We could consider that there is some additional noise in the measurement of yk but for simplicity we keep here with the
noiseless version.

14.5 Phase Retrieval and Outliers
235
It is a difï¬cult non-convex optimization problem with many local minima. To efï¬ciently
ï¬nd an acceptable solution, we need a starting point x0 that somehow points in the direc-
tion of the true solution x. The problem is that in large dimensions the probability that a
random vector x0 has an overlap |xT x0| > Îµ is exponentially small in N as soon as Îµ > 0.
We will explore here a technique that allows one to ï¬nd a vector x0 with non-vanishing
overlap with the true x.
The idea is to build some sort of weighted Wishart matrix such that this matrix will have
an outlier with non-zero overlap with the unknown true vector x. Consider the following
matrix:
M = 1
T
T

k=1
f (yk) akaT
k,
(14.60)
where the T vectors ak are of size N and f (y) is a function that we will choose later.
The function f (y) should be bounded above, otherwise we might have outliers domi-
nated by a few large values of f (yk). One such function that we will study is the sim-
ple threshold f (y) := (y âˆ’1). In large dimensions the results should not depend
on the precise statistics of the vectors ak provided they are sufï¬ciently random. Here
we will assume that all their components are standard iid Gaussian. This assumption
makes the problem invariant by rotation. Without loss of generality, we can assume the
true vector x is in the canonical direction e1. The weights f (yk) are therefore assumed
to be correlated to |[ak]1|2 = |aT
k e1|2 and independent of all other components of the
vectors ak.
Given that the ï¬rst row and column of M contain the element [ak]1, we write the matrix
in block form as in Section 1.2.5:
M =

M11
M12
M21
M22

,
(14.61)
with the (11) block of size 1 Ã— 1 and the (22) block of size (N âˆ’1) Ã— (N âˆ’1). To ï¬nd a
potential outlier, we look for the zeros of the Stieltjes transform gM(z) = Ï„((z1âˆ’M)âˆ’1).
Combining Eqs. (1.32) and (1.33),
NgM(z) = Tr G22(z) + 1 + Tr

G22(z)M21M12G22(z)

z âˆ’M11 âˆ’M12G22(z)M21
,
(14.62)
where G22(z) is the matrix resolvent of the rotationally invariant matrix M22 (i.e. the
matrix M without its ï¬rst row and column). In the large N limit we expect M22 to
have a continuous spectrum with an edge Î»+. When the condition for the denominator
to vanish, i.e.
Î»1 âˆ’M11 âˆ’M12G22(Î»1)M21 = 0,
(14.63)
has a solution for Î»1 > Î»+ we can say that the matrix M has an outlier. The overlap
between the corresponding eigenvector v1 and x is given by the residue
Ï± :=
vT
1x
2
|x|2
=
vT
1e1
2 = lim
zâ†’Î»1
z âˆ’Î»1
z âˆ’M11 âˆ’M12G22(z)M21
.
(14.64)
In the large N limit the scalar equation (14.63) becomes self-averaging. We have
M11 = 1
T
T

k=1
f (yk)([ak]1)2 T â†’âˆ
â†’
E
&
f (y)([a]1)2'
.
(14.65)

236
Edge Eigenvalues and Outliers
For the second term we have
M12G22(z)M21 =
T

k,â„“=1
1
T 2 f (yk)f (yâ„“)[ak]1[aâ„“]1
N

i,j>1
[ak]i[G22(z)]ij[aâ„“]j
T â†’âˆ
â†’
qE
&
f 2(y)([a]1)2'
h(z),
(14.66)
where q = N/T and
h(z) = Ï„
HHT
T
G22(z)

,
[H]ik = [ak]i
i > 1.
(14.67)
We can now put everything together and use lâ€™Hospitalâ€™s rule to compute the residue. For
convenience we deï¬ne the constants cn := E
&
f n(y)([a]1)2'
. There will be an outlier
with overlap
Ï± =
1
1 âˆ’qc2hâ€²(Î»1)
(14.68)
when there is a solution Î»1 > Î»+ of
Î»1 = c1 + qc2h(Î»1).
(14.69)
We will come back later to the computation of h(z). In the q â†’0 limit the matrix M
becomes proportional to the identity M = E[f (y)]1 := m11, so gM(z) = 1/(zâˆ’m1) and
h(z) = 1/(z âˆ’m1). For q = 0 we have a solution Î»1 = c1 which satisï¬es c1 â‰¥m1. In
this limit the overlap tends to one. The linear correction in q is easily obtained as we only
need h(z) to order zero. We obtain
Î»1 = c1 + q
c2
c1 âˆ’m1
+ O(q2),
Ï± = 1 âˆ’q
c2
(c1 âˆ’m1)2 + O(q2).
(14.70)
Note that c2/(c1 âˆ’m1)2 is always positive so the overlap decreases with q starting from
Ï± = 1 at q = 0. For the unit thresholding function f (y) = (y âˆ’1) we have m1 =
erfc(1/
âˆš
2) â‰ˆ0.317 and c1 = c2 = m1 + âˆš2/(eÏ€) â‰ˆ0.801 (see Fig. 14.7).
Since we have the freedom to choose any bounded function f (y) we should choose
the one that gives the largest overlap for the value of q given by our dataset. We will do an
easier computation, namely minimize the slope of the linear approximation in q. We want
fopt(y) = argmin
f (y)
c2
(c1 âˆ’m1)2 = argmin
f (y)
Ea
&
f 2(a2)a2'
Ea

f (a2)(a2 âˆ’1)
2,
(14.71)
where the law of a is N(0,1). A variational minimization gives
fopt(y) = 1 âˆ’1
y .
(14.72)
The optimal function is not bounded below and therefore the distribution of eigenvalues is
singular with c2 â†’âˆand m1 â†’âˆ’âˆ. One should think of this function as the limit of a
series of functions such that c2/(c1 âˆ’m1)2 â†’0. In Figure 14.7 we see that numerically
this function has indeed an overlap as a function of q with zero slope at the origin. As a
consequence it has non-zero overlap for much greater values of q (fewer data T ) than the
simple thresholding function. It turns out that our small q optimum f (y) = 1 âˆ’1/y is
actually the optimal function for all values of q.

14.5 Phase Retrieval and Outliers
237
0.0
0.5
1.0
1.5
2.0
q
0.0
0.2
0.4
0.6
0.8
1.0
f(y) = 1âˆ’1/ y
f(y) = Q(yâˆ’1)
linear approx.
Figure 14.7 Overlap Ï± :=
vT
1x
2 /|x|2 between the largest eigenvector and the true signal as a
function of q := N/T for two functions: the simple f (y) = (y âˆ’1) and the optimal f (y) =
1 âˆ’1/y. Each dot corresponds to a single matrix of aspect ratio q and NT = 106. The solid line
corresponds to the linear q approximation Eq. (14.70) in the thresholding case. For the optimal case,
the slope at the origin is zero.
For completeness we show here how to compute the function h(z). We have the fol-
lowing subordination relation (for the (22) block of the relevant matrices):
tM(Î¶) = tWq

Sf (qtM(Î¶))Î¶

,
(14.73)
where Sf (t) is the S-transform of a diagonal matrix with entries f (yk). We then have
h(z) = Ï„(WqG22(z)) = SÏ„
&
(Sz1 âˆ’M)âˆ’1 (M âˆ’Sz + Sz)
'
= S(zgM(z) âˆ’1),
(14.74)
with S := Sf (q(zgM(z) âˆ’1)). Since
SM(t) = Sf (qt)
1 + qt = t + 1
tÎ¶ ,
(14.75)
we have
h(z) = gM(z)[1 + q(zgM(z) âˆ’1)],
(14.76)
where the function gM(z) can be obtained by inverting the relation
z(g) =
 âˆ
âˆ’âˆ
dx
âˆš
2Ï€
f (x2)eâˆ’x2/2
1 âˆ’qgf (x2) + 1
g .
(14.77)
Bibliographical Notes
â€¢ For a recent review on the subject of free low-rank additive or multiplicative perturba-
tions, the interested reader should consult

238
Edge Eigenvalues and Outliers
â€“ M. Capitaine and C. Donati-Martin. Spectrum of deformed random matrices and free
probability. preprint arXiv:1607.05560, 2016.
â€¢ Equation (14.16) was ï¬rst published by both physicists and mathematicians, respectively,
in
â€“ S. F. Edwards and R. C. Jones. The eigenvalue spectrum of a large symmetric random
matrix. Journal of Physics A: Mathematical and General, 9(10):1595, 1976,
â€“ Z. FÂ¨uredi and J. KomlÂ´os. The eigenvalues of random symmetric matrices. Combina-
torica, 1(3):233â€“241, 1981.
â€¢ In 2005, Baik, Ben Arous and PÂ´echÂ´e studied the transition in the statistics of the largest
eigenvector in the presence of a low-rank perturbation. The transition from no outlier to
one outlier is now called the bbp (Baikâ€“Ben Arousâ€“PÂ´echÂ´e) transition; see
â€“ J. Baik, G. Ben Arous, and S. PÂ´echÂ´e. Phase transition of the largest eigenvalue for
nonnull complex sample covariance matrices. Annals of Probability, 33(5):1643â€“
1697, 2005,
â€“ D. FÂ´eral and S. PÂ´echÂ´e. The largest eigenvalue of rank one deformation of large
Wigner matrices. Communications in Mathematical Physics, 272(1):185â€“228, 2007.
â€¢ The case of outliers with a hard edge is treated in
â€“ S. PÂ´echÂ´e. Universality of local eigenvalue statistics for random sample covariance
matrices. PhD thesis, EPFL, 2003.
â€¢ The generalization to spiked tensors has recently been studied in
â€“ G. Ben Arous, S. Mei, A. Montanari, and M. Nica. The landscape of the spiked ten-
sor model. Communications on Pure and Applied Mathematics, 72(11):2282â€“2330,
2019.
â€¢ The case of additive or multiplicative perturbation to a general matrix (beyond Wigner
and Wishart) was worked out by
â€“ F. Benaych-Georges and R. R. Nadakuditi. The eigenvalues and eigenvectors of
ï¬nite, low rank perturbations of large random matrices. Advances in Mathematics,
227(1):494â€“521, 2011.
â€¢ For studies about the edge properties of Wigner matrices, see e.g.
â€“ M. J. Bowick and E. Brzin. Universal scaling of the tail of the density of eigenvalues
in random matrix models. Physics Letters B, 268(1):21â€“28, 1991,
and for the speciï¬c problem of the largest eigenvalue, the seminal paper of
â€“ C. A. Tracy and H. Widom. Level-spacing distributions and the Airy kernel. Com-
munications in Mathematical Physics, 159(1):151â€“174, 1994,
which has led to a ï¬‚urry of activity, see e.g.
â€“ S. Majumdar. Random matrices, the Ulam problem, directed polymers and growth
models, and sequence matching. In Les Houches Lecture Notes â€œComplex Systemsâ€,
volume 85. Elsevier Science, 2007.
â€¢ When the elements of the random matrix are fat tailed, the Tracyâ€“Widom law is modi-
ï¬ed, see
â€“ G. Biroli, J.-P. Bouchaud, and M. Potters. On the top eigenvalue of heavy-tailed
random matrices. Europhysics Letters (EPL), 78(1):10001, 2007,

14.5 Phase Retrieval and Outliers
239
â€“ A. Aufï¬nger, G. Ben Arous, and S. PÂ´echÂ´e. Poisson convergence for the largest eigen-
values of heavy tailed random matrices. Annales de lâ€™I.H.P. ProbabilitÂ´es et statis-
tiques, 45(3):589â€“610, 2009.
â€¢ For studies of inï¬nite variance LÂ´evy matrices, see
â€“ P. Cizeau and J. P. Bouchaud. Theory of LÂ´evy matrices. Physical Review E,
50:1810â€“1822, 1994,
â€“ Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, and I. Zahed. Free random LÂ´evy and
Wigner-LÂ´evy matrices. Physical Review E, 75:051126, 2007,
â€“ G. Ben Arous and A. Guionnet. The spectrum of heavy tailed random matrices.
Communications in Mathematical Physics, 278(3):715â€“751, 2008,
â€“ S. Belinschi, A. Dembo, and A. Guionnet. Spectral measure of heavy tailed
band and covariance random matrices. Communications in Mathematical Physics,
289(3):1023â€“1055, 2009.
â€¢ On the outlier method for the phase recovery problem, the reader is referred to
â€“ Y. M. Lu and G. Li. Phase transitions of spectral initialization for high-dimensional
nonconvex estimation. preprint arXiv:1702.06435, 2017,
â€“ W. Luo, W. Alghamdi, and Y. M. Lu. Optimal spectral initialization for signal recov-
ery with applications to phase retrieval. IEEE Transactions on Signal Processing,
67(9):2347â€“2356, 2019,
and for a review, see
â€“ A. Stern, editor. Optical Compressive Imaging. CRC Press, Boca Raton, Fla., 2017.


Part III
Applications


15
Addition and Multiplication: Recipes and Examples
In the second part of this book, we have built the necessary tools to compute the spectrum
of sums and products of free random matrices. In this chapter we will review the results
previously obtained and show how they work on concrete, simple examples. More sophisti-
cated examples, and some applications to real world data, will be developed in subsequent
chapters.
15.1 Summary
We introduced the concept of freeness which can be summarized by the following intuitive
statement: two large matrices are free if their eigenbases are related by a random rotation.
In particular a large matrix drawn from a rotationally invariant ensemble is free with respect
to any matrix independent of it, for example a deterministic matrix.1 For example A and
OBOT are free when O is a random rotation matrix (in the large dimension limit). When A
and B are free, their R- and S-transforms are, respectively, additive and multiplicative:
RA+B(x) = RA(x) + RB(x),
SAB(t) = SA(t)SB(t).
(15.1)
The free product needs some clariï¬cation as AB is in general not a symmetric matrix,
the S-transform SAB(t) in fact relates to the eigenvalues of the matrix
âˆš
AB
âˆš
A, which are
the same as those of
âˆš
BA
âˆš
B when both A and B are positive semi-deï¬nite (otherwise the
square-root is ill deï¬ned).
15.1.1 R- and S-Transforms
The R- and S-transforms are deï¬ned by the following relations:
gA(z) = Ï„
&
(z âˆ’A)âˆ’1'
,
(15.2)
tA(Î¶) = Ï„
&
(1 âˆ’Î¶ âˆ’1A)âˆ’1'
âˆ’1 = Î¶gA(Î¶) âˆ’1;
(15.3)
RA(x) = zA(x) âˆ’1
x,
SA(t) = t + 1
tÎ¶A(t) if Ï„(A)  0,
(15.4)
1 By large, we mean that all normalized moments computed using freeness are correct up to corrections that are O(1/N).
243

244
Addition and Multiplication: Recipes and Examples
where zA(x) and Î¶A(t) are the inverse functions of gA(z) and tA(Î¶), respectively.
Under multiplication by a scalar they behave as
RÎ±A(x) = Î±RA(Î±x),
SÎ±A(t) = Î±âˆ’1SA(t).
(15.5)
While the R-transform behaves simply under a shift by a scalar,
RA+Î±1(x) = Î± + RA(x),
(15.6)
there is no simple formula for computing the S-transform of a shifted matrix. On the other
hand, the S-transform is simple under matrix inversion:
SAâˆ’1(x) =
1
SA(âˆ’x âˆ’1).
(15.7)
The two transforms are related by the following equivalent identities:
SA(t) =
1
RA(tSA(t)),
RA(x) =
1
SA(xRA(x)).
(15.8)
The identity matrix has particularly simple transforms:
g1(z) =
1
z âˆ’1
t1(Î¶) =
1
Î¶ âˆ’1;
(15.9)
R1(x) = 1,
S1(t) = 1.
(15.10)
The R- and S-transforms have the following Taylor expansion for small arguments:
RA(x) = Îº1 + Îº2x + Îº3x2 + Â· Â· Â· ,
SA(x) = 1
Îº1
âˆ’Îº2
Îº3
1
x + 2Îº2
2 âˆ’Îº1Îº3
Îº5
1
x2 + Â· Â· Â· ,
(15.11)
where Îºn are the free cumulants of A:
Îº1 = Ï„(A),
Îº2 = Ï„(A2) âˆ’Ï„ 2(A),
Îº3 = Ï„(A3) âˆ’3Ï„(A)Ï„(A2) + 2Ï„ 3(A).
(15.12)
Combining Eqs. (15.7) and (15.11), we can obtain the inverse moments of A from its
S-transform. In particular,
Ï„(Aâˆ’1) = SA(âˆ’1),
Ï„(Aâˆ’2) = SA(âˆ’1)

SA(âˆ’1) âˆ’Sâ€²
A(âˆ’1)

.
(15.13)
15.1.2 Computing the Eigenvalue Density
The R-transform provides a systematic way to obtain the spectrum of the sum C of two
independent matrices A and B, where at least one of them is rotationally invariant. Here is
a simple recipe to compute the eigenvalue density of a free sum of matrices:
1 Find gB(z) and gA(z).
2 Invert gB(z) and gA(z) to get zB(g) and zA(g), and hence RB(x) and RA(x).

15.2 R- and S-Transforms and Moments of Useful Ensembles
245
3 RC(x) = RB(x) + RA(x), which gives zC(g) = RC(g) âˆ’gâˆ’1.
4 Solve zC(g) = z for gC(z).
5 Use Eq. (2.47) to ï¬nd the density:
ÏC(Î») = limÎ·â†’0+ Im gC(Î» âˆ’iÎ·)
Ï€
.
(15.14)
In the multiplicative case (C = A
1
2 BA
1
2 ), the recipe is similar:
1 Find tB(Î¶) and tA(Î¶).
2 Invert tB(Î¶) and tA(Î¶) to get Î¶B(t) and Î¶A(t), and hence SB(t) and SA(t).
3 SC(t) = SB(t)SA(t), which gives Î¶C(t)SC(t)t = t + 1.
4 Solve Î¶C(t) = Î¶ for tC(Î¶).
5 Equation (2.47) for gC(z) = (tC(z) + 1)/z is equivalent to
ÏC(Î») = limÎ·â†’0+ Im tC(Î» âˆ’iÎ·)
Ï€Î»
.
(15.15)
In some cases, the equation in step 4 is exactly solvable. But it is usually a high order
polynomial equation, or worse, a transcendental equation. In these cases numerical solution
is still possible. There always exists at least one solution that satisï¬es
g(z) = zâˆ’1 + O(zâˆ’2)
(15.16)
for z â†’âˆ. Since the eigenvalues of B and A are real, their R- and S-transforms are real
for real arguments. Hence the equation in step 4 is an equation with real coefï¬cients. In
order to ï¬nd a non-zero eigenvalue density we need to ï¬nd solutions with a strictly positive
imaginary part when the parameter Î· goes to zero. When the equation is quadratic or cubic,
complex solutions come in complex conjugated pairs: therefore, at most one solution will
have a strictly positive imaginary part. As a numerical trick, Ï€Ï(Î») can be equated with the
maximum of the imaginary part of all two or three solutions (the density will be zero when
all solutions are real). For higher order polynomial and transcendental equations, we have
to be more careful as there can be spurious complex solutions with positive imaginary part.
Exercise 15.2.1 shows how to do these computations in concrete cases.
15.2 R- and S-Transforms and Moments of Useful Ensembles
15.2.1 Wigner Ensemble
The Wigner ensemble is rotationally invariant, therefore a Wigner matrix is free from any
matrix from which it is independent. For a Wigner matrix X of variance Ïƒ 2, the R-transform
reads
RX(x) = Ïƒ 2x.
(15.17)
The Wigner matrix is stable under free addition, i.e. the free sum of two Wigner matrices
of variance Ïƒ 2
1 and Ïƒ 2
2 is a Wigner with variance Ïƒ 2
1 + Ïƒ 2
2 .

246
Addition and Multiplication: Recipes and Examples
The Wigner matrix is traceless (Ï„(X) = 0), so its S-transform is ill-deï¬ned. However, we
can shift the mean of the entries of X by a certain parameter m. We then have RX+m(x) =
m + Ïƒ 2x. We can use Eq. (11.97) and compute the S-transform:
SX+m(t) =
âˆš
m2 + 4Ïƒ 2t âˆ’m
2Ïƒ 2t
=
m
2Ïƒ 2t
â›
â
2
1 + 4Ïƒ 2t
m2 âˆ’1
â
â .
(15.18)
It is regular at t = 0 whenever m > 0 and tends to (Ïƒ âˆšt)âˆ’1 when m â†’0.
Finally, let us recall the formula for the positive moments of Wigner matrices:
Ï„(X2k) =
(2k)!
(k + 1)k!2 Ïƒ 2k;
Ï„(X2k+1) = 0.
(15.19)
The negative moments of X are all inï¬nite, because the density of zero eigenvalues is
positive.
15.2.2 Wishart Ensemble
For a white Wishart matrix Wq with parameter q = N/T , one has (see Section 10.1)
RWq(x) =
1
1 âˆ’qx .
(15.20)
To compute its S-transform we ï¬rst remember that its Stieltjes transform g(z) satisï¬es Eq.
(4.37), which can be written as an equation for t(Î¶) or its inverse Î¶(t):
Î¶t âˆ’(1 + qt)(t + 1) = 0
â‡’
SWq(t) =
1
1 + qt .
(15.21)
The ï¬rst few moments of a white Wishart matrix are given by
Ï„(Wq) = 1,
Ï„(W2
q) = 1 + q;
Ï„(Wâˆ’1
q ) =
1
1 âˆ’q,
Ï„(Wâˆ’2
q ) =
1
(1 âˆ’q)3 . (15.22)
15.2.3 Inverse-Wishart Ensemble
We take the opportunity of this summary of R- and S-transforms to introduce a very
useful ensemble of matrices, namely the inverse-Wishart ensemble. We will call an inverse-
Wishart matrix2 the inverse of a white Wishart matrix, which, we recall, has unit normalized
trace.
For a Wishart matrix to be invertible we need to have q < 1. Let Wq be such a matrix.
Using Eq. (11.116) we can show that
SWâˆ’1
q (t) = 1 âˆ’q âˆ’qt.
(15.23)
2 More generally the inverse of a Wishart matrix with any covariance C can be called an inverse-Wishart but we will only
consider white inverse-Wishart matrices.

15.2 R- and S-Transforms and Moments of Useful Ensembles
247
Since Ï„(Wâˆ’1
q ) = 1/(1 âˆ’q), we deï¬ne the (normalized) inverse-Wishart as
W
p =
(1 âˆ’q)Wâˆ’1
q
and call p := q/(1 âˆ’q). Rescaling and changing variable we obtain
S
W
p(t) = 1 âˆ’pt.
(15.24)
By construction the inverse-Wishart has mean 1 and variance p. Using Eq. (15.11), we ï¬nd
that it has Îº3(
W
p) = 2p2, which is higher than the skewness of a white Wishart matrix
with the same variance (Îº3(Wq) = q2).
From the S-transform we can ï¬nd the R-transform using Eq. (15.8):
R
W
p(x) = 1 âˆ’âˆš1 âˆ’4px
2px
.
(15.25)
To ï¬nd the Stieltjes transform and the density, it is easier to compute the T-transform from
Eq. (11.63) and convert the result into a Stieltjes transform:
g
W
p(z) = (1 + 2p)z âˆ’1 âˆ’
Â±
âƒ*
(z âˆ’1)2 âˆ’4pz
2pz2
.
(15.26)
We can use Eq. (2.47) to ï¬nd the density of eigenvalues or do the following change of
variable in the white Wishart density (Eq. (4.43)):
x = 1 âˆ’q
Î»
and
p =
q
1 âˆ’q .
(15.27)
Both methods give (see Fig. 15.1)
Ï
W
p(x) =
*
(x+ âˆ’x)(x âˆ’xâˆ’)
2Ï€px2
,
xâˆ’< x < x+,
(15.28)
with the edges of the spectrum given by
xÂ± = 2p + 1 Â± 2
*
2(p + 1).
(15.29)
From the Stieltjes transform we can obtain
Ï„(
W
âˆ’1
p ) = âˆ’lim
zâ†’0 g
W
p(z) = 1 + p.
(15.30)
Other low moments of the inverse-Wishart matrix read
Ï„(
W
p) = 1,
Ï„(
W
2
p) = 1 + p;
Ï„(
W
âˆ’1
p ) = 1 + p,
Ï„(
W
âˆ’2
p ) = (1 + p)(1 + 2p).
(15.31)
Finally, the large N inverse-Wishart matrix potential can be obtained from the real part
of the Stieltjes transform using (5.38)
V
W
p(x) = 1
px + 1 + 2p
p
log x.
(15.32)

248
Addition and Multiplication: Recipes and Examples
0
1
2
3
4
l
0.00
0.25
0.50
0.75
1.00
1.25
1.50
r(l)
inverse-Wishart p = 1/ 2
white Wishart q = 1/ 2
Figure 15.1 Density of eigenvalues for an inverse-Wishart distribution with p = 1/2. The white
Wishart distribution (q = 1/2) is shown for comparison. Both laws are normalized, have unit mean
and variance 1/2.
For completeness we give here the law of the elements of a general (not necessarily
white) inverse-Wishart matrix at ï¬nite N. We recall the law of a general Wishart matrix
Eq. (4.16):
P (E) = (T/2)NT/2

N(T/2)
(det E)(T âˆ’Nâˆ’1)/2
(det C)T/2
exp
(
âˆ’T
2 Tr

ECâˆ’1)
,
(15.33)
where E is an N Ã— N Wishart matrix measured over T time steps with true correlations
C and normalized such that E[E] = C. We deï¬ne the inverse-Wishart as
W
= Eâˆ’1. Note
that a ï¬nite N Wishart matrix has
E
&
Eâˆ’1'
=
T
T âˆ’N âˆ’1Câˆ’1 =: %,
(15.34)
where we have deï¬ned the matrix % such that E[
W
] = %. To do the change of variable
E â†’
W
in the joint probability density, we need to multiply by the Jacobian (det
W
)âˆ’Nâˆ’1
(see Eq. (1.41)). Putting everything together we obtain
P(
W
) = (T âˆ’N âˆ’1)NT/2
2NT/2
N(T/2)
(det %)T/2
(det
W
)(T +N+1)/2 exp
(
âˆ’T âˆ’N âˆ’1
2
Tr

W
âˆ’1%
)
.
(15.35)
In the scalar case N = 1, the inverse-Wishart distribution reduces to an inverse-gamma
distribution:
P(m) =
ba

(a)mâˆ’aâˆ’1eâˆ’b/m,
(15.36)
with b = (T âˆ’2)%/2 and a = T/2.

15.3 Worked-Out Examples: Addition
249
Exercise 15.2.1
Free product of two Wishart matrices
In this exercise, we will compute the eigenvalue distribution of a matrix E =
(Wq0)
1
2 Wq(Wq0)
1
2 ; as we will see in Section 17.1, this matrix would be the
sample covariance matrix of data with true covariance given by a Wishart with
q0 observed over T samples such that q = N/T .
(a)
Using Eq. (15.21) and the multiplicativity of the S-transform, write the S-
transform of E.
(b)
Using the deï¬nition of the S-transform write an equation for tE(z). It is a
cubic equation in t. If either q0 or q goes to zero, it reduces to the standard
MarË‡cenkoâ€“Pastur quadratic equation.
(c)
Use Eq. (15.15) and a numerical root ï¬nder to plot the eigenvalue density of
E for q0 = 1/2 and q âˆˆ{1/4,1/2,3/4}. In practice you can work with Î· = 0;
of the three roots of your cubic equation, at most one will have a positive
imaginary part. When all three solutions are real ÏE(Î») = 0.
(d)
Generate numerically two independent Wishart matrices with q = 1/2
(N = 1000 and T = 2000) and compute E = (Wq0)
1
2 Wq(Wq0)
1
2 . Note that
the square-root of a matrix is obtained by applying the square-roots to
its eigenvalues. Diagonalize your E and compare its density with your
result in (c).
15.3 Worked-Out Examples: Addition
15.3.1 The Arcsine Law
Consider the free sum of two symmetric orthogonal matrices, i.e. matrices with eigen-
values Â±1 with equal weights. Let M1 and M2 be two such matrices, their Stieltjes and
R-transforms are given by
g(z) =
z
z2 âˆ’1
and
R(g) =
*
1 + 4g2 âˆ’1
2g
,
(15.37)
from which we can deduce that M = 1
2(M1 + M2) has an R-transform given by
RM(g) =
*
1 + g2 âˆ’1
g
,
(15.38)
where we have used the scaling RÎ±A(x) = Î±RA(Î±x) with Î± = 1/2.
The corresponding Stieltjes transform reads
gM(z) =
1
z
*
1 âˆ’1/z2 .
(15.39)

250
Addition and Multiplication: Recipes and Examples
From this expression we deduce that the density of eigenvalues is given by the centered
arcsine law:
ÏM(Î») = 1
Ï€
1
âˆš
1 âˆ’Î»2,
Î» âˆˆ(âˆ’1,1),
(15.40)
and zero elsewhere. This corresponds to a special case of the Jacobi ensemble that we have
encountered in Section 7.1.3.
15.3.2 Sum of Uniform Densities
Suppose now we want to compute the eigenvalue distribution of a matrix M = U+OUOT ,
where U is a diagonal matrix with entries uniformly distributed between âˆ’1 and 1 (e.g.
[U]kk = 1 + (1 âˆ’2k)/N) and O a random orthogonal matrix. This is the free sum of two
matrices with uniform eigenvalue density.
First we need to compute the Stieltjes transform of U. We have
ÏU(Î») = 1
2,
Î» âˆˆ(âˆ’1,1).
(15.41)
The corresponding Stieltjes transform is3
gU(z) = 1
2
 1
âˆ’1
dÎ»
z âˆ’Î» = 1
2 log
z + 1
z âˆ’1

.
(15.42)
Note that when âˆ’1 < Î» < 1 the argument of the log in gU(z) is negative so Im g(Î» âˆ’
iÎ·) = iÏ€/2, consistent with a uniform distribution of eigenvalues. We then compute the
R-transform by ï¬nding the inverse of gU(z):
z(g) = e2g + 1
e2g âˆ’1 = coth(g).
(15.43)
And so the R-transform of U is given by
RU(g) = coth(g) âˆ’1
g .
(15.44)
The R-transform of M is twice that of U. To ï¬nd the Stieltjes transform of U we thus need
to solve
z = RM(g) + 1
g = 2 coth(g) âˆ’1
g,
(15.45)
for g(z). This is a transcendental equation and we need to solve it for complex z near the
real axis. Before attempting to do this, it is useful to plot z(g) (Fig. 15.2). The region where
z = z(g) does not have real solutions is where the eigenvalues are. This region is between
a local maximum and a local minimum of z(g). We should look for complex solutions
of Eq. (15.45) near the real axis for Re(z) between âˆ’1.54 and 1.54. We can then put this
equation into a complex non-linear solver. The density will be given by Im g(z)/Ï€ for Im(z)
3 A more general uniform density between [m âˆ’a,m + a] has mean m, variance a2/3 and
gU(z) = log((z âˆ’m + a)/(z âˆ’m âˆ’a))/(2a).

15.3 Worked-Out Examples: Addition
251
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
g
âˆ’10
âˆ’5
0
5
10
= R(g)+ 1/ g
1.25 1.50 1.75
1.55
1.56
Figure 15.2 The function z(g) = RM(g) + 1/g for the free sum of two ï¬‚at distributions. Note that
there is a region of z near [âˆ’1.5,1.5] when z = z(g) does not have real solutions. This is where the
eigenvalues lie. The inset shows a zoom of the region near z = 1.5, indicating more clearly that z(g)
has a minimum at g+ â‰ˆ1.49, so Î»+ = z(g+). The exact edges of the spectrum are Î»Â± â‰ˆÂ±1.5429.
âˆ’2
âˆ’1
0
1
2
l
0.0
0.1
0.2
0.3
r(l)
Figure 15.3 Density of eigenvalues for the free sum of two uniform distributions. Continuous curve
was computed using a numerical solution of Eq. (15.45). The histogram is a numerical simulation
with N = 5000.
very small and Re(z) in the desired range. Note that complex solutions come in conjugated
pairs, and it is hard to force the solver to ï¬nd the correct one. This is not a problem; since
their imaginary parts have the same absolute value, we can just use
Ï(Î») = | Im g(Î» âˆ’iÎ·)|
Ï€
for some small Î·.
(15.46)
We have plotted the resulting density in Figure 15.3.

252
Addition and Multiplication: Recipes and Examples
15.4 Worked-Out Examples: Multiplication
15.4.1 Free Product of a Wishart and an Inverse-Wishart
Consider the free product of a Wishart Wq of parameter q and an independent inverse-
Wishart
W
p of parameter p, i.e. E = *
W
pWq
*
W
p. We already have the building
blocks:
S
W
p(t) = 1 âˆ’pt;
SWq(t) =
1
1 + qt
â‡’
SE(t) = 1 âˆ’pt
1 + qt ,
(15.47)
leading to
Î¶E(t) = (t + 1)(1 + qt)
t(1 âˆ’pt)
.
(15.48)
Inverting this relation to obtain tE(Î¶) leads to a quadratic equation for t:
(t + 1)(1 + qt) = Î¶t(1 âˆ’pt),
(15.49)
which can be explicitly solved as
tE(z) = z âˆ’q âˆ’1 âˆ’
Â±
âƒ*
(q + 1 âˆ’z)2 âˆ’4(q + zp)
2(q + zp)
.
(15.50)
Using Eq. (15.15) ï¬nally yields
ÏE(Î») =
*
4(pÎ» + q) âˆ’(1 âˆ’q âˆ’Î»)2
2Ï€Î»(pÎ» + q)
.
(15.51)
The edges of the support are given by
Î»Â± =
&
1 + q + 2p Â± 2
*
(1 + p)(q + p)
'
.
(15.52)
One can check that the limit p â†’0 recovers the trivial case
W
0 = 1 for which the
MarË‡cenkoâ€“Pastur edges indeed read
Î»Â± = (1 + q) Â± 2 âˆšq.
(15.53)
Exercise 15.4.1
Free product of Wishart and inverse-Wishart
(a)
Generate numerically a normalized inverse-Wishart
W
p for p = 1/4 and
N = 1000. Check that Ï„(
W
p) = 1 and Ï„(
W
2
p) = 1.25. Plot a normalized
histogram of the eigenvalues of
W
p and compare with Eq. (15.28).
(b)
Generate an independent Wishart Wq with q = 1/4 and compute E =
*
W
pWq
*
W
p. To compute *
W
p, diagonalize
W
p, take the square-root of
its eigenvalues and reconstruct *
W
q. Check that Ï„(E) = 1 and Ï„(E2) = 1.5.
Plot a normalized histogram of the eigenvalues of E and compare with
Eq. (15.51).

15.4 Worked-Out Examples: Multiplication
253
(c)
For every eigenvector vk of E compute Î¾k := vT
k
W
pvk, and make a scatter
plot of Î¾k vs Î»k, the eigenvalue of vk. Your scatter plot should show a noisy
straight line. We will see in Chapter 19, Eq. (19.49), that this is related to the
fact that linear shrinkage is the optimal estimator of the true covariance from
the sample covariance when the true covariance is an inverse-Wishart.
15.4.2 Free Product of Projectors
As our last simple example, consider a space of large dimension N, and a projector P1
on a subspace of dimension N1 â‰¤N, i.e. a diagonal matrix with N1 diagonal elements
equal to unity, and N âˆ’N1 elements equal to zero. We now introduce a second projector
P2 on a subspace of dimension N2 â‰¤N, and would like to study the eigenvalues of the
free product of these two projectors, P = P1P2. Clearly, all eigenvalues of P must lie in the
interval (0,1).
As now usual, we ï¬rst need to compute the T-transform of P1 and P2. We deï¬ne the
ratios qa = Na/N, a = 1,2. Since Pa has Na eigenvalues equal to unity and N âˆ’Na
eigenvalues equal to zero, one ï¬nds
gPa(z) = 1
N
( Na
z âˆ’1 + N âˆ’Na
z
)
= qa âˆ’1 + z
z(z âˆ’1)
â‡’
tPa(Î¶) =
qa
Î¶ âˆ’1.
(15.54)
Therefore, the inverse of the T-transforms just read Î¶Pa(t) = 1 + qa/t, and
SPa(t) = t + 1
t + qa
â‡’
SP =
(t + 1)2
(t + q1)(t + q2).
(15.55)
Now, going backwards,
Î¶P(t) = (t + q1)(t + q2)
t(t + 1)
,
(15.56)
again leading to a quadratic equation for tP(Î¶):
(Î¶ âˆ’1)t2 + (Î¶ âˆ’q1 âˆ’q2)t âˆ’q1q2 = 0,
(15.57)
whose solution is
t(Î¶) = (q1 + q2 âˆ’Î¶) +
Â±
âƒ*
Î¶ 2 âˆ’2Î¶(q1 + q2 âˆ’2q1q2) + (q1 âˆ’q2)2
2(Î¶ âˆ’1)
,
(15.58)
where the notation
Â±
âƒâˆšÂ· deï¬ned by Eq. (4.56) ensures that we pick the correct root. Note
that the argument under the square-root has zeros for
Î»Â± = q1 + q2 âˆ’2q1q2 Â± 2
*
q1q2(1 âˆ’q1)(1 âˆ’q2).
(15.59)
One can check that Î»âˆ’â‰¥0, the zero bound being reached for q1 = q2. Note also that
Î»+Î»âˆ’= (q1 âˆ’q2)2.

254
Addition and Multiplication: Recipes and Examples
The Stieltjes transform of P can thus be written as
gP(z) = 1
z + (q1 + q2 âˆ’z) +
Â±
âƒ*
z2 âˆ’2z(q1 + q2 âˆ’2q1q2) + (q1 âˆ’q2)2
2z(z âˆ’1)
.
(15.60)
This quantity has poles at z = 0 and z = 1, and an imaginary part when z âˆˆ(Î»+,Î»âˆ’). The
spectrum of P therefore has a continuous part, given by
ÏP(Î») =
*
(Î»+ âˆ’Î»)(Î» âˆ’Î»âˆ’)
2Ï€Î»(1 âˆ’Î»)
,
(15.61)
and two delta peaks, A0Î´(Î») and A1Î´(Î» âˆ’1). To ï¬nd the amplitude of the potential poles,
we need to compute the numerator of (15.60) at the values z = 0 and z = 1. Remember
that
Â±
âƒâˆšÂ· equals âˆ’âˆšÂ· on the real axis left of the left edge and âˆšÂ· right of the right edge.
The amplitude of the z = 0 pole of gP(z) is
A0 = 1 âˆ’(q1 + q2) âˆ’
*
(q1 âˆ’q2)2
2
= 1 âˆ’min(q1,q2),
(15.62)
while the amplitude of the z = 1 pole is
A1 = (q1 + q2 âˆ’1) +
*
1 âˆ’2(q1 + q2 âˆ’2q1q2) + (q1 âˆ’q2)2
2
= (q1 + q2 âˆ’1) +
*
(q1 + q2 âˆ’1)2
2
= max(q1 + q2 âˆ’1,0).
(15.63)
This makes a lot of sense geometrically: our product of two projectors can only have a unit
eigenvalue if the sum of the dimensions of space spanned by these two projectors exceeds
the total dimension N, i.e. when N1 + N2 > N. Otherwise, there cannot be (generically)
any eigenvalue beyond Î»+.
When q1 + q2 < 1, the density of non-zero eigenvalues (15.61) is the same (up to
a normalization) as the density of eigenvalues of a Jacobi matrix (7.20). If we match the
edges of the spectrum we ï¬nd the identiï¬cation c1 = qmax/qmin and c+ = 1/qmin. The ratio
of normalization 1/c+ = qmin implies that the product of projectors density has a missing
mass of 1âˆ’qmin, which is precisely the Dirac mass at zero. The special case q1 = q2 = 1/2
was discussed in the context of 2 Ã— 2 matrices in Exercise 12.5.2. In that case, half of the
eigenvalues are zero and the other half are distributed according to the arcsine law: the
arcsine law is the limit of a Jacobi matrix with c1 â†’1 and c+ â†’2.
There is an alternative, geometric interpretation of the above calculation that turns out
to be useful in many different contexts, see Section 17.4 for an extended discussion. The
eigenvectors of projector P1 form a set of N1 orthonormal vectors xÎ±, Î± = 1, . . . ,N1,
from which an N1 Ã—N matrix X of components (xÎ±)i can be formed. Similarly, we deï¬ne
an N2 Ã— N matrix Y of components (yÎ²)i, Î² = 1, . . . ,N2. Now, one can write P as
P = XT XYT Y.
(15.64)

15.4 Worked-Out Examples: Multiplication
255
The non-zero eigenvalues of P are the same as the non-zero eigenvalues of MT M (or those
of MMT ), where M is the N1 Ã— N2 matrix of overlaps:
MÎ±,Î² :=
N

i=1
(xÎ±)i(yÎ²)i.
(15.65)
The eigenvalues of P correspond to the square of the singular values s of M. The geo-
metrical interpretation of these singular values is as follows: the largest singular value
corresponds to the maximum overlap between any normalized linear combination of the
xÎ± on the one hand and of the yÎ² on the other hand. These two linear combinations deï¬ne
two one-dimensional subspaces of the spaces spanned by xÎ± and yÎ². Once these optimal
directions are removed, one can again ask the same question for the remaining N1 âˆ’1 and
N2 âˆ’1 dimensional subspaces, deï¬ning the second largest singular value of M, and so on.
15.4.3 The Jacobi Ensemble Revisited
We saw in the previous section that the free product of two random projectors has a
very simple S-transform and that its non-zero eigenvalues are given by those of a Jacobi
matrix. We suspect that the Jacobi ensemble has itself a simple S-transform. Rather than
computing its S-transform from its Stieltjes transform (7.18), let us just use the properties
of the S-transform to compute it directly from the deï¬nition of a Jacobi matrix.
Recall from Chapter 7, an N Ã— N Jacobi matrix J is deï¬ned as (1 + E)âˆ’1 where the
matrix E is the free product of the inverse of an unnormalized Wishart matrix W1 with
T1 = c1N and another unnormalized Wishart W2 with T2 = c2N.
The two Wishart matrices have S-transforms given by
SW1,2(t) = T1,2
1
1 + câˆ’1
1,2t
= N
1
c1,2 + t .
(15.66)
Using the relation for inverse matrices (15.7), we ï¬nd
SWâˆ’1
1 (t) = Nâˆ’1(c1 âˆ’t âˆ’1).
(15.67)
The S-transform of E is just the product
SE(t) = SWâˆ’1
1 (t)SW2(t) = c1 âˆ’t âˆ’1
c2 + t
.
(15.68)
The next step is to shift the matrix E by 1. As mentioned earlier, there is no easy rule
to compute the S-transform of a shifted matrix. So this will be the hardest part of the
computation.
The R-transform behaves simply under shift. The trick is to use one of Eqs. (15.8) to
write an equation for RE(x), shift E by 1 and use the other R-S relation to ï¬nd back an
equation for SE+1(t). First we write
(c2 + t)SE = c1 âˆ’t âˆ’1.
(15.69)
The second of Eqs. (15.8) can be interpreted as the replacements S â†’1/R and t â†’xR
and gives
(1 + x âˆ’c1 + RE)RE + c2 = 0.
(15.70)
Now RE(x) = RE+1(x) âˆ’1, so
(x âˆ’c1 + RE+1)(RE+1 âˆ’1) + c2 = 0.
(15.71)

256
Addition and Multiplication: Recipes and Examples
Following the ï¬rst of Eqs. (15.8), we make the replacements R â†’1/S and x â†’tS and
ï¬nd
1 âˆ’c1 + t + (c2 + c1 âˆ’t âˆ’1)SE+1 = 0
â‡’
SE+1(t) =
t + 1 âˆ’c1
t + 1 âˆ’c2 âˆ’c1
.
(15.72)
Finally, using the relation for inverse matrices, Eq. (15.7) gives
SJ(t) = t + c2 + c1
t + c1
.
(15.73)
We can verify that the T-transform of the Jacobi ensemble
tJ(Î¶) =
c1 + 1 âˆ’c+Î¶ +
Â±
âƒ/
c2
+Î¶ 2 âˆ’2(c1c+ + c+ âˆ’2c1)Î¶ + (c1 âˆ’1)2
2(Î¶ âˆ’1)
(15.74)
is compatible with our previous result on the Stieltjes transform, Eq. (7.18). We can use
the Taylor series of the S-transform (15.11) to ï¬nd the ï¬rst few cumulants:
Îº1 =
c1
c1 + c2
,
Îº2 =
c1c2
(c1 + c2)3,
Îº3 = (c1 âˆ’c2)c1c2
(c1 + c2)5
.
(15.75)
From the S-transform we can compute the R-transform using Eq. (15.8):
RJ(x) = x âˆ’c1 âˆ’c2 âˆ’
*
x2 + 2(c1 âˆ’c2)x + (c1 + c2)2
2x
.
(15.76)
Finally, we note that the arcsine law is a Jacobi matrix with c1 = c2 = 1 and has the
following transform:
S(t) = t + 2
t + 1,
R(x) = x âˆ’2 âˆ’
*
x2 + 4
2x
.
(15.77)
For the centered arcsine law we have Rs(t) = 2R(2x) + 1 and we recover Eq. (15.38).

16
Products of Many Random Matrices
In this chapter we consider an issue of importance in many different ï¬elds: that of prod-
ucts of many random matrices. This problem arises, for example, when one considers the
transmission of light in a succession of slabs of different optical indices, or the propagation
of an electron in a disordered wire, or the way displacements propagate in granular media.
It also appears in the context of chaotic systems when one wants to understand how a
small difference in initial conditions â€œpropagatesâ€ as the dynamics unfolds. In this context,
one usually linearizes the dynamics in the vicinity of the unperturbed trajectory. If one
takes stroboscopic snapshots of the system, the perturbation is obtained as the product
of matrices (corresponding to the linearized dynamics) applied on the initial perturbation
(see Chapter 1). If the phase space of the system is large enough, and the dynamics chaotic
enough, one may expect that approximating the problem as a product of large, free matrices
should be a good starting point.
16.1 Products of Many Free Matrices
The speciï¬c problem we will study is therefore the following: consider the symmetrized
product of K matrices, deï¬ned as
MK = AKAKâˆ’1 . . . A2A1AT
1AT
2 . . . AT
Kâˆ’1AT
K,
(16.1)
where all Ai are identically distributed and mutually free, i.e. randomly rotated with respect
to one another. We know now that in such a case the S-transforms simply multiply. Noting
as Si(z) the S-transform of AiAT
i , and SMK(z) the S-transform of MK, one has
SMK(z) =
K
,
i=1
Si(z) â‰¡S1(z)K.
(16.2)
Now, it is intuitive that all the eigenvalues of MK will behave for large K as Î¼K, where Î¼
is itself a random variable which we will characterize below. We take this as an assumption
and indeed show that the distribution of Î¼â€™s tends to a well-deï¬ned function Ïâˆ(Î¼) as
K â†’âˆ. Note here a crucial difference with the case of sums of random matrices. If we
assume that the eigenvalues of a sum of K free random matrices behave as K Ã— Î¼, one can
257

258
Products of Many Random Matrices
easily establish that the distribution of Î¼â€™s collapses for large K to Î´(Î¼âˆ’Ï„(A)), with, once
again, Ï„(.) = Tr(.)/N. For products of random matrices, on the other hand, the distribution
of Î¼ remains non-trivial, as we will ï¬nd below.
Let us compute SMK(z) in the large K limit using our ansatz that the eigenvalues of M
are indeed of the form Î¼K. We ï¬rst compute the function tK(z) equal to
tK(z) :=

Î¼K
z âˆ’Î¼K Ïâˆ(Î¼)dÎ¼ = âˆ’

1
1 âˆ’zÎ¼âˆ’K Ïâˆ(Î¼)dÎ¼.
(16.3)
Setting z := uK, we see that for K â†’âˆthere is no contribution to this integral from the
region Î¼ < u, whereas the region Î¼ > u simply yields
tK(z) â‰ˆâˆ’P>(z1/K);
P>(u) :=
 âˆ
u
Ïâˆ(Î¼)dÎ¼.
(16.4)
The next step to get the S-transform is to compute the functional inverse of tK(z). Within
the same approximation, this is given by
tâˆ’1
K (z) =
&
P (âˆ’1)
>
(âˆ’z)
'K
,
(16.5)
where P (âˆ’1)
>
is the functional inverse of the cumulative distribution function P>. Finally,
by deï¬nition,
SMK(z) :=
1 + z
ztâˆ’1
K (z)
= S1(z)K.
(16.6)
Hence one ï¬nds, in the large K limit where ((1 + z)/z)1/K â†’1,
P (âˆ’1)
>
(âˆ’z) =
1
S1(z) â‡’P>(Î¼) = âˆ’S(âˆ’1)
1
 1
Î¼

(16.7)
and ï¬nally Ïâˆ(Î¼) = âˆ’P â€²
>(Î¼). The ï¬nal result is therefore quite simple, and entirely
depends on the S-transform of AiAT
i .
A simple case is when AiAT
i is a large Wishart matrix, with parameter q â‰¤1. In this case
S1(z) = (1+qz)âˆ’1, from which one easily works out that Ïâˆ(Î¼) = 1/q for Î¼ âˆˆ(1âˆ’q,1)
and zero elsewhere (see Fig. 16.1 for an illustration).
In many cases of interest, the eigenvalue spectrum of AiAT
i has some symmetries,
coming from the underlying physical problem one is interested in. For example, when our
chaotic system is invariant under time reversal (like the dynamics of a Hamiltonian system),
each eigenvalue Î» must come with its inverse Î»âˆ’1. A simple example of a spectrum with
such a symmetry is the free log-normal, further discussed in the next section. It is deï¬ned
from its S-transform, given by
S0
LN(z) = eâˆ’a(z+ 1
2 ),
(16.8)

16.1 Products of Many Free Matrices
259
0.4
0.6
0.8
1.0
1.2
m = l 1/ K
0.0
0.5
1.0
1.5
2.0
2.5
r(m)
Figure 16.1 Sample density of Î¼ = Î»1/K for the free product of K = 40 white Wishart matrices
with q = 1/2 and N = 1000. The dark line corresponds to the asymptotic density (K â†’âˆ), which
is constant between 1 âˆ’q and 1 and zero elsewhere. The two dashed vertical lines give the exact
positions of the edges of the spectrum (Î¼âˆ’= 0.44 and Î¼+ = 1.10) for K = 40, as computed in
Exercise 16.1.1.
where the parameter a is related to the trace of the corresponding matrices equal to ea/2.
Multiplying K such matrices together leads to eigenvalues of the form Î¼K, with
P>(Î¼) = âˆ’

S0
LN
(âˆ’1)  1
Î¼

= 1
2 âˆ’log Î¼
a
,
(16.9)
corresponding to
Ïâˆ(Î¼) = âˆ’P â€²
>(Î¼) = 1
aÎ¼,
Î¼ âˆˆ(eâˆ’a/2,ea/2),
(16.10)
and zero elsewhere. One can explicitly check that Î¼âˆ’1 has the same probability distribution
function as Î¼.
One often describes the eigenvalues of large products of random matrices in terms of
the Lyapunov exponents , deï¬ned as the eigenvalues of
 = lim
Kâ†’âˆ
1
K log MK.
(16.11)
Therefore the Lyapunov exponents are simply related to the Î¼â€™s above as  â‰¡log Î¼. For
the free log-normal example, the distribution of  is found to be uniform between âˆ’a/2
and a/2.
Let us end this section with an important remark: we have up to now considered products
of K matrices with a ï¬xed spectrum, independent of K, which leads to a non-universal
distribution of Lyapunov exponents (i.e. a distribution that explicitly depends on the full
function S1(z)). Let us now instead assume that these matrices are of the form

260
Products of Many Random Matrices
AAT =

1 + a
2K

1 +
B
âˆš
K
,
(16.12)
where a is a parameter and B is traceless and characterized by its second cumulant b =
Ï„(B2). For large K, S1(z) can then be expanded as
S1(z) = 1 âˆ’a
2K âˆ’b
K z + o(Kâˆ’1).
(16.13)
Therefore, for large K, the product of such matrices converges to a matrix characterized by
SMK(z) =

1 âˆ’a
2K âˆ’b
K z
K
â†’eâˆ’a/2âˆ’bz,
(16.14)
which can be interpreted as a multiplicative clt for free matrices, since the detailed statis-
tics of B has disappeared. The choice b = a corresponds to the free log-normal with
inversion symmetry S0
LN (see next section).
Exercise 16.1.1
Edges of the spectrum for the free product of many white
Wishart matrices
In this exercise, we will compute the edges of the spectrum of eigenvalues of
a matrix M given by the free product of K large white Wishart matrices with
parameter q.
(a)
The S-transform of M is simply given by the S-transform of a white Wishart
raised to the power K. Using Eq. (11.92), write an equation for the inverse of
the T-transform, Î¶(t), of the matrix M. This is a polynomial equation of order
K + 1.
(b)
For odd N, plot Î¶(t) for various K and 0 < q < 1 and convince yourself that
there is always a region of Î¶ where Î¶(t) = Î¶ has no real solution. This region
is between a local maximum and a local minimum of Î¶(t). For even N, the
argument is more subtle, but the correct branch exists only between the same
two extrema.
(c)
Differentiate Î¶(t) with respect to t to ï¬nd an equation for the extrema of Î¶(t).
After simpliï¬cations and discarding the t = âˆ’1/q solution, this equation is
quadratic in tâˆ—with two solutions corresponding to the local minimum and
maximum. Find the two solutions tâˆ—
Â± and plug these back in your equation for
Î¶(t) to ï¬nd the edges of the spectrum Î»Â±.
(d)
Use your result for K = 40 and q = 0.5 to verify the edges of the spectrum
given in Figure 16.1.
(e)
Compute the large K limit of tâˆ—
Â±. You should ï¬nd tâˆ—
âˆ’â†’âˆ’1 and tâˆ—
+ â†’
(q(K âˆ’1))âˆ’1. Show that at large K we have Î»1/K
âˆ’
â†’1 âˆ’q and Î»1/K
+
â†’1.

16.2 The Free Log-Normal
261
16.2 The Free Log-Normal
There exists a free version of the log-normal. Its S-transform is given by
SLN(t) = eâˆ’a/2âˆ’bt.
(16.15)
As a two-parameter family, the free log-normal is stable in the sense that the free product of
two free log-normals with parameters a1,b1 and a2,b2 is a free log-normal with parameters
a = a1 +a2, b = b1 +b2. The ï¬rst three free cumulants can be computed from Eq. (16.15):
SLN(t) = eâˆ’a/2
(
1 âˆ’bt + 1
2b2t2
)
+ O(t3).
(16.16)
Comparing with Eq. (15.11), this leads to
Îº1 = ea/2,
Îº2 = bea,
Îº3 = 3b2
2 e2a.
(16.17)
In the special case b = a, the free log-normal S0
LN has the additional property that its
matrix inverse has exactly the same law. Indeed, we have shown in Section 11.4.4 that the
following general relation holds:
SMâˆ’1(t) =
1
SM(âˆ’t âˆ’1),
(16.18)
or, in the free log-normal case with a = b,
SMâˆ’1(t) = ea/2âˆ’b(1+t) = SM(t)
(16.19)
when b = a. This implies that the eigenvalue distribution is invariant under Î» â†’1/Î» and
therefore that M has unit determinant. Let us study in more detail the eigenvalue spectrum
for the symmetric case a = b. By looking for the real extrema of
Î¶(t) = t + 1
t
ea(t+1/2),
(16.20)
we can ï¬nd the points tÂ± where t(Î¶) ceases to be invertible, which in turn give the edges
of the spectrum Î»Â± = Î¶(tÂ±):
tÂ± =
Â±
/
1 + 4
a âˆ’1
2
(16.21)
or
Î»+ = 1
Î»âˆ’
=
( 1a
4 +
1
1 + a
4
)2
exp
â›
â
2
a + a2
4
â
â .
(16.22)
Note that Î»+ = Î»âˆ’= 1 when a = b = 0, corresponding to the identity matrix. The
eigenvalue distribution is symmetric in Î» â†’1/Î» so the density Ï(â„“) of â„“= log(Î») is even.
Figure 16.2 shows the density of â„“for a = 100.

262
Products of Many Random Matrices
âˆ’50
âˆ’25
0
25
50
â„“
0.000
0.002
0.004
0.006
0.008
0.010
0.012
r(â„“)
free log-normal
Wigner
Figure 16.2 Probability density of â„“= log(Î») for a symmetric free log-normal (16.15) with a = b =
100 compared with a Wigner semi-circle with the same endpoints. As expected, the distribution is
even in â„“. For a â‰²1 the density of â„“is indistinguishable to the eye from a Wigner semi-circle (not
shown), whereas for a â†’âˆthe distribution of â„“/a tends to a uniform distribution on [âˆ’1/2,1/2].
In the more general case a  b, the whole distribution of â„“= log(Î») is just shifted by
(a âˆ’b)/2, as expected from the scaling property of the S-transform upon multiplication by
a scalar.
16.3 A Multiplicative Dyson Brownian Motion
Let us now consider the problem of multiplying random matrices close to unity from a
slightly different angle. Consider the following iterative construction for N Ã— N matrices:
Mn+1 = M
1
2n
&
(1 + aÎµ
2 )1 + âˆšÎµBn
'
M
1
2n,
(16.23)
where Bn is a sequence of identical, free, traceless N Ã— N matrices and Îµ â‰ª1. Using
second order perturbation theory, one can deduce an iteration formula for the eigenvalues
Î»i,n of Mn, which reads
Î»i,n+1 = Î»i,n

1 + aÎµ
2 + âˆšÎµvT
i,nBnvi,n

+ Îµ

ji
Î»i,nÎ»j,n(vT
i,nBnvj,n)2
Î»i,n âˆ’Î»j,n
,
(16.24)
where vi,n are the corresponding eigenvectors. Noting that Mn and Bn are mutually free
and that Ï„(Bn) = 0, one has, in the large N limit (using, for example, Eq. (12.8)),
E[vT
i,nBnvi,n] = 0;
E[(vT
i,nBnvj,n)2] = b
N ,
(16.25)

16.3 A Multiplicative Dyson Brownian Motion
263
where b := Ï„(B2
n). Choosing Îµ = dt, an inï¬nitesimal time scale, we end up with a
multiplicative version of the Dyson Brownian motion in ï¬ctitious time t:
dÎ»i
dt = a
2Î»i + b
N

ji
Î»iÎ»j
Î»i âˆ’Î»j
+
1
b
N Î»iÎ¾i,
(16.26)
where Î¾i is a Langevin noise, independent for each Î»i (compare with Eq. (9.9)).
Now, let us consider the â€œtimeâ€ dependent Stieltjes transform, deï¬ned as usual as
g(z,t) = 1
N

i
1
z âˆ’Î»i(t).
(16.27)
Its evolution is obtained as
âˆ‚g
âˆ‚t = 1
N

i
1
(z âˆ’Î»i)2
dÎ»i
dt = âˆ’1
N
âˆ‚
âˆ‚z

i
1
(z âˆ’Î»i)
dÎ»i
dt .
(16.28)
After manipulations very similar to those encountered in Section 9.3.1, and retaining only
leading terms in N, one ï¬nally obtains
âˆ‚g
âˆ‚t = 1
2
âˆ‚
âˆ‚z
&
(2b âˆ’a)zg âˆ’bz2g2'
.
(16.29)
Now, introduce the auxiliary function h(â„“,t) := eâ„“g(eâ„“,t) + a/2b âˆ’1, which obeys
âˆ‚h
âˆ‚t = âˆ’bhâˆ‚h
âˆ‚â„“.
(16.30)
This is precisely the Burgersâ€™ equation (9.37), up to a rescaling of time t â†’bt. Its solution
obeys the following self-consistent equation obtained using the method of characteristics
(see Section 10.1):
h(â„“,t) = h0(â„“âˆ’bth(â„“,t));
h0(â„“) := h(â„“,0) =
1
1 âˆ’eâˆ’â„“+ a
2b âˆ’1,
(16.31)
where we have assumed that at time t = 0 the dynamics starts from the identity matrix:
M0 = 1, for which g(z,0) = (z âˆ’1)âˆ’1. Hence, with z = eâ„“,
g(z,t) =
1
z âˆ’et(bzg(z,t)+a/2âˆ’b) .
(16.32)
Now, let us compare this equation to the one obeyed by the Stieltjes transform of the free
log-normal. Injecting t = zgLN âˆ’1 in
z =
t + 1
tSLN(t)
(16.33)
and using Eq. (16.15), one ï¬nds
zgLN âˆ’1 = gLNea/2âˆ’b+bzgLN â†’gLN =
1
z âˆ’ebzgLN+a/2âˆ’b,
(16.34)
which coincides with Eq. (16.32) for t = 1, as it should. For arbitrary times, one ï¬nds that
the density corresponding to the multiplicative Dyson Brownian motion, Eq. (16.26), is the
free log-normal, with parameters ta and tb.

264
Products of Many Random Matrices
16.4 The Matrix Kesten Problem
The Kesten iteration for scalar random variables appears in many different situations.
It is deï¬ned by
Zn+1 = zn(1 + Zn),
(16.35)
where zn are iid random variables. In the following, we will assume that
zn = 1 + Îµm + âˆšÎµÏƒÎ·n,
(16.36)
where Îµ â‰ª1 and Î·n are iid random variables, of zero mean and unit variance. Setting
Zn = Un/Îµ and expanding to ï¬rst order in Îµ, one obtains
Un+1 = Îµ(1 + Îµm + âˆšÎµÏƒÎ·n)

1 + Un
Îµ

= Un + ÎµmUn + âˆšÎµÏƒÎ·nUn + Îµ
(16.37)
or, in the continuous time limit dt = Îµ, the following Langevin equation:
dU
dt = 1 + mU + ÏƒÎ·U.
(16.38)
The corresponding Fokkerâ€“Planck equation reads
âˆ‚P (U,t)
âˆ‚t
= âˆ’âˆ‚
âˆ‚U [(1 + mU)P] + Ïƒ 2
2
âˆ‚2
âˆ‚U2
&
U2P
'
.
(16.39)
This process has a stationary distribution provided the drift m is negative. We will thus
write m = âˆ’Ë†m with Ë†m > 0. The corresponding stationary distribution Peq(U) obeys
(1 âˆ’Ë†mU)Peq = Ïƒ 2
2
âˆ‚
âˆ‚U
&
U2P
'
,
(16.40)
which leads to
Peq(U) =
2Î¼

(Î¼)Ïƒ 2Î¼
eâˆ’
2
Ïƒ2U
U1+Î¼ ;
Î¼ := 1 + 2 Ë†m/Ïƒ 2,
(16.41)
to wit, the distribution of U is an inverse-gamma, with a power-law tail Uâˆ’1âˆ’Î¼ with a
non-universal exponent Î¼ = 1 + 2 Ë†m/Ïƒ 2.
Now we can generalize the Kesten iteration for symmetric matrices as1
Un+1 = Îµ
1
1 + Un
Îµ

(1 + mÎµ)1 + âˆšÎµÏƒB
 1
1 + Un
Îµ ,
(16.42)
or
Un+1 âˆ’Un = Îµ (1 + mUn) + Ïƒ âˆšÎµ
*
UnB
*
Un.
(16.43)
Following the same steps as in the previous section, we obtain a differential equation for
the eigenvalues of U (where we neglect the noise when N â†’âˆ):
dÎ»i
dt = 1 âˆ’Ë†mÎ»i + Ïƒ 2
N

ji
Î»iÎ»j
Î»i âˆ’Î»j
,
(16.44)
where we again assume that m < 0 in order to ï¬nd a stationary state for our process. The
corresponding evolution of the Stieltjes transform reads, for large N,
1 The results of this section have been obtained in collaboration with T. GautiÂ´e and P. Le Doussal.

16.4 The Matrix Kesten Problem
265
âˆ‚g
âˆ‚t = âˆ‚
âˆ‚z
(
âˆ’g + (Ïƒ 2 + Ë†m)zg âˆ’1
2Ïƒ 2z2g2
)
.
(16.45)
If an equilibrium density exists, its Stieltjes transform must obey
1
2Ïƒ 2z2g2 + (1 âˆ’(Ïƒ 2 + Ë†m)z)g + C = 0,
(16.46)
where C is a constant determined by the fact that zg â†’1 when z â†’âˆ. Hence,
C = 1
2Ïƒ 2 + Ë†m.
(16.47)
From the second order equation on g one gets
g =
1
Ïƒ 2z2
(
((Ïƒ 2 + Ë†m)z âˆ’1) âˆ’
Â±
âƒ/
Ë†m2z2 âˆ’2(Ïƒ 2 + Ë†m)z + 1
)
.
(16.48)
As usual, the density of eigenvalues is non-zero when the square-root becomes imaginary.
The edges are thus given by the roots of the second degree polynomial inside the square-
root, namely
Î»Â± = Ïƒ 2 + Ë†m Â±
*
Ïƒ 2(Ïƒ 2 + 2 Ë†m)
Ë†m2
.
(16.49)
So only when Ë†m â†’0 can the spectrum extend to inï¬nity, with a power-law decay as
Î»âˆ’3/2. Otherwise, the power law is truncated beyond 2Ïƒ 2/ Ë†m2. Note that, contrary to the
scalar Kesten case, the exponent of the power law is universal, with Î¼ = 1/2.
In fact, if one stares at Eq. (16.48), one realizes that the stationary Kesten matrix U is an
inverse-Wishart matrix. Indeed, the eigenvalue spectrum given by Eq. (16.48) maps into
the MarË‡cenkoâ€“Pastur law, Eq. (4.43), provided one makes the following transformation:
Î» â†’x =
2
Ïƒ 2 + 2 Ë†m
1
Î».
(16.50)
The parameter q of the MarË‡cenkoâ€“Pastur law is then given by
q =
Ïƒ 2
Ïƒ 2 + 2 Ë†m = 1
Î¼ < 1.
(16.51)
Although not trivial, this result is not so surprising: since Wishart matrices are the matrix
equivalent of the scalar gamma distribution, the matrix equivalent of the Kesten variable
distributed as an inverse-gamma, Eq. (16.41), is an inverse-Wishart.
Bibliographical Notes
â€¢ For a general reference on products of random matrices and their applications, see
â€“ A. Crisanti, G. Paladin, and A. Vulpiani. Products of Random Matrices in Statisti-
cal Physics. Series in Solid-State Sciences, Vol. 104. Springer Science & Business
Media, 2012.
â€¢ For the speciï¬c case of electrons in disordered media, see
â€“ C. W. J. Beenakker. Random-matrix theory of quantum transport. Reviews of Modern
Physics, 69:731â€“808, 1997,
and for applications to granular media, see

266
Products of Many Random Matrices
â€“ L. Yan, J.-P. Bouchaud, and M. Wyart. Edge mode ampliï¬cation in disordered elastic
networks. Soft Matter, 13:5795â€“5801, 2017.
â€¢ For the numerical calculation of the spectrum of Lyapunov exponents, see
â€“ G. Benettin, L. Galgani, A. Giorgilli, and J.-M. Strelcyn. Lyapunov characteristic
exponents for smooth dynamical systems and for Hamiltonian systems; a method for
computing all of them. Part 1: Theory. Meccanica, 15(1):9â€“20, 1980.
â€¢ For the study of the limiting distribution of the spectrum of Lyapunov exponents, see the
early work of
â€“ C. M. Newman. The distribution of Lyapunov exponents: Exact results for random
matrices. Communications in Mathematical Physics, 103(1):121â€“126, 1986,
and, using free random matrix methods,
â€“ G. Tucci. Asymptotic products of independent gaussian random matrices with corre-
lated entries. Electronic Communications in Probability, 16:353â€“364, 2011.
â€¢ About the classical Kesten variable, see
â€“ H. Kesten. Random difference equations and renewal theory for products of random
matrices. Acta Math., 131:207â€“248, 1973,
â€“ C. de Calan, J. M. Luck, T. M. Nieuwenhuizen, and D. Petritis. On the distribution
of a random variable occurring in 1d disordered systems. Journal of Physics A:
Mathematical and General, 18(3):501â€“523, 1985,
and for the particular equation (16.38),
â€“ J.-P. Bouchaud, A. Comtet, A. Georges, and P. Le Doussal. Classical diffusion of a
particle in a one-dimensional random force ï¬eld. Annals of Physics, 201(2):285â€“341,
1990,
â€“ J.-P. Bouchaud and M. MÂ´ezard. Wealth condensation in a simple model of economy.
Physica A: Statistical Mechanics and Its Applications, 282(3):536â€“545, 2000.

17
Sample Covariance Matrices
In this chapter, we will show how to compute the various transforms (S(t), t(z), g(z)) for
sample covariance matrices (scm) when the data has non-trivial true correlations, i.e. is
characterized by a non-diagonal true underlying covariance matrix C and possibly non-
trivial temporal correlations as well. More precisely, N time series of length T are stored
in a rectangular N Ã— T matrix H. The sample covariance matrix is deï¬ned as
E = 1
T HHT .
(17.1)
If the N time series are stationary, we expect that for T â‰«N, the scm E converges to the
â€œtrueâ€ covariance matrix C. The non-trivial correlations encoded in the off-diagonal ele-
ments of C are what we henceforth call spatial (or cross-sectional) correlations. But the T
samples might also be non-independent and we will also model these temporal correlations.
Of course, the data might have both types of correlations (spatial and temporal).
We will be interested in the eigenvalues {Î»k} of E and their density ÏE(Î»), which we
will compute from the knowledge of its Stieltjes transform gE(z) using Eq. (2.47). We can
also compute the singular values {sk} of H; note that these singular values are related to the
eigenvalues of E via sk = âˆšT Î»k.
17.1 Spatial Correlations
Consider the case where H are multivariate Gaussian observations, drawn from N(0,C).
We saw in Section 4.2.4 that E is then a general Wishart matrix with column covariance C,
and can be written as
E = C
1
2 WqC
1
2 .
(17.2)
We recognize this formula as the free product of the covariance matrix C and a white
Wishart of parameter q, Wq. Note that since the white Wishart is rotationally invariant, it
is free from any matrix C. From the multiplicativity of the S-transform and the form of the
S-transform of the white Wishart (Eq. (15.21)), we have
SE(t) = SC(t)
1 + qt .
(17.3)
267

268
Sample Covariance Matrices
We can also use the subordination relation of the free product (Eq. (11.109)) to write this
relation in terms of T-transforms:
tE(z) = tC (Z(z)),
Z(z) =
z
1 + qtE(z).
(17.4)
This last expression can be written in terms of the more familiar Stieltjes transform using
t(z) = zg(z) âˆ’1:
zgE(z) = ZgC(Z),
where
Z =
z
1 âˆ’q + qzgE(z).
(17.5)
This is the central equation that allows one to infer the â€œtrueâ€ spectral density of C, ÏC(Î»),
from the empirically observed spectrum of E. Note that this equation can equivalently be
rewritten in terms of the spectral density of C as
gE(z) =

ÏC(Î¼)dÎ¼
z âˆ’Î¼(1 âˆ’q + qzgE(z)).
(17.6)
We will see in Chapter 20 some real world applications of this formula. One of the most
important properties of Eq. (17.5) is its universality: it holds (in the large N limit) much
beyond the restricted perimeter of multivariate Gaussian observations H. In fact, as soon
as the observations have a ï¬nite second moment, the relation between the â€œtrueâ€ spectral
density ÏC and the empirical Stieltjes transform gE(z) is given by Eq. (17.5).
Let us discuss some interesting limiting cases. First, when q â†’0, i.e. when T â‰«N,
one expects that E â‰ˆC. This is indeed what one ï¬nds since in that limit Z = z + O(q);
hence gE(z) = gC(z) and ÏE = ÏC.
Second, consider the case C = 1, for which gC(Z) = 1/(Z âˆ’1). We thus obtain
zgE(z) =
Z
Z âˆ’1 =
z
(z âˆ’1 + q âˆ’qzgE(z)) â†’
1
gE(z) = z1 + q âˆ’qzgE(z),
(17.7)
which coincides with Eq. (4.37). In the next exercise, we consider the case where C is an
inverse-Wishart matrix, in which case some explicit results can be obtained.
We can also infer some properties of the spectrum of E using the moment generating
function. The T-transform of E can be expressed as the following power series for z â†’âˆ:
tE(z) âˆ’â†’
zâ†’âˆ
âˆ

k=1
Ï„(Ek)zâˆ’k.
(17.8)
We thus deduce that
Z(z) âˆ’â†’
zâ†’âˆ
z
1 + q âˆ
k=1 Ï„(Ek)zâˆ’k .
Therefore we have, for z â†’âˆ,
tC(Z(z)) âˆ’â†’
zâ†’âˆ
âˆ

k=1
Ï„(Ck)
zk

1 + q
âˆ

â„“=1
Ï„(Eâ„“)zâˆ’â„“
k
.
(17.9)

17.1 Spatial Correlations
269
Hence, one can thus relate the moments of ÏE with the moments of ÏC by taking z â†’âˆ
in Eq. (17.4), namely
âˆ

k=1
Ï„(Ek)
zk
=
âˆ

k=1
Ï„(Ck)
zk

1 + q
âˆ

â„“=1
Ï„(Eâ„“)zâˆ’â„“
k
.
(17.10)
In particular, Eq. (17.10) yields the ï¬rst three moments of ÏE:
Ï„(E) = Ï„(C),
Ï„(E2) = Ï„(C2) + q,
(17.11)
Ï„(E3) = Ï„(C3) + 3qÏ„(C2) + q2.
We thus see that the mean of E is equal to that of C, whereas the variance of E is equal to
that of C plus q. As expected, the spectrum of the sample covariance matrix E is always
wider (for q > 0) than the spectrum of the population covariance matrix C.
Another interesting expansion concerns the case where q < 1, such that E is invertible.
Hence gE(z) for z â†’0 is analytic and one readily ï¬nds
gE(z) âˆ’â†’
zâ†’0 âˆ’
âˆ

k=1
Ï„

Eâˆ’k
zkâˆ’1.
(17.12)
This allows us to study the moments of Eâˆ’1, which turn out to be important quantities for
many applications. Using Eq. (17.5), we can actually relate the moments of the spectrum
Eâˆ’1 to those of Câˆ’1. Indeed, for z â†’0,
Z(z) =
z
1 âˆ’q âˆ’q âˆ
k=1 Ï„

Eâˆ’k
zk .
Hence, we obtain the following expansion:
âˆ

k=1
Ï„

Eâˆ’k
zk =
âˆ

k=1
Ï„

Câˆ’k 
z
1 âˆ’q
k 
1
1 âˆ’
q
1âˆ’q
âˆ
â„“=1 Ï„

Eâˆ’â„“
zâ„“
k
.
(17.13)
After a little work, we get
Ï„

Eâˆ’1
= Ï„

Câˆ’1
1 âˆ’q ,
Ï„

Eâˆ’2
= Ï„

Câˆ’2
(1 âˆ’q)2 + qÏ„

Câˆ’12
(1 âˆ’q)3 .
(17.14)
We will discuss in Section 20.2.1 a direct application of these formulas: Ï„(Eâˆ’1) turns out
to be related to the â€œout-of-sampleâ€ risk of an optimized portfolio of ï¬nancial instruments.
Exercise 17.1.1
The exponential moving average sample covariance matrix
(EMA-SCM)
Instead of measuring the sample covariance matrix using a ï¬‚at average over
a ï¬xed time window T , one can compute the average using an exponential

270
Sample Covariance Matrices
weighted moving average. Let us compute the spectrum of such a matrix in the
null case of iid data. Imagine we have an inï¬nite time series of vectors of size N
{xt} for t from minus inï¬nity to now. We deï¬ne the ema-scm (on time scale Ï„c) as
E(t) = Î³c
t
tâ€²=âˆ’âˆ
(1 âˆ’Î³c)tâˆ’tâ€²xtâ€²xT
tâ€²,
(17.15)
where Î³c := 1/Ï„c. Hence,
E(t) = (1 âˆ’Î³c)E(t âˆ’1) + Î³cxtâ€²xT
tâ€².
(17.16)
The second term on the right hand side can be thought of as a Wishart matrix
with T = 1 (or q = N). Now, both E(t) and E(t âˆ’1) are equal in law so we
write
E
in law
= (1 âˆ’Î³c)E + Î³cWq=N.
(17.17)
(a)
Given that E and W are free, use the properties of the R-transform to get the
equation
RE(x) = (1 âˆ’Î³c)RE((1 âˆ’Î³c)x) + Î³c(1 âˆ’NÎ³cx).
(17.18)
(b)
Take the limit N â†’âˆ, Ï„c â†’âˆwith q := N/Ï„c ï¬xed to get the following
differential equation for RE(x):
RE(x) = âˆ’x d
dx RE(x) +
1
1 âˆ’qx .
(17.19)
(c)
The deï¬nition of E is properly normalized, Ï„(E) = 1 [show this using
Eq. (17.17)], so we have the initial condition R(0) = 1. Show that
RE(x) = âˆ’log(1 âˆ’qx)
qx
(17.20)
solves your equation with the correct initial condition. Compute the variance
Îº2(E).
(d)
To compute the spectrum of eigenvalues of E, one needs to solve a complex
transcendental equation. First write z(g), the inverse of g(z). For q = 1/2 plot
z as a function of g (for âˆ’4 < g < 2). You will see that there are values of
z that are never attained by z(g), in other words g(z) has no real solutions for
these z. Numerically ï¬nd complex solutions for g(z) in that range. Plot the
density of eigenvalues ÏE(Î») given by Eq. (2.47). Plot also the density for a
Wishart with the same mean and variance.
(e)
Construct numerically the matrix E as in Eq. (17.15). Use N = 1000, Ï„c =
2000 and use at least 10 000 values for tâ€². Plot the eigenvalue distribution of
your numerical E against the distribution found in (d).

17.2 Temporal Correlations
271
17.2 Temporal Correlations
17.2.1 General Case
A common problem in data analysis arises when samples are not independent. Intuitively,
correlated samples are somehow redundant and the sample covariance matrix should
behave as if we had observed not T samples but an effective number T âˆ—< T . Let us
analyze more precisely the sample covariance matrix in the presence of correlated samples.
We will start with the case when the true spatial correlations are zero, i.e. C = 1. Our data
can then be written in a rectangular N Ã— T matrix H satisfying
E[HitHjs] = Î´ijKts,
(17.21)
where K is the T Ã— T temporal covariance matrix that we assumed to be normalized as
Ï„(K) = 1. Following the same arguments as in Section 4.2.4, we can write
H = H0K
1
2,
(17.22)
where H0 is a white rectangular matrix. So the sample covariance matrix becomes
E = 1
T HHT = 1
T H0KHT
0.
(17.23)
Now this is not quite the free product of the matrix K and a white Wishart, but if we deï¬ne
the (T Ã— T ) matrix F as
F = 1
N HT H = 1
N K
1
2 HT
0H0K
1
2 â‰¡K
1
2 W1/qK
1
2,
(17.24)
then F is the free product of the matrix K and a white Wishart matrix with parameter 1/q.
Hence,
SF(t) =
SK(t)
1 + t/q .
(17.25)
To ï¬nd the S-transform of E, we go back to Section 4.1.1, where we obtained Eq. (4.5)
relating the Stieltjes transforms of E and F. In terms of the T-transform, the relation is even
simpler:
tF(z) = qtE(qz)
â‡’
Î¶E(t) = qÎ¶F(qt),
(17.26)
where the functions Î¶(t) are the inverse T-transforms. Using the deï¬nition of the
S-transform (Eq. (11.92)), we ï¬nally get
SE(t) = SK(qt)
1 + qt ,
(17.27)
which can be expressed as a relation between inverse T-transforms:
Î¶E(t) = q(1 + t)Î¶K(qt).
(17.28)

272
Sample Covariance Matrices
We can also write a subordination relation between the T-transforms:
qtE(z) = tK

z
q(1 + tE(z))

.
(17.29)
This is a general formula that we specialize to the case of exponential temporal correlations
in the next section. Note that in the limit z â†’0, the above equation gives access to Ï„(Eâˆ’1).
Using
tE(z) =
zâ†’0 âˆ’1 âˆ’Ï„(Eâˆ’1)z + O(z2),
(17.30)
we ï¬nd
Ï„(Eâˆ’1) = âˆ’
1
qÎ¶K(âˆ’q).
(17.31)
17.2.2 Exponential Correlations
The most common form of temporal correlation in experimental data is the decaying expo-
nential, corresponding to a matrix Kts in Eq. (17.21) given by
Kts := a|tâˆ’s|,
(17.32)
where 1/ log(a) deï¬nes the temporal span of the correlations.
In Appendix A.3 we explicitly compute the S-transform of K. The result reads
SK(t) =
t + 1
*
1 + (b2 âˆ’1)t2 + bt
,
(17.33)
where b := (1 + a2)/(1 âˆ’a2). From SK one can also obtain Î¶K and its inverse tK, which
read
Î¶K(t) =
*
1 + (b2 âˆ’1)t2
t
+ b,
tK(Î¶) = âˆ’
1
*
Î¶ 2 âˆ’2Î¶b + 1
.
(17.34)
Combining Eq. (17.27) with Eq. (17.33), we get
SE(t) =
1
*
1 + (b2 âˆ’1)(qt)2 + bqt
.
(17.35)
From the S-transform, we ï¬nd
Î¶E(t) = 1 + t
tSE(t) = 1 + t
t
 /
1 + (b2 âˆ’1)(qt)2 + bqt

,
(17.36)
which when inverted leads to a fourth order equation for tE(z) that must be solved numeri-
cally, leading to the densities plotted in Fig. 17.1. However, one can obtain some informa-
tion on Ï„(Eâˆ’1). From Eqs. (17.31) and (17.34), one obtains
Ï„(Eâˆ’1) =
1
*
q2(b2 âˆ’1) + 1 âˆ’bq
:=
1
1 âˆ’qâˆ—,
(17.37)

17.2 Temporal Correlations
273
0.0
0.5
1.0
1.5
2.0
2.5
l
0.0
0.2
0.4
0.6
0.8
1.0
1.2
r(l)
q = 0.01
b = 25
q = 0.17
b = 1.5
q = 0.25
b = 1
Figure 17.1 Density of eigenvalues for a sample covariance matrix with exponential temporal
correlations for three choices of parameters q and b such that qb = 0.25. All three densities are
normalized, have mean 1 and variance Ïƒ 2
E = qb = 0.25. The solid light gray one is the MarË‡cenkoâ€“
Pastur density (q = 0.25), the dotted black one is very close to the limiting density for q â†’0 with
Ïƒ 2 = bq ï¬xed.
0.0
0.2
0.4
0.6
0.8
1.0
a
0.0
0.2
0.4
0.6
0.8
1.0
q*
q =0.5
q =0.1
Figure 17.2 Effective value qâˆ—versus the one-lag autocorrelation coefï¬cient a for a sample
covariance matrix with exponential temporal correlations shown for two values of q. The dashed
lines indicate the approximation (valid at small a) qâˆ—= q(1 + 2a2). The approximation means that,
for 10% autocorrelation, qâˆ—is only 2% greater than q.
where qâˆ—= N/T âˆ—deï¬nes the effective length of the time series, reduced by temporal
correlations (compare with Eq. (17.14) with C = 1). Figure 17.2 shows qâˆ—as a function
of a. As expected, qâˆ—= q for a = 0 (no temporal correlations), whereas qâˆ—â†’1 when
a â†’1, i.e. when Ï„c â†’âˆ. In this limit, E becomes singular.

274
Sample Covariance Matrices
Looking at Eq. (17.35), one notices that when b â‰«1 (corresponding to a â†’1,
i.e. slowly decaying correlations), the S-transform depends on b and q only through the
combination qb. One can thus deï¬ne a new limiting distribution corresponding to the limit
q â†’0, b â†’âˆwith qb = Ïƒ 2 (which turns out to be the variance of the distribution, see
below). The S-transform of this limiting distribution is given by
S(t) =
1
*
1 + (Ïƒ 2t)2 + Ïƒ 2t
,
(17.38)
while the equation for the T-transform boils down to a cubic equation that reads:
z2t2(z) âˆ’2Ïƒ 2zt2(z)(1 + t(z)) = (1 + t(z))2.
(17.39)
The corresponding R-transform is
R(z) =
1
âˆš
1 âˆ’2Ïƒ 2z
= 1 + Ïƒ 2z + 3
2Ïƒ 4z2 + O(z3).
(17.40)
The last equation gives its ï¬rst three cumulants: its average is equal to one, its variance
is Ïƒ 2 as announced above, and its skewness is Îº3 = 3
2Ïƒ 4. We notice that this skewness is
larger than that of a white Wishart with the same variance (q = Ïƒ 2) for which Îº3 = Ïƒ 4.
The equations for the Stieltjes g(z) and the T-transform are both cubic equations. The
corresponding distribution of eigenvalues is shown in Figure 17.3. Note that, unlike the
MarË‡cenkoâ€“Pastur, there is always a strictly positive lower edge of the spectrum Î»âˆ’> 0
and no Dirac at zero even when Ïƒ 2 > 1. Unfortunately, the equation giving Î»Â± is a fourth
order equation that does not have a concise solution.
0
1
2
3
4
5
l
0.0
0.5
1.0
1.5
2.0
r(l)
s2 = 0.25
s2 = 0.5
s2 = 1
Figure 17.3 Density of eigenvalues for the limiting distribution of sample covariance matrix with
exponential temporal correlations WÏƒ 2 for three choices of the parameter Ïƒ 2: 0.25, 0.5 and 1.

17.2 Temporal Correlations
275
An intuitive way to understand this particular random matrix ensemble is to consider
N independent Ornsteinâ€“Uhlenbeck processes with the same correlation time Ï„c that we
record over a long time T . We sample the data at interval , such that the total number
of observations is T /. We then construct a sample covariance matrix of the N variables
from these observations. If  â‰«Ï„c, then each sample can be considered independent
and the sample covariance matrix will be a MarË‡cenkoâ€“Pastur with q = N/T . But if
we â€œoversampleâ€ at intervals  â‰ªÏ„c, such that our observations are strongly correlated,
then the resulting sample covariance matrix no longer depends on  but only on Ï„c. The
sample covariance matrix converges in this case to our new random matrix characterized
by Eq. (17.38), with parameter Ïƒ 2 = qb = NÏ„c/T .
17.2.3 Spatial and Temporal Correlations
In the general case where spatial and temporal correlations exist, the sample covariance
matrix can be written as
E = 1
T HHT = 1
T C
1
2 H0KHT
0C
1
2,
(17.41)
using the same notations as above. After similar manipulations, the S-transform of E is
found to be given by
SE(t) = SC(t)SK(qt)
1 + qt
,
(17.42)
which leads to
Î¶E(t) = qtÎ¶C(t)Î¶K(qt),
(17.43)
or, in terms of T-transforms,
qtE(z) = tK

z
qtE(z)Î¶C(tE(z))

.
(17.44)
When C = 1, Î¶C(t) = (1 + t)/t and one recovers Eq. (17.29). Specializing to the case of
exponential correlations in the limit q â†’0, a â†’1, qb = Ïƒ 2, we obtain the following
equation for the T-transform of the limiting distribution, now for an arbitrary covariance
matrix C:
z2 âˆ’2Ïƒ 2ztE(z)Î¶C(tE(z)) = Î¶ 2
C(tE(z)),
(17.45)
where we used tK(z) = âˆ’1/
âˆš
z2 âˆ’2zb + 1. When C = 1, one recovers Eq. (17.39).
When C is an inverse-Wishart matrix, Î¶C(t) = (t + 1)/t(1 âˆ’pt), the equation for tE(z) is
of fourth order.
Note ï¬nally that Eq. (17.44), in the limit z â†’0, yields a simple generalization of
Eq. (17.31) that reads
Ï„(Eâˆ’1) = âˆ’Ï„(Câˆ’1)
qÎ¶K(âˆ’q).
(17.46)

276
Sample Covariance Matrices
Comparing with Eq. (17.14) allows us to deï¬ne an effective length of the time series which,
interestingly, is independent of C and reads
qâˆ—:= N
T âˆ—= 1 + qÎ¶K(âˆ’q).
(17.47)
Exercise 17.2.1
On the futility of oversampling
Consider data consisting of N variables (columns) with true correlation C and
T independent observations (rows). Instead of computing the sample covariance
matrix with these T observations, we repeat each one m times and sum over
mT columns. Obviously the redundant columns should not change the sample
covariance matrix, hence it should have the same spectrum as the one using only
the original T observations.
(a)
The redundancy of columns can be modeled as a temporal correlation with
an mT Ã— mT covariance matrix K that is block diagonal with T blocks of
size K and all the values within one block equal to 1 and zero outside the
blocks. Show that this matrix has T eigenvalues equal to m and (T âˆ’1)m zero
eigenvalues.
(b)
Compute tK(z) for this model.
(c)
Show that SK(t) = (1 + t)/(1 + mt).
(d)
If we include the redundant columns we have a value of qm = N/(mT ), but
we need to take temporal correlations into account so SE(t) = SC(t)SK(qmt)/
(1 + qmt). Show that in this case SE(t) = SC(t)/(1 + qt) with q = N/T ,
which is the result without the redundant columns.
17.3 Time Dependent Variance
Another common and important situation is when the N correlated time series are het-
eroskedastic, i.e. have a time dependent variance. More precisely, we consider a model
where
xt
i = ÏƒtHit,
(17.48)
where Ïƒt is time dependent, and
E[HitHjs] = Î´tsCij,
(17.49)
i.e. xt
i is the product of a time dependent factor Ïƒt and a random variable with a general
correlation structure C but no time correlations. The scm E can be expressed as
E =
T

t=1
Pt,
Pt := 1
T Ïƒ 2
t HtHT
t ,
(17.50)

17.3 Time Dependent Variance
277
where each Pt is a rank-1 matrix with a non-zero eigenvalue that converges, when N and
T tend to inï¬nity, to qÏƒ 2
t Ï„(C) with, as always, q = N/T .
We will ï¬rst consider the case C = 1, i.e. a structureless covariance matrix. In this case,
the vectors xt are rotationally invariant, the matrix E can be viewed as the free sum of a
large number of rank-1 matrices, each with a non-zero eigenvalue equal to qÏƒ 2
t . Hence,
RE(g) =
T

t=1
Rt(g).
(17.51)
To compute the R-transform of the matrix E we need to compute the R-transform of a
rank-1 matrix. Note that since there are T terms in the sum, we will need to know Rt(g)
including correction of order 1/N:
gt(z) = 1
N
N âˆ’1
z
+
1
z âˆ’qÏƒ 2t

= 1
z + 1
N
qÏƒ 2
t
z(z âˆ’qÏƒ 2t ).
(17.52)
Inverting to ï¬rst order in 1/N we ï¬nd
zt(g) = 1
g + 1
N
qÏƒ 2
t
1 âˆ’qÏƒ 2t g .
(17.53)
Now, since R(z) = z(g) âˆ’1/z, we ï¬nd
RE(g) = 1
T
T

t=1
Ïƒ 2
t
1 âˆ’qÏƒ 2t g .
(17.54)
The ï¬‚uctuations of Ïƒ 2
t can be stochastic or deterministic. In the large T limit we can encode
them with a probability density P(s) for s = Ïƒ 2 and convert the sum into an integral,
leading to1
RE(g) =
 âˆ
0
sP(s)
1 âˆ’qsg ds.
(17.55)
Note that if the variance is always 1 (i.e. P(s) = Î´(s âˆ’1)), we recover the R-transform of
a Wishart matrix of parameter q:
Rq(g) =
1
1 âˆ’qg .
(17.56)
In the general case, the R-transform of E is simply related to the T-transform of the distri-
bution of s:
RE(g) = ts
 1
qg

.
(17.57)
1 When the distribution of s is bounded, the integral (17.55) always converges for small enough g and the R-transform is well
deï¬ned near zero. For unbounded s, the R-transform can be singular at zero indicating that the distribution of eigenvalues
doesnâ€™t have an upper edge.

278
Sample Covariance Matrices
In the more general case where C is not the identity matrix, one can again write the scm as
E = C
1
2 EC
1
2 , where E corresponds to the case C = 1 that we just treated. Hence, using
the fact that C and E are mutually free, the S-transform of E is simply given by
SE(t) = SC(t)SE(t).
(17.58)
Another way to treat the problem is to view the ï¬‚uctuating variance as a diagonal temporal
covariance matrix with entries drawn from P(s). Following Section 17.2.3, we can write
SE(t) = Ss(qt)
1 + qt ,
SE(t) = SC(t)Ss(qt)
1 + qt
,
(17.59)
with Ss(t) the S-transform associated with ts(Î¶).
A particular case of interest for ï¬nancial applications is when P(s) is an inverse-gamma
distribution. When xt is a Gaussian multivariate vector, one obtains for Ïƒtxt a Student
multivariate distribution (see bibliographical notes for more on this topic).
17.4 Empirical Cross-Covariance Matrices
Let us now consider two time series xt and yt, each of length T , but of different
dimensions, respectively N1 and N2. The empirical cross-covariance matrix is an N1Ã—N2
rectangular matrix deï¬ned as
Exy = 1
T
T

t=1
xt(yt)T .
(17.60)
Let us assume that the â€œtrueâ€ cross-covariance matrix E[xyT ] is zero, i.e. that there are
no true cross-correlations between our two sets of variables. What is the singular value
spectrum of Exy in this case?
As with scm that are described by the MarË‡cenkoâ€“Pastur law when N,T â†’âˆwith
a ï¬xed ratio q = N/T , we expect that some non-trivial results will appear in the limit
N1,N2,T â†’âˆwith q1 = N1/T and q2 = N2/T ï¬nite. A convenient way to perform
this analysis is to consider the eigenvalues of the N1 Ã— N1 matrix Mxy = ExyETxy, which
are equal to the square of the singular values s of Exy.
The matrix Mxy shares the same non-zero eigenvalues as those of ExEy, where Ex
and Ey are the dual T Ã— T sample covariance matrices:
Ex = xT x,
Ey = yT y.
(17.61)
Hence one can compute the spectral density of Mxy using the free product formalism and
infer the spectrum of the product ExEy. However, the result will depend on the â€œtrueâ€
covariance matrices of x and y, which are usually unknown in practical applications.
A way to obtain a universal result is to consider the sample-normalized principal
components of x and of y, which we call x and y, such that the corresponding dual
covariance matrix Ex has N1 eigenvalues exactly equal to 1 and T âˆ’N1 eigenvalues
exactly equal to zero, whereas Ey has N2 eigenvalues exactly equal to 1 and T âˆ’N2
eigenvalues exactly equal to zero. This is precisely the problem studied in Section 15.4.2.
The singular value spectrum of Exy is thus given by
Ï(s) = max(q1 + q2 âˆ’1,0)Î´(s âˆ’1) + Re
*
(s2 âˆ’Î³âˆ’)(Î³+ âˆ’s2)
Ï€s(1 âˆ’s2)
,
(17.62)

17.4 Empirical Cross-Covariance Matrices
279
where Î³Â± are given by
Î³Â± = q1 + q2 âˆ’2q1q2 Â± 2
*
q1q2(1 âˆ’q1)(1 âˆ’q2),
0 â‰¤Î³Â± â‰¤1.
(17.63)
The allowed sâ€™s are all between 0 and 1, as they should be, since these singular values can
be interpreted as correlation coefï¬cients between some linear combination of the xâ€™s and
some other linear combination of the yâ€™s.
In the limit T â†’âˆat ï¬xed N1, N2, all singular values collapse to zero, as they
should since there are no true correlations between x and y. The allowed band in the limit
q1,q2 â†’0 becomes
s âˆˆ

| âˆšq1 âˆ’âˆšq2|, âˆšq1 + âˆšq2

,
showing that for ï¬xed N1,N2, the order of magnitude of allowed singular values decays
as T âˆ’1
2 . The above result allows one to devise precise statistical tests to detect â€œtrueâ€
cross-correlations between sets of variables.
Bibliographical Notes
â€¢ The subject of this chapter is treated in several books, see e.g.
â€“ Z. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matri-
ces. Springer-Verlag, New York, 2010,
â€“ A. M. Tulino and S. VerdÂ´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,
â€“ L. Pastur and M. Scherbina. Eigenvalue Distribution of Large Random Matrices.
American Mathematical Society, Providence, Rhode Island, 2010,
â€“ R. Couillet and M. Debbah. Random Matrix Methods for Wireless Communications.
Cambridge University Press, Cambridge, 2011.
â€¢ The initial â€œhistoricalâ€ paper is of course
â€“ V. A. Marchenko and L. A. Pastur. Distribution of eigenvalues for some sets of
random matrices. Matematicheskii Sbornik, 114(4):507â€“536, 1967,
with many posterior rediscoveries â€“ as for example in2
â€“ A. Crisanti and H. Sompolinsky. Dynamics of spin systems with randomly asym-
metric bonds: Langevin dynamics and a spherical model. Physical Review A,
36:4922â€“4939, 1987,
â€“ A. Sengupta and P. P. Mitra. Distributions of singular values for some random matri-
ces. Physical Review E, 60(3):3389, 1999.
â€¢ The universality of Eq. (17.5) is discussed in
â€“ J. W. Silverstein and Z. Bai. On the empirical distribution of eigenvalues of a
class of large dimensional random matrices. Journal of Multivariate Analysis,
54(2):175â€“192, 1995.
â€¢ The case of power-law distributed variables and deviations from MarË‡cenkoâ€“Pastur are
discussed in
2 In fact, Crisanti and Sompolinsky [1987] themselves cite an unpublished work of D. Movshovitz and H. Sompolinsky.

280
Sample Covariance Matrices
â€“ S. Belinschi, A. Dembo, and A. Guionnet. Spectral measure of heavy tailed
band and covariance random matrices. Communications in Mathematical Physics,
289(3):1023â€“1055, 2009,
â€“ G. Biroli, J.-P. Bouchaud, and M. Potters. On the top eigenvalue of heavy-tailed
random matrices. Europhysics Letters (EPL), 78(1):10001, 2007,
â€“ A. Aufï¬nger, G. Ben Arous, and S. PÂ´echÂ´e. Poisson convergence for the largest eigen-
values of heavy tailed random matrices. Annales de lâ€™I.H.P. ProbabilitÂ´es et statis-
tiques, 45(3):589â€“610, 2009.
â€¢ The case with spatial and temporal correlations was studied in
â€“ A. Sengupta and P. P. Mitra. Distributions of singular values for some random matri-
ces. Physical Review E, 60(3):3389, 1999,
â€“ Z. Burda, J. Jurkiewicz, and B. WacÅ‚aw. Spectral moments of correlated Wishart
matrices. Physical Review E, 71:026111, 2005.
â€¢ Multivariate Student (or elliptical) variables and their associated sample covariance
matrices are studied in
â€“ G. Biroli, J.-P. Bouchaud, and M. Potters. The Student ensemble of correlation matri-
ces: eigenvalue spectrum and Kullback-Leibler entropy. preprint arXiv:0710.0802,
2007,
â€“ N. El Karoui et al. Concentration of measure and spectra of random matrices: Appli-
cations to correlation matrices, elliptical distributions and beyond. The Annals of
Applied Probability, 19(6):2362â€“2405, 2009.
â€¢ Large cross-correlation matrices and corresponding null-hypothesis statistical tests are
studied in
â€“ I. M. Johnstone. Multivariate analysis and Jacobi ensembles: Largest eigenvalue, Tra-
cyWidom limits and rates of convergence. The Annals of Statistics, 36(6):2638â€“2716,
2008,
â€“ J.-P. Bouchaud, L. Laloux, M. A. Miceli, and M. Potters. Large dimension forecast-
ing models and random singular value spectra. The European Physical Journal B,
55(2):201â€“207, 2007,
â€“ Y. Yang and G. Pan. Independence test for high dimensional data based on regu-
larized canonical correlation coefï¬cients. The Annals of Statistics, 43(2):467â€“500,
2015.
â€¢ For an early derivation of Eq. (17.62) without the use of free probability methods, see
â€“ K. W. Wachter. The limiting empirical measure of multiple discriminant ratios. The
Annals of Statistics, 8(5):937â€“957, 1980.

18
Bayesian Estimation
In this chapter we will review the subject of Bayesian estimation, with a particular focus on
matrix estimation. The general situation one encounters is one where the observed matrix
is a noisy version of the â€œtrueâ€ matrix one wants to estimate. For example, in the case of
additive noise, one observes a matrix E which is the true matrix C plus a random matrix X
that plays the role of noise, to wit,
E = C + X.
(18.1)
In the case of multiplicative noise, the observed matrix E has the form
E = C
1
2 WC
1
2 .
(18.2)
When W is a white Wishart matrix, this is the problem of sample covariance matrix encoun-
tered in Chapter 17.
In general, the true matrix C is unknown to us. We would like to know the probability
of C given that we have observed E, i.e. compute P(C|E). This is the general subject of
Bayesian estimation, which we introduce and discuss in this chapter.
18.1 Bayesian Estimation
Before doing Bayesian theory on random matrices (see Section 18.3), we ï¬rst review
Bayesian estimation and see it at work on simpler examples.
18.1.1 General Framework
Imagine we have an observable variable y that we would like to infer from the observation
of a related variable x. The variables x and y can be scalars, vectors, matrices, higher
dimensional objects . . . We postulate that we know the random process that generates y
given x, i.e. y could be a noisy version of x or more generally y could be drawn from
a known distribution with x as a parameter. The generation process of y is encoded in a
probability distribution P(y|x), which is called the sampling distribution or the likelihood
function.
281

282
Bayesian Estimation
Given our knowledge of P(y|x), we would like to write the inference probability
P(x|y), also called the posterior distribution. To do so, we can use Bayesâ€™ rule:
P(x|y) = P(y|x)P0(x)
P(y)
.
(18.3)
To obtain the desired probability, Bayesâ€™ rule tells us that we need to know the prior
distribution P0(x). In theory P0(x) is the distribution from which x is drawn and it is
in some cases knowable. In many practical applications, however, x is actually not random
but simply unknown and P0(x) encodes our ignorance of x. It should represent our best
(probabilistic) guess of x before we observe the data y. The determination (or arbitrariness)
of the prior P0(x) is considered to be one of the weak points of the Bayesian approach.
Often P0(x) is just taken to be constant, i.e. no prior knowledge at all on x. However, note
that P0(x) = constant is not invariant upon changes of variables, for if xâ€² = f (x) is a non-
linear transformation of x, then P0(xâ€²) is no longer constant! In Section 18.1.3, we will see
how the arbitrariness in the choice of P0(x) can be used to simplify modeling.
The other distribution appearing in Bayesâ€™ rule P(y) is actually just a normalization
factor. Indeed, y is assumed to be known, therefore P(y) is just a ï¬xed number that can be
computed by normalizing the posterior distribution. One therefore often simpliï¬es Bayesâ€™
rule as
P(x|y) = 1
Z P(y|x)P0(x),
Z :=

dx P(y|x)P0(x),
(18.4)
where P(y|x) represents the measurement (or noise) process and P0(x) the (often arbitrary)
prior distribution.
From the posterior distribution P(x|y) we can build an estimator of x. The optimal esti-
mator depends on the problem at hand, namely, which quantity are we trying to optimize.
The most common Bayesian estimators are
1 mmse: The posterior mean E[x]y. It minimizes a quadratic loss function and is hence
called the Minimum Mean Square Error estimator.
2 mave: The posterior median or Minimum Absolute Value Error estimator.
3 map: The Maximum A Posteriori estimator, deï¬ned as Ë†x = argmaxx P(x|y).
18.1.2 A Simple Estimation Problem
Consider the simplest one-dimensional estimation problem:
y = x + Îµ,
(18.5)
where x is some signal to be estimated, Îµ is an independent noise, and y is the observation.
Then P(y|x) is simply PÎµ(.) evaluated at y âˆ’x:
P(y|x) = PÎµ(y âˆ’x).
(18.6)

18.1 Bayesian Estimation
283
Suppose further that Îµ is a centered Gaussian noise with variance Ïƒ 2
n , where the subscript
n means â€œnoiseâ€. Then we have
P(y|x) =
1
*
2Ï€Ïƒ 2n
exp

âˆ’(y âˆ’x)2
2Ïƒ 2n

.
(18.7)
Then we get that
P(x|y) âˆP0(x) exp
2xy âˆ’x2
2Ïƒ 2n

,
(18.8)
where P0(x) is the prior distribution of x and we have dropped x-independent factors.
Depending on the choice of P0(x) we will get different posterior distributions and hence
different estimators of x.
Gaussian Prior
Suppose ï¬rst P0(x) is a Gaussian with variance Ïƒ 2
s (for signal) centered at x0. Then
P(x|y) âˆexp

âˆ’(x âˆ’x0)2
2Ïƒ 2s
+ 2xy âˆ’x2
2Ïƒ 2n

=
1
âˆš
2Ï€Ïƒ 2 exp

âˆ’

x âˆ’Ë†x
2
2Ïƒ 2

,
(18.9)
with
Ë†x := x0 + r(y âˆ’x0) = (1 âˆ’r)x0 + ry;
Ïƒ 2 := rÏƒ 2
n,
(18.10)
where the signal-to-noise ratio r is r = Ïƒ 2
s /(Ïƒ 2
s + Ïƒ 2
n ). The posterior distribution is thus a
Gaussian centered around Ë†x and of variance Ïƒ 2.
For a Gaussian distribution the mean, median and maximum probability values are all
equal to Ë†x, which is therefore the optimal estimator in all three standard procedures, mmse,
mave and map. This estimator is called the linear shrinkage estimator as it is linear in
the observed variable y. The linear coefï¬cient of y is the signal-to-noise ratio r, a number
smaller than one that shrinks the observed value towards the a priori mean x0.
Note that this estimator can also be obtained in a completely different framework: it
is the afï¬ne estimator that minimizes the mean square error. The estimator is afï¬ne by
construction and minimization only involves ï¬rst and second moments; it is therefore not
too surprising that we recover Eq. (18.10), see Exercise 18.1.2. As so often in optimization
problems, assuming Gaussian ï¬‚uctuations is equivalent to imposing an afï¬ne solution.
Another important property of the linear shrinkage estimator is that it is rather conserva-
tive: it is biased towards x0. By assumption x ï¬‚uctuates with variance Ïƒ 2
s and y ï¬‚uctuates
with variance Ïƒ 2
s + Ïƒ 2
n . This allows us to compute the variance of the estimator Ë†x(y) as
V[Ë†x(y)] = r2(Ïƒ 2
s + Ïƒ 2
n ) =
Ïƒ 4
s
Ïƒ 2s + Ïƒ 2n
â‰¤Ïƒ 2
s .
(18.11)

284
Bayesian Estimation
So the variance of the estimator1 is not only smaller than that of the observed variable y it
is also smaller than the ï¬‚uctuations of the true variable x!
Exercise 18.1.1
Optimal afï¬ne estimator
Suppose that we observe a variable y that has some non-zero covariance with
an unknown variable x that we would like to estimate. We will show that the
best afï¬ne estimator of x is given by the linear shrinkage estimator (18.10). The
variables x and y can be drawn from any distribution with ï¬nite variance. We
write the general afï¬ne estimator
Ë†x = ay + b,
(18.12)
and choose a and b to minimize the expected mean square error.
(a)
Initially assume that x and y have zero mean â€“ we will relax this assumption
later. Show that
E
&
(x âˆ’Ë†x)2'
= a2Ïƒ 2
y + b2 + Ïƒ 2
x âˆ’2aÏƒ 2
xy,
(18.13)
where Ïƒ 2
x , Ïƒ 2
y and Ïƒ 2
xy are the variances of x, y and their covariance.
(b)
Show that the optimal estimator has a = Ïƒ 2
xy/Ïƒ 2
y and b = 0.
(c)
Compute b in the non-zero mean case by considering x âˆ’x0 estimated using
y âˆ’y0.
(d)
Compute Ïƒ 2
y and Ïƒ 2
xy when y = x + Îµ with Îµ independent of x.
(e)
Show that when E[Îµ] = 0 we recover Eq. (18.10).
Bernoulli Prior
When P0(x) is non-Gaussian, the obtained estimators are in general non-linear. As a second
example suppose that P0(x) is Bernoulli random variable with P0(x = 1) = P0(x = âˆ’1) =
1/2. Then, after a few simple manipulations one obtains
P(x|y) = 1
2

1 + tanh
 y
Ïƒ 2n

Î´x,1 +

1 âˆ’tanh
 y
Ïƒ 2n

Î´x,âˆ’1

.
(18.14)
The posterior distribution is now a discrete function that takes on only two values, namely
Â±1. In this case the maximum probability and the median are such that
Ë†xmap(y) = sign(y).
(18.15)
1 One should not confuse the variance of the posterior distribution rÏƒ2n with the variance of the estimator rÏƒ2s . The ï¬rst one
measures the remaining uncertainty about x once we have observed y while the second measures the variability of Ë†x(y) when
we repeat the experiment multiple times with varying x and noise Îµ.

18.1 Bayesian Estimation
285
It is also easy to calculate the mmse estimator:
Ë†xmmse(y) = E[x]y = tanh
 y
Ïƒ 2n

.
(18.16)
It may seem odd that the mmse estimator takes continuous values between âˆ’1 and 1 while
we postulated that the true x can only be equal to Â±1. Nevertheless, in order to minimize the
variance it is optimal to shoot somewhere in the middle of âˆ’1 and 1 as choosing the wrong
sign costs a lot in terms of variance. The estimator Ë†x(y) is biased, i.e. E[Ë†xmmse|x]  x. It
also has a variance strictly less than 1, whereas the variance of the true x is unity.
Laplace Prior
As a third example, consider a Laplace distribution
P0(x) = b
2eâˆ’b|x|
(18.17)
for the prior, with variance 2bâˆ’2. In this case the posterior distribution is given by
P(x|y) âˆexp

âˆ’b|x| + 2xy âˆ’x2
2Ïƒ 2n

.
(18.18)
The mmse and mave estimators can be computed but the results are not very enlightening
as they are given by an ugly combination of error functions and even inverse error functions
(for mave). The map estimator is both simpler and more interesting in this case. It is
given by
Ë†xmap(y) =
0
0
for |y| < bÏƒ 2
n,
y âˆ’bÏƒ 2
n sign(y)
otherwise.
(18.19)
The map estimator is sparse in the sense that in a non-zero fraction of cases it takes the
exact value of zero. Note that the true variable x itself is not sparse: it is almost surely
non-zero. This example is a toy-model for the â€œlassoâ€ regularization that we will study in
Section 18.2.2.
Non-Gaussian Noise
The noise in Eq. (18.5) can also be non-Gaussian. When the noise has fat tails, one can even
be in the counter-intuitive situation where the estimator is not monotonic in the observed
variable, i.e. the best estimate of x decreases as a function of its noisy version y. For
example, if x is centered unit Gaussian and Îµ is a centered unit Cauchy noise, we have
P(x|y) âˆ
eâˆ’x2/2
(y âˆ’x)2 + 1.
(18.20)
Whereas the Cauchy noise Îµ and the observation y do not have a ï¬rst moment, the
posterior distribution of x is regularized by the Gaussian weight and all its moments

286
Bayesian Estimation
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
y
âˆ’1.0
âˆ’0.5
0.0
0.5
1.0
[x]y
Figure 18.1 A non-monotonic optimal estimator. The mmse estimator of a Gaussian variable
corrupted by Cauchy noise (see Eq. (18.21)). For small absolute observations y, the estimator is
almost linear with slope 2 âˆ’âˆš2/eÏ€/erfc(1/
âˆš
2) â‰ˆ0.475 (dashed line).
are ï¬nite. After some tedious calculation we arrive at the conditional mean or mmse
estimator:
E[x]y = y + Im()
Re(),
where
 = eiyerfc
1 + iy
âˆš
2

.
(18.21)
The shape of the estimator as a function of y is not obvious from this expression but it is
plotted numerically in Figure 18.1. The interpretation is the following:
â€¢ When we observe a small (order 1) value of y, we can assume that it was generated by a
moderate x with moderate noise, hence we are in the regime of the linear estimator with
a signal-to-noise ratio close to one-half (Ë†x â‰ˆ0.475y).
â€¢ On the other hand, when y is much larger than the standard deviation of x it becomes
clear that y can only be large because the noise takes extreme values. When the noise is
large our knowledge of x decreases, hence the estimator tends to zero as |y| â†’âˆ.
18.1.3 Conjugate Priors
The main weakness of Bayesian estimation is the reliance on a prior distribution for the
variable we want to estimate. In many practical applications one does not have a prob-
abilistic or statistical knowledge of P0(x). The variable x is a ï¬xed quantity that we do
not know, so how are we supposed to know about P0(x)? In such cases we are left with
making a reasonable practical guess. Since P0(x) is just a guess, we can at least choose a
functional form for P0(x) that makes computation easy. This is the idea behind â€œconjugate
priorsâ€.

18.1 Bayesian Estimation
287
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x
0.0
0.5
1.0
1.5
2.0
P0(x)
a = 2,b = 1
a = 2,b = 2
a = 30,b = 30
Figure 18.2 The inverse-gamma distribution Eq. (18.24). Its mean is given by b/(a âˆ’1) and it
becomes increasingly peaked around this mean as both a and b become large.
When we studied the one-dimensional estimation of a variable x corrupted by additive
Gaussian noise (Eq. (18.5), with Gaussian Îµ) we found that choosing a Gaussian prior for
x gave us a Gaussian posterior distribution. In many other cases, we can ï¬nd a family of
prior distributions that will similarly keep the posterior distribution in the same family. This
concept is better explained in an example.
Imagine we are given a series of T numbers {yi} generated independently from a cen-
tered Gaussian distribution of variance c that is unknown to us. We use the variable c rather
than Ïƒ 2 to avoid the confusion between the estimation of Ïƒ and that of c = Ïƒ 2. The joint
probability of the yâ€™s is given by
P(y|c) =
1
(2Ï€c)T/2 exp

âˆ’yT y
2c

.
(18.22)
The posterior distribution is thus given by
P(c|y) âˆP0(c)câˆ’T/2 exp

âˆ’yT y
2c

.
(18.23)
Now if the prior P0(c) has the form P0(c) âˆcâˆ’aâˆ’1eâˆ’b/c, the posterior will also be of that
form with modiï¬ed values for a and b. Such a P0(c) will thus be our conjugate prior. This
law is precisely the inverse-gamma distribution (see Fig. 18.2):
P0(c) =
ba

(a)câˆ’aâˆ’1eâˆ’b/c
(c â‰¥0).
(18.24)
It describes a non-negative variable, as a variance should. It is properly normalized when
a > 0 and has mean b/(a âˆ’1) whenever a > 1. If we choose such a law as our variance

288
Bayesian Estimation
prior, the posterior distribution after having observed the vector y is also an inverse-gamma
with parameters
ap = a + T
2
and
bp = b + yT y
2 .
(18.25)
The mmse estimator can then just be read off from the mean of an inverse-gamma distribu-
tion:
E[c]y =
bp
ap âˆ’1 =
2b + yT y
2(a âˆ’1) + T ,
(18.26)
which can be written explicitly in the form of a linear shrinkage estimator:
E[c]y = (1 âˆ’r)c0 + r yT y
T
with
r =
T
2(a âˆ’1) + T ,
(18.27)
and c0 = b/(a âˆ’1) is the mean of the prior. We see that r â†’1 when T â†’âˆ: in this case
the prior guess on c0 disappears and one is left with the naive empirical estimator yT y/T .
Exercise 18.1.2
Conjugate prior for the amplitude of a Laplace distribution
Suppose that we observe T variables yi drawn from a Laplace distribution
(18.17) with unknown amplitude b. We would like to estimate b using the
Bayesian method with conjugate prior.
(a)
Write the joint probability density of elements of the vector y for a given b.
This is the likelihood function P(y|b).
(b)
As a function of b, the likelihood function has the same form as a gamma
distribution (4.17). Using a gamma distribution with parameters a0 and b0
for the prior on b show that the posterior distribution of b is also a gamma
distribution. Find the posterior parameters ap and bp.
(c)
Given that the mean of a gamma distribution is given by a/b, write the mmse
estimator in this case.
(d)
Compute the estimator in the two limiting cases T = 0 and T â†’âˆ.
(e)
Write your estimator from (c) as a shrinkage estimator interpolating
between these two limits. Show that the signal-to-noise ratio r is given by
r = T m/(T m + 2b0) where m =  |yi|/T . Note that in this case the
shrinkage estimator is non-linear in the naive estimate Ë†b = 1/(2m).
18.2 Estimating a Vector: Ridge and LASSO
A very standard problem for which Bayesian ideas are helpful is linear regression. Assume
we want to estimate the parameters ai of a multi-linear regression, where we assume that
an observable y can be written as

18.2 Estimating a Vector: Ridge and LASSO
289
y =
N

i=1
aixi + Îµ,
(18.28)
where xi are N observable quantities and Îµ is noise (not directly observable). We observe
a time series of y of length T that we stack into a vector y, whereas the different xi are
stacked into an N Ã— T data matrix Hit = xt
i , and Îµ is the corresponding T -dimensional
noise vector. We thus write
y = HT a + Îµ,
(18.29)
where a is an N-dimensional vector of coefï¬cients we want to estimate. We assume the
following structure for the random variables x and Îµ:
1
T E[ÎµÎµT ] = Ïƒ 2
n 1;
1
T E[HHT ] = C,
(18.30)
where C can be an arbitrary covariance matrix, but we will assume it to be the identity 1 in
the following, unless otherwise stated.
Classical linear regression would ï¬nd the coefï¬cient vector a that minimizes the error
E = âˆ¥y âˆ’HT aâˆ¥2 on a given dataset. As is well known, the regression coefï¬cients are
given by
areg =

HHT âˆ’1 Hy.
(18.31)
This equation can be derived easily by taking the derivatives of E with respect to all ai and
setting them to zero. Note that when q := N/T < 1, HHT is in general invertible, but
when q â‰¥1 (i.e. when there is not enough data), Eq. (18.31) is a priori ill deï¬ned.
In a Bayesian estimation framework, we want to write the posterior distribution P(a|y)
and build an estimator of a from it. We expect that the Bayesian approach will work better
than linear regression â€œout of sampleâ€, i.e. on a new independent sample. The reason is
that the linear regression method minimizes an â€œin-sampleâ€ error, and is thus devised to ï¬t
best the details of the observed dataset, with no regard to overï¬tting considerations. These
concepts will be clariï¬ed in Section 18.2.3.
Following the approach of Section 18.1, we write the posterior distribution as
P(a|y) âˆP0(a) exp

âˆ’1
2Ïƒ 2n
>>y âˆ’HT a
>>2

,
(18.32)
where Ïƒ 2
n is the variance of the noise Îµ. Now, the art is to choose an adequate prior
distribution P0(a).
18.2.1 Ridge Regression
The likelihood function in Eq. (18.32) is a Gaussian function of a, so choosing a
Gaussian prior for P0(a) will give us a Gaussian posterior. To construct a Gaussian
distribution for P0(a) we need to choose a prior mean a0 and a prior covariance matrix.

290
Bayesian Estimation
Regression coefï¬cients can be positive or negative, so the most natural prior mean is the
zero vector a0 = 0. In the absence of any other information about the direction in which
a may point, we should make a rotationally invariant prior for the covariance matrix.2 The
only rotationally invariant choice is a multiple of the identity Ïƒ 2
s 1 for the prior covariance.
Assuming that the coefï¬cients ai are iid gives the same answer. However, we do not have a
good argument to set the scale of the covariance Ïƒ 2
s ; we will come back to this point later.
The posterior distribution is then written
P(a|y) âˆexp

âˆ’1
2Ïƒ 2n

aT

HHT + Ïƒ 2
n
Ïƒ 2s
1

a âˆ’2aT Hy

.
(18.33)
As announced, the posterior is a multivariate Gaussian distribution. The mmse, mave and
map estimator are all equal to the mode of the distribution, given by3
E[a]y =
HHT
T
+ Î¶1
âˆ’1 Hy
T ,
Î¶ := Ïƒ 2
n
T Ïƒ 2s
.
(18.34)
This is called the â€œridgeâ€ regression estimator, as it amounts to adding weight on the
diagonal of the sample covariance matrix (HHT )/T . This can also be seen as a shrinkage
of the covariance matrix towards the identity, as we will discuss further in Section 18.3
below.
Another way to understand what ridge regression means is to notice that Eq. (18.31)
involves the inverse of the covariance matrix (HHT )/T , which can be unstable in large
dimensions. This instability can lead to very large coefï¬cients in a. One can thus regularize
the regression problem by adding a quadratic (or L2-norm) penalty for a so the vector does
not become too big:
aridge = argmin
a
&>>y âˆ’HT a
>>2 + T Î¶ âˆ¥aâˆ¥2'
.
(18.35)
Setting Î¶ = 0 we recover the standard regression. The solution of the regularized opti-
mization problem yields exactly Eq. (18.34); it is often called the Tikhonov regularization.
Note that the resulting equation for aridge remains well deï¬ned even when q â‰¥1 as long as
Î¶ > 0.
In both approaches (Bayesian and Tikhonov regularization) the result depends on the
choice of the parameter Î¶ = Ïƒ 2
n /(T Ïƒ 2
s ) which is hard to estimate a priori. The modern way
of ï¬xing Î¶ in practical applications is by using a validation (or cross-validation) method.
The idea is to ï¬nd the value of aridge on part of the data (the â€œtraining setâ€) and measure the
quality of the regression on another, non-overlapping part of the data (the â€œvalidation setâ€).
The value of Î¶ is then chosen as the one that gives the lowest error on the validation set.
2 This assumption relies on our hypothesis that the covariance matrix C of the xâ€™s is the identity matrix. Otherwise, the
eigenvectors of C could be used to construct non-rotationally invariant priors.
3 We have introduced a factor of 1/T in the deï¬nition of Î¶ so it parameterizes the shift in the normalized covariance matrix
(HHT )/T . It turns out to be the proper scaling in the large N limit with q = N/T ï¬xed. Note that if the elements of a and H
are of order one, the variance of the elements of HT a is of order N; for the noise to contribute signiï¬cantly in the large N limit
we must have Ïƒ2n of order N and hence Î¶ of order 1.

18.2 Estimating a Vector: Ridge and LASSO
291
In cross-validation, the procedure is repeated with multiple validation sets (always disjoint
from the training set) and the error is then averaged over these sets.
18.2.2 LASSO
Another common estimating method for vectors is the â€œlassoâ€ method4 which combines a
Laplace prior with the map estimator.
In this method, the prior distribution amounts to assuming that the coefï¬cients of a are
iid Laplace random number with variance 2bâˆ’2. The posterior then becomes
P(a|y) âˆexp

âˆ’b
N

i=1
|ai| âˆ’
1
2Ïƒ 2n
>>y âˆ’HT a
>>2

.
As in the toy model Eq. (18.18), the mmse and mave estimators look rather ugly, but the map
one is quite simple. It is given by the maximum of the argument of the above exponential:
alasso = argmin
a
-
2bÏƒ 2
n
N

i=1
|ai| +
>>y âˆ’HT a
>>2
.
.
(18.36)
This minimization amounts to regularizing the standard regression estimation with an abso-
lute value penalty (also called L1-norm penalty), instead of the quadratic penalty for the
ridge regression. Interestingly, the solution to this minimization problem leads to a sparse
estimator: the absolute value penalty strongly disfavors small values of |ai| and prefers
to set these values to zero. Only sufï¬ciently relevant coefï¬cients ai are retained â€“ lasso
automatically selects the salient factors (this is the â€œsoâ€ part in lasso), which is very useful
for interpreting the regression results intuitively.
Note that the true vector a is not sparse, as the probability to ï¬nd a coefï¬cient ai to
be exactly zero is itself zero for the prior Laplace distribution, which does not contain a
singular Î´(a) peak. The sparsity of the lasso estimator alasso is controlled by the param-
eter b. When bÏƒ 2
n â†’0, the penalty disappears and all the coefï¬cients of the vector a are
non-zero (barring exceptional cases). When bÏƒ 2
n â†’âˆ, on the other hand, all coefï¬cients
are zero. In fact, the number of non-zero coefï¬cients is a monotonic decreasing function of
bÏƒ 2
n . As for the parameter Î¶ for the ridge regression, it is hard to come up with a good prior
value for b, which should be estimated again using validation or cross-validation methods
(Figure 18.3). Finally we note that it is sometimes useful to combine the L1 penalty of
lasso with the L2 penalty of ridge, the resulting estimator is called an elastic net.
18.2.3 In-Sample and Out-of-Sample Error
Standard linear regression is built to minimize the sum of the squared-residuals on the
dataset at hand. We call this error the in-sample error. In many cases, we are interested in
4 lasso stands for Least Absolute Shrinkage and Selection Operator.

292
Bayesian Estimation
0.0
0.5
1.0
1.5
2.0
b
500
1000
1500
2000
2
0.0
0.2
0.4
0.6
0.8
1.0
fzero
2
out
2
in
fzero
Figure 18.3 Illustration of the validation method in lasso regularization. We built a linear model
with 500 coefï¬cients drawn from a Laplace distribution with b = 1 and Gaussian noise Ïƒ 2n = 1000.
The model is estimated using T = 1000 unit Gaussian data and validated using a different set of
the same size. The error on the training set is minimal with no regularization and gets worse as b
increases (dashed line, left axis). The validation error (full line, left axis) is minimal for b about equal
to 1. The dotted line (right axis) shows the fraction of the coefï¬cient estimated to be exactly zero;
this number grows from zero (no regularization) to almost 1 (strong regularization).
the predictive power of a linear model and the relevant error is the mean square error on a
new independent but statistically equivalent dataset: the out-of-sample error. If the number
of ï¬tted variables (degree of freedom) is small with respect to the number of samples,
the in-sample error is a good estimator of the out-of-sample error and the standard linear
regression is also the optimal linear predictive model. The situation changes radically when
the number of ï¬tted variables becomes comparable to the number of samples, as we discuss
below.
To summarize, linear regression, regularized or not, addresses two types of task:
â€¢ In-sample estimator: we observe some H1 and y1, and estimate a.
â€¢ Out-of-sample prediction: we observe some other H2, non-overlapping with H1, and
use them to predict y2 with the in-sample estimate of a.
The result of the in-sample estimation is given by Eq. (18.31), which we write as
areg = Eâˆ’1b;
E := 1
T H1HT
1,
b := 1
T H1y1.
(18.37)
This is the best in-sample estimator. However, this is not necessarily the case for the out-
of-sample prediction.
Note that both the standard regression and the ridge regression estimator (Eq. (18.34))
are of the form Ë†a = âˆ’1b with  = E and  = E + Î¶1, respectively. We will compute

18.2 Estimating a Vector: Ridge and LASSO
293
in the following the in-sample and out-of-sample estimation error for any estimator of
that nature.
Recalling that E[ÎµÎµT ] = Ïƒ 2
n 1 and after some calculations, one ï¬nds that the in-sample
(R2
in) error is given by
R2
in(Ë†a) = 1
T
<
Ïƒ 2
n
&
T âˆ’2 Tr(âˆ’1E) + Tr(âˆ’1Eâˆ’1E)
'
+ aT 
E âˆ’2Eâˆ’1E + Eâˆ’1Eâˆ’1E

a
=
.
(18.38)
In the special case  = E, we have
R2
in(areg) = Ïƒ 2
n
T (T âˆ’N) = (1 âˆ’q)Ïƒ 2
n,
which is smaller than the true error, which is simply equal to Ïƒ 2
n . In fact, the error goes
to zero as q â†’1, i.e. when the number of parameters becomes equal to the number of
observations. This error reduction is called â€œoverï¬ttingâ€, or in-sample bias: if the task is
to ï¬nd the best model that explains past data, one can do better than the true error. Note
that the above result is quite special, in the sense that it actually does not depend on either
E or a.
Next we calculate the expected out-of-sample (R2
out) error. We draw another matrix H2
of size N Ã— T2 and consider another independent noise vector Îµ2 of variance Ïƒ 2
n and size
T2 (where T2 does not need to be equal to T , it can even be equal to 1). We calculate
R2
out(Ë†a) = 1
T2
EH2,Îµ2
&>>HT
2a + Îµ2 âˆ’HT
2 Ë†a
>>2'
= 1
T2
EH2,Îµ2
->>>>HT
2a + Îµ2 âˆ’HT
2âˆ’1E1a âˆ’HT
2âˆ’1 1
T H1Îµ1
>>>>
2.
,
(18.39)
where we denote E1 := T âˆ’1H1HT
1. We now assume that T âˆ’1
2
E[H2HT
2] = C with a general
covariance C.
In the standard regression case,  = E and Ë†a = areg and we have
R2
out(areg) = 1
T2
EH2,Îµ2
->>>>Îµ2 âˆ’HT
2 âˆ’1 1
T H1Îµ1
>>>>
2.
= Ïƒ 2
n + Ïƒ 2
n
T Tr(Eâˆ’1C).
(18.40)
Now since E is a sample covariance matrix with true covariance C, we have
Tr(Eâˆ’1C) = Tr

Câˆ’1
2 Wâˆ’1
q Câˆ’1
2 C

= Tr(Wâˆ’1
q ) â‰ˆ
N
1 âˆ’q,
(18.41)
where Wq denotes a standard Wishart matrix. Thus we ï¬nd
R2
out(areg) = Ïƒ 2
n + qÏƒ 2
n
1 âˆ’q =
Ïƒ 2
n
1 âˆ’q = R2
in(areg)
(1 âˆ’q)2 .
(18.42)

294
Bayesian Estimation
As an illustration see Figure 18.3 where without regularization (b = 0) we have indeed
R2
out/R2
in â‰ˆ(1 âˆ’q)âˆ’2 = 4. Thus, we see that we can make precise statements about the
following intuitive inequalities:
in-sample error â‰¤true error â‰¤out-of-sample error.
(18.43)
Note that the out-of-sample error tends to âˆas N â†’T .
Now, let us compute the expected out-of-sample error (R2
out) for the ridge predictor
aridge, parameterized by Î¶. The result reads
R2
out(aridge) = Ïƒ 2
n + Ïƒ 2
n
T Tr(Câˆ’1Eâˆ’1) + Î¶ 2 Tr(Câˆ’1aaT âˆ’1),
(18.44)
with  = E + Î¶1. Expanding to linear order for small Î¶ then leads to
R2
out(aridge) = R2
out(areg) âˆ’2Ïƒ 2
n
T
Tr(CEâˆ’2)Î¶ + O(Î¶ 2)
= R2
out(areg) âˆ’
2Ïƒ 2
n q
(1 âˆ’q)3 Ï„(Câˆ’1)Î¶ + O(Î¶ 2),
(18.45)
where Ï„(.) = Tr(.)/N and we have used the fact that Ï„(Wâˆ’2
q ) = (1 âˆ’q)âˆ’3. The important
point here is that the coefï¬cient in front of Î¶ is negative, i.e. to ï¬rst order, the ridge estimator
has a lower out-of-sample error than the naive regression estimator:
ridge estimation error < naive estimation error.
(18.46)
However, the ridge estimator introduces a systematic bias since âˆ¥aridgeâˆ¥2 < âˆ¥aregâˆ¥2 when
Î¶ > 0. This gives the third term in Eq. (18.44), which becomes large for larger Î¶. So one
indeed expects that there should exist an optimal value of Î¶ (which depends on the speciï¬c
problem at hand) which minimizes the out-of-sample error. We now show how this optimal
out-of-sample error can be elegantly computed in the large N limit.
The Large N Limit
In the large N limit we can recover the fact that the ridge estimator of Section 18.2.1
minimizes the out-of-sample risk without the need of a Gaussian prior on a. We will also
ï¬nd an interesting relation between the Wishart Stieltjes transform and the out-of-sample
risk of the ridge estimator.
In the following we will assume that the elements of the out-of-sample data H2 are iid
with unit variance, i.e. that C = 1. Then when  = E1 + Î¶1, Eq. (18.44) becomes, in the
large N limit,
R2
out(aridge) = Ïƒ 2
n

1 âˆ’qgWq (âˆ’Î¶)

+ Î¶

qÏƒ 2
n âˆ’Î¶|a|2
gâ€²
Wq (âˆ’Î¶),
(18.47)
where we have used that E1 is a Wishart matrix free from aaT , and that
Ï„((z1 âˆ’E1)âˆ’1) = gWq (z);
Ï„((z1 âˆ’E1)âˆ’2) = âˆ’gâ€²
Wq (z).
(18.48)
For Î¶ = 0, we have gWq (0) = âˆ’1/(1 âˆ’q) and we thus recover Eq. (18.42). In the
large N limit, the out-of-sample error for an estimator with  = E1 + Î¶1 depends on the

18.3 Bayesian Estimation of the True Covariance Matrix
295
vector a only through its norm |a|2, regardless of the distribution of its components. The
optimal value of Î¶ must then also only depend on |a|2.
Now, we know that when a is drawn from a Gaussian distribution, the value Î¶opt =
Ïƒ 2n /(T Ïƒ 2s ) is optimal. In the large N limit, |a|2 is self-averaging and equal to NÏƒ 2s . So
Î¶opt = qÏƒ 2n /|a|2. We can check directly that this value is optimal by computing the
derivative of Eq. (18.47) with respect to Î¶ evaluated at Î¶opt. Indeed we have
Ïƒ 2
n qgâ€²
Wq (âˆ’Î¶opt) âˆ’Î¶opt|a|2gâ€²
Wq (âˆ’Î¶opt) = 0.
(18.49)
For the optimal value of Î¶ we also have
R2
out(aridge) = Ïƒ 2
n

1 âˆ’qgWq (âˆ’Î¶opt)

,
(18.50)
where gWq (z) is given by Eq. (4.40). Since âˆ’gWq (âˆ’z) is positive and monotonically
decreasing for z > 0, we recover that the optimal ridge out-of-sample error is smaller than
that of the standard regression.
18.3 Bayesian Estimation of the True Covariance Matrix
We now apply the Bayesian estimation method to covariance matrices. From empirical data,
we measure the sample covariance matrix E, and want to infer the most reliable information
about the â€œtrueâ€ underlying covariance matrix C. Hence we write Bayesâ€™ equation for
conditional probabilities for matrices:
P(C|E) âˆP(E|C)P0(C).
(18.51)
We now recall Eq. (4.16) established in Chapter 4 for Gaussian observations:
P (E|C) âˆ(det C)âˆ’T/2 exp
(
âˆ’T
2 Tr

Câˆ’1E
)
.
(18.52)
As explained in Section 18.1.3, in the absence of any meaningful prior information, it is
interesting to pick a conjugate prior, which here is of the form
P0 (C) âˆ(det C)a exp
&
âˆ’b Tr

Câˆ’1X
'
(18.53)
for some matrix X, which turns out to be proportional to the prior mean of C. Indeed,
this prior is in fact the probability density of the elements of an inverse-Wishart matrix.
Consider an inverse-Wishart matrix C of size N, T âˆ—degree of freedom and centered at a
(positive deï¬nite) matrix X. If T âˆ—> N + 1, C has the density (see Eq. (15.35))
P(C) âˆ(det C)âˆ’(T âˆ—+N+1)/2 exp
(
âˆ’T âˆ—âˆ’N âˆ’1
2
Tr

Câˆ’1X
)
.
(18.54)
Note that here T âˆ—> N is some parameter that is unrelated to the length of the time series
T . The chosen normalization is such that E0[C] = X. As T âˆ—â†’âˆ, we have C â†’X.
With this prior we thus obtain
P(C|E) âˆ(det C)âˆ’(T +T âˆ—+N+1)/2 exp
(
âˆ’T
2 Tr

Câˆ’1Eâˆ—)
,
(18.55)

296
Bayesian Estimation
where we deï¬ne
Eâˆ—:= E + T âˆ—âˆ’N âˆ’1
T
X.
(18.56)
We now notice that (18.55) is, by construction, also a probability density for an inverse-
Wishart with Â¯T = T + T âˆ—, with
E[C|E] =
T Eâˆ—
T + T âˆ—âˆ’N âˆ’1 = rE + (1 âˆ’r)X,
(18.57)
with
r =
T
T + T âˆ—âˆ’N âˆ’1.
(18.58)
Hence we recover a linear shrinkage, similar to Eq. (18.10) in the case of a scalar variable
with a Gaussian prior. We will recover this shrinkage formula in the context of rotationally
invariant estimators in the next chapter, see Eq. (19.49).
We end with the following remarks:
â€¢ The linear shrinkage works even for the ï¬nite N case, i.e. without the large N hypothesis.
â€¢ In general, if one has no idea of what X should be, one can use the identity matrix, i.e.
E[C|E] = rE + (1 âˆ’r)1.
(18.59)
Another simple choice is a covariance matrix X corresponding to a one-factor model (see
Section 20.4.2):
Xij = Ïƒ 2
s

Î´ij + Ï(1 âˆ’Î´ij)

,
(18.60)
where Ï is the average pairwise correlation (which can also be learned using validation).
â€¢ Note that T âˆ—(or equivalently r) is generally unknown. It may be inferred from the data
or learned using validation.
â€¢ As we will see in Chapter 20, the linear shrinkage works quite well in ï¬nancial applica-
tions, showing that inverse-Wishart is not a bad prior for the true covariance matrix in
that case (see Fig. 15.1).
Bibliographical Notes
â€¢ Some general references on Bayesian methods and statistical inference:
â€“ E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University Press,
Cambridge, 2003,
â€“ G. James, D. Witten, T. Hastie, and R. Tibshirani. An Introduction to Statistical
Learning: with Applications in R. Springer, New York, 2013.

19
Eigenvector Overlaps and Rotationally Invariant
Estimators
19.1 Eigenvector Overlaps
19.1.1 Setting the Stage
We saw in the ï¬rst two parts of this book how tools from rmt allow one to infer many
properties of the eigenvalue distribution, encoded in the trace of the resolvent of the random
matrix under scrutiny. As in the previous chapters, random matrices of particular interest
are of the form
E = C + X,
or
E = C
1
2 WC
1
2,
(19.1)
where X and W represent some â€œnoiseâ€, for example X might be a Wigner matrix in the
additive case and W a white Wishart matrix in the multiplicative case, whereas C is the
â€œtrueâ€, uncorrupted matrix that one would like to measure. One often calls E the sample
matrix and C the population matrix.
In this section we want to discuss the properties of the eigenvectors of E, and in particu-
lar their relation with the eigenvectors of C. There are, at least, two natural questions about
the eigenvectors of the sample matrix E:
1 How similar are sample eigenvectors [vi]iâˆˆ(1,N) of E and the true ones [ui]iâˆˆ(1,N) of C?
2 What information can we learn by observing two independent realizations â€“ say
E = C
1
2 WC
1
2 and Eâ€² = C
1
2 Wâ€²C
1
2 in the multiplicative case â€“ that remain correlated
through C?
A natural quantity to characterize the similarity between two arbitrary vectors â€“ say Ï‡
and Î¶ â€“ is the scalar product of Ï‡ and Î¶. More formally, we deï¬ne the â€œoverlapâ€ as Ï‡ T Î¶.
Since the eigenvectors of real symmetric matrices are only deï¬ned up to a sign, it is in
fact more natural to consider the squared overlaps (Ï‡ T Î¶)2. In the ï¬rst problem alluded to
above, we want to understand the relation between the eigenvectors of the sample matrix
[vi]iâˆˆ(1,N) and those of the population matrix [ui]iâˆˆ(1,N). The matrix of squared overlaps
is deï¬ned as (vi
T uj)2, which actually forms a so-called bi-stochastic matrix (positive
elements with the sums over both rows and columns all equal to unity).
297

298
Eigenvector Overlaps and Rotationally Invariant Estimators
In order to study these overlaps, the central tool is again the resolvent matrix (and not
its normalized trace as for the Stieltjes transform), which we recall is deï¬ned as
GA(z) = (z1 âˆ’A)âˆ’1 ,
(19.2)
for any arbitrary symmetric matrix A. Now, if we expand GE over the eigenvectors v of E,
we obtain that
uT GE(z)u =
N

i=1

uT vi
2
z âˆ’Î»i
,
(19.3)
for any u in RN.
We thus see from Eq. (19.3) that each pole of the resolvent deï¬nes a projection onto
the corresponding sample eigenvectors. This suggests that the techniques we need to apply
are very similar to the ones used above to study the density of states. However, one should
immediately stress that contrarily to eigenvalues, each eigenvector vi for any given i contin-
ues to ï¬‚uctuate when N â†’âˆand never reaches a deterministic limit. As a consequence,
we will need to introduce some averaging procedure to obtain a well-deï¬ned result. We
will thus consider the following quantity:
(Î»i,Î¼j) := NE[(vT
i uj)2],
(19.4)
where the expectation E can be interpreted either as an average over different realizations
of the randomness or, perhaps more meaningfully for applications, as an average for a
ï¬xed sample over small intervals of sample eigenvalues, of width dÎ» = Î·. We choose Î· in
the range 1 â‰«Î· â‰«Nâˆ’1 (say Î· = Nâˆ’1/2) such that there are many eigenvalues in the
interval dÎ», while keeping dÎ» sufï¬ciently small for the spectral density to be approximately
constant. Interestingly, the two procedures lead to the same result for large matrices, i.e.
the locally smoothed quantity (Î»,Î¼) is â€œself-averagingâ€. A way to do this smoothing
automatically is, as we explained in Chapter 2, to choose z = Î»i âˆ’iÎ· in Eq. (19.3),
leading to
Im uT
j GE(Î»i âˆ’iÎ·)uj â‰ˆÏ€ÏE(Î»i) Ã— (Î»i,Î¼j),
(19.5)
provided Î· is in the range 1 â‰«Î· â‰«Nâˆ’1. Note that we have replaced N

uT
j vi
2 with
(Î»i,Î¼j), to emphasize the fact that we expect typical square overlaps to be of order 1/N,
such that  is of order unity when N â†’âˆ. This assumption will indeed be shown to hold
below. In fact, when uj and vi are completely uncorrelated, one ï¬nds (Î»i,Î¼j) = 1.
For the second question, the main quantity of interest is, similarly, the (mean squared)
overlap between the eigenvectors of two independent noisy matrices E and Eâ€²:
(Î»i,Î»â€²
j) := NE[(vT
i vâ€²
j)2],
(19.6)
where [Î»â€²
i]iâˆˆ(1,N) and [vâ€²
i]iâˆˆ(1,N) are the eigenvalues and eigenvectors of Eâ€², i.e. another
sample matrix that is independent from E but with the same underlying population

19.1 Eigenvector Overlaps
299
matrix C. In order to get access to (Î»i,Î»â€²
j) deï¬ned in Eq. (19.5), one should consider
the following quantity:
Ïˆ(z,zâ€²) = 1
N Tr

GE(z)GEâ€²(zâ€²)

.
(19.7)
After simple manipulations one readily obtains a generalized Sokhotskiâ€“Plemelj formula,
where Î· is such that 1 â‰«Î· â‰«Nâˆ’1:
Re

Ïˆ(Î»i âˆ’iÎ·,Î»â€²
i + iÎ·) âˆ’Ïˆ(Î»i âˆ’iÎ·,Î»â€²
i âˆ’iÎ·)

â‰ˆ2Ï€2ÏE(Î»i)ÏEâ€²(Î»â€²
i)(Î»i,Î»â€²
j). (19.8)
This representation allows one to obtain interesting results for the overlaps between the
eigenvectors of two independently drawn random matrices, see Eq. (19.14).
19.1.2 Overlaps in the Additive Case
Now, we can use the subordination relation for the resolvent of the sum of two free matrices
established in Chapter 13, Eq. (13.44), which in the present case reads
E[GE(z)] = GC (z âˆ’RX(gE(z))) .
(19.9)
Since we choose uj to be an eigenvector of C with eigenvalue Î¼j, one ï¬nds
uT
j GE(Î»i âˆ’iÎ·)uj =
1
Î»i âˆ’iÎ· âˆ’RX (gE(Î»i âˆ’iÎ·)) âˆ’Î¼j
,
(19.10)
where we have dropped the expectation value as the left hand side is self-averaging when Î·
is in the correct range. The imaginary part of this quantity, calculated for Î· â†’0, gives
access to (Î»i,Î¼j). The formula simpliï¬es in the common case where the noise matrix X
is a Wigner matrix, such that RX(z) = Ïƒ 2z. In this case, one ï¬nally obtains a Lorentzian
shape for the squared overlaps:
(Î»,Î¼) =
Ïƒ 2
(Î¼ âˆ’Î» + Ïƒ 2 hE(Î»))2 + Ïƒ 4Ï€2ÏE(Î»)2,
(19.11)
where we have decomposed the Stieltjes transform into its real and imaginary parts as
gE(x) = hE(x) + iÏ€ÏE(x); note that hE(x) is equal to Ï€ times the Hilbert transform of
ÏE(x).
In Figure 19.1, we illustrate this formula in the case where C is a Wigner matrix with
parameter Ïƒ 2 = 1. For a ï¬xed Î», the overlap peaks for
Î¼ = Î» âˆ’Ïƒ 2 hE(Î»),
(19.12)
with a width âˆ¼Ïƒ 2ÏE(Î»). When Ïƒ â†’0, i.e. in the absence of noise, one recovers
(Î»,Î¼) â†’Î´(Î» âˆ’Î¼),
(19.13)
as expected since in this case the eigenvectors of E are trivially the same as those of C.
Note that apart from the singular case Ïƒ = 0, (Î»,Î¼) is found to be of order unity when
N â†’âˆ. But because of the factor N in Eq. (19.6), the overlaps between vi and uj are of
order Nâˆ’1/2 as soon as Ïƒ > 0.

300
Eigenvector Overlaps and Rotationally Invariant Estimators
âˆ’3
âˆ’2
âˆ’1
0
1
2
3
l
0
1
2
3
4
5
6
F(l, m)
simulation m = âˆ’1.5
simulation m = 0.5
theory m = âˆ’1.5
theory m = 0.5
Figure 19.1 Normalized squared-overlap function (Î»,Î¼) for E = X1 + X2, the sum of two unit
Wigner matrices, compared with a numerical simulation for Î¼ = âˆ’1.5 and Î¼ = 0.5. The simulations
are for a single sample of size N = 2000, each data point corresponds to N times the square overlap
between an eigenvector of E and one of X1 averaged over eigenvectors with eigenvalues within
distance Î· = 2/
âˆš
N of Î¼.
Now suppose that E = C + X and Eâ€² = C + Xâ€², where X and Xâ€² are two independent
Wigner matrices with the same variance Ïƒ 2. Using Eq. (19.8), one can compute the
expected overlap between the eigenvectors of E and Eâ€². After a little work, one can estab-
lish the following result for (Î»,Î»), i.e. the typical overlap around the same eigenvalues
for E and Eâ€²:
(Î»,Î») =
Ïƒ 2
2f2(Î»)2
âˆ‚Î»f1(Î»)
(âˆ‚Î»f1(Î»))2 + (âˆ‚Î»f2(Î»))2,
(19.14)
where
f1(Î») = Î» âˆ’Ïƒ 2 hE(Î»);
f2(Î») = Ïƒ 2Ï€ÏE(Î»);
hE(Î») := Re[gE(Î»)].
(19.15)
Note that in the large N limit, gE(z) = gEâ€²(z).
The formula for (Î»,Î»â€²) is more cumbersome; for a ï¬xed Î»â€², one ï¬nds again a humped
shaped function with a maximum at Î»â€² â‰ˆÎ». The most striking aspect of this formula,
however, is that only gE(z) (which is measurable from data) is needed to compute the
expected overlap (Î»,Î»â€²); the knowledge of the â€œtrueâ€ matrix C is not needed to judge
whether or not the observed overlap between the eigenvectors of E and Eâ€² is compatible
with the hypothesis that such matrices are both noisy versions of the same unknown C.
19.1.3 Overlaps in the Multiplicative Case
We now repeat the same steps in the case where E = C
1
2 WqC
1
2 , where Wq is a Wishart
matrix of parameter q. We know that in this case the matrix subordination formula reads as
(13.47), which can be rewritten as

19.2 Rotationally Invariant Estimators
301
GE(z) = Z(z)
z
GC(Z(z)),
with
Z =
z
1 âˆ’q + qzgE(z).
(19.16)
This allows us to compute
uT
j GE(Î»i âˆ’iÎ·)uj = Z(Î»i âˆ’iÎ·)
Î»i âˆ’iÎ·
1
Z(Î»i âˆ’iÎ·) âˆ’Î¼j
,
(19.17)
and ï¬nally, taking the imaginary part in the limit Î· â†’0+,
(Î»,Î¼) =
qÎ¼Î»
(Î¼(1 âˆ’q) âˆ’Î» + qÎ¼Î» hE(Î»))2 + q2Î¼2Î»2Ï€2Ï2
E(Î»),
(19.18)
where, again, hE denotes the real part of the Stieltjes transform gE. Note that in the limit
q â†’0, (Î»,Î¼) becomes more and more peaked around Î» â‰ˆÎ¼, with an amplitude
that diverges for q = 0. Indeed, in this limiting case, one should ï¬nd that the sample
eigenvectors vi become equal to the population ones ui. More generally, (Î»,Î¼) for a
ï¬xed Î¼ has a Lorentzian humped shape as a function of Î», which peaks for Î» â‰ˆÎ¼.
Now suppose that E = C
1
2 WqC
1
2 and Eâ€² = C
1
2 Wâ€²qC
1
2 , where Wq and Wâ€²q are two
independent Wishart matrices with the same parameter q. Using Eq. (19.8), one can again
compute the expected overlap between the eigenvectors of E and Eâ€². The ï¬nal formula is
however too cumbersome to be reported here, see Bun et al. [2018]. The formula simpliï¬es
in the limit where C is close to the identity matrix, in the sense that Ï„(C2) = 1 + Ïµ, with
Ïµ â†’0. In this case:
(Î»,Î»â€²) = 1 + Ïµ [2 hE(Î») âˆ’1] 
2 hE(Î»â€²) âˆ’1

+ O(Ïµ2).
(19.19)
More generally, the squared overlaps only depend on gE(z) (which is measurable from
data). Again, the knowledge of the â€œtrueâ€ matrix C is not needed to judge whether or not
the observed overlap between the eigenvectors of E and Eâ€² is compatible with the hypoth-
esis that such matrices are both noisy versions of the same unknown C. This is particularly
important in ï¬nancial applications, where E and Eâ€² may correspond to covariance matrices
measured on two non-overlapping periods. In such a case, the hypothesis that the true C is
indeed the same in the two periods may not be warranted and can be directly tested using
the overlap formula.
19.2 Rotationally Invariant Estimators
19.2.1 Setting the Stage
The results derived above concerning the overlaps between the eigenvectors of sample
E and population (or â€œtrueâ€) C matrices allow one to construct a rotationally invariant
estimator of C knowing E. The idea can be framed within the Bayesian approach of the
previous chapter, when the prior knowledge about C is mute about the possible directions
in which the eigenvectors of C are pointing. More formally, this can be expressed by saying
that the prior distribution P0(C) is rotation invariant, i.e.
P0(C) = P0(OCOT ),
(19.20)

302
Eigenvector Overlaps and Rotationally Invariant Estimators
where O is an arbitray rotation matrix. Examples of rotationally invariant priors are pro-
vided by the orthogonal ensemble introduced in Chapter 5, where P0(C) only depends on
Tr(C).
Now, since the posterior probability of C given E is given by
P(C|E) âˆ(det C)âˆ’T/2 exp
(
âˆ’T
2 Tr

Câˆ’1E
)
P0(C),
(19.21)
it is easy to verify that the mmse estimator of C transforms in the same way as E under an
arbitrary rotation O, i.e.
E[C|OEOT ] =

C P(C|OEOT )P0(C)DC
= O
(
C P(C|E)P0(C)DC
)
OT
= OE(C|E)OT,
(19.22)
using the change of variable C = OT CO, and the explicit form of P(C|E) given in Eq.
(19.21).
More generally, if we call (E) an estimator of C given E, this estimator is rotationally
invariant if and only if
(OEOT ) = O(E)OT,
(19.23)
for any orthogonal matrix O. This means in words that, if the scm E is rotated by some O,
then our estimation of C must be rotated in the same fashion. Intuitively, this is because we
have no prior assumption on the eigenvectors of C, so the only special directions in which
C can point are those singled out by E itself. Estimators abiding by Eq. (19.23) are called
rotationally invariant estimators (rie).
An alternative interpretation of Eq. (19.23) is that (E) can be diagonalized in the same
basis as E, up to a ï¬xed rotation matrix  . But consistent with our rotationally invariant
prior on C, there is no natural guess for  , except the identity matrix 1. Hence we conclude
that (E) has the same eigenvectors as those of E, and write
(E) =
N

i=1
Î¾ivivT
i ,
(19.24)
where vi are, as above, the eigenvectors of E, and where Î¾i is a function of all empirical
eigenvalues [Î»j]jâˆˆ(1,N). We now show how these Î¾i can be optimally chosen, and opera-
tionally computed from data in the limit N â†’âˆ.
19.2.2 The Optimal RIE
Suppose we ask the following question: what is the optimal choice of Î¾i such that (E),
deï¬ned by Eq. (19.24), is as close as possible to the true C? If the eigenvectors of E were

19.2 Rotationally Invariant Estimators
303
equal to those of C, i.e. if vi = ui, âˆ€i, the solution would trivially be Î¾i = Î¼i. But in
the case where vi  ui, the solution is a priori non-trivial. So we want to minimize the
following least-square error:
Tr((E) âˆ’C)2 =
N

i=1
vT
i ((E) âˆ’C)2vi =
N

i=1

Î¾2
i âˆ’2Î¾ivT
i Cvi + vT
i C2vi

.
(19.25)
Minimizing over Î¾k and noting that the third term in the equation above is independent of
the Î¾â€™s, it is easy to get the following expression for the optimal Î¾k:
Î¾k = vT
kCvk.
(19.26)
This is all very well but seems completely absurd: we assume that we do not know the true
C and want to ï¬nd the best estimator of C knowing E, and we ï¬nd an equation for the Î¾
that we cannot compute unless we know C.
Because Eq. (19.26) requires in principle knowledge we do not have, it is often called
the â€œoracleâ€ estimator. But as we will see in the next section, the large N limit allows one
to actually compute the optimal Î¾â€™s from the data alone, without having to know C.
19.2.3 The Large Dimension Miracle
Let us ï¬rst rewrite Eq. (19.26) in terms of the overlaps introduced in Section 19.1. Expand-
ing over the eigenvectors of C we ï¬nd
Î¾k =
N

j=1
vT
kujÎ¼juT
j vk =
N

j=1
Î¼j

uT
j vk
2
(19.27)
âˆ’â†’
Nâ†’âˆ

dÎ¼ ÏC(Î¼)Î¼(Î»k,Î¼),
(19.28)
where Î»k is the eigenvalue of the sample matrix E associated with vk. In other words, Î¾k
is an average over the eigenvalues of C, weighted by the square overlaps (Î»k,Î¼). Now,
using Eq. (19.5), we can also write
Î¾k =
N

j=1
Î¼j

uT
j vk
2
=
1
Ï€ÏE(Î»k) lim
Î·â†’0+ Im
N

j=1
uT
j Î¼jGE(Î»k âˆ’iÎ·)uj
=
1
Ï€ÏE(Î»k) lim
Î·â†’0+ Im Ï„ (CGE(Î»k âˆ’iÎ·)) .
(19.29)
We now use the fact that in both the additive and the multiplicative case, the matrices GE
and GC are related by a subordination equation of the type
GE(z) = Y(z)GC(Z(z)),
(19.30)

304
Eigenvector Overlaps and Rotationally Invariant Estimators
with Y(z) = 1 in the additive case and Y(z) = Z(z)/z in the multiplicative case. Hence we
can write the following series of equalities:
Ï„ (CGE(z)) = YÏ„ (CGC(Z)) = YÏ„

(C âˆ’Z1 + Z1)(Z1 âˆ’C)âˆ’1
= YZgC(Z) âˆ’Y = Z(z)gE(z) âˆ’Y(z).
(19.31)
But since Z(z) only depend on gE(z), we see that the ï¬nal formula for Î¾k does not explicitly
depend on C anymore and reads
Î¾k =
1
Ï€ÏE(Î»k) lim
Î·â†’0+ Z(zk)gE(zk) âˆ’Y(zk),
zk := Î»k âˆ’iÎ·.
(19.32)
Since all the quantities on the right hand side can be estimated from the data alone, this
formula will lend itself to real world applications. Let us ï¬rst explore this formula for two
simple cases, for an additive model and for a multiplicative model.
19.2.4 The Additive Case
For a general noise matrix X, one has Z(z) = z âˆ’RX(gE(z)), leading to the following
mapping between the empirical eigenvalues Î» and the rie eigenvalues Î¾:
Î¾(Î») = Î» âˆ’limÎ·â†’0+ Im RX(gE(z))gE(z)
limÎ·â†’0+ Im gE(z)
,
z = Î» âˆ’iÎ·.
(19.33)
If there is no noise, i.e. X = 0 and hence RX = 0, we ï¬nd as expected Î¾(Î») = Î». If X is
small, then
RX(x) = Ïµx + Â· Â· Â· ,
(19.34)
where we have assumed Ï„(X) = 0 and Ïµ = Ï„(X2) is small. Hence we ï¬nd
Î¾(Î») = Î» âˆ’2Ïµ hE(Î») + Â· Â· Â· .
(19.35)
A natural case to consider is when X is Wigner noise, for which RX(x) = Ïƒ 2
n x exactly,
such that the equation above is exact with Ïµ = Ïƒ 2
n , for arbitrary values of Ïƒn. When C
is another Wigner matrix with variance Ïƒ 2
s , then E is clearly also a Wigner matrix with
variance Ïƒ 2 = Ïƒ 2
n + Ïƒ 2
s . In this case, when âˆ’2Ïƒ < Î» < 2Ïƒ,
hE(Î») =
Î»
2Ïƒ 2 .
(19.36)
Hence we obtain, from Eq. (2.38),
Î¾(Î») = Î» âˆ’Î»Ïƒ 2
n
Ïƒ 2 = rÎ»,
r := Ïƒ 2
s
Ïƒ 2,
(19.37)

19.2 Rotationally Invariant Estimators
305
which is the linear shrinkage obtained for Gaussian variables in Chapter 18. In fact, this
shrinkage formula is expected elementwise, since all elements are Gaussian random vari-
ables:1
(E)ij = r Eij,
(19.38)
see Eq. (18.10) with x0 = 0.
Exercise 19.2.1
Additive RIE for the sum of two matrices from the same distri-
bution
In this exercise we will ï¬nd a simple form for a rie estimator when the noise
is drawn from the same distribution as the signal, i.e.
E = C + X,
(19.39)
with X and C mutually free matrices drawn from the same ensemble.
(a)
Write a relationship between RX(g) and RE(g).
(b)
Given that gE(z)RE(gE(z)) = zgE(z) âˆ’1, what is gE(z)RX(gE(z))?
(c)
Use Eq. (19.33) and the fact that z is real in the limit Î· â†’0+ to show that
Î¾(Î») = Î»/2.
(d)
Given that  = E[C]E (see Section 19.4), ï¬nd a simple symmetry argument
to show that  = E/2.
(e)
Generate numerically two independent symmetric orthogonal matrices M1
and M2 with N = 1000 (see Exercise 1.2.4). Compute the eigenvalues Î»k
and eigenvectors vk of the sum of these two matrices.
(f)
Plot the normalized histogram of the Î»kâ€™s and compare with the arcsine law
between âˆ’2 and 2 (Ï(Î») = 1/(Ï€
âˆš
4 âˆ’Î»2)).
(g)
Make a scatter plot vT
kM1vk vs Î»k and compare with Î»/2.
19.2.5 The Multiplicative Case
We can now tackle the multiplicative case, which includes the important practical problem
of estimating the true covariance matrix given a sample covariance matrix. In the mul-
tiplicative case, it is more elegant to use the subordination relation Eq. (13.46) for the
T-matrix rather than for the resolvent. In the present setting we thus write
TE(z) = TC[z SW(tE(z))].
(19.40)
1 Note however that there is a slight subtlety here: the linear shrinkage equation (19.37) only holds in the absence of outliers, i.e.
empirical eigenvalues that fall outside the interval (âˆ’2Ïƒ,2Ïƒ). For such eigenvalues, shrinkage is non-linear. For a similar
situation in the multiplicative case, see Figure 19.2.

306
Eigenvector Overlaps and Rotationally Invariant Estimators
In terms of T-transforms, Eq. (19.29) reads
Î¾(Î») = limÎ·â†’0+ Im Ï„(CTC[z SW(tE(z))])
limÎ·â†’0+ Im tE(z)
;
z = Î» âˆ’iÎ·.
(19.41)
Since TC(z) = C(z1 âˆ’C)âˆ’1, we have, with t = tE(z) as a shorthand,
Ï„ [CTC(zSW(t))] = Ï„
&
C2(zSW(t)1 âˆ’C)âˆ’1'
= Ï„
&
C(C âˆ’zSW(t)1 + zSW(t)1)(zSW(t)1 âˆ’C)âˆ’1'
= Ï„(C) + zSW(t)tC(zSW(t))
= Ï„(C) + zSW(t)tE(z).
(19.42)
The ï¬rst term Ï„(C) is real and does not contribute to the imaginary part that we have to
compute, so we obtain
Î¾(Î») = Î»limÎ·â†’0+ Im SW(tE(z))tE(z)
limÎ·â†’0+ Im tE(z)
,
z = Î» âˆ’iÎ·.
(19.43)
Equation (19.43) is very general. It applies to sample covariance matrices where the noise
matrix W is a white Wishart, but it also applies to more general multiplicative noise
processes.
In the special case of sample covariance matrices E = C
1
2 WqC
1
2 with N/T = q, we
know that SWq(t) = (1 + qt)âˆ’1. In the bulk region Î»âˆ’< Î» < Î»+, t = tE(z) is complex
with non-zero imaginary part when z = Î» âˆ’iÎ·. Hence
Î¾(Î») = Î»
limÎ·â†’0+ Im
t
1+qt
limÎ·â†’0+ Im t
=
Î»
|1 + qtE(Î» âˆ’iÎ·)|2

Î·â†’0
,
(19.44)
where we have used the fact that
Im
t
1 + qt = Im t(1 + qtâ‹†)
|1 + qt|2 = Im t + q|t|2
|1 + qt|2 =
1
|1 + qt|2 Im t.
(19.45)
Equation (19.44) can be interpreted as a form of non-linear shrinkage. A way to see this is
to note that below Î»âˆ’and above Î»+ (the edges of the sample spectrum) tE(Î») is real. From
the very deï¬nition,
tE(Î») =
 Î»+
Î»âˆ’
dÎ»â€²ÏE(Î»â€²)
Î»â€²
Î» âˆ’Î»â€² = Î»gE(Î») âˆ’1,
(19.46)
for any Î» outside or at the edges of the spectrum. Hence, since Î»âˆ’â‰¥0 for covariance
matrices, tE(Î»âˆ’) < 0 and tE(Î»+) > 0. Hence, one directly establishes that the support of
the rie (E) is narrower than that of E:
Î¾(Î»âˆ’) â‰¥Î»âˆ’;
Î¾(Î»+) â‰¤Î»+,
(19.47)
where the inequalities are saturated for q = 0, in which case, as expected Î¾(Î») = Î»,
âˆ€Î» âˆˆ(Î»âˆ’,Î»+). A more in-depth discussion of the properties of Eq. (19.44) is given in
Section 19.3.

19.2 Rotationally Invariant Estimators
307
0
1
2
3
4
l
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x(l)
RIE
linear
Figure 19.2 The rie estimator (19.44) for a true covariance matrix given by an inverse-Wishart of
variance p = 0.25 observed using data with aspect ratio q = 0.25. On the support of the sample
density (Î» âˆˆ[0.17,3.33]), the rie matches perfectly the linear shrinkage estimator (19.49) (with
r = 1/2), but it is different from it outside of the expected spectrum.
Using Eq. (19.46), the shrinkage equation (19.44) can be rewritten as
Î¾(Î») =
Î»
|1 âˆ’q + qÎ»gE(Î» âˆ’iÎ·)|2

Î·â†’0+ ,
(19.48)
a result ï¬rst derived in Ledoit and PÂ´echÂ´e [2011].
Equation (19.44) considerably simpliï¬es in the case where the true covariance matrix C
is an inverse-Wishart matrix of parameter p. Injecting the explicit form of tE(z) given by
Eq. (15.50) into Eq. (19.44) leads, after simple manipulations, to
Î¾(Î») = q + Î»p
p + q = rÎ» + (1 âˆ’r);
r :=
p
p + q,
(19.49)
i.e. exactly the linear shrinkage result derived in a Bayesian framework in Section 18.3.
Note that the result Eq. (19.49) only holds between Î»âˆ’and Î»+, given in Eq. (15.52). The
full function Î¾(Î») when C is an inverse-Wishart matrix is given in Figure 19.2.
Exercise 19.2.2
RIE when the true covariance matrix is Wishart
Assume that the true covariance matrix C is given by a Wishart matrix with
parameter q0. This case is a tractable model for which the computation can be
done semi-analytically (we will get cubic equations!).
We observe a sample covariance matrix E over T = qN time intervals. E is
the free product of C and another Wishart matrix of parameter q:
E = C
1
2 WC
1
2 .
(19.50)

308
Eigenvector Overlaps and Rotationally Invariant Estimators
(a)
Given that the S-transform of the true covariance is SC(t) = 1/(1 + q0t) and
the S-transform of the Wishart is SW(t) = 1/(1 + qt), use the product of
S-transforms for the free product and Eq. (11.92) to write an equation for
tE(z). It should be a cubic equation in t.
(b)
Using a numerical polynomial solver (e.g. np.roots) solve for tE(z) for z real
between 0 and 4, choose q0 = 1/4 and q = 1/2. Choose the root with positive
imaginary part. Use Eqs. (11.89) and (2.47) to ï¬nd the eigenvalue density and
plot this density. The edge of the spectrum should be (slightly below) 0.05594
and (slightly above) 3.746.
(c)
For Î» in the range [0.05594, 3.746] plot the optimal cleaning function (use the
same solution tE(z) as in (b)):
Î¾(Î») =
Î»
|1 + qt(Î»)|2 .
(19.51)
(d)
For N = 1000 numerically generate C (q0 = 1/4), two versions of W1,2
(q = 1/2) and hence two versions of E1,2 := C
1
2 W1,2C
1
2 . E1 will be the
â€œin-sampleâ€ matrix and E2 the â€œout-of-sampleâ€ matrix. Check that Ï„(C) =
Ï„(W1,2) = Ï„(E1,2) = 1 and that Ï„(C2) = 1.25, Ï„(W2
1,2) = 1.5 and
Ï„(E2
1,2) = 1.75.
(e)
Plot the normalized histogram of the eigenvalues of E1, it should match your
plot in (b).
(f)
For every eigenvalue, eigenvector pair (Î»k,vk) of E1 compute Î¾val(Î»k) :=
vT
kE2vk. Plot Î¾val(Î»k) vs Î»k and compare with your answer in (c).
Exercise 19.2.3
Multiplicative RIE when the signal and the noise have the same
distribution
(a)
Adapt the arguments of Exercise 19.2.1 to the multiplicative case with C and
W two free matrices drawn from the same ensemble. Show that in this case
Î¾(Î») =
âˆš
Î».
(b)
Redo Exercise 19.2.2 with q = q0 = 1/4, compare your Î¾(Î») with
âˆš
Î».
19.2.6 RIE for Outliers
So far, we have focused on â€œcleaningâ€ the bulk eigenvectors. But it turns out that the
formulas above are also valid for outliers of C that appear as outliers of E. One can show
that, outside the bulk, gE(z) and tE(z) are analytic on the real axis and thus, for small Î·,
Im gE(Î» âˆ’iÎ·) = âˆ’Î·gâ€²
E(Î»),
Im tE(Î» âˆ’iÎ·) = âˆ’Î·tâ€²
E(Î»).
(19.52)
Then Eqs. (19.33) and (19.43) simplify to

19.3 Properties of the Optimal RIE for Covariance Matrices
309
Î¾(Î») = Î» âˆ’d
dg

gRX(g)

,
g = gE(Î»),
Î¾(Î») = Î» d
dt [tSW(t)],
t = tE(Î»),
(19.53)
respectively for the additive and multiplicative cases.
19.3 Properties of the Optimal RIE for Covariance Matrices
Even though the optimal non-linear shrinkage function (19.44), (19.48) seems relatively
simple, it is not immediately clear what is the effect induced by the transformation Î»i â†’
Î¾(Î»i). In this section, we thus give some quantitative properties of the optimal estimator 
to understand the impact of the optimal non-linear shrinkage function.
First let us consider the moments of the spectrum of . From Eqs. (19.24) and (19.26)
we immediately derive that
Tr  =

j=1
Î¼juT
j

i=1
vivT
i

uj = Tr C,
(19.54)
meaning that the cleaning operation preserves the trace of the population matrix C, as it
should do. For the second moment, we have
Tr 2 =
N

j,k=1
Î¼jÎ¼k
N

i=1
(vT
i uj)2(vT
i uk)2.
Now, if we deï¬ne the matrix Ajk as N
i=1(vT
i uj)2(vT
i uk)2 for j,k = 1,N, it is not hard to
see that it is a matrix with non-negative entries and whose rows all sum to unity (remember
that all viâ€™s are normalized to unity). The matrix A is therefore a (bi-)stochastic matrix
and the Perronâ€“Frobenius theorem tells us that its largest eigenvalue is equal to unity (see
Section 1.2.2). Hence, we deduce the following general inequality:
N

j,k=1
Aj,k Î¼jÎ¼k â‰¤
N

j=1
Î¼2
j,
which implies that
Tr 2 â‰¤Tr C2 â‰¤Tr E2,
(19.55)
where the last inequality comes from Eq. (17.11). In words, this result states that the
spectrum of  is narrower than the spectrum of C, which is itself narrower than the
spectrum of E. The optimal rie therefore tells us that we had better be even more cautious
than simply bringing back the sample eigenvalues to their estimated true locations. This is
because we have only partial information about the true eigenbasis of C. In particular, one
should always shrink downward (resp. upward) the small (resp. top) eigenvalues compared
to their true locations Î¼i for any i âˆˆ(1,N), except for the trivial case C = 1.

310
Eigenvector Overlaps and Rotationally Invariant Estimators
Next, we consider the asymptotic behavior of the optimal non-linear shrinkage function
(19.44), (19.48). Throughout the following, suppose that we have an outlier at the left of
the lower bound of ÏE and let us assume q < 1 so that E has no exact zero mode. We know
from Section 19.2.6 that the estimator (19.44) holds for outliers. Moreover, we have that
Î»gE(Î») = O(Î») for Î» â†’0. This allows us to conclude from Eq. (19.26) that, for outliers
very close to zero,
Î¾(Î») =
Î»
(1 âˆ’q)2 + O(Î»2),
(19.56)
which is in agreement with Eq. (19.55): small eigenvalues must be pushed upwards for
q > 0.
The other asymptotic limit Î» â†’âˆis also useful since it gives us the behavior of the
non-linear shrinkage function Î¾ for large outliers. In that case, we know from Eq. (17.8)
that limÎ»â†’âˆÎ»tE(Î») âˆ¼Î»âˆ’1Ï„(E). Therefore, we conclude that
Î¾(Î») â‰ˆ
Î»

1 + qÎ»âˆ’1Ï„(E) + O(Î»âˆ’2)
2 â‰ˆÎ» âˆ’2qÏ„(E) + O(Î»âˆ’1).
(19.57)
If all variances are normalized to unity such that Ï„(E) = Ï„(C) = 1, then we simply obtain
Î¾(Î») â‰ˆÎ» âˆ’2q + O(Î»âˆ’1).
(19.58)
It is interesting to compare this with Eq. (14.54) for large rank-1 perturbations, which gives
Î» â‰ˆÎ¼ + q for Î» â†’âˆ. As a result, we deduce from Eq. (19.58) that Î¾(Î») â‰ˆÎ¼ âˆ’q and we
therefore ï¬nd the following ordering relation:
Î¾(Î») < Î¼ < Î»,
(19.59)
for isolated and large eigenvalues Î» and for q > 0. Again, this result is in agree-
ment with Eq. (19.55): large eigenvalues should be reduced downward for any q > 0,
even below the â€œtrueâ€ value of the outlier Î¼. More generally, the non-linear shrink-
age function Î¾ interpolates smoothly between Î»/(1 âˆ’q)2 for small Î» to Î» âˆ’2q for
large Î».
19.4 Conditional Average in Free Probability
In this section we give an alternative derivation of the rie formula, Eq. (19.29). This
derivation is more elegant, albeit more abstract. In particular, it does not rely on the
computation of eigenvector overlap, so by itself it misses the important link between the
rie and the computation of overlaps.
In the context of free probability, we work with abstract objects (E, C, etc.) that satisfy
the axioms of Chapter 11. We can think of them as inï¬nite-dimensional matrices. We are
given the matrix E that was obtained by free operations from an unknown matrix C. For
instance it could be given by a combination of free product and free sum.
The matrix E is generated from the matrix C; in this sense, E depends on C. We would
like to ï¬nd the best estimator (in the least-square sense) of C given E. It is given by the
conditional average
 = E[C]E.
(19.60)

19.5 Real Data
311
In this abstract context, the only object we know is E so  must be a function of E. Let us
call this function (E). The fact that  is a function of E only imposes that  commutes
with E, i.e. that  is diagonal in the eigenbasis of E. One way to determine the function
(E) is to compute all possible moments of the form mk = Ï„[(E)Ek]. They can be
combined in the function
F(z) := Ï„
&
Î¾(E)(z1 âˆ’E)âˆ’1'
(19.61)
via its Taylor series at z â†’âˆ. Using Eq. (19.60), we write
F(z) = Ï„
&
E[C]|E (z1 âˆ’E)âˆ’1'
.
(19.62)
But the operator Ï„[.] contains the expectation value over all variables, both trace and
randomness. So by the law of total expectation, Ï„(E[.]) = Ï„(.) and
F(z) = Ï„
&
C(z1 âˆ’E)âˆ’1'
.
(19.63)
To recover the function Î¾(Î») from F(z) we use a spectral decomposition of E:
F(z) =

ÏE(Î») Î¾(Î»)
z âˆ’Î»dÎ»,
(19.64)
so
lim
Î·â†’0+ Im F(Î» âˆ’iÎ·) = Ï€ÏE(Î»)Î¾(Î»),
(19.65)
which is equivalent to
Î¾(Î») =
lim
Î·â†’0+
Im F(Î» âˆ’iÎ·)
Im gE(Î» âˆ’iÎ·),
(19.66)
itself equivalent to Eq. (19.29).
19.5 Real Data
As stated above, the good news about the rie estimator is that it only depends on transforms
of the observable matrix E, such as gE(z) and tE(z) and the R- or S-transform of the noise
process. One may think that real world applications should be relatively straightforward.
However, we need to know the behavior of the limiting transforms on the real axis, precisely
where the discrete N transforms gN(z) and tN(z) fail to converge.
We will discuss here how to compute these transforms using either a parametric ï¬t or
a non-parametric approximation on the sample eigenvalues. In both cases we will tackle
the multiplicative case with a Wishart noise but the discussion can be adapted to cover
the additive case or any other type of noise. In Section 19.6 we will discuss an alternative
approach using two datasets (or disjoint subsets of the original data).
19.5.1 Parametric Approach
Ansatz on ÏC or SC
One can postulate a convenient functional form for ÏC(Î») and ï¬t the associated parameters
on the data. This allows one to obtain analytical formulas for all the relevant transforms,
from which one can extract the exact behavior on the real axis.

312
Eigenvector Overlaps and Rotationally Invariant Estimators
The simplest (most tractable) choice for ÏC(Î») is the inverse-Wishart distribution. In
this case ÏE(Î») can be computed exactly (see Eq. (15.51)) and the optimal estimator is
linear within the bulk of the spectrum, cf. Eq. (19.49). When the sample covariance matrix
is normalized such that Ï„(E) = 1, the inverse-Wishart has a single parameter p that
needs to be estimated from the data. As an estimate, one can use for example the second
moment of E:
Ï„

E2
= 1 + p + q,
(19.67)
or its ï¬rst inverse moment:
Ï„

Eâˆ’1
= 1 + p
1 âˆ’q ,
(19.68)
which is obtained using Eq. (15.13) with SE(t) = (1 âˆ’pt)/(1 + qt), or simply by
noting that Ï„(Wâˆ’1
q
W
âˆ’1
p ) = Ï„(Wâˆ’1
q )Ï„(
W
âˆ’1
p ) for free matrices, and using the results of
Sections 15.2.2 and 15.2.3.
When the distribution of sample eigenvalues appears to be bounded from above and
below, one can use a more complicated but still relatively tractable ansatz for ÏC(Î»), by
postulating a simple form for its S-transform. For example using
SC(t) = (1 âˆ’p1t)(1 âˆ’p2t)
1 + q1t
â‡”
SE(t) = (1 âˆ’p1t)(1 âˆ’p2t)
(1 + qt)(1 + q1t) ,
(19.69)
one ï¬nds that tE(Î¶) (and hence ÏE(Î»)) is the solution of a cubic equation. Higher order
terms in t in the numerator or the denominator will give higher order equations for tE(Î¶).
The parameters p1, p2, q1, etc. can be evaluated from the ï¬rst few moments and inverse
moments of E or by ï¬tting the observed density of eigenvalues. However, the particularly
convenient choice Eq. (19.69) does not work when the observed distribution of eigenvalues
does not have enough skewness, as in the example shown in Figure 19.3.
Parametric Fit of ÏE
Another approach consists of postulating a form for the density of sample eigenvalues and
ï¬tting its parameters. For example, one can postulate that
ÏE(Î») = Zâˆ’1 (1 + a1Î» + a2Î»2)
*
(Î» âˆ’Î»âˆ’)(Î»+ âˆ’Î»)
1 + b1Î» + b2Î»2
,
(19.70)
where Î»Â± are ï¬xed to the smallest/largest observed eigenvalues, and a1, a2, b1 and b2
are ï¬tted on the data by minimizing the square error on the cumulative distribution. The
normalization factor Z can be computed during the ï¬tting procedure. This particular form
ï¬ts very well for sample data generated numerically (see Fig. 19.3 left). To ï¬nd the optimal
shrinkage function (19.48), we then reconstruct the complex Stieltjes transform gE(xâˆ’i0+)
numerically, by using the ï¬tted Ï(Î») and computing its Hilbert transform. The issue with
such an approach is that even when Eq. (19.70) is a good ï¬t to the sample density of
eigenvalues, it cannot be obtained as the result of the free product of a Wishart and some

19.5 Real Data
313
0
1
2
3
l
0.0
0.2
0.4
0.6
0.8
1.0
r(l)
theoretical density
ï¬tted density
sample N = 1000
0
1
2
3
l
0.0
0.5
1.0
1.5
x(l)
simulation oracle
theoretical shrinkage
shrinkage from ï¬t
Figure 19.3 Parametric ï¬t illustrated on an example where the true covariance has a uniform density
of eigenvalues with mean 1 and variance 0.2 (see Eq. (15.42)). A single sample covariance matrix
with N = 1000 and q = 0.4 was generated, and the ad-hoc distribution (19.70) was ï¬tted to the
eigenvalue cdf. The left-hand ï¬gure shows a histogram of the sample eigenvalues compared with
the theoretical distribution and the ad-hoc ï¬t. The right-hand ï¬gure shows the theoretical optimal
shrinkage and the one obtained from the ï¬t. The agreement is barely satisfactory, in particular the
shrinkage from the ï¬t is non-monotonic. The dots show the oracle estimator Î¾k = vT
k Cvk computed
within the same simulation.
given density. As a consequence the approximate estimator generated by such an ansatz is
typically non-monotonic, whereas the exact shrinkage function should be.2
The Case of an Unbounded Support
On some real datasets, such as ï¬nancial time series, it is hard to detect a clear boundary
between bulk eigenvalues and the large outliers. In this case one may suspect that the
distribution of eigenvalues of the true covariance matrix C is itself unbounded. In that case,
one may try a parametric ï¬t for which the density of C extends to inï¬nity. For example, if
we suspect that the true distribution has a sharp left edge but a power-law right tail, we may
choose to model ÏC(Î») as a shifted half Studentâ€™s t-distribution, i.e.
ÏC(Î») = (Î» âˆ’Î»âˆ’)
2
a âˆšÏ€Î¼


1+Î¼
2


 Î¼
2


1 + (Î» âˆ’Î»âˆ’)2
a2Î¼
âˆ’1+Î¼
2
,
(19.71)
where (Î» âˆ’Î»âˆ’) indicates that the density is non-zero only for Î» > Î»âˆ’, chosen
to be the center of the Studentâ€™s t-distribution. These densities do not have an upper
edge, instead they fall off as Ï(Î») âˆ¼Î»âˆ’Î¼âˆ’1 for large Î». For integer values of the tail
2 Although we have not been able to ï¬nd a simple proof of this property, we strongly believe that it holds in full generality.

314
Eigenvector Overlaps and Rotationally Invariant Estimators
exponent Î¼, the Stieltjes transform gC(z) can be computed analytically. For example for
Î¼ = 3 we ï¬nd
gÎ¼=3(z) =
âˆš
3Ï€u3 + 6au2 + 9a2 âˆš
3Ï€u + 36a3 log

âˆ’u/
âˆš
3a2

+ 18a3
âˆš
3Ï€

3a2 + u22
,
(19.72)
where u = z âˆ’Î»âˆ’. Note that this Stieltjes transform has an essential singularity at z = Î»âˆ’
and a branch cut on the real axis from Î»âˆ’to +âˆindicating that the density has no upper
bound. For Î¼ = 3 both the mean and the variance of the eigenvalue density are ï¬nite. We
thus ï¬x Î»âˆ’= 1âˆ’2
âˆš
3a2/Ï€ such that Ï„(C) = 1 and adjust a to obtain the desired variance
given by Ï„(C2) âˆ’1 = 3a2(1 âˆ’(2/Ï€)2).
In cases like this one, where we have an analytic form for gC(z) but no simple formula
for its S-transform, we can numerically solve the subordination relation
tE(Î¶) = tC

Î¶
1 + qtE(Î¶)

,
(19.73)
with tC(Î¶) = Î¶gC(Î¶) âˆ’1, using an efï¬cient numerical ï¬xed point equation solver. Most of
the time a simple iteration would ï¬nd the ï¬xed point, but for some values of Î¶ and q it is
sometimes difï¬cult to ï¬nd an initial condition for the iteration to converge so it is better to
use a robust ï¬xed point solver.
Let us end on a technical remark: for unbounded densities, g(z) is not analytic at z = âˆ,
which does not conform to some hypotheses made throughout the book. Intuitively, there
is no longer any clear distinction between bulk eigenvalues and outliers. For a ï¬xed value
of N, and for sufï¬ciently large Î», the distance between two successive eigenvalues will at
some point become much larger than 1/N. Fortunately, the very same rie formula holds
both for bulk and for outlier eigenvalues, so we can close our eyes and safely apply Eq.
(19.27) for unbounded densities as well.
19.5.2 Kernel Methods
Another approach to compute the Stieltjes and/or the T-transform on the real axis is to
work directly with the discrete eigenvalues Î»k of E. As stated earlier we cannot simply
evaluate the discrete gN(z) at a point z = Î»k because gN(z) is inï¬nite precisely at the
points z âˆˆ{Î»k}; this is the reason why gN(z) does not converge to the limiting gE(z) on the
support of Ï(Î»).
The idea here is to generalize the standard kernel method to estimate continuous den-
sities from discrete data. Having observed a set of N eigenvalues [Î»k]kâˆˆ(1,N), a smooth
estimator of the density is constructed as
Ïs(x) := 1
N
N

k=1
KÎ·k(x âˆ’Î»k),
(19.74)

19.5 Real Data
315
where KÎ· is some adequately chosen kernel of width Î· (possibly k-dependent), normalized
such that
 +âˆ
âˆ’âˆ
du KÎ·(u) = 1,
(19.75)
such that
 +âˆ
âˆ’âˆ
dx Ïs(x) = 1.
(19.76)
A standard choice for K is a Gaussian distribution, but we will discuss more appropriate
choices for the Stieltjes transform below.
Now, let us similarly deï¬ne a smoothed Stieltjes transform as
gs(z) := 1
N
N

k=1
gK,Î·k(z âˆ’Î»k),
(19.77)
where gK,Î· is the Stieltjes transform of the kernel KÎ·, treated as a density:
gK,Î·(z) :=
 +âˆ
âˆ’âˆ
du KÎ·(u)
z âˆ’u ;
Im(z)  0.
(19.78)
Note that since Im gK,Î·(x âˆ’i0+) = iÏ€KÎ·(x), one immediately concludes that
Im gs(x âˆ’i0+) = iÏ€Ïs(x)
(19.79)
for any smoothing kernel KÎ·. Hence gs(z) is the natural generalization of smoothed densi-
ties for Stieltjes transforms. Correspondingly, the real part of the smoothed Stieltjes is the
Hilbert transform (up to a Ï€ factor) of the smoothed density, i.e.
hs(x) := Re gs(x âˆ’i0+) = âˆ’
 âˆ
âˆ’âˆ
dÎ» Ïs(Î»)
x âˆ’Î».
(19.80)
Two choices for the kernel KÎ· are specially interesting. One is the Cauchy kernel:
KC
Î· (u) := 1
Ï€
Î·
u2 + Î·2,
(19.81)
from which one gets
gKC,Î·(z) =
1
z Â± iÎ·,
Â± = sign (Im(z)) .
(19.82)
Hence, in this case, we ï¬nd that the smoothed Stieltjes transform we are looking for is
nothing but the discrete Stieltjes transform computed with a k-dependent width Î·k:
gC
s (z) := 1
N
N

k=1
1
z âˆ’Î»k âˆ’iÎ·k
,
Im(z) < 0,
(19.83)
which we can now safely compute numerically on the real axis, i.e. when z = x âˆ’i0+, and
plug in the corresponding formulas for the rie estimator Î¾(Î»).

316
Eigenvector Overlaps and Rotationally Invariant Estimators
0
1
2
3
l
0.0
0.2
0.4
0.6
0.8
1.0
1.2
r(l)
theoretical density
Wigner kernel
Cauchy kernel
0
1
2
3
l
0.0
0.5
1.0
1.5
x(l)
theoretical shrinkage
Wigner kernel
isotonic Wigner kernel
Figure 19.4 Non-parametric kernel methods applied to the same problem as in Figure 19.3. An
approximation of gE(Î» âˆ’i0+) is computed with the Cauchy kernel (19.82) and the Wigner kernel
(19.85) both with Î·k = Î· = Nâˆ’1/2. (left) We compare the two smoothed densities with the
theoretical one. Both are quite good but the Wigner kernel is better where the density changes rapidly.
(right) From the smoothed Stieltjes transforms we compute the shrinkage function for both methods.
Only the result of the Wigner kernel is shown (the Cauchy kernel is comparable albeit slightly
worse). Kernel methods give non-monotonic shrinkage functions which can be easily rectiï¬ed using
an isotonic regression (19.86), which improves the agreement with the theoretical curve.
Another interesting choice for numerical applications is the semi-circle â€œWigner kernelâ€,
which has sharp edges. To wit,
KW
Î· (u) =
*
4Î·2 âˆ’u2
2Ï€Î·2
for
âˆ’2Î· â‰¤u â‰¤2Î·,
(19.84)
and 0 when |u| > 2Î·. In this case, we obtain
gW
s (z) := 1
N
N

k=1
z âˆ’Î»k
2Î·2
k
â›
â1 âˆ’
2
1 âˆ’
4Î·2
k
(z âˆ’Î»k)2
â
â .
(19.85)
Figure 19.4 gives an illustration of the kernel method using both the Cauchy kernel and the
Wigner kernel, with Î·k = Î· = Nâˆ’1/2.
We end this section with two practical implementation points regarding the kernel
methods.
1 Since the optimal rie estimator Î¾(Î») should be monotonic in Î», one should rectify
possibly non-monotonic numerical estimators using an isotonic regression. The isotonic
regression Ë†yk of some data yk is given by
Ë†yk = argmin
T

k=1

Ë†yk âˆ’yk
2
with
Ë†y1 â‰¤Ë†y2 â‰¤Â· Â· Â· â‰¤Ë†yT âˆ’1 â‰¤Ë†yT .
(19.86)

19.6 Validation and RIE
317
It is the monotonic sequence that is the closest (in the least-square sense) to the original
data.
2 In most situations, we are interested in reconstructing the optimal rie matrix  =
 Î¾(Î»k)vkvT
k and hence we need to evaluate the shrinkage function Î¾(Î») precisely at the
sample eigenvalues {Î»k}. We have found empirically that excluding the point Î»k itself
from the kernel estimator consistently gives better results than including it. For example,
in the Cauchy case, one should compute
gC
s (Î»â„“âˆ’i0+) â‰ˆ
1
N âˆ’1
N

k=1
kâ„“
1
Î»â„“âˆ’Î»k âˆ’iÎ·k
,
(19.87)
when estimating Eq. (19.48).
19.6 Validation and RIE
The idea of validation to determine the rie is to compute the eigenvectors vi of E the scm
of a training set and compute their unbiased variance of a different dataset: the validation
set. More formally, this is written
Î¾Ã—(Î»i) := vi
T Eâ€²vi,
(19.88)
where Eâ€² is the validation scm. The training set is also called the in-sample data and the
validation set the out-of-sample data.
In practical applications, we have typically a single dataset that needs to be split into
a training and a validation set. If we are not too worried about temporal order, any block
of the data can serve as the validation set. In K-fold cross-validation, the data is split into
K blocks, one block is the validation set and the union of the K âˆ’1 others serves as the
training set. The procedure is then repeated successively choosing the K possible validation
sets, see Figure 19.5.
In the following, we will assume that the true covariance matrix C is the same on both
datasets so that E = C
1
2 WC
1
2 and Eâ€² = C
1
2 Wâ€²C
1
2 , where Wâ€² is independent from W.
Expanding over the eigenvectors vâ€²
j of Eâ€², we get
Î¾Ã—(Î»i) =
N

k=1

vi
T vâ€²
k
2 Î»â€²
k
(19.89)
or, in the large N limit and using the deï¬nition of  given in Eq. (19.6),
Î¾Ã—(Î») âˆ’â†’
Nâ†’âˆ

ÏEâ€²(Î»â€²)(Î»,Î»â€²)Î»â€² dÎ»â€².
(19.90)
Now, there is an exact relation between  and , which reads
(Î»,Î»â€²) = 1
N
N

j=1
(Î»,Î¼j)(Î»â€²,Î¼j)
(19.91)

318
Eigenvector Overlaps and Rotationally Invariant Estimators
or, in the continuum limit,
(Î»,Î»â€²) =

ÏC(Î¼)(Î»,Î¼)(Î»â€²,Î¼) dÎ¼.
(19.92)
Intuitively, this relation can be understood as follows: we expect that, from the very
deï¬nition of , the eigenvectors of E and Eâ€² can be written as
vi =
1
âˆš
N
N

j=1
Îµij
/
(Î»i,Î¼j) uj;
vâ€²
k =
1
âˆš
N
N

â„“=1
Îµâ€²
kâ„“
/
(Î»â€²
k,Î¼â„“) uâ„“,
(19.93)
where uj are the eigenvectors of C and Îµij are independent random variables of mean
zero and variance one, such that
E[ÎµijÎµkâ„“] = Î´ikÎ´jâ„“,
E[Îµâ€²
ijÎµâ€²
kâ„“] = Î´ikÎ´jâ„“,
E[ÎµijÎµâ€²
kâ„“] = 0.
(19.94)
This so-called ergodic assumption can be justiï¬ed from considerations about the Dyson
Brownian motion of eigenvectors, see Eq. (9.10), but this goes beyond the scope of this
book. In any case, if we now compute E[(vT
i vâ€²
k)2] using the ergodic assumption and
remembering that uT
j uâ„“= Î´jâ„“, we ï¬nd
NE[(vT
i vâ€²
k)2] = 1
N
N

j=1
(Î»i,Î¼j)(Î»â€²
k,Î¼j),
(19.95)
which is precisely Eq. (19.91).
Injecting Eq. (19.91) into Eq. (19.89), we thus ï¬nd
Î¾Ã—(Î») = 1
N2
N

k=1
â›
â
N

j=1
(Î»,Î¼j)(Î»â€²
k,Î¼j)
â
â Î»â€²
k
= 1
N2
N

j=1
(Î»,Î¼j)
 N

k=1
(Î»â€²
k,Î¼j)Î»â€²
k

.
(19.96)
The last term in parenthesis can be computed by using the very deï¬nition of Eâ€²:
N

k=1
(Î»â€²
k,Î¼j)Î»â€²
k â‰¡Nuj
T Eâ€²uj
= NC
1
2 uT
j Wâ€²C
1
2 uj = NÎ¼juT
j Wâ€²uj.
(19.97)
Now, the idea is that since Wâ€² is independent of C, averaging over any small interval of Î¼j
will amount to replacing uj
T Wâ€²uj by its average over randomly oriented vectors u, which
is equal to unity:
E[uT Wâ€²u] = Ï„(Wâ€²uuT ) = Ï„(Wâ€²)Ï„(uuT ) = 1.
(19.98)
Hence, from Eq. (19.96) we ï¬nally obtain
Î¾Ã—(Î») = 1
N
N

j=1
(Î»,Î¼j)Î¼j âˆ’â†’
Nâ†’âˆ

ÏC(Î¼) (Î»,Î¼) Î¼ dÎ¼,
(19.99)

19.6 Validation and RIE
319
0.0
0.5
1.0
1.5
2.0
2.5
3.0
l
0.50
0.75
1.00
1.25
1.50
1.75
x(l)
cross-validation
theoretical shrinkage
isotonic cross-validation
Figure 19.5 Shrinkage function Î¾(Î») computed for the same problem as in Figure 19.3, now using
cross-validation. The dataset is divided into K = 10 blocks of equal length. For each block, we
compute the N = 1000 eigenvalues Î»b
i and eigenvectors vb
i of the sample covariance matrix using
the rest of the data (of new length 9T/10), and compute Î¾Ã—(Î»b
i ) := vb
i
T Eâ€²vb
i , with Eâ€² the sample
covariance matrix of the considered block. The dots correspond to the 10 Ã— 1000 pairs (Î»b
i ,Î¾Ã—(Î»b
i )).
The full line is an isotonic regression through the dots. The procedure has a slight bias as we in fact
compute the optimal shrinkage for a value of q equal to qÃ— = 10N/9T , but otherwise the agreement
with the optimal curve is quite good.
which precisely coincides with the deï¬nition of the optimal non-linear shrinkage function
Î¾(Î»), see Eq. (19.27).
This result is very interesting and indicates that one can approximate Î¾(Î») by considering
the quadratic form between the eigenvectors of a given realization of C â€“ say E â€“ and
another realization of C â€“ say Eâ€² â€“ even if the two empirical matrices are characterized by
different values of the ratio N/T . This method is illustrated in Figure 19.5.
Bibliographical Notes
â€¢ For a recent review on the subject of cleaning noisy covariance matrices and rie, see
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning correlation matrices. Risk magazine,
2016,
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1â€“109, 2017.
â€¢ The original work of Ledoit and PÂ´echÂ´e and its operational implementation, see
â€“ O. Ledoit and S. PÂ´echÂ´e. Eigenvectors of some large sample covariance matrix ensem-
bles. Probability Theory and Related Fields, 151(1-2):233â€“264, 2011,
â€“ O. Ledoit and M. Wolf. Nonlinear shrinkage estimation of large-dimensional covari-
ance matrices. The Annals of Statistics, 40(2):1024â€“1060, 2012,

320
Eigenvector Overlaps and Rotationally Invariant Estimators
â€“ O. Ledoit and M. Wolf. Nonlinear shrinkage of the covariance matrix for port-
folio selection: Markowitz meets Goldilocks. The Review of Financial Studies,
30(12):4349â€“4388, 2017.
â€¢ Rotationally invariant estimators for general additive and multiplicative models:
â€“ J. Bun, R. Allez, J.-P. Bouchaud, and M. Potters. Rotational invariant estimator for
general noisy matrices. IEEE Transactions on Information Theory, 62:7475â€“7490,
2016.
â€¢ Rotationally invariant estimators for outliers:
â€“ J. Bun and A. Knowles. An optimal rotational invariant estimator for general covari-
ance matrices. Unpublished, 2016. preprint available on researchgate.net,,
see also Bun et al. [2017].
â€¢ Overlaps between the eigenvectors of correlated matrices:
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Overlaps between eigenvectors of correlated
random matrices. Physical Review E, 98:052145, 2018.
â€¢ Rotationally invariant estimators for cross-correlation matrices:
â€“ F. Benaych-Georges, J.-P. Bouchaud, and M. Potters. Optimal cleaning for singular
values of cross-covariance matrices. preprint arXiv:1901.05543, 2019.

20
Applications to Finance
20.1 Portfolio Theory
One of the arch-problems in quantitative ï¬nance is portfolio construction. For example, one
may consider an investment universe made of N stocks (or more generally N risky assets)
that one should bundle up in a portfolio to achieve optimal performance, according to some
quality measure that we will discuss below.
20.1.1 Returns and Risk Free Rate
We call pi,t the price of stock i at time t and deï¬ne the returns over some elementary time
scale (say one day) as
ri,t := pi,t âˆ’pi,tâˆ’1
pi,tâˆ’1
.
(20.1)
The portfolio weight Ï€i is the dollar amount invested on asset i, which can be positive
(corresponding to buys) or negative (corresponding to short sales). The total capital to be
invested is C. Naively, one should have C = 
i Ï€i. But one can borrow cash, so that

i Ï€i > C and pay the risk free rate r0 on the borrowed amount, or conversely under-
invest in stocks (
i Ï€i < C) and invest the remaining capital at the risk free rate, assumed
to be the same r0.1 Then the total return of the portfolio (in dollar terms) is
Rt =

i
Ï€iri,t + (C âˆ’

i
Ï€i)r0,
(20.2)
so that the excess return (over the risk free rate) is
Rt âˆ’Cr0 :=

i
Ï€i(ri,t âˆ’r0),
(20.3)
where ri,t âˆ’r0 is the excess return of asset i. From now on, we will denote ri,t âˆ’r0 by ri,t.
We will assume that these excess returns are characterized by some expected gains gi and
a covariance matrix C, with
1 In general, the risk free rate to borrow is different from the one to lend, but we will neglect the difference here.
321

322
Applications to Finance
Cij = Cov[rirj].
(20.4)
The problem, of course, is that both the vector of expected gains g and the covariance
matrix C are unknown to the investor, who must come up with his/her best guess for these
quantities. Forming expectations of future returns is the job of the investor, based on his/her
information, anticipations, and hunch. We will not attempt to model the sophisticated
process at work in the mind of investors, and simply assume that g is known. In the simplest
case, the investor has no preferences and g = g1, corresponding to the same expected return
for all assets. Another possibility is to assume that g is, for all practical purposes, a random
vector in RN.
As far as C is concerned, the most natural choice is to use the sample covariance matrix
E, determined using a series of past returns of length T . However, as we already know
from Chapter 17, the eigenvalues of E can be quite far from those of C when q = N/T
is not very small. On the other hand, T cannot be as large as one could wish, the most
important reason being that the (ï¬nancial) world is non-stationary. For a start, many large
ï¬rms that exist in 2019 did not exist 25 years ago. More generally, it is far from clear that
the parameters of the underlying statistical process (if such a thing exists) can be considered
as constant in time, so mixing different epochs is in general not warranted. On the other
hand, due to experimental constraints, the limitation of data points can be a problem even
in a stationary world.
20.1.2 Portfolio Risk
The risk of a portfolio is traditionally measured as the variance of its returns, namely
R2 := V[R] =

i,j
Ï€iÏ€jCov[rirj] = Ï€ T CÏ€.
(20.5)
Other measures of risk can however be considered, such as the expected shortfall Sp (or
conditional value at risk), deï¬ned as
Sp = âˆ’1
p
 Rp
âˆ’âˆ
dR R P(R),
(20.6)
where Rp is the p-quantile, with for example p = 0.01 for the 1% negative tail events:
p =
 Rp
âˆ’âˆ
dR P(R).
(20.7)
If P(R) is Gaussian, then all risk measures are equivalent and subsumed in V[R].
20.1.3 Markowitz Optimal Portfolio Theory
For the reader not familiar with Markowitzâ€™s optimal portfolio theory, we recall in this
section some of the most important results. Suppose that an investor wants to invest in a

20.1 Portfolio Theory
323
portfolio containing N different assets, with optimal weights Ï€ to be determined. An intu-
itive strategy is the so-called mean-variance optimization: the investor seeks an allocation
such that the variance of the portfolio is minimized given an expected return target. It is not
hard to see that this mean-variance optimization can be translated into a simple quadratic
optimization program with a linear constraint. Markowitzâ€™s optimal portfolio amounts to
solving the following quadratic optimization problem:
$ minÏ€âˆˆRN 1
2Ï€ T CÏ€
subject to Ï€ T g â‰¥G
(20.8)
where G is the desired (or should we say hoped for) gain. Without further constraints â€“ such
as the positivity of all weights necessary if short positions are not allowed â€“ this problem
can be easily solved by introducing a Lagrangian multiplier Î³ and writing
min
Ï€âˆˆRN
1
2Ï€ T CÏ€ âˆ’Î³ Ï€ T g.
(20.9)
Assuming that C is invertible, it is not hard to ï¬nd the optimal solution and the value of Î³
such that overall expected return is exactly G. It is given by
Ï€C = G Câˆ’1g
gT Câˆ’1g,
(20.10)
which, as noted above, requires the knowledge of both C and g, which are a priori
unknown. Note that even if the predictions gi of our investor are completely wrong, it still
makes sense to look for the minimum risk portfolio consistent with his/her expectations.
But we are left with the problem of estimating C, or maybe Câˆ’1 before applying
Markowitzâ€™s formula, Eq. (20.10). We will see below why one should actually ï¬nd the
best estimator of C itself before inverting it and determining the weights.
What is the risk associated with this optimal allocation strategy, measured as the variance
of the returns of the portfolio? If one knew the population correlation matrix C, the true
optimal risk associated with Ï€C would be given by
R2
true := Ï€ T
CCÏ€C =
G2
gT Câˆ’1g.
(20.11)
However, the optimal strategy (20.10) is not attainable in practice as the matrix C is
unknown. What can one do then, and how poorly is the realized risk of the portfolio
estimated?
20.1.4 Predicted and Realized Risk
One obvious â€“ but far too naive â€“ way to use the Markowitz optimal portfolio is to apply
(20.10) using the scm E as is, instead of C. Recalling the results of Chapter 17, it is not
hard to see that this strategy will suffer from strong biases whenever T is not sufï¬ciently
large compared to N.

324
Applications to Finance
Notwithstanding, the optimal investment weights using the scm E read
Ï€E = G Eâˆ’1g
gT Eâˆ’1g,
(20.12)
and the minimum risk associated with this portfolio is thus given by
R2
in = Ï€ T
EE Ï€E =
G2
gT Eâˆ’1g,
(20.13)
which is known as the â€œin-sampleâ€ risk, or the predicted risk. It is â€œin-sampleâ€ because it
is entirely constructed using the available data. The realized risk in the next period, with
fresh data, is correspondingly called out-of-sample.
Using the convexity with respect to E of gT Eâˆ’1g, we ï¬nd from the Jensen inequality
that, for ï¬xed predicted gains g,
E[gT Eâˆ’1g] â‰¥gT E

E
âˆ’1g = gT Câˆ’1g,
(20.14)
where the last equality holds because E is an unbiased estimator of C. Hence, we conclude
that the in-sample risk is lower than the â€œtrueâ€ risk and therefore the optimal portfolio
Ï€E suffers from an in-sample bias: its predicted risk underestimates the true optimal risk
Rtrue. Intuitively this comes from the fact that Ï€E attempts to exploit all the idiosyncracies
that happened during the in-sample period, and therefore manages to reduce the risk below
the true optimal risk. But the situation is even worse, because the future out-of-sample or
realized risk, turns out to be larger than the true risk. Indeed, let us denote by Eâ€² the scm of
this out-of-sample period; the out-of-sample risk is then naturally deï¬ned by
R2
out = Ï€ T
EEâ€² Ï€E = G2gT Eâˆ’1Eâ€²Eâˆ’1g
(gT Eâˆ’1g)2
.
(20.15)
For large matrices, we expect the result to be self-averaging and given by its expectation
value (over the measurement noise). But if the measurement noise in the in-sample period
(contained in Ï€E) can be assumed to be independent from that of the out-of-sample
period, then Ï€E and Eâ€² are uncorrelated and we get, for N â†’âˆ,
Ï€ T
EEâ€²Ï€E = Ï€ T
ECÏ€E.
(20.16)
Now, from the optimality of Ï€C, we also know that
Ï€ T
CCÏ€C â‰¤Ï€ T
ECÏ€E,
(20.17)
so we readily obtain the following general inequalities:
R2
in â‰¤R2
true â‰¤R2
out.
(20.18)
We plot in Figure 20.1 an illustration of these inequalities. One can see how using
Ï€E is clearly overoptimistic and can potentially lead to disastrous results in practice.

20.2 The High-Dimensional Limit
325
0.0
0.5
1.0
1.5
2.0
2
0.00
0.25
0.50
0.75
1.00
1.25
1.50
2
in(E)
2
in( )
2
true
2
out( )
2
out(E)
Figure 20.1 Efï¬cient frontier associated with the mean-variance optimal portfolio (20.10) for g = 1
and C an inverse-Wishart matrix with p = 0.5, for q = 0.5. The black line depicts the expected
gain as a function of the true optimal risk (20.11). The gray lines correspond to the realized (out-of-
sample) risk using either the scm E or its rie version . Both estimates are above the true risk, but
less so for rie. Finally, the dashed lines represent the predicted (in-sample) risk, again using either
the scm E or its rie version . R and G in arbitrary units, such that Rtrue = 1 for G = 1.
This conclusion in fact holds for different risk measures, such as the expected shortfall
measure mentioned in Section 20.1.2.
20.2 The High-Dimensional Limit
20.2.1 In-Sample vs Out-of-Sample Risk: Exact Results
In the limit of large matrices and with some assumptions on the structure g, we can make
the general inequalities Eq. (20.18) more precise using the random matrix theory tools from
the previous chapters. Let us suppose that the vector of predictors g points in a random
direction, in the sense that the covariance matrix C and ggT are mutually free. This is not
necessarily a natural assumption. For example, the simplest â€œagnosticâ€ prediction g = 1 is
often nearly collinear with the top eigenvector of C (see Section 20.4). So we rather think
here of market neutral, sector neutral predictors that attempt to capture very idiosyncratic
characteristics of ï¬rms.
Now, if M is a positive deï¬nite matrix that is free from ggT , then in the large N limit:
gT Mg
N
= 1
N Tr[ggT M]
=
freeness
g2
N Ï„(M),
(20.19)

326
Applications to Finance
where we recall that Ï„ is the normalized trace operator. We can always normalize the
prediction vector such that g2/N = 1, so setting M = {Eâˆ’1, Câˆ’1}, we can directly estimate
Eqs. (20.13), (20.11) and (20.15) and ï¬nd
R2
in â†’
G2
NÏ„(Eâˆ’1),
R2
true â†’
G2
NÏ„(Câˆ’1),
R2
out â†’G2Ï„(Eâˆ’1CEâˆ’1)
NÏ„ 2(Eâˆ’1)
.
(20.20)
Let us focus on the ï¬rst two terms above. For q < 1, we know from Eq. (17.14) that, in
the high-dimensional limit, Ï„(Câˆ’1) = (1 âˆ’q)Ï„(Eâˆ’1). As a result, we have, for N â†’âˆ,
R2
in = (1 âˆ’q)R2
true.
(20.21)
Hence, for any q âˆˆ(0,1), we see that the in-sample risk associated with Ï€E always
provides an overoptimistic estimator. Even better, we are able to quantify precisely the
risk underestimation factor thanks to Eq. (20.21).
Next we would like to ï¬nd the same type of relation for the â€œout-of-sampleâ€ risk. In
order to do so, we write E = C
1
2 WqC
1
2 where Wq is a white Wishart matrix of parameter
q, independent from C. Plugging this representation into Eq. (20.15), we ï¬nd that the out-
of-sample risk can be expressed as
R2
out =
G2Ï„(Câˆ’1Wâˆ’2
q )
NÏ„ 2(Eâˆ’1)
when N â†’âˆ. Now, since Wq and C are asymptotically free, we also have
Ï„(Câˆ’1Wâˆ’2
q ) = Ï„(Câˆ’1) Ï„(Wâˆ’2
q ).
(20.22)
Hence, using again Eq. (17.14) yields
R2
out = G2(1 âˆ’q)2 Ï„(Wâˆ’2
q )
NÏ„(Câˆ’1).
(20.23)
Finally, we know from Eq. (15.22) that Ï„(Wâˆ’2
q ) = (1 âˆ’q)âˆ’3 for q < 1. Hence, we ï¬nally
get
R2
out = R2
true
1 âˆ’q .
(20.24)
All in all, we have obtained the following asymptotic relation:
R2
in
1 âˆ’q = R2
true = (1 âˆ’q)R2
out,
(20.25)
which holds for a completely general C.

20.2 The High-Dimensional Limit
327
Hence, if one invests with the â€œnaiveâ€ weights Ï€E, it turns out that the predicted risk Rin
underestimates the realized risk Rout by a factor (1 âˆ’q), and in the extreme case N = T
or q = 1, the in-sample risk is equal to zero while the out-of-sample risk diverges (for
N â†’âˆ). We thus conclude that, as announced, the use of the scm E for the Markowitz
optimization problem can lead to disastrous results. This suggests that we should use a
more reliable estimator of C in order to control the out-of-sample risk.
20.2.2 Out-of-Sample Risk Minimization
We insisted throughout the last section that the right quantity to control in portfolio man-
agement is the realized, out-of-sample risk. It is also clear from Eq. (20.25) that using the
sample estimate E is a very bad idea, and hence it is natural to wonder which estimator
of C one should use to minimize this out-of-sample risk? The Markowitz formula (20.10)
naively suggests that one should look for a faithful estimator of the so-called precision
matrix Câˆ’1. But in fact, since the expected out-of-sample risk involves the matrix C lin-
early, it is that matrix that should be estimated.
Let us show this using another route, in the context of rotationally invariant estimators,
which we considered in Chapter 19. Let us deï¬ne our rie as
 =
N

i=1
Î¾(Î»i)vivT
i ,
(20.26)
where we recall that vi are the sample eigenvectors of E and Î¾(Â·) is a function that has to
be determined using some optimality criterion.
Suppose that we construct a Markowitz optimal portfolio Ï€ using this rie. Again, we
assume that the vector g is random, and independent from . Consequently, the estimate
(20.19) is still valid, such that the realized risk associated with the portfolio Ï€ reads, for
N â†’âˆ,
R2
out() = G2 Tr

âˆ’1Câˆ’1

Tr âˆ’1
2
.
(20.27)
Using the decomposition (20.26) of , we can rewrite the numerator as
Tr

âˆ’1Câˆ’1
=
N

i=1
vi
T Cvi
Î¾2(Î»i) ,
(20.28)
while the denominator of Eq. (20.27) is

Tr âˆ’12
=
 N

i=1
1
Î¾(Î»i)
2
.
(20.29)
Regrouping these last two equations allows us to express Eq. (20.27) as

328
Applications to Finance
R2
out() = G2
N

i=1
vi
T Cvi
Î¾2(Î»i)
 N

i=1
1
Î¾(Î»i)
âˆ’2
.
(20.30)
Our aim is to ï¬nd the optimal shrinkage function Î¾(Î»j) associated with the sample eigen-
values [Î»j]N
j=1, such that the out-of-sample risk is minimized. This can be done by solving,
for a given j, the following ï¬rst order condition:
âˆ‚R2
out()
âˆ‚Î¾(Î»j)
= 0,
âˆ€j = 1, . . . ,N.
(20.31)
By performing the derivative with respect to Î¾(Î»j) in (20.30), one obtains
âˆ’2vj
T Cvj
Î¾3(Î»j)
 N

i=1
1
Î¾(Î»i)
âˆ’2
+
2
Î¾2(Î»j)
 N

i=1
vi
T Cvi
Î¾2(Î»i)
 N

i=1
1
Î¾(Î»i)
âˆ’3
= 0.
(20.32)
The solution to this equation is given by
Î¾(Î»j) = Avj
T Cvj,
(20.33)
where A is an arbitrary constant at this stage. But since the trace of the rie must match that
of C, this constant A must be equal to 1. Hence we recover precisely the oracle estimator
that we have studied in Chapter 19.
As a conclusion, the optimal rie (19.26) actually minimizes the out-of-sample risk
within the class of rotationally invariant estimators. Moreover, the corresponding â€œoptimalâ€
realized risk is given by
R2
out() =
G2
Tr

()âˆ’1,
(20.34)
where we used the notable property that, for any n âˆˆZ,
Tr[()nC] =
N

i=1
Î¾(Î»i)n Tr[vivT
i C] =
N

i=1
Î¾(Î»i)nvT
i Cvi â‰¡Tr[()n+1].
(20.35)
20.2.3 The Inverse-Wishart Model: Explicit Results
In this section, we specialize the result (20.34) to the case when C is an inverse-Wishart
matrix with parameter p > 0, corresponding to the simple linear shrinkage optimal
estimator. First, we read from Eq. (15.30) that
Ï„

Câˆ’1
= âˆ’gC(0) = 1 + p,
(20.36)
so that we get from Eq. (20.20) that, in the large N limit,
R2
true = G2
N
1
1 + p .
(20.37)
Next, we see from Eq. (20.34) that the optimal out-of-sample risk requires the compu-
tation of Ï„(()âˆ’1). In general, the computation of this quantity is highly non-trivial but

20.2 The High-Dimensional Limit
329
some simpliï¬cations appear when C is an inverse-Wishart matrix. In the large-dimension
limit, the ï¬nal result reads
Ï„(()âˆ’1) = âˆ’

1 + q
p

gE

âˆ’q
p

= 1 +
p2
p + q + pq ,
(20.38)
and therefore we have from Eq. (20.34)
R2
out() = G2
N
p + q + pq
(p + q)(1 + p),
(20.39)
and so it is clear from Eqs. (20.39) and (20.37) that, for any p > 0,
R2out()
R2true
= 1 + q
pq
(p + q)(1 + p) â‰¥1,
(20.40)
where the last inequality becomes an equality only when q = 0, as it should.
It is also interesting to evaluate the in-sample risk associated with the optimal rie. It is
deï¬ned by
R2
in() = G2 Tr

()âˆ’1E()âˆ’1
NÏ„2(()âˆ’1)
,
(20.41)
where the most challenging term is the numerator. Using the fact that the eigenvalues of
 are given by the linear shrinkage formula (19.49), one can once again ï¬nd a closed
formula.2 The ï¬nal result is written
R2
in() = G2
N
p + q
(1 + p)(p + q(p + 1)),
(20.42)
and we therefore deduce with Eq. (20.37) that, for any p > 0,
R2
in()
R2true
= 1 âˆ’
pq
p + q(1 + p) â‰¤1,
(20.43)
where the inequality becomes an equality for q = 0 as above.
Finally, one may easily check from Eqs. (20.25), (20.40) and (20.43) that
R2
in() âˆ’R2
in(E) â‰¥0,
R2
out() âˆ’R2
out(E) â‰¤0,
(20.44)
showing explicitly that we indeed reduce the overï¬tting by using the oracle estimator
instead of the scm in the high-dimensional framework: both the in-sample and out-of-
sample risks computed using  are closer to the true risk than when computed with the raw
empirical matrix E. The results shown in Figure 20.1 correspond to the inverse-Wishart
case with p = q = 1
2.
Exercise 20.2.1
Optimal portfolio when the true covariance matrix is Wishart
In this exercise we continue the analysis of Exercise 19.2.2 assuming that
we measure an scm from data with a true covariance given by a Wishart with
parameter q0.
2 Details of this computation can be found in Bun et al. [2017].

330
Applications to Finance
(a)
The minimum risk portfolio with expected gain G is given by
Ï€ = G Câˆ’1g
gT Câˆ’1g,
(20.45)
where C is the covariance matrix (or an estimator of it) and g is the vector of
expected gains. Compute the matrix  by taking the matrix E1 and replacing
its eigenvalues Î»k by Î¾(Î»k) and keeping the same eigenvectors. Use the result
of Exercise 19.2.2(c) for Î¾(Î»), if some Î»k are below 0.05594 or above 3.746
replace them by 0.05594 and 3.746 respectively. This is so that you do not
have to worry about ï¬nding the correct solution tE(z) for z outside of the
bulk.
(b)
Build the three portfolios Ï€C, Ï€E and Ï€ by computing Eq. (20.45) for the
three matrices C, E1 and  using G = 1 and g = e1, the vector with 1 in the
ï¬rst component and 0 everywhere else. These three portfolios correspond to
the true optimal, the naive optimal and the cleaned optimal. The true optimal
is in general unobtainable. For these three portfolios compute the in-sample
risk Rin := Ï€ T E1Ï€, the true risk Rtrue := Ï€ T CÏ€ and the out-of-sample risk
Rout := Ï€ T E2Ï€.
(c)
Comment on these nine values. For Ï€C and Ï€E you should ï¬nd exact
theoretical values. The out-of-sample risk for Ï€ should better than for Ï€E
but worse than for Ï€C.
20.3 The Statistics of Price Changes: A Short Overview
20.3.1 Bachelierâ€™s First Law
The simplest property of ï¬nancial prices, dating back to Bachelierâ€™s thesis, states that typi-
cal price variations grow like the square-root of time. More formally, under the assumption
that price returns have zero mean (which is usually a good approximation on short time
scales), then the price variogram
V(Ï„) := E[(log pt+Ï„ âˆ’log pt)2]
(20.46)
grows linearly with time lag Ï„, such that V(Ï„) = Ïƒ 2Ï„.
20.3.2 Signature Plots
Assume now that a price series is described by
log pt = log p0 +
t
tâ€²=1
rtâ€²,
(20.47)
where the return series rt is covariance-stationary with zero mean and covariance
Cov (rtâ€²,rtâ€²â€²) = Ïƒ 2Cr(|tâ€² âˆ’tâ€²â€²|).
(20.48)

20.3 The Statistics of Price Changes: A Short Overview
331
The case of a random walk with uncorrelated price returns corresponds to Cr(u) = Î´u,0,
where Î´u,0 is the Kronecker delta function. A trending random walk has Cr(u) > 0 and a
mean-reverting random walk has Cr(u) < 0. How does this affect Bachelierâ€™s ï¬rst law?
One important implication is that the volatility observed by sampling price series on a
given time scale Ï„ is itself dependent on that time scale. More precisely, the volatility at
scale Ï„ is given by
Ïƒ 2(Ï„) := V(Ï„)
Ï„
= Ïƒ 2(1)
-
1 + 2
Ï„

u=1

1 âˆ’u
Ï„

Cr(u)
.
.
(20.49)
A plot of Ïƒ(Ï„) versus Ï„ is called a volatility signature plot. The case of an uncorrelated ran-
dom walk leads to a ï¬‚at signature plot. Positive correlations (which correspond to trends)
lead to an increase in Ïƒ(Ï„) with increasing Ï„. Negative correlations (which correspond to
mean reversion) lead to a decrease in Ïƒ(Ï„) with increasing Ï„.
20.3.3 Volatility Signature Plots for Real Price Series
Quite remarkably, the volatility signature plots of most liquid assets (stocks, futures,
FX, . . . ) are nowadays almost ï¬‚at for values of Ï„ ranging from a few seconds to a
few months (beyond which it becomes dubious whether the statistical assumption of
stationarity still holds). For example, for the S&P500 E-mini futures contract, which is
one of the most liquid contracts in the world, Ïƒ(Ï„) only decreases by about 20% from
short time scales (seconds) to long time scales (weeks). For single stocks, however,
some interesting deviations from a ï¬‚at horizontal line can be detected, see Figure 20.2.
The exact form of a volatility signature plot depends on the microstructural details of
the underlying asset, but most liquid contracts in this market have a similar volatility
signature plot.
20.3.4 Heavy Tails
An overwhelming body of empirical evidence from a vast array of ï¬nancial instruments
(including stocks, currencies, interest rates, commodities, and even implied volatility)
shows that unconditional distributions of returns have fat tails, which decay as a power law
for large arguments and are much heavier than the tails of a Gaussian distribution.
On short time scales (between about a minute and a few hours), the empirical density
function of returns r can be ï¬t reasonably well by a Studentâ€™s t-distribution, see Figure
20.3. Studentâ€™s t-distributions read
P(r) =
1
a âˆšÏ€Î¼


1+Î¼
2


 Î¼
2


1 + r2
a2Î¼
âˆ’1+Î¼
2
,
(20.50)
where a is a parameter ï¬xing the scale of r. Studentâ€™s t is such that P(r) decays for large
r as |r|âˆ’1âˆ’Î¼, where Î¼ is the tail exponent. Empirically, the tail parameter Î¼ is consistently
found to be around 3 for a wide variety of different markets (see Fig. 20.3), which suggests

332
Applications to Finance
0
50
100
150
200
t
0.90
0.95
1.00
1.05
s2(t)
constant
US stock data
Figure 20.2 Average signature plot for the normalized returns of US stocks, where the x-axis is
in days. The data consists of the returns of 1725 US companies over the period 2012â€“2019 (2000
business days), returns are normalized by a one-year exponential estimate of their past volatility. To
a ï¬rst approximation Ïƒ 2(Ï„) is independent of Ï„. The signature plot allows us to see deviations from
this pure random walk behavior. One can see that stocks tend to mean-revert slightly at short times
(Ï„ < 50 days) and trend at longer times. The effect is stronger on the many low liquidity stocks
included in this dataset.
âˆ’15
âˆ’10
âˆ’5
0
5
10
15
x = r/ s
10âˆ’6
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
10âˆ’1
100
P(x)
Studentâ€™s t m = 3
Gaussian
US stock data
Figure 20.3 Empirical distribution of normalized daily stock returns compared with a Gaussian and
a Studentâ€™s t-distribution with Î¼ = 3 and the same variance. Same data as in Figure 20.2.
some kind of universality in the mechanism leading to extreme returns. This universality
hints at the fact that fundamental factors are probably unimportant in determining the
amplitude of most large price jumps. Interestingly, many studies indeed suggest that large
price moves are often not associated with an identiï¬able piece of news that would rationally
explain wild valuation swings.

20.3 The Statistics of Price Changes: A Short Overview
333
20.3.5 Volatility Clustering
Although considering the unconditional distribution of returns is informative, it is also
somewhat misleading. Returns are in fact very far from being iid random variables â€“
although they are indeed nearly uncorrelated, as their (almost) ï¬‚at signature plots demon-
strate. Therefore, returns are not simply independent random variables drawn from
the Studentâ€™s t-distribution. Such an iid model would predict that upon time aggregation the
distribution of returns would quickly converge to a Gaussian distribution on longer time
scales. Empirical data indicates that this is not the case, and that returns remain substantially
non-Gaussian on time scales up to weeks or even months.
The dynamics of ï¬nancial markets is in fact highly intermittent, with periods of intense
activity intertwined with periods of relative calm. In intuitive terms, the volatility of ï¬nan-
cial returns is itself a dynamic variable that changes over time with a broad distribution of
characteristic frequencies, a phenomenon called heteroskedasticity. In more formal terms,
returns can be represented by the product of a time dependent volatility component Ïƒt and
an iid directional component Îµt,
rt := ÏƒtÎµt.
(20.51)
In this representation, Îµt are iid (but not necessarily Gaussian) random variables of unit
variance and Ïƒt are positive random variables with long memory. This is illustrated in
Figure 20.4 where we show the autocorrelation of the squared returns, which gives access
to E[Ïƒ 2
t Ïƒ 2
t+Ï„].
It is worth pointing out that volatilities Ïƒ and scaled returns Îµ are in fact not independent
random variables. It is well documented that positive past returns tend to decrease
100
101
102
t
10âˆ’2
r(r2
t ,r2
t+ t)
US stock data
r
tâˆ’0.5
Figure 20.4 Average autocorrelation function of squared daily returns for the US stock data described
in Figure 20.3. The autocorrelation decays very slowly with the time difference Ï„. A power law Ï„âˆ’Î³
with Î³ = 0.5 is plotted to guide the eye. Note the three peaks at Ï„ = 65, 130 and 195 business days
correspond to the periodicity of highly volatile earning announcements.

334
Applications to Finance
future volatilities and that negative past returns tend to increase future volatilities (i.e.
E[ÎµtÏƒt+Ï„] < 0 for Ï„ > 0). This is called the leverage effect. Importantly, however, past
volatilities do not give much information on the sign of future returns (i.e. E[ÎµtÏƒt+Ï„] â‰ˆ0
for Ï„ < 0).
20.4 Empirical Covariance Matrices
20.4.1 Empirical Eigenvalue Spectrum
We are now in position to investigate the empirical covariance matrix E of a collection of
stocks. For deï¬niteness, we choose q = 0.25 by selecting N = 500 stocks observed at the
daily time scale, with time series of length T = 2000 days. The distribution of eigenvalues
of E is shown in Figure 20.5. We observe a rather broad distribution centered around 1, but
with a slowly decaying tail and a top eigenvalue Î»1 found to be âˆ¼100 times larger than the
mean. The top eigenvector v corresponds to the dominant risk factor. It is closely aligned
with the uniform mode [e]i = 1/
âˆš
N, i.e. all stocks moving in sync â€“ hence the name
â€œmarket modeâ€ to describe the top eigenvector. Numerically, one ï¬nds |vT e| â‰ˆ0.95.
20.4.2 A One-Factor Model
The simplest model aimed at describing the co-movement of different stocks is to assume
that the return ri,t of stock i at time t can be decomposed into a common factor ft and iid
idiosyncratic components Îµi,t, to wit,
0
1
2
3
4
l
0.0
0.5
1.0
1.5
P(l)
3
10
30
100
300
l
10âˆ’4
10âˆ’3
10âˆ’2
P> (l)
Figure 20.5 Eigenvalue distribution of the scm, averaged over for three random sets of 500 US stocks,
each measured on 2000 business days. Returns are normalized as in Figure 20.3, corresponding to
Â¯Î» = 0.97. The inset shows the complementary cumulative distribution for the largest eigenvalues
indicating a power-law behavior for large Î», as P>(Î») â‰ˆÎ»âˆ’4/3. Note the largest eigenvalue Î»1 â‰ˆ
0.2N, which corresponds to the â€œmarket modeâ€, i.e. the risk factor where all stocks move in the same
direction.

20.4 Empirical Covariance Matrices
335
ri,t = Î²ift + Îµi,t,
(20.52)
where ft is often thought of as the â€œmarket factorâ€. Assuming further that ft,Îµi,t are
uncorrelated random variables of mean zero and variance, respectively, Ïƒ 2
f and Ïƒ 2
Îµ , the
covariance matrix Cij is simply given by
Cij = Î²iÎ²jÏƒ 2
f + Î´ijÏƒ 2
Îµ .
(20.53)
Hence, the matrix C is equal to Ïƒ 2
Îµ 1 plus a rank-1 perturbation Ïƒ 2
f Î²Î²T . The eigenvalues
are all equal to Ïƒ 2
Îµ , save the largest one, equal to Ïƒ 2
Îµ + Ïƒ 2
f |Î²|2. The corresponding top
eigenvector uf is parallel to Î². When the Î²iâ€™s are not too far from one another, this top
eigenvector is aligned with the uniform vector e, as found empirically.
From the analysis conducted in Chapter 14, we know that the eigenvalues of the empir-
ical matrix corresponding to such a model are composed of a MarË‡cenkoâ€“Pastur â€œseaâ€
between Î»âˆ’= Ïƒ 2
Îµ (1 âˆ’âˆšq)2 and Î»+ = Ïƒ 2
Îµ (1 + âˆšq)2, and an outlier located at
Î»1 = Ïƒ 2
Îµ (1 + a)(1 + q
a ),
(20.54)
with a = Ïƒ 2
f |Î²|2/Ïƒ 2
Îµ , and provided a > âˆšq (see Section 14.4). Since |Î²|2 = O(N), the
last condition is easily satisï¬ed for large portfolios. When a â‰«1, one thus ï¬nds
Î»1 â‰ˆÏƒ 2
f |Î²|2.
(20.55)
Since empirically Î»1 â‰ˆ0.2N for the correlation matrix for which Tr C = N, we deduce
that for that normalization Ïƒ 2
Îµ â‰ˆ0.8. The MarË‡cenkoâ€“Pastur sea for the value of q =
1/4 used in Figure 20.5 should thus extend between Î»âˆ’â‰ˆ0.2 and Î»+ â‰ˆ1.8. Figure
20.5 however reveals that âˆ¼20 eigenvalues lie beyond Î»+, a clear sign that more factors
are needed to describe the co-movement of stocks. This is expected: the industrial sector
(energy, ï¬nancial, technology, etc.) to which a given stock belongs is bound to have some
inï¬‚uence on its returns as well.
20.4.3 The Rotationally Invariant Estimator for Stocks
We now determine the optimal rie corresponding to the empirical spectrum shown in
Figure 20.5. As explained in Chapter 19, there are two possible ways to do this. One is
to use Eq. (19.44) with an appropriately regularized empirical Stieltjes transform â€“ for
example by adding a small imaginary part to Î» equal to Nâˆ’1/2. The second is to use a cross-
validation method, see Eq. (19.88), which is theoretically equivalent as we have shown in
Section 19.6. The two methods are compared in Figure 20.6, and agree quite well provided
one chooses a slightly higher, effective value qâˆ—, so as to mimic the effect of temporal
correlations and ï¬‚uctuating variance that lead to an effective reduction of the size of the
sample (see the discussion in Section 17.2.3).
The shape of the non-linear function Î¾(Î») is interesting. It is broadly in line with the
inverse-Wishart toy model shown in Figure 19.2: Î¾(Î») is concave for small Î», becomes
approximately linear within the MarË‡cenkoâ€“Pastur region, and becomes convex for larger Î».

336
Applications to Finance
0.0
0.5
1.0
1.5
2.0
2.5
3.0
l
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x(l)
cross-validation
RIE q = 1
4
RIE q* = 1
3
0
5
10
15
20
0
5
10
15
20
Figure 20.6 Non-linear shrinkage function Î¾(Î») computed using cross-validation and rie averaged
over three datasets. Each dataset consists of 500 US stocks measured over 2000 business days. Cross-
validation is computed by removing a block of 100 days (20 times) to compute the out-of-sample
variance of each eigenvector (see Eq. (19.88)). rie is computed using the sample Stieltjes transform
evaluated with an imaginary part Î· = Nâˆ’1/2. Results are shown for q = N/T = 1/4 and also for
qâˆ—= 1/3, chosen to mimic the effects of temporal correlations and ï¬‚uctuating variance that lead
to an effective reduction of the size of the sample (cf. Section 17.2.3). All three curves have been
regularized through an isotonic ï¬t, i.e. a ï¬t that respects the monotonicity of the function.
For very large Î», however, Î¾(Î») becomes linear again (not shown), in a way compatible
with the general formula Eq. (19.57). The use of rie for portfolio construction in real
applications is discussed in the references below.
Bibliographical Notes
â€¢ For a general introduction on Markowitz portfolio construction, see
â€“ E. J. Elton and M. J. Gruber. Modern portfolio theory, 1950 to date. Journal of
Banking & Finance, 21(11):1743â€“1759, 1997,
â€“ M. Rubinstein. Markowitzâ€™s portfolio selection: A ï¬fty-year retrospective. The Jour-
nal of Finance, 57(3):1041â€“1045, 2002,
â€“ P. N. Kolm, R. Ttnc, and F. J. Fabozzi. 60 years of portfolio optimization: Prac-
tical challenges and current trends. European Journal of Operational Research,
234(2):356â€“371, 2014.
â€¢ For an application of the rie technique to portfolio construction, see
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning correlation matrices. Risk magazine,
2016,
â€“ J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1â€“109, 2017,

20.4 Empirical Covariance Matrices
337
â€“ P.-A. Reigneron, V. Nguyen, S. Ciliberti, P. Seager, and J.-P. Bouchaud. Agnostic
allocation portfolios: A sweet spot in the risk-based jungle? The Journal of Portfolio
Management, 46 (4), 22â€“38, 2020,
and references therein.
â€¢ For reviews on the main stylized facts of asset returns, see, e.g.
â€“ R. Cont. Empirical properties of asset returns: Stylized facts and statistical issues.
Quantitative Finance, 1(2):223â€“236, 2001,
â€“ J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pric-
ing: From Statistical Physics to Risk Management. Cambridge University Press,
Cambridge, 2nd edition, 2003,
â€“ A. Chakraborti, I. M. Toke, M. Patriarca, and F. Abergel. Econophysics review: I.
Empirical facts. Quantitative Finance, 11(7):991â€“1012, 2011,
â€“ J.-P. Bouchaud, J. Bonart, J. Donier, and M. Gould. Trades, Quotes and Prices.
Cambridge University Press, Cambridge, 2nd edition, 2018.
â€¢ For early work on the comparison between the MarË‡cenkoâ€“Pastur spectrum and the eigen-
values of ï¬nancial covariance matrices, see
â€“ L. Laloux, P. Cizeau, J.-P. Bouchaud, and M. Potters. Noise dressing of ï¬nancial
correlation matrices. Physical Review Letters, 83:1467â€“1470, Aug 1999,
â€“ V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. N. Amaral, T. Guhr, and H. E. Stanley.
Random matrix approach to cross correlations in ï¬nancial data. Physical Review E,
65(6):066126, 2002.
â€¢ For a study of the dependence of the instantaneous covariance matrix on some dynamical
indicators, and the use of rmt in this case, see
â€“ P.-A. Reigneron, R. Allez, and J.-P. Bouchaud. Principal regression analysis and
the index leverage effect. Physica A: Statistical Mechanics and its Applications,
390(17):3026â€“3035, 2011,
â€“ A. Karami, R. Benichou, M. Benzaquen, and J.-P. Bouchaud. Conditional correla-
tions and principal regression analysis for futures. preprint arXiv:1912.12354, 2019.


Appendix
Mathematical Tools
A.1 Saddle Point Method
In this appendix, we brieï¬‚y review the saddle point method (sometimes also called the
Laplace method, the steepest descent or the stationary phase approximation). Consider the
integral
I =
 +âˆ
âˆ’âˆ
etF(x)dx.
(A.1)
We want to ï¬nd an approximation for this integral when t â†’âˆ. First consider the
case where F(x) is real. The key idea of the Laplace method is that when t is large I
is dominated by the maximum of F(x) plus Gaussian ï¬‚uctuations around it. Suppose F
reaches its maximum at a unique point xâˆ—, then around xâˆ—we have
F(x) = F(xâˆ—) + F â€²â€²(xâˆ—)
2
(x âˆ’xâˆ—)2 + O(|x âˆ’xâˆ—|3),
(A.2)
where F â€²â€²(xâˆ—) < 0. Thus for large t, we have, after a Gaussian integral over x âˆ’xâˆ—,
I âˆ¼
2
2Ï€
âˆ’F â€²â€²(xâˆ—)t etF(xâˆ—),
(A.3)
where the symbol âˆ¼means that the ratio of both sides of the equation tends to 1 as t â†’âˆ.
Often we are only interested in
lim
tâ†’âˆ
1
t log I = F(xâˆ—),
(A.4)
in which case we do not need to compute the prefactor.
Things are more subtle when F(x) is a complex analytic function of z = x + iy. One
could be tempted to think that one can just apply the Laplace method to Re(F(x)). But this
is grossly wrong, because exp(it Im(F(x))) oscillates so fast that the contribution of any
small neighborhood of xâˆ—is killed.
The idea of the steepest descent or the stationary phase approximation relies on the fact
that, for any analytic function, the Cauchyâ€“Riemann condition ensures that
âƒ—âˆ‡Re(F(z)) Â· âƒ—âˆ‡Im(F(z)) = 0,
(A.5)
339

340
Mathematical Tools
where the gradient is in the two-dimensional complex plane. This means that the contour
lines of Re(F(z)) are everywhere orthogonal to the contour lines of Im(F(z)), or alterna-
tively that along the lines of steepest ascent (or descent) of Re(F(z)), the imaginary part
Im(F(z)) is constant.
Now, one can always deform the integration contour from the real line in Eq. (A.1)
to any curve in the complex plane that starts at z = âˆ’âˆ+ i0 and ends at z = +âˆ+ i0.
Since exp(tF(z)) has no poles, this (by Cauchyâ€™s theorem) does not change the value of the
integral. But again because of the Cauchyâ€“Riemann condition, âˆ‡2 Re(F(z)) = 0, which
means that Re(F(z)) has no maximum or minimum in the complex plane, but may have
a saddle point zâˆ—where âƒ—âˆ‡Re(F(z)) = 0, increasing in one direction and decreasing in
another (see Fig. A.1). Choosing the contour that crosses the saddle point zâˆ—by following a
path that is locally orthogonal to the contour lines of Re(F(z)) allows the phase Im(F(z))
to be stationary, hence avoiding the nulliï¬cation of the integral by rapid oscillations. The
ï¬nal result is then
I âˆ¼
2
2Ï€
âˆ’tF â€²â€²(zâˆ—) etF(zâˆ—).
(A.6)
The method is illustrated in Figure A.1 for the example of the Airy function, deï¬ned as
Ai(t) = 1
2Ï€
 +âˆ
âˆ’âˆ
dzetF(z,t),
F(z,t) := i

z + z3
3t

.
(A.7)
For a ï¬xed, large positive t, the points for which F â€²(z,t) = 0 are given by zÂ± = Â±i âˆšt. The
contour lines of Re(F(z,t)) are plotted in Figure A.1. For Im(z) < 0, Re(F(z,t)) â†’+âˆ
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
x
âˆ’6
âˆ’4
âˆ’2
0
2
4
6
y
0i
0i
0i
-12
-12
-9
-9
-6
-6
-4
-4
-3
-3
-2
-2
-1
0
1
2
2
3
3
4
4
6
6
9
9
12
12
Figure A.1 Contour lines of the real part of F(z,t) = i

z + z3/3t

for t = 16 (black lines). The
iso-phase line Im F(z,t) = 0 is also shown (gray lines). The black circle and square are the two
solutions zÂ± of F â€²(z,t) = 0. The relevant saddle is zâˆ—= +4i, for which F(zâˆ—,t) = âˆ’8/3.

A.2 Tricomiâ€™s Formula
341
when Re(z) â†’Â±âˆ, so one cannot deform the contour in that direction without introducing
enormous uncontrollable contributions. When Im(z) > 0, on the contrary, Re(F(z,t)) â†’
âˆ’âˆwhen Re(z) â†’Â±âˆ, so one can start at z = âˆ’âˆ+ i0, and travel upwards in the
complex plane to the line where Im(F(z,t)) = 0 in a region where Re(F(z,t)) is so
negative that there is no contribution to the integral from this part. Then one stays on the
iso-phase line Im(F(z,t)) = 0 and climbs upwards to the saddle point zâˆ—= z+ = +i âˆšt,
for which F(zâˆ—,t) = âˆ’2 âˆšt/3. One then continues on the same iso-phase line downwards,
and closes the contour towards z = +âˆ+ i0. Hence, we conclude that
lim
tâ†’âˆlog Ai(t) = tF(zâˆ—,t) = âˆ’2
3t3/2.
(A.8)
A more precise expression, including the prefactor in Eq. (A.6), is written
Ai(t) âˆ¼
1
2 âˆšÏ€t1/4 eâˆ’2
3 t3/2.
(A.9)
Exercise A.1.1
Saddle point method for the factorial function: Stirlingâ€™s approx-
imation
We are going to estimate the factorial function for large arguments using an
integral representation and the saddle point approximation:
n! = 
[n + 1] =
 âˆ
0
xneâˆ’xdx.
(A.10)
(a)
Write n! in the form Eq. (A.1) for some function F(x).
(b)
Show that xâˆ—= n is the solution to F â€²(x) = 0.
(c)
Let I0(n) = nF(xâˆ—(n)) be an approximation of log(n! ). Compare this
approximation to the exact value for n = 10 and 100.
(d)
Include the Gaussian corrections to the saddle point: Let I1(n) = log(I) where
I is given by Eq. (A.3) for your function F(x). Show that
I1(n) = n log(n) âˆ’n + 1
2 log(2Ï€n).
(A.11)
(e)
Compare I1(n) and log(n! ) for n = 10 and 100.
A.2 Tricomiâ€™s Formula
Suppose one has the following integral equation to solve for Ï(x):
f (x) = âˆ’

dxâ€² Ï(xâ€²)
x âˆ’xâ€²,
(A.12)
where f (x) is an arbitrary function. This problem arises in many contexts, in particular
when one studies the equilibrium density of the eigenvalues of some random matrices as in
Section 5.5, or the equilibrium density of dislocations in solids, see Landau et al. [2012],
Chapter 30, from which the material of the present appendix is derived.

342
Mathematical Tools
We will consider the case where some â€œhard wallsâ€ are present at x = a and x = b,
conï¬ning the density to an interval smaller than its â€œnaturalâ€ extension in the absence of
these walls. By continuity, we will also get the solution when these walls are exactly located
at these natural boundaries of the spectrum.1
The general solution of Eq. (A.12) is given by Tricomiâ€™s formula which reads
Ï(x) = âˆ’
1
Ï€2 âˆš(x âˆ’a)(b âˆ’x)

âˆ’
 b
a
dxâ€² *
(xâ€² âˆ’a)(b âˆ’xâ€²) f (xâ€²)
x âˆ’xâ€² + C

,
(A.13)
where C is a constant to be determined by some conditions that Ï(x) must fulï¬ll.
The simplest case corresponds to f (x) = 0, i.e. no conï¬ning potential apart from the
walls. The solution then reads
Ï(x) = âˆ’
C
Ï€2 âˆš(x âˆ’a)(b âˆ’x).
(A.14)
The normalization of Ï(x) then yields C = âˆ’Ï€ and one recovers the arcsine law encoun-
tered in Sections 5.5, 7.2 and 15.3.1:
Ï(x) =
1
Ï€ âˆš(x âˆ’a)(b âˆ’x).
(A.15)
The canonical Wigner case corresponds to f (x) = x/2Ïƒ 2. We look for the values of a,b
that are precisely such that the density vanishes at these points, so that the conï¬ning walls
no longer have any effect. By symmetry one must have a = âˆ’b. One then obtains
Ï(x) = âˆ’
1
2Ï€2Ïƒ 2 âˆš(x + b)(b âˆ’x)

âˆ’
 b
âˆ’b
dxâ€² *
(xâ€² + b)(b âˆ’xâ€²)xâ€² âˆ’x + x
x âˆ’xâ€²
+ C

.
(A.16)
Using
x âˆ’
 b
âˆ’b
dxâ€²
âˆš(xâ€² + b)(b âˆ’xâ€²)
x âˆ’xâ€²
= Ï€x2
(A.17)
one has
Ï(x) = âˆ’
1
2Ï€2Ïƒ 2 âˆš(x + b)(b âˆ’x)

Ï€x2 + Câ€²
,
(A.18)
where Câ€² = C âˆ’
 b
âˆ’b dxâ€² âˆš(xâ€² + b)(b âˆ’xâ€²) = C âˆ’Ï€b2/2.
For Ï(x) to vanish at the edges, we need to choose Câ€² = âˆ’Ï€b2, ï¬nally leading to
Ï(x) =
1
2Ï€Ïƒ 2
*
b2 âˆ’x2.
(A.19)
Finally, for this distribution to be normalized one must choose b = 2Ïƒ, as it should be.
1 Formulas directly adapted to two free boundaries, or one free boundary and one hard wall, can be found in Landau et al.
[2012], Chapter 30.

A.3 Toeplitz and Circulant Matrices
343
The case studied by Dean and Majumdar, i.e. when all eigenvalues of a Wigner matrix
are contrained to be positive (Eq. (5.93)), corresponds to Eq. (A.13) with a = 0, b > 0
and Ïƒ = 1 (i.e. f (x) = x/2), determined again such that Ï(b) = 0. The solution is
bâˆ—= 4/
âˆš
3 and
Ï(x) = 1
4Ï€
1
bâˆ—âˆ’x
x
(bâˆ—+ 2x).
(A.20)
Note that very generically the density of eigenvalues (or dislocations) diverges as 1/
âˆš
d
near a hard wall, where d is the distance to that wall. When the boundary is free, on the
other hand, the density of eigenvalues (or dislocations) vanishes as
âˆš
d.
A.3 Toeplitz and Circulant Matrices
A Toeplitz matrix is such that its elements Kij only depend on the â€œdistanceâ€ between
indices |i âˆ’j|. An example of a Toeplitz matrix is provided by the covariance of the
elements of a stationary time series. For instance consider an AR(1) process xt in the steady
state, deï¬ned by
xt = axtâˆ’1 + Îµt,
(A.21)
where Îµt are iid centered random numbers with variance 1/(1 âˆ’a2) such that xt has unit
variance in the steady state. Then
Kts = E[xtxs] = aâˆ’|tâˆ’s| with 0 < a < 1,
(A.22)
describing decaying exponential correlations. The parameter a measures the decay of the
correlation; we can deï¬ne a correlation time Ï„c := 1/(1 âˆ’a). This time is always â‰¥1
(equal to 1 when K = 1) and tends to inï¬nity as a â†’1. More explicitly, K is a T Ã— T
matrix that reads
K =
â›
âœâœâœâœâœâœâœâœâ
1
a
a2
. . .
aT âˆ’2
aT âˆ’1
a
1
a
. . .
aT âˆ’3
aT âˆ’2
a2
a
1
. . .
aT âˆ’4
aT âˆ’3
...
...
...
...
...
...
aT âˆ’2
aT âˆ’3
aT âˆ’4
. . .
1
a
aT âˆ’1
aT âˆ’2
aT âˆ’3
. . .
a
1
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(A.23)
In an inï¬nite system, it can be diagonalized by plane waves (Fourier transform), since
+âˆ

s=âˆ’âˆ
Ktse2Ï€ixs = e2Ï€ixt
+âˆ

s=âˆ’âˆ
Ktse2Ï€ix(sâˆ’t) = e2Ï€ixt
+âˆ

â„“=âˆ’âˆ
a|â„“|e2Ï€ixâ„“,
(A.24)
showing that e2Ï€ixs are eigenvectors of K as soon as its elements only depend on |s âˆ’t|.

344
Mathematical Tools
For ï¬nite T , however, this diagonalization is only approximate as there are â€œboundary
effectsâ€ at the edge of the matrix. If the correlation time Ï„c is not too large (i.e. if the
matrix elements Kts decay sufï¬ciently fast with |t âˆ’s|) these boundary effects should be
negligible. One way to make this diagonalization exact is to modify the matrix so as to have
the distance |t âˆ’s| deï¬ned on a circle.2 We deï¬ne a new matrix ËœK by
ËœKts = aâˆ’min(|tâˆ’s|,|tâˆ’s+T |,|tâˆ’sâˆ’T |),
(A.25)
ËœK =
â›
âœâœâœâœâœâœâœâœâ
1
a
a2
. . .
a2
a
a
1
a
. . .
a3
a2
a2
a
1
. . .
a4
a3
...
...
...
...
...
...
a2
a3
a4
. . .
1
a
a
a2
a3
. . .
a
1
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
.
(A.26)
It may seem that we have greatly modiï¬ed matrix K as we have changed about half of its
elements, but if Ï„c â‰ªT most of the elements we have changed were essentially zero and
remain essentially zero. Only a ï¬nite number (â‰ˆ2Ï„ 2
c ) of elements in the bottom right and
top left corners have really changed. Changing a ï¬nite number of off-diagonal elements in
an asymptotically large matrix should not change its spectrum. The matrix ËœK, which we
will call K again, is called a circulant matrix and can be exactly diagonalized by Fourier
transform for ï¬nite T . More precisely, its eigenvectors are
[vk]â„“= e2Ï€ikâ„“/T for 0 â‰¤k â‰¤T /2.
(A.27)
Note that to each vk correspond two eigenvectors, namely its real and imaginary parts,
except for v0 and vT/2 which are real and have multiplicity 1. The eigenvalues associated
with k = 0 and k = T /2 are, respectively, the largest (Î»+) and smallest (Î»âˆ’) and are
given by
Î»+ = 1 + 2
T/2âˆ’1

k=1
ak + aT/2 â‰ˆ1 + a
1 âˆ’a,
Î»âˆ’= 1 + 2
T/2âˆ’1

k=1
(âˆ’a)k + (âˆ’a)T/2 â‰ˆ1 âˆ’a
1 + a = 1
Î»+
.
(A.28)
In terms of the correlation time: Î»+ = 2Ï„c âˆ’1. We label the eigenvalues of K by an index
xk = 2k/T so that 0 â‰¤xk â‰¤1. As T â†’âˆ, xk becomes a continuous parameter x and the
different multiplicity of the ï¬rst and the last eigenvalues does not matter. The eigenvalues
can be written
Î»(x) =
1 âˆ’a2
1 + a2 âˆ’2a cos(Ï€x) for 0 â‰¤x â‰¤1.
(A.29)
2 The Toeplitz matrix K can in fact be diagonalized exactly, see O. Narayan, B. Sriram Shastry, arXiv:2006.15436v2.

A.3 Toeplitz and Circulant Matrices
345
For a more general form Kts = K(|t âˆ’s|), the eigenvalues read
Î»(x) = 1 + 2
âˆ

â„“=1
K(â„“) cos(Ï€xâ„“).
(A.30)
The T-transform of K can then be computed as
tK(z) =
 1
0
1 âˆ’a2
z(1 + a2 âˆ’2a cos(Ï€x)) âˆ’(1 âˆ’a2)dx.
(A.31)
Using
 1
0
dx
c âˆ’d cos(Ï€x) =
1
âˆšc âˆ’d âˆšc + d
,
(A.32)
and after some manipulations we ï¬nd
tK(z) =
1
âˆšz âˆ’Î»âˆ’
âˆšz âˆ’Î»+
with Î»Â± = 1 Â± a
1 âˆ“a .
(A.33)
We can also deduce the density of eigenvalues (see Fig. A.2):
ÏK(Î») =
1
Ï€Î»
*
(Î» âˆ’Î»âˆ’)(Î»+ âˆ’Î»)
for Î»âˆ’< Î» < Î»+.
(A.34)
This density has integrable singularities at Î» = Î»Â±. It is normalized and its mean is 1. We
can also invert tK(z) with the equation
t2Î¶ 2
K âˆ’2bt2Î¶K + t2 âˆ’1 = 0, where b = 1 + a2
1 âˆ’a2,
(A.35)
0
2
4
6
0
1
2
3
4
5
6
a = 0.25
a = 0.5
a = 0.75
Figure A.2 Density of eigenvalues for the decaying exponential covariance matrix K for three values
of a: 0.25, 0.5 and 0.75.

346
Mathematical Tools
and get
Î¶K(t) = bt2 +
*
(b2 âˆ’1)t4 + t2
t2
,
(A.36)
so the S-transform is given by
SK(t) =
t + 1
*
1 + (b2 âˆ’1)t2 + bt
(A.37)
= 1 âˆ’(b âˆ’1)t + O(t2),
where the last equality tells us that the matrix K has mean 1 and variance Ïƒ 2
K = b âˆ’1 =
a2/(1 âˆ’a2).
Bibliographical Notes
â€¢ On the saddle point method, see the particularly clear exposition in
â€“ J. Hinch. Perturbation Methods. Cambridge Texts in Applied Mathematics. Cam-
bridge University Press, Cambridge, 1991.
â€¢ About the Tricomi method and the role of boundaries, Chapter 30 of Landau and Lif-
shitzâ€™s Theory of Elasticity is enlightening:
â€“ L. D. Landau, L. P. Pitaevskii, A. M. Kosevich, and E. M. Lifshitz. Theory of Elas-
ticity. Butterworth-Heinemann, Oxford, 3rd edition, 2012.
For its use in the context of random matrix theory, see
â€“ D. S. Dean and S. N. Majumdar. Extreme value statistics of eigenvalues of Gaussian
random matrices. Physical Review E, 77:041108, 2008.

Index
AR(1) process, 343
arcsine law, 80, 101, 106, 195
Bachelier, 330
BBP transition, 225
Bernoulli variable, 284
beta, 30
distribution, 97
ensemble, 59
branch cut, 22, 52, 70
Brownian motion, 112
Burgersâ€™ equation, 128, 136, 263
Catalan numbers, 37
Cauchy
distribution, 28
kernel, 24, 315
transform, see Stieltjes transform
central limit theorem, 160, 169, 183
characteristic
function, 112, 158
polynomial, 5
expected, 85
higher moment, 95
Chebyshev polynomial, 104, 186
Cholesky decomposition, 56
circulant matrix, 343
circular law, 34
concentration of measure, 16
conjugate prior, 286
conjugate variable, 118
constant, 156, 162
convolution, 111
Coulomb potential, 64
cross-covariance, 278
cross-validation, 317
cumulant, 112, 157, 163
mixed, 168, 181
Dirac delta, 20, 142
Dyson
Brownian motion, 122
index, see beta
Edgeworth series, 186
eigenvalue, 5
density, 16
critical, 71, 228
edge, 26, 71
Jacobi, 100
sample, 20
white Wishart, 50
Wigner, 18, 26
interlacing theorem, 10
multiplicity, 5
repulsion, 64
eigenvector, 5
overlap, 227, 297
elastic net, 291
empirical spectral distribution, 20
ergodicity, 118
Eulerâ€“Matytsin equation, 151
excess return, 321
expected shortfall, 322
exponential
correlation, 343
moving average, 269
fat tail distribution, 229, 285, 331
ï¬nite free
addition, 187
product, 190
Fokkerâ€“Planck equation, 119, 131
free
addition, 140, 163
log-normal, 261
product, 56, 170
347

348
Index
freeness, 162
collection, 169, 196
large matrices, 177, 243
Frobenius norm, 17
gamma distribution, 46, 97
Gaussian distribution, 112
Gaussian ensemble, 16
orthogonal (goe), 17
symplectic (gse), 33
unitary (gue), 32
Gershgorin circle theorem, 9
Ginibre ensemble, 34
Girkoâ€™s law, see circular law
Gramâ€“Schmidt procedure, 56
Haar measure, 140, 178
Harish-Chandra integral, see hciz integral
hciz integral
low rank, 209
hciz integral, 141
full rank, 149
low rank, 141
Hermite polynomial, 84, 188
Hermitian matrix, 30
Hessian matrix, 74
heteroskedasticity, 276, 333
Hilbert transform, 68, 299
Hubbardâ€“Stratonovich identity, 202
in-sample, 291, 317, 324
independence, 156
pairwise, 161
instanton, 152
inverse-gamma distribution, 248, 287
inverse-Wishart, 246, 295, 328
involution, 156
isotonic regression, 316
ItË†o
lemma, 114, 124
prescription, 113
Itzyksonâ€“Zuber integral, see hciz integral
Jacobi
ensemble, 98, 255
centered-range, 102
polynomial, 103, 104
Jacobian, 13, 47, 59
Jensen inequality, 324
Karlinâ€“McGregor formula, 133, 150
Kesten problem, 264
lâ€™Hospitalâ€™s rule, 227
Laguerre polynomial, 88, 189
Langevin equation, 116, 119
Laplace distribution, 285
Laplacian, discrete, 106
large deviation, 76, 79
Laurent polynomial, 66
law of large numbers, 160, 169
Legendre polynomial, 103
level spacing, 63
local universality, 130, 135
Lyapunov exponent, 259
Mahalanobis norm, 56
MANOVA ensemble, 97
map estimator, 282
MarË‡cenkoâ€“Pastur, see Wishart matrix
market mode, 334
Markowitz optimization, 322
Master equation, 4
matrix
derivative, 12
determinant lemma, 11, 232
function, 12
matrix potential, 58
and Hilbert transform, 68
convex, 70
inverse-Wishart, 247
Jacobi, 100
non-polynomial, 68
white Wishart, 47, 59
Wigner, 19, 58
Matytsin, see Eulerâ€“Matytsin equation
mave estimator, 282
maximum likelihood, 64
mmse estimator, 282, 302
moment, 17, 156
addition, 156
generating function, 20, 158
Wigner matrix, 17
momentâ€“cumulant relation
commutative, 159
free, 167
multivariate Gaussian, 44
non-crossing partition, 37, 166
normal matrix, 9
normalized trace, 16
one-cut assumption, 70
one-factor model, 334
oracle, 303
Ornsteinâ€“Uhlenbeck process, 116
orthogonal
ensemble, 58
matrix, 6, 18
out-of-sample, 292, 317, 324

Index
349
outlier, 149, 221, 308
overï¬tting, 293
Perronâ€“Frobenius theorem, 9, 309
phase retrieval, 234
planar diagrams, 72
polynomial
monic, 5, 186
real-rooted, 186
portfolio, 321
risk, 322
positive semi-deï¬nite, 4
precision matrix, 327
principal component analysis, 6
principal components, 56
pseudo-inverse, 179
quarter-circle law, 34
quaternion, 32
R-transform, 138, 224
arcsine law, 249
identity matrix, 244
inverse-Wishart, 247
Jacobi ensemble, 256
scaling, 139, 244
symmetric orthogonal matrix, 249
uniform density, 250
white Wishart, 138
Wigner, 138
rank-1 matrix, 141, 223
regression, 289
lasso, 291
ridge, 289
replica trick, 199
resolvent, 19, 126, 204
Ricatti equation, 83
risk free rate, 321
rotationally invariant ensemble, 18
rotationally invariant estimator, 302, 327
S-transform, 172, 233
arcsine law, 256
identity matrix, 172, 244
inverse matrix, 175
inverse-Wishart, 247
Jacobi ensemble, 256
scaling, 172, 244
white Wishart, 246
Wigner, 245, 246
saddle point method, 339
sample covariance matrix, 43, 267
scalar, 156
Schur complement, 11, 21, 48
self-averaging, 16
semi-circle law, 18, 26
Shermanâ€“Morrison formula, 11, 223
shrinkage, 283, 296, 305
signature plot, 331
singular values, 7
Sokhotskiâ€“Plemelj formula, 26, 172
spin-glass, 215
Â±
âƒâˆšÂ· notation, 54
*-algebra, 155
stationary phase approximation, 143, 339
Stieltjes transform
correct branch, 23
discrete, 19
identity matrix, 244
inverse-Wishart, 247
inversion formula, 26
invertibility, 145, 224
Jacobi, 100
limiting, 20
uniform density, 250
white Wishart, 50
Wigner, 22
Stirlingâ€™s approximation, 341
stochastic
calculus, 112
matrix, 4
Stratonovich prescription, 113
Studentâ€™s t-distribution, 118, 331
subordination relation
addition, 170
product, 174
resolvent, 207
symmetric
matrix, 5
orthogonal matrix, 13, 139, 249
symplectic
ensemble, 59
matrix, 33
T-matrix, 172
T-transform, 171
identity matrix, 244
inverse matrix, 175
temporal correlation, 271, 330
Tikhonov regularization, 290
Toeplitz matrix, 343
traceless, 162
tracial state, 155
training set, 317
Tricomi equation, 79, 341
uniform density, 69, 250
unitary
ensemble, 59, 93
matrix, 31
validation set, 290, 317
Vandermonde determinant, 61, 91
variance, 156

350
Index
ï¬‚uctuation, 276, 333
Wigner matrix, 18
Wishart matrix, 46
Weingarten function
orthogonal, 178
unitary, 181
whitening, 56
Wickâ€™s theorem, 45, 178
Wiener process, 112
Wigner
ensemble, 17, 30
kernel, 316
surmise, 63
Wishart matrix, 45
complex, 92
non-white, 55
white, 47

